{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['58 tGravityAcc-energy()-Y', '59 tGravityAcc-energy()-Z', '104 tBodyAccJerk-entropy()-Y', '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y', '132 tBodyGyro-max()-Z', '134 tBodyGyro-min()-Y','138 tBodyGyro-energy()-Y', '141 tBodyGyro-iqr()-Y',\n",
    " '167 tBodyGyroJerk-mad()-X','168 tBodyGyroJerk-mad()-Y','177 tBodyGyroJerk-energy()-X', '181 tBodyGyroJerk-iqr()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8', '484 fBodyGyro-bandsEnergy()-17,32','487 fBodyGyro-bandsEnergy()-1,24']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X', '7 tBodyAcc-mad()-X', '10 tBodyAcc-max()-X', '17 tBodyAcc-energy()-X', '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()', '215 tGravityAccMag-std()', '217 tGravityAccMag-max()', '269 fBodyAcc-std()-X', '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X', '286 fBodyAcc-iqr()-Y', '303 fBodyAcc-bandsEnergy()-1,8', '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '368 fBodyAccJerk-entropy()-Y', '390 fBodyAccJerk-bandsEnergy()-1,16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>104 tBodyAccJerk-entropy()-Y</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>132 tBodyGyro-max()-Z</th>\n",
       "      <th>134 tBodyGyro-min()-Y</th>\n",
       "      <th>138 tBodyGyro-energy()-Y</th>\n",
       "      <th>141 tBodyGyro-iqr()-Y</th>\n",
       "      <th>167 tBodyGyroJerk-mad()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>217 tGravityAccMag-max()</th>\n",
       "      <th>269 fBodyAcc-std()-X</th>\n",
       "      <th>275 fBodyAcc-max()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>286 fBodyAcc-iqr()-Y</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>368 fBodyAccJerk-entropy()-Y</th>\n",
       "      <th>390 fBodyAccJerk-bandsEnergy()-1,16</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.793046</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>-0.747566</td>\n",
       "      <td>0.914895</td>\n",
       "      <td>-0.999354</td>\n",
       "      <td>-0.978614</td>\n",
       "      <td>-0.992165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.946305</td>\n",
       "      <td>-0.995422</td>\n",
       "      <td>-0.993756</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.989709</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.655362</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>-0.745870</td>\n",
       "      <td>0.908110</td>\n",
       "      <td>-0.999897</td>\n",
       "      <td>-0.989345</td>\n",
       "      <td>-0.989876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.978711</td>\n",
       "      <td>-0.998680</td>\n",
       "      <td>-0.999372</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.980784</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.673274</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>-0.743277</td>\n",
       "      <td>0.905753</td>\n",
       "      <td>-0.999828</td>\n",
       "      <td>-0.995144</td>\n",
       "      <td>-0.987868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.986496</td>\n",
       "      <td>-0.996313</td>\n",
       "      <td>-0.998158</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.977242</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.754968</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>-0.743277</td>\n",
       "      <td>0.905753</td>\n",
       "      <td>-0.999902</td>\n",
       "      <td>-0.994165</td>\n",
       "      <td>-0.991241</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.986496</td>\n",
       "      <td>-0.996312</td>\n",
       "      <td>-0.997404</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.991902</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.746258</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>-0.749780</td>\n",
       "      <td>0.911184</td>\n",
       "      <td>-0.999952</td>\n",
       "      <td>-0.993337</td>\n",
       "      <td>-0.992882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990962</td>\n",
       "      <td>-0.998606</td>\n",
       "      <td>-0.999277</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.988180</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.605462</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>-0.015472</td>\n",
       "      <td>0.396623</td>\n",
       "      <td>-0.419947</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>-0.533656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.221989</td>\n",
       "      <td>-0.318185</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.395202</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>0.455341</td>\n",
       "      <td>-0.775736</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.608132</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>-0.223612</td>\n",
       "      <td>0.373761</td>\n",
       "      <td>-0.405579</td>\n",
       "      <td>0.023374</td>\n",
       "      <td>-0.609540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069568</td>\n",
       "      <td>-0.267430</td>\n",
       "      <td>-0.332146</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.547702</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>0.357697</td>\n",
       "      <td>-0.780751</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.497936</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>-0.176254</td>\n",
       "      <td>0.373761</td>\n",
       "      <td>-0.305023</td>\n",
       "      <td>0.073383</td>\n",
       "      <td>-0.662918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083233</td>\n",
       "      <td>-0.173212</td>\n",
       "      <td>-0.160368</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.588790</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>0.422191</td>\n",
       "      <td>-0.783616</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.501824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>-0.176254</td>\n",
       "      <td>0.473542</td>\n",
       "      <td>-0.298515</td>\n",
       "      <td>0.042519</td>\n",
       "      <td>-0.645452</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098052</td>\n",
       "      <td>-0.158192</td>\n",
       "      <td>-0.147421</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.548936</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>0.346965</td>\n",
       "      <td>-0.821137</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.505438</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>-0.262266</td>\n",
       "      <td>0.452599</td>\n",
       "      <td>-0.474331</td>\n",
       "      <td>-0.370964</td>\n",
       "      <td>-0.688840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051914</td>\n",
       "      <td>-0.270794</td>\n",
       "      <td>-0.417612</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.409732</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>0.340934</td>\n",
       "      <td>-0.825848</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      58 tGravityAcc-energy()-Y  59 tGravityAcc-energy()-Z  \\\n",
       "0                     -0.970905                  -0.975510   \n",
       "1                     -0.970583                  -0.978500   \n",
       "2                     -0.970368                  -0.981672   \n",
       "3                     -0.969400                  -0.982420   \n",
       "4                     -0.967051                  -0.984363   \n",
       "...                         ...                        ...   \n",
       "7347                  -0.918375                  -0.995193   \n",
       "7348                  -0.902880                  -0.995151   \n",
       "7349                  -0.907561                  -0.995450   \n",
       "7350                  -0.910648                  -0.998824   \n",
       "7351                  -0.910579                  -0.998144   \n",
       "\n",
       "      104 tBodyAccJerk-entropy()-Y  125 tBodyGyro-std()-Y  \\\n",
       "0                        -0.793046              -0.976623   \n",
       "1                        -0.655362              -0.989046   \n",
       "2                        -0.673274              -0.993552   \n",
       "3                        -0.754968              -0.992407   \n",
       "4                        -0.746258              -0.992378   \n",
       "...                            ...                    ...   \n",
       "7347                      0.605462               0.084878   \n",
       "7348                      0.608132               0.098249   \n",
       "7349                      0.497936               0.185902   \n",
       "7350                      0.501824               0.190360   \n",
       "7351                      0.505438               0.022216   \n",
       "\n",
       "      128 tBodyGyro-mad()-Y  132 tBodyGyro-max()-Z  134 tBodyGyro-min()-Y  \\\n",
       "0                 -0.976353              -0.747566               0.914895   \n",
       "1                 -0.989038              -0.745870               0.908110   \n",
       "2                 -0.994122              -0.743277               0.905753   \n",
       "3                 -0.993142              -0.743277               0.905753   \n",
       "4                 -0.992542              -0.749780               0.911184   \n",
       "...                     ...                    ...                    ...   \n",
       "7347               0.065142              -0.015472               0.396623   \n",
       "7348               0.091791              -0.223612               0.373761   \n",
       "7349               0.170686              -0.176254               0.373761   \n",
       "7350               0.178939              -0.176254               0.473542   \n",
       "7351              -0.073681              -0.262266               0.452599   \n",
       "\n",
       "      138 tBodyGyro-energy()-Y  141 tBodyGyro-iqr()-Y  \\\n",
       "0                    -0.999354              -0.978614   \n",
       "1                    -0.999897              -0.989345   \n",
       "2                    -0.999828              -0.995144   \n",
       "3                    -0.999902              -0.994165   \n",
       "4                    -0.999952              -0.993337   \n",
       "...                        ...                    ...   \n",
       "7347                 -0.419947               0.019043   \n",
       "7348                 -0.405579               0.023374   \n",
       "7349                 -0.305023               0.073383   \n",
       "7350                 -0.298515               0.042519   \n",
       "7351                 -0.474331              -0.370964   \n",
       "\n",
       "      167 tBodyGyroJerk-mad()-X  ...  217 tGravityAccMag-max()  \\\n",
       "0                     -0.992165  ...                 -0.946305   \n",
       "1                     -0.989876  ...                 -0.978711   \n",
       "2                     -0.987868  ...                 -0.986496   \n",
       "3                     -0.991241  ...                 -0.986496   \n",
       "4                     -0.992882  ...                 -0.990962   \n",
       "...                         ...  ...                       ...   \n",
       "7347                  -0.533656  ...                  0.000026   \n",
       "7348                  -0.609540  ...                 -0.069568   \n",
       "7349                  -0.662918  ...                 -0.083233   \n",
       "7350                  -0.645452  ...                 -0.098052   \n",
       "7351                  -0.688840  ...                 -0.051914   \n",
       "\n",
       "      269 fBodyAcc-std()-X  275 fBodyAcc-max()-X  282 fBodyAcc-energy()-X  \\\n",
       "0                -0.995422             -0.993756                -0.999968   \n",
       "1                -0.998680             -0.999372                -0.999991   \n",
       "2                -0.996313             -0.998158                -0.999969   \n",
       "3                -0.996312             -0.997404                -0.999975   \n",
       "4                -0.998606             -0.999277                -0.999990   \n",
       "...                    ...                   ...                      ...   \n",
       "7347             -0.221989             -0.318185                -0.674230   \n",
       "7348             -0.267430             -0.332146                -0.705580   \n",
       "7349             -0.173212             -0.160368                -0.692379   \n",
       "7350             -0.158192             -0.147421                -0.693098   \n",
       "7351             -0.270794             -0.417612                -0.731037   \n",
       "\n",
       "      286 fBodyAcc-iqr()-Y  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                -0.989709                       -0.999963   \n",
       "1                -0.980784                       -0.999996   \n",
       "2                -0.977242                       -0.999989   \n",
       "3                -0.991902                       -0.999989   \n",
       "4                -0.988180                       -0.999994   \n",
       "...                    ...                             ...   \n",
       "7347             -0.395202                       -0.684177   \n",
       "7348             -0.547702                       -0.726986   \n",
       "7349             -0.588790                       -0.655263   \n",
       "7350             -0.548936                       -0.643425   \n",
       "7351             -0.409732                       -0.709495   \n",
       "\n",
       "      315 fBodyAcc-bandsEnergy()-1,24  368 fBodyAccJerk-entropy()-Y  \\\n",
       "0                           -0.999971                     -1.000000   \n",
       "1                           -0.999992                     -1.000000   \n",
       "2                           -0.999972                     -1.000000   \n",
       "3                           -0.999977                     -1.000000   \n",
       "4                           -0.999991                     -1.000000   \n",
       "...                               ...                           ...   \n",
       "7347                        -0.668164                      0.455341   \n",
       "7348                        -0.705435                      0.357697   \n",
       "7349                        -0.684729                      0.422191   \n",
       "7350                        -0.685088                      0.346965   \n",
       "7351                        -0.727441                      0.340934   \n",
       "\n",
       "      390 fBodyAccJerk-bandsEnergy()-1,16  Subject  \n",
       "0                               -0.999982        1  \n",
       "1                               -0.999987        1  \n",
       "2                               -0.999963        1  \n",
       "3                               -0.999978        1  \n",
       "4                               -0.999988        1  \n",
       "...                                   ...      ...  \n",
       "7347                            -0.775736       30  \n",
       "7348                            -0.780751       30  \n",
       "7349                            -0.783616       30  \n",
       "7350                            -0.821137       30  \n",
       "7351                            -0.825848       30  \n",
       "\n",
       "[7352 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[(X_train['Subject'] == 1) | (X_train['Subject'] == 3) | (X_train['Subject'] == 5)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Subject'] == 1) | (y_train['Subject'] == 3) | (y_train['Subject'] == 5)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 32):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.46254026889801, Final Batch Loss: 1.1207815408706665\n",
      "Epoch 2, Loss: 4.4357898235321045, Final Batch Loss: 1.1027637720108032\n",
      "Epoch 3, Loss: 4.420918107032776, Final Batch Loss: 1.0981957912445068\n",
      "Epoch 4, Loss: 4.405193567276001, Final Batch Loss: 1.0957447290420532\n",
      "Epoch 5, Loss: 4.404281854629517, Final Batch Loss: 1.101460576057434\n",
      "Epoch 6, Loss: 4.38023841381073, Final Batch Loss: 1.101142168045044\n",
      "Epoch 7, Loss: 4.369419574737549, Final Batch Loss: 1.0993815660476685\n",
      "Epoch 8, Loss: 4.340961217880249, Final Batch Loss: 1.0874613523483276\n",
      "Epoch 9, Loss: 4.324763774871826, Final Batch Loss: 1.088905692100525\n",
      "Epoch 10, Loss: 4.327301263809204, Final Batch Loss: 1.079336404800415\n",
      "Epoch 11, Loss: 4.289063811302185, Final Batch Loss: 1.0742193460464478\n",
      "Epoch 12, Loss: 4.271415114402771, Final Batch Loss: 1.0777920484542847\n",
      "Epoch 13, Loss: 4.267841100692749, Final Batch Loss: 1.0603107213974\n",
      "Epoch 14, Loss: 4.20968770980835, Final Batch Loss: 1.0226712226867676\n",
      "Epoch 15, Loss: 4.211079835891724, Final Batch Loss: 1.056872010231018\n",
      "Epoch 16, Loss: 4.173359513282776, Final Batch Loss: 1.0465346574783325\n",
      "Epoch 17, Loss: 4.1141815185546875, Final Batch Loss: 1.022156834602356\n",
      "Epoch 18, Loss: 4.099012613296509, Final Batch Loss: 1.0340900421142578\n",
      "Epoch 19, Loss: 4.071403801441193, Final Batch Loss: 1.0408568382263184\n",
      "Epoch 20, Loss: 3.9467862248420715, Final Batch Loss: 0.9717062711715698\n",
      "Epoch 21, Loss: 3.9347116947174072, Final Batch Loss: 0.994558572769165\n",
      "Epoch 22, Loss: 3.8597071766853333, Final Batch Loss: 0.9758622646331787\n",
      "Epoch 23, Loss: 3.815354645252228, Final Batch Loss: 0.9457790851593018\n",
      "Epoch 24, Loss: 3.684311270713806, Final Batch Loss: 0.8788928389549255\n",
      "Epoch 25, Loss: 3.6766178011894226, Final Batch Loss: 0.9371622204780579\n",
      "Epoch 26, Loss: 3.6030226945877075, Final Batch Loss: 0.9023363590240479\n",
      "Epoch 27, Loss: 3.5353574752807617, Final Batch Loss: 0.9065833687782288\n",
      "Epoch 28, Loss: 3.4799413084983826, Final Batch Loss: 0.8810824155807495\n",
      "Epoch 29, Loss: 3.46121883392334, Final Batch Loss: 0.8748937845230103\n",
      "Epoch 30, Loss: 3.343297302722931, Final Batch Loss: 0.8148302435874939\n",
      "Epoch 31, Loss: 3.31024706363678, Final Batch Loss: 0.800442636013031\n",
      "Epoch 32, Loss: 3.294755756855011, Final Batch Loss: 0.7925550937652588\n",
      "Epoch 33, Loss: 3.1241052746772766, Final Batch Loss: 0.7084662318229675\n",
      "Epoch 34, Loss: 3.2139461040496826, Final Batch Loss: 0.7918608784675598\n",
      "Epoch 35, Loss: 3.21706086397171, Final Batch Loss: 0.8533185124397278\n",
      "Epoch 36, Loss: 3.107014775276184, Final Batch Loss: 0.7792490124702454\n",
      "Epoch 37, Loss: 3.1251354217529297, Final Batch Loss: 0.806300163269043\n",
      "Epoch 38, Loss: 3.007079541683197, Final Batch Loss: 0.6762315630912781\n",
      "Epoch 39, Loss: 3.05866539478302, Final Batch Loss: 0.7537637948989868\n",
      "Epoch 40, Loss: 3.0337976217269897, Final Batch Loss: 0.7996439933776855\n",
      "Epoch 41, Loss: 2.911620855331421, Final Batch Loss: 0.672481894493103\n",
      "Epoch 42, Loss: 3.0110567212104797, Final Batch Loss: 0.7636783123016357\n",
      "Epoch 43, Loss: 3.009068548679352, Final Batch Loss: 0.7747998833656311\n",
      "Epoch 44, Loss: 2.908286988735199, Final Batch Loss: 0.7396832704544067\n",
      "Epoch 45, Loss: 2.891039550304413, Final Batch Loss: 0.7222073078155518\n",
      "Epoch 46, Loss: 2.94842529296875, Final Batch Loss: 0.7733601331710815\n",
      "Epoch 47, Loss: 2.804627299308777, Final Batch Loss: 0.6591688990592957\n",
      "Epoch 48, Loss: 2.874590277671814, Final Batch Loss: 0.7778103947639465\n",
      "Epoch 49, Loss: 2.886648178100586, Final Batch Loss: 0.7468591332435608\n",
      "Epoch 50, Loss: 2.819745659828186, Final Batch Loss: 0.7313882112503052\n",
      "Epoch 51, Loss: 2.830013155937195, Final Batch Loss: 0.711867094039917\n",
      "Epoch 52, Loss: 2.7683308720588684, Final Batch Loss: 0.6665970087051392\n",
      "Epoch 53, Loss: 2.8121140003204346, Final Batch Loss: 0.7154704332351685\n",
      "Epoch 54, Loss: 2.7153490781784058, Final Batch Loss: 0.6598049402236938\n",
      "Epoch 55, Loss: 2.7531580328941345, Final Batch Loss: 0.6115932464599609\n",
      "Epoch 56, Loss: 2.7118767499923706, Final Batch Loss: 0.6662489771842957\n",
      "Epoch 57, Loss: 2.7167664766311646, Final Batch Loss: 0.6839002370834351\n",
      "Epoch 58, Loss: 2.701328694820404, Final Batch Loss: 0.6151239275932312\n",
      "Epoch 59, Loss: 2.701864719390869, Final Batch Loss: 0.6363567113876343\n",
      "Epoch 60, Loss: 2.782107949256897, Final Batch Loss: 0.7491760849952698\n",
      "Epoch 61, Loss: 2.647751271724701, Final Batch Loss: 0.6488255262374878\n",
      "Epoch 62, Loss: 2.63789039850235, Final Batch Loss: 0.6264380216598511\n",
      "Epoch 63, Loss: 2.6585482358932495, Final Batch Loss: 0.6640567183494568\n",
      "Epoch 64, Loss: 2.6462067365646362, Final Batch Loss: 0.6762369275093079\n",
      "Epoch 65, Loss: 2.647491693496704, Final Batch Loss: 0.7518184781074524\n",
      "Epoch 66, Loss: 2.661406457424164, Final Batch Loss: 0.750809371471405\n",
      "Epoch 67, Loss: 2.5157846808433533, Final Batch Loss: 0.5559506416320801\n",
      "Epoch 68, Loss: 2.523772418498993, Final Batch Loss: 0.5922326445579529\n",
      "Epoch 69, Loss: 2.5594241619110107, Final Batch Loss: 0.5843343138694763\n",
      "Epoch 70, Loss: 2.514494776725769, Final Batch Loss: 0.6144233345985413\n",
      "Epoch 71, Loss: 2.5696756839752197, Final Batch Loss: 0.6646106243133545\n",
      "Epoch 72, Loss: 2.529181957244873, Final Batch Loss: 0.6254456639289856\n",
      "Epoch 73, Loss: 2.3935651183128357, Final Batch Loss: 0.534683346748352\n",
      "Epoch 74, Loss: 2.530986487865448, Final Batch Loss: 0.6276665329933167\n",
      "Epoch 75, Loss: 2.4239709973335266, Final Batch Loss: 0.5823655724525452\n",
      "Epoch 76, Loss: 2.4416614174842834, Final Batch Loss: 0.5796002745628357\n",
      "Epoch 77, Loss: 2.472296953201294, Final Batch Loss: 0.660708487033844\n",
      "Epoch 78, Loss: 2.4767353534698486, Final Batch Loss: 0.6739282011985779\n",
      "Epoch 79, Loss: 2.5054765939712524, Final Batch Loss: 0.6584860682487488\n",
      "Epoch 80, Loss: 2.375700533390045, Final Batch Loss: 0.557944118976593\n",
      "Epoch 81, Loss: 2.4903149604797363, Final Batch Loss: 0.7157551050186157\n",
      "Epoch 82, Loss: 2.4171290397644043, Final Batch Loss: 0.6081240177154541\n",
      "Epoch 83, Loss: 2.4116836190223694, Final Batch Loss: 0.6072737574577332\n",
      "Epoch 84, Loss: 2.329179286956787, Final Batch Loss: 0.5049829483032227\n",
      "Epoch 85, Loss: 2.323423743247986, Final Batch Loss: 0.5335619449615479\n",
      "Epoch 86, Loss: 2.400088310241699, Final Batch Loss: 0.627453625202179\n",
      "Epoch 87, Loss: 2.349127411842346, Final Batch Loss: 0.6575470566749573\n",
      "Epoch 88, Loss: 2.325024366378784, Final Batch Loss: 0.6417258381843567\n",
      "Epoch 89, Loss: 2.2715969681739807, Final Batch Loss: 0.574108362197876\n",
      "Epoch 90, Loss: 2.2850568890571594, Final Batch Loss: 0.6253966689109802\n",
      "Epoch 91, Loss: 2.177207797765732, Final Batch Loss: 0.45766934752464294\n",
      "Epoch 92, Loss: 2.2461301386356354, Final Batch Loss: 0.645694375038147\n",
      "Epoch 93, Loss: 2.2274560630321503, Final Batch Loss: 0.5609466433525085\n",
      "Epoch 94, Loss: 2.243430256843567, Final Batch Loss: 0.6111828684806824\n",
      "Epoch 95, Loss: 2.1491981744766235, Final Batch Loss: 0.49687284231185913\n",
      "Epoch 96, Loss: 2.2564859986305237, Final Batch Loss: 0.5971527695655823\n",
      "Epoch 97, Loss: 2.2141165733337402, Final Batch Loss: 0.6257023811340332\n",
      "Epoch 98, Loss: 2.2307523488998413, Final Batch Loss: 0.5622039437294006\n",
      "Epoch 99, Loss: 2.220996171236038, Final Batch Loss: 0.6550907492637634\n",
      "Epoch 100, Loss: 2.120398163795471, Final Batch Loss: 0.534185528755188\n",
      "Epoch 101, Loss: 2.0961329638957977, Final Batch Loss: 0.5297552943229675\n",
      "Epoch 102, Loss: 2.1039204597473145, Final Batch Loss: 0.49746960401535034\n",
      "Epoch 103, Loss: 2.0792189836502075, Final Batch Loss: 0.5124910473823547\n",
      "Epoch 104, Loss: 2.0561080276966095, Final Batch Loss: 0.5109484791755676\n",
      "Epoch 105, Loss: 2.075475513935089, Final Batch Loss: 0.4866507351398468\n",
      "Epoch 106, Loss: 2.0499883592128754, Final Batch Loss: 0.5148244500160217\n",
      "Epoch 107, Loss: 2.003257304430008, Final Batch Loss: 0.4465923011302948\n",
      "Epoch 108, Loss: 2.0484143793582916, Final Batch Loss: 0.49422410130500793\n",
      "Epoch 109, Loss: 2.0310331881046295, Final Batch Loss: 0.4859325587749481\n",
      "Epoch 110, Loss: 2.013445168733597, Final Batch Loss: 0.4091254770755768\n",
      "Epoch 111, Loss: 2.0044547617435455, Final Batch Loss: 0.4211088716983795\n",
      "Epoch 112, Loss: 2.0703845024108887, Final Batch Loss: 0.5204761624336243\n",
      "Epoch 113, Loss: 1.9848237037658691, Final Batch Loss: 0.5422587394714355\n",
      "Epoch 114, Loss: 1.9635710716247559, Final Batch Loss: 0.4966263473033905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 1.9992477893829346, Final Batch Loss: 0.46284279227256775\n",
      "Epoch 116, Loss: 2.0786145627498627, Final Batch Loss: 0.5714457035064697\n",
      "Epoch 117, Loss: 2.0094995200634003, Final Batch Loss: 0.5522043108940125\n",
      "Epoch 118, Loss: 1.8884226977825165, Final Batch Loss: 0.42965462803840637\n",
      "Epoch 119, Loss: 1.9299651384353638, Final Batch Loss: 0.48140838742256165\n",
      "Epoch 120, Loss: 1.96125128865242, Final Batch Loss: 0.5051612854003906\n",
      "Epoch 121, Loss: 1.988013356924057, Final Batch Loss: 0.535514235496521\n",
      "Epoch 122, Loss: 1.909878134727478, Final Batch Loss: 0.45976826548576355\n",
      "Epoch 123, Loss: 1.8081561028957367, Final Batch Loss: 0.37854087352752686\n",
      "Epoch 124, Loss: 1.9088220000267029, Final Batch Loss: 0.47315800189971924\n",
      "Epoch 125, Loss: 1.8978033363819122, Final Batch Loss: 0.4691257178783417\n",
      "Epoch 126, Loss: 2.027652680873871, Final Batch Loss: 0.6075213551521301\n",
      "Epoch 127, Loss: 1.7796018421649933, Final Batch Loss: 0.4141729176044464\n",
      "Epoch 128, Loss: 1.8222093284130096, Final Batch Loss: 0.4388998746871948\n",
      "Epoch 129, Loss: 1.8822587430477142, Final Batch Loss: 0.43902498483657837\n",
      "Epoch 130, Loss: 1.8137340545654297, Final Batch Loss: 0.4459523558616638\n",
      "Epoch 131, Loss: 1.805796504020691, Final Batch Loss: 0.37071341276168823\n",
      "Epoch 132, Loss: 1.8051202297210693, Final Batch Loss: 0.40759599208831787\n",
      "Epoch 133, Loss: 2.0136044919490814, Final Batch Loss: 0.5760730504989624\n",
      "Epoch 134, Loss: 1.8295939266681671, Final Batch Loss: 0.47578203678131104\n",
      "Epoch 135, Loss: 1.9348175525665283, Final Batch Loss: 0.497121125459671\n",
      "Epoch 136, Loss: 1.8288880288600922, Final Batch Loss: 0.4300503432750702\n",
      "Epoch 137, Loss: 1.8680526912212372, Final Batch Loss: 0.44007235765457153\n",
      "Epoch 138, Loss: 1.7870231568813324, Final Batch Loss: 0.4147073030471802\n",
      "Epoch 139, Loss: 1.7503965497016907, Final Batch Loss: 0.3428262770175934\n",
      "Epoch 140, Loss: 1.8181049227714539, Final Batch Loss: 0.4426870346069336\n",
      "Epoch 141, Loss: 1.7821938395500183, Final Batch Loss: 0.40900319814682007\n",
      "Epoch 142, Loss: 1.7856369018554688, Final Batch Loss: 0.3316211998462677\n",
      "Epoch 143, Loss: 1.8217409551143646, Final Batch Loss: 0.4154943525791168\n",
      "Epoch 144, Loss: 1.8242466747760773, Final Batch Loss: 0.43054160475730896\n",
      "Epoch 145, Loss: 1.865486741065979, Final Batch Loss: 0.5606768131256104\n",
      "Epoch 146, Loss: 1.7908954322338104, Final Batch Loss: 0.46199992299079895\n",
      "Epoch 147, Loss: 1.8909959495067596, Final Batch Loss: 0.5190817713737488\n",
      "Epoch 148, Loss: 1.72421532869339, Final Batch Loss: 0.37817269563674927\n",
      "Epoch 149, Loss: 1.833372324705124, Final Batch Loss: 0.5072493553161621\n",
      "Epoch 150, Loss: 1.7159942090511322, Final Batch Loss: 0.38814640045166016\n",
      "Epoch 151, Loss: 1.8256049752235413, Final Batch Loss: 0.4814598858356476\n",
      "Epoch 152, Loss: 1.7816084921360016, Final Batch Loss: 0.3838754892349243\n",
      "Epoch 153, Loss: 1.7572544813156128, Final Batch Loss: 0.4013368785381317\n",
      "Epoch 154, Loss: 1.7831794619560242, Final Batch Loss: 0.39978721737861633\n",
      "Epoch 155, Loss: 1.7402081787586212, Final Batch Loss: 0.3984502851963043\n",
      "Epoch 156, Loss: 1.858135163784027, Final Batch Loss: 0.5181815028190613\n",
      "Epoch 157, Loss: 1.806270271539688, Final Batch Loss: 0.4012244641780853\n",
      "Epoch 158, Loss: 1.8381653726100922, Final Batch Loss: 0.5573962330818176\n",
      "Epoch 159, Loss: 1.7474860846996307, Final Batch Loss: 0.3855120837688446\n",
      "Epoch 160, Loss: 1.750550627708435, Final Batch Loss: 0.47852790355682373\n",
      "Epoch 161, Loss: 1.6934970617294312, Final Batch Loss: 0.35061314702033997\n",
      "Epoch 162, Loss: 1.7793278694152832, Final Batch Loss: 0.47846266627311707\n",
      "Epoch 163, Loss: 1.7961953580379486, Final Batch Loss: 0.4875739514827728\n",
      "Epoch 164, Loss: 1.7215790748596191, Final Batch Loss: 0.44065865874290466\n",
      "Epoch 165, Loss: 1.6995766758918762, Final Batch Loss: 0.4059927463531494\n",
      "Epoch 166, Loss: 1.69598788022995, Final Batch Loss: 0.4311351180076599\n",
      "Epoch 167, Loss: 1.7259183824062347, Final Batch Loss: 0.34812891483306885\n",
      "Epoch 168, Loss: 1.7319145202636719, Final Batch Loss: 0.447613388299942\n",
      "Epoch 169, Loss: 1.6775338351726532, Final Batch Loss: 0.42221009731292725\n",
      "Epoch 170, Loss: 1.7685591280460358, Final Batch Loss: 0.4389956593513489\n",
      "Epoch 171, Loss: 1.7351261675357819, Final Batch Loss: 0.4315250515937805\n",
      "Epoch 172, Loss: 1.7828127145767212, Final Batch Loss: 0.4994232654571533\n",
      "Epoch 173, Loss: 1.6880418062210083, Final Batch Loss: 0.4294261634349823\n",
      "Epoch 174, Loss: 1.6922345757484436, Final Batch Loss: 0.45072299242019653\n",
      "Epoch 175, Loss: 1.757120668888092, Final Batch Loss: 0.4664769768714905\n",
      "Epoch 176, Loss: 1.6838326752185822, Final Batch Loss: 0.34215253591537476\n",
      "Epoch 177, Loss: 1.732431322336197, Final Batch Loss: 0.477986216545105\n",
      "Epoch 178, Loss: 1.8185324668884277, Final Batch Loss: 0.5117220878601074\n",
      "Epoch 179, Loss: 1.839797019958496, Final Batch Loss: 0.43009525537490845\n",
      "Epoch 180, Loss: 1.6215353906154633, Final Batch Loss: 0.35838329792022705\n",
      "Epoch 181, Loss: 1.6395885050296783, Final Batch Loss: 0.2928767502307892\n",
      "Epoch 182, Loss: 1.7398068606853485, Final Batch Loss: 0.5144479870796204\n",
      "Epoch 183, Loss: 1.7238223850727081, Final Batch Loss: 0.4289649724960327\n",
      "Epoch 184, Loss: 1.717282086610794, Final Batch Loss: 0.45432642102241516\n",
      "Epoch 185, Loss: 1.6798333525657654, Final Batch Loss: 0.4355394244194031\n",
      "Epoch 186, Loss: 1.750040203332901, Final Batch Loss: 0.48735955357551575\n",
      "Epoch 187, Loss: 1.6667832434177399, Final Batch Loss: 0.33066391944885254\n",
      "Epoch 188, Loss: 1.745599240064621, Final Batch Loss: 0.41663381457328796\n",
      "Epoch 189, Loss: 1.7241377234458923, Final Batch Loss: 0.4587438106536865\n",
      "Epoch 190, Loss: 1.7046633958816528, Final Batch Loss: 0.49499452114105225\n",
      "Epoch 191, Loss: 1.6202402114868164, Final Batch Loss: 0.3792005777359009\n",
      "Epoch 192, Loss: 1.6953159868717194, Final Batch Loss: 0.3865872025489807\n",
      "Epoch 193, Loss: 1.6826317310333252, Final Batch Loss: 0.4315509498119354\n",
      "Epoch 194, Loss: 1.6774373650550842, Final Batch Loss: 0.3491421043872833\n",
      "Epoch 195, Loss: 1.6716462075710297, Final Batch Loss: 0.3979890048503876\n",
      "Epoch 196, Loss: 1.6871511936187744, Final Batch Loss: 0.45508721470832825\n",
      "Epoch 197, Loss: 1.6893744468688965, Final Batch Loss: 0.4427362084388733\n",
      "Epoch 198, Loss: 1.6914407014846802, Final Batch Loss: 0.46375957131385803\n",
      "Epoch 199, Loss: 1.6074825525283813, Final Batch Loss: 0.36869046092033386\n",
      "Epoch 200, Loss: 1.6162249743938446, Final Batch Loss: 0.3454716205596924\n",
      "Epoch 201, Loss: 1.6736401617527008, Final Batch Loss: 0.458168625831604\n",
      "Epoch 202, Loss: 1.7148244976997375, Final Batch Loss: 0.5048492550849915\n",
      "Epoch 203, Loss: 1.6366746425628662, Final Batch Loss: 0.3481539785861969\n",
      "Epoch 204, Loss: 1.6450871229171753, Final Batch Loss: 0.3465103209018707\n",
      "Epoch 205, Loss: 1.6279383301734924, Final Batch Loss: 0.42191600799560547\n",
      "Epoch 206, Loss: 1.688496321439743, Final Batch Loss: 0.41686445474624634\n",
      "Epoch 207, Loss: 1.6836934685707092, Final Batch Loss: 0.4296970069408417\n",
      "Epoch 208, Loss: 1.7431703805923462, Final Batch Loss: 0.5248567461967468\n",
      "Epoch 209, Loss: 1.6425648629665375, Final Batch Loss: 0.42167654633522034\n",
      "Epoch 210, Loss: 1.678258866071701, Final Batch Loss: 0.4273330867290497\n",
      "Epoch 211, Loss: 1.6853155493736267, Final Batch Loss: 0.3927699625492096\n",
      "Epoch 212, Loss: 1.578009307384491, Final Batch Loss: 0.3505125045776367\n",
      "Epoch 213, Loss: 1.6858532130718231, Final Batch Loss: 0.44528910517692566\n",
      "Epoch 214, Loss: 1.5605094730854034, Final Batch Loss: 0.31877657771110535\n",
      "Epoch 215, Loss: 1.684400737285614, Final Batch Loss: 0.43934160470962524\n",
      "Epoch 216, Loss: 1.6127737760543823, Final Batch Loss: 0.46340176463127136\n",
      "Epoch 217, Loss: 1.7504655420780182, Final Batch Loss: 0.5636313557624817\n",
      "Epoch 218, Loss: 1.5669794976711273, Final Batch Loss: 0.3522159159183502\n",
      "Epoch 219, Loss: 1.6821602582931519, Final Batch Loss: 0.5525450110435486\n",
      "Epoch 220, Loss: 1.62276890873909, Final Batch Loss: 0.3776041269302368\n",
      "Epoch 221, Loss: 1.6306540369987488, Final Batch Loss: 0.5078322291374207\n",
      "Epoch 222, Loss: 1.642275482416153, Final Batch Loss: 0.43271464109420776\n",
      "Epoch 223, Loss: 1.5389527380466461, Final Batch Loss: 0.3344425857067108\n",
      "Epoch 224, Loss: 1.6068383753299713, Final Batch Loss: 0.39567330479621887\n",
      "Epoch 225, Loss: 1.6103469729423523, Final Batch Loss: 0.4174734354019165\n",
      "Epoch 226, Loss: 1.6442541182041168, Final Batch Loss: 0.4437049627304077\n",
      "Epoch 227, Loss: 1.5731719732284546, Final Batch Loss: 0.3556334972381592\n",
      "Epoch 228, Loss: 1.56429323554039, Final Batch Loss: 0.3580615818500519\n",
      "Epoch 229, Loss: 1.53531014919281, Final Batch Loss: 0.38049086928367615\n",
      "Epoch 230, Loss: 1.6729298830032349, Final Batch Loss: 0.4508135914802551\n",
      "Epoch 231, Loss: 1.6173272728919983, Final Batch Loss: 0.3244182765483856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, Loss: 1.5888424515724182, Final Batch Loss: 0.3477987051010132\n",
      "Epoch 233, Loss: 1.4960710108280182, Final Batch Loss: 0.2846378982067108\n",
      "Epoch 234, Loss: 1.585477501153946, Final Batch Loss: 0.36152133345603943\n",
      "Epoch 235, Loss: 1.6741330027580261, Final Batch Loss: 0.4488990008831024\n",
      "Epoch 236, Loss: 1.577851265668869, Final Batch Loss: 0.40044957399368286\n",
      "Epoch 237, Loss: 1.5899345576763153, Final Batch Loss: 0.3361367881298065\n",
      "Epoch 238, Loss: 1.6636081635951996, Final Batch Loss: 0.4771822690963745\n",
      "Epoch 239, Loss: 1.5211482346057892, Final Batch Loss: 0.38095733523368835\n",
      "Epoch 240, Loss: 1.5664551258087158, Final Batch Loss: 0.35198986530303955\n",
      "Epoch 241, Loss: 1.611538589000702, Final Batch Loss: 0.45732375979423523\n",
      "Epoch 242, Loss: 1.6216195821762085, Final Batch Loss: 0.46444860100746155\n",
      "Epoch 243, Loss: 1.71381875872612, Final Batch Loss: 0.5076996684074402\n",
      "Epoch 244, Loss: 1.5484177470207214, Final Batch Loss: 0.3378707766532898\n",
      "Epoch 245, Loss: 1.5459803342819214, Final Batch Loss: 0.3445214331150055\n",
      "Epoch 246, Loss: 1.5216987133026123, Final Batch Loss: 0.33777761459350586\n",
      "Epoch 247, Loss: 1.503871738910675, Final Batch Loss: 0.29905205965042114\n",
      "Epoch 248, Loss: 1.6089838743209839, Final Batch Loss: 0.4131945073604584\n",
      "Epoch 249, Loss: 1.5616521835327148, Final Batch Loss: 0.3690427839756012\n",
      "Epoch 250, Loss: 1.575184404850006, Final Batch Loss: 0.4313523471355438\n",
      "Epoch 251, Loss: 1.5064682960510254, Final Batch Loss: 0.3322657644748688\n",
      "Epoch 252, Loss: 1.5168797671794891, Final Batch Loss: 0.4046790897846222\n",
      "Epoch 253, Loss: 1.553045630455017, Final Batch Loss: 0.3968138098716736\n",
      "Epoch 254, Loss: 1.6151655912399292, Final Batch Loss: 0.4121907353401184\n",
      "Epoch 255, Loss: 1.6021639108657837, Final Batch Loss: 0.46539008617401123\n",
      "Epoch 256, Loss: 1.6009369194507599, Final Batch Loss: 0.39623281359672546\n",
      "Epoch 257, Loss: 1.5761108994483948, Final Batch Loss: 0.3743658661842346\n",
      "Epoch 258, Loss: 1.625987946987152, Final Batch Loss: 0.4389091432094574\n",
      "Epoch 259, Loss: 1.483306109905243, Final Batch Loss: 0.3043443560600281\n",
      "Epoch 260, Loss: 1.518265962600708, Final Batch Loss: 0.3202182948589325\n",
      "Epoch 261, Loss: 1.5768133401870728, Final Batch Loss: 0.36173537373542786\n",
      "Epoch 262, Loss: 1.7072728276252747, Final Batch Loss: 0.4701024293899536\n",
      "Epoch 263, Loss: 1.5890534222126007, Final Batch Loss: 0.3582095801830292\n",
      "Epoch 264, Loss: 1.6663400530815125, Final Batch Loss: 0.4914098381996155\n",
      "Epoch 265, Loss: 1.568214237689972, Final Batch Loss: 0.4285174310207367\n",
      "Epoch 266, Loss: 1.5446805953979492, Final Batch Loss: 0.3483473062515259\n",
      "Epoch 267, Loss: 1.594589501619339, Final Batch Loss: 0.3918444514274597\n",
      "Epoch 268, Loss: 1.473010778427124, Final Batch Loss: 0.33426979184150696\n",
      "Epoch 269, Loss: 1.4750538170337677, Final Batch Loss: 0.4012259542942047\n",
      "Epoch 270, Loss: 1.593824863433838, Final Batch Loss: 0.4282175302505493\n",
      "Epoch 271, Loss: 1.6078476011753082, Final Batch Loss: 0.4477117657661438\n",
      "Epoch 272, Loss: 1.5848925411701202, Final Batch Loss: 0.418221652507782\n",
      "Epoch 273, Loss: 1.461463302373886, Final Batch Loss: 0.3126583397388458\n",
      "Epoch 274, Loss: 1.618498057126999, Final Batch Loss: 0.4269045293331146\n",
      "Epoch 275, Loss: 1.455532193183899, Final Batch Loss: 0.31472551822662354\n",
      "Epoch 276, Loss: 1.545844942331314, Final Batch Loss: 0.3849775791168213\n",
      "Epoch 277, Loss: 1.4080458134412766, Final Batch Loss: 0.22731326520442963\n",
      "Epoch 278, Loss: 1.6020941138267517, Final Batch Loss: 0.4389416575431824\n",
      "Epoch 279, Loss: 1.471989780664444, Final Batch Loss: 0.3334875702857971\n",
      "Epoch 280, Loss: 1.5635473728179932, Final Batch Loss: 0.4019946753978729\n",
      "Epoch 281, Loss: 1.4981609880924225, Final Batch Loss: 0.3451947569847107\n",
      "Epoch 282, Loss: 1.5675196945667267, Final Batch Loss: 0.4124060869216919\n",
      "Epoch 283, Loss: 1.5198397636413574, Final Batch Loss: 0.3580150008201599\n",
      "Epoch 284, Loss: 1.6439545452594757, Final Batch Loss: 0.4509531855583191\n",
      "Epoch 285, Loss: 1.559186577796936, Final Batch Loss: 0.4587092995643616\n",
      "Epoch 286, Loss: 1.5030722320079803, Final Batch Loss: 0.42203056812286377\n",
      "Epoch 287, Loss: 1.6332764029502869, Final Batch Loss: 0.47043970227241516\n",
      "Epoch 288, Loss: 1.4882208406925201, Final Batch Loss: 0.2913461923599243\n",
      "Epoch 289, Loss: 1.525408685207367, Final Batch Loss: 0.38012343645095825\n",
      "Epoch 290, Loss: 1.5664411783218384, Final Batch Loss: 0.3878267705440521\n",
      "Epoch 291, Loss: 1.5521975755691528, Final Batch Loss: 0.42125236988067627\n",
      "Epoch 292, Loss: 1.5346249341964722, Final Batch Loss: 0.38277724385261536\n",
      "Epoch 293, Loss: 1.5678319036960602, Final Batch Loss: 0.38934049010276794\n",
      "Epoch 294, Loss: 1.5602288246154785, Final Batch Loss: 0.3801209032535553\n",
      "Epoch 295, Loss: 1.4508329629898071, Final Batch Loss: 0.356647253036499\n",
      "Epoch 296, Loss: 1.4864037036895752, Final Batch Loss: 0.3640673756599426\n",
      "Epoch 297, Loss: 1.5184301137924194, Final Batch Loss: 0.4442341923713684\n",
      "Epoch 298, Loss: 1.463283121585846, Final Batch Loss: 0.306276798248291\n",
      "Epoch 299, Loss: 1.471218854188919, Final Batch Loss: 0.2935592234134674\n",
      "Epoch 300, Loss: 1.5684042870998383, Final Batch Loss: 0.41979143023490906\n",
      "Epoch 301, Loss: 1.5301717221736908, Final Batch Loss: 0.4207938015460968\n",
      "Epoch 302, Loss: 1.5658250153064728, Final Batch Loss: 0.44993552565574646\n",
      "Epoch 303, Loss: 1.5332922339439392, Final Batch Loss: 0.3874265253543854\n",
      "Epoch 304, Loss: 1.538959264755249, Final Batch Loss: 0.369828999042511\n",
      "Epoch 305, Loss: 1.5954812467098236, Final Batch Loss: 0.4827207326889038\n",
      "Epoch 306, Loss: 1.5388861894607544, Final Batch Loss: 0.3929244577884674\n",
      "Epoch 307, Loss: 1.571308583021164, Final Batch Loss: 0.47149455547332764\n",
      "Epoch 308, Loss: 1.5108310282230377, Final Batch Loss: 0.4390672445297241\n",
      "Epoch 309, Loss: 1.5155134797096252, Final Batch Loss: 0.3933185636997223\n",
      "Epoch 310, Loss: 1.5447301268577576, Final Batch Loss: 0.37635114789009094\n",
      "Epoch 311, Loss: 1.4743312299251556, Final Batch Loss: 0.33563101291656494\n",
      "Epoch 312, Loss: 1.582655280828476, Final Batch Loss: 0.5040158033370972\n",
      "Epoch 313, Loss: 1.5829480290412903, Final Batch Loss: 0.3794988989830017\n",
      "Epoch 314, Loss: 1.4531604945659637, Final Batch Loss: 0.3595874607563019\n",
      "Epoch 315, Loss: 1.4192449748516083, Final Batch Loss: 0.3270580768585205\n",
      "Epoch 316, Loss: 1.4777402877807617, Final Batch Loss: 0.36923912167549133\n",
      "Epoch 317, Loss: 1.4875150322914124, Final Batch Loss: 0.40859532356262207\n",
      "Epoch 318, Loss: 1.5350221991539001, Final Batch Loss: 0.3127320110797882\n",
      "Epoch 319, Loss: 1.5540707409381866, Final Batch Loss: 0.4284334182739258\n",
      "Epoch 320, Loss: 1.4430106580257416, Final Batch Loss: 0.30806058645248413\n",
      "Epoch 321, Loss: 1.5498204827308655, Final Batch Loss: 0.3834916949272156\n",
      "Epoch 322, Loss: 1.5236309468746185, Final Batch Loss: 0.3615625500679016\n",
      "Epoch 323, Loss: 1.4555574357509613, Final Batch Loss: 0.2836966812610626\n",
      "Epoch 324, Loss: 1.386616587638855, Final Batch Loss: 0.270891934633255\n",
      "Epoch 325, Loss: 1.5002402663230896, Final Batch Loss: 0.32758790254592896\n",
      "Epoch 326, Loss: 1.6079312562942505, Final Batch Loss: 0.5011798143386841\n",
      "Epoch 327, Loss: 1.5325581431388855, Final Batch Loss: 0.41037070751190186\n",
      "Epoch 328, Loss: 1.4660747051239014, Final Batch Loss: 0.38760265707969666\n",
      "Epoch 329, Loss: 1.4922146201133728, Final Batch Loss: 0.3900093138217926\n",
      "Epoch 330, Loss: 1.5195605754852295, Final Batch Loss: 0.41662031412124634\n",
      "Epoch 331, Loss: 1.4631171822547913, Final Batch Loss: 0.3372235894203186\n",
      "Epoch 332, Loss: 1.481595516204834, Final Batch Loss: 0.37784814834594727\n",
      "Epoch 333, Loss: 1.5049324929714203, Final Batch Loss: 0.3833836019039154\n",
      "Epoch 334, Loss: 1.5462213456630707, Final Batch Loss: 0.3920566737651825\n",
      "Epoch 335, Loss: 1.358665257692337, Final Batch Loss: 0.2575004994869232\n",
      "Epoch 336, Loss: 1.4904370307922363, Final Batch Loss: 0.3822336792945862\n",
      "Epoch 337, Loss: 1.5018503665924072, Final Batch Loss: 0.3578217327594757\n",
      "Epoch 338, Loss: 1.4625895619392395, Final Batch Loss: 0.35506922006607056\n",
      "Epoch 339, Loss: 1.5902969539165497, Final Batch Loss: 0.4884780943393707\n",
      "Epoch 340, Loss: 1.4281354546546936, Final Batch Loss: 0.33486977219581604\n",
      "Epoch 341, Loss: 1.550476610660553, Final Batch Loss: 0.39771097898483276\n",
      "Epoch 342, Loss: 1.5205240845680237, Final Batch Loss: 0.3795669972896576\n",
      "Epoch 343, Loss: 1.4209098517894745, Final Batch Loss: 0.3077380359172821\n",
      "Epoch 344, Loss: 1.5037598609924316, Final Batch Loss: 0.37018850445747375\n",
      "Epoch 345, Loss: 1.491409569978714, Final Batch Loss: 0.3993360996246338\n",
      "Epoch 346, Loss: 1.4717596769332886, Final Batch Loss: 0.3512532711029053\n",
      "Epoch 347, Loss: 1.4542509615421295, Final Batch Loss: 0.307147741317749\n",
      "Epoch 348, Loss: 1.392647385597229, Final Batch Loss: 0.36443713307380676\n",
      "Epoch 349, Loss: 1.4018428027629852, Final Batch Loss: 0.32949915528297424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, Loss: 1.5289038717746735, Final Batch Loss: 0.4199568033218384\n",
      "Epoch 351, Loss: 1.5490712821483612, Final Batch Loss: 0.3778736889362335\n",
      "Epoch 352, Loss: 1.60634183883667, Final Batch Loss: 0.49673232436180115\n",
      "Epoch 353, Loss: 1.4698325395584106, Final Batch Loss: 0.2864762544631958\n",
      "Epoch 354, Loss: 1.5151329934597015, Final Batch Loss: 0.38673433661460876\n",
      "Epoch 355, Loss: 1.36726775765419, Final Batch Loss: 0.34147822856903076\n",
      "Epoch 356, Loss: 1.4528244733810425, Final Batch Loss: 0.3669106960296631\n",
      "Epoch 357, Loss: 1.4442530870437622, Final Batch Loss: 0.34034037590026855\n",
      "Epoch 358, Loss: 1.4893198609352112, Final Batch Loss: 0.39098361134529114\n",
      "Epoch 359, Loss: 1.4317610561847687, Final Batch Loss: 0.3210538625717163\n",
      "Epoch 360, Loss: 1.5031564235687256, Final Batch Loss: 0.4154200851917267\n",
      "Epoch 361, Loss: 1.4412689208984375, Final Batch Loss: 0.35596680641174316\n",
      "Epoch 362, Loss: 1.4900359809398651, Final Batch Loss: 0.3765724003314972\n",
      "Epoch 363, Loss: 1.507045954465866, Final Batch Loss: 0.4027666449546814\n",
      "Epoch 364, Loss: 1.4289551079273224, Final Batch Loss: 0.363228976726532\n",
      "Epoch 365, Loss: 1.5521837174892426, Final Batch Loss: 0.3565322458744049\n",
      "Epoch 366, Loss: 1.422239899635315, Final Batch Loss: 0.2805287539958954\n",
      "Epoch 367, Loss: 1.442900687456131, Final Batch Loss: 0.2607422173023224\n",
      "Epoch 368, Loss: 1.5273899137973785, Final Batch Loss: 0.46133187413215637\n",
      "Epoch 369, Loss: 1.4333179891109467, Final Batch Loss: 0.3221191465854645\n",
      "Epoch 370, Loss: 1.5094053447246552, Final Batch Loss: 0.4090139865875244\n",
      "Epoch 371, Loss: 1.4675160944461823, Final Batch Loss: 0.3523792326450348\n",
      "Epoch 372, Loss: 1.4876216053962708, Final Batch Loss: 0.38657382130622864\n",
      "Epoch 373, Loss: 1.4298609793186188, Final Batch Loss: 0.3529079258441925\n",
      "Epoch 374, Loss: 1.4618798196315765, Final Batch Loss: 0.4092411398887634\n",
      "Epoch 375, Loss: 1.4047791659832, Final Batch Loss: 0.3186851441860199\n",
      "Epoch 376, Loss: 1.5204132795333862, Final Batch Loss: 0.4366217255592346\n",
      "Epoch 377, Loss: 1.5040916800498962, Final Batch Loss: 0.40388837456703186\n",
      "Epoch 378, Loss: 1.4175553917884827, Final Batch Loss: 0.32474485039711\n",
      "Epoch 379, Loss: 1.5006076991558075, Final Batch Loss: 0.4495305120944977\n",
      "Epoch 380, Loss: 1.4294854402542114, Final Batch Loss: 0.3756515681743622\n",
      "Epoch 381, Loss: 1.3651015162467957, Final Batch Loss: 0.2877490818500519\n",
      "Epoch 382, Loss: 1.4184949100017548, Final Batch Loss: 0.3176053762435913\n",
      "Epoch 383, Loss: 1.3982477188110352, Final Batch Loss: 0.40767142176628113\n",
      "Epoch 384, Loss: 1.3524698466062546, Final Batch Loss: 0.2499707192182541\n",
      "Epoch 385, Loss: 1.5144572854042053, Final Batch Loss: 0.5151515603065491\n",
      "Epoch 386, Loss: 1.5670835673809052, Final Batch Loss: 0.4794329106807709\n",
      "Epoch 387, Loss: 1.507690966129303, Final Batch Loss: 0.4500262141227722\n",
      "Epoch 388, Loss: 1.5251816511154175, Final Batch Loss: 0.4394291043281555\n",
      "Epoch 389, Loss: 1.41251939535141, Final Batch Loss: 0.3648530840873718\n",
      "Epoch 390, Loss: 1.4664418697357178, Final Batch Loss: 0.3695099353790283\n",
      "Epoch 391, Loss: 1.4479502439498901, Final Batch Loss: 0.39001670479774475\n",
      "Epoch 392, Loss: 1.5187720358371735, Final Batch Loss: 0.4289696514606476\n",
      "Epoch 393, Loss: 1.426495909690857, Final Batch Loss: 0.3992973566055298\n",
      "Epoch 394, Loss: 1.4194623231887817, Final Batch Loss: 0.3659875988960266\n",
      "Epoch 395, Loss: 1.4305508434772491, Final Batch Loss: 0.35934239625930786\n",
      "Epoch 396, Loss: 1.3356660604476929, Final Batch Loss: 0.2851647436618805\n",
      "Epoch 397, Loss: 1.497608482837677, Final Batch Loss: 0.4798949062824249\n",
      "Epoch 398, Loss: 1.4305226802825928, Final Batch Loss: 0.33008602261543274\n",
      "Epoch 399, Loss: 1.4719419181346893, Final Batch Loss: 0.4443720579147339\n",
      "Epoch 400, Loss: 1.470784455537796, Final Batch Loss: 0.3318291902542114\n",
      "Epoch 401, Loss: 1.4308179020881653, Final Batch Loss: 0.3664514422416687\n",
      "Epoch 402, Loss: 1.4523823261260986, Final Batch Loss: 0.4098930358886719\n",
      "Epoch 403, Loss: 1.444567084312439, Final Batch Loss: 0.40576061606407166\n",
      "Epoch 404, Loss: 1.4284934103488922, Final Batch Loss: 0.3718581199645996\n",
      "Epoch 405, Loss: 1.4988960027694702, Final Batch Loss: 0.41570499539375305\n",
      "Epoch 406, Loss: 1.4219205677509308, Final Batch Loss: 0.3275183141231537\n",
      "Epoch 407, Loss: 1.4540022313594818, Final Batch Loss: 0.3741036355495453\n",
      "Epoch 408, Loss: 1.3584255576133728, Final Batch Loss: 0.2776896059513092\n",
      "Epoch 409, Loss: 1.373254656791687, Final Batch Loss: 0.32693806290626526\n",
      "Epoch 410, Loss: 1.3555794954299927, Final Batch Loss: 0.3154839873313904\n",
      "Epoch 411, Loss: 1.3758643567562103, Final Batch Loss: 0.312190443277359\n",
      "Epoch 412, Loss: 1.3815124332904816, Final Batch Loss: 0.3296750783920288\n",
      "Epoch 413, Loss: 1.400159329175949, Final Batch Loss: 0.4220257103443146\n",
      "Epoch 414, Loss: 1.4413901567459106, Final Batch Loss: 0.4012371599674225\n",
      "Epoch 415, Loss: 1.3269311785697937, Final Batch Loss: 0.2744311988353729\n",
      "Epoch 416, Loss: 1.443308413028717, Final Batch Loss: 0.4073145389556885\n",
      "Epoch 417, Loss: 1.3949515521526337, Final Batch Loss: 0.49115678668022156\n",
      "Epoch 418, Loss: 1.4183533489704132, Final Batch Loss: 0.40355443954467773\n",
      "Epoch 419, Loss: 1.3299601674079895, Final Batch Loss: 0.30795252323150635\n",
      "Epoch 420, Loss: 1.4193556010723114, Final Batch Loss: 0.37401220202445984\n",
      "Epoch 421, Loss: 1.3417854309082031, Final Batch Loss: 0.27549415826797485\n",
      "Epoch 422, Loss: 1.4941290616989136, Final Batch Loss: 0.4624366760253906\n",
      "Epoch 423, Loss: 1.4291059970855713, Final Batch Loss: 0.43854936957359314\n",
      "Epoch 424, Loss: 1.363136887550354, Final Batch Loss: 0.35585227608680725\n",
      "Epoch 425, Loss: 1.3883355259895325, Final Batch Loss: 0.3074939250946045\n",
      "Epoch 426, Loss: 1.2973629534244537, Final Batch Loss: 0.28888487815856934\n",
      "Epoch 427, Loss: 1.3597740232944489, Final Batch Loss: 0.3019111454486847\n",
      "Epoch 428, Loss: 1.430318146944046, Final Batch Loss: 0.37201836705207825\n",
      "Epoch 429, Loss: 1.4412339627742767, Final Batch Loss: 0.44790342450141907\n",
      "Epoch 430, Loss: 1.4474749863147736, Final Batch Loss: 0.41197583079338074\n",
      "Epoch 431, Loss: 1.3711209297180176, Final Batch Loss: 0.36170193552970886\n",
      "Epoch 432, Loss: 1.3366627395153046, Final Batch Loss: 0.309815913438797\n",
      "Epoch 433, Loss: 1.3930619657039642, Final Batch Loss: 0.28842926025390625\n",
      "Epoch 434, Loss: 1.3964757025241852, Final Batch Loss: 0.39987504482269287\n",
      "Epoch 435, Loss: 1.413644015789032, Final Batch Loss: 0.3658575117588043\n",
      "Epoch 436, Loss: 1.4105661511421204, Final Batch Loss: 0.4074777364730835\n",
      "Epoch 437, Loss: 1.3029891103506088, Final Batch Loss: 0.23843418061733246\n",
      "Epoch 438, Loss: 1.3917202949523926, Final Batch Loss: 0.3811587691307068\n",
      "Epoch 439, Loss: 1.4155671000480652, Final Batch Loss: 0.35318735241889954\n",
      "Epoch 440, Loss: 1.438200205564499, Final Batch Loss: 0.40134066343307495\n",
      "Epoch 441, Loss: 1.417257159948349, Final Batch Loss: 0.3916579484939575\n",
      "Epoch 442, Loss: 1.2491230368614197, Final Batch Loss: 0.23915740847587585\n",
      "Epoch 443, Loss: 1.4584738612174988, Final Batch Loss: 0.4416041672229767\n",
      "Epoch 444, Loss: 1.4225568175315857, Final Batch Loss: 0.39582937955856323\n",
      "Epoch 445, Loss: 1.4803152978420258, Final Batch Loss: 0.4560917317867279\n",
      "Epoch 446, Loss: 1.3525211811065674, Final Batch Loss: 0.2985624670982361\n",
      "Epoch 447, Loss: 1.36907497048378, Final Batch Loss: 0.3231508135795593\n",
      "Epoch 448, Loss: 1.4342519640922546, Final Batch Loss: 0.4273037612438202\n",
      "Epoch 449, Loss: 1.3391337394714355, Final Batch Loss: 0.3288611173629761\n",
      "Epoch 450, Loss: 1.356038510799408, Final Batch Loss: 0.3654111623764038\n",
      "Epoch 451, Loss: 1.404837280511856, Final Batch Loss: 0.410919189453125\n",
      "Epoch 452, Loss: 1.417915552854538, Final Batch Loss: 0.38673463463783264\n",
      "Epoch 453, Loss: 1.3690061569213867, Final Batch Loss: 0.3195480704307556\n",
      "Epoch 454, Loss: 1.437683254480362, Final Batch Loss: 0.4148186445236206\n",
      "Epoch 455, Loss: 1.320594847202301, Final Batch Loss: 0.26156097650527954\n",
      "Epoch 456, Loss: 1.4063247740268707, Final Batch Loss: 0.3530963361263275\n",
      "Epoch 457, Loss: 1.3649676442146301, Final Batch Loss: 0.32446742057800293\n",
      "Epoch 458, Loss: 1.435040682554245, Final Batch Loss: 0.40132561326026917\n",
      "Epoch 459, Loss: 1.3015196621418, Final Batch Loss: 0.2756487727165222\n",
      "Epoch 460, Loss: 1.4325346648693085, Final Batch Loss: 0.3793688714504242\n",
      "Epoch 461, Loss: 1.3737896382808685, Final Batch Loss: 0.38195306062698364\n",
      "Epoch 462, Loss: 1.4356056153774261, Final Batch Loss: 0.4658704102039337\n",
      "Epoch 463, Loss: 1.4228910207748413, Final Batch Loss: 0.43640175461769104\n",
      "Epoch 464, Loss: 1.337356060743332, Final Batch Loss: 0.3021196126937866\n",
      "Epoch 465, Loss: 1.4292010068893433, Final Batch Loss: 0.44462305307388306\n",
      "Epoch 466, Loss: 1.3584057092666626, Final Batch Loss: 0.3530089557170868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 467, Loss: 1.361792117357254, Final Batch Loss: 0.2719392478466034\n",
      "Epoch 468, Loss: 1.390498399734497, Final Batch Loss: 0.3855532109737396\n",
      "Epoch 469, Loss: 1.3763847351074219, Final Batch Loss: 0.36725956201553345\n",
      "Epoch 470, Loss: 1.2620291709899902, Final Batch Loss: 0.30548685789108276\n",
      "Epoch 471, Loss: 1.3204111158847809, Final Batch Loss: 0.34674614667892456\n",
      "Epoch 472, Loss: 1.3342255651950836, Final Batch Loss: 0.3355320692062378\n",
      "Epoch 473, Loss: 1.3244010508060455, Final Batch Loss: 0.29319071769714355\n",
      "Epoch 474, Loss: 1.310328096151352, Final Batch Loss: 0.2525748014450073\n",
      "Epoch 475, Loss: 1.315259724855423, Final Batch Loss: 0.3298474848270416\n",
      "Epoch 476, Loss: 1.408355563879013, Final Batch Loss: 0.3991222083568573\n",
      "Epoch 477, Loss: 1.462614506483078, Final Batch Loss: 0.3864757716655731\n",
      "Epoch 478, Loss: 1.4061349630355835, Final Batch Loss: 0.42386534810066223\n",
      "Epoch 479, Loss: 1.3082861304283142, Final Batch Loss: 0.3263297975063324\n",
      "Epoch 480, Loss: 1.3686690926551819, Final Batch Loss: 0.3652826249599457\n",
      "Epoch 481, Loss: 1.4074216783046722, Final Batch Loss: 0.42900264263153076\n",
      "Epoch 482, Loss: 1.3646421134471893, Final Batch Loss: 0.3251667618751526\n",
      "Epoch 483, Loss: 1.2742330431938171, Final Batch Loss: 0.2588963210582733\n",
      "Epoch 484, Loss: 1.3454456627368927, Final Batch Loss: 0.32741814851760864\n",
      "Epoch 485, Loss: 1.2900465726852417, Final Batch Loss: 0.26089876890182495\n",
      "Epoch 486, Loss: 1.455182671546936, Final Batch Loss: 0.4896731674671173\n",
      "Epoch 487, Loss: 1.3338188827037811, Final Batch Loss: 0.34213969111442566\n",
      "Epoch 488, Loss: 1.3578076362609863, Final Batch Loss: 0.3605456054210663\n",
      "Epoch 489, Loss: 1.4087381064891815, Final Batch Loss: 0.3455670177936554\n",
      "Epoch 490, Loss: 1.3688269555568695, Final Batch Loss: 0.36052682995796204\n",
      "Epoch 491, Loss: 1.2885468304157257, Final Batch Loss: 0.31344878673553467\n",
      "Epoch 492, Loss: 1.4211194515228271, Final Batch Loss: 0.40178146958351135\n",
      "Epoch 493, Loss: 1.3459595441818237, Final Batch Loss: 0.3543045222759247\n",
      "Epoch 494, Loss: 1.3796427249908447, Final Batch Loss: 0.37390005588531494\n",
      "Epoch 495, Loss: 1.4121174812316895, Final Batch Loss: 0.41767531633377075\n",
      "Epoch 496, Loss: 1.3259894251823425, Final Batch Loss: 0.3148244321346283\n",
      "Epoch 497, Loss: 1.4212455749511719, Final Batch Loss: 0.33877769112586975\n",
      "Epoch 498, Loss: 1.3278561532497406, Final Batch Loss: 0.2811290919780731\n",
      "Epoch 499, Loss: 1.3433338105678558, Final Batch Loss: 0.27160272002220154\n",
      "Epoch 500, Loss: 1.324611783027649, Final Batch Loss: 0.2935296297073364\n",
      "Epoch 501, Loss: 1.4072137475013733, Final Batch Loss: 0.3691299259662628\n",
      "Epoch 502, Loss: 1.3365303575992584, Final Batch Loss: 0.33752551674842834\n",
      "Epoch 503, Loss: 1.3307119607925415, Final Batch Loss: 0.37040063738822937\n",
      "Epoch 504, Loss: 1.3555451929569244, Final Batch Loss: 0.3102911114692688\n",
      "Epoch 505, Loss: 1.41398024559021, Final Batch Loss: 0.40241673588752747\n",
      "Epoch 506, Loss: 1.383057951927185, Final Batch Loss: 0.4189121425151825\n",
      "Epoch 507, Loss: 1.3464053869247437, Final Batch Loss: 0.36034858226776123\n",
      "Epoch 508, Loss: 1.2647476494312286, Final Batch Loss: 0.27481111884117126\n",
      "Epoch 509, Loss: 1.3810093700885773, Final Batch Loss: 0.3714850842952728\n",
      "Epoch 510, Loss: 1.3728235960006714, Final Batch Loss: 0.37068167328834534\n",
      "Epoch 511, Loss: 1.396154761314392, Final Batch Loss: 0.30810028314590454\n",
      "Epoch 512, Loss: 1.3322193324565887, Final Batch Loss: 0.2932584881782532\n",
      "Epoch 513, Loss: 1.3906205296516418, Final Batch Loss: 0.38783133029937744\n",
      "Epoch 514, Loss: 1.203035056591034, Final Batch Loss: 0.27395114302635193\n",
      "Epoch 515, Loss: 1.265010505914688, Final Batch Loss: 0.30923813581466675\n",
      "Epoch 516, Loss: 1.3038785755634308, Final Batch Loss: 0.30165883898735046\n",
      "Epoch 517, Loss: 1.3094496130943298, Final Batch Loss: 0.3317248523235321\n",
      "Epoch 518, Loss: 1.2300658524036407, Final Batch Loss: 0.22861716151237488\n",
      "Epoch 519, Loss: 1.310114324092865, Final Batch Loss: 0.3231627643108368\n",
      "Epoch 520, Loss: 1.2646263986825943, Final Batch Loss: 0.23749695718288422\n",
      "Epoch 521, Loss: 1.2588246166706085, Final Batch Loss: 0.29125046730041504\n",
      "Epoch 522, Loss: 1.2661288380622864, Final Batch Loss: 0.30465835332870483\n",
      "Epoch 523, Loss: 1.299541562795639, Final Batch Loss: 0.27237972617149353\n",
      "Epoch 524, Loss: 1.449378103017807, Final Batch Loss: 0.39984041452407837\n",
      "Epoch 525, Loss: 1.2351260483264923, Final Batch Loss: 0.26706480979919434\n",
      "Epoch 526, Loss: 1.3258423507213593, Final Batch Loss: 0.34720829129219055\n",
      "Epoch 527, Loss: 1.3505996465682983, Final Batch Loss: 0.3632022440433502\n",
      "Epoch 528, Loss: 1.3141826689243317, Final Batch Loss: 0.30244094133377075\n",
      "Epoch 529, Loss: 1.4152163863182068, Final Batch Loss: 0.3807160556316376\n",
      "Epoch 530, Loss: 1.320751428604126, Final Batch Loss: 0.3513387441635132\n",
      "Epoch 531, Loss: 1.3861033618450165, Final Batch Loss: 0.4372650682926178\n",
      "Epoch 532, Loss: 1.4769979119300842, Final Batch Loss: 0.49052128195762634\n",
      "Epoch 533, Loss: 1.384826421737671, Final Batch Loss: 0.3827745318412781\n",
      "Epoch 534, Loss: 1.3423533737659454, Final Batch Loss: 0.2735716998577118\n",
      "Epoch 535, Loss: 1.2993757724761963, Final Batch Loss: 0.26957303285598755\n",
      "Epoch 536, Loss: 1.3279524147510529, Final Batch Loss: 0.2846592962741852\n",
      "Epoch 537, Loss: 1.4230572283267975, Final Batch Loss: 0.3624137341976166\n",
      "Epoch 538, Loss: 1.3998158872127533, Final Batch Loss: 0.3859310746192932\n",
      "Epoch 539, Loss: 1.4169972240924835, Final Batch Loss: 0.48299047350883484\n",
      "Epoch 540, Loss: 1.33712238073349, Final Batch Loss: 0.36249810457229614\n",
      "Epoch 541, Loss: 1.3458235561847687, Final Batch Loss: 0.31977227330207825\n",
      "Epoch 542, Loss: 1.2764919698238373, Final Batch Loss: 0.261682391166687\n",
      "Epoch 543, Loss: 1.3883162438869476, Final Batch Loss: 0.4006713926792145\n",
      "Epoch 544, Loss: 1.3598605692386627, Final Batch Loss: 0.3271171748638153\n",
      "Epoch 545, Loss: 1.3077100217342377, Final Batch Loss: 0.2773606777191162\n",
      "Epoch 546, Loss: 1.3752323389053345, Final Batch Loss: 0.41448044776916504\n",
      "Epoch 547, Loss: 1.311150461435318, Final Batch Loss: 0.28521451354026794\n",
      "Epoch 548, Loss: 1.3797062635421753, Final Batch Loss: 0.30837827920913696\n",
      "Epoch 549, Loss: 1.3492050170898438, Final Batch Loss: 0.3648429214954376\n",
      "Epoch 550, Loss: 1.363410234451294, Final Batch Loss: 0.3606538474559784\n",
      "Epoch 551, Loss: 1.3416133224964142, Final Batch Loss: 0.37231019139289856\n",
      "Epoch 552, Loss: 1.3424464166164398, Final Batch Loss: 0.37136009335517883\n",
      "Epoch 553, Loss: 1.2894073128700256, Final Batch Loss: 0.3237355947494507\n",
      "Epoch 554, Loss: 1.288773775100708, Final Batch Loss: 0.305700421333313\n",
      "Epoch 555, Loss: 1.3527248203754425, Final Batch Loss: 0.4043252170085907\n",
      "Epoch 556, Loss: 1.3061997890472412, Final Batch Loss: 0.3248223066329956\n",
      "Epoch 557, Loss: 1.3835269808769226, Final Batch Loss: 0.398541122674942\n",
      "Epoch 558, Loss: 1.3789858222007751, Final Batch Loss: 0.3924086391925812\n",
      "Epoch 559, Loss: 1.2240433990955353, Final Batch Loss: 0.24309298396110535\n",
      "Epoch 560, Loss: 1.3978367149829865, Final Batch Loss: 0.43372878432273865\n",
      "Epoch 561, Loss: 1.3025093376636505, Final Batch Loss: 0.3689144551753998\n",
      "Epoch 562, Loss: 1.2926529347896576, Final Batch Loss: 0.29617032408714294\n",
      "Epoch 563, Loss: 1.3151778280735016, Final Batch Loss: 0.35864266753196716\n",
      "Epoch 564, Loss: 1.3173834383487701, Final Batch Loss: 0.31061387062072754\n",
      "Epoch 565, Loss: 1.276949018239975, Final Batch Loss: 0.27633580565452576\n",
      "Epoch 566, Loss: 1.2586159408092499, Final Batch Loss: 0.29558393359184265\n",
      "Epoch 567, Loss: 1.3087647557258606, Final Batch Loss: 0.3750850558280945\n",
      "Epoch 568, Loss: 1.2520845830440521, Final Batch Loss: 0.31096741557121277\n",
      "Epoch 569, Loss: 1.3347980380058289, Final Batch Loss: 0.3860657513141632\n",
      "Epoch 570, Loss: 1.3181284070014954, Final Batch Loss: 0.3602208197116852\n",
      "Epoch 571, Loss: 1.3040513396263123, Final Batch Loss: 0.33895033597946167\n",
      "Epoch 572, Loss: 1.2293434143066406, Final Batch Loss: 0.22910180687904358\n",
      "Epoch 573, Loss: 1.331129938364029, Final Batch Loss: 0.33004602789878845\n",
      "Epoch 574, Loss: 1.255051702260971, Final Batch Loss: 0.2704014778137207\n",
      "Epoch 575, Loss: 1.2837978303432465, Final Batch Loss: 0.36391451954841614\n",
      "Epoch 576, Loss: 1.285754919052124, Final Batch Loss: 0.35225921869277954\n",
      "Epoch 577, Loss: 1.2894845604896545, Final Batch Loss: 0.2889561355113983\n",
      "Epoch 578, Loss: 1.2480226457118988, Final Batch Loss: 0.34066563844680786\n",
      "Epoch 579, Loss: 1.2900607287883759, Final Batch Loss: 0.33455100655555725\n",
      "Epoch 580, Loss: 1.3335239887237549, Final Batch Loss: 0.33822640776634216\n",
      "Epoch 581, Loss: 1.2397228479385376, Final Batch Loss: 0.3416266441345215\n",
      "Epoch 582, Loss: 1.2786304652690887, Final Batch Loss: 0.27250543236732483\n",
      "Epoch 583, Loss: 1.354815512895584, Final Batch Loss: 0.3359421491622925\n",
      "Epoch 584, Loss: 1.3282038569450378, Final Batch Loss: 0.3564397394657135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 585, Loss: 1.3665966987609863, Final Batch Loss: 0.38084688782691956\n",
      "Epoch 586, Loss: 1.3038719296455383, Final Batch Loss: 0.34036901593208313\n",
      "Epoch 587, Loss: 1.2483476549386978, Final Batch Loss: 0.23632179200649261\n",
      "Epoch 588, Loss: 1.2273258417844772, Final Batch Loss: 0.24677814543247223\n",
      "Epoch 589, Loss: 1.2878269255161285, Final Batch Loss: 0.306086927652359\n",
      "Epoch 590, Loss: 1.279260128736496, Final Batch Loss: 0.3037359118461609\n",
      "Epoch 591, Loss: 1.3630820214748383, Final Batch Loss: 0.39643213152885437\n",
      "Epoch 592, Loss: 1.3781876862049103, Final Batch Loss: 0.407463401556015\n",
      "Epoch 593, Loss: 1.2727429270744324, Final Batch Loss: 0.3663250505924225\n",
      "Epoch 594, Loss: 1.2431473433971405, Final Batch Loss: 0.27526456117630005\n",
      "Epoch 595, Loss: 1.2010610550642014, Final Batch Loss: 0.21752415597438812\n",
      "Epoch 596, Loss: 1.2659139335155487, Final Batch Loss: 0.3232969045639038\n",
      "Epoch 597, Loss: 1.3268458545207977, Final Batch Loss: 0.37648120522499084\n",
      "Epoch 598, Loss: 1.2969807386398315, Final Batch Loss: 0.3080410659313202\n",
      "Epoch 599, Loss: 1.1781545579433441, Final Batch Loss: 0.2427801787853241\n",
      "Epoch 600, Loss: 1.2562343776226044, Final Batch Loss: 0.3160097599029541\n",
      "Epoch 601, Loss: 1.2597561478614807, Final Batch Loss: 0.33044740557670593\n",
      "Epoch 602, Loss: 1.3139342963695526, Final Batch Loss: 0.3406555950641632\n",
      "Epoch 603, Loss: 1.200394630432129, Final Batch Loss: 0.25092998147010803\n",
      "Epoch 604, Loss: 1.2724821269512177, Final Batch Loss: 0.27994000911712646\n",
      "Epoch 605, Loss: 1.2857779562473297, Final Batch Loss: 0.3689460754394531\n",
      "Epoch 606, Loss: 1.3873894810676575, Final Batch Loss: 0.3798011839389801\n",
      "Epoch 607, Loss: 1.2181255519390106, Final Batch Loss: 0.26920318603515625\n",
      "Epoch 608, Loss: 1.3352197706699371, Final Batch Loss: 0.3659074008464813\n",
      "Epoch 609, Loss: 1.2241188436746597, Final Batch Loss: 0.2114843875169754\n",
      "Epoch 610, Loss: 1.2984366118907928, Final Batch Loss: 0.3161911964416504\n",
      "Epoch 611, Loss: 1.3318794667720795, Final Batch Loss: 0.3582289218902588\n",
      "Epoch 612, Loss: 1.2657764256000519, Final Batch Loss: 0.2830880880355835\n",
      "Epoch 613, Loss: 1.3145095705986023, Final Batch Loss: 0.34070074558258057\n",
      "Epoch 614, Loss: 1.2797677218914032, Final Batch Loss: 0.2903403639793396\n",
      "Epoch 615, Loss: 1.332890808582306, Final Batch Loss: 0.32206910848617554\n",
      "Epoch 616, Loss: 1.2554293125867844, Final Batch Loss: 0.23970113694667816\n",
      "Epoch 617, Loss: 1.197941541671753, Final Batch Loss: 0.2508874237537384\n",
      "Epoch 618, Loss: 1.25903782248497, Final Batch Loss: 0.28870469331741333\n",
      "Epoch 619, Loss: 1.3192734718322754, Final Batch Loss: 0.33299022912979126\n",
      "Epoch 620, Loss: 1.280927985906601, Final Batch Loss: 0.3446823060512543\n",
      "Epoch 621, Loss: 1.3104425370693207, Final Batch Loss: 0.41112208366394043\n",
      "Epoch 622, Loss: 1.224606603384018, Final Batch Loss: 0.3130161762237549\n",
      "Epoch 623, Loss: 1.3968070447444916, Final Batch Loss: 0.4220536947250366\n",
      "Epoch 624, Loss: 1.331634521484375, Final Batch Loss: 0.41992515325546265\n",
      "Epoch 625, Loss: 1.2563479244709015, Final Batch Loss: 0.28581345081329346\n",
      "Epoch 626, Loss: 1.3012619614601135, Final Batch Loss: 0.3187341094017029\n",
      "Epoch 627, Loss: 1.219795897603035, Final Batch Loss: 0.297322541475296\n",
      "Epoch 628, Loss: 1.2635523080825806, Final Batch Loss: 0.3272641897201538\n",
      "Epoch 629, Loss: 1.2612498104572296, Final Batch Loss: 0.318126916885376\n",
      "Epoch 630, Loss: 1.223481297492981, Final Batch Loss: 0.30233848094940186\n",
      "Epoch 631, Loss: 1.2387498915195465, Final Batch Loss: 0.2962745130062103\n",
      "Epoch 632, Loss: 1.306752234697342, Final Batch Loss: 0.35910773277282715\n",
      "Epoch 633, Loss: 1.2647659182548523, Final Batch Loss: 0.3486790657043457\n",
      "Epoch 634, Loss: 1.3094481825828552, Final Batch Loss: 0.3315076231956482\n",
      "Epoch 635, Loss: 1.2797657549381256, Final Batch Loss: 0.3171083927154541\n",
      "Epoch 636, Loss: 1.2578135430812836, Final Batch Loss: 0.330812007188797\n",
      "Epoch 637, Loss: 1.2440154254436493, Final Batch Loss: 0.2982388138771057\n",
      "Epoch 638, Loss: 1.2804220616817474, Final Batch Loss: 0.2927306294441223\n",
      "Epoch 639, Loss: 1.3038249909877777, Final Batch Loss: 0.3631364107131958\n",
      "Epoch 640, Loss: 1.3353260159492493, Final Batch Loss: 0.3609680235385895\n",
      "Epoch 641, Loss: 1.2309829890727997, Final Batch Loss: 0.26098665595054626\n",
      "Epoch 642, Loss: 1.3000233471393585, Final Batch Loss: 0.39118897914886475\n",
      "Epoch 643, Loss: 1.2281956374645233, Final Batch Loss: 0.30843275785446167\n",
      "Epoch 644, Loss: 1.3656745553016663, Final Batch Loss: 0.39997419714927673\n",
      "Epoch 645, Loss: 1.2521607875823975, Final Batch Loss: 0.27979227900505066\n",
      "Epoch 646, Loss: 1.2504195272922516, Final Batch Loss: 0.30690890550613403\n",
      "Epoch 647, Loss: 1.1941573321819305, Final Batch Loss: 0.24616840481758118\n",
      "Epoch 648, Loss: 1.2775656580924988, Final Batch Loss: 0.2678711712360382\n",
      "Epoch 649, Loss: 1.2348416447639465, Final Batch Loss: 0.3204156756401062\n",
      "Epoch 650, Loss: 1.1949258148670197, Final Batch Loss: 0.2742101550102234\n",
      "Epoch 651, Loss: 1.3534152805805206, Final Batch Loss: 0.36942848563194275\n",
      "Epoch 652, Loss: 1.3017644882202148, Final Batch Loss: 0.33714982867240906\n",
      "Epoch 653, Loss: 1.3340979218482971, Final Batch Loss: 0.4263353943824768\n",
      "Epoch 654, Loss: 1.3201000392436981, Final Batch Loss: 0.3916100859642029\n",
      "Epoch 655, Loss: 1.3196178674697876, Final Batch Loss: 0.3421157896518707\n",
      "Epoch 656, Loss: 1.2139408886432648, Final Batch Loss: 0.31325477361679077\n",
      "Epoch 657, Loss: 1.3146217465400696, Final Batch Loss: 0.405760794878006\n",
      "Epoch 658, Loss: 1.2738892436027527, Final Batch Loss: 0.3195434808731079\n",
      "Epoch 659, Loss: 1.2635679841041565, Final Batch Loss: 0.26122379302978516\n",
      "Epoch 660, Loss: 1.359107255935669, Final Batch Loss: 0.3777216374874115\n",
      "Epoch 661, Loss: 1.2578822672367096, Final Batch Loss: 0.3379087746143341\n",
      "Epoch 662, Loss: 1.262081652879715, Final Batch Loss: 0.2693791091442108\n",
      "Epoch 663, Loss: 1.2691292464733124, Final Batch Loss: 0.29567572474479675\n",
      "Epoch 664, Loss: 1.2702238261699677, Final Batch Loss: 0.3185788094997406\n",
      "Epoch 665, Loss: 1.246577888727188, Final Batch Loss: 0.28821495175361633\n",
      "Epoch 666, Loss: 1.3006078898906708, Final Batch Loss: 0.3830818235874176\n",
      "Epoch 667, Loss: 1.2801752388477325, Final Batch Loss: 0.34199702739715576\n",
      "Epoch 668, Loss: 1.264706701040268, Final Batch Loss: 0.28992220759391785\n",
      "Epoch 669, Loss: 1.3093897998332977, Final Batch Loss: 0.3824231028556824\n",
      "Epoch 670, Loss: 1.2535033822059631, Final Batch Loss: 0.3055495321750641\n",
      "Epoch 671, Loss: 1.2758191525936127, Final Batch Loss: 0.3387902081012726\n",
      "Epoch 672, Loss: 1.2412813901901245, Final Batch Loss: 0.34162601828575134\n",
      "Epoch 673, Loss: 1.2063156068325043, Final Batch Loss: 0.29977741837501526\n",
      "Epoch 674, Loss: 1.2593582272529602, Final Batch Loss: 0.31765586137771606\n",
      "Epoch 675, Loss: 1.2549129128456116, Final Batch Loss: 0.3364587426185608\n",
      "Epoch 676, Loss: 1.318120539188385, Final Batch Loss: 0.3670831322669983\n",
      "Epoch 677, Loss: 1.1300530582666397, Final Batch Loss: 0.24626524746418\n",
      "Epoch 678, Loss: 1.2562868297100067, Final Batch Loss: 0.33802637457847595\n",
      "Epoch 679, Loss: 1.1819771528244019, Final Batch Loss: 0.22676187753677368\n",
      "Epoch 680, Loss: 1.2547171413898468, Final Batch Loss: 0.3410075008869171\n",
      "Epoch 681, Loss: 1.1755969822406769, Final Batch Loss: 0.2546459436416626\n",
      "Epoch 682, Loss: 1.2248165905475616, Final Batch Loss: 0.31901121139526367\n",
      "Epoch 683, Loss: 1.1982117295265198, Final Batch Loss: 0.28582802414894104\n",
      "Epoch 684, Loss: 1.1993362754583359, Final Batch Loss: 0.29778340458869934\n",
      "Epoch 685, Loss: 1.2496387362480164, Final Batch Loss: 0.34176844358444214\n",
      "Epoch 686, Loss: 1.1550735384225845, Final Batch Loss: 0.230581596493721\n",
      "Epoch 687, Loss: 1.18857803940773, Final Batch Loss: 0.29151651263237\n",
      "Epoch 688, Loss: 1.2615737617015839, Final Batch Loss: 0.31970012187957764\n",
      "Epoch 689, Loss: 1.2925355732440948, Final Batch Loss: 0.2857220768928528\n",
      "Epoch 690, Loss: 1.2122727632522583, Final Batch Loss: 0.28349795937538147\n",
      "Epoch 691, Loss: 1.1924632638692856, Final Batch Loss: 0.24447564780712128\n",
      "Epoch 692, Loss: 1.2236877083778381, Final Batch Loss: 0.3002217710018158\n",
      "Epoch 693, Loss: 1.2191314697265625, Final Batch Loss: 0.2881494164466858\n",
      "Epoch 694, Loss: 1.2066477239131927, Final Batch Loss: 0.2577374279499054\n",
      "Epoch 695, Loss: 1.2308304011821747, Final Batch Loss: 0.3562943935394287\n",
      "Epoch 696, Loss: 1.2514914274215698, Final Batch Loss: 0.30752599239349365\n",
      "Epoch 697, Loss: 1.2328439950942993, Final Batch Loss: 0.3134000897407532\n",
      "Epoch 698, Loss: 1.373853087425232, Final Batch Loss: 0.44314730167388916\n",
      "Epoch 699, Loss: 1.2316382825374603, Final Batch Loss: 0.28693950176239014\n",
      "Epoch 700, Loss: 1.2141750752925873, Final Batch Loss: 0.2608394920825958\n",
      "Epoch 701, Loss: 1.307049185037613, Final Batch Loss: 0.44673576951026917\n",
      "Epoch 702, Loss: 1.1919311881065369, Final Batch Loss: 0.32546064257621765\n",
      "Epoch 703, Loss: 1.199653536081314, Final Batch Loss: 0.23652470111846924\n",
      "Epoch 704, Loss: 1.2332242131233215, Final Batch Loss: 0.3004157245159149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 705, Loss: 1.3014658391475677, Final Batch Loss: 0.2662307620048523\n",
      "Epoch 706, Loss: 1.2636765837669373, Final Batch Loss: 0.380465567111969\n",
      "Epoch 707, Loss: 1.2291052341461182, Final Batch Loss: 0.3090576231479645\n",
      "Epoch 708, Loss: 1.21495720744133, Final Batch Loss: 0.2880304455757141\n",
      "Epoch 709, Loss: 1.2516563832759857, Final Batch Loss: 0.3196824789047241\n",
      "Epoch 710, Loss: 1.3120480477809906, Final Batch Loss: 0.41798409819602966\n",
      "Epoch 711, Loss: 1.2529789805412292, Final Batch Loss: 0.3241836726665497\n",
      "Epoch 712, Loss: 1.2717039287090302, Final Batch Loss: 0.3649281859397888\n",
      "Epoch 713, Loss: 1.1893104165792465, Final Batch Loss: 0.240646630525589\n",
      "Epoch 714, Loss: 1.190853625535965, Final Batch Loss: 0.2982018291950226\n",
      "Epoch 715, Loss: 1.1639185845851898, Final Batch Loss: 0.29400700330734253\n",
      "Epoch 716, Loss: 1.145133376121521, Final Batch Loss: 0.275720477104187\n",
      "Epoch 717, Loss: 1.2446527183055878, Final Batch Loss: 0.3873085081577301\n",
      "Epoch 718, Loss: 1.219460368156433, Final Batch Loss: 0.2600010335445404\n",
      "Epoch 719, Loss: 1.2704533636569977, Final Batch Loss: 0.35350489616394043\n",
      "Epoch 720, Loss: 1.2541656792163849, Final Batch Loss: 0.3523114025592804\n",
      "Epoch 721, Loss: 1.2623185813426971, Final Batch Loss: 0.3204708397388458\n",
      "Epoch 722, Loss: 1.20706906914711, Final Batch Loss: 0.3144337832927704\n",
      "Epoch 723, Loss: 1.2163018584251404, Final Batch Loss: 0.2920408546924591\n",
      "Epoch 724, Loss: 1.2312549948692322, Final Batch Loss: 0.3136935532093048\n",
      "Epoch 725, Loss: 1.251173585653305, Final Batch Loss: 0.28673917055130005\n",
      "Epoch 726, Loss: 1.178880214691162, Final Batch Loss: 0.2841346263885498\n",
      "Epoch 727, Loss: 1.1978379487991333, Final Batch Loss: 0.3557712435722351\n",
      "Epoch 728, Loss: 1.1837249100208282, Final Batch Loss: 0.33702564239501953\n",
      "Epoch 729, Loss: 1.1846685707569122, Final Batch Loss: 0.28925544023513794\n",
      "Epoch 730, Loss: 1.1664916276931763, Final Batch Loss: 0.3268698453903198\n",
      "Epoch 731, Loss: 1.2554883658885956, Final Batch Loss: 0.35358965396881104\n",
      "Epoch 732, Loss: 1.2155500054359436, Final Batch Loss: 0.27872568368911743\n",
      "Epoch 733, Loss: 1.1903859972953796, Final Batch Loss: 0.2316913604736328\n",
      "Epoch 734, Loss: 1.2177934646606445, Final Batch Loss: 0.28563451766967773\n",
      "Epoch 735, Loss: 1.1753624379634857, Final Batch Loss: 0.2198173999786377\n",
      "Epoch 736, Loss: 1.2663092613220215, Final Batch Loss: 0.32989072799682617\n",
      "Epoch 737, Loss: 1.2650489807128906, Final Batch Loss: 0.3277767598628998\n",
      "Epoch 738, Loss: 1.1535959988832474, Final Batch Loss: 0.24965189397335052\n",
      "Epoch 739, Loss: 1.154616266489029, Final Batch Loss: 0.20615649223327637\n",
      "Epoch 740, Loss: 1.2432532012462616, Final Batch Loss: 0.4237869083881378\n",
      "Epoch 741, Loss: 1.1762978732585907, Final Batch Loss: 0.25589632987976074\n",
      "Epoch 742, Loss: 1.1884499192237854, Final Batch Loss: 0.3071540892124176\n",
      "Epoch 743, Loss: 1.1697668135166168, Final Batch Loss: 0.2898235023021698\n",
      "Epoch 744, Loss: 1.2390463650226593, Final Batch Loss: 0.2734951078891754\n",
      "Epoch 745, Loss: 1.238659679889679, Final Batch Loss: 0.34122946858406067\n",
      "Epoch 746, Loss: 1.137944296002388, Final Batch Loss: 0.22328783571720123\n",
      "Epoch 747, Loss: 1.2536849081516266, Final Batch Loss: 0.3426501750946045\n",
      "Epoch 748, Loss: 1.2809840738773346, Final Batch Loss: 0.3955942988395691\n",
      "Epoch 749, Loss: 1.2626232206821442, Final Batch Loss: 0.4240521788597107\n",
      "Epoch 750, Loss: 1.1455757915973663, Final Batch Loss: 0.26973000168800354\n",
      "Epoch 751, Loss: 1.145998790860176, Final Batch Loss: 0.23175467550754547\n",
      "Epoch 752, Loss: 1.2027732133865356, Final Batch Loss: 0.32163721323013306\n",
      "Epoch 753, Loss: 1.155150592327118, Final Batch Loss: 0.26685217022895813\n",
      "Epoch 754, Loss: 1.2294387221336365, Final Batch Loss: 0.33277782797813416\n",
      "Epoch 755, Loss: 1.2500060498714447, Final Batch Loss: 0.3095368444919586\n",
      "Epoch 756, Loss: 1.1527146995067596, Final Batch Loss: 0.22000879049301147\n",
      "Epoch 757, Loss: 1.2984070479869843, Final Batch Loss: 0.36141830682754517\n",
      "Epoch 758, Loss: 1.2900967001914978, Final Batch Loss: 0.3035278618335724\n",
      "Epoch 759, Loss: 1.227581262588501, Final Batch Loss: 0.31322363018989563\n",
      "Epoch 760, Loss: 1.0975819677114487, Final Batch Loss: 0.2296125739812851\n",
      "Epoch 761, Loss: 1.095258429646492, Final Batch Loss: 0.2979203164577484\n",
      "Epoch 762, Loss: 1.2101270854473114, Final Batch Loss: 0.31833770871162415\n",
      "Epoch 763, Loss: 1.2481469810009003, Final Batch Loss: 0.35602691769599915\n",
      "Epoch 764, Loss: 1.1995930671691895, Final Batch Loss: 0.33813053369522095\n",
      "Epoch 765, Loss: 1.1532180160284042, Final Batch Loss: 0.22772599756717682\n",
      "Epoch 766, Loss: 1.3572444021701813, Final Batch Loss: 0.37599778175354004\n",
      "Epoch 767, Loss: 1.1600047647953033, Final Batch Loss: 0.2467396855354309\n",
      "Epoch 768, Loss: 1.2153988480567932, Final Batch Loss: 0.2906273901462555\n",
      "Epoch 769, Loss: 1.188783049583435, Final Batch Loss: 0.30861958861351013\n",
      "Epoch 770, Loss: 1.250802755355835, Final Batch Loss: 0.2613224387168884\n",
      "Epoch 771, Loss: 1.2136950939893723, Final Batch Loss: 0.3363015651702881\n",
      "Epoch 772, Loss: 1.22939994931221, Final Batch Loss: 0.336712509393692\n",
      "Epoch 773, Loss: 1.2399047017097473, Final Batch Loss: 0.3217487931251526\n",
      "Epoch 774, Loss: 1.1826488077640533, Final Batch Loss: 0.28644710779190063\n",
      "Epoch 775, Loss: 1.224398136138916, Final Batch Loss: 0.3311629593372345\n",
      "Epoch 776, Loss: 1.2130901217460632, Final Batch Loss: 0.2698446810245514\n",
      "Epoch 777, Loss: 1.150512456893921, Final Batch Loss: 0.2754724621772766\n",
      "Epoch 778, Loss: 1.2236226499080658, Final Batch Loss: 0.2600630223751068\n",
      "Epoch 779, Loss: 1.1721864342689514, Final Batch Loss: 0.29140615463256836\n",
      "Epoch 780, Loss: 1.2573901116847992, Final Batch Loss: 0.3552981913089752\n",
      "Epoch 781, Loss: 1.2717115879058838, Final Batch Loss: 0.358269065618515\n",
      "Epoch 782, Loss: 1.310438185930252, Final Batch Loss: 0.43904000520706177\n",
      "Epoch 783, Loss: 1.1670724749565125, Final Batch Loss: 0.2954222559928894\n",
      "Epoch 784, Loss: 1.1921690702438354, Final Batch Loss: 0.30652281641960144\n",
      "Epoch 785, Loss: 1.2999885380268097, Final Batch Loss: 0.36910101771354675\n",
      "Epoch 786, Loss: 1.2399923205375671, Final Batch Loss: 0.3556516170501709\n",
      "Epoch 787, Loss: 1.1911438703536987, Final Batch Loss: 0.3127686083316803\n",
      "Epoch 788, Loss: 1.2251825630664825, Final Batch Loss: 0.3186429738998413\n",
      "Epoch 789, Loss: 1.2396244704723358, Final Batch Loss: 0.32673853635787964\n",
      "Epoch 790, Loss: 1.1344564706087112, Final Batch Loss: 0.2789986729621887\n",
      "Epoch 791, Loss: 1.0935567617416382, Final Batch Loss: 0.21123036742210388\n",
      "Epoch 792, Loss: 1.1701322048902512, Final Batch Loss: 0.23981435596942902\n",
      "Epoch 793, Loss: 1.1870605498552322, Final Batch Loss: 0.24516163766384125\n",
      "Epoch 794, Loss: 1.2137273252010345, Final Batch Loss: 0.3194831311702728\n",
      "Epoch 795, Loss: 1.1674400866031647, Final Batch Loss: 0.282012403011322\n",
      "Epoch 796, Loss: 1.2037230432033539, Final Batch Loss: 0.30105486512184143\n",
      "Epoch 797, Loss: 1.2109678089618683, Final Batch Loss: 0.32395899295806885\n",
      "Epoch 798, Loss: 1.2482816576957703, Final Batch Loss: 0.33149483799934387\n",
      "Epoch 799, Loss: 1.263682872056961, Final Batch Loss: 0.3727492094039917\n",
      "Epoch 800, Loss: 1.151871219277382, Final Batch Loss: 0.21461708843708038\n",
      "Epoch 801, Loss: 1.2185166478157043, Final Batch Loss: 0.3400901257991791\n",
      "Epoch 802, Loss: 1.1640736162662506, Final Batch Loss: 0.22844046354293823\n",
      "Epoch 803, Loss: 1.1203726381063461, Final Batch Loss: 0.21760542690753937\n",
      "Epoch 804, Loss: 1.1479486972093582, Final Batch Loss: 0.2447107583284378\n",
      "Epoch 805, Loss: 1.247747614979744, Final Batch Loss: 0.3267376720905304\n",
      "Epoch 806, Loss: 1.2922308593988419, Final Batch Loss: 0.40384137630462646\n",
      "Epoch 807, Loss: 1.2284579277038574, Final Batch Loss: 0.2831626832485199\n",
      "Epoch 808, Loss: 1.2344013452529907, Final Batch Loss: 0.4293830394744873\n",
      "Epoch 809, Loss: 1.230515480041504, Final Batch Loss: 0.29368922114372253\n",
      "Epoch 810, Loss: 1.1849022805690765, Final Batch Loss: 0.26745307445526123\n",
      "Epoch 811, Loss: 1.2068294137716293, Final Batch Loss: 0.3238426148891449\n",
      "Epoch 812, Loss: 1.1693600714206696, Final Batch Loss: 0.29192522168159485\n",
      "Epoch 813, Loss: 1.1642449498176575, Final Batch Loss: 0.2833355963230133\n",
      "Epoch 814, Loss: 1.2636412382125854, Final Batch Loss: 0.35365766286849976\n",
      "Epoch 815, Loss: 1.2075075507164001, Final Batch Loss: 0.3435145914554596\n",
      "Epoch 816, Loss: 1.1483860611915588, Final Batch Loss: 0.29029616713523865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 817, Loss: 1.152230441570282, Final Batch Loss: 0.25442826747894287\n",
      "Epoch 818, Loss: 1.2513581812381744, Final Batch Loss: 0.2512829601764679\n",
      "Epoch 819, Loss: 1.122334972023964, Final Batch Loss: 0.2126302868127823\n",
      "Epoch 820, Loss: 1.188817322254181, Final Batch Loss: 0.3214363753795624\n",
      "Epoch 821, Loss: 1.1753288507461548, Final Batch Loss: 0.32148677110671997\n",
      "Epoch 822, Loss: 1.2172948122024536, Final Batch Loss: 0.278108686208725\n",
      "Epoch 823, Loss: 1.2454696148633957, Final Batch Loss: 0.3165428340435028\n",
      "Epoch 824, Loss: 1.2294220328330994, Final Batch Loss: 0.3174079358577728\n",
      "Epoch 825, Loss: 1.2149900794029236, Final Batch Loss: 0.3121686577796936\n",
      "Epoch 826, Loss: 1.2615401148796082, Final Batch Loss: 0.2995428442955017\n",
      "Epoch 827, Loss: 1.2565435767173767, Final Batch Loss: 0.412733256816864\n",
      "Epoch 828, Loss: 1.2273118793964386, Final Batch Loss: 0.35470208525657654\n",
      "Epoch 829, Loss: 1.1124934554100037, Final Batch Loss: 0.2150568664073944\n",
      "Epoch 830, Loss: 1.157337635755539, Final Batch Loss: 0.28166142106056213\n",
      "Epoch 831, Loss: 1.207948088645935, Final Batch Loss: 0.3398835361003876\n",
      "Epoch 832, Loss: 1.1280977874994278, Final Batch Loss: 0.21824507415294647\n",
      "Epoch 833, Loss: 1.2107758224010468, Final Batch Loss: 0.289467990398407\n",
      "Epoch 834, Loss: 1.2009751945734024, Final Batch Loss: 0.22663919627666473\n",
      "Epoch 835, Loss: 1.1961693167686462, Final Batch Loss: 0.28523123264312744\n",
      "Epoch 836, Loss: 1.155502736568451, Final Batch Loss: 0.28465893864631653\n",
      "Epoch 837, Loss: 1.2029935717582703, Final Batch Loss: 0.2873535752296448\n",
      "Epoch 838, Loss: 1.1328945457935333, Final Batch Loss: 0.274661123752594\n",
      "Epoch 839, Loss: 1.238776832818985, Final Batch Loss: 0.3304651975631714\n",
      "Epoch 840, Loss: 1.2690321505069733, Final Batch Loss: 0.37328049540519714\n",
      "Epoch 841, Loss: 1.232466146349907, Final Batch Loss: 0.23605553805828094\n",
      "Epoch 842, Loss: 1.2225487232208252, Final Batch Loss: 0.34584876894950867\n",
      "Epoch 843, Loss: 1.1715902090072632, Final Batch Loss: 0.2517048120498657\n",
      "Epoch 844, Loss: 1.228283703327179, Final Batch Loss: 0.29829126596450806\n",
      "Epoch 845, Loss: 1.2332906424999237, Final Batch Loss: 0.3130604326725006\n",
      "Epoch 846, Loss: 1.1400141417980194, Final Batch Loss: 0.2722158432006836\n",
      "Epoch 847, Loss: 1.2030547857284546, Final Batch Loss: 0.2883012890815735\n",
      "Epoch 848, Loss: 1.1472771763801575, Final Batch Loss: 0.24985039234161377\n",
      "Epoch 849, Loss: 1.117110550403595, Final Batch Loss: 0.24380096793174744\n",
      "Epoch 850, Loss: 1.193380355834961, Final Batch Loss: 0.3592562973499298\n",
      "Epoch 851, Loss: 1.1689200401306152, Final Batch Loss: 0.2779495418071747\n",
      "Epoch 852, Loss: 1.2172779440879822, Final Batch Loss: 0.3312233090400696\n",
      "Epoch 853, Loss: 1.222087800502777, Final Batch Loss: 0.26495856046676636\n",
      "Epoch 854, Loss: 1.2379551231861115, Final Batch Loss: 0.3753281831741333\n",
      "Epoch 855, Loss: 1.2774309813976288, Final Batch Loss: 0.3796791136264801\n",
      "Epoch 856, Loss: 1.2118287980556488, Final Batch Loss: 0.3361797332763672\n",
      "Epoch 857, Loss: 1.1291413307189941, Final Batch Loss: 0.28023603558540344\n",
      "Epoch 858, Loss: 1.1565910279750824, Final Batch Loss: 0.2780345380306244\n",
      "Epoch 859, Loss: 1.1352056711912155, Final Batch Loss: 0.2774908244609833\n",
      "Epoch 860, Loss: 1.1833966672420502, Final Batch Loss: 0.31746673583984375\n",
      "Epoch 861, Loss: 1.12677600979805, Final Batch Loss: 0.25690916180610657\n",
      "Epoch 862, Loss: 1.1048050820827484, Final Batch Loss: 0.22214630246162415\n",
      "Epoch 863, Loss: 1.246367633342743, Final Batch Loss: 0.340783953666687\n",
      "Epoch 864, Loss: 1.1799298524856567, Final Batch Loss: 0.2678282856941223\n",
      "Epoch 865, Loss: 1.2144538462162018, Final Batch Loss: 0.3091614544391632\n",
      "Epoch 866, Loss: 1.2820717990398407, Final Batch Loss: 0.3452041447162628\n",
      "Epoch 867, Loss: 1.2027159631252289, Final Batch Loss: 0.3326047658920288\n",
      "Epoch 868, Loss: 1.2480905652046204, Final Batch Loss: 0.32467740774154663\n",
      "Epoch 869, Loss: 1.1985786855220795, Final Batch Loss: 0.32140854001045227\n",
      "Epoch 870, Loss: 1.1895892918109894, Final Batch Loss: 0.3219393193721771\n",
      "Epoch 871, Loss: 1.1848221123218536, Final Batch Loss: 0.27139392495155334\n",
      "Epoch 872, Loss: 1.1893677860498428, Final Batch Loss: 0.36326611042022705\n",
      "Epoch 873, Loss: 1.1367178559303284, Final Batch Loss: 0.2779003381729126\n",
      "Epoch 874, Loss: 1.2644048035144806, Final Batch Loss: 0.33729735016822815\n",
      "Epoch 875, Loss: 1.1295722424983978, Final Batch Loss: 0.27643871307373047\n",
      "Epoch 876, Loss: 1.1654082238674164, Final Batch Loss: 0.24490955471992493\n",
      "Epoch 877, Loss: 1.174476996064186, Final Batch Loss: 0.3219565153121948\n",
      "Epoch 878, Loss: 1.2090910375118256, Final Batch Loss: 0.33247944712638855\n",
      "Epoch 879, Loss: 1.172541856765747, Final Batch Loss: 0.3255557715892792\n",
      "Epoch 880, Loss: 1.1314880549907684, Final Batch Loss: 0.22606629133224487\n",
      "Epoch 881, Loss: 1.122660368680954, Final Batch Loss: 0.2655555307865143\n",
      "Epoch 882, Loss: 1.1628896296024323, Final Batch Loss: 0.29564952850341797\n",
      "Epoch 883, Loss: 1.2303646802902222, Final Batch Loss: 0.3918982148170471\n",
      "Epoch 884, Loss: 1.2604590952396393, Final Batch Loss: 0.43615466356277466\n",
      "Epoch 885, Loss: 1.155188500881195, Final Batch Loss: 0.3028929829597473\n",
      "Epoch 886, Loss: 1.3004924058914185, Final Batch Loss: 0.42881011962890625\n",
      "Epoch 887, Loss: 1.146546095609665, Final Batch Loss: 0.2772082984447479\n",
      "Epoch 888, Loss: 1.1864426136016846, Final Batch Loss: 0.29169854521751404\n",
      "Epoch 889, Loss: 1.1873916983604431, Final Batch Loss: 0.2775494158267975\n",
      "Epoch 890, Loss: 1.094330072402954, Final Batch Loss: 0.25641632080078125\n",
      "Epoch 891, Loss: 1.2051788866519928, Final Batch Loss: 0.31009435653686523\n",
      "Epoch 892, Loss: 1.222436249256134, Final Batch Loss: 0.36418792605400085\n",
      "Epoch 893, Loss: 1.1465493440628052, Final Batch Loss: 0.26860305666923523\n",
      "Epoch 894, Loss: 1.1300232857465744, Final Batch Loss: 0.2190776914358139\n",
      "Epoch 895, Loss: 1.1355680227279663, Final Batch Loss: 0.20242804288864136\n",
      "Epoch 896, Loss: 1.077519878745079, Final Batch Loss: 0.22625885903835297\n",
      "Epoch 897, Loss: 1.1719207465648651, Final Batch Loss: 0.28642791509628296\n",
      "Epoch 898, Loss: 1.1685215830802917, Final Batch Loss: 0.30321186780929565\n",
      "Epoch 899, Loss: 1.1344506591558456, Final Batch Loss: 0.29459479451179504\n",
      "Epoch 900, Loss: 1.2846559882164001, Final Batch Loss: 0.3808026909828186\n",
      "Epoch 901, Loss: 1.1435090154409409, Final Batch Loss: 0.2495177537202835\n",
      "Epoch 902, Loss: 1.1791155338287354, Final Batch Loss: 0.287154883146286\n",
      "Epoch 903, Loss: 1.212492048740387, Final Batch Loss: 0.31283384561538696\n",
      "Epoch 904, Loss: 1.0981264859437943, Final Batch Loss: 0.20226679742336273\n",
      "Epoch 905, Loss: 1.1632589101791382, Final Batch Loss: 0.32494252920150757\n",
      "Epoch 906, Loss: 1.0597789138555527, Final Batch Loss: 0.21799792349338531\n",
      "Epoch 907, Loss: 1.1171002984046936, Final Batch Loss: 0.2976424992084503\n",
      "Epoch 908, Loss: 1.1173043251037598, Final Batch Loss: 0.3153625726699829\n",
      "Epoch 909, Loss: 1.202414095401764, Final Batch Loss: 0.3506687879562378\n",
      "Epoch 910, Loss: 1.1422683596611023, Final Batch Loss: 0.31151774525642395\n",
      "Epoch 911, Loss: 1.2182357013225555, Final Batch Loss: 0.3500567078590393\n",
      "Epoch 912, Loss: 1.1476415246725082, Final Batch Loss: 0.36192309856414795\n",
      "Epoch 913, Loss: 1.1937963366508484, Final Batch Loss: 0.2776089906692505\n",
      "Epoch 914, Loss: 1.0607509762048721, Final Batch Loss: 0.2183610051870346\n",
      "Epoch 915, Loss: 1.1676173210144043, Final Batch Loss: 0.2927636504173279\n",
      "Epoch 916, Loss: 1.1855534464120865, Final Batch Loss: 0.22277404367923737\n",
      "Epoch 917, Loss: 1.0686187446117401, Final Batch Loss: 0.2838529050350189\n",
      "Epoch 918, Loss: 1.1305347383022308, Final Batch Loss: 0.2732442319393158\n",
      "Epoch 919, Loss: 1.2268993705511093, Final Batch Loss: 0.43149253726005554\n",
      "Epoch 920, Loss: 1.218930035829544, Final Batch Loss: 0.38789188861846924\n",
      "Epoch 921, Loss: 1.1287076771259308, Final Batch Loss: 0.23294302821159363\n",
      "Epoch 922, Loss: 1.3161678612232208, Final Batch Loss: 0.3936252295970917\n",
      "Epoch 923, Loss: 1.232665941119194, Final Batch Loss: 0.33445775508880615\n",
      "Epoch 924, Loss: 1.2196162939071655, Final Batch Loss: 0.31163811683654785\n",
      "Epoch 925, Loss: 1.1849782764911652, Final Batch Loss: 0.2228093147277832\n",
      "Epoch 926, Loss: 1.1905662417411804, Final Batch Loss: 0.297450989484787\n",
      "Epoch 927, Loss: 1.2356916069984436, Final Batch Loss: 0.37184903025627136\n",
      "Epoch 928, Loss: 1.211942121386528, Final Batch Loss: 0.3650740385055542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 929, Loss: 1.1143867373466492, Final Batch Loss: 0.2704884707927704\n",
      "Epoch 930, Loss: 1.1169876456260681, Final Batch Loss: 0.2551977038383484\n",
      "Epoch 931, Loss: 1.1283843219280243, Final Batch Loss: 0.28964972496032715\n",
      "Epoch 932, Loss: 1.1659768521785736, Final Batch Loss: 0.3338090777397156\n",
      "Epoch 933, Loss: 1.212689071893692, Final Batch Loss: 0.30165988206863403\n",
      "Epoch 934, Loss: 1.166881114244461, Final Batch Loss: 0.26625609397888184\n",
      "Epoch 935, Loss: 1.1803386211395264, Final Batch Loss: 0.2705051600933075\n",
      "Epoch 936, Loss: 1.1929387748241425, Final Batch Loss: 0.27549082040786743\n",
      "Epoch 937, Loss: 1.1025586426258087, Final Batch Loss: 0.2355513870716095\n",
      "Epoch 938, Loss: 1.1894583404064178, Final Batch Loss: 0.2939741611480713\n",
      "Epoch 939, Loss: 1.186290830373764, Final Batch Loss: 0.2946799099445343\n",
      "Epoch 940, Loss: 1.153095930814743, Final Batch Loss: 0.2586660087108612\n",
      "Epoch 941, Loss: 1.1383515447378159, Final Batch Loss: 0.26019981503486633\n",
      "Epoch 942, Loss: 1.0585047006607056, Final Batch Loss: 0.25483623147010803\n",
      "Epoch 943, Loss: 1.1057656854391098, Final Batch Loss: 0.31465429067611694\n",
      "Epoch 944, Loss: 1.0887158811092377, Final Batch Loss: 0.22740373015403748\n",
      "Epoch 945, Loss: 1.2302509248256683, Final Batch Loss: 0.3410666584968567\n",
      "Epoch 946, Loss: 1.2087299525737762, Final Batch Loss: 0.3428061902523041\n",
      "Epoch 947, Loss: 1.046863928437233, Final Batch Loss: 0.18108204007148743\n",
      "Epoch 948, Loss: 1.1211063116788864, Final Batch Loss: 0.24222032725811005\n",
      "Epoch 949, Loss: 1.1030203700065613, Final Batch Loss: 0.28238680958747864\n",
      "Epoch 950, Loss: 1.103069469332695, Final Batch Loss: 0.22298084199428558\n",
      "Epoch 951, Loss: 1.1564027667045593, Final Batch Loss: 0.300762414932251\n",
      "Epoch 952, Loss: 1.08969384431839, Final Batch Loss: 0.26222899556159973\n",
      "Epoch 953, Loss: 1.00272136926651, Final Batch Loss: 0.13921064138412476\n",
      "Epoch 954, Loss: 1.0695289075374603, Final Batch Loss: 0.26623445749282837\n",
      "Epoch 955, Loss: 1.1599111258983612, Final Batch Loss: 0.2621043920516968\n",
      "Epoch 956, Loss: 1.1489775776863098, Final Batch Loss: 0.2893654406070709\n",
      "Epoch 957, Loss: 1.196891725063324, Final Batch Loss: 0.3394899368286133\n",
      "Epoch 958, Loss: 1.072474628686905, Final Batch Loss: 0.23455822467803955\n",
      "Epoch 959, Loss: 1.2084596455097198, Final Batch Loss: 0.3443237841129303\n",
      "Epoch 960, Loss: 1.0817997604608536, Final Batch Loss: 0.2063598483800888\n",
      "Epoch 961, Loss: 1.2153225243091583, Final Batch Loss: 0.27044346928596497\n",
      "Epoch 962, Loss: 1.1578848659992218, Final Batch Loss: 0.27507925033569336\n",
      "Epoch 963, Loss: 1.0730006992816925, Final Batch Loss: 0.25350022315979004\n",
      "Epoch 964, Loss: 1.1330004334449768, Final Batch Loss: 0.25517404079437256\n",
      "Epoch 965, Loss: 1.1292644441127777, Final Batch Loss: 0.2717452347278595\n",
      "Epoch 966, Loss: 1.1687982678413391, Final Batch Loss: 0.3101937174797058\n",
      "Epoch 967, Loss: 1.1741232573986053, Final Batch Loss: 0.3413618803024292\n",
      "Epoch 968, Loss: 1.0959749966859818, Final Batch Loss: 0.19664330780506134\n",
      "Epoch 969, Loss: 1.1337630450725555, Final Batch Loss: 0.25444868206977844\n",
      "Epoch 970, Loss: 1.185325562953949, Final Batch Loss: 0.32405784726142883\n",
      "Epoch 971, Loss: 1.116012990474701, Final Batch Loss: 0.27363696694374084\n",
      "Epoch 972, Loss: 1.1455160081386566, Final Batch Loss: 0.29548269510269165\n",
      "Epoch 973, Loss: 1.1515294015407562, Final Batch Loss: 0.33807188272476196\n",
      "Epoch 974, Loss: 1.0847040712833405, Final Batch Loss: 0.2192625105381012\n",
      "Epoch 975, Loss: 1.155454158782959, Final Batch Loss: 0.2680824398994446\n",
      "Epoch 976, Loss: 1.0998317003250122, Final Batch Loss: 0.275246798992157\n",
      "Epoch 977, Loss: 1.1640537083148956, Final Batch Loss: 0.32014280557632446\n",
      "Epoch 978, Loss: 1.135306105017662, Final Batch Loss: 0.2967553436756134\n",
      "Epoch 979, Loss: 1.2216322720050812, Final Batch Loss: 0.3742212653160095\n",
      "Epoch 980, Loss: 1.1122168451547623, Final Batch Loss: 0.2704659700393677\n",
      "Epoch 981, Loss: 1.1370268166065216, Final Batch Loss: 0.2858836054801941\n",
      "Epoch 982, Loss: 1.1189012229442596, Final Batch Loss: 0.30458858609199524\n",
      "Epoch 983, Loss: 1.098569631576538, Final Batch Loss: 0.2657405436038971\n",
      "Epoch 984, Loss: 1.1010088622570038, Final Batch Loss: 0.28351446986198425\n",
      "Epoch 985, Loss: 1.1460078060626984, Final Batch Loss: 0.3382914066314697\n",
      "Epoch 986, Loss: 1.0920306295156479, Final Batch Loss: 0.22878994047641754\n",
      "Epoch 987, Loss: 1.1673372983932495, Final Batch Loss: 0.28643888235092163\n",
      "Epoch 988, Loss: 1.1515144109725952, Final Batch Loss: 0.31265345215797424\n",
      "Epoch 989, Loss: 1.1463394463062286, Final Batch Loss: 0.360302209854126\n",
      "Epoch 990, Loss: 1.1223190426826477, Final Batch Loss: 0.2607465386390686\n",
      "Epoch 991, Loss: 1.0271539092063904, Final Batch Loss: 0.18467217683792114\n",
      "Epoch 992, Loss: 1.1449110209941864, Final Batch Loss: 0.2612764835357666\n",
      "Epoch 993, Loss: 1.123984083533287, Final Batch Loss: 0.2273254543542862\n",
      "Epoch 994, Loss: 1.0662320852279663, Final Batch Loss: 0.25691911578178406\n",
      "Epoch 995, Loss: 1.173831731081009, Final Batch Loss: 0.3594478666782379\n",
      "Epoch 996, Loss: 1.224988043308258, Final Batch Loss: 0.3377090394496918\n",
      "Epoch 997, Loss: 1.1580220311880112, Final Batch Loss: 0.3244849741458893\n",
      "Epoch 998, Loss: 1.1206192076206207, Final Batch Loss: 0.26373234391212463\n",
      "Epoch 999, Loss: 1.082690715789795, Final Batch Loss: 0.2661978006362915\n",
      "Epoch 1000, Loss: 1.2055263221263885, Final Batch Loss: 0.3777604103088379\n",
      "Epoch 1001, Loss: 1.1254884600639343, Final Batch Loss: 0.3070085644721985\n",
      "Epoch 1002, Loss: 1.1305920481681824, Final Batch Loss: 0.32646429538726807\n",
      "Epoch 1003, Loss: 1.17241932451725, Final Batch Loss: 0.3423503041267395\n",
      "Epoch 1004, Loss: 1.1434716880321503, Final Batch Loss: 0.3110993206501007\n",
      "Epoch 1005, Loss: 1.2493211030960083, Final Batch Loss: 0.37277522683143616\n",
      "Epoch 1006, Loss: 1.1566939055919647, Final Batch Loss: 0.30409887433052063\n",
      "Epoch 1007, Loss: 1.2172408998012543, Final Batch Loss: 0.3507940173149109\n",
      "Epoch 1008, Loss: 1.1256269812583923, Final Batch Loss: 0.2559948265552521\n",
      "Epoch 1009, Loss: 1.2436710000038147, Final Batch Loss: 0.4034479260444641\n",
      "Epoch 1010, Loss: 1.141821712255478, Final Batch Loss: 0.2911178469657898\n",
      "Epoch 1011, Loss: 1.1825334131717682, Final Batch Loss: 0.3315725028514862\n",
      "Epoch 1012, Loss: 1.1616586297750473, Final Batch Loss: 0.30576393008232117\n",
      "Epoch 1013, Loss: 1.0933856070041656, Final Batch Loss: 0.2723255753517151\n",
      "Epoch 1014, Loss: 1.0909869074821472, Final Batch Loss: 0.24469375610351562\n",
      "Epoch 1015, Loss: 1.1439226269721985, Final Batch Loss: 0.33949095010757446\n",
      "Epoch 1016, Loss: 1.1057139486074448, Final Batch Loss: 0.24570085108280182\n",
      "Epoch 1017, Loss: 1.1775128245353699, Final Batch Loss: 0.2940883934497833\n",
      "Epoch 1018, Loss: 1.100576013326645, Final Batch Loss: 0.22785994410514832\n",
      "Epoch 1019, Loss: 1.1327600628137589, Final Batch Loss: 0.2469230741262436\n",
      "Epoch 1020, Loss: 1.079092562198639, Final Batch Loss: 0.2588537335395813\n",
      "Epoch 1021, Loss: 1.1096071749925613, Final Batch Loss: 0.27782922983169556\n",
      "Epoch 1022, Loss: 1.122343271970749, Final Batch Loss: 0.2899492084980011\n",
      "Epoch 1023, Loss: 1.0169813632965088, Final Batch Loss: 0.2090732604265213\n",
      "Epoch 1024, Loss: 1.0095457434654236, Final Batch Loss: 0.1848437786102295\n",
      "Epoch 1025, Loss: 1.1295478343963623, Final Batch Loss: 0.3083822727203369\n",
      "Epoch 1026, Loss: 1.0790116637945175, Final Batch Loss: 0.18125684559345245\n",
      "Epoch 1027, Loss: 1.0723609924316406, Final Batch Loss: 0.2602881193161011\n",
      "Epoch 1028, Loss: 1.1023199558258057, Final Batch Loss: 0.287909597158432\n",
      "Epoch 1029, Loss: 1.1396893858909607, Final Batch Loss: 0.3138432204723358\n",
      "Epoch 1030, Loss: 1.1125126481056213, Final Batch Loss: 0.27436402440071106\n",
      "Epoch 1031, Loss: 1.0426188558340073, Final Batch Loss: 0.18421946465969086\n",
      "Epoch 1032, Loss: 1.1765835583209991, Final Batch Loss: 0.2934844195842743\n",
      "Epoch 1033, Loss: 1.146595537662506, Final Batch Loss: 0.27903974056243896\n",
      "Epoch 1034, Loss: 1.1220634281635284, Final Batch Loss: 0.27029043436050415\n",
      "Epoch 1035, Loss: 1.040016531944275, Final Batch Loss: 0.25802507996559143\n",
      "Epoch 1036, Loss: 1.0823019742965698, Final Batch Loss: 0.26272982358932495\n",
      "Epoch 1037, Loss: 1.065695434808731, Final Batch Loss: 0.213759183883667\n",
      "Epoch 1038, Loss: 1.1348450481891632, Final Batch Loss: 0.28440818190574646\n",
      "Epoch 1039, Loss: 1.13336381316185, Final Batch Loss: 0.32992929220199585\n",
      "Epoch 1040, Loss: 1.1026736795902252, Final Batch Loss: 0.2655247151851654\n",
      "Epoch 1041, Loss: 1.1362051516771317, Final Batch Loss: 0.3240965008735657\n",
      "Epoch 1042, Loss: 1.2156340181827545, Final Batch Loss: 0.31820356845855713\n",
      "Epoch 1043, Loss: 1.2709253132343292, Final Batch Loss: 0.3593471944332123\n",
      "Epoch 1044, Loss: 1.0816069394350052, Final Batch Loss: 0.21893461048603058\n",
      "Epoch 1045, Loss: 1.090772807598114, Final Batch Loss: 0.27910566329956055\n",
      "Epoch 1046, Loss: 1.1092962473630905, Final Batch Loss: 0.24492251873016357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1047, Loss: 1.0804264098405838, Final Batch Loss: 0.18933559954166412\n",
      "Epoch 1048, Loss: 1.153156504034996, Final Batch Loss: 0.3569800853729248\n",
      "Epoch 1049, Loss: 1.1274266690015793, Final Batch Loss: 0.24953491985797882\n",
      "Epoch 1050, Loss: 1.1350491493940353, Final Batch Loss: 0.3158310353755951\n",
      "Epoch 1051, Loss: 1.177287757396698, Final Batch Loss: 0.3576240837574005\n",
      "Epoch 1052, Loss: 1.052769050002098, Final Batch Loss: 0.22717873752117157\n",
      "Epoch 1053, Loss: 1.1421501338481903, Final Batch Loss: 0.3012959063053131\n",
      "Epoch 1054, Loss: 1.1640753746032715, Final Batch Loss: 0.3056579530239105\n",
      "Epoch 1055, Loss: 1.0420735776424408, Final Batch Loss: 0.2295825183391571\n",
      "Epoch 1056, Loss: 1.069199338555336, Final Batch Loss: 0.2901623547077179\n",
      "Epoch 1057, Loss: 1.1214693933725357, Final Batch Loss: 0.31704631447792053\n",
      "Epoch 1058, Loss: 1.0555820763111115, Final Batch Loss: 0.22679507732391357\n",
      "Epoch 1059, Loss: 1.0816578567028046, Final Batch Loss: 0.19808125495910645\n",
      "Epoch 1060, Loss: 1.067152813076973, Final Batch Loss: 0.20781774818897247\n",
      "Epoch 1061, Loss: 1.126125305891037, Final Batch Loss: 0.26276394724845886\n",
      "Epoch 1062, Loss: 0.9950534403324127, Final Batch Loss: 0.2025471180677414\n",
      "Epoch 1063, Loss: 1.136171966791153, Final Batch Loss: 0.2787405252456665\n",
      "Epoch 1064, Loss: 1.1611110121011734, Final Batch Loss: 0.3803146779537201\n",
      "Epoch 1065, Loss: 1.0743874162435532, Final Batch Loss: 0.2869529128074646\n",
      "Epoch 1066, Loss: 1.1161812245845795, Final Batch Loss: 0.2880537211894989\n",
      "Epoch 1067, Loss: 1.151649922132492, Final Batch Loss: 0.335018515586853\n",
      "Epoch 1068, Loss: 1.0632322877645493, Final Batch Loss: 0.2791007459163666\n",
      "Epoch 1069, Loss: 1.173896074295044, Final Batch Loss: 0.2702164947986603\n",
      "Epoch 1070, Loss: 1.0748717039823532, Final Batch Loss: 0.2177038937807083\n",
      "Epoch 1071, Loss: 1.1492435932159424, Final Batch Loss: 0.31450355052948\n",
      "Epoch 1072, Loss: 1.140973538160324, Final Batch Loss: 0.25857633352279663\n",
      "Epoch 1073, Loss: 1.088357150554657, Final Batch Loss: 0.2585071623325348\n",
      "Epoch 1074, Loss: 1.1296688169240952, Final Batch Loss: 0.32394251227378845\n",
      "Epoch 1075, Loss: 1.1620970219373703, Final Batch Loss: 0.35080891847610474\n",
      "Epoch 1076, Loss: 1.1723315864801407, Final Batch Loss: 0.28206920623779297\n",
      "Epoch 1077, Loss: 1.1798800826072693, Final Batch Loss: 0.3284730911254883\n",
      "Epoch 1078, Loss: 1.2502018809318542, Final Batch Loss: 0.39861762523651123\n",
      "Epoch 1079, Loss: 1.106286272406578, Final Batch Loss: 0.2623676061630249\n",
      "Epoch 1080, Loss: 1.061632364988327, Final Batch Loss: 0.23110736906528473\n",
      "Epoch 1081, Loss: 1.0400493741035461, Final Batch Loss: 0.22788406908512115\n",
      "Epoch 1082, Loss: 1.1173804998397827, Final Batch Loss: 0.3231954276561737\n",
      "Epoch 1083, Loss: 1.1738698482513428, Final Batch Loss: 0.3275449872016907\n",
      "Epoch 1084, Loss: 1.0641170144081116, Final Batch Loss: 0.25339892506599426\n",
      "Epoch 1085, Loss: 1.1143073290586472, Final Batch Loss: 0.3224882483482361\n",
      "Epoch 1086, Loss: 1.1536281108856201, Final Batch Loss: 0.2932141423225403\n",
      "Epoch 1087, Loss: 1.065255418419838, Final Batch Loss: 0.2071094661951065\n",
      "Epoch 1088, Loss: 1.1425420343875885, Final Batch Loss: 0.28960174322128296\n",
      "Epoch 1089, Loss: 1.0588546991348267, Final Batch Loss: 0.20058095455169678\n",
      "Epoch 1090, Loss: 1.1555563062429428, Final Batch Loss: 0.30002841353416443\n",
      "Epoch 1091, Loss: 1.0980809777975082, Final Batch Loss: 0.2910340130329132\n",
      "Epoch 1092, Loss: 1.0592647045850754, Final Batch Loss: 0.22177697718143463\n",
      "Epoch 1093, Loss: 1.123249277472496, Final Batch Loss: 0.24585174024105072\n",
      "Epoch 1094, Loss: 1.0993754267692566, Final Batch Loss: 0.27563318610191345\n",
      "Epoch 1095, Loss: 1.1360148340463638, Final Batch Loss: 0.25991564989089966\n",
      "Epoch 1096, Loss: 1.1828366219997406, Final Batch Loss: 0.3374703824520111\n",
      "Epoch 1097, Loss: 1.1230539083480835, Final Batch Loss: 0.32507064938545227\n",
      "Epoch 1098, Loss: 1.1440922021865845, Final Batch Loss: 0.3410590589046478\n",
      "Epoch 1099, Loss: 1.0897535681724548, Final Batch Loss: 0.24597513675689697\n",
      "Epoch 1100, Loss: 1.0641568303108215, Final Batch Loss: 0.2770420014858246\n",
      "Epoch 1101, Loss: 1.1053386628627777, Final Batch Loss: 0.3124633729457855\n",
      "Epoch 1102, Loss: 1.205308973789215, Final Batch Loss: 0.3227494955062866\n",
      "Epoch 1103, Loss: 1.134677916765213, Final Batch Loss: 0.2930728793144226\n",
      "Epoch 1104, Loss: 1.1268629878759384, Final Batch Loss: 0.2843482196331024\n",
      "Epoch 1105, Loss: 1.1128024905920029, Final Batch Loss: 0.30364444851875305\n",
      "Epoch 1106, Loss: 1.1335221081972122, Final Batch Loss: 0.35571086406707764\n",
      "Epoch 1107, Loss: 1.1378901898860931, Final Batch Loss: 0.27519097924232483\n",
      "Epoch 1108, Loss: 1.2085686326026917, Final Batch Loss: 0.34085407853126526\n",
      "Epoch 1109, Loss: 1.1297903954982758, Final Batch Loss: 0.2483803629875183\n",
      "Epoch 1110, Loss: 1.0785594284534454, Final Batch Loss: 0.3097415864467621\n",
      "Epoch 1111, Loss: 1.13566155731678, Final Batch Loss: 0.36369696259498596\n",
      "Epoch 1112, Loss: 1.0633484423160553, Final Batch Loss: 0.2463257610797882\n",
      "Epoch 1113, Loss: 1.0411850214004517, Final Batch Loss: 0.24926835298538208\n",
      "Epoch 1114, Loss: 1.0949354767799377, Final Batch Loss: 0.2821009159088135\n",
      "Epoch 1115, Loss: 1.0622741878032684, Final Batch Loss: 0.29036304354667664\n",
      "Epoch 1116, Loss: 1.1096071153879166, Final Batch Loss: 0.2951289415359497\n",
      "Epoch 1117, Loss: 1.1972039937973022, Final Batch Loss: 0.3832454979419708\n",
      "Epoch 1118, Loss: 1.1699529588222504, Final Batch Loss: 0.2902957499027252\n",
      "Epoch 1119, Loss: 1.1992696225643158, Final Batch Loss: 0.3719249665737152\n",
      "Epoch 1120, Loss: 1.1447813212871552, Final Batch Loss: 0.2977103888988495\n",
      "Epoch 1121, Loss: 1.175420954823494, Final Batch Loss: 0.35207054018974304\n",
      "Epoch 1122, Loss: 1.0497731119394302, Final Batch Loss: 0.27164289355278015\n",
      "Epoch 1123, Loss: 1.0932507514953613, Final Batch Loss: 0.27279800176620483\n",
      "Epoch 1124, Loss: 1.1731023490428925, Final Batch Loss: 0.37459778785705566\n",
      "Epoch 1125, Loss: 1.0958440899848938, Final Batch Loss: 0.28960204124450684\n",
      "Epoch 1126, Loss: 1.0332237929105759, Final Batch Loss: 0.2216816395521164\n",
      "Epoch 1127, Loss: 1.0335026383399963, Final Batch Loss: 0.18434719741344452\n",
      "Epoch 1128, Loss: 1.0084156692028046, Final Batch Loss: 0.20527257025241852\n",
      "Epoch 1129, Loss: 1.0900213569402695, Final Batch Loss: 0.3353472054004669\n",
      "Epoch 1130, Loss: 1.0806919485330582, Final Batch Loss: 0.25035765767097473\n",
      "Epoch 1131, Loss: 1.2460837364196777, Final Batch Loss: 0.40701544284820557\n",
      "Epoch 1132, Loss: 1.1831376552581787, Final Batch Loss: 0.30110475420951843\n",
      "Epoch 1133, Loss: 1.1913806349039078, Final Batch Loss: 0.3382487893104553\n",
      "Epoch 1134, Loss: 1.027927577495575, Final Batch Loss: 0.17667768895626068\n",
      "Epoch 1135, Loss: 1.2418726533651352, Final Batch Loss: 0.43913671374320984\n",
      "Epoch 1136, Loss: 1.0464501082897186, Final Batch Loss: 0.19483333826065063\n",
      "Epoch 1137, Loss: 1.1316581964492798, Final Batch Loss: 0.3132438361644745\n",
      "Epoch 1138, Loss: 1.0445362031459808, Final Batch Loss: 0.22353065013885498\n",
      "Epoch 1139, Loss: 1.1859657168388367, Final Batch Loss: 0.3557676672935486\n",
      "Epoch 1140, Loss: 1.0932775735855103, Final Batch Loss: 0.3093540668487549\n",
      "Epoch 1141, Loss: 1.141124039888382, Final Batch Loss: 0.3335443139076233\n",
      "Epoch 1142, Loss: 1.1220019310712814, Final Batch Loss: 0.3082582652568817\n",
      "Epoch 1143, Loss: 1.0683346539735794, Final Batch Loss: 0.2857631742954254\n",
      "Epoch 1144, Loss: 1.0914559364318848, Final Batch Loss: 0.23067304491996765\n",
      "Epoch 1145, Loss: 1.1189803183078766, Final Batch Loss: 0.28094398975372314\n",
      "Epoch 1146, Loss: 1.0072316378355026, Final Batch Loss: 0.26784878969192505\n",
      "Epoch 1147, Loss: 1.1483997106552124, Final Batch Loss: 0.3411085903644562\n",
      "Epoch 1148, Loss: 1.080065757036209, Final Batch Loss: 0.31626519560813904\n",
      "Epoch 1149, Loss: 1.0199704021215439, Final Batch Loss: 0.19769775867462158\n",
      "Epoch 1150, Loss: 0.9843615218997002, Final Batch Loss: 0.11784345656633377\n",
      "Epoch 1151, Loss: 1.1202217042446136, Final Batch Loss: 0.3816765248775482\n",
      "Epoch 1152, Loss: 1.053102195262909, Final Batch Loss: 0.2423720508813858\n",
      "Epoch 1153, Loss: 1.0151475667953491, Final Batch Loss: 0.17461541295051575\n",
      "Epoch 1154, Loss: 1.046821653842926, Final Batch Loss: 0.2690601050853729\n",
      "Epoch 1155, Loss: 1.114987701177597, Final Batch Loss: 0.22380587458610535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1156, Loss: 1.0587111711502075, Final Batch Loss: 0.2745199203491211\n",
      "Epoch 1157, Loss: 1.0490491539239883, Final Batch Loss: 0.2113361805677414\n",
      "Epoch 1158, Loss: 1.0944098979234695, Final Batch Loss: 0.24482707679271698\n",
      "Epoch 1159, Loss: 1.037025898694992, Final Batch Loss: 0.30474066734313965\n",
      "Epoch 1160, Loss: 1.1264599859714508, Final Batch Loss: 0.3088175356388092\n",
      "Epoch 1161, Loss: 1.2010650336742401, Final Batch Loss: 0.3972683548927307\n",
      "Epoch 1162, Loss: 1.1211219429969788, Final Batch Loss: 0.2533992826938629\n",
      "Epoch 1163, Loss: 1.0469526648521423, Final Batch Loss: 0.22555401921272278\n",
      "Epoch 1164, Loss: 1.173934817314148, Final Batch Loss: 0.31660085916519165\n",
      "Epoch 1165, Loss: 1.0298988968133926, Final Batch Loss: 0.24162378907203674\n",
      "Epoch 1166, Loss: 1.0540800541639328, Final Batch Loss: 0.2200898379087448\n",
      "Epoch 1167, Loss: 1.0123824328184128, Final Batch Loss: 0.18512822687625885\n",
      "Epoch 1168, Loss: 1.086359903216362, Final Batch Loss: 0.27100855112075806\n",
      "Epoch 1169, Loss: 1.1789704263210297, Final Batch Loss: 0.341964989900589\n",
      "Epoch 1170, Loss: 1.0063206255435944, Final Batch Loss: 0.22373512387275696\n",
      "Epoch 1171, Loss: 1.0978116393089294, Final Batch Loss: 0.24437260627746582\n",
      "Epoch 1172, Loss: 1.16771599650383, Final Batch Loss: 0.2731947898864746\n",
      "Epoch 1173, Loss: 1.1236957013607025, Final Batch Loss: 0.24024875462055206\n",
      "Epoch 1174, Loss: 1.1563339531421661, Final Batch Loss: 0.291995644569397\n",
      "Epoch 1175, Loss: 1.1059799939393997, Final Batch Loss: 0.27089497447013855\n",
      "Epoch 1176, Loss: 1.104754477739334, Final Batch Loss: 0.26609566807746887\n",
      "Epoch 1177, Loss: 1.0750019699335098, Final Batch Loss: 0.2571508586406708\n",
      "Epoch 1178, Loss: 1.0773182809352875, Final Batch Loss: 0.2707989513874054\n",
      "Epoch 1179, Loss: 1.0782489776611328, Final Batch Loss: 0.272023469209671\n",
      "Epoch 1180, Loss: 1.0794574320316315, Final Batch Loss: 0.27930375933647156\n",
      "Epoch 1181, Loss: 1.038804218173027, Final Batch Loss: 0.25275686383247375\n",
      "Epoch 1182, Loss: 1.0583840906620026, Final Batch Loss: 0.24113473296165466\n",
      "Epoch 1183, Loss: 1.0598362535238266, Final Batch Loss: 0.2487056404352188\n",
      "Epoch 1184, Loss: 1.0920288860797882, Final Batch Loss: 0.3002127408981323\n",
      "Epoch 1185, Loss: 1.020874559879303, Final Batch Loss: 0.1825413703918457\n",
      "Epoch 1186, Loss: 1.0393399894237518, Final Batch Loss: 0.22289668023586273\n",
      "Epoch 1187, Loss: 1.1188111901283264, Final Batch Loss: 0.2609870731830597\n",
      "Epoch 1188, Loss: 1.019059270620346, Final Batch Loss: 0.21118390560150146\n",
      "Epoch 1189, Loss: 1.0545724630355835, Final Batch Loss: 0.2581183612346649\n",
      "Epoch 1190, Loss: 1.075787514448166, Final Batch Loss: 0.24957555532455444\n",
      "Epoch 1191, Loss: 1.0826340317726135, Final Batch Loss: 0.30539241433143616\n",
      "Epoch 1192, Loss: 1.0511007606983185, Final Batch Loss: 0.264034241437912\n",
      "Epoch 1193, Loss: 1.11777825653553, Final Batch Loss: 0.2950995862483978\n",
      "Epoch 1194, Loss: 1.0944746881723404, Final Batch Loss: 0.24042223393917084\n",
      "Epoch 1195, Loss: 1.0604217797517776, Final Batch Loss: 0.24056755006313324\n",
      "Epoch 1196, Loss: 1.071062594652176, Final Batch Loss: 0.19810907542705536\n",
      "Epoch 1197, Loss: 1.1527184396982193, Final Batch Loss: 0.40934866666793823\n",
      "Epoch 1198, Loss: 1.0480885356664658, Final Batch Loss: 0.2645515501499176\n",
      "Epoch 1199, Loss: 1.1111508011817932, Final Batch Loss: 0.27770835161209106\n",
      "Epoch 1200, Loss: 1.1511261463165283, Final Batch Loss: 0.30551445484161377\n",
      "Epoch 1201, Loss: 1.1538263708353043, Final Batch Loss: 0.36589908599853516\n",
      "Epoch 1202, Loss: 1.1635419875383377, Final Batch Loss: 0.35552164912223816\n",
      "Epoch 1203, Loss: 1.1095660328865051, Final Batch Loss: 0.3100258409976959\n",
      "Epoch 1204, Loss: 1.129524290561676, Final Batch Loss: 0.32236015796661377\n",
      "Epoch 1205, Loss: 1.0301639884710312, Final Batch Loss: 0.2453046590089798\n",
      "Epoch 1206, Loss: 1.0171915292739868, Final Batch Loss: 0.26886478066444397\n",
      "Epoch 1207, Loss: 1.0721350461244583, Final Batch Loss: 0.25519314408302307\n",
      "Epoch 1208, Loss: 1.089793473482132, Final Batch Loss: 0.334227055311203\n",
      "Epoch 1209, Loss: 1.062588393688202, Final Batch Loss: 0.21593794226646423\n",
      "Epoch 1210, Loss: 1.1338344812393188, Final Batch Loss: 0.32065385580062866\n",
      "Epoch 1211, Loss: 1.1265470087528229, Final Batch Loss: 0.3248428404331207\n",
      "Epoch 1212, Loss: 1.098332405090332, Final Batch Loss: 0.2688010632991791\n",
      "Epoch 1213, Loss: 1.0996244251728058, Final Batch Loss: 0.28703364729881287\n",
      "Epoch 1214, Loss: 1.0653641670942307, Final Batch Loss: 0.2912280857563019\n",
      "Epoch 1215, Loss: 1.087765246629715, Final Batch Loss: 0.2849213182926178\n",
      "Epoch 1216, Loss: 1.0498175621032715, Final Batch Loss: 0.22330176830291748\n",
      "Epoch 1217, Loss: 1.0562025606632233, Final Batch Loss: 0.2775128185749054\n",
      "Epoch 1218, Loss: 1.0523496270179749, Final Batch Loss: 0.212742418050766\n",
      "Epoch 1219, Loss: 1.1356861740350723, Final Batch Loss: 0.3771297037601471\n",
      "Epoch 1220, Loss: 1.0448912531137466, Final Batch Loss: 0.23015707731246948\n",
      "Epoch 1221, Loss: 1.028857097029686, Final Batch Loss: 0.2330811619758606\n",
      "Epoch 1222, Loss: 0.9992815852165222, Final Batch Loss: 0.2454923838376999\n",
      "Epoch 1223, Loss: 0.9819143712520599, Final Batch Loss: 0.18684934079647064\n",
      "Epoch 1224, Loss: 1.0979405641555786, Final Batch Loss: 0.35748428106307983\n",
      "Epoch 1225, Loss: 1.113233059644699, Final Batch Loss: 0.3157026767730713\n",
      "Epoch 1226, Loss: 1.004969283938408, Final Batch Loss: 0.2208564430475235\n",
      "Epoch 1227, Loss: 1.1146428734064102, Final Batch Loss: 0.3383374810218811\n",
      "Epoch 1228, Loss: 1.0220609307289124, Final Batch Loss: 0.2153349071741104\n",
      "Epoch 1229, Loss: 1.1552267968654633, Final Batch Loss: 0.3045685589313507\n",
      "Epoch 1230, Loss: 1.1083304733037949, Final Batch Loss: 0.2836858630180359\n",
      "Epoch 1231, Loss: 1.1156198978424072, Final Batch Loss: 0.2851814031600952\n",
      "Epoch 1232, Loss: 1.0344845205545425, Final Batch Loss: 0.24096208810806274\n",
      "Epoch 1233, Loss: 1.0313083827495575, Final Batch Loss: 0.25325021147727966\n",
      "Epoch 1234, Loss: 1.1311545073986053, Final Batch Loss: 0.37196090817451477\n",
      "Epoch 1235, Loss: 1.1570586264133453, Final Batch Loss: 0.3532544672489166\n",
      "Epoch 1236, Loss: 1.12160924077034, Final Batch Loss: 0.3019278645515442\n",
      "Epoch 1237, Loss: 1.1252520382404327, Final Batch Loss: 0.32699671387672424\n",
      "Epoch 1238, Loss: 1.110545039176941, Final Batch Loss: 0.286828875541687\n",
      "Epoch 1239, Loss: 1.0888997167348862, Final Batch Loss: 0.3035341501235962\n",
      "Epoch 1240, Loss: 1.0343286246061325, Final Batch Loss: 0.18908821046352386\n",
      "Epoch 1241, Loss: 1.1200746297836304, Final Batch Loss: 0.2765099108219147\n",
      "Epoch 1242, Loss: 1.0554813593626022, Final Batch Loss: 0.20449085533618927\n",
      "Epoch 1243, Loss: 1.041497752070427, Final Batch Loss: 0.21171791851520538\n",
      "Epoch 1244, Loss: 1.0493691265583038, Final Batch Loss: 0.27274882793426514\n",
      "Epoch 1245, Loss: 1.0066474974155426, Final Batch Loss: 0.24989008903503418\n",
      "Epoch 1246, Loss: 1.1332926005125046, Final Batch Loss: 0.3490932881832123\n",
      "Epoch 1247, Loss: 1.013558253645897, Final Batch Loss: 0.22144891321659088\n",
      "Epoch 1248, Loss: 1.0535318851470947, Final Batch Loss: 0.2668873369693756\n",
      "Epoch 1249, Loss: 1.1848296225070953, Final Batch Loss: 0.39425596594810486\n",
      "Epoch 1250, Loss: 0.9771499633789062, Final Batch Loss: 0.23327475786209106\n",
      "Epoch 1251, Loss: 1.075542852282524, Final Batch Loss: 0.21875852346420288\n",
      "Epoch 1252, Loss: 1.0402987152338028, Final Batch Loss: 0.24074684083461761\n",
      "Epoch 1253, Loss: 1.1197221279144287, Final Batch Loss: 0.2931903004646301\n",
      "Epoch 1254, Loss: 0.9984960108995438, Final Batch Loss: 0.2246946394443512\n",
      "Epoch 1255, Loss: 1.025828629732132, Final Batch Loss: 0.2754880487918854\n",
      "Epoch 1256, Loss: 1.0724370628595352, Final Batch Loss: 0.2504434287548065\n",
      "Epoch 1257, Loss: 1.0486340671777725, Final Batch Loss: 0.30432718992233276\n",
      "Epoch 1258, Loss: 1.0485666990280151, Final Batch Loss: 0.25221312046051025\n",
      "Epoch 1259, Loss: 1.0286469906568527, Final Batch Loss: 0.2506042420864105\n",
      "Epoch 1260, Loss: 1.0325357019901276, Final Batch Loss: 0.2222377210855484\n",
      "Epoch 1261, Loss: 1.171465128660202, Final Batch Loss: 0.3615392744541168\n",
      "Epoch 1262, Loss: 0.9871756881475449, Final Batch Loss: 0.22020629048347473\n",
      "Epoch 1263, Loss: 1.0715059638023376, Final Batch Loss: 0.22355961799621582\n",
      "Epoch 1264, Loss: 1.0955235362052917, Final Batch Loss: 0.34408336877822876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1265, Loss: 1.0995381325483322, Final Batch Loss: 0.3335626423358917\n",
      "Epoch 1266, Loss: 1.0745206475257874, Final Batch Loss: 0.2902721166610718\n",
      "Epoch 1267, Loss: 0.9763940125703812, Final Batch Loss: 0.22673410177230835\n",
      "Epoch 1268, Loss: 1.0692762732505798, Final Batch Loss: 0.28759056329727173\n",
      "Epoch 1269, Loss: 0.988326370716095, Final Batch Loss: 0.26580625772476196\n",
      "Epoch 1270, Loss: 1.0334930419921875, Final Batch Loss: 0.22547867894172668\n",
      "Epoch 1271, Loss: 1.096198558807373, Final Batch Loss: 0.2388104796409607\n",
      "Epoch 1272, Loss: 1.1185053884983063, Final Batch Loss: 0.3170023560523987\n",
      "Epoch 1273, Loss: 1.0787272453308105, Final Batch Loss: 0.30735060572624207\n",
      "Epoch 1274, Loss: 1.1084401905536652, Final Batch Loss: 0.2874639630317688\n",
      "Epoch 1275, Loss: 1.0291585326194763, Final Batch Loss: 0.2181393802165985\n",
      "Epoch 1276, Loss: 1.0598646104335785, Final Batch Loss: 0.26862213015556335\n",
      "Epoch 1277, Loss: 1.0317907929420471, Final Batch Loss: 0.21286216378211975\n",
      "Epoch 1278, Loss: 1.063129723072052, Final Batch Loss: 0.29466232657432556\n",
      "Epoch 1279, Loss: 1.079456940293312, Final Batch Loss: 0.24774207174777985\n",
      "Epoch 1280, Loss: 1.1415484249591827, Final Batch Loss: 0.31678101420402527\n",
      "Epoch 1281, Loss: 1.0198469161987305, Final Batch Loss: 0.18261371552944183\n",
      "Epoch 1282, Loss: 1.0509868264198303, Final Batch Loss: 0.1794007420539856\n",
      "Epoch 1283, Loss: 1.1288688480854034, Final Batch Loss: 0.3380710482597351\n",
      "Epoch 1284, Loss: 1.0500107109546661, Final Batch Loss: 0.2729381024837494\n",
      "Epoch 1285, Loss: 1.008812591433525, Final Batch Loss: 0.1876993030309677\n",
      "Epoch 1286, Loss: 1.0159133523702621, Final Batch Loss: 0.20465697348117828\n",
      "Epoch 1287, Loss: 0.9729849398136139, Final Batch Loss: 0.22213856875896454\n",
      "Epoch 1288, Loss: 1.008566901087761, Final Batch Loss: 0.26388758420944214\n",
      "Epoch 1289, Loss: 1.0947066843509674, Final Batch Loss: 0.3215346336364746\n",
      "Epoch 1290, Loss: 1.0357411056756973, Final Batch Loss: 0.2663653790950775\n",
      "Epoch 1291, Loss: 1.1759819090366364, Final Batch Loss: 0.3688608407974243\n",
      "Epoch 1292, Loss: 1.047374963760376, Final Batch Loss: 0.22958765923976898\n",
      "Epoch 1293, Loss: 1.040851891040802, Final Batch Loss: 0.2516229450702667\n",
      "Epoch 1294, Loss: 0.9471830576658249, Final Batch Loss: 0.1753268986940384\n",
      "Epoch 1295, Loss: 1.0779538452625275, Final Batch Loss: 0.3112718462944031\n",
      "Epoch 1296, Loss: 1.050443559885025, Final Batch Loss: 0.27508699893951416\n",
      "Epoch 1297, Loss: 1.1059262156486511, Final Batch Loss: 0.2577732801437378\n",
      "Epoch 1298, Loss: 1.1497707515954971, Final Batch Loss: 0.33220237493515015\n",
      "Epoch 1299, Loss: 1.1450147926807404, Final Batch Loss: 0.3225247859954834\n",
      "Epoch 1300, Loss: 1.0536364912986755, Final Batch Loss: 0.2989688515663147\n",
      "Epoch 1301, Loss: 1.086229294538498, Final Batch Loss: 0.24613720178604126\n",
      "Epoch 1302, Loss: 1.0440082550048828, Final Batch Loss: 0.20060890913009644\n",
      "Epoch 1303, Loss: 1.045941337943077, Final Batch Loss: 0.20522458851337433\n",
      "Epoch 1304, Loss: 1.0356114655733109, Final Batch Loss: 0.23856456577777863\n",
      "Epoch 1305, Loss: 1.0433223843574524, Final Batch Loss: 0.22319762408733368\n",
      "Epoch 1306, Loss: 1.08774234354496, Final Batch Loss: 0.3438608646392822\n",
      "Epoch 1307, Loss: 1.056160345673561, Final Batch Loss: 0.2505331039428711\n",
      "Epoch 1308, Loss: 1.057580143213272, Final Batch Loss: 0.21479544043540955\n",
      "Epoch 1309, Loss: 1.1072909235954285, Final Batch Loss: 0.2931830883026123\n",
      "Epoch 1310, Loss: 1.0973576307296753, Final Batch Loss: 0.25694385170936584\n",
      "Epoch 1311, Loss: 1.0488896816968918, Final Batch Loss: 0.30029597878456116\n",
      "Epoch 1312, Loss: 1.0860944390296936, Final Batch Loss: 0.3026043474674225\n",
      "Epoch 1313, Loss: 1.0163245350122452, Final Batch Loss: 0.24460390210151672\n",
      "Epoch 1314, Loss: 1.0544840842485428, Final Batch Loss: 0.2638634443283081\n",
      "Epoch 1315, Loss: 1.0719057619571686, Final Batch Loss: 0.3116725981235504\n",
      "Epoch 1316, Loss: 1.0497532784938812, Final Batch Loss: 0.2764635384082794\n",
      "Epoch 1317, Loss: 1.0581080615520477, Final Batch Loss: 0.2877524197101593\n",
      "Epoch 1318, Loss: 1.110810935497284, Final Batch Loss: 0.2713560461997986\n",
      "Epoch 1319, Loss: 0.966189444065094, Final Batch Loss: 0.1916661560535431\n",
      "Epoch 1320, Loss: 1.1174142211675644, Final Batch Loss: 0.3331395983695984\n",
      "Epoch 1321, Loss: 1.0050213187932968, Final Batch Loss: 0.2576906681060791\n",
      "Epoch 1322, Loss: 1.0668856054544449, Final Batch Loss: 0.2404634654521942\n",
      "Epoch 1323, Loss: 1.0306792110204697, Final Batch Loss: 0.26334503293037415\n",
      "Epoch 1324, Loss: 1.0990906059741974, Final Batch Loss: 0.2778051495552063\n",
      "Epoch 1325, Loss: 1.034992441534996, Final Batch Loss: 0.16555960476398468\n",
      "Epoch 1326, Loss: 1.0320944041013718, Final Batch Loss: 0.23373976349830627\n",
      "Epoch 1327, Loss: 1.1018934696912766, Final Batch Loss: 0.33514273166656494\n",
      "Epoch 1328, Loss: 1.0766073614358902, Final Batch Loss: 0.26432204246520996\n",
      "Epoch 1329, Loss: 1.0971735119819641, Final Batch Loss: 0.21013444662094116\n",
      "Epoch 1330, Loss: 1.0333929508924484, Final Batch Loss: 0.24988488852977753\n",
      "Epoch 1331, Loss: 1.1414500772953033, Final Batch Loss: 0.3562183678150177\n",
      "Epoch 1332, Loss: 1.1905843019485474, Final Batch Loss: 0.33276402950286865\n",
      "Epoch 1333, Loss: 1.200533166527748, Final Batch Loss: 0.39026808738708496\n",
      "Epoch 1334, Loss: 1.1077975928783417, Final Batch Loss: 0.3391454517841339\n",
      "Epoch 1335, Loss: 0.9951317012310028, Final Batch Loss: 0.21814130246639252\n",
      "Epoch 1336, Loss: 1.071721076965332, Final Batch Loss: 0.23118926584720612\n",
      "Epoch 1337, Loss: 1.0708846151828766, Final Batch Loss: 0.2522459626197815\n",
      "Epoch 1338, Loss: 1.0290852636098862, Final Batch Loss: 0.1957072764635086\n",
      "Epoch 1339, Loss: 1.0800972878932953, Final Batch Loss: 0.2986561357975006\n",
      "Epoch 1340, Loss: 1.0735418945550919, Final Batch Loss: 0.2993503510951996\n",
      "Epoch 1341, Loss: 1.082387536764145, Final Batch Loss: 0.2941338121891022\n",
      "Epoch 1342, Loss: 1.0153444856405258, Final Batch Loss: 0.2174699604511261\n",
      "Epoch 1343, Loss: 1.148866593837738, Final Batch Loss: 0.3044757544994354\n",
      "Epoch 1344, Loss: 1.130627691745758, Final Batch Loss: 0.2896273136138916\n",
      "Epoch 1345, Loss: 1.1816875636577606, Final Batch Loss: 0.36867499351501465\n",
      "Epoch 1346, Loss: 1.0882615745067596, Final Batch Loss: 0.27467647194862366\n",
      "Epoch 1347, Loss: 1.0116995126008987, Final Batch Loss: 0.2643485963344574\n",
      "Epoch 1348, Loss: 1.0660394132137299, Final Batch Loss: 0.243737131357193\n",
      "Epoch 1349, Loss: 1.077129602432251, Final Batch Loss: 0.23428308963775635\n",
      "Epoch 1350, Loss: 1.0129664093255997, Final Batch Loss: 0.23714205622673035\n",
      "Epoch 1351, Loss: 1.1065678149461746, Final Batch Loss: 0.2719379961490631\n",
      "Epoch 1352, Loss: 1.1111881732940674, Final Batch Loss: 0.3125959038734436\n",
      "Epoch 1353, Loss: 1.1121766716241837, Final Batch Loss: 0.24207721650600433\n",
      "Epoch 1354, Loss: 1.0692750215530396, Final Batch Loss: 0.2701329290866852\n",
      "Epoch 1355, Loss: 1.1179021298885345, Final Batch Loss: 0.2791498601436615\n",
      "Epoch 1356, Loss: 1.0541406273841858, Final Batch Loss: 0.27762553095817566\n",
      "Epoch 1357, Loss: 1.1830697357654572, Final Batch Loss: 0.3457823395729065\n",
      "Epoch 1358, Loss: 1.0130334198474884, Final Batch Loss: 0.2022465467453003\n",
      "Epoch 1359, Loss: 1.0184750109910965, Final Batch Loss: 0.22565729916095734\n",
      "Epoch 1360, Loss: 1.241656094789505, Final Batch Loss: 0.43580538034439087\n",
      "Epoch 1361, Loss: 1.0578656494617462, Final Batch Loss: 0.26956942677497864\n",
      "Epoch 1362, Loss: 1.188174307346344, Final Batch Loss: 0.3589557111263275\n",
      "Epoch 1363, Loss: 1.1293023228645325, Final Batch Loss: 0.3129325211048126\n",
      "Epoch 1364, Loss: 1.0704719424247742, Final Batch Loss: 0.2072986364364624\n",
      "Epoch 1365, Loss: 1.1508006751537323, Final Batch Loss: 0.292680948972702\n",
      "Epoch 1366, Loss: 1.1151608675718307, Final Batch Loss: 0.3682015836238861\n",
      "Epoch 1367, Loss: 1.0668373554944992, Final Batch Loss: 0.21711179614067078\n",
      "Epoch 1368, Loss: 0.9924034774303436, Final Batch Loss: 0.20149627327919006\n",
      "Epoch 1369, Loss: 0.9694548398256302, Final Batch Loss: 0.2026185840368271\n",
      "Epoch 1370, Loss: 1.0938495695590973, Final Batch Loss: 0.2514118552207947\n",
      "Epoch 1371, Loss: 1.026727169752121, Final Batch Loss: 0.31769612431526184\n",
      "Epoch 1372, Loss: 1.021711453795433, Final Batch Loss: 0.1979101002216339\n",
      "Epoch 1373, Loss: 1.070202812552452, Final Batch Loss: 0.31430473923683167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1374, Loss: 1.0385285466909409, Final Batch Loss: 0.19609442353248596\n",
      "Epoch 1375, Loss: 1.0432450473308563, Final Batch Loss: 0.2675468325614929\n",
      "Epoch 1376, Loss: 1.0660244971513748, Final Batch Loss: 0.24469251930713654\n",
      "Epoch 1377, Loss: 1.068865180015564, Final Batch Loss: 0.26788607239723206\n",
      "Epoch 1378, Loss: 1.010272353887558, Final Batch Loss: 0.15721495449543\n",
      "Epoch 1379, Loss: 0.9981371462345123, Final Batch Loss: 0.17463894188404083\n",
      "Epoch 1380, Loss: 1.0220645815134048, Final Batch Loss: 0.21520407497882843\n",
      "Epoch 1381, Loss: 1.1111096739768982, Final Batch Loss: 0.3258749842643738\n",
      "Epoch 1382, Loss: 1.059246003627777, Final Batch Loss: 0.2800379693508148\n",
      "Epoch 1383, Loss: 1.0876969695091248, Final Batch Loss: 0.311942458152771\n",
      "Epoch 1384, Loss: 1.0807802230119705, Final Batch Loss: 0.30916479229927063\n",
      "Epoch 1385, Loss: 1.0506843626499176, Final Batch Loss: 0.2876826822757721\n",
      "Epoch 1386, Loss: 1.1259477138519287, Final Batch Loss: 0.36470484733581543\n",
      "Epoch 1387, Loss: 1.122212529182434, Final Batch Loss: 0.27154049277305603\n",
      "Epoch 1388, Loss: 1.090191900730133, Final Batch Loss: 0.29460373520851135\n",
      "Epoch 1389, Loss: 1.01455357670784, Final Batch Loss: 0.2343095988035202\n",
      "Epoch 1390, Loss: 1.047808513045311, Final Batch Loss: 0.29048094153404236\n",
      "Epoch 1391, Loss: 1.0478603839874268, Final Batch Loss: 0.2477755844593048\n",
      "Epoch 1392, Loss: 1.0724912881851196, Final Batch Loss: 0.30582934617996216\n",
      "Epoch 1393, Loss: 1.105569064617157, Final Batch Loss: 0.3517245948314667\n",
      "Epoch 1394, Loss: 1.0791069716215134, Final Batch Loss: 0.2776835560798645\n",
      "Epoch 1395, Loss: 1.0849249213933945, Final Batch Loss: 0.2844998240470886\n",
      "Epoch 1396, Loss: 1.0431193262338638, Final Batch Loss: 0.19801883399486542\n",
      "Epoch 1397, Loss: 1.09348863363266, Final Batch Loss: 0.2896660566329956\n",
      "Epoch 1398, Loss: 1.0700722336769104, Final Batch Loss: 0.3645387589931488\n",
      "Epoch 1399, Loss: 1.1372063010931015, Final Batch Loss: 0.31164926290512085\n",
      "Epoch 1400, Loss: 1.0202085226774216, Final Batch Loss: 0.24130862951278687\n",
      "Epoch 1401, Loss: 1.0449117571115494, Final Batch Loss: 0.2665870487689972\n",
      "Epoch 1402, Loss: 1.132275030016899, Final Batch Loss: 0.31717243790626526\n",
      "Epoch 1403, Loss: 1.1093221008777618, Final Batch Loss: 0.33097782731056213\n",
      "Epoch 1404, Loss: 1.0181929916143417, Final Batch Loss: 0.22384291887283325\n",
      "Epoch 1405, Loss: 1.042734444141388, Final Batch Loss: 0.24779391288757324\n",
      "Epoch 1406, Loss: 1.0657608211040497, Final Batch Loss: 0.21822649240493774\n",
      "Epoch 1407, Loss: 1.0065833628177643, Final Batch Loss: 0.2344113290309906\n",
      "Epoch 1408, Loss: 1.0555201470851898, Final Batch Loss: 0.2526581287384033\n",
      "Epoch 1409, Loss: 1.0231047719717026, Final Batch Loss: 0.26109862327575684\n",
      "Epoch 1410, Loss: 1.1392106115818024, Final Batch Loss: 0.32731735706329346\n",
      "Epoch 1411, Loss: 1.0310700684785843, Final Batch Loss: 0.2677396535873413\n",
      "Epoch 1412, Loss: 1.0275537818670273, Final Batch Loss: 0.21820460259914398\n",
      "Epoch 1413, Loss: 1.0558270364999771, Final Batch Loss: 0.2782440483570099\n",
      "Epoch 1414, Loss: 1.0750520080327988, Final Batch Loss: 0.3062494993209839\n",
      "Epoch 1415, Loss: 0.9954172223806381, Final Batch Loss: 0.2203037589788437\n",
      "Epoch 1416, Loss: 1.02571801841259, Final Batch Loss: 0.2908652126789093\n",
      "Epoch 1417, Loss: 1.0673238933086395, Final Batch Loss: 0.28276774287223816\n",
      "Epoch 1418, Loss: 1.0274535715579987, Final Batch Loss: 0.2120853215456009\n",
      "Epoch 1419, Loss: 1.0920699685811996, Final Batch Loss: 0.3275206983089447\n",
      "Epoch 1420, Loss: 1.0254738628864288, Final Batch Loss: 0.23156793415546417\n",
      "Epoch 1421, Loss: 1.0603816360235214, Final Batch Loss: 0.30618664622306824\n",
      "Epoch 1422, Loss: 1.0302716493606567, Final Batch Loss: 0.2384376972913742\n",
      "Epoch 1423, Loss: 0.9862924814224243, Final Batch Loss: 0.15768417716026306\n",
      "Epoch 1424, Loss: 0.9568454772233963, Final Batch Loss: 0.1561434417963028\n",
      "Epoch 1425, Loss: 1.1133282780647278, Final Batch Loss: 0.2558281719684601\n",
      "Epoch 1426, Loss: 1.0302020013332367, Final Batch Loss: 0.19561119377613068\n",
      "Epoch 1427, Loss: 1.1173171401023865, Final Batch Loss: 0.33819901943206787\n",
      "Epoch 1428, Loss: 1.0844424217939377, Final Batch Loss: 0.32386529445648193\n",
      "Epoch 1429, Loss: 1.069126933813095, Final Batch Loss: 0.33158671855926514\n",
      "Epoch 1430, Loss: 1.1139309704303741, Final Batch Loss: 0.3202928900718689\n",
      "Epoch 1431, Loss: 1.1103792488574982, Final Batch Loss: 0.3011099100112915\n",
      "Epoch 1432, Loss: 1.0525903701782227, Final Batch Loss: 0.24766328930854797\n",
      "Epoch 1433, Loss: 1.1626303493976593, Final Batch Loss: 0.362697571516037\n",
      "Epoch 1434, Loss: 1.1689282357692719, Final Batch Loss: 0.329039990901947\n",
      "Epoch 1435, Loss: 1.0729770958423615, Final Batch Loss: 0.2787347733974457\n",
      "Epoch 1436, Loss: 1.0441799312829971, Final Batch Loss: 0.23642225563526154\n",
      "Epoch 1437, Loss: 1.043522447347641, Final Batch Loss: 0.27520138025283813\n",
      "Epoch 1438, Loss: 1.0500057190656662, Final Batch Loss: 0.2719409167766571\n",
      "Epoch 1439, Loss: 1.103873386979103, Final Batch Loss: 0.25745296478271484\n",
      "Epoch 1440, Loss: 1.0541242361068726, Final Batch Loss: 0.2529062330722809\n",
      "Epoch 1441, Loss: 1.1739543676376343, Final Batch Loss: 0.3878532946109772\n",
      "Epoch 1442, Loss: 1.0565758645534515, Final Batch Loss: 0.3332943916320801\n",
      "Epoch 1443, Loss: 1.1175837367773056, Final Batch Loss: 0.28989094495773315\n",
      "Epoch 1444, Loss: 1.096948266029358, Final Batch Loss: 0.3220645487308502\n",
      "Epoch 1445, Loss: 1.1125731468200684, Final Batch Loss: 0.25269949436187744\n",
      "Epoch 1446, Loss: 1.0198899507522583, Final Batch Loss: 0.22693376243114471\n",
      "Epoch 1447, Loss: 1.0546802580356598, Final Batch Loss: 0.24367734789848328\n",
      "Epoch 1448, Loss: 1.0641050338745117, Final Batch Loss: 0.3081988990306854\n",
      "Epoch 1449, Loss: 1.1260507106781006, Final Batch Loss: 0.3251461684703827\n",
      "Epoch 1450, Loss: 1.0614678114652634, Final Batch Loss: 0.2349054366350174\n",
      "Epoch 1451, Loss: 1.1231681853532791, Final Batch Loss: 0.36670467257499695\n",
      "Epoch 1452, Loss: 0.9769317060709, Final Batch Loss: 0.20870280265808105\n",
      "Epoch 1453, Loss: 0.9470227658748627, Final Batch Loss: 0.17193658649921417\n",
      "Epoch 1454, Loss: 1.077158346772194, Final Batch Loss: 0.2925988435745239\n",
      "Epoch 1455, Loss: 1.0966099053621292, Final Batch Loss: 0.3388708829879761\n",
      "Epoch 1456, Loss: 1.0559908747673035, Final Batch Loss: 0.3717038929462433\n",
      "Epoch 1457, Loss: 1.1053105890750885, Final Batch Loss: 0.32939469814300537\n",
      "Epoch 1458, Loss: 1.0318667888641357, Final Batch Loss: 0.24306610226631165\n",
      "Epoch 1459, Loss: 1.0914494395256042, Final Batch Loss: 0.3176313042640686\n",
      "Epoch 1460, Loss: 0.954193577170372, Final Batch Loss: 0.19537517428398132\n",
      "Epoch 1461, Loss: 1.130955621600151, Final Batch Loss: 0.3368909955024719\n",
      "Epoch 1462, Loss: 1.0583004802465439, Final Batch Loss: 0.25109708309173584\n",
      "Epoch 1463, Loss: 1.0400751233100891, Final Batch Loss: 0.2859729528427124\n",
      "Epoch 1464, Loss: 1.0426336973905563, Final Batch Loss: 0.22456766664981842\n",
      "Epoch 1465, Loss: 1.0480746030807495, Final Batch Loss: 0.2515963017940521\n",
      "Epoch 1466, Loss: 1.034242331981659, Final Batch Loss: 0.23014703392982483\n",
      "Epoch 1467, Loss: 1.050527572631836, Final Batch Loss: 0.29705050587654114\n",
      "Epoch 1468, Loss: 0.9912284463644028, Final Batch Loss: 0.2199154794216156\n",
      "Epoch 1469, Loss: 1.0055809617042542, Final Batch Loss: 0.2296009510755539\n",
      "Epoch 1470, Loss: 1.0235787630081177, Final Batch Loss: 0.26666998863220215\n",
      "Epoch 1471, Loss: 1.0441454499959946, Final Batch Loss: 0.21193890273571014\n",
      "Epoch 1472, Loss: 1.0134125500917435, Final Batch Loss: 0.23819126188755035\n",
      "Epoch 1473, Loss: 0.997848629951477, Final Batch Loss: 0.2171154022216797\n",
      "Epoch 1474, Loss: 1.0333616435527802, Final Batch Loss: 0.27861860394477844\n",
      "Epoch 1475, Loss: 1.0944805145263672, Final Batch Loss: 0.2649705111980438\n",
      "Epoch 1476, Loss: 1.0037268996238708, Final Batch Loss: 0.20774631202220917\n",
      "Epoch 1477, Loss: 1.0178232192993164, Final Batch Loss: 0.2342005968093872\n",
      "Epoch 1478, Loss: 0.9883872270584106, Final Batch Loss: 0.21615058183670044\n",
      "Epoch 1479, Loss: 1.0243780314922333, Final Batch Loss: 0.2750210762023926\n",
      "Epoch 1480, Loss: 1.1090391874313354, Final Batch Loss: 0.3059123158454895\n",
      "Epoch 1481, Loss: 1.0844862014055252, Final Batch Loss: 0.26367345452308655\n",
      "Epoch 1482, Loss: 1.0642209649085999, Final Batch Loss: 0.2954604923725128\n",
      "Epoch 1483, Loss: 1.1197595000267029, Final Batch Loss: 0.2770405411720276\n",
      "Epoch 1484, Loss: 1.1456346064805984, Final Batch Loss: 0.34339639544487\n",
      "Epoch 1485, Loss: 1.021719828248024, Final Batch Loss: 0.16581009328365326\n",
      "Epoch 1486, Loss: 1.047540470957756, Final Batch Loss: 0.2516919672489166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1487, Loss: 0.9805580675601959, Final Batch Loss: 0.19218891859054565\n",
      "Epoch 1488, Loss: 1.0860118716955185, Final Batch Loss: 0.2815313935279846\n",
      "Epoch 1489, Loss: 1.0574417561292648, Final Batch Loss: 0.2902018427848816\n",
      "Epoch 1490, Loss: 1.017825186252594, Final Batch Loss: 0.24576857686042786\n",
      "Epoch 1491, Loss: 1.0659172534942627, Final Batch Loss: 0.2764914035797119\n",
      "Epoch 1492, Loss: 1.075501188635826, Final Batch Loss: 0.3137766420841217\n",
      "Epoch 1493, Loss: 1.1178123652935028, Final Batch Loss: 0.36113667488098145\n",
      "Epoch 1494, Loss: 1.0865544229745865, Final Batch Loss: 0.31122007966041565\n",
      "Epoch 1495, Loss: 1.0975139141082764, Final Batch Loss: 0.27146703004837036\n",
      "Epoch 1496, Loss: 1.055167555809021, Final Batch Loss: 0.29498329758644104\n",
      "Epoch 1497, Loss: 0.9642763584852219, Final Batch Loss: 0.14914511144161224\n",
      "Epoch 1498, Loss: 1.0538176149129868, Final Batch Loss: 0.24284619092941284\n",
      "Epoch 1499, Loss: 1.1132310330867767, Final Batch Loss: 0.3220241367816925\n",
      "Epoch 1500, Loss: 1.050497144460678, Final Batch Loss: 0.22531740367412567\n",
      "Epoch 1501, Loss: 1.0463005155324936, Final Batch Loss: 0.24781642854213715\n",
      "Epoch 1502, Loss: 1.0213088393211365, Final Batch Loss: 0.2593121826648712\n",
      "Epoch 1503, Loss: 1.0436987578868866, Final Batch Loss: 0.3331276476383209\n",
      "Epoch 1504, Loss: 1.0745285898447037, Final Batch Loss: 0.2272339016199112\n",
      "Epoch 1505, Loss: 1.0857096463441849, Final Batch Loss: 0.3001199960708618\n",
      "Epoch 1506, Loss: 1.0406023412942886, Final Batch Loss: 0.22417020797729492\n",
      "Epoch 1507, Loss: 1.0156231373548508, Final Batch Loss: 0.17352594435214996\n",
      "Epoch 1508, Loss: 1.057107925415039, Final Batch Loss: 0.29211461544036865\n",
      "Epoch 1509, Loss: 1.05155748128891, Final Batch Loss: 0.1696096956729889\n",
      "Epoch 1510, Loss: 1.132894903421402, Final Batch Loss: 0.34069129824638367\n",
      "Epoch 1511, Loss: 1.0758768320083618, Final Batch Loss: 0.32623985409736633\n",
      "Epoch 1512, Loss: 1.1255245059728622, Final Batch Loss: 0.37665441632270813\n",
      "Epoch 1513, Loss: 1.0576266199350357, Final Batch Loss: 0.22927618026733398\n",
      "Epoch 1514, Loss: 1.0352096408605576, Final Batch Loss: 0.17088593542575836\n",
      "Epoch 1515, Loss: 1.0292795896530151, Final Batch Loss: 0.22061078250408173\n",
      "Epoch 1516, Loss: 1.1541056782007217, Final Batch Loss: 0.35365864634513855\n",
      "Epoch 1517, Loss: 1.053070291876793, Final Batch Loss: 0.2909550964832306\n",
      "Epoch 1518, Loss: 1.071214810013771, Final Batch Loss: 0.30945897102355957\n",
      "Epoch 1519, Loss: 0.9826654195785522, Final Batch Loss: 0.1816723644733429\n",
      "Epoch 1520, Loss: 0.9433849006891251, Final Batch Loss: 0.1725717931985855\n",
      "Epoch 1521, Loss: 1.08869469165802, Final Batch Loss: 0.33994001150131226\n",
      "Epoch 1522, Loss: 1.074589103460312, Final Batch Loss: 0.20902876555919647\n",
      "Epoch 1523, Loss: 0.9879283457994461, Final Batch Loss: 0.24623939394950867\n",
      "Epoch 1524, Loss: 1.0320331156253815, Final Batch Loss: 0.2393275648355484\n",
      "Epoch 1525, Loss: 0.9584819674491882, Final Batch Loss: 0.2289726287126541\n",
      "Epoch 1526, Loss: 1.0977658331394196, Final Batch Loss: 0.3574531376361847\n",
      "Epoch 1527, Loss: 0.9849888235330582, Final Batch Loss: 0.1961439847946167\n",
      "Epoch 1528, Loss: 1.0683329403400421, Final Batch Loss: 0.2499987781047821\n",
      "Epoch 1529, Loss: 1.119799181818962, Final Batch Loss: 0.3433062732219696\n",
      "Epoch 1530, Loss: 1.11020028591156, Final Batch Loss: 0.3126124441623688\n",
      "Epoch 1531, Loss: 1.0459716320037842, Final Batch Loss: 0.32409757375717163\n",
      "Epoch 1532, Loss: 1.0161041617393494, Final Batch Loss: 0.24123553931713104\n",
      "Epoch 1533, Loss: 1.0178845822811127, Final Batch Loss: 0.24189944565296173\n",
      "Epoch 1534, Loss: 1.1025155931711197, Final Batch Loss: 0.31714287400245667\n",
      "Epoch 1535, Loss: 0.9965663701295853, Final Batch Loss: 0.24291843175888062\n",
      "Epoch 1536, Loss: 1.1098830997943878, Final Batch Loss: 0.27871575951576233\n",
      "Epoch 1537, Loss: 1.0613490492105484, Final Batch Loss: 0.2687660753726959\n",
      "Epoch 1538, Loss: 1.083969622850418, Final Batch Loss: 0.2516961991786957\n",
      "Epoch 1539, Loss: 1.0695245265960693, Final Batch Loss: 0.2786775827407837\n",
      "Epoch 1540, Loss: 1.0007134228944778, Final Batch Loss: 0.19116614758968353\n",
      "Epoch 1541, Loss: 1.0467979460954666, Final Batch Loss: 0.28848156332969666\n",
      "Epoch 1542, Loss: 1.0146536082029343, Final Batch Loss: 0.2713831067085266\n",
      "Epoch 1543, Loss: 1.1107188910245895, Final Batch Loss: 0.3061634600162506\n",
      "Epoch 1544, Loss: 1.0182581841945648, Final Batch Loss: 0.27224013209342957\n",
      "Epoch 1545, Loss: 1.083258256316185, Final Batch Loss: 0.34204578399658203\n",
      "Epoch 1546, Loss: 0.9612666070461273, Final Batch Loss: 0.13403236865997314\n",
      "Epoch 1547, Loss: 1.0306624472141266, Final Batch Loss: 0.27042725682258606\n",
      "Epoch 1548, Loss: 1.0784777700901031, Final Batch Loss: 0.26069024205207825\n",
      "Epoch 1549, Loss: 1.0554880201816559, Final Batch Loss: 0.25987163186073303\n",
      "Epoch 1550, Loss: 1.0443351715803146, Final Batch Loss: 0.26615265011787415\n",
      "Epoch 1551, Loss: 1.1661194860935211, Final Batch Loss: 0.3540368676185608\n",
      "Epoch 1552, Loss: 1.0983018726110458, Final Batch Loss: 0.33362242579460144\n",
      "Epoch 1553, Loss: 1.031662940979004, Final Batch Loss: 0.24058403074741364\n",
      "Epoch 1554, Loss: 1.0372630506753922, Final Batch Loss: 0.2978762686252594\n",
      "Epoch 1555, Loss: 0.9928190559148788, Final Batch Loss: 0.23071901500225067\n",
      "Epoch 1556, Loss: 1.053536057472229, Final Batch Loss: 0.23202168941497803\n",
      "Epoch 1557, Loss: 1.0724085122346878, Final Batch Loss: 0.3599025309085846\n",
      "Epoch 1558, Loss: 1.0255983620882034, Final Batch Loss: 0.2454959601163864\n",
      "Epoch 1559, Loss: 1.0752640962600708, Final Batch Loss: 0.3074944317340851\n",
      "Epoch 1560, Loss: 1.0372643023729324, Final Batch Loss: 0.27364251017570496\n",
      "Epoch 1561, Loss: 1.0452783405780792, Final Batch Loss: 0.2751685678958893\n",
      "Epoch 1562, Loss: 1.0681999921798706, Final Batch Loss: 0.25305354595184326\n",
      "Epoch 1563, Loss: 1.0387022793293, Final Batch Loss: 0.25488072633743286\n",
      "Epoch 1564, Loss: 0.9609150290489197, Final Batch Loss: 0.20003189146518707\n",
      "Epoch 1565, Loss: 1.0604802519083023, Final Batch Loss: 0.22333888709545135\n",
      "Epoch 1566, Loss: 1.0288380831480026, Final Batch Loss: 0.23192989826202393\n",
      "Epoch 1567, Loss: 1.0130377113819122, Final Batch Loss: 0.19019058346748352\n",
      "Epoch 1568, Loss: 0.9956512004137039, Final Batch Loss: 0.21125592291355133\n",
      "Epoch 1569, Loss: 1.0839954763650894, Final Batch Loss: 0.351765513420105\n",
      "Epoch 1570, Loss: 1.0847881585359573, Final Batch Loss: 0.3029436469078064\n",
      "Epoch 1571, Loss: 1.0341813266277313, Final Batch Loss: 0.2781715989112854\n",
      "Epoch 1572, Loss: 1.115809440612793, Final Batch Loss: 0.27935123443603516\n",
      "Epoch 1573, Loss: 1.1242393404245377, Final Batch Loss: 0.3434574007987976\n",
      "Epoch 1574, Loss: 1.0414841324090958, Final Batch Loss: 0.2570748031139374\n",
      "Epoch 1575, Loss: 1.0264787673950195, Final Batch Loss: 0.24723505973815918\n",
      "Epoch 1576, Loss: 1.1107543408870697, Final Batch Loss: 0.34029051661491394\n",
      "Epoch 1577, Loss: 1.051808848977089, Final Batch Loss: 0.2972005009651184\n",
      "Epoch 1578, Loss: 1.0508254617452621, Final Batch Loss: 0.2588992714881897\n",
      "Epoch 1579, Loss: 0.9832602739334106, Final Batch Loss: 0.20625099539756775\n",
      "Epoch 1580, Loss: 1.0991176664829254, Final Batch Loss: 0.2942185401916504\n",
      "Epoch 1581, Loss: 0.981910452246666, Final Batch Loss: 0.19733156263828278\n",
      "Epoch 1582, Loss: 1.1052003800868988, Final Batch Loss: 0.3521198332309723\n",
      "Epoch 1583, Loss: 1.1134603917598724, Final Batch Loss: 0.34113189578056335\n",
      "Epoch 1584, Loss: 0.9816895127296448, Final Batch Loss: 0.2406606525182724\n",
      "Epoch 1585, Loss: 1.1181072890758514, Final Batch Loss: 0.27779731154441833\n",
      "Epoch 1586, Loss: 1.0397247523069382, Final Batch Loss: 0.22982670366764069\n",
      "Epoch 1587, Loss: 1.223588913679123, Final Batch Loss: 0.4709172546863556\n",
      "Epoch 1588, Loss: 1.015172466635704, Final Batch Loss: 0.1941653937101364\n",
      "Epoch 1589, Loss: 1.0246665924787521, Final Batch Loss: 0.25075405836105347\n",
      "Epoch 1590, Loss: 1.002388373017311, Final Batch Loss: 0.19866974651813507\n",
      "Epoch 1591, Loss: 1.016306221485138, Final Batch Loss: 0.19605614244937897\n",
      "Epoch 1592, Loss: 1.0093927830457687, Final Batch Loss: 0.20926763117313385\n",
      "Epoch 1593, Loss: 0.965027466416359, Final Batch Loss: 0.182758167386055\n",
      "Epoch 1594, Loss: 1.0230385214090347, Final Batch Loss: 0.24280701577663422\n",
      "Epoch 1595, Loss: 1.06015844643116, Final Batch Loss: 0.31573978066444397\n",
      "Epoch 1596, Loss: 1.0436688959598541, Final Batch Loss: 0.2521064579486847\n",
      "Epoch 1597, Loss: 1.001030683517456, Final Batch Loss: 0.21349462866783142\n",
      "Epoch 1598, Loss: 1.0553034096956253, Final Batch Loss: 0.2652553915977478\n",
      "Epoch 1599, Loss: 0.9666490107774734, Final Batch Loss: 0.16102828085422516\n",
      "Epoch 1600, Loss: 0.9768595695495605, Final Batch Loss: 0.1987733095884323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1601, Loss: 1.0090856552124023, Final Batch Loss: 0.20783624053001404\n",
      "Epoch 1602, Loss: 1.0232750624418259, Final Batch Loss: 0.2602975070476532\n",
      "Epoch 1603, Loss: 1.0345234274864197, Final Batch Loss: 0.27984583377838135\n",
      "Epoch 1604, Loss: 1.1253546476364136, Final Batch Loss: 0.35580313205718994\n",
      "Epoch 1605, Loss: 0.9812155514955521, Final Batch Loss: 0.2125370353460312\n",
      "Epoch 1606, Loss: 1.0041429549455643, Final Batch Loss: 0.19123820960521698\n",
      "Epoch 1607, Loss: 1.0656948238611221, Final Batch Loss: 0.30619823932647705\n",
      "Epoch 1608, Loss: 1.0055919736623764, Final Batch Loss: 0.23230956494808197\n",
      "Epoch 1609, Loss: 1.0055598467588425, Final Batch Loss: 0.2280150204896927\n",
      "Epoch 1610, Loss: 1.0774245709180832, Final Batch Loss: 0.29146724939346313\n",
      "Epoch 1611, Loss: 1.0240646749734879, Final Batch Loss: 0.24576763808727264\n",
      "Epoch 1612, Loss: 1.067541480064392, Final Batch Loss: 0.24541142582893372\n",
      "Epoch 1613, Loss: 1.0988489985466003, Final Batch Loss: 0.35530367493629456\n",
      "Epoch 1614, Loss: 0.9949458837509155, Final Batch Loss: 0.15226507186889648\n",
      "Epoch 1615, Loss: 1.037546768784523, Final Batch Loss: 0.26589301228523254\n",
      "Epoch 1616, Loss: 0.9777592420578003, Final Batch Loss: 0.2121567577123642\n",
      "Epoch 1617, Loss: 0.9960635453462601, Final Batch Loss: 0.14282740652561188\n",
      "Epoch 1618, Loss: 1.0664945393800735, Final Batch Loss: 0.2603394389152527\n",
      "Epoch 1619, Loss: 1.0219527184963226, Final Batch Loss: 0.24171164631843567\n",
      "Epoch 1620, Loss: 0.9949887543916702, Final Batch Loss: 0.26033008098602295\n",
      "Epoch 1621, Loss: 1.0342346876859665, Final Batch Loss: 0.29432061314582825\n",
      "Epoch 1622, Loss: 1.0057087689638138, Final Batch Loss: 0.2494661509990692\n",
      "Epoch 1623, Loss: 1.1197501868009567, Final Batch Loss: 0.3255535662174225\n",
      "Epoch 1624, Loss: 1.049118533730507, Final Batch Loss: 0.19614364206790924\n",
      "Epoch 1625, Loss: 1.0468695163726807, Final Batch Loss: 0.2439301311969757\n",
      "Epoch 1626, Loss: 1.0468652695417404, Final Batch Loss: 0.3221510946750641\n",
      "Epoch 1627, Loss: 1.1386995613574982, Final Batch Loss: 0.3165476322174072\n",
      "Epoch 1628, Loss: 1.1498737186193466, Final Batch Loss: 0.33897820115089417\n",
      "Epoch 1629, Loss: 1.0951901972293854, Final Batch Loss: 0.1429290771484375\n",
      "Epoch 1630, Loss: 1.0810249000787735, Final Batch Loss: 0.24765242636203766\n",
      "Epoch 1631, Loss: 1.088844746351242, Final Batch Loss: 0.2609129250049591\n",
      "Epoch 1632, Loss: 1.1456228494644165, Final Batch Loss: 0.3634282946586609\n",
      "Epoch 1633, Loss: 1.1887142956256866, Final Batch Loss: 0.31204402446746826\n",
      "Epoch 1634, Loss: 1.1100630462169647, Final Batch Loss: 0.30431056022644043\n",
      "Epoch 1635, Loss: 1.0284280627965927, Final Batch Loss: 0.24135294556617737\n",
      "Epoch 1636, Loss: 1.019513577222824, Final Batch Loss: 0.2515158951282501\n",
      "Epoch 1637, Loss: 1.0016816705465317, Final Batch Loss: 0.23986120522022247\n",
      "Epoch 1638, Loss: 1.0097923129796982, Final Batch Loss: 0.28381404280662537\n",
      "Epoch 1639, Loss: 1.0351729691028595, Final Batch Loss: 0.2386547327041626\n",
      "Epoch 1640, Loss: 1.012520581483841, Final Batch Loss: 0.27210119366645813\n",
      "Epoch 1641, Loss: 1.1345914900302887, Final Batch Loss: 0.23374849557876587\n",
      "Epoch 1642, Loss: 0.9593768566846848, Final Batch Loss: 0.17602168023586273\n",
      "Epoch 1643, Loss: 0.9787200689315796, Final Batch Loss: 0.15346349775791168\n",
      "Epoch 1644, Loss: 1.050974503159523, Final Batch Loss: 0.2415052354335785\n",
      "Epoch 1645, Loss: 1.0208966434001923, Final Batch Loss: 0.25697338581085205\n",
      "Epoch 1646, Loss: 1.0215104222297668, Final Batch Loss: 0.2489185333251953\n",
      "Epoch 1647, Loss: 1.0991554260253906, Final Batch Loss: 0.28878307342529297\n",
      "Epoch 1648, Loss: 0.9918355792760849, Final Batch Loss: 0.23239393532276154\n",
      "Epoch 1649, Loss: 1.028717577457428, Final Batch Loss: 0.26909908652305603\n",
      "Epoch 1650, Loss: 1.0306898802518845, Final Batch Loss: 0.2273234724998474\n",
      "Epoch 1651, Loss: 1.0184953957796097, Final Batch Loss: 0.21224616467952728\n",
      "Epoch 1652, Loss: 0.9753200858831406, Final Batch Loss: 0.23084399104118347\n",
      "Epoch 1653, Loss: 1.0350823402404785, Final Batch Loss: 0.2510718107223511\n",
      "Epoch 1654, Loss: 1.0173162370920181, Final Batch Loss: 0.21794365346431732\n",
      "Epoch 1655, Loss: 0.9816090017557144, Final Batch Loss: 0.15744729340076447\n",
      "Epoch 1656, Loss: 1.0532114505767822, Final Batch Loss: 0.29158228635787964\n",
      "Epoch 1657, Loss: 1.0080466866493225, Final Batch Loss: 0.22693964838981628\n",
      "Epoch 1658, Loss: 1.032869815826416, Final Batch Loss: 0.2566792964935303\n",
      "Epoch 1659, Loss: 1.0741161853075027, Final Batch Loss: 0.3041985332965851\n",
      "Epoch 1660, Loss: 1.0559728145599365, Final Batch Loss: 0.34371840953826904\n",
      "Epoch 1661, Loss: 0.9828772246837616, Final Batch Loss: 0.22478465735912323\n",
      "Epoch 1662, Loss: 1.0885257124900818, Final Batch Loss: 0.25318393111228943\n",
      "Epoch 1663, Loss: 0.9984717071056366, Final Batch Loss: 0.26043105125427246\n",
      "Epoch 1664, Loss: 1.061819538474083, Final Batch Loss: 0.2935071289539337\n",
      "Epoch 1665, Loss: 1.0152435302734375, Final Batch Loss: 0.2993465065956116\n",
      "Epoch 1666, Loss: 1.0576347410678864, Final Batch Loss: 0.2758004665374756\n",
      "Epoch 1667, Loss: 0.9881781339645386, Final Batch Loss: 0.22890478372573853\n",
      "Epoch 1668, Loss: 1.027079313993454, Final Batch Loss: 0.29600778222084045\n",
      "Epoch 1669, Loss: 1.0639098137617111, Final Batch Loss: 0.22268836200237274\n",
      "Epoch 1670, Loss: 1.0155708491802216, Final Batch Loss: 0.2774944305419922\n",
      "Epoch 1671, Loss: 1.0109133571386337, Final Batch Loss: 0.21116696298122406\n",
      "Epoch 1672, Loss: 1.0209615975618362, Final Batch Loss: 0.2795983552932739\n",
      "Epoch 1673, Loss: 0.9891990125179291, Final Batch Loss: 0.23880474269390106\n",
      "Epoch 1674, Loss: 1.0412789583206177, Final Batch Loss: 0.2558668255805969\n",
      "Epoch 1675, Loss: 1.091890498995781, Final Batch Loss: 0.3379698395729065\n",
      "Epoch 1676, Loss: 1.040323257446289, Final Batch Loss: 0.32006755471229553\n",
      "Epoch 1677, Loss: 1.0070111006498337, Final Batch Loss: 0.2110452950000763\n",
      "Epoch 1678, Loss: 1.0090715438127518, Final Batch Loss: 0.20179645717144012\n",
      "Epoch 1679, Loss: 1.032723918557167, Final Batch Loss: 0.2815052568912506\n",
      "Epoch 1680, Loss: 0.9859737306833267, Final Batch Loss: 0.2587345838546753\n",
      "Epoch 1681, Loss: 1.0125723779201508, Final Batch Loss: 0.30766561627388\n",
      "Epoch 1682, Loss: 1.05894635617733, Final Batch Loss: 0.2869560718536377\n",
      "Epoch 1683, Loss: 1.0251499712467194, Final Batch Loss: 0.2800966799259186\n",
      "Epoch 1684, Loss: 1.0241759270429611, Final Batch Loss: 0.2551289200782776\n",
      "Epoch 1685, Loss: 0.9933656603097916, Final Batch Loss: 0.19430053234100342\n",
      "Epoch 1686, Loss: 1.115320235490799, Final Batch Loss: 0.24082934856414795\n",
      "Epoch 1687, Loss: 1.0555256009101868, Final Batch Loss: 0.2822304368019104\n",
      "Epoch 1688, Loss: 1.0764624327421188, Final Batch Loss: 0.34393927454948425\n",
      "Epoch 1689, Loss: 1.0006365478038788, Final Batch Loss: 0.28323593735694885\n",
      "Epoch 1690, Loss: 1.0118034034967422, Final Batch Loss: 0.20716498792171478\n",
      "Epoch 1691, Loss: 1.0188166797161102, Final Batch Loss: 0.2340405434370041\n",
      "Epoch 1692, Loss: 0.9959330409765244, Final Batch Loss: 0.21730174124240875\n",
      "Epoch 1693, Loss: 0.9429640918970108, Final Batch Loss: 0.17737208306789398\n",
      "Epoch 1694, Loss: 1.0108262300491333, Final Batch Loss: 0.2097623646259308\n",
      "Epoch 1695, Loss: 1.060901165008545, Final Batch Loss: 0.2949007451534271\n",
      "Epoch 1696, Loss: 0.9965773224830627, Final Batch Loss: 0.2791402041912079\n",
      "Epoch 1697, Loss: 1.0894465297460556, Final Batch Loss: 0.3709512948989868\n",
      "Epoch 1698, Loss: 0.9812688827514648, Final Batch Loss: 0.15901698172092438\n",
      "Epoch 1699, Loss: 1.0547333508729935, Final Batch Loss: 0.23455701768398285\n",
      "Epoch 1700, Loss: 0.9535587728023529, Final Batch Loss: 0.19098590314388275\n",
      "Epoch 1701, Loss: 1.081891030073166, Final Batch Loss: 0.27752673625946045\n",
      "Epoch 1702, Loss: 1.0600752532482147, Final Batch Loss: 0.29660096764564514\n",
      "Epoch 1703, Loss: 0.9757819175720215, Final Batch Loss: 0.22776161134243011\n",
      "Epoch 1704, Loss: 1.1028213649988174, Final Batch Loss: 0.2914945185184479\n",
      "Epoch 1705, Loss: 0.9902926236391068, Final Batch Loss: 0.19325843453407288\n",
      "Epoch 1706, Loss: 1.1043176352977753, Final Batch Loss: 0.26748785376548767\n",
      "Epoch 1707, Loss: 0.9701342582702637, Final Batch Loss: 0.16958466172218323\n",
      "Epoch 1708, Loss: 1.027504712343216, Final Batch Loss: 0.21599125862121582\n",
      "Epoch 1709, Loss: 1.0292703062295914, Final Batch Loss: 0.2475859522819519\n",
      "Epoch 1710, Loss: 1.0083298236131668, Final Batch Loss: 0.2628726661205292\n",
      "Epoch 1711, Loss: 0.9682179689407349, Final Batch Loss: 0.19852575659751892\n",
      "Epoch 1712, Loss: 1.050321951508522, Final Batch Loss: 0.2826799750328064\n",
      "Epoch 1713, Loss: 1.0626969188451767, Final Batch Loss: 0.31994301080703735\n",
      "Epoch 1714, Loss: 1.0296284854412079, Final Batch Loss: 0.27710965275764465\n",
      "Epoch 1715, Loss: 0.9846829026937485, Final Batch Loss: 0.25774991512298584\n",
      "Epoch 1716, Loss: 1.0252619087696075, Final Batch Loss: 0.2584194839000702\n",
      "Epoch 1717, Loss: 1.060373455286026, Final Batch Loss: 0.2978193461894989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1718, Loss: 0.9843079000711441, Final Batch Loss: 0.20526781678199768\n",
      "Epoch 1719, Loss: 0.9955772161483765, Final Batch Loss: 0.22739653289318085\n",
      "Epoch 1720, Loss: 0.9917158782482147, Final Batch Loss: 0.2755958139896393\n",
      "Epoch 1721, Loss: 1.1066615581512451, Final Batch Loss: 0.2863270938396454\n",
      "Epoch 1722, Loss: 1.0342506915330887, Final Batch Loss: 0.27265283465385437\n",
      "Epoch 1723, Loss: 1.0600336194038391, Final Batch Loss: 0.28289034962654114\n",
      "Epoch 1724, Loss: 0.9369292855262756, Final Batch Loss: 0.20185348391532898\n",
      "Epoch 1725, Loss: 0.9732176661491394, Final Batch Loss: 0.18379661440849304\n",
      "Epoch 1726, Loss: 0.9911666065454483, Final Batch Loss: 0.22515419125556946\n",
      "Epoch 1727, Loss: 0.9723675400018692, Final Batch Loss: 0.2092398703098297\n",
      "Epoch 1728, Loss: 1.002544328570366, Final Batch Loss: 0.24874022603034973\n",
      "Epoch 1729, Loss: 1.0719178318977356, Final Batch Loss: 0.2936212420463562\n",
      "Epoch 1730, Loss: 0.9248795956373215, Final Batch Loss: 0.14812298119068146\n",
      "Epoch 1731, Loss: 1.0254694670438766, Final Batch Loss: 0.24513769149780273\n",
      "Epoch 1732, Loss: 1.0742251873016357, Final Batch Loss: 0.32162487506866455\n",
      "Epoch 1733, Loss: 1.0174905508756638, Final Batch Loss: 0.27305105328559875\n",
      "Epoch 1734, Loss: 1.1811091750860214, Final Batch Loss: 0.37685784697532654\n",
      "Epoch 1735, Loss: 0.9870928972959518, Final Batch Loss: 0.18056601285934448\n",
      "Epoch 1736, Loss: 1.0787586569786072, Final Batch Loss: 0.3394702970981598\n",
      "Epoch 1737, Loss: 0.996510311961174, Final Batch Loss: 0.1969822198152542\n",
      "Epoch 1738, Loss: 1.1174646764993668, Final Batch Loss: 0.3602375388145447\n",
      "Epoch 1739, Loss: 0.9806070774793625, Final Batch Loss: 0.2658678889274597\n",
      "Epoch 1740, Loss: 0.9560367614030838, Final Batch Loss: 0.17230406403541565\n",
      "Epoch 1741, Loss: 0.9749810099601746, Final Batch Loss: 0.20678457617759705\n",
      "Epoch 1742, Loss: 1.01716910302639, Final Batch Loss: 0.2785753607749939\n",
      "Epoch 1743, Loss: 1.026106521487236, Final Batch Loss: 0.2728855311870575\n",
      "Epoch 1744, Loss: 1.0652563869953156, Final Batch Loss: 0.34745949506759644\n",
      "Epoch 1745, Loss: 1.0264116078615189, Final Batch Loss: 0.18574108183383942\n",
      "Epoch 1746, Loss: 0.973177120089531, Final Batch Loss: 0.19882605969905853\n",
      "Epoch 1747, Loss: 1.0335948765277863, Final Batch Loss: 0.24360191822052002\n",
      "Epoch 1748, Loss: 0.9616156667470932, Final Batch Loss: 0.1869247704744339\n",
      "Epoch 1749, Loss: 0.9869929254055023, Final Batch Loss: 0.28760844469070435\n",
      "Epoch 1750, Loss: 0.9539783298969269, Final Batch Loss: 0.1590835154056549\n",
      "Epoch 1751, Loss: 1.0627127140760422, Final Batch Loss: 0.3214079439640045\n",
      "Epoch 1752, Loss: 1.1104614287614822, Final Batch Loss: 0.23947246372699738\n",
      "Epoch 1753, Loss: 1.0354025065898895, Final Batch Loss: 0.2653087079524994\n",
      "Epoch 1754, Loss: 1.0125915110111237, Final Batch Loss: 0.22794358432292938\n",
      "Epoch 1755, Loss: 1.040098324418068, Final Batch Loss: 0.23040348291397095\n",
      "Epoch 1756, Loss: 1.082206904888153, Final Batch Loss: 0.27201640605926514\n",
      "Epoch 1757, Loss: 0.9964631795883179, Final Batch Loss: 0.22484402358531952\n",
      "Epoch 1758, Loss: 1.038120225071907, Final Batch Loss: 0.2960432171821594\n",
      "Epoch 1759, Loss: 1.0554889291524887, Final Batch Loss: 0.2845141589641571\n",
      "Epoch 1760, Loss: 1.079712599515915, Final Batch Loss: 0.3258451521396637\n",
      "Epoch 1761, Loss: 1.0843904912471771, Final Batch Loss: 0.3015902042388916\n",
      "Epoch 1762, Loss: 0.9570184797048569, Final Batch Loss: 0.16230203211307526\n",
      "Epoch 1763, Loss: 0.935429111123085, Final Batch Loss: 0.17492274940013885\n",
      "Epoch 1764, Loss: 1.0127563625574112, Final Batch Loss: 0.24538324773311615\n",
      "Epoch 1765, Loss: 0.9799436032772064, Final Batch Loss: 0.22633540630340576\n",
      "Epoch 1766, Loss: 0.9647378921508789, Final Batch Loss: 0.1861315369606018\n",
      "Epoch 1767, Loss: 1.0319751501083374, Final Batch Loss: 0.29770663380622864\n",
      "Epoch 1768, Loss: 0.920835331082344, Final Batch Loss: 0.16666333377361298\n",
      "Epoch 1769, Loss: 0.9896930158138275, Final Batch Loss: 0.20808720588684082\n",
      "Epoch 1770, Loss: 1.045960173010826, Final Batch Loss: 0.29385697841644287\n",
      "Epoch 1771, Loss: 1.0124033391475677, Final Batch Loss: 0.21953102946281433\n",
      "Epoch 1772, Loss: 0.9684483110904694, Final Batch Loss: 0.23797009885311127\n",
      "Epoch 1773, Loss: 1.1544315218925476, Final Batch Loss: 0.3637722432613373\n",
      "Epoch 1774, Loss: 0.9972202628850937, Final Batch Loss: 0.2482435554265976\n",
      "Epoch 1775, Loss: 1.0784863978624344, Final Batch Loss: 0.2343679815530777\n",
      "Epoch 1776, Loss: 1.0382471829652786, Final Batch Loss: 0.31907331943511963\n",
      "Epoch 1777, Loss: 1.0565969944000244, Final Batch Loss: 0.29222801327705383\n",
      "Epoch 1778, Loss: 0.9716779142618179, Final Batch Loss: 0.22180742025375366\n",
      "Epoch 1779, Loss: 1.0266329050064087, Final Batch Loss: 0.2937103807926178\n",
      "Epoch 1780, Loss: 0.9903852343559265, Final Batch Loss: 0.24627552926540375\n",
      "Epoch 1781, Loss: 1.039492815732956, Final Batch Loss: 0.23846827447414398\n",
      "Epoch 1782, Loss: 0.9967135190963745, Final Batch Loss: 0.2052181363105774\n",
      "Epoch 1783, Loss: 0.9671000391244888, Final Batch Loss: 0.16972213983535767\n",
      "Epoch 1784, Loss: 1.0366041511297226, Final Batch Loss: 0.2617398500442505\n",
      "Epoch 1785, Loss: 1.0145933479070663, Final Batch Loss: 0.28696224093437195\n",
      "Epoch 1786, Loss: 1.0733098685741425, Final Batch Loss: 0.2995738685131073\n",
      "Epoch 1787, Loss: 0.9649046361446381, Final Batch Loss: 0.27122530341148376\n",
      "Epoch 1788, Loss: 1.0066668093204498, Final Batch Loss: 0.26028451323509216\n",
      "Epoch 1789, Loss: 1.0049457252025604, Final Batch Loss: 0.25974491238594055\n",
      "Epoch 1790, Loss: 1.0730679333209991, Final Batch Loss: 0.22738446295261383\n",
      "Epoch 1791, Loss: 1.0031181424856186, Final Batch Loss: 0.2740814983844757\n",
      "Epoch 1792, Loss: 0.9963057488203049, Final Batch Loss: 0.24458636343479156\n",
      "Epoch 1793, Loss: 1.0406901985406876, Final Batch Loss: 0.2343536764383316\n",
      "Epoch 1794, Loss: 1.074890986084938, Final Batch Loss: 0.2816283106803894\n",
      "Epoch 1795, Loss: 1.0079678446054459, Final Batch Loss: 0.27115076780319214\n",
      "Epoch 1796, Loss: 1.0707890093326569, Final Batch Loss: 0.2489616870880127\n",
      "Epoch 1797, Loss: 1.051035925745964, Final Batch Loss: 0.27096104621887207\n",
      "Epoch 1798, Loss: 0.9946873337030411, Final Batch Loss: 0.24555017054080963\n",
      "Epoch 1799, Loss: 1.0237746238708496, Final Batch Loss: 0.23326048254966736\n",
      "Epoch 1800, Loss: 1.0106913596391678, Final Batch Loss: 0.21325550973415375\n",
      "Epoch 1801, Loss: 0.9877261370420456, Final Batch Loss: 0.27269846200942993\n",
      "Epoch 1802, Loss: 0.9563850909471512, Final Batch Loss: 0.19819127023220062\n",
      "Epoch 1803, Loss: 1.0471433401107788, Final Batch Loss: 0.2515695095062256\n",
      "Epoch 1804, Loss: 1.0092825442552567, Final Batch Loss: 0.2402239739894867\n",
      "Epoch 1805, Loss: 1.0930196791887283, Final Batch Loss: 0.2958243787288666\n",
      "Epoch 1806, Loss: 1.0270543843507767, Final Batch Loss: 0.3217318058013916\n",
      "Epoch 1807, Loss: 1.040553256869316, Final Batch Loss: 0.30643901228904724\n",
      "Epoch 1808, Loss: 1.0032293498516083, Final Batch Loss: 0.23898759484291077\n",
      "Epoch 1809, Loss: 1.0297782570123672, Final Batch Loss: 0.2656240463256836\n",
      "Epoch 1810, Loss: 1.0142923593521118, Final Batch Loss: 0.25486963987350464\n",
      "Epoch 1811, Loss: 0.961191713809967, Final Batch Loss: 0.20555296540260315\n",
      "Epoch 1812, Loss: 0.9602395743131638, Final Batch Loss: 0.18477177619934082\n",
      "Epoch 1813, Loss: 1.0391454100608826, Final Batch Loss: 0.3016888201236725\n",
      "Epoch 1814, Loss: 1.0035312175750732, Final Batch Loss: 0.299704372882843\n",
      "Epoch 1815, Loss: 0.9867581278085709, Final Batch Loss: 0.2505671977996826\n",
      "Epoch 1816, Loss: 0.9662693589925766, Final Batch Loss: 0.24512024223804474\n",
      "Epoch 1817, Loss: 1.0689152032136917, Final Batch Loss: 0.313881516456604\n",
      "Epoch 1818, Loss: 1.1006473004817963, Final Batch Loss: 0.3440554738044739\n",
      "Epoch 1819, Loss: 0.9202212393283844, Final Batch Loss: 0.185077503323555\n",
      "Epoch 1820, Loss: 1.032975897192955, Final Batch Loss: 0.25407087802886963\n",
      "Epoch 1821, Loss: 1.00544935464859, Final Batch Loss: 0.24557256698608398\n",
      "Epoch 1822, Loss: 0.926536962389946, Final Batch Loss: 0.21200741827487946\n",
      "Epoch 1823, Loss: 1.0036668330430984, Final Batch Loss: 0.30456313490867615\n",
      "Epoch 1824, Loss: 1.0480291694402695, Final Batch Loss: 0.23456783592700958\n",
      "Epoch 1825, Loss: 0.9730066955089569, Final Batch Loss: 0.1978091597557068\n",
      "Epoch 1826, Loss: 1.1041582226753235, Final Batch Loss: 0.29440343379974365\n",
      "Epoch 1827, Loss: 1.017267569899559, Final Batch Loss: 0.24985185265541077\n",
      "Epoch 1828, Loss: 1.04544098675251, Final Batch Loss: 0.30724287033081055\n",
      "Epoch 1829, Loss: 1.03131765127182, Final Batch Loss: 0.30168017745018005\n",
      "Epoch 1830, Loss: 1.04921393096447, Final Batch Loss: 0.2624906897544861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1831, Loss: 0.9887423813343048, Final Batch Loss: 0.2167663723230362\n",
      "Epoch 1832, Loss: 0.9977082312107086, Final Batch Loss: 0.21195495128631592\n",
      "Epoch 1833, Loss: 0.9684800803661346, Final Batch Loss: 0.18091920018196106\n",
      "Epoch 1834, Loss: 1.0428681373596191, Final Batch Loss: 0.273571640253067\n",
      "Epoch 1835, Loss: 0.9665893167257309, Final Batch Loss: 0.18979798257350922\n",
      "Epoch 1836, Loss: 0.9839647263288498, Final Batch Loss: 0.25619152188301086\n",
      "Epoch 1837, Loss: 1.038729190826416, Final Batch Loss: 0.27350112795829773\n",
      "Epoch 1838, Loss: 1.0605975836515427, Final Batch Loss: 0.25065934658050537\n",
      "Epoch 1839, Loss: 1.0491784811019897, Final Batch Loss: 0.23716318607330322\n",
      "Epoch 1840, Loss: 1.0505008399486542, Final Batch Loss: 0.23727163672447205\n",
      "Epoch 1841, Loss: 1.0088450461626053, Final Batch Loss: 0.22080358862876892\n",
      "Epoch 1842, Loss: 0.9932959377765656, Final Batch Loss: 0.24633172154426575\n",
      "Epoch 1843, Loss: 1.00308558344841, Final Batch Loss: 0.24660567939281464\n",
      "Epoch 1844, Loss: 1.0186186581850052, Final Batch Loss: 0.2572137713432312\n",
      "Epoch 1845, Loss: 1.018201321363449, Final Batch Loss: 0.22627326846122742\n",
      "Epoch 1846, Loss: 1.0171794444322586, Final Batch Loss: 0.26576873660087585\n",
      "Epoch 1847, Loss: 0.8534135520458221, Final Batch Loss: 0.1469714343547821\n",
      "Epoch 1848, Loss: 0.9716583788394928, Final Batch Loss: 0.27297142148017883\n",
      "Epoch 1849, Loss: 0.9751957803964615, Final Batch Loss: 0.2394915521144867\n",
      "Epoch 1850, Loss: 0.9419925659894943, Final Batch Loss: 0.22011034190654755\n",
      "Epoch 1851, Loss: 0.9942016303539276, Final Batch Loss: 0.26597726345062256\n",
      "Epoch 1852, Loss: 1.024678111076355, Final Batch Loss: 0.27941298484802246\n",
      "Epoch 1853, Loss: 0.9833197593688965, Final Batch Loss: 0.26389434933662415\n",
      "Epoch 1854, Loss: 0.9739637076854706, Final Batch Loss: 0.23149257898330688\n",
      "Epoch 1855, Loss: 1.0958025604486465, Final Batch Loss: 0.3534766435623169\n",
      "Epoch 1856, Loss: 0.9991386085748672, Final Batch Loss: 0.2753448188304901\n",
      "Epoch 1857, Loss: 0.9414544552564621, Final Batch Loss: 0.24220703542232513\n",
      "Epoch 1858, Loss: 0.9750861674547195, Final Batch Loss: 0.20621316134929657\n",
      "Epoch 1859, Loss: 1.0669796913862228, Final Batch Loss: 0.22389431297779083\n",
      "Epoch 1860, Loss: 0.9244934171438217, Final Batch Loss: 0.13999880850315094\n",
      "Epoch 1861, Loss: 0.9872213453054428, Final Batch Loss: 0.25569427013397217\n",
      "Epoch 1862, Loss: 0.9926530718803406, Final Batch Loss: 0.2534213364124298\n",
      "Epoch 1863, Loss: 1.0643967688083649, Final Batch Loss: 0.36108818650245667\n",
      "Epoch 1864, Loss: 1.0574736446142197, Final Batch Loss: 0.281644344329834\n",
      "Epoch 1865, Loss: 1.019733041524887, Final Batch Loss: 0.21854400634765625\n",
      "Epoch 1866, Loss: 0.9628327339887619, Final Batch Loss: 0.21464720368385315\n",
      "Epoch 1867, Loss: 1.0507216900587082, Final Batch Loss: 0.3328998386859894\n",
      "Epoch 1868, Loss: 0.9558021128177643, Final Batch Loss: 0.20813852548599243\n",
      "Epoch 1869, Loss: 1.0263622105121613, Final Batch Loss: 0.35352739691734314\n",
      "Epoch 1870, Loss: 0.9550623893737793, Final Batch Loss: 0.1757688671350479\n",
      "Epoch 1871, Loss: 0.9622592031955719, Final Batch Loss: 0.21696272492408752\n",
      "Epoch 1872, Loss: 1.0099440515041351, Final Batch Loss: 0.20373013615608215\n",
      "Epoch 1873, Loss: 1.0324221849441528, Final Batch Loss: 0.30644646286964417\n",
      "Epoch 1874, Loss: 0.9231894910335541, Final Batch Loss: 0.19629082083702087\n",
      "Epoch 1875, Loss: 1.1481896042823792, Final Batch Loss: 0.3175570070743561\n",
      "Epoch 1876, Loss: 1.096002459526062, Final Batch Loss: 0.3728032410144806\n",
      "Epoch 1877, Loss: 0.9933181256055832, Final Batch Loss: 0.29598918557167053\n",
      "Epoch 1878, Loss: 0.9946305602788925, Final Batch Loss: 0.2964317202568054\n",
      "Epoch 1879, Loss: 0.9968423545360565, Final Batch Loss: 0.15886908769607544\n",
      "Epoch 1880, Loss: 1.0072328448295593, Final Batch Loss: 0.28966617584228516\n",
      "Epoch 1881, Loss: 0.9848348945379257, Final Batch Loss: 0.24094431102275848\n",
      "Epoch 1882, Loss: 0.9470549076795578, Final Batch Loss: 0.2086891084909439\n",
      "Epoch 1883, Loss: 0.9887601286172867, Final Batch Loss: 0.28177687525749207\n",
      "Epoch 1884, Loss: 0.9666950553655624, Final Batch Loss: 0.19466455280780792\n",
      "Epoch 1885, Loss: 0.9847409129142761, Final Batch Loss: 0.28951701521873474\n",
      "Epoch 1886, Loss: 0.9644337594509125, Final Batch Loss: 0.22821348905563354\n",
      "Epoch 1887, Loss: 0.9859217703342438, Final Batch Loss: 0.26852381229400635\n",
      "Epoch 1888, Loss: 1.02668796479702, Final Batch Loss: 0.2582220435142517\n",
      "Epoch 1889, Loss: 0.9812321364879608, Final Batch Loss: 0.1772756427526474\n",
      "Epoch 1890, Loss: 0.9451688677072525, Final Batch Loss: 0.16412818431854248\n",
      "Epoch 1891, Loss: 0.9670353829860687, Final Batch Loss: 0.1889212727546692\n",
      "Epoch 1892, Loss: 0.9253539890050888, Final Batch Loss: 0.19971312582492828\n",
      "Epoch 1893, Loss: 1.0671699941158295, Final Batch Loss: 0.2892404794692993\n",
      "Epoch 1894, Loss: 1.0054797232151031, Final Batch Loss: 0.22136080265045166\n",
      "Epoch 1895, Loss: 0.9990395903587341, Final Batch Loss: 0.23425441980361938\n",
      "Epoch 1896, Loss: 0.9740634262561798, Final Batch Loss: 0.2153252363204956\n",
      "Epoch 1897, Loss: 0.9927113056182861, Final Batch Loss: 0.25100526213645935\n",
      "Epoch 1898, Loss: 0.9834248572587967, Final Batch Loss: 0.24109968543052673\n",
      "Epoch 1899, Loss: 1.0496366173028946, Final Batch Loss: 0.28930944204330444\n",
      "Epoch 1900, Loss: 1.049925148487091, Final Batch Loss: 0.2793164551258087\n",
      "Epoch 1901, Loss: 1.0048376321792603, Final Batch Loss: 0.28606054186820984\n",
      "Epoch 1902, Loss: 0.9671863317489624, Final Batch Loss: 0.2598625719547272\n",
      "Epoch 1903, Loss: 0.9712230116128922, Final Batch Loss: 0.18822520971298218\n",
      "Epoch 1904, Loss: 0.9459964334964752, Final Batch Loss: 0.20588915050029755\n",
      "Epoch 1905, Loss: 0.9805365651845932, Final Batch Loss: 0.2281639128923416\n",
      "Epoch 1906, Loss: 1.1558142602443695, Final Batch Loss: 0.31482017040252686\n",
      "Epoch 1907, Loss: 1.0861542969942093, Final Batch Loss: 0.2576492130756378\n",
      "Epoch 1908, Loss: 0.9924698770046234, Final Batch Loss: 0.22738952934741974\n",
      "Epoch 1909, Loss: 1.007532849907875, Final Batch Loss: 0.2453456073999405\n",
      "Epoch 1910, Loss: 0.9557392001152039, Final Batch Loss: 0.20313861966133118\n",
      "Epoch 1911, Loss: 1.1767110526561737, Final Batch Loss: 0.31157150864601135\n",
      "Epoch 1912, Loss: 1.0080193430185318, Final Batch Loss: 0.2869082987308502\n",
      "Epoch 1913, Loss: 1.08501797914505, Final Batch Loss: 0.3262333571910858\n",
      "Epoch 1914, Loss: 0.9214919656515121, Final Batch Loss: 0.1735689491033554\n",
      "Epoch 1915, Loss: 1.035196229815483, Final Batch Loss: 0.245656356215477\n",
      "Epoch 1916, Loss: 1.1079542189836502, Final Batch Loss: 0.3521212339401245\n",
      "Epoch 1917, Loss: 1.0643117427825928, Final Batch Loss: 0.26776111125946045\n",
      "Epoch 1918, Loss: 0.964424654841423, Final Batch Loss: 0.20984339714050293\n",
      "Epoch 1919, Loss: 0.9415785074234009, Final Batch Loss: 0.2181534767150879\n",
      "Epoch 1920, Loss: 0.9507336914539337, Final Batch Loss: 0.2077697068452835\n",
      "Epoch 1921, Loss: 1.0328737050294876, Final Batch Loss: 0.24505920708179474\n",
      "Epoch 1922, Loss: 0.9344891309738159, Final Batch Loss: 0.17506898939609528\n",
      "Epoch 1923, Loss: 1.0118163377046585, Final Batch Loss: 0.24907004833221436\n",
      "Epoch 1924, Loss: 1.065928876399994, Final Batch Loss: 0.3345882296562195\n",
      "Epoch 1925, Loss: 0.9419830590486526, Final Batch Loss: 0.21975575387477875\n",
      "Epoch 1926, Loss: 1.114426389336586, Final Batch Loss: 0.3344033658504486\n",
      "Epoch 1927, Loss: 0.9871727526187897, Final Batch Loss: 0.2529066503047943\n",
      "Epoch 1928, Loss: 1.020574539899826, Final Batch Loss: 0.2510988116264343\n",
      "Epoch 1929, Loss: 1.0346207916736603, Final Batch Loss: 0.3092813491821289\n",
      "Epoch 1930, Loss: 0.9232186824083328, Final Batch Loss: 0.1648653894662857\n",
      "Epoch 1931, Loss: 0.9735383987426758, Final Batch Loss: 0.18986402451992035\n",
      "Epoch 1932, Loss: 1.0102025866508484, Final Batch Loss: 0.23932945728302002\n",
      "Epoch 1933, Loss: 0.9635040462017059, Final Batch Loss: 0.18014922738075256\n",
      "Epoch 1934, Loss: 0.9215358793735504, Final Batch Loss: 0.17226581275463104\n",
      "Epoch 1935, Loss: 1.1178712099790573, Final Batch Loss: 0.34677690267562866\n",
      "Epoch 1936, Loss: 1.0163913667201996, Final Batch Loss: 0.2834925055503845\n",
      "Epoch 1937, Loss: 0.9844381213188171, Final Batch Loss: 0.2158244252204895\n",
      "Epoch 1938, Loss: 1.0844283550977707, Final Batch Loss: 0.2958833873271942\n",
      "Epoch 1939, Loss: 0.9396525472402573, Final Batch Loss: 0.2137838751077652\n",
      "Epoch 1940, Loss: 0.9804647713899612, Final Batch Loss: 0.2569449245929718\n",
      "Epoch 1941, Loss: 1.0025086998939514, Final Batch Loss: 0.23680678009986877\n",
      "Epoch 1942, Loss: 0.9705907851457596, Final Batch Loss: 0.23846197128295898\n",
      "Epoch 1943, Loss: 0.9858790189027786, Final Batch Loss: 0.19786062836647034\n",
      "Epoch 1944, Loss: 1.0119759738445282, Final Batch Loss: 0.21652597188949585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1945, Loss: 1.0546806305646896, Final Batch Loss: 0.31523510813713074\n",
      "Epoch 1946, Loss: 1.014614224433899, Final Batch Loss: 0.2502008378505707\n",
      "Epoch 1947, Loss: 0.9895427376031876, Final Batch Loss: 0.22634783387184143\n",
      "Epoch 1948, Loss: 1.0834500789642334, Final Batch Loss: 0.2642463445663452\n",
      "Epoch 1949, Loss: 1.0836468786001205, Final Batch Loss: 0.3152162730693817\n",
      "Epoch 1950, Loss: 1.129047155380249, Final Batch Loss: 0.2752567231655121\n",
      "Epoch 1951, Loss: 1.035991132259369, Final Batch Loss: 0.2724056541919708\n",
      "Epoch 1952, Loss: 1.0440096408128738, Final Batch Loss: 0.22215460240840912\n",
      "Epoch 1953, Loss: 1.016547292470932, Final Batch Loss: 0.2934184968471527\n",
      "Epoch 1954, Loss: 0.9407493472099304, Final Batch Loss: 0.1753484010696411\n",
      "Epoch 1955, Loss: 0.9762209206819534, Final Batch Loss: 0.2825268805027008\n",
      "Epoch 1956, Loss: 1.0427958071231842, Final Batch Loss: 0.30414122343063354\n",
      "Epoch 1957, Loss: 1.0997228920459747, Final Batch Loss: 0.3235955834388733\n",
      "Epoch 1958, Loss: 1.0062557756900787, Final Batch Loss: 0.22688478231430054\n",
      "Epoch 1959, Loss: 0.9839477092027664, Final Batch Loss: 0.22503703832626343\n",
      "Epoch 1960, Loss: 0.962116926908493, Final Batch Loss: 0.23573917150497437\n",
      "Epoch 1961, Loss: 0.9600202590227127, Final Batch Loss: 0.20387503504753113\n",
      "Epoch 1962, Loss: 0.9501330405473709, Final Batch Loss: 0.23516470193862915\n",
      "Epoch 1963, Loss: 1.01968814432621, Final Batch Loss: 0.19034726917743683\n",
      "Epoch 1964, Loss: 0.9267693161964417, Final Batch Loss: 0.19588935375213623\n",
      "Epoch 1965, Loss: 1.0665045976638794, Final Batch Loss: 0.2782415747642517\n",
      "Epoch 1966, Loss: 1.0114676356315613, Final Batch Loss: 0.2322470098733902\n",
      "Epoch 1967, Loss: 1.0241923183202744, Final Batch Loss: 0.2991088330745697\n",
      "Epoch 1968, Loss: 1.0108489990234375, Final Batch Loss: 0.24984689056873322\n",
      "Epoch 1969, Loss: 0.9912713468074799, Final Batch Loss: 0.26644107699394226\n",
      "Epoch 1970, Loss: 1.0332856178283691, Final Batch Loss: 0.2364768087863922\n",
      "Epoch 1971, Loss: 1.0292920768260956, Final Batch Loss: 0.24732816219329834\n",
      "Epoch 1972, Loss: 0.9945030361413956, Final Batch Loss: 0.19582898914813995\n",
      "Epoch 1973, Loss: 0.9611671715974808, Final Batch Loss: 0.1804589331150055\n",
      "Epoch 1974, Loss: 1.0010534971952438, Final Batch Loss: 0.2399933934211731\n",
      "Epoch 1975, Loss: 0.9287583082914352, Final Batch Loss: 0.1559189110994339\n",
      "Epoch 1976, Loss: 1.0091210454702377, Final Batch Loss: 0.2545740604400635\n",
      "Epoch 1977, Loss: 1.1426493525505066, Final Batch Loss: 0.40054014325141907\n",
      "Epoch 1978, Loss: 0.9693046659231186, Final Batch Loss: 0.2408682405948639\n",
      "Epoch 1979, Loss: 1.0330404490232468, Final Batch Loss: 0.31280237436294556\n",
      "Epoch 1980, Loss: 1.0412495881319046, Final Batch Loss: 0.27854710817337036\n",
      "Epoch 1981, Loss: 0.9177606999874115, Final Batch Loss: 0.1730709820985794\n",
      "Epoch 1982, Loss: 1.065207615494728, Final Batch Loss: 0.3434762954711914\n",
      "Epoch 1983, Loss: 0.9508683979511261, Final Batch Loss: 0.20485307276248932\n",
      "Epoch 1984, Loss: 0.9710757285356522, Final Batch Loss: 0.2106442153453827\n",
      "Epoch 1985, Loss: 1.0032824277877808, Final Batch Loss: 0.22481989860534668\n",
      "Epoch 1986, Loss: 1.0184310376644135, Final Batch Loss: 0.28090932965278625\n",
      "Epoch 1987, Loss: 1.00632144510746, Final Batch Loss: 0.2790241241455078\n",
      "Epoch 1988, Loss: 0.9736014604568481, Final Batch Loss: 0.22761298716068268\n",
      "Epoch 1989, Loss: 1.1122216582298279, Final Batch Loss: 0.34143733978271484\n",
      "Epoch 1990, Loss: 1.13404580950737, Final Batch Loss: 0.3777627646923065\n",
      "Epoch 1991, Loss: 1.065935641527176, Final Batch Loss: 0.2679120600223541\n",
      "Epoch 1992, Loss: 1.089274600148201, Final Batch Loss: 0.3086269795894623\n",
      "Epoch 1993, Loss: 1.038847103714943, Final Batch Loss: 0.3172961473464966\n",
      "Epoch 1994, Loss: 1.0056788921356201, Final Batch Loss: 0.26204636693000793\n",
      "Epoch 1995, Loss: 1.0249853879213333, Final Batch Loss: 0.304806649684906\n",
      "Epoch 1996, Loss: 0.914796993136406, Final Batch Loss: 0.17415426671504974\n",
      "Epoch 1997, Loss: 0.9410428702831268, Final Batch Loss: 0.1920498162508011\n",
      "Epoch 1998, Loss: 1.006864532828331, Final Batch Loss: 0.2396940141916275\n",
      "Epoch 1999, Loss: 1.0227778106927872, Final Batch Loss: 0.2608257830142975\n",
      "Epoch 2000, Loss: 1.1000894755125046, Final Batch Loss: 0.2954898774623871\n",
      "Epoch 2001, Loss: 1.0038644671440125, Final Batch Loss: 0.2844364047050476\n",
      "Epoch 2002, Loss: 1.052194520831108, Final Batch Loss: 0.2840202748775482\n",
      "Epoch 2003, Loss: 0.9975612461566925, Final Batch Loss: 0.28561484813690186\n",
      "Epoch 2004, Loss: 0.9358518570661545, Final Batch Loss: 0.21274299919605255\n",
      "Epoch 2005, Loss: 1.193046510219574, Final Batch Loss: 0.3630155920982361\n",
      "Epoch 2006, Loss: 0.9551815241575241, Final Batch Loss: 0.20700857043266296\n",
      "Epoch 2007, Loss: 1.019822508096695, Final Batch Loss: 0.2491673231124878\n",
      "Epoch 2008, Loss: 0.9805278927087784, Final Batch Loss: 0.24368177354335785\n",
      "Epoch 2009, Loss: 0.9373062402009964, Final Batch Loss: 0.21385259926319122\n",
      "Epoch 2010, Loss: 0.9982516318559647, Final Batch Loss: 0.25012362003326416\n",
      "Epoch 2011, Loss: 1.0857293605804443, Final Batch Loss: 0.28654658794403076\n",
      "Epoch 2012, Loss: 0.9504278302192688, Final Batch Loss: 0.1526687890291214\n",
      "Epoch 2013, Loss: 0.9416806548833847, Final Batch Loss: 0.2478017657995224\n",
      "Epoch 2014, Loss: 0.9447448253631592, Final Batch Loss: 0.1870070844888687\n",
      "Epoch 2015, Loss: 0.9163131266832352, Final Batch Loss: 0.19770687818527222\n",
      "Epoch 2016, Loss: 0.9775558859109879, Final Batch Loss: 0.2439642697572708\n",
      "Epoch 2017, Loss: 0.9605229794979095, Final Batch Loss: 0.21766415238380432\n",
      "Epoch 2018, Loss: 1.0456713736057281, Final Batch Loss: 0.28419625759124756\n",
      "Epoch 2019, Loss: 1.0259207636117935, Final Batch Loss: 0.24332551658153534\n",
      "Epoch 2020, Loss: 1.0078667402267456, Final Batch Loss: 0.2512616217136383\n",
      "Epoch 2021, Loss: 1.0841334462165833, Final Batch Loss: 0.3201391100883484\n",
      "Epoch 2022, Loss: 0.9909894317388535, Final Batch Loss: 0.1881360113620758\n",
      "Epoch 2023, Loss: 1.000101014971733, Final Batch Loss: 0.2406146079301834\n",
      "Epoch 2024, Loss: 1.0672901719808578, Final Batch Loss: 0.2722097635269165\n",
      "Epoch 2025, Loss: 0.9818990975618362, Final Batch Loss: 0.22362440824508667\n",
      "Epoch 2026, Loss: 1.0077105015516281, Final Batch Loss: 0.1932515799999237\n",
      "Epoch 2027, Loss: 0.9564585238695145, Final Batch Loss: 0.21879254281520844\n",
      "Epoch 2028, Loss: 0.9630448818206787, Final Batch Loss: 0.24532531201839447\n",
      "Epoch 2029, Loss: 1.0445028096437454, Final Batch Loss: 0.30447375774383545\n",
      "Epoch 2030, Loss: 1.0121358633041382, Final Batch Loss: 0.2230704426765442\n",
      "Epoch 2031, Loss: 0.9808085709810257, Final Batch Loss: 0.2460547387599945\n",
      "Epoch 2032, Loss: 0.959596574306488, Final Batch Loss: 0.251163512468338\n",
      "Epoch 2033, Loss: 1.0040263682603836, Final Batch Loss: 0.21333497762680054\n",
      "Epoch 2034, Loss: 0.9103020578622818, Final Batch Loss: 0.17444266378879547\n",
      "Epoch 2035, Loss: 1.038254052400589, Final Batch Loss: 0.31793203949928284\n",
      "Epoch 2036, Loss: 1.0506023317575455, Final Batch Loss: 0.32731181383132935\n",
      "Epoch 2037, Loss: 1.0439567565917969, Final Batch Loss: 0.3761211037635803\n",
      "Epoch 2038, Loss: 0.996319442987442, Final Batch Loss: 0.22089417278766632\n",
      "Epoch 2039, Loss: 1.0149991512298584, Final Batch Loss: 0.26300546526908875\n",
      "Epoch 2040, Loss: 1.0036584734916687, Final Batch Loss: 0.24387772381305695\n",
      "Epoch 2041, Loss: 0.9551598876714706, Final Batch Loss: 0.18585193157196045\n",
      "Epoch 2042, Loss: 0.9563236385583878, Final Batch Loss: 0.23237155377864838\n",
      "Epoch 2043, Loss: 1.0230566412210464, Final Batch Loss: 0.2667018473148346\n",
      "Epoch 2044, Loss: 0.9665189683437347, Final Batch Loss: 0.2735799551010132\n",
      "Epoch 2045, Loss: 0.9786535650491714, Final Batch Loss: 0.22388771176338196\n",
      "Epoch 2046, Loss: 1.1065411120653152, Final Batch Loss: 0.32529178261756897\n",
      "Epoch 2047, Loss: 0.9569241404533386, Final Batch Loss: 0.2334158569574356\n",
      "Epoch 2048, Loss: 1.011114850640297, Final Batch Loss: 0.28352636098861694\n",
      "Epoch 2049, Loss: 1.00615756213665, Final Batch Loss: 0.2954486608505249\n",
      "Epoch 2050, Loss: 0.9655610918998718, Final Batch Loss: 0.2381778061389923\n",
      "Epoch 2051, Loss: 0.9641717672348022, Final Batch Loss: 0.27799639105796814\n",
      "Epoch 2052, Loss: 1.0289823114871979, Final Batch Loss: 0.20608308911323547\n",
      "Epoch 2053, Loss: 1.0280659645795822, Final Batch Loss: 0.31086090207099915\n",
      "Epoch 2054, Loss: 0.9627945423126221, Final Batch Loss: 0.23805896937847137\n",
      "Epoch 2055, Loss: 0.9737820625305176, Final Batch Loss: 0.23744289577007294\n",
      "Epoch 2056, Loss: 1.0662556737661362, Final Batch Loss: 0.3049702048301697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2057, Loss: 1.0736129879951477, Final Batch Loss: 0.3355669677257538\n",
      "Epoch 2058, Loss: 1.0196750611066818, Final Batch Loss: 0.2932901382446289\n",
      "Epoch 2059, Loss: 1.063261941075325, Final Batch Loss: 0.3240460753440857\n",
      "Epoch 2060, Loss: 1.0482332706451416, Final Batch Loss: 0.280486524105072\n",
      "Epoch 2061, Loss: 0.9741450548171997, Final Batch Loss: 0.19644039869308472\n",
      "Epoch 2062, Loss: 1.020550087094307, Final Batch Loss: 0.21413592994213104\n",
      "Epoch 2063, Loss: 1.047688290476799, Final Batch Loss: 0.23866404592990875\n",
      "Epoch 2064, Loss: 0.9590174704790115, Final Batch Loss: 0.19520476460456848\n",
      "Epoch 2065, Loss: 0.9097508788108826, Final Batch Loss: 0.15646345913410187\n",
      "Epoch 2066, Loss: 0.9410274922847748, Final Batch Loss: 0.25952067971229553\n",
      "Epoch 2067, Loss: 0.8699800372123718, Final Batch Loss: 0.18847928941249847\n",
      "Epoch 2068, Loss: 1.0515134185552597, Final Batch Loss: 0.33439791202545166\n",
      "Epoch 2069, Loss: 1.0249882936477661, Final Batch Loss: 0.257672518491745\n",
      "Epoch 2070, Loss: 0.9941045343875885, Final Batch Loss: 0.2720053195953369\n",
      "Epoch 2071, Loss: 1.118343323469162, Final Batch Loss: 0.3360143303871155\n",
      "Epoch 2072, Loss: 1.0663882493972778, Final Batch Loss: 0.290760338306427\n",
      "Epoch 2073, Loss: 0.9604217410087585, Final Batch Loss: 0.18031853437423706\n",
      "Epoch 2074, Loss: 1.0000130981206894, Final Batch Loss: 0.19500717520713806\n",
      "Epoch 2075, Loss: 0.9475914090871811, Final Batch Loss: 0.24213944375514984\n",
      "Epoch 2076, Loss: 0.9807073175907135, Final Batch Loss: 0.2722925543785095\n",
      "Epoch 2077, Loss: 0.9136555790901184, Final Batch Loss: 0.1804044544696808\n",
      "Epoch 2078, Loss: 0.9967161864042282, Final Batch Loss: 0.2615709602832794\n",
      "Epoch 2079, Loss: 1.0516579002141953, Final Batch Loss: 0.32006028294563293\n",
      "Epoch 2080, Loss: 1.059599220752716, Final Batch Loss: 0.25143444538116455\n",
      "Epoch 2081, Loss: 0.9433299601078033, Final Batch Loss: 0.24318937957286835\n",
      "Epoch 2082, Loss: 0.9933741837739944, Final Batch Loss: 0.23750896751880646\n",
      "Epoch 2083, Loss: 0.9852306544780731, Final Batch Loss: 0.197652667760849\n",
      "Epoch 2084, Loss: 1.0260195583105087, Final Batch Loss: 0.23047107458114624\n",
      "Epoch 2085, Loss: 0.9329480230808258, Final Batch Loss: 0.19874151051044464\n",
      "Epoch 2086, Loss: 0.9916111081838608, Final Batch Loss: 0.1780575066804886\n",
      "Epoch 2087, Loss: 0.9348637014627457, Final Batch Loss: 0.19983862340450287\n",
      "Epoch 2088, Loss: 1.0329036265611649, Final Batch Loss: 0.23473574221134186\n",
      "Epoch 2089, Loss: 0.9484757781028748, Final Batch Loss: 0.2700836658477783\n",
      "Epoch 2090, Loss: 0.8920874893665314, Final Batch Loss: 0.17931799590587616\n",
      "Epoch 2091, Loss: 0.9925105720758438, Final Batch Loss: 0.24095936119556427\n",
      "Epoch 2092, Loss: 1.0254813879728317, Final Batch Loss: 0.2834261655807495\n",
      "Epoch 2093, Loss: 0.8964288979768753, Final Batch Loss: 0.17806115746498108\n",
      "Epoch 2094, Loss: 0.971195250749588, Final Batch Loss: 0.2674758732318878\n",
      "Epoch 2095, Loss: 1.0295537263154984, Final Batch Loss: 0.29575595259666443\n",
      "Epoch 2096, Loss: 1.0000697672367096, Final Batch Loss: 0.272726446390152\n",
      "Epoch 2097, Loss: 0.923776239156723, Final Batch Loss: 0.21160657703876495\n",
      "Epoch 2098, Loss: 1.0625795722007751, Final Batch Loss: 0.3512902855873108\n",
      "Epoch 2099, Loss: 0.9331229478120804, Final Batch Loss: 0.1989559531211853\n",
      "Epoch 2100, Loss: 0.9601862281560898, Final Batch Loss: 0.17125806212425232\n",
      "Epoch 2101, Loss: 1.049773171544075, Final Batch Loss: 0.2779797911643982\n",
      "Epoch 2102, Loss: 0.9793814420700073, Final Batch Loss: 0.20797179639339447\n",
      "Epoch 2103, Loss: 0.9202621877193451, Final Batch Loss: 0.15665924549102783\n",
      "Epoch 2104, Loss: 1.0007072389125824, Final Batch Loss: 0.294843465089798\n",
      "Epoch 2105, Loss: 0.9894429445266724, Final Batch Loss: 0.2506040334701538\n",
      "Epoch 2106, Loss: 0.9409459829330444, Final Batch Loss: 0.22911663353443146\n",
      "Epoch 2107, Loss: 0.9542113989591599, Final Batch Loss: 0.18794937431812286\n",
      "Epoch 2108, Loss: 0.9448873549699783, Final Batch Loss: 0.17172174155712128\n",
      "Epoch 2109, Loss: 0.9554478973150253, Final Batch Loss: 0.15721048414707184\n",
      "Epoch 2110, Loss: 1.0076181292533875, Final Batch Loss: 0.2759013772010803\n",
      "Epoch 2111, Loss: 1.0002726316452026, Final Batch Loss: 0.23099300265312195\n",
      "Epoch 2112, Loss: 0.9531316459178925, Final Batch Loss: 0.16969192028045654\n",
      "Epoch 2113, Loss: 0.9830506145954132, Final Batch Loss: 0.2348068803548813\n",
      "Epoch 2114, Loss: 1.0541614145040512, Final Batch Loss: 0.3645648658275604\n",
      "Epoch 2115, Loss: 0.9537910223007202, Final Batch Loss: 0.21113483607769012\n",
      "Epoch 2116, Loss: 1.032638892531395, Final Batch Loss: 0.28974398970603943\n",
      "Epoch 2117, Loss: 1.0746090114116669, Final Batch Loss: 0.3052411675453186\n",
      "Epoch 2118, Loss: 1.0249548554420471, Final Batch Loss: 0.28328603506088257\n",
      "Epoch 2119, Loss: 0.981966182589531, Final Batch Loss: 0.2416958510875702\n",
      "Epoch 2120, Loss: 0.9907592982053757, Final Batch Loss: 0.29838740825653076\n",
      "Epoch 2121, Loss: 1.013468086719513, Final Batch Loss: 0.22065812349319458\n",
      "Epoch 2122, Loss: 0.9452434927225113, Final Batch Loss: 0.24588240683078766\n",
      "Epoch 2123, Loss: 0.9886436462402344, Final Batch Loss: 0.2509782314300537\n",
      "Epoch 2124, Loss: 1.0242005288600922, Final Batch Loss: 0.29284369945526123\n",
      "Epoch 2125, Loss: 0.9167643487453461, Final Batch Loss: 0.15386706590652466\n",
      "Epoch 2126, Loss: 1.1082556545734406, Final Batch Loss: 0.3799495995044708\n",
      "Epoch 2127, Loss: 0.937018483877182, Final Batch Loss: 0.20985804498195648\n",
      "Epoch 2128, Loss: 0.9452641159296036, Final Batch Loss: 0.19329415261745453\n",
      "Epoch 2129, Loss: 1.0203697681427002, Final Batch Loss: 0.29993951320648193\n",
      "Epoch 2130, Loss: 1.1010659337043762, Final Batch Loss: 0.32286253571510315\n",
      "Epoch 2131, Loss: 0.9544181376695633, Final Batch Loss: 0.26564890146255493\n",
      "Epoch 2132, Loss: 0.9370551407337189, Final Batch Loss: 0.2226574420928955\n",
      "Epoch 2133, Loss: 0.9993713647127151, Final Batch Loss: 0.2673181891441345\n",
      "Epoch 2134, Loss: 0.9835775643587112, Final Batch Loss: 0.24943004548549652\n",
      "Epoch 2135, Loss: 0.9766075760126114, Final Batch Loss: 0.20042049884796143\n",
      "Epoch 2136, Loss: 1.0607464164495468, Final Batch Loss: 0.29841625690460205\n",
      "Epoch 2137, Loss: 1.0036527961492538, Final Batch Loss: 0.2629474997520447\n",
      "Epoch 2138, Loss: 1.0250009298324585, Final Batch Loss: 0.24371002614498138\n",
      "Epoch 2139, Loss: 1.0440899580717087, Final Batch Loss: 0.2447775900363922\n",
      "Epoch 2140, Loss: 0.9481044560670853, Final Batch Loss: 0.18840675055980682\n",
      "Epoch 2141, Loss: 1.015906736254692, Final Batch Loss: 0.26800045371055603\n",
      "Epoch 2142, Loss: 1.1184952706098557, Final Batch Loss: 0.33869218826293945\n",
      "Epoch 2143, Loss: 1.0011805295944214, Final Batch Loss: 0.2130199670791626\n",
      "Epoch 2144, Loss: 0.9624218344688416, Final Batch Loss: 0.22739940881729126\n",
      "Epoch 2145, Loss: 1.0593713074922562, Final Batch Loss: 0.2634401023387909\n",
      "Epoch 2146, Loss: 0.9872754663228989, Final Batch Loss: 0.25997194647789\n",
      "Epoch 2147, Loss: 0.9858282506465912, Final Batch Loss: 0.3052137494087219\n",
      "Epoch 2148, Loss: 1.0311180800199509, Final Batch Loss: 0.2819982171058655\n",
      "Epoch 2149, Loss: 1.089909315109253, Final Batch Loss: 0.29662081599235535\n",
      "Epoch 2150, Loss: 1.0378733575344086, Final Batch Loss: 0.2878248691558838\n",
      "Epoch 2151, Loss: 0.9973995685577393, Final Batch Loss: 0.24056097865104675\n",
      "Epoch 2152, Loss: 1.056248962879181, Final Batch Loss: 0.27962708473205566\n",
      "Epoch 2153, Loss: 0.9969772696495056, Final Batch Loss: 0.27678656578063965\n",
      "Epoch 2154, Loss: 1.144538938999176, Final Batch Loss: 0.3634519875049591\n",
      "Epoch 2155, Loss: 1.0156800150871277, Final Batch Loss: 0.2609303593635559\n",
      "Epoch 2156, Loss: 0.9727964103221893, Final Batch Loss: 0.2306220978498459\n",
      "Epoch 2157, Loss: 1.1784063875675201, Final Batch Loss: 0.3786444067955017\n",
      "Epoch 2158, Loss: 1.0067349821329117, Final Batch Loss: 0.23753485083580017\n",
      "Epoch 2159, Loss: 1.0362188965082169, Final Batch Loss: 0.2676667273044586\n",
      "Epoch 2160, Loss: 1.0383301824331284, Final Batch Loss: 0.24426324665546417\n",
      "Epoch 2161, Loss: 0.9174683839082718, Final Batch Loss: 0.2324640452861786\n",
      "Epoch 2162, Loss: 1.0904306024312973, Final Batch Loss: 0.3097994029521942\n",
      "Epoch 2163, Loss: 0.990533784031868, Final Batch Loss: 0.2372589111328125\n",
      "Epoch 2164, Loss: 1.089236781001091, Final Batch Loss: 0.2896927297115326\n",
      "Epoch 2165, Loss: 0.9354876577854156, Final Batch Loss: 0.22178705036640167\n",
      "Epoch 2166, Loss: 0.9631978124380112, Final Batch Loss: 0.2743588984012604\n",
      "Epoch 2167, Loss: 0.9392131865024567, Final Batch Loss: 0.2093043476343155\n",
      "Epoch 2168, Loss: 1.0615816563367844, Final Batch Loss: 0.39059826731681824\n",
      "Epoch 2169, Loss: 0.9528937041759491, Final Batch Loss: 0.2713199555873871\n",
      "Epoch 2170, Loss: 1.066492035984993, Final Batch Loss: 0.3443942368030548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2171, Loss: 0.9586875140666962, Final Batch Loss: 0.26819249987602234\n",
      "Epoch 2172, Loss: 0.9325002133846283, Final Batch Loss: 0.19167910516262054\n",
      "Epoch 2173, Loss: 1.0298127830028534, Final Batch Loss: 0.25955724716186523\n",
      "Epoch 2174, Loss: 1.1254730969667435, Final Batch Loss: 0.37795931100845337\n",
      "Epoch 2175, Loss: 1.028716817498207, Final Batch Loss: 0.2718285620212555\n",
      "Epoch 2176, Loss: 0.9661079347133636, Final Batch Loss: 0.31415680050849915\n",
      "Epoch 2177, Loss: 0.971672534942627, Final Batch Loss: 0.2294117659330368\n",
      "Epoch 2178, Loss: 0.9954632669687271, Final Batch Loss: 0.259683758020401\n",
      "Epoch 2179, Loss: 0.9879033714532852, Final Batch Loss: 0.2592024803161621\n",
      "Epoch 2180, Loss: 1.0356429070234299, Final Batch Loss: 0.21479594707489014\n",
      "Epoch 2181, Loss: 0.9841415286064148, Final Batch Loss: 0.2876804769039154\n",
      "Epoch 2182, Loss: 1.0633222609758377, Final Batch Loss: 0.36877331137657166\n",
      "Epoch 2183, Loss: 0.8951377272605896, Final Batch Loss: 0.1602410078048706\n",
      "Epoch 2184, Loss: 0.8632053583860397, Final Batch Loss: 0.19510166347026825\n",
      "Epoch 2185, Loss: 0.9022235423326492, Final Batch Loss: 0.18439379334449768\n",
      "Epoch 2186, Loss: 1.0064400732517242, Final Batch Loss: 0.28043848276138306\n",
      "Epoch 2187, Loss: 1.0052090734243393, Final Batch Loss: 0.2460789531469345\n",
      "Epoch 2188, Loss: 1.026333510875702, Final Batch Loss: 0.2912823259830475\n",
      "Epoch 2189, Loss: 1.0110229551792145, Final Batch Loss: 0.23864270746707916\n",
      "Epoch 2190, Loss: 0.9732465893030167, Final Batch Loss: 0.1933470368385315\n",
      "Epoch 2191, Loss: 0.9902331084012985, Final Batch Loss: 0.30367279052734375\n",
      "Epoch 2192, Loss: 0.883195772767067, Final Batch Loss: 0.15999940037727356\n",
      "Epoch 2193, Loss: 1.0321354120969772, Final Batch Loss: 0.2779669165611267\n",
      "Epoch 2194, Loss: 0.9321940243244171, Final Batch Loss: 0.19215165078639984\n",
      "Epoch 2195, Loss: 1.0777204036712646, Final Batch Loss: 0.40219953656196594\n",
      "Epoch 2196, Loss: 0.9626884758472443, Final Batch Loss: 0.2786715030670166\n",
      "Epoch 2197, Loss: 0.8961727470159531, Final Batch Loss: 0.1517418920993805\n",
      "Epoch 2198, Loss: 0.9017199277877808, Final Batch Loss: 0.14210884273052216\n",
      "Epoch 2199, Loss: 0.9203861206769943, Final Batch Loss: 0.15854115784168243\n",
      "Epoch 2200, Loss: 1.0175449699163437, Final Batch Loss: 0.33669179677963257\n",
      "Epoch 2201, Loss: 0.9903373122215271, Final Batch Loss: 0.21793442964553833\n",
      "Epoch 2202, Loss: 0.9178683757781982, Final Batch Loss: 0.23868824541568756\n",
      "Epoch 2203, Loss: 1.0420507490634918, Final Batch Loss: 0.25073567032814026\n",
      "Epoch 2204, Loss: 0.9490278363227844, Final Batch Loss: 0.16591717302799225\n",
      "Epoch 2205, Loss: 0.9128362238407135, Final Batch Loss: 0.20062163472175598\n",
      "Epoch 2206, Loss: 0.9978552013635635, Final Batch Loss: 0.251508891582489\n",
      "Epoch 2207, Loss: 0.9684349745512009, Final Batch Loss: 0.2436130791902542\n",
      "Epoch 2208, Loss: 0.9303043931722641, Final Batch Loss: 0.18951968848705292\n",
      "Epoch 2209, Loss: 0.9627330750226974, Final Batch Loss: 0.22788475453853607\n",
      "Epoch 2210, Loss: 1.0440417677164078, Final Batch Loss: 0.271399587392807\n",
      "Epoch 2211, Loss: 0.9704734683036804, Final Batch Loss: 0.2207862287759781\n",
      "Epoch 2212, Loss: 1.0529203414916992, Final Batch Loss: 0.3004131615161896\n",
      "Epoch 2213, Loss: 0.9459706544876099, Final Batch Loss: 0.1890474259853363\n",
      "Epoch 2214, Loss: 0.9188041538000107, Final Batch Loss: 0.21135304868221283\n",
      "Epoch 2215, Loss: 1.0372543632984161, Final Batch Loss: 0.3353700041770935\n",
      "Epoch 2216, Loss: 1.0827797502279282, Final Batch Loss: 0.36078786849975586\n",
      "Epoch 2217, Loss: 1.0341839045286179, Final Batch Loss: 0.253556489944458\n",
      "Epoch 2218, Loss: 1.0281489342451096, Final Batch Loss: 0.22865934669971466\n",
      "Epoch 2219, Loss: 0.9698214381933212, Final Batch Loss: 0.19913440942764282\n",
      "Epoch 2220, Loss: 1.0655288696289062, Final Batch Loss: 0.31639766693115234\n",
      "Epoch 2221, Loss: 0.9801945984363556, Final Batch Loss: 0.199065700173378\n",
      "Epoch 2222, Loss: 0.9752066284418106, Final Batch Loss: 0.19615112245082855\n",
      "Epoch 2223, Loss: 0.9681834876537323, Final Batch Loss: 0.21364489197731018\n",
      "Epoch 2224, Loss: 0.9339607357978821, Final Batch Loss: 0.20074668526649475\n",
      "Epoch 2225, Loss: 1.0187292993068695, Final Batch Loss: 0.2653554379940033\n",
      "Epoch 2226, Loss: 0.9431704729795456, Final Batch Loss: 0.2373008280992508\n",
      "Epoch 2227, Loss: 1.0472871661186218, Final Batch Loss: 0.2634419798851013\n",
      "Epoch 2228, Loss: 1.0346104353666306, Final Batch Loss: 0.20758113265037537\n",
      "Epoch 2229, Loss: 1.0667535662651062, Final Batch Loss: 0.2842563986778259\n",
      "Epoch 2230, Loss: 0.941732183098793, Final Batch Loss: 0.20383520424365997\n",
      "Epoch 2231, Loss: 1.0142157077789307, Final Batch Loss: 0.19411933422088623\n",
      "Epoch 2232, Loss: 0.9721197932958603, Final Batch Loss: 0.26483675837516785\n",
      "Epoch 2233, Loss: 0.9221379458904266, Final Batch Loss: 0.19855844974517822\n",
      "Epoch 2234, Loss: 0.9689312726259232, Final Batch Loss: 0.24554432928562164\n",
      "Epoch 2235, Loss: 0.9963531941175461, Final Batch Loss: 0.22300998866558075\n",
      "Epoch 2236, Loss: 1.0767314434051514, Final Batch Loss: 0.28098273277282715\n",
      "Epoch 2237, Loss: 1.0271748155355453, Final Batch Loss: 0.3108339309692383\n",
      "Epoch 2238, Loss: 1.0105503648519516, Final Batch Loss: 0.22107654809951782\n",
      "Epoch 2239, Loss: 0.9215428829193115, Final Batch Loss: 0.21736156940460205\n",
      "Epoch 2240, Loss: 0.9619854092597961, Final Batch Loss: 0.29477182030677795\n",
      "Epoch 2241, Loss: 0.90476855635643, Final Batch Loss: 0.15321305394172668\n",
      "Epoch 2242, Loss: 0.9940434545278549, Final Batch Loss: 0.2867344915866852\n",
      "Epoch 2243, Loss: 0.9888755232095718, Final Batch Loss: 0.21969996392726898\n",
      "Epoch 2244, Loss: 0.9604952484369278, Final Batch Loss: 0.2276085913181305\n",
      "Epoch 2245, Loss: 1.0359415262937546, Final Batch Loss: 0.2555135190486908\n",
      "Epoch 2246, Loss: 0.9981091618537903, Final Batch Loss: 0.25100111961364746\n",
      "Epoch 2247, Loss: 0.9096876978874207, Final Batch Loss: 0.23077428340911865\n",
      "Epoch 2248, Loss: 0.9999657869338989, Final Batch Loss: 0.21344685554504395\n",
      "Epoch 2249, Loss: 1.0698318034410477, Final Batch Loss: 0.38106033205986023\n",
      "Epoch 2250, Loss: 1.0054555237293243, Final Batch Loss: 0.2972263693809509\n",
      "Epoch 2251, Loss: 1.0269000232219696, Final Batch Loss: 0.33455154299736023\n",
      "Epoch 2252, Loss: 0.9906409680843353, Final Batch Loss: 0.2663147449493408\n",
      "Epoch 2253, Loss: 0.9251883029937744, Final Batch Loss: 0.20542499423027039\n",
      "Epoch 2254, Loss: 0.9667951017618179, Final Batch Loss: 0.2246391773223877\n",
      "Epoch 2255, Loss: 1.018332153558731, Final Batch Loss: 0.2751339375972748\n",
      "Epoch 2256, Loss: 1.034697949886322, Final Batch Loss: 0.2839270234107971\n",
      "Epoch 2257, Loss: 0.9382000416517258, Final Batch Loss: 0.18597166240215302\n",
      "Epoch 2258, Loss: 0.9411561489105225, Final Batch Loss: 0.2639301121234894\n",
      "Epoch 2259, Loss: 0.9080796986818314, Final Batch Loss: 0.2081473171710968\n",
      "Epoch 2260, Loss: 1.001194342970848, Final Batch Loss: 0.2660285532474518\n",
      "Epoch 2261, Loss: 0.9947388023138046, Final Batch Loss: 0.29837939143180847\n",
      "Epoch 2262, Loss: 0.9065448641777039, Final Batch Loss: 0.21234676241874695\n",
      "Epoch 2263, Loss: 0.8886041343212128, Final Batch Loss: 0.15062011778354645\n",
      "Epoch 2264, Loss: 0.9500498324632645, Final Batch Loss: 0.2555754780769348\n",
      "Epoch 2265, Loss: 1.077151745557785, Final Batch Loss: 0.28809496760368347\n",
      "Epoch 2266, Loss: 0.9079713672399521, Final Batch Loss: 0.1751192808151245\n",
      "Epoch 2267, Loss: 0.9922071397304535, Final Batch Loss: 0.22514842450618744\n",
      "Epoch 2268, Loss: 0.9046027362346649, Final Batch Loss: 0.16371193528175354\n",
      "Epoch 2269, Loss: 1.0412912964820862, Final Batch Loss: 0.2990886867046356\n",
      "Epoch 2270, Loss: 1.0172819644212723, Final Batch Loss: 0.2249661684036255\n",
      "Epoch 2271, Loss: 0.9413472563028336, Final Batch Loss: 0.26435863971710205\n",
      "Epoch 2272, Loss: 1.0422229021787643, Final Batch Loss: 0.3241032361984253\n",
      "Epoch 2273, Loss: 0.9584177881479263, Final Batch Loss: 0.20880164206027985\n",
      "Epoch 2274, Loss: 0.9794937521219254, Final Batch Loss: 0.22548845410346985\n",
      "Epoch 2275, Loss: 1.0177928805351257, Final Batch Loss: 0.28007644414901733\n",
      "Epoch 2276, Loss: 0.9679693430662155, Final Batch Loss: 0.27249398827552795\n",
      "Epoch 2277, Loss: 0.9797732681035995, Final Batch Loss: 0.23770125210285187\n",
      "Epoch 2278, Loss: 0.9111000895500183, Final Batch Loss: 0.1798848956823349\n",
      "Epoch 2279, Loss: 0.9554452449083328, Final Batch Loss: 0.25030940771102905\n",
      "Epoch 2280, Loss: 0.9709706455469131, Final Batch Loss: 0.2954883873462677\n",
      "Epoch 2281, Loss: 1.0022791028022766, Final Batch Loss: 0.2564425766468048\n",
      "Epoch 2282, Loss: 0.9352962374687195, Final Batch Loss: 0.23639895021915436\n",
      "Epoch 2283, Loss: 0.9322862774133682, Final Batch Loss: 0.23790015280246735\n",
      "Epoch 2284, Loss: 0.912476196885109, Final Batch Loss: 0.21273598074913025\n",
      "Epoch 2285, Loss: 0.9742483496665955, Final Batch Loss: 0.2467295527458191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2286, Loss: 0.9914791584014893, Final Batch Loss: 0.26568126678466797\n",
      "Epoch 2287, Loss: 0.9562497138977051, Final Batch Loss: 0.30065250396728516\n",
      "Epoch 2288, Loss: 0.9859117865562439, Final Batch Loss: 0.2573581039905548\n",
      "Epoch 2289, Loss: 1.0281602293252945, Final Batch Loss: 0.3002606928348541\n",
      "Epoch 2290, Loss: 1.0340440422296524, Final Batch Loss: 0.28652456402778625\n",
      "Epoch 2291, Loss: 0.995290145277977, Final Batch Loss: 0.2657923102378845\n",
      "Epoch 2292, Loss: 1.000111535191536, Final Batch Loss: 0.3064800798892975\n",
      "Epoch 2293, Loss: 0.9638974219560623, Final Batch Loss: 0.26669076085090637\n",
      "Epoch 2294, Loss: 0.9294462502002716, Final Batch Loss: 0.20584988594055176\n",
      "Epoch 2295, Loss: 0.9613607376813889, Final Batch Loss: 0.2246566116809845\n",
      "Epoch 2296, Loss: 0.9674457013607025, Final Batch Loss: 0.2344217449426651\n",
      "Epoch 2297, Loss: 0.9215682595968246, Final Batch Loss: 0.24014881253242493\n",
      "Epoch 2298, Loss: 0.9741386622190475, Final Batch Loss: 0.21409772336483002\n",
      "Epoch 2299, Loss: 0.9550704061985016, Final Batch Loss: 0.1865146905183792\n",
      "Epoch 2300, Loss: 0.9820067584514618, Final Batch Loss: 0.2807914912700653\n",
      "Epoch 2301, Loss: 0.8917528539896011, Final Batch Loss: 0.19400766491889954\n",
      "Epoch 2302, Loss: 0.9397705048322678, Final Batch Loss: 0.24706557393074036\n",
      "Epoch 2303, Loss: 0.9432010799646378, Final Batch Loss: 0.1956452578306198\n",
      "Epoch 2304, Loss: 0.9923576265573502, Final Batch Loss: 0.27905502915382385\n",
      "Epoch 2305, Loss: 0.960596576333046, Final Batch Loss: 0.23846563696861267\n",
      "Epoch 2306, Loss: 1.0639456361532211, Final Batch Loss: 0.3578658998012543\n",
      "Epoch 2307, Loss: 0.9749181270599365, Final Batch Loss: 0.1634947657585144\n",
      "Epoch 2308, Loss: 1.0511988997459412, Final Batch Loss: 0.25491541624069214\n",
      "Epoch 2309, Loss: 1.0165656805038452, Final Batch Loss: 0.26625487208366394\n",
      "Epoch 2310, Loss: 0.9162542074918747, Final Batch Loss: 0.13978490233421326\n",
      "Epoch 2311, Loss: 0.9397614002227783, Final Batch Loss: 0.15154893696308136\n",
      "Epoch 2312, Loss: 0.9700888097286224, Final Batch Loss: 0.25712913274765015\n",
      "Epoch 2313, Loss: 0.8794380277395248, Final Batch Loss: 0.180311918258667\n",
      "Epoch 2314, Loss: 0.9857096970081329, Final Batch Loss: 0.2024277299642563\n",
      "Epoch 2315, Loss: 0.9522300660610199, Final Batch Loss: 0.21125906705856323\n",
      "Epoch 2316, Loss: 0.948615550994873, Final Batch Loss: 0.2721375823020935\n",
      "Epoch 2317, Loss: 0.9953186511993408, Final Batch Loss: 0.2884017527103424\n",
      "Epoch 2318, Loss: 1.0986497402191162, Final Batch Loss: 0.37860479950904846\n",
      "Epoch 2319, Loss: 0.9589995443820953, Final Batch Loss: 0.20236024260520935\n",
      "Epoch 2320, Loss: 1.1129493713378906, Final Batch Loss: 0.21732205152511597\n",
      "Epoch 2321, Loss: 1.0097750276327133, Final Batch Loss: 0.2943665385246277\n",
      "Epoch 2322, Loss: 1.0543067455291748, Final Batch Loss: 0.2892524302005768\n",
      "Epoch 2323, Loss: 0.9996279031038284, Final Batch Loss: 0.17662212252616882\n",
      "Epoch 2324, Loss: 0.9794260561466217, Final Batch Loss: 0.23001597821712494\n",
      "Epoch 2325, Loss: 0.9751153141260147, Final Batch Loss: 0.26786690950393677\n",
      "Epoch 2326, Loss: 0.9434861540794373, Final Batch Loss: 0.17526163160800934\n",
      "Epoch 2327, Loss: 0.9590623378753662, Final Batch Loss: 0.19989770650863647\n",
      "Epoch 2328, Loss: 0.9938801229000092, Final Batch Loss: 0.2540716230869293\n",
      "Epoch 2329, Loss: 0.9696235209703445, Final Batch Loss: 0.22117231786251068\n",
      "Epoch 2330, Loss: 0.9424749910831451, Final Batch Loss: 0.1927800476551056\n",
      "Epoch 2331, Loss: 0.9775905013084412, Final Batch Loss: 0.2614056468009949\n",
      "Epoch 2332, Loss: 1.0097054541110992, Final Batch Loss: 0.2556798756122589\n",
      "Epoch 2333, Loss: 1.0208749771118164, Final Batch Loss: 0.23674744367599487\n",
      "Epoch 2334, Loss: 0.9692132025957108, Final Batch Loss: 0.24146559834480286\n",
      "Epoch 2335, Loss: 1.0620047897100449, Final Batch Loss: 0.27884387969970703\n",
      "Epoch 2336, Loss: 0.95091612637043, Final Batch Loss: 0.17436479032039642\n",
      "Epoch 2337, Loss: 1.0116062462329865, Final Batch Loss: 0.2337416410446167\n",
      "Epoch 2338, Loss: 1.0469290912151337, Final Batch Loss: 0.25785449147224426\n",
      "Epoch 2339, Loss: 0.9585882276296616, Final Batch Loss: 0.27562010288238525\n",
      "Epoch 2340, Loss: 0.9677030146121979, Final Batch Loss: 0.2546205520629883\n",
      "Epoch 2341, Loss: 1.02827687561512, Final Batch Loss: 0.3007424473762512\n",
      "Epoch 2342, Loss: 0.9036862403154373, Final Batch Loss: 0.1598128378391266\n",
      "Epoch 2343, Loss: 1.0660664439201355, Final Batch Loss: 0.29008185863494873\n",
      "Epoch 2344, Loss: 1.02234748005867, Final Batch Loss: 0.33977019786834717\n",
      "Epoch 2345, Loss: 0.9532433897256851, Final Batch Loss: 0.2726922631263733\n",
      "Epoch 2346, Loss: 0.9073978513479233, Final Batch Loss: 0.17994575202465057\n",
      "Epoch 2347, Loss: 1.0228384733200073, Final Batch Loss: 0.29610675573349\n",
      "Epoch 2348, Loss: 0.9967341423034668, Final Batch Loss: 0.29349270462989807\n",
      "Epoch 2349, Loss: 0.9815434068441391, Final Batch Loss: 0.2598954141139984\n",
      "Epoch 2350, Loss: 1.0165690183639526, Final Batch Loss: 0.30365198850631714\n",
      "Epoch 2351, Loss: 1.0350700169801712, Final Batch Loss: 0.30533894896507263\n",
      "Epoch 2352, Loss: 1.0429350584745407, Final Batch Loss: 0.29122528433799744\n",
      "Epoch 2353, Loss: 1.0071778297424316, Final Batch Loss: 0.3185056746006012\n",
      "Epoch 2354, Loss: 1.0729001760482788, Final Batch Loss: 0.2810450494289398\n",
      "Epoch 2355, Loss: 0.963617280125618, Final Batch Loss: 0.2278926819562912\n",
      "Epoch 2356, Loss: 1.1036636233329773, Final Batch Loss: 0.36700230836868286\n",
      "Epoch 2357, Loss: 0.9475085139274597, Final Batch Loss: 0.23062075674533844\n",
      "Epoch 2358, Loss: 1.0619727075099945, Final Batch Loss: 0.3394472301006317\n",
      "Epoch 2359, Loss: 0.964637041091919, Final Batch Loss: 0.2598942220211029\n",
      "Epoch 2360, Loss: 1.0245776772499084, Final Batch Loss: 0.2540336549282074\n",
      "Epoch 2361, Loss: 0.9386622160673141, Final Batch Loss: 0.20341697335243225\n",
      "Epoch 2362, Loss: 0.9693225175142288, Final Batch Loss: 0.27176839113235474\n",
      "Epoch 2363, Loss: 0.8769339174032211, Final Batch Loss: 0.19726192951202393\n",
      "Epoch 2364, Loss: 0.890365406870842, Final Batch Loss: 0.13165709376335144\n",
      "Epoch 2365, Loss: 0.9023952335119247, Final Batch Loss: 0.14640288054943085\n",
      "Epoch 2366, Loss: 0.9228304028511047, Final Batch Loss: 0.14253847301006317\n",
      "Epoch 2367, Loss: 0.9689607918262482, Final Batch Loss: 0.22253592312335968\n",
      "Epoch 2368, Loss: 0.9287880510091782, Final Batch Loss: 0.18352270126342773\n",
      "Epoch 2369, Loss: 0.9223999232053757, Final Batch Loss: 0.22867320477962494\n",
      "Epoch 2370, Loss: 0.9129021763801575, Final Batch Loss: 0.19048629701137543\n",
      "Epoch 2371, Loss: 0.9342185407876968, Final Batch Loss: 0.2653627395629883\n",
      "Epoch 2372, Loss: 1.0163092464208603, Final Batch Loss: 0.30739516019821167\n",
      "Epoch 2373, Loss: 0.9312359690666199, Final Batch Loss: 0.2085675448179245\n",
      "Epoch 2374, Loss: 0.9755402356386185, Final Batch Loss: 0.2228764146566391\n",
      "Epoch 2375, Loss: 0.9245407432317734, Final Batch Loss: 0.2463575154542923\n",
      "Epoch 2376, Loss: 0.9116884469985962, Final Batch Loss: 0.1691562682390213\n",
      "Epoch 2377, Loss: 0.8745959252119064, Final Batch Loss: 0.18589918315410614\n",
      "Epoch 2378, Loss: 1.0551362335681915, Final Batch Loss: 0.34991586208343506\n",
      "Epoch 2379, Loss: 0.9310426861047745, Final Batch Loss: 0.2194746881723404\n",
      "Epoch 2380, Loss: 0.9839935153722763, Final Batch Loss: 0.2106407880783081\n",
      "Epoch 2381, Loss: 0.9787961542606354, Final Batch Loss: 0.24942690134048462\n",
      "Epoch 2382, Loss: 1.0434938073158264, Final Batch Loss: 0.32287657260894775\n",
      "Epoch 2383, Loss: 1.0466047525405884, Final Batch Loss: 0.3168871998786926\n",
      "Epoch 2384, Loss: 0.9905819296836853, Final Batch Loss: 0.2572609484195709\n",
      "Epoch 2385, Loss: 0.9251221418380737, Final Batch Loss: 0.25162622332572937\n",
      "Epoch 2386, Loss: 0.9678691774606705, Final Batch Loss: 0.2686261832714081\n",
      "Epoch 2387, Loss: 0.9803939014673233, Final Batch Loss: 0.243840292096138\n",
      "Epoch 2388, Loss: 0.9647179841995239, Final Batch Loss: 0.2793136537075043\n",
      "Epoch 2389, Loss: 0.9901850074529648, Final Batch Loss: 0.29648515582084656\n",
      "Epoch 2390, Loss: 0.8997432291507721, Final Batch Loss: 0.21482107043266296\n",
      "Epoch 2391, Loss: 0.9925980716943741, Final Batch Loss: 0.2949962019920349\n",
      "Epoch 2392, Loss: 0.8914869725704193, Final Batch Loss: 0.17950904369354248\n",
      "Epoch 2393, Loss: 0.9149031043052673, Final Batch Loss: 0.19040755927562714\n",
      "Epoch 2394, Loss: 0.9737174957990646, Final Batch Loss: 0.2528742551803589\n",
      "Epoch 2395, Loss: 0.9991581588983536, Final Batch Loss: 0.30870065093040466\n",
      "Epoch 2396, Loss: 0.8844941705465317, Final Batch Loss: 0.1290626972913742\n",
      "Epoch 2397, Loss: 0.9076972603797913, Final Batch Loss: 0.18212255835533142\n",
      "Epoch 2398, Loss: 0.9891038686037064, Final Batch Loss: 0.29568055272102356\n",
      "Epoch 2399, Loss: 0.937133863568306, Final Batch Loss: 0.21081902086734772\n",
      "Epoch 2400, Loss: 1.0277140587568283, Final Batch Loss: 0.303636372089386\n",
      "Epoch 2401, Loss: 1.0219887793064117, Final Batch Loss: 0.24721059203147888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2402, Loss: 0.956603392958641, Final Batch Loss: 0.2204490303993225\n",
      "Epoch 2403, Loss: 1.0096322298049927, Final Batch Loss: 0.24500243365764618\n",
      "Epoch 2404, Loss: 0.995913490653038, Final Batch Loss: 0.22481763362884521\n",
      "Epoch 2405, Loss: 0.8678942918777466, Final Batch Loss: 0.1942446529865265\n",
      "Epoch 2406, Loss: 0.9883973449468613, Final Batch Loss: 0.23874256014823914\n",
      "Epoch 2407, Loss: 0.9706056863069534, Final Batch Loss: 0.2540033757686615\n",
      "Epoch 2408, Loss: 0.9363912492990494, Final Batch Loss: 0.19127298891544342\n",
      "Epoch 2409, Loss: 0.9555652290582657, Final Batch Loss: 0.19958676397800446\n",
      "Epoch 2410, Loss: 0.9654518663883209, Final Batch Loss: 0.20291298627853394\n",
      "Epoch 2411, Loss: 0.978505477309227, Final Batch Loss: 0.2580742835998535\n",
      "Epoch 2412, Loss: 0.9730939567089081, Final Batch Loss: 0.24329453706741333\n",
      "Epoch 2413, Loss: 1.0370250791311264, Final Batch Loss: 0.25976890325546265\n",
      "Epoch 2414, Loss: 0.9791607111692429, Final Batch Loss: 0.25150546431541443\n",
      "Epoch 2415, Loss: 0.9356912970542908, Final Batch Loss: 0.20939302444458008\n",
      "Epoch 2416, Loss: 0.9421754032373428, Final Batch Loss: 0.23381423950195312\n",
      "Epoch 2417, Loss: 0.9064552932977676, Final Batch Loss: 0.19465278089046478\n",
      "Epoch 2418, Loss: 1.0101299732923508, Final Batch Loss: 0.35168397426605225\n",
      "Epoch 2419, Loss: 0.8904220312833786, Final Batch Loss: 0.139173224568367\n",
      "Epoch 2420, Loss: 0.9566980600357056, Final Batch Loss: 0.238850399851799\n",
      "Epoch 2421, Loss: 1.0094561129808426, Final Batch Loss: 0.2940847873687744\n",
      "Epoch 2422, Loss: 0.8868840038776398, Final Batch Loss: 0.14697690308094025\n",
      "Epoch 2423, Loss: 0.9732492715120316, Final Batch Loss: 0.2347046285867691\n",
      "Epoch 2424, Loss: 1.0300292521715164, Final Batch Loss: 0.31164830923080444\n",
      "Epoch 2425, Loss: 1.0441770553588867, Final Batch Loss: 0.2542998492717743\n",
      "Epoch 2426, Loss: 0.9423561096191406, Final Batch Loss: 0.2397443801164627\n",
      "Epoch 2427, Loss: 0.9270496219396591, Final Batch Loss: 0.19896207749843597\n",
      "Epoch 2428, Loss: 0.8634866029024124, Final Batch Loss: 0.13510394096374512\n",
      "Epoch 2429, Loss: 0.9981140196323395, Final Batch Loss: 0.2759891450405121\n",
      "Epoch 2430, Loss: 1.0287833660840988, Final Batch Loss: 0.28171804547309875\n",
      "Epoch 2431, Loss: 0.8807146102190018, Final Batch Loss: 0.18723039329051971\n",
      "Epoch 2432, Loss: 1.0036218762397766, Final Batch Loss: 0.3187865912914276\n",
      "Epoch 2433, Loss: 1.013309046626091, Final Batch Loss: 0.2559361159801483\n",
      "Epoch 2434, Loss: 1.027160331606865, Final Batch Loss: 0.2970767915248871\n",
      "Epoch 2435, Loss: 0.8982456773519516, Final Batch Loss: 0.1754329800605774\n",
      "Epoch 2436, Loss: 0.9624422937631607, Final Batch Loss: 0.24356356263160706\n",
      "Epoch 2437, Loss: 0.9521738886833191, Final Batch Loss: 0.2193673700094223\n",
      "Epoch 2438, Loss: 0.984147310256958, Final Batch Loss: 0.25632545351982117\n",
      "Epoch 2439, Loss: 0.9145812094211578, Final Batch Loss: 0.18206439912319183\n",
      "Epoch 2440, Loss: 0.9866277873516083, Final Batch Loss: 0.3063546419143677\n",
      "Epoch 2441, Loss: 0.9754548668861389, Final Batch Loss: 0.2698848247528076\n",
      "Epoch 2442, Loss: 0.9605420082807541, Final Batch Loss: 0.2523665130138397\n",
      "Epoch 2443, Loss: 0.9029737710952759, Final Batch Loss: 0.2720176577568054\n",
      "Epoch 2444, Loss: 0.9434568285942078, Final Batch Loss: 0.21222053468227386\n",
      "Epoch 2445, Loss: 0.9686222225427628, Final Batch Loss: 0.192650705575943\n",
      "Epoch 2446, Loss: 0.9214478582143784, Final Batch Loss: 0.24690395593643188\n",
      "Epoch 2447, Loss: 1.0503036379814148, Final Batch Loss: 0.28699174523353577\n",
      "Epoch 2448, Loss: 0.9939125627279282, Final Batch Loss: 0.2681700885295868\n",
      "Epoch 2449, Loss: 0.949822798371315, Final Batch Loss: 0.19119758903980255\n",
      "Epoch 2450, Loss: 1.1143155544996262, Final Batch Loss: 0.3914162814617157\n",
      "Epoch 2451, Loss: 0.9260778427124023, Final Batch Loss: 0.2763528525829315\n",
      "Epoch 2452, Loss: 1.0064211934804916, Final Batch Loss: 0.2998329997062683\n",
      "Epoch 2453, Loss: 0.9368611872196198, Final Batch Loss: 0.19739463925361633\n",
      "Epoch 2454, Loss: 1.0114445984363556, Final Batch Loss: 0.22900879383087158\n",
      "Epoch 2455, Loss: 0.931983157992363, Final Batch Loss: 0.24728816747665405\n",
      "Epoch 2456, Loss: 0.8584467321634293, Final Batch Loss: 0.160643070936203\n",
      "Epoch 2457, Loss: 1.0425196141004562, Final Batch Loss: 0.32492366433143616\n",
      "Epoch 2458, Loss: 1.036182314157486, Final Batch Loss: 0.3366728127002716\n",
      "Epoch 2459, Loss: 0.9826559722423553, Final Batch Loss: 0.25528568029403687\n",
      "Epoch 2460, Loss: 0.9537850916385651, Final Batch Loss: 0.17084701359272003\n",
      "Epoch 2461, Loss: 0.9716768115758896, Final Batch Loss: 0.20481981337070465\n",
      "Epoch 2462, Loss: 0.9538094401359558, Final Batch Loss: 0.21566279232501984\n",
      "Epoch 2463, Loss: 0.8786068558692932, Final Batch Loss: 0.16512848436832428\n",
      "Epoch 2464, Loss: 0.9937335550785065, Final Batch Loss: 0.28121721744537354\n",
      "Epoch 2465, Loss: 0.9874866306781769, Final Batch Loss: 0.24681200087070465\n",
      "Epoch 2466, Loss: 0.9662358313798904, Final Batch Loss: 0.25340262055397034\n",
      "Epoch 2467, Loss: 1.0532369762659073, Final Batch Loss: 0.29245245456695557\n",
      "Epoch 2468, Loss: 0.9337086975574493, Final Batch Loss: 0.1737716943025589\n",
      "Epoch 2469, Loss: 0.9337502270936966, Final Batch Loss: 0.2027314007282257\n",
      "Epoch 2470, Loss: 1.090181589126587, Final Batch Loss: 0.3254481852054596\n",
      "Epoch 2471, Loss: 0.9892643541097641, Final Batch Loss: 0.2458924949169159\n",
      "Epoch 2472, Loss: 1.0022920668125153, Final Batch Loss: 0.3010377883911133\n",
      "Epoch 2473, Loss: 0.9775128364562988, Final Batch Loss: 0.21619360148906708\n",
      "Epoch 2474, Loss: 0.8791543543338776, Final Batch Loss: 0.17096082866191864\n",
      "Epoch 2475, Loss: 0.95249342918396, Final Batch Loss: 0.261353462934494\n",
      "Epoch 2476, Loss: 0.8754996657371521, Final Batch Loss: 0.21216857433319092\n",
      "Epoch 2477, Loss: 0.9919274896383286, Final Batch Loss: 0.26894593238830566\n",
      "Epoch 2478, Loss: 0.9950666427612305, Final Batch Loss: 0.23409977555274963\n",
      "Epoch 2479, Loss: 1.0318266302347183, Final Batch Loss: 0.3234539330005646\n",
      "Epoch 2480, Loss: 0.9431760907173157, Final Batch Loss: 0.22978222370147705\n",
      "Epoch 2481, Loss: 1.0124707520008087, Final Batch Loss: 0.26732534170150757\n",
      "Epoch 2482, Loss: 0.9916711300611496, Final Batch Loss: 0.23851439356803894\n",
      "Epoch 2483, Loss: 1.028810903429985, Final Batch Loss: 0.2903871536254883\n",
      "Epoch 2484, Loss: 0.9028336703777313, Final Batch Loss: 0.19758450984954834\n",
      "Epoch 2485, Loss: 1.002960592508316, Final Batch Loss: 0.234884113073349\n",
      "Epoch 2486, Loss: 0.9142179638147354, Final Batch Loss: 0.22198806703090668\n",
      "Epoch 2487, Loss: 0.9854457378387451, Final Batch Loss: 0.2873951196670532\n",
      "Epoch 2488, Loss: 0.8987650871276855, Final Batch Loss: 0.26484888792037964\n",
      "Epoch 2489, Loss: 0.9874411523342133, Final Batch Loss: 0.21850192546844482\n",
      "Epoch 2490, Loss: 0.9584557861089706, Final Batch Loss: 0.1650371551513672\n",
      "Epoch 2491, Loss: 1.010934755206108, Final Batch Loss: 0.2736237645149231\n",
      "Epoch 2492, Loss: 0.9086091369390488, Final Batch Loss: 0.26097601652145386\n",
      "Epoch 2493, Loss: 0.9223717749118805, Final Batch Loss: 0.22532136738300323\n",
      "Epoch 2494, Loss: 0.9079287797212601, Final Batch Loss: 0.24386709928512573\n",
      "Epoch 2495, Loss: 0.9755749702453613, Final Batch Loss: 0.2518196105957031\n",
      "Epoch 2496, Loss: 0.8679127395153046, Final Batch Loss: 0.16206157207489014\n",
      "Epoch 2497, Loss: 0.9108833074569702, Final Batch Loss: 0.22564642131328583\n",
      "Epoch 2498, Loss: 0.8733227103948593, Final Batch Loss: 0.20901960134506226\n",
      "Epoch 2499, Loss: 0.9767884463071823, Final Batch Loss: 0.23061658442020416\n",
      "Epoch 2500, Loss: 0.9320878386497498, Final Batch Loss: 0.2624962031841278\n",
      "Epoch 2501, Loss: 0.9623749852180481, Final Batch Loss: 0.24876613914966583\n",
      "Epoch 2502, Loss: 0.9308562874794006, Final Batch Loss: 0.28543052077293396\n",
      "Epoch 2503, Loss: 0.9468075484037399, Final Batch Loss: 0.22469763457775116\n",
      "Epoch 2504, Loss: 0.914248451590538, Final Batch Loss: 0.20085062086582184\n",
      "Epoch 2505, Loss: 0.956519290804863, Final Batch Loss: 0.23947463929653168\n",
      "Epoch 2506, Loss: 0.9229840636253357, Final Batch Loss: 0.19274285435676575\n",
      "Epoch 2507, Loss: 0.9987805336713791, Final Batch Loss: 0.32353004813194275\n",
      "Epoch 2508, Loss: 1.008895367383957, Final Batch Loss: 0.3197447955608368\n",
      "Epoch 2509, Loss: 0.9373686760663986, Final Batch Loss: 0.21199266612529755\n",
      "Epoch 2510, Loss: 0.9338817298412323, Final Batch Loss: 0.26264292001724243\n",
      "Epoch 2511, Loss: 0.9895275682210922, Final Batch Loss: 0.24695666134357452\n",
      "Epoch 2512, Loss: 0.9205750226974487, Final Batch Loss: 0.24209089577198029\n",
      "Epoch 2513, Loss: 0.9675214886665344, Final Batch Loss: 0.27663207054138184\n",
      "Epoch 2514, Loss: 0.9390782862901688, Final Batch Loss: 0.19460079073905945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2515, Loss: 1.013528272509575, Final Batch Loss: 0.3110213577747345\n",
      "Epoch 2516, Loss: 0.9662924110889435, Final Batch Loss: 0.20019981265068054\n",
      "Epoch 2517, Loss: 0.9468155801296234, Final Batch Loss: 0.2824243903160095\n",
      "Epoch 2518, Loss: 0.9091734141111374, Final Batch Loss: 0.20173649489879608\n",
      "Epoch 2519, Loss: 0.872189000248909, Final Batch Loss: 0.1884387731552124\n",
      "Epoch 2520, Loss: 0.8752762824296951, Final Batch Loss: 0.15860897302627563\n",
      "Epoch 2521, Loss: 1.0100285410881042, Final Batch Loss: 0.30858129262924194\n",
      "Epoch 2522, Loss: 0.8967128843069077, Final Batch Loss: 0.21965280175209045\n",
      "Epoch 2523, Loss: 0.858071818947792, Final Batch Loss: 0.18145030736923218\n",
      "Epoch 2524, Loss: 0.8907857239246368, Final Batch Loss: 0.20337162911891937\n",
      "Epoch 2525, Loss: 0.9092346131801605, Final Batch Loss: 0.22912511229515076\n",
      "Epoch 2526, Loss: 0.8998075276613235, Final Batch Loss: 0.21527907252311707\n",
      "Epoch 2527, Loss: 1.0038045197725296, Final Batch Loss: 0.285141259431839\n",
      "Epoch 2528, Loss: 0.8684357553720474, Final Batch Loss: 0.17537973821163177\n",
      "Epoch 2529, Loss: 0.9167883992195129, Final Batch Loss: 0.16726040840148926\n",
      "Epoch 2530, Loss: 0.9418417364358902, Final Batch Loss: 0.235746830701828\n",
      "Epoch 2531, Loss: 0.9213032275438309, Final Batch Loss: 0.21542955935001373\n",
      "Epoch 2532, Loss: 0.8926301449537277, Final Batch Loss: 0.1595449000597\n",
      "Epoch 2533, Loss: 0.9400332719087601, Final Batch Loss: 0.21962697803974152\n",
      "Epoch 2534, Loss: 0.8844736814498901, Final Batch Loss: 0.19098281860351562\n",
      "Epoch 2535, Loss: 0.8946752995252609, Final Batch Loss: 0.23756587505340576\n",
      "Epoch 2536, Loss: 1.0214173346757889, Final Batch Loss: 0.28055843710899353\n",
      "Epoch 2537, Loss: 0.944517657160759, Final Batch Loss: 0.259823203086853\n",
      "Epoch 2538, Loss: 1.0021889954805374, Final Batch Loss: 0.27014169096946716\n",
      "Epoch 2539, Loss: 0.9030750542879105, Final Batch Loss: 0.19853049516677856\n",
      "Epoch 2540, Loss: 1.0016223043203354, Final Batch Loss: 0.2944674789905548\n",
      "Epoch 2541, Loss: 0.9295825213193893, Final Batch Loss: 0.27743640542030334\n",
      "Epoch 2542, Loss: 0.9519333094358444, Final Batch Loss: 0.25175634026527405\n",
      "Epoch 2543, Loss: 1.024769589304924, Final Batch Loss: 0.30089083313941956\n",
      "Epoch 2544, Loss: 0.8964884430170059, Final Batch Loss: 0.19222722947597504\n",
      "Epoch 2545, Loss: 0.9437779784202576, Final Batch Loss: 0.2600554823875427\n",
      "Epoch 2546, Loss: 0.9224576354026794, Final Batch Loss: 0.1372183859348297\n",
      "Epoch 2547, Loss: 1.0172121226787567, Final Batch Loss: 0.2812919318675995\n",
      "Epoch 2548, Loss: 0.9733308255672455, Final Batch Loss: 0.19268260896205902\n",
      "Epoch 2549, Loss: 0.9527670443058014, Final Batch Loss: 0.2520664632320404\n",
      "Epoch 2550, Loss: 0.9302109777927399, Final Batch Loss: 0.20354625582695007\n",
      "Epoch 2551, Loss: 0.9311798810958862, Final Batch Loss: 0.1970856785774231\n",
      "Epoch 2552, Loss: 1.0584253370761871, Final Batch Loss: 0.33729174733161926\n",
      "Epoch 2553, Loss: 0.9395885169506073, Final Batch Loss: 0.27337464690208435\n",
      "Epoch 2554, Loss: 0.9272676855325699, Final Batch Loss: 0.19677944481372833\n",
      "Epoch 2555, Loss: 0.9128768444061279, Final Batch Loss: 0.1742585152387619\n",
      "Epoch 2556, Loss: 1.0151958763599396, Final Batch Loss: 0.31998711824417114\n",
      "Epoch 2557, Loss: 0.9739430397748947, Final Batch Loss: 0.24701978266239166\n",
      "Epoch 2558, Loss: 0.9848255664110184, Final Batch Loss: 0.2619403004646301\n",
      "Epoch 2559, Loss: 1.015136942267418, Final Batch Loss: 0.3529912531375885\n",
      "Epoch 2560, Loss: 1.0097838640213013, Final Batch Loss: 0.2557586133480072\n",
      "Epoch 2561, Loss: 1.0379142612218857, Final Batch Loss: 0.32189804315567017\n",
      "Epoch 2562, Loss: 1.0635478496551514, Final Batch Loss: 0.31821632385253906\n",
      "Epoch 2563, Loss: 0.9516192078590393, Final Batch Loss: 0.26731887459754944\n",
      "Epoch 2564, Loss: 0.9780024737119675, Final Batch Loss: 0.27128130197525024\n",
      "Epoch 2565, Loss: 0.9986465275287628, Final Batch Loss: 0.332792729139328\n",
      "Epoch 2566, Loss: 0.9274382293224335, Final Batch Loss: 0.23586875200271606\n",
      "Epoch 2567, Loss: 1.0351476520299911, Final Batch Loss: 0.3181076645851135\n",
      "Epoch 2568, Loss: 0.9777478724718094, Final Batch Loss: 0.22954416275024414\n",
      "Epoch 2569, Loss: 0.9440616071224213, Final Batch Loss: 0.20179685950279236\n",
      "Epoch 2570, Loss: 0.9492541700601578, Final Batch Loss: 0.2186739593744278\n",
      "Epoch 2571, Loss: 0.8881269246339798, Final Batch Loss: 0.14116960763931274\n",
      "Epoch 2572, Loss: 0.9077126234769821, Final Batch Loss: 0.15861649811267853\n",
      "Epoch 2573, Loss: 0.9580546319484711, Final Batch Loss: 0.20524829626083374\n",
      "Epoch 2574, Loss: 0.9627116322517395, Final Batch Loss: 0.3275530934333801\n",
      "Epoch 2575, Loss: 0.8921050280332565, Final Batch Loss: 0.20411723852157593\n",
      "Epoch 2576, Loss: 0.918145552277565, Final Batch Loss: 0.22524486482143402\n",
      "Epoch 2577, Loss: 0.9268289804458618, Final Batch Loss: 0.24571001529693604\n",
      "Epoch 2578, Loss: 0.8777446150779724, Final Batch Loss: 0.16811653971672058\n",
      "Epoch 2579, Loss: 0.9660480916500092, Final Batch Loss: 0.22567340731620789\n",
      "Epoch 2580, Loss: 0.9278292953968048, Final Batch Loss: 0.19466710090637207\n",
      "Epoch 2581, Loss: 1.0421927273273468, Final Batch Loss: 0.35475197434425354\n",
      "Epoch 2582, Loss: 0.9796037077903748, Final Batch Loss: 0.2588348686695099\n",
      "Epoch 2583, Loss: 1.069078877568245, Final Batch Loss: 0.29414913058280945\n",
      "Epoch 2584, Loss: 0.8849859684705734, Final Batch Loss: 0.2393062263727188\n",
      "Epoch 2585, Loss: 0.869571790099144, Final Batch Loss: 0.18078047037124634\n",
      "Epoch 2586, Loss: 0.8728239238262177, Final Batch Loss: 0.1567840576171875\n",
      "Epoch 2587, Loss: 0.9971299469470978, Final Batch Loss: 0.28185608983039856\n",
      "Epoch 2588, Loss: 0.8633370995521545, Final Batch Loss: 0.1816757172346115\n",
      "Epoch 2589, Loss: 0.9177342355251312, Final Batch Loss: 0.18697132170200348\n",
      "Epoch 2590, Loss: 1.0166336745023727, Final Batch Loss: 0.3182497024536133\n",
      "Epoch 2591, Loss: 0.9366271495819092, Final Batch Loss: 0.19604142010211945\n",
      "Epoch 2592, Loss: 0.9624612480401993, Final Batch Loss: 0.27005523443222046\n",
      "Epoch 2593, Loss: 0.8899971097707748, Final Batch Loss: 0.17366544902324677\n",
      "Epoch 2594, Loss: 0.9472355395555496, Final Batch Loss: 0.2717365324497223\n",
      "Epoch 2595, Loss: 0.9158718585968018, Final Batch Loss: 0.21723495423793793\n",
      "Epoch 2596, Loss: 0.9612914323806763, Final Batch Loss: 0.24014312028884888\n",
      "Epoch 2597, Loss: 1.0246217399835587, Final Batch Loss: 0.21953731775283813\n",
      "Epoch 2598, Loss: 1.0212324559688568, Final Batch Loss: 0.3026593327522278\n",
      "Epoch 2599, Loss: 0.9568144530057907, Final Batch Loss: 0.2625664174556732\n",
      "Epoch 2600, Loss: 0.9694912433624268, Final Batch Loss: 0.28923872113227844\n",
      "Epoch 2601, Loss: 0.9396295100450516, Final Batch Loss: 0.3062639534473419\n",
      "Epoch 2602, Loss: 0.8651071488857269, Final Batch Loss: 0.19616736471652985\n",
      "Epoch 2603, Loss: 0.9414781183004379, Final Batch Loss: 0.21775391697883606\n",
      "Epoch 2604, Loss: 0.8768595457077026, Final Batch Loss: 0.18492496013641357\n",
      "Epoch 2605, Loss: 0.9665124118328094, Final Batch Loss: 0.22202573716640472\n",
      "Epoch 2606, Loss: 0.8510336577892303, Final Batch Loss: 0.170519158244133\n",
      "Epoch 2607, Loss: 0.9229897558689117, Final Batch Loss: 0.19180426001548767\n",
      "Epoch 2608, Loss: 0.8661069571971893, Final Batch Loss: 0.1646602898836136\n",
      "Epoch 2609, Loss: 0.9342277199029922, Final Batch Loss: 0.21292726695537567\n",
      "Epoch 2610, Loss: 0.8958283960819244, Final Batch Loss: 0.2303479015827179\n",
      "Epoch 2611, Loss: 0.8968449532985687, Final Batch Loss: 0.2025839239358902\n",
      "Epoch 2612, Loss: 0.9132803380489349, Final Batch Loss: 0.2199651002883911\n",
      "Epoch 2613, Loss: 0.9672027379274368, Final Batch Loss: 0.221811443567276\n",
      "Epoch 2614, Loss: 0.9057259410619736, Final Batch Loss: 0.20911195874214172\n",
      "Epoch 2615, Loss: 0.9065277129411697, Final Batch Loss: 0.26984211802482605\n",
      "Epoch 2616, Loss: 0.9175352305173874, Final Batch Loss: 0.1987626999616623\n",
      "Epoch 2617, Loss: 0.9231530576944351, Final Batch Loss: 0.21645885705947876\n",
      "Epoch 2618, Loss: 0.943893775343895, Final Batch Loss: 0.24648073315620422\n",
      "Epoch 2619, Loss: 0.9572596251964569, Final Batch Loss: 0.2530534565448761\n",
      "Epoch 2620, Loss: 0.9022872298955917, Final Batch Loss: 0.2029966562986374\n",
      "Epoch 2621, Loss: 0.945254921913147, Final Batch Loss: 0.23423048853874207\n",
      "Epoch 2622, Loss: 0.9365264475345612, Final Batch Loss: 0.28825843334198\n",
      "Epoch 2623, Loss: 0.9768299162387848, Final Batch Loss: 0.24100379645824432\n",
      "Epoch 2624, Loss: 0.9169548451900482, Final Batch Loss: 0.20055393874645233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2625, Loss: 0.8620655834674835, Final Batch Loss: 0.19211162626743317\n",
      "Epoch 2626, Loss: 0.9616694748401642, Final Batch Loss: 0.23180504143238068\n",
      "Epoch 2627, Loss: 0.904201552271843, Final Batch Loss: 0.19593898952007294\n",
      "Epoch 2628, Loss: 0.9014676213264465, Final Batch Loss: 0.2481919378042221\n",
      "Epoch 2629, Loss: 0.9716648012399673, Final Batch Loss: 0.3158128559589386\n",
      "Epoch 2630, Loss: 0.9280444532632828, Final Batch Loss: 0.2635231018066406\n",
      "Epoch 2631, Loss: 0.8840475827455521, Final Batch Loss: 0.19485948979854584\n",
      "Epoch 2632, Loss: 0.8756111115217209, Final Batch Loss: 0.18336492776870728\n",
      "Epoch 2633, Loss: 0.8611517548561096, Final Batch Loss: 0.21239039301872253\n",
      "Epoch 2634, Loss: 0.958286240696907, Final Batch Loss: 0.258306622505188\n",
      "Epoch 2635, Loss: 0.9309254884719849, Final Batch Loss: 0.20653894543647766\n",
      "Epoch 2636, Loss: 0.9119417667388916, Final Batch Loss: 0.2335873246192932\n",
      "Epoch 2637, Loss: 0.8973028510808945, Final Batch Loss: 0.2288697361946106\n",
      "Epoch 2638, Loss: 0.8515480309724808, Final Batch Loss: 0.18823181092739105\n",
      "Epoch 2639, Loss: 0.9000603258609772, Final Batch Loss: 0.21285377442836761\n",
      "Epoch 2640, Loss: 1.101698487997055, Final Batch Loss: 0.37779930233955383\n",
      "Epoch 2641, Loss: 0.926915243268013, Final Batch Loss: 0.2723199129104614\n",
      "Epoch 2642, Loss: 0.9228575825691223, Final Batch Loss: 0.2517487704753876\n",
      "Epoch 2643, Loss: 0.8991088569164276, Final Batch Loss: 0.15961439907550812\n",
      "Epoch 2644, Loss: 0.9651401042938232, Final Batch Loss: 0.28153473138809204\n",
      "Epoch 2645, Loss: 0.9866527616977692, Final Batch Loss: 0.2250850796699524\n",
      "Epoch 2646, Loss: 0.9223074615001678, Final Batch Loss: 0.24108324944972992\n",
      "Epoch 2647, Loss: 0.8273153007030487, Final Batch Loss: 0.12576928734779358\n",
      "Epoch 2648, Loss: 0.9330590665340424, Final Batch Loss: 0.2654683291912079\n",
      "Epoch 2649, Loss: 0.9008642137050629, Final Batch Loss: 0.18917526304721832\n",
      "Epoch 2650, Loss: 0.986768826842308, Final Batch Loss: 0.24316060543060303\n",
      "Epoch 2651, Loss: 0.9078932851552963, Final Batch Loss: 0.23534289002418518\n",
      "Epoch 2652, Loss: 0.9628539532423019, Final Batch Loss: 0.17767569422721863\n",
      "Epoch 2653, Loss: 0.9891027510166168, Final Batch Loss: 0.25519880652427673\n",
      "Epoch 2654, Loss: 0.8700704425573349, Final Batch Loss: 0.186578169465065\n",
      "Epoch 2655, Loss: 0.9792759269475937, Final Batch Loss: 0.22921742498874664\n",
      "Epoch 2656, Loss: 0.9643449634313583, Final Batch Loss: 0.26450714468955994\n",
      "Epoch 2657, Loss: 0.9650018662214279, Final Batch Loss: 0.2798755168914795\n",
      "Epoch 2658, Loss: 1.0003980249166489, Final Batch Loss: 0.2929721772670746\n",
      "Epoch 2659, Loss: 0.9970031827688217, Final Batch Loss: 0.20254360139369965\n",
      "Epoch 2660, Loss: 1.0774979293346405, Final Batch Loss: 0.35358312726020813\n",
      "Epoch 2661, Loss: 1.0758691728115082, Final Batch Loss: 0.30942532420158386\n",
      "Epoch 2662, Loss: 1.0309543758630753, Final Batch Loss: 0.22825556993484497\n",
      "Epoch 2663, Loss: 1.0301503241062164, Final Batch Loss: 0.23967087268829346\n",
      "Epoch 2664, Loss: 1.0122243463993073, Final Batch Loss: 0.3005939722061157\n",
      "Epoch 2665, Loss: 0.9945030361413956, Final Batch Loss: 0.20545724034309387\n",
      "Epoch 2666, Loss: 0.9863316267728806, Final Batch Loss: 0.24296173453330994\n",
      "Epoch 2667, Loss: 0.9011916667222977, Final Batch Loss: 0.20727317035198212\n",
      "Epoch 2668, Loss: 0.9866437911987305, Final Batch Loss: 0.201838880777359\n",
      "Epoch 2669, Loss: 0.8713512420654297, Final Batch Loss: 0.19911473989486694\n",
      "Epoch 2670, Loss: 0.991734504699707, Final Batch Loss: 0.2844908833503723\n",
      "Epoch 2671, Loss: 0.9360838979482651, Final Batch Loss: 0.21213294565677643\n",
      "Epoch 2672, Loss: 0.9566175192594528, Final Batch Loss: 0.22031863033771515\n",
      "Epoch 2673, Loss: 0.9265812486410141, Final Batch Loss: 0.1931256502866745\n",
      "Epoch 2674, Loss: 1.0076594948768616, Final Batch Loss: 0.23003293573856354\n",
      "Epoch 2675, Loss: 0.9325532466173172, Final Batch Loss: 0.2438598871231079\n",
      "Epoch 2676, Loss: 0.9715016484260559, Final Batch Loss: 0.28729745745658875\n",
      "Epoch 2677, Loss: 0.9988482445478439, Final Batch Loss: 0.29119861125946045\n",
      "Epoch 2678, Loss: 1.0150988101959229, Final Batch Loss: 0.3697700798511505\n",
      "Epoch 2679, Loss: 0.8134187310934067, Final Batch Loss: 0.12409406900405884\n",
      "Epoch 2680, Loss: 0.9415227621793747, Final Batch Loss: 0.22651144862174988\n",
      "Epoch 2681, Loss: 0.9241957813501358, Final Batch Loss: 0.18616290390491486\n",
      "Epoch 2682, Loss: 0.9784386605024338, Final Batch Loss: 0.32752224802970886\n",
      "Epoch 2683, Loss: 0.8434897512197495, Final Batch Loss: 0.20457075536251068\n",
      "Epoch 2684, Loss: 0.942225456237793, Final Batch Loss: 0.200308158993721\n",
      "Epoch 2685, Loss: 0.8874608129262924, Final Batch Loss: 0.21280793845653534\n",
      "Epoch 2686, Loss: 0.8693608492612839, Final Batch Loss: 0.189075767993927\n",
      "Epoch 2687, Loss: 1.004509523510933, Final Batch Loss: 0.27061569690704346\n",
      "Epoch 2688, Loss: 0.9135806262493134, Final Batch Loss: 0.24342210590839386\n",
      "Epoch 2689, Loss: 1.0456667393445969, Final Batch Loss: 0.322630912065506\n",
      "Epoch 2690, Loss: 0.876822367310524, Final Batch Loss: 0.21369469165802002\n",
      "Epoch 2691, Loss: 0.9535619020462036, Final Batch Loss: 0.3306475877761841\n",
      "Epoch 2692, Loss: 0.9502681791782379, Final Batch Loss: 0.17331483960151672\n",
      "Epoch 2693, Loss: 0.903702586889267, Final Batch Loss: 0.2224985510110855\n",
      "Epoch 2694, Loss: 1.071130707859993, Final Batch Loss: 0.3073454797267914\n",
      "Epoch 2695, Loss: 0.9706462174654007, Final Batch Loss: 0.27302029728889465\n",
      "Epoch 2696, Loss: 0.8831194490194321, Final Batch Loss: 0.16524025797843933\n",
      "Epoch 2697, Loss: 0.9229651540517807, Final Batch Loss: 0.2638016641139984\n",
      "Epoch 2698, Loss: 0.9127957224845886, Final Batch Loss: 0.1280895471572876\n",
      "Epoch 2699, Loss: 0.8724673390388489, Final Batch Loss: 0.182213693857193\n",
      "Epoch 2700, Loss: 0.8599163591861725, Final Batch Loss: 0.19377996027469635\n",
      "Epoch 2701, Loss: 0.9616352617740631, Final Batch Loss: 0.32070276141166687\n",
      "Epoch 2702, Loss: 0.9513634145259857, Final Batch Loss: 0.17302021384239197\n",
      "Epoch 2703, Loss: 0.875429630279541, Final Batch Loss: 0.17722319066524506\n",
      "Epoch 2704, Loss: 1.0390120148658752, Final Batch Loss: 0.3001764416694641\n",
      "Epoch 2705, Loss: 0.975420206785202, Final Batch Loss: 0.18784204125404358\n",
      "Epoch 2706, Loss: 0.970430850982666, Final Batch Loss: 0.23809374868869781\n",
      "Epoch 2707, Loss: 0.8592673093080521, Final Batch Loss: 0.16266413033008575\n",
      "Epoch 2708, Loss: 0.8520269244909286, Final Batch Loss: 0.18629704415798187\n",
      "Epoch 2709, Loss: 0.9227267056703568, Final Batch Loss: 0.2003183662891388\n",
      "Epoch 2710, Loss: 0.9097936600446701, Final Batch Loss: 0.2052868902683258\n",
      "Epoch 2711, Loss: 0.9031436294317245, Final Batch Loss: 0.17542022466659546\n",
      "Epoch 2712, Loss: 0.9810718446969986, Final Batch Loss: 0.25147682428359985\n",
      "Epoch 2713, Loss: 1.0101170092821121, Final Batch Loss: 0.30696627497673035\n",
      "Epoch 2714, Loss: 1.0663142800331116, Final Batch Loss: 0.3133549094200134\n",
      "Epoch 2715, Loss: 0.8286968916654587, Final Batch Loss: 0.13514158129692078\n",
      "Epoch 2716, Loss: 0.9197282642126083, Final Batch Loss: 0.22807924449443817\n",
      "Epoch 2717, Loss: 0.9901099354028702, Final Batch Loss: 0.3053935468196869\n",
      "Epoch 2718, Loss: 0.8657424002885818, Final Batch Loss: 0.18190883100032806\n",
      "Epoch 2719, Loss: 0.9741459786891937, Final Batch Loss: 0.25061559677124023\n",
      "Epoch 2720, Loss: 0.9999147057533264, Final Batch Loss: 0.2707415223121643\n",
      "Epoch 2721, Loss: 0.9524566978216171, Final Batch Loss: 0.22072936594486237\n",
      "Epoch 2722, Loss: 1.0039320886135101, Final Batch Loss: 0.26926174759864807\n",
      "Epoch 2723, Loss: 0.9069105386734009, Final Batch Loss: 0.23521827161312103\n",
      "Epoch 2724, Loss: 0.9145356863737106, Final Batch Loss: 0.27499374747276306\n",
      "Epoch 2725, Loss: 0.9875654131174088, Final Batch Loss: 0.3253808319568634\n",
      "Epoch 2726, Loss: 0.9089511036872864, Final Batch Loss: 0.19052112102508545\n",
      "Epoch 2727, Loss: 0.9613841772079468, Final Batch Loss: 0.23269669711589813\n",
      "Epoch 2728, Loss: 0.939332589507103, Final Batch Loss: 0.23891407251358032\n",
      "Epoch 2729, Loss: 0.9128335863351822, Final Batch Loss: 0.19066599011421204\n",
      "Epoch 2730, Loss: 0.8846109956502914, Final Batch Loss: 0.2404719591140747\n",
      "Epoch 2731, Loss: 0.9416133016347885, Final Batch Loss: 0.22979971766471863\n",
      "Epoch 2732, Loss: 0.8377242386341095, Final Batch Loss: 0.1760641634464264\n",
      "Epoch 2733, Loss: 0.8988889008760452, Final Batch Loss: 0.2212705761194229\n",
      "Epoch 2734, Loss: 0.9439910352230072, Final Batch Loss: 0.23580493032932281\n",
      "Epoch 2735, Loss: 0.9122558683156967, Final Batch Loss: 0.27578485012054443\n",
      "Epoch 2736, Loss: 0.9538464546203613, Final Batch Loss: 0.2719474732875824\n",
      "Epoch 2737, Loss: 0.960583359003067, Final Batch Loss: 0.23695790767669678\n",
      "Epoch 2738, Loss: 0.9894726425409317, Final Batch Loss: 0.3027641177177429\n",
      "Epoch 2739, Loss: 0.895179033279419, Final Batch Loss: 0.2414090484380722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2740, Loss: 0.9344638735055923, Final Batch Loss: 0.25078901648521423\n",
      "Epoch 2741, Loss: 0.8740935921669006, Final Batch Loss: 0.19749325513839722\n",
      "Epoch 2742, Loss: 0.9903148859739304, Final Batch Loss: 0.2512754499912262\n",
      "Epoch 2743, Loss: 0.9634308367967606, Final Batch Loss: 0.2836325168609619\n",
      "Epoch 2744, Loss: 0.9123431593179703, Final Batch Loss: 0.1829799860715866\n",
      "Epoch 2745, Loss: 0.8613248020410538, Final Batch Loss: 0.16870419681072235\n",
      "Epoch 2746, Loss: 0.9261563867330551, Final Batch Loss: 0.2571859359741211\n",
      "Epoch 2747, Loss: 0.9077745974063873, Final Batch Loss: 0.18466661870479584\n",
      "Epoch 2748, Loss: 0.8811950087547302, Final Batch Loss: 0.19524511694908142\n",
      "Epoch 2749, Loss: 0.9834341257810593, Final Batch Loss: 0.3431045711040497\n",
      "Epoch 2750, Loss: 1.00058114528656, Final Batch Loss: 0.3070675730705261\n",
      "Epoch 2751, Loss: 0.9328062832355499, Final Batch Loss: 0.23532575368881226\n",
      "Epoch 2752, Loss: 0.8880953639745712, Final Batch Loss: 0.13599200546741486\n",
      "Epoch 2753, Loss: 0.9375758320093155, Final Batch Loss: 0.24420902132987976\n",
      "Epoch 2754, Loss: 0.9393632411956787, Final Batch Loss: 0.23378899693489075\n",
      "Epoch 2755, Loss: 0.8873782306909561, Final Batch Loss: 0.22635281085968018\n",
      "Epoch 2756, Loss: 0.9274473786354065, Final Batch Loss: 0.24360956251621246\n",
      "Epoch 2757, Loss: 0.9739020615816116, Final Batch Loss: 0.21650898456573486\n",
      "Epoch 2758, Loss: 0.9625409990549088, Final Batch Loss: 0.20418816804885864\n",
      "Epoch 2759, Loss: 0.9542737603187561, Final Batch Loss: 0.3117607533931732\n",
      "Epoch 2760, Loss: 0.8649455159902573, Final Batch Loss: 0.1948041319847107\n",
      "Epoch 2761, Loss: 0.864152804017067, Final Batch Loss: 0.18685263395309448\n",
      "Epoch 2762, Loss: 0.9550042897462845, Final Batch Loss: 0.20406316220760345\n",
      "Epoch 2763, Loss: 0.9508959203958511, Final Batch Loss: 0.2626473009586334\n",
      "Epoch 2764, Loss: 0.9728839993476868, Final Batch Loss: 0.28315412998199463\n",
      "Epoch 2765, Loss: 0.9351306259632111, Final Batch Loss: 0.24738328158855438\n",
      "Epoch 2766, Loss: 1.023203656077385, Final Batch Loss: 0.30807068943977356\n",
      "Epoch 2767, Loss: 0.9707293957471848, Final Batch Loss: 0.21812449395656586\n",
      "Epoch 2768, Loss: 0.9139498472213745, Final Batch Loss: 0.22628171741962433\n",
      "Epoch 2769, Loss: 0.9014723002910614, Final Batch Loss: 0.23252147436141968\n",
      "Epoch 2770, Loss: 0.9431266635656357, Final Batch Loss: 0.25586724281311035\n",
      "Epoch 2771, Loss: 0.9479248374700546, Final Batch Loss: 0.28690779209136963\n",
      "Epoch 2772, Loss: 0.9471366107463837, Final Batch Loss: 0.18463005125522614\n",
      "Epoch 2773, Loss: 0.9754994660615921, Final Batch Loss: 0.2781718373298645\n",
      "Epoch 2774, Loss: 0.8610154837369919, Final Batch Loss: 0.20668329298496246\n",
      "Epoch 2775, Loss: 0.8963531255722046, Final Batch Loss: 0.19188019633293152\n",
      "Epoch 2776, Loss: 0.993569016456604, Final Batch Loss: 0.276432603597641\n",
      "Epoch 2777, Loss: 0.8725441694259644, Final Batch Loss: 0.1988002210855484\n",
      "Epoch 2778, Loss: 0.9800632148981094, Final Batch Loss: 0.2804582417011261\n",
      "Epoch 2779, Loss: 0.8004001677036285, Final Batch Loss: 0.17385755479335785\n",
      "Epoch 2780, Loss: 0.9503455311059952, Final Batch Loss: 0.2573937773704529\n",
      "Epoch 2781, Loss: 0.9677560329437256, Final Batch Loss: 0.31418296694755554\n",
      "Epoch 2782, Loss: 0.9487665444612503, Final Batch Loss: 0.2419506460428238\n",
      "Epoch 2783, Loss: 0.9033495187759399, Final Batch Loss: 0.21637554466724396\n",
      "Epoch 2784, Loss: 1.0554719269275665, Final Batch Loss: 0.3692854046821594\n",
      "Epoch 2785, Loss: 0.8636095225811005, Final Batch Loss: 0.18243597447872162\n",
      "Epoch 2786, Loss: 0.9634931683540344, Final Batch Loss: 0.24781562387943268\n",
      "Epoch 2787, Loss: 0.8975766897201538, Final Batch Loss: 0.20132949948310852\n",
      "Epoch 2788, Loss: 0.966658815741539, Final Batch Loss: 0.2559572160243988\n",
      "Epoch 2789, Loss: 0.921499952673912, Final Batch Loss: 0.24279309809207916\n",
      "Epoch 2790, Loss: 0.8477573841810226, Final Batch Loss: 0.18092474341392517\n",
      "Epoch 2791, Loss: 0.9270024299621582, Final Batch Loss: 0.27338504791259766\n",
      "Epoch 2792, Loss: 0.859979584813118, Final Batch Loss: 0.1993211805820465\n",
      "Epoch 2793, Loss: 1.0711701810359955, Final Batch Loss: 0.29463693499565125\n",
      "Epoch 2794, Loss: 0.9575331509113312, Final Batch Loss: 0.23243124783039093\n",
      "Epoch 2795, Loss: 0.9550992995500565, Final Batch Loss: 0.2361474484205246\n",
      "Epoch 2796, Loss: 1.0003372430801392, Final Batch Loss: 0.28461530804634094\n",
      "Epoch 2797, Loss: 0.9160849452018738, Final Batch Loss: 0.2470909208059311\n",
      "Epoch 2798, Loss: 0.8852124363183975, Final Batch Loss: 0.19071289896965027\n",
      "Epoch 2799, Loss: 0.9247722923755646, Final Batch Loss: 0.2535097002983093\n",
      "Epoch 2800, Loss: 0.9033442437648773, Final Batch Loss: 0.19200359284877777\n",
      "Epoch 2801, Loss: 0.9290942698717117, Final Batch Loss: 0.24846990406513214\n",
      "Epoch 2802, Loss: 0.9752262532711029, Final Batch Loss: 0.25744694471359253\n",
      "Epoch 2803, Loss: 1.0318838059902191, Final Batch Loss: 0.3205532431602478\n",
      "Epoch 2804, Loss: 0.892015665769577, Final Batch Loss: 0.2346014529466629\n",
      "Epoch 2805, Loss: 0.9250378012657166, Final Batch Loss: 0.255104660987854\n",
      "Epoch 2806, Loss: 0.9295789897441864, Final Batch Loss: 0.21687227487564087\n",
      "Epoch 2807, Loss: 0.9145153909921646, Final Batch Loss: 0.23969149589538574\n",
      "Epoch 2808, Loss: 0.8438362926244736, Final Batch Loss: 0.17671987414360046\n",
      "Epoch 2809, Loss: 1.0423740893602371, Final Batch Loss: 0.35548701882362366\n",
      "Epoch 2810, Loss: 0.9035453349351883, Final Batch Loss: 0.21458178758621216\n",
      "Epoch 2811, Loss: 0.9059530049562454, Final Batch Loss: 0.2013978213071823\n",
      "Epoch 2812, Loss: 0.9101439416408539, Final Batch Loss: 0.23471805453300476\n",
      "Epoch 2813, Loss: 0.957015872001648, Final Batch Loss: 0.2659529745578766\n",
      "Epoch 2814, Loss: 0.9218155294656754, Final Batch Loss: 0.23714420199394226\n",
      "Epoch 2815, Loss: 0.9906700700521469, Final Batch Loss: 0.3198700249195099\n",
      "Epoch 2816, Loss: 0.9457909762859344, Final Batch Loss: 0.24103279411792755\n",
      "Epoch 2817, Loss: 0.9676803946495056, Final Batch Loss: 0.26579225063323975\n",
      "Epoch 2818, Loss: 0.9471403211355209, Final Batch Loss: 0.2871532142162323\n",
      "Epoch 2819, Loss: 0.8810691982507706, Final Batch Loss: 0.19341720640659332\n",
      "Epoch 2820, Loss: 1.0614537447690964, Final Batch Loss: 0.2807747721672058\n",
      "Epoch 2821, Loss: 0.8597798645496368, Final Batch Loss: 0.17741720378398895\n",
      "Epoch 2822, Loss: 0.9528712034225464, Final Batch Loss: 0.1801498532295227\n",
      "Epoch 2823, Loss: 0.9864340126514435, Final Batch Loss: 0.19622235000133514\n",
      "Epoch 2824, Loss: 0.8886671960353851, Final Batch Loss: 0.14110733568668365\n",
      "Epoch 2825, Loss: 0.9384700506925583, Final Batch Loss: 0.24643084406852722\n",
      "Epoch 2826, Loss: 0.9365575462579727, Final Batch Loss: 0.25113818049430847\n",
      "Epoch 2827, Loss: 0.893695205450058, Final Batch Loss: 0.19093048572540283\n",
      "Epoch 2828, Loss: 0.96846704185009, Final Batch Loss: 0.24099740386009216\n",
      "Epoch 2829, Loss: 0.9054501354694366, Final Batch Loss: 0.20341284573078156\n",
      "Epoch 2830, Loss: 0.880450889468193, Final Batch Loss: 0.17078861594200134\n",
      "Epoch 2831, Loss: 0.9301855862140656, Final Batch Loss: 0.2500585913658142\n",
      "Epoch 2832, Loss: 0.9739154875278473, Final Batch Loss: 0.3067505657672882\n",
      "Epoch 2833, Loss: 0.9433145374059677, Final Batch Loss: 0.24965374171733856\n",
      "Epoch 2834, Loss: 0.961117297410965, Final Batch Loss: 0.3113134503364563\n",
      "Epoch 2835, Loss: 1.0423395186662674, Final Batch Loss: 0.33208155632019043\n",
      "Epoch 2836, Loss: 0.9127912521362305, Final Batch Loss: 0.22855857014656067\n",
      "Epoch 2837, Loss: 0.9133232086896896, Final Batch Loss: 0.22233469784259796\n",
      "Epoch 2838, Loss: 0.8844310939311981, Final Batch Loss: 0.24406912922859192\n",
      "Epoch 2839, Loss: 1.0295803099870682, Final Batch Loss: 0.24552597105503082\n",
      "Epoch 2840, Loss: 1.0331991910934448, Final Batch Loss: 0.29805803298950195\n",
      "Epoch 2841, Loss: 0.9230266213417053, Final Batch Loss: 0.1878267377614975\n",
      "Epoch 2842, Loss: 0.8720633834600449, Final Batch Loss: 0.20771212875843048\n",
      "Epoch 2843, Loss: 0.9058318585157394, Final Batch Loss: 0.21662890911102295\n",
      "Epoch 2844, Loss: 0.9982016682624817, Final Batch Loss: 0.3112390637397766\n",
      "Epoch 2845, Loss: 0.8784618675708771, Final Batch Loss: 0.14191901683807373\n",
      "Epoch 2846, Loss: 0.9721382558345795, Final Batch Loss: 0.2479841262102127\n",
      "Epoch 2847, Loss: 0.9043156802654266, Final Batch Loss: 0.21739672124385834\n",
      "Epoch 2848, Loss: 0.893772304058075, Final Batch Loss: 0.22953540086746216\n",
      "Epoch 2849, Loss: 0.8905016779899597, Final Batch Loss: 0.21758513152599335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2850, Loss: 0.8988686949014664, Final Batch Loss: 0.24511733651161194\n",
      "Epoch 2851, Loss: 0.8726103007793427, Final Batch Loss: 0.20269708335399628\n",
      "Epoch 2852, Loss: 0.9385572373867035, Final Batch Loss: 0.2576256990432739\n",
      "Epoch 2853, Loss: 0.8410378843545914, Final Batch Loss: 0.15022335946559906\n",
      "Epoch 2854, Loss: 0.9377688318490982, Final Batch Loss: 0.23049338161945343\n",
      "Epoch 2855, Loss: 0.8882124423980713, Final Batch Loss: 0.2078896462917328\n",
      "Epoch 2856, Loss: 0.9719339162111282, Final Batch Loss: 0.22819414734840393\n",
      "Epoch 2857, Loss: 0.9214855283498764, Final Batch Loss: 0.20948566496372223\n",
      "Epoch 2858, Loss: 0.925577387213707, Final Batch Loss: 0.22933226823806763\n",
      "Epoch 2859, Loss: 0.9333908259868622, Final Batch Loss: 0.25125765800476074\n",
      "Epoch 2860, Loss: 0.9095146656036377, Final Batch Loss: 0.19227918982505798\n",
      "Epoch 2861, Loss: 0.8920131325721741, Final Batch Loss: 0.2269531935453415\n",
      "Epoch 2862, Loss: 1.0380722433328629, Final Batch Loss: 0.3311086595058441\n",
      "Epoch 2863, Loss: 0.8839239925146103, Final Batch Loss: 0.18940745294094086\n",
      "Epoch 2864, Loss: 0.942002609372139, Final Batch Loss: 0.25106391310691833\n",
      "Epoch 2865, Loss: 0.9164976328611374, Final Batch Loss: 0.22644959390163422\n",
      "Epoch 2866, Loss: 0.8905957788228989, Final Batch Loss: 0.21416495740413666\n",
      "Epoch 2867, Loss: 1.0837966203689575, Final Batch Loss: 0.2828187942504883\n",
      "Epoch 2868, Loss: 0.9490139484405518, Final Batch Loss: 0.15177075564861298\n",
      "Epoch 2869, Loss: 0.9700384140014648, Final Batch Loss: 0.2968144416809082\n",
      "Epoch 2870, Loss: 0.9136245250701904, Final Batch Loss: 0.25615522265434265\n",
      "Epoch 2871, Loss: 0.9376446157693863, Final Batch Loss: 0.2501746118068695\n",
      "Epoch 2872, Loss: 1.0055015683174133, Final Batch Loss: 0.27879974246025085\n",
      "Epoch 2873, Loss: 1.0359033793210983, Final Batch Loss: 0.4165242910385132\n",
      "Epoch 2874, Loss: 1.0347184985876083, Final Batch Loss: 0.2558077573776245\n",
      "Epoch 2875, Loss: 0.8849072754383087, Final Batch Loss: 0.22264376282691956\n",
      "Epoch 2876, Loss: 0.9236837774515152, Final Batch Loss: 0.2894047200679779\n",
      "Epoch 2877, Loss: 0.8278641402721405, Final Batch Loss: 0.14711502194404602\n",
      "Epoch 2878, Loss: 0.9877814054489136, Final Batch Loss: 0.239701047539711\n",
      "Epoch 2879, Loss: 0.9451769143342972, Final Batch Loss: 0.3254902958869934\n",
      "Epoch 2880, Loss: 0.9430270940065384, Final Batch Loss: 0.2948448956012726\n",
      "Epoch 2881, Loss: 0.9605470448732376, Final Batch Loss: 0.24417084455490112\n",
      "Epoch 2882, Loss: 0.9493234604597092, Final Batch Loss: 0.2746115028858185\n",
      "Epoch 2883, Loss: 0.8980636894702911, Final Batch Loss: 0.1914345920085907\n",
      "Epoch 2884, Loss: 0.8831114321947098, Final Batch Loss: 0.2438966929912567\n",
      "Epoch 2885, Loss: 0.912709653377533, Final Batch Loss: 0.1846213936805725\n",
      "Epoch 2886, Loss: 0.8766912221908569, Final Batch Loss: 0.23331812024116516\n",
      "Epoch 2887, Loss: 0.9367331713438034, Final Batch Loss: 0.25855737924575806\n",
      "Epoch 2888, Loss: 0.9595669358968735, Final Batch Loss: 0.16459441184997559\n",
      "Epoch 2889, Loss: 0.9775575697422028, Final Batch Loss: 0.328046977519989\n",
      "Epoch 2890, Loss: 0.8370095789432526, Final Batch Loss: 0.18900711834430695\n",
      "Epoch 2891, Loss: 0.8664941787719727, Final Batch Loss: 0.17949992418289185\n",
      "Epoch 2892, Loss: 0.9311545640230179, Final Batch Loss: 0.14995643496513367\n",
      "Epoch 2893, Loss: 0.9368537366390228, Final Batch Loss: 0.24888797104358673\n",
      "Epoch 2894, Loss: 0.9147582650184631, Final Batch Loss: 0.23457098007202148\n",
      "Epoch 2895, Loss: 0.8706825971603394, Final Batch Loss: 0.20059290528297424\n",
      "Epoch 2896, Loss: 0.9256739914417267, Final Batch Loss: 0.17283381521701813\n",
      "Epoch 2897, Loss: 0.8504899144172668, Final Batch Loss: 0.15029411017894745\n",
      "Epoch 2898, Loss: 0.9420711249113083, Final Batch Loss: 0.2957298457622528\n",
      "Epoch 2899, Loss: 0.9418940991163254, Final Batch Loss: 0.2524324357509613\n",
      "Epoch 2900, Loss: 0.8411373496055603, Final Batch Loss: 0.10377311706542969\n",
      "Epoch 2901, Loss: 0.906650573015213, Final Batch Loss: 0.20527678728103638\n",
      "Epoch 2902, Loss: 0.9036925137042999, Final Batch Loss: 0.24690158665180206\n",
      "Epoch 2903, Loss: 0.893982008099556, Final Batch Loss: 0.16991840302944183\n",
      "Epoch 2904, Loss: 0.9816510677337646, Final Batch Loss: 0.3210577964782715\n",
      "Epoch 2905, Loss: 0.9937483966350555, Final Batch Loss: 0.3117353916168213\n",
      "Epoch 2906, Loss: 0.9398936331272125, Final Batch Loss: 0.21198542416095734\n",
      "Epoch 2907, Loss: 1.0638865232467651, Final Batch Loss: 0.3575650453567505\n",
      "Epoch 2908, Loss: 0.9081322997808456, Final Batch Loss: 0.19790099561214447\n",
      "Epoch 2909, Loss: 0.9004078209400177, Final Batch Loss: 0.23018129169940948\n",
      "Epoch 2910, Loss: 0.8149342387914658, Final Batch Loss: 0.13283592462539673\n",
      "Epoch 2911, Loss: 0.8664830327033997, Final Batch Loss: 0.19655275344848633\n",
      "Epoch 2912, Loss: 0.8730112314224243, Final Batch Loss: 0.17967548966407776\n",
      "Epoch 2913, Loss: 0.8713234513998032, Final Batch Loss: 0.18060488998889923\n",
      "Epoch 2914, Loss: 0.962246373295784, Final Batch Loss: 0.2780107855796814\n",
      "Epoch 2915, Loss: 0.8879323601722717, Final Batch Loss: 0.15347659587860107\n",
      "Epoch 2916, Loss: 0.8842456787824631, Final Batch Loss: 0.19982585310935974\n",
      "Epoch 2917, Loss: 0.9850887656211853, Final Batch Loss: 0.2214130014181137\n",
      "Epoch 2918, Loss: 0.9245513379573822, Final Batch Loss: 0.2635740041732788\n",
      "Epoch 2919, Loss: 0.9349756091833115, Final Batch Loss: 0.2991423010826111\n",
      "Epoch 2920, Loss: 0.8142580389976501, Final Batch Loss: 0.13348177075386047\n",
      "Epoch 2921, Loss: 0.945202961564064, Final Batch Loss: 0.2617040276527405\n",
      "Epoch 2922, Loss: 0.86021788418293, Final Batch Loss: 0.19737383723258972\n",
      "Epoch 2923, Loss: 0.8388732820749283, Final Batch Loss: 0.22700707614421844\n",
      "Epoch 2924, Loss: 0.9568423181772232, Final Batch Loss: 0.2748372554779053\n",
      "Epoch 2925, Loss: 0.8764755129814148, Final Batch Loss: 0.24649527668952942\n",
      "Epoch 2926, Loss: 0.8660477548837662, Final Batch Loss: 0.20786000788211823\n",
      "Epoch 2927, Loss: 0.932112455368042, Final Batch Loss: 0.24832241237163544\n",
      "Epoch 2928, Loss: 0.917563870549202, Final Batch Loss: 0.18714269995689392\n",
      "Epoch 2929, Loss: 0.8792066425085068, Final Batch Loss: 0.2377757877111435\n",
      "Epoch 2930, Loss: 0.8937993049621582, Final Batch Loss: 0.23024962842464447\n",
      "Epoch 2931, Loss: 0.9569540768861771, Final Batch Loss: 0.2225027233362198\n",
      "Epoch 2932, Loss: 0.9517771750688553, Final Batch Loss: 0.32544559240341187\n",
      "Epoch 2933, Loss: 0.9268010854721069, Final Batch Loss: 0.3095865249633789\n",
      "Epoch 2934, Loss: 0.9070492684841156, Final Batch Loss: 0.21780802309513092\n",
      "Epoch 2935, Loss: 0.8928785473108292, Final Batch Loss: 0.21613457798957825\n",
      "Epoch 2936, Loss: 0.8505091965198517, Final Batch Loss: 0.20498868823051453\n",
      "Epoch 2937, Loss: 0.9762857407331467, Final Batch Loss: 0.2657839059829712\n",
      "Epoch 2938, Loss: 0.9476242214441299, Final Batch Loss: 0.29713353514671326\n",
      "Epoch 2939, Loss: 0.899119108915329, Final Batch Loss: 0.2162449210882187\n",
      "Epoch 2940, Loss: 0.9123919010162354, Final Batch Loss: 0.22714099287986755\n",
      "Epoch 2941, Loss: 0.8945698440074921, Final Batch Loss: 0.23995260894298553\n",
      "Epoch 2942, Loss: 0.9515617340803146, Final Batch Loss: 0.21478673815727234\n",
      "Epoch 2943, Loss: 0.9110463410615921, Final Batch Loss: 0.2623889148235321\n",
      "Epoch 2944, Loss: 0.9926299899816513, Final Batch Loss: 0.22409607470035553\n",
      "Epoch 2945, Loss: 0.9554498642683029, Final Batch Loss: 0.23767222464084625\n",
      "Epoch 2946, Loss: 0.9040926247835159, Final Batch Loss: 0.2070646584033966\n",
      "Epoch 2947, Loss: 0.9930458813905716, Final Batch Loss: 0.28643885254859924\n",
      "Epoch 2948, Loss: 0.8698144853115082, Final Batch Loss: 0.20022903382778168\n",
      "Epoch 2949, Loss: 0.9654952585697174, Final Batch Loss: 0.25216934084892273\n",
      "Epoch 2950, Loss: 0.8797207474708557, Final Batch Loss: 0.17843371629714966\n",
      "Epoch 2951, Loss: 0.8880864679813385, Final Batch Loss: 0.1491098552942276\n",
      "Epoch 2952, Loss: 0.9416818767786026, Final Batch Loss: 0.26287996768951416\n",
      "Epoch 2953, Loss: 0.9549142271280289, Final Batch Loss: 0.2161846160888672\n",
      "Epoch 2954, Loss: 0.9868746101856232, Final Batch Loss: 0.28713715076446533\n",
      "Epoch 2955, Loss: 0.9401264488697052, Final Batch Loss: 0.2212308794260025\n",
      "Epoch 2956, Loss: 0.8982144445180893, Final Batch Loss: 0.23182739317417145\n",
      "Epoch 2957, Loss: 0.8944922089576721, Final Batch Loss: 0.20111675560474396\n",
      "Epoch 2958, Loss: 0.9010243266820908, Final Batch Loss: 0.23246073722839355\n",
      "Epoch 2959, Loss: 0.9787398725748062, Final Batch Loss: 0.35718441009521484\n",
      "Epoch 2960, Loss: 0.9501502066850662, Final Batch Loss: 0.252300888299942\n",
      "Epoch 2961, Loss: 0.9582063406705856, Final Batch Loss: 0.23562003672122955\n",
      "Epoch 2962, Loss: 0.9623857736587524, Final Batch Loss: 0.34294384717941284\n",
      "Epoch 2963, Loss: 0.8306551277637482, Final Batch Loss: 0.18848182260990143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2964, Loss: 0.9466012567281723, Final Batch Loss: 0.20400989055633545\n",
      "Epoch 2965, Loss: 0.9507174491882324, Final Batch Loss: 0.23470033705234528\n",
      "Epoch 2966, Loss: 0.9778401851654053, Final Batch Loss: 0.23936259746551514\n",
      "Epoch 2967, Loss: 0.9048690646886826, Final Batch Loss: 0.2414432168006897\n",
      "Epoch 2968, Loss: 1.0197776556015015, Final Batch Loss: 0.25683093070983887\n",
      "Epoch 2969, Loss: 0.9523288011550903, Final Batch Loss: 0.1920880377292633\n",
      "Epoch 2970, Loss: 0.8898872435092926, Final Batch Loss: 0.26317599415779114\n",
      "Epoch 2971, Loss: 0.985097736120224, Final Batch Loss: 0.30091536045074463\n",
      "Epoch 2972, Loss: 1.016625627875328, Final Batch Loss: 0.2801344394683838\n",
      "Epoch 2973, Loss: 0.9721071422100067, Final Batch Loss: 0.31178873777389526\n",
      "Epoch 2974, Loss: 0.9589461386203766, Final Batch Loss: 0.22330927848815918\n",
      "Epoch 2975, Loss: 0.8619244694709778, Final Batch Loss: 0.2118237018585205\n",
      "Epoch 2976, Loss: 0.85517318546772, Final Batch Loss: 0.20528554916381836\n",
      "Epoch 2977, Loss: 0.8628035634756088, Final Batch Loss: 0.1837766021490097\n",
      "Epoch 2978, Loss: 0.9410805851221085, Final Batch Loss: 0.2481396645307541\n",
      "Epoch 2979, Loss: 0.8767769783735275, Final Batch Loss: 0.2408275157213211\n",
      "Epoch 2980, Loss: 0.9911560714244843, Final Batch Loss: 0.33082839846611023\n",
      "Epoch 2981, Loss: 0.9393424987792969, Final Batch Loss: 0.16913141310214996\n",
      "Epoch 2982, Loss: 0.9167561233043671, Final Batch Loss: 0.20272842049598694\n",
      "Epoch 2983, Loss: 0.9423740655183792, Final Batch Loss: 0.2822144329547882\n",
      "Epoch 2984, Loss: 0.8756877481937408, Final Batch Loss: 0.19020259380340576\n",
      "Epoch 2985, Loss: 0.8800720125436783, Final Batch Loss: 0.19903992116451263\n",
      "Epoch 2986, Loss: 0.8573008030653, Final Batch Loss: 0.20727978646755219\n",
      "Epoch 2987, Loss: 0.947659820318222, Final Batch Loss: 0.2604711651802063\n",
      "Epoch 2988, Loss: 0.8610805720090866, Final Batch Loss: 0.1711549013853073\n",
      "Epoch 2989, Loss: 0.8972339183092117, Final Batch Loss: 0.2669845223426819\n",
      "Epoch 2990, Loss: 0.9187799543142319, Final Batch Loss: 0.3069058656692505\n",
      "Epoch 2991, Loss: 0.9176339954137802, Final Batch Loss: 0.2754219174385071\n",
      "Epoch 2992, Loss: 0.9069574326276779, Final Batch Loss: 0.22763323783874512\n",
      "Epoch 2993, Loss: 0.894182562828064, Final Batch Loss: 0.2029007077217102\n",
      "Epoch 2994, Loss: 0.8753118216991425, Final Batch Loss: 0.2049293965101242\n",
      "Epoch 2995, Loss: 0.8817073255777359, Final Batch Loss: 0.21164239943027496\n",
      "Epoch 2996, Loss: 0.9395680874586105, Final Batch Loss: 0.23997145891189575\n",
      "Epoch 2997, Loss: 0.9168476015329361, Final Batch Loss: 0.20379997789859772\n",
      "Epoch 2998, Loss: 0.8575996905565262, Final Batch Loss: 0.16466376185417175\n",
      "Epoch 2999, Loss: 0.9651547372341156, Final Batch Loss: 0.23761631548404694\n",
      "Epoch 3000, Loss: 0.8737886250019073, Final Batch Loss: 0.17625334858894348\n",
      "Epoch 3001, Loss: 0.87193264067173, Final Batch Loss: 0.2674187123775482\n",
      "Epoch 3002, Loss: 0.8959415405988693, Final Batch Loss: 0.20830053091049194\n",
      "Epoch 3003, Loss: 0.8551780134439468, Final Batch Loss: 0.23126456141471863\n",
      "Epoch 3004, Loss: 0.902106523513794, Final Batch Loss: 0.25975653529167175\n",
      "Epoch 3005, Loss: 0.8917833864688873, Final Batch Loss: 0.16517981886863708\n",
      "Epoch 3006, Loss: 0.9369387328624725, Final Batch Loss: 0.25812867283821106\n",
      "Epoch 3007, Loss: 0.9458061456680298, Final Batch Loss: 0.19850152730941772\n",
      "Epoch 3008, Loss: 0.9200546145439148, Final Batch Loss: 0.15247483551502228\n",
      "Epoch 3009, Loss: 0.9344025552272797, Final Batch Loss: 0.2705591917037964\n",
      "Epoch 3010, Loss: 0.9483824521303177, Final Batch Loss: 0.2583494782447815\n",
      "Epoch 3011, Loss: 0.8731749951839447, Final Batch Loss: 0.20192137360572815\n",
      "Epoch 3012, Loss: 0.9254176765680313, Final Batch Loss: 0.19817152619361877\n",
      "Epoch 3013, Loss: 1.0553407967090607, Final Batch Loss: 0.2771050035953522\n",
      "Epoch 3014, Loss: 0.8929054439067841, Final Batch Loss: 0.22529034316539764\n",
      "Epoch 3015, Loss: 0.8454212248325348, Final Batch Loss: 0.23772576451301575\n",
      "Epoch 3016, Loss: 0.9136282801628113, Final Batch Loss: 0.2510697841644287\n",
      "Epoch 3017, Loss: 0.9090548157691956, Final Batch Loss: 0.2135036587715149\n",
      "Epoch 3018, Loss: 0.889296144247055, Final Batch Loss: 0.19988813996315002\n",
      "Epoch 3019, Loss: 0.935186356306076, Final Batch Loss: 0.296138733625412\n",
      "Epoch 3020, Loss: 0.8801146745681763, Final Batch Loss: 0.2518558204174042\n",
      "Epoch 3021, Loss: 0.8630154728889465, Final Batch Loss: 0.20442479848861694\n",
      "Epoch 3022, Loss: 0.9842653125524521, Final Batch Loss: 0.2289661318063736\n",
      "Epoch 3023, Loss: 0.9180139899253845, Final Batch Loss: 0.23238451778888702\n",
      "Epoch 3024, Loss: 0.9793370068073273, Final Batch Loss: 0.25783026218414307\n",
      "Epoch 3025, Loss: 1.006494477391243, Final Batch Loss: 0.3194616138935089\n",
      "Epoch 3026, Loss: 0.8511823266744614, Final Batch Loss: 0.2214735746383667\n",
      "Epoch 3027, Loss: 0.9231095314025879, Final Batch Loss: 0.20284579694271088\n",
      "Epoch 3028, Loss: 0.8571884632110596, Final Batch Loss: 0.20934484899044037\n",
      "Epoch 3029, Loss: 0.9290093630552292, Final Batch Loss: 0.22571197152137756\n",
      "Epoch 3030, Loss: 0.945583239197731, Final Batch Loss: 0.2714677155017853\n",
      "Epoch 3031, Loss: 0.9337367117404938, Final Batch Loss: 0.18748679757118225\n",
      "Epoch 3032, Loss: 0.9124388694763184, Final Batch Loss: 0.2167419195175171\n",
      "Epoch 3033, Loss: 1.0179199278354645, Final Batch Loss: 0.2739012837409973\n",
      "Epoch 3034, Loss: 0.9632177948951721, Final Batch Loss: 0.2579098343849182\n",
      "Epoch 3035, Loss: 0.986813023686409, Final Batch Loss: 0.25142496824264526\n",
      "Epoch 3036, Loss: 0.9070552736520767, Final Batch Loss: 0.18670065701007843\n",
      "Epoch 3037, Loss: 0.9510960578918457, Final Batch Loss: 0.2225644737482071\n",
      "Epoch 3038, Loss: 0.9827651530504227, Final Batch Loss: 0.24194402992725372\n",
      "Epoch 3039, Loss: 0.8534164875745773, Final Batch Loss: 0.16715578734874725\n",
      "Epoch 3040, Loss: 0.8957752734422684, Final Batch Loss: 0.21607428789138794\n",
      "Epoch 3041, Loss: 0.9794910550117493, Final Batch Loss: 0.2704854905605316\n",
      "Epoch 3042, Loss: 0.9233071953058243, Final Batch Loss: 0.24999545514583588\n",
      "Epoch 3043, Loss: 0.946278527379036, Final Batch Loss: 0.3035982847213745\n",
      "Epoch 3044, Loss: 0.896001324057579, Final Batch Loss: 0.23822017014026642\n",
      "Epoch 3045, Loss: 0.9878434538841248, Final Batch Loss: 0.30441486835479736\n",
      "Epoch 3046, Loss: 0.9192130714654922, Final Batch Loss: 0.2779390811920166\n",
      "Epoch 3047, Loss: 0.9913655072450638, Final Batch Loss: 0.27074745297431946\n",
      "Epoch 3048, Loss: 0.9109187871217728, Final Batch Loss: 0.28254234790802\n",
      "Epoch 3049, Loss: 0.9066239446401596, Final Batch Loss: 0.2465030550956726\n",
      "Epoch 3050, Loss: 0.8447953909635544, Final Batch Loss: 0.14099112153053284\n",
      "Epoch 3051, Loss: 0.9391738474369049, Final Batch Loss: 0.16295643150806427\n",
      "Epoch 3052, Loss: 0.9488862454891205, Final Batch Loss: 0.2612951397895813\n",
      "Epoch 3053, Loss: 0.8483255505561829, Final Batch Loss: 0.21816274523735046\n",
      "Epoch 3054, Loss: 0.8212122321128845, Final Batch Loss: 0.15886341035366058\n",
      "Epoch 3055, Loss: 0.9915319234132767, Final Batch Loss: 0.21238315105438232\n",
      "Epoch 3056, Loss: 0.9186814725399017, Final Batch Loss: 0.23324836790561676\n",
      "Epoch 3057, Loss: 0.8851979076862335, Final Batch Loss: 0.2217736691236496\n",
      "Epoch 3058, Loss: 0.9008311331272125, Final Batch Loss: 0.24755226075649261\n",
      "Epoch 3059, Loss: 0.8916275650262833, Final Batch Loss: 0.1608652025461197\n",
      "Epoch 3060, Loss: 0.9689968079328537, Final Batch Loss: 0.307068407535553\n",
      "Epoch 3061, Loss: 0.8858667016029358, Final Batch Loss: 0.20185807347297668\n",
      "Epoch 3062, Loss: 0.8854130804538727, Final Batch Loss: 0.18420818448066711\n",
      "Epoch 3063, Loss: 0.9118077605962753, Final Batch Loss: 0.26050320267677307\n",
      "Epoch 3064, Loss: 0.9304004907608032, Final Batch Loss: 0.19016487896442413\n",
      "Epoch 3065, Loss: 0.9848236739635468, Final Batch Loss: 0.27635458111763\n",
      "Epoch 3066, Loss: 0.9555998146533966, Final Batch Loss: 0.2650187909603119\n",
      "Epoch 3067, Loss: 0.8745898902416229, Final Batch Loss: 0.21050946414470673\n",
      "Epoch 3068, Loss: 0.9869832247495651, Final Batch Loss: 0.21028797328472137\n",
      "Epoch 3069, Loss: 0.9289019107818604, Final Batch Loss: 0.2614027261734009\n",
      "Epoch 3070, Loss: 0.9522311836481094, Final Batch Loss: 0.30372384190559387\n",
      "Epoch 3071, Loss: 0.9386846125125885, Final Batch Loss: 0.24361449480056763\n",
      "Epoch 3072, Loss: 0.8928156197071075, Final Batch Loss: 0.23909235000610352\n",
      "Epoch 3073, Loss: 0.9275306761264801, Final Batch Loss: 0.230990469455719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3074, Loss: 0.9348053485155106, Final Batch Loss: 0.20986653864383698\n",
      "Epoch 3075, Loss: 0.8134546875953674, Final Batch Loss: 0.12370473146438599\n",
      "Epoch 3076, Loss: 1.0009097456932068, Final Batch Loss: 0.323011577129364\n",
      "Epoch 3077, Loss: 0.9280750602483749, Final Batch Loss: 0.24101091921329498\n",
      "Epoch 3078, Loss: 0.9626261293888092, Final Batch Loss: 0.1990041434764862\n",
      "Epoch 3079, Loss: 0.9041110575199127, Final Batch Loss: 0.2203179895877838\n",
      "Epoch 3080, Loss: 0.8999356627464294, Final Batch Loss: 0.2178584337234497\n",
      "Epoch 3081, Loss: 1.0840971022844315, Final Batch Loss: 0.42075973749160767\n",
      "Epoch 3082, Loss: 0.8464120626449585, Final Batch Loss: 0.16803577542304993\n",
      "Epoch 3083, Loss: 1.0015659034252167, Final Batch Loss: 0.30268338322639465\n",
      "Epoch 3084, Loss: 0.9877284318208694, Final Batch Loss: 0.31786951422691345\n",
      "Epoch 3085, Loss: 0.9098940491676331, Final Batch Loss: 0.24396838247776031\n",
      "Epoch 3086, Loss: 0.8974239081144333, Final Batch Loss: 0.17851319909095764\n",
      "Epoch 3087, Loss: 0.8462558537721634, Final Batch Loss: 0.15451376140117645\n",
      "Epoch 3088, Loss: 0.8625502735376358, Final Batch Loss: 0.164441779255867\n",
      "Epoch 3089, Loss: 0.9251854568719864, Final Batch Loss: 0.3520161211490631\n",
      "Epoch 3090, Loss: 0.8879761397838593, Final Batch Loss: 0.24498233199119568\n",
      "Epoch 3091, Loss: 0.9327300786972046, Final Batch Loss: 0.24801981449127197\n",
      "Epoch 3092, Loss: 0.8490862101316452, Final Batch Loss: 0.1764238029718399\n",
      "Epoch 3093, Loss: 0.8583963364362717, Final Batch Loss: 0.17492292821407318\n",
      "Epoch 3094, Loss: 0.8431071937084198, Final Batch Loss: 0.2668314576148987\n",
      "Epoch 3095, Loss: 0.9993256628513336, Final Batch Loss: 0.29397621750831604\n",
      "Epoch 3096, Loss: 0.9382403790950775, Final Batch Loss: 0.22353853285312653\n",
      "Epoch 3097, Loss: 0.9308017641305923, Final Batch Loss: 0.3005492389202118\n",
      "Epoch 3098, Loss: 0.9391702711582184, Final Batch Loss: 0.24426355957984924\n",
      "Epoch 3099, Loss: 0.9981761574745178, Final Batch Loss: 0.30328088998794556\n",
      "Epoch 3100, Loss: 0.8435047417879105, Final Batch Loss: 0.17498907446861267\n",
      "Epoch 3101, Loss: 0.8388893008232117, Final Batch Loss: 0.178354412317276\n",
      "Epoch 3102, Loss: 0.8655477911233902, Final Batch Loss: 0.12884756922721863\n",
      "Epoch 3103, Loss: 0.8780425935983658, Final Batch Loss: 0.1769946664571762\n",
      "Epoch 3104, Loss: 0.9815538674592972, Final Batch Loss: 0.3114415109157562\n",
      "Epoch 3105, Loss: 0.9594979286193848, Final Batch Loss: 0.2991940379142761\n",
      "Epoch 3106, Loss: 0.8980283290147781, Final Batch Loss: 0.20245391130447388\n",
      "Epoch 3107, Loss: 0.9403998106718063, Final Batch Loss: 0.2785114347934723\n",
      "Epoch 3108, Loss: 0.9561325758695602, Final Batch Loss: 0.19639204442501068\n",
      "Epoch 3109, Loss: 0.8404781073331833, Final Batch Loss: 0.12860994040966034\n",
      "Epoch 3110, Loss: 0.8709816187620163, Final Batch Loss: 0.14033110439777374\n",
      "Epoch 3111, Loss: 0.8262366205453873, Final Batch Loss: 0.19348357617855072\n",
      "Epoch 3112, Loss: 1.018138438463211, Final Batch Loss: 0.2624276578426361\n",
      "Epoch 3113, Loss: 0.9289274215698242, Final Batch Loss: 0.16782568395137787\n",
      "Epoch 3114, Loss: 0.9332929998636246, Final Batch Loss: 0.263952374458313\n",
      "Epoch 3115, Loss: 0.9340807646512985, Final Batch Loss: 0.2128830999135971\n",
      "Epoch 3116, Loss: 0.9202219992876053, Final Batch Loss: 0.25921571254730225\n",
      "Epoch 3117, Loss: 0.9304592460393906, Final Batch Loss: 0.2723596692085266\n",
      "Epoch 3118, Loss: 0.7905487269163132, Final Batch Loss: 0.1249660849571228\n",
      "Epoch 3119, Loss: 0.9344504177570343, Final Batch Loss: 0.23853173851966858\n",
      "Epoch 3120, Loss: 0.8796343207359314, Final Batch Loss: 0.2594168782234192\n",
      "Epoch 3121, Loss: 0.9380025714635849, Final Batch Loss: 0.22046776115894318\n",
      "Epoch 3122, Loss: 0.8707494288682938, Final Batch Loss: 0.15095502138137817\n",
      "Epoch 3123, Loss: 1.032209426164627, Final Batch Loss: 0.3014569878578186\n",
      "Epoch 3124, Loss: 0.9185649901628494, Final Batch Loss: 0.249918594956398\n",
      "Epoch 3125, Loss: 0.9486071616411209, Final Batch Loss: 0.18096747994422913\n",
      "Epoch 3126, Loss: 1.0095961391925812, Final Batch Loss: 0.3123883605003357\n",
      "Epoch 3127, Loss: 0.9640715271234512, Final Batch Loss: 0.2365868091583252\n",
      "Epoch 3128, Loss: 0.9407314658164978, Final Batch Loss: 0.2795027196407318\n",
      "Epoch 3129, Loss: 0.9251958727836609, Final Batch Loss: 0.2407095730304718\n",
      "Epoch 3130, Loss: 0.818669781088829, Final Batch Loss: 0.20443303883075714\n",
      "Epoch 3131, Loss: 0.7899550944566727, Final Batch Loss: 0.1908407062292099\n",
      "Epoch 3132, Loss: 0.8890143483877182, Final Batch Loss: 0.19144701957702637\n",
      "Epoch 3133, Loss: 0.9298586547374725, Final Batch Loss: 0.255520224571228\n",
      "Epoch 3134, Loss: 0.7829194068908691, Final Batch Loss: 0.14050701260566711\n",
      "Epoch 3135, Loss: 0.855040431022644, Final Batch Loss: 0.2298329919576645\n",
      "Epoch 3136, Loss: 0.8951385021209717, Final Batch Loss: 0.3123020827770233\n",
      "Epoch 3137, Loss: 0.9364705234766006, Final Batch Loss: 0.1983509659767151\n",
      "Epoch 3138, Loss: 0.8467975109815598, Final Batch Loss: 0.22050513327121735\n",
      "Epoch 3139, Loss: 0.8760367631912231, Final Batch Loss: 0.21025720238685608\n",
      "Epoch 3140, Loss: 0.9602567851543427, Final Batch Loss: 0.2803022265434265\n",
      "Epoch 3141, Loss: 0.9030192494392395, Final Batch Loss: 0.222209632396698\n",
      "Epoch 3142, Loss: 0.9207575768232346, Final Batch Loss: 0.19664175808429718\n",
      "Epoch 3143, Loss: 0.8779810816049576, Final Batch Loss: 0.21203075349330902\n",
      "Epoch 3144, Loss: 0.9316519796848297, Final Batch Loss: 0.25541654229164124\n",
      "Epoch 3145, Loss: 0.8963378518819809, Final Batch Loss: 0.24761788547039032\n",
      "Epoch 3146, Loss: 0.8399475663900375, Final Batch Loss: 0.16401691734790802\n",
      "Epoch 3147, Loss: 0.9501792341470718, Final Batch Loss: 0.20584136247634888\n",
      "Epoch 3148, Loss: 0.9301109910011292, Final Batch Loss: 0.30173686146736145\n",
      "Epoch 3149, Loss: 0.8178009390830994, Final Batch Loss: 0.1618371158838272\n",
      "Epoch 3150, Loss: 1.054418906569481, Final Batch Loss: 0.4170820713043213\n",
      "Epoch 3151, Loss: 0.8756259977817535, Final Batch Loss: 0.20793253183364868\n",
      "Epoch 3152, Loss: 0.9290519058704376, Final Batch Loss: 0.24865196645259857\n",
      "Epoch 3153, Loss: 1.015131950378418, Final Batch Loss: 0.3317272663116455\n",
      "Epoch 3154, Loss: 0.9218677133321762, Final Batch Loss: 0.17298012971878052\n",
      "Epoch 3155, Loss: 0.9553225636482239, Final Batch Loss: 0.2406548708677292\n",
      "Epoch 3156, Loss: 1.011718213558197, Final Batch Loss: 0.28498193621635437\n",
      "Epoch 3157, Loss: 0.8284036070108414, Final Batch Loss: 0.2088043987751007\n",
      "Epoch 3158, Loss: 0.8956402689218521, Final Batch Loss: 0.25186246633529663\n",
      "Epoch 3159, Loss: 0.8688768148422241, Final Batch Loss: 0.23580802977085114\n",
      "Epoch 3160, Loss: 0.8754079341888428, Final Batch Loss: 0.2080891728401184\n",
      "Epoch 3161, Loss: 0.8811715543270111, Final Batch Loss: 0.22368955612182617\n",
      "Epoch 3162, Loss: 0.8585779070854187, Final Batch Loss: 0.16468584537506104\n",
      "Epoch 3163, Loss: 0.8812607377767563, Final Batch Loss: 0.15767936408519745\n",
      "Epoch 3164, Loss: 0.9560576528310776, Final Batch Loss: 0.2540018558502197\n",
      "Epoch 3165, Loss: 0.9134592860937119, Final Batch Loss: 0.25334757566452026\n",
      "Epoch 3166, Loss: 0.8967403769493103, Final Batch Loss: 0.2320556491613388\n",
      "Epoch 3167, Loss: 0.8613331168889999, Final Batch Loss: 0.1683809608221054\n",
      "Epoch 3168, Loss: 0.8539752513170242, Final Batch Loss: 0.23890268802642822\n",
      "Epoch 3169, Loss: 0.8652240484952927, Final Batch Loss: 0.17447547614574432\n",
      "Epoch 3170, Loss: 0.988564670085907, Final Batch Loss: 0.3148837685585022\n",
      "Epoch 3171, Loss: 0.8854875564575195, Final Batch Loss: 0.2291952222585678\n",
      "Epoch 3172, Loss: 0.8578382730484009, Final Batch Loss: 0.22639930248260498\n",
      "Epoch 3173, Loss: 0.9139410704374313, Final Batch Loss: 0.20805467665195465\n",
      "Epoch 3174, Loss: 0.8940552324056625, Final Batch Loss: 0.18925350904464722\n",
      "Epoch 3175, Loss: 0.9157498627901077, Final Batch Loss: 0.2480102926492691\n",
      "Epoch 3176, Loss: 0.9230163842439651, Final Batch Loss: 0.2161528766155243\n",
      "Epoch 3177, Loss: 0.8226200938224792, Final Batch Loss: 0.16208483278751373\n",
      "Epoch 3178, Loss: 0.9000464677810669, Final Batch Loss: 0.2823678255081177\n",
      "Epoch 3179, Loss: 0.9025979191064835, Final Batch Loss: 0.2481040358543396\n",
      "Epoch 3180, Loss: 0.9206216484308243, Final Batch Loss: 0.22278662025928497\n",
      "Epoch 3181, Loss: 0.8731686025857925, Final Batch Loss: 0.19019979238510132\n",
      "Epoch 3182, Loss: 0.8478983044624329, Final Batch Loss: 0.1955965757369995\n",
      "Epoch 3183, Loss: 0.877331092953682, Final Batch Loss: 0.25975537300109863\n",
      "Epoch 3184, Loss: 0.8721441924571991, Final Batch Loss: 0.2175777405500412\n",
      "Epoch 3185, Loss: 0.8891773372888565, Final Batch Loss: 0.15322154760360718\n",
      "Epoch 3186, Loss: 0.8699136823415756, Final Batch Loss: 0.20232783257961273\n",
      "Epoch 3187, Loss: 0.9022336453199387, Final Batch Loss: 0.18450288474559784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3188, Loss: 0.8192166537046432, Final Batch Loss: 0.17279481887817383\n",
      "Epoch 3189, Loss: 0.8291653245687485, Final Batch Loss: 0.2032821923494339\n",
      "Epoch 3190, Loss: 0.9177507311105728, Final Batch Loss: 0.2704377770423889\n",
      "Epoch 3191, Loss: 0.8417301923036575, Final Batch Loss: 0.16937226057052612\n",
      "Epoch 3192, Loss: 0.8879400342702866, Final Batch Loss: 0.167988121509552\n",
      "Epoch 3193, Loss: 0.9465801268815994, Final Batch Loss: 0.272774338722229\n",
      "Epoch 3194, Loss: 0.9572506695985794, Final Batch Loss: 0.20389246940612793\n",
      "Epoch 3195, Loss: 0.9332042932510376, Final Batch Loss: 0.2672288417816162\n",
      "Epoch 3196, Loss: 0.875287190079689, Final Batch Loss: 0.23389755189418793\n",
      "Epoch 3197, Loss: 0.8831084370613098, Final Batch Loss: 0.21194201707839966\n",
      "Epoch 3198, Loss: 0.9034318625926971, Final Batch Loss: 0.23213215172290802\n",
      "Epoch 3199, Loss: 0.9167518615722656, Final Batch Loss: 0.21365569531917572\n",
      "Epoch 3200, Loss: 0.8362497538328171, Final Batch Loss: 0.16390834748744965\n",
      "Epoch 3201, Loss: 0.9668219536542892, Final Batch Loss: 0.36415594816207886\n",
      "Epoch 3202, Loss: 0.826813131570816, Final Batch Loss: 0.20773345232009888\n",
      "Epoch 3203, Loss: 0.9185899049043655, Final Batch Loss: 0.22722120583057404\n",
      "Epoch 3204, Loss: 0.854302391409874, Final Batch Loss: 0.22409005463123322\n",
      "Epoch 3205, Loss: 0.9017873257398605, Final Batch Loss: 0.19222630560398102\n",
      "Epoch 3206, Loss: 0.8665158301591873, Final Batch Loss: 0.17799831926822662\n",
      "Epoch 3207, Loss: 0.8724116384983063, Final Batch Loss: 0.22677691280841827\n",
      "Epoch 3208, Loss: 0.7689713537693024, Final Batch Loss: 0.14114277064800262\n",
      "Epoch 3209, Loss: 0.8372654914855957, Final Batch Loss: 0.2119997888803482\n",
      "Epoch 3210, Loss: 0.8656907230615616, Final Batch Loss: 0.20799177885055542\n",
      "Epoch 3211, Loss: 0.8398878276348114, Final Batch Loss: 0.1456509530544281\n",
      "Epoch 3212, Loss: 0.9492445737123489, Final Batch Loss: 0.27929770946502686\n",
      "Epoch 3213, Loss: 0.88377845287323, Final Batch Loss: 0.2076834738254547\n",
      "Epoch 3214, Loss: 0.8229269534349442, Final Batch Loss: 0.16953787207603455\n",
      "Epoch 3215, Loss: 0.876228004693985, Final Batch Loss: 0.18094398081302643\n",
      "Epoch 3216, Loss: 0.9034218788146973, Final Batch Loss: 0.22318747639656067\n",
      "Epoch 3217, Loss: 0.8842729777097702, Final Batch Loss: 0.15994617342948914\n",
      "Epoch 3218, Loss: 0.777848556637764, Final Batch Loss: 0.1210448294878006\n",
      "Epoch 3219, Loss: 0.8513467907905579, Final Batch Loss: 0.23949918150901794\n",
      "Epoch 3220, Loss: 0.811361774802208, Final Batch Loss: 0.16560174524784088\n",
      "Epoch 3221, Loss: 0.8660019636154175, Final Batch Loss: 0.16259220242500305\n",
      "Epoch 3222, Loss: 0.9079308658838272, Final Batch Loss: 0.2443837970495224\n",
      "Epoch 3223, Loss: 0.9026396423578262, Final Batch Loss: 0.2696956992149353\n",
      "Epoch 3224, Loss: 0.8951486647129059, Final Batch Loss: 0.23467522859573364\n",
      "Epoch 3225, Loss: 0.8397002816200256, Final Batch Loss: 0.1355469524860382\n",
      "Epoch 3226, Loss: 0.9130073636770248, Final Batch Loss: 0.23467512428760529\n",
      "Epoch 3227, Loss: 0.8507188260555267, Final Batch Loss: 0.23395302891731262\n",
      "Epoch 3228, Loss: 0.8752857744693756, Final Batch Loss: 0.16671422123908997\n",
      "Epoch 3229, Loss: 0.8698685169219971, Final Batch Loss: 0.2263788878917694\n",
      "Epoch 3230, Loss: 0.8205358982086182, Final Batch Loss: 0.21068516373634338\n",
      "Epoch 3231, Loss: 0.8887898325920105, Final Batch Loss: 0.21241331100463867\n",
      "Epoch 3232, Loss: 0.9319669157266617, Final Batch Loss: 0.2797980308532715\n",
      "Epoch 3233, Loss: 0.9140982031822205, Final Batch Loss: 0.2558159828186035\n",
      "Epoch 3234, Loss: 0.9152713865041733, Final Batch Loss: 0.2720354199409485\n",
      "Epoch 3235, Loss: 0.955277755856514, Final Batch Loss: 0.2902070879936218\n",
      "Epoch 3236, Loss: 0.9039042592048645, Final Batch Loss: 0.14822478592395782\n",
      "Epoch 3237, Loss: 0.9185819178819656, Final Batch Loss: 0.2236838936805725\n",
      "Epoch 3238, Loss: 0.8583226799964905, Final Batch Loss: 0.15549933910369873\n",
      "Epoch 3239, Loss: 0.910736620426178, Final Batch Loss: 0.17678378522396088\n",
      "Epoch 3240, Loss: 1.0331568270921707, Final Batch Loss: 0.28790074586868286\n",
      "Epoch 3241, Loss: 0.9166569411754608, Final Batch Loss: 0.18948017060756683\n",
      "Epoch 3242, Loss: 0.8862742483615875, Final Batch Loss: 0.19841855764389038\n",
      "Epoch 3243, Loss: 0.9875321239233017, Final Batch Loss: 0.21165548264980316\n",
      "Epoch 3244, Loss: 0.9083937555551529, Final Batch Loss: 0.23594063520431519\n",
      "Epoch 3245, Loss: 0.8917255103588104, Final Batch Loss: 0.222805917263031\n",
      "Epoch 3246, Loss: 0.9350644201040268, Final Batch Loss: 0.2758888900279999\n",
      "Epoch 3247, Loss: 1.0084373652935028, Final Batch Loss: 0.2829761207103729\n",
      "Epoch 3248, Loss: 0.86111781001091, Final Batch Loss: 0.19238847494125366\n",
      "Epoch 3249, Loss: 0.9109794795513153, Final Batch Loss: 0.27871009707450867\n",
      "Epoch 3250, Loss: 0.9282637238502502, Final Batch Loss: 0.26342785358428955\n",
      "Epoch 3251, Loss: 0.9241465032100677, Final Batch Loss: 0.25965049862861633\n",
      "Epoch 3252, Loss: 0.9557789713144302, Final Batch Loss: 0.2742210328578949\n",
      "Epoch 3253, Loss: 0.9088564813137054, Final Batch Loss: 0.2594429552555084\n",
      "Epoch 3254, Loss: 0.9106920957565308, Final Batch Loss: 0.2285429984331131\n",
      "Epoch 3255, Loss: 0.8297661244869232, Final Batch Loss: 0.12826509773731232\n",
      "Epoch 3256, Loss: 0.9351512789726257, Final Batch Loss: 0.1949584037065506\n",
      "Epoch 3257, Loss: 0.9254473149776459, Final Batch Loss: 0.2769952714443207\n",
      "Epoch 3258, Loss: 0.9154898971319199, Final Batch Loss: 0.20162691175937653\n",
      "Epoch 3259, Loss: 0.9225024431943893, Final Batch Loss: 0.2622733414173126\n",
      "Epoch 3260, Loss: 0.862897664308548, Final Batch Loss: 0.18524570763111115\n",
      "Epoch 3261, Loss: 0.9874550700187683, Final Batch Loss: 0.31329452991485596\n",
      "Epoch 3262, Loss: 0.9280072152614594, Final Batch Loss: 0.18677666783332825\n",
      "Epoch 3263, Loss: 0.9753758311271667, Final Batch Loss: 0.2536481022834778\n",
      "Epoch 3264, Loss: 0.9243237227201462, Final Batch Loss: 0.2465573251247406\n",
      "Epoch 3265, Loss: 0.9155876040458679, Final Batch Loss: 0.23231080174446106\n",
      "Epoch 3266, Loss: 0.8741043359041214, Final Batch Loss: 0.17546319961547852\n",
      "Epoch 3267, Loss: 0.8820962905883789, Final Batch Loss: 0.24793754518032074\n",
      "Epoch 3268, Loss: 0.9280054569244385, Final Batch Loss: 0.2107565701007843\n",
      "Epoch 3269, Loss: 0.8354763239622116, Final Batch Loss: 0.1746978461742401\n",
      "Epoch 3270, Loss: 0.8771560788154602, Final Batch Loss: 0.19257350265979767\n",
      "Epoch 3271, Loss: 0.9472385793924332, Final Batch Loss: 0.21591681241989136\n",
      "Epoch 3272, Loss: 0.9307399094104767, Final Batch Loss: 0.31940004229545593\n",
      "Epoch 3273, Loss: 0.9778321087360382, Final Batch Loss: 0.28624793887138367\n",
      "Epoch 3274, Loss: 0.9045960307121277, Final Batch Loss: 0.26193106174468994\n",
      "Epoch 3275, Loss: 0.8652889728546143, Final Batch Loss: 0.19602899253368378\n",
      "Epoch 3276, Loss: 0.8407744765281677, Final Batch Loss: 0.17591167986392975\n",
      "Epoch 3277, Loss: 0.8609006553888321, Final Batch Loss: 0.21716134250164032\n",
      "Epoch 3278, Loss: 0.9327391535043716, Final Batch Loss: 0.19370000064373016\n",
      "Epoch 3279, Loss: 0.8862719833850861, Final Batch Loss: 0.16913896799087524\n",
      "Epoch 3280, Loss: 0.8723894506692886, Final Batch Loss: 0.22982272505760193\n",
      "Epoch 3281, Loss: 0.8654634654521942, Final Batch Loss: 0.2176177203655243\n",
      "Epoch 3282, Loss: 0.9181838631629944, Final Batch Loss: 0.29037365317344666\n",
      "Epoch 3283, Loss: 0.8771803379058838, Final Batch Loss: 0.24354512989521027\n",
      "Epoch 3284, Loss: 0.8064342439174652, Final Batch Loss: 0.14457876980304718\n",
      "Epoch 3285, Loss: 0.9195775389671326, Final Batch Loss: 0.24726565182209015\n",
      "Epoch 3286, Loss: 0.7923508882522583, Final Batch Loss: 0.13470473885536194\n",
      "Epoch 3287, Loss: 0.9399154633283615, Final Batch Loss: 0.20178206264972687\n",
      "Epoch 3288, Loss: 0.9918445944786072, Final Batch Loss: 0.3084268569946289\n",
      "Epoch 3289, Loss: 0.9854899793863297, Final Batch Loss: 0.30536946654319763\n",
      "Epoch 3290, Loss: 0.9023482948541641, Final Batch Loss: 0.19490520656108856\n",
      "Epoch 3291, Loss: 0.9249370098114014, Final Batch Loss: 0.21186380088329315\n",
      "Epoch 3292, Loss: 0.9270305931568146, Final Batch Loss: 0.2128506302833557\n",
      "Epoch 3293, Loss: 0.8332322090864182, Final Batch Loss: 0.1406877189874649\n",
      "Epoch 3294, Loss: 0.8181382119655609, Final Batch Loss: 0.16485553979873657\n",
      "Epoch 3295, Loss: 0.8885395973920822, Final Batch Loss: 0.2525711953639984\n",
      "Epoch 3296, Loss: 0.8549302071332932, Final Batch Loss: 0.20088303089141846\n",
      "Epoch 3297, Loss: 0.9316268563270569, Final Batch Loss: 0.21931110322475433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3298, Loss: 0.900105744600296, Final Batch Loss: 0.24887461960315704\n",
      "Epoch 3299, Loss: 0.8590459227561951, Final Batch Loss: 0.24551524221897125\n",
      "Epoch 3300, Loss: 0.9338051080703735, Final Batch Loss: 0.27469396591186523\n",
      "Epoch 3301, Loss: 0.845266729593277, Final Batch Loss: 0.2169206440448761\n",
      "Epoch 3302, Loss: 0.8589135408401489, Final Batch Loss: 0.18736116588115692\n",
      "Epoch 3303, Loss: 0.934712365269661, Final Batch Loss: 0.3050059676170349\n",
      "Epoch 3304, Loss: 0.8024770766496658, Final Batch Loss: 0.16848570108413696\n",
      "Epoch 3305, Loss: 0.9353683143854141, Final Batch Loss: 0.24151332676410675\n",
      "Epoch 3306, Loss: 1.0985804200172424, Final Batch Loss: 0.3879462778568268\n",
      "Epoch 3307, Loss: 0.8683034479618073, Final Batch Loss: 0.21079008281230927\n",
      "Epoch 3308, Loss: 0.9273401349782944, Final Batch Loss: 0.2491958886384964\n",
      "Epoch 3309, Loss: 0.9132374674081802, Final Batch Loss: 0.24874894320964813\n",
      "Epoch 3310, Loss: 0.9252751618623734, Final Batch Loss: 0.2588884234428406\n",
      "Epoch 3311, Loss: 0.8538047820329666, Final Batch Loss: 0.21527139842510223\n",
      "Epoch 3312, Loss: 0.9052161574363708, Final Batch Loss: 0.22388628125190735\n",
      "Epoch 3313, Loss: 0.7835666090250015, Final Batch Loss: 0.1517113596200943\n",
      "Epoch 3314, Loss: 0.9014470726251602, Final Batch Loss: 0.22839416563510895\n",
      "Epoch 3315, Loss: 0.8598986566066742, Final Batch Loss: 0.19936516880989075\n",
      "Epoch 3316, Loss: 0.7959123402833939, Final Batch Loss: 0.11286774277687073\n",
      "Epoch 3317, Loss: 0.9424175471067429, Final Batch Loss: 0.3207957446575165\n",
      "Epoch 3318, Loss: 0.8613183200359344, Final Batch Loss: 0.2329723834991455\n",
      "Epoch 3319, Loss: 0.9198289066553116, Final Batch Loss: 0.29074329137802124\n",
      "Epoch 3320, Loss: 0.8151417970657349, Final Batch Loss: 0.23968861997127533\n",
      "Epoch 3321, Loss: 0.8324036598205566, Final Batch Loss: 0.2310265153646469\n",
      "Epoch 3322, Loss: 0.9330947697162628, Final Batch Loss: 0.261473149061203\n",
      "Epoch 3323, Loss: 0.804549366235733, Final Batch Loss: 0.16307799518108368\n",
      "Epoch 3324, Loss: 0.9319134503602982, Final Batch Loss: 0.24406014382839203\n",
      "Epoch 3325, Loss: 0.8276468217372894, Final Batch Loss: 0.15452983975410461\n",
      "Epoch 3326, Loss: 0.9271048456430435, Final Batch Loss: 0.2552785277366638\n",
      "Epoch 3327, Loss: 1.0703876912593842, Final Batch Loss: 0.25839290022850037\n",
      "Epoch 3328, Loss: 0.977464959025383, Final Batch Loss: 0.26564398407936096\n",
      "Epoch 3329, Loss: 0.9891705960035324, Final Batch Loss: 0.2891855835914612\n",
      "Epoch 3330, Loss: 0.9132483601570129, Final Batch Loss: 0.2276337444782257\n",
      "Epoch 3331, Loss: 0.8744615316390991, Final Batch Loss: 0.159652978181839\n",
      "Epoch 3332, Loss: 0.8242760896682739, Final Batch Loss: 0.18489353358745575\n",
      "Epoch 3333, Loss: 0.9370596259832382, Final Batch Loss: 0.24144260585308075\n",
      "Epoch 3334, Loss: 0.8251463770866394, Final Batch Loss: 0.19364891946315765\n",
      "Epoch 3335, Loss: 0.9686528593301773, Final Batch Loss: 0.33461225032806396\n",
      "Epoch 3336, Loss: 0.9306818842887878, Final Batch Loss: 0.21316857635974884\n",
      "Epoch 3337, Loss: 0.9997313022613525, Final Batch Loss: 0.2901560962200165\n",
      "Epoch 3338, Loss: 0.8854743540287018, Final Batch Loss: 0.2523707151412964\n",
      "Epoch 3339, Loss: 0.9120446145534515, Final Batch Loss: 0.17680981755256653\n",
      "Epoch 3340, Loss: 0.9188522845506668, Final Batch Loss: 0.18634429574012756\n",
      "Epoch 3341, Loss: 0.8605794757604599, Final Batch Loss: 0.16165566444396973\n",
      "Epoch 3342, Loss: 0.8497620075941086, Final Batch Loss: 0.2031298577785492\n",
      "Epoch 3343, Loss: 0.9256800711154938, Final Batch Loss: 0.3073675036430359\n",
      "Epoch 3344, Loss: 0.9451557993888855, Final Batch Loss: 0.18009711802005768\n",
      "Epoch 3345, Loss: 0.8381971120834351, Final Batch Loss: 0.19199220836162567\n",
      "Epoch 3346, Loss: 0.8281989097595215, Final Batch Loss: 0.19033756852149963\n",
      "Epoch 3347, Loss: 0.8440705686807632, Final Batch Loss: 0.13900095224380493\n",
      "Epoch 3348, Loss: 0.8785240054130554, Final Batch Loss: 0.1953950822353363\n",
      "Epoch 3349, Loss: 0.9139963984489441, Final Batch Loss: 0.1981915533542633\n",
      "Epoch 3350, Loss: 0.8689968436956406, Final Batch Loss: 0.20885904133319855\n",
      "Epoch 3351, Loss: 0.9590741544961929, Final Batch Loss: 0.19697430729866028\n",
      "Epoch 3352, Loss: 0.8495219051837921, Final Batch Loss: 0.17662888765335083\n",
      "Epoch 3353, Loss: 0.9342355132102966, Final Batch Loss: 0.21538959443569183\n",
      "Epoch 3354, Loss: 0.8922974616289139, Final Batch Loss: 0.2285902351140976\n",
      "Epoch 3355, Loss: 0.8766930401325226, Final Batch Loss: 0.17358233034610748\n",
      "Epoch 3356, Loss: 0.9331820011138916, Final Batch Loss: 0.25806930661201477\n",
      "Epoch 3357, Loss: 0.8719451129436493, Final Batch Loss: 0.22653703391551971\n",
      "Epoch 3358, Loss: 0.9005809873342514, Final Batch Loss: 0.19419530034065247\n",
      "Epoch 3359, Loss: 0.9436231404542923, Final Batch Loss: 0.2562914490699768\n",
      "Epoch 3360, Loss: 0.9435245990753174, Final Batch Loss: 0.27028730511665344\n",
      "Epoch 3361, Loss: 0.8707543015480042, Final Batch Loss: 0.22092489898204803\n",
      "Epoch 3362, Loss: 0.8040458709001541, Final Batch Loss: 0.1491130143404007\n",
      "Epoch 3363, Loss: 0.8619686365127563, Final Batch Loss: 0.2272258847951889\n",
      "Epoch 3364, Loss: 0.9042628258466721, Final Batch Loss: 0.2447616308927536\n",
      "Epoch 3365, Loss: 0.9108871668577194, Final Batch Loss: 0.2973248064517975\n",
      "Epoch 3366, Loss: 0.8414808362722397, Final Batch Loss: 0.1642724722623825\n",
      "Epoch 3367, Loss: 0.8656768053770065, Final Batch Loss: 0.19654257595539093\n",
      "Epoch 3368, Loss: 0.8357093185186386, Final Batch Loss: 0.23064590990543365\n",
      "Epoch 3369, Loss: 0.827607735991478, Final Batch Loss: 0.1431007832288742\n",
      "Epoch 3370, Loss: 0.8177262172102928, Final Batch Loss: 0.09722761064767838\n",
      "Epoch 3371, Loss: 0.8078664690256119, Final Batch Loss: 0.19291017949581146\n",
      "Epoch 3372, Loss: 0.8072961270809174, Final Batch Loss: 0.1943129599094391\n",
      "Epoch 3373, Loss: 0.8504175543785095, Final Batch Loss: 0.18087416887283325\n",
      "Epoch 3374, Loss: 0.9044369161128998, Final Batch Loss: 0.26918479800224304\n",
      "Epoch 3375, Loss: 0.8207219392061234, Final Batch Loss: 0.1935514360666275\n",
      "Epoch 3376, Loss: 0.8727434873580933, Final Batch Loss: 0.3047599494457245\n",
      "Epoch 3377, Loss: 0.837922140955925, Final Batch Loss: 0.17006666958332062\n",
      "Epoch 3378, Loss: 0.8136260211467743, Final Batch Loss: 0.16287733614444733\n",
      "Epoch 3379, Loss: 0.942302331328392, Final Batch Loss: 0.2596520781517029\n",
      "Epoch 3380, Loss: 0.9162497520446777, Final Batch Loss: 0.23054906725883484\n",
      "Epoch 3381, Loss: 0.9608516991138458, Final Batch Loss: 0.2698478698730469\n",
      "Epoch 3382, Loss: 0.9060033559799194, Final Batch Loss: 0.24646878242492676\n",
      "Epoch 3383, Loss: 0.8858274072408676, Final Batch Loss: 0.16027656197547913\n",
      "Epoch 3384, Loss: 0.9606275856494904, Final Batch Loss: 0.27595606446266174\n",
      "Epoch 3385, Loss: 0.9533880352973938, Final Batch Loss: 0.20581702888011932\n",
      "Epoch 3386, Loss: 0.9247855544090271, Final Batch Loss: 0.22442086040973663\n",
      "Epoch 3387, Loss: 0.9141145944595337, Final Batch Loss: 0.18762914836406708\n",
      "Epoch 3388, Loss: 0.9382529705762863, Final Batch Loss: 0.2399744987487793\n",
      "Epoch 3389, Loss: 0.9933865070343018, Final Batch Loss: 0.25600284337997437\n",
      "Epoch 3390, Loss: 0.9243748188018799, Final Batch Loss: 0.23810726404190063\n",
      "Epoch 3391, Loss: 1.0077122151851654, Final Batch Loss: 0.31044667959213257\n",
      "Epoch 3392, Loss: 0.9295243471860886, Final Batch Loss: 0.1871086210012436\n",
      "Epoch 3393, Loss: 0.8908378928899765, Final Batch Loss: 0.26992058753967285\n",
      "Epoch 3394, Loss: 0.958423376083374, Final Batch Loss: 0.25459519028663635\n",
      "Epoch 3395, Loss: 0.8529357612133026, Final Batch Loss: 0.15186576545238495\n",
      "Epoch 3396, Loss: 0.8941037207841873, Final Batch Loss: 0.20092496275901794\n",
      "Epoch 3397, Loss: 0.9098079800605774, Final Batch Loss: 0.22157691419124603\n",
      "Epoch 3398, Loss: 0.8570638447999954, Final Batch Loss: 0.16011808812618256\n",
      "Epoch 3399, Loss: 1.0140255093574524, Final Batch Loss: 0.2990736663341522\n",
      "Epoch 3400, Loss: 0.8916287869215012, Final Batch Loss: 0.22756804525852203\n",
      "Epoch 3401, Loss: 0.9349798560142517, Final Batch Loss: 0.27986809611320496\n",
      "Epoch 3402, Loss: 0.8440800458192825, Final Batch Loss: 0.20297123491764069\n",
      "Epoch 3403, Loss: 0.9593189656734467, Final Batch Loss: 0.29084113240242004\n",
      "Epoch 3404, Loss: 0.9404021203517914, Final Batch Loss: 0.2642464339733124\n",
      "Epoch 3405, Loss: 0.8613276034593582, Final Batch Loss: 0.23573406040668488\n",
      "Epoch 3406, Loss: 0.8335048407316208, Final Batch Loss: 0.18954817950725555\n",
      "Epoch 3407, Loss: 0.8516225218772888, Final Batch Loss: 0.2675327956676483\n",
      "Epoch 3408, Loss: 0.9132180958986282, Final Batch Loss: 0.23226496577262878\n",
      "Epoch 3409, Loss: 0.858609989285469, Final Batch Loss: 0.20725564658641815\n",
      "Epoch 3410, Loss: 0.8546896725893021, Final Batch Loss: 0.1605987548828125\n",
      "Epoch 3411, Loss: 0.8311108201742172, Final Batch Loss: 0.14231835305690765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3412, Loss: 0.9055808931589127, Final Batch Loss: 0.21241532266139984\n",
      "Epoch 3413, Loss: 0.8189403414726257, Final Batch Loss: 0.1754855066537857\n",
      "Epoch 3414, Loss: 0.883431926369667, Final Batch Loss: 0.26343223452568054\n",
      "Epoch 3415, Loss: 0.8843614608049393, Final Batch Loss: 0.19416505098342896\n",
      "Epoch 3416, Loss: 0.8047404736280441, Final Batch Loss: 0.20647065341472626\n",
      "Epoch 3417, Loss: 0.9416309148073196, Final Batch Loss: 0.23233164846897125\n",
      "Epoch 3418, Loss: 0.9244458675384521, Final Batch Loss: 0.33026039600372314\n",
      "Epoch 3419, Loss: 0.8776065409183502, Final Batch Loss: 0.27457770705223083\n",
      "Epoch 3420, Loss: 0.910394474864006, Final Batch Loss: 0.2276623249053955\n",
      "Epoch 3421, Loss: 1.0158834606409073, Final Batch Loss: 0.32261165976524353\n",
      "Epoch 3422, Loss: 0.8829724341630936, Final Batch Loss: 0.14865030348300934\n",
      "Epoch 3423, Loss: 0.9834146797657013, Final Batch Loss: 0.3143497109413147\n",
      "Epoch 3424, Loss: 0.8844030052423477, Final Batch Loss: 0.1711001992225647\n",
      "Epoch 3425, Loss: 1.03655344247818, Final Batch Loss: 0.3570820689201355\n",
      "Epoch 3426, Loss: 0.8767148554325104, Final Batch Loss: 0.18988706171512604\n",
      "Epoch 3427, Loss: 0.8721053302288055, Final Batch Loss: 0.28269317746162415\n",
      "Epoch 3428, Loss: 0.9124940782785416, Final Batch Loss: 0.18767429888248444\n",
      "Epoch 3429, Loss: 0.8705694377422333, Final Batch Loss: 0.12602248787879944\n",
      "Epoch 3430, Loss: 0.9853530377149582, Final Batch Loss: 0.2609020471572876\n",
      "Epoch 3431, Loss: 0.9740677773952484, Final Batch Loss: 0.29509443044662476\n",
      "Epoch 3432, Loss: 0.8767029792070389, Final Batch Loss: 0.20745262503623962\n",
      "Epoch 3433, Loss: 0.9535170644521713, Final Batch Loss: 0.22962461411952972\n",
      "Epoch 3434, Loss: 0.9490402191877365, Final Batch Loss: 0.2666207253932953\n",
      "Epoch 3435, Loss: 0.9659077078104019, Final Batch Loss: 0.24214152991771698\n",
      "Epoch 3436, Loss: 0.9172044545412064, Final Batch Loss: 0.18911148607730865\n",
      "Epoch 3437, Loss: 0.9406758397817612, Final Batch Loss: 0.1826283186674118\n",
      "Epoch 3438, Loss: 0.9411205500364304, Final Batch Loss: 0.16789503395557404\n",
      "Epoch 3439, Loss: 0.8666134923696518, Final Batch Loss: 0.21292556822299957\n",
      "Epoch 3440, Loss: 0.9338593035936356, Final Batch Loss: 0.24740594625473022\n",
      "Epoch 3441, Loss: 0.8279823511838913, Final Batch Loss: 0.13976258039474487\n",
      "Epoch 3442, Loss: 0.9110761135816574, Final Batch Loss: 0.2353571653366089\n",
      "Epoch 3443, Loss: 0.8913102447986603, Final Batch Loss: 0.20539048314094543\n",
      "Epoch 3444, Loss: 1.006044328212738, Final Batch Loss: 0.33159273862838745\n",
      "Epoch 3445, Loss: 0.8115077316761017, Final Batch Loss: 0.19356676936149597\n",
      "Epoch 3446, Loss: 0.9426829963922501, Final Batch Loss: 0.23141789436340332\n",
      "Epoch 3447, Loss: 0.9010005593299866, Final Batch Loss: 0.147635355591774\n",
      "Epoch 3448, Loss: 0.9871449917554855, Final Batch Loss: 0.33222246170043945\n",
      "Epoch 3449, Loss: 0.9183172285556793, Final Batch Loss: 0.26183784008026123\n",
      "Epoch 3450, Loss: 0.8605575114488602, Final Batch Loss: 0.18941156566143036\n",
      "Epoch 3451, Loss: 0.9577972590923309, Final Batch Loss: 0.32023316621780396\n",
      "Epoch 3452, Loss: 0.8923446983098984, Final Batch Loss: 0.23379604518413544\n",
      "Epoch 3453, Loss: 0.8725226670503616, Final Batch Loss: 0.21096323430538177\n",
      "Epoch 3454, Loss: 0.8714795708656311, Final Batch Loss: 0.2338862419128418\n",
      "Epoch 3455, Loss: 0.9553873091936111, Final Batch Loss: 0.2268853485584259\n",
      "Epoch 3456, Loss: 0.8409732282161713, Final Batch Loss: 0.16826102137565613\n",
      "Epoch 3457, Loss: 0.9101595878601074, Final Batch Loss: 0.26510775089263916\n",
      "Epoch 3458, Loss: 0.9534864276647568, Final Batch Loss: 0.18721669912338257\n",
      "Epoch 3459, Loss: 0.877839982509613, Final Batch Loss: 0.14841406047344208\n",
      "Epoch 3460, Loss: 0.9493622183799744, Final Batch Loss: 0.21299441158771515\n",
      "Epoch 3461, Loss: 0.9466860592365265, Final Batch Loss: 0.271587073802948\n",
      "Epoch 3462, Loss: 0.9021277576684952, Final Batch Loss: 0.21642974019050598\n",
      "Epoch 3463, Loss: 0.9655437469482422, Final Batch Loss: 0.2978679835796356\n",
      "Epoch 3464, Loss: 0.8731011003255844, Final Batch Loss: 0.193938747048378\n",
      "Epoch 3465, Loss: 0.8657012581825256, Final Batch Loss: 0.13335788249969482\n",
      "Epoch 3466, Loss: 0.8275750428438187, Final Batch Loss: 0.21646825969219208\n",
      "Epoch 3467, Loss: 0.9044271856546402, Final Batch Loss: 0.2704049050807953\n",
      "Epoch 3468, Loss: 0.8173963278532028, Final Batch Loss: 0.14371183514595032\n",
      "Epoch 3469, Loss: 0.8328931778669357, Final Batch Loss: 0.17469322681427002\n",
      "Epoch 3470, Loss: 0.8611358851194382, Final Batch Loss: 0.2198323905467987\n",
      "Epoch 3471, Loss: 0.8935375064611435, Final Batch Loss: 0.1981654167175293\n",
      "Epoch 3472, Loss: 0.8610545098781586, Final Batch Loss: 0.21357078850269318\n",
      "Epoch 3473, Loss: 0.8173104375600815, Final Batch Loss: 0.2812299132347107\n",
      "Epoch 3474, Loss: 0.8123973160982132, Final Batch Loss: 0.15114952623844147\n",
      "Epoch 3475, Loss: 0.9570039659738541, Final Batch Loss: 0.2514117956161499\n",
      "Epoch 3476, Loss: 0.8662342578172684, Final Batch Loss: 0.18513721227645874\n",
      "Epoch 3477, Loss: 0.955323651432991, Final Batch Loss: 0.28783097863197327\n",
      "Epoch 3478, Loss: 0.8789986819028854, Final Batch Loss: 0.23152785003185272\n",
      "Epoch 3479, Loss: 0.8525704741477966, Final Batch Loss: 0.21101480722427368\n",
      "Epoch 3480, Loss: 0.8924876600503922, Final Batch Loss: 0.23105177283287048\n",
      "Epoch 3481, Loss: 0.869119793176651, Final Batch Loss: 0.18332688510417938\n",
      "Epoch 3482, Loss: 0.9154499471187592, Final Batch Loss: 0.25232139229774475\n",
      "Epoch 3483, Loss: 0.938209593296051, Final Batch Loss: 0.25279366970062256\n",
      "Epoch 3484, Loss: 0.8376508951187134, Final Batch Loss: 0.16906622052192688\n",
      "Epoch 3485, Loss: 0.8257711827754974, Final Batch Loss: 0.15998919308185577\n",
      "Epoch 3486, Loss: 0.9369663745164871, Final Batch Loss: 0.219575896859169\n",
      "Epoch 3487, Loss: 0.9581699073314667, Final Batch Loss: 0.24108852446079254\n",
      "Epoch 3488, Loss: 0.8572657555341721, Final Batch Loss: 0.22265486419200897\n",
      "Epoch 3489, Loss: 0.8948094248771667, Final Batch Loss: 0.26432564854621887\n",
      "Epoch 3490, Loss: 0.8134688884019852, Final Batch Loss: 0.19798973202705383\n",
      "Epoch 3491, Loss: 0.7860031574964523, Final Batch Loss: 0.1245604008436203\n",
      "Epoch 3492, Loss: 0.8158535063266754, Final Batch Loss: 0.16230076551437378\n",
      "Epoch 3493, Loss: 0.9173723012208939, Final Batch Loss: 0.21624699234962463\n",
      "Epoch 3494, Loss: 0.9291858524084091, Final Batch Loss: 0.23812313377857208\n",
      "Epoch 3495, Loss: 0.7818899154663086, Final Batch Loss: 0.13123168051242828\n",
      "Epoch 3496, Loss: 0.8330691605806351, Final Batch Loss: 0.2009785771369934\n",
      "Epoch 3497, Loss: 0.8367654085159302, Final Batch Loss: 0.2237764447927475\n",
      "Epoch 3498, Loss: 0.8881169110536575, Final Batch Loss: 0.26740562915802\n",
      "Epoch 3499, Loss: 0.8910524249076843, Final Batch Loss: 0.26850712299346924\n",
      "Epoch 3500, Loss: 0.8326638638973236, Final Batch Loss: 0.1398724615573883\n",
      "Epoch 3501, Loss: 0.8716138005256653, Final Batch Loss: 0.16875337064266205\n",
      "Epoch 3502, Loss: 0.9056166410446167, Final Batch Loss: 0.2541675269603729\n",
      "Epoch 3503, Loss: 0.9205892533063889, Final Batch Loss: 0.2134479135274887\n",
      "Epoch 3504, Loss: 0.903179794549942, Final Batch Loss: 0.23143702745437622\n",
      "Epoch 3505, Loss: 0.9425368756055832, Final Batch Loss: 0.2642068862915039\n",
      "Epoch 3506, Loss: 0.9393478333950043, Final Batch Loss: 0.2172858715057373\n",
      "Epoch 3507, Loss: 0.9295665770769119, Final Batch Loss: 0.18144206702709198\n",
      "Epoch 3508, Loss: 0.9610738307237625, Final Batch Loss: 0.2544960081577301\n",
      "Epoch 3509, Loss: 0.8786817342042923, Final Batch Loss: 0.22709280252456665\n",
      "Epoch 3510, Loss: 0.9202958345413208, Final Batch Loss: 0.2376277595758438\n",
      "Epoch 3511, Loss: 0.8708501607179642, Final Batch Loss: 0.169609934091568\n",
      "Epoch 3512, Loss: 0.9285417199134827, Final Batch Loss: 0.2691110074520111\n",
      "Epoch 3513, Loss: 0.7872014939785004, Final Batch Loss: 0.163173645734787\n",
      "Epoch 3514, Loss: 0.8525835424661636, Final Batch Loss: 0.23121586441993713\n",
      "Epoch 3515, Loss: 0.9287318587303162, Final Batch Loss: 0.23004360496997833\n",
      "Epoch 3516, Loss: 0.8331808149814606, Final Batch Loss: 0.20108242332935333\n",
      "Epoch 3517, Loss: 0.8345442414283752, Final Batch Loss: 0.23010943830013275\n",
      "Epoch 3518, Loss: 0.9063550680875778, Final Batch Loss: 0.28457164764404297\n",
      "Epoch 3519, Loss: 0.8404748737812042, Final Batch Loss: 0.22136306762695312\n",
      "Epoch 3520, Loss: 0.7674959450960159, Final Batch Loss: 0.15162120759487152\n",
      "Epoch 3521, Loss: 0.9329306185245514, Final Batch Loss: 0.2270774245262146\n",
      "Epoch 3522, Loss: 0.8344070017337799, Final Batch Loss: 0.19196109473705292\n",
      "Epoch 3523, Loss: 0.8633733987808228, Final Batch Loss: 0.24821151793003082\n",
      "Epoch 3524, Loss: 0.9254524260759354, Final Batch Loss: 0.2683710753917694\n",
      "Epoch 3525, Loss: 0.9316679835319519, Final Batch Loss: 0.3430902659893036\n",
      "Epoch 3526, Loss: 0.8604000061750412, Final Batch Loss: 0.13838985562324524\n",
      "Epoch 3527, Loss: 0.9717088043689728, Final Batch Loss: 0.22137518227100372\n",
      "Epoch 3528, Loss: 0.885922372341156, Final Batch Loss: 0.24782663583755493\n",
      "Epoch 3529, Loss: 0.8825079798698425, Final Batch Loss: 0.2771800756454468\n",
      "Epoch 3530, Loss: 0.9297269880771637, Final Batch Loss: 0.16518756747245789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3531, Loss: 0.8435254842042923, Final Batch Loss: 0.18165159225463867\n",
      "Epoch 3532, Loss: 0.9761120975017548, Final Batch Loss: 0.32382822036743164\n",
      "Epoch 3533, Loss: 0.8990536779165268, Final Batch Loss: 0.14423280954360962\n",
      "Epoch 3534, Loss: 0.9607521593570709, Final Batch Loss: 0.2952432632446289\n",
      "Epoch 3535, Loss: 0.9227110296487808, Final Batch Loss: 0.2107091248035431\n",
      "Epoch 3536, Loss: 1.030061349272728, Final Batch Loss: 0.33156102895736694\n",
      "Epoch 3537, Loss: 1.0088579654693604, Final Batch Loss: 0.27963268756866455\n",
      "Epoch 3538, Loss: 0.8435445129871368, Final Batch Loss: 0.17937767505645752\n",
      "Epoch 3539, Loss: 0.847272202372551, Final Batch Loss: 0.20349055528640747\n",
      "Epoch 3540, Loss: 0.8450166657567024, Final Batch Loss: 0.10318595916032791\n",
      "Epoch 3541, Loss: 0.8784075379371643, Final Batch Loss: 0.24186953902244568\n",
      "Epoch 3542, Loss: 0.9211365133523941, Final Batch Loss: 0.27889901399612427\n",
      "Epoch 3543, Loss: 0.9107325077056885, Final Batch Loss: 0.2792539596557617\n",
      "Epoch 3544, Loss: 0.9388192892074585, Final Batch Loss: 0.21109402179718018\n",
      "Epoch 3545, Loss: 0.8868797272443771, Final Batch Loss: 0.2092520296573639\n",
      "Epoch 3546, Loss: 0.8619701713323593, Final Batch Loss: 0.25332191586494446\n",
      "Epoch 3547, Loss: 0.8409864604473114, Final Batch Loss: 0.17820289731025696\n",
      "Epoch 3548, Loss: 0.8798688054084778, Final Batch Loss: 0.22576139867305756\n",
      "Epoch 3549, Loss: 0.7829001694917679, Final Batch Loss: 0.12739744782447815\n",
      "Epoch 3550, Loss: 0.9356888830661774, Final Batch Loss: 0.19165658950805664\n",
      "Epoch 3551, Loss: 0.8240567296743393, Final Batch Loss: 0.19340717792510986\n",
      "Epoch 3552, Loss: 0.8369672745466232, Final Batch Loss: 0.23286683857440948\n",
      "Epoch 3553, Loss: 0.8454560190439224, Final Batch Loss: 0.17753277719020844\n",
      "Epoch 3554, Loss: 0.9145996868610382, Final Batch Loss: 0.257975697517395\n",
      "Epoch 3555, Loss: 0.8857007175683975, Final Batch Loss: 0.27110159397125244\n",
      "Epoch 3556, Loss: 0.832150787115097, Final Batch Loss: 0.18541692197322845\n",
      "Epoch 3557, Loss: 0.8949969261884689, Final Batch Loss: 0.20553645491600037\n",
      "Epoch 3558, Loss: 0.9022343456745148, Final Batch Loss: 0.21051372587680817\n",
      "Epoch 3559, Loss: 0.8665132224559784, Final Batch Loss: 0.18334029614925385\n",
      "Epoch 3560, Loss: 0.9000545889139175, Final Batch Loss: 0.24116042256355286\n",
      "Epoch 3561, Loss: 0.8657148331403732, Final Batch Loss: 0.17651429772377014\n",
      "Epoch 3562, Loss: 0.8989658057689667, Final Batch Loss: 0.22953738272190094\n",
      "Epoch 3563, Loss: 0.8559436351060867, Final Batch Loss: 0.14164242148399353\n",
      "Epoch 3564, Loss: 0.8745276182889938, Final Batch Loss: 0.20679356157779694\n",
      "Epoch 3565, Loss: 0.8212855160236359, Final Batch Loss: 0.15623177587985992\n",
      "Epoch 3566, Loss: 0.8922120034694672, Final Batch Loss: 0.19382712244987488\n",
      "Epoch 3567, Loss: 0.79030342400074, Final Batch Loss: 0.1436166763305664\n",
      "Epoch 3568, Loss: 0.9185421466827393, Final Batch Loss: 0.31507641077041626\n",
      "Epoch 3569, Loss: 0.8471161276102066, Final Batch Loss: 0.23306167125701904\n",
      "Epoch 3570, Loss: 0.8029839992523193, Final Batch Loss: 0.1662432849407196\n",
      "Epoch 3571, Loss: 0.8502022922039032, Final Batch Loss: 0.21641506254673004\n",
      "Epoch 3572, Loss: 0.8995580971240997, Final Batch Loss: 0.24881325662136078\n",
      "Epoch 3573, Loss: 0.9212229549884796, Final Batch Loss: 0.2556132376194\n",
      "Epoch 3574, Loss: 0.9418445229530334, Final Batch Loss: 0.25708046555519104\n",
      "Epoch 3575, Loss: 0.8198825418949127, Final Batch Loss: 0.16658978164196014\n",
      "Epoch 3576, Loss: 0.8861905038356781, Final Batch Loss: 0.23270420730113983\n",
      "Epoch 3577, Loss: 0.8714642226696014, Final Batch Loss: 0.22461830079555511\n",
      "Epoch 3578, Loss: 0.8517447113990784, Final Batch Loss: 0.215926855802536\n",
      "Epoch 3579, Loss: 0.8288709074258804, Final Batch Loss: 0.16922900080680847\n",
      "Epoch 3580, Loss: 0.8762004375457764, Final Batch Loss: 0.19762617349624634\n",
      "Epoch 3581, Loss: 0.8587221056222916, Final Batch Loss: 0.20612427592277527\n",
      "Epoch 3582, Loss: 0.8475110977888107, Final Batch Loss: 0.23718468844890594\n",
      "Epoch 3583, Loss: 0.7373529076576233, Final Batch Loss: 0.1286202073097229\n",
      "Epoch 3584, Loss: 0.8569531887769699, Final Batch Loss: 0.14415724575519562\n",
      "Epoch 3585, Loss: 0.8519688546657562, Final Batch Loss: 0.22622735798358917\n",
      "Epoch 3586, Loss: 0.8893298953771591, Final Batch Loss: 0.13532477617263794\n",
      "Epoch 3587, Loss: 0.782772108912468, Final Batch Loss: 0.1592482328414917\n",
      "Epoch 3588, Loss: 0.8528231233358383, Final Batch Loss: 0.20584626495838165\n",
      "Epoch 3589, Loss: 0.8677501380443573, Final Batch Loss: 0.16969354450702667\n",
      "Epoch 3590, Loss: 0.8755654692649841, Final Batch Loss: 0.1765499711036682\n",
      "Epoch 3591, Loss: 0.9923154413700104, Final Batch Loss: 0.34221208095550537\n",
      "Epoch 3592, Loss: 0.864299088716507, Final Batch Loss: 0.20750169456005096\n",
      "Epoch 3593, Loss: 0.9297663122415543, Final Batch Loss: 0.30344176292419434\n",
      "Epoch 3594, Loss: 0.9135405719280243, Final Batch Loss: 0.2635876536369324\n",
      "Epoch 3595, Loss: 0.8433691710233688, Final Batch Loss: 0.18638034164905548\n",
      "Epoch 3596, Loss: 0.8398469388484955, Final Batch Loss: 0.15943416953086853\n",
      "Epoch 3597, Loss: 0.8431675285100937, Final Batch Loss: 0.19536858797073364\n",
      "Epoch 3598, Loss: 0.8776768743991852, Final Batch Loss: 0.23922184109687805\n",
      "Epoch 3599, Loss: 0.8095329701900482, Final Batch Loss: 0.13043050467967987\n",
      "Epoch 3600, Loss: 1.0090366154909134, Final Batch Loss: 0.27955594658851624\n",
      "Epoch 3601, Loss: 0.9473780542612076, Final Batch Loss: 0.3378322422504425\n",
      "Epoch 3602, Loss: 0.9691022485494614, Final Batch Loss: 0.2846737205982208\n",
      "Epoch 3603, Loss: 1.0367828011512756, Final Batch Loss: 0.3334018886089325\n",
      "Epoch 3604, Loss: 0.9989313781261444, Final Batch Loss: 0.2711185812950134\n",
      "Epoch 3605, Loss: 1.0013256967067719, Final Batch Loss: 0.3210076689720154\n",
      "Epoch 3606, Loss: 0.9943772703409195, Final Batch Loss: 0.2510225176811218\n",
      "Epoch 3607, Loss: 0.8856845498085022, Final Batch Loss: 0.19300290942192078\n",
      "Epoch 3608, Loss: 0.8582103997468948, Final Batch Loss: 0.20347201824188232\n",
      "Epoch 3609, Loss: 0.8584280908107758, Final Batch Loss: 0.19677725434303284\n",
      "Epoch 3610, Loss: 0.8449505269527435, Final Batch Loss: 0.1950647383928299\n",
      "Epoch 3611, Loss: 0.8512425273656845, Final Batch Loss: 0.1452600061893463\n",
      "Epoch 3612, Loss: 0.9183980226516724, Final Batch Loss: 0.23704522848129272\n",
      "Epoch 3613, Loss: 0.9150035232305527, Final Batch Loss: 0.22263722121715546\n",
      "Epoch 3614, Loss: 0.8640526235103607, Final Batch Loss: 0.17730915546417236\n",
      "Epoch 3615, Loss: 0.8209626972675323, Final Batch Loss: 0.17010380327701569\n",
      "Epoch 3616, Loss: 0.8936391472816467, Final Batch Loss: 0.1917668879032135\n",
      "Epoch 3617, Loss: 0.8366311490535736, Final Batch Loss: 0.1850421130657196\n",
      "Epoch 3618, Loss: 0.8714618235826492, Final Batch Loss: 0.22632692754268646\n",
      "Epoch 3619, Loss: 0.8357179313898087, Final Batch Loss: 0.18071241676807404\n",
      "Epoch 3620, Loss: 0.8343079090118408, Final Batch Loss: 0.18853065371513367\n",
      "Epoch 3621, Loss: 0.8091395795345306, Final Batch Loss: 0.17983615398406982\n",
      "Epoch 3622, Loss: 0.8515705317258835, Final Batch Loss: 0.22580161690711975\n",
      "Epoch 3623, Loss: 0.9253572076559067, Final Batch Loss: 0.28709012269973755\n",
      "Epoch 3624, Loss: 0.8936083912849426, Final Batch Loss: 0.2018880993127823\n",
      "Epoch 3625, Loss: 0.9030457139015198, Final Batch Loss: 0.21587304770946503\n",
      "Epoch 3626, Loss: 0.8977233171463013, Final Batch Loss: 0.25444239377975464\n",
      "Epoch 3627, Loss: 0.8561598807573318, Final Batch Loss: 0.25386425852775574\n",
      "Epoch 3628, Loss: 0.871031254529953, Final Batch Loss: 0.18635523319244385\n",
      "Epoch 3629, Loss: 0.8823115527629852, Final Batch Loss: 0.23078863322734833\n",
      "Epoch 3630, Loss: 0.88678939640522, Final Batch Loss: 0.2249247431755066\n",
      "Epoch 3631, Loss: 0.9112051576375961, Final Batch Loss: 0.22745168209075928\n",
      "Epoch 3632, Loss: 0.9438580125570297, Final Batch Loss: 0.25393301248550415\n",
      "Epoch 3633, Loss: 0.8637945801019669, Final Batch Loss: 0.23459449410438538\n",
      "Epoch 3634, Loss: 0.8863542228937149, Final Batch Loss: 0.1494356095790863\n",
      "Epoch 3635, Loss: 0.8610805422067642, Final Batch Loss: 0.21240822970867157\n",
      "Epoch 3636, Loss: 0.8956025242805481, Final Batch Loss: 0.26769647002220154\n",
      "Epoch 3637, Loss: 0.8554850518703461, Final Batch Loss: 0.27685630321502686\n",
      "Epoch 3638, Loss: 0.8367993831634521, Final Batch Loss: 0.12949047982692719\n",
      "Epoch 3639, Loss: 1.0844069570302963, Final Batch Loss: 0.39611950516700745\n",
      "Epoch 3640, Loss: 0.883489340543747, Final Batch Loss: 0.187971830368042\n",
      "Epoch 3641, Loss: 0.8937114775180817, Final Batch Loss: 0.23544800281524658\n",
      "Epoch 3642, Loss: 0.8400845974683762, Final Batch Loss: 0.21662232279777527\n",
      "Epoch 3643, Loss: 0.7801490873098373, Final Batch Loss: 0.18231891095638275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3644, Loss: 0.7960687577724457, Final Batch Loss: 0.20544257760047913\n",
      "Epoch 3645, Loss: 0.8712477684020996, Final Batch Loss: 0.16401143372058868\n",
      "Epoch 3646, Loss: 0.8745002299547195, Final Batch Loss: 0.16206128895282745\n",
      "Epoch 3647, Loss: 0.8510493189096451, Final Batch Loss: 0.18901431560516357\n",
      "Epoch 3648, Loss: 0.8092322945594788, Final Batch Loss: 0.17254593968391418\n",
      "Epoch 3649, Loss: 0.9270483702421188, Final Batch Loss: 0.3069595396518707\n",
      "Epoch 3650, Loss: 0.8387214094400406, Final Batch Loss: 0.2003980427980423\n",
      "Epoch 3651, Loss: 0.8995185792446136, Final Batch Loss: 0.24811053276062012\n",
      "Epoch 3652, Loss: 0.8556789308786392, Final Batch Loss: 0.22112664580345154\n",
      "Epoch 3653, Loss: 0.868092954158783, Final Batch Loss: 0.16158629953861237\n",
      "Epoch 3654, Loss: 0.8524327725172043, Final Batch Loss: 0.2332231104373932\n",
      "Epoch 3655, Loss: 0.878210186958313, Final Batch Loss: 0.3080597221851349\n",
      "Epoch 3656, Loss: 0.8539606779813766, Final Batch Loss: 0.2248620241880417\n",
      "Epoch 3657, Loss: 0.9285464733839035, Final Batch Loss: 0.291434109210968\n",
      "Epoch 3658, Loss: 0.8496291786432266, Final Batch Loss: 0.20964586734771729\n",
      "Epoch 3659, Loss: 0.8051205724477768, Final Batch Loss: 0.17994508147239685\n",
      "Epoch 3660, Loss: 0.8534614890813828, Final Batch Loss: 0.15328453481197357\n",
      "Epoch 3661, Loss: 0.8760005682706833, Final Batch Loss: 0.23631581664085388\n",
      "Epoch 3662, Loss: 0.9651956856250763, Final Batch Loss: 0.16715295612812042\n",
      "Epoch 3663, Loss: 0.8453236967325211, Final Batch Loss: 0.19548793137073517\n",
      "Epoch 3664, Loss: 0.8601226210594177, Final Batch Loss: 0.21284016966819763\n",
      "Epoch 3665, Loss: 0.8299681693315506, Final Batch Loss: 0.24773897230625153\n",
      "Epoch 3666, Loss: 0.894295871257782, Final Batch Loss: 0.2205461710691452\n",
      "Epoch 3667, Loss: 0.7885164171457291, Final Batch Loss: 0.13707491755485535\n",
      "Epoch 3668, Loss: 0.9061780571937561, Final Batch Loss: 0.2221759706735611\n",
      "Epoch 3669, Loss: 0.836235448718071, Final Batch Loss: 0.22395414113998413\n",
      "Epoch 3670, Loss: 0.9063746929168701, Final Batch Loss: 0.21132299304008484\n",
      "Epoch 3671, Loss: 0.796302929520607, Final Batch Loss: 0.21275900304317474\n",
      "Epoch 3672, Loss: 0.9029187113046646, Final Batch Loss: 0.2425023764371872\n",
      "Epoch 3673, Loss: 0.9582674205303192, Final Batch Loss: 0.23691292107105255\n",
      "Epoch 3674, Loss: 0.8757693916559219, Final Batch Loss: 0.22599467635154724\n",
      "Epoch 3675, Loss: 0.7922977954149246, Final Batch Loss: 0.18576060235500336\n",
      "Epoch 3676, Loss: 0.850856363773346, Final Batch Loss: 0.16119946539402008\n",
      "Epoch 3677, Loss: 0.8067224025726318, Final Batch Loss: 0.16794970631599426\n",
      "Epoch 3678, Loss: 0.8114054799079895, Final Batch Loss: 0.1644994467496872\n",
      "Epoch 3679, Loss: 0.8433126360177994, Final Batch Loss: 0.20579300820827484\n",
      "Epoch 3680, Loss: 0.8631276935338974, Final Batch Loss: 0.25375592708587646\n",
      "Epoch 3681, Loss: 0.8005735725164413, Final Batch Loss: 0.1764042228460312\n",
      "Epoch 3682, Loss: 0.9144473075866699, Final Batch Loss: 0.24071146547794342\n",
      "Epoch 3683, Loss: 0.7962965220212936, Final Batch Loss: 0.15708284080028534\n",
      "Epoch 3684, Loss: 0.9189423620700836, Final Batch Loss: 0.2836593687534332\n",
      "Epoch 3685, Loss: 0.8503245115280151, Final Batch Loss: 0.19150879979133606\n",
      "Epoch 3686, Loss: 0.8381574600934982, Final Batch Loss: 0.1939217448234558\n",
      "Epoch 3687, Loss: 0.8611566573381424, Final Batch Loss: 0.2424621731042862\n",
      "Epoch 3688, Loss: 0.8913971036672592, Final Batch Loss: 0.27354896068573\n",
      "Epoch 3689, Loss: 0.8292561620473862, Final Batch Loss: 0.1996442973613739\n",
      "Epoch 3690, Loss: 0.8364145904779434, Final Batch Loss: 0.17100292444229126\n",
      "Epoch 3691, Loss: 0.9095599949359894, Final Batch Loss: 0.1991576999425888\n",
      "Epoch 3692, Loss: 0.9711281955242157, Final Batch Loss: 0.34185531735420227\n",
      "Epoch 3693, Loss: 0.8829450309276581, Final Batch Loss: 0.16255782544612885\n",
      "Epoch 3694, Loss: 0.9344162195920944, Final Batch Loss: 0.24504317343235016\n",
      "Epoch 3695, Loss: 0.8088863044977188, Final Batch Loss: 0.18131734430789948\n",
      "Epoch 3696, Loss: 0.8056499063968658, Final Batch Loss: 0.16198499500751495\n",
      "Epoch 3697, Loss: 0.8047637790441513, Final Batch Loss: 0.17410385608673096\n",
      "Epoch 3698, Loss: 0.8460938334465027, Final Batch Loss: 0.2532361149787903\n",
      "Epoch 3699, Loss: 0.8892572671175003, Final Batch Loss: 0.22797971963882446\n",
      "Epoch 3700, Loss: 0.8585156798362732, Final Batch Loss: 0.20469515025615692\n",
      "Epoch 3701, Loss: 0.956495851278305, Final Batch Loss: 0.287991464138031\n",
      "Epoch 3702, Loss: 0.8277518153190613, Final Batch Loss: 0.15647666156291962\n",
      "Epoch 3703, Loss: 0.8249944746494293, Final Batch Loss: 0.18528567254543304\n",
      "Epoch 3704, Loss: 0.9401597529649734, Final Batch Loss: 0.28154483437538147\n",
      "Epoch 3705, Loss: 0.8835251182317734, Final Batch Loss: 0.22493773698806763\n",
      "Epoch 3706, Loss: 0.8072209060192108, Final Batch Loss: 0.18807251751422882\n",
      "Epoch 3707, Loss: 0.90061454474926, Final Batch Loss: 0.19848868250846863\n",
      "Epoch 3708, Loss: 0.8558128327131271, Final Batch Loss: 0.21224799752235413\n",
      "Epoch 3709, Loss: 0.8217677175998688, Final Batch Loss: 0.19539235532283783\n",
      "Epoch 3710, Loss: 0.8679542392492294, Final Batch Loss: 0.16704024374485016\n",
      "Epoch 3711, Loss: 0.9603923559188843, Final Batch Loss: 0.23181860148906708\n",
      "Epoch 3712, Loss: 1.0249584913253784, Final Batch Loss: 0.2944389283657074\n",
      "Epoch 3713, Loss: 0.8322043120861053, Final Batch Loss: 0.20272301137447357\n",
      "Epoch 3714, Loss: 0.859310507774353, Final Batch Loss: 0.20787103474140167\n",
      "Epoch 3715, Loss: 0.8131681978702545, Final Batch Loss: 0.1816556751728058\n",
      "Epoch 3716, Loss: 0.8506039977073669, Final Batch Loss: 0.2049517184495926\n",
      "Epoch 3717, Loss: 0.8222483694553375, Final Batch Loss: 0.2276349514722824\n",
      "Epoch 3718, Loss: 0.8897936195135117, Final Batch Loss: 0.2958633005619049\n",
      "Epoch 3719, Loss: 0.9239172637462616, Final Batch Loss: 0.3094938397407532\n",
      "Epoch 3720, Loss: 0.8762309402227402, Final Batch Loss: 0.26704317331314087\n",
      "Epoch 3721, Loss: 0.866311177611351, Final Batch Loss: 0.19070734083652496\n",
      "Epoch 3722, Loss: 0.9196175783872604, Final Batch Loss: 0.283555805683136\n",
      "Epoch 3723, Loss: 0.8696568012237549, Final Batch Loss: 0.15389934182167053\n",
      "Epoch 3724, Loss: 0.8084794282913208, Final Batch Loss: 0.14479811489582062\n",
      "Epoch 3725, Loss: 0.806063711643219, Final Batch Loss: 0.15354019403457642\n",
      "Epoch 3726, Loss: 0.8453884720802307, Final Batch Loss: 0.1971188634634018\n",
      "Epoch 3727, Loss: 0.866489365696907, Final Batch Loss: 0.1867796927690506\n",
      "Epoch 3728, Loss: 0.9129184484481812, Final Batch Loss: 0.27264824509620667\n",
      "Epoch 3729, Loss: 0.7568466663360596, Final Batch Loss: 0.12531106173992157\n",
      "Epoch 3730, Loss: 0.9001224935054779, Final Batch Loss: 0.29107147455215454\n",
      "Epoch 3731, Loss: 0.8817340582609177, Final Batch Loss: 0.2696598470211029\n",
      "Epoch 3732, Loss: 0.8724425137042999, Final Batch Loss: 0.2070186287164688\n",
      "Epoch 3733, Loss: 0.8003973811864853, Final Batch Loss: 0.20908305048942566\n",
      "Epoch 3734, Loss: 0.8337864130735397, Final Batch Loss: 0.23436982929706573\n",
      "Epoch 3735, Loss: 0.8023308515548706, Final Batch Loss: 0.18509326875209808\n",
      "Epoch 3736, Loss: 0.919897124171257, Final Batch Loss: 0.2731190025806427\n",
      "Epoch 3737, Loss: 0.8237060606479645, Final Batch Loss: 0.21191927790641785\n",
      "Epoch 3738, Loss: 0.805669829249382, Final Batch Loss: 0.19465956091880798\n",
      "Epoch 3739, Loss: 0.8020137846469879, Final Batch Loss: 0.20606622099876404\n",
      "Epoch 3740, Loss: 0.898596853017807, Final Batch Loss: 0.21619784832000732\n",
      "Epoch 3741, Loss: 0.8343053609132767, Final Batch Loss: 0.2633384168148041\n",
      "Epoch 3742, Loss: 0.843666285276413, Final Batch Loss: 0.22081030905246735\n",
      "Epoch 3743, Loss: 0.9957320690155029, Final Batch Loss: 0.3578794002532959\n",
      "Epoch 3744, Loss: 0.8798270970582962, Final Batch Loss: 0.2505182921886444\n",
      "Epoch 3745, Loss: 1.0299812704324722, Final Batch Loss: 0.3088326156139374\n",
      "Epoch 3746, Loss: 0.9379249662160873, Final Batch Loss: 0.2512415647506714\n",
      "Epoch 3747, Loss: 0.8624792546033859, Final Batch Loss: 0.22612100839614868\n",
      "Epoch 3748, Loss: 0.9243579804897308, Final Batch Loss: 0.23158521950244904\n",
      "Epoch 3749, Loss: 0.9851081669330597, Final Batch Loss: 0.24413645267486572\n",
      "Epoch 3750, Loss: 0.9271737933158875, Final Batch Loss: 0.3248329758644104\n",
      "Epoch 3751, Loss: 0.9236729294061661, Final Batch Loss: 0.2346724420785904\n",
      "Epoch 3752, Loss: 0.9065085649490356, Final Batch Loss: 0.2303832620382309\n",
      "Epoch 3753, Loss: 0.8667377382516861, Final Batch Loss: 0.2231733500957489\n",
      "Epoch 3754, Loss: 0.8331223726272583, Final Batch Loss: 0.2237415909767151\n",
      "Epoch 3755, Loss: 0.8293311297893524, Final Batch Loss: 0.17654801905155182\n",
      "Epoch 3756, Loss: 0.9316190332174301, Final Batch Loss: 0.29176270961761475\n",
      "Epoch 3757, Loss: 0.8384409993886948, Final Batch Loss: 0.2516888678073883\n",
      "Epoch 3758, Loss: 0.7850086838006973, Final Batch Loss: 0.17839515209197998\n",
      "Epoch 3759, Loss: 0.8928070813417435, Final Batch Loss: 0.2562609314918518\n",
      "Epoch 3760, Loss: 0.8497473299503326, Final Batch Loss: 0.18670515716075897\n",
      "Epoch 3761, Loss: 0.7944868952035904, Final Batch Loss: 0.17017321288585663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3762, Loss: 0.8660148978233337, Final Batch Loss: 0.18933455646038055\n",
      "Epoch 3763, Loss: 0.779471829533577, Final Batch Loss: 0.15257568657398224\n",
      "Epoch 3764, Loss: 0.8627854883670807, Final Batch Loss: 0.24841159582138062\n",
      "Epoch 3765, Loss: 0.8734811544418335, Final Batch Loss: 0.2678109407424927\n",
      "Epoch 3766, Loss: 0.8191305696964264, Final Batch Loss: 0.17063391208648682\n",
      "Epoch 3767, Loss: 0.8289149850606918, Final Batch Loss: 0.20808497071266174\n",
      "Epoch 3768, Loss: 0.9084954857826233, Final Batch Loss: 0.2858915627002716\n",
      "Epoch 3769, Loss: 0.769475594162941, Final Batch Loss: 0.2104048877954483\n",
      "Epoch 3770, Loss: 0.7726401537656784, Final Batch Loss: 0.20848409831523895\n",
      "Epoch 3771, Loss: 0.9321886152029037, Final Batch Loss: 0.26822730898857117\n",
      "Epoch 3772, Loss: 0.9727137237787247, Final Batch Loss: 0.36631229519844055\n",
      "Epoch 3773, Loss: 0.9442576915025711, Final Batch Loss: 0.3441241979598999\n",
      "Epoch 3774, Loss: 0.8400497883558273, Final Batch Loss: 0.2440241575241089\n",
      "Epoch 3775, Loss: 0.9330484569072723, Final Batch Loss: 0.31828904151916504\n",
      "Epoch 3776, Loss: 0.8800519108772278, Final Batch Loss: 0.26665422320365906\n",
      "Epoch 3777, Loss: 0.7884554117918015, Final Batch Loss: 0.14330939948558807\n",
      "Epoch 3778, Loss: 0.920295849442482, Final Batch Loss: 0.1846492439508438\n",
      "Epoch 3779, Loss: 0.8586586564779282, Final Batch Loss: 0.14685513079166412\n",
      "Epoch 3780, Loss: 0.8514605313539505, Final Batch Loss: 0.22978293895721436\n",
      "Epoch 3781, Loss: 0.7538265585899353, Final Batch Loss: 0.1355309635400772\n",
      "Epoch 3782, Loss: 0.8944463580846786, Final Batch Loss: 0.2898637354373932\n",
      "Epoch 3783, Loss: 0.870550811290741, Final Batch Loss: 0.21872782707214355\n",
      "Epoch 3784, Loss: 0.9514507800340652, Final Batch Loss: 0.3345485031604767\n",
      "Epoch 3785, Loss: 0.833622008562088, Final Batch Loss: 0.19912277162075043\n",
      "Epoch 3786, Loss: 0.8998721241950989, Final Batch Loss: 0.28988945484161377\n",
      "Epoch 3787, Loss: 0.8596306890249252, Final Batch Loss: 0.19078415632247925\n",
      "Epoch 3788, Loss: 0.8745635747909546, Final Batch Loss: 0.21897456049919128\n",
      "Epoch 3789, Loss: 0.8006322681903839, Final Batch Loss: 0.16468545794487\n",
      "Epoch 3790, Loss: 0.9143501371145248, Final Batch Loss: 0.1903604120016098\n",
      "Epoch 3791, Loss: 0.9022830426692963, Final Batch Loss: 0.2589065730571747\n",
      "Epoch 3792, Loss: 0.960605576634407, Final Batch Loss: 0.26257437467575073\n",
      "Epoch 3793, Loss: 0.833945095539093, Final Batch Loss: 0.23564890027046204\n",
      "Epoch 3794, Loss: 0.7722546309232712, Final Batch Loss: 0.14844360947608948\n",
      "Epoch 3795, Loss: 0.8034535199403763, Final Batch Loss: 0.20838351547718048\n",
      "Epoch 3796, Loss: 0.7733213678002357, Final Batch Loss: 0.10923949629068375\n",
      "Epoch 3797, Loss: 0.8646934479475021, Final Batch Loss: 0.23189693689346313\n",
      "Epoch 3798, Loss: 0.8218177258968353, Final Batch Loss: 0.22296850383281708\n",
      "Epoch 3799, Loss: 0.8287451416254044, Final Batch Loss: 0.17493188381195068\n",
      "Epoch 3800, Loss: 0.8179686665534973, Final Batch Loss: 0.19452178478240967\n",
      "Epoch 3801, Loss: 0.8565170913934708, Final Batch Loss: 0.15865209698677063\n",
      "Epoch 3802, Loss: 0.8220424652099609, Final Batch Loss: 0.1920645833015442\n",
      "Epoch 3803, Loss: 0.7452654987573624, Final Batch Loss: 0.14808759093284607\n",
      "Epoch 3804, Loss: 0.8354383856058121, Final Batch Loss: 0.17144188284873962\n",
      "Epoch 3805, Loss: 0.825016126036644, Final Batch Loss: 0.17347455024719238\n",
      "Epoch 3806, Loss: 0.7782276570796967, Final Batch Loss: 0.17035724222660065\n",
      "Epoch 3807, Loss: 0.8385401368141174, Final Batch Loss: 0.21497303247451782\n",
      "Epoch 3808, Loss: 0.7821624875068665, Final Batch Loss: 0.14545057713985443\n",
      "Epoch 3809, Loss: 0.8106928765773773, Final Batch Loss: 0.2424575239419937\n",
      "Epoch 3810, Loss: 0.8511708825826645, Final Batch Loss: 0.21475385129451752\n",
      "Epoch 3811, Loss: 0.9603784382343292, Final Batch Loss: 0.25627124309539795\n",
      "Epoch 3812, Loss: 0.8410982489585876, Final Batch Loss: 0.1656976044178009\n",
      "Epoch 3813, Loss: 0.8796964585781097, Final Batch Loss: 0.272818922996521\n",
      "Epoch 3814, Loss: 0.8539115488529205, Final Batch Loss: 0.25230464339256287\n",
      "Epoch 3815, Loss: 0.8112233430147171, Final Batch Loss: 0.21370893716812134\n",
      "Epoch 3816, Loss: 0.7812572121620178, Final Batch Loss: 0.19839709997177124\n",
      "Epoch 3817, Loss: 0.8714651763439178, Final Batch Loss: 0.23049700260162354\n",
      "Epoch 3818, Loss: 0.7744864374399185, Final Batch Loss: 0.14917834103107452\n",
      "Epoch 3819, Loss: 0.8429352641105652, Final Batch Loss: 0.16928504407405853\n",
      "Epoch 3820, Loss: 0.8652050495147705, Final Batch Loss: 0.19775144755840302\n",
      "Epoch 3821, Loss: 0.7835559695959091, Final Batch Loss: 0.1357906013727188\n",
      "Epoch 3822, Loss: 0.7699200659990311, Final Batch Loss: 0.1741514801979065\n",
      "Epoch 3823, Loss: 0.885576531291008, Final Batch Loss: 0.21325157582759857\n",
      "Epoch 3824, Loss: 0.9322924017906189, Final Batch Loss: 0.25303328037261963\n",
      "Epoch 3825, Loss: 0.89547099173069, Final Batch Loss: 0.16529995203018188\n",
      "Epoch 3826, Loss: 0.8191886395215988, Final Batch Loss: 0.16172350943088531\n",
      "Epoch 3827, Loss: 0.7965870797634125, Final Batch Loss: 0.1507451981306076\n",
      "Epoch 3828, Loss: 0.9032171219587326, Final Batch Loss: 0.2191101461648941\n",
      "Epoch 3829, Loss: 0.8702538460493088, Final Batch Loss: 0.20659038424491882\n",
      "Epoch 3830, Loss: 0.895366296172142, Final Batch Loss: 0.23137789964675903\n",
      "Epoch 3831, Loss: 0.8616021871566772, Final Batch Loss: 0.18306660652160645\n",
      "Epoch 3832, Loss: 0.9019881337881088, Final Batch Loss: 0.27434080839157104\n",
      "Epoch 3833, Loss: 0.8472031503915787, Final Batch Loss: 0.2338460087776184\n",
      "Epoch 3834, Loss: 0.8222559988498688, Final Batch Loss: 0.2472124695777893\n",
      "Epoch 3835, Loss: 0.8562294542789459, Final Batch Loss: 0.19869333505630493\n",
      "Epoch 3836, Loss: 0.8460806310176849, Final Batch Loss: 0.2233593463897705\n",
      "Epoch 3837, Loss: 0.9873128235340118, Final Batch Loss: 0.31130051612854004\n",
      "Epoch 3838, Loss: 0.9490731805562973, Final Batch Loss: 0.24761411547660828\n",
      "Epoch 3839, Loss: 0.8450884371995926, Final Batch Loss: 0.22735439240932465\n",
      "Epoch 3840, Loss: 0.7842898070812225, Final Batch Loss: 0.15280799567699432\n",
      "Epoch 3841, Loss: 0.8678288012742996, Final Batch Loss: 0.2063121199607849\n",
      "Epoch 3842, Loss: 0.8419239968061447, Final Batch Loss: 0.18782338500022888\n",
      "Epoch 3843, Loss: 0.8172207176685333, Final Batch Loss: 0.18524175882339478\n",
      "Epoch 3844, Loss: 0.8147339671850204, Final Batch Loss: 0.21389712393283844\n",
      "Epoch 3845, Loss: 0.8755484521389008, Final Batch Loss: 0.2206428200006485\n",
      "Epoch 3846, Loss: 0.9365947395563126, Final Batch Loss: 0.31855320930480957\n",
      "Epoch 3847, Loss: 0.9369634389877319, Final Batch Loss: 0.32690906524658203\n",
      "Epoch 3848, Loss: 0.8489862531423569, Final Batch Loss: 0.19410906732082367\n",
      "Epoch 3849, Loss: 0.9593058526515961, Final Batch Loss: 0.3605501353740692\n",
      "Epoch 3850, Loss: 0.8472840636968613, Final Batch Loss: 0.19980621337890625\n",
      "Epoch 3851, Loss: 0.9163675159215927, Final Batch Loss: 0.22972355782985687\n",
      "Epoch 3852, Loss: 0.8532007038593292, Final Batch Loss: 0.2042718380689621\n",
      "Epoch 3853, Loss: 0.847249448299408, Final Batch Loss: 0.17670205235481262\n",
      "Epoch 3854, Loss: 0.8736861497163773, Final Batch Loss: 0.25654906034469604\n",
      "Epoch 3855, Loss: 0.8207554966211319, Final Batch Loss: 0.18473418056964874\n",
      "Epoch 3856, Loss: 0.8358316123485565, Final Batch Loss: 0.14470721781253815\n",
      "Epoch 3857, Loss: 0.9352520108222961, Final Batch Loss: 0.2869448959827423\n",
      "Epoch 3858, Loss: 0.8747562021017075, Final Batch Loss: 0.23286181688308716\n",
      "Epoch 3859, Loss: 0.8684269785881042, Final Batch Loss: 0.23685111105442047\n",
      "Epoch 3860, Loss: 0.9450641423463821, Final Batch Loss: 0.2458636462688446\n",
      "Epoch 3861, Loss: 0.8739603161811829, Final Batch Loss: 0.20453979074954987\n",
      "Epoch 3862, Loss: 0.9981903284788132, Final Batch Loss: 0.28820762038230896\n",
      "Epoch 3863, Loss: 0.8244936764240265, Final Batch Loss: 0.1811171919107437\n",
      "Epoch 3864, Loss: 0.8235763311386108, Final Batch Loss: 0.15405021607875824\n",
      "Epoch 3865, Loss: 0.9093932062387466, Final Batch Loss: 0.21620972454547882\n",
      "Epoch 3866, Loss: 0.8505073934793472, Final Batch Loss: 0.20216375589370728\n",
      "Epoch 3867, Loss: 0.8905057013034821, Final Batch Loss: 0.28266212344169617\n",
      "Epoch 3868, Loss: 0.9128604084253311, Final Batch Loss: 0.2955275774002075\n",
      "Epoch 3869, Loss: 0.8271233141422272, Final Batch Loss: 0.1812848150730133\n",
      "Epoch 3870, Loss: 0.8987197577953339, Final Batch Loss: 0.2498895227909088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3871, Loss: 0.908166378736496, Final Batch Loss: 0.1823505014181137\n",
      "Epoch 3872, Loss: 0.911157414317131, Final Batch Loss: 0.2841542959213257\n",
      "Epoch 3873, Loss: 0.8752624690532684, Final Batch Loss: 0.20068581402301788\n",
      "Epoch 3874, Loss: 0.9726517349481583, Final Batch Loss: 0.28355756402015686\n",
      "Epoch 3875, Loss: 0.9465141743421555, Final Batch Loss: 0.20891453325748444\n",
      "Epoch 3876, Loss: 0.7894762754440308, Final Batch Loss: 0.15731042623519897\n",
      "Epoch 3877, Loss: 0.9343788921833038, Final Batch Loss: 0.2887495458126068\n",
      "Epoch 3878, Loss: 0.905226856470108, Final Batch Loss: 0.2654001712799072\n",
      "Epoch 3879, Loss: 0.8027003109455109, Final Batch Loss: 0.1796843707561493\n",
      "Epoch 3880, Loss: 0.809242382645607, Final Batch Loss: 0.1693129688501358\n",
      "Epoch 3881, Loss: 0.9807049334049225, Final Batch Loss: 0.3428136110305786\n",
      "Epoch 3882, Loss: 0.8154284209012985, Final Batch Loss: 0.1238410621881485\n",
      "Epoch 3883, Loss: 0.8214431256055832, Final Batch Loss: 0.22514745593070984\n",
      "Epoch 3884, Loss: 0.9186008721590042, Final Batch Loss: 0.231119304895401\n",
      "Epoch 3885, Loss: 0.8933191150426865, Final Batch Loss: 0.2335105985403061\n",
      "Epoch 3886, Loss: 0.8909152746200562, Final Batch Loss: 0.24583905935287476\n",
      "Epoch 3887, Loss: 0.7856605350971222, Final Batch Loss: 0.1846279352903366\n",
      "Epoch 3888, Loss: 0.9239252805709839, Final Batch Loss: 0.294312983751297\n",
      "Epoch 3889, Loss: 0.8566537201404572, Final Batch Loss: 0.22030963003635406\n",
      "Epoch 3890, Loss: 0.8052895665168762, Final Batch Loss: 0.2182309925556183\n",
      "Epoch 3891, Loss: 0.891797810792923, Final Batch Loss: 0.23224666714668274\n",
      "Epoch 3892, Loss: 0.7791994139552116, Final Batch Loss: 0.1034412607550621\n",
      "Epoch 3893, Loss: 0.7724772393703461, Final Batch Loss: 0.15156987309455872\n",
      "Epoch 3894, Loss: 0.9232282936573029, Final Batch Loss: 0.27468541264533997\n",
      "Epoch 3895, Loss: 0.8037992715835571, Final Batch Loss: 0.1656804084777832\n",
      "Epoch 3896, Loss: 0.8720078021287918, Final Batch Loss: 0.24131326377391815\n",
      "Epoch 3897, Loss: 0.7967099994421005, Final Batch Loss: 0.2046695351600647\n",
      "Epoch 3898, Loss: 0.943687379360199, Final Batch Loss: 0.293672651052475\n",
      "Epoch 3899, Loss: 0.7835152223706245, Final Batch Loss: 0.1169864609837532\n",
      "Epoch 3900, Loss: 0.845839336514473, Final Batch Loss: 0.21351799368858337\n",
      "Epoch 3901, Loss: 0.917155459523201, Final Batch Loss: 0.21799686551094055\n",
      "Epoch 3902, Loss: 0.8256902396678925, Final Batch Loss: 0.2157381772994995\n",
      "Epoch 3903, Loss: 0.9302990138530731, Final Batch Loss: 0.20524567365646362\n",
      "Epoch 3904, Loss: 0.7984033972024918, Final Batch Loss: 0.14105865359306335\n",
      "Epoch 3905, Loss: 0.8558600544929504, Final Batch Loss: 0.22002242505550385\n",
      "Epoch 3906, Loss: 0.8440097123384476, Final Batch Loss: 0.19801010191440582\n",
      "Epoch 3907, Loss: 0.8050390481948853, Final Batch Loss: 0.17172855138778687\n",
      "Epoch 3908, Loss: 0.8407130539417267, Final Batch Loss: 0.1998683661222458\n",
      "Epoch 3909, Loss: 0.8242606669664383, Final Batch Loss: 0.19781674444675446\n",
      "Epoch 3910, Loss: 0.8243959844112396, Final Batch Loss: 0.18878205120563507\n",
      "Epoch 3911, Loss: 0.8474051356315613, Final Batch Loss: 0.22482316195964813\n",
      "Epoch 3912, Loss: 0.8045637458562851, Final Batch Loss: 0.1618189513683319\n",
      "Epoch 3913, Loss: 0.7963569611310959, Final Batch Loss: 0.16834072768688202\n",
      "Epoch 3914, Loss: 0.8460809886455536, Final Batch Loss: 0.25138455629348755\n",
      "Epoch 3915, Loss: 0.8393072932958603, Final Batch Loss: 0.194758802652359\n",
      "Epoch 3916, Loss: 0.8528955727815628, Final Batch Loss: 0.19671855866909027\n",
      "Epoch 3917, Loss: 0.8575270026922226, Final Batch Loss: 0.2180856615304947\n",
      "Epoch 3918, Loss: 0.8530765473842621, Final Batch Loss: 0.26673370599746704\n",
      "Epoch 3919, Loss: 0.9455441981554031, Final Batch Loss: 0.26751047372817993\n",
      "Epoch 3920, Loss: 0.8622788786888123, Final Batch Loss: 0.18089015781879425\n",
      "Epoch 3921, Loss: 0.835669681429863, Final Batch Loss: 0.21877221763134003\n",
      "Epoch 3922, Loss: 0.8563088178634644, Final Batch Loss: 0.23059163987636566\n",
      "Epoch 3923, Loss: 0.8735857158899307, Final Batch Loss: 0.23815661668777466\n",
      "Epoch 3924, Loss: 0.8044526427984238, Final Batch Loss: 0.20538540184497833\n",
      "Epoch 3925, Loss: 0.8529236912727356, Final Batch Loss: 0.19960437715053558\n",
      "Epoch 3926, Loss: 0.8152075856924057, Final Batch Loss: 0.17097686231136322\n",
      "Epoch 3927, Loss: 0.8364685326814651, Final Batch Loss: 0.2077455371618271\n",
      "Epoch 3928, Loss: 0.9198761284351349, Final Batch Loss: 0.26160475611686707\n",
      "Epoch 3929, Loss: 0.9601944088935852, Final Batch Loss: 0.33674389123916626\n",
      "Epoch 3930, Loss: 0.8540290296077728, Final Batch Loss: 0.20232586562633514\n",
      "Epoch 3931, Loss: 0.8153262287378311, Final Batch Loss: 0.17158159613609314\n",
      "Epoch 3932, Loss: 0.8164108544588089, Final Batch Loss: 0.16847631335258484\n",
      "Epoch 3933, Loss: 0.802985206246376, Final Batch Loss: 0.2195156365633011\n",
      "Epoch 3934, Loss: 0.8815128803253174, Final Batch Loss: 0.23218244314193726\n",
      "Epoch 3935, Loss: 0.883239820599556, Final Batch Loss: 0.1971127837896347\n",
      "Epoch 3936, Loss: 0.8174011260271072, Final Batch Loss: 0.14687111973762512\n",
      "Epoch 3937, Loss: 0.7947399765253067, Final Batch Loss: 0.22356471419334412\n",
      "Epoch 3938, Loss: 0.9207764565944672, Final Batch Loss: 0.16489115357398987\n",
      "Epoch 3939, Loss: 0.694057509303093, Final Batch Loss: 0.12338635325431824\n",
      "Epoch 3940, Loss: 0.8403382003307343, Final Batch Loss: 0.2198355346918106\n",
      "Epoch 3941, Loss: 0.8305623233318329, Final Batch Loss: 0.2530056834220886\n",
      "Epoch 3942, Loss: 0.8950565457344055, Final Batch Loss: 0.20971958339214325\n",
      "Epoch 3943, Loss: 0.8145455718040466, Final Batch Loss: 0.1473391354084015\n",
      "Epoch 3944, Loss: 0.888301745057106, Final Batch Loss: 0.1924847960472107\n",
      "Epoch 3945, Loss: 0.8534270823001862, Final Batch Loss: 0.20932461321353912\n",
      "Epoch 3946, Loss: 0.8275177180767059, Final Batch Loss: 0.19554239511489868\n",
      "Epoch 3947, Loss: 0.901004284620285, Final Batch Loss: 0.2254180908203125\n",
      "Epoch 3948, Loss: 0.8787852674722672, Final Batch Loss: 0.2676696181297302\n",
      "Epoch 3949, Loss: 0.8264251351356506, Final Batch Loss: 0.18585602939128876\n",
      "Epoch 3950, Loss: 0.882971853017807, Final Batch Loss: 0.240536630153656\n",
      "Epoch 3951, Loss: 0.8104735314846039, Final Batch Loss: 0.16602763533592224\n",
      "Epoch 3952, Loss: 0.9771420955657959, Final Batch Loss: 0.29403525590896606\n",
      "Epoch 3953, Loss: 0.9603269547224045, Final Batch Loss: 0.24585382640361786\n",
      "Epoch 3954, Loss: 0.8474436104297638, Final Batch Loss: 0.1935548037290573\n",
      "Epoch 3955, Loss: 0.7870073169469833, Final Batch Loss: 0.16799236834049225\n",
      "Epoch 3956, Loss: 0.8161526173353195, Final Batch Loss: 0.17708119750022888\n",
      "Epoch 3957, Loss: 0.8919896930456161, Final Batch Loss: 0.2765169143676758\n",
      "Epoch 3958, Loss: 0.8012364953756332, Final Batch Loss: 0.14471472799777985\n",
      "Epoch 3959, Loss: 0.8609776943922043, Final Batch Loss: 0.2438511848449707\n",
      "Epoch 3960, Loss: 0.964229166507721, Final Batch Loss: 0.21280315518379211\n",
      "Epoch 3961, Loss: 0.8168800622224808, Final Batch Loss: 0.21306383609771729\n",
      "Epoch 3962, Loss: 0.8637739568948746, Final Batch Loss: 0.2814458906650543\n",
      "Epoch 3963, Loss: 0.7996467500925064, Final Batch Loss: 0.20349501073360443\n",
      "Epoch 3964, Loss: 0.8608441799879074, Final Batch Loss: 0.1943121701478958\n",
      "Epoch 3965, Loss: 0.8752047419548035, Final Batch Loss: 0.21888825297355652\n",
      "Epoch 3966, Loss: 0.8010997474193573, Final Batch Loss: 0.17384634912014008\n",
      "Epoch 3967, Loss: 0.922038659453392, Final Batch Loss: 0.24394603073596954\n",
      "Epoch 3968, Loss: 0.7975515425205231, Final Batch Loss: 0.2425413429737091\n",
      "Epoch 3969, Loss: 0.8777370154857635, Final Batch Loss: 0.2187483012676239\n",
      "Epoch 3970, Loss: 0.9460103064775467, Final Batch Loss: 0.2658848762512207\n",
      "Epoch 3971, Loss: 0.8221051394939423, Final Batch Loss: 0.17348557710647583\n",
      "Epoch 3972, Loss: 0.8775187581777573, Final Batch Loss: 0.16786471009254456\n",
      "Epoch 3973, Loss: 0.8828351944684982, Final Batch Loss: 0.19594734907150269\n",
      "Epoch 3974, Loss: 0.7941534668207169, Final Batch Loss: 0.1755349040031433\n",
      "Epoch 3975, Loss: 0.841292068362236, Final Batch Loss: 0.21338355541229248\n",
      "Epoch 3976, Loss: 0.8982079774141312, Final Batch Loss: 0.2540432810783386\n",
      "Epoch 3977, Loss: 0.8401154726743698, Final Batch Loss: 0.18601055443286896\n",
      "Epoch 3978, Loss: 0.865590050816536, Final Batch Loss: 0.2351968139410019\n",
      "Epoch 3979, Loss: 0.8467230200767517, Final Batch Loss: 0.23441269993782043\n",
      "Epoch 3980, Loss: 0.9077358841896057, Final Batch Loss: 0.2700147032737732\n",
      "Epoch 3981, Loss: 0.8871264308691025, Final Batch Loss: 0.20714400708675385\n",
      "Epoch 3982, Loss: 0.7904280126094818, Final Batch Loss: 0.13507340848445892\n",
      "Epoch 3983, Loss: 0.8207522183656693, Final Batch Loss: 0.21678851544857025\n",
      "Epoch 3984, Loss: 0.8189686685800552, Final Batch Loss: 0.20409923791885376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3985, Loss: 0.8456044942140579, Final Batch Loss: 0.18224908411502838\n",
      "Epoch 3986, Loss: 0.8600645363330841, Final Batch Loss: 0.17610818147659302\n",
      "Epoch 3987, Loss: 0.8393539190292358, Final Batch Loss: 0.19272169470787048\n",
      "Epoch 3988, Loss: 0.8740893453359604, Final Batch Loss: 0.17600755393505096\n",
      "Epoch 3989, Loss: 0.8629787266254425, Final Batch Loss: 0.16044455766677856\n",
      "Epoch 3990, Loss: 0.8563846200704575, Final Batch Loss: 0.1838858276605606\n",
      "Epoch 3991, Loss: 0.8692651689052582, Final Batch Loss: 0.23008115589618683\n",
      "Epoch 3992, Loss: 0.8344018757343292, Final Batch Loss: 0.10332615673542023\n",
      "Epoch 3993, Loss: 0.8452337086200714, Final Batch Loss: 0.16373148560523987\n",
      "Epoch 3994, Loss: 0.8698229789733887, Final Batch Loss: 0.2005731165409088\n",
      "Epoch 3995, Loss: 0.8835696429014206, Final Batch Loss: 0.27624085545539856\n",
      "Epoch 3996, Loss: 0.9028125554323196, Final Batch Loss: 0.2796337604522705\n",
      "Epoch 3997, Loss: 0.8423527032136917, Final Batch Loss: 0.18482302129268646\n",
      "Epoch 3998, Loss: 0.7677540630102158, Final Batch Loss: 0.18279199302196503\n",
      "Epoch 3999, Loss: 0.9014582484960556, Final Batch Loss: 0.22693407535552979\n",
      "Epoch 4000, Loss: 0.8881122171878815, Final Batch Loss: 0.2654467523097992\n",
      "Epoch 4001, Loss: 0.8823462128639221, Final Batch Loss: 0.19728489220142365\n",
      "Epoch 4002, Loss: 0.8422049134969711, Final Batch Loss: 0.24741210043430328\n",
      "Epoch 4003, Loss: 0.7973909676074982, Final Batch Loss: 0.18836122751235962\n",
      "Epoch 4004, Loss: 0.864428699016571, Final Batch Loss: 0.21423931419849396\n",
      "Epoch 4005, Loss: 0.9162746965885162, Final Batch Loss: 0.2991885840892792\n",
      "Epoch 4006, Loss: 0.9065640419721603, Final Batch Loss: 0.24044273793697357\n",
      "Epoch 4007, Loss: 0.7581100761890411, Final Batch Loss: 0.14110615849494934\n",
      "Epoch 4008, Loss: 0.8280995190143585, Final Batch Loss: 0.14468972384929657\n",
      "Epoch 4009, Loss: 0.7828678786754608, Final Batch Loss: 0.18119150400161743\n",
      "Epoch 4010, Loss: 0.8187718465924263, Final Batch Loss: 0.27290913462638855\n",
      "Epoch 4011, Loss: 0.7798351049423218, Final Batch Loss: 0.2057870775461197\n",
      "Epoch 4012, Loss: 0.8537218868732452, Final Batch Loss: 0.23402951657772064\n",
      "Epoch 4013, Loss: 0.916588082909584, Final Batch Loss: 0.32394957542419434\n",
      "Epoch 4014, Loss: 0.8228517174720764, Final Batch Loss: 0.22236405313014984\n",
      "Epoch 4015, Loss: 0.7963322699069977, Final Batch Loss: 0.19063252210617065\n",
      "Epoch 4016, Loss: 0.8467868715524673, Final Batch Loss: 0.2312137633562088\n",
      "Epoch 4017, Loss: 1.0084009170532227, Final Batch Loss: 0.35972753167152405\n",
      "Epoch 4018, Loss: 0.8252341002225876, Final Batch Loss: 0.17467868328094482\n",
      "Epoch 4019, Loss: 0.9095639586448669, Final Batch Loss: 0.22809213399887085\n",
      "Epoch 4020, Loss: 0.8627662658691406, Final Batch Loss: 0.2106076180934906\n",
      "Epoch 4021, Loss: 0.9028683751821518, Final Batch Loss: 0.2294461578130722\n",
      "Epoch 4022, Loss: 0.8104371875524521, Final Batch Loss: 0.16506589949131012\n",
      "Epoch 4023, Loss: 0.7771850526332855, Final Batch Loss: 0.07944834232330322\n",
      "Epoch 4024, Loss: 0.9820160418748856, Final Batch Loss: 0.3303356468677521\n",
      "Epoch 4025, Loss: 0.8534208387136459, Final Batch Loss: 0.22941924631595612\n",
      "Epoch 4026, Loss: 0.7510919272899628, Final Batch Loss: 0.16360048949718475\n",
      "Epoch 4027, Loss: 0.838635042309761, Final Batch Loss: 0.19590415060520172\n",
      "Epoch 4028, Loss: 0.7790169268846512, Final Batch Loss: 0.18826723098754883\n",
      "Epoch 4029, Loss: 0.8190490901470184, Final Batch Loss: 0.20270408689975739\n",
      "Epoch 4030, Loss: 0.8778008073568344, Final Batch Loss: 0.22501547634601593\n",
      "Epoch 4031, Loss: 0.8399259299039841, Final Batch Loss: 0.2436220943927765\n",
      "Epoch 4032, Loss: 0.8980017453432083, Final Batch Loss: 0.2271970808506012\n",
      "Epoch 4033, Loss: 0.8526619225740433, Final Batch Loss: 0.248118057847023\n",
      "Epoch 4034, Loss: 0.8704655021429062, Final Batch Loss: 0.2438599318265915\n",
      "Epoch 4035, Loss: 0.8989592790603638, Final Batch Loss: 0.26274076104164124\n",
      "Epoch 4036, Loss: 0.7687455415725708, Final Batch Loss: 0.17613540589809418\n",
      "Epoch 4037, Loss: 0.8021329343318939, Final Batch Loss: 0.2240356206893921\n",
      "Epoch 4038, Loss: 0.8087226152420044, Final Batch Loss: 0.19954319298267365\n",
      "Epoch 4039, Loss: 0.8743244707584381, Final Batch Loss: 0.2337374985218048\n",
      "Epoch 4040, Loss: 0.783123105764389, Final Batch Loss: 0.20930613577365875\n",
      "Epoch 4041, Loss: 0.846947431564331, Final Batch Loss: 0.1849055290222168\n",
      "Epoch 4042, Loss: 0.7729387879371643, Final Batch Loss: 0.1664951592683792\n",
      "Epoch 4043, Loss: 0.9085368812084198, Final Batch Loss: 0.24731126427650452\n",
      "Epoch 4044, Loss: 0.7860044091939926, Final Batch Loss: 0.12663471698760986\n",
      "Epoch 4045, Loss: 0.8630234748125076, Final Batch Loss: 0.19305017590522766\n",
      "Epoch 4046, Loss: 0.8313008248806, Final Batch Loss: 0.2540724277496338\n",
      "Epoch 4047, Loss: 0.828602522611618, Final Batch Loss: 0.15601208806037903\n",
      "Epoch 4048, Loss: 0.856116846203804, Final Batch Loss: 0.24821975827217102\n",
      "Epoch 4049, Loss: 0.9659046530723572, Final Batch Loss: 0.32785022258758545\n",
      "Epoch 4050, Loss: 0.992790013551712, Final Batch Loss: 0.26103898882865906\n",
      "Epoch 4051, Loss: 1.0311627835035324, Final Batch Loss: 0.3202008605003357\n",
      "Epoch 4052, Loss: 0.9858666509389877, Final Batch Loss: 0.3001956343650818\n",
      "Epoch 4053, Loss: 0.9123619943857193, Final Batch Loss: 0.21872928738594055\n",
      "Epoch 4054, Loss: 0.8051147609949112, Final Batch Loss: 0.18278338015079498\n",
      "Epoch 4055, Loss: 0.8520172983407974, Final Batch Loss: 0.2715335190296173\n",
      "Epoch 4056, Loss: 0.8931102752685547, Final Batch Loss: 0.19280453026294708\n",
      "Epoch 4057, Loss: 0.7795769721269608, Final Batch Loss: 0.17476333677768707\n",
      "Epoch 4058, Loss: 0.9207828789949417, Final Batch Loss: 0.2975538671016693\n",
      "Epoch 4059, Loss: 0.8314100056886673, Final Batch Loss: 0.14134112000465393\n",
      "Epoch 4060, Loss: 0.8474467098712921, Final Batch Loss: 0.2327725738286972\n",
      "Epoch 4061, Loss: 0.8213450908660889, Final Batch Loss: 0.22135800123214722\n",
      "Epoch 4062, Loss: 0.7548390701413155, Final Batch Loss: 0.12288502603769302\n",
      "Epoch 4063, Loss: 0.7812344431877136, Final Batch Loss: 0.21860286593437195\n",
      "Epoch 4064, Loss: 0.8487508445978165, Final Batch Loss: 0.21086615324020386\n",
      "Epoch 4065, Loss: 0.9005225300788879, Final Batch Loss: 0.3148888051509857\n",
      "Epoch 4066, Loss: 0.8531397134065628, Final Batch Loss: 0.2527477443218231\n",
      "Epoch 4067, Loss: 0.9006737470626831, Final Batch Loss: 0.190256729722023\n",
      "Epoch 4068, Loss: 0.8544625043869019, Final Batch Loss: 0.2231673151254654\n",
      "Epoch 4069, Loss: 0.7933237552642822, Final Batch Loss: 0.1528393179178238\n",
      "Epoch 4070, Loss: 0.8124438226222992, Final Batch Loss: 0.17322644591331482\n",
      "Epoch 4071, Loss: 0.7993019670248032, Final Batch Loss: 0.2060970664024353\n",
      "Epoch 4072, Loss: 0.8181609511375427, Final Batch Loss: 0.22315825521945953\n",
      "Epoch 4073, Loss: 0.8504306823015213, Final Batch Loss: 0.2744607925415039\n",
      "Epoch 4074, Loss: 0.8500823378562927, Final Batch Loss: 0.16930624842643738\n",
      "Epoch 4075, Loss: 0.7927773743867874, Final Batch Loss: 0.19315089285373688\n",
      "Epoch 4076, Loss: 0.7921739220619202, Final Batch Loss: 0.19897069036960602\n",
      "Epoch 4077, Loss: 0.8991814106702805, Final Batch Loss: 0.24860700964927673\n",
      "Epoch 4078, Loss: 0.8176820576190948, Final Batch Loss: 0.2335008680820465\n",
      "Epoch 4079, Loss: 0.8556808978319168, Final Batch Loss: 0.17467406392097473\n",
      "Epoch 4080, Loss: 0.8580344468355179, Final Batch Loss: 0.26158642768859863\n",
      "Epoch 4081, Loss: 0.7783266305923462, Final Batch Loss: 0.18055911362171173\n",
      "Epoch 4082, Loss: 0.8122623711824417, Final Batch Loss: 0.1691664159297943\n",
      "Epoch 4083, Loss: 0.9076749533414841, Final Batch Loss: 0.24344316124916077\n",
      "Epoch 4084, Loss: 0.8450373560190201, Final Batch Loss: 0.18150846660137177\n",
      "Epoch 4085, Loss: 0.8773176521062851, Final Batch Loss: 0.1931985467672348\n",
      "Epoch 4086, Loss: 0.7390364855527878, Final Batch Loss: 0.12171414494514465\n",
      "Epoch 4087, Loss: 0.9056995958089828, Final Batch Loss: 0.2843855917453766\n",
      "Epoch 4088, Loss: 0.8101455867290497, Final Batch Loss: 0.2144981175661087\n",
      "Epoch 4089, Loss: 0.7951390147209167, Final Batch Loss: 0.14591138064861298\n",
      "Epoch 4090, Loss: 0.7434041798114777, Final Batch Loss: 0.14205610752105713\n",
      "Epoch 4091, Loss: 0.8135793805122375, Final Batch Loss: 0.2564592957496643\n",
      "Epoch 4092, Loss: 0.7651135474443436, Final Batch Loss: 0.1770227700471878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4093, Loss: 0.8556806594133377, Final Batch Loss: 0.2492559403181076\n",
      "Epoch 4094, Loss: 0.8223836272954941, Final Batch Loss: 0.1791853904724121\n",
      "Epoch 4095, Loss: 0.7884866744279861, Final Batch Loss: 0.17949114739894867\n",
      "Epoch 4096, Loss: 0.8137486428022385, Final Batch Loss: 0.2291196882724762\n",
      "Epoch 4097, Loss: 0.8069747686386108, Final Batch Loss: 0.16018669307231903\n",
      "Epoch 4098, Loss: 0.7825993597507477, Final Batch Loss: 0.17770324647426605\n",
      "Epoch 4099, Loss: 0.9096214473247528, Final Batch Loss: 0.2308419793844223\n",
      "Epoch 4100, Loss: 0.8328676074743271, Final Batch Loss: 0.17996525764465332\n",
      "Epoch 4101, Loss: 0.8760819137096405, Final Batch Loss: 0.22227096557617188\n",
      "Epoch 4102, Loss: 0.7822675853967667, Final Batch Loss: 0.16348092257976532\n",
      "Epoch 4103, Loss: 0.8105212599039078, Final Batch Loss: 0.20986521244049072\n",
      "Epoch 4104, Loss: 0.8063998073339462, Final Batch Loss: 0.21469421684741974\n",
      "Epoch 4105, Loss: 0.8367043882608414, Final Batch Loss: 0.1676805168390274\n",
      "Epoch 4106, Loss: 0.832024872303009, Final Batch Loss: 0.24753786623477936\n",
      "Epoch 4107, Loss: 0.8612263053655624, Final Batch Loss: 0.23043622076511383\n",
      "Epoch 4108, Loss: 0.8624778240919113, Final Batch Loss: 0.23389717936515808\n",
      "Epoch 4109, Loss: 0.839251697063446, Final Batch Loss: 0.16133029758930206\n",
      "Epoch 4110, Loss: 0.8540204465389252, Final Batch Loss: 0.2684331238269806\n",
      "Epoch 4111, Loss: 0.7810738235712051, Final Batch Loss: 0.18160922825336456\n",
      "Epoch 4112, Loss: 1.017914518713951, Final Batch Loss: 0.3569731116294861\n",
      "Epoch 4113, Loss: 0.8743240386247635, Final Batch Loss: 0.217921644449234\n",
      "Epoch 4114, Loss: 0.8461394160985947, Final Batch Loss: 0.21426045894622803\n",
      "Epoch 4115, Loss: 0.8053777515888214, Final Batch Loss: 0.16974377632141113\n",
      "Epoch 4116, Loss: 0.8268112391233444, Final Batch Loss: 0.1743091642856598\n",
      "Epoch 4117, Loss: 0.9774197638034821, Final Batch Loss: 0.31823840737342834\n",
      "Epoch 4118, Loss: 0.8711222112178802, Final Batch Loss: 0.2596995234489441\n",
      "Epoch 4119, Loss: 0.8921183347702026, Final Batch Loss: 0.27773675322532654\n",
      "Epoch 4120, Loss: 0.9098055958747864, Final Batch Loss: 0.2795920670032501\n",
      "Epoch 4121, Loss: 0.8662416338920593, Final Batch Loss: 0.21887676417827606\n",
      "Epoch 4122, Loss: 0.8455414772033691, Final Batch Loss: 0.1882127970457077\n",
      "Epoch 4123, Loss: 0.858110785484314, Final Batch Loss: 0.2170262336730957\n",
      "Epoch 4124, Loss: 0.8130200803279877, Final Batch Loss: 0.16423512995243073\n",
      "Epoch 4125, Loss: 0.8985070884227753, Final Batch Loss: 0.25405198335647583\n",
      "Epoch 4126, Loss: 0.8764625787734985, Final Batch Loss: 0.16337354481220245\n",
      "Epoch 4127, Loss: 0.9215946644544601, Final Batch Loss: 0.3189755976200104\n",
      "Epoch 4128, Loss: 0.8875778764486313, Final Batch Loss: 0.23890551924705505\n",
      "Epoch 4129, Loss: 0.934764176607132, Final Batch Loss: 0.21568956971168518\n",
      "Epoch 4130, Loss: 0.8238919824361801, Final Batch Loss: 0.12852038443088531\n",
      "Epoch 4131, Loss: 0.7641075700521469, Final Batch Loss: 0.14557239413261414\n",
      "Epoch 4132, Loss: 0.7721498012542725, Final Batch Loss: 0.17929375171661377\n",
      "Epoch 4133, Loss: 0.9040437936782837, Final Batch Loss: 0.2506011128425598\n",
      "Epoch 4134, Loss: 0.8445098698139191, Final Batch Loss: 0.21086743474006653\n",
      "Epoch 4135, Loss: 0.7947750985622406, Final Batch Loss: 0.1775379329919815\n",
      "Epoch 4136, Loss: 0.8078556507825851, Final Batch Loss: 0.1877610981464386\n",
      "Epoch 4137, Loss: 0.700767457485199, Final Batch Loss: 0.12523546814918518\n",
      "Epoch 4138, Loss: 0.9334314316511154, Final Batch Loss: 0.2929728925228119\n",
      "Epoch 4139, Loss: 0.8155072182416916, Final Batch Loss: 0.17418856918811798\n",
      "Epoch 4140, Loss: 0.7631885856389999, Final Batch Loss: 0.1534620225429535\n",
      "Epoch 4141, Loss: 0.8157912790775299, Final Batch Loss: 0.1976749747991562\n",
      "Epoch 4142, Loss: 0.8455094546079636, Final Batch Loss: 0.24695220589637756\n",
      "Epoch 4143, Loss: 0.8227168470621109, Final Batch Loss: 0.2025199681520462\n",
      "Epoch 4144, Loss: 0.7845132201910019, Final Batch Loss: 0.21337653696537018\n",
      "Epoch 4145, Loss: 0.8568357229232788, Final Batch Loss: 0.16510739922523499\n",
      "Epoch 4146, Loss: 0.9070513844490051, Final Batch Loss: 0.27700313925743103\n",
      "Epoch 4147, Loss: 0.8546544015407562, Final Batch Loss: 0.2400534749031067\n",
      "Epoch 4148, Loss: 0.9423727840185165, Final Batch Loss: 0.3584776520729065\n",
      "Epoch 4149, Loss: 1.044225499033928, Final Batch Loss: 0.39787012338638306\n",
      "Epoch 4150, Loss: 0.8356630504131317, Final Batch Loss: 0.24095185101032257\n",
      "Epoch 4151, Loss: 0.924278512597084, Final Batch Loss: 0.2704818844795227\n",
      "Epoch 4152, Loss: 0.9327983111143112, Final Batch Loss: 0.2943767011165619\n",
      "Epoch 4153, Loss: 0.8639596551656723, Final Batch Loss: 0.20197823643684387\n",
      "Epoch 4154, Loss: 0.8628029227256775, Final Batch Loss: 0.22669048607349396\n",
      "Epoch 4155, Loss: 0.8151271790266037, Final Batch Loss: 0.2047366499900818\n",
      "Epoch 4156, Loss: 0.8029537498950958, Final Batch Loss: 0.2572784125804901\n",
      "Epoch 4157, Loss: 0.8455684334039688, Final Batch Loss: 0.2524954676628113\n",
      "Epoch 4158, Loss: 0.9064120352268219, Final Batch Loss: 0.2580878436565399\n",
      "Epoch 4159, Loss: 0.8818646520376205, Final Batch Loss: 0.20991313457489014\n",
      "Epoch 4160, Loss: 0.8972296565771103, Final Batch Loss: 0.27520740032196045\n",
      "Epoch 4161, Loss: 0.8245821297168732, Final Batch Loss: 0.19318783283233643\n",
      "Epoch 4162, Loss: 0.9806976020336151, Final Batch Loss: 0.3206261098384857\n",
      "Epoch 4163, Loss: 0.784815102815628, Final Batch Loss: 0.2067967653274536\n",
      "Epoch 4164, Loss: 0.8733530342578888, Final Batch Loss: 0.2585785984992981\n",
      "Epoch 4165, Loss: 0.817151814699173, Final Batch Loss: 0.2110675424337387\n",
      "Epoch 4166, Loss: 0.8265457153320312, Final Batch Loss: 0.22100825607776642\n",
      "Epoch 4167, Loss: 0.790951669216156, Final Batch Loss: 0.2343568503856659\n",
      "Epoch 4168, Loss: 0.8256934285163879, Final Batch Loss: 0.22140821814537048\n",
      "Epoch 4169, Loss: 0.8663268834352493, Final Batch Loss: 0.25262922048568726\n",
      "Epoch 4170, Loss: 0.8279121369123459, Final Batch Loss: 0.1952918916940689\n",
      "Epoch 4171, Loss: 0.8202698975801468, Final Batch Loss: 0.1482052057981491\n",
      "Epoch 4172, Loss: 1.0317929983139038, Final Batch Loss: 0.2830543518066406\n",
      "Epoch 4173, Loss: 0.9397204667329788, Final Batch Loss: 0.31256037950515747\n",
      "Epoch 4174, Loss: 0.8945285826921463, Final Batch Loss: 0.21683074533939362\n",
      "Epoch 4175, Loss: 0.8149438798427582, Final Batch Loss: 0.2384774535894394\n",
      "Epoch 4176, Loss: 0.8460092544555664, Final Batch Loss: 0.23086105287075043\n",
      "Epoch 4177, Loss: 0.8983878344297409, Final Batch Loss: 0.15419124066829681\n",
      "Epoch 4178, Loss: 0.8395097404718399, Final Batch Loss: 0.2227889895439148\n",
      "Epoch 4179, Loss: 0.8850093185901642, Final Batch Loss: 0.2516694664955139\n",
      "Epoch 4180, Loss: 0.8349055051803589, Final Batch Loss: 0.20253047347068787\n",
      "Epoch 4181, Loss: 0.8452138900756836, Final Batch Loss: 0.2411489635705948\n",
      "Epoch 4182, Loss: 0.798461452126503, Final Batch Loss: 0.1808219999074936\n",
      "Epoch 4183, Loss: 0.9441348165273666, Final Batch Loss: 0.3007711172103882\n",
      "Epoch 4184, Loss: 0.9607584327459335, Final Batch Loss: 0.30727455019950867\n",
      "Epoch 4185, Loss: 0.8701049387454987, Final Batch Loss: 0.23141999542713165\n",
      "Epoch 4186, Loss: 0.8750254809856415, Final Batch Loss: 0.25990644097328186\n",
      "Epoch 4187, Loss: 0.9149911552667618, Final Batch Loss: 0.2238398790359497\n",
      "Epoch 4188, Loss: 0.8023773729801178, Final Batch Loss: 0.1917458176612854\n",
      "Epoch 4189, Loss: 0.9115492850542068, Final Batch Loss: 0.27107948064804077\n",
      "Epoch 4190, Loss: 0.7965809553861618, Final Batch Loss: 0.18723906576633453\n",
      "Epoch 4191, Loss: 0.8876619786024094, Final Batch Loss: 0.26466992497444153\n",
      "Epoch 4192, Loss: 0.9915167391300201, Final Batch Loss: 0.24989132583141327\n",
      "Epoch 4193, Loss: 0.8068825155496597, Final Batch Loss: 0.19600878655910492\n",
      "Epoch 4194, Loss: 0.8693298995494843, Final Batch Loss: 0.2745620906352997\n",
      "Epoch 4195, Loss: 0.9183436930179596, Final Batch Loss: 0.22398385405540466\n",
      "Epoch 4196, Loss: 0.8086269199848175, Final Batch Loss: 0.21057231724262238\n",
      "Epoch 4197, Loss: 0.9006954282522202, Final Batch Loss: 0.23141390085220337\n",
      "Epoch 4198, Loss: 0.9649139791727066, Final Batch Loss: 0.22077235579490662\n",
      "Epoch 4199, Loss: 0.8560703247785568, Final Batch Loss: 0.2126735895872116\n",
      "Epoch 4200, Loss: 0.7601947337388992, Final Batch Loss: 0.19960372149944305\n",
      "Epoch 4201, Loss: 0.8122269511222839, Final Batch Loss: 0.24049320816993713\n",
      "Epoch 4202, Loss: 0.9000028669834137, Final Batch Loss: 0.29720985889434814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4203, Loss: 0.8165910989046097, Final Batch Loss: 0.17723757028579712\n",
      "Epoch 4204, Loss: 0.8852129131555557, Final Batch Loss: 0.28758707642555237\n",
      "Epoch 4205, Loss: 0.8339722901582718, Final Batch Loss: 0.17576220631599426\n",
      "Epoch 4206, Loss: 0.9227366298437119, Final Batch Loss: 0.27752116322517395\n",
      "Epoch 4207, Loss: 0.9199899286031723, Final Batch Loss: 0.3083339035511017\n",
      "Epoch 4208, Loss: 0.9040032178163528, Final Batch Loss: 0.18304096162319183\n",
      "Epoch 4209, Loss: 0.8783636689186096, Final Batch Loss: 0.21758373081684113\n",
      "Epoch 4210, Loss: 0.8314053267240524, Final Batch Loss: 0.17097441852092743\n",
      "Epoch 4211, Loss: 0.90499547123909, Final Batch Loss: 0.27504366636276245\n",
      "Epoch 4212, Loss: 0.8562415838241577, Final Batch Loss: 0.20807014405727386\n",
      "Epoch 4213, Loss: 0.8973706364631653, Final Batch Loss: 0.2621243894100189\n",
      "Epoch 4214, Loss: 0.8458875417709351, Final Batch Loss: 0.18644973635673523\n",
      "Epoch 4215, Loss: 0.8436461240053177, Final Batch Loss: 0.193730890750885\n",
      "Epoch 4216, Loss: 0.8470176011323929, Final Batch Loss: 0.21434329450130463\n",
      "Epoch 4217, Loss: 0.7734426856040955, Final Batch Loss: 0.14162012934684753\n",
      "Epoch 4218, Loss: 0.797383189201355, Final Batch Loss: 0.20479606091976166\n",
      "Epoch 4219, Loss: 0.8537070751190186, Final Batch Loss: 0.2606569826602936\n",
      "Epoch 4220, Loss: 0.793343797326088, Final Batch Loss: 0.21120800077915192\n",
      "Epoch 4221, Loss: 0.7849390357732773, Final Batch Loss: 0.20862656831741333\n",
      "Epoch 4222, Loss: 0.804096981883049, Final Batch Loss: 0.17145593464374542\n",
      "Epoch 4223, Loss: 0.8524966090917587, Final Batch Loss: 0.22391365468502045\n",
      "Epoch 4224, Loss: 0.8985250443220139, Final Batch Loss: 0.24519063532352448\n",
      "Epoch 4225, Loss: 0.8116452544927597, Final Batch Loss: 0.2142246812582016\n",
      "Epoch 4226, Loss: 0.8445107191801071, Final Batch Loss: 0.19711560010910034\n",
      "Epoch 4227, Loss: 0.764394499361515, Final Batch Loss: 0.10532737523317337\n",
      "Epoch 4228, Loss: 0.8723812252283096, Final Batch Loss: 0.1491585075855255\n",
      "Epoch 4229, Loss: 0.7956646233797073, Final Batch Loss: 0.15052060782909393\n",
      "Epoch 4230, Loss: 0.7444429397583008, Final Batch Loss: 0.15422607958316803\n",
      "Epoch 4231, Loss: 0.7912259697914124, Final Batch Loss: 0.15112647414207458\n",
      "Epoch 4232, Loss: 0.896599069237709, Final Batch Loss: 0.27219146490097046\n",
      "Epoch 4233, Loss: 0.764623373746872, Final Batch Loss: 0.16530166566371918\n",
      "Epoch 4234, Loss: 0.8121733069419861, Final Batch Loss: 0.22018013894557953\n",
      "Epoch 4235, Loss: 0.8000652343034744, Final Batch Loss: 0.1624281257390976\n",
      "Epoch 4236, Loss: 0.8587151765823364, Final Batch Loss: 0.1977643370628357\n",
      "Epoch 4237, Loss: 0.7996370047330856, Final Batch Loss: 0.15952101349830627\n",
      "Epoch 4238, Loss: 0.7142177820205688, Final Batch Loss: 0.13612745702266693\n",
      "Epoch 4239, Loss: 0.8051970601081848, Final Batch Loss: 0.2024998664855957\n",
      "Epoch 4240, Loss: 0.8426884412765503, Final Batch Loss: 0.16122178733348846\n",
      "Epoch 4241, Loss: 0.8206116408109665, Final Batch Loss: 0.23093068599700928\n",
      "Epoch 4242, Loss: 0.7464417442679405, Final Batch Loss: 0.12056759744882584\n",
      "Epoch 4243, Loss: 0.8915357738733292, Final Batch Loss: 0.2929289937019348\n",
      "Epoch 4244, Loss: 0.8198308348655701, Final Batch Loss: 0.15002401173114777\n",
      "Epoch 4245, Loss: 0.8525975346565247, Final Batch Loss: 0.22072982788085938\n",
      "Epoch 4246, Loss: 0.7605357021093369, Final Batch Loss: 0.14734545350074768\n",
      "Epoch 4247, Loss: 0.8413997739553452, Final Batch Loss: 0.25329452753067017\n",
      "Epoch 4248, Loss: 0.8077664375305176, Final Batch Loss: 0.1698986142873764\n",
      "Epoch 4249, Loss: 0.8197563737630844, Final Batch Loss: 0.1695062518119812\n",
      "Epoch 4250, Loss: 0.7865347564220428, Final Batch Loss: 0.21087713539600372\n",
      "Epoch 4251, Loss: 0.8301498740911484, Final Batch Loss: 0.22176428139209747\n",
      "Epoch 4252, Loss: 0.8218857795000076, Final Batch Loss: 0.2640937864780426\n",
      "Epoch 4253, Loss: 0.840133786201477, Final Batch Loss: 0.20660065114498138\n",
      "Epoch 4254, Loss: 0.7880002707242966, Final Batch Loss: 0.2608013153076172\n",
      "Epoch 4255, Loss: 0.8594973236322403, Final Batch Loss: 0.23181287944316864\n",
      "Epoch 4256, Loss: 0.8266897350549698, Final Batch Loss: 0.20655235648155212\n",
      "Epoch 4257, Loss: 0.8249537944793701, Final Batch Loss: 0.2413020133972168\n",
      "Epoch 4258, Loss: 0.7864562124013901, Final Batch Loss: 0.16593186557292938\n",
      "Epoch 4259, Loss: 0.7553355544805527, Final Batch Loss: 0.13911516964435577\n",
      "Epoch 4260, Loss: 0.8945166766643524, Final Batch Loss: 0.26390397548675537\n",
      "Epoch 4261, Loss: 0.7769865989685059, Final Batch Loss: 0.196774423122406\n",
      "Epoch 4262, Loss: 0.8001515418291092, Final Batch Loss: 0.2003607302904129\n",
      "Epoch 4263, Loss: 0.8591440916061401, Final Batch Loss: 0.22866681218147278\n",
      "Epoch 4264, Loss: 0.8751804083585739, Final Batch Loss: 0.2117060422897339\n",
      "Epoch 4265, Loss: 0.8887417018413544, Final Batch Loss: 0.23363058269023895\n",
      "Epoch 4266, Loss: 0.8934657275676727, Final Batch Loss: 0.2304857075214386\n",
      "Epoch 4267, Loss: 0.964252769947052, Final Batch Loss: 0.29124826192855835\n",
      "Epoch 4268, Loss: 0.8887398838996887, Final Batch Loss: 0.20745211839675903\n",
      "Epoch 4269, Loss: 0.9389369636774063, Final Batch Loss: 0.16312213242053986\n",
      "Epoch 4270, Loss: 0.9528837651014328, Final Batch Loss: 0.24672719836235046\n",
      "Epoch 4271, Loss: 0.8674900233745575, Final Batch Loss: 0.25095483660697937\n",
      "Epoch 4272, Loss: 0.8883317410945892, Final Batch Loss: 0.25376057624816895\n",
      "Epoch 4273, Loss: 0.79929119348526, Final Batch Loss: 0.16383256018161774\n",
      "Epoch 4274, Loss: 0.8127981126308441, Final Batch Loss: 0.1831279844045639\n",
      "Epoch 4275, Loss: 0.7861746475100517, Final Batch Loss: 0.1241191104054451\n",
      "Epoch 4276, Loss: 0.8032805472612381, Final Batch Loss: 0.1919742375612259\n",
      "Epoch 4277, Loss: 0.9577145427465439, Final Batch Loss: 0.2589048147201538\n",
      "Epoch 4278, Loss: 0.8614481687545776, Final Batch Loss: 0.19728203117847443\n",
      "Epoch 4279, Loss: 0.8692445307970047, Final Batch Loss: 0.2262880504131317\n",
      "Epoch 4280, Loss: 0.7549474090337753, Final Batch Loss: 0.17157062888145447\n",
      "Epoch 4281, Loss: 0.8919766396284103, Final Batch Loss: 0.26353809237480164\n",
      "Epoch 4282, Loss: 0.7861950397491455, Final Batch Loss: 0.17824576795101166\n",
      "Epoch 4283, Loss: 0.8379460722208023, Final Batch Loss: 0.17551933228969574\n",
      "Epoch 4284, Loss: 0.8946468681097031, Final Batch Loss: 0.2210116684436798\n",
      "Epoch 4285, Loss: 0.8408915400505066, Final Batch Loss: 0.15981757640838623\n",
      "Epoch 4286, Loss: 0.7879071235656738, Final Batch Loss: 0.19771923124790192\n",
      "Epoch 4287, Loss: 1.0468772947788239, Final Batch Loss: 0.3469727337360382\n",
      "Epoch 4288, Loss: 0.7811954021453857, Final Batch Loss: 0.1455112248659134\n",
      "Epoch 4289, Loss: 0.7556507885456085, Final Batch Loss: 0.1724376082420349\n",
      "Epoch 4290, Loss: 0.8047298192977905, Final Batch Loss: 0.22989797592163086\n",
      "Epoch 4291, Loss: 0.8817164748907089, Final Batch Loss: 0.2555500566959381\n",
      "Epoch 4292, Loss: 0.7579691261053085, Final Batch Loss: 0.14088232815265656\n",
      "Epoch 4293, Loss: 0.780684232711792, Final Batch Loss: 0.16952595114707947\n",
      "Epoch 4294, Loss: 0.7718625962734222, Final Batch Loss: 0.20970122516155243\n",
      "Epoch 4295, Loss: 0.8667036890983582, Final Batch Loss: 0.22906404733657837\n",
      "Epoch 4296, Loss: 0.8150821328163147, Final Batch Loss: 0.2001984715461731\n",
      "Epoch 4297, Loss: 0.8954607248306274, Final Batch Loss: 0.22189950942993164\n",
      "Epoch 4298, Loss: 0.7752439826726913, Final Batch Loss: 0.1404658854007721\n",
      "Epoch 4299, Loss: 0.8322657942771912, Final Batch Loss: 0.22420136630535126\n",
      "Epoch 4300, Loss: 0.7532791793346405, Final Batch Loss: 0.2146347612142563\n",
      "Epoch 4301, Loss: 0.7867074012756348, Final Batch Loss: 0.2234206348657608\n",
      "Epoch 4302, Loss: 0.7707312703132629, Final Batch Loss: 0.1477055847644806\n",
      "Epoch 4303, Loss: 0.8425050526857376, Final Batch Loss: 0.23233430087566376\n",
      "Epoch 4304, Loss: 0.725990928709507, Final Batch Loss: 0.11105988174676895\n",
      "Epoch 4305, Loss: 0.7179302424192429, Final Batch Loss: 0.14513979852199554\n",
      "Epoch 4306, Loss: 0.8089695274829865, Final Batch Loss: 0.1871172934770584\n",
      "Epoch 4307, Loss: 0.7713534832000732, Final Batch Loss: 0.15718358755111694\n",
      "Epoch 4308, Loss: 0.8109068721532822, Final Batch Loss: 0.19486083090305328\n",
      "Epoch 4309, Loss: 0.7910314500331879, Final Batch Loss: 0.1724890172481537\n",
      "Epoch 4310, Loss: 0.7483810782432556, Final Batch Loss: 0.09502853453159332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4311, Loss: 0.8831955641508102, Final Batch Loss: 0.16606728732585907\n",
      "Epoch 4312, Loss: 0.8726547360420227, Final Batch Loss: 0.2610173523426056\n",
      "Epoch 4313, Loss: 0.8272203505039215, Final Batch Loss: 0.1990404725074768\n",
      "Epoch 4314, Loss: 0.8570577651262283, Final Batch Loss: 0.28173065185546875\n",
      "Epoch 4315, Loss: 0.8467078655958176, Final Batch Loss: 0.19165244698524475\n",
      "Epoch 4316, Loss: 0.8089990466833115, Final Batch Loss: 0.21275658905506134\n",
      "Epoch 4317, Loss: 0.8596163839101791, Final Batch Loss: 0.25015750527381897\n",
      "Epoch 4318, Loss: 0.8012103587388992, Final Batch Loss: 0.1441185027360916\n",
      "Epoch 4319, Loss: 0.8481106609106064, Final Batch Loss: 0.17189083993434906\n",
      "Epoch 4320, Loss: 0.8667427003383636, Final Batch Loss: 0.26568603515625\n",
      "Epoch 4321, Loss: 0.909025028347969, Final Batch Loss: 0.241760715842247\n",
      "Epoch 4322, Loss: 0.761784166097641, Final Batch Loss: 0.17445160448551178\n",
      "Epoch 4323, Loss: 0.8431645631790161, Final Batch Loss: 0.19051691889762878\n",
      "Epoch 4324, Loss: 0.7768748849630356, Final Batch Loss: 0.21141071617603302\n",
      "Epoch 4325, Loss: 0.8425560593605042, Final Batch Loss: 0.15105527639389038\n",
      "Epoch 4326, Loss: 0.8419737666845322, Final Batch Loss: 0.15004679560661316\n",
      "Epoch 4327, Loss: 0.9319725632667542, Final Batch Loss: 0.34692054986953735\n",
      "Epoch 4328, Loss: 0.872690886259079, Final Batch Loss: 0.18159256875514984\n",
      "Epoch 4329, Loss: 0.7317666932940483, Final Batch Loss: 0.11165425926446915\n",
      "Epoch 4330, Loss: 0.7885553985834122, Final Batch Loss: 0.17605136334896088\n",
      "Epoch 4331, Loss: 0.8606478124856949, Final Batch Loss: 0.27087822556495667\n",
      "Epoch 4332, Loss: 0.7985191345214844, Final Batch Loss: 0.2516939342021942\n",
      "Epoch 4333, Loss: 0.8334051370620728, Final Batch Loss: 0.1675386130809784\n",
      "Epoch 4334, Loss: 0.8013391643762589, Final Batch Loss: 0.1838952600955963\n",
      "Epoch 4335, Loss: 0.8369878530502319, Final Batch Loss: 0.20686334371566772\n",
      "Epoch 4336, Loss: 0.8174922168254852, Final Batch Loss: 0.20064838230609894\n",
      "Epoch 4337, Loss: 0.8139506280422211, Final Batch Loss: 0.22118216753005981\n",
      "Epoch 4338, Loss: 0.9221691638231277, Final Batch Loss: 0.20826175808906555\n",
      "Epoch 4339, Loss: 0.7560062855482101, Final Batch Loss: 0.10227504372596741\n",
      "Epoch 4340, Loss: 0.8275154680013657, Final Batch Loss: 0.18947875499725342\n",
      "Epoch 4341, Loss: 0.8311688303947449, Final Batch Loss: 0.23170244693756104\n",
      "Epoch 4342, Loss: 0.8340607434511185, Final Batch Loss: 0.2593056559562683\n",
      "Epoch 4343, Loss: 0.808053508400917, Final Batch Loss: 0.20293055474758148\n",
      "Epoch 4344, Loss: 0.8426634818315506, Final Batch Loss: 0.22069518268108368\n",
      "Epoch 4345, Loss: 0.7554030865430832, Final Batch Loss: 0.17627467215061188\n",
      "Epoch 4346, Loss: 0.7812445163726807, Final Batch Loss: 0.2071617692708969\n",
      "Epoch 4347, Loss: 0.8269918113946915, Final Batch Loss: 0.2806713283061981\n",
      "Epoch 4348, Loss: 0.8124754130840302, Final Batch Loss: 0.21537691354751587\n",
      "Epoch 4349, Loss: 0.7826334685087204, Final Batch Loss: 0.15561169385910034\n",
      "Epoch 4350, Loss: 0.7763899564743042, Final Batch Loss: 0.13804778456687927\n",
      "Epoch 4351, Loss: 0.8518652319908142, Final Batch Loss: 0.23637662827968597\n",
      "Epoch 4352, Loss: 0.7980340719223022, Final Batch Loss: 0.24939414858818054\n",
      "Epoch 4353, Loss: 0.797006294131279, Final Batch Loss: 0.23074933886528015\n",
      "Epoch 4354, Loss: 0.9162066280841827, Final Batch Loss: 0.2555084824562073\n",
      "Epoch 4355, Loss: 0.7946676015853882, Final Batch Loss: 0.14062833786010742\n",
      "Epoch 4356, Loss: 0.8043236434459686, Final Batch Loss: 0.23731963336467743\n",
      "Epoch 4357, Loss: 0.8320055603981018, Final Batch Loss: 0.12615592777729034\n",
      "Epoch 4358, Loss: 0.8572688102722168, Final Batch Loss: 0.2307635247707367\n",
      "Epoch 4359, Loss: 0.785920575261116, Final Batch Loss: 0.14828646183013916\n",
      "Epoch 4360, Loss: 0.7844749689102173, Final Batch Loss: 0.21108661592006683\n",
      "Epoch 4361, Loss: 0.7887170314788818, Final Batch Loss: 0.18438920378684998\n",
      "Epoch 4362, Loss: 0.7607864439487457, Final Batch Loss: 0.14952458441257477\n",
      "Epoch 4363, Loss: 0.7940874397754669, Final Batch Loss: 0.1632205694913864\n",
      "Epoch 4364, Loss: 0.8976800590753555, Final Batch Loss: 0.22990338504314423\n",
      "Epoch 4365, Loss: 0.8280903100967407, Final Batch Loss: 0.22660553455352783\n",
      "Epoch 4366, Loss: 0.8266117721796036, Final Batch Loss: 0.2282942235469818\n",
      "Epoch 4367, Loss: 0.867101177573204, Final Batch Loss: 0.18013711273670197\n",
      "Epoch 4368, Loss: 0.797538235783577, Final Batch Loss: 0.1538698971271515\n",
      "Epoch 4369, Loss: 0.8547991067171097, Final Batch Loss: 0.15962179005146027\n",
      "Epoch 4370, Loss: 0.7409326136112213, Final Batch Loss: 0.1993466317653656\n",
      "Epoch 4371, Loss: 0.8753042370080948, Final Batch Loss: 0.218036949634552\n",
      "Epoch 4372, Loss: 0.8018079847097397, Final Batch Loss: 0.17150026559829712\n",
      "Epoch 4373, Loss: 0.8136033266782761, Final Batch Loss: 0.2214757204055786\n",
      "Epoch 4374, Loss: 0.8156550824642181, Final Batch Loss: 0.22151678800582886\n",
      "Epoch 4375, Loss: 0.7546050697565079, Final Batch Loss: 0.13734300434589386\n",
      "Epoch 4376, Loss: 0.7463412433862686, Final Batch Loss: 0.13945788145065308\n",
      "Epoch 4377, Loss: 0.7741923183202744, Final Batch Loss: 0.19288332760334015\n",
      "Epoch 4378, Loss: 0.9312445819377899, Final Batch Loss: 0.3102717399597168\n",
      "Epoch 4379, Loss: 0.7754710018634796, Final Batch Loss: 0.17197830975055695\n",
      "Epoch 4380, Loss: 0.7043853551149368, Final Batch Loss: 0.14549995958805084\n",
      "Epoch 4381, Loss: 0.8188945800065994, Final Batch Loss: 0.2570423483848572\n",
      "Epoch 4382, Loss: 0.9037007689476013, Final Batch Loss: 0.22880007326602936\n",
      "Epoch 4383, Loss: 0.9232527762651443, Final Batch Loss: 0.28489819169044495\n",
      "Epoch 4384, Loss: 0.9149030148983002, Final Batch Loss: 0.25200557708740234\n",
      "Epoch 4385, Loss: 0.8846871107816696, Final Batch Loss: 0.20429164171218872\n",
      "Epoch 4386, Loss: 0.7911674380302429, Final Batch Loss: 0.18045133352279663\n",
      "Epoch 4387, Loss: 0.8196017295122147, Final Batch Loss: 0.27575844526290894\n",
      "Epoch 4388, Loss: 0.9526025652885437, Final Batch Loss: 0.34473717212677\n",
      "Epoch 4389, Loss: 0.7976153939962387, Final Batch Loss: 0.24465149641036987\n",
      "Epoch 4390, Loss: 0.7530604898929596, Final Batch Loss: 0.16409018635749817\n",
      "Epoch 4391, Loss: 0.8032244741916656, Final Batch Loss: 0.1679827719926834\n",
      "Epoch 4392, Loss: 0.8386455923318863, Final Batch Loss: 0.26663270592689514\n",
      "Epoch 4393, Loss: 0.9216715097427368, Final Batch Loss: 0.25185176730155945\n",
      "Epoch 4394, Loss: 0.7634001970291138, Final Batch Loss: 0.2020767778158188\n",
      "Epoch 4395, Loss: 0.8211079686880112, Final Batch Loss: 0.1983141452074051\n",
      "Epoch 4396, Loss: 0.8802911043167114, Final Batch Loss: 0.1780177503824234\n",
      "Epoch 4397, Loss: 0.8241765201091766, Final Batch Loss: 0.22210387885570526\n",
      "Epoch 4398, Loss: 0.9007579684257507, Final Batch Loss: 0.27252545952796936\n",
      "Epoch 4399, Loss: 0.8866696953773499, Final Batch Loss: 0.270951509475708\n",
      "Epoch 4400, Loss: 0.9795290678739548, Final Batch Loss: 0.2197389304637909\n",
      "Epoch 4401, Loss: 0.9633197337388992, Final Batch Loss: 0.22583423554897308\n",
      "Epoch 4402, Loss: 0.9749197959899902, Final Batch Loss: 0.23344095051288605\n",
      "Epoch 4403, Loss: 0.9244008958339691, Final Batch Loss: 0.16430579125881195\n",
      "Epoch 4404, Loss: 0.9520676136016846, Final Batch Loss: 0.28082624077796936\n",
      "Epoch 4405, Loss: 0.825824037194252, Final Batch Loss: 0.15978562831878662\n",
      "Epoch 4406, Loss: 0.8597602844238281, Final Batch Loss: 0.16207769513130188\n",
      "Epoch 4407, Loss: 0.8228815943002701, Final Batch Loss: 0.1954379379749298\n",
      "Epoch 4408, Loss: 0.9414072781801224, Final Batch Loss: 0.1937345415353775\n",
      "Epoch 4409, Loss: 0.9837369620800018, Final Batch Loss: 0.24711349606513977\n",
      "Epoch 4410, Loss: 0.9223772138357162, Final Batch Loss: 0.2829742133617401\n",
      "Epoch 4411, Loss: 0.935372531414032, Final Batch Loss: 0.21374079585075378\n",
      "Epoch 4412, Loss: 0.8822313249111176, Final Batch Loss: 0.21627478301525116\n",
      "Epoch 4413, Loss: 0.8131335377693176, Final Batch Loss: 0.21070744097232819\n",
      "Epoch 4414, Loss: 0.806490957736969, Final Batch Loss: 0.1674225926399231\n",
      "Epoch 4415, Loss: 0.7951264381408691, Final Batch Loss: 0.19292868673801422\n",
      "Epoch 4416, Loss: 0.7537513822317123, Final Batch Loss: 0.1607886403799057\n",
      "Epoch 4417, Loss: 0.6902466416358948, Final Batch Loss: 0.11064688861370087\n",
      "Epoch 4418, Loss: 0.8728150278329849, Final Batch Loss: 0.23621059954166412\n",
      "Epoch 4419, Loss: 0.8160357624292374, Final Batch Loss: 0.17909683287143707\n",
      "Epoch 4420, Loss: 0.8717689365148544, Final Batch Loss: 0.3038446307182312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4421, Loss: 0.8855913728475571, Final Batch Loss: 0.22012659907341003\n",
      "Epoch 4422, Loss: 0.8701341599225998, Final Batch Loss: 0.19946278631687164\n",
      "Epoch 4423, Loss: 0.822128564119339, Final Batch Loss: 0.1890539675951004\n",
      "Epoch 4424, Loss: 0.8562418520450592, Final Batch Loss: 0.21734732389450073\n",
      "Epoch 4425, Loss: 0.9184561520814896, Final Batch Loss: 0.31148990988731384\n",
      "Epoch 4426, Loss: 0.7555619031190872, Final Batch Loss: 0.168791264295578\n",
      "Epoch 4427, Loss: 0.7441106140613556, Final Batch Loss: 0.21380724012851715\n",
      "Epoch 4428, Loss: 0.7958396524190903, Final Batch Loss: 0.16423934698104858\n",
      "Epoch 4429, Loss: 0.8005939722061157, Final Batch Loss: 0.17512214183807373\n",
      "Epoch 4430, Loss: 0.7845445573329926, Final Batch Loss: 0.17647819221019745\n",
      "Epoch 4431, Loss: 0.8582379519939423, Final Batch Loss: 0.24595074355602264\n",
      "Epoch 4432, Loss: 0.7976067513227463, Final Batch Loss: 0.17274650931358337\n",
      "Epoch 4433, Loss: 0.8147225081920624, Final Batch Loss: 0.1969911754131317\n",
      "Epoch 4434, Loss: 0.8501561433076859, Final Batch Loss: 0.19410459697246552\n",
      "Epoch 4435, Loss: 0.8246609270572662, Final Batch Loss: 0.2285684198141098\n",
      "Epoch 4436, Loss: 0.7911177575588226, Final Batch Loss: 0.2126501053571701\n",
      "Epoch 4437, Loss: 0.8319816142320633, Final Batch Loss: 0.2730677127838135\n",
      "Epoch 4438, Loss: 0.8234299570322037, Final Batch Loss: 0.18919141590595245\n",
      "Epoch 4439, Loss: 0.7888810932636261, Final Batch Loss: 0.22670520842075348\n",
      "Epoch 4440, Loss: 0.8519711196422577, Final Batch Loss: 0.2729790210723877\n",
      "Epoch 4441, Loss: 0.8309258669614792, Final Batch Loss: 0.13175268471240997\n",
      "Epoch 4442, Loss: 0.963382214307785, Final Batch Loss: 0.354145348072052\n",
      "Epoch 4443, Loss: 0.8424727618694305, Final Batch Loss: 0.2562216520309448\n",
      "Epoch 4444, Loss: 0.7720339745283127, Final Batch Loss: 0.17126452922821045\n",
      "Epoch 4445, Loss: 0.821563795208931, Final Batch Loss: 0.21072083711624146\n",
      "Epoch 4446, Loss: 0.8503041118383408, Final Batch Loss: 0.22354759275913239\n",
      "Epoch 4447, Loss: 0.9044217616319656, Final Batch Loss: 0.21491830050945282\n",
      "Epoch 4448, Loss: 0.875324159860611, Final Batch Loss: 0.21453802287578583\n",
      "Epoch 4449, Loss: 0.8196081668138504, Final Batch Loss: 0.19375315308570862\n",
      "Epoch 4450, Loss: 0.8398175239562988, Final Batch Loss: 0.18362164497375488\n",
      "Epoch 4451, Loss: 0.8444795310497284, Final Batch Loss: 0.2603212594985962\n",
      "Epoch 4452, Loss: 0.8745869100093842, Final Batch Loss: 0.2689131200313568\n",
      "Epoch 4453, Loss: 0.7786281555891037, Final Batch Loss: 0.17792844772338867\n",
      "Epoch 4454, Loss: 0.8366616368293762, Final Batch Loss: 0.19577042758464813\n",
      "Epoch 4455, Loss: 0.785845160484314, Final Batch Loss: 0.18673211336135864\n",
      "Epoch 4456, Loss: 0.8125303536653519, Final Batch Loss: 0.2542261481285095\n",
      "Epoch 4457, Loss: 0.7606049627065659, Final Batch Loss: 0.19197599589824677\n",
      "Epoch 4458, Loss: 0.7316106930375099, Final Batch Loss: 0.2306627482175827\n",
      "Epoch 4459, Loss: 0.7558805197477341, Final Batch Loss: 0.17030049860477448\n",
      "Epoch 4460, Loss: 0.7254498898983002, Final Batch Loss: 0.1643502414226532\n",
      "Epoch 4461, Loss: 0.8367614895105362, Final Batch Loss: 0.25682276487350464\n",
      "Epoch 4462, Loss: 0.8394324332475662, Final Batch Loss: 0.26470980048179626\n",
      "Epoch 4463, Loss: 0.9925997406244278, Final Batch Loss: 0.2925589978694916\n",
      "Epoch 4464, Loss: 0.9036036878824234, Final Batch Loss: 0.21560612320899963\n",
      "Epoch 4465, Loss: 0.8711647987365723, Final Batch Loss: 0.24891187250614166\n",
      "Epoch 4466, Loss: 0.7770551964640617, Final Batch Loss: 0.09642472118139267\n",
      "Epoch 4467, Loss: 0.8295578062534332, Final Batch Loss: 0.22256185114383698\n",
      "Epoch 4468, Loss: 0.8007187247276306, Final Batch Loss: 0.21380363404750824\n",
      "Epoch 4469, Loss: 0.8503534644842148, Final Batch Loss: 0.2721875011920929\n",
      "Epoch 4470, Loss: 0.8325713723897934, Final Batch Loss: 0.2138110101222992\n",
      "Epoch 4471, Loss: 0.8206666558980942, Final Batch Loss: 0.2367619127035141\n",
      "Epoch 4472, Loss: 0.7933277487754822, Final Batch Loss: 0.1626492738723755\n",
      "Epoch 4473, Loss: 0.7456941530108452, Final Batch Loss: 0.12401772290468216\n",
      "Epoch 4474, Loss: 0.8332641869783401, Final Batch Loss: 0.22332468628883362\n",
      "Epoch 4475, Loss: 0.8031798601150513, Final Batch Loss: 0.18896721303462982\n",
      "Epoch 4476, Loss: 0.7539374530315399, Final Batch Loss: 0.20327863097190857\n",
      "Epoch 4477, Loss: 0.8156416714191437, Final Batch Loss: 0.18033817410469055\n",
      "Epoch 4478, Loss: 0.7617508322000504, Final Batch Loss: 0.16360299289226532\n",
      "Epoch 4479, Loss: 0.8172627240419388, Final Batch Loss: 0.23335246741771698\n",
      "Epoch 4480, Loss: 0.7916528731584549, Final Batch Loss: 0.1663709580898285\n",
      "Epoch 4481, Loss: 0.6967033818364143, Final Batch Loss: 0.10061753541231155\n",
      "Epoch 4482, Loss: 0.8133164346218109, Final Batch Loss: 0.20656818151474\n",
      "Epoch 4483, Loss: 0.8297746926546097, Final Batch Loss: 0.26688453555107117\n",
      "Epoch 4484, Loss: 0.7877234667539597, Final Batch Loss: 0.14237536489963531\n",
      "Epoch 4485, Loss: 0.8092004805803299, Final Batch Loss: 0.19701999425888062\n",
      "Epoch 4486, Loss: 0.869937852025032, Final Batch Loss: 0.2125874012708664\n",
      "Epoch 4487, Loss: 0.8290867656469345, Final Batch Loss: 0.21035845577716827\n",
      "Epoch 4488, Loss: 0.9615371525287628, Final Batch Loss: 0.2965138256549835\n",
      "Epoch 4489, Loss: 0.8442098051309586, Final Batch Loss: 0.20298242568969727\n",
      "Epoch 4490, Loss: 0.8132093101739883, Final Batch Loss: 0.26762425899505615\n",
      "Epoch 4491, Loss: 0.9202949106693268, Final Batch Loss: 0.26411038637161255\n",
      "Epoch 4492, Loss: 0.7845807075500488, Final Batch Loss: 0.19738388061523438\n",
      "Epoch 4493, Loss: 0.8047014623880386, Final Batch Loss: 0.1754806935787201\n",
      "Epoch 4494, Loss: 0.7698240280151367, Final Batch Loss: 0.14762459695339203\n",
      "Epoch 4495, Loss: 0.8714339435100555, Final Batch Loss: 0.27662304043769836\n",
      "Epoch 4496, Loss: 0.8559439331293106, Final Batch Loss: 0.16601689159870148\n",
      "Epoch 4497, Loss: 0.8216947913169861, Final Batch Loss: 0.24037013947963715\n",
      "Epoch 4498, Loss: 0.9348466992378235, Final Batch Loss: 0.25827085971832275\n",
      "Epoch 4499, Loss: 0.7916784733533859, Final Batch Loss: 0.15792620182037354\n",
      "Epoch 4500, Loss: 0.9239353984594345, Final Batch Loss: 0.2380896955728531\n",
      "Epoch 4501, Loss: 0.8003159165382385, Final Batch Loss: 0.23481667041778564\n",
      "Epoch 4502, Loss: 0.8138802796602249, Final Batch Loss: 0.19440346956253052\n",
      "Epoch 4503, Loss: 0.8058279752731323, Final Batch Loss: 0.1953323483467102\n",
      "Epoch 4504, Loss: 0.8384824842214584, Final Batch Loss: 0.22967329621315002\n",
      "Epoch 4505, Loss: 0.8233852684497833, Final Batch Loss: 0.19863949716091156\n",
      "Epoch 4506, Loss: 0.7754041329026222, Final Batch Loss: 0.12272369116544724\n",
      "Epoch 4507, Loss: 0.7664442509412766, Final Batch Loss: 0.23102207481861115\n",
      "Epoch 4508, Loss: 0.8163490146398544, Final Batch Loss: 0.18070745468139648\n",
      "Epoch 4509, Loss: 0.7792638093233109, Final Batch Loss: 0.1941092610359192\n",
      "Epoch 4510, Loss: 0.7499396204948425, Final Batch Loss: 0.1526079624891281\n",
      "Epoch 4511, Loss: 0.8202432841062546, Final Batch Loss: 0.17675811052322388\n",
      "Epoch 4512, Loss: 0.7831732034683228, Final Batch Loss: 0.19487757980823517\n",
      "Epoch 4513, Loss: 0.8107739239931107, Final Batch Loss: 0.1861489713191986\n",
      "Epoch 4514, Loss: 0.9384456872940063, Final Batch Loss: 0.2949988842010498\n",
      "Epoch 4515, Loss: 0.8148300349712372, Final Batch Loss: 0.14724653959274292\n",
      "Epoch 4516, Loss: 0.8282386064529419, Final Batch Loss: 0.17140227556228638\n",
      "Epoch 4517, Loss: 0.8943888396024704, Final Batch Loss: 0.16347743570804596\n",
      "Epoch 4518, Loss: 0.8198350369930267, Final Batch Loss: 0.14913104474544525\n",
      "Epoch 4519, Loss: 0.8165872693061829, Final Batch Loss: 0.14420263469219208\n",
      "Epoch 4520, Loss: 0.782589465379715, Final Batch Loss: 0.19362324476242065\n",
      "Epoch 4521, Loss: 0.8069712817668915, Final Batch Loss: 0.1690068542957306\n",
      "Epoch 4522, Loss: 0.8024345636367798, Final Batch Loss: 0.1984827071428299\n",
      "Epoch 4523, Loss: 0.7633389979600906, Final Batch Loss: 0.18890134990215302\n",
      "Epoch 4524, Loss: 0.7684787660837173, Final Batch Loss: 0.17547358572483063\n",
      "Epoch 4525, Loss: 0.8337539732456207, Final Batch Loss: 0.20328767597675323\n",
      "Epoch 4526, Loss: 0.7663837671279907, Final Batch Loss: 0.21461781859397888\n",
      "Epoch 4527, Loss: 0.8281733989715576, Final Batch Loss: 0.24667829275131226\n",
      "Epoch 4528, Loss: 0.7835314869880676, Final Batch Loss: 0.16980384290218353\n",
      "Epoch 4529, Loss: 0.781215712428093, Final Batch Loss: 0.16114060580730438\n",
      "Epoch 4530, Loss: 0.8955401033163071, Final Batch Loss: 0.2946827709674835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4531, Loss: 0.7610562741756439, Final Batch Loss: 0.18940198421478271\n",
      "Epoch 4532, Loss: 0.7516239285469055, Final Batch Loss: 0.14242222905158997\n",
      "Epoch 4533, Loss: 0.8425866216421127, Final Batch Loss: 0.21617022156715393\n",
      "Epoch 4534, Loss: 0.869748592376709, Final Batch Loss: 0.26977941393852234\n",
      "Epoch 4535, Loss: 0.8140484392642975, Final Batch Loss: 0.23687463998794556\n",
      "Epoch 4536, Loss: 0.7856332808732986, Final Batch Loss: 0.19765008985996246\n",
      "Epoch 4537, Loss: 0.7436961457133293, Final Batch Loss: 0.11653111129999161\n",
      "Epoch 4538, Loss: 0.8750783801078796, Final Batch Loss: 0.25423485040664673\n",
      "Epoch 4539, Loss: 0.7619083225727081, Final Batch Loss: 0.1939702332019806\n",
      "Epoch 4540, Loss: 0.824515163898468, Final Batch Loss: 0.22719435393810272\n",
      "Epoch 4541, Loss: 0.792449951171875, Final Batch Loss: 0.20817643404006958\n",
      "Epoch 4542, Loss: 0.8059595227241516, Final Batch Loss: 0.20450396835803986\n",
      "Epoch 4543, Loss: 0.7714478820562363, Final Batch Loss: 0.1915987730026245\n",
      "Epoch 4544, Loss: 0.8390944749116898, Final Batch Loss: 0.21051576733589172\n",
      "Epoch 4545, Loss: 0.8888314664363861, Final Batch Loss: 0.23547042906284332\n",
      "Epoch 4546, Loss: 0.8063646554946899, Final Batch Loss: 0.1834048181772232\n",
      "Epoch 4547, Loss: 0.7676502764225006, Final Batch Loss: 0.18583504855632782\n",
      "Epoch 4548, Loss: 0.7621823996305466, Final Batch Loss: 0.1466979831457138\n",
      "Epoch 4549, Loss: 0.8491262048482895, Final Batch Loss: 0.21752198040485382\n",
      "Epoch 4550, Loss: 0.8292182236909866, Final Batch Loss: 0.21630048751831055\n",
      "Epoch 4551, Loss: 0.850585088133812, Final Batch Loss: 0.1901334971189499\n",
      "Epoch 4552, Loss: 0.747680276632309, Final Batch Loss: 0.1441027820110321\n",
      "Epoch 4553, Loss: 0.738032728433609, Final Batch Loss: 0.16350261867046356\n",
      "Epoch 4554, Loss: 0.8819725215435028, Final Batch Loss: 0.3011179566383362\n",
      "Epoch 4555, Loss: 0.8886953592300415, Final Batch Loss: 0.20093387365341187\n",
      "Epoch 4556, Loss: 0.8310595005750656, Final Batch Loss: 0.29065898060798645\n",
      "Epoch 4557, Loss: 0.8064238131046295, Final Batch Loss: 0.1849885880947113\n",
      "Epoch 4558, Loss: 0.6958753913640976, Final Batch Loss: 0.1314166635274887\n",
      "Epoch 4559, Loss: 0.816176176071167, Final Batch Loss: 0.2170635610818863\n",
      "Epoch 4560, Loss: 0.8721138685941696, Final Batch Loss: 0.2741030156612396\n",
      "Epoch 4561, Loss: 0.8445353507995605, Final Batch Loss: 0.23506063222885132\n",
      "Epoch 4562, Loss: 0.9295763224363327, Final Batch Loss: 0.16453076899051666\n",
      "Epoch 4563, Loss: 0.8112030476331711, Final Batch Loss: 0.1683630645275116\n",
      "Epoch 4564, Loss: 0.8091380000114441, Final Batch Loss: 0.1755857616662979\n",
      "Epoch 4565, Loss: 0.8727392703294754, Final Batch Loss: 0.25219032168388367\n",
      "Epoch 4566, Loss: 0.9303328543901443, Final Batch Loss: 0.2617757320404053\n",
      "Epoch 4567, Loss: 0.8535332828760147, Final Batch Loss: 0.17897628247737885\n",
      "Epoch 4568, Loss: 0.9292560070753098, Final Batch Loss: 0.23736344277858734\n",
      "Epoch 4569, Loss: 0.838842585682869, Final Batch Loss: 0.14950330555438995\n",
      "Epoch 4570, Loss: 0.7972507476806641, Final Batch Loss: 0.16985514760017395\n",
      "Epoch 4571, Loss: 0.9217168092727661, Final Batch Loss: 0.2465650886297226\n",
      "Epoch 4572, Loss: 0.8562251776456833, Final Batch Loss: 0.24534972012043\n",
      "Epoch 4573, Loss: 0.7798890024423599, Final Batch Loss: 0.19562217593193054\n",
      "Epoch 4574, Loss: 0.7898263037204742, Final Batch Loss: 0.20255930721759796\n",
      "Epoch 4575, Loss: 0.7491483464837074, Final Batch Loss: 0.12413772195577621\n",
      "Epoch 4576, Loss: 0.8546519130468369, Final Batch Loss: 0.2280484437942505\n",
      "Epoch 4577, Loss: 0.798796147108078, Final Batch Loss: 0.20043186843395233\n",
      "Epoch 4578, Loss: 0.8729638010263443, Final Batch Loss: 0.2662928104400635\n",
      "Epoch 4579, Loss: 0.8437045812606812, Final Batch Loss: 0.27308389544487\n",
      "Epoch 4580, Loss: 0.8295786827802658, Final Batch Loss: 0.20429770648479462\n",
      "Epoch 4581, Loss: 0.9355693757534027, Final Batch Loss: 0.23777052760124207\n",
      "Epoch 4582, Loss: 0.8324769735336304, Final Batch Loss: 0.1367054134607315\n",
      "Epoch 4583, Loss: 0.9330327957868576, Final Batch Loss: 0.25522100925445557\n",
      "Epoch 4584, Loss: 0.8099743723869324, Final Batch Loss: 0.17211511731147766\n",
      "Epoch 4585, Loss: 0.812266007065773, Final Batch Loss: 0.19254860281944275\n",
      "Epoch 4586, Loss: 0.7324953824281693, Final Batch Loss: 0.15313158929347992\n",
      "Epoch 4587, Loss: 0.9121400564908981, Final Batch Loss: 0.2681395411491394\n",
      "Epoch 4588, Loss: 0.8500335067510605, Final Batch Loss: 0.22712190449237823\n",
      "Epoch 4589, Loss: 0.8567004799842834, Final Batch Loss: 0.1912243366241455\n",
      "Epoch 4590, Loss: 0.9510071128606796, Final Batch Loss: 0.33552995324134827\n",
      "Epoch 4591, Loss: 0.8369414657354355, Final Batch Loss: 0.1781121790409088\n",
      "Epoch 4592, Loss: 0.9173084944486618, Final Batch Loss: 0.246675044298172\n",
      "Epoch 4593, Loss: 0.9735196530818939, Final Batch Loss: 0.3643098473548889\n",
      "Epoch 4594, Loss: 0.7422870695590973, Final Batch Loss: 0.15082670748233795\n",
      "Epoch 4595, Loss: 0.8310582786798477, Final Batch Loss: 0.2420835644006729\n",
      "Epoch 4596, Loss: 0.8991718143224716, Final Batch Loss: 0.25205904245376587\n",
      "Epoch 4597, Loss: 0.8147977441549301, Final Batch Loss: 0.1909395158290863\n",
      "Epoch 4598, Loss: 0.8107649683952332, Final Batch Loss: 0.18032661080360413\n",
      "Epoch 4599, Loss: 0.8754780143499374, Final Batch Loss: 0.15213802456855774\n",
      "Epoch 4600, Loss: 0.7607734054327011, Final Batch Loss: 0.19047336280345917\n",
      "Epoch 4601, Loss: 0.7872729748487473, Final Batch Loss: 0.15428701043128967\n",
      "Epoch 4602, Loss: 0.7711475193500519, Final Batch Loss: 0.18156208097934723\n",
      "Epoch 4603, Loss: 0.9135351777076721, Final Batch Loss: 0.19086971879005432\n",
      "Epoch 4604, Loss: 0.8331604599952698, Final Batch Loss: 0.13738219439983368\n",
      "Epoch 4605, Loss: 0.7474024146795273, Final Batch Loss: 0.16907013952732086\n",
      "Epoch 4606, Loss: 0.7482904195785522, Final Batch Loss: 0.1864250749349594\n",
      "Epoch 4607, Loss: 0.8602368235588074, Final Batch Loss: 0.22947528958320618\n",
      "Epoch 4608, Loss: 0.8198737800121307, Final Batch Loss: 0.23123909533023834\n",
      "Epoch 4609, Loss: 0.8134683221578598, Final Batch Loss: 0.19162903726100922\n",
      "Epoch 4610, Loss: 0.8523609042167664, Final Batch Loss: 0.18964813649654388\n",
      "Epoch 4611, Loss: 0.8645863682031631, Final Batch Loss: 0.24981285631656647\n",
      "Epoch 4612, Loss: 0.8522146046161652, Final Batch Loss: 0.2585783302783966\n",
      "Epoch 4613, Loss: 0.8102431297302246, Final Batch Loss: 0.18367408215999603\n",
      "Epoch 4614, Loss: 0.8197733014822006, Final Batch Loss: 0.19026252627372742\n",
      "Epoch 4615, Loss: 0.7896360456943512, Final Batch Loss: 0.1744658648967743\n",
      "Epoch 4616, Loss: 0.8368349224328995, Final Batch Loss: 0.1647757887840271\n",
      "Epoch 4617, Loss: 0.8945036381483078, Final Batch Loss: 0.26370546221733093\n",
      "Epoch 4618, Loss: 0.9004734009504318, Final Batch Loss: 0.2554384469985962\n",
      "Epoch 4619, Loss: 0.8524497598409653, Final Batch Loss: 0.21057218313217163\n",
      "Epoch 4620, Loss: 0.8968597501516342, Final Batch Loss: 0.2384198158979416\n",
      "Epoch 4621, Loss: 0.8391138464212418, Final Batch Loss: 0.26840639114379883\n",
      "Epoch 4622, Loss: 0.8403866440057755, Final Batch Loss: 0.233428955078125\n",
      "Epoch 4623, Loss: 1.034738302230835, Final Batch Loss: 0.3127729892730713\n",
      "Epoch 4624, Loss: 0.7992637604475021, Final Batch Loss: 0.2002945840358734\n",
      "Epoch 4625, Loss: 0.9582146108150482, Final Batch Loss: 0.25711339712142944\n",
      "Epoch 4626, Loss: 0.7271558493375778, Final Batch Loss: 0.12915192544460297\n",
      "Epoch 4627, Loss: 0.9057512134313583, Final Batch Loss: 0.26835283637046814\n",
      "Epoch 4628, Loss: 0.7910488396883011, Final Batch Loss: 0.16516610980033875\n",
      "Epoch 4629, Loss: 0.7471764832735062, Final Batch Loss: 0.15259909629821777\n",
      "Epoch 4630, Loss: 0.7876850664615631, Final Batch Loss: 0.12701815366744995\n",
      "Epoch 4631, Loss: 0.7955278009176254, Final Batch Loss: 0.22633016109466553\n",
      "Epoch 4632, Loss: 0.8363495469093323, Final Batch Loss: 0.2164858877658844\n",
      "Epoch 4633, Loss: 0.8034788519144058, Final Batch Loss: 0.17185355722904205\n",
      "Epoch 4634, Loss: 0.7676093280315399, Final Batch Loss: 0.20481164753437042\n",
      "Epoch 4635, Loss: 0.8077547997236252, Final Batch Loss: 0.24730239808559418\n",
      "Epoch 4636, Loss: 0.7828009873628616, Final Batch Loss: 0.1511891484260559\n",
      "Epoch 4637, Loss: 0.7172179520130157, Final Batch Loss: 0.15014533698558807\n",
      "Epoch 4638, Loss: 0.7464380264282227, Final Batch Loss: 0.15321050584316254\n",
      "Epoch 4639, Loss: 0.9071741551160812, Final Batch Loss: 0.31288638710975647\n",
      "Epoch 4640, Loss: 0.823499009013176, Final Batch Loss: 0.2290523648262024\n",
      "Epoch 4641, Loss: 0.7908513993024826, Final Batch Loss: 0.17430172860622406\n",
      "Epoch 4642, Loss: 0.9065040498971939, Final Batch Loss: 0.2545037865638733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4643, Loss: 0.7744732052087784, Final Batch Loss: 0.16194850206375122\n",
      "Epoch 4644, Loss: 0.7726091295480728, Final Batch Loss: 0.16565503180027008\n",
      "Epoch 4645, Loss: 0.7337992340326309, Final Batch Loss: 0.1467619091272354\n",
      "Epoch 4646, Loss: 0.7592011243104935, Final Batch Loss: 0.1796189397573471\n",
      "Epoch 4647, Loss: 0.767309308052063, Final Batch Loss: 0.13132505118846893\n",
      "Epoch 4648, Loss: 0.7505527436733246, Final Batch Loss: 0.15003849565982819\n",
      "Epoch 4649, Loss: 0.7285124957561493, Final Batch Loss: 0.1917819380760193\n",
      "Epoch 4650, Loss: 0.7424536645412445, Final Batch Loss: 0.1752786636352539\n",
      "Epoch 4651, Loss: 0.7574361264705658, Final Batch Loss: 0.2107093781232834\n",
      "Epoch 4652, Loss: 0.7719143331050873, Final Batch Loss: 0.19043587148189545\n",
      "Epoch 4653, Loss: 0.7401917278766632, Final Batch Loss: 0.1917572021484375\n",
      "Epoch 4654, Loss: 0.7698903530836105, Final Batch Loss: 0.1876802295446396\n",
      "Epoch 4655, Loss: 0.7874967753887177, Final Batch Loss: 0.1785888671875\n",
      "Epoch 4656, Loss: 0.8061792254447937, Final Batch Loss: 0.20604057610034943\n",
      "Epoch 4657, Loss: 0.8232095390558243, Final Batch Loss: 0.1296963393688202\n",
      "Epoch 4658, Loss: 0.7758196741342545, Final Batch Loss: 0.15335960686206818\n",
      "Epoch 4659, Loss: 0.757248654961586, Final Batch Loss: 0.13991613686084747\n",
      "Epoch 4660, Loss: 0.7707363069057465, Final Batch Loss: 0.21545544266700745\n",
      "Epoch 4661, Loss: 0.7918154001235962, Final Batch Loss: 0.1961732804775238\n",
      "Epoch 4662, Loss: 0.803938165307045, Final Batch Loss: 0.1852790117263794\n",
      "Epoch 4663, Loss: 0.8067808151245117, Final Batch Loss: 0.17454706132411957\n",
      "Epoch 4664, Loss: 0.8038458824157715, Final Batch Loss: 0.1917291134595871\n",
      "Epoch 4665, Loss: 0.7701658755540848, Final Batch Loss: 0.18437370657920837\n",
      "Epoch 4666, Loss: 0.7836848646402359, Final Batch Loss: 0.19691762328147888\n",
      "Epoch 4667, Loss: 0.9015929251909256, Final Batch Loss: 0.2907879054546356\n",
      "Epoch 4668, Loss: 0.8284387141466141, Final Batch Loss: 0.22859105467796326\n",
      "Epoch 4669, Loss: 0.8360504508018494, Final Batch Loss: 0.25759297609329224\n",
      "Epoch 4670, Loss: 0.8794018179178238, Final Batch Loss: 0.2908702492713928\n",
      "Epoch 4671, Loss: 0.9110472202301025, Final Batch Loss: 0.17341487109661102\n",
      "Epoch 4672, Loss: 0.7372350692749023, Final Batch Loss: 0.13131999969482422\n",
      "Epoch 4673, Loss: 0.7544347047805786, Final Batch Loss: 0.20791363716125488\n",
      "Epoch 4674, Loss: 0.8303535282611847, Final Batch Loss: 0.2070588767528534\n",
      "Epoch 4675, Loss: 0.8033892661333084, Final Batch Loss: 0.21134254336357117\n",
      "Epoch 4676, Loss: 0.8260089010000229, Final Batch Loss: 0.1904890239238739\n",
      "Epoch 4677, Loss: 0.8043234646320343, Final Batch Loss: 0.16107159852981567\n",
      "Epoch 4678, Loss: 0.8768650740385056, Final Batch Loss: 0.2911055088043213\n",
      "Epoch 4679, Loss: 0.761667937040329, Final Batch Loss: 0.1907195895910263\n",
      "Epoch 4680, Loss: 0.8373722732067108, Final Batch Loss: 0.19682401418685913\n",
      "Epoch 4681, Loss: 0.8082369267940521, Final Batch Loss: 0.18170525133609772\n",
      "Epoch 4682, Loss: 0.8486796617507935, Final Batch Loss: 0.2810042202472687\n",
      "Epoch 4683, Loss: 0.8738058507442474, Final Batch Loss: 0.23191410303115845\n",
      "Epoch 4684, Loss: 0.7790350690484047, Final Batch Loss: 0.1138615682721138\n",
      "Epoch 4685, Loss: 0.7933499068021774, Final Batch Loss: 0.1632688194513321\n",
      "Epoch 4686, Loss: 0.8202261626720428, Final Batch Loss: 0.21326689422130585\n",
      "Epoch 4687, Loss: 0.8137063384056091, Final Batch Loss: 0.2153523862361908\n",
      "Epoch 4688, Loss: 0.7718376368284225, Final Batch Loss: 0.20305046439170837\n",
      "Epoch 4689, Loss: 0.7273190319538116, Final Batch Loss: 0.13095387816429138\n",
      "Epoch 4690, Loss: 0.868705615401268, Final Batch Loss: 0.21998125314712524\n",
      "Epoch 4691, Loss: 0.8040916472673416, Final Batch Loss: 0.20900726318359375\n",
      "Epoch 4692, Loss: 0.8496926873922348, Final Batch Loss: 0.21304136514663696\n",
      "Epoch 4693, Loss: 0.733122430741787, Final Batch Loss: 0.11733400076627731\n",
      "Epoch 4694, Loss: 0.7713126689195633, Final Batch Loss: 0.154086172580719\n",
      "Epoch 4695, Loss: 0.9134842753410339, Final Batch Loss: 0.23939473927021027\n",
      "Epoch 4696, Loss: 0.8207317441701889, Final Batch Loss: 0.2547753155231476\n",
      "Epoch 4697, Loss: 0.7825908958911896, Final Batch Loss: 0.18452881276607513\n",
      "Epoch 4698, Loss: 0.7744123488664627, Final Batch Loss: 0.15849681198596954\n",
      "Epoch 4699, Loss: 0.8126559108495712, Final Batch Loss: 0.1578148454427719\n",
      "Epoch 4700, Loss: 0.8568306118249893, Final Batch Loss: 0.19203293323516846\n",
      "Epoch 4701, Loss: 0.9380994737148285, Final Batch Loss: 0.26076602935791016\n",
      "Epoch 4702, Loss: 0.8179926723241806, Final Batch Loss: 0.197737917304039\n",
      "Epoch 4703, Loss: 0.8715345710515976, Final Batch Loss: 0.193679541349411\n",
      "Epoch 4704, Loss: 0.8923438340425491, Final Batch Loss: 0.26294243335723877\n",
      "Epoch 4705, Loss: 0.7540508359670639, Final Batch Loss: 0.16415904462337494\n",
      "Epoch 4706, Loss: 0.8383887112140656, Final Batch Loss: 0.22635719180107117\n",
      "Epoch 4707, Loss: 0.7457761019468307, Final Batch Loss: 0.10507169365882874\n",
      "Epoch 4708, Loss: 0.8462624102830887, Final Batch Loss: 0.27248096466064453\n",
      "Epoch 4709, Loss: 0.8003163486719131, Final Batch Loss: 0.11655232310295105\n",
      "Epoch 4710, Loss: 0.8581786453723907, Final Batch Loss: 0.2378031611442566\n",
      "Epoch 4711, Loss: 0.826644629240036, Final Batch Loss: 0.27348577976226807\n",
      "Epoch 4712, Loss: 0.8041445910930634, Final Batch Loss: 0.1924036741256714\n",
      "Epoch 4713, Loss: 0.8760860562324524, Final Batch Loss: 0.2909751832485199\n",
      "Epoch 4714, Loss: 0.7218719944357872, Final Batch Loss: 0.12277435511350632\n",
      "Epoch 4715, Loss: 0.8946421444416046, Final Batch Loss: 0.26267939805984497\n",
      "Epoch 4716, Loss: 0.8796490728855133, Final Batch Loss: 0.2823902368545532\n",
      "Epoch 4717, Loss: 0.7634519189596176, Final Batch Loss: 0.1733936220407486\n",
      "Epoch 4718, Loss: 0.7671976983547211, Final Batch Loss: 0.1875748485326767\n",
      "Epoch 4719, Loss: 0.8401756882667542, Final Batch Loss: 0.18701940774917603\n",
      "Epoch 4720, Loss: 0.7849079221487045, Final Batch Loss: 0.19122092425823212\n",
      "Epoch 4721, Loss: 0.9186064302921295, Final Batch Loss: 0.2750382125377655\n",
      "Epoch 4722, Loss: 0.8558406829833984, Final Batch Loss: 0.25783470273017883\n",
      "Epoch 4723, Loss: 0.974008098244667, Final Batch Loss: 0.3473992943763733\n",
      "Epoch 4724, Loss: 0.7125502526760101, Final Batch Loss: 0.12923181056976318\n",
      "Epoch 4725, Loss: 0.8561390191316605, Final Batch Loss: 0.23221375048160553\n",
      "Epoch 4726, Loss: 0.8376898616552353, Final Batch Loss: 0.1924525946378708\n",
      "Epoch 4727, Loss: 0.8699274212121964, Final Batch Loss: 0.2799954116344452\n",
      "Epoch 4728, Loss: 0.8038503229618073, Final Batch Loss: 0.220636785030365\n",
      "Epoch 4729, Loss: 0.8647369295358658, Final Batch Loss: 0.2264338731765747\n",
      "Epoch 4730, Loss: 0.8055744767189026, Final Batch Loss: 0.22197113931179047\n",
      "Epoch 4731, Loss: 0.8738416582345963, Final Batch Loss: 0.23798280954360962\n",
      "Epoch 4732, Loss: 0.7971949726343155, Final Batch Loss: 0.2951425611972809\n",
      "Epoch 4733, Loss: 0.8923143148422241, Final Batch Loss: 0.231163889169693\n",
      "Epoch 4734, Loss: 0.916468933224678, Final Batch Loss: 0.30512383580207825\n",
      "Epoch 4735, Loss: 0.8117896318435669, Final Batch Loss: 0.202276811003685\n",
      "Epoch 4736, Loss: 0.8847544938325882, Final Batch Loss: 0.20172226428985596\n",
      "Epoch 4737, Loss: 0.791582465171814, Final Batch Loss: 0.17175157368183136\n",
      "Epoch 4738, Loss: 0.7924970984458923, Final Batch Loss: 0.16367334127426147\n",
      "Epoch 4739, Loss: 0.7567657828330994, Final Batch Loss: 0.18764646351337433\n",
      "Epoch 4740, Loss: 0.8877578377723694, Final Batch Loss: 0.2042684704065323\n",
      "Epoch 4741, Loss: 0.7637015879154205, Final Batch Loss: 0.18243077397346497\n",
      "Epoch 4742, Loss: 0.8683943450450897, Final Batch Loss: 0.1925494372844696\n",
      "Epoch 4743, Loss: 0.7642681300640106, Final Batch Loss: 0.19857163727283478\n",
      "Epoch 4744, Loss: 0.7913191467523575, Final Batch Loss: 0.1936151683330536\n",
      "Epoch 4745, Loss: 0.7385284155607224, Final Batch Loss: 0.17842988669872284\n",
      "Epoch 4746, Loss: 0.7407004833221436, Final Batch Loss: 0.21769089996814728\n",
      "Epoch 4747, Loss: 0.8070046752691269, Final Batch Loss: 0.1798669993877411\n",
      "Epoch 4748, Loss: 0.73418889939785, Final Batch Loss: 0.15277977287769318\n",
      "Epoch 4749, Loss: 0.7810138911008835, Final Batch Loss: 0.15141251683235168\n",
      "Epoch 4750, Loss: 0.9506907165050507, Final Batch Loss: 0.2746528685092926\n",
      "Epoch 4751, Loss: 0.8397183865308762, Final Batch Loss: 0.19831135869026184\n",
      "Epoch 4752, Loss: 0.7485954314470291, Final Batch Loss: 0.1283736675977707\n",
      "Epoch 4753, Loss: 0.7374045848846436, Final Batch Loss: 0.15438413619995117\n",
      "Epoch 4754, Loss: 0.7883880138397217, Final Batch Loss: 0.19789926707744598\n",
      "Epoch 4755, Loss: 0.8320683538913727, Final Batch Loss: 0.19626788794994354\n",
      "Epoch 4756, Loss: 0.7841796204447746, Final Batch Loss: 0.11450757831335068\n",
      "Epoch 4757, Loss: 0.817450225353241, Final Batch Loss: 0.22099576890468597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4758, Loss: 0.7769823521375656, Final Batch Loss: 0.17801232635974884\n",
      "Epoch 4759, Loss: 0.862817570567131, Final Batch Loss: 0.2315366119146347\n",
      "Epoch 4760, Loss: 0.8113760650157928, Final Batch Loss: 0.1965470016002655\n",
      "Epoch 4761, Loss: 0.923923447728157, Final Batch Loss: 0.3693113327026367\n",
      "Epoch 4762, Loss: 0.8524850010871887, Final Batch Loss: 0.23305410146713257\n",
      "Epoch 4763, Loss: 0.779274582862854, Final Batch Loss: 0.17727474868297577\n",
      "Epoch 4764, Loss: 0.8234779536724091, Final Batch Loss: 0.2590814530849457\n",
      "Epoch 4765, Loss: 0.8092759102582932, Final Batch Loss: 0.14968524873256683\n",
      "Epoch 4766, Loss: 0.8360952287912369, Final Batch Loss: 0.14567098021507263\n",
      "Epoch 4767, Loss: 0.9908273369073868, Final Batch Loss: 0.2258293628692627\n",
      "Epoch 4768, Loss: 0.8712935894727707, Final Batch Loss: 0.217428058385849\n",
      "Epoch 4769, Loss: 0.7878182530403137, Final Batch Loss: 0.14843201637268066\n",
      "Epoch 4770, Loss: 0.9154326766729355, Final Batch Loss: 0.21141794323921204\n",
      "Epoch 4771, Loss: 0.7797548919916153, Final Batch Loss: 0.18008460104465485\n",
      "Epoch 4772, Loss: 0.7069127708673477, Final Batch Loss: 0.15124987065792084\n",
      "Epoch 4773, Loss: 0.8304926455020905, Final Batch Loss: 0.17897465825080872\n",
      "Epoch 4774, Loss: 0.8639131188392639, Final Batch Loss: 0.19724124670028687\n",
      "Epoch 4775, Loss: 0.8623397201299667, Final Batch Loss: 0.25590795278549194\n",
      "Epoch 4776, Loss: 0.8599870949983597, Final Batch Loss: 0.23251867294311523\n",
      "Epoch 4777, Loss: 0.7941551059484482, Final Batch Loss: 0.19534532725811005\n",
      "Epoch 4778, Loss: 0.7813586592674255, Final Batch Loss: 0.18522846698760986\n",
      "Epoch 4779, Loss: 0.8018159121274948, Final Batch Loss: 0.2013130784034729\n",
      "Epoch 4780, Loss: 0.7744676768779755, Final Batch Loss: 0.18396402895450592\n",
      "Epoch 4781, Loss: 0.7629737257957458, Final Batch Loss: 0.2247377187013626\n",
      "Epoch 4782, Loss: 0.9538010656833649, Final Batch Loss: 0.29073548316955566\n",
      "Epoch 4783, Loss: 0.8099901676177979, Final Batch Loss: 0.18635053932666779\n",
      "Epoch 4784, Loss: 0.8773439228534698, Final Batch Loss: 0.30432257056236267\n",
      "Epoch 4785, Loss: 0.7707290798425674, Final Batch Loss: 0.20187237858772278\n",
      "Epoch 4786, Loss: 0.7811644524335861, Final Batch Loss: 0.225102961063385\n",
      "Epoch 4787, Loss: 0.8216791450977325, Final Batch Loss: 0.29337918758392334\n",
      "Epoch 4788, Loss: 0.7550382763147354, Final Batch Loss: 0.1859392374753952\n",
      "Epoch 4789, Loss: 0.6571915671229362, Final Batch Loss: 0.11939889937639236\n",
      "Epoch 4790, Loss: 0.7393050789833069, Final Batch Loss: 0.1620236337184906\n",
      "Epoch 4791, Loss: 0.8643722236156464, Final Batch Loss: 0.20787332952022552\n",
      "Epoch 4792, Loss: 0.7133495211601257, Final Batch Loss: 0.15909743309020996\n",
      "Epoch 4793, Loss: 0.7707661688327789, Final Batch Loss: 0.2304150015115738\n",
      "Epoch 4794, Loss: 0.7768290936946869, Final Batch Loss: 0.18840686976909637\n",
      "Epoch 4795, Loss: 0.8544519990682602, Final Batch Loss: 0.16320113837718964\n",
      "Epoch 4796, Loss: 0.8093987554311752, Final Batch Loss: 0.20132064819335938\n",
      "Epoch 4797, Loss: 0.8155407905578613, Final Batch Loss: 0.24646465480327606\n",
      "Epoch 4798, Loss: 0.9607472568750381, Final Batch Loss: 0.3472651243209839\n",
      "Epoch 4799, Loss: 0.8858309090137482, Final Batch Loss: 0.2612954080104828\n",
      "Epoch 4800, Loss: 0.7621728181838989, Final Batch Loss: 0.1879805326461792\n",
      "Epoch 4801, Loss: 0.9525674432516098, Final Batch Loss: 0.35621151328086853\n",
      "Epoch 4802, Loss: 0.7852443158626556, Final Batch Loss: 0.18248316645622253\n",
      "Epoch 4803, Loss: 0.7804221212863922, Final Batch Loss: 0.1647619754076004\n",
      "Epoch 4804, Loss: 0.7565110325813293, Final Batch Loss: 0.14826996624469757\n",
      "Epoch 4805, Loss: 0.8634825944900513, Final Batch Loss: 0.2954668402671814\n",
      "Epoch 4806, Loss: 0.848392516374588, Final Batch Loss: 0.26448896527290344\n",
      "Epoch 4807, Loss: 0.7970462441444397, Final Batch Loss: 0.21667471528053284\n",
      "Epoch 4808, Loss: 0.7780420631170273, Final Batch Loss: 0.16050608456134796\n",
      "Epoch 4809, Loss: 0.8849441260099411, Final Batch Loss: 0.1904935985803604\n",
      "Epoch 4810, Loss: 0.8991624414920807, Final Batch Loss: 0.2532992959022522\n",
      "Epoch 4811, Loss: 0.8196089565753937, Final Batch Loss: 0.17263071238994598\n",
      "Epoch 4812, Loss: 0.8121592551469803, Final Batch Loss: 0.2258327454328537\n",
      "Epoch 4813, Loss: 0.7672445178031921, Final Batch Loss: 0.2480207234621048\n",
      "Epoch 4814, Loss: 0.7919333875179291, Final Batch Loss: 0.20795683562755585\n",
      "Epoch 4815, Loss: 0.7491893023252487, Final Batch Loss: 0.16907425224781036\n",
      "Epoch 4816, Loss: 0.832948237657547, Final Batch Loss: 0.22173550724983215\n",
      "Epoch 4817, Loss: 0.8326935321092606, Final Batch Loss: 0.2400384247303009\n",
      "Epoch 4818, Loss: 0.7535291314125061, Final Batch Loss: 0.1409534364938736\n",
      "Epoch 4819, Loss: 0.8533155918121338, Final Batch Loss: 0.23328787088394165\n",
      "Epoch 4820, Loss: 0.8432665765285492, Final Batch Loss: 0.2008320540189743\n",
      "Epoch 4821, Loss: 0.8333050012588501, Final Batch Loss: 0.2463611215353012\n",
      "Epoch 4822, Loss: 0.956860676407814, Final Batch Loss: 0.28440186381340027\n",
      "Epoch 4823, Loss: 0.867119088768959, Final Batch Loss: 0.251994252204895\n",
      "Epoch 4824, Loss: 0.7461365610361099, Final Batch Loss: 0.1725132316350937\n",
      "Epoch 4825, Loss: 0.8196959942579269, Final Batch Loss: 0.17800316214561462\n",
      "Epoch 4826, Loss: 0.7908832132816315, Final Batch Loss: 0.26584482192993164\n",
      "Epoch 4827, Loss: 0.8126469850540161, Final Batch Loss: 0.2242475301027298\n",
      "Epoch 4828, Loss: 0.7852144837379456, Final Batch Loss: 0.18365861475467682\n",
      "Epoch 4829, Loss: 0.748916432261467, Final Batch Loss: 0.13766725361347198\n",
      "Epoch 4830, Loss: 0.8215768784284592, Final Batch Loss: 0.1964840292930603\n",
      "Epoch 4831, Loss: 0.7180968672037125, Final Batch Loss: 0.15511521697044373\n",
      "Epoch 4832, Loss: 0.7231219261884689, Final Batch Loss: 0.15526942908763885\n",
      "Epoch 4833, Loss: 0.767702117562294, Final Batch Loss: 0.21105636656284332\n",
      "Epoch 4834, Loss: 0.7726431041955948, Final Batch Loss: 0.14017240703105927\n",
      "Epoch 4835, Loss: 0.7049072533845901, Final Batch Loss: 0.17691421508789062\n",
      "Epoch 4836, Loss: 0.8511610478162766, Final Batch Loss: 0.2510579228401184\n",
      "Epoch 4837, Loss: 0.9023431986570358, Final Batch Loss: 0.28430962562561035\n",
      "Epoch 4838, Loss: 0.782478392124176, Final Batch Loss: 0.1930960714817047\n",
      "Epoch 4839, Loss: 0.9168336987495422, Final Batch Loss: 0.2218862622976303\n",
      "Epoch 4840, Loss: 0.8791726529598236, Final Batch Loss: 0.2507941424846649\n",
      "Epoch 4841, Loss: 0.7582727521657944, Final Batch Loss: 0.16899707913398743\n",
      "Epoch 4842, Loss: 0.7511121332645416, Final Batch Loss: 0.16270367801189423\n",
      "Epoch 4843, Loss: 0.8001641184091568, Final Batch Loss: 0.1640927940607071\n",
      "Epoch 4844, Loss: 0.7946431636810303, Final Batch Loss: 0.2455368936061859\n",
      "Epoch 4845, Loss: 0.8291298747062683, Final Batch Loss: 0.1558740884065628\n",
      "Epoch 4846, Loss: 0.7914073914289474, Final Batch Loss: 0.20630088448524475\n",
      "Epoch 4847, Loss: 0.817293256521225, Final Batch Loss: 0.1980617791414261\n",
      "Epoch 4848, Loss: 0.7679933905601501, Final Batch Loss: 0.18581828474998474\n",
      "Epoch 4849, Loss: 0.778952494263649, Final Batch Loss: 0.21684983372688293\n",
      "Epoch 4850, Loss: 0.7939321398735046, Final Batch Loss: 0.22500570118427277\n",
      "Epoch 4851, Loss: 0.8072852939367294, Final Batch Loss: 0.19536565244197845\n",
      "Epoch 4852, Loss: 0.9039160460233688, Final Batch Loss: 0.2539207935333252\n",
      "Epoch 4853, Loss: 0.8767181038856506, Final Batch Loss: 0.21470731496810913\n",
      "Epoch 4854, Loss: 0.8594756126403809, Final Batch Loss: 0.2056574523448944\n",
      "Epoch 4855, Loss: 0.9019960314035416, Final Batch Loss: 0.27690181136131287\n",
      "Epoch 4856, Loss: 0.8762373626232147, Final Batch Loss: 0.24798016250133514\n",
      "Epoch 4857, Loss: 0.8699775189161301, Final Batch Loss: 0.2821408808231354\n",
      "Epoch 4858, Loss: 0.761845275759697, Final Batch Loss: 0.16501720249652863\n",
      "Epoch 4859, Loss: 0.7676731944084167, Final Batch Loss: 0.14825867116451263\n",
      "Epoch 4860, Loss: 0.7999818921089172, Final Batch Loss: 0.1908927708864212\n",
      "Epoch 4861, Loss: 0.7626008838415146, Final Batch Loss: 0.1674192249774933\n",
      "Epoch 4862, Loss: 0.8426773697137833, Final Batch Loss: 0.19754157960414886\n",
      "Epoch 4863, Loss: 0.8123256117105484, Final Batch Loss: 0.19908683001995087\n",
      "Epoch 4864, Loss: 0.7875480502843857, Final Batch Loss: 0.15664903819561005\n",
      "Epoch 4865, Loss: 0.8068556487560272, Final Batch Loss: 0.18116645514965057\n",
      "Epoch 4866, Loss: 0.7517284750938416, Final Batch Loss: 0.19772540032863617\n",
      "Epoch 4867, Loss: 0.8347402364015579, Final Batch Loss: 0.2076561450958252\n",
      "Epoch 4868, Loss: 0.7694510519504547, Final Batch Loss: 0.21044427156448364\n",
      "Epoch 4869, Loss: 0.8043535798788071, Final Batch Loss: 0.21391047537326813\n",
      "Epoch 4870, Loss: 0.8059518337249756, Final Batch Loss: 0.17194393277168274\n",
      "Epoch 4871, Loss: 0.8053712993860245, Final Batch Loss: 0.19135504961013794\n",
      "Epoch 4872, Loss: 0.7353264093399048, Final Batch Loss: 0.14848953485488892\n",
      "Epoch 4873, Loss: 0.8596489876508713, Final Batch Loss: 0.2113882601261139\n",
      "Epoch 4874, Loss: 0.8116749972105026, Final Batch Loss: 0.21891851723194122\n",
      "Epoch 4875, Loss: 0.7431953698396683, Final Batch Loss: 0.1524292230606079\n",
      "Epoch 4876, Loss: 0.7863065749406815, Final Batch Loss: 0.23703955113887787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4877, Loss: 0.760812982916832, Final Batch Loss: 0.1845485419034958\n",
      "Epoch 4878, Loss: 0.8474281579256058, Final Batch Loss: 0.2035142481327057\n",
      "Epoch 4879, Loss: 0.7101077735424042, Final Batch Loss: 0.1388215869665146\n",
      "Epoch 4880, Loss: 0.8366015553474426, Final Batch Loss: 0.25969383120536804\n",
      "Epoch 4881, Loss: 0.7883305847644806, Final Batch Loss: 0.20899870991706848\n",
      "Epoch 4882, Loss: 0.8113857954740524, Final Batch Loss: 0.2479681521654129\n",
      "Epoch 4883, Loss: 0.7529919445514679, Final Batch Loss: 0.1895359605550766\n",
      "Epoch 4884, Loss: 0.7684686779975891, Final Batch Loss: 0.18439607322216034\n",
      "Epoch 4885, Loss: 0.8217227011919022, Final Batch Loss: 0.2035268396139145\n",
      "Epoch 4886, Loss: 0.7703333795070648, Final Batch Loss: 0.15845812857151031\n",
      "Epoch 4887, Loss: 0.7723049819469452, Final Batch Loss: 0.14120110869407654\n",
      "Epoch 4888, Loss: 0.7877360582351685, Final Batch Loss: 0.1947687417268753\n",
      "Epoch 4889, Loss: 0.8626251667737961, Final Batch Loss: 0.34568992257118225\n",
      "Epoch 4890, Loss: 0.8530738949775696, Final Batch Loss: 0.26668480038642883\n",
      "Epoch 4891, Loss: 0.769209012389183, Final Batch Loss: 0.22197583317756653\n",
      "Epoch 4892, Loss: 0.7202295362949371, Final Batch Loss: 0.1399059146642685\n",
      "Epoch 4893, Loss: 0.8454984128475189, Final Batch Loss: 0.2638704180717468\n",
      "Epoch 4894, Loss: 0.8696005940437317, Final Batch Loss: 0.2661011815071106\n",
      "Epoch 4895, Loss: 0.7821829468011856, Final Batch Loss: 0.19155821204185486\n",
      "Epoch 4896, Loss: 0.7356204241514206, Final Batch Loss: 0.12946629524230957\n",
      "Epoch 4897, Loss: 0.8091719299554825, Final Batch Loss: 0.21691961586475372\n",
      "Epoch 4898, Loss: 0.9090573340654373, Final Batch Loss: 0.2580273449420929\n",
      "Epoch 4899, Loss: 0.8453093469142914, Final Batch Loss: 0.23053468763828278\n",
      "Epoch 4900, Loss: 0.7341265231370926, Final Batch Loss: 0.16293500363826752\n",
      "Epoch 4901, Loss: 0.7287714928388596, Final Batch Loss: 0.1656992882490158\n",
      "Epoch 4902, Loss: 0.6775159984827042, Final Batch Loss: 0.11903643608093262\n",
      "Epoch 4903, Loss: 0.7100348174571991, Final Batch Loss: 0.12203848361968994\n",
      "Epoch 4904, Loss: 0.7330697029829025, Final Batch Loss: 0.15156270563602448\n",
      "Epoch 4905, Loss: 0.8239252269268036, Final Batch Loss: 0.27130433917045593\n",
      "Epoch 4906, Loss: 0.931027814745903, Final Batch Loss: 0.30096569657325745\n",
      "Epoch 4907, Loss: 0.792126789689064, Final Batch Loss: 0.13990181684494019\n",
      "Epoch 4908, Loss: 0.7779980450868607, Final Batch Loss: 0.18146789073944092\n",
      "Epoch 4909, Loss: 0.6977259963750839, Final Batch Loss: 0.12856335937976837\n",
      "Epoch 4910, Loss: 0.822352409362793, Final Batch Loss: 0.22087444365024567\n",
      "Epoch 4911, Loss: 0.7833404242992401, Final Batch Loss: 0.21134603023529053\n",
      "Epoch 4912, Loss: 0.8345177471637726, Final Batch Loss: 0.15819784998893738\n",
      "Epoch 4913, Loss: 0.7470237612724304, Final Batch Loss: 0.160306915640831\n",
      "Epoch 4914, Loss: 0.8137922585010529, Final Batch Loss: 0.21778865158557892\n",
      "Epoch 4915, Loss: 0.7659991234540939, Final Batch Loss: 0.19842137396335602\n",
      "Epoch 4916, Loss: 0.7114567309617996, Final Batch Loss: 0.13966475427150726\n",
      "Epoch 4917, Loss: 0.8107359260320663, Final Batch Loss: 0.1459033191204071\n",
      "Epoch 4918, Loss: 0.7492252737283707, Final Batch Loss: 0.1614048033952713\n",
      "Epoch 4919, Loss: 0.803319588303566, Final Batch Loss: 0.195253387093544\n",
      "Epoch 4920, Loss: 0.7903988659381866, Final Batch Loss: 0.21580101549625397\n",
      "Epoch 4921, Loss: 0.8834870010614395, Final Batch Loss: 0.18197916448116302\n",
      "Epoch 4922, Loss: 0.8162969797849655, Final Batch Loss: 0.1906348615884781\n",
      "Epoch 4923, Loss: 0.8180053234100342, Final Batch Loss: 0.2339613288640976\n",
      "Epoch 4924, Loss: 0.7245417535305023, Final Batch Loss: 0.13494254648685455\n",
      "Epoch 4925, Loss: 0.7572778016328812, Final Batch Loss: 0.1892862170934677\n",
      "Epoch 4926, Loss: 0.7161873430013657, Final Batch Loss: 0.13470689952373505\n",
      "Epoch 4927, Loss: 0.7923973202705383, Final Batch Loss: 0.20621488988399506\n",
      "Epoch 4928, Loss: 0.7340838015079498, Final Batch Loss: 0.1853174865245819\n",
      "Epoch 4929, Loss: 0.7092767953872681, Final Batch Loss: 0.1302415132522583\n",
      "Epoch 4930, Loss: 0.7530266642570496, Final Batch Loss: 0.18180501461029053\n",
      "Epoch 4931, Loss: 0.8238181322813034, Final Batch Loss: 0.20309944450855255\n",
      "Epoch 4932, Loss: 0.8134463876485825, Final Batch Loss: 0.2439434975385666\n",
      "Epoch 4933, Loss: 0.7908940613269806, Final Batch Loss: 0.1670573502779007\n",
      "Epoch 4934, Loss: 0.8626104444265366, Final Batch Loss: 0.18426214158535004\n",
      "Epoch 4935, Loss: 0.7989989221096039, Final Batch Loss: 0.22668419778347015\n",
      "Epoch 4936, Loss: 0.74635449051857, Final Batch Loss: 0.238322913646698\n",
      "Epoch 4937, Loss: 0.8445563018321991, Final Batch Loss: 0.25393804907798767\n",
      "Epoch 4938, Loss: 0.7663443088531494, Final Batch Loss: 0.14696238934993744\n",
      "Epoch 4939, Loss: 0.7400148808956146, Final Batch Loss: 0.12531670928001404\n",
      "Epoch 4940, Loss: 0.7529362589120865, Final Batch Loss: 0.2183416187763214\n",
      "Epoch 4941, Loss: 0.74799545109272, Final Batch Loss: 0.1459648460149765\n",
      "Epoch 4942, Loss: 0.8045858442783356, Final Batch Loss: 0.17651315033435822\n",
      "Epoch 4943, Loss: 0.7723684906959534, Final Batch Loss: 0.16770221292972565\n",
      "Epoch 4944, Loss: 0.7741375714540482, Final Batch Loss: 0.19578540325164795\n",
      "Epoch 4945, Loss: 0.9171619117259979, Final Batch Loss: 0.30552488565444946\n",
      "Epoch 4946, Loss: 0.7728855907917023, Final Batch Loss: 0.12167134881019592\n",
      "Epoch 4947, Loss: 0.8150406628847122, Final Batch Loss: 0.21118059754371643\n",
      "Epoch 4948, Loss: 0.853726327419281, Final Batch Loss: 0.23925264179706573\n",
      "Epoch 4949, Loss: 0.6944225579500198, Final Batch Loss: 0.13394634425640106\n",
      "Epoch 4950, Loss: 0.7164128124713898, Final Batch Loss: 0.1643276810646057\n",
      "Epoch 4951, Loss: 0.7360150814056396, Final Batch Loss: 0.123787522315979\n",
      "Epoch 4952, Loss: 0.7865827828645706, Final Batch Loss: 0.16704298555850983\n",
      "Epoch 4953, Loss: 0.8200258165597916, Final Batch Loss: 0.17839747667312622\n",
      "Epoch 4954, Loss: 0.8337620347738266, Final Batch Loss: 0.2425716519355774\n",
      "Epoch 4955, Loss: 0.7517705708742142, Final Batch Loss: 0.18920575082302094\n",
      "Epoch 4956, Loss: 0.8483924865722656, Final Batch Loss: 0.19168947637081146\n",
      "Epoch 4957, Loss: 0.7270572260022163, Final Batch Loss: 0.10497038811445236\n",
      "Epoch 4958, Loss: 0.8236077129840851, Final Batch Loss: 0.21180178225040436\n",
      "Epoch 4959, Loss: 0.8509657084941864, Final Batch Loss: 0.2248004823923111\n",
      "Epoch 4960, Loss: 0.856858491897583, Final Batch Loss: 0.26248010993003845\n",
      "Epoch 4961, Loss: 0.8478133827447891, Final Batch Loss: 0.28609776496887207\n",
      "Epoch 4962, Loss: 0.7907672822475433, Final Batch Loss: 0.18077905476093292\n",
      "Epoch 4963, Loss: 0.7995878607034683, Final Batch Loss: 0.18149849772453308\n",
      "Epoch 4964, Loss: 0.8714713007211685, Final Batch Loss: 0.2316710650920868\n",
      "Epoch 4965, Loss: 0.9256910085678101, Final Batch Loss: 0.32166075706481934\n",
      "Epoch 4966, Loss: 0.7275061905384064, Final Batch Loss: 0.1523388922214508\n",
      "Epoch 4967, Loss: 0.8376937359571457, Final Batch Loss: 0.25620198249816895\n",
      "Epoch 4968, Loss: 0.7967369109392166, Final Batch Loss: 0.2559734284877777\n",
      "Epoch 4969, Loss: 0.7293925583362579, Final Batch Loss: 0.15509049594402313\n",
      "Epoch 4970, Loss: 0.7746045887470245, Final Batch Loss: 0.23732230067253113\n",
      "Epoch 4971, Loss: 0.7904343605041504, Final Batch Loss: 0.17679841816425323\n",
      "Epoch 4972, Loss: 0.8691661953926086, Final Batch Loss: 0.17059114575386047\n",
      "Epoch 4973, Loss: 0.7699310779571533, Final Batch Loss: 0.12685781717300415\n",
      "Epoch 4974, Loss: 0.8288682997226715, Final Batch Loss: 0.22091560065746307\n",
      "Epoch 4975, Loss: 0.7896232157945633, Final Batch Loss: 0.27717748284339905\n",
      "Epoch 4976, Loss: 0.7173322886228561, Final Batch Loss: 0.14945343136787415\n",
      "Epoch 4977, Loss: 0.762733206152916, Final Batch Loss: 0.1426602005958557\n",
      "Epoch 4978, Loss: 0.6853898167610168, Final Batch Loss: 0.12483057379722595\n",
      "Epoch 4979, Loss: 0.7614918500185013, Final Batch Loss: 0.1736292988061905\n",
      "Epoch 4980, Loss: 0.8639730662107468, Final Batch Loss: 0.27622538805007935\n",
      "Epoch 4981, Loss: 0.7777810990810394, Final Batch Loss: 0.20067353546619415\n",
      "Epoch 4982, Loss: 0.9507512897253036, Final Batch Loss: 0.2932933270931244\n",
      "Epoch 4983, Loss: 0.9006167054176331, Final Batch Loss: 0.26066094636917114\n",
      "Epoch 4984, Loss: 0.8500801920890808, Final Batch Loss: 0.19977132976055145\n",
      "Epoch 4985, Loss: 0.9377757161855698, Final Batch Loss: 0.2095436453819275\n",
      "Epoch 4986, Loss: 0.7982327491044998, Final Batch Loss: 0.18800190091133118\n",
      "Epoch 4987, Loss: 0.8474168926477432, Final Batch Loss: 0.20530256628990173\n",
      "Epoch 4988, Loss: 0.7828458547592163, Final Batch Loss: 0.20642656087875366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4989, Loss: 0.815289780497551, Final Batch Loss: 0.18744049966335297\n",
      "Epoch 4990, Loss: 0.7996864467859268, Final Batch Loss: 0.18686211109161377\n",
      "Epoch 4991, Loss: 0.727469339966774, Final Batch Loss: 0.1858012080192566\n",
      "Epoch 4992, Loss: 0.7945561707019806, Final Batch Loss: 0.22272811830043793\n",
      "Epoch 4993, Loss: 0.7980935126543045, Final Batch Loss: 0.20293426513671875\n",
      "Epoch 4994, Loss: 0.8530859351158142, Final Batch Loss: 0.27576375007629395\n",
      "Epoch 4995, Loss: 0.6978410929441452, Final Batch Loss: 0.1262092888355255\n",
      "Epoch 4996, Loss: 0.778217002749443, Final Batch Loss: 0.1701742559671402\n",
      "Epoch 4997, Loss: 0.7508836984634399, Final Batch Loss: 0.211943119764328\n",
      "Epoch 4998, Loss: 0.8435530066490173, Final Batch Loss: 0.29746881127357483\n",
      "Epoch 4999, Loss: 0.8038301467895508, Final Batch Loss: 0.20640720427036285\n",
      "Epoch 5000, Loss: 0.7367640882730484, Final Batch Loss: 0.1396598219871521\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  1  2]\n",
      " [ 4 53  5]\n",
      " [ 0  2 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.922     0.940     0.931        50\n",
      "           1      0.946     0.855     0.898        62\n",
      "           2      0.833     0.946     0.886        37\n",
      "\n",
      "    accuracy                          0.906       149\n",
      "   macro avg      0.900     0.914     0.905       149\n",
      "weighted avg      0.910     0.906     0.906       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 User Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
