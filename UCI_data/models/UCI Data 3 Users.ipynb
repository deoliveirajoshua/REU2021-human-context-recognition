{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '484 fBodyGyro-bandsEnergy()-17,32',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = len(act_features) + len(sub_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_features = ['58 tGravityAcc-energy()-Y', '59 tGravityAcc-energy()-Z', '104 tBodyAccJerk-entropy()-Y', '125 tBodyGyro-std()-Y',\n",
    "#  '128 tBodyGyro-mad()-Y', '132 tBodyGyro-max()-Z', '134 tBodyGyro-min()-Y','138 tBodyGyro-energy()-Y', '141 tBodyGyro-iqr()-Y',\n",
    "#  '167 tBodyGyroJerk-mad()-X','168 tBodyGyroJerk-mad()-Y','177 tBodyGyroJerk-energy()-X', '181 tBodyGyroJerk-iqr()-Y',\n",
    "#  '475 fBodyGyro-bandsEnergy()-1,8', '484 fBodyGyro-bandsEnergy()-17,32','487 fBodyGyro-bandsEnergy()-1,24']\n",
    "\n",
    "# act_features = ['4 tBodyAcc-std()-X', '7 tBodyAcc-mad()-X', '10 tBodyAcc-max()-X', '17 tBodyAcc-energy()-X', '202 tBodyAccMag-std()',\n",
    "#  '204 tBodyAccMag-max()', '215 tGravityAccMag-std()', '217 tGravityAccMag-max()', '269 fBodyAcc-std()-X', '275 fBodyAcc-max()-X',\n",
    "#  '282 fBodyAcc-energy()-X', '286 fBodyAcc-iqr()-Y', '303 fBodyAcc-bandsEnergy()-1,8', '315 fBodyAcc-bandsEnergy()-1,24',\n",
    "#  '368 fBodyAccJerk-entropy()-Y', '390 fBodyAccJerk-bandsEnergy()-1,16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  125 tBodyGyro-std()-Y  ...  \\\n",
       "0                     -0.975510              -0.976623  ...   \n",
       "1                     -0.978500              -0.989046  ...   \n",
       "2                     -0.981672              -0.993552  ...   \n",
       "3                     -0.982420              -0.992407  ...   \n",
       "4                     -0.984363              -0.992378  ...   \n",
       "...                         ...                    ...  ...   \n",
       "7347                  -0.995193               0.084878  ...   \n",
       "7348                  -0.995151               0.098249  ...   \n",
       "7349                  -0.995450               0.185902  ...   \n",
       "7350                  -0.998824               0.190360  ...   \n",
       "7351                  -0.998144               0.022216  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  506 fBodyAccMag-max()  509 fBodyAccMag-energy()  \\\n",
       "0                 -0.948870              -0.974321                 -0.998285   \n",
       "1                 -0.975777              -0.978226                 -0.999472   \n",
       "2                 -0.985594              -0.993062                 -0.999807   \n",
       "3                 -0.983524              -0.990230                 -0.999770   \n",
       "4                 -0.992324              -0.990506                 -0.999873   \n",
       "...                     ...                    ...                       ...   \n",
       "7347              -0.007392              -0.401674                 -0.584282   \n",
       "7348              -0.172448              -0.410577                 -0.632536   \n",
       "7349              -0.216074              -0.362904                 -0.641170   \n",
       "7350              -0.220443              -0.397687                 -0.663579   \n",
       "7351              -0.146649              -0.620014                 -0.698087   \n",
       "\n",
       "      Subject  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "7347       30  \n",
       "7348       30  \n",
       "7349       30  \n",
       "7350       30  \n",
       "7351       30  \n",
       "\n",
       "[7352 rows x 46 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[(X_train['Subject'] == 1) | (X_train['Subject'] == 3) | (X_train['Subject'] == 5)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Subject'] == 1) | (y_train['Subject'] == 3) | (y_train['Subject'] == 5)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.3997204303741455, Final Batch Loss: 1.0972342491149902\n",
      "Epoch 2, Loss: 4.381514191627502, Final Batch Loss: 1.0924073457717896\n",
      "Epoch 3, Loss: 4.378718852996826, Final Batch Loss: 1.1029056310653687\n",
      "Epoch 4, Loss: 4.365106463432312, Final Batch Loss: 1.093891978263855\n",
      "Epoch 5, Loss: 4.346585988998413, Final Batch Loss: 1.0894182920455933\n",
      "Epoch 6, Loss: 4.334478735923767, Final Batch Loss: 1.0815176963806152\n",
      "Epoch 7, Loss: 4.304195880889893, Final Batch Loss: 1.0701406002044678\n",
      "Epoch 8, Loss: 4.303332209587097, Final Batch Loss: 1.0701162815093994\n",
      "Epoch 9, Loss: 4.275639772415161, Final Batch Loss: 1.0689666271209717\n",
      "Epoch 10, Loss: 4.2488096952438354, Final Batch Loss: 1.0608785152435303\n",
      "Epoch 11, Loss: 4.232369065284729, Final Batch Loss: 1.057252287864685\n",
      "Epoch 12, Loss: 4.200371742248535, Final Batch Loss: 1.0480458736419678\n",
      "Epoch 13, Loss: 4.165109276771545, Final Batch Loss: 1.045552134513855\n",
      "Epoch 14, Loss: 4.116541266441345, Final Batch Loss: 1.029228925704956\n",
      "Epoch 15, Loss: 4.0449042320251465, Final Batch Loss: 1.0049128532409668\n",
      "Epoch 16, Loss: 4.013120412826538, Final Batch Loss: 1.013921856880188\n",
      "Epoch 17, Loss: 3.9197006225585938, Final Batch Loss: 0.9572170376777649\n",
      "Epoch 18, Loss: 3.883915066719055, Final Batch Loss: 0.9798592329025269\n",
      "Epoch 19, Loss: 3.741716504096985, Final Batch Loss: 0.901019811630249\n",
      "Epoch 20, Loss: 3.7181772589683533, Final Batch Loss: 0.9252269268035889\n",
      "Epoch 21, Loss: 3.613045036792755, Final Batch Loss: 0.8682605028152466\n",
      "Epoch 22, Loss: 3.516598880290985, Final Batch Loss: 0.9069092273712158\n",
      "Epoch 23, Loss: 3.4510286450386047, Final Batch Loss: 0.8359272480010986\n",
      "Epoch 24, Loss: 3.3967880606651306, Final Batch Loss: 0.8233110904693604\n",
      "Epoch 25, Loss: 3.3132545351982117, Final Batch Loss: 0.850413978099823\n",
      "Epoch 26, Loss: 3.2102442383766174, Final Batch Loss: 0.7796175479888916\n",
      "Epoch 27, Loss: 3.256349265575409, Final Batch Loss: 0.8628979325294495\n",
      "Epoch 28, Loss: 3.1551859974861145, Final Batch Loss: 0.7952399849891663\n",
      "Epoch 29, Loss: 2.9798161387443542, Final Batch Loss: 0.6937149167060852\n",
      "Epoch 30, Loss: 3.0683984756469727, Final Batch Loss: 0.7529885172843933\n",
      "Epoch 31, Loss: 2.995351791381836, Final Batch Loss: 0.7768137454986572\n",
      "Epoch 32, Loss: 2.9573797583580017, Final Batch Loss: 0.7383311986923218\n",
      "Epoch 33, Loss: 2.8758243322372437, Final Batch Loss: 0.6903422474861145\n",
      "Epoch 34, Loss: 2.853435754776001, Final Batch Loss: 0.7681166529655457\n",
      "Epoch 35, Loss: 2.831031382083893, Final Batch Loss: 0.7293251752853394\n",
      "Epoch 36, Loss: 2.8212019205093384, Final Batch Loss: 0.721259593963623\n",
      "Epoch 37, Loss: 2.7882760763168335, Final Batch Loss: 0.7765600085258484\n",
      "Epoch 38, Loss: 2.6020031571388245, Final Batch Loss: 0.5950687527656555\n",
      "Epoch 39, Loss: 2.7280152440071106, Final Batch Loss: 0.7934688329696655\n",
      "Epoch 40, Loss: 2.5828118920326233, Final Batch Loss: 0.6743324398994446\n",
      "Epoch 41, Loss: 2.5915857553482056, Final Batch Loss: 0.7121022939682007\n",
      "Epoch 42, Loss: 2.570396602153778, Final Batch Loss: 0.6561819911003113\n",
      "Epoch 43, Loss: 2.498329281806946, Final Batch Loss: 0.6768888831138611\n",
      "Epoch 44, Loss: 2.389142632484436, Final Batch Loss: 0.5924857258796692\n",
      "Epoch 45, Loss: 2.429215669631958, Final Batch Loss: 0.6642552614212036\n",
      "Epoch 46, Loss: 2.426007330417633, Final Batch Loss: 0.5889742374420166\n",
      "Epoch 47, Loss: 2.304149627685547, Final Batch Loss: 0.5393454432487488\n",
      "Epoch 48, Loss: 2.3305447101593018, Final Batch Loss: 0.5587419867515564\n",
      "Epoch 49, Loss: 2.274363160133362, Final Batch Loss: 0.5633400678634644\n",
      "Epoch 50, Loss: 2.2378268241882324, Final Batch Loss: 0.5133271813392639\n",
      "Epoch 51, Loss: 2.241777718067169, Final Batch Loss: 0.6435760855674744\n",
      "Epoch 52, Loss: 2.2753191590309143, Final Batch Loss: 0.6415665745735168\n",
      "Epoch 53, Loss: 2.092597156763077, Final Batch Loss: 0.43785521388053894\n",
      "Epoch 54, Loss: 2.064545691013336, Final Batch Loss: 0.5027939081192017\n",
      "Epoch 55, Loss: 2.193570554256439, Final Batch Loss: 0.5425885915756226\n",
      "Epoch 56, Loss: 2.0365524291992188, Final Batch Loss: 0.44615501165390015\n",
      "Epoch 57, Loss: 2.0528460144996643, Final Batch Loss: 0.5238553285598755\n",
      "Epoch 58, Loss: 2.1109659075737, Final Batch Loss: 0.5663436055183411\n",
      "Epoch 59, Loss: 1.9800948202610016, Final Batch Loss: 0.4139348566532135\n",
      "Epoch 60, Loss: 2.0270599722862244, Final Batch Loss: 0.4893871247768402\n",
      "Epoch 61, Loss: 2.0609201192855835, Final Batch Loss: 0.5127649307250977\n",
      "Epoch 62, Loss: 2.128447473049164, Final Batch Loss: 0.5809104442596436\n",
      "Epoch 63, Loss: 1.954404205083847, Final Batch Loss: 0.4776177406311035\n",
      "Epoch 64, Loss: 1.9983075857162476, Final Batch Loss: 0.5114502310752869\n",
      "Epoch 65, Loss: 1.998285859823227, Final Batch Loss: 0.47384384274482727\n",
      "Epoch 66, Loss: 1.9597178101539612, Final Batch Loss: 0.543940007686615\n",
      "Epoch 67, Loss: 1.9859926998615265, Final Batch Loss: 0.5162376165390015\n",
      "Epoch 68, Loss: 2.0153102576732635, Final Batch Loss: 0.6131289601325989\n",
      "Epoch 69, Loss: 1.8850785195827484, Final Batch Loss: 0.5034845471382141\n",
      "Epoch 70, Loss: 1.9299480617046356, Final Batch Loss: 0.4752379059791565\n",
      "Epoch 71, Loss: 1.837881475687027, Final Batch Loss: 0.44304874539375305\n",
      "Epoch 72, Loss: 1.7591338157653809, Final Batch Loss: 0.36135441064834595\n",
      "Epoch 73, Loss: 1.8651774227619171, Final Batch Loss: 0.4529476761817932\n",
      "Epoch 74, Loss: 1.8567612767219543, Final Batch Loss: 0.4581097662448883\n",
      "Epoch 75, Loss: 1.8159759044647217, Final Batch Loss: 0.43220755457878113\n",
      "Epoch 76, Loss: 1.7252509593963623, Final Batch Loss: 0.4377283751964569\n",
      "Epoch 77, Loss: 1.7985013127326965, Final Batch Loss: 0.44949498772621155\n",
      "Epoch 78, Loss: 1.7931990027427673, Final Batch Loss: 0.5180167555809021\n",
      "Epoch 79, Loss: 1.6826278865337372, Final Batch Loss: 0.3044840693473816\n",
      "Epoch 80, Loss: 1.7838527858257294, Final Batch Loss: 0.4915296137332916\n",
      "Epoch 81, Loss: 1.7437436282634735, Final Batch Loss: 0.422995001077652\n",
      "Epoch 82, Loss: 1.7513705492019653, Final Batch Loss: 0.36289170384407043\n",
      "Epoch 83, Loss: 1.7354162335395813, Final Batch Loss: 0.43472251296043396\n",
      "Epoch 84, Loss: 1.716351956129074, Final Batch Loss: 0.4122109115123749\n",
      "Epoch 85, Loss: 1.6640265583992004, Final Batch Loss: 0.35275936126708984\n",
      "Epoch 86, Loss: 1.7797946333885193, Final Batch Loss: 0.5157872438430786\n",
      "Epoch 87, Loss: 1.792375773191452, Final Batch Loss: 0.5628506541252136\n",
      "Epoch 88, Loss: 1.6235538721084595, Final Batch Loss: 0.35667046904563904\n",
      "Epoch 89, Loss: 1.7013031244277954, Final Batch Loss: 0.37500855326652527\n",
      "Epoch 90, Loss: 1.6392386257648468, Final Batch Loss: 0.36903178691864014\n",
      "Epoch 91, Loss: 1.6860734224319458, Final Batch Loss: 0.4318757653236389\n",
      "Epoch 92, Loss: 1.7190389931201935, Final Batch Loss: 0.42026805877685547\n",
      "Epoch 93, Loss: 1.706208884716034, Final Batch Loss: 0.47519075870513916\n",
      "Epoch 94, Loss: 1.739884227514267, Final Batch Loss: 0.5230914950370789\n",
      "Epoch 95, Loss: 1.7517583668231964, Final Batch Loss: 0.48483195900917053\n",
      "Epoch 96, Loss: 1.552809238433838, Final Batch Loss: 0.3535921275615692\n",
      "Epoch 97, Loss: 1.5921313762664795, Final Batch Loss: 0.361232727766037\n",
      "Epoch 98, Loss: 1.68509241938591, Final Batch Loss: 0.443035364151001\n",
      "Epoch 99, Loss: 1.6240026652812958, Final Batch Loss: 0.3480088710784912\n",
      "Epoch 100, Loss: 1.7199211120605469, Final Batch Loss: 0.4928973615169525\n",
      "Epoch 101, Loss: 1.5925729274749756, Final Batch Loss: 0.39125770330429077\n",
      "Epoch 102, Loss: 1.6320077180862427, Final Batch Loss: 0.43839502334594727\n",
      "Epoch 103, Loss: 1.5666764974594116, Final Batch Loss: 0.4023391902446747\n",
      "Epoch 104, Loss: 1.5567368268966675, Final Batch Loss: 0.3337160050868988\n",
      "Epoch 105, Loss: 1.546332448720932, Final Batch Loss: 0.352195143699646\n",
      "Epoch 106, Loss: 1.640324741601944, Final Batch Loss: 0.5047140717506409\n",
      "Epoch 107, Loss: 1.5502400398254395, Final Batch Loss: 0.3427090346813202\n",
      "Epoch 108, Loss: 1.7002234756946564, Final Batch Loss: 0.5496242046356201\n",
      "Epoch 109, Loss: 1.5758760869503021, Final Batch Loss: 0.4278429448604584\n",
      "Epoch 110, Loss: 1.5030615031719208, Final Batch Loss: 0.3412187397480011\n",
      "Epoch 111, Loss: 1.5676213800907135, Final Batch Loss: 0.43852710723876953\n",
      "Epoch 112, Loss: 1.6828173696994781, Final Batch Loss: 0.4855126142501831\n",
      "Epoch 113, Loss: 1.5552455484867096, Final Batch Loss: 0.41736871004104614\n",
      "Epoch 114, Loss: 1.5761324763298035, Final Batch Loss: 0.45654821395874023\n",
      "Epoch 115, Loss: 1.5214591026306152, Final Batch Loss: 0.3367515504360199\n",
      "Epoch 116, Loss: 1.5511102378368378, Final Batch Loss: 0.39848777651786804\n",
      "Epoch 117, Loss: 1.5644451975822449, Final Batch Loss: 0.4331139922142029\n",
      "Epoch 118, Loss: 1.546576738357544, Final Batch Loss: 0.4463110566139221\n",
      "Epoch 119, Loss: 1.4862610399723053, Final Batch Loss: 0.346625953912735\n",
      "Epoch 120, Loss: 1.4190987646579742, Final Batch Loss: 0.29213181138038635\n",
      "Epoch 121, Loss: 1.4490772783756256, Final Batch Loss: 0.332480788230896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122, Loss: 1.43062362074852, Final Batch Loss: 0.36402657628059387\n",
      "Epoch 123, Loss: 1.5153269171714783, Final Batch Loss: 0.44022050499916077\n",
      "Epoch 124, Loss: 1.418712317943573, Final Batch Loss: 0.3013910949230194\n",
      "Epoch 125, Loss: 1.5400589406490326, Final Batch Loss: 0.40896740555763245\n",
      "Epoch 126, Loss: 1.4849500060081482, Final Batch Loss: 0.3775028586387634\n",
      "Epoch 127, Loss: 1.5101602673530579, Final Batch Loss: 0.4206604063510895\n",
      "Epoch 128, Loss: 1.5099565088748932, Final Batch Loss: 0.3792424499988556\n",
      "Epoch 129, Loss: 1.4042828381061554, Final Batch Loss: 0.3427603840827942\n",
      "Epoch 130, Loss: 1.3664564490318298, Final Batch Loss: 0.26675182580947876\n",
      "Epoch 131, Loss: 1.4281480610370636, Final Batch Loss: 0.33807799220085144\n",
      "Epoch 132, Loss: 1.4432835578918457, Final Batch Loss: 0.390703022480011\n",
      "Epoch 133, Loss: 1.4089800715446472, Final Batch Loss: 0.34204909205436707\n",
      "Epoch 134, Loss: 1.4980100691318512, Final Batch Loss: 0.41976198554039\n",
      "Epoch 135, Loss: 1.3909820914268494, Final Batch Loss: 0.330196350812912\n",
      "Epoch 136, Loss: 1.3292700052261353, Final Batch Loss: 0.26239433884620667\n",
      "Epoch 137, Loss: 1.3461699187755585, Final Batch Loss: 0.23601502180099487\n",
      "Epoch 138, Loss: 1.3837749361991882, Final Batch Loss: 0.3104384243488312\n",
      "Epoch 139, Loss: 1.5540747046470642, Final Batch Loss: 0.4682053327560425\n",
      "Epoch 140, Loss: 1.3728009164333344, Final Batch Loss: 0.3116506338119507\n",
      "Epoch 141, Loss: 1.3893739879131317, Final Batch Loss: 0.3140771985054016\n",
      "Epoch 142, Loss: 1.4149818122386932, Final Batch Loss: 0.34192168712615967\n",
      "Epoch 143, Loss: 1.400914490222931, Final Batch Loss: 0.32843828201293945\n",
      "Epoch 144, Loss: 1.4959852993488312, Final Batch Loss: 0.44458407163619995\n",
      "Epoch 145, Loss: 1.4014171361923218, Final Batch Loss: 0.3578755855560303\n",
      "Epoch 146, Loss: 1.4632704555988312, Final Batch Loss: 0.39764562249183655\n",
      "Epoch 147, Loss: 1.478231132030487, Final Batch Loss: 0.4167994260787964\n",
      "Epoch 148, Loss: 1.4262084066867828, Final Batch Loss: 0.36185386776924133\n",
      "Epoch 149, Loss: 1.3786774575710297, Final Batch Loss: 0.2876257300376892\n",
      "Epoch 150, Loss: 1.4152542650699615, Final Batch Loss: 0.46836552023887634\n",
      "Epoch 151, Loss: 1.3355126976966858, Final Batch Loss: 0.28952622413635254\n",
      "Epoch 152, Loss: 1.3767998218536377, Final Batch Loss: 0.3444097340106964\n",
      "Epoch 153, Loss: 1.4123554825782776, Final Batch Loss: 0.37716713547706604\n",
      "Epoch 154, Loss: 1.3524922728538513, Final Batch Loss: 0.3325958847999573\n",
      "Epoch 155, Loss: 1.3546806871891022, Final Batch Loss: 0.3342249393463135\n",
      "Epoch 156, Loss: 1.3634913265705109, Final Batch Loss: 0.3461267352104187\n",
      "Epoch 157, Loss: 1.4053010940551758, Final Batch Loss: 0.41454923152923584\n",
      "Epoch 158, Loss: 1.320843368768692, Final Batch Loss: 0.3302902281284332\n",
      "Epoch 159, Loss: 1.3100471496582031, Final Batch Loss: 0.32022953033447266\n",
      "Epoch 160, Loss: 1.2814057767391205, Final Batch Loss: 0.2950144410133362\n",
      "Epoch 161, Loss: 1.3866400718688965, Final Batch Loss: 0.4212724566459656\n",
      "Epoch 162, Loss: 1.3612129390239716, Final Batch Loss: 0.3125873804092407\n",
      "Epoch 163, Loss: 1.3392892181873322, Final Batch Loss: 0.3502846658229828\n",
      "Epoch 164, Loss: 1.2995527684688568, Final Batch Loss: 0.27770528197288513\n",
      "Epoch 165, Loss: 1.3937009572982788, Final Batch Loss: 0.3935488164424896\n",
      "Epoch 166, Loss: 1.2991093695163727, Final Batch Loss: 0.26927441358566284\n",
      "Epoch 167, Loss: 1.4326357245445251, Final Batch Loss: 0.41139960289001465\n",
      "Epoch 168, Loss: 1.3337611258029938, Final Batch Loss: 0.35862213373184204\n",
      "Epoch 169, Loss: 1.303463488817215, Final Batch Loss: 0.34071120619773865\n",
      "Epoch 170, Loss: 1.2924568355083466, Final Batch Loss: 0.30647265911102295\n",
      "Epoch 171, Loss: 1.3062970042228699, Final Batch Loss: 0.377794086933136\n",
      "Epoch 172, Loss: 1.2790310382843018, Final Batch Loss: 0.3686554729938507\n",
      "Epoch 173, Loss: 1.3286937177181244, Final Batch Loss: 0.30953580141067505\n",
      "Epoch 174, Loss: 1.258559226989746, Final Batch Loss: 0.24960729479789734\n",
      "Epoch 175, Loss: 1.3819648325443268, Final Batch Loss: 0.4148831069469452\n",
      "Epoch 176, Loss: 1.28351029753685, Final Batch Loss: 0.31635287404060364\n",
      "Epoch 177, Loss: 1.3078962862491608, Final Batch Loss: 0.4064248502254486\n",
      "Epoch 178, Loss: 1.2575056552886963, Final Batch Loss: 0.34322354197502136\n",
      "Epoch 179, Loss: 1.3436202108860016, Final Batch Loss: 0.3901253640651703\n",
      "Epoch 180, Loss: 1.2317872047424316, Final Batch Loss: 0.2729046046733856\n",
      "Epoch 181, Loss: 1.20614393055439, Final Batch Loss: 0.23830486834049225\n",
      "Epoch 182, Loss: 1.3032035827636719, Final Batch Loss: 0.36823394894599915\n",
      "Epoch 183, Loss: 1.3128419518470764, Final Batch Loss: 0.38025230169296265\n",
      "Epoch 184, Loss: 1.246461272239685, Final Batch Loss: 0.2662017047405243\n",
      "Epoch 185, Loss: 1.2500827610492706, Final Batch Loss: 0.2909509837627411\n",
      "Epoch 186, Loss: 1.202203318476677, Final Batch Loss: 0.2435794621706009\n",
      "Epoch 187, Loss: 1.2492465674877167, Final Batch Loss: 0.34089159965515137\n",
      "Epoch 188, Loss: 1.2716677635908127, Final Batch Loss: 0.23625080287456512\n",
      "Epoch 189, Loss: 1.3557437658309937, Final Batch Loss: 0.4025840759277344\n",
      "Epoch 190, Loss: 1.2362191379070282, Final Batch Loss: 0.2919779121875763\n",
      "Epoch 191, Loss: 1.3128821849822998, Final Batch Loss: 0.4000365436077118\n",
      "Epoch 192, Loss: 1.1971688866615295, Final Batch Loss: 0.2752749025821686\n",
      "Epoch 193, Loss: 1.2613431215286255, Final Batch Loss: 0.2888604998588562\n",
      "Epoch 194, Loss: 1.2517715096473694, Final Batch Loss: 0.35453662276268005\n",
      "Epoch 195, Loss: 1.2252840101718903, Final Batch Loss: 0.30633604526519775\n",
      "Epoch 196, Loss: 1.2503086626529694, Final Batch Loss: 0.2893638014793396\n",
      "Epoch 197, Loss: 1.2037261426448822, Final Batch Loss: 0.29991284012794495\n",
      "Epoch 198, Loss: 1.277537763118744, Final Batch Loss: 0.35959523916244507\n",
      "Epoch 199, Loss: 1.1379902958869934, Final Batch Loss: 0.24902412295341492\n",
      "Epoch 200, Loss: 1.2646830379962921, Final Batch Loss: 0.3700084090232849\n",
      "Epoch 201, Loss: 1.266601949930191, Final Batch Loss: 0.3271614611148834\n",
      "Epoch 202, Loss: 1.2139799892902374, Final Batch Loss: 0.35388463735580444\n",
      "Epoch 203, Loss: 1.2349747717380524, Final Batch Loss: 0.3268926739692688\n",
      "Epoch 204, Loss: 1.260115146636963, Final Batch Loss: 0.3079189360141754\n",
      "Epoch 205, Loss: 1.2128299176692963, Final Batch Loss: 0.2776767313480377\n",
      "Epoch 206, Loss: 1.2929557859897614, Final Batch Loss: 0.3322334289550781\n",
      "Epoch 207, Loss: 1.1817451417446136, Final Batch Loss: 0.2978366017341614\n",
      "Epoch 208, Loss: 1.2260279059410095, Final Batch Loss: 0.3359473645687103\n",
      "Epoch 209, Loss: 1.1534007787704468, Final Batch Loss: 0.25335073471069336\n",
      "Epoch 210, Loss: 1.2234185338020325, Final Batch Loss: 0.33294442296028137\n",
      "Epoch 211, Loss: 1.1424520611763, Final Batch Loss: 0.2367338240146637\n",
      "Epoch 212, Loss: 1.2446674406528473, Final Batch Loss: 0.4143664538860321\n",
      "Epoch 213, Loss: 1.1558547616004944, Final Batch Loss: 0.2452927827835083\n",
      "Epoch 214, Loss: 1.152267962694168, Final Batch Loss: 0.2964824140071869\n",
      "Epoch 215, Loss: 1.1334372460842133, Final Batch Loss: 0.2818444073200226\n",
      "Epoch 216, Loss: 1.1001932471990585, Final Batch Loss: 0.26917028427124023\n",
      "Epoch 217, Loss: 1.1832107901573181, Final Batch Loss: 0.2336655855178833\n",
      "Epoch 218, Loss: 1.1235233843326569, Final Batch Loss: 0.2707602381706238\n",
      "Epoch 219, Loss: 1.1638324856758118, Final Batch Loss: 0.28414061665534973\n",
      "Epoch 220, Loss: 1.2287292182445526, Final Batch Loss: 0.29332372546195984\n",
      "Epoch 221, Loss: 1.188293993473053, Final Batch Loss: 0.30227968096733093\n",
      "Epoch 222, Loss: 1.2004557847976685, Final Batch Loss: 0.3161357045173645\n",
      "Epoch 223, Loss: 1.1856001615524292, Final Batch Loss: 0.31896138191223145\n",
      "Epoch 224, Loss: 1.1932582706212997, Final Batch Loss: 0.36954259872436523\n",
      "Epoch 225, Loss: 1.1410637199878693, Final Batch Loss: 0.2886449098587036\n",
      "Epoch 226, Loss: 1.0719442069530487, Final Batch Loss: 0.2546417713165283\n",
      "Epoch 227, Loss: 1.1187549531459808, Final Batch Loss: 0.27433961629867554\n",
      "Epoch 228, Loss: 1.1721239537000656, Final Batch Loss: 0.21925745904445648\n",
      "Epoch 229, Loss: 1.1331183463335037, Final Batch Loss: 0.30237242579460144\n",
      "Epoch 230, Loss: 1.2707097977399826, Final Batch Loss: 0.42995548248291016\n",
      "Epoch 231, Loss: 1.159522384405136, Final Batch Loss: 0.3011399507522583\n",
      "Epoch 232, Loss: 1.1090909093618393, Final Batch Loss: 0.28031909465789795\n",
      "Epoch 233, Loss: 1.077560007572174, Final Batch Loss: 0.24247968196868896\n",
      "Epoch 234, Loss: 1.097033903002739, Final Batch Loss: 0.28640514612197876\n",
      "Epoch 235, Loss: 1.1162933111190796, Final Batch Loss: 0.23839032649993896\n",
      "Epoch 236, Loss: 1.1493263840675354, Final Batch Loss: 0.29114943742752075\n",
      "Epoch 237, Loss: 1.0496565699577332, Final Batch Loss: 0.21031363308429718\n",
      "Epoch 238, Loss: 1.065893217921257, Final Batch Loss: 0.26381441950798035\n",
      "Epoch 239, Loss: 1.1151150315999985, Final Batch Loss: 0.23776264488697052\n",
      "Epoch 240, Loss: 1.1246265470981598, Final Batch Loss: 0.2928317189216614\n",
      "Epoch 241, Loss: 1.0851495862007141, Final Batch Loss: 0.24900534749031067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242, Loss: 1.0981028378009796, Final Batch Loss: 0.2859252989292145\n",
      "Epoch 243, Loss: 1.035250410437584, Final Batch Loss: 0.21909651160240173\n",
      "Epoch 244, Loss: 1.083603635430336, Final Batch Loss: 0.261148601770401\n",
      "Epoch 245, Loss: 1.0601800084114075, Final Batch Loss: 0.2717747390270233\n",
      "Epoch 246, Loss: 1.0194407552480698, Final Batch Loss: 0.22062383592128754\n",
      "Epoch 247, Loss: 1.0917380154132843, Final Batch Loss: 0.2578723132610321\n",
      "Epoch 248, Loss: 1.0803286731243134, Final Batch Loss: 0.28041040897369385\n",
      "Epoch 249, Loss: 1.0773170590400696, Final Batch Loss: 0.2778557240962982\n",
      "Epoch 250, Loss: 1.0156473219394684, Final Batch Loss: 0.21018394827842712\n",
      "Epoch 251, Loss: 1.1178608238697052, Final Batch Loss: 0.2949255704879761\n",
      "Epoch 252, Loss: 1.0370043069124222, Final Batch Loss: 0.2191426008939743\n",
      "Epoch 253, Loss: 1.018190711736679, Final Batch Loss: 0.22200115025043488\n",
      "Epoch 254, Loss: 1.1462814509868622, Final Batch Loss: 0.2991639971733093\n",
      "Epoch 255, Loss: 1.1205615550279617, Final Batch Loss: 0.25169843435287476\n",
      "Epoch 256, Loss: 1.1022336035966873, Final Batch Loss: 0.2782669961452484\n",
      "Epoch 257, Loss: 1.0242115259170532, Final Batch Loss: 0.24715660512447357\n",
      "Epoch 258, Loss: 1.1067753732204437, Final Batch Loss: 0.31659752130508423\n",
      "Epoch 259, Loss: 1.1936180591583252, Final Batch Loss: 0.38837677240371704\n",
      "Epoch 260, Loss: 1.0650067180395126, Final Batch Loss: 0.2650052607059479\n",
      "Epoch 261, Loss: 1.0705029517412186, Final Batch Loss: 0.31480544805526733\n",
      "Epoch 262, Loss: 1.000318005681038, Final Batch Loss: 0.2392396181821823\n",
      "Epoch 263, Loss: 1.0514344274997711, Final Batch Loss: 0.2634788453578949\n",
      "Epoch 264, Loss: 1.0291778445243835, Final Batch Loss: 0.26362746953964233\n",
      "Epoch 265, Loss: 1.001497521996498, Final Batch Loss: 0.23614110052585602\n",
      "Epoch 266, Loss: 1.0228515714406967, Final Batch Loss: 0.24794094264507294\n",
      "Epoch 267, Loss: 1.1534245908260345, Final Batch Loss: 0.38043561577796936\n",
      "Epoch 268, Loss: 1.0898035168647766, Final Batch Loss: 0.2923392951488495\n",
      "Epoch 269, Loss: 1.0610071867704391, Final Batch Loss: 0.30366331338882446\n",
      "Epoch 270, Loss: 1.0176853090524673, Final Batch Loss: 0.3100583851337433\n",
      "Epoch 271, Loss: 1.0963388234376907, Final Batch Loss: 0.3155445456504822\n",
      "Epoch 272, Loss: 1.0218678563833237, Final Batch Loss: 0.2757290303707123\n",
      "Epoch 273, Loss: 1.151323601603508, Final Batch Loss: 0.3952105939388275\n",
      "Epoch 274, Loss: 1.1266902685165405, Final Batch Loss: 0.3449821174144745\n",
      "Epoch 275, Loss: 1.0158932209014893, Final Batch Loss: 0.2469889223575592\n",
      "Epoch 276, Loss: 1.0215031802654266, Final Batch Loss: 0.22935621440410614\n",
      "Epoch 277, Loss: 1.0223847329616547, Final Batch Loss: 0.29691749811172485\n",
      "Epoch 278, Loss: 1.058308258652687, Final Batch Loss: 0.28056302666664124\n",
      "Epoch 279, Loss: 1.0445299446582794, Final Batch Loss: 0.2775244414806366\n",
      "Epoch 280, Loss: 0.951638862490654, Final Batch Loss: 0.20584288239479065\n",
      "Epoch 281, Loss: 0.9696625769138336, Final Batch Loss: 0.21631920337677002\n",
      "Epoch 282, Loss: 0.9576517790555954, Final Batch Loss: 0.214176744222641\n",
      "Epoch 283, Loss: 0.9971944391727448, Final Batch Loss: 0.20226725935935974\n",
      "Epoch 284, Loss: 1.0083890706300735, Final Batch Loss: 0.2525811791419983\n",
      "Epoch 285, Loss: 0.9673680514097214, Final Batch Loss: 0.170333132147789\n",
      "Epoch 286, Loss: 0.9706173986196518, Final Batch Loss: 0.23960258066654205\n",
      "Epoch 287, Loss: 0.9680750966072083, Final Batch Loss: 0.23433655500411987\n",
      "Epoch 288, Loss: 0.9797855615615845, Final Batch Loss: 0.18422609567642212\n",
      "Epoch 289, Loss: 1.0026102513074875, Final Batch Loss: 0.23790431022644043\n",
      "Epoch 290, Loss: 1.0258450359106064, Final Batch Loss: 0.3156988024711609\n",
      "Epoch 291, Loss: 0.9605105668306351, Final Batch Loss: 0.22788217663764954\n",
      "Epoch 292, Loss: 1.1100279837846756, Final Batch Loss: 0.35749444365501404\n",
      "Epoch 293, Loss: 0.9012866765260696, Final Batch Loss: 0.18645219504833221\n",
      "Epoch 294, Loss: 0.9026162773370743, Final Batch Loss: 0.1948567032814026\n",
      "Epoch 295, Loss: 0.8754224926233292, Final Batch Loss: 0.18217577040195465\n",
      "Epoch 296, Loss: 0.8730141222476959, Final Batch Loss: 0.19048066437244415\n",
      "Epoch 297, Loss: 1.0307931452989578, Final Batch Loss: 0.28916090726852417\n",
      "Epoch 298, Loss: 0.9461740553379059, Final Batch Loss: 0.26092448830604553\n",
      "Epoch 299, Loss: 1.0341912358999252, Final Batch Loss: 0.26689818501472473\n",
      "Epoch 300, Loss: 0.944413885474205, Final Batch Loss: 0.18175417184829712\n",
      "Epoch 301, Loss: 0.9613313972949982, Final Batch Loss: 0.23307554423809052\n",
      "Epoch 302, Loss: 0.9822368770837784, Final Batch Loss: 0.24047210812568665\n",
      "Epoch 303, Loss: 0.9162078648805618, Final Batch Loss: 0.1760408580303192\n",
      "Epoch 304, Loss: 1.008846640586853, Final Batch Loss: 0.3429602384567261\n",
      "Epoch 305, Loss: 0.8870895206928253, Final Batch Loss: 0.1383666694164276\n",
      "Epoch 306, Loss: 1.052850753068924, Final Batch Loss: 0.2809849977493286\n",
      "Epoch 307, Loss: 0.9506845027208328, Final Batch Loss: 0.2242973893880844\n",
      "Epoch 308, Loss: 0.9496724456548691, Final Batch Loss: 0.23292496800422668\n",
      "Epoch 309, Loss: 0.979415625333786, Final Batch Loss: 0.28842970728874207\n",
      "Epoch 310, Loss: 0.97926464676857, Final Batch Loss: 0.2594074010848999\n",
      "Epoch 311, Loss: 1.0433918088674545, Final Batch Loss: 0.2893836200237274\n",
      "Epoch 312, Loss: 0.8737711906433105, Final Batch Loss: 0.1739371120929718\n",
      "Epoch 313, Loss: 0.9755450934171677, Final Batch Loss: 0.24000869691371918\n",
      "Epoch 314, Loss: 0.9670660644769669, Final Batch Loss: 0.2518608272075653\n",
      "Epoch 315, Loss: 0.8869069516658783, Final Batch Loss: 0.22033138573169708\n",
      "Epoch 316, Loss: 0.9272172600030899, Final Batch Loss: 0.2431468814611435\n",
      "Epoch 317, Loss: 0.9070088565349579, Final Batch Loss: 0.1905490607023239\n",
      "Epoch 318, Loss: 0.9007561802864075, Final Batch Loss: 0.21645019948482513\n",
      "Epoch 319, Loss: 0.9044815599918365, Final Batch Loss: 0.2398429661989212\n",
      "Epoch 320, Loss: 1.0164049714803696, Final Batch Loss: 0.2321169674396515\n",
      "Epoch 321, Loss: 0.8702425956726074, Final Batch Loss: 0.2334849238395691\n",
      "Epoch 322, Loss: 0.9443406611680984, Final Batch Loss: 0.2971693277359009\n",
      "Epoch 323, Loss: 0.9139763861894608, Final Batch Loss: 0.22213347256183624\n",
      "Epoch 324, Loss: 0.9194966703653336, Final Batch Loss: 0.23136843740940094\n",
      "Epoch 325, Loss: 0.8246976286172867, Final Batch Loss: 0.15981075167655945\n",
      "Epoch 326, Loss: 0.9020369499921799, Final Batch Loss: 0.1878102719783783\n",
      "Epoch 327, Loss: 0.8662132024765015, Final Batch Loss: 0.16122812032699585\n",
      "Epoch 328, Loss: 0.8930067867040634, Final Batch Loss: 0.20793524384498596\n",
      "Epoch 329, Loss: 0.8504346609115601, Final Batch Loss: 0.17587175965309143\n",
      "Epoch 330, Loss: 0.8778290003538132, Final Batch Loss: 0.21292713284492493\n",
      "Epoch 331, Loss: 0.9169721454381943, Final Batch Loss: 0.23745325207710266\n",
      "Epoch 332, Loss: 0.8710366040468216, Final Batch Loss: 0.25037118792533875\n",
      "Epoch 333, Loss: 0.9515504091978073, Final Batch Loss: 0.29630497097969055\n",
      "Epoch 334, Loss: 0.8871618956327438, Final Batch Loss: 0.21235769987106323\n",
      "Epoch 335, Loss: 0.9618411064147949, Final Batch Loss: 0.31014639139175415\n",
      "Epoch 336, Loss: 0.9286751747131348, Final Batch Loss: 0.23673996329307556\n",
      "Epoch 337, Loss: 0.9071742594242096, Final Batch Loss: 0.21396133303642273\n",
      "Epoch 338, Loss: 0.8457961976528168, Final Batch Loss: 0.20005902647972107\n",
      "Epoch 339, Loss: 0.8709910213947296, Final Batch Loss: 0.16526678204536438\n",
      "Epoch 340, Loss: 0.8916918933391571, Final Batch Loss: 0.26292362809181213\n",
      "Epoch 341, Loss: 0.9333322048187256, Final Batch Loss: 0.3091478645801544\n",
      "Epoch 342, Loss: 0.9580987989902496, Final Batch Loss: 0.2234325408935547\n",
      "Epoch 343, Loss: 0.8425305038690567, Final Batch Loss: 0.2308538407087326\n",
      "Epoch 344, Loss: 0.8167952597141266, Final Batch Loss: 0.2423374205827713\n",
      "Epoch 345, Loss: 0.833315908908844, Final Batch Loss: 0.22894147038459778\n",
      "Epoch 346, Loss: 0.8549446165561676, Final Batch Loss: 0.18681906163692474\n",
      "Epoch 347, Loss: 0.8580818176269531, Final Batch Loss: 0.1788385659456253\n",
      "Epoch 348, Loss: 0.8389832675457001, Final Batch Loss: 0.23999790847301483\n",
      "Epoch 349, Loss: 0.8377249091863632, Final Batch Loss: 0.21336530148983002\n",
      "Epoch 350, Loss: 0.8076775968074799, Final Batch Loss: 0.15316499769687653\n",
      "Epoch 351, Loss: 0.8523256778717041, Final Batch Loss: 0.20235365629196167\n",
      "Epoch 352, Loss: 0.7348476201295853, Final Batch Loss: 0.13956335186958313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353, Loss: 0.8068462461233139, Final Batch Loss: 0.1876753717660904\n",
      "Epoch 354, Loss: 0.8317515701055527, Final Batch Loss: 0.20365147292613983\n",
      "Epoch 355, Loss: 0.7855215221643448, Final Batch Loss: 0.18607836961746216\n",
      "Epoch 356, Loss: 0.8965376615524292, Final Batch Loss: 0.2945229113101959\n",
      "Epoch 357, Loss: 0.8927715122699738, Final Batch Loss: 0.20725251734256744\n",
      "Epoch 358, Loss: 0.9176859855651855, Final Batch Loss: 0.25743886828422546\n",
      "Epoch 359, Loss: 0.9507704675197601, Final Batch Loss: 0.29271143674850464\n",
      "Epoch 360, Loss: 0.8619048744440079, Final Batch Loss: 0.20344631373882294\n",
      "Epoch 361, Loss: 0.8706487566232681, Final Batch Loss: 0.2703411281108856\n",
      "Epoch 362, Loss: 0.8374117314815521, Final Batch Loss: 0.22020773589611053\n",
      "Epoch 363, Loss: 0.822934553027153, Final Batch Loss: 0.21418531239032745\n",
      "Epoch 364, Loss: 0.7185035943984985, Final Batch Loss: 0.14975506067276\n",
      "Epoch 365, Loss: 0.8629109710454941, Final Batch Loss: 0.2152431160211563\n",
      "Epoch 366, Loss: 0.9064168184995651, Final Batch Loss: 0.2593841254711151\n",
      "Epoch 367, Loss: 0.8397602736949921, Final Batch Loss: 0.2479282021522522\n",
      "Epoch 368, Loss: 0.8663803040981293, Final Batch Loss: 0.22247149050235748\n",
      "Epoch 369, Loss: 0.8718642741441727, Final Batch Loss: 0.2552347183227539\n",
      "Epoch 370, Loss: 0.8082668334245682, Final Batch Loss: 0.16074946522712708\n",
      "Epoch 371, Loss: 0.784869834780693, Final Batch Loss: 0.139363631606102\n",
      "Epoch 372, Loss: 0.8250372558832169, Final Batch Loss: 0.19730986654758453\n",
      "Epoch 373, Loss: 0.8825937211513519, Final Batch Loss: 0.19268815219402313\n",
      "Epoch 374, Loss: 0.7443252503871918, Final Batch Loss: 0.13675284385681152\n",
      "Epoch 375, Loss: 0.7900299280881882, Final Batch Loss: 0.1658756583929062\n",
      "Epoch 376, Loss: 0.8050077855587006, Final Batch Loss: 0.17336775362491608\n",
      "Epoch 377, Loss: 0.802610844373703, Final Batch Loss: 0.17497438192367554\n",
      "Epoch 378, Loss: 0.8232639580965042, Final Batch Loss: 0.22117935121059418\n",
      "Epoch 379, Loss: 0.8522915691137314, Final Batch Loss: 0.2330891340970993\n",
      "Epoch 380, Loss: 0.8493322879076004, Final Batch Loss: 0.18099914491176605\n",
      "Epoch 381, Loss: 0.7262680530548096, Final Batch Loss: 0.1599089503288269\n",
      "Epoch 382, Loss: 0.8410150557756424, Final Batch Loss: 0.241489976644516\n",
      "Epoch 383, Loss: 0.7914057374000549, Final Batch Loss: 0.18569126725196838\n",
      "Epoch 384, Loss: 0.7887452244758606, Final Batch Loss: 0.15327809751033783\n",
      "Epoch 385, Loss: 0.8475548326969147, Final Batch Loss: 0.17879833281040192\n",
      "Epoch 386, Loss: 0.7789707183837891, Final Batch Loss: 0.16007937490940094\n",
      "Epoch 387, Loss: 0.8293748050928116, Final Batch Loss: 0.23478281497955322\n",
      "Epoch 388, Loss: 0.7496294230222702, Final Batch Loss: 0.15359075367450714\n",
      "Epoch 389, Loss: 0.9052524268627167, Final Batch Loss: 0.2715533673763275\n",
      "Epoch 390, Loss: 0.910079151391983, Final Batch Loss: 0.2619289457798004\n",
      "Epoch 391, Loss: 0.7168569415807724, Final Batch Loss: 0.14340050518512726\n",
      "Epoch 392, Loss: 0.8168827593326569, Final Batch Loss: 0.22841382026672363\n",
      "Epoch 393, Loss: 0.7916683554649353, Final Batch Loss: 0.15349285304546356\n",
      "Epoch 394, Loss: 0.7868427336215973, Final Batch Loss: 0.2234756201505661\n",
      "Epoch 395, Loss: 0.9066632091999054, Final Batch Loss: 0.26816698908805847\n",
      "Epoch 396, Loss: 0.8574010580778122, Final Batch Loss: 0.23972438275814056\n",
      "Epoch 397, Loss: 0.8233445435762405, Final Batch Loss: 0.2603440582752228\n",
      "Epoch 398, Loss: 0.827917292714119, Final Batch Loss: 0.23097214102745056\n",
      "Epoch 399, Loss: 0.9064193964004517, Final Batch Loss: 0.29884642362594604\n",
      "Epoch 400, Loss: 0.9528264105319977, Final Batch Loss: 0.3096812963485718\n",
      "Epoch 401, Loss: 0.8625851720571518, Final Batch Loss: 0.21364623308181763\n",
      "Epoch 402, Loss: 0.840221494436264, Final Batch Loss: 0.1951223462820053\n",
      "Epoch 403, Loss: 0.768197551369667, Final Batch Loss: 0.15781503915786743\n",
      "Epoch 404, Loss: 0.7778008133172989, Final Batch Loss: 0.1975821554660797\n",
      "Epoch 405, Loss: 0.7531446367502213, Final Batch Loss: 0.17713524401187897\n",
      "Epoch 406, Loss: 0.7428022921085358, Final Batch Loss: 0.1401153951883316\n",
      "Epoch 407, Loss: 0.8145822882652283, Final Batch Loss: 0.26156505942344666\n",
      "Epoch 408, Loss: 0.7831151634454727, Final Batch Loss: 0.16965016722679138\n",
      "Epoch 409, Loss: 0.6958170086145401, Final Batch Loss: 0.12502126395702362\n",
      "Epoch 410, Loss: 0.8088129609823227, Final Batch Loss: 0.20830132067203522\n",
      "Epoch 411, Loss: 0.7501466125249863, Final Batch Loss: 0.21637876331806183\n",
      "Epoch 412, Loss: 0.8059055954217911, Final Batch Loss: 0.2210969626903534\n",
      "Epoch 413, Loss: 0.6875047236680984, Final Batch Loss: 0.12993071973323822\n",
      "Epoch 414, Loss: 0.7954552620649338, Final Batch Loss: 0.1610836684703827\n",
      "Epoch 415, Loss: 0.7971471846103668, Final Batch Loss: 0.20277699828147888\n",
      "Epoch 416, Loss: 0.78404600918293, Final Batch Loss: 0.24010911583900452\n",
      "Epoch 417, Loss: 0.7775529325008392, Final Batch Loss: 0.15665706992149353\n",
      "Epoch 418, Loss: 0.7878683805465698, Final Batch Loss: 0.15000909566879272\n",
      "Epoch 419, Loss: 0.7869335263967514, Final Batch Loss: 0.2371358573436737\n",
      "Epoch 420, Loss: 0.7903097569942474, Final Batch Loss: 0.2042083591222763\n",
      "Epoch 421, Loss: 0.6877566576004028, Final Batch Loss: 0.13936501741409302\n",
      "Epoch 422, Loss: 0.7664649039506912, Final Batch Loss: 0.217798113822937\n",
      "Epoch 423, Loss: 0.8511075377464294, Final Batch Loss: 0.24158309400081635\n",
      "Epoch 424, Loss: 0.73558010160923, Final Batch Loss: 0.22279232740402222\n",
      "Epoch 425, Loss: 0.7118646651506424, Final Batch Loss: 0.16958902776241302\n",
      "Epoch 426, Loss: 0.6955641210079193, Final Batch Loss: 0.1538386046886444\n",
      "Epoch 427, Loss: 0.7148640751838684, Final Batch Loss: 0.213491290807724\n",
      "Epoch 428, Loss: 0.7368905395269394, Final Batch Loss: 0.12932057678699493\n",
      "Epoch 429, Loss: 0.7252055555582047, Final Batch Loss: 0.18954159319400787\n",
      "Epoch 430, Loss: 0.7766160070896149, Final Batch Loss: 0.22386199235916138\n",
      "Epoch 431, Loss: 0.6826400011777878, Final Batch Loss: 0.16400738060474396\n",
      "Epoch 432, Loss: 0.748499870300293, Final Batch Loss: 0.12914705276489258\n",
      "Epoch 433, Loss: 0.7684134095907211, Final Batch Loss: 0.23064830899238586\n",
      "Epoch 434, Loss: 0.6769911795854568, Final Batch Loss: 0.15968364477157593\n",
      "Epoch 435, Loss: 0.6144448593258858, Final Batch Loss: 0.08872916549444199\n",
      "Epoch 436, Loss: 0.7343840599060059, Final Batch Loss: 0.18521694839000702\n",
      "Epoch 437, Loss: 0.7035459578037262, Final Batch Loss: 0.21734407544136047\n",
      "Epoch 438, Loss: 0.7038429379463196, Final Batch Loss: 0.191408172249794\n",
      "Epoch 439, Loss: 0.716898187994957, Final Batch Loss: 0.18623144924640656\n",
      "Epoch 440, Loss: 0.726837545633316, Final Batch Loss: 0.19492687284946442\n",
      "Epoch 441, Loss: 0.6527407169342041, Final Batch Loss: 0.15146306157112122\n",
      "Epoch 442, Loss: 0.7830515205860138, Final Batch Loss: 0.2864854633808136\n",
      "Epoch 443, Loss: 0.6533970981836319, Final Batch Loss: 0.17459172010421753\n",
      "Epoch 444, Loss: 0.7173762023448944, Final Batch Loss: 0.14584313333034515\n",
      "Epoch 445, Loss: 0.8119664192199707, Final Batch Loss: 0.21513287723064423\n",
      "Epoch 446, Loss: 0.6329718232154846, Final Batch Loss: 0.11946864426136017\n",
      "Epoch 447, Loss: 0.6525071263313293, Final Batch Loss: 0.16778969764709473\n",
      "Epoch 448, Loss: 0.6405430734157562, Final Batch Loss: 0.13197658956050873\n",
      "Epoch 449, Loss: 0.6617008075118065, Final Batch Loss: 0.17885002493858337\n",
      "Epoch 450, Loss: 0.7698619365692139, Final Batch Loss: 0.15498268604278564\n",
      "Epoch 451, Loss: 0.7180373817682266, Final Batch Loss: 0.20201165974140167\n",
      "Epoch 452, Loss: 0.6615184545516968, Final Batch Loss: 0.16088911890983582\n",
      "Epoch 453, Loss: 0.6636946350336075, Final Batch Loss: 0.17118841409683228\n",
      "Epoch 454, Loss: 0.6668659076094627, Final Batch Loss: 0.11615218967199326\n",
      "Epoch 455, Loss: 0.7057661414146423, Final Batch Loss: 0.17356693744659424\n",
      "Epoch 456, Loss: 0.6066787466406822, Final Batch Loss: 0.11394568532705307\n",
      "Epoch 457, Loss: 0.6994020491838455, Final Batch Loss: 0.16521547734737396\n",
      "Epoch 458, Loss: 0.6976561993360519, Final Batch Loss: 0.20965605974197388\n",
      "Epoch 459, Loss: 0.5806281566619873, Final Batch Loss: 0.13060356676578522\n",
      "Epoch 460, Loss: 0.7216391414403915, Final Batch Loss: 0.20131193101406097\n",
      "Epoch 461, Loss: 0.5678679943084717, Final Batch Loss: 0.10177955031394958\n",
      "Epoch 462, Loss: 0.6352775692939758, Final Batch Loss: 0.1472826451063156\n",
      "Epoch 463, Loss: 0.6897142678499222, Final Batch Loss: 0.17673198878765106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464, Loss: 0.7178536504507065, Final Batch Loss: 0.1355428546667099\n",
      "Epoch 465, Loss: 0.6358726173639297, Final Batch Loss: 0.13939279317855835\n",
      "Epoch 466, Loss: 0.6961572617292404, Final Batch Loss: 0.18837301433086395\n",
      "Epoch 467, Loss: 0.6502118110656738, Final Batch Loss: 0.16766266524791718\n",
      "Epoch 468, Loss: 0.6916481107473373, Final Batch Loss: 0.12557660043239594\n",
      "Epoch 469, Loss: 0.6354625523090363, Final Batch Loss: 0.08488419651985168\n",
      "Epoch 470, Loss: 0.7383991032838821, Final Batch Loss: 0.22964462637901306\n",
      "Epoch 471, Loss: 0.6850774586200714, Final Batch Loss: 0.22012142837047577\n",
      "Epoch 472, Loss: 0.7160612195730209, Final Batch Loss: 0.20222750306129456\n",
      "Epoch 473, Loss: 0.7337418049573898, Final Batch Loss: 0.22636258602142334\n",
      "Epoch 474, Loss: 0.7370244413614273, Final Batch Loss: 0.23783788084983826\n",
      "Epoch 475, Loss: 0.7927712798118591, Final Batch Loss: 0.2741914987564087\n",
      "Epoch 476, Loss: 0.7317758798599243, Final Batch Loss: 0.1655021756887436\n",
      "Epoch 477, Loss: 0.6915335953235626, Final Batch Loss: 0.1745425909757614\n",
      "Epoch 478, Loss: 0.6457033604383469, Final Batch Loss: 0.15777242183685303\n",
      "Epoch 479, Loss: 0.6827113032341003, Final Batch Loss: 0.1704883724451065\n",
      "Epoch 480, Loss: 0.6629750207066536, Final Batch Loss: 0.21949094533920288\n",
      "Epoch 481, Loss: 0.663387268781662, Final Batch Loss: 0.17639416456222534\n",
      "Epoch 482, Loss: 0.6999904364347458, Final Batch Loss: 0.16523785889148712\n",
      "Epoch 483, Loss: 0.6779011189937592, Final Batch Loss: 0.17460082471370697\n",
      "Epoch 484, Loss: 0.5917599350214005, Final Batch Loss: 0.11742772161960602\n",
      "Epoch 485, Loss: 0.6949209421873093, Final Batch Loss: 0.1900964081287384\n",
      "Epoch 486, Loss: 0.6152321249246597, Final Batch Loss: 0.16462500393390656\n",
      "Epoch 487, Loss: 0.7536109536886215, Final Batch Loss: 0.14488105475902557\n",
      "Epoch 488, Loss: 0.7023547291755676, Final Batch Loss: 0.22157153487205505\n",
      "Epoch 489, Loss: 0.8453018367290497, Final Batch Loss: 0.30696746706962585\n",
      "Epoch 490, Loss: 0.5972569063305855, Final Batch Loss: 0.1127992495894432\n",
      "Epoch 491, Loss: 0.6776597648859024, Final Batch Loss: 0.16622674465179443\n",
      "Epoch 492, Loss: 0.6847510635852814, Final Batch Loss: 0.22364699840545654\n",
      "Epoch 493, Loss: 0.7603670954704285, Final Batch Loss: 0.2545969784259796\n",
      "Epoch 494, Loss: 0.7695917338132858, Final Batch Loss: 0.22845645248889923\n",
      "Epoch 495, Loss: 0.6585754454135895, Final Batch Loss: 0.15963736176490784\n",
      "Epoch 496, Loss: 0.628547377884388, Final Batch Loss: 0.11894270032644272\n",
      "Epoch 497, Loss: 0.5552162975072861, Final Batch Loss: 0.07285992801189423\n",
      "Epoch 498, Loss: 0.6370657682418823, Final Batch Loss: 0.16778796911239624\n",
      "Epoch 499, Loss: 0.7986079752445221, Final Batch Loss: 0.23415955901145935\n",
      "Epoch 500, Loss: 0.7257568836212158, Final Batch Loss: 0.2582536041736603\n",
      "Epoch 501, Loss: 0.599120169878006, Final Batch Loss: 0.09619851410388947\n",
      "Epoch 502, Loss: 0.6873319298028946, Final Batch Loss: 0.19846391677856445\n",
      "Epoch 503, Loss: 0.7606954574584961, Final Batch Loss: 0.17754609882831573\n",
      "Epoch 504, Loss: 0.6967940032482147, Final Batch Loss: 0.1581498682498932\n",
      "Epoch 505, Loss: 0.6925029456615448, Final Batch Loss: 0.18194842338562012\n",
      "Epoch 506, Loss: 0.6674022078514099, Final Batch Loss: 0.1656048446893692\n",
      "Epoch 507, Loss: 0.523062564432621, Final Batch Loss: 0.07895735651254654\n",
      "Epoch 508, Loss: 0.673444077372551, Final Batch Loss: 0.15853136777877808\n",
      "Epoch 509, Loss: 0.6294905096292496, Final Batch Loss: 0.14787894487380981\n",
      "Epoch 510, Loss: 0.6532027572393417, Final Batch Loss: 0.16293549537658691\n",
      "Epoch 511, Loss: 0.6224549636244774, Final Batch Loss: 0.13504739105701447\n",
      "Epoch 512, Loss: 0.6359187886118889, Final Batch Loss: 0.16420146822929382\n",
      "Epoch 513, Loss: 0.6117776781320572, Final Batch Loss: 0.1663166731595993\n",
      "Epoch 514, Loss: 0.6235457137227058, Final Batch Loss: 0.12123862653970718\n",
      "Epoch 515, Loss: 0.7888228595256805, Final Batch Loss: 0.35195091366767883\n",
      "Epoch 516, Loss: 0.6985218971967697, Final Batch Loss: 0.2225867062807083\n",
      "Epoch 517, Loss: 0.6694208607077599, Final Batch Loss: 0.2285749763250351\n",
      "Epoch 518, Loss: 0.5758869536221027, Final Batch Loss: 0.058998022228479385\n",
      "Epoch 519, Loss: 0.7004630118608475, Final Batch Loss: 0.22094371914863586\n",
      "Epoch 520, Loss: 0.6104260832071304, Final Batch Loss: 0.19247059524059296\n",
      "Epoch 521, Loss: 0.6431975066661835, Final Batch Loss: 0.160032719373703\n",
      "Epoch 522, Loss: 0.6485634595155716, Final Batch Loss: 0.20052455365657806\n",
      "Epoch 523, Loss: 0.6134505197405815, Final Batch Loss: 0.0840713158249855\n",
      "Epoch 524, Loss: 0.6175147667527199, Final Batch Loss: 0.2072972059249878\n",
      "Epoch 525, Loss: 0.6603194549679756, Final Batch Loss: 0.11837080866098404\n",
      "Epoch 526, Loss: 0.6662245839834213, Final Batch Loss: 0.1851176768541336\n",
      "Epoch 527, Loss: 0.6759952530264854, Final Batch Loss: 0.20914341509342194\n",
      "Epoch 528, Loss: 0.7346279323101044, Final Batch Loss: 0.25914329290390015\n",
      "Epoch 529, Loss: 0.8029215931892395, Final Batch Loss: 0.20273847877979279\n",
      "Epoch 530, Loss: 0.5626963824033737, Final Batch Loss: 0.12394560128450394\n",
      "Epoch 531, Loss: 0.5682775378227234, Final Batch Loss: 0.09247919917106628\n",
      "Epoch 532, Loss: 0.5849613696336746, Final Batch Loss: 0.1261947900056839\n",
      "Epoch 533, Loss: 0.6191981732845306, Final Batch Loss: 0.13745512068271637\n",
      "Epoch 534, Loss: 0.5805084481835365, Final Batch Loss: 0.10174877196550369\n",
      "Epoch 535, Loss: 0.6614489555358887, Final Batch Loss: 0.1896037459373474\n",
      "Epoch 536, Loss: 0.639843687415123, Final Batch Loss: 0.2095228135585785\n",
      "Epoch 537, Loss: 0.6955597773194313, Final Batch Loss: 0.2356664389371872\n",
      "Epoch 538, Loss: 0.6938295215368271, Final Batch Loss: 0.17354758083820343\n",
      "Epoch 539, Loss: 0.5564165040850639, Final Batch Loss: 0.134762242436409\n",
      "Epoch 540, Loss: 0.6126939356327057, Final Batch Loss: 0.14155393838882446\n",
      "Epoch 541, Loss: 0.6420982331037521, Final Batch Loss: 0.19803784787654877\n",
      "Epoch 542, Loss: 0.6637539565563202, Final Batch Loss: 0.19555746018886566\n",
      "Epoch 543, Loss: 0.6201114058494568, Final Batch Loss: 0.11486247181892395\n",
      "Epoch 544, Loss: 0.5216599702835083, Final Batch Loss: 0.06533026695251465\n",
      "Epoch 545, Loss: 0.6617331206798553, Final Batch Loss: 0.20970861613750458\n",
      "Epoch 546, Loss: 0.5414799600839615, Final Batch Loss: 0.1404363363981247\n",
      "Epoch 547, Loss: 0.5605790689587593, Final Batch Loss: 0.12539580464363098\n",
      "Epoch 548, Loss: 0.6071959584951401, Final Batch Loss: 0.1201542466878891\n",
      "Epoch 549, Loss: 0.6189796924591064, Final Batch Loss: 0.16968761384487152\n",
      "Epoch 550, Loss: 0.6407580077648163, Final Batch Loss: 0.17451655864715576\n",
      "Epoch 551, Loss: 0.5650487691164017, Final Batch Loss: 0.11450351774692535\n",
      "Epoch 552, Loss: 0.5972023010253906, Final Batch Loss: 0.13895219564437866\n",
      "Epoch 553, Loss: 0.5854825749993324, Final Batch Loss: 0.12383994460105896\n",
      "Epoch 554, Loss: 0.6289318799972534, Final Batch Loss: 0.14020675420761108\n",
      "Epoch 555, Loss: 0.6089399307966232, Final Batch Loss: 0.15296636521816254\n",
      "Epoch 556, Loss: 0.6317698508501053, Final Batch Loss: 0.22768142819404602\n",
      "Epoch 557, Loss: 0.6671940237283707, Final Batch Loss: 0.20295853912830353\n",
      "Epoch 558, Loss: 0.5337501093745232, Final Batch Loss: 0.10312185436487198\n",
      "Epoch 559, Loss: 0.5796038433909416, Final Batch Loss: 0.11696771532297134\n",
      "Epoch 560, Loss: 0.6099111139774323, Final Batch Loss: 0.1493745595216751\n",
      "Epoch 561, Loss: 0.5850853323936462, Final Batch Loss: 0.13803353905677795\n",
      "Epoch 562, Loss: 0.6518407464027405, Final Batch Loss: 0.15886364877223969\n",
      "Epoch 563, Loss: 0.6253693625330925, Final Batch Loss: 0.12133512645959854\n",
      "Epoch 564, Loss: 0.6118661016225815, Final Batch Loss: 0.14349395036697388\n",
      "Epoch 565, Loss: 0.5698424279689789, Final Batch Loss: 0.09124500304460526\n",
      "Epoch 566, Loss: 0.5953805670142174, Final Batch Loss: 0.09459881484508514\n",
      "Epoch 567, Loss: 0.6204209476709366, Final Batch Loss: 0.14788863062858582\n",
      "Epoch 568, Loss: 0.564611166715622, Final Batch Loss: 0.14728759229183197\n",
      "Epoch 569, Loss: 0.6022354364395142, Final Batch Loss: 0.1339556872844696\n",
      "Epoch 570, Loss: 0.6298890337347984, Final Batch Loss: 0.20688092708587646\n",
      "Epoch 571, Loss: 0.5897100865840912, Final Batch Loss: 0.15163032710552216\n",
      "Epoch 572, Loss: 0.6292107403278351, Final Batch Loss: 0.1873837113380432\n",
      "Epoch 573, Loss: 0.6808150708675385, Final Batch Loss: 0.18097659945487976\n",
      "Epoch 574, Loss: 0.5305803641676903, Final Batch Loss: 0.1438000202178955\n",
      "Epoch 575, Loss: 0.5935610681772232, Final Batch Loss: 0.14013445377349854\n",
      "Epoch 576, Loss: 0.5670225769281387, Final Batch Loss: 0.2005530148744583\n",
      "Epoch 577, Loss: 0.5802149772644043, Final Batch Loss: 0.14875608682632446\n",
      "Epoch 578, Loss: 0.5807774662971497, Final Batch Loss: 0.15736626088619232\n",
      "Epoch 579, Loss: 0.623160257935524, Final Batch Loss: 0.1709541529417038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 580, Loss: 0.6118069887161255, Final Batch Loss: 0.09361955523490906\n",
      "Epoch 581, Loss: 0.5912818685173988, Final Batch Loss: 0.11796311289072037\n",
      "Epoch 582, Loss: 0.6484256833791733, Final Batch Loss: 0.18507355451583862\n",
      "Epoch 583, Loss: 0.6383431553840637, Final Batch Loss: 0.17744120955467224\n",
      "Epoch 584, Loss: 0.5768755227327347, Final Batch Loss: 0.14162437617778778\n",
      "Epoch 585, Loss: 0.626919575035572, Final Batch Loss: 0.17906229197978973\n",
      "Epoch 586, Loss: 0.5527009889483452, Final Batch Loss: 0.1642577350139618\n",
      "Epoch 587, Loss: 0.5496431738138199, Final Batch Loss: 0.12297316640615463\n",
      "Epoch 588, Loss: 0.6074490770697594, Final Batch Loss: 0.1628192961215973\n",
      "Epoch 589, Loss: 0.5069751664996147, Final Batch Loss: 0.11133420467376709\n",
      "Epoch 590, Loss: 0.5822722166776657, Final Batch Loss: 0.14691781997680664\n",
      "Epoch 591, Loss: 0.5331781581044197, Final Batch Loss: 0.11050057411193848\n",
      "Epoch 592, Loss: 0.5115334615111351, Final Batch Loss: 0.13239839673042297\n",
      "Epoch 593, Loss: 0.6277252584695816, Final Batch Loss: 0.21901220083236694\n",
      "Epoch 594, Loss: 0.6365789771080017, Final Batch Loss: 0.2063603699207306\n",
      "Epoch 595, Loss: 0.4915037453174591, Final Batch Loss: 0.15957731008529663\n",
      "Epoch 596, Loss: 0.4977151155471802, Final Batch Loss: 0.10777764022350311\n",
      "Epoch 597, Loss: 0.5430693328380585, Final Batch Loss: 0.08440247923135757\n",
      "Epoch 598, Loss: 0.6324575394392014, Final Batch Loss: 0.22526875138282776\n",
      "Epoch 599, Loss: 0.5200568735599518, Final Batch Loss: 0.08748362958431244\n",
      "Epoch 600, Loss: 0.5593107640743256, Final Batch Loss: 0.14829935133457184\n",
      "Epoch 601, Loss: 0.6119750142097473, Final Batch Loss: 0.15476687252521515\n",
      "Epoch 602, Loss: 0.5867016240954399, Final Batch Loss: 0.12970830500125885\n",
      "Epoch 603, Loss: 0.5490214303135872, Final Batch Loss: 0.11455675214529037\n",
      "Epoch 604, Loss: 0.5902452692389488, Final Batch Loss: 0.15340076386928558\n",
      "Epoch 605, Loss: 0.5154823064804077, Final Batch Loss: 0.10781001299619675\n",
      "Epoch 606, Loss: 0.5438843965530396, Final Batch Loss: 0.13098189234733582\n",
      "Epoch 607, Loss: 0.585576742887497, Final Batch Loss: 0.13301701843738556\n",
      "Epoch 608, Loss: 0.7664180472493172, Final Batch Loss: 0.2818117141723633\n",
      "Epoch 609, Loss: 0.6364092379808426, Final Batch Loss: 0.20295388996601105\n",
      "Epoch 610, Loss: 0.6675883233547211, Final Batch Loss: 0.20319180190563202\n",
      "Epoch 611, Loss: 0.5757237449288368, Final Batch Loss: 0.10897455364465714\n",
      "Epoch 612, Loss: 0.5921651273965836, Final Batch Loss: 0.1605398952960968\n",
      "Epoch 613, Loss: 0.5283364951610565, Final Batch Loss: 0.13113757967948914\n",
      "Epoch 614, Loss: 0.6090557426214218, Final Batch Loss: 0.15445934236049652\n",
      "Epoch 615, Loss: 0.600473128259182, Final Batch Loss: 0.12230826169252396\n",
      "Epoch 616, Loss: 0.6076273918151855, Final Batch Loss: 0.16945554316043854\n",
      "Epoch 617, Loss: 0.5405038148164749, Final Batch Loss: 0.09922558069229126\n",
      "Epoch 618, Loss: 0.6329533606767654, Final Batch Loss: 0.19820483028888702\n",
      "Epoch 619, Loss: 0.5124161019921303, Final Batch Loss: 0.10007330775260925\n",
      "Epoch 620, Loss: 0.5095217376947403, Final Batch Loss: 0.10036785900592804\n",
      "Epoch 621, Loss: 0.5093768388032913, Final Batch Loss: 0.09898976236581802\n",
      "Epoch 622, Loss: 0.5277781635522842, Final Batch Loss: 0.12659507989883423\n",
      "Epoch 623, Loss: 0.5448808744549751, Final Batch Loss: 0.1573319137096405\n",
      "Epoch 624, Loss: 0.5671408697962761, Final Batch Loss: 0.11019433289766312\n",
      "Epoch 625, Loss: 0.6091887131333351, Final Batch Loss: 0.20126482844352722\n",
      "Epoch 626, Loss: 0.6136410012841225, Final Batch Loss: 0.2563472092151642\n",
      "Epoch 627, Loss: 0.5663788244128227, Final Batch Loss: 0.17864035069942474\n",
      "Epoch 628, Loss: 0.5131015405058861, Final Batch Loss: 0.10925140231847763\n",
      "Epoch 629, Loss: 0.5035214275121689, Final Batch Loss: 0.09333605319261551\n",
      "Epoch 630, Loss: 0.6003766506910324, Final Batch Loss: 0.15236937999725342\n",
      "Epoch 631, Loss: 0.6088021248579025, Final Batch Loss: 0.2000659555196762\n",
      "Epoch 632, Loss: 0.5673231035470963, Final Batch Loss: 0.13282957673072815\n",
      "Epoch 633, Loss: 0.46958472579717636, Final Batch Loss: 0.0770966112613678\n",
      "Epoch 634, Loss: 0.5206056162714958, Final Batch Loss: 0.1613583266735077\n",
      "Epoch 635, Loss: 0.5872117951512337, Final Batch Loss: 0.2070736140012741\n",
      "Epoch 636, Loss: 0.6429970934987068, Final Batch Loss: 0.10048741847276688\n",
      "Epoch 637, Loss: 0.614076629281044, Final Batch Loss: 0.24053893983364105\n",
      "Epoch 638, Loss: 0.5804723799228668, Final Batch Loss: 0.2018129527568817\n",
      "Epoch 639, Loss: 0.610355369746685, Final Batch Loss: 0.24850137531757355\n",
      "Epoch 640, Loss: 0.5731866806745529, Final Batch Loss: 0.1642124205827713\n",
      "Epoch 641, Loss: 0.5976891666650772, Final Batch Loss: 0.13734650611877441\n",
      "Epoch 642, Loss: 0.6532343775033951, Final Batch Loss: 0.24123594164848328\n",
      "Epoch 643, Loss: 0.7362688183784485, Final Batch Loss: 0.2385096251964569\n",
      "Epoch 644, Loss: 0.653541088104248, Final Batch Loss: 0.2136356383562088\n",
      "Epoch 645, Loss: 0.6197440028190613, Final Batch Loss: 0.17428486049175262\n",
      "Epoch 646, Loss: 0.7005666941404343, Final Batch Loss: 0.19455042481422424\n",
      "Epoch 647, Loss: 0.5932851582765579, Final Batch Loss: 0.17240507900714874\n",
      "Epoch 648, Loss: 0.6958932280540466, Final Batch Loss: 0.1906355619430542\n",
      "Epoch 649, Loss: 0.6254407316446304, Final Batch Loss: 0.19116491079330444\n",
      "Epoch 650, Loss: 0.6199520155787468, Final Batch Loss: 0.12194604426622391\n",
      "Epoch 651, Loss: 0.6051372960209846, Final Batch Loss: 0.20213967561721802\n",
      "Epoch 652, Loss: 0.49884674698114395, Final Batch Loss: 0.1344500482082367\n",
      "Epoch 653, Loss: 0.5009284615516663, Final Batch Loss: 0.13083714246749878\n",
      "Epoch 654, Loss: 0.5061887726187706, Final Batch Loss: 0.11106271296739578\n",
      "Epoch 655, Loss: 0.5941779464483261, Final Batch Loss: 0.18280808627605438\n",
      "Epoch 656, Loss: 0.517926312983036, Final Batch Loss: 0.10837322473526001\n",
      "Epoch 657, Loss: 0.49664629995822906, Final Batch Loss: 0.08380822837352753\n",
      "Epoch 658, Loss: 0.5112110003829002, Final Batch Loss: 0.16509248316287994\n",
      "Epoch 659, Loss: 0.8218356221914291, Final Batch Loss: 0.14968614280223846\n",
      "Epoch 660, Loss: 0.5336874201893806, Final Batch Loss: 0.14841574430465698\n",
      "Epoch 661, Loss: 0.5571468845009804, Final Batch Loss: 0.13057895004749298\n",
      "Epoch 662, Loss: 0.5239311382174492, Final Batch Loss: 0.12841372191905975\n",
      "Epoch 663, Loss: 0.48300017416477203, Final Batch Loss: 0.09832833707332611\n",
      "Epoch 664, Loss: 0.5535022914409637, Final Batch Loss: 0.16528365015983582\n",
      "Epoch 665, Loss: 0.611858069896698, Final Batch Loss: 0.1567823439836502\n",
      "Epoch 666, Loss: 0.5168277695775032, Final Batch Loss: 0.12036504596471786\n",
      "Epoch 667, Loss: 0.6748412847518921, Final Batch Loss: 0.17436672747135162\n",
      "Epoch 668, Loss: 0.5181278437376022, Final Batch Loss: 0.11127963662147522\n",
      "Epoch 669, Loss: 0.5749987736344337, Final Batch Loss: 0.19417962431907654\n",
      "Epoch 670, Loss: 0.5853395015001297, Final Batch Loss: 0.10411551594734192\n",
      "Epoch 671, Loss: 0.4754953756928444, Final Batch Loss: 0.11423731595277786\n",
      "Epoch 672, Loss: 0.5108585804700851, Final Batch Loss: 0.131058007478714\n",
      "Epoch 673, Loss: 0.5407293066382408, Final Batch Loss: 0.06776688992977142\n",
      "Epoch 674, Loss: 0.46487829089164734, Final Batch Loss: 0.11568817496299744\n",
      "Epoch 675, Loss: 0.526599146425724, Final Batch Loss: 0.13910801708698273\n",
      "Epoch 676, Loss: 0.5400605425238609, Final Batch Loss: 0.1792973279953003\n",
      "Epoch 677, Loss: 0.5940440893173218, Final Batch Loss: 0.12657926976680756\n",
      "Epoch 678, Loss: 0.6036131531000137, Final Batch Loss: 0.14458835124969482\n",
      "Epoch 679, Loss: 0.5091219246387482, Final Batch Loss: 0.1321948915719986\n",
      "Epoch 680, Loss: 0.5642921775579453, Final Batch Loss: 0.1139335185289383\n",
      "Epoch 681, Loss: 0.6198559179902077, Final Batch Loss: 0.11414843052625656\n",
      "Epoch 682, Loss: 0.5294515118002892, Final Batch Loss: 0.18461370468139648\n",
      "Epoch 683, Loss: 0.46115365624427795, Final Batch Loss: 0.1326962411403656\n",
      "Epoch 684, Loss: 0.4556497484445572, Final Batch Loss: 0.08802970498800278\n",
      "Epoch 685, Loss: 0.5421473756432533, Final Batch Loss: 0.12924902141094208\n",
      "Epoch 686, Loss: 0.5906536653637886, Final Batch Loss: 0.2144385725259781\n",
      "Epoch 687, Loss: 0.49903617799282074, Final Batch Loss: 0.12662582099437714\n",
      "Epoch 688, Loss: 0.6153252869844437, Final Batch Loss: 0.19620013236999512\n",
      "Epoch 689, Loss: 0.5049053058028221, Final Batch Loss: 0.13549764454364777\n",
      "Epoch 690, Loss: 0.6167636662721634, Final Batch Loss: 0.16200505197048187\n",
      "Epoch 691, Loss: 0.5173844397068024, Final Batch Loss: 0.1598866581916809\n",
      "Epoch 692, Loss: 0.5346917510032654, Final Batch Loss: 0.1469515711069107\n",
      "Epoch 693, Loss: 0.5258148163557053, Final Batch Loss: 0.09951777011156082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694, Loss: 0.5041719824075699, Final Batch Loss: 0.08813341706991196\n",
      "Epoch 695, Loss: 0.5935093760490417, Final Batch Loss: 0.13045383989810944\n",
      "Epoch 696, Loss: 0.5235731527209282, Final Batch Loss: 0.10033509880304337\n",
      "Epoch 697, Loss: 0.47057898342609406, Final Batch Loss: 0.09980466961860657\n",
      "Epoch 698, Loss: 0.467299722135067, Final Batch Loss: 0.07431791722774506\n",
      "Epoch 699, Loss: 0.484270878136158, Final Batch Loss: 0.09528142958879471\n",
      "Epoch 700, Loss: 0.6356266289949417, Final Batch Loss: 0.1927153617143631\n",
      "Epoch 701, Loss: 0.5937847048044205, Final Batch Loss: 0.16289769113063812\n",
      "Epoch 702, Loss: 0.5262392833828926, Final Batch Loss: 0.18046900629997253\n",
      "Epoch 703, Loss: 0.5984703078866005, Final Batch Loss: 0.19763898849487305\n",
      "Epoch 704, Loss: 0.46827051043510437, Final Batch Loss: 0.09024148434400558\n",
      "Epoch 705, Loss: 0.4567188173532486, Final Batch Loss: 0.11567629128694534\n",
      "Epoch 706, Loss: 0.4453911669552326, Final Batch Loss: 0.057306330651044846\n",
      "Epoch 707, Loss: 0.4875161871314049, Final Batch Loss: 0.1356056034564972\n",
      "Epoch 708, Loss: 0.4942447394132614, Final Batch Loss: 0.10564861446619034\n",
      "Epoch 709, Loss: 0.48338067531585693, Final Batch Loss: 0.15252049267292023\n",
      "Epoch 710, Loss: 0.4650719538331032, Final Batch Loss: 0.09888766705989838\n",
      "Epoch 711, Loss: 0.5784541070461273, Final Batch Loss: 0.19655169546604156\n",
      "Epoch 712, Loss: 0.5038052722811699, Final Batch Loss: 0.16497978568077087\n",
      "Epoch 713, Loss: 0.48534470796585083, Final Batch Loss: 0.12554289400577545\n",
      "Epoch 714, Loss: 0.4739414155483246, Final Batch Loss: 0.09889097511768341\n",
      "Epoch 715, Loss: 0.4997065141797066, Final Batch Loss: 0.1110946536064148\n",
      "Epoch 716, Loss: 0.5313073918223381, Final Batch Loss: 0.14104068279266357\n",
      "Epoch 717, Loss: 0.6015907153487206, Final Batch Loss: 0.21577046811580658\n",
      "Epoch 718, Loss: 0.5410796999931335, Final Batch Loss: 0.13165412843227386\n",
      "Epoch 719, Loss: 0.4487182945013046, Final Batch Loss: 0.08325565606355667\n",
      "Epoch 720, Loss: 0.6622865796089172, Final Batch Loss: 0.2582572400569916\n",
      "Epoch 721, Loss: 0.5030957981944084, Final Batch Loss: 0.11929390579462051\n",
      "Epoch 722, Loss: 0.4387979879975319, Final Batch Loss: 0.07132350653409958\n",
      "Epoch 723, Loss: 0.453005351126194, Final Batch Loss: 0.06706088781356812\n",
      "Epoch 724, Loss: 0.4353427141904831, Final Batch Loss: 0.05706840008497238\n",
      "Epoch 725, Loss: 0.5530127733945847, Final Batch Loss: 0.17207954823970795\n",
      "Epoch 726, Loss: 0.45821789279580116, Final Batch Loss: 0.05466769263148308\n",
      "Epoch 727, Loss: 0.484668493270874, Final Batch Loss: 0.12903179228305817\n",
      "Epoch 728, Loss: 0.49388302117586136, Final Batch Loss: 0.146133154630661\n",
      "Epoch 729, Loss: 0.47787363082170486, Final Batch Loss: 0.12230592221021652\n",
      "Epoch 730, Loss: 0.5359847098588943, Final Batch Loss: 0.13744467496871948\n",
      "Epoch 731, Loss: 0.5266724303364754, Final Batch Loss: 0.16027119755744934\n",
      "Epoch 732, Loss: 0.46551312506198883, Final Batch Loss: 0.0910673588514328\n",
      "Epoch 733, Loss: 0.5205104127526283, Final Batch Loss: 0.11437962204217911\n",
      "Epoch 734, Loss: 0.5001309812068939, Final Batch Loss: 0.09133686125278473\n",
      "Epoch 735, Loss: 0.4804896041750908, Final Batch Loss: 0.09324481338262558\n",
      "Epoch 736, Loss: 0.6180479526519775, Final Batch Loss: 0.17690254747867584\n",
      "Epoch 737, Loss: 0.5368938818573952, Final Batch Loss: 0.10138098895549774\n",
      "Epoch 738, Loss: 0.5168306455016136, Final Batch Loss: 0.12563832104206085\n",
      "Epoch 739, Loss: 0.5112724304199219, Final Batch Loss: 0.1260320395231247\n",
      "Epoch 740, Loss: 0.5695107653737068, Final Batch Loss: 0.17856228351593018\n",
      "Epoch 741, Loss: 0.5397281348705292, Final Batch Loss: 0.17763184010982513\n",
      "Epoch 742, Loss: 0.5597842484712601, Final Batch Loss: 0.07341258227825165\n",
      "Epoch 743, Loss: 0.5022082328796387, Final Batch Loss: 0.06509040296077728\n",
      "Epoch 744, Loss: 0.5288032591342926, Final Batch Loss: 0.12411556392908096\n",
      "Epoch 745, Loss: 0.5590335950255394, Final Batch Loss: 0.12027131766080856\n",
      "Epoch 746, Loss: 0.485165499150753, Final Batch Loss: 0.13628825545310974\n",
      "Epoch 747, Loss: 0.5363113805651665, Final Batch Loss: 0.14778828620910645\n",
      "Epoch 748, Loss: 0.4754856675863266, Final Batch Loss: 0.13120853900909424\n",
      "Epoch 749, Loss: 0.5556084737181664, Final Batch Loss: 0.18409714102745056\n",
      "Epoch 750, Loss: 0.5827460885047913, Final Batch Loss: 0.14981508255004883\n",
      "Epoch 751, Loss: 0.5194713845849037, Final Batch Loss: 0.16076157987117767\n",
      "Epoch 752, Loss: 0.5327699109911919, Final Batch Loss: 0.07018692046403885\n",
      "Epoch 753, Loss: 0.5609511286020279, Final Batch Loss: 0.1856316775083542\n",
      "Epoch 754, Loss: 0.5162801519036293, Final Batch Loss: 0.10012508183717728\n",
      "Epoch 755, Loss: 0.6077258065342903, Final Batch Loss: 0.1950855255126953\n",
      "Epoch 756, Loss: 0.5032971575856209, Final Batch Loss: 0.11753128468990326\n",
      "Epoch 757, Loss: 0.4242944270372391, Final Batch Loss: 0.07865051925182343\n",
      "Epoch 758, Loss: 0.5656462162733078, Final Batch Loss: 0.1513051837682724\n",
      "Epoch 759, Loss: 0.4607192352414131, Final Batch Loss: 0.0902385413646698\n",
      "Epoch 760, Loss: 0.5489497631788254, Final Batch Loss: 0.17768777906894684\n",
      "Epoch 761, Loss: 0.4942774921655655, Final Batch Loss: 0.15201234817504883\n",
      "Epoch 762, Loss: 0.4784613102674484, Final Batch Loss: 0.11914601922035217\n",
      "Epoch 763, Loss: 0.5034327656030655, Final Batch Loss: 0.11718861013650894\n",
      "Epoch 764, Loss: 0.6068837940692902, Final Batch Loss: 0.12146197259426117\n",
      "Epoch 765, Loss: 0.583116702735424, Final Batch Loss: 0.23723715543746948\n",
      "Epoch 766, Loss: 0.4670382887125015, Final Batch Loss: 0.07720217108726501\n",
      "Epoch 767, Loss: 0.5081777125597, Final Batch Loss: 0.13858112692832947\n",
      "Epoch 768, Loss: 0.47606661170721054, Final Batch Loss: 0.14042121171951294\n",
      "Epoch 769, Loss: 0.46937740594148636, Final Batch Loss: 0.1340486854314804\n",
      "Epoch 770, Loss: 0.5663851350545883, Final Batch Loss: 0.1637076437473297\n",
      "Epoch 771, Loss: 0.530191645026207, Final Batch Loss: 0.17464616894721985\n",
      "Epoch 772, Loss: 0.5153702571988106, Final Batch Loss: 0.12348970770835876\n",
      "Epoch 773, Loss: 0.390791691839695, Final Batch Loss: 0.0717829018831253\n",
      "Epoch 774, Loss: 0.48183540999889374, Final Batch Loss: 0.11611850559711456\n",
      "Epoch 775, Loss: 0.5577680766582489, Final Batch Loss: 0.1675819307565689\n",
      "Epoch 776, Loss: 0.4497984051704407, Final Batch Loss: 0.10040976852178574\n",
      "Epoch 777, Loss: 0.4222967252135277, Final Batch Loss: 0.0749925747513771\n",
      "Epoch 778, Loss: 0.40631044656038284, Final Batch Loss: 0.07141272723674774\n",
      "Epoch 779, Loss: 0.468740813434124, Final Batch Loss: 0.08084993064403534\n",
      "Epoch 780, Loss: 0.5557370185852051, Final Batch Loss: 0.11428935825824738\n",
      "Epoch 781, Loss: 0.5465759932994843, Final Batch Loss: 0.14342378079891205\n",
      "Epoch 782, Loss: 0.5026600509881973, Final Batch Loss: 0.13738368451595306\n",
      "Epoch 783, Loss: 0.5054651349782944, Final Batch Loss: 0.11252402514219284\n",
      "Epoch 784, Loss: 0.43457549065351486, Final Batch Loss: 0.09191177785396576\n",
      "Epoch 785, Loss: 0.47502487152814865, Final Batch Loss: 0.10839816182851791\n",
      "Epoch 786, Loss: 0.4758017361164093, Final Batch Loss: 0.1505826711654663\n",
      "Epoch 787, Loss: 0.45481232553720474, Final Batch Loss: 0.09664563834667206\n",
      "Epoch 788, Loss: 0.43611469119787216, Final Batch Loss: 0.0781266987323761\n",
      "Epoch 789, Loss: 0.4395349696278572, Final Batch Loss: 0.06483122706413269\n",
      "Epoch 790, Loss: 0.5350386798381805, Final Batch Loss: 0.10365042835474014\n",
      "Epoch 791, Loss: 0.43993647396564484, Final Batch Loss: 0.0968719944357872\n",
      "Epoch 792, Loss: 0.48431409895420074, Final Batch Loss: 0.0794169008731842\n",
      "Epoch 793, Loss: 0.536176323890686, Final Batch Loss: 0.16470062732696533\n",
      "Epoch 794, Loss: 0.4721008390188217, Final Batch Loss: 0.125613272190094\n",
      "Epoch 795, Loss: 0.5637684464454651, Final Batch Loss: 0.16813813149929047\n",
      "Epoch 796, Loss: 0.48707742989063263, Final Batch Loss: 0.10459786653518677\n",
      "Epoch 797, Loss: 0.4418356269598007, Final Batch Loss: 0.0530867725610733\n",
      "Epoch 798, Loss: 0.49419206380844116, Final Batch Loss: 0.10915179550647736\n",
      "Epoch 799, Loss: 0.7600160837173462, Final Batch Loss: 0.3418004512786865\n",
      "Epoch 800, Loss: 0.5038260221481323, Final Batch Loss: 0.14333213865756989\n",
      "Epoch 801, Loss: 0.5330453589558601, Final Batch Loss: 0.14076530933380127\n",
      "Epoch 802, Loss: 0.5325061604380608, Final Batch Loss: 0.13191400468349457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 803, Loss: 0.4738389253616333, Final Batch Loss: 0.10024018585681915\n",
      "Epoch 804, Loss: 0.4259011298418045, Final Batch Loss: 0.07813809812068939\n",
      "Epoch 805, Loss: 0.5168446823954582, Final Batch Loss: 0.13665711879730225\n",
      "Epoch 806, Loss: 0.5401217266917229, Final Batch Loss: 0.18500255048274994\n",
      "Epoch 807, Loss: 0.6122609004378319, Final Batch Loss: 0.22603978216648102\n",
      "Epoch 808, Loss: 0.4985056668519974, Final Batch Loss: 0.0849853977560997\n",
      "Epoch 809, Loss: 0.47513195127248764, Final Batch Loss: 0.10642771422863007\n",
      "Epoch 810, Loss: 0.5290291011333466, Final Batch Loss: 0.10757718980312347\n",
      "Epoch 811, Loss: 0.4873933047056198, Final Batch Loss: 0.10284513235092163\n",
      "Epoch 812, Loss: 0.51285320520401, Final Batch Loss: 0.13181523978710175\n",
      "Epoch 813, Loss: 0.5381122007966042, Final Batch Loss: 0.13932332396507263\n",
      "Epoch 814, Loss: 0.5028945282101631, Final Batch Loss: 0.13496717810630798\n",
      "Epoch 815, Loss: 0.5128909945487976, Final Batch Loss: 0.12236345559358597\n",
      "Epoch 816, Loss: 0.4469035863876343, Final Batch Loss: 0.06471678614616394\n",
      "Epoch 817, Loss: 0.5074153989553452, Final Batch Loss: 0.07104424387216568\n",
      "Epoch 818, Loss: 0.4870525896549225, Final Batch Loss: 0.12304183095693588\n",
      "Epoch 819, Loss: 0.5304481834173203, Final Batch Loss: 0.16409312188625336\n",
      "Epoch 820, Loss: 0.4468386322259903, Final Batch Loss: 0.084103062748909\n",
      "Epoch 821, Loss: 0.4726675897836685, Final Batch Loss: 0.08840605616569519\n",
      "Epoch 822, Loss: 0.4419260397553444, Final Batch Loss: 0.1452852040529251\n",
      "Epoch 823, Loss: 0.4782422408461571, Final Batch Loss: 0.10301848500967026\n",
      "Epoch 824, Loss: 0.5089973509311676, Final Batch Loss: 0.09382063150405884\n",
      "Epoch 825, Loss: 0.4526531472802162, Final Batch Loss: 0.1269213855266571\n",
      "Epoch 826, Loss: 0.5155146270990372, Final Batch Loss: 0.13841941952705383\n",
      "Epoch 827, Loss: 0.5254600346088409, Final Batch Loss: 0.11113226413726807\n",
      "Epoch 828, Loss: 0.4734441637992859, Final Batch Loss: 0.06677985936403275\n",
      "Epoch 829, Loss: 0.38031088560819626, Final Batch Loss: 0.07872714102268219\n",
      "Epoch 830, Loss: 0.48041122406721115, Final Batch Loss: 0.14318010210990906\n",
      "Epoch 831, Loss: 0.5833598598837852, Final Batch Loss: 0.2959035336971283\n",
      "Epoch 832, Loss: 0.5067369192838669, Final Batch Loss: 0.14052794873714447\n",
      "Epoch 833, Loss: 0.5749397054314613, Final Batch Loss: 0.18436400592327118\n",
      "Epoch 834, Loss: 0.6748023480176926, Final Batch Loss: 0.28507015109062195\n",
      "Epoch 835, Loss: 0.4724867641925812, Final Batch Loss: 0.15347598493099213\n",
      "Epoch 836, Loss: 0.45754171907901764, Final Batch Loss: 0.09047947078943253\n",
      "Epoch 837, Loss: 0.49306267499923706, Final Batch Loss: 0.10412551462650299\n",
      "Epoch 838, Loss: 0.47201021760702133, Final Batch Loss: 0.08943969756364822\n",
      "Epoch 839, Loss: 0.4752104803919792, Final Batch Loss: 0.09670688211917877\n",
      "Epoch 840, Loss: 0.523415096104145, Final Batch Loss: 0.18140128254890442\n",
      "Epoch 841, Loss: 0.4100723974406719, Final Batch Loss: 0.06153854355216026\n",
      "Epoch 842, Loss: 0.513208344578743, Final Batch Loss: 0.08043000847101212\n",
      "Epoch 843, Loss: 0.5201274380087852, Final Batch Loss: 0.13163520395755768\n",
      "Epoch 844, Loss: 0.4420629069209099, Final Batch Loss: 0.08876806497573853\n",
      "Epoch 845, Loss: 0.5045900195837021, Final Batch Loss: 0.15091989934444427\n",
      "Epoch 846, Loss: 0.4362095445394516, Final Batch Loss: 0.047939762473106384\n",
      "Epoch 847, Loss: 0.5256698876619339, Final Batch Loss: 0.16586273908615112\n",
      "Epoch 848, Loss: 0.4068622700870037, Final Batch Loss: 0.05545055493712425\n",
      "Epoch 849, Loss: 0.4481469765305519, Final Batch Loss: 0.13075366616249084\n",
      "Epoch 850, Loss: 0.4418208375573158, Final Batch Loss: 0.14557217061519623\n",
      "Epoch 851, Loss: 0.5202986747026443, Final Batch Loss: 0.17558173835277557\n",
      "Epoch 852, Loss: 0.5008509606122971, Final Batch Loss: 0.1471397429704666\n",
      "Epoch 853, Loss: 0.5261870101094246, Final Batch Loss: 0.2102913111448288\n",
      "Epoch 854, Loss: 0.5408290922641754, Final Batch Loss: 0.21525873243808746\n",
      "Epoch 855, Loss: 0.5575153157114983, Final Batch Loss: 0.14973539113998413\n",
      "Epoch 856, Loss: 0.4885016977787018, Final Batch Loss: 0.1688319593667984\n",
      "Epoch 857, Loss: 0.4720405489206314, Final Batch Loss: 0.13967154920101166\n",
      "Epoch 858, Loss: 0.46214011311531067, Final Batch Loss: 0.14219120144844055\n",
      "Epoch 859, Loss: 0.5608381778001785, Final Batch Loss: 0.08994945883750916\n",
      "Epoch 860, Loss: 0.6328719854354858, Final Batch Loss: 0.2511882781982422\n",
      "Epoch 861, Loss: 0.4396943598985672, Final Batch Loss: 0.09247846156358719\n",
      "Epoch 862, Loss: 0.4860779643058777, Final Batch Loss: 0.16007830202579498\n",
      "Epoch 863, Loss: 0.5041135549545288, Final Batch Loss: 0.21497546136379242\n",
      "Epoch 864, Loss: 0.3875676542520523, Final Batch Loss: 0.07603785395622253\n",
      "Epoch 865, Loss: 0.5377201810479164, Final Batch Loss: 0.12538310885429382\n",
      "Epoch 866, Loss: 0.37418655306100845, Final Batch Loss: 0.10497687757015228\n",
      "Epoch 867, Loss: 0.4893421530723572, Final Batch Loss: 0.15268853306770325\n",
      "Epoch 868, Loss: 0.4839264079928398, Final Batch Loss: 0.13027423620224\n",
      "Epoch 869, Loss: 0.47718239575624466, Final Batch Loss: 0.11197026073932648\n",
      "Epoch 870, Loss: 0.49357596412301064, Final Batch Loss: 0.05900240316987038\n",
      "Epoch 871, Loss: 0.4477598965167999, Final Batch Loss: 0.14012007415294647\n",
      "Epoch 872, Loss: 0.4867304861545563, Final Batch Loss: 0.14544813334941864\n",
      "Epoch 873, Loss: 0.46988794207572937, Final Batch Loss: 0.14189130067825317\n",
      "Epoch 874, Loss: 0.46834469586610794, Final Batch Loss: 0.1485268473625183\n",
      "Epoch 875, Loss: 0.49038466066122055, Final Batch Loss: 0.13796170055866241\n",
      "Epoch 876, Loss: 0.5269896686077118, Final Batch Loss: 0.1890120655298233\n",
      "Epoch 877, Loss: 0.4577159658074379, Final Batch Loss: 0.142069473862648\n",
      "Epoch 878, Loss: 0.4955022484064102, Final Batch Loss: 0.13296368718147278\n",
      "Epoch 879, Loss: 0.45831916481256485, Final Batch Loss: 0.1782255321741104\n",
      "Epoch 880, Loss: 0.47033102065324783, Final Batch Loss: 0.15409503877162933\n",
      "Epoch 881, Loss: 0.4862615242600441, Final Batch Loss: 0.1509181559085846\n",
      "Epoch 882, Loss: 0.4246245473623276, Final Batch Loss: 0.10188433527946472\n",
      "Epoch 883, Loss: 0.40578994899988174, Final Batch Loss: 0.052960678935050964\n",
      "Epoch 884, Loss: 0.49656131118535995, Final Batch Loss: 0.12853260338306427\n",
      "Epoch 885, Loss: 0.4278462454676628, Final Batch Loss: 0.12880802154541016\n",
      "Epoch 886, Loss: 0.44548678398132324, Final Batch Loss: 0.0727216824889183\n",
      "Epoch 887, Loss: 0.4429527148604393, Final Batch Loss: 0.14765042066574097\n",
      "Epoch 888, Loss: 0.3884638175368309, Final Batch Loss: 0.06360361725091934\n",
      "Epoch 889, Loss: 0.4421784207224846, Final Batch Loss: 0.10846894979476929\n",
      "Epoch 890, Loss: 0.5508414804935455, Final Batch Loss: 0.17079727351665497\n",
      "Epoch 891, Loss: 0.34155745431780815, Final Batch Loss: 0.037590596824884415\n",
      "Epoch 892, Loss: 0.3965957537293434, Final Batch Loss: 0.07070118188858032\n",
      "Epoch 893, Loss: 0.4589087590575218, Final Batch Loss: 0.09694653004407883\n",
      "Epoch 894, Loss: 0.45915690064430237, Final Batch Loss: 0.07969297468662262\n",
      "Epoch 895, Loss: 0.4195759445428848, Final Batch Loss: 0.09781202673912048\n",
      "Epoch 896, Loss: 0.44023001194000244, Final Batch Loss: 0.10314217209815979\n",
      "Epoch 897, Loss: 0.5082096382975578, Final Batch Loss: 0.11571051180362701\n",
      "Epoch 898, Loss: 0.5494376793503761, Final Batch Loss: 0.16522593796253204\n",
      "Epoch 899, Loss: 0.44432633370161057, Final Batch Loss: 0.10311833769083023\n",
      "Epoch 900, Loss: 0.4034029506146908, Final Batch Loss: 0.043268393725156784\n",
      "Epoch 901, Loss: 0.44832200556993484, Final Batch Loss: 0.10488397628068924\n",
      "Epoch 902, Loss: 0.40976858884096146, Final Batch Loss: 0.09901750832796097\n",
      "Epoch 903, Loss: 0.4704250544309616, Final Batch Loss: 0.16426284611225128\n",
      "Epoch 904, Loss: 0.4538423418998718, Final Batch Loss: 0.0853886678814888\n",
      "Epoch 905, Loss: 0.5127543210983276, Final Batch Loss: 0.16613483428955078\n",
      "Epoch 906, Loss: 0.5924938097596169, Final Batch Loss: 0.08617640286684036\n",
      "Epoch 907, Loss: 0.5296612679958344, Final Batch Loss: 0.135883629322052\n",
      "Epoch 908, Loss: 0.5073533356189728, Final Batch Loss: 0.09296546131372452\n",
      "Epoch 909, Loss: 0.4969836696982384, Final Batch Loss: 0.18001890182495117\n",
      "Epoch 910, Loss: 0.4425751641392708, Final Batch Loss: 0.10793335735797882\n",
      "Epoch 911, Loss: 0.44806522130966187, Final Batch Loss: 0.10400207340717316\n",
      "Epoch 912, Loss: 0.40120501816272736, Final Batch Loss: 0.06903567165136337\n",
      "Epoch 913, Loss: 0.4317452609539032, Final Batch Loss: 0.07261215150356293\n",
      "Epoch 914, Loss: 0.4876680448651314, Final Batch Loss: 0.189083993434906\n",
      "Epoch 915, Loss: 0.42358046025037766, Final Batch Loss: 0.09843870997428894\n",
      "Epoch 916, Loss: 0.40310800075531006, Final Batch Loss: 0.0734778568148613\n",
      "Epoch 917, Loss: 0.4186319671571255, Final Batch Loss: 0.07623061537742615\n",
      "Epoch 918, Loss: 0.3775720000267029, Final Batch Loss: 0.0793469026684761\n",
      "Epoch 919, Loss: 0.40591298788785934, Final Batch Loss: 0.10610567033290863\n",
      "Epoch 920, Loss: 0.3791751489043236, Final Batch Loss: 0.07940495014190674\n",
      "Epoch 921, Loss: 0.5183328911662102, Final Batch Loss: 0.19370053708553314\n",
      "Epoch 922, Loss: 0.3811720460653305, Final Batch Loss: 0.11392832547426224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 923, Loss: 0.4550352916121483, Final Batch Loss: 0.10969989746809006\n",
      "Epoch 924, Loss: 0.37641339004039764, Final Batch Loss: 0.06522340327501297\n",
      "Epoch 925, Loss: 0.4593811258673668, Final Batch Loss: 0.1437612622976303\n",
      "Epoch 926, Loss: 0.47621408849954605, Final Batch Loss: 0.10643588751554489\n",
      "Epoch 927, Loss: 0.45911508053541183, Final Batch Loss: 0.09670136868953705\n",
      "Epoch 928, Loss: 0.5799301341176033, Final Batch Loss: 0.20046687126159668\n",
      "Epoch 929, Loss: 0.443289078772068, Final Batch Loss: 0.1130673885345459\n",
      "Epoch 930, Loss: 0.4987741634249687, Final Batch Loss: 0.17699430882930756\n",
      "Epoch 931, Loss: 0.3977965638041496, Final Batch Loss: 0.08175494521856308\n",
      "Epoch 932, Loss: 0.5347399413585663, Final Batch Loss: 0.20421446859836578\n",
      "Epoch 933, Loss: 0.4499878063797951, Final Batch Loss: 0.10263676941394806\n",
      "Epoch 934, Loss: 0.42957427352666855, Final Batch Loss: 0.07410098612308502\n",
      "Epoch 935, Loss: 0.5219073295593262, Final Batch Loss: 0.16209827363491058\n",
      "Epoch 936, Loss: 0.4451686814427376, Final Batch Loss: 0.09239497780799866\n",
      "Epoch 937, Loss: 0.5061626732349396, Final Batch Loss: 0.18969401717185974\n",
      "Epoch 938, Loss: 0.48601803183555603, Final Batch Loss: 0.10378912091255188\n",
      "Epoch 939, Loss: 0.3805727884173393, Final Batch Loss: 0.08381875604391098\n",
      "Epoch 940, Loss: 0.4543346166610718, Final Batch Loss: 0.13326866924762726\n",
      "Epoch 941, Loss: 0.5141221210360527, Final Batch Loss: 0.14870862662792206\n",
      "Epoch 942, Loss: 0.5188166201114655, Final Batch Loss: 0.19836698472499847\n",
      "Epoch 943, Loss: 0.4521772488951683, Final Batch Loss: 0.12542963027954102\n",
      "Epoch 944, Loss: 0.5015366598963737, Final Batch Loss: 0.1791587471961975\n",
      "Epoch 945, Loss: 0.49387477338314056, Final Batch Loss: 0.14516262710094452\n",
      "Epoch 946, Loss: 0.389432929456234, Final Batch Loss: 0.0785970389842987\n",
      "Epoch 947, Loss: 0.52407006919384, Final Batch Loss: 0.16852514445781708\n",
      "Epoch 948, Loss: 0.40588729828596115, Final Batch Loss: 0.1350594311952591\n",
      "Epoch 949, Loss: 0.39210884273052216, Final Batch Loss: 0.09794778376817703\n",
      "Epoch 950, Loss: 0.42326319962739944, Final Batch Loss: 0.09295741468667984\n",
      "Epoch 951, Loss: 0.476300872862339, Final Batch Loss: 0.13886211812496185\n",
      "Epoch 952, Loss: 0.44461490213871, Final Batch Loss: 0.12513919174671173\n",
      "Epoch 953, Loss: 0.443863645195961, Final Batch Loss: 0.09273000061511993\n",
      "Epoch 954, Loss: 0.48590904474258423, Final Batch Loss: 0.10921405255794525\n",
      "Epoch 955, Loss: 0.455743245780468, Final Batch Loss: 0.15673547983169556\n",
      "Epoch 956, Loss: 0.40823041647672653, Final Batch Loss: 0.11313606798648834\n",
      "Epoch 957, Loss: 0.42692844569683075, Final Batch Loss: 0.11951775848865509\n",
      "Epoch 958, Loss: 0.39684373140335083, Final Batch Loss: 0.060411237180233\n",
      "Epoch 959, Loss: 0.40111157670617104, Final Batch Loss: 0.11743045598268509\n",
      "Epoch 960, Loss: 0.41344543546438217, Final Batch Loss: 0.08904919028282166\n",
      "Epoch 961, Loss: 0.46810282766819, Final Batch Loss: 0.086766317486763\n",
      "Epoch 962, Loss: 0.45465654879808426, Final Batch Loss: 0.07251853495836258\n",
      "Epoch 963, Loss: 0.41773662716150284, Final Batch Loss: 0.1268741339445114\n",
      "Epoch 964, Loss: 0.4151305556297302, Final Batch Loss: 0.11895301192998886\n",
      "Epoch 965, Loss: 0.384210966527462, Final Batch Loss: 0.08234109729528427\n",
      "Epoch 966, Loss: 0.4155951142311096, Final Batch Loss: 0.09962494671344757\n",
      "Epoch 967, Loss: 0.4468091204762459, Final Batch Loss: 0.10811638087034225\n",
      "Epoch 968, Loss: 0.4827118217945099, Final Batch Loss: 0.14743344485759735\n",
      "Epoch 969, Loss: 0.41334058344364166, Final Batch Loss: 0.11063622683286667\n",
      "Epoch 970, Loss: 0.4191390797495842, Final Batch Loss: 0.11521715670824051\n",
      "Epoch 971, Loss: 0.5011943876743317, Final Batch Loss: 0.07377129048109055\n",
      "Epoch 972, Loss: 0.46448037028312683, Final Batch Loss: 0.1292600780725479\n",
      "Epoch 973, Loss: 0.4465437978506088, Final Batch Loss: 0.11335278302431107\n",
      "Epoch 974, Loss: 0.3560921251773834, Final Batch Loss: 0.06465337425470352\n",
      "Epoch 975, Loss: 0.39290858060121536, Final Batch Loss: 0.06745828688144684\n",
      "Epoch 976, Loss: 0.503933772444725, Final Batch Loss: 0.18060971796512604\n",
      "Epoch 977, Loss: 0.3475332036614418, Final Batch Loss: 0.037935465574264526\n",
      "Epoch 978, Loss: 0.3999554067850113, Final Batch Loss: 0.10251417011022568\n",
      "Epoch 979, Loss: 0.38827139884233475, Final Batch Loss: 0.1168651357293129\n",
      "Epoch 980, Loss: 0.4595205634832382, Final Batch Loss: 0.12951308488845825\n",
      "Epoch 981, Loss: 0.36901385337114334, Final Batch Loss: 0.05856284499168396\n",
      "Epoch 982, Loss: 0.451808325946331, Final Batch Loss: 0.0998566672205925\n",
      "Epoch 983, Loss: 0.37424398958683014, Final Batch Loss: 0.0795777216553688\n",
      "Epoch 984, Loss: 0.4450894594192505, Final Batch Loss: 0.13689443469047546\n",
      "Epoch 985, Loss: 0.3771643824875355, Final Batch Loss: 0.05740373209118843\n",
      "Epoch 986, Loss: 0.37685526907444, Final Batch Loss: 0.08507639169692993\n",
      "Epoch 987, Loss: 0.4448191225528717, Final Batch Loss: 0.11165732890367508\n",
      "Epoch 988, Loss: 0.39198704808950424, Final Batch Loss: 0.11092787235975266\n",
      "Epoch 989, Loss: 0.45775411278009415, Final Batch Loss: 0.14588160812854767\n",
      "Epoch 990, Loss: 0.39617660641670227, Final Batch Loss: 0.11086618900299072\n",
      "Epoch 991, Loss: 0.49249279499053955, Final Batch Loss: 0.16457761824131012\n",
      "Epoch 992, Loss: 0.42633019387722015, Final Batch Loss: 0.11290941387414932\n",
      "Epoch 993, Loss: 0.37694481760263443, Final Batch Loss: 0.06695763021707535\n",
      "Epoch 994, Loss: 0.4158046618103981, Final Batch Loss: 0.07764825969934464\n",
      "Epoch 995, Loss: 0.5270884707570076, Final Batch Loss: 0.2104494422674179\n",
      "Epoch 996, Loss: 0.38111022859811783, Final Batch Loss: 0.10573329776525497\n",
      "Epoch 997, Loss: 0.3582977466285229, Final Batch Loss: 0.09186367690563202\n",
      "Epoch 998, Loss: 0.34093670919537544, Final Batch Loss: 0.06164446845650673\n",
      "Epoch 999, Loss: 0.44470952451229095, Final Batch Loss: 0.11655693501234055\n",
      "Epoch 1000, Loss: 0.4361160546541214, Final Batch Loss: 0.10048222541809082\n",
      "Epoch 1001, Loss: 0.44458065181970596, Final Batch Loss: 0.10884448140859604\n",
      "Epoch 1002, Loss: 0.4315151795744896, Final Batch Loss: 0.08651852607727051\n",
      "Epoch 1003, Loss: 0.3997866287827492, Final Batch Loss: 0.0692187026143074\n",
      "Epoch 1004, Loss: 0.4397691711783409, Final Batch Loss: 0.12419673055410385\n",
      "Epoch 1005, Loss: 0.3545268103480339, Final Batch Loss: 0.06953667104244232\n",
      "Epoch 1006, Loss: 0.5071221962571144, Final Batch Loss: 0.12772919237613678\n",
      "Epoch 1007, Loss: 0.44602010399103165, Final Batch Loss: 0.15802015364170074\n",
      "Epoch 1008, Loss: 0.4587034359574318, Final Batch Loss: 0.09116613119840622\n",
      "Epoch 1009, Loss: 0.4723980352282524, Final Batch Loss: 0.0892869383096695\n",
      "Epoch 1010, Loss: 0.48502953350543976, Final Batch Loss: 0.20021174848079681\n",
      "Epoch 1011, Loss: 0.3512517809867859, Final Batch Loss: 0.08400209248065948\n",
      "Epoch 1012, Loss: 0.3909371867775917, Final Batch Loss: 0.0446566641330719\n",
      "Epoch 1013, Loss: 0.46788617223501205, Final Batch Loss: 0.1211266815662384\n",
      "Epoch 1014, Loss: 0.3891632705926895, Final Batch Loss: 0.1025753766298294\n",
      "Epoch 1015, Loss: 0.428862601518631, Final Batch Loss: 0.08195097744464874\n",
      "Epoch 1016, Loss: 0.38755206018686295, Final Batch Loss: 0.08282535523176193\n",
      "Epoch 1017, Loss: 0.4029209576547146, Final Batch Loss: 0.05743144080042839\n",
      "Epoch 1018, Loss: 0.41377899050712585, Final Batch Loss: 0.09963880479335785\n",
      "Epoch 1019, Loss: 0.42508482187986374, Final Batch Loss: 0.10102478414773941\n",
      "Epoch 1020, Loss: 0.3857068419456482, Final Batch Loss: 0.11019361019134521\n",
      "Epoch 1021, Loss: 0.3335619606077671, Final Batch Loss: 0.036214884370565414\n",
      "Epoch 1022, Loss: 0.4790859594941139, Final Batch Loss: 0.19408825039863586\n",
      "Epoch 1023, Loss: 0.5156432315707207, Final Batch Loss: 0.2170216292142868\n",
      "Epoch 1024, Loss: 0.3876524344086647, Final Batch Loss: 0.08789271861314774\n",
      "Epoch 1025, Loss: 0.530917651951313, Final Batch Loss: 0.20573952794075012\n",
      "Epoch 1026, Loss: 0.3554121293127537, Final Batch Loss: 0.10689328610897064\n",
      "Epoch 1027, Loss: 0.3886334151029587, Final Batch Loss: 0.09753203392028809\n",
      "Epoch 1028, Loss: 0.366012379527092, Final Batch Loss: 0.06516491621732712\n",
      "Epoch 1029, Loss: 0.46140865236520767, Final Batch Loss: 0.1169235035777092\n",
      "Epoch 1030, Loss: 0.522094190120697, Final Batch Loss: 0.179104745388031\n",
      "Epoch 1031, Loss: 0.390878364443779, Final Batch Loss: 0.08164770156145096\n",
      "Epoch 1032, Loss: 0.36998772621154785, Final Batch Loss: 0.10181151330471039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1033, Loss: 0.4087952747941017, Final Batch Loss: 0.11024050414562225\n",
      "Epoch 1034, Loss: 0.44138773530721664, Final Batch Loss: 0.152287095785141\n",
      "Epoch 1035, Loss: 0.38432837277650833, Final Batch Loss: 0.11152684688568115\n",
      "Epoch 1036, Loss: 0.5099041238427162, Final Batch Loss: 0.21670745313167572\n",
      "Epoch 1037, Loss: 0.39950308203697205, Final Batch Loss: 0.09852968901395798\n",
      "Epoch 1038, Loss: 0.406957283616066, Final Batch Loss: 0.0688476487994194\n",
      "Epoch 1039, Loss: 0.37871088087558746, Final Batch Loss: 0.08661700040102005\n",
      "Epoch 1040, Loss: 0.47090328484773636, Final Batch Loss: 0.1363619863986969\n",
      "Epoch 1041, Loss: 0.42673759162425995, Final Batch Loss: 0.12158770114183426\n",
      "Epoch 1042, Loss: 0.39753713458776474, Final Batch Loss: 0.13160523772239685\n",
      "Epoch 1043, Loss: 0.42190222442150116, Final Batch Loss: 0.11293521523475647\n",
      "Epoch 1044, Loss: 0.3522323742508888, Final Batch Loss: 0.08458264172077179\n",
      "Epoch 1045, Loss: 0.41589973866939545, Final Batch Loss: 0.12898340821266174\n",
      "Epoch 1046, Loss: 0.3616073876619339, Final Batch Loss: 0.07047272473573685\n",
      "Epoch 1047, Loss: 0.363383524119854, Final Batch Loss: 0.07021414488554001\n",
      "Epoch 1048, Loss: 0.41143036633729935, Final Batch Loss: 0.0930219292640686\n",
      "Epoch 1049, Loss: 0.3816630281507969, Final Batch Loss: 0.05845058336853981\n",
      "Epoch 1050, Loss: 0.3530086353421211, Final Batch Loss: 0.08874697238206863\n",
      "Epoch 1051, Loss: 0.3684257008135319, Final Batch Loss: 0.12788626551628113\n",
      "Epoch 1052, Loss: 0.35049913823604584, Final Batch Loss: 0.061761267483234406\n",
      "Epoch 1053, Loss: 0.3584945648908615, Final Batch Loss: 0.09463000297546387\n",
      "Epoch 1054, Loss: 0.35009555518627167, Final Batch Loss: 0.081197090446949\n",
      "Epoch 1055, Loss: 0.3499711826443672, Final Batch Loss: 0.09135061502456665\n",
      "Epoch 1056, Loss: 0.40002845227718353, Final Batch Loss: 0.0648418441414833\n",
      "Epoch 1057, Loss: 0.5007126182317734, Final Batch Loss: 0.19759440422058105\n",
      "Epoch 1058, Loss: 0.41285840421915054, Final Batch Loss: 0.13038615882396698\n",
      "Epoch 1059, Loss: 0.4777636379003525, Final Batch Loss: 0.12238109856843948\n",
      "Epoch 1060, Loss: 0.4037044197320938, Final Batch Loss: 0.09175147861242294\n",
      "Epoch 1061, Loss: 0.40692636370658875, Final Batch Loss: 0.12465900182723999\n",
      "Epoch 1062, Loss: 0.3420299366116524, Final Batch Loss: 0.09343046694993973\n",
      "Epoch 1063, Loss: 0.3941977620124817, Final Batch Loss: 0.10569719970226288\n",
      "Epoch 1064, Loss: 0.5010936483740807, Final Batch Loss: 0.18422679603099823\n",
      "Epoch 1065, Loss: 0.43294230848550797, Final Batch Loss: 0.09763243049383163\n",
      "Epoch 1066, Loss: 0.574545830488205, Final Batch Loss: 0.2249179631471634\n",
      "Epoch 1067, Loss: 0.38159117102622986, Final Batch Loss: 0.10497507452964783\n",
      "Epoch 1068, Loss: 0.45501135289669037, Final Batch Loss: 0.12501364946365356\n",
      "Epoch 1069, Loss: 0.3678695410490036, Final Batch Loss: 0.07831737399101257\n",
      "Epoch 1070, Loss: 0.45413537323474884, Final Batch Loss: 0.1295415610074997\n",
      "Epoch 1071, Loss: 0.31071820110082626, Final Batch Loss: 0.06838444620370865\n",
      "Epoch 1072, Loss: 0.394353412091732, Final Batch Loss: 0.08385448902845383\n",
      "Epoch 1073, Loss: 0.34332189708948135, Final Batch Loss: 0.07170357555150986\n",
      "Epoch 1074, Loss: 0.39945224672555923, Final Batch Loss: 0.08175946027040482\n",
      "Epoch 1075, Loss: 0.3308388441801071, Final Batch Loss: 0.06078588217496872\n",
      "Epoch 1076, Loss: 0.47715259343385696, Final Batch Loss: 0.06887034326791763\n",
      "Epoch 1077, Loss: 0.3394985944032669, Final Batch Loss: 0.0766647607088089\n",
      "Epoch 1078, Loss: 0.40506964176893234, Final Batch Loss: 0.14014500379562378\n",
      "Epoch 1079, Loss: 0.3675783649086952, Final Batch Loss: 0.07044550031423569\n",
      "Epoch 1080, Loss: 0.4026188924908638, Final Batch Loss: 0.09117326885461807\n",
      "Epoch 1081, Loss: 0.4229642152786255, Final Batch Loss: 0.09812682121992111\n",
      "Epoch 1082, Loss: 0.3779132440686226, Final Batch Loss: 0.08653928339481354\n",
      "Epoch 1083, Loss: 0.442887581884861, Final Batch Loss: 0.1187821477651596\n",
      "Epoch 1084, Loss: 0.3279455825686455, Final Batch Loss: 0.09758368879556656\n",
      "Epoch 1085, Loss: 0.40368328243494034, Final Batch Loss: 0.09105627983808517\n",
      "Epoch 1086, Loss: 0.37528449296951294, Final Batch Loss: 0.12287263572216034\n",
      "Epoch 1087, Loss: 0.3050213139504194, Final Batch Loss: 0.021401403471827507\n",
      "Epoch 1088, Loss: 0.37022407725453377, Final Batch Loss: 0.041743721812963486\n",
      "Epoch 1089, Loss: 0.4894116446375847, Final Batch Loss: 0.1582002341747284\n",
      "Epoch 1090, Loss: 0.5427039749920368, Final Batch Loss: 0.25231483578681946\n",
      "Epoch 1091, Loss: 0.46822864562273026, Final Batch Loss: 0.16276578605175018\n",
      "Epoch 1092, Loss: 0.36840733885765076, Final Batch Loss: 0.10624179244041443\n",
      "Epoch 1093, Loss: 0.4544149935245514, Final Batch Loss: 0.1170327216386795\n",
      "Epoch 1094, Loss: 0.4227591045200825, Final Batch Loss: 0.14244228601455688\n",
      "Epoch 1095, Loss: 0.3900160975754261, Final Batch Loss: 0.046782661229372025\n",
      "Epoch 1096, Loss: 0.49347154051065445, Final Batch Loss: 0.12990818917751312\n",
      "Epoch 1097, Loss: 0.36518796160817146, Final Batch Loss: 0.04046425595879555\n",
      "Epoch 1098, Loss: 0.45179155468940735, Final Batch Loss: 0.11332336068153381\n",
      "Epoch 1099, Loss: 0.41792692989110947, Final Batch Loss: 0.09071974456310272\n",
      "Epoch 1100, Loss: 0.39986037462949753, Final Batch Loss: 0.08575205504894257\n",
      "Epoch 1101, Loss: 0.3705255761742592, Final Batch Loss: 0.08553751558065414\n",
      "Epoch 1102, Loss: 0.33767715096473694, Final Batch Loss: 0.061359234154224396\n",
      "Epoch 1103, Loss: 0.38974764198064804, Final Batch Loss: 0.07422276586294174\n",
      "Epoch 1104, Loss: 0.4275631532073021, Final Batch Loss: 0.1720375120639801\n",
      "Epoch 1105, Loss: 0.5199402868747711, Final Batch Loss: 0.20109820365905762\n",
      "Epoch 1106, Loss: 0.3611578121781349, Final Batch Loss: 0.06066497415304184\n",
      "Epoch 1107, Loss: 0.4064789265394211, Final Batch Loss: 0.10132701694965363\n",
      "Epoch 1108, Loss: 0.39679960906505585, Final Batch Loss: 0.13199906051158905\n",
      "Epoch 1109, Loss: 0.4556301161646843, Final Batch Loss: 0.08990798890590668\n",
      "Epoch 1110, Loss: 0.40421437472105026, Final Batch Loss: 0.12017304450273514\n",
      "Epoch 1111, Loss: 0.4398873522877693, Final Batch Loss: 0.1745554804801941\n",
      "Epoch 1112, Loss: 0.3401144668459892, Final Batch Loss: 0.08469490706920624\n",
      "Epoch 1113, Loss: 0.47295066714286804, Final Batch Loss: 0.07027166336774826\n",
      "Epoch 1114, Loss: 0.49846379458904266, Final Batch Loss: 0.11748223006725311\n",
      "Epoch 1115, Loss: 0.36250296235084534, Final Batch Loss: 0.09330058097839355\n",
      "Epoch 1116, Loss: 0.3995159938931465, Final Batch Loss: 0.11763521283864975\n",
      "Epoch 1117, Loss: 0.42878614366054535, Final Batch Loss: 0.10899384319782257\n",
      "Epoch 1118, Loss: 0.3943435400724411, Final Batch Loss: 0.09604185819625854\n",
      "Epoch 1119, Loss: 0.37733864039182663, Final Batch Loss: 0.119040846824646\n",
      "Epoch 1120, Loss: 0.4271779954433441, Final Batch Loss: 0.14719492197036743\n",
      "Epoch 1121, Loss: 0.44603969901800156, Final Batch Loss: 0.16022373735904694\n",
      "Epoch 1122, Loss: 0.3764639273285866, Final Batch Loss: 0.08373512327671051\n",
      "Epoch 1123, Loss: 0.44115153700113297, Final Batch Loss: 0.09441414475440979\n",
      "Epoch 1124, Loss: 0.41440480202436447, Final Batch Loss: 0.0804317370057106\n",
      "Epoch 1125, Loss: 0.4202999174594879, Final Batch Loss: 0.12624575197696686\n",
      "Epoch 1126, Loss: 0.4038238003849983, Final Batch Loss: 0.0954347550868988\n",
      "Epoch 1127, Loss: 0.3362397141754627, Final Batch Loss: 0.05107492581009865\n",
      "Epoch 1128, Loss: 0.5035878643393517, Final Batch Loss: 0.2400386482477188\n",
      "Epoch 1129, Loss: 0.33113620057702065, Final Batch Loss: 0.03963087499141693\n",
      "Epoch 1130, Loss: 0.35636106133461, Final Batch Loss: 0.06851426512002945\n",
      "Epoch 1131, Loss: 0.41565751284360886, Final Batch Loss: 0.12853802740573883\n",
      "Epoch 1132, Loss: 0.3921244591474533, Final Batch Loss: 0.1443832516670227\n",
      "Epoch 1133, Loss: 0.3905121758580208, Final Batch Loss: 0.12212567776441574\n",
      "Epoch 1134, Loss: 0.4320681542158127, Final Batch Loss: 0.11050914227962494\n",
      "Epoch 1135, Loss: 0.3481238782405853, Final Batch Loss: 0.06380366533994675\n",
      "Epoch 1136, Loss: 0.39534851908683777, Final Batch Loss: 0.08867668360471725\n",
      "Epoch 1137, Loss: 0.35011597722768784, Final Batch Loss: 0.06515926867723465\n",
      "Epoch 1138, Loss: 0.41519445180892944, Final Batch Loss: 0.08204773813486099\n",
      "Epoch 1139, Loss: 0.37267960608005524, Final Batch Loss: 0.05968689173460007\n",
      "Epoch 1140, Loss: 0.3171442784368992, Final Batch Loss: 0.07250317186117172\n",
      "Epoch 1141, Loss: 0.3460787758231163, Final Batch Loss: 0.0773942843079567\n",
      "Epoch 1142, Loss: 0.3812289871275425, Final Batch Loss: 0.09419407695531845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1143, Loss: 0.3412860259413719, Final Batch Loss: 0.09583555907011032\n",
      "Epoch 1144, Loss: 0.41494715213775635, Final Batch Loss: 0.1363723874092102\n",
      "Epoch 1145, Loss: 0.3422917127609253, Final Batch Loss: 0.09697538614273071\n",
      "Epoch 1146, Loss: 0.4065222516655922, Final Batch Loss: 0.07176140695810318\n",
      "Epoch 1147, Loss: 0.44476959854364395, Final Batch Loss: 0.14009322226047516\n",
      "Epoch 1148, Loss: 0.40168581157922745, Final Batch Loss: 0.1015058159828186\n",
      "Epoch 1149, Loss: 0.44270144402980804, Final Batch Loss: 0.09654388576745987\n",
      "Epoch 1150, Loss: 0.3492010682821274, Final Batch Loss: 0.043921396136283875\n",
      "Epoch 1151, Loss: 0.365500595420599, Final Batch Loss: 0.0933549553155899\n",
      "Epoch 1152, Loss: 0.3211553767323494, Final Batch Loss: 0.07834114879369736\n",
      "Epoch 1153, Loss: 0.539318822324276, Final Batch Loss: 0.09071002155542374\n",
      "Epoch 1154, Loss: 0.38941648602485657, Final Batch Loss: 0.11953756213188171\n",
      "Epoch 1155, Loss: 0.4162010923027992, Final Batch Loss: 0.13791775703430176\n",
      "Epoch 1156, Loss: 0.41506053507328033, Final Batch Loss: 0.1581830531358719\n",
      "Epoch 1157, Loss: 0.3856862857937813, Final Batch Loss: 0.05123407393693924\n",
      "Epoch 1158, Loss: 0.3486532121896744, Final Batch Loss: 0.05207069218158722\n",
      "Epoch 1159, Loss: 0.3145478442311287, Final Batch Loss: 0.03721317648887634\n",
      "Epoch 1160, Loss: 0.3895137086510658, Final Batch Loss: 0.09996980428695679\n",
      "Epoch 1161, Loss: 0.45286593586206436, Final Batch Loss: 0.17455971240997314\n",
      "Epoch 1162, Loss: 0.4419376626610756, Final Batch Loss: 0.1300169974565506\n",
      "Epoch 1163, Loss: 0.4583804979920387, Final Batch Loss: 0.14032074809074402\n",
      "Epoch 1164, Loss: 0.3746165409684181, Final Batch Loss: 0.11628145724534988\n",
      "Epoch 1165, Loss: 0.3458653911948204, Final Batch Loss: 0.09707293659448624\n",
      "Epoch 1166, Loss: 0.42049604654312134, Final Batch Loss: 0.13439102470874786\n",
      "Epoch 1167, Loss: 0.3593752607703209, Final Batch Loss: 0.08104558289051056\n",
      "Epoch 1168, Loss: 0.3630319833755493, Final Batch Loss: 0.06623167544603348\n",
      "Epoch 1169, Loss: 0.35557765513658524, Final Batch Loss: 0.11668720096349716\n",
      "Epoch 1170, Loss: 0.32187315821647644, Final Batch Loss: 0.05131535977125168\n",
      "Epoch 1171, Loss: 0.3323531746864319, Final Batch Loss: 0.0806526467204094\n",
      "Epoch 1172, Loss: 0.43683306127786636, Final Batch Loss: 0.12484478205442429\n",
      "Epoch 1173, Loss: 0.37131156772375107, Final Batch Loss: 0.11465979367494583\n",
      "Epoch 1174, Loss: 0.3665773868560791, Final Batch Loss: 0.06880874186754227\n",
      "Epoch 1175, Loss: 0.32603931427001953, Final Batch Loss: 0.0806521400809288\n",
      "Epoch 1176, Loss: 0.33535461127758026, Final Batch Loss: 0.0598677322268486\n",
      "Epoch 1177, Loss: 0.29643555730581284, Final Batch Loss: 0.06311874836683273\n",
      "Epoch 1178, Loss: 0.4275052025914192, Final Batch Loss: 0.034696243703365326\n",
      "Epoch 1179, Loss: 0.30979087948799133, Final Batch Loss: 0.06651372462511063\n",
      "Epoch 1180, Loss: 0.36776087805628777, Final Batch Loss: 0.12922289967536926\n",
      "Epoch 1181, Loss: 0.34668152034282684, Final Batch Loss: 0.04897163808345795\n",
      "Epoch 1182, Loss: 0.34773971885442734, Final Batch Loss: 0.1051269993185997\n",
      "Epoch 1183, Loss: 0.4493717923760414, Final Batch Loss: 0.17194926738739014\n",
      "Epoch 1184, Loss: 0.484578900039196, Final Batch Loss: 0.1322360783815384\n",
      "Epoch 1185, Loss: 0.36075788736343384, Final Batch Loss: 0.06674651801586151\n",
      "Epoch 1186, Loss: 0.34430376440286636, Final Batch Loss: 0.058311961591243744\n",
      "Epoch 1187, Loss: 0.34576932713389397, Final Batch Loss: 0.044695813208818436\n",
      "Epoch 1188, Loss: 0.4289427064359188, Final Batch Loss: 0.06050059571862221\n",
      "Epoch 1189, Loss: 0.29164597019553185, Final Batch Loss: 0.041276175528764725\n",
      "Epoch 1190, Loss: 0.3842310644686222, Final Batch Loss: 0.1537373661994934\n",
      "Epoch 1191, Loss: 0.3761952593922615, Final Batch Loss: 0.09334441274404526\n",
      "Epoch 1192, Loss: 0.405733086168766, Final Batch Loss: 0.1264037787914276\n",
      "Epoch 1193, Loss: 0.3218023255467415, Final Batch Loss: 0.06597958505153656\n",
      "Epoch 1194, Loss: 0.3487808182835579, Final Batch Loss: 0.09247751533985138\n",
      "Epoch 1195, Loss: 0.3627037927508354, Final Batch Loss: 0.0585150420665741\n",
      "Epoch 1196, Loss: 0.42121659964323044, Final Batch Loss: 0.13748319447040558\n",
      "Epoch 1197, Loss: 0.3723033741116524, Final Batch Loss: 0.07686673849821091\n",
      "Epoch 1198, Loss: 0.37760619819164276, Final Batch Loss: 0.07120804488658905\n",
      "Epoch 1199, Loss: 0.327114462852478, Final Batch Loss: 0.08731336891651154\n",
      "Epoch 1200, Loss: 0.37233373522758484, Final Batch Loss: 0.06252261251211166\n",
      "Epoch 1201, Loss: 0.3253391906619072, Final Batch Loss: 0.07307376712560654\n",
      "Epoch 1202, Loss: 0.4015002027153969, Final Batch Loss: 0.08526761829853058\n",
      "Epoch 1203, Loss: 0.3662804663181305, Final Batch Loss: 0.1425916701555252\n",
      "Epoch 1204, Loss: 0.2978475019335747, Final Batch Loss: 0.06541861593723297\n",
      "Epoch 1205, Loss: 0.34585727006196976, Final Batch Loss: 0.05679605156183243\n",
      "Epoch 1206, Loss: 0.33674731105566025, Final Batch Loss: 0.11110682040452957\n",
      "Epoch 1207, Loss: 0.3657185509800911, Final Batch Loss: 0.10682307183742523\n",
      "Epoch 1208, Loss: 0.3575439453125, Final Batch Loss: 0.07084983587265015\n",
      "Epoch 1209, Loss: 0.2998465970158577, Final Batch Loss: 0.05566995590925217\n",
      "Epoch 1210, Loss: 0.34268176555633545, Final Batch Loss: 0.09550666809082031\n",
      "Epoch 1211, Loss: 0.41304638981819153, Final Batch Loss: 0.10447046160697937\n",
      "Epoch 1212, Loss: 0.4381251037120819, Final Batch Loss: 0.2409033328294754\n",
      "Epoch 1213, Loss: 0.32166143506765366, Final Batch Loss: 0.03529556095600128\n",
      "Epoch 1214, Loss: 0.3917151913046837, Final Batch Loss: 0.10569103807210922\n",
      "Epoch 1215, Loss: 0.35402151942253113, Final Batch Loss: 0.05994413048028946\n",
      "Epoch 1216, Loss: 0.3796583004295826, Final Batch Loss: 0.10673512518405914\n",
      "Epoch 1217, Loss: 0.3380168154835701, Final Batch Loss: 0.04716469347476959\n",
      "Epoch 1218, Loss: 0.3087982200086117, Final Batch Loss: 0.057939428836107254\n",
      "Epoch 1219, Loss: 0.33391986787319183, Final Batch Loss: 0.0907416045665741\n",
      "Epoch 1220, Loss: 0.3268230967223644, Final Batch Loss: 0.10654521733522415\n",
      "Epoch 1221, Loss: 0.36956655979156494, Final Batch Loss: 0.08005568385124207\n",
      "Epoch 1222, Loss: 0.43756598979234695, Final Batch Loss: 0.15434396266937256\n",
      "Epoch 1223, Loss: 0.3218935914337635, Final Batch Loss: 0.08991901576519012\n",
      "Epoch 1224, Loss: 0.35650669038295746, Final Batch Loss: 0.12466562539339066\n",
      "Epoch 1225, Loss: 0.4005092978477478, Final Batch Loss: 0.0997689962387085\n",
      "Epoch 1226, Loss: 0.3826979696750641, Final Batch Loss: 0.06340612471103668\n",
      "Epoch 1227, Loss: 0.3571764752268791, Final Batch Loss: 0.07962485402822495\n",
      "Epoch 1228, Loss: 0.38288623839616776, Final Batch Loss: 0.10696738958358765\n",
      "Epoch 1229, Loss: 0.4514720290899277, Final Batch Loss: 0.17165294289588928\n",
      "Epoch 1230, Loss: 0.40859561413526535, Final Batch Loss: 0.15051889419555664\n",
      "Epoch 1231, Loss: 0.48190607875585556, Final Batch Loss: 0.2214583158493042\n",
      "Epoch 1232, Loss: 0.4179205223917961, Final Batch Loss: 0.17136476933956146\n",
      "Epoch 1233, Loss: 0.3737860172986984, Final Batch Loss: 0.17335699498653412\n",
      "Epoch 1234, Loss: 0.33309341594576836, Final Batch Loss: 0.08908060938119888\n",
      "Epoch 1235, Loss: 0.4394928440451622, Final Batch Loss: 0.15932367742061615\n",
      "Epoch 1236, Loss: 0.42956535518169403, Final Batch Loss: 0.12819145619869232\n",
      "Epoch 1237, Loss: 0.3480028286576271, Final Batch Loss: 0.08421234786510468\n",
      "Epoch 1238, Loss: 0.326683945953846, Final Batch Loss: 0.07148232311010361\n",
      "Epoch 1239, Loss: 0.3799811154603958, Final Batch Loss: 0.08620777726173401\n",
      "Epoch 1240, Loss: 0.39048372581601143, Final Batch Loss: 0.131048783659935\n",
      "Epoch 1241, Loss: 0.3899351432919502, Final Batch Loss: 0.09387564659118652\n",
      "Epoch 1242, Loss: 0.3251226991415024, Final Batch Loss: 0.08980742841959\n",
      "Epoch 1243, Loss: 0.36663036048412323, Final Batch Loss: 0.10693676024675369\n",
      "Epoch 1244, Loss: 0.39187853783369064, Final Batch Loss: 0.10783270001411438\n",
      "Epoch 1245, Loss: 0.4030965305864811, Final Batch Loss: 0.14100730419158936\n",
      "Epoch 1246, Loss: 0.4174302741885185, Final Batch Loss: 0.09740197658538818\n",
      "Epoch 1247, Loss: 0.3851012662053108, Final Batch Loss: 0.08542914688587189\n",
      "Epoch 1248, Loss: 0.3997621312737465, Final Batch Loss: 0.1285107433795929\n",
      "Epoch 1249, Loss: 0.36969779431819916, Final Batch Loss: 0.08617039024829865\n",
      "Epoch 1250, Loss: 0.32895513623952866, Final Batch Loss: 0.06962765753269196\n",
      "Epoch 1251, Loss: 0.4710638001561165, Final Batch Loss: 0.22052913904190063\n",
      "Epoch 1252, Loss: 0.34997568279504776, Final Batch Loss: 0.09375062584877014\n",
      "Epoch 1253, Loss: 0.3888961151242256, Final Batch Loss: 0.09731120616197586\n",
      "Epoch 1254, Loss: 0.38653183728456497, Final Batch Loss: 0.1591721773147583\n",
      "Epoch 1255, Loss: 0.29871345683932304, Final Batch Loss: 0.0876317247748375\n",
      "Epoch 1256, Loss: 0.4252038076519966, Final Batch Loss: 0.06520034372806549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1257, Loss: 0.2604956552386284, Final Batch Loss: 0.05570496618747711\n",
      "Epoch 1258, Loss: 0.38192832469940186, Final Batch Loss: 0.07269515842199326\n",
      "Epoch 1259, Loss: 0.3316890224814415, Final Batch Loss: 0.11176060140132904\n",
      "Epoch 1260, Loss: 0.4174353703856468, Final Batch Loss: 0.1298753023147583\n",
      "Epoch 1261, Loss: 0.3944746069610119, Final Batch Loss: 0.15624083578586578\n",
      "Epoch 1262, Loss: 0.30125660821795464, Final Batch Loss: 0.07410155236721039\n",
      "Epoch 1263, Loss: 0.42158808559179306, Final Batch Loss: 0.13973619043827057\n",
      "Epoch 1264, Loss: 0.3542643189430237, Final Batch Loss: 0.06789342314004898\n",
      "Epoch 1265, Loss: 0.3569076508283615, Final Batch Loss: 0.1038542240858078\n",
      "Epoch 1266, Loss: 0.36853692308068275, Final Batch Loss: 0.09842059016227722\n",
      "Epoch 1267, Loss: 0.34374750405550003, Final Batch Loss: 0.09267465025186539\n",
      "Epoch 1268, Loss: 0.3087921552360058, Final Batch Loss: 0.0508100800216198\n",
      "Epoch 1269, Loss: 0.31997938826680183, Final Batch Loss: 0.04792718216776848\n",
      "Epoch 1270, Loss: 0.30367541685700417, Final Batch Loss: 0.07556131482124329\n",
      "Epoch 1271, Loss: 0.2868117652833462, Final Batch Loss: 0.10745791345834732\n",
      "Epoch 1272, Loss: 0.37933848798274994, Final Batch Loss: 0.09804686903953552\n",
      "Epoch 1273, Loss: 0.263062059879303, Final Batch Loss: 0.028077691793441772\n",
      "Epoch 1274, Loss: 0.2845313213765621, Final Batch Loss: 0.030194316059350967\n",
      "Epoch 1275, Loss: 0.33055926114320755, Final Batch Loss: 0.10403401404619217\n",
      "Epoch 1276, Loss: 0.3301444537937641, Final Batch Loss: 0.10731491446495056\n",
      "Epoch 1277, Loss: 0.34282186627388, Final Batch Loss: 0.06819003820419312\n",
      "Epoch 1278, Loss: 0.29321300238370895, Final Batch Loss: 0.06770995259284973\n",
      "Epoch 1279, Loss: 0.32792021334171295, Final Batch Loss: 0.10355468094348907\n",
      "Epoch 1280, Loss: 0.404916875064373, Final Batch Loss: 0.10510869324207306\n",
      "Epoch 1281, Loss: 0.3420376256108284, Final Batch Loss: 0.11814985424280167\n",
      "Epoch 1282, Loss: 0.42629481106996536, Final Batch Loss: 0.16771867871284485\n",
      "Epoch 1283, Loss: 0.43350496888160706, Final Batch Loss: 0.07675585150718689\n",
      "Epoch 1284, Loss: 0.3742554262280464, Final Batch Loss: 0.10051175951957703\n",
      "Epoch 1285, Loss: 0.42622650414705276, Final Batch Loss: 0.11516857147216797\n",
      "Epoch 1286, Loss: 0.44005507230758667, Final Batch Loss: 0.2019807994365692\n",
      "Epoch 1287, Loss: 0.4014722928404808, Final Batch Loss: 0.11029910296201706\n",
      "Epoch 1288, Loss: 0.4788118377327919, Final Batch Loss: 0.12297001481056213\n",
      "Epoch 1289, Loss: 0.3147845044732094, Final Batch Loss: 0.0665455311536789\n",
      "Epoch 1290, Loss: 0.36848387867212296, Final Batch Loss: 0.09495067596435547\n",
      "Epoch 1291, Loss: 0.30504730343818665, Final Batch Loss: 0.08323660492897034\n",
      "Epoch 1292, Loss: 0.3775324523448944, Final Batch Loss: 0.08636803925037384\n",
      "Epoch 1293, Loss: 0.3144647367298603, Final Batch Loss: 0.05080970749258995\n",
      "Epoch 1294, Loss: 0.3440457358956337, Final Batch Loss: 0.08224083483219147\n",
      "Epoch 1295, Loss: 0.3313528373837471, Final Batch Loss: 0.08756765723228455\n",
      "Epoch 1296, Loss: 0.29128591530025005, Final Batch Loss: 0.02864389680325985\n",
      "Epoch 1297, Loss: 0.4172664284706116, Final Batch Loss: 0.17188376188278198\n",
      "Epoch 1298, Loss: 0.3006777837872505, Final Batch Loss: 0.04423500597476959\n",
      "Epoch 1299, Loss: 0.33350104838609695, Final Batch Loss: 0.07784506678581238\n",
      "Epoch 1300, Loss: 0.3963804915547371, Final Batch Loss: 0.10173235833644867\n",
      "Epoch 1301, Loss: 0.40150709450244904, Final Batch Loss: 0.13279405236244202\n",
      "Epoch 1302, Loss: 0.27137457951903343, Final Batch Loss: 0.031536251306533813\n",
      "Epoch 1303, Loss: 0.38697783276438713, Final Batch Loss: 0.10082020610570908\n",
      "Epoch 1304, Loss: 0.3574123680591583, Final Batch Loss: 0.10813841968774796\n",
      "Epoch 1305, Loss: 0.27831897139549255, Final Batch Loss: 0.06177207827568054\n",
      "Epoch 1306, Loss: 0.3592788204550743, Final Batch Loss: 0.1301954686641693\n",
      "Epoch 1307, Loss: 0.30842308327555656, Final Batch Loss: 0.039603475481271744\n",
      "Epoch 1308, Loss: 0.30893929675221443, Final Batch Loss: 0.0735122337937355\n",
      "Epoch 1309, Loss: 0.3488207682967186, Final Batch Loss: 0.10385842621326447\n",
      "Epoch 1310, Loss: 0.3597414791584015, Final Batch Loss: 0.07562927901744843\n",
      "Epoch 1311, Loss: 0.3104833848774433, Final Batch Loss: 0.052120864391326904\n",
      "Epoch 1312, Loss: 0.3164290152490139, Final Batch Loss: 0.02957843616604805\n",
      "Epoch 1313, Loss: 0.3576238453388214, Final Batch Loss: 0.12203284353017807\n",
      "Epoch 1314, Loss: 0.31521571055054665, Final Batch Loss: 0.09909353405237198\n",
      "Epoch 1315, Loss: 0.30729789659380913, Final Batch Loss: 0.053555432707071304\n",
      "Epoch 1316, Loss: 0.3661820814013481, Final Batch Loss: 0.08146116882562637\n",
      "Epoch 1317, Loss: 0.29632772505283356, Final Batch Loss: 0.06225024163722992\n",
      "Epoch 1318, Loss: 0.2665235511958599, Final Batch Loss: 0.03733706846833229\n",
      "Epoch 1319, Loss: 0.32602708786726, Final Batch Loss: 0.07551898807287216\n",
      "Epoch 1320, Loss: 0.3281293287873268, Final Batch Loss: 0.0760638564825058\n",
      "Epoch 1321, Loss: 0.26575711742043495, Final Batch Loss: 0.05494808405637741\n",
      "Epoch 1322, Loss: 0.37084487825632095, Final Batch Loss: 0.12983739376068115\n",
      "Epoch 1323, Loss: 0.31915541738271713, Final Batch Loss: 0.0933438315987587\n",
      "Epoch 1324, Loss: 0.29403409734368324, Final Batch Loss: 0.06124713271856308\n",
      "Epoch 1325, Loss: 0.3935289978981018, Final Batch Loss: 0.1161128431558609\n",
      "Epoch 1326, Loss: 0.3445461355149746, Final Batch Loss: 0.13344186544418335\n",
      "Epoch 1327, Loss: 0.35449904948472977, Final Batch Loss: 0.14082825183868408\n",
      "Epoch 1328, Loss: 0.3777119815349579, Final Batch Loss: 0.11361567676067352\n",
      "Epoch 1329, Loss: 0.3730468824505806, Final Batch Loss: 0.11050210893154144\n",
      "Epoch 1330, Loss: 0.30572712421417236, Final Batch Loss: 0.06464243680238724\n",
      "Epoch 1331, Loss: 0.2625482641160488, Final Batch Loss: 0.04247138276696205\n",
      "Epoch 1332, Loss: 0.279467910528183, Final Batch Loss: 0.05525340139865875\n",
      "Epoch 1333, Loss: 0.26721200346946716, Final Batch Loss: 0.07233358174562454\n",
      "Epoch 1334, Loss: 0.27460385859012604, Final Batch Loss: 0.05897042155265808\n",
      "Epoch 1335, Loss: 0.25078601762652397, Final Batch Loss: 0.03543491289019585\n",
      "Epoch 1336, Loss: 0.29175852984189987, Final Batch Loss: 0.08172175288200378\n",
      "Epoch 1337, Loss: 0.35837722569704056, Final Batch Loss: 0.10751456767320633\n",
      "Epoch 1338, Loss: 0.3721327781677246, Final Batch Loss: 0.15357790887355804\n",
      "Epoch 1339, Loss: 0.40004900470376015, Final Batch Loss: 0.14265070855617523\n",
      "Epoch 1340, Loss: 0.32509657368063927, Final Batch Loss: 0.08869064599275589\n",
      "Epoch 1341, Loss: 0.32250983640551567, Final Batch Loss: 0.05371398851275444\n",
      "Epoch 1342, Loss: 0.3123803287744522, Final Batch Loss: 0.09430009871721268\n",
      "Epoch 1343, Loss: 0.30407650023698807, Final Batch Loss: 0.05560014396905899\n",
      "Epoch 1344, Loss: 0.2926747612655163, Final Batch Loss: 0.06074853613972664\n",
      "Epoch 1345, Loss: 0.2536725513637066, Final Batch Loss: 0.04009722173213959\n",
      "Epoch 1346, Loss: 0.32239747792482376, Final Batch Loss: 0.11124242097139359\n",
      "Epoch 1347, Loss: 0.4003123976290226, Final Batch Loss: 0.16583223640918732\n",
      "Epoch 1348, Loss: 0.4033253788948059, Final Batch Loss: 0.17627350986003876\n",
      "Epoch 1349, Loss: 0.39477282017469406, Final Batch Loss: 0.1812191903591156\n",
      "Epoch 1350, Loss: 0.3877720534801483, Final Batch Loss: 0.10287965089082718\n",
      "Epoch 1351, Loss: 0.3737045079469681, Final Batch Loss: 0.09639438986778259\n",
      "Epoch 1352, Loss: 0.37479500100016594, Final Batch Loss: 0.10843607783317566\n",
      "Epoch 1353, Loss: 0.39911942929029465, Final Batch Loss: 0.07229369878768921\n",
      "Epoch 1354, Loss: 0.275245763361454, Final Batch Loss: 0.051342450082302094\n",
      "Epoch 1355, Loss: 0.33489685505628586, Final Batch Loss: 0.09438113123178482\n",
      "Epoch 1356, Loss: 0.3983704000711441, Final Batch Loss: 0.1589212268590927\n",
      "Epoch 1357, Loss: 0.28466272354125977, Final Batch Loss: 0.07144909352064133\n",
      "Epoch 1358, Loss: 0.3303159587085247, Final Batch Loss: 0.0566694401204586\n",
      "Epoch 1359, Loss: 0.3553331419825554, Final Batch Loss: 0.08290789276361465\n",
      "Epoch 1360, Loss: 0.31729426234960556, Final Batch Loss: 0.07862692326307297\n",
      "Epoch 1361, Loss: 0.3668961562216282, Final Batch Loss: 0.09001019597053528\n",
      "Epoch 1362, Loss: 0.318233136087656, Final Batch Loss: 0.08265633881092072\n",
      "Epoch 1363, Loss: 0.3187151402235031, Final Batch Loss: 0.06600405275821686\n",
      "Epoch 1364, Loss: 0.29022977128624916, Final Batch Loss: 0.056305792182683945\n",
      "Epoch 1365, Loss: 0.3188544437289238, Final Batch Loss: 0.03966290503740311\n",
      "Epoch 1366, Loss: 0.40854284167289734, Final Batch Loss: 0.1615256667137146\n",
      "Epoch 1367, Loss: 0.337518859654665, Final Batch Loss: 0.11806706339120865\n",
      "Epoch 1368, Loss: 0.3335068076848984, Final Batch Loss: 0.07781488448381424\n",
      "Epoch 1369, Loss: 0.3686103895306587, Final Batch Loss: 0.09526289254426956\n",
      "Epoch 1370, Loss: 0.4114801362156868, Final Batch Loss: 0.10181491076946259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1371, Loss: 0.3567451387643814, Final Batch Loss: 0.1043572723865509\n",
      "Epoch 1372, Loss: 0.35885022580623627, Final Batch Loss: 0.11568213999271393\n",
      "Epoch 1373, Loss: 0.31359072774648666, Final Batch Loss: 0.07987235486507416\n",
      "Epoch 1374, Loss: 0.3763660863041878, Final Batch Loss: 0.12693363428115845\n",
      "Epoch 1375, Loss: 0.36432473361492157, Final Batch Loss: 0.05762534588575363\n",
      "Epoch 1376, Loss: 0.28413382917642593, Final Batch Loss: 0.07635396718978882\n",
      "Epoch 1377, Loss: 0.29915817454457283, Final Batch Loss: 0.04449561983346939\n",
      "Epoch 1378, Loss: 0.2954740822315216, Final Batch Loss: 0.06272131949663162\n",
      "Epoch 1379, Loss: 0.2619019150733948, Final Batch Loss: 0.08881690353155136\n",
      "Epoch 1380, Loss: 0.3241730146110058, Final Batch Loss: 0.10769522190093994\n",
      "Epoch 1381, Loss: 0.2956470511853695, Final Batch Loss: 0.02754700556397438\n",
      "Epoch 1382, Loss: 0.3377673327922821, Final Batch Loss: 0.08161376416683197\n",
      "Epoch 1383, Loss: 0.268730953335762, Final Batch Loss: 0.03795403987169266\n",
      "Epoch 1384, Loss: 0.3455609679222107, Final Batch Loss: 0.07338104397058487\n",
      "Epoch 1385, Loss: 0.3068593367934227, Final Batch Loss: 0.11692661046981812\n",
      "Epoch 1386, Loss: 0.3184325434267521, Final Batch Loss: 0.10334035754203796\n",
      "Epoch 1387, Loss: 0.3083180971443653, Final Batch Loss: 0.07296031713485718\n",
      "Epoch 1388, Loss: 0.38004349917173386, Final Batch Loss: 0.12835009396076202\n",
      "Epoch 1389, Loss: 0.323778361082077, Final Batch Loss: 0.08708764612674713\n",
      "Epoch 1390, Loss: 0.324310515075922, Final Batch Loss: 0.11580697447061539\n",
      "Epoch 1391, Loss: 0.3739801272749901, Final Batch Loss: 0.09874217957258224\n",
      "Epoch 1392, Loss: 0.3355255350470543, Final Batch Loss: 0.08097709715366364\n",
      "Epoch 1393, Loss: 0.3337639570236206, Final Batch Loss: 0.06795474886894226\n",
      "Epoch 1394, Loss: 0.31756657361984253, Final Batch Loss: 0.07159477472305298\n",
      "Epoch 1395, Loss: 0.29585912078619003, Final Batch Loss: 0.11968532204627991\n",
      "Epoch 1396, Loss: 0.3116329610347748, Final Batch Loss: 0.0721355602145195\n",
      "Epoch 1397, Loss: 0.4129028953611851, Final Batch Loss: 0.14913062751293182\n",
      "Epoch 1398, Loss: 0.3485478423535824, Final Batch Loss: 0.0509219653904438\n",
      "Epoch 1399, Loss: 0.37986961752176285, Final Batch Loss: 0.09964834898710251\n",
      "Epoch 1400, Loss: 0.33820535987615585, Final Batch Loss: 0.09906162321567535\n",
      "Epoch 1401, Loss: 0.33815011382102966, Final Batch Loss: 0.1430695801973343\n",
      "Epoch 1402, Loss: 0.29714080691337585, Final Batch Loss: 0.04634023457765579\n",
      "Epoch 1403, Loss: 0.35532350465655327, Final Batch Loss: 0.091091588139534\n",
      "Epoch 1404, Loss: 0.32009652256965637, Final Batch Loss: 0.12208868563175201\n",
      "Epoch 1405, Loss: 0.33271248638629913, Final Batch Loss: 0.09648232161998749\n",
      "Epoch 1406, Loss: 0.3054926171898842, Final Batch Loss: 0.06286259740591049\n",
      "Epoch 1407, Loss: 0.35003092139959335, Final Batch Loss: 0.10448474436998367\n",
      "Epoch 1408, Loss: 0.42987465113401413, Final Batch Loss: 0.21039751172065735\n",
      "Epoch 1409, Loss: 0.37036988884210587, Final Batch Loss: 0.05957160145044327\n",
      "Epoch 1410, Loss: 0.31463785842061043, Final Batch Loss: 0.054254207760095596\n",
      "Epoch 1411, Loss: 0.3492969237267971, Final Batch Loss: 0.12926973402500153\n",
      "Epoch 1412, Loss: 0.3318180814385414, Final Batch Loss: 0.08095410466194153\n",
      "Epoch 1413, Loss: 0.4597156308591366, Final Batch Loss: 0.13124212622642517\n",
      "Epoch 1414, Loss: 0.3248094916343689, Final Batch Loss: 0.08987591415643692\n",
      "Epoch 1415, Loss: 0.23546290397644043, Final Batch Loss: 0.04759185016155243\n",
      "Epoch 1416, Loss: 0.3500373959541321, Final Batch Loss: 0.07520563155412674\n",
      "Epoch 1417, Loss: 0.27381712570786476, Final Batch Loss: 0.044827889651060104\n",
      "Epoch 1418, Loss: 0.2803238332271576, Final Batch Loss: 0.026942085474729538\n",
      "Epoch 1419, Loss: 0.3099460266530514, Final Batch Loss: 0.07578423619270325\n",
      "Epoch 1420, Loss: 0.31111300364136696, Final Batch Loss: 0.1244179755449295\n",
      "Epoch 1421, Loss: 0.3356354683637619, Final Batch Loss: 0.12848679721355438\n",
      "Epoch 1422, Loss: 0.35259126871824265, Final Batch Loss: 0.1108340173959732\n",
      "Epoch 1423, Loss: 0.35347121953964233, Final Batch Loss: 0.1120792105793953\n",
      "Epoch 1424, Loss: 0.30591007322072983, Final Batch Loss: 0.05035087466239929\n",
      "Epoch 1425, Loss: 0.3962828367948532, Final Batch Loss: 0.1254519820213318\n",
      "Epoch 1426, Loss: 0.27620165422558784, Final Batch Loss: 0.045749731361866\n",
      "Epoch 1427, Loss: 0.5016917064785957, Final Batch Loss: 0.15443116426467896\n",
      "Epoch 1428, Loss: 0.43477657437324524, Final Batch Loss: 0.14924843609333038\n",
      "Epoch 1429, Loss: 0.3258814364671707, Final Batch Loss: 0.049949392676353455\n",
      "Epoch 1430, Loss: 0.25182731822133064, Final Batch Loss: 0.059365853667259216\n",
      "Epoch 1431, Loss: 0.37246446311473846, Final Batch Loss: 0.10597125440835953\n",
      "Epoch 1432, Loss: 0.26430419459939003, Final Batch Loss: 0.03291008621454239\n",
      "Epoch 1433, Loss: 0.3126892037689686, Final Batch Loss: 0.08253690600395203\n",
      "Epoch 1434, Loss: 0.29667919874191284, Final Batch Loss: 0.11784680187702179\n",
      "Epoch 1435, Loss: 0.3304344490170479, Final Batch Loss: 0.04228706285357475\n",
      "Epoch 1436, Loss: 0.4372815005481243, Final Batch Loss: 0.15663792192935944\n",
      "Epoch 1437, Loss: 0.32920457795262337, Final Batch Loss: 0.1051291823387146\n",
      "Epoch 1438, Loss: 0.4558613635599613, Final Batch Loss: 0.18916842341423035\n",
      "Epoch 1439, Loss: 0.352162629365921, Final Batch Loss: 0.06845881044864655\n",
      "Epoch 1440, Loss: 0.39484817534685135, Final Batch Loss: 0.09147579222917557\n",
      "Epoch 1441, Loss: 0.30323031917214394, Final Batch Loss: 0.07578250765800476\n",
      "Epoch 1442, Loss: 0.31798649579286575, Final Batch Loss: 0.09366270154714584\n",
      "Epoch 1443, Loss: 0.3042324110865593, Final Batch Loss: 0.07074024528265\n",
      "Epoch 1444, Loss: 0.3975932225584984, Final Batch Loss: 0.13490082323551178\n",
      "Epoch 1445, Loss: 0.42070095613598824, Final Batch Loss: 0.13799898326396942\n",
      "Epoch 1446, Loss: 0.3324788436293602, Final Batch Loss: 0.026096567511558533\n",
      "Epoch 1447, Loss: 0.4362056851387024, Final Batch Loss: 0.16688811779022217\n",
      "Epoch 1448, Loss: 0.3570437803864479, Final Batch Loss: 0.06565852463245392\n",
      "Epoch 1449, Loss: 0.30440544337034225, Final Batch Loss: 0.07054761052131653\n",
      "Epoch 1450, Loss: 0.2830220051109791, Final Batch Loss: 0.0515022911131382\n",
      "Epoch 1451, Loss: 0.30458005517721176, Final Batch Loss: 0.08257008343935013\n",
      "Epoch 1452, Loss: 0.30161387473344803, Final Batch Loss: 0.08092083036899567\n",
      "Epoch 1453, Loss: 0.2893484905362129, Final Batch Loss: 0.028044097125530243\n",
      "Epoch 1454, Loss: 0.29496605694293976, Final Batch Loss: 0.04411114752292633\n",
      "Epoch 1455, Loss: 0.26703793555498123, Final Batch Loss: 0.062073733657598495\n",
      "Epoch 1456, Loss: 0.2659691907465458, Final Batch Loss: 0.05917143076658249\n",
      "Epoch 1457, Loss: 0.35447971522808075, Final Batch Loss: 0.12455891817808151\n",
      "Epoch 1458, Loss: 0.3307814858853817, Final Batch Loss: 0.12668190896511078\n",
      "Epoch 1459, Loss: 0.25971244648098946, Final Batch Loss: 0.04704540967941284\n",
      "Epoch 1460, Loss: 0.40723852813243866, Final Batch Loss: 0.11158886551856995\n",
      "Epoch 1461, Loss: 0.287185899913311, Final Batch Loss: 0.027635209262371063\n",
      "Epoch 1462, Loss: 0.2754750959575176, Final Batch Loss: 0.058411724865436554\n",
      "Epoch 1463, Loss: 0.34882380813360214, Final Batch Loss: 0.06462977826595306\n",
      "Epoch 1464, Loss: 0.2808899097144604, Final Batch Loss: 0.04624181613326073\n",
      "Epoch 1465, Loss: 0.30105452239513397, Final Batch Loss: 0.08862379938364029\n",
      "Epoch 1466, Loss: 0.3046802133321762, Final Batch Loss: 0.05123147368431091\n",
      "Epoch 1467, Loss: 0.25073838979005814, Final Batch Loss: 0.08634347468614578\n",
      "Epoch 1468, Loss: 0.3361455127596855, Final Batch Loss: 0.09368358552455902\n",
      "Epoch 1469, Loss: 0.3832952696830034, Final Batch Loss: 0.1770322471857071\n",
      "Epoch 1470, Loss: 0.2822931446135044, Final Batch Loss: 0.04397085681557655\n",
      "Epoch 1471, Loss: 0.2767086289823055, Final Batch Loss: 0.10117647051811218\n",
      "Epoch 1472, Loss: 0.3296830877661705, Final Batch Loss: 0.15040212869644165\n",
      "Epoch 1473, Loss: 0.3386196903884411, Final Batch Loss: 0.09573371708393097\n",
      "Epoch 1474, Loss: 0.3103376552462578, Final Batch Loss: 0.06218215823173523\n",
      "Epoch 1475, Loss: 0.30991215258836746, Final Batch Loss: 0.05659658834338188\n",
      "Epoch 1476, Loss: 0.3492426574230194, Final Batch Loss: 0.05263258516788483\n",
      "Epoch 1477, Loss: 0.3110848069190979, Final Batch Loss: 0.05543597787618637\n",
      "Epoch 1478, Loss: 0.2542533874511719, Final Batch Loss: 0.05901095271110535\n",
      "Epoch 1479, Loss: 0.3356459513306618, Final Batch Loss: 0.0936114639043808\n",
      "Epoch 1480, Loss: 0.26776508800685406, Final Batch Loss: 0.030376000329852104\n",
      "Epoch 1481, Loss: 0.3241645134985447, Final Batch Loss: 0.07846280932426453\n",
      "Epoch 1482, Loss: 0.3476811423897743, Final Batch Loss: 0.04403702914714813\n",
      "Epoch 1483, Loss: 0.313802607357502, Final Batch Loss: 0.06261994689702988\n",
      "Epoch 1484, Loss: 0.26176295056939125, Final Batch Loss: 0.017821136862039566\n",
      "Epoch 1485, Loss: 0.2780114896595478, Final Batch Loss: 0.08815030753612518\n",
      "Epoch 1486, Loss: 0.43077918514609337, Final Batch Loss: 0.20108552277088165\n",
      "Epoch 1487, Loss: 0.2687397934496403, Final Batch Loss: 0.04014645144343376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1488, Loss: 0.35799767076969147, Final Batch Loss: 0.10352377593517303\n",
      "Epoch 1489, Loss: 0.37675904110074043, Final Batch Loss: 0.12447374314069748\n",
      "Epoch 1490, Loss: 0.270359855145216, Final Batch Loss: 0.041730210185050964\n",
      "Epoch 1491, Loss: 0.36955221742391586, Final Batch Loss: 0.13020987808704376\n",
      "Epoch 1492, Loss: 0.267411258071661, Final Batch Loss: 0.051003601402044296\n",
      "Epoch 1493, Loss: 0.3266753479838371, Final Batch Loss: 0.06457092612981796\n",
      "Epoch 1494, Loss: 0.31317251548171043, Final Batch Loss: 0.11318696290254593\n",
      "Epoch 1495, Loss: 0.3141139782965183, Final Batch Loss: 0.0642763003706932\n",
      "Epoch 1496, Loss: 0.3129577226936817, Final Batch Loss: 0.11887628585100174\n",
      "Epoch 1497, Loss: 0.33771533891558647, Final Batch Loss: 0.0908094123005867\n",
      "Epoch 1498, Loss: 0.31975556910037994, Final Batch Loss: 0.06174255907535553\n",
      "Epoch 1499, Loss: 0.3297077640891075, Final Batch Loss: 0.056020960211753845\n",
      "Epoch 1500, Loss: 0.3005739748477936, Final Batch Loss: 0.07377280294895172\n",
      "Epoch 1501, Loss: 0.3562857545912266, Final Batch Loss: 0.13492675125598907\n",
      "Epoch 1502, Loss: 0.29293273016810417, Final Batch Loss: 0.08260985463857651\n",
      "Epoch 1503, Loss: 0.24842527881264687, Final Batch Loss: 0.029285598546266556\n",
      "Epoch 1504, Loss: 0.3536415845155716, Final Batch Loss: 0.09146089851856232\n",
      "Epoch 1505, Loss: 0.3425150588154793, Final Batch Loss: 0.12885400652885437\n",
      "Epoch 1506, Loss: 0.35611148551106453, Final Batch Loss: 0.10955702513456345\n",
      "Epoch 1507, Loss: 0.3659091703593731, Final Batch Loss: 0.15453195571899414\n",
      "Epoch 1508, Loss: 0.341905377805233, Final Batch Loss: 0.05206120014190674\n",
      "Epoch 1509, Loss: 0.3593238666653633, Final Batch Loss: 0.11115484684705734\n",
      "Epoch 1510, Loss: 0.2976548857986927, Final Batch Loss: 0.054132718592882156\n",
      "Epoch 1511, Loss: 0.2785870246589184, Final Batch Loss: 0.07513604313135147\n",
      "Epoch 1512, Loss: 0.27701620757579803, Final Batch Loss: 0.0450400747358799\n",
      "Epoch 1513, Loss: 0.27202484756708145, Final Batch Loss: 0.060711510479450226\n",
      "Epoch 1514, Loss: 0.25908007472753525, Final Batch Loss: 0.029830820858478546\n",
      "Epoch 1515, Loss: 0.2829599753022194, Final Batch Loss: 0.05797325447201729\n",
      "Epoch 1516, Loss: 0.2613310106098652, Final Batch Loss: 0.042310770601034164\n",
      "Epoch 1517, Loss: 0.3483726307749748, Final Batch Loss: 0.14475229382514954\n",
      "Epoch 1518, Loss: 0.28790971636772156, Final Batch Loss: 0.06049398332834244\n",
      "Epoch 1519, Loss: 0.2909739874303341, Final Batch Loss: 0.05552775412797928\n",
      "Epoch 1520, Loss: 0.27320850640535355, Final Batch Loss: 0.12311530858278275\n",
      "Epoch 1521, Loss: 0.2773275710642338, Final Batch Loss: 0.08185852319002151\n",
      "Epoch 1522, Loss: 0.31822893768548965, Final Batch Loss: 0.06446122378110886\n",
      "Epoch 1523, Loss: 0.31047473102808, Final Batch Loss: 0.03807660937309265\n",
      "Epoch 1524, Loss: 0.3255428485572338, Final Batch Loss: 0.08109043538570404\n",
      "Epoch 1525, Loss: 0.33945659920573235, Final Batch Loss: 0.10046549141407013\n",
      "Epoch 1526, Loss: 0.34557150304317474, Final Batch Loss: 0.04275147616863251\n",
      "Epoch 1527, Loss: 0.2756939195096493, Final Batch Loss: 0.10040031373500824\n",
      "Epoch 1528, Loss: 0.2846486195921898, Final Batch Loss: 0.0415547750890255\n",
      "Epoch 1529, Loss: 0.21392995864152908, Final Batch Loss: 0.03311878815293312\n",
      "Epoch 1530, Loss: 0.2412395477294922, Final Batch Loss: 0.06627941876649857\n",
      "Epoch 1531, Loss: 0.3049689829349518, Final Batch Loss: 0.08110986649990082\n",
      "Epoch 1532, Loss: 0.2831187844276428, Final Batch Loss: 0.0450076162815094\n",
      "Epoch 1533, Loss: 0.29608556628227234, Final Batch Loss: 0.07095229625701904\n",
      "Epoch 1534, Loss: 0.40232111513614655, Final Batch Loss: 0.16052845120429993\n",
      "Epoch 1535, Loss: 0.3281655013561249, Final Batch Loss: 0.10925275087356567\n",
      "Epoch 1536, Loss: 0.30895474553108215, Final Batch Loss: 0.06585227698087692\n",
      "Epoch 1537, Loss: 0.3170970529317856, Final Batch Loss: 0.09809169918298721\n",
      "Epoch 1538, Loss: 0.32244477793574333, Final Batch Loss: 0.15924641489982605\n",
      "Epoch 1539, Loss: 0.39870060235261917, Final Batch Loss: 0.1266975998878479\n",
      "Epoch 1540, Loss: 0.4601024091243744, Final Batch Loss: 0.09876679629087448\n",
      "Epoch 1541, Loss: 0.4559331610798836, Final Batch Loss: 0.14897547662258148\n",
      "Epoch 1542, Loss: 0.43152718991041183, Final Batch Loss: 0.09842690080404282\n",
      "Epoch 1543, Loss: 0.45440614968538284, Final Batch Loss: 0.2072642594575882\n",
      "Epoch 1544, Loss: 0.3608456254005432, Final Batch Loss: 0.04247765988111496\n",
      "Epoch 1545, Loss: 0.40951913595199585, Final Batch Loss: 0.09598855674266815\n",
      "Epoch 1546, Loss: 0.2730190195143223, Final Batch Loss: 0.029766235500574112\n",
      "Epoch 1547, Loss: 0.3710719980299473, Final Batch Loss: 0.09902637451887131\n",
      "Epoch 1548, Loss: 0.25647298246622086, Final Batch Loss: 0.028384651988744736\n",
      "Epoch 1549, Loss: 0.23682147450745106, Final Batch Loss: 0.027834372594952583\n",
      "Epoch 1550, Loss: 0.36373482272028923, Final Batch Loss: 0.05959348753094673\n",
      "Epoch 1551, Loss: 0.4657434970140457, Final Batch Loss: 0.24137873947620392\n",
      "Epoch 1552, Loss: 0.2624713405966759, Final Batch Loss: 0.0911622866988182\n",
      "Epoch 1553, Loss: 0.38568979129195213, Final Batch Loss: 0.1191437691450119\n",
      "Epoch 1554, Loss: 0.2541027832776308, Final Batch Loss: 0.030414531007409096\n",
      "Epoch 1555, Loss: 0.3099711909890175, Final Batch Loss: 0.06880404055118561\n",
      "Epoch 1556, Loss: 0.4106508195400238, Final Batch Loss: 0.12357677519321442\n",
      "Epoch 1557, Loss: 0.32188547030091286, Final Batch Loss: 0.062358222901821136\n",
      "Epoch 1558, Loss: 0.2523840293288231, Final Batch Loss: 0.06888309121131897\n",
      "Epoch 1559, Loss: 0.2882050834596157, Final Batch Loss: 0.060487594455480576\n",
      "Epoch 1560, Loss: 0.274834219366312, Final Batch Loss: 0.08615659922361374\n",
      "Epoch 1561, Loss: 0.30558227375149727, Final Batch Loss: 0.04194488003849983\n",
      "Epoch 1562, Loss: 0.3245866075158119, Final Batch Loss: 0.1056259348988533\n",
      "Epoch 1563, Loss: 0.3294895216822624, Final Batch Loss: 0.12590964138507843\n",
      "Epoch 1564, Loss: 0.27242323383688927, Final Batch Loss: 0.0697491317987442\n",
      "Epoch 1565, Loss: 0.3020198456943035, Final Batch Loss: 0.06749330461025238\n",
      "Epoch 1566, Loss: 0.3122767508029938, Final Batch Loss: 0.08781696856021881\n",
      "Epoch 1567, Loss: 0.3273260183632374, Final Batch Loss: 0.07847665995359421\n",
      "Epoch 1568, Loss: 0.3269819989800453, Final Batch Loss: 0.101506806910038\n",
      "Epoch 1569, Loss: 0.25612665712833405, Final Batch Loss: 0.02336297184228897\n",
      "Epoch 1570, Loss: 0.2673297170549631, Final Batch Loss: 0.028886178508400917\n",
      "Epoch 1571, Loss: 0.29222026094794273, Final Batch Loss: 0.10949672758579254\n",
      "Epoch 1572, Loss: 0.36813072860240936, Final Batch Loss: 0.12084898352622986\n",
      "Epoch 1573, Loss: 0.33763810619711876, Final Batch Loss: 0.13411112129688263\n",
      "Epoch 1574, Loss: 0.3176909014582634, Final Batch Loss: 0.11888264864683151\n",
      "Epoch 1575, Loss: 0.30167949572205544, Final Batch Loss: 0.060814905911684036\n",
      "Epoch 1576, Loss: 0.25461987778544426, Final Batch Loss: 0.04669354110956192\n",
      "Epoch 1577, Loss: 0.27959463372826576, Final Batch Loss: 0.03698296472430229\n",
      "Epoch 1578, Loss: 0.280109416693449, Final Batch Loss: 0.07111796736717224\n",
      "Epoch 1579, Loss: 0.27404481917619705, Final Batch Loss: 0.04882274940609932\n",
      "Epoch 1580, Loss: 0.20314782112836838, Final Batch Loss: 0.03916101157665253\n",
      "Epoch 1581, Loss: 0.28583864122629166, Final Batch Loss: 0.0865379050374031\n",
      "Epoch 1582, Loss: 0.2734128590673208, Final Batch Loss: 0.031242182478308678\n",
      "Epoch 1583, Loss: 0.39379870146512985, Final Batch Loss: 0.09995432198047638\n",
      "Epoch 1584, Loss: 0.30590886250138283, Final Batch Loss: 0.10691598802804947\n",
      "Epoch 1585, Loss: 0.2746230736374855, Final Batch Loss: 0.048650093376636505\n",
      "Epoch 1586, Loss: 0.31851208955049515, Final Batch Loss: 0.09420761466026306\n",
      "Epoch 1587, Loss: 0.32977261394262314, Final Batch Loss: 0.11643163114786148\n",
      "Epoch 1588, Loss: 0.3516271859407425, Final Batch Loss: 0.0846925899386406\n",
      "Epoch 1589, Loss: 0.4271879196166992, Final Batch Loss: 0.12095104157924652\n",
      "Epoch 1590, Loss: 0.31448343209922314, Final Batch Loss: 0.029176993295550346\n",
      "Epoch 1591, Loss: 0.24133644625544548, Final Batch Loss: 0.04179629683494568\n",
      "Epoch 1592, Loss: 0.2629007622599602, Final Batch Loss: 0.040829602628946304\n",
      "Epoch 1593, Loss: 0.28073134645819664, Final Batch Loss: 0.09438236802816391\n",
      "Epoch 1594, Loss: 0.3323964327573776, Final Batch Loss: 0.0991789773106575\n",
      "Epoch 1595, Loss: 0.415895439684391, Final Batch Loss: 0.09982974827289581\n",
      "Epoch 1596, Loss: 0.29832639545202255, Final Batch Loss: 0.04144918918609619\n",
      "Epoch 1597, Loss: 0.3876708708703518, Final Batch Loss: 0.16321061551570892\n",
      "Epoch 1598, Loss: 0.21684978529810905, Final Batch Loss: 0.037885334342718124\n",
      "Epoch 1599, Loss: 0.2999359108507633, Final Batch Loss: 0.05835976451635361\n",
      "Epoch 1600, Loss: 0.301750797778368, Final Batch Loss: 0.07379385083913803\n",
      "Epoch 1601, Loss: 0.2587182577699423, Final Batch Loss: 0.017864501103758812\n",
      "Epoch 1602, Loss: 0.2973990850150585, Final Batch Loss: 0.08294149488210678\n",
      "Epoch 1603, Loss: 0.2404656559228897, Final Batch Loss: 0.04659773409366608\n",
      "Epoch 1604, Loss: 0.27664536610245705, Final Batch Loss: 0.06696733087301254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1605, Loss: 0.2722771652042866, Final Batch Loss: 0.04828896373510361\n",
      "Epoch 1606, Loss: 0.32353173196315765, Final Batch Loss: 0.07663895189762115\n",
      "Epoch 1607, Loss: 0.2839871421456337, Final Batch Loss: 0.05591423809528351\n",
      "Epoch 1608, Loss: 0.3156231790781021, Final Batch Loss: 0.07949255406856537\n",
      "Epoch 1609, Loss: 0.270596731454134, Final Batch Loss: 0.03538653254508972\n",
      "Epoch 1610, Loss: 0.2522825412452221, Final Batch Loss: 0.04505673050880432\n",
      "Epoch 1611, Loss: 0.22391068190336227, Final Batch Loss: 0.029920514672994614\n",
      "Epoch 1612, Loss: 0.3959828242659569, Final Batch Loss: 0.14377045631408691\n",
      "Epoch 1613, Loss: 0.44930828362703323, Final Batch Loss: 0.14203062653541565\n",
      "Epoch 1614, Loss: 0.33049479126930237, Final Batch Loss: 0.08195407688617706\n",
      "Epoch 1615, Loss: 0.34670859947800636, Final Batch Loss: 0.07248225808143616\n",
      "Epoch 1616, Loss: 0.40050058811903, Final Batch Loss: 0.11971434950828552\n",
      "Epoch 1617, Loss: 0.29791592061519623, Final Batch Loss: 0.06024312973022461\n",
      "Epoch 1618, Loss: 0.34848765283823013, Final Batch Loss: 0.08998677879571915\n",
      "Epoch 1619, Loss: 0.2712917886674404, Final Batch Loss: 0.029764097183942795\n",
      "Epoch 1620, Loss: 0.3475131243467331, Final Batch Loss: 0.049736201763153076\n",
      "Epoch 1621, Loss: 0.31576910242438316, Final Batch Loss: 0.11489983648061752\n",
      "Epoch 1622, Loss: 0.25092358887195587, Final Batch Loss: 0.044604700058698654\n",
      "Epoch 1623, Loss: 0.3341158665716648, Final Batch Loss: 0.12151116877794266\n",
      "Epoch 1624, Loss: 0.23433521762490273, Final Batch Loss: 0.02238813042640686\n",
      "Epoch 1625, Loss: 0.26602432131767273, Final Batch Loss: 0.056544333696365356\n",
      "Epoch 1626, Loss: 0.22760767303407192, Final Batch Loss: 0.021406514570116997\n",
      "Epoch 1627, Loss: 0.285534393042326, Final Batch Loss: 0.10958530008792877\n",
      "Epoch 1628, Loss: 0.26883700862526894, Final Batch Loss: 0.07218572497367859\n",
      "Epoch 1629, Loss: 0.2668030261993408, Final Batch Loss: 0.03269467130303383\n",
      "Epoch 1630, Loss: 0.28558122366666794, Final Batch Loss: 0.04396143928170204\n",
      "Epoch 1631, Loss: 0.30870218575000763, Final Batch Loss: 0.08374869078397751\n",
      "Epoch 1632, Loss: 0.3496790826320648, Final Batch Loss: 0.04878447949886322\n",
      "Epoch 1633, Loss: 0.2733740210533142, Final Batch Loss: 0.07287121564149857\n",
      "Epoch 1634, Loss: 0.3325681500136852, Final Batch Loss: 0.12665657699108124\n",
      "Epoch 1635, Loss: 0.2829599417746067, Final Batch Loss: 0.08371465653181076\n",
      "Epoch 1636, Loss: 0.2766120433807373, Final Batch Loss: 0.06299179047346115\n",
      "Epoch 1637, Loss: 0.3290969133377075, Final Batch Loss: 0.07474739849567413\n",
      "Epoch 1638, Loss: 0.28101877495646477, Final Batch Loss: 0.06689383834600449\n",
      "Epoch 1639, Loss: 0.25999021902680397, Final Batch Loss: 0.04135706648230553\n",
      "Epoch 1640, Loss: 0.27169934287667274, Final Batch Loss: 0.09527590870857239\n",
      "Epoch 1641, Loss: 0.22729413956403732, Final Batch Loss: 0.01829637587070465\n",
      "Epoch 1642, Loss: 0.2213149070739746, Final Batch Loss: 0.03673676401376724\n",
      "Epoch 1643, Loss: 0.3029838800430298, Final Batch Loss: 0.05321042612195015\n",
      "Epoch 1644, Loss: 0.27603475004434586, Final Batch Loss: 0.035552978515625\n",
      "Epoch 1645, Loss: 0.2459193877875805, Final Batch Loss: 0.051997020840644836\n",
      "Epoch 1646, Loss: 0.29435960948467255, Final Batch Loss: 0.08128786832094193\n",
      "Epoch 1647, Loss: 0.34476976841688156, Final Batch Loss: 0.09226090461015701\n",
      "Epoch 1648, Loss: 0.2487625703215599, Final Batch Loss: 0.07108174264431\n",
      "Epoch 1649, Loss: 0.25242770835757256, Final Batch Loss: 0.03581022843718529\n",
      "Epoch 1650, Loss: 0.26878566667437553, Final Batch Loss: 0.03232627734541893\n",
      "Epoch 1651, Loss: 0.3803754895925522, Final Batch Loss: 0.12627647817134857\n",
      "Epoch 1652, Loss: 0.27444904297590256, Final Batch Loss: 0.08642152696847916\n",
      "Epoch 1653, Loss: 0.23369178548455238, Final Batch Loss: 0.017270784825086594\n",
      "Epoch 1654, Loss: 0.27961745485663414, Final Batch Loss: 0.09719251096248627\n",
      "Epoch 1655, Loss: 0.264822032302618, Final Batch Loss: 0.06310880929231644\n",
      "Epoch 1656, Loss: 0.27364542335271835, Final Batch Loss: 0.06012134253978729\n",
      "Epoch 1657, Loss: 0.19676902145147324, Final Batch Loss: 0.03511819988489151\n",
      "Epoch 1658, Loss: 0.2446720339357853, Final Batch Loss: 0.040630221366882324\n",
      "Epoch 1659, Loss: 0.27885565161705017, Final Batch Loss: 0.09557294100522995\n",
      "Epoch 1660, Loss: 0.3210773393511772, Final Batch Loss: 0.08446145057678223\n",
      "Epoch 1661, Loss: 0.2844957020133734, Final Batch Loss: 0.023056408390402794\n",
      "Epoch 1662, Loss: 0.22946487367153168, Final Batch Loss: 0.06102566793560982\n",
      "Epoch 1663, Loss: 0.2765074260532856, Final Batch Loss: 0.08154713362455368\n",
      "Epoch 1664, Loss: 0.29116829484701157, Final Batch Loss: 0.08504997938871384\n",
      "Epoch 1665, Loss: 0.20010172203183174, Final Batch Loss: 0.036992184817790985\n",
      "Epoch 1666, Loss: 0.2549581900238991, Final Batch Loss: 0.014116890728473663\n",
      "Epoch 1667, Loss: 0.274976447224617, Final Batch Loss: 0.10568340122699738\n",
      "Epoch 1668, Loss: 0.29612357914447784, Final Batch Loss: 0.07145864516496658\n",
      "Epoch 1669, Loss: 0.24033265560865402, Final Batch Loss: 0.03499763458967209\n",
      "Epoch 1670, Loss: 0.29705872386693954, Final Batch Loss: 0.06448039412498474\n",
      "Epoch 1671, Loss: 0.2767930328845978, Final Batch Loss: 0.05765712261199951\n",
      "Epoch 1672, Loss: 0.2396549992263317, Final Batch Loss: 0.025329571217298508\n",
      "Epoch 1673, Loss: 0.2789243124425411, Final Batch Loss: 0.07927536964416504\n",
      "Epoch 1674, Loss: 0.2962283454835415, Final Batch Loss: 0.06577181071043015\n",
      "Epoch 1675, Loss: 0.23755185678601265, Final Batch Loss: 0.04762301966547966\n",
      "Epoch 1676, Loss: 0.2201359160244465, Final Batch Loss: 0.04573512449860573\n",
      "Epoch 1677, Loss: 0.423683475703001, Final Batch Loss: 0.1759977638721466\n",
      "Epoch 1678, Loss: 0.3428419828414917, Final Batch Loss: 0.0775936022400856\n",
      "Epoch 1679, Loss: 0.24428580328822136, Final Batch Loss: 0.03210606426000595\n",
      "Epoch 1680, Loss: 0.3151449114084244, Final Batch Loss: 0.07635083794593811\n",
      "Epoch 1681, Loss: 0.28808068111538887, Final Batch Loss: 0.0811903178691864\n",
      "Epoch 1682, Loss: 0.2778424732387066, Final Batch Loss: 0.08106132596731186\n",
      "Epoch 1683, Loss: 0.30083754658699036, Final Batch Loss: 0.054779618978500366\n",
      "Epoch 1684, Loss: 0.20959502831101418, Final Batch Loss: 0.043318334966897964\n",
      "Epoch 1685, Loss: 0.3214726373553276, Final Batch Loss: 0.07112506777048111\n",
      "Epoch 1686, Loss: 0.21088984981179237, Final Batch Loss: 0.04998374357819557\n",
      "Epoch 1687, Loss: 0.18905045464634895, Final Batch Loss: 0.027801308780908585\n",
      "Epoch 1688, Loss: 0.26487720757722855, Final Batch Loss: 0.032207902520895004\n",
      "Epoch 1689, Loss: 0.23386060446500778, Final Batch Loss: 0.10240449011325836\n",
      "Epoch 1690, Loss: 0.3009544126689434, Final Batch Loss: 0.07138044387102127\n",
      "Epoch 1691, Loss: 0.25799963623285294, Final Batch Loss: 0.04181373119354248\n",
      "Epoch 1692, Loss: 0.2965519055724144, Final Batch Loss: 0.05431082099676132\n",
      "Epoch 1693, Loss: 0.3181931786239147, Final Batch Loss: 0.07222916930913925\n",
      "Epoch 1694, Loss: 0.3581334091722965, Final Batch Loss: 0.042974282056093216\n",
      "Epoch 1695, Loss: 0.2445804737508297, Final Batch Loss: 0.050671953707933426\n",
      "Epoch 1696, Loss: 0.30072059482336044, Final Batch Loss: 0.07011071592569351\n",
      "Epoch 1697, Loss: 0.32744961231946945, Final Batch Loss: 0.05466911941766739\n",
      "Epoch 1698, Loss: 0.3176211416721344, Final Batch Loss: 0.15752802789211273\n",
      "Epoch 1699, Loss: 0.31408020853996277, Final Batch Loss: 0.062345605343580246\n",
      "Epoch 1700, Loss: 0.3226611986756325, Final Batch Loss: 0.11155108362436295\n",
      "Epoch 1701, Loss: 0.28068894520401955, Final Batch Loss: 0.10283061116933823\n",
      "Epoch 1702, Loss: 0.35411863401532173, Final Batch Loss: 0.05507655814290047\n",
      "Epoch 1703, Loss: 0.28733086585998535, Final Batch Loss: 0.03643698990345001\n",
      "Epoch 1704, Loss: 0.2819954827427864, Final Batch Loss: 0.07322608679533005\n",
      "Epoch 1705, Loss: 0.3529597334563732, Final Batch Loss: 0.12136705964803696\n",
      "Epoch 1706, Loss: 0.2891930006444454, Final Batch Loss: 0.0888110026717186\n",
      "Epoch 1707, Loss: 0.27607037499547005, Final Batch Loss: 0.09027506411075592\n",
      "Epoch 1708, Loss: 0.3407743275165558, Final Batch Loss: 0.07938196510076523\n",
      "Epoch 1709, Loss: 0.23434529080986977, Final Batch Loss: 0.06949151307344437\n",
      "Epoch 1710, Loss: 0.26869030483067036, Final Batch Loss: 0.022334003821015358\n",
      "Epoch 1711, Loss: 0.2729554995894432, Final Batch Loss: 0.058013271540403366\n",
      "Epoch 1712, Loss: 0.3081699199974537, Final Batch Loss: 0.08109934628009796\n",
      "Epoch 1713, Loss: 0.2603563703596592, Final Batch Loss: 0.07549621164798737\n",
      "Epoch 1714, Loss: 0.301383875310421, Final Batch Loss: 0.03819318488240242\n",
      "Epoch 1715, Loss: 0.311816917732358, Final Batch Loss: 0.08063608407974243\n",
      "Epoch 1716, Loss: 0.4884687792509794, Final Batch Loss: 0.2691020369529724\n",
      "Epoch 1717, Loss: 0.4174483045935631, Final Batch Loss: 0.16770099103450775\n",
      "Epoch 1718, Loss: 0.322818074375391, Final Batch Loss: 0.05548836663365364\n",
      "Epoch 1719, Loss: 0.28015754744410515, Final Batch Loss: 0.08333814889192581\n",
      "Epoch 1720, Loss: 0.2836550213396549, Final Batch Loss: 0.05922921001911163\n",
      "Epoch 1721, Loss: 0.3207581229507923, Final Batch Loss: 0.017017733305692673\n",
      "Epoch 1722, Loss: 0.4054741710424423, Final Batch Loss: 0.06619159877300262\n",
      "Epoch 1723, Loss: 0.2977103888988495, Final Batch Loss: 0.07449430972337723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1724, Loss: 0.35377340018749237, Final Batch Loss: 0.04380429536104202\n",
      "Epoch 1725, Loss: 0.402065247297287, Final Batch Loss: 0.11964808404445648\n",
      "Epoch 1726, Loss: 0.3216947019100189, Final Batch Loss: 0.15953953564167023\n",
      "Epoch 1727, Loss: 0.28318532928824425, Final Batch Loss: 0.058417681604623795\n",
      "Epoch 1728, Loss: 0.32303331419825554, Final Batch Loss: 0.04303398355841637\n",
      "Epoch 1729, Loss: 0.3443467877805233, Final Batch Loss: 0.15382206439971924\n",
      "Epoch 1730, Loss: 0.2632659152150154, Final Batch Loss: 0.05216985195875168\n",
      "Epoch 1731, Loss: 0.31590037420392036, Final Batch Loss: 0.05645664036273956\n",
      "Epoch 1732, Loss: 0.3563331589102745, Final Batch Loss: 0.11596234142780304\n",
      "Epoch 1733, Loss: 0.2831539437174797, Final Batch Loss: 0.08132490515708923\n",
      "Epoch 1734, Loss: 0.3157714121043682, Final Batch Loss: 0.11069387942552567\n",
      "Epoch 1735, Loss: 0.26482609659433365, Final Batch Loss: 0.09693772345781326\n",
      "Epoch 1736, Loss: 0.27937280014157295, Final Batch Loss: 0.09579616785049438\n",
      "Epoch 1737, Loss: 0.28665463626384735, Final Batch Loss: 0.09793847799301147\n",
      "Epoch 1738, Loss: 0.2699877917766571, Final Batch Loss: 0.06495517492294312\n",
      "Epoch 1739, Loss: 0.2889043614268303, Final Batch Loss: 0.1004331186413765\n",
      "Epoch 1740, Loss: 0.30211077630519867, Final Batch Loss: 0.05449306592345238\n",
      "Epoch 1741, Loss: 0.3580513708293438, Final Batch Loss: 0.121490478515625\n",
      "Epoch 1742, Loss: 0.28046298772096634, Final Batch Loss: 0.09888023138046265\n",
      "Epoch 1743, Loss: 0.33339497447013855, Final Batch Loss: 0.08151731640100479\n",
      "Epoch 1744, Loss: 0.26675695180892944, Final Batch Loss: 0.04628690704703331\n",
      "Epoch 1745, Loss: 0.2757494170218706, Final Batch Loss: 0.030871892347931862\n",
      "Epoch 1746, Loss: 0.27822938561439514, Final Batch Loss: 0.06549005955457687\n",
      "Epoch 1747, Loss: 0.28115396201610565, Final Batch Loss: 0.1414080411195755\n",
      "Epoch 1748, Loss: 0.2674633413553238, Final Batch Loss: 0.015618093311786652\n",
      "Epoch 1749, Loss: 0.23783757910132408, Final Batch Loss: 0.08364453911781311\n",
      "Epoch 1750, Loss: 0.2527819164097309, Final Batch Loss: 0.045402079820632935\n",
      "Epoch 1751, Loss: 0.3539307303726673, Final Batch Loss: 0.12007548660039902\n",
      "Epoch 1752, Loss: 0.3909059837460518, Final Batch Loss: 0.20694264769554138\n",
      "Epoch 1753, Loss: 0.3313199318945408, Final Batch Loss: 0.06596928089857101\n",
      "Epoch 1754, Loss: 0.3655061200261116, Final Batch Loss: 0.07670249789953232\n",
      "Epoch 1755, Loss: 0.3121348209679127, Final Batch Loss: 0.12036410719156265\n",
      "Epoch 1756, Loss: 0.26005202159285545, Final Batch Loss: 0.08019052445888519\n",
      "Epoch 1757, Loss: 0.22244907543063164, Final Batch Loss: 0.023511163890361786\n",
      "Epoch 1758, Loss: 0.30889421701431274, Final Batch Loss: 0.10857148468494415\n",
      "Epoch 1759, Loss: 0.2955155335366726, Final Batch Loss: 0.06331635266542435\n",
      "Epoch 1760, Loss: 0.34345241636037827, Final Batch Loss: 0.11311531066894531\n",
      "Epoch 1761, Loss: 0.37213415652513504, Final Batch Loss: 0.10480397939682007\n",
      "Epoch 1762, Loss: 0.31604960188269615, Final Batch Loss: 0.11610878258943558\n",
      "Epoch 1763, Loss: 0.27371541783213615, Final Batch Loss: 0.04491819813847542\n",
      "Epoch 1764, Loss: 0.35331422090530396, Final Batch Loss: 0.10101110488176346\n",
      "Epoch 1765, Loss: 0.3111134022474289, Final Batch Loss: 0.0941479429602623\n",
      "Epoch 1766, Loss: 0.2391248680651188, Final Batch Loss: 0.023951608687639236\n",
      "Epoch 1767, Loss: 0.3730397969484329, Final Batch Loss: 0.13248054683208466\n",
      "Epoch 1768, Loss: 0.30309589579701424, Final Batch Loss: 0.08865594118833542\n",
      "Epoch 1769, Loss: 0.24475686252117157, Final Batch Loss: 0.057257626205682755\n",
      "Epoch 1770, Loss: 0.3237367942929268, Final Batch Loss: 0.07918355613946915\n",
      "Epoch 1771, Loss: 0.3784191347658634, Final Batch Loss: 0.1745961308479309\n",
      "Epoch 1772, Loss: 0.30745020136237144, Final Batch Loss: 0.06795574724674225\n",
      "Epoch 1773, Loss: 0.2899868041276932, Final Batch Loss: 0.07439995557069778\n",
      "Epoch 1774, Loss: 0.34134508669376373, Final Batch Loss: 0.0820189118385315\n",
      "Epoch 1775, Loss: 0.27536677941679955, Final Batch Loss: 0.0998658686876297\n",
      "Epoch 1776, Loss: 0.2691901810467243, Final Batch Loss: 0.05205192044377327\n",
      "Epoch 1777, Loss: 0.3318357914686203, Final Batch Loss: 0.05336335301399231\n",
      "Epoch 1778, Loss: 0.33001724630594254, Final Batch Loss: 0.06221012398600578\n",
      "Epoch 1779, Loss: 0.3082294501364231, Final Batch Loss: 0.03867395594716072\n",
      "Epoch 1780, Loss: 0.24696809612214565, Final Batch Loss: 0.02513805590569973\n",
      "Epoch 1781, Loss: 0.31034084782004356, Final Batch Loss: 0.03772861883044243\n",
      "Epoch 1782, Loss: 0.2094040047377348, Final Batch Loss: 0.023227447643876076\n",
      "Epoch 1783, Loss: 0.31933988258242607, Final Batch Loss: 0.08414677530527115\n",
      "Epoch 1784, Loss: 0.242045221850276, Final Batch Loss: 0.026183107867836952\n",
      "Epoch 1785, Loss: 0.20792651735246181, Final Batch Loss: 0.06252774596214294\n",
      "Epoch 1786, Loss: 0.2881460525095463, Final Batch Loss: 0.07925060391426086\n",
      "Epoch 1787, Loss: 0.2693210653960705, Final Batch Loss: 0.03934217244386673\n",
      "Epoch 1788, Loss: 0.2484740950167179, Final Batch Loss: 0.07699816673994064\n",
      "Epoch 1789, Loss: 0.262423325330019, Final Batch Loss: 0.06472481787204742\n",
      "Epoch 1790, Loss: 0.34902335703372955, Final Batch Loss: 0.09105623513460159\n",
      "Epoch 1791, Loss: 0.24724212661385536, Final Batch Loss: 0.05584847182035446\n",
      "Epoch 1792, Loss: 0.27006734162569046, Final Batch Loss: 0.05756807327270508\n",
      "Epoch 1793, Loss: 0.27783801406621933, Final Batch Loss: 0.08709654211997986\n",
      "Epoch 1794, Loss: 0.2609335258603096, Final Batch Loss: 0.09098220616579056\n",
      "Epoch 1795, Loss: 0.20090338587760925, Final Batch Loss: 0.03892954811453819\n",
      "Epoch 1796, Loss: 0.22017663344740868, Final Batch Loss: 0.0373687706887722\n",
      "Epoch 1797, Loss: 0.2834114730358124, Final Batch Loss: 0.06415785104036331\n",
      "Epoch 1798, Loss: 0.3221397064626217, Final Batch Loss: 0.1135062724351883\n",
      "Epoch 1799, Loss: 0.39631468430161476, Final Batch Loss: 0.18605685234069824\n",
      "Epoch 1800, Loss: 0.27318261563777924, Final Batch Loss: 0.03369409590959549\n",
      "Epoch 1801, Loss: 0.2791783735156059, Final Batch Loss: 0.0761997178196907\n",
      "Epoch 1802, Loss: 0.2226283736526966, Final Batch Loss: 0.04117549583315849\n",
      "Epoch 1803, Loss: 0.3084755390882492, Final Batch Loss: 0.06846544146537781\n",
      "Epoch 1804, Loss: 0.3283981531858444, Final Batch Loss: 0.08684822916984558\n",
      "Epoch 1805, Loss: 0.3170553669333458, Final Batch Loss: 0.06380189955234528\n",
      "Epoch 1806, Loss: 0.30278072506189346, Final Batch Loss: 0.06345989555120468\n",
      "Epoch 1807, Loss: 0.29428986832499504, Final Batch Loss: 0.057191308587789536\n",
      "Epoch 1808, Loss: 0.3137134090065956, Final Batch Loss: 0.11908188462257385\n",
      "Epoch 1809, Loss: 0.26181209832429886, Final Batch Loss: 0.035083040595054626\n",
      "Epoch 1810, Loss: 0.23822971433401108, Final Batch Loss: 0.03886966407299042\n",
      "Epoch 1811, Loss: 0.3912689760327339, Final Batch Loss: 0.1482751965522766\n",
      "Epoch 1812, Loss: 0.2825290895998478, Final Batch Loss: 0.04546738788485527\n",
      "Epoch 1813, Loss: 0.2649091109633446, Final Batch Loss: 0.0609816312789917\n",
      "Epoch 1814, Loss: 0.3583364076912403, Final Batch Loss: 0.12230092287063599\n",
      "Epoch 1815, Loss: 0.26975033804774284, Final Batch Loss: 0.06456683576107025\n",
      "Epoch 1816, Loss: 0.2998461611568928, Final Batch Loss: 0.06166974827647209\n",
      "Epoch 1817, Loss: 0.3309205397963524, Final Batch Loss: 0.11645698547363281\n",
      "Epoch 1818, Loss: 0.2991776801645756, Final Batch Loss: 0.061427924782037735\n",
      "Epoch 1819, Loss: 0.23909926414489746, Final Batch Loss: 0.048398662358522415\n",
      "Epoch 1820, Loss: 0.21238812245428562, Final Batch Loss: 0.024911100044846535\n",
      "Epoch 1821, Loss: 0.21465983986854553, Final Batch Loss: 0.03749629482626915\n",
      "Epoch 1822, Loss: 0.2240836713463068, Final Batch Loss: 0.06398911774158478\n",
      "Epoch 1823, Loss: 0.26523173227906227, Final Batch Loss: 0.05501634627580643\n",
      "Epoch 1824, Loss: 0.3096798248589039, Final Batch Loss: 0.032123032957315445\n",
      "Epoch 1825, Loss: 0.2609209343791008, Final Batch Loss: 0.0721496120095253\n",
      "Epoch 1826, Loss: 0.19689790718257427, Final Batch Loss: 0.019124744459986687\n",
      "Epoch 1827, Loss: 0.24707935377955437, Final Batch Loss: 0.07302337139844894\n",
      "Epoch 1828, Loss: 0.2564661130309105, Final Batch Loss: 0.02891077846288681\n",
      "Epoch 1829, Loss: 0.2724807634949684, Final Batch Loss: 0.09432828426361084\n",
      "Epoch 1830, Loss: 0.25686755776405334, Final Batch Loss: 0.051278695464134216\n",
      "Epoch 1831, Loss: 0.33396483212709427, Final Batch Loss: 0.10497817397117615\n",
      "Epoch 1832, Loss: 0.24725595489144325, Final Batch Loss: 0.05156432092189789\n",
      "Epoch 1833, Loss: 0.2729133814573288, Final Batch Loss: 0.0795387551188469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1834, Loss: 0.2262884508818388, Final Batch Loss: 0.018851296976208687\n",
      "Epoch 1835, Loss: 0.24794910103082657, Final Batch Loss: 0.026925675570964813\n",
      "Epoch 1836, Loss: 0.2756410725414753, Final Batch Loss: 0.0354815311729908\n",
      "Epoch 1837, Loss: 0.23157710954546928, Final Batch Loss: 0.03802269697189331\n",
      "Epoch 1838, Loss: 0.29699942097067833, Final Batch Loss: 0.07599135488271713\n",
      "Epoch 1839, Loss: 0.27651477977633476, Final Batch Loss: 0.051920246332883835\n",
      "Epoch 1840, Loss: 0.32734669744968414, Final Batch Loss: 0.06705961376428604\n",
      "Epoch 1841, Loss: 0.35396405309438705, Final Batch Loss: 0.09053821116685867\n",
      "Epoch 1842, Loss: 0.2654396668076515, Final Batch Loss: 0.07130960375070572\n",
      "Epoch 1843, Loss: 0.25698596239089966, Final Batch Loss: 0.04442123696208\n",
      "Epoch 1844, Loss: 0.28297949582338333, Final Batch Loss: 0.09709326922893524\n",
      "Epoch 1845, Loss: 0.2646520175039768, Final Batch Loss: 0.08695945888757706\n",
      "Epoch 1846, Loss: 0.1957797985523939, Final Batch Loss: 0.023490989580750465\n",
      "Epoch 1847, Loss: 0.2461491972208023, Final Batch Loss: 0.06304331868886948\n",
      "Epoch 1848, Loss: 0.21364018693566322, Final Batch Loss: 0.04089410975575447\n",
      "Epoch 1849, Loss: 0.24409125745296478, Final Batch Loss: 0.10412591695785522\n",
      "Epoch 1850, Loss: 0.458882212638855, Final Batch Loss: 0.13889636099338531\n",
      "Epoch 1851, Loss: 0.33973048999905586, Final Batch Loss: 0.09857937693595886\n",
      "Epoch 1852, Loss: 0.322571262717247, Final Batch Loss: 0.06541912257671356\n",
      "Epoch 1853, Loss: 0.2984221801161766, Final Batch Loss: 0.0968920961022377\n",
      "Epoch 1854, Loss: 0.3016711249947548, Final Batch Loss: 0.045774318277835846\n",
      "Epoch 1855, Loss: 0.24792246520519257, Final Batch Loss: 0.06591425091028214\n",
      "Epoch 1856, Loss: 0.25977861136198044, Final Batch Loss: 0.059771399945020676\n",
      "Epoch 1857, Loss: 0.24410489574074745, Final Batch Loss: 0.08967430144548416\n",
      "Epoch 1858, Loss: 0.30482451245188713, Final Batch Loss: 0.07109682261943817\n",
      "Epoch 1859, Loss: 0.3518095798790455, Final Batch Loss: 0.13131026923656464\n",
      "Epoch 1860, Loss: 0.3018350154161453, Final Batch Loss: 0.10023591667413712\n",
      "Epoch 1861, Loss: 0.2668076381087303, Final Batch Loss: 0.06271915137767792\n",
      "Epoch 1862, Loss: 0.23949653282761574, Final Batch Loss: 0.06646783649921417\n",
      "Epoch 1863, Loss: 0.43491319566965103, Final Batch Loss: 0.18182910978794098\n",
      "Epoch 1864, Loss: 0.28811752423644066, Final Batch Loss: 0.06984514743089676\n",
      "Epoch 1865, Loss: 0.2376059591770172, Final Batch Loss: 0.06784357875585556\n",
      "Epoch 1866, Loss: 0.25610753893852234, Final Batch Loss: 0.05818619206547737\n",
      "Epoch 1867, Loss: 0.29794029146432877, Final Batch Loss: 0.04175867140293121\n",
      "Epoch 1868, Loss: 0.18196817860007286, Final Batch Loss: 0.026422016322612762\n",
      "Epoch 1869, Loss: 0.3573821112513542, Final Batch Loss: 0.12370546907186508\n",
      "Epoch 1870, Loss: 0.23030875623226166, Final Batch Loss: 0.04712351784110069\n",
      "Epoch 1871, Loss: 0.27088644728064537, Final Batch Loss: 0.0785253643989563\n",
      "Epoch 1872, Loss: 0.2831597179174423, Final Batch Loss: 0.06224266439676285\n",
      "Epoch 1873, Loss: 0.2669946625828743, Final Batch Loss: 0.08818186074495316\n",
      "Epoch 1874, Loss: 0.25916313380002975, Final Batch Loss: 0.07898705452680588\n",
      "Epoch 1875, Loss: 0.26256516948342323, Final Batch Loss: 0.07062606513500214\n",
      "Epoch 1876, Loss: 0.3109898790717125, Final Batch Loss: 0.030779652297496796\n",
      "Epoch 1877, Loss: 0.1798887401819229, Final Batch Loss: 0.017525603994727135\n",
      "Epoch 1878, Loss: 0.24348758533596992, Final Batch Loss: 0.0808536633849144\n",
      "Epoch 1879, Loss: 0.2986912168562412, Final Batch Loss: 0.048980869352817535\n",
      "Epoch 1880, Loss: 0.2712077535688877, Final Batch Loss: 0.08934467285871506\n",
      "Epoch 1881, Loss: 0.418537724763155, Final Batch Loss: 0.17636902630329132\n",
      "Epoch 1882, Loss: 0.33170241862535477, Final Batch Loss: 0.09346769005060196\n",
      "Epoch 1883, Loss: 0.4137934073805809, Final Batch Loss: 0.14469021558761597\n",
      "Epoch 1884, Loss: 0.31126198917627335, Final Batch Loss: 0.07037574797868729\n",
      "Epoch 1885, Loss: 0.3617172911763191, Final Batch Loss: 0.08515159040689468\n",
      "Epoch 1886, Loss: 0.39960021525621414, Final Batch Loss: 0.1382136195898056\n",
      "Epoch 1887, Loss: 0.3567834347486496, Final Batch Loss: 0.08978196978569031\n",
      "Epoch 1888, Loss: 0.33648350089788437, Final Batch Loss: 0.01910819113254547\n",
      "Epoch 1889, Loss: 0.3187656868249178, Final Batch Loss: 0.02814982272684574\n",
      "Epoch 1890, Loss: 0.27002058550715446, Final Batch Loss: 0.10448216646909714\n",
      "Epoch 1891, Loss: 0.24682151898741722, Final Batch Loss: 0.0406615175306797\n",
      "Epoch 1892, Loss: 0.36773666739463806, Final Batch Loss: 0.15064182877540588\n",
      "Epoch 1893, Loss: 0.29596393555402756, Final Batch Loss: 0.042711466550827026\n",
      "Epoch 1894, Loss: 0.2637418657541275, Final Batch Loss: 0.034265272319316864\n",
      "Epoch 1895, Loss: 0.3275011256337166, Final Batch Loss: 0.12210678309202194\n",
      "Epoch 1896, Loss: 0.276066180318594, Final Batch Loss: 0.06615976989269257\n",
      "Epoch 1897, Loss: 0.2547529265284538, Final Batch Loss: 0.03920804336667061\n",
      "Epoch 1898, Loss: 0.3307668939232826, Final Batch Loss: 0.08999375253915787\n",
      "Epoch 1899, Loss: 0.3220977783203125, Final Batch Loss: 0.11324240267276764\n",
      "Epoch 1900, Loss: 0.32757942005991936, Final Batch Loss: 0.04390225186944008\n",
      "Epoch 1901, Loss: 0.2753226011991501, Final Batch Loss: 0.09343037009239197\n",
      "Epoch 1902, Loss: 0.20346742868423462, Final Batch Loss: 0.07880841195583344\n",
      "Epoch 1903, Loss: 0.26247715577483177, Final Batch Loss: 0.04477815702557564\n",
      "Epoch 1904, Loss: 0.22255120426416397, Final Batch Loss: 0.04320812597870827\n",
      "Epoch 1905, Loss: 0.24321072921156883, Final Batch Loss: 0.054671671241521835\n",
      "Epoch 1906, Loss: 0.27374761179089546, Final Batch Loss: 0.06410309672355652\n",
      "Epoch 1907, Loss: 0.2660400792956352, Final Batch Loss: 0.07055225223302841\n",
      "Epoch 1908, Loss: 0.3062853515148163, Final Batch Loss: 0.0934208557009697\n",
      "Epoch 1909, Loss: 0.25291816890239716, Final Batch Loss: 0.04497659206390381\n",
      "Epoch 1910, Loss: 0.24506070092320442, Final Batch Loss: 0.04906214773654938\n",
      "Epoch 1911, Loss: 0.2653512544929981, Final Batch Loss: 0.054016564041376114\n",
      "Epoch 1912, Loss: 0.2642069533467293, Final Batch Loss: 0.06296077370643616\n",
      "Epoch 1913, Loss: 0.19585328362882137, Final Batch Loss: 0.021020149812102318\n",
      "Epoch 1914, Loss: 0.30316948890686035, Final Batch Loss: 0.08314355462789536\n",
      "Epoch 1915, Loss: 0.20856180787086487, Final Batch Loss: 0.035463329404592514\n",
      "Epoch 1916, Loss: 0.2342940717935562, Final Batch Loss: 0.06024622172117233\n",
      "Epoch 1917, Loss: 0.321651428937912, Final Batch Loss: 0.08600499480962753\n",
      "Epoch 1918, Loss: 0.2992338202893734, Final Batch Loss: 0.09265809506177902\n",
      "Epoch 1919, Loss: 0.33533522859215736, Final Batch Loss: 0.09885770082473755\n",
      "Epoch 1920, Loss: 0.2478955052793026, Final Batch Loss: 0.08338002115488052\n",
      "Epoch 1921, Loss: 0.3007870987057686, Final Batch Loss: 0.09786844998598099\n",
      "Epoch 1922, Loss: 0.41492506861686707, Final Batch Loss: 0.221282497048378\n",
      "Epoch 1923, Loss: 0.2079821154475212, Final Batch Loss: 0.04069733992218971\n",
      "Epoch 1924, Loss: 0.2669578418135643, Final Batch Loss: 0.028858058154582977\n",
      "Epoch 1925, Loss: 0.27405335009098053, Final Batch Loss: 0.0657237321138382\n",
      "Epoch 1926, Loss: 0.28852005302906036, Final Batch Loss: 0.0646209791302681\n",
      "Epoch 1927, Loss: 0.25047608464956284, Final Batch Loss: 0.07432935386896133\n",
      "Epoch 1928, Loss: 0.30764200165867805, Final Batch Loss: 0.07154952734708786\n",
      "Epoch 1929, Loss: 0.2308662198483944, Final Batch Loss: 0.07260523736476898\n",
      "Epoch 1930, Loss: 0.2793315127491951, Final Batch Loss: 0.05978132784366608\n",
      "Epoch 1931, Loss: 0.30120304226875305, Final Batch Loss: 0.05483148992061615\n",
      "Epoch 1932, Loss: 0.29027891531586647, Final Batch Loss: 0.1019020602107048\n",
      "Epoch 1933, Loss: 0.3256504498422146, Final Batch Loss: 0.06094317510724068\n",
      "Epoch 1934, Loss: 0.2552203871309757, Final Batch Loss: 0.025357257574796677\n",
      "Epoch 1935, Loss: 0.23648639395833015, Final Batch Loss: 0.05533426254987717\n",
      "Epoch 1936, Loss: 0.19153950735926628, Final Batch Loss: 0.029582537710666656\n",
      "Epoch 1937, Loss: 0.2291325367987156, Final Batch Loss: 0.046752531081438065\n",
      "Epoch 1938, Loss: 0.2662777304649353, Final Batch Loss: 0.0738910511136055\n",
      "Epoch 1939, Loss: 0.19028412364423275, Final Batch Loss: 0.0423118881881237\n",
      "Epoch 1940, Loss: 0.24843166396021843, Final Batch Loss: 0.049845606088638306\n",
      "Epoch 1941, Loss: 0.24612289667129517, Final Batch Loss: 0.04271123930811882\n",
      "Epoch 1942, Loss: 0.3824082240462303, Final Batch Loss: 0.1638854593038559\n",
      "Epoch 1943, Loss: 0.31105659902095795, Final Batch Loss: 0.1149078905582428\n",
      "Epoch 1944, Loss: 0.23890097066760063, Final Batch Loss: 0.07791517674922943\n",
      "Epoch 1945, Loss: 0.26421136036515236, Final Batch Loss: 0.12238147109746933\n",
      "Epoch 1946, Loss: 0.33107034116983414, Final Batch Loss: 0.08052408695220947\n",
      "Epoch 1947, Loss: 0.2950839661061764, Final Batch Loss: 0.07006055861711502\n",
      "Epoch 1948, Loss: 0.2431545853614807, Final Batch Loss: 0.0366462804377079\n",
      "Epoch 1949, Loss: 0.28540219366550446, Final Batch Loss: 0.07652613520622253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1950, Loss: 0.3195798099040985, Final Batch Loss: 0.11251892894506454\n",
      "Epoch 1951, Loss: 0.2255174182355404, Final Batch Loss: 0.04809623584151268\n",
      "Epoch 1952, Loss: 0.25558581203222275, Final Batch Loss: 0.05725148320198059\n",
      "Epoch 1953, Loss: 0.2467656284570694, Final Batch Loss: 0.03965340182185173\n",
      "Epoch 1954, Loss: 0.20469949394464493, Final Batch Loss: 0.031309738755226135\n",
      "Epoch 1955, Loss: 0.24150115996599197, Final Batch Loss: 0.07847476750612259\n",
      "Epoch 1956, Loss: 0.3094586804509163, Final Batch Loss: 0.06525296717882156\n",
      "Epoch 1957, Loss: 0.26591743156313896, Final Batch Loss: 0.07653933018445969\n",
      "Epoch 1958, Loss: 0.22703278809785843, Final Batch Loss: 0.0556754432618618\n",
      "Epoch 1959, Loss: 0.2542719580233097, Final Batch Loss: 0.059244412928819656\n",
      "Epoch 1960, Loss: 0.26001423597335815, Final Batch Loss: 0.05334046110510826\n",
      "Epoch 1961, Loss: 0.2612835317850113, Final Batch Loss: 0.04580829292535782\n",
      "Epoch 1962, Loss: 0.21921833232045174, Final Batch Loss: 0.04432792589068413\n",
      "Epoch 1963, Loss: 0.25467901676893234, Final Batch Loss: 0.015466313809156418\n",
      "Epoch 1964, Loss: 0.24209261126816273, Final Batch Loss: 0.024680787697434425\n",
      "Epoch 1965, Loss: 0.2646744176745415, Final Batch Loss: 0.07487190514802933\n",
      "Epoch 1966, Loss: 0.1986472336575389, Final Batch Loss: 0.015287759713828564\n",
      "Epoch 1967, Loss: 0.17942285537719727, Final Batch Loss: 0.021759165450930595\n",
      "Epoch 1968, Loss: 0.2336689718067646, Final Batch Loss: 0.06566550582647324\n",
      "Epoch 1969, Loss: 0.20277824252843857, Final Batch Loss: 0.04041053727269173\n",
      "Epoch 1970, Loss: 0.2560410611331463, Final Batch Loss: 0.03945298492908478\n",
      "Epoch 1971, Loss: 0.20109594985842705, Final Batch Loss: 0.015742851421236992\n",
      "Epoch 1972, Loss: 0.24205860123038292, Final Batch Loss: 0.021588381379842758\n",
      "Epoch 1973, Loss: 0.23509588837623596, Final Batch Loss: 0.07112680375576019\n",
      "Epoch 1974, Loss: 0.28210993111133575, Final Batch Loss: 0.06826078146696091\n",
      "Epoch 1975, Loss: 0.30631790310144424, Final Batch Loss: 0.10132373869419098\n",
      "Epoch 1976, Loss: 0.27354346215724945, Final Batch Loss: 0.053896062076091766\n",
      "Epoch 1977, Loss: 0.23333889245986938, Final Batch Loss: 0.07604341208934784\n",
      "Epoch 1978, Loss: 0.25070012360811234, Final Batch Loss: 0.03435596823692322\n",
      "Epoch 1979, Loss: 0.23918874189257622, Final Batch Loss: 0.06290940195322037\n",
      "Epoch 1980, Loss: 0.29472459852695465, Final Batch Loss: 0.0670183077454567\n",
      "Epoch 1981, Loss: 0.23421449959278107, Final Batch Loss: 0.04497570917010307\n",
      "Epoch 1982, Loss: 0.19366706535220146, Final Batch Loss: 0.028578907251358032\n",
      "Epoch 1983, Loss: 0.34200093150138855, Final Batch Loss: 0.11483210325241089\n",
      "Epoch 1984, Loss: 0.252271194010973, Final Batch Loss: 0.031106457114219666\n",
      "Epoch 1985, Loss: 0.20074957609176636, Final Batch Loss: 0.039418648928403854\n",
      "Epoch 1986, Loss: 0.20087558403611183, Final Batch Loss: 0.03427231311798096\n",
      "Epoch 1987, Loss: 0.3010697662830353, Final Batch Loss: 0.0782080814242363\n",
      "Epoch 1988, Loss: 0.22965450212359428, Final Batch Loss: 0.049699872732162476\n",
      "Epoch 1989, Loss: 0.20939698442816734, Final Batch Loss: 0.036863986402750015\n",
      "Epoch 1990, Loss: 0.3315618708729744, Final Batch Loss: 0.06390209496021271\n",
      "Epoch 1991, Loss: 0.2087357398122549, Final Batch Loss: 0.01429835520684719\n",
      "Epoch 1992, Loss: 0.2470865622162819, Final Batch Loss: 0.05056421086192131\n",
      "Epoch 1993, Loss: 0.3150917515158653, Final Batch Loss: 0.13330483436584473\n",
      "Epoch 1994, Loss: 0.29709674790501595, Final Batch Loss: 0.11032607406377792\n",
      "Epoch 1995, Loss: 0.29206058010458946, Final Batch Loss: 0.09705840796232224\n",
      "Epoch 1996, Loss: 0.257864635437727, Final Batch Loss: 0.05281616747379303\n",
      "Epoch 1997, Loss: 0.2566893808543682, Final Batch Loss: 0.07910740375518799\n",
      "Epoch 1998, Loss: 0.24702487513422966, Final Batch Loss: 0.07157295942306519\n",
      "Epoch 1999, Loss: 0.20504062995314598, Final Batch Loss: 0.05802028626203537\n",
      "Epoch 2000, Loss: 0.23882416263222694, Final Batch Loss: 0.03899506852030754\n",
      "Epoch 2001, Loss: 0.2045457623898983, Final Batch Loss: 0.052880045026540756\n",
      "Epoch 2002, Loss: 0.2580025754868984, Final Batch Loss: 0.08488226681947708\n",
      "Epoch 2003, Loss: 0.28193000331521034, Final Batch Loss: 0.045605603605508804\n",
      "Epoch 2004, Loss: 0.2665950059890747, Final Batch Loss: 0.09055312722921371\n",
      "Epoch 2005, Loss: 0.25752080976963043, Final Batch Loss: 0.047791317105293274\n",
      "Epoch 2006, Loss: 0.20663324184715748, Final Batch Loss: 0.062242697924375534\n",
      "Epoch 2007, Loss: 0.183477645739913, Final Batch Loss: 0.026105334982275963\n",
      "Epoch 2008, Loss: 0.18410893715918064, Final Batch Loss: 0.022237272933125496\n",
      "Epoch 2009, Loss: 0.21354015544056892, Final Batch Loss: 0.04365003481507301\n",
      "Epoch 2010, Loss: 0.25610102340579033, Final Batch Loss: 0.09157177805900574\n",
      "Epoch 2011, Loss: 0.24400640651583672, Final Batch Loss: 0.08499610424041748\n",
      "Epoch 2012, Loss: 0.29062526673078537, Final Batch Loss: 0.06248544901609421\n",
      "Epoch 2013, Loss: 0.3049440234899521, Final Batch Loss: 0.10321902483701706\n",
      "Epoch 2014, Loss: 0.24826636910438538, Final Batch Loss: 0.03753083199262619\n",
      "Epoch 2015, Loss: 0.3134962059557438, Final Batch Loss: 0.14186270534992218\n",
      "Epoch 2016, Loss: 0.23799757845699787, Final Batch Loss: 0.03089708276093006\n",
      "Epoch 2017, Loss: 0.28862834721803665, Final Batch Loss: 0.07621162384748459\n",
      "Epoch 2018, Loss: 0.22723359614610672, Final Batch Loss: 0.07148552685976028\n",
      "Epoch 2019, Loss: 0.24600249901413918, Final Batch Loss: 0.06685531884431839\n",
      "Epoch 2020, Loss: 0.21928379870951176, Final Batch Loss: 0.02097906731069088\n",
      "Epoch 2021, Loss: 0.22641729563474655, Final Batch Loss: 0.03905303776264191\n",
      "Epoch 2022, Loss: 0.24961146712303162, Final Batch Loss: 0.056706853210926056\n",
      "Epoch 2023, Loss: 0.23441676050424576, Final Batch Loss: 0.039501190185546875\n",
      "Epoch 2024, Loss: 0.27439160645008087, Final Batch Loss: 0.10175581276416779\n",
      "Epoch 2025, Loss: 0.21763655170798302, Final Batch Loss: 0.05222543701529503\n",
      "Epoch 2026, Loss: 0.35312992334365845, Final Batch Loss: 0.09131897985935211\n",
      "Epoch 2027, Loss: 0.20663359761238098, Final Batch Loss: 0.031538669019937515\n",
      "Epoch 2028, Loss: 0.258621733635664, Final Batch Loss: 0.017408695071935654\n",
      "Epoch 2029, Loss: 0.25876856595277786, Final Batch Loss: 0.06267786771059036\n",
      "Epoch 2030, Loss: 0.2165720872581005, Final Batch Loss: 0.05049426853656769\n",
      "Epoch 2031, Loss: 0.29350172355771065, Final Batch Loss: 0.10535399615764618\n",
      "Epoch 2032, Loss: 0.23013906925916672, Final Batch Loss: 0.03759340941905975\n",
      "Epoch 2033, Loss: 0.2687642052769661, Final Batch Loss: 0.10444579273462296\n",
      "Epoch 2034, Loss: 0.24503928050398827, Final Batch Loss: 0.03268514201045036\n",
      "Epoch 2035, Loss: 0.21976124867796898, Final Batch Loss: 0.06195393204689026\n",
      "Epoch 2036, Loss: 0.3111531510949135, Final Batch Loss: 0.08073743432760239\n",
      "Epoch 2037, Loss: 0.3098558858036995, Final Batch Loss: 0.07182859629392624\n",
      "Epoch 2038, Loss: 0.20070681534707546, Final Batch Loss: 0.010901635512709618\n",
      "Epoch 2039, Loss: 0.26831458136439323, Final Batch Loss: 0.07056532055139542\n",
      "Epoch 2040, Loss: 0.3212129697203636, Final Batch Loss: 0.10582464188337326\n",
      "Epoch 2041, Loss: 0.22583400458097458, Final Batch Loss: 0.07513047754764557\n",
      "Epoch 2042, Loss: 0.17032820358872414, Final Batch Loss: 0.024030055850744247\n",
      "Epoch 2043, Loss: 0.3322632163763046, Final Batch Loss: 0.0664353221654892\n",
      "Epoch 2044, Loss: 0.25701047852635384, Final Batch Loss: 0.0794515609741211\n",
      "Epoch 2045, Loss: 0.2894330322742462, Final Batch Loss: 0.07965578138828278\n",
      "Epoch 2046, Loss: 0.23529548570513725, Final Batch Loss: 0.059647947549819946\n",
      "Epoch 2047, Loss: 0.2533318065106869, Final Batch Loss: 0.0729622170329094\n",
      "Epoch 2048, Loss: 0.21676332503557205, Final Batch Loss: 0.03965005651116371\n",
      "Epoch 2049, Loss: 0.3175612725317478, Final Batch Loss: 0.1133764311671257\n",
      "Epoch 2050, Loss: 0.18410498648881912, Final Batch Loss: 0.025427140295505524\n",
      "Epoch 2051, Loss: 0.3224417492747307, Final Batch Loss: 0.1020483523607254\n",
      "Epoch 2052, Loss: 0.3392067514359951, Final Batch Loss: 0.13613130152225494\n",
      "Epoch 2053, Loss: 0.39695151150226593, Final Batch Loss: 0.17646755278110504\n",
      "Epoch 2054, Loss: 0.3337550163269043, Final Batch Loss: 0.13334736227989197\n",
      "Epoch 2055, Loss: 0.2742792069911957, Final Batch Loss: 0.04594600945711136\n",
      "Epoch 2056, Loss: 0.39512114971876144, Final Batch Loss: 0.1017119437456131\n",
      "Epoch 2057, Loss: 0.33035621978342533, Final Batch Loss: 0.15124206244945526\n",
      "Epoch 2058, Loss: 0.30106497928500175, Final Batch Loss: 0.14095638692378998\n",
      "Epoch 2059, Loss: 0.24022453650832176, Final Batch Loss: 0.024977799504995346\n",
      "Epoch 2060, Loss: 0.25217119604349136, Final Batch Loss: 0.05111473798751831\n",
      "Epoch 2061, Loss: 0.22906146943569183, Final Batch Loss: 0.031894732266664505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2062, Loss: 0.25869734957814217, Final Batch Loss: 0.038931459188461304\n",
      "Epoch 2063, Loss: 0.2087855078279972, Final Batch Loss: 0.061752188950777054\n",
      "Epoch 2064, Loss: 0.2798347733914852, Final Batch Loss: 0.0415143221616745\n",
      "Epoch 2065, Loss: 0.2790471315383911, Final Batch Loss: 0.09198980778455734\n",
      "Epoch 2066, Loss: 0.31890908256173134, Final Batch Loss: 0.11507653445005417\n",
      "Epoch 2067, Loss: 0.2620086185634136, Final Batch Loss: 0.09761611372232437\n",
      "Epoch 2068, Loss: 0.34107937291264534, Final Batch Loss: 0.0538509376347065\n",
      "Epoch 2069, Loss: 0.2592509910464287, Final Batch Loss: 0.03753814473748207\n",
      "Epoch 2070, Loss: 0.2699650563299656, Final Batch Loss: 0.04793739691376686\n",
      "Epoch 2071, Loss: 0.24314911104738712, Final Batch Loss: 0.026207944378256798\n",
      "Epoch 2072, Loss: 0.23719307966530323, Final Batch Loss: 0.03015202470123768\n",
      "Epoch 2073, Loss: 0.2594155967235565, Final Batch Loss: 0.08455923944711685\n",
      "Epoch 2074, Loss: 0.2269258350133896, Final Batch Loss: 0.05398385226726532\n",
      "Epoch 2075, Loss: 0.2961042895913124, Final Batch Loss: 0.10000328719615936\n",
      "Epoch 2076, Loss: 0.2536931224167347, Final Batch Loss: 0.05189911276102066\n",
      "Epoch 2077, Loss: 0.3191456124186516, Final Batch Loss: 0.1377415508031845\n",
      "Epoch 2078, Loss: 0.4105457440018654, Final Batch Loss: 0.18373534083366394\n",
      "Epoch 2079, Loss: 0.3873853385448456, Final Batch Loss: 0.12279506027698517\n",
      "Epoch 2080, Loss: 0.32344119995832443, Final Batch Loss: 0.07999013364315033\n",
      "Epoch 2081, Loss: 0.32562030851840973, Final Batch Loss: 0.1368020623922348\n",
      "Epoch 2082, Loss: 0.23881188035011292, Final Batch Loss: 0.06623666733503342\n",
      "Epoch 2083, Loss: 0.37380706518888474, Final Batch Loss: 0.13274848461151123\n",
      "Epoch 2084, Loss: 0.36651165038347244, Final Batch Loss: 0.1329692006111145\n",
      "Epoch 2085, Loss: 0.3204260654747486, Final Batch Loss: 0.061912745237350464\n",
      "Epoch 2086, Loss: 0.32819148525595665, Final Batch Loss: 0.03216629847884178\n",
      "Epoch 2087, Loss: 0.38367512077093124, Final Batch Loss: 0.04649899899959564\n",
      "Epoch 2088, Loss: 0.3526560254395008, Final Batch Loss: 0.11844091862440109\n",
      "Epoch 2089, Loss: 0.33421649783849716, Final Batch Loss: 0.052238062024116516\n",
      "Epoch 2090, Loss: 0.29373766109347343, Final Batch Loss: 0.05127817019820213\n",
      "Epoch 2091, Loss: 0.26599010825157166, Final Batch Loss: 0.048513516783714294\n",
      "Epoch 2092, Loss: 0.2552819363772869, Final Batch Loss: 0.08639992028474808\n",
      "Epoch 2093, Loss: 0.26774023473262787, Final Batch Loss: 0.04673290625214577\n",
      "Epoch 2094, Loss: 0.29784978181123734, Final Batch Loss: 0.02262001484632492\n",
      "Epoch 2095, Loss: 0.31476710364222527, Final Batch Loss: 0.07970617711544037\n",
      "Epoch 2096, Loss: 0.24114572256803513, Final Batch Loss: 0.058170463889837265\n",
      "Epoch 2097, Loss: 0.25603094324469566, Final Batch Loss: 0.024480555206537247\n",
      "Epoch 2098, Loss: 0.3085010461509228, Final Batch Loss: 0.04103415831923485\n",
      "Epoch 2099, Loss: 0.3119911924004555, Final Batch Loss: 0.08731149137020111\n",
      "Epoch 2100, Loss: 0.26775890588760376, Final Batch Loss: 0.06537250429391861\n",
      "Epoch 2101, Loss: 0.2571704313158989, Final Batch Loss: 0.03524111211299896\n",
      "Epoch 2102, Loss: 0.281858216971159, Final Batch Loss: 0.08268440514802933\n",
      "Epoch 2103, Loss: 0.33340711891651154, Final Batch Loss: 0.1083429679274559\n",
      "Epoch 2104, Loss: 0.2577199228107929, Final Batch Loss: 0.02127848193049431\n",
      "Epoch 2105, Loss: 0.23110312595963478, Final Batch Loss: 0.037042465060949326\n",
      "Epoch 2106, Loss: 0.23341375961899757, Final Batch Loss: 0.03946898505091667\n",
      "Epoch 2107, Loss: 0.25904357247054577, Final Batch Loss: 0.0249736737459898\n",
      "Epoch 2108, Loss: 0.17440121434628963, Final Batch Loss: 0.01568766124546528\n",
      "Epoch 2109, Loss: 0.18447570502758026, Final Batch Loss: 0.049883365631103516\n",
      "Epoch 2110, Loss: 0.23839741945266724, Final Batch Loss: 0.07302846759557724\n",
      "Epoch 2111, Loss: 0.27221762388944626, Final Batch Loss: 0.07975862920284271\n",
      "Epoch 2112, Loss: 0.19968071766197681, Final Batch Loss: 0.03100576438009739\n",
      "Epoch 2113, Loss: 0.24556554481387138, Final Batch Loss: 0.07622337341308594\n",
      "Epoch 2114, Loss: 0.21158688329160213, Final Batch Loss: 0.010192101821303368\n",
      "Epoch 2115, Loss: 0.3199912719428539, Final Batch Loss: 0.12882986664772034\n",
      "Epoch 2116, Loss: 0.30089256912469864, Final Batch Loss: 0.0890800952911377\n",
      "Epoch 2117, Loss: 0.30529341101646423, Final Batch Loss: 0.07452946156263351\n",
      "Epoch 2118, Loss: 0.39885135740041733, Final Batch Loss: 0.18752029538154602\n",
      "Epoch 2119, Loss: 0.25756118819117546, Final Batch Loss: 0.07067777216434479\n",
      "Epoch 2120, Loss: 0.41534164547920227, Final Batch Loss: 0.1470891833305359\n",
      "Epoch 2121, Loss: 0.30487630888819695, Final Batch Loss: 0.1073153018951416\n",
      "Epoch 2122, Loss: 0.29534850642085075, Final Batch Loss: 0.08598101884126663\n",
      "Epoch 2123, Loss: 0.21818801760673523, Final Batch Loss: 0.032930441200733185\n",
      "Epoch 2124, Loss: 0.24451138451695442, Final Batch Loss: 0.035779472440481186\n",
      "Epoch 2125, Loss: 0.2484777867794037, Final Batch Loss: 0.037503212690353394\n",
      "Epoch 2126, Loss: 0.2936836965382099, Final Batch Loss: 0.052619170397520065\n",
      "Epoch 2127, Loss: 0.19076960161328316, Final Batch Loss: 0.05183668062090874\n",
      "Epoch 2128, Loss: 0.2676832005381584, Final Batch Loss: 0.1054169237613678\n",
      "Epoch 2129, Loss: 0.1918146200478077, Final Batch Loss: 0.04763166233897209\n",
      "Epoch 2130, Loss: 0.20192690566182137, Final Batch Loss: 0.04663320258259773\n",
      "Epoch 2131, Loss: 0.2477462813258171, Final Batch Loss: 0.05133166164159775\n",
      "Epoch 2132, Loss: 0.19249075278639793, Final Batch Loss: 0.05482175201177597\n",
      "Epoch 2133, Loss: 0.24596579745411873, Final Batch Loss: 0.08384166657924652\n",
      "Epoch 2134, Loss: 0.29910338670015335, Final Batch Loss: 0.10222536325454712\n",
      "Epoch 2135, Loss: 0.2456039935350418, Final Batch Loss: 0.10178093612194061\n",
      "Epoch 2136, Loss: 0.23103446513414383, Final Batch Loss: 0.05295950546860695\n",
      "Epoch 2137, Loss: 0.25968391448259354, Final Batch Loss: 0.03110738843679428\n",
      "Epoch 2138, Loss: 0.3197837918996811, Final Batch Loss: 0.0936509221792221\n",
      "Epoch 2139, Loss: 0.26436199992895126, Final Batch Loss: 0.0948270931839943\n",
      "Epoch 2140, Loss: 0.24717041105031967, Final Batch Loss: 0.031757187098264694\n",
      "Epoch 2141, Loss: 0.2534824050962925, Final Batch Loss: 0.06807529181241989\n",
      "Epoch 2142, Loss: 0.19033029675483704, Final Batch Loss: 0.03358760103583336\n",
      "Epoch 2143, Loss: 0.3086642883718014, Final Batch Loss: 0.05857725068926811\n",
      "Epoch 2144, Loss: 0.33312641829252243, Final Batch Loss: 0.12925508618354797\n",
      "Epoch 2145, Loss: 0.28349414467811584, Final Batch Loss: 0.10727590322494507\n",
      "Epoch 2146, Loss: 0.2942516691982746, Final Batch Loss: 0.07520989328622818\n",
      "Epoch 2147, Loss: 0.3256395757198334, Final Batch Loss: 0.12303785979747772\n",
      "Epoch 2148, Loss: 0.24640626460313797, Final Batch Loss: 0.02154359593987465\n",
      "Epoch 2149, Loss: 0.1892128884792328, Final Batch Loss: 0.02469060942530632\n",
      "Epoch 2150, Loss: 0.29120080173015594, Final Batch Loss: 0.11230923235416412\n",
      "Epoch 2151, Loss: 0.2572053838521242, Final Batch Loss: 0.0758509561419487\n",
      "Epoch 2152, Loss: 0.2588103674352169, Final Batch Loss: 0.06633317470550537\n",
      "Epoch 2153, Loss: 0.301742572337389, Final Batch Loss: 0.09403367340564728\n",
      "Epoch 2154, Loss: 0.26801125332713127, Final Batch Loss: 0.04670610651373863\n",
      "Epoch 2155, Loss: 0.3130627628415823, Final Batch Loss: 0.12322591245174408\n",
      "Epoch 2156, Loss: 0.2602877840399742, Final Batch Loss: 0.0633760318160057\n",
      "Epoch 2157, Loss: 0.25694110058248043, Final Batch Loss: 0.02012590877711773\n",
      "Epoch 2158, Loss: 0.255283460021019, Final Batch Loss: 0.07345772534608841\n",
      "Epoch 2159, Loss: 0.24230113998055458, Final Batch Loss: 0.05441342294216156\n",
      "Epoch 2160, Loss: 0.1859387382864952, Final Batch Loss: 0.0323864221572876\n",
      "Epoch 2161, Loss: 0.3640510104596615, Final Batch Loss: 0.11084527522325516\n",
      "Epoch 2162, Loss: 0.3282614201307297, Final Batch Loss: 0.15528079867362976\n",
      "Epoch 2163, Loss: 0.44135843589901924, Final Batch Loss: 0.11965753883123398\n",
      "Epoch 2164, Loss: 0.26685068756341934, Final Batch Loss: 0.07204261422157288\n",
      "Epoch 2165, Loss: 0.37557394430041313, Final Batch Loss: 0.15781891345977783\n",
      "Epoch 2166, Loss: 0.4162379018962383, Final Batch Loss: 0.07852239906787872\n",
      "Epoch 2167, Loss: 0.36211641132831573, Final Batch Loss: 0.03526142239570618\n",
      "Epoch 2168, Loss: 0.3565268740057945, Final Batch Loss: 0.09339345991611481\n",
      "Epoch 2169, Loss: 0.3161362111568451, Final Batch Loss: 0.13689841330051422\n",
      "Epoch 2170, Loss: 0.2714298702776432, Final Batch Loss: 0.03299245610833168\n",
      "Epoch 2171, Loss: 0.2965796925127506, Final Batch Loss: 0.03339514508843422\n",
      "Epoch 2172, Loss: 0.24983744323253632, Final Batch Loss: 0.06886716187000275\n",
      "Epoch 2173, Loss: 0.2984512895345688, Final Batch Loss: 0.04192163795232773\n",
      "Epoch 2174, Loss: 0.2768244631588459, Final Batch Loss: 0.01669781282544136\n",
      "Epoch 2175, Loss: 0.26567529886960983, Final Batch Loss: 0.06885773688554764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2176, Loss: 0.2457980178296566, Final Batch Loss: 0.09975134581327438\n",
      "Epoch 2177, Loss: 0.27954255789518356, Final Batch Loss: 0.07363124191761017\n",
      "Epoch 2178, Loss: 0.2793011702597141, Final Batch Loss: 0.06186223775148392\n",
      "Epoch 2179, Loss: 0.23765693977475166, Final Batch Loss: 0.06869371980428696\n",
      "Epoch 2180, Loss: 0.23877912759780884, Final Batch Loss: 0.0378679484128952\n",
      "Epoch 2181, Loss: 0.21871327236294746, Final Batch Loss: 0.045079462230205536\n",
      "Epoch 2182, Loss: 0.2575768157839775, Final Batch Loss: 0.05978870764374733\n",
      "Epoch 2183, Loss: 0.2786783166229725, Final Batch Loss: 0.09529164433479309\n",
      "Epoch 2184, Loss: 0.20927648711949587, Final Batch Loss: 0.010972217656672001\n",
      "Epoch 2185, Loss: 0.23554221540689468, Final Batch Loss: 0.0449683740735054\n",
      "Epoch 2186, Loss: 0.20696831122040749, Final Batch Loss: 0.05429757758975029\n",
      "Epoch 2187, Loss: 0.20722227543592453, Final Batch Loss: 0.0384795106947422\n",
      "Epoch 2188, Loss: 0.24731745198369026, Final Batch Loss: 0.056494057178497314\n",
      "Epoch 2189, Loss: 0.22032329812645912, Final Batch Loss: 0.035092663019895554\n",
      "Epoch 2190, Loss: 0.3740443494170904, Final Batch Loss: 0.2372855544090271\n",
      "Epoch 2191, Loss: 0.19326253980398178, Final Batch Loss: 0.03997395560145378\n",
      "Epoch 2192, Loss: 0.27760619670152664, Final Batch Loss: 0.09509561210870743\n",
      "Epoch 2193, Loss: 0.1887111458927393, Final Batch Loss: 0.020189916715025902\n",
      "Epoch 2194, Loss: 0.3037229999899864, Final Batch Loss: 0.0610375814139843\n",
      "Epoch 2195, Loss: 0.255095724016428, Final Batch Loss: 0.08533545583486557\n",
      "Epoch 2196, Loss: 0.2190239503979683, Final Batch Loss: 0.06998053193092346\n",
      "Epoch 2197, Loss: 0.35783663019537926, Final Batch Loss: 0.07600433379411697\n",
      "Epoch 2198, Loss: 0.2091471403837204, Final Batch Loss: 0.04072248563170433\n",
      "Epoch 2199, Loss: 0.29648565500974655, Final Batch Loss: 0.1295180320739746\n",
      "Epoch 2200, Loss: 0.20623328536748886, Final Batch Loss: 0.027975179255008698\n",
      "Epoch 2201, Loss: 0.2098452765494585, Final Batch Loss: 0.041512396186590195\n",
      "Epoch 2202, Loss: 0.24515824764966965, Final Batch Loss: 0.043892424553632736\n",
      "Epoch 2203, Loss: 0.2437489926815033, Final Batch Loss: 0.041484538465738297\n",
      "Epoch 2204, Loss: 0.2910054996609688, Final Batch Loss: 0.05076063051819801\n",
      "Epoch 2205, Loss: 0.1916993372142315, Final Batch Loss: 0.025911957025527954\n",
      "Epoch 2206, Loss: 0.2127523459494114, Final Batch Loss: 0.052086859941482544\n",
      "Epoch 2207, Loss: 0.24600084125995636, Final Batch Loss: 0.06465493887662888\n",
      "Epoch 2208, Loss: 0.19310637190937996, Final Batch Loss: 0.06034905090928078\n",
      "Epoch 2209, Loss: 0.1683255322277546, Final Batch Loss: 0.053520988672971725\n",
      "Epoch 2210, Loss: 0.19727591797709465, Final Batch Loss: 0.02822917327284813\n",
      "Epoch 2211, Loss: 0.19642185978591442, Final Batch Loss: 0.0281828623265028\n",
      "Epoch 2212, Loss: 0.276633869856596, Final Batch Loss: 0.049565400928258896\n",
      "Epoch 2213, Loss: 0.21707897633314133, Final Batch Loss: 0.04507235065102577\n",
      "Epoch 2214, Loss: 0.2256644442677498, Final Batch Loss: 0.056787602603435516\n",
      "Epoch 2215, Loss: 0.27573465183377266, Final Batch Loss: 0.06284145265817642\n",
      "Epoch 2216, Loss: 0.22219388745725155, Final Batch Loss: 0.07390358299016953\n",
      "Epoch 2217, Loss: 0.24320489168167114, Final Batch Loss: 0.09321153163909912\n",
      "Epoch 2218, Loss: 0.20431193336844444, Final Batch Loss: 0.02409004047513008\n",
      "Epoch 2219, Loss: 0.2162080816924572, Final Batch Loss: 0.057726409286260605\n",
      "Epoch 2220, Loss: 0.3170725330710411, Final Batch Loss: 0.04860137403011322\n",
      "Epoch 2221, Loss: 0.17301414534449577, Final Batch Loss: 0.0459911972284317\n",
      "Epoch 2222, Loss: 0.3530387692153454, Final Batch Loss: 0.11064966768026352\n",
      "Epoch 2223, Loss: 0.3021596632897854, Final Batch Loss: 0.12379980832338333\n",
      "Epoch 2224, Loss: 0.2621305473148823, Final Batch Loss: 0.06903763860464096\n",
      "Epoch 2225, Loss: 0.2451389841735363, Final Batch Loss: 0.0867273360490799\n",
      "Epoch 2226, Loss: 0.2568792551755905, Final Batch Loss: 0.061728205531835556\n",
      "Epoch 2227, Loss: 0.23355262726545334, Final Batch Loss: 0.029712073504924774\n",
      "Epoch 2228, Loss: 0.28436746448278427, Final Batch Loss: 0.069273941218853\n",
      "Epoch 2229, Loss: 0.14082159847021103, Final Batch Loss: 0.01790100336074829\n",
      "Epoch 2230, Loss: 0.27348050475120544, Final Batch Loss: 0.0916363000869751\n",
      "Epoch 2231, Loss: 0.3246666453778744, Final Batch Loss: 0.10148639231920242\n",
      "Epoch 2232, Loss: 0.16518279165029526, Final Batch Loss: 0.03343818336725235\n",
      "Epoch 2233, Loss: 0.29448623210191727, Final Batch Loss: 0.05948600172996521\n",
      "Epoch 2234, Loss: 0.16609144490212202, Final Batch Loss: 0.009613304398953915\n",
      "Epoch 2235, Loss: 0.2164201308041811, Final Batch Loss: 0.023122811689972878\n",
      "Epoch 2236, Loss: 0.25319407880306244, Final Batch Loss: 0.060869891196489334\n",
      "Epoch 2237, Loss: 0.23334823548793793, Final Batch Loss: 0.06271692365407944\n",
      "Epoch 2238, Loss: 0.22293240204453468, Final Batch Loss: 0.016891203820705414\n",
      "Epoch 2239, Loss: 0.24091634526848793, Final Batch Loss: 0.05829673632979393\n",
      "Epoch 2240, Loss: 0.24004598706960678, Final Batch Loss: 0.053019654005765915\n",
      "Epoch 2241, Loss: 0.34178342670202255, Final Batch Loss: 0.03260327875614166\n",
      "Epoch 2242, Loss: 0.21027671173214912, Final Batch Loss: 0.027903325855731964\n",
      "Epoch 2243, Loss: 0.27974309772253036, Final Batch Loss: 0.09546362608671188\n",
      "Epoch 2244, Loss: 0.2805156037211418, Final Batch Loss: 0.07176152616739273\n",
      "Epoch 2245, Loss: 0.22428741119801998, Final Batch Loss: 0.02674364484846592\n",
      "Epoch 2246, Loss: 0.3965965658426285, Final Batch Loss: 0.18583108484745026\n",
      "Epoch 2247, Loss: 0.23865864053368568, Final Batch Loss: 0.03864198923110962\n",
      "Epoch 2248, Loss: 0.22869404032826424, Final Batch Loss: 0.06544727087020874\n",
      "Epoch 2249, Loss: 0.16225621104240417, Final Batch Loss: 0.020614132285118103\n",
      "Epoch 2250, Loss: 0.24137894064188004, Final Batch Loss: 0.06379205733537674\n",
      "Epoch 2251, Loss: 0.17706764116883278, Final Batch Loss: 0.03521740436553955\n",
      "Epoch 2252, Loss: 0.21107809245586395, Final Batch Loss: 0.06030522286891937\n",
      "Epoch 2253, Loss: 0.15304600074887276, Final Batch Loss: 0.04910414665937424\n",
      "Epoch 2254, Loss: 0.20818262174725533, Final Batch Loss: 0.047305572777986526\n",
      "Epoch 2255, Loss: 0.28165552392601967, Final Batch Loss: 0.08967164158821106\n",
      "Epoch 2256, Loss: 0.2499503158032894, Final Batch Loss: 0.043720416724681854\n",
      "Epoch 2257, Loss: 0.2662617042660713, Final Batch Loss: 0.03165121749043465\n",
      "Epoch 2258, Loss: 0.27476031333208084, Final Batch Loss: 0.04642252251505852\n",
      "Epoch 2259, Loss: 0.16970039159059525, Final Batch Loss: 0.01624048501253128\n",
      "Epoch 2260, Loss: 0.20109792053699493, Final Batch Loss: 0.07224159687757492\n",
      "Epoch 2261, Loss: 0.2090940922498703, Final Batch Loss: 0.033939749002456665\n",
      "Epoch 2262, Loss: 0.18521957658231258, Final Batch Loss: 0.029921097680926323\n",
      "Epoch 2263, Loss: 0.1764868814498186, Final Batch Loss: 0.02872706577181816\n",
      "Epoch 2264, Loss: 0.2300940342247486, Final Batch Loss: 0.040622685104608536\n",
      "Epoch 2265, Loss: 0.2613185755908489, Final Batch Loss: 0.02974671870470047\n",
      "Epoch 2266, Loss: 0.2256362959742546, Final Batch Loss: 0.05085363984107971\n",
      "Epoch 2267, Loss: 0.2428189292550087, Final Batch Loss: 0.0724359080195427\n",
      "Epoch 2268, Loss: 0.2264661304652691, Final Batch Loss: 0.028601665049791336\n",
      "Epoch 2269, Loss: 0.2014583721756935, Final Batch Loss: 0.03148999810218811\n",
      "Epoch 2270, Loss: 0.2450980804860592, Final Batch Loss: 0.10917658358812332\n",
      "Epoch 2271, Loss: 0.23651529103517532, Final Batch Loss: 0.06107914447784424\n",
      "Epoch 2272, Loss: 0.20234769955277443, Final Batch Loss: 0.04340368136763573\n",
      "Epoch 2273, Loss: 0.27533531934022903, Final Batch Loss: 0.053064797073602676\n",
      "Epoch 2274, Loss: 0.2032981961965561, Final Batch Loss: 0.02033688873052597\n",
      "Epoch 2275, Loss: 0.17437116615474224, Final Batch Loss: 0.010751636698842049\n",
      "Epoch 2276, Loss: 0.217633917927742, Final Batch Loss: 0.03357850760221481\n",
      "Epoch 2277, Loss: 0.19362071715295315, Final Batch Loss: 0.01969454251229763\n",
      "Epoch 2278, Loss: 0.19513437431305647, Final Batch Loss: 0.008195540867745876\n",
      "Epoch 2279, Loss: 0.23645112663507462, Final Batch Loss: 0.07533696293830872\n",
      "Epoch 2280, Loss: 0.23833218216896057, Final Batch Loss: 0.05956771969795227\n",
      "Epoch 2281, Loss: 0.20878847315907478, Final Batch Loss: 0.043730076402425766\n",
      "Epoch 2282, Loss: 0.22869573906064034, Final Batch Loss: 0.0285269133746624\n",
      "Epoch 2283, Loss: 0.22631292790174484, Final Batch Loss: 0.083295077085495\n",
      "Epoch 2284, Loss: 0.22462670505046844, Final Batch Loss: 0.07268015295267105\n",
      "Epoch 2285, Loss: 0.23431098088622093, Final Batch Loss: 0.0654359683394432\n",
      "Epoch 2286, Loss: 0.2210734263062477, Final Batch Loss: 0.028520997613668442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2287, Loss: 0.23347270861268044, Final Batch Loss: 0.0546480156481266\n",
      "Epoch 2288, Loss: 0.24991383776068687, Final Batch Loss: 0.09944196045398712\n",
      "Epoch 2289, Loss: 0.16347862035036087, Final Batch Loss: 0.018677428364753723\n",
      "Epoch 2290, Loss: 0.23207739926874638, Final Batch Loss: 0.026810387149453163\n",
      "Epoch 2291, Loss: 0.22265179734677076, Final Batch Loss: 0.00911859329789877\n",
      "Epoch 2292, Loss: 0.251097708940506, Final Batch Loss: 0.08194620162248611\n",
      "Epoch 2293, Loss: 0.29773683473467827, Final Batch Loss: 0.08148982375860214\n",
      "Epoch 2294, Loss: 0.1877676397562027, Final Batch Loss: 0.0159207284450531\n",
      "Epoch 2295, Loss: 0.23334873840212822, Final Batch Loss: 0.11104651540517807\n",
      "Epoch 2296, Loss: 0.20418847724795341, Final Batch Loss: 0.058939073234796524\n",
      "Epoch 2297, Loss: 0.22298365086317062, Final Batch Loss: 0.03538334369659424\n",
      "Epoch 2298, Loss: 0.23572023212909698, Final Batch Loss: 0.08677033334970474\n",
      "Epoch 2299, Loss: 0.2864893339574337, Final Batch Loss: 0.050077397376298904\n",
      "Epoch 2300, Loss: 0.20682409778237343, Final Batch Loss: 0.048620596528053284\n",
      "Epoch 2301, Loss: 0.22874630242586136, Final Batch Loss: 0.024416647851467133\n",
      "Epoch 2302, Loss: 0.2757238820195198, Final Batch Loss: 0.05295678600668907\n",
      "Epoch 2303, Loss: 0.20663517713546753, Final Batch Loss: 0.04166053235530853\n",
      "Epoch 2304, Loss: 0.23808394372463226, Final Batch Loss: 0.03431069850921631\n",
      "Epoch 2305, Loss: 0.24335037358105183, Final Batch Loss: 0.018388492986559868\n",
      "Epoch 2306, Loss: 0.168562114238739, Final Batch Loss: 0.035247113555669785\n",
      "Epoch 2307, Loss: 0.22988388687372208, Final Batch Loss: 0.045256108045578\n",
      "Epoch 2308, Loss: 0.2665964923799038, Final Batch Loss: 0.15824991464614868\n",
      "Epoch 2309, Loss: 0.25548170506954193, Final Batch Loss: 0.08356305956840515\n",
      "Epoch 2310, Loss: 0.33706727623939514, Final Batch Loss: 0.08664008229970932\n",
      "Epoch 2311, Loss: 0.2848329097032547, Final Batch Loss: 0.04432685673236847\n",
      "Epoch 2312, Loss: 0.19811625592410564, Final Batch Loss: 0.029856296256184578\n",
      "Epoch 2313, Loss: 0.23734460771083832, Final Batch Loss: 0.06924483180046082\n",
      "Epoch 2314, Loss: 0.3719044551253319, Final Batch Loss: 0.10396548360586166\n",
      "Epoch 2315, Loss: 0.33196016401052475, Final Batch Loss: 0.09973569214344025\n",
      "Epoch 2316, Loss: 0.25683654844760895, Final Batch Loss: 0.09588413685560226\n",
      "Epoch 2317, Loss: 0.31549055129289627, Final Batch Loss: 0.11187682300806046\n",
      "Epoch 2318, Loss: 0.2702274676412344, Final Batch Loss: 0.02947702817618847\n",
      "Epoch 2319, Loss: 0.2843273654580116, Final Batch Loss: 0.07667315751314163\n",
      "Epoch 2320, Loss: 0.33646394684910774, Final Batch Loss: 0.120454341173172\n",
      "Epoch 2321, Loss: 0.2679502833634615, Final Batch Loss: 0.02654903568327427\n",
      "Epoch 2322, Loss: 0.27723274007439613, Final Batch Loss: 0.08506258577108383\n",
      "Epoch 2323, Loss: 0.3102284222841263, Final Batch Loss: 0.10268699377775192\n",
      "Epoch 2324, Loss: 0.2591988444328308, Final Batch Loss: 0.08780911564826965\n",
      "Epoch 2325, Loss: 0.27497267350554466, Final Batch Loss: 0.051985859870910645\n",
      "Epoch 2326, Loss: 0.2673373147845268, Final Batch Loss: 0.055454421788454056\n",
      "Epoch 2327, Loss: 0.3692917414009571, Final Batch Loss: 0.17745903134346008\n",
      "Epoch 2328, Loss: 0.38634441420435905, Final Batch Loss: 0.08832084387540817\n",
      "Epoch 2329, Loss: 0.37577157840132713, Final Batch Loss: 0.19716763496398926\n",
      "Epoch 2330, Loss: 0.579773023724556, Final Batch Loss: 0.25765499472618103\n",
      "Epoch 2331, Loss: 0.43254221230745316, Final Batch Loss: 0.11872366070747375\n",
      "Epoch 2332, Loss: 0.36525561287999153, Final Batch Loss: 0.1971341371536255\n",
      "Epoch 2333, Loss: 0.31908101215958595, Final Batch Loss: 0.05985574051737785\n",
      "Epoch 2334, Loss: 0.39456500113010406, Final Batch Loss: 0.07926440238952637\n",
      "Epoch 2335, Loss: 0.2653185874223709, Final Batch Loss: 0.04131171107292175\n",
      "Epoch 2336, Loss: 0.42442190647125244, Final Batch Loss: 0.19383050501346588\n",
      "Epoch 2337, Loss: 0.2782921865582466, Final Batch Loss: 0.05516684800386429\n",
      "Epoch 2338, Loss: 0.3332030698657036, Final Batch Loss: 0.10263818502426147\n",
      "Epoch 2339, Loss: 0.21675580739974976, Final Batch Loss: 0.05426570400595665\n",
      "Epoch 2340, Loss: 0.2197667844593525, Final Batch Loss: 0.07092874497175217\n",
      "Epoch 2341, Loss: 0.2052425630390644, Final Batch Loss: 0.04461852088570595\n",
      "Epoch 2342, Loss: 0.20236924290657043, Final Batch Loss: 0.022431250661611557\n",
      "Epoch 2343, Loss: 0.2570258118212223, Final Batch Loss: 0.09584325551986694\n",
      "Epoch 2344, Loss: 0.2084745392203331, Final Batch Loss: 0.06368045508861542\n",
      "Epoch 2345, Loss: 0.26799725368618965, Final Batch Loss: 0.07486710697412491\n",
      "Epoch 2346, Loss: 0.269608698785305, Final Batch Loss: 0.06517013907432556\n",
      "Epoch 2347, Loss: 0.24515517428517342, Final Batch Loss: 0.04893352836370468\n",
      "Epoch 2348, Loss: 0.2240682691335678, Final Batch Loss: 0.06949902325868607\n",
      "Epoch 2349, Loss: 0.24666161090135574, Final Batch Loss: 0.0839700996875763\n",
      "Epoch 2350, Loss: 0.2364490032196045, Final Batch Loss: 0.052947092801332474\n",
      "Epoch 2351, Loss: 0.2601160891354084, Final Batch Loss: 0.06820328533649445\n",
      "Epoch 2352, Loss: 0.24176336824893951, Final Batch Loss: 0.029283057898283005\n",
      "Epoch 2353, Loss: 0.2385706678032875, Final Batch Loss: 0.07921404391527176\n",
      "Epoch 2354, Loss: 0.2765093334019184, Final Batch Loss: 0.047473762184381485\n",
      "Epoch 2355, Loss: 0.2408984899520874, Final Batch Loss: 0.04167523607611656\n",
      "Epoch 2356, Loss: 0.24329976364970207, Final Batch Loss: 0.05628068745136261\n",
      "Epoch 2357, Loss: 0.34573082625865936, Final Batch Loss: 0.1383110135793686\n",
      "Epoch 2358, Loss: 0.2629306949675083, Final Batch Loss: 0.04810843616724014\n",
      "Epoch 2359, Loss: 0.15838053449988365, Final Batch Loss: 0.037227507680654526\n",
      "Epoch 2360, Loss: 0.24443740025162697, Final Batch Loss: 0.07944465428590775\n",
      "Epoch 2361, Loss: 0.21000919118523598, Final Batch Loss: 0.020607616752386093\n",
      "Epoch 2362, Loss: 0.229077760130167, Final Batch Loss: 0.037766408175230026\n",
      "Epoch 2363, Loss: 0.3140998035669327, Final Batch Loss: 0.13586094975471497\n",
      "Epoch 2364, Loss: 0.2573002129793167, Final Batch Loss: 0.062153663486242294\n",
      "Epoch 2365, Loss: 0.28045435436069965, Final Batch Loss: 0.11761955916881561\n",
      "Epoch 2366, Loss: 0.1855792384594679, Final Batch Loss: 0.028352847322821617\n",
      "Epoch 2367, Loss: 0.2978963367640972, Final Batch Loss: 0.12836331129074097\n",
      "Epoch 2368, Loss: 0.3264744356274605, Final Batch Loss: 0.12058461457490921\n",
      "Epoch 2369, Loss: 0.28420595452189445, Final Batch Loss: 0.060211434960365295\n",
      "Epoch 2370, Loss: 0.322001364082098, Final Batch Loss: 0.14201676845550537\n",
      "Epoch 2371, Loss: 0.18589551374316216, Final Batch Loss: 0.04460013657808304\n",
      "Epoch 2372, Loss: 0.2503780759871006, Final Batch Loss: 0.06379229575395584\n",
      "Epoch 2373, Loss: 0.17529487237334251, Final Batch Loss: 0.03650317341089249\n",
      "Epoch 2374, Loss: 0.203392181545496, Final Batch Loss: 0.06253046542406082\n",
      "Epoch 2375, Loss: 0.2588808611035347, Final Batch Loss: 0.03901628032326698\n",
      "Epoch 2376, Loss: 0.2655199095606804, Final Batch Loss: 0.06932854652404785\n",
      "Epoch 2377, Loss: 0.16553692147135735, Final Batch Loss: 0.05424037203192711\n",
      "Epoch 2378, Loss: 0.25354650616645813, Final Batch Loss: 0.07241038233041763\n",
      "Epoch 2379, Loss: 0.200966477394104, Final Batch Loss: 0.054309871047735214\n",
      "Epoch 2380, Loss: 0.228256328497082, Final Batch Loss: 0.005959814880043268\n",
      "Epoch 2381, Loss: 0.20225709304213524, Final Batch Loss: 0.03240309655666351\n",
      "Epoch 2382, Loss: 0.23697924613952637, Final Batch Loss: 0.08677393198013306\n",
      "Epoch 2383, Loss: 0.27484994009137154, Final Batch Loss: 0.06845726072788239\n",
      "Epoch 2384, Loss: 0.20733867399394512, Final Batch Loss: 0.025335511192679405\n",
      "Epoch 2385, Loss: 0.23686867579817772, Final Batch Loss: 0.060186054557561874\n",
      "Epoch 2386, Loss: 0.33064430952072144, Final Batch Loss: 0.11612509936094284\n",
      "Epoch 2387, Loss: 0.23883990570902824, Final Batch Loss: 0.03236359730362892\n",
      "Epoch 2388, Loss: 0.28695469722151756, Final Batch Loss: 0.03784000501036644\n",
      "Epoch 2389, Loss: 0.27801158279180527, Final Batch Loss: 0.05811023339629173\n",
      "Epoch 2390, Loss: 0.18775148503482342, Final Batch Loss: 0.012621035799384117\n",
      "Epoch 2391, Loss: 0.20933767035603523, Final Batch Loss: 0.03788510337471962\n",
      "Epoch 2392, Loss: 0.23203831538558006, Final Batch Loss: 0.035135526210069656\n",
      "Epoch 2393, Loss: 0.2047276832163334, Final Batch Loss: 0.04623562470078468\n",
      "Epoch 2394, Loss: 0.15057694166898727, Final Batch Loss: 0.016678329557180405\n",
      "Epoch 2395, Loss: 0.22596385702490807, Final Batch Loss: 0.042173150926828384\n",
      "Epoch 2396, Loss: 0.21208027936518192, Final Batch Loss: 0.026366228237748146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2397, Loss: 0.1799248531460762, Final Batch Loss: 0.05081849545240402\n",
      "Epoch 2398, Loss: 0.2661005835980177, Final Batch Loss: 0.08926507085561752\n",
      "Epoch 2399, Loss: 0.23061734065413475, Final Batch Loss: 0.05721230432391167\n",
      "Epoch 2400, Loss: 0.2670709677040577, Final Batch Loss: 0.10528922826051712\n",
      "Epoch 2401, Loss: 0.2690899074077606, Final Batch Loss: 0.049799561500549316\n",
      "Epoch 2402, Loss: 0.18168324045836926, Final Batch Loss: 0.01970241405069828\n",
      "Epoch 2403, Loss: 0.25614524260163307, Final Batch Loss: 0.08394376188516617\n",
      "Epoch 2404, Loss: 0.30905288085341454, Final Batch Loss: 0.059638503938913345\n",
      "Epoch 2405, Loss: 0.2742762193083763, Final Batch Loss: 0.08066531270742416\n",
      "Epoch 2406, Loss: 0.2497065868228674, Final Batch Loss: 0.031220318749547005\n",
      "Epoch 2407, Loss: 0.23128752782940865, Final Batch Loss: 0.07165856659412384\n",
      "Epoch 2408, Loss: 0.1785760000348091, Final Batch Loss: 0.013473276048898697\n",
      "Epoch 2409, Loss: 0.23839707300066948, Final Batch Loss: 0.056523002684116364\n",
      "Epoch 2410, Loss: 0.17517738975584507, Final Batch Loss: 0.04220597818493843\n",
      "Epoch 2411, Loss: 0.18086042627692223, Final Batch Loss: 0.06318087875843048\n",
      "Epoch 2412, Loss: 0.1915941722691059, Final Batch Loss: 0.09111019968986511\n",
      "Epoch 2413, Loss: 0.2375117875635624, Final Batch Loss: 0.0697278380393982\n",
      "Epoch 2414, Loss: 0.18954188376665115, Final Batch Loss: 0.05361888185143471\n",
      "Epoch 2415, Loss: 0.2790554352104664, Final Batch Loss: 0.1006849929690361\n",
      "Epoch 2416, Loss: 0.18667032569646835, Final Batch Loss: 0.05318770185112953\n",
      "Epoch 2417, Loss: 0.21925444342195988, Final Batch Loss: 0.04630850628018379\n",
      "Epoch 2418, Loss: 0.2941202335059643, Final Batch Loss: 0.10804769396781921\n",
      "Epoch 2419, Loss: 0.20242789015173912, Final Batch Loss: 0.050088558346033096\n",
      "Epoch 2420, Loss: 0.228533536195755, Final Batch Loss: 0.05334225669503212\n",
      "Epoch 2421, Loss: 0.21990875899791718, Final Batch Loss: 0.06009335070848465\n",
      "Epoch 2422, Loss: 0.20377599447965622, Final Batch Loss: 0.037506744265556335\n",
      "Epoch 2423, Loss: 0.170628996565938, Final Batch Loss: 0.026125965639948845\n",
      "Epoch 2424, Loss: 0.23829533532261848, Final Batch Loss: 0.03537782281637192\n",
      "Epoch 2425, Loss: 0.179048053920269, Final Batch Loss: 0.0654960498213768\n",
      "Epoch 2426, Loss: 0.19180290959775448, Final Batch Loss: 0.02035669796168804\n",
      "Epoch 2427, Loss: 0.19496573321521282, Final Batch Loss: 0.05428541451692581\n",
      "Epoch 2428, Loss: 0.1214109817519784, Final Batch Loss: 0.01278888899832964\n",
      "Epoch 2429, Loss: 0.24948999285697937, Final Batch Loss: 0.05258451774716377\n",
      "Epoch 2430, Loss: 0.21613750606775284, Final Batch Loss: 0.08234364539384842\n",
      "Epoch 2431, Loss: 0.22288182750344276, Final Batch Loss: 0.07455099374055862\n",
      "Epoch 2432, Loss: 0.1886899434030056, Final Batch Loss: 0.04026774689555168\n",
      "Epoch 2433, Loss: 0.2526530884206295, Final Batch Loss: 0.03500327467918396\n",
      "Epoch 2434, Loss: 0.21032490581274033, Final Batch Loss: 0.043999314308166504\n",
      "Epoch 2435, Loss: 0.23820848390460014, Final Batch Loss: 0.02167433127760887\n",
      "Epoch 2436, Loss: 0.3001638762652874, Final Batch Loss: 0.05919475480914116\n",
      "Epoch 2437, Loss: 0.2973824068903923, Final Batch Loss: 0.08009801805019379\n",
      "Epoch 2438, Loss: 0.22492742538452148, Final Batch Loss: 0.031935758888721466\n",
      "Epoch 2439, Loss: 0.2513000536710024, Final Batch Loss: 0.06457050889730453\n",
      "Epoch 2440, Loss: 0.2053019218146801, Final Batch Loss: 0.03158299997448921\n",
      "Epoch 2441, Loss: 0.2621222473680973, Final Batch Loss: 0.07059004157781601\n",
      "Epoch 2442, Loss: 0.19358387030661106, Final Batch Loss: 0.03058585710823536\n",
      "Epoch 2443, Loss: 0.20099132135510445, Final Batch Loss: 0.04214603453874588\n",
      "Epoch 2444, Loss: 0.26487215235829353, Final Batch Loss: 0.08536814898252487\n",
      "Epoch 2445, Loss: 0.1520820576697588, Final Batch Loss: 0.030441582202911377\n",
      "Epoch 2446, Loss: 0.2580597437918186, Final Batch Loss: 0.07914252579212189\n",
      "Epoch 2447, Loss: 0.1908925585448742, Final Batch Loss: 0.02055053785443306\n",
      "Epoch 2448, Loss: 0.2768232859671116, Final Batch Loss: 0.10789792984724045\n",
      "Epoch 2449, Loss: 0.23013294488191605, Final Batch Loss: 0.10818980634212494\n",
      "Epoch 2450, Loss: 0.2554728053510189, Final Batch Loss: 0.08640025556087494\n",
      "Epoch 2451, Loss: 0.21465061604976654, Final Batch Loss: 0.0364711731672287\n",
      "Epoch 2452, Loss: 0.34821880608797073, Final Batch Loss: 0.17076319456100464\n",
      "Epoch 2453, Loss: 0.2440997064113617, Final Batch Loss: 0.03774794191122055\n",
      "Epoch 2454, Loss: 0.2536827586591244, Final Batch Loss: 0.06446981430053711\n",
      "Epoch 2455, Loss: 0.3392220661044121, Final Batch Loss: 0.10440609604120255\n",
      "Epoch 2456, Loss: 0.2093820758163929, Final Batch Loss: 0.052627649158239365\n",
      "Epoch 2457, Loss: 0.21630588732659817, Final Batch Loss: 0.025670694187283516\n",
      "Epoch 2458, Loss: 0.22524308413267136, Final Batch Loss: 0.046887073665857315\n",
      "Epoch 2459, Loss: 0.32202959433197975, Final Batch Loss: 0.07624422013759613\n",
      "Epoch 2460, Loss: 0.14625361002981663, Final Batch Loss: 0.030150147154927254\n",
      "Epoch 2461, Loss: 0.29630501940846443, Final Batch Loss: 0.08889338374137878\n",
      "Epoch 2462, Loss: 0.24120353534817696, Final Batch Loss: 0.045570824295282364\n",
      "Epoch 2463, Loss: 0.29533233493566513, Final Batch Loss: 0.08352375775575638\n",
      "Epoch 2464, Loss: 0.2394946850836277, Final Batch Loss: 0.08562619239091873\n",
      "Epoch 2465, Loss: 0.2890280894935131, Final Batch Loss: 0.09272900968790054\n",
      "Epoch 2466, Loss: 0.3197682425379753, Final Batch Loss: 0.07887545973062515\n",
      "Epoch 2467, Loss: 0.2044454775750637, Final Batch Loss: 0.04866497963666916\n",
      "Epoch 2468, Loss: 0.16820833459496498, Final Batch Loss: 0.026795465499162674\n",
      "Epoch 2469, Loss: 0.20851997658610344, Final Batch Loss: 0.03203931078314781\n",
      "Epoch 2470, Loss: 0.22903259098529816, Final Batch Loss: 0.05702162906527519\n",
      "Epoch 2471, Loss: 0.2000889889895916, Final Batch Loss: 0.06838532537221909\n",
      "Epoch 2472, Loss: 0.19448775053024292, Final Batch Loss: 0.04155305400490761\n",
      "Epoch 2473, Loss: 0.16074640303850174, Final Batch Loss: 0.013009484857320786\n",
      "Epoch 2474, Loss: 0.1560148298740387, Final Batch Loss: 0.01559821330010891\n",
      "Epoch 2475, Loss: 0.1939617209136486, Final Batch Loss: 0.028838537633419037\n",
      "Epoch 2476, Loss: 0.21747931372374296, Final Batch Loss: 0.014176945202052593\n",
      "Epoch 2477, Loss: 0.22292020171880722, Final Batch Loss: 0.044489484280347824\n",
      "Epoch 2478, Loss: 0.21551452204585075, Final Batch Loss: 0.05405908077955246\n",
      "Epoch 2479, Loss: 0.19205233827233315, Final Batch Loss: 0.03398972749710083\n",
      "Epoch 2480, Loss: 0.22679134085774422, Final Batch Loss: 0.041271138936281204\n",
      "Epoch 2481, Loss: 0.1825434621423483, Final Batch Loss: 0.013760773465037346\n",
      "Epoch 2482, Loss: 0.2981802970170975, Final Batch Loss: 0.1070074737071991\n",
      "Epoch 2483, Loss: 0.2123810090124607, Final Batch Loss: 0.034699101001024246\n",
      "Epoch 2484, Loss: 0.30030860006809235, Final Batch Loss: 0.07459696382284164\n",
      "Epoch 2485, Loss: 0.20606489479541779, Final Batch Loss: 0.03933948650956154\n",
      "Epoch 2486, Loss: 0.27522386983036995, Final Batch Loss: 0.08954958617687225\n",
      "Epoch 2487, Loss: 0.24602991715073586, Final Batch Loss: 0.02752416953444481\n",
      "Epoch 2488, Loss: 0.3638252504169941, Final Batch Loss: 0.0661272332072258\n",
      "Epoch 2489, Loss: 0.3068516440689564, Final Batch Loss: 0.059430647641420364\n",
      "Epoch 2490, Loss: 0.25155097991228104, Final Batch Loss: 0.04160786047577858\n",
      "Epoch 2491, Loss: 0.2638867199420929, Final Batch Loss: 0.05665656551718712\n",
      "Epoch 2492, Loss: 0.2225363366305828, Final Batch Loss: 0.03673180192708969\n",
      "Epoch 2493, Loss: 0.22289973869919777, Final Batch Loss: 0.07047269493341446\n",
      "Epoch 2494, Loss: 0.3100082129240036, Final Batch Loss: 0.13197800517082214\n",
      "Epoch 2495, Loss: 0.24764886125922203, Final Batch Loss: 0.059103891253471375\n",
      "Epoch 2496, Loss: 0.17394156008958817, Final Batch Loss: 0.023911062628030777\n",
      "Epoch 2497, Loss: 0.18766401335597038, Final Batch Loss: 0.027734067291021347\n",
      "Epoch 2498, Loss: 0.2436816319823265, Final Batch Loss: 0.036856409162282944\n",
      "Epoch 2499, Loss: 0.2585826590657234, Final Batch Loss: 0.059202902019023895\n",
      "Epoch 2500, Loss: 0.16193997487425804, Final Batch Loss: 0.03411118686199188\n",
      "Epoch 2501, Loss: 0.21103314124047756, Final Batch Loss: 0.057551298290491104\n",
      "Epoch 2502, Loss: 0.341138131916523, Final Batch Loss: 0.15340077877044678\n",
      "Epoch 2503, Loss: 0.34254441037774086, Final Batch Loss: 0.14224104583263397\n",
      "Epoch 2504, Loss: 0.20457562431693077, Final Batch Loss: 0.03943600133061409\n",
      "Epoch 2505, Loss: 0.3234397955238819, Final Batch Loss: 0.1358022540807724\n",
      "Epoch 2506, Loss: 0.30563241988420486, Final Batch Loss: 0.12710092961788177\n",
      "Epoch 2507, Loss: 0.24743041768670082, Final Batch Loss: 0.06048792973160744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2508, Loss: 0.19635925255715847, Final Batch Loss: 0.026083102449774742\n",
      "Epoch 2509, Loss: 0.3380207419395447, Final Batch Loss: 0.1272164285182953\n",
      "Epoch 2510, Loss: 0.25771019235253334, Final Batch Loss: 0.042630091309547424\n",
      "Epoch 2511, Loss: 0.2590908966958523, Final Batch Loss: 0.08184681087732315\n",
      "Epoch 2512, Loss: 0.20527425967156887, Final Batch Loss: 0.02656347118318081\n",
      "Epoch 2513, Loss: 0.26300638169050217, Final Batch Loss: 0.09304662048816681\n",
      "Epoch 2514, Loss: 0.2697400264441967, Final Batch Loss: 0.061659980565309525\n",
      "Epoch 2515, Loss: 0.23982643336057663, Final Batch Loss: 0.07990717887878418\n",
      "Epoch 2516, Loss: 0.20978760160505772, Final Batch Loss: 0.08942361176013947\n",
      "Epoch 2517, Loss: 0.2571401596069336, Final Batch Loss: 0.042772434651851654\n",
      "Epoch 2518, Loss: 0.17517882958054543, Final Batch Loss: 0.02093500643968582\n",
      "Epoch 2519, Loss: 0.27499278634786606, Final Batch Loss: 0.031898319721221924\n",
      "Epoch 2520, Loss: 0.2935705706477165, Final Batch Loss: 0.0386238656938076\n",
      "Epoch 2521, Loss: 0.2706793583929539, Final Batch Loss: 0.09608679264783859\n",
      "Epoch 2522, Loss: 0.345151886343956, Final Batch Loss: 0.1311608999967575\n",
      "Epoch 2523, Loss: 0.2908546179533005, Final Batch Loss: 0.06893911957740784\n",
      "Epoch 2524, Loss: 0.1984824389219284, Final Batch Loss: 0.018552083522081375\n",
      "Epoch 2525, Loss: 0.20806602761149406, Final Batch Loss: 0.07260029017925262\n",
      "Epoch 2526, Loss: 0.20060854125767946, Final Batch Loss: 0.011595484800636768\n",
      "Epoch 2527, Loss: 0.1532242614775896, Final Batch Loss: 0.035583171993494034\n",
      "Epoch 2528, Loss: 0.14695494435727596, Final Batch Loss: 0.021233269944787025\n",
      "Epoch 2529, Loss: 0.19980967417359352, Final Batch Loss: 0.05882963910698891\n",
      "Epoch 2530, Loss: 0.22080788761377335, Final Batch Loss: 0.07214964926242828\n",
      "Epoch 2531, Loss: 0.22673315927386284, Final Batch Loss: 0.04117322340607643\n",
      "Epoch 2532, Loss: 0.2574519105255604, Final Batch Loss: 0.04472579434514046\n",
      "Epoch 2533, Loss: 0.1805903259664774, Final Batch Loss: 0.018484653905034065\n",
      "Epoch 2534, Loss: 0.2564234919846058, Final Batch Loss: 0.05509944632649422\n",
      "Epoch 2535, Loss: 0.2315007895231247, Final Batch Loss: 0.025089185684919357\n",
      "Epoch 2536, Loss: 0.26058240979909897, Final Batch Loss: 0.148238867521286\n",
      "Epoch 2537, Loss: 0.2851452939212322, Final Batch Loss: 0.09188965708017349\n",
      "Epoch 2538, Loss: 0.2459430806338787, Final Batch Loss: 0.06685864180326462\n",
      "Epoch 2539, Loss: 0.24086690694093704, Final Batch Loss: 0.05535193905234337\n",
      "Epoch 2540, Loss: 0.19966425746679306, Final Batch Loss: 0.040485929697752\n",
      "Epoch 2541, Loss: 0.2852393873035908, Final Batch Loss: 0.06644665449857712\n",
      "Epoch 2542, Loss: 0.3233485296368599, Final Batch Loss: 0.13131746649742126\n",
      "Epoch 2543, Loss: 0.290778998285532, Final Batch Loss: 0.034841664135456085\n",
      "Epoch 2544, Loss: 0.3549721762537956, Final Batch Loss: 0.10945020616054535\n",
      "Epoch 2545, Loss: 0.24613142013549805, Final Batch Loss: 0.02549666166305542\n",
      "Epoch 2546, Loss: 0.27712592855095863, Final Batch Loss: 0.10525907576084137\n",
      "Epoch 2547, Loss: 0.18245509266853333, Final Batch Loss: 0.047721292823553085\n",
      "Epoch 2548, Loss: 0.2831253670156002, Final Batch Loss: 0.043407391756772995\n",
      "Epoch 2549, Loss: 0.23373936861753464, Final Batch Loss: 0.06779304146766663\n",
      "Epoch 2550, Loss: 0.2128676436841488, Final Batch Loss: 0.0705755427479744\n",
      "Epoch 2551, Loss: 0.2415531799197197, Final Batch Loss: 0.03977196663618088\n",
      "Epoch 2552, Loss: 0.21507915295660496, Final Batch Loss: 0.025400666519999504\n",
      "Epoch 2553, Loss: 0.18552290089428425, Final Batch Loss: 0.027402261272072792\n",
      "Epoch 2554, Loss: 0.24425868317484856, Final Batch Loss: 0.09164456278085709\n",
      "Epoch 2555, Loss: 0.31916167587041855, Final Batch Loss: 0.1526738703250885\n",
      "Epoch 2556, Loss: 0.2620455287396908, Final Batch Loss: 0.06957467645406723\n",
      "Epoch 2557, Loss: 0.30970413237810135, Final Batch Loss: 0.10499409586191177\n",
      "Epoch 2558, Loss: 0.2502382658421993, Final Batch Loss: 0.09895345568656921\n",
      "Epoch 2559, Loss: 0.2397016528993845, Final Batch Loss: 0.022142821922898293\n",
      "Epoch 2560, Loss: 0.2676308825612068, Final Batch Loss: 0.03203496336936951\n",
      "Epoch 2561, Loss: 0.3024023286998272, Final Batch Loss: 0.04995880648493767\n",
      "Epoch 2562, Loss: 0.1961678732186556, Final Batch Loss: 0.046735554933547974\n",
      "Epoch 2563, Loss: 0.2691144719719887, Final Batch Loss: 0.059547990560531616\n",
      "Epoch 2564, Loss: 0.23744186758995056, Final Batch Loss: 0.0404842384159565\n",
      "Epoch 2565, Loss: 0.33325257152318954, Final Batch Loss: 0.1049429401755333\n",
      "Epoch 2566, Loss: 0.2595203295350075, Final Batch Loss: 0.06576599180698395\n",
      "Epoch 2567, Loss: 0.246871929615736, Final Batch Loss: 0.08594817668199539\n",
      "Epoch 2568, Loss: 0.3055656887590885, Final Batch Loss: 0.12227936834096909\n",
      "Epoch 2569, Loss: 0.20720715261995792, Final Batch Loss: 0.025825249031186104\n",
      "Epoch 2570, Loss: 0.20773179829120636, Final Batch Loss: 0.048042722046375275\n",
      "Epoch 2571, Loss: 0.31271104514598846, Final Batch Loss: 0.07419842481613159\n",
      "Epoch 2572, Loss: 0.23645831272006035, Final Batch Loss: 0.07055174559354782\n",
      "Epoch 2573, Loss: 0.29445527866482735, Final Batch Loss: 0.08684803545475006\n",
      "Epoch 2574, Loss: 0.2165512926876545, Final Batch Loss: 0.027651768177747726\n",
      "Epoch 2575, Loss: 0.2089174035936594, Final Batch Loss: 0.061679065227508545\n",
      "Epoch 2576, Loss: 0.2075840998440981, Final Batch Loss: 0.07487575709819794\n",
      "Epoch 2577, Loss: 0.2295642662793398, Final Batch Loss: 0.09405770152807236\n",
      "Epoch 2578, Loss: 0.2054559588432312, Final Batch Loss: 0.07240252196788788\n",
      "Epoch 2579, Loss: 0.19409427791833878, Final Batch Loss: 0.03832550346851349\n",
      "Epoch 2580, Loss: 0.17957167327404022, Final Batch Loss: 0.024060025811195374\n",
      "Epoch 2581, Loss: 0.2047080099582672, Final Batch Loss: 0.06595215946435928\n",
      "Epoch 2582, Loss: 0.18715820647776127, Final Batch Loss: 0.051065314561128616\n",
      "Epoch 2583, Loss: 0.21780549362301826, Final Batch Loss: 0.0690712109208107\n",
      "Epoch 2584, Loss: 0.1896572019904852, Final Batch Loss: 0.0827043354511261\n",
      "Epoch 2585, Loss: 0.14855679869651794, Final Batch Loss: 0.020200956612825394\n",
      "Epoch 2586, Loss: 0.20696596428751945, Final Batch Loss: 0.02047942951321602\n",
      "Epoch 2587, Loss: 0.26811162754893303, Final Batch Loss: 0.06694997847080231\n",
      "Epoch 2588, Loss: 0.16184970177710056, Final Batch Loss: 0.01629619114100933\n",
      "Epoch 2589, Loss: 0.1800406314432621, Final Batch Loss: 0.013901494443416595\n",
      "Epoch 2590, Loss: 0.17284845933318138, Final Batch Loss: 0.04125700518488884\n",
      "Epoch 2591, Loss: 0.1607912164181471, Final Batch Loss: 0.04342516511678696\n",
      "Epoch 2592, Loss: 0.197897557169199, Final Batch Loss: 0.09453076869249344\n",
      "Epoch 2593, Loss: 0.18311721831560135, Final Batch Loss: 0.028412330895662308\n",
      "Epoch 2594, Loss: 0.25490808859467506, Final Batch Loss: 0.09561343491077423\n",
      "Epoch 2595, Loss: 0.25805073976516724, Final Batch Loss: 0.08155129104852676\n",
      "Epoch 2596, Loss: 0.27025792747735977, Final Batch Loss: 0.07840272039175034\n",
      "Epoch 2597, Loss: 0.19383585080504417, Final Batch Loss: 0.06968304514884949\n",
      "Epoch 2598, Loss: 0.27805307880043983, Final Batch Loss: 0.12020188570022583\n",
      "Epoch 2599, Loss: 0.22515716776251793, Final Batch Loss: 0.04864926263689995\n",
      "Epoch 2600, Loss: 0.28652939200401306, Final Batch Loss: 0.12077613174915314\n",
      "Epoch 2601, Loss: 0.26285895332694054, Final Batch Loss: 0.07206408679485321\n",
      "Epoch 2602, Loss: 0.21520615741610527, Final Batch Loss: 0.04915013536810875\n",
      "Epoch 2603, Loss: 0.33279887214303017, Final Batch Loss: 0.1284153014421463\n",
      "Epoch 2604, Loss: 0.28887268528342247, Final Batch Loss: 0.12752665579319\n",
      "Epoch 2605, Loss: 0.2478652074933052, Final Batch Loss: 0.049893852323293686\n",
      "Epoch 2606, Loss: 0.2869298793375492, Final Batch Loss: 0.0980808436870575\n",
      "Epoch 2607, Loss: 0.2271590642631054, Final Batch Loss: 0.029379844665527344\n",
      "Epoch 2608, Loss: 0.27913638576865196, Final Batch Loss: 0.06552591919898987\n",
      "Epoch 2609, Loss: 0.3053118772804737, Final Batch Loss: 0.11098356544971466\n",
      "Epoch 2610, Loss: 0.2406749725341797, Final Batch Loss: 0.04445542395114899\n",
      "Epoch 2611, Loss: 0.338173508644104, Final Batch Loss: 0.13479255139827728\n",
      "Epoch 2612, Loss: 0.20466902293264866, Final Batch Loss: 0.02974165417253971\n",
      "Epoch 2613, Loss: 0.22267146036028862, Final Batch Loss: 0.038952071219682693\n",
      "Epoch 2614, Loss: 0.2869591899216175, Final Batch Loss: 0.08551421761512756\n",
      "Epoch 2615, Loss: 0.2655089944601059, Final Batch Loss: 0.11381067335605621\n",
      "Epoch 2616, Loss: 0.2840985506772995, Final Batch Loss: 0.035720378160476685\n",
      "Epoch 2617, Loss: 0.29900916665792465, Final Batch Loss: 0.10003542900085449\n",
      "Epoch 2618, Loss: 0.19706201180815697, Final Batch Loss: 0.040551699697971344\n",
      "Epoch 2619, Loss: 0.20248164981603622, Final Batch Loss: 0.0575387142598629\n",
      "Epoch 2620, Loss: 0.25886228308081627, Final Batch Loss: 0.019771356135606766\n",
      "Epoch 2621, Loss: 0.17229192703962326, Final Batch Loss: 0.02683388441801071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2622, Loss: 0.21860099211335182, Final Batch Loss: 0.09445322304964066\n",
      "Epoch 2623, Loss: 0.28367720544338226, Final Batch Loss: 0.07461001724004745\n",
      "Epoch 2624, Loss: 0.26495586708188057, Final Batch Loss: 0.04999776929616928\n",
      "Epoch 2625, Loss: 0.16981889866292477, Final Batch Loss: 0.025049524381756783\n",
      "Epoch 2626, Loss: 0.27039655297994614, Final Batch Loss: 0.06961823254823685\n",
      "Epoch 2627, Loss: 0.16147399507462978, Final Batch Loss: 0.027388377115130424\n",
      "Epoch 2628, Loss: 0.2895534858107567, Final Batch Loss: 0.08669154345989227\n",
      "Epoch 2629, Loss: 0.1892183981835842, Final Batch Loss: 0.03455408662557602\n",
      "Epoch 2630, Loss: 0.2934257388114929, Final Batch Loss: 0.1041795164346695\n",
      "Epoch 2631, Loss: 0.19344788417220116, Final Batch Loss: 0.0628112331032753\n",
      "Epoch 2632, Loss: 0.1899283193051815, Final Batch Loss: 0.032406821846961975\n",
      "Epoch 2633, Loss: 0.2001582495868206, Final Batch Loss: 0.04153457656502724\n",
      "Epoch 2634, Loss: 0.2031162530183792, Final Batch Loss: 0.03657308220863342\n",
      "Epoch 2635, Loss: 0.18404978141188622, Final Batch Loss: 0.047516077756881714\n",
      "Epoch 2636, Loss: 0.25244441255927086, Final Batch Loss: 0.08333881944417953\n",
      "Epoch 2637, Loss: 0.3054502047598362, Final Batch Loss: 0.10950703918933868\n",
      "Epoch 2638, Loss: 0.20203982666134834, Final Batch Loss: 0.04467922821640968\n",
      "Epoch 2639, Loss: 0.2648232989013195, Final Batch Loss: 0.061978697776794434\n",
      "Epoch 2640, Loss: 0.20688991993665695, Final Batch Loss: 0.03650101274251938\n",
      "Epoch 2641, Loss: 0.2649631127715111, Final Batch Loss: 0.06295334547758102\n",
      "Epoch 2642, Loss: 0.1956282276660204, Final Batch Loss: 0.038793858140707016\n",
      "Epoch 2643, Loss: 0.19972240179777145, Final Batch Loss: 0.0634709894657135\n",
      "Epoch 2644, Loss: 0.22901026159524918, Final Batch Loss: 0.03852415084838867\n",
      "Epoch 2645, Loss: 0.2068997472524643, Final Batch Loss: 0.047831255942583084\n",
      "Epoch 2646, Loss: 0.190325440838933, Final Batch Loss: 0.052649546414613724\n",
      "Epoch 2647, Loss: 0.18326938711106777, Final Batch Loss: 0.026186112314462662\n",
      "Epoch 2648, Loss: 0.17917301133275032, Final Batch Loss: 0.02976280450820923\n",
      "Epoch 2649, Loss: 0.1650216467678547, Final Batch Loss: 0.03887470066547394\n",
      "Epoch 2650, Loss: 0.2027980536222458, Final Batch Loss: 0.0772227942943573\n",
      "Epoch 2651, Loss: 0.2700537219643593, Final Batch Loss: 0.06393889337778091\n",
      "Epoch 2652, Loss: 0.20889941789209843, Final Batch Loss: 0.021067483350634575\n",
      "Epoch 2653, Loss: 0.22555513307452202, Final Batch Loss: 0.084819495677948\n",
      "Epoch 2654, Loss: 0.17281894944608212, Final Batch Loss: 0.02579878456890583\n",
      "Epoch 2655, Loss: 0.3140871375799179, Final Batch Loss: 0.1128772497177124\n",
      "Epoch 2656, Loss: 0.35896704345941544, Final Batch Loss: 0.09890399128198624\n",
      "Epoch 2657, Loss: 0.2612394094467163, Final Batch Loss: 0.04837512597441673\n",
      "Epoch 2658, Loss: 0.21544326841831207, Final Batch Loss: 0.04197089374065399\n",
      "Epoch 2659, Loss: 0.2255551740527153, Final Batch Loss: 0.08201152831315994\n",
      "Epoch 2660, Loss: 0.3038330301642418, Final Batch Loss: 0.0883159413933754\n",
      "Epoch 2661, Loss: 0.23841429129242897, Final Batch Loss: 0.07253750413656235\n",
      "Epoch 2662, Loss: 0.24633842520415783, Final Batch Loss: 0.026802202686667442\n",
      "Epoch 2663, Loss: 0.25162217393517494, Final Batch Loss: 0.09206362813711166\n",
      "Epoch 2664, Loss: 0.3213801681995392, Final Batch Loss: 0.06427264213562012\n",
      "Epoch 2665, Loss: 0.36010999232530594, Final Batch Loss: 0.10947863757610321\n",
      "Epoch 2666, Loss: 0.2148387124761939, Final Batch Loss: 0.01442811544984579\n",
      "Epoch 2667, Loss: 0.23263659700751305, Final Batch Loss: 0.03521668165922165\n",
      "Epoch 2668, Loss: 0.22737836465239525, Final Batch Loss: 0.05851324275135994\n",
      "Epoch 2669, Loss: 0.22315343096852303, Final Batch Loss: 0.06144125387072563\n",
      "Epoch 2670, Loss: 0.20515993237495422, Final Batch Loss: 0.03580084070563316\n",
      "Epoch 2671, Loss: 0.21552342176437378, Final Batch Loss: 0.04291648045182228\n",
      "Epoch 2672, Loss: 0.28175102919340134, Final Batch Loss: 0.03452733904123306\n",
      "Epoch 2673, Loss: 0.2531297169625759, Final Batch Loss: 0.08977944403886795\n",
      "Epoch 2674, Loss: 0.2768295779824257, Final Batch Loss: 0.07783360779285431\n",
      "Epoch 2675, Loss: 0.2173093929886818, Final Batch Loss: 0.03294045105576515\n",
      "Epoch 2676, Loss: 0.23270912654697895, Final Batch Loss: 0.02923819236457348\n",
      "Epoch 2677, Loss: 0.3035995066165924, Final Batch Loss: 0.11585436761379242\n",
      "Epoch 2678, Loss: 0.20605755783617496, Final Batch Loss: 0.027270624414086342\n",
      "Epoch 2679, Loss: 0.27488307282328606, Final Batch Loss: 0.07647084444761276\n",
      "Epoch 2680, Loss: 0.22204596176743507, Final Batch Loss: 0.05056450515985489\n",
      "Epoch 2681, Loss: 0.19320156797766685, Final Batch Loss: 0.05321621149778366\n",
      "Epoch 2682, Loss: 0.17789870873093605, Final Batch Loss: 0.06512980163097382\n",
      "Epoch 2683, Loss: 0.15759682655334473, Final Batch Loss: 0.03735613450407982\n",
      "Epoch 2684, Loss: 0.1907926108688116, Final Batch Loss: 0.02944096364080906\n",
      "Epoch 2685, Loss: 0.17040102556347847, Final Batch Loss: 0.026539703831076622\n",
      "Epoch 2686, Loss: 0.22256477922201157, Final Batch Loss: 0.038911715149879456\n",
      "Epoch 2687, Loss: 0.1443511638790369, Final Batch Loss: 0.024547995999455452\n",
      "Epoch 2688, Loss: 0.2075368259102106, Final Batch Loss: 0.05328764766454697\n",
      "Epoch 2689, Loss: 0.1597975455224514, Final Batch Loss: 0.024412155151367188\n",
      "Epoch 2690, Loss: 0.18852902576327324, Final Batch Loss: 0.0455525778234005\n",
      "Epoch 2691, Loss: 0.18518462032079697, Final Batch Loss: 0.05784013122320175\n",
      "Epoch 2692, Loss: 0.20370538905262947, Final Batch Loss: 0.03296017274260521\n",
      "Epoch 2693, Loss: 0.17630888521671295, Final Batch Loss: 0.056956805288791656\n",
      "Epoch 2694, Loss: 0.14377734437584877, Final Batch Loss: 0.017230648547410965\n",
      "Epoch 2695, Loss: 0.21615278348326683, Final Batch Loss: 0.05891387164592743\n",
      "Epoch 2696, Loss: 0.20625989884138107, Final Batch Loss: 0.054774168878793716\n",
      "Epoch 2697, Loss: 0.23645976185798645, Final Batch Loss: 0.08418836444616318\n",
      "Epoch 2698, Loss: 0.15696857031434774, Final Batch Loss: 0.00686833169311285\n",
      "Epoch 2699, Loss: 0.21800311654806137, Final Batch Loss: 0.07192853093147278\n",
      "Epoch 2700, Loss: 0.24152979254722595, Final Batch Loss: 0.10072532296180725\n",
      "Epoch 2701, Loss: 0.20204069465398788, Final Batch Loss: 0.045899052172899246\n",
      "Epoch 2702, Loss: 0.17846368439495564, Final Batch Loss: 0.030124807730317116\n",
      "Epoch 2703, Loss: 0.30832666903734207, Final Batch Loss: 0.06781179457902908\n",
      "Epoch 2704, Loss: 0.3612983599305153, Final Batch Loss: 0.1515468806028366\n",
      "Epoch 2705, Loss: 0.3074546381831169, Final Batch Loss: 0.16376246511936188\n",
      "Epoch 2706, Loss: 0.2899573780596256, Final Batch Loss: 0.12841975688934326\n",
      "Epoch 2707, Loss: 0.17287889495491982, Final Batch Loss: 0.038340017199516296\n",
      "Epoch 2708, Loss: 0.194146990776062, Final Batch Loss: 0.03184301033616066\n",
      "Epoch 2709, Loss: 0.30096010863780975, Final Batch Loss: 0.09010063111782074\n",
      "Epoch 2710, Loss: 0.25757860764861107, Final Batch Loss: 0.07573148608207703\n",
      "Epoch 2711, Loss: 0.1724284626543522, Final Batch Loss: 0.04267130792140961\n",
      "Epoch 2712, Loss: 0.20282038487493992, Final Batch Loss: 0.08253635466098785\n",
      "Epoch 2713, Loss: 0.20149295590817928, Final Batch Loss: 0.025630192831158638\n",
      "Epoch 2714, Loss: 0.2080147285014391, Final Batch Loss: 0.05524769425392151\n",
      "Epoch 2715, Loss: 0.32077954337000847, Final Batch Loss: 0.1227174773812294\n",
      "Epoch 2716, Loss: 0.16746000945568085, Final Batch Loss: 0.04079509153962135\n",
      "Epoch 2717, Loss: 0.2065856121480465, Final Batch Loss: 0.05870172008872032\n",
      "Epoch 2718, Loss: 0.18735873699188232, Final Batch Loss: 0.033833764493465424\n",
      "Epoch 2719, Loss: 0.23659431003034115, Final Batch Loss: 0.024011773988604546\n",
      "Epoch 2720, Loss: 0.19319358840584755, Final Batch Loss: 0.0355098657310009\n",
      "Epoch 2721, Loss: 0.20912234857678413, Final Batch Loss: 0.047366801649332047\n",
      "Epoch 2722, Loss: 0.26093507185578346, Final Batch Loss: 0.09347590059041977\n",
      "Epoch 2723, Loss: 0.22342776879668236, Final Batch Loss: 0.08216456323862076\n",
      "Epoch 2724, Loss: 0.22907723858952522, Final Batch Loss: 0.08828063309192657\n",
      "Epoch 2725, Loss: 0.24315943755209446, Final Batch Loss: 0.09574399888515472\n",
      "Epoch 2726, Loss: 0.20846930891275406, Final Batch Loss: 0.05748819559812546\n",
      "Epoch 2727, Loss: 0.2238805703818798, Final Batch Loss: 0.0684645026922226\n",
      "Epoch 2728, Loss: 0.23232287727296352, Final Batch Loss: 0.014923518523573875\n",
      "Epoch 2729, Loss: 0.23353642597794533, Final Batch Loss: 0.07162017375230789\n",
      "Epoch 2730, Loss: 0.1797310784459114, Final Batch Loss: 0.029807206243276596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2731, Loss: 0.14689489174634218, Final Batch Loss: 0.013038848526775837\n",
      "Epoch 2732, Loss: 0.18284355383366346, Final Batch Loss: 0.009368469007313251\n",
      "Epoch 2733, Loss: 0.18582932464778423, Final Batch Loss: 0.023943109437823296\n",
      "Epoch 2734, Loss: 0.2575337141752243, Final Batch Loss: 0.04980548843741417\n",
      "Epoch 2735, Loss: 0.24066984467208385, Final Batch Loss: 0.01909853145480156\n",
      "Epoch 2736, Loss: 0.23666420578956604, Final Batch Loss: 0.036197222769260406\n",
      "Epoch 2737, Loss: 0.21648229286074638, Final Batch Loss: 0.0762903168797493\n",
      "Epoch 2738, Loss: 0.2841305583715439, Final Batch Loss: 0.0839017853140831\n",
      "Epoch 2739, Loss: 0.39897800981998444, Final Batch Loss: 0.08336996287107468\n",
      "Epoch 2740, Loss: 0.2501162849366665, Final Batch Loss: 0.031689923256635666\n",
      "Epoch 2741, Loss: 0.281288206577301, Final Batch Loss: 0.0923198014497757\n",
      "Epoch 2742, Loss: 0.24051712080836296, Final Batch Loss: 0.04378747195005417\n",
      "Epoch 2743, Loss: 0.17831379547715187, Final Batch Loss: 0.04483388364315033\n",
      "Epoch 2744, Loss: 0.21940525993704796, Final Batch Loss: 0.062443409115076065\n",
      "Epoch 2745, Loss: 0.2272550594061613, Final Batch Loss: 0.027279136702418327\n",
      "Epoch 2746, Loss: 0.2320939265191555, Final Batch Loss: 0.04435800760984421\n",
      "Epoch 2747, Loss: 0.25320738181471825, Final Batch Loss: 0.05695285275578499\n",
      "Epoch 2748, Loss: 0.2708640433847904, Final Batch Loss: 0.03984454646706581\n",
      "Epoch 2749, Loss: 0.2175171934068203, Final Batch Loss: 0.028102166950702667\n",
      "Epoch 2750, Loss: 0.1852411814033985, Final Batch Loss: 0.048310499638319016\n",
      "Epoch 2751, Loss: 0.267727330327034, Final Batch Loss: 0.10540216416120529\n",
      "Epoch 2752, Loss: 0.18896520882844925, Final Batch Loss: 0.03792022168636322\n",
      "Epoch 2753, Loss: 0.2430407851934433, Final Batch Loss: 0.039686474949121475\n",
      "Epoch 2754, Loss: 0.2510747332125902, Final Batch Loss: 0.02887232042849064\n",
      "Epoch 2755, Loss: 0.26413821429014206, Final Batch Loss: 0.0788627490401268\n",
      "Epoch 2756, Loss: 0.26108400523662567, Final Batch Loss: 0.06168775632977486\n",
      "Epoch 2757, Loss: 0.2429296337068081, Final Batch Loss: 0.04145205393433571\n",
      "Epoch 2758, Loss: 0.15217557176947594, Final Batch Loss: 0.019356505945324898\n",
      "Epoch 2759, Loss: 0.30842597782611847, Final Batch Loss: 0.08834638446569443\n",
      "Epoch 2760, Loss: 0.21906564757227898, Final Batch Loss: 0.08957528322935104\n",
      "Epoch 2761, Loss: 0.19410120882093906, Final Batch Loss: 0.021119875833392143\n",
      "Epoch 2762, Loss: 0.26834026724100113, Final Batch Loss: 0.08338169753551483\n",
      "Epoch 2763, Loss: 0.1484105233103037, Final Batch Loss: 0.016645684838294983\n",
      "Epoch 2764, Loss: 0.28822868689894676, Final Batch Loss: 0.04956727474927902\n",
      "Epoch 2765, Loss: 0.23002837225794792, Final Batch Loss: 0.023052126169204712\n",
      "Epoch 2766, Loss: 0.2200302118435502, Final Batch Loss: 0.015326154418289661\n",
      "Epoch 2767, Loss: 0.21259969845414162, Final Batch Loss: 0.04038842022418976\n",
      "Epoch 2768, Loss: 0.23108982108533382, Final Batch Loss: 0.07243441790342331\n",
      "Epoch 2769, Loss: 0.21075579337775707, Final Batch Loss: 0.05757322907447815\n",
      "Epoch 2770, Loss: 0.1916757058352232, Final Batch Loss: 0.04910208657383919\n",
      "Epoch 2771, Loss: 0.2400955632328987, Final Batch Loss: 0.048285163938999176\n",
      "Epoch 2772, Loss: 0.1636693738400936, Final Batch Loss: 0.023594489321112633\n",
      "Epoch 2773, Loss: 0.1657840497791767, Final Batch Loss: 0.039431557059288025\n",
      "Epoch 2774, Loss: 0.21645615249872208, Final Batch Loss: 0.020087704062461853\n",
      "Epoch 2775, Loss: 0.25901904329657555, Final Batch Loss: 0.05068781599402428\n",
      "Epoch 2776, Loss: 0.2316160462796688, Final Batch Loss: 0.0742257609963417\n",
      "Epoch 2777, Loss: 0.2047886624932289, Final Batch Loss: 0.04773849621415138\n",
      "Epoch 2778, Loss: 0.21475526690483093, Final Batch Loss: 0.02022937312722206\n",
      "Epoch 2779, Loss: 0.20055778697133064, Final Batch Loss: 0.09445536136627197\n",
      "Epoch 2780, Loss: 0.13388902507722378, Final Batch Loss: 0.023822680115699768\n",
      "Epoch 2781, Loss: 0.17686115764081478, Final Batch Loss: 0.059639617800712585\n",
      "Epoch 2782, Loss: 0.2959650531411171, Final Batch Loss: 0.15462608635425568\n",
      "Epoch 2783, Loss: 0.20616316050291061, Final Batch Loss: 0.0339231975376606\n",
      "Epoch 2784, Loss: 0.21428942680358887, Final Batch Loss: 0.03943903371691704\n",
      "Epoch 2785, Loss: 0.23555858805775642, Final Batch Loss: 0.04412029683589935\n",
      "Epoch 2786, Loss: 0.2165626399219036, Final Batch Loss: 0.07039051502943039\n",
      "Epoch 2787, Loss: 0.32872141152620316, Final Batch Loss: 0.0319993682205677\n",
      "Epoch 2788, Loss: 0.28427285701036453, Final Batch Loss: 0.03795734792947769\n",
      "Epoch 2789, Loss: 0.23844699189066887, Final Batch Loss: 0.05542083829641342\n",
      "Epoch 2790, Loss: 0.27829816564917564, Final Batch Loss: 0.09567458927631378\n",
      "Epoch 2791, Loss: 0.17091112211346626, Final Batch Loss: 0.017284736037254333\n",
      "Epoch 2792, Loss: 0.21044831350445747, Final Batch Loss: 0.05016616731882095\n",
      "Epoch 2793, Loss: 0.33726268634200096, Final Batch Loss: 0.16394786536693573\n",
      "Epoch 2794, Loss: 0.2210683375597, Final Batch Loss: 0.07068762183189392\n",
      "Epoch 2795, Loss: 0.18778925761580467, Final Batch Loss: 0.04299287870526314\n",
      "Epoch 2796, Loss: 0.23281089216470718, Final Batch Loss: 0.04067397862672806\n",
      "Epoch 2797, Loss: 0.249628197401762, Final Batch Loss: 0.13024817407131195\n",
      "Epoch 2798, Loss: 0.2077905796468258, Final Batch Loss: 0.0337597131729126\n",
      "Epoch 2799, Loss: 0.24023180082440376, Final Batch Loss: 0.052706487476825714\n",
      "Epoch 2800, Loss: 0.21333221718668938, Final Batch Loss: 0.07227007299661636\n",
      "Epoch 2801, Loss: 0.23644012585282326, Final Batch Loss: 0.08553313463926315\n",
      "Epoch 2802, Loss: 0.26362110674381256, Final Batch Loss: 0.028265628963708878\n",
      "Epoch 2803, Loss: 0.22698754258453846, Final Batch Loss: 0.11116383969783783\n",
      "Epoch 2804, Loss: 0.17136585153639317, Final Batch Loss: 0.018010659143328667\n",
      "Epoch 2805, Loss: 0.19425531290471554, Final Batch Loss: 0.04198400676250458\n",
      "Epoch 2806, Loss: 0.2617061622440815, Final Batch Loss: 0.10508818924427032\n",
      "Epoch 2807, Loss: 0.3099425584077835, Final Batch Loss: 0.04714714363217354\n",
      "Epoch 2808, Loss: 0.22607942670583725, Final Batch Loss: 0.027642179280519485\n",
      "Epoch 2809, Loss: 0.22250422462821007, Final Batch Loss: 0.10241436958312988\n",
      "Epoch 2810, Loss: 0.19496867433190346, Final Batch Loss: 0.05324721708893776\n",
      "Epoch 2811, Loss: 0.18914809450507164, Final Batch Loss: 0.03774651139974594\n",
      "Epoch 2812, Loss: 0.2829432636499405, Final Batch Loss: 0.10684026777744293\n",
      "Epoch 2813, Loss: 0.2455091942101717, Final Batch Loss: 0.027382785454392433\n",
      "Epoch 2814, Loss: 0.2830372732132673, Final Batch Loss: 0.11453265696763992\n",
      "Epoch 2815, Loss: 0.20751188695430756, Final Batch Loss: 0.041104380041360855\n",
      "Epoch 2816, Loss: 0.2192782536149025, Final Batch Loss: 0.0384988933801651\n",
      "Epoch 2817, Loss: 0.16284889727830887, Final Batch Loss: 0.02822452038526535\n",
      "Epoch 2818, Loss: 0.20894385129213333, Final Batch Loss: 0.03717675432562828\n",
      "Epoch 2819, Loss: 0.22874242439866066, Final Batch Loss: 0.06457801163196564\n",
      "Epoch 2820, Loss: 0.18353094905614853, Final Batch Loss: 0.022686637938022614\n",
      "Epoch 2821, Loss: 0.22293593361973763, Final Batch Loss: 0.03529495745897293\n",
      "Epoch 2822, Loss: 0.15829866006970406, Final Batch Loss: 0.04231496527791023\n",
      "Epoch 2823, Loss: 0.2663562633097172, Final Batch Loss: 0.11656863987445831\n",
      "Epoch 2824, Loss: 0.25594666600227356, Final Batch Loss: 0.06229708716273308\n",
      "Epoch 2825, Loss: 0.2522854767739773, Final Batch Loss: 0.12085152417421341\n",
      "Epoch 2826, Loss: 0.2444295957684517, Final Batch Loss: 0.08242827653884888\n",
      "Epoch 2827, Loss: 0.24465343728661537, Final Batch Loss: 0.05984900891780853\n",
      "Epoch 2828, Loss: 0.1909317597746849, Final Batch Loss: 0.03020850196480751\n",
      "Epoch 2829, Loss: 0.2450322024524212, Final Batch Loss: 0.05589886009693146\n",
      "Epoch 2830, Loss: 0.15261896140873432, Final Batch Loss: 0.019016912207007408\n",
      "Epoch 2831, Loss: 0.19714686274528503, Final Batch Loss: 0.02277953550219536\n",
      "Epoch 2832, Loss: 0.1922132410109043, Final Batch Loss: 0.06694614142179489\n",
      "Epoch 2833, Loss: 0.24106740206480026, Final Batch Loss: 0.04634669050574303\n",
      "Epoch 2834, Loss: 0.2295108549296856, Final Batch Loss: 0.06339439004659653\n",
      "Epoch 2835, Loss: 0.17162644490599632, Final Batch Loss: 0.03770839795470238\n",
      "Epoch 2836, Loss: 0.19386255368590355, Final Batch Loss: 0.05324874445796013\n",
      "Epoch 2837, Loss: 0.20095385052263737, Final Batch Loss: 0.02266749180853367\n",
      "Epoch 2838, Loss: 0.22028175555169582, Final Batch Loss: 0.01941213198006153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2839, Loss: 0.23284000158309937, Final Batch Loss: 0.10307560116052628\n",
      "Epoch 2840, Loss: 0.18817413039505482, Final Batch Loss: 0.021515419706702232\n",
      "Epoch 2841, Loss: 0.1629638895392418, Final Batch Loss: 0.02199842408299446\n",
      "Epoch 2842, Loss: 0.1867083627730608, Final Batch Loss: 0.028697026893496513\n",
      "Epoch 2843, Loss: 0.19032283127307892, Final Batch Loss: 0.07129649817943573\n",
      "Epoch 2844, Loss: 0.2975226603448391, Final Batch Loss: 0.09984487295150757\n",
      "Epoch 2845, Loss: 0.2699658200144768, Final Batch Loss: 0.0737878605723381\n",
      "Epoch 2846, Loss: 0.21621167846024036, Final Batch Loss: 0.02630792371928692\n",
      "Epoch 2847, Loss: 0.1940501667559147, Final Batch Loss: 0.06162746623158455\n",
      "Epoch 2848, Loss: 0.2341819554567337, Final Batch Loss: 0.08782821893692017\n",
      "Epoch 2849, Loss: 0.16176273487508297, Final Batch Loss: 0.06400083750486374\n",
      "Epoch 2850, Loss: 0.20647402852773666, Final Batch Loss: 0.027192767709493637\n",
      "Epoch 2851, Loss: 0.17577003687620163, Final Batch Loss: 0.04086428880691528\n",
      "Epoch 2852, Loss: 0.3245340660214424, Final Batch Loss: 0.12282203137874603\n",
      "Epoch 2853, Loss: 0.21721899881958961, Final Batch Loss: 0.032968927174806595\n",
      "Epoch 2854, Loss: 0.19272019900381565, Final Batch Loss: 0.05870932713150978\n",
      "Epoch 2855, Loss: 0.17601574212312698, Final Batch Loss: 0.03448684141039848\n",
      "Epoch 2856, Loss: 0.2928091622889042, Final Batch Loss: 0.07664557546377182\n",
      "Epoch 2857, Loss: 0.2807769402861595, Final Batch Loss: 0.12098203599452972\n",
      "Epoch 2858, Loss: 0.21995601430535316, Final Batch Loss: 0.047923266887664795\n",
      "Epoch 2859, Loss: 0.3023134730756283, Final Batch Loss: 0.039580103009939194\n",
      "Epoch 2860, Loss: 0.19089852273464203, Final Batch Loss: 0.048835355788469315\n",
      "Epoch 2861, Loss: 0.20689131692051888, Final Batch Loss: 0.04366163909435272\n",
      "Epoch 2862, Loss: 0.25465672463178635, Final Batch Loss: 0.0927848219871521\n",
      "Epoch 2863, Loss: 0.24115986749529839, Final Batch Loss: 0.03415759652853012\n",
      "Epoch 2864, Loss: 0.24809370189905167, Final Batch Loss: 0.04757381230592728\n",
      "Epoch 2865, Loss: 0.21970142237842083, Final Batch Loss: 0.022542549297213554\n",
      "Epoch 2866, Loss: 0.1384446956217289, Final Batch Loss: 0.024086160585284233\n",
      "Epoch 2867, Loss: 0.20252658799290657, Final Batch Loss: 0.05384571850299835\n",
      "Epoch 2868, Loss: 0.12731016986072063, Final Batch Loss: 0.042059946805238724\n",
      "Epoch 2869, Loss: 0.2046216744929552, Final Batch Loss: 0.0617232583463192\n",
      "Epoch 2870, Loss: 0.1501847729086876, Final Batch Loss: 0.022746287286281586\n",
      "Epoch 2871, Loss: 0.20290265418589115, Final Batch Loss: 0.011741133406758308\n",
      "Epoch 2872, Loss: 0.20723122917115688, Final Batch Loss: 0.04199790954589844\n",
      "Epoch 2873, Loss: 0.2179578561335802, Final Batch Loss: 0.06641139090061188\n",
      "Epoch 2874, Loss: 0.24063784070312977, Final Batch Loss: 0.02461131103336811\n",
      "Epoch 2875, Loss: 0.21314891800284386, Final Batch Loss: 0.021880213171243668\n",
      "Epoch 2876, Loss: 0.23153597861528397, Final Batch Loss: 0.02362184226512909\n",
      "Epoch 2877, Loss: 0.13262903783470392, Final Batch Loss: 0.014636085368692875\n",
      "Epoch 2878, Loss: 0.27142006903886795, Final Batch Loss: 0.09052271395921707\n",
      "Epoch 2879, Loss: 0.18229315429925919, Final Batch Loss: 0.017970304936170578\n",
      "Epoch 2880, Loss: 0.19231633469462395, Final Batch Loss: 0.0639403834939003\n",
      "Epoch 2881, Loss: 0.21869738027453423, Final Batch Loss: 0.043189264833927155\n",
      "Epoch 2882, Loss: 0.14785651490092278, Final Batch Loss: 0.04646996781229973\n",
      "Epoch 2883, Loss: 0.17391939461231232, Final Batch Loss: 0.03701760619878769\n",
      "Epoch 2884, Loss: 0.23507994040846825, Final Batch Loss: 0.11690954864025116\n",
      "Epoch 2885, Loss: 0.1450264398008585, Final Batch Loss: 0.020367294549942017\n",
      "Epoch 2886, Loss: 0.19027471914887428, Final Batch Loss: 0.04453812167048454\n",
      "Epoch 2887, Loss: 0.15801244601607323, Final Batch Loss: 0.022619470953941345\n",
      "Epoch 2888, Loss: 0.17991433292627335, Final Batch Loss: 0.050909221172332764\n",
      "Epoch 2889, Loss: 0.30811912938952446, Final Batch Loss: 0.017762742936611176\n",
      "Epoch 2890, Loss: 0.21090616285800934, Final Batch Loss: 0.03593658283352852\n",
      "Epoch 2891, Loss: 0.28351249173283577, Final Batch Loss: 0.07310646772384644\n",
      "Epoch 2892, Loss: 0.2710735946893692, Final Batch Loss: 0.07395460456609726\n",
      "Epoch 2893, Loss: 0.2904531955718994, Final Batch Loss: 0.11020379513502121\n",
      "Epoch 2894, Loss: 0.25172998756170273, Final Batch Loss: 0.06448023021221161\n",
      "Epoch 2895, Loss: 0.14490429870784283, Final Batch Loss: 0.02440446801483631\n",
      "Epoch 2896, Loss: 0.22713883966207504, Final Batch Loss: 0.03444838151335716\n",
      "Epoch 2897, Loss: 0.2655056491494179, Final Batch Loss: 0.098565973341465\n",
      "Epoch 2898, Loss: 0.20576902478933334, Final Batch Loss: 0.036195073276758194\n",
      "Epoch 2899, Loss: 0.23298409581184387, Final Batch Loss: 0.13334651291370392\n",
      "Epoch 2900, Loss: 0.2663060687482357, Final Batch Loss: 0.04641609266400337\n",
      "Epoch 2901, Loss: 0.20012440904974937, Final Batch Loss: 0.04093398153781891\n",
      "Epoch 2902, Loss: 0.21148407086730003, Final Batch Loss: 0.08485615253448486\n",
      "Epoch 2903, Loss: 0.13265862315893173, Final Batch Loss: 0.027681371197104454\n",
      "Epoch 2904, Loss: 0.21081366203725338, Final Batch Loss: 0.020983295515179634\n",
      "Epoch 2905, Loss: 0.1692943014204502, Final Batch Loss: 0.05457303673028946\n",
      "Epoch 2906, Loss: 0.18968777917325497, Final Batch Loss: 0.026775145903229713\n",
      "Epoch 2907, Loss: 0.21560193039476871, Final Batch Loss: 0.023928487673401833\n",
      "Epoch 2908, Loss: 0.21952953934669495, Final Batch Loss: 0.03675062581896782\n",
      "Epoch 2909, Loss: 0.30479503981769085, Final Batch Loss: 0.04512215033173561\n",
      "Epoch 2910, Loss: 0.21300604194402695, Final Batch Loss: 0.05786857008934021\n",
      "Epoch 2911, Loss: 0.2341406475752592, Final Batch Loss: 0.07424566894769669\n",
      "Epoch 2912, Loss: 0.17566321231424809, Final Batch Loss: 0.02017739973962307\n",
      "Epoch 2913, Loss: 0.15984405018389225, Final Batch Loss: 0.03006777912378311\n",
      "Epoch 2914, Loss: 0.32531068474054337, Final Batch Loss: 0.10534438490867615\n",
      "Epoch 2915, Loss: 0.22760161012411118, Final Batch Loss: 0.023055467754602432\n",
      "Epoch 2916, Loss: 0.20256281085312366, Final Batch Loss: 0.05736824870109558\n",
      "Epoch 2917, Loss: 0.2039873469620943, Final Batch Loss: 0.02041747234761715\n",
      "Epoch 2918, Loss: 0.18169289268553257, Final Batch Loss: 0.030246561393141747\n",
      "Epoch 2919, Loss: 0.1944791991263628, Final Batch Loss: 0.04780193045735359\n",
      "Epoch 2920, Loss: 0.2639099098742008, Final Batch Loss: 0.056602153927087784\n",
      "Epoch 2921, Loss: 0.24508406594395638, Final Batch Loss: 0.10101485252380371\n",
      "Epoch 2922, Loss: 0.2268933579325676, Final Batch Loss: 0.07354302704334259\n",
      "Epoch 2923, Loss: 0.2693163026124239, Final Batch Loss: 0.10665231198072433\n",
      "Epoch 2924, Loss: 0.25102071464061737, Final Batch Loss: 0.06824395805597305\n",
      "Epoch 2925, Loss: 0.6162511333823204, Final Batch Loss: 0.4393143355846405\n",
      "Epoch 2926, Loss: 0.24731513857841492, Final Batch Loss: 0.06666645407676697\n",
      "Epoch 2927, Loss: 0.2781250737607479, Final Batch Loss: 0.08334437757730484\n",
      "Epoch 2928, Loss: 0.2890314534306526, Final Batch Loss: 0.06771250069141388\n",
      "Epoch 2929, Loss: 0.24622167274355888, Final Batch Loss: 0.056980401277542114\n",
      "Epoch 2930, Loss: 0.20643487945199013, Final Batch Loss: 0.014134742319583893\n",
      "Epoch 2931, Loss: 0.20803197845816612, Final Batch Loss: 0.044072411954402924\n",
      "Epoch 2932, Loss: 0.27106277644634247, Final Batch Loss: 0.054843395948410034\n",
      "Epoch 2933, Loss: 0.22834420949220657, Final Batch Loss: 0.05424705892801285\n",
      "Epoch 2934, Loss: 0.2778927553445101, Final Batch Loss: 0.10641759634017944\n",
      "Epoch 2935, Loss: 0.3644489049911499, Final Batch Loss: 0.20029082894325256\n",
      "Epoch 2936, Loss: 0.3524699695408344, Final Batch Loss: 0.22032999992370605\n",
      "Epoch 2937, Loss: 0.24456162750720978, Final Batch Loss: 0.04320628196001053\n",
      "Epoch 2938, Loss: 0.215306106954813, Final Batch Loss: 0.044704776257276535\n",
      "Epoch 2939, Loss: 0.24112393707036972, Final Batch Loss: 0.05651334673166275\n",
      "Epoch 2940, Loss: 0.25464411079883575, Final Batch Loss: 0.10320701450109482\n",
      "Epoch 2941, Loss: 0.3528253622353077, Final Batch Loss: 0.22904524207115173\n",
      "Epoch 2942, Loss: 0.23374115116894245, Final Batch Loss: 0.06778954714536667\n",
      "Epoch 2943, Loss: 0.22943477425724268, Final Batch Loss: 0.010988437570631504\n",
      "Epoch 2944, Loss: 0.27179716527462006, Final Batch Loss: 0.08699889481067657\n",
      "Epoch 2945, Loss: 0.30531567335128784, Final Batch Loss: 0.1724672168493271\n",
      "Epoch 2946, Loss: 0.20220756344497204, Final Batch Loss: 0.060826994478702545\n",
      "Epoch 2947, Loss: 0.16611562110483646, Final Batch Loss: 0.027900952845811844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2948, Loss: 0.20050062239170074, Final Batch Loss: 0.057409998029470444\n",
      "Epoch 2949, Loss: 0.23413470946252346, Final Batch Loss: 0.10207204520702362\n",
      "Epoch 2950, Loss: 0.17099937982857227, Final Batch Loss: 0.02441822737455368\n",
      "Epoch 2951, Loss: 0.2977086007595062, Final Batch Loss: 0.05844644457101822\n",
      "Epoch 2952, Loss: 0.23061861097812653, Final Batch Loss: 0.0744781419634819\n",
      "Epoch 2953, Loss: 0.1774036157876253, Final Batch Loss: 0.014677727594971657\n",
      "Epoch 2954, Loss: 0.17731659673154354, Final Batch Loss: 0.048258084803819656\n",
      "Epoch 2955, Loss: 0.2561003342270851, Final Batch Loss: 0.047551229596138\n",
      "Epoch 2956, Loss: 0.17927495576441288, Final Batch Loss: 0.07258055359125137\n",
      "Epoch 2957, Loss: 0.1612808695062995, Final Batch Loss: 0.007178801111876965\n",
      "Epoch 2958, Loss: 0.1713130883872509, Final Batch Loss: 0.044732265174388885\n",
      "Epoch 2959, Loss: 0.22366321459412575, Final Batch Loss: 0.06351480633020401\n",
      "Epoch 2960, Loss: 0.15242970548570156, Final Batch Loss: 0.01723998598754406\n",
      "Epoch 2961, Loss: 0.11165995802730322, Final Batch Loss: 0.014825067482888699\n",
      "Epoch 2962, Loss: 0.1604539304971695, Final Batch Loss: 0.02347681298851967\n",
      "Epoch 2963, Loss: 0.2189711518585682, Final Batch Loss: 0.04703139141201973\n",
      "Epoch 2964, Loss: 0.19193825870752335, Final Batch Loss: 0.06831466406583786\n",
      "Epoch 2965, Loss: 0.2059008963406086, Final Batch Loss: 0.07260167598724365\n",
      "Epoch 2966, Loss: 0.21891393512487411, Final Batch Loss: 0.05730050057172775\n",
      "Epoch 2967, Loss: 0.1998494379222393, Final Batch Loss: 0.0459163598716259\n",
      "Epoch 2968, Loss: 0.2976880744099617, Final Batch Loss: 0.039464451372623444\n",
      "Epoch 2969, Loss: 0.22765633836388588, Final Batch Loss: 0.07111606746912003\n",
      "Epoch 2970, Loss: 0.25836290419101715, Final Batch Loss: 0.046892691403627396\n",
      "Epoch 2971, Loss: 0.22054817155003548, Final Batch Loss: 0.062219105660915375\n",
      "Epoch 2972, Loss: 0.23715394735336304, Final Batch Loss: 0.0436452180147171\n",
      "Epoch 2973, Loss: 0.20386126264929771, Final Batch Loss: 0.06580866128206253\n",
      "Epoch 2974, Loss: 0.2031238153576851, Final Batch Loss: 0.059657711535692215\n",
      "Epoch 2975, Loss: 0.1835572775453329, Final Batch Loss: 0.02142784558236599\n",
      "Epoch 2976, Loss: 0.17578503862023354, Final Batch Loss: 0.03624420240521431\n",
      "Epoch 2977, Loss: 0.19339407421648502, Final Batch Loss: 0.017601950094103813\n",
      "Epoch 2978, Loss: 0.23928100615739822, Final Batch Loss: 0.09459231793880463\n",
      "Epoch 2979, Loss: 0.19077733345329762, Final Batch Loss: 0.014600714668631554\n",
      "Epoch 2980, Loss: 0.2477262280881405, Final Batch Loss: 0.07481196522712708\n",
      "Epoch 2981, Loss: 0.3570527881383896, Final Batch Loss: 0.16127566993236542\n",
      "Epoch 2982, Loss: 0.37817147374153137, Final Batch Loss: 0.19047661125659943\n",
      "Epoch 2983, Loss: 0.27462758123874664, Final Batch Loss: 0.11627829074859619\n",
      "Epoch 2984, Loss: 0.1993011049926281, Final Batch Loss: 0.028141923248767853\n",
      "Epoch 2985, Loss: 0.22912541031837463, Final Batch Loss: 0.0415959507226944\n",
      "Epoch 2986, Loss: 0.24409237131476402, Final Batch Loss: 0.04256337508559227\n",
      "Epoch 2987, Loss: 0.2881630063056946, Final Batch Loss: 0.0853133499622345\n",
      "Epoch 2988, Loss: 0.15695716440677643, Final Batch Loss: 0.04656967893242836\n",
      "Epoch 2989, Loss: 0.47385504096746445, Final Batch Loss: 0.3071713149547577\n",
      "Epoch 2990, Loss: 0.24138798378407955, Final Batch Loss: 0.09960374981164932\n",
      "Epoch 2991, Loss: 0.2032417505979538, Final Batch Loss: 0.033108264207839966\n",
      "Epoch 2992, Loss: 0.21183239668607712, Final Batch Loss: 0.0824432298541069\n",
      "Epoch 2993, Loss: 0.24055246263742447, Final Batch Loss: 0.09466429054737091\n",
      "Epoch 2994, Loss: 0.214974207803607, Final Batch Loss: 0.029269153252243996\n",
      "Epoch 2995, Loss: 0.2687896564602852, Final Batch Loss: 0.1547294408082962\n",
      "Epoch 2996, Loss: 0.17179488949477673, Final Batch Loss: 0.05927584320306778\n",
      "Epoch 2997, Loss: 0.20873808674514294, Final Batch Loss: 0.10564232617616653\n",
      "Epoch 2998, Loss: 0.19872510433197021, Final Batch Loss: 0.019967440515756607\n",
      "Epoch 2999, Loss: 0.23457859084010124, Final Batch Loss: 0.13024021685123444\n",
      "Epoch 3000, Loss: 0.20831195637583733, Final Batch Loss: 0.039768267422914505\n",
      "Epoch 3001, Loss: 0.23992226272821426, Final Batch Loss: 0.04354317486286163\n",
      "Epoch 3002, Loss: 0.20109590888023376, Final Batch Loss: 0.06412084400653839\n",
      "Epoch 3003, Loss: 0.23511910624802113, Final Batch Loss: 0.027539851143956184\n",
      "Epoch 3004, Loss: 0.19760286808013916, Final Batch Loss: 0.06228253245353699\n",
      "Epoch 3005, Loss: 0.242311991751194, Final Batch Loss: 0.09389282763004303\n",
      "Epoch 3006, Loss: 0.2714012414216995, Final Batch Loss: 0.07213883846998215\n",
      "Epoch 3007, Loss: 0.2535194158554077, Final Batch Loss: 0.08193028718233109\n",
      "Epoch 3008, Loss: 0.2465226873755455, Final Batch Loss: 0.047114696353673935\n",
      "Epoch 3009, Loss: 0.16491854842752218, Final Batch Loss: 0.010462901555001736\n",
      "Epoch 3010, Loss: 0.2168719284236431, Final Batch Loss: 0.052458055317401886\n",
      "Epoch 3011, Loss: 0.1817583478987217, Final Batch Loss: 0.051159005612134933\n",
      "Epoch 3012, Loss: 0.1804117001593113, Final Batch Loss: 0.03897545486688614\n",
      "Epoch 3013, Loss: 0.24875417537987232, Final Batch Loss: 0.08256763964891434\n",
      "Epoch 3014, Loss: 0.1604855079203844, Final Batch Loss: 0.017308568581938744\n",
      "Epoch 3015, Loss: 0.27182994969189167, Final Batch Loss: 0.11769652366638184\n",
      "Epoch 3016, Loss: 0.17179526947438717, Final Batch Loss: 0.06478430330753326\n",
      "Epoch 3017, Loss: 0.24217040836811066, Final Batch Loss: 0.04989024996757507\n",
      "Epoch 3018, Loss: 0.13630284322425723, Final Batch Loss: 0.007416039239615202\n",
      "Epoch 3019, Loss: 0.20728731527924538, Final Batch Loss: 0.062094297260046005\n",
      "Epoch 3020, Loss: 0.22622746974229813, Final Batch Loss: 0.0480763241648674\n",
      "Epoch 3021, Loss: 0.18342667445540428, Final Batch Loss: 0.03193226084113121\n",
      "Epoch 3022, Loss: 0.2205633483827114, Final Batch Loss: 0.09185805916786194\n",
      "Epoch 3023, Loss: 0.155543839558959, Final Batch Loss: 0.015841154381632805\n",
      "Epoch 3024, Loss: 0.20817556977272034, Final Batch Loss: 0.051111362874507904\n",
      "Epoch 3025, Loss: 0.203486992046237, Final Batch Loss: 0.04952792450785637\n",
      "Epoch 3026, Loss: 0.24494192749261856, Final Batch Loss: 0.13757480680942535\n",
      "Epoch 3027, Loss: 0.2382438499480486, Final Batch Loss: 0.029368380084633827\n",
      "Epoch 3028, Loss: 0.32767385989427567, Final Batch Loss: 0.11383216083049774\n",
      "Epoch 3029, Loss: 0.20450496580451727, Final Batch Loss: 0.06416726112365723\n",
      "Epoch 3030, Loss: 0.3091014474630356, Final Batch Loss: 0.10826777666807175\n",
      "Epoch 3031, Loss: 0.21586275100708008, Final Batch Loss: 0.0473279170691967\n",
      "Epoch 3032, Loss: 0.22469699010252953, Final Batch Loss: 0.047162316739559174\n",
      "Epoch 3033, Loss: 0.20369933545589447, Final Batch Loss: 0.04429164156317711\n",
      "Epoch 3034, Loss: 0.15698823146522045, Final Batch Loss: 0.036018118262290955\n",
      "Epoch 3035, Loss: 0.2062712386250496, Final Batch Loss: 0.07874537259340286\n",
      "Epoch 3036, Loss: 0.20105288922786713, Final Batch Loss: 0.0739627256989479\n",
      "Epoch 3037, Loss: 0.23204165324568748, Final Batch Loss: 0.04874414950609207\n",
      "Epoch 3038, Loss: 0.2447056919336319, Final Batch Loss: 0.0531122088432312\n",
      "Epoch 3039, Loss: 0.18427514471113682, Final Batch Loss: 0.017466025426983833\n",
      "Epoch 3040, Loss: 0.2059050053358078, Final Batch Loss: 0.077271468937397\n",
      "Epoch 3041, Loss: 0.39178716018795967, Final Batch Loss: 0.12950338423252106\n",
      "Epoch 3042, Loss: 0.2854016460478306, Final Batch Loss: 0.11803802102804184\n",
      "Epoch 3043, Loss: 0.2598342038691044, Final Batch Loss: 0.05077356472611427\n",
      "Epoch 3044, Loss: 0.3523560129106045, Final Batch Loss: 0.13258886337280273\n",
      "Epoch 3045, Loss: 0.22752158716320992, Final Batch Loss: 0.04893948510289192\n",
      "Epoch 3046, Loss: 0.2974995821714401, Final Batch Loss: 0.09263424575328827\n",
      "Epoch 3047, Loss: 0.30955854803323746, Final Batch Loss: 0.07524314522743225\n",
      "Epoch 3048, Loss: 0.22792984172701836, Final Batch Loss: 0.05930105224251747\n",
      "Epoch 3049, Loss: 0.16765975952148438, Final Batch Loss: 0.04982534795999527\n",
      "Epoch 3050, Loss: 0.2683357745409012, Final Batch Loss: 0.12453389167785645\n",
      "Epoch 3051, Loss: 0.40558741241693497, Final Batch Loss: 0.2041897177696228\n",
      "Epoch 3052, Loss: 0.20409706979990005, Final Batch Loss: 0.039386216551065445\n",
      "Epoch 3053, Loss: 0.20704301446676254, Final Batch Loss: 0.024786394089460373\n",
      "Epoch 3054, Loss: 0.24812744185328484, Final Batch Loss: 0.04943093657493591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3055, Loss: 0.1677748505026102, Final Batch Loss: 0.027496447786688805\n",
      "Epoch 3056, Loss: 0.18448995053768158, Final Batch Loss: 0.017660893499851227\n",
      "Epoch 3057, Loss: 0.22281333059072495, Final Batch Loss: 0.08469819277524948\n",
      "Epoch 3058, Loss: 0.19937308505177498, Final Batch Loss: 0.05032777413725853\n",
      "Epoch 3059, Loss: 0.16748567298054695, Final Batch Loss: 0.03578843176364899\n",
      "Epoch 3060, Loss: 0.23040145263075829, Final Batch Loss: 0.07456780225038528\n",
      "Epoch 3061, Loss: 0.21277070976793766, Final Batch Loss: 0.10671432316303253\n",
      "Epoch 3062, Loss: 0.2089678831398487, Final Batch Loss: 0.0716959536075592\n",
      "Epoch 3063, Loss: 0.18231647089123726, Final Batch Loss: 0.034269362688064575\n",
      "Epoch 3064, Loss: 0.21636910736560822, Final Batch Loss: 0.03346628323197365\n",
      "Epoch 3065, Loss: 0.18295034766197205, Final Batch Loss: 0.021976346150040627\n",
      "Epoch 3066, Loss: 0.24311740137636662, Final Batch Loss: 0.13638189435005188\n",
      "Epoch 3067, Loss: 0.1995190791785717, Final Batch Loss: 0.04897052049636841\n",
      "Epoch 3068, Loss: 0.24505474790930748, Final Batch Loss: 0.08839859813451767\n",
      "Epoch 3069, Loss: 0.1663736253976822, Final Batch Loss: 0.024205852299928665\n",
      "Epoch 3070, Loss: 0.12434621900320053, Final Batch Loss: 0.015728965401649475\n",
      "Epoch 3071, Loss: 0.20301537588238716, Final Batch Loss: 0.10247661173343658\n",
      "Epoch 3072, Loss: 0.18344751745462418, Final Batch Loss: 0.033781375735998154\n",
      "Epoch 3073, Loss: 0.21188946068286896, Final Batch Loss: 0.057610832154750824\n",
      "Epoch 3074, Loss: 0.21903011947870255, Final Batch Loss: 0.04323513060808182\n",
      "Epoch 3075, Loss: 0.16247202083468437, Final Batch Loss: 0.014735836535692215\n",
      "Epoch 3076, Loss: 0.1596679538488388, Final Batch Loss: 0.03631765395402908\n",
      "Epoch 3077, Loss: 0.18992445431649685, Final Batch Loss: 0.07551905512809753\n",
      "Epoch 3078, Loss: 0.21491007506847382, Final Batch Loss: 0.05496668070554733\n",
      "Epoch 3079, Loss: 0.266971156001091, Final Batch Loss: 0.02724412828683853\n",
      "Epoch 3080, Loss: 0.23489313200116158, Final Batch Loss: 0.03072993829846382\n",
      "Epoch 3081, Loss: 0.20733986794948578, Final Batch Loss: 0.08739572763442993\n",
      "Epoch 3082, Loss: 0.16387938521802425, Final Batch Loss: 0.03518025577068329\n",
      "Epoch 3083, Loss: 0.24144890904426575, Final Batch Loss: 0.08389495313167572\n",
      "Epoch 3084, Loss: 0.17582470551133156, Final Batch Loss: 0.05089521408081055\n",
      "Epoch 3085, Loss: 0.22031230479478836, Final Batch Loss: 0.07214940339326859\n",
      "Epoch 3086, Loss: 0.2942035775631666, Final Batch Loss: 0.14155124127864838\n",
      "Epoch 3087, Loss: 0.14031834714114666, Final Batch Loss: 0.010164229199290276\n",
      "Epoch 3088, Loss: 0.2130148783326149, Final Batch Loss: 0.05457011237740517\n",
      "Epoch 3089, Loss: 0.1898854747414589, Final Batch Loss: 0.08262940496206284\n",
      "Epoch 3090, Loss: 0.19362197443842888, Final Batch Loss: 0.03665179759263992\n",
      "Epoch 3091, Loss: 0.11807059124112129, Final Batch Loss: 0.009026173502206802\n",
      "Epoch 3092, Loss: 0.22253650799393654, Final Batch Loss: 0.06008107215166092\n",
      "Epoch 3093, Loss: 0.18175972811877728, Final Batch Loss: 0.024627575650811195\n",
      "Epoch 3094, Loss: 0.21347504667937756, Final Batch Loss: 0.029459258541464806\n",
      "Epoch 3095, Loss: 0.14631681889295578, Final Batch Loss: 0.028020337224006653\n",
      "Epoch 3096, Loss: 0.19002170115709305, Final Batch Loss: 0.03325663506984711\n",
      "Epoch 3097, Loss: 0.1917167864739895, Final Batch Loss: 0.03338296338915825\n",
      "Epoch 3098, Loss: 0.22305302694439888, Final Batch Loss: 0.025746719911694527\n",
      "Epoch 3099, Loss: 0.11798074468970299, Final Batch Loss: 0.016863994300365448\n",
      "Epoch 3100, Loss: 0.18169695511460304, Final Batch Loss: 0.05311039090156555\n",
      "Epoch 3101, Loss: 0.22261963784694672, Final Batch Loss: 0.04263289272785187\n",
      "Epoch 3102, Loss: 0.20575331151485443, Final Batch Loss: 0.03995801508426666\n",
      "Epoch 3103, Loss: 0.2292599454522133, Final Batch Loss: 0.07626140862703323\n",
      "Epoch 3104, Loss: 0.12913532927632332, Final Batch Loss: 0.023440515622496605\n",
      "Epoch 3105, Loss: 0.25280629098415375, Final Batch Loss: 0.10272878408432007\n",
      "Epoch 3106, Loss: 0.19293297454714775, Final Batch Loss: 0.05193903297185898\n",
      "Epoch 3107, Loss: 0.14198160730302334, Final Batch Loss: 0.027761586010456085\n",
      "Epoch 3108, Loss: 0.21217865869402885, Final Batch Loss: 0.06259729713201523\n",
      "Epoch 3109, Loss: 0.21469027921557426, Final Batch Loss: 0.05007675662636757\n",
      "Epoch 3110, Loss: 0.1453956440091133, Final Batch Loss: 0.017015807330608368\n",
      "Epoch 3111, Loss: 0.2014186903834343, Final Batch Loss: 0.06372621655464172\n",
      "Epoch 3112, Loss: 0.1667147669941187, Final Batch Loss: 0.01039319671690464\n",
      "Epoch 3113, Loss: 0.17246093228459358, Final Batch Loss: 0.016092780977487564\n",
      "Epoch 3114, Loss: 0.21057469211518764, Final Batch Loss: 0.0813242569565773\n",
      "Epoch 3115, Loss: 0.21282966434955597, Final Batch Loss: 0.07001400738954544\n",
      "Epoch 3116, Loss: 0.15726638585329056, Final Batch Loss: 0.054950494319200516\n",
      "Epoch 3117, Loss: 0.2659657336771488, Final Batch Loss: 0.07114098966121674\n",
      "Epoch 3118, Loss: 0.3003871254622936, Final Batch Loss: 0.1947299838066101\n",
      "Epoch 3119, Loss: 0.21009231358766556, Final Batch Loss: 0.056439705193042755\n",
      "Epoch 3120, Loss: 0.2752261124551296, Final Batch Loss: 0.07392030954360962\n",
      "Epoch 3121, Loss: 0.24201297014951706, Final Batch Loss: 0.07145550101995468\n",
      "Epoch 3122, Loss: 0.41231444850564003, Final Batch Loss: 0.15149879455566406\n",
      "Epoch 3123, Loss: 0.2669465132057667, Final Batch Loss: 0.07819587737321854\n",
      "Epoch 3124, Loss: 0.29484641179442406, Final Batch Loss: 0.12109707295894623\n",
      "Epoch 3125, Loss: 0.3035332038998604, Final Batch Loss: 0.07478991150856018\n",
      "Epoch 3126, Loss: 0.24778522551059723, Final Batch Loss: 0.05342930555343628\n",
      "Epoch 3127, Loss: 0.17266086488962173, Final Batch Loss: 0.037751704454422\n",
      "Epoch 3128, Loss: 0.21366779878735542, Final Batch Loss: 0.033560022711753845\n",
      "Epoch 3129, Loss: 0.349196121096611, Final Batch Loss: 0.11295787245035172\n",
      "Epoch 3130, Loss: 0.2677912153303623, Final Batch Loss: 0.06927160918712616\n",
      "Epoch 3131, Loss: 0.29745739325881004, Final Batch Loss: 0.04555337503552437\n",
      "Epoch 3132, Loss: 0.2820894159376621, Final Batch Loss: 0.10163850337266922\n",
      "Epoch 3133, Loss: 0.19240864738821983, Final Batch Loss: 0.059886377304792404\n",
      "Epoch 3134, Loss: 0.2109030019491911, Final Batch Loss: 0.020258033648133278\n",
      "Epoch 3135, Loss: 0.1795394252985716, Final Batch Loss: 0.043032512068748474\n",
      "Epoch 3136, Loss: 0.21228602528572083, Final Batch Loss: 0.06591101735830307\n",
      "Epoch 3137, Loss: 0.1902630291879177, Final Batch Loss: 0.033366620540618896\n",
      "Epoch 3138, Loss: 0.15355442464351654, Final Batch Loss: 0.027677148580551147\n",
      "Epoch 3139, Loss: 0.20477810129523277, Final Batch Loss: 0.035841021686792374\n",
      "Epoch 3140, Loss: 0.24385305866599083, Final Batch Loss: 0.10272461175918579\n",
      "Epoch 3141, Loss: 0.21163628995418549, Final Batch Loss: 0.07210369408130646\n",
      "Epoch 3142, Loss: 0.17775441147387028, Final Batch Loss: 0.02810967154800892\n",
      "Epoch 3143, Loss: 0.34391625225543976, Final Batch Loss: 0.05632293224334717\n",
      "Epoch 3144, Loss: 0.28835492208600044, Final Batch Loss: 0.07436517626047134\n",
      "Epoch 3145, Loss: 0.17171185836195946, Final Batch Loss: 0.03194381296634674\n",
      "Epoch 3146, Loss: 0.221274146810174, Final Batch Loss: 0.025590328499674797\n",
      "Epoch 3147, Loss: 0.1693240888416767, Final Batch Loss: 0.03920212388038635\n",
      "Epoch 3148, Loss: 0.15879862289875746, Final Batch Loss: 0.00929099228233099\n",
      "Epoch 3149, Loss: 0.27119849622249603, Final Batch Loss: 0.07539111375808716\n",
      "Epoch 3150, Loss: 0.14703513123095036, Final Batch Loss: 0.034763630479574203\n",
      "Epoch 3151, Loss: 0.20290343463420868, Final Batch Loss: 0.06800326704978943\n",
      "Epoch 3152, Loss: 0.22081372141838074, Final Batch Loss: 0.06083543598651886\n",
      "Epoch 3153, Loss: 0.17081080004572868, Final Batch Loss: 0.041625238955020905\n",
      "Epoch 3154, Loss: 0.1371895927004516, Final Batch Loss: 0.0037832814268767834\n",
      "Epoch 3155, Loss: 0.2112871464341879, Final Batch Loss: 0.08742331713438034\n",
      "Epoch 3156, Loss: 0.17193500138819218, Final Batch Loss: 0.0252287145704031\n",
      "Epoch 3157, Loss: 0.1584004908800125, Final Batch Loss: 0.016256539151072502\n",
      "Epoch 3158, Loss: 0.17679069750010967, Final Batch Loss: 0.029566721990704536\n",
      "Epoch 3159, Loss: 0.1491713486611843, Final Batch Loss: 0.03971195966005325\n",
      "Epoch 3160, Loss: 0.13896884582936764, Final Batch Loss: 0.02773207798600197\n",
      "Epoch 3161, Loss: 0.21743497624993324, Final Batch Loss: 0.04045640677213669\n",
      "Epoch 3162, Loss: 0.20284606888890266, Final Batch Loss: 0.05081265792250633\n",
      "Epoch 3163, Loss: 0.15451664105057716, Final Batch Loss: 0.05266500264406204\n",
      "Epoch 3164, Loss: 0.28772613033652306, Final Batch Loss: 0.13915088772773743\n",
      "Epoch 3165, Loss: 0.19402621872723103, Final Batch Loss: 0.09112094342708588\n",
      "Epoch 3166, Loss: 0.23269464820623398, Final Batch Loss: 0.05945920944213867\n",
      "Epoch 3167, Loss: 0.25549524649977684, Final Batch Loss: 0.06624758988618851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3168, Loss: 0.27814244106411934, Final Batch Loss: 0.02236822620034218\n",
      "Epoch 3169, Loss: 0.2069732453674078, Final Batch Loss: 0.07112956047058105\n",
      "Epoch 3170, Loss: 0.186992559581995, Final Batch Loss: 0.01783636584877968\n",
      "Epoch 3171, Loss: 0.2497946061193943, Final Batch Loss: 0.029614631086587906\n",
      "Epoch 3172, Loss: 0.18269162625074387, Final Batch Loss: 0.020100794732570648\n",
      "Epoch 3173, Loss: 0.1482307519763708, Final Batch Loss: 0.027977032586932182\n",
      "Epoch 3174, Loss: 0.16234438121318817, Final Batch Loss: 0.028849735856056213\n",
      "Epoch 3175, Loss: 0.26682592555880547, Final Batch Loss: 0.07280982285737991\n",
      "Epoch 3176, Loss: 0.1834159679710865, Final Batch Loss: 0.009032968431711197\n",
      "Epoch 3177, Loss: 0.30678053945302963, Final Batch Loss: 0.14245398342609406\n",
      "Epoch 3178, Loss: 0.20303112268447876, Final Batch Loss: 0.09465784579515457\n",
      "Epoch 3179, Loss: 0.1642259545624256, Final Batch Loss: 0.025616489350795746\n",
      "Epoch 3180, Loss: 0.20005968399345875, Final Batch Loss: 0.05681944265961647\n",
      "Epoch 3181, Loss: 0.26076139137148857, Final Batch Loss: 0.07401822507381439\n",
      "Epoch 3182, Loss: 0.22439809516072273, Final Batch Loss: 0.0624164417386055\n",
      "Epoch 3183, Loss: 0.28488320484757423, Final Batch Loss: 0.12189970165491104\n",
      "Epoch 3184, Loss: 0.19938961416482925, Final Batch Loss: 0.04023361578583717\n",
      "Epoch 3185, Loss: 0.15816201642155647, Final Batch Loss: 0.012234609574079514\n",
      "Epoch 3186, Loss: 0.23322128131985664, Final Batch Loss: 0.02540605142712593\n",
      "Epoch 3187, Loss: 0.17659300565719604, Final Batch Loss: 0.012647368013858795\n",
      "Epoch 3188, Loss: 0.1622719932347536, Final Batch Loss: 0.04008221998810768\n",
      "Epoch 3189, Loss: 0.26140039786696434, Final Batch Loss: 0.04289846122264862\n",
      "Epoch 3190, Loss: 0.24536142870783806, Final Batch Loss: 0.11196514964103699\n",
      "Epoch 3191, Loss: 0.33440321683883667, Final Batch Loss: 0.11464861035346985\n",
      "Epoch 3192, Loss: 0.31527918204665184, Final Batch Loss: 0.04883940517902374\n",
      "Epoch 3193, Loss: 0.2069811150431633, Final Batch Loss: 0.039912521839141846\n",
      "Epoch 3194, Loss: 0.2844153828918934, Final Batch Loss: 0.060955148190259933\n",
      "Epoch 3195, Loss: 0.25832366570830345, Final Batch Loss: 0.08313269168138504\n",
      "Epoch 3196, Loss: 0.22744335606694221, Final Batch Loss: 0.09338641166687012\n",
      "Epoch 3197, Loss: 0.3241383172571659, Final Batch Loss: 0.12811239063739777\n",
      "Epoch 3198, Loss: 0.21763448789715767, Final Batch Loss: 0.055529169738292694\n",
      "Epoch 3199, Loss: 0.20788257755339146, Final Batch Loss: 0.01627003587782383\n",
      "Epoch 3200, Loss: 0.217967351898551, Final Batch Loss: 0.02243904210627079\n",
      "Epoch 3201, Loss: 0.2172517441213131, Final Batch Loss: 0.05530893802642822\n",
      "Epoch 3202, Loss: 0.19031982496380806, Final Batch Loss: 0.0546891912817955\n",
      "Epoch 3203, Loss: 0.15886747278273106, Final Batch Loss: 0.031724944710731506\n",
      "Epoch 3204, Loss: 0.20728738233447075, Final Batch Loss: 0.03029974177479744\n",
      "Epoch 3205, Loss: 0.17903174087405205, Final Batch Loss: 0.011424876749515533\n",
      "Epoch 3206, Loss: 0.15813330560922623, Final Batch Loss: 0.019940432161092758\n",
      "Epoch 3207, Loss: 0.23587368056178093, Final Batch Loss: 0.022967692464590073\n",
      "Epoch 3208, Loss: 0.22933649830520153, Final Batch Loss: 0.02038160152733326\n",
      "Epoch 3209, Loss: 0.16352459136396646, Final Batch Loss: 0.01285474468022585\n",
      "Epoch 3210, Loss: 0.18390506878495216, Final Batch Loss: 0.07401618361473083\n",
      "Epoch 3211, Loss: 0.21347350999712944, Final Batch Loss: 0.04072173684835434\n",
      "Epoch 3212, Loss: 0.17703727632761002, Final Batch Loss: 0.04346335679292679\n",
      "Epoch 3213, Loss: 0.29134315624833107, Final Batch Loss: 0.06538261473178864\n",
      "Epoch 3214, Loss: 0.15342224948108196, Final Batch Loss: 0.017478331923484802\n",
      "Epoch 3215, Loss: 0.17923086043447256, Final Batch Loss: 0.01444318424910307\n",
      "Epoch 3216, Loss: 0.17182646691799164, Final Batch Loss: 0.01327822357416153\n",
      "Epoch 3217, Loss: 0.1508272271603346, Final Batch Loss: 0.037532392889261246\n",
      "Epoch 3218, Loss: 0.22557582799345255, Final Batch Loss: 0.0314810574054718\n",
      "Epoch 3219, Loss: 0.1578848622739315, Final Batch Loss: 0.05291006714105606\n",
      "Epoch 3220, Loss: 0.1116152610629797, Final Batch Loss: 0.013387532904744148\n",
      "Epoch 3221, Loss: 0.2156833978369832, Final Batch Loss: 0.01166523713618517\n",
      "Epoch 3222, Loss: 0.17618153430521488, Final Batch Loss: 0.02742554061114788\n",
      "Epoch 3223, Loss: 0.19252840243279934, Final Batch Loss: 0.019748182967305183\n",
      "Epoch 3224, Loss: 0.1354088969528675, Final Batch Loss: 0.014336498454213142\n",
      "Epoch 3225, Loss: 0.23235267400741577, Final Batch Loss: 0.054308775812387466\n",
      "Epoch 3226, Loss: 0.14969901740550995, Final Batch Loss: 0.01761569269001484\n",
      "Epoch 3227, Loss: 0.1646759118884802, Final Batch Loss: 0.024558963254094124\n",
      "Epoch 3228, Loss: 0.1547874379903078, Final Batch Loss: 0.018033189699053764\n",
      "Epoch 3229, Loss: 0.26573055051267147, Final Batch Loss: 0.016346225515007973\n",
      "Epoch 3230, Loss: 0.16690512746572495, Final Batch Loss: 0.0722714439034462\n",
      "Epoch 3231, Loss: 0.24601687863469124, Final Batch Loss: 0.07647552341222763\n",
      "Epoch 3232, Loss: 0.17589099146425724, Final Batch Loss: 0.027863459661602974\n",
      "Epoch 3233, Loss: 0.11194125097244978, Final Batch Loss: 0.014101824723184109\n",
      "Epoch 3234, Loss: 0.23019573464989662, Final Batch Loss: 0.09758114814758301\n",
      "Epoch 3235, Loss: 0.19746854156255722, Final Batch Loss: 0.06511962413787842\n",
      "Epoch 3236, Loss: 0.14815109223127365, Final Batch Loss: 0.01737281307578087\n",
      "Epoch 3237, Loss: 0.16411256045103073, Final Batch Loss: 0.029035508632659912\n",
      "Epoch 3238, Loss: 0.2678780071437359, Final Batch Loss: 0.11805310845375061\n",
      "Epoch 3239, Loss: 0.1648706141859293, Final Batch Loss: 0.018088361248373985\n",
      "Epoch 3240, Loss: 0.14329798705875874, Final Batch Loss: 0.024346500635147095\n",
      "Epoch 3241, Loss: 0.1415955275297165, Final Batch Loss: 0.042393382638692856\n",
      "Epoch 3242, Loss: 0.19044581428170204, Final Batch Loss: 0.05499732494354248\n",
      "Epoch 3243, Loss: 0.2241563517600298, Final Batch Loss: 0.08767202496528625\n",
      "Epoch 3244, Loss: 0.2835686635226011, Final Batch Loss: 0.07885491847991943\n",
      "Epoch 3245, Loss: 0.20908482745289803, Final Batch Loss: 0.05219647288322449\n",
      "Epoch 3246, Loss: 0.230553537607193, Final Batch Loss: 0.06315199285745621\n",
      "Epoch 3247, Loss: 0.2714878208935261, Final Batch Loss: 0.05666034668684006\n",
      "Epoch 3248, Loss: 0.1963314265012741, Final Batch Loss: 0.03888682648539543\n",
      "Epoch 3249, Loss: 0.2261252999305725, Final Batch Loss: 0.10981439054012299\n",
      "Epoch 3250, Loss: 0.15427007619291544, Final Batch Loss: 0.01387199480086565\n",
      "Epoch 3251, Loss: 0.22721131518483162, Final Batch Loss: 0.07582336664199829\n",
      "Epoch 3252, Loss: 0.2284787893295288, Final Batch Loss: 0.05348051339387894\n",
      "Epoch 3253, Loss: 0.22061129286885262, Final Batch Loss: 0.08709758520126343\n",
      "Epoch 3254, Loss: 0.2468351237475872, Final Batch Loss: 0.03501182049512863\n",
      "Epoch 3255, Loss: 0.1931467354297638, Final Batch Loss: 0.034787628799676895\n",
      "Epoch 3256, Loss: 0.3241840600967407, Final Batch Loss: 0.16288146376609802\n",
      "Epoch 3257, Loss: 0.1365701649338007, Final Batch Loss: 0.025430982932448387\n",
      "Epoch 3258, Loss: 0.23958488926291466, Final Batch Loss: 0.030912533402442932\n",
      "Epoch 3259, Loss: 0.28346600383520126, Final Batch Loss: 0.12268439680337906\n",
      "Epoch 3260, Loss: 0.26334039494395256, Final Batch Loss: 0.06295088678598404\n",
      "Epoch 3261, Loss: 0.20902209170162678, Final Batch Loss: 0.015512438490986824\n",
      "Epoch 3262, Loss: 0.18780997954308987, Final Batch Loss: 0.04162592813372612\n",
      "Epoch 3263, Loss: 0.21501600742340088, Final Batch Loss: 0.023783426731824875\n",
      "Epoch 3264, Loss: 0.15014350414276123, Final Batch Loss: 0.015487410128116608\n",
      "Epoch 3265, Loss: 0.19304877892136574, Final Batch Loss: 0.0601649172604084\n",
      "Epoch 3266, Loss: 0.20989418029785156, Final Batch Loss: 0.07971201837062836\n",
      "Epoch 3267, Loss: 0.17839770298451185, Final Batch Loss: 0.0214623361825943\n",
      "Epoch 3268, Loss: 0.214424641802907, Final Batch Loss: 0.04241878539323807\n",
      "Epoch 3269, Loss: 0.22484951466321945, Final Batch Loss: 0.06479612737894058\n",
      "Epoch 3270, Loss: 0.14503386989235878, Final Batch Loss: 0.04503583535552025\n",
      "Epoch 3271, Loss: 0.18271831050515175, Final Batch Loss: 0.03672972694039345\n",
      "Epoch 3272, Loss: 0.1771028023213148, Final Batch Loss: 0.024035902693867683\n",
      "Epoch 3273, Loss: 0.14564534183591604, Final Batch Loss: 0.012416626326739788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3274, Loss: 0.10920004174113274, Final Batch Loss: 0.02378751151263714\n",
      "Epoch 3275, Loss: 0.16261935606598854, Final Batch Loss: 0.052507590502500534\n",
      "Epoch 3276, Loss: 0.15113514475524426, Final Batch Loss: 0.06420847028493881\n",
      "Epoch 3277, Loss: 0.2298857532441616, Final Batch Loss: 0.09938177466392517\n",
      "Epoch 3278, Loss: 0.17912346869707108, Final Batch Loss: 0.05342620983719826\n",
      "Epoch 3279, Loss: 0.23367445543408394, Final Batch Loss: 0.0551096647977829\n",
      "Epoch 3280, Loss: 0.18766376376152039, Final Batch Loss: 0.031363051384687424\n",
      "Epoch 3281, Loss: 0.22141989320516586, Final Batch Loss: 0.09617109596729279\n",
      "Epoch 3282, Loss: 0.19916359335184097, Final Batch Loss: 0.07768752425909042\n",
      "Epoch 3283, Loss: 0.22436805628240108, Final Batch Loss: 0.05367783084511757\n",
      "Epoch 3284, Loss: 0.24090748466551304, Final Batch Loss: 0.025193387642502785\n",
      "Epoch 3285, Loss: 0.21428019553422928, Final Batch Loss: 0.07009479403495789\n",
      "Epoch 3286, Loss: 0.20349644124507904, Final Batch Loss: 0.03776635229587555\n",
      "Epoch 3287, Loss: 0.221011720597744, Final Batch Loss: 0.03901287913322449\n",
      "Epoch 3288, Loss: 0.1191681893542409, Final Batch Loss: 0.009724843315780163\n",
      "Epoch 3289, Loss: 0.16232913359999657, Final Batch Loss: 0.06234743446111679\n",
      "Epoch 3290, Loss: 0.2554255537688732, Final Batch Loss: 0.09003797173500061\n",
      "Epoch 3291, Loss: 0.16540686413645744, Final Batch Loss: 0.026466209441423416\n",
      "Epoch 3292, Loss: 0.19425150752067566, Final Batch Loss: 0.03358671814203262\n",
      "Epoch 3293, Loss: 0.15747522469609976, Final Batch Loss: 0.014275151304900646\n",
      "Epoch 3294, Loss: 0.17570233345031738, Final Batch Loss: 0.0841469094157219\n",
      "Epoch 3295, Loss: 0.21600058674812317, Final Batch Loss: 0.04780763015151024\n",
      "Epoch 3296, Loss: 0.20376300439238548, Final Batch Loss: 0.03937452659010887\n",
      "Epoch 3297, Loss: 0.18085234984755516, Final Batch Loss: 0.01779841259121895\n",
      "Epoch 3298, Loss: 0.25910915061831474, Final Batch Loss: 0.08954223245382309\n",
      "Epoch 3299, Loss: 0.2574002780020237, Final Batch Loss: 0.12828058004379272\n",
      "Epoch 3300, Loss: 0.18586184829473495, Final Batch Loss: 0.04228826239705086\n",
      "Epoch 3301, Loss: 0.2193812821060419, Final Batch Loss: 0.05028771236538887\n",
      "Epoch 3302, Loss: 0.17448043823242188, Final Batch Loss: 0.03891588747501373\n",
      "Epoch 3303, Loss: 0.18991832062602043, Final Batch Loss: 0.019977018237113953\n",
      "Epoch 3304, Loss: 0.305826298892498, Final Batch Loss: 0.06998930871486664\n",
      "Epoch 3305, Loss: 0.1653760802000761, Final Batch Loss: 0.019466547295451164\n",
      "Epoch 3306, Loss: 0.20503049716353416, Final Batch Loss: 0.05498042330145836\n",
      "Epoch 3307, Loss: 0.22102727368474007, Final Batch Loss: 0.04339608550071716\n",
      "Epoch 3308, Loss: 0.15707501582801342, Final Batch Loss: 0.02944296784698963\n",
      "Epoch 3309, Loss: 0.2151825875043869, Final Batch Loss: 0.09327210485935211\n",
      "Epoch 3310, Loss: 0.21256228908896446, Final Batch Loss: 0.043569959700107574\n",
      "Epoch 3311, Loss: 0.16922309063374996, Final Batch Loss: 0.055858999490737915\n",
      "Epoch 3312, Loss: 0.3528604954481125, Final Batch Loss: 0.13243867456912994\n",
      "Epoch 3313, Loss: 0.1883301269263029, Final Batch Loss: 0.023774797096848488\n",
      "Epoch 3314, Loss: 0.20394524931907654, Final Batch Loss: 0.07230297476053238\n",
      "Epoch 3315, Loss: 0.22231504693627357, Final Batch Loss: 0.035175859928131104\n",
      "Epoch 3316, Loss: 0.1645128559321165, Final Batch Loss: 0.042104024440050125\n",
      "Epoch 3317, Loss: 0.2785989083349705, Final Batch Loss: 0.1593443900346756\n",
      "Epoch 3318, Loss: 0.24472089484333992, Final Batch Loss: 0.050525277853012085\n",
      "Epoch 3319, Loss: 0.17810681462287903, Final Batch Loss: 0.0609411895275116\n",
      "Epoch 3320, Loss: 0.15229644253849983, Final Batch Loss: 0.031054392457008362\n",
      "Epoch 3321, Loss: 0.22378050163388252, Final Batch Loss: 0.10522247850894928\n",
      "Epoch 3322, Loss: 0.20822828263044357, Final Batch Loss: 0.049340348690748215\n",
      "Epoch 3323, Loss: 0.25457220524549484, Final Batch Loss: 0.052231840789318085\n",
      "Epoch 3324, Loss: 0.16098188422620296, Final Batch Loss: 0.015905098989605904\n",
      "Epoch 3325, Loss: 0.16135700419545174, Final Batch Loss: 0.033177077770233154\n",
      "Epoch 3326, Loss: 0.25191687047481537, Final Batch Loss: 0.11248864978551865\n",
      "Epoch 3327, Loss: 0.15475963056087494, Final Batch Loss: 0.03251063823699951\n",
      "Epoch 3328, Loss: 0.19208965450525284, Final Batch Loss: 0.03589003533124924\n",
      "Epoch 3329, Loss: 0.13081231340765953, Final Batch Loss: 0.009740296751260757\n",
      "Epoch 3330, Loss: 0.18507028371095657, Final Batch Loss: 0.0429903008043766\n",
      "Epoch 3331, Loss: 0.1556511204689741, Final Batch Loss: 0.05371180176734924\n",
      "Epoch 3332, Loss: 0.2913046218454838, Final Batch Loss: 0.15743276476860046\n",
      "Epoch 3333, Loss: 0.29770442470908165, Final Batch Loss: 0.1111956387758255\n",
      "Epoch 3334, Loss: 0.1698286309838295, Final Batch Loss: 0.02799837663769722\n",
      "Epoch 3335, Loss: 0.1801269520074129, Final Batch Loss: 0.06775535643100739\n",
      "Epoch 3336, Loss: 0.19546026177704334, Final Batch Loss: 0.017481403425335884\n",
      "Epoch 3337, Loss: 0.142079534009099, Final Batch Loss: 0.03130396828055382\n",
      "Epoch 3338, Loss: 0.11564403586089611, Final Batch Loss: 0.0160389244556427\n",
      "Epoch 3339, Loss: 0.13566980324685574, Final Batch Loss: 0.0474497452378273\n",
      "Epoch 3340, Loss: 0.1935270093381405, Final Batch Loss: 0.06309188902378082\n",
      "Epoch 3341, Loss: 0.14872062765061855, Final Batch Loss: 0.021305786445736885\n",
      "Epoch 3342, Loss: 0.1536623928695917, Final Batch Loss: 0.033439915627241135\n",
      "Epoch 3343, Loss: 0.13638438284397125, Final Batch Loss: 0.018959740176796913\n",
      "Epoch 3344, Loss: 0.20984679833054543, Final Batch Loss: 0.02415289729833603\n",
      "Epoch 3345, Loss: 0.13249017484486103, Final Batch Loss: 0.039540745317935944\n",
      "Epoch 3346, Loss: 0.229485921561718, Final Batch Loss: 0.023951880633831024\n",
      "Epoch 3347, Loss: 0.12905129604041576, Final Batch Loss: 0.017211152240633965\n",
      "Epoch 3348, Loss: 0.15505672246217728, Final Batch Loss: 0.012063831090927124\n",
      "Epoch 3349, Loss: 0.20494253188371658, Final Batch Loss: 0.05253343656659126\n",
      "Epoch 3350, Loss: 0.20520811900496483, Final Batch Loss: 0.021314967423677444\n",
      "Epoch 3351, Loss: 0.2163681797683239, Final Batch Loss: 0.039453212171792984\n",
      "Epoch 3352, Loss: 0.16742706298828125, Final Batch Loss: 0.039254866540431976\n",
      "Epoch 3353, Loss: 0.21109097823500633, Final Batch Loss: 0.06518636643886566\n",
      "Epoch 3354, Loss: 0.15876764990389347, Final Batch Loss: 0.07036016881465912\n",
      "Epoch 3355, Loss: 0.22985298745334148, Final Batch Loss: 0.019296225160360336\n",
      "Epoch 3356, Loss: 0.1706445161253214, Final Batch Loss: 0.0220594834536314\n",
      "Epoch 3357, Loss: 0.2229723408818245, Final Batch Loss: 0.045468688011169434\n",
      "Epoch 3358, Loss: 0.18612784519791603, Final Batch Loss: 0.05484895780682564\n",
      "Epoch 3359, Loss: 0.21082286164164543, Final Batch Loss: 0.03752614185214043\n",
      "Epoch 3360, Loss: 0.2216174267232418, Final Batch Loss: 0.051598742604255676\n",
      "Epoch 3361, Loss: 0.23581062257289886, Final Batch Loss: 0.09450286626815796\n",
      "Epoch 3362, Loss: 0.21024142764508724, Final Batch Loss: 0.1211874857544899\n",
      "Epoch 3363, Loss: 0.21241900697350502, Final Batch Loss: 0.0653841495513916\n",
      "Epoch 3364, Loss: 0.20840293914079666, Final Batch Loss: 0.09179921448230743\n",
      "Epoch 3365, Loss: 0.25765926763415337, Final Batch Loss: 0.12888959050178528\n",
      "Epoch 3366, Loss: 0.2675694711506367, Final Batch Loss: 0.03979332000017166\n",
      "Epoch 3367, Loss: 0.3037606328725815, Final Batch Loss: 0.04647163301706314\n",
      "Epoch 3368, Loss: 0.19233695790171623, Final Batch Loss: 0.06808467954397202\n",
      "Epoch 3369, Loss: 0.15938524901866913, Final Batch Loss: 0.02044331282377243\n",
      "Epoch 3370, Loss: 0.27994224429130554, Final Batch Loss: 0.08877664059400558\n",
      "Epoch 3371, Loss: 0.33841225132346153, Final Batch Loss: 0.10219914466142654\n",
      "Epoch 3372, Loss: 0.1381866242736578, Final Batch Loss: 0.018694622442126274\n",
      "Epoch 3373, Loss: 0.16665995121002197, Final Batch Loss: 0.06518417596817017\n",
      "Epoch 3374, Loss: 0.13245881535112858, Final Batch Loss: 0.01392718218266964\n",
      "Epoch 3375, Loss: 0.22897685319185257, Final Batch Loss: 0.06134162098169327\n",
      "Epoch 3376, Loss: 0.2320847548544407, Final Batch Loss: 0.05462021008133888\n",
      "Epoch 3377, Loss: 0.3088229391723871, Final Batch Loss: 0.16704578697681427\n",
      "Epoch 3378, Loss: 0.25105807930231094, Final Batch Loss: 0.05549737811088562\n",
      "Epoch 3379, Loss: 0.32912037521600723, Final Batch Loss: 0.18640857934951782\n",
      "Epoch 3380, Loss: 0.1853177584707737, Final Batch Loss: 0.01262521743774414\n",
      "Epoch 3381, Loss: 0.14842521771788597, Final Batch Loss: 0.01439608447253704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3382, Loss: 0.19853705167770386, Final Batch Loss: 0.08290047943592072\n",
      "Epoch 3383, Loss: 0.1680984515696764, Final Batch Loss: 0.016582975164055824\n",
      "Epoch 3384, Loss: 0.213272325694561, Final Batch Loss: 0.044262923300266266\n",
      "Epoch 3385, Loss: 0.18208078294992447, Final Batch Loss: 0.02052280679345131\n",
      "Epoch 3386, Loss: 0.2195960395038128, Final Batch Loss: 0.05343993753194809\n",
      "Epoch 3387, Loss: 0.1672077216207981, Final Batch Loss: 0.031367797404527664\n",
      "Epoch 3388, Loss: 0.2964734211564064, Final Batch Loss: 0.13493913412094116\n",
      "Epoch 3389, Loss: 0.1871046144515276, Final Batch Loss: 0.053565774112939835\n",
      "Epoch 3390, Loss: 0.20468881726264954, Final Batch Loss: 0.0752747431397438\n",
      "Epoch 3391, Loss: 0.25779280066490173, Final Batch Loss: 0.03729995712637901\n",
      "Epoch 3392, Loss: 0.18363895267248154, Final Batch Loss: 0.05616585910320282\n",
      "Epoch 3393, Loss: 0.1478247633203864, Final Batch Loss: 0.014027888886630535\n",
      "Epoch 3394, Loss: 0.25517651438713074, Final Batch Loss: 0.05374140292406082\n",
      "Epoch 3395, Loss: 0.18103943578898907, Final Batch Loss: 0.04714711010456085\n",
      "Epoch 3396, Loss: 0.27562516927719116, Final Batch Loss: 0.11068751662969589\n",
      "Epoch 3397, Loss: 0.2784861735999584, Final Batch Loss: 0.051692746579647064\n",
      "Epoch 3398, Loss: 0.21460244432091713, Final Batch Loss: 0.044409800320863724\n",
      "Epoch 3399, Loss: 0.27765390649437904, Final Batch Loss: 0.16097649931907654\n",
      "Epoch 3400, Loss: 0.28041731007397175, Final Batch Loss: 0.12773223221302032\n",
      "Epoch 3401, Loss: 0.321564968675375, Final Batch Loss: 0.06103597208857536\n",
      "Epoch 3402, Loss: 0.2702735997736454, Final Batch Loss: 0.03637273609638214\n",
      "Epoch 3403, Loss: 0.2408245950937271, Final Batch Loss: 0.05338846892118454\n",
      "Epoch 3404, Loss: 0.20824332907795906, Final Batch Loss: 0.042164694517850876\n",
      "Epoch 3405, Loss: 0.27959635108709335, Final Batch Loss: 0.07012603431940079\n",
      "Epoch 3406, Loss: 0.2834651693701744, Final Batch Loss: 0.09823627024888992\n",
      "Epoch 3407, Loss: 0.2191842533648014, Final Batch Loss: 0.06778459995985031\n",
      "Epoch 3408, Loss: 0.18807528540492058, Final Batch Loss: 0.056457120925188065\n",
      "Epoch 3409, Loss: 0.17870544269680977, Final Batch Loss: 0.045720282942056656\n",
      "Epoch 3410, Loss: 0.22783448174595833, Final Batch Loss: 0.06411445885896683\n",
      "Epoch 3411, Loss: 0.18996846489608288, Final Batch Loss: 0.08452009409666061\n",
      "Epoch 3412, Loss: 0.2994021996855736, Final Batch Loss: 0.04890589788556099\n",
      "Epoch 3413, Loss: 0.17560824751853943, Final Batch Loss: 0.034017689526081085\n",
      "Epoch 3414, Loss: 0.15860886499285698, Final Batch Loss: 0.02831401862204075\n",
      "Epoch 3415, Loss: 0.26227932423353195, Final Batch Loss: 0.07300704717636108\n",
      "Epoch 3416, Loss: 0.1910368138924241, Final Batch Loss: 0.015159052796661854\n",
      "Epoch 3417, Loss: 0.265865258872509, Final Batch Loss: 0.060828905552625656\n",
      "Epoch 3418, Loss: 0.17011527717113495, Final Batch Loss: 0.032179493457078934\n",
      "Epoch 3419, Loss: 0.2857249453663826, Final Batch Loss: 0.047875627875328064\n",
      "Epoch 3420, Loss: 0.18846877850592136, Final Batch Loss: 0.07136797904968262\n",
      "Epoch 3421, Loss: 0.11905217543244362, Final Batch Loss: 0.03703264892101288\n",
      "Epoch 3422, Loss: 0.22572718560695648, Final Batch Loss: 0.03460071235895157\n",
      "Epoch 3423, Loss: 0.22969985008239746, Final Batch Loss: 0.05932994931936264\n",
      "Epoch 3424, Loss: 0.16304028406739235, Final Batch Loss: 0.009881434962153435\n",
      "Epoch 3425, Loss: 0.19980822876095772, Final Batch Loss: 0.045336637645959854\n",
      "Epoch 3426, Loss: 0.20866022631525993, Final Batch Loss: 0.03433943912386894\n",
      "Epoch 3427, Loss: 0.19865060970187187, Final Batch Loss: 0.05630757659673691\n",
      "Epoch 3428, Loss: 0.18773489631712437, Final Batch Loss: 0.06103230640292168\n",
      "Epoch 3429, Loss: 0.1817691009491682, Final Batch Loss: 0.02246629074215889\n",
      "Epoch 3430, Loss: 0.15473458357155323, Final Batch Loss: 0.02054394595324993\n",
      "Epoch 3431, Loss: 0.1527850739657879, Final Batch Loss: 0.05137888342142105\n",
      "Epoch 3432, Loss: 0.1802417654544115, Final Batch Loss: 0.02822069637477398\n",
      "Epoch 3433, Loss: 0.14312936179339886, Final Batch Loss: 0.04277460277080536\n",
      "Epoch 3434, Loss: 0.20851148292422295, Final Batch Loss: 0.08272480964660645\n",
      "Epoch 3435, Loss: 0.22162006050348282, Final Batch Loss: 0.0808945745229721\n",
      "Epoch 3436, Loss: 0.12748456746339798, Final Batch Loss: 0.05198972299695015\n",
      "Epoch 3437, Loss: 0.20060844905674458, Final Batch Loss: 0.04041792452335358\n",
      "Epoch 3438, Loss: 0.1693052351474762, Final Batch Loss: 0.033839136362075806\n",
      "Epoch 3439, Loss: 0.21732657589018345, Final Batch Loss: 0.024982305243611336\n",
      "Epoch 3440, Loss: 0.21650119125843048, Final Batch Loss: 0.06397751718759537\n",
      "Epoch 3441, Loss: 0.23093008622527122, Final Batch Loss: 0.049735184758901596\n",
      "Epoch 3442, Loss: 0.11636721435934305, Final Batch Loss: 0.020769067108631134\n",
      "Epoch 3443, Loss: 0.23724975436925888, Final Batch Loss: 0.061001572757959366\n",
      "Epoch 3444, Loss: 0.21867237985134125, Final Batch Loss: 0.02531856670975685\n",
      "Epoch 3445, Loss: 0.27856970578432083, Final Batch Loss: 0.18687908351421356\n",
      "Epoch 3446, Loss: 0.24306410737335682, Final Batch Loss: 0.028669441118836403\n",
      "Epoch 3447, Loss: 0.21855377964675426, Final Batch Loss: 0.03025069646537304\n",
      "Epoch 3448, Loss: 0.2117023542523384, Final Batch Loss: 0.08672915399074554\n",
      "Epoch 3449, Loss: 0.26341043040156364, Final Batch Loss: 0.057067353278398514\n",
      "Epoch 3450, Loss: 0.1918472796678543, Final Batch Loss: 0.029449880123138428\n",
      "Epoch 3451, Loss: 0.18239120207726955, Final Batch Loss: 0.0273729357868433\n",
      "Epoch 3452, Loss: 0.198806993663311, Final Batch Loss: 0.028842482715845108\n",
      "Epoch 3453, Loss: 0.13334705121815205, Final Batch Loss: 0.011903846636414528\n",
      "Epoch 3454, Loss: 0.1411720421165228, Final Batch Loss: 0.030588731169700623\n",
      "Epoch 3455, Loss: 0.2299061268568039, Final Batch Loss: 0.018796484917402267\n",
      "Epoch 3456, Loss: 0.19130063243210316, Final Batch Loss: 0.009702375158667564\n",
      "Epoch 3457, Loss: 0.23296083137392998, Final Batch Loss: 0.023359503597021103\n",
      "Epoch 3458, Loss: 0.1937423888593912, Final Batch Loss: 0.07357173413038254\n",
      "Epoch 3459, Loss: 0.21018126234412193, Final Batch Loss: 0.09404197335243225\n",
      "Epoch 3460, Loss: 0.2798122614622116, Final Batch Loss: 0.08151416480541229\n",
      "Epoch 3461, Loss: 0.49945126101374626, Final Batch Loss: 0.2338854968547821\n",
      "Epoch 3462, Loss: 0.37337180227041245, Final Batch Loss: 0.18654277920722961\n",
      "Epoch 3463, Loss: 0.22428599372506142, Final Batch Loss: 0.038053959608078\n",
      "Epoch 3464, Loss: 0.28475275821983814, Final Batch Loss: 0.030252831056714058\n",
      "Epoch 3465, Loss: 0.18283215910196304, Final Batch Loss: 0.034751150757074356\n",
      "Epoch 3466, Loss: 0.22630036249756813, Final Batch Loss: 0.0831259936094284\n",
      "Epoch 3467, Loss: 0.2910661958158016, Final Batch Loss: 0.09581059962511063\n",
      "Epoch 3468, Loss: 0.21743042021989822, Final Batch Loss: 0.05712512135505676\n",
      "Epoch 3469, Loss: 0.22512417286634445, Final Batch Loss: 0.08760692179203033\n",
      "Epoch 3470, Loss: 0.14378430135548115, Final Batch Loss: 0.021438751369714737\n",
      "Epoch 3471, Loss: 0.24827784672379494, Final Batch Loss: 0.06350285559892654\n",
      "Epoch 3472, Loss: 0.25714557245373726, Final Batch Loss: 0.1328049600124359\n",
      "Epoch 3473, Loss: 0.21555308997631073, Final Batch Loss: 0.07405325025320053\n",
      "Epoch 3474, Loss: 0.23105255141854286, Final Batch Loss: 0.056587640196084976\n",
      "Epoch 3475, Loss: 0.16028568521142006, Final Batch Loss: 0.03939485549926758\n",
      "Epoch 3476, Loss: 0.13184267841279507, Final Batch Loss: 0.02147730439901352\n",
      "Epoch 3477, Loss: 0.166013078764081, Final Batch Loss: 0.018393030390143394\n",
      "Epoch 3478, Loss: 0.16650654189288616, Final Batch Loss: 0.0158951748162508\n",
      "Epoch 3479, Loss: 0.1981630139052868, Final Batch Loss: 0.05287652462720871\n",
      "Epoch 3480, Loss: 0.20356560125946999, Final Batch Loss: 0.027341019362211227\n",
      "Epoch 3481, Loss: 0.15137931890785694, Final Batch Loss: 0.04850257188081741\n",
      "Epoch 3482, Loss: 0.11565415002405643, Final Batch Loss: 0.019044406712055206\n",
      "Epoch 3483, Loss: 0.16455380618572235, Final Batch Loss: 0.025822807103395462\n",
      "Epoch 3484, Loss: 0.16028502956032753, Final Batch Loss: 0.03777496889233589\n",
      "Epoch 3485, Loss: 0.16239860653877258, Final Batch Loss: 0.03780684247612953\n",
      "Epoch 3486, Loss: 0.17964756954461336, Final Batch Loss: 0.01055378932505846\n",
      "Epoch 3487, Loss: 0.18712534476071596, Final Batch Loss: 0.03224712982773781\n",
      "Epoch 3488, Loss: 0.3373658359050751, Final Batch Loss: 0.16258223354816437\n",
      "Epoch 3489, Loss: 0.1368314092978835, Final Batch Loss: 0.05714713782072067\n",
      "Epoch 3490, Loss: 0.19381229765713215, Final Batch Loss: 0.01826498471200466\n",
      "Epoch 3491, Loss: 0.14758630841970444, Final Batch Loss: 0.04460471123456955\n",
      "Epoch 3492, Loss: 0.25289659947156906, Final Batch Loss: 0.11643806099891663\n",
      "Epoch 3493, Loss: 0.18742734473198652, Final Batch Loss: 0.015491723082959652\n",
      "Epoch 3494, Loss: 0.219236109405756, Final Batch Loss: 0.0697026252746582\n",
      "Epoch 3495, Loss: 0.3500819057226181, Final Batch Loss: 0.13485343754291534\n",
      "Epoch 3496, Loss: 0.26271598786115646, Final Batch Loss: 0.10850342363119125\n",
      "Epoch 3497, Loss: 0.2832094468176365, Final Batch Loss: 0.045721326023340225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3498, Loss: 0.34513380378484726, Final Batch Loss: 0.04262395575642586\n",
      "Epoch 3499, Loss: 0.24496622383594513, Final Batch Loss: 0.03803183138370514\n",
      "Epoch 3500, Loss: 0.15247298497706652, Final Batch Loss: 0.012871242128312588\n",
      "Epoch 3501, Loss: 0.18298848904669285, Final Batch Loss: 0.018744727596640587\n",
      "Epoch 3502, Loss: 0.2264716736972332, Final Batch Loss: 0.0339287705719471\n",
      "Epoch 3503, Loss: 0.16416305676102638, Final Batch Loss: 0.020404648035764694\n",
      "Epoch 3504, Loss: 0.22286921553313732, Final Batch Loss: 0.07070966809988022\n",
      "Epoch 3505, Loss: 0.16412285342812538, Final Batch Loss: 0.03164517879486084\n",
      "Epoch 3506, Loss: 0.2234484665095806, Final Batch Loss: 0.03750378638505936\n",
      "Epoch 3507, Loss: 0.19166321121156216, Final Batch Loss: 0.07124193012714386\n",
      "Epoch 3508, Loss: 0.24067337065935135, Final Batch Loss: 0.08342961221933365\n",
      "Epoch 3509, Loss: 0.16637300327420235, Final Batch Loss: 0.03279603272676468\n",
      "Epoch 3510, Loss: 0.1872076615691185, Final Batch Loss: 0.04092775657773018\n",
      "Epoch 3511, Loss: 0.17786432430148125, Final Batch Loss: 0.028665076941251755\n",
      "Epoch 3512, Loss: 0.24840101227164268, Final Batch Loss: 0.0980716198682785\n",
      "Epoch 3513, Loss: 0.19658319652080536, Final Batch Loss: 0.03522716462612152\n",
      "Epoch 3514, Loss: 0.21682846173644066, Final Batch Loss: 0.046462420374155045\n",
      "Epoch 3515, Loss: 0.2529820166528225, Final Batch Loss: 0.02002440020442009\n",
      "Epoch 3516, Loss: 0.11608445085585117, Final Batch Loss: 0.01708630658686161\n",
      "Epoch 3517, Loss: 0.17385918274521828, Final Batch Loss: 0.05372147634625435\n",
      "Epoch 3518, Loss: 0.1883063204586506, Final Batch Loss: 0.04588347300887108\n",
      "Epoch 3519, Loss: 0.23350946232676506, Final Batch Loss: 0.10089562088251114\n",
      "Epoch 3520, Loss: 0.2744250390678644, Final Batch Loss: 0.1378038227558136\n",
      "Epoch 3521, Loss: 0.22226806730031967, Final Batch Loss: 0.027656737715005875\n",
      "Epoch 3522, Loss: 0.12089357152581215, Final Batch Loss: 0.010085534304380417\n",
      "Epoch 3523, Loss: 0.25795790180563927, Final Batch Loss: 0.04044586792588234\n",
      "Epoch 3524, Loss: 0.20622199960052967, Final Batch Loss: 0.024827035143971443\n",
      "Epoch 3525, Loss: 0.22077141515910625, Final Batch Loss: 0.10928622633218765\n",
      "Epoch 3526, Loss: 0.15211028046905994, Final Batch Loss: 0.04664818197488785\n",
      "Epoch 3527, Loss: 0.17497224360704422, Final Batch Loss: 0.022035468369722366\n",
      "Epoch 3528, Loss: 0.1530137024819851, Final Batch Loss: 0.02694793790578842\n",
      "Epoch 3529, Loss: 0.20130705088377, Final Batch Loss: 0.02470964379608631\n",
      "Epoch 3530, Loss: 0.1748108398169279, Final Batch Loss: 0.018103647977113724\n",
      "Epoch 3531, Loss: 0.21262320317327976, Final Batch Loss: 0.08688580989837646\n",
      "Epoch 3532, Loss: 0.16473636403679848, Final Batch Loss: 0.03121473826467991\n",
      "Epoch 3533, Loss: 0.13671299256384373, Final Batch Loss: 0.06255519390106201\n",
      "Epoch 3534, Loss: 0.16666713543236256, Final Batch Loss: 0.027544455602765083\n",
      "Epoch 3535, Loss: 0.13521630875766277, Final Batch Loss: 0.04228284955024719\n",
      "Epoch 3536, Loss: 0.18537432700395584, Final Batch Loss: 0.05345300957560539\n",
      "Epoch 3537, Loss: 0.2418709695339203, Final Batch Loss: 0.09585072100162506\n",
      "Epoch 3538, Loss: 0.23120730742812157, Final Batch Loss: 0.07162369042634964\n",
      "Epoch 3539, Loss: 0.13250189367681742, Final Batch Loss: 0.01272930670529604\n",
      "Epoch 3540, Loss: 0.22092944756150246, Final Batch Loss: 0.05672774091362953\n",
      "Epoch 3541, Loss: 0.153715243563056, Final Batch Loss: 0.03927510231733322\n",
      "Epoch 3542, Loss: 0.27225466445088387, Final Batch Loss: 0.097239650785923\n",
      "Epoch 3543, Loss: 0.20849533565342426, Final Batch Loss: 0.08817476034164429\n",
      "Epoch 3544, Loss: 0.22088328935205936, Final Batch Loss: 0.02268320880830288\n",
      "Epoch 3545, Loss: 0.19180532917380333, Final Batch Loss: 0.033423274755477905\n",
      "Epoch 3546, Loss: 0.16991882398724556, Final Batch Loss: 0.048097606748342514\n",
      "Epoch 3547, Loss: 0.22597191855311394, Final Batch Loss: 0.05865931138396263\n",
      "Epoch 3548, Loss: 0.18081370927393436, Final Batch Loss: 0.05690294876694679\n",
      "Epoch 3549, Loss: 0.14706671703606844, Final Batch Loss: 0.011065577156841755\n",
      "Epoch 3550, Loss: 0.14410815201699734, Final Batch Loss: 0.017109448090195656\n",
      "Epoch 3551, Loss: 0.2104048077017069, Final Batch Loss: 0.0599554181098938\n",
      "Epoch 3552, Loss: 0.14483834616839886, Final Batch Loss: 0.026450002565979958\n",
      "Epoch 3553, Loss: 0.12391545623540878, Final Batch Loss: 0.03114740177989006\n",
      "Epoch 3554, Loss: 0.15850896388292313, Final Batch Loss: 0.03654184564948082\n",
      "Epoch 3555, Loss: 0.16571408230811357, Final Batch Loss: 0.008574952371418476\n",
      "Epoch 3556, Loss: 0.1776960138231516, Final Batch Loss: 0.06748301535844803\n",
      "Epoch 3557, Loss: 0.17285753414034843, Final Batch Loss: 0.024230338633060455\n",
      "Epoch 3558, Loss: 0.19068782031536102, Final Batch Loss: 0.06918556988239288\n",
      "Epoch 3559, Loss: 0.1279218541458249, Final Batch Loss: 0.011778612621128559\n",
      "Epoch 3560, Loss: 0.15769416093826294, Final Batch Loss: 0.0543019101023674\n",
      "Epoch 3561, Loss: 0.13350404798984528, Final Batch Loss: 0.030858362093567848\n",
      "Epoch 3562, Loss: 0.09000756591558456, Final Batch Loss: 0.015243381261825562\n",
      "Epoch 3563, Loss: 0.2283267378807068, Final Batch Loss: 0.05317007750272751\n",
      "Epoch 3564, Loss: 0.1293171988800168, Final Batch Loss: 0.011405306868255138\n",
      "Epoch 3565, Loss: 0.1342514418065548, Final Batch Loss: 0.03805847093462944\n",
      "Epoch 3566, Loss: 0.2997276932001114, Final Batch Loss: 0.14598311483860016\n",
      "Epoch 3567, Loss: 0.17603003792464733, Final Batch Loss: 0.04477590695023537\n",
      "Epoch 3568, Loss: 0.1466282308101654, Final Batch Loss: 0.028670547530055046\n",
      "Epoch 3569, Loss: 0.20230813324451447, Final Batch Loss: 0.03562319278717041\n",
      "Epoch 3570, Loss: 0.16407201066613197, Final Batch Loss: 0.021045126020908356\n",
      "Epoch 3571, Loss: 0.1550838416442275, Final Batch Loss: 0.01246029045432806\n",
      "Epoch 3572, Loss: 0.14343916811048985, Final Batch Loss: 0.006250226870179176\n",
      "Epoch 3573, Loss: 0.12791243754327297, Final Batch Loss: 0.006832608953118324\n",
      "Epoch 3574, Loss: 0.15534265525639057, Final Batch Loss: 0.019257085397839546\n",
      "Epoch 3575, Loss: 0.13836796954274178, Final Batch Loss: 0.022949572652578354\n",
      "Epoch 3576, Loss: 0.1238930793479085, Final Batch Loss: 0.009798754937946796\n",
      "Epoch 3577, Loss: 0.14222165942192078, Final Batch Loss: 0.033356837928295135\n",
      "Epoch 3578, Loss: 0.19704840332269669, Final Batch Loss: 0.08894157409667969\n",
      "Epoch 3579, Loss: 0.1382058560848236, Final Batch Loss: 0.03265472128987312\n",
      "Epoch 3580, Loss: 0.2640685588121414, Final Batch Loss: 0.03321726247668266\n",
      "Epoch 3581, Loss: 0.21963907405734062, Final Batch Loss: 0.09520888328552246\n",
      "Epoch 3582, Loss: 0.1353903580456972, Final Batch Loss: 0.037547655403614044\n",
      "Epoch 3583, Loss: 0.2146212588995695, Final Batch Loss: 0.07558556646108627\n",
      "Epoch 3584, Loss: 0.16638440266251564, Final Batch Loss: 0.01471571996808052\n",
      "Epoch 3585, Loss: 0.2517869994044304, Final Batch Loss: 0.05563243478536606\n",
      "Epoch 3586, Loss: 0.19642282649874687, Final Batch Loss: 0.06632570177316666\n",
      "Epoch 3587, Loss: 0.12915374524891376, Final Batch Loss: 0.040880586951971054\n",
      "Epoch 3588, Loss: 0.12729372084140778, Final Batch Loss: 0.02076125703752041\n",
      "Epoch 3589, Loss: 0.24153317511081696, Final Batch Loss: 0.04713905230164528\n",
      "Epoch 3590, Loss: 0.17429180070757866, Final Batch Loss: 0.019573282450437546\n",
      "Epoch 3591, Loss: 0.2612664923071861, Final Batch Loss: 0.058116380125284195\n",
      "Epoch 3592, Loss: 0.19065332040190697, Final Batch Loss: 0.04457066208124161\n",
      "Epoch 3593, Loss: 0.2099386788904667, Final Batch Loss: 0.008529551327228546\n",
      "Epoch 3594, Loss: 0.2360668107867241, Final Batch Loss: 0.0783110037446022\n",
      "Epoch 3595, Loss: 0.2187737040221691, Final Batch Loss: 0.0416865199804306\n",
      "Epoch 3596, Loss: 0.18455752171576023, Final Batch Loss: 0.020358609035611153\n",
      "Epoch 3597, Loss: 0.16875219531357288, Final Batch Loss: 0.029515983536839485\n",
      "Epoch 3598, Loss: 0.16996153816580772, Final Batch Loss: 0.0409618504345417\n",
      "Epoch 3599, Loss: 0.2679595407098532, Final Batch Loss: 0.12352626025676727\n",
      "Epoch 3600, Loss: 0.15321127697825432, Final Batch Loss: 0.0071745626628398895\n",
      "Epoch 3601, Loss: 0.22900701873004436, Final Batch Loss: 0.011951891705393791\n",
      "Epoch 3602, Loss: 0.3102188464254141, Final Batch Loss: 0.011017238721251488\n",
      "Epoch 3603, Loss: 0.3317337930202484, Final Batch Loss: 0.06282233446836472\n",
      "Epoch 3604, Loss: 0.22223572432994843, Final Batch Loss: 0.01888425275683403\n",
      "Epoch 3605, Loss: 0.32030588015913963, Final Batch Loss: 0.09388675540685654\n",
      "Epoch 3606, Loss: 0.25112010166049004, Final Batch Loss: 0.05370099097490311\n",
      "Epoch 3607, Loss: 0.2697000578045845, Final Batch Loss: 0.04360423982143402\n",
      "Epoch 3608, Loss: 0.31853628158569336, Final Batch Loss: 0.10000480711460114\n",
      "Epoch 3609, Loss: 0.21429022774100304, Final Batch Loss: 0.014815784990787506\n",
      "Epoch 3610, Loss: 0.17898643016815186, Final Batch Loss: 0.03163275495171547\n",
      "Epoch 3611, Loss: 0.1364804827608168, Final Batch Loss: 0.00686216214671731\n",
      "Epoch 3612, Loss: 0.1689193770289421, Final Batch Loss: 0.037452999502420425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3613, Loss: 0.17322290688753128, Final Batch Loss: 0.04473089799284935\n",
      "Epoch 3614, Loss: 0.15274621546268463, Final Batch Loss: 0.021637193858623505\n",
      "Epoch 3615, Loss: 0.17005979269742966, Final Batch Loss: 0.037277791649103165\n",
      "Epoch 3616, Loss: 0.2611463963985443, Final Batch Loss: 0.13589586317539215\n",
      "Epoch 3617, Loss: 0.17117029055953026, Final Batch Loss: 0.052743177860975266\n",
      "Epoch 3618, Loss: 0.17817684076726437, Final Batch Loss: 0.028540322557091713\n",
      "Epoch 3619, Loss: 0.20676051452755928, Final Batch Loss: 0.027366362512111664\n",
      "Epoch 3620, Loss: 0.24551654793322086, Final Batch Loss: 0.1304551661014557\n",
      "Epoch 3621, Loss: 0.193709097802639, Final Batch Loss: 0.04222596064209938\n",
      "Epoch 3622, Loss: 0.18326617404818535, Final Batch Loss: 0.07674839347600937\n",
      "Epoch 3623, Loss: 0.1883121356368065, Final Batch Loss: 0.02046603336930275\n",
      "Epoch 3624, Loss: 0.29088287614285946, Final Batch Loss: 0.15169820189476013\n",
      "Epoch 3625, Loss: 0.16004827246069908, Final Batch Loss: 0.07262659817934036\n",
      "Epoch 3626, Loss: 0.19232462905347347, Final Batch Loss: 0.06942300498485565\n",
      "Epoch 3627, Loss: 0.35169293731451035, Final Batch Loss: 0.07722680270671844\n",
      "Epoch 3628, Loss: 0.2879093810915947, Final Batch Loss: 0.10077626258134842\n",
      "Epoch 3629, Loss: 0.2769993357360363, Final Batch Loss: 0.07782749086618423\n",
      "Epoch 3630, Loss: 0.2588011361658573, Final Batch Loss: 0.052573833614587784\n",
      "Epoch 3631, Loss: 0.23410363495349884, Final Batch Loss: 0.024248942732810974\n",
      "Epoch 3632, Loss: 0.251944150775671, Final Batch Loss: 0.03814292699098587\n",
      "Epoch 3633, Loss: 0.24185743182897568, Final Batch Loss: 0.06116431578993797\n",
      "Epoch 3634, Loss: 0.26612075604498386, Final Batch Loss: 0.08335017412900925\n",
      "Epoch 3635, Loss: 0.2571181058883667, Final Batch Loss: 0.0586465522646904\n",
      "Epoch 3636, Loss: 0.1561795435845852, Final Batch Loss: 0.04277382791042328\n",
      "Epoch 3637, Loss: 0.1961879851296544, Final Batch Loss: 0.014554821886122227\n",
      "Epoch 3638, Loss: 0.22752670384943485, Final Batch Loss: 0.023920027539134026\n",
      "Epoch 3639, Loss: 0.20722327008843422, Final Batch Loss: 0.08140487223863602\n",
      "Epoch 3640, Loss: 0.16395722515881062, Final Batch Loss: 0.01634971983730793\n",
      "Epoch 3641, Loss: 0.17720824666321278, Final Batch Loss: 0.03542616218328476\n",
      "Epoch 3642, Loss: 0.1987861478701234, Final Batch Loss: 0.015414089895784855\n",
      "Epoch 3643, Loss: 0.2537475395947695, Final Batch Loss: 0.08132041990756989\n",
      "Epoch 3644, Loss: 0.1899022664874792, Final Batch Loss: 0.09074374288320541\n",
      "Epoch 3645, Loss: 0.20367705635726452, Final Batch Loss: 0.0712992250919342\n",
      "Epoch 3646, Loss: 0.24984485283493996, Final Batch Loss: 0.09977979212999344\n",
      "Epoch 3647, Loss: 0.19859111122787, Final Batch Loss: 0.019573645666241646\n",
      "Epoch 3648, Loss: 0.25754614174366, Final Batch Loss: 0.09687536209821701\n",
      "Epoch 3649, Loss: 0.2612566202878952, Final Batch Loss: 0.07330601662397385\n",
      "Epoch 3650, Loss: 0.27280042693018913, Final Batch Loss: 0.0819159671664238\n",
      "Epoch 3651, Loss: 0.20056572929024696, Final Batch Loss: 0.023078758269548416\n",
      "Epoch 3652, Loss: 0.30028675496578217, Final Batch Loss: 0.13756409287452698\n",
      "Epoch 3653, Loss: 0.25990669056773186, Final Batch Loss: 0.050449904054403305\n",
      "Epoch 3654, Loss: 0.24063462391495705, Final Batch Loss: 0.03592957928776741\n",
      "Epoch 3655, Loss: 0.16122986003756523, Final Batch Loss: 0.039230041205883026\n",
      "Epoch 3656, Loss: 0.301255464553833, Final Batch Loss: 0.09672801941633224\n",
      "Epoch 3657, Loss: 0.23804325982928276, Final Batch Loss: 0.07137393206357956\n",
      "Epoch 3658, Loss: 0.18374890461564064, Final Batch Loss: 0.022269945591688156\n",
      "Epoch 3659, Loss: 0.26313998363912106, Final Batch Loss: 0.03109092451632023\n",
      "Epoch 3660, Loss: 0.21088650450110435, Final Batch Loss: 0.060364749282598495\n",
      "Epoch 3661, Loss: 0.3570030890405178, Final Batch Loss: 0.19079868495464325\n",
      "Epoch 3662, Loss: 0.3159852586686611, Final Batch Loss: 0.16833436489105225\n",
      "Epoch 3663, Loss: 0.21461454965174198, Final Batch Loss: 0.02674860693514347\n",
      "Epoch 3664, Loss: 0.37605827301740646, Final Batch Loss: 0.07613580673933029\n",
      "Epoch 3665, Loss: 0.16983567271381617, Final Batch Loss: 0.00727179367095232\n",
      "Epoch 3666, Loss: 0.16677499562501907, Final Batch Loss: 0.03984232619404793\n",
      "Epoch 3667, Loss: 0.22700583189725876, Final Batch Loss: 0.1219794824719429\n",
      "Epoch 3668, Loss: 0.20645399577915668, Final Batch Loss: 0.022585449740290642\n",
      "Epoch 3669, Loss: 0.41681044548749924, Final Batch Loss: 0.2161901295185089\n",
      "Epoch 3670, Loss: 0.23858067207038403, Final Batch Loss: 0.10743090510368347\n",
      "Epoch 3671, Loss: 0.18630308657884598, Final Batch Loss: 0.04952728748321533\n",
      "Epoch 3672, Loss: 0.260690163820982, Final Batch Loss: 0.08801276981830597\n",
      "Epoch 3673, Loss: 0.2023472636938095, Final Batch Loss: 0.07013057172298431\n",
      "Epoch 3674, Loss: 0.26975712552666664, Final Batch Loss: 0.0996711254119873\n",
      "Epoch 3675, Loss: 0.19187032617628574, Final Batch Loss: 0.06410194933414459\n",
      "Epoch 3676, Loss: 0.19491459429264069, Final Batch Loss: 0.07089895009994507\n",
      "Epoch 3677, Loss: 0.1831935103982687, Final Batch Loss: 0.014929108321666718\n",
      "Epoch 3678, Loss: 0.24910877645015717, Final Batch Loss: 0.06472527235746384\n",
      "Epoch 3679, Loss: 0.23695021867752075, Final Batch Loss: 0.07442177087068558\n",
      "Epoch 3680, Loss: 0.25843101739883423, Final Batch Loss: 0.0534394234418869\n",
      "Epoch 3681, Loss: 0.171719029545784, Final Batch Loss: 0.05540807917714119\n",
      "Epoch 3682, Loss: 0.21234685741364956, Final Batch Loss: 0.029396919533610344\n",
      "Epoch 3683, Loss: 0.0970936194062233, Final Batch Loss: 0.02198844775557518\n",
      "Epoch 3684, Loss: 0.14833426475524902, Final Batch Loss: 0.03880850970745087\n",
      "Epoch 3685, Loss: 0.23112622648477554, Final Batch Loss: 0.08002039045095444\n",
      "Epoch 3686, Loss: 0.1818918902426958, Final Batch Loss: 0.028589563444256783\n",
      "Epoch 3687, Loss: 0.14441253431141376, Final Batch Loss: 0.025523774325847626\n",
      "Epoch 3688, Loss: 0.18566259182989597, Final Batch Loss: 0.07159880548715591\n",
      "Epoch 3689, Loss: 0.1392082218080759, Final Batch Loss: 0.040075864642858505\n",
      "Epoch 3690, Loss: 0.17754586972296238, Final Batch Loss: 0.03073819912970066\n",
      "Epoch 3691, Loss: 0.18192900717258453, Final Batch Loss: 0.03701776638627052\n",
      "Epoch 3692, Loss: 0.1456341901794076, Final Batch Loss: 0.044864192605018616\n",
      "Epoch 3693, Loss: 0.15950431860983372, Final Batch Loss: 0.06561482697725296\n",
      "Epoch 3694, Loss: 0.16169782169163227, Final Batch Loss: 0.03853859379887581\n",
      "Epoch 3695, Loss: 0.19017682038247585, Final Batch Loss: 0.05852070450782776\n",
      "Epoch 3696, Loss: 0.22161413356661797, Final Batch Loss: 0.07590848207473755\n",
      "Epoch 3697, Loss: 0.15659493580460548, Final Batch Loss: 0.03145432099699974\n",
      "Epoch 3698, Loss: 0.18029923737049103, Final Batch Loss: 0.044314946979284286\n",
      "Epoch 3699, Loss: 0.16352743282914162, Final Batch Loss: 0.05514689162373543\n",
      "Epoch 3700, Loss: 0.1478655654937029, Final Batch Loss: 0.02335997484624386\n",
      "Epoch 3701, Loss: 0.14692137762904167, Final Batch Loss: 0.01406656950712204\n",
      "Epoch 3702, Loss: 0.13363638147711754, Final Batch Loss: 0.04626376926898956\n",
      "Epoch 3703, Loss: 0.16715620085597038, Final Batch Loss: 0.04284008592367172\n",
      "Epoch 3704, Loss: 0.23468439280986786, Final Batch Loss: 0.05040794983506203\n",
      "Epoch 3705, Loss: 0.27758947759866714, Final Batch Loss: 0.09886392951011658\n",
      "Epoch 3706, Loss: 0.17262912169098854, Final Batch Loss: 0.04858659207820892\n",
      "Epoch 3707, Loss: 0.1935525182634592, Final Batch Loss: 0.028216814622282982\n",
      "Epoch 3708, Loss: 0.23036318644881248, Final Batch Loss: 0.08090811222791672\n",
      "Epoch 3709, Loss: 0.1573487389832735, Final Batch Loss: 0.05077094957232475\n",
      "Epoch 3710, Loss: 0.1427971189841628, Final Batch Loss: 0.010715781711041927\n",
      "Epoch 3711, Loss: 0.3073226325213909, Final Batch Loss: 0.125170037150383\n",
      "Epoch 3712, Loss: 0.19652313739061356, Final Batch Loss: 0.07168213278055191\n",
      "Epoch 3713, Loss: 0.2194420713931322, Final Batch Loss: 0.06735799461603165\n",
      "Epoch 3714, Loss: 0.2037994135171175, Final Batch Loss: 0.02152479626238346\n",
      "Epoch 3715, Loss: 0.21481012925505638, Final Batch Loss: 0.0653381273150444\n",
      "Epoch 3716, Loss: 0.24842476844787598, Final Batch Loss: 0.07308956980705261\n",
      "Epoch 3717, Loss: 0.2172594591975212, Final Batch Loss: 0.0390738882124424\n",
      "Epoch 3718, Loss: 0.24306954629719257, Final Batch Loss: 0.059090644121170044\n",
      "Epoch 3719, Loss: 0.21989888697862625, Final Batch Loss: 0.07999397069215775\n",
      "Epoch 3720, Loss: 0.13957932125777006, Final Batch Loss: 0.014955698512494564\n",
      "Epoch 3721, Loss: 0.15314572677016258, Final Batch Loss: 0.032690003514289856\n",
      "Epoch 3722, Loss: 0.15694233775138855, Final Batch Loss: 0.017992503941059113\n",
      "Epoch 3723, Loss: 0.12003960832953453, Final Batch Loss: 0.026949450373649597\n",
      "Epoch 3724, Loss: 0.20023908466100693, Final Batch Loss: 0.035342127084732056\n",
      "Epoch 3725, Loss: 0.1749986819922924, Final Batch Loss: 0.05815517157316208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3726, Loss: 0.10071184486150742, Final Batch Loss: 0.01740630716085434\n",
      "Epoch 3727, Loss: 0.15449306555092335, Final Batch Loss: 0.04018107429146767\n",
      "Epoch 3728, Loss: 0.1927013099193573, Final Batch Loss: 0.04461846500635147\n",
      "Epoch 3729, Loss: 0.10211022570729256, Final Batch Loss: 0.022204726934432983\n",
      "Epoch 3730, Loss: 0.15388897247612476, Final Batch Loss: 0.01841598190367222\n",
      "Epoch 3731, Loss: 0.15777664631605148, Final Batch Loss: 0.00864294171333313\n",
      "Epoch 3732, Loss: 0.12672301195561886, Final Batch Loss: 0.015828914940357208\n",
      "Epoch 3733, Loss: 0.18429730460047722, Final Batch Loss: 0.031164227053523064\n",
      "Epoch 3734, Loss: 0.23523563891649246, Final Batch Loss: 0.09837739914655685\n",
      "Epoch 3735, Loss: 0.180972121655941, Final Batch Loss: 0.02539149299263954\n",
      "Epoch 3736, Loss: 0.1558913141489029, Final Batch Loss: 0.041585467755794525\n",
      "Epoch 3737, Loss: 0.1913685193285346, Final Batch Loss: 0.009462381713092327\n",
      "Epoch 3738, Loss: 0.14638147316873074, Final Batch Loss: 0.04961381480097771\n",
      "Epoch 3739, Loss: 0.21692641451954842, Final Batch Loss: 0.0838971957564354\n",
      "Epoch 3740, Loss: 0.18284356221556664, Final Batch Loss: 0.016379185020923615\n",
      "Epoch 3741, Loss: 0.17242787964642048, Final Batch Loss: 0.04671637713909149\n",
      "Epoch 3742, Loss: 0.18099252879619598, Final Batch Loss: 0.07473228871822357\n",
      "Epoch 3743, Loss: 0.16741788387298584, Final Batch Loss: 0.03139117360115051\n",
      "Epoch 3744, Loss: 0.21208623051643372, Final Batch Loss: 0.06009598448872566\n",
      "Epoch 3745, Loss: 0.2445988990366459, Final Batch Loss: 0.06130252033472061\n",
      "Epoch 3746, Loss: 0.17875923216342926, Final Batch Loss: 0.04552360251545906\n",
      "Epoch 3747, Loss: 0.25983552262187004, Final Batch Loss: 0.035324595868587494\n",
      "Epoch 3748, Loss: 0.24086454510688782, Final Batch Loss: 0.0951651930809021\n",
      "Epoch 3749, Loss: 0.2680184990167618, Final Batch Loss: 0.11393487453460693\n",
      "Epoch 3750, Loss: 0.24092348664999008, Final Batch Loss: 0.048378437757492065\n",
      "Epoch 3751, Loss: 0.17689229175448418, Final Batch Loss: 0.036988887935876846\n",
      "Epoch 3752, Loss: 0.1681274939328432, Final Batch Loss: 0.03959977254271507\n",
      "Epoch 3753, Loss: 0.1892189010977745, Final Batch Loss: 0.07680113613605499\n",
      "Epoch 3754, Loss: 0.1558920480310917, Final Batch Loss: 0.057857539504766464\n",
      "Epoch 3755, Loss: 0.19380224496126175, Final Batch Loss: 0.06822219491004944\n",
      "Epoch 3756, Loss: 0.3094847071915865, Final Batch Loss: 0.14232121407985687\n",
      "Epoch 3757, Loss: 0.1689187977463007, Final Batch Loss: 0.02118956856429577\n",
      "Epoch 3758, Loss: 0.16537378169596195, Final Batch Loss: 0.018630174919962883\n",
      "Epoch 3759, Loss: 0.23693815618753433, Final Batch Loss: 0.08239107578992844\n",
      "Epoch 3760, Loss: 0.21620609238743782, Final Batch Loss: 0.07876435667276382\n",
      "Epoch 3761, Loss: 0.23961805179715157, Final Batch Loss: 0.04915172606706619\n",
      "Epoch 3762, Loss: 0.17621947545558214, Final Batch Loss: 0.008452394045889378\n",
      "Epoch 3763, Loss: 0.19212099723517895, Final Batch Loss: 0.07490308582782745\n",
      "Epoch 3764, Loss: 0.20690108835697174, Final Batch Loss: 0.10092224925756454\n",
      "Epoch 3765, Loss: 0.21534455567598343, Final Batch Loss: 0.08306986838579178\n",
      "Epoch 3766, Loss: 0.15952646918594837, Final Batch Loss: 0.05630408227443695\n",
      "Epoch 3767, Loss: 0.1293761134147644, Final Batch Loss: 0.05089419335126877\n",
      "Epoch 3768, Loss: 0.15436239168047905, Final Batch Loss: 0.024102548137307167\n",
      "Epoch 3769, Loss: 0.24294847436249256, Final Batch Loss: 0.057168081402778625\n",
      "Epoch 3770, Loss: 0.16376490518450737, Final Batch Loss: 0.027034547179937363\n",
      "Epoch 3771, Loss: 0.21748104691505432, Final Batch Loss: 0.01881306990981102\n",
      "Epoch 3772, Loss: 0.15745420521125197, Final Batch Loss: 0.007526244502514601\n",
      "Epoch 3773, Loss: 0.3202057033777237, Final Batch Loss: 0.22062253952026367\n",
      "Epoch 3774, Loss: 0.15322988480329514, Final Batch Loss: 0.03966974839568138\n",
      "Epoch 3775, Loss: 0.23542168363928795, Final Batch Loss: 0.07758123427629471\n",
      "Epoch 3776, Loss: 0.23947996273636818, Final Batch Loss: 0.06259069591760635\n",
      "Epoch 3777, Loss: 0.20388125628232956, Final Batch Loss: 0.05530793219804764\n",
      "Epoch 3778, Loss: 0.2316040750592947, Final Batch Loss: 0.07264244556427002\n",
      "Epoch 3779, Loss: 0.16280090808868408, Final Batch Loss: 0.050814252346754074\n",
      "Epoch 3780, Loss: 0.19762193225324154, Final Batch Loss: 0.05732322484254837\n",
      "Epoch 3781, Loss: 0.20642059296369553, Final Batch Loss: 0.0459817573428154\n",
      "Epoch 3782, Loss: 0.23208148032426834, Final Batch Loss: 0.07386590540409088\n",
      "Epoch 3783, Loss: 0.14098582603037357, Final Batch Loss: 0.061183687299489975\n",
      "Epoch 3784, Loss: 0.18537655100226402, Final Batch Loss: 0.04522889479994774\n",
      "Epoch 3785, Loss: 0.2271016202867031, Final Batch Loss: 0.04232235252857208\n",
      "Epoch 3786, Loss: 0.3283853158354759, Final Batch Loss: 0.09128612279891968\n",
      "Epoch 3787, Loss: 0.1367635391652584, Final Batch Loss: 0.047669101506471634\n",
      "Epoch 3788, Loss: 0.17506833747029305, Final Batch Loss: 0.025919660925865173\n",
      "Epoch 3789, Loss: 0.17427825555205345, Final Batch Loss: 0.04684603959321976\n",
      "Epoch 3790, Loss: 0.23450320959091187, Final Batch Loss: 0.10289284586906433\n",
      "Epoch 3791, Loss: 0.2114550769329071, Final Batch Loss: 0.04323543980717659\n",
      "Epoch 3792, Loss: 0.14107363298535347, Final Batch Loss: 0.043145641684532166\n",
      "Epoch 3793, Loss: 0.11395210679620504, Final Batch Loss: 0.008203626610338688\n",
      "Epoch 3794, Loss: 0.16130953468382359, Final Batch Loss: 0.016489334404468536\n",
      "Epoch 3795, Loss: 0.18281329423189163, Final Batch Loss: 0.03126594051718712\n",
      "Epoch 3796, Loss: 0.19444401282817125, Final Batch Loss: 0.08141149580478668\n",
      "Epoch 3797, Loss: 0.1529037095606327, Final Batch Loss: 0.02055002748966217\n",
      "Epoch 3798, Loss: 0.14151156693696976, Final Batch Loss: 0.013960450887680054\n",
      "Epoch 3799, Loss: 0.1700961124151945, Final Batch Loss: 0.015681156888604164\n",
      "Epoch 3800, Loss: 0.14084781613200903, Final Batch Loss: 0.008955991826951504\n",
      "Epoch 3801, Loss: 0.19254372641444206, Final Batch Loss: 0.019330330193042755\n",
      "Epoch 3802, Loss: 0.2411305010318756, Final Batch Loss: 0.11694101244211197\n",
      "Epoch 3803, Loss: 0.19180410541594028, Final Batch Loss: 0.07114695757627487\n",
      "Epoch 3804, Loss: 0.16571948491036892, Final Batch Loss: 0.05038294568657875\n",
      "Epoch 3805, Loss: 0.1777416691184044, Final Batch Loss: 0.016554925590753555\n",
      "Epoch 3806, Loss: 0.20398320257663727, Final Batch Loss: 0.05981958284974098\n",
      "Epoch 3807, Loss: 0.22289029508829117, Final Batch Loss: 0.03885853290557861\n",
      "Epoch 3808, Loss: 0.34374070540070534, Final Batch Loss: 0.16804219782352448\n",
      "Epoch 3809, Loss: 0.1971776857972145, Final Batch Loss: 0.059654876589775085\n",
      "Epoch 3810, Loss: 0.19079405069351196, Final Batch Loss: 0.03183497488498688\n",
      "Epoch 3811, Loss: 0.2576910965144634, Final Batch Loss: 0.07076719403266907\n",
      "Epoch 3812, Loss: 0.16348369047045708, Final Batch Loss: 0.0387583002448082\n",
      "Epoch 3813, Loss: 0.2738761380314827, Final Batch Loss: 0.07568937540054321\n",
      "Epoch 3814, Loss: 0.13361196778714657, Final Batch Loss: 0.02582947164773941\n",
      "Epoch 3815, Loss: 0.22030063904821873, Final Batch Loss: 0.08817509561777115\n",
      "Epoch 3816, Loss: 0.2573254406452179, Final Batch Loss: 0.05004115030169487\n",
      "Epoch 3817, Loss: 0.2639359086751938, Final Batch Loss: 0.07598395645618439\n",
      "Epoch 3818, Loss: 0.17072379775345325, Final Batch Loss: 0.019036272540688515\n",
      "Epoch 3819, Loss: 0.1830701120197773, Final Batch Loss: 0.015279028564691544\n",
      "Epoch 3820, Loss: 0.18105346895754337, Final Batch Loss: 0.03449821472167969\n",
      "Epoch 3821, Loss: 0.2242929246276617, Final Batch Loss: 0.10637862980365753\n",
      "Epoch 3822, Loss: 0.16815942898392677, Final Batch Loss: 0.03830951824784279\n",
      "Epoch 3823, Loss: 0.1652478501200676, Final Batch Loss: 0.03388303890824318\n",
      "Epoch 3824, Loss: 0.16695384122431278, Final Batch Loss: 0.015720905736088753\n",
      "Epoch 3825, Loss: 0.16789692640304565, Final Batch Loss: 0.031084369868040085\n",
      "Epoch 3826, Loss: 0.18558469787240028, Final Batch Loss: 0.03821759298443794\n",
      "Epoch 3827, Loss: 0.15862946771085262, Final Batch Loss: 0.02013896219432354\n",
      "Epoch 3828, Loss: 0.16968829836696386, Final Batch Loss: 0.010782766155898571\n",
      "Epoch 3829, Loss: 0.20545120351016521, Final Batch Loss: 0.08505996316671371\n",
      "Epoch 3830, Loss: 0.11901069339364767, Final Batch Loss: 0.012907507829368114\n",
      "Epoch 3831, Loss: 0.14854825288057327, Final Batch Loss: 0.03864523395895958\n",
      "Epoch 3832, Loss: 0.2014945149421692, Final Batch Loss: 0.054076578468084335\n",
      "Epoch 3833, Loss: 0.16569141671061516, Final Batch Loss: 0.03141874819993973\n",
      "Epoch 3834, Loss: 0.1572430543601513, Final Batch Loss: 0.060833707451820374\n",
      "Epoch 3835, Loss: 0.1747955810278654, Final Batch Loss: 0.040212493389844894\n",
      "Epoch 3836, Loss: 0.15029129199683666, Final Batch Loss: 0.028291618451476097\n",
      "Epoch 3837, Loss: 0.17398438788950443, Final Batch Loss: 0.040904682129621506\n",
      "Epoch 3838, Loss: 0.1376980096101761, Final Batch Loss: 0.01832636632025242\n",
      "Epoch 3839, Loss: 0.15173596888780594, Final Batch Loss: 0.02453656494617462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3840, Loss: 0.2033408284187317, Final Batch Loss: 0.041417911648750305\n",
      "Epoch 3841, Loss: 0.1471461933106184, Final Batch Loss: 0.019751811400055885\n",
      "Epoch 3842, Loss: 0.19220292195677757, Final Batch Loss: 0.05293864384293556\n",
      "Epoch 3843, Loss: 0.12236981093883514, Final Batch Loss: 0.013229431584477425\n",
      "Epoch 3844, Loss: 0.11957301385700703, Final Batch Loss: 0.009752634912729263\n",
      "Epoch 3845, Loss: 0.16116194240748882, Final Batch Loss: 0.018970215693116188\n",
      "Epoch 3846, Loss: 0.1530279228463769, Final Batch Loss: 0.015132403932511806\n",
      "Epoch 3847, Loss: 0.17498569749295712, Final Batch Loss: 0.05126720294356346\n",
      "Epoch 3848, Loss: 0.15186812728643417, Final Batch Loss: 0.026670074090361595\n",
      "Epoch 3849, Loss: 0.15628663823008537, Final Batch Loss: 0.03556971624493599\n",
      "Epoch 3850, Loss: 0.144510755315423, Final Batch Loss: 0.025229936465620995\n",
      "Epoch 3851, Loss: 0.20812051370739937, Final Batch Loss: 0.03961891308426857\n",
      "Epoch 3852, Loss: 0.18203707039356232, Final Batch Loss: 0.0319242998957634\n",
      "Epoch 3853, Loss: 0.1897509377449751, Final Batch Loss: 0.06440800428390503\n",
      "Epoch 3854, Loss: 0.23550474271178246, Final Batch Loss: 0.11867021024227142\n",
      "Epoch 3855, Loss: 0.24945675395429134, Final Batch Loss: 0.08295387774705887\n",
      "Epoch 3856, Loss: 0.1436441382393241, Final Batch Loss: 0.009247719310224056\n",
      "Epoch 3857, Loss: 0.24126187339425087, Final Batch Loss: 0.029792655259370804\n",
      "Epoch 3858, Loss: 0.26966702938079834, Final Batch Loss: 0.08447367697954178\n",
      "Epoch 3859, Loss: 0.1222813818603754, Final Batch Loss: 0.01808960549533367\n",
      "Epoch 3860, Loss: 0.20331409573554993, Final Batch Loss: 0.037580154836177826\n",
      "Epoch 3861, Loss: 0.16942463628947735, Final Batch Loss: 0.01850784942507744\n",
      "Epoch 3862, Loss: 0.13557920046150684, Final Batch Loss: 0.029346304014325142\n",
      "Epoch 3863, Loss: 0.12979453802108765, Final Batch Loss: 0.04569803550839424\n",
      "Epoch 3864, Loss: 0.2002637768164277, Final Batch Loss: 0.0125878332182765\n",
      "Epoch 3865, Loss: 0.1537963356822729, Final Batch Loss: 0.05124552547931671\n",
      "Epoch 3866, Loss: 0.20695742731913924, Final Batch Loss: 0.005098426248878241\n",
      "Epoch 3867, Loss: 0.16145117720589042, Final Batch Loss: 0.0060946024022996426\n",
      "Epoch 3868, Loss: 0.19390195794403553, Final Batch Loss: 0.02610175497829914\n",
      "Epoch 3869, Loss: 0.11786659061908722, Final Batch Loss: 0.025244591757655144\n",
      "Epoch 3870, Loss: 0.16991718485951424, Final Batch Loss: 0.064265675842762\n",
      "Epoch 3871, Loss: 0.13790257554501295, Final Batch Loss: 0.015393565408885479\n",
      "Epoch 3872, Loss: 0.142401322722435, Final Batch Loss: 0.017336972057819366\n",
      "Epoch 3873, Loss: 0.20770789124071598, Final Batch Loss: 0.0800812616944313\n",
      "Epoch 3874, Loss: 0.18885906785726547, Final Batch Loss: 0.057748254388570786\n",
      "Epoch 3875, Loss: 0.2258564792573452, Final Batch Loss: 0.05732465162873268\n",
      "Epoch 3876, Loss: 0.1422404833137989, Final Batch Loss: 0.05061442404985428\n",
      "Epoch 3877, Loss: 0.15935270674526691, Final Batch Loss: 0.02539558708667755\n",
      "Epoch 3878, Loss: 0.17923163622617722, Final Batch Loss: 0.04904480278491974\n",
      "Epoch 3879, Loss: 0.12756876647472382, Final Batch Loss: 0.024525919929146767\n",
      "Epoch 3880, Loss: 0.2488064356148243, Final Batch Loss: 0.10301966220140457\n",
      "Epoch 3881, Loss: 0.1895415261387825, Final Batch Loss: 0.048878639936447144\n",
      "Epoch 3882, Loss: 0.25934217870235443, Final Batch Loss: 0.057474441826343536\n",
      "Epoch 3883, Loss: 0.20364071801304817, Final Batch Loss: 0.06612297147512436\n",
      "Epoch 3884, Loss: 0.24757318943738937, Final Batch Loss: 0.060673415660858154\n",
      "Epoch 3885, Loss: 0.178162332624197, Final Batch Loss: 0.03448401391506195\n",
      "Epoch 3886, Loss: 0.22307134978473186, Final Batch Loss: 0.05400862917304039\n",
      "Epoch 3887, Loss: 0.18202609196305275, Final Batch Loss: 0.036579228937625885\n",
      "Epoch 3888, Loss: 0.19218511879444122, Final Batch Loss: 0.020455550402402878\n",
      "Epoch 3889, Loss: 0.2936713248491287, Final Batch Loss: 0.061650946736335754\n",
      "Epoch 3890, Loss: 0.2178804688155651, Final Batch Loss: 0.04070908576250076\n",
      "Epoch 3891, Loss: 0.19495756179094315, Final Batch Loss: 0.03643568605184555\n",
      "Epoch 3892, Loss: 0.2887064106762409, Final Batch Loss: 0.11398763209581375\n",
      "Epoch 3893, Loss: 0.26945796236395836, Final Batch Loss: 0.13090300559997559\n",
      "Epoch 3894, Loss: 0.2576383128762245, Final Batch Loss: 0.07887300848960876\n",
      "Epoch 3895, Loss: 0.3288351818919182, Final Batch Loss: 0.06156217306852341\n",
      "Epoch 3896, Loss: 0.3232288137078285, Final Batch Loss: 0.14713723957538605\n",
      "Epoch 3897, Loss: 0.16210898384451866, Final Batch Loss: 0.03768714889883995\n",
      "Epoch 3898, Loss: 0.24575277417898178, Final Batch Loss: 0.03647284582257271\n",
      "Epoch 3899, Loss: 0.2600097618997097, Final Batch Loss: 0.059042785316705704\n",
      "Epoch 3900, Loss: 0.2251397930085659, Final Batch Loss: 0.06678260862827301\n",
      "Epoch 3901, Loss: 0.23388709500432014, Final Batch Loss: 0.01877884939312935\n",
      "Epoch 3902, Loss: 0.16625018324702978, Final Batch Loss: 0.015249754302203655\n",
      "Epoch 3903, Loss: 0.20934760943055153, Final Batch Loss: 0.08341899514198303\n",
      "Epoch 3904, Loss: 0.18434539809823036, Final Batch Loss: 0.06217445805668831\n",
      "Epoch 3905, Loss: 0.16649667546153069, Final Batch Loss: 0.0354677177965641\n",
      "Epoch 3906, Loss: 0.1573974695056677, Final Batch Loss: 0.024802686646580696\n",
      "Epoch 3907, Loss: 0.1907694675028324, Final Batch Loss: 0.021359119564294815\n",
      "Epoch 3908, Loss: 0.13898437470197678, Final Batch Loss: 0.018347270786762238\n",
      "Epoch 3909, Loss: 0.14567032642662525, Final Batch Loss: 0.014720207080245018\n",
      "Epoch 3910, Loss: 0.22868908382952213, Final Batch Loss: 0.07013362646102905\n",
      "Epoch 3911, Loss: 0.18471918720752, Final Batch Loss: 0.019800448790192604\n",
      "Epoch 3912, Loss: 0.25965865701436996, Final Batch Loss: 0.07541963458061218\n",
      "Epoch 3913, Loss: 0.16200810484588146, Final Batch Loss: 0.02224460057914257\n",
      "Epoch 3914, Loss: 0.2829265929758549, Final Batch Loss: 0.06731733679771423\n",
      "Epoch 3915, Loss: 0.2552375793457031, Final Batch Loss: 0.047647781670093536\n",
      "Epoch 3916, Loss: 0.20745685696601868, Final Batch Loss: 0.02427126094698906\n",
      "Epoch 3917, Loss: 0.18566621281206608, Final Batch Loss: 0.01703575998544693\n",
      "Epoch 3918, Loss: 0.12810641713440418, Final Batch Loss: 0.016472818329930305\n",
      "Epoch 3919, Loss: 0.15273629315197468, Final Batch Loss: 0.013870235532522202\n",
      "Epoch 3920, Loss: 0.25202231481671333, Final Batch Loss: 0.05711708590388298\n",
      "Epoch 3921, Loss: 0.20481888204813004, Final Batch Loss: 0.06468895822763443\n",
      "Epoch 3922, Loss: 0.14834526553750038, Final Batch Loss: 0.04253344610333443\n",
      "Epoch 3923, Loss: 0.1344634396955371, Final Batch Loss: 0.007901466451585293\n",
      "Epoch 3924, Loss: 0.216520257294178, Final Batch Loss: 0.04798725247383118\n",
      "Epoch 3925, Loss: 0.12466567195951939, Final Batch Loss: 0.043930117040872574\n",
      "Epoch 3926, Loss: 0.12790043838322163, Final Batch Loss: 0.023295274004340172\n",
      "Epoch 3927, Loss: 0.2214336171746254, Final Batch Loss: 0.08838744461536407\n",
      "Epoch 3928, Loss: 0.14183658175170422, Final Batch Loss: 0.031144529581069946\n",
      "Epoch 3929, Loss: 0.21147293969988823, Final Batch Loss: 0.09150873124599457\n",
      "Epoch 3930, Loss: 0.2552608586847782, Final Batch Loss: 0.09068973362445831\n",
      "Epoch 3931, Loss: 0.205885861068964, Final Batch Loss: 0.021239954978227615\n",
      "Epoch 3932, Loss: 0.13245131075382233, Final Batch Loss: 0.013215139508247375\n",
      "Epoch 3933, Loss: 0.1398394452407956, Final Batch Loss: 0.049459636211395264\n",
      "Epoch 3934, Loss: 0.1573743112385273, Final Batch Loss: 0.03969515115022659\n",
      "Epoch 3935, Loss: 0.12896485347300768, Final Batch Loss: 0.01272581983357668\n",
      "Epoch 3936, Loss: 0.17645679414272308, Final Batch Loss: 0.009490309283137321\n",
      "Epoch 3937, Loss: 0.13745707273483276, Final Batch Loss: 0.05815684050321579\n",
      "Epoch 3938, Loss: 0.13379053957760334, Final Batch Loss: 0.06848406791687012\n",
      "Epoch 3939, Loss: 0.18649330362677574, Final Batch Loss: 0.03750339522957802\n",
      "Epoch 3940, Loss: 0.2507262174040079, Final Batch Loss: 0.10798437148332596\n",
      "Epoch 3941, Loss: 0.22712425328791142, Final Batch Loss: 0.007238542661070824\n",
      "Epoch 3942, Loss: 0.2847457751631737, Final Batch Loss: 0.06468261033296585\n",
      "Epoch 3943, Loss: 0.19373420998454094, Final Batch Loss: 0.06631230562925339\n",
      "Epoch 3944, Loss: 0.32231203839182854, Final Batch Loss: 0.0886947512626648\n",
      "Epoch 3945, Loss: 0.3622053563594818, Final Batch Loss: 0.10255486518144608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3946, Loss: 0.36702368408441544, Final Batch Loss: 0.07944061607122421\n",
      "Epoch 3947, Loss: 0.2860342413187027, Final Batch Loss: 0.06361466646194458\n",
      "Epoch 3948, Loss: 0.29911205545067787, Final Batch Loss: 0.13112734258174896\n",
      "Epoch 3949, Loss: 0.35989686474204063, Final Batch Loss: 0.1408880352973938\n",
      "Epoch 3950, Loss: 0.44226760417222977, Final Batch Loss: 0.1741921603679657\n",
      "Epoch 3951, Loss: 0.2909775637090206, Final Batch Loss: 0.0713202953338623\n",
      "Epoch 3952, Loss: 0.2052417304366827, Final Batch Loss: 0.024420151486992836\n",
      "Epoch 3953, Loss: 0.27758652716875076, Final Batch Loss: 0.06836254149675369\n",
      "Epoch 3954, Loss: 0.30156275629997253, Final Batch Loss: 0.1314246505498886\n",
      "Epoch 3955, Loss: 0.21566198393702507, Final Batch Loss: 0.05445927754044533\n",
      "Epoch 3956, Loss: 0.2392379380762577, Final Batch Loss: 0.0421266071498394\n",
      "Epoch 3957, Loss: 0.17130428366363049, Final Batch Loss: 0.01887098327279091\n",
      "Epoch 3958, Loss: 0.35808511823415756, Final Batch Loss: 0.12857161462306976\n",
      "Epoch 3959, Loss: 0.24970917589962482, Final Batch Loss: 0.018631981685757637\n",
      "Epoch 3960, Loss: 0.3467365726828575, Final Batch Loss: 0.08087039738893509\n",
      "Epoch 3961, Loss: 0.20307458564639091, Final Batch Loss: 0.05418938398361206\n",
      "Epoch 3962, Loss: 0.35238685831427574, Final Batch Loss: 0.1396445631980896\n",
      "Epoch 3963, Loss: 0.17194918915629387, Final Batch Loss: 0.04161342605948448\n",
      "Epoch 3964, Loss: 0.20449005346745253, Final Batch Loss: 0.007932311855256557\n",
      "Epoch 3965, Loss: 0.1818726509809494, Final Batch Loss: 0.03370913490653038\n",
      "Epoch 3966, Loss: 0.19893247075378895, Final Batch Loss: 0.0167401023209095\n",
      "Epoch 3967, Loss: 0.17868964932858944, Final Batch Loss: 0.04027608409523964\n",
      "Epoch 3968, Loss: 0.15798433125019073, Final Batch Loss: 0.03343438729643822\n",
      "Epoch 3969, Loss: 0.1830136403441429, Final Batch Loss: 0.06425801664590836\n",
      "Epoch 3970, Loss: 0.18526562862098217, Final Batch Loss: 0.061259400099515915\n",
      "Epoch 3971, Loss: 0.14846815913915634, Final Batch Loss: 0.030542148277163506\n",
      "Epoch 3972, Loss: 0.2398339994251728, Final Batch Loss: 0.09205609560012817\n",
      "Epoch 3973, Loss: 0.12092352844774723, Final Batch Loss: 0.008929623290896416\n",
      "Epoch 3974, Loss: 0.19567333534359932, Final Batch Loss: 0.039461810141801834\n",
      "Epoch 3975, Loss: 0.1839413344860077, Final Batch Loss: 0.01989518478512764\n",
      "Epoch 3976, Loss: 0.17877357080578804, Final Batch Loss: 0.026273977011442184\n",
      "Epoch 3977, Loss: 0.24985377490520477, Final Batch Loss: 0.08280064165592194\n",
      "Epoch 3978, Loss: 0.19458398967981339, Final Batch Loss: 0.03244399651885033\n",
      "Epoch 3979, Loss: 0.18057455494999886, Final Batch Loss: 0.03456940874457359\n",
      "Epoch 3980, Loss: 0.16244345344603062, Final Batch Loss: 0.03423620015382767\n",
      "Epoch 3981, Loss: 0.18588586896657944, Final Batch Loss: 0.07320627570152283\n",
      "Epoch 3982, Loss: 0.16266415268182755, Final Batch Loss: 0.02088446170091629\n",
      "Epoch 3983, Loss: 0.3005690537393093, Final Batch Loss: 0.1265649050474167\n",
      "Epoch 3984, Loss: 0.21655354648828506, Final Batch Loss: 0.05034844949841499\n",
      "Epoch 3985, Loss: 0.1513329353183508, Final Batch Loss: 0.03564118966460228\n",
      "Epoch 3986, Loss: 0.21404629573225975, Final Batch Loss: 0.03636375442147255\n",
      "Epoch 3987, Loss: 0.12605933472514153, Final Batch Loss: 0.01639944314956665\n",
      "Epoch 3988, Loss: 0.15751183219254017, Final Batch Loss: 0.015685753896832466\n",
      "Epoch 3989, Loss: 0.1490417718887329, Final Batch Loss: 0.04077565670013428\n",
      "Epoch 3990, Loss: 0.12127343192696571, Final Batch Loss: 0.021684739738702774\n",
      "Epoch 3991, Loss: 0.24608881026506424, Final Batch Loss: 0.050142623484134674\n",
      "Epoch 3992, Loss: 0.16228550765663385, Final Batch Loss: 0.015371381305158138\n",
      "Epoch 3993, Loss: 0.23904889449477196, Final Batch Loss: 0.05425877124071121\n",
      "Epoch 3994, Loss: 0.14576885476708412, Final Batch Loss: 0.030613254755735397\n",
      "Epoch 3995, Loss: 0.10523747839033604, Final Batch Loss: 0.024904632940888405\n",
      "Epoch 3996, Loss: 0.20541783422231674, Final Batch Loss: 0.018603771924972534\n",
      "Epoch 3997, Loss: 0.14728743396699429, Final Batch Loss: 0.014608602970838547\n",
      "Epoch 3998, Loss: 0.1428614743053913, Final Batch Loss: 0.04553396254777908\n",
      "Epoch 3999, Loss: 0.1457953192293644, Final Batch Loss: 0.03241971507668495\n",
      "Epoch 4000, Loss: 0.1562944147735834, Final Batch Loss: 0.01824251003563404\n",
      "Epoch 4001, Loss: 0.22684776037931442, Final Batch Loss: 0.06743980944156647\n",
      "Epoch 4002, Loss: 0.2032558135688305, Final Batch Loss: 0.05353609472513199\n",
      "Epoch 4003, Loss: 0.20133092626929283, Final Batch Loss: 0.062305644154548645\n",
      "Epoch 4004, Loss: 0.18455947190523148, Final Batch Loss: 0.0735912024974823\n",
      "Epoch 4005, Loss: 0.1579559314996004, Final Batch Loss: 0.011334000155329704\n",
      "Epoch 4006, Loss: 0.14453842677176, Final Batch Loss: 0.029198573902249336\n",
      "Epoch 4007, Loss: 0.15829022601246834, Final Batch Loss: 0.04541800916194916\n",
      "Epoch 4008, Loss: 0.1762018366716802, Final Batch Loss: 0.005484537687152624\n",
      "Epoch 4009, Loss: 0.24997007753700018, Final Batch Loss: 0.013566835783421993\n",
      "Epoch 4010, Loss: 0.19150571152567863, Final Batch Loss: 0.0369141548871994\n",
      "Epoch 4011, Loss: 0.22614560276269913, Final Batch Loss: 0.10232800990343094\n",
      "Epoch 4012, Loss: 0.1841189991682768, Final Batch Loss: 0.0696888267993927\n",
      "Epoch 4013, Loss: 0.24074462056159973, Final Batch Loss: 0.059460610151290894\n",
      "Epoch 4014, Loss: 0.23415754362940788, Final Batch Loss: 0.07309425622224808\n",
      "Epoch 4015, Loss: 0.19622808508574963, Final Batch Loss: 0.05680335685610771\n",
      "Epoch 4016, Loss: 0.09241535235196352, Final Batch Loss: 0.01885295659303665\n",
      "Epoch 4017, Loss: 0.18433447182178497, Final Batch Loss: 0.06316611170768738\n",
      "Epoch 4018, Loss: 0.2043197974562645, Final Batch Loss: 0.06285638362169266\n",
      "Epoch 4019, Loss: 0.12333088740706444, Final Batch Loss: 0.04243996739387512\n",
      "Epoch 4020, Loss: 0.1436285600066185, Final Batch Loss: 0.01463283970952034\n",
      "Epoch 4021, Loss: 0.1749192401766777, Final Batch Loss: 0.01602575182914734\n",
      "Epoch 4022, Loss: 0.18728965148329735, Final Batch Loss: 0.036226727068424225\n",
      "Epoch 4023, Loss: 0.16012767888605595, Final Batch Loss: 0.052542850375175476\n",
      "Epoch 4024, Loss: 0.20469440147280693, Final Batch Loss: 0.03935592994093895\n",
      "Epoch 4025, Loss: 0.1493228916078806, Final Batch Loss: 0.010959804989397526\n",
      "Epoch 4026, Loss: 0.132011360488832, Final Batch Loss: 0.011789219453930855\n",
      "Epoch 4027, Loss: 0.11573975160717964, Final Batch Loss: 0.013053636997938156\n",
      "Epoch 4028, Loss: 0.19069548323750496, Final Batch Loss: 0.09215310961008072\n",
      "Epoch 4029, Loss: 0.1307178344577551, Final Batch Loss: 0.013886768370866776\n",
      "Epoch 4030, Loss: 0.18344992399215698, Final Batch Loss: 0.0552397184073925\n",
      "Epoch 4031, Loss: 0.19714997708797455, Final Batch Loss: 0.04454552382230759\n",
      "Epoch 4032, Loss: 0.1700190268456936, Final Batch Loss: 0.054587893187999725\n",
      "Epoch 4033, Loss: 0.16927623562514782, Final Batch Loss: 0.09179360419511795\n",
      "Epoch 4034, Loss: 0.16329646483063698, Final Batch Loss: 0.056393709033727646\n",
      "Epoch 4035, Loss: 0.16018474660813808, Final Batch Loss: 0.049741264432668686\n",
      "Epoch 4036, Loss: 0.22211753204464912, Final Batch Loss: 0.05220648646354675\n",
      "Epoch 4037, Loss: 0.19561286643147469, Final Batch Loss: 0.05768445134162903\n",
      "Epoch 4038, Loss: 0.14016524329781532, Final Batch Loss: 0.02337409183382988\n",
      "Epoch 4039, Loss: 0.17013263422995806, Final Batch Loss: 0.011720131151378155\n",
      "Epoch 4040, Loss: 0.15025958232581615, Final Batch Loss: 0.02513688988983631\n",
      "Epoch 4041, Loss: 0.12806540541350842, Final Batch Loss: 0.008873142302036285\n",
      "Epoch 4042, Loss: 0.1720038577914238, Final Batch Loss: 0.043555114418268204\n",
      "Epoch 4043, Loss: 0.1517448090016842, Final Batch Loss: 0.07765577733516693\n",
      "Epoch 4044, Loss: 0.11012641713023186, Final Batch Loss: 0.016643155366182327\n",
      "Epoch 4045, Loss: 0.20827863179147243, Final Batch Loss: 0.07964354753494263\n",
      "Epoch 4046, Loss: 0.15053788293153048, Final Batch Loss: 0.009922872297465801\n",
      "Epoch 4047, Loss: 0.28559689223766327, Final Batch Loss: 0.16126950085163116\n",
      "Epoch 4048, Loss: 0.25485266372561455, Final Batch Loss: 0.1373313069343567\n",
      "Epoch 4049, Loss: 0.19721600785851479, Final Batch Loss: 0.057676706463098526\n",
      "Epoch 4050, Loss: 0.18952734768390656, Final Batch Loss: 0.02933346852660179\n",
      "Epoch 4051, Loss: 0.14067193027585745, Final Batch Loss: 0.03368881344795227\n",
      "Epoch 4052, Loss: 0.213465278968215, Final Batch Loss: 0.06654267758131027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4053, Loss: 0.11877783946692944, Final Batch Loss: 0.03870308771729469\n",
      "Epoch 4054, Loss: 0.17440220341086388, Final Batch Loss: 0.063575878739357\n",
      "Epoch 4055, Loss: 0.21496820822358131, Final Batch Loss: 0.10572224855422974\n",
      "Epoch 4056, Loss: 0.17132054269313812, Final Batch Loss: 0.023466065526008606\n",
      "Epoch 4057, Loss: 0.14559496007859707, Final Batch Loss: 0.016012592241168022\n",
      "Epoch 4058, Loss: 0.1590859591960907, Final Batch Loss: 0.037999603897333145\n",
      "Epoch 4059, Loss: 0.21417617611587048, Final Batch Loss: 0.07500816136598587\n",
      "Epoch 4060, Loss: 0.16727631073445082, Final Batch Loss: 0.06842023134231567\n",
      "Epoch 4061, Loss: 0.19224416464567184, Final Batch Loss: 0.0654744803905487\n",
      "Epoch 4062, Loss: 0.19219592958688736, Final Batch Loss: 0.07353872060775757\n",
      "Epoch 4063, Loss: 0.18231667578220367, Final Batch Loss: 0.021728936582803726\n",
      "Epoch 4064, Loss: 0.1282636821269989, Final Batch Loss: 0.018021658062934875\n",
      "Epoch 4065, Loss: 0.23667126893997192, Final Batch Loss: 0.07426787167787552\n",
      "Epoch 4066, Loss: 0.17290053516626358, Final Batch Loss: 0.05266134440898895\n",
      "Epoch 4067, Loss: 0.1673118006438017, Final Batch Loss: 0.02158869244158268\n",
      "Epoch 4068, Loss: 0.24464140459895134, Final Batch Loss: 0.09032705426216125\n",
      "Epoch 4069, Loss: 0.18204776383936405, Final Batch Loss: 0.04096555337309837\n",
      "Epoch 4070, Loss: 0.1921494361013174, Final Batch Loss: 0.0839526578783989\n",
      "Epoch 4071, Loss: 0.22402092814445496, Final Batch Loss: 0.07914552837610245\n",
      "Epoch 4072, Loss: 0.1845079232007265, Final Batch Loss: 0.0711718425154686\n",
      "Epoch 4073, Loss: 0.13849573768675327, Final Batch Loss: 0.03571281209588051\n",
      "Epoch 4074, Loss: 0.21896249055862427, Final Batch Loss: 0.06988693028688431\n",
      "Epoch 4075, Loss: 0.18052652664482594, Final Batch Loss: 0.06632481515407562\n",
      "Epoch 4076, Loss: 0.10548354312777519, Final Batch Loss: 0.012655550613999367\n",
      "Epoch 4077, Loss: 0.22904962673783302, Final Batch Loss: 0.038016315549612045\n",
      "Epoch 4078, Loss: 0.21336860954761505, Final Batch Loss: 0.050772953778505325\n",
      "Epoch 4079, Loss: 0.13770325854420662, Final Batch Loss: 0.030411507934331894\n",
      "Epoch 4080, Loss: 0.12920107506215572, Final Batch Loss: 0.019332027062773705\n",
      "Epoch 4081, Loss: 0.1744242925196886, Final Batch Loss: 0.040675435215234756\n",
      "Epoch 4082, Loss: 0.2256140522658825, Final Batch Loss: 0.040438346564769745\n",
      "Epoch 4083, Loss: 0.16482631489634514, Final Batch Loss: 0.054985519498586655\n",
      "Epoch 4084, Loss: 0.1148190489038825, Final Batch Loss: 0.01391144935041666\n",
      "Epoch 4085, Loss: 0.1225791797041893, Final Batch Loss: 0.02216172404587269\n",
      "Epoch 4086, Loss: 0.11167073994874954, Final Batch Loss: 0.019581163302063942\n",
      "Epoch 4087, Loss: 0.10958653129637241, Final Batch Loss: 0.021120646968483925\n",
      "Epoch 4088, Loss: 0.12398143112659454, Final Batch Loss: 0.04185127839446068\n",
      "Epoch 4089, Loss: 0.1749941147863865, Final Batch Loss: 0.0400615930557251\n",
      "Epoch 4090, Loss: 0.17631946504116058, Final Batch Loss: 0.03134783357381821\n",
      "Epoch 4091, Loss: 0.11043204693123698, Final Batch Loss: 0.0073854695074260235\n",
      "Epoch 4092, Loss: 0.2003389149904251, Final Batch Loss: 0.11329222470521927\n",
      "Epoch 4093, Loss: 0.15457754582166672, Final Batch Loss: 0.02665127068758011\n",
      "Epoch 4094, Loss: 0.20254147797822952, Final Batch Loss: 0.04362347722053528\n",
      "Epoch 4095, Loss: 0.17567892745137215, Final Batch Loss: 0.05867147445678711\n",
      "Epoch 4096, Loss: 0.21082378923892975, Final Batch Loss: 0.023839764297008514\n",
      "Epoch 4097, Loss: 0.21662676334381104, Final Batch Loss: 0.04304414615035057\n",
      "Epoch 4098, Loss: 0.2106879986822605, Final Batch Loss: 0.030672751367092133\n",
      "Epoch 4099, Loss: 0.16665616817772388, Final Batch Loss: 0.031709618866443634\n",
      "Epoch 4100, Loss: 0.2176330778747797, Final Batch Loss: 0.01875450275838375\n",
      "Epoch 4101, Loss: 0.18772687390446663, Final Batch Loss: 0.008184988051652908\n",
      "Epoch 4102, Loss: 0.1748772319406271, Final Batch Loss: 0.05776374414563179\n",
      "Epoch 4103, Loss: 0.20788379944860935, Final Batch Loss: 0.06833349168300629\n",
      "Epoch 4104, Loss: 0.2205382101237774, Final Batch Loss: 0.08924856036901474\n",
      "Epoch 4105, Loss: 0.22911038249731064, Final Batch Loss: 0.07175768166780472\n",
      "Epoch 4106, Loss: 0.1412595696747303, Final Batch Loss: 0.011193031445145607\n",
      "Epoch 4107, Loss: 0.1420423649251461, Final Batch Loss: 0.040953587740659714\n",
      "Epoch 4108, Loss: 0.21538174524903297, Final Batch Loss: 0.048007477074861526\n",
      "Epoch 4109, Loss: 0.17612536065280437, Final Batch Loss: 0.05841713771224022\n",
      "Epoch 4110, Loss: 0.17258150689303875, Final Batch Loss: 0.042258769273757935\n",
      "Epoch 4111, Loss: 0.281247153878212, Final Batch Loss: 0.08308277279138565\n",
      "Epoch 4112, Loss: 0.23541811667382717, Final Batch Loss: 0.09052613377571106\n",
      "Epoch 4113, Loss: 0.13591921888291836, Final Batch Loss: 0.01914031244814396\n",
      "Epoch 4114, Loss: 0.18024501483887434, Final Batch Loss: 0.00919716339558363\n",
      "Epoch 4115, Loss: 0.23084210231900215, Final Batch Loss: 0.04006548598408699\n",
      "Epoch 4116, Loss: 0.3002987168729305, Final Batch Loss: 0.04747679829597473\n",
      "Epoch 4117, Loss: 0.18873995542526245, Final Batch Loss: 0.035532303154468536\n",
      "Epoch 4118, Loss: 0.169668260961771, Final Batch Loss: 0.0638716071844101\n",
      "Epoch 4119, Loss: 0.17414437234401703, Final Batch Loss: 0.024448787793517113\n",
      "Epoch 4120, Loss: 0.2743562888354063, Final Batch Loss: 0.023451058194041252\n",
      "Epoch 4121, Loss: 0.14830855280160904, Final Batch Loss: 0.05658556520938873\n",
      "Epoch 4122, Loss: 0.12472396343946457, Final Batch Loss: 0.009168142452836037\n",
      "Epoch 4123, Loss: 0.16348830424249172, Final Batch Loss: 0.02873443253338337\n",
      "Epoch 4124, Loss: 0.15771207492798567, Final Batch Loss: 0.05131449177861214\n",
      "Epoch 4125, Loss: 0.29085443913936615, Final Batch Loss: 0.06959693878889084\n",
      "Epoch 4126, Loss: 0.17347414698451757, Final Batch Loss: 0.013006526045501232\n",
      "Epoch 4127, Loss: 0.13489427603781223, Final Batch Loss: 0.012752754613757133\n",
      "Epoch 4128, Loss: 0.1543787606060505, Final Batch Loss: 0.02696320042014122\n",
      "Epoch 4129, Loss: 0.16580522805452347, Final Batch Loss: 0.027591556310653687\n",
      "Epoch 4130, Loss: 0.18413493409752846, Final Batch Loss: 0.018606357276439667\n",
      "Epoch 4131, Loss: 0.17655704356729984, Final Batch Loss: 0.06459750980138779\n",
      "Epoch 4132, Loss: 0.21740521863102913, Final Batch Loss: 0.013694413006305695\n",
      "Epoch 4133, Loss: 0.23094087839126587, Final Batch Loss: 0.07117226719856262\n",
      "Epoch 4134, Loss: 0.1748216636478901, Final Batch Loss: 0.0464153066277504\n",
      "Epoch 4135, Loss: 0.21395613998174667, Final Batch Loss: 0.04510660469532013\n",
      "Epoch 4136, Loss: 0.18740089982748032, Final Batch Loss: 0.06071409955620766\n",
      "Epoch 4137, Loss: 0.13588670827448368, Final Batch Loss: 0.01984539069235325\n",
      "Epoch 4138, Loss: 0.2110789231956005, Final Batch Loss: 0.021174903959035873\n",
      "Epoch 4139, Loss: 0.17434010282158852, Final Batch Loss: 0.057801418006420135\n",
      "Epoch 4140, Loss: 0.15580651443451643, Final Batch Loss: 0.010525788180530071\n",
      "Epoch 4141, Loss: 0.17436936311423779, Final Batch Loss: 0.06458935141563416\n",
      "Epoch 4142, Loss: 0.13527310825884342, Final Batch Loss: 0.041265759617090225\n",
      "Epoch 4143, Loss: 0.15140456333756447, Final Batch Loss: 0.028356611728668213\n",
      "Epoch 4144, Loss: 0.1762215569615364, Final Batch Loss: 0.0400802344083786\n",
      "Epoch 4145, Loss: 0.11852213740348816, Final Batch Loss: 0.02455679327249527\n",
      "Epoch 4146, Loss: 0.15885362774133682, Final Batch Loss: 0.04511784389615059\n",
      "Epoch 4147, Loss: 0.1454753428697586, Final Batch Loss: 0.03669358417391777\n",
      "Epoch 4148, Loss: 0.16351479291915894, Final Batch Loss: 0.020491667091846466\n",
      "Epoch 4149, Loss: 0.12001568265259266, Final Batch Loss: 0.03684700280427933\n",
      "Epoch 4150, Loss: 0.131626115180552, Final Batch Loss: 0.020166300237178802\n",
      "Epoch 4151, Loss: 0.12608122639358044, Final Batch Loss: 0.030621495097875595\n",
      "Epoch 4152, Loss: 0.20563719421625137, Final Batch Loss: 0.05487101897597313\n",
      "Epoch 4153, Loss: 0.1325404830276966, Final Batch Loss: 0.016140401363372803\n",
      "Epoch 4154, Loss: 0.19234004244208336, Final Batch Loss: 0.04835078865289688\n",
      "Epoch 4155, Loss: 0.16406949423253536, Final Batch Loss: 0.028948770835995674\n",
      "Epoch 4156, Loss: 0.160472659394145, Final Batch Loss: 0.016539793461561203\n",
      "Epoch 4157, Loss: 0.22895167209208012, Final Batch Loss: 0.0808391124010086\n",
      "Epoch 4158, Loss: 0.12882182467728853, Final Batch Loss: 0.02747546136379242\n",
      "Epoch 4159, Loss: 0.2228492982685566, Final Batch Loss: 0.04731607064604759\n",
      "Epoch 4160, Loss: 0.14380758814513683, Final Batch Loss: 0.037493277341127396\n",
      "Epoch 4161, Loss: 0.1229560524225235, Final Batch Loss: 0.01649741642177105\n",
      "Epoch 4162, Loss: 0.24451440759003162, Final Batch Loss: 0.11626260727643967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4163, Loss: 0.13802834413945675, Final Batch Loss: 0.019788993522524834\n",
      "Epoch 4164, Loss: 0.16564059862866998, Final Batch Loss: 0.006206689868122339\n",
      "Epoch 4165, Loss: 0.16854117158800364, Final Batch Loss: 0.03840632364153862\n",
      "Epoch 4166, Loss: 0.09479997307062149, Final Batch Loss: 0.016770698130130768\n",
      "Epoch 4167, Loss: 0.12119285203516483, Final Batch Loss: 0.02742833085358143\n",
      "Epoch 4168, Loss: 0.21684416756033897, Final Batch Loss: 0.024593664333224297\n",
      "Epoch 4169, Loss: 0.14435327425599098, Final Batch Loss: 0.019111517816781998\n",
      "Epoch 4170, Loss: 0.1637314073741436, Final Batch Loss: 0.05009940266609192\n",
      "Epoch 4171, Loss: 0.1573234722018242, Final Batch Loss: 0.04790617525577545\n",
      "Epoch 4172, Loss: 0.25059837475419044, Final Batch Loss: 0.12138475477695465\n",
      "Epoch 4173, Loss: 0.22386090271174908, Final Batch Loss: 0.08769752830266953\n",
      "Epoch 4174, Loss: 0.18794414401054382, Final Batch Loss: 0.08815144747495651\n",
      "Epoch 4175, Loss: 0.2084050327539444, Final Batch Loss: 0.04340404272079468\n",
      "Epoch 4176, Loss: 0.20288177393376827, Final Batch Loss: 0.024865107610821724\n",
      "Epoch 4177, Loss: 0.212662517093122, Final Batch Loss: 0.015435985289514065\n",
      "Epoch 4178, Loss: 0.2399132065474987, Final Batch Loss: 0.0855562835931778\n",
      "Epoch 4179, Loss: 0.16682890616357327, Final Batch Loss: 0.02448202669620514\n",
      "Epoch 4180, Loss: 0.1543540619313717, Final Batch Loss: 0.01467403769493103\n",
      "Epoch 4181, Loss: 0.11858028639107943, Final Batch Loss: 0.010449753142893314\n",
      "Epoch 4182, Loss: 0.22422892227768898, Final Batch Loss: 0.07963672280311584\n",
      "Epoch 4183, Loss: 0.19195489771664143, Final Batch Loss: 0.10197322070598602\n",
      "Epoch 4184, Loss: 0.18175992742180824, Final Batch Loss: 0.06193181872367859\n",
      "Epoch 4185, Loss: 0.1477048061788082, Final Batch Loss: 0.02960379049181938\n",
      "Epoch 4186, Loss: 0.1808202713727951, Final Batch Loss: 0.04118654876947403\n",
      "Epoch 4187, Loss: 0.18107737507671118, Final Batch Loss: 0.0058873360976576805\n",
      "Epoch 4188, Loss: 0.19789362512528896, Final Batch Loss: 0.030360383912920952\n",
      "Epoch 4189, Loss: 0.18615127354860306, Final Batch Loss: 0.03953161463141441\n",
      "Epoch 4190, Loss: 0.24401351436972618, Final Batch Loss: 0.06869500130414963\n",
      "Epoch 4191, Loss: 0.15896091051399708, Final Batch Loss: 0.06607405841350555\n",
      "Epoch 4192, Loss: 0.18311353027820587, Final Batch Loss: 0.04809486120939255\n",
      "Epoch 4193, Loss: 0.2180873742327094, Final Batch Loss: 0.10775816440582275\n",
      "Epoch 4194, Loss: 0.14618200901895761, Final Batch Loss: 0.027265463024377823\n",
      "Epoch 4195, Loss: 0.23793964460492134, Final Batch Loss: 0.08346577733755112\n",
      "Epoch 4196, Loss: 0.14038556814193726, Final Batch Loss: 0.025393055751919746\n",
      "Epoch 4197, Loss: 0.20263539999723434, Final Batch Loss: 0.034617677330970764\n",
      "Epoch 4198, Loss: 0.17048918642103672, Final Batch Loss: 0.021256284788250923\n",
      "Epoch 4199, Loss: 0.17735720612108707, Final Batch Loss: 0.07576563209295273\n",
      "Epoch 4200, Loss: 0.21265058405697346, Final Batch Loss: 0.08667392283678055\n",
      "Epoch 4201, Loss: 0.17589434422552586, Final Batch Loss: 0.00809362344443798\n",
      "Epoch 4202, Loss: 0.10656676907092333, Final Batch Loss: 0.015302165411412716\n",
      "Epoch 4203, Loss: 0.2240640390664339, Final Batch Loss: 0.027693582698702812\n",
      "Epoch 4204, Loss: 0.20064950361847878, Final Batch Loss: 0.04528753086924553\n",
      "Epoch 4205, Loss: 0.2533223479986191, Final Batch Loss: 0.08831973373889923\n",
      "Epoch 4206, Loss: 0.18657522089779377, Final Batch Loss: 0.027087191119790077\n",
      "Epoch 4207, Loss: 0.14196881838142872, Final Batch Loss: 0.05157599598169327\n",
      "Epoch 4208, Loss: 0.21773573756217957, Final Batch Loss: 0.07997143268585205\n",
      "Epoch 4209, Loss: 0.13811160251498222, Final Batch Loss: 0.019754549488425255\n",
      "Epoch 4210, Loss: 0.2876591943204403, Final Batch Loss: 0.09465157985687256\n",
      "Epoch 4211, Loss: 0.19322766363620758, Final Batch Loss: 0.023510541766881943\n",
      "Epoch 4212, Loss: 0.18747607804834843, Final Batch Loss: 0.02651493065059185\n",
      "Epoch 4213, Loss: 0.29083746299147606, Final Batch Loss: 0.09004528075456619\n",
      "Epoch 4214, Loss: 0.3625759780406952, Final Batch Loss: 0.08368109911680222\n",
      "Epoch 4215, Loss: 0.21114546619355679, Final Batch Loss: 0.02969546429812908\n",
      "Epoch 4216, Loss: 0.25860894098877907, Final Batch Loss: 0.07556355744600296\n",
      "Epoch 4217, Loss: 0.23565728031098843, Final Batch Loss: 0.06391829997301102\n",
      "Epoch 4218, Loss: 0.19984672404825687, Final Batch Loss: 0.032675374299287796\n",
      "Epoch 4219, Loss: 0.19820579886436462, Final Batch Loss: 0.020394861698150635\n",
      "Epoch 4220, Loss: 0.18306446447968483, Final Batch Loss: 0.03046991303563118\n",
      "Epoch 4221, Loss: 0.17617027461528778, Final Batch Loss: 0.08445142209529877\n",
      "Epoch 4222, Loss: 0.14938485249876976, Final Batch Loss: 0.040540847927331924\n",
      "Epoch 4223, Loss: 0.1817277166992426, Final Batch Loss: 0.07133104652166367\n",
      "Epoch 4224, Loss: 0.20566654577851295, Final Batch Loss: 0.03616926074028015\n",
      "Epoch 4225, Loss: 0.11251646466553211, Final Batch Loss: 0.027416428551077843\n",
      "Epoch 4226, Loss: 0.11175461672246456, Final Batch Loss: 0.015837252140045166\n",
      "Epoch 4227, Loss: 0.16802182793617249, Final Batch Loss: 0.012500457465648651\n",
      "Epoch 4228, Loss: 0.17436669021844864, Final Batch Loss: 0.01834069937467575\n",
      "Epoch 4229, Loss: 0.19052435085177422, Final Batch Loss: 0.05060087889432907\n",
      "Epoch 4230, Loss: 0.19471077993512154, Final Batch Loss: 0.03064487874507904\n",
      "Epoch 4231, Loss: 0.20158478058874607, Final Batch Loss: 0.022638602182269096\n",
      "Epoch 4232, Loss: 0.16232809238135815, Final Batch Loss: 0.06540292501449585\n",
      "Epoch 4233, Loss: 0.1255834884941578, Final Batch Loss: 0.0403776653110981\n",
      "Epoch 4234, Loss: 0.19142169132828712, Final Batch Loss: 0.04515966400504112\n",
      "Epoch 4235, Loss: 0.16745013184845448, Final Batch Loss: 0.043250273913145065\n",
      "Epoch 4236, Loss: 0.12908651679754257, Final Batch Loss: 0.009350484237074852\n",
      "Epoch 4237, Loss: 0.1160757802426815, Final Batch Loss: 0.012702375650405884\n",
      "Epoch 4238, Loss: 0.1802174299955368, Final Batch Loss: 0.03501342609524727\n",
      "Epoch 4239, Loss: 0.22322319075465202, Final Batch Loss: 0.05796097591519356\n",
      "Epoch 4240, Loss: 0.1553042596206069, Final Batch Loss: 0.010765991173684597\n",
      "Epoch 4241, Loss: 0.13086857832968235, Final Batch Loss: 0.024692943319678307\n",
      "Epoch 4242, Loss: 0.2104952707886696, Final Batch Loss: 0.017147604376077652\n",
      "Epoch 4243, Loss: 0.11230586050078273, Final Batch Loss: 0.007044877391308546\n",
      "Epoch 4244, Loss: 0.14753244817256927, Final Batch Loss: 0.05883483216166496\n",
      "Epoch 4245, Loss: 0.11996820382773876, Final Batch Loss: 0.01657690852880478\n",
      "Epoch 4246, Loss: 0.11429102346301079, Final Batch Loss: 0.0275008175522089\n",
      "Epoch 4247, Loss: 0.13190564885735512, Final Batch Loss: 0.02182166464626789\n",
      "Epoch 4248, Loss: 0.15777171030640602, Final Batch Loss: 0.06949552893638611\n",
      "Epoch 4249, Loss: 0.11126668658107519, Final Batch Loss: 0.022602038457989693\n",
      "Epoch 4250, Loss: 0.13194722961634398, Final Batch Loss: 0.021852683275938034\n",
      "Epoch 4251, Loss: 0.12305491557344794, Final Batch Loss: 0.0039564198814332485\n",
      "Epoch 4252, Loss: 0.15726739168167114, Final Batch Loss: 0.019747665151953697\n",
      "Epoch 4253, Loss: 0.1636220347136259, Final Batch Loss: 0.04307924956083298\n",
      "Epoch 4254, Loss: 0.15866960026323795, Final Batch Loss: 0.02687309868633747\n",
      "Epoch 4255, Loss: 0.20555781945586205, Final Batch Loss: 0.04061156511306763\n",
      "Epoch 4256, Loss: 0.18039040453732014, Final Batch Loss: 0.05543292313814163\n",
      "Epoch 4257, Loss: 0.18050673976540565, Final Batch Loss: 0.06644371896982193\n",
      "Epoch 4258, Loss: 0.17335232719779015, Final Batch Loss: 0.06157049164175987\n",
      "Epoch 4259, Loss: 0.1293738540261984, Final Batch Loss: 0.026367031037807465\n",
      "Epoch 4260, Loss: 0.16590265929698944, Final Batch Loss: 0.03511381521821022\n",
      "Epoch 4261, Loss: 0.12705395184457302, Final Batch Loss: 0.03550364077091217\n",
      "Epoch 4262, Loss: 0.17330074682831764, Final Batch Loss: 0.04723252356052399\n",
      "Epoch 4263, Loss: 0.12625995185226202, Final Batch Loss: 0.015101836062967777\n",
      "Epoch 4264, Loss: 0.15043815225362778, Final Batch Loss: 0.023944074288010597\n",
      "Epoch 4265, Loss: 0.23152538016438484, Final Batch Loss: 0.08021744340658188\n",
      "Epoch 4266, Loss: 0.1731370985507965, Final Batch Loss: 0.042330123484134674\n",
      "Epoch 4267, Loss: 0.1208022478967905, Final Batch Loss: 0.006039317697286606\n",
      "Epoch 4268, Loss: 0.15946077927947044, Final Batch Loss: 0.0224659014493227\n",
      "Epoch 4269, Loss: 0.14710458647459745, Final Batch Loss: 0.013302252627909184\n",
      "Epoch 4270, Loss: 0.2362208366394043, Final Batch Loss: 0.06130590662360191\n",
      "Epoch 4271, Loss: 0.17521828413009644, Final Batch Loss: 0.03223591670393944\n",
      "Epoch 4272, Loss: 0.297883290797472, Final Batch Loss: 0.05937277153134346\n",
      "Epoch 4273, Loss: 0.2031957358121872, Final Batch Loss: 0.03637617081403732\n",
      "Epoch 4274, Loss: 0.1760403737425804, Final Batch Loss: 0.024738654494285583\n",
      "Epoch 4275, Loss: 0.20940371043980122, Final Batch Loss: 0.007294273003935814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4276, Loss: 0.23239649832248688, Final Batch Loss: 0.04533441737294197\n",
      "Epoch 4277, Loss: 0.17959917150437832, Final Batch Loss: 0.07457072287797928\n",
      "Epoch 4278, Loss: 0.13635686412453651, Final Batch Loss: 0.029564978554844856\n",
      "Epoch 4279, Loss: 0.18146887607872486, Final Batch Loss: 0.0267824437469244\n",
      "Epoch 4280, Loss: 0.15300634363666177, Final Batch Loss: 0.006065709982067347\n",
      "Epoch 4281, Loss: 0.1981863807886839, Final Batch Loss: 0.08289148658514023\n",
      "Epoch 4282, Loss: 0.13906610198318958, Final Batch Loss: 0.024106258526444435\n",
      "Epoch 4283, Loss: 0.14510589372366667, Final Batch Loss: 0.011040582321584225\n",
      "Epoch 4284, Loss: 0.13802788592875004, Final Batch Loss: 0.042878154665231705\n",
      "Epoch 4285, Loss: 0.17138725891709328, Final Batch Loss: 0.023055927827954292\n",
      "Epoch 4286, Loss: 0.17275474593043327, Final Batch Loss: 0.028494903817772865\n",
      "Epoch 4287, Loss: 0.1825716868042946, Final Batch Loss: 0.02135055512189865\n",
      "Epoch 4288, Loss: 0.2313910908997059, Final Batch Loss: 0.060117900371551514\n",
      "Epoch 4289, Loss: 0.2644702773541212, Final Batch Loss: 0.13156402111053467\n",
      "Epoch 4290, Loss: 0.24678083881735802, Final Batch Loss: 0.08108094334602356\n",
      "Epoch 4291, Loss: 0.14333559572696686, Final Batch Loss: 0.030139852315187454\n",
      "Epoch 4292, Loss: 0.17929130792617798, Final Batch Loss: 0.05574150010943413\n",
      "Epoch 4293, Loss: 0.11094947718083858, Final Batch Loss: 0.02452274225652218\n",
      "Epoch 4294, Loss: 0.18101445585489273, Final Batch Loss: 0.04401111602783203\n",
      "Epoch 4295, Loss: 0.14523141458630562, Final Batch Loss: 0.024845877662301064\n",
      "Epoch 4296, Loss: 0.1513770204037428, Final Batch Loss: 0.03289110213518143\n",
      "Epoch 4297, Loss: 0.11818903218954802, Final Batch Loss: 0.014319014735519886\n",
      "Epoch 4298, Loss: 0.07623689668253064, Final Batch Loss: 0.007575544063001871\n",
      "Epoch 4299, Loss: 0.11598881892859936, Final Batch Loss: 0.03907216712832451\n",
      "Epoch 4300, Loss: 0.16564257675781846, Final Batch Loss: 0.005760496947914362\n",
      "Epoch 4301, Loss: 0.16225412674248219, Final Batch Loss: 0.020190827548503876\n",
      "Epoch 4302, Loss: 0.13778489641845226, Final Batch Loss: 0.013642562553286552\n",
      "Epoch 4303, Loss: 0.11366735957562923, Final Batch Loss: 0.031642161309719086\n",
      "Epoch 4304, Loss: 0.17689394392073154, Final Batch Loss: 0.023395033553242683\n",
      "Epoch 4305, Loss: 0.1378344912081957, Final Batch Loss: 0.021840205416083336\n",
      "Epoch 4306, Loss: 0.17480752058327198, Final Batch Loss: 0.0452660508453846\n",
      "Epoch 4307, Loss: 0.14823588728904724, Final Batch Loss: 0.05142907425761223\n",
      "Epoch 4308, Loss: 0.16467559151351452, Final Batch Loss: 0.02976284921169281\n",
      "Epoch 4309, Loss: 0.17701473459601402, Final Batch Loss: 0.041837915778160095\n",
      "Epoch 4310, Loss: 0.13955357857048512, Final Batch Loss: 0.055435795336961746\n",
      "Epoch 4311, Loss: 0.11439318815246224, Final Batch Loss: 0.0030710422433912754\n",
      "Epoch 4312, Loss: 0.12269694544374943, Final Batch Loss: 0.02143983542919159\n",
      "Epoch 4313, Loss: 0.12113144993782043, Final Batch Loss: 0.027844635769724846\n",
      "Epoch 4314, Loss: 0.120576661080122, Final Batch Loss: 0.01648475043475628\n",
      "Epoch 4315, Loss: 0.14887136965990067, Final Batch Loss: 0.048476442694664\n",
      "Epoch 4316, Loss: 0.08041802607476711, Final Batch Loss: 0.025741998106241226\n",
      "Epoch 4317, Loss: 0.3091875985264778, Final Batch Loss: 0.143544539809227\n",
      "Epoch 4318, Loss: 0.11420834343880415, Final Batch Loss: 0.02457750216126442\n",
      "Epoch 4319, Loss: 0.14842372946441174, Final Batch Loss: 0.03542158752679825\n",
      "Epoch 4320, Loss: 0.08441956620663404, Final Batch Loss: 0.010755813680589199\n",
      "Epoch 4321, Loss: 0.1279875487089157, Final Batch Loss: 0.035204313695430756\n",
      "Epoch 4322, Loss: 0.23274641670286655, Final Batch Loss: 0.13345879316329956\n",
      "Epoch 4323, Loss: 0.10853399708867073, Final Batch Loss: 0.02362537384033203\n",
      "Epoch 4324, Loss: 0.10337774502113461, Final Batch Loss: 0.006331319455057383\n",
      "Epoch 4325, Loss: 0.16613502707332373, Final Batch Loss: 0.03301958367228508\n",
      "Epoch 4326, Loss: 0.15429330244660378, Final Batch Loss: 0.0235110055655241\n",
      "Epoch 4327, Loss: 0.1158426683396101, Final Batch Loss: 0.02138049714267254\n",
      "Epoch 4328, Loss: 0.14944356493651867, Final Batch Loss: 0.02725350670516491\n",
      "Epoch 4329, Loss: 0.15511882491409779, Final Batch Loss: 0.018340328708291054\n",
      "Epoch 4330, Loss: 0.18302416056394577, Final Batch Loss: 0.045004718005657196\n",
      "Epoch 4331, Loss: 0.20508821681141853, Final Batch Loss: 0.02031945437192917\n",
      "Epoch 4332, Loss: 0.1600932888686657, Final Batch Loss: 0.019050171598792076\n",
      "Epoch 4333, Loss: 0.11431553959846497, Final Batch Loss: 0.0296053234487772\n",
      "Epoch 4334, Loss: 0.12702659890055656, Final Batch Loss: 0.013407381251454353\n",
      "Epoch 4335, Loss: 0.2258107177913189, Final Batch Loss: 0.07182261347770691\n",
      "Epoch 4336, Loss: 0.09482668153941631, Final Batch Loss: 0.024814536795020103\n",
      "Epoch 4337, Loss: 0.09619524562731385, Final Batch Loss: 0.004366180393844843\n",
      "Epoch 4338, Loss: 0.2090478502213955, Final Batch Loss: 0.03397125378251076\n",
      "Epoch 4339, Loss: 0.16238310001790524, Final Batch Loss: 0.0745001807808876\n",
      "Epoch 4340, Loss: 0.19021201506257057, Final Batch Loss: 0.032459329813718796\n",
      "Epoch 4341, Loss: 0.11015047412365675, Final Batch Loss: 0.012943125329911709\n",
      "Epoch 4342, Loss: 0.16539734276011586, Final Batch Loss: 0.00410964572802186\n",
      "Epoch 4343, Loss: 0.11748994514346123, Final Batch Loss: 0.017881937325000763\n",
      "Epoch 4344, Loss: 0.12471781857311726, Final Batch Loss: 0.018556132912635803\n",
      "Epoch 4345, Loss: 0.16601948626339436, Final Batch Loss: 0.02249586209654808\n",
      "Epoch 4346, Loss: 0.13166876882314682, Final Batch Loss: 0.04367256537079811\n",
      "Epoch 4347, Loss: 0.18798882141709328, Final Batch Loss: 0.09814999252557755\n",
      "Epoch 4348, Loss: 0.10271473601460457, Final Batch Loss: 0.039987411350011826\n",
      "Epoch 4349, Loss: 0.23365575820207596, Final Batch Loss: 0.11537286639213562\n",
      "Epoch 4350, Loss: 0.10663825646042824, Final Batch Loss: 0.016124626621603966\n",
      "Epoch 4351, Loss: 0.197756789624691, Final Batch Loss: 0.09625497460365295\n",
      "Epoch 4352, Loss: 0.15048188529908657, Final Batch Loss: 0.005570469424128532\n",
      "Epoch 4353, Loss: 0.2519967518746853, Final Batch Loss: 0.09950822591781616\n",
      "Epoch 4354, Loss: 0.13926984928548336, Final Batch Loss: 0.025516819208860397\n",
      "Epoch 4355, Loss: 0.22977624833583832, Final Batch Loss: 0.035845305770635605\n",
      "Epoch 4356, Loss: 0.1113440664485097, Final Batch Loss: 0.018767358735203743\n",
      "Epoch 4357, Loss: 0.15452479384839535, Final Batch Loss: 0.008442191407084465\n",
      "Epoch 4358, Loss: 0.21110939607024193, Final Batch Loss: 0.05332960933446884\n",
      "Epoch 4359, Loss: 0.159221313893795, Final Batch Loss: 0.04770287498831749\n",
      "Epoch 4360, Loss: 0.11079937033355236, Final Batch Loss: 0.02162320353090763\n",
      "Epoch 4361, Loss: 0.12406386621296406, Final Batch Loss: 0.03092946857213974\n",
      "Epoch 4362, Loss: 0.1307427790015936, Final Batch Loss: 0.03171403706073761\n",
      "Epoch 4363, Loss: 0.12026325613260269, Final Batch Loss: 0.017682025209069252\n",
      "Epoch 4364, Loss: 0.12499073892831802, Final Batch Loss: 0.02463390864431858\n",
      "Epoch 4365, Loss: 0.15744070708751678, Final Batch Loss: 0.03846190869808197\n",
      "Epoch 4366, Loss: 0.16103377752006054, Final Batch Loss: 0.030915318056941032\n",
      "Epoch 4367, Loss: 0.2473153192549944, Final Batch Loss: 0.08412999659776688\n",
      "Epoch 4368, Loss: 0.2041651550680399, Final Batch Loss: 0.027964970096945763\n",
      "Epoch 4369, Loss: 0.1466052234172821, Final Batch Loss: 0.06200731173157692\n",
      "Epoch 4370, Loss: 0.2651488818228245, Final Batch Loss: 0.13855642080307007\n",
      "Epoch 4371, Loss: 0.18622792325913906, Final Batch Loss: 0.06689264625310898\n",
      "Epoch 4372, Loss: 0.2557887677103281, Final Batch Loss: 0.1684068888425827\n",
      "Epoch 4373, Loss: 0.15584606491029263, Final Batch Loss: 0.06258619576692581\n",
      "Epoch 4374, Loss: 0.15027308464050293, Final Batch Loss: 0.02119269408285618\n",
      "Epoch 4375, Loss: 0.16798260249197483, Final Batch Loss: 0.029373224824666977\n",
      "Epoch 4376, Loss: 0.14926442503929138, Final Batch Loss: 0.07205085456371307\n",
      "Epoch 4377, Loss: 0.17232714593410492, Final Batch Loss: 0.03296981006860733\n",
      "Epoch 4378, Loss: 0.15732706524431705, Final Batch Loss: 0.06563448905944824\n",
      "Epoch 4379, Loss: 0.13949876092374325, Final Batch Loss: 0.03783580660820007\n",
      "Epoch 4380, Loss: 0.19230966828763485, Final Batch Loss: 0.0866328626871109\n",
      "Epoch 4381, Loss: 0.1625104881823063, Final Batch Loss: 0.038431473076343536\n",
      "Epoch 4382, Loss: 0.1269538290798664, Final Batch Loss: 0.025540199130773544\n",
      "Epoch 4383, Loss: 0.21913651004433632, Final Batch Loss: 0.0740252211689949\n",
      "Epoch 4384, Loss: 0.15423256438225508, Final Batch Loss: 0.0151421083137393\n",
      "Epoch 4385, Loss: 0.19291102141141891, Final Batch Loss: 0.03416421636939049\n",
      "Epoch 4386, Loss: 0.32823799178004265, Final Batch Loss: 0.16084454953670502\n",
      "Epoch 4387, Loss: 0.1646346803754568, Final Batch Loss: 0.024503860622644424\n",
      "Epoch 4388, Loss: 0.2842738665640354, Final Batch Loss: 0.03637048974633217\n",
      "Epoch 4389, Loss: 0.2496754378080368, Final Batch Loss: 0.05244672670960426\n",
      "Epoch 4390, Loss: 0.29867861792445183, Final Batch Loss: 0.09783992916345596\n",
      "Epoch 4391, Loss: 0.23786219581961632, Final Batch Loss: 0.06271133571863174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4392, Loss: 0.21314450353384018, Final Batch Loss: 0.052008140832185745\n",
      "Epoch 4393, Loss: 0.19766542501747608, Final Batch Loss: 0.06814172118902206\n",
      "Epoch 4394, Loss: 0.23006179556250572, Final Batch Loss: 0.046576809138059616\n",
      "Epoch 4395, Loss: 0.28218600153923035, Final Batch Loss: 0.05983846262097359\n",
      "Epoch 4396, Loss: 0.1308082714676857, Final Batch Loss: 0.0079549141228199\n",
      "Epoch 4397, Loss: 0.22298304364085197, Final Batch Loss: 0.023557044565677643\n",
      "Epoch 4398, Loss: 0.14661278203129768, Final Batch Loss: 0.020736491307616234\n",
      "Epoch 4399, Loss: 0.15104080364108086, Final Batch Loss: 0.008287552744150162\n",
      "Epoch 4400, Loss: 0.2234417013823986, Final Batch Loss: 0.076360784471035\n",
      "Epoch 4401, Loss: 0.19548357650637627, Final Batch Loss: 0.04769650101661682\n",
      "Epoch 4402, Loss: 0.139330567792058, Final Batch Loss: 0.015753068029880524\n",
      "Epoch 4403, Loss: 0.25114801339805126, Final Batch Loss: 0.021103737875819206\n",
      "Epoch 4404, Loss: 0.1276681236922741, Final Batch Loss: 0.03670886531472206\n",
      "Epoch 4405, Loss: 0.1856590248644352, Final Batch Loss: 0.04479608312249184\n",
      "Epoch 4406, Loss: 0.20064100064337254, Final Batch Loss: 0.01530936174094677\n",
      "Epoch 4407, Loss: 0.1853134874254465, Final Batch Loss: 0.0546322800219059\n",
      "Epoch 4408, Loss: 0.13488191738724709, Final Batch Loss: 0.023377321660518646\n",
      "Epoch 4409, Loss: 0.22595689818263054, Final Batch Loss: 0.06241394579410553\n",
      "Epoch 4410, Loss: 0.22889057733118534, Final Batch Loss: 0.08496306836605072\n",
      "Epoch 4411, Loss: 0.1462524738162756, Final Batch Loss: 0.04138074070215225\n",
      "Epoch 4412, Loss: 0.11576761025935411, Final Batch Loss: 0.009428243152797222\n",
      "Epoch 4413, Loss: 0.17737052962183952, Final Batch Loss: 0.026490332558751106\n",
      "Epoch 4414, Loss: 0.13864831067621708, Final Batch Loss: 0.011576343327760696\n",
      "Epoch 4415, Loss: 0.16091611236333847, Final Batch Loss: 0.03971615061163902\n",
      "Epoch 4416, Loss: 0.14577078633010387, Final Batch Loss: 0.013788610696792603\n",
      "Epoch 4417, Loss: 0.12794636376202106, Final Batch Loss: 0.05530504137277603\n",
      "Epoch 4418, Loss: 0.12344148196280003, Final Batch Loss: 0.03559642657637596\n",
      "Epoch 4419, Loss: 0.16634883359074593, Final Batch Loss: 0.04410024359822273\n",
      "Epoch 4420, Loss: 0.1736525110900402, Final Batch Loss: 0.03906615078449249\n",
      "Epoch 4421, Loss: 0.1888422891497612, Final Batch Loss: 0.053723838180303574\n",
      "Epoch 4422, Loss: 0.17336140759289265, Final Batch Loss: 0.04111700877547264\n",
      "Epoch 4423, Loss: 0.14531124103814363, Final Batch Loss: 0.04548555985093117\n",
      "Epoch 4424, Loss: 0.1945813875645399, Final Batch Loss: 0.0580349899828434\n",
      "Epoch 4425, Loss: 0.15149464271962643, Final Batch Loss: 0.016766225919127464\n",
      "Epoch 4426, Loss: 0.13687013275921345, Final Batch Loss: 0.019393714144825935\n",
      "Epoch 4427, Loss: 0.15951570123434067, Final Batch Loss: 0.03744994476437569\n",
      "Epoch 4428, Loss: 0.21185734495520592, Final Batch Loss: 0.07972913980484009\n",
      "Epoch 4429, Loss: 0.1639118231832981, Final Batch Loss: 0.021742545068264008\n",
      "Epoch 4430, Loss: 0.2003476284444332, Final Batch Loss: 0.03939864784479141\n",
      "Epoch 4431, Loss: 0.13695776835083961, Final Batch Loss: 0.019630257040262222\n",
      "Epoch 4432, Loss: 0.10391515586525202, Final Batch Loss: 0.012783338315784931\n",
      "Epoch 4433, Loss: 0.20669713616371155, Final Batch Loss: 0.04868980497121811\n",
      "Epoch 4434, Loss: 0.1099029891192913, Final Batch Loss: 0.011554444208741188\n",
      "Epoch 4435, Loss: 0.11275638453662395, Final Batch Loss: 0.01932230405509472\n",
      "Epoch 4436, Loss: 0.1347822453826666, Final Batch Loss: 0.022196726873517036\n",
      "Epoch 4437, Loss: 0.11888891598209739, Final Batch Loss: 0.04452139511704445\n",
      "Epoch 4438, Loss: 0.19312540255486965, Final Batch Loss: 0.11073300242424011\n",
      "Epoch 4439, Loss: 0.14881056174635887, Final Batch Loss: 0.014884207397699356\n",
      "Epoch 4440, Loss: 0.14583460427820683, Final Batch Loss: 0.021086404100060463\n",
      "Epoch 4441, Loss: 0.15563914366066456, Final Batch Loss: 0.030689213424921036\n",
      "Epoch 4442, Loss: 0.18096046894788742, Final Batch Loss: 0.04394466429948807\n",
      "Epoch 4443, Loss: 0.13180772215127945, Final Batch Loss: 0.0190433282405138\n",
      "Epoch 4444, Loss: 0.18568948656320572, Final Batch Loss: 0.0405903123319149\n",
      "Epoch 4445, Loss: 0.12798654660582542, Final Batch Loss: 0.032720014452934265\n",
      "Epoch 4446, Loss: 0.19228419847786427, Final Batch Loss: 0.10374501347541809\n",
      "Epoch 4447, Loss: 0.13235593773424625, Final Batch Loss: 0.042489536106586456\n",
      "Epoch 4448, Loss: 0.2514778710901737, Final Batch Loss: 0.09982050210237503\n",
      "Epoch 4449, Loss: 0.2157016098499298, Final Batch Loss: 0.02539782226085663\n",
      "Epoch 4450, Loss: 0.13525702618062496, Final Batch Loss: 0.022003645077347755\n",
      "Epoch 4451, Loss: 0.13264552131295204, Final Batch Loss: 0.03288332745432854\n",
      "Epoch 4452, Loss: 0.13762612827122211, Final Batch Loss: 0.03223012760281563\n",
      "Epoch 4453, Loss: 0.22769856825470924, Final Batch Loss: 0.09009817242622375\n",
      "Epoch 4454, Loss: 0.16470611095428467, Final Batch Loss: 0.02389362081885338\n",
      "Epoch 4455, Loss: 0.15710287541151047, Final Batch Loss: 0.035903993993997574\n",
      "Epoch 4456, Loss: 0.1889357790350914, Final Batch Loss: 0.05986236035823822\n",
      "Epoch 4457, Loss: 0.24273480661213398, Final Batch Loss: 0.030914803966879845\n",
      "Epoch 4458, Loss: 0.236874895170331, Final Batch Loss: 0.07134146988391876\n",
      "Epoch 4459, Loss: 0.16549891605973244, Final Batch Loss: 0.05065879225730896\n",
      "Epoch 4460, Loss: 0.14943903218954802, Final Batch Loss: 0.01333708968013525\n",
      "Epoch 4461, Loss: 0.22714981995522976, Final Batch Loss: 0.024771137163043022\n",
      "Epoch 4462, Loss: 0.2873864844441414, Final Batch Loss: 0.08853085339069366\n",
      "Epoch 4463, Loss: 0.37189168483018875, Final Batch Loss: 0.17976117134094238\n",
      "Epoch 4464, Loss: 0.14022477716207504, Final Batch Loss: 0.026315806433558464\n",
      "Epoch 4465, Loss: 0.20814712345600128, Final Batch Loss: 0.019830692559480667\n",
      "Epoch 4466, Loss: 0.19779827445745468, Final Batch Loss: 0.025759471580386162\n",
      "Epoch 4467, Loss: 0.20562328398227692, Final Batch Loss: 0.030458606779575348\n",
      "Epoch 4468, Loss: 0.2998319901525974, Final Batch Loss: 0.10967282205820084\n",
      "Epoch 4469, Loss: 0.26825741305947304, Final Batch Loss: 0.10070453584194183\n",
      "Epoch 4470, Loss: 0.199927169829607, Final Batch Loss: 0.044628504663705826\n",
      "Epoch 4471, Loss: 0.16424577683210373, Final Batch Loss: 0.04975959286093712\n",
      "Epoch 4472, Loss: 0.2683740705251694, Final Batch Loss: 0.11735321581363678\n",
      "Epoch 4473, Loss: 0.2156105637550354, Final Batch Loss: 0.07382281124591827\n",
      "Epoch 4474, Loss: 0.15840043127536774, Final Batch Loss: 0.042558200657367706\n",
      "Epoch 4475, Loss: 0.2510107010602951, Final Batch Loss: 0.04160919785499573\n",
      "Epoch 4476, Loss: 0.11135476268827915, Final Batch Loss: 0.0239331666380167\n",
      "Epoch 4477, Loss: 0.16816424764692783, Final Batch Loss: 0.03093833662569523\n",
      "Epoch 4478, Loss: 0.13255451153963804, Final Batch Loss: 0.01433777716010809\n",
      "Epoch 4479, Loss: 0.14528685621917248, Final Batch Loss: 0.029739055782556534\n",
      "Epoch 4480, Loss: 0.1678946064785123, Final Batch Loss: 0.010007181204855442\n",
      "Epoch 4481, Loss: 0.11364376079291105, Final Batch Loss: 0.011714682914316654\n",
      "Epoch 4482, Loss: 0.11807176936417818, Final Batch Loss: 0.025850197300314903\n",
      "Epoch 4483, Loss: 0.12154288217425346, Final Batch Loss: 0.03760703280568123\n",
      "Epoch 4484, Loss: 0.14742586947977543, Final Batch Loss: 0.03829878941178322\n",
      "Epoch 4485, Loss: 0.15624519437551498, Final Batch Loss: 0.03683933988213539\n",
      "Epoch 4486, Loss: 0.1596228312700987, Final Batch Loss: 0.02621992863714695\n",
      "Epoch 4487, Loss: 0.1657741293311119, Final Batch Loss: 0.06596949696540833\n",
      "Epoch 4488, Loss: 0.10760186612606049, Final Batch Loss: 0.02211153879761696\n",
      "Epoch 4489, Loss: 0.1370184225961566, Final Batch Loss: 0.008305246941745281\n",
      "Epoch 4490, Loss: 0.16557303071022034, Final Batch Loss: 0.03649355098605156\n",
      "Epoch 4491, Loss: 0.1363373789936304, Final Batch Loss: 0.018227871507406235\n",
      "Epoch 4492, Loss: 0.22199070639908314, Final Batch Loss: 0.12182749062776566\n",
      "Epoch 4493, Loss: 0.14588924497365952, Final Batch Loss: 0.03763377666473389\n",
      "Epoch 4494, Loss: 0.23120510950684547, Final Batch Loss: 0.05278250575065613\n",
      "Epoch 4495, Loss: 0.12480642506852746, Final Batch Loss: 0.005901825148612261\n",
      "Epoch 4496, Loss: 0.12679350562393665, Final Batch Loss: 0.03135370835661888\n",
      "Epoch 4497, Loss: 0.12393382750451565, Final Batch Loss: 0.022019265219569206\n",
      "Epoch 4498, Loss: 0.13400350883603096, Final Batch Loss: 0.01589907705783844\n",
      "Epoch 4499, Loss: 0.18495184369385242, Final Batch Loss: 0.05122487619519234\n",
      "Epoch 4500, Loss: 0.14527670666575432, Final Batch Loss: 0.021444331854581833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4501, Loss: 0.1977192871272564, Final Batch Loss: 0.03997572511434555\n",
      "Epoch 4502, Loss: 0.0838077450171113, Final Batch Loss: 0.008449471555650234\n",
      "Epoch 4503, Loss: 0.11499529331922531, Final Batch Loss: 0.004325293004512787\n",
      "Epoch 4504, Loss: 0.1137530505657196, Final Batch Loss: 0.018197936937212944\n",
      "Epoch 4505, Loss: 0.10939788445830345, Final Batch Loss: 0.017610087990760803\n",
      "Epoch 4506, Loss: 0.11635071411728859, Final Batch Loss: 0.020267637446522713\n",
      "Epoch 4507, Loss: 0.15306126326322556, Final Batch Loss: 0.06058972328901291\n",
      "Epoch 4508, Loss: 0.10804248973727226, Final Batch Loss: 0.011241082102060318\n",
      "Epoch 4509, Loss: 0.12330197542905807, Final Batch Loss: 0.023365775123238564\n",
      "Epoch 4510, Loss: 0.1889810934662819, Final Batch Loss: 0.04393723979592323\n",
      "Epoch 4511, Loss: 0.17687316238880157, Final Batch Loss: 0.035759180784225464\n",
      "Epoch 4512, Loss: 0.1322935400530696, Final Batch Loss: 0.012617279775440693\n",
      "Epoch 4513, Loss: 0.14249522518366575, Final Batch Loss: 0.045535024255514145\n",
      "Epoch 4514, Loss: 0.164139186963439, Final Batch Loss: 0.07004350423812866\n",
      "Epoch 4515, Loss: 0.2063931692391634, Final Batch Loss: 0.02336304821074009\n",
      "Epoch 4516, Loss: 0.17840707767754793, Final Batch Loss: 0.015470565296709538\n",
      "Epoch 4517, Loss: 0.2001103088259697, Final Batch Loss: 0.06934095919132233\n",
      "Epoch 4518, Loss: 0.2385962028056383, Final Batch Loss: 0.0616179034113884\n",
      "Epoch 4519, Loss: 0.21237154118716717, Final Batch Loss: 0.051346227526664734\n",
      "Epoch 4520, Loss: 0.1717718094587326, Final Batch Loss: 0.027107637375593185\n",
      "Epoch 4521, Loss: 0.22676853463053703, Final Batch Loss: 0.019420087337493896\n",
      "Epoch 4522, Loss: 0.2549544423818588, Final Batch Loss: 0.0757019892334938\n",
      "Epoch 4523, Loss: 0.2357760313898325, Final Batch Loss: 0.11951376497745514\n",
      "Epoch 4524, Loss: 0.14884375780820847, Final Batch Loss: 0.03944230452179909\n",
      "Epoch 4525, Loss: 0.15918230544775724, Final Batch Loss: 0.06846100836992264\n",
      "Epoch 4526, Loss: 0.17158843390643597, Final Batch Loss: 0.03505585342645645\n",
      "Epoch 4527, Loss: 0.19952132925391197, Final Batch Loss: 0.09463956952095032\n",
      "Epoch 4528, Loss: 0.23006426729261875, Final Batch Loss: 0.07657437026500702\n",
      "Epoch 4529, Loss: 0.21006057411432266, Final Batch Loss: 0.054008934646844864\n",
      "Epoch 4530, Loss: 0.13027451001107693, Final Batch Loss: 0.04908920079469681\n",
      "Epoch 4531, Loss: 0.13672733027487993, Final Batch Loss: 0.010835218243300915\n",
      "Epoch 4532, Loss: 0.20183111354708672, Final Batch Loss: 0.04519448056817055\n",
      "Epoch 4533, Loss: 0.1402726536616683, Final Batch Loss: 0.030908988788723946\n",
      "Epoch 4534, Loss: 0.19909081421792507, Final Batch Loss: 0.011640658602118492\n",
      "Epoch 4535, Loss: 0.21814890764653683, Final Batch Loss: 0.06112081930041313\n",
      "Epoch 4536, Loss: 0.14804639667272568, Final Batch Loss: 0.034065477550029755\n",
      "Epoch 4537, Loss: 0.1575122131034732, Final Batch Loss: 0.013383042998611927\n",
      "Epoch 4538, Loss: 0.15449736639857292, Final Batch Loss: 0.019922960549592972\n",
      "Epoch 4539, Loss: 0.1417689099907875, Final Batch Loss: 0.025587204843759537\n",
      "Epoch 4540, Loss: 0.09967534989118576, Final Batch Loss: 0.01596929505467415\n",
      "Epoch 4541, Loss: 0.13481207564473152, Final Batch Loss: 0.06056124344468117\n",
      "Epoch 4542, Loss: 0.13943953439593315, Final Batch Loss: 0.0548977293074131\n",
      "Epoch 4543, Loss: 0.17038670554757118, Final Batch Loss: 0.0780261978507042\n",
      "Epoch 4544, Loss: 0.2682654485106468, Final Batch Loss: 0.10523860901594162\n",
      "Epoch 4545, Loss: 0.20704498700797558, Final Batch Loss: 0.0171396192163229\n",
      "Epoch 4546, Loss: 0.18268444016575813, Final Batch Loss: 0.04059889167547226\n",
      "Epoch 4547, Loss: 0.19993865303695202, Final Batch Loss: 0.04006175324320793\n",
      "Epoch 4548, Loss: 0.1514483503997326, Final Batch Loss: 0.03131207823753357\n",
      "Epoch 4549, Loss: 0.21143481880426407, Final Batch Loss: 0.06465533375740051\n",
      "Epoch 4550, Loss: 0.1520077856257558, Final Batch Loss: 0.013226696290075779\n",
      "Epoch 4551, Loss: 0.21225911751389503, Final Batch Loss: 0.04916643351316452\n",
      "Epoch 4552, Loss: 0.1519009880721569, Final Batch Loss: 0.05339224264025688\n",
      "Epoch 4553, Loss: 0.31369517743587494, Final Batch Loss: 0.16245214641094208\n",
      "Epoch 4554, Loss: 0.17476564645767212, Final Batch Loss: 0.057142581790685654\n",
      "Epoch 4555, Loss: 0.1138126477599144, Final Batch Loss: 0.024477245286107063\n",
      "Epoch 4556, Loss: 0.13144676387310028, Final Batch Loss: 0.037386879324913025\n",
      "Epoch 4557, Loss: 0.16489184834063053, Final Batch Loss: 0.031228233128786087\n",
      "Epoch 4558, Loss: 0.14641074277460575, Final Batch Loss: 0.02419634349644184\n",
      "Epoch 4559, Loss: 0.101079230196774, Final Batch Loss: 0.012516898103058338\n",
      "Epoch 4560, Loss: 0.16579315066337585, Final Batch Loss: 0.032808706164360046\n",
      "Epoch 4561, Loss: 0.13559474796056747, Final Batch Loss: 0.014828871935606003\n",
      "Epoch 4562, Loss: 0.1795697007328272, Final Batch Loss: 0.06649143248796463\n",
      "Epoch 4563, Loss: 0.10303288884460926, Final Batch Loss: 0.03415936976671219\n",
      "Epoch 4564, Loss: 0.11909698136150837, Final Batch Loss: 0.027820933610200882\n",
      "Epoch 4565, Loss: 0.12100290320813656, Final Batch Loss: 0.013855844736099243\n",
      "Epoch 4566, Loss: 0.29328122921288013, Final Batch Loss: 0.15482378005981445\n",
      "Epoch 4567, Loss: 0.17061970196664333, Final Batch Loss: 0.07374314218759537\n",
      "Epoch 4568, Loss: 0.30536989122629166, Final Batch Loss: 0.1610415279865265\n",
      "Epoch 4569, Loss: 0.1914279842749238, Final Batch Loss: 0.03996887058019638\n",
      "Epoch 4570, Loss: 0.11200060695409775, Final Batch Loss: 0.023383038118481636\n",
      "Epoch 4571, Loss: 0.16491407761350274, Final Batch Loss: 0.0075098746456205845\n",
      "Epoch 4572, Loss: 0.1968241771683097, Final Batch Loss: 0.10065771639347076\n",
      "Epoch 4573, Loss: 0.19222972728312016, Final Batch Loss: 0.01829846389591694\n",
      "Epoch 4574, Loss: 0.19845931511372328, Final Batch Loss: 0.014201185666024685\n",
      "Epoch 4575, Loss: 0.2257584184408188, Final Batch Loss: 0.03948593512177467\n",
      "Epoch 4576, Loss: 0.19119654595851898, Final Batch Loss: 0.05416179075837135\n",
      "Epoch 4577, Loss: 0.10411278903484344, Final Batch Loss: 0.02986319735646248\n",
      "Epoch 4578, Loss: 0.1603784766048193, Final Batch Loss: 0.03928303346037865\n",
      "Epoch 4579, Loss: 0.12231125216931105, Final Batch Loss: 0.05837315693497658\n",
      "Epoch 4580, Loss: 0.12717251293361187, Final Batch Loss: 0.019772998988628387\n",
      "Epoch 4581, Loss: 0.14836863242089748, Final Batch Loss: 0.020608654245734215\n",
      "Epoch 4582, Loss: 0.14194075390696526, Final Batch Loss: 0.05341397225856781\n",
      "Epoch 4583, Loss: 0.1528414161875844, Final Batch Loss: 0.0645805075764656\n",
      "Epoch 4584, Loss: 0.20228828862309456, Final Batch Loss: 0.022495731711387634\n",
      "Epoch 4585, Loss: 0.28614558931440115, Final Batch Loss: 0.1445448100566864\n",
      "Epoch 4586, Loss: 0.0930688139051199, Final Batch Loss: 0.036411430686712265\n",
      "Epoch 4587, Loss: 0.3138202764093876, Final Batch Loss: 0.05085444822907448\n",
      "Epoch 4588, Loss: 0.1336497999727726, Final Batch Loss: 0.034963589161634445\n",
      "Epoch 4589, Loss: 0.2469816729426384, Final Batch Loss: 0.043079860508441925\n",
      "Epoch 4590, Loss: 0.24187683500349522, Final Batch Loss: 0.09768830984830856\n",
      "Epoch 4591, Loss: 0.16141586750745773, Final Batch Loss: 0.02959667518734932\n",
      "Epoch 4592, Loss: 0.22137320041656494, Final Batch Loss: 0.02238878235220909\n",
      "Epoch 4593, Loss: 0.2363433502614498, Final Batch Loss: 0.031879909336566925\n",
      "Epoch 4594, Loss: 0.15112222358584404, Final Batch Loss: 0.03206081688404083\n",
      "Epoch 4595, Loss: 0.15296421200037003, Final Batch Loss: 0.04043214023113251\n",
      "Epoch 4596, Loss: 0.10201851278543472, Final Batch Loss: 0.01779983937740326\n",
      "Epoch 4597, Loss: 0.11571013974025846, Final Batch Loss: 0.004830953199416399\n",
      "Epoch 4598, Loss: 0.12545420043170452, Final Batch Loss: 0.026448221877217293\n",
      "Epoch 4599, Loss: 0.08559723384678364, Final Batch Loss: 0.008346866816282272\n",
      "Epoch 4600, Loss: 0.16290803998708725, Final Batch Loss: 0.07063011825084686\n",
      "Epoch 4601, Loss: 0.19094730913639069, Final Batch Loss: 0.11024091392755508\n",
      "Epoch 4602, Loss: 0.10800780169665813, Final Batch Loss: 0.02558951824903488\n",
      "Epoch 4603, Loss: 0.14145056158304214, Final Batch Loss: 0.03613877668976784\n",
      "Epoch 4604, Loss: 0.13278423342853785, Final Batch Loss: 0.011732147075235844\n",
      "Epoch 4605, Loss: 0.17945310100913048, Final Batch Loss: 0.07841522991657257\n",
      "Epoch 4606, Loss: 0.09983741119503975, Final Batch Loss: 0.020206870511174202\n",
      "Epoch 4607, Loss: 0.11828734446316957, Final Batch Loss: 0.011282549239695072\n",
      "Epoch 4608, Loss: 0.069394969381392, Final Batch Loss: 0.006579953245818615\n",
      "Epoch 4609, Loss: 0.1848519667983055, Final Batch Loss: 0.04530793055891991\n",
      "Epoch 4610, Loss: 0.14694446604698896, Final Batch Loss: 0.04579697176814079\n",
      "Epoch 4611, Loss: 0.20596569776535034, Final Batch Loss: 0.04257330298423767\n",
      "Epoch 4612, Loss: 0.1356424018740654, Final Batch Loss: 0.037336207926273346\n",
      "Epoch 4613, Loss: 0.31325266882777214, Final Batch Loss: 0.05739326775074005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4614, Loss: 0.12914924137294292, Final Batch Loss: 0.020181715488433838\n",
      "Epoch 4615, Loss: 0.20027368515729904, Final Batch Loss: 0.05784035846590996\n",
      "Epoch 4616, Loss: 0.13709156401455402, Final Batch Loss: 0.04759877175092697\n",
      "Epoch 4617, Loss: 0.13510762387886643, Final Batch Loss: 0.007428681943565607\n",
      "Epoch 4618, Loss: 0.17274820618331432, Final Batch Loss: 0.0922003909945488\n",
      "Epoch 4619, Loss: 0.1897078799083829, Final Batch Loss: 0.013768007047474384\n",
      "Epoch 4620, Loss: 0.17408035323023796, Final Batch Loss: 0.017788754776120186\n",
      "Epoch 4621, Loss: 0.16557851620018482, Final Batch Loss: 0.012773459777235985\n",
      "Epoch 4622, Loss: 0.12704261671751738, Final Batch Loss: 0.024590997025370598\n",
      "Epoch 4623, Loss: 0.17775190249085426, Final Batch Loss: 0.05707047879695892\n",
      "Epoch 4624, Loss: 0.13942071609199047, Final Batch Loss: 0.017715726047754288\n",
      "Epoch 4625, Loss: 0.08705489221028984, Final Batch Loss: 0.002636763034388423\n",
      "Epoch 4626, Loss: 0.20428560301661491, Final Batch Loss: 0.07843589782714844\n",
      "Epoch 4627, Loss: 0.16219166666269302, Final Batch Loss: 0.052397098392248154\n",
      "Epoch 4628, Loss: 0.2278045192360878, Final Batch Loss: 0.037051722407341\n",
      "Epoch 4629, Loss: 0.14722597878426313, Final Batch Loss: 0.018977411091327667\n",
      "Epoch 4630, Loss: 0.1757149174809456, Final Batch Loss: 0.035854265093803406\n",
      "Epoch 4631, Loss: 0.17518052458763123, Final Batch Loss: 0.04226412996649742\n",
      "Epoch 4632, Loss: 0.2333945296704769, Final Batch Loss: 0.07421848922967911\n",
      "Epoch 4633, Loss: 0.17254756204783916, Final Batch Loss: 0.037087954580783844\n",
      "Epoch 4634, Loss: 0.209489144384861, Final Batch Loss: 0.03552747145295143\n",
      "Epoch 4635, Loss: 0.21243315190076828, Final Batch Loss: 0.08273842930793762\n",
      "Epoch 4636, Loss: 0.13671513088047504, Final Batch Loss: 0.0479070208966732\n",
      "Epoch 4637, Loss: 0.12799543607980013, Final Batch Loss: 0.008120442740619183\n",
      "Epoch 4638, Loss: 0.14954045973718166, Final Batch Loss: 0.05756653845310211\n",
      "Epoch 4639, Loss: 0.1240724679082632, Final Batch Loss: 0.04701511189341545\n",
      "Epoch 4640, Loss: 0.14938709046691656, Final Batch Loss: 0.01516397949308157\n",
      "Epoch 4641, Loss: 0.13113734312355518, Final Batch Loss: 0.02635628543794155\n",
      "Epoch 4642, Loss: 0.11508693546056747, Final Batch Loss: 0.019708499312400818\n",
      "Epoch 4643, Loss: 0.16397936642169952, Final Batch Loss: 0.03874950855970383\n",
      "Epoch 4644, Loss: 0.1536085568368435, Final Batch Loss: 0.031530871987342834\n",
      "Epoch 4645, Loss: 0.1457757893949747, Final Batch Loss: 0.026961177587509155\n",
      "Epoch 4646, Loss: 0.13165684090927243, Final Batch Loss: 0.007757569197565317\n",
      "Epoch 4647, Loss: 0.11400667671114206, Final Batch Loss: 0.01990547962486744\n",
      "Epoch 4648, Loss: 0.100625429302454, Final Batch Loss: 0.022655563428997993\n",
      "Epoch 4649, Loss: 0.13638278748840094, Final Batch Loss: 0.04831329733133316\n",
      "Epoch 4650, Loss: 0.11453959345817566, Final Batch Loss: 0.024050278589129448\n",
      "Epoch 4651, Loss: 0.10751219186931849, Final Batch Loss: 0.004670402966439724\n",
      "Epoch 4652, Loss: 0.15011912398040295, Final Batch Loss: 0.036844950169324875\n",
      "Epoch 4653, Loss: 0.25163230020552874, Final Batch Loss: 0.15088340640068054\n",
      "Epoch 4654, Loss: 0.11042433977127075, Final Batch Loss: 0.020279595628380775\n",
      "Epoch 4655, Loss: 0.10709725879132748, Final Batch Loss: 0.017385203391313553\n",
      "Epoch 4656, Loss: 0.17551595717668533, Final Batch Loss: 0.06705784052610397\n",
      "Epoch 4657, Loss: 0.27486808225512505, Final Batch Loss: 0.12716436386108398\n",
      "Epoch 4658, Loss: 0.20352496393024921, Final Batch Loss: 0.0542059950530529\n",
      "Epoch 4659, Loss: 0.26970041915774345, Final Batch Loss: 0.08215001225471497\n",
      "Epoch 4660, Loss: 0.14289040584117174, Final Batch Loss: 0.011243795044720173\n",
      "Epoch 4661, Loss: 0.18923740833997726, Final Batch Loss: 0.028522808104753494\n",
      "Epoch 4662, Loss: 0.2310890443623066, Final Batch Loss: 0.05523065850138664\n",
      "Epoch 4663, Loss: 0.23207968100905418, Final Batch Loss: 0.035454992204904556\n",
      "Epoch 4664, Loss: 0.09698084741830826, Final Batch Loss: 0.01987016387283802\n",
      "Epoch 4665, Loss: 0.12199782207608223, Final Batch Loss: 0.055383775383234024\n",
      "Epoch 4666, Loss: 0.13624978438019753, Final Batch Loss: 0.016157958656549454\n",
      "Epoch 4667, Loss: 0.11144241690635681, Final Batch Loss: 0.032916855067014694\n",
      "Epoch 4668, Loss: 0.12474295683205128, Final Batch Loss: 0.04487568512558937\n",
      "Epoch 4669, Loss: 0.22810332477092743, Final Batch Loss: 0.10625483840703964\n",
      "Epoch 4670, Loss: 0.32336275465786457, Final Batch Loss: 0.09623851627111435\n",
      "Epoch 4671, Loss: 0.2051828745752573, Final Batch Loss: 0.022882556542754173\n",
      "Epoch 4672, Loss: 0.2514185290783644, Final Batch Loss: 0.15801937878131866\n",
      "Epoch 4673, Loss: 0.1944323182106018, Final Batch Loss: 0.05532508343458176\n",
      "Epoch 4674, Loss: 0.08969679661095142, Final Batch Loss: 0.016497233882546425\n",
      "Epoch 4675, Loss: 0.21335986256599426, Final Batch Loss: 0.03357667848467827\n",
      "Epoch 4676, Loss: 0.14225596003234386, Final Batch Loss: 0.03402533754706383\n",
      "Epoch 4677, Loss: 0.17046761140227318, Final Batch Loss: 0.04881768301129341\n",
      "Epoch 4678, Loss: 0.18970136530697346, Final Batch Loss: 0.03410807251930237\n",
      "Epoch 4679, Loss: 0.08274343144148588, Final Batch Loss: 0.010394646786153316\n",
      "Epoch 4680, Loss: 0.15989448688924313, Final Batch Loss: 0.021456750109791756\n",
      "Epoch 4681, Loss: 0.16632577404379845, Final Batch Loss: 0.033879850059747696\n",
      "Epoch 4682, Loss: 0.15151002258062363, Final Batch Loss: 0.019022343680262566\n",
      "Epoch 4683, Loss: 0.12794319167733192, Final Batch Loss: 0.024040473625063896\n",
      "Epoch 4684, Loss: 0.15989945363253355, Final Batch Loss: 0.009804136119782925\n",
      "Epoch 4685, Loss: 0.09192158933728933, Final Batch Loss: 0.014792428351938725\n",
      "Epoch 4686, Loss: 0.12656532879918814, Final Batch Loss: 0.013725697062909603\n",
      "Epoch 4687, Loss: 0.1801673136651516, Final Batch Loss: 0.01926969550549984\n",
      "Epoch 4688, Loss: 0.19231394678354263, Final Batch Loss: 0.0652824193239212\n",
      "Epoch 4689, Loss: 0.13074413500726223, Final Batch Loss: 0.016743510961532593\n",
      "Epoch 4690, Loss: 0.1544588478282094, Final Batch Loss: 0.013627045787870884\n",
      "Epoch 4691, Loss: 0.18349295854568481, Final Batch Loss: 0.03367605060338974\n",
      "Epoch 4692, Loss: 0.1355838105082512, Final Batch Loss: 0.039545174688100815\n",
      "Epoch 4693, Loss: 0.172171238809824, Final Batch Loss: 0.014170099049806595\n",
      "Epoch 4694, Loss: 0.14484621211886406, Final Batch Loss: 0.02355043776333332\n",
      "Epoch 4695, Loss: 0.12218888849020004, Final Batch Loss: 0.03862166032195091\n",
      "Epoch 4696, Loss: 0.13619632180780172, Final Batch Loss: 0.030579503625631332\n",
      "Epoch 4697, Loss: 0.29743451811373234, Final Batch Loss: 0.18883968889713287\n",
      "Epoch 4698, Loss: 0.19571873918175697, Final Batch Loss: 0.062349833548069\n",
      "Epoch 4699, Loss: 0.19759590178728104, Final Batch Loss: 0.059778667986392975\n",
      "Epoch 4700, Loss: 0.19192641228437424, Final Batch Loss: 0.05380089208483696\n",
      "Epoch 4701, Loss: 0.19369789585471153, Final Batch Loss: 0.041923414915800095\n",
      "Epoch 4702, Loss: 0.2023140974342823, Final Batch Loss: 0.045073796063661575\n",
      "Epoch 4703, Loss: 0.2540286108851433, Final Batch Loss: 0.1521201878786087\n",
      "Epoch 4704, Loss: 0.08891491033136845, Final Batch Loss: 0.014458073303103447\n",
      "Epoch 4705, Loss: 0.08628234593197703, Final Batch Loss: 0.005339765455573797\n",
      "Epoch 4706, Loss: 0.12354641780257225, Final Batch Loss: 0.02152809128165245\n",
      "Epoch 4707, Loss: 0.15441063418984413, Final Batch Loss: 0.04608495905995369\n",
      "Epoch 4708, Loss: 0.0927890483289957, Final Batch Loss: 0.036810703575611115\n",
      "Epoch 4709, Loss: 0.1508991215378046, Final Batch Loss: 0.026425160467624664\n",
      "Epoch 4710, Loss: 0.1420206930488348, Final Batch Loss: 0.041453488171100616\n",
      "Epoch 4711, Loss: 0.11610274016857147, Final Batch Loss: 0.03242453932762146\n",
      "Epoch 4712, Loss: 0.1409850725904107, Final Batch Loss: 0.008521364070475101\n",
      "Epoch 4713, Loss: 0.15067857317626476, Final Batch Loss: 0.018560143187642097\n",
      "Epoch 4714, Loss: 0.17692549526691437, Final Batch Loss: 0.0671345442533493\n",
      "Epoch 4715, Loss: 0.14004704169929028, Final Batch Loss: 0.025792069733142853\n",
      "Epoch 4716, Loss: 0.13664280995726585, Final Batch Loss: 0.02042136713862419\n",
      "Epoch 4717, Loss: 0.22765994258224964, Final Batch Loss: 0.11084269732236862\n",
      "Epoch 4718, Loss: 0.12168742157518864, Final Batch Loss: 0.005443481728434563\n",
      "Epoch 4719, Loss: 0.10763986967504025, Final Batch Loss: 0.02132478915154934\n",
      "Epoch 4720, Loss: 0.18061323277652264, Final Batch Loss: 0.06898647546768188\n",
      "Epoch 4721, Loss: 0.08796193264424801, Final Batch Loss: 0.02818775363266468\n",
      "Epoch 4722, Loss: 0.14641520101577044, Final Batch Loss: 0.03705969825387001\n",
      "Epoch 4723, Loss: 0.08910412527620792, Final Batch Loss: 0.010147970169782639\n",
      "Epoch 4724, Loss: 0.10590937826782465, Final Batch Loss: 0.02330894209444523\n",
      "Epoch 4725, Loss: 0.1715252846479416, Final Batch Loss: 0.07548853754997253\n",
      "Epoch 4726, Loss: 0.23379915207624435, Final Batch Loss: 0.0384722501039505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4727, Loss: 0.15931647643446922, Final Batch Loss: 0.01381731778383255\n",
      "Epoch 4728, Loss: 0.16938798129558563, Final Batch Loss: 0.06541893631219864\n",
      "Epoch 4729, Loss: 0.15915502235293388, Final Batch Loss: 0.040096916258335114\n",
      "Epoch 4730, Loss: 0.25556607358157635, Final Batch Loss: 0.113731250166893\n",
      "Epoch 4731, Loss: 0.1436034683138132, Final Batch Loss: 0.020348764955997467\n",
      "Epoch 4732, Loss: 0.17729326151311398, Final Batch Loss: 0.04560296609997749\n",
      "Epoch 4733, Loss: 0.12446110229939222, Final Batch Loss: 0.011314292438328266\n",
      "Epoch 4734, Loss: 0.16804043482989073, Final Batch Loss: 0.09379181265830994\n",
      "Epoch 4735, Loss: 0.15592829883098602, Final Batch Loss: 0.051941752433776855\n",
      "Epoch 4736, Loss: 0.13743771240115166, Final Batch Loss: 0.06668779253959656\n",
      "Epoch 4737, Loss: 0.16023628786206245, Final Batch Loss: 0.056377191096544266\n",
      "Epoch 4738, Loss: 0.14393997099250555, Final Batch Loss: 0.014935052953660488\n",
      "Epoch 4739, Loss: 0.1672134380787611, Final Batch Loss: 0.05684127286076546\n",
      "Epoch 4740, Loss: 0.1477384138852358, Final Batch Loss: 0.04682084918022156\n",
      "Epoch 4741, Loss: 0.2723390832543373, Final Batch Loss: 0.12811744213104248\n",
      "Epoch 4742, Loss: 0.13645430468022823, Final Batch Loss: 0.02128303237259388\n",
      "Epoch 4743, Loss: 0.1549449134618044, Final Batch Loss: 0.04878458008170128\n",
      "Epoch 4744, Loss: 0.1993337357416749, Final Batch Loss: 0.004868675954639912\n",
      "Epoch 4745, Loss: 0.20733314380049706, Final Batch Loss: 0.05302492156624794\n",
      "Epoch 4746, Loss: 0.18527703545987606, Final Batch Loss: 0.06438973546028137\n",
      "Epoch 4747, Loss: 0.21153843589127064, Final Batch Loss: 0.03130805492401123\n",
      "Epoch 4748, Loss: 0.19968298077583313, Final Batch Loss: 0.03720711171627045\n",
      "Epoch 4749, Loss: 0.18649842962622643, Final Batch Loss: 0.07403329014778137\n",
      "Epoch 4750, Loss: 0.18709411285817623, Final Batch Loss: 0.02790576033294201\n",
      "Epoch 4751, Loss: 0.15846913307905197, Final Batch Loss: 0.03028544783592224\n",
      "Epoch 4752, Loss: 0.1687217690050602, Final Batch Loss: 0.03295379504561424\n",
      "Epoch 4753, Loss: 0.27905664034187794, Final Batch Loss: 0.12335045635700226\n",
      "Epoch 4754, Loss: 0.16083040088415146, Final Batch Loss: 0.02971290796995163\n",
      "Epoch 4755, Loss: 0.13346601091325283, Final Batch Loss: 0.00450383685529232\n",
      "Epoch 4756, Loss: 0.12420765869319439, Final Batch Loss: 0.011633606627583504\n",
      "Epoch 4757, Loss: 0.16380036994814873, Final Batch Loss: 0.026777256280183792\n",
      "Epoch 4758, Loss: 0.10471389256417751, Final Batch Loss: 0.027079762890934944\n",
      "Epoch 4759, Loss: 0.09910934884101152, Final Batch Loss: 0.009210227988660336\n",
      "Epoch 4760, Loss: 0.16650534607470036, Final Batch Loss: 0.05562960356473923\n",
      "Epoch 4761, Loss: 0.19106747582554817, Final Batch Loss: 0.03126969560980797\n",
      "Epoch 4762, Loss: 0.18107368424534798, Final Batch Loss: 0.07678988575935364\n",
      "Epoch 4763, Loss: 0.16148122772574425, Final Batch Loss: 0.0496365986764431\n",
      "Epoch 4764, Loss: 0.22739003505557775, Final Batch Loss: 0.10606294870376587\n",
      "Epoch 4765, Loss: 0.131690951064229, Final Batch Loss: 0.04007679969072342\n",
      "Epoch 4766, Loss: 0.09296247176826, Final Batch Loss: 0.01567806303501129\n",
      "Epoch 4767, Loss: 0.11031182203441858, Final Batch Loss: 0.011504299007356167\n",
      "Epoch 4768, Loss: 0.21447914466261864, Final Batch Loss: 0.10272666066884995\n",
      "Epoch 4769, Loss: 0.21321986988186836, Final Batch Loss: 0.09707747399806976\n",
      "Epoch 4770, Loss: 0.08800190547481179, Final Batch Loss: 0.005217679310590029\n",
      "Epoch 4771, Loss: 0.15463889949023724, Final Batch Loss: 0.0305636003613472\n",
      "Epoch 4772, Loss: 0.12857556622475386, Final Batch Loss: 0.0041134534403681755\n",
      "Epoch 4773, Loss: 0.17294961214065552, Final Batch Loss: 0.055094242095947266\n",
      "Epoch 4774, Loss: 0.109905906021595, Final Batch Loss: 0.030995652079582214\n",
      "Epoch 4775, Loss: 0.10983124561607838, Final Batch Loss: 0.015862083062529564\n",
      "Epoch 4776, Loss: 0.10100159421563148, Final Batch Loss: 0.01239163801074028\n",
      "Epoch 4777, Loss: 0.1412622146308422, Final Batch Loss: 0.033425722271203995\n",
      "Epoch 4778, Loss: 0.17513779364526272, Final Batch Loss: 0.050998199731111526\n",
      "Epoch 4779, Loss: 0.17728065699338913, Final Batch Loss: 0.013336662203073502\n",
      "Epoch 4780, Loss: 0.18152740970253944, Final Batch Loss: 0.010566145181655884\n",
      "Epoch 4781, Loss: 0.13413238152861595, Final Batch Loss: 0.01816779002547264\n",
      "Epoch 4782, Loss: 0.18874328583478928, Final Batch Loss: 0.0771559402346611\n",
      "Epoch 4783, Loss: 0.1891022641211748, Final Batch Loss: 0.07526826113462448\n",
      "Epoch 4784, Loss: 0.21878719702363014, Final Batch Loss: 0.08199989050626755\n",
      "Epoch 4785, Loss: 0.15320704318583012, Final Batch Loss: 0.023324809968471527\n",
      "Epoch 4786, Loss: 0.16383873857557774, Final Batch Loss: 0.03138289973139763\n",
      "Epoch 4787, Loss: 0.13049796037375927, Final Batch Loss: 0.017349781468510628\n",
      "Epoch 4788, Loss: 0.1414703093469143, Final Batch Loss: 0.033271096646785736\n",
      "Epoch 4789, Loss: 0.11330106854438782, Final Batch Loss: 0.020654916763305664\n",
      "Epoch 4790, Loss: 0.15638412535190582, Final Batch Loss: 0.012020472437143326\n",
      "Epoch 4791, Loss: 0.1683216281235218, Final Batch Loss: 0.0891508162021637\n",
      "Epoch 4792, Loss: 0.10610971227288246, Final Batch Loss: 0.047506462782621384\n",
      "Epoch 4793, Loss: 0.10124961100518703, Final Batch Loss: 0.02739780582487583\n",
      "Epoch 4794, Loss: 0.13619201071560383, Final Batch Loss: 0.02439415641129017\n",
      "Epoch 4795, Loss: 0.1634200643748045, Final Batch Loss: 0.008211914449930191\n",
      "Epoch 4796, Loss: 0.20219330862164497, Final Batch Loss: 0.03211555629968643\n",
      "Epoch 4797, Loss: 0.17313583940267563, Final Batch Loss: 0.06221489980816841\n",
      "Epoch 4798, Loss: 0.10021371394395828, Final Batch Loss: 0.031045250594615936\n",
      "Epoch 4799, Loss: 0.1314343772828579, Final Batch Loss: 0.015843819826841354\n",
      "Epoch 4800, Loss: 0.13534296862781048, Final Batch Loss: 0.026428965851664543\n",
      "Epoch 4801, Loss: 0.293771892786026, Final Batch Loss: 0.13285087049007416\n",
      "Epoch 4802, Loss: 0.3091080002486706, Final Batch Loss: 0.12264256924390793\n",
      "Epoch 4803, Loss: 0.2036051508039236, Final Batch Loss: 0.026690302416682243\n",
      "Epoch 4804, Loss: 0.44555317610502243, Final Batch Loss: 0.16823020577430725\n",
      "Epoch 4805, Loss: 0.16996809467673302, Final Batch Loss: 0.020257439464330673\n",
      "Epoch 4806, Loss: 0.21985428780317307, Final Batch Loss: 0.020030783489346504\n",
      "Epoch 4807, Loss: 0.1730283871293068, Final Batch Loss: 0.037832051515579224\n",
      "Epoch 4808, Loss: 0.2437051199376583, Final Batch Loss: 0.05554269254207611\n",
      "Epoch 4809, Loss: 0.21167756989598274, Final Batch Loss: 0.045612286776304245\n",
      "Epoch 4810, Loss: 0.20646024122834206, Final Batch Loss: 0.04504210501909256\n",
      "Epoch 4811, Loss: 0.20882449438795447, Final Batch Loss: 0.0074098980985581875\n",
      "Epoch 4812, Loss: 0.1621809583157301, Final Batch Loss: 0.01576150394976139\n",
      "Epoch 4813, Loss: 0.14968493022024632, Final Batch Loss: 0.04190410301089287\n",
      "Epoch 4814, Loss: 0.21952653862535954, Final Batch Loss: 0.018958738073706627\n",
      "Epoch 4815, Loss: 0.12834740243852139, Final Batch Loss: 0.027892647311091423\n",
      "Epoch 4816, Loss: 0.13794177398085594, Final Batch Loss: 0.031131422147154808\n",
      "Epoch 4817, Loss: 0.09618447860702872, Final Batch Loss: 0.006328248884528875\n",
      "Epoch 4818, Loss: 0.13605398684740067, Final Batch Loss: 0.057712920010089874\n",
      "Epoch 4819, Loss: 0.09794951975345612, Final Batch Loss: 0.01162983663380146\n",
      "Epoch 4820, Loss: 0.13875648193061352, Final Batch Loss: 0.017520679160952568\n",
      "Epoch 4821, Loss: 0.09075554832816124, Final Batch Loss: 0.02249360829591751\n",
      "Epoch 4822, Loss: 0.1455677840858698, Final Batch Loss: 0.01922292821109295\n",
      "Epoch 4823, Loss: 0.162683492526412, Final Batch Loss: 0.03350334241986275\n",
      "Epoch 4824, Loss: 0.18310784175992012, Final Batch Loss: 0.052640318870544434\n",
      "Epoch 4825, Loss: 0.13199015893042088, Final Batch Loss: 0.061147015541791916\n",
      "Epoch 4826, Loss: 0.13386225141584873, Final Batch Loss: 0.02304728701710701\n",
      "Epoch 4827, Loss: 0.2804456166923046, Final Batch Loss: 0.09445597231388092\n",
      "Epoch 4828, Loss: 0.10846439562737942, Final Batch Loss: 0.02542637661099434\n",
      "Epoch 4829, Loss: 0.2834727130830288, Final Batch Loss: 0.05486983433365822\n",
      "Epoch 4830, Loss: 0.30605556443333626, Final Batch Loss: 0.07208649814128876\n",
      "Epoch 4831, Loss: 0.2633640766143799, Final Batch Loss: 0.0314941368997097\n",
      "Epoch 4832, Loss: 0.25374095141887665, Final Batch Loss: 0.06857768446207047\n",
      "Epoch 4833, Loss: 0.16704539768397808, Final Batch Loss: 0.01899087242782116\n",
      "Epoch 4834, Loss: 0.25782887265086174, Final Batch Loss: 0.07734568417072296\n",
      "Epoch 4835, Loss: 0.34562002308666706, Final Batch Loss: 0.15780426561832428\n",
      "Epoch 4836, Loss: 0.22135775163769722, Final Batch Loss: 0.045941226184368134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4837, Loss: 0.2507607527077198, Final Batch Loss: 0.09688659012317657\n",
      "Epoch 4838, Loss: 0.16589908301830292, Final Batch Loss: 0.02395692467689514\n",
      "Epoch 4839, Loss: 0.19777172803878784, Final Batch Loss: 0.05826978012919426\n",
      "Epoch 4840, Loss: 0.20102250576019287, Final Batch Loss: 0.06552720069885254\n",
      "Epoch 4841, Loss: 0.15534723363816738, Final Batch Loss: 0.03068971075117588\n",
      "Epoch 4842, Loss: 0.14921888709068298, Final Batch Loss: 0.03234217315912247\n",
      "Epoch 4843, Loss: 0.19722305051982403, Final Batch Loss: 0.08327262103557587\n",
      "Epoch 4844, Loss: 0.19475125148892403, Final Batch Loss: 0.09697505086660385\n",
      "Epoch 4845, Loss: 0.1487869331613183, Final Batch Loss: 0.012138615362346172\n",
      "Epoch 4846, Loss: 0.1234983541071415, Final Batch Loss: 0.02373364381492138\n",
      "Epoch 4847, Loss: 0.11541824880987406, Final Batch Loss: 0.015465980395674706\n",
      "Epoch 4848, Loss: 0.1442486196756363, Final Batch Loss: 0.02996443212032318\n",
      "Epoch 4849, Loss: 0.09388485038653016, Final Batch Loss: 0.00629133777692914\n",
      "Epoch 4850, Loss: 0.08684223890304565, Final Batch Loss: 0.023110510781407356\n",
      "Epoch 4851, Loss: 0.1583881452679634, Final Batch Loss: 0.04646223783493042\n",
      "Epoch 4852, Loss: 0.13426613435149193, Final Batch Loss: 0.03429446741938591\n",
      "Epoch 4853, Loss: 0.19283829256892204, Final Batch Loss: 0.0646316185593605\n",
      "Epoch 4854, Loss: 0.11747507844120264, Final Batch Loss: 0.013292179442942142\n",
      "Epoch 4855, Loss: 0.13207610696554184, Final Batch Loss: 0.007499745115637779\n",
      "Epoch 4856, Loss: 0.14112871047109365, Final Batch Loss: 0.040220506489276886\n",
      "Epoch 4857, Loss: 0.1682273279875517, Final Batch Loss: 0.01571395806968212\n",
      "Epoch 4858, Loss: 0.29217742569744587, Final Batch Loss: 0.06527723371982574\n",
      "Epoch 4859, Loss: 0.14325129240751266, Final Batch Loss: 0.020849358290433884\n",
      "Epoch 4860, Loss: 0.19821409694850445, Final Batch Loss: 0.06767895817756653\n",
      "Epoch 4861, Loss: 0.14808937534689903, Final Batch Loss: 0.045274678617715836\n",
      "Epoch 4862, Loss: 0.14864041842520237, Final Batch Loss: 0.05284098908305168\n",
      "Epoch 4863, Loss: 0.13436471670866013, Final Batch Loss: 0.030253447592258453\n",
      "Epoch 4864, Loss: 0.13096698001027107, Final Batch Loss: 0.0062975771725177765\n",
      "Epoch 4865, Loss: 0.17225334327667952, Final Batch Loss: 0.07654523849487305\n",
      "Epoch 4866, Loss: 0.19097861647605896, Final Batch Loss: 0.017121780663728714\n",
      "Epoch 4867, Loss: 0.19337231293320656, Final Batch Loss: 0.053848832845687866\n",
      "Epoch 4868, Loss: 0.13995408336631954, Final Batch Loss: 0.003218951402232051\n",
      "Epoch 4869, Loss: 0.1744130775332451, Final Batch Loss: 0.047759171575307846\n",
      "Epoch 4870, Loss: 0.17805413901805878, Final Batch Loss: 0.028607521206140518\n",
      "Epoch 4871, Loss: 0.1744533721357584, Final Batch Loss: 0.028363971039652824\n",
      "Epoch 4872, Loss: 0.20173538103699684, Final Batch Loss: 0.047939665615558624\n",
      "Epoch 4873, Loss: 0.17184049263596535, Final Batch Loss: 0.04148539900779724\n",
      "Epoch 4874, Loss: 0.1479948665946722, Final Batch Loss: 0.0695551261305809\n",
      "Epoch 4875, Loss: 0.1902313306927681, Final Batch Loss: 0.07059789448976517\n",
      "Epoch 4876, Loss: 0.15785684250295162, Final Batch Loss: 0.017146332189440727\n",
      "Epoch 4877, Loss: 0.15012166183441877, Final Batch Loss: 0.00809216033667326\n",
      "Epoch 4878, Loss: 0.184131046757102, Final Batch Loss: 0.08580172806978226\n",
      "Epoch 4879, Loss: 0.14596432447433472, Final Batch Loss: 0.04328569769859314\n",
      "Epoch 4880, Loss: 0.20298867486417294, Final Batch Loss: 0.027619319036602974\n",
      "Epoch 4881, Loss: 0.1104818545281887, Final Batch Loss: 0.028029339388012886\n",
      "Epoch 4882, Loss: 0.12977362237870693, Final Batch Loss: 0.020628007128834724\n",
      "Epoch 4883, Loss: 0.14997265953570604, Final Batch Loss: 0.010107590816915035\n",
      "Epoch 4884, Loss: 0.2136143371462822, Final Batch Loss: 0.12818175554275513\n",
      "Epoch 4885, Loss: 0.28487958386540413, Final Batch Loss: 0.15141071379184723\n",
      "Epoch 4886, Loss: 0.3827555924654007, Final Batch Loss: 0.15018323063850403\n",
      "Epoch 4887, Loss: 0.28154394403100014, Final Batch Loss: 0.03716060146689415\n",
      "Epoch 4888, Loss: 0.23111535608768463, Final Batch Loss: 0.07329697161912918\n",
      "Epoch 4889, Loss: 0.2572450824081898, Final Batch Loss: 0.11957086622714996\n",
      "Epoch 4890, Loss: 0.1644013486802578, Final Batch Loss: 0.03183778375387192\n",
      "Epoch 4891, Loss: 0.18260388635098934, Final Batch Loss: 0.08157140761613846\n",
      "Epoch 4892, Loss: 0.20083268359303474, Final Batch Loss: 0.07136593759059906\n",
      "Epoch 4893, Loss: 0.17568837478756905, Final Batch Loss: 0.017074357718229294\n",
      "Epoch 4894, Loss: 0.23203469067811966, Final Batch Loss: 0.08461205661296844\n",
      "Epoch 4895, Loss: 0.1760691963136196, Final Batch Loss: 0.04197966307401657\n",
      "Epoch 4896, Loss: 0.1714825425297022, Final Batch Loss: 0.0805075541138649\n",
      "Epoch 4897, Loss: 0.11932645831257105, Final Batch Loss: 0.009843542240560055\n",
      "Epoch 4898, Loss: 0.14562634751200676, Final Batch Loss: 0.07027392089366913\n",
      "Epoch 4899, Loss: 0.200107304379344, Final Batch Loss: 0.06850143522024155\n",
      "Epoch 4900, Loss: 0.13058565463870764, Final Batch Loss: 0.010272121988236904\n",
      "Epoch 4901, Loss: 0.19647478871047497, Final Batch Loss: 0.06300339847803116\n",
      "Epoch 4902, Loss: 0.16039621271193027, Final Batch Loss: 0.020563971251249313\n",
      "Epoch 4903, Loss: 0.1234650481492281, Final Batch Loss: 0.012883076444268227\n",
      "Epoch 4904, Loss: 0.15083149634301662, Final Batch Loss: 0.028520599007606506\n",
      "Epoch 4905, Loss: 0.15534002147614956, Final Batch Loss: 0.04172077402472496\n",
      "Epoch 4906, Loss: 0.13625256344676018, Final Batch Loss: 0.020561078563332558\n",
      "Epoch 4907, Loss: 0.12243245169520378, Final Batch Loss: 0.03991100564599037\n",
      "Epoch 4908, Loss: 0.21216084994375706, Final Batch Loss: 0.028736673295497894\n",
      "Epoch 4909, Loss: 0.16587024927139282, Final Batch Loss: 0.022829383611679077\n",
      "Epoch 4910, Loss: 0.16629932448267937, Final Batch Loss: 0.02450341358780861\n",
      "Epoch 4911, Loss: 0.1632903553545475, Final Batch Loss: 0.0216884333640337\n",
      "Epoch 4912, Loss: 0.11908264271914959, Final Batch Loss: 0.02470429614186287\n",
      "Epoch 4913, Loss: 0.14864022471010685, Final Batch Loss: 0.02767547219991684\n",
      "Epoch 4914, Loss: 0.16962759755551815, Final Batch Loss: 0.03542795404791832\n",
      "Epoch 4915, Loss: 0.13488679099828005, Final Batch Loss: 0.035320427268743515\n",
      "Epoch 4916, Loss: 0.17013513506390154, Final Batch Loss: 0.0035359023604542017\n",
      "Epoch 4917, Loss: 0.16290882416069508, Final Batch Loss: 0.04886498674750328\n",
      "Epoch 4918, Loss: 0.14863764494657516, Final Batch Loss: 0.0495733842253685\n",
      "Epoch 4919, Loss: 0.26860518753528595, Final Batch Loss: 0.13348279893398285\n",
      "Epoch 4920, Loss: 0.20044966973364353, Final Batch Loss: 0.06357420235872269\n",
      "Epoch 4921, Loss: 0.2533730994910002, Final Batch Loss: 0.13861201703548431\n",
      "Epoch 4922, Loss: 0.20398796908557415, Final Batch Loss: 0.08357031643390656\n",
      "Epoch 4923, Loss: 0.1365598365664482, Final Batch Loss: 0.017184551805257797\n",
      "Epoch 4924, Loss: 0.14245294779539108, Final Batch Loss: 0.05951467528939247\n",
      "Epoch 4925, Loss: 0.11732116062194109, Final Batch Loss: 0.0068327514454722404\n",
      "Epoch 4926, Loss: 0.18047917261719704, Final Batch Loss: 0.03468829020857811\n",
      "Epoch 4927, Loss: 0.16542291082441807, Final Batch Loss: 0.07325344532728195\n",
      "Epoch 4928, Loss: 0.16138648986816406, Final Batch Loss: 0.013203542679548264\n",
      "Epoch 4929, Loss: 0.11030887626111507, Final Batch Loss: 0.01573972776532173\n",
      "Epoch 4930, Loss: 0.13404067419469357, Final Batch Loss: 0.046702414751052856\n",
      "Epoch 4931, Loss: 0.15590029396116734, Final Batch Loss: 0.03955642133951187\n",
      "Epoch 4932, Loss: 0.16613496840000153, Final Batch Loss: 0.03066354990005493\n",
      "Epoch 4933, Loss: 0.203243101015687, Final Batch Loss: 0.0718044638633728\n",
      "Epoch 4934, Loss: 0.22279437817633152, Final Batch Loss: 0.07959946244955063\n",
      "Epoch 4935, Loss: 0.1915695033967495, Final Batch Loss: 0.08654295653104782\n",
      "Epoch 4936, Loss: 0.1748771108686924, Final Batch Loss: 0.052975308150053024\n",
      "Epoch 4937, Loss: 0.15325574204325676, Final Batch Loss: 0.03688498213887215\n",
      "Epoch 4938, Loss: 0.16473775170743465, Final Batch Loss: 0.023019583895802498\n",
      "Epoch 4939, Loss: 0.14503161050379276, Final Batch Loss: 0.027657097205519676\n",
      "Epoch 4940, Loss: 0.17250634357333183, Final Batch Loss: 0.06100193038582802\n",
      "Epoch 4941, Loss: 0.2323610484600067, Final Batch Loss: 0.1321902573108673\n",
      "Epoch 4942, Loss: 0.16202817857265472, Final Batch Loss: 0.017924733459949493\n",
      "Epoch 4943, Loss: 0.17830386012792587, Final Batch Loss: 0.08069058507680893\n",
      "Epoch 4944, Loss: 0.12557690311223269, Final Batch Loss: 0.009012636728584766\n",
      "Epoch 4945, Loss: 0.26482323557138443, Final Batch Loss: 0.044257257133722305\n",
      "Epoch 4946, Loss: 0.2610367313027382, Final Batch Loss: 0.11769434064626694\n",
      "Epoch 4947, Loss: 0.16574369370937347, Final Batch Loss: 0.0326174832880497\n",
      "Epoch 4948, Loss: 0.20525357499718666, Final Batch Loss: 0.03699053078889847\n",
      "Epoch 4949, Loss: 0.21303854510188103, Final Batch Loss: 0.06061013042926788\n",
      "Epoch 4950, Loss: 0.1501194890588522, Final Batch Loss: 0.08011511713266373\n",
      "Epoch 4951, Loss: 0.13885829038918018, Final Batch Loss: 0.04064510762691498\n",
      "Epoch 4952, Loss: 0.15467009879648685, Final Batch Loss: 0.06717802584171295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4953, Loss: 0.17127998359501362, Final Batch Loss: 0.024126362055540085\n",
      "Epoch 4954, Loss: 0.22800536267459393, Final Batch Loss: 0.01887788064777851\n",
      "Epoch 4955, Loss: 0.21449160762131214, Final Batch Loss: 0.029476815834641457\n",
      "Epoch 4956, Loss: 0.23807748779654503, Final Batch Loss: 0.04945414513349533\n",
      "Epoch 4957, Loss: 0.15358491986989975, Final Batch Loss: 0.03508133441209793\n",
      "Epoch 4958, Loss: 0.17118071299046278, Final Batch Loss: 0.06517712026834488\n",
      "Epoch 4959, Loss: 0.15402194671332836, Final Batch Loss: 0.061006974428892136\n",
      "Epoch 4960, Loss: 0.10691486671566963, Final Batch Loss: 0.022004857659339905\n",
      "Epoch 4961, Loss: 0.16918272525072098, Final Batch Loss: 0.02488017827272415\n",
      "Epoch 4962, Loss: 0.12632405292242765, Final Batch Loss: 0.015217668376863003\n",
      "Epoch 4963, Loss: 0.13764787325635552, Final Batch Loss: 0.00644302973523736\n",
      "Epoch 4964, Loss: 0.1374781308695674, Final Batch Loss: 0.009819778613746166\n",
      "Epoch 4965, Loss: 0.14551164209842682, Final Batch Loss: 0.06410267949104309\n",
      "Epoch 4966, Loss: 0.152947086840868, Final Batch Loss: 0.08517776429653168\n",
      "Epoch 4967, Loss: 0.16511715948581696, Final Batch Loss: 0.036857686936855316\n",
      "Epoch 4968, Loss: 0.16029971651732922, Final Batch Loss: 0.04765090346336365\n",
      "Epoch 4969, Loss: 0.11544789746403694, Final Batch Loss: 0.013732817023992538\n",
      "Epoch 4970, Loss: 0.1089907307177782, Final Batch Loss: 0.017800191417336464\n",
      "Epoch 4971, Loss: 0.12651472352445126, Final Batch Loss: 0.029162755236029625\n",
      "Epoch 4972, Loss: 0.1295378701761365, Final Batch Loss: 0.051243510097265244\n",
      "Epoch 4973, Loss: 0.17980118468403816, Final Batch Loss: 0.028163205832242966\n",
      "Epoch 4974, Loss: 0.14108102582395077, Final Batch Loss: 0.038934897631406784\n",
      "Epoch 4975, Loss: 0.10952484793961048, Final Batch Loss: 0.020007802173495293\n",
      "Epoch 4976, Loss: 0.12004441395401955, Final Batch Loss: 0.020520685240626335\n",
      "Epoch 4977, Loss: 0.1435688454657793, Final Batch Loss: 0.023054981604218483\n",
      "Epoch 4978, Loss: 0.1587152425199747, Final Batch Loss: 0.039879415184259415\n",
      "Epoch 4979, Loss: 0.1496539656072855, Final Batch Loss: 0.03320726752281189\n",
      "Epoch 4980, Loss: 0.18413181230425835, Final Batch Loss: 0.036749426275491714\n",
      "Epoch 4981, Loss: 0.12998180277645588, Final Batch Loss: 0.03849022090435028\n",
      "Epoch 4982, Loss: 0.1593312807381153, Final Batch Loss: 0.05701901391148567\n",
      "Epoch 4983, Loss: 0.18309172987937927, Final Batch Loss: 0.07528915256261826\n",
      "Epoch 4984, Loss: 0.10584649536758661, Final Batch Loss: 0.01450327318161726\n",
      "Epoch 4985, Loss: 0.15063573233783245, Final Batch Loss: 0.06542704999446869\n",
      "Epoch 4986, Loss: 0.08576812967658043, Final Batch Loss: 0.010278096422553062\n",
      "Epoch 4987, Loss: 0.12651227973401546, Final Batch Loss: 0.014953600242733955\n",
      "Epoch 4988, Loss: 0.24679961428046227, Final Batch Loss: 0.03513661399483681\n",
      "Epoch 4989, Loss: 0.15518127661198378, Final Batch Loss: 0.009545705281198025\n",
      "Epoch 4990, Loss: 0.09085065498948097, Final Batch Loss: 0.016032816842198372\n",
      "Epoch 4991, Loss: 0.10052217449992895, Final Batch Loss: 0.009398004971444607\n",
      "Epoch 4992, Loss: 0.20042968355119228, Final Batch Loss: 0.06670036911964417\n",
      "Epoch 4993, Loss: 0.23233851790428162, Final Batch Loss: 0.09359358996152878\n",
      "Epoch 4994, Loss: 0.19126782193779945, Final Batch Loss: 0.04346059635281563\n",
      "Epoch 4995, Loss: 0.11144620599225163, Final Batch Loss: 0.00419960031285882\n",
      "Epoch 4996, Loss: 0.09892275184392929, Final Batch Loss: 0.04412712901830673\n",
      "Epoch 4997, Loss: 0.14325599744915962, Final Batch Loss: 0.012300319969654083\n",
      "Epoch 4998, Loss: 0.080819146707654, Final Batch Loss: 0.016487764194607735\n",
      "Epoch 4999, Loss: 0.1399026494473219, Final Batch Loss: 0.051052361726760864\n",
      "Epoch 5000, Loss: 0.1378037966787815, Final Batch Loss: 0.017401736229658127\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  2  1]\n",
      " [ 0 47  3]\n",
      " [ 0  0 45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.944     0.971        54\n",
      "           1      0.959     0.940     0.949        50\n",
      "           2      0.918     1.000     0.957        45\n",
      "\n",
      "    accuracy                          0.960       149\n",
      "   macro avg      0.959     0.961     0.959       149\n",
      "weighted avg      0.962     0.960     0.960       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 User Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
