{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [7, 8, 11]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.4215309619903564, Final Batch Loss: 2.2218258380889893\n",
      "Epoch 2, Loss: 4.403317928314209, Final Batch Loss: 2.1971335411071777\n",
      "Epoch 3, Loss: 4.397551774978638, Final Batch Loss: 2.1888062953948975\n",
      "Epoch 4, Loss: 4.396804094314575, Final Batch Loss: 2.1892781257629395\n",
      "Epoch 5, Loss: 4.407264232635498, Final Batch Loss: 2.211202621459961\n",
      "Epoch 6, Loss: 4.39528751373291, Final Batch Loss: 2.1955366134643555\n",
      "Epoch 7, Loss: 4.394120931625366, Final Batch Loss: 2.191605567932129\n",
      "Epoch 8, Loss: 4.409440517425537, Final Batch Loss: 2.2227532863616943\n",
      "Epoch 9, Loss: 4.392135143280029, Final Batch Loss: 2.197908639907837\n",
      "Epoch 10, Loss: 4.394420623779297, Final Batch Loss: 2.2086758613586426\n",
      "Epoch 11, Loss: 4.397554636001587, Final Batch Loss: 2.210569381713867\n",
      "Epoch 12, Loss: 4.37575626373291, Final Batch Loss: 2.1807048320770264\n",
      "Epoch 13, Loss: 4.383045673370361, Final Batch Loss: 2.1945455074310303\n",
      "Epoch 14, Loss: 4.3770434856414795, Final Batch Loss: 2.1971664428710938\n",
      "Epoch 15, Loss: 4.352653741836548, Final Batch Loss: 2.167448043823242\n",
      "Epoch 16, Loss: 4.340081214904785, Final Batch Loss: 2.1509716510772705\n",
      "Epoch 17, Loss: 4.345245838165283, Final Batch Loss: 2.166759967803955\n",
      "Epoch 18, Loss: 4.337540626525879, Final Batch Loss: 2.160632610321045\n",
      "Epoch 19, Loss: 4.328538656234741, Final Batch Loss: 2.1665918827056885\n",
      "Epoch 20, Loss: 4.298634767532349, Final Batch Loss: 2.12449049949646\n",
      "Epoch 21, Loss: 4.30071496963501, Final Batch Loss: 2.1402101516723633\n",
      "Epoch 22, Loss: 4.311263561248779, Final Batch Loss: 2.1669797897338867\n",
      "Epoch 23, Loss: 4.285498142242432, Final Batch Loss: 2.144165277481079\n",
      "Epoch 24, Loss: 4.285941123962402, Final Batch Loss: 2.152388572692871\n",
      "Epoch 25, Loss: 4.266810655593872, Final Batch Loss: 2.1495680809020996\n",
      "Epoch 26, Loss: 4.215130090713501, Final Batch Loss: 2.0913445949554443\n",
      "Epoch 27, Loss: 4.213433742523193, Final Batch Loss: 2.1022427082061768\n",
      "Epoch 28, Loss: 4.218405246734619, Final Batch Loss: 2.129673480987549\n",
      "Epoch 29, Loss: 4.184366464614868, Final Batch Loss: 2.103163719177246\n",
      "Epoch 30, Loss: 4.143791913986206, Final Batch Loss: 2.0636801719665527\n",
      "Epoch 31, Loss: 4.1400792598724365, Final Batch Loss: 2.0858798027038574\n",
      "Epoch 32, Loss: 4.080793142318726, Final Batch Loss: 2.0178117752075195\n",
      "Epoch 33, Loss: 4.0710530281066895, Final Batch Loss: 2.0333006381988525\n",
      "Epoch 34, Loss: 4.042498826980591, Final Batch Loss: 2.046855926513672\n",
      "Epoch 35, Loss: 4.001312851905823, Final Batch Loss: 2.0215609073638916\n",
      "Epoch 36, Loss: 3.9229483604431152, Final Batch Loss: 1.896981954574585\n",
      "Epoch 37, Loss: 3.952662229537964, Final Batch Loss: 1.9969019889831543\n",
      "Epoch 38, Loss: 3.90964674949646, Final Batch Loss: 1.971588134765625\n",
      "Epoch 39, Loss: 3.8142014741897583, Final Batch Loss: 1.8932068347930908\n",
      "Epoch 40, Loss: 3.7833210229873657, Final Batch Loss: 1.8766316175460815\n",
      "Epoch 41, Loss: 3.775586724281311, Final Batch Loss: 1.9036321640014648\n",
      "Epoch 42, Loss: 3.693847179412842, Final Batch Loss: 1.8183494806289673\n",
      "Epoch 43, Loss: 3.631660223007202, Final Batch Loss: 1.791828989982605\n",
      "Epoch 44, Loss: 3.6394827365875244, Final Batch Loss: 1.8550480604171753\n",
      "Epoch 45, Loss: 3.5924819707870483, Final Batch Loss: 1.8347716331481934\n",
      "Epoch 46, Loss: 3.5580501556396484, Final Batch Loss: 1.8088922500610352\n",
      "Epoch 47, Loss: 3.443290591239929, Final Batch Loss: 1.695299506187439\n",
      "Epoch 48, Loss: 3.4329752922058105, Final Batch Loss: 1.7469781637191772\n",
      "Epoch 49, Loss: 3.3401418924331665, Final Batch Loss: 1.6412583589553833\n",
      "Epoch 50, Loss: 3.3703848123550415, Final Batch Loss: 1.731008768081665\n",
      "Epoch 51, Loss: 3.249570369720459, Final Batch Loss: 1.5912599563598633\n",
      "Epoch 52, Loss: 3.3055925369262695, Final Batch Loss: 1.6587353944778442\n",
      "Epoch 53, Loss: 3.1418793201446533, Final Batch Loss: 1.5136312246322632\n",
      "Epoch 54, Loss: 3.0971779823303223, Final Batch Loss: 1.5092731714248657\n",
      "Epoch 55, Loss: 3.0552382469177246, Final Batch Loss: 1.5206189155578613\n",
      "Epoch 56, Loss: 3.108051896095276, Final Batch Loss: 1.5511059761047363\n",
      "Epoch 57, Loss: 2.9784337282180786, Final Batch Loss: 1.475454568862915\n",
      "Epoch 58, Loss: 2.9832299947738647, Final Batch Loss: 1.4512579441070557\n",
      "Epoch 59, Loss: 2.963757872581482, Final Batch Loss: 1.4311970472335815\n",
      "Epoch 60, Loss: 2.998753309249878, Final Batch Loss: 1.4981565475463867\n",
      "Epoch 61, Loss: 2.8612879514694214, Final Batch Loss: 1.40680730342865\n",
      "Epoch 62, Loss: 2.824765920639038, Final Batch Loss: 1.3385939598083496\n",
      "Epoch 63, Loss: 2.753688335418701, Final Batch Loss: 1.341078281402588\n",
      "Epoch 64, Loss: 2.8215290307998657, Final Batch Loss: 1.4121662378311157\n",
      "Epoch 65, Loss: 2.755839467048645, Final Batch Loss: 1.343930721282959\n",
      "Epoch 66, Loss: 2.836973547935486, Final Batch Loss: 1.3906477689743042\n",
      "Epoch 67, Loss: 2.790289521217346, Final Batch Loss: 1.4334055185317993\n",
      "Epoch 68, Loss: 2.827664613723755, Final Batch Loss: 1.4518309831619263\n",
      "Epoch 69, Loss: 2.735714912414551, Final Batch Loss: 1.3626813888549805\n",
      "Epoch 70, Loss: 2.7531514167785645, Final Batch Loss: 1.353386640548706\n",
      "Epoch 71, Loss: 2.686849355697632, Final Batch Loss: 1.3045438528060913\n",
      "Epoch 72, Loss: 2.7565969228744507, Final Batch Loss: 1.3722673654556274\n",
      "Epoch 73, Loss: 2.6496018171310425, Final Batch Loss: 1.3315556049346924\n",
      "Epoch 74, Loss: 2.673020601272583, Final Batch Loss: 1.3287012577056885\n",
      "Epoch 75, Loss: 2.6353330612182617, Final Batch Loss: 1.2679240703582764\n",
      "Epoch 76, Loss: 2.653599500656128, Final Batch Loss: 1.35664963722229\n",
      "Epoch 77, Loss: 2.5997992753982544, Final Batch Loss: 1.3210772275924683\n",
      "Epoch 78, Loss: 2.577671527862549, Final Batch Loss: 1.2399793863296509\n",
      "Epoch 79, Loss: 2.563716769218445, Final Batch Loss: 1.2680383920669556\n",
      "Epoch 80, Loss: 2.5704413652420044, Final Batch Loss: 1.264304757118225\n",
      "Epoch 81, Loss: 2.5322861671447754, Final Batch Loss: 1.2838709354400635\n",
      "Epoch 82, Loss: 2.429800510406494, Final Batch Loss: 1.2048734426498413\n",
      "Epoch 83, Loss: 2.4527173042297363, Final Batch Loss: 1.154505729675293\n",
      "Epoch 84, Loss: 2.4616527557373047, Final Batch Loss: 1.275322675704956\n",
      "Epoch 85, Loss: 2.494660258293152, Final Batch Loss: 1.259108066558838\n",
      "Epoch 86, Loss: 2.4624141454696655, Final Batch Loss: 1.2520430088043213\n",
      "Epoch 87, Loss: 2.3365797996520996, Final Batch Loss: 1.0788260698318481\n",
      "Epoch 88, Loss: 2.480608582496643, Final Batch Loss: 1.2930349111557007\n",
      "Epoch 89, Loss: 2.504987120628357, Final Batch Loss: 1.3167736530303955\n",
      "Epoch 90, Loss: 2.371339440345764, Final Batch Loss: 1.1912236213684082\n",
      "Epoch 91, Loss: 2.3605979681015015, Final Batch Loss: 1.1500028371810913\n",
      "Epoch 92, Loss: 2.3543211221694946, Final Batch Loss: 1.159536361694336\n",
      "Epoch 93, Loss: 2.4776450395584106, Final Batch Loss: 1.2421694993972778\n",
      "Epoch 94, Loss: 2.3791085481643677, Final Batch Loss: 1.2101377248764038\n",
      "Epoch 95, Loss: 2.3248218297958374, Final Batch Loss: 1.2310340404510498\n",
      "Epoch 96, Loss: 2.268661856651306, Final Batch Loss: 1.113702416419983\n",
      "Epoch 97, Loss: 2.29096782207489, Final Batch Loss: 1.1119303703308105\n",
      "Epoch 98, Loss: 2.4808269739151, Final Batch Loss: 1.3206647634506226\n",
      "Epoch 99, Loss: 2.403150200843811, Final Batch Loss: 1.2217837572097778\n",
      "Epoch 100, Loss: 2.2920572757720947, Final Batch Loss: 1.1636900901794434\n",
      "Epoch 101, Loss: 2.24532151222229, Final Batch Loss: 1.0783021450042725\n",
      "Epoch 102, Loss: 2.2619552612304688, Final Batch Loss: 1.104473352432251\n",
      "Epoch 103, Loss: 2.3245855569839478, Final Batch Loss: 1.2052580118179321\n",
      "Epoch 104, Loss: 2.276214599609375, Final Batch Loss: 1.1304303407669067\n",
      "Epoch 105, Loss: 2.262810707092285, Final Batch Loss: 1.1418302059173584\n",
      "Epoch 106, Loss: 2.1711442470550537, Final Batch Loss: 1.0497642755508423\n",
      "Epoch 107, Loss: 2.2155215740203857, Final Batch Loss: 1.108479619026184\n",
      "Epoch 108, Loss: 2.185189962387085, Final Batch Loss: 1.069324254989624\n",
      "Epoch 109, Loss: 2.1719290018081665, Final Batch Loss: 1.0455632209777832\n",
      "Epoch 110, Loss: 2.2277292013168335, Final Batch Loss: 1.113491415977478\n",
      "Epoch 111, Loss: 2.164161205291748, Final Batch Loss: 1.0508458614349365\n",
      "Epoch 112, Loss: 2.106491446495056, Final Batch Loss: 1.0554020404815674\n",
      "Epoch 113, Loss: 2.0736979246139526, Final Batch Loss: 1.0104787349700928\n",
      "Epoch 114, Loss: 2.1424407958984375, Final Batch Loss: 1.1036182641983032\n",
      "Epoch 115, Loss: 2.1050844192504883, Final Batch Loss: 1.0303473472595215\n",
      "Epoch 116, Loss: 2.075324237346649, Final Batch Loss: 1.0817253589630127\n",
      "Epoch 117, Loss: 2.0854532718658447, Final Batch Loss: 0.9953107833862305\n",
      "Epoch 118, Loss: 2.0983930826187134, Final Batch Loss: 1.0920383930206299\n",
      "Epoch 119, Loss: 2.1612765789031982, Final Batch Loss: 1.1113823652267456\n",
      "Epoch 120, Loss: 2.134834408760071, Final Batch Loss: 1.0120365619659424\n",
      "Epoch 121, Loss: 2.1092673540115356, Final Batch Loss: 1.1032912731170654\n",
      "Epoch 122, Loss: 1.9567033648490906, Final Batch Loss: 0.9062981009483337\n",
      "Epoch 123, Loss: 2.098466992378235, Final Batch Loss: 1.023651123046875\n",
      "Epoch 124, Loss: 2.049920082092285, Final Batch Loss: 1.004609227180481\n",
      "Epoch 125, Loss: 2.1131691932678223, Final Batch Loss: 1.045057773590088\n",
      "Epoch 126, Loss: 2.07064950466156, Final Batch Loss: 1.0349693298339844\n",
      "Epoch 127, Loss: 2.0963770151138306, Final Batch Loss: 1.0434858798980713\n",
      "Epoch 128, Loss: 2.073494791984558, Final Batch Loss: 1.0367978811264038\n",
      "Epoch 129, Loss: 2.173547863960266, Final Batch Loss: 1.1249197721481323\n",
      "Epoch 130, Loss: 2.1197075247764587, Final Batch Loss: 1.1211580038070679\n",
      "Epoch 131, Loss: 2.098330020904541, Final Batch Loss: 1.0294110774993896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, Loss: 1.9846792817115784, Final Batch Loss: 0.963965117931366\n",
      "Epoch 133, Loss: 2.060519814491272, Final Batch Loss: 1.080288290977478\n",
      "Epoch 134, Loss: 2.0446518659591675, Final Batch Loss: 1.0149414539337158\n",
      "Epoch 135, Loss: 2.017331838607788, Final Batch Loss: 1.004321813583374\n",
      "Epoch 136, Loss: 1.9706460237503052, Final Batch Loss: 0.9676200151443481\n",
      "Epoch 137, Loss: 2.044661521911621, Final Batch Loss: 1.0907007455825806\n",
      "Epoch 138, Loss: 2.0276201367378235, Final Batch Loss: 1.0592381954193115\n",
      "Epoch 139, Loss: 2.120629072189331, Final Batch Loss: 1.0860201120376587\n",
      "Epoch 140, Loss: 1.9853315949440002, Final Batch Loss: 0.9795411229133606\n",
      "Epoch 141, Loss: 1.9286714792251587, Final Batch Loss: 0.9375386834144592\n",
      "Epoch 142, Loss: 1.9493716359138489, Final Batch Loss: 0.9706511497497559\n",
      "Epoch 143, Loss: 2.0741665959358215, Final Batch Loss: 1.1200658082962036\n",
      "Epoch 144, Loss: 1.9832167029380798, Final Batch Loss: 0.9951614141464233\n",
      "Epoch 145, Loss: 1.9489283561706543, Final Batch Loss: 0.9617593884468079\n",
      "Epoch 146, Loss: 1.9954880475997925, Final Batch Loss: 1.0165809392929077\n",
      "Epoch 147, Loss: 1.9874780774116516, Final Batch Loss: 1.0216748714447021\n",
      "Epoch 148, Loss: 2.063673973083496, Final Batch Loss: 1.0439509153366089\n",
      "Epoch 149, Loss: 1.9330846667289734, Final Batch Loss: 0.9373651146888733\n",
      "Epoch 150, Loss: 1.9134031534194946, Final Batch Loss: 0.9694771766662598\n",
      "Epoch 151, Loss: 1.8949840664863586, Final Batch Loss: 0.9746689796447754\n",
      "Epoch 152, Loss: 1.9224873781204224, Final Batch Loss: 0.9766176342964172\n",
      "Epoch 153, Loss: 1.8438371419906616, Final Batch Loss: 0.8810492157936096\n",
      "Epoch 154, Loss: 2.022104859352112, Final Batch Loss: 0.9949779510498047\n",
      "Epoch 155, Loss: 1.8958945870399475, Final Batch Loss: 0.9724655747413635\n",
      "Epoch 156, Loss: 1.8370392322540283, Final Batch Loss: 0.8943896889686584\n",
      "Epoch 157, Loss: 1.8647862672805786, Final Batch Loss: 0.9393262267112732\n",
      "Epoch 158, Loss: 1.8077306747436523, Final Batch Loss: 0.8624417781829834\n",
      "Epoch 159, Loss: 1.7832404375076294, Final Batch Loss: 0.7760211229324341\n",
      "Epoch 160, Loss: 1.8682520985603333, Final Batch Loss: 0.8956366777420044\n",
      "Epoch 161, Loss: 1.8790605664253235, Final Batch Loss: 0.9452505111694336\n",
      "Epoch 162, Loss: 1.8840976357460022, Final Batch Loss: 0.9481236934661865\n",
      "Epoch 163, Loss: 1.7938616275787354, Final Batch Loss: 0.8594580888748169\n",
      "Epoch 164, Loss: 1.9144456386566162, Final Batch Loss: 1.012715220451355\n",
      "Epoch 165, Loss: 1.9227039813995361, Final Batch Loss: 1.0417847633361816\n",
      "Epoch 166, Loss: 1.9410519003868103, Final Batch Loss: 0.9998688697814941\n",
      "Epoch 167, Loss: 1.9358364343643188, Final Batch Loss: 1.0367348194122314\n",
      "Epoch 168, Loss: 1.8616668581962585, Final Batch Loss: 0.9523077607154846\n",
      "Epoch 169, Loss: 1.828481674194336, Final Batch Loss: 0.9051055908203125\n",
      "Epoch 170, Loss: 1.8826953768730164, Final Batch Loss: 0.9655352830886841\n",
      "Epoch 171, Loss: 1.8248311281204224, Final Batch Loss: 0.9064297676086426\n",
      "Epoch 172, Loss: 1.8742120265960693, Final Batch Loss: 0.9576488733291626\n",
      "Epoch 173, Loss: 1.7907320857048035, Final Batch Loss: 0.8714184165000916\n",
      "Epoch 174, Loss: 1.6231445670127869, Final Batch Loss: 0.7339960336685181\n",
      "Epoch 175, Loss: 1.7209245562553406, Final Batch Loss: 0.8264254331588745\n",
      "Epoch 176, Loss: 1.73872709274292, Final Batch Loss: 0.8567715287208557\n",
      "Epoch 177, Loss: 1.7472909092903137, Final Batch Loss: 0.8676097989082336\n",
      "Epoch 178, Loss: 1.7728436589241028, Final Batch Loss: 0.8366717100143433\n",
      "Epoch 179, Loss: 1.7576719522476196, Final Batch Loss: 0.8234695792198181\n",
      "Epoch 180, Loss: 1.8248482942581177, Final Batch Loss: 0.8674423098564148\n",
      "Epoch 181, Loss: 1.8801828026771545, Final Batch Loss: 0.9333575367927551\n",
      "Epoch 182, Loss: 1.864947259426117, Final Batch Loss: 0.9295075535774231\n",
      "Epoch 183, Loss: 1.780511736869812, Final Batch Loss: 0.8535921573638916\n",
      "Epoch 184, Loss: 1.8323509097099304, Final Batch Loss: 0.9899282455444336\n",
      "Epoch 185, Loss: 1.7543981671333313, Final Batch Loss: 0.8650873899459839\n",
      "Epoch 186, Loss: 1.763814389705658, Final Batch Loss: 0.9004360437393188\n",
      "Epoch 187, Loss: 1.680440366268158, Final Batch Loss: 0.8323139548301697\n",
      "Epoch 188, Loss: 1.7120487689971924, Final Batch Loss: 0.8462239503860474\n",
      "Epoch 189, Loss: 1.8240805864334106, Final Batch Loss: 0.9508803486824036\n",
      "Epoch 190, Loss: 1.7047744393348694, Final Batch Loss: 0.876856803894043\n",
      "Epoch 191, Loss: 1.7949859499931335, Final Batch Loss: 0.8945577144622803\n",
      "Epoch 192, Loss: 1.717062771320343, Final Batch Loss: 0.906575620174408\n",
      "Epoch 193, Loss: 1.829873263835907, Final Batch Loss: 0.9399414658546448\n",
      "Epoch 194, Loss: 1.7136227488517761, Final Batch Loss: 0.8421239256858826\n",
      "Epoch 195, Loss: 1.6952559351921082, Final Batch Loss: 0.830044150352478\n",
      "Epoch 196, Loss: 1.6954187750816345, Final Batch Loss: 0.8853540420532227\n",
      "Epoch 197, Loss: 1.7868637442588806, Final Batch Loss: 0.9018932580947876\n",
      "Epoch 198, Loss: 1.7132630348205566, Final Batch Loss: 0.8385463356971741\n",
      "Epoch 199, Loss: 1.7998616099357605, Final Batch Loss: 0.890451967716217\n",
      "Epoch 200, Loss: 1.717439591884613, Final Batch Loss: 0.8420032262802124\n",
      "Epoch 201, Loss: 1.6143453121185303, Final Batch Loss: 0.8141415119171143\n",
      "Epoch 202, Loss: 1.6880725026130676, Final Batch Loss: 0.8420003652572632\n",
      "Epoch 203, Loss: 1.6840254068374634, Final Batch Loss: 0.8300775289535522\n",
      "Epoch 204, Loss: 1.6105056405067444, Final Batch Loss: 0.7524222731590271\n",
      "Epoch 205, Loss: 1.7061039805412292, Final Batch Loss: 0.9241582751274109\n",
      "Epoch 206, Loss: 1.7149413228034973, Final Batch Loss: 0.8839948177337646\n",
      "Epoch 207, Loss: 1.6989265084266663, Final Batch Loss: 0.8783917427062988\n",
      "Epoch 208, Loss: 1.594773292541504, Final Batch Loss: 0.8110455274581909\n",
      "Epoch 209, Loss: 1.6521709561347961, Final Batch Loss: 0.7776989936828613\n",
      "Epoch 210, Loss: 1.6976242661476135, Final Batch Loss: 0.8603573441505432\n",
      "Epoch 211, Loss: 1.5946826338768005, Final Batch Loss: 0.7345153093338013\n",
      "Epoch 212, Loss: 1.6974876523017883, Final Batch Loss: 0.8935869336128235\n",
      "Epoch 213, Loss: 1.6158104538917542, Final Batch Loss: 0.7454055547714233\n",
      "Epoch 214, Loss: 1.5734115839004517, Final Batch Loss: 0.7752564549446106\n",
      "Epoch 215, Loss: 1.7158904671669006, Final Batch Loss: 0.8643191456794739\n",
      "Epoch 216, Loss: 1.692857563495636, Final Batch Loss: 0.8093204498291016\n",
      "Epoch 217, Loss: 1.6001461148262024, Final Batch Loss: 0.7866565585136414\n",
      "Epoch 218, Loss: 1.5961084365844727, Final Batch Loss: 0.7557986974716187\n",
      "Epoch 219, Loss: 1.7142746448516846, Final Batch Loss: 0.9078012108802795\n",
      "Epoch 220, Loss: 1.6470292806625366, Final Batch Loss: 0.8527431488037109\n",
      "Epoch 221, Loss: 1.6308928728103638, Final Batch Loss: 0.8149746060371399\n",
      "Epoch 222, Loss: 1.6629220247268677, Final Batch Loss: 0.831173300743103\n",
      "Epoch 223, Loss: 1.5398576855659485, Final Batch Loss: 0.8022044897079468\n",
      "Epoch 224, Loss: 1.6265466213226318, Final Batch Loss: 0.7667989134788513\n",
      "Epoch 225, Loss: 1.7144174575805664, Final Batch Loss: 0.9005512595176697\n",
      "Epoch 226, Loss: 1.5485853552818298, Final Batch Loss: 0.7795059680938721\n",
      "Epoch 227, Loss: 1.7485100626945496, Final Batch Loss: 0.9303591847419739\n",
      "Epoch 228, Loss: 1.523120939731598, Final Batch Loss: 0.6996819376945496\n",
      "Epoch 229, Loss: 1.6436828970909119, Final Batch Loss: 0.8568606376647949\n",
      "Epoch 230, Loss: 1.6050326228141785, Final Batch Loss: 0.8157803416252136\n",
      "Epoch 231, Loss: 1.6839905381202698, Final Batch Loss: 0.8890023231506348\n",
      "Epoch 232, Loss: 1.7034143805503845, Final Batch Loss: 0.9044674038887024\n",
      "Epoch 233, Loss: 1.5953285098075867, Final Batch Loss: 0.8108512163162231\n",
      "Epoch 234, Loss: 1.5101160407066345, Final Batch Loss: 0.7290518283843994\n",
      "Epoch 235, Loss: 1.6708757281303406, Final Batch Loss: 0.8073086738586426\n",
      "Epoch 236, Loss: 1.6507514715194702, Final Batch Loss: 0.8429147601127625\n",
      "Epoch 237, Loss: 1.670616328716278, Final Batch Loss: 0.8898201584815979\n",
      "Epoch 238, Loss: 1.5783916115760803, Final Batch Loss: 0.8091580867767334\n",
      "Epoch 239, Loss: 1.5351179838180542, Final Batch Loss: 0.7589911818504333\n",
      "Epoch 240, Loss: 1.489562749862671, Final Batch Loss: 0.6750109195709229\n",
      "Epoch 241, Loss: 1.4880484342575073, Final Batch Loss: 0.7070984244346619\n",
      "Epoch 242, Loss: 1.5524533987045288, Final Batch Loss: 0.7641993165016174\n",
      "Epoch 243, Loss: 1.5426834225654602, Final Batch Loss: 0.7734158039093018\n",
      "Epoch 244, Loss: 1.5870836973190308, Final Batch Loss: 0.7765356302261353\n",
      "Epoch 245, Loss: 1.5719034671783447, Final Batch Loss: 0.8260643482208252\n",
      "Epoch 246, Loss: 1.6017868518829346, Final Batch Loss: 0.7937989234924316\n",
      "Epoch 247, Loss: 1.546520173549652, Final Batch Loss: 0.8164222240447998\n",
      "Epoch 248, Loss: 1.550916075706482, Final Batch Loss: 0.768571674823761\n",
      "Epoch 249, Loss: 1.5508107542991638, Final Batch Loss: 0.7325546145439148\n",
      "Epoch 250, Loss: 1.5784833431243896, Final Batch Loss: 0.7558850646018982\n",
      "Epoch 251, Loss: 1.5889087319374084, Final Batch Loss: 0.8242169618606567\n",
      "Epoch 252, Loss: 1.5127668380737305, Final Batch Loss: 0.7601652145385742\n",
      "Epoch 253, Loss: 1.5740979313850403, Final Batch Loss: 0.7639374732971191\n",
      "Epoch 254, Loss: 1.499859631061554, Final Batch Loss: 0.7232419848442078\n",
      "Epoch 255, Loss: 1.5367060899734497, Final Batch Loss: 0.739523708820343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256, Loss: 1.430402159690857, Final Batch Loss: 0.6922003030776978\n",
      "Epoch 257, Loss: 1.5355114340782166, Final Batch Loss: 0.7725788354873657\n",
      "Epoch 258, Loss: 1.5034218430519104, Final Batch Loss: 0.7414293885231018\n",
      "Epoch 259, Loss: 1.5344973802566528, Final Batch Loss: 0.8008553981781006\n",
      "Epoch 260, Loss: 1.5504257678985596, Final Batch Loss: 0.8184704184532166\n",
      "Epoch 261, Loss: 1.6314207315444946, Final Batch Loss: 0.8651587963104248\n",
      "Epoch 262, Loss: 1.4177204966545105, Final Batch Loss: 0.6638674736022949\n",
      "Epoch 263, Loss: 1.4141563177108765, Final Batch Loss: 0.6687158346176147\n",
      "Epoch 264, Loss: 1.4679888486862183, Final Batch Loss: 0.713234543800354\n",
      "Epoch 265, Loss: 1.5730953812599182, Final Batch Loss: 0.7577569484710693\n",
      "Epoch 266, Loss: 1.591762363910675, Final Batch Loss: 0.7781559228897095\n",
      "Epoch 267, Loss: 1.4339736104011536, Final Batch Loss: 0.6564956903457642\n",
      "Epoch 268, Loss: 1.5562406182289124, Final Batch Loss: 0.7862505316734314\n",
      "Epoch 269, Loss: 1.5504652857780457, Final Batch Loss: 0.7928656935691833\n",
      "Epoch 270, Loss: 1.507296085357666, Final Batch Loss: 0.7473315596580505\n",
      "Epoch 271, Loss: 1.5688708424568176, Final Batch Loss: 0.7816912531852722\n",
      "Epoch 272, Loss: 1.3831101059913635, Final Batch Loss: 0.6734707355499268\n",
      "Epoch 273, Loss: 1.3778566122055054, Final Batch Loss: 0.6649993062019348\n",
      "Epoch 274, Loss: 1.5521517992019653, Final Batch Loss: 0.7373095154762268\n",
      "Epoch 275, Loss: 1.5051493048667908, Final Batch Loss: 0.813866913318634\n",
      "Epoch 276, Loss: 1.5244407653808594, Final Batch Loss: 0.7920956611633301\n",
      "Epoch 277, Loss: 1.5082632899284363, Final Batch Loss: 0.697076678276062\n",
      "Epoch 278, Loss: 1.516718327999115, Final Batch Loss: 0.7794909477233887\n",
      "Epoch 279, Loss: 1.441267192363739, Final Batch Loss: 0.6402648687362671\n",
      "Epoch 280, Loss: 1.4524149298667908, Final Batch Loss: 0.7762998342514038\n",
      "Epoch 281, Loss: 1.4888747334480286, Final Batch Loss: 0.7341891527175903\n",
      "Epoch 282, Loss: 1.5046496391296387, Final Batch Loss: 0.8167254328727722\n",
      "Epoch 283, Loss: 1.5087621212005615, Final Batch Loss: 0.7835565805435181\n",
      "Epoch 284, Loss: 1.394732654094696, Final Batch Loss: 0.6837838888168335\n",
      "Epoch 285, Loss: 1.5981311798095703, Final Batch Loss: 0.879705548286438\n",
      "Epoch 286, Loss: 1.5281376242637634, Final Batch Loss: 0.7861634492874146\n",
      "Epoch 287, Loss: 1.443259835243225, Final Batch Loss: 0.7017558813095093\n",
      "Epoch 288, Loss: 1.3555768132209778, Final Batch Loss: 0.6083776354789734\n",
      "Epoch 289, Loss: 1.4291074872016907, Final Batch Loss: 0.6887856721878052\n",
      "Epoch 290, Loss: 1.4813250303268433, Final Batch Loss: 0.7485275268554688\n",
      "Epoch 291, Loss: 1.5857662558555603, Final Batch Loss: 0.8839807510375977\n",
      "Epoch 292, Loss: 1.5054758787155151, Final Batch Loss: 0.8238080739974976\n",
      "Epoch 293, Loss: 1.4478085041046143, Final Batch Loss: 0.7112085819244385\n",
      "Epoch 294, Loss: 1.446925938129425, Final Batch Loss: 0.7068191766738892\n",
      "Epoch 295, Loss: 1.407864511013031, Final Batch Loss: 0.7398523092269897\n",
      "Epoch 296, Loss: 1.324781596660614, Final Batch Loss: 0.6545649170875549\n",
      "Epoch 297, Loss: 1.387154996395111, Final Batch Loss: 0.6800188422203064\n",
      "Epoch 298, Loss: 1.5172137022018433, Final Batch Loss: 0.7563962936401367\n",
      "Epoch 299, Loss: 1.4825862646102905, Final Batch Loss: 0.7197247743606567\n",
      "Epoch 300, Loss: 1.3711549043655396, Final Batch Loss: 0.6710737943649292\n",
      "Epoch 301, Loss: 1.5156678557395935, Final Batch Loss: 0.8244425654411316\n",
      "Epoch 302, Loss: 1.5009675025939941, Final Batch Loss: 0.7742629647254944\n",
      "Epoch 303, Loss: 1.5328874588012695, Final Batch Loss: 0.8338807225227356\n",
      "Epoch 304, Loss: 1.4616069197654724, Final Batch Loss: 0.6834844350814819\n",
      "Epoch 305, Loss: 1.3912663459777832, Final Batch Loss: 0.6580556631088257\n",
      "Epoch 306, Loss: 1.3768370151519775, Final Batch Loss: 0.6476183533668518\n",
      "Epoch 307, Loss: 1.3736953139305115, Final Batch Loss: 0.6716006994247437\n",
      "Epoch 308, Loss: 1.4562864899635315, Final Batch Loss: 0.6682288646697998\n",
      "Epoch 309, Loss: 1.4143543243408203, Final Batch Loss: 0.7076693773269653\n",
      "Epoch 310, Loss: 1.417555809020996, Final Batch Loss: 0.73619145154953\n",
      "Epoch 311, Loss: 1.5453683733940125, Final Batch Loss: 0.8026294112205505\n",
      "Epoch 312, Loss: 1.4059492945671082, Final Batch Loss: 0.7038654088973999\n",
      "Epoch 313, Loss: 1.4263063073158264, Final Batch Loss: 0.7251070141792297\n",
      "Epoch 314, Loss: 1.4453335404396057, Final Batch Loss: 0.7166027426719666\n",
      "Epoch 315, Loss: 1.3699662685394287, Final Batch Loss: 0.6757684350013733\n",
      "Epoch 316, Loss: 1.2957400679588318, Final Batch Loss: 0.6201200485229492\n",
      "Epoch 317, Loss: 1.336990475654602, Final Batch Loss: 0.6511194109916687\n",
      "Epoch 318, Loss: 1.4556153416633606, Final Batch Loss: 0.7435708045959473\n",
      "Epoch 319, Loss: 1.3199285864830017, Final Batch Loss: 0.6726052165031433\n",
      "Epoch 320, Loss: 1.391278326511383, Final Batch Loss: 0.7494787573814392\n",
      "Epoch 321, Loss: 1.3463695049285889, Final Batch Loss: 0.6519325971603394\n",
      "Epoch 322, Loss: 1.3397759795188904, Final Batch Loss: 0.6233701109886169\n",
      "Epoch 323, Loss: 1.3544695377349854, Final Batch Loss: 0.6553652286529541\n",
      "Epoch 324, Loss: 1.2736608982086182, Final Batch Loss: 0.6200118660926819\n",
      "Epoch 325, Loss: 1.3925206661224365, Final Batch Loss: 0.6909244656562805\n",
      "Epoch 326, Loss: 1.4681950211524963, Final Batch Loss: 0.7689458727836609\n",
      "Epoch 327, Loss: 1.401968777179718, Final Batch Loss: 0.6883320212364197\n",
      "Epoch 328, Loss: 1.39142644405365, Final Batch Loss: 0.6681832671165466\n",
      "Epoch 329, Loss: 1.3599551320075989, Final Batch Loss: 0.6608959436416626\n",
      "Epoch 330, Loss: 1.3392812609672546, Final Batch Loss: 0.6984825730323792\n",
      "Epoch 331, Loss: 1.4154564142227173, Final Batch Loss: 0.6777201294898987\n",
      "Epoch 332, Loss: 1.2952678799629211, Final Batch Loss: 0.6422117948532104\n",
      "Epoch 333, Loss: 1.3703795075416565, Final Batch Loss: 0.6829401850700378\n",
      "Epoch 334, Loss: 1.4444212317466736, Final Batch Loss: 0.7865567803382874\n",
      "Epoch 335, Loss: 1.3772559762001038, Final Batch Loss: 0.7089157104492188\n",
      "Epoch 336, Loss: 1.4869451522827148, Final Batch Loss: 0.7461476922035217\n",
      "Epoch 337, Loss: 1.3536837697029114, Final Batch Loss: 0.6667789220809937\n",
      "Epoch 338, Loss: 1.27389395236969, Final Batch Loss: 0.6406167149543762\n",
      "Epoch 339, Loss: 1.3234955668449402, Final Batch Loss: 0.6863687634468079\n",
      "Epoch 340, Loss: 1.3711287379264832, Final Batch Loss: 0.7070967555046082\n",
      "Epoch 341, Loss: 1.3952723145484924, Final Batch Loss: 0.7463662624359131\n",
      "Epoch 342, Loss: 1.4934585094451904, Final Batch Loss: 0.7543914318084717\n",
      "Epoch 343, Loss: 1.412320613861084, Final Batch Loss: 0.6891126036643982\n",
      "Epoch 344, Loss: 1.3503329753875732, Final Batch Loss: 0.6474642753601074\n",
      "Epoch 345, Loss: 1.2679231762886047, Final Batch Loss: 0.5836261510848999\n",
      "Epoch 346, Loss: 1.3534544706344604, Final Batch Loss: 0.709641695022583\n",
      "Epoch 347, Loss: 1.3714109659194946, Final Batch Loss: 0.6440466046333313\n",
      "Epoch 348, Loss: 1.322447121143341, Final Batch Loss: 0.6990170478820801\n",
      "Epoch 349, Loss: 1.303272306919098, Final Batch Loss: 0.625495195388794\n",
      "Epoch 350, Loss: 1.3344139456748962, Final Batch Loss: 0.6509989500045776\n",
      "Epoch 351, Loss: 1.3266114592552185, Final Batch Loss: 0.7030590176582336\n",
      "Epoch 352, Loss: 1.269339144229889, Final Batch Loss: 0.5425325036048889\n",
      "Epoch 353, Loss: 1.3606921434402466, Final Batch Loss: 0.6531590819358826\n",
      "Epoch 354, Loss: 1.3256170153617859, Final Batch Loss: 0.6254075169563293\n",
      "Epoch 355, Loss: 1.3121306896209717, Final Batch Loss: 0.6398646235466003\n",
      "Epoch 356, Loss: 1.341131865978241, Final Batch Loss: 0.6942718029022217\n",
      "Epoch 357, Loss: 1.3127523064613342, Final Batch Loss: 0.6304461359977722\n",
      "Epoch 358, Loss: 1.414304494857788, Final Batch Loss: 0.7004215121269226\n",
      "Epoch 359, Loss: 1.3115386366844177, Final Batch Loss: 0.5452787280082703\n",
      "Epoch 360, Loss: 1.413303792476654, Final Batch Loss: 0.7132407426834106\n",
      "Epoch 361, Loss: 1.3069756627082825, Final Batch Loss: 0.640847384929657\n",
      "Epoch 362, Loss: 1.2612173557281494, Final Batch Loss: 0.6153985261917114\n",
      "Epoch 363, Loss: 1.3866857886314392, Final Batch Loss: 0.7264708280563354\n",
      "Epoch 364, Loss: 1.292923629283905, Final Batch Loss: 0.6725597381591797\n",
      "Epoch 365, Loss: 1.3044930696487427, Final Batch Loss: 0.6758336424827576\n",
      "Epoch 366, Loss: 1.4566974639892578, Final Batch Loss: 0.7411638498306274\n",
      "Epoch 367, Loss: 1.3527504801750183, Final Batch Loss: 0.6835370063781738\n",
      "Epoch 368, Loss: 1.300648033618927, Final Batch Loss: 0.6082505583763123\n",
      "Epoch 369, Loss: 1.3950796723365784, Final Batch Loss: 0.6483063101768494\n",
      "Epoch 370, Loss: 1.249953269958496, Final Batch Loss: 0.6115272045135498\n",
      "Epoch 371, Loss: 1.264782965183258, Final Batch Loss: 0.6726729869842529\n",
      "Epoch 372, Loss: 1.3232375383377075, Final Batch Loss: 0.6670953631401062\n",
      "Epoch 373, Loss: 1.2800758481025696, Final Batch Loss: 0.6282840371131897\n",
      "Epoch 374, Loss: 1.3689096570014954, Final Batch Loss: 0.6091084480285645\n",
      "Epoch 375, Loss: 1.276659905910492, Final Batch Loss: 0.6636627316474915\n",
      "Epoch 376, Loss: 1.340294063091278, Final Batch Loss: 0.6497949361801147\n",
      "Epoch 377, Loss: 1.2456398010253906, Final Batch Loss: 0.566889226436615\n",
      "Epoch 378, Loss: 1.2212881445884705, Final Batch Loss: 0.5880399346351624\n",
      "Epoch 379, Loss: 1.2266948223114014, Final Batch Loss: 0.587913990020752\n",
      "Epoch 380, Loss: 1.296734631061554, Final Batch Loss: 0.6337429881095886\n",
      "Epoch 381, Loss: 1.2716889381408691, Final Batch Loss: 0.6548910737037659\n",
      "Epoch 382, Loss: 1.4096503257751465, Final Batch Loss: 0.7166933417320251\n",
      "Epoch 383, Loss: 1.276840627193451, Final Batch Loss: 0.6366931200027466\n",
      "Epoch 384, Loss: 1.338758647441864, Final Batch Loss: 0.7087475657463074\n",
      "Epoch 385, Loss: 1.4291773438453674, Final Batch Loss: 0.7305465340614319\n",
      "Epoch 386, Loss: 1.3165398836135864, Final Batch Loss: 0.6154331564903259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387, Loss: 1.2279903292655945, Final Batch Loss: 0.5793604254722595\n",
      "Epoch 388, Loss: 1.2662855386734009, Final Batch Loss: 0.5819966197013855\n",
      "Epoch 389, Loss: 1.2296691536903381, Final Batch Loss: 0.6289507150650024\n",
      "Epoch 390, Loss: 1.2545166015625, Final Batch Loss: 0.6296453475952148\n",
      "Epoch 391, Loss: 1.3731114268302917, Final Batch Loss: 0.7760433554649353\n",
      "Epoch 392, Loss: 1.2833396196365356, Final Batch Loss: 0.6412765979766846\n",
      "Epoch 393, Loss: 1.2306493520736694, Final Batch Loss: 0.5745717883110046\n",
      "Epoch 394, Loss: 1.339910089969635, Final Batch Loss: 0.6641918420791626\n",
      "Epoch 395, Loss: 1.1911677122116089, Final Batch Loss: 0.5976247787475586\n",
      "Epoch 396, Loss: 1.3392083048820496, Final Batch Loss: 0.6854782700538635\n",
      "Epoch 397, Loss: 1.3519300818443298, Final Batch Loss: 0.6590479612350464\n",
      "Epoch 398, Loss: 1.2397579550743103, Final Batch Loss: 0.6330707669258118\n",
      "Epoch 399, Loss: 1.2350067496299744, Final Batch Loss: 0.5810716152191162\n",
      "Epoch 400, Loss: 1.2904295325279236, Final Batch Loss: 0.649077832698822\n",
      "Epoch 401, Loss: 1.3516004085540771, Final Batch Loss: 0.6854040026664734\n",
      "Epoch 402, Loss: 1.237030029296875, Final Batch Loss: 0.5092814564704895\n",
      "Epoch 403, Loss: 1.2756364345550537, Final Batch Loss: 0.696722686290741\n",
      "Epoch 404, Loss: 1.3507490754127502, Final Batch Loss: 0.7380179762840271\n",
      "Epoch 405, Loss: 1.2995717525482178, Final Batch Loss: 0.6844099760055542\n",
      "Epoch 406, Loss: 1.274185299873352, Final Batch Loss: 0.6139316558837891\n",
      "Epoch 407, Loss: 1.2990140914916992, Final Batch Loss: 0.6178969144821167\n",
      "Epoch 408, Loss: 1.2333397269248962, Final Batch Loss: 0.6116277575492859\n",
      "Epoch 409, Loss: 1.3019213676452637, Final Batch Loss: 0.6749524474143982\n",
      "Epoch 410, Loss: 1.2410812973976135, Final Batch Loss: 0.5914347171783447\n",
      "Epoch 411, Loss: 1.3020464777946472, Final Batch Loss: 0.6336304545402527\n",
      "Epoch 412, Loss: 1.2893438339233398, Final Batch Loss: 0.6315979361534119\n",
      "Epoch 413, Loss: 1.1689351797103882, Final Batch Loss: 0.5971409678459167\n",
      "Epoch 414, Loss: 1.1513822078704834, Final Batch Loss: 0.5132797956466675\n",
      "Epoch 415, Loss: 1.4601620435714722, Final Batch Loss: 0.8143883943557739\n",
      "Epoch 416, Loss: 1.2980440258979797, Final Batch Loss: 0.6654354333877563\n",
      "Epoch 417, Loss: 1.3335962891578674, Final Batch Loss: 0.7045096755027771\n",
      "Epoch 418, Loss: 1.2232064008712769, Final Batch Loss: 0.640738308429718\n",
      "Epoch 419, Loss: 1.389847755432129, Final Batch Loss: 0.7706281542778015\n",
      "Epoch 420, Loss: 1.2675277590751648, Final Batch Loss: 0.6487343907356262\n",
      "Epoch 421, Loss: 1.2934903502464294, Final Batch Loss: 0.6169800162315369\n",
      "Epoch 422, Loss: 1.2580799460411072, Final Batch Loss: 0.6355854272842407\n",
      "Epoch 423, Loss: 1.2354835271835327, Final Batch Loss: 0.6270303130149841\n",
      "Epoch 424, Loss: 1.331682026386261, Final Batch Loss: 0.687416672706604\n",
      "Epoch 425, Loss: 1.2535223364830017, Final Batch Loss: 0.618762195110321\n",
      "Epoch 426, Loss: 1.2173699736595154, Final Batch Loss: 0.5861078500747681\n",
      "Epoch 427, Loss: 1.341694414615631, Final Batch Loss: 0.7181351780891418\n",
      "Epoch 428, Loss: 1.2281920909881592, Final Batch Loss: 0.6424627900123596\n",
      "Epoch 429, Loss: 1.2082403898239136, Final Batch Loss: 0.6011024713516235\n",
      "Epoch 430, Loss: 1.2758858799934387, Final Batch Loss: 0.6436128616333008\n",
      "Epoch 431, Loss: 1.249910056591034, Final Batch Loss: 0.591436505317688\n",
      "Epoch 432, Loss: 1.2247450351715088, Final Batch Loss: 0.6075302958488464\n",
      "Epoch 433, Loss: 1.2964253425598145, Final Batch Loss: 0.6533327102661133\n",
      "Epoch 434, Loss: 1.2926133275032043, Final Batch Loss: 0.6849724650382996\n",
      "Epoch 435, Loss: 1.389909267425537, Final Batch Loss: 0.7443104982376099\n",
      "Epoch 436, Loss: 1.3131266832351685, Final Batch Loss: 0.6493822336196899\n",
      "Epoch 437, Loss: 1.1688734889030457, Final Batch Loss: 0.5454537272453308\n",
      "Epoch 438, Loss: 1.2331408262252808, Final Batch Loss: 0.679184079170227\n",
      "Epoch 439, Loss: 1.3045774698257446, Final Batch Loss: 0.7293950915336609\n",
      "Epoch 440, Loss: 1.1225412487983704, Final Batch Loss: 0.5409444570541382\n",
      "Epoch 441, Loss: 1.1617445349693298, Final Batch Loss: 0.5445545315742493\n",
      "Epoch 442, Loss: 1.1979095935821533, Final Batch Loss: 0.5828774571418762\n",
      "Epoch 443, Loss: 1.2049656510353088, Final Batch Loss: 0.658076286315918\n",
      "Epoch 444, Loss: 1.190822958946228, Final Batch Loss: 0.5421319603919983\n",
      "Epoch 445, Loss: 1.2541786432266235, Final Batch Loss: 0.6541756391525269\n",
      "Epoch 446, Loss: 1.2576503157615662, Final Batch Loss: 0.6691738963127136\n",
      "Epoch 447, Loss: 1.3107839226722717, Final Batch Loss: 0.6888659596443176\n",
      "Epoch 448, Loss: 1.256485104560852, Final Batch Loss: 0.5792349576950073\n",
      "Epoch 449, Loss: 1.2387147545814514, Final Batch Loss: 0.6627391576766968\n",
      "Epoch 450, Loss: 1.247148334980011, Final Batch Loss: 0.6209462881088257\n",
      "Epoch 451, Loss: 1.2163533568382263, Final Batch Loss: 0.646507978439331\n",
      "Epoch 452, Loss: 1.1781912446022034, Final Batch Loss: 0.6402778029441833\n",
      "Epoch 453, Loss: 1.175456702709198, Final Batch Loss: 0.5619091391563416\n",
      "Epoch 454, Loss: 1.300529956817627, Final Batch Loss: 0.6934865117073059\n",
      "Epoch 455, Loss: 1.2181491255760193, Final Batch Loss: 0.695324182510376\n",
      "Epoch 456, Loss: 1.138806939125061, Final Batch Loss: 0.5408188700675964\n",
      "Epoch 457, Loss: 1.1657100319862366, Final Batch Loss: 0.5785155296325684\n",
      "Epoch 458, Loss: 1.128521203994751, Final Batch Loss: 0.5306025147438049\n",
      "Epoch 459, Loss: 1.2040717601776123, Final Batch Loss: 0.5984400510787964\n",
      "Epoch 460, Loss: 1.1760466694831848, Final Batch Loss: 0.5476337671279907\n",
      "Epoch 461, Loss: 1.2310508489608765, Final Batch Loss: 0.6129336357116699\n",
      "Epoch 462, Loss: 1.1332900524139404, Final Batch Loss: 0.5546392202377319\n",
      "Epoch 463, Loss: 1.1879081726074219, Final Batch Loss: 0.5762368440628052\n",
      "Epoch 464, Loss: 1.1873362064361572, Final Batch Loss: 0.5795871615409851\n",
      "Epoch 465, Loss: 1.0515941381454468, Final Batch Loss: 0.5273343920707703\n",
      "Epoch 466, Loss: 1.164608120918274, Final Batch Loss: 0.5294831991195679\n",
      "Epoch 467, Loss: 1.1772881150245667, Final Batch Loss: 0.5682561993598938\n",
      "Epoch 468, Loss: 1.2505375146865845, Final Batch Loss: 0.664616048336029\n",
      "Epoch 469, Loss: 1.195460855960846, Final Batch Loss: 0.5726854205131531\n",
      "Epoch 470, Loss: 1.197226345539093, Final Batch Loss: 0.6667478084564209\n",
      "Epoch 471, Loss: 1.2063069939613342, Final Batch Loss: 0.6193516850471497\n",
      "Epoch 472, Loss: 1.102709710597992, Final Batch Loss: 0.5772982835769653\n",
      "Epoch 473, Loss: 1.3256571888923645, Final Batch Loss: 0.7252732515335083\n",
      "Epoch 474, Loss: 1.1914729475975037, Final Batch Loss: 0.5913223028182983\n",
      "Epoch 475, Loss: 1.2709067463874817, Final Batch Loss: 0.6725246906280518\n",
      "Epoch 476, Loss: 1.0613005757331848, Final Batch Loss: 0.5200873613357544\n",
      "Epoch 477, Loss: 1.112339437007904, Final Batch Loss: 0.5705492496490479\n",
      "Epoch 478, Loss: 1.0678714513778687, Final Batch Loss: 0.5011552572250366\n",
      "Epoch 479, Loss: 1.1283484101295471, Final Batch Loss: 0.5266062617301941\n",
      "Epoch 480, Loss: 1.2085000276565552, Final Batch Loss: 0.5953456163406372\n",
      "Epoch 481, Loss: 1.1072900891304016, Final Batch Loss: 0.5324106812477112\n",
      "Epoch 482, Loss: 1.1864893436431885, Final Batch Loss: 0.5788037776947021\n",
      "Epoch 483, Loss: 1.2304890155792236, Final Batch Loss: 0.6610788702964783\n",
      "Epoch 484, Loss: 1.133029818534851, Final Batch Loss: 0.5415224432945251\n",
      "Epoch 485, Loss: 1.1214534044265747, Final Batch Loss: 0.5950458645820618\n",
      "Epoch 486, Loss: 1.135556161403656, Final Batch Loss: 0.5939322710037231\n",
      "Epoch 487, Loss: 1.0963189601898193, Final Batch Loss: 0.5479380488395691\n",
      "Epoch 488, Loss: 1.1436550617218018, Final Batch Loss: 0.5082293748855591\n",
      "Epoch 489, Loss: 1.1935257315635681, Final Batch Loss: 0.5385515689849854\n",
      "Epoch 490, Loss: 1.2490019798278809, Final Batch Loss: 0.6645179390907288\n",
      "Epoch 491, Loss: 1.1731237173080444, Final Batch Loss: 0.5082940459251404\n",
      "Epoch 492, Loss: 1.2067723274230957, Final Batch Loss: 0.6729729175567627\n",
      "Epoch 493, Loss: 1.0780344605445862, Final Batch Loss: 0.5538955926895142\n",
      "Epoch 494, Loss: 1.1196401119232178, Final Batch Loss: 0.5404409170150757\n",
      "Epoch 495, Loss: 1.0904751420021057, Final Batch Loss: 0.5025604963302612\n",
      "Epoch 496, Loss: 1.1975240111351013, Final Batch Loss: 0.601998507976532\n",
      "Epoch 497, Loss: 1.2407965064048767, Final Batch Loss: 0.647833526134491\n",
      "Epoch 498, Loss: 1.0999948382377625, Final Batch Loss: 0.5751621127128601\n",
      "Epoch 499, Loss: 1.2498387098312378, Final Batch Loss: 0.6422709226608276\n",
      "Epoch 500, Loss: 1.069841593503952, Final Batch Loss: 0.49911579489707947\n",
      "Epoch 501, Loss: 1.1599236726760864, Final Batch Loss: 0.6216753125190735\n",
      "Epoch 502, Loss: 1.1980257034301758, Final Batch Loss: 0.6532968878746033\n",
      "Epoch 503, Loss: 0.9471682012081146, Final Batch Loss: 0.45897603034973145\n",
      "Epoch 504, Loss: 1.1454275846481323, Final Batch Loss: 0.5235942602157593\n",
      "Epoch 505, Loss: 1.1577442288398743, Final Batch Loss: 0.5993883013725281\n",
      "Epoch 506, Loss: 1.0979152917861938, Final Batch Loss: 0.580312967300415\n",
      "Epoch 507, Loss: 1.1172915697097778, Final Batch Loss: 0.6061258316040039\n",
      "Epoch 508, Loss: 1.1454842686653137, Final Batch Loss: 0.6269433498382568\n",
      "Epoch 509, Loss: 1.188152015209198, Final Batch Loss: 0.5988404750823975\n",
      "Epoch 510, Loss: 1.1970344185829163, Final Batch Loss: 0.6354620456695557\n",
      "Epoch 511, Loss: 1.1412227153778076, Final Batch Loss: 0.5837810635566711\n",
      "Epoch 512, Loss: 1.1255685687065125, Final Batch Loss: 0.6076880693435669\n",
      "Epoch 513, Loss: 0.9899225533008575, Final Batch Loss: 0.46998491883277893\n",
      "Epoch 514, Loss: 1.247384786605835, Final Batch Loss: 0.709495484828949\n",
      "Epoch 515, Loss: 1.1779033541679382, Final Batch Loss: 0.6185035109519958\n",
      "Epoch 516, Loss: 1.02712082862854, Final Batch Loss: 0.3730372190475464\n",
      "Epoch 517, Loss: 1.0981690883636475, Final Batch Loss: 0.5815343260765076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 518, Loss: 1.1818885803222656, Final Batch Loss: 0.6225134134292603\n",
      "Epoch 519, Loss: 1.138512372970581, Final Batch Loss: 0.5969226956367493\n",
      "Epoch 520, Loss: 1.0375156104564667, Final Batch Loss: 0.45021572709083557\n",
      "Epoch 521, Loss: 1.2286695837974548, Final Batch Loss: 0.6206141710281372\n",
      "Epoch 522, Loss: 1.1924087405204773, Final Batch Loss: 0.5991215109825134\n",
      "Epoch 523, Loss: 1.0927762985229492, Final Batch Loss: 0.5976192355155945\n",
      "Epoch 524, Loss: 1.037881225347519, Final Batch Loss: 0.49299392104148865\n",
      "Epoch 525, Loss: 1.0349567830562592, Final Batch Loss: 0.5381659269332886\n",
      "Epoch 526, Loss: 1.0238357782363892, Final Batch Loss: 0.4841494560241699\n",
      "Epoch 527, Loss: 1.2498230934143066, Final Batch Loss: 0.6964094042778015\n",
      "Epoch 528, Loss: 1.1553869247436523, Final Batch Loss: 0.6365339159965515\n",
      "Epoch 529, Loss: 1.119148701429367, Final Batch Loss: 0.4817448556423187\n",
      "Epoch 530, Loss: 1.058634102344513, Final Batch Loss: 0.5012627243995667\n",
      "Epoch 531, Loss: 1.1371829509735107, Final Batch Loss: 0.5394155383110046\n",
      "Epoch 532, Loss: 1.0227144360542297, Final Batch Loss: 0.47802186012268066\n",
      "Epoch 533, Loss: 1.0585382580757141, Final Batch Loss: 0.5161325931549072\n",
      "Epoch 534, Loss: 1.078808069229126, Final Batch Loss: 0.523449718952179\n",
      "Epoch 535, Loss: 1.0741578340530396, Final Batch Loss: 0.5398023128509521\n",
      "Epoch 536, Loss: 1.0634320974349976, Final Batch Loss: 0.5494331121444702\n",
      "Epoch 537, Loss: 1.0213831663131714, Final Batch Loss: 0.47388559579849243\n",
      "Epoch 538, Loss: 1.0840592086315155, Final Batch Loss: 0.4961146414279938\n",
      "Epoch 539, Loss: 1.167916476726532, Final Batch Loss: 0.5358750224113464\n",
      "Epoch 540, Loss: 1.0259170532226562, Final Batch Loss: 0.5180976390838623\n",
      "Epoch 541, Loss: 1.0808206796646118, Final Batch Loss: 0.5621775984764099\n",
      "Epoch 542, Loss: 1.0352476239204407, Final Batch Loss: 0.5228034257888794\n",
      "Epoch 543, Loss: 1.0478291809558868, Final Batch Loss: 0.557978093624115\n",
      "Epoch 544, Loss: 0.9768316447734833, Final Batch Loss: 0.4885878562927246\n",
      "Epoch 545, Loss: 1.0017589628696442, Final Batch Loss: 0.4989425837993622\n",
      "Epoch 546, Loss: 1.177436888217926, Final Batch Loss: 0.5970182418823242\n",
      "Epoch 547, Loss: 0.9617760479450226, Final Batch Loss: 0.456599622964859\n",
      "Epoch 548, Loss: 1.1686394810676575, Final Batch Loss: 0.6019478440284729\n",
      "Epoch 549, Loss: 1.0623154938220978, Final Batch Loss: 0.577692449092865\n",
      "Epoch 550, Loss: 1.1538476943969727, Final Batch Loss: 0.6480211615562439\n",
      "Epoch 551, Loss: 0.9460538327693939, Final Batch Loss: 0.47410571575164795\n",
      "Epoch 552, Loss: 1.0740700662136078, Final Batch Loss: 0.5851154327392578\n",
      "Epoch 553, Loss: 1.0490857362747192, Final Batch Loss: 0.49541395902633667\n",
      "Epoch 554, Loss: 0.9595155417919159, Final Batch Loss: 0.43310150504112244\n",
      "Epoch 555, Loss: 0.9622538983821869, Final Batch Loss: 0.493264764547348\n",
      "Epoch 556, Loss: 1.0355364978313446, Final Batch Loss: 0.48883989453315735\n",
      "Epoch 557, Loss: 1.024397611618042, Final Batch Loss: 0.5222064852714539\n",
      "Epoch 558, Loss: 1.079831063747406, Final Batch Loss: 0.5364263653755188\n",
      "Epoch 559, Loss: 1.0351923704147339, Final Batch Loss: 0.515871524810791\n",
      "Epoch 560, Loss: 1.1413412690162659, Final Batch Loss: 0.6366570591926575\n",
      "Epoch 561, Loss: 1.0282817482948303, Final Batch Loss: 0.5136836171150208\n",
      "Epoch 562, Loss: 1.07402104139328, Final Batch Loss: 0.5195809006690979\n",
      "Epoch 563, Loss: 1.0409010648727417, Final Batch Loss: 0.5674021244049072\n",
      "Epoch 564, Loss: 1.0195474028587341, Final Batch Loss: 0.5596256256103516\n",
      "Epoch 565, Loss: 1.0165478587150574, Final Batch Loss: 0.4684520363807678\n",
      "Epoch 566, Loss: 1.0397655665874481, Final Batch Loss: 0.4997421205043793\n",
      "Epoch 567, Loss: 1.0358618199825287, Final Batch Loss: 0.4430285394191742\n",
      "Epoch 568, Loss: 1.0535673201084137, Final Batch Loss: 0.4952016770839691\n",
      "Epoch 569, Loss: 0.9850930273532867, Final Batch Loss: 0.5030542016029358\n",
      "Epoch 570, Loss: 0.9648961424827576, Final Batch Loss: 0.45686107873916626\n",
      "Epoch 571, Loss: 0.9609929621219635, Final Batch Loss: 0.45618852972984314\n",
      "Epoch 572, Loss: 0.9560254812240601, Final Batch Loss: 0.456856369972229\n",
      "Epoch 573, Loss: 1.114692211151123, Final Batch Loss: 0.6093342900276184\n",
      "Epoch 574, Loss: 1.0280365943908691, Final Batch Loss: 0.5142638087272644\n",
      "Epoch 575, Loss: 1.0326127409934998, Final Batch Loss: 0.5167245864868164\n",
      "Epoch 576, Loss: 1.0441924929618835, Final Batch Loss: 0.5101553797721863\n",
      "Epoch 577, Loss: 0.9567841589450836, Final Batch Loss: 0.5008513331413269\n",
      "Epoch 578, Loss: 0.9432543814182281, Final Batch Loss: 0.466030478477478\n",
      "Epoch 579, Loss: 1.084135353565216, Final Batch Loss: 0.5918064117431641\n",
      "Epoch 580, Loss: 1.011014074087143, Final Batch Loss: 0.5139597654342651\n",
      "Epoch 581, Loss: 1.0176299810409546, Final Batch Loss: 0.535900354385376\n",
      "Epoch 582, Loss: 0.9902787208557129, Final Batch Loss: 0.5076289176940918\n",
      "Epoch 583, Loss: 1.0221142172813416, Final Batch Loss: 0.47901028394699097\n",
      "Epoch 584, Loss: 1.0566071271896362, Final Batch Loss: 0.519614040851593\n",
      "Epoch 585, Loss: 1.053057312965393, Final Batch Loss: 0.5171574950218201\n",
      "Epoch 586, Loss: 0.9924823343753815, Final Batch Loss: 0.46601220965385437\n",
      "Epoch 587, Loss: 1.0400272011756897, Final Batch Loss: 0.5343201160430908\n",
      "Epoch 588, Loss: 1.1022031009197235, Final Batch Loss: 0.6076785922050476\n",
      "Epoch 589, Loss: 1.0451458096504211, Final Batch Loss: 0.5223033428192139\n",
      "Epoch 590, Loss: 1.0379688441753387, Final Batch Loss: 0.576695442199707\n",
      "Epoch 591, Loss: 0.9523832499980927, Final Batch Loss: 0.43856778740882874\n",
      "Epoch 592, Loss: 1.0100182890892029, Final Batch Loss: 0.5371550917625427\n",
      "Epoch 593, Loss: 0.9848364293575287, Final Batch Loss: 0.4315392076969147\n",
      "Epoch 594, Loss: 1.050185203552246, Final Batch Loss: 0.5007033944129944\n",
      "Epoch 595, Loss: 1.043825477361679, Final Batch Loss: 0.5469940900802612\n",
      "Epoch 596, Loss: 1.0490708947181702, Final Batch Loss: 0.5218150019645691\n",
      "Epoch 597, Loss: 1.0326503217220306, Final Batch Loss: 0.4674282371997833\n",
      "Epoch 598, Loss: 1.0374306440353394, Final Batch Loss: 0.5603656768798828\n",
      "Epoch 599, Loss: 0.9219613075256348, Final Batch Loss: 0.4394817054271698\n",
      "Epoch 600, Loss: 0.9834966659545898, Final Batch Loss: 0.5257358551025391\n",
      "Epoch 601, Loss: 1.0505946278572083, Final Batch Loss: 0.5492460131645203\n",
      "Epoch 602, Loss: 0.9994239509105682, Final Batch Loss: 0.4869600236415863\n",
      "Epoch 603, Loss: 1.0691424012184143, Final Batch Loss: 0.5985267162322998\n",
      "Epoch 604, Loss: 0.9351058006286621, Final Batch Loss: 0.468039870262146\n",
      "Epoch 605, Loss: 1.0743439197540283, Final Batch Loss: 0.5695734024047852\n",
      "Epoch 606, Loss: 1.0285796523094177, Final Batch Loss: 0.5495132803916931\n",
      "Epoch 607, Loss: 1.0559621453285217, Final Batch Loss: 0.5207489728927612\n",
      "Epoch 608, Loss: 0.9784352481365204, Final Batch Loss: 0.4692704975605011\n",
      "Epoch 609, Loss: 1.0018578469753265, Final Batch Loss: 0.4792480766773224\n",
      "Epoch 610, Loss: 0.9632776379585266, Final Batch Loss: 0.43954503536224365\n",
      "Epoch 611, Loss: 0.9982117414474487, Final Batch Loss: 0.4876646399497986\n",
      "Epoch 612, Loss: 0.9849050939083099, Final Batch Loss: 0.4815026819705963\n",
      "Epoch 613, Loss: 0.9669801294803619, Final Batch Loss: 0.48998722434043884\n",
      "Epoch 614, Loss: 0.973638266324997, Final Batch Loss: 0.4934864640235901\n",
      "Epoch 615, Loss: 0.9576096534729004, Final Batch Loss: 0.47054556012153625\n",
      "Epoch 616, Loss: 0.9764260649681091, Final Batch Loss: 0.5208505392074585\n",
      "Epoch 617, Loss: 0.9171048402786255, Final Batch Loss: 0.4295934736728668\n",
      "Epoch 618, Loss: 0.8362542688846588, Final Batch Loss: 0.40018463134765625\n",
      "Epoch 619, Loss: 0.9425764679908752, Final Batch Loss: 0.41423678398132324\n",
      "Epoch 620, Loss: 1.0026911199092865, Final Batch Loss: 0.4987212121486664\n",
      "Epoch 621, Loss: 1.1002193689346313, Final Batch Loss: 0.5859029293060303\n",
      "Epoch 622, Loss: 0.931180328130722, Final Batch Loss: 0.4345839321613312\n",
      "Epoch 623, Loss: 0.9105835258960724, Final Batch Loss: 0.4923252761363983\n",
      "Epoch 624, Loss: 0.9500203132629395, Final Batch Loss: 0.4663695991039276\n",
      "Epoch 625, Loss: 0.984896183013916, Final Batch Loss: 0.5076566338539124\n",
      "Epoch 626, Loss: 1.0689541697502136, Final Batch Loss: 0.5005403161048889\n",
      "Epoch 627, Loss: 0.9452472627162933, Final Batch Loss: 0.48574185371398926\n",
      "Epoch 628, Loss: 0.9880839288234711, Final Batch Loss: 0.4993273913860321\n",
      "Epoch 629, Loss: 1.0277439653873444, Final Batch Loss: 0.5503639578819275\n",
      "Epoch 630, Loss: 1.0594338178634644, Final Batch Loss: 0.5548876523971558\n",
      "Epoch 631, Loss: 1.0407946109771729, Final Batch Loss: 0.4969683885574341\n",
      "Epoch 632, Loss: 1.0205150246620178, Final Batch Loss: 0.47242021560668945\n",
      "Epoch 633, Loss: 0.8407933712005615, Final Batch Loss: 0.3691953122615814\n",
      "Epoch 634, Loss: 0.9079368412494659, Final Batch Loss: 0.40373095870018005\n",
      "Epoch 635, Loss: 0.8333041667938232, Final Batch Loss: 0.37036192417144775\n",
      "Epoch 636, Loss: 0.9567939043045044, Final Batch Loss: 0.48864197731018066\n",
      "Epoch 637, Loss: 0.9474400877952576, Final Batch Loss: 0.4979131817817688\n",
      "Epoch 638, Loss: 0.9595507085323334, Final Batch Loss: 0.5073581337928772\n",
      "Epoch 639, Loss: 0.9331671297550201, Final Batch Loss: 0.44764116406440735\n",
      "Epoch 640, Loss: 0.967400312423706, Final Batch Loss: 0.5539203882217407\n",
      "Epoch 641, Loss: 0.852400541305542, Final Batch Loss: 0.3937559127807617\n",
      "Epoch 642, Loss: 0.9948321282863617, Final Batch Loss: 0.612893283367157\n",
      "Epoch 643, Loss: 0.8736527264118195, Final Batch Loss: 0.431302011013031\n",
      "Epoch 644, Loss: 0.9289585053920746, Final Batch Loss: 0.4620915353298187\n",
      "Epoch 645, Loss: 0.9588185548782349, Final Batch Loss: 0.49168455600738525\n",
      "Epoch 646, Loss: 0.9177369773387909, Final Batch Loss: 0.4533662497997284\n",
      "Epoch 647, Loss: 1.0223467648029327, Final Batch Loss: 0.4864347279071808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 648, Loss: 0.9839467108249664, Final Batch Loss: 0.4839204251766205\n",
      "Epoch 649, Loss: 0.9998628795146942, Final Batch Loss: 0.4546950161457062\n",
      "Epoch 650, Loss: 0.9371673166751862, Final Batch Loss: 0.4451275169849396\n",
      "Epoch 651, Loss: 0.8944272696971893, Final Batch Loss: 0.44687420129776\n",
      "Epoch 652, Loss: 0.9046740829944611, Final Batch Loss: 0.4435204565525055\n",
      "Epoch 653, Loss: 0.956069827079773, Final Batch Loss: 0.45644640922546387\n",
      "Epoch 654, Loss: 1.0377519726753235, Final Batch Loss: 0.5463011860847473\n",
      "Epoch 655, Loss: 0.9709022045135498, Final Batch Loss: 0.4330049157142639\n",
      "Epoch 656, Loss: 0.9030851423740387, Final Batch Loss: 0.4644705653190613\n",
      "Epoch 657, Loss: 0.8852317631244659, Final Batch Loss: 0.4215424060821533\n",
      "Epoch 658, Loss: 0.8671998977661133, Final Batch Loss: 0.39749404788017273\n",
      "Epoch 659, Loss: 0.8379648625850677, Final Batch Loss: 0.38484522700309753\n",
      "Epoch 660, Loss: 1.0064613819122314, Final Batch Loss: 0.5287742018699646\n",
      "Epoch 661, Loss: 0.9994760751724243, Final Batch Loss: 0.4899880886077881\n",
      "Epoch 662, Loss: 0.8663371503353119, Final Batch Loss: 0.43066930770874023\n",
      "Epoch 663, Loss: 0.9855006337165833, Final Batch Loss: 0.51434326171875\n",
      "Epoch 664, Loss: 0.816320925951004, Final Batch Loss: 0.3733820617198944\n",
      "Epoch 665, Loss: 1.0028367638587952, Final Batch Loss: 0.5151458382606506\n",
      "Epoch 666, Loss: 0.8660438060760498, Final Batch Loss: 0.4374261200428009\n",
      "Epoch 667, Loss: 0.8800943493843079, Final Batch Loss: 0.46155238151550293\n",
      "Epoch 668, Loss: 0.9371697306632996, Final Batch Loss: 0.4418479800224304\n",
      "Epoch 669, Loss: 0.9103244543075562, Final Batch Loss: 0.4078477621078491\n",
      "Epoch 670, Loss: 0.8466817438602448, Final Batch Loss: 0.3655431866645813\n",
      "Epoch 671, Loss: 0.9439084529876709, Final Batch Loss: 0.48472315073013306\n",
      "Epoch 672, Loss: 0.8641006350517273, Final Batch Loss: 0.4180302619934082\n",
      "Epoch 673, Loss: 0.9802271127700806, Final Batch Loss: 0.4529736042022705\n",
      "Epoch 674, Loss: 0.7918113768100739, Final Batch Loss: 0.33002519607543945\n",
      "Epoch 675, Loss: 0.8851849436759949, Final Batch Loss: 0.4531475901603699\n",
      "Epoch 676, Loss: 0.9106862843036652, Final Batch Loss: 0.4398263990879059\n",
      "Epoch 677, Loss: 0.929473489522934, Final Batch Loss: 0.4980919063091278\n",
      "Epoch 678, Loss: 0.8929747939109802, Final Batch Loss: 0.4306776821613312\n",
      "Epoch 679, Loss: 0.7966297268867493, Final Batch Loss: 0.3636419475078583\n",
      "Epoch 680, Loss: 0.9219360947608948, Final Batch Loss: 0.4263954162597656\n",
      "Epoch 681, Loss: 0.9987430274486542, Final Batch Loss: 0.566397488117218\n",
      "Epoch 682, Loss: 0.8422436714172363, Final Batch Loss: 0.3698238730430603\n",
      "Epoch 683, Loss: 0.8609078824520111, Final Batch Loss: 0.4210736155509949\n",
      "Epoch 684, Loss: 0.9374355673789978, Final Batch Loss: 0.43052536249160767\n",
      "Epoch 685, Loss: 0.8339762687683105, Final Batch Loss: 0.42424219846725464\n",
      "Epoch 686, Loss: 0.8002034723758698, Final Batch Loss: 0.32764506340026855\n",
      "Epoch 687, Loss: 0.836378425359726, Final Batch Loss: 0.40909337997436523\n",
      "Epoch 688, Loss: 0.8271943032741547, Final Batch Loss: 0.43244051933288574\n",
      "Epoch 689, Loss: 0.8236614167690277, Final Batch Loss: 0.41460341215133667\n",
      "Epoch 690, Loss: 0.9784009158611298, Final Batch Loss: 0.5745642781257629\n",
      "Epoch 691, Loss: 0.7927462458610535, Final Batch Loss: 0.3624337315559387\n",
      "Epoch 692, Loss: 0.8299769759178162, Final Batch Loss: 0.41073644161224365\n",
      "Epoch 693, Loss: 0.8124307990074158, Final Batch Loss: 0.4081157147884369\n",
      "Epoch 694, Loss: 0.795992910861969, Final Batch Loss: 0.3597203493118286\n",
      "Epoch 695, Loss: 0.83783158659935, Final Batch Loss: 0.44916272163391113\n",
      "Epoch 696, Loss: 0.8922741711139679, Final Batch Loss: 0.41064968705177307\n",
      "Epoch 697, Loss: 0.9660176932811737, Final Batch Loss: 0.4659740626811981\n",
      "Epoch 698, Loss: 0.9918099343776703, Final Batch Loss: 0.5605409741401672\n",
      "Epoch 699, Loss: 0.9443868100643158, Final Batch Loss: 0.4328567683696747\n",
      "Epoch 700, Loss: 0.824822723865509, Final Batch Loss: 0.3849606513977051\n",
      "Epoch 701, Loss: 0.8894045352935791, Final Batch Loss: 0.41518643498420715\n",
      "Epoch 702, Loss: 0.8264670073986053, Final Batch Loss: 0.36979779601097107\n",
      "Epoch 703, Loss: 0.9429108500480652, Final Batch Loss: 0.44261980056762695\n",
      "Epoch 704, Loss: 0.8609371483325958, Final Batch Loss: 0.38425111770629883\n",
      "Epoch 705, Loss: 0.8621829748153687, Final Batch Loss: 0.45163607597351074\n",
      "Epoch 706, Loss: 0.8430118560791016, Final Batch Loss: 0.44564956426620483\n",
      "Epoch 707, Loss: 0.8467226624488831, Final Batch Loss: 0.4360421895980835\n",
      "Epoch 708, Loss: 0.8512971103191376, Final Batch Loss: 0.36239004135131836\n",
      "Epoch 709, Loss: 0.7744469046592712, Final Batch Loss: 0.32704147696495056\n",
      "Epoch 710, Loss: 0.8212031722068787, Final Batch Loss: 0.3667590022087097\n",
      "Epoch 711, Loss: 0.8609039485454559, Final Batch Loss: 0.44540220499038696\n",
      "Epoch 712, Loss: 0.8711560964584351, Final Batch Loss: 0.4295666813850403\n",
      "Epoch 713, Loss: 0.9363536536693573, Final Batch Loss: 0.5481762886047363\n",
      "Epoch 714, Loss: 0.9680704474449158, Final Batch Loss: 0.5418257713317871\n",
      "Epoch 715, Loss: 0.8557138741016388, Final Batch Loss: 0.39521411061286926\n",
      "Epoch 716, Loss: 0.8003112971782684, Final Batch Loss: 0.39362385869026184\n",
      "Epoch 717, Loss: 0.803216427564621, Final Batch Loss: 0.3730939030647278\n",
      "Epoch 718, Loss: 0.9010175466537476, Final Batch Loss: 0.4545225203037262\n",
      "Epoch 719, Loss: 0.8923108577728271, Final Batch Loss: 0.4737188220024109\n",
      "Epoch 720, Loss: 0.742840439081192, Final Batch Loss: 0.35483840107917786\n",
      "Epoch 721, Loss: 0.8694588541984558, Final Batch Loss: 0.38873013854026794\n",
      "Epoch 722, Loss: 0.9928062558174133, Final Batch Loss: 0.5197442173957825\n",
      "Epoch 723, Loss: 0.8442533016204834, Final Batch Loss: 0.39822545647621155\n",
      "Epoch 724, Loss: 0.835102915763855, Final Batch Loss: 0.40663304924964905\n",
      "Epoch 725, Loss: 0.9179215729236603, Final Batch Loss: 0.47819820046424866\n",
      "Epoch 726, Loss: 0.7473629117012024, Final Batch Loss: 0.32525238394737244\n",
      "Epoch 727, Loss: 0.8066082000732422, Final Batch Loss: 0.38655588030815125\n",
      "Epoch 728, Loss: 1.0220553278923035, Final Batch Loss: 0.5242885947227478\n",
      "Epoch 729, Loss: 0.8667134642601013, Final Batch Loss: 0.46560147404670715\n",
      "Epoch 730, Loss: 0.9005867540836334, Final Batch Loss: 0.4881308376789093\n",
      "Epoch 731, Loss: 0.7994242310523987, Final Batch Loss: 0.45333486795425415\n",
      "Epoch 732, Loss: 0.8919808864593506, Final Batch Loss: 0.45251068472862244\n",
      "Epoch 733, Loss: 0.9437135756015778, Final Batch Loss: 0.5280546545982361\n",
      "Epoch 734, Loss: 0.9285281002521515, Final Batch Loss: 0.508262038230896\n",
      "Epoch 735, Loss: 0.8997935354709625, Final Batch Loss: 0.44867831468582153\n",
      "Epoch 736, Loss: 0.8304029703140259, Final Batch Loss: 0.4217323660850525\n",
      "Epoch 737, Loss: 0.7720299661159515, Final Batch Loss: 0.38252928853034973\n",
      "Epoch 738, Loss: 0.8352081775665283, Final Batch Loss: 0.4129382371902466\n",
      "Epoch 739, Loss: 0.8404306471347809, Final Batch Loss: 0.45451050996780396\n",
      "Epoch 740, Loss: 0.9225425124168396, Final Batch Loss: 0.4568476378917694\n",
      "Epoch 741, Loss: 0.8266726732254028, Final Batch Loss: 0.4262862205505371\n",
      "Epoch 742, Loss: 0.783125638961792, Final Batch Loss: 0.4158618748188019\n",
      "Epoch 743, Loss: 0.9172727465629578, Final Batch Loss: 0.4985313415527344\n",
      "Epoch 744, Loss: 0.7384500503540039, Final Batch Loss: 0.32037433981895447\n",
      "Epoch 745, Loss: 0.7901712954044342, Final Batch Loss: 0.3744139075279236\n",
      "Epoch 746, Loss: 0.7438527047634125, Final Batch Loss: 0.3757389783859253\n",
      "Epoch 747, Loss: 0.794762909412384, Final Batch Loss: 0.41535356640815735\n",
      "Epoch 748, Loss: 0.8404552340507507, Final Batch Loss: 0.3922693431377411\n",
      "Epoch 749, Loss: 0.7335128784179688, Final Batch Loss: 0.3527713418006897\n",
      "Epoch 750, Loss: 0.7194534540176392, Final Batch Loss: 0.37793493270874023\n",
      "Epoch 751, Loss: 0.8724482953548431, Final Batch Loss: 0.4744873046875\n",
      "Epoch 752, Loss: 0.8308969438076019, Final Batch Loss: 0.4430837631225586\n",
      "Epoch 753, Loss: 0.8129271864891052, Final Batch Loss: 0.4650288224220276\n",
      "Epoch 754, Loss: 0.9325433671474457, Final Batch Loss: 0.4982646107673645\n",
      "Epoch 755, Loss: 0.8593879640102386, Final Batch Loss: 0.4581497609615326\n",
      "Epoch 756, Loss: 0.8075027465820312, Final Batch Loss: 0.42350175976753235\n",
      "Epoch 757, Loss: 0.7673680186271667, Final Batch Loss: 0.4194965958595276\n",
      "Epoch 758, Loss: 0.7417130470275879, Final Batch Loss: 0.3302444517612457\n",
      "Epoch 759, Loss: 0.8795081675052643, Final Batch Loss: 0.5085222125053406\n",
      "Epoch 760, Loss: 0.7466813325881958, Final Batch Loss: 0.3780084252357483\n",
      "Epoch 761, Loss: 0.7346133887767792, Final Batch Loss: 0.3558529317378998\n",
      "Epoch 762, Loss: 0.7892102301120758, Final Batch Loss: 0.4203343391418457\n",
      "Epoch 763, Loss: 0.7236308157444, Final Batch Loss: 0.3360512852668762\n",
      "Epoch 764, Loss: 0.8291435241699219, Final Batch Loss: 0.4170137345790863\n",
      "Epoch 765, Loss: 0.7688824534416199, Final Batch Loss: 0.3889468312263489\n",
      "Epoch 766, Loss: 0.7742834985256195, Final Batch Loss: 0.3861379027366638\n",
      "Epoch 767, Loss: 0.8204957544803619, Final Batch Loss: 0.42530199885368347\n",
      "Epoch 768, Loss: 0.7869080007076263, Final Batch Loss: 0.34452134370803833\n",
      "Epoch 769, Loss: 0.7229875028133392, Final Batch Loss: 0.3255746364593506\n",
      "Epoch 770, Loss: 0.672618567943573, Final Batch Loss: 0.2918974459171295\n",
      "Epoch 771, Loss: 0.71193066239357, Final Batch Loss: 0.38061264157295227\n",
      "Epoch 772, Loss: 0.8386576175689697, Final Batch Loss: 0.4809685945510864\n",
      "Epoch 773, Loss: 0.8815146386623383, Final Batch Loss: 0.4596635699272156\n",
      "Epoch 774, Loss: 0.7618182003498077, Final Batch Loss: 0.37261703610420227\n",
      "Epoch 775, Loss: 0.7611349821090698, Final Batch Loss: 0.35969600081443787\n",
      "Epoch 776, Loss: 0.7504823803901672, Final Batch Loss: 0.31179988384246826\n",
      "Epoch 777, Loss: 0.8104203939437866, Final Batch Loss: 0.44149723649024963\n",
      "Epoch 778, Loss: 0.8557092547416687, Final Batch Loss: 0.4343353509902954\n",
      "Epoch 779, Loss: 0.781624436378479, Final Batch Loss: 0.41116011142730713\n",
      "Epoch 780, Loss: 0.7152980268001556, Final Batch Loss: 0.39955928921699524\n",
      "Epoch 781, Loss: 0.7149466276168823, Final Batch Loss: 0.366073340177536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 782, Loss: 0.729989618062973, Final Batch Loss: 0.34941792488098145\n",
      "Epoch 783, Loss: 0.7470808923244476, Final Batch Loss: 0.3433610498905182\n",
      "Epoch 784, Loss: 0.7542896270751953, Final Batch Loss: 0.3163164258003235\n",
      "Epoch 785, Loss: 0.7660920917987823, Final Batch Loss: 0.37000706791877747\n",
      "Epoch 786, Loss: 0.7383472621440887, Final Batch Loss: 0.31072065234184265\n",
      "Epoch 787, Loss: 0.8252815008163452, Final Batch Loss: 0.4594864845275879\n",
      "Epoch 788, Loss: 0.755350798368454, Final Batch Loss: 0.40228500962257385\n",
      "Epoch 789, Loss: 0.8531094193458557, Final Batch Loss: 0.37872105836868286\n",
      "Epoch 790, Loss: 0.7177852690219879, Final Batch Loss: 0.34406742453575134\n",
      "Epoch 791, Loss: 0.8225936889648438, Final Batch Loss: 0.4114927351474762\n",
      "Epoch 792, Loss: 0.7529380917549133, Final Batch Loss: 0.347458153963089\n",
      "Epoch 793, Loss: 0.7633995711803436, Final Batch Loss: 0.3671276867389679\n",
      "Epoch 794, Loss: 0.7492226958274841, Final Batch Loss: 0.3558654189109802\n",
      "Epoch 795, Loss: 0.7221977710723877, Final Batch Loss: 0.3202815353870392\n",
      "Epoch 796, Loss: 0.6803995370864868, Final Batch Loss: 0.31316137313842773\n",
      "Epoch 797, Loss: 0.729379266500473, Final Batch Loss: 0.3379599452018738\n",
      "Epoch 798, Loss: 0.9089098572731018, Final Batch Loss: 0.4121173322200775\n",
      "Epoch 799, Loss: 0.8387993276119232, Final Batch Loss: 0.4935849606990814\n",
      "Epoch 800, Loss: 0.7361677885055542, Final Batch Loss: 0.33438801765441895\n",
      "Epoch 801, Loss: 0.7616433203220367, Final Batch Loss: 0.39235901832580566\n",
      "Epoch 802, Loss: 0.7987131178379059, Final Batch Loss: 0.37683194875717163\n",
      "Epoch 803, Loss: 0.8091078102588654, Final Batch Loss: 0.4139697253704071\n",
      "Epoch 804, Loss: 0.8461583852767944, Final Batch Loss: 0.43606826663017273\n",
      "Epoch 805, Loss: 0.8081034123897552, Final Batch Loss: 0.4313243627548218\n",
      "Epoch 806, Loss: 0.789504200220108, Final Batch Loss: 0.3745415210723877\n",
      "Epoch 807, Loss: 0.8321016132831573, Final Batch Loss: 0.5104827880859375\n",
      "Epoch 808, Loss: 0.8581533432006836, Final Batch Loss: 0.47738245129585266\n",
      "Epoch 809, Loss: 0.6692111194133759, Final Batch Loss: 0.29477712512016296\n",
      "Epoch 810, Loss: 0.6870075464248657, Final Batch Loss: 0.31155529618263245\n",
      "Epoch 811, Loss: 0.8328549861907959, Final Batch Loss: 0.4590153396129608\n",
      "Epoch 812, Loss: 0.7895747125148773, Final Batch Loss: 0.4060667157173157\n",
      "Epoch 813, Loss: 0.8629602193832397, Final Batch Loss: 0.4228806495666504\n",
      "Epoch 814, Loss: 0.7868605256080627, Final Batch Loss: 0.4306083917617798\n",
      "Epoch 815, Loss: 0.777706116437912, Final Batch Loss: 0.3707500100135803\n",
      "Epoch 816, Loss: 0.83640056848526, Final Batch Loss: 0.43190282583236694\n",
      "Epoch 817, Loss: 0.8123019635677338, Final Batch Loss: 0.4360940456390381\n",
      "Epoch 818, Loss: 0.7073495388031006, Final Batch Loss: 0.37141481041908264\n",
      "Epoch 819, Loss: 0.775664359331131, Final Batch Loss: 0.36083585023880005\n",
      "Epoch 820, Loss: 0.6963698863983154, Final Batch Loss: 0.2717697024345398\n",
      "Epoch 821, Loss: 0.7875178456306458, Final Batch Loss: 0.4071826636791229\n",
      "Epoch 822, Loss: 0.843000590801239, Final Batch Loss: 0.4899480938911438\n",
      "Epoch 823, Loss: 0.76876300573349, Final Batch Loss: 0.37680456042289734\n",
      "Epoch 824, Loss: 0.7606110572814941, Final Batch Loss: 0.3755389451980591\n",
      "Epoch 825, Loss: 0.8260366022586823, Final Batch Loss: 0.3899545669555664\n",
      "Epoch 826, Loss: 0.7756711542606354, Final Batch Loss: 0.40735042095184326\n",
      "Epoch 827, Loss: 0.6917272508144379, Final Batch Loss: 0.3690883219242096\n",
      "Epoch 828, Loss: 0.7983892560005188, Final Batch Loss: 0.38072076439857483\n",
      "Epoch 829, Loss: 0.6269036829471588, Final Batch Loss: 0.2643146216869354\n",
      "Epoch 830, Loss: 0.7066126763820648, Final Batch Loss: 0.3659078776836395\n",
      "Epoch 831, Loss: 0.8349128067493439, Final Batch Loss: 0.4729335308074951\n",
      "Epoch 832, Loss: 0.7490872442722321, Final Batch Loss: 0.3543657064437866\n",
      "Epoch 833, Loss: 0.7407706081867218, Final Batch Loss: 0.413306325674057\n",
      "Epoch 834, Loss: 0.7200812995433807, Final Batch Loss: 0.30816131830215454\n",
      "Epoch 835, Loss: 0.6348676383495331, Final Batch Loss: 0.2809695303440094\n",
      "Epoch 836, Loss: 0.7977401614189148, Final Batch Loss: 0.4168681800365448\n",
      "Epoch 837, Loss: 0.7398251891136169, Final Batch Loss: 0.3946652412414551\n",
      "Epoch 838, Loss: 0.6900677978992462, Final Batch Loss: 0.3176756799221039\n",
      "Epoch 839, Loss: 0.7112928926944733, Final Batch Loss: 0.2920977473258972\n",
      "Epoch 840, Loss: 0.8179759085178375, Final Batch Loss: 0.4592655897140503\n",
      "Epoch 841, Loss: 0.6476613283157349, Final Batch Loss: 0.316326767206192\n",
      "Epoch 842, Loss: 0.7152108252048492, Final Batch Loss: 0.3761131763458252\n",
      "Epoch 843, Loss: 0.6953467130661011, Final Batch Loss: 0.3813555836677551\n",
      "Epoch 844, Loss: 0.748377650976181, Final Batch Loss: 0.3713275194168091\n",
      "Epoch 845, Loss: 0.7201642692089081, Final Batch Loss: 0.36832502484321594\n",
      "Epoch 846, Loss: 0.6773920953273773, Final Batch Loss: 0.3324473798274994\n",
      "Epoch 847, Loss: 0.6585386991500854, Final Batch Loss: 0.2870010733604431\n",
      "Epoch 848, Loss: 0.6636396050453186, Final Batch Loss: 0.27718380093574524\n",
      "Epoch 849, Loss: 0.7815689742565155, Final Batch Loss: 0.34631845355033875\n",
      "Epoch 850, Loss: 0.6833343207836151, Final Batch Loss: 0.3072128891944885\n",
      "Epoch 851, Loss: 0.7972512245178223, Final Batch Loss: 0.34061557054519653\n",
      "Epoch 852, Loss: 0.7893281579017639, Final Batch Loss: 0.41067174077033997\n",
      "Epoch 853, Loss: 0.6755048334598541, Final Batch Loss: 0.35754474997520447\n",
      "Epoch 854, Loss: 0.7086511254310608, Final Batch Loss: 0.3464444577693939\n",
      "Epoch 855, Loss: 0.7864993512630463, Final Batch Loss: 0.39811891317367554\n",
      "Epoch 856, Loss: 0.7536525428295135, Final Batch Loss: 0.3725114166736603\n",
      "Epoch 857, Loss: 0.7628514766693115, Final Batch Loss: 0.3777271807193756\n",
      "Epoch 858, Loss: 0.7561704814434052, Final Batch Loss: 0.29866355657577515\n",
      "Epoch 859, Loss: 0.6975689828395844, Final Batch Loss: 0.35669201612472534\n",
      "Epoch 860, Loss: 0.7743672132492065, Final Batch Loss: 0.3908654451370239\n",
      "Epoch 861, Loss: 0.7151607871055603, Final Batch Loss: 0.33417221903800964\n",
      "Epoch 862, Loss: 0.6585285067558289, Final Batch Loss: 0.26460185647010803\n",
      "Epoch 863, Loss: 0.7483298182487488, Final Batch Loss: 0.3715353012084961\n",
      "Epoch 864, Loss: 0.7774424850940704, Final Batch Loss: 0.441756010055542\n",
      "Epoch 865, Loss: 0.7184377312660217, Final Batch Loss: 0.33836397528648376\n",
      "Epoch 866, Loss: 0.7760457694530487, Final Batch Loss: 0.3754103183746338\n",
      "Epoch 867, Loss: 0.7005687057971954, Final Batch Loss: 0.362900048494339\n",
      "Epoch 868, Loss: 0.7542913556098938, Final Batch Loss: 0.35668516159057617\n",
      "Epoch 869, Loss: 0.7127779722213745, Final Batch Loss: 0.29013732075691223\n",
      "Epoch 870, Loss: 0.818174421787262, Final Batch Loss: 0.42614415287971497\n",
      "Epoch 871, Loss: 0.5829916745424271, Final Batch Loss: 0.24970336258411407\n",
      "Epoch 872, Loss: 0.7156374156475067, Final Batch Loss: 0.36235538125038147\n",
      "Epoch 873, Loss: 0.621593564748764, Final Batch Loss: 0.3227385878562927\n",
      "Epoch 874, Loss: 0.6967737078666687, Final Batch Loss: 0.3602350056171417\n",
      "Epoch 875, Loss: 0.7688912451267242, Final Batch Loss: 0.4034808278083801\n",
      "Epoch 876, Loss: 0.8264440894126892, Final Batch Loss: 0.39965906739234924\n",
      "Epoch 877, Loss: 0.7273609340190887, Final Batch Loss: 0.30510103702545166\n",
      "Epoch 878, Loss: 0.6929081678390503, Final Batch Loss: 0.3605736792087555\n",
      "Epoch 879, Loss: 0.64601731300354, Final Batch Loss: 0.3145020008087158\n",
      "Epoch 880, Loss: 0.7395643293857574, Final Batch Loss: 0.36922702193260193\n",
      "Epoch 881, Loss: 0.6901652216911316, Final Batch Loss: 0.28742021322250366\n",
      "Epoch 882, Loss: 0.6986714005470276, Final Batch Loss: 0.36125046014785767\n",
      "Epoch 883, Loss: 0.6902847588062286, Final Batch Loss: 0.30410975217819214\n",
      "Epoch 884, Loss: 0.740188479423523, Final Batch Loss: 0.37154147028923035\n",
      "Epoch 885, Loss: 0.6806566417217255, Final Batch Loss: 0.3163650929927826\n",
      "Epoch 886, Loss: 0.7280712425708771, Final Batch Loss: 0.3787101209163666\n",
      "Epoch 887, Loss: 0.7011623382568359, Final Batch Loss: 0.37657755613327026\n",
      "Epoch 888, Loss: 0.7520557940006256, Final Batch Loss: 0.4017206132411957\n",
      "Epoch 889, Loss: 0.8206973969936371, Final Batch Loss: 0.409498393535614\n",
      "Epoch 890, Loss: 0.6741169393062592, Final Batch Loss: 0.326900839805603\n",
      "Epoch 891, Loss: 0.7773071527481079, Final Batch Loss: 0.3858392834663391\n",
      "Epoch 892, Loss: 0.8322194814682007, Final Batch Loss: 0.3816356360912323\n",
      "Epoch 893, Loss: 0.7477633655071259, Final Batch Loss: 0.3422645628452301\n",
      "Epoch 894, Loss: 0.7399542927742004, Final Batch Loss: 0.38279417157173157\n",
      "Epoch 895, Loss: 0.5943005979061127, Final Batch Loss: 0.2776549756526947\n",
      "Epoch 896, Loss: 0.6838206648826599, Final Batch Loss: 0.3241119980812073\n",
      "Epoch 897, Loss: 0.69575235247612, Final Batch Loss: 0.3591155409812927\n",
      "Epoch 898, Loss: 0.6768445670604706, Final Batch Loss: 0.29504504799842834\n",
      "Epoch 899, Loss: 0.7754387557506561, Final Batch Loss: 0.3513108789920807\n",
      "Epoch 900, Loss: 0.7473315596580505, Final Batch Loss: 0.33563563227653503\n",
      "Epoch 901, Loss: 0.7039017379283905, Final Batch Loss: 0.39664676785469055\n",
      "Epoch 902, Loss: 0.6559340357780457, Final Batch Loss: 0.3436366021633148\n",
      "Epoch 903, Loss: 0.7620455026626587, Final Batch Loss: 0.31478551030158997\n",
      "Epoch 904, Loss: 0.6940447986125946, Final Batch Loss: 0.3877418041229248\n",
      "Epoch 905, Loss: 0.6346392035484314, Final Batch Loss: 0.33689868450164795\n",
      "Epoch 906, Loss: 0.6689051985740662, Final Batch Loss: 0.34558430314064026\n",
      "Epoch 907, Loss: 0.6766276359558105, Final Batch Loss: 0.27420878410339355\n",
      "Epoch 908, Loss: 0.7150411307811737, Final Batch Loss: 0.30468007922172546\n",
      "Epoch 909, Loss: 0.6803090274333954, Final Batch Loss: 0.32906603813171387\n",
      "Epoch 910, Loss: 0.7567696869373322, Final Batch Loss: 0.3663110136985779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 911, Loss: 0.8746548891067505, Final Batch Loss: 0.3939501643180847\n",
      "Epoch 912, Loss: 0.6796793341636658, Final Batch Loss: 0.32959598302841187\n",
      "Epoch 913, Loss: 0.6780797839164734, Final Batch Loss: 0.29226386547088623\n",
      "Epoch 914, Loss: 0.7323736250400543, Final Batch Loss: 0.3826208710670471\n",
      "Epoch 915, Loss: 0.6663410365581512, Final Batch Loss: 0.35865524411201477\n",
      "Epoch 916, Loss: 0.8019772469997406, Final Batch Loss: 0.4157492220401764\n",
      "Epoch 917, Loss: 0.668382465839386, Final Batch Loss: 0.320167601108551\n",
      "Epoch 918, Loss: 0.7013532817363739, Final Batch Loss: 0.3604418933391571\n",
      "Epoch 919, Loss: 0.6888675391674042, Final Batch Loss: 0.35946714878082275\n",
      "Epoch 920, Loss: 0.7545562386512756, Final Batch Loss: 0.3822711706161499\n",
      "Epoch 921, Loss: 0.6418726742267609, Final Batch Loss: 0.2785387635231018\n",
      "Epoch 922, Loss: 0.7425849139690399, Final Batch Loss: 0.38883447647094727\n",
      "Epoch 923, Loss: 0.7553906440734863, Final Batch Loss: 0.372163861989975\n",
      "Epoch 924, Loss: 0.6454392075538635, Final Batch Loss: 0.354932963848114\n",
      "Epoch 925, Loss: 0.6607427895069122, Final Batch Loss: 0.31015026569366455\n",
      "Epoch 926, Loss: 0.6767047941684723, Final Batch Loss: 0.295232892036438\n",
      "Epoch 927, Loss: 0.7309987246990204, Final Batch Loss: 0.33464550971984863\n",
      "Epoch 928, Loss: 0.8579973578453064, Final Batch Loss: 0.47652649879455566\n",
      "Epoch 929, Loss: 0.6855111718177795, Final Batch Loss: 0.3601735234260559\n",
      "Epoch 930, Loss: 0.6382265388965607, Final Batch Loss: 0.28869494795799255\n",
      "Epoch 931, Loss: 0.6032010316848755, Final Batch Loss: 0.27763065695762634\n",
      "Epoch 932, Loss: 0.726501077413559, Final Batch Loss: 0.3804071545600891\n",
      "Epoch 933, Loss: 0.7750915586948395, Final Batch Loss: 0.4333117604255676\n",
      "Epoch 934, Loss: 0.6159490644931793, Final Batch Loss: 0.2752935290336609\n",
      "Epoch 935, Loss: 0.6422683298587799, Final Batch Loss: 0.3210764527320862\n",
      "Epoch 936, Loss: 0.684425950050354, Final Batch Loss: 0.3456787168979645\n",
      "Epoch 937, Loss: 0.7179264426231384, Final Batch Loss: 0.3073301315307617\n",
      "Epoch 938, Loss: 0.6767996549606323, Final Batch Loss: 0.2848290503025055\n",
      "Epoch 939, Loss: 0.6920854151248932, Final Batch Loss: 0.3307490050792694\n",
      "Epoch 940, Loss: 0.8866597712039948, Final Batch Loss: 0.44609326124191284\n",
      "Epoch 941, Loss: 0.814024418592453, Final Batch Loss: 0.43567708134651184\n",
      "Epoch 942, Loss: 0.6587446928024292, Final Batch Loss: 0.3155706524848938\n",
      "Epoch 943, Loss: 0.6550271809101105, Final Batch Loss: 0.3201729953289032\n",
      "Epoch 944, Loss: 0.6890248507261276, Final Batch Loss: 0.4418555796146393\n",
      "Epoch 945, Loss: 0.6195466071367264, Final Batch Loss: 0.24944235384464264\n",
      "Epoch 946, Loss: 0.6941834390163422, Final Batch Loss: 0.36863020062446594\n",
      "Epoch 947, Loss: 0.7441860735416412, Final Batch Loss: 0.35908153653144836\n",
      "Epoch 948, Loss: 0.7500483095645905, Final Batch Loss: 0.33608201146125793\n",
      "Epoch 949, Loss: 0.6731486320495605, Final Batch Loss: 0.31958234310150146\n",
      "Epoch 950, Loss: 0.6417412161827087, Final Batch Loss: 0.29262498021125793\n",
      "Epoch 951, Loss: 0.7519955933094025, Final Batch Loss: 0.4200276732444763\n",
      "Epoch 952, Loss: 0.7205638289451599, Final Batch Loss: 0.3626876771450043\n",
      "Epoch 953, Loss: 0.7175553143024445, Final Batch Loss: 0.39434316754341125\n",
      "Epoch 954, Loss: 0.583328127861023, Final Batch Loss: 0.2581427991390228\n",
      "Epoch 955, Loss: 0.725589781999588, Final Batch Loss: 0.35372281074523926\n",
      "Epoch 956, Loss: 0.6929080188274384, Final Batch Loss: 0.39908936619758606\n",
      "Epoch 957, Loss: 0.7207475006580353, Final Batch Loss: 0.35304731130599976\n",
      "Epoch 958, Loss: 0.6949386298656464, Final Batch Loss: 0.3862009048461914\n",
      "Epoch 959, Loss: 0.7315251529216766, Final Batch Loss: 0.4035012722015381\n",
      "Epoch 960, Loss: 0.7374787926673889, Final Batch Loss: 0.3772895634174347\n",
      "Epoch 961, Loss: 0.7310735583305359, Final Batch Loss: 0.38535335659980774\n",
      "Epoch 962, Loss: 0.6764673292636871, Final Batch Loss: 0.3122726380825043\n",
      "Epoch 963, Loss: 0.718389630317688, Final Batch Loss: 0.39054667949676514\n",
      "Epoch 964, Loss: 0.6616739630699158, Final Batch Loss: 0.29824334383010864\n",
      "Epoch 965, Loss: 0.7274935841560364, Final Batch Loss: 0.3503033518791199\n",
      "Epoch 966, Loss: 0.6871567964553833, Final Batch Loss: 0.25886011123657227\n",
      "Epoch 967, Loss: 0.6973901391029358, Final Batch Loss: 0.31601765751838684\n",
      "Epoch 968, Loss: 0.758454829454422, Final Batch Loss: 0.4542505145072937\n",
      "Epoch 969, Loss: 0.6869524419307709, Final Batch Loss: 0.3383844196796417\n",
      "Epoch 970, Loss: 0.6740740239620209, Final Batch Loss: 0.37767335772514343\n",
      "Epoch 971, Loss: 0.6078686118125916, Final Batch Loss: 0.2908610999584198\n",
      "Epoch 972, Loss: 0.6662376224994659, Final Batch Loss: 0.32967299222946167\n",
      "Epoch 973, Loss: 0.6074616312980652, Final Batch Loss: 0.2785833477973938\n",
      "Epoch 974, Loss: 0.7094565331935883, Final Batch Loss: 0.3630281090736389\n",
      "Epoch 975, Loss: 0.6378704607486725, Final Batch Loss: 0.35464274883270264\n",
      "Epoch 976, Loss: 0.7195574343204498, Final Batch Loss: 0.348076194524765\n",
      "Epoch 977, Loss: 0.653661459684372, Final Batch Loss: 0.32299500703811646\n",
      "Epoch 978, Loss: 0.5990452766418457, Final Batch Loss: 0.2583810091018677\n",
      "Epoch 979, Loss: 0.5331035107374191, Final Batch Loss: 0.2230728715658188\n",
      "Epoch 980, Loss: 0.5509049594402313, Final Batch Loss: 0.27608606219291687\n",
      "Epoch 981, Loss: 0.5729134678840637, Final Batch Loss: 0.263950377702713\n",
      "Epoch 982, Loss: 0.6347312331199646, Final Batch Loss: 0.2884162366390228\n",
      "Epoch 983, Loss: 0.6228358745574951, Final Batch Loss: 0.3202110528945923\n",
      "Epoch 984, Loss: 0.7046557068824768, Final Batch Loss: 0.3750160038471222\n",
      "Epoch 985, Loss: 0.6830758452415466, Final Batch Loss: 0.3686692416667938\n",
      "Epoch 986, Loss: 0.6707723140716553, Final Batch Loss: 0.32382872700691223\n",
      "Epoch 987, Loss: 0.8390703499317169, Final Batch Loss: 0.4603237509727478\n",
      "Epoch 988, Loss: 0.7023306488990784, Final Batch Loss: 0.3044123351573944\n",
      "Epoch 989, Loss: 0.7045983076095581, Final Batch Loss: 0.320732980966568\n",
      "Epoch 990, Loss: 0.5455698370933533, Final Batch Loss: 0.23435243964195251\n",
      "Epoch 991, Loss: 0.6010857224464417, Final Batch Loss: 0.28593558073043823\n",
      "Epoch 992, Loss: 0.7023583054542542, Final Batch Loss: 0.3859901428222656\n",
      "Epoch 993, Loss: 0.6480183899402618, Final Batch Loss: 0.2847278416156769\n",
      "Epoch 994, Loss: 0.6479954719543457, Final Batch Loss: 0.29922351241111755\n",
      "Epoch 995, Loss: 0.6372496783733368, Final Batch Loss: 0.2959839999675751\n",
      "Epoch 996, Loss: 0.6686464846134186, Final Batch Loss: 0.3681562542915344\n",
      "Epoch 997, Loss: 0.6174518167972565, Final Batch Loss: 0.29343241453170776\n",
      "Epoch 998, Loss: 0.6625879108905792, Final Batch Loss: 0.345369815826416\n",
      "Epoch 999, Loss: 0.6063145697116852, Final Batch Loss: 0.2788756191730499\n",
      "Epoch 1000, Loss: 0.6539681553840637, Final Batch Loss: 0.3160037398338318\n",
      "Epoch 1001, Loss: 0.6794021427631378, Final Batch Loss: 0.3170970380306244\n",
      "Epoch 1002, Loss: 0.6777736246585846, Final Batch Loss: 0.33561354875564575\n",
      "Epoch 1003, Loss: 0.6658709645271301, Final Batch Loss: 0.287134051322937\n",
      "Epoch 1004, Loss: 0.6538448333740234, Final Batch Loss: 0.3488471806049347\n",
      "Epoch 1005, Loss: 0.7930106222629547, Final Batch Loss: 0.47783875465393066\n",
      "Epoch 1006, Loss: 0.6942296624183655, Final Batch Loss: 0.345705509185791\n",
      "Epoch 1007, Loss: 0.6277244985103607, Final Batch Loss: 0.29615744948387146\n",
      "Epoch 1008, Loss: 0.6628692150115967, Final Batch Loss: 0.3416920006275177\n",
      "Epoch 1009, Loss: 0.6455662250518799, Final Batch Loss: 0.31494492292404175\n",
      "Epoch 1010, Loss: 0.6813446581363678, Final Batch Loss: 0.39637649059295654\n",
      "Epoch 1011, Loss: 0.6381367743015289, Final Batch Loss: 0.3126058578491211\n",
      "Epoch 1012, Loss: 0.590090423822403, Final Batch Loss: 0.2599967122077942\n",
      "Epoch 1013, Loss: 0.8578931093215942, Final Batch Loss: 0.5094783902168274\n",
      "Epoch 1014, Loss: 0.708941251039505, Final Batch Loss: 0.40775343775749207\n",
      "Epoch 1015, Loss: 0.6459597647190094, Final Batch Loss: 0.3203372657299042\n",
      "Epoch 1016, Loss: 0.6307204067707062, Final Batch Loss: 0.27889055013656616\n",
      "Epoch 1017, Loss: 0.6002527475357056, Final Batch Loss: 0.28005367517471313\n",
      "Epoch 1018, Loss: 0.578369140625, Final Batch Loss: 0.2879337668418884\n",
      "Epoch 1019, Loss: 0.6746892631053925, Final Batch Loss: 0.3021191656589508\n",
      "Epoch 1020, Loss: 0.7433651983737946, Final Batch Loss: 0.43249788880348206\n",
      "Epoch 1021, Loss: 0.6333846151828766, Final Batch Loss: 0.326861709356308\n",
      "Epoch 1022, Loss: 0.659158855676651, Final Batch Loss: 0.29293161630630493\n",
      "Epoch 1023, Loss: 0.633169412612915, Final Batch Loss: 0.24863535165786743\n",
      "Epoch 1024, Loss: 0.6042844355106354, Final Batch Loss: 0.2794628143310547\n",
      "Epoch 1025, Loss: 0.5686058700084686, Final Batch Loss: 0.2785941958427429\n",
      "Epoch 1026, Loss: 0.6163467168807983, Final Batch Loss: 0.31416550278663635\n",
      "Epoch 1027, Loss: 0.6871154606342316, Final Batch Loss: 0.28260982036590576\n",
      "Epoch 1028, Loss: 0.6347396671772003, Final Batch Loss: 0.3072790801525116\n",
      "Epoch 1029, Loss: 0.6302403211593628, Final Batch Loss: 0.35850605368614197\n",
      "Epoch 1030, Loss: 0.5985705703496933, Final Batch Loss: 0.22906754910945892\n",
      "Epoch 1031, Loss: 0.6253337562084198, Final Batch Loss: 0.33929747343063354\n",
      "Epoch 1032, Loss: 0.7065015733242035, Final Batch Loss: 0.36326345801353455\n",
      "Epoch 1033, Loss: 0.5878193080425262, Final Batch Loss: 0.2845577001571655\n",
      "Epoch 1034, Loss: 0.6469060480594635, Final Batch Loss: 0.33680883049964905\n",
      "Epoch 1035, Loss: 0.7381007969379425, Final Batch Loss: 0.40253856778144836\n",
      "Epoch 1036, Loss: 0.6207837760448456, Final Batch Loss: 0.31674808263778687\n",
      "Epoch 1037, Loss: 0.6909791231155396, Final Batch Loss: 0.361153244972229\n",
      "Epoch 1038, Loss: 0.6387886703014374, Final Batch Loss: 0.35515889525413513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1039, Loss: 0.6495890915393829, Final Batch Loss: 0.33570703864097595\n",
      "Epoch 1040, Loss: 0.5801747441291809, Final Batch Loss: 0.29556548595428467\n",
      "Epoch 1041, Loss: 0.6618485450744629, Final Batch Loss: 0.2785467505455017\n",
      "Epoch 1042, Loss: 0.6593926548957825, Final Batch Loss: 0.3121950924396515\n",
      "Epoch 1043, Loss: 0.6141230463981628, Final Batch Loss: 0.32451772689819336\n",
      "Epoch 1044, Loss: 0.6884312629699707, Final Batch Loss: 0.3783690333366394\n",
      "Epoch 1045, Loss: 0.6409764289855957, Final Batch Loss: 0.30406269431114197\n",
      "Epoch 1046, Loss: 0.5983224809169769, Final Batch Loss: 0.2983010411262512\n",
      "Epoch 1047, Loss: 0.6765726804733276, Final Batch Loss: 0.3424188792705536\n",
      "Epoch 1048, Loss: 0.6267887055873871, Final Batch Loss: 0.2937278151512146\n",
      "Epoch 1049, Loss: 0.7190218567848206, Final Batch Loss: 0.39445996284484863\n",
      "Epoch 1050, Loss: 0.6670904457569122, Final Batch Loss: 0.34705957770347595\n",
      "Epoch 1051, Loss: 0.673409640789032, Final Batch Loss: 0.3369915783405304\n",
      "Epoch 1052, Loss: 0.5636855661869049, Final Batch Loss: 0.2777652442455292\n",
      "Epoch 1053, Loss: 0.5771519243717194, Final Batch Loss: 0.2924599051475525\n",
      "Epoch 1054, Loss: 0.5897367000579834, Final Batch Loss: 0.2520398199558258\n",
      "Epoch 1055, Loss: 0.5323152244091034, Final Batch Loss: 0.27315953373908997\n",
      "Epoch 1056, Loss: 0.7953822612762451, Final Batch Loss: 0.44289839267730713\n",
      "Epoch 1057, Loss: 0.6319108307361603, Final Batch Loss: 0.31310731172561646\n",
      "Epoch 1058, Loss: 0.5944298505783081, Final Batch Loss: 0.25728678703308105\n",
      "Epoch 1059, Loss: 0.8310478925704956, Final Batch Loss: 0.4775564670562744\n",
      "Epoch 1060, Loss: 0.5804972052574158, Final Batch Loss: 0.28699877858161926\n",
      "Epoch 1061, Loss: 0.6382374167442322, Final Batch Loss: 0.31067076325416565\n",
      "Epoch 1062, Loss: 0.6065163612365723, Final Batch Loss: 0.29078584909439087\n",
      "Epoch 1063, Loss: 0.6126498878002167, Final Batch Loss: 0.31137603521347046\n",
      "Epoch 1064, Loss: 0.6221649944782257, Final Batch Loss: 0.27876368165016174\n",
      "Epoch 1065, Loss: 0.6810776591300964, Final Batch Loss: 0.35644984245300293\n",
      "Epoch 1066, Loss: 0.6302692294120789, Final Batch Loss: 0.3170996308326721\n",
      "Epoch 1067, Loss: 0.643276184797287, Final Batch Loss: 0.34053564071655273\n",
      "Epoch 1068, Loss: 0.6157622933387756, Final Batch Loss: 0.3258585035800934\n",
      "Epoch 1069, Loss: 0.5957813560962677, Final Batch Loss: 0.24400880932807922\n",
      "Epoch 1070, Loss: 0.6201376914978027, Final Batch Loss: 0.30043575167655945\n",
      "Epoch 1071, Loss: 0.5885306298732758, Final Batch Loss: 0.3113703429698944\n",
      "Epoch 1072, Loss: 0.6905744075775146, Final Batch Loss: 0.3534861207008362\n",
      "Epoch 1073, Loss: 0.6563476324081421, Final Batch Loss: 0.36109602451324463\n",
      "Epoch 1074, Loss: 0.5787132084369659, Final Batch Loss: 0.2964579463005066\n",
      "Epoch 1075, Loss: 0.5884182751178741, Final Batch Loss: 0.2871703803539276\n",
      "Epoch 1076, Loss: 0.5867564082145691, Final Batch Loss: 0.28420019149780273\n",
      "Epoch 1077, Loss: 0.6723556816577911, Final Batch Loss: 0.31351348757743835\n",
      "Epoch 1078, Loss: 0.6911625862121582, Final Batch Loss: 0.3910403847694397\n",
      "Epoch 1079, Loss: 0.6670050323009491, Final Batch Loss: 0.3267294466495514\n",
      "Epoch 1080, Loss: 0.6146803796291351, Final Batch Loss: 0.30935153365135193\n",
      "Epoch 1081, Loss: 0.5987249910831451, Final Batch Loss: 0.2956562936306\n",
      "Epoch 1082, Loss: 0.6023565828800201, Final Batch Loss: 0.2848018407821655\n",
      "Epoch 1083, Loss: 0.6506659686565399, Final Batch Loss: 0.33841848373413086\n",
      "Epoch 1084, Loss: 0.5815017819404602, Final Batch Loss: 0.21941488981246948\n",
      "Epoch 1085, Loss: 0.6089059710502625, Final Batch Loss: 0.2462199628353119\n",
      "Epoch 1086, Loss: 0.6098873019218445, Final Batch Loss: 0.28023388981819153\n",
      "Epoch 1087, Loss: 0.6412209570407867, Final Batch Loss: 0.3094188868999481\n",
      "Epoch 1088, Loss: 0.5849684476852417, Final Batch Loss: 0.28149476647377014\n",
      "Epoch 1089, Loss: 0.7392600476741791, Final Batch Loss: 0.4106277525424957\n",
      "Epoch 1090, Loss: 0.5982625186443329, Final Batch Loss: 0.2740461528301239\n",
      "Epoch 1091, Loss: 0.6336301863193512, Final Batch Loss: 0.3266262412071228\n",
      "Epoch 1092, Loss: 0.612974226474762, Final Batch Loss: 0.2884904444217682\n",
      "Epoch 1093, Loss: 0.5510618388652802, Final Batch Loss: 0.30033078789711\n",
      "Epoch 1094, Loss: 0.6986914277076721, Final Batch Loss: 0.38715699315071106\n",
      "Epoch 1095, Loss: 0.6235750019550323, Final Batch Loss: 0.29604893922805786\n",
      "Epoch 1096, Loss: 0.5777213275432587, Final Batch Loss: 0.25880002975463867\n",
      "Epoch 1097, Loss: 0.51055608689785, Final Batch Loss: 0.23570357263088226\n",
      "Epoch 1098, Loss: 0.6431040167808533, Final Batch Loss: 0.345753937959671\n",
      "Epoch 1099, Loss: 0.5441343188285828, Final Batch Loss: 0.24200952053070068\n",
      "Epoch 1100, Loss: 0.5448125302791595, Final Batch Loss: 0.2740432918071747\n",
      "Epoch 1101, Loss: 0.566815972328186, Final Batch Loss: 0.285834401845932\n",
      "Epoch 1102, Loss: 0.5832605957984924, Final Batch Loss: 0.2748109996318817\n",
      "Epoch 1103, Loss: 0.6220090091228485, Final Batch Loss: 0.34766238927841187\n",
      "Epoch 1104, Loss: 0.6726016402244568, Final Batch Loss: 0.29174187779426575\n",
      "Epoch 1105, Loss: 0.6016016900539398, Final Batch Loss: 0.2997327148914337\n",
      "Epoch 1106, Loss: 0.5956347584724426, Final Batch Loss: 0.28784844279289246\n",
      "Epoch 1107, Loss: 0.6965155005455017, Final Batch Loss: 0.3826134204864502\n",
      "Epoch 1108, Loss: 0.6436195075511932, Final Batch Loss: 0.34803104400634766\n",
      "Epoch 1109, Loss: 0.5417448580265045, Final Batch Loss: 0.28725671768188477\n",
      "Epoch 1110, Loss: 0.6202240288257599, Final Batch Loss: 0.36180949211120605\n",
      "Epoch 1111, Loss: 0.6201782822608948, Final Batch Loss: 0.33700332045555115\n",
      "Epoch 1112, Loss: 0.5611081719398499, Final Batch Loss: 0.2628428339958191\n",
      "Epoch 1113, Loss: 0.6007632613182068, Final Batch Loss: 0.3025798201560974\n",
      "Epoch 1114, Loss: 0.5694037079811096, Final Batch Loss: 0.2927466928958893\n",
      "Epoch 1115, Loss: 0.5609858185052872, Final Batch Loss: 0.23923887312412262\n",
      "Epoch 1116, Loss: 0.6604177355766296, Final Batch Loss: 0.345292866230011\n",
      "Epoch 1117, Loss: 0.6373547911643982, Final Batch Loss: 0.3686756491661072\n",
      "Epoch 1118, Loss: 0.529893696308136, Final Batch Loss: 0.23643019795417786\n",
      "Epoch 1119, Loss: 0.6113527715206146, Final Batch Loss: 0.282330185174942\n",
      "Epoch 1120, Loss: 0.5659925788640976, Final Batch Loss: 0.24654562771320343\n",
      "Epoch 1121, Loss: 0.5823963284492493, Final Batch Loss: 0.28763124346733093\n",
      "Epoch 1122, Loss: 0.6414987742900848, Final Batch Loss: 0.3278326988220215\n",
      "Epoch 1123, Loss: 0.5176911652088165, Final Batch Loss: 0.23561927676200867\n",
      "Epoch 1124, Loss: 0.5365390479564667, Final Batch Loss: 0.2491661012172699\n",
      "Epoch 1125, Loss: 0.6293506920337677, Final Batch Loss: 0.2674216330051422\n",
      "Epoch 1126, Loss: 0.6126745641231537, Final Batch Loss: 0.22851061820983887\n",
      "Epoch 1127, Loss: 0.5788368284702301, Final Batch Loss: 0.3088345527648926\n",
      "Epoch 1128, Loss: 0.5849865227937698, Final Batch Loss: 0.3375331163406372\n",
      "Epoch 1129, Loss: 0.5633921921253204, Final Batch Loss: 0.2833130955696106\n",
      "Epoch 1130, Loss: 0.6523388922214508, Final Batch Loss: 0.39645859599113464\n",
      "Epoch 1131, Loss: 0.5399907678365707, Final Batch Loss: 0.24708814918994904\n",
      "Epoch 1132, Loss: 0.5871642231941223, Final Batch Loss: 0.2734578549861908\n",
      "Epoch 1133, Loss: 0.5699560791254044, Final Batch Loss: 0.2486325353384018\n",
      "Epoch 1134, Loss: 0.5791324377059937, Final Batch Loss: 0.2680935859680176\n",
      "Epoch 1135, Loss: 0.5988552123308182, Final Batch Loss: 0.2443433552980423\n",
      "Epoch 1136, Loss: 0.6065273880958557, Final Batch Loss: 0.25920405983924866\n",
      "Epoch 1137, Loss: 0.5583466440439224, Final Batch Loss: 0.24055419862270355\n",
      "Epoch 1138, Loss: 0.5394417941570282, Final Batch Loss: 0.28169330954551697\n",
      "Epoch 1139, Loss: 0.5933732092380524, Final Batch Loss: 0.27030283212661743\n",
      "Epoch 1140, Loss: 0.6392539441585541, Final Batch Loss: 0.2945748567581177\n",
      "Epoch 1141, Loss: 0.5742453932762146, Final Batch Loss: 0.3037201166152954\n",
      "Epoch 1142, Loss: 0.5599765777587891, Final Batch Loss: 0.29960861802101135\n",
      "Epoch 1143, Loss: 0.5829912424087524, Final Batch Loss: 0.2897024154663086\n",
      "Epoch 1144, Loss: 0.6731502413749695, Final Batch Loss: 0.3590083420276642\n",
      "Epoch 1145, Loss: 0.5702773034572601, Final Batch Loss: 0.2877146303653717\n",
      "Epoch 1146, Loss: 0.6263320744037628, Final Batch Loss: 0.32975006103515625\n",
      "Epoch 1147, Loss: 0.5451820343732834, Final Batch Loss: 0.24396751821041107\n",
      "Epoch 1148, Loss: 0.6002386510372162, Final Batch Loss: 0.28444409370422363\n",
      "Epoch 1149, Loss: 0.6190438568592072, Final Batch Loss: 0.27415385842323303\n",
      "Epoch 1150, Loss: 0.7661158740520477, Final Batch Loss: 0.33102384209632874\n",
      "Epoch 1151, Loss: 0.6044669151306152, Final Batch Loss: 0.3258758783340454\n",
      "Epoch 1152, Loss: 0.6020047068595886, Final Batch Loss: 0.3111424744129181\n",
      "Epoch 1153, Loss: 0.6377974152565002, Final Batch Loss: 0.3159657418727875\n",
      "Epoch 1154, Loss: 0.5903855860233307, Final Batch Loss: 0.32061493396759033\n",
      "Epoch 1155, Loss: 0.578056663274765, Final Batch Loss: 0.3090214431285858\n",
      "Epoch 1156, Loss: 0.6874672770500183, Final Batch Loss: 0.3902992606163025\n",
      "Epoch 1157, Loss: 0.5660338401794434, Final Batch Loss: 0.27030548453330994\n",
      "Epoch 1158, Loss: 0.6355187296867371, Final Batch Loss: 0.33105164766311646\n",
      "Epoch 1159, Loss: 0.6779598295688629, Final Batch Loss: 0.3625403046607971\n",
      "Epoch 1160, Loss: 0.5859860181808472, Final Batch Loss: 0.29061588644981384\n",
      "Epoch 1161, Loss: 0.5427950322628021, Final Batch Loss: 0.26770836114883423\n",
      "Epoch 1162, Loss: 0.50946444272995, Final Batch Loss: 0.19828736782073975\n",
      "Epoch 1163, Loss: 0.5843044966459274, Final Batch Loss: 0.3370724618434906\n",
      "Epoch 1164, Loss: 0.6651930809020996, Final Batch Loss: 0.3140993118286133\n",
      "Epoch 1165, Loss: 0.5706649124622345, Final Batch Loss: 0.2785964608192444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1166, Loss: 0.5602606236934662, Final Batch Loss: 0.2710472643375397\n",
      "Epoch 1167, Loss: 0.6144270896911621, Final Batch Loss: 0.3379577696323395\n",
      "Epoch 1168, Loss: 0.5800814926624298, Final Batch Loss: 0.2604052722454071\n",
      "Epoch 1169, Loss: 0.6164461970329285, Final Batch Loss: 0.3182564377784729\n",
      "Epoch 1170, Loss: 0.5915980339050293, Final Batch Loss: 0.25555646419525146\n",
      "Epoch 1171, Loss: 0.5900099873542786, Final Batch Loss: 0.2849307656288147\n",
      "Epoch 1172, Loss: 0.5080664455890656, Final Batch Loss: 0.20719680190086365\n",
      "Epoch 1173, Loss: 0.5961478352546692, Final Batch Loss: 0.3468300402164459\n",
      "Epoch 1174, Loss: 0.5876196622848511, Final Batch Loss: 0.27271270751953125\n",
      "Epoch 1175, Loss: 0.575197771191597, Final Batch Loss: 0.24092046916484833\n",
      "Epoch 1176, Loss: 0.555338054895401, Final Batch Loss: 0.27820757031440735\n",
      "Epoch 1177, Loss: 0.6027949750423431, Final Batch Loss: 0.3402758836746216\n",
      "Epoch 1178, Loss: 0.5851878523826599, Final Batch Loss: 0.28328806161880493\n",
      "Epoch 1179, Loss: 0.6104613244533539, Final Batch Loss: 0.30012303590774536\n",
      "Epoch 1180, Loss: 0.607710599899292, Final Batch Loss: 0.28719785809516907\n",
      "Epoch 1181, Loss: 0.5488873422145844, Final Batch Loss: 0.280565470457077\n",
      "Epoch 1182, Loss: 0.6327225565910339, Final Batch Loss: 0.3202895522117615\n",
      "Epoch 1183, Loss: 0.5805936753749847, Final Batch Loss: 0.2595655024051666\n",
      "Epoch 1184, Loss: 0.5560353100299835, Final Batch Loss: 0.28960058093070984\n",
      "Epoch 1185, Loss: 0.6155371367931366, Final Batch Loss: 0.3361567556858063\n",
      "Epoch 1186, Loss: 0.6764958500862122, Final Batch Loss: 0.3961666524410248\n",
      "Epoch 1187, Loss: 0.4851432144641876, Final Batch Loss: 0.17172548174858093\n",
      "Epoch 1188, Loss: 0.6284242868423462, Final Batch Loss: 0.3261502683162689\n",
      "Epoch 1189, Loss: 0.6064785420894623, Final Batch Loss: 0.2863595187664032\n",
      "Epoch 1190, Loss: 0.5720464885234833, Final Batch Loss: 0.2702621817588806\n",
      "Epoch 1191, Loss: 0.5538425445556641, Final Batch Loss: 0.2659841775894165\n",
      "Epoch 1192, Loss: 0.6850837171077728, Final Batch Loss: 0.34118586778640747\n",
      "Epoch 1193, Loss: 0.7290720641613007, Final Batch Loss: 0.40434494614601135\n",
      "Epoch 1194, Loss: 0.6244072020053864, Final Batch Loss: 0.3128661513328552\n",
      "Epoch 1195, Loss: 0.574807733297348, Final Batch Loss: 0.23918727040290833\n",
      "Epoch 1196, Loss: 0.5624926090240479, Final Batch Loss: 0.2937736511230469\n",
      "Epoch 1197, Loss: 0.5488837957382202, Final Batch Loss: 0.290214866399765\n",
      "Epoch 1198, Loss: 0.5043365955352783, Final Batch Loss: 0.20523163676261902\n",
      "Epoch 1199, Loss: 0.6102282702922821, Final Batch Loss: 0.3090255856513977\n",
      "Epoch 1200, Loss: 0.6139014065265656, Final Batch Loss: 0.3048580586910248\n",
      "Epoch 1201, Loss: 0.539004847407341, Final Batch Loss: 0.21695999801158905\n",
      "Epoch 1202, Loss: 0.7141117751598358, Final Batch Loss: 0.3894287347793579\n",
      "Epoch 1203, Loss: 0.5634019672870636, Final Batch Loss: 0.31729015707969666\n",
      "Epoch 1204, Loss: 0.5715436339378357, Final Batch Loss: 0.2871928811073303\n",
      "Epoch 1205, Loss: 0.574016883969307, Final Batch Loss: 0.2205515056848526\n",
      "Epoch 1206, Loss: 0.5340519398450851, Final Batch Loss: 0.22645197808742523\n",
      "Epoch 1207, Loss: 0.5929731726646423, Final Batch Loss: 0.26816698908805847\n",
      "Epoch 1208, Loss: 0.6554651856422424, Final Batch Loss: 0.38470014929771423\n",
      "Epoch 1209, Loss: 0.5437712371349335, Final Batch Loss: 0.28144192695617676\n",
      "Epoch 1210, Loss: 0.5921755433082581, Final Batch Loss: 0.29695621132850647\n",
      "Epoch 1211, Loss: 0.621161699295044, Final Batch Loss: 0.3038942217826843\n",
      "Epoch 1212, Loss: 0.6630130708217621, Final Batch Loss: 0.29893720149993896\n",
      "Epoch 1213, Loss: 0.5702181458473206, Final Batch Loss: 0.2872077524662018\n",
      "Epoch 1214, Loss: 0.6093090772628784, Final Batch Loss: 0.3032498359680176\n",
      "Epoch 1215, Loss: 0.6822243928909302, Final Batch Loss: 0.3887947201728821\n",
      "Epoch 1216, Loss: 0.49433276057243347, Final Batch Loss: 0.18473035097122192\n",
      "Epoch 1217, Loss: 0.6591187715530396, Final Batch Loss: 0.37076160311698914\n",
      "Epoch 1218, Loss: 0.6218108236789703, Final Batch Loss: 0.28818264603614807\n",
      "Epoch 1219, Loss: 0.626461535692215, Final Batch Loss: 0.3176637291908264\n",
      "Epoch 1220, Loss: 0.5501271784305573, Final Batch Loss: 0.23626244068145752\n",
      "Epoch 1221, Loss: 0.4990660548210144, Final Batch Loss: 0.2513484060764313\n",
      "Epoch 1222, Loss: 0.6905494630336761, Final Batch Loss: 0.3799261748790741\n",
      "Epoch 1223, Loss: 0.6286250948905945, Final Batch Loss: 0.2677402198314667\n",
      "Epoch 1224, Loss: 0.5587983131408691, Final Batch Loss: 0.2764253616333008\n",
      "Epoch 1225, Loss: 0.5619046986103058, Final Batch Loss: 0.2701703906059265\n",
      "Epoch 1226, Loss: 0.591478019952774, Final Batch Loss: 0.2730284631252289\n",
      "Epoch 1227, Loss: 0.5365047454833984, Final Batch Loss: 0.25002193450927734\n",
      "Epoch 1228, Loss: 0.6751681566238403, Final Batch Loss: 0.38981887698173523\n",
      "Epoch 1229, Loss: 0.5440352261066437, Final Batch Loss: 0.23499137163162231\n",
      "Epoch 1230, Loss: 0.5662528872489929, Final Batch Loss: 0.31279703974723816\n",
      "Epoch 1231, Loss: 0.5293666571378708, Final Batch Loss: 0.23529253900051117\n",
      "Epoch 1232, Loss: 0.5472691357135773, Final Batch Loss: 0.2862928509712219\n",
      "Epoch 1233, Loss: 0.5685992538928986, Final Batch Loss: 0.2701062262058258\n",
      "Epoch 1234, Loss: 0.5732750296592712, Final Batch Loss: 0.29181063175201416\n",
      "Epoch 1235, Loss: 0.5971867144107819, Final Batch Loss: 0.30287808179855347\n",
      "Epoch 1236, Loss: 0.5925054252147675, Final Batch Loss: 0.2883298993110657\n",
      "Epoch 1237, Loss: 0.6387558132410049, Final Batch Loss: 0.4238612949848175\n",
      "Epoch 1238, Loss: 0.6713716387748718, Final Batch Loss: 0.3256711959838867\n",
      "Epoch 1239, Loss: 0.5494491159915924, Final Batch Loss: 0.2745208442211151\n",
      "Epoch 1240, Loss: 0.5531180500984192, Final Batch Loss: 0.27707985043525696\n",
      "Epoch 1241, Loss: 0.6522892415523529, Final Batch Loss: 0.2923412024974823\n",
      "Epoch 1242, Loss: 0.5447700917720795, Final Batch Loss: 0.2950516641139984\n",
      "Epoch 1243, Loss: 0.5482700765132904, Final Batch Loss: 0.28818371891975403\n",
      "Epoch 1244, Loss: 0.6457797288894653, Final Batch Loss: 0.35817036032676697\n",
      "Epoch 1245, Loss: 0.5627430379390717, Final Batch Loss: 0.2395283281803131\n",
      "Epoch 1246, Loss: 0.6239621639251709, Final Batch Loss: 0.3244410753250122\n",
      "Epoch 1247, Loss: 0.5001616030931473, Final Batch Loss: 0.22649721801280975\n",
      "Epoch 1248, Loss: 0.5110367834568024, Final Batch Loss: 0.22566208243370056\n",
      "Epoch 1249, Loss: 0.5409004092216492, Final Batch Loss: 0.23841270804405212\n",
      "Epoch 1250, Loss: 0.5664425194263458, Final Batch Loss: 0.30701008439064026\n",
      "Epoch 1251, Loss: 0.5792178809642792, Final Batch Loss: 0.2861996293067932\n",
      "Epoch 1252, Loss: 0.7281919419765472, Final Batch Loss: 0.35463520884513855\n",
      "Epoch 1253, Loss: 0.6169725656509399, Final Batch Loss: 0.35573944449424744\n",
      "Epoch 1254, Loss: 0.5700342953205109, Final Batch Loss: 0.26394280791282654\n",
      "Epoch 1255, Loss: 0.5485201776027679, Final Batch Loss: 0.26881739497184753\n",
      "Epoch 1256, Loss: 0.5285362303256989, Final Batch Loss: 0.21145585179328918\n",
      "Epoch 1257, Loss: 0.5660758763551712, Final Batch Loss: 0.18418757617473602\n",
      "Epoch 1258, Loss: 0.5585263073444366, Final Batch Loss: 0.2864037752151489\n",
      "Epoch 1259, Loss: 0.5135194659233093, Final Batch Loss: 0.25219690799713135\n",
      "Epoch 1260, Loss: 0.5752533376216888, Final Batch Loss: 0.2810016870498657\n",
      "Epoch 1261, Loss: 0.5245045125484467, Final Batch Loss: 0.206702321767807\n",
      "Epoch 1262, Loss: 0.6152236461639404, Final Batch Loss: 0.3262694776058197\n",
      "Epoch 1263, Loss: 0.6719656586647034, Final Batch Loss: 0.3349655270576477\n",
      "Epoch 1264, Loss: 0.5171152800321579, Final Batch Loss: 0.2705397605895996\n",
      "Epoch 1265, Loss: 0.5599384903907776, Final Batch Loss: 0.27656474709510803\n",
      "Epoch 1266, Loss: 0.5755209624767303, Final Batch Loss: 0.2592928409576416\n",
      "Epoch 1267, Loss: 0.5354243218898773, Final Batch Loss: 0.21380114555358887\n",
      "Epoch 1268, Loss: 0.5694334805011749, Final Batch Loss: 0.25390172004699707\n",
      "Epoch 1269, Loss: 0.5723730325698853, Final Batch Loss: 0.2545752227306366\n",
      "Epoch 1270, Loss: 0.5277263969182968, Final Batch Loss: 0.2909279465675354\n",
      "Epoch 1271, Loss: 0.5366115570068359, Final Batch Loss: 0.2321297526359558\n",
      "Epoch 1272, Loss: 0.5142972469329834, Final Batch Loss: 0.22193452715873718\n",
      "Epoch 1273, Loss: 0.5917873978614807, Final Batch Loss: 0.32621923089027405\n",
      "Epoch 1274, Loss: 0.6099491119384766, Final Batch Loss: 0.26292988657951355\n",
      "Epoch 1275, Loss: 0.5156756639480591, Final Batch Loss: 0.22909143567085266\n",
      "Epoch 1276, Loss: 0.5692938566207886, Final Batch Loss: 0.30003491044044495\n",
      "Epoch 1277, Loss: 0.5628381073474884, Final Batch Loss: 0.28678640723228455\n",
      "Epoch 1278, Loss: 0.47064509987831116, Final Batch Loss: 0.2153758704662323\n",
      "Epoch 1279, Loss: 0.5151466429233551, Final Batch Loss: 0.23131996393203735\n",
      "Epoch 1280, Loss: 0.5918273031711578, Final Batch Loss: 0.33844271302223206\n",
      "Epoch 1281, Loss: 0.5238025039434433, Final Batch Loss: 0.2184765785932541\n",
      "Epoch 1282, Loss: 0.7206184267997742, Final Batch Loss: 0.3585837185382843\n",
      "Epoch 1283, Loss: 0.5814243257045746, Final Batch Loss: 0.3019644618034363\n",
      "Epoch 1284, Loss: 0.5780843198299408, Final Batch Loss: 0.2584848701953888\n",
      "Epoch 1285, Loss: 0.5083366483449936, Final Batch Loss: 0.2276710420846939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1286, Loss: 0.5891133844852448, Final Batch Loss: 0.28947100043296814\n",
      "Epoch 1287, Loss: 0.6867775321006775, Final Batch Loss: 0.30916059017181396\n",
      "Epoch 1288, Loss: 0.7063987255096436, Final Batch Loss: 0.4198760688304901\n",
      "Epoch 1289, Loss: 0.5953017771244049, Final Batch Loss: 0.34539881348609924\n",
      "Epoch 1290, Loss: 0.46939945220947266, Final Batch Loss: 0.21646744012832642\n",
      "Epoch 1291, Loss: 0.5841465890407562, Final Batch Loss: 0.2877611815929413\n",
      "Epoch 1292, Loss: 0.5313195139169693, Final Batch Loss: 0.2469043880701065\n",
      "Epoch 1293, Loss: 0.584036111831665, Final Batch Loss: 0.30140048265457153\n",
      "Epoch 1294, Loss: 0.5705974400043488, Final Batch Loss: 0.27488550543785095\n",
      "Epoch 1295, Loss: 0.5315920412540436, Final Batch Loss: 0.2587592601776123\n",
      "Epoch 1296, Loss: 0.5378326773643494, Final Batch Loss: 0.2589131295681\n",
      "Epoch 1297, Loss: 0.5700762271881104, Final Batch Loss: 0.2441924512386322\n",
      "Epoch 1298, Loss: 0.5795176923274994, Final Batch Loss: 0.27036285400390625\n",
      "Epoch 1299, Loss: 0.6114358305931091, Final Batch Loss: 0.28708916902542114\n",
      "Epoch 1300, Loss: 0.5437794327735901, Final Batch Loss: 0.28801244497299194\n",
      "Epoch 1301, Loss: 0.4953982084989548, Final Batch Loss: 0.22169221937656403\n",
      "Epoch 1302, Loss: 0.6305323243141174, Final Batch Loss: 0.36518633365631104\n",
      "Epoch 1303, Loss: 0.5660386979579926, Final Batch Loss: 0.30726751685142517\n",
      "Epoch 1304, Loss: 0.6141921579837799, Final Batch Loss: 0.33189234137535095\n",
      "Epoch 1305, Loss: 0.5568867325782776, Final Batch Loss: 0.27197200059890747\n",
      "Epoch 1306, Loss: 0.5959553122520447, Final Batch Loss: 0.26926663517951965\n",
      "Epoch 1307, Loss: 0.5538263618946075, Final Batch Loss: 0.2970257103443146\n",
      "Epoch 1308, Loss: 0.5425525903701782, Final Batch Loss: 0.3048890233039856\n",
      "Epoch 1309, Loss: 0.5665493607521057, Final Batch Loss: 0.26723650097846985\n",
      "Epoch 1310, Loss: 0.5854421854019165, Final Batch Loss: 0.30547985434532166\n",
      "Epoch 1311, Loss: 0.48405328392982483, Final Batch Loss: 0.20679140090942383\n",
      "Epoch 1312, Loss: 0.6590665280818939, Final Batch Loss: 0.35073909163475037\n",
      "Epoch 1313, Loss: 0.6119982302188873, Final Batch Loss: 0.31576505303382874\n",
      "Epoch 1314, Loss: 0.5934275984764099, Final Batch Loss: 0.33019372820854187\n",
      "Epoch 1315, Loss: 0.6297067701816559, Final Batch Loss: 0.3509905934333801\n",
      "Epoch 1316, Loss: 0.5392647683620453, Final Batch Loss: 0.27695974707603455\n",
      "Epoch 1317, Loss: 0.5683205127716064, Final Batch Loss: 0.30764147639274597\n",
      "Epoch 1318, Loss: 0.597151905298233, Final Batch Loss: 0.32668405771255493\n",
      "Epoch 1319, Loss: 0.6508401334285736, Final Batch Loss: 0.39429691433906555\n",
      "Epoch 1320, Loss: 0.5361945033073425, Final Batch Loss: 0.25221148133277893\n",
      "Epoch 1321, Loss: 0.5886785984039307, Final Batch Loss: 0.3089059293270111\n",
      "Epoch 1322, Loss: 0.6165040135383606, Final Batch Loss: 0.3026273846626282\n",
      "Epoch 1323, Loss: 0.7091666460037231, Final Batch Loss: 0.39074569940567017\n",
      "Epoch 1324, Loss: 0.45900751650333405, Final Batch Loss: 0.189470574259758\n",
      "Epoch 1325, Loss: 0.5693845152854919, Final Batch Loss: 0.30293115973472595\n",
      "Epoch 1326, Loss: 0.5998017340898514, Final Batch Loss: 0.2276577204465866\n",
      "Epoch 1327, Loss: 0.5530599504709244, Final Batch Loss: 0.24215824902057648\n",
      "Epoch 1328, Loss: 0.6111330389976501, Final Batch Loss: 0.3493896424770355\n",
      "Epoch 1329, Loss: 0.5505149364471436, Final Batch Loss: 0.31858378648757935\n",
      "Epoch 1330, Loss: 0.4676004499197006, Final Batch Loss: 0.2170569747686386\n",
      "Epoch 1331, Loss: 0.5349391996860504, Final Batch Loss: 0.2982732057571411\n",
      "Epoch 1332, Loss: 0.5335100740194321, Final Batch Loss: 0.231974259018898\n",
      "Epoch 1333, Loss: 0.5280054956674576, Final Batch Loss: 0.2842063307762146\n",
      "Epoch 1334, Loss: 0.6033624708652496, Final Batch Loss: 0.31756144762039185\n",
      "Epoch 1335, Loss: 0.6951469480991364, Final Batch Loss: 0.35153716802597046\n",
      "Epoch 1336, Loss: 0.5781440287828445, Final Batch Loss: 0.3406467139720917\n",
      "Epoch 1337, Loss: 0.5789992809295654, Final Batch Loss: 0.2470400333404541\n",
      "Epoch 1338, Loss: 0.6940253674983978, Final Batch Loss: 0.38810956478118896\n",
      "Epoch 1339, Loss: 0.6203287839889526, Final Batch Loss: 0.2979084551334381\n",
      "Epoch 1340, Loss: 0.4852585196495056, Final Batch Loss: 0.2151516079902649\n",
      "Epoch 1341, Loss: 0.5166388899087906, Final Batch Loss: 0.24570776522159576\n",
      "Epoch 1342, Loss: 0.5143819451332092, Final Batch Loss: 0.20943975448608398\n",
      "Epoch 1343, Loss: 0.513395369052887, Final Batch Loss: 0.2331017553806305\n",
      "Epoch 1344, Loss: 0.7008081376552582, Final Batch Loss: 0.3732507824897766\n",
      "Epoch 1345, Loss: 0.653668224811554, Final Batch Loss: 0.35249027609825134\n",
      "Epoch 1346, Loss: 0.6036328077316284, Final Batch Loss: 0.3107307553291321\n",
      "Epoch 1347, Loss: 0.5585839450359344, Final Batch Loss: 0.27959227561950684\n",
      "Epoch 1348, Loss: 0.45937836170196533, Final Batch Loss: 0.22272588312625885\n",
      "Epoch 1349, Loss: 0.6061208844184875, Final Batch Loss: 0.3539741039276123\n",
      "Epoch 1350, Loss: 0.5113016963005066, Final Batch Loss: 0.26046398282051086\n",
      "Epoch 1351, Loss: 0.4954824894666672, Final Batch Loss: 0.283868670463562\n",
      "Epoch 1352, Loss: 0.6120898723602295, Final Batch Loss: 0.33532968163490295\n",
      "Epoch 1353, Loss: 0.6075215637683868, Final Batch Loss: 0.28435686230659485\n",
      "Epoch 1354, Loss: 0.4721430838108063, Final Batch Loss: 0.19135519862174988\n",
      "Epoch 1355, Loss: 0.5276679396629333, Final Batch Loss: 0.22272461652755737\n",
      "Epoch 1356, Loss: 0.5681886523962021, Final Batch Loss: 0.3218550384044647\n",
      "Epoch 1357, Loss: 0.6122885942459106, Final Batch Loss: 0.32521024346351624\n",
      "Epoch 1358, Loss: 0.522400364279747, Final Batch Loss: 0.20931373536586761\n",
      "Epoch 1359, Loss: 0.5168458968400955, Final Batch Loss: 0.2692311704158783\n",
      "Epoch 1360, Loss: 0.6171484887599945, Final Batch Loss: 0.3198077380657196\n",
      "Epoch 1361, Loss: 0.5769399404525757, Final Batch Loss: 0.27460986375808716\n",
      "Epoch 1362, Loss: 0.5644406378269196, Final Batch Loss: 0.2967311143875122\n",
      "Epoch 1363, Loss: 0.5358002036809921, Final Batch Loss: 0.31109023094177246\n",
      "Epoch 1364, Loss: 0.5982808172702789, Final Batch Loss: 0.3384653925895691\n",
      "Epoch 1365, Loss: 0.5373920202255249, Final Batch Loss: 0.24295365810394287\n",
      "Epoch 1366, Loss: 0.569498211145401, Final Batch Loss: 0.24861443042755127\n",
      "Epoch 1367, Loss: 0.5827215313911438, Final Batch Loss: 0.3208767771720886\n",
      "Epoch 1368, Loss: 0.5624793469905853, Final Batch Loss: 0.2604984641075134\n",
      "Epoch 1369, Loss: 0.6276107430458069, Final Batch Loss: 0.30193600058555603\n",
      "Epoch 1370, Loss: 0.5076712667942047, Final Batch Loss: 0.27680957317352295\n",
      "Epoch 1371, Loss: 0.5487436801195145, Final Batch Loss: 0.24561937153339386\n",
      "Epoch 1372, Loss: 0.4660525918006897, Final Batch Loss: 0.23764193058013916\n",
      "Epoch 1373, Loss: 0.5521013736724854, Final Batch Loss: 0.27606192231178284\n",
      "Epoch 1374, Loss: 0.5496871471405029, Final Batch Loss: 0.26507002115249634\n",
      "Epoch 1375, Loss: 0.6604925990104675, Final Batch Loss: 0.37932413816452026\n",
      "Epoch 1376, Loss: 0.48545581102371216, Final Batch Loss: 0.22089093923568726\n",
      "Epoch 1377, Loss: 0.6104207634925842, Final Batch Loss: 0.3184698224067688\n",
      "Epoch 1378, Loss: 0.48478303849697113, Final Batch Loss: 0.24525617063045502\n",
      "Epoch 1379, Loss: 0.541610985994339, Final Batch Loss: 0.26488131284713745\n",
      "Epoch 1380, Loss: 0.6105184853076935, Final Batch Loss: 0.35360559821128845\n",
      "Epoch 1381, Loss: 0.5562919974327087, Final Batch Loss: 0.2767241299152374\n",
      "Epoch 1382, Loss: 0.6164159178733826, Final Batch Loss: 0.2819560766220093\n",
      "Epoch 1383, Loss: 0.5025446861982346, Final Batch Loss: 0.2573999762535095\n",
      "Epoch 1384, Loss: 0.5540619045495987, Final Batch Loss: 0.3128393590450287\n",
      "Epoch 1385, Loss: 0.5171987861394882, Final Batch Loss: 0.280661404132843\n",
      "Epoch 1386, Loss: 0.5820078253746033, Final Batch Loss: 0.2800293266773224\n",
      "Epoch 1387, Loss: 0.5336074233055115, Final Batch Loss: 0.2260473072528839\n",
      "Epoch 1388, Loss: 0.6481903493404388, Final Batch Loss: 0.3806832730770111\n",
      "Epoch 1389, Loss: 0.5577186644077301, Final Batch Loss: 0.2940499484539032\n",
      "Epoch 1390, Loss: 0.6013221442699432, Final Batch Loss: 0.3035608232021332\n",
      "Epoch 1391, Loss: 0.5151990801095963, Final Batch Loss: 0.24217544496059418\n",
      "Epoch 1392, Loss: 0.629963219165802, Final Batch Loss: 0.31507277488708496\n",
      "Epoch 1393, Loss: 0.5658139586448669, Final Batch Loss: 0.2877372205257416\n",
      "Epoch 1394, Loss: 0.5296694338321686, Final Batch Loss: 0.24235033988952637\n",
      "Epoch 1395, Loss: 0.5613798201084137, Final Batch Loss: 0.2723384499549866\n",
      "Epoch 1396, Loss: 0.44962091743946075, Final Batch Loss: 0.2090246081352234\n",
      "Epoch 1397, Loss: 0.48282912373542786, Final Batch Loss: 0.22007736563682556\n",
      "Epoch 1398, Loss: 0.5308608114719391, Final Batch Loss: 0.23033571243286133\n",
      "Epoch 1399, Loss: 0.5196536630392075, Final Batch Loss: 0.2737688720226288\n",
      "Epoch 1400, Loss: 0.4992011785507202, Final Batch Loss: 0.23005086183547974\n",
      "Epoch 1401, Loss: 0.5969809591770172, Final Batch Loss: 0.3469833731651306\n",
      "Epoch 1402, Loss: 0.6355101466178894, Final Batch Loss: 0.3355768918991089\n",
      "Epoch 1403, Loss: 0.48128415644168854, Final Batch Loss: 0.2278997153043747\n",
      "Epoch 1404, Loss: 0.6575694978237152, Final Batch Loss: 0.36460164189338684\n",
      "Epoch 1405, Loss: 0.5318634510040283, Final Batch Loss: 0.26739445328712463\n",
      "Epoch 1406, Loss: 0.5784130692481995, Final Batch Loss: 0.2759609818458557\n",
      "Epoch 1407, Loss: 0.4486364871263504, Final Batch Loss: 0.19087658822536469\n",
      "Epoch 1408, Loss: 0.4880872666835785, Final Batch Loss: 0.20405343174934387\n",
      "Epoch 1409, Loss: 0.6356340050697327, Final Batch Loss: 0.3700816333293915\n",
      "Epoch 1410, Loss: 0.5751292705535889, Final Batch Loss: 0.2620360255241394\n",
      "Epoch 1411, Loss: 0.558923065662384, Final Batch Loss: 0.34350672364234924\n",
      "Epoch 1412, Loss: 0.4830477833747864, Final Batch Loss: 0.2257535457611084\n",
      "Epoch 1413, Loss: 0.5001094490289688, Final Batch Loss: 0.25179487466812134\n",
      "Epoch 1414, Loss: 0.5783438682556152, Final Batch Loss: 0.344868004322052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1415, Loss: 0.5220566838979721, Final Batch Loss: 0.20520974695682526\n",
      "Epoch 1416, Loss: 0.5002400428056717, Final Batch Loss: 0.2180403620004654\n",
      "Epoch 1417, Loss: 0.5838034451007843, Final Batch Loss: 0.3220321834087372\n",
      "Epoch 1418, Loss: 0.6800727546215057, Final Batch Loss: 0.4149795472621918\n",
      "Epoch 1419, Loss: 0.5862076580524445, Final Batch Loss: 0.3294590413570404\n",
      "Epoch 1420, Loss: 0.559604212641716, Final Batch Loss: 0.31040409207344055\n",
      "Epoch 1421, Loss: 0.5237331986427307, Final Batch Loss: 0.2862257659435272\n",
      "Epoch 1422, Loss: 0.5096083581447601, Final Batch Loss: 0.26656702160835266\n",
      "Epoch 1423, Loss: 0.5412114262580872, Final Batch Loss: 0.2302379012107849\n",
      "Epoch 1424, Loss: 0.550172358751297, Final Batch Loss: 0.2527131140232086\n",
      "Epoch 1425, Loss: 0.5775001645088196, Final Batch Loss: 0.3227069675922394\n",
      "Epoch 1426, Loss: 0.5564504563808441, Final Batch Loss: 0.22001072764396667\n",
      "Epoch 1427, Loss: 0.6833885908126831, Final Batch Loss: 0.37724569439888\n",
      "Epoch 1428, Loss: 0.4521515965461731, Final Batch Loss: 0.18590760231018066\n",
      "Epoch 1429, Loss: 0.6095162332057953, Final Batch Loss: 0.30817797780036926\n",
      "Epoch 1430, Loss: 0.528192937374115, Final Batch Loss: 0.2803146541118622\n",
      "Epoch 1431, Loss: 0.45561182498931885, Final Batch Loss: 0.19092407822608948\n",
      "Epoch 1432, Loss: 0.5127101093530655, Final Batch Loss: 0.23560120165348053\n",
      "Epoch 1433, Loss: 0.44497863948345184, Final Batch Loss: 0.14860863983631134\n",
      "Epoch 1434, Loss: 0.4858516603708267, Final Batch Loss: 0.218152716755867\n",
      "Epoch 1435, Loss: 0.5929039716720581, Final Batch Loss: 0.33798861503601074\n",
      "Epoch 1436, Loss: 0.4820174127817154, Final Batch Loss: 0.2427576184272766\n",
      "Epoch 1437, Loss: 0.46159040927886963, Final Batch Loss: 0.19008493423461914\n",
      "Epoch 1438, Loss: 0.5688013136386871, Final Batch Loss: 0.29837536811828613\n",
      "Epoch 1439, Loss: 0.5428995192050934, Final Batch Loss: 0.2392926812171936\n",
      "Epoch 1440, Loss: 0.4631352424621582, Final Batch Loss: 0.23873311281204224\n",
      "Epoch 1441, Loss: 0.4766097664833069, Final Batch Loss: 0.1827162206172943\n",
      "Epoch 1442, Loss: 0.5062054246664047, Final Batch Loss: 0.24822424352169037\n",
      "Epoch 1443, Loss: 0.48082366585731506, Final Batch Loss: 0.18220064043998718\n",
      "Epoch 1444, Loss: 0.5412982106208801, Final Batch Loss: 0.28500133752822876\n",
      "Epoch 1445, Loss: 0.5298793911933899, Final Batch Loss: 0.24682733416557312\n",
      "Epoch 1446, Loss: 0.6335109174251556, Final Batch Loss: 0.3367927670478821\n",
      "Epoch 1447, Loss: 0.5523821413516998, Final Batch Loss: 0.19935888051986694\n",
      "Epoch 1448, Loss: 0.4914379268884659, Final Batch Loss: 0.2535247504711151\n",
      "Epoch 1449, Loss: 0.6895393133163452, Final Batch Loss: 0.41831815242767334\n",
      "Epoch 1450, Loss: 0.5518850386142731, Final Batch Loss: 0.2957661747932434\n",
      "Epoch 1451, Loss: 0.5302381217479706, Final Batch Loss: 0.2399711012840271\n",
      "Epoch 1452, Loss: 0.5277915000915527, Final Batch Loss: 0.2095465362071991\n",
      "Epoch 1453, Loss: 0.5137879848480225, Final Batch Loss: 0.25300249457359314\n",
      "Epoch 1454, Loss: 0.5330458730459213, Final Batch Loss: 0.298824280500412\n",
      "Epoch 1455, Loss: 0.4818195551633835, Final Batch Loss: 0.18725670874118805\n",
      "Epoch 1456, Loss: 0.5542531609535217, Final Batch Loss: 0.28400760889053345\n",
      "Epoch 1457, Loss: 0.6179714500904083, Final Batch Loss: 0.30460479855537415\n",
      "Epoch 1458, Loss: 0.441559836268425, Final Batch Loss: 0.21953184902668\n",
      "Epoch 1459, Loss: 0.57752925157547, Final Batch Loss: 0.29797959327697754\n",
      "Epoch 1460, Loss: 0.3784196823835373, Final Batch Loss: 0.13054029643535614\n",
      "Epoch 1461, Loss: 0.45834311842918396, Final Batch Loss: 0.20272982120513916\n",
      "Epoch 1462, Loss: 0.5353990793228149, Final Batch Loss: 0.2812741994857788\n",
      "Epoch 1463, Loss: 0.5558116137981415, Final Batch Loss: 0.30505093932151794\n",
      "Epoch 1464, Loss: 0.441775381565094, Final Batch Loss: 0.18196004629135132\n",
      "Epoch 1465, Loss: 0.5214585214853287, Final Batch Loss: 0.2818649411201477\n",
      "Epoch 1466, Loss: 0.4813186377286911, Final Batch Loss: 0.21196337044239044\n",
      "Epoch 1467, Loss: 0.5155479162931442, Final Batch Loss: 0.2203451246023178\n",
      "Epoch 1468, Loss: 0.5079318284988403, Final Batch Loss: 0.21711957454681396\n",
      "Epoch 1469, Loss: 0.6034574210643768, Final Batch Loss: 0.3017112910747528\n",
      "Epoch 1470, Loss: 0.50080905854702, Final Batch Loss: 0.21353764832019806\n",
      "Epoch 1471, Loss: 0.6228488087654114, Final Batch Loss: 0.27975741028785706\n",
      "Epoch 1472, Loss: 0.469510942697525, Final Batch Loss: 0.20389854907989502\n",
      "Epoch 1473, Loss: 0.5186299979686737, Final Batch Loss: 0.2783415615558624\n",
      "Epoch 1474, Loss: 0.5331844985485077, Final Batch Loss: 0.261158287525177\n",
      "Epoch 1475, Loss: 0.455581471323967, Final Batch Loss: 0.21571367979049683\n",
      "Epoch 1476, Loss: 0.4567880630493164, Final Batch Loss: 0.2018871307373047\n",
      "Epoch 1477, Loss: 0.5748282074928284, Final Batch Loss: 0.27268916368484497\n",
      "Epoch 1478, Loss: 0.45928940176963806, Final Batch Loss: 0.23686358332633972\n",
      "Epoch 1479, Loss: 0.46724115312099457, Final Batch Loss: 0.23663289844989777\n",
      "Epoch 1480, Loss: 0.44734442234039307, Final Batch Loss: 0.2074543535709381\n",
      "Epoch 1481, Loss: 0.5326583534479141, Final Batch Loss: 0.24745522439479828\n",
      "Epoch 1482, Loss: 0.49895988404750824, Final Batch Loss: 0.2740992307662964\n",
      "Epoch 1483, Loss: 0.5158732831478119, Final Batch Loss: 0.23813962936401367\n",
      "Epoch 1484, Loss: 0.5713295936584473, Final Batch Loss: 0.3412289023399353\n",
      "Epoch 1485, Loss: 0.5960222482681274, Final Batch Loss: 0.2981726825237274\n",
      "Epoch 1486, Loss: 0.5774584412574768, Final Batch Loss: 0.2686114013195038\n",
      "Epoch 1487, Loss: 0.5796014964580536, Final Batch Loss: 0.298382043838501\n",
      "Epoch 1488, Loss: 0.6047507524490356, Final Batch Loss: 0.3353153467178345\n",
      "Epoch 1489, Loss: 0.54606893658638, Final Batch Loss: 0.25026771426200867\n",
      "Epoch 1490, Loss: 0.5082612037658691, Final Batch Loss: 0.2633351981639862\n",
      "Epoch 1491, Loss: 0.5796331912279129, Final Batch Loss: 0.36014074087142944\n",
      "Epoch 1492, Loss: 0.4993193447589874, Final Batch Loss: 0.24306249618530273\n",
      "Epoch 1493, Loss: 0.5401989817619324, Final Batch Loss: 0.26095902919769287\n",
      "Epoch 1494, Loss: 0.4981290400028229, Final Batch Loss: 0.26206156611442566\n",
      "Epoch 1495, Loss: 0.5704870820045471, Final Batch Loss: 0.29145878553390503\n",
      "Epoch 1496, Loss: 0.5575253963470459, Final Batch Loss: 0.30252188444137573\n",
      "Epoch 1497, Loss: 0.6200321912765503, Final Batch Loss: 0.3417046070098877\n",
      "Epoch 1498, Loss: 0.6016905903816223, Final Batch Loss: 0.29633525013923645\n",
      "Epoch 1499, Loss: 0.48140598833560944, Final Batch Loss: 0.23239222168922424\n",
      "Epoch 1500, Loss: 0.5357513427734375, Final Batch Loss: 0.20326527953147888\n",
      "Epoch 1501, Loss: 0.536795973777771, Final Batch Loss: 0.31579679250717163\n",
      "Epoch 1502, Loss: 0.46575213968753815, Final Batch Loss: 0.21371544897556305\n",
      "Epoch 1503, Loss: 0.597589448094368, Final Batch Loss: 0.35434404015541077\n",
      "Epoch 1504, Loss: 0.5123120248317719, Final Batch Loss: 0.2233526110649109\n",
      "Epoch 1505, Loss: 0.5766852498054504, Final Batch Loss: 0.27780207991600037\n",
      "Epoch 1506, Loss: 0.503622516989708, Final Batch Loss: 0.2704625129699707\n",
      "Epoch 1507, Loss: 0.5644072145223618, Final Batch Loss: 0.32825183868408203\n",
      "Epoch 1508, Loss: 0.5435307025909424, Final Batch Loss: 0.26544877886772156\n",
      "Epoch 1509, Loss: 0.5686260759830475, Final Batch Loss: 0.307323157787323\n",
      "Epoch 1510, Loss: 0.5482455641031265, Final Batch Loss: 0.30792340636253357\n",
      "Epoch 1511, Loss: 0.5177485644817352, Final Batch Loss: 0.24203580617904663\n",
      "Epoch 1512, Loss: 0.5568044185638428, Final Batch Loss: 0.24627715349197388\n",
      "Epoch 1513, Loss: 0.5340117812156677, Final Batch Loss: 0.2729945778846741\n",
      "Epoch 1514, Loss: 0.5292773842811584, Final Batch Loss: 0.21275588870048523\n",
      "Epoch 1515, Loss: 0.50121209025383, Final Batch Loss: 0.24088913202285767\n",
      "Epoch 1516, Loss: 0.5358670800924301, Final Batch Loss: 0.24583841860294342\n",
      "Epoch 1517, Loss: 0.484791100025177, Final Batch Loss: 0.2730141282081604\n",
      "Epoch 1518, Loss: 0.5282922089099884, Final Batch Loss: 0.3009915053844452\n",
      "Epoch 1519, Loss: 0.5922128707170486, Final Batch Loss: 0.34230607748031616\n",
      "Epoch 1520, Loss: 0.45252907276153564, Final Batch Loss: 0.15497919917106628\n",
      "Epoch 1521, Loss: 0.5622585713863373, Final Batch Loss: 0.2938028872013092\n",
      "Epoch 1522, Loss: 0.5238173604011536, Final Batch Loss: 0.28947123885154724\n",
      "Epoch 1523, Loss: 0.48873765766620636, Final Batch Loss: 0.2759835422039032\n",
      "Epoch 1524, Loss: 0.6398223787546158, Final Batch Loss: 0.3934132158756256\n",
      "Epoch 1525, Loss: 0.5176509916782379, Final Batch Loss: 0.25726041197776794\n",
      "Epoch 1526, Loss: 0.4903879761695862, Final Batch Loss: 0.2586658000946045\n",
      "Epoch 1527, Loss: 0.5973803699016571, Final Batch Loss: 0.2769336402416229\n",
      "Epoch 1528, Loss: 0.5404694080352783, Final Batch Loss: 0.28764596581459045\n",
      "Epoch 1529, Loss: 0.5480504035949707, Final Batch Loss: 0.2753864824771881\n",
      "Epoch 1530, Loss: 0.5670182257890701, Final Batch Loss: 0.17282654345035553\n",
      "Epoch 1531, Loss: 0.5192991644144058, Final Batch Loss: 0.27927759289741516\n",
      "Epoch 1532, Loss: 0.49014976620674133, Final Batch Loss: 0.26051065325737\n",
      "Epoch 1533, Loss: 0.567694365978241, Final Batch Loss: 0.29317590594291687\n",
      "Epoch 1534, Loss: 0.4553617388010025, Final Batch Loss: 0.22209343314170837\n",
      "Epoch 1535, Loss: 0.5259127020835876, Final Batch Loss: 0.29440075159072876\n",
      "Epoch 1536, Loss: 0.45844805240631104, Final Batch Loss: 0.24676866829395294\n",
      "Epoch 1537, Loss: 0.463422954082489, Final Batch Loss: 0.2151893675327301\n",
      "Epoch 1538, Loss: 0.42779769003391266, Final Batch Loss: 0.22009530663490295\n",
      "Epoch 1539, Loss: 0.5275128334760666, Final Batch Loss: 0.2497207671403885\n",
      "Epoch 1540, Loss: 0.5198045670986176, Final Batch Loss: 0.25218716263771057\n",
      "Epoch 1541, Loss: 0.4789588004350662, Final Batch Loss: 0.24696096777915955\n",
      "Epoch 1542, Loss: 0.4825808256864548, Final Batch Loss: 0.25041067600250244\n",
      "Epoch 1543, Loss: 0.5709408074617386, Final Batch Loss: 0.3219933807849884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1544, Loss: 0.6068433821201324, Final Batch Loss: 0.2907502353191376\n",
      "Epoch 1545, Loss: 0.6745728403329849, Final Batch Loss: 0.4378194212913513\n",
      "Epoch 1546, Loss: 0.430666521191597, Final Batch Loss: 0.15600235760211945\n",
      "Epoch 1547, Loss: 0.5087724477052689, Final Batch Loss: 0.24020721018314362\n",
      "Epoch 1548, Loss: 0.5037245154380798, Final Batch Loss: 0.2523553967475891\n",
      "Epoch 1549, Loss: 0.484060600399971, Final Batch Loss: 0.20500780642032623\n",
      "Epoch 1550, Loss: 0.6452876925468445, Final Batch Loss: 0.42567163705825806\n",
      "Epoch 1551, Loss: 0.5548368245363235, Final Batch Loss: 0.38177689909935\n",
      "Epoch 1552, Loss: 0.43765483796596527, Final Batch Loss: 0.17227785289287567\n",
      "Epoch 1553, Loss: 0.4614603519439697, Final Batch Loss: 0.21861737966537476\n",
      "Epoch 1554, Loss: 0.4753740131855011, Final Batch Loss: 0.2165292203426361\n",
      "Epoch 1555, Loss: 0.5189711600542068, Final Batch Loss: 0.2473200112581253\n",
      "Epoch 1556, Loss: 0.6600600183010101, Final Batch Loss: 0.2997993528842926\n",
      "Epoch 1557, Loss: 0.47477543354034424, Final Batch Loss: 0.2210320234298706\n",
      "Epoch 1558, Loss: 0.5211417376995087, Final Batch Loss: 0.23425132036209106\n",
      "Epoch 1559, Loss: 0.4302867650985718, Final Batch Loss: 0.2071523368358612\n",
      "Epoch 1560, Loss: 0.5760933756828308, Final Batch Loss: 0.30259016156196594\n",
      "Epoch 1561, Loss: 0.6095739156007767, Final Batch Loss: 0.36221566796302795\n",
      "Epoch 1562, Loss: 0.4292132705450058, Final Batch Loss: 0.16054357588291168\n",
      "Epoch 1563, Loss: 0.45983201265335083, Final Batch Loss: 0.21317695081233978\n",
      "Epoch 1564, Loss: 0.5100708305835724, Final Batch Loss: 0.282406747341156\n",
      "Epoch 1565, Loss: 0.5256927311420441, Final Batch Loss: 0.26040899753570557\n",
      "Epoch 1566, Loss: 0.485636830329895, Final Batch Loss: 0.2642804682254791\n",
      "Epoch 1567, Loss: 0.5868686586618423, Final Batch Loss: 0.34023571014404297\n",
      "Epoch 1568, Loss: 0.5401910841464996, Final Batch Loss: 0.2818542718887329\n",
      "Epoch 1569, Loss: 0.48482759296894073, Final Batch Loss: 0.16854603588581085\n",
      "Epoch 1570, Loss: 0.5069608688354492, Final Batch Loss: 0.2873450815677643\n",
      "Epoch 1571, Loss: 0.4420604556798935, Final Batch Loss: 0.16414739191532135\n",
      "Epoch 1572, Loss: 0.44287315011024475, Final Batch Loss: 0.22801245748996735\n",
      "Epoch 1573, Loss: 0.5484675467014313, Final Batch Loss: 0.3083263635635376\n",
      "Epoch 1574, Loss: 0.6067396998405457, Final Batch Loss: 0.37611204385757446\n",
      "Epoch 1575, Loss: 0.5086462497711182, Final Batch Loss: 0.2585591971874237\n",
      "Epoch 1576, Loss: 0.5356332957744598, Final Batch Loss: 0.28486576676368713\n",
      "Epoch 1577, Loss: 0.4102223515510559, Final Batch Loss: 0.19720792770385742\n",
      "Epoch 1578, Loss: 0.5034022629261017, Final Batch Loss: 0.24067330360412598\n",
      "Epoch 1579, Loss: 0.4264554977416992, Final Batch Loss: 0.21848289668560028\n",
      "Epoch 1580, Loss: 0.5120713412761688, Final Batch Loss: 0.2588203549385071\n",
      "Epoch 1581, Loss: 0.5802640914916992, Final Batch Loss: 0.34702497720718384\n",
      "Epoch 1582, Loss: 0.43526266515254974, Final Batch Loss: 0.21033622324466705\n",
      "Epoch 1583, Loss: 0.4971102476119995, Final Batch Loss: 0.2649500072002411\n",
      "Epoch 1584, Loss: 0.5076321065425873, Final Batch Loss: 0.26322948932647705\n",
      "Epoch 1585, Loss: 0.44585733115673065, Final Batch Loss: 0.2221948504447937\n",
      "Epoch 1586, Loss: 0.4362610876560211, Final Batch Loss: 0.22577854990959167\n",
      "Epoch 1587, Loss: 0.5293875187635422, Final Batch Loss: 0.234956756234169\n",
      "Epoch 1588, Loss: 0.5254565328359604, Final Batch Loss: 0.29309794306755066\n",
      "Epoch 1589, Loss: 0.5395749807357788, Final Batch Loss: 0.27851423621177673\n",
      "Epoch 1590, Loss: 0.5346840918064117, Final Batch Loss: 0.2837808132171631\n",
      "Epoch 1591, Loss: 0.5077647864818573, Final Batch Loss: 0.2527824938297272\n",
      "Epoch 1592, Loss: 0.5052978545427322, Final Batch Loss: 0.2603054642677307\n",
      "Epoch 1593, Loss: 0.4849056750535965, Final Batch Loss: 0.26705074310302734\n",
      "Epoch 1594, Loss: 0.5659069716930389, Final Batch Loss: 0.26725834608078003\n",
      "Epoch 1595, Loss: 0.4664906710386276, Final Batch Loss: 0.20575015246868134\n",
      "Epoch 1596, Loss: 0.4840846359729767, Final Batch Loss: 0.2465876042842865\n",
      "Epoch 1597, Loss: 0.47751203179359436, Final Batch Loss: 0.23114129900932312\n",
      "Epoch 1598, Loss: 0.5635645240545273, Final Batch Loss: 0.3396259844303131\n",
      "Epoch 1599, Loss: 0.41796259582042694, Final Batch Loss: 0.19940099120140076\n",
      "Epoch 1600, Loss: 0.4460997134447098, Final Batch Loss: 0.20695249736309052\n",
      "Epoch 1601, Loss: 0.4170874357223511, Final Batch Loss: 0.20361436903476715\n",
      "Epoch 1602, Loss: 0.5410668551921844, Final Batch Loss: 0.29098308086395264\n",
      "Epoch 1603, Loss: 0.5319732576608658, Final Batch Loss: 0.21435274183750153\n",
      "Epoch 1604, Loss: 0.49118663370609283, Final Batch Loss: 0.25282350182533264\n",
      "Epoch 1605, Loss: 0.514883503317833, Final Batch Loss: 0.2651323974132538\n",
      "Epoch 1606, Loss: 0.5319255739450455, Final Batch Loss: 0.29236292839050293\n",
      "Epoch 1607, Loss: 0.4728059619665146, Final Batch Loss: 0.25870031118392944\n",
      "Epoch 1608, Loss: 0.6685720235109329, Final Batch Loss: 0.44394591450691223\n",
      "Epoch 1609, Loss: 0.5109895914793015, Final Batch Loss: 0.28594663739204407\n",
      "Epoch 1610, Loss: 0.4555921405553818, Final Batch Loss: 0.21208688616752625\n",
      "Epoch 1611, Loss: 0.4591262936592102, Final Batch Loss: 0.20355316996574402\n",
      "Epoch 1612, Loss: 0.48752361536026, Final Batch Loss: 0.2928245961666107\n",
      "Epoch 1613, Loss: 0.4937271475791931, Final Batch Loss: 0.22899124026298523\n",
      "Epoch 1614, Loss: 0.49460844695568085, Final Batch Loss: 0.23497237265110016\n",
      "Epoch 1615, Loss: 0.5442720055580139, Final Batch Loss: 0.28123021125793457\n",
      "Epoch 1616, Loss: 0.511221706867218, Final Batch Loss: 0.2972698211669922\n",
      "Epoch 1617, Loss: 0.4956147372722626, Final Batch Loss: 0.2664344608783722\n",
      "Epoch 1618, Loss: 0.4360116273164749, Final Batch Loss: 0.1798439770936966\n",
      "Epoch 1619, Loss: 0.540994793176651, Final Batch Loss: 0.2807239890098572\n",
      "Epoch 1620, Loss: 0.4622438997030258, Final Batch Loss: 0.218056783080101\n",
      "Epoch 1621, Loss: 0.5075485557317734, Final Batch Loss: 0.23016776144504547\n",
      "Epoch 1622, Loss: 0.48655906319618225, Final Batch Loss: 0.16430321335792542\n",
      "Epoch 1623, Loss: 0.6071456968784332, Final Batch Loss: 0.31439587473869324\n",
      "Epoch 1624, Loss: 0.5140535682439804, Final Batch Loss: 0.3037430942058563\n",
      "Epoch 1625, Loss: 0.5222672671079636, Final Batch Loss: 0.23959313333034515\n",
      "Epoch 1626, Loss: 0.4769051969051361, Final Batch Loss: 0.20436719059944153\n",
      "Epoch 1627, Loss: 0.5104067027568817, Final Batch Loss: 0.2682284712791443\n",
      "Epoch 1628, Loss: 0.5176199972629547, Final Batch Loss: 0.25543108582496643\n",
      "Epoch 1629, Loss: 0.5299587696790695, Final Batch Loss: 0.2978609800338745\n",
      "Epoch 1630, Loss: 0.5052942484617233, Final Batch Loss: 0.21466390788555145\n",
      "Epoch 1631, Loss: 0.5844962894916534, Final Batch Loss: 0.27510958909988403\n",
      "Epoch 1632, Loss: 0.5016431957483292, Final Batch Loss: 0.23002608120441437\n",
      "Epoch 1633, Loss: 0.5621961057186127, Final Batch Loss: 0.2889431416988373\n",
      "Epoch 1634, Loss: 0.5086225122213364, Final Batch Loss: 0.23478998243808746\n",
      "Epoch 1635, Loss: 0.49462494254112244, Final Batch Loss: 0.23616066575050354\n",
      "Epoch 1636, Loss: 0.4491710513830185, Final Batch Loss: 0.2621779441833496\n",
      "Epoch 1637, Loss: 0.5055860579013824, Final Batch Loss: 0.26189789175987244\n",
      "Epoch 1638, Loss: 0.5144588202238083, Final Batch Loss: 0.2695535123348236\n",
      "Epoch 1639, Loss: 0.4518146812915802, Final Batch Loss: 0.24324819445610046\n",
      "Epoch 1640, Loss: 0.530210331082344, Final Batch Loss: 0.30466917157173157\n",
      "Epoch 1641, Loss: 0.5566260069608688, Final Batch Loss: 0.3306628167629242\n",
      "Epoch 1642, Loss: 0.4202527403831482, Final Batch Loss: 0.20361623167991638\n",
      "Epoch 1643, Loss: 0.6092601418495178, Final Batch Loss: 0.2960774004459381\n",
      "Epoch 1644, Loss: 0.5096863061189651, Final Batch Loss: 0.2935923933982849\n",
      "Epoch 1645, Loss: 0.44794613122940063, Final Batch Loss: 0.19509783387184143\n",
      "Epoch 1646, Loss: 0.49974048137664795, Final Batch Loss: 0.2191779613494873\n",
      "Epoch 1647, Loss: 0.4221871346235275, Final Batch Loss: 0.19107742607593536\n",
      "Epoch 1648, Loss: 0.6112720519304276, Final Batch Loss: 0.38588500022888184\n",
      "Epoch 1649, Loss: 0.4846733808517456, Final Batch Loss: 0.2586852014064789\n",
      "Epoch 1650, Loss: 0.43156442046165466, Final Batch Loss: 0.18733617663383484\n",
      "Epoch 1651, Loss: 0.5736075788736343, Final Batch Loss: 0.34854817390441895\n",
      "Epoch 1652, Loss: 0.4249946027994156, Final Batch Loss: 0.16842259466648102\n",
      "Epoch 1653, Loss: 0.5352820008993149, Final Batch Loss: 0.3181647062301636\n",
      "Epoch 1654, Loss: 0.4884389489889145, Final Batch Loss: 0.22136466205120087\n",
      "Epoch 1655, Loss: 0.49883466958999634, Final Batch Loss: 0.24018648266792297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1656, Loss: 0.4497091472148895, Final Batch Loss: 0.20764097571372986\n",
      "Epoch 1657, Loss: 0.4421032667160034, Final Batch Loss: 0.21052253246307373\n",
      "Epoch 1658, Loss: 0.425717294216156, Final Batch Loss: 0.2178260087966919\n",
      "Epoch 1659, Loss: 0.4540203809738159, Final Batch Loss: 0.18604040145874023\n",
      "Epoch 1660, Loss: 0.47208990156650543, Final Batch Loss: 0.17773748934268951\n",
      "Epoch 1661, Loss: 0.4914039224386215, Final Batch Loss: 0.2551574110984802\n",
      "Epoch 1662, Loss: 0.49813514947891235, Final Batch Loss: 0.28933706879615784\n",
      "Epoch 1663, Loss: 0.5058461129665375, Final Batch Loss: 0.2655923068523407\n",
      "Epoch 1664, Loss: 0.5678170323371887, Final Batch Loss: 0.2933712899684906\n",
      "Epoch 1665, Loss: 0.47994765639305115, Final Batch Loss: 0.2338140308856964\n",
      "Epoch 1666, Loss: 0.4885333180427551, Final Batch Loss: 0.23562002182006836\n",
      "Epoch 1667, Loss: 0.4841350167989731, Final Batch Loss: 0.22005952894687653\n",
      "Epoch 1668, Loss: 0.4125718027353287, Final Batch Loss: 0.20771615207195282\n",
      "Epoch 1669, Loss: 0.6578011810779572, Final Batch Loss: 0.3927631080150604\n",
      "Epoch 1670, Loss: 0.6190922558307648, Final Batch Loss: 0.3623863160610199\n",
      "Epoch 1671, Loss: 0.5296692550182343, Final Batch Loss: 0.27057963609695435\n",
      "Epoch 1672, Loss: 0.4800914078950882, Final Batch Loss: 0.2644945979118347\n",
      "Epoch 1673, Loss: 0.49486710131168365, Final Batch Loss: 0.20308361947536469\n",
      "Epoch 1674, Loss: 0.48790693283081055, Final Batch Loss: 0.20913460850715637\n",
      "Epoch 1675, Loss: 0.4318715035915375, Final Batch Loss: 0.1831856369972229\n",
      "Epoch 1676, Loss: 0.43608514964580536, Final Batch Loss: 0.19311310350894928\n",
      "Epoch 1677, Loss: 0.5159299224615097, Final Batch Loss: 0.24388064444065094\n",
      "Epoch 1678, Loss: 0.4826558232307434, Final Batch Loss: 0.25271734595298767\n",
      "Epoch 1679, Loss: 0.47016167640686035, Final Batch Loss: 0.216766357421875\n",
      "Epoch 1680, Loss: 0.4621775895357132, Final Batch Loss: 0.21683001518249512\n",
      "Epoch 1681, Loss: 0.550239771604538, Final Batch Loss: 0.30366870760917664\n",
      "Epoch 1682, Loss: 0.5093770772218704, Final Batch Loss: 0.24620671570301056\n",
      "Epoch 1683, Loss: 0.4661625921726227, Final Batch Loss: 0.1725277602672577\n",
      "Epoch 1684, Loss: 0.4728926718235016, Final Batch Loss: 0.2783817648887634\n",
      "Epoch 1685, Loss: 0.5251953899860382, Final Batch Loss: 0.2436102330684662\n",
      "Epoch 1686, Loss: 0.4902392327785492, Final Batch Loss: 0.18399393558502197\n",
      "Epoch 1687, Loss: 0.5264542251825333, Final Batch Loss: 0.24198465049266815\n",
      "Epoch 1688, Loss: 0.4361463189125061, Final Batch Loss: 0.22382712364196777\n",
      "Epoch 1689, Loss: 0.5472094118595123, Final Batch Loss: 0.2854765057563782\n",
      "Epoch 1690, Loss: 0.4438628703355789, Final Batch Loss: 0.21487672626972198\n",
      "Epoch 1691, Loss: 0.48856499791145325, Final Batch Loss: 0.23177409172058105\n",
      "Epoch 1692, Loss: 0.45380447804927826, Final Batch Loss: 0.2366163432598114\n",
      "Epoch 1693, Loss: 0.4065718948841095, Final Batch Loss: 0.16383443772792816\n",
      "Epoch 1694, Loss: 0.5326020270586014, Final Batch Loss: 0.28438541293144226\n",
      "Epoch 1695, Loss: 0.5283084213733673, Final Batch Loss: 0.24054408073425293\n",
      "Epoch 1696, Loss: 0.46997615694999695, Final Batch Loss: 0.2560758888721466\n",
      "Epoch 1697, Loss: 0.5022312700748444, Final Batch Loss: 0.2773285508155823\n",
      "Epoch 1698, Loss: 0.5553842782974243, Final Batch Loss: 0.26029402017593384\n",
      "Epoch 1699, Loss: 0.5107623487710953, Final Batch Loss: 0.27217844128608704\n",
      "Epoch 1700, Loss: 0.504078134894371, Final Batch Loss: 0.2410072237253189\n",
      "Epoch 1701, Loss: 0.45359818637371063, Final Batch Loss: 0.21630530059337616\n",
      "Epoch 1702, Loss: 0.45674657821655273, Final Batch Loss: 0.22747749090194702\n",
      "Epoch 1703, Loss: 0.44606080651283264, Final Batch Loss: 0.2286897450685501\n",
      "Epoch 1704, Loss: 0.41206590831279755, Final Batch Loss: 0.2519286870956421\n",
      "Epoch 1705, Loss: 0.4025040715932846, Final Batch Loss: 0.1994345635175705\n",
      "Epoch 1706, Loss: 0.5033454298973083, Final Batch Loss: 0.23893949389457703\n",
      "Epoch 1707, Loss: 0.6149150729179382, Final Batch Loss: 0.38561031222343445\n",
      "Epoch 1708, Loss: 0.41809214651584625, Final Batch Loss: 0.16440851986408234\n",
      "Epoch 1709, Loss: 0.49535825848579407, Final Batch Loss: 0.2288060486316681\n",
      "Epoch 1710, Loss: 0.49505311250686646, Final Batch Loss: 0.28777575492858887\n",
      "Epoch 1711, Loss: 0.4082731753587723, Final Batch Loss: 0.17773109674453735\n",
      "Epoch 1712, Loss: 0.451905757188797, Final Batch Loss: 0.2254958599805832\n",
      "Epoch 1713, Loss: 0.42451462149620056, Final Batch Loss: 0.1960376799106598\n",
      "Epoch 1714, Loss: 0.48054254055023193, Final Batch Loss: 0.24539582431316376\n",
      "Epoch 1715, Loss: 0.5142313838005066, Final Batch Loss: 0.25469058752059937\n",
      "Epoch 1716, Loss: 0.544408917427063, Final Batch Loss: 0.27571263909339905\n",
      "Epoch 1717, Loss: 0.4196903705596924, Final Batch Loss: 0.21441727876663208\n",
      "Epoch 1718, Loss: 0.4610058218240738, Final Batch Loss: 0.1849878579378128\n",
      "Epoch 1719, Loss: 0.5509715229272842, Final Batch Loss: 0.3047020435333252\n",
      "Epoch 1720, Loss: 0.5967389047145844, Final Batch Loss: 0.3382430076599121\n",
      "Epoch 1721, Loss: 0.5211372673511505, Final Batch Loss: 0.2490110993385315\n",
      "Epoch 1722, Loss: 0.45314086973667145, Final Batch Loss: 0.1911562830209732\n",
      "Epoch 1723, Loss: 0.5111753046512604, Final Batch Loss: 0.29107022285461426\n",
      "Epoch 1724, Loss: 0.44282859563827515, Final Batch Loss: 0.23579388856887817\n",
      "Epoch 1725, Loss: 0.4380663335323334, Final Batch Loss: 0.19548341631889343\n",
      "Epoch 1726, Loss: 0.4458600729703903, Final Batch Loss: 0.20729216933250427\n",
      "Epoch 1727, Loss: 0.43048955500125885, Final Batch Loss: 0.18871386349201202\n",
      "Epoch 1728, Loss: 0.47394368052482605, Final Batch Loss: 0.21853086352348328\n",
      "Epoch 1729, Loss: 0.4407884329557419, Final Batch Loss: 0.17101599276065826\n",
      "Epoch 1730, Loss: 0.4193541705608368, Final Batch Loss: 0.1667439043521881\n",
      "Epoch 1731, Loss: 0.47588007152080536, Final Batch Loss: 0.23411399126052856\n",
      "Epoch 1732, Loss: 0.566157728433609, Final Batch Loss: 0.25647786259651184\n",
      "Epoch 1733, Loss: 0.49247710406780243, Final Batch Loss: 0.22744585573673248\n",
      "Epoch 1734, Loss: 0.47750355303287506, Final Batch Loss: 0.24140210449695587\n",
      "Epoch 1735, Loss: 0.4433120936155319, Final Batch Loss: 0.21912682056427002\n",
      "Epoch 1736, Loss: 0.5575730502605438, Final Batch Loss: 0.22701939940452576\n",
      "Epoch 1737, Loss: 0.4327816665172577, Final Batch Loss: 0.16804882884025574\n",
      "Epoch 1738, Loss: 0.5030582398176193, Final Batch Loss: 0.2836059629917145\n",
      "Epoch 1739, Loss: 0.5113272070884705, Final Batch Loss: 0.26500821113586426\n",
      "Epoch 1740, Loss: 0.41490840911865234, Final Batch Loss: 0.21457086503505707\n",
      "Epoch 1741, Loss: 0.4756743013858795, Final Batch Loss: 0.2623802125453949\n",
      "Epoch 1742, Loss: 0.5294839143753052, Final Batch Loss: 0.26248833537101746\n",
      "Epoch 1743, Loss: 0.4772645831108093, Final Batch Loss: 0.254756897687912\n",
      "Epoch 1744, Loss: 0.44489479064941406, Final Batch Loss: 0.23703637719154358\n",
      "Epoch 1745, Loss: 0.48982764780521393, Final Batch Loss: 0.23975329101085663\n",
      "Epoch 1746, Loss: 0.46120092272758484, Final Batch Loss: 0.22322265803813934\n",
      "Epoch 1747, Loss: 0.5318180322647095, Final Batch Loss: 0.2857181131839752\n",
      "Epoch 1748, Loss: 0.45917271077632904, Final Batch Loss: 0.2482714056968689\n",
      "Epoch 1749, Loss: 0.5789187401533127, Final Batch Loss: 0.3416372835636139\n",
      "Epoch 1750, Loss: 0.40360401570796967, Final Batch Loss: 0.20592989027500153\n",
      "Epoch 1751, Loss: 0.4321786165237427, Final Batch Loss: 0.21762715280056\n",
      "Epoch 1752, Loss: 0.5824252665042877, Final Batch Loss: 0.3552919328212738\n",
      "Epoch 1753, Loss: 0.48992978036403656, Final Batch Loss: 0.27944469451904297\n",
      "Epoch 1754, Loss: 0.48440177738666534, Final Batch Loss: 0.22155289351940155\n",
      "Epoch 1755, Loss: 0.4148353934288025, Final Batch Loss: 0.18649104237556458\n",
      "Epoch 1756, Loss: 0.42738719284534454, Final Batch Loss: 0.22003205120563507\n",
      "Epoch 1757, Loss: 0.48192039132118225, Final Batch Loss: 0.21386826038360596\n",
      "Epoch 1758, Loss: 0.4071188420057297, Final Batch Loss: 0.17812775075435638\n",
      "Epoch 1759, Loss: 0.5157704502344131, Final Batch Loss: 0.2683820128440857\n",
      "Epoch 1760, Loss: 0.4376613348722458, Final Batch Loss: 0.2021346241235733\n",
      "Epoch 1761, Loss: 0.49670350551605225, Final Batch Loss: 0.22906914353370667\n",
      "Epoch 1762, Loss: 0.3929917514324188, Final Batch Loss: 0.17344728112220764\n",
      "Epoch 1763, Loss: 0.4580467492341995, Final Batch Loss: 0.241872176527977\n",
      "Epoch 1764, Loss: 0.4449808746576309, Final Batch Loss: 0.2584631145000458\n",
      "Epoch 1765, Loss: 0.43956172466278076, Final Batch Loss: 0.19837263226509094\n",
      "Epoch 1766, Loss: 0.4435018301010132, Final Batch Loss: 0.2670544683933258\n",
      "Epoch 1767, Loss: 0.4717232584953308, Final Batch Loss: 0.2524683475494385\n",
      "Epoch 1768, Loss: 0.4544793963432312, Final Batch Loss: 0.26264050602912903\n",
      "Epoch 1769, Loss: 0.4123470038175583, Final Batch Loss: 0.20318590104579926\n",
      "Epoch 1770, Loss: 0.6089736372232437, Final Batch Loss: 0.36055177450180054\n",
      "Epoch 1771, Loss: 0.44991323351860046, Final Batch Loss: 0.20553496479988098\n",
      "Epoch 1772, Loss: 0.5210875272750854, Final Batch Loss: 0.2650292217731476\n",
      "Epoch 1773, Loss: 0.46116214990615845, Final Batch Loss: 0.2541552484035492\n",
      "Epoch 1774, Loss: 0.5878183245658875, Final Batch Loss: 0.35831379890441895\n",
      "Epoch 1775, Loss: 0.5774275660514832, Final Batch Loss: 0.35720065236091614\n",
      "Epoch 1776, Loss: 0.45101454854011536, Final Batch Loss: 0.2526543438434601\n",
      "Epoch 1777, Loss: 0.5043109208345413, Final Batch Loss: 0.288700670003891\n",
      "Epoch 1778, Loss: 0.49034249782562256, Final Batch Loss: 0.254830926656723\n",
      "Epoch 1779, Loss: 0.40790048241615295, Final Batch Loss: 0.19640840590000153\n",
      "Epoch 1780, Loss: 0.43542107939720154, Final Batch Loss: 0.18730255961418152\n",
      "Epoch 1781, Loss: 0.498180091381073, Final Batch Loss: 0.2977140247821808\n",
      "Epoch 1782, Loss: 0.5018555521965027, Final Batch Loss: 0.2532571852207184\n",
      "Epoch 1783, Loss: 0.4544355571269989, Final Batch Loss: 0.24217607080936432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1784, Loss: 0.4549279510974884, Final Batch Loss: 0.20864687860012054\n",
      "Epoch 1785, Loss: 0.49617427587509155, Final Batch Loss: 0.2576678991317749\n",
      "Epoch 1786, Loss: 0.5357855558395386, Final Batch Loss: 0.25227290391921997\n",
      "Epoch 1787, Loss: 0.5109785348176956, Final Batch Loss: 0.2275412231683731\n",
      "Epoch 1788, Loss: 0.49158716201782227, Final Batch Loss: 0.23027122020721436\n",
      "Epoch 1789, Loss: 0.5002808272838593, Final Batch Loss: 0.2446892261505127\n",
      "Epoch 1790, Loss: 0.5051875561475754, Final Batch Loss: 0.25948476791381836\n",
      "Epoch 1791, Loss: 0.42507463693618774, Final Batch Loss: 0.19820375740528107\n",
      "Epoch 1792, Loss: 0.46376919746398926, Final Batch Loss: 0.2562074661254883\n",
      "Epoch 1793, Loss: 0.5536226630210876, Final Batch Loss: 0.29257306456565857\n",
      "Epoch 1794, Loss: 0.5169377326965332, Final Batch Loss: 0.3021429181098938\n",
      "Epoch 1795, Loss: 0.4466845989227295, Final Batch Loss: 0.19850808382034302\n",
      "Epoch 1796, Loss: 0.4341873675584793, Final Batch Loss: 0.21537907421588898\n",
      "Epoch 1797, Loss: 0.5001445561647415, Final Batch Loss: 0.2063285857439041\n",
      "Epoch 1798, Loss: 0.4010588228702545, Final Batch Loss: 0.19481414556503296\n",
      "Epoch 1799, Loss: 0.5538997650146484, Final Batch Loss: 0.27804622054100037\n",
      "Epoch 1800, Loss: 0.49964439868927, Final Batch Loss: 0.2717888057231903\n",
      "Epoch 1801, Loss: 0.4357345253229141, Final Batch Loss: 0.20196977257728577\n",
      "Epoch 1802, Loss: 0.4426213353872299, Final Batch Loss: 0.2237078845500946\n",
      "Epoch 1803, Loss: 0.4710640609264374, Final Batch Loss: 0.26912498474121094\n",
      "Epoch 1804, Loss: 0.3600969910621643, Final Batch Loss: 0.16168291866779327\n",
      "Epoch 1805, Loss: 0.4594176411628723, Final Batch Loss: 0.19975942373275757\n",
      "Epoch 1806, Loss: 0.4813123345375061, Final Batch Loss: 0.25455373525619507\n",
      "Epoch 1807, Loss: 0.49193859100341797, Final Batch Loss: 0.2568455636501312\n",
      "Epoch 1808, Loss: 0.43362075090408325, Final Batch Loss: 0.20998764038085938\n",
      "Epoch 1809, Loss: 0.4084156006574631, Final Batch Loss: 0.15235699713230133\n",
      "Epoch 1810, Loss: 0.43638528883457184, Final Batch Loss: 0.20635411143302917\n",
      "Epoch 1811, Loss: 0.3771599978208542, Final Batch Loss: 0.18466664850711823\n",
      "Epoch 1812, Loss: 0.4901515245437622, Final Batch Loss: 0.24297311902046204\n",
      "Epoch 1813, Loss: 0.4740269184112549, Final Batch Loss: 0.23001356422901154\n",
      "Epoch 1814, Loss: 0.483100950717926, Final Batch Loss: 0.28440359234809875\n",
      "Epoch 1815, Loss: 0.42919017374515533, Final Batch Loss: 0.21028421819210052\n",
      "Epoch 1816, Loss: 0.377851665019989, Final Batch Loss: 0.17102576792240143\n",
      "Epoch 1817, Loss: 0.5102965831756592, Final Batch Loss: 0.21847090125083923\n",
      "Epoch 1818, Loss: 0.4033007025718689, Final Batch Loss: 0.18912500143051147\n",
      "Epoch 1819, Loss: 0.436510294675827, Final Batch Loss: 0.19710959494113922\n",
      "Epoch 1820, Loss: 0.5195727050304413, Final Batch Loss: 0.286588191986084\n",
      "Epoch 1821, Loss: 0.4723547399044037, Final Batch Loss: 0.19436857104301453\n",
      "Epoch 1822, Loss: 0.4234458804130554, Final Batch Loss: 0.1926504224538803\n",
      "Epoch 1823, Loss: 0.4302045851945877, Final Batch Loss: 0.18087217211723328\n",
      "Epoch 1824, Loss: 0.4223310053348541, Final Batch Loss: 0.20749570429325104\n",
      "Epoch 1825, Loss: 0.5299994349479675, Final Batch Loss: 0.27470430731773376\n",
      "Epoch 1826, Loss: 0.40841180086135864, Final Batch Loss: 0.1711961179971695\n",
      "Epoch 1827, Loss: 0.47884728014469147, Final Batch Loss: 0.2575109004974365\n",
      "Epoch 1828, Loss: 0.5237823277711868, Final Batch Loss: 0.27407580614089966\n",
      "Epoch 1829, Loss: 0.5193565785884857, Final Batch Loss: 0.24356791377067566\n",
      "Epoch 1830, Loss: 0.4326229393482208, Final Batch Loss: 0.19362010061740875\n",
      "Epoch 1831, Loss: 0.39484304189682007, Final Batch Loss: 0.13921579718589783\n",
      "Epoch 1832, Loss: 0.4807123988866806, Final Batch Loss: 0.2644991874694824\n",
      "Epoch 1833, Loss: 0.4232127070426941, Final Batch Loss: 0.19200704991817474\n",
      "Epoch 1834, Loss: 0.35501621663570404, Final Batch Loss: 0.14300473034381866\n",
      "Epoch 1835, Loss: 0.5225510001182556, Final Batch Loss: 0.2527449429035187\n",
      "Epoch 1836, Loss: 0.46991075575351715, Final Batch Loss: 0.23824700713157654\n",
      "Epoch 1837, Loss: 0.4697425961494446, Final Batch Loss: 0.19898635149002075\n",
      "Epoch 1838, Loss: 0.4533027410507202, Final Batch Loss: 0.1804170310497284\n",
      "Epoch 1839, Loss: 0.4529202878475189, Final Batch Loss: 0.22876177728176117\n",
      "Epoch 1840, Loss: 0.41027380526065826, Final Batch Loss: 0.1606743037700653\n",
      "Epoch 1841, Loss: 0.46308812499046326, Final Batch Loss: 0.22596171498298645\n",
      "Epoch 1842, Loss: 0.4410400390625, Final Batch Loss: 0.20378366112709045\n",
      "Epoch 1843, Loss: 0.43545928597450256, Final Batch Loss: 0.20704258978366852\n",
      "Epoch 1844, Loss: 0.5102737843990326, Final Batch Loss: 0.24061307311058044\n",
      "Epoch 1845, Loss: 0.4624016433954239, Final Batch Loss: 0.22814778983592987\n",
      "Epoch 1846, Loss: 0.5141470283269882, Final Batch Loss: 0.27811673283576965\n",
      "Epoch 1847, Loss: 0.42765019834041595, Final Batch Loss: 0.2227715104818344\n",
      "Epoch 1848, Loss: 0.46224574744701385, Final Batch Loss: 0.19928963482379913\n",
      "Epoch 1849, Loss: 0.5198869556188583, Final Batch Loss: 0.1771451085805893\n",
      "Epoch 1850, Loss: 0.5020599067211151, Final Batch Loss: 0.27532172203063965\n",
      "Epoch 1851, Loss: 0.42363953590393066, Final Batch Loss: 0.17410996556282043\n",
      "Epoch 1852, Loss: 0.36269383132457733, Final Batch Loss: 0.16854804754257202\n",
      "Epoch 1853, Loss: 0.4590217024087906, Final Batch Loss: 0.19406168162822723\n",
      "Epoch 1854, Loss: 0.5005767196416855, Final Batch Loss: 0.2577231228351593\n",
      "Epoch 1855, Loss: 0.5002689510583878, Final Batch Loss: 0.2562803328037262\n",
      "Epoch 1856, Loss: 0.4452893137931824, Final Batch Loss: 0.2562074661254883\n",
      "Epoch 1857, Loss: 0.43222226202487946, Final Batch Loss: 0.21204890310764313\n",
      "Epoch 1858, Loss: 0.4130554348230362, Final Batch Loss: 0.16502036154270172\n",
      "Epoch 1859, Loss: 0.4891032725572586, Final Batch Loss: 0.25765663385391235\n",
      "Epoch 1860, Loss: 0.4303947985172272, Final Batch Loss: 0.189482182264328\n",
      "Epoch 1861, Loss: 0.41248437762260437, Final Batch Loss: 0.2135481983423233\n",
      "Epoch 1862, Loss: 0.41940319538116455, Final Batch Loss: 0.16770747303962708\n",
      "Epoch 1863, Loss: 0.3759779632091522, Final Batch Loss: 0.1552189141511917\n",
      "Epoch 1864, Loss: 0.3864726126194, Final Batch Loss: 0.1784030795097351\n",
      "Epoch 1865, Loss: 0.5048655271530151, Final Batch Loss: 0.22749045491218567\n",
      "Epoch 1866, Loss: 0.396257683634758, Final Batch Loss: 0.17344693839550018\n",
      "Epoch 1867, Loss: 0.46618881821632385, Final Batch Loss: 0.25596883893013\n",
      "Epoch 1868, Loss: 0.5027924031019211, Final Batch Loss: 0.2707170844078064\n",
      "Epoch 1869, Loss: 0.5008497834205627, Final Batch Loss: 0.23256835341453552\n",
      "Epoch 1870, Loss: 0.5568795800209045, Final Batch Loss: 0.2637224793434143\n",
      "Epoch 1871, Loss: 0.47777268290519714, Final Batch Loss: 0.2727968096733093\n",
      "Epoch 1872, Loss: 0.4819323420524597, Final Batch Loss: 0.2195652723312378\n",
      "Epoch 1873, Loss: 0.3971993029117584, Final Batch Loss: 0.18674428761005402\n",
      "Epoch 1874, Loss: 0.5288260579109192, Final Batch Loss: 0.2954261302947998\n",
      "Epoch 1875, Loss: 0.42024514079093933, Final Batch Loss: 0.20067599415779114\n",
      "Epoch 1876, Loss: 0.3581860065460205, Final Batch Loss: 0.14979778230190277\n",
      "Epoch 1877, Loss: 0.49537013471126556, Final Batch Loss: 0.2735914885997772\n",
      "Epoch 1878, Loss: 0.46804697811603546, Final Batch Loss: 0.22639933228492737\n",
      "Epoch 1879, Loss: 0.39154474437236786, Final Batch Loss: 0.152414932847023\n",
      "Epoch 1880, Loss: 0.5780459046363831, Final Batch Loss: 0.36819615960121155\n",
      "Epoch 1881, Loss: 0.5160186141729355, Final Batch Loss: 0.2787426710128784\n",
      "Epoch 1882, Loss: 0.43405018746852875, Final Batch Loss: 0.20610664784908295\n",
      "Epoch 1883, Loss: 0.4750189781188965, Final Batch Loss: 0.23970305919647217\n",
      "Epoch 1884, Loss: 0.6144206672906876, Final Batch Loss: 0.38998180627822876\n",
      "Epoch 1885, Loss: 0.3942759782075882, Final Batch Loss: 0.20118270814418793\n",
      "Epoch 1886, Loss: 0.5434990674257278, Final Batch Loss: 0.3214360475540161\n",
      "Epoch 1887, Loss: 0.5058594942092896, Final Batch Loss: 0.306221067905426\n",
      "Epoch 1888, Loss: 0.5584177374839783, Final Batch Loss: 0.3137471377849579\n",
      "Epoch 1889, Loss: 0.5155661702156067, Final Batch Loss: 0.2883888781070709\n",
      "Epoch 1890, Loss: 0.4834275245666504, Final Batch Loss: 0.2805517911911011\n",
      "Epoch 1891, Loss: 0.43537889420986176, Final Batch Loss: 0.22660911083221436\n",
      "Epoch 1892, Loss: 0.3622821420431137, Final Batch Loss: 0.17381425201892853\n",
      "Epoch 1893, Loss: 0.5092876702547073, Final Batch Loss: 0.2659332752227783\n",
      "Epoch 1894, Loss: 0.47948046028614044, Final Batch Loss: 0.2009209245443344\n",
      "Epoch 1895, Loss: 0.5809244215488434, Final Batch Loss: 0.3505118191242218\n",
      "Epoch 1896, Loss: 0.5211343467235565, Final Batch Loss: 0.2147652506828308\n",
      "Epoch 1897, Loss: 0.43044106662273407, Final Batch Loss: 0.2036695033311844\n",
      "Epoch 1898, Loss: 0.47132714092731476, Final Batch Loss: 0.2346765100955963\n",
      "Epoch 1899, Loss: 0.4379062354564667, Final Batch Loss: 0.25968778133392334\n",
      "Epoch 1900, Loss: 0.4134421646595001, Final Batch Loss: 0.19745032489299774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1901, Loss: 0.47755737602710724, Final Batch Loss: 0.25835201144218445\n",
      "Epoch 1902, Loss: 0.485254168510437, Final Batch Loss: 0.32029420137405396\n",
      "Epoch 1903, Loss: 0.3771245926618576, Final Batch Loss: 0.15104441344738007\n",
      "Epoch 1904, Loss: 0.45483507215976715, Final Batch Loss: 0.23017899692058563\n",
      "Epoch 1905, Loss: 0.4059593826532364, Final Batch Loss: 0.1981467455625534\n",
      "Epoch 1906, Loss: 0.44731709361076355, Final Batch Loss: 0.2377920299768448\n",
      "Epoch 1907, Loss: 0.4258602112531662, Final Batch Loss: 0.1833631992340088\n",
      "Epoch 1908, Loss: 0.48461954295635223, Final Batch Loss: 0.2662235498428345\n",
      "Epoch 1909, Loss: 0.47460582852363586, Final Batch Loss: 0.2352718859910965\n",
      "Epoch 1910, Loss: 0.5036173611879349, Final Batch Loss: 0.22778604924678802\n",
      "Epoch 1911, Loss: 0.4431286156177521, Final Batch Loss: 0.21164549887180328\n",
      "Epoch 1912, Loss: 0.4416058659553528, Final Batch Loss: 0.24150420725345612\n",
      "Epoch 1913, Loss: 0.4491353780031204, Final Batch Loss: 0.17271099984645844\n",
      "Epoch 1914, Loss: 0.44251875579357147, Final Batch Loss: 0.2120930403470993\n",
      "Epoch 1915, Loss: 0.42291565239429474, Final Batch Loss: 0.20333757996559143\n",
      "Epoch 1916, Loss: 0.4681203067302704, Final Batch Loss: 0.21983154118061066\n",
      "Epoch 1917, Loss: 0.5633017718791962, Final Batch Loss: 0.2770633399486542\n",
      "Epoch 1918, Loss: 0.4138774424791336, Final Batch Loss: 0.1921214759349823\n",
      "Epoch 1919, Loss: 0.4876367002725601, Final Batch Loss: 0.23085756599903107\n",
      "Epoch 1920, Loss: 0.5144784897565842, Final Batch Loss: 0.24727632105350494\n",
      "Epoch 1921, Loss: 0.4403736889362335, Final Batch Loss: 0.22142453491687775\n",
      "Epoch 1922, Loss: 0.41693374514579773, Final Batch Loss: 0.19824106991291046\n",
      "Epoch 1923, Loss: 0.4692932665348053, Final Batch Loss: 0.2578638792037964\n",
      "Epoch 1924, Loss: 0.4590151607990265, Final Batch Loss: 0.22062630951404572\n",
      "Epoch 1925, Loss: 0.39903680980205536, Final Batch Loss: 0.10905550420284271\n",
      "Epoch 1926, Loss: 0.5756858736276627, Final Batch Loss: 0.3340693414211273\n",
      "Epoch 1927, Loss: 0.41085365414619446, Final Batch Loss: 0.1981111764907837\n",
      "Epoch 1928, Loss: 0.44452910125255585, Final Batch Loss: 0.24171791970729828\n",
      "Epoch 1929, Loss: 0.46694613993167877, Final Batch Loss: 0.21575017273426056\n",
      "Epoch 1930, Loss: 0.4066507816314697, Final Batch Loss: 0.18837524950504303\n",
      "Epoch 1931, Loss: 0.4439167082309723, Final Batch Loss: 0.2288385033607483\n",
      "Epoch 1932, Loss: 0.40020887553691864, Final Batch Loss: 0.17035871744155884\n",
      "Epoch 1933, Loss: 0.47195814549922943, Final Batch Loss: 0.2515381872653961\n",
      "Epoch 1934, Loss: 0.4965427815914154, Final Batch Loss: 0.2989095151424408\n",
      "Epoch 1935, Loss: 0.4157911688089371, Final Batch Loss: 0.20715834200382233\n",
      "Epoch 1936, Loss: 0.457585871219635, Final Batch Loss: 0.2201923429965973\n",
      "Epoch 1937, Loss: 0.4664716124534607, Final Batch Loss: 0.2559880316257477\n",
      "Epoch 1938, Loss: 0.4540340453386307, Final Batch Loss: 0.18453161418437958\n",
      "Epoch 1939, Loss: 0.3773798644542694, Final Batch Loss: 0.1896170973777771\n",
      "Epoch 1940, Loss: 0.4154598116874695, Final Batch Loss: 0.19242265820503235\n",
      "Epoch 1941, Loss: 0.4460013508796692, Final Batch Loss: 0.201374813914299\n",
      "Epoch 1942, Loss: 0.46353569626808167, Final Batch Loss: 0.2314923256635666\n",
      "Epoch 1943, Loss: 0.43415743112564087, Final Batch Loss: 0.2041531652212143\n",
      "Epoch 1944, Loss: 0.46511097252368927, Final Batch Loss: 0.25565868616104126\n",
      "Epoch 1945, Loss: 0.45580412447452545, Final Batch Loss: 0.1959467977285385\n",
      "Epoch 1946, Loss: 0.5419127643108368, Final Batch Loss: 0.2893662750720978\n",
      "Epoch 1947, Loss: 0.4260580688714981, Final Batch Loss: 0.20369909703731537\n",
      "Epoch 1948, Loss: 0.5307634770870209, Final Batch Loss: 0.2649264931678772\n",
      "Epoch 1949, Loss: 0.5183963179588318, Final Batch Loss: 0.26829972863197327\n",
      "Epoch 1950, Loss: 0.5281267762184143, Final Batch Loss: 0.3334655165672302\n",
      "Epoch 1951, Loss: 0.49294400215148926, Final Batch Loss: 0.27362924814224243\n",
      "Epoch 1952, Loss: 0.4491049349308014, Final Batch Loss: 0.2245829701423645\n",
      "Epoch 1953, Loss: 0.46268317103385925, Final Batch Loss: 0.2551605701446533\n",
      "Epoch 1954, Loss: 0.4456061124801636, Final Batch Loss: 0.26467326283454895\n",
      "Epoch 1955, Loss: 0.4349856823682785, Final Batch Loss: 0.22653818130493164\n",
      "Epoch 1956, Loss: 0.4973160922527313, Final Batch Loss: 0.20301690697669983\n",
      "Epoch 1957, Loss: 0.38505719602108, Final Batch Loss: 0.19443054497241974\n",
      "Epoch 1958, Loss: 0.4774724990129471, Final Batch Loss: 0.2420487254858017\n",
      "Epoch 1959, Loss: 0.4575303941965103, Final Batch Loss: 0.2412954717874527\n",
      "Epoch 1960, Loss: 0.5062912404537201, Final Batch Loss: 0.24847197532653809\n",
      "Epoch 1961, Loss: 0.47980979084968567, Final Batch Loss: 0.3048173189163208\n",
      "Epoch 1962, Loss: 0.3955935537815094, Final Batch Loss: 0.16574950516223907\n",
      "Epoch 1963, Loss: 0.5036871582269669, Final Batch Loss: 0.24149467051029205\n",
      "Epoch 1964, Loss: 0.46460041403770447, Final Batch Loss: 0.2430272102355957\n",
      "Epoch 1965, Loss: 0.5356837809085846, Final Batch Loss: 0.28449201583862305\n",
      "Epoch 1966, Loss: 0.42261262238025665, Final Batch Loss: 0.1890307366847992\n",
      "Epoch 1967, Loss: 0.522866353392601, Final Batch Loss: 0.29986873269081116\n",
      "Epoch 1968, Loss: 0.4705236405134201, Final Batch Loss: 0.26138633489608765\n",
      "Epoch 1969, Loss: 0.45629699528217316, Final Batch Loss: 0.2231781780719757\n",
      "Epoch 1970, Loss: 0.4944335073232651, Final Batch Loss: 0.30173254013061523\n",
      "Epoch 1971, Loss: 0.43152229487895966, Final Batch Loss: 0.2385178655385971\n",
      "Epoch 1972, Loss: 0.6127970814704895, Final Batch Loss: 0.38147157430648804\n",
      "Epoch 1973, Loss: 0.5016536265611649, Final Batch Loss: 0.2949167788028717\n",
      "Epoch 1974, Loss: 0.601996123790741, Final Batch Loss: 0.33737170696258545\n",
      "Epoch 1975, Loss: 0.48993828892707825, Final Batch Loss: 0.2272660732269287\n",
      "Epoch 1976, Loss: 0.5132827609777451, Final Batch Loss: 0.32324543595314026\n",
      "Epoch 1977, Loss: 0.44850292801856995, Final Batch Loss: 0.23050759732723236\n",
      "Epoch 1978, Loss: 0.46086573600769043, Final Batch Loss: 0.23246031999588013\n",
      "Epoch 1979, Loss: 0.48081400990486145, Final Batch Loss: 0.2750496566295624\n",
      "Epoch 1980, Loss: 0.5179354548454285, Final Batch Loss: 0.221629798412323\n",
      "Epoch 1981, Loss: 0.38932742178440094, Final Batch Loss: 0.15705764293670654\n",
      "Epoch 1982, Loss: 0.49662233889102936, Final Batch Loss: 0.29101377725601196\n",
      "Epoch 1983, Loss: 0.42850834131240845, Final Batch Loss: 0.16071167588233948\n",
      "Epoch 1984, Loss: 0.4828778952360153, Final Batch Loss: 0.2540419101715088\n",
      "Epoch 1985, Loss: 0.3616584539413452, Final Batch Loss: 0.16558419167995453\n",
      "Epoch 1986, Loss: 0.4389997273683548, Final Batch Loss: 0.2596527338027954\n",
      "Epoch 1987, Loss: 0.4113460034132004, Final Batch Loss: 0.21513737738132477\n",
      "Epoch 1988, Loss: 0.540882334113121, Final Batch Loss: 0.2993394434452057\n",
      "Epoch 1989, Loss: 0.4368462860584259, Final Batch Loss: 0.2536870241165161\n",
      "Epoch 1990, Loss: 0.37752048671245575, Final Batch Loss: 0.17891676723957062\n",
      "Epoch 1991, Loss: 0.35729271173477173, Final Batch Loss: 0.1445741355419159\n",
      "Epoch 1992, Loss: 0.36175209283828735, Final Batch Loss: 0.11878903210163116\n",
      "Epoch 1993, Loss: 0.45187903940677643, Final Batch Loss: 0.17828376591205597\n",
      "Epoch 1994, Loss: 0.4387541115283966, Final Batch Loss: 0.23354944586753845\n",
      "Epoch 1995, Loss: 0.4492826759815216, Final Batch Loss: 0.19382011890411377\n",
      "Epoch 1996, Loss: 0.4549926817417145, Final Batch Loss: 0.26771894097328186\n",
      "Epoch 1997, Loss: 0.4353959411382675, Final Batch Loss: 0.18262819945812225\n",
      "Epoch 1998, Loss: 0.35158155858516693, Final Batch Loss: 0.15407660603523254\n",
      "Epoch 1999, Loss: 0.4535542279481888, Final Batch Loss: 0.27975091338157654\n",
      "Epoch 2000, Loss: 0.4074176847934723, Final Batch Loss: 0.19515001773834229\n",
      "Epoch 2001, Loss: 0.367188423871994, Final Batch Loss: 0.15254999697208405\n",
      "Epoch 2002, Loss: 0.3708374500274658, Final Batch Loss: 0.1825045645236969\n",
      "Epoch 2003, Loss: 0.46165868639945984, Final Batch Loss: 0.23980273306369781\n",
      "Epoch 2004, Loss: 0.5031176060438156, Final Batch Loss: 0.26440608501434326\n",
      "Epoch 2005, Loss: 0.51954485476017, Final Batch Loss: 0.2883768379688263\n",
      "Epoch 2006, Loss: 0.40693721175193787, Final Batch Loss: 0.18358160555362701\n",
      "Epoch 2007, Loss: 0.3966006487607956, Final Batch Loss: 0.2146863490343094\n",
      "Epoch 2008, Loss: 0.39191222190856934, Final Batch Loss: 0.16977010667324066\n",
      "Epoch 2009, Loss: 0.3995777815580368, Final Batch Loss: 0.20144933462142944\n",
      "Epoch 2010, Loss: 0.4271434247493744, Final Batch Loss: 0.19682322442531586\n",
      "Epoch 2011, Loss: 0.47687067091464996, Final Batch Loss: 0.2724100947380066\n",
      "Epoch 2012, Loss: 0.4620652496814728, Final Batch Loss: 0.25112384557724\n",
      "Epoch 2013, Loss: 0.5003519952297211, Final Batch Loss: 0.3282184302806854\n",
      "Epoch 2014, Loss: 0.4420713931322098, Final Batch Loss: 0.19736281037330627\n",
      "Epoch 2015, Loss: 0.4406549781560898, Final Batch Loss: 0.21668563783168793\n",
      "Epoch 2016, Loss: 0.4676126390695572, Final Batch Loss: 0.23870009183883667\n",
      "Epoch 2017, Loss: 0.4358370900154114, Final Batch Loss: 0.20066718757152557\n",
      "Epoch 2018, Loss: 0.4668584018945694, Final Batch Loss: 0.23406216502189636\n",
      "Epoch 2019, Loss: 0.4748784303665161, Final Batch Loss: 0.2259722203016281\n",
      "Epoch 2020, Loss: 0.35522596538066864, Final Batch Loss: 0.17177700996398926\n",
      "Epoch 2021, Loss: 0.4548790454864502, Final Batch Loss: 0.244342640042305\n",
      "Epoch 2022, Loss: 0.44127078354358673, Final Batch Loss: 0.22686491906642914\n",
      "Epoch 2023, Loss: 0.4287991374731064, Final Batch Loss: 0.2310730218887329\n",
      "Epoch 2024, Loss: 0.40530233085155487, Final Batch Loss: 0.19945134222507477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2025, Loss: 0.4470851868391037, Final Batch Loss: 0.2261112779378891\n",
      "Epoch 2026, Loss: 0.4192056655883789, Final Batch Loss: 0.17509134113788605\n",
      "Epoch 2027, Loss: 0.4839530438184738, Final Batch Loss: 0.21738891303539276\n",
      "Epoch 2028, Loss: 0.39045462012290955, Final Batch Loss: 0.17099390923976898\n",
      "Epoch 2029, Loss: 0.4059710204601288, Final Batch Loss: 0.1708647459745407\n",
      "Epoch 2030, Loss: 0.4915548712015152, Final Batch Loss: 0.2797502279281616\n",
      "Epoch 2031, Loss: 0.545608326792717, Final Batch Loss: 0.3422735035419464\n",
      "Epoch 2032, Loss: 0.39138181507587433, Final Batch Loss: 0.22007687389850616\n",
      "Epoch 2033, Loss: 0.45462438464164734, Final Batch Loss: 0.2014070749282837\n",
      "Epoch 2034, Loss: 0.46238288283348083, Final Batch Loss: 0.26919570565223694\n",
      "Epoch 2035, Loss: 0.43346288800239563, Final Batch Loss: 0.2426382154226303\n",
      "Epoch 2036, Loss: 0.4743647128343582, Final Batch Loss: 0.2240675538778305\n",
      "Epoch 2037, Loss: 0.43050533533096313, Final Batch Loss: 0.22165977954864502\n",
      "Epoch 2038, Loss: 0.42084386944770813, Final Batch Loss: 0.17017221450805664\n",
      "Epoch 2039, Loss: 0.35136649012565613, Final Batch Loss: 0.14528591930866241\n",
      "Epoch 2040, Loss: 0.4008864462375641, Final Batch Loss: 0.20546557009220123\n",
      "Epoch 2041, Loss: 0.40388156473636627, Final Batch Loss: 0.18468454480171204\n",
      "Epoch 2042, Loss: 0.41560862958431244, Final Batch Loss: 0.23351562023162842\n",
      "Epoch 2043, Loss: 0.4427240192890167, Final Batch Loss: 0.21596257388591766\n",
      "Epoch 2044, Loss: 0.38246405124664307, Final Batch Loss: 0.19290849566459656\n",
      "Epoch 2045, Loss: 0.5218780487775803, Final Batch Loss: 0.2273084968328476\n",
      "Epoch 2046, Loss: 0.4718310534954071, Final Batch Loss: 0.24936401844024658\n",
      "Epoch 2047, Loss: 0.4190955311059952, Final Batch Loss: 0.14967192709445953\n",
      "Epoch 2048, Loss: 0.4510100185871124, Final Batch Loss: 0.19507765769958496\n",
      "Epoch 2049, Loss: 0.4514892101287842, Final Batch Loss: 0.19585156440734863\n",
      "Epoch 2050, Loss: 0.40859459340572357, Final Batch Loss: 0.16871091723442078\n",
      "Epoch 2051, Loss: 0.4426548629999161, Final Batch Loss: 0.2281901091337204\n",
      "Epoch 2052, Loss: 0.4939424842596054, Final Batch Loss: 0.2725014388561249\n",
      "Epoch 2053, Loss: 0.42940154671669006, Final Batch Loss: 0.21106542646884918\n",
      "Epoch 2054, Loss: 0.4794009327888489, Final Batch Loss: 0.19390136003494263\n",
      "Epoch 2055, Loss: 0.38668106496334076, Final Batch Loss: 0.19989462196826935\n",
      "Epoch 2056, Loss: 0.40087535977363586, Final Batch Loss: 0.16774849593639374\n",
      "Epoch 2057, Loss: 0.4773430824279785, Final Batch Loss: 0.2926101088523865\n",
      "Epoch 2058, Loss: 0.5238737612962723, Final Batch Loss: 0.28057757019996643\n",
      "Epoch 2059, Loss: 0.4219162315130234, Final Batch Loss: 0.20449958741664886\n",
      "Epoch 2060, Loss: 0.4268468767404556, Final Batch Loss: 0.24743907153606415\n",
      "Epoch 2061, Loss: 0.4975908249616623, Final Batch Loss: 0.2943268120288849\n",
      "Epoch 2062, Loss: 0.42964722216129303, Final Batch Loss: 0.2149185836315155\n",
      "Epoch 2063, Loss: 0.3707631677389145, Final Batch Loss: 0.16846667230129242\n",
      "Epoch 2064, Loss: 0.488228440284729, Final Batch Loss: 0.2664264142513275\n",
      "Epoch 2065, Loss: 0.42691317200660706, Final Batch Loss: 0.22261396050453186\n",
      "Epoch 2066, Loss: 0.40707623958587646, Final Batch Loss: 0.18222956359386444\n",
      "Epoch 2067, Loss: 0.4686421751976013, Final Batch Loss: 0.23907111585140228\n",
      "Epoch 2068, Loss: 0.4005362242460251, Final Batch Loss: 0.2250520884990692\n",
      "Epoch 2069, Loss: 0.38392604887485504, Final Batch Loss: 0.16310834884643555\n",
      "Epoch 2070, Loss: 0.4129717946052551, Final Batch Loss: 0.17506693303585052\n",
      "Epoch 2071, Loss: 0.3700624853372574, Final Batch Loss: 0.17206822335720062\n",
      "Epoch 2072, Loss: 0.3763657361268997, Final Batch Loss: 0.18137292563915253\n",
      "Epoch 2073, Loss: 0.4034412056207657, Final Batch Loss: 0.1857898235321045\n",
      "Epoch 2074, Loss: 0.3874195069074631, Final Batch Loss: 0.1702749878168106\n",
      "Epoch 2075, Loss: 0.3975592106580734, Final Batch Loss: 0.1871650069952011\n",
      "Epoch 2076, Loss: 0.4570044279098511, Final Batch Loss: 0.23564757406711578\n",
      "Epoch 2077, Loss: 0.3905121386051178, Final Batch Loss: 0.16900555789470673\n",
      "Epoch 2078, Loss: 0.4354456216096878, Final Batch Loss: 0.20874017477035522\n",
      "Epoch 2079, Loss: 0.4395529329776764, Final Batch Loss: 0.26132556796073914\n",
      "Epoch 2080, Loss: 0.5975180566310883, Final Batch Loss: 0.30677536129951477\n",
      "Epoch 2081, Loss: 0.34136416018009186, Final Batch Loss: 0.1465265303850174\n",
      "Epoch 2082, Loss: 0.4246950000524521, Final Batch Loss: 0.20497392117977142\n",
      "Epoch 2083, Loss: 0.35668498277664185, Final Batch Loss: 0.13583573698997498\n",
      "Epoch 2084, Loss: 0.46162615716457367, Final Batch Loss: 0.19938375055789948\n",
      "Epoch 2085, Loss: 0.3719472736120224, Final Batch Loss: 0.16443084180355072\n",
      "Epoch 2086, Loss: 0.4559634178876877, Final Batch Loss: 0.26053953170776367\n",
      "Epoch 2087, Loss: 0.4666011482477188, Final Batch Loss: 0.2837187349796295\n",
      "Epoch 2088, Loss: 0.4665137231349945, Final Batch Loss: 0.226576566696167\n",
      "Epoch 2089, Loss: 0.41687048971652985, Final Batch Loss: 0.21644984185695648\n",
      "Epoch 2090, Loss: 0.3937627822160721, Final Batch Loss: 0.23194359242916107\n",
      "Epoch 2091, Loss: 0.5089833289384842, Final Batch Loss: 0.27420780062675476\n",
      "Epoch 2092, Loss: 0.38117529451847076, Final Batch Loss: 0.1863899528980255\n",
      "Epoch 2093, Loss: 0.3653952032327652, Final Batch Loss: 0.13132861256599426\n",
      "Epoch 2094, Loss: 0.47531265020370483, Final Batch Loss: 0.2507613003253937\n",
      "Epoch 2095, Loss: 0.4493919759988785, Final Batch Loss: 0.21688959002494812\n",
      "Epoch 2096, Loss: 0.38979238271713257, Final Batch Loss: 0.17007865011692047\n",
      "Epoch 2097, Loss: 0.4543464332818985, Final Batch Loss: 0.24774658679962158\n",
      "Epoch 2098, Loss: 0.40146198868751526, Final Batch Loss: 0.1967208981513977\n",
      "Epoch 2099, Loss: 0.3753475248813629, Final Batch Loss: 0.1724625676870346\n",
      "Epoch 2100, Loss: 0.4334931820631027, Final Batch Loss: 0.2055504024028778\n",
      "Epoch 2101, Loss: 0.46440696716308594, Final Batch Loss: 0.2481287270784378\n",
      "Epoch 2102, Loss: 0.4414599537849426, Final Batch Loss: 0.23855926096439362\n",
      "Epoch 2103, Loss: 0.4483891725540161, Final Batch Loss: 0.2337493896484375\n",
      "Epoch 2104, Loss: 0.34060361981391907, Final Batch Loss: 0.17198631167411804\n",
      "Epoch 2105, Loss: 0.3978955000638962, Final Batch Loss: 0.19798029959201813\n",
      "Epoch 2106, Loss: 0.3801875114440918, Final Batch Loss: 0.17743466794490814\n",
      "Epoch 2107, Loss: 0.46663594245910645, Final Batch Loss: 0.24638566374778748\n",
      "Epoch 2108, Loss: 0.39217011630535126, Final Batch Loss: 0.13669230043888092\n",
      "Epoch 2109, Loss: 0.5141299068927765, Final Batch Loss: 0.29782259464263916\n",
      "Epoch 2110, Loss: 0.39354003965854645, Final Batch Loss: 0.20783010125160217\n",
      "Epoch 2111, Loss: 0.42794254422187805, Final Batch Loss: 0.2512369155883789\n",
      "Epoch 2112, Loss: 0.4541477560997009, Final Batch Loss: 0.23168042302131653\n",
      "Epoch 2113, Loss: 0.3973727524280548, Final Batch Loss: 0.20581376552581787\n",
      "Epoch 2114, Loss: 0.5024385452270508, Final Batch Loss: 0.30391690135002136\n",
      "Epoch 2115, Loss: 0.4893109053373337, Final Batch Loss: 0.266606867313385\n",
      "Epoch 2116, Loss: 0.42594774067401886, Final Batch Loss: 0.17714983224868774\n",
      "Epoch 2117, Loss: 0.3645339608192444, Final Batch Loss: 0.18516914546489716\n",
      "Epoch 2118, Loss: 0.45031656324863434, Final Batch Loss: 0.2632905840873718\n",
      "Epoch 2119, Loss: 0.49009720981121063, Final Batch Loss: 0.2527211606502533\n",
      "Epoch 2120, Loss: 0.48812392354011536, Final Batch Loss: 0.28441205620765686\n",
      "Epoch 2121, Loss: 0.39237235486507416, Final Batch Loss: 0.2098143845796585\n",
      "Epoch 2122, Loss: 0.446803480386734, Final Batch Loss: 0.2350715547800064\n",
      "Epoch 2123, Loss: 0.46987029910087585, Final Batch Loss: 0.20601758360862732\n",
      "Epoch 2124, Loss: 0.3983818292617798, Final Batch Loss: 0.22409790754318237\n",
      "Epoch 2125, Loss: 0.49237577617168427, Final Batch Loss: 0.2760620415210724\n",
      "Epoch 2126, Loss: 0.3537847548723221, Final Batch Loss: 0.15337052941322327\n",
      "Epoch 2127, Loss: 0.3856751322746277, Final Batch Loss: 0.16834495961666107\n",
      "Epoch 2128, Loss: 0.4127453565597534, Final Batch Loss: 0.207804337143898\n",
      "Epoch 2129, Loss: 0.4126414954662323, Final Batch Loss: 0.23959189653396606\n",
      "Epoch 2130, Loss: 0.3825990557670593, Final Batch Loss: 0.19450226426124573\n",
      "Epoch 2131, Loss: 0.442080557346344, Final Batch Loss: 0.2664981484413147\n",
      "Epoch 2132, Loss: 0.4041774719953537, Final Batch Loss: 0.17658819258213043\n",
      "Epoch 2133, Loss: 0.3995204120874405, Final Batch Loss: 0.19037427008152008\n",
      "Epoch 2134, Loss: 0.4085462689399719, Final Batch Loss: 0.20546524226665497\n",
      "Epoch 2135, Loss: 0.37696124613285065, Final Batch Loss: 0.13044482469558716\n",
      "Epoch 2136, Loss: 0.34459608793258667, Final Batch Loss: 0.15398117899894714\n",
      "Epoch 2137, Loss: 0.5980942398309708, Final Batch Loss: 0.4077777564525604\n",
      "Epoch 2138, Loss: 0.4942457526922226, Final Batch Loss: 0.3162505030632019\n",
      "Epoch 2139, Loss: 0.37056925892829895, Final Batch Loss: 0.17592856287956238\n",
      "Epoch 2140, Loss: 0.3704441040754318, Final Batch Loss: 0.175941601395607\n",
      "Epoch 2141, Loss: 0.396613672375679, Final Batch Loss: 0.17757609486579895\n",
      "Epoch 2142, Loss: 0.3990672379732132, Final Batch Loss: 0.1946726143360138\n",
      "Epoch 2143, Loss: 0.45682521164417267, Final Batch Loss: 0.27379554510116577\n",
      "Epoch 2144, Loss: 0.39556509256362915, Final Batch Loss: 0.1855745166540146\n",
      "Epoch 2145, Loss: 0.4143974930047989, Final Batch Loss: 0.20596639811992645\n",
      "Epoch 2146, Loss: 0.4157966673374176, Final Batch Loss: 0.17424345016479492\n",
      "Epoch 2147, Loss: 0.43514998257160187, Final Batch Loss: 0.24282380938529968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2148, Loss: 0.38956108689308167, Final Batch Loss: 0.17760306596755981\n",
      "Epoch 2149, Loss: 0.42719756066799164, Final Batch Loss: 0.20378702878952026\n",
      "Epoch 2150, Loss: 0.3929806798696518, Final Batch Loss: 0.19727149605751038\n",
      "Epoch 2151, Loss: 0.43025651574134827, Final Batch Loss: 0.18552590906620026\n",
      "Epoch 2152, Loss: 0.3501143306493759, Final Batch Loss: 0.12701748311519623\n",
      "Epoch 2153, Loss: 0.4682679623365402, Final Batch Loss: 0.27366358041763306\n",
      "Epoch 2154, Loss: 0.39587633311748505, Final Batch Loss: 0.22272883355617523\n",
      "Epoch 2155, Loss: 0.3987932503223419, Final Batch Loss: 0.19809205830097198\n",
      "Epoch 2156, Loss: 0.401185005903244, Final Batch Loss: 0.164659321308136\n",
      "Epoch 2157, Loss: 0.3609238415956497, Final Batch Loss: 0.14657707512378693\n",
      "Epoch 2158, Loss: 0.37832410633563995, Final Batch Loss: 0.2176114022731781\n",
      "Epoch 2159, Loss: 0.37230201065540314, Final Batch Loss: 0.1594158411026001\n",
      "Epoch 2160, Loss: 0.4264985918998718, Final Batch Loss: 0.21003884077072144\n",
      "Epoch 2161, Loss: 0.48449958860874176, Final Batch Loss: 0.29016759991645813\n",
      "Epoch 2162, Loss: 0.39587657153606415, Final Batch Loss: 0.17686796188354492\n",
      "Epoch 2163, Loss: 0.47258977591991425, Final Batch Loss: 0.2840520441532135\n",
      "Epoch 2164, Loss: 0.40222644805908203, Final Batch Loss: 0.18300697207450867\n",
      "Epoch 2165, Loss: 0.47605061531066895, Final Batch Loss: 0.26863381266593933\n",
      "Epoch 2166, Loss: 0.44083935022354126, Final Batch Loss: 0.22557543218135834\n",
      "Epoch 2167, Loss: 0.35631251335144043, Final Batch Loss: 0.19003434479236603\n",
      "Epoch 2168, Loss: 0.4657846689224243, Final Batch Loss: 0.2795371413230896\n",
      "Epoch 2169, Loss: 0.40915483236312866, Final Batch Loss: 0.18325600028038025\n",
      "Epoch 2170, Loss: 0.46386970579624176, Final Batch Loss: 0.28197407722473145\n",
      "Epoch 2171, Loss: 0.45567405223846436, Final Batch Loss: 0.2290242612361908\n",
      "Epoch 2172, Loss: 0.38618361949920654, Final Batch Loss: 0.14348183572292328\n",
      "Epoch 2173, Loss: 0.39455723762512207, Final Batch Loss: 0.16539299488067627\n",
      "Epoch 2174, Loss: 0.4498981237411499, Final Batch Loss: 0.23662543296813965\n",
      "Epoch 2175, Loss: 0.5820908546447754, Final Batch Loss: 0.3316980004310608\n",
      "Epoch 2176, Loss: 0.397747740149498, Final Batch Loss: 0.21888157725334167\n",
      "Epoch 2177, Loss: 0.3995346128940582, Final Batch Loss: 0.1963168978691101\n",
      "Epoch 2178, Loss: 0.3953949958086014, Final Batch Loss: 0.21424904465675354\n",
      "Epoch 2179, Loss: 0.43778450787067413, Final Batch Loss: 0.24835211038589478\n",
      "Epoch 2180, Loss: 0.4505130648612976, Final Batch Loss: 0.27846261858940125\n",
      "Epoch 2181, Loss: 0.42879393696784973, Final Batch Loss: 0.2384631633758545\n",
      "Epoch 2182, Loss: 0.3212033808231354, Final Batch Loss: 0.1691443771123886\n",
      "Epoch 2183, Loss: 0.37606029212474823, Final Batch Loss: 0.15790289640426636\n",
      "Epoch 2184, Loss: 0.39586181938648224, Final Batch Loss: 0.16005833446979523\n",
      "Epoch 2185, Loss: 0.3729286342859268, Final Batch Loss: 0.1756574958562851\n",
      "Epoch 2186, Loss: 0.3765873312950134, Final Batch Loss: 0.16403375566005707\n",
      "Epoch 2187, Loss: 0.42409847676754, Final Batch Loss: 0.1919708251953125\n",
      "Epoch 2188, Loss: 0.4290895462036133, Final Batch Loss: 0.2400360256433487\n",
      "Epoch 2189, Loss: 0.5067486017942429, Final Batch Loss: 0.22157438099384308\n",
      "Epoch 2190, Loss: 0.39068688452243805, Final Batch Loss: 0.16028526425361633\n",
      "Epoch 2191, Loss: 0.4446555972099304, Final Batch Loss: 0.2241726517677307\n",
      "Epoch 2192, Loss: 0.38821400701999664, Final Batch Loss: 0.21132096648216248\n",
      "Epoch 2193, Loss: 0.495125412940979, Final Batch Loss: 0.2988429367542267\n",
      "Epoch 2194, Loss: 0.4939461946487427, Final Batch Loss: 0.3030658960342407\n",
      "Epoch 2195, Loss: 0.43682099878787994, Final Batch Loss: 0.22056344151496887\n",
      "Epoch 2196, Loss: 0.3621060252189636, Final Batch Loss: 0.1470693051815033\n",
      "Epoch 2197, Loss: 0.4143295884132385, Final Batch Loss: 0.20444482564926147\n",
      "Epoch 2198, Loss: 0.47583478689193726, Final Batch Loss: 0.26951849460601807\n",
      "Epoch 2199, Loss: 0.4238386005163193, Final Batch Loss: 0.20497673749923706\n",
      "Epoch 2200, Loss: 0.43773598968982697, Final Batch Loss: 0.21848830580711365\n",
      "Epoch 2201, Loss: 0.40044476091861725, Final Batch Loss: 0.180757075548172\n",
      "Epoch 2202, Loss: 0.44604945182800293, Final Batch Loss: 0.1975078582763672\n",
      "Epoch 2203, Loss: 0.5687834322452545, Final Batch Loss: 0.33994385600090027\n",
      "Epoch 2204, Loss: 0.3434569835662842, Final Batch Loss: 0.16505558788776398\n",
      "Epoch 2205, Loss: 0.4134625792503357, Final Batch Loss: 0.17429503798484802\n",
      "Epoch 2206, Loss: 0.5295069664716721, Final Batch Loss: 0.2983247935771942\n",
      "Epoch 2207, Loss: 0.5461863428354263, Final Batch Loss: 0.3399656414985657\n",
      "Epoch 2208, Loss: 0.3660820499062538, Final Batch Loss: 0.12354122847318649\n",
      "Epoch 2209, Loss: 0.4110162705183029, Final Batch Loss: 0.22247245907783508\n",
      "Epoch 2210, Loss: 0.5378429442644119, Final Batch Loss: 0.3497408926486969\n",
      "Epoch 2211, Loss: 0.3955894559621811, Final Batch Loss: 0.14042599499225616\n",
      "Epoch 2212, Loss: 0.33394068479537964, Final Batch Loss: 0.14812619984149933\n",
      "Epoch 2213, Loss: 0.46865780651569366, Final Batch Loss: 0.2616131901741028\n",
      "Epoch 2214, Loss: 0.39248280227184296, Final Batch Loss: 0.19422820210456848\n",
      "Epoch 2215, Loss: 0.5093918293714523, Final Batch Loss: 0.27651333808898926\n",
      "Epoch 2216, Loss: 0.3408886194229126, Final Batch Loss: 0.16784238815307617\n",
      "Epoch 2217, Loss: 0.4203993231058121, Final Batch Loss: 0.14767371118068695\n",
      "Epoch 2218, Loss: 0.4253523498773575, Final Batch Loss: 0.21872282028198242\n",
      "Epoch 2219, Loss: 0.37406353652477264, Final Batch Loss: 0.18016067147254944\n",
      "Epoch 2220, Loss: 0.37836185097694397, Final Batch Loss: 0.1731215864419937\n",
      "Epoch 2221, Loss: 0.3577762395143509, Final Batch Loss: 0.13013923168182373\n",
      "Epoch 2222, Loss: 0.3467832952737808, Final Batch Loss: 0.16682520508766174\n",
      "Epoch 2223, Loss: 0.37310661375522614, Final Batch Loss: 0.1930111199617386\n",
      "Epoch 2224, Loss: 0.37859050929546356, Final Batch Loss: 0.18007728457450867\n",
      "Epoch 2225, Loss: 0.4724104106426239, Final Batch Loss: 0.28741639852523804\n",
      "Epoch 2226, Loss: 0.3629658967256546, Final Batch Loss: 0.14953398704528809\n",
      "Epoch 2227, Loss: 0.36238107085227966, Final Batch Loss: 0.1609586477279663\n",
      "Epoch 2228, Loss: 0.4037625938653946, Final Batch Loss: 0.20045873522758484\n",
      "Epoch 2229, Loss: 0.4076356440782547, Final Batch Loss: 0.1904880702495575\n",
      "Epoch 2230, Loss: 0.44554929435253143, Final Batch Loss: 0.264516144990921\n",
      "Epoch 2231, Loss: 0.3703133761882782, Final Batch Loss: 0.17720438539981842\n",
      "Epoch 2232, Loss: 0.3791615217924118, Final Batch Loss: 0.18066157400608063\n",
      "Epoch 2233, Loss: 0.3779933899641037, Final Batch Loss: 0.2067905217409134\n",
      "Epoch 2234, Loss: 0.3971029818058014, Final Batch Loss: 0.214877650141716\n",
      "Epoch 2235, Loss: 0.40849803388118744, Final Batch Loss: 0.17436657845973969\n",
      "Epoch 2236, Loss: 0.49028465151786804, Final Batch Loss: 0.3010040819644928\n",
      "Epoch 2237, Loss: 0.4578746259212494, Final Batch Loss: 0.25851285457611084\n",
      "Epoch 2238, Loss: 0.4669375866651535, Final Batch Loss: 0.2399589568376541\n",
      "Epoch 2239, Loss: 0.5130029618740082, Final Batch Loss: 0.30985134840011597\n",
      "Epoch 2240, Loss: 0.46631380915641785, Final Batch Loss: 0.2982091009616852\n",
      "Epoch 2241, Loss: 0.41329535841941833, Final Batch Loss: 0.1982789784669876\n",
      "Epoch 2242, Loss: 0.3198549747467041, Final Batch Loss: 0.1557716280221939\n",
      "Epoch 2243, Loss: 0.5340903252363205, Final Batch Loss: 0.3143228590488434\n",
      "Epoch 2244, Loss: 0.39416924118995667, Final Batch Loss: 0.23279844224452972\n",
      "Epoch 2245, Loss: 0.2769036665558815, Final Batch Loss: 0.12354458123445511\n",
      "Epoch 2246, Loss: 0.35481907427310944, Final Batch Loss: 0.13285662233829498\n",
      "Epoch 2247, Loss: 0.4688321501016617, Final Batch Loss: 0.28970110416412354\n",
      "Epoch 2248, Loss: 0.3524448871612549, Final Batch Loss: 0.20876266062259674\n",
      "Epoch 2249, Loss: 0.36888451874256134, Final Batch Loss: 0.16584594547748566\n",
      "Epoch 2250, Loss: 0.4752018600702286, Final Batch Loss: 0.2532899081707001\n",
      "Epoch 2251, Loss: 0.3145250976085663, Final Batch Loss: 0.16285613179206848\n",
      "Epoch 2252, Loss: 0.4119616001844406, Final Batch Loss: 0.21886581182479858\n",
      "Epoch 2253, Loss: 0.3906416594982147, Final Batch Loss: 0.15866486728191376\n",
      "Epoch 2254, Loss: 0.3527246117591858, Final Batch Loss: 0.17343489825725555\n",
      "Epoch 2255, Loss: 0.35684116184711456, Final Batch Loss: 0.15203909575939178\n",
      "Epoch 2256, Loss: 0.47626858949661255, Final Batch Loss: 0.21663802862167358\n",
      "Epoch 2257, Loss: 0.3841222673654556, Final Batch Loss: 0.17214486002922058\n",
      "Epoch 2258, Loss: 0.35690610110759735, Final Batch Loss: 0.1296350508928299\n",
      "Epoch 2259, Loss: 0.3804890215396881, Final Batch Loss: 0.13330484926700592\n",
      "Epoch 2260, Loss: 0.3698294013738632, Final Batch Loss: 0.13898931443691254\n",
      "Epoch 2261, Loss: 0.45612238347530365, Final Batch Loss: 0.20859768986701965\n",
      "Epoch 2262, Loss: 0.35123996436595917, Final Batch Loss: 0.16519610583782196\n",
      "Epoch 2263, Loss: 0.3899597078561783, Final Batch Loss: 0.20923326909542084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2264, Loss: 0.4165849983692169, Final Batch Loss: 0.2266935110092163\n",
      "Epoch 2265, Loss: 0.48904550075531006, Final Batch Loss: 0.30973389744758606\n",
      "Epoch 2266, Loss: 0.3659443110227585, Final Batch Loss: 0.18010522425174713\n",
      "Epoch 2267, Loss: 0.37892043590545654, Final Batch Loss: 0.18581606447696686\n",
      "Epoch 2268, Loss: 0.4084385335445404, Final Batch Loss: 0.2047082781791687\n",
      "Epoch 2269, Loss: 0.35819897055625916, Final Batch Loss: 0.18961775302886963\n",
      "Epoch 2270, Loss: 0.41863152384757996, Final Batch Loss: 0.20374976098537445\n",
      "Epoch 2271, Loss: 0.3782593905925751, Final Batch Loss: 0.1833764612674713\n",
      "Epoch 2272, Loss: 0.42241787910461426, Final Batch Loss: 0.19817082583904266\n",
      "Epoch 2273, Loss: 0.4261549264192581, Final Batch Loss: 0.2078690379858017\n",
      "Epoch 2274, Loss: 0.4200570583343506, Final Batch Loss: 0.20160003006458282\n",
      "Epoch 2275, Loss: 0.3919616788625717, Final Batch Loss: 0.21341799199581146\n",
      "Epoch 2276, Loss: 0.3877270966768265, Final Batch Loss: 0.19846169650554657\n",
      "Epoch 2277, Loss: 0.4195980131626129, Final Batch Loss: 0.225968599319458\n",
      "Epoch 2278, Loss: 0.43308861553668976, Final Batch Loss: 0.26857149600982666\n",
      "Epoch 2279, Loss: 0.4181472212076187, Final Batch Loss: 0.18119879066944122\n",
      "Epoch 2280, Loss: 0.47356659173965454, Final Batch Loss: 0.22539807856082916\n",
      "Epoch 2281, Loss: 0.4205983430147171, Final Batch Loss: 0.22190912067890167\n",
      "Epoch 2282, Loss: 0.40880732238292694, Final Batch Loss: 0.21191099286079407\n",
      "Epoch 2283, Loss: 0.4771987348794937, Final Batch Loss: 0.24455079436302185\n",
      "Epoch 2284, Loss: 0.4139510542154312, Final Batch Loss: 0.22666704654693604\n",
      "Epoch 2285, Loss: 0.523150846362114, Final Batch Loss: 0.31240037083625793\n",
      "Epoch 2286, Loss: 0.3115471452474594, Final Batch Loss: 0.13911327719688416\n",
      "Epoch 2287, Loss: 0.41327929496765137, Final Batch Loss: 0.14618319272994995\n",
      "Epoch 2288, Loss: 0.45346900820732117, Final Batch Loss: 0.23752140998840332\n",
      "Epoch 2289, Loss: 0.3421494960784912, Final Batch Loss: 0.16172859072685242\n",
      "Epoch 2290, Loss: 0.3650617301464081, Final Batch Loss: 0.16680853068828583\n",
      "Epoch 2291, Loss: 0.45567578077316284, Final Batch Loss: 0.18368500471115112\n",
      "Epoch 2292, Loss: 0.4884963780641556, Final Batch Loss: 0.25089991092681885\n",
      "Epoch 2293, Loss: 0.4372491091489792, Final Batch Loss: 0.1778603345155716\n",
      "Epoch 2294, Loss: 0.37069734930992126, Final Batch Loss: 0.17893829941749573\n",
      "Epoch 2295, Loss: 0.42087338864803314, Final Batch Loss: 0.2041516900062561\n",
      "Epoch 2296, Loss: 0.45328284800052643, Final Batch Loss: 0.22375212609767914\n",
      "Epoch 2297, Loss: 0.4272267669439316, Final Batch Loss: 0.2352714240550995\n",
      "Epoch 2298, Loss: 0.36302636563777924, Final Batch Loss: 0.18694251775741577\n",
      "Epoch 2299, Loss: 0.40563569962978363, Final Batch Loss: 0.23272240161895752\n",
      "Epoch 2300, Loss: 0.4166545867919922, Final Batch Loss: 0.2206135392189026\n",
      "Epoch 2301, Loss: 0.3408859521150589, Final Batch Loss: 0.1443110555410385\n",
      "Epoch 2302, Loss: 0.5489325970411301, Final Batch Loss: 0.34896016120910645\n",
      "Epoch 2303, Loss: 0.4101007878780365, Final Batch Loss: 0.1683628261089325\n",
      "Epoch 2304, Loss: 0.4022757261991501, Final Batch Loss: 0.2527259886264801\n",
      "Epoch 2305, Loss: 0.41269779205322266, Final Batch Loss: 0.24390339851379395\n",
      "Epoch 2306, Loss: 0.5478478372097015, Final Batch Loss: 0.2670665383338928\n",
      "Epoch 2307, Loss: 0.38681860268116, Final Batch Loss: 0.16667678952217102\n",
      "Epoch 2308, Loss: 0.37692081928253174, Final Batch Loss: 0.18442662060260773\n",
      "Epoch 2309, Loss: 0.33905938267707825, Final Batch Loss: 0.1710495948791504\n",
      "Epoch 2310, Loss: 0.5372650474309921, Final Batch Loss: 0.37004104256629944\n",
      "Epoch 2311, Loss: 0.5548814535140991, Final Batch Loss: 0.3222425878047943\n",
      "Epoch 2312, Loss: 0.4239175468683243, Final Batch Loss: 0.2181909680366516\n",
      "Epoch 2313, Loss: 0.38913096487522125, Final Batch Loss: 0.19077393412590027\n",
      "Epoch 2314, Loss: 0.4080243706703186, Final Batch Loss: 0.20443689823150635\n",
      "Epoch 2315, Loss: 0.38158848881721497, Final Batch Loss: 0.19238093495368958\n",
      "Epoch 2316, Loss: 0.480421781539917, Final Batch Loss: 0.26254594326019287\n",
      "Epoch 2317, Loss: 0.3956872522830963, Final Batch Loss: 0.18217185139656067\n",
      "Epoch 2318, Loss: 0.3832100182771683, Final Batch Loss: 0.1877327263355255\n",
      "Epoch 2319, Loss: 0.42728452384471893, Final Batch Loss: 0.25284630060195923\n",
      "Epoch 2320, Loss: 0.4053329676389694, Final Batch Loss: 0.2067578136920929\n",
      "Epoch 2321, Loss: 0.4048411399126053, Final Batch Loss: 0.17018777132034302\n",
      "Epoch 2322, Loss: 0.5025100558996201, Final Batch Loss: 0.29245808720588684\n",
      "Epoch 2323, Loss: 0.36586761474609375, Final Batch Loss: 0.14761613309383392\n",
      "Epoch 2324, Loss: 0.38036398589611053, Final Batch Loss: 0.17222177982330322\n",
      "Epoch 2325, Loss: 0.3516232669353485, Final Batch Loss: 0.18451817333698273\n",
      "Epoch 2326, Loss: 0.31160596013069153, Final Batch Loss: 0.11098235845565796\n",
      "Epoch 2327, Loss: 0.44313493371009827, Final Batch Loss: 0.2344302237033844\n",
      "Epoch 2328, Loss: 0.3971029669046402, Final Batch Loss: 0.14907260239124298\n",
      "Epoch 2329, Loss: 0.4381880760192871, Final Batch Loss: 0.26552844047546387\n",
      "Epoch 2330, Loss: 0.5128885507583618, Final Batch Loss: 0.24550625681877136\n",
      "Epoch 2331, Loss: 0.35017630457878113, Final Batch Loss: 0.15709935128688812\n",
      "Epoch 2332, Loss: 0.3828183114528656, Final Batch Loss: 0.1823727935552597\n",
      "Epoch 2333, Loss: 0.40805333852767944, Final Batch Loss: 0.18149754405021667\n",
      "Epoch 2334, Loss: 0.4379798173904419, Final Batch Loss: 0.18661978840827942\n",
      "Epoch 2335, Loss: 0.40215492248535156, Final Batch Loss: 0.1913641393184662\n",
      "Epoch 2336, Loss: 0.3486887812614441, Final Batch Loss: 0.15387684106826782\n",
      "Epoch 2337, Loss: 0.4354603737592697, Final Batch Loss: 0.2004597932100296\n",
      "Epoch 2338, Loss: 0.3802160620689392, Final Batch Loss: 0.1453656554222107\n",
      "Epoch 2339, Loss: 0.38693735003471375, Final Batch Loss: 0.19506020843982697\n",
      "Epoch 2340, Loss: 0.4143128991127014, Final Batch Loss: 0.18148306012153625\n",
      "Epoch 2341, Loss: 0.39676304161548615, Final Batch Loss: 0.20382830500602722\n",
      "Epoch 2342, Loss: 0.4863617271184921, Final Batch Loss: 0.2730501890182495\n",
      "Epoch 2343, Loss: 0.4276520162820816, Final Batch Loss: 0.23972779512405396\n",
      "Epoch 2344, Loss: 0.444023460149765, Final Batch Loss: 0.2517164945602417\n",
      "Epoch 2345, Loss: 0.37312184274196625, Final Batch Loss: 0.17309702932834625\n",
      "Epoch 2346, Loss: 0.42452751100063324, Final Batch Loss: 0.20629575848579407\n",
      "Epoch 2347, Loss: 0.36466385424137115, Final Batch Loss: 0.16812889277935028\n",
      "Epoch 2348, Loss: 0.47195087373256683, Final Batch Loss: 0.26378729939460754\n",
      "Epoch 2349, Loss: 0.3739232271909714, Final Batch Loss: 0.16304123401641846\n",
      "Epoch 2350, Loss: 0.3985501080751419, Final Batch Loss: 0.21189656853675842\n",
      "Epoch 2351, Loss: 0.45309095084667206, Final Batch Loss: 0.2085464596748352\n",
      "Epoch 2352, Loss: 0.39453649520874023, Final Batch Loss: 0.20722617208957672\n",
      "Epoch 2353, Loss: 0.4055706113576889, Final Batch Loss: 0.20753875374794006\n",
      "Epoch 2354, Loss: 0.48011650145053864, Final Batch Loss: 0.2717846930027008\n",
      "Epoch 2355, Loss: 0.39913706481456757, Final Batch Loss: 0.22471153736114502\n",
      "Epoch 2356, Loss: 0.3971993029117584, Final Batch Loss: 0.21328699588775635\n",
      "Epoch 2357, Loss: 0.3352214992046356, Final Batch Loss: 0.18276095390319824\n",
      "Epoch 2358, Loss: 0.37342071533203125, Final Batch Loss: 0.200795516371727\n",
      "Epoch 2359, Loss: 0.33405208587646484, Final Batch Loss: 0.17409318685531616\n",
      "Epoch 2360, Loss: 0.4530393183231354, Final Batch Loss: 0.25265780091285706\n",
      "Epoch 2361, Loss: 0.3906770348548889, Final Batch Loss: 0.20579759776592255\n",
      "Epoch 2362, Loss: 0.35629037022590637, Final Batch Loss: 0.1692708432674408\n",
      "Epoch 2363, Loss: 0.4216795712709427, Final Batch Loss: 0.16480974853038788\n",
      "Epoch 2364, Loss: 0.3212665468454361, Final Batch Loss: 0.1098967045545578\n",
      "Epoch 2365, Loss: 0.39027079939842224, Final Batch Loss: 0.20131896436214447\n",
      "Epoch 2366, Loss: 0.44798651337623596, Final Batch Loss: 0.24022476375102997\n",
      "Epoch 2367, Loss: 0.4020650386810303, Final Batch Loss: 0.19500847160816193\n",
      "Epoch 2368, Loss: 0.39822356402873993, Final Batch Loss: 0.20524191856384277\n",
      "Epoch 2369, Loss: 0.3935883790254593, Final Batch Loss: 0.19211266934871674\n",
      "Epoch 2370, Loss: 0.4004107862710953, Final Batch Loss: 0.18268465995788574\n",
      "Epoch 2371, Loss: 0.3444702625274658, Final Batch Loss: 0.15154603123664856\n",
      "Epoch 2372, Loss: 0.4110606461763382, Final Batch Loss: 0.20500274002552032\n",
      "Epoch 2373, Loss: 0.39883558452129364, Final Batch Loss: 0.19049662351608276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2374, Loss: 0.3311406224966049, Final Batch Loss: 0.1354677826166153\n",
      "Epoch 2375, Loss: 0.3924836218357086, Final Batch Loss: 0.1923523247241974\n",
      "Epoch 2376, Loss: 0.3890782445669174, Final Batch Loss: 0.218475341796875\n",
      "Epoch 2377, Loss: 0.3810888081789017, Final Batch Loss: 0.1707419455051422\n",
      "Epoch 2378, Loss: 0.38747963309288025, Final Batch Loss: 0.21945448219776154\n",
      "Epoch 2379, Loss: 0.4265851080417633, Final Batch Loss: 0.2502225339412689\n",
      "Epoch 2380, Loss: 0.3962756246328354, Final Batch Loss: 0.20174778997898102\n",
      "Epoch 2381, Loss: 0.39850808680057526, Final Batch Loss: 0.23103339970111847\n",
      "Epoch 2382, Loss: 0.4298058748245239, Final Batch Loss: 0.2474384307861328\n",
      "Epoch 2383, Loss: 0.4033639281988144, Final Batch Loss: 0.2138241082429886\n",
      "Epoch 2384, Loss: 0.4655786007642746, Final Batch Loss: 0.2642782926559448\n",
      "Epoch 2385, Loss: 0.5435824990272522, Final Batch Loss: 0.30208683013916016\n",
      "Epoch 2386, Loss: 0.36411242187023163, Final Batch Loss: 0.18535982072353363\n",
      "Epoch 2387, Loss: 0.3405957818031311, Final Batch Loss: 0.1837138533592224\n",
      "Epoch 2388, Loss: 0.4643568992614746, Final Batch Loss: 0.288128525018692\n",
      "Epoch 2389, Loss: 0.358028769493103, Final Batch Loss: 0.1394226849079132\n",
      "Epoch 2390, Loss: 0.45741425454616547, Final Batch Loss: 0.256007581949234\n",
      "Epoch 2391, Loss: 0.5754417479038239, Final Batch Loss: 0.36488595604896545\n",
      "Epoch 2392, Loss: 0.43133997917175293, Final Batch Loss: 0.23346443474292755\n",
      "Epoch 2393, Loss: 0.44244201481342316, Final Batch Loss: 0.22636820375919342\n",
      "Epoch 2394, Loss: 0.4357430636882782, Final Batch Loss: 0.24719828367233276\n",
      "Epoch 2395, Loss: 0.4172254502773285, Final Batch Loss: 0.2321440726518631\n",
      "Epoch 2396, Loss: 0.39646464586257935, Final Batch Loss: 0.2077171802520752\n",
      "Epoch 2397, Loss: 0.3981639891862869, Final Batch Loss: 0.2379515916109085\n",
      "Epoch 2398, Loss: 0.3720380365848541, Final Batch Loss: 0.19412271678447723\n",
      "Epoch 2399, Loss: 0.36774997413158417, Final Batch Loss: 0.14663845300674438\n",
      "Epoch 2400, Loss: 0.4488445073366165, Final Batch Loss: 0.28656503558158875\n",
      "Epoch 2401, Loss: 0.425330713391304, Final Batch Loss: 0.20061750710010529\n",
      "Epoch 2402, Loss: 0.432476282119751, Final Batch Loss: 0.22733473777770996\n",
      "Epoch 2403, Loss: 0.4041374921798706, Final Batch Loss: 0.15616704523563385\n",
      "Epoch 2404, Loss: 0.43579477071762085, Final Batch Loss: 0.22548581659793854\n",
      "Epoch 2405, Loss: 0.4061453193426132, Final Batch Loss: 0.17644216120243073\n",
      "Epoch 2406, Loss: 0.37883980572223663, Final Batch Loss: 0.18227519094944\n",
      "Epoch 2407, Loss: 0.341544970870018, Final Batch Loss: 0.16422459483146667\n",
      "Epoch 2408, Loss: 0.3581212908029556, Final Batch Loss: 0.15350699424743652\n",
      "Epoch 2409, Loss: 0.46954917907714844, Final Batch Loss: 0.24289438128471375\n",
      "Epoch 2410, Loss: 0.36908717453479767, Final Batch Loss: 0.14710600674152374\n",
      "Epoch 2411, Loss: 0.412805512547493, Final Batch Loss: 0.22147147357463837\n",
      "Epoch 2412, Loss: 0.3857990652322769, Final Batch Loss: 0.1778821051120758\n",
      "Epoch 2413, Loss: 0.3486032634973526, Final Batch Loss: 0.17787249386310577\n",
      "Epoch 2414, Loss: 0.44822221994400024, Final Batch Loss: 0.23821425437927246\n",
      "Epoch 2415, Loss: 0.3877321034669876, Final Batch Loss: 0.18640878796577454\n",
      "Epoch 2416, Loss: 0.344036340713501, Final Batch Loss: 0.16318650543689728\n",
      "Epoch 2417, Loss: 0.5021744668483734, Final Batch Loss: 0.3200880289077759\n",
      "Epoch 2418, Loss: 0.374689057469368, Final Batch Loss: 0.23249857127666473\n",
      "Epoch 2419, Loss: 0.3779001832008362, Final Batch Loss: 0.1954691857099533\n",
      "Epoch 2420, Loss: 0.4499645531177521, Final Batch Loss: 0.20093940198421478\n",
      "Epoch 2421, Loss: 0.4333101063966751, Final Batch Loss: 0.24314135313034058\n",
      "Epoch 2422, Loss: 0.36019203066825867, Final Batch Loss: 0.16077467799186707\n",
      "Epoch 2423, Loss: 0.42366328835487366, Final Batch Loss: 0.22977308928966522\n",
      "Epoch 2424, Loss: 0.4364723861217499, Final Batch Loss: 0.18588018417358398\n",
      "Epoch 2425, Loss: 0.3899374306201935, Final Batch Loss: 0.19109003245830536\n",
      "Epoch 2426, Loss: 0.4558737576007843, Final Batch Loss: 0.23141182959079742\n",
      "Epoch 2427, Loss: 0.42918525636196136, Final Batch Loss: 0.24602195620536804\n",
      "Epoch 2428, Loss: 0.48947592079639435, Final Batch Loss: 0.29139938950538635\n",
      "Epoch 2429, Loss: 0.3307375907897949, Final Batch Loss: 0.13620144128799438\n",
      "Epoch 2430, Loss: 0.376030758023262, Final Batch Loss: 0.15564928948879242\n",
      "Epoch 2431, Loss: 0.3462713360786438, Final Batch Loss: 0.1722961962223053\n",
      "Epoch 2432, Loss: 0.39886291325092316, Final Batch Loss: 0.161682590842247\n",
      "Epoch 2433, Loss: 0.34872959554195404, Final Batch Loss: 0.1735447496175766\n",
      "Epoch 2434, Loss: 0.38118259608745575, Final Batch Loss: 0.18362335860729218\n",
      "Epoch 2435, Loss: 0.5017893314361572, Final Batch Loss: 0.24183428287506104\n",
      "Epoch 2436, Loss: 0.3605349659919739, Final Batch Loss: 0.19577591121196747\n",
      "Epoch 2437, Loss: 0.32636700570583344, Final Batch Loss: 0.15122629702091217\n",
      "Epoch 2438, Loss: 0.41182956099510193, Final Batch Loss: 0.2750607430934906\n",
      "Epoch 2439, Loss: 0.35932059586048126, Final Batch Loss: 0.1518060266971588\n",
      "Epoch 2440, Loss: 0.409413143992424, Final Batch Loss: 0.23816704750061035\n",
      "Epoch 2441, Loss: 0.3980124890804291, Final Batch Loss: 0.23434174060821533\n",
      "Epoch 2442, Loss: 0.4400273263454437, Final Batch Loss: 0.25482824444770813\n",
      "Epoch 2443, Loss: 0.32714971899986267, Final Batch Loss: 0.1446985900402069\n",
      "Epoch 2444, Loss: 0.41567154228687286, Final Batch Loss: 0.21058447659015656\n",
      "Epoch 2445, Loss: 0.3911930322647095, Final Batch Loss: 0.2000722587108612\n",
      "Epoch 2446, Loss: 0.34624843299388885, Final Batch Loss: 0.1610654592514038\n",
      "Epoch 2447, Loss: 0.4009527415037155, Final Batch Loss: 0.23165647685527802\n",
      "Epoch 2448, Loss: 0.3510962873697281, Final Batch Loss: 0.1362287700176239\n",
      "Epoch 2449, Loss: 0.39659981429576874, Final Batch Loss: 0.22617469727993011\n",
      "Epoch 2450, Loss: 0.32663723081350327, Final Batch Loss: 0.12175879627466202\n",
      "Epoch 2451, Loss: 0.3873966634273529, Final Batch Loss: 0.18411943316459656\n",
      "Epoch 2452, Loss: 0.47856923937797546, Final Batch Loss: 0.23230759799480438\n",
      "Epoch 2453, Loss: 0.38390085101127625, Final Batch Loss: 0.21521927416324615\n",
      "Epoch 2454, Loss: 0.35533323884010315, Final Batch Loss: 0.15270116925239563\n",
      "Epoch 2455, Loss: 0.4257795214653015, Final Batch Loss: 0.24203433096408844\n",
      "Epoch 2456, Loss: 0.4525109529495239, Final Batch Loss: 0.229976624250412\n",
      "Epoch 2457, Loss: 0.359408438205719, Final Batch Loss: 0.15574465692043304\n",
      "Epoch 2458, Loss: 0.3847957104444504, Final Batch Loss: 0.13722862303256989\n",
      "Epoch 2459, Loss: 0.34436896443367004, Final Batch Loss: 0.15358670055866241\n",
      "Epoch 2460, Loss: 0.3274126946926117, Final Batch Loss: 0.16488420963287354\n",
      "Epoch 2461, Loss: 0.4016038775444031, Final Batch Loss: 0.17501571774482727\n",
      "Epoch 2462, Loss: 0.3824268579483032, Final Batch Loss: 0.2031250298023224\n",
      "Epoch 2463, Loss: 0.42311128973960876, Final Batch Loss: 0.23162150382995605\n",
      "Epoch 2464, Loss: 0.3971613645553589, Final Batch Loss: 0.19056828320026398\n",
      "Epoch 2465, Loss: 0.357461616396904, Final Batch Loss: 0.13482780754566193\n",
      "Epoch 2466, Loss: 0.4604966342449188, Final Batch Loss: 0.23383794724941254\n",
      "Epoch 2467, Loss: 0.41723981499671936, Final Batch Loss: 0.24383901059627533\n",
      "Epoch 2468, Loss: 0.42254702746868134, Final Batch Loss: 0.23925279080867767\n",
      "Epoch 2469, Loss: 0.4225118011236191, Final Batch Loss: 0.21182380616664886\n",
      "Epoch 2470, Loss: 0.3554535359144211, Final Batch Loss: 0.14628484845161438\n",
      "Epoch 2471, Loss: 0.4095587134361267, Final Batch Loss: 0.1889171451330185\n",
      "Epoch 2472, Loss: 0.4432230740785599, Final Batch Loss: 0.23086585104465485\n",
      "Epoch 2473, Loss: 0.4782373756170273, Final Batch Loss: 0.25184884667396545\n",
      "Epoch 2474, Loss: 0.37072838842868805, Final Batch Loss: 0.1851760894060135\n",
      "Epoch 2475, Loss: 0.3870133012533188, Final Batch Loss: 0.18897117674350739\n",
      "Epoch 2476, Loss: 0.41510163247585297, Final Batch Loss: 0.19708296656608582\n",
      "Epoch 2477, Loss: 0.3341217041015625, Final Batch Loss: 0.13868509232997894\n",
      "Epoch 2478, Loss: 0.325849324464798, Final Batch Loss: 0.13810193538665771\n",
      "Epoch 2479, Loss: 0.33748023211956024, Final Batch Loss: 0.14556017518043518\n",
      "Epoch 2480, Loss: 0.3321310728788376, Final Batch Loss: 0.14944897592067719\n",
      "Epoch 2481, Loss: 0.37533606588840485, Final Batch Loss: 0.1739950031042099\n",
      "Epoch 2482, Loss: 0.36788734793663025, Final Batch Loss: 0.1554960459470749\n",
      "Epoch 2483, Loss: 0.3675599843263626, Final Batch Loss: 0.18667861819267273\n",
      "Epoch 2484, Loss: 0.36996251344680786, Final Batch Loss: 0.21394862234592438\n",
      "Epoch 2485, Loss: 0.46153524518013, Final Batch Loss: 0.2753901183605194\n",
      "Epoch 2486, Loss: 0.39426907896995544, Final Batch Loss: 0.19379335641860962\n",
      "Epoch 2487, Loss: 0.33493202924728394, Final Batch Loss: 0.16353154182434082\n",
      "Epoch 2488, Loss: 0.338323175907135, Final Batch Loss: 0.12990640103816986\n",
      "Epoch 2489, Loss: 0.4722036123275757, Final Batch Loss: 0.23355501890182495\n",
      "Epoch 2490, Loss: 0.3749701976776123, Final Batch Loss: 0.14440320432186127\n",
      "Epoch 2491, Loss: 0.41784335672855377, Final Batch Loss: 0.1759151667356491\n",
      "Epoch 2492, Loss: 0.33360014855861664, Final Batch Loss: 0.17213007807731628\n",
      "Epoch 2493, Loss: 0.39543308317661285, Final Batch Loss: 0.17460978031158447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2494, Loss: 0.46938371658325195, Final Batch Loss: 0.2720339596271515\n",
      "Epoch 2495, Loss: 0.3731907904148102, Final Batch Loss: 0.15588663518428802\n",
      "Epoch 2496, Loss: 0.35743892192840576, Final Batch Loss: 0.14280134439468384\n",
      "Epoch 2497, Loss: 0.3498051017522812, Final Batch Loss: 0.1352335810661316\n",
      "Epoch 2498, Loss: 0.4267425090074539, Final Batch Loss: 0.22126366198062897\n",
      "Epoch 2499, Loss: 0.3716576248407364, Final Batch Loss: 0.16537874937057495\n",
      "Epoch 2500, Loss: 0.37561771273612976, Final Batch Loss: 0.13982251286506653\n",
      "Epoch 2501, Loss: 0.379568874835968, Final Batch Loss: 0.18398526310920715\n",
      "Epoch 2502, Loss: 0.3486082851886749, Final Batch Loss: 0.15294213593006134\n",
      "Epoch 2503, Loss: 0.35293978452682495, Final Batch Loss: 0.17118577659130096\n",
      "Epoch 2504, Loss: 0.30747782438993454, Final Batch Loss: 0.11944595724344254\n",
      "Epoch 2505, Loss: 0.3753815293312073, Final Batch Loss: 0.19881054759025574\n",
      "Epoch 2506, Loss: 0.39168961346149445, Final Batch Loss: 0.15142378211021423\n",
      "Epoch 2507, Loss: 0.32016830146312714, Final Batch Loss: 0.160380020737648\n",
      "Epoch 2508, Loss: 0.40501587092876434, Final Batch Loss: 0.20795388519763947\n",
      "Epoch 2509, Loss: 0.3151639997959137, Final Batch Loss: 0.16138018667697906\n",
      "Epoch 2510, Loss: 0.3499668389558792, Final Batch Loss: 0.16335314512252808\n",
      "Epoch 2511, Loss: 0.35592688620090485, Final Batch Loss: 0.19426080584526062\n",
      "Epoch 2512, Loss: 0.4579351991415024, Final Batch Loss: 0.26551946997642517\n",
      "Epoch 2513, Loss: 0.5704140663146973, Final Batch Loss: 0.3783820569515228\n",
      "Epoch 2514, Loss: 0.39241963624954224, Final Batch Loss: 0.23173333704471588\n",
      "Epoch 2515, Loss: 0.28858113288879395, Final Batch Loss: 0.11923512816429138\n",
      "Epoch 2516, Loss: 0.32651205360889435, Final Batch Loss: 0.16977669298648834\n",
      "Epoch 2517, Loss: 0.3422587066888809, Final Batch Loss: 0.1477961242198944\n",
      "Epoch 2518, Loss: 0.41875843703746796, Final Batch Loss: 0.2214776575565338\n",
      "Epoch 2519, Loss: 0.3779545873403549, Final Batch Loss: 0.18900525569915771\n",
      "Epoch 2520, Loss: 0.48218880593776703, Final Batch Loss: 0.25746798515319824\n",
      "Epoch 2521, Loss: 0.429885134100914, Final Batch Loss: 0.1948571801185608\n",
      "Epoch 2522, Loss: 0.417337104678154, Final Batch Loss: 0.2336679995059967\n",
      "Epoch 2523, Loss: 0.29954665899276733, Final Batch Loss: 0.16220328211784363\n",
      "Epoch 2524, Loss: 0.40114229917526245, Final Batch Loss: 0.20090243220329285\n",
      "Epoch 2525, Loss: 0.42465969920158386, Final Batch Loss: 0.2591179311275482\n",
      "Epoch 2526, Loss: 0.39473219215869904, Final Batch Loss: 0.1636846363544464\n",
      "Epoch 2527, Loss: 0.39604510366916656, Final Batch Loss: 0.19618113338947296\n",
      "Epoch 2528, Loss: 0.40438732504844666, Final Batch Loss: 0.21124176681041718\n",
      "Epoch 2529, Loss: 0.4102230817079544, Final Batch Loss: 0.230787456035614\n",
      "Epoch 2530, Loss: 0.3802158832550049, Final Batch Loss: 0.1960427612066269\n",
      "Epoch 2531, Loss: 0.36605705320835114, Final Batch Loss: 0.1776229292154312\n",
      "Epoch 2532, Loss: 0.35335516929626465, Final Batch Loss: 0.18042966723442078\n",
      "Epoch 2533, Loss: 0.40709246695041656, Final Batch Loss: 0.19752438366413116\n",
      "Epoch 2534, Loss: 0.35496582090854645, Final Batch Loss: 0.15985135734081268\n",
      "Epoch 2535, Loss: 0.3934609591960907, Final Batch Loss: 0.2229221910238266\n",
      "Epoch 2536, Loss: 0.4174905866384506, Final Batch Loss: 0.17900563776493073\n",
      "Epoch 2537, Loss: 0.31337694823741913, Final Batch Loss: 0.15024669468402863\n",
      "Epoch 2538, Loss: 0.35442259907722473, Final Batch Loss: 0.1537884622812271\n",
      "Epoch 2539, Loss: 0.44492393732070923, Final Batch Loss: 0.2810201644897461\n",
      "Epoch 2540, Loss: 0.37645667791366577, Final Batch Loss: 0.19302503764629364\n",
      "Epoch 2541, Loss: 0.357234463095665, Final Batch Loss: 0.21266746520996094\n",
      "Epoch 2542, Loss: 0.4020497053861618, Final Batch Loss: 0.21878455579280853\n",
      "Epoch 2543, Loss: 0.3623010665178299, Final Batch Loss: 0.14614704251289368\n",
      "Epoch 2544, Loss: 0.43564727902412415, Final Batch Loss: 0.24933812022209167\n",
      "Epoch 2545, Loss: 0.34557779133319855, Final Batch Loss: 0.20387156307697296\n",
      "Epoch 2546, Loss: 0.335518479347229, Final Batch Loss: 0.16176986694335938\n",
      "Epoch 2547, Loss: 0.33551472425460815, Final Batch Loss: 0.1661117672920227\n",
      "Epoch 2548, Loss: 0.35325872898101807, Final Batch Loss: 0.19878347218036652\n",
      "Epoch 2549, Loss: 0.37253129482269287, Final Batch Loss: 0.1459263116121292\n",
      "Epoch 2550, Loss: 0.479967400431633, Final Batch Loss: 0.29091161489486694\n",
      "Epoch 2551, Loss: 0.3965270519256592, Final Batch Loss: 0.21564213931560516\n",
      "Epoch 2552, Loss: 0.34091396629810333, Final Batch Loss: 0.15177977085113525\n",
      "Epoch 2553, Loss: 0.4380931109189987, Final Batch Loss: 0.22191739082336426\n",
      "Epoch 2554, Loss: 0.37600068747997284, Final Batch Loss: 0.20651374757289886\n",
      "Epoch 2555, Loss: 0.3654478043317795, Final Batch Loss: 0.17676140367984772\n",
      "Epoch 2556, Loss: 0.4278996139764786, Final Batch Loss: 0.19680435955524445\n",
      "Epoch 2557, Loss: 0.3571447730064392, Final Batch Loss: 0.19665127992630005\n",
      "Epoch 2558, Loss: 0.3455577790737152, Final Batch Loss: 0.16516682505607605\n",
      "Epoch 2559, Loss: 0.3816085308790207, Final Batch Loss: 0.17508119344711304\n",
      "Epoch 2560, Loss: 0.3650800287723541, Final Batch Loss: 0.1583770513534546\n",
      "Epoch 2561, Loss: 0.3585493862628937, Final Batch Loss: 0.20991010963916779\n",
      "Epoch 2562, Loss: 0.44987648725509644, Final Batch Loss: 0.2515912652015686\n",
      "Epoch 2563, Loss: 0.40739648044109344, Final Batch Loss: 0.20444351434707642\n",
      "Epoch 2564, Loss: 0.4285724312067032, Final Batch Loss: 0.19891928136348724\n",
      "Epoch 2565, Loss: 0.36400552093982697, Final Batch Loss: 0.18033379316329956\n",
      "Epoch 2566, Loss: 0.4442548304796219, Final Batch Loss: 0.21272851526737213\n",
      "Epoch 2567, Loss: 0.4115552604198456, Final Batch Loss: 0.22006657719612122\n",
      "Epoch 2568, Loss: 0.3606167435646057, Final Batch Loss: 0.16526982188224792\n",
      "Epoch 2569, Loss: 0.4186351150274277, Final Batch Loss: 0.22421704232692719\n",
      "Epoch 2570, Loss: 0.3460099846124649, Final Batch Loss: 0.15797823667526245\n",
      "Epoch 2571, Loss: 0.35027751326560974, Final Batch Loss: 0.16129857301712036\n",
      "Epoch 2572, Loss: 0.36977915465831757, Final Batch Loss: 0.18998239934444427\n",
      "Epoch 2573, Loss: 0.40949903428554535, Final Batch Loss: 0.21819353103637695\n",
      "Epoch 2574, Loss: 0.4407753050327301, Final Batch Loss: 0.2500006854534149\n",
      "Epoch 2575, Loss: 0.4261378347873688, Final Batch Loss: 0.24686875939369202\n",
      "Epoch 2576, Loss: 0.3785656541585922, Final Batch Loss: 0.1928240805864334\n",
      "Epoch 2577, Loss: 0.4573436975479126, Final Batch Loss: 0.25587591528892517\n",
      "Epoch 2578, Loss: 0.594524621963501, Final Batch Loss: 0.3987243175506592\n",
      "Epoch 2579, Loss: 0.34613272547721863, Final Batch Loss: 0.18755346536636353\n",
      "Epoch 2580, Loss: 0.41678065061569214, Final Batch Loss: 0.2148229330778122\n",
      "Epoch 2581, Loss: 0.4355749785900116, Final Batch Loss: 0.2064409703016281\n",
      "Epoch 2582, Loss: 0.3271840363740921, Final Batch Loss: 0.12780843675136566\n",
      "Epoch 2583, Loss: 0.35781703889369965, Final Batch Loss: 0.1923508495092392\n",
      "Epoch 2584, Loss: 0.5091565400362015, Final Batch Loss: 0.315897673368454\n",
      "Epoch 2585, Loss: 0.33731840550899506, Final Batch Loss: 0.14179900288581848\n",
      "Epoch 2586, Loss: 0.3632053732872009, Final Batch Loss: 0.1621738225221634\n",
      "Epoch 2587, Loss: 0.34972845017910004, Final Batch Loss: 0.19704607129096985\n",
      "Epoch 2588, Loss: 0.5535829365253448, Final Batch Loss: 0.3967832624912262\n",
      "Epoch 2589, Loss: 0.3248738497495651, Final Batch Loss: 0.14044161140918732\n",
      "Epoch 2590, Loss: 0.382002130150795, Final Batch Loss: 0.19534745812416077\n",
      "Epoch 2591, Loss: 0.30954359471797943, Final Batch Loss: 0.1538505107164383\n",
      "Epoch 2592, Loss: 0.44813401997089386, Final Batch Loss: 0.20885241031646729\n",
      "Epoch 2593, Loss: 0.3788920193910599, Final Batch Loss: 0.19975967705249786\n",
      "Epoch 2594, Loss: 0.36340929567813873, Final Batch Loss: 0.21061380207538605\n",
      "Epoch 2595, Loss: 0.3757815659046173, Final Batch Loss: 0.1888091266155243\n",
      "Epoch 2596, Loss: 0.38075239956378937, Final Batch Loss: 0.16433046758174896\n",
      "Epoch 2597, Loss: 0.40602871775627136, Final Batch Loss: 0.2214457243680954\n",
      "Epoch 2598, Loss: 0.4372653067111969, Final Batch Loss: 0.2611927390098572\n",
      "Epoch 2599, Loss: 0.3739013522863388, Final Batch Loss: 0.17285561561584473\n",
      "Epoch 2600, Loss: 0.3421352654695511, Final Batch Loss: 0.14749786257743835\n",
      "Epoch 2601, Loss: 0.3606972247362137, Final Batch Loss: 0.18717913329601288\n",
      "Epoch 2602, Loss: 0.5037571340799332, Final Batch Loss: 0.22462056577205658\n",
      "Epoch 2603, Loss: 0.4162262976169586, Final Batch Loss: 0.21279412508010864\n",
      "Epoch 2604, Loss: 0.3844812661409378, Final Batch Loss: 0.19636911153793335\n",
      "Epoch 2605, Loss: 0.3509739488363266, Final Batch Loss: 0.1733870804309845\n",
      "Epoch 2606, Loss: 0.39408911764621735, Final Batch Loss: 0.20208722352981567\n",
      "Epoch 2607, Loss: 0.3643556982278824, Final Batch Loss: 0.17651331424713135\n",
      "Epoch 2608, Loss: 0.3850433677434921, Final Batch Loss: 0.21012091636657715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2609, Loss: 0.3598521649837494, Final Batch Loss: 0.19320112466812134\n",
      "Epoch 2610, Loss: 0.3747628927230835, Final Batch Loss: 0.20037153363227844\n",
      "Epoch 2611, Loss: 0.3070480078458786, Final Batch Loss: 0.13089625537395477\n",
      "Epoch 2612, Loss: 0.40364716947078705, Final Batch Loss: 0.19860732555389404\n",
      "Epoch 2613, Loss: 0.37924981862306595, Final Batch Loss: 0.11096606403589249\n",
      "Epoch 2614, Loss: 0.3832479864358902, Final Batch Loss: 0.1956780105829239\n",
      "Epoch 2615, Loss: 0.3142331540584564, Final Batch Loss: 0.14845475554466248\n",
      "Epoch 2616, Loss: 0.46411360800266266, Final Batch Loss: 0.2711963355541229\n",
      "Epoch 2617, Loss: 0.36261774599552155, Final Batch Loss: 0.1775381714105606\n",
      "Epoch 2618, Loss: 0.37501972913742065, Final Batch Loss: 0.19648276269435883\n",
      "Epoch 2619, Loss: 0.3501347005367279, Final Batch Loss: 0.1778772920370102\n",
      "Epoch 2620, Loss: 0.3788587301969528, Final Batch Loss: 0.18632517755031586\n",
      "Epoch 2621, Loss: 0.41768184304237366, Final Batch Loss: 0.24254949390888214\n",
      "Epoch 2622, Loss: 0.2618776932358742, Final Batch Loss: 0.13800367712974548\n",
      "Epoch 2623, Loss: 0.36654119193553925, Final Batch Loss: 0.14482514560222626\n",
      "Epoch 2624, Loss: 0.4298431873321533, Final Batch Loss: 0.1951003074645996\n",
      "Epoch 2625, Loss: 0.4108784347772598, Final Batch Loss: 0.20664183795452118\n",
      "Epoch 2626, Loss: 0.4023302048444748, Final Batch Loss: 0.23119120299816132\n",
      "Epoch 2627, Loss: 0.3822995126247406, Final Batch Loss: 0.19335432350635529\n",
      "Epoch 2628, Loss: 0.3650560528039932, Final Batch Loss: 0.18806388974189758\n",
      "Epoch 2629, Loss: 0.42211100459098816, Final Batch Loss: 0.2287599742412567\n",
      "Epoch 2630, Loss: 0.3663344532251358, Final Batch Loss: 0.18072091042995453\n",
      "Epoch 2631, Loss: 0.36570796370506287, Final Batch Loss: 0.20221756398677826\n",
      "Epoch 2632, Loss: 0.4020937383174896, Final Batch Loss: 0.21808792650699615\n",
      "Epoch 2633, Loss: 0.35420480370521545, Final Batch Loss: 0.17277969419956207\n",
      "Epoch 2634, Loss: 0.382070392370224, Final Batch Loss: 0.19385308027267456\n",
      "Epoch 2635, Loss: 0.42336784303188324, Final Batch Loss: 0.24199536442756653\n",
      "Epoch 2636, Loss: 0.33157438039779663, Final Batch Loss: 0.15711645781993866\n",
      "Epoch 2637, Loss: 0.3591098189353943, Final Batch Loss: 0.2002558708190918\n",
      "Epoch 2638, Loss: 0.42460300028324127, Final Batch Loss: 0.23969204723834991\n",
      "Epoch 2639, Loss: 0.3364049345254898, Final Batch Loss: 0.13136960566043854\n",
      "Epoch 2640, Loss: 0.329908549785614, Final Batch Loss: 0.15743163228034973\n",
      "Epoch 2641, Loss: 0.3610944449901581, Final Batch Loss: 0.1635769009590149\n",
      "Epoch 2642, Loss: 0.3567010760307312, Final Batch Loss: 0.15251965820789337\n",
      "Epoch 2643, Loss: 0.36673523485660553, Final Batch Loss: 0.18890878558158875\n",
      "Epoch 2644, Loss: 0.3827119320631027, Final Batch Loss: 0.20672520995140076\n",
      "Epoch 2645, Loss: 0.3731798380613327, Final Batch Loss: 0.18451853096485138\n",
      "Epoch 2646, Loss: 0.40802644193172455, Final Batch Loss: 0.24830833077430725\n",
      "Epoch 2647, Loss: 0.3380945026874542, Final Batch Loss: 0.1409820020198822\n",
      "Epoch 2648, Loss: 0.31293678283691406, Final Batch Loss: 0.16137689352035522\n",
      "Epoch 2649, Loss: 0.38390228152275085, Final Batch Loss: 0.16907936334609985\n",
      "Epoch 2650, Loss: 0.5751667320728302, Final Batch Loss: 0.36409834027290344\n",
      "Epoch 2651, Loss: 0.4990256577730179, Final Batch Loss: 0.33548855781555176\n",
      "Epoch 2652, Loss: 0.43024881184101105, Final Batch Loss: 0.21116632223129272\n",
      "Epoch 2653, Loss: 0.36327484250068665, Final Batch Loss: 0.18004582822322845\n",
      "Epoch 2654, Loss: 0.357286661863327, Final Batch Loss: 0.156877800822258\n",
      "Epoch 2655, Loss: 0.34927043318748474, Final Batch Loss: 0.1892460584640503\n",
      "Epoch 2656, Loss: 0.3357377350330353, Final Batch Loss: 0.13925692439079285\n",
      "Epoch 2657, Loss: 0.35745543241500854, Final Batch Loss: 0.16137276589870453\n",
      "Epoch 2658, Loss: 0.32611240446567535, Final Batch Loss: 0.15653321146965027\n",
      "Epoch 2659, Loss: 0.4424872100353241, Final Batch Loss: 0.2436743825674057\n",
      "Epoch 2660, Loss: 0.3075511306524277, Final Batch Loss: 0.13754163682460785\n",
      "Epoch 2661, Loss: 0.43900996446609497, Final Batch Loss: 0.24538293480873108\n",
      "Epoch 2662, Loss: 0.33271291851997375, Final Batch Loss: 0.17757010459899902\n",
      "Epoch 2663, Loss: 0.361773282289505, Final Batch Loss: 0.1965251863002777\n",
      "Epoch 2664, Loss: 0.37619854509830475, Final Batch Loss: 0.18000127375125885\n",
      "Epoch 2665, Loss: 0.36383409798145294, Final Batch Loss: 0.18799279630184174\n",
      "Epoch 2666, Loss: 0.3450087755918503, Final Batch Loss: 0.2027735710144043\n",
      "Epoch 2667, Loss: 0.4053616523742676, Final Batch Loss: 0.1404828131198883\n",
      "Epoch 2668, Loss: 0.370831623673439, Final Batch Loss: 0.18190091848373413\n",
      "Epoch 2669, Loss: 0.38178619742393494, Final Batch Loss: 0.1885194629430771\n",
      "Epoch 2670, Loss: 0.32385003566741943, Final Batch Loss: 0.17992745339870453\n",
      "Epoch 2671, Loss: 0.36535583436489105, Final Batch Loss: 0.1884385198354721\n",
      "Epoch 2672, Loss: 0.3190726190805435, Final Batch Loss: 0.1332577019929886\n",
      "Epoch 2673, Loss: 0.3910396844148636, Final Batch Loss: 0.22042012214660645\n",
      "Epoch 2674, Loss: 0.35034413635730743, Final Batch Loss: 0.1741085946559906\n",
      "Epoch 2675, Loss: 0.33052997291088104, Final Batch Loss: 0.1548619419336319\n",
      "Epoch 2676, Loss: 0.3049731180071831, Final Batch Loss: 0.11677321046590805\n",
      "Epoch 2677, Loss: 0.4008760154247284, Final Batch Loss: 0.20164327323436737\n",
      "Epoch 2678, Loss: 0.33535414934158325, Final Batch Loss: 0.1549399346113205\n",
      "Epoch 2679, Loss: 0.3089674264192581, Final Batch Loss: 0.13676661252975464\n",
      "Epoch 2680, Loss: 0.33262799680233, Final Batch Loss: 0.15823903679847717\n",
      "Epoch 2681, Loss: 0.3522738814353943, Final Batch Loss: 0.18083615601062775\n",
      "Epoch 2682, Loss: 0.3673850819468498, Final Batch Loss: 0.10523929446935654\n",
      "Epoch 2683, Loss: 0.3488212078809738, Final Batch Loss: 0.1803523302078247\n",
      "Epoch 2684, Loss: 0.3806173801422119, Final Batch Loss: 0.22968725860118866\n",
      "Epoch 2685, Loss: 0.38643449544906616, Final Batch Loss: 0.20368362963199615\n",
      "Epoch 2686, Loss: 0.3503172695636749, Final Batch Loss: 0.13501131534576416\n",
      "Epoch 2687, Loss: 0.3562380224466324, Final Batch Loss: 0.15559080243110657\n",
      "Epoch 2688, Loss: 0.37876294553279877, Final Batch Loss: 0.19635392725467682\n",
      "Epoch 2689, Loss: 0.36611685156822205, Final Batch Loss: 0.18551898002624512\n",
      "Epoch 2690, Loss: 0.29255402088165283, Final Batch Loss: 0.12981827557086945\n",
      "Epoch 2691, Loss: 0.27771181613206863, Final Batch Loss: 0.10987930744886398\n",
      "Epoch 2692, Loss: 0.43010658025741577, Final Batch Loss: 0.25265756249427795\n",
      "Epoch 2693, Loss: 0.37106239795684814, Final Batch Loss: 0.2330615073442459\n",
      "Epoch 2694, Loss: 0.35377615690231323, Final Batch Loss: 0.1893545538187027\n",
      "Epoch 2695, Loss: 0.3306746482849121, Final Batch Loss: 0.15382914245128632\n",
      "Epoch 2696, Loss: 0.41083110868930817, Final Batch Loss: 0.22202835977077484\n",
      "Epoch 2697, Loss: 0.39981725811958313, Final Batch Loss: 0.18770478665828705\n",
      "Epoch 2698, Loss: 0.2917022258043289, Final Batch Loss: 0.12476974725723267\n",
      "Epoch 2699, Loss: 0.4059271663427353, Final Batch Loss: 0.2113598734140396\n",
      "Epoch 2700, Loss: 0.328310951590538, Final Batch Loss: 0.168378084897995\n",
      "Epoch 2701, Loss: 0.4830321818590164, Final Batch Loss: 0.2865040600299835\n",
      "Epoch 2702, Loss: 0.3927666246891022, Final Batch Loss: 0.23883387446403503\n",
      "Epoch 2703, Loss: 0.3252483606338501, Final Batch Loss: 0.16284172236919403\n",
      "Epoch 2704, Loss: 0.42905011773109436, Final Batch Loss: 0.18202194571495056\n",
      "Epoch 2705, Loss: 0.32632309198379517, Final Batch Loss: 0.1423594206571579\n",
      "Epoch 2706, Loss: 0.41262589395046234, Final Batch Loss: 0.1909046322107315\n",
      "Epoch 2707, Loss: 0.37246064841747284, Final Batch Loss: 0.21541999280452728\n",
      "Epoch 2708, Loss: 0.4402497559785843, Final Batch Loss: 0.2536548972129822\n",
      "Epoch 2709, Loss: 0.38685761392116547, Final Batch Loss: 0.22803187370300293\n",
      "Epoch 2710, Loss: 0.47527511417865753, Final Batch Loss: 0.3338122069835663\n",
      "Epoch 2711, Loss: 0.34631432592868805, Final Batch Loss: 0.1417054831981659\n",
      "Epoch 2712, Loss: 0.3110102117061615, Final Batch Loss: 0.11512751877307892\n",
      "Epoch 2713, Loss: 0.3811107575893402, Final Batch Loss: 0.19448277354240417\n",
      "Epoch 2714, Loss: 0.4068820923566818, Final Batch Loss: 0.20636360347270966\n",
      "Epoch 2715, Loss: 0.35182228684425354, Final Batch Loss: 0.17746450006961823\n",
      "Epoch 2716, Loss: 0.3783939927816391, Final Batch Loss: 0.1585221141576767\n",
      "Epoch 2717, Loss: 0.3938635587692261, Final Batch Loss: 0.2151532769203186\n",
      "Epoch 2718, Loss: 0.34579458832740784, Final Batch Loss: 0.18869362771511078\n",
      "Epoch 2719, Loss: 0.43110282719135284, Final Batch Loss: 0.23181486129760742\n",
      "Epoch 2720, Loss: 0.3827490955591202, Final Batch Loss: 0.2077726125717163\n",
      "Epoch 2721, Loss: 0.32840827107429504, Final Batch Loss: 0.15443140268325806\n",
      "Epoch 2722, Loss: 0.39366455376148224, Final Batch Loss: 0.20536962151527405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2723, Loss: 0.33336275815963745, Final Batch Loss: 0.17659157514572144\n",
      "Epoch 2724, Loss: 0.2961992472410202, Final Batch Loss: 0.14322414994239807\n",
      "Epoch 2725, Loss: 0.3086751848459244, Final Batch Loss: 0.14539341628551483\n",
      "Epoch 2726, Loss: 0.3216094821691513, Final Batch Loss: 0.11960934102535248\n",
      "Epoch 2727, Loss: 0.3541140705347061, Final Batch Loss: 0.20081816613674164\n",
      "Epoch 2728, Loss: 0.33776089549064636, Final Batch Loss: 0.1633281707763672\n",
      "Epoch 2729, Loss: 0.38643187284469604, Final Batch Loss: 0.20916856825351715\n",
      "Epoch 2730, Loss: 0.334902286529541, Final Batch Loss: 0.15509909391403198\n",
      "Epoch 2731, Loss: 0.2946070432662964, Final Batch Loss: 0.11761265993118286\n",
      "Epoch 2732, Loss: 0.3103068917989731, Final Batch Loss: 0.13925398886203766\n",
      "Epoch 2733, Loss: 0.3327038139104843, Final Batch Loss: 0.1331532895565033\n",
      "Epoch 2734, Loss: 0.29164785146713257, Final Batch Loss: 0.16423377394676208\n",
      "Epoch 2735, Loss: 0.37070250511169434, Final Batch Loss: 0.14501211047172546\n",
      "Epoch 2736, Loss: 0.3699033260345459, Final Batch Loss: 0.22388389706611633\n",
      "Epoch 2737, Loss: 0.4672561287879944, Final Batch Loss: 0.2873571515083313\n",
      "Epoch 2738, Loss: 0.3879721015691757, Final Batch Loss: 0.2091924101114273\n",
      "Epoch 2739, Loss: 0.2963840365409851, Final Batch Loss: 0.13977010548114777\n",
      "Epoch 2740, Loss: 0.3906962275505066, Final Batch Loss: 0.15591055154800415\n",
      "Epoch 2741, Loss: 0.3554944843053818, Final Batch Loss: 0.18225157260894775\n",
      "Epoch 2742, Loss: 0.33297835290431976, Final Batch Loss: 0.14335034787654877\n",
      "Epoch 2743, Loss: 0.2841425761580467, Final Batch Loss: 0.11929046362638474\n",
      "Epoch 2744, Loss: 0.3404199630022049, Final Batch Loss: 0.19623662531375885\n",
      "Epoch 2745, Loss: 0.36056937277317047, Final Batch Loss: 0.18600144982337952\n",
      "Epoch 2746, Loss: 0.4819185733795166, Final Batch Loss: 0.29394012689590454\n",
      "Epoch 2747, Loss: 0.378946989774704, Final Batch Loss: 0.21278725564479828\n",
      "Epoch 2748, Loss: 0.3650455176830292, Final Batch Loss: 0.2148609757423401\n",
      "Epoch 2749, Loss: 0.32503485679626465, Final Batch Loss: 0.15122437477111816\n",
      "Epoch 2750, Loss: 0.38092076778411865, Final Batch Loss: 0.2192268669605255\n",
      "Epoch 2751, Loss: 0.38655081391334534, Final Batch Loss: 0.22217175364494324\n",
      "Epoch 2752, Loss: 0.31936977803707123, Final Batch Loss: 0.1670134961605072\n",
      "Epoch 2753, Loss: 0.34767569601535797, Final Batch Loss: 0.1555582582950592\n",
      "Epoch 2754, Loss: 0.3383907228708267, Final Batch Loss: 0.17338180541992188\n",
      "Epoch 2755, Loss: 0.3581119328737259, Final Batch Loss: 0.1918572038412094\n",
      "Epoch 2756, Loss: 0.3153516501188278, Final Batch Loss: 0.16672274470329285\n",
      "Epoch 2757, Loss: 0.373417928814888, Final Batch Loss: 0.18395085632801056\n",
      "Epoch 2758, Loss: 0.3715042620897293, Final Batch Loss: 0.1696120649576187\n",
      "Epoch 2759, Loss: 0.35980580747127533, Final Batch Loss: 0.15951354801654816\n",
      "Epoch 2760, Loss: 0.40678802132606506, Final Batch Loss: 0.1998492330312729\n",
      "Epoch 2761, Loss: 0.31967008113861084, Final Batch Loss: 0.159099742770195\n",
      "Epoch 2762, Loss: 0.47911977767944336, Final Batch Loss: 0.28489547967910767\n",
      "Epoch 2763, Loss: 0.32144737243652344, Final Batch Loss: 0.15104712545871735\n",
      "Epoch 2764, Loss: 0.3663824051618576, Final Batch Loss: 0.18892802298069\n",
      "Epoch 2765, Loss: 0.44982439279556274, Final Batch Loss: 0.2604331374168396\n",
      "Epoch 2766, Loss: 0.3558691442012787, Final Batch Loss: 0.1964767724275589\n",
      "Epoch 2767, Loss: 0.3061218410730362, Final Batch Loss: 0.1605486422777176\n",
      "Epoch 2768, Loss: 0.29446941614151, Final Batch Loss: 0.12829281389713287\n",
      "Epoch 2769, Loss: 0.3605606257915497, Final Batch Loss: 0.17556433379650116\n",
      "Epoch 2770, Loss: 0.34582722187042236, Final Batch Loss: 0.16178949177265167\n",
      "Epoch 2771, Loss: 0.36116354167461395, Final Batch Loss: 0.16316430270671844\n",
      "Epoch 2772, Loss: 0.33703726530075073, Final Batch Loss: 0.2053985446691513\n",
      "Epoch 2773, Loss: 0.37496963143348694, Final Batch Loss: 0.1476355940103531\n",
      "Epoch 2774, Loss: 0.4346674233675003, Final Batch Loss: 0.27306848764419556\n",
      "Epoch 2775, Loss: 0.34912844002246857, Final Batch Loss: 0.16534174978733063\n",
      "Epoch 2776, Loss: 0.3160855919122696, Final Batch Loss: 0.1435733437538147\n",
      "Epoch 2777, Loss: 0.3769492506980896, Final Batch Loss: 0.2050202190876007\n",
      "Epoch 2778, Loss: 0.34825389087200165, Final Batch Loss: 0.17616388201713562\n",
      "Epoch 2779, Loss: 0.2897510975599289, Final Batch Loss: 0.14164353907108307\n",
      "Epoch 2780, Loss: 0.27663692831993103, Final Batch Loss: 0.12774890661239624\n",
      "Epoch 2781, Loss: 0.4925539940595627, Final Batch Loss: 0.22746191918849945\n",
      "Epoch 2782, Loss: 0.4016685485839844, Final Batch Loss: 0.25585561990737915\n",
      "Epoch 2783, Loss: 0.35411736369132996, Final Batch Loss: 0.16219329833984375\n",
      "Epoch 2784, Loss: 0.3620329350233078, Final Batch Loss: 0.19000272452831268\n",
      "Epoch 2785, Loss: 0.4391528517007828, Final Batch Loss: 0.21224379539489746\n",
      "Epoch 2786, Loss: 0.3452591896057129, Final Batch Loss: 0.17430941760540009\n",
      "Epoch 2787, Loss: 0.3746694475412369, Final Batch Loss: 0.19593247771263123\n",
      "Epoch 2788, Loss: 0.36490100622177124, Final Batch Loss: 0.1883179396390915\n",
      "Epoch 2789, Loss: 0.287490651011467, Final Batch Loss: 0.0956130176782608\n",
      "Epoch 2790, Loss: 0.39618130028247833, Final Batch Loss: 0.19422249495983124\n",
      "Epoch 2791, Loss: 0.3452734053134918, Final Batch Loss: 0.1583331823348999\n",
      "Epoch 2792, Loss: 0.437348335981369, Final Batch Loss: 0.26476117968559265\n",
      "Epoch 2793, Loss: 0.3612649291753769, Final Batch Loss: 0.18159104883670807\n",
      "Epoch 2794, Loss: 0.291642963886261, Final Batch Loss: 0.13179437816143036\n",
      "Epoch 2795, Loss: 0.4336256980895996, Final Batch Loss: 0.27281177043914795\n",
      "Epoch 2796, Loss: 0.37707145512104034, Final Batch Loss: 0.2152499556541443\n",
      "Epoch 2797, Loss: 0.32585856318473816, Final Batch Loss: 0.13319118320941925\n",
      "Epoch 2798, Loss: 0.3882303833961487, Final Batch Loss: 0.1804727166891098\n",
      "Epoch 2799, Loss: 0.3153431862592697, Final Batch Loss: 0.14364656805992126\n",
      "Epoch 2800, Loss: 0.31519360840320587, Final Batch Loss: 0.16284754872322083\n",
      "Epoch 2801, Loss: 0.33522237837314606, Final Batch Loss: 0.18303170800209045\n",
      "Epoch 2802, Loss: 0.3208094835281372, Final Batch Loss: 0.16232329607009888\n",
      "Epoch 2803, Loss: 0.3548903465270996, Final Batch Loss: 0.21119989454746246\n",
      "Epoch 2804, Loss: 0.32428497076034546, Final Batch Loss: 0.1599448323249817\n",
      "Epoch 2805, Loss: 0.2915240079164505, Final Batch Loss: 0.14225420355796814\n",
      "Epoch 2806, Loss: 0.31454044580459595, Final Batch Loss: 0.1669468730688095\n",
      "Epoch 2807, Loss: 0.35573340952396393, Final Batch Loss: 0.17222978174686432\n",
      "Epoch 2808, Loss: 0.32480569183826447, Final Batch Loss: 0.12713679671287537\n",
      "Epoch 2809, Loss: 0.3514827787876129, Final Batch Loss: 0.19636240601539612\n",
      "Epoch 2810, Loss: 0.2589178532361984, Final Batch Loss: 0.12481242418289185\n",
      "Epoch 2811, Loss: 0.4115707129240036, Final Batch Loss: 0.15014304220676422\n",
      "Epoch 2812, Loss: 0.38071420788764954, Final Batch Loss: 0.2087252140045166\n",
      "Epoch 2813, Loss: 0.34667107462882996, Final Batch Loss: 0.17690055072307587\n",
      "Epoch 2814, Loss: 0.30259403586387634, Final Batch Loss: 0.13436785340309143\n",
      "Epoch 2815, Loss: 0.3845760375261307, Final Batch Loss: 0.16697090864181519\n",
      "Epoch 2816, Loss: 0.34182755649089813, Final Batch Loss: 0.16609573364257812\n",
      "Epoch 2817, Loss: 0.34242260456085205, Final Batch Loss: 0.20300796627998352\n",
      "Epoch 2818, Loss: 0.3640950918197632, Final Batch Loss: 0.18809840083122253\n",
      "Epoch 2819, Loss: 0.30901719629764557, Final Batch Loss: 0.1294216513633728\n",
      "Epoch 2820, Loss: 0.3537725657224655, Final Batch Loss: 0.22539891302585602\n",
      "Epoch 2821, Loss: 0.3349148631095886, Final Batch Loss: 0.16583245992660522\n",
      "Epoch 2822, Loss: 0.3141036480665207, Final Batch Loss: 0.15433713793754578\n",
      "Epoch 2823, Loss: 0.3376200497150421, Final Batch Loss: 0.18921098113059998\n",
      "Epoch 2824, Loss: 0.37438271939754486, Final Batch Loss: 0.21487008035182953\n",
      "Epoch 2825, Loss: 0.29409198462963104, Final Batch Loss: 0.13728591799736023\n",
      "Epoch 2826, Loss: 0.3105926066637039, Final Batch Loss: 0.13339659571647644\n",
      "Epoch 2827, Loss: 0.28297488391399384, Final Batch Loss: 0.12602445483207703\n",
      "Epoch 2828, Loss: 0.36920174956321716, Final Batch Loss: 0.17307084798812866\n",
      "Epoch 2829, Loss: 0.37568728625774384, Final Batch Loss: 0.17276713252067566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2830, Loss: 0.3629082888364792, Final Batch Loss: 0.13390058279037476\n",
      "Epoch 2831, Loss: 0.48459750413894653, Final Batch Loss: 0.22789734601974487\n",
      "Epoch 2832, Loss: 0.36480294167995453, Final Batch Loss: 0.1907215714454651\n",
      "Epoch 2833, Loss: 0.3494306951761246, Final Batch Loss: 0.13719503581523895\n",
      "Epoch 2834, Loss: 0.3160753399133682, Final Batch Loss: 0.13873708248138428\n",
      "Epoch 2835, Loss: 0.3060935139656067, Final Batch Loss: 0.12525227665901184\n",
      "Epoch 2836, Loss: 0.40157149732112885, Final Batch Loss: 0.2146652191877365\n",
      "Epoch 2837, Loss: 0.3645551800727844, Final Batch Loss: 0.20544199645519257\n",
      "Epoch 2838, Loss: 0.3488660156726837, Final Batch Loss: 0.19493652880191803\n",
      "Epoch 2839, Loss: 0.29194168746471405, Final Batch Loss: 0.11353398859500885\n",
      "Epoch 2840, Loss: 0.3047681599855423, Final Batch Loss: 0.12927977740764618\n",
      "Epoch 2841, Loss: 0.3984362781047821, Final Batch Loss: 0.19868262112140656\n",
      "Epoch 2842, Loss: 0.37975823879241943, Final Batch Loss: 0.2017606645822525\n",
      "Epoch 2843, Loss: 0.3402116000652313, Final Batch Loss: 0.14871470630168915\n",
      "Epoch 2844, Loss: 0.306356817483902, Final Batch Loss: 0.18001578748226166\n",
      "Epoch 2845, Loss: 0.28381167352199554, Final Batch Loss: 0.13527953624725342\n",
      "Epoch 2846, Loss: 0.4255295991897583, Final Batch Loss: 0.20655830204486847\n",
      "Epoch 2847, Loss: 0.3516656309366226, Final Batch Loss: 0.1850292682647705\n",
      "Epoch 2848, Loss: 0.4665609151124954, Final Batch Loss: 0.2205072045326233\n",
      "Epoch 2849, Loss: 0.31872275471687317, Final Batch Loss: 0.15998323261737823\n",
      "Epoch 2850, Loss: 0.348450243473053, Final Batch Loss: 0.1623096466064453\n",
      "Epoch 2851, Loss: 0.31808166205883026, Final Batch Loss: 0.1557675153017044\n",
      "Epoch 2852, Loss: 0.43248143792152405, Final Batch Loss: 0.2124149054288864\n",
      "Epoch 2853, Loss: 0.3857373595237732, Final Batch Loss: 0.2018941193819046\n",
      "Epoch 2854, Loss: 0.32951124012470245, Final Batch Loss: 0.1546584963798523\n",
      "Epoch 2855, Loss: 0.33809205889701843, Final Batch Loss: 0.19950129091739655\n",
      "Epoch 2856, Loss: 0.3466949164867401, Final Batch Loss: 0.19669771194458008\n",
      "Epoch 2857, Loss: 0.38083814084529877, Final Batch Loss: 0.234590083360672\n",
      "Epoch 2858, Loss: 0.3230220675468445, Final Batch Loss: 0.13408547639846802\n",
      "Epoch 2859, Loss: 0.3023899495601654, Final Batch Loss: 0.1414935141801834\n",
      "Epoch 2860, Loss: 0.3418916314840317, Final Batch Loss: 0.19924676418304443\n",
      "Epoch 2861, Loss: 0.30665963888168335, Final Batch Loss: 0.15090814232826233\n",
      "Epoch 2862, Loss: 0.3604782670736313, Final Batch Loss: 0.2218773066997528\n",
      "Epoch 2863, Loss: 0.4469650685787201, Final Batch Loss: 0.3042294681072235\n",
      "Epoch 2864, Loss: 0.379947230219841, Final Batch Loss: 0.17932304739952087\n",
      "Epoch 2865, Loss: 0.4060102254152298, Final Batch Loss: 0.26159927248954773\n",
      "Epoch 2866, Loss: 0.28515858948230743, Final Batch Loss: 0.12202422320842743\n",
      "Epoch 2867, Loss: 0.38613130152225494, Final Batch Loss: 0.18835733830928802\n",
      "Epoch 2868, Loss: 0.38350367546081543, Final Batch Loss: 0.165442556142807\n",
      "Epoch 2869, Loss: 0.3870151489973068, Final Batch Loss: 0.18784277141094208\n",
      "Epoch 2870, Loss: 0.2959897667169571, Final Batch Loss: 0.1701595038175583\n",
      "Epoch 2871, Loss: 0.33900555968284607, Final Batch Loss: 0.15636886656284332\n",
      "Epoch 2872, Loss: 0.32574377954006195, Final Batch Loss: 0.19239410758018494\n",
      "Epoch 2873, Loss: 0.3885805159807205, Final Batch Loss: 0.1952807456254959\n",
      "Epoch 2874, Loss: 0.3224946856498718, Final Batch Loss: 0.16779738664627075\n",
      "Epoch 2875, Loss: 0.35682816803455353, Final Batch Loss: 0.20546691119670868\n",
      "Epoch 2876, Loss: 0.326265849173069, Final Batch Loss: 0.11155729740858078\n",
      "Epoch 2877, Loss: 0.28463294357061386, Final Batch Loss: 0.11641926318407059\n",
      "Epoch 2878, Loss: 0.37308360636234283, Final Batch Loss: 0.18594706058502197\n",
      "Epoch 2879, Loss: 0.3382529318332672, Final Batch Loss: 0.12934227287769318\n",
      "Epoch 2880, Loss: 0.33912375569343567, Final Batch Loss: 0.16204947233200073\n",
      "Epoch 2881, Loss: 0.3314741849899292, Final Batch Loss: 0.20363861322402954\n",
      "Epoch 2882, Loss: 0.4001091718673706, Final Batch Loss: 0.15664583444595337\n",
      "Epoch 2883, Loss: 0.3470013439655304, Final Batch Loss: 0.1492319107055664\n",
      "Epoch 2884, Loss: 0.36554718017578125, Final Batch Loss: 0.1605609804391861\n",
      "Epoch 2885, Loss: 0.38649530708789825, Final Batch Loss: 0.17045485973358154\n",
      "Epoch 2886, Loss: 0.3788505047559738, Final Batch Loss: 0.1569289118051529\n",
      "Epoch 2887, Loss: 0.3268980085849762, Final Batch Loss: 0.14481627941131592\n",
      "Epoch 2888, Loss: 0.3256080746650696, Final Batch Loss: 0.14375664293766022\n",
      "Epoch 2889, Loss: 0.31179599463939667, Final Batch Loss: 0.15949179232120514\n",
      "Epoch 2890, Loss: 0.3542984127998352, Final Batch Loss: 0.20585580170154572\n",
      "Epoch 2891, Loss: 0.3623061925172806, Final Batch Loss: 0.1468386948108673\n",
      "Epoch 2892, Loss: 0.3768833428621292, Final Batch Loss: 0.2126564234495163\n",
      "Epoch 2893, Loss: 0.36325159668922424, Final Batch Loss: 0.21519368886947632\n",
      "Epoch 2894, Loss: 0.3396466374397278, Final Batch Loss: 0.1661931425333023\n",
      "Epoch 2895, Loss: 0.2627391964197159, Final Batch Loss: 0.08958736062049866\n",
      "Epoch 2896, Loss: 0.3270980268716812, Final Batch Loss: 0.14470341801643372\n",
      "Epoch 2897, Loss: 0.3461079001426697, Final Batch Loss: 0.18739645183086395\n",
      "Epoch 2898, Loss: 0.34744498133659363, Final Batch Loss: 0.1722387671470642\n",
      "Epoch 2899, Loss: 0.28300096094608307, Final Batch Loss: 0.1119503527879715\n",
      "Epoch 2900, Loss: 0.5238315314054489, Final Batch Loss: 0.37411046028137207\n",
      "Epoch 2901, Loss: 0.34797099232673645, Final Batch Loss: 0.1559210866689682\n",
      "Epoch 2902, Loss: 0.30895794928073883, Final Batch Loss: 0.1582472324371338\n",
      "Epoch 2903, Loss: 0.2930877357721329, Final Batch Loss: 0.14708459377288818\n",
      "Epoch 2904, Loss: 0.3147660195827484, Final Batch Loss: 0.17881055176258087\n",
      "Epoch 2905, Loss: 0.28881193697452545, Final Batch Loss: 0.140144944190979\n",
      "Epoch 2906, Loss: 0.3782346248626709, Final Batch Loss: 0.2144014984369278\n",
      "Epoch 2907, Loss: 0.26921308785676956, Final Batch Loss: 0.10703472048044205\n",
      "Epoch 2908, Loss: 0.2876081317663193, Final Batch Loss: 0.1292484551668167\n",
      "Epoch 2909, Loss: 0.324423149228096, Final Batch Loss: 0.16837728023529053\n",
      "Epoch 2910, Loss: 0.37185968458652496, Final Batch Loss: 0.226030632853508\n",
      "Epoch 2911, Loss: 0.4036937355995178, Final Batch Loss: 0.1829688549041748\n",
      "Epoch 2912, Loss: 0.329879492521286, Final Batch Loss: 0.20264112949371338\n",
      "Epoch 2913, Loss: 0.3035791516304016, Final Batch Loss: 0.13073568046092987\n",
      "Epoch 2914, Loss: 0.3039935678243637, Final Batch Loss: 0.1495848000049591\n",
      "Epoch 2915, Loss: 0.259989358484745, Final Batch Loss: 0.09821083396673203\n",
      "Epoch 2916, Loss: 0.39258426427841187, Final Batch Loss: 0.19566583633422852\n",
      "Epoch 2917, Loss: 0.27116039395332336, Final Batch Loss: 0.09631983935832977\n",
      "Epoch 2918, Loss: 0.3283032178878784, Final Batch Loss: 0.15216025710105896\n",
      "Epoch 2919, Loss: 0.3518378287553787, Final Batch Loss: 0.19281433522701263\n",
      "Epoch 2920, Loss: 0.4515603482723236, Final Batch Loss: 0.2781296968460083\n",
      "Epoch 2921, Loss: 0.3426911234855652, Final Batch Loss: 0.14117738604545593\n",
      "Epoch 2922, Loss: 0.2942696213722229, Final Batch Loss: 0.14391162991523743\n",
      "Epoch 2923, Loss: 0.387238085269928, Final Batch Loss: 0.20898254215717316\n",
      "Epoch 2924, Loss: 0.3715677708387375, Final Batch Loss: 0.16663098335266113\n",
      "Epoch 2925, Loss: 0.27997034043073654, Final Batch Loss: 0.1059219017624855\n",
      "Epoch 2926, Loss: 0.4136681407690048, Final Batch Loss: 0.18840622901916504\n",
      "Epoch 2927, Loss: 0.3087354302406311, Final Batch Loss: 0.13005253672599792\n",
      "Epoch 2928, Loss: 0.387203574180603, Final Batch Loss: 0.25013890862464905\n",
      "Epoch 2929, Loss: 0.49249911308288574, Final Batch Loss: 0.2782531976699829\n",
      "Epoch 2930, Loss: 0.3423377722501755, Final Batch Loss: 0.1760604828596115\n",
      "Epoch 2931, Loss: 0.33595989644527435, Final Batch Loss: 0.1357339322566986\n",
      "Epoch 2932, Loss: 0.296131893992424, Final Batch Loss: 0.12429417669773102\n",
      "Epoch 2933, Loss: 0.3256360590457916, Final Batch Loss: 0.14041472971439362\n",
      "Epoch 2934, Loss: 0.3675619810819626, Final Batch Loss: 0.20708045363426208\n",
      "Epoch 2935, Loss: 0.35514359176158905, Final Batch Loss: 0.17521998286247253\n",
      "Epoch 2936, Loss: 0.3183172941207886, Final Batch Loss: 0.1871849149465561\n",
      "Epoch 2937, Loss: 0.32917454838752747, Final Batch Loss: 0.17203542590141296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2938, Loss: 0.29382021725177765, Final Batch Loss: 0.1104712039232254\n",
      "Epoch 2939, Loss: 0.4029913395643234, Final Batch Loss: 0.21339567005634308\n",
      "Epoch 2940, Loss: 0.31200550496578217, Final Batch Loss: 0.12876176834106445\n",
      "Epoch 2941, Loss: 0.2895323932170868, Final Batch Loss: 0.1465071588754654\n",
      "Epoch 2942, Loss: 0.35235871374607086, Final Batch Loss: 0.19576819241046906\n",
      "Epoch 2943, Loss: 0.3407018482685089, Final Batch Loss: 0.20395895838737488\n",
      "Epoch 2944, Loss: 0.32443687319755554, Final Batch Loss: 0.15854264795780182\n",
      "Epoch 2945, Loss: 0.2759498804807663, Final Batch Loss: 0.12848956882953644\n",
      "Epoch 2946, Loss: 0.3511774390935898, Final Batch Loss: 0.173390731215477\n",
      "Epoch 2947, Loss: 0.33661313354969025, Final Batch Loss: 0.17541632056236267\n",
      "Epoch 2948, Loss: 0.3384789377450943, Final Batch Loss: 0.15510113537311554\n",
      "Epoch 2949, Loss: 0.40073417127132416, Final Batch Loss: 0.2391044795513153\n",
      "Epoch 2950, Loss: 0.3486492186784744, Final Batch Loss: 0.17372441291809082\n",
      "Epoch 2951, Loss: 0.3351473808288574, Final Batch Loss: 0.1509317308664322\n",
      "Epoch 2952, Loss: 0.3036615550518036, Final Batch Loss: 0.17372526228427887\n",
      "Epoch 2953, Loss: 0.3461960554122925, Final Batch Loss: 0.13766808807849884\n",
      "Epoch 2954, Loss: 0.3111697882413864, Final Batch Loss: 0.1469823271036148\n",
      "Epoch 2955, Loss: 0.34107184410095215, Final Batch Loss: 0.1784212440252304\n",
      "Epoch 2956, Loss: 0.35156549513339996, Final Batch Loss: 0.1728263795375824\n",
      "Epoch 2957, Loss: 0.2617543935775757, Final Batch Loss: 0.10821379721164703\n",
      "Epoch 2958, Loss: 0.4444950819015503, Final Batch Loss: 0.2449565827846527\n",
      "Epoch 2959, Loss: 0.36454416811466217, Final Batch Loss: 0.19783715903759003\n",
      "Epoch 2960, Loss: 0.3814458101987839, Final Batch Loss: 0.20079058408737183\n",
      "Epoch 2961, Loss: 0.33665911853313446, Final Batch Loss: 0.19573242962360382\n",
      "Epoch 2962, Loss: 0.3434174358844757, Final Batch Loss: 0.1942029744386673\n",
      "Epoch 2963, Loss: 0.3444261699914932, Final Batch Loss: 0.20767304301261902\n",
      "Epoch 2964, Loss: 0.43701450526714325, Final Batch Loss: 0.23767440021038055\n",
      "Epoch 2965, Loss: 0.318468876183033, Final Batch Loss: 0.1106095090508461\n",
      "Epoch 2966, Loss: 0.45541083812713623, Final Batch Loss: 0.2654965817928314\n",
      "Epoch 2967, Loss: 0.35460296273231506, Final Batch Loss: 0.14819036424160004\n",
      "Epoch 2968, Loss: 0.42268502712249756, Final Batch Loss: 0.24396926164627075\n",
      "Epoch 2969, Loss: 0.38366253674030304, Final Batch Loss: 0.14268022775650024\n",
      "Epoch 2970, Loss: 0.33117032051086426, Final Batch Loss: 0.1749372035264969\n",
      "Epoch 2971, Loss: 0.36799366772174835, Final Batch Loss: 0.15649695694446564\n",
      "Epoch 2972, Loss: 0.3516528010368347, Final Batch Loss: 0.18034586310386658\n",
      "Epoch 2973, Loss: 0.3962934911251068, Final Batch Loss: 0.2328784018754959\n",
      "Epoch 2974, Loss: 0.35794635117053986, Final Batch Loss: 0.1275099217891693\n",
      "Epoch 2975, Loss: 0.43917687237262726, Final Batch Loss: 0.18103207647800446\n",
      "Epoch 2976, Loss: 0.30050021409988403, Final Batch Loss: 0.13704025745391846\n",
      "Epoch 2977, Loss: 0.3124419301748276, Final Batch Loss: 0.14786086976528168\n",
      "Epoch 2978, Loss: 0.33177265524864197, Final Batch Loss: 0.13797855377197266\n",
      "Epoch 2979, Loss: 0.3231736421585083, Final Batch Loss: 0.1556306928396225\n",
      "Epoch 2980, Loss: 0.34370552003383636, Final Batch Loss: 0.19132912158966064\n",
      "Epoch 2981, Loss: 0.40356411039829254, Final Batch Loss: 0.2504810690879822\n",
      "Epoch 2982, Loss: 0.42853520810604095, Final Batch Loss: 0.18591010570526123\n",
      "Epoch 2983, Loss: 0.41172297298908234, Final Batch Loss: 0.22698159515857697\n",
      "Epoch 2984, Loss: 0.36509889364242554, Final Batch Loss: 0.15877732634544373\n",
      "Epoch 2985, Loss: 0.42778265476226807, Final Batch Loss: 0.24890688061714172\n",
      "Epoch 2986, Loss: 0.345188245177269, Final Batch Loss: 0.17591500282287598\n",
      "Epoch 2987, Loss: 0.34031419456005096, Final Batch Loss: 0.18578363955020905\n",
      "Epoch 2988, Loss: 0.3666817843914032, Final Batch Loss: 0.20748890936374664\n",
      "Epoch 2989, Loss: 0.32874439656734467, Final Batch Loss: 0.16862985491752625\n",
      "Epoch 2990, Loss: 0.3301474303007126, Final Batch Loss: 0.15379652380943298\n",
      "Epoch 2991, Loss: 0.40021972358226776, Final Batch Loss: 0.1599644273519516\n",
      "Epoch 2992, Loss: 0.35379326343536377, Final Batch Loss: 0.1544826328754425\n",
      "Epoch 2993, Loss: 0.33953574299812317, Final Batch Loss: 0.15728598833084106\n",
      "Epoch 2994, Loss: 0.3716166168451309, Final Batch Loss: 0.20482729375362396\n",
      "Epoch 2995, Loss: 0.3595411628484726, Final Batch Loss: 0.19326159358024597\n",
      "Epoch 2996, Loss: 0.2936728149652481, Final Batch Loss: 0.09939481317996979\n",
      "Epoch 2997, Loss: 0.3017602860927582, Final Batch Loss: 0.1640428900718689\n",
      "Epoch 2998, Loss: 0.35441090166568756, Final Batch Loss: 0.14168134331703186\n",
      "Epoch 2999, Loss: 0.3289160579442978, Final Batch Loss: 0.1900927871465683\n",
      "Epoch 3000, Loss: 0.3095705956220627, Final Batch Loss: 0.16620667278766632\n",
      "Epoch 3001, Loss: 0.3393770158290863, Final Batch Loss: 0.18306100368499756\n",
      "Epoch 3002, Loss: 0.3173900544643402, Final Batch Loss: 0.15438203513622284\n",
      "Epoch 3003, Loss: 0.3646801710128784, Final Batch Loss: 0.18417063355445862\n",
      "Epoch 3004, Loss: 0.3241442143917084, Final Batch Loss: 0.1769077479839325\n",
      "Epoch 3005, Loss: 0.29027462005615234, Final Batch Loss: 0.1630118489265442\n",
      "Epoch 3006, Loss: 0.337593637406826, Final Batch Loss: 0.11121628433465958\n",
      "Epoch 3007, Loss: 0.2590116336941719, Final Batch Loss: 0.09165554493665695\n",
      "Epoch 3008, Loss: 0.3061727285385132, Final Batch Loss: 0.1555270552635193\n",
      "Epoch 3009, Loss: 0.30698761343955994, Final Batch Loss: 0.14394056797027588\n",
      "Epoch 3010, Loss: 0.36575351655483246, Final Batch Loss: 0.1828322857618332\n",
      "Epoch 3011, Loss: 0.4510136693716049, Final Batch Loss: 0.29969125986099243\n",
      "Epoch 3012, Loss: 0.31949450075626373, Final Batch Loss: 0.14602746069431305\n",
      "Epoch 3013, Loss: 0.30763787031173706, Final Batch Loss: 0.13943582773208618\n",
      "Epoch 3014, Loss: 0.3878069370985031, Final Batch Loss: 0.2217256873846054\n",
      "Epoch 3015, Loss: 0.3112688213586807, Final Batch Loss: 0.1320958435535431\n",
      "Epoch 3016, Loss: 0.3224083483219147, Final Batch Loss: 0.15985552966594696\n",
      "Epoch 3017, Loss: 0.31220175325870514, Final Batch Loss: 0.14849504828453064\n",
      "Epoch 3018, Loss: 0.329851359128952, Final Batch Loss: 0.14118897914886475\n",
      "Epoch 3019, Loss: 0.3130687549710274, Final Batch Loss: 0.12043296545743942\n",
      "Epoch 3020, Loss: 0.3544969856739044, Final Batch Loss: 0.1804186999797821\n",
      "Epoch 3021, Loss: 0.3591250777244568, Final Batch Loss: 0.1784612089395523\n",
      "Epoch 3022, Loss: 0.41156452894210815, Final Batch Loss: 0.22733357548713684\n",
      "Epoch 3023, Loss: 0.3926853537559509, Final Batch Loss: 0.2079455852508545\n",
      "Epoch 3024, Loss: 0.2879893183708191, Final Batch Loss: 0.15019379556179047\n",
      "Epoch 3025, Loss: 0.3295917361974716, Final Batch Loss: 0.1210329532623291\n",
      "Epoch 3026, Loss: 0.3200010508298874, Final Batch Loss: 0.1487281620502472\n",
      "Epoch 3027, Loss: 0.2609521150588989, Final Batch Loss: 0.10599853098392487\n",
      "Epoch 3028, Loss: 0.3837709426879883, Final Batch Loss: 0.1934012770652771\n",
      "Epoch 3029, Loss: 0.3170067071914673, Final Batch Loss: 0.14782053232192993\n",
      "Epoch 3030, Loss: 0.3541144132614136, Final Batch Loss: 0.151816725730896\n",
      "Epoch 3031, Loss: 0.36528438329696655, Final Batch Loss: 0.22697488963603973\n",
      "Epoch 3032, Loss: 0.3548889309167862, Final Batch Loss: 0.21060781180858612\n",
      "Epoch 3033, Loss: 0.3195701092481613, Final Batch Loss: 0.1664705127477646\n",
      "Epoch 3034, Loss: 0.38340964913368225, Final Batch Loss: 0.1764673888683319\n",
      "Epoch 3035, Loss: 0.43545737862586975, Final Batch Loss: 0.21809962391853333\n",
      "Epoch 3036, Loss: 0.3062659800052643, Final Batch Loss: 0.15608231723308563\n",
      "Epoch 3037, Loss: 0.3811355382204056, Final Batch Loss: 0.18485890328884125\n",
      "Epoch 3038, Loss: 0.394559770822525, Final Batch Loss: 0.24341852962970734\n",
      "Epoch 3039, Loss: 0.380503848195076, Final Batch Loss: 0.18899290263652802\n",
      "Epoch 3040, Loss: 0.32742939889431, Final Batch Loss: 0.16313202679157257\n",
      "Epoch 3041, Loss: 0.34157173335552216, Final Batch Loss: 0.1948716938495636\n",
      "Epoch 3042, Loss: 0.3427620232105255, Final Batch Loss: 0.1624225676059723\n",
      "Epoch 3043, Loss: 0.3618876188993454, Final Batch Loss: 0.1878952980041504\n",
      "Epoch 3044, Loss: 0.32022222876548767, Final Batch Loss: 0.13372112810611725\n",
      "Epoch 3045, Loss: 0.3215954601764679, Final Batch Loss: 0.11582008004188538\n",
      "Epoch 3046, Loss: 0.3281492590904236, Final Batch Loss: 0.17017735540866852\n",
      "Epoch 3047, Loss: 0.34597326815128326, Final Batch Loss: 0.17821092903614044\n",
      "Epoch 3048, Loss: 0.3081129193305969, Final Batch Loss: 0.1438003033399582\n",
      "Epoch 3049, Loss: 0.3636181950569153, Final Batch Loss: 0.16310517489910126\n",
      "Epoch 3050, Loss: 0.3607170134782791, Final Batch Loss: 0.2038695365190506\n",
      "Epoch 3051, Loss: 0.37125563621520996, Final Batch Loss: 0.19886313378810883\n",
      "Epoch 3052, Loss: 0.30833643674850464, Final Batch Loss: 0.10584278404712677\n",
      "Epoch 3053, Loss: 0.377142533659935, Final Batch Loss: 0.21508224308490753\n",
      "Epoch 3054, Loss: 0.3178061246871948, Final Batch Loss: 0.17182786762714386\n",
      "Epoch 3055, Loss: 0.2816341519355774, Final Batch Loss: 0.13061384856700897\n",
      "Epoch 3056, Loss: 0.3595452904701233, Final Batch Loss: 0.1884605586528778\n",
      "Epoch 3057, Loss: 0.3761214017868042, Final Batch Loss: 0.22441963851451874\n",
      "Epoch 3058, Loss: 0.3157099187374115, Final Batch Loss: 0.14088347554206848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3059, Loss: 0.26666291058063507, Final Batch Loss: 0.10977146029472351\n",
      "Epoch 3060, Loss: 0.35527200996875763, Final Batch Loss: 0.15771131217479706\n",
      "Epoch 3061, Loss: 0.3895651698112488, Final Batch Loss: 0.23421870172023773\n",
      "Epoch 3062, Loss: 0.4046681523323059, Final Batch Loss: 0.234112948179245\n",
      "Epoch 3063, Loss: 0.4563487768173218, Final Batch Loss: 0.28193914890289307\n",
      "Epoch 3064, Loss: 0.3775385618209839, Final Batch Loss: 0.2123601734638214\n",
      "Epoch 3065, Loss: 0.27513665705919266, Final Batch Loss: 0.09363312274217606\n",
      "Epoch 3066, Loss: 0.3366992026567459, Final Batch Loss: 0.1762121468782425\n",
      "Epoch 3067, Loss: 0.3310416489839554, Final Batch Loss: 0.16539566218852997\n",
      "Epoch 3068, Loss: 0.3789127320051193, Final Batch Loss: 0.19811998307704926\n",
      "Epoch 3069, Loss: 0.26425478607416153, Final Batch Loss: 0.09036112576723099\n",
      "Epoch 3070, Loss: 0.3733683377504349, Final Batch Loss: 0.22324571013450623\n",
      "Epoch 3071, Loss: 0.3218768984079361, Final Batch Loss: 0.17075330018997192\n",
      "Epoch 3072, Loss: 0.363344669342041, Final Batch Loss: 0.2231312096118927\n",
      "Epoch 3073, Loss: 0.397119864821434, Final Batch Loss: 0.21623595058918\n",
      "Epoch 3074, Loss: 0.34075868129730225, Final Batch Loss: 0.14525024592876434\n",
      "Epoch 3075, Loss: 0.3071134239435196, Final Batch Loss: 0.15398260951042175\n",
      "Epoch 3076, Loss: 0.3029068261384964, Final Batch Loss: 0.15333327651023865\n",
      "Epoch 3077, Loss: 0.39523348212242126, Final Batch Loss: 0.1999311000108719\n",
      "Epoch 3078, Loss: 0.3621528893709183, Final Batch Loss: 0.1842382550239563\n",
      "Epoch 3079, Loss: 0.3345656991004944, Final Batch Loss: 0.16851837933063507\n",
      "Epoch 3080, Loss: 0.29477666318416595, Final Batch Loss: 0.13660483062267303\n",
      "Epoch 3081, Loss: 0.3087758421897888, Final Batch Loss: 0.14160683751106262\n",
      "Epoch 3082, Loss: 0.3058038204908371, Final Batch Loss: 0.11074858903884888\n",
      "Epoch 3083, Loss: 0.4512791186571121, Final Batch Loss: 0.2960490584373474\n",
      "Epoch 3084, Loss: 0.39461152255535126, Final Batch Loss: 0.26684918999671936\n",
      "Epoch 3085, Loss: 0.35393403470516205, Final Batch Loss: 0.16607435047626495\n",
      "Epoch 3086, Loss: 0.3729817420244217, Final Batch Loss: 0.19356322288513184\n",
      "Epoch 3087, Loss: 0.32775838673114777, Final Batch Loss: 0.1672356277704239\n",
      "Epoch 3088, Loss: 0.32771800458431244, Final Batch Loss: 0.1644599884748459\n",
      "Epoch 3089, Loss: 0.38296452164649963, Final Batch Loss: 0.24539968371391296\n",
      "Epoch 3090, Loss: 0.28939758241176605, Final Batch Loss: 0.131717711687088\n",
      "Epoch 3091, Loss: 0.25743748247623444, Final Batch Loss: 0.09756910800933838\n",
      "Epoch 3092, Loss: 0.34899652004241943, Final Batch Loss: 0.180643230676651\n",
      "Epoch 3093, Loss: 0.3263009339570999, Final Batch Loss: 0.14635363221168518\n",
      "Epoch 3094, Loss: 0.40811627358198166, Final Batch Loss: 0.1247541531920433\n",
      "Epoch 3095, Loss: 0.4138030409812927, Final Batch Loss: 0.2720780670642853\n",
      "Epoch 3096, Loss: 0.3481088727712631, Final Batch Loss: 0.15595991909503937\n",
      "Epoch 3097, Loss: 0.26983436197042465, Final Batch Loss: 0.10226624459028244\n",
      "Epoch 3098, Loss: 0.275477759540081, Final Batch Loss: 0.10059820860624313\n",
      "Epoch 3099, Loss: 0.26736435294151306, Final Batch Loss: 0.10384686291217804\n",
      "Epoch 3100, Loss: 0.2916964367032051, Final Batch Loss: 0.11932358890771866\n",
      "Epoch 3101, Loss: 0.3205797076225281, Final Batch Loss: 0.1001204252243042\n",
      "Epoch 3102, Loss: 0.289026603102684, Final Batch Loss: 0.11835560202598572\n",
      "Epoch 3103, Loss: 0.3135334253311157, Final Batch Loss: 0.11885540187358856\n",
      "Epoch 3104, Loss: 0.3343266248703003, Final Batch Loss: 0.1485571563243866\n",
      "Epoch 3105, Loss: 0.27878613770008087, Final Batch Loss: 0.14189322292804718\n",
      "Epoch 3106, Loss: 0.30612988770008087, Final Batch Loss: 0.13453814387321472\n",
      "Epoch 3107, Loss: 0.3569376915693283, Final Batch Loss: 0.16443364322185516\n",
      "Epoch 3108, Loss: 0.27687080204486847, Final Batch Loss: 0.1336417943239212\n",
      "Epoch 3109, Loss: 0.3451453149318695, Final Batch Loss: 0.18998046219348907\n",
      "Epoch 3110, Loss: 0.35346463322639465, Final Batch Loss: 0.14360426366329193\n",
      "Epoch 3111, Loss: 0.3649100363254547, Final Batch Loss: 0.17639100551605225\n",
      "Epoch 3112, Loss: 0.2922635078430176, Final Batch Loss: 0.1528787910938263\n",
      "Epoch 3113, Loss: 0.30900995433330536, Final Batch Loss: 0.12067796289920807\n",
      "Epoch 3114, Loss: 0.2975273132324219, Final Batch Loss: 0.14109374582767487\n",
      "Epoch 3115, Loss: 0.2607565000653267, Final Batch Loss: 0.12273714691400528\n",
      "Epoch 3116, Loss: 0.38370752334594727, Final Batch Loss: 0.21349065005779266\n",
      "Epoch 3117, Loss: 0.25650229305028915, Final Batch Loss: 0.07835917919874191\n",
      "Epoch 3118, Loss: 0.3299618810415268, Final Batch Loss: 0.18088331818580627\n",
      "Epoch 3119, Loss: 0.3579946756362915, Final Batch Loss: 0.14073729515075684\n",
      "Epoch 3120, Loss: 0.29722926020622253, Final Batch Loss: 0.1410202831029892\n",
      "Epoch 3121, Loss: 0.26825498044490814, Final Batch Loss: 0.11359654366970062\n",
      "Epoch 3122, Loss: 0.3265641927719116, Final Batch Loss: 0.17654912173748016\n",
      "Epoch 3123, Loss: 0.39701129496097565, Final Batch Loss: 0.21636299788951874\n",
      "Epoch 3124, Loss: 0.2864459306001663, Final Batch Loss: 0.09509742259979248\n",
      "Epoch 3125, Loss: 0.2805732488632202, Final Batch Loss: 0.13025124371051788\n",
      "Epoch 3126, Loss: 0.3048868626356125, Final Batch Loss: 0.1599729359149933\n",
      "Epoch 3127, Loss: 0.3221333771944046, Final Batch Loss: 0.1532210111618042\n",
      "Epoch 3128, Loss: 0.28983379900455475, Final Batch Loss: 0.15733149647712708\n",
      "Epoch 3129, Loss: 0.27578046917915344, Final Batch Loss: 0.11558432877063751\n",
      "Epoch 3130, Loss: 0.34826697409152985, Final Batch Loss: 0.20936231315135956\n",
      "Epoch 3131, Loss: 0.31184399127960205, Final Batch Loss: 0.15293405950069427\n",
      "Epoch 3132, Loss: 0.2548268735408783, Final Batch Loss: 0.08716243505477905\n",
      "Epoch 3133, Loss: 0.39265885949134827, Final Batch Loss: 0.1967049539089203\n",
      "Epoch 3134, Loss: 0.3649352788925171, Final Batch Loss: 0.1979530304670334\n",
      "Epoch 3135, Loss: 0.2810049057006836, Final Batch Loss: 0.10104157030582428\n",
      "Epoch 3136, Loss: 0.3811069577932358, Final Batch Loss: 0.20294305682182312\n",
      "Epoch 3137, Loss: 0.2952638119459152, Final Batch Loss: 0.13525108993053436\n",
      "Epoch 3138, Loss: 0.29038169980049133, Final Batch Loss: 0.13695625960826874\n",
      "Epoch 3139, Loss: 0.3115207850933075, Final Batch Loss: 0.13767020404338837\n",
      "Epoch 3140, Loss: 0.3716129660606384, Final Batch Loss: 0.16144078969955444\n",
      "Epoch 3141, Loss: 0.2995293512940407, Final Batch Loss: 0.11956485360860825\n",
      "Epoch 3142, Loss: 0.315873384475708, Final Batch Loss: 0.17004892230033875\n",
      "Epoch 3143, Loss: 0.45366616547107697, Final Batch Loss: 0.256394624710083\n",
      "Epoch 3144, Loss: 0.2908920496702194, Final Batch Loss: 0.15536248683929443\n",
      "Epoch 3145, Loss: 0.44543540477752686, Final Batch Loss: 0.22253921627998352\n",
      "Epoch 3146, Loss: 0.34809111058712006, Final Batch Loss: 0.18237221240997314\n",
      "Epoch 3147, Loss: 0.325789138674736, Final Batch Loss: 0.1476833075284958\n",
      "Epoch 3148, Loss: 0.27587461471557617, Final Batch Loss: 0.126024529337883\n",
      "Epoch 3149, Loss: 0.28863073885440826, Final Batch Loss: 0.13826104998588562\n",
      "Epoch 3150, Loss: 0.3121853172779083, Final Batch Loss: 0.17706404626369476\n",
      "Epoch 3151, Loss: 0.2530507892370224, Final Batch Loss: 0.11751019954681396\n",
      "Epoch 3152, Loss: 0.4167613312602043, Final Batch Loss: 0.3093751072883606\n",
      "Epoch 3153, Loss: 0.3159361332654953, Final Batch Loss: 0.1682637631893158\n",
      "Epoch 3154, Loss: 0.36234964430332184, Final Batch Loss: 0.13435527682304382\n",
      "Epoch 3155, Loss: 0.3836249113082886, Final Batch Loss: 0.23501531779766083\n",
      "Epoch 3156, Loss: 0.36050812900066376, Final Batch Loss: 0.2164526879787445\n",
      "Epoch 3157, Loss: 0.2974177449941635, Final Batch Loss: 0.12986503541469574\n",
      "Epoch 3158, Loss: 0.2755591571331024, Final Batch Loss: 0.14089414477348328\n",
      "Epoch 3159, Loss: 0.30024006962776184, Final Batch Loss: 0.1860784888267517\n",
      "Epoch 3160, Loss: 0.2721666693687439, Final Batch Loss: 0.12843720614910126\n",
      "Epoch 3161, Loss: 0.4430573135614395, Final Batch Loss: 0.2760680615901947\n",
      "Epoch 3162, Loss: 0.33008573949337006, Final Batch Loss: 0.14760065078735352\n",
      "Epoch 3163, Loss: 0.4316588044166565, Final Batch Loss: 0.2603297829627991\n",
      "Epoch 3164, Loss: 0.39789290726184845, Final Batch Loss: 0.2147655487060547\n",
      "Epoch 3165, Loss: 0.2852170616388321, Final Batch Loss: 0.0912645012140274\n",
      "Epoch 3166, Loss: 0.2796122431755066, Final Batch Loss: 0.14017285406589508\n",
      "Epoch 3167, Loss: 0.37189528346061707, Final Batch Loss: 0.17055395245552063\n",
      "Epoch 3168, Loss: 0.2798025906085968, Final Batch Loss: 0.14323651790618896\n",
      "Epoch 3169, Loss: 0.3470335155725479, Final Batch Loss: 0.2016962319612503\n",
      "Epoch 3170, Loss: 0.3213665932416916, Final Batch Loss: 0.15816403925418854\n",
      "Epoch 3171, Loss: 0.3392074704170227, Final Batch Loss: 0.1308513730764389\n",
      "Epoch 3172, Loss: 0.32186131179332733, Final Batch Loss: 0.14593394100666046\n",
      "Epoch 3173, Loss: 0.37751305103302, Final Batch Loss: 0.2102290838956833\n",
      "Epoch 3174, Loss: 0.30052459239959717, Final Batch Loss: 0.14271092414855957\n",
      "Epoch 3175, Loss: 0.28027205169200897, Final Batch Loss: 0.11503012478351593\n",
      "Epoch 3176, Loss: 0.278395876288414, Final Batch Loss: 0.09973938763141632\n",
      "Epoch 3177, Loss: 0.34524138271808624, Final Batch Loss: 0.1089867502450943\n",
      "Epoch 3178, Loss: 0.36392588913440704, Final Batch Loss: 0.19677087664604187\n",
      "Epoch 3179, Loss: 0.35028165578842163, Final Batch Loss: 0.18592485785484314\n",
      "Epoch 3180, Loss: 0.4576056897640228, Final Batch Loss: 0.26037997007369995\n",
      "Epoch 3181, Loss: 0.33724890649318695, Final Batch Loss: 0.1859859973192215\n",
      "Epoch 3182, Loss: 0.33766093850135803, Final Batch Loss: 0.17195431888103485\n",
      "Epoch 3183, Loss: 0.3581384867429733, Final Batch Loss: 0.17687486112117767\n",
      "Epoch 3184, Loss: 0.32000191509723663, Final Batch Loss: 0.15138067305088043\n",
      "Epoch 3185, Loss: 0.3036481589078903, Final Batch Loss: 0.13867177069187164\n",
      "Epoch 3186, Loss: 0.3199481666088104, Final Batch Loss: 0.1413624882698059\n",
      "Epoch 3187, Loss: 0.6326174885034561, Final Batch Loss: 0.4677601158618927\n",
      "Epoch 3188, Loss: 0.29212360829114914, Final Batch Loss: 0.10573673993349075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3189, Loss: 0.3742438107728958, Final Batch Loss: 0.23570634424686432\n",
      "Epoch 3190, Loss: 0.31875358521938324, Final Batch Loss: 0.17726410925388336\n",
      "Epoch 3191, Loss: 0.3354855179786682, Final Batch Loss: 0.14306382834911346\n",
      "Epoch 3192, Loss: 0.3669014871120453, Final Batch Loss: 0.14803124964237213\n",
      "Epoch 3193, Loss: 0.42644381523132324, Final Batch Loss: 0.2520497441291809\n",
      "Epoch 3194, Loss: 0.30499663949012756, Final Batch Loss: 0.13984741270542145\n",
      "Epoch 3195, Loss: 0.3274901211261749, Final Batch Loss: 0.18206174671649933\n",
      "Epoch 3196, Loss: 0.34439438581466675, Final Batch Loss: 0.12613336741924286\n",
      "Epoch 3197, Loss: 0.34741075336933136, Final Batch Loss: 0.18038992583751678\n",
      "Epoch 3198, Loss: 0.43781180679798126, Final Batch Loss: 0.28291699290275574\n",
      "Epoch 3199, Loss: 0.30413636565208435, Final Batch Loss: 0.10338886082172394\n",
      "Epoch 3200, Loss: 0.3297825902700424, Final Batch Loss: 0.15692827105522156\n",
      "Epoch 3201, Loss: 0.2912609428167343, Final Batch Loss: 0.14507921040058136\n",
      "Epoch 3202, Loss: 0.35193027555942535, Final Batch Loss: 0.18687628209590912\n",
      "Epoch 3203, Loss: 0.39225490391254425, Final Batch Loss: 0.20381715893745422\n",
      "Epoch 3204, Loss: 0.3074386864900589, Final Batch Loss: 0.13357648253440857\n",
      "Epoch 3205, Loss: 0.2621912285685539, Final Batch Loss: 0.1444113701581955\n",
      "Epoch 3206, Loss: 0.2510436475276947, Final Batch Loss: 0.06612418591976166\n",
      "Epoch 3207, Loss: 0.41807921230793, Final Batch Loss: 0.28667181730270386\n",
      "Epoch 3208, Loss: 0.3612605407834053, Final Batch Loss: 0.12218181043863297\n",
      "Epoch 3209, Loss: 0.31897374987602234, Final Batch Loss: 0.17846405506134033\n",
      "Epoch 3210, Loss: 0.3702182471752167, Final Batch Loss: 0.24909383058547974\n",
      "Epoch 3211, Loss: 0.37512294948101044, Final Batch Loss: 0.1651439517736435\n",
      "Epoch 3212, Loss: 0.27585994452238083, Final Batch Loss: 0.12252437323331833\n",
      "Epoch 3213, Loss: 0.30505527555942535, Final Batch Loss: 0.13218092918395996\n",
      "Epoch 3214, Loss: 0.3004033863544464, Final Batch Loss: 0.13914579153060913\n",
      "Epoch 3215, Loss: 0.38313063979148865, Final Batch Loss: 0.2083345204591751\n",
      "Epoch 3216, Loss: 0.38411474227905273, Final Batch Loss: 0.18846289813518524\n",
      "Epoch 3217, Loss: 0.28024929761886597, Final Batch Loss: 0.1388535052537918\n",
      "Epoch 3218, Loss: 0.31969350576400757, Final Batch Loss: 0.13731639087200165\n",
      "Epoch 3219, Loss: 0.42265649139881134, Final Batch Loss: 0.25866082310676575\n",
      "Epoch 3220, Loss: 0.3594600111246109, Final Batch Loss: 0.1890871226787567\n",
      "Epoch 3221, Loss: 0.3209786117076874, Final Batch Loss: 0.18345029652118683\n",
      "Epoch 3222, Loss: 0.2966712936758995, Final Batch Loss: 0.09162283688783646\n",
      "Epoch 3223, Loss: 0.3747188597917557, Final Batch Loss: 0.22247175872325897\n",
      "Epoch 3224, Loss: 0.3354833871126175, Final Batch Loss: 0.14943569898605347\n",
      "Epoch 3225, Loss: 0.3982091397047043, Final Batch Loss: 0.2360079288482666\n",
      "Epoch 3226, Loss: 0.435488224029541, Final Batch Loss: 0.2810972332954407\n",
      "Epoch 3227, Loss: 0.3651365041732788, Final Batch Loss: 0.16986465454101562\n",
      "Epoch 3228, Loss: 0.3213372528553009, Final Batch Loss: 0.17255984246730804\n",
      "Epoch 3229, Loss: 0.3702499717473984, Final Batch Loss: 0.22666092216968536\n",
      "Epoch 3230, Loss: 0.3383565843105316, Final Batch Loss: 0.1512288898229599\n",
      "Epoch 3231, Loss: 0.4212978631258011, Final Batch Loss: 0.20265737175941467\n",
      "Epoch 3232, Loss: 0.3346240222454071, Final Batch Loss: 0.18860596418380737\n",
      "Epoch 3233, Loss: 0.37983639538288116, Final Batch Loss: 0.2375776469707489\n",
      "Epoch 3234, Loss: 0.31319209933280945, Final Batch Loss: 0.1573294848203659\n",
      "Epoch 3235, Loss: 0.2642192170023918, Final Batch Loss: 0.14130325615406036\n",
      "Epoch 3236, Loss: 0.2976745069026947, Final Batch Loss: 0.15075665712356567\n",
      "Epoch 3237, Loss: 0.2976851761341095, Final Batch Loss: 0.11828559637069702\n",
      "Epoch 3238, Loss: 0.30201277136802673, Final Batch Loss: 0.17404821515083313\n",
      "Epoch 3239, Loss: 0.40041524171829224, Final Batch Loss: 0.24074073135852814\n",
      "Epoch 3240, Loss: 0.3131691962480545, Final Batch Loss: 0.15786005556583405\n",
      "Epoch 3241, Loss: 0.2631164863705635, Final Batch Loss: 0.09282010048627853\n",
      "Epoch 3242, Loss: 0.3296874463558197, Final Batch Loss: 0.14818620681762695\n",
      "Epoch 3243, Loss: 0.40866371989250183, Final Batch Loss: 0.23836061358451843\n",
      "Epoch 3244, Loss: 0.42412494122982025, Final Batch Loss: 0.2821026146411896\n",
      "Epoch 3245, Loss: 0.3614548295736313, Final Batch Loss: 0.2276485711336136\n",
      "Epoch 3246, Loss: 0.3285018652677536, Final Batch Loss: 0.14731089770793915\n",
      "Epoch 3247, Loss: 0.34731927514076233, Final Batch Loss: 0.18795424699783325\n",
      "Epoch 3248, Loss: 0.27657099813222885, Final Batch Loss: 0.15328679978847504\n",
      "Epoch 3249, Loss: 0.3612622618675232, Final Batch Loss: 0.2033631056547165\n",
      "Epoch 3250, Loss: 0.348624624311924, Final Batch Loss: 0.12116100639104843\n",
      "Epoch 3251, Loss: 0.38119035959243774, Final Batch Loss: 0.187198206782341\n",
      "Epoch 3252, Loss: 0.28681115061044693, Final Batch Loss: 0.10975945740938187\n",
      "Epoch 3253, Loss: 0.30666975677013397, Final Batch Loss: 0.16657407581806183\n",
      "Epoch 3254, Loss: 0.27479730546474457, Final Batch Loss: 0.10664273798465729\n",
      "Epoch 3255, Loss: 0.2776067778468132, Final Batch Loss: 0.11510316282510757\n",
      "Epoch 3256, Loss: 0.34638310968875885, Final Batch Loss: 0.18359814584255219\n",
      "Epoch 3257, Loss: 0.3531152456998825, Final Batch Loss: 0.2068074494600296\n",
      "Epoch 3258, Loss: 0.34300441294908524, Final Batch Loss: 0.22046560049057007\n",
      "Epoch 3259, Loss: 0.29319848120212555, Final Batch Loss: 0.12750089168548584\n",
      "Epoch 3260, Loss: 0.3689415007829666, Final Batch Loss: 0.20056134462356567\n",
      "Epoch 3261, Loss: 0.33638565242290497, Final Batch Loss: 0.20403870940208435\n",
      "Epoch 3262, Loss: 0.37888863682746887, Final Batch Loss: 0.1900881677865982\n",
      "Epoch 3263, Loss: 0.2545052170753479, Final Batch Loss: 0.11559788882732391\n",
      "Epoch 3264, Loss: 0.2986208349466324, Final Batch Loss: 0.1457231640815735\n",
      "Epoch 3265, Loss: 0.237564317882061, Final Batch Loss: 0.0991174653172493\n",
      "Epoch 3266, Loss: 0.3520500957965851, Final Batch Loss: 0.16883011162281036\n",
      "Epoch 3267, Loss: 0.2448224276304245, Final Batch Loss: 0.09044209122657776\n",
      "Epoch 3268, Loss: 0.3135756701231003, Final Batch Loss: 0.1672866940498352\n",
      "Epoch 3269, Loss: 0.3412218391895294, Final Batch Loss: 0.18992067873477936\n",
      "Epoch 3270, Loss: 0.40118029713630676, Final Batch Loss: 0.254163533449173\n",
      "Epoch 3271, Loss: 0.37176404893398285, Final Batch Loss: 0.21079221367835999\n",
      "Epoch 3272, Loss: 0.276838518679142, Final Batch Loss: 0.10781123489141464\n",
      "Epoch 3273, Loss: 0.3996841609477997, Final Batch Loss: 0.17187538743019104\n",
      "Epoch 3274, Loss: 0.4413023144006729, Final Batch Loss: 0.26772916316986084\n",
      "Epoch 3275, Loss: 0.33650659024715424, Final Batch Loss: 0.1960258185863495\n",
      "Epoch 3276, Loss: 0.29942452907562256, Final Batch Loss: 0.16819259524345398\n",
      "Epoch 3277, Loss: 0.32475487887859344, Final Batch Loss: 0.16391050815582275\n",
      "Epoch 3278, Loss: 0.30203086137771606, Final Batch Loss: 0.180670365691185\n",
      "Epoch 3279, Loss: 0.3274277299642563, Final Batch Loss: 0.15538153052330017\n",
      "Epoch 3280, Loss: 0.2731311395764351, Final Batch Loss: 0.12140292674303055\n",
      "Epoch 3281, Loss: 0.3407115042209625, Final Batch Loss: 0.17722351849079132\n",
      "Epoch 3282, Loss: 0.23496343195438385, Final Batch Loss: 0.11268117278814316\n",
      "Epoch 3283, Loss: 0.3628324121236801, Final Batch Loss: 0.19241903722286224\n",
      "Epoch 3284, Loss: 0.32029570639133453, Final Batch Loss: 0.11656196415424347\n",
      "Epoch 3285, Loss: 0.3304954171180725, Final Batch Loss: 0.07685822248458862\n",
      "Epoch 3286, Loss: 0.32542693614959717, Final Batch Loss: 0.18364591896533966\n",
      "Epoch 3287, Loss: 0.24074213206768036, Final Batch Loss: 0.1226397454738617\n",
      "Epoch 3288, Loss: 0.24561799317598343, Final Batch Loss: 0.10946761816740036\n",
      "Epoch 3289, Loss: 0.3373343050479889, Final Batch Loss: 0.14209657907485962\n",
      "Epoch 3290, Loss: 0.2537575215101242, Final Batch Loss: 0.11560210585594177\n",
      "Epoch 3291, Loss: 0.370001420378685, Final Batch Loss: 0.19135728478431702\n",
      "Epoch 3292, Loss: 0.2857750132679939, Final Batch Loss: 0.16484178602695465\n",
      "Epoch 3293, Loss: 0.3132125288248062, Final Batch Loss: 0.1472632884979248\n",
      "Epoch 3294, Loss: 0.35055777430534363, Final Batch Loss: 0.19291521608829498\n",
      "Epoch 3295, Loss: 0.3033025413751602, Final Batch Loss: 0.13990864157676697\n",
      "Epoch 3296, Loss: 0.29217277467250824, Final Batch Loss: 0.13901500403881073\n",
      "Epoch 3297, Loss: 0.2894703298807144, Final Batch Loss: 0.1621968150138855\n",
      "Epoch 3298, Loss: 0.2726055383682251, Final Batch Loss: 0.12065358459949493\n",
      "Epoch 3299, Loss: 0.3492729514837265, Final Batch Loss: 0.1662215143442154\n",
      "Epoch 3300, Loss: 0.33217965066432953, Final Batch Loss: 0.19552713632583618\n",
      "Epoch 3301, Loss: 0.2929406091570854, Final Batch Loss: 0.10749939829111099\n",
      "Epoch 3302, Loss: 0.27436360716819763, Final Batch Loss: 0.13472384214401245\n",
      "Epoch 3303, Loss: 0.3817411959171295, Final Batch Loss: 0.22101110219955444\n",
      "Epoch 3304, Loss: 0.31846629083156586, Final Batch Loss: 0.15472280979156494\n",
      "Epoch 3305, Loss: 0.36439549177885056, Final Batch Loss: 0.10855530947446823\n",
      "Epoch 3306, Loss: 0.3463946729898453, Final Batch Loss: 0.12384718656539917\n",
      "Epoch 3307, Loss: 0.40206871926784515, Final Batch Loss: 0.24474042654037476\n",
      "Epoch 3308, Loss: 0.3260043114423752, Final Batch Loss: 0.17902600765228271\n",
      "Epoch 3309, Loss: 0.28446489572525024, Final Batch Loss: 0.1329200565814972\n",
      "Epoch 3310, Loss: 0.28901562839746475, Final Batch Loss: 0.11551878601312637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3311, Loss: 0.3111753910779953, Final Batch Loss: 0.18256400525569916\n",
      "Epoch 3312, Loss: 0.290753036737442, Final Batch Loss: 0.14636002480983734\n",
      "Epoch 3313, Loss: 0.291357196867466, Final Batch Loss: 0.1085614487528801\n",
      "Epoch 3314, Loss: 0.2876351401209831, Final Batch Loss: 0.11218642443418503\n",
      "Epoch 3315, Loss: 0.3257558345794678, Final Batch Loss: 0.17619875073432922\n",
      "Epoch 3316, Loss: 0.3367885500192642, Final Batch Loss: 0.15738661587238312\n",
      "Epoch 3317, Loss: 0.3478383719921112, Final Batch Loss: 0.16691064834594727\n",
      "Epoch 3318, Loss: 0.29497678577899933, Final Batch Loss: 0.14391088485717773\n",
      "Epoch 3319, Loss: 0.39773237705230713, Final Batch Loss: 0.23929660022258759\n",
      "Epoch 3320, Loss: 0.32809045910835266, Final Batch Loss: 0.18345552682876587\n",
      "Epoch 3321, Loss: 0.3209218382835388, Final Batch Loss: 0.19121719896793365\n",
      "Epoch 3322, Loss: 0.3250609338283539, Final Batch Loss: 0.16964757442474365\n",
      "Epoch 3323, Loss: 0.26689238101243973, Final Batch Loss: 0.11591262370347977\n",
      "Epoch 3324, Loss: 0.3140852749347687, Final Batch Loss: 0.14081496000289917\n",
      "Epoch 3325, Loss: 0.3881978839635849, Final Batch Loss: 0.17017245292663574\n",
      "Epoch 3326, Loss: 0.36705586314201355, Final Batch Loss: 0.20179130136966705\n",
      "Epoch 3327, Loss: 0.3442129045724869, Final Batch Loss: 0.17138487100601196\n",
      "Epoch 3328, Loss: 0.26540181040763855, Final Batch Loss: 0.09677967429161072\n",
      "Epoch 3329, Loss: 0.35755057632923126, Final Batch Loss: 0.19987332820892334\n",
      "Epoch 3330, Loss: 0.2952730804681778, Final Batch Loss: 0.14692190289497375\n",
      "Epoch 3331, Loss: 0.37658075988292694, Final Batch Loss: 0.2256215363740921\n",
      "Epoch 3332, Loss: 0.27982963621616364, Final Batch Loss: 0.1274385005235672\n",
      "Epoch 3333, Loss: 0.3394305109977722, Final Batch Loss: 0.1698838621377945\n",
      "Epoch 3334, Loss: 0.27944065630435944, Final Batch Loss: 0.13678553700447083\n",
      "Epoch 3335, Loss: 0.29374338686466217, Final Batch Loss: 0.14387018978595734\n",
      "Epoch 3336, Loss: 0.2881636470556259, Final Batch Loss: 0.16264450550079346\n",
      "Epoch 3337, Loss: 0.33453409373760223, Final Batch Loss: 0.17532147467136383\n",
      "Epoch 3338, Loss: 0.4365367442369461, Final Batch Loss: 0.2407711148262024\n",
      "Epoch 3339, Loss: 0.36674799025058746, Final Batch Loss: 0.22784043848514557\n",
      "Epoch 3340, Loss: 0.3143134117126465, Final Batch Loss: 0.16958275437355042\n",
      "Epoch 3341, Loss: 0.39051856100559235, Final Batch Loss: 0.26038187742233276\n",
      "Epoch 3342, Loss: 0.3439456969499588, Final Batch Loss: 0.14664624631404877\n",
      "Epoch 3343, Loss: 0.29887694120407104, Final Batch Loss: 0.14506563544273376\n",
      "Epoch 3344, Loss: 0.31024686992168427, Final Batch Loss: 0.1446121335029602\n",
      "Epoch 3345, Loss: 0.3259051442146301, Final Batch Loss: 0.16185593605041504\n",
      "Epoch 3346, Loss: 0.33181633055210114, Final Batch Loss: 0.15367534756660461\n",
      "Epoch 3347, Loss: 0.3566877841949463, Final Batch Loss: 0.1587211638689041\n",
      "Epoch 3348, Loss: 0.24686644226312637, Final Batch Loss: 0.13315798342227936\n",
      "Epoch 3349, Loss: 0.2970132380723953, Final Batch Loss: 0.11505994200706482\n",
      "Epoch 3350, Loss: 0.34943529963493347, Final Batch Loss: 0.20180822908878326\n",
      "Epoch 3351, Loss: 0.38915829360485077, Final Batch Loss: 0.19606134295463562\n",
      "Epoch 3352, Loss: 0.34180375933647156, Final Batch Loss: 0.16470979154109955\n",
      "Epoch 3353, Loss: 0.30225183069705963, Final Batch Loss: 0.15579061210155487\n",
      "Epoch 3354, Loss: 0.2543568015098572, Final Batch Loss: 0.08700181543827057\n",
      "Epoch 3355, Loss: 0.3640380799770355, Final Batch Loss: 0.21033203601837158\n",
      "Epoch 3356, Loss: 0.35576553642749786, Final Batch Loss: 0.181627094745636\n",
      "Epoch 3357, Loss: 0.364097535610199, Final Batch Loss: 0.20820946991443634\n",
      "Epoch 3358, Loss: 0.31730446219444275, Final Batch Loss: 0.16379594802856445\n",
      "Epoch 3359, Loss: 0.3279834985733032, Final Batch Loss: 0.19103223085403442\n",
      "Epoch 3360, Loss: 0.2809993773698807, Final Batch Loss: 0.13495025038719177\n",
      "Epoch 3361, Loss: 0.3371603712439537, Final Batch Loss: 0.11465802043676376\n",
      "Epoch 3362, Loss: 0.3498799055814743, Final Batch Loss: 0.202910915017128\n",
      "Epoch 3363, Loss: 0.3312842845916748, Final Batch Loss: 0.18499036133289337\n",
      "Epoch 3364, Loss: 0.3556472510099411, Final Batch Loss: 0.18258705735206604\n",
      "Epoch 3365, Loss: 0.2755630239844322, Final Batch Loss: 0.11450735479593277\n",
      "Epoch 3366, Loss: 0.3074013888835907, Final Batch Loss: 0.1406484842300415\n",
      "Epoch 3367, Loss: 0.3161574602127075, Final Batch Loss: 0.141672745347023\n",
      "Epoch 3368, Loss: 0.30265170335769653, Final Batch Loss: 0.14024056494235992\n",
      "Epoch 3369, Loss: 0.26553425937891006, Final Batch Loss: 0.11690907925367355\n",
      "Epoch 3370, Loss: 0.35859547555446625, Final Batch Loss: 0.21738073229789734\n",
      "Epoch 3371, Loss: 0.4521517902612686, Final Batch Loss: 0.277370810508728\n",
      "Epoch 3372, Loss: 0.25624848157167435, Final Batch Loss: 0.10329484194517136\n",
      "Epoch 3373, Loss: 0.3078150004148483, Final Batch Loss: 0.09626033902168274\n",
      "Epoch 3374, Loss: 0.3042752593755722, Final Batch Loss: 0.16102644801139832\n",
      "Epoch 3375, Loss: 0.31055784225463867, Final Batch Loss: 0.14197362959384918\n",
      "Epoch 3376, Loss: 0.34103548526763916, Final Batch Loss: 0.17507722973823547\n",
      "Epoch 3377, Loss: 0.4046269953250885, Final Batch Loss: 0.22794854640960693\n",
      "Epoch 3378, Loss: 0.27852584421634674, Final Batch Loss: 0.14910654723644257\n",
      "Epoch 3379, Loss: 0.33108751475811005, Final Batch Loss: 0.17815552651882172\n",
      "Epoch 3380, Loss: 0.30326882004737854, Final Batch Loss: 0.09538613259792328\n",
      "Epoch 3381, Loss: 0.2953035905957222, Final Batch Loss: 0.12374123185873032\n",
      "Epoch 3382, Loss: 0.3040374517440796, Final Batch Loss: 0.16313371062278748\n",
      "Epoch 3383, Loss: 0.32313336431980133, Final Batch Loss: 0.19078004360198975\n",
      "Epoch 3384, Loss: 0.2999895364046097, Final Batch Loss: 0.13204148411750793\n",
      "Epoch 3385, Loss: 0.3644719868898392, Final Batch Loss: 0.22876599431037903\n",
      "Epoch 3386, Loss: 0.3096715807914734, Final Batch Loss: 0.15764935314655304\n",
      "Epoch 3387, Loss: 0.3215108811855316, Final Batch Loss: 0.17146170139312744\n",
      "Epoch 3388, Loss: 0.3235870450735092, Final Batch Loss: 0.17801186442375183\n",
      "Epoch 3389, Loss: 0.3244488686323166, Final Batch Loss: 0.18640364706516266\n",
      "Epoch 3390, Loss: 0.2954142838716507, Final Batch Loss: 0.1538911908864975\n",
      "Epoch 3391, Loss: 0.305124968290329, Final Batch Loss: 0.14515478909015656\n",
      "Epoch 3392, Loss: 0.28931839764118195, Final Batch Loss: 0.12687768042087555\n",
      "Epoch 3393, Loss: 0.3766820728778839, Final Batch Loss: 0.13867565989494324\n",
      "Epoch 3394, Loss: 0.31981001794338226, Final Batch Loss: 0.1663133203983307\n",
      "Epoch 3395, Loss: 0.343656450510025, Final Batch Loss: 0.20295506715774536\n",
      "Epoch 3396, Loss: 0.3114427179098129, Final Batch Loss: 0.13952891528606415\n",
      "Epoch 3397, Loss: 0.2741575837135315, Final Batch Loss: 0.11870494484901428\n",
      "Epoch 3398, Loss: 0.35403643548488617, Final Batch Loss: 0.18122848868370056\n",
      "Epoch 3399, Loss: 0.3953847289085388, Final Batch Loss: 0.2235129475593567\n",
      "Epoch 3400, Loss: 0.3150210827589035, Final Batch Loss: 0.17843301594257355\n",
      "Epoch 3401, Loss: 0.22079893201589584, Final Batch Loss: 0.08790374547243118\n",
      "Epoch 3402, Loss: 0.26407990604639053, Final Batch Loss: 0.1208881065249443\n",
      "Epoch 3403, Loss: 0.3011693060398102, Final Batch Loss: 0.13351133465766907\n",
      "Epoch 3404, Loss: 0.4129607826471329, Final Batch Loss: 0.21845053136348724\n",
      "Epoch 3405, Loss: 0.30976706743240356, Final Batch Loss: 0.17791305482387543\n",
      "Epoch 3406, Loss: 0.28630003333091736, Final Batch Loss: 0.14246325194835663\n",
      "Epoch 3407, Loss: 0.4333435446023941, Final Batch Loss: 0.28454113006591797\n",
      "Epoch 3408, Loss: 0.30658451467752457, Final Batch Loss: 0.19743120670318604\n",
      "Epoch 3409, Loss: 0.28944486379623413, Final Batch Loss: 0.16093026101589203\n",
      "Epoch 3410, Loss: 0.2956143766641617, Final Batch Loss: 0.16092965006828308\n",
      "Epoch 3411, Loss: 0.31236255168914795, Final Batch Loss: 0.16790062189102173\n",
      "Epoch 3412, Loss: 0.3148077130317688, Final Batch Loss: 0.16420547664165497\n",
      "Epoch 3413, Loss: 0.32686176896095276, Final Batch Loss: 0.18341603875160217\n",
      "Epoch 3414, Loss: 0.27006152272224426, Final Batch Loss: 0.1276865154504776\n",
      "Epoch 3415, Loss: 0.34472307562828064, Final Batch Loss: 0.21016861498355865\n",
      "Epoch 3416, Loss: 0.3353687673807144, Final Batch Loss: 0.13303467631340027\n",
      "Epoch 3417, Loss: 0.31699930131435394, Final Batch Loss: 0.15849657356739044\n",
      "Epoch 3418, Loss: 0.28519223630428314, Final Batch Loss: 0.14052686095237732\n",
      "Epoch 3419, Loss: 0.3419654369354248, Final Batch Loss: 0.20081102848052979\n",
      "Epoch 3420, Loss: 0.30357587337493896, Final Batch Loss: 0.2032613456249237\n",
      "Epoch 3421, Loss: 0.3328340947628021, Final Batch Loss: 0.15232381224632263\n",
      "Epoch 3422, Loss: 0.3688724935054779, Final Batch Loss: 0.07698306441307068\n",
      "Epoch 3423, Loss: 0.3365660756826401, Final Batch Loss: 0.1868525892496109\n",
      "Epoch 3424, Loss: 0.3939405381679535, Final Batch Loss: 0.19369646906852722\n",
      "Epoch 3425, Loss: 0.2879454344511032, Final Batch Loss: 0.13170692324638367\n",
      "Epoch 3426, Loss: 0.3530757427215576, Final Batch Loss: 0.1557612419128418\n",
      "Epoch 3427, Loss: 0.26714230328798294, Final Batch Loss: 0.1210310235619545\n",
      "Epoch 3428, Loss: 0.2746397480368614, Final Batch Loss: 0.1232985183596611\n",
      "Epoch 3429, Loss: 0.3169020414352417, Final Batch Loss: 0.13088083267211914\n",
      "Epoch 3430, Loss: 0.33032073080539703, Final Batch Loss: 0.1463700830936432\n",
      "Epoch 3431, Loss: 0.32518261671066284, Final Batch Loss: 0.18244636058807373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3432, Loss: 0.30421018600463867, Final Batch Loss: 0.15849119424819946\n",
      "Epoch 3433, Loss: 0.30702945590019226, Final Batch Loss: 0.15106165409088135\n",
      "Epoch 3434, Loss: 0.3039245158433914, Final Batch Loss: 0.15060006082057953\n",
      "Epoch 3435, Loss: 0.28034505248069763, Final Batch Loss: 0.14691857993602753\n",
      "Epoch 3436, Loss: 0.2899524122476578, Final Batch Loss: 0.12468387186527252\n",
      "Epoch 3437, Loss: 0.2696790471673012, Final Batch Loss: 0.09167448431253433\n",
      "Epoch 3438, Loss: 0.30688466131687164, Final Batch Loss: 0.1749076247215271\n",
      "Epoch 3439, Loss: 0.3318476751446724, Final Batch Loss: 0.12129191309213638\n",
      "Epoch 3440, Loss: 0.35776035487651825, Final Batch Loss: 0.19604408740997314\n",
      "Epoch 3441, Loss: 0.3199666738510132, Final Batch Loss: 0.1470205932855606\n",
      "Epoch 3442, Loss: 0.2509324625134468, Final Batch Loss: 0.10964632779359818\n",
      "Epoch 3443, Loss: 0.30086036026477814, Final Batch Loss: 0.14895930886268616\n",
      "Epoch 3444, Loss: 0.3364047259092331, Final Batch Loss: 0.18299981951713562\n",
      "Epoch 3445, Loss: 0.2798963487148285, Final Batch Loss: 0.1490783393383026\n",
      "Epoch 3446, Loss: 0.27189961075782776, Final Batch Loss: 0.133416086435318\n",
      "Epoch 3447, Loss: 0.2929605543613434, Final Batch Loss: 0.1284378319978714\n",
      "Epoch 3448, Loss: 0.3078605681657791, Final Batch Loss: 0.1608743816614151\n",
      "Epoch 3449, Loss: 0.2989303320646286, Final Batch Loss: 0.16652612388134003\n",
      "Epoch 3450, Loss: 0.318941593170166, Final Batch Loss: 0.1398072987794876\n",
      "Epoch 3451, Loss: 0.28454068303108215, Final Batch Loss: 0.14868223667144775\n",
      "Epoch 3452, Loss: 0.3222145140171051, Final Batch Loss: 0.16180753707885742\n",
      "Epoch 3453, Loss: 0.3215167075395584, Final Batch Loss: 0.18361467123031616\n",
      "Epoch 3454, Loss: 0.3101649433374405, Final Batch Loss: 0.12968812882900238\n",
      "Epoch 3455, Loss: 0.2854281812906265, Final Batch Loss: 0.15377461910247803\n",
      "Epoch 3456, Loss: 0.3556966632604599, Final Batch Loss: 0.20383742451667786\n",
      "Epoch 3457, Loss: 0.3467729389667511, Final Batch Loss: 0.19179792702198029\n",
      "Epoch 3458, Loss: 0.3659511208534241, Final Batch Loss: 0.1789012849330902\n",
      "Epoch 3459, Loss: 0.34992973506450653, Final Batch Loss: 0.18757668137550354\n",
      "Epoch 3460, Loss: 0.2900223582983017, Final Batch Loss: 0.15686261653900146\n",
      "Epoch 3461, Loss: 0.3370985835790634, Final Batch Loss: 0.20949645340442657\n",
      "Epoch 3462, Loss: 0.3936312049627304, Final Batch Loss: 0.20341680943965912\n",
      "Epoch 3463, Loss: 0.29366621375083923, Final Batch Loss: 0.14912036061286926\n",
      "Epoch 3464, Loss: 0.3212178945541382, Final Batch Loss: 0.1815689653158188\n",
      "Epoch 3465, Loss: 0.2758183479309082, Final Batch Loss: 0.10765540599822998\n",
      "Epoch 3466, Loss: 0.28055180609226227, Final Batch Loss: 0.1167871505022049\n",
      "Epoch 3467, Loss: 0.3203175514936447, Final Batch Loss: 0.16505640745162964\n",
      "Epoch 3468, Loss: 0.2774790823459625, Final Batch Loss: 0.12511536478996277\n",
      "Epoch 3469, Loss: 0.3337913304567337, Final Batch Loss: 0.17517678439617157\n",
      "Epoch 3470, Loss: 0.3328566551208496, Final Batch Loss: 0.16724935173988342\n",
      "Epoch 3471, Loss: 0.2817107141017914, Final Batch Loss: 0.0784568339586258\n",
      "Epoch 3472, Loss: 0.3579055666923523, Final Batch Loss: 0.2232615202665329\n",
      "Epoch 3473, Loss: 0.3828852027654648, Final Batch Loss: 0.2139221727848053\n",
      "Epoch 3474, Loss: 0.3167400658130646, Final Batch Loss: 0.18165136873722076\n",
      "Epoch 3475, Loss: 0.3422132134437561, Final Batch Loss: 0.18046368658542633\n",
      "Epoch 3476, Loss: 0.271884486079216, Final Batch Loss: 0.1245100349187851\n",
      "Epoch 3477, Loss: 0.2742091193795204, Final Batch Loss: 0.10338994115591049\n",
      "Epoch 3478, Loss: 0.3421057313680649, Final Batch Loss: 0.16610555350780487\n",
      "Epoch 3479, Loss: 0.32986024022102356, Final Batch Loss: 0.18031565845012665\n",
      "Epoch 3480, Loss: 0.31530630588531494, Final Batch Loss: 0.16217292845249176\n",
      "Epoch 3481, Loss: 0.2953404188156128, Final Batch Loss: 0.143898606300354\n",
      "Epoch 3482, Loss: 0.327177494764328, Final Batch Loss: 0.14563654363155365\n",
      "Epoch 3483, Loss: 0.3515918701887131, Final Batch Loss: 0.20222298800945282\n",
      "Epoch 3484, Loss: 0.24436818808317184, Final Batch Loss: 0.10074201971292496\n",
      "Epoch 3485, Loss: 0.302147313952446, Final Batch Loss: 0.14599759876728058\n",
      "Epoch 3486, Loss: 0.2775207385420799, Final Batch Loss: 0.16648727655410767\n",
      "Epoch 3487, Loss: 0.31741680204868317, Final Batch Loss: 0.15685836970806122\n",
      "Epoch 3488, Loss: 0.3814173489809036, Final Batch Loss: 0.22712618112564087\n",
      "Epoch 3489, Loss: 0.3241371214389801, Final Batch Loss: 0.1534905731678009\n",
      "Epoch 3490, Loss: 0.3539063483476639, Final Batch Loss: 0.17597630620002747\n",
      "Epoch 3491, Loss: 0.2598217949271202, Final Batch Loss: 0.11281502991914749\n",
      "Epoch 3492, Loss: 0.36270567774772644, Final Batch Loss: 0.1999257653951645\n",
      "Epoch 3493, Loss: 0.32098108530044556, Final Batch Loss: 0.13775373995304108\n",
      "Epoch 3494, Loss: 0.34013527631759644, Final Batch Loss: 0.16510042548179626\n",
      "Epoch 3495, Loss: 0.3000546097755432, Final Batch Loss: 0.1483784317970276\n",
      "Epoch 3496, Loss: 0.2801762893795967, Final Batch Loss: 0.08672332018613815\n",
      "Epoch 3497, Loss: 0.2456970065832138, Final Batch Loss: 0.06993629038333893\n",
      "Epoch 3498, Loss: 0.26295603811740875, Final Batch Loss: 0.13074888288974762\n",
      "Epoch 3499, Loss: 0.3274963051080704, Final Batch Loss: 0.12871955335140228\n",
      "Epoch 3500, Loss: 0.23516902327537537, Final Batch Loss: 0.10427144169807434\n",
      "Epoch 3501, Loss: 0.2766995280981064, Final Batch Loss: 0.13892149925231934\n",
      "Epoch 3502, Loss: 0.29285797476768494, Final Batch Loss: 0.16601702570915222\n",
      "Epoch 3503, Loss: 0.3276073932647705, Final Batch Loss: 0.17445899546146393\n",
      "Epoch 3504, Loss: 0.25661200284957886, Final Batch Loss: 0.13302043080329895\n",
      "Epoch 3505, Loss: 0.24857990443706512, Final Batch Loss: 0.10509875416755676\n",
      "Epoch 3506, Loss: 0.39050982892513275, Final Batch Loss: 0.16303624212741852\n",
      "Epoch 3507, Loss: 0.28025174885988235, Final Batch Loss: 0.11839691549539566\n",
      "Epoch 3508, Loss: 0.29951246082782745, Final Batch Loss: 0.14814545214176178\n",
      "Epoch 3509, Loss: 0.2515149414539337, Final Batch Loss: 0.11107952892780304\n",
      "Epoch 3510, Loss: 0.25824420154094696, Final Batch Loss: 0.1281578242778778\n",
      "Epoch 3511, Loss: 0.2819715142250061, Final Batch Loss: 0.12422795593738556\n",
      "Epoch 3512, Loss: 0.32651205360889435, Final Batch Loss: 0.150621697306633\n",
      "Epoch 3513, Loss: 0.33329659700393677, Final Batch Loss: 0.14618681371212006\n",
      "Epoch 3514, Loss: 0.3112735003232956, Final Batch Loss: 0.16222698986530304\n",
      "Epoch 3515, Loss: 0.29365255683660507, Final Batch Loss: 0.17492935061454773\n",
      "Epoch 3516, Loss: 0.24838106334209442, Final Batch Loss: 0.111565962433815\n",
      "Epoch 3517, Loss: 0.3248318284749985, Final Batch Loss: 0.17933997511863708\n",
      "Epoch 3518, Loss: 0.282809741795063, Final Batch Loss: 0.179495170712471\n",
      "Epoch 3519, Loss: 0.3208855092525482, Final Batch Loss: 0.15522508323192596\n",
      "Epoch 3520, Loss: 0.29055558890104294, Final Batch Loss: 0.10690892487764359\n",
      "Epoch 3521, Loss: 0.373908668756485, Final Batch Loss: 0.1635495126247406\n",
      "Epoch 3522, Loss: 0.2513163983821869, Final Batch Loss: 0.10856842994689941\n",
      "Epoch 3523, Loss: 0.26015230268239975, Final Batch Loss: 0.11293280869722366\n",
      "Epoch 3524, Loss: 0.38226863741874695, Final Batch Loss: 0.22183354198932648\n",
      "Epoch 3525, Loss: 0.2610504701733589, Final Batch Loss: 0.1173601970076561\n",
      "Epoch 3526, Loss: 0.2550078257918358, Final Batch Loss: 0.11918509751558304\n",
      "Epoch 3527, Loss: 0.2866505980491638, Final Batch Loss: 0.14069785177707672\n",
      "Epoch 3528, Loss: 0.24608654528856277, Final Batch Loss: 0.08671817928552628\n",
      "Epoch 3529, Loss: 0.2552367076277733, Final Batch Loss: 0.10755639523267746\n",
      "Epoch 3530, Loss: 0.3277307450771332, Final Batch Loss: 0.18888606131076813\n",
      "Epoch 3531, Loss: 0.2750242277979851, Final Batch Loss: 0.10582748800516129\n",
      "Epoch 3532, Loss: 0.3236835151910782, Final Batch Loss: 0.1868269443511963\n",
      "Epoch 3533, Loss: 0.34210966527462006, Final Batch Loss: 0.16822059452533722\n",
      "Epoch 3534, Loss: 0.31152619421482086, Final Batch Loss: 0.13386817276477814\n",
      "Epoch 3535, Loss: 0.3287432789802551, Final Batch Loss: 0.13412460684776306\n",
      "Epoch 3536, Loss: 0.3729089945554733, Final Batch Loss: 0.2196640968322754\n",
      "Epoch 3537, Loss: 0.32520677149295807, Final Batch Loss: 0.16268816590309143\n",
      "Epoch 3538, Loss: 0.29638567566871643, Final Batch Loss: 0.16184557974338531\n",
      "Epoch 3539, Loss: 0.32403217256069183, Final Batch Loss: 0.2030840367078781\n",
      "Epoch 3540, Loss: 0.2595030218362808, Final Batch Loss: 0.12736405432224274\n",
      "Epoch 3541, Loss: 0.3134455010294914, Final Batch Loss: 0.19354698061943054\n",
      "Epoch 3542, Loss: 0.23764877766370773, Final Batch Loss: 0.09679686278104782\n",
      "Epoch 3543, Loss: 0.29480113089084625, Final Batch Loss: 0.17580284178256989\n",
      "Epoch 3544, Loss: 0.2445085197687149, Final Batch Loss: 0.11622351408004761\n",
      "Epoch 3545, Loss: 0.28358835726976395, Final Batch Loss: 0.1225426122546196\n",
      "Epoch 3546, Loss: 0.24971428513526917, Final Batch Loss: 0.1130731850862503\n",
      "Epoch 3547, Loss: 0.2990131676197052, Final Batch Loss: 0.16131825745105743\n",
      "Epoch 3548, Loss: 0.29562707245349884, Final Batch Loss: 0.14067839086055756\n",
      "Epoch 3549, Loss: 0.3821554332971573, Final Batch Loss: 0.19831734895706177\n",
      "Epoch 3550, Loss: 0.3686687648296356, Final Batch Loss: 0.18546327948570251\n",
      "Epoch 3551, Loss: 0.22561337798833847, Final Batch Loss: 0.08381672948598862\n",
      "Epoch 3552, Loss: 0.3244284614920616, Final Batch Loss: 0.11961018294095993\n",
      "Epoch 3553, Loss: 0.32918089628219604, Final Batch Loss: 0.14804746210575104\n",
      "Epoch 3554, Loss: 0.3954378515481949, Final Batch Loss: 0.24768152832984924\n",
      "Epoch 3555, Loss: 0.31114859879016876, Final Batch Loss: 0.1595025658607483\n",
      "Epoch 3556, Loss: 0.2491418570280075, Final Batch Loss: 0.13008663058280945\n",
      "Epoch 3557, Loss: 0.3549913316965103, Final Batch Loss: 0.20312967896461487\n",
      "Epoch 3558, Loss: 0.35420481860637665, Final Batch Loss: 0.1833045780658722\n",
      "Epoch 3559, Loss: 0.3277183771133423, Final Batch Loss: 0.18665046989917755\n",
      "Epoch 3560, Loss: 0.37383319437503815, Final Batch Loss: 0.23104330897331238\n",
      "Epoch 3561, Loss: 0.3520284965634346, Final Batch Loss: 0.24390250444412231\n",
      "Epoch 3562, Loss: 0.2621748819947243, Final Batch Loss: 0.11777787655591965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3563, Loss: 0.3883369565010071, Final Batch Loss: 0.19695961475372314\n",
      "Epoch 3564, Loss: 0.3044092282652855, Final Batch Loss: 0.11598148196935654\n",
      "Epoch 3565, Loss: 0.3220144212245941, Final Batch Loss: 0.18316154181957245\n",
      "Epoch 3566, Loss: 0.28372301161289215, Final Batch Loss: 0.15510433912277222\n",
      "Epoch 3567, Loss: 0.3240040987730026, Final Batch Loss: 0.1623666137456894\n",
      "Epoch 3568, Loss: 0.3085411489009857, Final Batch Loss: 0.18304099142551422\n",
      "Epoch 3569, Loss: 0.2367323562502861, Final Batch Loss: 0.08376576751470566\n",
      "Epoch 3570, Loss: 0.27673089504241943, Final Batch Loss: 0.12385670840740204\n",
      "Epoch 3571, Loss: 0.3183424025774002, Final Batch Loss: 0.19263657927513123\n",
      "Epoch 3572, Loss: 0.2762719839811325, Final Batch Loss: 0.15203750133514404\n",
      "Epoch 3573, Loss: 0.3257938623428345, Final Batch Loss: 0.15283265709877014\n",
      "Epoch 3574, Loss: 0.2734304517507553, Final Batch Loss: 0.14085420966148376\n",
      "Epoch 3575, Loss: 0.29812783002853394, Final Batch Loss: 0.14287100732326508\n",
      "Epoch 3576, Loss: 0.2803105562925339, Final Batch Loss: 0.13267479836940765\n",
      "Epoch 3577, Loss: 0.2851584851741791, Final Batch Loss: 0.16207170486450195\n",
      "Epoch 3578, Loss: 0.25670118629932404, Final Batch Loss: 0.09752444922924042\n",
      "Epoch 3579, Loss: 0.2969675734639168, Final Batch Loss: 0.09466322511434555\n",
      "Epoch 3580, Loss: 0.2837304025888443, Final Batch Loss: 0.13230621814727783\n",
      "Epoch 3581, Loss: 0.30750036984682083, Final Batch Loss: 0.19669805467128754\n",
      "Epoch 3582, Loss: 0.29254619777202606, Final Batch Loss: 0.1658187061548233\n",
      "Epoch 3583, Loss: 0.3708776533603668, Final Batch Loss: 0.19590044021606445\n",
      "Epoch 3584, Loss: 0.32208220660686493, Final Batch Loss: 0.14923323690891266\n",
      "Epoch 3585, Loss: 0.2536795362830162, Final Batch Loss: 0.10538313537836075\n",
      "Epoch 3586, Loss: 0.28706008195877075, Final Batch Loss: 0.12147974967956543\n",
      "Epoch 3587, Loss: 0.2957383766770363, Final Batch Loss: 0.11273422092199326\n",
      "Epoch 3588, Loss: 0.24610309302806854, Final Batch Loss: 0.13809923827648163\n",
      "Epoch 3589, Loss: 0.3812401443719864, Final Batch Loss: 0.16879326105117798\n",
      "Epoch 3590, Loss: 0.28716257959604263, Final Batch Loss: 0.11155729740858078\n",
      "Epoch 3591, Loss: 0.26623231172561646, Final Batch Loss: 0.125271737575531\n",
      "Epoch 3592, Loss: 0.30953627824783325, Final Batch Loss: 0.16064389050006866\n",
      "Epoch 3593, Loss: 0.39232318103313446, Final Batch Loss: 0.2157486230134964\n",
      "Epoch 3594, Loss: 0.3414160907268524, Final Batch Loss: 0.16794367134571075\n",
      "Epoch 3595, Loss: 0.34848763048648834, Final Batch Loss: 0.17980016767978668\n",
      "Epoch 3596, Loss: 0.28447867184877396, Final Batch Loss: 0.11024516075849533\n",
      "Epoch 3597, Loss: 0.35929566621780396, Final Batch Loss: 0.14965030550956726\n",
      "Epoch 3598, Loss: 0.2539181262254715, Final Batch Loss: 0.08277443051338196\n",
      "Epoch 3599, Loss: 0.2846674472093582, Final Batch Loss: 0.12753504514694214\n",
      "Epoch 3600, Loss: 0.3274165987968445, Final Batch Loss: 0.15060292184352875\n",
      "Epoch 3601, Loss: 0.3095761388540268, Final Batch Loss: 0.14047440886497498\n",
      "Epoch 3602, Loss: 0.31670477986335754, Final Batch Loss: 0.16984865069389343\n",
      "Epoch 3603, Loss: 0.32033713161945343, Final Batch Loss: 0.1646711677312851\n",
      "Epoch 3604, Loss: 0.28949204087257385, Final Batch Loss: 0.13879378139972687\n",
      "Epoch 3605, Loss: 0.27280673384666443, Final Batch Loss: 0.14165113866329193\n",
      "Epoch 3606, Loss: 0.28796470165252686, Final Batch Loss: 0.1605088710784912\n",
      "Epoch 3607, Loss: 0.30741043388843536, Final Batch Loss: 0.15318945050239563\n",
      "Epoch 3608, Loss: 0.31085893511772156, Final Batch Loss: 0.17672812938690186\n",
      "Epoch 3609, Loss: 0.2930728793144226, Final Batch Loss: 0.1276886910200119\n",
      "Epoch 3610, Loss: 0.4493337571620941, Final Batch Loss: 0.3190162479877472\n",
      "Epoch 3611, Loss: 0.2922680974006653, Final Batch Loss: 0.1513841450214386\n",
      "Epoch 3612, Loss: 0.2917511612176895, Final Batch Loss: 0.13529710471630096\n",
      "Epoch 3613, Loss: 0.3207009732723236, Final Batch Loss: 0.17661812901496887\n",
      "Epoch 3614, Loss: 0.300359807908535, Final Batch Loss: 0.11963864415884018\n",
      "Epoch 3615, Loss: 0.3435988128185272, Final Batch Loss: 0.2035175859928131\n",
      "Epoch 3616, Loss: 0.30025799572467804, Final Batch Loss: 0.10818508267402649\n",
      "Epoch 3617, Loss: 0.29081372916698456, Final Batch Loss: 0.14380595088005066\n",
      "Epoch 3618, Loss: 0.3502374589443207, Final Batch Loss: 0.2302592396736145\n",
      "Epoch 3619, Loss: 0.3359108716249466, Final Batch Loss: 0.19406822323799133\n",
      "Epoch 3620, Loss: 0.30309879034757614, Final Batch Loss: 0.08628594130277634\n",
      "Epoch 3621, Loss: 0.24714139848947525, Final Batch Loss: 0.09400684386491776\n",
      "Epoch 3622, Loss: 0.34399887919425964, Final Batch Loss: 0.12607045471668243\n",
      "Epoch 3623, Loss: 0.3271147608757019, Final Batch Loss: 0.16592076420783997\n",
      "Epoch 3624, Loss: 0.379937008023262, Final Batch Loss: 0.1407182365655899\n",
      "Epoch 3625, Loss: 0.29608266800642014, Final Batch Loss: 0.12103427201509476\n",
      "Epoch 3626, Loss: 0.3515204191207886, Final Batch Loss: 0.16773442924022675\n",
      "Epoch 3627, Loss: 0.34779173135757446, Final Batch Loss: 0.16908042132854462\n",
      "Epoch 3628, Loss: 0.2241768091917038, Final Batch Loss: 0.0916789174079895\n",
      "Epoch 3629, Loss: 0.3192460536956787, Final Batch Loss: 0.15618589520454407\n",
      "Epoch 3630, Loss: 0.26787321269512177, Final Batch Loss: 0.10481472313404083\n",
      "Epoch 3631, Loss: 0.2883695289492607, Final Batch Loss: 0.17567743360996246\n",
      "Epoch 3632, Loss: 0.2595196068286896, Final Batch Loss: 0.1378304362297058\n",
      "Epoch 3633, Loss: 0.295780673623085, Final Batch Loss: 0.1482282429933548\n",
      "Epoch 3634, Loss: 0.31930334866046906, Final Batch Loss: 0.16772325336933136\n",
      "Epoch 3635, Loss: 0.2824675664305687, Final Batch Loss: 0.11488261073827744\n",
      "Epoch 3636, Loss: 0.23916032165288925, Final Batch Loss: 0.11306124180555344\n",
      "Epoch 3637, Loss: 0.3495381027460098, Final Batch Loss: 0.20414479076862335\n",
      "Epoch 3638, Loss: 0.2959446832537651, Final Batch Loss: 0.12316765636205673\n",
      "Epoch 3639, Loss: 0.23715321719646454, Final Batch Loss: 0.09748953580856323\n",
      "Epoch 3640, Loss: 0.30842019617557526, Final Batch Loss: 0.17421121895313263\n",
      "Epoch 3641, Loss: 0.2884697765111923, Final Batch Loss: 0.1325923204421997\n",
      "Epoch 3642, Loss: 0.26294684410095215, Final Batch Loss: 0.1190677285194397\n",
      "Epoch 3643, Loss: 0.2880528122186661, Final Batch Loss: 0.1285950392484665\n",
      "Epoch 3644, Loss: 0.26536107808351517, Final Batch Loss: 0.1461888700723648\n",
      "Epoch 3645, Loss: 0.27351853251457214, Final Batch Loss: 0.14477907121181488\n",
      "Epoch 3646, Loss: 0.2789350152015686, Final Batch Loss: 0.15329627692699432\n",
      "Epoch 3647, Loss: 0.3054383397102356, Final Batch Loss: 0.12775468826293945\n",
      "Epoch 3648, Loss: 0.29815153777599335, Final Batch Loss: 0.16126812994480133\n",
      "Epoch 3649, Loss: 0.36391739547252655, Final Batch Loss: 0.22770345211029053\n",
      "Epoch 3650, Loss: 0.2586214765906334, Final Batch Loss: 0.12180431932210922\n",
      "Epoch 3651, Loss: 0.35471242666244507, Final Batch Loss: 0.19347813725471497\n",
      "Epoch 3652, Loss: 0.25905653089284897, Final Batch Loss: 0.0976385846734047\n",
      "Epoch 3653, Loss: 0.3115149587392807, Final Batch Loss: 0.17313826084136963\n",
      "Epoch 3654, Loss: 0.2822449207305908, Final Batch Loss: 0.13087508082389832\n",
      "Epoch 3655, Loss: 0.2826912999153137, Final Batch Loss: 0.14321096241474152\n",
      "Epoch 3656, Loss: 0.31018826365470886, Final Batch Loss: 0.18316861987113953\n",
      "Epoch 3657, Loss: 0.30409926176071167, Final Batch Loss: 0.13215382397174835\n",
      "Epoch 3658, Loss: 0.29365451633930206, Final Batch Loss: 0.16871441900730133\n",
      "Epoch 3659, Loss: 0.36088743805885315, Final Batch Loss: 0.2269338220357895\n",
      "Epoch 3660, Loss: 0.2305569425225258, Final Batch Loss: 0.10506308823823929\n",
      "Epoch 3661, Loss: 0.29122889041900635, Final Batch Loss: 0.16549910604953766\n",
      "Epoch 3662, Loss: 0.26940231770277023, Final Batch Loss: 0.14935514330863953\n",
      "Epoch 3663, Loss: 0.2149357870221138, Final Batch Loss: 0.11570438742637634\n",
      "Epoch 3664, Loss: 0.33923865854740143, Final Batch Loss: 0.1589352935552597\n",
      "Epoch 3665, Loss: 0.29424501955509186, Final Batch Loss: 0.1430147886276245\n",
      "Epoch 3666, Loss: 0.32743634283542633, Final Batch Loss: 0.1842700093984604\n",
      "Epoch 3667, Loss: 0.34688839316368103, Final Batch Loss: 0.22311656177043915\n",
      "Epoch 3668, Loss: 0.30810435116291046, Final Batch Loss: 0.17199914157390594\n",
      "Epoch 3669, Loss: 0.3904673010110855, Final Batch Loss: 0.24849267303943634\n",
      "Epoch 3670, Loss: 0.3728112429380417, Final Batch Loss: 0.21636086702346802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3671, Loss: 0.2649761438369751, Final Batch Loss: 0.11408679187297821\n",
      "Epoch 3672, Loss: 0.29311956465244293, Final Batch Loss: 0.14292673766613007\n",
      "Epoch 3673, Loss: 0.30979736149311066, Final Batch Loss: 0.14091864228248596\n",
      "Epoch 3674, Loss: 0.43911541998386383, Final Batch Loss: 0.2944895923137665\n",
      "Epoch 3675, Loss: 0.3031318038702011, Final Batch Loss: 0.1682588756084442\n",
      "Epoch 3676, Loss: 0.2670424357056618, Final Batch Loss: 0.11520897597074509\n",
      "Epoch 3677, Loss: 0.28702129423618317, Final Batch Loss: 0.1579875946044922\n",
      "Epoch 3678, Loss: 0.30652226507663727, Final Batch Loss: 0.1534261256456375\n",
      "Epoch 3679, Loss: 0.28920404613018036, Final Batch Loss: 0.12634089589118958\n",
      "Epoch 3680, Loss: 0.26070500165224075, Final Batch Loss: 0.1417182981967926\n",
      "Epoch 3681, Loss: 0.3487890809774399, Final Batch Loss: 0.21673829853534698\n",
      "Epoch 3682, Loss: 0.278559610247612, Final Batch Loss: 0.09488342702388763\n",
      "Epoch 3683, Loss: 0.30080293118953705, Final Batch Loss: 0.14131349325180054\n",
      "Epoch 3684, Loss: 0.2567066252231598, Final Batch Loss: 0.14017483592033386\n",
      "Epoch 3685, Loss: 0.24640730023384094, Final Batch Loss: 0.10307779908180237\n",
      "Epoch 3686, Loss: 0.32718007266521454, Final Batch Loss: 0.1938326060771942\n",
      "Epoch 3687, Loss: 0.33605392277240753, Final Batch Loss: 0.20071151852607727\n",
      "Epoch 3688, Loss: 0.24276850372552872, Final Batch Loss: 0.09694775193929672\n",
      "Epoch 3689, Loss: 0.3039829432964325, Final Batch Loss: 0.13444435596466064\n",
      "Epoch 3690, Loss: 0.3279830068349838, Final Batch Loss: 0.14760912954807281\n",
      "Epoch 3691, Loss: 0.25980865210294724, Final Batch Loss: 0.09860610216856003\n",
      "Epoch 3692, Loss: 0.3187347576022148, Final Batch Loss: 0.21096841990947723\n",
      "Epoch 3693, Loss: 0.3176575005054474, Final Batch Loss: 0.16110770404338837\n",
      "Epoch 3694, Loss: 0.287382036447525, Final Batch Loss: 0.15323702991008759\n",
      "Epoch 3695, Loss: 0.42370906472206116, Final Batch Loss: 0.2690178155899048\n",
      "Epoch 3696, Loss: 0.2863392308354378, Final Batch Loss: 0.118179552257061\n",
      "Epoch 3697, Loss: 0.23982392996549606, Final Batch Loss: 0.10161105543375015\n",
      "Epoch 3698, Loss: 0.25560547411441803, Final Batch Loss: 0.14617864787578583\n",
      "Epoch 3699, Loss: 0.31459707021713257, Final Batch Loss: 0.16029299795627594\n",
      "Epoch 3700, Loss: 0.2849140912294388, Final Batch Loss: 0.14341984689235687\n",
      "Epoch 3701, Loss: 0.2760184109210968, Final Batch Loss: 0.14838430285453796\n",
      "Epoch 3702, Loss: 0.23558280616998672, Final Batch Loss: 0.09602408856153488\n",
      "Epoch 3703, Loss: 0.2858160436153412, Final Batch Loss: 0.1574500948190689\n",
      "Epoch 3704, Loss: 0.2742956429719925, Final Batch Loss: 0.14419005811214447\n",
      "Epoch 3705, Loss: 0.3761757016181946, Final Batch Loss: 0.21877264976501465\n",
      "Epoch 3706, Loss: 0.32545362412929535, Final Batch Loss: 0.16031059622764587\n",
      "Epoch 3707, Loss: 0.31822744756937027, Final Batch Loss: 0.20572608709335327\n",
      "Epoch 3708, Loss: 0.28055785596370697, Final Batch Loss: 0.13951222598552704\n",
      "Epoch 3709, Loss: 0.27022748440504074, Final Batch Loss: 0.11732826381921768\n",
      "Epoch 3710, Loss: 0.2247055470943451, Final Batch Loss: 0.07706379890441895\n",
      "Epoch 3711, Loss: 0.3280743509531021, Final Batch Loss: 0.16464689373970032\n",
      "Epoch 3712, Loss: 0.2779385447502136, Final Batch Loss: 0.13375528156757355\n",
      "Epoch 3713, Loss: 0.3278331458568573, Final Batch Loss: 0.18145522475242615\n",
      "Epoch 3714, Loss: 0.29605965316295624, Final Batch Loss: 0.15311354398727417\n",
      "Epoch 3715, Loss: 0.27448420226573944, Final Batch Loss: 0.13390745222568512\n",
      "Epoch 3716, Loss: 0.24222508072853088, Final Batch Loss: 0.0973101556301117\n",
      "Epoch 3717, Loss: 0.27983932197093964, Final Batch Loss: 0.11563785374164581\n",
      "Epoch 3718, Loss: 0.2866663336753845, Final Batch Loss: 0.16603796184062958\n",
      "Epoch 3719, Loss: 0.33508817851543427, Final Batch Loss: 0.14010189473628998\n",
      "Epoch 3720, Loss: 0.3712485283613205, Final Batch Loss: 0.21531130373477936\n",
      "Epoch 3721, Loss: 0.2593507021665573, Final Batch Loss: 0.1290135383605957\n",
      "Epoch 3722, Loss: 0.2723708301782608, Final Batch Loss: 0.1330237090587616\n",
      "Epoch 3723, Loss: 0.26920678466558456, Final Batch Loss: 0.14543016254901886\n",
      "Epoch 3724, Loss: 0.29471255838871, Final Batch Loss: 0.15302057564258575\n",
      "Epoch 3725, Loss: 0.26739703118801117, Final Batch Loss: 0.09284636378288269\n",
      "Epoch 3726, Loss: 0.26528219878673553, Final Batch Loss: 0.12535622715950012\n",
      "Epoch 3727, Loss: 0.2564166933298111, Final Batch Loss: 0.10903723537921906\n",
      "Epoch 3728, Loss: 0.28966765105724335, Final Batch Loss: 0.16115617752075195\n",
      "Epoch 3729, Loss: 0.2866784334182739, Final Batch Loss: 0.13620778918266296\n",
      "Epoch 3730, Loss: 0.3165244460105896, Final Batch Loss: 0.14859339594841003\n",
      "Epoch 3731, Loss: 0.2834663391113281, Final Batch Loss: 0.16245940327644348\n",
      "Epoch 3732, Loss: 0.3225029408931732, Final Batch Loss: 0.15978577733039856\n",
      "Epoch 3733, Loss: 0.30155596137046814, Final Batch Loss: 0.1282847672700882\n",
      "Epoch 3734, Loss: 0.26376578211784363, Final Batch Loss: 0.13046012818813324\n",
      "Epoch 3735, Loss: 0.2928169369697571, Final Batch Loss: 0.12764038145542145\n",
      "Epoch 3736, Loss: 0.29529836773872375, Final Batch Loss: 0.16099420189857483\n",
      "Epoch 3737, Loss: 0.2536025792360306, Final Batch Loss: 0.12234954535961151\n",
      "Epoch 3738, Loss: 0.3002038300037384, Final Batch Loss: 0.15972889959812164\n",
      "Epoch 3739, Loss: 0.3001880347728729, Final Batch Loss: 0.13586561381816864\n",
      "Epoch 3740, Loss: 0.2926242798566818, Final Batch Loss: 0.14526748657226562\n",
      "Epoch 3741, Loss: 0.33152488619089127, Final Batch Loss: 0.2106521725654602\n",
      "Epoch 3742, Loss: 0.2782858610153198, Final Batch Loss: 0.11144724488258362\n",
      "Epoch 3743, Loss: 0.28900016844272614, Final Batch Loss: 0.133745476603508\n",
      "Epoch 3744, Loss: 0.2717228829860687, Final Batch Loss: 0.1413019448518753\n",
      "Epoch 3745, Loss: 0.24126072973012924, Final Batch Loss: 0.11562075465917587\n",
      "Epoch 3746, Loss: 0.2689168155193329, Final Batch Loss: 0.1292954832315445\n",
      "Epoch 3747, Loss: 0.3156198039650917, Final Batch Loss: 0.11750995367765427\n",
      "Epoch 3748, Loss: 0.27132782340049744, Final Batch Loss: 0.13463374972343445\n",
      "Epoch 3749, Loss: 0.327824130654335, Final Batch Loss: 0.19700421392917633\n",
      "Epoch 3750, Loss: 0.3149099797010422, Final Batch Loss: 0.14594441652297974\n",
      "Epoch 3751, Loss: 0.31017981469631195, Final Batch Loss: 0.16568605601787567\n",
      "Epoch 3752, Loss: 0.24607928097248077, Final Batch Loss: 0.09760153293609619\n",
      "Epoch 3753, Loss: 0.29022514820098877, Final Batch Loss: 0.12043580412864685\n",
      "Epoch 3754, Loss: 0.2899944335222244, Final Batch Loss: 0.15481328964233398\n",
      "Epoch 3755, Loss: 0.3487117439508438, Final Batch Loss: 0.1577785462141037\n",
      "Epoch 3756, Loss: 0.21840592473745346, Final Batch Loss: 0.055304042994976044\n",
      "Epoch 3757, Loss: 0.41872913390398026, Final Batch Loss: 0.29737499356269836\n",
      "Epoch 3758, Loss: 0.22018550336360931, Final Batch Loss: 0.10470262914896011\n",
      "Epoch 3759, Loss: 0.28762298822402954, Final Batch Loss: 0.1644415408372879\n",
      "Epoch 3760, Loss: 0.3087787479162216, Final Batch Loss: 0.16127555072307587\n",
      "Epoch 3761, Loss: 0.25732147693634033, Final Batch Loss: 0.09441372752189636\n",
      "Epoch 3762, Loss: 0.30392754077911377, Final Batch Loss: 0.16556347906589508\n",
      "Epoch 3763, Loss: 0.2494249939918518, Final Batch Loss: 0.0885087102651596\n",
      "Epoch 3764, Loss: 0.2770887389779091, Final Batch Loss: 0.12275069206953049\n",
      "Epoch 3765, Loss: 0.23943448811769485, Final Batch Loss: 0.10894721001386642\n",
      "Epoch 3766, Loss: 0.24464628100395203, Final Batch Loss: 0.10302826762199402\n",
      "Epoch 3767, Loss: 0.22764690220355988, Final Batch Loss: 0.07298672199249268\n",
      "Epoch 3768, Loss: 0.241498202085495, Final Batch Loss: 0.11782489717006683\n",
      "Epoch 3769, Loss: 0.3813009709119797, Final Batch Loss: 0.2344638854265213\n",
      "Epoch 3770, Loss: 0.27552779018878937, Final Batch Loss: 0.14731574058532715\n",
      "Epoch 3771, Loss: 0.24054189771413803, Final Batch Loss: 0.08587769418954849\n",
      "Epoch 3772, Loss: 0.3239297717809677, Final Batch Loss: 0.17534089088439941\n",
      "Epoch 3773, Loss: 0.3385046571493149, Final Batch Loss: 0.16864392161369324\n",
      "Epoch 3774, Loss: 0.3366514891386032, Final Batch Loss: 0.14636652171611786\n",
      "Epoch 3775, Loss: 0.395721435546875, Final Batch Loss: 0.16581816971302032\n",
      "Epoch 3776, Loss: 0.3489791303873062, Final Batch Loss: 0.14259368181228638\n",
      "Epoch 3777, Loss: 0.3270297348499298, Final Batch Loss: 0.18662764132022858\n",
      "Epoch 3778, Loss: 0.2760121673345566, Final Batch Loss: 0.14777851104736328\n",
      "Epoch 3779, Loss: 0.29440081864595413, Final Batch Loss: 0.08292800933122635\n",
      "Epoch 3780, Loss: 0.34217222034931183, Final Batch Loss: 0.21275393664836884\n",
      "Epoch 3781, Loss: 0.3215746060013771, Final Batch Loss: 0.19930601119995117\n",
      "Epoch 3782, Loss: 0.2934802919626236, Final Batch Loss: 0.12891817092895508\n",
      "Epoch 3783, Loss: 0.2968427911400795, Final Batch Loss: 0.180599644780159\n",
      "Epoch 3784, Loss: 0.2795492559671402, Final Batch Loss: 0.15246185660362244\n",
      "Epoch 3785, Loss: 0.2608384042978287, Final Batch Loss: 0.12895071506500244\n",
      "Epoch 3786, Loss: 0.35053756833076477, Final Batch Loss: 0.21295969188213348\n",
      "Epoch 3787, Loss: 0.29889968037605286, Final Batch Loss: 0.1558370441198349\n",
      "Epoch 3788, Loss: 0.3053596615791321, Final Batch Loss: 0.17642508447170258\n",
      "Epoch 3789, Loss: 0.28873711824417114, Final Batch Loss: 0.11834663152694702\n",
      "Epoch 3790, Loss: 0.42847755551338196, Final Batch Loss: 0.22510279715061188\n",
      "Epoch 3791, Loss: 0.2769622206687927, Final Batch Loss: 0.1439518928527832\n",
      "Epoch 3792, Loss: 0.25873401015996933, Final Batch Loss: 0.11568618565797806\n",
      "Epoch 3793, Loss: 0.24733654409646988, Final Batch Loss: 0.12553489208221436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3794, Loss: 0.30671875923871994, Final Batch Loss: 0.11542008072137833\n",
      "Epoch 3795, Loss: 0.31282586604356766, Final Batch Loss: 0.12333781272172928\n",
      "Epoch 3796, Loss: 0.2546195536851883, Final Batch Loss: 0.13085602223873138\n",
      "Epoch 3797, Loss: 0.29029381275177, Final Batch Loss: 0.14554142951965332\n",
      "Epoch 3798, Loss: 0.2998803108930588, Final Batch Loss: 0.14801323413848877\n",
      "Epoch 3799, Loss: 0.37349042296409607, Final Batch Loss: 0.161627858877182\n",
      "Epoch 3800, Loss: 0.28075337409973145, Final Batch Loss: 0.13380135595798492\n",
      "Epoch 3801, Loss: 0.34075409173965454, Final Batch Loss: 0.19099970161914825\n",
      "Epoch 3802, Loss: 0.46019019186496735, Final Batch Loss: 0.3019275367259979\n",
      "Epoch 3803, Loss: 0.29734523594379425, Final Batch Loss: 0.14786817133426666\n",
      "Epoch 3804, Loss: 0.2733283042907715, Final Batch Loss: 0.12897223234176636\n",
      "Epoch 3805, Loss: 0.26207148283720016, Final Batch Loss: 0.13859380781650543\n",
      "Epoch 3806, Loss: 0.2639337033033371, Final Batch Loss: 0.14059144258499146\n",
      "Epoch 3807, Loss: 0.29007096588611603, Final Batch Loss: 0.15776671469211578\n",
      "Epoch 3808, Loss: 0.32439522445201874, Final Batch Loss: 0.17113153636455536\n",
      "Epoch 3809, Loss: 0.318088598549366, Final Batch Loss: 0.12038271874189377\n",
      "Epoch 3810, Loss: 0.28570038080215454, Final Batch Loss: 0.15730947256088257\n",
      "Epoch 3811, Loss: 0.3135620653629303, Final Batch Loss: 0.17288507521152496\n",
      "Epoch 3812, Loss: 0.30084554851055145, Final Batch Loss: 0.13747109472751617\n",
      "Epoch 3813, Loss: 0.394148126244545, Final Batch Loss: 0.20419929921627045\n",
      "Epoch 3814, Loss: 0.31555669009685516, Final Batch Loss: 0.13263502717018127\n",
      "Epoch 3815, Loss: 0.22125958651304245, Final Batch Loss: 0.0900011882185936\n",
      "Epoch 3816, Loss: 0.27411743253469467, Final Batch Loss: 0.09393610805273056\n",
      "Epoch 3817, Loss: 0.34164123237133026, Final Batch Loss: 0.1889556646347046\n",
      "Epoch 3818, Loss: 0.2788897678256035, Final Batch Loss: 0.116826631128788\n",
      "Epoch 3819, Loss: 0.2699667364358902, Final Batch Loss: 0.14993155002593994\n",
      "Epoch 3820, Loss: 0.30534669756889343, Final Batch Loss: 0.1351056843996048\n",
      "Epoch 3821, Loss: 0.32772399485111237, Final Batch Loss: 0.16981308162212372\n",
      "Epoch 3822, Loss: 0.20525529235601425, Final Batch Loss: 0.07309649139642715\n",
      "Epoch 3823, Loss: 0.27127474546432495, Final Batch Loss: 0.1384340077638626\n",
      "Epoch 3824, Loss: 0.2907865643501282, Final Batch Loss: 0.11969895660877228\n",
      "Epoch 3825, Loss: 0.2680722773075104, Final Batch Loss: 0.13241659104824066\n",
      "Epoch 3826, Loss: 0.2719847559928894, Final Batch Loss: 0.13735783100128174\n",
      "Epoch 3827, Loss: 0.29804979264736176, Final Batch Loss: 0.14925958216190338\n",
      "Epoch 3828, Loss: 0.360861599445343, Final Batch Loss: 0.1980576068162918\n",
      "Epoch 3829, Loss: 0.26294782757759094, Final Batch Loss: 0.12721841037273407\n",
      "Epoch 3830, Loss: 0.3411606550216675, Final Batch Loss: 0.18968632817268372\n",
      "Epoch 3831, Loss: 0.2868211716413498, Final Batch Loss: 0.14695799350738525\n",
      "Epoch 3832, Loss: 0.3105614483356476, Final Batch Loss: 0.14515191316604614\n",
      "Epoch 3833, Loss: 0.29251987487077713, Final Batch Loss: 0.17132647335529327\n",
      "Epoch 3834, Loss: 0.2885430157184601, Final Batch Loss: 0.13341017067432404\n",
      "Epoch 3835, Loss: 0.2955511510372162, Final Batch Loss: 0.16324687004089355\n",
      "Epoch 3836, Loss: 0.2611689120531082, Final Batch Loss: 0.11459621787071228\n",
      "Epoch 3837, Loss: 0.2968957647681236, Final Batch Loss: 0.19458289444446564\n",
      "Epoch 3838, Loss: 0.27073749154806137, Final Batch Loss: 0.12461137026548386\n",
      "Epoch 3839, Loss: 0.3117714300751686, Final Batch Loss: 0.19504320621490479\n",
      "Epoch 3840, Loss: 0.3449922055006027, Final Batch Loss: 0.18161644041538239\n",
      "Epoch 3841, Loss: 0.3007655441761017, Final Batch Loss: 0.14557494223117828\n",
      "Epoch 3842, Loss: 0.31221842765808105, Final Batch Loss: 0.14138878881931305\n",
      "Epoch 3843, Loss: 0.3000943809747696, Final Batch Loss: 0.17009110748767853\n",
      "Epoch 3844, Loss: 0.31986522674560547, Final Batch Loss: 0.18019086122512817\n",
      "Epoch 3845, Loss: 0.28289927542209625, Final Batch Loss: 0.13331492245197296\n",
      "Epoch 3846, Loss: 0.3378538340330124, Final Batch Loss: 0.1664644479751587\n",
      "Epoch 3847, Loss: 0.3534716069698334, Final Batch Loss: 0.167738676071167\n",
      "Epoch 3848, Loss: 0.2977188378572464, Final Batch Loss: 0.14123845100402832\n",
      "Epoch 3849, Loss: 0.2916732728481293, Final Batch Loss: 0.16088438034057617\n",
      "Epoch 3850, Loss: 0.3374452590942383, Final Batch Loss: 0.1668078601360321\n",
      "Epoch 3851, Loss: 0.38919009268283844, Final Batch Loss: 0.26364272832870483\n",
      "Epoch 3852, Loss: 0.3830351084470749, Final Batch Loss: 0.2260213941335678\n",
      "Epoch 3853, Loss: 0.24358264356851578, Final Batch Loss: 0.10249029844999313\n",
      "Epoch 3854, Loss: 0.3113100230693817, Final Batch Loss: 0.17013315856456757\n",
      "Epoch 3855, Loss: 0.3246832489967346, Final Batch Loss: 0.14882919192314148\n",
      "Epoch 3856, Loss: 0.25553513318300247, Final Batch Loss: 0.0979735478758812\n",
      "Epoch 3857, Loss: 0.3471248745918274, Final Batch Loss: 0.207096129655838\n",
      "Epoch 3858, Loss: 0.32487742602825165, Final Batch Loss: 0.20149889588356018\n",
      "Epoch 3859, Loss: 0.27409883588552475, Final Batch Loss: 0.11927425116300583\n",
      "Epoch 3860, Loss: 0.2939942181110382, Final Batch Loss: 0.1484314352273941\n",
      "Epoch 3861, Loss: 0.27626579254865646, Final Batch Loss: 0.10273665934801102\n",
      "Epoch 3862, Loss: 0.28518766164779663, Final Batch Loss: 0.14763401448726654\n",
      "Epoch 3863, Loss: 0.2710365131497383, Final Batch Loss: 0.1187087669968605\n",
      "Epoch 3864, Loss: 0.34787091612815857, Final Batch Loss: 0.17935873568058014\n",
      "Epoch 3865, Loss: 0.3085985630750656, Final Batch Loss: 0.14511488378047943\n",
      "Epoch 3866, Loss: 0.36814412474632263, Final Batch Loss: 0.2115212231874466\n",
      "Epoch 3867, Loss: 0.2818564176559448, Final Batch Loss: 0.14128722250461578\n",
      "Epoch 3868, Loss: 0.38301996886730194, Final Batch Loss: 0.241908997297287\n",
      "Epoch 3869, Loss: 0.22676365822553635, Final Batch Loss: 0.11163981258869171\n",
      "Epoch 3870, Loss: 0.23907185345888138, Final Batch Loss: 0.10499633103609085\n",
      "Epoch 3871, Loss: 0.24762897938489914, Final Batch Loss: 0.09173474460840225\n",
      "Epoch 3872, Loss: 0.25400378555059433, Final Batch Loss: 0.0993838980793953\n",
      "Epoch 3873, Loss: 0.2804235517978668, Final Batch Loss: 0.1612580120563507\n",
      "Epoch 3874, Loss: 0.30963311344385147, Final Batch Loss: 0.18701951205730438\n",
      "Epoch 3875, Loss: 0.30138538777828217, Final Batch Loss: 0.154073566198349\n",
      "Epoch 3876, Loss: 0.25536464154720306, Final Batch Loss: 0.12954676151275635\n",
      "Epoch 3877, Loss: 0.30885739624500275, Final Batch Loss: 0.15004999935626984\n",
      "Epoch 3878, Loss: 0.31911222636699677, Final Batch Loss: 0.17681866884231567\n",
      "Epoch 3879, Loss: 0.3805917203426361, Final Batch Loss: 0.1920800507068634\n",
      "Epoch 3880, Loss: 0.2664736360311508, Final Batch Loss: 0.10118164122104645\n",
      "Epoch 3881, Loss: 0.27573806047439575, Final Batch Loss: 0.16515015065670013\n",
      "Epoch 3882, Loss: 0.31956881284713745, Final Batch Loss: 0.1943446546792984\n",
      "Epoch 3883, Loss: 0.3437894582748413, Final Batch Loss: 0.18267393112182617\n",
      "Epoch 3884, Loss: 0.3225311040878296, Final Batch Loss: 0.17496803402900696\n",
      "Epoch 3885, Loss: 0.2967636287212372, Final Batch Loss: 0.14991964399814606\n",
      "Epoch 3886, Loss: 0.253912515938282, Final Batch Loss: 0.11001617461442947\n",
      "Epoch 3887, Loss: 0.2965850830078125, Final Batch Loss: 0.13444124162197113\n",
      "Epoch 3888, Loss: 0.27719954401254654, Final Batch Loss: 0.11812012642621994\n",
      "Epoch 3889, Loss: 0.2803642004728317, Final Batch Loss: 0.1421981155872345\n",
      "Epoch 3890, Loss: 0.2487584576010704, Final Batch Loss: 0.11681578308343887\n",
      "Epoch 3891, Loss: 0.2874971553683281, Final Batch Loss: 0.11964049190282822\n",
      "Epoch 3892, Loss: 0.22901622205972672, Final Batch Loss: 0.09512010961771011\n",
      "Epoch 3893, Loss: 0.3045467734336853, Final Batch Loss: 0.12676836550235748\n",
      "Epoch 3894, Loss: 0.2787068337202072, Final Batch Loss: 0.1283254623413086\n",
      "Epoch 3895, Loss: 0.3081854283809662, Final Batch Loss: 0.16240374743938446\n",
      "Epoch 3896, Loss: 0.29819299280643463, Final Batch Loss: 0.18109475076198578\n",
      "Epoch 3897, Loss: 0.38328565657138824, Final Batch Loss: 0.22486162185668945\n",
      "Epoch 3898, Loss: 0.29342658817768097, Final Batch Loss: 0.155546173453331\n",
      "Epoch 3899, Loss: 0.26789334416389465, Final Batch Loss: 0.10112084448337555\n",
      "Epoch 3900, Loss: 0.297155424952507, Final Batch Loss: 0.15231646597385406\n",
      "Epoch 3901, Loss: 0.3796103298664093, Final Batch Loss: 0.2438385933637619\n",
      "Epoch 3902, Loss: 0.30223043262958527, Final Batch Loss: 0.14232824742794037\n",
      "Epoch 3903, Loss: 0.3538435250520706, Final Batch Loss: 0.2111772894859314\n",
      "Epoch 3904, Loss: 0.3351316750049591, Final Batch Loss: 0.14772531390190125\n",
      "Epoch 3905, Loss: 0.29854416847229004, Final Batch Loss: 0.13728101551532745\n",
      "Epoch 3906, Loss: 0.27613455057144165, Final Batch Loss: 0.15281440317630768\n",
      "Epoch 3907, Loss: 0.25035491585731506, Final Batch Loss: 0.13354089856147766\n",
      "Epoch 3908, Loss: 0.2269372120499611, Final Batch Loss: 0.08862308412790298\n",
      "Epoch 3909, Loss: 0.24575531482696533, Final Batch Loss: 0.10503220558166504\n",
      "Epoch 3910, Loss: 0.32127727568149567, Final Batch Loss: 0.136208176612854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3911, Loss: 0.2418195977807045, Final Batch Loss: 0.07962291687726974\n",
      "Epoch 3912, Loss: 0.28687623888254166, Final Batch Loss: 0.06385750323534012\n",
      "Epoch 3913, Loss: 0.24357113242149353, Final Batch Loss: 0.10142458975315094\n",
      "Epoch 3914, Loss: 0.29912036657333374, Final Batch Loss: 0.16005192697048187\n",
      "Epoch 3915, Loss: 0.2642492577433586, Final Batch Loss: 0.09879504889249802\n",
      "Epoch 3916, Loss: 0.30877985060214996, Final Batch Loss: 0.13253013789653778\n",
      "Epoch 3917, Loss: 0.31031733751296997, Final Batch Loss: 0.1779552400112152\n",
      "Epoch 3918, Loss: 0.29420316219329834, Final Batch Loss: 0.1515807956457138\n",
      "Epoch 3919, Loss: 0.26420148462057114, Final Batch Loss: 0.11672013252973557\n",
      "Epoch 3920, Loss: 0.36514216661453247, Final Batch Loss: 0.21159738302230835\n",
      "Epoch 3921, Loss: 0.25010761618614197, Final Batch Loss: 0.09240905940532684\n",
      "Epoch 3922, Loss: 0.27617092430591583, Final Batch Loss: 0.1369423270225525\n",
      "Epoch 3923, Loss: 0.32523849606513977, Final Batch Loss: 0.18882238864898682\n",
      "Epoch 3924, Loss: 0.24024991691112518, Final Batch Loss: 0.10499812662601471\n",
      "Epoch 3925, Loss: 0.31836967170238495, Final Batch Loss: 0.18134497106075287\n",
      "Epoch 3926, Loss: 0.3490845263004303, Final Batch Loss: 0.18658174574375153\n",
      "Epoch 3927, Loss: 0.2730930894613266, Final Batch Loss: 0.1631166636943817\n",
      "Epoch 3928, Loss: 0.25559263676404953, Final Batch Loss: 0.1110200509428978\n",
      "Epoch 3929, Loss: 0.3573794811964035, Final Batch Loss: 0.16229982674121857\n",
      "Epoch 3930, Loss: 0.29856471717357635, Final Batch Loss: 0.16349856555461884\n",
      "Epoch 3931, Loss: 0.4603334665298462, Final Batch Loss: 0.25459933280944824\n",
      "Epoch 3932, Loss: 0.2742457687854767, Final Batch Loss: 0.10108959674835205\n",
      "Epoch 3933, Loss: 0.3181552439928055, Final Batch Loss: 0.16559410095214844\n",
      "Epoch 3934, Loss: 0.3278251588344574, Final Batch Loss: 0.18213099241256714\n",
      "Epoch 3935, Loss: 0.2824596166610718, Final Batch Loss: 0.1547185778617859\n",
      "Epoch 3936, Loss: 0.25537047535181046, Final Batch Loss: 0.12486637383699417\n",
      "Epoch 3937, Loss: 0.2922348529100418, Final Batch Loss: 0.1091543436050415\n",
      "Epoch 3938, Loss: 0.2896828353404999, Final Batch Loss: 0.1380966305732727\n",
      "Epoch 3939, Loss: 0.30807268619537354, Final Batch Loss: 0.13927610218524933\n",
      "Epoch 3940, Loss: 0.38672125339508057, Final Batch Loss: 0.2707202136516571\n",
      "Epoch 3941, Loss: 0.2595800757408142, Final Batch Loss: 0.1459866613149643\n",
      "Epoch 3942, Loss: 0.31443727016448975, Final Batch Loss: 0.15752869844436646\n",
      "Epoch 3943, Loss: 0.2879720479249954, Final Batch Loss: 0.12897458672523499\n",
      "Epoch 3944, Loss: 0.2914339005947113, Final Batch Loss: 0.12911267578601837\n",
      "Epoch 3945, Loss: 0.25750555098056793, Final Batch Loss: 0.1232708990573883\n",
      "Epoch 3946, Loss: 0.2996206134557724, Final Batch Loss: 0.17686034739017487\n",
      "Epoch 3947, Loss: 0.2535729557275772, Final Batch Loss: 0.11135140061378479\n",
      "Epoch 3948, Loss: 0.279340535402298, Final Batch Loss: 0.12632527947425842\n",
      "Epoch 3949, Loss: 0.29554012417793274, Final Batch Loss: 0.10627993941307068\n",
      "Epoch 3950, Loss: 0.2601568028330803, Final Batch Loss: 0.12245755642652512\n",
      "Epoch 3951, Loss: 0.2823574095964432, Final Batch Loss: 0.1274801790714264\n",
      "Epoch 3952, Loss: 0.2817452400922775, Final Batch Loss: 0.14665734767913818\n",
      "Epoch 3953, Loss: 0.24948500096797943, Final Batch Loss: 0.11695089936256409\n",
      "Epoch 3954, Loss: 0.29888541996479034, Final Batch Loss: 0.16601034998893738\n",
      "Epoch 3955, Loss: 0.29648618400096893, Final Batch Loss: 0.15282712876796722\n",
      "Epoch 3956, Loss: 0.34767191112041473, Final Batch Loss: 0.19289296865463257\n",
      "Epoch 3957, Loss: 0.41237524151802063, Final Batch Loss: 0.20386962592601776\n",
      "Epoch 3958, Loss: 0.35515913367271423, Final Batch Loss: 0.1265171766281128\n",
      "Epoch 3959, Loss: 0.332467719912529, Final Batch Loss: 0.204253688454628\n",
      "Epoch 3960, Loss: 0.2654828652739525, Final Batch Loss: 0.12253107875585556\n",
      "Epoch 3961, Loss: 0.32371287047863007, Final Batch Loss: 0.1605185866355896\n",
      "Epoch 3962, Loss: 0.3376500606536865, Final Batch Loss: 0.19612069427967072\n",
      "Epoch 3963, Loss: 0.2722151279449463, Final Batch Loss: 0.1502712219953537\n",
      "Epoch 3964, Loss: 0.2370043843984604, Final Batch Loss: 0.12472066283226013\n",
      "Epoch 3965, Loss: 0.2800164669752121, Final Batch Loss: 0.12072098255157471\n",
      "Epoch 3966, Loss: 0.28434915840625763, Final Batch Loss: 0.14902520179748535\n",
      "Epoch 3967, Loss: 0.32485394179821014, Final Batch Loss: 0.15374964475631714\n",
      "Epoch 3968, Loss: 0.24079665541648865, Final Batch Loss: 0.11640267819166183\n",
      "Epoch 3969, Loss: 0.2769661024212837, Final Batch Loss: 0.15374921262264252\n",
      "Epoch 3970, Loss: 0.299004890024662, Final Batch Loss: 0.19009847939014435\n",
      "Epoch 3971, Loss: 0.22410419583320618, Final Batch Loss: 0.09900940954685211\n",
      "Epoch 3972, Loss: 0.3016059398651123, Final Batch Loss: 0.14502151310443878\n",
      "Epoch 3973, Loss: 0.3778545558452606, Final Batch Loss: 0.16779199242591858\n",
      "Epoch 3974, Loss: 0.311231404542923, Final Batch Loss: 0.1799718737602234\n",
      "Epoch 3975, Loss: 0.2814316898584366, Final Batch Loss: 0.13747692108154297\n",
      "Epoch 3976, Loss: 0.3317589908838272, Final Batch Loss: 0.1627897024154663\n",
      "Epoch 3977, Loss: 0.2225356101989746, Final Batch Loss: 0.10719666630029678\n",
      "Epoch 3978, Loss: 0.2667749300599098, Final Batch Loss: 0.1241820678114891\n",
      "Epoch 3979, Loss: 0.26375219970941544, Final Batch Loss: 0.1163109615445137\n",
      "Epoch 3980, Loss: 0.2810475528240204, Final Batch Loss: 0.1578456610441208\n",
      "Epoch 3981, Loss: 0.2658597305417061, Final Batch Loss: 0.14975878596305847\n",
      "Epoch 3982, Loss: 0.2935415357351303, Final Batch Loss: 0.164473295211792\n",
      "Epoch 3983, Loss: 0.3069578856229782, Final Batch Loss: 0.15287908911705017\n",
      "Epoch 3984, Loss: 0.23706736415624619, Final Batch Loss: 0.1027531698346138\n",
      "Epoch 3985, Loss: 0.3271164745092392, Final Batch Loss: 0.154924675822258\n",
      "Epoch 3986, Loss: 0.2272719070315361, Final Batch Loss: 0.1079796850681305\n",
      "Epoch 3987, Loss: 0.32408981025218964, Final Batch Loss: 0.1524384766817093\n",
      "Epoch 3988, Loss: 0.2458438277244568, Final Batch Loss: 0.12714071571826935\n",
      "Epoch 3989, Loss: 0.2941439598798752, Final Batch Loss: 0.128456249833107\n",
      "Epoch 3990, Loss: 0.3189859986305237, Final Batch Loss: 0.1719714105129242\n",
      "Epoch 3991, Loss: 0.2922724038362503, Final Batch Loss: 0.15427060425281525\n",
      "Epoch 3992, Loss: 0.23671342432498932, Final Batch Loss: 0.08573131263256073\n",
      "Epoch 3993, Loss: 0.3396340012550354, Final Batch Loss: 0.22413690388202667\n",
      "Epoch 3994, Loss: 0.2546858489513397, Final Batch Loss: 0.09742653369903564\n",
      "Epoch 3995, Loss: 0.3541366159915924, Final Batch Loss: 0.1734391152858734\n",
      "Epoch 3996, Loss: 0.28976066410541534, Final Batch Loss: 0.16365393996238708\n",
      "Epoch 3997, Loss: 0.3207281678915024, Final Batch Loss: 0.15121524035930634\n",
      "Epoch 3998, Loss: 0.3014247864484787, Final Batch Loss: 0.1620229333639145\n",
      "Epoch 3999, Loss: 0.2825803756713867, Final Batch Loss: 0.17123208940029144\n",
      "Epoch 4000, Loss: 0.28652068972587585, Final Batch Loss: 0.1373465210199356\n",
      "Epoch 4001, Loss: 0.2477445751428604, Final Batch Loss: 0.09319035708904266\n",
      "Epoch 4002, Loss: 0.2882924750447273, Final Batch Loss: 0.16836759448051453\n",
      "Epoch 4003, Loss: 0.25433196127414703, Final Batch Loss: 0.12287922203540802\n",
      "Epoch 4004, Loss: 0.2819724977016449, Final Batch Loss: 0.1545887142419815\n",
      "Epoch 4005, Loss: 0.29792650043964386, Final Batch Loss: 0.13249650597572327\n",
      "Epoch 4006, Loss: 0.27302275598049164, Final Batch Loss: 0.13302531838417053\n",
      "Epoch 4007, Loss: 0.308816559612751, Final Batch Loss: 0.19025671482086182\n",
      "Epoch 4008, Loss: 0.38400059938430786, Final Batch Loss: 0.23097074031829834\n",
      "Epoch 4009, Loss: 0.2676648050546646, Final Batch Loss: 0.12648016214370728\n",
      "Epoch 4010, Loss: 0.28944776952266693, Final Batch Loss: 0.1634880006313324\n",
      "Epoch 4011, Loss: 0.32612623274326324, Final Batch Loss: 0.17328621447086334\n",
      "Epoch 4012, Loss: 0.3121805489063263, Final Batch Loss: 0.15708397328853607\n",
      "Epoch 4013, Loss: 0.3403743505477905, Final Batch Loss: 0.12978094816207886\n",
      "Epoch 4014, Loss: 0.24229975044727325, Final Batch Loss: 0.10391880571842194\n",
      "Epoch 4015, Loss: 0.3054490089416504, Final Batch Loss: 0.15483714640140533\n",
      "Epoch 4016, Loss: 0.2754022032022476, Final Batch Loss: 0.14573052525520325\n",
      "Epoch 4017, Loss: 0.28640230000019073, Final Batch Loss: 0.1597595512866974\n",
      "Epoch 4018, Loss: 0.33369705080986023, Final Batch Loss: 0.1516810953617096\n",
      "Epoch 4019, Loss: 0.24170760065317154, Final Batch Loss: 0.12681511044502258\n",
      "Epoch 4020, Loss: 0.30985958874225616, Final Batch Loss: 0.14780278503894806\n",
      "Epoch 4021, Loss: 0.2628730610013008, Final Batch Loss: 0.1391613632440567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4022, Loss: 0.3410678654909134, Final Batch Loss: 0.1646823137998581\n",
      "Epoch 4023, Loss: 0.4118116796016693, Final Batch Loss: 0.25174733996391296\n",
      "Epoch 4024, Loss: 0.2747899144887924, Final Batch Loss: 0.1595281958580017\n",
      "Epoch 4025, Loss: 0.27899931371212006, Final Batch Loss: 0.12606030702590942\n",
      "Epoch 4026, Loss: 0.2904382646083832, Final Batch Loss: 0.14579522609710693\n",
      "Epoch 4027, Loss: 0.21468309313058853, Final Batch Loss: 0.10114207863807678\n",
      "Epoch 4028, Loss: 0.3018970787525177, Final Batch Loss: 0.18022112548351288\n",
      "Epoch 4029, Loss: 0.34884414076805115, Final Batch Loss: 0.1836998015642166\n",
      "Epoch 4030, Loss: 0.2567107602953911, Final Batch Loss: 0.0966464951634407\n",
      "Epoch 4031, Loss: 0.33453069627285004, Final Batch Loss: 0.2187965363264084\n",
      "Epoch 4032, Loss: 0.2661297768354416, Final Batch Loss: 0.12540754675865173\n",
      "Epoch 4033, Loss: 0.22042948007583618, Final Batch Loss: 0.07356858253479004\n",
      "Epoch 4034, Loss: 0.2604152038693428, Final Batch Loss: 0.11936699599027634\n",
      "Epoch 4035, Loss: 0.3826410621404648, Final Batch Loss: 0.2175005078315735\n",
      "Epoch 4036, Loss: 0.3099266290664673, Final Batch Loss: 0.14296266436576843\n",
      "Epoch 4037, Loss: 0.31167565286159515, Final Batch Loss: 0.1454867571592331\n",
      "Epoch 4038, Loss: 0.28146663308143616, Final Batch Loss: 0.15236692130565643\n",
      "Epoch 4039, Loss: 0.32794247567653656, Final Batch Loss: 0.1815379410982132\n",
      "Epoch 4040, Loss: 0.26795319467782974, Final Batch Loss: 0.1068996712565422\n",
      "Epoch 4041, Loss: 0.2481328696012497, Final Batch Loss: 0.10778799653053284\n",
      "Epoch 4042, Loss: 0.26509443670511246, Final Batch Loss: 0.14294050633907318\n",
      "Epoch 4043, Loss: 0.2348848208785057, Final Batch Loss: 0.0947108194231987\n",
      "Epoch 4044, Loss: 0.26890187710523605, Final Batch Loss: 0.1541707068681717\n",
      "Epoch 4045, Loss: 0.26098522543907166, Final Batch Loss: 0.12953625619411469\n",
      "Epoch 4046, Loss: 0.25162287056446075, Final Batch Loss: 0.13495463132858276\n",
      "Epoch 4047, Loss: 0.2871633544564247, Final Batch Loss: 0.11481297761201859\n",
      "Epoch 4048, Loss: 0.27949826419353485, Final Batch Loss: 0.1463916152715683\n",
      "Epoch 4049, Loss: 0.2833324074745178, Final Batch Loss: 0.12530863285064697\n",
      "Epoch 4050, Loss: 0.2441495582461357, Final Batch Loss: 0.13880984485149384\n",
      "Epoch 4051, Loss: 0.2604295313358307, Final Batch Loss: 0.13313829898834229\n",
      "Epoch 4052, Loss: 0.2634623423218727, Final Batch Loss: 0.11962825804948807\n",
      "Epoch 4053, Loss: 0.22677883505821228, Final Batch Loss: 0.12359321117401123\n",
      "Epoch 4054, Loss: 0.2876532971858978, Final Batch Loss: 0.13305802643299103\n",
      "Epoch 4055, Loss: 0.23877552896738052, Final Batch Loss: 0.12698103487491608\n",
      "Epoch 4056, Loss: 0.2411147952079773, Final Batch Loss: 0.10540911555290222\n",
      "Epoch 4057, Loss: 0.281538724899292, Final Batch Loss: 0.1403636634349823\n",
      "Epoch 4058, Loss: 0.23354405909776688, Final Batch Loss: 0.10840443521738052\n",
      "Epoch 4059, Loss: 0.2389809563755989, Final Batch Loss: 0.10426411777734756\n",
      "Epoch 4060, Loss: 0.27726586163043976, Final Batch Loss: 0.08114798367023468\n",
      "Epoch 4061, Loss: 0.24332774430513382, Final Batch Loss: 0.11322597414255142\n",
      "Epoch 4062, Loss: 0.2957286834716797, Final Batch Loss: 0.19801625609397888\n",
      "Epoch 4063, Loss: 0.22315071523189545, Final Batch Loss: 0.11755883693695068\n",
      "Epoch 4064, Loss: 0.3156878501176834, Final Batch Loss: 0.15696649253368378\n",
      "Epoch 4065, Loss: 0.2952600121498108, Final Batch Loss: 0.16046853363513947\n",
      "Epoch 4066, Loss: 0.30324506759643555, Final Batch Loss: 0.1711132973432541\n",
      "Epoch 4067, Loss: 0.3126644045114517, Final Batch Loss: 0.17219024896621704\n",
      "Epoch 4068, Loss: 0.30274273455142975, Final Batch Loss: 0.15547117590904236\n",
      "Epoch 4069, Loss: 0.29607687145471573, Final Batch Loss: 0.17713400721549988\n",
      "Epoch 4070, Loss: 0.29738181829452515, Final Batch Loss: 0.14574724435806274\n",
      "Epoch 4071, Loss: 0.27013399451971054, Final Batch Loss: 0.11484279483556747\n",
      "Epoch 4072, Loss: 0.25144466757774353, Final Batch Loss: 0.08594143390655518\n",
      "Epoch 4073, Loss: 0.2919813245534897, Final Batch Loss: 0.14999036490917206\n",
      "Epoch 4074, Loss: 0.33592019975185394, Final Batch Loss: 0.19385211169719696\n",
      "Epoch 4075, Loss: 0.28754813224077225, Final Batch Loss: 0.11810929328203201\n",
      "Epoch 4076, Loss: 0.33033089339733124, Final Batch Loss: 0.19888868927955627\n",
      "Epoch 4077, Loss: 0.3474782258272171, Final Batch Loss: 0.15483959019184113\n",
      "Epoch 4078, Loss: 0.26842283457517624, Final Batch Loss: 0.08510039001703262\n",
      "Epoch 4079, Loss: 0.25646595656871796, Final Batch Loss: 0.07463324069976807\n",
      "Epoch 4080, Loss: 0.25997135043144226, Final Batch Loss: 0.11632315814495087\n",
      "Epoch 4081, Loss: 0.3230568468570709, Final Batch Loss: 0.17082473635673523\n",
      "Epoch 4082, Loss: 0.24666719138622284, Final Batch Loss: 0.07695406675338745\n",
      "Epoch 4083, Loss: 0.3187415674328804, Final Batch Loss: 0.19762389361858368\n",
      "Epoch 4084, Loss: 0.33776184171438217, Final Batch Loss: 0.23010678589344025\n",
      "Epoch 4085, Loss: 0.31984464824199677, Final Batch Loss: 0.16936175525188446\n",
      "Epoch 4086, Loss: 0.23473986983299255, Final Batch Loss: 0.10785239934921265\n",
      "Epoch 4087, Loss: 0.23948776721954346, Final Batch Loss: 0.10585665702819824\n",
      "Epoch 4088, Loss: 0.32215017080307007, Final Batch Loss: 0.1261318027973175\n",
      "Epoch 4089, Loss: 0.2698363810777664, Final Batch Loss: 0.15162496268749237\n",
      "Epoch 4090, Loss: 0.29436543583869934, Final Batch Loss: 0.16625180840492249\n",
      "Epoch 4091, Loss: 0.39338749647140503, Final Batch Loss: 0.14719586074352264\n",
      "Epoch 4092, Loss: 0.3202396631240845, Final Batch Loss: 0.15270276367664337\n",
      "Epoch 4093, Loss: 0.2837904393672943, Final Batch Loss: 0.0910271555185318\n",
      "Epoch 4094, Loss: 0.32631558179855347, Final Batch Loss: 0.17762987315654755\n",
      "Epoch 4095, Loss: 0.3062341660261154, Final Batch Loss: 0.14383457601070404\n",
      "Epoch 4096, Loss: 0.36179742217063904, Final Batch Loss: 0.19958825409412384\n",
      "Epoch 4097, Loss: 0.2549631968140602, Final Batch Loss: 0.10788745433092117\n",
      "Epoch 4098, Loss: 0.23768454045057297, Final Batch Loss: 0.1218639388680458\n",
      "Epoch 4099, Loss: 0.22433090209960938, Final Batch Loss: 0.09518706798553467\n",
      "Epoch 4100, Loss: 0.24542880803346634, Final Batch Loss: 0.06635207682847977\n",
      "Epoch 4101, Loss: 0.2814607471227646, Final Batch Loss: 0.14706026017665863\n",
      "Epoch 4102, Loss: 0.26257166266441345, Final Batch Loss: 0.12935037910938263\n",
      "Epoch 4103, Loss: 0.29220152646303177, Final Batch Loss: 0.17506951093673706\n",
      "Epoch 4104, Loss: 0.3495244011282921, Final Batch Loss: 0.23387251794338226\n",
      "Epoch 4105, Loss: 0.21920444816350937, Final Batch Loss: 0.06905486434698105\n",
      "Epoch 4106, Loss: 0.45957089960575104, Final Batch Loss: 0.29448166489601135\n",
      "Epoch 4107, Loss: 0.29470135271549225, Final Batch Loss: 0.15944455564022064\n",
      "Epoch 4108, Loss: 0.29071998596191406, Final Batch Loss: 0.14464765787124634\n",
      "Epoch 4109, Loss: 0.34924115240573883, Final Batch Loss: 0.1801747828722\n",
      "Epoch 4110, Loss: 0.29248249530792236, Final Batch Loss: 0.12899143993854523\n",
      "Epoch 4111, Loss: 0.2567448318004608, Final Batch Loss: 0.10883370041847229\n",
      "Epoch 4112, Loss: 0.305439293384552, Final Batch Loss: 0.16686709225177765\n",
      "Epoch 4113, Loss: 0.25779107213020325, Final Batch Loss: 0.14303791522979736\n",
      "Epoch 4114, Loss: 0.2872651666402817, Final Batch Loss: 0.12726546823978424\n",
      "Epoch 4115, Loss: 0.25304240733385086, Final Batch Loss: 0.10843013972043991\n",
      "Epoch 4116, Loss: 0.2703101709485054, Final Batch Loss: 0.14997334778308868\n",
      "Epoch 4117, Loss: 0.278454951941967, Final Batch Loss: 0.1635473668575287\n",
      "Epoch 4118, Loss: 0.28364943712949753, Final Batch Loss: 0.1595754474401474\n",
      "Epoch 4119, Loss: 0.22504831850528717, Final Batch Loss: 0.10797373950481415\n",
      "Epoch 4120, Loss: 0.28835999220609665, Final Batch Loss: 0.11825200170278549\n",
      "Epoch 4121, Loss: 0.24728772044181824, Final Batch Loss: 0.09989988803863525\n",
      "Epoch 4122, Loss: 0.25585415959358215, Final Batch Loss: 0.1032375693321228\n",
      "Epoch 4123, Loss: 0.2346106693148613, Final Batch Loss: 0.10911566764116287\n",
      "Epoch 4124, Loss: 0.31250444054603577, Final Batch Loss: 0.15642592310905457\n",
      "Epoch 4125, Loss: 0.34562447667121887, Final Batch Loss: 0.20589670538902283\n",
      "Epoch 4126, Loss: 0.26627161353826523, Final Batch Loss: 0.11709911376237869\n",
      "Epoch 4127, Loss: 0.22247688472270966, Final Batch Loss: 0.090672567486763\n",
      "Epoch 4128, Loss: 0.25761066377162933, Final Batch Loss: 0.1347031146287918\n",
      "Epoch 4129, Loss: 0.30140064656734467, Final Batch Loss: 0.16335970163345337\n",
      "Epoch 4130, Loss: 0.29752201586961746, Final Batch Loss: 0.18041539192199707\n",
      "Epoch 4131, Loss: 0.2796892672777176, Final Batch Loss: 0.1370968520641327\n",
      "Epoch 4132, Loss: 0.42534489184617996, Final Batch Loss: 0.10394606739282608\n",
      "Epoch 4133, Loss: 0.25217733532190323, Final Batch Loss: 0.1375742405653\n",
      "Epoch 4134, Loss: 0.2554740384221077, Final Batch Loss: 0.13958358764648438\n",
      "Epoch 4135, Loss: 0.21976950019598007, Final Batch Loss: 0.07975120097398758\n",
      "Epoch 4136, Loss: 0.38664618134498596, Final Batch Loss: 0.2440643608570099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4137, Loss: 0.2754139304161072, Final Batch Loss: 0.17515794932842255\n",
      "Epoch 4138, Loss: 0.42692919075489044, Final Batch Loss: 0.25629475712776184\n",
      "Epoch 4139, Loss: 0.2794790714979172, Final Batch Loss: 0.11436915397644043\n",
      "Epoch 4140, Loss: 0.30428706109523773, Final Batch Loss: 0.1547294706106186\n",
      "Epoch 4141, Loss: 0.317136712372303, Final Batch Loss: 0.19789312779903412\n",
      "Epoch 4142, Loss: 0.3315814584493637, Final Batch Loss: 0.18827728927135468\n",
      "Epoch 4143, Loss: 0.2661574184894562, Final Batch Loss: 0.15065443515777588\n",
      "Epoch 4144, Loss: 0.25707244873046875, Final Batch Loss: 0.08638979494571686\n",
      "Epoch 4145, Loss: 0.2365136221051216, Final Batch Loss: 0.11917620152235031\n",
      "Epoch 4146, Loss: 0.26075489819049835, Final Batch Loss: 0.11305029690265656\n",
      "Epoch 4147, Loss: 0.37812960147857666, Final Batch Loss: 0.2204681634902954\n",
      "Epoch 4148, Loss: 0.27807871997356415, Final Batch Loss: 0.15371830761432648\n",
      "Epoch 4149, Loss: 0.29272016882896423, Final Batch Loss: 0.09467782080173492\n",
      "Epoch 4150, Loss: 0.3033590614795685, Final Batch Loss: 0.16390474140644073\n",
      "Epoch 4151, Loss: 0.32103443145751953, Final Batch Loss: 0.16866803169250488\n",
      "Epoch 4152, Loss: 0.2829975336790085, Final Batch Loss: 0.1382877230644226\n",
      "Epoch 4153, Loss: 0.23644401878118515, Final Batch Loss: 0.1286005973815918\n",
      "Epoch 4154, Loss: 0.2613731771707535, Final Batch Loss: 0.11265528202056885\n",
      "Epoch 4155, Loss: 0.27395791560411453, Final Batch Loss: 0.15005230903625488\n",
      "Epoch 4156, Loss: 0.2834474593400955, Final Batch Loss: 0.1024370938539505\n",
      "Epoch 4157, Loss: 0.3286970555782318, Final Batch Loss: 0.20946992933750153\n",
      "Epoch 4158, Loss: 0.27592917531728745, Final Batch Loss: 0.1196962222456932\n",
      "Epoch 4159, Loss: 0.29748618602752686, Final Batch Loss: 0.16164293885231018\n",
      "Epoch 4160, Loss: 0.29820749163627625, Final Batch Loss: 0.15138843655586243\n",
      "Epoch 4161, Loss: 0.2993384599685669, Final Batch Loss: 0.14771021902561188\n",
      "Epoch 4162, Loss: 0.2750852480530739, Final Batch Loss: 0.12415202707052231\n",
      "Epoch 4163, Loss: 0.3122134655714035, Final Batch Loss: 0.1634591966867447\n",
      "Epoch 4164, Loss: 0.29474952816963196, Final Batch Loss: 0.1440819352865219\n",
      "Epoch 4165, Loss: 0.27594132721424103, Final Batch Loss: 0.1509857028722763\n",
      "Epoch 4166, Loss: 0.26030919700860977, Final Batch Loss: 0.093445785343647\n",
      "Epoch 4167, Loss: 0.2964271903038025, Final Batch Loss: 0.1521737426519394\n",
      "Epoch 4168, Loss: 0.29253676533699036, Final Batch Loss: 0.09773331880569458\n",
      "Epoch 4169, Loss: 0.30169929563999176, Final Batch Loss: 0.16490744054317474\n",
      "Epoch 4170, Loss: 0.281452976167202, Final Batch Loss: 0.12001717835664749\n",
      "Epoch 4171, Loss: 0.29451894760131836, Final Batch Loss: 0.13222207129001617\n",
      "Epoch 4172, Loss: 0.2978678047657013, Final Batch Loss: 0.13464486598968506\n",
      "Epoch 4173, Loss: 0.2869032472372055, Final Batch Loss: 0.14685827493667603\n",
      "Epoch 4174, Loss: 0.31196229159832, Final Batch Loss: 0.16365374624729156\n",
      "Epoch 4175, Loss: 0.2911735475063324, Final Batch Loss: 0.10473552346229553\n",
      "Epoch 4176, Loss: 0.25190312415361404, Final Batch Loss: 0.12488957494497299\n",
      "Epoch 4177, Loss: 0.2772969603538513, Final Batch Loss: 0.13549597561359406\n",
      "Epoch 4178, Loss: 0.2895573675632477, Final Batch Loss: 0.1295037716627121\n",
      "Epoch 4179, Loss: 0.27611637115478516, Final Batch Loss: 0.12761345505714417\n",
      "Epoch 4180, Loss: 0.2697528600692749, Final Batch Loss: 0.1459796130657196\n",
      "Epoch 4181, Loss: 0.3625669777393341, Final Batch Loss: 0.2057509869337082\n",
      "Epoch 4182, Loss: 0.33775079250335693, Final Batch Loss: 0.20604029297828674\n",
      "Epoch 4183, Loss: 0.2518966570496559, Final Batch Loss: 0.10939937084913254\n",
      "Epoch 4184, Loss: 0.26440735906362534, Final Batch Loss: 0.09919201582670212\n",
      "Epoch 4185, Loss: 0.28051842749118805, Final Batch Loss: 0.13960406184196472\n",
      "Epoch 4186, Loss: 0.2737074941396713, Final Batch Loss: 0.1413189172744751\n",
      "Epoch 4187, Loss: 0.2870987504720688, Final Batch Loss: 0.12241940200328827\n",
      "Epoch 4188, Loss: 0.27646517008543015, Final Batch Loss: 0.1678609549999237\n",
      "Epoch 4189, Loss: 0.2877410054206848, Final Batch Loss: 0.15516318380832672\n",
      "Epoch 4190, Loss: 0.3058229386806488, Final Batch Loss: 0.12646733224391937\n",
      "Epoch 4191, Loss: 0.2704693526029587, Final Batch Loss: 0.14077407121658325\n",
      "Epoch 4192, Loss: 0.2382093444466591, Final Batch Loss: 0.10981499403715134\n",
      "Epoch 4193, Loss: 0.24557848274707794, Final Batch Loss: 0.0842108428478241\n",
      "Epoch 4194, Loss: 0.22937366366386414, Final Batch Loss: 0.10295441746711731\n",
      "Epoch 4195, Loss: 0.22225449234247208, Final Batch Loss: 0.09232708066701889\n",
      "Epoch 4196, Loss: 0.29208292067050934, Final Batch Loss: 0.15106207132339478\n",
      "Epoch 4197, Loss: 0.2941002771258354, Final Batch Loss: 0.18120077252388\n",
      "Epoch 4198, Loss: 0.38088035583496094, Final Batch Loss: 0.19748510420322418\n",
      "Epoch 4199, Loss: 0.25290409475564957, Final Batch Loss: 0.11407997459173203\n",
      "Epoch 4200, Loss: 0.262742280960083, Final Batch Loss: 0.1060979813337326\n",
      "Epoch 4201, Loss: 0.23614270985126495, Final Batch Loss: 0.1078370213508606\n",
      "Epoch 4202, Loss: 0.2011796198785305, Final Batch Loss: 0.060578685253858566\n",
      "Epoch 4203, Loss: 0.30977313220500946, Final Batch Loss: 0.13736523687839508\n",
      "Epoch 4204, Loss: 0.25757522135972977, Final Batch Loss: 0.12312699109315872\n",
      "Epoch 4205, Loss: 0.2699022442102432, Final Batch Loss: 0.10290101170539856\n",
      "Epoch 4206, Loss: 0.3236631602048874, Final Batch Loss: 0.16460366547107697\n",
      "Epoch 4207, Loss: 0.26771093904972076, Final Batch Loss: 0.1407018005847931\n",
      "Epoch 4208, Loss: 0.23015670478343964, Final Batch Loss: 0.09136272966861725\n",
      "Epoch 4209, Loss: 0.27737152576446533, Final Batch Loss: 0.12983183562755585\n",
      "Epoch 4210, Loss: 0.22044458240270615, Final Batch Loss: 0.0835062637925148\n",
      "Epoch 4211, Loss: 0.2917516380548477, Final Batch Loss: 0.09112109243869781\n",
      "Epoch 4212, Loss: 0.3250843286514282, Final Batch Loss: 0.18022020161151886\n",
      "Epoch 4213, Loss: 0.26176854223012924, Final Batch Loss: 0.1493789553642273\n",
      "Epoch 4214, Loss: 0.256249837577343, Final Batch Loss: 0.09934567660093307\n",
      "Epoch 4215, Loss: 0.29655811190605164, Final Batch Loss: 0.16956354677677155\n",
      "Epoch 4216, Loss: 0.28044215589761734, Final Batch Loss: 0.10592611879110336\n",
      "Epoch 4217, Loss: 0.2548743709921837, Final Batch Loss: 0.10636436194181442\n",
      "Epoch 4218, Loss: 0.24842143803834915, Final Batch Loss: 0.10554762929677963\n",
      "Epoch 4219, Loss: 0.28583434969186783, Final Batch Loss: 0.10295047610998154\n",
      "Epoch 4220, Loss: 0.2761758118867874, Final Batch Loss: 0.14305062592029572\n",
      "Epoch 4221, Loss: 0.3803805559873581, Final Batch Loss: 0.19703687727451324\n",
      "Epoch 4222, Loss: 0.3019949495792389, Final Batch Loss: 0.1692785620689392\n",
      "Epoch 4223, Loss: 0.23184755444526672, Final Batch Loss: 0.06841517984867096\n",
      "Epoch 4224, Loss: 0.2528762221336365, Final Batch Loss: 0.10229720175266266\n",
      "Epoch 4225, Loss: 0.2465176284313202, Final Batch Loss: 0.11588069796562195\n",
      "Epoch 4226, Loss: 0.3258476257324219, Final Batch Loss: 0.13190102577209473\n",
      "Epoch 4227, Loss: 0.3109402731060982, Final Batch Loss: 0.20242013037204742\n",
      "Epoch 4228, Loss: 0.3276006951928139, Final Batch Loss: 0.1096586361527443\n",
      "Epoch 4229, Loss: 0.32391880452632904, Final Batch Loss: 0.12603124976158142\n",
      "Epoch 4230, Loss: 0.3137412369251251, Final Batch Loss: 0.13917258381843567\n",
      "Epoch 4231, Loss: 0.2697870582342148, Final Batch Loss: 0.12685923278331757\n",
      "Epoch 4232, Loss: 0.25195854157209396, Final Batch Loss: 0.1309211701154709\n",
      "Epoch 4233, Loss: 0.3402206152677536, Final Batch Loss: 0.17230096459388733\n",
      "Epoch 4234, Loss: 0.3463938683271408, Final Batch Loss: 0.22307349741458893\n",
      "Epoch 4235, Loss: 0.23999619483947754, Final Batch Loss: 0.12276017665863037\n",
      "Epoch 4236, Loss: 0.32276010513305664, Final Batch Loss: 0.19062024354934692\n",
      "Epoch 4237, Loss: 0.3782503753900528, Final Batch Loss: 0.2505870461463928\n",
      "Epoch 4238, Loss: 0.26807205379009247, Final Batch Loss: 0.1344638615846634\n",
      "Epoch 4239, Loss: 0.3136994242668152, Final Batch Loss: 0.15980970859527588\n",
      "Epoch 4240, Loss: 0.3110649883747101, Final Batch Loss: 0.16823476552963257\n",
      "Epoch 4241, Loss: 0.26022782176733017, Final Batch Loss: 0.1537994146347046\n",
      "Epoch 4242, Loss: 0.2530791759490967, Final Batch Loss: 0.11119355261325836\n",
      "Epoch 4243, Loss: 0.31708869338035583, Final Batch Loss: 0.14448557794094086\n",
      "Epoch 4244, Loss: 0.27383726835250854, Final Batch Loss: 0.13538655638694763\n",
      "Epoch 4245, Loss: 0.30009161680936813, Final Batch Loss: 0.17655588686466217\n",
      "Epoch 4246, Loss: 0.24483462423086166, Final Batch Loss: 0.09713958948850632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4247, Loss: 0.298631876707077, Final Batch Loss: 0.14657890796661377\n",
      "Epoch 4248, Loss: 0.2811807841062546, Final Batch Loss: 0.15603171288967133\n",
      "Epoch 4249, Loss: 0.33618664741516113, Final Batch Loss: 0.16624346375465393\n",
      "Epoch 4250, Loss: 0.3065215051174164, Final Batch Loss: 0.16550733149051666\n",
      "Epoch 4251, Loss: 0.34053659439086914, Final Batch Loss: 0.210288867354393\n",
      "Epoch 4252, Loss: 0.28254012763500214, Final Batch Loss: 0.13085442781448364\n",
      "Epoch 4253, Loss: 0.23211079835891724, Final Batch Loss: 0.071873739361763\n",
      "Epoch 4254, Loss: 0.30999618768692017, Final Batch Loss: 0.17041994631290436\n",
      "Epoch 4255, Loss: 0.2855403423309326, Final Batch Loss: 0.16895776987075806\n",
      "Epoch 4256, Loss: 0.30357545614242554, Final Batch Loss: 0.15033753216266632\n",
      "Epoch 4257, Loss: 0.25599300116300583, Final Batch Loss: 0.14615723490715027\n",
      "Epoch 4258, Loss: 0.25412070751190186, Final Batch Loss: 0.11270369589328766\n",
      "Epoch 4259, Loss: 0.32068243622779846, Final Batch Loss: 0.1593296229839325\n",
      "Epoch 4260, Loss: 0.263723649084568, Final Batch Loss: 0.11761761456727982\n",
      "Epoch 4261, Loss: 0.29979486763477325, Final Batch Loss: 0.15265756845474243\n",
      "Epoch 4262, Loss: 0.2802600935101509, Final Batch Loss: 0.16404390335083008\n",
      "Epoch 4263, Loss: 0.2844031900167465, Final Batch Loss: 0.13588739931583405\n",
      "Epoch 4264, Loss: 0.2693854868412018, Final Batch Loss: 0.16214317083358765\n",
      "Epoch 4265, Loss: 0.2789882868528366, Final Batch Loss: 0.14214977622032166\n",
      "Epoch 4266, Loss: 0.38628751039505005, Final Batch Loss: 0.2655423581600189\n",
      "Epoch 4267, Loss: 0.27769825607538223, Final Batch Loss: 0.10778213292360306\n",
      "Epoch 4268, Loss: 0.24642907828092575, Final Batch Loss: 0.11244819313287735\n",
      "Epoch 4269, Loss: 0.2889525443315506, Final Batch Loss: 0.13663731515407562\n",
      "Epoch 4270, Loss: 0.21690359711647034, Final Batch Loss: 0.07955560088157654\n",
      "Epoch 4271, Loss: 0.2906586229801178, Final Batch Loss: 0.14885225892066956\n",
      "Epoch 4272, Loss: 0.25697243213653564, Final Batch Loss: 0.11326998472213745\n",
      "Epoch 4273, Loss: 0.27362991869449615, Final Batch Loss: 0.13849732279777527\n",
      "Epoch 4274, Loss: 0.2747720628976822, Final Batch Loss: 0.14891581237316132\n",
      "Epoch 4275, Loss: 0.30667872726917267, Final Batch Loss: 0.15575379133224487\n",
      "Epoch 4276, Loss: 0.24937525391578674, Final Batch Loss: 0.12381064891815186\n",
      "Epoch 4277, Loss: 0.2651187628507614, Final Batch Loss: 0.13589999079704285\n",
      "Epoch 4278, Loss: 0.30432941019535065, Final Batch Loss: 0.1460471749305725\n",
      "Epoch 4279, Loss: 0.317056305706501, Final Batch Loss: 0.11318906396627426\n",
      "Epoch 4280, Loss: 0.26855336129665375, Final Batch Loss: 0.13953028619289398\n",
      "Epoch 4281, Loss: 0.3192257434129715, Final Batch Loss: 0.14831393957138062\n",
      "Epoch 4282, Loss: 0.32851381599903107, Final Batch Loss: 0.17851437628269196\n",
      "Epoch 4283, Loss: 0.35594770312309265, Final Batch Loss: 0.23500700294971466\n",
      "Epoch 4284, Loss: 0.27984996140003204, Final Batch Loss: 0.14852380752563477\n",
      "Epoch 4285, Loss: 0.30420419573783875, Final Batch Loss: 0.13271063566207886\n",
      "Epoch 4286, Loss: 0.32858802378177643, Final Batch Loss: 0.22079786658287048\n",
      "Epoch 4287, Loss: 0.2772909179329872, Final Batch Loss: 0.10351013392210007\n",
      "Epoch 4288, Loss: 0.2526075541973114, Final Batch Loss: 0.1010168045759201\n",
      "Epoch 4289, Loss: 0.2996101751923561, Final Batch Loss: 0.1845027655363083\n",
      "Epoch 4290, Loss: 0.279262512922287, Final Batch Loss: 0.12622278928756714\n",
      "Epoch 4291, Loss: 0.2617555856704712, Final Batch Loss: 0.12615738809108734\n",
      "Epoch 4292, Loss: 0.278103843331337, Final Batch Loss: 0.1266699582338333\n",
      "Epoch 4293, Loss: 0.3424425721168518, Final Batch Loss: 0.19020070135593414\n",
      "Epoch 4294, Loss: 0.30155273526906967, Final Batch Loss: 0.1163438931107521\n",
      "Epoch 4295, Loss: 0.31639115512371063, Final Batch Loss: 0.17932535707950592\n",
      "Epoch 4296, Loss: 0.2578545808792114, Final Batch Loss: 0.10485950112342834\n",
      "Epoch 4297, Loss: 0.2408454269170761, Final Batch Loss: 0.10641896724700928\n",
      "Epoch 4298, Loss: 0.2325144186615944, Final Batch Loss: 0.10319601744413376\n",
      "Epoch 4299, Loss: 0.2373812422156334, Final Batch Loss: 0.09473057836294174\n",
      "Epoch 4300, Loss: 0.25482623279094696, Final Batch Loss: 0.08092977106571198\n",
      "Epoch 4301, Loss: 0.3290834054350853, Final Batch Loss: 0.21921537816524506\n",
      "Epoch 4302, Loss: 0.3014446496963501, Final Batch Loss: 0.1538279801607132\n",
      "Epoch 4303, Loss: 0.21944085508584976, Final Batch Loss: 0.10412867367267609\n",
      "Epoch 4304, Loss: 0.20988355576992035, Final Batch Loss: 0.10185474902391434\n",
      "Epoch 4305, Loss: 0.23984180390834808, Final Batch Loss: 0.10534071922302246\n",
      "Epoch 4306, Loss: 0.3324742913246155, Final Batch Loss: 0.19375745952129364\n",
      "Epoch 4307, Loss: 0.2790205776691437, Final Batch Loss: 0.18720610439777374\n",
      "Epoch 4308, Loss: 0.22436318546533585, Final Batch Loss: 0.12468040734529495\n",
      "Epoch 4309, Loss: 0.2649862840771675, Final Batch Loss: 0.11529723554849625\n",
      "Epoch 4310, Loss: 0.28521624207496643, Final Batch Loss: 0.11271093785762787\n",
      "Epoch 4311, Loss: 0.3027661144733429, Final Batch Loss: 0.1699008047580719\n",
      "Epoch 4312, Loss: 0.2342284917831421, Final Batch Loss: 0.09488089382648468\n",
      "Epoch 4313, Loss: 0.2648962736129761, Final Batch Loss: 0.0841013640165329\n",
      "Epoch 4314, Loss: 0.23391804099082947, Final Batch Loss: 0.08040335774421692\n",
      "Epoch 4315, Loss: 0.2737082988023758, Final Batch Loss: 0.13361890614032745\n",
      "Epoch 4316, Loss: 0.2610785737633705, Final Batch Loss: 0.11216666549444199\n",
      "Epoch 4317, Loss: 0.3096553832292557, Final Batch Loss: 0.12567541003227234\n",
      "Epoch 4318, Loss: 0.29377205669879913, Final Batch Loss: 0.1621895581483841\n",
      "Epoch 4319, Loss: 0.23630917072296143, Final Batch Loss: 0.11174216866493225\n",
      "Epoch 4320, Loss: 0.28112204372882843, Final Batch Loss: 0.16001179814338684\n",
      "Epoch 4321, Loss: 0.26989661157131195, Final Batch Loss: 0.15338261425495148\n",
      "Epoch 4322, Loss: 0.22766700387001038, Final Batch Loss: 0.11067801713943481\n",
      "Epoch 4323, Loss: 0.29735995829105377, Final Batch Loss: 0.16512444615364075\n",
      "Epoch 4324, Loss: 0.2641677111387253, Final Batch Loss: 0.13140928745269775\n",
      "Epoch 4325, Loss: 0.4229932576417923, Final Batch Loss: 0.30999866127967834\n",
      "Epoch 4326, Loss: 0.31192509829998016, Final Batch Loss: 0.1807592511177063\n",
      "Epoch 4327, Loss: 0.3093066066503525, Final Batch Loss: 0.1524312049150467\n",
      "Epoch 4328, Loss: 0.2925529181957245, Final Batch Loss: 0.1771249920129776\n",
      "Epoch 4329, Loss: 0.4058322012424469, Final Batch Loss: 0.31550323963165283\n",
      "Epoch 4330, Loss: 0.294499509036541, Final Batch Loss: 0.12056923657655716\n",
      "Epoch 4331, Loss: 0.24679601937532425, Final Batch Loss: 0.12478057295084\n",
      "Epoch 4332, Loss: 0.2555486261844635, Final Batch Loss: 0.12047295272350311\n",
      "Epoch 4333, Loss: 0.23756646364927292, Final Batch Loss: 0.08894697576761246\n",
      "Epoch 4334, Loss: 0.24578862637281418, Final Batch Loss: 0.06492064148187637\n",
      "Epoch 4335, Loss: 0.2668677866458893, Final Batch Loss: 0.12111331522464752\n",
      "Epoch 4336, Loss: 0.23834910243749619, Final Batch Loss: 0.1095070019364357\n",
      "Epoch 4337, Loss: 0.3096836656332016, Final Batch Loss: 0.14602410793304443\n",
      "Epoch 4338, Loss: 0.2638966739177704, Final Batch Loss: 0.13587984442710876\n",
      "Epoch 4339, Loss: 0.24433793872594833, Final Batch Loss: 0.11497538536787033\n",
      "Epoch 4340, Loss: 0.20759984105825424, Final Batch Loss: 0.0883948877453804\n",
      "Epoch 4341, Loss: 0.33555763959884644, Final Batch Loss: 0.15412062406539917\n",
      "Epoch 4342, Loss: 0.23513507097959518, Final Batch Loss: 0.10578570514917374\n",
      "Epoch 4343, Loss: 0.27011604607105255, Final Batch Loss: 0.12608157098293304\n",
      "Epoch 4344, Loss: 0.319235697388649, Final Batch Loss: 0.1563674807548523\n",
      "Epoch 4345, Loss: 0.32789018005132675, Final Batch Loss: 0.20968236029148102\n",
      "Epoch 4346, Loss: 0.31389812380075455, Final Batch Loss: 0.18980775773525238\n",
      "Epoch 4347, Loss: 0.2030637413263321, Final Batch Loss: 0.05997282266616821\n",
      "Epoch 4348, Loss: 0.3120296970009804, Final Batch Loss: 0.18927261233329773\n",
      "Epoch 4349, Loss: 0.28817032277584076, Final Batch Loss: 0.14237725734710693\n",
      "Epoch 4350, Loss: 0.34873101115226746, Final Batch Loss: 0.20843106508255005\n",
      "Epoch 4351, Loss: 0.26064226031303406, Final Batch Loss: 0.1455744355916977\n",
      "Epoch 4352, Loss: 0.29890362173318863, Final Batch Loss: 0.17571921646595\n",
      "Epoch 4353, Loss: 0.32188595831394196, Final Batch Loss: 0.2070249766111374\n",
      "Epoch 4354, Loss: 0.28860658407211304, Final Batch Loss: 0.12816667556762695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4355, Loss: 0.26094695180654526, Final Batch Loss: 0.10740246623754501\n",
      "Epoch 4356, Loss: 0.3308815434575081, Final Batch Loss: 0.22869321703910828\n",
      "Epoch 4357, Loss: 0.26354338228702545, Final Batch Loss: 0.15006448328495026\n",
      "Epoch 4358, Loss: 0.2510756105184555, Final Batch Loss: 0.1093132495880127\n",
      "Epoch 4359, Loss: 0.25936735421419144, Final Batch Loss: 0.12274006754159927\n",
      "Epoch 4360, Loss: 0.27849723398685455, Final Batch Loss: 0.14243046939373016\n",
      "Epoch 4361, Loss: 0.243789441883564, Final Batch Loss: 0.10182113200426102\n",
      "Epoch 4362, Loss: 0.33814939111471176, Final Batch Loss: 0.2300991266965866\n",
      "Epoch 4363, Loss: 0.3156350553035736, Final Batch Loss: 0.19560621678829193\n",
      "Epoch 4364, Loss: 0.24669162184000015, Final Batch Loss: 0.09639472514390945\n",
      "Epoch 4365, Loss: 0.3149922043085098, Final Batch Loss: 0.13710054755210876\n",
      "Epoch 4366, Loss: 0.23663169890642166, Final Batch Loss: 0.09748228639364243\n",
      "Epoch 4367, Loss: 0.2748945504426956, Final Batch Loss: 0.15057790279388428\n",
      "Epoch 4368, Loss: 0.2410418838262558, Final Batch Loss: 0.10169006884098053\n",
      "Epoch 4369, Loss: 0.32084929943084717, Final Batch Loss: 0.2008451670408249\n",
      "Epoch 4370, Loss: 0.23701850324869156, Final Batch Loss: 0.08363590389490128\n",
      "Epoch 4371, Loss: 0.23346569389104843, Final Batch Loss: 0.10175097733736038\n",
      "Epoch 4372, Loss: 0.30200693011283875, Final Batch Loss: 0.1303589940071106\n",
      "Epoch 4373, Loss: 0.2831747308373451, Final Batch Loss: 0.17320747673511505\n",
      "Epoch 4374, Loss: 0.3046407550573349, Final Batch Loss: 0.1489572376012802\n",
      "Epoch 4375, Loss: 0.2556423395872116, Final Batch Loss: 0.09868988394737244\n",
      "Epoch 4376, Loss: 0.23305026441812515, Final Batch Loss: 0.10763392597436905\n",
      "Epoch 4377, Loss: 0.29501233994960785, Final Batch Loss: 0.13967549800872803\n",
      "Epoch 4378, Loss: 0.2527143359184265, Final Batch Loss: 0.11246438324451447\n",
      "Epoch 4379, Loss: 0.26313452422618866, Final Batch Loss: 0.13102500140666962\n",
      "Epoch 4380, Loss: 0.2494298592209816, Final Batch Loss: 0.11973985284566879\n",
      "Epoch 4381, Loss: 0.2486463338136673, Final Batch Loss: 0.1081143170595169\n",
      "Epoch 4382, Loss: 0.3081008344888687, Final Batch Loss: 0.196218341588974\n",
      "Epoch 4383, Loss: 0.26649003475904465, Final Batch Loss: 0.10476284474134445\n",
      "Epoch 4384, Loss: 0.23562310636043549, Final Batch Loss: 0.10327060520648956\n",
      "Epoch 4385, Loss: 0.2557440549135208, Final Batch Loss: 0.12007381021976471\n",
      "Epoch 4386, Loss: 0.3103003054857254, Final Batch Loss: 0.14588648080825806\n",
      "Epoch 4387, Loss: 0.3426075726747513, Final Batch Loss: 0.2156600058078766\n",
      "Epoch 4388, Loss: 0.250105656683445, Final Batch Loss: 0.13540585339069366\n",
      "Epoch 4389, Loss: 0.2581458017230034, Final Batch Loss: 0.15985165536403656\n",
      "Epoch 4390, Loss: 0.2611670345067978, Final Batch Loss: 0.12144200503826141\n",
      "Epoch 4391, Loss: 0.2842981517314911, Final Batch Loss: 0.15829527378082275\n",
      "Epoch 4392, Loss: 0.34117700159549713, Final Batch Loss: 0.1886248141527176\n",
      "Epoch 4393, Loss: 0.3150639832019806, Final Batch Loss: 0.16219761967658997\n",
      "Epoch 4394, Loss: 0.3052690774202347, Final Batch Loss: 0.14372196793556213\n",
      "Epoch 4395, Loss: 0.27665529400110245, Final Batch Loss: 0.15471407771110535\n",
      "Epoch 4396, Loss: 0.3595771938562393, Final Batch Loss: 0.20811472833156586\n",
      "Epoch 4397, Loss: 0.34554461389780045, Final Batch Loss: 0.2281334102153778\n",
      "Epoch 4398, Loss: 0.263726606965065, Final Batch Loss: 0.15192456543445587\n",
      "Epoch 4399, Loss: 0.3624972850084305, Final Batch Loss: 0.22499802708625793\n",
      "Epoch 4400, Loss: 0.2871662825345993, Final Batch Loss: 0.13840918242931366\n",
      "Epoch 4401, Loss: 0.37218236923217773, Final Batch Loss: 0.22686102986335754\n",
      "Epoch 4402, Loss: 0.2970998287200928, Final Batch Loss: 0.1522398293018341\n",
      "Epoch 4403, Loss: 0.24733129888772964, Final Batch Loss: 0.13593098521232605\n",
      "Epoch 4404, Loss: 0.27439309656620026, Final Batch Loss: 0.12950445711612701\n",
      "Epoch 4405, Loss: 0.2943901866674423, Final Batch Loss: 0.15420445799827576\n",
      "Epoch 4406, Loss: 0.2870466262102127, Final Batch Loss: 0.13381549715995789\n",
      "Epoch 4407, Loss: 0.3081607222557068, Final Batch Loss: 0.1746336668729782\n",
      "Epoch 4408, Loss: 0.3542136996984482, Final Batch Loss: 0.19919326901435852\n",
      "Epoch 4409, Loss: 0.2916288375854492, Final Batch Loss: 0.1332526057958603\n",
      "Epoch 4410, Loss: 0.3371599465608597, Final Batch Loss: 0.1582905799150467\n",
      "Epoch 4411, Loss: 0.3625610023736954, Final Batch Loss: 0.1460176557302475\n",
      "Epoch 4412, Loss: 0.3298013210296631, Final Batch Loss: 0.19832542538642883\n",
      "Epoch 4413, Loss: 0.29578426480293274, Final Batch Loss: 0.1702723205089569\n",
      "Epoch 4414, Loss: 0.27100689709186554, Final Batch Loss: 0.10844981670379639\n",
      "Epoch 4415, Loss: 0.29542914032936096, Final Batch Loss: 0.14818832278251648\n",
      "Epoch 4416, Loss: 0.2632573992013931, Final Batch Loss: 0.13773226737976074\n",
      "Epoch 4417, Loss: 0.26513659954071045, Final Batch Loss: 0.15173445641994476\n",
      "Epoch 4418, Loss: 0.2921505868434906, Final Batch Loss: 0.14497928321361542\n",
      "Epoch 4419, Loss: 0.2750026807188988, Final Batch Loss: 0.1244397833943367\n",
      "Epoch 4420, Loss: 0.28974229097366333, Final Batch Loss: 0.1281004101037979\n",
      "Epoch 4421, Loss: 0.22405438125133514, Final Batch Loss: 0.09772036969661713\n",
      "Epoch 4422, Loss: 0.28375017642974854, Final Batch Loss: 0.1438349187374115\n",
      "Epoch 4423, Loss: 0.28680404275655746, Final Batch Loss: 0.1710769385099411\n",
      "Epoch 4424, Loss: 0.29295913875102997, Final Batch Loss: 0.139144167304039\n",
      "Epoch 4425, Loss: 0.3171307295560837, Final Batch Loss: 0.17950722575187683\n",
      "Epoch 4426, Loss: 0.21404682099819183, Final Batch Loss: 0.08197915554046631\n",
      "Epoch 4427, Loss: 0.31655852496623993, Final Batch Loss: 0.19477851688861847\n",
      "Epoch 4428, Loss: 0.22690539807081223, Final Batch Loss: 0.08754032105207443\n",
      "Epoch 4429, Loss: 0.2816180884838104, Final Batch Loss: 0.1357705444097519\n",
      "Epoch 4430, Loss: 0.2829542011022568, Final Batch Loss: 0.14884884655475616\n",
      "Epoch 4431, Loss: 0.3283134549856186, Final Batch Loss: 0.18646366894245148\n",
      "Epoch 4432, Loss: 0.3021569699048996, Final Batch Loss: 0.17836423218250275\n",
      "Epoch 4433, Loss: 0.2262875735759735, Final Batch Loss: 0.08923755586147308\n",
      "Epoch 4434, Loss: 0.23726879805326462, Final Batch Loss: 0.1366034746170044\n",
      "Epoch 4435, Loss: 0.2934694364666939, Final Batch Loss: 0.16959276795387268\n",
      "Epoch 4436, Loss: 0.31314893066883087, Final Batch Loss: 0.18101246654987335\n",
      "Epoch 4437, Loss: 0.2332777976989746, Final Batch Loss: 0.09698605537414551\n",
      "Epoch 4438, Loss: 0.2621307671070099, Final Batch Loss: 0.12343406677246094\n",
      "Epoch 4439, Loss: 0.34296873211860657, Final Batch Loss: 0.17161878943443298\n",
      "Epoch 4440, Loss: 0.29762549698352814, Final Batch Loss: 0.16871076822280884\n",
      "Epoch 4441, Loss: 0.22918760776519775, Final Batch Loss: 0.09314681589603424\n",
      "Epoch 4442, Loss: 0.2776986286044121, Final Batch Loss: 0.1534627377986908\n",
      "Epoch 4443, Loss: 0.34674617648124695, Final Batch Loss: 0.17114178836345673\n",
      "Epoch 4444, Loss: 0.23565871268510818, Final Batch Loss: 0.10358395427465439\n",
      "Epoch 4445, Loss: 0.24928414076566696, Final Batch Loss: 0.094378761947155\n",
      "Epoch 4446, Loss: 0.30848217010498047, Final Batch Loss: 0.16979211568832397\n",
      "Epoch 4447, Loss: 0.21764785051345825, Final Batch Loss: 0.11779174208641052\n",
      "Epoch 4448, Loss: 0.2255955934524536, Final Batch Loss: 0.11403065174818039\n",
      "Epoch 4449, Loss: 0.2246851548552513, Final Batch Loss: 0.0889887884259224\n",
      "Epoch 4450, Loss: 0.31421326100826263, Final Batch Loss: 0.1766628921031952\n",
      "Epoch 4451, Loss: 0.2437596395611763, Final Batch Loss: 0.11822644621133804\n",
      "Epoch 4452, Loss: 0.346233531832695, Final Batch Loss: 0.18769875168800354\n",
      "Epoch 4453, Loss: 0.28431931883096695, Final Batch Loss: 0.10369113832712173\n",
      "Epoch 4454, Loss: 0.2806387171149254, Final Batch Loss: 0.1613643616437912\n",
      "Epoch 4455, Loss: 0.27242834866046906, Final Batch Loss: 0.1483365148305893\n",
      "Epoch 4456, Loss: 0.37951377034187317, Final Batch Loss: 0.24130746722221375\n",
      "Epoch 4457, Loss: 0.24756668508052826, Final Batch Loss: 0.1119946539402008\n",
      "Epoch 4458, Loss: 0.3146604746580124, Final Batch Loss: 0.15629231929779053\n",
      "Epoch 4459, Loss: 0.2586277648806572, Final Batch Loss: 0.14995406568050385\n",
      "Epoch 4460, Loss: 0.3038531094789505, Final Batch Loss: 0.15422601997852325\n",
      "Epoch 4461, Loss: 0.2684921771287918, Final Batch Loss: 0.14055196940898895\n",
      "Epoch 4462, Loss: 0.2562391757965088, Final Batch Loss: 0.11019103229045868\n",
      "Epoch 4463, Loss: 0.26498428732156754, Final Batch Loss: 0.1582002341747284\n",
      "Epoch 4464, Loss: 0.24250614643096924, Final Batch Loss: 0.07103085517883301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4465, Loss: 0.3658033013343811, Final Batch Loss: 0.16635072231292725\n",
      "Epoch 4466, Loss: 0.2911233603954315, Final Batch Loss: 0.13051879405975342\n",
      "Epoch 4467, Loss: 0.2575469836592674, Final Batch Loss: 0.11822658032178879\n",
      "Epoch 4468, Loss: 0.3682539761066437, Final Batch Loss: 0.2079239785671234\n",
      "Epoch 4469, Loss: 0.27321430295705795, Final Batch Loss: 0.1021430566906929\n",
      "Epoch 4470, Loss: 0.30008306354284286, Final Batch Loss: 0.11978942900896072\n",
      "Epoch 4471, Loss: 0.2633967995643616, Final Batch Loss: 0.09911787509918213\n",
      "Epoch 4472, Loss: 0.3423616588115692, Final Batch Loss: 0.20419622957706451\n",
      "Epoch 4473, Loss: 0.2959268242120743, Final Batch Loss: 0.13043341040611267\n",
      "Epoch 4474, Loss: 0.3151381313800812, Final Batch Loss: 0.18691645562648773\n",
      "Epoch 4475, Loss: 0.2610357403755188, Final Batch Loss: 0.14354631304740906\n",
      "Epoch 4476, Loss: 0.25235406309366226, Final Batch Loss: 0.15657971799373627\n",
      "Epoch 4477, Loss: 0.2814462333917618, Final Batch Loss: 0.12332142889499664\n",
      "Epoch 4478, Loss: 0.22006909549236298, Final Batch Loss: 0.09792917966842651\n",
      "Epoch 4479, Loss: 0.26357222348451614, Final Batch Loss: 0.15418481826782227\n",
      "Epoch 4480, Loss: 0.2241065725684166, Final Batch Loss: 0.11129985749721527\n",
      "Epoch 4481, Loss: 0.24989326298236847, Final Batch Loss: 0.12677329778671265\n",
      "Epoch 4482, Loss: 0.3189939707517624, Final Batch Loss: 0.13145498931407928\n",
      "Epoch 4483, Loss: 0.30035005509853363, Final Batch Loss: 0.17146697640419006\n",
      "Epoch 4484, Loss: 0.26139891147613525, Final Batch Loss: 0.15664316713809967\n",
      "Epoch 4485, Loss: 0.21611307561397552, Final Batch Loss: 0.09961776435375214\n",
      "Epoch 4486, Loss: 0.24131476879119873, Final Batch Loss: 0.1014900952577591\n",
      "Epoch 4487, Loss: 0.36965763568878174, Final Batch Loss: 0.2329338639974594\n",
      "Epoch 4488, Loss: 0.23619984090328217, Final Batch Loss: 0.12468751519918442\n",
      "Epoch 4489, Loss: 0.23736248165369034, Final Batch Loss: 0.11226721853017807\n",
      "Epoch 4490, Loss: 0.2324128895998001, Final Batch Loss: 0.1217954158782959\n",
      "Epoch 4491, Loss: 0.3144032955169678, Final Batch Loss: 0.17342223227024078\n",
      "Epoch 4492, Loss: 0.2768512964248657, Final Batch Loss: 0.14613929390907288\n",
      "Epoch 4493, Loss: 0.23081570863723755, Final Batch Loss: 0.13449923694133759\n",
      "Epoch 4494, Loss: 0.3233064264059067, Final Batch Loss: 0.1517445594072342\n",
      "Epoch 4495, Loss: 0.25141796469688416, Final Batch Loss: 0.1278214156627655\n",
      "Epoch 4496, Loss: 0.25320737808942795, Final Batch Loss: 0.11022382229566574\n",
      "Epoch 4497, Loss: 0.21247857064008713, Final Batch Loss: 0.08582647889852524\n",
      "Epoch 4498, Loss: 0.2866673618555069, Final Batch Loss: 0.15162554383277893\n",
      "Epoch 4499, Loss: 0.3089591711759567, Final Batch Loss: 0.1253446340560913\n",
      "Epoch 4500, Loss: 0.2355426475405693, Final Batch Loss: 0.12873531877994537\n",
      "Epoch 4501, Loss: 0.2547757029533386, Final Batch Loss: 0.11085985600948334\n",
      "Epoch 4502, Loss: 0.2923167943954468, Final Batch Loss: 0.17478521168231964\n",
      "Epoch 4503, Loss: 0.23781787604093552, Final Batch Loss: 0.0712544396519661\n",
      "Epoch 4504, Loss: 0.27170583605766296, Final Batch Loss: 0.11917591094970703\n",
      "Epoch 4505, Loss: 0.22567931562662125, Final Batch Loss: 0.10487625002861023\n",
      "Epoch 4506, Loss: 0.24354276061058044, Final Batch Loss: 0.10363541543483734\n",
      "Epoch 4507, Loss: 0.3072673976421356, Final Batch Loss: 0.15952269732952118\n",
      "Epoch 4508, Loss: 0.22584547847509384, Final Batch Loss: 0.07347551733255386\n",
      "Epoch 4509, Loss: 0.2976526468992233, Final Batch Loss: 0.08398009836673737\n",
      "Epoch 4510, Loss: 0.3155130594968796, Final Batch Loss: 0.12016506493091583\n",
      "Epoch 4511, Loss: 0.2701513171195984, Final Batch Loss: 0.1539817899465561\n",
      "Epoch 4512, Loss: 0.23394352197647095, Final Batch Loss: 0.12271790206432343\n",
      "Epoch 4513, Loss: 0.32049012184143066, Final Batch Loss: 0.17170995473861694\n",
      "Epoch 4514, Loss: 0.21811756491661072, Final Batch Loss: 0.09735000133514404\n",
      "Epoch 4515, Loss: 0.22921865433454514, Final Batch Loss: 0.10469537228345871\n",
      "Epoch 4516, Loss: 0.26652340590953827, Final Batch Loss: 0.12715208530426025\n",
      "Epoch 4517, Loss: 0.28888557851314545, Final Batch Loss: 0.17783582210540771\n",
      "Epoch 4518, Loss: 0.2694500610232353, Final Batch Loss: 0.10191109031438828\n",
      "Epoch 4519, Loss: 0.22776850312948227, Final Batch Loss: 0.09745506197214127\n",
      "Epoch 4520, Loss: 0.2187018096446991, Final Batch Loss: 0.09216359257698059\n",
      "Epoch 4521, Loss: 0.2259981781244278, Final Batch Loss: 0.10522108525037766\n",
      "Epoch 4522, Loss: 0.27126823365688324, Final Batch Loss: 0.14348824322223663\n",
      "Epoch 4523, Loss: 0.2720899060368538, Final Batch Loss: 0.1539376676082611\n",
      "Epoch 4524, Loss: 0.3048006519675255, Final Batch Loss: 0.19593572616577148\n",
      "Epoch 4525, Loss: 0.25142090022563934, Final Batch Loss: 0.12033286690711975\n",
      "Epoch 4526, Loss: 0.23904123157262802, Final Batch Loss: 0.10433318465948105\n",
      "Epoch 4527, Loss: 0.26598478108644485, Final Batch Loss: 0.1606329381465912\n",
      "Epoch 4528, Loss: 0.2829614356160164, Final Batch Loss: 0.16639535129070282\n",
      "Epoch 4529, Loss: 0.24518287926912308, Final Batch Loss: 0.11185970157384872\n",
      "Epoch 4530, Loss: 0.29062458872795105, Final Batch Loss: 0.1305340826511383\n",
      "Epoch 4531, Loss: 0.24711983650922775, Final Batch Loss: 0.11684968322515488\n",
      "Epoch 4532, Loss: 0.2680567055940628, Final Batch Loss: 0.1397262066602707\n",
      "Epoch 4533, Loss: 0.28009238839149475, Final Batch Loss: 0.16467450559139252\n",
      "Epoch 4534, Loss: 0.29259228706359863, Final Batch Loss: 0.16953101754188538\n",
      "Epoch 4535, Loss: 0.21539517492055893, Final Batch Loss: 0.1149374470114708\n",
      "Epoch 4536, Loss: 0.30851319432258606, Final Batch Loss: 0.14798834919929504\n",
      "Epoch 4537, Loss: 0.24692081660032272, Final Batch Loss: 0.12064791470766068\n",
      "Epoch 4538, Loss: 0.3217300772666931, Final Batch Loss: 0.1900329440832138\n",
      "Epoch 4539, Loss: 0.2873092442750931, Final Batch Loss: 0.15702161192893982\n",
      "Epoch 4540, Loss: 0.2749699428677559, Final Batch Loss: 0.1652458757162094\n",
      "Epoch 4541, Loss: 0.27245621383190155, Final Batch Loss: 0.16754721105098724\n",
      "Epoch 4542, Loss: 0.27795474231243134, Final Batch Loss: 0.147854745388031\n",
      "Epoch 4543, Loss: 0.2352142333984375, Final Batch Loss: 0.08953876793384552\n",
      "Epoch 4544, Loss: 0.2825622856616974, Final Batch Loss: 0.13641412556171417\n",
      "Epoch 4545, Loss: 0.2822532206773758, Final Batch Loss: 0.140168234705925\n",
      "Epoch 4546, Loss: 0.27361077070236206, Final Batch Loss: 0.14815659821033478\n",
      "Epoch 4547, Loss: 0.2727455347776413, Final Batch Loss: 0.1273367702960968\n",
      "Epoch 4548, Loss: 0.258529968559742, Final Batch Loss: 0.13961967825889587\n",
      "Epoch 4549, Loss: 0.2874990403652191, Final Batch Loss: 0.1701769083738327\n",
      "Epoch 4550, Loss: 0.20801008492708206, Final Batch Loss: 0.07716811448335648\n",
      "Epoch 4551, Loss: 0.3748260885477066, Final Batch Loss: 0.24523749947547913\n",
      "Epoch 4552, Loss: 0.21174054592847824, Final Batch Loss: 0.10676437616348267\n",
      "Epoch 4553, Loss: 0.6188098788261414, Final Batch Loss: 0.44965359568595886\n",
      "Epoch 4554, Loss: 0.3175749182701111, Final Batch Loss: 0.13304011523723602\n",
      "Epoch 4555, Loss: 0.2654467225074768, Final Batch Loss: 0.13478390872478485\n",
      "Epoch 4556, Loss: 0.32814663648605347, Final Batch Loss: 0.1673518419265747\n",
      "Epoch 4557, Loss: 0.29365722090005875, Final Batch Loss: 0.17142049968242645\n",
      "Epoch 4558, Loss: 0.22678468376398087, Final Batch Loss: 0.11686697602272034\n",
      "Epoch 4559, Loss: 0.2638590633869171, Final Batch Loss: 0.11141358315944672\n",
      "Epoch 4560, Loss: 0.27116020023822784, Final Batch Loss: 0.15182384848594666\n",
      "Epoch 4561, Loss: 0.28350380063056946, Final Batch Loss: 0.17802555859088898\n",
      "Epoch 4562, Loss: 0.26840685307979584, Final Batch Loss: 0.11678002774715424\n",
      "Epoch 4563, Loss: 0.27196890115737915, Final Batch Loss: 0.1342405527830124\n",
      "Epoch 4564, Loss: 0.26678289473056793, Final Batch Loss: 0.11014682054519653\n",
      "Epoch 4565, Loss: 0.28256288170814514, Final Batch Loss: 0.14758123457431793\n",
      "Epoch 4566, Loss: 0.2259170189499855, Final Batch Loss: 0.09527049213647842\n",
      "Epoch 4567, Loss: 0.293850377202034, Final Batch Loss: 0.16740472614765167\n",
      "Epoch 4568, Loss: 0.22429772466421127, Final Batch Loss: 0.09723830968141556\n",
      "Epoch 4569, Loss: 0.24333620071411133, Final Batch Loss: 0.11874207854270935\n",
      "Epoch 4570, Loss: 0.24365224689245224, Final Batch Loss: 0.11463382095098495\n",
      "Epoch 4571, Loss: 0.2221638783812523, Final Batch Loss: 0.09963817149400711\n",
      "Epoch 4572, Loss: 0.26466667652130127, Final Batch Loss: 0.10255730152130127\n",
      "Epoch 4573, Loss: 0.18900198489427567, Final Batch Loss: 0.07711900770664215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4574, Loss: 0.23559682816267014, Final Batch Loss: 0.10250278562307358\n",
      "Epoch 4575, Loss: 0.21051392704248428, Final Batch Loss: 0.08696439117193222\n",
      "Epoch 4576, Loss: 0.43414202332496643, Final Batch Loss: 0.2910040020942688\n",
      "Epoch 4577, Loss: 0.2679751366376877, Final Batch Loss: 0.13195200264453888\n",
      "Epoch 4578, Loss: 0.28599636256694794, Final Batch Loss: 0.14569789171218872\n",
      "Epoch 4579, Loss: 0.2278863713145256, Final Batch Loss: 0.0886564627289772\n",
      "Epoch 4580, Loss: 0.2750013917684555, Final Batch Loss: 0.15717898309230804\n",
      "Epoch 4581, Loss: 0.22788546234369278, Final Batch Loss: 0.09474452584981918\n",
      "Epoch 4582, Loss: 0.2639595568180084, Final Batch Loss: 0.12553448975086212\n",
      "Epoch 4583, Loss: 0.22655583918094635, Final Batch Loss: 0.0706121176481247\n",
      "Epoch 4584, Loss: 0.251204177737236, Final Batch Loss: 0.1254979372024536\n",
      "Epoch 4585, Loss: 0.2506110668182373, Final Batch Loss: 0.13936291635036469\n",
      "Epoch 4586, Loss: 0.27757370471954346, Final Batch Loss: 0.1379818171262741\n",
      "Epoch 4587, Loss: 0.2497987598180771, Final Batch Loss: 0.14913417398929596\n",
      "Epoch 4588, Loss: 0.2993280366063118, Final Batch Loss: 0.10645770281553268\n",
      "Epoch 4589, Loss: 0.2658047527074814, Final Batch Loss: 0.11531254649162292\n",
      "Epoch 4590, Loss: 0.22895367443561554, Final Batch Loss: 0.09326882660388947\n",
      "Epoch 4591, Loss: 0.2882067859172821, Final Batch Loss: 0.18742673099040985\n",
      "Epoch 4592, Loss: 0.2901100888848305, Final Batch Loss: 0.18396127223968506\n",
      "Epoch 4593, Loss: 0.29639481008052826, Final Batch Loss: 0.1707076132297516\n",
      "Epoch 4594, Loss: 0.21401380002498627, Final Batch Loss: 0.06481289863586426\n",
      "Epoch 4595, Loss: 0.23538753390312195, Final Batch Loss: 0.10835537314414978\n",
      "Epoch 4596, Loss: 0.2861715108156204, Final Batch Loss: 0.12320122122764587\n",
      "Epoch 4597, Loss: 0.2610958516597748, Final Batch Loss: 0.14068245887756348\n",
      "Epoch 4598, Loss: 0.313268318772316, Final Batch Loss: 0.17557036876678467\n",
      "Epoch 4599, Loss: 0.2852589040994644, Final Batch Loss: 0.14107279479503632\n",
      "Epoch 4600, Loss: 0.2443436086177826, Final Batch Loss: 0.10345287621021271\n",
      "Epoch 4601, Loss: 0.26092834025621414, Final Batch Loss: 0.1471872478723526\n",
      "Epoch 4602, Loss: 0.3484968841075897, Final Batch Loss: 0.20583410561084747\n",
      "Epoch 4603, Loss: 0.24581407755613327, Final Batch Loss: 0.11425342410802841\n",
      "Epoch 4604, Loss: 0.24751336127519608, Final Batch Loss: 0.07659914344549179\n",
      "Epoch 4605, Loss: 0.3279559463262558, Final Batch Loss: 0.17039629817008972\n",
      "Epoch 4606, Loss: 0.36907491087913513, Final Batch Loss: 0.22617211937904358\n",
      "Epoch 4607, Loss: 0.2415812388062477, Final Batch Loss: 0.10251516848802567\n",
      "Epoch 4608, Loss: 0.23420403897762299, Final Batch Loss: 0.12278759479522705\n",
      "Epoch 4609, Loss: 0.2291460782289505, Final Batch Loss: 0.10012227296829224\n",
      "Epoch 4610, Loss: 0.2645450085401535, Final Batch Loss: 0.1448633372783661\n",
      "Epoch 4611, Loss: 0.2729741036891937, Final Batch Loss: 0.1334921419620514\n",
      "Epoch 4612, Loss: 0.27172479778528214, Final Batch Loss: 0.11508042365312576\n",
      "Epoch 4613, Loss: 0.23606159538030624, Final Batch Loss: 0.0979917123913765\n",
      "Epoch 4614, Loss: 0.2732613533735275, Final Batch Loss: 0.13325536251068115\n",
      "Epoch 4615, Loss: 0.24813885986804962, Final Batch Loss: 0.09906268119812012\n",
      "Epoch 4616, Loss: 0.28970812261104584, Final Batch Loss: 0.17476356029510498\n",
      "Epoch 4617, Loss: 0.2920016720890999, Final Batch Loss: 0.12255264073610306\n",
      "Epoch 4618, Loss: 0.31244485825300217, Final Batch Loss: 0.2087249457836151\n",
      "Epoch 4619, Loss: 0.2657119333744049, Final Batch Loss: 0.13790510594844818\n",
      "Epoch 4620, Loss: 0.2784438580274582, Final Batch Loss: 0.1683313548564911\n",
      "Epoch 4621, Loss: 0.2603430598974228, Final Batch Loss: 0.13021692633628845\n",
      "Epoch 4622, Loss: 0.25034207850694656, Final Batch Loss: 0.10356726497411728\n",
      "Epoch 4623, Loss: 0.24476154893636703, Final Batch Loss: 0.1059824749827385\n",
      "Epoch 4624, Loss: 0.245842345058918, Final Batch Loss: 0.1229327917098999\n",
      "Epoch 4625, Loss: 0.3101997822523117, Final Batch Loss: 0.15701256692409515\n",
      "Epoch 4626, Loss: 0.20680895447731018, Final Batch Loss: 0.087688148021698\n",
      "Epoch 4627, Loss: 0.265752837061882, Final Batch Loss: 0.13789749145507812\n",
      "Epoch 4628, Loss: 0.2624783590435982, Final Batch Loss: 0.15256589651107788\n",
      "Epoch 4629, Loss: 0.30798690021038055, Final Batch Loss: 0.12739573419094086\n",
      "Epoch 4630, Loss: 0.258805014193058, Final Batch Loss: 0.1472315788269043\n",
      "Epoch 4631, Loss: 0.2527473568916321, Final Batch Loss: 0.09481488168239594\n",
      "Epoch 4632, Loss: 0.2383275032043457, Final Batch Loss: 0.10710029304027557\n",
      "Epoch 4633, Loss: 0.2927442938089371, Final Batch Loss: 0.12793132662773132\n",
      "Epoch 4634, Loss: 0.236860953271389, Final Batch Loss: 0.09887637943029404\n",
      "Epoch 4635, Loss: 0.2411041408777237, Final Batch Loss: 0.12849988043308258\n",
      "Epoch 4636, Loss: 0.2696334570646286, Final Batch Loss: 0.1437557488679886\n",
      "Epoch 4637, Loss: 0.21046335995197296, Final Batch Loss: 0.1136898547410965\n",
      "Epoch 4638, Loss: 0.327947199344635, Final Batch Loss: 0.18810302019119263\n",
      "Epoch 4639, Loss: 0.3330424502491951, Final Batch Loss: 0.22065110504627228\n",
      "Epoch 4640, Loss: 0.2798196002840996, Final Batch Loss: 0.15703503787517548\n",
      "Epoch 4641, Loss: 0.22308262437582016, Final Batch Loss: 0.0898137167096138\n",
      "Epoch 4642, Loss: 0.3185933232307434, Final Batch Loss: 0.15218791365623474\n",
      "Epoch 4643, Loss: 0.26973435282707214, Final Batch Loss: 0.1639474332332611\n",
      "Epoch 4644, Loss: 0.23871324956417084, Final Batch Loss: 0.12863317131996155\n",
      "Epoch 4645, Loss: 0.2186707705259323, Final Batch Loss: 0.11691199988126755\n",
      "Epoch 4646, Loss: 0.2709645479917526, Final Batch Loss: 0.1590331792831421\n",
      "Epoch 4647, Loss: 0.2601560205221176, Final Batch Loss: 0.11797697842121124\n",
      "Epoch 4648, Loss: 0.22838325053453445, Final Batch Loss: 0.11658096313476562\n",
      "Epoch 4649, Loss: 0.24676889926195145, Final Batch Loss: 0.11139265447854996\n",
      "Epoch 4650, Loss: 0.35143017768859863, Final Batch Loss: 0.22150780260562897\n",
      "Epoch 4651, Loss: 0.23243986815214157, Final Batch Loss: 0.1198154091835022\n",
      "Epoch 4652, Loss: 0.23460140079259872, Final Batch Loss: 0.13022328913211823\n",
      "Epoch 4653, Loss: 0.2029913365840912, Final Batch Loss: 0.0592934787273407\n",
      "Epoch 4654, Loss: 0.28612494468688965, Final Batch Loss: 0.16564489901065826\n",
      "Epoch 4655, Loss: 0.26552120596170425, Final Batch Loss: 0.16581453382968903\n",
      "Epoch 4656, Loss: 0.21530624479055405, Final Batch Loss: 0.07491867989301682\n",
      "Epoch 4657, Loss: 0.2682711258530617, Final Batch Loss: 0.155411958694458\n",
      "Epoch 4658, Loss: 0.25099901109933853, Final Batch Loss: 0.12820619344711304\n",
      "Epoch 4659, Loss: 0.2744351699948311, Final Batch Loss: 0.16370654106140137\n",
      "Epoch 4660, Loss: 0.2940293848514557, Final Batch Loss: 0.17452193796634674\n",
      "Epoch 4661, Loss: 0.24408428370952606, Final Batch Loss: 0.1283995658159256\n",
      "Epoch 4662, Loss: 0.3184804171323776, Final Batch Loss: 0.19407133758068085\n",
      "Epoch 4663, Loss: 0.24879328906536102, Final Batch Loss: 0.13316911458969116\n",
      "Epoch 4664, Loss: 0.2384248748421669, Final Batch Loss: 0.10013400763273239\n",
      "Epoch 4665, Loss: 0.2632724940776825, Final Batch Loss: 0.12960173189640045\n",
      "Epoch 4666, Loss: 0.24464285373687744, Final Batch Loss: 0.0953393280506134\n",
      "Epoch 4667, Loss: 0.28451577574014664, Final Batch Loss: 0.09473208338022232\n",
      "Epoch 4668, Loss: 0.22320178896188736, Final Batch Loss: 0.08936963230371475\n",
      "Epoch 4669, Loss: 0.25825002789497375, Final Batch Loss: 0.09606419503688812\n",
      "Epoch 4670, Loss: 0.24217771738767624, Final Batch Loss: 0.1159687414765358\n",
      "Epoch 4671, Loss: 0.27534131705760956, Final Batch Loss: 0.1557573825120926\n",
      "Epoch 4672, Loss: 0.2809283211827278, Final Batch Loss: 0.12321730703115463\n",
      "Epoch 4673, Loss: 0.26875627040863037, Final Batch Loss: 0.13559658825397491\n",
      "Epoch 4674, Loss: 0.25988492369651794, Final Batch Loss: 0.12549342215061188\n",
      "Epoch 4675, Loss: 0.20069631189107895, Final Batch Loss: 0.07032012194395065\n",
      "Epoch 4676, Loss: 0.2526732534170151, Final Batch Loss: 0.133283331990242\n",
      "Epoch 4677, Loss: 0.4555417597293854, Final Batch Loss: 0.3039771020412445\n",
      "Epoch 4678, Loss: 0.26649975776672363, Final Batch Loss: 0.13189147412776947\n",
      "Epoch 4679, Loss: 0.21594010293483734, Final Batch Loss: 0.09132064878940582\n",
      "Epoch 4680, Loss: 0.2328917160630226, Final Batch Loss: 0.09278326481580734\n",
      "Epoch 4681, Loss: 0.22671500593423843, Final Batch Loss: 0.11336200684309006\n",
      "Epoch 4682, Loss: 0.2326575592160225, Final Batch Loss: 0.12336196005344391\n",
      "Epoch 4683, Loss: 0.22870834916830063, Final Batch Loss: 0.12164825946092606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4684, Loss: 0.27927879989147186, Final Batch Loss: 0.15731242299079895\n",
      "Epoch 4685, Loss: 0.2535058259963989, Final Batch Loss: 0.11661502718925476\n",
      "Epoch 4686, Loss: 0.26656343787908554, Final Batch Loss: 0.17930257320404053\n",
      "Epoch 4687, Loss: 0.2822490409016609, Final Batch Loss: 0.1696174591779709\n",
      "Epoch 4688, Loss: 0.25757136940956116, Final Batch Loss: 0.12762421369552612\n",
      "Epoch 4689, Loss: 0.27054041624069214, Final Batch Loss: 0.13054709136486053\n",
      "Epoch 4690, Loss: 0.28596464544534683, Final Batch Loss: 0.11303464323282242\n",
      "Epoch 4691, Loss: 0.24741563200950623, Final Batch Loss: 0.10715709626674652\n",
      "Epoch 4692, Loss: 0.22142375260591507, Final Batch Loss: 0.08357607573270798\n",
      "Epoch 4693, Loss: 0.27670613676309586, Final Batch Loss: 0.11200832575559616\n",
      "Epoch 4694, Loss: 0.23782780766487122, Final Batch Loss: 0.11055432260036469\n",
      "Epoch 4695, Loss: 0.20988701283931732, Final Batch Loss: 0.09908168017864227\n",
      "Epoch 4696, Loss: 0.22705604135990143, Final Batch Loss: 0.1152566522359848\n",
      "Epoch 4697, Loss: 0.21568460762500763, Final Batch Loss: 0.08621451258659363\n",
      "Epoch 4698, Loss: 0.24026773124933243, Final Batch Loss: 0.11425059288740158\n",
      "Epoch 4699, Loss: 0.29088176786899567, Final Batch Loss: 0.13108858466148376\n",
      "Epoch 4700, Loss: 0.23393592238426208, Final Batch Loss: 0.0993475615978241\n",
      "Epoch 4701, Loss: 0.23764364421367645, Final Batch Loss: 0.12713858485221863\n",
      "Epoch 4702, Loss: 0.22920017689466476, Final Batch Loss: 0.13131318986415863\n",
      "Epoch 4703, Loss: 0.2289843186736107, Final Batch Loss: 0.11470203846693039\n",
      "Epoch 4704, Loss: 0.22857916355133057, Final Batch Loss: 0.10632195323705673\n",
      "Epoch 4705, Loss: 0.2345944419503212, Final Batch Loss: 0.1115403026342392\n",
      "Epoch 4706, Loss: 0.20960527658462524, Final Batch Loss: 0.0862191915512085\n",
      "Epoch 4707, Loss: 0.2597962021827698, Final Batch Loss: 0.1296592354774475\n",
      "Epoch 4708, Loss: 0.23892053216695786, Final Batch Loss: 0.11229617148637772\n",
      "Epoch 4709, Loss: 0.25644079595804214, Final Batch Loss: 0.1360878199338913\n",
      "Epoch 4710, Loss: 0.22433459758758545, Final Batch Loss: 0.11938658356666565\n",
      "Epoch 4711, Loss: 0.2542929947376251, Final Batch Loss: 0.1390278935432434\n",
      "Epoch 4712, Loss: 0.3452513962984085, Final Batch Loss: 0.2026003897190094\n",
      "Epoch 4713, Loss: 0.24732588231563568, Final Batch Loss: 0.10230749845504761\n",
      "Epoch 4714, Loss: 0.242426335811615, Final Batch Loss: 0.10044564306735992\n",
      "Epoch 4715, Loss: 0.3375644385814667, Final Batch Loss: 0.16563841700553894\n",
      "Epoch 4716, Loss: 0.26066282391548157, Final Batch Loss: 0.09645526111125946\n",
      "Epoch 4717, Loss: 0.33293968439102173, Final Batch Loss: 0.2092607319355011\n",
      "Epoch 4718, Loss: 0.28850623965263367, Final Batch Loss: 0.0778503268957138\n",
      "Epoch 4719, Loss: 0.2593678683042526, Final Batch Loss: 0.12176086008548737\n",
      "Epoch 4720, Loss: 0.21842432022094727, Final Batch Loss: 0.09084098041057587\n",
      "Epoch 4721, Loss: 0.2166544832289219, Final Batch Loss: 0.056044820696115494\n",
      "Epoch 4722, Loss: 0.26524750888347626, Final Batch Loss: 0.13140778243541718\n",
      "Epoch 4723, Loss: 0.33633750677108765, Final Batch Loss: 0.15893389284610748\n",
      "Epoch 4724, Loss: 0.27464258670806885, Final Batch Loss: 0.13842837512493134\n",
      "Epoch 4725, Loss: 0.2777102217078209, Final Batch Loss: 0.12173370271921158\n",
      "Epoch 4726, Loss: 0.3515138328075409, Final Batch Loss: 0.20367120206356049\n",
      "Epoch 4727, Loss: 0.2838515490293503, Final Batch Loss: 0.17576806247234344\n",
      "Epoch 4728, Loss: 0.2494679093360901, Final Batch Loss: 0.11902186274528503\n",
      "Epoch 4729, Loss: 0.2883620485663414, Final Batch Loss: 0.12431737035512924\n",
      "Epoch 4730, Loss: 0.1978558450937271, Final Batch Loss: 0.0615156888961792\n",
      "Epoch 4731, Loss: 0.24357447028160095, Final Batch Loss: 0.10658259689807892\n",
      "Epoch 4732, Loss: 0.272511251270771, Final Batch Loss: 0.16806437075138092\n",
      "Epoch 4733, Loss: 0.2138989195227623, Final Batch Loss: 0.09327136725187302\n",
      "Epoch 4734, Loss: 0.2641092985868454, Final Batch Loss: 0.12844137847423553\n",
      "Epoch 4735, Loss: 0.2753617987036705, Final Batch Loss: 0.16702374815940857\n",
      "Epoch 4736, Loss: 0.23052598536014557, Final Batch Loss: 0.09724913537502289\n",
      "Epoch 4737, Loss: 0.30825085937976837, Final Batch Loss: 0.16672810912132263\n",
      "Epoch 4738, Loss: 0.2658104673027992, Final Batch Loss: 0.09368666261434555\n",
      "Epoch 4739, Loss: 0.3011302351951599, Final Batch Loss: 0.13647358119487762\n",
      "Epoch 4740, Loss: 0.27150916308164597, Final Batch Loss: 0.1533338725566864\n",
      "Epoch 4741, Loss: 0.29095253348350525, Final Batch Loss: 0.14506937563419342\n",
      "Epoch 4742, Loss: 0.312828928232193, Final Batch Loss: 0.18514201045036316\n",
      "Epoch 4743, Loss: 0.2419068068265915, Final Batch Loss: 0.11060896515846252\n",
      "Epoch 4744, Loss: 0.23037611693143845, Final Batch Loss: 0.09015663713216782\n",
      "Epoch 4745, Loss: 0.33660888671875, Final Batch Loss: 0.17326736450195312\n",
      "Epoch 4746, Loss: 0.24052701890468597, Final Batch Loss: 0.1126556247472763\n",
      "Epoch 4747, Loss: 0.24091272801160812, Final Batch Loss: 0.12834730744361877\n",
      "Epoch 4748, Loss: 0.24491170048713684, Final Batch Loss: 0.12233076989650726\n",
      "Epoch 4749, Loss: 0.292132630944252, Final Batch Loss: 0.1531805694103241\n",
      "Epoch 4750, Loss: 0.2795894667506218, Final Batch Loss: 0.15580087900161743\n",
      "Epoch 4751, Loss: 0.25241993367671967, Final Batch Loss: 0.10359954833984375\n",
      "Epoch 4752, Loss: 0.2892206013202667, Final Batch Loss: 0.1345362365245819\n",
      "Epoch 4753, Loss: 0.3091309368610382, Final Batch Loss: 0.17112471163272858\n",
      "Epoch 4754, Loss: 0.3115624785423279, Final Batch Loss: 0.15534117817878723\n",
      "Epoch 4755, Loss: 0.2780625522136688, Final Batch Loss: 0.1598779559135437\n",
      "Epoch 4756, Loss: 0.18920262157917023, Final Batch Loss: 0.08031710237264633\n",
      "Epoch 4757, Loss: 0.33484233915805817, Final Batch Loss: 0.1846444010734558\n",
      "Epoch 4758, Loss: 0.2609929218888283, Final Batch Loss: 0.16037267446517944\n",
      "Epoch 4759, Loss: 0.24743641167879105, Final Batch Loss: 0.12883733212947845\n",
      "Epoch 4760, Loss: 0.22740207612514496, Final Batch Loss: 0.10672321915626526\n",
      "Epoch 4761, Loss: 0.2402673289179802, Final Batch Loss: 0.11350139230489731\n",
      "Epoch 4762, Loss: 0.23296894133090973, Final Batch Loss: 0.10699345171451569\n",
      "Epoch 4763, Loss: 0.2682398706674576, Final Batch Loss: 0.1571691930294037\n",
      "Epoch 4764, Loss: 0.26755568385124207, Final Batch Loss: 0.13124753534793854\n",
      "Epoch 4765, Loss: 0.2982531934976578, Final Batch Loss: 0.12106215953826904\n",
      "Epoch 4766, Loss: 0.263992540538311, Final Batch Loss: 0.11923470348119736\n",
      "Epoch 4767, Loss: 0.24867283552885056, Final Batch Loss: 0.13492533564567566\n",
      "Epoch 4768, Loss: 0.2707521766424179, Final Batch Loss: 0.1280113309621811\n",
      "Epoch 4769, Loss: 0.2357913702726364, Final Batch Loss: 0.09929530322551727\n",
      "Epoch 4770, Loss: 0.241640105843544, Final Batch Loss: 0.1412845402956009\n",
      "Epoch 4771, Loss: 0.20322897285223007, Final Batch Loss: 0.0909448117017746\n",
      "Epoch 4772, Loss: 0.24865776300430298, Final Batch Loss: 0.1404869556427002\n",
      "Epoch 4773, Loss: 0.2533038556575775, Final Batch Loss: 0.11028411984443665\n",
      "Epoch 4774, Loss: 0.2162264958024025, Final Batch Loss: 0.08397463709115982\n",
      "Epoch 4775, Loss: 0.21926021575927734, Final Batch Loss: 0.09034004807472229\n",
      "Epoch 4776, Loss: 0.27328166365623474, Final Batch Loss: 0.13386045396327972\n",
      "Epoch 4777, Loss: 0.32372236251831055, Final Batch Loss: 0.17617309093475342\n",
      "Epoch 4778, Loss: 0.2202126830816269, Final Batch Loss: 0.09081709384918213\n",
      "Epoch 4779, Loss: 0.25362998992204666, Final Batch Loss: 0.14509305357933044\n",
      "Epoch 4780, Loss: 0.2766646295785904, Final Batch Loss: 0.15556994080543518\n",
      "Epoch 4781, Loss: 0.23039349168539047, Final Batch Loss: 0.10327032953500748\n",
      "Epoch 4782, Loss: 0.23388932645320892, Final Batch Loss: 0.08666485548019409\n",
      "Epoch 4783, Loss: 0.26017244905233383, Final Batch Loss: 0.15348123013973236\n",
      "Epoch 4784, Loss: 0.23843291401863098, Final Batch Loss: 0.10529442131519318\n",
      "Epoch 4785, Loss: 0.20752284675836563, Final Batch Loss: 0.08056484907865524\n",
      "Epoch 4786, Loss: 0.3199758529663086, Final Batch Loss: 0.15337009727954865\n",
      "Epoch 4787, Loss: 0.2453082576394081, Final Batch Loss: 0.10320479422807693\n",
      "Epoch 4788, Loss: 0.29965414851903915, Final Batch Loss: 0.11589469760656357\n",
      "Epoch 4789, Loss: 0.2391824573278427, Final Batch Loss: 0.1279917061328888\n",
      "Epoch 4790, Loss: 0.23994304239749908, Final Batch Loss: 0.07926861941814423\n",
      "Epoch 4791, Loss: 0.3424181714653969, Final Batch Loss: 0.22024813294410706\n",
      "Epoch 4792, Loss: 0.2874022424221039, Final Batch Loss: 0.158531054854393\n",
      "Epoch 4793, Loss: 0.2179993838071823, Final Batch Loss: 0.08287632465362549\n",
      "Epoch 4794, Loss: 0.23700117319822311, Final Batch Loss: 0.12062300741672516\n",
      "Epoch 4795, Loss: 0.22550076991319656, Final Batch Loss: 0.09798208624124527\n",
      "Epoch 4796, Loss: 0.2933370769023895, Final Batch Loss: 0.1357005387544632\n",
      "Epoch 4797, Loss: 0.27743013203144073, Final Batch Loss: 0.16720323264598846\n",
      "Epoch 4798, Loss: 0.22342734038829803, Final Batch Loss: 0.11256363242864609\n",
      "Epoch 4799, Loss: 0.2787783741950989, Final Batch Loss: 0.14617493748664856\n",
      "Epoch 4800, Loss: 0.3210691809654236, Final Batch Loss: 0.1327545940876007\n",
      "Epoch 4801, Loss: 0.22560621052980423, Final Batch Loss: 0.06893264502286911\n",
      "Epoch 4802, Loss: 0.29510602355003357, Final Batch Loss: 0.15526393055915833\n",
      "Epoch 4803, Loss: 0.3001488149166107, Final Batch Loss: 0.15617899596691132\n",
      "Epoch 4804, Loss: 0.18109818175435066, Final Batch Loss: 0.053266603499650955\n",
      "Epoch 4805, Loss: 0.2151862233877182, Final Batch Loss: 0.10307949781417847\n",
      "Epoch 4806, Loss: 0.22245485335588455, Final Batch Loss: 0.08797077089548111\n",
      "Epoch 4807, Loss: 0.2876933515071869, Final Batch Loss: 0.12554751336574554\n",
      "Epoch 4808, Loss: 0.3111705482006073, Final Batch Loss: 0.1855073720216751\n",
      "Epoch 4809, Loss: 0.2562205195426941, Final Batch Loss: 0.15659013390541077\n",
      "Epoch 4810, Loss: 0.2634694427251816, Final Batch Loss: 0.13269270956516266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4811, Loss: 0.31383170187473297, Final Batch Loss: 0.16401049494743347\n",
      "Epoch 4812, Loss: 0.2973639518022537, Final Batch Loss: 0.159095898270607\n",
      "Epoch 4813, Loss: 0.2789681851863861, Final Batch Loss: 0.13612860441207886\n",
      "Epoch 4814, Loss: 0.2357311025261879, Final Batch Loss: 0.12653611600399017\n",
      "Epoch 4815, Loss: 0.2897080332040787, Final Batch Loss: 0.16835671663284302\n",
      "Epoch 4816, Loss: 0.23204652220010757, Final Batch Loss: 0.12423132359981537\n",
      "Epoch 4817, Loss: 0.22488322108983994, Final Batch Loss: 0.08565666526556015\n",
      "Epoch 4818, Loss: 0.2948049306869507, Final Batch Loss: 0.1649380326271057\n",
      "Epoch 4819, Loss: 0.25783146917819977, Final Batch Loss: 0.12582790851593018\n",
      "Epoch 4820, Loss: 0.2594936341047287, Final Batch Loss: 0.11153995990753174\n",
      "Epoch 4821, Loss: 0.21911034733057022, Final Batch Loss: 0.06377581506967545\n",
      "Epoch 4822, Loss: 0.25185662508010864, Final Batch Loss: 0.10590508580207825\n",
      "Epoch 4823, Loss: 0.27342377603054047, Final Batch Loss: 0.1258266270160675\n",
      "Epoch 4824, Loss: 0.21938182413578033, Final Batch Loss: 0.11977340281009674\n",
      "Epoch 4825, Loss: 0.2767447307705879, Final Batch Loss: 0.16109023988246918\n",
      "Epoch 4826, Loss: 0.34279483556747437, Final Batch Loss: 0.21487924456596375\n",
      "Epoch 4827, Loss: 0.23197372257709503, Final Batch Loss: 0.11179135739803314\n",
      "Epoch 4828, Loss: 0.24279873818159103, Final Batch Loss: 0.14326202869415283\n",
      "Epoch 4829, Loss: 0.2163023129105568, Final Batch Loss: 0.10661812871694565\n",
      "Epoch 4830, Loss: 0.27901728451251984, Final Batch Loss: 0.14607444405555725\n",
      "Epoch 4831, Loss: 0.26434338837862015, Final Batch Loss: 0.12447448819875717\n",
      "Epoch 4832, Loss: 0.2809786945581436, Final Batch Loss: 0.1265292912721634\n",
      "Epoch 4833, Loss: 0.1885906681418419, Final Batch Loss: 0.052647821605205536\n",
      "Epoch 4834, Loss: 0.23740793019533157, Final Batch Loss: 0.11938939243555069\n",
      "Epoch 4835, Loss: 0.2690543383359909, Final Batch Loss: 0.13794690370559692\n",
      "Epoch 4836, Loss: 0.2907855957746506, Final Batch Loss: 0.15437009930610657\n",
      "Epoch 4837, Loss: 0.2844594940543175, Final Batch Loss: 0.18405687808990479\n",
      "Epoch 4838, Loss: 0.256071537733078, Final Batch Loss: 0.12876927852630615\n",
      "Epoch 4839, Loss: 0.2504928261041641, Final Batch Loss: 0.13465233147144318\n",
      "Epoch 4840, Loss: 0.2629060447216034, Final Batch Loss: 0.15047909319400787\n",
      "Epoch 4841, Loss: 0.28907422721385956, Final Batch Loss: 0.1486499309539795\n",
      "Epoch 4842, Loss: 0.2424355447292328, Final Batch Loss: 0.09355534613132477\n",
      "Epoch 4843, Loss: 0.24885034561157227, Final Batch Loss: 0.11974479258060455\n",
      "Epoch 4844, Loss: 0.29070543497800827, Final Batch Loss: 0.17955735325813293\n",
      "Epoch 4845, Loss: 0.24989893287420273, Final Batch Loss: 0.1450466513633728\n",
      "Epoch 4846, Loss: 0.22366134077310562, Final Batch Loss: 0.07820265740156174\n",
      "Epoch 4847, Loss: 0.2166411504149437, Final Batch Loss: 0.10795670747756958\n",
      "Epoch 4848, Loss: 0.36497123539447784, Final Batch Loss: 0.22871588170528412\n",
      "Epoch 4849, Loss: 0.23428203165531158, Final Batch Loss: 0.0976492315530777\n",
      "Epoch 4850, Loss: 0.2006906270980835, Final Batch Loss: 0.09635493159294128\n",
      "Epoch 4851, Loss: 0.2723558098077774, Final Batch Loss: 0.17286469042301178\n",
      "Epoch 4852, Loss: 0.24433324486017227, Final Batch Loss: 0.12516902387142181\n",
      "Epoch 4853, Loss: 0.27057238668203354, Final Batch Loss: 0.12088633328676224\n",
      "Epoch 4854, Loss: 0.23597436398267746, Final Batch Loss: 0.07022511214017868\n",
      "Epoch 4855, Loss: 0.19049106538295746, Final Batch Loss: 0.07728449255228043\n",
      "Epoch 4856, Loss: 0.2639433890581131, Final Batch Loss: 0.1237829327583313\n",
      "Epoch 4857, Loss: 0.2419706955552101, Final Batch Loss: 0.1132485494017601\n",
      "Epoch 4858, Loss: 0.20356591790914536, Final Batch Loss: 0.06502627581357956\n",
      "Epoch 4859, Loss: 0.252207450568676, Final Batch Loss: 0.13416644930839539\n",
      "Epoch 4860, Loss: 0.24134797602891922, Final Batch Loss: 0.118316151201725\n",
      "Epoch 4861, Loss: 0.328652560710907, Final Batch Loss: 0.1858624368906021\n",
      "Epoch 4862, Loss: 0.2733589857816696, Final Batch Loss: 0.14540685713291168\n",
      "Epoch 4863, Loss: 0.34065696597099304, Final Batch Loss: 0.10387946665287018\n",
      "Epoch 4864, Loss: 0.29047413170337677, Final Batch Loss: 0.16021192073822021\n",
      "Epoch 4865, Loss: 0.23477939516305923, Final Batch Loss: 0.13158579170703888\n",
      "Epoch 4866, Loss: 0.25629088282585144, Final Batch Loss: 0.10750171542167664\n",
      "Epoch 4867, Loss: 0.28773923218250275, Final Batch Loss: 0.12751854956150055\n",
      "Epoch 4868, Loss: 0.2768855690956116, Final Batch Loss: 0.128976508975029\n",
      "Epoch 4869, Loss: 0.27733948826789856, Final Batch Loss: 0.1571584790945053\n",
      "Epoch 4870, Loss: 0.20927656441926956, Final Batch Loss: 0.0855078473687172\n",
      "Epoch 4871, Loss: 0.2902083694934845, Final Batch Loss: 0.17776517570018768\n",
      "Epoch 4872, Loss: 0.25424008816480637, Final Batch Loss: 0.11701487749814987\n",
      "Epoch 4873, Loss: 0.31024543941020966, Final Batch Loss: 0.1837099939584732\n",
      "Epoch 4874, Loss: 0.23664721846580505, Final Batch Loss: 0.0858214944601059\n",
      "Epoch 4875, Loss: 0.2427857145667076, Final Batch Loss: 0.09381870180368423\n",
      "Epoch 4876, Loss: 0.29415756464004517, Final Batch Loss: 0.14519977569580078\n",
      "Epoch 4877, Loss: 0.22654396295547485, Final Batch Loss: 0.09185740351676941\n",
      "Epoch 4878, Loss: 0.3245803043246269, Final Batch Loss: 0.20022204518318176\n",
      "Epoch 4879, Loss: 0.27368784695863724, Final Batch Loss: 0.1561499536037445\n",
      "Epoch 4880, Loss: 0.27261991053819656, Final Batch Loss: 0.11956387013196945\n",
      "Epoch 4881, Loss: 0.2557763308286667, Final Batch Loss: 0.09969422221183777\n",
      "Epoch 4882, Loss: 0.20404411107301712, Final Batch Loss: 0.09737912565469742\n",
      "Epoch 4883, Loss: 0.2405598759651184, Final Batch Loss: 0.1422048658132553\n",
      "Epoch 4884, Loss: 0.22937018424272537, Final Batch Loss: 0.10458674281835556\n",
      "Epoch 4885, Loss: 0.24607918411493301, Final Batch Loss: 0.12096347659826279\n",
      "Epoch 4886, Loss: 0.24514879286289215, Final Batch Loss: 0.11180703341960907\n",
      "Epoch 4887, Loss: 0.23874831199645996, Final Batch Loss: 0.10988372564315796\n",
      "Epoch 4888, Loss: 0.2301926463842392, Final Batch Loss: 0.11471247673034668\n",
      "Epoch 4889, Loss: 0.21970397979021072, Final Batch Loss: 0.10049283504486084\n",
      "Epoch 4890, Loss: 0.26084253191947937, Final Batch Loss: 0.11163945496082306\n",
      "Epoch 4891, Loss: 0.27948976680636406, Final Batch Loss: 0.06165684387087822\n",
      "Epoch 4892, Loss: 0.30325504392385483, Final Batch Loss: 0.1987113654613495\n",
      "Epoch 4893, Loss: 0.19261084496974945, Final Batch Loss: 0.07308661937713623\n",
      "Epoch 4894, Loss: 0.27179569005966187, Final Batch Loss: 0.1377059370279312\n",
      "Epoch 4895, Loss: 0.23048630356788635, Final Batch Loss: 0.11962714046239853\n",
      "Epoch 4896, Loss: 0.2495751455426216, Final Batch Loss: 0.08882582932710648\n",
      "Epoch 4897, Loss: 0.26469510048627853, Final Batch Loss: 0.15515270829200745\n",
      "Epoch 4898, Loss: 0.34484025835990906, Final Batch Loss: 0.20548835396766663\n",
      "Epoch 4899, Loss: 0.23271583020687103, Final Batch Loss: 0.11017517000436783\n",
      "Epoch 4900, Loss: 0.2639741897583008, Final Batch Loss: 0.16007766127586365\n",
      "Epoch 4901, Loss: 0.21551986038684845, Final Batch Loss: 0.10148991644382477\n",
      "Epoch 4902, Loss: 0.2747681364417076, Final Batch Loss: 0.15605658292770386\n",
      "Epoch 4903, Loss: 0.2921842932701111, Final Batch Loss: 0.15395303070545197\n",
      "Epoch 4904, Loss: 0.31464867293834686, Final Batch Loss: 0.16597121953964233\n",
      "Epoch 4905, Loss: 0.20803453773260117, Final Batch Loss: 0.08177059143781662\n",
      "Epoch 4906, Loss: 0.20690106600522995, Final Batch Loss: 0.08562164753675461\n",
      "Epoch 4907, Loss: 0.2673359736800194, Final Batch Loss: 0.15150031447410583\n",
      "Epoch 4908, Loss: 0.2670951262116432, Final Batch Loss: 0.1634647250175476\n",
      "Epoch 4909, Loss: 0.21775884926319122, Final Batch Loss: 0.09476230293512344\n",
      "Epoch 4910, Loss: 0.2556459680199623, Final Batch Loss: 0.11892958730459213\n",
      "Epoch 4911, Loss: 0.2522258013486862, Final Batch Loss: 0.13369111716747284\n",
      "Epoch 4912, Loss: 0.2791513651609421, Final Batch Loss: 0.12663424015045166\n",
      "Epoch 4913, Loss: 0.2491801381111145, Final Batch Loss: 0.12876112759113312\n",
      "Epoch 4914, Loss: 0.23804468661546707, Final Batch Loss: 0.09695041924715042\n",
      "Epoch 4915, Loss: 0.3540444076061249, Final Batch Loss: 0.1927458494901657\n",
      "Epoch 4916, Loss: 0.18884316086769104, Final Batch Loss: 0.0806078314781189\n",
      "Epoch 4917, Loss: 0.29241499304771423, Final Batch Loss: 0.12228111922740936\n",
      "Epoch 4918, Loss: 0.23730170726776123, Final Batch Loss: 0.11664893478155136\n",
      "Epoch 4919, Loss: 0.3037605285644531, Final Batch Loss: 0.17781418561935425\n",
      "Epoch 4920, Loss: 0.2547270506620407, Final Batch Loss: 0.10130812227725983\n",
      "Epoch 4921, Loss: 0.24245361238718033, Final Batch Loss: 0.11867884546518326\n",
      "Epoch 4922, Loss: 0.23843223601579666, Final Batch Loss: 0.08692175894975662\n",
      "Epoch 4923, Loss: 0.2470865324139595, Final Batch Loss: 0.11947769671678543\n",
      "Epoch 4924, Loss: 0.26178915798664093, Final Batch Loss: 0.1489579826593399\n",
      "Epoch 4925, Loss: 0.27690957486629486, Final Batch Loss: 0.13163653016090393\n",
      "Epoch 4926, Loss: 0.18180199712514877, Final Batch Loss: 0.07626224309206009\n",
      "Epoch 4927, Loss: 0.27220548689365387, Final Batch Loss: 0.13796201348304749\n",
      "Epoch 4928, Loss: 0.23039057105779648, Final Batch Loss: 0.10857260227203369\n",
      "Epoch 4929, Loss: 0.31911601126194, Final Batch Loss: 0.18895219266414642\n",
      "Epoch 4930, Loss: 0.2939412370324135, Final Batch Loss: 0.07040471583604813\n",
      "Epoch 4931, Loss: 0.3496282547712326, Final Batch Loss: 0.25698861479759216\n",
      "Epoch 4932, Loss: 0.22916483134031296, Final Batch Loss: 0.0828123614192009\n",
      "Epoch 4933, Loss: 0.2213001251220703, Final Batch Loss: 0.1265585869550705\n",
      "Epoch 4934, Loss: 0.24120258539915085, Final Batch Loss: 0.07007014006376266\n",
      "Epoch 4935, Loss: 0.32812589406967163, Final Batch Loss: 0.22875230014324188\n",
      "Epoch 4936, Loss: 0.24545114487409592, Final Batch Loss: 0.13362394273281097\n",
      "Epoch 4937, Loss: 0.29307152330875397, Final Batch Loss: 0.12646234035491943\n",
      "Epoch 4938, Loss: 0.257351316511631, Final Batch Loss: 0.15973231196403503\n",
      "Epoch 4939, Loss: 0.2711533308029175, Final Batch Loss: 0.15952272713184357\n",
      "Epoch 4940, Loss: 0.2594054043292999, Final Batch Loss: 0.11547228693962097\n",
      "Epoch 4941, Loss: 0.21662407368421555, Final Batch Loss: 0.07992321997880936\n",
      "Epoch 4942, Loss: 0.2926713079214096, Final Batch Loss: 0.13436628878116608\n",
      "Epoch 4943, Loss: 0.2646012231707573, Final Batch Loss: 0.11621535569429398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4944, Loss: 0.23557303845882416, Final Batch Loss: 0.0638996958732605\n",
      "Epoch 4945, Loss: 0.27147282660007477, Final Batch Loss: 0.1186814159154892\n",
      "Epoch 4946, Loss: 0.30435270071029663, Final Batch Loss: 0.13013911247253418\n",
      "Epoch 4947, Loss: 0.2551223114132881, Final Batch Loss: 0.13478685915470123\n",
      "Epoch 4948, Loss: 0.2536313831806183, Final Batch Loss: 0.13702450692653656\n",
      "Epoch 4949, Loss: 0.21855850517749786, Final Batch Loss: 0.11394274979829788\n",
      "Epoch 4950, Loss: 0.21196738630533218, Final Batch Loss: 0.0789516344666481\n",
      "Epoch 4951, Loss: 0.3173287957906723, Final Batch Loss: 0.15231026709079742\n",
      "Epoch 4952, Loss: 0.283323772251606, Final Batch Loss: 0.19365011155605316\n",
      "Epoch 4953, Loss: 0.3092087209224701, Final Batch Loss: 0.13325832784175873\n",
      "Epoch 4954, Loss: 0.34014804661273956, Final Batch Loss: 0.21501202881336212\n",
      "Epoch 4955, Loss: 0.2588166892528534, Final Batch Loss: 0.1438741832971573\n",
      "Epoch 4956, Loss: 0.20368432253599167, Final Batch Loss: 0.07757727056741714\n",
      "Epoch 4957, Loss: 0.23098419606685638, Final Batch Loss: 0.12767302989959717\n",
      "Epoch 4958, Loss: 0.27546562999486923, Final Batch Loss: 0.11302023380994797\n",
      "Epoch 4959, Loss: 0.2931828647851944, Final Batch Loss: 0.13818895816802979\n",
      "Epoch 4960, Loss: 0.21024128794670105, Final Batch Loss: 0.08658666163682938\n",
      "Epoch 4961, Loss: 0.27356599271297455, Final Batch Loss: 0.12962619960308075\n",
      "Epoch 4962, Loss: 0.2669195830821991, Final Batch Loss: 0.14239825308322906\n",
      "Epoch 4963, Loss: 0.19453317672014236, Final Batch Loss: 0.0689932182431221\n",
      "Epoch 4964, Loss: 0.21579571068286896, Final Batch Loss: 0.09548298269510269\n",
      "Epoch 4965, Loss: 0.2775869518518448, Final Batch Loss: 0.11693945527076721\n",
      "Epoch 4966, Loss: 0.2742704600095749, Final Batch Loss: 0.1423761397600174\n",
      "Epoch 4967, Loss: 0.20148568600416183, Final Batch Loss: 0.09826404601335526\n",
      "Epoch 4968, Loss: 0.23380661010742188, Final Batch Loss: 0.10014843940734863\n",
      "Epoch 4969, Loss: 0.209725521504879, Final Batch Loss: 0.10117541998624802\n",
      "Epoch 4970, Loss: 0.2735072821378708, Final Batch Loss: 0.15869881212711334\n",
      "Epoch 4971, Loss: 0.2915404736995697, Final Batch Loss: 0.12527678906917572\n",
      "Epoch 4972, Loss: 0.2723313570022583, Final Batch Loss: 0.15194660425186157\n",
      "Epoch 4973, Loss: 0.2402225285768509, Final Batch Loss: 0.0970764309167862\n",
      "Epoch 4974, Loss: 0.29348931461572647, Final Batch Loss: 0.18181303143501282\n",
      "Epoch 4975, Loss: 0.23699354380369186, Final Batch Loss: 0.11207304894924164\n",
      "Epoch 4976, Loss: 0.28358636051416397, Final Batch Loss: 0.1606208235025406\n",
      "Epoch 4977, Loss: 0.25091397762298584, Final Batch Loss: 0.13014298677444458\n",
      "Epoch 4978, Loss: 0.22546512633562088, Final Batch Loss: 0.11204788088798523\n",
      "Epoch 4979, Loss: 0.20332535356283188, Final Batch Loss: 0.0581972673535347\n",
      "Epoch 4980, Loss: 0.24424105137586594, Final Batch Loss: 0.09592104703187943\n",
      "Epoch 4981, Loss: 0.24201997369527817, Final Batch Loss: 0.11038070172071457\n",
      "Epoch 4982, Loss: 0.20355983823537827, Final Batch Loss: 0.06416594237089157\n",
      "Epoch 4983, Loss: 0.21279533952474594, Final Batch Loss: 0.09765482693910599\n",
      "Epoch 4984, Loss: 0.2565357908606529, Final Batch Loss: 0.11391028016805649\n",
      "Epoch 4985, Loss: 0.2183927595615387, Final Batch Loss: 0.09493252635002136\n",
      "Epoch 4986, Loss: 0.26502825319767, Final Batch Loss: 0.1631140112876892\n",
      "Epoch 4987, Loss: 0.23688171803951263, Final Batch Loss: 0.09778019785881042\n",
      "Epoch 4988, Loss: 0.3021189570426941, Final Batch Loss: 0.12143895030021667\n",
      "Epoch 4989, Loss: 0.22165248543024063, Final Batch Loss: 0.10248077660799026\n",
      "Epoch 4990, Loss: 0.26751042157411575, Final Batch Loss: 0.11403772979974747\n",
      "Epoch 4991, Loss: 0.23261304199695587, Final Batch Loss: 0.12942785024642944\n",
      "Epoch 4992, Loss: 0.3271337226033211, Final Batch Loss: 0.11333062499761581\n",
      "Epoch 4993, Loss: 0.2832802012562752, Final Batch Loss: 0.164591982960701\n",
      "Epoch 4994, Loss: 0.21000616252422333, Final Batch Loss: 0.0976741760969162\n",
      "Epoch 4995, Loss: 0.19829323142766953, Final Batch Loss: 0.09667173773050308\n",
      "Epoch 4996, Loss: 0.2657037004828453, Final Batch Loss: 0.16078819334506989\n",
      "Epoch 4997, Loss: 0.2152213156223297, Final Batch Loss: 0.12099312245845795\n",
      "Epoch 4998, Loss: 0.24716749042272568, Final Batch Loss: 0.14225049316883087\n",
      "Epoch 4999, Loss: 0.24831916391849518, Final Batch Loss: 0.13749094307422638\n",
      "Epoch 5000, Loss: 0.24573327600955963, Final Batch Loss: 0.1332571655511856\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0  0  0  0  0  0  0]\n",
      " [ 0 13  0  0  0  0  0  0  0]\n",
      " [ 0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  0  0  0  6  0  0  5]\n",
      " [ 0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  1  0  0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        13\n",
      "           1    1.00000   1.00000   1.00000        13\n",
      "           2    1.00000   1.00000   1.00000        12\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    0.85714   0.54545   0.66667        11\n",
      "           6    1.00000   1.00000   1.00000         8\n",
      "           7    1.00000   1.00000   1.00000         6\n",
      "           8    0.58333   0.87500   0.70000         8\n",
      "\n",
      "    accuracy                        0.93258        89\n",
      "   macro avg    0.93783   0.93561   0.92963        89\n",
      "weighted avg    0.94489   0.93258   0.93184        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_Group_2_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0]\n",
      " [ 0  0  0  0 13  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  6]\n",
      " [ 1  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.90909   1.00000   0.95238        10\n",
      "           1    1.00000   1.00000   1.00000        12\n",
      "           2    1.00000   1.00000   1.00000        10\n",
      "           3    1.00000   1.00000   1.00000         7\n",
      "           4    1.00000   1.00000   1.00000        13\n",
      "           5    0.00000   0.00000   0.00000         6\n",
      "           6    1.00000   0.91667   0.95652        12\n",
      "           7    1.00000   1.00000   1.00000         9\n",
      "           8    0.62500   1.00000   0.76923        10\n",
      "\n",
      "    accuracy                        0.92135        89\n",
      "   macro avg    0.83712   0.87963   0.85313        89\n",
      "weighted avg    0.88023   0.92135   0.89544        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
