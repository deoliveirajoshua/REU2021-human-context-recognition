{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([7, 8, 11])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([7, 8, 11])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 7:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 8:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.194821000099182, Final Batch Loss: 1.0940951108932495\n",
      "Epoch 2, Loss: 2.187681794166565, Final Batch Loss: 1.08612060546875\n",
      "Epoch 3, Loss: 2.1929904222488403, Final Batch Loss: 1.0979920625686646\n",
      "Epoch 4, Loss: 2.1882786750793457, Final Batch Loss: 1.0929324626922607\n",
      "Epoch 5, Loss: 2.1913892030715942, Final Batch Loss: 1.0973268747329712\n",
      "Epoch 6, Loss: 2.1845779418945312, Final Batch Loss: 1.0919898748397827\n",
      "Epoch 7, Loss: 2.1818145513534546, Final Batch Loss: 1.0886448621749878\n",
      "Epoch 8, Loss: 2.180667996406555, Final Batch Loss: 1.0873827934265137\n",
      "Epoch 9, Loss: 2.180412173271179, Final Batch Loss: 1.0866116285324097\n",
      "Epoch 10, Loss: 2.17889666557312, Final Batch Loss: 1.0933647155761719\n",
      "Epoch 11, Loss: 2.1677247285842896, Final Batch Loss: 1.0761233568191528\n",
      "Epoch 12, Loss: 2.1666961908340454, Final Batch Loss: 1.0846829414367676\n",
      "Epoch 13, Loss: 2.1622544527053833, Final Batch Loss: 1.0786479711532593\n",
      "Epoch 14, Loss: 2.159706711769104, Final Batch Loss: 1.0804791450500488\n",
      "Epoch 15, Loss: 2.1528018712997437, Final Batch Loss: 1.0785199403762817\n",
      "Epoch 16, Loss: 2.1535340547561646, Final Batch Loss: 1.0822739601135254\n",
      "Epoch 17, Loss: 2.141903877258301, Final Batch Loss: 1.0660160779953003\n",
      "Epoch 18, Loss: 2.1321332454681396, Final Batch Loss: 1.058104395866394\n",
      "Epoch 19, Loss: 2.1288912296295166, Final Batch Loss: 1.063415288925171\n",
      "Epoch 20, Loss: 2.110050320625305, Final Batch Loss: 1.0568461418151855\n",
      "Epoch 21, Loss: 2.105354905128479, Final Batch Loss: 1.0579568147659302\n",
      "Epoch 22, Loss: 2.083880662918091, Final Batch Loss: 1.0273194313049316\n",
      "Epoch 23, Loss: 2.064710021018982, Final Batch Loss: 1.0173567533493042\n",
      "Epoch 24, Loss: 2.060268521308899, Final Batch Loss: 1.019819736480713\n",
      "Epoch 25, Loss: 2.0483102798461914, Final Batch Loss: 1.0165660381317139\n",
      "Epoch 26, Loss: 2.067865490913391, Final Batch Loss: 1.0364139080047607\n",
      "Epoch 27, Loss: 2.024571657180786, Final Batch Loss: 1.0003633499145508\n",
      "Epoch 28, Loss: 2.0074548721313477, Final Batch Loss: 1.012160301208496\n",
      "Epoch 29, Loss: 1.9927342534065247, Final Batch Loss: 0.9909374117851257\n",
      "Epoch 30, Loss: 1.961921513080597, Final Batch Loss: 0.9806540608406067\n",
      "Epoch 31, Loss: 1.9678809642791748, Final Batch Loss: 0.9564850330352783\n",
      "Epoch 32, Loss: 1.9306378364562988, Final Batch Loss: 0.9683436751365662\n",
      "Epoch 33, Loss: 1.902488648891449, Final Batch Loss: 0.9588165879249573\n",
      "Epoch 34, Loss: 1.8733935952186584, Final Batch Loss: 0.9320926666259766\n",
      "Epoch 35, Loss: 1.8501059412956238, Final Batch Loss: 0.9249191284179688\n",
      "Epoch 36, Loss: 1.8485124111175537, Final Batch Loss: 0.9231971502304077\n",
      "Epoch 37, Loss: 1.833202838897705, Final Batch Loss: 0.9487048983573914\n",
      "Epoch 38, Loss: 1.7725621461868286, Final Batch Loss: 0.9050626158714294\n",
      "Epoch 39, Loss: 1.7086589336395264, Final Batch Loss: 0.8742195963859558\n",
      "Epoch 40, Loss: 1.7028294205665588, Final Batch Loss: 0.8408448100090027\n",
      "Epoch 41, Loss: 1.7060216665267944, Final Batch Loss: 0.8608515858650208\n",
      "Epoch 42, Loss: 1.6462035179138184, Final Batch Loss: 0.8107173442840576\n",
      "Epoch 43, Loss: 1.5840407013893127, Final Batch Loss: 0.7851570248603821\n",
      "Epoch 44, Loss: 1.6201634407043457, Final Batch Loss: 0.8127816915512085\n",
      "Epoch 45, Loss: 1.601044476032257, Final Batch Loss: 0.7965023517608643\n",
      "Epoch 46, Loss: 1.5457040071487427, Final Batch Loss: 0.7679422497749329\n",
      "Epoch 47, Loss: 1.587418794631958, Final Batch Loss: 0.7602488398551941\n",
      "Epoch 48, Loss: 1.4988941550254822, Final Batch Loss: 0.6950822472572327\n",
      "Epoch 49, Loss: 1.4600370526313782, Final Batch Loss: 0.6973569989204407\n",
      "Epoch 50, Loss: 1.5024920105934143, Final Batch Loss: 0.7372190356254578\n",
      "Epoch 51, Loss: 1.4606613516807556, Final Batch Loss: 0.7109811305999756\n",
      "Epoch 52, Loss: 1.436750888824463, Final Batch Loss: 0.7086173892021179\n",
      "Epoch 53, Loss: 1.4109230041503906, Final Batch Loss: 0.670545756816864\n",
      "Epoch 54, Loss: 1.4565958380699158, Final Batch Loss: 0.7628097534179688\n",
      "Epoch 55, Loss: 1.410769522190094, Final Batch Loss: 0.7186642289161682\n",
      "Epoch 56, Loss: 1.3499440550804138, Final Batch Loss: 0.6410492062568665\n",
      "Epoch 57, Loss: 1.4133631587028503, Final Batch Loss: 0.7263562083244324\n",
      "Epoch 58, Loss: 1.3400748372077942, Final Batch Loss: 0.7188852429389954\n",
      "Epoch 59, Loss: 1.3197436332702637, Final Batch Loss: 0.6346005201339722\n",
      "Epoch 60, Loss: 1.323709785938263, Final Batch Loss: 0.6493998169898987\n",
      "Epoch 61, Loss: 1.3509159684181213, Final Batch Loss: 0.688204824924469\n",
      "Epoch 62, Loss: 1.3519076704978943, Final Batch Loss: 0.6780206561088562\n",
      "Epoch 63, Loss: 1.30769282579422, Final Batch Loss: 0.6311084032058716\n",
      "Epoch 64, Loss: 1.3411884903907776, Final Batch Loss: 0.6803115010261536\n",
      "Epoch 65, Loss: 1.30594140291214, Final Batch Loss: 0.6235483884811401\n",
      "Epoch 66, Loss: 1.321341097354889, Final Batch Loss: 0.6895274519920349\n",
      "Epoch 67, Loss: 1.3205331563949585, Final Batch Loss: 0.6782042384147644\n",
      "Epoch 68, Loss: 1.220845103263855, Final Batch Loss: 0.6331606507301331\n",
      "Epoch 69, Loss: 1.2832440733909607, Final Batch Loss: 0.6438882350921631\n",
      "Epoch 70, Loss: 1.2942431569099426, Final Batch Loss: 0.6752752065658569\n",
      "Epoch 71, Loss: 1.2666231989860535, Final Batch Loss: 0.6315791606903076\n",
      "Epoch 72, Loss: 1.2465539574623108, Final Batch Loss: 0.6337901949882507\n",
      "Epoch 73, Loss: 1.1961420774459839, Final Batch Loss: 0.5603402853012085\n",
      "Epoch 74, Loss: 1.2604909539222717, Final Batch Loss: 0.6331877708435059\n",
      "Epoch 75, Loss: 1.2081339359283447, Final Batch Loss: 0.6015674471855164\n",
      "Epoch 76, Loss: 1.203050434589386, Final Batch Loss: 0.6272860765457153\n",
      "Epoch 77, Loss: 1.2062185406684875, Final Batch Loss: 0.593716025352478\n",
      "Epoch 78, Loss: 1.2291966676712036, Final Batch Loss: 0.6285756230354309\n",
      "Epoch 79, Loss: 1.1710171699523926, Final Batch Loss: 0.5610610246658325\n",
      "Epoch 80, Loss: 1.1908215284347534, Final Batch Loss: 0.5932992696762085\n",
      "Epoch 81, Loss: 1.1788393259048462, Final Batch Loss: 0.6215822696685791\n",
      "Epoch 82, Loss: 1.098838448524475, Final Batch Loss: 0.5352029800415039\n",
      "Epoch 83, Loss: 1.1354455947875977, Final Batch Loss: 0.5628200769424438\n",
      "Epoch 84, Loss: 1.1604986190795898, Final Batch Loss: 0.5896700024604797\n",
      "Epoch 85, Loss: 1.1113439798355103, Final Batch Loss: 0.5581843256950378\n",
      "Epoch 86, Loss: 1.10110741853714, Final Batch Loss: 0.558754563331604\n",
      "Epoch 87, Loss: 1.1222335696220398, Final Batch Loss: 0.5804504156112671\n",
      "Epoch 88, Loss: 1.064595103263855, Final Batch Loss: 0.5657902956008911\n",
      "Epoch 89, Loss: 1.0737959742546082, Final Batch Loss: 0.5695357918739319\n",
      "Epoch 90, Loss: 1.0935664772987366, Final Batch Loss: 0.5406762957572937\n",
      "Epoch 91, Loss: 1.071928322315216, Final Batch Loss: 0.5346704721450806\n",
      "Epoch 92, Loss: 1.015079826116562, Final Batch Loss: 0.48508498072624207\n",
      "Epoch 93, Loss: 1.0525969862937927, Final Batch Loss: 0.5409573316574097\n",
      "Epoch 94, Loss: 1.0570377707481384, Final Batch Loss: 0.5534934401512146\n",
      "Epoch 95, Loss: 1.0944975018501282, Final Batch Loss: 0.5917733311653137\n",
      "Epoch 96, Loss: 1.0585692524909973, Final Batch Loss: 0.5097046494483948\n",
      "Epoch 97, Loss: 0.9646233022212982, Final Batch Loss: 0.45235249400138855\n",
      "Epoch 98, Loss: 1.0311238169670105, Final Batch Loss: 0.5408881902694702\n",
      "Epoch 99, Loss: 1.0390443801879883, Final Batch Loss: 0.5378088355064392\n",
      "Epoch 100, Loss: 1.00219064950943, Final Batch Loss: 0.5197786092758179\n",
      "Epoch 101, Loss: 1.0588718056678772, Final Batch Loss: 0.5401922464370728\n",
      "Epoch 102, Loss: 0.9812257289886475, Final Batch Loss: 0.5270962715148926\n",
      "Epoch 103, Loss: 1.0275417566299438, Final Batch Loss: 0.526528000831604\n",
      "Epoch 104, Loss: 0.9268122911453247, Final Batch Loss: 0.451594740152359\n",
      "Epoch 105, Loss: 0.993587076663971, Final Batch Loss: 0.49636757373809814\n",
      "Epoch 106, Loss: 0.9811671376228333, Final Batch Loss: 0.5026172399520874\n",
      "Epoch 107, Loss: 0.9698664546012878, Final Batch Loss: 0.46875542402267456\n",
      "Epoch 108, Loss: 0.9241230189800262, Final Batch Loss: 0.4163707196712494\n",
      "Epoch 109, Loss: 0.9064620137214661, Final Batch Loss: 0.42399466037750244\n",
      "Epoch 110, Loss: 0.8772020637989044, Final Batch Loss: 0.409471720457077\n",
      "Epoch 111, Loss: 0.9352636337280273, Final Batch Loss: 0.47932353615760803\n",
      "Epoch 112, Loss: 0.8673543334007263, Final Batch Loss: 0.4362312853336334\n",
      "Epoch 113, Loss: 0.946421205997467, Final Batch Loss: 0.456124484539032\n",
      "Epoch 114, Loss: 0.849788099527359, Final Batch Loss: 0.41179654002189636\n",
      "Epoch 115, Loss: 0.8547901511192322, Final Batch Loss: 0.41190192103385925\n",
      "Epoch 116, Loss: 0.8297212421894073, Final Batch Loss: 0.394161581993103\n",
      "Epoch 117, Loss: 0.8591907918453217, Final Batch Loss: 0.4114445745944977\n",
      "Epoch 118, Loss: 0.8788775503635406, Final Batch Loss: 0.47605523467063904\n",
      "Epoch 119, Loss: 0.8492994904518127, Final Batch Loss: 0.40857943892478943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 0.8816953897476196, Final Batch Loss: 0.4636005461215973\n",
      "Epoch 121, Loss: 0.8585975468158722, Final Batch Loss: 0.4347858130931854\n",
      "Epoch 122, Loss: 0.8403315544128418, Final Batch Loss: 0.4852052927017212\n",
      "Epoch 123, Loss: 0.8303239643573761, Final Batch Loss: 0.43681320548057556\n",
      "Epoch 124, Loss: 0.8155931532382965, Final Batch Loss: 0.4601527154445648\n",
      "Epoch 125, Loss: 0.7348661124706268, Final Batch Loss: 0.3362320363521576\n",
      "Epoch 126, Loss: 0.7765724360942841, Final Batch Loss: 0.4059750437736511\n",
      "Epoch 127, Loss: 0.7991903126239777, Final Batch Loss: 0.3518221378326416\n",
      "Epoch 128, Loss: 0.7860892117023468, Final Batch Loss: 0.39407721161842346\n",
      "Epoch 129, Loss: 0.7725684642791748, Final Batch Loss: 0.4056309759616852\n",
      "Epoch 130, Loss: 0.7773584723472595, Final Batch Loss: 0.3845042288303375\n",
      "Epoch 131, Loss: 0.7187990844249725, Final Batch Loss: 0.3698207139968872\n",
      "Epoch 132, Loss: 0.7183696329593658, Final Batch Loss: 0.34036776423454285\n",
      "Epoch 133, Loss: 0.7188697755336761, Final Batch Loss: 0.3517047166824341\n",
      "Epoch 134, Loss: 0.7450582981109619, Final Batch Loss: 0.37558504939079285\n",
      "Epoch 135, Loss: 0.7264488637447357, Final Batch Loss: 0.3851228654384613\n",
      "Epoch 136, Loss: 0.6721921563148499, Final Batch Loss: 0.3023938536643982\n",
      "Epoch 137, Loss: 0.667709231376648, Final Batch Loss: 0.34822455048561096\n",
      "Epoch 138, Loss: 0.7289561033248901, Final Batch Loss: 0.36505326628685\n",
      "Epoch 139, Loss: 0.7081815898418427, Final Batch Loss: 0.38017189502716064\n",
      "Epoch 140, Loss: 0.7117282748222351, Final Batch Loss: 0.3823840022087097\n",
      "Epoch 141, Loss: 0.649651974439621, Final Batch Loss: 0.3227578401565552\n",
      "Epoch 142, Loss: 0.7386678755283356, Final Batch Loss: 0.3729933798313141\n",
      "Epoch 143, Loss: 0.6799933910369873, Final Batch Loss: 0.34505900740623474\n",
      "Epoch 144, Loss: 0.676918238401413, Final Batch Loss: 0.38220953941345215\n",
      "Epoch 145, Loss: 0.6331399083137512, Final Batch Loss: 0.3257507383823395\n",
      "Epoch 146, Loss: 0.6839556097984314, Final Batch Loss: 0.36483535170555115\n",
      "Epoch 147, Loss: 0.6303440630435944, Final Batch Loss: 0.3218100666999817\n",
      "Epoch 148, Loss: 0.6257587969303131, Final Batch Loss: 0.30841583013534546\n",
      "Epoch 149, Loss: 0.6202586889266968, Final Batch Loss: 0.31091052293777466\n",
      "Epoch 150, Loss: 0.7155918180942535, Final Batch Loss: 0.3808436393737793\n",
      "Epoch 151, Loss: 0.686248391866684, Final Batch Loss: 0.36606061458587646\n",
      "Epoch 152, Loss: 0.6359008550643921, Final Batch Loss: 0.3179861903190613\n",
      "Epoch 153, Loss: 0.6353457570075989, Final Batch Loss: 0.35189175605773926\n",
      "Epoch 154, Loss: 0.6220696568489075, Final Batch Loss: 0.2721630036830902\n",
      "Epoch 155, Loss: 0.6183731555938721, Final Batch Loss: 0.32199952006340027\n",
      "Epoch 156, Loss: 0.5509694814682007, Final Batch Loss: 0.2864026129245758\n",
      "Epoch 157, Loss: 0.5638038814067841, Final Batch Loss: 0.27563121914863586\n",
      "Epoch 158, Loss: 0.5188574194908142, Final Batch Loss: 0.21677890419960022\n",
      "Epoch 159, Loss: 0.5875767171382904, Final Batch Loss: 0.3204878866672516\n",
      "Epoch 160, Loss: 0.5244041681289673, Final Batch Loss: 0.2597731649875641\n",
      "Epoch 161, Loss: 0.590199738740921, Final Batch Loss: 0.24458488821983337\n",
      "Epoch 162, Loss: 0.5460608005523682, Final Batch Loss: 0.2550446689128876\n",
      "Epoch 163, Loss: 0.5989077985286713, Final Batch Loss: 0.320610374212265\n",
      "Epoch 164, Loss: 0.5632559955120087, Final Batch Loss: 0.2957427501678467\n",
      "Epoch 165, Loss: 0.5508590042591095, Final Batch Loss: 0.2716366946697235\n",
      "Epoch 166, Loss: 0.5883822441101074, Final Batch Loss: 0.294349730014801\n",
      "Epoch 167, Loss: 0.5430931448936462, Final Batch Loss: 0.2526159882545471\n",
      "Epoch 168, Loss: 0.5413096249103546, Final Batch Loss: 0.2447110116481781\n",
      "Epoch 169, Loss: 0.5216152667999268, Final Batch Loss: 0.2527540326118469\n",
      "Epoch 170, Loss: 0.5232109129428864, Final Batch Loss: 0.2732037901878357\n",
      "Epoch 171, Loss: 0.5523810684680939, Final Batch Loss: 0.2587307095527649\n",
      "Epoch 172, Loss: 0.569882720708847, Final Batch Loss: 0.3000069260597229\n",
      "Epoch 173, Loss: 0.5490397065877914, Final Batch Loss: 0.31650614738464355\n",
      "Epoch 174, Loss: 0.5884519219398499, Final Batch Loss: 0.3195571303367615\n",
      "Epoch 175, Loss: 0.5666807591915131, Final Batch Loss: 0.25369518995285034\n",
      "Epoch 176, Loss: 0.5026422142982483, Final Batch Loss: 0.25338828563690186\n",
      "Epoch 177, Loss: 0.490651473402977, Final Batch Loss: 0.24632060527801514\n",
      "Epoch 178, Loss: 0.5426379144191742, Final Batch Loss: 0.28839442133903503\n",
      "Epoch 179, Loss: 0.5215779840946198, Final Batch Loss: 0.24224847555160522\n",
      "Epoch 180, Loss: 0.502476692199707, Final Batch Loss: 0.19454002380371094\n",
      "Epoch 181, Loss: 0.4773867726325989, Final Batch Loss: 0.2096330225467682\n",
      "Epoch 182, Loss: 0.5338406562805176, Final Batch Loss: 0.23937836289405823\n",
      "Epoch 183, Loss: 0.4716021716594696, Final Batch Loss: 0.22104096412658691\n",
      "Epoch 184, Loss: 0.43206971883773804, Final Batch Loss: 0.19420099258422852\n",
      "Epoch 185, Loss: 0.47259268164634705, Final Batch Loss: 0.22574995458126068\n",
      "Epoch 186, Loss: 0.49019308388233185, Final Batch Loss: 0.2668159008026123\n",
      "Epoch 187, Loss: 0.5017984211444855, Final Batch Loss: 0.25505733489990234\n",
      "Epoch 188, Loss: 0.45932522416114807, Final Batch Loss: 0.27488189935684204\n",
      "Epoch 189, Loss: 0.5585399270057678, Final Batch Loss: 0.3026343286037445\n",
      "Epoch 190, Loss: 0.5300400406122208, Final Batch Loss: 0.24499769508838654\n",
      "Epoch 191, Loss: 0.47156891226768494, Final Batch Loss: 0.25001025199890137\n",
      "Epoch 192, Loss: 0.5326850563287735, Final Batch Loss: 0.22621916234493256\n",
      "Epoch 193, Loss: 0.49332550168037415, Final Batch Loss: 0.2752004563808441\n",
      "Epoch 194, Loss: 0.4797982722520828, Final Batch Loss: 0.286575049161911\n",
      "Epoch 195, Loss: 0.45565488934516907, Final Batch Loss: 0.2694694995880127\n",
      "Epoch 196, Loss: 0.4690232425928116, Final Batch Loss: 0.24222543835639954\n",
      "Epoch 197, Loss: 0.48039986193180084, Final Batch Loss: 0.26203909516334534\n",
      "Epoch 198, Loss: 0.48753029108047485, Final Batch Loss: 0.2656117379665375\n",
      "Epoch 199, Loss: 0.4445730894804001, Final Batch Loss: 0.20999325811862946\n",
      "Epoch 200, Loss: 0.47003932297229767, Final Batch Loss: 0.2650074064731598\n",
      "Epoch 201, Loss: 0.4715268760919571, Final Batch Loss: 0.2509046196937561\n",
      "Epoch 202, Loss: 0.4715391546487808, Final Batch Loss: 0.25585609674453735\n",
      "Epoch 203, Loss: 0.4435685873031616, Final Batch Loss: 0.22665546834468842\n",
      "Epoch 204, Loss: 0.45983758568763733, Final Batch Loss: 0.23180022835731506\n",
      "Epoch 205, Loss: 0.3850401043891907, Final Batch Loss: 0.16544297337532043\n",
      "Epoch 206, Loss: 0.4202996641397476, Final Batch Loss: 0.20524747669696808\n",
      "Epoch 207, Loss: 0.4195367246866226, Final Batch Loss: 0.22377990186214447\n",
      "Epoch 208, Loss: 0.44442176818847656, Final Batch Loss: 0.2062825709581375\n",
      "Epoch 209, Loss: 0.43368376791477203, Final Batch Loss: 0.21791350841522217\n",
      "Epoch 210, Loss: 0.3957971781492233, Final Batch Loss: 0.17978668212890625\n",
      "Epoch 211, Loss: 0.45146533846855164, Final Batch Loss: 0.25581446290016174\n",
      "Epoch 212, Loss: 0.4463735669851303, Final Batch Loss: 0.1999533623456955\n",
      "Epoch 213, Loss: 0.37368088960647583, Final Batch Loss: 0.18729206919670105\n",
      "Epoch 214, Loss: 0.42529332637786865, Final Batch Loss: 0.20973308384418488\n",
      "Epoch 215, Loss: 0.4110659211874008, Final Batch Loss: 0.18604271113872528\n",
      "Epoch 216, Loss: 0.44924454391002655, Final Batch Loss: 0.20766016840934753\n",
      "Epoch 217, Loss: 0.4345216602087021, Final Batch Loss: 0.2295641452074051\n",
      "Epoch 218, Loss: 0.4220114201307297, Final Batch Loss: 0.2196105569601059\n",
      "Epoch 219, Loss: 0.4323009252548218, Final Batch Loss: 0.2038247287273407\n",
      "Epoch 220, Loss: 0.4278262108564377, Final Batch Loss: 0.25626593828201294\n",
      "Epoch 221, Loss: 0.3761488050222397, Final Batch Loss: 0.15765300393104553\n",
      "Epoch 222, Loss: 0.3978346139192581, Final Batch Loss: 0.19668756425380707\n",
      "Epoch 223, Loss: 0.45950473845005035, Final Batch Loss: 0.20707662403583527\n",
      "Epoch 224, Loss: 0.37821485102176666, Final Batch Loss: 0.15647576749324799\n",
      "Epoch 225, Loss: 0.4702479988336563, Final Batch Loss: 0.2678925395011902\n",
      "Epoch 226, Loss: 0.4755017161369324, Final Batch Loss: 0.25475892424583435\n",
      "Epoch 227, Loss: 0.39278729259967804, Final Batch Loss: 0.20885230600833893\n",
      "Epoch 228, Loss: 0.4526199400424957, Final Batch Loss: 0.2673584818840027\n",
      "Epoch 229, Loss: 0.34689269959926605, Final Batch Loss: 0.1579662710428238\n",
      "Epoch 230, Loss: 0.36685678362846375, Final Batch Loss: 0.16350196301937103\n",
      "Epoch 231, Loss: 0.44120602309703827, Final Batch Loss: 0.23146672546863556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, Loss: 0.45037248730659485, Final Batch Loss: 0.21635288000106812\n",
      "Epoch 233, Loss: 0.4000405967235565, Final Batch Loss: 0.24245291948318481\n",
      "Epoch 234, Loss: 0.36835335195064545, Final Batch Loss: 0.17048227787017822\n",
      "Epoch 235, Loss: 0.45512695610523224, Final Batch Loss: 0.28506162762641907\n",
      "Epoch 236, Loss: 0.3810323476791382, Final Batch Loss: 0.20404665172100067\n",
      "Epoch 237, Loss: 0.43941356241703033, Final Batch Loss: 0.18143607676029205\n",
      "Epoch 238, Loss: 0.4039086848497391, Final Batch Loss: 0.2054852396249771\n",
      "Epoch 239, Loss: 0.4128054082393646, Final Batch Loss: 0.23039352893829346\n",
      "Epoch 240, Loss: 0.41428859531879425, Final Batch Loss: 0.20402799546718597\n",
      "Epoch 241, Loss: 0.34912462532520294, Final Batch Loss: 0.18586604297161102\n",
      "Epoch 242, Loss: 0.381752073764801, Final Batch Loss: 0.17305772006511688\n",
      "Epoch 243, Loss: 0.4020598381757736, Final Batch Loss: 0.1815967857837677\n",
      "Epoch 244, Loss: 0.4144749343395233, Final Batch Loss: 0.19775962829589844\n",
      "Epoch 245, Loss: 0.38048340380191803, Final Batch Loss: 0.18998567759990692\n",
      "Epoch 246, Loss: 0.373596727848053, Final Batch Loss: 0.18535663187503815\n",
      "Epoch 247, Loss: 0.34836946427822113, Final Batch Loss: 0.16043603420257568\n",
      "Epoch 248, Loss: 0.36895713210105896, Final Batch Loss: 0.19877584278583527\n",
      "Epoch 249, Loss: 0.3611038029193878, Final Batch Loss: 0.18338559567928314\n",
      "Epoch 250, Loss: 0.33824729919433594, Final Batch Loss: 0.12940876185894012\n",
      "Epoch 251, Loss: 0.3948422372341156, Final Batch Loss: 0.20497505366802216\n",
      "Epoch 252, Loss: 0.3945809155702591, Final Batch Loss: 0.20095624029636383\n",
      "Epoch 253, Loss: 0.39854346215724945, Final Batch Loss: 0.19015419483184814\n",
      "Epoch 254, Loss: 0.37022289633750916, Final Batch Loss: 0.16864649951457977\n",
      "Epoch 255, Loss: 0.4144434481859207, Final Batch Loss: 0.2192438244819641\n",
      "Epoch 256, Loss: 0.3631021976470947, Final Batch Loss: 0.14147786796092987\n",
      "Epoch 257, Loss: 0.3891209214925766, Final Batch Loss: 0.1663922518491745\n",
      "Epoch 258, Loss: 0.37847231328487396, Final Batch Loss: 0.21548904478549957\n",
      "Epoch 259, Loss: 0.33477745950222015, Final Batch Loss: 0.1521594524383545\n",
      "Epoch 260, Loss: 0.32989542186260223, Final Batch Loss: 0.12881311774253845\n",
      "Epoch 261, Loss: 0.3487866222858429, Final Batch Loss: 0.15973101556301117\n",
      "Epoch 262, Loss: 0.3839575946331024, Final Batch Loss: 0.1976163536310196\n",
      "Epoch 263, Loss: 0.3883572369813919, Final Batch Loss: 0.2172633707523346\n",
      "Epoch 264, Loss: 0.36626654863357544, Final Batch Loss: 0.2072231024503708\n",
      "Epoch 265, Loss: 0.3249518573284149, Final Batch Loss: 0.17057287693023682\n",
      "Epoch 266, Loss: 0.3614869862794876, Final Batch Loss: 0.20426815748214722\n",
      "Epoch 267, Loss: 0.3498070687055588, Final Batch Loss: 0.1596684604883194\n",
      "Epoch 268, Loss: 0.36159302294254303, Final Batch Loss: 0.19198527932167053\n",
      "Epoch 269, Loss: 0.3954469710588455, Final Batch Loss: 0.19085180759429932\n",
      "Epoch 270, Loss: 0.36932213604450226, Final Batch Loss: 0.1518506556749344\n",
      "Epoch 271, Loss: 0.36383645236492157, Final Batch Loss: 0.19286005198955536\n",
      "Epoch 272, Loss: 0.3496665805578232, Final Batch Loss: 0.12442706525325775\n",
      "Epoch 273, Loss: 0.3897283375263214, Final Batch Loss: 0.20635712146759033\n",
      "Epoch 274, Loss: 0.4347444772720337, Final Batch Loss: 0.2521732747554779\n",
      "Epoch 275, Loss: 0.42728589475154877, Final Batch Loss: 0.25564587116241455\n",
      "Epoch 276, Loss: 0.4409235864877701, Final Batch Loss: 0.25483274459838867\n",
      "Epoch 277, Loss: 0.33624711632728577, Final Batch Loss: 0.18262812495231628\n",
      "Epoch 278, Loss: 0.32785218954086304, Final Batch Loss: 0.13979586958885193\n",
      "Epoch 279, Loss: 0.36204417049884796, Final Batch Loss: 0.20029889047145844\n",
      "Epoch 280, Loss: 0.34245388209819794, Final Batch Loss: 0.1547233909368515\n",
      "Epoch 281, Loss: 0.34194326400756836, Final Batch Loss: 0.15433461964130402\n",
      "Epoch 282, Loss: 0.4006321132183075, Final Batch Loss: 0.22337734699249268\n",
      "Epoch 283, Loss: 0.3048166036605835, Final Batch Loss: 0.13440224528312683\n",
      "Epoch 284, Loss: 0.3620695024728775, Final Batch Loss: 0.18644633889198303\n",
      "Epoch 285, Loss: 0.3541579395532608, Final Batch Loss: 0.20613673329353333\n",
      "Epoch 286, Loss: 0.3407144397497177, Final Batch Loss: 0.16700655221939087\n",
      "Epoch 287, Loss: 0.33764052391052246, Final Batch Loss: 0.179774671792984\n",
      "Epoch 288, Loss: 0.3746398538351059, Final Batch Loss: 0.2173265963792801\n",
      "Epoch 289, Loss: 0.3493534028530121, Final Batch Loss: 0.17067088186740875\n",
      "Epoch 290, Loss: 0.40309806168079376, Final Batch Loss: 0.22863227128982544\n",
      "Epoch 291, Loss: 0.35877829790115356, Final Batch Loss: 0.15784497559070587\n",
      "Epoch 292, Loss: 0.32516252994537354, Final Batch Loss: 0.1445588916540146\n",
      "Epoch 293, Loss: 0.3490016460418701, Final Batch Loss: 0.20894835889339447\n",
      "Epoch 294, Loss: 0.37462277710437775, Final Batch Loss: 0.21059651672840118\n",
      "Epoch 295, Loss: 0.2983391284942627, Final Batch Loss: 0.1531601846218109\n",
      "Epoch 296, Loss: 0.32842967659235, Final Batch Loss: 0.2069282829761505\n",
      "Epoch 297, Loss: 0.3413200229406357, Final Batch Loss: 0.17039300501346588\n",
      "Epoch 298, Loss: 0.3289550244808197, Final Batch Loss: 0.17842525243759155\n",
      "Epoch 299, Loss: 0.33757950365543365, Final Batch Loss: 0.18257558345794678\n",
      "Epoch 300, Loss: 0.3473058044910431, Final Batch Loss: 0.12448008358478546\n",
      "Epoch 301, Loss: 0.32972894608974457, Final Batch Loss: 0.17447318136692047\n",
      "Epoch 302, Loss: 0.3980354517698288, Final Batch Loss: 0.19916915893554688\n",
      "Epoch 303, Loss: 0.332476407289505, Final Batch Loss: 0.17920562624931335\n",
      "Epoch 304, Loss: 0.3085748255252838, Final Batch Loss: 0.15315312147140503\n",
      "Epoch 305, Loss: 0.30278025567531586, Final Batch Loss: 0.14495110511779785\n",
      "Epoch 306, Loss: 0.3423076570034027, Final Batch Loss: 0.19674155116081238\n",
      "Epoch 307, Loss: 0.32055823504924774, Final Batch Loss: 0.15985631942749023\n",
      "Epoch 308, Loss: 0.32765553891658783, Final Batch Loss: 0.15476615726947784\n",
      "Epoch 309, Loss: 0.3509532958269119, Final Batch Loss: 0.15578368306159973\n",
      "Epoch 310, Loss: 0.31735794246196747, Final Batch Loss: 0.13952842354774475\n",
      "Epoch 311, Loss: 0.3697137236595154, Final Batch Loss: 0.14355146884918213\n",
      "Epoch 312, Loss: 0.4096711128950119, Final Batch Loss: 0.24739797413349152\n",
      "Epoch 313, Loss: 0.3244350105524063, Final Batch Loss: 0.196478009223938\n",
      "Epoch 314, Loss: 0.30978162586688995, Final Batch Loss: 0.1343710869550705\n",
      "Epoch 315, Loss: 0.31519363820552826, Final Batch Loss: 0.16310067474842072\n",
      "Epoch 316, Loss: 0.32506148517131805, Final Batch Loss: 0.17875996232032776\n",
      "Epoch 317, Loss: 0.38128362596035004, Final Batch Loss: 0.17476817965507507\n",
      "Epoch 318, Loss: 0.32830481231212616, Final Batch Loss: 0.1480194479227066\n",
      "Epoch 319, Loss: 0.3161013126373291, Final Batch Loss: 0.1549062877893448\n",
      "Epoch 320, Loss: 0.3583827614784241, Final Batch Loss: 0.1556866616010666\n",
      "Epoch 321, Loss: 0.3032121881842613, Final Batch Loss: 0.11685595661401749\n",
      "Epoch 322, Loss: 0.30684223771095276, Final Batch Loss: 0.14545056223869324\n",
      "Epoch 323, Loss: 0.26198313385248184, Final Batch Loss: 0.11595053225755692\n",
      "Epoch 324, Loss: 0.3868098706007004, Final Batch Loss: 0.20434606075286865\n",
      "Epoch 325, Loss: 0.33427178859710693, Final Batch Loss: 0.1670452505350113\n",
      "Epoch 326, Loss: 0.28900246322155, Final Batch Loss: 0.1518472135066986\n",
      "Epoch 327, Loss: 0.3150906413793564, Final Batch Loss: 0.1552208811044693\n",
      "Epoch 328, Loss: 0.3141240179538727, Final Batch Loss: 0.13326279819011688\n",
      "Epoch 329, Loss: 0.32344819605350494, Final Batch Loss: 0.12693080306053162\n",
      "Epoch 330, Loss: 0.3108735680580139, Final Batch Loss: 0.16208064556121826\n",
      "Epoch 331, Loss: 0.28165051341056824, Final Batch Loss: 0.12992070615291595\n",
      "Epoch 332, Loss: 0.34637266397476196, Final Batch Loss: 0.1831432729959488\n",
      "Epoch 333, Loss: 0.3727696090936661, Final Batch Loss: 0.23675668239593506\n",
      "Epoch 334, Loss: 0.36319129168987274, Final Batch Loss: 0.1616598665714264\n",
      "Epoch 335, Loss: 0.33543696999549866, Final Batch Loss: 0.17480508983135223\n",
      "Epoch 336, Loss: 0.3602965176105499, Final Batch Loss: 0.14182132482528687\n",
      "Epoch 337, Loss: 0.3635867089033127, Final Batch Loss: 0.24669454991817474\n",
      "Epoch 338, Loss: 0.3272693306207657, Final Batch Loss: 0.16811279952526093\n",
      "Epoch 339, Loss: 0.3069559931755066, Final Batch Loss: 0.13162833452224731\n",
      "Epoch 340, Loss: 0.408401295542717, Final Batch Loss: 0.24898137152194977\n",
      "Epoch 341, Loss: 0.2653803452849388, Final Batch Loss: 0.09893868118524551\n",
      "Epoch 342, Loss: 0.39467330276966095, Final Batch Loss: 0.20104940235614777\n",
      "Epoch 343, Loss: 0.35128363966941833, Final Batch Loss: 0.186052605509758\n",
      "Epoch 344, Loss: 0.31702007353305817, Final Batch Loss: 0.14476607739925385\n",
      "Epoch 345, Loss: 0.295993909239769, Final Batch Loss: 0.12831559777259827\n",
      "Epoch 346, Loss: 0.2813555747270584, Final Batch Loss: 0.15294675529003143\n",
      "Epoch 347, Loss: 0.2816874235868454, Final Batch Loss: 0.11738766729831696\n",
      "Epoch 348, Loss: 0.38395604491233826, Final Batch Loss: 0.20294728875160217\n",
      "Epoch 349, Loss: 0.3423622399568558, Final Batch Loss: 0.18812920153141022\n",
      "Epoch 350, Loss: 0.2792547196149826, Final Batch Loss: 0.1171846091747284\n",
      "Epoch 351, Loss: 0.3499123305082321, Final Batch Loss: 0.17695482075214386\n",
      "Epoch 352, Loss: 0.28505654633045197, Final Batch Loss: 0.1393992006778717\n",
      "Epoch 353, Loss: 0.35255685448646545, Final Batch Loss: 0.16280589997768402\n",
      "Epoch 354, Loss: 0.3462854474782944, Final Batch Loss: 0.16304849088191986\n",
      "Epoch 355, Loss: 0.32544200122356415, Final Batch Loss: 0.1569499522447586\n",
      "Epoch 356, Loss: 0.36790595948696136, Final Batch Loss: 0.21095305681228638\n",
      "Epoch 357, Loss: 0.3300309181213379, Final Batch Loss: 0.17862847447395325\n",
      "Epoch 358, Loss: 0.2888100743293762, Final Batch Loss: 0.1353176385164261\n",
      "Epoch 359, Loss: 0.30588287115097046, Final Batch Loss: 0.1547262817621231\n",
      "Epoch 360, Loss: 0.2707946002483368, Final Batch Loss: 0.13037225604057312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361, Loss: 0.3046671897172928, Final Batch Loss: 0.1606651395559311\n",
      "Epoch 362, Loss: 0.28363221883773804, Final Batch Loss: 0.14639294147491455\n",
      "Epoch 363, Loss: 0.3352998197078705, Final Batch Loss: 0.17532287538051605\n",
      "Epoch 364, Loss: 0.2973029240965843, Final Batch Loss: 0.09730026870965958\n",
      "Epoch 365, Loss: 0.2865329831838608, Final Batch Loss: 0.14889885485172272\n",
      "Epoch 366, Loss: 0.3281131237745285, Final Batch Loss: 0.17584361135959625\n",
      "Epoch 367, Loss: 0.3122592866420746, Final Batch Loss: 0.13723859190940857\n",
      "Epoch 368, Loss: 0.28802041709423065, Final Batch Loss: 0.13983871042728424\n",
      "Epoch 369, Loss: 0.3066236227750778, Final Batch Loss: 0.16250498592853546\n",
      "Epoch 370, Loss: 0.29623544216156006, Final Batch Loss: 0.1556214988231659\n",
      "Epoch 371, Loss: 0.34386056661605835, Final Batch Loss: 0.20355741679668427\n",
      "Epoch 372, Loss: 0.3115878850221634, Final Batch Loss: 0.1763436198234558\n",
      "Epoch 373, Loss: 0.2675231024622917, Final Batch Loss: 0.11772208660840988\n",
      "Epoch 374, Loss: 0.33824464678764343, Final Batch Loss: 0.20907258987426758\n",
      "Epoch 375, Loss: 0.33595195412635803, Final Batch Loss: 0.16387397050857544\n",
      "Epoch 376, Loss: 0.2846665531396866, Final Batch Loss: 0.13170717656612396\n",
      "Epoch 377, Loss: 0.3109564185142517, Final Batch Loss: 0.17316940426826477\n",
      "Epoch 378, Loss: 0.3534381240606308, Final Batch Loss: 0.15717235207557678\n",
      "Epoch 379, Loss: 0.3677337318658829, Final Batch Loss: 0.16773618757724762\n",
      "Epoch 380, Loss: 0.2567264810204506, Final Batch Loss: 0.10226865857839584\n",
      "Epoch 381, Loss: 0.27046342194080353, Final Batch Loss: 0.1283615678548813\n",
      "Epoch 382, Loss: 0.2923300489783287, Final Batch Loss: 0.16830328106880188\n",
      "Epoch 383, Loss: 0.2978401631116867, Final Batch Loss: 0.14693909883499146\n",
      "Epoch 384, Loss: 0.2655191868543625, Final Batch Loss: 0.12000410258769989\n",
      "Epoch 385, Loss: 0.33528372645378113, Final Batch Loss: 0.1835068315267563\n",
      "Epoch 386, Loss: 0.2413444146513939, Final Batch Loss: 0.11562388390302658\n",
      "Epoch 387, Loss: 0.3100205212831497, Final Batch Loss: 0.20030246675014496\n",
      "Epoch 388, Loss: 0.3064820021390915, Final Batch Loss: 0.13279256224632263\n",
      "Epoch 389, Loss: 0.3108995407819748, Final Batch Loss: 0.18411733210086823\n",
      "Epoch 390, Loss: 0.29965367168188095, Final Batch Loss: 0.19460727274417877\n",
      "Epoch 391, Loss: 0.3060229420661926, Final Batch Loss: 0.15459302067756653\n",
      "Epoch 392, Loss: 0.26564860343933105, Final Batch Loss: 0.09910848736763\n",
      "Epoch 393, Loss: 0.3084157258272171, Final Batch Loss: 0.148335799574852\n",
      "Epoch 394, Loss: 0.2848076671361923, Final Batch Loss: 0.127456396818161\n",
      "Epoch 395, Loss: 0.27156777679920197, Final Batch Loss: 0.1443408727645874\n",
      "Epoch 396, Loss: 0.29190853238105774, Final Batch Loss: 0.15194158256053925\n",
      "Epoch 397, Loss: 0.2943739593029022, Final Batch Loss: 0.15938420593738556\n",
      "Epoch 398, Loss: 0.27212735265493393, Final Batch Loss: 0.12268594652414322\n",
      "Epoch 399, Loss: 0.25836239010095596, Final Batch Loss: 0.09569253772497177\n",
      "Epoch 400, Loss: 0.2693593502044678, Final Batch Loss: 0.16234064102172852\n",
      "Epoch 401, Loss: 0.324777789413929, Final Batch Loss: 0.24200335144996643\n",
      "Epoch 402, Loss: 0.2871257960796356, Final Batch Loss: 0.18795756995677948\n",
      "Epoch 403, Loss: 0.2949230521917343, Final Batch Loss: 0.14736762642860413\n",
      "Epoch 404, Loss: 0.3556172400712967, Final Batch Loss: 0.1886088103055954\n",
      "Epoch 405, Loss: 0.28429505974054337, Final Batch Loss: 0.16894084215164185\n",
      "Epoch 406, Loss: 0.2517278641462326, Final Batch Loss: 0.14154092967510223\n",
      "Epoch 407, Loss: 0.3006402403116226, Final Batch Loss: 0.12124717235565186\n",
      "Epoch 408, Loss: 0.27067984640598297, Final Batch Loss: 0.12258829176425934\n",
      "Epoch 409, Loss: 0.266671858727932, Final Batch Loss: 0.11875679343938828\n",
      "Epoch 410, Loss: 0.3155897706747055, Final Batch Loss: 0.1712135523557663\n",
      "Epoch 411, Loss: 0.2738547995686531, Final Batch Loss: 0.1160460039973259\n",
      "Epoch 412, Loss: 0.26811161637306213, Final Batch Loss: 0.14188696444034576\n",
      "Epoch 413, Loss: 0.25801824033260345, Final Batch Loss: 0.13594022393226624\n",
      "Epoch 414, Loss: 0.266550675034523, Final Batch Loss: 0.14216752350330353\n",
      "Epoch 415, Loss: 0.29290971159935, Final Batch Loss: 0.15394866466522217\n",
      "Epoch 416, Loss: 0.24677322059869766, Final Batch Loss: 0.14373885095119476\n",
      "Epoch 417, Loss: 0.2787303179502487, Final Batch Loss: 0.15335120260715485\n",
      "Epoch 418, Loss: 0.25574591755867004, Final Batch Loss: 0.11586898565292358\n",
      "Epoch 419, Loss: 0.25043751299381256, Final Batch Loss: 0.11801476776599884\n",
      "Epoch 420, Loss: 0.30873751640319824, Final Batch Loss: 0.08733688294887543\n",
      "Epoch 421, Loss: 0.30578671395778656, Final Batch Loss: 0.13833469152450562\n",
      "Epoch 422, Loss: 0.24281657487154007, Final Batch Loss: 0.09803874045610428\n",
      "Epoch 423, Loss: 0.30286672711372375, Final Batch Loss: 0.09373542666435242\n",
      "Epoch 424, Loss: 0.3131883144378662, Final Batch Loss: 0.17863067984580994\n",
      "Epoch 425, Loss: 0.27703776955604553, Final Batch Loss: 0.10108299553394318\n",
      "Epoch 426, Loss: 0.2629307210445404, Final Batch Loss: 0.0767354816198349\n",
      "Epoch 427, Loss: 0.3536463677883148, Final Batch Loss: 0.18458408117294312\n",
      "Epoch 428, Loss: 0.2470775544643402, Final Batch Loss: 0.1434326022863388\n",
      "Epoch 429, Loss: 0.2741828113794327, Final Batch Loss: 0.11451725661754608\n",
      "Epoch 430, Loss: 0.27838554978370667, Final Batch Loss: 0.12468813359737396\n",
      "Epoch 431, Loss: 0.2244223803281784, Final Batch Loss: 0.10380112379789352\n",
      "Epoch 432, Loss: 0.33472296595573425, Final Batch Loss: 0.19891183078289032\n",
      "Epoch 433, Loss: 0.21642187237739563, Final Batch Loss: 0.09495946764945984\n",
      "Epoch 434, Loss: 0.27835117280483246, Final Batch Loss: 0.14921057224273682\n",
      "Epoch 435, Loss: 0.2685866132378578, Final Batch Loss: 0.11580749601125717\n",
      "Epoch 436, Loss: 0.34910689294338226, Final Batch Loss: 0.23445922136306763\n",
      "Epoch 437, Loss: 0.26092755794525146, Final Batch Loss: 0.1335907131433487\n",
      "Epoch 438, Loss: 0.3180264085531235, Final Batch Loss: 0.1776379942893982\n",
      "Epoch 439, Loss: 0.2516162768006325, Final Batch Loss: 0.1306905448436737\n",
      "Epoch 440, Loss: 0.30671150982379913, Final Batch Loss: 0.12977512180805206\n",
      "Epoch 441, Loss: 0.24512333422899246, Final Batch Loss: 0.11353414505720139\n",
      "Epoch 442, Loss: 0.27946120500564575, Final Batch Loss: 0.164810448884964\n",
      "Epoch 443, Loss: 0.30259934067726135, Final Batch Loss: 0.17324654757976532\n",
      "Epoch 444, Loss: 0.30679768323898315, Final Batch Loss: 0.14733508229255676\n",
      "Epoch 445, Loss: 0.28057458996772766, Final Batch Loss: 0.1518312245607376\n",
      "Epoch 446, Loss: 0.19838866591453552, Final Batch Loss: 0.09600570797920227\n",
      "Epoch 447, Loss: 0.3438552990555763, Final Batch Loss: 0.2316638082265854\n",
      "Epoch 448, Loss: 0.3112429827451706, Final Batch Loss: 0.2047124207019806\n",
      "Epoch 449, Loss: 0.22925952076911926, Final Batch Loss: 0.06746280193328857\n",
      "Epoch 450, Loss: 0.2824297994375229, Final Batch Loss: 0.1519761085510254\n",
      "Epoch 451, Loss: 0.26523977518081665, Final Batch Loss: 0.1310148984193802\n",
      "Epoch 452, Loss: 0.22507759183645248, Final Batch Loss: 0.12093053758144379\n",
      "Epoch 453, Loss: 0.2802026867866516, Final Batch Loss: 0.11911274492740631\n",
      "Epoch 454, Loss: 0.3306269198656082, Final Batch Loss: 0.1747177243232727\n",
      "Epoch 455, Loss: 0.33667321503162384, Final Batch Loss: 0.21577633917331696\n",
      "Epoch 456, Loss: 0.27329041063785553, Final Batch Loss: 0.1584697663784027\n",
      "Epoch 457, Loss: 0.22720694541931152, Final Batch Loss: 0.0939255803823471\n",
      "Epoch 458, Loss: 0.19970020651817322, Final Batch Loss: 0.05973294377326965\n",
      "Epoch 459, Loss: 0.2718818038702011, Final Batch Loss: 0.15127389132976532\n",
      "Epoch 460, Loss: 0.25263069570064545, Final Batch Loss: 0.12653027474880219\n",
      "Epoch 461, Loss: 0.29994091391563416, Final Batch Loss: 0.14126168191432953\n",
      "Epoch 462, Loss: 0.29330025613307953, Final Batch Loss: 0.16131015121936798\n",
      "Epoch 463, Loss: 0.2879929840564728, Final Batch Loss: 0.12763559818267822\n",
      "Epoch 464, Loss: 0.2532399445772171, Final Batch Loss: 0.1366412490606308\n",
      "Epoch 465, Loss: 0.2539338022470474, Final Batch Loss: 0.12598665058612823\n",
      "Epoch 466, Loss: 0.28439557552337646, Final Batch Loss: 0.14688165485858917\n",
      "Epoch 467, Loss: 0.24848245829343796, Final Batch Loss: 0.14426299929618835\n",
      "Epoch 468, Loss: 0.2862972021102905, Final Batch Loss: 0.162928968667984\n",
      "Epoch 469, Loss: 0.3184362053871155, Final Batch Loss: 0.14568504691123962\n",
      "Epoch 470, Loss: 0.23504458367824554, Final Batch Loss: 0.09364446997642517\n",
      "Epoch 471, Loss: 0.2748521640896797, Final Batch Loss: 0.16530270874500275\n",
      "Epoch 472, Loss: 0.299224317073822, Final Batch Loss: 0.1603369414806366\n",
      "Epoch 473, Loss: 0.3494786024093628, Final Batch Loss: 0.19058778882026672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474, Loss: 0.26550111919641495, Final Batch Loss: 0.11218336969614029\n",
      "Epoch 475, Loss: 0.20687826722860336, Final Batch Loss: 0.07662975043058395\n",
      "Epoch 476, Loss: 0.24028626084327698, Final Batch Loss: 0.12452979385852814\n",
      "Epoch 477, Loss: 0.29573675990104675, Final Batch Loss: 0.15177646279335022\n",
      "Epoch 478, Loss: 0.2763407379388809, Final Batch Loss: 0.14095255732536316\n",
      "Epoch 479, Loss: 0.2456832081079483, Final Batch Loss: 0.14985893666744232\n",
      "Epoch 480, Loss: 0.23323450982570648, Final Batch Loss: 0.09374047815799713\n",
      "Epoch 481, Loss: 0.2928350493311882, Final Batch Loss: 0.17062677443027496\n",
      "Epoch 482, Loss: 0.21842925250530243, Final Batch Loss: 0.10093077272176743\n",
      "Epoch 483, Loss: 0.21830424666404724, Final Batch Loss: 0.09108956158161163\n",
      "Epoch 484, Loss: 0.27171407639980316, Final Batch Loss: 0.12641370296478271\n",
      "Epoch 485, Loss: 0.34399107843637466, Final Batch Loss: 0.2232641577720642\n",
      "Epoch 486, Loss: 0.2248760685324669, Final Batch Loss: 0.11347854137420654\n",
      "Epoch 487, Loss: 0.2377454712986946, Final Batch Loss: 0.11788103729486465\n",
      "Epoch 488, Loss: 0.22942032665014267, Final Batch Loss: 0.1338825821876526\n",
      "Epoch 489, Loss: 0.258176326751709, Final Batch Loss: 0.12630030512809753\n",
      "Epoch 490, Loss: 0.23948195576667786, Final Batch Loss: 0.0968228280544281\n",
      "Epoch 491, Loss: 0.22581737488508224, Final Batch Loss: 0.08809065073728561\n",
      "Epoch 492, Loss: 0.24964334070682526, Final Batch Loss: 0.15207211673259735\n",
      "Epoch 493, Loss: 0.2411198616027832, Final Batch Loss: 0.13212910294532776\n",
      "Epoch 494, Loss: 0.2131594568490982, Final Batch Loss: 0.10995957255363464\n",
      "Epoch 495, Loss: 0.25080984830856323, Final Batch Loss: 0.14674675464630127\n",
      "Epoch 496, Loss: 0.2984918802976608, Final Batch Loss: 0.1694599986076355\n",
      "Epoch 497, Loss: 0.3626185655593872, Final Batch Loss: 0.1838531643152237\n",
      "Epoch 498, Loss: 0.2644112780690193, Final Batch Loss: 0.10243844240903854\n",
      "Epoch 499, Loss: 0.23811747133731842, Final Batch Loss: 0.10797083377838135\n",
      "Epoch 500, Loss: 0.28712988644838333, Final Batch Loss: 0.1673818677663803\n",
      "Epoch 501, Loss: 0.25607360899448395, Final Batch Loss: 0.10711534321308136\n",
      "Epoch 502, Loss: 0.24113298952579498, Final Batch Loss: 0.1453196406364441\n",
      "Epoch 503, Loss: 0.2902720123529434, Final Batch Loss: 0.12941108644008636\n",
      "Epoch 504, Loss: 0.2988565117120743, Final Batch Loss: 0.16943024098873138\n",
      "Epoch 505, Loss: 0.29048217833042145, Final Batch Loss: 0.14651857316493988\n",
      "Epoch 506, Loss: 0.24178878217935562, Final Batch Loss: 0.1245041936635971\n",
      "Epoch 507, Loss: 0.22462229430675507, Final Batch Loss: 0.12107643485069275\n",
      "Epoch 508, Loss: 0.2398400753736496, Final Batch Loss: 0.08478230237960815\n",
      "Epoch 509, Loss: 0.22520232200622559, Final Batch Loss: 0.12574340403079987\n",
      "Epoch 510, Loss: 0.26708540320396423, Final Batch Loss: 0.13185392320156097\n",
      "Epoch 511, Loss: 0.264359250664711, Final Batch Loss: 0.14515213668346405\n",
      "Epoch 512, Loss: 0.24461181461811066, Final Batch Loss: 0.1279190629720688\n",
      "Epoch 513, Loss: 0.23774657398462296, Final Batch Loss: 0.10301931947469711\n",
      "Epoch 514, Loss: 0.25104302167892456, Final Batch Loss: 0.09334790706634521\n",
      "Epoch 515, Loss: 0.2597590684890747, Final Batch Loss: 0.11431825160980225\n",
      "Epoch 516, Loss: 0.2759975343942642, Final Batch Loss: 0.16680404543876648\n",
      "Epoch 517, Loss: 0.25216276943683624, Final Batch Loss: 0.1486242264509201\n",
      "Epoch 518, Loss: 0.2491995468735695, Final Batch Loss: 0.1299588829278946\n",
      "Epoch 519, Loss: 0.23596441745758057, Final Batch Loss: 0.11506327986717224\n",
      "Epoch 520, Loss: 0.25001372396945953, Final Batch Loss: 0.14371441304683685\n",
      "Epoch 521, Loss: 0.3356742262840271, Final Batch Loss: 0.19790689647197723\n",
      "Epoch 522, Loss: 0.21699267625808716, Final Batch Loss: 0.11362482607364655\n",
      "Epoch 523, Loss: 0.2713293135166168, Final Batch Loss: 0.14892973005771637\n",
      "Epoch 524, Loss: 0.20304442197084427, Final Batch Loss: 0.1262589991092682\n",
      "Epoch 525, Loss: 0.1878163367509842, Final Batch Loss: 0.07336600124835968\n",
      "Epoch 526, Loss: 0.3104218542575836, Final Batch Loss: 0.18322674930095673\n",
      "Epoch 527, Loss: 0.28635528683662415, Final Batch Loss: 0.14396187663078308\n",
      "Epoch 528, Loss: 0.2504762038588524, Final Batch Loss: 0.11217775195837021\n",
      "Epoch 529, Loss: 0.2576175630092621, Final Batch Loss: 0.10559967160224915\n",
      "Epoch 530, Loss: 0.2665262669324875, Final Batch Loss: 0.14761579036712646\n",
      "Epoch 531, Loss: 0.19583817571401596, Final Batch Loss: 0.08348899334669113\n",
      "Epoch 532, Loss: 0.2461673766374588, Final Batch Loss: 0.15362051129341125\n",
      "Epoch 533, Loss: 0.26153427362442017, Final Batch Loss: 0.13119442760944366\n",
      "Epoch 534, Loss: 0.23925946652889252, Final Batch Loss: 0.11450688540935516\n",
      "Epoch 535, Loss: 0.2168992981314659, Final Batch Loss: 0.09899923950433731\n",
      "Epoch 536, Loss: 0.24480780959129333, Final Batch Loss: 0.1287793070077896\n",
      "Epoch 537, Loss: 0.24091758579015732, Final Batch Loss: 0.1174345538020134\n",
      "Epoch 538, Loss: 0.28005364537239075, Final Batch Loss: 0.1604042798280716\n",
      "Epoch 539, Loss: 0.1924665868282318, Final Batch Loss: 0.07490836828947067\n",
      "Epoch 540, Loss: 0.2163049578666687, Final Batch Loss: 0.12047122418880463\n",
      "Epoch 541, Loss: 0.23224132508039474, Final Batch Loss: 0.12510229647159576\n",
      "Epoch 542, Loss: 0.21445173025131226, Final Batch Loss: 0.10677661001682281\n",
      "Epoch 543, Loss: 0.2883812338113785, Final Batch Loss: 0.16222816705703735\n",
      "Epoch 544, Loss: 0.23676209896802902, Final Batch Loss: 0.11789198219776154\n",
      "Epoch 545, Loss: 0.2885621711611748, Final Batch Loss: 0.18456129729747772\n",
      "Epoch 546, Loss: 0.24012602865695953, Final Batch Loss: 0.10370047390460968\n",
      "Epoch 547, Loss: 0.21366425603628159, Final Batch Loss: 0.11209765821695328\n",
      "Epoch 548, Loss: 0.20378749072551727, Final Batch Loss: 0.08165621757507324\n",
      "Epoch 549, Loss: 0.28402212262153625, Final Batch Loss: 0.16736014187335968\n",
      "Epoch 550, Loss: 0.25956957042217255, Final Batch Loss: 0.16149024665355682\n",
      "Epoch 551, Loss: 0.20174483954906464, Final Batch Loss: 0.07150663435459137\n",
      "Epoch 552, Loss: 0.2363004833459854, Final Batch Loss: 0.0850810557603836\n",
      "Epoch 553, Loss: 0.24974022805690765, Final Batch Loss: 0.16498436033725739\n",
      "Epoch 554, Loss: 0.27405182272195816, Final Batch Loss: 0.11824759095907211\n",
      "Epoch 555, Loss: 0.20898708701133728, Final Batch Loss: 0.0750017911195755\n",
      "Epoch 556, Loss: 0.2072562873363495, Final Batch Loss: 0.07178439199924469\n",
      "Epoch 557, Loss: 0.23629483580589294, Final Batch Loss: 0.12326985597610474\n",
      "Epoch 558, Loss: 0.27896328270435333, Final Batch Loss: 0.13979306817054749\n",
      "Epoch 559, Loss: 0.2121930718421936, Final Batch Loss: 0.11275163292884827\n",
      "Epoch 560, Loss: 0.21140725165605545, Final Batch Loss: 0.08108057826757431\n",
      "Epoch 561, Loss: 0.22861547023057938, Final Batch Loss: 0.09091327339410782\n",
      "Epoch 562, Loss: 0.27120262384414673, Final Batch Loss: 0.16634546220302582\n",
      "Epoch 563, Loss: 0.26946185529232025, Final Batch Loss: 0.11453717947006226\n",
      "Epoch 564, Loss: 0.22809797525405884, Final Batch Loss: 0.07963335514068604\n",
      "Epoch 565, Loss: 0.2633702754974365, Final Batch Loss: 0.1629980057477951\n",
      "Epoch 566, Loss: 0.21739080548286438, Final Batch Loss: 0.0896836519241333\n",
      "Epoch 567, Loss: 0.24836720526218414, Final Batch Loss: 0.14275091886520386\n",
      "Epoch 568, Loss: 0.23673152178525925, Final Batch Loss: 0.08169964700937271\n",
      "Epoch 569, Loss: 0.22021672874689102, Final Batch Loss: 0.12267489731311798\n",
      "Epoch 570, Loss: 0.22986385971307755, Final Batch Loss: 0.1315591186285019\n",
      "Epoch 571, Loss: 0.25025104731321335, Final Batch Loss: 0.12218723446130753\n",
      "Epoch 572, Loss: 0.1947534903883934, Final Batch Loss: 0.10545679926872253\n",
      "Epoch 573, Loss: 0.24977263063192368, Final Batch Loss: 0.13104358315467834\n",
      "Epoch 574, Loss: 0.24392087757587433, Final Batch Loss: 0.14047031104564667\n",
      "Epoch 575, Loss: 0.23728343099355698, Final Batch Loss: 0.13410641252994537\n",
      "Epoch 576, Loss: 0.19548021256923676, Final Batch Loss: 0.0730985701084137\n",
      "Epoch 577, Loss: 0.2872302979230881, Final Batch Loss: 0.15497645735740662\n",
      "Epoch 578, Loss: 0.2167719006538391, Final Batch Loss: 0.12856830656528473\n",
      "Epoch 579, Loss: 0.2473914623260498, Final Batch Loss: 0.12549257278442383\n",
      "Epoch 580, Loss: 0.23555320501327515, Final Batch Loss: 0.10966037213802338\n",
      "Epoch 581, Loss: 0.20455610752105713, Final Batch Loss: 0.08735278993844986\n",
      "Epoch 582, Loss: 0.24840451031923294, Final Batch Loss: 0.14253631234169006\n",
      "Epoch 583, Loss: 0.23319025337696075, Final Batch Loss: 0.10711152851581573\n",
      "Epoch 584, Loss: 0.24168305844068527, Final Batch Loss: 0.12951239943504333\n",
      "Epoch 585, Loss: 0.21355045586824417, Final Batch Loss: 0.1281634122133255\n",
      "Epoch 586, Loss: 0.21685046702623367, Final Batch Loss: 0.1137039065361023\n",
      "Epoch 587, Loss: 0.22992772608995438, Final Batch Loss: 0.1347516030073166\n",
      "Epoch 588, Loss: 0.2586086392402649, Final Batch Loss: 0.12980663776397705\n",
      "Epoch 589, Loss: 0.21838714182376862, Final Batch Loss: 0.10981346666812897\n",
      "Epoch 590, Loss: 0.27075599133968353, Final Batch Loss: 0.16814300417900085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591, Loss: 0.23255717009305954, Final Batch Loss: 0.12131541222333908\n",
      "Epoch 592, Loss: 0.25553426146507263, Final Batch Loss: 0.12758806347846985\n",
      "Epoch 593, Loss: 0.25044751167297363, Final Batch Loss: 0.15840600430965424\n",
      "Epoch 594, Loss: 0.18965575471520424, Final Batch Loss: 0.05430785194039345\n",
      "Epoch 595, Loss: 0.22160355746746063, Final Batch Loss: 0.11226360499858856\n",
      "Epoch 596, Loss: 0.24751675128936768, Final Batch Loss: 0.09798088669776917\n",
      "Epoch 597, Loss: 0.21939803659915924, Final Batch Loss: 0.12552447617053986\n",
      "Epoch 598, Loss: 0.2024768516421318, Final Batch Loss: 0.09251881390810013\n",
      "Epoch 599, Loss: 0.24795977771282196, Final Batch Loss: 0.09052757918834686\n",
      "Epoch 600, Loss: 0.2083085998892784, Final Batch Loss: 0.11162827908992767\n",
      "Epoch 601, Loss: 0.2289675883948803, Final Batch Loss: 0.0514957420527935\n",
      "Epoch 602, Loss: 0.22548004984855652, Final Batch Loss: 0.1047089621424675\n",
      "Epoch 603, Loss: 0.24453027546405792, Final Batch Loss: 0.11912989616394043\n",
      "Epoch 604, Loss: 0.24973291158676147, Final Batch Loss: 0.1346055567264557\n",
      "Epoch 605, Loss: 0.21479491144418716, Final Batch Loss: 0.08138933032751083\n",
      "Epoch 606, Loss: 0.2196992114186287, Final Batch Loss: 0.1204121932387352\n",
      "Epoch 607, Loss: 0.2595558166503906, Final Batch Loss: 0.1507672220468521\n",
      "Epoch 608, Loss: 0.23689569532871246, Final Batch Loss: 0.16318319737911224\n",
      "Epoch 609, Loss: 0.21542555838823318, Final Batch Loss: 0.13378536701202393\n",
      "Epoch 610, Loss: 0.22518332302570343, Final Batch Loss: 0.0955827534198761\n",
      "Epoch 611, Loss: 0.22262857854366302, Final Batch Loss: 0.09821614623069763\n",
      "Epoch 612, Loss: 0.22983920574188232, Final Batch Loss: 0.07505720853805542\n",
      "Epoch 613, Loss: 0.27021050453186035, Final Batch Loss: 0.17464815080165863\n",
      "Epoch 614, Loss: 0.2886725664138794, Final Batch Loss: 0.19392608106136322\n",
      "Epoch 615, Loss: 0.20008571445941925, Final Batch Loss: 0.09504836052656174\n",
      "Epoch 616, Loss: 0.23858985304832458, Final Batch Loss: 0.1329188048839569\n",
      "Epoch 617, Loss: 0.21458804607391357, Final Batch Loss: 0.11677724868059158\n",
      "Epoch 618, Loss: 0.23200979828834534, Final Batch Loss: 0.08887043595314026\n",
      "Epoch 619, Loss: 0.2542237117886543, Final Batch Loss: 0.12062834948301315\n",
      "Epoch 620, Loss: 0.19880889356136322, Final Batch Loss: 0.1096164807677269\n",
      "Epoch 621, Loss: 0.23174723982810974, Final Batch Loss: 0.11420676112174988\n",
      "Epoch 622, Loss: 0.2384994998574257, Final Batch Loss: 0.11411833018064499\n",
      "Epoch 623, Loss: 0.21481192857027054, Final Batch Loss: 0.12463731318712234\n",
      "Epoch 624, Loss: 0.23873984813690186, Final Batch Loss: 0.11749808490276337\n",
      "Epoch 625, Loss: 0.19123855233192444, Final Batch Loss: 0.10406360775232315\n",
      "Epoch 626, Loss: 0.14723332598805428, Final Batch Loss: 0.0459824837744236\n",
      "Epoch 627, Loss: 0.20034746825695038, Final Batch Loss: 0.10721654444932938\n",
      "Epoch 628, Loss: 0.21142173558473587, Final Batch Loss: 0.10323011130094528\n",
      "Epoch 629, Loss: 0.21076753735542297, Final Batch Loss: 0.11334087699651718\n",
      "Epoch 630, Loss: 0.20685048401355743, Final Batch Loss: 0.10708607733249664\n",
      "Epoch 631, Loss: 0.1944514811038971, Final Batch Loss: 0.11012529581785202\n",
      "Epoch 632, Loss: 0.22398237884044647, Final Batch Loss: 0.09097418189048767\n",
      "Epoch 633, Loss: 0.18373043090105057, Final Batch Loss: 0.09501060843467712\n",
      "Epoch 634, Loss: 0.1714504510164261, Final Batch Loss: 0.06755124032497406\n",
      "Epoch 635, Loss: 0.14448878541588783, Final Batch Loss: 0.05613728240132332\n",
      "Epoch 636, Loss: 0.19124441593885422, Final Batch Loss: 0.08318789303302765\n",
      "Epoch 637, Loss: 0.19918058067560196, Final Batch Loss: 0.09949874132871628\n",
      "Epoch 638, Loss: 0.2009165808558464, Final Batch Loss: 0.10008088499307632\n",
      "Epoch 639, Loss: 0.20562264323234558, Final Batch Loss: 0.12873844802379608\n",
      "Epoch 640, Loss: 0.20307644456624985, Final Batch Loss: 0.1289748102426529\n",
      "Epoch 641, Loss: 0.1902746558189392, Final Batch Loss: 0.0653718113899231\n",
      "Epoch 642, Loss: 0.22160308808088303, Final Batch Loss: 0.10816860944032669\n",
      "Epoch 643, Loss: 0.17301424592733383, Final Batch Loss: 0.09379693120718002\n",
      "Epoch 644, Loss: 0.1441010981798172, Final Batch Loss: 0.07074173539876938\n",
      "Epoch 645, Loss: 0.16558930277824402, Final Batch Loss: 0.08165731281042099\n",
      "Epoch 646, Loss: 0.18365208059549332, Final Batch Loss: 0.09773916006088257\n",
      "Epoch 647, Loss: 0.19128624349832535, Final Batch Loss: 0.08484995365142822\n",
      "Epoch 648, Loss: 0.150561161339283, Final Batch Loss: 0.06992620974779129\n",
      "Epoch 649, Loss: 0.22027964144945145, Final Batch Loss: 0.13084347546100616\n",
      "Epoch 650, Loss: 0.166312076151371, Final Batch Loss: 0.07767773419618607\n",
      "Epoch 651, Loss: 0.19052419811487198, Final Batch Loss: 0.08579966425895691\n",
      "Epoch 652, Loss: 0.20832818001508713, Final Batch Loss: 0.12495332956314087\n",
      "Epoch 653, Loss: 0.1739298179745674, Final Batch Loss: 0.10126511752605438\n",
      "Epoch 654, Loss: 0.1912858709692955, Final Batch Loss: 0.09520057588815689\n",
      "Epoch 655, Loss: 0.1758854165673256, Final Batch Loss: 0.08797769993543625\n",
      "Epoch 656, Loss: 0.14928506314754486, Final Batch Loss: 0.056547604501247406\n",
      "Epoch 657, Loss: 0.2153581604361534, Final Batch Loss: 0.11553744971752167\n",
      "Epoch 658, Loss: 0.16896074265241623, Final Batch Loss: 0.09591354429721832\n",
      "Epoch 659, Loss: 0.18444816023111343, Final Batch Loss: 0.08492777496576309\n",
      "Epoch 660, Loss: 0.158377543091774, Final Batch Loss: 0.08408747613430023\n",
      "Epoch 661, Loss: 0.2180330902338028, Final Batch Loss: 0.1389392614364624\n",
      "Epoch 662, Loss: 0.23666474223136902, Final Batch Loss: 0.09239757061004639\n",
      "Epoch 663, Loss: 0.164155974984169, Final Batch Loss: 0.07343540340662003\n",
      "Epoch 664, Loss: 0.22712790966033936, Final Batch Loss: 0.11682995408773422\n",
      "Epoch 665, Loss: 0.22191006690263748, Final Batch Loss: 0.11475620418787003\n",
      "Epoch 666, Loss: 0.15298546105623245, Final Batch Loss: 0.08386509865522385\n",
      "Epoch 667, Loss: 0.1539686881005764, Final Batch Loss: 0.055635470896959305\n",
      "Epoch 668, Loss: 0.22200575470924377, Final Batch Loss: 0.1018507182598114\n",
      "Epoch 669, Loss: 0.21641792356967926, Final Batch Loss: 0.13296017050743103\n",
      "Epoch 670, Loss: 0.1608828380703926, Final Batch Loss: 0.08330727368593216\n",
      "Epoch 671, Loss: 0.1735999435186386, Final Batch Loss: 0.09954386204481125\n",
      "Epoch 672, Loss: 0.19537260383367538, Final Batch Loss: 0.11434008926153183\n",
      "Epoch 673, Loss: 0.15839546918869019, Final Batch Loss: 0.07207736372947693\n",
      "Epoch 674, Loss: 0.1743720918893814, Final Batch Loss: 0.11092153191566467\n",
      "Epoch 675, Loss: 0.18129032850265503, Final Batch Loss: 0.09282275289297104\n",
      "Epoch 676, Loss: 0.21728114783763885, Final Batch Loss: 0.1267731934785843\n",
      "Epoch 677, Loss: 0.15892814099788666, Final Batch Loss: 0.06481873989105225\n",
      "Epoch 678, Loss: 0.15058492496609688, Final Batch Loss: 0.05544683709740639\n",
      "Epoch 679, Loss: 0.16422483325004578, Final Batch Loss: 0.04436203092336655\n",
      "Epoch 680, Loss: 0.1760087013244629, Final Batch Loss: 0.08631877601146698\n",
      "Epoch 681, Loss: 0.1427059881389141, Final Batch Loss: 0.04865826293826103\n",
      "Epoch 682, Loss: 0.17937355488538742, Final Batch Loss: 0.08383454382419586\n",
      "Epoch 683, Loss: 0.1729554682970047, Final Batch Loss: 0.12013489007949829\n",
      "Epoch 684, Loss: 0.1835261955857277, Final Batch Loss: 0.08553539961576462\n",
      "Epoch 685, Loss: 0.15805558860301971, Final Batch Loss: 0.0669158399105072\n",
      "Epoch 686, Loss: 0.20573345571756363, Final Batch Loss: 0.08432783931493759\n",
      "Epoch 687, Loss: 0.22786789387464523, Final Batch Loss: 0.12111590802669525\n",
      "Epoch 688, Loss: 0.18607860803604126, Final Batch Loss: 0.0845360979437828\n",
      "Epoch 689, Loss: 0.21683920919895172, Final Batch Loss: 0.08840590715408325\n",
      "Epoch 690, Loss: 0.18428945541381836, Final Batch Loss: 0.07269767671823502\n",
      "Epoch 691, Loss: 0.1856774240732193, Final Batch Loss: 0.10031741857528687\n",
      "Epoch 692, Loss: 0.2211482748389244, Final Batch Loss: 0.12691627442836761\n",
      "Epoch 693, Loss: 0.20121462643146515, Final Batch Loss: 0.12587228417396545\n",
      "Epoch 694, Loss: 0.1826229840517044, Final Batch Loss: 0.08897665143013\n",
      "Epoch 695, Loss: 0.16988731920719147, Final Batch Loss: 0.06357934325933456\n",
      "Epoch 696, Loss: 0.20032750070095062, Final Batch Loss: 0.12099888175725937\n",
      "Epoch 697, Loss: 0.20723667740821838, Final Batch Loss: 0.13894103467464447\n",
      "Epoch 698, Loss: 0.17222318053245544, Final Batch Loss: 0.06703303754329681\n",
      "Epoch 699, Loss: 0.15658824890851974, Final Batch Loss: 0.0930984690785408\n",
      "Epoch 700, Loss: 0.1792226806282997, Final Batch Loss: 0.11227022856473923\n",
      "Epoch 701, Loss: 0.13643670082092285, Final Batch Loss: 0.0833774283528328\n",
      "Epoch 702, Loss: 0.16453150659799576, Final Batch Loss: 0.08069361746311188\n",
      "Epoch 703, Loss: 0.12787781655788422, Final Batch Loss: 0.03938402980566025\n",
      "Epoch 704, Loss: 0.15229209512472153, Final Batch Loss: 0.07835496962070465\n",
      "Epoch 705, Loss: 0.16036075353622437, Final Batch Loss: 0.095299631357193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706, Loss: 0.2185848131775856, Final Batch Loss: 0.11021564155817032\n",
      "Epoch 707, Loss: 0.18488820642232895, Final Batch Loss: 0.11320354044437408\n",
      "Epoch 708, Loss: 0.17029016464948654, Final Batch Loss: 0.10428224503993988\n",
      "Epoch 709, Loss: 0.13015178218483925, Final Batch Loss: 0.04777618125081062\n",
      "Epoch 710, Loss: 0.13483044132590294, Final Batch Loss: 0.04606816545128822\n",
      "Epoch 711, Loss: 0.16224296391010284, Final Batch Loss: 0.07427509129047394\n",
      "Epoch 712, Loss: 0.24239607900381088, Final Batch Loss: 0.14806850254535675\n",
      "Epoch 713, Loss: 0.16721979528665543, Final Batch Loss: 0.049979597330093384\n",
      "Epoch 714, Loss: 0.1667218580842018, Final Batch Loss: 0.0881500393152237\n",
      "Epoch 715, Loss: 0.14617808908224106, Final Batch Loss: 0.06618043780326843\n",
      "Epoch 716, Loss: 0.15878215432167053, Final Batch Loss: 0.06564939767122269\n",
      "Epoch 717, Loss: 0.22525008022785187, Final Batch Loss: 0.1281197965145111\n",
      "Epoch 718, Loss: 0.1481364592909813, Final Batch Loss: 0.0647106021642685\n",
      "Epoch 719, Loss: 0.23526624590158463, Final Batch Loss: 0.13459552824497223\n",
      "Epoch 720, Loss: 0.19187890738248825, Final Batch Loss: 0.09344764798879623\n",
      "Epoch 721, Loss: 0.19173581898212433, Final Batch Loss: 0.1017070785164833\n",
      "Epoch 722, Loss: 0.17125550657510757, Final Batch Loss: 0.07230260968208313\n",
      "Epoch 723, Loss: 0.18668555468320847, Final Batch Loss: 0.07470415532588959\n",
      "Epoch 724, Loss: 0.14962510764598846, Final Batch Loss: 0.08760946244001389\n",
      "Epoch 725, Loss: 0.2670168876647949, Final Batch Loss: 0.13431553542613983\n",
      "Epoch 726, Loss: 0.1514534056186676, Final Batch Loss: 0.07457421720027924\n",
      "Epoch 727, Loss: 0.1412607841193676, Final Batch Loss: 0.08899526298046112\n",
      "Epoch 728, Loss: 0.1751314401626587, Final Batch Loss: 0.09199793636798859\n",
      "Epoch 729, Loss: 0.16017675399780273, Final Batch Loss: 0.06521572917699814\n",
      "Epoch 730, Loss: 0.1565428525209427, Final Batch Loss: 0.07767106592655182\n",
      "Epoch 731, Loss: 0.15540927276015282, Final Batch Loss: 0.046905267983675\n",
      "Epoch 732, Loss: 0.18707619979977608, Final Batch Loss: 0.056627992540597916\n",
      "Epoch 733, Loss: 0.158503957092762, Final Batch Loss: 0.05449535697698593\n",
      "Epoch 734, Loss: 0.1885620504617691, Final Batch Loss: 0.08103594183921814\n",
      "Epoch 735, Loss: 0.1617349237203598, Final Batch Loss: 0.0823373794555664\n",
      "Epoch 736, Loss: 0.17035119980573654, Final Batch Loss: 0.10490477085113525\n",
      "Epoch 737, Loss: 0.2023817077279091, Final Batch Loss: 0.10775058716535568\n",
      "Epoch 738, Loss: 0.18251457065343857, Final Batch Loss: 0.10670403391122818\n",
      "Epoch 739, Loss: 0.17018556594848633, Final Batch Loss: 0.07307615131139755\n",
      "Epoch 740, Loss: 0.1845695823431015, Final Batch Loss: 0.11032509803771973\n",
      "Epoch 741, Loss: 0.1652480512857437, Final Batch Loss: 0.08035815507173538\n",
      "Epoch 742, Loss: 0.17462848871946335, Final Batch Loss: 0.09956739842891693\n",
      "Epoch 743, Loss: 0.21520330011844635, Final Batch Loss: 0.12899155914783478\n",
      "Epoch 744, Loss: 0.13289490342140198, Final Batch Loss: 0.06614718586206436\n",
      "Epoch 745, Loss: 0.19048168882727623, Final Batch Loss: 0.1393970400094986\n",
      "Epoch 746, Loss: 0.21625206619501114, Final Batch Loss: 0.12520380318164825\n",
      "Epoch 747, Loss: 0.20017290115356445, Final Batch Loss: 0.10818871855735779\n",
      "Epoch 748, Loss: 0.27847860753536224, Final Batch Loss: 0.1258583962917328\n",
      "Epoch 749, Loss: 0.1831602118909359, Final Batch Loss: 0.061584483832120895\n",
      "Epoch 750, Loss: 0.12367220222949982, Final Batch Loss: 0.06204778701066971\n",
      "Epoch 751, Loss: 0.20164316147565842, Final Batch Loss: 0.10338173061609268\n",
      "Epoch 752, Loss: 0.14807700365781784, Final Batch Loss: 0.08030982315540314\n",
      "Epoch 753, Loss: 0.19628623872995377, Final Batch Loss: 0.12807326018810272\n",
      "Epoch 754, Loss: 0.17001395672559738, Final Batch Loss: 0.08069109171628952\n",
      "Epoch 755, Loss: 0.18332404643297195, Final Batch Loss: 0.11307980120182037\n",
      "Epoch 756, Loss: 0.14130999147891998, Final Batch Loss: 0.056403495371341705\n",
      "Epoch 757, Loss: 0.22166023403406143, Final Batch Loss: 0.16643282771110535\n",
      "Epoch 758, Loss: 0.12678775191307068, Final Batch Loss: 0.07315284013748169\n",
      "Epoch 759, Loss: 0.1889803633093834, Final Batch Loss: 0.12264066934585571\n",
      "Epoch 760, Loss: 0.1206485815346241, Final Batch Loss: 0.03805513307452202\n",
      "Epoch 761, Loss: 0.1521267220377922, Final Batch Loss: 0.08100254088640213\n",
      "Epoch 762, Loss: 0.1897677406668663, Final Batch Loss: 0.11630447208881378\n",
      "Epoch 763, Loss: 0.15576791018247604, Final Batch Loss: 0.06987269222736359\n",
      "Epoch 764, Loss: 0.15395543724298477, Final Batch Loss: 0.039895497262477875\n",
      "Epoch 765, Loss: 0.1325468048453331, Final Batch Loss: 0.036435566842556\n",
      "Epoch 766, Loss: 0.1454145312309265, Final Batch Loss: 0.08152380585670471\n",
      "Epoch 767, Loss: 0.13988300040364265, Final Batch Loss: 0.038002859801054\n",
      "Epoch 768, Loss: 0.18191299587488174, Final Batch Loss: 0.1011231392621994\n",
      "Epoch 769, Loss: 0.15090658515691757, Final Batch Loss: 0.06885504722595215\n",
      "Epoch 770, Loss: 0.1334577351808548, Final Batch Loss: 0.0699128806591034\n",
      "Epoch 771, Loss: 0.16077078878879547, Final Batch Loss: 0.08836482465267181\n",
      "Epoch 772, Loss: 0.1346321478486061, Final Batch Loss: 0.05699072778224945\n",
      "Epoch 773, Loss: 0.15352091193199158, Final Batch Loss: 0.06681334227323532\n",
      "Epoch 774, Loss: 0.11426433175802231, Final Batch Loss: 0.0456337109208107\n",
      "Epoch 775, Loss: 0.18459246307611465, Final Batch Loss: 0.11148574948310852\n",
      "Epoch 776, Loss: 0.17066925764083862, Final Batch Loss: 0.09453469514846802\n",
      "Epoch 777, Loss: 0.14192603155970573, Final Batch Loss: 0.046786922961473465\n",
      "Epoch 778, Loss: 0.12121808901429176, Final Batch Loss: 0.053511109203100204\n",
      "Epoch 779, Loss: 0.14301741868257523, Final Batch Loss: 0.06321447342634201\n",
      "Epoch 780, Loss: 0.16918286681175232, Final Batch Loss: 0.10968837141990662\n",
      "Epoch 781, Loss: 0.11912703886628151, Final Batch Loss: 0.03359731659293175\n",
      "Epoch 782, Loss: 0.1424204334616661, Final Batch Loss: 0.07403716444969177\n",
      "Epoch 783, Loss: 0.20869719237089157, Final Batch Loss: 0.13861261308193207\n",
      "Epoch 784, Loss: 0.23990197479724884, Final Batch Loss: 0.16752062737941742\n",
      "Epoch 785, Loss: 0.17817208915948868, Final Batch Loss: 0.08327443152666092\n",
      "Epoch 786, Loss: 0.18146032094955444, Final Batch Loss: 0.09535229206085205\n",
      "Epoch 787, Loss: 0.14250577986240387, Final Batch Loss: 0.04197779297828674\n",
      "Epoch 788, Loss: 0.16171710193157196, Final Batch Loss: 0.09899533540010452\n",
      "Epoch 789, Loss: 0.18870119005441666, Final Batch Loss: 0.12207526713609695\n",
      "Epoch 790, Loss: 0.13935548812150955, Final Batch Loss: 0.06131424009799957\n",
      "Epoch 791, Loss: 0.2055668979883194, Final Batch Loss: 0.11448127031326294\n",
      "Epoch 792, Loss: 0.1469958983361721, Final Batch Loss: 0.061259713023900986\n",
      "Epoch 793, Loss: 0.13016992062330246, Final Batch Loss: 0.04858098179101944\n",
      "Epoch 794, Loss: 0.14345011115074158, Final Batch Loss: 0.0739930272102356\n",
      "Epoch 795, Loss: 0.10321906208992004, Final Batch Loss: 0.03712472319602966\n",
      "Epoch 796, Loss: 0.14376568049192429, Final Batch Loss: 0.06710319966077805\n",
      "Epoch 797, Loss: 0.18416985869407654, Final Batch Loss: 0.1214633584022522\n",
      "Epoch 798, Loss: 0.14822319895029068, Final Batch Loss: 0.08986540883779526\n",
      "Epoch 799, Loss: 0.12594007700681686, Final Batch Loss: 0.05147755891084671\n",
      "Epoch 800, Loss: 0.15314801782369614, Final Batch Loss: 0.06166163831949234\n",
      "Epoch 801, Loss: 0.11775883659720421, Final Batch Loss: 0.05633778125047684\n",
      "Epoch 802, Loss: 0.12440355122089386, Final Batch Loss: 0.06176675856113434\n",
      "Epoch 803, Loss: 0.12224342115223408, Final Batch Loss: 0.02632339484989643\n",
      "Epoch 804, Loss: 0.19458885490894318, Final Batch Loss: 0.1216135248541832\n",
      "Epoch 805, Loss: 0.1189328245818615, Final Batch Loss: 0.04501235857605934\n",
      "Epoch 806, Loss: 0.15521681308746338, Final Batch Loss: 0.10264114290475845\n",
      "Epoch 807, Loss: 0.10811902955174446, Final Batch Loss: 0.03578013554215431\n",
      "Epoch 808, Loss: 0.11186232790350914, Final Batch Loss: 0.04350823536515236\n",
      "Epoch 809, Loss: 0.16314877569675446, Final Batch Loss: 0.0835404098033905\n",
      "Epoch 810, Loss: 0.10219522193074226, Final Batch Loss: 0.04372752830386162\n",
      "Epoch 811, Loss: 0.15354033187031746, Final Batch Loss: 0.10656102001667023\n",
      "Epoch 812, Loss: 0.13083896413445473, Final Batch Loss: 0.07548800855875015\n",
      "Epoch 813, Loss: 0.15450186282396317, Final Batch Loss: 0.06321088969707489\n",
      "Epoch 814, Loss: 0.15401257574558258, Final Batch Loss: 0.08001361787319183\n",
      "Epoch 815, Loss: 0.10576007887721062, Final Batch Loss: 0.041496384888887405\n",
      "Epoch 816, Loss: 0.1576140858232975, Final Batch Loss: 0.04199005290865898\n",
      "Epoch 817, Loss: 0.1497327834367752, Final Batch Loss: 0.07552995532751083\n",
      "Epoch 818, Loss: 0.08774971030652523, Final Batch Loss: 0.022555289790034294\n",
      "Epoch 819, Loss: 0.1103556863963604, Final Batch Loss: 0.05605890229344368\n",
      "Epoch 820, Loss: 0.14363589882850647, Final Batch Loss: 0.08921242505311966\n",
      "Epoch 821, Loss: 0.1382228322327137, Final Batch Loss: 0.07628785073757172\n",
      "Epoch 822, Loss: 0.12060585245490074, Final Batch Loss: 0.08547073602676392\n",
      "Epoch 823, Loss: 0.15707773342728615, Final Batch Loss: 0.036684799939394\n",
      "Epoch 824, Loss: 0.19949457794427872, Final Batch Loss: 0.10993541032075882\n",
      "Epoch 825, Loss: 0.15717904269695282, Final Batch Loss: 0.09260737895965576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826, Loss: 0.1275472417473793, Final Batch Loss: 0.063339464366436\n",
      "Epoch 827, Loss: 0.11043020710349083, Final Batch Loss: 0.048352062702178955\n",
      "Epoch 828, Loss: 0.11789648234844208, Final Batch Loss: 0.06853475421667099\n",
      "Epoch 829, Loss: 0.12401791289448738, Final Batch Loss: 0.08112910389900208\n",
      "Epoch 830, Loss: 0.1526060625910759, Final Batch Loss: 0.08028611540794373\n",
      "Epoch 831, Loss: 0.17119847238063812, Final Batch Loss: 0.11937470734119415\n",
      "Epoch 832, Loss: 0.1403675302863121, Final Batch Loss: 0.06360404193401337\n",
      "Epoch 833, Loss: 0.08991629630327225, Final Batch Loss: 0.03605389967560768\n",
      "Epoch 834, Loss: 0.1380355879664421, Final Batch Loss: 0.06209392100572586\n",
      "Epoch 835, Loss: 0.12405746802687645, Final Batch Loss: 0.0737365335226059\n",
      "Epoch 836, Loss: 0.11900689825415611, Final Batch Loss: 0.0582745224237442\n",
      "Epoch 837, Loss: 0.12034399807453156, Final Batch Loss: 0.07239978015422821\n",
      "Epoch 838, Loss: 0.12120800837874413, Final Batch Loss: 0.05372954532504082\n",
      "Epoch 839, Loss: 0.13579054921865463, Final Batch Loss: 0.06795381009578705\n",
      "Epoch 840, Loss: 0.17390075884759426, Final Batch Loss: 0.1427997499704361\n",
      "Epoch 841, Loss: 0.15532875061035156, Final Batch Loss: 0.08234521001577377\n",
      "Epoch 842, Loss: 0.12352053821086884, Final Batch Loss: 0.04006611555814743\n",
      "Epoch 843, Loss: 0.1801055260002613, Final Batch Loss: 0.12530569732189178\n",
      "Epoch 844, Loss: 0.12458726018667221, Final Batch Loss: 0.04562218487262726\n",
      "Epoch 845, Loss: 0.12425834313035011, Final Batch Loss: 0.05589262768626213\n",
      "Epoch 846, Loss: 0.1979132518172264, Final Batch Loss: 0.12907510995864868\n",
      "Epoch 847, Loss: 0.12985334545373917, Final Batch Loss: 0.06903943419456482\n",
      "Epoch 848, Loss: 0.12319925054907799, Final Batch Loss: 0.05702092871069908\n",
      "Epoch 849, Loss: 0.16025559231638908, Final Batch Loss: 0.12998102605342865\n",
      "Epoch 850, Loss: 0.1360286995768547, Final Batch Loss: 0.06060565263032913\n",
      "Epoch 851, Loss: 0.09257087484002113, Final Batch Loss: 0.03267425298690796\n",
      "Epoch 852, Loss: 0.17123892903327942, Final Batch Loss: 0.09017254412174225\n",
      "Epoch 853, Loss: 0.14311878755688667, Final Batch Loss: 0.05112199857831001\n",
      "Epoch 854, Loss: 0.11010516434907913, Final Batch Loss: 0.05091281980276108\n",
      "Epoch 855, Loss: 0.19373252242803574, Final Batch Loss: 0.1269296407699585\n",
      "Epoch 856, Loss: 0.10741940513253212, Final Batch Loss: 0.03799927607178688\n",
      "Epoch 857, Loss: 0.13593050837516785, Final Batch Loss: 0.07714784145355225\n",
      "Epoch 858, Loss: 0.16440511867403984, Final Batch Loss: 0.10948492586612701\n",
      "Epoch 859, Loss: 0.1402827724814415, Final Batch Loss: 0.07284434139728546\n",
      "Epoch 860, Loss: 0.11557283625006676, Final Batch Loss: 0.039693403989076614\n",
      "Epoch 861, Loss: 0.18897319212555885, Final Batch Loss: 0.12806209921836853\n",
      "Epoch 862, Loss: 0.09665373340249062, Final Batch Loss: 0.032165754586458206\n",
      "Epoch 863, Loss: 0.11305195465683937, Final Batch Loss: 0.05319054797291756\n",
      "Epoch 864, Loss: 0.1832675039768219, Final Batch Loss: 0.12060379236936569\n",
      "Epoch 865, Loss: 0.14775599539279938, Final Batch Loss: 0.08185199648141861\n",
      "Epoch 866, Loss: 0.14658594131469727, Final Batch Loss: 0.06302329152822495\n",
      "Epoch 867, Loss: 0.12616433948278427, Final Batch Loss: 0.06454670429229736\n",
      "Epoch 868, Loss: 0.10908791050314903, Final Batch Loss: 0.05081041157245636\n",
      "Epoch 869, Loss: 0.11036417633295059, Final Batch Loss: 0.06689213216304779\n",
      "Epoch 870, Loss: 0.11430924758315086, Final Batch Loss: 0.05747867748141289\n",
      "Epoch 871, Loss: 0.14784781634807587, Final Batch Loss: 0.07079660147428513\n",
      "Epoch 872, Loss: 0.1118030995130539, Final Batch Loss: 0.03806619346141815\n",
      "Epoch 873, Loss: 0.13140546157956123, Final Batch Loss: 0.054239626973867416\n",
      "Epoch 874, Loss: 0.148194070905447, Final Batch Loss: 0.09567420929670334\n",
      "Epoch 875, Loss: 0.20943711698055267, Final Batch Loss: 0.1354944407939911\n",
      "Epoch 876, Loss: 0.10119317844510078, Final Batch Loss: 0.05379840359091759\n",
      "Epoch 877, Loss: 0.09259819984436035, Final Batch Loss: 0.01664777100086212\n",
      "Epoch 878, Loss: 0.1199810616672039, Final Batch Loss: 0.07293878495693207\n",
      "Epoch 879, Loss: 0.10266376286745071, Final Batch Loss: 0.04131872579455376\n",
      "Epoch 880, Loss: 0.11907344684004784, Final Batch Loss: 0.05769124627113342\n",
      "Epoch 881, Loss: 0.1446591019630432, Final Batch Loss: 0.06687687337398529\n",
      "Epoch 882, Loss: 0.17605987936258316, Final Batch Loss: 0.09834431111812592\n",
      "Epoch 883, Loss: 0.15605809167027473, Final Batch Loss: 0.04750969633460045\n",
      "Epoch 884, Loss: 0.11476633697748184, Final Batch Loss: 0.03423771262168884\n",
      "Epoch 885, Loss: 0.18510092049837112, Final Batch Loss: 0.13126152753829956\n",
      "Epoch 886, Loss: 0.17497717589139938, Final Batch Loss: 0.09982727468013763\n",
      "Epoch 887, Loss: 0.15341965854167938, Final Batch Loss: 0.0940239429473877\n",
      "Epoch 888, Loss: 0.20438643544912338, Final Batch Loss: 0.08522509038448334\n",
      "Epoch 889, Loss: 0.0998205617070198, Final Batch Loss: 0.04949403181672096\n",
      "Epoch 890, Loss: 0.09030977636575699, Final Batch Loss: 0.03830922022461891\n",
      "Epoch 891, Loss: 0.13143927603960037, Final Batch Loss: 0.06873352825641632\n",
      "Epoch 892, Loss: 0.1145884357392788, Final Batch Loss: 0.043574947863817215\n",
      "Epoch 893, Loss: 0.08559719659388065, Final Batch Loss: 0.029707377776503563\n",
      "Epoch 894, Loss: 0.15426427870988846, Final Batch Loss: 0.08439180254936218\n",
      "Epoch 895, Loss: 0.11233580112457275, Final Batch Loss: 0.05874919146299362\n",
      "Epoch 896, Loss: 0.08985522389411926, Final Batch Loss: 0.04701380431652069\n",
      "Epoch 897, Loss: 0.1363591030240059, Final Batch Loss: 0.0838729739189148\n",
      "Epoch 898, Loss: 0.11969103664159775, Final Batch Loss: 0.06082502752542496\n",
      "Epoch 899, Loss: 0.10125155746936798, Final Batch Loss: 0.0412508025765419\n",
      "Epoch 900, Loss: 0.13184558227658272, Final Batch Loss: 0.08328238874673843\n",
      "Epoch 901, Loss: 0.10216811671853065, Final Batch Loss: 0.05772180110216141\n",
      "Epoch 902, Loss: 0.10611990839242935, Final Batch Loss: 0.043665654957294464\n",
      "Epoch 903, Loss: 0.130135178565979, Final Batch Loss: 0.04579916596412659\n",
      "Epoch 904, Loss: 0.10489769652485847, Final Batch Loss: 0.06678717583417892\n",
      "Epoch 905, Loss: 0.10591145604848862, Final Batch Loss: 0.04206584393978119\n",
      "Epoch 906, Loss: 0.12763500213623047, Final Batch Loss: 0.060164108872413635\n",
      "Epoch 907, Loss: 0.0936359353363514, Final Batch Loss: 0.04019499197602272\n",
      "Epoch 908, Loss: 0.13262738287448883, Final Batch Loss: 0.051618002355098724\n",
      "Epoch 909, Loss: 0.10956860706210136, Final Batch Loss: 0.06373796612024307\n",
      "Epoch 910, Loss: 0.10324601083993912, Final Batch Loss: 0.03255742788314819\n",
      "Epoch 911, Loss: 0.10016682930290699, Final Batch Loss: 0.023846851661801338\n",
      "Epoch 912, Loss: 0.17944969981908798, Final Batch Loss: 0.10575377941131592\n",
      "Epoch 913, Loss: 0.09216052293777466, Final Batch Loss: 0.026783384382724762\n",
      "Epoch 914, Loss: 0.1003992035984993, Final Batch Loss: 0.051416896283626556\n",
      "Epoch 915, Loss: 0.08341247960925102, Final Batch Loss: 0.04323181137442589\n",
      "Epoch 916, Loss: 0.14958486706018448, Final Batch Loss: 0.09492294490337372\n",
      "Epoch 917, Loss: 0.1255016140639782, Final Batch Loss: 0.0676146000623703\n",
      "Epoch 918, Loss: 0.09867355599999428, Final Batch Loss: 0.05977976322174072\n",
      "Epoch 919, Loss: 0.24512809328734875, Final Batch Loss: 0.22412484884262085\n",
      "Epoch 920, Loss: 0.14780748635530472, Final Batch Loss: 0.0682750716805458\n",
      "Epoch 921, Loss: 0.1420070342719555, Final Batch Loss: 0.09015294164419174\n",
      "Epoch 922, Loss: 0.13781064376235008, Final Batch Loss: 0.052318718284368515\n",
      "Epoch 923, Loss: 0.12647829577326775, Final Batch Loss: 0.04399728402495384\n",
      "Epoch 924, Loss: 0.23031149059534073, Final Batch Loss: 0.1416730135679245\n",
      "Epoch 925, Loss: 0.23633936792612076, Final Batch Loss: 0.11393313109874725\n",
      "Epoch 926, Loss: 0.12503526359796524, Final Batch Loss: 0.05621645599603653\n",
      "Epoch 927, Loss: 0.201516255736351, Final Batch Loss: 0.12608839571475983\n",
      "Epoch 928, Loss: 0.21167026460170746, Final Batch Loss: 0.10431676357984543\n",
      "Epoch 929, Loss: 0.10320178419351578, Final Batch Loss: 0.05784538388252258\n",
      "Epoch 930, Loss: 0.14024287834763527, Final Batch Loss: 0.08313463628292084\n",
      "Epoch 931, Loss: 0.14529821649193764, Final Batch Loss: 0.06175317242741585\n",
      "Epoch 932, Loss: 0.15750783309340477, Final Batch Loss: 0.11224228143692017\n",
      "Epoch 933, Loss: 0.18286175280809402, Final Batch Loss: 0.07691838592290878\n",
      "Epoch 934, Loss: 0.19816183298826218, Final Batch Loss: 0.14055320620536804\n",
      "Epoch 935, Loss: 0.11896418780088425, Final Batch Loss: 0.030256234109401703\n",
      "Epoch 936, Loss: 0.1296028010547161, Final Batch Loss: 0.05108555778861046\n",
      "Epoch 937, Loss: 0.12597719579935074, Final Batch Loss: 0.0783037543296814\n",
      "Epoch 938, Loss: 0.15338128060102463, Final Batch Loss: 0.08354643732309341\n",
      "Epoch 939, Loss: 0.12194972857832909, Final Batch Loss: 0.06375700980424881\n",
      "Epoch 940, Loss: 0.16114256531000137, Final Batch Loss: 0.08001996576786041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 941, Loss: 0.1556723415851593, Final Batch Loss: 0.07526452094316483\n",
      "Epoch 942, Loss: 0.1621323674917221, Final Batch Loss: 0.08773412555456161\n",
      "Epoch 943, Loss: 0.11214075982570648, Final Batch Loss: 0.05750477313995361\n",
      "Epoch 944, Loss: 0.09945295751094818, Final Batch Loss: 0.0308341383934021\n",
      "Epoch 945, Loss: 0.14881175756454468, Final Batch Loss: 0.07203175872564316\n",
      "Epoch 946, Loss: 0.1488039866089821, Final Batch Loss: 0.0855490192770958\n",
      "Epoch 947, Loss: 0.13969585672020912, Final Batch Loss: 0.08644517511129379\n",
      "Epoch 948, Loss: 0.12489400058984756, Final Batch Loss: 0.05697569251060486\n",
      "Epoch 949, Loss: 0.12946609407663345, Final Batch Loss: 0.0671444684267044\n",
      "Epoch 950, Loss: 0.11705813184380531, Final Batch Loss: 0.05441092327237129\n",
      "Epoch 951, Loss: 0.08769287168979645, Final Batch Loss: 0.03734886646270752\n",
      "Epoch 952, Loss: 0.09291350282728672, Final Batch Loss: 0.027941377833485603\n",
      "Epoch 953, Loss: 0.1124650165438652, Final Batch Loss: 0.05321526154875755\n",
      "Epoch 954, Loss: 0.10616986826062202, Final Batch Loss: 0.05760326609015465\n",
      "Epoch 955, Loss: 0.13011695444583893, Final Batch Loss: 0.09126296639442444\n",
      "Epoch 956, Loss: 0.08436697907745838, Final Batch Loss: 0.02438250742852688\n",
      "Epoch 957, Loss: 0.07570688612759113, Final Batch Loss: 0.01473688893020153\n",
      "Epoch 958, Loss: 0.15515419468283653, Final Batch Loss: 0.11186958849430084\n",
      "Epoch 959, Loss: 0.09029363468289375, Final Batch Loss: 0.030008774250745773\n",
      "Epoch 960, Loss: 0.09313751757144928, Final Batch Loss: 0.06200527027249336\n",
      "Epoch 961, Loss: 0.11071950197219849, Final Batch Loss: 0.04495304822921753\n",
      "Epoch 962, Loss: 0.10616441443562508, Final Batch Loss: 0.06777936220169067\n",
      "Epoch 963, Loss: 0.1264120750129223, Final Batch Loss: 0.037314508110284805\n",
      "Epoch 964, Loss: 0.07828914560377598, Final Batch Loss: 0.02449594996869564\n",
      "Epoch 965, Loss: 0.10934785008430481, Final Batch Loss: 0.043215565383434296\n",
      "Epoch 966, Loss: 0.10430295020341873, Final Batch Loss: 0.06262455880641937\n",
      "Epoch 967, Loss: 0.18193461000919342, Final Batch Loss: 0.12279093265533447\n",
      "Epoch 968, Loss: 0.13170276209712029, Final Batch Loss: 0.07594174891710281\n",
      "Epoch 969, Loss: 0.12604034319519997, Final Batch Loss: 0.0658329650759697\n",
      "Epoch 970, Loss: 0.09197455272078514, Final Batch Loss: 0.03982844948768616\n",
      "Epoch 971, Loss: 0.15077094733715057, Final Batch Loss: 0.10066564381122589\n",
      "Epoch 972, Loss: 0.07136182300746441, Final Batch Loss: 0.025589948520064354\n",
      "Epoch 973, Loss: 0.07406296581029892, Final Batch Loss: 0.025690078735351562\n",
      "Epoch 974, Loss: 0.11368335038423538, Final Batch Loss: 0.04327422380447388\n",
      "Epoch 975, Loss: 0.14328249543905258, Final Batch Loss: 0.07169786095619202\n",
      "Epoch 976, Loss: 0.16178599745035172, Final Batch Loss: 0.100953109562397\n",
      "Epoch 977, Loss: 0.1426774524152279, Final Batch Loss: 0.08092878758907318\n",
      "Epoch 978, Loss: 0.10126239061355591, Final Batch Loss: 0.05439545214176178\n",
      "Epoch 979, Loss: 0.08919618651270866, Final Batch Loss: 0.06046358123421669\n",
      "Epoch 980, Loss: 0.16484839469194412, Final Batch Loss: 0.10761284083127975\n",
      "Epoch 981, Loss: 0.08639894612133503, Final Batch Loss: 0.027112720534205437\n",
      "Epoch 982, Loss: 0.09612831845879555, Final Batch Loss: 0.05645754560828209\n",
      "Epoch 983, Loss: 0.10342550650238991, Final Batch Loss: 0.045916054397821426\n",
      "Epoch 984, Loss: 0.11274846643209457, Final Batch Loss: 0.0702730119228363\n",
      "Epoch 985, Loss: 0.09537641704082489, Final Batch Loss: 0.03010568767786026\n",
      "Epoch 986, Loss: 0.12285979464650154, Final Batch Loss: 0.06996411830186844\n",
      "Epoch 987, Loss: 0.10979549959301949, Final Batch Loss: 0.035603757947683334\n",
      "Epoch 988, Loss: 0.13546722754836082, Final Batch Loss: 0.07976404577493668\n",
      "Epoch 989, Loss: 0.16305287927389145, Final Batch Loss: 0.07067111134529114\n",
      "Epoch 990, Loss: 0.0729875061661005, Final Batch Loss: 0.02480214647948742\n",
      "Epoch 991, Loss: 0.12750620767474174, Final Batch Loss: 0.0722997859120369\n",
      "Epoch 992, Loss: 0.15505892410874367, Final Batch Loss: 0.10037121176719666\n",
      "Epoch 993, Loss: 0.1717965193092823, Final Batch Loss: 0.13216806948184967\n",
      "Epoch 994, Loss: 0.0970524437725544, Final Batch Loss: 0.05817931145429611\n",
      "Epoch 995, Loss: 0.15655231475830078, Final Batch Loss: 0.10522982478141785\n",
      "Epoch 996, Loss: 0.1484546959400177, Final Batch Loss: 0.09305211156606674\n",
      "Epoch 997, Loss: 0.10345006734132767, Final Batch Loss: 0.024754315614700317\n",
      "Epoch 998, Loss: 0.13896894827485085, Final Batch Loss: 0.06070058420300484\n",
      "Epoch 999, Loss: 0.07797357067465782, Final Batch Loss: 0.03888425976037979\n",
      "Epoch 1000, Loss: 0.11534112691879272, Final Batch Loss: 0.04251433163881302\n",
      "Epoch 1001, Loss: 0.09434922598302364, Final Batch Loss: 0.025854727253317833\n",
      "Epoch 1002, Loss: 0.14040913805365562, Final Batch Loss: 0.10326766222715378\n",
      "Epoch 1003, Loss: 0.08215394802391529, Final Batch Loss: 0.051683682948350906\n",
      "Epoch 1004, Loss: 0.10515488684177399, Final Batch Loss: 0.04537241533398628\n",
      "Epoch 1005, Loss: 0.09555156528949738, Final Batch Loss: 0.04296039417386055\n",
      "Epoch 1006, Loss: 0.09962253272533417, Final Batch Loss: 0.019899457693099976\n",
      "Epoch 1007, Loss: 0.1209774948656559, Final Batch Loss: 0.08339861035346985\n",
      "Epoch 1008, Loss: 0.12475474551320076, Final Batch Loss: 0.06858867406845093\n",
      "Epoch 1009, Loss: 0.0940678808838129, Final Batch Loss: 0.023697586730122566\n",
      "Epoch 1010, Loss: 0.17486639320850372, Final Batch Loss: 0.06972847878932953\n",
      "Epoch 1011, Loss: 0.11030080169439316, Final Batch Loss: 0.03513222932815552\n",
      "Epoch 1012, Loss: 0.11898867040872574, Final Batch Loss: 0.05348633974790573\n",
      "Epoch 1013, Loss: 0.13609542697668076, Final Batch Loss: 0.047223448753356934\n",
      "Epoch 1014, Loss: 0.10195712372660637, Final Batch Loss: 0.04572566971182823\n",
      "Epoch 1015, Loss: 0.08789749816060066, Final Batch Loss: 0.05369950830936432\n",
      "Epoch 1016, Loss: 0.08965379744768143, Final Batch Loss: 0.010228969156742096\n",
      "Epoch 1017, Loss: 0.13839102536439896, Final Batch Loss: 0.07207636535167694\n",
      "Epoch 1018, Loss: 0.12810105457901955, Final Batch Loss: 0.06611256301403046\n",
      "Epoch 1019, Loss: 0.08624966442584991, Final Batch Loss: 0.0349256731569767\n",
      "Epoch 1020, Loss: 0.14753757417201996, Final Batch Loss: 0.07477191090583801\n",
      "Epoch 1021, Loss: 0.09680039808154106, Final Batch Loss: 0.037102557718753815\n",
      "Epoch 1022, Loss: 0.13356006145477295, Final Batch Loss: 0.08681494742631912\n",
      "Epoch 1023, Loss: 0.07979661226272583, Final Batch Loss: 0.03458220511674881\n",
      "Epoch 1024, Loss: 0.11507575586438179, Final Batch Loss: 0.07587582617998123\n",
      "Epoch 1025, Loss: 0.1298786960542202, Final Batch Loss: 0.07882967591285706\n",
      "Epoch 1026, Loss: 0.08858737163245678, Final Batch Loss: 0.02377384714782238\n",
      "Epoch 1027, Loss: 0.11794629693031311, Final Batch Loss: 0.08604855090379715\n",
      "Epoch 1028, Loss: 0.11066456511616707, Final Batch Loss: 0.07460975646972656\n",
      "Epoch 1029, Loss: 0.12531708925962448, Final Batch Loss: 0.08927413076162338\n",
      "Epoch 1030, Loss: 0.140765693038702, Final Batch Loss: 0.03743002936244011\n",
      "Epoch 1031, Loss: 0.14838533848524094, Final Batch Loss: 0.07926838099956512\n",
      "Epoch 1032, Loss: 0.12661855295300484, Final Batch Loss: 0.05734087899327278\n",
      "Epoch 1033, Loss: 0.1396162435412407, Final Batch Loss: 0.07114125788211823\n",
      "Epoch 1034, Loss: 0.15565932914614677, Final Batch Loss: 0.10545782744884491\n",
      "Epoch 1035, Loss: 0.15321621671319008, Final Batch Loss: 0.10489088296890259\n",
      "Epoch 1036, Loss: 0.11212669685482979, Final Batch Loss: 0.022425387054681778\n",
      "Epoch 1037, Loss: 0.1416149027645588, Final Batch Loss: 0.0619492344558239\n",
      "Epoch 1038, Loss: 0.14982179552316666, Final Batch Loss: 0.11459474265575409\n",
      "Epoch 1039, Loss: 0.12896602600812912, Final Batch Loss: 0.06502538919448853\n",
      "Epoch 1040, Loss: 0.07309072092175484, Final Batch Loss: 0.032278358936309814\n",
      "Epoch 1041, Loss: 0.10355035960674286, Final Batch Loss: 0.06721840798854828\n",
      "Epoch 1042, Loss: 0.10342463105916977, Final Batch Loss: 0.06581994146108627\n",
      "Epoch 1043, Loss: 0.12669460847973824, Final Batch Loss: 0.04971412941813469\n",
      "Epoch 1044, Loss: 0.08620996214449406, Final Batch Loss: 0.02928422950208187\n",
      "Epoch 1045, Loss: 0.098060617223382, Final Batch Loss: 0.0219190064817667\n",
      "Epoch 1046, Loss: 0.11707471311092377, Final Batch Loss: 0.07136458903551102\n",
      "Epoch 1047, Loss: 0.1387864090502262, Final Batch Loss: 0.09093963354825974\n",
      "Epoch 1048, Loss: 0.09231473505496979, Final Batch Loss: 0.03866379335522652\n",
      "Epoch 1049, Loss: 0.10996909812092781, Final Batch Loss: 0.07692620903253555\n",
      "Epoch 1050, Loss: 0.12436876818537712, Final Batch Loss: 0.05266456678509712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1051, Loss: 0.17012163251638412, Final Batch Loss: 0.09087897092103958\n",
      "Epoch 1052, Loss: 0.08109729364514351, Final Batch Loss: 0.03978263586759567\n",
      "Epoch 1053, Loss: 0.10556602105498314, Final Batch Loss: 0.050572436302900314\n",
      "Epoch 1054, Loss: 0.13235680386424065, Final Batch Loss: 0.07242351770401001\n",
      "Epoch 1055, Loss: 0.11836599186062813, Final Batch Loss: 0.05761033669114113\n",
      "Epoch 1056, Loss: 0.09671373292803764, Final Batch Loss: 0.04195684567093849\n",
      "Epoch 1057, Loss: 0.15386738255620003, Final Batch Loss: 0.11467043310403824\n",
      "Epoch 1058, Loss: 0.1158429216593504, Final Batch Loss: 0.022220438346266747\n",
      "Epoch 1059, Loss: 0.12890030443668365, Final Batch Loss: 0.09783139824867249\n",
      "Epoch 1060, Loss: 0.11133275553584099, Final Batch Loss: 0.07420734316110611\n",
      "Epoch 1061, Loss: 0.1002877689898014, Final Batch Loss: 0.04505434259772301\n",
      "Epoch 1062, Loss: 0.13009319454431534, Final Batch Loss: 0.08283933252096176\n",
      "Epoch 1063, Loss: 0.10648263245820999, Final Batch Loss: 0.0512046292424202\n",
      "Epoch 1064, Loss: 0.14643951132893562, Final Batch Loss: 0.05820576474070549\n",
      "Epoch 1065, Loss: 0.08093703165650368, Final Batch Loss: 0.053634632378816605\n",
      "Epoch 1066, Loss: 0.11529827490448952, Final Batch Loss: 0.06698877364397049\n",
      "Epoch 1067, Loss: 0.1540280096232891, Final Batch Loss: 0.09394349157810211\n",
      "Epoch 1068, Loss: 0.10160661488771439, Final Batch Loss: 0.053082648664712906\n",
      "Epoch 1069, Loss: 0.11158071644604206, Final Batch Loss: 0.030970895662903786\n",
      "Epoch 1070, Loss: 0.14812544360756874, Final Batch Loss: 0.10120213776826859\n",
      "Epoch 1071, Loss: 0.10401643067598343, Final Batch Loss: 0.029310934245586395\n",
      "Epoch 1072, Loss: 0.07432171888649464, Final Batch Loss: 0.028139254078269005\n",
      "Epoch 1073, Loss: 0.18108224123716354, Final Batch Loss: 0.09752944111824036\n",
      "Epoch 1074, Loss: 0.09043032675981522, Final Batch Loss: 0.019811056554317474\n",
      "Epoch 1075, Loss: 0.12093116343021393, Final Batch Loss: 0.07643942534923553\n",
      "Epoch 1076, Loss: 0.1239633858203888, Final Batch Loss: 0.07207750529050827\n",
      "Epoch 1077, Loss: 0.12945511937141418, Final Batch Loss: 0.0700995922088623\n",
      "Epoch 1078, Loss: 0.11730549484491348, Final Batch Loss: 0.043893203139305115\n",
      "Epoch 1079, Loss: 0.11488119885325432, Final Batch Loss: 0.06782753765583038\n",
      "Epoch 1080, Loss: 0.09637608751654625, Final Batch Loss: 0.04483490437269211\n",
      "Epoch 1081, Loss: 0.08737714402377605, Final Batch Loss: 0.05625230446457863\n",
      "Epoch 1082, Loss: 0.09718949347734451, Final Batch Loss: 0.04065924882888794\n",
      "Epoch 1083, Loss: 0.12162068858742714, Final Batch Loss: 0.09774093329906464\n",
      "Epoch 1084, Loss: 0.12018176540732384, Final Batch Loss: 0.08058172464370728\n",
      "Epoch 1085, Loss: 0.1308448500931263, Final Batch Loss: 0.07190152257680893\n",
      "Epoch 1086, Loss: 0.08385802432894707, Final Batch Loss: 0.04223674535751343\n",
      "Epoch 1087, Loss: 0.12072262540459633, Final Batch Loss: 0.05410035327076912\n",
      "Epoch 1088, Loss: 0.0889795795083046, Final Batch Loss: 0.03134734928607941\n",
      "Epoch 1089, Loss: 0.07958517037332058, Final Batch Loss: 0.022739877924323082\n",
      "Epoch 1090, Loss: 0.0893254466354847, Final Batch Loss: 0.02280374988913536\n",
      "Epoch 1091, Loss: 0.11124186590313911, Final Batch Loss: 0.03609655424952507\n",
      "Epoch 1092, Loss: 0.08835908025503159, Final Batch Loss: 0.0484679713845253\n",
      "Epoch 1093, Loss: 0.08813116699457169, Final Batch Loss: 0.03241544961929321\n",
      "Epoch 1094, Loss: 0.10894887521862984, Final Batch Loss: 0.05539720132946968\n",
      "Epoch 1095, Loss: 0.10467829555273056, Final Batch Loss: 0.054618965834379196\n",
      "Epoch 1096, Loss: 0.07777707651257515, Final Batch Loss: 0.03072112798690796\n",
      "Epoch 1097, Loss: 0.13757270574569702, Final Batch Loss: 0.07409872114658356\n",
      "Epoch 1098, Loss: 0.1019655391573906, Final Batch Loss: 0.06989015638828278\n",
      "Epoch 1099, Loss: 0.09424262121319771, Final Batch Loss: 0.06252526491880417\n",
      "Epoch 1100, Loss: 0.11688469350337982, Final Batch Loss: 0.07066964358091354\n",
      "Epoch 1101, Loss: 0.07750390097498894, Final Batch Loss: 0.045188795775175095\n",
      "Epoch 1102, Loss: 0.08336831629276276, Final Batch Loss: 0.04934636130928993\n",
      "Epoch 1103, Loss: 0.15892920270562172, Final Batch Loss: 0.09749379754066467\n",
      "Epoch 1104, Loss: 0.09376462548971176, Final Batch Loss: 0.032519008964300156\n",
      "Epoch 1105, Loss: 0.08056550845503807, Final Batch Loss: 0.029093578457832336\n",
      "Epoch 1106, Loss: 0.11473330482840538, Final Batch Loss: 0.04192548617720604\n",
      "Epoch 1107, Loss: 0.10816549882292747, Final Batch Loss: 0.025289099663496017\n",
      "Epoch 1108, Loss: 0.07454820349812508, Final Batch Loss: 0.03914530202746391\n",
      "Epoch 1109, Loss: 0.09546668268740177, Final Batch Loss: 0.06474794447422028\n",
      "Epoch 1110, Loss: 0.10341580957174301, Final Batch Loss: 0.03349769115447998\n",
      "Epoch 1111, Loss: 0.19585540145635605, Final Batch Loss: 0.11082934588193893\n",
      "Epoch 1112, Loss: 0.07797904126346111, Final Batch Loss: 0.022427821531891823\n",
      "Epoch 1113, Loss: 0.11099684610962868, Final Batch Loss: 0.0678323432803154\n",
      "Epoch 1114, Loss: 0.11253199726343155, Final Batch Loss: 0.040562763810157776\n",
      "Epoch 1115, Loss: 0.09325260110199451, Final Batch Loss: 0.06976894289255142\n",
      "Epoch 1116, Loss: 0.12571584060788155, Final Batch Loss: 0.07011552155017853\n",
      "Epoch 1117, Loss: 0.08763938769698143, Final Batch Loss: 0.03887847065925598\n",
      "Epoch 1118, Loss: 0.094459667801857, Final Batch Loss: 0.050668999552726746\n",
      "Epoch 1119, Loss: 0.09674717299640179, Final Batch Loss: 0.019420264288783073\n",
      "Epoch 1120, Loss: 0.10045800730586052, Final Batch Loss: 0.0427546389400959\n",
      "Epoch 1121, Loss: 0.10932226851582527, Final Batch Loss: 0.03739699348807335\n",
      "Epoch 1122, Loss: 0.10755656659603119, Final Batch Loss: 0.06481677293777466\n",
      "Epoch 1123, Loss: 0.11920686438679695, Final Batch Loss: 0.07595811039209366\n",
      "Epoch 1124, Loss: 0.10692057386040688, Final Batch Loss: 0.05608025938272476\n",
      "Epoch 1125, Loss: 0.14776071906089783, Final Batch Loss: 0.09203550219535828\n",
      "Epoch 1126, Loss: 0.09245715104043484, Final Batch Loss: 0.018783336505293846\n",
      "Epoch 1127, Loss: 0.0884205810725689, Final Batch Loss: 0.054380252957344055\n",
      "Epoch 1128, Loss: 0.11456146836280823, Final Batch Loss: 0.05675381049513817\n",
      "Epoch 1129, Loss: 0.07007338106632233, Final Batch Loss: 0.02766358107328415\n",
      "Epoch 1130, Loss: 0.07583425752818584, Final Batch Loss: 0.02706870250403881\n",
      "Epoch 1131, Loss: 0.1157211996614933, Final Batch Loss: 0.06655663996934891\n",
      "Epoch 1132, Loss: 0.12163382023572922, Final Batch Loss: 0.08459893614053726\n",
      "Epoch 1133, Loss: 0.13796883076429367, Final Batch Loss: 0.06934335082769394\n",
      "Epoch 1134, Loss: 0.12676841765642166, Final Batch Loss: 0.06083226948976517\n",
      "Epoch 1135, Loss: 0.1176246702671051, Final Batch Loss: 0.06399966776371002\n",
      "Epoch 1136, Loss: 0.08729647193104029, Final Batch Loss: 0.015604049898684025\n",
      "Epoch 1137, Loss: 0.10484279319643974, Final Batch Loss: 0.05119815841317177\n",
      "Epoch 1138, Loss: 0.09820998087525368, Final Batch Loss: 0.06141500547528267\n",
      "Epoch 1139, Loss: 0.10027248784899712, Final Batch Loss: 0.042713358998298645\n",
      "Epoch 1140, Loss: 0.11002889648079872, Final Batch Loss: 0.04312310740351677\n",
      "Epoch 1141, Loss: 0.11886543035507202, Final Batch Loss: 0.08823317289352417\n",
      "Epoch 1142, Loss: 0.062185028567910194, Final Batch Loss: 0.029052739962935448\n",
      "Epoch 1143, Loss: 0.08479930087924004, Final Batch Loss: 0.036769647151231766\n",
      "Epoch 1144, Loss: 0.07624027505517006, Final Batch Loss: 0.03575563430786133\n",
      "Epoch 1145, Loss: 0.11897002533078194, Final Batch Loss: 0.03707807883620262\n",
      "Epoch 1146, Loss: 0.10633987188339233, Final Batch Loss: 0.067009836435318\n",
      "Epoch 1147, Loss: 0.1313524805009365, Final Batch Loss: 0.07569178193807602\n",
      "Epoch 1148, Loss: 0.11379373073577881, Final Batch Loss: 0.0424148365855217\n",
      "Epoch 1149, Loss: 0.09284911304712296, Final Batch Loss: 0.047622162848711014\n",
      "Epoch 1150, Loss: 0.12414340302348137, Final Batch Loss: 0.06534203886985779\n",
      "Epoch 1151, Loss: 0.0808091051876545, Final Batch Loss: 0.03418582305312157\n",
      "Epoch 1152, Loss: 0.09729746729135513, Final Batch Loss: 0.07013857364654541\n",
      "Epoch 1153, Loss: 0.12152082473039627, Final Batch Loss: 0.03483676165342331\n",
      "Epoch 1154, Loss: 0.2765094265341759, Final Batch Loss: 0.23721615970134735\n",
      "Epoch 1155, Loss: 0.09799648076295853, Final Batch Loss: 0.02410305291414261\n",
      "Epoch 1156, Loss: 0.09543735906481743, Final Batch Loss: 0.0660005435347557\n",
      "Epoch 1157, Loss: 0.09032848477363586, Final Batch Loss: 0.03692213445901871\n",
      "Epoch 1158, Loss: 0.10100989043712616, Final Batch Loss: 0.06557612866163254\n",
      "Epoch 1159, Loss: 0.08887293748557568, Final Batch Loss: 0.030878202989697456\n",
      "Epoch 1160, Loss: 0.10310696437954903, Final Batch Loss: 0.056818973273038864\n",
      "Epoch 1161, Loss: 0.12471350282430649, Final Batch Loss: 0.08425500988960266\n",
      "Epoch 1162, Loss: 0.09401129186153412, Final Batch Loss: 0.054173942655324936\n",
      "Epoch 1163, Loss: 0.11766158789396286, Final Batch Loss: 0.05498627573251724\n",
      "Epoch 1164, Loss: 0.0973206628113985, Final Batch Loss: 0.01880231685936451\n",
      "Epoch 1165, Loss: 0.09758603200316429, Final Batch Loss: 0.05947152525186539\n",
      "Epoch 1166, Loss: 0.08660769462585449, Final Batch Loss: 0.04226624220609665\n",
      "Epoch 1167, Loss: 0.08189067617058754, Final Batch Loss: 0.041614316403865814\n",
      "Epoch 1168, Loss: 0.14066745340824127, Final Batch Loss: 0.07172799110412598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1169, Loss: 0.05109090358018875, Final Batch Loss: 0.01697450503706932\n",
      "Epoch 1170, Loss: 0.18589989840984344, Final Batch Loss: 0.09762744605541229\n",
      "Epoch 1171, Loss: 0.11717478930950165, Final Batch Loss: 0.06336669623851776\n",
      "Epoch 1172, Loss: 0.1196468211710453, Final Batch Loss: 0.07996325194835663\n",
      "Epoch 1173, Loss: 0.09046001732349396, Final Batch Loss: 0.053636226803064346\n",
      "Epoch 1174, Loss: 0.09538997896015644, Final Batch Loss: 0.06917832046747208\n",
      "Epoch 1175, Loss: 0.08849882706999779, Final Batch Loss: 0.026745252311229706\n",
      "Epoch 1176, Loss: 0.08215996250510216, Final Batch Loss: 0.03544143959879875\n",
      "Epoch 1177, Loss: 0.10289457067847252, Final Batch Loss: 0.06376350671052933\n",
      "Epoch 1178, Loss: 0.05621592327952385, Final Batch Loss: 0.022529423236846924\n",
      "Epoch 1179, Loss: 0.13381212204694748, Final Batch Loss: 0.0882529467344284\n",
      "Epoch 1180, Loss: 0.11369284987449646, Final Batch Loss: 0.06157398223876953\n",
      "Epoch 1181, Loss: 0.07289524376392365, Final Batch Loss: 0.03792501986026764\n",
      "Epoch 1182, Loss: 0.11002538725733757, Final Batch Loss: 0.06905756890773773\n",
      "Epoch 1183, Loss: 0.09372024983167648, Final Batch Loss: 0.02640663832426071\n",
      "Epoch 1184, Loss: 0.09488374553620815, Final Batch Loss: 0.06602951139211655\n",
      "Epoch 1185, Loss: 0.09839779511094093, Final Batch Loss: 0.06171676889061928\n",
      "Epoch 1186, Loss: 0.07310952804982662, Final Batch Loss: 0.053033601492643356\n",
      "Epoch 1187, Loss: 0.08300135657191277, Final Batch Loss: 0.037715982645750046\n",
      "Epoch 1188, Loss: 0.13544109091162682, Final Batch Loss: 0.0816865935921669\n",
      "Epoch 1189, Loss: 0.12358438596129417, Final Batch Loss: 0.05004486069083214\n",
      "Epoch 1190, Loss: 0.08328588772565126, Final Batch Loss: 0.009583343751728535\n",
      "Epoch 1191, Loss: 0.14310359209775925, Final Batch Loss: 0.07657019793987274\n",
      "Epoch 1192, Loss: 0.06328005529940128, Final Batch Loss: 0.04143965244293213\n",
      "Epoch 1193, Loss: 0.07611231505870819, Final Batch Loss: 0.04384506121277809\n",
      "Epoch 1194, Loss: 0.07792416401207447, Final Batch Loss: 0.058977678418159485\n",
      "Epoch 1195, Loss: 0.08424385264515877, Final Batch Loss: 0.052809782326221466\n",
      "Epoch 1196, Loss: 0.10515063628554344, Final Batch Loss: 0.05419095233082771\n",
      "Epoch 1197, Loss: 0.10131621733307838, Final Batch Loss: 0.053961317986249924\n",
      "Epoch 1198, Loss: 0.06696280464529991, Final Batch Loss: 0.029750164598226547\n",
      "Epoch 1199, Loss: 0.10302744433283806, Final Batch Loss: 0.05996738374233246\n",
      "Epoch 1200, Loss: 0.09010683372616768, Final Batch Loss: 0.04921465367078781\n",
      "Epoch 1201, Loss: 0.07892394065856934, Final Batch Loss: 0.044826094061136246\n",
      "Epoch 1202, Loss: 0.10738744959235191, Final Batch Loss: 0.0792609304189682\n",
      "Epoch 1203, Loss: 0.06323731690645218, Final Batch Loss: 0.034610360860824585\n",
      "Epoch 1204, Loss: 0.10058804228901863, Final Batch Loss: 0.03990710899233818\n",
      "Epoch 1205, Loss: 0.08362245187163353, Final Batch Loss: 0.027022186666727066\n",
      "Epoch 1206, Loss: 0.09867231920361519, Final Batch Loss: 0.061898570507764816\n",
      "Epoch 1207, Loss: 0.1125633679330349, Final Batch Loss: 0.03774143382906914\n",
      "Epoch 1208, Loss: 0.06923399679362774, Final Batch Loss: 0.044699687510728836\n",
      "Epoch 1209, Loss: 0.08174090273678303, Final Batch Loss: 0.028409840539097786\n",
      "Epoch 1210, Loss: 0.08478863909840584, Final Batch Loss: 0.04035091772675514\n",
      "Epoch 1211, Loss: 0.10153307765722275, Final Batch Loss: 0.04143265262246132\n",
      "Epoch 1212, Loss: 0.08742502517998219, Final Batch Loss: 0.027057794854044914\n",
      "Epoch 1213, Loss: 0.09547494910657406, Final Batch Loss: 0.018133504316210747\n",
      "Epoch 1214, Loss: 0.07049930281937122, Final Batch Loss: 0.02371719293296337\n",
      "Epoch 1215, Loss: 0.09130153432488441, Final Batch Loss: 0.04213818535208702\n",
      "Epoch 1216, Loss: 0.1208825446665287, Final Batch Loss: 0.04903371259570122\n",
      "Epoch 1217, Loss: 0.0982392355799675, Final Batch Loss: 0.06435295939445496\n",
      "Epoch 1218, Loss: 0.15668510645627975, Final Batch Loss: 0.08530858159065247\n",
      "Epoch 1219, Loss: 0.1275382824242115, Final Batch Loss: 0.10332615673542023\n",
      "Epoch 1220, Loss: 0.09120763838291168, Final Batch Loss: 0.030750829726457596\n",
      "Epoch 1221, Loss: 0.07776831090450287, Final Batch Loss: 0.03384179621934891\n",
      "Epoch 1222, Loss: 0.060467446222901344, Final Batch Loss: 0.01792280562222004\n",
      "Epoch 1223, Loss: 0.07390519604086876, Final Batch Loss: 0.03945687413215637\n",
      "Epoch 1224, Loss: 0.07831043377518654, Final Batch Loss: 0.0465182326734066\n",
      "Epoch 1225, Loss: 0.08374476619064808, Final Batch Loss: 0.023583287373185158\n",
      "Epoch 1226, Loss: 0.09332745894789696, Final Batch Loss: 0.026126015931367874\n",
      "Epoch 1227, Loss: 0.14887633547186852, Final Batch Loss: 0.050580840557813644\n",
      "Epoch 1228, Loss: 0.05779780261218548, Final Batch Loss: 0.030667679384350777\n",
      "Epoch 1229, Loss: 0.11076229065656662, Final Batch Loss: 0.07368656992912292\n",
      "Epoch 1230, Loss: 0.07925140857696533, Final Batch Loss: 0.04381593316793442\n",
      "Epoch 1231, Loss: 0.10612360760569572, Final Batch Loss: 0.06515761464834213\n",
      "Epoch 1232, Loss: 0.09929004311561584, Final Batch Loss: 0.04153204336762428\n",
      "Epoch 1233, Loss: 0.08590821921825409, Final Batch Loss: 0.03824703395366669\n",
      "Epoch 1234, Loss: 0.0913228876888752, Final Batch Loss: 0.046580009162425995\n",
      "Epoch 1235, Loss: 0.09641146287322044, Final Batch Loss: 0.050309859216213226\n",
      "Epoch 1236, Loss: 0.07514524087309837, Final Batch Loss: 0.023388519883155823\n",
      "Epoch 1237, Loss: 0.0764809250831604, Final Batch Loss: 0.03792833909392357\n",
      "Epoch 1238, Loss: 0.08389594033360481, Final Batch Loss: 0.02755143865942955\n",
      "Epoch 1239, Loss: 0.093122323974967, Final Batch Loss: 0.06552549451589584\n",
      "Epoch 1240, Loss: 0.14615415036678314, Final Batch Loss: 0.10087895393371582\n",
      "Epoch 1241, Loss: 0.12508259154856205, Final Batch Loss: 0.027757147327065468\n",
      "Epoch 1242, Loss: 0.10511573031544685, Final Batch Loss: 0.0674273744225502\n",
      "Epoch 1243, Loss: 0.07101241871714592, Final Batch Loss: 0.019407447427511215\n",
      "Epoch 1244, Loss: 0.12549082934856415, Final Batch Loss: 0.03288266062736511\n",
      "Epoch 1245, Loss: 0.06398800946772099, Final Batch Loss: 0.0186444241553545\n",
      "Epoch 1246, Loss: 0.07796216569840908, Final Batch Loss: 0.0249213594943285\n",
      "Epoch 1247, Loss: 0.07304784562438726, Final Batch Loss: 0.01446476113051176\n",
      "Epoch 1248, Loss: 0.11058425530791283, Final Batch Loss: 0.06272856146097183\n",
      "Epoch 1249, Loss: 0.10542741045355797, Final Batch Loss: 0.05281480401754379\n",
      "Epoch 1250, Loss: 0.07173540070652962, Final Batch Loss: 0.018693551421165466\n",
      "Epoch 1251, Loss: 0.12949995324015617, Final Batch Loss: 0.09051436185836792\n",
      "Epoch 1252, Loss: 0.08610332198441029, Final Batch Loss: 0.029946258291602135\n",
      "Epoch 1253, Loss: 0.0938523169606924, Final Batch Loss: 0.0661410465836525\n",
      "Epoch 1254, Loss: 0.06581003591418266, Final Batch Loss: 0.026985589414834976\n",
      "Epoch 1255, Loss: 0.0761316567659378, Final Batch Loss: 0.034533627331256866\n",
      "Epoch 1256, Loss: 0.10476195812225342, Final Batch Loss: 0.0572807751595974\n",
      "Epoch 1257, Loss: 0.08906044811010361, Final Batch Loss: 0.045508965849876404\n",
      "Epoch 1258, Loss: 0.075635626912117, Final Batch Loss: 0.03635311871767044\n",
      "Epoch 1259, Loss: 0.07489077374339104, Final Batch Loss: 0.04032788798213005\n",
      "Epoch 1260, Loss: 0.08210961893200874, Final Batch Loss: 0.03596062585711479\n",
      "Epoch 1261, Loss: 0.04877791553735733, Final Batch Loss: 0.02329397201538086\n",
      "Epoch 1262, Loss: 0.074995968490839, Final Batch Loss: 0.019063826650381088\n",
      "Epoch 1263, Loss: 0.07087355479598045, Final Batch Loss: 0.05178900063037872\n",
      "Epoch 1264, Loss: 0.06644373014569283, Final Batch Loss: 0.010964363813400269\n",
      "Epoch 1265, Loss: 0.09432204440236092, Final Batch Loss: 0.03102552518248558\n",
      "Epoch 1266, Loss: 0.05762055329978466, Final Batch Loss: 0.012146668508648872\n",
      "Epoch 1267, Loss: 0.06291058100759983, Final Batch Loss: 0.030054928734898567\n",
      "Epoch 1268, Loss: 0.11458990722894669, Final Batch Loss: 0.05443602055311203\n",
      "Epoch 1269, Loss: 0.05766282603144646, Final Batch Loss: 0.030403073877096176\n",
      "Epoch 1270, Loss: 0.17807696759700775, Final Batch Loss: 0.1373235136270523\n",
      "Epoch 1271, Loss: 0.10037355124950409, Final Batch Loss: 0.05512331426143646\n",
      "Epoch 1272, Loss: 0.09186869859695435, Final Batch Loss: 0.031760819256305695\n",
      "Epoch 1273, Loss: 0.11608873680233955, Final Batch Loss: 0.05992874503135681\n",
      "Epoch 1274, Loss: 0.12209615856409073, Final Batch Loss: 0.04675985127687454\n",
      "Epoch 1275, Loss: 0.05269813910126686, Final Batch Loss: 0.02097383514046669\n",
      "Epoch 1276, Loss: 0.08679303899407387, Final Batch Loss: 0.03326672315597534\n",
      "Epoch 1277, Loss: 0.11183593794703484, Final Batch Loss: 0.07212138921022415\n",
      "Epoch 1278, Loss: 0.06899850443005562, Final Batch Loss: 0.032878369092941284\n",
      "Epoch 1279, Loss: 0.06508939899504185, Final Batch Loss: 0.03520892187952995\n",
      "Epoch 1280, Loss: 0.16565881669521332, Final Batch Loss: 0.09498463571071625\n",
      "Epoch 1281, Loss: 0.09653457254171371, Final Batch Loss: 0.03776124492287636\n",
      "Epoch 1282, Loss: 0.07360227406024933, Final Batch Loss: 0.03739119693636894\n",
      "Epoch 1283, Loss: 0.10629362985491753, Final Batch Loss: 0.05549724027514458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1284, Loss: 0.06537429429590702, Final Batch Loss: 0.02396899275481701\n",
      "Epoch 1285, Loss: 0.08988130837678909, Final Batch Loss: 0.03246980533003807\n",
      "Epoch 1286, Loss: 0.1485440358519554, Final Batch Loss: 0.11519326269626617\n",
      "Epoch 1287, Loss: 0.10703045502305031, Final Batch Loss: 0.025082949548959732\n",
      "Epoch 1288, Loss: 0.07707834802567959, Final Batch Loss: 0.02343401499092579\n",
      "Epoch 1289, Loss: 0.0965087078511715, Final Batch Loss: 0.04869771748781204\n",
      "Epoch 1290, Loss: 0.07313859090209007, Final Batch Loss: 0.04993172734975815\n",
      "Epoch 1291, Loss: 0.043805450201034546, Final Batch Loss: 0.014919646084308624\n",
      "Epoch 1292, Loss: 0.08134440891444683, Final Batch Loss: 0.059143003076314926\n",
      "Epoch 1293, Loss: 0.09019988030195236, Final Batch Loss: 0.03925596922636032\n",
      "Epoch 1294, Loss: 0.08918671868741512, Final Batch Loss: 0.06164590269327164\n",
      "Epoch 1295, Loss: 0.066742442548275, Final Batch Loss: 0.05200574919581413\n",
      "Epoch 1296, Loss: 0.059068622067570686, Final Batch Loss: 0.03944050148129463\n",
      "Epoch 1297, Loss: 0.118558868765831, Final Batch Loss: 0.08640775829553604\n",
      "Epoch 1298, Loss: 0.07417535223066807, Final Batch Loss: 0.020644882693886757\n",
      "Epoch 1299, Loss: 0.10284793842583895, Final Batch Loss: 0.08774543553590775\n",
      "Epoch 1300, Loss: 0.1748659871518612, Final Batch Loss: 0.13797730207443237\n",
      "Epoch 1301, Loss: 0.11325535923242569, Final Batch Loss: 0.061604369431734085\n",
      "Epoch 1302, Loss: 0.08413173258304596, Final Batch Loss: 0.02143438160419464\n",
      "Epoch 1303, Loss: 0.07208894938230515, Final Batch Loss: 0.03938129171729088\n",
      "Epoch 1304, Loss: 0.07950639724731445, Final Batch Loss: 0.0368645079433918\n",
      "Epoch 1305, Loss: 0.07092863321304321, Final Batch Loss: 0.037696510553359985\n",
      "Epoch 1306, Loss: 0.08805292844772339, Final Batch Loss: 0.015645824372768402\n",
      "Epoch 1307, Loss: 0.04428028129041195, Final Batch Loss: 0.01770808733999729\n",
      "Epoch 1308, Loss: 0.07970740646123886, Final Batch Loss: 0.04033108055591583\n",
      "Epoch 1309, Loss: 0.09831613674759865, Final Batch Loss: 0.037469375878572464\n",
      "Epoch 1310, Loss: 0.09170025959610939, Final Batch Loss: 0.05878998339176178\n",
      "Epoch 1311, Loss: 0.08365858718752861, Final Batch Loss: 0.04763982817530632\n",
      "Epoch 1312, Loss: 0.10006439872086048, Final Batch Loss: 0.016943978145718575\n",
      "Epoch 1313, Loss: 0.11307773552834988, Final Batch Loss: 0.014965010806918144\n",
      "Epoch 1314, Loss: 0.11983519047498703, Final Batch Loss: 0.07580062747001648\n",
      "Epoch 1315, Loss: 0.06144845765084028, Final Batch Loss: 0.011234349571168423\n",
      "Epoch 1316, Loss: 0.04883266054093838, Final Batch Loss: 0.01766318827867508\n",
      "Epoch 1317, Loss: 0.0879920944571495, Final Batch Loss: 0.0446295365691185\n",
      "Epoch 1318, Loss: 0.0875902958214283, Final Batch Loss: 0.04767973721027374\n",
      "Epoch 1319, Loss: 0.10133741423487663, Final Batch Loss: 0.05221320688724518\n",
      "Epoch 1320, Loss: 0.11464648321270943, Final Batch Loss: 0.07427936792373657\n",
      "Epoch 1321, Loss: 0.09126444905996323, Final Batch Loss: 0.045849479734897614\n",
      "Epoch 1322, Loss: 0.10881143808364868, Final Batch Loss: 0.05380437523126602\n",
      "Epoch 1323, Loss: 0.13609210029244423, Final Batch Loss: 0.09469194710254669\n",
      "Epoch 1324, Loss: 0.08103716559708118, Final Batch Loss: 0.02168838120996952\n",
      "Epoch 1325, Loss: 0.07139456272125244, Final Batch Loss: 0.024530474096536636\n",
      "Epoch 1326, Loss: 0.08126453310251236, Final Batch Loss: 0.0365876667201519\n",
      "Epoch 1327, Loss: 0.05379894282668829, Final Batch Loss: 0.014094955287873745\n",
      "Epoch 1328, Loss: 0.09330885484814644, Final Batch Loss: 0.02612420544028282\n",
      "Epoch 1329, Loss: 0.06974881701171398, Final Batch Loss: 0.05173875391483307\n",
      "Epoch 1330, Loss: 0.06809687614440918, Final Batch Loss: 0.023674272000789642\n",
      "Epoch 1331, Loss: 0.11691117659211159, Final Batch Loss: 0.0771428644657135\n",
      "Epoch 1332, Loss: 0.07369449362158775, Final Batch Loss: 0.047677066177129745\n",
      "Epoch 1333, Loss: 0.06127990782260895, Final Batch Loss: 0.021150697022676468\n",
      "Epoch 1334, Loss: 0.04866080731153488, Final Batch Loss: 0.02791081927716732\n",
      "Epoch 1335, Loss: 0.07213479094207287, Final Batch Loss: 0.05524760112166405\n",
      "Epoch 1336, Loss: 0.09661050140857697, Final Batch Loss: 0.07956010103225708\n",
      "Epoch 1337, Loss: 0.06112902611494064, Final Batch Loss: 0.024801883846521378\n",
      "Epoch 1338, Loss: 0.061179716140031815, Final Batch Loss: 0.02585669606924057\n",
      "Epoch 1339, Loss: 0.059286193922162056, Final Batch Loss: 0.031896159052848816\n",
      "Epoch 1340, Loss: 0.04657980799674988, Final Batch Loss: 0.014000501483678818\n",
      "Epoch 1341, Loss: 0.05246529169380665, Final Batch Loss: 0.015924053266644478\n",
      "Epoch 1342, Loss: 0.0825525913387537, Final Batch Loss: 0.06603339314460754\n",
      "Epoch 1343, Loss: 0.13333093002438545, Final Batch Loss: 0.03580744192004204\n",
      "Epoch 1344, Loss: 0.05255003459751606, Final Batch Loss: 0.016998423263430595\n",
      "Epoch 1345, Loss: 0.1376228928565979, Final Batch Loss: 0.09977007657289505\n",
      "Epoch 1346, Loss: 0.06418900098651648, Final Batch Loss: 0.012969671748578548\n",
      "Epoch 1347, Loss: 0.0665575098246336, Final Batch Loss: 0.021759210154414177\n",
      "Epoch 1348, Loss: 0.08000286668539047, Final Batch Loss: 0.040505461394786835\n",
      "Epoch 1349, Loss: 0.046007716096937656, Final Batch Loss: 0.013523046858608723\n",
      "Epoch 1350, Loss: 0.05988943949341774, Final Batch Loss: 0.01854322850704193\n",
      "Epoch 1351, Loss: 0.083484822884202, Final Batch Loss: 0.05645367503166199\n",
      "Epoch 1352, Loss: 0.0661061517894268, Final Batch Loss: 0.027811799198389053\n",
      "Epoch 1353, Loss: 0.11019555106759071, Final Batch Loss: 0.05180172994732857\n",
      "Epoch 1354, Loss: 0.10432136803865433, Final Batch Loss: 0.061687175184488297\n",
      "Epoch 1355, Loss: 0.06454330869019032, Final Batch Loss: 0.02029578946530819\n",
      "Epoch 1356, Loss: 0.09991386905312538, Final Batch Loss: 0.07217791676521301\n",
      "Epoch 1357, Loss: 0.05316379852592945, Final Batch Loss: 0.025631703436374664\n",
      "Epoch 1358, Loss: 0.08004218339920044, Final Batch Loss: 0.041680578142404556\n",
      "Epoch 1359, Loss: 0.05401564855128527, Final Batch Loss: 0.010624301619827747\n",
      "Epoch 1360, Loss: 0.06638677604496479, Final Batch Loss: 0.036194078624248505\n",
      "Epoch 1361, Loss: 0.11072524264454842, Final Batch Loss: 0.06870000809431076\n",
      "Epoch 1362, Loss: 0.047855060547590256, Final Batch Loss: 0.01894870586693287\n",
      "Epoch 1363, Loss: 0.061759233474731445, Final Batch Loss: 0.03839295729994774\n",
      "Epoch 1364, Loss: 0.06532522290945053, Final Batch Loss: 0.019482076168060303\n",
      "Epoch 1365, Loss: 0.043645504862070084, Final Batch Loss: 0.01731686294078827\n",
      "Epoch 1366, Loss: 0.08063995465636253, Final Batch Loss: 0.04293157160282135\n",
      "Epoch 1367, Loss: 0.1207574512809515, Final Batch Loss: 0.09206248819828033\n",
      "Epoch 1368, Loss: 0.06323742493987083, Final Batch Loss: 0.023132838308811188\n",
      "Epoch 1369, Loss: 0.06893568113446236, Final Batch Loss: 0.04329409450292587\n",
      "Epoch 1370, Loss: 0.07611166499555111, Final Batch Loss: 0.046506669372320175\n",
      "Epoch 1371, Loss: 0.11737147159874439, Final Batch Loss: 0.08994141966104507\n",
      "Epoch 1372, Loss: 0.08829079940915108, Final Batch Loss: 0.04728543758392334\n",
      "Epoch 1373, Loss: 0.08266488835215569, Final Batch Loss: 0.057032544165849686\n",
      "Epoch 1374, Loss: 0.07287561520934105, Final Batch Loss: 0.037168409675359726\n",
      "Epoch 1375, Loss: 0.06972508132457733, Final Batch Loss: 0.020485661923885345\n",
      "Epoch 1376, Loss: 0.05280562862753868, Final Batch Loss: 0.016232259571552277\n",
      "Epoch 1377, Loss: 0.10670694708824158, Final Batch Loss: 0.05345549061894417\n",
      "Epoch 1378, Loss: 0.08373215235769749, Final Batch Loss: 0.020739758387207985\n",
      "Epoch 1379, Loss: 0.04689014330506325, Final Batch Loss: 0.017487335950136185\n",
      "Epoch 1380, Loss: 0.055969079956412315, Final Batch Loss: 0.021464521065354347\n",
      "Epoch 1381, Loss: 0.09126365557312965, Final Batch Loss: 0.04424535483121872\n",
      "Epoch 1382, Loss: 0.11773437447845936, Final Batch Loss: 0.09603126347064972\n",
      "Epoch 1383, Loss: 0.04640917107462883, Final Batch Loss: 0.03772202879190445\n",
      "Epoch 1384, Loss: 0.0982331670820713, Final Batch Loss: 0.04866451025009155\n",
      "Epoch 1385, Loss: 0.055321477353572845, Final Batch Loss: 0.03561671823263168\n",
      "Epoch 1386, Loss: 0.0343254990875721, Final Batch Loss: 0.01236153393983841\n",
      "Epoch 1387, Loss: 0.09902477450668812, Final Batch Loss: 0.06944752484560013\n",
      "Epoch 1388, Loss: 0.06465765088796616, Final Batch Loss: 0.033365845680236816\n",
      "Epoch 1389, Loss: 0.09767309948801994, Final Batch Loss: 0.05216170474886894\n",
      "Epoch 1390, Loss: 0.058861615136265755, Final Batch Loss: 0.027548572048544884\n",
      "Epoch 1391, Loss: 0.059508549980819225, Final Batch Loss: 0.012493106536567211\n",
      "Epoch 1392, Loss: 0.07960844598710537, Final Batch Loss: 0.025911448523402214\n",
      "Epoch 1393, Loss: 0.08761538006365299, Final Batch Loss: 0.07418499141931534\n",
      "Epoch 1394, Loss: 0.035632635466754436, Final Batch Loss: 0.008345278911292553\n",
      "Epoch 1395, Loss: 0.06943082995712757, Final Batch Loss: 0.04056331887841225\n",
      "Epoch 1396, Loss: 0.11095619574189186, Final Batch Loss: 0.06484577059745789\n",
      "Epoch 1397, Loss: 0.052303675562143326, Final Batch Loss: 0.0208173505961895\n",
      "Epoch 1398, Loss: 0.0832701064646244, Final Batch Loss: 0.015466641634702682\n",
      "Epoch 1399, Loss: 0.07076401449739933, Final Batch Loss: 0.039973802864551544\n",
      "Epoch 1400, Loss: 0.06625364534556866, Final Batch Loss: 0.017470208927989006\n",
      "Epoch 1401, Loss: 0.09113547764718533, Final Batch Loss: 0.07547323405742645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1402, Loss: 0.08778241835534573, Final Batch Loss: 0.06750306487083435\n",
      "Epoch 1403, Loss: 0.08309807255864143, Final Batch Loss: 0.04500286653637886\n",
      "Epoch 1404, Loss: 0.06952287629246712, Final Batch Loss: 0.03783420845866203\n",
      "Epoch 1405, Loss: 0.04236625414341688, Final Batch Loss: 0.013493501581251621\n",
      "Epoch 1406, Loss: 0.06639239005744457, Final Batch Loss: 0.039928875863552094\n",
      "Epoch 1407, Loss: 0.07018903270363808, Final Batch Loss: 0.03743026405572891\n",
      "Epoch 1408, Loss: 0.04803807660937309, Final Batch Loss: 0.022666364908218384\n",
      "Epoch 1409, Loss: 0.041642592754215, Final Batch Loss: 0.006503300275653601\n",
      "Epoch 1410, Loss: 0.04978622682392597, Final Batch Loss: 0.017596179619431496\n",
      "Epoch 1411, Loss: 0.05286688357591629, Final Batch Loss: 0.020225558429956436\n",
      "Epoch 1412, Loss: 0.14647223055362701, Final Batch Loss: 0.0642126277089119\n",
      "Epoch 1413, Loss: 0.1115976981818676, Final Batch Loss: 0.06859762221574783\n",
      "Epoch 1414, Loss: 0.06788396649062634, Final Batch Loss: 0.03859950602054596\n",
      "Epoch 1415, Loss: 0.09316599369049072, Final Batch Loss: 0.04396853595972061\n",
      "Epoch 1416, Loss: 0.11127114295959473, Final Batch Loss: 0.04461245238780975\n",
      "Epoch 1417, Loss: 0.04398212768137455, Final Batch Loss: 0.014304142445325851\n",
      "Epoch 1418, Loss: 0.07463332079350948, Final Batch Loss: 0.013293134048581123\n",
      "Epoch 1419, Loss: 0.06893221661448479, Final Batch Loss: 0.03323088958859444\n",
      "Epoch 1420, Loss: 0.1489164475351572, Final Batch Loss: 0.02410316653549671\n",
      "Epoch 1421, Loss: 0.060340067371726036, Final Batch Loss: 0.03299448639154434\n",
      "Epoch 1422, Loss: 0.0499996691942215, Final Batch Loss: 0.01958300918340683\n",
      "Epoch 1423, Loss: 0.050220074132084846, Final Batch Loss: 0.03397589921951294\n",
      "Epoch 1424, Loss: 0.11887631937861443, Final Batch Loss: 0.07166530191898346\n",
      "Epoch 1425, Loss: 0.07666799053549767, Final Batch Loss: 0.04003164544701576\n",
      "Epoch 1426, Loss: 0.09874022379517555, Final Batch Loss: 0.04578762501478195\n",
      "Epoch 1427, Loss: 0.09409292880445719, Final Batch Loss: 0.07847686856985092\n",
      "Epoch 1428, Loss: 0.07627592235803604, Final Batch Loss: 0.04481467232108116\n",
      "Epoch 1429, Loss: 0.09631084278225899, Final Batch Loss: 0.03810586780309677\n",
      "Epoch 1430, Loss: 0.05905783362686634, Final Batch Loss: 0.034290507435798645\n",
      "Epoch 1431, Loss: 0.062495943158864975, Final Batch Loss: 0.03026249259710312\n",
      "Epoch 1432, Loss: 0.08162319287657738, Final Batch Loss: 0.05439579114317894\n",
      "Epoch 1433, Loss: 0.07842330448329449, Final Batch Loss: 0.0214249175041914\n",
      "Epoch 1434, Loss: 0.042618720792233944, Final Batch Loss: 0.015038293786346912\n",
      "Epoch 1435, Loss: 0.039809130132198334, Final Batch Loss: 0.01151912659406662\n",
      "Epoch 1436, Loss: 0.1298968903720379, Final Batch Loss: 0.09632916748523712\n",
      "Epoch 1437, Loss: 0.053668588399887085, Final Batch Loss: 0.03048192895948887\n",
      "Epoch 1438, Loss: 0.05875181034207344, Final Batch Loss: 0.02839568443596363\n",
      "Epoch 1439, Loss: 0.11143071576952934, Final Batch Loss: 0.07075464725494385\n",
      "Epoch 1440, Loss: 0.09228743612766266, Final Batch Loss: 0.04475263133645058\n",
      "Epoch 1441, Loss: 0.07810877077281475, Final Batch Loss: 0.04687880724668503\n",
      "Epoch 1442, Loss: 0.07582196593284607, Final Batch Loss: 0.032619357109069824\n",
      "Epoch 1443, Loss: 0.0776780303567648, Final Batch Loss: 0.04864860326051712\n",
      "Epoch 1444, Loss: 0.034772722981870174, Final Batch Loss: 0.012767384760081768\n",
      "Epoch 1445, Loss: 0.05223874747753143, Final Batch Loss: 0.011071126908063889\n",
      "Epoch 1446, Loss: 0.04581453278660774, Final Batch Loss: 0.03334839269518852\n",
      "Epoch 1447, Loss: 0.09123024623841047, Final Batch Loss: 0.010560150258243084\n",
      "Epoch 1448, Loss: 0.05882192216813564, Final Batch Loss: 0.03419472649693489\n",
      "Epoch 1449, Loss: 0.10078775510191917, Final Batch Loss: 0.04148496687412262\n",
      "Epoch 1450, Loss: 0.05523284990340471, Final Batch Loss: 0.043921079486608505\n",
      "Epoch 1451, Loss: 0.1078537181019783, Final Batch Loss: 0.06805328279733658\n",
      "Epoch 1452, Loss: 0.07731052488088608, Final Batch Loss: 0.03676973655819893\n",
      "Epoch 1453, Loss: 0.09745771065354347, Final Batch Loss: 0.06802354753017426\n",
      "Epoch 1454, Loss: 0.052917031571269035, Final Batch Loss: 0.024597804993391037\n",
      "Epoch 1455, Loss: 0.05229356978088617, Final Batch Loss: 0.014052298851311207\n",
      "Epoch 1456, Loss: 0.06029646098613739, Final Batch Loss: 0.0360487662255764\n",
      "Epoch 1457, Loss: 0.04539870843291283, Final Batch Loss: 0.018075402826070786\n",
      "Epoch 1458, Loss: 0.07190446555614471, Final Batch Loss: 0.017710711807012558\n",
      "Epoch 1459, Loss: 0.06045098975300789, Final Batch Loss: 0.03741130232810974\n",
      "Epoch 1460, Loss: 0.06599593907594681, Final Batch Loss: 0.0326998345553875\n",
      "Epoch 1461, Loss: 0.053846852853894234, Final Batch Loss: 0.028852902352809906\n",
      "Epoch 1462, Loss: 0.0648813508450985, Final Batch Loss: 0.04002774506807327\n",
      "Epoch 1463, Loss: 0.07151293940842152, Final Batch Loss: 0.02886858768761158\n",
      "Epoch 1464, Loss: 0.046778079122304916, Final Batch Loss: 0.012907367199659348\n",
      "Epoch 1465, Loss: 0.07057131081819534, Final Batch Loss: 0.033381346613168716\n",
      "Epoch 1466, Loss: 0.04544074833393097, Final Batch Loss: 0.017579946666955948\n",
      "Epoch 1467, Loss: 0.0860060267150402, Final Batch Loss: 0.035437434911727905\n",
      "Epoch 1468, Loss: 0.0939122661948204, Final Batch Loss: 0.02425893396139145\n",
      "Epoch 1469, Loss: 0.05469956062734127, Final Batch Loss: 0.03244097903370857\n",
      "Epoch 1470, Loss: 0.057710884138941765, Final Batch Loss: 0.022370720282197\n",
      "Epoch 1471, Loss: 0.09075546264648438, Final Batch Loss: 0.05153602734208107\n",
      "Epoch 1472, Loss: 0.05380085110664368, Final Batch Loss: 0.03483239561319351\n",
      "Epoch 1473, Loss: 0.06477536913007498, Final Batch Loss: 0.013170226477086544\n",
      "Epoch 1474, Loss: 0.047777408733963966, Final Batch Loss: 0.023156490176916122\n",
      "Epoch 1475, Loss: 0.06768467463552952, Final Batch Loss: 0.026374736800789833\n",
      "Epoch 1476, Loss: 0.06041628494858742, Final Batch Loss: 0.014944151043891907\n",
      "Epoch 1477, Loss: 0.12106495723128319, Final Batch Loss: 0.04682300612330437\n",
      "Epoch 1478, Loss: 0.05323328450322151, Final Batch Loss: 0.029681243002414703\n",
      "Epoch 1479, Loss: 0.09664862044155598, Final Batch Loss: 0.07355469465255737\n",
      "Epoch 1480, Loss: 0.06604674458503723, Final Batch Loss: 0.02707786113023758\n",
      "Epoch 1481, Loss: 0.10422680899500847, Final Batch Loss: 0.04201625660061836\n",
      "Epoch 1482, Loss: 0.07460455968976021, Final Batch Loss: 0.02211013063788414\n",
      "Epoch 1483, Loss: 0.06911163963377476, Final Batch Loss: 0.029543964192271233\n",
      "Epoch 1484, Loss: 0.0844256728887558, Final Batch Loss: 0.04730486124753952\n",
      "Epoch 1485, Loss: 0.1319686658680439, Final Batch Loss: 0.033489812165498734\n",
      "Epoch 1486, Loss: 0.0666636060923338, Final Batch Loss: 0.03791818767786026\n",
      "Epoch 1487, Loss: 0.07704713009297848, Final Batch Loss: 0.0489499494433403\n",
      "Epoch 1488, Loss: 0.09243141673505306, Final Batch Loss: 0.06256432831287384\n",
      "Epoch 1489, Loss: 0.09180321544408798, Final Batch Loss: 0.05930737778544426\n",
      "Epoch 1490, Loss: 0.08668336644768715, Final Batch Loss: 0.0456855371594429\n",
      "Epoch 1491, Loss: 0.07510688342154026, Final Batch Loss: 0.017400166019797325\n",
      "Epoch 1492, Loss: 0.053811101242899895, Final Batch Loss: 0.020746441558003426\n",
      "Epoch 1493, Loss: 0.045011334121227264, Final Batch Loss: 0.011457838118076324\n",
      "Epoch 1494, Loss: 0.0510600320994854, Final Batch Loss: 0.021504243835806847\n",
      "Epoch 1495, Loss: 0.05490755755454302, Final Batch Loss: 0.00772284809499979\n",
      "Epoch 1496, Loss: 0.1038887333124876, Final Batch Loss: 0.024378230795264244\n",
      "Epoch 1497, Loss: 0.05652050860226154, Final Batch Loss: 0.032270126044750214\n",
      "Epoch 1498, Loss: 0.09427278488874435, Final Batch Loss: 0.05606646090745926\n",
      "Epoch 1499, Loss: 0.08100139163434505, Final Batch Loss: 0.057675208896398544\n",
      "Epoch 1500, Loss: 0.08513243310153484, Final Batch Loss: 0.06158194690942764\n",
      "Epoch 1501, Loss: 0.0837652925401926, Final Batch Loss: 0.054026007652282715\n",
      "Epoch 1502, Loss: 0.1586155816912651, Final Batch Loss: 0.11474859714508057\n",
      "Epoch 1503, Loss: 0.05692112818360329, Final Batch Loss: 0.018922779709100723\n",
      "Epoch 1504, Loss: 0.09203173872083426, Final Batch Loss: 0.07792230695486069\n",
      "Epoch 1505, Loss: 0.10794616676867008, Final Batch Loss: 0.08285609632730484\n",
      "Epoch 1506, Loss: 0.052850525826215744, Final Batch Loss: 0.012769833207130432\n",
      "Epoch 1507, Loss: 0.051653383299708366, Final Batch Loss: 0.01093626581132412\n",
      "Epoch 1508, Loss: 0.05676732771098614, Final Batch Loss: 0.026186149567365646\n",
      "Epoch 1509, Loss: 0.06638893485069275, Final Batch Loss: 0.04787976294755936\n",
      "Epoch 1510, Loss: 0.04422768950462341, Final Batch Loss: 0.0189362782984972\n",
      "Epoch 1511, Loss: 0.11206236854195595, Final Batch Loss: 0.044760216027498245\n",
      "Epoch 1512, Loss: 0.062247661873698235, Final Batch Loss: 0.038168977946043015\n",
      "Epoch 1513, Loss: 0.03451121971011162, Final Batch Loss: 0.016663406044244766\n",
      "Epoch 1514, Loss: 0.053466327488422394, Final Batch Loss: 0.019522443413734436\n",
      "Epoch 1515, Loss: 0.046462505124509335, Final Batch Loss: 0.015354917384684086\n",
      "Epoch 1516, Loss: 0.07589254342019558, Final Batch Loss: 0.058488693088293076\n",
      "Epoch 1517, Loss: 0.052003493532538414, Final Batch Loss: 0.024947451427578926\n",
      "Epoch 1518, Loss: 0.07780414447188377, Final Batch Loss: 0.03768067806959152\n",
      "Epoch 1519, Loss: 0.0500238835811615, Final Batch Loss: 0.018294207751750946\n",
      "Epoch 1520, Loss: 0.042479921132326126, Final Batch Loss: 0.012582305818796158\n",
      "Epoch 1521, Loss: 0.07838514819741249, Final Batch Loss: 0.05392074957489967\n",
      "Epoch 1522, Loss: 0.057855935767292976, Final Batch Loss: 0.03208303824067116\n",
      "Epoch 1523, Loss: 0.07836512848734856, Final Batch Loss: 0.019747033715248108\n",
      "Epoch 1524, Loss: 0.07243571057915688, Final Batch Loss: 0.05527988076210022\n",
      "Epoch 1525, Loss: 0.04410844761878252, Final Batch Loss: 0.008464743383228779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1526, Loss: 0.08288581855595112, Final Batch Loss: 0.060863226652145386\n",
      "Epoch 1527, Loss: 0.07895767316222191, Final Batch Loss: 0.037463948130607605\n",
      "Epoch 1528, Loss: 0.02856232365593314, Final Batch Loss: 0.004551952239125967\n",
      "Epoch 1529, Loss: 0.06779040955007076, Final Batch Loss: 0.042629603296518326\n",
      "Epoch 1530, Loss: 0.11123854294419289, Final Batch Loss: 0.04535004869103432\n",
      "Epoch 1531, Loss: 0.07595453597605228, Final Batch Loss: 0.05503430962562561\n",
      "Epoch 1532, Loss: 0.05565302260220051, Final Batch Loss: 0.0286442581564188\n",
      "Epoch 1533, Loss: 0.17211373895406723, Final Batch Loss: 0.06633668392896652\n",
      "Epoch 1534, Loss: 0.07491614483296871, Final Batch Loss: 0.02965247817337513\n",
      "Epoch 1535, Loss: 0.06805943511426449, Final Batch Loss: 0.021309448406100273\n",
      "Epoch 1536, Loss: 0.07761747017502785, Final Batch Loss: 0.045783162117004395\n",
      "Epoch 1537, Loss: 0.09109057113528252, Final Batch Loss: 0.03292355686426163\n",
      "Epoch 1538, Loss: 0.10701112449169159, Final Batch Loss: 0.03814859688282013\n",
      "Epoch 1539, Loss: 0.05774479731917381, Final Batch Loss: 0.03873969614505768\n",
      "Epoch 1540, Loss: 0.10749008506536484, Final Batch Loss: 0.045317143201828\n",
      "Epoch 1541, Loss: 0.0983654260635376, Final Batch Loss: 0.04554909095168114\n",
      "Epoch 1542, Loss: 0.0915779359638691, Final Batch Loss: 0.04148692637681961\n",
      "Epoch 1543, Loss: 0.06896778382360935, Final Batch Loss: 0.024519754573702812\n",
      "Epoch 1544, Loss: 0.09857083298265934, Final Batch Loss: 0.08390134572982788\n",
      "Epoch 1545, Loss: 0.0863412395119667, Final Batch Loss: 0.04933135211467743\n",
      "Epoch 1546, Loss: 0.10615565627813339, Final Batch Loss: 0.04532239958643913\n",
      "Epoch 1547, Loss: 0.09026230871677399, Final Batch Loss: 0.0548643134534359\n",
      "Epoch 1548, Loss: 0.04506652429699898, Final Batch Loss: 0.016791345551609993\n",
      "Epoch 1549, Loss: 0.06350426562130451, Final Batch Loss: 0.02782249264419079\n",
      "Epoch 1550, Loss: 0.08511275053024292, Final Batch Loss: 0.049361102283000946\n",
      "Epoch 1551, Loss: 0.06056982837617397, Final Batch Loss: 0.024115825071930885\n",
      "Epoch 1552, Loss: 0.07235805317759514, Final Batch Loss: 0.03716398403048515\n",
      "Epoch 1553, Loss: 0.03957749158143997, Final Batch Loss: 0.01206769049167633\n",
      "Epoch 1554, Loss: 0.06944920308887959, Final Batch Loss: 0.05100376158952713\n",
      "Epoch 1555, Loss: 0.058236751705408096, Final Batch Loss: 0.023598965257406235\n",
      "Epoch 1556, Loss: 0.1116502471268177, Final Batch Loss: 0.06468326598405838\n",
      "Epoch 1557, Loss: 0.0995076335966587, Final Batch Loss: 0.0375160276889801\n",
      "Epoch 1558, Loss: 0.08487164601683617, Final Batch Loss: 0.06563246995210648\n",
      "Epoch 1559, Loss: 0.05481312423944473, Final Batch Loss: 0.01668199896812439\n",
      "Epoch 1560, Loss: 0.050246549770236015, Final Batch Loss: 0.023576632142066956\n",
      "Epoch 1561, Loss: 0.0839470624923706, Final Batch Loss: 0.05281134694814682\n",
      "Epoch 1562, Loss: 0.06952706351876259, Final Batch Loss: 0.019790396094322205\n",
      "Epoch 1563, Loss: 0.11599081009626389, Final Batch Loss: 0.05923549830913544\n",
      "Epoch 1564, Loss: 0.17118974402546883, Final Batch Loss: 0.11639933288097382\n",
      "Epoch 1565, Loss: 0.08400756865739822, Final Batch Loss: 0.037847161293029785\n",
      "Epoch 1566, Loss: 0.09539608657360077, Final Batch Loss: 0.05799358710646629\n",
      "Epoch 1567, Loss: 0.06605663150548935, Final Batch Loss: 0.030050065368413925\n",
      "Epoch 1568, Loss: 0.09417915530502796, Final Batch Loss: 0.06581296771764755\n",
      "Epoch 1569, Loss: 0.09627890214323997, Final Batch Loss: 0.02576253190636635\n",
      "Epoch 1570, Loss: 0.13564803823828697, Final Batch Loss: 0.10173159837722778\n",
      "Epoch 1571, Loss: 0.07817859947681427, Final Batch Loss: 0.04947476089000702\n",
      "Epoch 1572, Loss: 0.06726296991109848, Final Batch Loss: 0.036822207272052765\n",
      "Epoch 1573, Loss: 0.07351785153150558, Final Batch Loss: 0.0482083261013031\n",
      "Epoch 1574, Loss: 0.09328758344054222, Final Batch Loss: 0.06083573028445244\n",
      "Epoch 1575, Loss: 0.17666106671094894, Final Batch Loss: 0.1044098436832428\n",
      "Epoch 1576, Loss: 0.10595211945474148, Final Batch Loss: 0.016382483765482903\n",
      "Epoch 1577, Loss: 0.07059738226234913, Final Batch Loss: 0.031027862802147865\n",
      "Epoch 1578, Loss: 0.09861210361123085, Final Batch Loss: 0.019900966435670853\n",
      "Epoch 1579, Loss: 0.22634314373135567, Final Batch Loss: 0.18517674505710602\n",
      "Epoch 1580, Loss: 0.0959464106708765, Final Batch Loss: 0.024769606068730354\n",
      "Epoch 1581, Loss: 0.11940192058682442, Final Batch Loss: 0.061851825565099716\n",
      "Epoch 1582, Loss: 0.03069390496239066, Final Batch Loss: 0.005181584041565657\n",
      "Epoch 1583, Loss: 0.11182210594415665, Final Batch Loss: 0.05378260836005211\n",
      "Epoch 1584, Loss: 0.10136862471699715, Final Batch Loss: 0.0711265504360199\n",
      "Epoch 1585, Loss: 0.08142194524407387, Final Batch Loss: 0.04715104401111603\n",
      "Epoch 1586, Loss: 0.18372973799705505, Final Batch Loss: 0.13622811436653137\n",
      "Epoch 1587, Loss: 0.07428706251084805, Final Batch Loss: 0.027336912229657173\n",
      "Epoch 1588, Loss: 0.061542998999357224, Final Batch Loss: 0.02210366353392601\n",
      "Epoch 1589, Loss: 0.068318797275424, Final Batch Loss: 0.044498223811388016\n",
      "Epoch 1590, Loss: 0.07114676386117935, Final Batch Loss: 0.035255324095487595\n",
      "Epoch 1591, Loss: 0.05072626657783985, Final Batch Loss: 0.023574529215693474\n",
      "Epoch 1592, Loss: 0.10924992337822914, Final Batch Loss: 0.034418705850839615\n",
      "Epoch 1593, Loss: 0.05115991830825806, Final Batch Loss: 0.018584556877613068\n",
      "Epoch 1594, Loss: 0.10987587459385395, Final Batch Loss: 0.08025466650724411\n",
      "Epoch 1595, Loss: 0.06088836304843426, Final Batch Loss: 0.03106103651225567\n",
      "Epoch 1596, Loss: 0.0890690479427576, Final Batch Loss: 0.012183891609311104\n",
      "Epoch 1597, Loss: 0.13892626017332077, Final Batch Loss: 0.04862256348133087\n",
      "Epoch 1598, Loss: 0.05496420431882143, Final Batch Loss: 0.04037560150027275\n",
      "Epoch 1599, Loss: 0.06114824861288071, Final Batch Loss: 0.020123451948165894\n",
      "Epoch 1600, Loss: 0.09985467419028282, Final Batch Loss: 0.07100504636764526\n",
      "Epoch 1601, Loss: 0.06075673922896385, Final Batch Loss: 0.011985965073108673\n",
      "Epoch 1602, Loss: 0.06802825070917606, Final Batch Loss: 0.024495059624314308\n",
      "Epoch 1603, Loss: 0.08054040744900703, Final Batch Loss: 0.04793151095509529\n",
      "Epoch 1604, Loss: 0.06579579599201679, Final Batch Loss: 0.039314016699790955\n",
      "Epoch 1605, Loss: 0.11124996468424797, Final Batch Loss: 0.07531796395778656\n",
      "Epoch 1606, Loss: 0.09500172361731529, Final Batch Loss: 0.055397093296051025\n",
      "Epoch 1607, Loss: 0.057801853865385056, Final Batch Loss: 0.03462784364819527\n",
      "Epoch 1608, Loss: 0.09600071795284748, Final Batch Loss: 0.08096467703580856\n",
      "Epoch 1609, Loss: 0.06509998254477978, Final Batch Loss: 0.01754850707948208\n",
      "Epoch 1610, Loss: 0.08094039186835289, Final Batch Loss: 0.048484332859516144\n",
      "Epoch 1611, Loss: 0.06495181284844875, Final Batch Loss: 0.02588963694870472\n",
      "Epoch 1612, Loss: 0.05024142563343048, Final Batch Loss: 0.027781855314970016\n",
      "Epoch 1613, Loss: 0.05555150285363197, Final Batch Loss: 0.026935793459415436\n",
      "Epoch 1614, Loss: 0.05520315654575825, Final Batch Loss: 0.030018074437975883\n",
      "Epoch 1615, Loss: 0.07372306473553181, Final Batch Loss: 0.015379918739199638\n",
      "Epoch 1616, Loss: 0.05795665644109249, Final Batch Loss: 0.04615268483757973\n",
      "Epoch 1617, Loss: 0.05375340208411217, Final Batch Loss: 0.014667518436908722\n",
      "Epoch 1618, Loss: 0.06753917783498764, Final Batch Loss: 0.04193902760744095\n",
      "Epoch 1619, Loss: 0.10742038115859032, Final Batch Loss: 0.05529184266924858\n",
      "Epoch 1620, Loss: 0.09145268052816391, Final Batch Loss: 0.03324664384126663\n",
      "Epoch 1621, Loss: 0.07598397880792618, Final Batch Loss: 0.045051928609609604\n",
      "Epoch 1622, Loss: 0.07199993543326855, Final Batch Loss: 0.04530920460820198\n",
      "Epoch 1623, Loss: 0.07949618436396122, Final Batch Loss: 0.06052706018090248\n",
      "Epoch 1624, Loss: 0.06932544521987438, Final Batch Loss: 0.047121137380599976\n",
      "Epoch 1625, Loss: 0.12578632310032845, Final Batch Loss: 0.09928499907255173\n",
      "Epoch 1626, Loss: 0.07724988088011742, Final Batch Loss: 0.04773886501789093\n",
      "Epoch 1627, Loss: 0.1275409311056137, Final Batch Loss: 0.0862821638584137\n",
      "Epoch 1628, Loss: 0.06364499125629663, Final Batch Loss: 0.05301526188850403\n",
      "Epoch 1629, Loss: 0.06573109328746796, Final Batch Loss: 0.05492117628455162\n",
      "Epoch 1630, Loss: 0.0815692599862814, Final Batch Loss: 0.06149699538946152\n",
      "Epoch 1631, Loss: 0.05240961443632841, Final Batch Loss: 0.012487745843827724\n",
      "Epoch 1632, Loss: 0.08738892711699009, Final Batch Loss: 0.030526863411068916\n",
      "Epoch 1633, Loss: 0.03961142897605896, Final Batch Loss: 0.012569872662425041\n",
      "Epoch 1634, Loss: 0.08326935395598412, Final Batch Loss: 0.048657309263944626\n",
      "Epoch 1635, Loss: 0.12375347688794136, Final Batch Loss: 0.06414426863193512\n",
      "Epoch 1636, Loss: 0.07540645450353622, Final Batch Loss: 0.041349731385707855\n",
      "Epoch 1637, Loss: 0.042667191475629807, Final Batch Loss: 0.020262867212295532\n",
      "Epoch 1638, Loss: 0.08001652453094721, Final Batch Loss: 0.007127583958208561\n",
      "Epoch 1639, Loss: 0.10469435155391693, Final Batch Loss: 0.07296900451183319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1640, Loss: 0.09020978212356567, Final Batch Loss: 0.04786747694015503\n",
      "Epoch 1641, Loss: 0.11690641567111015, Final Batch Loss: 0.0697120726108551\n",
      "Epoch 1642, Loss: 0.059819238260388374, Final Batch Loss: 0.01601955108344555\n",
      "Epoch 1643, Loss: 0.04398871399462223, Final Batch Loss: 0.01426473818719387\n",
      "Epoch 1644, Loss: 0.04313359968364239, Final Batch Loss: 0.0098137017339468\n",
      "Epoch 1645, Loss: 0.04161802399903536, Final Batch Loss: 0.030118150636553764\n",
      "Epoch 1646, Loss: 0.06143193505704403, Final Batch Loss: 0.020514799281954765\n",
      "Epoch 1647, Loss: 0.0360895860940218, Final Batch Loss: 0.00979243777692318\n",
      "Epoch 1648, Loss: 0.0699322298169136, Final Batch Loss: 0.05923474207520485\n",
      "Epoch 1649, Loss: 0.06326334550976753, Final Batch Loss: 0.01694067195057869\n",
      "Epoch 1650, Loss: 0.08759983256459236, Final Batch Loss: 0.04852471500635147\n",
      "Epoch 1651, Loss: 0.057737911120057106, Final Batch Loss: 0.034148912876844406\n",
      "Epoch 1652, Loss: 0.05532652325928211, Final Batch Loss: 0.0451459065079689\n",
      "Epoch 1653, Loss: 0.08777927048504353, Final Batch Loss: 0.06962420791387558\n",
      "Epoch 1654, Loss: 0.09928985685110092, Final Batch Loss: 0.07438477873802185\n",
      "Epoch 1655, Loss: 0.059045059606432915, Final Batch Loss: 0.035681888461112976\n",
      "Epoch 1656, Loss: 0.08841164037585258, Final Batch Loss: 0.04379452019929886\n",
      "Epoch 1657, Loss: 0.0684334933757782, Final Batch Loss: 0.040112946182489395\n",
      "Epoch 1658, Loss: 0.09448204003274441, Final Batch Loss: 0.07355168461799622\n",
      "Epoch 1659, Loss: 0.06259629502892494, Final Batch Loss: 0.032595884054899216\n",
      "Epoch 1660, Loss: 0.04929767921566963, Final Batch Loss: 0.01626892387866974\n",
      "Epoch 1661, Loss: 0.08210319839417934, Final Batch Loss: 0.05718287453055382\n",
      "Epoch 1662, Loss: 0.060303712263703346, Final Batch Loss: 0.024583203718066216\n",
      "Epoch 1663, Loss: 0.06348856911063194, Final Batch Loss: 0.030274465680122375\n",
      "Epoch 1664, Loss: 0.04958842508494854, Final Batch Loss: 0.020413266494870186\n",
      "Epoch 1665, Loss: 0.0519261471927166, Final Batch Loss: 0.017328813672065735\n",
      "Epoch 1666, Loss: 0.05794622376561165, Final Batch Loss: 0.021269161254167557\n",
      "Epoch 1667, Loss: 0.06193390488624573, Final Batch Loss: 0.04207340627908707\n",
      "Epoch 1668, Loss: 0.07675477489829063, Final Batch Loss: 0.05528247356414795\n",
      "Epoch 1669, Loss: 0.07111912220716476, Final Batch Loss: 0.026772070676088333\n",
      "Epoch 1670, Loss: 0.10039069876074791, Final Batch Loss: 0.034831542521715164\n",
      "Epoch 1671, Loss: 0.05679275840520859, Final Batch Loss: 0.019584566354751587\n",
      "Epoch 1672, Loss: 0.05568433180451393, Final Batch Loss: 0.03995365649461746\n",
      "Epoch 1673, Loss: 0.03643819782882929, Final Batch Loss: 0.02644827403128147\n",
      "Epoch 1674, Loss: 0.07909215986728668, Final Batch Loss: 0.04422743618488312\n",
      "Epoch 1675, Loss: 0.06391274556517601, Final Batch Loss: 0.0159950889647007\n",
      "Epoch 1676, Loss: 0.048376371152698994, Final Batch Loss: 0.014826596714556217\n",
      "Epoch 1677, Loss: 0.06839307583868504, Final Batch Loss: 0.04708625003695488\n",
      "Epoch 1678, Loss: 0.04836046323180199, Final Batch Loss: 0.020180903375148773\n",
      "Epoch 1679, Loss: 0.07210495881736279, Final Batch Loss: 0.054021790623664856\n",
      "Epoch 1680, Loss: 0.09667151421308517, Final Batch Loss: 0.049775607883930206\n",
      "Epoch 1681, Loss: 0.05071581806987524, Final Batch Loss: 0.04064454883337021\n",
      "Epoch 1682, Loss: 0.04516114108264446, Final Batch Loss: 0.018961120396852493\n",
      "Epoch 1683, Loss: 0.05864330939948559, Final Batch Loss: 0.0176782738417387\n",
      "Epoch 1684, Loss: 0.0763459112495184, Final Batch Loss: 0.04862840473651886\n",
      "Epoch 1685, Loss: 0.05882096663117409, Final Batch Loss: 0.02517000213265419\n",
      "Epoch 1686, Loss: 0.03844845946878195, Final Batch Loss: 0.009646461345255375\n",
      "Epoch 1687, Loss: 0.052632084116339684, Final Batch Loss: 0.03811015188694\n",
      "Epoch 1688, Loss: 0.0696931704878807, Final Batch Loss: 0.045935921370983124\n",
      "Epoch 1689, Loss: 0.02991589391604066, Final Batch Loss: 0.006528912577778101\n",
      "Epoch 1690, Loss: 0.06785506382584572, Final Batch Loss: 0.03355390951037407\n",
      "Epoch 1691, Loss: 0.06702849455177784, Final Batch Loss: 0.02066938765347004\n",
      "Epoch 1692, Loss: 0.07869567535817623, Final Batch Loss: 0.05102811008691788\n",
      "Epoch 1693, Loss: 0.05136752128601074, Final Batch Loss: 0.01464545726776123\n",
      "Epoch 1694, Loss: 0.048018431290984154, Final Batch Loss: 0.010144354775547981\n",
      "Epoch 1695, Loss: 0.07069481536746025, Final Batch Loss: 0.05030333623290062\n",
      "Epoch 1696, Loss: 0.04910064674913883, Final Batch Loss: 0.025904634967446327\n",
      "Epoch 1697, Loss: 0.04502651281654835, Final Batch Loss: 0.023005014285445213\n",
      "Epoch 1698, Loss: 0.06542958505451679, Final Batch Loss: 0.043462980538606644\n",
      "Epoch 1699, Loss: 0.04560816194862127, Final Batch Loss: 0.03287028521299362\n",
      "Epoch 1700, Loss: 0.046173710376024246, Final Batch Loss: 0.02676728181540966\n",
      "Epoch 1701, Loss: 0.03938789293169975, Final Batch Loss: 0.023023253306746483\n",
      "Epoch 1702, Loss: 0.03820366691797972, Final Batch Loss: 0.008534741587936878\n",
      "Epoch 1703, Loss: 0.02846939116716385, Final Batch Loss: 0.006697574630379677\n",
      "Epoch 1704, Loss: 0.04226638004183769, Final Batch Loss: 0.0323052778840065\n",
      "Epoch 1705, Loss: 0.04514496400952339, Final Batch Loss: 0.026697341352701187\n",
      "Epoch 1706, Loss: 0.04019490396603942, Final Batch Loss: 0.03380846232175827\n",
      "Epoch 1707, Loss: 0.03953689616173506, Final Batch Loss: 0.012560728006064892\n",
      "Epoch 1708, Loss: 0.04835468903183937, Final Batch Loss: 0.01047232374548912\n",
      "Epoch 1709, Loss: 0.04503662511706352, Final Batch Loss: 0.021822823211550713\n",
      "Epoch 1710, Loss: 0.031846011988818645, Final Batch Loss: 0.0101942652836442\n",
      "Epoch 1711, Loss: 0.04244549060240388, Final Batch Loss: 0.0066778273321688175\n",
      "Epoch 1712, Loss: 0.043735112994909286, Final Batch Loss: 0.0037743039429187775\n",
      "Epoch 1713, Loss: 0.08084795251488686, Final Batch Loss: 0.06012246385216713\n",
      "Epoch 1714, Loss: 0.07551857456564903, Final Batch Loss: 0.03393101319670677\n",
      "Epoch 1715, Loss: 0.04179307213053107, Final Batch Loss: 0.03477581962943077\n",
      "Epoch 1716, Loss: 0.04239367973059416, Final Batch Loss: 0.027354490011930466\n",
      "Epoch 1717, Loss: 0.13643916696310043, Final Batch Loss: 0.08196871727705002\n",
      "Epoch 1718, Loss: 0.06343341432511806, Final Batch Loss: 0.051483798772096634\n",
      "Epoch 1719, Loss: 0.039467714726924896, Final Batch Loss: 0.011347457766532898\n",
      "Epoch 1720, Loss: 0.06569106224924326, Final Batch Loss: 0.05827967822551727\n",
      "Epoch 1721, Loss: 0.10331104323267937, Final Batch Loss: 0.08308043330907822\n",
      "Epoch 1722, Loss: 0.05162077583372593, Final Batch Loss: 0.036278728395700455\n",
      "Epoch 1723, Loss: 0.08941926993429661, Final Batch Loss: 0.016040103510022163\n",
      "Epoch 1724, Loss: 0.045581660233438015, Final Batch Loss: 0.01157511305063963\n",
      "Epoch 1725, Loss: 0.08482009172439575, Final Batch Loss: 0.05030251666903496\n",
      "Epoch 1726, Loss: 0.061362938955426216, Final Batch Loss: 0.03427058085799217\n",
      "Epoch 1727, Loss: 0.06577319279313087, Final Batch Loss: 0.018202651292085648\n",
      "Epoch 1728, Loss: 0.07930511049926281, Final Batch Loss: 0.06681498140096664\n",
      "Epoch 1729, Loss: 0.044331179931759834, Final Batch Loss: 0.03855796158313751\n",
      "Epoch 1730, Loss: 0.04955968260765076, Final Batch Loss: 0.01066775992512703\n",
      "Epoch 1731, Loss: 0.05482082813978195, Final Batch Loss: 0.033660389482975006\n",
      "Epoch 1732, Loss: 0.04865701962262392, Final Batch Loss: 0.011111699976027012\n",
      "Epoch 1733, Loss: 0.05967894196510315, Final Batch Loss: 0.01126587763428688\n",
      "Epoch 1734, Loss: 0.042708925902843475, Final Batch Loss: 0.02011299878358841\n",
      "Epoch 1735, Loss: 0.028091217391192913, Final Batch Loss: 0.006914566271007061\n",
      "Epoch 1736, Loss: 0.05175236240029335, Final Batch Loss: 0.02888784557580948\n",
      "Epoch 1737, Loss: 0.05081003252416849, Final Batch Loss: 0.006637130863964558\n",
      "Epoch 1738, Loss: 0.08044325932860374, Final Batch Loss: 0.05492846295237541\n",
      "Epoch 1739, Loss: 0.0681384950876236, Final Batch Loss: 0.02296675741672516\n",
      "Epoch 1740, Loss: 0.0218168324790895, Final Batch Loss: 0.007040238473564386\n",
      "Epoch 1741, Loss: 0.07508021593093872, Final Batch Loss: 0.04590411111712456\n",
      "Epoch 1742, Loss: 0.05617984011769295, Final Batch Loss: 0.03148943558335304\n",
      "Epoch 1743, Loss: 0.044467455707490444, Final Batch Loss: 0.005188084207475185\n",
      "Epoch 1744, Loss: 0.025160963647067547, Final Batch Loss: 0.006271897815167904\n",
      "Epoch 1745, Loss: 0.048515643924474716, Final Batch Loss: 0.027386385947465897\n",
      "Epoch 1746, Loss: 0.04352854564785957, Final Batch Loss: 0.03238007053732872\n",
      "Epoch 1747, Loss: 0.0484127476811409, Final Batch Loss: 0.011052712798118591\n",
      "Epoch 1748, Loss: 0.0668240562081337, Final Batch Loss: 0.03148456662893295\n",
      "Epoch 1749, Loss: 0.034039054065942764, Final Batch Loss: 0.026052316650748253\n",
      "Epoch 1750, Loss: 0.034732130356132984, Final Batch Loss: 0.010476143099367619\n",
      "Epoch 1751, Loss: 0.06178066320717335, Final Batch Loss: 0.038970839232206345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1752, Loss: 0.0372200682759285, Final Batch Loss: 0.016353564336895943\n",
      "Epoch 1753, Loss: 0.06138683669269085, Final Batch Loss: 0.04258570447564125\n",
      "Epoch 1754, Loss: 0.060067327693104744, Final Batch Loss: 0.03466838225722313\n",
      "Epoch 1755, Loss: 0.05559023469686508, Final Batch Loss: 0.025544827803969383\n",
      "Epoch 1756, Loss: 0.041109586134552956, Final Batch Loss: 0.005567880347371101\n",
      "Epoch 1757, Loss: 0.04410933703184128, Final Batch Loss: 0.022265097126364708\n",
      "Epoch 1758, Loss: 0.05568213760852814, Final Batch Loss: 0.0250482689589262\n",
      "Epoch 1759, Loss: 0.03728384803980589, Final Batch Loss: 0.009772346355021\n",
      "Epoch 1760, Loss: 0.03929919749498367, Final Batch Loss: 0.008193006739020348\n",
      "Epoch 1761, Loss: 0.07992121204733849, Final Batch Loss: 0.04836129769682884\n",
      "Epoch 1762, Loss: 0.06785690784454346, Final Batch Loss: 0.0201805979013443\n",
      "Epoch 1763, Loss: 0.059212835505604744, Final Batch Loss: 0.02988392859697342\n",
      "Epoch 1764, Loss: 0.02435934543609619, Final Batch Loss: 0.006620461121201515\n",
      "Epoch 1765, Loss: 0.11397150158882141, Final Batch Loss: 0.06274949014186859\n",
      "Epoch 1766, Loss: 0.06192045984789729, Final Batch Loss: 0.0055296639911830425\n",
      "Epoch 1767, Loss: 0.04825819842517376, Final Batch Loss: 0.019306151196360588\n",
      "Epoch 1768, Loss: 0.07316198293119669, Final Batch Loss: 0.01062034908682108\n",
      "Epoch 1769, Loss: 0.09977041557431221, Final Batch Loss: 0.03828347474336624\n",
      "Epoch 1770, Loss: 0.08686089143157005, Final Batch Loss: 0.04214703291654587\n",
      "Epoch 1771, Loss: 0.061136893928050995, Final Batch Loss: 0.011098101735115051\n",
      "Epoch 1772, Loss: 0.1167245451360941, Final Batch Loss: 0.09308154135942459\n",
      "Epoch 1773, Loss: 0.049718054942786694, Final Batch Loss: 0.01213762816041708\n",
      "Epoch 1774, Loss: 0.09333385527133942, Final Batch Loss: 0.06485156714916229\n",
      "Epoch 1775, Loss: 0.05243114195764065, Final Batch Loss: 0.029410643503069878\n",
      "Epoch 1776, Loss: 0.09732754901051521, Final Batch Loss: 0.06142539530992508\n",
      "Epoch 1777, Loss: 0.05838875472545624, Final Batch Loss: 0.029374098405241966\n",
      "Epoch 1778, Loss: 0.024337454698979855, Final Batch Loss: 0.014051012694835663\n",
      "Epoch 1779, Loss: 0.08499874360859394, Final Batch Loss: 0.005429552868008614\n",
      "Epoch 1780, Loss: 0.09988373890519142, Final Batch Loss: 0.0637814924120903\n",
      "Epoch 1781, Loss: 0.05023525655269623, Final Batch Loss: 0.0314764641225338\n",
      "Epoch 1782, Loss: 0.04765752051025629, Final Batch Loss: 0.0414554588496685\n",
      "Epoch 1783, Loss: 0.049612393602728844, Final Batch Loss: 0.03245015814900398\n",
      "Epoch 1784, Loss: 0.06787290005013347, Final Batch Loss: 0.06214863806962967\n",
      "Epoch 1785, Loss: 0.04505261732265353, Final Batch Loss: 0.0036056148819625378\n",
      "Epoch 1786, Loss: 0.054447053000330925, Final Batch Loss: 0.027863774448633194\n",
      "Epoch 1787, Loss: 0.048814337234944105, Final Batch Loss: 0.005227364134043455\n",
      "Epoch 1788, Loss: 0.041671655140817165, Final Batch Loss: 0.027560705319046974\n",
      "Epoch 1789, Loss: 0.09046711213886738, Final Batch Loss: 0.06362469494342804\n",
      "Epoch 1790, Loss: 0.062414069660007954, Final Batch Loss: 0.04903637617826462\n",
      "Epoch 1791, Loss: 0.05272466689348221, Final Batch Loss: 0.03397884592413902\n",
      "Epoch 1792, Loss: 0.03353874571621418, Final Batch Loss: 0.005439186468720436\n",
      "Epoch 1793, Loss: 0.0520580243319273, Final Batch Loss: 0.014402734115719795\n",
      "Epoch 1794, Loss: 0.030110278632491827, Final Batch Loss: 0.0073825824074447155\n",
      "Epoch 1795, Loss: 0.03311866242438555, Final Batch Loss: 0.022933075204491615\n",
      "Epoch 1796, Loss: 0.036428842693567276, Final Batch Loss: 0.011680610477924347\n",
      "Epoch 1797, Loss: 0.044711166992783546, Final Batch Loss: 0.02805233746767044\n",
      "Epoch 1798, Loss: 0.03938675718382001, Final Batch Loss: 0.03161262348294258\n",
      "Epoch 1799, Loss: 0.030462774448096752, Final Batch Loss: 0.010114415548741817\n",
      "Epoch 1800, Loss: 0.022554592229425907, Final Batch Loss: 0.004813867621123791\n",
      "Epoch 1801, Loss: 0.06388702243566513, Final Batch Loss: 0.009620800614356995\n",
      "Epoch 1802, Loss: 0.05541912652552128, Final Batch Loss: 0.00861814059317112\n",
      "Epoch 1803, Loss: 0.06860826723277569, Final Batch Loss: 0.03909928724169731\n",
      "Epoch 1804, Loss: 0.053348174318671227, Final Batch Loss: 0.029753224924206734\n",
      "Epoch 1805, Loss: 0.09429104486480355, Final Batch Loss: 0.004877223167568445\n",
      "Epoch 1806, Loss: 0.05270310211926699, Final Batch Loss: 0.00934742484241724\n",
      "Epoch 1807, Loss: 0.04456410277634859, Final Batch Loss: 0.03539202734827995\n",
      "Epoch 1808, Loss: 0.04388570040464401, Final Batch Loss: 0.0320182703435421\n",
      "Epoch 1809, Loss: 0.059534864500164986, Final Batch Loss: 0.004754902794957161\n",
      "Epoch 1810, Loss: 0.04897086136043072, Final Batch Loss: 0.007988272234797478\n",
      "Epoch 1811, Loss: 0.029993118718266487, Final Batch Loss: 0.008280621841549873\n",
      "Epoch 1812, Loss: 0.05274875182658434, Final Batch Loss: 0.003159099258482456\n",
      "Epoch 1813, Loss: 0.058666642755270004, Final Batch Loss: 0.0346340648829937\n",
      "Epoch 1814, Loss: 0.033844917081296444, Final Batch Loss: 0.011350422166287899\n",
      "Epoch 1815, Loss: 0.052046055905520916, Final Batch Loss: 0.010459004901349545\n",
      "Epoch 1816, Loss: 0.054804353043437004, Final Batch Loss: 0.029040025547146797\n",
      "Epoch 1817, Loss: 0.037136802449822426, Final Batch Loss: 0.0065552424639463425\n",
      "Epoch 1818, Loss: 0.08592110872268677, Final Batch Loss: 0.041677407920360565\n",
      "Epoch 1819, Loss: 0.0779222697019577, Final Batch Loss: 0.026529330760240555\n",
      "Epoch 1820, Loss: 0.0787251340225339, Final Batch Loss: 0.07268960773944855\n",
      "Epoch 1821, Loss: 0.04178286250680685, Final Batch Loss: 0.011192354373633862\n",
      "Epoch 1822, Loss: 0.0652772206813097, Final Batch Loss: 0.027459191158413887\n",
      "Epoch 1823, Loss: 0.05669587617740035, Final Batch Loss: 0.0069917491637170315\n",
      "Epoch 1824, Loss: 0.04922479949891567, Final Batch Loss: 0.029692893847823143\n",
      "Epoch 1825, Loss: 0.038467773236334324, Final Batch Loss: 0.01030553225427866\n",
      "Epoch 1826, Loss: 0.028647293336689472, Final Batch Loss: 0.012604973278939724\n",
      "Epoch 1827, Loss: 0.04621997848153114, Final Batch Loss: 0.024499136954545975\n",
      "Epoch 1828, Loss: 0.045183999463915825, Final Batch Loss: 0.014356657862663269\n",
      "Epoch 1829, Loss: 0.041624956764280796, Final Batch Loss: 0.032509710639715195\n",
      "Epoch 1830, Loss: 0.07245780155062675, Final Batch Loss: 0.056421779096126556\n",
      "Epoch 1831, Loss: 0.04602577164769173, Final Batch Loss: 0.03234213590621948\n",
      "Epoch 1832, Loss: 0.09092557150870562, Final Batch Loss: 0.0784417986869812\n",
      "Epoch 1833, Loss: 0.08589882403612137, Final Batch Loss: 0.04648637771606445\n",
      "Epoch 1834, Loss: 0.05204200465232134, Final Batch Loss: 0.011422497220337391\n",
      "Epoch 1835, Loss: 0.09320159256458282, Final Batch Loss: 0.06391523033380508\n",
      "Epoch 1836, Loss: 0.0384528380818665, Final Batch Loss: 0.004372869152575731\n",
      "Epoch 1837, Loss: 0.058823857456445694, Final Batch Loss: 0.04099166765809059\n",
      "Epoch 1838, Loss: 0.056301359087228775, Final Batch Loss: 0.025633430108428\n",
      "Epoch 1839, Loss: 0.053561518900096416, Final Batch Loss: 0.041844017803668976\n",
      "Epoch 1840, Loss: 0.05533015355467796, Final Batch Loss: 0.03138785809278488\n",
      "Epoch 1841, Loss: 0.03953544981777668, Final Batch Loss: 0.032651595771312714\n",
      "Epoch 1842, Loss: 0.06824715621769428, Final Batch Loss: 0.029159406200051308\n",
      "Epoch 1843, Loss: 0.056542713195085526, Final Batch Loss: 0.04721160978078842\n",
      "Epoch 1844, Loss: 0.039519691839814186, Final Batch Loss: 0.03188610449433327\n",
      "Epoch 1845, Loss: 0.03213302604854107, Final Batch Loss: 0.01124262623488903\n",
      "Epoch 1846, Loss: 0.05320148356258869, Final Batch Loss: 0.03165218606591225\n",
      "Epoch 1847, Loss: 0.041256826370954514, Final Batch Loss: 0.030924992635846138\n",
      "Epoch 1848, Loss: 0.05187041312456131, Final Batch Loss: 0.03104819357395172\n",
      "Epoch 1849, Loss: 0.09903916344046593, Final Batch Loss: 0.07876154035329819\n",
      "Epoch 1850, Loss: 0.02144959382712841, Final Batch Loss: 0.0036801770329475403\n",
      "Epoch 1851, Loss: 0.037062469869852066, Final Batch Loss: 0.015667103230953217\n",
      "Epoch 1852, Loss: 0.05073119793087244, Final Batch Loss: 0.00869076419621706\n",
      "Epoch 1853, Loss: 0.05836331844329834, Final Batch Loss: 0.0467817485332489\n",
      "Epoch 1854, Loss: 0.04487203620374203, Final Batch Loss: 0.015734581276774406\n",
      "Epoch 1855, Loss: 0.0336923124268651, Final Batch Loss: 0.027150388807058334\n",
      "Epoch 1856, Loss: 0.03228779276832938, Final Batch Loss: 0.025459783151745796\n",
      "Epoch 1857, Loss: 0.06165760010480881, Final Batch Loss: 0.04407700523734093\n",
      "Epoch 1858, Loss: 0.05393650941550732, Final Batch Loss: 0.04545198008418083\n",
      "Epoch 1859, Loss: 0.0483605139888823, Final Batch Loss: 0.007402016315609217\n",
      "Epoch 1860, Loss: 0.047271743416786194, Final Batch Loss: 0.029248200356960297\n",
      "Epoch 1861, Loss: 0.059712888672947884, Final Batch Loss: 0.02216833271086216\n",
      "Epoch 1862, Loss: 0.04471306689083576, Final Batch Loss: 0.00693867914378643\n",
      "Epoch 1863, Loss: 0.03658345155417919, Final Batch Loss: 0.016026338562369347\n",
      "Epoch 1864, Loss: 0.029816947877407074, Final Batch Loss: 0.007602585479617119\n",
      "Epoch 1865, Loss: 0.052015285938978195, Final Batch Loss: 0.04103466868400574\n",
      "Epoch 1866, Loss: 0.03549964865669608, Final Batch Loss: 0.006898838561028242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1867, Loss: 0.03495483659207821, Final Batch Loss: 0.0170762836933136\n",
      "Epoch 1868, Loss: 0.019848445430397987, Final Batch Loss: 0.009256855584681034\n",
      "Epoch 1869, Loss: 0.049964895471930504, Final Batch Loss: 0.024220997467637062\n",
      "Epoch 1870, Loss: 0.022747600451111794, Final Batch Loss: 0.006327364593744278\n",
      "Epoch 1871, Loss: 0.05953889340162277, Final Batch Loss: 0.023057810962200165\n",
      "Epoch 1872, Loss: 0.0270981565117836, Final Batch Loss: 0.012430502101778984\n",
      "Epoch 1873, Loss: 0.030461757443845272, Final Batch Loss: 0.007519620470702648\n",
      "Epoch 1874, Loss: 0.025001160800457, Final Batch Loss: 0.014907355420291424\n",
      "Epoch 1875, Loss: 0.04240529052913189, Final Batch Loss: 0.01323772594332695\n",
      "Epoch 1876, Loss: 0.04313527699559927, Final Batch Loss: 0.005623961798846722\n",
      "Epoch 1877, Loss: 0.06677205674350262, Final Batch Loss: 0.02530858851969242\n",
      "Epoch 1878, Loss: 0.04946324694901705, Final Batch Loss: 0.012237907387316227\n",
      "Epoch 1879, Loss: 0.05171741358935833, Final Batch Loss: 0.02743903174996376\n",
      "Epoch 1880, Loss: 0.024908585473895073, Final Batch Loss: 0.014770044945180416\n",
      "Epoch 1881, Loss: 0.11938261799514294, Final Batch Loss: 0.08870913088321686\n",
      "Epoch 1882, Loss: 0.024676747620105743, Final Batch Loss: 0.004623431712388992\n",
      "Epoch 1883, Loss: 0.07355803437530994, Final Batch Loss: 0.029832718893885612\n",
      "Epoch 1884, Loss: 0.13807808980345726, Final Batch Loss: 0.09564687311649323\n",
      "Epoch 1885, Loss: 0.05837652273476124, Final Batch Loss: 0.03204909339547157\n",
      "Epoch 1886, Loss: 0.056733109056949615, Final Batch Loss: 0.01900649443268776\n",
      "Epoch 1887, Loss: 0.06579557061195374, Final Batch Loss: 0.04218526929616928\n",
      "Epoch 1888, Loss: 0.04922989755868912, Final Batch Loss: 0.024625597521662712\n",
      "Epoch 1889, Loss: 0.06728241220116615, Final Batch Loss: 0.037386633455753326\n",
      "Epoch 1890, Loss: 0.051927048712968826, Final Batch Loss: 0.01630621775984764\n",
      "Epoch 1891, Loss: 0.06659352779388428, Final Batch Loss: 0.048432834446430206\n",
      "Epoch 1892, Loss: 0.036654993891716, Final Batch Loss: 0.016486916691064835\n",
      "Epoch 1893, Loss: 0.049190569669008255, Final Batch Loss: 0.023797770962119102\n",
      "Epoch 1894, Loss: 0.03313314821571112, Final Batch Loss: 0.006986158899962902\n",
      "Epoch 1895, Loss: 0.03609917964786291, Final Batch Loss: 0.010884023271501064\n",
      "Epoch 1896, Loss: 0.06453848630189896, Final Batch Loss: 0.031970180571079254\n",
      "Epoch 1897, Loss: 0.03809969127178192, Final Batch Loss: 0.028864433988928795\n",
      "Epoch 1898, Loss: 0.04849941469728947, Final Batch Loss: 0.019762055948376656\n",
      "Epoch 1899, Loss: 0.037533123046159744, Final Batch Loss: 0.02680610679090023\n",
      "Epoch 1900, Loss: 0.03384871315211058, Final Batch Loss: 0.006170564331114292\n",
      "Epoch 1901, Loss: 0.05209827423095703, Final Batch Loss: 0.0319569855928421\n",
      "Epoch 1902, Loss: 0.04800209030508995, Final Batch Loss: 0.02546779438853264\n",
      "Epoch 1903, Loss: 0.0341524756513536, Final Batch Loss: 0.007496432866901159\n",
      "Epoch 1904, Loss: 0.0343890655785799, Final Batch Loss: 0.011493369936943054\n",
      "Epoch 1905, Loss: 0.05094498302787542, Final Batch Loss: 0.04212939739227295\n",
      "Epoch 1906, Loss: 0.0633529219776392, Final Batch Loss: 0.019841158762574196\n",
      "Epoch 1907, Loss: 0.020279187709093094, Final Batch Loss: 0.008679001592099667\n",
      "Epoch 1908, Loss: 0.040595900267362595, Final Batch Loss: 0.032014328986406326\n",
      "Epoch 1909, Loss: 0.024296621792018414, Final Batch Loss: 0.006254567764699459\n",
      "Epoch 1910, Loss: 0.06040232069790363, Final Batch Loss: 0.03923804685473442\n",
      "Epoch 1911, Loss: 0.0448964387178421, Final Batch Loss: 0.018188098445534706\n",
      "Epoch 1912, Loss: 0.10163683537393808, Final Batch Loss: 0.09527017176151276\n",
      "Epoch 1913, Loss: 0.08243058994412422, Final Batch Loss: 0.05738811939954758\n",
      "Epoch 1914, Loss: 0.07017497718334198, Final Batch Loss: 0.01809317246079445\n",
      "Epoch 1915, Loss: 0.08701438829302788, Final Batch Loss: 0.046134818345308304\n",
      "Epoch 1916, Loss: 0.047170890495181084, Final Batch Loss: 0.018254155293107033\n",
      "Epoch 1917, Loss: 0.0722581036388874, Final Batch Loss: 0.039471451193094254\n",
      "Epoch 1918, Loss: 0.03761044889688492, Final Batch Loss: 0.015941936522722244\n",
      "Epoch 1919, Loss: 0.037307679653167725, Final Batch Loss: 0.01835336536169052\n",
      "Epoch 1920, Loss: 0.04709235206246376, Final Batch Loss: 0.020082440227270126\n",
      "Epoch 1921, Loss: 0.03988350508734584, Final Batch Loss: 0.007706038188189268\n",
      "Epoch 1922, Loss: 0.03621425526216626, Final Batch Loss: 0.004015274811536074\n",
      "Epoch 1923, Loss: 0.03785134945064783, Final Batch Loss: 0.006477040238678455\n",
      "Epoch 1924, Loss: 0.06716089323163033, Final Batch Loss: 0.022297557443380356\n",
      "Epoch 1925, Loss: 0.06960889138281345, Final Batch Loss: 0.05315101891756058\n",
      "Epoch 1926, Loss: 0.05706105101853609, Final Batch Loss: 0.04222370684146881\n",
      "Epoch 1927, Loss: 0.030270115938037634, Final Batch Loss: 0.0042993067763745785\n",
      "Epoch 1928, Loss: 0.048908078111708164, Final Batch Loss: 0.009941046126186848\n",
      "Epoch 1929, Loss: 0.0423618545755744, Final Batch Loss: 0.006741846911609173\n",
      "Epoch 1930, Loss: 0.04133636085316539, Final Batch Loss: 0.004141207318753004\n",
      "Epoch 1931, Loss: 0.05334569467231631, Final Batch Loss: 0.006648197304457426\n",
      "Epoch 1932, Loss: 0.03378791082650423, Final Batch Loss: 0.005231895484030247\n",
      "Epoch 1933, Loss: 0.03831350337713957, Final Batch Loss: 0.008266131393611431\n",
      "Epoch 1934, Loss: 0.03905744384974241, Final Batch Loss: 0.013685538433492184\n",
      "Epoch 1935, Loss: 0.019838443025946617, Final Batch Loss: 0.0070449477061629295\n",
      "Epoch 1936, Loss: 0.0499443244189024, Final Batch Loss: 0.03272407129406929\n",
      "Epoch 1937, Loss: 0.030775321647524834, Final Batch Loss: 0.014408743008971214\n",
      "Epoch 1938, Loss: 0.04198027262464166, Final Batch Loss: 0.03458458557724953\n",
      "Epoch 1939, Loss: 0.034290185663849115, Final Batch Loss: 0.007722339127212763\n",
      "Epoch 1940, Loss: 0.03404519613832235, Final Batch Loss: 0.00811974611133337\n",
      "Epoch 1941, Loss: 0.04880747012794018, Final Batch Loss: 0.018465539440512657\n",
      "Epoch 1942, Loss: 0.0256314342841506, Final Batch Loss: 0.004807651974260807\n",
      "Epoch 1943, Loss: 0.03559071198105812, Final Batch Loss: 0.02352079190313816\n",
      "Epoch 1944, Loss: 0.035317884758114815, Final Batch Loss: 0.01649324595928192\n",
      "Epoch 1945, Loss: 0.03704108763486147, Final Batch Loss: 0.03113141842186451\n",
      "Epoch 1946, Loss: 0.02318239863961935, Final Batch Loss: 0.011235162615776062\n",
      "Epoch 1947, Loss: 0.06032442860305309, Final Batch Loss: 0.04105142131447792\n",
      "Epoch 1948, Loss: 0.019601148087531328, Final Batch Loss: 0.005833212751895189\n",
      "Epoch 1949, Loss: 0.02455376274883747, Final Batch Loss: 0.004957189783453941\n",
      "Epoch 1950, Loss: 0.030130775645375252, Final Batch Loss: 0.010072018951177597\n",
      "Epoch 1951, Loss: 0.032786605414003134, Final Batch Loss: 0.004327063914388418\n",
      "Epoch 1952, Loss: 0.028006721753627062, Final Batch Loss: 0.022361069917678833\n",
      "Epoch 1953, Loss: 0.01730986312031746, Final Batch Loss: 0.006208519451320171\n",
      "Epoch 1954, Loss: 0.02003787737339735, Final Batch Loss: 0.008230475708842278\n",
      "Epoch 1955, Loss: 0.04234792338684201, Final Batch Loss: 0.03621213883161545\n",
      "Epoch 1956, Loss: 0.04464567452669144, Final Batch Loss: 0.026785004884004593\n",
      "Epoch 1957, Loss: 0.03425462171435356, Final Batch Loss: 0.010199295356869698\n",
      "Epoch 1958, Loss: 0.0211960282176733, Final Batch Loss: 0.0059384312480688095\n",
      "Epoch 1959, Loss: 0.029351068660616875, Final Batch Loss: 0.006045334041118622\n",
      "Epoch 1960, Loss: 0.035760088823735714, Final Batch Loss: 0.012291931547224522\n",
      "Epoch 1961, Loss: 0.08475770428776741, Final Batch Loss: 0.05847911164164543\n",
      "Epoch 1962, Loss: 0.05193452909588814, Final Batch Loss: 0.03769974783062935\n",
      "Epoch 1963, Loss: 0.04834266472607851, Final Batch Loss: 0.04234270751476288\n",
      "Epoch 1964, Loss: 0.04933383036404848, Final Batch Loss: 0.008820299990475178\n",
      "Epoch 1965, Loss: 0.027885806746780872, Final Batch Loss: 0.0090855797752738\n",
      "Epoch 1966, Loss: 0.04960248060524464, Final Batch Loss: 0.03323765844106674\n",
      "Epoch 1967, Loss: 0.04280628822743893, Final Batch Loss: 0.018836600705981255\n",
      "Epoch 1968, Loss: 0.04439159017056227, Final Batch Loss: 0.029837053269147873\n",
      "Epoch 1969, Loss: 0.05266088619828224, Final Batch Loss: 0.041809894144535065\n",
      "Epoch 1970, Loss: 0.08670639991760254, Final Batch Loss: 0.06963811069726944\n",
      "Epoch 1971, Loss: 0.07875900343060493, Final Batch Loss: 0.03368235006928444\n",
      "Epoch 1972, Loss: 0.03844426339492202, Final Batch Loss: 0.006715999450534582\n",
      "Epoch 1973, Loss: 0.05145328305661678, Final Batch Loss: 0.023895760998129845\n",
      "Epoch 1974, Loss: 0.052965059876441956, Final Batch Loss: 0.03249280899763107\n",
      "Epoch 1975, Loss: 0.045510982628911734, Final Batch Loss: 0.0047882250510156155\n",
      "Epoch 1976, Loss: 0.023180725052952766, Final Batch Loss: 0.004490282386541367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1977, Loss: 0.08021381963044405, Final Batch Loss: 0.014874850399792194\n",
      "Epoch 1978, Loss: 0.03053849469870329, Final Batch Loss: 0.010325002484023571\n",
      "Epoch 1979, Loss: 0.040579311549663544, Final Batch Loss: 0.02793595753610134\n",
      "Epoch 1980, Loss: 0.040614654775708914, Final Batch Loss: 0.007310987915843725\n",
      "Epoch 1981, Loss: 0.03915648069232702, Final Batch Loss: 0.009781465865671635\n",
      "Epoch 1982, Loss: 0.05525708105415106, Final Batch Loss: 0.01450913306325674\n",
      "Epoch 1983, Loss: 0.0511859729886055, Final Batch Loss: 0.011493749916553497\n",
      "Epoch 1984, Loss: 0.0340972407720983, Final Batch Loss: 0.02725793980062008\n",
      "Epoch 1985, Loss: 0.06168397329747677, Final Batch Loss: 0.050544485449790955\n",
      "Epoch 1986, Loss: 0.02725206851027906, Final Batch Loss: 0.003573385765776038\n",
      "Epoch 1987, Loss: 0.025269598700106144, Final Batch Loss: 0.011464385315775871\n",
      "Epoch 1988, Loss: 0.048948854207992554, Final Batch Loss: 0.03163052722811699\n",
      "Epoch 1989, Loss: 0.04741052957251668, Final Batch Loss: 0.04040301963686943\n",
      "Epoch 1990, Loss: 0.026359397917985916, Final Batch Loss: 0.008894464001059532\n",
      "Epoch 1991, Loss: 0.0491260914131999, Final Batch Loss: 0.034944791346788406\n",
      "Epoch 1992, Loss: 0.06968998536467552, Final Batch Loss: 0.038326334208250046\n",
      "Epoch 1993, Loss: 0.07674174010753632, Final Batch Loss: 0.058094363659620285\n",
      "Epoch 1994, Loss: 0.024004315491765738, Final Batch Loss: 0.006860233377665281\n",
      "Epoch 1995, Loss: 0.049671048298478127, Final Batch Loss: 0.0168654453009367\n",
      "Epoch 1996, Loss: 0.1316887717694044, Final Batch Loss: 0.10791368037462234\n",
      "Epoch 1997, Loss: 0.050527969375252724, Final Batch Loss: 0.01268857903778553\n",
      "Epoch 1998, Loss: 0.05008075386285782, Final Batch Loss: 0.0277386661618948\n",
      "Epoch 1999, Loss: 0.032614522613584995, Final Batch Loss: 0.012187645770609379\n",
      "Epoch 2000, Loss: 0.03637984115630388, Final Batch Loss: 0.023173965513706207\n",
      "Epoch 2001, Loss: 0.04630513396114111, Final Batch Loss: 0.013003363274037838\n",
      "Epoch 2002, Loss: 0.041369060054421425, Final Batch Loss: 0.017749719321727753\n",
      "Epoch 2003, Loss: 0.048771792091429234, Final Batch Loss: 0.03542746976017952\n",
      "Epoch 2004, Loss: 0.0415768176317215, Final Batch Loss: 0.02827855758368969\n",
      "Epoch 2005, Loss: 0.044161705300211906, Final Batch Loss: 0.032533492892980576\n",
      "Epoch 2006, Loss: 0.041243502870202065, Final Batch Loss: 0.014991704374551773\n",
      "Epoch 2007, Loss: 0.06669309735298157, Final Batch Loss: 0.032435234636068344\n",
      "Epoch 2008, Loss: 0.045724788680672646, Final Batch Loss: 0.02361561357975006\n",
      "Epoch 2009, Loss: 0.08274819329380989, Final Batch Loss: 0.03483745828270912\n",
      "Epoch 2010, Loss: 0.0608502421528101, Final Batch Loss: 0.028583047911524773\n",
      "Epoch 2011, Loss: 0.029952414333820343, Final Batch Loss: 0.010568123310804367\n",
      "Epoch 2012, Loss: 0.03829884296283126, Final Batch Loss: 0.03135927766561508\n",
      "Epoch 2013, Loss: 0.03991161286830902, Final Batch Loss: 0.01878899335861206\n",
      "Epoch 2014, Loss: 0.035772904753685, Final Batch Loss: 0.018284616991877556\n",
      "Epoch 2015, Loss: 0.05316145904362202, Final Batch Loss: 0.02600427344441414\n",
      "Epoch 2016, Loss: 0.031225997023284435, Final Batch Loss: 0.006809216924011707\n",
      "Epoch 2017, Loss: 0.05080925300717354, Final Batch Loss: 0.01717406138777733\n",
      "Epoch 2018, Loss: 0.023678711615502834, Final Batch Loss: 0.00544400792568922\n",
      "Epoch 2019, Loss: 0.04820607416331768, Final Batch Loss: 0.020982926711440086\n",
      "Epoch 2020, Loss: 0.05957369692623615, Final Batch Loss: 0.024429211392998695\n",
      "Epoch 2021, Loss: 0.05050269141793251, Final Batch Loss: 0.02034485712647438\n",
      "Epoch 2022, Loss: 0.03453414887189865, Final Batch Loss: 0.018771816045045853\n",
      "Epoch 2023, Loss: 0.05907353758811951, Final Batch Loss: 0.03291554003953934\n",
      "Epoch 2024, Loss: 0.024074799846857786, Final Batch Loss: 0.006521343719214201\n",
      "Epoch 2025, Loss: 0.056529074907302856, Final Batch Loss: 0.017109118402004242\n",
      "Epoch 2026, Loss: 0.022446474991738796, Final Batch Loss: 0.00912543386220932\n",
      "Epoch 2027, Loss: 0.03728126361966133, Final Batch Loss: 0.009656939655542374\n",
      "Epoch 2028, Loss: 0.05329547077417374, Final Batch Loss: 0.030830221250653267\n",
      "Epoch 2029, Loss: 0.07706639543175697, Final Batch Loss: 0.03831995278596878\n",
      "Epoch 2030, Loss: 0.039346424862742424, Final Batch Loss: 0.01617770828306675\n",
      "Epoch 2031, Loss: 0.03234418295323849, Final Batch Loss: 0.008402926847338676\n",
      "Epoch 2032, Loss: 0.024904287420213223, Final Batch Loss: 0.006934537552297115\n",
      "Epoch 2033, Loss: 0.04127120366320014, Final Batch Loss: 0.03521478921175003\n",
      "Epoch 2034, Loss: 0.027932821540161967, Final Batch Loss: 0.0021934795659035444\n",
      "Epoch 2035, Loss: 0.02895977720618248, Final Batch Loss: 0.011979226022958755\n",
      "Epoch 2036, Loss: 0.05133645422756672, Final Batch Loss: 0.031770333647727966\n",
      "Epoch 2037, Loss: 0.07476611249148846, Final Batch Loss: 0.06238270178437233\n",
      "Epoch 2038, Loss: 0.10446699149906635, Final Batch Loss: 0.10000596195459366\n",
      "Epoch 2039, Loss: 0.07397763151675463, Final Batch Loss: 0.008517888374626637\n",
      "Epoch 2040, Loss: 0.026486011687666178, Final Batch Loss: 0.006440442521125078\n",
      "Epoch 2041, Loss: 0.03971830359660089, Final Batch Loss: 0.0032945943530648947\n",
      "Epoch 2042, Loss: 0.05148744583129883, Final Batch Loss: 0.015480753034353256\n",
      "Epoch 2043, Loss: 0.019053245428949594, Final Batch Loss: 0.005560650024563074\n",
      "Epoch 2044, Loss: 0.06198870949447155, Final Batch Loss: 0.017145270481705666\n",
      "Epoch 2045, Loss: 0.030574332922697067, Final Batch Loss: 0.005479058250784874\n",
      "Epoch 2046, Loss: 0.06069664657115936, Final Batch Loss: 0.043043531477451324\n",
      "Epoch 2047, Loss: 0.04464717349037528, Final Batch Loss: 0.005000346805900335\n",
      "Epoch 2048, Loss: 0.04717925935983658, Final Batch Loss: 0.031669534742832184\n",
      "Epoch 2049, Loss: 0.039469894021749496, Final Batch Loss: 0.03166211023926735\n",
      "Epoch 2050, Loss: 0.030561393592506647, Final Batch Loss: 0.006488426122814417\n",
      "Epoch 2051, Loss: 0.035711552016437054, Final Batch Loss: 0.01107755210250616\n",
      "Epoch 2052, Loss: 0.04571603611111641, Final Batch Loss: 0.02932504564523697\n",
      "Epoch 2053, Loss: 0.09786874428391457, Final Batch Loss: 0.07407435774803162\n",
      "Epoch 2054, Loss: 0.08319107629358768, Final Batch Loss: 0.029302960261702538\n",
      "Epoch 2055, Loss: 0.059030331671237946, Final Batch Loss: 0.02506449818611145\n",
      "Epoch 2056, Loss: 0.027284914627671242, Final Batch Loss: 0.008503202348947525\n",
      "Epoch 2057, Loss: 0.09177193976938725, Final Batch Loss: 0.0306126419454813\n",
      "Epoch 2058, Loss: 0.04563025012612343, Final Batch Loss: 0.019981591030955315\n",
      "Epoch 2059, Loss: 0.07078010868281126, Final Batch Loss: 0.05541136488318443\n",
      "Epoch 2060, Loss: 0.040703131817281246, Final Batch Loss: 0.03284355625510216\n",
      "Epoch 2061, Loss: 0.033114612102508545, Final Batch Loss: 0.024047093465924263\n",
      "Epoch 2062, Loss: 0.048351291101425886, Final Batch Loss: 0.006762310396879911\n",
      "Epoch 2063, Loss: 0.03882661089301109, Final Batch Loss: 0.019485294818878174\n",
      "Epoch 2064, Loss: 0.08744574803858995, Final Batch Loss: 0.08196456730365753\n",
      "Epoch 2065, Loss: 0.05926076602190733, Final Batch Loss: 0.009835894219577312\n",
      "Epoch 2066, Loss: 0.055538692977279425, Final Batch Loss: 0.04930434748530388\n",
      "Epoch 2067, Loss: 0.03790625906549394, Final Batch Loss: 0.003101136302575469\n",
      "Epoch 2068, Loss: 0.019548964221030474, Final Batch Loss: 0.00333723658695817\n",
      "Epoch 2069, Loss: 0.0587602723389864, Final Batch Loss: 0.03155705705285072\n",
      "Epoch 2070, Loss: 0.04811939503997564, Final Batch Loss: 0.01229590829461813\n",
      "Epoch 2071, Loss: 0.04371554683893919, Final Batch Loss: 0.03536827489733696\n",
      "Epoch 2072, Loss: 0.036139171570539474, Final Batch Loss: 0.025299452245235443\n",
      "Epoch 2073, Loss: 0.11639015004038811, Final Batch Loss: 0.09378691762685776\n",
      "Epoch 2074, Loss: 0.06500586681067944, Final Batch Loss: 0.036995161324739456\n",
      "Epoch 2075, Loss: 0.02563605085015297, Final Batch Loss: 0.020986832678318024\n",
      "Epoch 2076, Loss: 0.07293026335537434, Final Batch Loss: 0.019082752987742424\n",
      "Epoch 2077, Loss: 0.04383050836622715, Final Batch Loss: 0.02600046992301941\n",
      "Epoch 2078, Loss: 0.023859252221882343, Final Batch Loss: 0.00793569814413786\n",
      "Epoch 2079, Loss: 0.027782685123384, Final Batch Loss: 0.00863224733620882\n",
      "Epoch 2080, Loss: 0.06749991327524185, Final Batch Loss: 0.050123367458581924\n",
      "Epoch 2081, Loss: 0.03507838398218155, Final Batch Loss: 0.021502433344721794\n",
      "Epoch 2082, Loss: 0.05082496628165245, Final Batch Loss: 0.018305055797100067\n",
      "Epoch 2083, Loss: 0.04022243060171604, Final Batch Loss: 0.02102089300751686\n",
      "Epoch 2084, Loss: 0.04015943966805935, Final Batch Loss: 0.028962470591068268\n",
      "Epoch 2085, Loss: 0.041683191899210215, Final Batch Loss: 0.03403345122933388\n",
      "Epoch 2086, Loss: 0.04296611389145255, Final Batch Loss: 0.037134863436222076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2087, Loss: 0.06614017114043236, Final Batch Loss: 0.024381164461374283\n",
      "Epoch 2088, Loss: 0.062203334644436836, Final Batch Loss: 0.028518272563815117\n",
      "Epoch 2089, Loss: 0.057624596171081066, Final Batch Loss: 0.010415642522275448\n",
      "Epoch 2090, Loss: 0.0543795358389616, Final Batch Loss: 0.0448584221303463\n",
      "Epoch 2091, Loss: 0.04697459191083908, Final Batch Loss: 0.028017982840538025\n",
      "Epoch 2092, Loss: 0.06706855073571205, Final Batch Loss: 0.043154358863830566\n",
      "Epoch 2093, Loss: 0.03252876363694668, Final Batch Loss: 0.022125273942947388\n",
      "Epoch 2094, Loss: 0.06663986667990685, Final Batch Loss: 0.02650703862309456\n",
      "Epoch 2095, Loss: 0.024169422686100006, Final Batch Loss: 0.010413686744868755\n",
      "Epoch 2096, Loss: 0.07928019016981125, Final Batch Loss: 0.036440376192331314\n",
      "Epoch 2097, Loss: 0.06056543439626694, Final Batch Loss: 0.028742581605911255\n",
      "Epoch 2098, Loss: 0.020875880494713783, Final Batch Loss: 0.005764276720583439\n",
      "Epoch 2099, Loss: 0.06354149430990219, Final Batch Loss: 0.027183659374713898\n",
      "Epoch 2100, Loss: 0.035246373154222965, Final Batch Loss: 0.024341078475117683\n",
      "Epoch 2101, Loss: 0.028573532588779926, Final Batch Loss: 0.011943246237933636\n",
      "Epoch 2102, Loss: 0.031253453344106674, Final Batch Loss: 0.012796675786376\n",
      "Epoch 2103, Loss: 0.02416947577148676, Final Batch Loss: 0.007066239602863789\n",
      "Epoch 2104, Loss: 0.03340001171454787, Final Batch Loss: 0.006767266895622015\n",
      "Epoch 2105, Loss: 0.05588249862194061, Final Batch Loss: 0.020265158265829086\n",
      "Epoch 2106, Loss: 0.03473605401813984, Final Batch Loss: 0.009336158633232117\n",
      "Epoch 2107, Loss: 0.05092677287757397, Final Batch Loss: 0.026122067123651505\n",
      "Epoch 2108, Loss: 0.024459330830723047, Final Batch Loss: 0.007144254166632891\n",
      "Epoch 2109, Loss: 0.06200375873595476, Final Batch Loss: 0.012634132988750935\n",
      "Epoch 2110, Loss: 0.02658422151580453, Final Batch Loss: 0.0026588630862534046\n",
      "Epoch 2111, Loss: 0.03237204346805811, Final Batch Loss: 0.00452540535479784\n",
      "Epoch 2112, Loss: 0.09015808999538422, Final Batch Loss: 0.0433187410235405\n",
      "Epoch 2113, Loss: 0.0558161623775959, Final Batch Loss: 0.015116222202777863\n",
      "Epoch 2114, Loss: 0.03193857241421938, Final Batch Loss: 0.011880277656018734\n",
      "Epoch 2115, Loss: 0.035215877927839756, Final Batch Loss: 0.0310649611055851\n",
      "Epoch 2116, Loss: 0.04797751363366842, Final Batch Loss: 0.032600171864032745\n",
      "Epoch 2117, Loss: 0.04329652711749077, Final Batch Loss: 0.015796272084116936\n",
      "Epoch 2118, Loss: 0.027644944842904806, Final Batch Loss: 0.007027385290712118\n",
      "Epoch 2119, Loss: 0.05119498632848263, Final Batch Loss: 0.029497940093278885\n",
      "Epoch 2120, Loss: 0.08746074140071869, Final Batch Loss: 0.03922190144658089\n",
      "Epoch 2121, Loss: 0.07934010215103626, Final Batch Loss: 0.06892796605825424\n",
      "Epoch 2122, Loss: 0.03845492284744978, Final Batch Loss: 0.025894980877637863\n",
      "Epoch 2123, Loss: 0.050434705801308155, Final Batch Loss: 0.005264812149107456\n",
      "Epoch 2124, Loss: 0.03269055858254433, Final Batch Loss: 0.006305567920207977\n",
      "Epoch 2125, Loss: 0.03939751069992781, Final Batch Loss: 0.008612020872533321\n",
      "Epoch 2126, Loss: 0.044042615219950676, Final Batch Loss: 0.026260707527399063\n",
      "Epoch 2127, Loss: 0.029414139222353697, Final Batch Loss: 0.005070473533123732\n",
      "Epoch 2128, Loss: 0.028249479830265045, Final Batch Loss: 0.010254617780447006\n",
      "Epoch 2129, Loss: 0.04482015408575535, Final Batch Loss: 0.009114360436797142\n",
      "Epoch 2130, Loss: 0.07355113700032234, Final Batch Loss: 0.03903752192854881\n",
      "Epoch 2131, Loss: 0.044267090037465096, Final Batch Loss: 0.011240968480706215\n",
      "Epoch 2132, Loss: 0.04175896476954222, Final Batch Loss: 0.0284830741584301\n",
      "Epoch 2133, Loss: 0.04756937921047211, Final Batch Loss: 0.030837249010801315\n",
      "Epoch 2134, Loss: 0.03193202521651983, Final Batch Loss: 0.024062907323241234\n",
      "Epoch 2135, Loss: 0.035966960713267326, Final Batch Loss: 0.02582014910876751\n",
      "Epoch 2136, Loss: 0.06329221650958061, Final Batch Loss: 0.03530377149581909\n",
      "Epoch 2137, Loss: 0.04314074665307999, Final Batch Loss: 0.02435462735593319\n",
      "Epoch 2138, Loss: 0.03168188966810703, Final Batch Loss: 0.023287450894713402\n",
      "Epoch 2139, Loss: 0.01726717664860189, Final Batch Loss: 0.0038408038672059774\n",
      "Epoch 2140, Loss: 0.05004444159567356, Final Batch Loss: 0.013830611482262611\n",
      "Epoch 2141, Loss: 0.03395563643425703, Final Batch Loss: 0.008352353237569332\n",
      "Epoch 2142, Loss: 0.052005138248205185, Final Batch Loss: 0.011529751121997833\n",
      "Epoch 2143, Loss: 0.07810121402144432, Final Batch Loss: 0.036691468209028244\n",
      "Epoch 2144, Loss: 0.03850577585399151, Final Batch Loss: 0.02821432799100876\n",
      "Epoch 2145, Loss: 0.03737474884837866, Final Batch Loss: 0.028063079342246056\n",
      "Epoch 2146, Loss: 0.0175546295940876, Final Batch Loss: 0.006081688217818737\n",
      "Epoch 2147, Loss: 0.03335513034835458, Final Batch Loss: 0.028263317421078682\n",
      "Epoch 2148, Loss: 0.043570270761847496, Final Batch Loss: 0.012168971821665764\n",
      "Epoch 2149, Loss: 0.06419077329337597, Final Batch Loss: 0.027011847123503685\n",
      "Epoch 2150, Loss: 0.03406450618058443, Final Batch Loss: 0.023189226165413857\n",
      "Epoch 2151, Loss: 0.04182642977684736, Final Batch Loss: 0.03213343024253845\n",
      "Epoch 2152, Loss: 0.024309515487402678, Final Batch Loss: 0.004233093466609716\n",
      "Epoch 2153, Loss: 0.032724746502935886, Final Batch Loss: 0.02207648567855358\n",
      "Epoch 2154, Loss: 0.023981162812560797, Final Batch Loss: 0.006251140963286161\n",
      "Epoch 2155, Loss: 0.014412952121347189, Final Batch Loss: 0.005960602778941393\n",
      "Epoch 2156, Loss: 0.03128512483090162, Final Batch Loss: 0.02225683256983757\n",
      "Epoch 2157, Loss: 0.06471239402890205, Final Batch Loss: 0.05569989234209061\n",
      "Epoch 2158, Loss: 0.05780986696481705, Final Batch Loss: 0.024300992488861084\n",
      "Epoch 2159, Loss: 0.01684595411643386, Final Batch Loss: 0.004731982480734587\n",
      "Epoch 2160, Loss: 0.05049159564077854, Final Batch Loss: 0.029747018590569496\n",
      "Epoch 2161, Loss: 0.02936775516718626, Final Batch Loss: 0.014309437945485115\n",
      "Epoch 2162, Loss: 0.02260106708854437, Final Batch Loss: 0.00789494626224041\n",
      "Epoch 2163, Loss: 0.0463010985404253, Final Batch Loss: 0.021461784839630127\n",
      "Epoch 2164, Loss: 0.0169487576931715, Final Batch Loss: 0.003004145808517933\n",
      "Epoch 2165, Loss: 0.025699300225824118, Final Batch Loss: 0.007406927179545164\n",
      "Epoch 2166, Loss: 0.02233113069087267, Final Batch Loss: 0.011729535646736622\n",
      "Epoch 2167, Loss: 0.05017485097050667, Final Batch Loss: 0.03977302089333534\n",
      "Epoch 2168, Loss: 0.030053093098104, Final Batch Loss: 0.010087315924465656\n",
      "Epoch 2169, Loss: 0.04150598403066397, Final Batch Loss: 0.035201456397771835\n",
      "Epoch 2170, Loss: 0.05616781674325466, Final Batch Loss: 0.04464663565158844\n",
      "Epoch 2171, Loss: 0.016878336668014526, Final Batch Loss: 0.012019478715956211\n",
      "Epoch 2172, Loss: 0.09786200523376465, Final Batch Loss: 0.05167660489678383\n",
      "Epoch 2173, Loss: 0.024270903319120407, Final Batch Loss: 0.008868093602359295\n",
      "Epoch 2174, Loss: 0.028863495215773582, Final Batch Loss: 0.012189432978630066\n",
      "Epoch 2175, Loss: 0.025608244352042675, Final Batch Loss: 0.009483185596764088\n",
      "Epoch 2176, Loss: 0.045389228500425816, Final Batch Loss: 0.013480775989592075\n",
      "Epoch 2177, Loss: 0.03652426414191723, Final Batch Loss: 0.019435474649071693\n",
      "Epoch 2178, Loss: 0.06734986789524555, Final Batch Loss: 0.04710802435874939\n",
      "Epoch 2179, Loss: 0.031071101315319538, Final Batch Loss: 0.010249302722513676\n",
      "Epoch 2180, Loss: 0.05044109374284744, Final Batch Loss: 0.028317218646407127\n",
      "Epoch 2181, Loss: 0.054570749402046204, Final Batch Loss: 0.03563454747200012\n",
      "Epoch 2182, Loss: 0.07482962124049664, Final Batch Loss: 0.048391278833150864\n",
      "Epoch 2183, Loss: 0.03521072305738926, Final Batch Loss: 0.01665760576725006\n",
      "Epoch 2184, Loss: 0.030462218448519707, Final Batch Loss: 0.024556390941143036\n",
      "Epoch 2185, Loss: 0.04221713915467262, Final Batch Loss: 0.013758666813373566\n",
      "Epoch 2186, Loss: 0.01001471234485507, Final Batch Loss: 0.0051261624321341515\n",
      "Epoch 2187, Loss: 0.05304441973567009, Final Batch Loss: 0.03950496017932892\n",
      "Epoch 2188, Loss: 0.039411217905581, Final Batch Loss: 0.010012869723141193\n",
      "Epoch 2189, Loss: 0.053273389115929604, Final Batch Loss: 0.00384490005671978\n",
      "Epoch 2190, Loss: 0.027737512718886137, Final Batch Loss: 0.004845110233873129\n",
      "Epoch 2191, Loss: 0.04618408787064254, Final Batch Loss: 0.0033388647716492414\n",
      "Epoch 2192, Loss: 0.08329369686543941, Final Batch Loss: 0.07500303536653519\n",
      "Epoch 2193, Loss: 0.01618179166689515, Final Batch Loss: 0.005792437586933374\n",
      "Epoch 2194, Loss: 0.07009344920516014, Final Batch Loss: 0.05323367938399315\n",
      "Epoch 2195, Loss: 0.056792682968080044, Final Batch Loss: 0.05093814432621002\n",
      "Epoch 2196, Loss: 0.02570658503100276, Final Batch Loss: 0.0052828858606517315\n",
      "Epoch 2197, Loss: 0.013629668857902288, Final Batch Loss: 0.003978668246418238\n",
      "Epoch 2198, Loss: 0.021262962371110916, Final Batch Loss: 0.016928594559431076\n",
      "Epoch 2199, Loss: 0.01701563550159335, Final Batch Loss: 0.007153371814638376\n",
      "Epoch 2200, Loss: 0.03719688672572374, Final Batch Loss: 0.024837521836161613\n",
      "Epoch 2201, Loss: 0.029095537029206753, Final Batch Loss: 0.008344451896846294\n",
      "Epoch 2202, Loss: 0.03618637379258871, Final Batch Loss: 0.02084454707801342\n",
      "Epoch 2203, Loss: 0.07873765379190445, Final Batch Loss: 0.040343232452869415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2204, Loss: 0.04092888534069061, Final Batch Loss: 0.030359044671058655\n",
      "Epoch 2205, Loss: 0.061603160575032234, Final Batch Loss: 0.0106657724827528\n",
      "Epoch 2206, Loss: 0.14329324662685394, Final Batch Loss: 0.12102160602807999\n",
      "Epoch 2207, Loss: 0.042469062842428684, Final Batch Loss: 0.012003167532384396\n",
      "Epoch 2208, Loss: 0.024099901784211397, Final Batch Loss: 0.006802827585488558\n",
      "Epoch 2209, Loss: 0.05650019459426403, Final Batch Loss: 0.02983335219323635\n",
      "Epoch 2210, Loss: 0.06427390314638615, Final Batch Loss: 0.05501988157629967\n",
      "Epoch 2211, Loss: 0.06550300307571888, Final Batch Loss: 0.04596428573131561\n",
      "Epoch 2212, Loss: 0.03072806214913726, Final Batch Loss: 0.003956179600208998\n",
      "Epoch 2213, Loss: 0.07837885990738869, Final Batch Loss: 0.05055033788084984\n",
      "Epoch 2214, Loss: 0.05737245827913284, Final Batch Loss: 0.03530597686767578\n",
      "Epoch 2215, Loss: 0.05105423182249069, Final Batch Loss: 0.029498184099793434\n",
      "Epoch 2216, Loss: 0.06782381981611252, Final Batch Loss: 0.03633023798465729\n",
      "Epoch 2217, Loss: 0.09047100320458412, Final Batch Loss: 0.034521378576755524\n",
      "Epoch 2218, Loss: 0.03813170874491334, Final Batch Loss: 0.03194798529148102\n",
      "Epoch 2219, Loss: 0.039931872859597206, Final Batch Loss: 0.016712607815861702\n",
      "Epoch 2220, Loss: 0.021346817258745432, Final Batch Loss: 0.004760556388646364\n",
      "Epoch 2221, Loss: 0.057920102030038834, Final Batch Loss: 0.026789579540491104\n",
      "Epoch 2222, Loss: 0.01888676593080163, Final Batch Loss: 0.012611622922122478\n",
      "Epoch 2223, Loss: 0.03445803187787533, Final Batch Loss: 0.024701304733753204\n",
      "Epoch 2224, Loss: 0.03485347889363766, Final Batch Loss: 0.026683945208787918\n",
      "Epoch 2225, Loss: 0.053564393892884254, Final Batch Loss: 0.039664026349782944\n",
      "Epoch 2226, Loss: 0.06303481385111809, Final Batch Loss: 0.022481173276901245\n",
      "Epoch 2227, Loss: 0.05570671893656254, Final Batch Loss: 0.03689582645893097\n",
      "Epoch 2228, Loss: 0.03810578212141991, Final Batch Loss: 0.011881360784173012\n",
      "Epoch 2229, Loss: 0.0663755415007472, Final Batch Loss: 0.05105108022689819\n",
      "Epoch 2230, Loss: 0.026753924787044525, Final Batch Loss: 0.004573220387101173\n",
      "Epoch 2231, Loss: 0.03299361024983227, Final Batch Loss: 0.02996930107474327\n",
      "Epoch 2232, Loss: 0.05456216260790825, Final Batch Loss: 0.02854021079838276\n",
      "Epoch 2233, Loss: 0.03085137065500021, Final Batch Loss: 0.00456498097628355\n",
      "Epoch 2234, Loss: 0.050158397760242224, Final Batch Loss: 0.0438581146299839\n",
      "Epoch 2235, Loss: 0.036266171373426914, Final Batch Loss: 0.00804471131414175\n",
      "Epoch 2236, Loss: 0.036062571220099926, Final Batch Loss: 0.02760125696659088\n",
      "Epoch 2237, Loss: 0.02725166827440262, Final Batch Loss: 0.009211903437972069\n",
      "Epoch 2238, Loss: 0.024539452977478504, Final Batch Loss: 0.004996263422071934\n",
      "Epoch 2239, Loss: 0.016221572179347277, Final Batch Loss: 0.005250467453151941\n",
      "Epoch 2240, Loss: 0.03754575923085213, Final Batch Loss: 0.03199612349271774\n",
      "Epoch 2241, Loss: 0.018201326485723257, Final Batch Loss: 0.004132346715778112\n",
      "Epoch 2242, Loss: 0.09128229692578316, Final Batch Loss: 0.059815119951963425\n",
      "Epoch 2243, Loss: 0.029364929534494877, Final Batch Loss: 0.010884485207498074\n",
      "Epoch 2244, Loss: 0.04053059685975313, Final Batch Loss: 0.026371674612164497\n",
      "Epoch 2245, Loss: 0.05977647379040718, Final Batch Loss: 0.02920765057206154\n",
      "Epoch 2246, Loss: 0.046692002564668655, Final Batch Loss: 0.013908613473176956\n",
      "Epoch 2247, Loss: 0.06784466188400984, Final Batch Loss: 0.05841967463493347\n",
      "Epoch 2248, Loss: 0.04566050507128239, Final Batch Loss: 0.02068396657705307\n",
      "Epoch 2249, Loss: 0.07660527527332306, Final Batch Loss: 0.03896681219339371\n",
      "Epoch 2250, Loss: 0.02268283162266016, Final Batch Loss: 0.01645287126302719\n",
      "Epoch 2251, Loss: 0.052029069513082504, Final Batch Loss: 0.04161243885755539\n",
      "Epoch 2252, Loss: 0.04063175432384014, Final Batch Loss: 0.012019330635666847\n",
      "Epoch 2253, Loss: 0.060767827555537224, Final Batch Loss: 0.03924630582332611\n",
      "Epoch 2254, Loss: 0.1254090666770935, Final Batch Loss: 0.08849825710058212\n",
      "Epoch 2255, Loss: 0.04255559016019106, Final Batch Loss: 0.004889638163149357\n",
      "Epoch 2256, Loss: 0.05516020022332668, Final Batch Loss: 0.03850170969963074\n",
      "Epoch 2257, Loss: 0.06452410481870174, Final Batch Loss: 0.035279806703329086\n",
      "Epoch 2258, Loss: 0.10500416159629822, Final Batch Loss: 0.07924704253673553\n",
      "Epoch 2259, Loss: 0.018554892390966415, Final Batch Loss: 0.013223499990999699\n",
      "Epoch 2260, Loss: 0.056076258420944214, Final Batch Loss: 0.01849300041794777\n",
      "Epoch 2261, Loss: 0.02069413661956787, Final Batch Loss: 0.009127447381615639\n",
      "Epoch 2262, Loss: 0.07987605687230825, Final Batch Loss: 0.070247121155262\n",
      "Epoch 2263, Loss: 0.04899247922003269, Final Batch Loss: 0.011619234457612038\n",
      "Epoch 2264, Loss: 0.0610943753272295, Final Batch Loss: 0.0445118322968483\n",
      "Epoch 2265, Loss: 0.035060792695730925, Final Batch Loss: 0.005696678068488836\n",
      "Epoch 2266, Loss: 0.12558810412883759, Final Batch Loss: 0.04839450120925903\n",
      "Epoch 2267, Loss: 0.053931690752506256, Final Batch Loss: 0.017117779701948166\n",
      "Epoch 2268, Loss: 0.07133998814970255, Final Batch Loss: 0.061068035662174225\n",
      "Epoch 2269, Loss: 0.03494929522275925, Final Batch Loss: 0.026098722591996193\n",
      "Epoch 2270, Loss: 0.025425240397453308, Final Batch Loss: 0.005125204101204872\n",
      "Epoch 2271, Loss: 0.029378649778664112, Final Batch Loss: 0.011305133812129498\n",
      "Epoch 2272, Loss: 0.05846559628844261, Final Batch Loss: 0.019025836139917374\n",
      "Epoch 2273, Loss: 0.0561553668230772, Final Batch Loss: 0.006089219823479652\n",
      "Epoch 2274, Loss: 0.01985451765358448, Final Batch Loss: 0.0049068378284573555\n",
      "Epoch 2275, Loss: 0.05348954163491726, Final Batch Loss: 0.020421432331204414\n",
      "Epoch 2276, Loss: 0.0889885313808918, Final Batch Loss: 0.04443594813346863\n",
      "Epoch 2277, Loss: 0.044583845883607864, Final Batch Loss: 0.021559840068221092\n",
      "Epoch 2278, Loss: 0.057115938514471054, Final Batch Loss: 0.03495640307664871\n",
      "Epoch 2279, Loss: 0.04460516106337309, Final Batch Loss: 0.012364028953015804\n",
      "Epoch 2280, Loss: 0.02508790185675025, Final Batch Loss: 0.003938752692192793\n",
      "Epoch 2281, Loss: 0.04735169559717178, Final Batch Loss: 0.025185728445649147\n",
      "Epoch 2282, Loss: 0.07103846967220306, Final Batch Loss: 0.016305260360240936\n",
      "Epoch 2283, Loss: 0.030084488913416862, Final Batch Loss: 0.011795807629823685\n",
      "Epoch 2284, Loss: 0.02169443340972066, Final Batch Loss: 0.004878141451627016\n",
      "Epoch 2285, Loss: 0.03990139625966549, Final Batch Loss: 0.030118679627776146\n",
      "Epoch 2286, Loss: 0.04795515350997448, Final Batch Loss: 0.03352418541908264\n",
      "Epoch 2287, Loss: 0.052489448338747025, Final Batch Loss: 0.029315713793039322\n",
      "Epoch 2288, Loss: 0.02428267663344741, Final Batch Loss: 0.01662486605346203\n",
      "Epoch 2289, Loss: 0.08098957687616348, Final Batch Loss: 0.05400767922401428\n",
      "Epoch 2290, Loss: 0.030302544124424458, Final Batch Loss: 0.009495140053331852\n",
      "Epoch 2291, Loss: 0.05919421836733818, Final Batch Loss: 0.024937983602285385\n",
      "Epoch 2292, Loss: 0.04934063367545605, Final Batch Loss: 0.026006437838077545\n",
      "Epoch 2293, Loss: 0.06350583583116531, Final Batch Loss: 0.04136957600712776\n",
      "Epoch 2294, Loss: 0.05795667413622141, Final Batch Loss: 0.04855487868189812\n",
      "Epoch 2295, Loss: 0.07289558462798595, Final Batch Loss: 0.029060984030365944\n",
      "Epoch 2296, Loss: 0.061892639845609665, Final Batch Loss: 0.022708896547555923\n",
      "Epoch 2297, Loss: 0.08778163976967335, Final Batch Loss: 0.02693610079586506\n",
      "Epoch 2298, Loss: 0.023786628618836403, Final Batch Loss: 0.006686622276902199\n",
      "Epoch 2299, Loss: 0.07926459051668644, Final Batch Loss: 0.012795129790902138\n",
      "Epoch 2300, Loss: 0.05244780518114567, Final Batch Loss: 0.02918563224375248\n",
      "Epoch 2301, Loss: 0.1317297350615263, Final Batch Loss: 0.10716209560632706\n",
      "Epoch 2302, Loss: 0.039707483258098364, Final Batch Loss: 0.035058602690696716\n",
      "Epoch 2303, Loss: 0.05003886669874191, Final Batch Loss: 0.035954151302576065\n",
      "Epoch 2304, Loss: 0.17321328818798065, Final Batch Loss: 0.08230182528495789\n",
      "Epoch 2305, Loss: 0.06560451537370682, Final Batch Loss: 0.033418651670217514\n",
      "Epoch 2306, Loss: 0.050614625215530396, Final Batch Loss: 0.025798873975872993\n",
      "Epoch 2307, Loss: 0.11786247510463, Final Batch Loss: 0.00826855469495058\n",
      "Epoch 2308, Loss: 0.1242721751332283, Final Batch Loss: 0.07581909000873566\n",
      "Epoch 2309, Loss: 0.10101467929780483, Final Batch Loss: 0.030714942142367363\n",
      "Epoch 2310, Loss: 0.07221007347106934, Final Batch Loss: 0.020588889718055725\n",
      "Epoch 2311, Loss: 0.047930614091455936, Final Batch Loss: 0.040015529841184616\n",
      "Epoch 2312, Loss: 0.14236751198768616, Final Batch Loss: 0.052379630506038666\n",
      "Epoch 2313, Loss: 0.03661724319681525, Final Batch Loss: 0.007215607445687056\n",
      "Epoch 2314, Loss: 0.03756972774863243, Final Batch Loss: 0.030110681429505348\n",
      "Epoch 2315, Loss: 0.0815622229129076, Final Batch Loss: 0.05872727558016777\n",
      "Epoch 2316, Loss: 0.08390554040670395, Final Batch Loss: 0.07221205532550812\n",
      "Epoch 2317, Loss: 0.024964350275695324, Final Batch Loss: 0.008874914608895779\n",
      "Epoch 2318, Loss: 0.039628155529499054, Final Batch Loss: 0.009170373901724815\n",
      "Epoch 2319, Loss: 0.08245126903057098, Final Batch Loss: 0.05039942264556885\n",
      "Epoch 2320, Loss: 0.12410789355635643, Final Batch Loss: 0.07682257890701294\n",
      "Epoch 2321, Loss: 0.050509121268987656, Final Batch Loss: 0.020549412816762924\n",
      "Epoch 2322, Loss: 0.09995415061712265, Final Batch Loss: 0.061035800725221634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2323, Loss: 0.047013895586133, Final Batch Loss: 0.016894621774554253\n",
      "Epoch 2324, Loss: 0.023248855955898762, Final Batch Loss: 0.0067982422187924385\n",
      "Epoch 2325, Loss: 0.08003433793783188, Final Batch Loss: 0.049642808735370636\n",
      "Epoch 2326, Loss: 0.08431492187082767, Final Batch Loss: 0.008133256807923317\n",
      "Epoch 2327, Loss: 0.02198215201497078, Final Batch Loss: 0.008474932983517647\n",
      "Epoch 2328, Loss: 0.03218261431902647, Final Batch Loss: 0.021088462322950363\n",
      "Epoch 2329, Loss: 0.07000452093780041, Final Batch Loss: 0.03933686390519142\n",
      "Epoch 2330, Loss: 0.03158742608502507, Final Batch Loss: 0.025996573269367218\n",
      "Epoch 2331, Loss: 0.04925432242453098, Final Batch Loss: 0.03804996237158775\n",
      "Epoch 2332, Loss: 0.06498588435351849, Final Batch Loss: 0.04107915237545967\n",
      "Epoch 2333, Loss: 0.030772496946156025, Final Batch Loss: 0.017085999250411987\n",
      "Epoch 2334, Loss: 0.028529241681098938, Final Batch Loss: 0.008237242698669434\n",
      "Epoch 2335, Loss: 0.023851522710174322, Final Batch Loss: 0.005908138584345579\n",
      "Epoch 2336, Loss: 0.10423951968550682, Final Batch Loss: 0.031011033803224564\n",
      "Epoch 2337, Loss: 0.05194936878979206, Final Batch Loss: 0.03524843230843544\n",
      "Epoch 2338, Loss: 0.03438178915530443, Final Batch Loss: 0.01482399832457304\n",
      "Epoch 2339, Loss: 0.08124569058418274, Final Batch Loss: 0.047846175730228424\n",
      "Epoch 2340, Loss: 0.050940110348165035, Final Batch Loss: 0.03857850655913353\n",
      "Epoch 2341, Loss: 0.06694135488942266, Final Batch Loss: 0.007547237444669008\n",
      "Epoch 2342, Loss: 0.06720113009214401, Final Batch Loss: 0.03412775322794914\n",
      "Epoch 2343, Loss: 0.03514023497700691, Final Batch Loss: 0.015204271301627159\n",
      "Epoch 2344, Loss: 0.04544576909393072, Final Batch Loss: 0.00617395993322134\n",
      "Epoch 2345, Loss: 0.05105300806462765, Final Batch Loss: 0.013994606211781502\n",
      "Epoch 2346, Loss: 0.03570292703807354, Final Batch Loss: 0.02703310362994671\n",
      "Epoch 2347, Loss: 0.032998367212712765, Final Batch Loss: 0.010226371698081493\n",
      "Epoch 2348, Loss: 0.03957834839820862, Final Batch Loss: 0.023845164105296135\n",
      "Epoch 2349, Loss: 0.06419342197477818, Final Batch Loss: 0.029665527865290642\n",
      "Epoch 2350, Loss: 0.0424537044018507, Final Batch Loss: 0.007767817005515099\n",
      "Epoch 2351, Loss: 0.03907391428947449, Final Batch Loss: 0.018660536035895348\n",
      "Epoch 2352, Loss: 0.0401200857013464, Final Batch Loss: 0.030707944184541702\n",
      "Epoch 2353, Loss: 0.05646538734436035, Final Batch Loss: 0.04974263161420822\n",
      "Epoch 2354, Loss: 0.10495528019964695, Final Batch Loss: 0.09092105180025101\n",
      "Epoch 2355, Loss: 0.019038247875869274, Final Batch Loss: 0.009314472787082195\n",
      "Epoch 2356, Loss: 0.022970804944634438, Final Batch Loss: 0.008699825964868069\n",
      "Epoch 2357, Loss: 0.035455936565995216, Final Batch Loss: 0.026156967505812645\n",
      "Epoch 2358, Loss: 0.04041318641975522, Final Batch Loss: 0.005054038483649492\n",
      "Epoch 2359, Loss: 0.06089898385107517, Final Batch Loss: 0.03665779158473015\n",
      "Epoch 2360, Loss: 0.038079483434557915, Final Batch Loss: 0.01891038566827774\n",
      "Epoch 2361, Loss: 0.03891794569790363, Final Batch Loss: 0.021680695936083794\n",
      "Epoch 2362, Loss: 0.025666539557278156, Final Batch Loss: 0.013268629088997841\n",
      "Epoch 2363, Loss: 0.06566207483410835, Final Batch Loss: 0.05291137471795082\n",
      "Epoch 2364, Loss: 0.023213489912450314, Final Batch Loss: 0.016421794891357422\n",
      "Epoch 2365, Loss: 0.08533134311437607, Final Batch Loss: 0.021225295960903168\n",
      "Epoch 2366, Loss: 0.07390907406806946, Final Batch Loss: 0.05223589763045311\n",
      "Epoch 2367, Loss: 0.03695020917803049, Final Batch Loss: 0.011119098402559757\n",
      "Epoch 2368, Loss: 0.09891859441995621, Final Batch Loss: 0.08416059613227844\n",
      "Epoch 2369, Loss: 0.06160173751413822, Final Batch Loss: 0.021330418065190315\n",
      "Epoch 2370, Loss: 0.030048125423491, Final Batch Loss: 0.007874383591115475\n",
      "Epoch 2371, Loss: 0.047573029063642025, Final Batch Loss: 0.036670368164777756\n",
      "Epoch 2372, Loss: 0.05083915404975414, Final Batch Loss: 0.029822491109371185\n",
      "Epoch 2373, Loss: 0.03808915801346302, Final Batch Loss: 0.018462123349308968\n",
      "Epoch 2374, Loss: 0.0302233612164855, Final Batch Loss: 0.02039550617337227\n",
      "Epoch 2375, Loss: 0.039105836767703295, Final Batch Loss: 0.032292064279317856\n",
      "Epoch 2376, Loss: 0.07106620818376541, Final Batch Loss: 0.03482169285416603\n",
      "Epoch 2377, Loss: 0.03998532239347696, Final Batch Loss: 0.010210446082055569\n",
      "Epoch 2378, Loss: 0.05730286426842213, Final Batch Loss: 0.03910195827484131\n",
      "Epoch 2379, Loss: 0.05658058263361454, Final Batch Loss: 0.03858824074268341\n",
      "Epoch 2380, Loss: 0.06343141570687294, Final Batch Loss: 0.02698417380452156\n",
      "Epoch 2381, Loss: 0.06756491772830486, Final Batch Loss: 0.021302135661244392\n",
      "Epoch 2382, Loss: 0.03964971983805299, Final Batch Loss: 0.005542545113712549\n",
      "Epoch 2383, Loss: 0.037119598127901554, Final Batch Loss: 0.025336584076285362\n",
      "Epoch 2384, Loss: 0.05043560825288296, Final Batch Loss: 0.010399067774415016\n",
      "Epoch 2385, Loss: 0.07270253682509065, Final Batch Loss: 0.005165951792150736\n",
      "Epoch 2386, Loss: 0.09028191585093737, Final Batch Loss: 0.00918541569262743\n",
      "Epoch 2387, Loss: 0.043749770149588585, Final Batch Loss: 0.022760456427931786\n",
      "Epoch 2388, Loss: 0.03173206886276603, Final Batch Loss: 0.02422468550503254\n",
      "Epoch 2389, Loss: 0.04561834502965212, Final Batch Loss: 0.03745892271399498\n",
      "Epoch 2390, Loss: 0.04086090624332428, Final Batch Loss: 0.019463445991277695\n",
      "Epoch 2391, Loss: 0.023602494038641453, Final Batch Loss: 0.010652754455804825\n",
      "Epoch 2392, Loss: 0.08166069071739912, Final Batch Loss: 0.008788014762103558\n",
      "Epoch 2393, Loss: 0.017351148184388876, Final Batch Loss: 0.004183424171060324\n",
      "Epoch 2394, Loss: 0.04696889594197273, Final Batch Loss: 0.025141721591353416\n",
      "Epoch 2395, Loss: 0.018282816046848893, Final Batch Loss: 0.015194329433143139\n",
      "Epoch 2396, Loss: 0.02196122705936432, Final Batch Loss: 0.00677898246794939\n",
      "Epoch 2397, Loss: 0.10964776203036308, Final Batch Loss: 0.07985090464353561\n",
      "Epoch 2398, Loss: 0.01850923802703619, Final Batch Loss: 0.010160456411540508\n",
      "Epoch 2399, Loss: 0.02152830408886075, Final Batch Loss: 0.004361210856586695\n",
      "Epoch 2400, Loss: 0.03698201011866331, Final Batch Loss: 0.013470149599015713\n",
      "Epoch 2401, Loss: 0.03696959465742111, Final Batch Loss: 0.02905486896634102\n",
      "Epoch 2402, Loss: 0.028931022621691227, Final Batch Loss: 0.008849852718412876\n",
      "Epoch 2403, Loss: 0.020046416670084, Final Batch Loss: 0.005109380930662155\n",
      "Epoch 2404, Loss: 0.017700815573334694, Final Batch Loss: 0.008259356953203678\n",
      "Epoch 2405, Loss: 0.0937541127204895, Final Batch Loss: 0.042818062007427216\n",
      "Epoch 2406, Loss: 0.021443267818540335, Final Batch Loss: 0.005142909940332174\n",
      "Epoch 2407, Loss: 0.03861727099865675, Final Batch Loss: 0.014224761165678501\n",
      "Epoch 2408, Loss: 0.05800282396376133, Final Batch Loss: 0.03456581011414528\n",
      "Epoch 2409, Loss: 0.03690837882459164, Final Batch Loss: 0.008674712851643562\n",
      "Epoch 2410, Loss: 0.05500201880931854, Final Batch Loss: 0.03816080838441849\n",
      "Epoch 2411, Loss: 0.027121689170598984, Final Batch Loss: 0.012299573980271816\n",
      "Epoch 2412, Loss: 0.028643697500228882, Final Batch Loss: 0.009438179433345795\n",
      "Epoch 2413, Loss: 0.027053588069975376, Final Batch Loss: 0.010572933591902256\n",
      "Epoch 2414, Loss: 0.024580898694694042, Final Batch Loss: 0.018405437469482422\n",
      "Epoch 2415, Loss: 0.02270268928259611, Final Batch Loss: 0.005211534909904003\n",
      "Epoch 2416, Loss: 0.03223790414631367, Final Batch Loss: 0.008313635364174843\n",
      "Epoch 2417, Loss: 0.029361072462052107, Final Batch Loss: 0.023613953962922096\n",
      "Epoch 2418, Loss: 0.04449273645877838, Final Batch Loss: 0.01627742312848568\n",
      "Epoch 2419, Loss: 0.06930374167859554, Final Batch Loss: 0.050844140350818634\n",
      "Epoch 2420, Loss: 0.04107573768123984, Final Batch Loss: 0.003916689660400152\n",
      "Epoch 2421, Loss: 0.028728406876325607, Final Batch Loss: 0.010103696957230568\n",
      "Epoch 2422, Loss: 0.02039949083700776, Final Batch Loss: 0.014925091527402401\n",
      "Epoch 2423, Loss: 0.05718513950705528, Final Batch Loss: 0.0461471751332283\n",
      "Epoch 2424, Loss: 0.018263136968016624, Final Batch Loss: 0.0047578392550349236\n",
      "Epoch 2425, Loss: 0.09356595948338509, Final Batch Loss: 0.045846421271562576\n",
      "Epoch 2426, Loss: 0.05512307025492191, Final Batch Loss: 0.0351475365459919\n",
      "Epoch 2427, Loss: 0.023725940845906734, Final Batch Loss: 0.011487200856208801\n",
      "Epoch 2428, Loss: 0.04900995036587119, Final Batch Loss: 0.04191979765892029\n",
      "Epoch 2429, Loss: 0.041616156697273254, Final Batch Loss: 0.02555941231548786\n",
      "Epoch 2430, Loss: 0.10188590828329325, Final Batch Loss: 0.09178060293197632\n",
      "Epoch 2431, Loss: 0.012447671266272664, Final Batch Loss: 0.0026923895347863436\n",
      "Epoch 2432, Loss: 0.03208691906183958, Final Batch Loss: 0.005551139824092388\n",
      "Epoch 2433, Loss: 0.06656255666166544, Final Batch Loss: 0.010195028968155384\n",
      "Epoch 2434, Loss: 0.03858140576630831, Final Batch Loss: 0.025503413751721382\n",
      "Epoch 2435, Loss: 0.040176838636398315, Final Batch Loss: 0.0066574737429618835\n",
      "Epoch 2436, Loss: 0.042121692560613155, Final Batch Loss: 0.01431658398360014\n",
      "Epoch 2437, Loss: 0.05685601383447647, Final Batch Loss: 0.031612198799848557\n",
      "Epoch 2438, Loss: 0.027148266322910786, Final Batch Loss: 0.010583504103124142\n",
      "Epoch 2439, Loss: 0.04408998787403107, Final Batch Loss: 0.01856752671301365\n",
      "Epoch 2440, Loss: 0.08614932745695114, Final Batch Loss: 0.05785388499498367\n",
      "Epoch 2441, Loss: 0.02752379048615694, Final Batch Loss: 0.013337025418877602\n",
      "Epoch 2442, Loss: 0.06303980201482773, Final Batch Loss: 0.03894413262605667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2443, Loss: 0.06446131505072117, Final Batch Loss: 0.04935581237077713\n",
      "Epoch 2444, Loss: 0.031148691195994616, Final Batch Loss: 0.00725809158757329\n",
      "Epoch 2445, Loss: 0.032906537875533104, Final Batch Loss: 0.011015329509973526\n",
      "Epoch 2446, Loss: 0.05517945345491171, Final Batch Loss: 0.04665601626038551\n",
      "Epoch 2447, Loss: 0.022516540717333555, Final Batch Loss: 0.0068412586115300655\n",
      "Epoch 2448, Loss: 0.03318273741751909, Final Batch Loss: 0.0120081203058362\n",
      "Epoch 2449, Loss: 0.0327864121645689, Final Batch Loss: 0.007034434005618095\n",
      "Epoch 2450, Loss: 0.08050760580226779, Final Batch Loss: 0.07426968961954117\n",
      "Epoch 2451, Loss: 0.02936212671920657, Final Batch Loss: 0.023197203874588013\n",
      "Epoch 2452, Loss: 0.020583584904670715, Final Batch Loss: 0.010200468823313713\n",
      "Epoch 2453, Loss: 0.013797816820442677, Final Batch Loss: 0.007348394952714443\n",
      "Epoch 2454, Loss: 0.02374382922425866, Final Batch Loss: 0.005351640749722719\n",
      "Epoch 2455, Loss: 0.027505483012646437, Final Batch Loss: 0.004971736576408148\n",
      "Epoch 2456, Loss: 0.019301304128021002, Final Batch Loss: 0.006002210546284914\n",
      "Epoch 2457, Loss: 0.03202472534030676, Final Batch Loss: 0.01501335296779871\n",
      "Epoch 2458, Loss: 0.0997218657284975, Final Batch Loss: 0.07747819274663925\n",
      "Epoch 2459, Loss: 0.019177641719579697, Final Batch Loss: 0.004023898392915726\n",
      "Epoch 2460, Loss: 0.047141971066594124, Final Batch Loss: 0.03308406472206116\n",
      "Epoch 2461, Loss: 0.030727671459317207, Final Batch Loss: 0.011909589171409607\n",
      "Epoch 2462, Loss: 0.0736794974654913, Final Batch Loss: 0.047521378844976425\n",
      "Epoch 2463, Loss: 0.023907594149932265, Final Batch Loss: 0.003675501560792327\n",
      "Epoch 2464, Loss: 0.07331906957551837, Final Batch Loss: 0.0025066654197871685\n",
      "Epoch 2465, Loss: 0.027213547844439745, Final Batch Loss: 0.004513862077146769\n",
      "Epoch 2466, Loss: 0.03430961072444916, Final Batch Loss: 0.025475217029452324\n",
      "Epoch 2467, Loss: 0.035095563158392906, Final Batch Loss: 0.011379480361938477\n",
      "Epoch 2468, Loss: 0.04542674496769905, Final Batch Loss: 0.03016287460923195\n",
      "Epoch 2469, Loss: 0.04777572490274906, Final Batch Loss: 0.036714669317007065\n",
      "Epoch 2470, Loss: 0.0752240065485239, Final Batch Loss: 0.026881368830800056\n",
      "Epoch 2471, Loss: 0.04258802253752947, Final Batch Loss: 0.015355118550360203\n",
      "Epoch 2472, Loss: 0.08290653303265572, Final Batch Loss: 0.04212217405438423\n",
      "Epoch 2473, Loss: 0.0535489721223712, Final Batch Loss: 0.010316330008208752\n",
      "Epoch 2474, Loss: 0.3607479929924011, Final Batch Loss: 0.20539674162864685\n",
      "Epoch 2475, Loss: 0.07999704405665398, Final Batch Loss: 0.0414804145693779\n",
      "Epoch 2476, Loss: 0.09430331736803055, Final Batch Loss: 0.05671560391783714\n",
      "Epoch 2477, Loss: 0.04661308601498604, Final Batch Loss: 0.03395802527666092\n",
      "Epoch 2478, Loss: 0.12000548839569092, Final Batch Loss: 0.07870481163263321\n",
      "Epoch 2479, Loss: 0.09990926831960678, Final Batch Loss: 0.05131220445036888\n",
      "Epoch 2480, Loss: 0.11546573415398598, Final Batch Loss: 0.08405039459466934\n",
      "Epoch 2481, Loss: 0.08453897945582867, Final Batch Loss: 0.010085349902510643\n",
      "Epoch 2482, Loss: 0.03965072613209486, Final Batch Loss: 0.015067550353705883\n",
      "Epoch 2483, Loss: 0.03017678577452898, Final Batch Loss: 0.02202305942773819\n",
      "Epoch 2484, Loss: 0.07199253141880035, Final Batch Loss: 0.03812488168478012\n",
      "Epoch 2485, Loss: 0.043291036039590836, Final Batch Loss: 0.01758519560098648\n",
      "Epoch 2486, Loss: 0.07227297872304916, Final Batch Loss: 0.031767845153808594\n",
      "Epoch 2487, Loss: 0.05157854035496712, Final Batch Loss: 0.030599426478147507\n",
      "Epoch 2488, Loss: 0.03064212203025818, Final Batch Loss: 0.014383947476744652\n",
      "Epoch 2489, Loss: 0.04221528768539429, Final Batch Loss: 0.020012663677334785\n",
      "Epoch 2490, Loss: 0.03795318119227886, Final Batch Loss: 0.01750953681766987\n",
      "Epoch 2491, Loss: 0.03699520044028759, Final Batch Loss: 0.026357803493738174\n",
      "Epoch 2492, Loss: 0.021232067607343197, Final Batch Loss: 0.004379781894385815\n",
      "Epoch 2493, Loss: 0.03258466627448797, Final Batch Loss: 0.027187231928110123\n",
      "Epoch 2494, Loss: 0.11039257049560547, Final Batch Loss: 0.07117938995361328\n",
      "Epoch 2495, Loss: 0.01963435299694538, Final Batch Loss: 0.009664788842201233\n",
      "Epoch 2496, Loss: 0.026610339991748333, Final Batch Loss: 0.0174519345164299\n",
      "Epoch 2497, Loss: 0.052376480773091316, Final Batch Loss: 0.027722273021936417\n",
      "Epoch 2498, Loss: 0.024669152218848467, Final Batch Loss: 0.007192466873675585\n",
      "Epoch 2499, Loss: 0.0265248017385602, Final Batch Loss: 0.011836650781333447\n",
      "Epoch 2500, Loss: 0.01686001569032669, Final Batch Loss: 0.008570995181798935\n",
      "Epoch 2501, Loss: 0.014979806262999773, Final Batch Loss: 0.00763179874047637\n",
      "Epoch 2502, Loss: 0.029842590913176537, Final Batch Loss: 0.005071483552455902\n",
      "Epoch 2503, Loss: 0.11428480222821236, Final Batch Loss: 0.07855460792779922\n",
      "Epoch 2504, Loss: 0.0476237703114748, Final Batch Loss: 0.02308141440153122\n",
      "Epoch 2505, Loss: 0.048004400450736284, Final Batch Loss: 0.0050301034934818745\n",
      "Epoch 2506, Loss: 0.03847266733646393, Final Batch Loss: 0.028704149648547173\n",
      "Epoch 2507, Loss: 0.036227233707904816, Final Batch Loss: 0.020199647173285484\n",
      "Epoch 2508, Loss: 0.05761680379509926, Final Batch Loss: 0.03964725881814957\n",
      "Epoch 2509, Loss: 0.059227198362350464, Final Batch Loss: 0.021736543625593185\n",
      "Epoch 2510, Loss: 0.017199855763465166, Final Batch Loss: 0.003987013828009367\n",
      "Epoch 2511, Loss: 0.06434290204197168, Final Batch Loss: 0.006239798851311207\n",
      "Epoch 2512, Loss: 0.013343018013983965, Final Batch Loss: 0.007948139682412148\n",
      "Epoch 2513, Loss: 0.048078699968755245, Final Batch Loss: 0.005646484903991222\n",
      "Epoch 2514, Loss: 0.07723081670701504, Final Batch Loss: 0.04720860719680786\n",
      "Epoch 2515, Loss: 0.035566986072808504, Final Batch Loss: 0.0037355278618633747\n",
      "Epoch 2516, Loss: 0.032371994107961655, Final Batch Loss: 0.015155041590332985\n",
      "Epoch 2517, Loss: 0.05162536073476076, Final Batch Loss: 0.036291636526584625\n",
      "Epoch 2518, Loss: 0.01918335072696209, Final Batch Loss: 0.0034809932112693787\n",
      "Epoch 2519, Loss: 0.02899309527128935, Final Batch Loss: 0.011782159097492695\n",
      "Epoch 2520, Loss: 0.03950601536780596, Final Batch Loss: 0.009738870896399021\n",
      "Epoch 2521, Loss: 0.03063811920583248, Final Batch Loss: 0.013487253338098526\n",
      "Epoch 2522, Loss: 0.0371891213580966, Final Batch Loss: 0.03106069006025791\n",
      "Epoch 2523, Loss: 0.04268394783139229, Final Batch Loss: 0.03223603218793869\n",
      "Epoch 2524, Loss: 0.04601784888654947, Final Batch Loss: 0.036340150982141495\n",
      "Epoch 2525, Loss: 0.04480467364192009, Final Batch Loss: 0.016244247555732727\n",
      "Epoch 2526, Loss: 0.00850588153116405, Final Batch Loss: 0.003461559070274234\n",
      "Epoch 2527, Loss: 0.05403272435069084, Final Batch Loss: 0.03603869676589966\n",
      "Epoch 2528, Loss: 0.027054704260081053, Final Batch Loss: 0.02265992760658264\n",
      "Epoch 2529, Loss: 0.06284333765506744, Final Batch Loss: 0.04260130226612091\n",
      "Epoch 2530, Loss: 0.027807695791125298, Final Batch Loss: 0.0154618164524436\n",
      "Epoch 2531, Loss: 0.028020115569233894, Final Batch Loss: 0.020778607577085495\n",
      "Epoch 2532, Loss: 0.06714626215398312, Final Batch Loss: 0.026498427614569664\n",
      "Epoch 2533, Loss: 0.017140618059784174, Final Batch Loss: 0.005760830361396074\n",
      "Epoch 2534, Loss: 0.04255929123610258, Final Batch Loss: 0.027538083493709564\n",
      "Epoch 2535, Loss: 0.020669812336564064, Final Batch Loss: 0.01056304108351469\n",
      "Epoch 2536, Loss: 0.05927086994051933, Final Batch Loss: 0.0180264450609684\n",
      "Epoch 2537, Loss: 0.02093600109219551, Final Batch Loss: 0.008788439445197582\n",
      "Epoch 2538, Loss: 0.024626176804304123, Final Batch Loss: 0.014340080320835114\n",
      "Epoch 2539, Loss: 0.061994568444788456, Final Batch Loss: 0.04817504808306694\n",
      "Epoch 2540, Loss: 0.04922944959253073, Final Batch Loss: 0.014667275361716747\n",
      "Epoch 2541, Loss: 0.015490429475903511, Final Batch Loss: 0.00541657954454422\n",
      "Epoch 2542, Loss: 0.03777203895151615, Final Batch Loss: 0.011692622676491737\n",
      "Epoch 2543, Loss: 0.04304451262578368, Final Batch Loss: 0.006693968083709478\n",
      "Epoch 2544, Loss: 0.04090244462713599, Final Batch Loss: 0.007459515240043402\n",
      "Epoch 2545, Loss: 0.020312143489718437, Final Batch Loss: 0.01009486522525549\n",
      "Epoch 2546, Loss: 0.022962699178606272, Final Batch Loss: 0.004445899743586779\n",
      "Epoch 2547, Loss: 0.024356123991310596, Final Batch Loss: 0.011631536297500134\n",
      "Epoch 2548, Loss: 0.02334041055291891, Final Batch Loss: 0.010745472274720669\n",
      "Epoch 2549, Loss: 0.04477599263191223, Final Batch Loss: 0.03778814151883125\n",
      "Epoch 2550, Loss: 0.014599168673157692, Final Batch Loss: 0.009956574998795986\n",
      "Epoch 2551, Loss: 0.014749865047633648, Final Batch Loss: 0.006633359007537365\n",
      "Epoch 2552, Loss: 0.02096874825656414, Final Batch Loss: 0.01285229530185461\n",
      "Epoch 2553, Loss: 0.06189970299601555, Final Batch Loss: 0.020299412310123444\n",
      "Epoch 2554, Loss: 0.0700481254607439, Final Batch Loss: 0.019864702597260475\n",
      "Epoch 2555, Loss: 0.05425350368022919, Final Batch Loss: 0.03818120062351227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2556, Loss: 0.021173557033762336, Final Batch Loss: 0.0026253832038491964\n",
      "Epoch 2557, Loss: 0.03995420038700104, Final Batch Loss: 0.02984117716550827\n",
      "Epoch 2558, Loss: 0.07729449076578021, Final Batch Loss: 0.07114243507385254\n",
      "Epoch 2559, Loss: 0.03739550430327654, Final Batch Loss: 0.012792698107659817\n",
      "Epoch 2560, Loss: 0.03325514309108257, Final Batch Loss: 0.020826267078518867\n",
      "Epoch 2561, Loss: 0.044906480237841606, Final Batch Loss: 0.037060439586639404\n",
      "Epoch 2562, Loss: 0.04965097736567259, Final Batch Loss: 0.014913144521415234\n",
      "Epoch 2563, Loss: 0.020387924276292324, Final Batch Loss: 0.010515139438211918\n",
      "Epoch 2564, Loss: 0.03859354183077812, Final Batch Loss: 0.02077624946832657\n",
      "Epoch 2565, Loss: 0.027031006291508675, Final Batch Loss: 0.012947406619787216\n",
      "Epoch 2566, Loss: 0.048950144089758396, Final Batch Loss: 0.011289183981716633\n",
      "Epoch 2567, Loss: 0.018829648848623037, Final Batch Loss: 0.0066918241791427135\n",
      "Epoch 2568, Loss: 0.045954128727316856, Final Batch Loss: 0.011560225859284401\n",
      "Epoch 2569, Loss: 0.01815752638503909, Final Batch Loss: 0.00411078380420804\n",
      "Epoch 2570, Loss: 0.03370446898043156, Final Batch Loss: 0.015105484053492546\n",
      "Epoch 2571, Loss: 0.02779747499153018, Final Batch Loss: 0.005302864592522383\n",
      "Epoch 2572, Loss: 0.019441106356680393, Final Batch Loss: 0.014501196332275867\n",
      "Epoch 2573, Loss: 0.017319091130048037, Final Batch Loss: 0.006017797160893679\n",
      "Epoch 2574, Loss: 0.05036759562790394, Final Batch Loss: 0.045710477977991104\n",
      "Epoch 2575, Loss: 0.029863822273910046, Final Batch Loss: 0.01875445991754532\n",
      "Epoch 2576, Loss: 0.0691608153283596, Final Batch Loss: 0.05952345207333565\n",
      "Epoch 2577, Loss: 0.03034132719039917, Final Batch Loss: 0.019524287432432175\n",
      "Epoch 2578, Loss: 0.011310113593935966, Final Batch Loss: 0.0038854959420859814\n",
      "Epoch 2579, Loss: 0.03722077514976263, Final Batch Loss: 0.021610597148537636\n",
      "Epoch 2580, Loss: 0.04758655559271574, Final Batch Loss: 0.010731670074164867\n",
      "Epoch 2581, Loss: 0.10370684252120554, Final Batch Loss: 0.002683831611648202\n",
      "Epoch 2582, Loss: 0.08205941133201122, Final Batch Loss: 0.06441595405340195\n",
      "Epoch 2583, Loss: 0.025043742265552282, Final Batch Loss: 0.006865720730274916\n",
      "Epoch 2584, Loss: 0.042686439119279385, Final Batch Loss: 0.02867894619703293\n",
      "Epoch 2585, Loss: 0.03922205464914441, Final Batch Loss: 0.033597517758607864\n",
      "Epoch 2586, Loss: 0.018919366877526045, Final Batch Loss: 0.014678127132356167\n",
      "Epoch 2587, Loss: 0.03339902125298977, Final Batch Loss: 0.004522403702139854\n",
      "Epoch 2588, Loss: 0.04539236705750227, Final Batch Loss: 0.030742963775992393\n",
      "Epoch 2589, Loss: 0.0670637097209692, Final Batch Loss: 0.03809034824371338\n",
      "Epoch 2590, Loss: 0.03967066667973995, Final Batch Loss: 0.017955893650650978\n",
      "Epoch 2591, Loss: 0.03623948618769646, Final Batch Loss: 0.023116186261177063\n",
      "Epoch 2592, Loss: 0.0541659165173769, Final Batch Loss: 0.028026681393384933\n",
      "Epoch 2593, Loss: 0.014616493601351976, Final Batch Loss: 0.005210337694734335\n",
      "Epoch 2594, Loss: 0.06212266907095909, Final Batch Loss: 0.033586036413908005\n",
      "Epoch 2595, Loss: 0.06925354804843664, Final Batch Loss: 0.0574009045958519\n",
      "Epoch 2596, Loss: 0.017823082860559225, Final Batch Loss: 0.0047418647445738316\n",
      "Epoch 2597, Loss: 0.032733567990362644, Final Batch Loss: 0.02784869633615017\n",
      "Epoch 2598, Loss: 0.02072909497655928, Final Batch Loss: 0.00214386940933764\n",
      "Epoch 2599, Loss: 0.07408825913444161, Final Batch Loss: 0.06819814443588257\n",
      "Epoch 2600, Loss: 0.03672778559848666, Final Batch Loss: 0.0026955376379191875\n",
      "Epoch 2601, Loss: 0.028108639642596245, Final Batch Loss: 0.012618050910532475\n",
      "Epoch 2602, Loss: 0.01390368863940239, Final Batch Loss: 0.0031735431402921677\n",
      "Epoch 2603, Loss: 0.0591307170689106, Final Batch Loss: 0.039243511855602264\n",
      "Epoch 2604, Loss: 0.014966499526053667, Final Batch Loss: 0.0036666779778897762\n",
      "Epoch 2605, Loss: 0.015557915437966585, Final Batch Loss: 0.006338461767882109\n",
      "Epoch 2606, Loss: 0.029864267446100712, Final Batch Loss: 0.003267238847911358\n",
      "Epoch 2607, Loss: 0.03181794844567776, Final Batch Loss: 0.014345699921250343\n",
      "Epoch 2608, Loss: 0.047538790851831436, Final Batch Loss: 0.037985630333423615\n",
      "Epoch 2609, Loss: 0.029235229827463627, Final Batch Loss: 0.004493805579841137\n",
      "Epoch 2610, Loss: 0.010838182643055916, Final Batch Loss: 0.003674959298223257\n",
      "Epoch 2611, Loss: 0.03422838542610407, Final Batch Loss: 0.00945212785154581\n",
      "Epoch 2612, Loss: 0.024971320293843746, Final Batch Loss: 0.007756893523037434\n",
      "Epoch 2613, Loss: 0.042739637196063995, Final Batch Loss: 0.02326997183263302\n",
      "Epoch 2614, Loss: 0.0873800627887249, Final Batch Loss: 0.04045238345861435\n",
      "Epoch 2615, Loss: 0.016023352975025773, Final Batch Loss: 0.0038637558463960886\n",
      "Epoch 2616, Loss: 0.03422934468835592, Final Batch Loss: 0.00868272315710783\n",
      "Epoch 2617, Loss: 0.01452780980616808, Final Batch Loss: 0.005212374031543732\n",
      "Epoch 2618, Loss: 0.0749876294285059, Final Batch Loss: 0.05864482745528221\n",
      "Epoch 2619, Loss: 0.013036745367571712, Final Batch Loss: 0.0032897961791604757\n",
      "Epoch 2620, Loss: 0.03655182383954525, Final Batch Loss: 0.017638064920902252\n",
      "Epoch 2621, Loss: 0.06687944754958153, Final Batch Loss: 0.041263651102781296\n",
      "Epoch 2622, Loss: 0.04533636290580034, Final Batch Loss: 0.031547896564006805\n",
      "Epoch 2623, Loss: 0.013651896268129349, Final Batch Loss: 0.009081302210688591\n",
      "Epoch 2624, Loss: 0.03717551752924919, Final Batch Loss: 0.019768085330724716\n",
      "Epoch 2625, Loss: 0.04275192227214575, Final Batch Loss: 0.00562820490449667\n",
      "Epoch 2626, Loss: 0.011828364571556449, Final Batch Loss: 0.0030490101780742407\n",
      "Epoch 2627, Loss: 0.01646577101200819, Final Batch Loss: 0.0063462406396865845\n",
      "Epoch 2628, Loss: 0.05641399510204792, Final Batch Loss: 0.023172078654170036\n",
      "Epoch 2629, Loss: 0.014371233526617289, Final Batch Loss: 0.006938168779015541\n",
      "Epoch 2630, Loss: 0.014225204708054662, Final Batch Loss: 0.0026443612296134233\n",
      "Epoch 2631, Loss: 0.01672961376607418, Final Batch Loss: 0.012105555273592472\n",
      "Epoch 2632, Loss: 0.029924349393695593, Final Batch Loss: 0.02389148250222206\n",
      "Epoch 2633, Loss: 0.038141971454024315, Final Batch Loss: 0.017999088391661644\n",
      "Epoch 2634, Loss: 0.04347500018775463, Final Batch Loss: 0.02957361377775669\n",
      "Epoch 2635, Loss: 0.06861242651939392, Final Batch Loss: 0.03289560228586197\n",
      "Epoch 2636, Loss: 0.017609150148928165, Final Batch Loss: 0.01337044034153223\n",
      "Epoch 2637, Loss: 0.0290358392521739, Final Batch Loss: 0.019828008487820625\n",
      "Epoch 2638, Loss: 0.08080630749464035, Final Batch Loss: 0.025475721806287766\n",
      "Epoch 2639, Loss: 0.0893421322107315, Final Batch Loss: 0.05341058969497681\n",
      "Epoch 2640, Loss: 0.026104828575626016, Final Batch Loss: 0.022228604182600975\n",
      "Epoch 2641, Loss: 0.029016100335866213, Final Batch Loss: 0.005332937929779291\n",
      "Epoch 2642, Loss: 0.03458245890215039, Final Batch Loss: 0.006788446102291346\n",
      "Epoch 2643, Loss: 0.10025485418736935, Final Batch Loss: 0.0821298211812973\n",
      "Epoch 2644, Loss: 0.023446298204362392, Final Batch Loss: 0.006248896010220051\n",
      "Epoch 2645, Loss: 0.03366418369114399, Final Batch Loss: 0.012989576905965805\n",
      "Epoch 2646, Loss: 0.018125819973647594, Final Batch Loss: 0.01124951709061861\n",
      "Epoch 2647, Loss: 0.06206782907247543, Final Batch Loss: 0.003917042165994644\n",
      "Epoch 2648, Loss: 0.029039724729955196, Final Batch Loss: 0.020308446139097214\n",
      "Epoch 2649, Loss: 0.0343197388574481, Final Batch Loss: 0.020785970613360405\n",
      "Epoch 2650, Loss: 0.0447273226454854, Final Batch Loss: 0.013760709203779697\n",
      "Epoch 2651, Loss: 0.031455133110284805, Final Batch Loss: 0.013782942667603493\n",
      "Epoch 2652, Loss: 0.03836449235677719, Final Batch Loss: 0.022379886358976364\n",
      "Epoch 2653, Loss: 0.0281600346788764, Final Batch Loss: 0.018347080796957016\n",
      "Epoch 2654, Loss: 0.03579428931698203, Final Batch Loss: 0.0071563697420060635\n",
      "Epoch 2655, Loss: 0.052972367499023676, Final Batch Loss: 0.0057592191733419895\n",
      "Epoch 2656, Loss: 0.01596494996920228, Final Batch Loss: 0.007692310493439436\n",
      "Epoch 2657, Loss: 0.05069842655211687, Final Batch Loss: 0.012268953956663609\n",
      "Epoch 2658, Loss: 0.03617026610299945, Final Batch Loss: 0.02980000339448452\n",
      "Epoch 2659, Loss: 0.01063186302781105, Final Batch Loss: 0.004568379372358322\n",
      "Epoch 2660, Loss: 0.02876616269350052, Final Batch Loss: 0.015148313716053963\n",
      "Epoch 2661, Loss: 0.026550572365522385, Final Batch Loss: 0.012906311079859734\n",
      "Epoch 2662, Loss: 0.010309312958270311, Final Batch Loss: 0.006091500632464886\n",
      "Epoch 2663, Loss: 0.026646658778190613, Final Batch Loss: 0.017804717645049095\n",
      "Epoch 2664, Loss: 0.02167561510577798, Final Batch Loss: 0.006301825400441885\n",
      "Epoch 2665, Loss: 0.039953360334038734, Final Batch Loss: 0.013642903417348862\n",
      "Epoch 2666, Loss: 0.019661400467157364, Final Batch Loss: 0.01224055141210556\n",
      "Epoch 2667, Loss: 0.03228359017521143, Final Batch Loss: 0.015099807642400265\n",
      "Epoch 2668, Loss: 0.022826547734439373, Final Batch Loss: 0.009331136010587215\n",
      "Epoch 2669, Loss: 0.017775939544662833, Final Batch Loss: 0.0024516514968127012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2670, Loss: 0.1712130680680275, Final Batch Loss: 0.1344502717256546\n",
      "Epoch 2671, Loss: 0.02646491676568985, Final Batch Loss: 0.009755419567227364\n",
      "Epoch 2672, Loss: 0.057058652862906456, Final Batch Loss: 0.034104976803064346\n",
      "Epoch 2673, Loss: 0.021627753973007202, Final Batch Loss: 0.004503931850194931\n",
      "Epoch 2674, Loss: 0.015463280258700252, Final Batch Loss: 0.012452838942408562\n",
      "Epoch 2675, Loss: 0.027554655447602272, Final Batch Loss: 0.019307205453515053\n",
      "Epoch 2676, Loss: 0.024542965460568666, Final Batch Loss: 0.002870789263397455\n",
      "Epoch 2677, Loss: 0.038942886516451836, Final Batch Loss: 0.018585551530122757\n",
      "Epoch 2678, Loss: 0.016695866361260414, Final Batch Loss: 0.005624879151582718\n",
      "Epoch 2679, Loss: 0.026633151806890965, Final Batch Loss: 0.011117029003798962\n",
      "Epoch 2680, Loss: 0.05541342310607433, Final Batch Loss: 0.01734806038439274\n",
      "Epoch 2681, Loss: 0.04041038919240236, Final Batch Loss: 0.0037372270599007607\n",
      "Epoch 2682, Loss: 0.055844442918896675, Final Batch Loss: 0.02249280922114849\n",
      "Epoch 2683, Loss: 0.02318987250328064, Final Batch Loss: 0.007835978642106056\n",
      "Epoch 2684, Loss: 0.027610941790044308, Final Batch Loss: 0.010629205964505672\n",
      "Epoch 2685, Loss: 0.050958674401044846, Final Batch Loss: 0.037601497024297714\n",
      "Epoch 2686, Loss: 0.013435491360723972, Final Batch Loss: 0.0023405542597174644\n",
      "Epoch 2687, Loss: 0.038286587223410606, Final Batch Loss: 0.029484212398529053\n",
      "Epoch 2688, Loss: 0.01633173367008567, Final Batch Loss: 0.00631128391250968\n",
      "Epoch 2689, Loss: 0.012334041763097048, Final Batch Loss: 0.005969289690256119\n",
      "Epoch 2690, Loss: 0.04642248619347811, Final Batch Loss: 0.008913588710129261\n",
      "Epoch 2691, Loss: 0.03717494755983353, Final Batch Loss: 0.028410397469997406\n",
      "Epoch 2692, Loss: 0.024631126783788204, Final Batch Loss: 0.010117225348949432\n",
      "Epoch 2693, Loss: 0.01588299870491028, Final Batch Loss: 0.011346116662025452\n",
      "Epoch 2694, Loss: 0.016903570387512445, Final Batch Loss: 0.004275910090655088\n",
      "Epoch 2695, Loss: 0.012894242070615292, Final Batch Loss: 0.0049188584089279175\n",
      "Epoch 2696, Loss: 0.011936267837882042, Final Batch Loss: 0.006981804966926575\n",
      "Epoch 2697, Loss: 0.0051826899871230125, Final Batch Loss: 0.0024772423785179853\n",
      "Epoch 2698, Loss: 0.02088944800198078, Final Batch Loss: 0.010042150504887104\n",
      "Epoch 2699, Loss: 0.009208729257807136, Final Batch Loss: 0.0036730130668729544\n",
      "Epoch 2700, Loss: 0.027390402741730213, Final Batch Loss: 0.01704833284020424\n",
      "Epoch 2701, Loss: 0.055088188499212265, Final Batch Loss: 0.028380755335092545\n",
      "Epoch 2702, Loss: 0.014949148520827293, Final Batch Loss: 0.004214177839457989\n",
      "Epoch 2703, Loss: 0.029367780312895775, Final Batch Loss: 0.020021885633468628\n",
      "Epoch 2704, Loss: 0.018538509728386998, Final Batch Loss: 0.002763717668130994\n",
      "Epoch 2705, Loss: 0.025131389033049345, Final Batch Loss: 0.007202960085123777\n",
      "Epoch 2706, Loss: 0.008978570811450481, Final Batch Loss: 0.006183937657624483\n",
      "Epoch 2707, Loss: 0.017216806299984455, Final Batch Loss: 0.008260549046099186\n",
      "Epoch 2708, Loss: 0.010351526085287333, Final Batch Loss: 0.0031293611973524094\n",
      "Epoch 2709, Loss: 0.02154230559244752, Final Batch Loss: 0.0037681018002331257\n",
      "Epoch 2710, Loss: 0.011640872340649366, Final Batch Loss: 0.005504485685378313\n",
      "Epoch 2711, Loss: 0.005648803431540728, Final Batch Loss: 0.0019605662673711777\n",
      "Epoch 2712, Loss: 0.005112480255775154, Final Batch Loss: 0.0017429402796551585\n",
      "Epoch 2713, Loss: 0.0457855686545372, Final Batch Loss: 0.03311961889266968\n",
      "Epoch 2714, Loss: 0.037925098557025194, Final Batch Loss: 0.033639002591371536\n",
      "Epoch 2715, Loss: 0.03058796003460884, Final Batch Loss: 0.004260100424289703\n",
      "Epoch 2716, Loss: 0.009374747052788734, Final Batch Loss: 0.004008563235402107\n",
      "Epoch 2717, Loss: 0.042551989667117596, Final Batch Loss: 0.03749924153089523\n",
      "Epoch 2718, Loss: 0.0644422210752964, Final Batch Loss: 0.03558458015322685\n",
      "Epoch 2719, Loss: 0.05794171919114888, Final Batch Loss: 0.0035686150658875704\n",
      "Epoch 2720, Loss: 0.028061045799404383, Final Batch Loss: 0.006159591022878885\n",
      "Epoch 2721, Loss: 0.03132153209298849, Final Batch Loss: 0.02711719088256359\n",
      "Epoch 2722, Loss: 0.03192297741770744, Final Batch Loss: 0.006015533581376076\n",
      "Epoch 2723, Loss: 0.009968948550522327, Final Batch Loss: 0.0030136369168758392\n",
      "Epoch 2724, Loss: 0.008702375227585435, Final Batch Loss: 0.0025176627095788717\n",
      "Epoch 2725, Loss: 0.03818516945466399, Final Batch Loss: 0.03464842587709427\n",
      "Epoch 2726, Loss: 0.0193351375637576, Final Batch Loss: 0.0014197732089087367\n",
      "Epoch 2727, Loss: 0.048303019255399704, Final Batch Loss: 0.03135291114449501\n",
      "Epoch 2728, Loss: 0.02181255118921399, Final Batch Loss: 0.004440546501427889\n",
      "Epoch 2729, Loss: 0.04384969733655453, Final Batch Loss: 0.025386156514286995\n",
      "Epoch 2730, Loss: 0.057179929688572884, Final Batch Loss: 0.017593229189515114\n",
      "Epoch 2731, Loss: 0.026279535377398133, Final Batch Loss: 0.0030543359462171793\n",
      "Epoch 2732, Loss: 0.033224291168153286, Final Batch Loss: 0.00950209517031908\n",
      "Epoch 2733, Loss: 0.05509459413588047, Final Batch Loss: 0.03494803234934807\n",
      "Epoch 2734, Loss: 0.02437908621504903, Final Batch Loss: 0.0029586763121187687\n",
      "Epoch 2735, Loss: 0.009996024891734123, Final Batch Loss: 0.00444779871031642\n",
      "Epoch 2736, Loss: 0.036471396684646606, Final Batch Loss: 0.0035676397383213043\n",
      "Epoch 2737, Loss: 0.16681766230612993, Final Batch Loss: 0.16190530359745026\n",
      "Epoch 2738, Loss: 0.03342067915946245, Final Batch Loss: 0.00932361464947462\n",
      "Epoch 2739, Loss: 0.04068831540644169, Final Batch Loss: 0.015647143125534058\n",
      "Epoch 2740, Loss: 0.02935918979346752, Final Batch Loss: 0.013070384040474892\n",
      "Epoch 2741, Loss: 0.03835032717324793, Final Batch Loss: 0.034506168216466904\n",
      "Epoch 2742, Loss: 0.026053343899548054, Final Batch Loss: 0.010305318050086498\n",
      "Epoch 2743, Loss: 0.04026395455002785, Final Batch Loss: 0.024073945358395576\n",
      "Epoch 2744, Loss: 0.040571796242147684, Final Batch Loss: 0.0029062614776194096\n",
      "Epoch 2745, Loss: 0.03894149139523506, Final Batch Loss: 0.02112300880253315\n",
      "Epoch 2746, Loss: 0.01837555505335331, Final Batch Loss: 0.007116072811186314\n",
      "Epoch 2747, Loss: 0.040192628279328346, Final Batch Loss: 0.014586085453629494\n",
      "Epoch 2748, Loss: 0.03873145300894976, Final Batch Loss: 0.013772496022284031\n",
      "Epoch 2749, Loss: 0.028493084013462067, Final Batch Loss: 0.022397460415959358\n",
      "Epoch 2750, Loss: 0.013542477507144213, Final Batch Loss: 0.006087175104767084\n",
      "Epoch 2751, Loss: 0.02184154186397791, Final Batch Loss: 0.01176786981523037\n",
      "Epoch 2752, Loss: 0.04904575273394585, Final Batch Loss: 0.042788054794073105\n",
      "Epoch 2753, Loss: 0.013582427054643631, Final Batch Loss: 0.0035520093515515327\n",
      "Epoch 2754, Loss: 0.011862724320963025, Final Batch Loss: 0.0036130084190517664\n",
      "Epoch 2755, Loss: 0.008647250011563301, Final Batch Loss: 0.003151667769998312\n",
      "Epoch 2756, Loss: 0.03438213374465704, Final Batch Loss: 0.008317972533404827\n",
      "Epoch 2757, Loss: 0.024800191167742014, Final Batch Loss: 0.0050405883230268955\n",
      "Epoch 2758, Loss: 0.012971121817827225, Final Batch Loss: 0.0041303979232907295\n",
      "Epoch 2759, Loss: 0.015987790655344725, Final Batch Loss: 0.0037246006540954113\n",
      "Epoch 2760, Loss: 0.022835474461317062, Final Batch Loss: 0.013152001425623894\n",
      "Epoch 2761, Loss: 0.009990082122385502, Final Batch Loss: 0.004820338450372219\n",
      "Epoch 2762, Loss: 0.08303042501211166, Final Batch Loss: 0.06336738914251328\n",
      "Epoch 2763, Loss: 0.036298076156526804, Final Batch Loss: 0.03121136501431465\n",
      "Epoch 2764, Loss: 0.033876387402415276, Final Batch Loss: 0.01774776168167591\n",
      "Epoch 2765, Loss: 0.029003377072513103, Final Batch Loss: 0.0041056061163544655\n",
      "Epoch 2766, Loss: 0.03971752244979143, Final Batch Loss: 0.03028596192598343\n",
      "Epoch 2767, Loss: 0.02957071177661419, Final Batch Loss: 0.017813142389059067\n",
      "Epoch 2768, Loss: 0.040687983855605125, Final Batch Loss: 0.013919023796916008\n",
      "Epoch 2769, Loss: 0.031227781088091433, Final Batch Loss: 0.0013004062930122018\n",
      "Epoch 2770, Loss: 0.025189100531861186, Final Batch Loss: 0.00339479255490005\n",
      "Epoch 2771, Loss: 0.020822115009650588, Final Batch Loss: 0.00344741134904325\n",
      "Epoch 2772, Loss: 0.015788324177265167, Final Batch Loss: 0.011011768132448196\n",
      "Epoch 2773, Loss: 0.038099716417491436, Final Batch Loss: 0.02776830643415451\n",
      "Epoch 2774, Loss: 0.04719752399250865, Final Batch Loss: 0.04078773036599159\n",
      "Epoch 2775, Loss: 0.039083026349544525, Final Batch Loss: 0.009158959612250328\n",
      "Epoch 2776, Loss: 0.023040269501507282, Final Batch Loss: 0.012786848470568657\n",
      "Epoch 2777, Loss: 0.016084897331893444, Final Batch Loss: 0.013706402853131294\n",
      "Epoch 2778, Loss: 0.010515352711081505, Final Batch Loss: 0.004566414747387171\n",
      "Epoch 2779, Loss: 0.022962118964642286, Final Batch Loss: 0.018991487100720406\n",
      "Epoch 2780, Loss: 0.02628923486918211, Final Batch Loss: 0.008807358331978321\n",
      "Epoch 2781, Loss: 0.015484634321182966, Final Batch Loss: 0.009013237431645393\n",
      "Epoch 2782, Loss: 0.026013036025688052, Final Batch Loss: 0.003365120617672801\n",
      "Epoch 2783, Loss: 0.04021287243813276, Final Batch Loss: 0.035876642912626266\n",
      "Epoch 2784, Loss: 0.04824505373835564, Final Batch Loss: 0.03911541774868965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2785, Loss: 0.06030753022059798, Final Batch Loss: 0.05495670810341835\n",
      "Epoch 2786, Loss: 0.01555272052064538, Final Batch Loss: 0.0022540180943906307\n",
      "Epoch 2787, Loss: 0.03015710413455963, Final Batch Loss: 0.011192092671990395\n",
      "Epoch 2788, Loss: 0.023681762162595987, Final Batch Loss: 0.0065093678422272205\n",
      "Epoch 2789, Loss: 0.12903141975402832, Final Batch Loss: 0.08155442029237747\n",
      "Epoch 2790, Loss: 0.04837942589074373, Final Batch Loss: 0.03672805428504944\n",
      "Epoch 2791, Loss: 0.022602988174185157, Final Batch Loss: 0.0026387718971818686\n",
      "Epoch 2792, Loss: 0.04670706205070019, Final Batch Loss: 0.024303870275616646\n",
      "Epoch 2793, Loss: 0.010011720936745405, Final Batch Loss: 0.00559694180265069\n",
      "Epoch 2794, Loss: 0.029436787590384483, Final Batch Loss: 0.01909536123275757\n",
      "Epoch 2795, Loss: 0.02875136211514473, Final Batch Loss: 0.008079510182142258\n",
      "Epoch 2796, Loss: 0.06623788364231586, Final Batch Loss: 0.029968535527586937\n",
      "Epoch 2797, Loss: 0.06611372344195843, Final Batch Loss: 0.03547517955303192\n",
      "Epoch 2798, Loss: 0.026883652433753014, Final Batch Loss: 0.009559154510498047\n",
      "Epoch 2799, Loss: 0.08749047154560685, Final Batch Loss: 0.002901161555200815\n",
      "Epoch 2800, Loss: 0.0328499311581254, Final Batch Loss: 0.025150490924715996\n",
      "Epoch 2801, Loss: 0.10741763189435005, Final Batch Loss: 0.051740940660238266\n",
      "Epoch 2802, Loss: 0.05302674509584904, Final Batch Loss: 0.032192982733249664\n",
      "Epoch 2803, Loss: 0.023665708489716053, Final Batch Loss: 0.006078870035707951\n",
      "Epoch 2804, Loss: 0.037896390073001385, Final Batch Loss: 0.0024146800860762596\n",
      "Epoch 2805, Loss: 0.055334629490971565, Final Batch Loss: 0.020170962437987328\n",
      "Epoch 2806, Loss: 0.035445140674710274, Final Batch Loss: 0.026409661397337914\n",
      "Epoch 2807, Loss: 0.03871008846908808, Final Batch Loss: 0.02973763458430767\n",
      "Epoch 2808, Loss: 0.05722685158252716, Final Batch Loss: 0.012572798877954483\n",
      "Epoch 2809, Loss: 0.07970482856035233, Final Batch Loss: 0.04613948613405228\n",
      "Epoch 2810, Loss: 0.04751452058553696, Final Batch Loss: 0.022136865183711052\n",
      "Epoch 2811, Loss: 0.08570122439414263, Final Batch Loss: 0.014563803561031818\n",
      "Epoch 2812, Loss: 0.054700952023267746, Final Batch Loss: 0.03301476687192917\n",
      "Epoch 2813, Loss: 0.04371149744838476, Final Batch Loss: 0.009716055355966091\n",
      "Epoch 2814, Loss: 0.08537449315190315, Final Batch Loss: 0.0331098735332489\n",
      "Epoch 2815, Loss: 0.014678443782031536, Final Batch Loss: 0.0028901007026433945\n",
      "Epoch 2816, Loss: 0.03895431570708752, Final Batch Loss: 0.006191818043589592\n",
      "Epoch 2817, Loss: 0.056704665534198284, Final Batch Loss: 0.0440022349357605\n",
      "Epoch 2818, Loss: 0.06196005828678608, Final Batch Loss: 0.04682677611708641\n",
      "Epoch 2819, Loss: 0.04893101658672094, Final Batch Loss: 0.014439075253903866\n",
      "Epoch 2820, Loss: 0.06883090920746326, Final Batch Loss: 0.05254444479942322\n",
      "Epoch 2821, Loss: 0.0385678643360734, Final Batch Loss: 0.011300777085125446\n",
      "Epoch 2822, Loss: 0.034958597272634506, Final Batch Loss: 0.012306313961744308\n",
      "Epoch 2823, Loss: 0.014191505033522844, Final Batch Loss: 0.008587628602981567\n",
      "Epoch 2824, Loss: 0.0506706484593451, Final Batch Loss: 0.00441978732123971\n",
      "Epoch 2825, Loss: 0.05083788465708494, Final Batch Loss: 0.004689025692641735\n",
      "Epoch 2826, Loss: 0.0456987489014864, Final Batch Loss: 0.022268639877438545\n",
      "Epoch 2827, Loss: 0.08297917246818542, Final Batch Loss: 0.05117269232869148\n",
      "Epoch 2828, Loss: 0.04558038339018822, Final Batch Loss: 0.02798747830092907\n",
      "Epoch 2829, Loss: 0.027342654298990965, Final Batch Loss: 0.005520455073565245\n",
      "Epoch 2830, Loss: 0.025633980752900243, Final Batch Loss: 0.003863132791593671\n",
      "Epoch 2831, Loss: 0.06036009639501572, Final Batch Loss: 0.017914898693561554\n",
      "Epoch 2832, Loss: 0.016689382260665298, Final Batch Loss: 0.0034084522631019354\n",
      "Epoch 2833, Loss: 0.04530640132725239, Final Batch Loss: 0.020454363897442818\n",
      "Epoch 2834, Loss: 0.07534325495362282, Final Batch Loss: 0.043254729360342026\n",
      "Epoch 2835, Loss: 0.030930378939956427, Final Batch Loss: 0.004953646566718817\n",
      "Epoch 2836, Loss: 0.03725764527916908, Final Batch Loss: 0.02576814778149128\n",
      "Epoch 2837, Loss: 0.03694804443512112, Final Batch Loss: 0.0017336426535621285\n",
      "Epoch 2838, Loss: 0.01822871807962656, Final Batch Loss: 0.0032564057037234306\n",
      "Epoch 2839, Loss: 0.06645184941589832, Final Batch Loss: 0.05080065131187439\n",
      "Epoch 2840, Loss: 0.057663717307150364, Final Batch Loss: 0.05449899286031723\n",
      "Epoch 2841, Loss: 0.023249946534633636, Final Batch Loss: 0.0026966165751218796\n",
      "Epoch 2842, Loss: 0.04278275091201067, Final Batch Loss: 0.038518067449331284\n",
      "Epoch 2843, Loss: 0.03289808612316847, Final Batch Loss: 0.014634327031672001\n",
      "Epoch 2844, Loss: 0.05566017888486385, Final Batch Loss: 0.0382937490940094\n",
      "Epoch 2845, Loss: 0.03736485820263624, Final Batch Loss: 0.009109136648476124\n",
      "Epoch 2846, Loss: 0.0581764318048954, Final Batch Loss: 0.010037638247013092\n",
      "Epoch 2847, Loss: 0.018362779635936022, Final Batch Loss: 0.004392692353576422\n",
      "Epoch 2848, Loss: 0.04433474224060774, Final Batch Loss: 0.036936406046152115\n",
      "Epoch 2849, Loss: 0.045762352645397186, Final Batch Loss: 0.027377216145396233\n",
      "Epoch 2850, Loss: 0.035981147550046444, Final Batch Loss: 0.020414257422089577\n",
      "Epoch 2851, Loss: 0.030416415072977543, Final Batch Loss: 0.022433966398239136\n",
      "Epoch 2852, Loss: 0.053512748796492815, Final Batch Loss: 0.004339773673564196\n",
      "Epoch 2853, Loss: 0.023636113852262497, Final Batch Loss: 0.011928079649806023\n",
      "Epoch 2854, Loss: 0.06046759523451328, Final Batch Loss: 0.008219501003623009\n",
      "Epoch 2855, Loss: 0.07830583211034536, Final Batch Loss: 0.07039453089237213\n",
      "Epoch 2856, Loss: 0.024965315125882626, Final Batch Loss: 0.008319824002683163\n",
      "Epoch 2857, Loss: 0.0703807957470417, Final Batch Loss: 0.048536673188209534\n",
      "Epoch 2858, Loss: 0.010258722119033337, Final Batch Loss: 0.0057112229987978935\n",
      "Epoch 2859, Loss: 0.034800440073013306, Final Batch Loss: 0.010053725913167\n",
      "Epoch 2860, Loss: 0.020365926902741194, Final Batch Loss: 0.007466274779289961\n",
      "Epoch 2861, Loss: 0.05273915082216263, Final Batch Loss: 0.035593245178461075\n",
      "Epoch 2862, Loss: 0.059830786660313606, Final Batch Loss: 0.018061695620417595\n",
      "Epoch 2863, Loss: 0.038524326868355274, Final Batch Loss: 0.010982700623571873\n",
      "Epoch 2864, Loss: 0.030929981730878353, Final Batch Loss: 0.003566325642168522\n",
      "Epoch 2865, Loss: 0.13657310605049133, Final Batch Loss: 0.08167649060487747\n",
      "Epoch 2866, Loss: 0.10849396511912346, Final Batch Loss: 0.03022991493344307\n",
      "Epoch 2867, Loss: 0.03765703737735748, Final Batch Loss: 0.021422145888209343\n",
      "Epoch 2868, Loss: 0.04628258291631937, Final Batch Loss: 0.009814982302486897\n",
      "Epoch 2869, Loss: 0.0443840567022562, Final Batch Loss: 0.012643003836274147\n",
      "Epoch 2870, Loss: 0.08789501339197159, Final Batch Loss: 0.03784269466996193\n",
      "Epoch 2871, Loss: 0.02757943980395794, Final Batch Loss: 0.00890175811946392\n",
      "Epoch 2872, Loss: 0.04835487902164459, Final Batch Loss: 0.03247503563761711\n",
      "Epoch 2873, Loss: 0.04905021749436855, Final Batch Loss: 0.020666729658842087\n",
      "Epoch 2874, Loss: 0.05260162986814976, Final Batch Loss: 0.03446882963180542\n",
      "Epoch 2875, Loss: 0.052466302178800106, Final Batch Loss: 0.006216364912688732\n",
      "Epoch 2876, Loss: 0.024113715160638094, Final Batch Loss: 0.003845816943794489\n",
      "Epoch 2877, Loss: 0.049710825085639954, Final Batch Loss: 0.018502499908208847\n",
      "Epoch 2878, Loss: 0.05702880956232548, Final Batch Loss: 0.04327748715877533\n",
      "Epoch 2879, Loss: 0.029925593174993992, Final Batch Loss: 0.020097846165299416\n",
      "Epoch 2880, Loss: 0.16043375805020332, Final Batch Loss: 0.12599778175354004\n",
      "Epoch 2881, Loss: 0.058149367570877075, Final Batch Loss: 0.049245331436395645\n",
      "Epoch 2882, Loss: 0.07285940647125244, Final Batch Loss: 0.04128330945968628\n",
      "Epoch 2883, Loss: 0.09958593547344208, Final Batch Loss: 0.05562204122543335\n",
      "Epoch 2884, Loss: 0.04903197940438986, Final Batch Loss: 0.03967083618044853\n",
      "Epoch 2885, Loss: 0.053698496893048286, Final Batch Loss: 0.025472166016697884\n",
      "Epoch 2886, Loss: 0.032583486288785934, Final Batch Loss: 0.008052932098507881\n",
      "Epoch 2887, Loss: 0.012511549517512321, Final Batch Loss: 0.004807904362678528\n",
      "Epoch 2888, Loss: 0.07662415318191051, Final Batch Loss: 0.01792404241859913\n",
      "Epoch 2889, Loss: 0.017561878077685833, Final Batch Loss: 0.009676878340542316\n",
      "Epoch 2890, Loss: 0.01837422326207161, Final Batch Loss: 0.012609373778104782\n",
      "Epoch 2891, Loss: 0.054634299129247665, Final Batch Loss: 0.010902170091867447\n",
      "Epoch 2892, Loss: 0.11785989254713058, Final Batch Loss: 0.0507560670375824\n",
      "Epoch 2893, Loss: 0.029913613572716713, Final Batch Loss: 0.017733259126544\n",
      "Epoch 2894, Loss: 0.021587791852653027, Final Batch Loss: 0.01226284820586443\n",
      "Epoch 2895, Loss: 0.04485654830932617, Final Batch Loss: 0.014701854437589645\n",
      "Epoch 2896, Loss: 0.024233296513557434, Final Batch Loss: 0.011616566218435764\n",
      "Epoch 2897, Loss: 0.018380194902420044, Final Batch Loss: 0.01111674215644598\n",
      "Epoch 2898, Loss: 0.0623574610799551, Final Batch Loss: 0.01704995147883892\n",
      "Epoch 2899, Loss: 0.02499854378402233, Final Batch Loss: 0.010396444238722324\n",
      "Epoch 2900, Loss: 0.06360376067459583, Final Batch Loss: 0.04443966597318649\n",
      "Epoch 2901, Loss: 0.04491998441517353, Final Batch Loss: 0.028262946754693985\n",
      "Epoch 2902, Loss: 0.09627844393253326, Final Batch Loss: 0.04186868667602539\n",
      "Epoch 2903, Loss: 0.037230418995022774, Final Batch Loss: 0.020256265997886658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2904, Loss: 0.03247586451470852, Final Batch Loss: 0.006332410499453545\n",
      "Epoch 2905, Loss: 0.030409141443669796, Final Batch Loss: 0.008952208794653416\n",
      "Epoch 2906, Loss: 0.048727807588875294, Final Batch Loss: 0.04232175275683403\n",
      "Epoch 2907, Loss: 0.03920422960072756, Final Batch Loss: 0.008390854112803936\n",
      "Epoch 2908, Loss: 0.06788945198059082, Final Batch Loss: 0.057400718331336975\n",
      "Epoch 2909, Loss: 0.02512564556673169, Final Batch Loss: 0.006375125143676996\n",
      "Epoch 2910, Loss: 0.03762812912464142, Final Batch Loss: 0.01996258646249771\n",
      "Epoch 2911, Loss: 0.026287600863724947, Final Batch Loss: 0.0067100306041538715\n",
      "Epoch 2912, Loss: 0.036229951307177544, Final Batch Loss: 0.01988346502184868\n",
      "Epoch 2913, Loss: 0.02567297243513167, Final Batch Loss: 0.02193767949938774\n",
      "Epoch 2914, Loss: 0.028248485177755356, Final Batch Loss: 0.010616330429911613\n",
      "Epoch 2915, Loss: 0.04388867598026991, Final Batch Loss: 0.008964146487414837\n",
      "Epoch 2916, Loss: 0.049535179510712624, Final Batch Loss: 0.022318899631500244\n",
      "Epoch 2917, Loss: 0.0582257304340601, Final Batch Loss: 0.021483289077878\n",
      "Epoch 2918, Loss: 0.016677752137184143, Final Batch Loss: 0.004786482080817223\n",
      "Epoch 2919, Loss: 0.034910015761852264, Final Batch Loss: 0.009195560589432716\n",
      "Epoch 2920, Loss: 0.054300001822412014, Final Batch Loss: 0.0020908964797854424\n",
      "Epoch 2921, Loss: 0.018641529604792595, Final Batch Loss: 0.010774991475045681\n",
      "Epoch 2922, Loss: 0.02695202361792326, Final Batch Loss: 0.017723126336932182\n",
      "Epoch 2923, Loss: 0.05900830775499344, Final Batch Loss: 0.02770809829235077\n",
      "Epoch 2924, Loss: 0.032099721021950245, Final Batch Loss: 0.013773455284535885\n",
      "Epoch 2925, Loss: 0.03792592603713274, Final Batch Loss: 0.027218876406550407\n",
      "Epoch 2926, Loss: 0.0368462847545743, Final Batch Loss: 0.00915702898055315\n",
      "Epoch 2927, Loss: 0.022105233743786812, Final Batch Loss: 0.008288188837468624\n",
      "Epoch 2928, Loss: 0.02962114568799734, Final Batch Loss: 0.006241149269044399\n",
      "Epoch 2929, Loss: 0.06109228543937206, Final Batch Loss: 0.05158229544758797\n",
      "Epoch 2930, Loss: 0.05271015036851168, Final Batch Loss: 0.009317752905189991\n",
      "Epoch 2931, Loss: 0.10956222377717495, Final Batch Loss: 0.09048392623662949\n",
      "Epoch 2932, Loss: 0.0191963417455554, Final Batch Loss: 0.007883337326347828\n",
      "Epoch 2933, Loss: 0.04683615826070309, Final Batch Loss: 0.031371407210826874\n",
      "Epoch 2934, Loss: 0.05080309323966503, Final Batch Loss: 0.017975827679038048\n",
      "Epoch 2935, Loss: 0.058623747900128365, Final Batch Loss: 0.03669925779104233\n",
      "Epoch 2936, Loss: 0.15547887235879898, Final Batch Loss: 0.14275486767292023\n",
      "Epoch 2937, Loss: 0.04272133205085993, Final Batch Loss: 0.035776276141405106\n",
      "Epoch 2938, Loss: 0.053641959093511105, Final Batch Loss: 0.013373169116675854\n",
      "Epoch 2939, Loss: 0.07764801569283009, Final Batch Loss: 0.055274754762649536\n",
      "Epoch 2940, Loss: 0.05241740681231022, Final Batch Loss: 0.030368736013770103\n",
      "Epoch 2941, Loss: 0.03366667125374079, Final Batch Loss: 0.012554106302559376\n",
      "Epoch 2942, Loss: 0.015349660534411669, Final Batch Loss: 0.008529623970389366\n",
      "Epoch 2943, Loss: 0.04858434200286865, Final Batch Loss: 0.026999978348612785\n",
      "Epoch 2944, Loss: 0.03557350113987923, Final Batch Loss: 0.025097720324993134\n",
      "Epoch 2945, Loss: 0.017637422308325768, Final Batch Loss: 0.006018109619617462\n",
      "Epoch 2946, Loss: 0.06209569424390793, Final Batch Loss: 0.030142076313495636\n",
      "Epoch 2947, Loss: 0.06013680435717106, Final Batch Loss: 0.04500800743699074\n",
      "Epoch 2948, Loss: 0.024247626308351755, Final Batch Loss: 0.004876053426414728\n",
      "Epoch 2949, Loss: 0.028994640335440636, Final Batch Loss: 0.010899566113948822\n",
      "Epoch 2950, Loss: 0.06437056325376034, Final Batch Loss: 0.05877075344324112\n",
      "Epoch 2951, Loss: 0.0362406768836081, Final Batch Loss: 0.004837064538151026\n",
      "Epoch 2952, Loss: 0.02648179419338703, Final Batch Loss: 0.006446069106459618\n",
      "Epoch 2953, Loss: 0.03875542804598808, Final Batch Loss: 0.021514754742383957\n",
      "Epoch 2954, Loss: 0.016960579436272383, Final Batch Loss: 0.006742153782397509\n",
      "Epoch 2955, Loss: 0.05832825228571892, Final Batch Loss: 0.04176676645874977\n",
      "Epoch 2956, Loss: 0.026509265415370464, Final Batch Loss: 0.01822880282998085\n",
      "Epoch 2957, Loss: 0.04041046230122447, Final Batch Loss: 0.007584750186651945\n",
      "Epoch 2958, Loss: 0.028134155087172985, Final Batch Loss: 0.005806592293083668\n",
      "Epoch 2959, Loss: 0.025774233974516392, Final Batch Loss: 0.007150598801672459\n",
      "Epoch 2960, Loss: 0.04314270382747054, Final Batch Loss: 0.03886432573199272\n",
      "Epoch 2961, Loss: 0.043257858604192734, Final Batch Loss: 0.034010760486125946\n",
      "Epoch 2962, Loss: 0.021476877620443702, Final Batch Loss: 0.0033007150050252676\n",
      "Epoch 2963, Loss: 0.02699717227369547, Final Batch Loss: 0.008086836896836758\n",
      "Epoch 2964, Loss: 0.013630098663270473, Final Batch Loss: 0.004771608859300613\n",
      "Epoch 2965, Loss: 0.048941421788185835, Final Batch Loss: 0.041504375636577606\n",
      "Epoch 2966, Loss: 0.030588869005441666, Final Batch Loss: 0.011042812839150429\n",
      "Epoch 2967, Loss: 0.016938253305852413, Final Batch Loss: 0.008484634570777416\n",
      "Epoch 2968, Loss: 0.04586866404861212, Final Batch Loss: 0.005574273876845837\n",
      "Epoch 2969, Loss: 0.041823743376880884, Final Batch Loss: 0.005088327918201685\n",
      "Epoch 2970, Loss: 0.017450270242989063, Final Batch Loss: 0.012394309975206852\n",
      "Epoch 2971, Loss: 0.16105651669204235, Final Batch Loss: 0.14834384620189667\n",
      "Epoch 2972, Loss: 0.01507072290405631, Final Batch Loss: 0.007071503903716803\n",
      "Epoch 2973, Loss: 0.010986891109496355, Final Batch Loss: 0.005658518057316542\n",
      "Epoch 2974, Loss: 0.052036650478839874, Final Batch Loss: 0.005840431898832321\n",
      "Epoch 2975, Loss: 0.018024928402155638, Final Batch Loss: 0.013143534772098064\n",
      "Epoch 2976, Loss: 0.020546112675219774, Final Batch Loss: 0.014373854734003544\n",
      "Epoch 2977, Loss: 0.03547772625461221, Final Batch Loss: 0.0051778205670416355\n",
      "Epoch 2978, Loss: 0.013499791733920574, Final Batch Loss: 0.007015148643404245\n",
      "Epoch 2979, Loss: 0.03654606197960675, Final Batch Loss: 0.0011729404795914888\n",
      "Epoch 2980, Loss: 0.0190890459343791, Final Batch Loss: 0.0058483220636844635\n",
      "Epoch 2981, Loss: 0.03561878111213446, Final Batch Loss: 0.021866386756300926\n",
      "Epoch 2982, Loss: 0.05424331407994032, Final Batch Loss: 0.04811405763030052\n",
      "Epoch 2983, Loss: 0.023736908566206694, Final Batch Loss: 0.0030369902960956097\n",
      "Epoch 2984, Loss: 0.03469095192849636, Final Batch Loss: 0.01845473423600197\n",
      "Epoch 2985, Loss: 0.013424969278275967, Final Batch Loss: 0.004049324430525303\n",
      "Epoch 2986, Loss: 0.04462537169456482, Final Batch Loss: 0.033849526196718216\n",
      "Epoch 2987, Loss: 0.020271494751796126, Final Batch Loss: 0.0032855046447366476\n",
      "Epoch 2988, Loss: 0.03637290094047785, Final Batch Loss: 0.007050876505672932\n",
      "Epoch 2989, Loss: 0.0364665649831295, Final Batch Loss: 0.01391264982521534\n",
      "Epoch 2990, Loss: 0.014699826948344707, Final Batch Loss: 0.006448774598538876\n",
      "Epoch 2991, Loss: 0.014865549746900797, Final Batch Loss: 0.005181091371923685\n",
      "Epoch 2992, Loss: 0.008325904607772827, Final Batch Loss: 0.004169217776507139\n",
      "Epoch 2993, Loss: 0.010917971841990948, Final Batch Loss: 0.004844859708100557\n",
      "Epoch 2994, Loss: 0.011908553540706635, Final Batch Loss: 0.006265853065997362\n",
      "Epoch 2995, Loss: 0.012511105043813586, Final Batch Loss: 0.0031429564114660025\n",
      "Epoch 2996, Loss: 0.015200616791844368, Final Batch Loss: 0.006223659962415695\n",
      "Epoch 2997, Loss: 0.01964554376900196, Final Batch Loss: 0.0045031169429421425\n",
      "Epoch 2998, Loss: 0.01596261910162866, Final Batch Loss: 0.0034464483615010977\n",
      "Epoch 2999, Loss: 0.01293592108413577, Final Batch Loss: 0.007326571270823479\n",
      "Epoch 3000, Loss: 0.08730907924473286, Final Batch Loss: 0.07022708654403687\n",
      "Epoch 3001, Loss: 0.04920067824423313, Final Batch Loss: 0.013115352019667625\n",
      "Epoch 3002, Loss: 0.038682552985846996, Final Batch Loss: 0.03112727217376232\n",
      "Epoch 3003, Loss: 0.021641088649630547, Final Batch Loss: 0.017119375988841057\n",
      "Epoch 3004, Loss: 0.0313643179833889, Final Batch Loss: 0.0142582468688488\n",
      "Epoch 3005, Loss: 0.025929867289960384, Final Batch Loss: 0.006190567277371883\n",
      "Epoch 3006, Loss: 0.039863668847829103, Final Batch Loss: 0.032344333827495575\n",
      "Epoch 3007, Loss: 0.029149528592824936, Final Batch Loss: 0.009132470935583115\n",
      "Epoch 3008, Loss: 0.011733686551451683, Final Batch Loss: 0.004255200736224651\n",
      "Epoch 3009, Loss: 0.06358383037149906, Final Batch Loss: 0.04675934836268425\n",
      "Epoch 3010, Loss: 0.012813153211027384, Final Batch Loss: 0.0033595762215554714\n",
      "Epoch 3011, Loss: 0.029255762696266174, Final Batch Loss: 0.01827465184032917\n",
      "Epoch 3012, Loss: 0.03709164448082447, Final Batch Loss: 0.021536383777856827\n",
      "Epoch 3013, Loss: 0.02524900296702981, Final Batch Loss: 0.006533252540975809\n",
      "Epoch 3014, Loss: 0.017805451527237892, Final Batch Loss: 0.002596265636384487\n",
      "Epoch 3015, Loss: 0.061564719304442406, Final Batch Loss: 0.015587786212563515\n",
      "Epoch 3016, Loss: 0.010748615022748709, Final Batch Loss: 0.004298344720155001\n",
      "Epoch 3017, Loss: 0.026524582179263234, Final Batch Loss: 0.002401839243248105\n",
      "Epoch 3018, Loss: 0.025365642039105296, Final Batch Loss: 0.02347407117486\n",
      "Epoch 3019, Loss: 0.023201711708679795, Final Batch Loss: 0.0036724454257637262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3020, Loss: 0.0386956799775362, Final Batch Loss: 0.023644693195819855\n",
      "Epoch 3021, Loss: 0.022019428201019764, Final Batch Loss: 0.006588868796825409\n",
      "Epoch 3022, Loss: 0.013233989011496305, Final Batch Loss: 0.005249144043773413\n",
      "Epoch 3023, Loss: 0.039901847718283534, Final Batch Loss: 0.003234873292967677\n",
      "Epoch 3024, Loss: 0.034330059541389346, Final Batch Loss: 0.002755165798589587\n",
      "Epoch 3025, Loss: 0.017790661193430424, Final Batch Loss: 0.009350178763270378\n",
      "Epoch 3026, Loss: 0.02495321538299322, Final Batch Loss: 0.014643434435129166\n",
      "Epoch 3027, Loss: 0.011686130659654737, Final Batch Loss: 0.001401444198563695\n",
      "Epoch 3028, Loss: 0.05410923436284065, Final Batch Loss: 0.028750216588377953\n",
      "Epoch 3029, Loss: 0.0741436812095344, Final Batch Loss: 0.006117481272667646\n",
      "Epoch 3030, Loss: 0.020800895988941193, Final Batch Loss: 0.00831908080726862\n",
      "Epoch 3031, Loss: 0.032151962630450726, Final Batch Loss: 0.0278327614068985\n",
      "Epoch 3032, Loss: 0.022042538039386272, Final Batch Loss: 0.006313740275800228\n",
      "Epoch 3033, Loss: 0.0483585181646049, Final Batch Loss: 0.004755243193358183\n",
      "Epoch 3034, Loss: 0.032299754209816456, Final Batch Loss: 0.023601902648806572\n",
      "Epoch 3035, Loss: 0.018232393078505993, Final Batch Loss: 0.004500675946474075\n",
      "Epoch 3036, Loss: 0.07030265359207988, Final Batch Loss: 0.0633627399802208\n",
      "Epoch 3037, Loss: 0.02077019098214805, Final Batch Loss: 0.003880127100273967\n",
      "Epoch 3038, Loss: 0.02468685805797577, Final Batch Loss: 0.012088927440345287\n",
      "Epoch 3039, Loss: 0.016481283586472273, Final Batch Loss: 0.0028615337796509266\n",
      "Epoch 3040, Loss: 0.00916752079501748, Final Batch Loss: 0.006528072990477085\n",
      "Epoch 3041, Loss: 0.017326392233371735, Final Batch Loss: 0.013907665386795998\n",
      "Epoch 3042, Loss: 0.005782430758699775, Final Batch Loss: 0.0033997544087469578\n",
      "Epoch 3043, Loss: 0.005689471727237105, Final Batch Loss: 0.0030565119814127684\n",
      "Epoch 3044, Loss: 0.01906316983513534, Final Batch Loss: 0.015823353081941605\n",
      "Epoch 3045, Loss: 0.007723985007032752, Final Batch Loss: 0.0034376441035419703\n",
      "Epoch 3046, Loss: 0.016029944643378258, Final Batch Loss: 0.006947789341211319\n",
      "Epoch 3047, Loss: 0.007070028921589255, Final Batch Loss: 0.004455192945897579\n",
      "Epoch 3048, Loss: 0.0042301754001528025, Final Batch Loss: 0.002258949214592576\n",
      "Epoch 3049, Loss: 0.005993154365569353, Final Batch Loss: 0.002762468997389078\n",
      "Epoch 3050, Loss: 0.0104469102807343, Final Batch Loss: 0.007879884913563728\n",
      "Epoch 3051, Loss: 0.012333387043327093, Final Batch Loss: 0.0054384516552090645\n",
      "Epoch 3052, Loss: 0.02669224701821804, Final Batch Loss: 0.013396238908171654\n",
      "Epoch 3053, Loss: 0.06598905264399946, Final Batch Loss: 0.06308829039335251\n",
      "Epoch 3054, Loss: 0.014430254697799683, Final Batch Loss: 0.007411808241158724\n",
      "Epoch 3055, Loss: 0.025278611108660698, Final Batch Loss: 0.0038681719452142715\n",
      "Epoch 3056, Loss: 0.00916590727865696, Final Batch Loss: 0.0036477334797382355\n",
      "Epoch 3057, Loss: 0.04605697188526392, Final Batch Loss: 0.0028117867186665535\n",
      "Epoch 3058, Loss: 0.017407444305717945, Final Batch Loss: 0.008366864174604416\n",
      "Epoch 3059, Loss: 0.006722803460434079, Final Batch Loss: 0.0029883929528295994\n",
      "Epoch 3060, Loss: 0.0850514518097043, Final Batch Loss: 0.07460716366767883\n",
      "Epoch 3061, Loss: 0.01791450008749962, Final Batch Loss: 0.00854016188532114\n",
      "Epoch 3062, Loss: 0.024375563953071833, Final Batch Loss: 0.00728279585018754\n",
      "Epoch 3063, Loss: 0.04962543770670891, Final Batch Loss: 0.041384387761354446\n",
      "Epoch 3064, Loss: 0.016928374767303467, Final Batch Loss: 0.005451745353639126\n",
      "Epoch 3065, Loss: 0.02479846542701125, Final Batch Loss: 0.0030368338339030743\n",
      "Epoch 3066, Loss: 0.017426891718059778, Final Batch Loss: 0.005631912034004927\n",
      "Epoch 3067, Loss: 0.011128955753520131, Final Batch Loss: 0.0037051967810839415\n",
      "Epoch 3068, Loss: 0.024338235845789313, Final Batch Loss: 0.00288896425627172\n",
      "Epoch 3069, Loss: 0.013424288248643279, Final Batch Loss: 0.0032754584681242704\n",
      "Epoch 3070, Loss: 0.013399702496826649, Final Batch Loss: 0.003193761222064495\n",
      "Epoch 3071, Loss: 0.006477121030911803, Final Batch Loss: 0.0023241552989929914\n",
      "Epoch 3072, Loss: 0.014250587206333876, Final Batch Loss: 0.0034979931078851223\n",
      "Epoch 3073, Loss: 0.02280242380220443, Final Batch Loss: 0.0015015212120488286\n",
      "Epoch 3074, Loss: 0.024146312847733498, Final Batch Loss: 0.0123007632791996\n",
      "Epoch 3075, Loss: 0.01541204727254808, Final Batch Loss: 0.0018877123948186636\n",
      "Epoch 3076, Loss: 0.009047842351719737, Final Batch Loss: 0.0030143822077661753\n",
      "Epoch 3077, Loss: 0.05652473494410515, Final Batch Loss: 0.05163111165165901\n",
      "Epoch 3078, Loss: 0.014046215917915106, Final Batch Loss: 0.009062550961971283\n",
      "Epoch 3079, Loss: 0.0283053289167583, Final Batch Loss: 0.0030793393962085247\n",
      "Epoch 3080, Loss: 0.03268967242911458, Final Batch Loss: 0.007120676804333925\n",
      "Epoch 3081, Loss: 0.023100254591554403, Final Batch Loss: 0.01672171615064144\n",
      "Epoch 3082, Loss: 0.0044773044064641, Final Batch Loss: 0.0016394909471273422\n",
      "Epoch 3083, Loss: 0.013180835172533989, Final Batch Loss: 0.005502600688487291\n",
      "Epoch 3084, Loss: 0.04846469685435295, Final Batch Loss: 0.03966701403260231\n",
      "Epoch 3085, Loss: 0.016453713877126575, Final Batch Loss: 0.003096171421930194\n",
      "Epoch 3086, Loss: 0.030261372216045856, Final Batch Loss: 0.027618104591965675\n",
      "Epoch 3087, Loss: 0.029758750461041927, Final Batch Loss: 0.005387456156313419\n",
      "Epoch 3088, Loss: 0.015290497336536646, Final Batch Loss: 0.004516787361353636\n",
      "Epoch 3089, Loss: 0.011759957764297724, Final Batch Loss: 0.004269941244274378\n",
      "Epoch 3090, Loss: 0.010252987500280142, Final Batch Loss: 0.0024396576918661594\n",
      "Epoch 3091, Loss: 0.04470730316825211, Final Batch Loss: 0.04271078109741211\n",
      "Epoch 3092, Loss: 0.01983010768890381, Final Batch Loss: 0.005913830362260342\n",
      "Epoch 3093, Loss: 0.015353554394096136, Final Batch Loss: 0.007170441094785929\n",
      "Epoch 3094, Loss: 0.014074062695726752, Final Batch Loss: 0.011420504190027714\n",
      "Epoch 3095, Loss: 0.018066771095618606, Final Batch Loss: 0.0031535185407847166\n",
      "Epoch 3096, Loss: 0.025379901751875877, Final Batch Loss: 0.02037627249956131\n",
      "Epoch 3097, Loss: 0.04784880392253399, Final Batch Loss: 0.026182299479842186\n",
      "Epoch 3098, Loss: 0.01776109402999282, Final Batch Loss: 0.006964247208088636\n",
      "Epoch 3099, Loss: 0.04568863566964865, Final Batch Loss: 0.03622618690133095\n",
      "Epoch 3100, Loss: 0.005574469221755862, Final Batch Loss: 0.0028393438551574945\n",
      "Epoch 3101, Loss: 0.030509610660374165, Final Batch Loss: 0.016788767650723457\n",
      "Epoch 3102, Loss: 0.0704144611954689, Final Batch Loss: 0.017905399203300476\n",
      "Epoch 3103, Loss: 0.00912146782502532, Final Batch Loss: 0.0027548493817448616\n",
      "Epoch 3104, Loss: 0.01592516480013728, Final Batch Loss: 0.0032355166040360928\n",
      "Epoch 3105, Loss: 0.02148147625848651, Final Batch Loss: 0.0075989230535924435\n",
      "Epoch 3106, Loss: 0.005791571456938982, Final Batch Loss: 0.0033568008802831173\n",
      "Epoch 3107, Loss: 0.007205192465335131, Final Batch Loss: 0.0029514040797948837\n",
      "Epoch 3108, Loss: 0.03150334721431136, Final Batch Loss: 0.02709941379725933\n",
      "Epoch 3109, Loss: 0.008845311822369695, Final Batch Loss: 0.006064044311642647\n",
      "Epoch 3110, Loss: 0.00771613116376102, Final Batch Loss: 0.0033373984042555094\n",
      "Epoch 3111, Loss: 0.012869990430772305, Final Batch Loss: 0.009559720754623413\n",
      "Epoch 3112, Loss: 0.014706176239997149, Final Batch Loss: 0.008704992942512035\n",
      "Epoch 3113, Loss: 0.013369925320148468, Final Batch Loss: 0.010137146338820457\n",
      "Epoch 3114, Loss: 0.015651765279471874, Final Batch Loss: 0.00500818807631731\n",
      "Epoch 3115, Loss: 0.014863535761833191, Final Batch Loss: 0.009063049219548702\n",
      "Epoch 3116, Loss: 0.015264958841726184, Final Batch Loss: 0.011771163903176785\n",
      "Epoch 3117, Loss: 0.017354768933728337, Final Batch Loss: 0.002813298488035798\n",
      "Epoch 3118, Loss: 0.04261346999555826, Final Batch Loss: 0.031359683722257614\n",
      "Epoch 3119, Loss: 0.06505016423761845, Final Batch Loss: 0.04464687407016754\n",
      "Epoch 3120, Loss: 0.010219063144177198, Final Batch Loss: 0.003835495561361313\n",
      "Epoch 3121, Loss: 0.02727239695377648, Final Batch Loss: 0.002854448975995183\n",
      "Epoch 3122, Loss: 0.011464681243523955, Final Batch Loss: 0.0032241817098110914\n",
      "Epoch 3123, Loss: 0.036601172760128975, Final Batch Loss: 0.008658522740006447\n",
      "Epoch 3124, Loss: 0.03267684578895569, Final Batch Loss: 0.0182108823210001\n",
      "Epoch 3125, Loss: 0.010465638246387243, Final Batch Loss: 0.004929845687001944\n",
      "Epoch 3126, Loss: 0.05097943730652332, Final Batch Loss: 0.031358033418655396\n",
      "Epoch 3127, Loss: 0.03771480778232217, Final Batch Loss: 0.006258341018110514\n",
      "Epoch 3128, Loss: 0.014936776831746101, Final Batch Loss: 0.010540448129177094\n",
      "Epoch 3129, Loss: 0.019075541757047176, Final Batch Loss: 0.007713058032095432\n",
      "Epoch 3130, Loss: 0.03451112797483802, Final Batch Loss: 0.006917435210198164\n",
      "Epoch 3131, Loss: 0.04567494196817279, Final Batch Loss: 0.006171591114252806\n",
      "Epoch 3132, Loss: 0.019558489322662354, Final Batch Loss: 0.00950999278575182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3133, Loss: 0.01641875342465937, Final Batch Loss: 0.002610843861475587\n",
      "Epoch 3134, Loss: 0.008182102348655462, Final Batch Loss: 0.002127942629158497\n",
      "Epoch 3135, Loss: 0.00930213020183146, Final Batch Loss: 0.0017540103290230036\n",
      "Epoch 3136, Loss: 0.007299957564100623, Final Batch Loss: 0.0021257244516164064\n",
      "Epoch 3137, Loss: 0.013032934628427029, Final Batch Loss: 0.0089764054864645\n",
      "Epoch 3138, Loss: 0.015496816951781511, Final Batch Loss: 0.003676696214824915\n",
      "Epoch 3139, Loss: 0.0293848286382854, Final Batch Loss: 0.02416199818253517\n",
      "Epoch 3140, Loss: 0.014687882736325264, Final Batch Loss: 0.012678999453783035\n",
      "Epoch 3141, Loss: 0.055358825251460075, Final Batch Loss: 0.027462927624583244\n",
      "Epoch 3142, Loss: 0.038996235467493534, Final Batch Loss: 0.012291708029806614\n",
      "Epoch 3143, Loss: 0.0452794274315238, Final Batch Loss: 0.0307230856269598\n",
      "Epoch 3144, Loss: 0.020980938337743282, Final Batch Loss: 0.016755999997258186\n",
      "Epoch 3145, Loss: 0.012039925437420607, Final Batch Loss: 0.0026335990987718105\n",
      "Epoch 3146, Loss: 0.005169303622096777, Final Batch Loss: 0.0019388212822377682\n",
      "Epoch 3147, Loss: 0.02295493893325329, Final Batch Loss: 0.011185022071003914\n",
      "Epoch 3148, Loss: 0.07420646864920855, Final Batch Loss: 0.06462549418210983\n",
      "Epoch 3149, Loss: 0.00859124562703073, Final Batch Loss: 0.00288429600186646\n",
      "Epoch 3150, Loss: 0.026983886025846004, Final Batch Loss: 0.00508966576308012\n",
      "Epoch 3151, Loss: 0.03420889377593994, Final Batch Loss: 0.01566818729043007\n",
      "Epoch 3152, Loss: 0.017827852512709796, Final Batch Loss: 0.001100377063266933\n",
      "Epoch 3153, Loss: 0.01983047090470791, Final Batch Loss: 0.0027487874031066895\n",
      "Epoch 3154, Loss: 0.054015268571674824, Final Batch Loss: 0.04252679646015167\n",
      "Epoch 3155, Loss: 0.017802380491048098, Final Batch Loss: 0.005098762456327677\n",
      "Epoch 3156, Loss: 0.04320709081366658, Final Batch Loss: 0.03912725672125816\n",
      "Epoch 3157, Loss: 0.016866868594661355, Final Batch Loss: 0.0030723579693585634\n",
      "Epoch 3158, Loss: 0.043023597449064255, Final Batch Loss: 0.018190493807196617\n",
      "Epoch 3159, Loss: 0.042248932644724846, Final Batch Loss: 0.008278945460915565\n",
      "Epoch 3160, Loss: 0.019387895823456347, Final Batch Loss: 0.001318311900831759\n",
      "Epoch 3161, Loss: 0.010537799447774887, Final Batch Loss: 0.004624190274626017\n",
      "Epoch 3162, Loss: 0.038151150569319725, Final Batch Loss: 0.0233062282204628\n",
      "Epoch 3163, Loss: 0.009201316628605127, Final Batch Loss: 0.004773824941366911\n",
      "Epoch 3164, Loss: 0.02917134470771998, Final Batch Loss: 0.0017596156103536487\n",
      "Epoch 3165, Loss: 0.01668304856866598, Final Batch Loss: 0.006423628889024258\n",
      "Epoch 3166, Loss: 0.030612171161919832, Final Batch Loss: 0.023639442399144173\n",
      "Epoch 3167, Loss: 0.05144037865102291, Final Batch Loss: 0.010317722335457802\n",
      "Epoch 3168, Loss: 0.015771282371133566, Final Batch Loss: 0.009716578759253025\n",
      "Epoch 3169, Loss: 0.020388922304846346, Final Batch Loss: 0.01867872290313244\n",
      "Epoch 3170, Loss: 0.01979221496731043, Final Batch Loss: 0.009780587628483772\n",
      "Epoch 3171, Loss: 0.02654050989076495, Final Batch Loss: 0.0035655885003507137\n",
      "Epoch 3172, Loss: 0.0120765152387321, Final Batch Loss: 0.0040653967298567295\n",
      "Epoch 3173, Loss: 0.02544786734506488, Final Batch Loss: 0.024075573310256004\n",
      "Epoch 3174, Loss: 0.02131176646798849, Final Batch Loss: 0.017579887062311172\n",
      "Epoch 3175, Loss: 0.011262448620982468, Final Batch Loss: 0.0010014978470280766\n",
      "Epoch 3176, Loss: 0.0065014895517379045, Final Batch Loss: 0.0030775463674217463\n",
      "Epoch 3177, Loss: 0.002727162675000727, Final Batch Loss: 0.0012810253538191319\n",
      "Epoch 3178, Loss: 0.005603263387456536, Final Batch Loss: 0.0032839446794241667\n",
      "Epoch 3179, Loss: 0.025222165510058403, Final Batch Loss: 0.012934458442032337\n",
      "Epoch 3180, Loss: 0.007721650414168835, Final Batch Loss: 0.0021512494422495365\n",
      "Epoch 3181, Loss: 0.0872897170484066, Final Batch Loss: 0.043423689901828766\n",
      "Epoch 3182, Loss: 0.010322657646611333, Final Batch Loss: 0.0023882112000137568\n",
      "Epoch 3183, Loss: 0.026993079343810678, Final Batch Loss: 0.0023872817400842905\n",
      "Epoch 3184, Loss: 0.015424501150846481, Final Batch Loss: 0.004725680686533451\n",
      "Epoch 3185, Loss: 0.05118735879659653, Final Batch Loss: 0.008279230445623398\n",
      "Epoch 3186, Loss: 0.01170735596679151, Final Batch Loss: 0.009303861297667027\n",
      "Epoch 3187, Loss: 0.022081132046878338, Final Batch Loss: 0.01298136543482542\n",
      "Epoch 3188, Loss: 0.023873821133747697, Final Batch Loss: 0.003734179073944688\n",
      "Epoch 3189, Loss: 0.009569100802764297, Final Batch Loss: 0.00585492979735136\n",
      "Epoch 3190, Loss: 0.006936928955838084, Final Batch Loss: 0.002514627994969487\n",
      "Epoch 3191, Loss: 0.008166205137968063, Final Batch Loss: 0.0069123730063438416\n",
      "Epoch 3192, Loss: 0.029294347390532494, Final Batch Loss: 0.020767059177160263\n",
      "Epoch 3193, Loss: 0.032425215002149343, Final Batch Loss: 0.0046128383837640285\n",
      "Epoch 3194, Loss: 0.02816420281305909, Final Batch Loss: 0.024418242275714874\n",
      "Epoch 3195, Loss: 0.03657743940129876, Final Batch Loss: 0.03259860351681709\n",
      "Epoch 3196, Loss: 0.04195473808795214, Final Batch Loss: 0.031058598309755325\n",
      "Epoch 3197, Loss: 0.0889202356338501, Final Batch Loss: 0.03754078224301338\n",
      "Epoch 3198, Loss: 0.02371111698448658, Final Batch Loss: 0.015279200859367847\n",
      "Epoch 3199, Loss: 0.010608057025820017, Final Batch Loss: 0.0074612791649997234\n",
      "Epoch 3200, Loss: 0.014342968817800283, Final Batch Loss: 0.007487547118216753\n",
      "Epoch 3201, Loss: 0.03611693368293345, Final Batch Loss: 0.0026611110661178827\n",
      "Epoch 3202, Loss: 0.026550114154815674, Final Batch Loss: 0.015129396691918373\n",
      "Epoch 3203, Loss: 0.03326076827943325, Final Batch Loss: 0.01775798201560974\n",
      "Epoch 3204, Loss: 0.05165359377861023, Final Batch Loss: 0.035559091717004776\n",
      "Epoch 3205, Loss: 0.08770918752998114, Final Batch Loss: 0.07642142474651337\n",
      "Epoch 3206, Loss: 0.04478861577808857, Final Batch Loss: 0.028961986303329468\n",
      "Epoch 3207, Loss: 0.07463620509952307, Final Batch Loss: 0.06035197898745537\n",
      "Epoch 3208, Loss: 0.04256218671798706, Final Batch Loss: 0.00638846680521965\n",
      "Epoch 3209, Loss: 0.08271627128124237, Final Batch Loss: 0.06363476067781448\n",
      "Epoch 3210, Loss: 0.04281989950686693, Final Batch Loss: 0.015331939794123173\n",
      "Epoch 3211, Loss: 0.09220579639077187, Final Batch Loss: 0.05882049724459648\n",
      "Epoch 3212, Loss: 0.04654848389327526, Final Batch Loss: 0.012027757242321968\n",
      "Epoch 3213, Loss: 0.029191853012889624, Final Batch Loss: 0.0252508707344532\n",
      "Epoch 3214, Loss: 0.02811201848089695, Final Batch Loss: 0.01110747642815113\n",
      "Epoch 3215, Loss: 0.023569834884256124, Final Batch Loss: 0.0031239758245646954\n",
      "Epoch 3216, Loss: 0.033171726390719414, Final Batch Loss: 0.025844711810350418\n",
      "Epoch 3217, Loss: 0.11800869088619947, Final Batch Loss: 0.10328523069620132\n",
      "Epoch 3218, Loss: 0.04202864319086075, Final Batch Loss: 0.029911747202277184\n",
      "Epoch 3219, Loss: 0.1486196108162403, Final Batch Loss: 0.09343188256025314\n",
      "Epoch 3220, Loss: 0.10038925148546696, Final Batch Loss: 0.024504298344254494\n",
      "Epoch 3221, Loss: 0.1178528219461441, Final Batch Loss: 0.09332864731550217\n",
      "Epoch 3222, Loss: 0.04954083450138569, Final Batch Loss: 0.02065182290971279\n",
      "Epoch 3223, Loss: 0.15654075145721436, Final Batch Loss: 0.1015467494726181\n",
      "Epoch 3224, Loss: 0.05287753837183118, Final Batch Loss: 0.0049865772016346455\n",
      "Epoch 3225, Loss: 0.08102036779746413, Final Batch Loss: 0.006823462899774313\n",
      "Epoch 3226, Loss: 0.06077488511800766, Final Batch Loss: 0.041537802666425705\n",
      "Epoch 3227, Loss: 0.1514614149928093, Final Batch Loss: 0.07356198132038116\n",
      "Epoch 3228, Loss: 0.04961591586470604, Final Batch Loss: 0.021867332980036736\n",
      "Epoch 3229, Loss: 0.053478337824344635, Final Batch Loss: 0.033222880214452744\n",
      "Epoch 3230, Loss: 0.04493235424160957, Final Batch Loss: 0.026143867522478104\n",
      "Epoch 3231, Loss: 0.05378067120909691, Final Batch Loss: 0.03426608443260193\n",
      "Epoch 3232, Loss: 0.09149680659174919, Final Batch Loss: 0.05990716442465782\n",
      "Epoch 3233, Loss: 0.02720934245735407, Final Batch Loss: 0.011647616513073444\n",
      "Epoch 3234, Loss: 0.02136785490438342, Final Batch Loss: 0.007315118331462145\n",
      "Epoch 3235, Loss: 0.08157455362379551, Final Batch Loss: 0.025315361097455025\n",
      "Epoch 3236, Loss: 0.01866099238395691, Final Batch Loss: 0.004305019974708557\n",
      "Epoch 3237, Loss: 0.07365317456424236, Final Batch Loss: 0.055593833327293396\n",
      "Epoch 3238, Loss: 0.02150486782193184, Final Batch Loss: 0.0024197809398174286\n",
      "Epoch 3239, Loss: 0.026423297822475433, Final Batch Loss: 0.010488176718354225\n",
      "Epoch 3240, Loss: 0.030706215417012572, Final Batch Loss: 0.00355347222648561\n",
      "Epoch 3241, Loss: 0.012543801218271255, Final Batch Loss: 0.00894854124635458\n",
      "Epoch 3242, Loss: 0.04787447676062584, Final Batch Loss: 0.04176732152700424\n",
      "Epoch 3243, Loss: 0.045246610417962074, Final Batch Loss: 0.022741971537470818\n",
      "Epoch 3244, Loss: 0.026302662678062916, Final Batch Loss: 0.003948968835175037\n",
      "Epoch 3245, Loss: 0.04404270090162754, Final Batch Loss: 0.015309227630496025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3246, Loss: 0.07097156904637814, Final Batch Loss: 0.05384654924273491\n",
      "Epoch 3247, Loss: 0.0210330905392766, Final Batch Loss: 0.00917536299675703\n",
      "Epoch 3248, Loss: 0.013077467679977417, Final Batch Loss: 0.006140836048871279\n",
      "Epoch 3249, Loss: 0.05016608629375696, Final Batch Loss: 0.04103292152285576\n",
      "Epoch 3250, Loss: 0.01158517086878419, Final Batch Loss: 0.0068540978245437145\n",
      "Epoch 3251, Loss: 0.043640851974487305, Final Batch Loss: 0.006964042782783508\n",
      "Epoch 3252, Loss: 0.01367017487064004, Final Batch Loss: 0.005333328153938055\n",
      "Epoch 3253, Loss: 0.03205612115561962, Final Batch Loss: 0.0023312512785196304\n",
      "Epoch 3254, Loss: 0.018500257283449173, Final Batch Loss: 0.00982412975281477\n",
      "Epoch 3255, Loss: 0.14889622759073973, Final Batch Loss: 0.1369328498840332\n",
      "Epoch 3256, Loss: 0.015482497634366155, Final Batch Loss: 0.0023677016142755747\n",
      "Epoch 3257, Loss: 0.036289301700890064, Final Batch Loss: 0.010746990330517292\n",
      "Epoch 3258, Loss: 0.026057501323521137, Final Batch Loss: 0.009099609218537807\n",
      "Epoch 3259, Loss: 0.033240766264498234, Final Batch Loss: 0.004360099323093891\n",
      "Epoch 3260, Loss: 0.033339173533022404, Final Batch Loss: 0.006459209136664867\n",
      "Epoch 3261, Loss: 0.011460351292043924, Final Batch Loss: 0.002394957933574915\n",
      "Epoch 3262, Loss: 0.011455869302153587, Final Batch Loss: 0.0035934504121541977\n",
      "Epoch 3263, Loss: 0.011335850227624178, Final Batch Loss: 0.004823025315999985\n",
      "Epoch 3264, Loss: 0.0506784925237298, Final Batch Loss: 0.04575449973344803\n",
      "Epoch 3265, Loss: 0.0257985545322299, Final Batch Loss: 0.01782645843923092\n",
      "Epoch 3266, Loss: 0.029770415276288986, Final Batch Loss: 0.016499197110533714\n",
      "Epoch 3267, Loss: 0.013166468823328614, Final Batch Loss: 0.002591803902760148\n",
      "Epoch 3268, Loss: 0.0254879342392087, Final Batch Loss: 0.0033818865194916725\n",
      "Epoch 3269, Loss: 0.020147967152297497, Final Batch Loss: 0.012415817007422447\n",
      "Epoch 3270, Loss: 0.059584153816103935, Final Batch Loss: 0.04652462154626846\n",
      "Epoch 3271, Loss: 0.011265153996646404, Final Batch Loss: 0.004551595076918602\n",
      "Epoch 3272, Loss: 0.014673991361632943, Final Batch Loss: 0.010879660956561565\n",
      "Epoch 3273, Loss: 0.05528816021978855, Final Batch Loss: 0.02901585027575493\n",
      "Epoch 3274, Loss: 0.017431810963898897, Final Batch Loss: 0.005929280538111925\n",
      "Epoch 3275, Loss: 0.012348270509392023, Final Batch Loss: 0.009032904170453548\n",
      "Epoch 3276, Loss: 0.026276452466845512, Final Batch Loss: 0.0228398609906435\n",
      "Epoch 3277, Loss: 0.0317723136395216, Final Batch Loss: 0.010983379557728767\n",
      "Epoch 3278, Loss: 0.012979970779269934, Final Batch Loss: 0.009539236314594746\n",
      "Epoch 3279, Loss: 0.009331861510872841, Final Batch Loss: 0.005219711922109127\n",
      "Epoch 3280, Loss: 0.023994451388716698, Final Batch Loss: 0.014664954505860806\n",
      "Epoch 3281, Loss: 0.029729508329182863, Final Batch Loss: 0.0226899404078722\n",
      "Epoch 3282, Loss: 0.0166883640922606, Final Batch Loss: 0.013639985583722591\n",
      "Epoch 3283, Loss: 0.013451122678816319, Final Batch Loss: 0.005489947274327278\n",
      "Epoch 3284, Loss: 0.010129538364708424, Final Batch Loss: 0.0014923205599188805\n",
      "Epoch 3285, Loss: 0.0497959665954113, Final Batch Loss: 0.04420180991292\n",
      "Epoch 3286, Loss: 0.026874663308262825, Final Batch Loss: 0.017409155145287514\n",
      "Epoch 3287, Loss: 0.014021715149283409, Final Batch Loss: 0.0017386116087436676\n",
      "Epoch 3288, Loss: 0.005754581419751048, Final Batch Loss: 0.003338300157338381\n",
      "Epoch 3289, Loss: 0.028589324560016394, Final Batch Loss: 0.004436004441231489\n",
      "Epoch 3290, Loss: 0.011474086670204997, Final Batch Loss: 0.0035286012571305037\n",
      "Epoch 3291, Loss: 0.03960445546545088, Final Batch Loss: 0.00226756208576262\n",
      "Epoch 3292, Loss: 0.011838092468678951, Final Batch Loss: 0.0024795597419142723\n",
      "Epoch 3293, Loss: 0.04338296502828598, Final Batch Loss: 0.03061663545668125\n",
      "Epoch 3294, Loss: 0.008067569928243756, Final Batch Loss: 0.0031768318731337786\n",
      "Epoch 3295, Loss: 0.005351779866032302, Final Batch Loss: 0.0012563717318698764\n",
      "Epoch 3296, Loss: 0.013935715891420841, Final Batch Loss: 0.0037093479186296463\n",
      "Epoch 3297, Loss: 0.0349226389080286, Final Batch Loss: 0.016605358570814133\n",
      "Epoch 3298, Loss: 0.0663894647732377, Final Batch Loss: 0.06069397181272507\n",
      "Epoch 3299, Loss: 0.017239305190742016, Final Batch Loss: 0.009165910072624683\n",
      "Epoch 3300, Loss: 0.019185462733730674, Final Batch Loss: 0.0034833753015846014\n",
      "Epoch 3301, Loss: 0.013089149259030819, Final Batch Loss: 0.002134939655661583\n",
      "Epoch 3302, Loss: 0.02337563969194889, Final Batch Loss: 0.020025957375764847\n",
      "Epoch 3303, Loss: 0.027146633248776197, Final Batch Loss: 0.021366234868764877\n",
      "Epoch 3304, Loss: 0.009952559601515532, Final Batch Loss: 0.0038648382760584354\n",
      "Epoch 3305, Loss: 0.03695603460073471, Final Batch Loss: 0.0343581959605217\n",
      "Epoch 3306, Loss: 0.03531101858243346, Final Batch Loss: 0.006054127123206854\n",
      "Epoch 3307, Loss: 0.025044596288353205, Final Batch Loss: 0.018583718687295914\n",
      "Epoch 3308, Loss: 0.011176040628924966, Final Batch Loss: 0.002995711052790284\n",
      "Epoch 3309, Loss: 0.01990145188756287, Final Batch Loss: 0.0038423568475991488\n",
      "Epoch 3310, Loss: 0.02141230646520853, Final Batch Loss: 0.005676389671862125\n",
      "Epoch 3311, Loss: 0.017462611198425293, Final Batch Loss: 0.006417004391551018\n",
      "Epoch 3312, Loss: 0.008760145865380764, Final Batch Loss: 0.004405204672366381\n",
      "Epoch 3313, Loss: 0.012460841564461589, Final Batch Loss: 0.010111498646438122\n",
      "Epoch 3314, Loss: 0.027134348638355732, Final Batch Loss: 0.021822193637490273\n",
      "Epoch 3315, Loss: 0.0152635732665658, Final Batch Loss: 0.0029515987262129784\n",
      "Epoch 3316, Loss: 0.06825082842260599, Final Batch Loss: 0.052716270089149475\n",
      "Epoch 3317, Loss: 0.005854844697751105, Final Batch Loss: 0.0019515597959980369\n",
      "Epoch 3318, Loss: 0.03832572977989912, Final Batch Loss: 0.032811712473630905\n",
      "Epoch 3319, Loss: 0.03985258564352989, Final Batch Loss: 0.02207452990114689\n",
      "Epoch 3320, Loss: 0.039594273548573256, Final Batch Loss: 0.03476561978459358\n",
      "Epoch 3321, Loss: 0.00678204873111099, Final Batch Loss: 0.00523712020367384\n",
      "Epoch 3322, Loss: 0.037979429587721825, Final Batch Loss: 0.015474166721105576\n",
      "Epoch 3323, Loss: 0.018308718921616673, Final Batch Loss: 0.0014653850812464952\n",
      "Epoch 3324, Loss: 0.013168177101761103, Final Batch Loss: 0.002669531386345625\n",
      "Epoch 3325, Loss: 0.011552711250260472, Final Batch Loss: 0.0019783873576670885\n",
      "Epoch 3326, Loss: 0.01273082778789103, Final Batch Loss: 0.0036942234728485346\n",
      "Epoch 3327, Loss: 0.042148602195084095, Final Batch Loss: 0.006082897074520588\n",
      "Epoch 3328, Loss: 0.007108801510185003, Final Batch Loss: 0.0022894986905157566\n",
      "Epoch 3329, Loss: 0.00812186929397285, Final Batch Loss: 0.005772117059677839\n",
      "Epoch 3330, Loss: 0.025992017006501555, Final Batch Loss: 0.003306137165054679\n",
      "Epoch 3331, Loss: 0.019683898892253637, Final Batch Loss: 0.00697286007925868\n",
      "Epoch 3332, Loss: 0.006368683883920312, Final Batch Loss: 0.0033639310859143734\n",
      "Epoch 3333, Loss: 0.03116255858913064, Final Batch Loss: 0.0013138861395418644\n",
      "Epoch 3334, Loss: 0.01801918400451541, Final Batch Loss: 0.01026605349034071\n",
      "Epoch 3335, Loss: 0.0243300823494792, Final Batch Loss: 0.01354514341801405\n",
      "Epoch 3336, Loss: 0.027747374959290028, Final Batch Loss: 0.007856725715100765\n",
      "Epoch 3337, Loss: 0.012903565540909767, Final Batch Loss: 0.0041763633489608765\n",
      "Epoch 3338, Loss: 0.01401000190526247, Final Batch Loss: 0.009014338254928589\n",
      "Epoch 3339, Loss: 0.007356628309935331, Final Batch Loss: 0.0024004434235394\n",
      "Epoch 3340, Loss: 0.032542115077376366, Final Batch Loss: 0.019902093335986137\n",
      "Epoch 3341, Loss: 0.009770062752068043, Final Batch Loss: 0.005483441054821014\n",
      "Epoch 3342, Loss: 0.02065361081622541, Final Batch Loss: 0.003465678310021758\n",
      "Epoch 3343, Loss: 0.006582490401342511, Final Batch Loss: 0.00466058449819684\n",
      "Epoch 3344, Loss: 0.019443464349023998, Final Batch Loss: 0.017611315473914146\n",
      "Epoch 3345, Loss: 0.013060772325843573, Final Batch Loss: 0.006897233426570892\n",
      "Epoch 3346, Loss: 0.02089693956077099, Final Batch Loss: 0.007292814552783966\n",
      "Epoch 3347, Loss: 0.031159027013927698, Final Batch Loss: 0.026134435087442398\n",
      "Epoch 3348, Loss: 0.008030835073441267, Final Batch Loss: 0.0048472206108272076\n",
      "Epoch 3349, Loss: 0.03653245232999325, Final Batch Loss: 0.01391034759581089\n",
      "Epoch 3350, Loss: 0.011479747481644154, Final Batch Loss: 0.008745058439671993\n",
      "Epoch 3351, Loss: 0.02568557020276785, Final Batch Loss: 0.018170097842812538\n",
      "Epoch 3352, Loss: 0.01669168844819069, Final Batch Loss: 0.011042911559343338\n",
      "Epoch 3353, Loss: 0.006658290047198534, Final Batch Loss: 0.0020808978006243706\n",
      "Epoch 3354, Loss: 0.06657927110791206, Final Batch Loss: 0.019884563982486725\n",
      "Epoch 3355, Loss: 0.011163094313815236, Final Batch Loss: 0.003244200022891164\n",
      "Epoch 3356, Loss: 0.03479547891765833, Final Batch Loss: 0.012361540459096432\n",
      "Epoch 3357, Loss: 0.013468442717567086, Final Batch Loss: 0.0014542436692863703\n",
      "Epoch 3358, Loss: 0.012933674501255155, Final Batch Loss: 0.0029155772645026445\n",
      "Epoch 3359, Loss: 0.025617291685193777, Final Batch Loss: 0.020120661705732346\n",
      "Epoch 3360, Loss: 0.045622136211022735, Final Batch Loss: 0.0038963330443948507\n",
      "Epoch 3361, Loss: 0.025714170886203647, Final Batch Loss: 0.0024329840671271086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3362, Loss: 0.01392429182305932, Final Batch Loss: 0.0024806815199553967\n",
      "Epoch 3363, Loss: 0.009700984694063663, Final Batch Loss: 0.0042892820201814175\n",
      "Epoch 3364, Loss: 0.006346799666061997, Final Batch Loss: 0.0022268539760261774\n",
      "Epoch 3365, Loss: 0.025460459291934967, Final Batch Loss: 0.01020055916160345\n",
      "Epoch 3366, Loss: 0.006569086108356714, Final Batch Loss: 0.003988573327660561\n",
      "Epoch 3367, Loss: 0.04838562244549394, Final Batch Loss: 0.0013807700015604496\n",
      "Epoch 3368, Loss: 0.008690102025866508, Final Batch Loss: 0.0005336804315447807\n",
      "Epoch 3369, Loss: 0.04847627505660057, Final Batch Loss: 0.033956386148929596\n",
      "Epoch 3370, Loss: 0.01893474394455552, Final Batch Loss: 0.0031589982099831104\n",
      "Epoch 3371, Loss: 0.007610745262354612, Final Batch Loss: 0.004569036886096001\n",
      "Epoch 3372, Loss: 0.017447575461119413, Final Batch Loss: 0.013489624485373497\n",
      "Epoch 3373, Loss: 0.011248781811445951, Final Batch Loss: 0.008127696812152863\n",
      "Epoch 3374, Loss: 0.009261250030249357, Final Batch Loss: 0.004483240190893412\n",
      "Epoch 3375, Loss: 0.019160480936989188, Final Batch Loss: 0.016010859981179237\n",
      "Epoch 3376, Loss: 0.008527907193638384, Final Batch Loss: 0.0018742840038612485\n",
      "Epoch 3377, Loss: 0.014194943942129612, Final Batch Loss: 0.0019626272842288017\n",
      "Epoch 3378, Loss: 0.004624479333870113, Final Batch Loss: 0.0014332140563055873\n",
      "Epoch 3379, Loss: 0.016220718156546354, Final Batch Loss: 0.004657499026507139\n",
      "Epoch 3380, Loss: 0.0040380455320701, Final Batch Loss: 0.0017186951590701938\n",
      "Epoch 3381, Loss: 0.02156916749663651, Final Batch Loss: 0.020130915567278862\n",
      "Epoch 3382, Loss: 0.011249214876443148, Final Batch Loss: 0.002731401938945055\n",
      "Epoch 3383, Loss: 0.019000417087227106, Final Batch Loss: 0.011956857517361641\n",
      "Epoch 3384, Loss: 0.041811585426330566, Final Batch Loss: 0.02613396756350994\n",
      "Epoch 3385, Loss: 0.017060288228094578, Final Batch Loss: 0.01499180682003498\n",
      "Epoch 3386, Loss: 0.006498189410194755, Final Batch Loss: 0.0025981441140174866\n",
      "Epoch 3387, Loss: 0.013834254816174507, Final Batch Loss: 0.0019721398130059242\n",
      "Epoch 3388, Loss: 0.04561258526518941, Final Batch Loss: 0.00319457845762372\n",
      "Epoch 3389, Loss: 0.03015320561826229, Final Batch Loss: 0.00988992489874363\n",
      "Epoch 3390, Loss: 0.012135019525885582, Final Batch Loss: 0.008250313811004162\n",
      "Epoch 3391, Loss: 0.05894480366259813, Final Batch Loss: 0.04584409296512604\n",
      "Epoch 3392, Loss: 0.022899898700416088, Final Batch Loss: 0.009112040512263775\n",
      "Epoch 3393, Loss: 0.05392496008425951, Final Batch Loss: 0.041859496384859085\n",
      "Epoch 3394, Loss: 0.03516502911224961, Final Batch Loss: 0.030588088557124138\n",
      "Epoch 3395, Loss: 0.06882918439805508, Final Batch Loss: 0.020084844902157784\n",
      "Epoch 3396, Loss: 0.033029310405254364, Final Batch Loss: 0.008151102811098099\n",
      "Epoch 3397, Loss: 0.057624613866209984, Final Batch Loss: 0.03506366163492203\n",
      "Epoch 3398, Loss: 0.020541652105748653, Final Batch Loss: 0.002524777315557003\n",
      "Epoch 3399, Loss: 0.030302569270133972, Final Batch Loss: 0.01556727197021246\n",
      "Epoch 3400, Loss: 0.05610691010951996, Final Batch Loss: 0.032113026827573776\n",
      "Epoch 3401, Loss: 0.03430987196043134, Final Batch Loss: 0.00659985700622201\n",
      "Epoch 3402, Loss: 0.08806024119257927, Final Batch Loss: 0.03720909357070923\n",
      "Epoch 3403, Loss: 0.0985850915312767, Final Batch Loss: 0.03458233177661896\n",
      "Epoch 3404, Loss: 0.06950921565294266, Final Batch Loss: 0.01555672287940979\n",
      "Epoch 3405, Loss: 0.10596292838454247, Final Batch Loss: 0.054563313722610474\n",
      "Epoch 3406, Loss: 0.08621373772621155, Final Batch Loss: 0.051991719752550125\n",
      "Epoch 3407, Loss: 0.03911566734313965, Final Batch Loss: 0.023212973028421402\n",
      "Epoch 3408, Loss: 0.0632300116121769, Final Batch Loss: 0.018967527896165848\n",
      "Epoch 3409, Loss: 0.04387570358812809, Final Batch Loss: 0.019662868231534958\n",
      "Epoch 3410, Loss: 0.05214463919401169, Final Batch Loss: 0.01957612857222557\n",
      "Epoch 3411, Loss: 0.03371422179043293, Final Batch Loss: 0.023384444415569305\n",
      "Epoch 3412, Loss: 0.032061874866485596, Final Batch Loss: 0.0067370738834142685\n",
      "Epoch 3413, Loss: 0.04601290449500084, Final Batch Loss: 0.02278277277946472\n",
      "Epoch 3414, Loss: 0.08067085035145283, Final Batch Loss: 0.02151779644191265\n",
      "Epoch 3415, Loss: 0.054516417905688286, Final Batch Loss: 0.013502834364771843\n",
      "Epoch 3416, Loss: 0.04399032611399889, Final Batch Loss: 0.03155429661273956\n",
      "Epoch 3417, Loss: 0.03604678437113762, Final Batch Loss: 0.020560866221785545\n",
      "Epoch 3418, Loss: 0.032288942486047745, Final Batch Loss: 0.018985850736498833\n",
      "Epoch 3419, Loss: 0.03600975917652249, Final Batch Loss: 0.004311822820454836\n",
      "Epoch 3420, Loss: 0.09396834671497345, Final Batch Loss: 0.054222915321588516\n",
      "Epoch 3421, Loss: 0.03729068674147129, Final Batch Loss: 0.019747043028473854\n",
      "Epoch 3422, Loss: 0.05308370664715767, Final Batch Loss: 0.019864335656166077\n",
      "Epoch 3423, Loss: 0.041831133887171745, Final Batch Loss: 0.02637842856347561\n",
      "Epoch 3424, Loss: 0.03414137940853834, Final Batch Loss: 0.015359715558588505\n",
      "Epoch 3425, Loss: 0.03819063398987055, Final Batch Loss: 0.0016414755955338478\n",
      "Epoch 3426, Loss: 0.02824652288109064, Final Batch Loss: 0.013264261186122894\n",
      "Epoch 3427, Loss: 0.052494161762297153, Final Batch Loss: 0.010750562883913517\n",
      "Epoch 3428, Loss: 0.02926434064283967, Final Batch Loss: 0.003884787205606699\n",
      "Epoch 3429, Loss: 0.03392935451120138, Final Batch Loss: 0.00837960746139288\n",
      "Epoch 3430, Loss: 0.08385805413126945, Final Batch Loss: 0.037023670971393585\n",
      "Epoch 3431, Loss: 0.011971079744398594, Final Batch Loss: 0.002137969247996807\n",
      "Epoch 3432, Loss: 0.06273640505969524, Final Batch Loss: 0.023027757182717323\n",
      "Epoch 3433, Loss: 0.06421582587063313, Final Batch Loss: 0.028968749567866325\n",
      "Epoch 3434, Loss: 0.04574140580371022, Final Batch Loss: 0.0072457981295883656\n",
      "Epoch 3435, Loss: 0.05917489901185036, Final Batch Loss: 0.048091255128383636\n",
      "Epoch 3436, Loss: 0.042016803519800305, Final Batch Loss: 0.0025051080156117678\n",
      "Epoch 3437, Loss: 0.03133757133036852, Final Batch Loss: 0.008721291087567806\n",
      "Epoch 3438, Loss: 0.0668879859149456, Final Batch Loss: 0.05294894054532051\n",
      "Epoch 3439, Loss: 0.08487726747989655, Final Batch Loss: 0.03585191071033478\n",
      "Epoch 3440, Loss: 0.04220060631632805, Final Batch Loss: 0.029029326513409615\n",
      "Epoch 3441, Loss: 0.04420084320008755, Final Batch Loss: 0.028358489274978638\n",
      "Epoch 3442, Loss: 0.09699630923569202, Final Batch Loss: 0.07666421681642532\n",
      "Epoch 3443, Loss: 0.017967806197702885, Final Batch Loss: 0.006474267691373825\n",
      "Epoch 3444, Loss: 0.059441929683089256, Final Batch Loss: 0.016469119116663933\n",
      "Epoch 3445, Loss: 0.03071757312864065, Final Batch Loss: 0.01556373666971922\n",
      "Epoch 3446, Loss: 0.03263778705149889, Final Batch Loss: 0.009168227203190327\n",
      "Epoch 3447, Loss: 0.011499116662889719, Final Batch Loss: 0.00425307359546423\n",
      "Epoch 3448, Loss: 0.02419039746746421, Final Batch Loss: 0.016688313335180283\n",
      "Epoch 3449, Loss: 0.01292436383664608, Final Batch Loss: 0.005681834649294615\n",
      "Epoch 3450, Loss: 0.03497823141515255, Final Batch Loss: 0.0035984311252832413\n",
      "Epoch 3451, Loss: 0.036477800691500306, Final Batch Loss: 0.0025478454772382975\n",
      "Epoch 3452, Loss: 0.06397069245576859, Final Batch Loss: 0.05386187508702278\n",
      "Epoch 3453, Loss: 0.12500115297734737, Final Batch Loss: 0.10468905419111252\n",
      "Epoch 3454, Loss: 0.03969047777354717, Final Batch Loss: 0.012823224067687988\n",
      "Epoch 3455, Loss: 0.012994855642318726, Final Batch Loss: 0.0030470946803689003\n",
      "Epoch 3456, Loss: 0.019263414666056633, Final Batch Loss: 0.007252580486238003\n",
      "Epoch 3457, Loss: 0.018013885244727135, Final Batch Loss: 0.00792929157614708\n",
      "Epoch 3458, Loss: 0.012038810877129436, Final Batch Loss: 0.002872395096346736\n",
      "Epoch 3459, Loss: 0.014379408676177263, Final Batch Loss: 0.003088881727308035\n",
      "Epoch 3460, Loss: 0.012979988940060139, Final Batch Loss: 0.007932540960609913\n",
      "Epoch 3461, Loss: 0.05743019934743643, Final Batch Loss: 0.04805050790309906\n",
      "Epoch 3462, Loss: 0.015697758179157972, Final Batch Loss: 0.002079243306070566\n",
      "Epoch 3463, Loss: 0.02707168913912028, Final Batch Loss: 0.00174139894079417\n",
      "Epoch 3464, Loss: 0.010794220957905054, Final Batch Loss: 0.006466752849519253\n",
      "Epoch 3465, Loss: 0.01704926905222237, Final Batch Loss: 0.014076459221541882\n",
      "Epoch 3466, Loss: 0.0363617833936587, Final Batch Loss: 0.001565206446684897\n",
      "Epoch 3467, Loss: 0.03300156071782112, Final Batch Loss: 0.02977730520069599\n",
      "Epoch 3468, Loss: 0.030306273140013218, Final Batch Loss: 0.008648918010294437\n",
      "Epoch 3469, Loss: 0.009147305274382234, Final Batch Loss: 0.0027868153993040323\n",
      "Epoch 3470, Loss: 0.027867482393048704, Final Batch Loss: 0.0013189719757065177\n",
      "Epoch 3471, Loss: 0.023702341364696622, Final Batch Loss: 0.020755011588335037\n",
      "Epoch 3472, Loss: 0.06153681501746178, Final Batch Loss: 0.030247770249843597\n",
      "Epoch 3473, Loss: 0.009358719107694924, Final Batch Loss: 0.0015466591576114297\n",
      "Epoch 3474, Loss: 0.01745307631790638, Final Batch Loss: 0.0102308951318264\n",
      "Epoch 3475, Loss: 0.028920887038111687, Final Batch Loss: 0.01929255947470665\n",
      "Epoch 3476, Loss: 0.016346503980457783, Final Batch Loss: 0.004563687369227409\n",
      "Epoch 3477, Loss: 0.007399591617286205, Final Batch Loss: 0.004116426687687635\n",
      "Epoch 3478, Loss: 0.01392653095535934, Final Batch Loss: 0.0014739304315298796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3479, Loss: 0.0063009317964315414, Final Batch Loss: 0.002039275597780943\n",
      "Epoch 3480, Loss: 0.014102581888437271, Final Batch Loss: 0.009353338740766048\n",
      "Epoch 3481, Loss: 0.03025288926437497, Final Batch Loss: 0.001712434459477663\n",
      "Epoch 3482, Loss: 0.010509662795811892, Final Batch Loss: 0.005852000322192907\n",
      "Epoch 3483, Loss: 0.008495146408677101, Final Batch Loss: 0.004295281134545803\n",
      "Epoch 3484, Loss: 0.022227336186915636, Final Batch Loss: 0.016928255558013916\n",
      "Epoch 3485, Loss: 0.01042397366836667, Final Batch Loss: 0.005146352108567953\n",
      "Epoch 3486, Loss: 0.01534233521670103, Final Batch Loss: 0.004182933829724789\n",
      "Epoch 3487, Loss: 0.004549095523543656, Final Batch Loss: 0.001366752083413303\n",
      "Epoch 3488, Loss: 0.0041943013202399015, Final Batch Loss: 0.0017477127257734537\n",
      "Epoch 3489, Loss: 0.0333634230773896, Final Batch Loss: 0.003214702708646655\n",
      "Epoch 3490, Loss: 0.0067283709067851305, Final Batch Loss: 0.002205759519711137\n",
      "Epoch 3491, Loss: 0.007261411985382438, Final Batch Loss: 0.001346855191513896\n",
      "Epoch 3492, Loss: 0.04030595254153013, Final Batch Loss: 0.032231610268354416\n",
      "Epoch 3493, Loss: 0.010061560664325953, Final Batch Loss: 0.005861201789230108\n",
      "Epoch 3494, Loss: 0.0245861760340631, Final Batch Loss: 0.021319594234228134\n",
      "Epoch 3495, Loss: 0.005114350700750947, Final Batch Loss: 0.0020621558651328087\n",
      "Epoch 3496, Loss: 0.005601615994237363, Final Batch Loss: 0.001377090229652822\n",
      "Epoch 3497, Loss: 0.013725596480071545, Final Batch Loss: 0.008188425563275814\n",
      "Epoch 3498, Loss: 0.02018964011222124, Final Batch Loss: 0.010010634548962116\n",
      "Epoch 3499, Loss: 0.0038403718499466777, Final Batch Loss: 0.001245146500878036\n",
      "Epoch 3500, Loss: 0.004369051894173026, Final Batch Loss: 0.0011300574988126755\n",
      "Epoch 3501, Loss: 0.03049566491972655, Final Batch Loss: 0.0013015613658353686\n",
      "Epoch 3502, Loss: 0.006766645237803459, Final Batch Loss: 0.003778954967856407\n",
      "Epoch 3503, Loss: 0.037066982593387365, Final Batch Loss: 0.005188507493585348\n",
      "Epoch 3504, Loss: 0.03327006474137306, Final Batch Loss: 0.01668122410774231\n",
      "Epoch 3505, Loss: 0.012273749336600304, Final Batch Loss: 0.0039108991622924805\n",
      "Epoch 3506, Loss: 0.056343553587794304, Final Batch Loss: 0.04395238310098648\n",
      "Epoch 3507, Loss: 0.08830459113232791, Final Batch Loss: 0.08509112894535065\n",
      "Epoch 3508, Loss: 0.022006502375006676, Final Batch Loss: 0.009368672035634518\n",
      "Epoch 3509, Loss: 0.018594084307551384, Final Batch Loss: 0.0028265882283449173\n",
      "Epoch 3510, Loss: 0.01794299017637968, Final Batch Loss: 0.014150412753224373\n",
      "Epoch 3511, Loss: 0.011574627831578255, Final Batch Loss: 0.0033120056614279747\n",
      "Epoch 3512, Loss: 0.014767136424779892, Final Batch Loss: 0.006746111437678337\n",
      "Epoch 3513, Loss: 0.029349897522479296, Final Batch Loss: 0.02649066410958767\n",
      "Epoch 3514, Loss: 0.025868898257613182, Final Batch Loss: 0.008876997977495193\n",
      "Epoch 3515, Loss: 0.018593461252748966, Final Batch Loss: 0.006631502881646156\n",
      "Epoch 3516, Loss: 0.004529668600298464, Final Batch Loss: 0.001380874658934772\n",
      "Epoch 3517, Loss: 0.007223283173516393, Final Batch Loss: 0.0033737558405846357\n",
      "Epoch 3518, Loss: 0.026785640511661768, Final Batch Loss: 0.007088019046932459\n",
      "Epoch 3519, Loss: 0.019761799834668636, Final Batch Loss: 0.01691972278058529\n",
      "Epoch 3520, Loss: 0.04506070353090763, Final Batch Loss: 0.03933471441268921\n",
      "Epoch 3521, Loss: 0.05577738955616951, Final Batch Loss: 0.05335415154695511\n",
      "Epoch 3522, Loss: 0.06466513127088547, Final Batch Loss: 0.024970442056655884\n",
      "Epoch 3523, Loss: 0.022500860039144754, Final Batch Loss: 0.01560178678482771\n",
      "Epoch 3524, Loss: 0.0736513240262866, Final Batch Loss: 0.06258554011583328\n",
      "Epoch 3525, Loss: 0.014811931643635035, Final Batch Loss: 0.009064339101314545\n",
      "Epoch 3526, Loss: 0.08875066041946411, Final Batch Loss: 0.08137502521276474\n",
      "Epoch 3527, Loss: 0.013018070254474878, Final Batch Loss: 0.010112570598721504\n",
      "Epoch 3528, Loss: 0.013779166154563427, Final Batch Loss: 0.002049058675765991\n",
      "Epoch 3529, Loss: 0.013623836217448115, Final Batch Loss: 0.002642241073772311\n",
      "Epoch 3530, Loss: 0.0144080794416368, Final Batch Loss: 0.0028529646806418896\n",
      "Epoch 3531, Loss: 0.026011924259364605, Final Batch Loss: 0.018637031316757202\n",
      "Epoch 3532, Loss: 0.0358656607568264, Final Batch Loss: 0.02679711952805519\n",
      "Epoch 3533, Loss: 0.003920422401279211, Final Batch Loss: 0.001962252426892519\n",
      "Epoch 3534, Loss: 0.022710860706865788, Final Batch Loss: 0.0178864523768425\n",
      "Epoch 3535, Loss: 0.07136118784546852, Final Batch Loss: 0.057945799082517624\n",
      "Epoch 3536, Loss: 0.030847738322336227, Final Batch Loss: 0.0008328210678882897\n",
      "Epoch 3537, Loss: 0.023144072853028774, Final Batch Loss: 0.008274508640170097\n",
      "Epoch 3538, Loss: 0.009426622185856104, Final Batch Loss: 0.007399951107800007\n",
      "Epoch 3539, Loss: 0.014079948421567678, Final Batch Loss: 0.007515850476920605\n",
      "Epoch 3540, Loss: 0.01784233981743455, Final Batch Loss: 0.004414692055433989\n",
      "Epoch 3541, Loss: 0.01847438281401992, Final Batch Loss: 0.013296080753207207\n",
      "Epoch 3542, Loss: 0.0058931519743055105, Final Batch Loss: 0.0034492495469748974\n",
      "Epoch 3543, Loss: 0.07718526944518089, Final Batch Loss: 0.03463245928287506\n",
      "Epoch 3544, Loss: 0.021447103936225176, Final Batch Loss: 0.005349982995539904\n",
      "Epoch 3545, Loss: 0.018434857949614525, Final Batch Loss: 0.00900465901941061\n",
      "Epoch 3546, Loss: 0.015309006441384554, Final Batch Loss: 0.005950008053332567\n",
      "Epoch 3547, Loss: 0.04389573517255485, Final Batch Loss: 0.003313555149361491\n",
      "Epoch 3548, Loss: 0.04635921772569418, Final Batch Loss: 0.03227098286151886\n",
      "Epoch 3549, Loss: 0.03061264776624739, Final Batch Loss: 0.02671731635928154\n",
      "Epoch 3550, Loss: 0.028225727379322052, Final Batch Loss: 0.016727006062865257\n",
      "Epoch 3551, Loss: 0.019281544955447316, Final Batch Loss: 0.0034959588665515184\n",
      "Epoch 3552, Loss: 0.008151771733537316, Final Batch Loss: 0.0038840833585709333\n",
      "Epoch 3553, Loss: 0.013624228071421385, Final Batch Loss: 0.002196290995925665\n",
      "Epoch 3554, Loss: 0.02936061378568411, Final Batch Loss: 0.01636078953742981\n",
      "Epoch 3555, Loss: 0.005896749906241894, Final Batch Loss: 0.003440388012677431\n",
      "Epoch 3556, Loss: 0.028317099437117577, Final Batch Loss: 0.006565965712070465\n",
      "Epoch 3557, Loss: 0.017912987619638443, Final Batch Loss: 0.005699050612747669\n",
      "Epoch 3558, Loss: 0.023031897377222776, Final Batch Loss: 0.0036329696886241436\n",
      "Epoch 3559, Loss: 0.0054435464553534985, Final Batch Loss: 0.0009898338466882706\n",
      "Epoch 3560, Loss: 0.03302533691748977, Final Batch Loss: 0.030406823381781578\n",
      "Epoch 3561, Loss: 0.020768398186191916, Final Batch Loss: 0.0020442658569663763\n",
      "Epoch 3562, Loss: 0.010892550111748278, Final Batch Loss: 0.0011411166051402688\n",
      "Epoch 3563, Loss: 0.017371878726407886, Final Batch Loss: 0.0033869899343699217\n",
      "Epoch 3564, Loss: 0.026188788004219532, Final Batch Loss: 0.02224944718182087\n",
      "Epoch 3565, Loss: 0.024461927823722363, Final Batch Loss: 0.006448258645832539\n",
      "Epoch 3566, Loss: 0.0762336514890194, Final Batch Loss: 0.06468333303928375\n",
      "Epoch 3567, Loss: 0.01943392213433981, Final Batch Loss: 0.001699189655482769\n",
      "Epoch 3568, Loss: 0.05996023118495941, Final Batch Loss: 0.05568565055727959\n",
      "Epoch 3569, Loss: 0.07516691461205482, Final Batch Loss: 0.036661140620708466\n",
      "Epoch 3570, Loss: 0.023176887771114707, Final Batch Loss: 0.0033452787902206182\n",
      "Epoch 3571, Loss: 0.012035832740366459, Final Batch Loss: 0.002654576674103737\n",
      "Epoch 3572, Loss: 0.010402081068605185, Final Batch Loss: 0.004483436234295368\n",
      "Epoch 3573, Loss: 0.02483615162782371, Final Batch Loss: 0.0019234518986195326\n",
      "Epoch 3574, Loss: 0.015176542103290558, Final Batch Loss: 0.007328484207391739\n",
      "Epoch 3575, Loss: 0.01714644953608513, Final Batch Loss: 0.004679296165704727\n",
      "Epoch 3576, Loss: 0.03991045453585684, Final Batch Loss: 0.002850685501471162\n",
      "Epoch 3577, Loss: 0.008612536592409015, Final Batch Loss: 0.003037892049178481\n",
      "Epoch 3578, Loss: 0.005487453192472458, Final Batch Loss: 0.002394632436335087\n",
      "Epoch 3579, Loss: 0.030007838271558285, Final Batch Loss: 0.026378965005278587\n",
      "Epoch 3580, Loss: 0.012037951266393065, Final Batch Loss: 0.010005904361605644\n",
      "Epoch 3581, Loss: 0.03832731582224369, Final Batch Loss: 0.01961948350071907\n",
      "Epoch 3582, Loss: 0.026683025062084198, Final Batch Loss: 0.006094217300415039\n",
      "Epoch 3583, Loss: 0.011932970490306616, Final Batch Loss: 0.002662897575646639\n",
      "Epoch 3584, Loss: 0.06956190196797252, Final Batch Loss: 0.06343308836221695\n",
      "Epoch 3585, Loss: 0.035402833484113216, Final Batch Loss: 0.022455358877778053\n",
      "Epoch 3586, Loss: 0.009461850393563509, Final Batch Loss: 0.003204687964171171\n",
      "Epoch 3587, Loss: 0.01324358768761158, Final Batch Loss: 0.008342830464243889\n",
      "Epoch 3588, Loss: 0.025755930226296186, Final Batch Loss: 0.004761822056025267\n",
      "Epoch 3589, Loss: 0.03466844744980335, Final Batch Loss: 0.004127269610762596\n",
      "Epoch 3590, Loss: 0.0076011314522475, Final Batch Loss: 0.002383367856964469\n",
      "Epoch 3591, Loss: 0.035045486874878407, Final Batch Loss: 0.006549458019435406\n",
      "Epoch 3592, Loss: 0.005613066954538226, Final Batch Loss: 0.002144664293155074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3593, Loss: 0.008011371130123734, Final Batch Loss: 0.004163084551692009\n",
      "Epoch 3594, Loss: 0.004868646152317524, Final Batch Loss: 0.0024835786316543818\n",
      "Epoch 3595, Loss: 0.017807246185839176, Final Batch Loss: 0.013815753161907196\n",
      "Epoch 3596, Loss: 0.01070060976780951, Final Batch Loss: 0.003370449645444751\n",
      "Epoch 3597, Loss: 0.029918593354523182, Final Batch Loss: 0.02418905310332775\n",
      "Epoch 3598, Loss: 0.006474935216829181, Final Batch Loss: 0.0039031291380524635\n",
      "Epoch 3599, Loss: 0.008431613212451339, Final Batch Loss: 0.0026834842283278704\n",
      "Epoch 3600, Loss: 0.015638274839147925, Final Batch Loss: 0.013173552230000496\n",
      "Epoch 3601, Loss: 0.02635761839337647, Final Batch Loss: 0.003743113251402974\n",
      "Epoch 3602, Loss: 0.00796823250129819, Final Batch Loss: 0.001978074200451374\n",
      "Epoch 3603, Loss: 0.010705202352255583, Final Batch Loss: 0.0020841150544583797\n",
      "Epoch 3604, Loss: 0.01779661374166608, Final Batch Loss: 0.011191555298864841\n",
      "Epoch 3605, Loss: 0.014957439852878451, Final Batch Loss: 0.0024102095048874617\n",
      "Epoch 3606, Loss: 0.003910352475941181, Final Batch Loss: 0.0011804916430264711\n",
      "Epoch 3607, Loss: 0.020822740625590086, Final Batch Loss: 0.0010830252431333065\n",
      "Epoch 3608, Loss: 0.021974612260237336, Final Batch Loss: 0.01977776363492012\n",
      "Epoch 3609, Loss: 0.028730120975524187, Final Batch Loss: 0.024751607328653336\n",
      "Epoch 3610, Loss: 0.009068093029782176, Final Batch Loss: 0.002116328338161111\n",
      "Epoch 3611, Loss: 0.018802127335220575, Final Batch Loss: 0.0034590880386531353\n",
      "Epoch 3612, Loss: 0.011637309100478888, Final Batch Loss: 0.003679353278130293\n",
      "Epoch 3613, Loss: 0.034730575047433376, Final Batch Loss: 0.011061380617320538\n",
      "Epoch 3614, Loss: 0.0858475393615663, Final Batch Loss: 0.003010926302522421\n",
      "Epoch 3615, Loss: 0.032884654588997364, Final Batch Loss: 0.009084976278245449\n",
      "Epoch 3616, Loss: 0.014625281211920083, Final Batch Loss: 0.013202125206589699\n",
      "Epoch 3617, Loss: 0.04675251245498657, Final Batch Loss: 0.03441588953137398\n",
      "Epoch 3618, Loss: 0.01710264664143324, Final Batch Loss: 0.008838900364935398\n",
      "Epoch 3619, Loss: 0.02630523219704628, Final Batch Loss: 0.01823246106505394\n",
      "Epoch 3620, Loss: 0.015797782223671675, Final Batch Loss: 0.009483424946665764\n",
      "Epoch 3621, Loss: 0.020501588005572557, Final Batch Loss: 0.0014288262464106083\n",
      "Epoch 3622, Loss: 0.013246238697320223, Final Batch Loss: 0.0060256063006818295\n",
      "Epoch 3623, Loss: 0.061164725571870804, Final Batch Loss: 0.008419949561357498\n",
      "Epoch 3624, Loss: 0.015129837905988097, Final Batch Loss: 0.003881514770910144\n",
      "Epoch 3625, Loss: 0.04502120893448591, Final Batch Loss: 0.013770314864814281\n",
      "Epoch 3626, Loss: 0.010496809612959623, Final Batch Loss: 0.005002695135772228\n",
      "Epoch 3627, Loss: 0.07503720745444298, Final Batch Loss: 0.0335133820772171\n",
      "Epoch 3628, Loss: 0.010085934540256858, Final Batch Loss: 0.001525247236713767\n",
      "Epoch 3629, Loss: 0.012203573249280453, Final Batch Loss: 0.005117514170706272\n",
      "Epoch 3630, Loss: 0.031428960151970387, Final Batch Loss: 0.025859510526061058\n",
      "Epoch 3631, Loss: 0.01714463671669364, Final Batch Loss: 0.010275031439960003\n",
      "Epoch 3632, Loss: 0.01929535949602723, Final Batch Loss: 0.0056044659577310085\n",
      "Epoch 3633, Loss: 0.03224525763653219, Final Batch Loss: 0.029999777674674988\n",
      "Epoch 3634, Loss: 0.021687830798327923, Final Batch Loss: 0.016478588804602623\n",
      "Epoch 3635, Loss: 0.008904169080778956, Final Batch Loss: 0.001619110582396388\n",
      "Epoch 3636, Loss: 0.01851303828880191, Final Batch Loss: 0.0018170769326388836\n",
      "Epoch 3637, Loss: 0.07546424120664597, Final Batch Loss: 0.04427400231361389\n",
      "Epoch 3638, Loss: 0.018823162652552128, Final Batch Loss: 0.010342108085751534\n",
      "Epoch 3639, Loss: 0.017188538797199726, Final Batch Loss: 0.008255524560809135\n",
      "Epoch 3640, Loss: 0.038314216304570436, Final Batch Loss: 0.006095994729548693\n",
      "Epoch 3641, Loss: 0.04008071427233517, Final Batch Loss: 0.0371110737323761\n",
      "Epoch 3642, Loss: 0.028795949183404446, Final Batch Loss: 0.023432966321706772\n",
      "Epoch 3643, Loss: 0.006101153790950775, Final Batch Loss: 0.002098977565765381\n",
      "Epoch 3644, Loss: 0.023129534907639027, Final Batch Loss: 0.004049255512654781\n",
      "Epoch 3645, Loss: 0.012967489659786224, Final Batch Loss: 0.006275175604969263\n",
      "Epoch 3646, Loss: 0.009715982712805271, Final Batch Loss: 0.005853166803717613\n",
      "Epoch 3647, Loss: 0.04489234904758632, Final Batch Loss: 0.002585057867690921\n",
      "Epoch 3648, Loss: 0.008952225325629115, Final Batch Loss: 0.003583290381357074\n",
      "Epoch 3649, Loss: 0.03620256041176617, Final Batch Loss: 0.002468834863975644\n",
      "Epoch 3650, Loss: 0.008161392761394382, Final Batch Loss: 0.006972440052777529\n",
      "Epoch 3651, Loss: 0.005300829885527492, Final Batch Loss: 0.0018860944546759129\n",
      "Epoch 3652, Loss: 0.015442540170624852, Final Batch Loss: 0.0020643912721425295\n",
      "Epoch 3653, Loss: 0.012509774882346392, Final Batch Loss: 0.008084367960691452\n",
      "Epoch 3654, Loss: 0.005501652369275689, Final Batch Loss: 0.003798640100285411\n",
      "Epoch 3655, Loss: 0.008197760907933116, Final Batch Loss: 0.00570495892316103\n",
      "Epoch 3656, Loss: 0.026946645230054855, Final Batch Loss: 0.0024408288300037384\n",
      "Epoch 3657, Loss: 0.027002189308404922, Final Batch Loss: 0.010557027533650398\n",
      "Epoch 3658, Loss: 0.025671975454315543, Final Batch Loss: 0.0025833004619926214\n",
      "Epoch 3659, Loss: 0.013299184618517756, Final Batch Loss: 0.00366897857747972\n",
      "Epoch 3660, Loss: 0.008832058170810342, Final Batch Loss: 0.0022577589843422174\n",
      "Epoch 3661, Loss: 0.0052591669373214245, Final Batch Loss: 0.0029747597873210907\n",
      "Epoch 3662, Loss: 0.008654940174892545, Final Batch Loss: 0.001934681786224246\n",
      "Epoch 3663, Loss: 0.0035966818686574697, Final Batch Loss: 0.0020785785745829344\n",
      "Epoch 3664, Loss: 0.011319508077576756, Final Batch Loss: 0.00837466586381197\n",
      "Epoch 3665, Loss: 0.008155453484505415, Final Batch Loss: 0.004845571704208851\n",
      "Epoch 3666, Loss: 0.008040549699217081, Final Batch Loss: 0.0034447764046490192\n",
      "Epoch 3667, Loss: 0.023257488617673516, Final Batch Loss: 0.019795425236225128\n",
      "Epoch 3668, Loss: 0.015318645630031824, Final Batch Loss: 0.00813312828540802\n",
      "Epoch 3669, Loss: 0.025933324126526713, Final Batch Loss: 0.0017103708814829588\n",
      "Epoch 3670, Loss: 0.027683965861797333, Final Batch Loss: 0.02485836111009121\n",
      "Epoch 3671, Loss: 0.032114794477820396, Final Batch Loss: 0.007464360445737839\n",
      "Epoch 3672, Loss: 0.006962285377085209, Final Batch Loss: 0.0023495331406593323\n",
      "Epoch 3673, Loss: 0.011009945534169674, Final Batch Loss: 0.0021204305812716484\n",
      "Epoch 3674, Loss: 0.0424466161057353, Final Batch Loss: 0.033814799040555954\n",
      "Epoch 3675, Loss: 0.018281870987266302, Final Batch Loss: 0.013723918236792088\n",
      "Epoch 3676, Loss: 0.010353304678574204, Final Batch Loss: 0.006482742726802826\n",
      "Epoch 3677, Loss: 0.012973749078810215, Final Batch Loss: 0.002942761406302452\n",
      "Epoch 3678, Loss: 0.0030357864452525973, Final Batch Loss: 0.001287521212361753\n",
      "Epoch 3679, Loss: 0.006445460254326463, Final Batch Loss: 0.0013964378740638494\n",
      "Epoch 3680, Loss: 0.011236763617489487, Final Batch Loss: 0.0005876888171769679\n",
      "Epoch 3681, Loss: 0.006827722769230604, Final Batch Loss: 0.001465406734496355\n",
      "Epoch 3682, Loss: 0.013315299525856972, Final Batch Loss: 0.009383460506796837\n",
      "Epoch 3683, Loss: 0.004044163157232106, Final Batch Loss: 0.001007392886094749\n",
      "Epoch 3684, Loss: 0.007393887732177973, Final Batch Loss: 0.004793103318661451\n",
      "Epoch 3685, Loss: 0.044275450985878706, Final Batch Loss: 0.03718869015574455\n",
      "Epoch 3686, Loss: 0.007637590868398547, Final Batch Loss: 0.004884496331214905\n",
      "Epoch 3687, Loss: 0.01842315122485161, Final Batch Loss: 0.013219633139669895\n",
      "Epoch 3688, Loss: 0.008072623051702976, Final Batch Loss: 0.004140548408031464\n",
      "Epoch 3689, Loss: 0.03639806970022619, Final Batch Loss: 0.0010403597261756659\n",
      "Epoch 3690, Loss: 0.013293107505887747, Final Batch Loss: 0.009209384210407734\n",
      "Epoch 3691, Loss: 0.012551707914099097, Final Batch Loss: 0.002572283847257495\n",
      "Epoch 3692, Loss: 0.04327588202431798, Final Batch Loss: 0.005169835407286882\n",
      "Epoch 3693, Loss: 0.012630302924662828, Final Batch Loss: 0.010051300749182701\n",
      "Epoch 3694, Loss: 0.028209330514073372, Final Batch Loss: 0.013548707589507103\n",
      "Epoch 3695, Loss: 0.02691628783941269, Final Batch Loss: 0.009248480200767517\n",
      "Epoch 3696, Loss: 0.006831276346929371, Final Batch Loss: 0.0019263365538790822\n",
      "Epoch 3697, Loss: 0.018660538946278393, Final Batch Loss: 0.0018896638648584485\n",
      "Epoch 3698, Loss: 0.003262608079239726, Final Batch Loss: 0.000920985359698534\n",
      "Epoch 3699, Loss: 0.0083297083619982, Final Batch Loss: 0.0014067043084651232\n",
      "Epoch 3700, Loss: 0.00644695432856679, Final Batch Loss: 0.004681816324591637\n",
      "Epoch 3701, Loss: 0.009388180798850954, Final Batch Loss: 0.0016148098511621356\n",
      "Epoch 3702, Loss: 0.006573355640284717, Final Batch Loss: 0.0011876829667016864\n",
      "Epoch 3703, Loss: 0.004009930300526321, Final Batch Loss: 0.0012718726648017764\n",
      "Epoch 3704, Loss: 0.005889482912607491, Final Batch Loss: 0.0005762960063293576\n",
      "Epoch 3705, Loss: 0.003987237345427275, Final Batch Loss: 0.00110123329795897\n",
      "Epoch 3706, Loss: 0.024250934133306146, Final Batch Loss: 0.021204598248004913\n",
      "Epoch 3707, Loss: 0.00215303449658677, Final Batch Loss: 0.0004628439783118665\n",
      "Epoch 3708, Loss: 0.011139363516122103, Final Batch Loss: 0.0026982887648046017\n",
      "Epoch 3709, Loss: 0.06146752089262009, Final Batch Loss: 0.019827913492918015\n",
      "Epoch 3710, Loss: 0.027746270294301212, Final Batch Loss: 0.0018006012542173266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3711, Loss: 0.02080723876133561, Final Batch Loss: 0.004232740495353937\n",
      "Epoch 3712, Loss: 0.04353135637938976, Final Batch Loss: 0.035246919840574265\n",
      "Epoch 3713, Loss: 0.030924475751817226, Final Batch Loss: 0.0005536573007702827\n",
      "Epoch 3714, Loss: 0.004876989172771573, Final Batch Loss: 0.002279826905578375\n",
      "Epoch 3715, Loss: 0.07475454360246658, Final Batch Loss: 0.038542017340660095\n",
      "Epoch 3716, Loss: 0.0035666939802467823, Final Batch Loss: 0.0010889982804656029\n",
      "Epoch 3717, Loss: 0.0037129377014935017, Final Batch Loss: 0.0022283748257905245\n",
      "Epoch 3718, Loss: 0.04868250526487827, Final Batch Loss: 0.01916341297328472\n",
      "Epoch 3719, Loss: 0.050188178196549416, Final Batch Loss: 0.026940876618027687\n",
      "Epoch 3720, Loss: 0.07852950692176819, Final Batch Loss: 0.05965700373053551\n",
      "Epoch 3721, Loss: 0.07160160318017006, Final Batch Loss: 0.03773987293243408\n",
      "Epoch 3722, Loss: 0.01891399547457695, Final Batch Loss: 0.0038434984162449837\n",
      "Epoch 3723, Loss: 0.015148644335567951, Final Batch Loss: 0.01112835668027401\n",
      "Epoch 3724, Loss: 0.03167251392733306, Final Batch Loss: 0.0017670289380475879\n",
      "Epoch 3725, Loss: 0.01776462106499821, Final Batch Loss: 0.0010151403257623315\n",
      "Epoch 3726, Loss: 0.18568280898034573, Final Batch Loss: 0.16079896688461304\n",
      "Epoch 3727, Loss: 0.019170148763805628, Final Batch Loss: 0.0037574605084955692\n",
      "Epoch 3728, Loss: 0.14556212723255157, Final Batch Loss: 0.12609270215034485\n",
      "Epoch 3729, Loss: 0.013173503801226616, Final Batch Loss: 0.005123600363731384\n",
      "Epoch 3730, Loss: 0.013383439276367426, Final Batch Loss: 0.009596387855708599\n",
      "Epoch 3731, Loss: 0.01975503470748663, Final Batch Loss: 0.009751624427735806\n",
      "Epoch 3732, Loss: 0.02258733194321394, Final Batch Loss: 0.011463739909231663\n",
      "Epoch 3733, Loss: 0.011585414176806808, Final Batch Loss: 0.008209818974137306\n",
      "Epoch 3734, Loss: 0.006195405614562333, Final Batch Loss: 0.0013776939595118165\n",
      "Epoch 3735, Loss: 0.0051625322084873915, Final Batch Loss: 0.0035123827401548624\n",
      "Epoch 3736, Loss: 0.004157027578912675, Final Batch Loss: 0.00300951162353158\n",
      "Epoch 3737, Loss: 0.011584064224734902, Final Batch Loss: 0.008319562301039696\n",
      "Epoch 3738, Loss: 0.00590366218239069, Final Batch Loss: 0.0032114170026034117\n",
      "Epoch 3739, Loss: 0.020875349175184965, Final Batch Loss: 0.005515448283404112\n",
      "Epoch 3740, Loss: 0.004873374244198203, Final Batch Loss: 0.0018449476920068264\n",
      "Epoch 3741, Loss: 0.07389930263161659, Final Batch Loss: 0.059985868632793427\n",
      "Epoch 3742, Loss: 0.005106944474391639, Final Batch Loss: 0.0014822430675849319\n",
      "Epoch 3743, Loss: 0.03767109732143581, Final Batch Loss: 0.0354849249124527\n",
      "Epoch 3744, Loss: 0.01915855472907424, Final Batch Loss: 0.016495367512106895\n",
      "Epoch 3745, Loss: 0.04812931548804045, Final Batch Loss: 0.009099594317376614\n",
      "Epoch 3746, Loss: 0.028602530248463154, Final Batch Loss: 0.005737484432756901\n",
      "Epoch 3747, Loss: 0.009623275836929679, Final Batch Loss: 0.0025746363680809736\n",
      "Epoch 3748, Loss: 0.009507289621978998, Final Batch Loss: 0.00518985278904438\n",
      "Epoch 3749, Loss: 0.0025121928774751723, Final Batch Loss: 0.0009032138041220605\n",
      "Epoch 3750, Loss: 0.020491480827331543, Final Batch Loss: 0.017351750284433365\n",
      "Epoch 3751, Loss: 0.02828006399795413, Final Batch Loss: 0.003117199521511793\n",
      "Epoch 3752, Loss: 0.034659689757972956, Final Batch Loss: 0.0019046016968786716\n",
      "Epoch 3753, Loss: 0.004177770810201764, Final Batch Loss: 0.0013471574056893587\n",
      "Epoch 3754, Loss: 0.012033783830702305, Final Batch Loss: 0.004405896179378033\n",
      "Epoch 3755, Loss: 0.0072511822218075395, Final Batch Loss: 0.005606920458376408\n",
      "Epoch 3756, Loss: 0.05030971532687545, Final Batch Loss: 0.001504992600530386\n",
      "Epoch 3757, Loss: 0.01501622749492526, Final Batch Loss: 0.0020388741977512836\n",
      "Epoch 3758, Loss: 0.01903695473447442, Final Batch Loss: 0.012745190411806107\n",
      "Epoch 3759, Loss: 0.01195718499366194, Final Batch Loss: 0.010742468759417534\n",
      "Epoch 3760, Loss: 0.07076905202120543, Final Batch Loss: 0.0594760924577713\n",
      "Epoch 3761, Loss: 0.009059289935976267, Final Batch Loss: 0.005617881193757057\n",
      "Epoch 3762, Loss: 0.01944032241590321, Final Batch Loss: 0.00259687309153378\n",
      "Epoch 3763, Loss: 0.04072933364659548, Final Batch Loss: 0.011423560790717602\n",
      "Epoch 3764, Loss: 0.05831417813897133, Final Batch Loss: 0.01886937767267227\n",
      "Epoch 3765, Loss: 0.02475970145314932, Final Batch Loss: 0.002267853356897831\n",
      "Epoch 3766, Loss: 0.13523657992482185, Final Batch Loss: 0.1207604706287384\n",
      "Epoch 3767, Loss: 0.04809430241584778, Final Batch Loss: 0.019843116402626038\n",
      "Epoch 3768, Loss: 0.01269553555175662, Final Batch Loss: 0.0057876757346093655\n",
      "Epoch 3769, Loss: 0.02036183513700962, Final Batch Loss: 0.005609232001006603\n",
      "Epoch 3770, Loss: 0.03366186935454607, Final Batch Loss: 0.012023082934319973\n",
      "Epoch 3771, Loss: 0.12630947213619947, Final Batch Loss: 0.11372601985931396\n",
      "Epoch 3772, Loss: 0.0671545637305826, Final Batch Loss: 0.0038672832306474447\n",
      "Epoch 3773, Loss: 0.04035430820658803, Final Batch Loss: 0.005137466359883547\n",
      "Epoch 3774, Loss: 0.06120460294187069, Final Batch Loss: 0.048000894486904144\n",
      "Epoch 3775, Loss: 0.09742558002471924, Final Batch Loss: 0.0792623981833458\n",
      "Epoch 3776, Loss: 0.036340756341814995, Final Batch Loss: 0.025539539754390717\n",
      "Epoch 3777, Loss: 0.012525783386081457, Final Batch Loss: 0.009816107340157032\n",
      "Epoch 3778, Loss: 0.06896031089127064, Final Batch Loss: 0.01582607440650463\n",
      "Epoch 3779, Loss: 0.068791963160038, Final Batch Loss: 0.03593156114220619\n",
      "Epoch 3780, Loss: 0.08878679387271404, Final Batch Loss: 0.0726824477314949\n",
      "Epoch 3781, Loss: 0.10315602272748947, Final Batch Loss: 0.03633756935596466\n",
      "Epoch 3782, Loss: 0.03787748981267214, Final Batch Loss: 0.015211288817226887\n",
      "Epoch 3783, Loss: 0.03798537189140916, Final Batch Loss: 0.03232228755950928\n",
      "Epoch 3784, Loss: 0.04570768866688013, Final Batch Loss: 0.04056091606616974\n",
      "Epoch 3785, Loss: 0.04072372242808342, Final Batch Loss: 0.012791607528924942\n",
      "Epoch 3786, Loss: 0.026850514579564333, Final Batch Loss: 0.00675634341314435\n",
      "Epoch 3787, Loss: 0.034514772705733776, Final Batch Loss: 0.010483461432158947\n",
      "Epoch 3788, Loss: 0.020957460859790444, Final Batch Loss: 0.017998086288571358\n",
      "Epoch 3789, Loss: 0.0334466511849314, Final Batch Loss: 0.003874888876453042\n",
      "Epoch 3790, Loss: 0.033655851148068905, Final Batch Loss: 0.02914084494113922\n",
      "Epoch 3791, Loss: 0.017119942931458354, Final Batch Loss: 0.002046051202341914\n",
      "Epoch 3792, Loss: 0.04205869510769844, Final Batch Loss: 0.020432407036423683\n",
      "Epoch 3793, Loss: 0.011739509413018823, Final Batch Loss: 0.0035586014855653048\n",
      "Epoch 3794, Loss: 0.03585364995524287, Final Batch Loss: 0.0026021781377494335\n",
      "Epoch 3795, Loss: 0.02960603404790163, Final Batch Loss: 0.01044315006583929\n",
      "Epoch 3796, Loss: 0.031963043147698045, Final Batch Loss: 0.028723563998937607\n",
      "Epoch 3797, Loss: 0.049726301804184914, Final Batch Loss: 0.03308509290218353\n",
      "Epoch 3798, Loss: 0.035371989011764526, Final Batch Loss: 0.026339175179600716\n",
      "Epoch 3799, Loss: 0.03140657162293792, Final Batch Loss: 0.02681087516248226\n",
      "Epoch 3800, Loss: 0.14210841991007328, Final Batch Loss: 0.12244761735200882\n",
      "Epoch 3801, Loss: 0.012735072756186128, Final Batch Loss: 0.0029873347375541925\n",
      "Epoch 3802, Loss: 0.015765357529744506, Final Batch Loss: 0.002378797857090831\n",
      "Epoch 3803, Loss: 0.06009695120155811, Final Batch Loss: 0.03428320586681366\n",
      "Epoch 3804, Loss: 0.02646775497123599, Final Batch Loss: 0.004575708415359259\n",
      "Epoch 3805, Loss: 0.03166909050196409, Final Batch Loss: 0.012275622226297855\n",
      "Epoch 3806, Loss: 0.07821362279355526, Final Batch Loss: 0.05778913199901581\n",
      "Epoch 3807, Loss: 0.02504986757412553, Final Batch Loss: 0.004196178633719683\n",
      "Epoch 3808, Loss: 0.05484599340707064, Final Batch Loss: 0.04996314272284508\n",
      "Epoch 3809, Loss: 0.033788666129112244, Final Batch Loss: 0.0177573561668396\n",
      "Epoch 3810, Loss: 0.03133725468069315, Final Batch Loss: 0.014452802948653698\n",
      "Epoch 3811, Loss: 0.03586918814107776, Final Batch Loss: 0.004345394205302\n",
      "Epoch 3812, Loss: 0.022581350523978472, Final Batch Loss: 0.018978046253323555\n",
      "Epoch 3813, Loss: 0.016682023648172617, Final Batch Loss: 0.012507019564509392\n",
      "Epoch 3814, Loss: 0.04319917596876621, Final Batch Loss: 0.023737117648124695\n",
      "Epoch 3815, Loss: 0.02395971119403839, Final Batch Loss: 0.007130924612283707\n",
      "Epoch 3816, Loss: 0.021769230253994465, Final Batch Loss: 0.00406439695507288\n",
      "Epoch 3817, Loss: 0.02512345602735877, Final Batch Loss: 0.007560902740806341\n",
      "Epoch 3818, Loss: 0.024007081286981702, Final Batch Loss: 0.0020308580715209246\n",
      "Epoch 3819, Loss: 0.03331867605447769, Final Batch Loss: 0.018778059631586075\n",
      "Epoch 3820, Loss: 0.018017352558672428, Final Batch Loss: 0.012980385683476925\n",
      "Epoch 3821, Loss: 0.028759019449353218, Final Batch Loss: 0.022787416353821754\n",
      "Epoch 3822, Loss: 0.06984090059995651, Final Batch Loss: 0.04907293617725372\n",
      "Epoch 3823, Loss: 0.017429985105991364, Final Batch Loss: 0.0039903465658426285\n",
      "Epoch 3824, Loss: 0.026316923089325428, Final Batch Loss: 0.0057597169652581215\n",
      "Epoch 3825, Loss: 0.03519333899021149, Final Batch Loss: 0.011921918019652367\n",
      "Epoch 3826, Loss: 0.03993039671331644, Final Batch Loss: 0.009247704409062862\n",
      "Epoch 3827, Loss: 0.04043565783649683, Final Batch Loss: 0.02743810974061489\n",
      "Epoch 3828, Loss: 0.03283938649110496, Final Batch Loss: 0.0027640729676932096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3829, Loss: 0.04285057261586189, Final Batch Loss: 0.011957023292779922\n",
      "Epoch 3830, Loss: 0.016214586328715086, Final Batch Loss: 0.00480766361579299\n",
      "Epoch 3831, Loss: 0.028127314522862434, Final Batch Loss: 0.0113912895321846\n",
      "Epoch 3832, Loss: 0.019126024562865496, Final Batch Loss: 0.0035911290906369686\n",
      "Epoch 3833, Loss: 0.07763579487800598, Final Batch Loss: 0.023520413786172867\n",
      "Epoch 3834, Loss: 0.024513246957212687, Final Batch Loss: 0.0076498654671013355\n",
      "Epoch 3835, Loss: 0.00854021473787725, Final Batch Loss: 0.0026752108242362738\n",
      "Epoch 3836, Loss: 0.054537853808142245, Final Batch Loss: 0.0010165037820115685\n",
      "Epoch 3837, Loss: 0.14538519829511642, Final Batch Loss: 0.11159731447696686\n",
      "Epoch 3838, Loss: 0.027546890312805772, Final Batch Loss: 0.002053519943729043\n",
      "Epoch 3839, Loss: 0.08111659716814756, Final Batch Loss: 0.06625208258628845\n",
      "Epoch 3840, Loss: 0.015823015943169594, Final Batch Loss: 0.005530570633709431\n",
      "Epoch 3841, Loss: 0.02875666134059429, Final Batch Loss: 0.009963985532522202\n",
      "Epoch 3842, Loss: 0.009275333490222692, Final Batch Loss: 0.004309234209358692\n",
      "Epoch 3843, Loss: 0.009126575663685799, Final Batch Loss: 0.0020062769763171673\n",
      "Epoch 3844, Loss: 0.021487395279109478, Final Batch Loss: 0.01608114317059517\n",
      "Epoch 3845, Loss: 0.030914648436009884, Final Batch Loss: 0.005532451905310154\n",
      "Epoch 3846, Loss: 0.04316188860684633, Final Batch Loss: 0.032505687326192856\n",
      "Epoch 3847, Loss: 0.01660984056070447, Final Batch Loss: 0.007282818201929331\n",
      "Epoch 3848, Loss: 0.01328055839985609, Final Batch Loss: 0.0030478807166218758\n",
      "Epoch 3849, Loss: 0.015935668256133795, Final Batch Loss: 0.0076761809177696705\n",
      "Epoch 3850, Loss: 0.025169212836772203, Final Batch Loss: 0.007563451770693064\n",
      "Epoch 3851, Loss: 0.026534203439950943, Final Batch Loss: 0.009563054889440536\n",
      "Epoch 3852, Loss: 0.013735194690525532, Final Batch Loss: 0.0064030117355287075\n",
      "Epoch 3853, Loss: 0.013441163580864668, Final Batch Loss: 0.0033075478859245777\n",
      "Epoch 3854, Loss: 0.02402737643569708, Final Batch Loss: 0.004988710395991802\n",
      "Epoch 3855, Loss: 0.010366048663854599, Final Batch Loss: 0.006242474541068077\n",
      "Epoch 3856, Loss: 0.010037435218691826, Final Batch Loss: 0.00218846183270216\n",
      "Epoch 3857, Loss: 0.01779267704114318, Final Batch Loss: 0.002905378583818674\n",
      "Epoch 3858, Loss: 0.03927263943478465, Final Batch Loss: 0.004536859225481749\n",
      "Epoch 3859, Loss: 0.02473775204271078, Final Batch Loss: 0.012297851964831352\n",
      "Epoch 3860, Loss: 0.019116810988634825, Final Batch Loss: 0.007362002972513437\n",
      "Epoch 3861, Loss: 0.026518388651311398, Final Batch Loss: 0.00636614765971899\n",
      "Epoch 3862, Loss: 0.007806107169017196, Final Batch Loss: 0.002111321547999978\n",
      "Epoch 3863, Loss: 0.007479846710339189, Final Batch Loss: 0.0034839387517422438\n",
      "Epoch 3864, Loss: 0.026837544050067663, Final Batch Loss: 0.02548900619149208\n",
      "Epoch 3865, Loss: 0.041224928107112646, Final Batch Loss: 0.0033290586434304714\n",
      "Epoch 3866, Loss: 0.047848956659436226, Final Batch Loss: 0.03547718748450279\n",
      "Epoch 3867, Loss: 0.026240569539368153, Final Batch Loss: 0.012773937545716763\n",
      "Epoch 3868, Loss: 0.010382426902651787, Final Batch Loss: 0.00434494623914361\n",
      "Epoch 3869, Loss: 0.03776657022535801, Final Batch Loss: 0.03009350225329399\n",
      "Epoch 3870, Loss: 0.006145460298284888, Final Batch Loss: 0.0023695281706750393\n",
      "Epoch 3871, Loss: 0.05828059744089842, Final Batch Loss: 0.011130916886031628\n",
      "Epoch 3872, Loss: 0.02114039333537221, Final Batch Loss: 0.019850119948387146\n",
      "Epoch 3873, Loss: 0.005653653643094003, Final Batch Loss: 0.003714168444275856\n",
      "Epoch 3874, Loss: 0.010061542503535748, Final Batch Loss: 0.003956918139010668\n",
      "Epoch 3875, Loss: 0.016273148474283516, Final Batch Loss: 0.014489986933767796\n",
      "Epoch 3876, Loss: 0.012011106591671705, Final Batch Loss: 0.003532788250595331\n",
      "Epoch 3877, Loss: 0.022071714513003826, Final Batch Loss: 0.010496417060494423\n",
      "Epoch 3878, Loss: 0.031190686393529177, Final Batch Loss: 0.0016283602453768253\n",
      "Epoch 3879, Loss: 0.005268979235552251, Final Batch Loss: 0.0033848152961581945\n",
      "Epoch 3880, Loss: 0.017509855329990387, Final Batch Loss: 0.010715113952755928\n",
      "Epoch 3881, Loss: 0.015401923563331366, Final Batch Loss: 0.010990058071911335\n",
      "Epoch 3882, Loss: 0.018471632851287723, Final Batch Loss: 0.014957892708480358\n",
      "Epoch 3883, Loss: 0.026824946980923414, Final Batch Loss: 0.003045815508812666\n",
      "Epoch 3884, Loss: 0.0183122381567955, Final Batch Loss: 0.0047724200412631035\n",
      "Epoch 3885, Loss: 0.02181553142145276, Final Batch Loss: 0.01984269730746746\n",
      "Epoch 3886, Loss: 0.021150216925889254, Final Batch Loss: 0.017054161056876183\n",
      "Epoch 3887, Loss: 0.020512796472758055, Final Batch Loss: 0.014992281794548035\n",
      "Epoch 3888, Loss: 0.00829650741070509, Final Batch Loss: 0.0029455474577844143\n",
      "Epoch 3889, Loss: 0.039233404910191894, Final Batch Loss: 0.03605079650878906\n",
      "Epoch 3890, Loss: 0.027934621553868055, Final Batch Loss: 0.021749289706349373\n",
      "Epoch 3891, Loss: 0.0470705209299922, Final Batch Loss: 0.012046868912875652\n",
      "Epoch 3892, Loss: 0.02084045624360442, Final Batch Loss: 0.004743035417050123\n",
      "Epoch 3893, Loss: 0.019131682813167572, Final Batch Loss: 0.008851530030369759\n",
      "Epoch 3894, Loss: 0.0240105502307415, Final Batch Loss: 0.002368766814470291\n",
      "Epoch 3895, Loss: 0.006987434113398194, Final Batch Loss: 0.002060038736090064\n",
      "Epoch 3896, Loss: 0.046621985733509064, Final Batch Loss: 0.03144260495901108\n",
      "Epoch 3897, Loss: 0.014208771288394928, Final Batch Loss: 0.01208165567368269\n",
      "Epoch 3898, Loss: 0.0711115007288754, Final Batch Loss: 0.007137984503060579\n",
      "Epoch 3899, Loss: 0.06496338825672865, Final Batch Loss: 0.0035895397886633873\n",
      "Epoch 3900, Loss: 0.029225322883576155, Final Batch Loss: 0.023595251142978668\n",
      "Epoch 3901, Loss: 0.024801909923553467, Final Batch Loss: 0.010506653226912022\n",
      "Epoch 3902, Loss: 0.06473847525194287, Final Batch Loss: 0.006755330134183168\n",
      "Epoch 3903, Loss: 0.03904147446155548, Final Batch Loss: 0.026966681703925133\n",
      "Epoch 3904, Loss: 0.004800736904144287, Final Batch Loss: 0.0012353761121630669\n",
      "Epoch 3905, Loss: 0.026678609661757946, Final Batch Loss: 0.014047699980437756\n",
      "Epoch 3906, Loss: 0.019724996760487556, Final Batch Loss: 0.010602107271552086\n",
      "Epoch 3907, Loss: 0.04018492763862014, Final Batch Loss: 0.03740274906158447\n",
      "Epoch 3908, Loss: 0.022900080541148782, Final Batch Loss: 0.0015961399767547846\n",
      "Epoch 3909, Loss: 0.018695958191528916, Final Batch Loss: 0.0026065281126648188\n",
      "Epoch 3910, Loss: 0.018951494595967233, Final Batch Loss: 0.0010162316029891372\n",
      "Epoch 3911, Loss: 0.007354039000347257, Final Batch Loss: 0.002055092016234994\n",
      "Epoch 3912, Loss: 0.01757792418356985, Final Batch Loss: 0.001537988311611116\n",
      "Epoch 3913, Loss: 0.010486719198524952, Final Batch Loss: 0.005377131048589945\n",
      "Epoch 3914, Loss: 0.01064997143112123, Final Batch Loss: 0.0009804738219827414\n",
      "Epoch 3915, Loss: 0.010331399273127317, Final Batch Loss: 0.006314484868198633\n",
      "Epoch 3916, Loss: 0.08771246671676636, Final Batch Loss: 0.026296112686395645\n",
      "Epoch 3917, Loss: 0.011052165646106005, Final Batch Loss: 0.0059633394703269005\n",
      "Epoch 3918, Loss: 0.0038629714399576187, Final Batch Loss: 0.0012508176732808352\n",
      "Epoch 3919, Loss: 0.057545363903045654, Final Batch Loss: 0.04868519678711891\n",
      "Epoch 3920, Loss: 0.015570884104818106, Final Batch Loss: 0.008728348650038242\n",
      "Epoch 3921, Loss: 0.007571999449282885, Final Batch Loss: 0.0051935878582298756\n",
      "Epoch 3922, Loss: 0.014098633546382189, Final Batch Loss: 0.009785890579223633\n",
      "Epoch 3923, Loss: 0.06494230683892965, Final Batch Loss: 0.012162639759480953\n",
      "Epoch 3924, Loss: 0.009345499332994223, Final Batch Loss: 0.005102623254060745\n",
      "Epoch 3925, Loss: 0.006159126292914152, Final Batch Loss: 0.002025869209319353\n",
      "Epoch 3926, Loss: 0.03172582760453224, Final Batch Loss: 0.004539938643574715\n",
      "Epoch 3927, Loss: 0.033294131979346275, Final Batch Loss: 0.02010507695376873\n",
      "Epoch 3928, Loss: 0.010293640196323395, Final Batch Loss: 0.0006221504881978035\n",
      "Epoch 3929, Loss: 0.01934884302318096, Final Batch Loss: 0.011339303106069565\n",
      "Epoch 3930, Loss: 0.008180232718586922, Final Batch Loss: 0.0031510116532444954\n",
      "Epoch 3931, Loss: 0.02135512325912714, Final Batch Loss: 0.01626315712928772\n",
      "Epoch 3932, Loss: 0.01863290590699762, Final Batch Loss: 0.0016233626520261168\n",
      "Epoch 3933, Loss: 0.021124050952494144, Final Batch Loss: 0.004917890764772892\n",
      "Epoch 3934, Loss: 0.01579093048349023, Final Batch Loss: 0.009958185255527496\n",
      "Epoch 3935, Loss: 0.007251130184158683, Final Batch Loss: 0.0037442019674926996\n",
      "Epoch 3936, Loss: 0.019431719556450844, Final Batch Loss: 0.005993039347231388\n",
      "Epoch 3937, Loss: 0.023389988113194704, Final Batch Loss: 0.01972823403775692\n",
      "Epoch 3938, Loss: 0.006559667526744306, Final Batch Loss: 0.004926267545670271\n",
      "Epoch 3939, Loss: 0.02170109050348401, Final Batch Loss: 0.0023592994548380375\n",
      "Epoch 3940, Loss: 0.023645678535103798, Final Batch Loss: 0.012427374720573425\n",
      "Epoch 3941, Loss: 0.007973324274644256, Final Batch Loss: 0.005617702379822731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3942, Loss: 0.02563609927892685, Final Batch Loss: 0.008852794766426086\n",
      "Epoch 3943, Loss: 0.012881978880614042, Final Batch Loss: 0.0077833328396081924\n",
      "Epoch 3944, Loss: 0.004747930797748268, Final Batch Loss: 0.0017646703636273742\n",
      "Epoch 3945, Loss: 0.05340182315558195, Final Batch Loss: 0.006931218318641186\n",
      "Epoch 3946, Loss: 0.016769029898568988, Final Batch Loss: 0.002578397048637271\n",
      "Epoch 3947, Loss: 0.005617893533781171, Final Batch Loss: 0.0020130740012973547\n",
      "Epoch 3948, Loss: 0.018429158488288522, Final Batch Loss: 0.015719972550868988\n",
      "Epoch 3949, Loss: 0.006303903879597783, Final Batch Loss: 0.002580265747383237\n",
      "Epoch 3950, Loss: 0.003627680940553546, Final Batch Loss: 0.0018662110669538379\n",
      "Epoch 3951, Loss: 0.011848045513033867, Final Batch Loss: 0.006925839930772781\n",
      "Epoch 3952, Loss: 0.010373186320066452, Final Batch Loss: 0.00400325283408165\n",
      "Epoch 3953, Loss: 0.0033580100862309337, Final Batch Loss: 0.0016400291351601481\n",
      "Epoch 3954, Loss: 0.026750229066237807, Final Batch Loss: 0.0026443616952747107\n",
      "Epoch 3955, Loss: 0.028313953196629882, Final Batch Loss: 0.0023561937268823385\n",
      "Epoch 3956, Loss: 0.026845144107937813, Final Batch Loss: 0.0025853104889392853\n",
      "Epoch 3957, Loss: 0.013227813877165318, Final Batch Loss: 0.007845346815884113\n",
      "Epoch 3958, Loss: 0.008079455234110355, Final Batch Loss: 0.002166262362152338\n",
      "Epoch 3959, Loss: 0.022768931929022074, Final Batch Loss: 0.017040284350514412\n",
      "Epoch 3960, Loss: 0.004902054904960096, Final Batch Loss: 0.0011757462052628398\n",
      "Epoch 3961, Loss: 0.010595890693366528, Final Batch Loss: 0.004828309640288353\n",
      "Epoch 3962, Loss: 0.029184746323153377, Final Batch Loss: 0.0261056087911129\n",
      "Epoch 3963, Loss: 0.0074429858941584826, Final Batch Loss: 0.00405083317309618\n",
      "Epoch 3964, Loss: 0.015517652034759521, Final Batch Loss: 0.005000986158847809\n",
      "Epoch 3965, Loss: 0.0508807897567749, Final Batch Loss: 0.019189976155757904\n",
      "Epoch 3966, Loss: 0.006056727608665824, Final Batch Loss: 0.004058001562952995\n",
      "Epoch 3967, Loss: 0.03645452531054616, Final Batch Loss: 0.005105957854539156\n",
      "Epoch 3968, Loss: 0.05666173994541168, Final Batch Loss: 0.01816217228770256\n",
      "Epoch 3969, Loss: 0.03727365145459771, Final Batch Loss: 0.005421983543783426\n",
      "Epoch 3970, Loss: 0.02098137466236949, Final Batch Loss: 0.005351958330720663\n",
      "Epoch 3971, Loss: 0.05050286836922169, Final Batch Loss: 0.008022544905543327\n",
      "Epoch 3972, Loss: 0.02940558549016714, Final Batch Loss: 0.018449557945132256\n",
      "Epoch 3973, Loss: 0.02105770166963339, Final Batch Loss: 0.016053151339292526\n",
      "Epoch 3974, Loss: 0.009785572066903114, Final Batch Loss: 0.00201793247833848\n",
      "Epoch 3975, Loss: 0.018826738465577364, Final Batch Loss: 0.0019558933563530445\n",
      "Epoch 3976, Loss: 0.009576479671522975, Final Batch Loss: 0.007169992197304964\n",
      "Epoch 3977, Loss: 0.012452317518182099, Final Batch Loss: 0.0014054077910259366\n",
      "Epoch 3978, Loss: 0.004811781109310687, Final Batch Loss: 0.001821809564717114\n",
      "Epoch 3979, Loss: 0.03929726034402847, Final Batch Loss: 0.023095345124602318\n",
      "Epoch 3980, Loss: 0.07894770591519773, Final Batch Loss: 0.07571403682231903\n",
      "Epoch 3981, Loss: 0.006351072806864977, Final Batch Loss: 0.0021114847622811794\n",
      "Epoch 3982, Loss: 0.014589255210012197, Final Batch Loss: 0.009351278655230999\n",
      "Epoch 3983, Loss: 0.015036711003631353, Final Batch Loss: 0.009681054390966892\n",
      "Epoch 3984, Loss: 0.08388951979577541, Final Batch Loss: 0.056654978543519974\n",
      "Epoch 3985, Loss: 0.009809250477701426, Final Batch Loss: 0.004633627831935883\n",
      "Epoch 3986, Loss: 0.006280197063460946, Final Batch Loss: 0.004374546464532614\n",
      "Epoch 3987, Loss: 0.04361327365040779, Final Batch Loss: 0.0202014222741127\n",
      "Epoch 3988, Loss: 0.022803131490945816, Final Batch Loss: 0.012404264882206917\n",
      "Epoch 3989, Loss: 0.03869627229869366, Final Batch Loss: 0.02092139981687069\n",
      "Epoch 3990, Loss: 0.007199977873824537, Final Batch Loss: 0.005488877184689045\n",
      "Epoch 3991, Loss: 0.022410868434235454, Final Batch Loss: 0.0012768476735800505\n",
      "Epoch 3992, Loss: 0.004390636691823602, Final Batch Loss: 0.001161663793027401\n",
      "Epoch 3993, Loss: 0.006520279683172703, Final Batch Loss: 0.0018235375173389912\n",
      "Epoch 3994, Loss: 0.022657734341919422, Final Batch Loss: 0.005317372269928455\n",
      "Epoch 3995, Loss: 0.01349413930438459, Final Batch Loss: 0.002252840204164386\n",
      "Epoch 3996, Loss: 0.00729899771977216, Final Batch Loss: 0.0059586334973573685\n",
      "Epoch 3997, Loss: 0.014674669597297907, Final Batch Loss: 0.008139928802847862\n",
      "Epoch 3998, Loss: 0.013010842842049897, Final Batch Loss: 0.0015838417457416654\n",
      "Epoch 3999, Loss: 0.01719490997493267, Final Batch Loss: 0.007720143534243107\n",
      "Epoch 4000, Loss: 0.031046340707689524, Final Batch Loss: 0.004756104666739702\n",
      "Epoch 4001, Loss: 0.055543363094329834, Final Batch Loss: 0.047143783420324326\n",
      "Epoch 4002, Loss: 0.010044721886515617, Final Batch Loss: 0.0021947482600808144\n",
      "Epoch 4003, Loss: 0.010605548741295934, Final Batch Loss: 0.00785341951996088\n",
      "Epoch 4004, Loss: 0.04439970664680004, Final Batch Loss: 0.016478298231959343\n",
      "Epoch 4005, Loss: 0.12560446560382843, Final Batch Loss: 0.08125852793455124\n",
      "Epoch 4006, Loss: 0.009661352378316224, Final Batch Loss: 0.0013688952894881368\n",
      "Epoch 4007, Loss: 0.02348100650124252, Final Batch Loss: 0.019742785021662712\n",
      "Epoch 4008, Loss: 0.014920859597623348, Final Batch Loss: 0.001501159742474556\n",
      "Epoch 4009, Loss: 0.012764447834342718, Final Batch Loss: 0.00652713468298316\n",
      "Epoch 4010, Loss: 0.058738844469189644, Final Batch Loss: 0.02718559093773365\n",
      "Epoch 4011, Loss: 0.020875112153589725, Final Batch Loss: 0.010562559589743614\n",
      "Epoch 4012, Loss: 0.01063847541809082, Final Batch Loss: 0.006843216717243195\n",
      "Epoch 4013, Loss: 0.00590407510753721, Final Batch Loss: 0.0012352013727650046\n",
      "Epoch 4014, Loss: 0.005598540301434696, Final Batch Loss: 0.004092826042324305\n",
      "Epoch 4015, Loss: 0.011673689587041736, Final Batch Loss: 0.0025533379521220922\n",
      "Epoch 4016, Loss: 0.015114960086066276, Final Batch Loss: 0.0006953988340683281\n",
      "Epoch 4017, Loss: 0.02181993192061782, Final Batch Loss: 0.004947622772306204\n",
      "Epoch 4018, Loss: 0.05306457122787833, Final Batch Loss: 0.04637625068426132\n",
      "Epoch 4019, Loss: 0.005474176083225757, Final Batch Loss: 0.0007929744315333664\n",
      "Epoch 4020, Loss: 0.003022440942004323, Final Batch Loss: 0.0011472025653347373\n",
      "Epoch 4021, Loss: 0.011380598414689302, Final Batch Loss: 0.0069993603974580765\n",
      "Epoch 4022, Loss: 0.01052433461882174, Final Batch Loss: 0.003600598080083728\n",
      "Epoch 4023, Loss: 0.10814506700262427, Final Batch Loss: 0.10402510315179825\n",
      "Epoch 4024, Loss: 0.024330742890015244, Final Batch Loss: 0.021882569417357445\n",
      "Epoch 4025, Loss: 0.04566266015172005, Final Batch Loss: 0.02491852454841137\n",
      "Epoch 4026, Loss: 0.021786503260955215, Final Batch Loss: 0.002177187940105796\n",
      "Epoch 4027, Loss: 0.0652858279645443, Final Batch Loss: 0.037046320736408234\n",
      "Epoch 4028, Loss: 0.04518536292016506, Final Batch Loss: 0.02364976331591606\n",
      "Epoch 4029, Loss: 0.026212909258902073, Final Batch Loss: 0.016553033143281937\n",
      "Epoch 4030, Loss: 0.01776531513314694, Final Batch Loss: 0.015900686383247375\n",
      "Epoch 4031, Loss: 0.02655312931165099, Final Batch Loss: 0.0007060128264129162\n",
      "Epoch 4032, Loss: 0.011366980150341988, Final Batch Loss: 0.00541881937533617\n",
      "Epoch 4033, Loss: 0.022458425723016262, Final Batch Loss: 0.010201659984886646\n",
      "Epoch 4034, Loss: 0.02639078558422625, Final Batch Loss: 0.0017400200013071299\n",
      "Epoch 4035, Loss: 0.01156308501958847, Final Batch Loss: 0.003710199147462845\n",
      "Epoch 4036, Loss: 0.016869415994733572, Final Batch Loss: 0.010145538486540318\n",
      "Epoch 4037, Loss: 0.037344662472605705, Final Batch Loss: 0.027701804414391518\n",
      "Epoch 4038, Loss: 0.03057584073394537, Final Batch Loss: 0.0021718209609389305\n",
      "Epoch 4039, Loss: 0.04607548005878925, Final Batch Loss: 0.02947423793375492\n",
      "Epoch 4040, Loss: 0.010515317786484957, Final Batch Loss: 0.007228147704154253\n",
      "Epoch 4041, Loss: 0.003939563757739961, Final Batch Loss: 0.0018537499709054828\n",
      "Epoch 4042, Loss: 0.047148936544544995, Final Batch Loss: 0.0005796822952106595\n",
      "Epoch 4043, Loss: 0.04739936254918575, Final Batch Loss: 0.026310764253139496\n",
      "Epoch 4044, Loss: 0.005501060513779521, Final Batch Loss: 0.0025881254114210606\n",
      "Epoch 4045, Loss: 0.025878498796373606, Final Batch Loss: 0.0014094901271164417\n",
      "Epoch 4046, Loss: 0.0067970899399369955, Final Batch Loss: 0.002863914007321\n",
      "Epoch 4047, Loss: 0.005541937658563256, Final Batch Loss: 0.0022970130667090416\n",
      "Epoch 4048, Loss: 0.0150978472083807, Final Batch Loss: 0.007437377702444792\n",
      "Epoch 4049, Loss: 0.005577187170274556, Final Batch Loss: 0.004279184155166149\n",
      "Epoch 4050, Loss: 0.005716301151551306, Final Batch Loss: 0.0015240233624354005\n",
      "Epoch 4051, Loss: 0.00589025323279202, Final Batch Loss: 0.002568997209891677\n",
      "Epoch 4052, Loss: 0.02796154050156474, Final Batch Loss: 0.005558195989578962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4053, Loss: 0.02746274322271347, Final Batch Loss: 0.02183889038860798\n",
      "Epoch 4054, Loss: 0.03543390613049269, Final Batch Loss: 0.02263103611767292\n",
      "Epoch 4055, Loss: 0.007010262459516525, Final Batch Loss: 0.0005492810159921646\n",
      "Epoch 4056, Loss: 0.008883585687726736, Final Batch Loss: 0.0014085019938647747\n",
      "Epoch 4057, Loss: 0.041252413764595985, Final Batch Loss: 0.006042866036295891\n",
      "Epoch 4058, Loss: 0.04442406271118671, Final Batch Loss: 0.04276721552014351\n",
      "Epoch 4059, Loss: 0.005484935245476663, Final Batch Loss: 0.003897903487086296\n",
      "Epoch 4060, Loss: 0.016246748389676213, Final Batch Loss: 0.002991717541590333\n",
      "Epoch 4061, Loss: 0.010641983710229397, Final Batch Loss: 0.0014200406149029732\n",
      "Epoch 4062, Loss: 0.023252475075423717, Final Batch Loss: 0.013931620866060257\n",
      "Epoch 4063, Loss: 0.009833766147494316, Final Batch Loss: 0.0031484472565352917\n",
      "Epoch 4064, Loss: 0.015010221861302853, Final Batch Loss: 0.008793438784778118\n",
      "Epoch 4065, Loss: 0.05620556836947799, Final Batch Loss: 0.004544855561107397\n",
      "Epoch 4066, Loss: 0.030622292775660753, Final Batch Loss: 0.025777466595172882\n",
      "Epoch 4067, Loss: 0.016072183847427368, Final Batch Loss: 0.0061924271285533905\n",
      "Epoch 4068, Loss: 0.008768539875745773, Final Batch Loss: 0.00503455800935626\n",
      "Epoch 4069, Loss: 0.043974663596600294, Final Batch Loss: 0.005756772588938475\n",
      "Epoch 4070, Loss: 0.015841037034988403, Final Batch Loss: 0.009606454521417618\n",
      "Epoch 4071, Loss: 0.0028841699240729213, Final Batch Loss: 0.001277617528103292\n",
      "Epoch 4072, Loss: 0.00821011047810316, Final Batch Loss: 0.003215717151761055\n",
      "Epoch 4073, Loss: 0.009592001093551517, Final Batch Loss: 0.006603617686778307\n",
      "Epoch 4074, Loss: 0.0057888575829565525, Final Batch Loss: 0.0033248490653932095\n",
      "Epoch 4075, Loss: 0.0029926695860922337, Final Batch Loss: 0.0014004047261551023\n",
      "Epoch 4076, Loss: 0.020873201079666615, Final Batch Loss: 0.005535404197871685\n",
      "Epoch 4077, Loss: 0.014648317126557231, Final Batch Loss: 0.0029577899258583784\n",
      "Epoch 4078, Loss: 0.004623361164703965, Final Batch Loss: 0.0013220049440860748\n",
      "Epoch 4079, Loss: 0.010131399729289114, Final Batch Loss: 0.008209682069718838\n",
      "Epoch 4080, Loss: 0.03657920227851719, Final Batch Loss: 0.0010439188918098807\n",
      "Epoch 4081, Loss: 0.009311832371167839, Final Batch Loss: 0.001462833839468658\n",
      "Epoch 4082, Loss: 0.031815716181881726, Final Batch Loss: 0.029897261410951614\n",
      "Epoch 4083, Loss: 0.005639008013531566, Final Batch Loss: 0.001566530903801322\n",
      "Epoch 4084, Loss: 0.018189176567830145, Final Batch Loss: 0.0012815553927794099\n",
      "Epoch 4085, Loss: 0.009430710691958666, Final Batch Loss: 0.0025284597650170326\n",
      "Epoch 4086, Loss: 0.0036323773092590272, Final Batch Loss: 0.0007341880700550973\n",
      "Epoch 4087, Loss: 0.002520061214454472, Final Batch Loss: 0.0010404674103483558\n",
      "Epoch 4088, Loss: 0.00813105283305049, Final Batch Loss: 0.003312125336378813\n",
      "Epoch 4089, Loss: 0.002901286003179848, Final Batch Loss: 0.0014574097003787756\n",
      "Epoch 4090, Loss: 0.014673685654997826, Final Batch Loss: 0.0012228554114699364\n",
      "Epoch 4091, Loss: 0.004032191354781389, Final Batch Loss: 0.0020031160674989223\n",
      "Epoch 4092, Loss: 0.022666018921881914, Final Batch Loss: 0.020748363807797432\n",
      "Epoch 4093, Loss: 0.014307533157989383, Final Batch Loss: 0.010951756499707699\n",
      "Epoch 4094, Loss: 0.01564350607804954, Final Batch Loss: 0.014600667171180248\n",
      "Epoch 4095, Loss: 0.021112266462296247, Final Batch Loss: 0.01690591871738434\n",
      "Epoch 4096, Loss: 0.01133193273562938, Final Batch Loss: 0.0011384683893993497\n",
      "Epoch 4097, Loss: 0.004331975942477584, Final Batch Loss: 0.0027383456472307444\n",
      "Epoch 4098, Loss: 0.030696879606693983, Final Batch Loss: 0.025402210652828217\n",
      "Epoch 4099, Loss: 0.044109553564339876, Final Batch Loss: 0.03941703960299492\n",
      "Epoch 4100, Loss: 0.02000508736819029, Final Batch Loss: 0.00528667401522398\n",
      "Epoch 4101, Loss: 0.01717893686145544, Final Batch Loss: 0.0028336336836218834\n",
      "Epoch 4102, Loss: 0.008028192445635796, Final Batch Loss: 0.0062096379697322845\n",
      "Epoch 4103, Loss: 0.005324206955265254, Final Batch Loss: 0.0006693044560961425\n",
      "Epoch 4104, Loss: 0.030456152744591236, Final Batch Loss: 0.027947429567575455\n",
      "Epoch 4105, Loss: 0.004247887525707483, Final Batch Loss: 0.002690053777769208\n",
      "Epoch 4106, Loss: 0.010085571964737028, Final Batch Loss: 0.00917080044746399\n",
      "Epoch 4107, Loss: 0.008545371470972896, Final Batch Loss: 0.001974979182705283\n",
      "Epoch 4108, Loss: 0.009669278049841523, Final Batch Loss: 0.001036979490891099\n",
      "Epoch 4109, Loss: 0.007682225434109569, Final Batch Loss: 0.0037221757229417562\n",
      "Epoch 4110, Loss: 0.014633215265348554, Final Batch Loss: 0.0025457118172198534\n",
      "Epoch 4111, Loss: 0.0508597003063187, Final Batch Loss: 0.001877944334410131\n",
      "Epoch 4112, Loss: 0.022235615644603968, Final Batch Loss: 0.00751047907397151\n",
      "Epoch 4113, Loss: 0.008328978437930346, Final Batch Loss: 0.0059059374034404755\n",
      "Epoch 4114, Loss: 0.024007691303268075, Final Batch Loss: 0.020101776346564293\n",
      "Epoch 4115, Loss: 0.09998975787311792, Final Batch Loss: 0.014410675503313541\n",
      "Epoch 4116, Loss: 0.017141878604888916, Final Batch Loss: 0.01148852240294218\n",
      "Epoch 4117, Loss: 0.014422700740396976, Final Batch Loss: 0.011617172509431839\n",
      "Epoch 4118, Loss: 0.0089346575550735, Final Batch Loss: 0.004534874111413956\n",
      "Epoch 4119, Loss: 0.073501817882061, Final Batch Loss: 0.06347205489873886\n",
      "Epoch 4120, Loss: 0.008007314521819353, Final Batch Loss: 0.003089258447289467\n",
      "Epoch 4121, Loss: 0.017655422911047935, Final Batch Loss: 0.005000370554625988\n",
      "Epoch 4122, Loss: 0.00781573774293065, Final Batch Loss: 0.005022433120757341\n",
      "Epoch 4123, Loss: 0.006511411746032536, Final Batch Loss: 0.001287956372834742\n",
      "Epoch 4124, Loss: 0.009822499239817262, Final Batch Loss: 0.003767654998227954\n",
      "Epoch 4125, Loss: 0.00873948831576854, Final Batch Loss: 0.0070999301970005035\n",
      "Epoch 4126, Loss: 0.03233737195841968, Final Batch Loss: 0.0015415388625115156\n",
      "Epoch 4127, Loss: 0.005718264728784561, Final Batch Loss: 0.003766037290915847\n",
      "Epoch 4128, Loss: 0.012510105967521667, Final Batch Loss: 0.007949468679726124\n",
      "Epoch 4129, Loss: 0.00719321402721107, Final Batch Loss: 0.0042808204889297485\n",
      "Epoch 4130, Loss: 0.015300942119210958, Final Batch Loss: 0.008192294277250767\n",
      "Epoch 4131, Loss: 0.005308127962052822, Final Batch Loss: 0.00296551501378417\n",
      "Epoch 4132, Loss: 0.002558809064794332, Final Batch Loss: 0.0009682026575319469\n",
      "Epoch 4133, Loss: 0.0034380616853013635, Final Batch Loss: 0.0014201627345755696\n",
      "Epoch 4134, Loss: 0.01374249893706292, Final Batch Loss: 0.012105598114430904\n",
      "Epoch 4135, Loss: 0.023458122741430998, Final Batch Loss: 0.01996443420648575\n",
      "Epoch 4136, Loss: 0.012507441453635693, Final Batch Loss: 0.0051042791455984116\n",
      "Epoch 4137, Loss: 0.02309160539880395, Final Batch Loss: 0.0053177219815552235\n",
      "Epoch 4138, Loss: 0.005371699924580753, Final Batch Loss: 0.0035298557486385107\n",
      "Epoch 4139, Loss: 0.030539461760781705, Final Batch Loss: 0.0014547327300533652\n",
      "Epoch 4140, Loss: 0.003565196879208088, Final Batch Loss: 0.00199917983263731\n",
      "Epoch 4141, Loss: 0.010639157146215439, Final Batch Loss: 0.008784852921962738\n",
      "Epoch 4142, Loss: 0.001872542779892683, Final Batch Loss: 0.0004750789375975728\n",
      "Epoch 4143, Loss: 0.0037854048423469067, Final Batch Loss: 0.0025211835745722055\n",
      "Epoch 4144, Loss: 0.01122351991944015, Final Batch Loss: 0.0015715050976723433\n",
      "Epoch 4145, Loss: 0.08706344850361347, Final Batch Loss: 0.028790825977921486\n",
      "Epoch 4146, Loss: 0.009492157842032611, Final Batch Loss: 0.0009144590003415942\n",
      "Epoch 4147, Loss: 0.022842790465801954, Final Batch Loss: 0.017353912815451622\n",
      "Epoch 4148, Loss: 0.01579186355229467, Final Batch Loss: 0.0014674266567453742\n",
      "Epoch 4149, Loss: 0.04084054380655289, Final Batch Loss: 0.020883793011307716\n",
      "Epoch 4150, Loss: 0.020704245660454035, Final Batch Loss: 0.007361405063420534\n",
      "Epoch 4151, Loss: 0.017374088987708092, Final Batch Loss: 0.002273411490023136\n",
      "Epoch 4152, Loss: 0.00854398071533069, Final Batch Loss: 0.007878381758928299\n",
      "Epoch 4153, Loss: 0.009750994388014078, Final Batch Loss: 0.0010921475477516651\n",
      "Epoch 4154, Loss: 0.012375217396765947, Final Batch Loss: 0.007312009111046791\n",
      "Epoch 4155, Loss: 0.003164354944601655, Final Batch Loss: 0.0018681288929656148\n",
      "Epoch 4156, Loss: 0.0016854461282491684, Final Batch Loss: 0.0007054078159853816\n",
      "Epoch 4157, Loss: 0.00513812992721796, Final Batch Loss: 0.0027897569816559553\n",
      "Epoch 4158, Loss: 0.017941294703632593, Final Batch Loss: 0.003966125193983316\n",
      "Epoch 4159, Loss: 0.01441375131253153, Final Batch Loss: 0.013035373762249947\n",
      "Epoch 4160, Loss: 0.011031518690288067, Final Batch Loss: 0.004469184670597315\n",
      "Epoch 4161, Loss: 0.0050107440911233425, Final Batch Loss: 0.0008041514083743095\n",
      "Epoch 4162, Loss: 0.013663095189258456, Final Batch Loss: 0.0036297242622822523\n",
      "Epoch 4163, Loss: 0.009763823356479406, Final Batch Loss: 0.0029345573857426643\n",
      "Epoch 4164, Loss: 0.00572065613232553, Final Batch Loss: 0.0036181339528411627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4165, Loss: 0.005392624298110604, Final Batch Loss: 0.0018495540134608746\n",
      "Epoch 4166, Loss: 0.1010388390859589, Final Batch Loss: 0.09930823743343353\n",
      "Epoch 4167, Loss: 0.0036564460024237633, Final Batch Loss: 0.0025094335433095694\n",
      "Epoch 4168, Loss: 0.0058616502792574465, Final Batch Loss: 0.0050614383071660995\n",
      "Epoch 4169, Loss: 0.02023197617381811, Final Batch Loss: 0.017272766679525375\n",
      "Epoch 4170, Loss: 0.007806593494024128, Final Batch Loss: 0.0008624358451925218\n",
      "Epoch 4171, Loss: 0.004503021016716957, Final Batch Loss: 0.0025963771622627974\n",
      "Epoch 4172, Loss: 0.005883030127733946, Final Batch Loss: 0.003792841685935855\n",
      "Epoch 4173, Loss: 0.003443002060521394, Final Batch Loss: 0.0009601605706848204\n",
      "Epoch 4174, Loss: 0.004149102314841002, Final Batch Loss: 0.0009321360266767442\n",
      "Epoch 4175, Loss: 0.01390906237065792, Final Batch Loss: 0.005505981855094433\n",
      "Epoch 4176, Loss: 0.0021707150735892355, Final Batch Loss: 0.0007117603090591729\n",
      "Epoch 4177, Loss: 0.017987797735258937, Final Batch Loss: 0.0006365075241774321\n",
      "Epoch 4178, Loss: 0.0077241158578544855, Final Batch Loss: 0.0020009714644402266\n",
      "Epoch 4179, Loss: 0.007664910051971674, Final Batch Loss: 0.00304313562810421\n",
      "Epoch 4180, Loss: 0.01785396784543991, Final Batch Loss: 0.01672414317727089\n",
      "Epoch 4181, Loss: 0.13766713347285986, Final Batch Loss: 0.1288307011127472\n",
      "Epoch 4182, Loss: 0.003348654485307634, Final Batch Loss: 0.0019804509356617928\n",
      "Epoch 4183, Loss: 0.00497152260504663, Final Batch Loss: 0.0024534163530915976\n",
      "Epoch 4184, Loss: 0.05023086816072464, Final Batch Loss: 0.022867243736982346\n",
      "Epoch 4185, Loss: 0.04060108750127256, Final Batch Loss: 0.003325750818476081\n",
      "Epoch 4186, Loss: 0.026515849633142352, Final Batch Loss: 0.0030669260304421186\n",
      "Epoch 4187, Loss: 0.008919792249798775, Final Batch Loss: 0.004686109255999327\n",
      "Epoch 4188, Loss: 0.007768072420731187, Final Batch Loss: 0.0011752296704798937\n",
      "Epoch 4189, Loss: 0.043730996549129486, Final Batch Loss: 0.013790605589747429\n",
      "Epoch 4190, Loss: 0.01312456896994263, Final Batch Loss: 0.011241145431995392\n",
      "Epoch 4191, Loss: 0.003570943954400718, Final Batch Loss: 0.0020954336505383253\n",
      "Epoch 4192, Loss: 0.012652820209041238, Final Batch Loss: 0.010518195107579231\n",
      "Epoch 4193, Loss: 0.010622856090776622, Final Batch Loss: 0.0009679914219304919\n",
      "Epoch 4194, Loss: 0.02527088625356555, Final Batch Loss: 0.001276660244911909\n",
      "Epoch 4195, Loss: 0.006587087642401457, Final Batch Loss: 0.003047096775844693\n",
      "Epoch 4196, Loss: 0.019938366021960974, Final Batch Loss: 0.018832474946975708\n",
      "Epoch 4197, Loss: 0.00427962007233873, Final Batch Loss: 0.0036180398892611265\n",
      "Epoch 4198, Loss: 0.009172922233119607, Final Batch Loss: 0.005908368155360222\n",
      "Epoch 4199, Loss: 0.05668025091290474, Final Batch Loss: 0.04671802744269371\n",
      "Epoch 4200, Loss: 0.005961493938229978, Final Batch Loss: 0.0006559252506121993\n",
      "Epoch 4201, Loss: 0.0037593732122331858, Final Batch Loss: 0.002303305547684431\n",
      "Epoch 4202, Loss: 0.0399203235283494, Final Batch Loss: 0.010432400740683079\n",
      "Epoch 4203, Loss: 0.044668966671451926, Final Batch Loss: 0.042228542268276215\n",
      "Epoch 4204, Loss: 0.015739816473796964, Final Batch Loss: 0.0015936049167066813\n",
      "Epoch 4205, Loss: 0.0024379583774134517, Final Batch Loss: 0.0009327473817393184\n",
      "Epoch 4206, Loss: 0.007210723124444485, Final Batch Loss: 0.0020354939624667168\n",
      "Epoch 4207, Loss: 0.038765827543102205, Final Batch Loss: 0.03701506927609444\n",
      "Epoch 4208, Loss: 0.012340394197963178, Final Batch Loss: 0.0015848086914047599\n",
      "Epoch 4209, Loss: 0.05981968576088548, Final Batch Loss: 0.05230288952589035\n",
      "Epoch 4210, Loss: 0.015375763177871704, Final Batch Loss: 0.007404232397675514\n",
      "Epoch 4211, Loss: 0.007040152209810913, Final Batch Loss: 0.0011702267220243812\n",
      "Epoch 4212, Loss: 0.033947455114685, Final Batch Loss: 0.03255854919552803\n",
      "Epoch 4213, Loss: 0.010327895870432258, Final Batch Loss: 0.0023634626995772123\n",
      "Epoch 4214, Loss: 0.00591590884141624, Final Batch Loss: 0.0037349495105445385\n",
      "Epoch 4215, Loss: 0.03414691495709121, Final Batch Loss: 0.03225930780172348\n",
      "Epoch 4216, Loss: 0.006857570493593812, Final Batch Loss: 0.004170698579400778\n",
      "Epoch 4217, Loss: 0.00912177562713623, Final Batch Loss: 0.002137602772563696\n",
      "Epoch 4218, Loss: 0.019449313869699836, Final Batch Loss: 0.002916441997513175\n",
      "Epoch 4219, Loss: 0.004737652139738202, Final Batch Loss: 0.002195429289713502\n",
      "Epoch 4220, Loss: 0.004819668014533818, Final Batch Loss: 0.0031375917606055737\n",
      "Epoch 4221, Loss: 0.008983667939901352, Final Batch Loss: 0.007772370707243681\n",
      "Epoch 4222, Loss: 0.009710896294564009, Final Batch Loss: 0.003106021322309971\n",
      "Epoch 4223, Loss: 0.09898633137345314, Final Batch Loss: 0.02547386661171913\n",
      "Epoch 4224, Loss: 0.010606373194605112, Final Batch Loss: 0.002319702412933111\n",
      "Epoch 4225, Loss: 0.02671418059617281, Final Batch Loss: 0.023776251822710037\n",
      "Epoch 4226, Loss: 0.00904130091657862, Final Batch Loss: 0.0009735226049087942\n",
      "Epoch 4227, Loss: 0.0048304368974640965, Final Batch Loss: 0.0036038749385625124\n",
      "Epoch 4228, Loss: 0.08542971371207386, Final Batch Loss: 0.08370716869831085\n",
      "Epoch 4229, Loss: 0.008683884050697088, Final Batch Loss: 0.00454656220972538\n",
      "Epoch 4230, Loss: 0.013286512112244964, Final Batch Loss: 0.002606330206617713\n",
      "Epoch 4231, Loss: 0.019661478349007666, Final Batch Loss: 0.001850882195867598\n",
      "Epoch 4232, Loss: 0.013210125733166933, Final Batch Loss: 0.007701289374381304\n",
      "Epoch 4233, Loss: 0.01584782323334366, Final Batch Loss: 0.001570631400682032\n",
      "Epoch 4234, Loss: 0.008976280223578215, Final Batch Loss: 0.0038005439564585686\n",
      "Epoch 4235, Loss: 0.012453922186978161, Final Batch Loss: 0.0017165784956887364\n",
      "Epoch 4236, Loss: 0.02304267551517114, Final Batch Loss: 0.0006478879950009286\n",
      "Epoch 4237, Loss: 0.005121619440615177, Final Batch Loss: 0.0031104714144021273\n",
      "Epoch 4238, Loss: 0.010051925550214946, Final Batch Loss: 0.0018572023836895823\n",
      "Epoch 4239, Loss: 0.003792565723415464, Final Batch Loss: 0.0007001249468885362\n",
      "Epoch 4240, Loss: 0.020516186021268368, Final Batch Loss: 0.017849134281277657\n",
      "Epoch 4241, Loss: 0.008449729532003403, Final Batch Loss: 0.004713231697678566\n",
      "Epoch 4242, Loss: 0.0029271190869621933, Final Batch Loss: 0.0008849778096191585\n",
      "Epoch 4243, Loss: 0.005298165837302804, Final Batch Loss: 0.002874525263905525\n",
      "Epoch 4244, Loss: 0.011263250547926873, Final Batch Loss: 0.0005231128889136016\n",
      "Epoch 4245, Loss: 0.013601771555840969, Final Batch Loss: 0.009570073336362839\n",
      "Epoch 4246, Loss: 0.015857258811593056, Final Batch Loss: 0.001132851466536522\n",
      "Epoch 4247, Loss: 0.004049286828376353, Final Batch Loss: 0.0024291283916682005\n",
      "Epoch 4248, Loss: 0.008607744704931974, Final Batch Loss: 0.004560672678053379\n",
      "Epoch 4249, Loss: 0.0019656156073324382, Final Batch Loss: 0.0012367479503154755\n",
      "Epoch 4250, Loss: 0.006719070370309055, Final Batch Loss: 0.0013435286236926913\n",
      "Epoch 4251, Loss: 0.007718759821727872, Final Batch Loss: 0.0032697676215320826\n",
      "Epoch 4252, Loss: 0.01957397599471733, Final Batch Loss: 0.000868799805175513\n",
      "Epoch 4253, Loss: 0.0027529874350875616, Final Batch Loss: 0.0010925467358902097\n",
      "Epoch 4254, Loss: 0.004896191065199673, Final Batch Loss: 0.0013434226857498288\n",
      "Epoch 4255, Loss: 0.013712307321839035, Final Batch Loss: 0.0010489093838259578\n",
      "Epoch 4256, Loss: 0.008567218319512904, Final Batch Loss: 0.0012623734073713422\n",
      "Epoch 4257, Loss: 0.07954209018498659, Final Batch Loss: 0.07538554072380066\n",
      "Epoch 4258, Loss: 0.0062708735931664705, Final Batch Loss: 0.0042240615002810955\n",
      "Epoch 4259, Loss: 0.015505427261814475, Final Batch Loss: 0.0033869899343699217\n",
      "Epoch 4260, Loss: 0.016123839654028416, Final Batch Loss: 0.009958888404071331\n",
      "Epoch 4261, Loss: 0.00558182003442198, Final Batch Loss: 0.0016181451501324773\n",
      "Epoch 4262, Loss: 0.006211669882759452, Final Batch Loss: 0.002577101346105337\n",
      "Epoch 4263, Loss: 0.005003631464205682, Final Batch Loss: 0.0008560990681871772\n",
      "Epoch 4264, Loss: 0.0185966519638896, Final Batch Loss: 0.004554446786642075\n",
      "Epoch 4265, Loss: 0.003964907373301685, Final Batch Loss: 0.0010371714597567916\n",
      "Epoch 4266, Loss: 0.0034126125974580646, Final Batch Loss: 0.0009683409007266164\n",
      "Epoch 4267, Loss: 0.02518433891236782, Final Batch Loss: 0.011170022189617157\n",
      "Epoch 4268, Loss: 0.017496591666713357, Final Batch Loss: 0.01609927974641323\n",
      "Epoch 4269, Loss: 0.007533024880103767, Final Batch Loss: 0.0010327663039788604\n",
      "Epoch 4270, Loss: 0.012828453909605742, Final Batch Loss: 0.005695478059351444\n",
      "Epoch 4271, Loss: 0.010133542411495, Final Batch Loss: 0.00065681739943102\n",
      "Epoch 4272, Loss: 0.06673768581822515, Final Batch Loss: 0.06518429517745972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4273, Loss: 0.00784140732139349, Final Batch Loss: 0.00511533347889781\n",
      "Epoch 4274, Loss: 0.004026537179015577, Final Batch Loss: 0.002956124022603035\n",
      "Epoch 4275, Loss: 0.023906806483864784, Final Batch Loss: 0.008090553805232048\n",
      "Epoch 4276, Loss: 0.11662349104881287, Final Batch Loss: 0.09594479203224182\n",
      "Epoch 4277, Loss: 0.020813022507354617, Final Batch Loss: 0.0012288603466004133\n",
      "Epoch 4278, Loss: 0.003079101617913693, Final Batch Loss: 0.0007424603099934757\n",
      "Epoch 4279, Loss: 0.002622277825139463, Final Batch Loss: 0.0013055111048743129\n",
      "Epoch 4280, Loss: 0.02807625220157206, Final Batch Loss: 0.0032110123429447412\n",
      "Epoch 4281, Loss: 0.010360635002143681, Final Batch Loss: 0.0012730882735922933\n",
      "Epoch 4282, Loss: 0.03098435467109084, Final Batch Loss: 0.0030094324611127377\n",
      "Epoch 4283, Loss: 0.03419681917876005, Final Batch Loss: 0.009921041317284107\n",
      "Epoch 4284, Loss: 0.015428582788445055, Final Batch Loss: 0.0018809359753504395\n",
      "Epoch 4285, Loss: 0.01948009865009226, Final Batch Loss: 0.00027749562286771834\n",
      "Epoch 4286, Loss: 0.023999422788619995, Final Batch Loss: 0.008054550737142563\n",
      "Epoch 4287, Loss: 0.020146672148257494, Final Batch Loss: 0.014209678396582603\n",
      "Epoch 4288, Loss: 0.004247810924425721, Final Batch Loss: 0.001968289725482464\n",
      "Epoch 4289, Loss: 0.014657928491942585, Final Batch Loss: 0.0015664779348298907\n",
      "Epoch 4290, Loss: 0.009472533129155636, Final Batch Loss: 0.0008791666477918625\n",
      "Epoch 4291, Loss: 0.010517819609958678, Final Batch Loss: 0.0009616816532798111\n",
      "Epoch 4292, Loss: 0.017855116166174412, Final Batch Loss: 0.011846383102238178\n",
      "Epoch 4293, Loss: 0.021052389405667782, Final Batch Loss: 0.004145187325775623\n",
      "Epoch 4294, Loss: 0.08307306095957756, Final Batch Loss: 0.07535423338413239\n",
      "Epoch 4295, Loss: 0.10301217588130385, Final Batch Loss: 0.0005407050484791398\n",
      "Epoch 4296, Loss: 0.02944537391886115, Final Batch Loss: 0.02348657324910164\n",
      "Epoch 4297, Loss: 0.0298438910394907, Final Batch Loss: 0.012856869027018547\n",
      "Epoch 4298, Loss: 0.010804663295857608, Final Batch Loss: 0.0018934168620035052\n",
      "Epoch 4299, Loss: 0.016250719781965017, Final Batch Loss: 0.009573070332407951\n",
      "Epoch 4300, Loss: 0.00706198497209698, Final Batch Loss: 0.0015316588105633855\n",
      "Epoch 4301, Loss: 0.007720089459326118, Final Batch Loss: 0.0009637222974561155\n",
      "Epoch 4302, Loss: 0.04011642653495073, Final Batch Loss: 0.0246603861451149\n",
      "Epoch 4303, Loss: 0.013040087185800076, Final Batch Loss: 0.005261165555566549\n",
      "Epoch 4304, Loss: 0.0255578828509897, Final Batch Loss: 0.002671744441613555\n",
      "Epoch 4305, Loss: 0.03211360052227974, Final Batch Loss: 0.005145745351910591\n",
      "Epoch 4306, Loss: 0.0723698593210429, Final Batch Loss: 0.07128049433231354\n",
      "Epoch 4307, Loss: 0.007107607088983059, Final Batch Loss: 0.001661693211644888\n",
      "Epoch 4308, Loss: 0.07386483624577522, Final Batch Loss: 0.025132212787866592\n",
      "Epoch 4309, Loss: 0.01871334807947278, Final Batch Loss: 0.005641581956297159\n",
      "Epoch 4310, Loss: 0.04434573487378657, Final Batch Loss: 0.0012803173158317804\n",
      "Epoch 4311, Loss: 0.00454110954888165, Final Batch Loss: 0.0026137582026422024\n",
      "Epoch 4312, Loss: 0.016137263155542314, Final Batch Loss: 0.0011335614835843444\n",
      "Epoch 4313, Loss: 0.014983832836151123, Final Batch Loss: 0.006648716516792774\n",
      "Epoch 4314, Loss: 0.008383058186154813, Final Batch Loss: 0.0009565443615429103\n",
      "Epoch 4315, Loss: 0.07155393622815609, Final Batch Loss: 0.06496980041265488\n",
      "Epoch 4316, Loss: 0.02283731778152287, Final Batch Loss: 0.0027688269037753344\n",
      "Epoch 4317, Loss: 0.027533862565178424, Final Batch Loss: 0.0009191630524583161\n",
      "Epoch 4318, Loss: 0.09243960492312908, Final Batch Loss: 0.07660140842199326\n",
      "Epoch 4319, Loss: 0.05476910248398781, Final Batch Loss: 0.051342424005270004\n",
      "Epoch 4320, Loss: 0.027209392748773098, Final Batch Loss: 0.010145812295377254\n",
      "Epoch 4321, Loss: 0.15267734415829182, Final Batch Loss: 0.01858966238796711\n",
      "Epoch 4322, Loss: 0.07244526594877243, Final Batch Loss: 0.034197647124528885\n",
      "Epoch 4323, Loss: 0.04407988116145134, Final Batch Loss: 0.024506457149982452\n",
      "Epoch 4324, Loss: 0.01883094571530819, Final Batch Loss: 0.004883989691734314\n",
      "Epoch 4325, Loss: 0.01671359920874238, Final Batch Loss: 0.011107484810054302\n",
      "Epoch 4326, Loss: 0.009716054424643517, Final Batch Loss: 0.004765455145388842\n",
      "Epoch 4327, Loss: 0.040268758777529, Final Batch Loss: 0.0051812478341162205\n",
      "Epoch 4328, Loss: 0.0462240818887949, Final Batch Loss: 0.03891365975141525\n",
      "Epoch 4329, Loss: 0.029061305336654186, Final Batch Loss: 0.019813470542430878\n",
      "Epoch 4330, Loss: 0.014984858804382384, Final Batch Loss: 0.0013312582159414887\n",
      "Epoch 4331, Loss: 0.003711154859047383, Final Batch Loss: 0.0008146603940986097\n",
      "Epoch 4332, Loss: 0.007162458961829543, Final Batch Loss: 0.0046525681391358376\n",
      "Epoch 4333, Loss: 0.02192813961300999, Final Batch Loss: 0.0018094541737809777\n",
      "Epoch 4334, Loss: 0.016000345814973116, Final Batch Loss: 0.008497020229697227\n",
      "Epoch 4335, Loss: 0.007716154679656029, Final Batch Loss: 0.002288262825459242\n",
      "Epoch 4336, Loss: 0.005993603030219674, Final Batch Loss: 0.0013295600656419992\n",
      "Epoch 4337, Loss: 0.10158094670623541, Final Batch Loss: 0.09762754291296005\n",
      "Epoch 4338, Loss: 0.004344334360212088, Final Batch Loss: 0.0013795120175927877\n",
      "Epoch 4339, Loss: 0.06256452482193708, Final Batch Loss: 0.0547713041305542\n",
      "Epoch 4340, Loss: 0.01506054331548512, Final Batch Loss: 0.013864737935364246\n",
      "Epoch 4341, Loss: 0.035855584777891636, Final Batch Loss: 0.013154962100088596\n",
      "Epoch 4342, Loss: 0.022479766979813576, Final Batch Loss: 0.002703595906496048\n",
      "Epoch 4343, Loss: 0.011377473827451468, Final Batch Loss: 0.008325856178998947\n",
      "Epoch 4344, Loss: 0.04977394687011838, Final Batch Loss: 0.048700131475925446\n",
      "Epoch 4345, Loss: 0.0254042373271659, Final Batch Loss: 0.0008311198325827718\n",
      "Epoch 4346, Loss: 0.018002793891355395, Final Batch Loss: 0.016637708991765976\n",
      "Epoch 4347, Loss: 0.05135792610235512, Final Batch Loss: 0.048936836421489716\n",
      "Epoch 4348, Loss: 0.0059597669169306755, Final Batch Loss: 0.003051722887903452\n",
      "Epoch 4349, Loss: 0.03057454712688923, Final Batch Loss: 0.0013330671936273575\n",
      "Epoch 4350, Loss: 0.01001854008063674, Final Batch Loss: 0.005102533381432295\n",
      "Epoch 4351, Loss: 0.031185865867882967, Final Batch Loss: 0.0020598345436155796\n",
      "Epoch 4352, Loss: 0.026564066763967276, Final Batch Loss: 0.02388148009777069\n",
      "Epoch 4353, Loss: 0.012701361207291484, Final Batch Loss: 0.009334604255855083\n",
      "Epoch 4354, Loss: 0.02407739602494985, Final Batch Loss: 0.001887175370939076\n",
      "Epoch 4355, Loss: 0.008816776447929442, Final Batch Loss: 0.0010183585109189153\n",
      "Epoch 4356, Loss: 0.07245543226599693, Final Batch Loss: 0.04810604453086853\n",
      "Epoch 4357, Loss: 0.020745336543768644, Final Batch Loss: 0.002337097655981779\n",
      "Epoch 4358, Loss: 0.03874701261520386, Final Batch Loss: 0.012792445719242096\n",
      "Epoch 4359, Loss: 0.05559586430899799, Final Batch Loss: 0.002778102410957217\n",
      "Epoch 4360, Loss: 0.06884072395041585, Final Batch Loss: 0.004154857713729143\n",
      "Epoch 4361, Loss: 0.012153010116890073, Final Batch Loss: 0.001961289206519723\n",
      "Epoch 4362, Loss: 0.011571924202144146, Final Batch Loss: 0.0028018737211823463\n",
      "Epoch 4363, Loss: 0.06653811503201723, Final Batch Loss: 0.061607103794813156\n",
      "Epoch 4364, Loss: 0.0216333270072937, Final Batch Loss: 0.013630510307848454\n",
      "Epoch 4365, Loss: 0.019926192238926888, Final Batch Loss: 0.008385793305933475\n",
      "Epoch 4366, Loss: 0.016544138896279037, Final Batch Loss: 0.0015415166271850467\n",
      "Epoch 4367, Loss: 0.1862037144601345, Final Batch Loss: 0.15705671906471252\n",
      "Epoch 4368, Loss: 0.02941854950040579, Final Batch Loss: 0.010195502080023289\n",
      "Epoch 4369, Loss: 0.008248631842434406, Final Batch Loss: 0.003214468713849783\n",
      "Epoch 4370, Loss: 0.008930326905101538, Final Batch Loss: 0.004024883732199669\n",
      "Epoch 4371, Loss: 0.06366260349750519, Final Batch Loss: 0.04492532089352608\n",
      "Epoch 4372, Loss: 0.005091011989861727, Final Batch Loss: 0.0027397857047617435\n",
      "Epoch 4373, Loss: 0.02005886798724532, Final Batch Loss: 0.01706521213054657\n",
      "Epoch 4374, Loss: 0.04976124223321676, Final Batch Loss: 0.03540263697504997\n",
      "Epoch 4375, Loss: 0.010942904918920249, Final Batch Loss: 0.0009231098811142147\n",
      "Epoch 4376, Loss: 0.022177659906446934, Final Batch Loss: 0.0017989994958043098\n",
      "Epoch 4377, Loss: 0.05208381358534098, Final Batch Loss: 0.03657783940434456\n",
      "Epoch 4378, Loss: 0.03130945982411504, Final Batch Loss: 0.005762422922998667\n",
      "Epoch 4379, Loss: 0.033029576763510704, Final Batch Loss: 0.0066327545791864395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4380, Loss: 0.006979067460633814, Final Batch Loss: 0.0019014073768630624\n",
      "Epoch 4381, Loss: 0.017467038240283728, Final Batch Loss: 0.002132780384272337\n",
      "Epoch 4382, Loss: 0.013146769953891635, Final Batch Loss: 0.0037141272332519293\n",
      "Epoch 4383, Loss: 0.006585469236597419, Final Batch Loss: 0.003949597477912903\n",
      "Epoch 4384, Loss: 0.030580260790884495, Final Batch Loss: 0.014682571403682232\n",
      "Epoch 4385, Loss: 0.007173541001975536, Final Batch Loss: 0.0025652768090367317\n",
      "Epoch 4386, Loss: 0.02534251380711794, Final Batch Loss: 0.01285168994218111\n",
      "Epoch 4387, Loss: 0.00782151473686099, Final Batch Loss: 0.0030669872649013996\n",
      "Epoch 4388, Loss: 0.018936728360131383, Final Batch Loss: 0.0158388689160347\n",
      "Epoch 4389, Loss: 0.08554332563653588, Final Batch Loss: 0.0046946764923632145\n",
      "Epoch 4390, Loss: 0.032083919271826744, Final Batch Loss: 0.014468159526586533\n",
      "Epoch 4391, Loss: 0.009729543467983603, Final Batch Loss: 0.003626738442108035\n",
      "Epoch 4392, Loss: 0.09767766669392586, Final Batch Loss: 0.06413391977548599\n",
      "Epoch 4393, Loss: 0.09093120694160461, Final Batch Loss: 0.04475051164627075\n",
      "Epoch 4394, Loss: 0.011408584890887141, Final Batch Loss: 0.0019169074948877096\n",
      "Epoch 4395, Loss: 0.010013449820689857, Final Batch Loss: 0.008314727805554867\n",
      "Epoch 4396, Loss: 0.038708267733454704, Final Batch Loss: 0.020076114684343338\n",
      "Epoch 4397, Loss: 0.010125505737960339, Final Batch Loss: 0.005166895687580109\n",
      "Epoch 4398, Loss: 0.00597806298173964, Final Batch Loss: 0.003410506993532181\n",
      "Epoch 4399, Loss: 0.020336349727585912, Final Batch Loss: 0.016518540680408478\n",
      "Epoch 4400, Loss: 0.01104960753582418, Final Batch Loss: 0.001341822324320674\n",
      "Epoch 4401, Loss: 0.04626291245222092, Final Batch Loss: 0.034941062331199646\n",
      "Epoch 4402, Loss: 0.04164922144263983, Final Batch Loss: 0.03160099312663078\n",
      "Epoch 4403, Loss: 0.0077936790767125785, Final Batch Loss: 0.0006779527175240219\n",
      "Epoch 4404, Loss: 0.030761794885620475, Final Batch Loss: 0.02710927091538906\n",
      "Epoch 4405, Loss: 0.003619247814640403, Final Batch Loss: 0.0008447105064988136\n",
      "Epoch 4406, Loss: 0.009019538760185242, Final Batch Loss: 0.0027851341292262077\n",
      "Epoch 4407, Loss: 0.033582805655896664, Final Batch Loss: 0.009296220727264881\n",
      "Epoch 4408, Loss: 0.017323731211945415, Final Batch Loss: 0.01519336923956871\n",
      "Epoch 4409, Loss: 0.07514229603111744, Final Batch Loss: 0.022773003205657005\n",
      "Epoch 4410, Loss: 0.011625483864918351, Final Batch Loss: 0.0017814713064581156\n",
      "Epoch 4411, Loss: 0.03254268690943718, Final Batch Loss: 0.015795154497027397\n",
      "Epoch 4412, Loss: 0.005426867748610675, Final Batch Loss: 0.0017925583524629474\n",
      "Epoch 4413, Loss: 0.014742079889401793, Final Batch Loss: 0.012163822539150715\n",
      "Epoch 4414, Loss: 0.18823597952723503, Final Batch Loss: 0.15658371150493622\n",
      "Epoch 4415, Loss: 0.06689831428229809, Final Batch Loss: 0.03915690258145332\n",
      "Epoch 4416, Loss: 0.007108466466888785, Final Batch Loss: 0.00502873957157135\n",
      "Epoch 4417, Loss: 0.01308626844547689, Final Batch Loss: 0.00288996915332973\n",
      "Epoch 4418, Loss: 0.03144306689500809, Final Batch Loss: 0.01311144046485424\n",
      "Epoch 4419, Loss: 0.01353920390829444, Final Batch Loss: 0.005359572824090719\n",
      "Epoch 4420, Loss: 0.021147404331713915, Final Batch Loss: 0.01594071090221405\n",
      "Epoch 4421, Loss: 0.01567157916724682, Final Batch Loss: 0.009913147427141666\n",
      "Epoch 4422, Loss: 0.01884033204987645, Final Batch Loss: 0.0022777304984629154\n",
      "Epoch 4423, Loss: 0.007992656901478767, Final Batch Loss: 0.005702367052435875\n",
      "Epoch 4424, Loss: 0.01568612176924944, Final Batch Loss: 0.003954347223043442\n",
      "Epoch 4425, Loss: 0.013011094182729721, Final Batch Loss: 0.0044146571308374405\n",
      "Epoch 4426, Loss: 0.05059592425823212, Final Batch Loss: 0.02616356685757637\n",
      "Epoch 4427, Loss: 0.007391207152977586, Final Batch Loss: 0.0014773814473301172\n",
      "Epoch 4428, Loss: 0.022514812881127, Final Batch Loss: 0.02151382714509964\n",
      "Epoch 4429, Loss: 0.016071863239631057, Final Batch Loss: 0.0017465108539909124\n",
      "Epoch 4430, Loss: 0.012306124321185052, Final Batch Loss: 0.0011426903074607253\n",
      "Epoch 4431, Loss: 0.004197586385998875, Final Batch Loss: 0.0009520617895759642\n",
      "Epoch 4432, Loss: 0.00849121855571866, Final Batch Loss: 0.0026683430187404156\n",
      "Epoch 4433, Loss: 0.008648332674056292, Final Batch Loss: 0.0019399109296500683\n",
      "Epoch 4434, Loss: 0.013585241045802832, Final Batch Loss: 0.0030751428566873074\n",
      "Epoch 4435, Loss: 0.019153228029608727, Final Batch Loss: 0.004260026849806309\n",
      "Epoch 4436, Loss: 0.04241017019376159, Final Batch Loss: 0.03752227500081062\n",
      "Epoch 4437, Loss: 0.014431037474423647, Final Batch Loss: 0.009054691530764103\n",
      "Epoch 4438, Loss: 0.031077350256964564, Final Batch Loss: 0.027987059205770493\n",
      "Epoch 4439, Loss: 0.010095197707414627, Final Batch Loss: 0.0032244734466075897\n",
      "Epoch 4440, Loss: 0.09707947634160519, Final Batch Loss: 0.0722329244017601\n",
      "Epoch 4441, Loss: 0.014064018614590168, Final Batch Loss: 0.008638040162622929\n",
      "Epoch 4442, Loss: 0.017128715757280588, Final Batch Loss: 0.013162288814783096\n",
      "Epoch 4443, Loss: 0.015555024147033691, Final Batch Loss: 0.005793945863842964\n",
      "Epoch 4444, Loss: 0.04737560264766216, Final Batch Loss: 0.009707128629088402\n",
      "Epoch 4445, Loss: 0.06734221801161766, Final Batch Loss: 0.02052696794271469\n",
      "Epoch 4446, Loss: 0.007318746065720916, Final Batch Loss: 0.004715634975582361\n",
      "Epoch 4447, Loss: 0.0343852792866528, Final Batch Loss: 0.002755515743046999\n",
      "Epoch 4448, Loss: 0.044973138719797134, Final Batch Loss: 0.0377592071890831\n",
      "Epoch 4449, Loss: 0.01267287228256464, Final Batch Loss: 0.005645432975143194\n",
      "Epoch 4450, Loss: 0.01580445549916476, Final Batch Loss: 0.001002587261609733\n",
      "Epoch 4451, Loss: 0.04122049268335104, Final Batch Loss: 0.030243707820773125\n",
      "Epoch 4452, Loss: 0.05935300886631012, Final Batch Loss: 0.020867586135864258\n",
      "Epoch 4453, Loss: 0.017323348904028535, Final Batch Loss: 0.015286577865481377\n",
      "Epoch 4454, Loss: 0.008294568862766027, Final Batch Loss: 0.006170446984469891\n",
      "Epoch 4455, Loss: 0.03853977657854557, Final Batch Loss: 0.01813524030148983\n",
      "Epoch 4456, Loss: 0.027581654023379087, Final Batch Loss: 0.02223878540098667\n",
      "Epoch 4457, Loss: 0.0071397635620087385, Final Batch Loss: 0.003699203720316291\n",
      "Epoch 4458, Loss: 0.02388352993875742, Final Batch Loss: 0.015450391918420792\n",
      "Epoch 4459, Loss: 0.05127422302030027, Final Batch Loss: 0.048650436103343964\n",
      "Epoch 4460, Loss: 0.031892117112874985, Final Batch Loss: 0.0045048873871564865\n",
      "Epoch 4461, Loss: 0.018948324024677277, Final Batch Loss: 0.01002312358468771\n",
      "Epoch 4462, Loss: 0.03864758322015405, Final Batch Loss: 0.03545839712023735\n",
      "Epoch 4463, Loss: 0.03374995931517333, Final Batch Loss: 0.0014492858899757266\n",
      "Epoch 4464, Loss: 0.010642358800396323, Final Batch Loss: 0.0025012267287820578\n",
      "Epoch 4465, Loss: 0.009826494846493006, Final Batch Loss: 0.005739310756325722\n",
      "Epoch 4466, Loss: 0.010875147767364979, Final Batch Loss: 0.0036805076524615288\n",
      "Epoch 4467, Loss: 0.03629846032708883, Final Batch Loss: 0.0073193134739995\n",
      "Epoch 4468, Loss: 0.004926769528537989, Final Batch Loss: 0.002396754454821348\n",
      "Epoch 4469, Loss: 0.008036881452426314, Final Batch Loss: 0.003776215249672532\n",
      "Epoch 4470, Loss: 0.07125391252338886, Final Batch Loss: 0.05280385538935661\n",
      "Epoch 4471, Loss: 0.024911784566938877, Final Batch Loss: 0.0010387944057583809\n",
      "Epoch 4472, Loss: 0.006672373740002513, Final Batch Loss: 0.0028618930373340845\n",
      "Epoch 4473, Loss: 0.01349608413875103, Final Batch Loss: 0.005762862972915173\n",
      "Epoch 4474, Loss: 0.008231405168771744, Final Batch Loss: 0.004039546009153128\n",
      "Epoch 4475, Loss: 0.022104420699179173, Final Batch Loss: 0.008120792917907238\n",
      "Epoch 4476, Loss: 0.014619794674217701, Final Batch Loss: 0.012168924324214458\n",
      "Epoch 4477, Loss: 0.011526335496455431, Final Batch Loss: 0.0059886109083890915\n",
      "Epoch 4478, Loss: 0.03729559574276209, Final Batch Loss: 0.03439395874738693\n",
      "Epoch 4479, Loss: 0.038435143418610096, Final Batch Loss: 0.022854704409837723\n",
      "Epoch 4480, Loss: 0.010469916393049061, Final Batch Loss: 0.0017882586689665914\n",
      "Epoch 4481, Loss: 0.009862639009952545, Final Batch Loss: 0.0043108523823320866\n",
      "Epoch 4482, Loss: 0.012060452252626419, Final Batch Loss: 0.004838017746806145\n",
      "Epoch 4483, Loss: 0.01796624669805169, Final Batch Loss: 0.01367757935076952\n",
      "Epoch 4484, Loss: 0.021167823113501072, Final Batch Loss: 0.01934877596795559\n",
      "Epoch 4485, Loss: 0.00877411849796772, Final Batch Loss: 0.006418172735720873\n",
      "Epoch 4486, Loss: 0.0094135079998523, Final Batch Loss: 0.001091901445761323\n",
      "Epoch 4487, Loss: 0.0074367462657392025, Final Batch Loss: 0.004122160375118256\n",
      "Epoch 4488, Loss: 0.0036922828294336796, Final Batch Loss: 0.0015978885348886251\n",
      "Epoch 4489, Loss: 0.023906308691948652, Final Batch Loss: 0.00404460309073329\n",
      "Epoch 4490, Loss: 0.008562209084630013, Final Batch Loss: 0.004901740234345198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4491, Loss: 0.032284928718581796, Final Batch Loss: 0.02984180487692356\n",
      "Epoch 4492, Loss: 0.031750877387821674, Final Batch Loss: 0.023364830762147903\n",
      "Epoch 4493, Loss: 0.04318130429601297, Final Batch Loss: 0.0005150478682480752\n",
      "Epoch 4494, Loss: 0.031570207327604294, Final Batch Loss: 0.025958888232707977\n",
      "Epoch 4495, Loss: 0.01081690238788724, Final Batch Loss: 0.006034604739397764\n",
      "Epoch 4496, Loss: 0.0905319545418024, Final Batch Loss: 0.07158877700567245\n",
      "Epoch 4497, Loss: 0.005047012818977237, Final Batch Loss: 0.0029260986484587193\n",
      "Epoch 4498, Loss: 0.007506023510359228, Final Batch Loss: 0.0014146597823128104\n",
      "Epoch 4499, Loss: 0.009865454165264964, Final Batch Loss: 0.0029554960783571005\n",
      "Epoch 4500, Loss: 0.017693466157652438, Final Batch Loss: 0.001669765100814402\n",
      "Epoch 4501, Loss: 0.005038441624492407, Final Batch Loss: 0.002813922008499503\n",
      "Epoch 4502, Loss: 0.00767437438480556, Final Batch Loss: 0.0014204045291990042\n",
      "Epoch 4503, Loss: 0.014892671490088105, Final Batch Loss: 0.0032457190100103617\n",
      "Epoch 4504, Loss: 0.009687729179859161, Final Batch Loss: 0.005216826684772968\n",
      "Epoch 4505, Loss: 0.006442349869757891, Final Batch Loss: 0.0041710082441568375\n",
      "Epoch 4506, Loss: 0.031011581420898438, Final Batch Loss: 0.008190624415874481\n",
      "Epoch 4507, Loss: 0.0035431201104074717, Final Batch Loss: 0.0017360561760142446\n",
      "Epoch 4508, Loss: 0.043593610636889935, Final Batch Loss: 0.04159311577677727\n",
      "Epoch 4509, Loss: 0.01486426405608654, Final Batch Loss: 0.0022533638402819633\n",
      "Epoch 4510, Loss: 0.004058985156007111, Final Batch Loss: 0.0019111792789772153\n",
      "Epoch 4511, Loss: 0.009368221508339047, Final Batch Loss: 0.0036197479348629713\n",
      "Epoch 4512, Loss: 0.01777849067002535, Final Batch Loss: 0.0016644420102238655\n",
      "Epoch 4513, Loss: 0.008792135864496231, Final Batch Loss: 0.006313033401966095\n",
      "Epoch 4514, Loss: 0.009731174679473042, Final Batch Loss: 0.006916893646121025\n",
      "Epoch 4515, Loss: 0.004863656009547412, Final Batch Loss: 0.0013210695469751954\n",
      "Epoch 4516, Loss: 0.003020422300323844, Final Batch Loss: 0.0017124934820458293\n",
      "Epoch 4517, Loss: 0.008171913214027882, Final Batch Loss: 0.003911110572516918\n",
      "Epoch 4518, Loss: 0.044458004646003246, Final Batch Loss: 0.030212942510843277\n",
      "Epoch 4519, Loss: 0.0072248466312885284, Final Batch Loss: 0.0013708006590604782\n",
      "Epoch 4520, Loss: 0.02547347918152809, Final Batch Loss: 0.0040583182126283646\n",
      "Epoch 4521, Loss: 0.007279291050508618, Final Batch Loss: 0.0033240902703255415\n",
      "Epoch 4522, Loss: 0.004408566863276064, Final Batch Loss: 0.0028588769491761923\n",
      "Epoch 4523, Loss: 0.012057170271873474, Final Batch Loss: 0.0069327880628407\n",
      "Epoch 4524, Loss: 0.004627404501661658, Final Batch Loss: 0.002640459919348359\n",
      "Epoch 4525, Loss: 0.007782468339428306, Final Batch Loss: 0.0018044284079223871\n",
      "Epoch 4526, Loss: 0.002127178944647312, Final Batch Loss: 0.0013194795465096831\n",
      "Epoch 4527, Loss: 0.04292965237982571, Final Batch Loss: 0.003673511790111661\n",
      "Epoch 4528, Loss: 0.013281706254929304, Final Batch Loss: 0.010794137604534626\n",
      "Epoch 4529, Loss: 0.007824259693734348, Final Batch Loss: 0.0007850666297599673\n",
      "Epoch 4530, Loss: 0.019119636388495564, Final Batch Loss: 0.017572764307260513\n",
      "Epoch 4531, Loss: 0.006068045797292143, Final Batch Loss: 0.0005398467765189707\n",
      "Epoch 4532, Loss: 0.05765370558947325, Final Batch Loss: 0.0520755909383297\n",
      "Epoch 4533, Loss: 0.002975200768560171, Final Batch Loss: 0.0015411386266350746\n",
      "Epoch 4534, Loss: 0.002902079839259386, Final Batch Loss: 0.0012984207132831216\n",
      "Epoch 4535, Loss: 0.005923315649852157, Final Batch Loss: 0.003709082491695881\n",
      "Epoch 4536, Loss: 0.011581073282286525, Final Batch Loss: 0.002276764949783683\n",
      "Epoch 4537, Loss: 0.008865094161592424, Final Batch Loss: 0.001348557765595615\n",
      "Epoch 4538, Loss: 0.02262506925035268, Final Batch Loss: 0.0010073139565065503\n",
      "Epoch 4539, Loss: 0.005869045970030129, Final Batch Loss: 0.001649518613703549\n",
      "Epoch 4540, Loss: 0.005767020047642291, Final Batch Loss: 0.0051582888700068\n",
      "Epoch 4541, Loss: 0.005652567138895392, Final Batch Loss: 0.002922259969636798\n",
      "Epoch 4542, Loss: 0.002951056812889874, Final Batch Loss: 0.0018041400471702218\n",
      "Epoch 4543, Loss: 0.0027987799840047956, Final Batch Loss: 0.0011513927020132542\n",
      "Epoch 4544, Loss: 0.031429859111085534, Final Batch Loss: 0.028718916699290276\n",
      "Epoch 4545, Loss: 0.019756026566028595, Final Batch Loss: 0.016751212999224663\n",
      "Epoch 4546, Loss: 0.01206521736457944, Final Batch Loss: 0.004307201597839594\n",
      "Epoch 4547, Loss: 0.007559477584436536, Final Batch Loss: 0.004023517481982708\n",
      "Epoch 4548, Loss: 0.009391823783516884, Final Batch Loss: 0.004150699824094772\n",
      "Epoch 4549, Loss: 0.028342125471681356, Final Batch Loss: 0.0032064239494502544\n",
      "Epoch 4550, Loss: 0.004196933121420443, Final Batch Loss: 0.0011805164394900203\n",
      "Epoch 4551, Loss: 0.003373128012754023, Final Batch Loss: 0.0013595855562016368\n",
      "Epoch 4552, Loss: 0.01108637370634824, Final Batch Loss: 0.009733199141919613\n",
      "Epoch 4553, Loss: 0.010651908116415143, Final Batch Loss: 0.009030484594404697\n",
      "Epoch 4554, Loss: 0.0033090028446167707, Final Batch Loss: 0.0016076387837529182\n",
      "Epoch 4555, Loss: 0.0051321088103577495, Final Batch Loss: 0.0032598671969026327\n",
      "Epoch 4556, Loss: 0.009436097461730242, Final Batch Loss: 0.0024602310732007027\n",
      "Epoch 4557, Loss: 0.008161886362358928, Final Batch Loss: 0.0049210586585104465\n",
      "Epoch 4558, Loss: 0.02815363649278879, Final Batch Loss: 0.018425414338707924\n",
      "Epoch 4559, Loss: 0.004729625303298235, Final Batch Loss: 0.003057104768231511\n",
      "Epoch 4560, Loss: 0.01989144692197442, Final Batch Loss: 0.013555053621530533\n",
      "Epoch 4561, Loss: 0.0029287744546309114, Final Batch Loss: 0.0020295947324484587\n",
      "Epoch 4562, Loss: 0.038254897110164165, Final Batch Loss: 0.034604448825120926\n",
      "Epoch 4563, Loss: 0.004091004608199, Final Batch Loss: 0.001182252774015069\n",
      "Epoch 4564, Loss: 0.06068928446620703, Final Batch Loss: 0.056882329285144806\n",
      "Epoch 4565, Loss: 0.010536970105022192, Final Batch Loss: 0.007522133179008961\n",
      "Epoch 4566, Loss: 0.008395370794460177, Final Batch Loss: 0.0020260417368263006\n",
      "Epoch 4567, Loss: 0.04280232614837587, Final Batch Loss: 0.040335215628147125\n",
      "Epoch 4568, Loss: 0.06519946083426476, Final Batch Loss: 0.017393048852682114\n",
      "Epoch 4569, Loss: 0.01679334742948413, Final Batch Loss: 0.013230283744633198\n",
      "Epoch 4570, Loss: 0.014304277021437883, Final Batch Loss: 0.008856907486915588\n",
      "Epoch 4571, Loss: 0.009782374603673816, Final Batch Loss: 0.006105544976890087\n",
      "Epoch 4572, Loss: 0.013946033548563719, Final Batch Loss: 0.009644826874136925\n",
      "Epoch 4573, Loss: 0.12970137689262629, Final Batch Loss: 0.005213187076151371\n",
      "Epoch 4574, Loss: 0.0039215743308886886, Final Batch Loss: 0.002316547790542245\n",
      "Epoch 4575, Loss: 0.03078539646230638, Final Batch Loss: 0.0021651999559253454\n",
      "Epoch 4576, Loss: 0.020011169835925102, Final Batch Loss: 0.004755840636789799\n",
      "Epoch 4577, Loss: 0.06362581998109818, Final Batch Loss: 0.03514526039361954\n",
      "Epoch 4578, Loss: 0.07027747854590416, Final Batch Loss: 0.037413474172353745\n",
      "Epoch 4579, Loss: 0.0251166895031929, Final Batch Loss: 0.01588335819542408\n",
      "Epoch 4580, Loss: 0.029055439168587327, Final Batch Loss: 0.025615796446800232\n",
      "Epoch 4581, Loss: 0.0553571879863739, Final Batch Loss: 0.031818486750125885\n",
      "Epoch 4582, Loss: 0.014458792982622981, Final Batch Loss: 0.0010154757183045149\n",
      "Epoch 4583, Loss: 0.04030088405124843, Final Batch Loss: 0.002306450856849551\n",
      "Epoch 4584, Loss: 0.029949914664030075, Final Batch Loss: 0.013680778443813324\n",
      "Epoch 4585, Loss: 0.0587961720302701, Final Batch Loss: 0.04888908937573433\n",
      "Epoch 4586, Loss: 0.009521969710476696, Final Batch Loss: 0.008347338996827602\n",
      "Epoch 4587, Loss: 0.00858162296935916, Final Batch Loss: 0.006234949454665184\n",
      "Epoch 4588, Loss: 0.06150089530274272, Final Batch Loss: 0.05831014737486839\n",
      "Epoch 4589, Loss: 0.08404281921684742, Final Batch Loss: 0.06262239068746567\n",
      "Epoch 4590, Loss: 0.0076245039235800505, Final Batch Loss: 0.002693810733035207\n",
      "Epoch 4591, Loss: 0.004752885666675866, Final Batch Loss: 0.001916326000355184\n",
      "Epoch 4592, Loss: 0.01010105514433235, Final Batch Loss: 0.001222167513333261\n",
      "Epoch 4593, Loss: 0.073871762258932, Final Batch Loss: 0.06998082250356674\n",
      "Epoch 4594, Loss: 0.017287046648561954, Final Batch Loss: 0.008907120674848557\n",
      "Epoch 4595, Loss: 0.007381705800071359, Final Batch Loss: 0.002348752459511161\n",
      "Epoch 4596, Loss: 0.014504378661513329, Final Batch Loss: 0.00697129825130105\n",
      "Epoch 4597, Loss: 0.07230299338698387, Final Batch Loss: 0.04759407415986061\n",
      "Epoch 4598, Loss: 0.02737425360828638, Final Batch Loss: 0.0017418572679162025\n",
      "Epoch 4599, Loss: 0.012251128908246756, Final Batch Loss: 0.002479922492057085\n",
      "Epoch 4600, Loss: 0.10596693679690361, Final Batch Loss: 0.06165534630417824\n",
      "Epoch 4601, Loss: 0.035458272439427674, Final Batch Loss: 0.0017215554835274816\n",
      "Epoch 4602, Loss: 0.05984455719590187, Final Batch Loss: 0.05784804746508598\n",
      "Epoch 4603, Loss: 0.024383585900068283, Final Batch Loss: 0.018436402082443237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4604, Loss: 0.05821172799915075, Final Batch Loss: 0.0492493100464344\n",
      "Epoch 4605, Loss: 0.033830552361905575, Final Batch Loss: 0.020594291388988495\n",
      "Epoch 4606, Loss: 0.033730012364685535, Final Batch Loss: 0.02631501853466034\n",
      "Epoch 4607, Loss: 0.025911228731274605, Final Batch Loss: 0.003484874963760376\n",
      "Epoch 4608, Loss: 0.13901841267943382, Final Batch Loss: 0.11354877799749374\n",
      "Epoch 4609, Loss: 0.022859835997223854, Final Batch Loss: 0.018907206133008003\n",
      "Epoch 4610, Loss: 0.02742197271436453, Final Batch Loss: 0.010605248622596264\n",
      "Epoch 4611, Loss: 0.10000601969659328, Final Batch Loss: 0.07956815510988235\n",
      "Epoch 4612, Loss: 0.006279222550801933, Final Batch Loss: 0.001735998666845262\n",
      "Epoch 4613, Loss: 0.009616581024602056, Final Batch Loss: 0.003252284834161401\n",
      "Epoch 4614, Loss: 0.009061971679329872, Final Batch Loss: 0.003413162659853697\n",
      "Epoch 4615, Loss: 0.022689835634082556, Final Batch Loss: 0.004295303951948881\n",
      "Epoch 4616, Loss: 0.007823512074537575, Final Batch Loss: 0.0013333457754924893\n",
      "Epoch 4617, Loss: 0.04286295804195106, Final Batch Loss: 0.0019055630546063185\n",
      "Epoch 4618, Loss: 0.014480492973234504, Final Batch Loss: 0.0009129549725912511\n",
      "Epoch 4619, Loss: 0.0035873762099072337, Final Batch Loss: 0.0015721298987045884\n",
      "Epoch 4620, Loss: 0.005825098836794496, Final Batch Loss: 0.0033029194455593824\n",
      "Epoch 4621, Loss: 0.02787899086251855, Final Batch Loss: 0.020810917019844055\n",
      "Epoch 4622, Loss: 0.016670544631779194, Final Batch Loss: 0.012483146972954273\n",
      "Epoch 4623, Loss: 0.019746097270399332, Final Batch Loss: 0.006777557078748941\n",
      "Epoch 4624, Loss: 0.033421870321035385, Final Batch Loss: 0.017906740307807922\n",
      "Epoch 4625, Loss: 0.029271021485328674, Final Batch Loss: 0.005545763298869133\n",
      "Epoch 4626, Loss: 0.015360035467892885, Final Batch Loss: 0.011706320568919182\n",
      "Epoch 4627, Loss: 0.0067070715595036745, Final Batch Loss: 0.003425048431381583\n",
      "Epoch 4628, Loss: 0.02757355011999607, Final Batch Loss: 0.005466748028993607\n",
      "Epoch 4629, Loss: 0.030489179771393538, Final Batch Loss: 0.025017928332090378\n",
      "Epoch 4630, Loss: 0.018910008715465665, Final Batch Loss: 0.01673540472984314\n",
      "Epoch 4631, Loss: 0.008944883709773421, Final Batch Loss: 0.002617419930174947\n",
      "Epoch 4632, Loss: 0.00847180699929595, Final Batch Loss: 0.005908038001507521\n",
      "Epoch 4633, Loss: 0.011270228074863553, Final Batch Loss: 0.00338273704983294\n",
      "Epoch 4634, Loss: 0.00947444885969162, Final Batch Loss: 0.0031502326019108295\n",
      "Epoch 4635, Loss: 0.021071434719488025, Final Batch Loss: 0.017466247081756592\n",
      "Epoch 4636, Loss: 0.012248634360730648, Final Batch Loss: 0.0037605371326208115\n",
      "Epoch 4637, Loss: 0.02671953197568655, Final Batch Loss: 0.014316356740891933\n",
      "Epoch 4638, Loss: 0.034416806884109974, Final Batch Loss: 0.010120741091668606\n",
      "Epoch 4639, Loss: 0.010247021447867155, Final Batch Loss: 0.0038124057464301586\n",
      "Epoch 4640, Loss: 0.009878991171717644, Final Batch Loss: 0.004881748929619789\n",
      "Epoch 4641, Loss: 0.044652614276856184, Final Batch Loss: 0.038017794489860535\n",
      "Epoch 4642, Loss: 0.02052836399525404, Final Batch Loss: 0.014082202687859535\n",
      "Epoch 4643, Loss: 0.022640446200966835, Final Batch Loss: 0.008168958127498627\n",
      "Epoch 4644, Loss: 0.005409125005826354, Final Batch Loss: 0.0015786741860210896\n",
      "Epoch 4645, Loss: 0.010647126997355372, Final Batch Loss: 0.0005098372348584235\n",
      "Epoch 4646, Loss: 0.0027996465796604753, Final Batch Loss: 0.001528109540231526\n",
      "Epoch 4647, Loss: 0.02504449337720871, Final Batch Loss: 0.01631261594593525\n",
      "Epoch 4648, Loss: 0.061785017140209675, Final Batch Loss: 0.0578470379114151\n",
      "Epoch 4649, Loss: 0.0024726982228457928, Final Batch Loss: 0.0014301898190751672\n",
      "Epoch 4650, Loss: 0.01605428592301905, Final Batch Loss: 0.0023122953716665506\n",
      "Epoch 4651, Loss: 0.011026298045180738, Final Batch Loss: 0.009373973123729229\n",
      "Epoch 4652, Loss: 0.009656020207330585, Final Batch Loss: 0.002865075832232833\n",
      "Epoch 4653, Loss: 0.027629559859633446, Final Batch Loss: 0.022690480574965477\n",
      "Epoch 4654, Loss: 0.022841007681563497, Final Batch Loss: 0.0022196073550730944\n",
      "Epoch 4655, Loss: 0.00809828587807715, Final Batch Loss: 0.0030824479181319475\n",
      "Epoch 4656, Loss: 0.0050748297944664955, Final Batch Loss: 0.0010508955456316471\n",
      "Epoch 4657, Loss: 0.005120097892358899, Final Batch Loss: 0.003108447417616844\n",
      "Epoch 4658, Loss: 0.01675882237032056, Final Batch Loss: 0.004123534541577101\n",
      "Epoch 4659, Loss: 0.011103955330327153, Final Batch Loss: 0.0034945548977702856\n",
      "Epoch 4660, Loss: 0.013974625850096345, Final Batch Loss: 0.011747694574296474\n",
      "Epoch 4661, Loss: 0.03241191292181611, Final Batch Loss: 0.0026284330524504185\n",
      "Epoch 4662, Loss: 0.017644538776949048, Final Batch Loss: 0.014102224260568619\n",
      "Epoch 4663, Loss: 0.0033132118405774236, Final Batch Loss: 0.0010312009835615754\n",
      "Epoch 4664, Loss: 0.005186825059354305, Final Batch Loss: 0.0027205327060073614\n",
      "Epoch 4665, Loss: 0.03205071855336428, Final Batch Loss: 0.011519243009388447\n",
      "Epoch 4666, Loss: 0.010689321672543883, Final Batch Loss: 0.0036311650183051825\n",
      "Epoch 4667, Loss: 0.03504994709510356, Final Batch Loss: 0.0015656111063435674\n",
      "Epoch 4668, Loss: 0.0045898520620539784, Final Batch Loss: 0.0015677380142733455\n",
      "Epoch 4669, Loss: 0.014570354716852307, Final Batch Loss: 0.0018482732120901346\n",
      "Epoch 4670, Loss: 0.007765792892314494, Final Batch Loss: 0.0006875033723190427\n",
      "Epoch 4671, Loss: 0.025441249832510948, Final Batch Loss: 0.015464852564036846\n",
      "Epoch 4672, Loss: 0.02242658333852887, Final Batch Loss: 0.004825631622225046\n",
      "Epoch 4673, Loss: 0.026247835252434015, Final Batch Loss: 0.0023309760726988316\n",
      "Epoch 4674, Loss: 0.028468909673392773, Final Batch Loss: 0.009662480093538761\n",
      "Epoch 4675, Loss: 0.006750396685674787, Final Batch Loss: 0.0014248087536543608\n",
      "Epoch 4676, Loss: 0.005643193144351244, Final Batch Loss: 0.003311488777399063\n",
      "Epoch 4677, Loss: 0.03665579576045275, Final Batch Loss: 0.014415542595088482\n",
      "Epoch 4678, Loss: 0.007164097623899579, Final Batch Loss: 0.00335696036927402\n",
      "Epoch 4679, Loss: 0.007569390669232234, Final Batch Loss: 0.0004606566799338907\n",
      "Epoch 4680, Loss: 0.03276978060603142, Final Batch Loss: 0.019976401701569557\n",
      "Epoch 4681, Loss: 0.012524633202701807, Final Batch Loss: 0.005026025231927633\n",
      "Epoch 4682, Loss: 0.03983777388930321, Final Batch Loss: 0.014002328738570213\n",
      "Epoch 4683, Loss: 0.07263647951185703, Final Batch Loss: 0.042861949652433395\n",
      "Epoch 4684, Loss: 0.038775775814428926, Final Batch Loss: 0.03491386026144028\n",
      "Epoch 4685, Loss: 0.024184083566069603, Final Batch Loss: 0.011979072354733944\n",
      "Epoch 4686, Loss: 0.03604723792523146, Final Batch Loss: 0.0046400753781199455\n",
      "Epoch 4687, Loss: 0.0879589281976223, Final Batch Loss: 0.05217639356851578\n",
      "Epoch 4688, Loss: 0.016473248600959778, Final Batch Loss: 0.003347191959619522\n",
      "Epoch 4689, Loss: 0.007630200940184295, Final Batch Loss: 0.001172928954474628\n",
      "Epoch 4690, Loss: 0.021985956467688084, Final Batch Loss: 0.010874916799366474\n",
      "Epoch 4691, Loss: 0.02188585279509425, Final Batch Loss: 0.005905484315007925\n",
      "Epoch 4692, Loss: 0.010699245613068342, Final Batch Loss: 0.0054236771538853645\n",
      "Epoch 4693, Loss: 0.054861098527908325, Final Batch Loss: 0.04256249591708183\n",
      "Epoch 4694, Loss: 0.07165292277932167, Final Batch Loss: 0.047573868185281754\n",
      "Epoch 4695, Loss: 0.025859552901238203, Final Batch Loss: 0.019108200445771217\n",
      "Epoch 4696, Loss: 0.07751634903252125, Final Batch Loss: 0.023862337693572044\n",
      "Epoch 4697, Loss: 0.07263478636741638, Final Batch Loss: 0.04136521369218826\n",
      "Epoch 4698, Loss: 0.02649594470858574, Final Batch Loss: 0.013177402317523956\n",
      "Epoch 4699, Loss: 0.052660780027508736, Final Batch Loss: 0.028033476322889328\n",
      "Epoch 4700, Loss: 0.036784760653972626, Final Batch Loss: 0.0158732607960701\n",
      "Epoch 4701, Loss: 0.019025514251552522, Final Batch Loss: 0.0018461107974871993\n",
      "Epoch 4702, Loss: 0.026588221080601215, Final Batch Loss: 0.016921231523156166\n",
      "Epoch 4703, Loss: 0.0642469022423029, Final Batch Loss: 0.03063182346522808\n",
      "Epoch 4704, Loss: 0.006429912755265832, Final Batch Loss: 0.002480532741174102\n",
      "Epoch 4705, Loss: 0.01940239453688264, Final Batch Loss: 0.00559846730902791\n",
      "Epoch 4706, Loss: 0.02488689310848713, Final Batch Loss: 0.02080405317246914\n",
      "Epoch 4707, Loss: 0.01122973207384348, Final Batch Loss: 0.008276053704321384\n",
      "Epoch 4708, Loss: 0.056213654577732086, Final Batch Loss: 0.03121934086084366\n",
      "Epoch 4709, Loss: 0.013080684002488852, Final Batch Loss: 0.007428599987179041\n",
      "Epoch 4710, Loss: 0.011148757766932249, Final Batch Loss: 0.0032168938778340816\n",
      "Epoch 4711, Loss: 0.007434210507199168, Final Batch Loss: 0.0030640012118965387\n",
      "Epoch 4712, Loss: 0.03693574294447899, Final Batch Loss: 0.020873000845313072\n",
      "Epoch 4713, Loss: 0.05139663815498352, Final Batch Loss: 0.02542790211737156\n",
      "Epoch 4714, Loss: 0.00692249764688313, Final Batch Loss: 0.0027692990843206644\n",
      "Epoch 4715, Loss: 0.030475485138595104, Final Batch Loss: 0.02633797563612461\n",
      "Epoch 4716, Loss: 0.019391810055822134, Final Batch Loss: 0.004418937023729086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4717, Loss: 0.031828489154577255, Final Batch Loss: 0.00878199003636837\n",
      "Epoch 4718, Loss: 0.005664274329319596, Final Batch Loss: 0.0022782417945563793\n",
      "Epoch 4719, Loss: 0.0046934542478993535, Final Batch Loss: 0.0018628191901370883\n",
      "Epoch 4720, Loss: 0.027720195474103093, Final Batch Loss: 0.0037325595039874315\n",
      "Epoch 4721, Loss: 0.012963650864548981, Final Batch Loss: 0.0012135450961068273\n",
      "Epoch 4722, Loss: 0.05831116996705532, Final Batch Loss: 0.05243471637368202\n",
      "Epoch 4723, Loss: 0.03449141560122371, Final Batch Loss: 0.02885184809565544\n",
      "Epoch 4724, Loss: 0.09144913591444492, Final Batch Loss: 0.08913537114858627\n",
      "Epoch 4725, Loss: 0.06510607525706291, Final Batch Loss: 0.04801163077354431\n",
      "Epoch 4726, Loss: 0.015829280600883067, Final Batch Loss: 0.01445779763162136\n",
      "Epoch 4727, Loss: 0.03982554003596306, Final Batch Loss: 0.02027464099228382\n",
      "Epoch 4728, Loss: 0.03684932738542557, Final Batch Loss: 0.017942367121577263\n",
      "Epoch 4729, Loss: 0.04358166363090277, Final Batch Loss: 0.011389601044356823\n",
      "Epoch 4730, Loss: 0.013110592495650053, Final Batch Loss: 0.0021118787117302418\n",
      "Epoch 4731, Loss: 0.04746342124417424, Final Batch Loss: 0.04017248749732971\n",
      "Epoch 4732, Loss: 0.09703398123383522, Final Batch Loss: 0.03692302107810974\n",
      "Epoch 4733, Loss: 0.019758903654292226, Final Batch Loss: 0.0029073425102978945\n",
      "Epoch 4734, Loss: 0.09100524708628654, Final Batch Loss: 0.0671359971165657\n",
      "Epoch 4735, Loss: 0.010601805988699198, Final Batch Loss: 0.008453096263110638\n",
      "Epoch 4736, Loss: 0.03362008556723595, Final Batch Loss: 0.015551332384347916\n",
      "Epoch 4737, Loss: 0.00845604995265603, Final Batch Loss: 0.0016665994189679623\n",
      "Epoch 4738, Loss: 0.008295396575704217, Final Batch Loss: 0.002492388477548957\n",
      "Epoch 4739, Loss: 0.008395630400627851, Final Batch Loss: 0.005429850425571203\n",
      "Epoch 4740, Loss: 0.010606747353449464, Final Batch Loss: 0.0019550828728824854\n",
      "Epoch 4741, Loss: 0.027326952666044235, Final Batch Loss: 0.022646019235253334\n",
      "Epoch 4742, Loss: 0.028864988358691335, Final Batch Loss: 0.0017886713612824678\n",
      "Epoch 4743, Loss: 0.008857231587171555, Final Batch Loss: 0.0028347293846309185\n",
      "Epoch 4744, Loss: 0.024400045163929462, Final Batch Loss: 0.008266639895737171\n",
      "Epoch 4745, Loss: 0.012272830354049802, Final Batch Loss: 0.010001277551054955\n",
      "Epoch 4746, Loss: 0.02932767616584897, Final Batch Loss: 0.005613821092993021\n",
      "Epoch 4747, Loss: 0.038600748404860497, Final Batch Loss: 0.0239754319190979\n",
      "Epoch 4748, Loss: 0.03357817977666855, Final Batch Loss: 0.025795137509703636\n",
      "Epoch 4749, Loss: 0.007554474752396345, Final Batch Loss: 0.0028876755386590958\n",
      "Epoch 4750, Loss: 0.036598822800442576, Final Batch Loss: 0.003815504489466548\n",
      "Epoch 4751, Loss: 0.005585015518590808, Final Batch Loss: 0.0035812342539429665\n",
      "Epoch 4752, Loss: 0.007596266223117709, Final Batch Loss: 0.0057630110532045364\n",
      "Epoch 4753, Loss: 0.009193441132083535, Final Batch Loss: 0.005526314489543438\n",
      "Epoch 4754, Loss: 0.019132299814373255, Final Batch Loss: 0.014263086020946503\n",
      "Epoch 4755, Loss: 0.015632470371201634, Final Batch Loss: 0.013059692457318306\n",
      "Epoch 4756, Loss: 0.01467524515464902, Final Batch Loss: 0.006905891001224518\n",
      "Epoch 4757, Loss: 0.03592810034751892, Final Batch Loss: 0.015981949865818024\n",
      "Epoch 4758, Loss: 0.04888438433408737, Final Batch Loss: 0.04209126532077789\n",
      "Epoch 4759, Loss: 0.06611795723438263, Final Batch Loss: 0.061915989965200424\n",
      "Epoch 4760, Loss: 0.008263094816356897, Final Batch Loss: 0.0021956968121230602\n",
      "Epoch 4761, Loss: 0.009641591226682067, Final Batch Loss: 0.0028481290210038424\n",
      "Epoch 4762, Loss: 0.024021041579544544, Final Batch Loss: 0.021572792902588844\n",
      "Epoch 4763, Loss: 0.0033463536528870463, Final Batch Loss: 0.0013440089533105493\n",
      "Epoch 4764, Loss: 0.0035762442275881767, Final Batch Loss: 0.0021632062271237373\n",
      "Epoch 4765, Loss: 0.012166924308985472, Final Batch Loss: 0.006633317563682795\n",
      "Epoch 4766, Loss: 0.004811268183402717, Final Batch Loss: 0.0015008464688435197\n",
      "Epoch 4767, Loss: 0.011531904805451632, Final Batch Loss: 0.00815028790384531\n",
      "Epoch 4768, Loss: 0.014748032204806805, Final Batch Loss: 0.0070108938962221146\n",
      "Epoch 4769, Loss: 0.03825878584757447, Final Batch Loss: 0.03163402900099754\n",
      "Epoch 4770, Loss: 0.019956286065280437, Final Batch Loss: 0.0033757640048861504\n",
      "Epoch 4771, Loss: 0.026097901165485382, Final Batch Loss: 0.01670553907752037\n",
      "Epoch 4772, Loss: 0.04788320604711771, Final Batch Loss: 0.04348612204194069\n",
      "Epoch 4773, Loss: 0.008685752865858376, Final Batch Loss: 0.0011552810901775956\n",
      "Epoch 4774, Loss: 0.007022697012871504, Final Batch Loss: 0.003822576254606247\n",
      "Epoch 4775, Loss: 0.028041395358741283, Final Batch Loss: 0.009440899826586246\n",
      "Epoch 4776, Loss: 0.022265037056058645, Final Batch Loss: 0.004330604802817106\n",
      "Epoch 4777, Loss: 0.05826169264037162, Final Batch Loss: 0.0014030785532668233\n",
      "Epoch 4778, Loss: 0.08258097036741674, Final Batch Loss: 0.08029619604349136\n",
      "Epoch 4779, Loss: 0.09010902233421803, Final Batch Loss: 0.07992490381002426\n",
      "Epoch 4780, Loss: 0.09843059256672859, Final Batch Loss: 0.0456729494035244\n",
      "Epoch 4781, Loss: 0.01097403944004327, Final Batch Loss: 0.0013372610555961728\n",
      "Epoch 4782, Loss: 0.043435088358819485, Final Batch Loss: 0.035072583705186844\n",
      "Epoch 4783, Loss: 0.03452476114034653, Final Batch Loss: 0.010226497426629066\n",
      "Epoch 4784, Loss: 0.040103630162775517, Final Batch Loss: 0.00264002475887537\n",
      "Epoch 4785, Loss: 0.009924705140292645, Final Batch Loss: 0.004218631889671087\n",
      "Epoch 4786, Loss: 0.007244110223837197, Final Batch Loss: 0.005801972001791\n",
      "Epoch 4787, Loss: 0.024330590153113008, Final Batch Loss: 0.020995140075683594\n",
      "Epoch 4788, Loss: 0.015027958434075117, Final Batch Loss: 0.0037388154305517673\n",
      "Epoch 4789, Loss: 0.03678716137073934, Final Batch Loss: 0.034187331795692444\n",
      "Epoch 4790, Loss: 0.02235119277611375, Final Batch Loss: 0.005617501679807901\n",
      "Epoch 4791, Loss: 0.017454170156270266, Final Batch Loss: 0.013900385238230228\n",
      "Epoch 4792, Loss: 0.013764810282737017, Final Batch Loss: 0.009736685082316399\n",
      "Epoch 4793, Loss: 0.020302334800362587, Final Batch Loss: 0.010054847225546837\n",
      "Epoch 4794, Loss: 0.03591932263225317, Final Batch Loss: 0.02445535734295845\n",
      "Epoch 4795, Loss: 0.02777081774547696, Final Batch Loss: 0.006318649742752314\n",
      "Epoch 4796, Loss: 0.0196501380414702, Final Batch Loss: 0.0009736968786455691\n",
      "Epoch 4797, Loss: 0.02987338788807392, Final Batch Loss: 0.012117264792323112\n",
      "Epoch 4798, Loss: 0.018466404173523188, Final Batch Loss: 0.01236195582896471\n",
      "Epoch 4799, Loss: 0.009639624040573835, Final Batch Loss: 0.006337542086839676\n",
      "Epoch 4800, Loss: 0.009569086134433746, Final Batch Loss: 0.005125261377543211\n",
      "Epoch 4801, Loss: 0.029623291455209255, Final Batch Loss: 0.025548692792654037\n",
      "Epoch 4802, Loss: 0.007855391129851341, Final Batch Loss: 0.003907886799424887\n",
      "Epoch 4803, Loss: 0.01837143860757351, Final Batch Loss: 0.014299542643129826\n",
      "Epoch 4804, Loss: 0.007118197390809655, Final Batch Loss: 0.004679067526012659\n",
      "Epoch 4805, Loss: 0.014878610614687204, Final Batch Loss: 0.005337292794138193\n",
      "Epoch 4806, Loss: 0.037072951439768076, Final Batch Loss: 0.0023502768017351627\n",
      "Epoch 4807, Loss: 0.017464449629187584, Final Batch Loss: 0.012696385383605957\n",
      "Epoch 4808, Loss: 0.04329695273190737, Final Batch Loss: 0.0385565422475338\n",
      "Epoch 4809, Loss: 0.005260279402136803, Final Batch Loss: 0.0034187790006399155\n",
      "Epoch 4810, Loss: 0.00834207353182137, Final Batch Loss: 0.0030020710546523333\n",
      "Epoch 4811, Loss: 0.016036544926464558, Final Batch Loss: 0.0013213315978646278\n",
      "Epoch 4812, Loss: 0.029142812360078096, Final Batch Loss: 0.002036439720541239\n",
      "Epoch 4813, Loss: 0.017850368516519666, Final Batch Loss: 0.0035652604419738054\n",
      "Epoch 4814, Loss: 0.01577167445793748, Final Batch Loss: 0.0020803906954824924\n",
      "Epoch 4815, Loss: 0.011486484203487635, Final Batch Loss: 0.007272565737366676\n",
      "Epoch 4816, Loss: 0.011902956990525126, Final Batch Loss: 0.009837435558438301\n",
      "Epoch 4817, Loss: 0.01587323402054608, Final Batch Loss: 0.0017962672282010317\n",
      "Epoch 4818, Loss: 0.07390698790550232, Final Batch Loss: 0.03460489585995674\n",
      "Epoch 4819, Loss: 0.006381631130352616, Final Batch Loss: 0.0035715263802558184\n",
      "Epoch 4820, Loss: 0.05037176888436079, Final Batch Loss: 0.03544346243143082\n",
      "Epoch 4821, Loss: 0.046629410702735186, Final Batch Loss: 0.0039649237878620625\n",
      "Epoch 4822, Loss: 0.008607461350038648, Final Batch Loss: 0.0037262297701090574\n",
      "Epoch 4823, Loss: 0.011366815771907568, Final Batch Loss: 0.003004430327564478\n",
      "Epoch 4824, Loss: 0.003426644834689796, Final Batch Loss: 0.001729446230456233\n",
      "Epoch 4825, Loss: 0.016770658548921347, Final Batch Loss: 0.004589942749589682\n",
      "Epoch 4826, Loss: 0.015541747910901904, Final Batch Loss: 0.0026488692965358496\n",
      "Epoch 4827, Loss: 0.02009129524230957, Final Batch Loss: 0.010451815091073513\n",
      "Epoch 4828, Loss: 0.0035996478982269764, Final Batch Loss: 0.0016267811879515648\n",
      "Epoch 4829, Loss: 0.015667101833969355, Final Batch Loss: 0.009624093770980835\n",
      "Epoch 4830, Loss: 0.012558967107906938, Final Batch Loss: 0.0032612348441034555\n",
      "Epoch 4831, Loss: 0.007788474322296679, Final Batch Loss: 0.006013316567987204\n",
      "Epoch 4832, Loss: 0.010245299898087978, Final Batch Loss: 0.00757228210568428\n",
      "Epoch 4833, Loss: 0.006913649151101708, Final Batch Loss: 0.002989071188494563\n",
      "Epoch 4834, Loss: 0.06396309239789844, Final Batch Loss: 0.059673018753528595\n",
      "Epoch 4835, Loss: 0.01818909589201212, Final Batch Loss: 0.007611884735524654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4836, Loss: 0.012319701723754406, Final Batch Loss: 0.008878495544195175\n",
      "Epoch 4837, Loss: 0.005260297795757651, Final Batch Loss: 0.0021104898769408464\n",
      "Epoch 4838, Loss: 0.35779498144984245, Final Batch Loss: 0.29562515020370483\n",
      "Epoch 4839, Loss: 0.04813703894615173, Final Batch Loss: 0.04357583075761795\n",
      "Epoch 4840, Loss: 0.06343324668705463, Final Batch Loss: 0.03545493632555008\n",
      "Epoch 4841, Loss: 0.10436680354177952, Final Batch Loss: 0.07683125138282776\n",
      "Epoch 4842, Loss: 0.07941729202866554, Final Batch Loss: 0.059486836194992065\n",
      "Epoch 4843, Loss: 0.07194906647782773, Final Batch Loss: 0.001102493959479034\n",
      "Epoch 4844, Loss: 0.12535413727164268, Final Batch Loss: 0.07242521643638611\n",
      "Epoch 4845, Loss: 0.05174180865287781, Final Batch Loss: 0.019807294011116028\n",
      "Epoch 4846, Loss: 0.04170690383762121, Final Batch Loss: 0.014769605360925198\n",
      "Epoch 4847, Loss: 0.01171086821705103, Final Batch Loss: 0.0026330165565013885\n",
      "Epoch 4848, Loss: 0.06157076172530651, Final Batch Loss: 0.016860181465744972\n",
      "Epoch 4849, Loss: 0.028107539750635624, Final Batch Loss: 0.010791459120810032\n",
      "Epoch 4850, Loss: 0.018738204147666693, Final Batch Loss: 0.01130759622901678\n",
      "Epoch 4851, Loss: 0.023866118863224983, Final Batch Loss: 0.016804570332169533\n",
      "Epoch 4852, Loss: 0.01379131618887186, Final Batch Loss: 0.009494302794337273\n",
      "Epoch 4853, Loss: 0.015357591211795807, Final Batch Loss: 0.008400801569223404\n",
      "Epoch 4854, Loss: 0.017468609381467104, Final Batch Loss: 0.007125353906303644\n",
      "Epoch 4855, Loss: 0.00811865646392107, Final Batch Loss: 0.0037269615568220615\n",
      "Epoch 4856, Loss: 0.03154006972908974, Final Batch Loss: 0.009952973574399948\n",
      "Epoch 4857, Loss: 0.026286497013643384, Final Batch Loss: 0.00280855898745358\n",
      "Epoch 4858, Loss: 0.006861918140202761, Final Batch Loss: 0.002170389983803034\n",
      "Epoch 4859, Loss: 0.023874477483332157, Final Batch Loss: 0.013025868684053421\n",
      "Epoch 4860, Loss: 0.08187133446335793, Final Batch Loss: 0.031583961099386215\n",
      "Epoch 4861, Loss: 0.02057166351005435, Final Batch Loss: 0.0060372441075742245\n",
      "Epoch 4862, Loss: 0.019483843818306923, Final Batch Loss: 0.01540368888527155\n",
      "Epoch 4863, Loss: 0.014120180159807205, Final Batch Loss: 0.0029170410707592964\n",
      "Epoch 4864, Loss: 0.04964909330010414, Final Batch Loss: 0.002375580370426178\n",
      "Epoch 4865, Loss: 0.01033061696216464, Final Batch Loss: 0.005085422657430172\n",
      "Epoch 4866, Loss: 0.07054733298718929, Final Batch Loss: 0.016721194609999657\n",
      "Epoch 4867, Loss: 0.025252498453482985, Final Batch Loss: 0.0025799532886594534\n",
      "Epoch 4868, Loss: 0.005544274696148932, Final Batch Loss: 0.0017607529880478978\n",
      "Epoch 4869, Loss: 0.03742957522626966, Final Batch Loss: 0.0018406104063615203\n",
      "Epoch 4870, Loss: 0.08012665226124227, Final Batch Loss: 0.07696349918842316\n",
      "Epoch 4871, Loss: 0.005732829216867685, Final Batch Loss: 0.003314575180411339\n",
      "Epoch 4872, Loss: 0.028904077131301165, Final Batch Loss: 0.02661231905221939\n",
      "Epoch 4873, Loss: 0.05525489244610071, Final Batch Loss: 0.04527302458882332\n",
      "Epoch 4874, Loss: 0.044536651112139225, Final Batch Loss: 0.03942781314253807\n",
      "Epoch 4875, Loss: 0.031507247127592564, Final Batch Loss: 0.01521188486367464\n",
      "Epoch 4876, Loss: 0.008563691982999444, Final Batch Loss: 0.0053711142390966415\n",
      "Epoch 4877, Loss: 0.007919453084468842, Final Batch Loss: 0.005157883279025555\n",
      "Epoch 4878, Loss: 0.006628643721342087, Final Batch Loss: 0.0022815815173089504\n",
      "Epoch 4879, Loss: 0.015219028107821941, Final Batch Loss: 0.005283691920340061\n",
      "Epoch 4880, Loss: 0.036741732619702816, Final Batch Loss: 0.004305693320930004\n",
      "Epoch 4881, Loss: 0.009550722083076835, Final Batch Loss: 0.002001972170546651\n",
      "Epoch 4882, Loss: 0.01588208321481943, Final Batch Loss: 0.009152091108262539\n",
      "Epoch 4883, Loss: 0.01063347072340548, Final Batch Loss: 0.007337731309235096\n",
      "Epoch 4884, Loss: 0.015176754910498857, Final Batch Loss: 0.012981293722987175\n",
      "Epoch 4885, Loss: 0.011531205382198095, Final Batch Loss: 0.006833534222096205\n",
      "Epoch 4886, Loss: 0.02865593694150448, Final Batch Loss: 0.01572512835264206\n",
      "Epoch 4887, Loss: 0.01556424517184496, Final Batch Loss: 0.006790796294808388\n",
      "Epoch 4888, Loss: 0.015553740318864584, Final Batch Loss: 0.004929342772811651\n",
      "Epoch 4889, Loss: 0.029630891513079405, Final Batch Loss: 0.026162046939134598\n",
      "Epoch 4890, Loss: 0.0064519953448325396, Final Batch Loss: 0.0028908441308885813\n",
      "Epoch 4891, Loss: 0.006397446151822805, Final Batch Loss: 0.00208135973662138\n",
      "Epoch 4892, Loss: 0.011056571267545223, Final Batch Loss: 0.003157741390168667\n",
      "Epoch 4893, Loss: 0.02691917074844241, Final Batch Loss: 0.024028081446886063\n",
      "Epoch 4894, Loss: 0.010660232743248343, Final Batch Loss: 0.007885619066655636\n",
      "Epoch 4895, Loss: 0.027718046214431524, Final Batch Loss: 0.0017086989246308804\n",
      "Epoch 4896, Loss: 0.005743079353123903, Final Batch Loss: 0.002250860445201397\n",
      "Epoch 4897, Loss: 0.031923205591738224, Final Batch Loss: 0.011609568260610104\n",
      "Epoch 4898, Loss: 0.007097820518538356, Final Batch Loss: 0.0023140066768974066\n",
      "Epoch 4899, Loss: 0.024650387465953827, Final Batch Loss: 0.013580480590462685\n",
      "Epoch 4900, Loss: 0.011739699402824044, Final Batch Loss: 0.00038132653571665287\n",
      "Epoch 4901, Loss: 0.023994321934878826, Final Batch Loss: 0.01646748185157776\n",
      "Epoch 4902, Loss: 0.015882795909419656, Final Batch Loss: 0.013577599078416824\n",
      "Epoch 4903, Loss: 0.012477538082748652, Final Batch Loss: 0.005289157852530479\n",
      "Epoch 4904, Loss: 0.020744295325130224, Final Batch Loss: 0.002074624877423048\n",
      "Epoch 4905, Loss: 0.07098303409293294, Final Batch Loss: 0.06806694716215134\n",
      "Epoch 4906, Loss: 0.08943374967202544, Final Batch Loss: 0.08454709500074387\n",
      "Epoch 4907, Loss: 0.02088073908817023, Final Batch Loss: 0.0005644232733175159\n",
      "Epoch 4908, Loss: 0.11474774219095707, Final Batch Loss: 0.09181616455316544\n",
      "Epoch 4909, Loss: 0.033162547275424004, Final Batch Loss: 0.011137779802083969\n",
      "Epoch 4910, Loss: 0.029234914109110832, Final Batch Loss: 0.02228744514286518\n",
      "Epoch 4911, Loss: 0.021648154128342867, Final Batch Loss: 0.00456435838714242\n",
      "Epoch 4912, Loss: 0.03808020707219839, Final Batch Loss: 0.011448289267718792\n",
      "Epoch 4913, Loss: 0.03146622981876135, Final Batch Loss: 0.015618658624589443\n",
      "Epoch 4914, Loss: 0.030650547705590725, Final Batch Loss: 0.020659906789660454\n",
      "Epoch 4915, Loss: 0.03856557235121727, Final Batch Loss: 0.03038233518600464\n",
      "Epoch 4916, Loss: 0.016958129592239857, Final Batch Loss: 0.008960354141891003\n",
      "Epoch 4917, Loss: 0.045524852350354195, Final Batch Loss: 0.006940590217709541\n",
      "Epoch 4918, Loss: 0.09969460777938366, Final Batch Loss: 0.0741550549864769\n",
      "Epoch 4919, Loss: 0.010960673447698355, Final Batch Loss: 0.005678217858076096\n",
      "Epoch 4920, Loss: 0.0294177383184433, Final Batch Loss: 0.017850495874881744\n",
      "Epoch 4921, Loss: 0.023356580641120672, Final Batch Loss: 0.018344882875680923\n",
      "Epoch 4922, Loss: 0.05352596286684275, Final Batch Loss: 0.008318179287016392\n",
      "Epoch 4923, Loss: 0.01448432938195765, Final Batch Loss: 0.0025742577854543924\n",
      "Epoch 4924, Loss: 0.012079913634806871, Final Batch Loss: 0.004188592080026865\n",
      "Epoch 4925, Loss: 0.02518908679485321, Final Batch Loss: 0.007675401866436005\n",
      "Epoch 4926, Loss: 0.010065269423648715, Final Batch Loss: 0.002723213518038392\n",
      "Epoch 4927, Loss: 0.016800808254629374, Final Batch Loss: 0.004850134719163179\n",
      "Epoch 4928, Loss: 0.012382877990603447, Final Batch Loss: 0.009386683814227581\n",
      "Epoch 4929, Loss: 0.044303709641098976, Final Batch Loss: 0.023137282580137253\n",
      "Epoch 4930, Loss: 0.016422096407040954, Final Batch Loss: 0.0037248411681503057\n",
      "Epoch 4931, Loss: 0.03087684605270624, Final Batch Loss: 0.023616349324584007\n",
      "Epoch 4932, Loss: 0.017178650246933103, Final Batch Loss: 0.013639169745147228\n",
      "Epoch 4933, Loss: 0.013144122436642647, Final Batch Loss: 0.006569152232259512\n",
      "Epoch 4934, Loss: 0.010230162413790822, Final Batch Loss: 0.0025591321755200624\n",
      "Epoch 4935, Loss: 0.013904795981943607, Final Batch Loss: 0.007031838875263929\n",
      "Epoch 4936, Loss: 0.02045094594359398, Final Batch Loss: 0.004332469776272774\n",
      "Epoch 4937, Loss: 0.05918504390865564, Final Batch Loss: 0.015463278628885746\n",
      "Epoch 4938, Loss: 0.011150166392326355, Final Batch Loss: 0.00884233694523573\n",
      "Epoch 4939, Loss: 0.026242446154356003, Final Batch Loss: 0.0049714501947164536\n",
      "Epoch 4940, Loss: 0.03376163262873888, Final Batch Loss: 0.025768732652068138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4941, Loss: 0.03960661683231592, Final Batch Loss: 0.00700919795781374\n",
      "Epoch 4942, Loss: 0.026056276634335518, Final Batch Loss: 0.007183985784649849\n",
      "Epoch 4943, Loss: 0.005446347524411976, Final Batch Loss: 0.001656163134612143\n",
      "Epoch 4944, Loss: 0.0239004367031157, Final Batch Loss: 0.019389376044273376\n",
      "Epoch 4945, Loss: 0.017113859532400966, Final Batch Loss: 0.014422022737562656\n",
      "Epoch 4946, Loss: 0.05810175649821758, Final Batch Loss: 0.026376308873295784\n",
      "Epoch 4947, Loss: 0.015869629569351673, Final Batch Loss: 0.0064614564180374146\n",
      "Epoch 4948, Loss: 0.06161450408399105, Final Batch Loss: 0.046931784600019455\n",
      "Epoch 4949, Loss: 0.03748973039910197, Final Batch Loss: 0.0051189991645514965\n",
      "Epoch 4950, Loss: 0.012716234661638737, Final Batch Loss: 0.0025569526478648186\n",
      "Epoch 4951, Loss: 0.004520957241766155, Final Batch Loss: 0.001192718860693276\n",
      "Epoch 4952, Loss: 0.04993723798543215, Final Batch Loss: 0.03569062799215317\n",
      "Epoch 4953, Loss: 0.006484991870820522, Final Batch Loss: 0.0020560077391564846\n",
      "Epoch 4954, Loss: 0.030819129664450884, Final Batch Loss: 0.024611540138721466\n",
      "Epoch 4955, Loss: 0.028124261647462845, Final Batch Loss: 0.025726811960339546\n",
      "Epoch 4956, Loss: 0.040526014752686024, Final Batch Loss: 0.035686422139406204\n",
      "Epoch 4957, Loss: 0.020312772132456303, Final Batch Loss: 0.006868188269436359\n",
      "Epoch 4958, Loss: 0.0058396607637405396, Final Batch Loss: 0.0023832309525460005\n",
      "Epoch 4959, Loss: 0.01889957906678319, Final Batch Loss: 0.016209611669182777\n",
      "Epoch 4960, Loss: 0.03055269317701459, Final Batch Loss: 0.0031126835383474827\n",
      "Epoch 4961, Loss: 0.03695874358527362, Final Batch Loss: 0.002185606164857745\n",
      "Epoch 4962, Loss: 0.020709823351353407, Final Batch Loss: 0.007657820824533701\n",
      "Epoch 4963, Loss: 0.02841286174952984, Final Batch Loss: 0.01719561591744423\n",
      "Epoch 4964, Loss: 0.023436730494722724, Final Batch Loss: 0.0019374971743673086\n",
      "Epoch 4965, Loss: 0.02536777639761567, Final Batch Loss: 0.0027041477151215076\n",
      "Epoch 4966, Loss: 0.019860367756336927, Final Batch Loss: 0.005668974947184324\n",
      "Epoch 4967, Loss: 0.06382463127374649, Final Batch Loss: 0.054269276559352875\n",
      "Epoch 4968, Loss: 0.016048577148467302, Final Batch Loss: 0.005619425792247057\n",
      "Epoch 4969, Loss: 0.034432096872478724, Final Batch Loss: 0.027128534391522408\n",
      "Epoch 4970, Loss: 0.06303071789443493, Final Batch Loss: 0.03894806280732155\n",
      "Epoch 4971, Loss: 0.02368849888443947, Final Batch Loss: 0.007252315059304237\n",
      "Epoch 4972, Loss: 0.008729026245418936, Final Batch Loss: 0.0008633030229248106\n",
      "Epoch 4973, Loss: 0.03190510394051671, Final Batch Loss: 0.005368948448449373\n",
      "Epoch 4974, Loss: 0.049569856841117144, Final Batch Loss: 0.04262861981987953\n",
      "Epoch 4975, Loss: 0.03576506581157446, Final Batch Loss: 0.001696738414466381\n",
      "Epoch 4976, Loss: 0.01181796402670443, Final Batch Loss: 0.003508123802021146\n",
      "Epoch 4977, Loss: 0.007337827235460281, Final Batch Loss: 0.0010348777286708355\n",
      "Epoch 4978, Loss: 0.020695469225756824, Final Batch Loss: 0.0018818721873685718\n",
      "Epoch 4979, Loss: 0.01973878499120474, Final Batch Loss: 0.009238962084054947\n",
      "Epoch 4980, Loss: 0.007551976479589939, Final Batch Loss: 0.002986015286296606\n",
      "Epoch 4981, Loss: 0.011678324779495597, Final Batch Loss: 0.0028180561494082212\n",
      "Epoch 4982, Loss: 0.01427023159340024, Final Batch Loss: 0.009331218898296356\n",
      "Epoch 4983, Loss: 0.009756187442690134, Final Batch Loss: 0.0047311377711594105\n",
      "Epoch 4984, Loss: 0.020660371985286474, Final Batch Loss: 0.01634596846997738\n",
      "Epoch 4985, Loss: 0.006457493524067104, Final Batch Loss: 0.0016014314023777843\n",
      "Epoch 4986, Loss: 0.009285678155720234, Final Batch Loss: 0.001998733729124069\n",
      "Epoch 4987, Loss: 0.010766722494736314, Final Batch Loss: 0.0016464127693325281\n",
      "Epoch 4988, Loss: 0.022711037658154964, Final Batch Loss: 0.008741469122469425\n",
      "Epoch 4989, Loss: 0.005098056513816118, Final Batch Loss: 0.0026482578832656145\n",
      "Epoch 4990, Loss: 0.012639514054171741, Final Batch Loss: 0.0014874300686642528\n",
      "Epoch 4991, Loss: 0.01673723355634138, Final Batch Loss: 0.0006721855024807155\n",
      "Epoch 4992, Loss: 0.019786235177889466, Final Batch Loss: 0.0031188412103801966\n",
      "Epoch 4993, Loss: 0.01633817655965686, Final Batch Loss: 0.005851374473422766\n",
      "Epoch 4994, Loss: 0.017989843152463436, Final Batch Loss: 0.010270368307828903\n",
      "Epoch 4995, Loss: 0.02753825904801488, Final Batch Loss: 0.021200694143772125\n",
      "Epoch 4996, Loss: 0.007308690808713436, Final Batch Loss: 0.003438409650698304\n",
      "Epoch 4997, Loss: 0.028804839588701725, Final Batch Loss: 0.015613671392202377\n",
      "Epoch 4998, Loss: 0.005716440500691533, Final Batch Loss: 0.0025500860065221786\n",
      "Epoch 4999, Loss: 0.008063753601163626, Final Batch Loss: 0.004319964908063412\n",
      "Epoch 5000, Loss: 0.04433034220710397, Final Batch Loss: 0.04103630781173706\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24  2  0]\n",
      " [ 0 14  1]\n",
      " [ 0  1 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.923     0.960        26\n",
      "           1      0.824     0.933     0.875        15\n",
      "           2      0.962     0.962     0.962        26\n",
      "\n",
      "    accuracy                          0.940        67\n",
      "   macro avg      0.928     0.939     0.932        67\n",
      "weighted avg      0.946     0.940     0.942        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../saved_models/UCI 3 User Classifier Group 2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
