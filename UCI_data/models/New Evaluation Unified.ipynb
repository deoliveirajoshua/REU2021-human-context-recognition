{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.427680730819702, Final Batch Loss: 2.207463026046753\n",
      "Epoch 2, Loss: 4.430686712265015, Final Batch Loss: 2.216057777404785\n",
      "Epoch 3, Loss: 4.424543857574463, Final Batch Loss: 2.2134649753570557\n",
      "Epoch 4, Loss: 4.425635814666748, Final Batch Loss: 2.216773509979248\n",
      "Epoch 5, Loss: 4.419931888580322, Final Batch Loss: 2.206233501434326\n",
      "Epoch 6, Loss: 4.418008327484131, Final Batch Loss: 2.210620403289795\n",
      "Epoch 7, Loss: 4.418031692504883, Final Batch Loss: 2.214569091796875\n",
      "Epoch 8, Loss: 4.413766145706177, Final Batch Loss: 2.2116448879241943\n",
      "Epoch 9, Loss: 4.406684160232544, Final Batch Loss: 2.197998523712158\n",
      "Epoch 10, Loss: 4.407388210296631, Final Batch Loss: 2.2048020362854004\n",
      "Epoch 11, Loss: 4.402667284011841, Final Batch Loss: 2.1994669437408447\n",
      "Epoch 12, Loss: 4.4007298946380615, Final Batch Loss: 2.201507568359375\n",
      "Epoch 13, Loss: 4.397089719772339, Final Batch Loss: 2.20060133934021\n",
      "Epoch 14, Loss: 4.384135961532593, Final Batch Loss: 2.175703287124634\n",
      "Epoch 15, Loss: 4.392382860183716, Final Batch Loss: 2.1999154090881348\n",
      "Epoch 16, Loss: 4.386897563934326, Final Batch Loss: 2.189221143722534\n",
      "Epoch 17, Loss: 4.378387928009033, Final Batch Loss: 2.184009075164795\n",
      "Epoch 18, Loss: 4.377977609634399, Final Batch Loss: 2.187692165374756\n",
      "Epoch 19, Loss: 4.3749024868011475, Final Batch Loss: 2.1973514556884766\n",
      "Epoch 20, Loss: 4.36393141746521, Final Batch Loss: 2.1798431873321533\n",
      "Epoch 21, Loss: 4.354497671127319, Final Batch Loss: 2.1714749336242676\n",
      "Epoch 22, Loss: 4.337489604949951, Final Batch Loss: 2.158966541290283\n",
      "Epoch 23, Loss: 4.332868576049805, Final Batch Loss: 2.160583972930908\n",
      "Epoch 24, Loss: 4.323892831802368, Final Batch Loss: 2.155428647994995\n",
      "Epoch 25, Loss: 4.308244228363037, Final Batch Loss: 2.155097246170044\n",
      "Epoch 26, Loss: 4.28268837928772, Final Batch Loss: 2.142537832260132\n",
      "Epoch 27, Loss: 4.261971473693848, Final Batch Loss: 2.124284267425537\n",
      "Epoch 28, Loss: 4.256251335144043, Final Batch Loss: 2.1360623836517334\n",
      "Epoch 29, Loss: 4.212658405303955, Final Batch Loss: 2.1007473468780518\n",
      "Epoch 30, Loss: 4.173734188079834, Final Batch Loss: 2.076686382293701\n",
      "Epoch 31, Loss: 4.153368711471558, Final Batch Loss: 2.0753695964813232\n",
      "Epoch 32, Loss: 4.138857841491699, Final Batch Loss: 2.090090036392212\n",
      "Epoch 33, Loss: 4.069561004638672, Final Batch Loss: 2.0271754264831543\n",
      "Epoch 34, Loss: 4.018596410751343, Final Batch Loss: 2.0133578777313232\n",
      "Epoch 35, Loss: 3.947767972946167, Final Batch Loss: 1.9526714086532593\n",
      "Epoch 36, Loss: 3.918105959892273, Final Batch Loss: 1.930816650390625\n",
      "Epoch 37, Loss: 3.8694874048233032, Final Batch Loss: 1.9210796356201172\n",
      "Epoch 38, Loss: 3.811938524246216, Final Batch Loss: 1.8740906715393066\n",
      "Epoch 39, Loss: 3.7647889852523804, Final Batch Loss: 1.9123198986053467\n",
      "Epoch 40, Loss: 3.7362407445907593, Final Batch Loss: 1.927687644958496\n",
      "Epoch 41, Loss: 3.676405429840088, Final Batch Loss: 1.8119170665740967\n",
      "Epoch 42, Loss: 3.6081830263137817, Final Batch Loss: 1.8072428703308105\n",
      "Epoch 43, Loss: 3.5651828050613403, Final Batch Loss: 1.748468279838562\n",
      "Epoch 44, Loss: 3.5623528957366943, Final Batch Loss: 1.7701517343521118\n",
      "Epoch 45, Loss: 3.5515694618225098, Final Batch Loss: 1.7699248790740967\n",
      "Epoch 46, Loss: 3.5424857139587402, Final Batch Loss: 1.7613917589187622\n",
      "Epoch 47, Loss: 3.5023396015167236, Final Batch Loss: 1.759425163269043\n",
      "Epoch 48, Loss: 3.4727004766464233, Final Batch Loss: 1.7023522853851318\n",
      "Epoch 49, Loss: 3.4048221111297607, Final Batch Loss: 1.6664249897003174\n",
      "Epoch 50, Loss: 3.3220155239105225, Final Batch Loss: 1.6377359628677368\n",
      "Epoch 51, Loss: 3.29367458820343, Final Batch Loss: 1.6370126008987427\n",
      "Epoch 52, Loss: 3.3561140298843384, Final Batch Loss: 1.6980664730072021\n",
      "Epoch 53, Loss: 3.305046796798706, Final Batch Loss: 1.640128254890442\n",
      "Epoch 54, Loss: 3.2467775344848633, Final Batch Loss: 1.6288279294967651\n",
      "Epoch 55, Loss: 3.2622071504592896, Final Batch Loss: 1.618007779121399\n",
      "Epoch 56, Loss: 3.26789653301239, Final Batch Loss: 1.664939284324646\n",
      "Epoch 57, Loss: 3.205810785293579, Final Batch Loss: 1.5806719064712524\n",
      "Epoch 58, Loss: 3.1340138912200928, Final Batch Loss: 1.5619598627090454\n",
      "Epoch 59, Loss: 3.141270875930786, Final Batch Loss: 1.5705376863479614\n",
      "Epoch 60, Loss: 3.090615391731262, Final Batch Loss: 1.5037007331848145\n",
      "Epoch 61, Loss: 3.0445274114608765, Final Batch Loss: 1.4899840354919434\n",
      "Epoch 62, Loss: 3.003058075904846, Final Batch Loss: 1.5066949129104614\n",
      "Epoch 63, Loss: 3.0248059034347534, Final Batch Loss: 1.504125952720642\n",
      "Epoch 64, Loss: 2.9973286390304565, Final Batch Loss: 1.523878812789917\n",
      "Epoch 65, Loss: 2.983967423439026, Final Batch Loss: 1.5149105787277222\n",
      "Epoch 66, Loss: 2.9160571098327637, Final Batch Loss: 1.4529789686203003\n",
      "Epoch 67, Loss: 2.907975196838379, Final Batch Loss: 1.4598602056503296\n",
      "Epoch 68, Loss: 2.9258742332458496, Final Batch Loss: 1.5131093263626099\n",
      "Epoch 69, Loss: 2.8885629177093506, Final Batch Loss: 1.4613350629806519\n",
      "Epoch 70, Loss: 2.7853702306747437, Final Batch Loss: 1.3915071487426758\n",
      "Epoch 71, Loss: 2.857076048851013, Final Batch Loss: 1.4763166904449463\n",
      "Epoch 72, Loss: 2.753714680671692, Final Batch Loss: 1.3089145421981812\n",
      "Epoch 73, Loss: 2.803267478942871, Final Batch Loss: 1.3874444961547852\n",
      "Epoch 74, Loss: 2.7569446563720703, Final Batch Loss: 1.363296389579773\n",
      "Epoch 75, Loss: 2.7236177921295166, Final Batch Loss: 1.3799079656600952\n",
      "Epoch 76, Loss: 2.6762853860855103, Final Batch Loss: 1.3256810903549194\n",
      "Epoch 77, Loss: 2.7472599744796753, Final Batch Loss: 1.3670600652694702\n",
      "Epoch 78, Loss: 2.6539634466171265, Final Batch Loss: 1.3544210195541382\n",
      "Epoch 79, Loss: 2.6641393899917603, Final Batch Loss: 1.3676466941833496\n",
      "Epoch 80, Loss: 2.5798808336257935, Final Batch Loss: 1.3022820949554443\n",
      "Epoch 81, Loss: 2.5907760858535767, Final Batch Loss: 1.351156234741211\n",
      "Epoch 82, Loss: 2.5288106203079224, Final Batch Loss: 1.2389838695526123\n",
      "Epoch 83, Loss: 2.535393714904785, Final Batch Loss: 1.2557543516159058\n",
      "Epoch 84, Loss: 2.5846667289733887, Final Batch Loss: 1.261093258857727\n",
      "Epoch 85, Loss: 2.468820571899414, Final Batch Loss: 1.2246793508529663\n",
      "Epoch 86, Loss: 2.4929044246673584, Final Batch Loss: 1.2590335607528687\n",
      "Epoch 87, Loss: 2.400650382041931, Final Batch Loss: 1.2137963771820068\n",
      "Epoch 88, Loss: 2.4458407163619995, Final Batch Loss: 1.1815261840820312\n",
      "Epoch 89, Loss: 2.46901273727417, Final Batch Loss: 1.2776474952697754\n",
      "Epoch 90, Loss: 2.4914904832839966, Final Batch Loss: 1.226906180381775\n",
      "Epoch 91, Loss: 2.5200926065444946, Final Batch Loss: 1.2649009227752686\n",
      "Epoch 92, Loss: 2.3567386865615845, Final Batch Loss: 1.1266021728515625\n",
      "Epoch 93, Loss: 2.294934630393982, Final Batch Loss: 1.182819128036499\n",
      "Epoch 94, Loss: 2.3345004320144653, Final Batch Loss: 1.135949730873108\n",
      "Epoch 95, Loss: 2.395920157432556, Final Batch Loss: 1.1746826171875\n",
      "Epoch 96, Loss: 2.4036606550216675, Final Batch Loss: 1.1434069871902466\n",
      "Epoch 97, Loss: 2.3887263536453247, Final Batch Loss: 1.234887719154358\n",
      "Epoch 98, Loss: 2.3369414806365967, Final Batch Loss: 1.149930477142334\n",
      "Epoch 99, Loss: 2.269903063774109, Final Batch Loss: 1.1338545083999634\n",
      "Epoch 100, Loss: 2.3292816877365112, Final Batch Loss: 1.1584621667861938\n",
      "Epoch 101, Loss: 2.3067541122436523, Final Batch Loss: 1.1692962646484375\n",
      "Epoch 102, Loss: 2.3174983263015747, Final Batch Loss: 1.162434697151184\n",
      "Epoch 103, Loss: 2.347483992576599, Final Batch Loss: 1.2265112400054932\n",
      "Epoch 104, Loss: 2.359854817390442, Final Batch Loss: 1.1835049390792847\n",
      "Epoch 105, Loss: 2.3373974561691284, Final Batch Loss: 1.1685786247253418\n",
      "Epoch 106, Loss: 2.268654704093933, Final Batch Loss: 1.1502692699432373\n",
      "Epoch 107, Loss: 2.2742919921875, Final Batch Loss: 1.1829533576965332\n",
      "Epoch 108, Loss: 2.303876042366028, Final Batch Loss: 1.160798192024231\n",
      "Epoch 109, Loss: 2.15729820728302, Final Batch Loss: 1.0655003786087036\n",
      "Epoch 110, Loss: 2.243615746498108, Final Batch Loss: 1.173724889755249\n",
      "Epoch 111, Loss: 2.1775087118148804, Final Batch Loss: 1.1011542081832886\n",
      "Epoch 112, Loss: 2.2771244049072266, Final Batch Loss: 1.1603796482086182\n",
      "Epoch 113, Loss: 2.3470370769500732, Final Batch Loss: 1.2389980554580688\n",
      "Epoch 114, Loss: 2.1709136962890625, Final Batch Loss: 1.091289758682251\n",
      "Epoch 115, Loss: 2.1506550312042236, Final Batch Loss: 1.0391017198562622\n",
      "Epoch 116, Loss: 2.2028980255126953, Final Batch Loss: 1.1169792413711548\n",
      "Epoch 117, Loss: 2.1024224758148193, Final Batch Loss: 1.0651240348815918\n",
      "Epoch 118, Loss: 2.2071011066436768, Final Batch Loss: 1.1275110244750977\n",
      "Epoch 119, Loss: 2.1853750944137573, Final Batch Loss: 1.1235241889953613\n",
      "Epoch 120, Loss: 2.0409942865371704, Final Batch Loss: 1.0311237573623657\n",
      "Epoch 121, Loss: 2.0608400106430054, Final Batch Loss: 1.0499781370162964\n",
      "Epoch 122, Loss: 2.0546975135803223, Final Batch Loss: 1.0123393535614014\n",
      "Epoch 123, Loss: 2.1051011085510254, Final Batch Loss: 1.0897934436798096\n",
      "Epoch 124, Loss: 1.9705047607421875, Final Batch Loss: 0.9710988402366638\n",
      "Epoch 125, Loss: 1.9194475412368774, Final Batch Loss: 0.9471203088760376\n",
      "Epoch 126, Loss: 2.056731700897217, Final Batch Loss: 1.0115991830825806\n",
      "Epoch 127, Loss: 1.9757112264633179, Final Batch Loss: 0.9547430276870728\n",
      "Epoch 128, Loss: 1.8793330788612366, Final Batch Loss: 0.8852490186691284\n",
      "Epoch 129, Loss: 2.0138872265815735, Final Batch Loss: 0.996299684047699\n",
      "Epoch 130, Loss: 1.9307316541671753, Final Batch Loss: 0.9704099893569946\n",
      "Epoch 131, Loss: 2.0573232769966125, Final Batch Loss: 1.1273555755615234\n",
      "Epoch 132, Loss: 1.9523137211799622, Final Batch Loss: 0.9949676990509033\n",
      "Epoch 133, Loss: 1.8791999816894531, Final Batch Loss: 0.9219213128089905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, Loss: 1.895640790462494, Final Batch Loss: 0.9014413952827454\n",
      "Epoch 135, Loss: 1.9286581873893738, Final Batch Loss: 0.9512782692909241\n",
      "Epoch 136, Loss: 1.7677077651023865, Final Batch Loss: 0.8514296412467957\n",
      "Epoch 137, Loss: 1.8047423958778381, Final Batch Loss: 0.9092517495155334\n",
      "Epoch 138, Loss: 1.8391600251197815, Final Batch Loss: 0.947961688041687\n",
      "Epoch 139, Loss: 1.7866104245185852, Final Batch Loss: 0.8509378433227539\n",
      "Epoch 140, Loss: 1.778935194015503, Final Batch Loss: 0.9150768518447876\n",
      "Epoch 141, Loss: 1.817272126674652, Final Batch Loss: 0.942811906337738\n",
      "Epoch 142, Loss: 1.891390323638916, Final Batch Loss: 0.8947718739509583\n",
      "Epoch 143, Loss: 1.758235514163971, Final Batch Loss: 0.8392970561981201\n",
      "Epoch 144, Loss: 1.6116593480110168, Final Batch Loss: 0.7692733407020569\n",
      "Epoch 145, Loss: 1.6965954303741455, Final Batch Loss: 0.8963170647621155\n",
      "Epoch 146, Loss: 1.7073152661323547, Final Batch Loss: 0.8989359736442566\n",
      "Epoch 147, Loss: 1.7115889191627502, Final Batch Loss: 0.82979416847229\n",
      "Epoch 148, Loss: 1.7812432646751404, Final Batch Loss: 0.8992170691490173\n",
      "Epoch 149, Loss: 1.7732263207435608, Final Batch Loss: 0.8393867611885071\n",
      "Epoch 150, Loss: 1.696738600730896, Final Batch Loss: 0.8694972991943359\n",
      "Epoch 151, Loss: 1.6747854351997375, Final Batch Loss: 0.8700467348098755\n",
      "Epoch 152, Loss: 1.6602728962898254, Final Batch Loss: 0.771531343460083\n",
      "Epoch 153, Loss: 1.6117019653320312, Final Batch Loss: 0.7838000059127808\n",
      "Epoch 154, Loss: 1.6706148386001587, Final Batch Loss: 0.8605429530143738\n",
      "Epoch 155, Loss: 1.6151889562606812, Final Batch Loss: 0.8206714987754822\n",
      "Epoch 156, Loss: 1.6416566967964172, Final Batch Loss: 0.7991953492164612\n",
      "Epoch 157, Loss: 1.6175001859664917, Final Batch Loss: 0.7858010530471802\n",
      "Epoch 158, Loss: 1.6590187549591064, Final Batch Loss: 0.8326711058616638\n",
      "Epoch 159, Loss: 1.6047004461288452, Final Batch Loss: 0.8047816157341003\n",
      "Epoch 160, Loss: 1.691256046295166, Final Batch Loss: 0.8458437919616699\n",
      "Epoch 161, Loss: 1.6186140775680542, Final Batch Loss: 0.7916029691696167\n",
      "Epoch 162, Loss: 1.6609604954719543, Final Batch Loss: 0.785942018032074\n",
      "Epoch 163, Loss: 1.607835829257965, Final Batch Loss: 0.8204578161239624\n",
      "Epoch 164, Loss: 1.612067699432373, Final Batch Loss: 0.7886204123497009\n",
      "Epoch 165, Loss: 1.467886745929718, Final Batch Loss: 0.7445662617683411\n",
      "Epoch 166, Loss: 1.4968889355659485, Final Batch Loss: 0.7871037125587463\n",
      "Epoch 167, Loss: 1.4125076532363892, Final Batch Loss: 0.6675740480422974\n",
      "Epoch 168, Loss: 1.536640703678131, Final Batch Loss: 0.758367121219635\n",
      "Epoch 169, Loss: 1.4915271401405334, Final Batch Loss: 0.7704834938049316\n",
      "Epoch 170, Loss: 1.5373778939247131, Final Batch Loss: 0.7547069191932678\n",
      "Epoch 171, Loss: 1.5994146466255188, Final Batch Loss: 0.8657404184341431\n",
      "Epoch 172, Loss: 1.490441381931305, Final Batch Loss: 0.7854219079017639\n",
      "Epoch 173, Loss: 1.544455885887146, Final Batch Loss: 0.7405998706817627\n",
      "Epoch 174, Loss: 1.4476028084754944, Final Batch Loss: 0.6994317770004272\n",
      "Epoch 175, Loss: 1.5384476780891418, Final Batch Loss: 0.800258219242096\n",
      "Epoch 176, Loss: 1.4539109468460083, Final Batch Loss: 0.6526245474815369\n",
      "Epoch 177, Loss: 1.4338897466659546, Final Batch Loss: 0.67661452293396\n",
      "Epoch 178, Loss: 1.4862204194068909, Final Batch Loss: 0.7591491341590881\n",
      "Epoch 179, Loss: 1.4339337348937988, Final Batch Loss: 0.6785444617271423\n",
      "Epoch 180, Loss: 1.4366244077682495, Final Batch Loss: 0.714063823223114\n",
      "Epoch 181, Loss: 1.4913768768310547, Final Batch Loss: 0.811109185218811\n",
      "Epoch 182, Loss: 1.5142419934272766, Final Batch Loss: 0.7074326872825623\n",
      "Epoch 183, Loss: 1.4503962993621826, Final Batch Loss: 0.7041900157928467\n",
      "Epoch 184, Loss: 1.4991483688354492, Final Batch Loss: 0.7175090312957764\n",
      "Epoch 185, Loss: 1.4462263584136963, Final Batch Loss: 0.730054497718811\n",
      "Epoch 186, Loss: 1.412125587463379, Final Batch Loss: 0.6938395500183105\n",
      "Epoch 187, Loss: 1.3816770315170288, Final Batch Loss: 0.6940058469772339\n",
      "Epoch 188, Loss: 1.504835307598114, Final Batch Loss: 0.7078900337219238\n",
      "Epoch 189, Loss: 1.4021050333976746, Final Batch Loss: 0.7288800477981567\n",
      "Epoch 190, Loss: 1.41727215051651, Final Batch Loss: 0.7292647361755371\n",
      "Epoch 191, Loss: 1.3842191696166992, Final Batch Loss: 0.7885711193084717\n",
      "Epoch 192, Loss: 1.4121322631835938, Final Batch Loss: 0.6819179058074951\n",
      "Epoch 193, Loss: 1.3572474122047424, Final Batch Loss: 0.6327648758888245\n",
      "Epoch 194, Loss: 1.432544231414795, Final Batch Loss: 0.7255245447158813\n",
      "Epoch 195, Loss: 1.3763942122459412, Final Batch Loss: 0.6461862921714783\n",
      "Epoch 196, Loss: 1.5203874111175537, Final Batch Loss: 0.7945442795753479\n",
      "Epoch 197, Loss: 1.3981212973594666, Final Batch Loss: 0.7192553281784058\n",
      "Epoch 198, Loss: 1.383432149887085, Final Batch Loss: 0.7320367693901062\n",
      "Epoch 199, Loss: 1.453686237335205, Final Batch Loss: 0.7249143123626709\n",
      "Epoch 200, Loss: 1.290956437587738, Final Batch Loss: 0.6555708050727844\n",
      "Epoch 201, Loss: 1.3549942374229431, Final Batch Loss: 0.6343505382537842\n",
      "Epoch 202, Loss: 1.373983085155487, Final Batch Loss: 0.6215966939926147\n",
      "Epoch 203, Loss: 1.3448551297187805, Final Batch Loss: 0.6446467041969299\n",
      "Epoch 204, Loss: 1.3628965616226196, Final Batch Loss: 0.7330335974693298\n",
      "Epoch 205, Loss: 1.3008347749710083, Final Batch Loss: 0.6707256436347961\n",
      "Epoch 206, Loss: 1.3029353022575378, Final Batch Loss: 0.6614599227905273\n",
      "Epoch 207, Loss: 1.3294597268104553, Final Batch Loss: 0.6148268580436707\n",
      "Epoch 208, Loss: 1.3579086065292358, Final Batch Loss: 0.6807847619056702\n",
      "Epoch 209, Loss: 1.317756175994873, Final Batch Loss: 0.6966113448143005\n",
      "Epoch 210, Loss: 1.4152104258537292, Final Batch Loss: 0.7641947269439697\n",
      "Epoch 211, Loss: 1.2985838055610657, Final Batch Loss: 0.6228208541870117\n",
      "Epoch 212, Loss: 1.2954867482185364, Final Batch Loss: 0.6734986305236816\n",
      "Epoch 213, Loss: 1.3683328032493591, Final Batch Loss: 0.7035624980926514\n",
      "Epoch 214, Loss: 1.3340551257133484, Final Batch Loss: 0.6325935125350952\n",
      "Epoch 215, Loss: 1.2843432426452637, Final Batch Loss: 0.6378816962242126\n",
      "Epoch 216, Loss: 1.3837756514549255, Final Batch Loss: 0.6705703139305115\n",
      "Epoch 217, Loss: 1.2136386632919312, Final Batch Loss: 0.6142585277557373\n",
      "Epoch 218, Loss: 1.283400058746338, Final Batch Loss: 0.5843130946159363\n",
      "Epoch 219, Loss: 1.264791488647461, Final Batch Loss: 0.6605812311172485\n",
      "Epoch 220, Loss: 1.2351735830307007, Final Batch Loss: 0.6426740884780884\n",
      "Epoch 221, Loss: 1.3596667647361755, Final Batch Loss: 0.69780433177948\n",
      "Epoch 222, Loss: 1.376364529132843, Final Batch Loss: 0.7649770975112915\n",
      "Epoch 223, Loss: 1.3163521885871887, Final Batch Loss: 0.6389737725257874\n",
      "Epoch 224, Loss: 1.181596279144287, Final Batch Loss: 0.6034260392189026\n",
      "Epoch 225, Loss: 1.2945466041564941, Final Batch Loss: 0.6553085446357727\n",
      "Epoch 226, Loss: 1.2684845328330994, Final Batch Loss: 0.6188600659370422\n",
      "Epoch 227, Loss: 1.2333859205245972, Final Batch Loss: 0.6269065737724304\n",
      "Epoch 228, Loss: 1.1844972968101501, Final Batch Loss: 0.5842817425727844\n",
      "Epoch 229, Loss: 1.3341493010520935, Final Batch Loss: 0.6171524524688721\n",
      "Epoch 230, Loss: 1.2985668182373047, Final Batch Loss: 0.6316199898719788\n",
      "Epoch 231, Loss: 1.2130206823349, Final Batch Loss: 0.5559308528900146\n",
      "Epoch 232, Loss: 1.2919902205467224, Final Batch Loss: 0.6719400882720947\n",
      "Epoch 233, Loss: 1.2454211711883545, Final Batch Loss: 0.5884056091308594\n",
      "Epoch 234, Loss: 1.1606967449188232, Final Batch Loss: 0.5440371036529541\n",
      "Epoch 235, Loss: 1.2488885521888733, Final Batch Loss: 0.6388773322105408\n",
      "Epoch 236, Loss: 1.1524502038955688, Final Batch Loss: 0.5498029589653015\n",
      "Epoch 237, Loss: 1.1637820601463318, Final Batch Loss: 0.5541154742240906\n",
      "Epoch 238, Loss: 1.1666919589042664, Final Batch Loss: 0.5729092955589294\n",
      "Epoch 239, Loss: 1.2529614567756653, Final Batch Loss: 0.6075605750083923\n",
      "Epoch 240, Loss: 1.1652058959007263, Final Batch Loss: 0.5427360534667969\n",
      "Epoch 241, Loss: 1.172587811946869, Final Batch Loss: 0.5246860980987549\n",
      "Epoch 242, Loss: 1.2193379402160645, Final Batch Loss: 0.6547269821166992\n",
      "Epoch 243, Loss: 1.3048197627067566, Final Batch Loss: 0.6583721041679382\n",
      "Epoch 244, Loss: 1.233512043952942, Final Batch Loss: 0.6128897666931152\n",
      "Epoch 245, Loss: 1.2475347518920898, Final Batch Loss: 0.632041871547699\n",
      "Epoch 246, Loss: 1.2843709588050842, Final Batch Loss: 0.6200782060623169\n",
      "Epoch 247, Loss: 1.1194981932640076, Final Batch Loss: 0.5426522493362427\n",
      "Epoch 248, Loss: 1.0891842246055603, Final Batch Loss: 0.49302101135253906\n",
      "Epoch 249, Loss: 1.1370103359222412, Final Batch Loss: 0.5658481121063232\n",
      "Epoch 250, Loss: 1.1881813406944275, Final Batch Loss: 0.5997995138168335\n",
      "Epoch 251, Loss: 1.1959558129310608, Final Batch Loss: 0.5139422416687012\n",
      "Epoch 252, Loss: 1.2124013900756836, Final Batch Loss: 0.5888997316360474\n",
      "Epoch 253, Loss: 1.1785250306129456, Final Batch Loss: 0.5908291935920715\n",
      "Epoch 254, Loss: 1.1056492924690247, Final Batch Loss: 0.516526460647583\n",
      "Epoch 255, Loss: 1.122304081916809, Final Batch Loss: 0.551569938659668\n",
      "Epoch 256, Loss: 1.2804957628250122, Final Batch Loss: 0.6771456599235535\n",
      "Epoch 257, Loss: 1.1368934512138367, Final Batch Loss: 0.5430908203125\n",
      "Epoch 258, Loss: 1.2086812853813171, Final Batch Loss: 0.5664601922035217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259, Loss: 1.0779154300689697, Final Batch Loss: 0.5274035930633545\n",
      "Epoch 260, Loss: 1.1625540256500244, Final Batch Loss: 0.6344157457351685\n",
      "Epoch 261, Loss: 1.2747302651405334, Final Batch Loss: 0.6855672597885132\n",
      "Epoch 262, Loss: 1.0580711662769318, Final Batch Loss: 0.4954887330532074\n",
      "Epoch 263, Loss: 1.1215276718139648, Final Batch Loss: 0.4991387128829956\n",
      "Epoch 264, Loss: 1.094895362854004, Final Batch Loss: 0.5618290901184082\n",
      "Epoch 265, Loss: 1.1078624725341797, Final Batch Loss: 0.5289179682731628\n",
      "Epoch 266, Loss: 1.15422785282135, Final Batch Loss: 0.6271396279335022\n",
      "Epoch 267, Loss: 1.0685230493545532, Final Batch Loss: 0.5456312298774719\n",
      "Epoch 268, Loss: 1.2121337056159973, Final Batch Loss: 0.610424816608429\n",
      "Epoch 269, Loss: 1.1775811314582825, Final Batch Loss: 0.6590771675109863\n",
      "Epoch 270, Loss: 1.1283026337623596, Final Batch Loss: 0.5956453084945679\n",
      "Epoch 271, Loss: 1.147142231464386, Final Batch Loss: 0.5928599238395691\n",
      "Epoch 272, Loss: 1.1747117638587952, Final Batch Loss: 0.565301239490509\n",
      "Epoch 273, Loss: 1.130642056465149, Final Batch Loss: 0.5554454326629639\n",
      "Epoch 274, Loss: 1.0819201469421387, Final Batch Loss: 0.5117052793502808\n",
      "Epoch 275, Loss: 1.044532060623169, Final Batch Loss: 0.5078541040420532\n",
      "Epoch 276, Loss: 1.1337135434150696, Final Batch Loss: 0.54026859998703\n",
      "Epoch 277, Loss: 1.1372256875038147, Final Batch Loss: 0.628433108329773\n",
      "Epoch 278, Loss: 1.0785014033317566, Final Batch Loss: 0.57447350025177\n",
      "Epoch 279, Loss: 1.074288010597229, Final Batch Loss: 0.4914394021034241\n",
      "Epoch 280, Loss: 1.1137641072273254, Final Batch Loss: 0.5912513732910156\n",
      "Epoch 281, Loss: 0.9818389117717743, Final Batch Loss: 0.4599668085575104\n",
      "Epoch 282, Loss: 1.1582844853401184, Final Batch Loss: 0.6499554514884949\n",
      "Epoch 283, Loss: 1.1549859642982483, Final Batch Loss: 0.6402930617332458\n",
      "Epoch 284, Loss: 1.0483482778072357, Final Batch Loss: 0.4783913195133209\n",
      "Epoch 285, Loss: 1.0257201492786407, Final Batch Loss: 0.49494948983192444\n",
      "Epoch 286, Loss: 1.1300915479660034, Final Batch Loss: 0.514657199382782\n",
      "Epoch 287, Loss: 1.059208869934082, Final Batch Loss: 0.5119994878768921\n",
      "Epoch 288, Loss: 0.9965267181396484, Final Batch Loss: 0.5556033253669739\n",
      "Epoch 289, Loss: 1.0587838888168335, Final Batch Loss: 0.5059581398963928\n",
      "Epoch 290, Loss: 1.1382505297660828, Final Batch Loss: 0.5800012350082397\n",
      "Epoch 291, Loss: 1.0164492428302765, Final Batch Loss: 0.5189352035522461\n",
      "Epoch 292, Loss: 1.0112826824188232, Final Batch Loss: 0.5106682777404785\n",
      "Epoch 293, Loss: 1.1701816320419312, Final Batch Loss: 0.6095409393310547\n",
      "Epoch 294, Loss: 1.0369360148906708, Final Batch Loss: 0.549368143081665\n",
      "Epoch 295, Loss: 1.1461336016654968, Final Batch Loss: 0.5835669040679932\n",
      "Epoch 296, Loss: 1.0854060649871826, Final Batch Loss: 0.5265601873397827\n",
      "Epoch 297, Loss: 1.0042501389980316, Final Batch Loss: 0.4573017656803131\n",
      "Epoch 298, Loss: 1.0133361220359802, Final Batch Loss: 0.4452008605003357\n",
      "Epoch 299, Loss: 1.0806345343589783, Final Batch Loss: 0.5606815814971924\n",
      "Epoch 300, Loss: 1.0445311069488525, Final Batch Loss: 0.5261922478675842\n",
      "Epoch 301, Loss: 1.011498510837555, Final Batch Loss: 0.4559491276741028\n",
      "Epoch 302, Loss: 1.0754432678222656, Final Batch Loss: 0.5039025545120239\n",
      "Epoch 303, Loss: 1.1058929562568665, Final Batch Loss: 0.5060540437698364\n",
      "Epoch 304, Loss: 1.088611662387848, Final Batch Loss: 0.5705896615982056\n",
      "Epoch 305, Loss: 1.0055009722709656, Final Batch Loss: 0.4625072479248047\n",
      "Epoch 306, Loss: 1.0664055347442627, Final Batch Loss: 0.5345525145530701\n",
      "Epoch 307, Loss: 1.0459337532520294, Final Batch Loss: 0.5465762615203857\n",
      "Epoch 308, Loss: 1.1212746500968933, Final Batch Loss: 0.5361424684524536\n",
      "Epoch 309, Loss: 1.017718344926834, Final Batch Loss: 0.5242085456848145\n",
      "Epoch 310, Loss: 1.040310800075531, Final Batch Loss: 0.5395485162734985\n",
      "Epoch 311, Loss: 1.0339054465293884, Final Batch Loss: 0.495830237865448\n",
      "Epoch 312, Loss: 0.9955211877822876, Final Batch Loss: 0.5432612299919128\n",
      "Epoch 313, Loss: 1.0130772292613983, Final Batch Loss: 0.4740518033504486\n",
      "Epoch 314, Loss: 1.0352094173431396, Final Batch Loss: 0.5417096018791199\n",
      "Epoch 315, Loss: 0.9980864226818085, Final Batch Loss: 0.5267959833145142\n",
      "Epoch 316, Loss: 1.0567800998687744, Final Batch Loss: 0.5356191396713257\n",
      "Epoch 317, Loss: 0.9503155946731567, Final Batch Loss: 0.4983144700527191\n",
      "Epoch 318, Loss: 1.0047844350337982, Final Batch Loss: 0.5835601687431335\n",
      "Epoch 319, Loss: 1.0063181817531586, Final Batch Loss: 0.5352140069007874\n",
      "Epoch 320, Loss: 1.0293970108032227, Final Batch Loss: 0.4975097179412842\n",
      "Epoch 321, Loss: 1.0610844492912292, Final Batch Loss: 0.5502898097038269\n",
      "Epoch 322, Loss: 1.0229138433933258, Final Batch Loss: 0.5309942364692688\n",
      "Epoch 323, Loss: 1.0864253640174866, Final Batch Loss: 0.579030454158783\n",
      "Epoch 324, Loss: 0.984052449464798, Final Batch Loss: 0.4647499620914459\n",
      "Epoch 325, Loss: 1.0068633556365967, Final Batch Loss: 0.4902075529098511\n",
      "Epoch 326, Loss: 1.0008105039596558, Final Batch Loss: 0.4935154914855957\n",
      "Epoch 327, Loss: 1.0693374872207642, Final Batch Loss: 0.5428145527839661\n",
      "Epoch 328, Loss: 1.02040696144104, Final Batch Loss: 0.4932183623313904\n",
      "Epoch 329, Loss: 0.9638894200325012, Final Batch Loss: 0.4842992126941681\n",
      "Epoch 330, Loss: 0.9698635935783386, Final Batch Loss: 0.47026923298835754\n",
      "Epoch 331, Loss: 1.0330234467983246, Final Batch Loss: 0.5383032560348511\n",
      "Epoch 332, Loss: 0.9532914757728577, Final Batch Loss: 0.4435950517654419\n",
      "Epoch 333, Loss: 0.9942678809165955, Final Batch Loss: 0.4770749807357788\n",
      "Epoch 334, Loss: 1.0468432903289795, Final Batch Loss: 0.5160052180290222\n",
      "Epoch 335, Loss: 0.9971550405025482, Final Batch Loss: 0.4614535868167877\n",
      "Epoch 336, Loss: 1.0004523396492004, Final Batch Loss: 0.4565349221229553\n",
      "Epoch 337, Loss: 1.0595511198043823, Final Batch Loss: 0.5887297987937927\n",
      "Epoch 338, Loss: 1.0389270782470703, Final Batch Loss: 0.5119810700416565\n",
      "Epoch 339, Loss: 0.9621762037277222, Final Batch Loss: 0.4849337041378021\n",
      "Epoch 340, Loss: 1.0166916847229004, Final Batch Loss: 0.5223854780197144\n",
      "Epoch 341, Loss: 0.9678491950035095, Final Batch Loss: 0.5067630410194397\n",
      "Epoch 342, Loss: 0.9455435872077942, Final Batch Loss: 0.4157549738883972\n",
      "Epoch 343, Loss: 1.0929335355758667, Final Batch Loss: 0.6070723533630371\n",
      "Epoch 344, Loss: 1.0954947471618652, Final Batch Loss: 0.5114327669143677\n",
      "Epoch 345, Loss: 0.934732973575592, Final Batch Loss: 0.4878154993057251\n",
      "Epoch 346, Loss: 1.0181717276573181, Final Batch Loss: 0.5226745009422302\n",
      "Epoch 347, Loss: 0.9257713556289673, Final Batch Loss: 0.46211662888526917\n",
      "Epoch 348, Loss: 1.0170171856880188, Final Batch Loss: 0.4596974849700928\n",
      "Epoch 349, Loss: 0.9923785924911499, Final Batch Loss: 0.5241914391517639\n",
      "Epoch 350, Loss: 0.935465544462204, Final Batch Loss: 0.43431177735328674\n",
      "Epoch 351, Loss: 1.1143959760665894, Final Batch Loss: 0.5771909952163696\n",
      "Epoch 352, Loss: 1.045359879732132, Final Batch Loss: 0.5828084349632263\n",
      "Epoch 353, Loss: 0.9418489634990692, Final Batch Loss: 0.45730772614479065\n",
      "Epoch 354, Loss: 0.9360141158103943, Final Batch Loss: 0.47129490971565247\n",
      "Epoch 355, Loss: 1.0180359780788422, Final Batch Loss: 0.5258646607398987\n",
      "Epoch 356, Loss: 0.9682053327560425, Final Batch Loss: 0.4721628427505493\n",
      "Epoch 357, Loss: 0.9304295480251312, Final Batch Loss: 0.4557843506336212\n",
      "Epoch 358, Loss: 0.9469642341136932, Final Batch Loss: 0.422398179769516\n",
      "Epoch 359, Loss: 0.9726672172546387, Final Batch Loss: 0.434593141078949\n",
      "Epoch 360, Loss: 0.953961968421936, Final Batch Loss: 0.46794480085372925\n",
      "Epoch 361, Loss: 0.9944943785667419, Final Batch Loss: 0.43160557746887207\n",
      "Epoch 362, Loss: 1.0259526669979095, Final Batch Loss: 0.4895705282688141\n",
      "Epoch 363, Loss: 0.9799165427684784, Final Batch Loss: 0.49406901001930237\n",
      "Epoch 364, Loss: 1.0509646534919739, Final Batch Loss: 0.5261549353599548\n",
      "Epoch 365, Loss: 0.9251139760017395, Final Batch Loss: 0.4136009216308594\n",
      "Epoch 366, Loss: 0.9612932801246643, Final Batch Loss: 0.5005736351013184\n",
      "Epoch 367, Loss: 0.9644157290458679, Final Batch Loss: 0.4719710052013397\n",
      "Epoch 368, Loss: 0.9515926241874695, Final Batch Loss: 0.46666690707206726\n",
      "Epoch 369, Loss: 0.9414490163326263, Final Batch Loss: 0.43386754393577576\n",
      "Epoch 370, Loss: 0.8618271350860596, Final Batch Loss: 0.3571736216545105\n",
      "Epoch 371, Loss: 0.9430390000343323, Final Batch Loss: 0.46767762303352356\n",
      "Epoch 372, Loss: 0.959249883890152, Final Batch Loss: 0.4660719335079193\n",
      "Epoch 373, Loss: 0.8769464492797852, Final Batch Loss: 0.4113542437553406\n",
      "Epoch 374, Loss: 0.9938017427921295, Final Batch Loss: 0.4999535381793976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375, Loss: 0.9177896678447723, Final Batch Loss: 0.48070812225341797\n",
      "Epoch 376, Loss: 1.0286922454833984, Final Batch Loss: 0.5566493272781372\n",
      "Epoch 377, Loss: 1.0104429125785828, Final Batch Loss: 0.5500656962394714\n",
      "Epoch 378, Loss: 0.847988098859787, Final Batch Loss: 0.42865613102912903\n",
      "Epoch 379, Loss: 0.8794921040534973, Final Batch Loss: 0.4290086030960083\n",
      "Epoch 380, Loss: 0.8538897931575775, Final Batch Loss: 0.38201695680618286\n",
      "Epoch 381, Loss: 0.9061276912689209, Final Batch Loss: 0.4433367848396301\n",
      "Epoch 382, Loss: 0.9601548612117767, Final Batch Loss: 0.4993897080421448\n",
      "Epoch 383, Loss: 0.9535393714904785, Final Batch Loss: 0.4757559597492218\n",
      "Epoch 384, Loss: 0.9478330910205841, Final Batch Loss: 0.40308573842048645\n",
      "Epoch 385, Loss: 0.9417075216770172, Final Batch Loss: 0.4265904128551483\n",
      "Epoch 386, Loss: 1.011982798576355, Final Batch Loss: 0.540845513343811\n",
      "Epoch 387, Loss: 0.8460038006305695, Final Batch Loss: 0.4158148169517517\n",
      "Epoch 388, Loss: 0.8444273173809052, Final Batch Loss: 0.4067901372909546\n",
      "Epoch 389, Loss: 0.953632652759552, Final Batch Loss: 0.48436224460601807\n",
      "Epoch 390, Loss: 0.9253272414207458, Final Batch Loss: 0.38482218980789185\n",
      "Epoch 391, Loss: 0.8787699937820435, Final Batch Loss: 0.4452778398990631\n",
      "Epoch 392, Loss: 0.8383446335792542, Final Batch Loss: 0.40847697854042053\n",
      "Epoch 393, Loss: 0.9695973098278046, Final Batch Loss: 0.5211504697799683\n",
      "Epoch 394, Loss: 0.9274991154670715, Final Batch Loss: 0.46909835934638977\n",
      "Epoch 395, Loss: 0.9334194660186768, Final Batch Loss: 0.42102110385894775\n",
      "Epoch 396, Loss: 0.9411317408084869, Final Batch Loss: 0.44852203130722046\n",
      "Epoch 397, Loss: 0.9453006386756897, Final Batch Loss: 0.5122003555297852\n",
      "Epoch 398, Loss: 0.9182322025299072, Final Batch Loss: 0.45513391494750977\n",
      "Epoch 399, Loss: 0.880582183599472, Final Batch Loss: 0.4562144875526428\n",
      "Epoch 400, Loss: 0.9293890297412872, Final Batch Loss: 0.48477745056152344\n",
      "Epoch 401, Loss: 0.8261602818965912, Final Batch Loss: 0.3903812766075134\n",
      "Epoch 402, Loss: 0.9100599884986877, Final Batch Loss: 0.5081918835639954\n",
      "Epoch 403, Loss: 0.9237310588359833, Final Batch Loss: 0.4967634379863739\n",
      "Epoch 404, Loss: 0.9047726988792419, Final Batch Loss: 0.4472695291042328\n",
      "Epoch 405, Loss: 0.9082973599433899, Final Batch Loss: 0.40862855315208435\n",
      "Epoch 406, Loss: 0.8556360006332397, Final Batch Loss: 0.41522014141082764\n",
      "Epoch 407, Loss: 0.8853199183940887, Final Batch Loss: 0.4956882894039154\n",
      "Epoch 408, Loss: 0.8686339855194092, Final Batch Loss: 0.45676931738853455\n",
      "Epoch 409, Loss: 0.9030333459377289, Final Batch Loss: 0.4394718110561371\n",
      "Epoch 410, Loss: 0.9383681118488312, Final Batch Loss: 0.489632785320282\n",
      "Epoch 411, Loss: 0.8787297010421753, Final Batch Loss: 0.3799620270729065\n",
      "Epoch 412, Loss: 0.9448594152927399, Final Batch Loss: 0.4867163300514221\n",
      "Epoch 413, Loss: 0.9938242733478546, Final Batch Loss: 0.5654206275939941\n",
      "Epoch 414, Loss: 0.8459478616714478, Final Batch Loss: 0.3901599049568176\n",
      "Epoch 415, Loss: 0.9240248799324036, Final Batch Loss: 0.5166056156158447\n",
      "Epoch 416, Loss: 0.9359917640686035, Final Batch Loss: 0.43537449836730957\n",
      "Epoch 417, Loss: 0.8531043529510498, Final Batch Loss: 0.44631004333496094\n",
      "Epoch 418, Loss: 0.8963475823402405, Final Batch Loss: 0.41714346408843994\n",
      "Epoch 419, Loss: 0.8930050730705261, Final Batch Loss: 0.3872840404510498\n",
      "Epoch 420, Loss: 0.844061404466629, Final Batch Loss: 0.3689492344856262\n",
      "Epoch 421, Loss: 0.9025932550430298, Final Batch Loss: 0.48444437980651855\n",
      "Epoch 422, Loss: 0.9063343703746796, Final Batch Loss: 0.46745067834854126\n",
      "Epoch 423, Loss: 0.8231334686279297, Final Batch Loss: 0.38139569759368896\n",
      "Epoch 424, Loss: 0.9871199131011963, Final Batch Loss: 0.4880545437335968\n",
      "Epoch 425, Loss: 0.9578796327114105, Final Batch Loss: 0.5360082387924194\n",
      "Epoch 426, Loss: 0.8745491802692413, Final Batch Loss: 0.4508454203605652\n",
      "Epoch 427, Loss: 0.8927892744541168, Final Batch Loss: 0.4081990122795105\n",
      "Epoch 428, Loss: 0.8532478511333466, Final Batch Loss: 0.47759220004081726\n",
      "Epoch 429, Loss: 0.8653802275657654, Final Batch Loss: 0.40756481885910034\n",
      "Epoch 430, Loss: 0.825018048286438, Final Batch Loss: 0.365261971950531\n",
      "Epoch 431, Loss: 0.818922370672226, Final Batch Loss: 0.43126046657562256\n",
      "Epoch 432, Loss: 0.7851937115192413, Final Batch Loss: 0.38708043098449707\n",
      "Epoch 433, Loss: 0.9200855791568756, Final Batch Loss: 0.46351733803749084\n",
      "Epoch 434, Loss: 0.781728595495224, Final Batch Loss: 0.37953999638557434\n",
      "Epoch 435, Loss: 0.8432759940624237, Final Batch Loss: 0.4070483148097992\n",
      "Epoch 436, Loss: 0.8442499041557312, Final Batch Loss: 0.4329394996166229\n",
      "Epoch 437, Loss: 0.885668009519577, Final Batch Loss: 0.3731713593006134\n",
      "Epoch 438, Loss: 0.9062016904354095, Final Batch Loss: 0.4559086263179779\n",
      "Epoch 439, Loss: 0.9020573496818542, Final Batch Loss: 0.4977765679359436\n",
      "Epoch 440, Loss: 0.8389566540718079, Final Batch Loss: 0.4385952055454254\n",
      "Epoch 441, Loss: 0.8730336129665375, Final Batch Loss: 0.4569237530231476\n",
      "Epoch 442, Loss: 0.9035553336143494, Final Batch Loss: 0.47127842903137207\n",
      "Epoch 443, Loss: 0.8657008409500122, Final Batch Loss: 0.47282102704048157\n",
      "Epoch 444, Loss: 0.8726361989974976, Final Batch Loss: 0.44241663813591003\n",
      "Epoch 445, Loss: 0.7511132657527924, Final Batch Loss: 0.35473713278770447\n",
      "Epoch 446, Loss: 0.7848122715950012, Final Batch Loss: 0.3686602711677551\n",
      "Epoch 447, Loss: 0.9231632351875305, Final Batch Loss: 0.4698432385921478\n",
      "Epoch 448, Loss: 0.8119604289531708, Final Batch Loss: 0.45153072476387024\n",
      "Epoch 449, Loss: 0.866485595703125, Final Batch Loss: 0.47649019956588745\n",
      "Epoch 450, Loss: 0.8250155448913574, Final Batch Loss: 0.41035643219947815\n",
      "Epoch 451, Loss: 0.807915061712265, Final Batch Loss: 0.3518623113632202\n",
      "Epoch 452, Loss: 0.8597981035709381, Final Batch Loss: 0.4473261833190918\n",
      "Epoch 453, Loss: 0.8286896049976349, Final Batch Loss: 0.40756314992904663\n",
      "Epoch 454, Loss: 0.9217275083065033, Final Batch Loss: 0.49535641074180603\n",
      "Epoch 455, Loss: 0.9084274172782898, Final Batch Loss: 0.4570988118648529\n",
      "Epoch 456, Loss: 0.7651645541191101, Final Batch Loss: 0.3932162821292877\n",
      "Epoch 457, Loss: 0.7877622842788696, Final Batch Loss: 0.40585967898368835\n",
      "Epoch 458, Loss: 0.7808781266212463, Final Batch Loss: 0.3258478343486786\n",
      "Epoch 459, Loss: 0.8219532668590546, Final Batch Loss: 0.37871524691581726\n",
      "Epoch 460, Loss: 0.8189437389373779, Final Batch Loss: 0.421979695558548\n",
      "Epoch 461, Loss: 0.8130110800266266, Final Batch Loss: 0.3619687557220459\n",
      "Epoch 462, Loss: 0.8425423800945282, Final Batch Loss: 0.37057429552078247\n",
      "Epoch 463, Loss: 0.8500359058380127, Final Batch Loss: 0.4306662380695343\n",
      "Epoch 464, Loss: 0.8012103736400604, Final Batch Loss: 0.3516705334186554\n",
      "Epoch 465, Loss: 0.7709887027740479, Final Batch Loss: 0.28549838066101074\n",
      "Epoch 466, Loss: 0.8083925545215607, Final Batch Loss: 0.3650164008140564\n",
      "Epoch 467, Loss: 0.7769538164138794, Final Batch Loss: 0.40201541781425476\n",
      "Epoch 468, Loss: 0.8227607309818268, Final Batch Loss: 0.45928332209587097\n",
      "Epoch 469, Loss: 0.7708492279052734, Final Batch Loss: 0.37924277782440186\n",
      "Epoch 470, Loss: 0.8784504234790802, Final Batch Loss: 0.4393192231655121\n",
      "Epoch 471, Loss: 0.852070152759552, Final Batch Loss: 0.41857531666755676\n",
      "Epoch 472, Loss: 0.8667000532150269, Final Batch Loss: 0.4423326253890991\n",
      "Epoch 473, Loss: 0.813134104013443, Final Batch Loss: 0.3899485170841217\n",
      "Epoch 474, Loss: 0.8305066823959351, Final Batch Loss: 0.406113862991333\n",
      "Epoch 475, Loss: 0.7292262613773346, Final Batch Loss: 0.34834468364715576\n",
      "Epoch 476, Loss: 0.7668967545032501, Final Batch Loss: 0.30565550923347473\n",
      "Epoch 477, Loss: 0.9435284435749054, Final Batch Loss: 0.48766976594924927\n",
      "Epoch 478, Loss: 0.8544890284538269, Final Batch Loss: 0.42988815903663635\n",
      "Epoch 479, Loss: 0.7863684594631195, Final Batch Loss: 0.35285985469818115\n",
      "Epoch 480, Loss: 0.8278311789035797, Final Batch Loss: 0.4404064416885376\n",
      "Epoch 481, Loss: 0.8422921299934387, Final Batch Loss: 0.44363346695899963\n",
      "Epoch 482, Loss: 0.7798007428646088, Final Batch Loss: 0.34709417819976807\n",
      "Epoch 483, Loss: 0.7936658263206482, Final Batch Loss: 0.38078442215919495\n",
      "Epoch 484, Loss: 0.8914487361907959, Final Batch Loss: 0.5213658213615417\n",
      "Epoch 485, Loss: 0.7557012736797333, Final Batch Loss: 0.35434412956237793\n",
      "Epoch 486, Loss: 0.8151605725288391, Final Batch Loss: 0.4007352292537689\n",
      "Epoch 487, Loss: 0.799068808555603, Final Batch Loss: 0.364133358001709\n",
      "Epoch 488, Loss: 0.8147118985652924, Final Batch Loss: 0.39507049322128296\n",
      "Epoch 489, Loss: 0.8755900263786316, Final Batch Loss: 0.45165663957595825\n",
      "Epoch 490, Loss: 0.8198884725570679, Final Batch Loss: 0.4325813353061676\n",
      "Epoch 491, Loss: 0.7976681888103485, Final Batch Loss: 0.413102388381958\n",
      "Epoch 492, Loss: 0.7849239110946655, Final Batch Loss: 0.34478890895843506\n",
      "Epoch 493, Loss: 0.7732486724853516, Final Batch Loss: 0.39845383167266846\n",
      "Epoch 494, Loss: 0.8631927073001862, Final Batch Loss: 0.43883591890335083\n",
      "Epoch 495, Loss: 0.8378496170043945, Final Batch Loss: 0.4047216773033142\n",
      "Epoch 496, Loss: 0.8082187473773956, Final Batch Loss: 0.4293875992298126\n",
      "Epoch 497, Loss: 0.8242598474025726, Final Batch Loss: 0.4048794209957123\n",
      "Epoch 498, Loss: 0.8405181169509888, Final Batch Loss: 0.40413570404052734\n",
      "Epoch 499, Loss: 0.7864927649497986, Final Batch Loss: 0.4023245871067047\n",
      "Epoch 500, Loss: 0.7908557653427124, Final Batch Loss: 0.3569523096084595\n",
      "Epoch 501, Loss: 0.8588204681873322, Final Batch Loss: 0.44947361946105957\n",
      "Epoch 502, Loss: 0.8016241788864136, Final Batch Loss: 0.3521336317062378\n",
      "Epoch 503, Loss: 0.7758785784244537, Final Batch Loss: 0.3885113000869751\n",
      "Epoch 504, Loss: 0.8095202147960663, Final Batch Loss: 0.4121217131614685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505, Loss: 0.7728246748447418, Final Batch Loss: 0.3576502501964569\n",
      "Epoch 506, Loss: 0.8118916153907776, Final Batch Loss: 0.3717544674873352\n",
      "Epoch 507, Loss: 0.7920491397380829, Final Batch Loss: 0.44200947880744934\n",
      "Epoch 508, Loss: 0.8214981257915497, Final Batch Loss: 0.4409494698047638\n",
      "Epoch 509, Loss: 0.892907589673996, Final Batch Loss: 0.48063206672668457\n",
      "Epoch 510, Loss: 0.7852971255779266, Final Batch Loss: 0.3787388205528259\n",
      "Epoch 511, Loss: 0.7882833182811737, Final Batch Loss: 0.4200178384780884\n",
      "Epoch 512, Loss: 0.8218179047107697, Final Batch Loss: 0.4223887026309967\n",
      "Epoch 513, Loss: 0.7829708755016327, Final Batch Loss: 0.39925435185432434\n",
      "Epoch 514, Loss: 0.7803818583488464, Final Batch Loss: 0.3643655776977539\n",
      "Epoch 515, Loss: 0.7633792161941528, Final Batch Loss: 0.39694079756736755\n",
      "Epoch 516, Loss: 0.7839435338973999, Final Batch Loss: 0.3360777497291565\n",
      "Epoch 517, Loss: 0.8199158012866974, Final Batch Loss: 0.43150442838668823\n",
      "Epoch 518, Loss: 0.836047351360321, Final Batch Loss: 0.4196370840072632\n",
      "Epoch 519, Loss: 0.7704443335533142, Final Batch Loss: 0.3647582232952118\n",
      "Epoch 520, Loss: 0.8307362198829651, Final Batch Loss: 0.41393572092056274\n",
      "Epoch 521, Loss: 0.7409079372882843, Final Batch Loss: 0.3551309406757355\n",
      "Epoch 522, Loss: 0.7762355208396912, Final Batch Loss: 0.4088112413883209\n",
      "Epoch 523, Loss: 0.8945932388305664, Final Batch Loss: 0.3603731393814087\n",
      "Epoch 524, Loss: 0.7305936217308044, Final Batch Loss: 0.34449878334999084\n",
      "Epoch 525, Loss: 0.7557418942451477, Final Batch Loss: 0.40152591466903687\n",
      "Epoch 526, Loss: 0.7804952561855316, Final Batch Loss: 0.4301566183567047\n",
      "Epoch 527, Loss: 0.8125241100788116, Final Batch Loss: 0.4392629563808441\n",
      "Epoch 528, Loss: 0.7535804808139801, Final Batch Loss: 0.36591029167175293\n",
      "Epoch 529, Loss: 0.7606929242610931, Final Batch Loss: 0.37537306547164917\n",
      "Epoch 530, Loss: 0.7793362140655518, Final Batch Loss: 0.3538303077220917\n",
      "Epoch 531, Loss: 0.7988498508930206, Final Batch Loss: 0.3544612526893616\n",
      "Epoch 532, Loss: 0.7616571187973022, Final Batch Loss: 0.3722356855869293\n",
      "Epoch 533, Loss: 0.7797172963619232, Final Batch Loss: 0.4046236276626587\n",
      "Epoch 534, Loss: 0.7212942838668823, Final Batch Loss: 0.35829049348831177\n",
      "Epoch 535, Loss: 0.7216433584690094, Final Batch Loss: 0.3030796945095062\n",
      "Epoch 536, Loss: 0.7721675932407379, Final Batch Loss: 0.379256010055542\n",
      "Epoch 537, Loss: 0.8553299903869629, Final Batch Loss: 0.4831903278827667\n",
      "Epoch 538, Loss: 0.7320515811443329, Final Batch Loss: 0.32941770553588867\n",
      "Epoch 539, Loss: 0.7041143178939819, Final Batch Loss: 0.2838842570781708\n",
      "Epoch 540, Loss: 0.787105917930603, Final Batch Loss: 0.429052472114563\n",
      "Epoch 541, Loss: 0.7993969023227692, Final Batch Loss: 0.41549304127693176\n",
      "Epoch 542, Loss: 0.6970150470733643, Final Batch Loss: 0.3177182078361511\n",
      "Epoch 543, Loss: 0.7971347570419312, Final Batch Loss: 0.3787368834018707\n",
      "Epoch 544, Loss: 0.7442159354686737, Final Batch Loss: 0.3777415156364441\n",
      "Epoch 545, Loss: 0.7289874255657196, Final Batch Loss: 0.3737894892692566\n",
      "Epoch 546, Loss: 0.7119404971599579, Final Batch Loss: 0.26047518849372864\n",
      "Epoch 547, Loss: 0.8332493901252747, Final Batch Loss: 0.4705660045146942\n",
      "Epoch 548, Loss: 0.7238405048847198, Final Batch Loss: 0.3583563268184662\n",
      "Epoch 549, Loss: 0.804719865322113, Final Batch Loss: 0.41959452629089355\n",
      "Epoch 550, Loss: 0.7942118048667908, Final Batch Loss: 0.33903464674949646\n",
      "Epoch 551, Loss: 0.7604016661643982, Final Batch Loss: 0.3466600179672241\n",
      "Epoch 552, Loss: 0.6802520751953125, Final Batch Loss: 0.3380407392978668\n",
      "Epoch 553, Loss: 0.789844810962677, Final Batch Loss: 0.37680304050445557\n",
      "Epoch 554, Loss: 0.748117059469223, Final Batch Loss: 0.35866010189056396\n",
      "Epoch 555, Loss: 0.7296008467674255, Final Batch Loss: 0.327888548374176\n",
      "Epoch 556, Loss: 0.7662432789802551, Final Batch Loss: 0.3536287844181061\n",
      "Epoch 557, Loss: 0.7212620377540588, Final Batch Loss: 0.32987916469573975\n",
      "Epoch 558, Loss: 0.7715958654880524, Final Batch Loss: 0.3989318609237671\n",
      "Epoch 559, Loss: 0.7708485126495361, Final Batch Loss: 0.39260995388031006\n",
      "Epoch 560, Loss: 0.7352055311203003, Final Batch Loss: 0.3714623749256134\n",
      "Epoch 561, Loss: 0.7301552295684814, Final Batch Loss: 0.33906203508377075\n",
      "Epoch 562, Loss: 0.7133924067020416, Final Batch Loss: 0.35050004720687866\n",
      "Epoch 563, Loss: 0.8113616704940796, Final Batch Loss: 0.36358165740966797\n",
      "Epoch 564, Loss: 0.8197738528251648, Final Batch Loss: 0.5013486742973328\n",
      "Epoch 565, Loss: 0.7001650929450989, Final Batch Loss: 0.3139072060585022\n",
      "Epoch 566, Loss: 0.7363709211349487, Final Batch Loss: 0.3408912718296051\n",
      "Epoch 567, Loss: 0.7910726368427277, Final Batch Loss: 0.372087687253952\n",
      "Epoch 568, Loss: 0.8275002539157867, Final Batch Loss: 0.4416837692260742\n",
      "Epoch 569, Loss: 0.7647983729839325, Final Batch Loss: 0.38522064685821533\n",
      "Epoch 570, Loss: 0.698277622461319, Final Batch Loss: 0.303297221660614\n",
      "Epoch 571, Loss: 0.7048981189727783, Final Batch Loss: 0.30472373962402344\n",
      "Epoch 572, Loss: 0.7929391264915466, Final Batch Loss: 0.39847055077552795\n",
      "Epoch 573, Loss: 0.7771990299224854, Final Batch Loss: 0.4394289255142212\n",
      "Epoch 574, Loss: 0.828078418970108, Final Batch Loss: 0.35842543840408325\n",
      "Epoch 575, Loss: 0.7769657969474792, Final Batch Loss: 0.3876032531261444\n",
      "Epoch 576, Loss: 0.9583412408828735, Final Batch Loss: 0.570858359336853\n",
      "Epoch 577, Loss: 0.7657676637172699, Final Batch Loss: 0.37774014472961426\n",
      "Epoch 578, Loss: 0.7820017635822296, Final Batch Loss: 0.37411177158355713\n",
      "Epoch 579, Loss: 0.6976957619190216, Final Batch Loss: 0.29608750343322754\n",
      "Epoch 580, Loss: 0.7951760292053223, Final Batch Loss: 0.3922122120857239\n",
      "Epoch 581, Loss: 0.7674140334129333, Final Batch Loss: 0.3550482392311096\n",
      "Epoch 582, Loss: 0.7775973081588745, Final Batch Loss: 0.3518938422203064\n",
      "Epoch 583, Loss: 0.7845911383628845, Final Batch Loss: 0.4147147834300995\n",
      "Epoch 584, Loss: 0.6840613782405853, Final Batch Loss: 0.2793978452682495\n",
      "Epoch 585, Loss: 0.7066067159175873, Final Batch Loss: 0.34583550691604614\n",
      "Epoch 586, Loss: 0.7920998930931091, Final Batch Loss: 0.37275147438049316\n",
      "Epoch 587, Loss: 0.7735159695148468, Final Batch Loss: 0.41458895802497864\n",
      "Epoch 588, Loss: 0.6889676749706268, Final Batch Loss: 0.35321497917175293\n",
      "Epoch 589, Loss: 0.8446835875511169, Final Batch Loss: 0.43276071548461914\n",
      "Epoch 590, Loss: 0.7501019835472107, Final Batch Loss: 0.4583633244037628\n",
      "Epoch 591, Loss: 0.7805581390857697, Final Batch Loss: 0.4283920228481293\n",
      "Epoch 592, Loss: 0.7337324023246765, Final Batch Loss: 0.3836462199687958\n",
      "Epoch 593, Loss: 0.7766998410224915, Final Batch Loss: 0.3725546896457672\n",
      "Epoch 594, Loss: 0.7081746459007263, Final Batch Loss: 0.37892580032348633\n",
      "Epoch 595, Loss: 0.7138322591781616, Final Batch Loss: 0.3117634356021881\n",
      "Epoch 596, Loss: 0.7294420599937439, Final Batch Loss: 0.3919691741466522\n",
      "Epoch 597, Loss: 0.73637256026268, Final Batch Loss: 0.3836745023727417\n",
      "Epoch 598, Loss: 0.7534383535385132, Final Batch Loss: 0.39806443452835083\n",
      "Epoch 599, Loss: 0.7856839001178741, Final Batch Loss: 0.376461923122406\n",
      "Epoch 600, Loss: 0.7545106410980225, Final Batch Loss: 0.3558303415775299\n",
      "Epoch 601, Loss: 0.7308264672756195, Final Batch Loss: 0.3458024561405182\n",
      "Epoch 602, Loss: 0.7680473625659943, Final Batch Loss: 0.36065474152565\n",
      "Epoch 603, Loss: 0.699075311422348, Final Batch Loss: 0.3146267533302307\n",
      "Epoch 604, Loss: 0.7441566288471222, Final Batch Loss: 0.3692815601825714\n",
      "Epoch 605, Loss: 0.7277223467826843, Final Batch Loss: 0.3863355219364166\n",
      "Epoch 606, Loss: 0.8150179982185364, Final Batch Loss: 0.45298200845718384\n",
      "Epoch 607, Loss: 0.721177726984024, Final Batch Loss: 0.3394618630409241\n",
      "Epoch 608, Loss: 0.7652452886104584, Final Batch Loss: 0.36841142177581787\n",
      "Epoch 609, Loss: 0.7602325677871704, Final Batch Loss: 0.4227829873561859\n",
      "Epoch 610, Loss: 0.75550177693367, Final Batch Loss: 0.4380600154399872\n",
      "Epoch 611, Loss: 0.7129374444484711, Final Batch Loss: 0.3031160831451416\n",
      "Epoch 612, Loss: 0.7240054905414581, Final Batch Loss: 0.3676750361919403\n",
      "Epoch 613, Loss: 0.7036467790603638, Final Batch Loss: 0.38909319043159485\n",
      "Epoch 614, Loss: 0.64922034740448, Final Batch Loss: 0.2932003438472748\n",
      "Epoch 615, Loss: 0.7234712541103363, Final Batch Loss: 0.4289354979991913\n",
      "Epoch 616, Loss: 0.7210976481437683, Final Batch Loss: 0.328311562538147\n",
      "Epoch 617, Loss: 0.6884101629257202, Final Batch Loss: 0.37177708745002747\n",
      "Epoch 618, Loss: 0.7223931849002838, Final Batch Loss: 0.3507783114910126\n",
      "Epoch 619, Loss: 0.7500694096088409, Final Batch Loss: 0.40346404910087585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 620, Loss: 0.756821870803833, Final Batch Loss: 0.3892184793949127\n",
      "Epoch 621, Loss: 0.7630950510501862, Final Batch Loss: 0.4254116714000702\n",
      "Epoch 622, Loss: 0.6621132493019104, Final Batch Loss: 0.34005650877952576\n",
      "Epoch 623, Loss: 0.6596699059009552, Final Batch Loss: 0.28336456418037415\n",
      "Epoch 624, Loss: 0.7434115409851074, Final Batch Loss: 0.3835606575012207\n",
      "Epoch 625, Loss: 0.6546552777290344, Final Batch Loss: 0.30233675241470337\n",
      "Epoch 626, Loss: 0.683647096157074, Final Batch Loss: 0.39630329608917236\n",
      "Epoch 627, Loss: 0.6711766421794891, Final Batch Loss: 0.306753545999527\n",
      "Epoch 628, Loss: 0.7111932933330536, Final Batch Loss: 0.3860686719417572\n",
      "Epoch 629, Loss: 0.6928992867469788, Final Batch Loss: 0.3383241891860962\n",
      "Epoch 630, Loss: 0.7097508013248444, Final Batch Loss: 0.3415141999721527\n",
      "Epoch 631, Loss: 0.7597264051437378, Final Batch Loss: 0.39479348063468933\n",
      "Epoch 632, Loss: 0.7159443795681, Final Batch Loss: 0.37543606758117676\n",
      "Epoch 633, Loss: 0.7276363968849182, Final Batch Loss: 0.3280225098133087\n",
      "Epoch 634, Loss: 0.7441723048686981, Final Batch Loss: 0.3857632577419281\n",
      "Epoch 635, Loss: 0.8130253553390503, Final Batch Loss: 0.40956923365592957\n",
      "Epoch 636, Loss: 0.7146644294261932, Final Batch Loss: 0.3454976975917816\n",
      "Epoch 637, Loss: 0.6873205602169037, Final Batch Loss: 0.3258189558982849\n",
      "Epoch 638, Loss: 0.6547920405864716, Final Batch Loss: 0.30143409967422485\n",
      "Epoch 639, Loss: 0.7402129769325256, Final Batch Loss: 0.3417496681213379\n",
      "Epoch 640, Loss: 0.6659168303012848, Final Batch Loss: 0.28197652101516724\n",
      "Epoch 641, Loss: 0.7245886325836182, Final Batch Loss: 0.3878500759601593\n",
      "Epoch 642, Loss: 0.6759145259857178, Final Batch Loss: 0.34521403908729553\n",
      "Epoch 643, Loss: 0.7285535931587219, Final Batch Loss: 0.3387877345085144\n",
      "Epoch 644, Loss: 0.6299140751361847, Final Batch Loss: 0.33984363079071045\n",
      "Epoch 645, Loss: 0.6271951794624329, Final Batch Loss: 0.3057641386985779\n",
      "Epoch 646, Loss: 0.6612964868545532, Final Batch Loss: 0.2983686625957489\n",
      "Epoch 647, Loss: 0.6864572465419769, Final Batch Loss: 0.3056565821170807\n",
      "Epoch 648, Loss: 0.699783056974411, Final Batch Loss: 0.3330867886543274\n",
      "Epoch 649, Loss: 0.6022517085075378, Final Batch Loss: 0.24249672889709473\n",
      "Epoch 650, Loss: 0.6381133198738098, Final Batch Loss: 0.2517640292644501\n",
      "Epoch 651, Loss: 0.6483591496944427, Final Batch Loss: 0.30316221714019775\n",
      "Epoch 652, Loss: 0.6871378123760223, Final Batch Loss: 0.3929586708545685\n",
      "Epoch 653, Loss: 0.6604869663715363, Final Batch Loss: 0.3192749619483948\n",
      "Epoch 654, Loss: 0.6500389575958252, Final Batch Loss: 0.3406967222690582\n",
      "Epoch 655, Loss: 0.6969418823719025, Final Batch Loss: 0.3392086625099182\n",
      "Epoch 656, Loss: 0.6674889922142029, Final Batch Loss: 0.3391030430793762\n",
      "Epoch 657, Loss: 0.7176384627819061, Final Batch Loss: 0.3679904341697693\n",
      "Epoch 658, Loss: 0.7171936929225922, Final Batch Loss: 0.40713417530059814\n",
      "Epoch 659, Loss: 0.6484381854534149, Final Batch Loss: 0.33490368723869324\n",
      "Epoch 660, Loss: 0.7066068649291992, Final Batch Loss: 0.33131858706474304\n",
      "Epoch 661, Loss: 0.6213656067848206, Final Batch Loss: 0.3028465211391449\n",
      "Epoch 662, Loss: 0.6861200630664825, Final Batch Loss: 0.3046073317527771\n",
      "Epoch 663, Loss: 0.6394273340702057, Final Batch Loss: 0.2680202126502991\n",
      "Epoch 664, Loss: 0.671061247587204, Final Batch Loss: 0.3673643469810486\n",
      "Epoch 665, Loss: 0.5839964151382446, Final Batch Loss: 0.28037115931510925\n",
      "Epoch 666, Loss: 0.6441397964954376, Final Batch Loss: 0.3493155837059021\n",
      "Epoch 667, Loss: 0.6815804243087769, Final Batch Loss: 0.3749189078807831\n",
      "Epoch 668, Loss: 0.7776113152503967, Final Batch Loss: 0.3698520362377167\n",
      "Epoch 669, Loss: 0.6634153723716736, Final Batch Loss: 0.33337146043777466\n",
      "Epoch 670, Loss: 0.728432297706604, Final Batch Loss: 0.37554821372032166\n",
      "Epoch 671, Loss: 0.6833153963088989, Final Batch Loss: 0.4246131181716919\n",
      "Epoch 672, Loss: 0.6812422871589661, Final Batch Loss: 0.35917583107948303\n",
      "Epoch 673, Loss: 0.6029492020606995, Final Batch Loss: 0.2555828094482422\n",
      "Epoch 674, Loss: 0.6462046205997467, Final Batch Loss: 0.28988146781921387\n",
      "Epoch 675, Loss: 0.6538586616516113, Final Batch Loss: 0.3222004771232605\n",
      "Epoch 676, Loss: 0.6627483069896698, Final Batch Loss: 0.31499072909355164\n",
      "Epoch 677, Loss: 0.7121453881263733, Final Batch Loss: 0.38516080379486084\n",
      "Epoch 678, Loss: 0.72495436668396, Final Batch Loss: 0.3754347860813141\n",
      "Epoch 679, Loss: 0.5973327159881592, Final Batch Loss: 0.24774715304374695\n",
      "Epoch 680, Loss: 0.6709519028663635, Final Batch Loss: 0.3639672100543976\n",
      "Epoch 681, Loss: 0.6499508917331696, Final Batch Loss: 0.29827946424484253\n",
      "Epoch 682, Loss: 0.7214208543300629, Final Batch Loss: 0.36491015553474426\n",
      "Epoch 683, Loss: 0.6314942836761475, Final Batch Loss: 0.3313315808773041\n",
      "Epoch 684, Loss: 0.6515795588493347, Final Batch Loss: 0.3285183906555176\n",
      "Epoch 685, Loss: 0.7197616100311279, Final Batch Loss: 0.3362169861793518\n",
      "Epoch 686, Loss: 0.7006607949733734, Final Batch Loss: 0.3862822353839874\n",
      "Epoch 687, Loss: 0.6115719974040985, Final Batch Loss: 0.2862212657928467\n",
      "Epoch 688, Loss: 0.657230019569397, Final Batch Loss: 0.3247598707675934\n",
      "Epoch 689, Loss: 0.6565320789813995, Final Batch Loss: 0.3708839416503906\n",
      "Epoch 690, Loss: 0.7243805527687073, Final Batch Loss: 0.37764549255371094\n",
      "Epoch 691, Loss: 0.5615184307098389, Final Batch Loss: 0.2596387565135956\n",
      "Epoch 692, Loss: 0.6146615147590637, Final Batch Loss: 0.31128841638565063\n",
      "Epoch 693, Loss: 0.7210813462734222, Final Batch Loss: 0.3767858147621155\n",
      "Epoch 694, Loss: 0.62550950050354, Final Batch Loss: 0.3252527415752411\n",
      "Epoch 695, Loss: 0.6238089799880981, Final Batch Loss: 0.28802937269210815\n",
      "Epoch 696, Loss: 0.6188767552375793, Final Batch Loss: 0.29546767473220825\n",
      "Epoch 697, Loss: 0.6739065647125244, Final Batch Loss: 0.3222355246543884\n",
      "Epoch 698, Loss: 0.6099368929862976, Final Batch Loss: 0.29660114645957947\n",
      "Epoch 699, Loss: 0.6560879945755005, Final Batch Loss: 0.33185166120529175\n",
      "Epoch 700, Loss: 0.5821031630039215, Final Batch Loss: 0.2909819483757019\n",
      "Epoch 701, Loss: 0.6170165836811066, Final Batch Loss: 0.3271220922470093\n",
      "Epoch 702, Loss: 0.6126156151294708, Final Batch Loss: 0.3531690537929535\n",
      "Epoch 703, Loss: 0.6593126654624939, Final Batch Loss: 0.3052425682544708\n",
      "Epoch 704, Loss: 0.6239340901374817, Final Batch Loss: 0.31591594219207764\n",
      "Epoch 705, Loss: 0.6207007765769958, Final Batch Loss: 0.33276447653770447\n",
      "Epoch 706, Loss: 0.6127533614635468, Final Batch Loss: 0.30256935954093933\n",
      "Epoch 707, Loss: 0.6705935895442963, Final Batch Loss: 0.3053957223892212\n",
      "Epoch 708, Loss: 0.6311203837394714, Final Batch Loss: 0.2825540602207184\n",
      "Epoch 709, Loss: 0.6471053659915924, Final Batch Loss: 0.2699090242385864\n",
      "Epoch 710, Loss: 0.660174548625946, Final Batch Loss: 0.3509635031223297\n",
      "Epoch 711, Loss: 0.6881156265735626, Final Batch Loss: 0.35811924934387207\n",
      "Epoch 712, Loss: 0.594833254814148, Final Batch Loss: 0.26215896010398865\n",
      "Epoch 713, Loss: 0.6384901106357574, Final Batch Loss: 0.2949562966823578\n",
      "Epoch 714, Loss: 0.5540872663259506, Final Batch Loss: 0.23479227721691132\n",
      "Epoch 715, Loss: 0.6118633449077606, Final Batch Loss: 0.3099834620952606\n",
      "Epoch 716, Loss: 0.673345148563385, Final Batch Loss: 0.39128923416137695\n",
      "Epoch 717, Loss: 0.6128351837396622, Final Batch Loss: 0.3637029826641083\n",
      "Epoch 718, Loss: 0.7189108729362488, Final Batch Loss: 0.3827165961265564\n",
      "Epoch 719, Loss: 0.5901174396276474, Final Batch Loss: 0.24711079895496368\n",
      "Epoch 720, Loss: 0.6545772552490234, Final Batch Loss: 0.3413095474243164\n",
      "Epoch 721, Loss: 0.6197616159915924, Final Batch Loss: 0.309002161026001\n",
      "Epoch 722, Loss: 0.5418831706047058, Final Batch Loss: 0.2590780258178711\n",
      "Epoch 723, Loss: 0.6227380931377411, Final Batch Loss: 0.297761470079422\n",
      "Epoch 724, Loss: 0.5616601705551147, Final Batch Loss: 0.2695826292037964\n",
      "Epoch 725, Loss: 0.6950760781764984, Final Batch Loss: 0.36210277676582336\n",
      "Epoch 726, Loss: 0.6980950832366943, Final Batch Loss: 0.3800857365131378\n",
      "Epoch 727, Loss: 0.5998245775699615, Final Batch Loss: 0.2975256145000458\n",
      "Epoch 728, Loss: 0.6884800791740417, Final Batch Loss: 0.3619183301925659\n",
      "Epoch 729, Loss: 0.7022453844547272, Final Batch Loss: 0.36041733622550964\n",
      "Epoch 730, Loss: 0.6460603475570679, Final Batch Loss: 0.3174320161342621\n",
      "Epoch 731, Loss: 0.591223418712616, Final Batch Loss: 0.3114922046661377\n",
      "Epoch 732, Loss: 0.6234424114227295, Final Batch Loss: 0.27049946784973145\n",
      "Epoch 733, Loss: 0.6604700982570648, Final Batch Loss: 0.3100331425666809\n",
      "Epoch 734, Loss: 0.6273630857467651, Final Batch Loss: 0.3396708369255066\n",
      "Epoch 735, Loss: 0.624647319316864, Final Batch Loss: 0.3104006350040436\n",
      "Epoch 736, Loss: 0.6600174605846405, Final Batch Loss: 0.3241959512233734\n",
      "Epoch 737, Loss: 0.6321204900741577, Final Batch Loss: 0.3060944974422455\n",
      "Epoch 738, Loss: 0.6070969700813293, Final Batch Loss: 0.28577032685279846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 739, Loss: 0.6163233518600464, Final Batch Loss: 0.2835615575313568\n",
      "Epoch 740, Loss: 0.578627198934555, Final Batch Loss: 0.29312244057655334\n",
      "Epoch 741, Loss: 0.5826691389083862, Final Batch Loss: 0.2797697186470032\n",
      "Epoch 742, Loss: 0.6706496477127075, Final Batch Loss: 0.35038676857948303\n",
      "Epoch 743, Loss: 0.6668567061424255, Final Batch Loss: 0.36291828751564026\n",
      "Epoch 744, Loss: 0.6731998026371002, Final Batch Loss: 0.3119277060031891\n",
      "Epoch 745, Loss: 0.603815346956253, Final Batch Loss: 0.278548926115036\n",
      "Epoch 746, Loss: 0.584513247013092, Final Batch Loss: 0.28885746002197266\n",
      "Epoch 747, Loss: 0.5649120211601257, Final Batch Loss: 0.2555253505706787\n",
      "Epoch 748, Loss: 0.5801630914211273, Final Batch Loss: 0.23881429433822632\n",
      "Epoch 749, Loss: 0.6223383247852325, Final Batch Loss: 0.3249642848968506\n",
      "Epoch 750, Loss: 0.6084298193454742, Final Batch Loss: 0.3023872673511505\n",
      "Epoch 751, Loss: 0.6108256280422211, Final Batch Loss: 0.31558728218078613\n",
      "Epoch 752, Loss: 0.6528894305229187, Final Batch Loss: 0.3706210255622864\n",
      "Epoch 753, Loss: 0.6311776638031006, Final Batch Loss: 0.3244345188140869\n",
      "Epoch 754, Loss: 0.6082592606544495, Final Batch Loss: 0.28705868124961853\n",
      "Epoch 755, Loss: 0.5989666283130646, Final Batch Loss: 0.25344517827033997\n",
      "Epoch 756, Loss: 0.6124738156795502, Final Batch Loss: 0.2898940443992615\n",
      "Epoch 757, Loss: 0.6091589033603668, Final Batch Loss: 0.3497712314128876\n",
      "Epoch 758, Loss: 0.5619992613792419, Final Batch Loss: 0.2701038420200348\n",
      "Epoch 759, Loss: 0.5833296477794647, Final Batch Loss: 0.2783959209918976\n",
      "Epoch 760, Loss: 0.5818687975406647, Final Batch Loss: 0.2645163834095001\n",
      "Epoch 761, Loss: 0.6169193387031555, Final Batch Loss: 0.2674694359302521\n",
      "Epoch 762, Loss: 0.587088942527771, Final Batch Loss: 0.29106903076171875\n",
      "Epoch 763, Loss: 0.635470986366272, Final Batch Loss: 0.2707332372665405\n",
      "Epoch 764, Loss: 0.5825905650854111, Final Batch Loss: 0.34324002265930176\n",
      "Epoch 765, Loss: 0.5398473143577576, Final Batch Loss: 0.27039775252342224\n",
      "Epoch 766, Loss: 0.6291745603084564, Final Batch Loss: 0.3111509680747986\n",
      "Epoch 767, Loss: 0.6171846389770508, Final Batch Loss: 0.2903115153312683\n",
      "Epoch 768, Loss: 0.5329607129096985, Final Batch Loss: 0.2376004159450531\n",
      "Epoch 769, Loss: 0.5850030183792114, Final Batch Loss: 0.27596795558929443\n",
      "Epoch 770, Loss: 0.6653141677379608, Final Batch Loss: 0.3576517403125763\n",
      "Epoch 771, Loss: 0.5673733949661255, Final Batch Loss: 0.2925927937030792\n",
      "Epoch 772, Loss: 0.5966560244560242, Final Batch Loss: 0.2825603783130646\n",
      "Epoch 773, Loss: 0.5991922318935394, Final Batch Loss: 0.2821088135242462\n",
      "Epoch 774, Loss: 0.5779854357242584, Final Batch Loss: 0.3114854097366333\n",
      "Epoch 775, Loss: 0.6066888868808746, Final Batch Loss: 0.35071101784706116\n",
      "Epoch 776, Loss: 0.563490480184555, Final Batch Loss: 0.2871044874191284\n",
      "Epoch 777, Loss: 0.6626430451869965, Final Batch Loss: 0.34658846259117126\n",
      "Epoch 778, Loss: 0.6369184255599976, Final Batch Loss: 0.3136710822582245\n",
      "Epoch 779, Loss: 0.6017366945743561, Final Batch Loss: 0.3262174427509308\n",
      "Epoch 780, Loss: 0.6034881472587585, Final Batch Loss: 0.30822107195854187\n",
      "Epoch 781, Loss: 0.5239080488681793, Final Batch Loss: 0.26336869597435\n",
      "Epoch 782, Loss: 0.5808466374874115, Final Batch Loss: 0.2564148008823395\n",
      "Epoch 783, Loss: 0.6798274517059326, Final Batch Loss: 0.3810584247112274\n",
      "Epoch 784, Loss: 0.5780861377716064, Final Batch Loss: 0.27994057536125183\n",
      "Epoch 785, Loss: 0.5756030678749084, Final Batch Loss: 0.3083367347717285\n",
      "Epoch 786, Loss: 0.530060276389122, Final Batch Loss: 0.24262948334217072\n",
      "Epoch 787, Loss: 0.6008362174034119, Final Batch Loss: 0.2773315906524658\n",
      "Epoch 788, Loss: 0.6077540218830109, Final Batch Loss: 0.3507980406284332\n",
      "Epoch 789, Loss: 0.6635453104972839, Final Batch Loss: 0.33864858746528625\n",
      "Epoch 790, Loss: 0.5659294426441193, Final Batch Loss: 0.26008641719818115\n",
      "Epoch 791, Loss: 0.5852948427200317, Final Batch Loss: 0.32780835032463074\n",
      "Epoch 792, Loss: 0.5300627052783966, Final Batch Loss: 0.18685948848724365\n",
      "Epoch 793, Loss: 0.5959698110818863, Final Batch Loss: 0.24902893602848053\n",
      "Epoch 794, Loss: 0.534973531961441, Final Batch Loss: 0.27389979362487793\n",
      "Epoch 795, Loss: 0.5831238031387329, Final Batch Loss: 0.26419925689697266\n",
      "Epoch 796, Loss: 0.6302732825279236, Final Batch Loss: 0.3299545347690582\n",
      "Epoch 797, Loss: 0.6338459551334381, Final Batch Loss: 0.28061774373054504\n",
      "Epoch 798, Loss: 0.5988340377807617, Final Batch Loss: 0.3215537369251251\n",
      "Epoch 799, Loss: 0.5884380638599396, Final Batch Loss: 0.2888430953025818\n",
      "Epoch 800, Loss: 0.5811803340911865, Final Batch Loss: 0.29188981652259827\n",
      "Epoch 801, Loss: 0.5996986031532288, Final Batch Loss: 0.34332242608070374\n",
      "Epoch 802, Loss: 0.5990711450576782, Final Batch Loss: 0.325080931186676\n",
      "Epoch 803, Loss: 0.5772101879119873, Final Batch Loss: 0.26300638914108276\n",
      "Epoch 804, Loss: 0.5874950289726257, Final Batch Loss: 0.29302501678466797\n",
      "Epoch 805, Loss: 0.6097107529640198, Final Batch Loss: 0.3016784191131592\n",
      "Epoch 806, Loss: 0.7181792557239532, Final Batch Loss: 0.37775149941444397\n",
      "Epoch 807, Loss: 0.5840469300746918, Final Batch Loss: 0.33485493063926697\n",
      "Epoch 808, Loss: 0.5935047268867493, Final Batch Loss: 0.2874673306941986\n",
      "Epoch 809, Loss: 0.538301944732666, Final Batch Loss: 0.2945496737957001\n",
      "Epoch 810, Loss: 0.5158185064792633, Final Batch Loss: 0.21643272042274475\n",
      "Epoch 811, Loss: 0.5540383160114288, Final Batch Loss: 0.26166626811027527\n",
      "Epoch 812, Loss: 0.5510552674531937, Final Batch Loss: 0.23226474225521088\n",
      "Epoch 813, Loss: 0.5874159634113312, Final Batch Loss: 0.2963488698005676\n",
      "Epoch 814, Loss: 0.5692654550075531, Final Batch Loss: 0.28766754269599915\n",
      "Epoch 815, Loss: 0.597115159034729, Final Batch Loss: 0.2988445460796356\n",
      "Epoch 816, Loss: 0.582039088010788, Final Batch Loss: 0.26944079995155334\n",
      "Epoch 817, Loss: 0.533380001783371, Final Batch Loss: 0.2495577335357666\n",
      "Epoch 818, Loss: 0.595504954457283, Final Batch Loss: 0.2408267706632614\n",
      "Epoch 819, Loss: 0.5499533414840698, Final Batch Loss: 0.2493124008178711\n",
      "Epoch 820, Loss: 0.5693106353282928, Final Batch Loss: 0.2579730451107025\n",
      "Epoch 821, Loss: 0.6288141310214996, Final Batch Loss: 0.30207666754722595\n",
      "Epoch 822, Loss: 0.5703943967819214, Final Batch Loss: 0.2978897988796234\n",
      "Epoch 823, Loss: 0.5619049966335297, Final Batch Loss: 0.2766903340816498\n",
      "Epoch 824, Loss: 0.6254705190658569, Final Batch Loss: 0.3252373933792114\n",
      "Epoch 825, Loss: 0.6110653281211853, Final Batch Loss: 0.3178839385509491\n",
      "Epoch 826, Loss: 0.5390432476997375, Final Batch Loss: 0.2679148018360138\n",
      "Epoch 827, Loss: 0.5683652609586716, Final Batch Loss: 0.33424246311187744\n",
      "Epoch 828, Loss: 0.5452397465705872, Final Batch Loss: 0.223337322473526\n",
      "Epoch 829, Loss: 0.6206846833229065, Final Batch Loss: 0.26512932777404785\n",
      "Epoch 830, Loss: 0.5957403481006622, Final Batch Loss: 0.27567818760871887\n",
      "Epoch 831, Loss: 0.5597867965698242, Final Batch Loss: 0.3005627393722534\n",
      "Epoch 832, Loss: 0.5513255745172501, Final Batch Loss: 0.24752293527126312\n",
      "Epoch 833, Loss: 0.5841515958309174, Final Batch Loss: 0.29679450392723083\n",
      "Epoch 834, Loss: 0.5666425228118896, Final Batch Loss: 0.23176798224449158\n",
      "Epoch 835, Loss: 0.5708985328674316, Final Batch Loss: 0.2739790976047516\n",
      "Epoch 836, Loss: 0.5687908381223679, Final Batch Loss: 0.2474408894777298\n",
      "Epoch 837, Loss: 0.5747277438640594, Final Batch Loss: 0.29635828733444214\n",
      "Epoch 838, Loss: 0.528678685426712, Final Batch Loss: 0.2600018084049225\n",
      "Epoch 839, Loss: 0.6074720323085785, Final Batch Loss: 0.303219199180603\n",
      "Epoch 840, Loss: 0.6092144846916199, Final Batch Loss: 0.3430889844894409\n",
      "Epoch 841, Loss: 0.5410565286874771, Final Batch Loss: 0.22832734882831573\n",
      "Epoch 842, Loss: 0.5820586085319519, Final Batch Loss: 0.3024236857891083\n",
      "Epoch 843, Loss: 0.4854193925857544, Final Batch Loss: 0.21177956461906433\n",
      "Epoch 844, Loss: 0.5968642234802246, Final Batch Loss: 0.3064487874507904\n",
      "Epoch 845, Loss: 0.6205072402954102, Final Batch Loss: 0.27725091576576233\n",
      "Epoch 846, Loss: 0.5495473742485046, Final Batch Loss: 0.28629592061042786\n",
      "Epoch 847, Loss: 0.561152994632721, Final Batch Loss: 0.27510160207748413\n",
      "Epoch 848, Loss: 0.5448291599750519, Final Batch Loss: 0.3135056793689728\n",
      "Epoch 849, Loss: 0.5558726489543915, Final Batch Loss: 0.23578378558158875\n",
      "Epoch 850, Loss: 0.5525672435760498, Final Batch Loss: 0.26037925481796265\n",
      "Epoch 851, Loss: 0.5374778807163239, Final Batch Loss: 0.27375462651252747\n",
      "Epoch 852, Loss: 0.5492161512374878, Final Batch Loss: 0.27734068036079407\n",
      "Epoch 853, Loss: 0.583312451839447, Final Batch Loss: 0.329376757144928\n",
      "Epoch 854, Loss: 0.5718255341053009, Final Batch Loss: 0.3068198561668396\n",
      "Epoch 855, Loss: 0.5182186961174011, Final Batch Loss: 0.2517152726650238\n",
      "Epoch 856, Loss: 0.5543501973152161, Final Batch Loss: 0.28381258249282837\n",
      "Epoch 857, Loss: 0.6041893661022186, Final Batch Loss: 0.31963488459587097\n",
      "Epoch 858, Loss: 0.6251929700374603, Final Batch Loss: 0.35805144906044006\n",
      "Epoch 859, Loss: 0.5443857908248901, Final Batch Loss: 0.28780055046081543\n",
      "Epoch 860, Loss: 0.5520615577697754, Final Batch Loss: 0.25912711024284363\n",
      "Epoch 861, Loss: 0.5553364455699921, Final Batch Loss: 0.23773658275604248\n",
      "Epoch 862, Loss: 0.5157851129770279, Final Batch Loss: 0.269758015871048\n",
      "Epoch 863, Loss: 0.6277183592319489, Final Batch Loss: 0.28482523560523987\n",
      "Epoch 864, Loss: 0.5975622832775116, Final Batch Loss: 0.2890635132789612\n",
      "Epoch 865, Loss: 0.49667058885097504, Final Batch Loss: 0.20846359431743622\n",
      "Epoch 866, Loss: 0.563996821641922, Final Batch Loss: 0.2897128760814667\n",
      "Epoch 867, Loss: 0.5644733905792236, Final Batch Loss: 0.25262337923049927\n",
      "Epoch 868, Loss: 0.584019809961319, Final Batch Loss: 0.2883848547935486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 869, Loss: 0.6153967976570129, Final Batch Loss: 0.3196967840194702\n",
      "Epoch 870, Loss: 0.49916498363018036, Final Batch Loss: 0.2429686337709427\n",
      "Epoch 871, Loss: 0.5537358671426773, Final Batch Loss: 0.23896698653697968\n",
      "Epoch 872, Loss: 0.5461598038673401, Final Batch Loss: 0.30761879682540894\n",
      "Epoch 873, Loss: 0.5388583242893219, Final Batch Loss: 0.2618144750595093\n",
      "Epoch 874, Loss: 0.5844161510467529, Final Batch Loss: 0.2961503863334656\n",
      "Epoch 875, Loss: 0.5201964378356934, Final Batch Loss: 0.25297975540161133\n",
      "Epoch 876, Loss: 0.5330382883548737, Final Batch Loss: 0.2818930745124817\n",
      "Epoch 877, Loss: 0.5760189294815063, Final Batch Loss: 0.2705408036708832\n",
      "Epoch 878, Loss: 0.5168469548225403, Final Batch Loss: 0.22911551594734192\n",
      "Epoch 879, Loss: 0.5874143838882446, Final Batch Loss: 0.3131438195705414\n",
      "Epoch 880, Loss: 0.5129126608371735, Final Batch Loss: 0.22173887491226196\n",
      "Epoch 881, Loss: 0.6084227412939072, Final Batch Loss: 0.36826828122138977\n",
      "Epoch 882, Loss: 0.5773291885852814, Final Batch Loss: 0.2362891435623169\n",
      "Epoch 883, Loss: 0.6286490559577942, Final Batch Loss: 0.33181893825531006\n",
      "Epoch 884, Loss: 0.5875225067138672, Final Batch Loss: 0.2852928042411804\n",
      "Epoch 885, Loss: 0.5978241264820099, Final Batch Loss: 0.35744738578796387\n",
      "Epoch 886, Loss: 0.5171874314546585, Final Batch Loss: 0.23743222653865814\n",
      "Epoch 887, Loss: 0.5020323395729065, Final Batch Loss: 0.2648187279701233\n",
      "Epoch 888, Loss: 0.5297742187976837, Final Batch Loss: 0.27128365635871887\n",
      "Epoch 889, Loss: 0.5538670122623444, Final Batch Loss: 0.23972401022911072\n",
      "Epoch 890, Loss: 0.5812927484512329, Final Batch Loss: 0.3333151340484619\n",
      "Epoch 891, Loss: 0.63207146525383, Final Batch Loss: 0.3674912750720978\n",
      "Epoch 892, Loss: 0.6584802567958832, Final Batch Loss: 0.3729296028614044\n",
      "Epoch 893, Loss: 0.5881018340587616, Final Batch Loss: 0.363503098487854\n",
      "Epoch 894, Loss: 0.5070586949586868, Final Batch Loss: 0.2297614961862564\n",
      "Epoch 895, Loss: 0.49675267934799194, Final Batch Loss: 0.22280621528625488\n",
      "Epoch 896, Loss: 0.6162661910057068, Final Batch Loss: 0.3278852701187134\n",
      "Epoch 897, Loss: 0.5363045632839203, Final Batch Loss: 0.31350916624069214\n",
      "Epoch 898, Loss: 0.5012142658233643, Final Batch Loss: 0.2285522222518921\n",
      "Epoch 899, Loss: 0.6087281703948975, Final Batch Loss: 0.3296772837638855\n",
      "Epoch 900, Loss: 0.5388173907995224, Final Batch Loss: 0.3018302321434021\n",
      "Epoch 901, Loss: 0.5971329212188721, Final Batch Loss: 0.2914433777332306\n",
      "Epoch 902, Loss: 0.6328551471233368, Final Batch Loss: 0.35392507910728455\n",
      "Epoch 903, Loss: 0.5648166090250015, Final Batch Loss: 0.31806230545043945\n",
      "Epoch 904, Loss: 0.48721548914909363, Final Batch Loss: 0.2346053123474121\n",
      "Epoch 905, Loss: 0.51601642370224, Final Batch Loss: 0.2541627287864685\n",
      "Epoch 906, Loss: 0.5288269519805908, Final Batch Loss: 0.244081050157547\n",
      "Epoch 907, Loss: 0.5772743225097656, Final Batch Loss: 0.3200661838054657\n",
      "Epoch 908, Loss: 0.5303343832492828, Final Batch Loss: 0.2878932058811188\n",
      "Epoch 909, Loss: 0.5533708333969116, Final Batch Loss: 0.2869687080383301\n",
      "Epoch 910, Loss: 0.570921778678894, Final Batch Loss: 0.305198073387146\n",
      "Epoch 911, Loss: 0.5283848196268082, Final Batch Loss: 0.24488495290279388\n",
      "Epoch 912, Loss: 0.5818410813808441, Final Batch Loss: 0.3231649100780487\n",
      "Epoch 913, Loss: 0.52317775785923, Final Batch Loss: 0.23238645493984222\n",
      "Epoch 914, Loss: 0.5958832800388336, Final Batch Loss: 0.3324739634990692\n",
      "Epoch 915, Loss: 0.5880294144153595, Final Batch Loss: 0.3057936429977417\n",
      "Epoch 916, Loss: 0.5917135924100876, Final Batch Loss: 0.3546898365020752\n",
      "Epoch 917, Loss: 0.519503578543663, Final Batch Loss: 0.22125999629497528\n",
      "Epoch 918, Loss: 0.5650762170553207, Final Batch Loss: 0.3156687021255493\n",
      "Epoch 919, Loss: 0.4906176030635834, Final Batch Loss: 0.21515285968780518\n",
      "Epoch 920, Loss: 0.4992552250623703, Final Batch Loss: 0.2496216744184494\n",
      "Epoch 921, Loss: 0.6102428436279297, Final Batch Loss: 0.3535442352294922\n",
      "Epoch 922, Loss: 0.5261416584253311, Final Batch Loss: 0.29299330711364746\n",
      "Epoch 923, Loss: 0.512592613697052, Final Batch Loss: 0.23483380675315857\n",
      "Epoch 924, Loss: 0.5488044023513794, Final Batch Loss: 0.28598886728286743\n",
      "Epoch 925, Loss: 0.5151226669549942, Final Batch Loss: 0.2856868803501129\n",
      "Epoch 926, Loss: 0.5564729273319244, Final Batch Loss: 0.29013970494270325\n",
      "Epoch 927, Loss: 0.6340858936309814, Final Batch Loss: 0.32598352432250977\n",
      "Epoch 928, Loss: 0.5041155964136124, Final Batch Loss: 0.24085189402103424\n",
      "Epoch 929, Loss: 0.5278171300888062, Final Batch Loss: 0.267703115940094\n",
      "Epoch 930, Loss: 0.5455014556646347, Final Batch Loss: 0.3203636407852173\n",
      "Epoch 931, Loss: 0.5210798382759094, Final Batch Loss: 0.25695040822029114\n",
      "Epoch 932, Loss: 0.5106050968170166, Final Batch Loss: 0.24382683634757996\n",
      "Epoch 933, Loss: 0.5376836359500885, Final Batch Loss: 0.25062236189842224\n",
      "Epoch 934, Loss: 0.5445767343044281, Final Batch Loss: 0.2854905128479004\n",
      "Epoch 935, Loss: 0.4908410608768463, Final Batch Loss: 0.25085869431495667\n",
      "Epoch 936, Loss: 0.5289734303951263, Final Batch Loss: 0.27163657546043396\n",
      "Epoch 937, Loss: 0.5051198154687881, Final Batch Loss: 0.2331639975309372\n",
      "Epoch 938, Loss: 0.5385004878044128, Final Batch Loss: 0.28657057881355286\n",
      "Epoch 939, Loss: 0.5617665648460388, Final Batch Loss: 0.24985283613204956\n",
      "Epoch 940, Loss: 0.5327697396278381, Final Batch Loss: 0.28479015827178955\n",
      "Epoch 941, Loss: 0.510852113366127, Final Batch Loss: 0.2923193573951721\n",
      "Epoch 942, Loss: 0.47968582808971405, Final Batch Loss: 0.2223927229642868\n",
      "Epoch 943, Loss: 0.4941580891609192, Final Batch Loss: 0.21571394801139832\n",
      "Epoch 944, Loss: 0.5564174950122833, Final Batch Loss: 0.32846152782440186\n",
      "Epoch 945, Loss: 0.49393609166145325, Final Batch Loss: 0.21375185251235962\n",
      "Epoch 946, Loss: 0.582648515701294, Final Batch Loss: 0.32774579524993896\n",
      "Epoch 947, Loss: 0.4760429710149765, Final Batch Loss: 0.20910273492336273\n",
      "Epoch 948, Loss: 0.5282387137413025, Final Batch Loss: 0.23892027139663696\n",
      "Epoch 949, Loss: 0.5033455193042755, Final Batch Loss: 0.25125977396965027\n",
      "Epoch 950, Loss: 0.5020705014467239, Final Batch Loss: 0.2789711654186249\n",
      "Epoch 951, Loss: 0.5642890930175781, Final Batch Loss: 0.269288569688797\n",
      "Epoch 952, Loss: 0.4802142381668091, Final Batch Loss: 0.19898509979248047\n",
      "Epoch 953, Loss: 0.5137300044298172, Final Batch Loss: 0.23484666645526886\n",
      "Epoch 954, Loss: 0.437267541885376, Final Batch Loss: 0.1769411861896515\n",
      "Epoch 955, Loss: 0.5916725993156433, Final Batch Loss: 0.3103642761707306\n",
      "Epoch 956, Loss: 0.4826577305793762, Final Batch Loss: 0.22771570086479187\n",
      "Epoch 957, Loss: 0.49424047768116, Final Batch Loss: 0.208748921751976\n",
      "Epoch 958, Loss: 0.5187568366527557, Final Batch Loss: 0.23718327283859253\n",
      "Epoch 959, Loss: 0.5375941246747971, Final Batch Loss: 0.29205721616744995\n",
      "Epoch 960, Loss: 0.5226214677095413, Final Batch Loss: 0.24692125618457794\n",
      "Epoch 961, Loss: 0.508619412779808, Final Batch Loss: 0.24998097121715546\n",
      "Epoch 962, Loss: 0.4774989038705826, Final Batch Loss: 0.23777592182159424\n",
      "Epoch 963, Loss: 0.5062836706638336, Final Batch Loss: 0.2589516043663025\n",
      "Epoch 964, Loss: 0.5427186489105225, Final Batch Loss: 0.269486665725708\n",
      "Epoch 965, Loss: 0.5827772617340088, Final Batch Loss: 0.2964620888233185\n",
      "Epoch 966, Loss: 0.5950585007667542, Final Batch Loss: 0.3188958168029785\n",
      "Epoch 967, Loss: 0.46630924940109253, Final Batch Loss: 0.2091328203678131\n",
      "Epoch 968, Loss: 0.5408954620361328, Final Batch Loss: 0.2754044234752655\n",
      "Epoch 969, Loss: 0.4751771092414856, Final Batch Loss: 0.21290934085845947\n",
      "Epoch 970, Loss: 0.5503972619771957, Final Batch Loss: 0.3025663197040558\n",
      "Epoch 971, Loss: 0.4964233934879303, Final Batch Loss: 0.26674655079841614\n",
      "Epoch 972, Loss: 0.5601109564304352, Final Batch Loss: 0.28050485253334045\n",
      "Epoch 973, Loss: 0.5257844626903534, Final Batch Loss: 0.2700870633125305\n",
      "Epoch 974, Loss: 0.5537820756435394, Final Batch Loss: 0.3118863105773926\n",
      "Epoch 975, Loss: 0.5598810315132141, Final Batch Loss: 0.30302131175994873\n",
      "Epoch 976, Loss: 0.5407248437404633, Final Batch Loss: 0.2603299021720886\n",
      "Epoch 977, Loss: 0.5001871883869171, Final Batch Loss: 0.21029457449913025\n",
      "Epoch 978, Loss: 0.570843368768692, Final Batch Loss: 0.31169068813323975\n",
      "Epoch 979, Loss: 0.49268123507499695, Final Batch Loss: 0.26197564601898193\n",
      "Epoch 980, Loss: 0.5289303362369537, Final Batch Loss: 0.25139251351356506\n",
      "Epoch 981, Loss: 0.4866097718477249, Final Batch Loss: 0.22821561992168427\n",
      "Epoch 982, Loss: 0.5229280591011047, Final Batch Loss: 0.2558101415634155\n",
      "Epoch 983, Loss: 0.4861617982387543, Final Batch Loss: 0.2360466718673706\n",
      "Epoch 984, Loss: 0.48083189129829407, Final Batch Loss: 0.20865455269813538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 985, Loss: 0.5688741207122803, Final Batch Loss: 0.2809038460254669\n",
      "Epoch 986, Loss: 0.48236311972141266, Final Batch Loss: 0.21275736391544342\n",
      "Epoch 987, Loss: 0.47858448326587677, Final Batch Loss: 0.22587214410305023\n",
      "Epoch 988, Loss: 0.4811303913593292, Final Batch Loss: 0.2164570391178131\n",
      "Epoch 989, Loss: 0.5114339888095856, Final Batch Loss: 0.22218620777130127\n",
      "Epoch 990, Loss: 0.4978720545768738, Final Batch Loss: 0.26570579409599304\n",
      "Epoch 991, Loss: 0.5089499652385712, Final Batch Loss: 0.22776994109153748\n",
      "Epoch 992, Loss: 0.5389178991317749, Final Batch Loss: 0.2869088351726532\n",
      "Epoch 993, Loss: 0.5509200394153595, Final Batch Loss: 0.3063119351863861\n",
      "Epoch 994, Loss: 0.5069925636053085, Final Batch Loss: 0.24022303521633148\n",
      "Epoch 995, Loss: 0.4780499190092087, Final Batch Loss: 0.2515200972557068\n",
      "Epoch 996, Loss: 0.5223714858293533, Final Batch Loss: 0.2753411829471588\n",
      "Epoch 997, Loss: 0.48554790019989014, Final Batch Loss: 0.2413291484117508\n",
      "Epoch 998, Loss: 0.5305875986814499, Final Batch Loss: 0.24744127690792084\n",
      "Epoch 999, Loss: 0.4662424474954605, Final Batch Loss: 0.24830511212348938\n",
      "Epoch 1000, Loss: 0.507062092423439, Final Batch Loss: 0.22387145459651947\n",
      "Epoch 1001, Loss: 0.4988265633583069, Final Batch Loss: 0.2647627890110016\n",
      "Epoch 1002, Loss: 0.5090441554784775, Final Batch Loss: 0.24171452224254608\n",
      "Epoch 1003, Loss: 0.509012371301651, Final Batch Loss: 0.2540714740753174\n",
      "Epoch 1004, Loss: 0.5572785288095474, Final Batch Loss: 0.3085450828075409\n",
      "Epoch 1005, Loss: 0.52186518907547, Final Batch Loss: 0.28185731172561646\n",
      "Epoch 1006, Loss: 0.5037934929132462, Final Batch Loss: 0.24886952340602875\n",
      "Epoch 1007, Loss: 0.5615008771419525, Final Batch Loss: 0.29969218373298645\n",
      "Epoch 1008, Loss: 0.49353229999542236, Final Batch Loss: 0.19290664792060852\n",
      "Epoch 1009, Loss: 0.4644002765417099, Final Batch Loss: 0.23504285514354706\n",
      "Epoch 1010, Loss: 0.6787537038326263, Final Batch Loss: 0.4024738371372223\n",
      "Epoch 1011, Loss: 0.49928829073905945, Final Batch Loss: 0.21118411421775818\n",
      "Epoch 1012, Loss: 0.4868759512901306, Final Batch Loss: 0.23684552311897278\n",
      "Epoch 1013, Loss: 0.5009259581565857, Final Batch Loss: 0.21329250931739807\n",
      "Epoch 1014, Loss: 0.5651790797710419, Final Batch Loss: 0.2767387330532074\n",
      "Epoch 1015, Loss: 0.4804026335477829, Final Batch Loss: 0.2166767567396164\n",
      "Epoch 1016, Loss: 0.5011560618877411, Final Batch Loss: 0.22553330659866333\n",
      "Epoch 1017, Loss: 0.5698059797286987, Final Batch Loss: 0.31081315875053406\n",
      "Epoch 1018, Loss: 0.4927838295698166, Final Batch Loss: 0.26527509093284607\n",
      "Epoch 1019, Loss: 0.47213196754455566, Final Batch Loss: 0.2308276742696762\n",
      "Epoch 1020, Loss: 0.4855191111564636, Final Batch Loss: 0.2408410906791687\n",
      "Epoch 1021, Loss: 0.46758316457271576, Final Batch Loss: 0.24156664311885834\n",
      "Epoch 1022, Loss: 0.5347281098365784, Final Batch Loss: 0.2742491364479065\n",
      "Epoch 1023, Loss: 0.5742557048797607, Final Batch Loss: 0.28408175706863403\n",
      "Epoch 1024, Loss: 0.5310148149728775, Final Batch Loss: 0.3023732006549835\n",
      "Epoch 1025, Loss: 0.49263423681259155, Final Batch Loss: 0.23324790596961975\n",
      "Epoch 1026, Loss: 0.4846108853816986, Final Batch Loss: 0.26594898104667664\n",
      "Epoch 1027, Loss: 0.5359172821044922, Final Batch Loss: 0.2431228756904602\n",
      "Epoch 1028, Loss: 0.490189790725708, Final Batch Loss: 0.21405667066574097\n",
      "Epoch 1029, Loss: 0.5418328046798706, Final Batch Loss: 0.26605528593063354\n",
      "Epoch 1030, Loss: 0.5092281699180603, Final Batch Loss: 0.23042958974838257\n",
      "Epoch 1031, Loss: 0.5198160707950592, Final Batch Loss: 0.2798064947128296\n",
      "Epoch 1032, Loss: 0.44481588900089264, Final Batch Loss: 0.17232216894626617\n",
      "Epoch 1033, Loss: 0.49455223977565765, Final Batch Loss: 0.2572694718837738\n",
      "Epoch 1034, Loss: 0.47616836428642273, Final Batch Loss: 0.25718289613723755\n",
      "Epoch 1035, Loss: 0.5016267448663712, Final Batch Loss: 0.24036242067813873\n",
      "Epoch 1036, Loss: 0.5261689722537994, Final Batch Loss: 0.32447168231010437\n",
      "Epoch 1037, Loss: 0.5319619327783585, Final Batch Loss: 0.3096396028995514\n",
      "Epoch 1038, Loss: 0.5397883355617523, Final Batch Loss: 0.2835915684700012\n",
      "Epoch 1039, Loss: 0.484926775097847, Final Batch Loss: 0.23610281944274902\n",
      "Epoch 1040, Loss: 0.4229992777109146, Final Batch Loss: 0.1878558248281479\n",
      "Epoch 1041, Loss: 0.5317924916744232, Final Batch Loss: 0.24963343143463135\n",
      "Epoch 1042, Loss: 0.5530355274677277, Final Batch Loss: 0.32097327709198\n",
      "Epoch 1043, Loss: 0.48738059401512146, Final Batch Loss: 0.2355826497077942\n",
      "Epoch 1044, Loss: 0.5321188867092133, Final Batch Loss: 0.26441311836242676\n",
      "Epoch 1045, Loss: 0.5332435071468353, Final Batch Loss: 0.21529126167297363\n",
      "Epoch 1046, Loss: 0.4713710844516754, Final Batch Loss: 0.23600751161575317\n",
      "Epoch 1047, Loss: 0.4389886111021042, Final Batch Loss: 0.2055392861366272\n",
      "Epoch 1048, Loss: 0.5305997133255005, Final Batch Loss: 0.2855559289455414\n",
      "Epoch 1049, Loss: 0.4774249345064163, Final Batch Loss: 0.23116359114646912\n",
      "Epoch 1050, Loss: 0.5739679038524628, Final Batch Loss: 0.29537659883499146\n",
      "Epoch 1051, Loss: 0.5025149285793304, Final Batch Loss: 0.19224637746810913\n",
      "Epoch 1052, Loss: 0.5130044221878052, Final Batch Loss: 0.26457545161247253\n",
      "Epoch 1053, Loss: 0.5553514659404755, Final Batch Loss: 0.2837229073047638\n",
      "Epoch 1054, Loss: 0.49776460230350494, Final Batch Loss: 0.2672586441040039\n",
      "Epoch 1055, Loss: 0.5420952439308167, Final Batch Loss: 0.27479544281959534\n",
      "Epoch 1056, Loss: 0.5044493079185486, Final Batch Loss: 0.23343661427497864\n",
      "Epoch 1057, Loss: 0.5118406862020493, Final Batch Loss: 0.2321125715970993\n",
      "Epoch 1058, Loss: 0.4789120852947235, Final Batch Loss: 0.23521125316619873\n",
      "Epoch 1059, Loss: 0.45561012625694275, Final Batch Loss: 0.2188653200864792\n",
      "Epoch 1060, Loss: 0.5039378702640533, Final Batch Loss: 0.22589951753616333\n",
      "Epoch 1061, Loss: 0.4919552057981491, Final Batch Loss: 0.22234781086444855\n",
      "Epoch 1062, Loss: 0.53067946434021, Final Batch Loss: 0.2767239212989807\n",
      "Epoch 1063, Loss: 0.49254974722862244, Final Batch Loss: 0.23234379291534424\n",
      "Epoch 1064, Loss: 0.43280860781669617, Final Batch Loss: 0.19703309237957\n",
      "Epoch 1065, Loss: 0.5678103566169739, Final Batch Loss: 0.30592259764671326\n",
      "Epoch 1066, Loss: 0.5792849063873291, Final Batch Loss: 0.2642771005630493\n",
      "Epoch 1067, Loss: 0.578286200761795, Final Batch Loss: 0.31984251737594604\n",
      "Epoch 1068, Loss: 0.528448149561882, Final Batch Loss: 0.24538876116275787\n",
      "Epoch 1069, Loss: 0.43013185262680054, Final Batch Loss: 0.15968719124794006\n",
      "Epoch 1070, Loss: 0.49320146441459656, Final Batch Loss: 0.2544857859611511\n",
      "Epoch 1071, Loss: 0.5128405541181564, Final Batch Loss: 0.2889707088470459\n",
      "Epoch 1072, Loss: 0.4508030563592911, Final Batch Loss: 0.20376943051815033\n",
      "Epoch 1073, Loss: 0.4840739816427231, Final Batch Loss: 0.23914393782615662\n",
      "Epoch 1074, Loss: 0.5621257126331329, Final Batch Loss: 0.2933926284313202\n",
      "Epoch 1075, Loss: 0.4730442464351654, Final Batch Loss: 0.2505253255367279\n",
      "Epoch 1076, Loss: 0.51573146879673, Final Batch Loss: 0.23836873471736908\n",
      "Epoch 1077, Loss: 0.49529531598091125, Final Batch Loss: 0.24347811937332153\n",
      "Epoch 1078, Loss: 0.48443277180194855, Final Batch Loss: 0.22752033174037933\n",
      "Epoch 1079, Loss: 0.5341812074184418, Final Batch Loss: 0.31845763325691223\n",
      "Epoch 1080, Loss: 0.49354007840156555, Final Batch Loss: 0.2036500871181488\n",
      "Epoch 1081, Loss: 0.4800715297460556, Final Batch Loss: 0.2558046579360962\n",
      "Epoch 1082, Loss: 0.4966633468866348, Final Batch Loss: 0.21813873946666718\n",
      "Epoch 1083, Loss: 0.5480575561523438, Final Batch Loss: 0.25611811876296997\n",
      "Epoch 1084, Loss: 0.5422336757183075, Final Batch Loss: 0.3162122070789337\n",
      "Epoch 1085, Loss: 0.46370774507522583, Final Batch Loss: 0.21951958537101746\n",
      "Epoch 1086, Loss: 0.5276241898536682, Final Batch Loss: 0.27161073684692383\n",
      "Epoch 1087, Loss: 0.44391313195228577, Final Batch Loss: 0.22726967930793762\n",
      "Epoch 1088, Loss: 0.45639568567276, Final Batch Loss: 0.22827135026454926\n",
      "Epoch 1089, Loss: 0.5682401061058044, Final Batch Loss: 0.3151344656944275\n",
      "Epoch 1090, Loss: 0.4703713655471802, Final Batch Loss: 0.2537100613117218\n",
      "Epoch 1091, Loss: 0.471477746963501, Final Batch Loss: 0.22855713963508606\n",
      "Epoch 1092, Loss: 0.4709010422229767, Final Batch Loss: 0.22727343440055847\n",
      "Epoch 1093, Loss: 0.4968854784965515, Final Batch Loss: 0.23898836970329285\n",
      "Epoch 1094, Loss: 0.4788767397403717, Final Batch Loss: 0.2556539475917816\n",
      "Epoch 1095, Loss: 0.4873027801513672, Final Batch Loss: 0.2687212824821472\n",
      "Epoch 1096, Loss: 0.47751733660697937, Final Batch Loss: 0.22142812609672546\n",
      "Epoch 1097, Loss: 0.5912131369113922, Final Batch Loss: 0.28970760107040405\n",
      "Epoch 1098, Loss: 0.5143111646175385, Final Batch Loss: 0.2617324888706207\n",
      "Epoch 1099, Loss: 0.49803072214126587, Final Batch Loss: 0.2444995641708374\n",
      "Epoch 1100, Loss: 0.4708462208509445, Final Batch Loss: 0.23516981303691864\n",
      "Epoch 1101, Loss: 0.49974609911441803, Final Batch Loss: 0.2742558717727661\n",
      "Epoch 1102, Loss: 0.5092789679765701, Final Batch Loss: 0.23774130642414093\n",
      "Epoch 1103, Loss: 0.4924459308385849, Final Batch Loss: 0.2792930603027344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1104, Loss: 0.5457854270935059, Final Batch Loss: 0.28825199604034424\n",
      "Epoch 1105, Loss: 0.48526112735271454, Final Batch Loss: 0.251470148563385\n",
      "Epoch 1106, Loss: 0.46417757868766785, Final Batch Loss: 0.27057427167892456\n",
      "Epoch 1107, Loss: 0.543866902589798, Final Batch Loss: 0.2645227015018463\n",
      "Epoch 1108, Loss: 0.4986398071050644, Final Batch Loss: 0.22113187611103058\n",
      "Epoch 1109, Loss: 0.4749018996953964, Final Batch Loss: 0.24408385157585144\n",
      "Epoch 1110, Loss: 0.5127160847187042, Final Batch Loss: 0.2516275942325592\n",
      "Epoch 1111, Loss: 0.46984899044036865, Final Batch Loss: 0.23045772314071655\n",
      "Epoch 1112, Loss: 0.5153857916593552, Final Batch Loss: 0.2213905304670334\n",
      "Epoch 1113, Loss: 0.45533187687397003, Final Batch Loss: 0.21531565487384796\n",
      "Epoch 1114, Loss: 0.4532887786626816, Final Batch Loss: 0.1857251077890396\n",
      "Epoch 1115, Loss: 0.5876213610172272, Final Batch Loss: 0.30833467841148376\n",
      "Epoch 1116, Loss: 0.5091246515512466, Final Batch Loss: 0.2616766095161438\n",
      "Epoch 1117, Loss: 0.49206770956516266, Final Batch Loss: 0.2087423950433731\n",
      "Epoch 1118, Loss: 0.5325303971767426, Final Batch Loss: 0.2729320228099823\n",
      "Epoch 1119, Loss: 0.4847749173641205, Final Batch Loss: 0.22696101665496826\n",
      "Epoch 1120, Loss: 0.4921001046895981, Final Batch Loss: 0.2742806673049927\n",
      "Epoch 1121, Loss: 0.5204944163560867, Final Batch Loss: 0.24482481181621552\n",
      "Epoch 1122, Loss: 0.5021452009677887, Final Batch Loss: 0.24971327185630798\n",
      "Epoch 1123, Loss: 0.46310727298259735, Final Batch Loss: 0.2107539027929306\n",
      "Epoch 1124, Loss: 0.5108887553215027, Final Batch Loss: 0.2560076415538788\n",
      "Epoch 1125, Loss: 0.5607695579528809, Final Batch Loss: 0.2298007607460022\n",
      "Epoch 1126, Loss: 0.5451032519340515, Final Batch Loss: 0.3326605558395386\n",
      "Epoch 1127, Loss: 0.4981507360935211, Final Batch Loss: 0.2506141662597656\n",
      "Epoch 1128, Loss: 0.5214008688926697, Final Batch Loss: 0.21334517002105713\n",
      "Epoch 1129, Loss: 0.49369288980960846, Final Batch Loss: 0.2956978380680084\n",
      "Epoch 1130, Loss: 0.5123149007558823, Final Batch Loss: 0.2741469144821167\n",
      "Epoch 1131, Loss: 0.4973927438259125, Final Batch Loss: 0.21788406372070312\n",
      "Epoch 1132, Loss: 0.46265459060668945, Final Batch Loss: 0.239683136343956\n",
      "Epoch 1133, Loss: 0.5268323123455048, Final Batch Loss: 0.2604304850101471\n",
      "Epoch 1134, Loss: 0.49030186235904694, Final Batch Loss: 0.23681049048900604\n",
      "Epoch 1135, Loss: 0.4913479685783386, Final Batch Loss: 0.246454119682312\n",
      "Epoch 1136, Loss: 0.4963444173336029, Final Batch Loss: 0.2381567358970642\n",
      "Epoch 1137, Loss: 0.5001803487539291, Final Batch Loss: 0.24558068811893463\n",
      "Epoch 1138, Loss: 0.4244309961795807, Final Batch Loss: 0.18030580878257751\n",
      "Epoch 1139, Loss: 0.46261221170425415, Final Batch Loss: 0.21976271271705627\n",
      "Epoch 1140, Loss: 0.4661657065153122, Final Batch Loss: 0.22371110320091248\n",
      "Epoch 1141, Loss: 0.47213125228881836, Final Batch Loss: 0.26072391867637634\n",
      "Epoch 1142, Loss: 0.4880130738019943, Final Batch Loss: 0.2293698638677597\n",
      "Epoch 1143, Loss: 0.5052651762962341, Final Batch Loss: 0.23013189435005188\n",
      "Epoch 1144, Loss: 0.4779001921415329, Final Batch Loss: 0.2605758011341095\n",
      "Epoch 1145, Loss: 0.440242663025856, Final Batch Loss: 0.18186433613300323\n",
      "Epoch 1146, Loss: 0.43463172018527985, Final Batch Loss: 0.21284031867980957\n",
      "Epoch 1147, Loss: 0.5496624708175659, Final Batch Loss: 0.27441927790641785\n",
      "Epoch 1148, Loss: 0.45445650815963745, Final Batch Loss: 0.2048685997724533\n",
      "Epoch 1149, Loss: 0.49184438586235046, Final Batch Loss: 0.21895238757133484\n",
      "Epoch 1150, Loss: 0.5071111917495728, Final Batch Loss: 0.25103795528411865\n",
      "Epoch 1151, Loss: 0.4604523479938507, Final Batch Loss: 0.23689115047454834\n",
      "Epoch 1152, Loss: 0.4680701345205307, Final Batch Loss: 0.253379225730896\n",
      "Epoch 1153, Loss: 0.48142755031585693, Final Batch Loss: 0.2515828013420105\n",
      "Epoch 1154, Loss: 0.4572564959526062, Final Batch Loss: 0.2196301519870758\n",
      "Epoch 1155, Loss: 0.48055964708328247, Final Batch Loss: 0.2590831518173218\n",
      "Epoch 1156, Loss: 0.4355877935886383, Final Batch Loss: 0.21106797456741333\n",
      "Epoch 1157, Loss: 0.49544359743595123, Final Batch Loss: 0.2311745434999466\n",
      "Epoch 1158, Loss: 0.508913204073906, Final Batch Loss: 0.2590301036834717\n",
      "Epoch 1159, Loss: 0.4560348689556122, Final Batch Loss: 0.1899728775024414\n",
      "Epoch 1160, Loss: 0.4857405573129654, Final Batch Loss: 0.2238726168870926\n",
      "Epoch 1161, Loss: 0.5382274687290192, Final Batch Loss: 0.28895100951194763\n",
      "Epoch 1162, Loss: 0.5000638365745544, Final Batch Loss: 0.23752278089523315\n",
      "Epoch 1163, Loss: 0.4350462853908539, Final Batch Loss: 0.203134685754776\n",
      "Epoch 1164, Loss: 0.4527726471424103, Final Batch Loss: 0.23815560340881348\n",
      "Epoch 1165, Loss: 0.43631425499916077, Final Batch Loss: 0.19309115409851074\n",
      "Epoch 1166, Loss: 0.48224543035030365, Final Batch Loss: 0.232784241437912\n",
      "Epoch 1167, Loss: 0.5571296513080597, Final Batch Loss: 0.30282166600227356\n",
      "Epoch 1168, Loss: 0.5218046009540558, Final Batch Loss: 0.2711392939090729\n",
      "Epoch 1169, Loss: 0.4755188524723053, Final Batch Loss: 0.23751002550125122\n",
      "Epoch 1170, Loss: 0.5238887369632721, Final Batch Loss: 0.3278809189796448\n",
      "Epoch 1171, Loss: 0.5204408913850784, Final Batch Loss: 0.30284565687179565\n",
      "Epoch 1172, Loss: 0.4971475303173065, Final Batch Loss: 0.28234827518463135\n",
      "Epoch 1173, Loss: 0.4464562386274338, Final Batch Loss: 0.2279960662126541\n",
      "Epoch 1174, Loss: 0.4761921912431717, Final Batch Loss: 0.24684303998947144\n",
      "Epoch 1175, Loss: 0.49784936010837555, Final Batch Loss: 0.26965293288230896\n",
      "Epoch 1176, Loss: 0.45339371263980865, Final Batch Loss: 0.2108614444732666\n",
      "Epoch 1177, Loss: 0.48310133814811707, Final Batch Loss: 0.2659800946712494\n",
      "Epoch 1178, Loss: 0.4727216064929962, Final Batch Loss: 0.29141971468925476\n",
      "Epoch 1179, Loss: 0.49467095732688904, Final Batch Loss: 0.2653930187225342\n",
      "Epoch 1180, Loss: 0.48362843692302704, Final Batch Loss: 0.25149136781692505\n",
      "Epoch 1181, Loss: 0.4643665552139282, Final Batch Loss: 0.21530663967132568\n",
      "Epoch 1182, Loss: 0.5399596095085144, Final Batch Loss: 0.2867055833339691\n",
      "Epoch 1183, Loss: 0.4643934667110443, Final Batch Loss: 0.25169429183006287\n",
      "Epoch 1184, Loss: 0.4904409497976303, Final Batch Loss: 0.22946082055568695\n",
      "Epoch 1185, Loss: 0.4956985265016556, Final Batch Loss: 0.2348182052373886\n",
      "Epoch 1186, Loss: 0.4552741199731827, Final Batch Loss: 0.23090170323848724\n",
      "Epoch 1187, Loss: 0.4536176919937134, Final Batch Loss: 0.20956571400165558\n",
      "Epoch 1188, Loss: 0.5047722309827805, Final Batch Loss: 0.24158750474452972\n",
      "Epoch 1189, Loss: 0.5000587105751038, Final Batch Loss: 0.2439371943473816\n",
      "Epoch 1190, Loss: 0.5472019612789154, Final Batch Loss: 0.2474927306175232\n",
      "Epoch 1191, Loss: 0.4313509166240692, Final Batch Loss: 0.23598428070545197\n",
      "Epoch 1192, Loss: 0.4689429700374603, Final Batch Loss: 0.21314316987991333\n",
      "Epoch 1193, Loss: 0.4828263819217682, Final Batch Loss: 0.29508063197135925\n",
      "Epoch 1194, Loss: 0.5270561575889587, Final Batch Loss: 0.30396685004234314\n",
      "Epoch 1195, Loss: 0.5535626113414764, Final Batch Loss: 0.2813713252544403\n",
      "Epoch 1196, Loss: 0.45720455050468445, Final Batch Loss: 0.19211140275001526\n",
      "Epoch 1197, Loss: 0.4707542210817337, Final Batch Loss: 0.23375548422336578\n",
      "Epoch 1198, Loss: 0.5724782645702362, Final Batch Loss: 0.30554303526878357\n",
      "Epoch 1199, Loss: 0.47795553505420685, Final Batch Loss: 0.2637050449848175\n",
      "Epoch 1200, Loss: 0.4976996034383774, Final Batch Loss: 0.24962899088859558\n",
      "Epoch 1201, Loss: 0.43042680621147156, Final Batch Loss: 0.21117782592773438\n",
      "Epoch 1202, Loss: 0.5272045433521271, Final Batch Loss: 0.2731376588344574\n",
      "Epoch 1203, Loss: 0.4745391458272934, Final Batch Loss: 0.2570992410182953\n",
      "Epoch 1204, Loss: 0.5102475434541702, Final Batch Loss: 0.2737613618373871\n",
      "Epoch 1205, Loss: 0.5365845412015915, Final Batch Loss: 0.29825589060783386\n",
      "Epoch 1206, Loss: 0.48164546489715576, Final Batch Loss: 0.2524428367614746\n",
      "Epoch 1207, Loss: 0.44609588384628296, Final Batch Loss: 0.2438248097896576\n",
      "Epoch 1208, Loss: 0.47713957726955414, Final Batch Loss: 0.22756843268871307\n",
      "Epoch 1209, Loss: 0.5283073335886002, Final Batch Loss: 0.2003072053194046\n",
      "Epoch 1210, Loss: 0.40669508278369904, Final Batch Loss: 0.19246487319469452\n",
      "Epoch 1211, Loss: 0.4626040458679199, Final Batch Loss: 0.21026301383972168\n",
      "Epoch 1212, Loss: 0.4212646335363388, Final Batch Loss: 0.1762610524892807\n",
      "Epoch 1213, Loss: 0.4730547070503235, Final Batch Loss: 0.21455281972885132\n",
      "Epoch 1214, Loss: 0.4680401682853699, Final Batch Loss: 0.23599383234977722\n",
      "Epoch 1215, Loss: 0.4768691211938858, Final Batch Loss: 0.25833189487457275\n",
      "Epoch 1216, Loss: 0.5301437377929688, Final Batch Loss: 0.3213217854499817\n",
      "Epoch 1217, Loss: 0.5220755040645599, Final Batch Loss: 0.3129276633262634\n",
      "Epoch 1218, Loss: 0.467254176735878, Final Batch Loss: 0.22415786981582642\n",
      "Epoch 1219, Loss: 0.620878279209137, Final Batch Loss: 0.38952144980430603\n",
      "Epoch 1220, Loss: 0.4309510141611099, Final Batch Loss: 0.22319622337818146\n",
      "Epoch 1221, Loss: 0.4448201507329941, Final Batch Loss: 0.2062302678823471\n",
      "Epoch 1222, Loss: 0.4583442807197571, Final Batch Loss: 0.21740110218524933\n",
      "Epoch 1223, Loss: 0.4750743955373764, Final Batch Loss: 0.2727260887622833\n",
      "Epoch 1224, Loss: 0.5107172578573227, Final Batch Loss: 0.27496325969696045\n",
      "Epoch 1225, Loss: 0.4725933372974396, Final Batch Loss: 0.2118140459060669\n",
      "Epoch 1226, Loss: 0.47859983146190643, Final Batch Loss: 0.2757253050804138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1227, Loss: 0.515017569065094, Final Batch Loss: 0.2963407635688782\n",
      "Epoch 1228, Loss: 0.4706849008798599, Final Batch Loss: 0.2278835028409958\n",
      "Epoch 1229, Loss: 0.47936423122882843, Final Batch Loss: 0.22035808861255646\n",
      "Epoch 1230, Loss: 0.42845477163791656, Final Batch Loss: 0.21556560695171356\n",
      "Epoch 1231, Loss: 0.45714929699897766, Final Batch Loss: 0.22370944917201996\n",
      "Epoch 1232, Loss: 0.41360659897327423, Final Batch Loss: 0.16782653331756592\n",
      "Epoch 1233, Loss: 0.485301673412323, Final Batch Loss: 0.2995114028453827\n",
      "Epoch 1234, Loss: 0.46731410920619965, Final Batch Loss: 0.23985888063907623\n",
      "Epoch 1235, Loss: 0.41028299927711487, Final Batch Loss: 0.16729311645030975\n",
      "Epoch 1236, Loss: 0.4599601477384567, Final Batch Loss: 0.2168179750442505\n",
      "Epoch 1237, Loss: 0.4851405918598175, Final Batch Loss: 0.24342602491378784\n",
      "Epoch 1238, Loss: 0.5066523998975754, Final Batch Loss: 0.26897132396698\n",
      "Epoch 1239, Loss: 0.5019274055957794, Final Batch Loss: 0.21213877201080322\n",
      "Epoch 1240, Loss: 0.47877269983291626, Final Batch Loss: 0.21868130564689636\n",
      "Epoch 1241, Loss: 0.48610128462314606, Final Batch Loss: 0.26582854986190796\n",
      "Epoch 1242, Loss: 0.45560862123966217, Final Batch Loss: 0.2060849517583847\n",
      "Epoch 1243, Loss: 0.45296043157577515, Final Batch Loss: 0.2213307023048401\n",
      "Epoch 1244, Loss: 0.4967517852783203, Final Batch Loss: 0.23980236053466797\n",
      "Epoch 1245, Loss: 0.43692056834697723, Final Batch Loss: 0.22942334413528442\n",
      "Epoch 1246, Loss: 0.470608726143837, Final Batch Loss: 0.1905430108308792\n",
      "Epoch 1247, Loss: 0.42757995426654816, Final Batch Loss: 0.21230071783065796\n",
      "Epoch 1248, Loss: 0.47757889330387115, Final Batch Loss: 0.20295850932598114\n",
      "Epoch 1249, Loss: 0.4164290279150009, Final Batch Loss: 0.2147226333618164\n",
      "Epoch 1250, Loss: 0.5234302282333374, Final Batch Loss: 0.31950557231903076\n",
      "Epoch 1251, Loss: 0.4301220774650574, Final Batch Loss: 0.24518781900405884\n",
      "Epoch 1252, Loss: 0.45962610840797424, Final Batch Loss: 0.2707120478153229\n",
      "Epoch 1253, Loss: 0.476846843957901, Final Batch Loss: 0.24640712141990662\n",
      "Epoch 1254, Loss: 0.4718274772167206, Final Batch Loss: 0.2496468722820282\n",
      "Epoch 1255, Loss: 0.47371748089790344, Final Batch Loss: 0.2367347925901413\n",
      "Epoch 1256, Loss: 0.43653909862041473, Final Batch Loss: 0.25182896852493286\n",
      "Epoch 1257, Loss: 0.4180603325366974, Final Batch Loss: 0.14449676871299744\n",
      "Epoch 1258, Loss: 0.4317174106836319, Final Batch Loss: 0.19715280830860138\n",
      "Epoch 1259, Loss: 0.4763689637184143, Final Batch Loss: 0.23087167739868164\n",
      "Epoch 1260, Loss: 0.5290407240390778, Final Batch Loss: 0.26354745030403137\n",
      "Epoch 1261, Loss: 0.43593354523181915, Final Batch Loss: 0.20133763551712036\n",
      "Epoch 1262, Loss: 0.4327012896537781, Final Batch Loss: 0.21290473639965057\n",
      "Epoch 1263, Loss: 0.5043780356645584, Final Batch Loss: 0.29341116547584534\n",
      "Epoch 1264, Loss: 0.5057756900787354, Final Batch Loss: 0.27260082960128784\n",
      "Epoch 1265, Loss: 0.5265839099884033, Final Batch Loss: 0.2692047357559204\n",
      "Epoch 1266, Loss: 0.44217950105667114, Final Batch Loss: 0.21578417718410492\n",
      "Epoch 1267, Loss: 0.46114586293697357, Final Batch Loss: 0.19890762865543365\n",
      "Epoch 1268, Loss: 0.49076245725154877, Final Batch Loss: 0.2731459438800812\n",
      "Epoch 1269, Loss: 0.45587587356567383, Final Batch Loss: 0.20978939533233643\n",
      "Epoch 1270, Loss: 0.4780309647321701, Final Batch Loss: 0.22361986339092255\n",
      "Epoch 1271, Loss: 0.49870212376117706, Final Batch Loss: 0.2684329152107239\n",
      "Epoch 1272, Loss: 0.501496896147728, Final Batch Loss: 0.2544516324996948\n",
      "Epoch 1273, Loss: 0.4869493246078491, Final Batch Loss: 0.2476673573255539\n",
      "Epoch 1274, Loss: 0.42867547273635864, Final Batch Loss: 0.2071259617805481\n",
      "Epoch 1275, Loss: 0.5006144344806671, Final Batch Loss: 0.25716283917427063\n",
      "Epoch 1276, Loss: 0.492938369512558, Final Batch Loss: 0.2835266590118408\n",
      "Epoch 1277, Loss: 0.391853392124176, Final Batch Loss: 0.18420405685901642\n",
      "Epoch 1278, Loss: 0.44390246272087097, Final Batch Loss: 0.2542516887187958\n",
      "Epoch 1279, Loss: 0.42962446808815, Final Batch Loss: 0.17337265610694885\n",
      "Epoch 1280, Loss: 0.4270821511745453, Final Batch Loss: 0.2115572690963745\n",
      "Epoch 1281, Loss: 0.45606575906276703, Final Batch Loss: 0.233870267868042\n",
      "Epoch 1282, Loss: 0.47541242837905884, Final Batch Loss: 0.17498436570167542\n",
      "Epoch 1283, Loss: 0.5287568420171738, Final Batch Loss: 0.24827121198177338\n",
      "Epoch 1284, Loss: 0.4530446231365204, Final Batch Loss: 0.20890508592128754\n",
      "Epoch 1285, Loss: 0.4488523155450821, Final Batch Loss: 0.24970273673534393\n",
      "Epoch 1286, Loss: 0.4463982433080673, Final Batch Loss: 0.22769105434417725\n",
      "Epoch 1287, Loss: 0.4918584078550339, Final Batch Loss: 0.25673165917396545\n",
      "Epoch 1288, Loss: 0.46822676062583923, Final Batch Loss: 0.23181097209453583\n",
      "Epoch 1289, Loss: 0.47226597368717194, Final Batch Loss: 0.2342272400856018\n",
      "Epoch 1290, Loss: 0.5431720018386841, Final Batch Loss: 0.25543737411499023\n",
      "Epoch 1291, Loss: 0.42542289197444916, Final Batch Loss: 0.1898137778043747\n",
      "Epoch 1292, Loss: 0.46875080466270447, Final Batch Loss: 0.2285165786743164\n",
      "Epoch 1293, Loss: 0.4785461574792862, Final Batch Loss: 0.2476966828107834\n",
      "Epoch 1294, Loss: 0.4716181010007858, Final Batch Loss: 0.25397175550460815\n",
      "Epoch 1295, Loss: 0.5037156790494919, Final Batch Loss: 0.26688963174819946\n",
      "Epoch 1296, Loss: 0.4285283535718918, Final Batch Loss: 0.1850639283657074\n",
      "Epoch 1297, Loss: 0.44149015843868256, Final Batch Loss: 0.20144249498844147\n",
      "Epoch 1298, Loss: 0.5254218876361847, Final Batch Loss: 0.24767106771469116\n",
      "Epoch 1299, Loss: 0.4091704487800598, Final Batch Loss: 0.20298022031784058\n",
      "Epoch 1300, Loss: 0.4656315892934799, Final Batch Loss: 0.2374485582113266\n",
      "Epoch 1301, Loss: 0.5018179267644882, Final Batch Loss: 0.22327713668346405\n",
      "Epoch 1302, Loss: 0.43385086953639984, Final Batch Loss: 0.21748800575733185\n",
      "Epoch 1303, Loss: 0.4929492920637131, Final Batch Loss: 0.27476581931114197\n",
      "Epoch 1304, Loss: 0.5083052068948746, Final Batch Loss: 0.2648589611053467\n",
      "Epoch 1305, Loss: 0.4340154677629471, Final Batch Loss: 0.20698416233062744\n",
      "Epoch 1306, Loss: 0.48823219537734985, Final Batch Loss: 0.2827821969985962\n",
      "Epoch 1307, Loss: 0.49040788412094116, Final Batch Loss: 0.25820890069007874\n",
      "Epoch 1308, Loss: 0.4839738458395004, Final Batch Loss: 0.26750612258911133\n",
      "Epoch 1309, Loss: 0.4639440178871155, Final Batch Loss: 0.2555159628391266\n",
      "Epoch 1310, Loss: 0.5474708378314972, Final Batch Loss: 0.2423858940601349\n",
      "Epoch 1311, Loss: 0.4607805907726288, Final Batch Loss: 0.2180279791355133\n",
      "Epoch 1312, Loss: 0.4330858886241913, Final Batch Loss: 0.19844718277454376\n",
      "Epoch 1313, Loss: 0.4722941219806671, Final Batch Loss: 0.23976828157901764\n",
      "Epoch 1314, Loss: 0.5422443151473999, Final Batch Loss: 0.2607898712158203\n",
      "Epoch 1315, Loss: 0.4154735207557678, Final Batch Loss: 0.18206721544265747\n",
      "Epoch 1316, Loss: 0.4469624310731888, Final Batch Loss: 0.1965792328119278\n",
      "Epoch 1317, Loss: 0.43818309903144836, Final Batch Loss: 0.16833528876304626\n",
      "Epoch 1318, Loss: 0.44305087625980377, Final Batch Loss: 0.21141958236694336\n",
      "Epoch 1319, Loss: 0.49163584411144257, Final Batch Loss: 0.2632187008857727\n",
      "Epoch 1320, Loss: 0.4594670385122299, Final Batch Loss: 0.2386244684457779\n",
      "Epoch 1321, Loss: 0.462919145822525, Final Batch Loss: 0.21451248228549957\n",
      "Epoch 1322, Loss: 0.41147275269031525, Final Batch Loss: 0.20226126909255981\n",
      "Epoch 1323, Loss: 0.5071447491645813, Final Batch Loss: 0.24090337753295898\n",
      "Epoch 1324, Loss: 0.4834069162607193, Final Batch Loss: 0.22479383647441864\n",
      "Epoch 1325, Loss: 0.47870689630508423, Final Batch Loss: 0.2642669081687927\n",
      "Epoch 1326, Loss: 0.4678258001804352, Final Batch Loss: 0.26905113458633423\n",
      "Epoch 1327, Loss: 0.47754310071468353, Final Batch Loss: 0.22289110720157623\n",
      "Epoch 1328, Loss: 0.5070657432079315, Final Batch Loss: 0.2766791880130768\n",
      "Epoch 1329, Loss: 0.4945630878210068, Final Batch Loss: 0.22678081691265106\n",
      "Epoch 1330, Loss: 0.45264585316181183, Final Batch Loss: 0.23504161834716797\n",
      "Epoch 1331, Loss: 0.47493675351142883, Final Batch Loss: 0.2351314276456833\n",
      "Epoch 1332, Loss: 0.37846119701862335, Final Batch Loss: 0.1564156860113144\n",
      "Epoch 1333, Loss: 0.3959806114435196, Final Batch Loss: 0.19037798047065735\n",
      "Epoch 1334, Loss: 0.4350053369998932, Final Batch Loss: 0.20548640191555023\n",
      "Epoch 1335, Loss: 0.43088242411613464, Final Batch Loss: 0.21887095272541046\n",
      "Epoch 1336, Loss: 0.42287804186344147, Final Batch Loss: 0.1869056671857834\n",
      "Epoch 1337, Loss: 0.4628114253282547, Final Batch Loss: 0.21544985473155975\n",
      "Epoch 1338, Loss: 0.4182048738002777, Final Batch Loss: 0.2033177763223648\n",
      "Epoch 1339, Loss: 0.45428454875946045, Final Batch Loss: 0.2391924411058426\n",
      "Epoch 1340, Loss: 0.46315455436706543, Final Batch Loss: 0.21785487234592438\n",
      "Epoch 1341, Loss: 0.4571223258972168, Final Batch Loss: 0.22589808702468872\n",
      "Epoch 1342, Loss: 0.39946283400058746, Final Batch Loss: 0.15839692950248718\n",
      "Epoch 1343, Loss: 0.45510703325271606, Final Batch Loss: 0.25554358959198\n",
      "Epoch 1344, Loss: 0.4516551047563553, Final Batch Loss: 0.2605682611465454\n",
      "Epoch 1345, Loss: 0.42544037103652954, Final Batch Loss: 0.21977539360523224\n",
      "Epoch 1346, Loss: 0.42720094323158264, Final Batch Loss: 0.2042820006608963\n",
      "Epoch 1347, Loss: 0.4646433889865875, Final Batch Loss: 0.17017292976379395\n",
      "Epoch 1348, Loss: 0.4936981797218323, Final Batch Loss: 0.23951196670532227\n",
      "Epoch 1349, Loss: 0.4749971330165863, Final Batch Loss: 0.221331387758255\n",
      "Epoch 1350, Loss: 0.49191905558109283, Final Batch Loss: 0.27258771657943726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1351, Loss: 0.44481833279132843, Final Batch Loss: 0.18956197798252106\n",
      "Epoch 1352, Loss: 0.4658632129430771, Final Batch Loss: 0.25776201486587524\n",
      "Epoch 1353, Loss: 0.4506116211414337, Final Batch Loss: 0.18677306175231934\n",
      "Epoch 1354, Loss: 0.40703944861888885, Final Batch Loss: 0.2115510106086731\n",
      "Epoch 1355, Loss: 0.47229859232902527, Final Batch Loss: 0.2601165771484375\n",
      "Epoch 1356, Loss: 0.4367450773715973, Final Batch Loss: 0.1858941614627838\n",
      "Epoch 1357, Loss: 0.4111476093530655, Final Batch Loss: 0.1840851604938507\n",
      "Epoch 1358, Loss: 0.44017194211483, Final Batch Loss: 0.21697644889354706\n",
      "Epoch 1359, Loss: 0.44685186445713043, Final Batch Loss: 0.2402915507555008\n",
      "Epoch 1360, Loss: 0.49640992283821106, Final Batch Loss: 0.24049153923988342\n",
      "Epoch 1361, Loss: 0.4501632899045944, Final Batch Loss: 0.17903651297092438\n",
      "Epoch 1362, Loss: 0.42499834299087524, Final Batch Loss: 0.18994523584842682\n",
      "Epoch 1363, Loss: 0.5331979990005493, Final Batch Loss: 0.2626737952232361\n",
      "Epoch 1364, Loss: 0.41804783046245575, Final Batch Loss: 0.20302997529506683\n",
      "Epoch 1365, Loss: 0.4067174941301346, Final Batch Loss: 0.18562060594558716\n",
      "Epoch 1366, Loss: 0.43341219425201416, Final Batch Loss: 0.1848607212305069\n",
      "Epoch 1367, Loss: 0.4423891454935074, Final Batch Loss: 0.20422950387001038\n",
      "Epoch 1368, Loss: 0.4637965261936188, Final Batch Loss: 0.24948540329933167\n",
      "Epoch 1369, Loss: 0.47030898928642273, Final Batch Loss: 0.21323725581169128\n",
      "Epoch 1370, Loss: 0.4811871647834778, Final Batch Loss: 0.24976447224617004\n",
      "Epoch 1371, Loss: 0.464602991938591, Final Batch Loss: 0.19505555927753448\n",
      "Epoch 1372, Loss: 0.4692763537168503, Final Batch Loss: 0.2516022324562073\n",
      "Epoch 1373, Loss: 0.5248799473047256, Final Batch Loss: 0.24677027761936188\n",
      "Epoch 1374, Loss: 0.40067531168460846, Final Batch Loss: 0.15838849544525146\n",
      "Epoch 1375, Loss: 0.4042031466960907, Final Batch Loss: 0.16774454712867737\n",
      "Epoch 1376, Loss: 0.49471481144428253, Final Batch Loss: 0.18998120725154877\n",
      "Epoch 1377, Loss: 0.4508971720933914, Final Batch Loss: 0.2295779585838318\n",
      "Epoch 1378, Loss: 0.45572371780872345, Final Batch Loss: 0.22720347344875336\n",
      "Epoch 1379, Loss: 0.39993610978126526, Final Batch Loss: 0.17189036309719086\n",
      "Epoch 1380, Loss: 0.4305349737405777, Final Batch Loss: 0.21800963580608368\n",
      "Epoch 1381, Loss: 0.4436976909637451, Final Batch Loss: 0.23166660964488983\n",
      "Epoch 1382, Loss: 0.40311475098133087, Final Batch Loss: 0.20033907890319824\n",
      "Epoch 1383, Loss: 0.4200794845819473, Final Batch Loss: 0.19111992418766022\n",
      "Epoch 1384, Loss: 0.5452172011137009, Final Batch Loss: 0.3315301835536957\n",
      "Epoch 1385, Loss: 0.467783585190773, Final Batch Loss: 0.20550678670406342\n",
      "Epoch 1386, Loss: 0.46966464817523956, Final Batch Loss: 0.22892501950263977\n",
      "Epoch 1387, Loss: 0.4426281452178955, Final Batch Loss: 0.22261478006839752\n",
      "Epoch 1388, Loss: 0.46761682629585266, Final Batch Loss: 0.2508127689361572\n",
      "Epoch 1389, Loss: 0.42755943536758423, Final Batch Loss: 0.17533919215202332\n",
      "Epoch 1390, Loss: 0.460385262966156, Final Batch Loss: 0.2526366710662842\n",
      "Epoch 1391, Loss: 0.426958367228508, Final Batch Loss: 0.18514807522296906\n",
      "Epoch 1392, Loss: 0.4383842647075653, Final Batch Loss: 0.19998951256275177\n",
      "Epoch 1393, Loss: 0.42376919090747833, Final Batch Loss: 0.2321789264678955\n",
      "Epoch 1394, Loss: 0.509827196598053, Final Batch Loss: 0.2586979269981384\n",
      "Epoch 1395, Loss: 0.46317577362060547, Final Batch Loss: 0.26135995984077454\n",
      "Epoch 1396, Loss: 0.44808170199394226, Final Batch Loss: 0.2123338133096695\n",
      "Epoch 1397, Loss: 0.4888025373220444, Final Batch Loss: 0.2833469808101654\n",
      "Epoch 1398, Loss: 0.499026894569397, Final Batch Loss: 0.2306745946407318\n",
      "Epoch 1399, Loss: 0.4025868624448776, Final Batch Loss: 0.1988847404718399\n",
      "Epoch 1400, Loss: 0.4025818109512329, Final Batch Loss: 0.18171294033527374\n",
      "Epoch 1401, Loss: 0.5429564416408539, Final Batch Loss: 0.32785558700561523\n",
      "Epoch 1402, Loss: 0.44855034351348877, Final Batch Loss: 0.23298901319503784\n",
      "Epoch 1403, Loss: 0.4341703802347183, Final Batch Loss: 0.19918695092201233\n",
      "Epoch 1404, Loss: 0.4313390254974365, Final Batch Loss: 0.20398195087909698\n",
      "Epoch 1405, Loss: 0.4471794664859772, Final Batch Loss: 0.22366125881671906\n",
      "Epoch 1406, Loss: 0.4829293489456177, Final Batch Loss: 0.24316777288913727\n",
      "Epoch 1407, Loss: 0.44075097143650055, Final Batch Loss: 0.21979691088199615\n",
      "Epoch 1408, Loss: 0.4837960749864578, Final Batch Loss: 0.2412874698638916\n",
      "Epoch 1409, Loss: 0.4328075498342514, Final Batch Loss: 0.22114185988903046\n",
      "Epoch 1410, Loss: 0.5111772418022156, Final Batch Loss: 0.28685224056243896\n",
      "Epoch 1411, Loss: 0.4365965723991394, Final Batch Loss: 0.23464258015155792\n",
      "Epoch 1412, Loss: 0.36833956837654114, Final Batch Loss: 0.16694214940071106\n",
      "Epoch 1413, Loss: 0.46232618391513824, Final Batch Loss: 0.2552918493747711\n",
      "Epoch 1414, Loss: 0.4383131414651871, Final Batch Loss: 0.20515350997447968\n",
      "Epoch 1415, Loss: 0.4802761375904083, Final Batch Loss: 0.234779953956604\n",
      "Epoch 1416, Loss: 0.4494054913520813, Final Batch Loss: 0.18149667978286743\n",
      "Epoch 1417, Loss: 0.44026412069797516, Final Batch Loss: 0.23522396385669708\n",
      "Epoch 1418, Loss: 0.48273442685604095, Final Batch Loss: 0.20230557024478912\n",
      "Epoch 1419, Loss: 0.4074975997209549, Final Batch Loss: 0.1987711787223816\n",
      "Epoch 1420, Loss: 0.39426058530807495, Final Batch Loss: 0.16947419941425323\n",
      "Epoch 1421, Loss: 0.4475914388895035, Final Batch Loss: 0.18253232538700104\n",
      "Epoch 1422, Loss: 0.44984589517116547, Final Batch Loss: 0.24046190083026886\n",
      "Epoch 1423, Loss: 0.4211692810058594, Final Batch Loss: 0.1617582142353058\n",
      "Epoch 1424, Loss: 0.4545009285211563, Final Batch Loss: 0.215630441904068\n",
      "Epoch 1425, Loss: 0.4855113625526428, Final Batch Loss: 0.24256375432014465\n",
      "Epoch 1426, Loss: 0.47073620557785034, Final Batch Loss: 0.2444782704114914\n",
      "Epoch 1427, Loss: 0.4317810833454132, Final Batch Loss: 0.22457760572433472\n",
      "Epoch 1428, Loss: 0.46188248693943024, Final Batch Loss: 0.25614428520202637\n",
      "Epoch 1429, Loss: 0.39006438851356506, Final Batch Loss: 0.17397595942020416\n",
      "Epoch 1430, Loss: 0.5087529122829437, Final Batch Loss: 0.2577698230743408\n",
      "Epoch 1431, Loss: 0.408269464969635, Final Batch Loss: 0.1954706311225891\n",
      "Epoch 1432, Loss: 0.45860160887241364, Final Batch Loss: 0.25291919708251953\n",
      "Epoch 1433, Loss: 0.4298925846815109, Final Batch Loss: 0.21229800581932068\n",
      "Epoch 1434, Loss: 0.43063773214817047, Final Batch Loss: 0.22030580043792725\n",
      "Epoch 1435, Loss: 0.48449966311454773, Final Batch Loss: 0.2616014778614044\n",
      "Epoch 1436, Loss: 0.47220945358276367, Final Batch Loss: 0.22122877836227417\n",
      "Epoch 1437, Loss: 0.5173198878765106, Final Batch Loss: 0.33334848284721375\n",
      "Epoch 1438, Loss: 0.44663845002651215, Final Batch Loss: 0.2224961668252945\n",
      "Epoch 1439, Loss: 0.5423124432563782, Final Batch Loss: 0.3073938190937042\n",
      "Epoch 1440, Loss: 0.4310138672590256, Final Batch Loss: 0.19719381630420685\n",
      "Epoch 1441, Loss: 0.46285900473594666, Final Batch Loss: 0.24840384721755981\n",
      "Epoch 1442, Loss: 0.4240914136171341, Final Batch Loss: 0.23052309453487396\n",
      "Epoch 1443, Loss: 0.45733529329299927, Final Batch Loss: 0.19124463200569153\n",
      "Epoch 1444, Loss: 0.420980304479599, Final Batch Loss: 0.17053478956222534\n",
      "Epoch 1445, Loss: 0.4180169254541397, Final Batch Loss: 0.20079869031906128\n",
      "Epoch 1446, Loss: 0.45759111642837524, Final Batch Loss: 0.23104092478752136\n",
      "Epoch 1447, Loss: 0.5112074464559555, Final Batch Loss: 0.21931912004947662\n",
      "Epoch 1448, Loss: 0.4866654574871063, Final Batch Loss: 0.2791040241718292\n",
      "Epoch 1449, Loss: 0.45008398592472076, Final Batch Loss: 0.23921234905719757\n",
      "Epoch 1450, Loss: 0.500070109963417, Final Batch Loss: 0.24616147577762604\n",
      "Epoch 1451, Loss: 0.4355422258377075, Final Batch Loss: 0.2198389768600464\n",
      "Epoch 1452, Loss: 0.4346637576818466, Final Batch Loss: 0.2505306005477905\n",
      "Epoch 1453, Loss: 0.47830793261528015, Final Batch Loss: 0.28987857699394226\n",
      "Epoch 1454, Loss: 0.4235188364982605, Final Batch Loss: 0.23277337849140167\n",
      "Epoch 1455, Loss: 0.39907675981521606, Final Batch Loss: 0.20126299560070038\n",
      "Epoch 1456, Loss: 0.5157037675380707, Final Batch Loss: 0.2860769033432007\n",
      "Epoch 1457, Loss: 0.457315057516098, Final Batch Loss: 0.20911236107349396\n",
      "Epoch 1458, Loss: 0.41784754395484924, Final Batch Loss: 0.22034282982349396\n",
      "Epoch 1459, Loss: 0.4021841287612915, Final Batch Loss: 0.19763849675655365\n",
      "Epoch 1460, Loss: 0.5156651139259338, Final Batch Loss: 0.22605454921722412\n",
      "Epoch 1461, Loss: 0.4381088316440582, Final Batch Loss: 0.1703113317489624\n",
      "Epoch 1462, Loss: 0.4923394024372101, Final Batch Loss: 0.29586049914360046\n",
      "Epoch 1463, Loss: 0.44354015588760376, Final Batch Loss: 0.21278195083141327\n",
      "Epoch 1464, Loss: 0.4451029449701309, Final Batch Loss: 0.23847880959510803\n",
      "Epoch 1465, Loss: 0.43544885516166687, Final Batch Loss: 0.1827045977115631\n",
      "Epoch 1466, Loss: 0.4470314681529999, Final Batch Loss: 0.26223301887512207\n",
      "Epoch 1467, Loss: 0.39814023673534393, Final Batch Loss: 0.17152267694473267\n",
      "Epoch 1468, Loss: 0.4831514060497284, Final Batch Loss: 0.2920726239681244\n",
      "Epoch 1469, Loss: 0.41745004057884216, Final Batch Loss: 0.21582864224910736\n",
      "Epoch 1470, Loss: 0.38105960190296173, Final Batch Loss: 0.15244874358177185\n",
      "Epoch 1471, Loss: 0.4629286527633667, Final Batch Loss: 0.27569207549095154\n",
      "Epoch 1472, Loss: 0.45326170325279236, Final Batch Loss: 0.18202370405197144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1473, Loss: 0.45031075179576874, Final Batch Loss: 0.24498140811920166\n",
      "Epoch 1474, Loss: 0.4194630980491638, Final Batch Loss: 0.19008831679821014\n",
      "Epoch 1475, Loss: 0.47383520007133484, Final Batch Loss: 0.23533885180950165\n",
      "Epoch 1476, Loss: 0.47715863585472107, Final Batch Loss: 0.2231280505657196\n",
      "Epoch 1477, Loss: 0.449939489364624, Final Batch Loss: 0.21033179759979248\n",
      "Epoch 1478, Loss: 0.43134722113609314, Final Batch Loss: 0.22836902737617493\n",
      "Epoch 1479, Loss: 0.4272371679544449, Final Batch Loss: 0.22102148830890656\n",
      "Epoch 1480, Loss: 0.5067056715488434, Final Batch Loss: 0.28693887591362\n",
      "Epoch 1481, Loss: 0.48929542303085327, Final Batch Loss: 0.22669821977615356\n",
      "Epoch 1482, Loss: 0.5007976740598679, Final Batch Loss: 0.2898847758769989\n",
      "Epoch 1483, Loss: 0.3877035826444626, Final Batch Loss: 0.14449824392795563\n",
      "Epoch 1484, Loss: 0.46096622943878174, Final Batch Loss: 0.2487964779138565\n",
      "Epoch 1485, Loss: 0.4709091931581497, Final Batch Loss: 0.2722664177417755\n",
      "Epoch 1486, Loss: 0.4261261224746704, Final Batch Loss: 0.22165238857269287\n",
      "Epoch 1487, Loss: 0.46848973631858826, Final Batch Loss: 0.2627129852771759\n",
      "Epoch 1488, Loss: 0.4274759143590927, Final Batch Loss: 0.19242198765277863\n",
      "Epoch 1489, Loss: 0.4112146347761154, Final Batch Loss: 0.19513516128063202\n",
      "Epoch 1490, Loss: 0.4547189772129059, Final Batch Loss: 0.2549230456352234\n",
      "Epoch 1491, Loss: 0.4106094092130661, Final Batch Loss: 0.16995427012443542\n",
      "Epoch 1492, Loss: 0.4502585232257843, Final Batch Loss: 0.23186492919921875\n",
      "Epoch 1493, Loss: 0.4159301072359085, Final Batch Loss: 0.21309630572795868\n",
      "Epoch 1494, Loss: 0.44098909199237823, Final Batch Loss: 0.19789700210094452\n",
      "Epoch 1495, Loss: 0.38321296870708466, Final Batch Loss: 0.18770040571689606\n",
      "Epoch 1496, Loss: 0.4453911930322647, Final Batch Loss: 0.22487536072731018\n",
      "Epoch 1497, Loss: 0.4478438198566437, Final Batch Loss: 0.24026191234588623\n",
      "Epoch 1498, Loss: 0.4147769957780838, Final Batch Loss: 0.21661069989204407\n",
      "Epoch 1499, Loss: 0.5032745450735092, Final Batch Loss: 0.296723872423172\n",
      "Epoch 1500, Loss: 0.461896151304245, Final Batch Loss: 0.2780568301677704\n",
      "Epoch 1501, Loss: 0.43826593458652496, Final Batch Loss: 0.2170897275209427\n",
      "Epoch 1502, Loss: 0.40837472677230835, Final Batch Loss: 0.21150200068950653\n",
      "Epoch 1503, Loss: 0.4205859303474426, Final Batch Loss: 0.24031609296798706\n",
      "Epoch 1504, Loss: 0.4553648978471756, Final Batch Loss: 0.22494551539421082\n",
      "Epoch 1505, Loss: 0.405333936214447, Final Batch Loss: 0.2222636193037033\n",
      "Epoch 1506, Loss: 0.3772723078727722, Final Batch Loss: 0.18273091316223145\n",
      "Epoch 1507, Loss: 0.42898593842983246, Final Batch Loss: 0.20878493785858154\n",
      "Epoch 1508, Loss: 0.4216844290494919, Final Batch Loss: 0.2300189584493637\n",
      "Epoch 1509, Loss: 0.4885706901550293, Final Batch Loss: 0.2375863790512085\n",
      "Epoch 1510, Loss: 0.48887427151203156, Final Batch Loss: 0.2550511956214905\n",
      "Epoch 1511, Loss: 0.4382869154214859, Final Batch Loss: 0.2323378473520279\n",
      "Epoch 1512, Loss: 0.4123527407646179, Final Batch Loss: 0.19032643735408783\n",
      "Epoch 1513, Loss: 0.4381001740694046, Final Batch Loss: 0.19325977563858032\n",
      "Epoch 1514, Loss: 0.3782111555337906, Final Batch Loss: 0.16665883362293243\n",
      "Epoch 1515, Loss: 0.41388657689094543, Final Batch Loss: 0.1586986482143402\n",
      "Epoch 1516, Loss: 0.5264250040054321, Final Batch Loss: 0.27354660630226135\n",
      "Epoch 1517, Loss: 0.4040665179491043, Final Batch Loss: 0.1752329021692276\n",
      "Epoch 1518, Loss: 0.41342970728874207, Final Batch Loss: 0.22309640049934387\n",
      "Epoch 1519, Loss: 0.4326987862586975, Final Batch Loss: 0.2215414196252823\n",
      "Epoch 1520, Loss: 0.4497275799512863, Final Batch Loss: 0.21633955836296082\n",
      "Epoch 1521, Loss: 0.4430473744869232, Final Batch Loss: 0.24099721014499664\n",
      "Epoch 1522, Loss: 0.4508889615535736, Final Batch Loss: 0.2409857213497162\n",
      "Epoch 1523, Loss: 0.44817499816417694, Final Batch Loss: 0.23100697994232178\n",
      "Epoch 1524, Loss: 0.46313560009002686, Final Batch Loss: 0.26424115896224976\n",
      "Epoch 1525, Loss: 0.4250700920820236, Final Batch Loss: 0.23875629901885986\n",
      "Epoch 1526, Loss: 0.4647165685892105, Final Batch Loss: 0.23241344094276428\n",
      "Epoch 1527, Loss: 0.4582183510065079, Final Batch Loss: 0.21704313158988953\n",
      "Epoch 1528, Loss: 0.4115876257419586, Final Batch Loss: 0.18139038980007172\n",
      "Epoch 1529, Loss: 0.431160107254982, Final Batch Loss: 0.18616421520709991\n",
      "Epoch 1530, Loss: 0.4886568486690521, Final Batch Loss: 0.2807714641094208\n",
      "Epoch 1531, Loss: 0.4229959398508072, Final Batch Loss: 0.2531277537345886\n",
      "Epoch 1532, Loss: 0.4331206977367401, Final Batch Loss: 0.19799624383449554\n",
      "Epoch 1533, Loss: 0.4726940989494324, Final Batch Loss: 0.23747800290584564\n",
      "Epoch 1534, Loss: 0.3994445949792862, Final Batch Loss: 0.1797759234905243\n",
      "Epoch 1535, Loss: 0.3885587006807327, Final Batch Loss: 0.1602812260389328\n",
      "Epoch 1536, Loss: 0.40270717442035675, Final Batch Loss: 0.20853744447231293\n",
      "Epoch 1537, Loss: 0.44669313728809357, Final Batch Loss: 0.2601391077041626\n",
      "Epoch 1538, Loss: 0.40222547948360443, Final Batch Loss: 0.16030262410640717\n",
      "Epoch 1539, Loss: 0.3919414132833481, Final Batch Loss: 0.1865304410457611\n",
      "Epoch 1540, Loss: 0.41157104074954987, Final Batch Loss: 0.20379987359046936\n",
      "Epoch 1541, Loss: 0.45356355607509613, Final Batch Loss: 0.24156010150909424\n",
      "Epoch 1542, Loss: 0.40795017778873444, Final Batch Loss: 0.20521041750907898\n",
      "Epoch 1543, Loss: 0.4239036589860916, Final Batch Loss: 0.2328232228755951\n",
      "Epoch 1544, Loss: 0.3960212618112564, Final Batch Loss: 0.1535956710577011\n",
      "Epoch 1545, Loss: 0.46330922842025757, Final Batch Loss: 0.23802296817302704\n",
      "Epoch 1546, Loss: 0.4714403301477432, Final Batch Loss: 0.21061672270298004\n",
      "Epoch 1547, Loss: 0.4145027846097946, Final Batch Loss: 0.19942298531532288\n",
      "Epoch 1548, Loss: 0.4137325882911682, Final Batch Loss: 0.2084638476371765\n",
      "Epoch 1549, Loss: 0.40486928820610046, Final Batch Loss: 0.17521853744983673\n",
      "Epoch 1550, Loss: 0.42928555607795715, Final Batch Loss: 0.19875428080558777\n",
      "Epoch 1551, Loss: 0.44939403235912323, Final Batch Loss: 0.240779846906662\n",
      "Epoch 1552, Loss: 0.44950252771377563, Final Batch Loss: 0.19257888197898865\n",
      "Epoch 1553, Loss: 0.3753123879432678, Final Batch Loss: 0.16980214416980743\n",
      "Epoch 1554, Loss: 0.4103710800409317, Final Batch Loss: 0.20746715366840363\n",
      "Epoch 1555, Loss: 0.4315571188926697, Final Batch Loss: 0.19420447945594788\n",
      "Epoch 1556, Loss: 0.37495948374271393, Final Batch Loss: 0.17303608357906342\n",
      "Epoch 1557, Loss: 0.3823702335357666, Final Batch Loss: 0.18573570251464844\n",
      "Epoch 1558, Loss: 0.43109460175037384, Final Batch Loss: 0.18378454446792603\n",
      "Epoch 1559, Loss: 0.44306161999702454, Final Batch Loss: 0.20622853934764862\n",
      "Epoch 1560, Loss: 0.43829450011253357, Final Batch Loss: 0.21555498242378235\n",
      "Epoch 1561, Loss: 0.40753450989723206, Final Batch Loss: 0.20220385491847992\n",
      "Epoch 1562, Loss: 0.3981068283319473, Final Batch Loss: 0.19450487196445465\n",
      "Epoch 1563, Loss: 0.41910895705223083, Final Batch Loss: 0.21253527700901031\n",
      "Epoch 1564, Loss: 0.4057822674512863, Final Batch Loss: 0.20130476355552673\n",
      "Epoch 1565, Loss: 0.47099748253822327, Final Batch Loss: 0.2365829348564148\n",
      "Epoch 1566, Loss: 0.4716709107160568, Final Batch Loss: 0.22898626327514648\n",
      "Epoch 1567, Loss: 0.42181259393692017, Final Batch Loss: 0.22211365401744843\n",
      "Epoch 1568, Loss: 0.4018745869398117, Final Batch Loss: 0.19181154668331146\n",
      "Epoch 1569, Loss: 0.4248562157154083, Final Batch Loss: 0.2186235934495926\n",
      "Epoch 1570, Loss: 0.38856300711631775, Final Batch Loss: 0.14000102877616882\n",
      "Epoch 1571, Loss: 0.39339210093021393, Final Batch Loss: 0.18535380065441132\n",
      "Epoch 1572, Loss: 0.3982341140508652, Final Batch Loss: 0.20447027683258057\n",
      "Epoch 1573, Loss: 0.42927002906799316, Final Batch Loss: 0.1795082539319992\n",
      "Epoch 1574, Loss: 0.42474423348903656, Final Batch Loss: 0.19007045030593872\n",
      "Epoch 1575, Loss: 0.37434162199497223, Final Batch Loss: 0.16730214655399323\n",
      "Epoch 1576, Loss: 0.4357907623052597, Final Batch Loss: 0.21193309128284454\n",
      "Epoch 1577, Loss: 0.47191645205020905, Final Batch Loss: 0.24974875152111053\n",
      "Epoch 1578, Loss: 0.45966339111328125, Final Batch Loss: 0.23544643819332123\n",
      "Epoch 1579, Loss: 0.44139237701892853, Final Batch Loss: 0.20244969427585602\n",
      "Epoch 1580, Loss: 0.47645775973796844, Final Batch Loss: 0.2779056131839752\n",
      "Epoch 1581, Loss: 0.413645938038826, Final Batch Loss: 0.1703859567642212\n",
      "Epoch 1582, Loss: 0.43105633556842804, Final Batch Loss: 0.2284114807844162\n",
      "Epoch 1583, Loss: 0.41408614814281464, Final Batch Loss: 0.21072988212108612\n",
      "Epoch 1584, Loss: 0.39426425099372864, Final Batch Loss: 0.2102922648191452\n",
      "Epoch 1585, Loss: 0.39573533833026886, Final Batch Loss: 0.2204386591911316\n",
      "Epoch 1586, Loss: 0.463203564286232, Final Batch Loss: 0.21404071152210236\n",
      "Epoch 1587, Loss: 0.3960734158754349, Final Batch Loss: 0.20096942782402039\n",
      "Epoch 1588, Loss: 0.40094028413295746, Final Batch Loss: 0.219980850815773\n",
      "Epoch 1589, Loss: 0.46708595752716064, Final Batch Loss: 0.23526781797409058\n",
      "Epoch 1590, Loss: 0.42755185067653656, Final Batch Loss: 0.19288797676563263\n",
      "Epoch 1591, Loss: 0.3763543665409088, Final Batch Loss: 0.15173770487308502\n",
      "Epoch 1592, Loss: 0.3965476006269455, Final Batch Loss: 0.22093506157398224\n",
      "Epoch 1593, Loss: 0.4159283936023712, Final Batch Loss: 0.24017813801765442\n",
      "Epoch 1594, Loss: 0.4078395366668701, Final Batch Loss: 0.20438070595264435\n",
      "Epoch 1595, Loss: 0.4269159436225891, Final Batch Loss: 0.18592745065689087\n",
      "Epoch 1596, Loss: 0.4303535968065262, Final Batch Loss: 0.23795616626739502\n",
      "Epoch 1597, Loss: 0.45101596415042877, Final Batch Loss: 0.25768837332725525\n",
      "Epoch 1598, Loss: 0.42108598351478577, Final Batch Loss: 0.18853795528411865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1599, Loss: 0.4427857995033264, Final Batch Loss: 0.21787191927433014\n",
      "Epoch 1600, Loss: 0.38226792216300964, Final Batch Loss: 0.134333074092865\n",
      "Epoch 1601, Loss: 0.4446025937795639, Final Batch Loss: 0.2351267784833908\n",
      "Epoch 1602, Loss: 0.455046683549881, Final Batch Loss: 0.2549806833267212\n",
      "Epoch 1603, Loss: 0.42384280264377594, Final Batch Loss: 0.2600761651992798\n",
      "Epoch 1604, Loss: 0.39570729434490204, Final Batch Loss: 0.21502991020679474\n",
      "Epoch 1605, Loss: 0.43573032319545746, Final Batch Loss: 0.21309278905391693\n",
      "Epoch 1606, Loss: 0.40749767422676086, Final Batch Loss: 0.1865461766719818\n",
      "Epoch 1607, Loss: 0.3999033123254776, Final Batch Loss: 0.1896888017654419\n",
      "Epoch 1608, Loss: 0.4402652084827423, Final Batch Loss: 0.22087138891220093\n",
      "Epoch 1609, Loss: 0.4452107101678848, Final Batch Loss: 0.1998741775751114\n",
      "Epoch 1610, Loss: 0.42597101628780365, Final Batch Loss: 0.19574585556983948\n",
      "Epoch 1611, Loss: 0.4275464564561844, Final Batch Loss: 0.19640621542930603\n",
      "Epoch 1612, Loss: 0.4104302078485489, Final Batch Loss: 0.2072659134864807\n",
      "Epoch 1613, Loss: 0.47398483753204346, Final Batch Loss: 0.22505800426006317\n",
      "Epoch 1614, Loss: 0.40099433064460754, Final Batch Loss: 0.1855154037475586\n",
      "Epoch 1615, Loss: 0.46707841753959656, Final Batch Loss: 0.25461453199386597\n",
      "Epoch 1616, Loss: 0.415647029876709, Final Batch Loss: 0.19771267473697662\n",
      "Epoch 1617, Loss: 0.4100937992334366, Final Batch Loss: 0.19813863933086395\n",
      "Epoch 1618, Loss: 0.45472055673599243, Final Batch Loss: 0.2640857696533203\n",
      "Epoch 1619, Loss: 0.4989365339279175, Final Batch Loss: 0.25440046191215515\n",
      "Epoch 1620, Loss: 0.4282745271921158, Final Batch Loss: 0.19871187210083008\n",
      "Epoch 1621, Loss: 0.4305220991373062, Final Batch Loss: 0.21315549314022064\n",
      "Epoch 1622, Loss: 0.4439001828432083, Final Batch Loss: 0.22153620421886444\n",
      "Epoch 1623, Loss: 0.39740021526813507, Final Batch Loss: 0.2079533487558365\n",
      "Epoch 1624, Loss: 0.41797323524951935, Final Batch Loss: 0.19654209911823273\n",
      "Epoch 1625, Loss: 0.43399757146835327, Final Batch Loss: 0.21953655779361725\n",
      "Epoch 1626, Loss: 0.44555991888046265, Final Batch Loss: 0.26195329427719116\n",
      "Epoch 1627, Loss: 0.43309713900089264, Final Batch Loss: 0.21464748680591583\n",
      "Epoch 1628, Loss: 0.4999890625476837, Final Batch Loss: 0.28883546590805054\n",
      "Epoch 1629, Loss: 0.43749581277370453, Final Batch Loss: 0.23012717068195343\n",
      "Epoch 1630, Loss: 0.39923739433288574, Final Batch Loss: 0.17152643203735352\n",
      "Epoch 1631, Loss: 0.3869408220052719, Final Batch Loss: 0.20361173152923584\n",
      "Epoch 1632, Loss: 0.4302412122488022, Final Batch Loss: 0.27100804448127747\n",
      "Epoch 1633, Loss: 0.39504456520080566, Final Batch Loss: 0.19613204896450043\n",
      "Epoch 1634, Loss: 0.4171683043241501, Final Batch Loss: 0.20343543589115143\n",
      "Epoch 1635, Loss: 0.44241295754909515, Final Batch Loss: 0.24761685729026794\n",
      "Epoch 1636, Loss: 0.4078008681535721, Final Batch Loss: 0.1546929031610489\n",
      "Epoch 1637, Loss: 0.4250021278858185, Final Batch Loss: 0.23941200971603394\n",
      "Epoch 1638, Loss: 0.47053469717502594, Final Batch Loss: 0.27299433946609497\n",
      "Epoch 1639, Loss: 0.3735535740852356, Final Batch Loss: 0.17770208418369293\n",
      "Epoch 1640, Loss: 0.4186471551656723, Final Batch Loss: 0.2003049999475479\n",
      "Epoch 1641, Loss: 0.4171837568283081, Final Batch Loss: 0.1995561420917511\n",
      "Epoch 1642, Loss: 0.427580863237381, Final Batch Loss: 0.19436520338058472\n",
      "Epoch 1643, Loss: 0.382217600941658, Final Batch Loss: 0.19240731000900269\n",
      "Epoch 1644, Loss: 0.4679926484823227, Final Batch Loss: 0.23807767033576965\n",
      "Epoch 1645, Loss: 0.4413950443267822, Final Batch Loss: 0.24539496004581451\n",
      "Epoch 1646, Loss: 0.444485068321228, Final Batch Loss: 0.21858742833137512\n",
      "Epoch 1647, Loss: 0.4011359363794327, Final Batch Loss: 0.2171517014503479\n",
      "Epoch 1648, Loss: 0.3984428346157074, Final Batch Loss: 0.17078764736652374\n",
      "Epoch 1649, Loss: 0.395512655377388, Final Batch Loss: 0.19039088487625122\n",
      "Epoch 1650, Loss: 0.3808573782444, Final Batch Loss: 0.18224389851093292\n",
      "Epoch 1651, Loss: 0.40376974642276764, Final Batch Loss: 0.18104961514472961\n",
      "Epoch 1652, Loss: 0.4207031726837158, Final Batch Loss: 0.1958279013633728\n",
      "Epoch 1653, Loss: 0.44682398438453674, Final Batch Loss: 0.24160662293434143\n",
      "Epoch 1654, Loss: 0.3813413232564926, Final Batch Loss: 0.1768840253353119\n",
      "Epoch 1655, Loss: 0.39739489555358887, Final Batch Loss: 0.21650157868862152\n",
      "Epoch 1656, Loss: 0.38916605710983276, Final Batch Loss: 0.19044823944568634\n",
      "Epoch 1657, Loss: 0.43462875485420227, Final Batch Loss: 0.21510767936706543\n",
      "Epoch 1658, Loss: 0.4428660571575165, Final Batch Loss: 0.22162039577960968\n",
      "Epoch 1659, Loss: 0.402483731508255, Final Batch Loss: 0.1831306368112564\n",
      "Epoch 1660, Loss: 0.4205317497253418, Final Batch Loss: 0.20184099674224854\n",
      "Epoch 1661, Loss: 0.41562406718730927, Final Batch Loss: 0.18491698801517487\n",
      "Epoch 1662, Loss: 0.4218153804540634, Final Batch Loss: 0.192975714802742\n",
      "Epoch 1663, Loss: 0.3603672534227371, Final Batch Loss: 0.1640094667673111\n",
      "Epoch 1664, Loss: 0.4090309590101242, Final Batch Loss: 0.16479142010211945\n",
      "Epoch 1665, Loss: 0.37469375133514404, Final Batch Loss: 0.15656490623950958\n",
      "Epoch 1666, Loss: 0.4605429321527481, Final Batch Loss: 0.23711895942687988\n",
      "Epoch 1667, Loss: 0.4134363830089569, Final Batch Loss: 0.24569009244441986\n",
      "Epoch 1668, Loss: 0.3536858558654785, Final Batch Loss: 0.14944027364253998\n",
      "Epoch 1669, Loss: 0.4045775532722473, Final Batch Loss: 0.19832322001457214\n",
      "Epoch 1670, Loss: 0.36641767621040344, Final Batch Loss: 0.1657879650592804\n",
      "Epoch 1671, Loss: 0.36581796407699585, Final Batch Loss: 0.18434597551822662\n",
      "Epoch 1672, Loss: 0.40648606419563293, Final Batch Loss: 0.19867567718029022\n",
      "Epoch 1673, Loss: 0.4677143841981888, Final Batch Loss: 0.2355959564447403\n",
      "Epoch 1674, Loss: 0.41573721170425415, Final Batch Loss: 0.19145746529102325\n",
      "Epoch 1675, Loss: 0.39949333667755127, Final Batch Loss: 0.19681374728679657\n",
      "Epoch 1676, Loss: 0.3363562226295471, Final Batch Loss: 0.13452237844467163\n",
      "Epoch 1677, Loss: 0.4565707743167877, Final Batch Loss: 0.24229225516319275\n",
      "Epoch 1678, Loss: 0.4508674293756485, Final Batch Loss: 0.21781179308891296\n",
      "Epoch 1679, Loss: 0.43074382841587067, Final Batch Loss: 0.23404686152935028\n",
      "Epoch 1680, Loss: 0.4041900485754013, Final Batch Loss: 0.2145153433084488\n",
      "Epoch 1681, Loss: 0.4344521760940552, Final Batch Loss: 0.244668111205101\n",
      "Epoch 1682, Loss: 0.46153324842453003, Final Batch Loss: 0.28157007694244385\n",
      "Epoch 1683, Loss: 0.465796634554863, Final Batch Loss: 0.2613886594772339\n",
      "Epoch 1684, Loss: 0.39638616144657135, Final Batch Loss: 0.1976117044687271\n",
      "Epoch 1685, Loss: 0.4424470514059067, Final Batch Loss: 0.2148052155971527\n",
      "Epoch 1686, Loss: 0.4265774339437485, Final Batch Loss: 0.21820607781410217\n",
      "Epoch 1687, Loss: 0.420420840382576, Final Batch Loss: 0.24567849934101105\n",
      "Epoch 1688, Loss: 0.3886433243751526, Final Batch Loss: 0.1676873415708542\n",
      "Epoch 1689, Loss: 0.45211200416088104, Final Batch Loss: 0.22928941249847412\n",
      "Epoch 1690, Loss: 0.4257221519947052, Final Batch Loss: 0.17484250664710999\n",
      "Epoch 1691, Loss: 0.4071032851934433, Final Batch Loss: 0.1935836523771286\n",
      "Epoch 1692, Loss: 0.4129853695631027, Final Batch Loss: 0.21756751835346222\n",
      "Epoch 1693, Loss: 0.38646119832992554, Final Batch Loss: 0.1898530274629593\n",
      "Epoch 1694, Loss: 0.3894958943128586, Final Batch Loss: 0.21079187095165253\n",
      "Epoch 1695, Loss: 0.4222162067890167, Final Batch Loss: 0.22902783751487732\n",
      "Epoch 1696, Loss: 0.44384288787841797, Final Batch Loss: 0.22069430351257324\n",
      "Epoch 1697, Loss: 0.4712827354669571, Final Batch Loss: 0.24366864562034607\n",
      "Epoch 1698, Loss: 0.4447169303894043, Final Batch Loss: 0.1953655481338501\n",
      "Epoch 1699, Loss: 0.3862128257751465, Final Batch Loss: 0.17242597043514252\n",
      "Epoch 1700, Loss: 0.40863074362277985, Final Batch Loss: 0.19813697040081024\n",
      "Epoch 1701, Loss: 0.38582323491573334, Final Batch Loss: 0.2103160321712494\n",
      "Epoch 1702, Loss: 0.47277285158634186, Final Batch Loss: 0.30856791138648987\n",
      "Epoch 1703, Loss: 0.4159950315952301, Final Batch Loss: 0.20070840418338776\n",
      "Epoch 1704, Loss: 0.4287191480398178, Final Batch Loss: 0.1905338168144226\n",
      "Epoch 1705, Loss: 0.37615571916103363, Final Batch Loss: 0.19999291002750397\n",
      "Epoch 1706, Loss: 0.3983677327632904, Final Batch Loss: 0.18518337607383728\n",
      "Epoch 1707, Loss: 0.3893028050661087, Final Batch Loss: 0.15447120368480682\n",
      "Epoch 1708, Loss: 0.37413734197616577, Final Batch Loss: 0.1756785362958908\n",
      "Epoch 1709, Loss: 0.39938998222351074, Final Batch Loss: 0.19828444719314575\n",
      "Epoch 1710, Loss: 0.3942349851131439, Final Batch Loss: 0.16732601821422577\n",
      "Epoch 1711, Loss: 0.38718774914741516, Final Batch Loss: 0.19532282650470734\n",
      "Epoch 1712, Loss: 0.37029753625392914, Final Batch Loss: 0.19633683562278748\n",
      "Epoch 1713, Loss: 0.39338360726833344, Final Batch Loss: 0.21251645684242249\n",
      "Epoch 1714, Loss: 0.3673144429922104, Final Batch Loss: 0.14495956897735596\n",
      "Epoch 1715, Loss: 0.36998042464256287, Final Batch Loss: 0.19136294722557068\n",
      "Epoch 1716, Loss: 0.4409158229827881, Final Batch Loss: 0.21532662212848663\n",
      "Epoch 1717, Loss: 0.4463060796260834, Final Batch Loss: 0.16192010045051575\n",
      "Epoch 1718, Loss: 0.40434782207012177, Final Batch Loss: 0.20607808232307434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1719, Loss: 0.39175845682621, Final Batch Loss: 0.18133670091629028\n",
      "Epoch 1720, Loss: 0.423332616686821, Final Batch Loss: 0.22664709389209747\n",
      "Epoch 1721, Loss: 0.48878900706768036, Final Batch Loss: 0.2913006544113159\n",
      "Epoch 1722, Loss: 0.46157456934452057, Final Batch Loss: 0.24356786906719208\n",
      "Epoch 1723, Loss: 0.4200047105550766, Final Batch Loss: 0.22916682064533234\n",
      "Epoch 1724, Loss: 0.4316735565662384, Final Batch Loss: 0.2112191617488861\n",
      "Epoch 1725, Loss: 0.40959346294403076, Final Batch Loss: 0.20268484950065613\n",
      "Epoch 1726, Loss: 0.4420183449983597, Final Batch Loss: 0.21328553557395935\n",
      "Epoch 1727, Loss: 0.4118856191635132, Final Batch Loss: 0.20736457407474518\n",
      "Epoch 1728, Loss: 0.4209800064563751, Final Batch Loss: 0.2060537338256836\n",
      "Epoch 1729, Loss: 0.39409127831459045, Final Batch Loss: 0.15608331561088562\n",
      "Epoch 1730, Loss: 0.444414347410202, Final Batch Loss: 0.1772979497909546\n",
      "Epoch 1731, Loss: 0.43027274310588837, Final Batch Loss: 0.2430369257926941\n",
      "Epoch 1732, Loss: 0.4253784567117691, Final Batch Loss: 0.21360091865062714\n",
      "Epoch 1733, Loss: 0.38088376820087433, Final Batch Loss: 0.17459914088249207\n",
      "Epoch 1734, Loss: 0.38114021718502045, Final Batch Loss: 0.18849776685237885\n",
      "Epoch 1735, Loss: 0.408180296421051, Final Batch Loss: 0.17028573155403137\n",
      "Epoch 1736, Loss: 0.46647974848747253, Final Batch Loss: 0.2731102406978607\n",
      "Epoch 1737, Loss: 0.47247467935085297, Final Batch Loss: 0.2696058452129364\n",
      "Epoch 1738, Loss: 0.45365582406520844, Final Batch Loss: 0.2540031373500824\n",
      "Epoch 1739, Loss: 0.36340340971946716, Final Batch Loss: 0.1756383627653122\n",
      "Epoch 1740, Loss: 0.40040475130081177, Final Batch Loss: 0.1847108006477356\n",
      "Epoch 1741, Loss: 0.39084702730178833, Final Batch Loss: 0.19531406462192535\n",
      "Epoch 1742, Loss: 0.38720986247062683, Final Batch Loss: 0.1667793095111847\n",
      "Epoch 1743, Loss: 0.42604461312294006, Final Batch Loss: 0.21837937831878662\n",
      "Epoch 1744, Loss: 0.39593125879764557, Final Batch Loss: 0.2011694461107254\n",
      "Epoch 1745, Loss: 0.3966232091188431, Final Batch Loss: 0.18227346241474152\n",
      "Epoch 1746, Loss: 0.3760824352502823, Final Batch Loss: 0.1394331008195877\n",
      "Epoch 1747, Loss: 0.4474133104085922, Final Batch Loss: 0.24420586228370667\n",
      "Epoch 1748, Loss: 0.38867485523223877, Final Batch Loss: 0.19826145470142365\n",
      "Epoch 1749, Loss: 0.4005671888589859, Final Batch Loss: 0.18739545345306396\n",
      "Epoch 1750, Loss: 0.40537790954113007, Final Batch Loss: 0.1983359456062317\n",
      "Epoch 1751, Loss: 0.43294188380241394, Final Batch Loss: 0.22584456205368042\n",
      "Epoch 1752, Loss: 0.3963201642036438, Final Batch Loss: 0.21567188203334808\n",
      "Epoch 1753, Loss: 0.4118262231349945, Final Batch Loss: 0.18390867114067078\n",
      "Epoch 1754, Loss: 0.4181128591299057, Final Batch Loss: 0.22624847292900085\n",
      "Epoch 1755, Loss: 0.41992810368537903, Final Batch Loss: 0.17041108012199402\n",
      "Epoch 1756, Loss: 0.38863998651504517, Final Batch Loss: 0.17863497138023376\n",
      "Epoch 1757, Loss: 0.40836377441883087, Final Batch Loss: 0.21837195754051208\n",
      "Epoch 1758, Loss: 0.4079012870788574, Final Batch Loss: 0.20176920294761658\n",
      "Epoch 1759, Loss: 0.41748516261577606, Final Batch Loss: 0.19048669934272766\n",
      "Epoch 1760, Loss: 0.4113602191209793, Final Batch Loss: 0.22189076244831085\n",
      "Epoch 1761, Loss: 0.4079359769821167, Final Batch Loss: 0.1972469985485077\n",
      "Epoch 1762, Loss: 0.4144328534603119, Final Batch Loss: 0.20335635542869568\n",
      "Epoch 1763, Loss: 0.4323709160089493, Final Batch Loss: 0.21773886680603027\n",
      "Epoch 1764, Loss: 0.36636319756507874, Final Batch Loss: 0.18125423789024353\n",
      "Epoch 1765, Loss: 0.4243527799844742, Final Batch Loss: 0.25241732597351074\n",
      "Epoch 1766, Loss: 0.3884749859571457, Final Batch Loss: 0.20449912548065186\n",
      "Epoch 1767, Loss: 0.42566633224487305, Final Batch Loss: 0.24233973026275635\n",
      "Epoch 1768, Loss: 0.38862231373786926, Final Batch Loss: 0.18116675317287445\n",
      "Epoch 1769, Loss: 0.40621985495090485, Final Batch Loss: 0.1779697984457016\n",
      "Epoch 1770, Loss: 0.38974006474018097, Final Batch Loss: 0.18264587223529816\n",
      "Epoch 1771, Loss: 0.424532487988472, Final Batch Loss: 0.22494107484817505\n",
      "Epoch 1772, Loss: 0.37595419585704803, Final Batch Loss: 0.20353494584560394\n",
      "Epoch 1773, Loss: 0.37490423023700714, Final Batch Loss: 0.16475753486156464\n",
      "Epoch 1774, Loss: 0.4145081043243408, Final Batch Loss: 0.1971273422241211\n",
      "Epoch 1775, Loss: 0.4049723744392395, Final Batch Loss: 0.19760023057460785\n",
      "Epoch 1776, Loss: 0.4218818098306656, Final Batch Loss: 0.23632578551769257\n",
      "Epoch 1777, Loss: 0.41220977902412415, Final Batch Loss: 0.20812983810901642\n",
      "Epoch 1778, Loss: 0.3951946347951889, Final Batch Loss: 0.17827534675598145\n",
      "Epoch 1779, Loss: 0.36524464190006256, Final Batch Loss: 0.17997410893440247\n",
      "Epoch 1780, Loss: 0.40251488983631134, Final Batch Loss: 0.2019551396369934\n",
      "Epoch 1781, Loss: 0.39590342342853546, Final Batch Loss: 0.20020143687725067\n",
      "Epoch 1782, Loss: 0.45982901751995087, Final Batch Loss: 0.22593531012535095\n",
      "Epoch 1783, Loss: 0.4135490357875824, Final Batch Loss: 0.18958191573619843\n",
      "Epoch 1784, Loss: 0.4842192232608795, Final Batch Loss: 0.2647513747215271\n",
      "Epoch 1785, Loss: 0.4304695576429367, Final Batch Loss: 0.22707314789295197\n",
      "Epoch 1786, Loss: 0.4098852276802063, Final Batch Loss: 0.19530732929706573\n",
      "Epoch 1787, Loss: 0.445502370595932, Final Batch Loss: 0.23915919661521912\n",
      "Epoch 1788, Loss: 0.42899924516677856, Final Batch Loss: 0.22620485723018646\n",
      "Epoch 1789, Loss: 0.3747146427631378, Final Batch Loss: 0.1784295290708542\n",
      "Epoch 1790, Loss: 0.4509669691324234, Final Batch Loss: 0.279217004776001\n",
      "Epoch 1791, Loss: 0.3681696802377701, Final Batch Loss: 0.17626583576202393\n",
      "Epoch 1792, Loss: 0.3941694051027298, Final Batch Loss: 0.20102380216121674\n",
      "Epoch 1793, Loss: 0.4335210621356964, Final Batch Loss: 0.2174900323152542\n",
      "Epoch 1794, Loss: 0.3701886534690857, Final Batch Loss: 0.17135100066661835\n",
      "Epoch 1795, Loss: 0.46446576714515686, Final Batch Loss: 0.24245813488960266\n",
      "Epoch 1796, Loss: 0.37518545985221863, Final Batch Loss: 0.1539170742034912\n",
      "Epoch 1797, Loss: 0.4597807824611664, Final Batch Loss: 0.2436123490333557\n",
      "Epoch 1798, Loss: 0.3708644360303879, Final Batch Loss: 0.18115836381912231\n",
      "Epoch 1799, Loss: 0.3771373778581619, Final Batch Loss: 0.20176543295383453\n",
      "Epoch 1800, Loss: 0.37726882100105286, Final Batch Loss: 0.14271624386310577\n",
      "Epoch 1801, Loss: 0.3900265544652939, Final Batch Loss: 0.1607089340686798\n",
      "Epoch 1802, Loss: 0.4179871082305908, Final Batch Loss: 0.17522138357162476\n",
      "Epoch 1803, Loss: 0.3939947634935379, Final Batch Loss: 0.1887470930814743\n",
      "Epoch 1804, Loss: 0.39326730370521545, Final Batch Loss: 0.22486384212970734\n",
      "Epoch 1805, Loss: 0.3812426030635834, Final Batch Loss: 0.2095952332019806\n",
      "Epoch 1806, Loss: 0.47598229348659515, Final Batch Loss: 0.24069227278232574\n",
      "Epoch 1807, Loss: 0.42775560915470123, Final Batch Loss: 0.20606113970279694\n",
      "Epoch 1808, Loss: 0.42931489646434784, Final Batch Loss: 0.20254787802696228\n",
      "Epoch 1809, Loss: 0.4012364596128464, Final Batch Loss: 0.1565982848405838\n",
      "Epoch 1810, Loss: 0.4351482540369034, Final Batch Loss: 0.2463964819908142\n",
      "Epoch 1811, Loss: 0.4462741017341614, Final Batch Loss: 0.23239657282829285\n",
      "Epoch 1812, Loss: 0.4167623370885849, Final Batch Loss: 0.20441968739032745\n",
      "Epoch 1813, Loss: 0.41543765366077423, Final Batch Loss: 0.2173938751220703\n",
      "Epoch 1814, Loss: 0.3652167171239853, Final Batch Loss: 0.167033389210701\n",
      "Epoch 1815, Loss: 0.4161500185728073, Final Batch Loss: 0.2455655336380005\n",
      "Epoch 1816, Loss: 0.39348573982715607, Final Batch Loss: 0.21103139221668243\n",
      "Epoch 1817, Loss: 0.4272442013025284, Final Batch Loss: 0.22045016288757324\n",
      "Epoch 1818, Loss: 0.4053575098514557, Final Batch Loss: 0.16901081800460815\n",
      "Epoch 1819, Loss: 0.4224929064512253, Final Batch Loss: 0.1833125203847885\n",
      "Epoch 1820, Loss: 0.3759659677743912, Final Batch Loss: 0.17996717989444733\n",
      "Epoch 1821, Loss: 0.4217785447835922, Final Batch Loss: 0.24564462900161743\n",
      "Epoch 1822, Loss: 0.3962462991476059, Final Batch Loss: 0.2126525193452835\n",
      "Epoch 1823, Loss: 0.3868837058544159, Final Batch Loss: 0.1802842617034912\n",
      "Epoch 1824, Loss: 0.36652347445487976, Final Batch Loss: 0.1663350909948349\n",
      "Epoch 1825, Loss: 0.3348432779312134, Final Batch Loss: 0.1282234638929367\n",
      "Epoch 1826, Loss: 0.4215032160282135, Final Batch Loss: 0.20418889820575714\n",
      "Epoch 1827, Loss: 0.4280641973018646, Final Batch Loss: 0.21983109414577484\n",
      "Epoch 1828, Loss: 0.4048350155353546, Final Batch Loss: 0.18665483593940735\n",
      "Epoch 1829, Loss: 0.39883194863796234, Final Batch Loss: 0.12296022474765778\n",
      "Epoch 1830, Loss: 0.37075774371623993, Final Batch Loss: 0.13403743505477905\n",
      "Epoch 1831, Loss: 0.38695158064365387, Final Batch Loss: 0.21752823889255524\n",
      "Epoch 1832, Loss: 0.3423042744398117, Final Batch Loss: 0.14772765338420868\n",
      "Epoch 1833, Loss: 0.41329605877399445, Final Batch Loss: 0.22599922120571136\n",
      "Epoch 1834, Loss: 0.38427311182022095, Final Batch Loss: 0.19453319907188416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1835, Loss: 0.36587749421596527, Final Batch Loss: 0.19519415497779846\n",
      "Epoch 1836, Loss: 0.417455717921257, Final Batch Loss: 0.21142280101776123\n",
      "Epoch 1837, Loss: 0.39333267509937286, Final Batch Loss: 0.21041513979434967\n",
      "Epoch 1838, Loss: 0.4001989811658859, Final Batch Loss: 0.2298736870288849\n",
      "Epoch 1839, Loss: 0.4157457649707794, Final Batch Loss: 0.25049635767936707\n",
      "Epoch 1840, Loss: 0.4455750733613968, Final Batch Loss: 0.22254478931427002\n",
      "Epoch 1841, Loss: 0.4028429836034775, Final Batch Loss: 0.21422721445560455\n",
      "Epoch 1842, Loss: 0.4448816180229187, Final Batch Loss: 0.2588270604610443\n",
      "Epoch 1843, Loss: 0.3936845362186432, Final Batch Loss: 0.18868833780288696\n",
      "Epoch 1844, Loss: 0.3808010220527649, Final Batch Loss: 0.16325274109840393\n",
      "Epoch 1845, Loss: 0.3626323491334915, Final Batch Loss: 0.17174208164215088\n",
      "Epoch 1846, Loss: 0.3560233861207962, Final Batch Loss: 0.17849673330783844\n",
      "Epoch 1847, Loss: 0.37809984385967255, Final Batch Loss: 0.17943114042282104\n",
      "Epoch 1848, Loss: 0.48295269906520844, Final Batch Loss: 0.25272881984710693\n",
      "Epoch 1849, Loss: 0.3459954410791397, Final Batch Loss: 0.16186018288135529\n",
      "Epoch 1850, Loss: 0.4280952662229538, Final Batch Loss: 0.22801631689071655\n",
      "Epoch 1851, Loss: 0.35607610642910004, Final Batch Loss: 0.1881454735994339\n",
      "Epoch 1852, Loss: 0.3734142631292343, Final Batch Loss: 0.13451574742794037\n",
      "Epoch 1853, Loss: 0.39006808400154114, Final Batch Loss: 0.17706243693828583\n",
      "Epoch 1854, Loss: 0.41328027844429016, Final Batch Loss: 0.17630720138549805\n",
      "Epoch 1855, Loss: 0.396908164024353, Final Batch Loss: 0.206931009888649\n",
      "Epoch 1856, Loss: 0.37562114000320435, Final Batch Loss: 0.15219584107398987\n",
      "Epoch 1857, Loss: 0.4526277929544449, Final Batch Loss: 0.2704663872718811\n",
      "Epoch 1858, Loss: 0.38244834542274475, Final Batch Loss: 0.18448810279369354\n",
      "Epoch 1859, Loss: 0.48398028314113617, Final Batch Loss: 0.2661736309528351\n",
      "Epoch 1860, Loss: 0.4194357693195343, Final Batch Loss: 0.2227610945701599\n",
      "Epoch 1861, Loss: 0.42822712659835815, Final Batch Loss: 0.2753262519836426\n",
      "Epoch 1862, Loss: 0.41602690517902374, Final Batch Loss: 0.2166071981191635\n",
      "Epoch 1863, Loss: 0.35814622044563293, Final Batch Loss: 0.18121685087680817\n",
      "Epoch 1864, Loss: 0.3846392184495926, Final Batch Loss: 0.13388888537883759\n",
      "Epoch 1865, Loss: 0.3714538812637329, Final Batch Loss: 0.17956003546714783\n",
      "Epoch 1866, Loss: 0.38412120938301086, Final Batch Loss: 0.17098499834537506\n",
      "Epoch 1867, Loss: 0.38993170857429504, Final Batch Loss: 0.21427476406097412\n",
      "Epoch 1868, Loss: 0.3984583467245102, Final Batch Loss: 0.2077910304069519\n",
      "Epoch 1869, Loss: 0.4019903540611267, Final Batch Loss: 0.20107699930667877\n",
      "Epoch 1870, Loss: 0.4209984391927719, Final Batch Loss: 0.1813659518957138\n",
      "Epoch 1871, Loss: 0.393467515707016, Final Batch Loss: 0.17439210414886475\n",
      "Epoch 1872, Loss: 0.38299718499183655, Final Batch Loss: 0.13747143745422363\n",
      "Epoch 1873, Loss: 0.40302249789237976, Final Batch Loss: 0.21429674327373505\n",
      "Epoch 1874, Loss: 0.3665139526128769, Final Batch Loss: 0.1659865528345108\n",
      "Epoch 1875, Loss: 0.4240873456001282, Final Batch Loss: 0.21197840571403503\n",
      "Epoch 1876, Loss: 0.39092645049095154, Final Batch Loss: 0.18853385746479034\n",
      "Epoch 1877, Loss: 0.356208398938179, Final Batch Loss: 0.16363349556922913\n",
      "Epoch 1878, Loss: 0.39645373821258545, Final Batch Loss: 0.22605104744434357\n",
      "Epoch 1879, Loss: 0.4314500242471695, Final Batch Loss: 0.19909271597862244\n",
      "Epoch 1880, Loss: 0.3731653541326523, Final Batch Loss: 0.1947576105594635\n",
      "Epoch 1881, Loss: 0.37815406918525696, Final Batch Loss: 0.19183862209320068\n",
      "Epoch 1882, Loss: 0.3703031539916992, Final Batch Loss: 0.15217487514019012\n",
      "Epoch 1883, Loss: 0.4259551018476486, Final Batch Loss: 0.2104424387216568\n",
      "Epoch 1884, Loss: 0.41089464724063873, Final Batch Loss: 0.20439974963665009\n",
      "Epoch 1885, Loss: 0.3992246240377426, Final Batch Loss: 0.19779187440872192\n",
      "Epoch 1886, Loss: 0.3797723799943924, Final Batch Loss: 0.16512496769428253\n",
      "Epoch 1887, Loss: 0.3955218642950058, Final Batch Loss: 0.19128316640853882\n",
      "Epoch 1888, Loss: 0.3863401859998703, Final Batch Loss: 0.206960529088974\n",
      "Epoch 1889, Loss: 0.44041819870471954, Final Batch Loss: 0.23132988810539246\n",
      "Epoch 1890, Loss: 0.43767790496349335, Final Batch Loss: 0.23247268795967102\n",
      "Epoch 1891, Loss: 0.36483024060726166, Final Batch Loss: 0.16069762408733368\n",
      "Epoch 1892, Loss: 0.46636341512203217, Final Batch Loss: 0.28958436846733093\n",
      "Epoch 1893, Loss: 0.39955808222293854, Final Batch Loss: 0.18832235038280487\n",
      "Epoch 1894, Loss: 0.36169807612895966, Final Batch Loss: 0.15774381160736084\n",
      "Epoch 1895, Loss: 0.40586070716381073, Final Batch Loss: 0.18127691745758057\n",
      "Epoch 1896, Loss: 0.4409911781549454, Final Batch Loss: 0.24414020776748657\n",
      "Epoch 1897, Loss: 0.445125013589859, Final Batch Loss: 0.20974044501781464\n",
      "Epoch 1898, Loss: 0.34649938344955444, Final Batch Loss: 0.16103535890579224\n",
      "Epoch 1899, Loss: 0.3968479782342911, Final Batch Loss: 0.2109760046005249\n",
      "Epoch 1900, Loss: 0.4278615564107895, Final Batch Loss: 0.1720009297132492\n",
      "Epoch 1901, Loss: 0.38416624069213867, Final Batch Loss: 0.18279212713241577\n",
      "Epoch 1902, Loss: 0.34649473428726196, Final Batch Loss: 0.14539983868598938\n",
      "Epoch 1903, Loss: 0.4021214246749878, Final Batch Loss: 0.20859447121620178\n",
      "Epoch 1904, Loss: 0.47415097057819366, Final Batch Loss: 0.2634737491607666\n",
      "Epoch 1905, Loss: 0.40689265727996826, Final Batch Loss: 0.21352091431617737\n",
      "Epoch 1906, Loss: 0.42465533316135406, Final Batch Loss: 0.24055466055870056\n",
      "Epoch 1907, Loss: 0.44117386639118195, Final Batch Loss: 0.21049855649471283\n",
      "Epoch 1908, Loss: 0.3960595726966858, Final Batch Loss: 0.22775912284851074\n",
      "Epoch 1909, Loss: 0.3943169564008713, Final Batch Loss: 0.18964359164237976\n",
      "Epoch 1910, Loss: 0.3501538038253784, Final Batch Loss: 0.18705640733242035\n",
      "Epoch 1911, Loss: 0.38146884739398956, Final Batch Loss: 0.1856035590171814\n",
      "Epoch 1912, Loss: 0.3815007209777832, Final Batch Loss: 0.17321562767028809\n",
      "Epoch 1913, Loss: 0.3688340336084366, Final Batch Loss: 0.1951315551996231\n",
      "Epoch 1914, Loss: 0.3739720731973648, Final Batch Loss: 0.16665741801261902\n",
      "Epoch 1915, Loss: 0.45123836398124695, Final Batch Loss: 0.261621356010437\n",
      "Epoch 1916, Loss: 0.3530595898628235, Final Batch Loss: 0.17213353514671326\n",
      "Epoch 1917, Loss: 0.3747671842575073, Final Batch Loss: 0.18141727149486542\n",
      "Epoch 1918, Loss: 0.38621382415294647, Final Batch Loss: 0.18656565248966217\n",
      "Epoch 1919, Loss: 0.3718049079179764, Final Batch Loss: 0.19543974101543427\n",
      "Epoch 1920, Loss: 0.38891568779945374, Final Batch Loss: 0.21494582295417786\n",
      "Epoch 1921, Loss: 0.4234209954738617, Final Batch Loss: 0.21852877736091614\n",
      "Epoch 1922, Loss: 0.4086580127477646, Final Batch Loss: 0.17522045969963074\n",
      "Epoch 1923, Loss: 0.3513600379228592, Final Batch Loss: 0.17830008268356323\n",
      "Epoch 1924, Loss: 0.380365327000618, Final Batch Loss: 0.2151120901107788\n",
      "Epoch 1925, Loss: 0.43028780817985535, Final Batch Loss: 0.2529706656932831\n",
      "Epoch 1926, Loss: 0.4030255824327469, Final Batch Loss: 0.20156432688236237\n",
      "Epoch 1927, Loss: 0.356427326798439, Final Batch Loss: 0.1785956174135208\n",
      "Epoch 1928, Loss: 0.4117536097764969, Final Batch Loss: 0.22777388989925385\n",
      "Epoch 1929, Loss: 0.4007983058691025, Final Batch Loss: 0.22153057157993317\n",
      "Epoch 1930, Loss: 0.40295834839344025, Final Batch Loss: 0.19346962869167328\n",
      "Epoch 1931, Loss: 0.3694203644990921, Final Batch Loss: 0.14952416718006134\n",
      "Epoch 1932, Loss: 0.39127466082572937, Final Batch Loss: 0.21528886258602142\n",
      "Epoch 1933, Loss: 0.3821825683116913, Final Batch Loss: 0.17886830866336823\n",
      "Epoch 1934, Loss: 0.37759561836719513, Final Batch Loss: 0.16696280241012573\n",
      "Epoch 1935, Loss: 0.49524590373039246, Final Batch Loss: 0.24735283851623535\n",
      "Epoch 1936, Loss: 0.3672179877758026, Final Batch Loss: 0.1554509699344635\n",
      "Epoch 1937, Loss: 0.4294392317533493, Final Batch Loss: 0.21058697998523712\n",
      "Epoch 1938, Loss: 0.43340981006622314, Final Batch Loss: 0.2425934374332428\n",
      "Epoch 1939, Loss: 0.4005027860403061, Final Batch Loss: 0.1849111020565033\n",
      "Epoch 1940, Loss: 0.3904536962509155, Final Batch Loss: 0.16835227608680725\n",
      "Epoch 1941, Loss: 0.3838035464286804, Final Batch Loss: 0.1801227331161499\n",
      "Epoch 1942, Loss: 0.3744950592517853, Final Batch Loss: 0.1748376190662384\n",
      "Epoch 1943, Loss: 0.5215271413326263, Final Batch Loss: 0.2463037669658661\n",
      "Epoch 1944, Loss: 0.3890724927186966, Final Batch Loss: 0.1859731823205948\n",
      "Epoch 1945, Loss: 0.3915589451789856, Final Batch Loss: 0.1982758492231369\n",
      "Epoch 1946, Loss: 0.3781129866838455, Final Batch Loss: 0.1593201905488968\n",
      "Epoch 1947, Loss: 0.4076351970434189, Final Batch Loss: 0.19179344177246094\n",
      "Epoch 1948, Loss: 0.40259967744350433, Final Batch Loss: 0.23145902156829834\n",
      "Epoch 1949, Loss: 0.38659416139125824, Final Batch Loss: 0.20844432711601257\n",
      "Epoch 1950, Loss: 0.3982883095741272, Final Batch Loss: 0.194085031747818\n",
      "Epoch 1951, Loss: 0.4234054237604141, Final Batch Loss: 0.2252141237258911\n",
      "Epoch 1952, Loss: 0.44970281422138214, Final Batch Loss: 0.2058253139257431\n",
      "Epoch 1953, Loss: 0.4521256238222122, Final Batch Loss: 0.22060993313789368\n",
      "Epoch 1954, Loss: 0.39095522463321686, Final Batch Loss: 0.22151078283786774\n",
      "Epoch 1955, Loss: 0.4514399468898773, Final Batch Loss: 0.27232471108436584\n",
      "Epoch 1956, Loss: 0.35333535075187683, Final Batch Loss: 0.15420812368392944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1957, Loss: 0.41081997752189636, Final Batch Loss: 0.18157896399497986\n",
      "Epoch 1958, Loss: 0.4148724526166916, Final Batch Loss: 0.1922364979982376\n",
      "Epoch 1959, Loss: 0.36023351550102234, Final Batch Loss: 0.1778089851140976\n",
      "Epoch 1960, Loss: 0.3863231986761093, Final Batch Loss: 0.19524353742599487\n",
      "Epoch 1961, Loss: 0.3689892441034317, Final Batch Loss: 0.18587850034236908\n",
      "Epoch 1962, Loss: 0.3526372164487839, Final Batch Loss: 0.18358545005321503\n",
      "Epoch 1963, Loss: 0.41251736879348755, Final Batch Loss: 0.20769968628883362\n",
      "Epoch 1964, Loss: 0.38201549649238586, Final Batch Loss: 0.15649108588695526\n",
      "Epoch 1965, Loss: 0.43234682083129883, Final Batch Loss: 0.19172249734401703\n",
      "Epoch 1966, Loss: 0.3538935035467148, Final Batch Loss: 0.17286285758018494\n",
      "Epoch 1967, Loss: 0.4228839874267578, Final Batch Loss: 0.23232464492321014\n",
      "Epoch 1968, Loss: 0.4635273665189743, Final Batch Loss: 0.26538899540901184\n",
      "Epoch 1969, Loss: 0.34369732439517975, Final Batch Loss: 0.13508367538452148\n",
      "Epoch 1970, Loss: 0.381758451461792, Final Batch Loss: 0.18071861565113068\n",
      "Epoch 1971, Loss: 0.38470660150051117, Final Batch Loss: 0.22549396753311157\n",
      "Epoch 1972, Loss: 0.4559040814638138, Final Batch Loss: 0.26076993346214294\n",
      "Epoch 1973, Loss: 0.37254729866981506, Final Batch Loss: 0.16707776486873627\n",
      "Epoch 1974, Loss: 0.35301588475704193, Final Batch Loss: 0.1510096788406372\n",
      "Epoch 1975, Loss: 0.34536875784397125, Final Batch Loss: 0.14333197474479675\n",
      "Epoch 1976, Loss: 0.37290695309638977, Final Batch Loss: 0.18482153117656708\n",
      "Epoch 1977, Loss: 0.3671371042728424, Final Batch Loss: 0.18245545029640198\n",
      "Epoch 1978, Loss: 0.3634887635707855, Final Batch Loss: 0.17345596849918365\n",
      "Epoch 1979, Loss: 0.3587292730808258, Final Batch Loss: 0.17073649168014526\n",
      "Epoch 1980, Loss: 0.3804272413253784, Final Batch Loss: 0.16841228306293488\n",
      "Epoch 1981, Loss: 0.38738954067230225, Final Batch Loss: 0.2031090408563614\n",
      "Epoch 1982, Loss: 0.3953443765640259, Final Batch Loss: 0.18602930009365082\n",
      "Epoch 1983, Loss: 0.40087781846523285, Final Batch Loss: 0.20562304556369781\n",
      "Epoch 1984, Loss: 0.33833035826683044, Final Batch Loss: 0.1460198163986206\n",
      "Epoch 1985, Loss: 0.39294934272766113, Final Batch Loss: 0.22644561529159546\n",
      "Epoch 1986, Loss: 0.3855075091123581, Final Batch Loss: 0.1982894241809845\n",
      "Epoch 1987, Loss: 0.4039919972419739, Final Batch Loss: 0.21482062339782715\n",
      "Epoch 1988, Loss: 0.4400552660226822, Final Batch Loss: 0.2588091790676117\n",
      "Epoch 1989, Loss: 0.3825937658548355, Final Batch Loss: 0.1605890840291977\n",
      "Epoch 1990, Loss: 0.3857252299785614, Final Batch Loss: 0.19070753455162048\n",
      "Epoch 1991, Loss: 0.4293525516986847, Final Batch Loss: 0.2169777750968933\n",
      "Epoch 1992, Loss: 0.4009214788675308, Final Batch Loss: 0.2064967155456543\n",
      "Epoch 1993, Loss: 0.39173971116542816, Final Batch Loss: 0.17642898857593536\n",
      "Epoch 1994, Loss: 0.3876456916332245, Final Batch Loss: 0.1869755983352661\n",
      "Epoch 1995, Loss: 0.4035392105579376, Final Batch Loss: 0.21824093163013458\n",
      "Epoch 1996, Loss: 0.3942268043756485, Final Batch Loss: 0.20121382176876068\n",
      "Epoch 1997, Loss: 0.3968656659126282, Final Batch Loss: 0.22308292984962463\n",
      "Epoch 1998, Loss: 0.39787457883358, Final Batch Loss: 0.20049184560775757\n",
      "Epoch 1999, Loss: 0.4274446815252304, Final Batch Loss: 0.23365838825702667\n",
      "Epoch 2000, Loss: 0.3947732001543045, Final Batch Loss: 0.24124208092689514\n",
      "Epoch 2001, Loss: 0.4309569001197815, Final Batch Loss: 0.2156972587108612\n",
      "Epoch 2002, Loss: 0.4026302248239517, Final Batch Loss: 0.2199838012456894\n",
      "Epoch 2003, Loss: 0.4761609733104706, Final Batch Loss: 0.25792306661605835\n",
      "Epoch 2004, Loss: 0.40232476592063904, Final Batch Loss: 0.21464408934116364\n",
      "Epoch 2005, Loss: 0.36984358727931976, Final Batch Loss: 0.17057736217975616\n",
      "Epoch 2006, Loss: 0.39071232080459595, Final Batch Loss: 0.19129937887191772\n",
      "Epoch 2007, Loss: 0.35898810625076294, Final Batch Loss: 0.17407643795013428\n",
      "Epoch 2008, Loss: 0.40821656584739685, Final Batch Loss: 0.21675142645835876\n",
      "Epoch 2009, Loss: 0.4163557440042496, Final Batch Loss: 0.20727825164794922\n",
      "Epoch 2010, Loss: 0.3819531947374344, Final Batch Loss: 0.15988750755786896\n",
      "Epoch 2011, Loss: 0.35182464122772217, Final Batch Loss: 0.15252861380577087\n",
      "Epoch 2012, Loss: 0.36113257706165314, Final Batch Loss: 0.1942102015018463\n",
      "Epoch 2013, Loss: 0.40498898923397064, Final Batch Loss: 0.24418388307094574\n",
      "Epoch 2014, Loss: 0.41458114981651306, Final Batch Loss: 0.2258368283510208\n",
      "Epoch 2015, Loss: 0.36035600304603577, Final Batch Loss: 0.19156000018119812\n",
      "Epoch 2016, Loss: 0.37379537522792816, Final Batch Loss: 0.13671401143074036\n",
      "Epoch 2017, Loss: 0.37321504950523376, Final Batch Loss: 0.13727283477783203\n",
      "Epoch 2018, Loss: 0.43620528280735016, Final Batch Loss: 0.2331615686416626\n",
      "Epoch 2019, Loss: 0.3650370091199875, Final Batch Loss: 0.16582944989204407\n",
      "Epoch 2020, Loss: 0.3816617578268051, Final Batch Loss: 0.203195258975029\n",
      "Epoch 2021, Loss: 0.36385710537433624, Final Batch Loss: 0.17073164880275726\n",
      "Epoch 2022, Loss: 0.404403880238533, Final Batch Loss: 0.1855550855398178\n",
      "Epoch 2023, Loss: 0.43705806136131287, Final Batch Loss: 0.26726484298706055\n",
      "Epoch 2024, Loss: 0.356902077794075, Final Batch Loss: 0.1628982126712799\n",
      "Epoch 2025, Loss: 0.3895977586507797, Final Batch Loss: 0.21215985715389252\n",
      "Epoch 2026, Loss: 0.4045437425374985, Final Batch Loss: 0.21900592744350433\n",
      "Epoch 2027, Loss: 0.36970992386341095, Final Batch Loss: 0.1672656238079071\n",
      "Epoch 2028, Loss: 0.48933233320713043, Final Batch Loss: 0.30582377314567566\n",
      "Epoch 2029, Loss: 0.45588892698287964, Final Batch Loss: 0.2499075084924698\n",
      "Epoch 2030, Loss: 0.42468351125717163, Final Batch Loss: 0.22675879299640656\n",
      "Epoch 2031, Loss: 0.3927402049303055, Final Batch Loss: 0.21659766137599945\n",
      "Epoch 2032, Loss: 0.4133826941251755, Final Batch Loss: 0.2112840712070465\n",
      "Epoch 2033, Loss: 0.42048561573028564, Final Batch Loss: 0.21459293365478516\n",
      "Epoch 2034, Loss: 0.4105248898267746, Final Batch Loss: 0.21846997737884521\n",
      "Epoch 2035, Loss: 0.3825705647468567, Final Batch Loss: 0.17488062381744385\n",
      "Epoch 2036, Loss: 0.3803824186325073, Final Batch Loss: 0.2094087302684784\n",
      "Epoch 2037, Loss: 0.42305801808834076, Final Batch Loss: 0.2050485610961914\n",
      "Epoch 2038, Loss: 0.38433967530727386, Final Batch Loss: 0.20676597952842712\n",
      "Epoch 2039, Loss: 0.4480346739292145, Final Batch Loss: 0.239433154463768\n",
      "Epoch 2040, Loss: 0.38662089407444, Final Batch Loss: 0.16411243379116058\n",
      "Epoch 2041, Loss: 0.3959445357322693, Final Batch Loss: 0.16431115567684174\n",
      "Epoch 2042, Loss: 0.3643319606781006, Final Batch Loss: 0.17784404754638672\n",
      "Epoch 2043, Loss: 0.4036208242177963, Final Batch Loss: 0.1908074915409088\n",
      "Epoch 2044, Loss: 0.3891662061214447, Final Batch Loss: 0.19102203845977783\n",
      "Epoch 2045, Loss: 0.3851746618747711, Final Batch Loss: 0.1662176549434662\n",
      "Epoch 2046, Loss: 0.38170845806598663, Final Batch Loss: 0.23092293739318848\n",
      "Epoch 2047, Loss: 0.4091539531946182, Final Batch Loss: 0.20889443159103394\n",
      "Epoch 2048, Loss: 0.41338662803173065, Final Batch Loss: 0.18055035173892975\n",
      "Epoch 2049, Loss: 0.4029088467359543, Final Batch Loss: 0.234903484582901\n",
      "Epoch 2050, Loss: 0.4144112914800644, Final Batch Loss: 0.21027803421020508\n",
      "Epoch 2051, Loss: 0.40284349024295807, Final Batch Loss: 0.18738844990730286\n",
      "Epoch 2052, Loss: 0.35922037065029144, Final Batch Loss: 0.17177677154541016\n",
      "Epoch 2053, Loss: 0.42950278520584106, Final Batch Loss: 0.2514877915382385\n",
      "Epoch 2054, Loss: 0.3996172249317169, Final Batch Loss: 0.2158784717321396\n",
      "Epoch 2055, Loss: 0.33943063020706177, Final Batch Loss: 0.1655019223690033\n",
      "Epoch 2056, Loss: 0.35577327013015747, Final Batch Loss: 0.1714138239622116\n",
      "Epoch 2057, Loss: 0.37142542004585266, Final Batch Loss: 0.20125815272331238\n",
      "Epoch 2058, Loss: 0.4092114716768265, Final Batch Loss: 0.23782488703727722\n",
      "Epoch 2059, Loss: 0.3525976091623306, Final Batch Loss: 0.16668294370174408\n",
      "Epoch 2060, Loss: 0.36063800752162933, Final Batch Loss: 0.20133163034915924\n",
      "Epoch 2061, Loss: 0.37732623517513275, Final Batch Loss: 0.18741261959075928\n",
      "Epoch 2062, Loss: 0.4402801841497421, Final Batch Loss: 0.21444183588027954\n",
      "Epoch 2063, Loss: 0.3442104607820511, Final Batch Loss: 0.1486317366361618\n",
      "Epoch 2064, Loss: 0.4125194251537323, Final Batch Loss: 0.16860000789165497\n",
      "Epoch 2065, Loss: 0.45759907364845276, Final Batch Loss: 0.2199307382106781\n",
      "Epoch 2066, Loss: 0.4382389336824417, Final Batch Loss: 0.20685948431491852\n",
      "Epoch 2067, Loss: 0.32474856078624725, Final Batch Loss: 0.12309777736663818\n",
      "Epoch 2068, Loss: 0.4306378960609436, Final Batch Loss: 0.22089634835720062\n",
      "Epoch 2069, Loss: 0.3852817714214325, Final Batch Loss: 0.21232636272907257\n",
      "Epoch 2070, Loss: 0.3947616070508957, Final Batch Loss: 0.21615734696388245\n",
      "Epoch 2071, Loss: 0.3718768507242203, Final Batch Loss: 0.19020915031433105\n",
      "Epoch 2072, Loss: 0.3624354153871536, Final Batch Loss: 0.1853305846452713\n",
      "Epoch 2073, Loss: 0.4859846532344818, Final Batch Loss: 0.2896650731563568\n",
      "Epoch 2074, Loss: 0.36488670110702515, Final Batch Loss: 0.14249159395694733\n",
      "Epoch 2075, Loss: 0.35692155361175537, Final Batch Loss: 0.18310022354125977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2076, Loss: 0.35310132801532745, Final Batch Loss: 0.1789718121290207\n",
      "Epoch 2077, Loss: 0.4082156866788864, Final Batch Loss: 0.19122371077537537\n",
      "Epoch 2078, Loss: 0.4137539118528366, Final Batch Loss: 0.14353637397289276\n",
      "Epoch 2079, Loss: 0.3404557555913925, Final Batch Loss: 0.1711728423833847\n",
      "Epoch 2080, Loss: 0.34497761726379395, Final Batch Loss: 0.15672466158866882\n",
      "Epoch 2081, Loss: 0.3728652000427246, Final Batch Loss: 0.1910611391067505\n",
      "Epoch 2082, Loss: 0.40311771631240845, Final Batch Loss: 0.16724547743797302\n",
      "Epoch 2083, Loss: 0.4008585065603256, Final Batch Loss: 0.2042614221572876\n",
      "Epoch 2084, Loss: 0.3717174381017685, Final Batch Loss: 0.2120371013879776\n",
      "Epoch 2085, Loss: 0.4050038456916809, Final Batch Loss: 0.22212029993534088\n",
      "Epoch 2086, Loss: 0.40253573656082153, Final Batch Loss: 0.22694075107574463\n",
      "Epoch 2087, Loss: 0.3933544009923935, Final Batch Loss: 0.20164453983306885\n",
      "Epoch 2088, Loss: 0.36395055055618286, Final Batch Loss: 0.17517079412937164\n",
      "Epoch 2089, Loss: 0.3685534745454788, Final Batch Loss: 0.1987118273973465\n",
      "Epoch 2090, Loss: 0.39242690801620483, Final Batch Loss: 0.1866903007030487\n",
      "Epoch 2091, Loss: 0.3955419659614563, Final Batch Loss: 0.19779373705387115\n",
      "Epoch 2092, Loss: 0.39009013772010803, Final Batch Loss: 0.1867271214723587\n",
      "Epoch 2093, Loss: 0.44210468232631683, Final Batch Loss: 0.23814737796783447\n",
      "Epoch 2094, Loss: 0.4751480668783188, Final Batch Loss: 0.2854501008987427\n",
      "Epoch 2095, Loss: 0.35876041650772095, Final Batch Loss: 0.14502635598182678\n",
      "Epoch 2096, Loss: 0.36273428797721863, Final Batch Loss: 0.18400263786315918\n",
      "Epoch 2097, Loss: 0.38112038373947144, Final Batch Loss: 0.18153376877307892\n",
      "Epoch 2098, Loss: 0.39965422451496124, Final Batch Loss: 0.18993926048278809\n",
      "Epoch 2099, Loss: 0.353553369641304, Final Batch Loss: 0.18051286041736603\n",
      "Epoch 2100, Loss: 0.3648694157600403, Final Batch Loss: 0.1478908210992813\n",
      "Epoch 2101, Loss: 0.3393245339393616, Final Batch Loss: 0.18928329646587372\n",
      "Epoch 2102, Loss: 0.35331615805625916, Final Batch Loss: 0.17775510251522064\n",
      "Epoch 2103, Loss: 0.3603544980287552, Final Batch Loss: 0.19239716231822968\n",
      "Epoch 2104, Loss: 0.3554655909538269, Final Batch Loss: 0.15749236941337585\n",
      "Epoch 2105, Loss: 0.357390433549881, Final Batch Loss: 0.1706109493970871\n",
      "Epoch 2106, Loss: 0.3624768406152725, Final Batch Loss: 0.18117520213127136\n",
      "Epoch 2107, Loss: 0.40345990657806396, Final Batch Loss: 0.19605562090873718\n",
      "Epoch 2108, Loss: 0.398360475897789, Final Batch Loss: 0.22575850784778595\n",
      "Epoch 2109, Loss: 0.3927899897098541, Final Batch Loss: 0.18410621583461761\n",
      "Epoch 2110, Loss: 0.492854967713356, Final Batch Loss: 0.26996999979019165\n",
      "Epoch 2111, Loss: 0.38486020267009735, Final Batch Loss: 0.19597108662128448\n",
      "Epoch 2112, Loss: 0.3722306042909622, Final Batch Loss: 0.2151661068201065\n",
      "Epoch 2113, Loss: 0.3609236627817154, Final Batch Loss: 0.12133076786994934\n",
      "Epoch 2114, Loss: 0.3752242624759674, Final Batch Loss: 0.1867641806602478\n",
      "Epoch 2115, Loss: 0.36424875259399414, Final Batch Loss: 0.19563747942447662\n",
      "Epoch 2116, Loss: 0.4083198308944702, Final Batch Loss: 0.2239835113286972\n",
      "Epoch 2117, Loss: 0.36231985688209534, Final Batch Loss: 0.1841149628162384\n",
      "Epoch 2118, Loss: 0.3565881550312042, Final Batch Loss: 0.19253796339035034\n",
      "Epoch 2119, Loss: 0.35882727801799774, Final Batch Loss: 0.18779419362545013\n",
      "Epoch 2120, Loss: 0.38951441645622253, Final Batch Loss: 0.20060881972312927\n",
      "Epoch 2121, Loss: 0.4103738069534302, Final Batch Loss: 0.18319913744926453\n",
      "Epoch 2122, Loss: 0.3877793997526169, Final Batch Loss: 0.17842701077461243\n",
      "Epoch 2123, Loss: 0.323999747633934, Final Batch Loss: 0.14027255773544312\n",
      "Epoch 2124, Loss: 0.356631800532341, Final Batch Loss: 0.146736741065979\n",
      "Epoch 2125, Loss: 0.3545798063278198, Final Batch Loss: 0.17397046089172363\n",
      "Epoch 2126, Loss: 0.3310629725456238, Final Batch Loss: 0.16621512174606323\n",
      "Epoch 2127, Loss: 0.38072875142097473, Final Batch Loss: 0.18568861484527588\n",
      "Epoch 2128, Loss: 0.37734611332416534, Final Batch Loss: 0.1854844093322754\n",
      "Epoch 2129, Loss: 0.35991717875003815, Final Batch Loss: 0.17255185544490814\n",
      "Epoch 2130, Loss: 0.3512221425771713, Final Batch Loss: 0.150877445936203\n",
      "Epoch 2131, Loss: 0.371572345495224, Final Batch Loss: 0.1618162989616394\n",
      "Epoch 2132, Loss: 0.3584519773721695, Final Batch Loss: 0.17885713279247284\n",
      "Epoch 2133, Loss: 0.37507590651512146, Final Batch Loss: 0.17168371379375458\n",
      "Epoch 2134, Loss: 0.366626113653183, Final Batch Loss: 0.18066179752349854\n",
      "Epoch 2135, Loss: 0.38563138246536255, Final Batch Loss: 0.18398885428905487\n",
      "Epoch 2136, Loss: 0.3376632183790207, Final Batch Loss: 0.1869884729385376\n",
      "Epoch 2137, Loss: 0.4114103764295578, Final Batch Loss: 0.20754197239875793\n",
      "Epoch 2138, Loss: 0.4203159213066101, Final Batch Loss: 0.2305363565683365\n",
      "Epoch 2139, Loss: 0.38164572417736053, Final Batch Loss: 0.18969176709651947\n",
      "Epoch 2140, Loss: 0.38346758484840393, Final Batch Loss: 0.18438836932182312\n",
      "Epoch 2141, Loss: 0.3988868445158005, Final Batch Loss: 0.18086425960063934\n",
      "Epoch 2142, Loss: 0.39651085436344147, Final Batch Loss: 0.21949729323387146\n",
      "Epoch 2143, Loss: 0.3708103448152542, Final Batch Loss: 0.2004844695329666\n",
      "Epoch 2144, Loss: 0.4192066341638565, Final Batch Loss: 0.1672975867986679\n",
      "Epoch 2145, Loss: 0.3355146646499634, Final Batch Loss: 0.14755795896053314\n",
      "Epoch 2146, Loss: 0.3416074216365814, Final Batch Loss: 0.1725281924009323\n",
      "Epoch 2147, Loss: 0.38767147064208984, Final Batch Loss: 0.18624620139598846\n",
      "Epoch 2148, Loss: 0.36561693251132965, Final Batch Loss: 0.19206178188323975\n",
      "Epoch 2149, Loss: 0.39518535137176514, Final Batch Loss: 0.21547813713550568\n",
      "Epoch 2150, Loss: 0.41993050277233124, Final Batch Loss: 0.21743352711200714\n",
      "Epoch 2151, Loss: 0.3497401028871536, Final Batch Loss: 0.1713370680809021\n",
      "Epoch 2152, Loss: 0.3879192918539047, Final Batch Loss: 0.18854181468486786\n",
      "Epoch 2153, Loss: 0.4220171570777893, Final Batch Loss: 0.23026010394096375\n",
      "Epoch 2154, Loss: 0.342267781496048, Final Batch Loss: 0.16761481761932373\n",
      "Epoch 2155, Loss: 0.3905079513788223, Final Batch Loss: 0.1880313903093338\n",
      "Epoch 2156, Loss: 0.4213257282972336, Final Batch Loss: 0.23010918498039246\n",
      "Epoch 2157, Loss: 0.3542653024196625, Final Batch Loss: 0.12857158482074738\n",
      "Epoch 2158, Loss: 0.39223942160606384, Final Batch Loss: 0.19861820340156555\n",
      "Epoch 2159, Loss: 0.3757421672344208, Final Batch Loss: 0.15957914292812347\n",
      "Epoch 2160, Loss: 0.36081986129283905, Final Batch Loss: 0.16124199330806732\n",
      "Epoch 2161, Loss: 0.4055248498916626, Final Batch Loss: 0.17260943353176117\n",
      "Epoch 2162, Loss: 0.3641590476036072, Final Batch Loss: 0.17747673392295837\n",
      "Epoch 2163, Loss: 0.3594726324081421, Final Batch Loss: 0.1880597025156021\n",
      "Epoch 2164, Loss: 0.39097949862480164, Final Batch Loss: 0.14773374795913696\n",
      "Epoch 2165, Loss: 0.31315894424915314, Final Batch Loss: 0.13439153134822845\n",
      "Epoch 2166, Loss: 0.38223373889923096, Final Batch Loss: 0.13455818593502045\n",
      "Epoch 2167, Loss: 0.3501102030277252, Final Batch Loss: 0.18912184238433838\n",
      "Epoch 2168, Loss: 0.4029056429862976, Final Batch Loss: 0.20155133306980133\n",
      "Epoch 2169, Loss: 0.3396840840578079, Final Batch Loss: 0.13775067031383514\n",
      "Epoch 2170, Loss: 0.36634886264801025, Final Batch Loss: 0.1904609501361847\n",
      "Epoch 2171, Loss: 0.3023163080215454, Final Batch Loss: 0.12948493659496307\n",
      "Epoch 2172, Loss: 0.3914306163787842, Final Batch Loss: 0.21913927793502808\n",
      "Epoch 2173, Loss: 0.32878245413303375, Final Batch Loss: 0.16899842023849487\n",
      "Epoch 2174, Loss: 0.33709195256233215, Final Batch Loss: 0.16668561100959778\n",
      "Epoch 2175, Loss: 0.42456893622875214, Final Batch Loss: 0.24690285325050354\n",
      "Epoch 2176, Loss: 0.3568083792924881, Final Batch Loss: 0.2067255675792694\n",
      "Epoch 2177, Loss: 0.3836021423339844, Final Batch Loss: 0.19697806239128113\n",
      "Epoch 2178, Loss: 0.42134442925453186, Final Batch Loss: 0.17696112394332886\n",
      "Epoch 2179, Loss: 0.37843894958496094, Final Batch Loss: 0.22221706807613373\n",
      "Epoch 2180, Loss: 0.3797159641981125, Final Batch Loss: 0.15859860181808472\n",
      "Epoch 2181, Loss: 0.4115332365036011, Final Batch Loss: 0.24317581951618195\n",
      "Epoch 2182, Loss: 0.4573863744735718, Final Batch Loss: 0.22334598004817963\n",
      "Epoch 2183, Loss: 0.4022396057844162, Final Batch Loss: 0.20871594548225403\n",
      "Epoch 2184, Loss: 0.406338170170784, Final Batch Loss: 0.20014044642448425\n",
      "Epoch 2185, Loss: 0.3731774240732193, Final Batch Loss: 0.21206636726856232\n",
      "Epoch 2186, Loss: 0.4212687611579895, Final Batch Loss: 0.24746839702129364\n",
      "Epoch 2187, Loss: 0.37827594578266144, Final Batch Loss: 0.201516255736351\n",
      "Epoch 2188, Loss: 0.38593608140945435, Final Batch Loss: 0.2010025680065155\n",
      "Epoch 2189, Loss: 0.36679302155971527, Final Batch Loss: 0.15421710908412933\n",
      "Epoch 2190, Loss: 0.3473144769668579, Final Batch Loss: 0.17940157651901245\n",
      "Epoch 2191, Loss: 0.34967803955078125, Final Batch Loss: 0.1640121042728424\n",
      "Epoch 2192, Loss: 0.3414154499769211, Final Batch Loss: 0.15080904960632324\n",
      "Epoch 2193, Loss: 0.3676788955926895, Final Batch Loss: 0.16655662655830383\n",
      "Epoch 2194, Loss: 0.3897547125816345, Final Batch Loss: 0.19571614265441895\n",
      "Epoch 2195, Loss: 0.33270034193992615, Final Batch Loss: 0.1909925490617752\n",
      "Epoch 2196, Loss: 0.3831510990858078, Final Batch Loss: 0.20146514475345612\n",
      "Epoch 2197, Loss: 0.3887570947408676, Final Batch Loss: 0.16597065329551697\n",
      "Epoch 2198, Loss: 0.3973948359489441, Final Batch Loss: 0.22625355422496796\n",
      "Epoch 2199, Loss: 0.35805581510066986, Final Batch Loss: 0.14439336955547333\n",
      "Epoch 2200, Loss: 0.3559233099222183, Final Batch Loss: 0.1734863966703415\n",
      "Epoch 2201, Loss: 0.41728490591049194, Final Batch Loss: 0.23453877866268158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2202, Loss: 0.30886438488960266, Final Batch Loss: 0.14118699729442596\n",
      "Epoch 2203, Loss: 0.35103410482406616, Final Batch Loss: 0.17273294925689697\n",
      "Epoch 2204, Loss: 0.3840370178222656, Final Batch Loss: 0.21263785660266876\n",
      "Epoch 2205, Loss: 0.35387904942035675, Final Batch Loss: 0.17566946148872375\n",
      "Epoch 2206, Loss: 0.41075360774993896, Final Batch Loss: 0.21229444444179535\n",
      "Epoch 2207, Loss: 0.3828498423099518, Final Batch Loss: 0.16391856968402863\n",
      "Epoch 2208, Loss: 0.45270276069641113, Final Batch Loss: 0.308997243642807\n",
      "Epoch 2209, Loss: 0.3938539922237396, Final Batch Loss: 0.15215326845645905\n",
      "Epoch 2210, Loss: 0.35329796373844147, Final Batch Loss: 0.22426317632198334\n",
      "Epoch 2211, Loss: 0.3862854838371277, Final Batch Loss: 0.19522298872470856\n",
      "Epoch 2212, Loss: 0.3752613216638565, Final Batch Loss: 0.1643509864807129\n",
      "Epoch 2213, Loss: 0.3829960823059082, Final Batch Loss: 0.16916872560977936\n",
      "Epoch 2214, Loss: 0.4012761265039444, Final Batch Loss: 0.21874086558818817\n",
      "Epoch 2215, Loss: 0.4075409919023514, Final Batch Loss: 0.24387387931346893\n",
      "Epoch 2216, Loss: 0.3736788034439087, Final Batch Loss: 0.2039070874452591\n",
      "Epoch 2217, Loss: 0.37803933024406433, Final Batch Loss: 0.24293042719364166\n",
      "Epoch 2218, Loss: 0.38449233770370483, Final Batch Loss: 0.2290160059928894\n",
      "Epoch 2219, Loss: 0.37286505103111267, Final Batch Loss: 0.17613285779953003\n",
      "Epoch 2220, Loss: 0.3576667606830597, Final Batch Loss: 0.13684086501598358\n",
      "Epoch 2221, Loss: 0.3547663688659668, Final Batch Loss: 0.16980206966400146\n",
      "Epoch 2222, Loss: 0.3794521540403366, Final Batch Loss: 0.16607791185379028\n",
      "Epoch 2223, Loss: 0.34303465485572815, Final Batch Loss: 0.16142261028289795\n",
      "Epoch 2224, Loss: 0.307551845908165, Final Batch Loss: 0.14360180497169495\n",
      "Epoch 2225, Loss: 0.38983672857284546, Final Batch Loss: 0.2215440720319748\n",
      "Epoch 2226, Loss: 0.3740670531988144, Final Batch Loss: 0.14967960119247437\n",
      "Epoch 2227, Loss: 0.3838866502046585, Final Batch Loss: 0.19162872433662415\n",
      "Epoch 2228, Loss: 0.36122460663318634, Final Batch Loss: 0.17135442793369293\n",
      "Epoch 2229, Loss: 0.40357355773448944, Final Batch Loss: 0.2129342406988144\n",
      "Epoch 2230, Loss: 0.36632518470287323, Final Batch Loss: 0.18310511112213135\n",
      "Epoch 2231, Loss: 0.40969792008399963, Final Batch Loss: 0.18760094046592712\n",
      "Epoch 2232, Loss: 0.370386078953743, Final Batch Loss: 0.21598395705223083\n",
      "Epoch 2233, Loss: 0.36492031812667847, Final Batch Loss: 0.18311074376106262\n",
      "Epoch 2234, Loss: 0.4018992483615875, Final Batch Loss: 0.18428842723369598\n",
      "Epoch 2235, Loss: 0.3601323366165161, Final Batch Loss: 0.16077300906181335\n",
      "Epoch 2236, Loss: 0.3803211897611618, Final Batch Loss: 0.17163580656051636\n",
      "Epoch 2237, Loss: 0.4071149677038193, Final Batch Loss: 0.18438538908958435\n",
      "Epoch 2238, Loss: 0.3687606155872345, Final Batch Loss: 0.18351615965366364\n",
      "Epoch 2239, Loss: 0.35386398434638977, Final Batch Loss: 0.16905097663402557\n",
      "Epoch 2240, Loss: 0.35274672508239746, Final Batch Loss: 0.18156155943870544\n",
      "Epoch 2241, Loss: 0.3293931931257248, Final Batch Loss: 0.13598540425300598\n",
      "Epoch 2242, Loss: 0.353789284825325, Final Batch Loss: 0.15527839958667755\n",
      "Epoch 2243, Loss: 0.37373313307762146, Final Batch Loss: 0.15024839341640472\n",
      "Epoch 2244, Loss: 0.37032994627952576, Final Batch Loss: 0.20401433110237122\n",
      "Epoch 2245, Loss: 0.3338530659675598, Final Batch Loss: 0.15592028200626373\n",
      "Epoch 2246, Loss: 0.3531099259853363, Final Batch Loss: 0.17685970664024353\n",
      "Epoch 2247, Loss: 0.38038448989391327, Final Batch Loss: 0.1899963617324829\n",
      "Epoch 2248, Loss: 0.3814924359321594, Final Batch Loss: 0.2084532380104065\n",
      "Epoch 2249, Loss: 0.39112357795238495, Final Batch Loss: 0.20258751511573792\n",
      "Epoch 2250, Loss: 0.3385063409805298, Final Batch Loss: 0.16976900398731232\n",
      "Epoch 2251, Loss: 0.43226219713687897, Final Batch Loss: 0.27445632219314575\n",
      "Epoch 2252, Loss: 0.4341263920068741, Final Batch Loss: 0.20540788769721985\n",
      "Epoch 2253, Loss: 0.4017101973295212, Final Batch Loss: 0.20561549067497253\n",
      "Epoch 2254, Loss: 0.43973714113235474, Final Batch Loss: 0.21343879401683807\n",
      "Epoch 2255, Loss: 0.37807604670524597, Final Batch Loss: 0.149566188454628\n",
      "Epoch 2256, Loss: 0.35937023162841797, Final Batch Loss: 0.17923712730407715\n",
      "Epoch 2257, Loss: 0.3119988292455673, Final Batch Loss: 0.14829878509044647\n",
      "Epoch 2258, Loss: 0.41579632461071014, Final Batch Loss: 0.1780945360660553\n",
      "Epoch 2259, Loss: 0.3473740816116333, Final Batch Loss: 0.14661359786987305\n",
      "Epoch 2260, Loss: 0.38462573289871216, Final Batch Loss: 0.22105015814304352\n",
      "Epoch 2261, Loss: 0.3414926826953888, Final Batch Loss: 0.15251687169075012\n",
      "Epoch 2262, Loss: 0.3850177228450775, Final Batch Loss: 0.20576339960098267\n",
      "Epoch 2263, Loss: 0.3737444132566452, Final Batch Loss: 0.21132858097553253\n",
      "Epoch 2264, Loss: 0.32933828234672546, Final Batch Loss: 0.14312969148159027\n",
      "Epoch 2265, Loss: 0.3470696061849594, Final Batch Loss: 0.17674103379249573\n",
      "Epoch 2266, Loss: 0.34790319204330444, Final Batch Loss: 0.14350730180740356\n",
      "Epoch 2267, Loss: 0.3564111888408661, Final Batch Loss: 0.16018979251384735\n",
      "Epoch 2268, Loss: 0.3362039178609848, Final Batch Loss: 0.17451362311840057\n",
      "Epoch 2269, Loss: 0.36574701964855194, Final Batch Loss: 0.21205274760723114\n",
      "Epoch 2270, Loss: 0.36729781329631805, Final Batch Loss: 0.20007573068141937\n",
      "Epoch 2271, Loss: 0.3192588984966278, Final Batch Loss: 0.1579906940460205\n",
      "Epoch 2272, Loss: 0.37918949127197266, Final Batch Loss: 0.167785182595253\n",
      "Epoch 2273, Loss: 0.4012891501188278, Final Batch Loss: 0.20903322100639343\n",
      "Epoch 2274, Loss: 0.3416641354560852, Final Batch Loss: 0.1519254744052887\n",
      "Epoch 2275, Loss: 0.3653060644865036, Final Batch Loss: 0.1857004016637802\n",
      "Epoch 2276, Loss: 0.4745246320962906, Final Batch Loss: 0.24723605811595917\n",
      "Epoch 2277, Loss: 0.37925052642822266, Final Batch Loss: 0.22801625728607178\n",
      "Epoch 2278, Loss: 0.3876631110906601, Final Batch Loss: 0.22025929391384125\n",
      "Epoch 2279, Loss: 0.3559504896402359, Final Batch Loss: 0.1322222501039505\n",
      "Epoch 2280, Loss: 0.3761615604162216, Final Batch Loss: 0.17419235408306122\n",
      "Epoch 2281, Loss: 0.38478511571884155, Final Batch Loss: 0.16503480076789856\n",
      "Epoch 2282, Loss: 0.40131838619709015, Final Batch Loss: 0.19191686809062958\n",
      "Epoch 2283, Loss: 0.34458526968955994, Final Batch Loss: 0.17092463374137878\n",
      "Epoch 2284, Loss: 0.3603595942258835, Final Batch Loss: 0.20168769359588623\n",
      "Epoch 2285, Loss: 0.3560894876718521, Final Batch Loss: 0.17156468331813812\n",
      "Epoch 2286, Loss: 0.39625297486782074, Final Batch Loss: 0.2016800493001938\n",
      "Epoch 2287, Loss: 0.36977964639663696, Final Batch Loss: 0.195271298289299\n",
      "Epoch 2288, Loss: 0.34506717324256897, Final Batch Loss: 0.1778993457555771\n",
      "Epoch 2289, Loss: 0.3700786232948303, Final Batch Loss: 0.18610212206840515\n",
      "Epoch 2290, Loss: 0.3718290328979492, Final Batch Loss: 0.1783859133720398\n",
      "Epoch 2291, Loss: 0.37994901835918427, Final Batch Loss: 0.1943661868572235\n",
      "Epoch 2292, Loss: 0.43882739543914795, Final Batch Loss: 0.26122400164604187\n",
      "Epoch 2293, Loss: 0.3326627314090729, Final Batch Loss: 0.15585331618785858\n",
      "Epoch 2294, Loss: 0.3609773516654968, Final Batch Loss: 0.16624422371387482\n",
      "Epoch 2295, Loss: 0.33694539964199066, Final Batch Loss: 0.16816960275173187\n",
      "Epoch 2296, Loss: 0.3529455065727234, Final Batch Loss: 0.1733870953321457\n",
      "Epoch 2297, Loss: 0.3828086256980896, Final Batch Loss: 0.20657281577587128\n",
      "Epoch 2298, Loss: 0.38210973888635635, Final Batch Loss: 0.26297128200531006\n",
      "Epoch 2299, Loss: 0.3783833384513855, Final Batch Loss: 0.20052747428417206\n",
      "Epoch 2300, Loss: 0.4483606219291687, Final Batch Loss: 0.21149958670139313\n",
      "Epoch 2301, Loss: 0.3691387474536896, Final Batch Loss: 0.18494269251823425\n",
      "Epoch 2302, Loss: 0.3404712975025177, Final Batch Loss: 0.16425438225269318\n",
      "Epoch 2303, Loss: 0.3500557392835617, Final Batch Loss: 0.16326910257339478\n",
      "Epoch 2304, Loss: 0.3505120724439621, Final Batch Loss: 0.17159585654735565\n",
      "Epoch 2305, Loss: 0.3831748962402344, Final Batch Loss: 0.19977135956287384\n",
      "Epoch 2306, Loss: 0.4312557429075241, Final Batch Loss: 0.16344957053661346\n",
      "Epoch 2307, Loss: 0.37679623067379, Final Batch Loss: 0.17510318756103516\n",
      "Epoch 2308, Loss: 0.35331544280052185, Final Batch Loss: 0.20459014177322388\n",
      "Epoch 2309, Loss: 0.3382212668657303, Final Batch Loss: 0.17824514210224152\n",
      "Epoch 2310, Loss: 0.3983004093170166, Final Batch Loss: 0.24289819598197937\n",
      "Epoch 2311, Loss: 0.3444863110780716, Final Batch Loss: 0.1488427221775055\n",
      "Epoch 2312, Loss: 0.37540502846241, Final Batch Loss: 0.1537589579820633\n",
      "Epoch 2313, Loss: 0.3702384978532791, Final Batch Loss: 0.168818861246109\n",
      "Epoch 2314, Loss: 0.37310390174388885, Final Batch Loss: 0.17649108171463013\n",
      "Epoch 2315, Loss: 0.3896579444408417, Final Batch Loss: 0.18921947479248047\n",
      "Epoch 2316, Loss: 0.4021327495574951, Final Batch Loss: 0.1978919953107834\n",
      "Epoch 2317, Loss: 0.3455120623111725, Final Batch Loss: 0.20232394337654114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2318, Loss: 0.3644334077835083, Final Batch Loss: 0.17637448012828827\n",
      "Epoch 2319, Loss: 0.3539528101682663, Final Batch Loss: 0.17580366134643555\n",
      "Epoch 2320, Loss: 0.3587331622838974, Final Batch Loss: 0.15925076603889465\n",
      "Epoch 2321, Loss: 0.336998775601387, Final Batch Loss: 0.1498223841190338\n",
      "Epoch 2322, Loss: 0.320999875664711, Final Batch Loss: 0.1288662552833557\n",
      "Epoch 2323, Loss: 0.3361784666776657, Final Batch Loss: 0.18366700410842896\n",
      "Epoch 2324, Loss: 0.3766004890203476, Final Batch Loss: 0.1824573129415512\n",
      "Epoch 2325, Loss: 0.402330219745636, Final Batch Loss: 0.2370987981557846\n",
      "Epoch 2326, Loss: 0.3723214715719223, Final Batch Loss: 0.18884402513504028\n",
      "Epoch 2327, Loss: 0.4063268452882767, Final Batch Loss: 0.23258012533187866\n",
      "Epoch 2328, Loss: 0.35482360422611237, Final Batch Loss: 0.19289027154445648\n",
      "Epoch 2329, Loss: 0.3595229834318161, Final Batch Loss: 0.17814745008945465\n",
      "Epoch 2330, Loss: 0.33760999143123627, Final Batch Loss: 0.1297462433576584\n",
      "Epoch 2331, Loss: 0.36544662714004517, Final Batch Loss: 0.1680625081062317\n",
      "Epoch 2332, Loss: 0.36678414046764374, Final Batch Loss: 0.1924898326396942\n",
      "Epoch 2333, Loss: 0.3116186559200287, Final Batch Loss: 0.12669715285301208\n",
      "Epoch 2334, Loss: 0.34905681014060974, Final Batch Loss: 0.15313127636909485\n",
      "Epoch 2335, Loss: 0.3579919934272766, Final Batch Loss: 0.18891090154647827\n",
      "Epoch 2336, Loss: 0.36495013535022736, Final Batch Loss: 0.19459238648414612\n",
      "Epoch 2337, Loss: 0.37913069128990173, Final Batch Loss: 0.15318550169467926\n",
      "Epoch 2338, Loss: 0.33340542018413544, Final Batch Loss: 0.13756096363067627\n",
      "Epoch 2339, Loss: 0.43575243651866913, Final Batch Loss: 0.25635579228401184\n",
      "Epoch 2340, Loss: 0.33055292069911957, Final Batch Loss: 0.154652401804924\n",
      "Epoch 2341, Loss: 0.3595885783433914, Final Batch Loss: 0.2119901180267334\n",
      "Epoch 2342, Loss: 0.36452212929725647, Final Batch Loss: 0.21759499609470367\n",
      "Epoch 2343, Loss: 0.33605895936489105, Final Batch Loss: 0.1488712728023529\n",
      "Epoch 2344, Loss: 0.33443786203861237, Final Batch Loss: 0.17809975147247314\n",
      "Epoch 2345, Loss: 0.3109763562679291, Final Batch Loss: 0.1359831839799881\n",
      "Epoch 2346, Loss: 0.4095223397016525, Final Batch Loss: 0.2081589549779892\n",
      "Epoch 2347, Loss: 0.35287924110889435, Final Batch Loss: 0.1626259982585907\n",
      "Epoch 2348, Loss: 0.34327416121959686, Final Batch Loss: 0.18161475658416748\n",
      "Epoch 2349, Loss: 0.35054968297481537, Final Batch Loss: 0.15912212431430817\n",
      "Epoch 2350, Loss: 0.3542332202196121, Final Batch Loss: 0.2066752016544342\n",
      "Epoch 2351, Loss: 0.34525999426841736, Final Batch Loss: 0.1312519609928131\n",
      "Epoch 2352, Loss: 0.3205961436033249, Final Batch Loss: 0.17126333713531494\n",
      "Epoch 2353, Loss: 0.3677711486816406, Final Batch Loss: 0.19595913589000702\n",
      "Epoch 2354, Loss: 0.33437658846378326, Final Batch Loss: 0.15197071433067322\n",
      "Epoch 2355, Loss: 0.3313014656305313, Final Batch Loss: 0.15174062550067902\n",
      "Epoch 2356, Loss: 0.34504643082618713, Final Batch Loss: 0.1568301022052765\n",
      "Epoch 2357, Loss: 0.3854324370622635, Final Batch Loss: 0.1875624805688858\n",
      "Epoch 2358, Loss: 0.3275671750307083, Final Batch Loss: 0.1435004472732544\n",
      "Epoch 2359, Loss: 0.35619276762008667, Final Batch Loss: 0.16773907840251923\n",
      "Epoch 2360, Loss: 0.34840525686740875, Final Batch Loss: 0.18287824094295502\n",
      "Epoch 2361, Loss: 0.37425486743450165, Final Batch Loss: 0.18952155113220215\n",
      "Epoch 2362, Loss: 0.3834129422903061, Final Batch Loss: 0.21216921508312225\n",
      "Epoch 2363, Loss: 0.3462458848953247, Final Batch Loss: 0.180457204580307\n",
      "Epoch 2364, Loss: 0.35926151275634766, Final Batch Loss: 0.17060324549674988\n",
      "Epoch 2365, Loss: 0.3605296313762665, Final Batch Loss: 0.18417495489120483\n",
      "Epoch 2366, Loss: 0.3922156095504761, Final Batch Loss: 0.21321862936019897\n",
      "Epoch 2367, Loss: 0.3290790915489197, Final Batch Loss: 0.14846263825893402\n",
      "Epoch 2368, Loss: 0.3480989336967468, Final Batch Loss: 0.1900208294391632\n",
      "Epoch 2369, Loss: 0.3307938426733017, Final Batch Loss: 0.17175528407096863\n",
      "Epoch 2370, Loss: 0.2997636944055557, Final Batch Loss: 0.15704059600830078\n",
      "Epoch 2371, Loss: 0.3490677922964096, Final Batch Loss: 0.20170773565769196\n",
      "Epoch 2372, Loss: 0.2968183755874634, Final Batch Loss: 0.1452622413635254\n",
      "Epoch 2373, Loss: 0.3557921350002289, Final Batch Loss: 0.18361447751522064\n",
      "Epoch 2374, Loss: 0.29995208978652954, Final Batch Loss: 0.12576152384281158\n",
      "Epoch 2375, Loss: 0.3791498690843582, Final Batch Loss: 0.16503353416919708\n",
      "Epoch 2376, Loss: 0.30181995034217834, Final Batch Loss: 0.14965926110744476\n",
      "Epoch 2377, Loss: 0.3197632282972336, Final Batch Loss: 0.15936915576457977\n",
      "Epoch 2378, Loss: 0.3146737813949585, Final Batch Loss: 0.15630438923835754\n",
      "Epoch 2379, Loss: 0.3070245683193207, Final Batch Loss: 0.1561560332775116\n",
      "Epoch 2380, Loss: 0.2887779623270035, Final Batch Loss: 0.1269691288471222\n",
      "Epoch 2381, Loss: 0.3273245096206665, Final Batch Loss: 0.15545803308486938\n",
      "Epoch 2382, Loss: 0.32108597457408905, Final Batch Loss: 0.1588069498538971\n",
      "Epoch 2383, Loss: 0.3366829454898834, Final Batch Loss: 0.16186632215976715\n",
      "Epoch 2384, Loss: 0.31947164237499237, Final Batch Loss: 0.15773005783557892\n",
      "Epoch 2385, Loss: 0.3647546023130417, Final Batch Loss: 0.1666267365217209\n",
      "Epoch 2386, Loss: 0.342879518866539, Final Batch Loss: 0.1923549473285675\n",
      "Epoch 2387, Loss: 0.2952725887298584, Final Batch Loss: 0.15565615892410278\n",
      "Epoch 2388, Loss: 0.3426547348499298, Final Batch Loss: 0.19805696606636047\n",
      "Epoch 2389, Loss: 0.3467468023300171, Final Batch Loss: 0.17510144412517548\n",
      "Epoch 2390, Loss: 0.31353482604026794, Final Batch Loss: 0.14154548943042755\n",
      "Epoch 2391, Loss: 0.49749189615249634, Final Batch Loss: 0.2851850986480713\n",
      "Epoch 2392, Loss: 0.38543732464313507, Final Batch Loss: 0.2323581427335739\n",
      "Epoch 2393, Loss: 0.46835756301879883, Final Batch Loss: 0.2774498462677002\n",
      "Epoch 2394, Loss: 0.32999835908412933, Final Batch Loss: 0.14715513586997986\n",
      "Epoch 2395, Loss: 0.38695746660232544, Final Batch Loss: 0.19271032512187958\n",
      "Epoch 2396, Loss: 0.40491338074207306, Final Batch Loss: 0.19764351844787598\n",
      "Epoch 2397, Loss: 0.31658272445201874, Final Batch Loss: 0.14206385612487793\n",
      "Epoch 2398, Loss: 0.41226549446582794, Final Batch Loss: 0.23510776460170746\n",
      "Epoch 2399, Loss: 0.3354012966156006, Final Batch Loss: 0.16070683300495148\n",
      "Epoch 2400, Loss: 0.3998754173517227, Final Batch Loss: 0.20545557141304016\n",
      "Epoch 2401, Loss: 0.3637518137693405, Final Batch Loss: 0.1806890219449997\n",
      "Epoch 2402, Loss: 0.364569827914238, Final Batch Loss: 0.18315401673316956\n",
      "Epoch 2403, Loss: 0.37817874550819397, Final Batch Loss: 0.18122851848602295\n",
      "Epoch 2404, Loss: 0.3779830038547516, Final Batch Loss: 0.17584457993507385\n",
      "Epoch 2405, Loss: 0.41775479912757874, Final Batch Loss: 0.24451830983161926\n",
      "Epoch 2406, Loss: 0.35597261786460876, Final Batch Loss: 0.22258184850215912\n",
      "Epoch 2407, Loss: 0.3651982545852661, Final Batch Loss: 0.1867680698633194\n",
      "Epoch 2408, Loss: 0.3790765255689621, Final Batch Loss: 0.17275197803974152\n",
      "Epoch 2409, Loss: 0.3199320435523987, Final Batch Loss: 0.16912144422531128\n",
      "Epoch 2410, Loss: 0.35893845558166504, Final Batch Loss: 0.14099952578544617\n",
      "Epoch 2411, Loss: 0.32538579404354095, Final Batch Loss: 0.16894054412841797\n",
      "Epoch 2412, Loss: 0.3361782133579254, Final Batch Loss: 0.14136189222335815\n",
      "Epoch 2413, Loss: 0.38142290711402893, Final Batch Loss: 0.19985012710094452\n",
      "Epoch 2414, Loss: 0.3885873556137085, Final Batch Loss: 0.20741672813892365\n",
      "Epoch 2415, Loss: 0.3181458115577698, Final Batch Loss: 0.15665332973003387\n",
      "Epoch 2416, Loss: 0.35777050256729126, Final Batch Loss: 0.19254036247730255\n",
      "Epoch 2417, Loss: 0.35536570847034454, Final Batch Loss: 0.1742023229598999\n",
      "Epoch 2418, Loss: 0.3557765334844589, Final Batch Loss: 0.16855627298355103\n",
      "Epoch 2419, Loss: 0.3740231394767761, Final Batch Loss: 0.17575953900814056\n",
      "Epoch 2420, Loss: 0.31803812086582184, Final Batch Loss: 0.1568388044834137\n",
      "Epoch 2421, Loss: 0.35965779423713684, Final Batch Loss: 0.1820632964372635\n",
      "Epoch 2422, Loss: 0.34603434801101685, Final Batch Loss: 0.17932343482971191\n",
      "Epoch 2423, Loss: 0.3771909773349762, Final Batch Loss: 0.2519311308860779\n",
      "Epoch 2424, Loss: 0.3213743716478348, Final Batch Loss: 0.16543838381767273\n",
      "Epoch 2425, Loss: 0.3738401532173157, Final Batch Loss: 0.192262202501297\n",
      "Epoch 2426, Loss: 0.3172897398471832, Final Batch Loss: 0.13580608367919922\n",
      "Epoch 2427, Loss: 0.3792273849248886, Final Batch Loss: 0.19929508864879608\n",
      "Epoch 2428, Loss: 0.29708945751190186, Final Batch Loss: 0.10788081586360931\n",
      "Epoch 2429, Loss: 0.3527580797672272, Final Batch Loss: 0.1728179007768631\n",
      "Epoch 2430, Loss: 0.387259304523468, Final Batch Loss: 0.18892553448677063\n",
      "Epoch 2431, Loss: 0.37961341440677643, Final Batch Loss: 0.20051585137844086\n",
      "Epoch 2432, Loss: 0.3409435302019119, Final Batch Loss: 0.1681848019361496\n",
      "Epoch 2433, Loss: 0.3966934084892273, Final Batch Loss: 0.253854364156723\n",
      "Epoch 2434, Loss: 0.3543369770050049, Final Batch Loss: 0.1697639673948288\n",
      "Epoch 2435, Loss: 0.5472463667392731, Final Batch Loss: 0.3723774254322052\n",
      "Epoch 2436, Loss: 0.33732518553733826, Final Batch Loss: 0.15152990818023682\n",
      "Epoch 2437, Loss: 0.37840838730335236, Final Batch Loss: 0.2082696110010147\n",
      "Epoch 2438, Loss: 0.2981243431568146, Final Batch Loss: 0.14739549160003662\n",
      "Epoch 2439, Loss: 0.3231784850358963, Final Batch Loss: 0.1467798352241516\n",
      "Epoch 2440, Loss: 0.38609544932842255, Final Batch Loss: 0.19722585380077362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2441, Loss: 0.3295148015022278, Final Batch Loss: 0.1834631860256195\n",
      "Epoch 2442, Loss: 0.33698128163814545, Final Batch Loss: 0.18289101123809814\n",
      "Epoch 2443, Loss: 0.3333338052034378, Final Batch Loss: 0.15742552280426025\n",
      "Epoch 2444, Loss: 0.36061233282089233, Final Batch Loss: 0.15596964955329895\n",
      "Epoch 2445, Loss: 0.3351072520017624, Final Batch Loss: 0.19585664570331573\n",
      "Epoch 2446, Loss: 0.3555842190980911, Final Batch Loss: 0.19408632814884186\n",
      "Epoch 2447, Loss: 0.366772323846817, Final Batch Loss: 0.18657997250556946\n",
      "Epoch 2448, Loss: 0.3976830691099167, Final Batch Loss: 0.1622111052274704\n",
      "Epoch 2449, Loss: 0.345940038561821, Final Batch Loss: 0.17481741309165955\n",
      "Epoch 2450, Loss: 0.32381144165992737, Final Batch Loss: 0.14927364885807037\n",
      "Epoch 2451, Loss: 0.30054011940956116, Final Batch Loss: 0.13757872581481934\n",
      "Epoch 2452, Loss: 0.41875652968883514, Final Batch Loss: 0.20364180207252502\n",
      "Epoch 2453, Loss: 0.315920352935791, Final Batch Loss: 0.1400546133518219\n",
      "Epoch 2454, Loss: 0.3100646287202835, Final Batch Loss: 0.11196088790893555\n",
      "Epoch 2455, Loss: 0.33216407895088196, Final Batch Loss: 0.16244791448116302\n",
      "Epoch 2456, Loss: 0.33249160647392273, Final Batch Loss: 0.16045720875263214\n",
      "Epoch 2457, Loss: 0.33891409635543823, Final Batch Loss: 0.1512443870306015\n",
      "Epoch 2458, Loss: 0.34862977266311646, Final Batch Loss: 0.18669946491718292\n",
      "Epoch 2459, Loss: 0.4064703583717346, Final Batch Loss: 0.2045488953590393\n",
      "Epoch 2460, Loss: 0.3296957015991211, Final Batch Loss: 0.12829658389091492\n",
      "Epoch 2461, Loss: 0.34570495784282684, Final Batch Loss: 0.155508890748024\n",
      "Epoch 2462, Loss: 0.3377794176340103, Final Batch Loss: 0.2050555944442749\n",
      "Epoch 2463, Loss: 0.38476620614528656, Final Batch Loss: 0.19115953147411346\n",
      "Epoch 2464, Loss: 0.3423902243375778, Final Batch Loss: 0.18823719024658203\n",
      "Epoch 2465, Loss: 0.3471134603023529, Final Batch Loss: 0.18288838863372803\n",
      "Epoch 2466, Loss: 0.3344784677028656, Final Batch Loss: 0.1590382307767868\n",
      "Epoch 2467, Loss: 0.31949034333229065, Final Batch Loss: 0.15632493793964386\n",
      "Epoch 2468, Loss: 0.35333822667598724, Final Batch Loss: 0.1501884162425995\n",
      "Epoch 2469, Loss: 0.30726706981658936, Final Batch Loss: 0.13399583101272583\n",
      "Epoch 2470, Loss: 0.38489022850990295, Final Batch Loss: 0.17883487045764923\n",
      "Epoch 2471, Loss: 0.3411579877138138, Final Batch Loss: 0.17014868557453156\n",
      "Epoch 2472, Loss: 0.32666727900505066, Final Batch Loss: 0.14882953464984894\n",
      "Epoch 2473, Loss: 0.3715999573469162, Final Batch Loss: 0.21581918001174927\n",
      "Epoch 2474, Loss: 0.38542117178440094, Final Batch Loss: 0.1955222636461258\n",
      "Epoch 2475, Loss: 0.36552101373672485, Final Batch Loss: 0.15038301050662994\n",
      "Epoch 2476, Loss: 0.3290446251630783, Final Batch Loss: 0.1720436066389084\n",
      "Epoch 2477, Loss: 0.3729025721549988, Final Batch Loss: 0.20536178350448608\n",
      "Epoch 2478, Loss: 0.48148155212402344, Final Batch Loss: 0.15243735909461975\n",
      "Epoch 2479, Loss: 0.3697514235973358, Final Batch Loss: 0.21477513015270233\n",
      "Epoch 2480, Loss: 0.3685856908559799, Final Batch Loss: 0.24012811481952667\n",
      "Epoch 2481, Loss: 0.4053155928850174, Final Batch Loss: 0.21049034595489502\n",
      "Epoch 2482, Loss: 0.2964451313018799, Final Batch Loss: 0.13276295363903046\n",
      "Epoch 2483, Loss: 0.36309120059013367, Final Batch Loss: 0.17754514515399933\n",
      "Epoch 2484, Loss: 0.3162565752863884, Final Batch Loss: 0.11444971710443497\n",
      "Epoch 2485, Loss: 0.35122716426849365, Final Batch Loss: 0.1649886667728424\n",
      "Epoch 2486, Loss: 0.36325426399707794, Final Batch Loss: 0.22271987795829773\n",
      "Epoch 2487, Loss: 0.3380613625049591, Final Batch Loss: 0.17823053896427155\n",
      "Epoch 2488, Loss: 0.3139241188764572, Final Batch Loss: 0.15829017758369446\n",
      "Epoch 2489, Loss: 0.3670829236507416, Final Batch Loss: 0.219228595495224\n",
      "Epoch 2490, Loss: 0.32786446809768677, Final Batch Loss: 0.144768625497818\n",
      "Epoch 2491, Loss: 0.3133191168308258, Final Batch Loss: 0.14177517592906952\n",
      "Epoch 2492, Loss: 0.34360553324222565, Final Batch Loss: 0.20304208993911743\n",
      "Epoch 2493, Loss: 0.31248097866773605, Final Batch Loss: 0.12379761785268784\n",
      "Epoch 2494, Loss: 0.3349587917327881, Final Batch Loss: 0.1885320544242859\n",
      "Epoch 2495, Loss: 0.3368799686431885, Final Batch Loss: 0.16054008901119232\n",
      "Epoch 2496, Loss: 0.32246534526348114, Final Batch Loss: 0.1461322158575058\n",
      "Epoch 2497, Loss: 0.306986927986145, Final Batch Loss: 0.13916951417922974\n",
      "Epoch 2498, Loss: 0.3176132142543793, Final Batch Loss: 0.13460949063301086\n",
      "Epoch 2499, Loss: 0.3240773230791092, Final Batch Loss: 0.12266850471496582\n",
      "Epoch 2500, Loss: 0.3407357931137085, Final Batch Loss: 0.16927844285964966\n",
      "Epoch 2501, Loss: 0.2924211323261261, Final Batch Loss: 0.1373084932565689\n",
      "Epoch 2502, Loss: 0.3233657032251358, Final Batch Loss: 0.15001001954078674\n",
      "Epoch 2503, Loss: 0.4172366112470627, Final Batch Loss: 0.23338860273361206\n",
      "Epoch 2504, Loss: 0.33963005244731903, Final Batch Loss: 0.1734698861837387\n",
      "Epoch 2505, Loss: 0.3316697031259537, Final Batch Loss: 0.16946566104888916\n",
      "Epoch 2506, Loss: 0.2982065826654434, Final Batch Loss: 0.13654470443725586\n",
      "Epoch 2507, Loss: 0.28934501111507416, Final Batch Loss: 0.1337243765592575\n",
      "Epoch 2508, Loss: 0.33291487395763397, Final Batch Loss: 0.15145066380500793\n",
      "Epoch 2509, Loss: 0.3444247394800186, Final Batch Loss: 0.14848783612251282\n",
      "Epoch 2510, Loss: 0.3354883939027786, Final Batch Loss: 0.13527581095695496\n",
      "Epoch 2511, Loss: 0.3785078823566437, Final Batch Loss: 0.18881529569625854\n",
      "Epoch 2512, Loss: 0.3493807762861252, Final Batch Loss: 0.19768045842647552\n",
      "Epoch 2513, Loss: 0.3281097412109375, Final Batch Loss: 0.1650303304195404\n",
      "Epoch 2514, Loss: 0.35429665446281433, Final Batch Loss: 0.19390800595283508\n",
      "Epoch 2515, Loss: 0.327091708779335, Final Batch Loss: 0.14610198140144348\n",
      "Epoch 2516, Loss: 0.35905878245830536, Final Batch Loss: 0.15962865948677063\n",
      "Epoch 2517, Loss: 0.33015842735767365, Final Batch Loss: 0.14754965901374817\n",
      "Epoch 2518, Loss: 0.3450430780649185, Final Batch Loss: 0.16914506256580353\n",
      "Epoch 2519, Loss: 0.3815643787384033, Final Batch Loss: 0.24519187211990356\n",
      "Epoch 2520, Loss: 0.2901846766471863, Final Batch Loss: 0.13754937052726746\n",
      "Epoch 2521, Loss: 0.2833279073238373, Final Batch Loss: 0.12316130101680756\n",
      "Epoch 2522, Loss: 0.31063060462474823, Final Batch Loss: 0.1277989000082016\n",
      "Epoch 2523, Loss: 0.3454250991344452, Final Batch Loss: 0.2078491598367691\n",
      "Epoch 2524, Loss: 0.38217832148075104, Final Batch Loss: 0.21830904483795166\n",
      "Epoch 2525, Loss: 0.38929150998592377, Final Batch Loss: 0.15855924785137177\n",
      "Epoch 2526, Loss: 0.35780224204063416, Final Batch Loss: 0.196132630109787\n",
      "Epoch 2527, Loss: 0.3997602164745331, Final Batch Loss: 0.24870596826076508\n",
      "Epoch 2528, Loss: 0.3821622133255005, Final Batch Loss: 0.21329306066036224\n",
      "Epoch 2529, Loss: 0.2994668334722519, Final Batch Loss: 0.1609484702348709\n",
      "Epoch 2530, Loss: 0.36124107241630554, Final Batch Loss: 0.1875719130039215\n",
      "Epoch 2531, Loss: 0.3444749563932419, Final Batch Loss: 0.17269721627235413\n",
      "Epoch 2532, Loss: 0.39616310596466064, Final Batch Loss: 0.1728665679693222\n",
      "Epoch 2533, Loss: 0.3294995278120041, Final Batch Loss: 0.14198222756385803\n",
      "Epoch 2534, Loss: 0.3366492837667465, Final Batch Loss: 0.14801344275474548\n",
      "Epoch 2535, Loss: 0.3512887954711914, Final Batch Loss: 0.1603948175907135\n",
      "Epoch 2536, Loss: 0.33555687963962555, Final Batch Loss: 0.15215569734573364\n",
      "Epoch 2537, Loss: 0.3351048082113266, Final Batch Loss: 0.19242914021015167\n",
      "Epoch 2538, Loss: 0.30887754261493683, Final Batch Loss: 0.098006471991539\n",
      "Epoch 2539, Loss: 0.33672384917736053, Final Batch Loss: 0.15836790204048157\n",
      "Epoch 2540, Loss: 0.3429679423570633, Final Batch Loss: 0.1622021645307541\n",
      "Epoch 2541, Loss: 0.3173879086971283, Final Batch Loss: 0.1741638034582138\n",
      "Epoch 2542, Loss: 0.3026905357837677, Final Batch Loss: 0.1561795026063919\n",
      "Epoch 2543, Loss: 0.3489387035369873, Final Batch Loss: 0.17862708866596222\n",
      "Epoch 2544, Loss: 0.3346024751663208, Final Batch Loss: 0.1785060614347458\n",
      "Epoch 2545, Loss: 0.39584507048130035, Final Batch Loss: 0.22915200889110565\n",
      "Epoch 2546, Loss: 0.3203737586736679, Final Batch Loss: 0.18195581436157227\n",
      "Epoch 2547, Loss: 0.3478570729494095, Final Batch Loss: 0.18795329332351685\n",
      "Epoch 2548, Loss: 0.3309876620769501, Final Batch Loss: 0.15529972314834595\n",
      "Epoch 2549, Loss: 0.37868383526802063, Final Batch Loss: 0.2302781641483307\n",
      "Epoch 2550, Loss: 0.358598455786705, Final Batch Loss: 0.21345160901546478\n",
      "Epoch 2551, Loss: 0.3823583424091339, Final Batch Loss: 0.12948542833328247\n",
      "Epoch 2552, Loss: 0.34053073823451996, Final Batch Loss: 0.17564141750335693\n",
      "Epoch 2553, Loss: 0.35665835440158844, Final Batch Loss: 0.2024603933095932\n",
      "Epoch 2554, Loss: 0.35150469839572906, Final Batch Loss: 0.17572090029716492\n",
      "Epoch 2555, Loss: 0.3539351671934128, Final Batch Loss: 0.1951974779367447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2556, Loss: 0.30145420134067535, Final Batch Loss: 0.14439374208450317\n",
      "Epoch 2557, Loss: 0.31056736409664154, Final Batch Loss: 0.13776592910289764\n",
      "Epoch 2558, Loss: 0.3675057888031006, Final Batch Loss: 0.19255132973194122\n",
      "Epoch 2559, Loss: 0.3599853366613388, Final Batch Loss: 0.17086175084114075\n",
      "Epoch 2560, Loss: 0.3495914489030838, Final Batch Loss: 0.1795404702425003\n",
      "Epoch 2561, Loss: 0.3077353909611702, Final Batch Loss: 0.11879222840070724\n",
      "Epoch 2562, Loss: 0.38471782207489014, Final Batch Loss: 0.18301886320114136\n",
      "Epoch 2563, Loss: 0.39710135757923126, Final Batch Loss: 0.2012743055820465\n",
      "Epoch 2564, Loss: 0.34279774129390717, Final Batch Loss: 0.16900016367435455\n",
      "Epoch 2565, Loss: 0.38628871738910675, Final Batch Loss: 0.2385079562664032\n",
      "Epoch 2566, Loss: 0.3747195601463318, Final Batch Loss: 0.17797595262527466\n",
      "Epoch 2567, Loss: 0.3336430937051773, Final Batch Loss: 0.19966241717338562\n",
      "Epoch 2568, Loss: 0.3182102292776108, Final Batch Loss: 0.15802770853042603\n",
      "Epoch 2569, Loss: 0.289201058447361, Final Batch Loss: 0.08150472491979599\n",
      "Epoch 2570, Loss: 0.3335648328065872, Final Batch Loss: 0.19937166571617126\n",
      "Epoch 2571, Loss: 0.3762948215007782, Final Batch Loss: 0.19292530417442322\n",
      "Epoch 2572, Loss: 0.3115343153476715, Final Batch Loss: 0.14548447728157043\n",
      "Epoch 2573, Loss: 0.3838887959718704, Final Batch Loss: 0.21024253964424133\n",
      "Epoch 2574, Loss: 0.3603077381849289, Final Batch Loss: 0.1849089413881302\n",
      "Epoch 2575, Loss: 0.3117813169956207, Final Batch Loss: 0.1528913825750351\n",
      "Epoch 2576, Loss: 0.33804504573345184, Final Batch Loss: 0.13333605229854584\n",
      "Epoch 2577, Loss: 0.3160579055547714, Final Batch Loss: 0.13846005499362946\n",
      "Epoch 2578, Loss: 0.3423009067773819, Final Batch Loss: 0.20022886991500854\n",
      "Epoch 2579, Loss: 0.32220612466335297, Final Batch Loss: 0.14778003096580505\n",
      "Epoch 2580, Loss: 0.3360793739557266, Final Batch Loss: 0.16722336411476135\n",
      "Epoch 2581, Loss: 0.3121165633201599, Final Batch Loss: 0.1541098654270172\n",
      "Epoch 2582, Loss: 0.3519582450389862, Final Batch Loss: 0.2026365101337433\n",
      "Epoch 2583, Loss: 0.31860820949077606, Final Batch Loss: 0.15054242312908173\n",
      "Epoch 2584, Loss: 0.3625977039337158, Final Batch Loss: 0.18866172432899475\n",
      "Epoch 2585, Loss: 0.3121764808893204, Final Batch Loss: 0.14147640764713287\n",
      "Epoch 2586, Loss: 0.32675012946128845, Final Batch Loss: 0.15573008358478546\n",
      "Epoch 2587, Loss: 0.38847534358501434, Final Batch Loss: 0.19242000579833984\n",
      "Epoch 2588, Loss: 0.385709285736084, Final Batch Loss: 0.19917647540569305\n",
      "Epoch 2589, Loss: 0.37348058819770813, Final Batch Loss: 0.1999315470457077\n",
      "Epoch 2590, Loss: 0.36680759489536285, Final Batch Loss: 0.1867126226425171\n",
      "Epoch 2591, Loss: 0.36601458489894867, Final Batch Loss: 0.1739734262228012\n",
      "Epoch 2592, Loss: 0.3764886111021042, Final Batch Loss: 0.16187883913516998\n",
      "Epoch 2593, Loss: 0.34487439692020416, Final Batch Loss: 0.1655525416135788\n",
      "Epoch 2594, Loss: 0.31550631672143936, Final Batch Loss: 0.12466373294591904\n",
      "Epoch 2595, Loss: 0.3208644837141037, Final Batch Loss: 0.10742828249931335\n",
      "Epoch 2596, Loss: 0.3558676242828369, Final Batch Loss: 0.1866314858198166\n",
      "Epoch 2597, Loss: 0.33092740178108215, Final Batch Loss: 0.1601238250732422\n",
      "Epoch 2598, Loss: 0.34223274886608124, Final Batch Loss: 0.16685862839221954\n",
      "Epoch 2599, Loss: 0.3341955542564392, Final Batch Loss: 0.16980095207691193\n",
      "Epoch 2600, Loss: 0.33193913102149963, Final Batch Loss: 0.17506182193756104\n",
      "Epoch 2601, Loss: 0.3826163411140442, Final Batch Loss: 0.2098100483417511\n",
      "Epoch 2602, Loss: 0.33442866802215576, Final Batch Loss: 0.1312243938446045\n",
      "Epoch 2603, Loss: 0.3226364552974701, Final Batch Loss: 0.15486831963062286\n",
      "Epoch 2604, Loss: 0.38043126463890076, Final Batch Loss: 0.15927329659461975\n",
      "Epoch 2605, Loss: 0.3192185014486313, Final Batch Loss: 0.1550353467464447\n",
      "Epoch 2606, Loss: 0.3460303694009781, Final Batch Loss: 0.17214380204677582\n",
      "Epoch 2607, Loss: 0.349444642663002, Final Batch Loss: 0.19310100376605988\n",
      "Epoch 2608, Loss: 0.27869660407304764, Final Batch Loss: 0.11743751913309097\n",
      "Epoch 2609, Loss: 0.35813580453395844, Final Batch Loss: 0.19241148233413696\n",
      "Epoch 2610, Loss: 0.3530482202768326, Final Batch Loss: 0.1973416805267334\n",
      "Epoch 2611, Loss: 0.3412444144487381, Final Batch Loss: 0.16026191413402557\n",
      "Epoch 2612, Loss: 0.30057212710380554, Final Batch Loss: 0.12556643784046173\n",
      "Epoch 2613, Loss: 0.3124806135892868, Final Batch Loss: 0.16282722353935242\n",
      "Epoch 2614, Loss: 0.4107689708471298, Final Batch Loss: 0.18975991010665894\n",
      "Epoch 2615, Loss: 0.3380783200263977, Final Batch Loss: 0.16647407412528992\n",
      "Epoch 2616, Loss: 0.3665499985218048, Final Batch Loss: 0.20119595527648926\n",
      "Epoch 2617, Loss: 0.3410349488258362, Final Batch Loss: 0.14216248691082\n",
      "Epoch 2618, Loss: 0.341143861413002, Final Batch Loss: 0.13748270273208618\n",
      "Epoch 2619, Loss: 0.30897459387779236, Final Batch Loss: 0.13804903626441956\n",
      "Epoch 2620, Loss: 0.38507741689682007, Final Batch Loss: 0.21520116925239563\n",
      "Epoch 2621, Loss: 0.3798460513353348, Final Batch Loss: 0.17593440413475037\n",
      "Epoch 2622, Loss: 0.3348532170057297, Final Batch Loss: 0.16995763778686523\n",
      "Epoch 2623, Loss: 0.34739941358566284, Final Batch Loss: 0.1622653305530548\n",
      "Epoch 2624, Loss: 0.34212666749954224, Final Batch Loss: 0.2026566118001938\n",
      "Epoch 2625, Loss: 0.309715136885643, Final Batch Loss: 0.1587660014629364\n",
      "Epoch 2626, Loss: 0.3801014721393585, Final Batch Loss: 0.2289712280035019\n",
      "Epoch 2627, Loss: 0.32664354145526886, Final Batch Loss: 0.15589697659015656\n",
      "Epoch 2628, Loss: 0.29965364933013916, Final Batch Loss: 0.18528389930725098\n",
      "Epoch 2629, Loss: 0.2981511950492859, Final Batch Loss: 0.13790461421012878\n",
      "Epoch 2630, Loss: 0.41558246314525604, Final Batch Loss: 0.23512594401836395\n",
      "Epoch 2631, Loss: 0.32748933136463165, Final Batch Loss: 0.16408082842826843\n",
      "Epoch 2632, Loss: 0.34968437254428864, Final Batch Loss: 0.17414459586143494\n",
      "Epoch 2633, Loss: 0.302880123257637, Final Batch Loss: 0.13567306101322174\n",
      "Epoch 2634, Loss: 0.31488534808158875, Final Batch Loss: 0.1375836879014969\n",
      "Epoch 2635, Loss: 0.33625026047229767, Final Batch Loss: 0.14820615947246552\n",
      "Epoch 2636, Loss: 0.3397253006696701, Final Batch Loss: 0.14401987195014954\n",
      "Epoch 2637, Loss: 0.2868831008672714, Final Batch Loss: 0.12565326690673828\n",
      "Epoch 2638, Loss: 0.3469863384962082, Final Batch Loss: 0.2152349054813385\n",
      "Epoch 2639, Loss: 0.3582736998796463, Final Batch Loss: 0.19825926423072815\n",
      "Epoch 2640, Loss: 0.3373215049505234, Final Batch Loss: 0.20269560813903809\n",
      "Epoch 2641, Loss: 0.35650859773159027, Final Batch Loss: 0.15253163874149323\n",
      "Epoch 2642, Loss: 0.3308507204055786, Final Batch Loss: 0.18190522491931915\n",
      "Epoch 2643, Loss: 0.3362349271774292, Final Batch Loss: 0.1580815315246582\n",
      "Epoch 2644, Loss: 0.3186517804861069, Final Batch Loss: 0.16597682237625122\n",
      "Epoch 2645, Loss: 0.2836696356534958, Final Batch Loss: 0.13896749913692474\n",
      "Epoch 2646, Loss: 0.3420663923025131, Final Batch Loss: 0.14212381839752197\n",
      "Epoch 2647, Loss: 0.2890934646129608, Final Batch Loss: 0.12017981708049774\n",
      "Epoch 2648, Loss: 0.3260134607553482, Final Batch Loss: 0.1695353388786316\n",
      "Epoch 2649, Loss: 0.2994469478726387, Final Batch Loss: 0.10875757783651352\n",
      "Epoch 2650, Loss: 0.3000614643096924, Final Batch Loss: 0.14880837500095367\n",
      "Epoch 2651, Loss: 0.35094109177589417, Final Batch Loss: 0.15766660869121552\n",
      "Epoch 2652, Loss: 0.344990536570549, Final Batch Loss: 0.18469120562076569\n",
      "Epoch 2653, Loss: 0.3043045550584793, Final Batch Loss: 0.15215711295604706\n",
      "Epoch 2654, Loss: 0.29077883064746857, Final Batch Loss: 0.12690535187721252\n",
      "Epoch 2655, Loss: 0.29680661857128143, Final Batch Loss: 0.13850806653499603\n",
      "Epoch 2656, Loss: 0.29128792881965637, Final Batch Loss: 0.13199500739574432\n",
      "Epoch 2657, Loss: 0.3033524751663208, Final Batch Loss: 0.1523214727640152\n",
      "Epoch 2658, Loss: 0.3372911810874939, Final Batch Loss: 0.1978064477443695\n",
      "Epoch 2659, Loss: 0.3693414777517319, Final Batch Loss: 0.225881427526474\n",
      "Epoch 2660, Loss: 0.3784678727388382, Final Batch Loss: 0.22891151905059814\n",
      "Epoch 2661, Loss: 0.2834566533565521, Final Batch Loss: 0.13867616653442383\n",
      "Epoch 2662, Loss: 0.34458889067173004, Final Batch Loss: 0.17493444681167603\n",
      "Epoch 2663, Loss: 0.3547200411558151, Final Batch Loss: 0.16755282878875732\n",
      "Epoch 2664, Loss: 0.30665096640586853, Final Batch Loss: 0.13087710738182068\n",
      "Epoch 2665, Loss: 0.36171072721481323, Final Batch Loss: 0.18007034063339233\n",
      "Epoch 2666, Loss: 0.2876894325017929, Final Batch Loss: 0.10755379498004913\n",
      "Epoch 2667, Loss: 0.3480219841003418, Final Batch Loss: 0.19828137755393982\n",
      "Epoch 2668, Loss: 0.45474711060523987, Final Batch Loss: 0.30789914727211\n",
      "Epoch 2669, Loss: 0.32285209000110626, Final Batch Loss: 0.16570162773132324\n",
      "Epoch 2670, Loss: 0.3293742686510086, Final Batch Loss: 0.16872721910476685\n",
      "Epoch 2671, Loss: 0.36686818301677704, Final Batch Loss: 0.2138996124267578\n",
      "Epoch 2672, Loss: 0.340436726808548, Final Batch Loss: 0.16079798340797424\n",
      "Epoch 2673, Loss: 0.32907626032829285, Final Batch Loss: 0.16881953179836273\n",
      "Epoch 2674, Loss: 0.3516162484884262, Final Batch Loss: 0.18823587894439697\n",
      "Epoch 2675, Loss: 0.3191068172454834, Final Batch Loss: 0.16537432372570038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2676, Loss: 0.33749280869960785, Final Batch Loss: 0.152371346950531\n",
      "Epoch 2677, Loss: 0.30041003227233887, Final Batch Loss: 0.13269087672233582\n",
      "Epoch 2678, Loss: 0.3039423078298569, Final Batch Loss: 0.139414444565773\n",
      "Epoch 2679, Loss: 0.3074651062488556, Final Batch Loss: 0.15228842198848724\n",
      "Epoch 2680, Loss: 0.32479020953178406, Final Batch Loss: 0.1665297895669937\n",
      "Epoch 2681, Loss: 0.29619865119457245, Final Batch Loss: 0.14790809154510498\n",
      "Epoch 2682, Loss: 0.3031740039587021, Final Batch Loss: 0.1293836086988449\n",
      "Epoch 2683, Loss: 0.3958967477083206, Final Batch Loss: 0.21286356449127197\n",
      "Epoch 2684, Loss: 0.282321497797966, Final Batch Loss: 0.1390598863363266\n",
      "Epoch 2685, Loss: 0.3007839471101761, Final Batch Loss: 0.15827566385269165\n",
      "Epoch 2686, Loss: 0.30376259982585907, Final Batch Loss: 0.15948368608951569\n",
      "Epoch 2687, Loss: 0.3262690007686615, Final Batch Loss: 0.1775742918252945\n",
      "Epoch 2688, Loss: 0.32091693580150604, Final Batch Loss: 0.13830061256885529\n",
      "Epoch 2689, Loss: 0.28197646141052246, Final Batch Loss: 0.13934326171875\n",
      "Epoch 2690, Loss: 0.3384243994951248, Final Batch Loss: 0.168837308883667\n",
      "Epoch 2691, Loss: 0.325937956571579, Final Batch Loss: 0.1642356663942337\n",
      "Epoch 2692, Loss: 0.30171965062618256, Final Batch Loss: 0.12824119627475739\n",
      "Epoch 2693, Loss: 0.35998596251010895, Final Batch Loss: 0.1983269602060318\n",
      "Epoch 2694, Loss: 0.3606436401605606, Final Batch Loss: 0.18874509632587433\n",
      "Epoch 2695, Loss: 0.36138536036014557, Final Batch Loss: 0.22159446775913239\n",
      "Epoch 2696, Loss: 0.30233754217624664, Final Batch Loss: 0.13599058985710144\n",
      "Epoch 2697, Loss: 0.31255076825618744, Final Batch Loss: 0.16883257031440735\n",
      "Epoch 2698, Loss: 0.3229817897081375, Final Batch Loss: 0.1589440554380417\n",
      "Epoch 2699, Loss: 0.32748566567897797, Final Batch Loss: 0.1626347452402115\n",
      "Epoch 2700, Loss: 0.3297668695449829, Final Batch Loss: 0.16762366890907288\n",
      "Epoch 2701, Loss: 0.3435424417257309, Final Batch Loss: 0.17112335562705994\n",
      "Epoch 2702, Loss: 0.3331472724676132, Final Batch Loss: 0.17597219347953796\n",
      "Epoch 2703, Loss: 0.3192700445652008, Final Batch Loss: 0.17149578034877777\n",
      "Epoch 2704, Loss: 0.3292687237262726, Final Batch Loss: 0.12542824447155\n",
      "Epoch 2705, Loss: 0.3070572763681412, Final Batch Loss: 0.14149826765060425\n",
      "Epoch 2706, Loss: 0.3657398521900177, Final Batch Loss: 0.1917102038860321\n",
      "Epoch 2707, Loss: 0.32082902640104294, Final Batch Loss: 0.11533962935209274\n",
      "Epoch 2708, Loss: 0.328224778175354, Final Batch Loss: 0.1648252159357071\n",
      "Epoch 2709, Loss: 0.29857879877090454, Final Batch Loss: 0.15702466666698456\n",
      "Epoch 2710, Loss: 0.36837421357631683, Final Batch Loss: 0.19141605496406555\n",
      "Epoch 2711, Loss: 0.35651229321956635, Final Batch Loss: 0.14690512418746948\n",
      "Epoch 2712, Loss: 0.3203590214252472, Final Batch Loss: 0.1675218790769577\n",
      "Epoch 2713, Loss: 0.2982083857059479, Final Batch Loss: 0.1590336412191391\n",
      "Epoch 2714, Loss: 0.294918492436409, Final Batch Loss: 0.15158003568649292\n",
      "Epoch 2715, Loss: 0.35547472536563873, Final Batch Loss: 0.21316008269786835\n",
      "Epoch 2716, Loss: 0.33552582561969757, Final Batch Loss: 0.16649028658866882\n",
      "Epoch 2717, Loss: 0.345956951379776, Final Batch Loss: 0.1695975512266159\n",
      "Epoch 2718, Loss: 0.3432891219854355, Final Batch Loss: 0.1643582284450531\n",
      "Epoch 2719, Loss: 0.30492426455020905, Final Batch Loss: 0.1709451973438263\n",
      "Epoch 2720, Loss: 0.30061379075050354, Final Batch Loss: 0.13309966027736664\n",
      "Epoch 2721, Loss: 0.30779099464416504, Final Batch Loss: 0.14015409350395203\n",
      "Epoch 2722, Loss: 0.30952708423137665, Final Batch Loss: 0.13226062059402466\n",
      "Epoch 2723, Loss: 0.3432701379060745, Final Batch Loss: 0.17500437796115875\n",
      "Epoch 2724, Loss: 0.3210577517747879, Final Batch Loss: 0.14163784682750702\n",
      "Epoch 2725, Loss: 0.32851001620292664, Final Batch Loss: 0.1830131560564041\n",
      "Epoch 2726, Loss: 0.3298224210739136, Final Batch Loss: 0.19019198417663574\n",
      "Epoch 2727, Loss: 0.34024201333522797, Final Batch Loss: 0.1897299438714981\n",
      "Epoch 2728, Loss: 0.3085838705301285, Final Batch Loss: 0.16464008390903473\n",
      "Epoch 2729, Loss: 0.3754660040140152, Final Batch Loss: 0.20378419756889343\n",
      "Epoch 2730, Loss: 0.3348812609910965, Final Batch Loss: 0.16486386954784393\n",
      "Epoch 2731, Loss: 0.3579677641391754, Final Batch Loss: 0.18441413342952728\n",
      "Epoch 2732, Loss: 0.33714233338832855, Final Batch Loss: 0.19017404317855835\n",
      "Epoch 2733, Loss: 0.3313530534505844, Final Batch Loss: 0.16483597457408905\n",
      "Epoch 2734, Loss: 0.37195606529712677, Final Batch Loss: 0.2159232199192047\n",
      "Epoch 2735, Loss: 0.3519204556941986, Final Batch Loss: 0.17192760109901428\n",
      "Epoch 2736, Loss: 0.35577236115932465, Final Batch Loss: 0.20472659170627594\n",
      "Epoch 2737, Loss: 0.3523515462875366, Final Batch Loss: 0.2014654576778412\n",
      "Epoch 2738, Loss: 0.319156289100647, Final Batch Loss: 0.14078958332538605\n",
      "Epoch 2739, Loss: 0.31475697457790375, Final Batch Loss: 0.1877547651529312\n",
      "Epoch 2740, Loss: 0.3031354248523712, Final Batch Loss: 0.12875574827194214\n",
      "Epoch 2741, Loss: 0.2932567894458771, Final Batch Loss: 0.14236333966255188\n",
      "Epoch 2742, Loss: 0.34689703583717346, Final Batch Loss: 0.1603204309940338\n",
      "Epoch 2743, Loss: 0.30237235128879547, Final Batch Loss: 0.1681825965642929\n",
      "Epoch 2744, Loss: 0.30566778779029846, Final Batch Loss: 0.13488072156906128\n",
      "Epoch 2745, Loss: 0.3180978298187256, Final Batch Loss: 0.14362531900405884\n",
      "Epoch 2746, Loss: 0.30554066598415375, Final Batch Loss: 0.15044037997722626\n",
      "Epoch 2747, Loss: 0.37412506341934204, Final Batch Loss: 0.20299486815929413\n",
      "Epoch 2748, Loss: 0.3431093394756317, Final Batch Loss: 0.16959157586097717\n",
      "Epoch 2749, Loss: 0.28798559308052063, Final Batch Loss: 0.14494559168815613\n",
      "Epoch 2750, Loss: 0.327503502368927, Final Batch Loss: 0.16033756732940674\n",
      "Epoch 2751, Loss: 0.34613946080207825, Final Batch Loss: 0.2039627730846405\n",
      "Epoch 2752, Loss: 0.3148374408483505, Final Batch Loss: 0.1683184653520584\n",
      "Epoch 2753, Loss: 0.3778243660926819, Final Batch Loss: 0.22787249088287354\n",
      "Epoch 2754, Loss: 0.27298106998205185, Final Batch Loss: 0.11333230882883072\n",
      "Epoch 2755, Loss: 0.2953644096851349, Final Batch Loss: 0.12533675134181976\n",
      "Epoch 2756, Loss: 0.3544195890426636, Final Batch Loss: 0.19386349618434906\n",
      "Epoch 2757, Loss: 0.31820614635944366, Final Batch Loss: 0.16645467281341553\n",
      "Epoch 2758, Loss: 0.34294161200523376, Final Batch Loss: 0.14472632110118866\n",
      "Epoch 2759, Loss: 0.32061102986335754, Final Batch Loss: 0.14630408585071564\n",
      "Epoch 2760, Loss: 0.3605102449655533, Final Batch Loss: 0.18282443284988403\n",
      "Epoch 2761, Loss: 0.33970166742801666, Final Batch Loss: 0.16952495276927948\n",
      "Epoch 2762, Loss: 0.30973872542381287, Final Batch Loss: 0.1783330738544464\n",
      "Epoch 2763, Loss: 0.3255525678396225, Final Batch Loss: 0.16703832149505615\n",
      "Epoch 2764, Loss: 0.3196491599082947, Final Batch Loss: 0.12192434072494507\n",
      "Epoch 2765, Loss: 0.290327712893486, Final Batch Loss: 0.13879314064979553\n",
      "Epoch 2766, Loss: 0.2684510573744774, Final Batch Loss: 0.12181530147790909\n",
      "Epoch 2767, Loss: 0.3029610961675644, Final Batch Loss: 0.1251087486743927\n",
      "Epoch 2768, Loss: 0.3416517823934555, Final Batch Loss: 0.1547020822763443\n",
      "Epoch 2769, Loss: 0.30796581506729126, Final Batch Loss: 0.15808932483196259\n",
      "Epoch 2770, Loss: 0.3358345329761505, Final Batch Loss: 0.17477582395076752\n",
      "Epoch 2771, Loss: 0.2930179238319397, Final Batch Loss: 0.14736753702163696\n",
      "Epoch 2772, Loss: 0.3242683410644531, Final Batch Loss: 0.1656544953584671\n",
      "Epoch 2773, Loss: 0.3011690080165863, Final Batch Loss: 0.15572533011436462\n",
      "Epoch 2774, Loss: 0.3471778780221939, Final Batch Loss: 0.14117608964443207\n",
      "Epoch 2775, Loss: 0.30323004722595215, Final Batch Loss: 0.17030896246433258\n",
      "Epoch 2776, Loss: 0.3044540286064148, Final Batch Loss: 0.1512574553489685\n",
      "Epoch 2777, Loss: 0.2990341857075691, Final Batch Loss: 0.12152694910764694\n",
      "Epoch 2778, Loss: 0.34311017394065857, Final Batch Loss: 0.18607136607170105\n",
      "Epoch 2779, Loss: 0.3222452253103256, Final Batch Loss: 0.15126796066761017\n",
      "Epoch 2780, Loss: 0.31136347353458405, Final Batch Loss: 0.17493946850299835\n",
      "Epoch 2781, Loss: 0.31481001526117325, Final Batch Loss: 0.19083009660243988\n",
      "Epoch 2782, Loss: 0.3411199450492859, Final Batch Loss: 0.17477241158485413\n",
      "Epoch 2783, Loss: 0.29239721596241, Final Batch Loss: 0.14535629749298096\n",
      "Epoch 2784, Loss: 0.3325415849685669, Final Batch Loss: 0.18297117948532104\n",
      "Epoch 2785, Loss: 0.3633391112089157, Final Batch Loss: 0.19616924226284027\n",
      "Epoch 2786, Loss: 0.3114963620901108, Final Batch Loss: 0.13933447003364563\n",
      "Epoch 2787, Loss: 0.2890970706939697, Final Batch Loss: 0.14393286406993866\n",
      "Epoch 2788, Loss: 0.3264889270067215, Final Batch Loss: 0.184267058968544\n",
      "Epoch 2789, Loss: 0.3890784680843353, Final Batch Loss: 0.19816625118255615\n",
      "Epoch 2790, Loss: 0.2765168696641922, Final Batch Loss: 0.12322182953357697\n",
      "Epoch 2791, Loss: 0.3251087814569473, Final Batch Loss: 0.1481657773256302\n",
      "Epoch 2792, Loss: 0.3286278694868088, Final Batch Loss: 0.15967439115047455\n",
      "Epoch 2793, Loss: 0.3399847149848938, Final Batch Loss: 0.12064149975776672\n",
      "Epoch 2794, Loss: 0.33300480246543884, Final Batch Loss: 0.1732064187526703\n",
      "Epoch 2795, Loss: 0.3248763531446457, Final Batch Loss: 0.14434769749641418\n",
      "Epoch 2796, Loss: 0.31777718663215637, Final Batch Loss: 0.13496915996074677\n",
      "Epoch 2797, Loss: 0.3367205411195755, Final Batch Loss: 0.14262108504772186\n",
      "Epoch 2798, Loss: 0.332942470908165, Final Batch Loss: 0.17334848642349243\n",
      "Epoch 2799, Loss: 0.31634603440761566, Final Batch Loss: 0.13740308582782745\n",
      "Epoch 2800, Loss: 0.29409126937389374, Final Batch Loss: 0.14890344440937042\n",
      "Epoch 2801, Loss: 0.35928860306739807, Final Batch Loss: 0.20368511974811554\n",
      "Epoch 2802, Loss: 0.3273210674524307, Final Batch Loss: 0.19148463010787964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2803, Loss: 0.33656586706638336, Final Batch Loss: 0.15007422864437103\n",
      "Epoch 2804, Loss: 0.30820346623659134, Final Batch Loss: 0.11675667017698288\n",
      "Epoch 2805, Loss: 0.31690527498722076, Final Batch Loss: 0.17602171003818512\n",
      "Epoch 2806, Loss: 0.3697210103273392, Final Batch Loss: 0.202483132481575\n",
      "Epoch 2807, Loss: 0.35002341121435165, Final Batch Loss: 0.2257709950208664\n",
      "Epoch 2808, Loss: 0.34758682548999786, Final Batch Loss: 0.19253279268741608\n",
      "Epoch 2809, Loss: 0.3738870918750763, Final Batch Loss: 0.18391668796539307\n",
      "Epoch 2810, Loss: 0.39609576761722565, Final Batch Loss: 0.1844816356897354\n",
      "Epoch 2811, Loss: 0.3515186905860901, Final Batch Loss: 0.17246843874454498\n",
      "Epoch 2812, Loss: 0.309776246547699, Final Batch Loss: 0.15538381040096283\n",
      "Epoch 2813, Loss: 0.36586976051330566, Final Batch Loss: 0.2049979567527771\n",
      "Epoch 2814, Loss: 0.33897945284843445, Final Batch Loss: 0.1899416595697403\n",
      "Epoch 2815, Loss: 0.29850101470947266, Final Batch Loss: 0.1703088879585266\n",
      "Epoch 2816, Loss: 0.3091210797429085, Final Batch Loss: 0.1964438408613205\n",
      "Epoch 2817, Loss: 0.3061402887105942, Final Batch Loss: 0.1707618683576584\n",
      "Epoch 2818, Loss: 0.287174716591835, Final Batch Loss: 0.12681856751441956\n",
      "Epoch 2819, Loss: 0.29232172667980194, Final Batch Loss: 0.1455521434545517\n",
      "Epoch 2820, Loss: 0.3818029761314392, Final Batch Loss: 0.213910773396492\n",
      "Epoch 2821, Loss: 0.2966962903738022, Final Batch Loss: 0.152875617146492\n",
      "Epoch 2822, Loss: 0.3400309681892395, Final Batch Loss: 0.15887326002120972\n",
      "Epoch 2823, Loss: 0.38308680057525635, Final Batch Loss: 0.23710642755031586\n",
      "Epoch 2824, Loss: 0.3316735178232193, Final Batch Loss: 0.1854044795036316\n",
      "Epoch 2825, Loss: 0.38216185569763184, Final Batch Loss: 0.17835158109664917\n",
      "Epoch 2826, Loss: 0.29974427819252014, Final Batch Loss: 0.1392999142408371\n",
      "Epoch 2827, Loss: 0.29669521749019623, Final Batch Loss: 0.1270403116941452\n",
      "Epoch 2828, Loss: 0.332012802362442, Final Batch Loss: 0.1504855901002884\n",
      "Epoch 2829, Loss: 0.31111177802085876, Final Batch Loss: 0.151908278465271\n",
      "Epoch 2830, Loss: 0.31483030319213867, Final Batch Loss: 0.16044943034648895\n",
      "Epoch 2831, Loss: 0.3342380076646805, Final Batch Loss: 0.1601625233888626\n",
      "Epoch 2832, Loss: 0.3163229674100876, Final Batch Loss: 0.14041252434253693\n",
      "Epoch 2833, Loss: 0.30975203216075897, Final Batch Loss: 0.1731647253036499\n",
      "Epoch 2834, Loss: 0.33660559356212616, Final Batch Loss: 0.1296570599079132\n",
      "Epoch 2835, Loss: 0.338556706905365, Final Batch Loss: 0.2007754147052765\n",
      "Epoch 2836, Loss: 0.29199080169200897, Final Batch Loss: 0.14240124821662903\n",
      "Epoch 2837, Loss: 0.33062490820884705, Final Batch Loss: 0.17591942846775055\n",
      "Epoch 2838, Loss: 0.3070181757211685, Final Batch Loss: 0.1716604083776474\n",
      "Epoch 2839, Loss: 0.3271987736225128, Final Batch Loss: 0.13956978917121887\n",
      "Epoch 2840, Loss: 0.27947621047496796, Final Batch Loss: 0.14088287949562073\n",
      "Epoch 2841, Loss: 0.3561493009328842, Final Batch Loss: 0.20965160429477692\n",
      "Epoch 2842, Loss: 0.2734937220811844, Final Batch Loss: 0.14101475477218628\n",
      "Epoch 2843, Loss: 0.26945164799690247, Final Batch Loss: 0.12939117848873138\n",
      "Epoch 2844, Loss: 0.29288554191589355, Final Batch Loss: 0.13666485249996185\n",
      "Epoch 2845, Loss: 0.3333640992641449, Final Batch Loss: 0.1331527680158615\n",
      "Epoch 2846, Loss: 0.3901677429676056, Final Batch Loss: 0.18729953467845917\n",
      "Epoch 2847, Loss: 0.27799995988607407, Final Batch Loss: 0.12026029080152512\n",
      "Epoch 2848, Loss: 0.3285696506500244, Final Batch Loss: 0.18083181977272034\n",
      "Epoch 2849, Loss: 0.3354674130678177, Final Batch Loss: 0.17709767818450928\n",
      "Epoch 2850, Loss: 0.34728768467903137, Final Batch Loss: 0.14782306551933289\n",
      "Epoch 2851, Loss: 0.325712189078331, Final Batch Loss: 0.18408319354057312\n",
      "Epoch 2852, Loss: 0.3114282339811325, Final Batch Loss: 0.13979242742061615\n",
      "Epoch 2853, Loss: 0.31255288422107697, Final Batch Loss: 0.1821628212928772\n",
      "Epoch 2854, Loss: 0.3269924819469452, Final Batch Loss: 0.16170261800289154\n",
      "Epoch 2855, Loss: 0.2992664575576782, Final Batch Loss: 0.14327692985534668\n",
      "Epoch 2856, Loss: 0.38549821078777313, Final Batch Loss: 0.22045068442821503\n",
      "Epoch 2857, Loss: 0.265098936855793, Final Batch Loss: 0.11035219579935074\n",
      "Epoch 2858, Loss: 0.3246392160654068, Final Batch Loss: 0.1591370701789856\n",
      "Epoch 2859, Loss: 0.29091009497642517, Final Batch Loss: 0.16094262897968292\n",
      "Epoch 2860, Loss: 0.3121456354856491, Final Batch Loss: 0.16550669074058533\n",
      "Epoch 2861, Loss: 0.2782558351755142, Final Batch Loss: 0.12039045989513397\n",
      "Epoch 2862, Loss: 0.32454732060432434, Final Batch Loss: 0.18438692390918732\n",
      "Epoch 2863, Loss: 0.35731977969408035, Final Batch Loss: 0.2328096479177475\n",
      "Epoch 2864, Loss: 0.36253394186496735, Final Batch Loss: 0.17390486598014832\n",
      "Epoch 2865, Loss: 0.33568809926509857, Final Batch Loss: 0.16131366789340973\n",
      "Epoch 2866, Loss: 0.37326526641845703, Final Batch Loss: 0.1729704737663269\n",
      "Epoch 2867, Loss: 0.3037850558757782, Final Batch Loss: 0.17358742654323578\n",
      "Epoch 2868, Loss: 0.34399694204330444, Final Batch Loss: 0.16733168065547943\n",
      "Epoch 2869, Loss: 0.2785503715276718, Final Batch Loss: 0.11607532203197479\n",
      "Epoch 2870, Loss: 0.32194261252880096, Final Batch Loss: 0.15317793190479279\n",
      "Epoch 2871, Loss: 0.3441477566957474, Final Batch Loss: 0.14514119923114777\n",
      "Epoch 2872, Loss: 0.30500906705856323, Final Batch Loss: 0.14972734451293945\n",
      "Epoch 2873, Loss: 0.35108745098114014, Final Batch Loss: 0.17606325447559357\n",
      "Epoch 2874, Loss: 0.345890149474144, Final Batch Loss: 0.20178380608558655\n",
      "Epoch 2875, Loss: 0.4306306689977646, Final Batch Loss: 0.255770206451416\n",
      "Epoch 2876, Loss: 0.34834276139736176, Final Batch Loss: 0.18154053390026093\n",
      "Epoch 2877, Loss: 0.30991651117801666, Final Batch Loss: 0.15207819640636444\n",
      "Epoch 2878, Loss: 0.2990507334470749, Final Batch Loss: 0.15990011394023895\n",
      "Epoch 2879, Loss: 0.28137123584747314, Final Batch Loss: 0.13792650401592255\n",
      "Epoch 2880, Loss: 0.33235928416252136, Final Batch Loss: 0.17646914720535278\n",
      "Epoch 2881, Loss: 0.3481195420026779, Final Batch Loss: 0.15571920573711395\n",
      "Epoch 2882, Loss: 0.2885700762271881, Final Batch Loss: 0.1337849497795105\n",
      "Epoch 2883, Loss: 0.3416260927915573, Final Batch Loss: 0.203065425157547\n",
      "Epoch 2884, Loss: 0.3451053723692894, Final Batch Loss: 0.22763708233833313\n",
      "Epoch 2885, Loss: 0.3092564046382904, Final Batch Loss: 0.14647065103054047\n",
      "Epoch 2886, Loss: 0.2954960763454437, Final Batch Loss: 0.1395268440246582\n",
      "Epoch 2887, Loss: 0.2803103104233742, Final Batch Loss: 0.12073040753602982\n",
      "Epoch 2888, Loss: 0.31749626994132996, Final Batch Loss: 0.11845827102661133\n",
      "Epoch 2889, Loss: 0.28165262937545776, Final Batch Loss: 0.1326499581336975\n",
      "Epoch 2890, Loss: 0.3095625638961792, Final Batch Loss: 0.17616890370845795\n",
      "Epoch 2891, Loss: 0.32610877603292465, Final Batch Loss: 0.20258133113384247\n",
      "Epoch 2892, Loss: 0.3756977319717407, Final Batch Loss: 0.21617980301380157\n",
      "Epoch 2893, Loss: 0.31124842166900635, Final Batch Loss: 0.1350799798965454\n",
      "Epoch 2894, Loss: 0.28565192967653275, Final Batch Loss: 0.17016927897930145\n",
      "Epoch 2895, Loss: 0.30497415363788605, Final Batch Loss: 0.1636565923690796\n",
      "Epoch 2896, Loss: 0.35469281673431396, Final Batch Loss: 0.16776446998119354\n",
      "Epoch 2897, Loss: 0.3108877092599869, Final Batch Loss: 0.1568688452243805\n",
      "Epoch 2898, Loss: 0.26375144720077515, Final Batch Loss: 0.1308353841304779\n",
      "Epoch 2899, Loss: 0.3416690528392792, Final Batch Loss: 0.18740418553352356\n",
      "Epoch 2900, Loss: 0.2912196069955826, Final Batch Loss: 0.14856000244617462\n",
      "Epoch 2901, Loss: 0.2882201746106148, Final Batch Loss: 0.1193215474486351\n",
      "Epoch 2902, Loss: 0.35794293880462646, Final Batch Loss: 0.1616148203611374\n",
      "Epoch 2903, Loss: 0.4138687551021576, Final Batch Loss: 0.2257385551929474\n",
      "Epoch 2904, Loss: 0.3572446405887604, Final Batch Loss: 0.22385357320308685\n",
      "Epoch 2905, Loss: 0.3362140357494354, Final Batch Loss: 0.21818241477012634\n",
      "Epoch 2906, Loss: 0.33416807651519775, Final Batch Loss: 0.17919497191905975\n",
      "Epoch 2907, Loss: 0.28950032591819763, Final Batch Loss: 0.12625247240066528\n",
      "Epoch 2908, Loss: 0.2966720461845398, Final Batch Loss: 0.12825338542461395\n",
      "Epoch 2909, Loss: 0.39548924565315247, Final Batch Loss: 0.14856205880641937\n",
      "Epoch 2910, Loss: 0.29985976219177246, Final Batch Loss: 0.1562320739030838\n",
      "Epoch 2911, Loss: 0.31106333434581757, Final Batch Loss: 0.1701396107673645\n",
      "Epoch 2912, Loss: 0.3125739097595215, Final Batch Loss: 0.1568242907524109\n",
      "Epoch 2913, Loss: 0.27559328079223633, Final Batch Loss: 0.11975358426570892\n",
      "Epoch 2914, Loss: 0.2926391363143921, Final Batch Loss: 0.1271190345287323\n",
      "Epoch 2915, Loss: 0.2761087268590927, Final Batch Loss: 0.10330398380756378\n",
      "Epoch 2916, Loss: 0.3058687299489975, Final Batch Loss: 0.15050959587097168\n",
      "Epoch 2917, Loss: 0.33860310912132263, Final Batch Loss: 0.1944020390510559\n",
      "Epoch 2918, Loss: 0.30357952415943146, Final Batch Loss: 0.17028473317623138\n",
      "Epoch 2919, Loss: 0.3084796816110611, Final Batch Loss: 0.1255633682012558\n",
      "Epoch 2920, Loss: 0.35536202788352966, Final Batch Loss: 0.20121294260025024\n",
      "Epoch 2921, Loss: 0.3001079708337784, Final Batch Loss: 0.1563451886177063\n",
      "Epoch 2922, Loss: 0.2820299416780472, Final Batch Loss: 0.13605505228042603\n",
      "Epoch 2923, Loss: 0.34010569006204605, Final Batch Loss: 0.21794535219669342\n",
      "Epoch 2924, Loss: 0.28079260885715485, Final Batch Loss: 0.1468903124332428\n",
      "Epoch 2925, Loss: 0.2652729004621506, Final Batch Loss: 0.1229124367237091\n",
      "Epoch 2926, Loss: 0.3272928074002266, Final Batch Loss: 0.09869157522916794\n",
      "Epoch 2927, Loss: 0.3186771124601364, Final Batch Loss: 0.16943274438381195\n",
      "Epoch 2928, Loss: 0.36277346312999725, Final Batch Loss: 0.2001921683549881\n",
      "Epoch 2929, Loss: 0.3067476451396942, Final Batch Loss: 0.12835638225078583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2930, Loss: 0.3044571429491043, Final Batch Loss: 0.16091753542423248\n",
      "Epoch 2931, Loss: 0.2825697362422943, Final Batch Loss: 0.14081016182899475\n",
      "Epoch 2932, Loss: 0.31111471354961395, Final Batch Loss: 0.170412078499794\n",
      "Epoch 2933, Loss: 0.31909145414829254, Final Batch Loss: 0.17501986026763916\n",
      "Epoch 2934, Loss: 0.26719895005226135, Final Batch Loss: 0.11278973519802094\n",
      "Epoch 2935, Loss: 0.3778667747974396, Final Batch Loss: 0.2369101345539093\n",
      "Epoch 2936, Loss: 0.3108063042163849, Final Batch Loss: 0.16408082842826843\n",
      "Epoch 2937, Loss: 0.3240971118211746, Final Batch Loss: 0.14686650037765503\n",
      "Epoch 2938, Loss: 0.31574125587940216, Final Batch Loss: 0.17407046258449554\n",
      "Epoch 2939, Loss: 0.30069054663181305, Final Batch Loss: 0.1840636134147644\n",
      "Epoch 2940, Loss: 0.2903222441673279, Final Batch Loss: 0.11134809255599976\n",
      "Epoch 2941, Loss: 0.3654933422803879, Final Batch Loss: 0.190440833568573\n",
      "Epoch 2942, Loss: 0.34882137179374695, Final Batch Loss: 0.1937219202518463\n",
      "Epoch 2943, Loss: 0.28537289798259735, Final Batch Loss: 0.15599657595157623\n",
      "Epoch 2944, Loss: 0.30695536732673645, Final Batch Loss: 0.1654520034790039\n",
      "Epoch 2945, Loss: 0.2883577346801758, Final Batch Loss: 0.11347700655460358\n",
      "Epoch 2946, Loss: 0.30866436660289764, Final Batch Loss: 0.1649361103773117\n",
      "Epoch 2947, Loss: 0.2836174890398979, Final Batch Loss: 0.11083688586950302\n",
      "Epoch 2948, Loss: 0.3508186936378479, Final Batch Loss: 0.2025999277830124\n",
      "Epoch 2949, Loss: 0.37520933151245117, Final Batch Loss: 0.23283393681049347\n",
      "Epoch 2950, Loss: 0.3546789437532425, Final Batch Loss: 0.1684124767780304\n",
      "Epoch 2951, Loss: 0.3564024120569229, Final Batch Loss: 0.2055370956659317\n",
      "Epoch 2952, Loss: 0.3147550970315933, Final Batch Loss: 0.16765262186527252\n",
      "Epoch 2953, Loss: 0.2945147007703781, Final Batch Loss: 0.13738314807415009\n",
      "Epoch 2954, Loss: 0.31226809322834015, Final Batch Loss: 0.16072018444538116\n",
      "Epoch 2955, Loss: 0.31901316344738007, Final Batch Loss: 0.16285233199596405\n",
      "Epoch 2956, Loss: 0.31704625487327576, Final Batch Loss: 0.1665683090686798\n",
      "Epoch 2957, Loss: 0.3125441372394562, Final Batch Loss: 0.1291007399559021\n",
      "Epoch 2958, Loss: 0.307579830288887, Final Batch Loss: 0.1387811303138733\n",
      "Epoch 2959, Loss: 0.35942016541957855, Final Batch Loss: 0.1832016110420227\n",
      "Epoch 2960, Loss: 0.26363611221313477, Final Batch Loss: 0.11969104409217834\n",
      "Epoch 2961, Loss: 0.3905683904886246, Final Batch Loss: 0.22103816270828247\n",
      "Epoch 2962, Loss: 0.3442859798669815, Final Batch Loss: 0.18921659886837006\n",
      "Epoch 2963, Loss: 0.2963785231113434, Final Batch Loss: 0.1689833253622055\n",
      "Epoch 2964, Loss: 0.3990614116191864, Final Batch Loss: 0.25143304467201233\n",
      "Epoch 2965, Loss: 0.34627608954906464, Final Batch Loss: 0.2155507355928421\n",
      "Epoch 2966, Loss: 0.28000491857528687, Final Batch Loss: 0.1520044505596161\n",
      "Epoch 2967, Loss: 0.3117964267730713, Final Batch Loss: 0.17629359662532806\n",
      "Epoch 2968, Loss: 0.28213898837566376, Final Batch Loss: 0.12670807540416718\n",
      "Epoch 2969, Loss: 0.3331109285354614, Final Batch Loss: 0.1803353875875473\n",
      "Epoch 2970, Loss: 0.33770206570625305, Final Batch Loss: 0.1642068475484848\n",
      "Epoch 2971, Loss: 0.28670740872621536, Final Batch Loss: 0.12243968993425369\n",
      "Epoch 2972, Loss: 0.3104899078607559, Final Batch Loss: 0.15951737761497498\n",
      "Epoch 2973, Loss: 0.3040962293744087, Final Batch Loss: 0.122355155646801\n",
      "Epoch 2974, Loss: 0.3322007954120636, Final Batch Loss: 0.1787700653076172\n",
      "Epoch 2975, Loss: 0.33033473789691925, Final Batch Loss: 0.150906503200531\n",
      "Epoch 2976, Loss: 0.30235670506954193, Final Batch Loss: 0.15964584052562714\n",
      "Epoch 2977, Loss: 0.361291840672493, Final Batch Loss: 0.17020025849342346\n",
      "Epoch 2978, Loss: 0.3422228991985321, Final Batch Loss: 0.15878483653068542\n",
      "Epoch 2979, Loss: 0.30540631711483, Final Batch Loss: 0.1457865834236145\n",
      "Epoch 2980, Loss: 0.2710423320531845, Final Batch Loss: 0.13949106633663177\n",
      "Epoch 2981, Loss: 0.31716418266296387, Final Batch Loss: 0.17279449105262756\n",
      "Epoch 2982, Loss: 0.2697299048304558, Final Batch Loss: 0.11412467807531357\n",
      "Epoch 2983, Loss: 0.32869887351989746, Final Batch Loss: 0.13997386395931244\n",
      "Epoch 2984, Loss: 0.32072336971759796, Final Batch Loss: 0.14290907979011536\n",
      "Epoch 2985, Loss: 0.287906751036644, Final Batch Loss: 0.13539251685142517\n",
      "Epoch 2986, Loss: 0.3103802800178528, Final Batch Loss: 0.1252116709947586\n",
      "Epoch 2987, Loss: 0.31084510684013367, Final Batch Loss: 0.17877590656280518\n",
      "Epoch 2988, Loss: 0.3501574546098709, Final Batch Loss: 0.14353898167610168\n",
      "Epoch 2989, Loss: 0.31530578434467316, Final Batch Loss: 0.16008248925209045\n",
      "Epoch 2990, Loss: 0.32488520443439484, Final Batch Loss: 0.1927259862422943\n",
      "Epoch 2991, Loss: 0.33995482325553894, Final Batch Loss: 0.1642477959394455\n",
      "Epoch 2992, Loss: 0.29238009452819824, Final Batch Loss: 0.13218535482883453\n",
      "Epoch 2993, Loss: 0.3979376554489136, Final Batch Loss: 0.20543667674064636\n",
      "Epoch 2994, Loss: 0.29397526383399963, Final Batch Loss: 0.1402205377817154\n",
      "Epoch 2995, Loss: 0.27620796859264374, Final Batch Loss: 0.12853692471981049\n",
      "Epoch 2996, Loss: 0.3052842319011688, Final Batch Loss: 0.13121476769447327\n",
      "Epoch 2997, Loss: 0.26932794600725174, Final Batch Loss: 0.15082162618637085\n",
      "Epoch 2998, Loss: 0.36063826084136963, Final Batch Loss: 0.19379833340644836\n",
      "Epoch 2999, Loss: 0.3080509603023529, Final Batch Loss: 0.15895527601242065\n",
      "Epoch 3000, Loss: 0.25795529037714005, Final Batch Loss: 0.0923168733716011\n",
      "Epoch 3001, Loss: 0.2771947979927063, Final Batch Loss: 0.11372244358062744\n",
      "Epoch 3002, Loss: 0.3921581953763962, Final Batch Loss: 0.18487735092639923\n",
      "Epoch 3003, Loss: 0.2702483460307121, Final Batch Loss: 0.1239762231707573\n",
      "Epoch 3004, Loss: 0.34051038324832916, Final Batch Loss: 0.14380981028079987\n",
      "Epoch 3005, Loss: 0.2979952096939087, Final Batch Loss: 0.1518200784921646\n",
      "Epoch 3006, Loss: 0.2953566312789917, Final Batch Loss: 0.15787221491336823\n",
      "Epoch 3007, Loss: 0.33657853305339813, Final Batch Loss: 0.19330574572086334\n",
      "Epoch 3008, Loss: 0.29063835740089417, Final Batch Loss: 0.15031051635742188\n",
      "Epoch 3009, Loss: 0.288952574133873, Final Batch Loss: 0.17418405413627625\n",
      "Epoch 3010, Loss: 0.3343546688556671, Final Batch Loss: 0.17643757164478302\n",
      "Epoch 3011, Loss: 0.29977160692214966, Final Batch Loss: 0.17455735802650452\n",
      "Epoch 3012, Loss: 0.32710273563861847, Final Batch Loss: 0.1444646269083023\n",
      "Epoch 3013, Loss: 0.3105907142162323, Final Batch Loss: 0.1803526133298874\n",
      "Epoch 3014, Loss: 0.30267973244190216, Final Batch Loss: 0.1287296861410141\n",
      "Epoch 3015, Loss: 0.2806660085916519, Final Batch Loss: 0.13956762850284576\n",
      "Epoch 3016, Loss: 0.33387894928455353, Final Batch Loss: 0.20090238749980927\n",
      "Epoch 3017, Loss: 0.29201993346214294, Final Batch Loss: 0.1415432095527649\n",
      "Epoch 3018, Loss: 0.26988888531923294, Final Batch Loss: 0.10820800811052322\n",
      "Epoch 3019, Loss: 0.3387608826160431, Final Batch Loss: 0.18020974099636078\n",
      "Epoch 3020, Loss: 0.3091702312231064, Final Batch Loss: 0.1336219310760498\n",
      "Epoch 3021, Loss: 0.2954262048006058, Final Batch Loss: 0.15451626479625702\n",
      "Epoch 3022, Loss: 0.32808589935302734, Final Batch Loss: 0.17610211670398712\n",
      "Epoch 3023, Loss: 0.2599368467926979, Final Batch Loss: 0.12497403472661972\n",
      "Epoch 3024, Loss: 0.2877253443002701, Final Batch Loss: 0.1205604076385498\n",
      "Epoch 3025, Loss: 0.2796701341867447, Final Batch Loss: 0.12647119164466858\n",
      "Epoch 3026, Loss: 0.298036128282547, Final Batch Loss: 0.16380564868450165\n",
      "Epoch 3027, Loss: 0.28484514355659485, Final Batch Loss: 0.13888292014598846\n",
      "Epoch 3028, Loss: 0.2587229087948799, Final Batch Loss: 0.12066715210676193\n",
      "Epoch 3029, Loss: 0.372841939330101, Final Batch Loss: 0.2031574845314026\n",
      "Epoch 3030, Loss: 0.34149886667728424, Final Batch Loss: 0.21814973652362823\n",
      "Epoch 3031, Loss: 0.26239532232284546, Final Batch Loss: 0.11171390116214752\n",
      "Epoch 3032, Loss: 0.2734462320804596, Final Batch Loss: 0.1381688416004181\n",
      "Epoch 3033, Loss: 0.2774226516485214, Final Batch Loss: 0.15253131091594696\n",
      "Epoch 3034, Loss: 0.311573788523674, Final Batch Loss: 0.17034485936164856\n",
      "Epoch 3035, Loss: 0.32751205563545227, Final Batch Loss: 0.14954429864883423\n",
      "Epoch 3036, Loss: 0.32168304920196533, Final Batch Loss: 0.17898927628993988\n",
      "Epoch 3037, Loss: 0.41859257221221924, Final Batch Loss: 0.23975712060928345\n",
      "Epoch 3038, Loss: 0.295252725481987, Final Batch Loss: 0.14961981773376465\n",
      "Epoch 3039, Loss: 0.30539968609809875, Final Batch Loss: 0.1290590614080429\n",
      "Epoch 3040, Loss: 0.3268241137266159, Final Batch Loss: 0.17869850993156433\n",
      "Epoch 3041, Loss: 0.26591020822525024, Final Batch Loss: 0.13890814781188965\n",
      "Epoch 3042, Loss: 0.3076373189687729, Final Batch Loss: 0.17356474697589874\n",
      "Epoch 3043, Loss: 0.36435505747795105, Final Batch Loss: 0.12012481689453125\n",
      "Epoch 3044, Loss: 0.25270485132932663, Final Batch Loss: 0.13943104445934296\n",
      "Epoch 3045, Loss: 0.33033058047294617, Final Batch Loss: 0.1820453256368637\n",
      "Epoch 3046, Loss: 0.3264387547969818, Final Batch Loss: 0.15802761912345886\n",
      "Epoch 3047, Loss: 0.3073796033859253, Final Batch Loss: 0.15248504281044006\n",
      "Epoch 3048, Loss: 0.2958704084157944, Final Batch Loss: 0.12174005806446075\n",
      "Epoch 3049, Loss: 0.3358750641345978, Final Batch Loss: 0.12589223682880402\n",
      "Epoch 3050, Loss: 0.33354808390140533, Final Batch Loss: 0.14712247252464294\n",
      "Epoch 3051, Loss: 0.3267522007226944, Final Batch Loss: 0.16937558352947235\n",
      "Epoch 3052, Loss: 0.30336445569992065, Final Batch Loss: 0.14545005559921265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3053, Loss: 0.3247643858194351, Final Batch Loss: 0.13854248821735382\n",
      "Epoch 3054, Loss: 0.28902801871299744, Final Batch Loss: 0.13960431516170502\n",
      "Epoch 3055, Loss: 0.2793333902955055, Final Batch Loss: 0.11979389935731888\n",
      "Epoch 3056, Loss: 0.31736861169338226, Final Batch Loss: 0.13446608185768127\n",
      "Epoch 3057, Loss: 0.35270705819129944, Final Batch Loss: 0.19330596923828125\n",
      "Epoch 3058, Loss: 0.2901320457458496, Final Batch Loss: 0.13541948795318604\n",
      "Epoch 3059, Loss: 0.2883429676294327, Final Batch Loss: 0.1514834612607956\n",
      "Epoch 3060, Loss: 0.3083884194493294, Final Batch Loss: 0.12445082515478134\n",
      "Epoch 3061, Loss: 0.31183117628097534, Final Batch Loss: 0.13293218612670898\n",
      "Epoch 3062, Loss: 0.31254737079143524, Final Batch Loss: 0.18115699291229248\n",
      "Epoch 3063, Loss: 0.31698401272296906, Final Batch Loss: 0.17562083899974823\n",
      "Epoch 3064, Loss: 0.3107563406229019, Final Batch Loss: 0.17545810341835022\n",
      "Epoch 3065, Loss: 0.2911565750837326, Final Batch Loss: 0.12422601878643036\n",
      "Epoch 3066, Loss: 0.30380867421627045, Final Batch Loss: 0.17850592732429504\n",
      "Epoch 3067, Loss: 0.2736096978187561, Final Batch Loss: 0.1560351401567459\n",
      "Epoch 3068, Loss: 0.3110850602388382, Final Batch Loss: 0.11016213893890381\n",
      "Epoch 3069, Loss: 0.3106871545314789, Final Batch Loss: 0.1741175502538681\n",
      "Epoch 3070, Loss: 0.3283957540988922, Final Batch Loss: 0.14097025990486145\n",
      "Epoch 3071, Loss: 0.2745572477579117, Final Batch Loss: 0.11931690573692322\n",
      "Epoch 3072, Loss: 0.2547169104218483, Final Batch Loss: 0.1301453858613968\n",
      "Epoch 3073, Loss: 0.27489595115184784, Final Batch Loss: 0.16311703622341156\n",
      "Epoch 3074, Loss: 0.29994864761829376, Final Batch Loss: 0.15698453783988953\n",
      "Epoch 3075, Loss: 0.3195289522409439, Final Batch Loss: 0.17026644945144653\n",
      "Epoch 3076, Loss: 0.2987344264984131, Final Batch Loss: 0.12800472974777222\n",
      "Epoch 3077, Loss: 0.39000026881694794, Final Batch Loss: 0.20300890505313873\n",
      "Epoch 3078, Loss: 0.28932207077741623, Final Batch Loss: 0.12261015921831131\n",
      "Epoch 3079, Loss: 0.28201448917388916, Final Batch Loss: 0.1422920674085617\n",
      "Epoch 3080, Loss: 0.26784598082304, Final Batch Loss: 0.14889751374721527\n",
      "Epoch 3081, Loss: 0.2908612787723541, Final Batch Loss: 0.16620798408985138\n",
      "Epoch 3082, Loss: 0.2890596240758896, Final Batch Loss: 0.13326604664325714\n",
      "Epoch 3083, Loss: 0.3151077553629875, Final Batch Loss: 0.1078319326043129\n",
      "Epoch 3084, Loss: 0.27126894891262054, Final Batch Loss: 0.12574303150177002\n",
      "Epoch 3085, Loss: 0.325520858168602, Final Batch Loss: 0.14088116586208344\n",
      "Epoch 3086, Loss: 0.3310161978006363, Final Batch Loss: 0.15413735806941986\n",
      "Epoch 3087, Loss: 0.3340674787759781, Final Batch Loss: 0.16004014015197754\n",
      "Epoch 3088, Loss: 0.2784351110458374, Final Batch Loss: 0.14760112762451172\n",
      "Epoch 3089, Loss: 0.28998686373233795, Final Batch Loss: 0.14934678375720978\n",
      "Epoch 3090, Loss: 0.31162282824516296, Final Batch Loss: 0.13100633025169373\n",
      "Epoch 3091, Loss: 0.29492495208978653, Final Batch Loss: 0.17375509440898895\n",
      "Epoch 3092, Loss: 0.29791194945573807, Final Batch Loss: 0.12480560690164566\n",
      "Epoch 3093, Loss: 0.3007274344563484, Final Batch Loss: 0.11891178041696548\n",
      "Epoch 3094, Loss: 0.30783650279045105, Final Batch Loss: 0.13984546065330505\n",
      "Epoch 3095, Loss: 0.3364078998565674, Final Batch Loss: 0.19092939794063568\n",
      "Epoch 3096, Loss: 0.2988085597753525, Final Batch Loss: 0.11028629541397095\n",
      "Epoch 3097, Loss: 0.2624911591410637, Final Batch Loss: 0.15445619821548462\n",
      "Epoch 3098, Loss: 0.3346190005540848, Final Batch Loss: 0.17329451441764832\n",
      "Epoch 3099, Loss: 0.33189669251441956, Final Batch Loss: 0.16460798680782318\n",
      "Epoch 3100, Loss: 0.3239443898200989, Final Batch Loss: 0.1594780683517456\n",
      "Epoch 3101, Loss: 0.3210127353668213, Final Batch Loss: 0.14790932834148407\n",
      "Epoch 3102, Loss: 0.317213237285614, Final Batch Loss: 0.11548154056072235\n",
      "Epoch 3103, Loss: 0.26959484815597534, Final Batch Loss: 0.12453027069568634\n",
      "Epoch 3104, Loss: 0.3097330704331398, Final Batch Loss: 0.18995727598667145\n",
      "Epoch 3105, Loss: 0.29387399554252625, Final Batch Loss: 0.12848633527755737\n",
      "Epoch 3106, Loss: 0.3378969579935074, Final Batch Loss: 0.15704700350761414\n",
      "Epoch 3107, Loss: 0.3412088006734848, Final Batch Loss: 0.1790880560874939\n",
      "Epoch 3108, Loss: 0.3082168251276016, Final Batch Loss: 0.15937678515911102\n",
      "Epoch 3109, Loss: 0.3181534558534622, Final Batch Loss: 0.16249661147594452\n",
      "Epoch 3110, Loss: 0.28439608216285706, Final Batch Loss: 0.14565418660640717\n",
      "Epoch 3111, Loss: 0.26362308114767075, Final Batch Loss: 0.10151758044958115\n",
      "Epoch 3112, Loss: 0.3089617192745209, Final Batch Loss: 0.16597406566143036\n",
      "Epoch 3113, Loss: 0.35443779826164246, Final Batch Loss: 0.19592911005020142\n",
      "Epoch 3114, Loss: 0.3300144821405411, Final Batch Loss: 0.19856369495391846\n",
      "Epoch 3115, Loss: 0.2957955449819565, Final Batch Loss: 0.13307078182697296\n",
      "Epoch 3116, Loss: 0.2954370230436325, Final Batch Loss: 0.15220126509666443\n",
      "Epoch 3117, Loss: 0.27695760130882263, Final Batch Loss: 0.12685640156269073\n",
      "Epoch 3118, Loss: 0.31517720222473145, Final Batch Loss: 0.16971689462661743\n",
      "Epoch 3119, Loss: 0.32408158481121063, Final Batch Loss: 0.169983372092247\n",
      "Epoch 3120, Loss: 0.3308647722005844, Final Batch Loss: 0.15364880859851837\n",
      "Epoch 3121, Loss: 0.31304746866226196, Final Batch Loss: 0.13499115407466888\n",
      "Epoch 3122, Loss: 0.36470192670822144, Final Batch Loss: 0.20350389182567596\n",
      "Epoch 3123, Loss: 0.3377481549978256, Final Batch Loss: 0.16294047236442566\n",
      "Epoch 3124, Loss: 0.35635635256767273, Final Batch Loss: 0.1803547441959381\n",
      "Epoch 3125, Loss: 0.2926994413137436, Final Batch Loss: 0.16278669238090515\n",
      "Epoch 3126, Loss: 0.39018414914608, Final Batch Loss: 0.23059673607349396\n",
      "Epoch 3127, Loss: 0.3069862425327301, Final Batch Loss: 0.17944097518920898\n",
      "Epoch 3128, Loss: 0.28701071441173553, Final Batch Loss: 0.12441644072532654\n",
      "Epoch 3129, Loss: 0.307515412569046, Final Batch Loss: 0.1675225794315338\n",
      "Epoch 3130, Loss: 0.34062056243419647, Final Batch Loss: 0.19691529870033264\n",
      "Epoch 3131, Loss: 0.2834962159395218, Final Batch Loss: 0.10628010332584381\n",
      "Epoch 3132, Loss: 0.2762071490287781, Final Batch Loss: 0.1421491503715515\n",
      "Epoch 3133, Loss: 0.29771067202091217, Final Batch Loss: 0.16582845151424408\n",
      "Epoch 3134, Loss: 0.26037103682756424, Final Batch Loss: 0.11464837938547134\n",
      "Epoch 3135, Loss: 0.3707173466682434, Final Batch Loss: 0.14150817692279816\n",
      "Epoch 3136, Loss: 0.28125472366809845, Final Batch Loss: 0.09138555824756622\n",
      "Epoch 3137, Loss: 0.3552049398422241, Final Batch Loss: 0.18635399639606476\n",
      "Epoch 3138, Loss: 0.28927722573280334, Final Batch Loss: 0.1449909508228302\n",
      "Epoch 3139, Loss: 0.2552339509129524, Final Batch Loss: 0.12380855530500412\n",
      "Epoch 3140, Loss: 0.3056466281414032, Final Batch Loss: 0.17122603952884674\n",
      "Epoch 3141, Loss: 0.276206374168396, Final Batch Loss: 0.12498907744884491\n",
      "Epoch 3142, Loss: 0.33658556640148163, Final Batch Loss: 0.15833045542240143\n",
      "Epoch 3143, Loss: 0.3373842239379883, Final Batch Loss: 0.1780012995004654\n",
      "Epoch 3144, Loss: 0.2673187181353569, Final Batch Loss: 0.16053108870983124\n",
      "Epoch 3145, Loss: 0.2714044153690338, Final Batch Loss: 0.1292390078306198\n",
      "Epoch 3146, Loss: 0.28018946945667267, Final Batch Loss: 0.14907576143741608\n",
      "Epoch 3147, Loss: 0.31030888855457306, Final Batch Loss: 0.14619144797325134\n",
      "Epoch 3148, Loss: 0.359148770570755, Final Batch Loss: 0.2121339738368988\n",
      "Epoch 3149, Loss: 0.28758300840854645, Final Batch Loss: 0.1288745254278183\n",
      "Epoch 3150, Loss: 0.3171329200267792, Final Batch Loss: 0.1561666578054428\n",
      "Epoch 3151, Loss: 0.31739072501659393, Final Batch Loss: 0.15843689441680908\n",
      "Epoch 3152, Loss: 0.3203687518835068, Final Batch Loss: 0.1399681717157364\n",
      "Epoch 3153, Loss: 0.38083794713020325, Final Batch Loss: 0.2525542974472046\n",
      "Epoch 3154, Loss: 0.2857389450073242, Final Batch Loss: 0.13645803928375244\n",
      "Epoch 3155, Loss: 0.28726305067539215, Final Batch Loss: 0.14415079355239868\n",
      "Epoch 3156, Loss: 0.29291193187236786, Final Batch Loss: 0.14553695917129517\n",
      "Epoch 3157, Loss: 0.2946054935455322, Final Batch Loss: 0.14369995892047882\n",
      "Epoch 3158, Loss: 0.29924413561820984, Final Batch Loss: 0.16623187065124512\n",
      "Epoch 3159, Loss: 0.3337973803281784, Final Batch Loss: 0.15137434005737305\n",
      "Epoch 3160, Loss: 0.2812144011259079, Final Batch Loss: 0.1325264424085617\n",
      "Epoch 3161, Loss: 0.35439836978912354, Final Batch Loss: 0.1979578286409378\n",
      "Epoch 3162, Loss: 0.2998305559158325, Final Batch Loss: 0.14090043306350708\n",
      "Epoch 3163, Loss: 0.29561445116996765, Final Batch Loss: 0.13994766771793365\n",
      "Epoch 3164, Loss: 0.3682246655225754, Final Batch Loss: 0.19816112518310547\n",
      "Epoch 3165, Loss: 0.2783181071281433, Final Batch Loss: 0.14892633259296417\n",
      "Epoch 3166, Loss: 0.2461457997560501, Final Batch Loss: 0.10811558365821838\n",
      "Epoch 3167, Loss: 0.3178231716156006, Final Batch Loss: 0.18522773683071136\n",
      "Epoch 3168, Loss: 0.2884063944220543, Final Batch Loss: 0.16498957574367523\n",
      "Epoch 3169, Loss: 0.3008842021226883, Final Batch Loss: 0.14509503543376923\n",
      "Epoch 3170, Loss: 0.32350389659404755, Final Batch Loss: 0.17795045673847198\n",
      "Epoch 3171, Loss: 0.2215966284275055, Final Batch Loss: 0.09540791809558868\n",
      "Epoch 3172, Loss: 0.348927766084671, Final Batch Loss: 0.18987540900707245\n",
      "Epoch 3173, Loss: 0.2870989143848419, Final Batch Loss: 0.15374130010604858\n",
      "Epoch 3174, Loss: 0.2729729488492012, Final Batch Loss: 0.12294771522283554\n",
      "Epoch 3175, Loss: 0.3672831207513809, Final Batch Loss: 0.19606555998325348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3176, Loss: 0.26202042400836945, Final Batch Loss: 0.11385555565357208\n",
      "Epoch 3177, Loss: 0.2748144343495369, Final Batch Loss: 0.10351439565420151\n",
      "Epoch 3178, Loss: 0.3126819133758545, Final Batch Loss: 0.16962198913097382\n",
      "Epoch 3179, Loss: 0.2551390901207924, Final Batch Loss: 0.11250538378953934\n",
      "Epoch 3180, Loss: 0.32444362342357635, Final Batch Loss: 0.17299948632717133\n",
      "Epoch 3181, Loss: 0.31337524950504303, Final Batch Loss: 0.15673014521598816\n",
      "Epoch 3182, Loss: 0.2550777345895767, Final Batch Loss: 0.12941090762615204\n",
      "Epoch 3183, Loss: 0.2867053896188736, Final Batch Loss: 0.1450931578874588\n",
      "Epoch 3184, Loss: 0.3241332173347473, Final Batch Loss: 0.17741365730762482\n",
      "Epoch 3185, Loss: 0.2502620667219162, Final Batch Loss: 0.11925847828388214\n",
      "Epoch 3186, Loss: 0.3037033826112747, Final Batch Loss: 0.16936391592025757\n",
      "Epoch 3187, Loss: 0.2769696116447449, Final Batch Loss: 0.1357952505350113\n",
      "Epoch 3188, Loss: 0.3287440538406372, Final Batch Loss: 0.15750370919704437\n",
      "Epoch 3189, Loss: 0.290516659617424, Final Batch Loss: 0.12878918647766113\n",
      "Epoch 3190, Loss: 0.26853881776332855, Final Batch Loss: 0.13176822662353516\n",
      "Epoch 3191, Loss: 0.2563297674059868, Final Batch Loss: 0.1170458272099495\n",
      "Epoch 3192, Loss: 0.28179795295000076, Final Batch Loss: 0.11806865781545639\n",
      "Epoch 3193, Loss: 0.3294850289821625, Final Batch Loss: 0.18973897397518158\n",
      "Epoch 3194, Loss: 0.3084149658679962, Final Batch Loss: 0.15063033998012543\n",
      "Epoch 3195, Loss: 0.2554090842604637, Final Batch Loss: 0.14391496777534485\n",
      "Epoch 3196, Loss: 0.2577505335211754, Final Batch Loss: 0.10242217034101486\n",
      "Epoch 3197, Loss: 0.27400751411914825, Final Batch Loss: 0.15049782395362854\n",
      "Epoch 3198, Loss: 0.2589028626680374, Final Batch Loss: 0.12009231746196747\n",
      "Epoch 3199, Loss: 0.3138568103313446, Final Batch Loss: 0.13800179958343506\n",
      "Epoch 3200, Loss: 0.2909456044435501, Final Batch Loss: 0.15689338743686676\n",
      "Epoch 3201, Loss: 0.27538084983825684, Final Batch Loss: 0.14137502014636993\n",
      "Epoch 3202, Loss: 0.25893280655145645, Final Batch Loss: 0.09786798804998398\n",
      "Epoch 3203, Loss: 0.3943462446331978, Final Batch Loss: 0.2755967974662781\n",
      "Epoch 3204, Loss: 0.30095653235912323, Final Batch Loss: 0.16085289418697357\n",
      "Epoch 3205, Loss: 0.3249143809080124, Final Batch Loss: 0.19072285294532776\n",
      "Epoch 3206, Loss: 0.2725052386522293, Final Batch Loss: 0.10649242997169495\n",
      "Epoch 3207, Loss: 0.2942097932100296, Final Batch Loss: 0.16285035014152527\n",
      "Epoch 3208, Loss: 0.3010668009519577, Final Batch Loss: 0.15035569667816162\n",
      "Epoch 3209, Loss: 0.2937740162014961, Final Batch Loss: 0.17549340426921844\n",
      "Epoch 3210, Loss: 0.2764693647623062, Final Batch Loss: 0.16152361035346985\n",
      "Epoch 3211, Loss: 0.3210902363061905, Final Batch Loss: 0.18802759051322937\n",
      "Epoch 3212, Loss: 0.31165798008441925, Final Batch Loss: 0.1518542468547821\n",
      "Epoch 3213, Loss: 0.26293548941612244, Final Batch Loss: 0.13419748842716217\n",
      "Epoch 3214, Loss: 0.2936928942799568, Final Batch Loss: 0.11878060549497604\n",
      "Epoch 3215, Loss: 0.3203323185443878, Final Batch Loss: 0.14774949848651886\n",
      "Epoch 3216, Loss: 0.28554100543260574, Final Batch Loss: 0.1186436340212822\n",
      "Epoch 3217, Loss: 0.3061759024858475, Final Batch Loss: 0.1369028240442276\n",
      "Epoch 3218, Loss: 0.34474194049835205, Final Batch Loss: 0.20535098016262054\n",
      "Epoch 3219, Loss: 0.25130870938301086, Final Batch Loss: 0.12735037505626678\n",
      "Epoch 3220, Loss: 0.3109362870454788, Final Batch Loss: 0.17805641889572144\n",
      "Epoch 3221, Loss: 0.3671061396598816, Final Batch Loss: 0.176720529794693\n",
      "Epoch 3222, Loss: 0.2487863078713417, Final Batch Loss: 0.12977449595928192\n",
      "Epoch 3223, Loss: 0.2786160409450531, Final Batch Loss: 0.13037876784801483\n",
      "Epoch 3224, Loss: 0.28377245366573334, Final Batch Loss: 0.16928212344646454\n",
      "Epoch 3225, Loss: 0.37090472877025604, Final Batch Loss: 0.23111850023269653\n",
      "Epoch 3226, Loss: 0.35169172286987305, Final Batch Loss: 0.20397548377513885\n",
      "Epoch 3227, Loss: 0.269269734621048, Final Batch Loss: 0.12787923216819763\n",
      "Epoch 3228, Loss: 0.3337627202272415, Final Batch Loss: 0.1505487710237503\n",
      "Epoch 3229, Loss: 0.322821244597435, Final Batch Loss: 0.17027072608470917\n",
      "Epoch 3230, Loss: 0.3501318246126175, Final Batch Loss: 0.2207133024930954\n",
      "Epoch 3231, Loss: 0.3111077696084976, Final Batch Loss: 0.1330365389585495\n",
      "Epoch 3232, Loss: 0.2981674373149872, Final Batch Loss: 0.17269079387187958\n",
      "Epoch 3233, Loss: 0.296930193901062, Final Batch Loss: 0.16312697529792786\n",
      "Epoch 3234, Loss: 0.333553209900856, Final Batch Loss: 0.18145902454853058\n",
      "Epoch 3235, Loss: 0.2881559878587723, Final Batch Loss: 0.16015374660491943\n",
      "Epoch 3236, Loss: 0.3063655346632004, Final Batch Loss: 0.14505070447921753\n",
      "Epoch 3237, Loss: 0.3265214115381241, Final Batch Loss: 0.13572801649570465\n",
      "Epoch 3238, Loss: 0.31668832898139954, Final Batch Loss: 0.16826950013637543\n",
      "Epoch 3239, Loss: 0.30452248454093933, Final Batch Loss: 0.132346048951149\n",
      "Epoch 3240, Loss: 0.30644188821315765, Final Batch Loss: 0.1574762463569641\n",
      "Epoch 3241, Loss: 0.27291978895664215, Final Batch Loss: 0.13607734441757202\n",
      "Epoch 3242, Loss: 0.30624137818813324, Final Batch Loss: 0.14485491812229156\n",
      "Epoch 3243, Loss: 0.36099979281425476, Final Batch Loss: 0.1978549212217331\n",
      "Epoch 3244, Loss: 0.27742263674736023, Final Batch Loss: 0.12178380787372589\n",
      "Epoch 3245, Loss: 0.2565476894378662, Final Batch Loss: 0.10921569168567657\n",
      "Epoch 3246, Loss: 0.31220073997974396, Final Batch Loss: 0.1502746194601059\n",
      "Epoch 3247, Loss: 0.2908009737730026, Final Batch Loss: 0.13797327876091003\n",
      "Epoch 3248, Loss: 0.32009316980838776, Final Batch Loss: 0.15841102600097656\n",
      "Epoch 3249, Loss: 0.2801409214735031, Final Batch Loss: 0.12817244231700897\n",
      "Epoch 3250, Loss: 0.29569296538829803, Final Batch Loss: 0.1501501351594925\n",
      "Epoch 3251, Loss: 0.3056456297636032, Final Batch Loss: 0.16092655062675476\n",
      "Epoch 3252, Loss: 0.2759186327457428, Final Batch Loss: 0.15423740446567535\n",
      "Epoch 3253, Loss: 0.2916921600699425, Final Batch Loss: 0.16841436922550201\n",
      "Epoch 3254, Loss: 0.23738223314285278, Final Batch Loss: 0.10119295120239258\n",
      "Epoch 3255, Loss: 0.31345658004283905, Final Batch Loss: 0.1656465232372284\n",
      "Epoch 3256, Loss: 0.2624310627579689, Final Batch Loss: 0.12342620640993118\n",
      "Epoch 3257, Loss: 0.269498772919178, Final Batch Loss: 0.14471659064292908\n",
      "Epoch 3258, Loss: 0.26243942975997925, Final Batch Loss: 0.13491445779800415\n",
      "Epoch 3259, Loss: 0.3395111560821533, Final Batch Loss: 0.2032601684331894\n",
      "Epoch 3260, Loss: 0.293501153588295, Final Batch Loss: 0.14002151787281036\n",
      "Epoch 3261, Loss: 0.27285534143447876, Final Batch Loss: 0.1342778354883194\n",
      "Epoch 3262, Loss: 0.25982484966516495, Final Batch Loss: 0.15200625360012054\n",
      "Epoch 3263, Loss: 0.26009613275527954, Final Batch Loss: 0.1256706416606903\n",
      "Epoch 3264, Loss: 0.2979287803173065, Final Batch Loss: 0.16180381178855896\n",
      "Epoch 3265, Loss: 0.3000979870557785, Final Batch Loss: 0.15678443014621735\n",
      "Epoch 3266, Loss: 0.2357940748333931, Final Batch Loss: 0.12623725831508636\n",
      "Epoch 3267, Loss: 0.27898554503917694, Final Batch Loss: 0.16132928431034088\n",
      "Epoch 3268, Loss: 0.25419340282678604, Final Batch Loss: 0.07399985939264297\n",
      "Epoch 3269, Loss: 0.2893022447824478, Final Batch Loss: 0.14073482155799866\n",
      "Epoch 3270, Loss: 0.29677293449640274, Final Batch Loss: 0.12391158193349838\n",
      "Epoch 3271, Loss: 0.2707713544368744, Final Batch Loss: 0.13329795002937317\n",
      "Epoch 3272, Loss: 0.25934700667858124, Final Batch Loss: 0.14111168682575226\n",
      "Epoch 3273, Loss: 0.2553887888789177, Final Batch Loss: 0.11596215516328812\n",
      "Epoch 3274, Loss: 0.31118910014629364, Final Batch Loss: 0.1676090508699417\n",
      "Epoch 3275, Loss: 0.26999954879283905, Final Batch Loss: 0.12838566303253174\n",
      "Epoch 3276, Loss: 0.29463183879852295, Final Batch Loss: 0.18957127630710602\n",
      "Epoch 3277, Loss: 0.258868545293808, Final Batch Loss: 0.13446412980556488\n",
      "Epoch 3278, Loss: 0.2446988895535469, Final Batch Loss: 0.10578805953264236\n",
      "Epoch 3279, Loss: 0.25919075310230255, Final Batch Loss: 0.142251655459404\n",
      "Epoch 3280, Loss: 0.4197291433811188, Final Batch Loss: 0.23964667320251465\n",
      "Epoch 3281, Loss: 0.27794046700000763, Final Batch Loss: 0.13981062173843384\n",
      "Epoch 3282, Loss: 0.27599410712718964, Final Batch Loss: 0.120771124958992\n",
      "Epoch 3283, Loss: 0.29957349598407745, Final Batch Loss: 0.10011245310306549\n",
      "Epoch 3284, Loss: 0.29616697132587433, Final Batch Loss: 0.1380327194929123\n",
      "Epoch 3285, Loss: 0.3835020214319229, Final Batch Loss: 0.1962883025407791\n",
      "Epoch 3286, Loss: 0.3028441369533539, Final Batch Loss: 0.16801127791404724\n",
      "Epoch 3287, Loss: 0.2896748185157776, Final Batch Loss: 0.13408415019512177\n",
      "Epoch 3288, Loss: 0.32553307712078094, Final Batch Loss: 0.16537226736545563\n",
      "Epoch 3289, Loss: 0.23307273536920547, Final Batch Loss: 0.11132188141345978\n",
      "Epoch 3290, Loss: 0.3092111051082611, Final Batch Loss: 0.1543717384338379\n",
      "Epoch 3291, Loss: 0.35658738017082214, Final Batch Loss: 0.1725742369890213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3292, Loss: 0.28917915374040604, Final Batch Loss: 0.11464685946702957\n",
      "Epoch 3293, Loss: 0.26401136815547943, Final Batch Loss: 0.1256301999092102\n",
      "Epoch 3294, Loss: 0.280579149723053, Final Batch Loss: 0.09713008999824524\n",
      "Epoch 3295, Loss: 0.29072951525449753, Final Batch Loss: 0.16800765693187714\n",
      "Epoch 3296, Loss: 0.297559529542923, Final Batch Loss: 0.15600842237472534\n",
      "Epoch 3297, Loss: 0.2722364068031311, Final Batch Loss: 0.13870544731616974\n",
      "Epoch 3298, Loss: 0.27689141035079956, Final Batch Loss: 0.12760941684246063\n",
      "Epoch 3299, Loss: 0.3655638247728348, Final Batch Loss: 0.15606725215911865\n",
      "Epoch 3300, Loss: 0.2492312639951706, Final Batch Loss: 0.0978880375623703\n",
      "Epoch 3301, Loss: 0.41061238944530487, Final Batch Loss: 0.2583971321582794\n",
      "Epoch 3302, Loss: 0.3468658775091171, Final Batch Loss: 0.17062978446483612\n",
      "Epoch 3303, Loss: 0.26963305473327637, Final Batch Loss: 0.1361873745918274\n",
      "Epoch 3304, Loss: 0.25645893067121506, Final Batch Loss: 0.12226743251085281\n",
      "Epoch 3305, Loss: 0.28767766058444977, Final Batch Loss: 0.15818065404891968\n",
      "Epoch 3306, Loss: 0.2442173957824707, Final Batch Loss: 0.12189438194036484\n",
      "Epoch 3307, Loss: 0.2707858458161354, Final Batch Loss: 0.11692864447832108\n",
      "Epoch 3308, Loss: 0.24601618200540543, Final Batch Loss: 0.12045826762914658\n",
      "Epoch 3309, Loss: 0.30588506162166595, Final Batch Loss: 0.15810450911521912\n",
      "Epoch 3310, Loss: 0.25582490116357803, Final Batch Loss: 0.13746292889118195\n",
      "Epoch 3311, Loss: 0.28561754524707794, Final Batch Loss: 0.14006809890270233\n",
      "Epoch 3312, Loss: 0.3684203773736954, Final Batch Loss: 0.21274715662002563\n",
      "Epoch 3313, Loss: 0.27808934450149536, Final Batch Loss: 0.12831206619739532\n",
      "Epoch 3314, Loss: 0.26910586655139923, Final Batch Loss: 0.10418590903282166\n",
      "Epoch 3315, Loss: 0.263446643948555, Final Batch Loss: 0.1302451640367508\n",
      "Epoch 3316, Loss: 0.3014855235815048, Final Batch Loss: 0.14910952746868134\n",
      "Epoch 3317, Loss: 0.26235026121139526, Final Batch Loss: 0.11971385776996613\n",
      "Epoch 3318, Loss: 0.25531307607889175, Final Batch Loss: 0.10369227081537247\n",
      "Epoch 3319, Loss: 0.2842441350221634, Final Batch Loss: 0.15439952909946442\n",
      "Epoch 3320, Loss: 0.2736532688140869, Final Batch Loss: 0.12994353473186493\n",
      "Epoch 3321, Loss: 0.25021087378263474, Final Batch Loss: 0.12466446310281754\n",
      "Epoch 3322, Loss: 0.28762343525886536, Final Batch Loss: 0.13971161842346191\n",
      "Epoch 3323, Loss: 0.269976869225502, Final Batch Loss: 0.14592789113521576\n",
      "Epoch 3324, Loss: 0.2889951020479202, Final Batch Loss: 0.1502165049314499\n",
      "Epoch 3325, Loss: 0.28783518075942993, Final Batch Loss: 0.13674350082874298\n",
      "Epoch 3326, Loss: 0.2730177789926529, Final Batch Loss: 0.1282888650894165\n",
      "Epoch 3327, Loss: 0.24263442307710648, Final Batch Loss: 0.12338678538799286\n",
      "Epoch 3328, Loss: 0.32125574350357056, Final Batch Loss: 0.16247007250785828\n",
      "Epoch 3329, Loss: 0.3009936213493347, Final Batch Loss: 0.13260217010974884\n",
      "Epoch 3330, Loss: 0.2567678838968277, Final Batch Loss: 0.11483132839202881\n",
      "Epoch 3331, Loss: 0.2613386958837509, Final Batch Loss: 0.12163867056369781\n",
      "Epoch 3332, Loss: 0.2655383497476578, Final Batch Loss: 0.13159294426441193\n",
      "Epoch 3333, Loss: 0.20854934304952621, Final Batch Loss: 0.11609622091054916\n",
      "Epoch 3334, Loss: 0.34126999974250793, Final Batch Loss: 0.13265763223171234\n",
      "Epoch 3335, Loss: 0.31124934554100037, Final Batch Loss: 0.16516181826591492\n",
      "Epoch 3336, Loss: 0.279534712433815, Final Batch Loss: 0.1505751758813858\n",
      "Epoch 3337, Loss: 0.2903098911046982, Final Batch Loss: 0.1492806077003479\n",
      "Epoch 3338, Loss: 0.2523333206772804, Final Batch Loss: 0.1325480192899704\n",
      "Epoch 3339, Loss: 0.2978408932685852, Final Batch Loss: 0.16639786958694458\n",
      "Epoch 3340, Loss: 0.2862785756587982, Final Batch Loss: 0.135905921459198\n",
      "Epoch 3341, Loss: 0.27244380861520767, Final Batch Loss: 0.11908096820116043\n",
      "Epoch 3342, Loss: 0.2842506617307663, Final Batch Loss: 0.14051081240177155\n",
      "Epoch 3343, Loss: 0.2810760736465454, Final Batch Loss: 0.15199734270572662\n",
      "Epoch 3344, Loss: 0.29277969896793365, Final Batch Loss: 0.15203280746936798\n",
      "Epoch 3345, Loss: 0.31315866112709045, Final Batch Loss: 0.136921688914299\n",
      "Epoch 3346, Loss: 0.2843165248632431, Final Batch Loss: 0.15895316004753113\n",
      "Epoch 3347, Loss: 0.24941477924585342, Final Batch Loss: 0.11065978556871414\n",
      "Epoch 3348, Loss: 0.25259979814291, Final Batch Loss: 0.10527025908231735\n",
      "Epoch 3349, Loss: 0.2593160644173622, Final Batch Loss: 0.13533684611320496\n",
      "Epoch 3350, Loss: 0.3841914236545563, Final Batch Loss: 0.1841018944978714\n",
      "Epoch 3351, Loss: 0.30428335070610046, Final Batch Loss: 0.13219000399112701\n",
      "Epoch 3352, Loss: 0.25831638276576996, Final Batch Loss: 0.1252290904521942\n",
      "Epoch 3353, Loss: 0.26172272861003876, Final Batch Loss: 0.13745948672294617\n",
      "Epoch 3354, Loss: 0.2564612329006195, Final Batch Loss: 0.11375163495540619\n",
      "Epoch 3355, Loss: 0.2867821604013443, Final Batch Loss: 0.15827345848083496\n",
      "Epoch 3356, Loss: 0.3346155285835266, Final Batch Loss: 0.16166549921035767\n",
      "Epoch 3357, Loss: 0.2907060831785202, Final Batch Loss: 0.1638418436050415\n",
      "Epoch 3358, Loss: 0.2540924549102783, Final Batch Loss: 0.1009746789932251\n",
      "Epoch 3359, Loss: 0.37133534252643585, Final Batch Loss: 0.19615866243839264\n",
      "Epoch 3360, Loss: 0.27783335000276566, Final Batch Loss: 0.1551859974861145\n",
      "Epoch 3361, Loss: 0.31949062645435333, Final Batch Loss: 0.16034963726997375\n",
      "Epoch 3362, Loss: 0.277647465467453, Final Batch Loss: 0.12461201846599579\n",
      "Epoch 3363, Loss: 0.2763315290212631, Final Batch Loss: 0.1268409639596939\n",
      "Epoch 3364, Loss: 0.2561091408133507, Final Batch Loss: 0.1350758671760559\n",
      "Epoch 3365, Loss: 0.28272536396980286, Final Batch Loss: 0.12801241874694824\n",
      "Epoch 3366, Loss: 0.2876060977578163, Final Batch Loss: 0.12274818867444992\n",
      "Epoch 3367, Loss: 0.294120654463768, Final Batch Loss: 0.17475363612174988\n",
      "Epoch 3368, Loss: 0.2623233199119568, Final Batch Loss: 0.12485742568969727\n",
      "Epoch 3369, Loss: 0.37025874853134155, Final Batch Loss: 0.1563381850719452\n",
      "Epoch 3370, Loss: 0.252392053604126, Final Batch Loss: 0.09531310200691223\n",
      "Epoch 3371, Loss: 0.30390046536922455, Final Batch Loss: 0.16745251417160034\n",
      "Epoch 3372, Loss: 0.23974379897117615, Final Batch Loss: 0.12594835460186005\n",
      "Epoch 3373, Loss: 0.25559883564710617, Final Batch Loss: 0.09829335659742355\n",
      "Epoch 3374, Loss: 0.2359708771109581, Final Batch Loss: 0.10436836630105972\n",
      "Epoch 3375, Loss: 0.2919999659061432, Final Batch Loss: 0.14747354388237\n",
      "Epoch 3376, Loss: 0.2707655429840088, Final Batch Loss: 0.12793096899986267\n",
      "Epoch 3377, Loss: 0.3059825897216797, Final Batch Loss: 0.20385730266571045\n",
      "Epoch 3378, Loss: 0.23200666904449463, Final Batch Loss: 0.11361970752477646\n",
      "Epoch 3379, Loss: 0.2922474145889282, Final Batch Loss: 0.14798809587955475\n",
      "Epoch 3380, Loss: 0.28218047320842743, Final Batch Loss: 0.14832134544849396\n",
      "Epoch 3381, Loss: 0.2738509178161621, Final Batch Loss: 0.14979997277259827\n",
      "Epoch 3382, Loss: 0.25297898799180984, Final Batch Loss: 0.1294969618320465\n",
      "Epoch 3383, Loss: 0.293319433927536, Final Batch Loss: 0.15288609266281128\n",
      "Epoch 3384, Loss: 0.2818641662597656, Final Batch Loss: 0.13729943335056305\n",
      "Epoch 3385, Loss: 0.28329963982105255, Final Batch Loss: 0.12392836809158325\n",
      "Epoch 3386, Loss: 0.2616877630352974, Final Batch Loss: 0.10808330029249191\n",
      "Epoch 3387, Loss: 0.2860056012868881, Final Batch Loss: 0.13751138746738434\n",
      "Epoch 3388, Loss: 0.2675862982869148, Final Batch Loss: 0.16050931811332703\n",
      "Epoch 3389, Loss: 0.3169836848974228, Final Batch Loss: 0.13471952080726624\n",
      "Epoch 3390, Loss: 0.29947278648614883, Final Batch Loss: 0.1858944296836853\n",
      "Epoch 3391, Loss: 0.2952682375907898, Final Batch Loss: 0.10379934310913086\n",
      "Epoch 3392, Loss: 0.2877591997385025, Final Batch Loss: 0.1574903279542923\n",
      "Epoch 3393, Loss: 0.2627265527844429, Final Batch Loss: 0.11210214346647263\n",
      "Epoch 3394, Loss: 0.3313254117965698, Final Batch Loss: 0.17591942846775055\n",
      "Epoch 3395, Loss: 0.2490772232413292, Final Batch Loss: 0.12120101600885391\n",
      "Epoch 3396, Loss: 0.2728815972805023, Final Batch Loss: 0.1281292587518692\n",
      "Epoch 3397, Loss: 0.2911970317363739, Final Batch Loss: 0.1449471116065979\n",
      "Epoch 3398, Loss: 0.24641497433185577, Final Batch Loss: 0.13085967302322388\n",
      "Epoch 3399, Loss: 0.26791058480739594, Final Batch Loss: 0.12667891383171082\n",
      "Epoch 3400, Loss: 0.27791401743888855, Final Batch Loss: 0.15580837428569794\n",
      "Epoch 3401, Loss: 0.26243357360363007, Final Batch Loss: 0.10209402441978455\n",
      "Epoch 3402, Loss: 0.30602099746465683, Final Batch Loss: 0.18568696081638336\n",
      "Epoch 3403, Loss: 0.2564012110233307, Final Batch Loss: 0.10057471692562103\n",
      "Epoch 3404, Loss: 0.2744833081960678, Final Batch Loss: 0.1511780470609665\n",
      "Epoch 3405, Loss: 0.2808016985654831, Final Batch Loss: 0.12243057787418365\n",
      "Epoch 3406, Loss: 0.2437101975083351, Final Batch Loss: 0.10502871125936508\n",
      "Epoch 3407, Loss: 0.2956782877445221, Final Batch Loss: 0.12377691268920898\n",
      "Epoch 3408, Loss: 0.25829648971557617, Final Batch Loss: 0.13265565037727356\n",
      "Epoch 3409, Loss: 0.26172496378421783, Final Batch Loss: 0.11817842721939087\n",
      "Epoch 3410, Loss: 0.24701029062271118, Final Batch Loss: 0.10497958958148956\n",
      "Epoch 3411, Loss: 0.25848010182380676, Final Batch Loss: 0.12303605675697327\n",
      "Epoch 3412, Loss: 0.24942240118980408, Final Batch Loss: 0.11725184321403503\n",
      "Epoch 3413, Loss: 0.26763610541820526, Final Batch Loss: 0.11425721645355225\n",
      "Epoch 3414, Loss: 0.3243078142404556, Final Batch Loss: 0.1823304146528244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3415, Loss: 0.2758356183767319, Final Batch Loss: 0.1303115338087082\n",
      "Epoch 3416, Loss: 0.3037422224879265, Final Batch Loss: 0.1788642853498459\n",
      "Epoch 3417, Loss: 0.27824097871780396, Final Batch Loss: 0.13899391889572144\n",
      "Epoch 3418, Loss: 0.28832390904426575, Final Batch Loss: 0.1299792379140854\n",
      "Epoch 3419, Loss: 0.3198267072439194, Final Batch Loss: 0.1848876029253006\n",
      "Epoch 3420, Loss: 0.30009809136390686, Final Batch Loss: 0.17451351881027222\n",
      "Epoch 3421, Loss: 0.3016994148492813, Final Batch Loss: 0.1439199447631836\n",
      "Epoch 3422, Loss: 0.3362176865339279, Final Batch Loss: 0.19449813663959503\n",
      "Epoch 3423, Loss: 0.3142360597848892, Final Batch Loss: 0.1810963749885559\n",
      "Epoch 3424, Loss: 0.3415141999721527, Final Batch Loss: 0.20985423028469086\n",
      "Epoch 3425, Loss: 0.28741614520549774, Final Batch Loss: 0.1597590446472168\n",
      "Epoch 3426, Loss: 0.2968047857284546, Final Batch Loss: 0.1578138768672943\n",
      "Epoch 3427, Loss: 0.22592175006866455, Final Batch Loss: 0.11151642352342606\n",
      "Epoch 3428, Loss: 0.26897846162319183, Final Batch Loss: 0.12432651221752167\n",
      "Epoch 3429, Loss: 0.30122847855091095, Final Batch Loss: 0.1486293077468872\n",
      "Epoch 3430, Loss: 0.2505267336964607, Final Batch Loss: 0.09632069617509842\n",
      "Epoch 3431, Loss: 0.24246811121702194, Final Batch Loss: 0.11682774871587753\n",
      "Epoch 3432, Loss: 0.28393127024173737, Final Batch Loss: 0.1375391185283661\n",
      "Epoch 3433, Loss: 0.237182155251503, Final Batch Loss: 0.10425709187984467\n",
      "Epoch 3434, Loss: 0.32858262956142426, Final Batch Loss: 0.18887074291706085\n",
      "Epoch 3435, Loss: 0.32570865750312805, Final Batch Loss: 0.1869116574525833\n",
      "Epoch 3436, Loss: 0.32854774594306946, Final Batch Loss: 0.1481001079082489\n",
      "Epoch 3437, Loss: 0.302770733833313, Final Batch Loss: 0.12697456777095795\n",
      "Epoch 3438, Loss: 0.28585442900657654, Final Batch Loss: 0.157387375831604\n",
      "Epoch 3439, Loss: 0.25501926243305206, Final Batch Loss: 0.10877293348312378\n",
      "Epoch 3440, Loss: 0.2740126699209213, Final Batch Loss: 0.13824661076068878\n",
      "Epoch 3441, Loss: 0.23894209414720535, Final Batch Loss: 0.11370902508497238\n",
      "Epoch 3442, Loss: 0.2903357297182083, Final Batch Loss: 0.15700475871562958\n",
      "Epoch 3443, Loss: 0.275601826608181, Final Batch Loss: 0.15694038569927216\n",
      "Epoch 3444, Loss: 0.22475627064704895, Final Batch Loss: 0.1083298921585083\n",
      "Epoch 3445, Loss: 0.24265997856855392, Final Batch Loss: 0.12050066888332367\n",
      "Epoch 3446, Loss: 0.23614217340946198, Final Batch Loss: 0.1266883760690689\n",
      "Epoch 3447, Loss: 0.2877753973007202, Final Batch Loss: 0.12841962277889252\n",
      "Epoch 3448, Loss: 0.2623368054628372, Final Batch Loss: 0.13836605846881866\n",
      "Epoch 3449, Loss: 0.2411578670144081, Final Batch Loss: 0.12009116262197495\n",
      "Epoch 3450, Loss: 0.3392449617385864, Final Batch Loss: 0.2137935757637024\n",
      "Epoch 3451, Loss: 0.27042102068662643, Final Batch Loss: 0.15794163942337036\n",
      "Epoch 3452, Loss: 0.31512798368930817, Final Batch Loss: 0.16497702896595\n",
      "Epoch 3453, Loss: 0.2908076196908951, Final Batch Loss: 0.12181234359741211\n",
      "Epoch 3454, Loss: 0.26964038610458374, Final Batch Loss: 0.13428105413913727\n",
      "Epoch 3455, Loss: 0.26204199343919754, Final Batch Loss: 0.09658841043710709\n",
      "Epoch 3456, Loss: 0.25980666279792786, Final Batch Loss: 0.10692611336708069\n",
      "Epoch 3457, Loss: 0.2854780852794647, Final Batch Loss: 0.13937251269817352\n",
      "Epoch 3458, Loss: 0.3950153887271881, Final Batch Loss: 0.27745190262794495\n",
      "Epoch 3459, Loss: 0.28140221536159515, Final Batch Loss: 0.13246355950832367\n",
      "Epoch 3460, Loss: 0.2192472368478775, Final Batch Loss: 0.11871646344661713\n",
      "Epoch 3461, Loss: 0.2475930005311966, Final Batch Loss: 0.1045360118150711\n",
      "Epoch 3462, Loss: 0.26891791820526123, Final Batch Loss: 0.12670108675956726\n",
      "Epoch 3463, Loss: 0.27916865050792694, Final Batch Loss: 0.15385571122169495\n",
      "Epoch 3464, Loss: 0.27665263414382935, Final Batch Loss: 0.14729855954647064\n",
      "Epoch 3465, Loss: 0.32890525460243225, Final Batch Loss: 0.13756529986858368\n",
      "Epoch 3466, Loss: 0.2543405741453171, Final Batch Loss: 0.1265379637479782\n",
      "Epoch 3467, Loss: 0.3053201362490654, Final Batch Loss: 0.2021380513906479\n",
      "Epoch 3468, Loss: 0.2738371789455414, Final Batch Loss: 0.13287138938903809\n",
      "Epoch 3469, Loss: 0.24564269185066223, Final Batch Loss: 0.08905133605003357\n",
      "Epoch 3470, Loss: 0.34232109785079956, Final Batch Loss: 0.21550382673740387\n",
      "Epoch 3471, Loss: 0.3819696754217148, Final Batch Loss: 0.2206329107284546\n",
      "Epoch 3472, Loss: 0.2679080367088318, Final Batch Loss: 0.15370270609855652\n",
      "Epoch 3473, Loss: 0.2766069173812866, Final Batch Loss: 0.1255950927734375\n",
      "Epoch 3474, Loss: 0.2372300699353218, Final Batch Loss: 0.11792170256376266\n",
      "Epoch 3475, Loss: 0.25873512029647827, Final Batch Loss: 0.12300091981887817\n",
      "Epoch 3476, Loss: 0.3037005066871643, Final Batch Loss: 0.13418971002101898\n",
      "Epoch 3477, Loss: 0.26838892698287964, Final Batch Loss: 0.14371277391910553\n",
      "Epoch 3478, Loss: 0.2909018099308014, Final Batch Loss: 0.14346468448638916\n",
      "Epoch 3479, Loss: 0.3681793361902237, Final Batch Loss: 0.18832026422023773\n",
      "Epoch 3480, Loss: 0.26242636889219284, Final Batch Loss: 0.12158939987421036\n",
      "Epoch 3481, Loss: 0.25534988194704056, Final Batch Loss: 0.11191967874765396\n",
      "Epoch 3482, Loss: 0.2950015664100647, Final Batch Loss: 0.15043342113494873\n",
      "Epoch 3483, Loss: 0.2740562781691551, Final Batch Loss: 0.16194117069244385\n",
      "Epoch 3484, Loss: 0.2478906288743019, Final Batch Loss: 0.11105924099683762\n",
      "Epoch 3485, Loss: 0.23374324291944504, Final Batch Loss: 0.08934595435857773\n",
      "Epoch 3486, Loss: 0.279749259352684, Final Batch Loss: 0.12692490220069885\n",
      "Epoch 3487, Loss: 0.26144473999738693, Final Batch Loss: 0.11067939549684525\n",
      "Epoch 3488, Loss: 0.2670982927083969, Final Batch Loss: 0.1342458724975586\n",
      "Epoch 3489, Loss: 0.22368338704109192, Final Batch Loss: 0.10895753651857376\n",
      "Epoch 3490, Loss: 0.23713552206754684, Final Batch Loss: 0.09761986881494522\n",
      "Epoch 3491, Loss: 0.22329984605312347, Final Batch Loss: 0.08872702717781067\n",
      "Epoch 3492, Loss: 0.27537965774536133, Final Batch Loss: 0.14323316514492035\n",
      "Epoch 3493, Loss: 0.26630642265081406, Final Batch Loss: 0.15234939754009247\n",
      "Epoch 3494, Loss: 0.2925681471824646, Final Batch Loss: 0.15161581337451935\n",
      "Epoch 3495, Loss: 0.28007926791906357, Final Batch Loss: 0.1644573211669922\n",
      "Epoch 3496, Loss: 0.31193746626377106, Final Batch Loss: 0.14883287250995636\n",
      "Epoch 3497, Loss: 0.239907868206501, Final Batch Loss: 0.13903777301311493\n",
      "Epoch 3498, Loss: 0.24496576935052872, Final Batch Loss: 0.1411285698413849\n",
      "Epoch 3499, Loss: 0.3107838034629822, Final Batch Loss: 0.21375305950641632\n",
      "Epoch 3500, Loss: 0.282540500164032, Final Batch Loss: 0.14679791033267975\n",
      "Epoch 3501, Loss: 0.2823135554790497, Final Batch Loss: 0.14534690976142883\n",
      "Epoch 3502, Loss: 0.3032277375459671, Final Batch Loss: 0.1757378876209259\n",
      "Epoch 3503, Loss: 0.2424575462937355, Final Batch Loss: 0.11874537914991379\n",
      "Epoch 3504, Loss: 0.2808704748749733, Final Batch Loss: 0.15691019594669342\n",
      "Epoch 3505, Loss: 0.27159835398197174, Final Batch Loss: 0.13870160281658173\n",
      "Epoch 3506, Loss: 0.357574924826622, Final Batch Loss: 0.18579556047916412\n",
      "Epoch 3507, Loss: 0.2932036519050598, Final Batch Loss: 0.14529244601726532\n",
      "Epoch 3508, Loss: 0.2668999135494232, Final Batch Loss: 0.108226478099823\n",
      "Epoch 3509, Loss: 0.2731194496154785, Final Batch Loss: 0.1493081897497177\n",
      "Epoch 3510, Loss: 0.2975175082683563, Final Batch Loss: 0.16496886312961578\n",
      "Epoch 3511, Loss: 0.24732615053653717, Final Batch Loss: 0.12462332844734192\n",
      "Epoch 3512, Loss: 0.2849985510110855, Final Batch Loss: 0.12092790007591248\n",
      "Epoch 3513, Loss: 0.25272925943136215, Final Batch Loss: 0.10519925504922867\n",
      "Epoch 3514, Loss: 0.31833869218826294, Final Batch Loss: 0.13356448709964752\n",
      "Epoch 3515, Loss: 0.2847967743873596, Final Batch Loss: 0.1447238028049469\n",
      "Epoch 3516, Loss: 0.2113463580608368, Final Batch Loss: 0.06831835210323334\n",
      "Epoch 3517, Loss: 0.30450383573770523, Final Batch Loss: 0.10489783436059952\n",
      "Epoch 3518, Loss: 0.2744864150881767, Final Batch Loss: 0.17539024353027344\n",
      "Epoch 3519, Loss: 0.31806066632270813, Final Batch Loss: 0.15285427868366241\n",
      "Epoch 3520, Loss: 0.28160902857780457, Final Batch Loss: 0.1501573771238327\n",
      "Epoch 3521, Loss: 0.24637024849653244, Final Batch Loss: 0.13547369837760925\n",
      "Epoch 3522, Loss: 0.24893375486135483, Final Batch Loss: 0.11765455454587936\n",
      "Epoch 3523, Loss: 0.26898911595344543, Final Batch Loss: 0.12731991708278656\n",
      "Epoch 3524, Loss: 0.2886548340320587, Final Batch Loss: 0.11817149817943573\n",
      "Epoch 3525, Loss: 0.25892114639282227, Final Batch Loss: 0.12694543600082397\n",
      "Epoch 3526, Loss: 0.261673204600811, Final Batch Loss: 0.1536659151315689\n",
      "Epoch 3527, Loss: 0.26487596333026886, Final Batch Loss: 0.12060996890068054\n",
      "Epoch 3528, Loss: 0.27944380044937134, Final Batch Loss: 0.11969023942947388\n",
      "Epoch 3529, Loss: 0.25888437032699585, Final Batch Loss: 0.12212994694709778\n",
      "Epoch 3530, Loss: 0.27090173959732056, Final Batch Loss: 0.13505664467811584\n",
      "Epoch 3531, Loss: 0.24333225935697556, Final Batch Loss: 0.10903551429510117\n",
      "Epoch 3532, Loss: 0.27796778082847595, Final Batch Loss: 0.12011314928531647\n",
      "Epoch 3533, Loss: 0.2423388957977295, Final Batch Loss: 0.0862535834312439\n",
      "Epoch 3534, Loss: 0.2549576908349991, Final Batch Loss: 0.12539836764335632\n",
      "Epoch 3535, Loss: 0.2575167715549469, Final Batch Loss: 0.12442608177661896\n",
      "Epoch 3536, Loss: 0.26965227723121643, Final Batch Loss: 0.14317277073860168\n",
      "Epoch 3537, Loss: 0.3245757967233658, Final Batch Loss: 0.15159723162651062\n",
      "Epoch 3538, Loss: 0.25164178013801575, Final Batch Loss: 0.08742073178291321\n",
      "Epoch 3539, Loss: 0.2853102833032608, Final Batch Loss: 0.15858447551727295\n",
      "Epoch 3540, Loss: 0.27258481085300446, Final Batch Loss: 0.15752622485160828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3541, Loss: 0.28418294340372086, Final Batch Loss: 0.1591837853193283\n",
      "Epoch 3542, Loss: 0.28509652614593506, Final Batch Loss: 0.13738346099853516\n",
      "Epoch 3543, Loss: 0.2794328033924103, Final Batch Loss: 0.15422974526882172\n",
      "Epoch 3544, Loss: 0.27888573706150055, Final Batch Loss: 0.11553171277046204\n",
      "Epoch 3545, Loss: 0.24844594299793243, Final Batch Loss: 0.1270325481891632\n",
      "Epoch 3546, Loss: 0.2941227853298187, Final Batch Loss: 0.1420648992061615\n",
      "Epoch 3547, Loss: 0.21765859425067902, Final Batch Loss: 0.10205245018005371\n",
      "Epoch 3548, Loss: 0.2681204602122307, Final Batch Loss: 0.15102677047252655\n",
      "Epoch 3549, Loss: 0.32873305678367615, Final Batch Loss: 0.13400866091251373\n",
      "Epoch 3550, Loss: 0.29928503930568695, Final Batch Loss: 0.1695382297039032\n",
      "Epoch 3551, Loss: 0.25203385949134827, Final Batch Loss: 0.13513188064098358\n",
      "Epoch 3552, Loss: 0.28270190954208374, Final Batch Loss: 0.13387729227542877\n",
      "Epoch 3553, Loss: 0.31643936038017273, Final Batch Loss: 0.15423741936683655\n",
      "Epoch 3554, Loss: 0.2916324809193611, Final Batch Loss: 0.17162881791591644\n",
      "Epoch 3555, Loss: 0.2569498121738434, Final Batch Loss: 0.12738513946533203\n",
      "Epoch 3556, Loss: 0.24643665552139282, Final Batch Loss: 0.1361667960882187\n",
      "Epoch 3557, Loss: 0.31825537979602814, Final Batch Loss: 0.1702723354101181\n",
      "Epoch 3558, Loss: 0.22162646800279617, Final Batch Loss: 0.09389116615056992\n",
      "Epoch 3559, Loss: 0.2509295418858528, Final Batch Loss: 0.07151105254888535\n",
      "Epoch 3560, Loss: 0.23798193782567978, Final Batch Loss: 0.1191268339753151\n",
      "Epoch 3561, Loss: 0.32288122177124023, Final Batch Loss: 0.16794359683990479\n",
      "Epoch 3562, Loss: 0.29475319385528564, Final Batch Loss: 0.16116108000278473\n",
      "Epoch 3563, Loss: 0.2458135262131691, Final Batch Loss: 0.10874419659376144\n",
      "Epoch 3564, Loss: 0.25337108969688416, Final Batch Loss: 0.1406363993883133\n",
      "Epoch 3565, Loss: 0.30240175873041153, Final Batch Loss: 0.12103735655546188\n",
      "Epoch 3566, Loss: 0.254626527428627, Final Batch Loss: 0.12790773808956146\n",
      "Epoch 3567, Loss: 0.2969774156808853, Final Batch Loss: 0.1405050903558731\n",
      "Epoch 3568, Loss: 0.2861950993537903, Final Batch Loss: 0.1314823180437088\n",
      "Epoch 3569, Loss: 0.31349919736385345, Final Batch Loss: 0.1452668011188507\n",
      "Epoch 3570, Loss: 0.2451682910323143, Final Batch Loss: 0.11809726804494858\n",
      "Epoch 3571, Loss: 0.3065195828676224, Final Batch Loss: 0.14415386319160461\n",
      "Epoch 3572, Loss: 0.2736526280641556, Final Batch Loss: 0.1355132907629013\n",
      "Epoch 3573, Loss: 0.3058377504348755, Final Batch Loss: 0.1336883008480072\n",
      "Epoch 3574, Loss: 0.2554582804441452, Final Batch Loss: 0.12219668924808502\n",
      "Epoch 3575, Loss: 0.3592926859855652, Final Batch Loss: 0.20927618443965912\n",
      "Epoch 3576, Loss: 0.26558518409729004, Final Batch Loss: 0.14090614020824432\n",
      "Epoch 3577, Loss: 0.243741013109684, Final Batch Loss: 0.11701809614896774\n",
      "Epoch 3578, Loss: 0.2662537544965744, Final Batch Loss: 0.14380483329296112\n",
      "Epoch 3579, Loss: 0.28501096367836, Final Batch Loss: 0.132468581199646\n",
      "Epoch 3580, Loss: 0.2538202852010727, Final Batch Loss: 0.1174609363079071\n",
      "Epoch 3581, Loss: 0.25167732685804367, Final Batch Loss: 0.10822107642889023\n",
      "Epoch 3582, Loss: 0.3154110163450241, Final Batch Loss: 0.16097663342952728\n",
      "Epoch 3583, Loss: 0.29810386896133423, Final Batch Loss: 0.17102214694023132\n",
      "Epoch 3584, Loss: 0.2508406564593315, Final Batch Loss: 0.1135772243142128\n",
      "Epoch 3585, Loss: 0.24744940549135208, Final Batch Loss: 0.1199955865740776\n",
      "Epoch 3586, Loss: 0.26473046839237213, Final Batch Loss: 0.13806036114692688\n",
      "Epoch 3587, Loss: 0.2739561125636101, Final Batch Loss: 0.10357677191495895\n",
      "Epoch 3588, Loss: 0.26206864416599274, Final Batch Loss: 0.09455041587352753\n",
      "Epoch 3589, Loss: 0.272885799407959, Final Batch Loss: 0.1262965202331543\n",
      "Epoch 3590, Loss: 0.2670919820666313, Final Batch Loss: 0.10493593662977219\n",
      "Epoch 3591, Loss: 0.2569226399064064, Final Batch Loss: 0.155299574136734\n",
      "Epoch 3592, Loss: 0.23416616767644882, Final Batch Loss: 0.10070621222257614\n",
      "Epoch 3593, Loss: 0.31338073313236237, Final Batch Loss: 0.16161775588989258\n",
      "Epoch 3594, Loss: 0.2536304444074631, Final Batch Loss: 0.09714214503765106\n",
      "Epoch 3595, Loss: 0.30969811975955963, Final Batch Loss: 0.1512489765882492\n",
      "Epoch 3596, Loss: 0.26569536328315735, Final Batch Loss: 0.1561112105846405\n",
      "Epoch 3597, Loss: 0.3033060133457184, Final Batch Loss: 0.13051795959472656\n",
      "Epoch 3598, Loss: 0.3393522799015045, Final Batch Loss: 0.2085731029510498\n",
      "Epoch 3599, Loss: 0.2565896809101105, Final Batch Loss: 0.14182470738887787\n",
      "Epoch 3600, Loss: 0.26814042031764984, Final Batch Loss: 0.13300055265426636\n",
      "Epoch 3601, Loss: 0.2504020631313324, Final Batch Loss: 0.13446871936321259\n",
      "Epoch 3602, Loss: 0.23653221875429153, Final Batch Loss: 0.10994125157594681\n",
      "Epoch 3603, Loss: 0.25849566608667374, Final Batch Loss: 0.13438768684864044\n",
      "Epoch 3604, Loss: 0.2822222411632538, Final Batch Loss: 0.16281545162200928\n",
      "Epoch 3605, Loss: 0.2674008905887604, Final Batch Loss: 0.1348954737186432\n",
      "Epoch 3606, Loss: 0.27803169190883636, Final Batch Loss: 0.1350809633731842\n",
      "Epoch 3607, Loss: 0.22474341094493866, Final Batch Loss: 0.10415249317884445\n",
      "Epoch 3608, Loss: 0.22996950149536133, Final Batch Loss: 0.11177171021699905\n",
      "Epoch 3609, Loss: 0.2927857041358948, Final Batch Loss: 0.08607673645019531\n",
      "Epoch 3610, Loss: 0.2902715429663658, Final Batch Loss: 0.1734294444322586\n",
      "Epoch 3611, Loss: 0.2908662259578705, Final Batch Loss: 0.1375618875026703\n",
      "Epoch 3612, Loss: 0.258969284594059, Final Batch Loss: 0.14265456795692444\n",
      "Epoch 3613, Loss: 0.24803075194358826, Final Batch Loss: 0.12362637370824814\n",
      "Epoch 3614, Loss: 0.2899702340364456, Final Batch Loss: 0.13581430912017822\n",
      "Epoch 3615, Loss: 0.30418233573436737, Final Batch Loss: 0.14677801728248596\n",
      "Epoch 3616, Loss: 0.268234983086586, Final Batch Loss: 0.12639927864074707\n",
      "Epoch 3617, Loss: 0.2599741816520691, Final Batch Loss: 0.10981056094169617\n",
      "Epoch 3618, Loss: 0.23878458887338638, Final Batch Loss: 0.12846246361732483\n",
      "Epoch 3619, Loss: 0.25269920378923416, Final Batch Loss: 0.14550566673278809\n",
      "Epoch 3620, Loss: 0.2866627871990204, Final Batch Loss: 0.1706433892250061\n",
      "Epoch 3621, Loss: 0.34638215601444244, Final Batch Loss: 0.24611404538154602\n",
      "Epoch 3622, Loss: 0.3238968998193741, Final Batch Loss: 0.17702332139015198\n",
      "Epoch 3623, Loss: 0.3606862276792526, Final Batch Loss: 0.2200976461172104\n",
      "Epoch 3624, Loss: 0.20244144648313522, Final Batch Loss: 0.10101216286420822\n",
      "Epoch 3625, Loss: 0.22766303271055222, Final Batch Loss: 0.09947038441896439\n",
      "Epoch 3626, Loss: 0.2713427022099495, Final Batch Loss: 0.16087515652179718\n",
      "Epoch 3627, Loss: 0.23228194564580917, Final Batch Loss: 0.10701126605272293\n",
      "Epoch 3628, Loss: 0.2808450534939766, Final Batch Loss: 0.15797138214111328\n",
      "Epoch 3629, Loss: 0.3036789298057556, Final Batch Loss: 0.1689348816871643\n",
      "Epoch 3630, Loss: 0.26159224659204483, Final Batch Loss: 0.16575919091701508\n",
      "Epoch 3631, Loss: 0.39267122745513916, Final Batch Loss: 0.23099027574062347\n",
      "Epoch 3632, Loss: 0.264600470662117, Final Batch Loss: 0.15350019931793213\n",
      "Epoch 3633, Loss: 0.2491995021700859, Final Batch Loss: 0.08357200771570206\n",
      "Epoch 3634, Loss: 0.26046937704086304, Final Batch Loss: 0.11895100772380829\n",
      "Epoch 3635, Loss: 0.2589057832956314, Final Batch Loss: 0.10233737528324127\n",
      "Epoch 3636, Loss: 0.2629566639661789, Final Batch Loss: 0.13226284086704254\n",
      "Epoch 3637, Loss: 0.30090266466140747, Final Batch Loss: 0.1552056521177292\n",
      "Epoch 3638, Loss: 0.263468436896801, Final Batch Loss: 0.13906680047512054\n",
      "Epoch 3639, Loss: 0.293972909450531, Final Batch Loss: 0.12549850344657898\n",
      "Epoch 3640, Loss: 0.2440454363822937, Final Batch Loss: 0.12998183071613312\n",
      "Epoch 3641, Loss: 0.2667563408613205, Final Batch Loss: 0.13608893752098083\n",
      "Epoch 3642, Loss: 0.24465586990118027, Final Batch Loss: 0.1153234913945198\n",
      "Epoch 3643, Loss: 0.27294038981199265, Final Batch Loss: 0.11778637021780014\n",
      "Epoch 3644, Loss: 0.2178463563323021, Final Batch Loss: 0.09620026499032974\n",
      "Epoch 3645, Loss: 0.3019576817750931, Final Batch Loss: 0.17694924771785736\n",
      "Epoch 3646, Loss: 0.2574557662010193, Final Batch Loss: 0.1431794911623001\n",
      "Epoch 3647, Loss: 0.24272556602954865, Final Batch Loss: 0.09128983318805695\n",
      "Epoch 3648, Loss: 0.26302365958690643, Final Batch Loss: 0.1462491750717163\n",
      "Epoch 3649, Loss: 0.2529750466346741, Final Batch Loss: 0.15059132874011993\n",
      "Epoch 3650, Loss: 0.21382379531860352, Final Batch Loss: 0.09922127425670624\n",
      "Epoch 3651, Loss: 0.30514831840991974, Final Batch Loss: 0.16000543534755707\n",
      "Epoch 3652, Loss: 0.26056642830371857, Final Batch Loss: 0.1328027993440628\n",
      "Epoch 3653, Loss: 0.27954426407814026, Final Batch Loss: 0.13856151700019836\n",
      "Epoch 3654, Loss: 0.2773766666650772, Final Batch Loss: 0.13740424811840057\n",
      "Epoch 3655, Loss: 0.25686323642730713, Final Batch Loss: 0.13235744833946228\n",
      "Epoch 3656, Loss: 0.3197222352027893, Final Batch Loss: 0.15674525499343872\n",
      "Epoch 3657, Loss: 0.2871384769678116, Final Batch Loss: 0.17088359594345093\n",
      "Epoch 3658, Loss: 0.3415959030389786, Final Batch Loss: 0.2003050297498703\n",
      "Epoch 3659, Loss: 0.23298200219869614, Final Batch Loss: 0.08646056801080704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3660, Loss: 0.26298874616622925, Final Batch Loss: 0.11981399357318878\n",
      "Epoch 3661, Loss: 0.2585970088839531, Final Batch Loss: 0.15226954221725464\n",
      "Epoch 3662, Loss: 0.23972585797309875, Final Batch Loss: 0.11328965425491333\n",
      "Epoch 3663, Loss: 0.29287542402744293, Final Batch Loss: 0.1663496345281601\n",
      "Epoch 3664, Loss: 0.3527512103319168, Final Batch Loss: 0.22754895687103271\n",
      "Epoch 3665, Loss: 0.31705859303474426, Final Batch Loss: 0.15679772198200226\n",
      "Epoch 3666, Loss: 0.2104571983218193, Final Batch Loss: 0.10707496106624603\n",
      "Epoch 3667, Loss: 0.26206762343645096, Final Batch Loss: 0.09719570726156235\n",
      "Epoch 3668, Loss: 0.2661476582288742, Final Batch Loss: 0.1354411393404007\n",
      "Epoch 3669, Loss: 0.26335009187459946, Final Batch Loss: 0.11878690868616104\n",
      "Epoch 3670, Loss: 0.231513150036335, Final Batch Loss: 0.11360316723585129\n",
      "Epoch 3671, Loss: 0.2744612619280815, Final Batch Loss: 0.1523990035057068\n",
      "Epoch 3672, Loss: 0.31281308829784393, Final Batch Loss: 0.15599486231803894\n",
      "Epoch 3673, Loss: 0.287592276930809, Final Batch Loss: 0.14555048942565918\n",
      "Epoch 3674, Loss: 0.23458173125982285, Final Batch Loss: 0.12258213013410568\n",
      "Epoch 3675, Loss: 0.23421523720026016, Final Batch Loss: 0.09993103891611099\n",
      "Epoch 3676, Loss: 0.257466584444046, Final Batch Loss: 0.09832148253917694\n",
      "Epoch 3677, Loss: 0.29028572887182236, Final Batch Loss: 0.17439886927604675\n",
      "Epoch 3678, Loss: 0.277379646897316, Final Batch Loss: 0.11344517767429352\n",
      "Epoch 3679, Loss: 0.30405184626579285, Final Batch Loss: 0.12061792612075806\n",
      "Epoch 3680, Loss: 0.3401337116956711, Final Batch Loss: 0.16583764553070068\n",
      "Epoch 3681, Loss: 0.24589862674474716, Final Batch Loss: 0.12128224223852158\n",
      "Epoch 3682, Loss: 0.2830960378050804, Final Batch Loss: 0.16146714985370636\n",
      "Epoch 3683, Loss: 0.21374303102493286, Final Batch Loss: 0.1017070785164833\n",
      "Epoch 3684, Loss: 0.20728985965251923, Final Batch Loss: 0.07432976365089417\n",
      "Epoch 3685, Loss: 0.2631918340921402, Final Batch Loss: 0.12777242064476013\n",
      "Epoch 3686, Loss: 0.22075700759887695, Final Batch Loss: 0.10830210894346237\n",
      "Epoch 3687, Loss: 0.2560313791036606, Final Batch Loss: 0.1110369861125946\n",
      "Epoch 3688, Loss: 0.22875682264566422, Final Batch Loss: 0.12968258559703827\n",
      "Epoch 3689, Loss: 0.24577613919973373, Final Batch Loss: 0.12967385351657867\n",
      "Epoch 3690, Loss: 0.3813324272632599, Final Batch Loss: 0.2551843822002411\n",
      "Epoch 3691, Loss: 0.253024160861969, Final Batch Loss: 0.13262446224689484\n",
      "Epoch 3692, Loss: 0.24662360548973083, Final Batch Loss: 0.12518204748630524\n",
      "Epoch 3693, Loss: 0.2756129801273346, Final Batch Loss: 0.1468692421913147\n",
      "Epoch 3694, Loss: 0.29166457802057266, Final Batch Loss: 0.17565293610095978\n",
      "Epoch 3695, Loss: 0.27055974304676056, Final Batch Loss: 0.118165984749794\n",
      "Epoch 3696, Loss: 0.28035448491573334, Final Batch Loss: 0.14520813524723053\n",
      "Epoch 3697, Loss: 0.2219703048467636, Final Batch Loss: 0.11485017836093903\n",
      "Epoch 3698, Loss: 0.28136196732521057, Final Batch Loss: 0.11961673200130463\n",
      "Epoch 3699, Loss: 0.26068251579999924, Final Batch Loss: 0.15196223556995392\n",
      "Epoch 3700, Loss: 0.2335575893521309, Final Batch Loss: 0.1110663115978241\n",
      "Epoch 3701, Loss: 0.1964259073138237, Final Batch Loss: 0.08628928661346436\n",
      "Epoch 3702, Loss: 0.29732485115528107, Final Batch Loss: 0.135993093252182\n",
      "Epoch 3703, Loss: 0.29565195739269257, Final Batch Loss: 0.1460391730070114\n",
      "Epoch 3704, Loss: 0.2428073063492775, Final Batch Loss: 0.11179495602846146\n",
      "Epoch 3705, Loss: 0.2674940377473831, Final Batch Loss: 0.09120021760463715\n",
      "Epoch 3706, Loss: 0.2939063534140587, Final Batch Loss: 0.18286602199077606\n",
      "Epoch 3707, Loss: 0.2546473741531372, Final Batch Loss: 0.11106078326702118\n",
      "Epoch 3708, Loss: 0.26173480600118637, Final Batch Loss: 0.10173047333955765\n",
      "Epoch 3709, Loss: 0.231255404651165, Final Batch Loss: 0.11182808130979538\n",
      "Epoch 3710, Loss: 0.27669619023799896, Final Batch Loss: 0.14161039888858795\n",
      "Epoch 3711, Loss: 0.24797486513853073, Final Batch Loss: 0.12650999426841736\n",
      "Epoch 3712, Loss: 0.2865792065858841, Final Batch Loss: 0.11934250593185425\n",
      "Epoch 3713, Loss: 0.24850644916296005, Final Batch Loss: 0.15399865806102753\n",
      "Epoch 3714, Loss: 0.2116912454366684, Final Batch Loss: 0.09790262579917908\n",
      "Epoch 3715, Loss: 0.26790517568588257, Final Batch Loss: 0.1370587944984436\n",
      "Epoch 3716, Loss: 0.2227010801434517, Final Batch Loss: 0.10865689069032669\n",
      "Epoch 3717, Loss: 0.24372494965791702, Final Batch Loss: 0.1376558244228363\n",
      "Epoch 3718, Loss: 0.19914285093545914, Final Batch Loss: 0.08428066223859787\n",
      "Epoch 3719, Loss: 0.30710816383361816, Final Batch Loss: 0.18732447922229767\n",
      "Epoch 3720, Loss: 0.24970722198486328, Final Batch Loss: 0.12828606367111206\n",
      "Epoch 3721, Loss: 0.2489292398095131, Final Batch Loss: 0.10480601340532303\n",
      "Epoch 3722, Loss: 0.27843645960092545, Final Batch Loss: 0.12174452096223831\n",
      "Epoch 3723, Loss: 0.2529174014925957, Final Batch Loss: 0.13123467564582825\n",
      "Epoch 3724, Loss: 0.2617228478193283, Final Batch Loss: 0.13610288500785828\n",
      "Epoch 3725, Loss: 0.2760051116347313, Final Batch Loss: 0.11016619950532913\n",
      "Epoch 3726, Loss: 0.2410961017012596, Final Batch Loss: 0.09376242011785507\n",
      "Epoch 3727, Loss: 0.23650646954774857, Final Batch Loss: 0.12403764575719833\n",
      "Epoch 3728, Loss: 0.26015739142894745, Final Batch Loss: 0.10069037973880768\n",
      "Epoch 3729, Loss: 0.268053375184536, Final Batch Loss: 0.12459217756986618\n",
      "Epoch 3730, Loss: 0.2295435667037964, Final Batch Loss: 0.09685367345809937\n",
      "Epoch 3731, Loss: 0.23262201994657516, Final Batch Loss: 0.12619419395923615\n",
      "Epoch 3732, Loss: 0.2529097720980644, Final Batch Loss: 0.1286710947751999\n",
      "Epoch 3733, Loss: 0.2703220248222351, Final Batch Loss: 0.1325947642326355\n",
      "Epoch 3734, Loss: 0.25783322006464005, Final Batch Loss: 0.11733020097017288\n",
      "Epoch 3735, Loss: 0.2391485646367073, Final Batch Loss: 0.1339324414730072\n",
      "Epoch 3736, Loss: 0.2888697683811188, Final Batch Loss: 0.14574606716632843\n",
      "Epoch 3737, Loss: 0.30135658383369446, Final Batch Loss: 0.16846689581871033\n",
      "Epoch 3738, Loss: 0.35162901878356934, Final Batch Loss: 0.21296867728233337\n",
      "Epoch 3739, Loss: 0.2203214392066002, Final Batch Loss: 0.12105593085289001\n",
      "Epoch 3740, Loss: 0.2378731593489647, Final Batch Loss: 0.10667102783918381\n",
      "Epoch 3741, Loss: 0.2756689116358757, Final Batch Loss: 0.18223753571510315\n",
      "Epoch 3742, Loss: 0.24589452147483826, Final Batch Loss: 0.15047027170658112\n",
      "Epoch 3743, Loss: 0.3176785856485367, Final Batch Loss: 0.16096577048301697\n",
      "Epoch 3744, Loss: 0.35605454444885254, Final Batch Loss: 0.19079220294952393\n",
      "Epoch 3745, Loss: 0.3288303464651108, Final Batch Loss: 0.13894447684288025\n",
      "Epoch 3746, Loss: 0.24752992391586304, Final Batch Loss: 0.10703107714653015\n",
      "Epoch 3747, Loss: 0.24047303199768066, Final Batch Loss: 0.13348333537578583\n",
      "Epoch 3748, Loss: 0.2362433522939682, Final Batch Loss: 0.11600533127784729\n",
      "Epoch 3749, Loss: 0.23705172538757324, Final Batch Loss: 0.09710729122161865\n",
      "Epoch 3750, Loss: 0.25115717202425003, Final Batch Loss: 0.11407805234193802\n",
      "Epoch 3751, Loss: 0.27957963943481445, Final Batch Loss: 0.12114313244819641\n",
      "Epoch 3752, Loss: 0.25794434547424316, Final Batch Loss: 0.12601661682128906\n",
      "Epoch 3753, Loss: 0.2761841267347336, Final Batch Loss: 0.13493500649929047\n",
      "Epoch 3754, Loss: 0.294204518198967, Final Batch Loss: 0.11472281813621521\n",
      "Epoch 3755, Loss: 0.2564058229327202, Final Batch Loss: 0.1382332593202591\n",
      "Epoch 3756, Loss: 0.22128626704216003, Final Batch Loss: 0.10574052482843399\n",
      "Epoch 3757, Loss: 0.27966807782649994, Final Batch Loss: 0.14441461861133575\n",
      "Epoch 3758, Loss: 0.2450973391532898, Final Batch Loss: 0.1493539810180664\n",
      "Epoch 3759, Loss: 0.2765139788389206, Final Batch Loss: 0.13544537127017975\n",
      "Epoch 3760, Loss: 0.24084246158599854, Final Batch Loss: 0.11552326381206512\n",
      "Epoch 3761, Loss: 0.30857236683368683, Final Batch Loss: 0.1770601123571396\n",
      "Epoch 3762, Loss: 0.33901387453079224, Final Batch Loss: 0.23255500197410583\n",
      "Epoch 3763, Loss: 0.20676881819963455, Final Batch Loss: 0.110825315117836\n",
      "Epoch 3764, Loss: 0.2190730795264244, Final Batch Loss: 0.11843886226415634\n",
      "Epoch 3765, Loss: 0.31210627406835556, Final Batch Loss: 0.22047530114650726\n",
      "Epoch 3766, Loss: 0.2448442131280899, Final Batch Loss: 0.13272899389266968\n",
      "Epoch 3767, Loss: 0.20951850712299347, Final Batch Loss: 0.1014225110411644\n",
      "Epoch 3768, Loss: 0.3344186916947365, Final Batch Loss: 0.21470867097377777\n",
      "Epoch 3769, Loss: 0.2768786996603012, Final Batch Loss: 0.12954451143741608\n",
      "Epoch 3770, Loss: 0.22902213782072067, Final Batch Loss: 0.11582068353891373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3771, Loss: 0.26016145944595337, Final Batch Loss: 0.1292327642440796\n",
      "Epoch 3772, Loss: 0.2440718188881874, Final Batch Loss: 0.10938320308923721\n",
      "Epoch 3773, Loss: 0.30773548781871796, Final Batch Loss: 0.16205909848213196\n",
      "Epoch 3774, Loss: 0.2998206913471222, Final Batch Loss: 0.1763978749513626\n",
      "Epoch 3775, Loss: 0.23978553712368011, Final Batch Loss: 0.09474024176597595\n",
      "Epoch 3776, Loss: 0.21206888556480408, Final Batch Loss: 0.0937994122505188\n",
      "Epoch 3777, Loss: 0.28176990151405334, Final Batch Loss: 0.15018723905086517\n",
      "Epoch 3778, Loss: 0.28486455976963043, Final Batch Loss: 0.1352078914642334\n",
      "Epoch 3779, Loss: 0.23855619132518768, Final Batch Loss: 0.10780560970306396\n",
      "Epoch 3780, Loss: 0.2072657123208046, Final Batch Loss: 0.1052846759557724\n",
      "Epoch 3781, Loss: 0.24095893651247025, Final Batch Loss: 0.10275693982839584\n",
      "Epoch 3782, Loss: 0.331177182495594, Final Batch Loss: 0.22310036420822144\n",
      "Epoch 3783, Loss: 0.267481230199337, Final Batch Loss: 0.16711761057376862\n",
      "Epoch 3784, Loss: 0.24452074617147446, Final Batch Loss: 0.11093904823064804\n",
      "Epoch 3785, Loss: 0.2047218605875969, Final Batch Loss: 0.09450683742761612\n",
      "Epoch 3786, Loss: 0.25012338161468506, Final Batch Loss: 0.1310390681028366\n",
      "Epoch 3787, Loss: 0.3116542547941208, Final Batch Loss: 0.1585594266653061\n",
      "Epoch 3788, Loss: 0.22245217114686966, Final Batch Loss: 0.10445677489042282\n",
      "Epoch 3789, Loss: 0.256496861577034, Final Batch Loss: 0.1264248639345169\n",
      "Epoch 3790, Loss: 0.22293490916490555, Final Batch Loss: 0.11803704500198364\n",
      "Epoch 3791, Loss: 0.2826268821954727, Final Batch Loss: 0.16044814884662628\n",
      "Epoch 3792, Loss: 0.2996598184108734, Final Batch Loss: 0.12816449999809265\n",
      "Epoch 3793, Loss: 0.31364908814430237, Final Batch Loss: 0.19712363183498383\n",
      "Epoch 3794, Loss: 0.21436534821987152, Final Batch Loss: 0.09188351035118103\n",
      "Epoch 3795, Loss: 0.21437262743711472, Final Batch Loss: 0.09862799197435379\n",
      "Epoch 3796, Loss: 0.25910669565200806, Final Batch Loss: 0.13713477551937103\n",
      "Epoch 3797, Loss: 0.3741331249475479, Final Batch Loss: 0.20193448662757874\n",
      "Epoch 3798, Loss: 0.2690271735191345, Final Batch Loss: 0.13796287775039673\n",
      "Epoch 3799, Loss: 0.23408252000808716, Final Batch Loss: 0.11617503315210342\n",
      "Epoch 3800, Loss: 0.24990933388471603, Final Batch Loss: 0.09928364306688309\n",
      "Epoch 3801, Loss: 0.28454601764678955, Final Batch Loss: 0.16638807952404022\n",
      "Epoch 3802, Loss: 0.22262459993362427, Final Batch Loss: 0.10819991677999496\n",
      "Epoch 3803, Loss: 0.29584166407585144, Final Batch Loss: 0.11280079185962677\n",
      "Epoch 3804, Loss: 0.23324933648109436, Final Batch Loss: 0.11418867111206055\n",
      "Epoch 3805, Loss: 0.2144913449883461, Final Batch Loss: 0.09517839550971985\n",
      "Epoch 3806, Loss: 0.2126990109682083, Final Batch Loss: 0.09279162436723709\n",
      "Epoch 3807, Loss: 0.2705538272857666, Final Batch Loss: 0.11665186285972595\n",
      "Epoch 3808, Loss: 0.2363438382744789, Final Batch Loss: 0.10750552266836166\n",
      "Epoch 3809, Loss: 0.20118733495473862, Final Batch Loss: 0.10424874722957611\n",
      "Epoch 3810, Loss: 0.24083612859249115, Final Batch Loss: 0.1015070229768753\n",
      "Epoch 3811, Loss: 0.29987606406211853, Final Batch Loss: 0.15874701738357544\n",
      "Epoch 3812, Loss: 0.23885924369096756, Final Batch Loss: 0.09006712585687637\n",
      "Epoch 3813, Loss: 0.2650265693664551, Final Batch Loss: 0.1079818606376648\n",
      "Epoch 3814, Loss: 0.25265146791934967, Final Batch Loss: 0.09947684407234192\n",
      "Epoch 3815, Loss: 0.26529888063669205, Final Batch Loss: 0.1150551363825798\n",
      "Epoch 3816, Loss: 0.2779003977775574, Final Batch Loss: 0.10076665878295898\n",
      "Epoch 3817, Loss: 0.2234673947095871, Final Batch Loss: 0.10968416929244995\n",
      "Epoch 3818, Loss: 0.32200339436531067, Final Batch Loss: 0.19641061127185822\n",
      "Epoch 3819, Loss: 0.2019343227148056, Final Batch Loss: 0.0928054228425026\n",
      "Epoch 3820, Loss: 0.2869246080517769, Final Batch Loss: 0.16219334304332733\n",
      "Epoch 3821, Loss: 0.22980913519859314, Final Batch Loss: 0.10891833901405334\n",
      "Epoch 3822, Loss: 0.24412769079208374, Final Batch Loss: 0.11358791589736938\n",
      "Epoch 3823, Loss: 0.21367143839597702, Final Batch Loss: 0.09603804349899292\n",
      "Epoch 3824, Loss: 0.24551589787006378, Final Batch Loss: 0.14277729392051697\n",
      "Epoch 3825, Loss: 0.23190582543611526, Final Batch Loss: 0.11959514766931534\n",
      "Epoch 3826, Loss: 0.24197138845920563, Final Batch Loss: 0.0857076346874237\n",
      "Epoch 3827, Loss: 0.2550894469022751, Final Batch Loss: 0.1435575634241104\n",
      "Epoch 3828, Loss: 0.23250465095043182, Final Batch Loss: 0.10605117678642273\n",
      "Epoch 3829, Loss: 0.2119995579123497, Final Batch Loss: 0.10225369781255722\n",
      "Epoch 3830, Loss: 0.24827884137630463, Final Batch Loss: 0.13395999372005463\n",
      "Epoch 3831, Loss: 0.29346174001693726, Final Batch Loss: 0.1562941074371338\n",
      "Epoch 3832, Loss: 0.22494342178106308, Final Batch Loss: 0.10228582471609116\n",
      "Epoch 3833, Loss: 0.21269334107637405, Final Batch Loss: 0.09241847693920135\n",
      "Epoch 3834, Loss: 0.19921330362558365, Final Batch Loss: 0.09113374352455139\n",
      "Epoch 3835, Loss: 0.23179755359888077, Final Batch Loss: 0.14852330088615417\n",
      "Epoch 3836, Loss: 0.24861665815114975, Final Batch Loss: 0.1189718171954155\n",
      "Epoch 3837, Loss: 0.23750440776348114, Final Batch Loss: 0.10874934494495392\n",
      "Epoch 3838, Loss: 0.2722788453102112, Final Batch Loss: 0.10088483989238739\n",
      "Epoch 3839, Loss: 0.21919240802526474, Final Batch Loss: 0.11332856863737106\n",
      "Epoch 3840, Loss: 0.2772361785173416, Final Batch Loss: 0.14801542460918427\n",
      "Epoch 3841, Loss: 0.3423387110233307, Final Batch Loss: 0.22432172298431396\n",
      "Epoch 3842, Loss: 0.3003404587507248, Final Batch Loss: 0.10958623886108398\n",
      "Epoch 3843, Loss: 0.21249669045209885, Final Batch Loss: 0.11196979880332947\n",
      "Epoch 3844, Loss: 0.28171979635953903, Final Batch Loss: 0.15950366854667664\n",
      "Epoch 3845, Loss: 0.3222673386335373, Final Batch Loss: 0.16436994075775146\n",
      "Epoch 3846, Loss: 0.3244365304708481, Final Batch Loss: 0.1561126410961151\n",
      "Epoch 3847, Loss: 0.29028744995594025, Final Batch Loss: 0.1596277356147766\n",
      "Epoch 3848, Loss: 0.21593879908323288, Final Batch Loss: 0.09897580742835999\n",
      "Epoch 3849, Loss: 0.3557494580745697, Final Batch Loss: 0.1986507922410965\n",
      "Epoch 3850, Loss: 0.22876829653978348, Final Batch Loss: 0.09670134633779526\n",
      "Epoch 3851, Loss: 0.23911012709140778, Final Batch Loss: 0.14286288619041443\n",
      "Epoch 3852, Loss: 0.2695217579603195, Final Batch Loss: 0.12481550872325897\n",
      "Epoch 3853, Loss: 0.2504771575331688, Final Batch Loss: 0.10761641710996628\n",
      "Epoch 3854, Loss: 0.2443876564502716, Final Batch Loss: 0.1149950623512268\n",
      "Epoch 3855, Loss: 0.2326955944299698, Final Batch Loss: 0.13232101500034332\n",
      "Epoch 3856, Loss: 0.2523331418633461, Final Batch Loss: 0.1344945728778839\n",
      "Epoch 3857, Loss: 0.23865844309329987, Final Batch Loss: 0.11652673780918121\n",
      "Epoch 3858, Loss: 0.3349250629544258, Final Batch Loss: 0.2194918394088745\n",
      "Epoch 3859, Loss: 0.23924905806779861, Final Batch Loss: 0.12159169465303421\n",
      "Epoch 3860, Loss: 0.23751332610845566, Final Batch Loss: 0.11786563694477081\n",
      "Epoch 3861, Loss: 0.2428150773048401, Final Batch Loss: 0.12799720466136932\n",
      "Epoch 3862, Loss: 0.26595450937747955, Final Batch Loss: 0.16412575542926788\n",
      "Epoch 3863, Loss: 0.23748263716697693, Final Batch Loss: 0.1110600084066391\n",
      "Epoch 3864, Loss: 0.21066570281982422, Final Batch Loss: 0.07035781443119049\n",
      "Epoch 3865, Loss: 0.24908441305160522, Final Batch Loss: 0.13251332938671112\n",
      "Epoch 3866, Loss: 0.24034008383750916, Final Batch Loss: 0.11619758605957031\n",
      "Epoch 3867, Loss: 0.257392555475235, Final Batch Loss: 0.15090516209602356\n",
      "Epoch 3868, Loss: 0.23763194680213928, Final Batch Loss: 0.12458409368991852\n",
      "Epoch 3869, Loss: 0.27426277846097946, Final Batch Loss: 0.16632351279258728\n",
      "Epoch 3870, Loss: 0.22715122997760773, Final Batch Loss: 0.13347682356834412\n",
      "Epoch 3871, Loss: 0.2939322590827942, Final Batch Loss: 0.15634378790855408\n",
      "Epoch 3872, Loss: 0.22118014097213745, Final Batch Loss: 0.10050638020038605\n",
      "Epoch 3873, Loss: 0.2733532190322876, Final Batch Loss: 0.14738456904888153\n",
      "Epoch 3874, Loss: 0.2680140733718872, Final Batch Loss: 0.166298970580101\n",
      "Epoch 3875, Loss: 0.3044317662715912, Final Batch Loss: 0.15068748593330383\n",
      "Epoch 3876, Loss: 0.2801266089081764, Final Batch Loss: 0.11339335888624191\n",
      "Epoch 3877, Loss: 0.3207214027643204, Final Batch Loss: 0.1827925145626068\n",
      "Epoch 3878, Loss: 0.28480975329875946, Final Batch Loss: 0.12716476619243622\n",
      "Epoch 3879, Loss: 0.25420568883419037, Final Batch Loss: 0.13212941586971283\n",
      "Epoch 3880, Loss: 0.21584203839302063, Final Batch Loss: 0.08360634744167328\n",
      "Epoch 3881, Loss: 0.2553825154900551, Final Batch Loss: 0.1509176641702652\n",
      "Epoch 3882, Loss: 0.20593849569559097, Final Batch Loss: 0.1088874563574791\n",
      "Epoch 3883, Loss: 0.21296243369579315, Final Batch Loss: 0.08359576761722565\n",
      "Epoch 3884, Loss: 0.2466573640704155, Final Batch Loss: 0.09903175383806229\n",
      "Epoch 3885, Loss: 0.25402824580669403, Final Batch Loss: 0.13602685928344727\n",
      "Epoch 3886, Loss: 0.24685193598270416, Final Batch Loss: 0.14505057036876678\n",
      "Epoch 3887, Loss: 0.26841748505830765, Final Batch Loss: 0.12123674899339676\n",
      "Epoch 3888, Loss: 0.19518288224935532, Final Batch Loss: 0.10337755084037781\n",
      "Epoch 3889, Loss: 0.2683240845799446, Final Batch Loss: 0.1495383232831955\n",
      "Epoch 3890, Loss: 0.28751229494810104, Final Batch Loss: 0.16662490367889404\n",
      "Epoch 3891, Loss: 0.361449733376503, Final Batch Loss: 0.18207955360412598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3892, Loss: 0.30951405316591263, Final Batch Loss: 0.10798599570989609\n",
      "Epoch 3893, Loss: 0.22964348644018173, Final Batch Loss: 0.10345237702131271\n",
      "Epoch 3894, Loss: 0.24538438767194748, Final Batch Loss: 0.10322587937116623\n",
      "Epoch 3895, Loss: 0.24472838640213013, Final Batch Loss: 0.13580229878425598\n",
      "Epoch 3896, Loss: 0.3030903562903404, Final Batch Loss: 0.19663427770137787\n",
      "Epoch 3897, Loss: 0.2758636027574539, Final Batch Loss: 0.13802921772003174\n",
      "Epoch 3898, Loss: 0.19976559281349182, Final Batch Loss: 0.10750996321439743\n",
      "Epoch 3899, Loss: 0.23435087502002716, Final Batch Loss: 0.09613850712776184\n",
      "Epoch 3900, Loss: 0.27464354038238525, Final Batch Loss: 0.1458856165409088\n",
      "Epoch 3901, Loss: 0.2630322203040123, Final Batch Loss: 0.16088683903217316\n",
      "Epoch 3902, Loss: 0.22828210145235062, Final Batch Loss: 0.12902064621448517\n",
      "Epoch 3903, Loss: 0.2580939680337906, Final Batch Loss: 0.12386772036552429\n",
      "Epoch 3904, Loss: 0.27836109697818756, Final Batch Loss: 0.1410139501094818\n",
      "Epoch 3905, Loss: 0.2594131678342819, Final Batch Loss: 0.113888680934906\n",
      "Epoch 3906, Loss: 0.2214089334011078, Final Batch Loss: 0.11471671611070633\n",
      "Epoch 3907, Loss: 0.2932482659816742, Final Batch Loss: 0.16504421830177307\n",
      "Epoch 3908, Loss: 0.24949604272842407, Final Batch Loss: 0.12703783810138702\n",
      "Epoch 3909, Loss: 0.20640743523836136, Final Batch Loss: 0.10613914579153061\n",
      "Epoch 3910, Loss: 0.21644356101751328, Final Batch Loss: 0.10799979418516159\n",
      "Epoch 3911, Loss: 0.24063538014888763, Final Batch Loss: 0.11665607988834381\n",
      "Epoch 3912, Loss: 0.23253951221704483, Final Batch Loss: 0.10889272391796112\n",
      "Epoch 3913, Loss: 0.24412085860967636, Final Batch Loss: 0.11713717132806778\n",
      "Epoch 3914, Loss: 0.21072212606668472, Final Batch Loss: 0.11065208166837692\n",
      "Epoch 3915, Loss: 0.2215052917599678, Final Batch Loss: 0.0918469950556755\n",
      "Epoch 3916, Loss: 0.19707965105772018, Final Batch Loss: 0.10849636048078537\n",
      "Epoch 3917, Loss: 0.26780178397893906, Final Batch Loss: 0.11806676536798477\n",
      "Epoch 3918, Loss: 0.26978982985019684, Final Batch Loss: 0.14148767292499542\n",
      "Epoch 3919, Loss: 0.2144850790500641, Final Batch Loss: 0.0852825939655304\n",
      "Epoch 3920, Loss: 0.2807075008749962, Final Batch Loss: 0.1111917570233345\n",
      "Epoch 3921, Loss: 0.225144624710083, Final Batch Loss: 0.11576447635889053\n",
      "Epoch 3922, Loss: 0.23845477402210236, Final Batch Loss: 0.10968399047851562\n",
      "Epoch 3923, Loss: 0.30053146928548813, Final Batch Loss: 0.19691471755504608\n",
      "Epoch 3924, Loss: 0.240612231194973, Final Batch Loss: 0.12574057281017303\n",
      "Epoch 3925, Loss: 0.24339484423398972, Final Batch Loss: 0.09496036916971207\n",
      "Epoch 3926, Loss: 0.20431313663721085, Final Batch Loss: 0.10157494992017746\n",
      "Epoch 3927, Loss: 0.30902761965990067, Final Batch Loss: 0.19282911717891693\n",
      "Epoch 3928, Loss: 0.26024939864873886, Final Batch Loss: 0.11061472445726395\n",
      "Epoch 3929, Loss: 0.27612772583961487, Final Batch Loss: 0.14935173094272614\n",
      "Epoch 3930, Loss: 0.2231207937002182, Final Batch Loss: 0.09562748670578003\n",
      "Epoch 3931, Loss: 0.2522543892264366, Final Batch Loss: 0.12410517781972885\n",
      "Epoch 3932, Loss: 0.2206268608570099, Final Batch Loss: 0.09206093847751617\n",
      "Epoch 3933, Loss: 0.30987122654914856, Final Batch Loss: 0.1684565544128418\n",
      "Epoch 3934, Loss: 0.19967906177043915, Final Batch Loss: 0.09382934123277664\n",
      "Epoch 3935, Loss: 0.29803574085235596, Final Batch Loss: 0.1555332988500595\n",
      "Epoch 3936, Loss: 0.2193932756781578, Final Batch Loss: 0.10263345390558243\n",
      "Epoch 3937, Loss: 0.249958798289299, Final Batch Loss: 0.14209163188934326\n",
      "Epoch 3938, Loss: 0.229966938495636, Final Batch Loss: 0.11685152351856232\n",
      "Epoch 3939, Loss: 0.2645009011030197, Final Batch Loss: 0.11586672067642212\n",
      "Epoch 3940, Loss: 0.20895115286111832, Final Batch Loss: 0.09206368774175644\n",
      "Epoch 3941, Loss: 0.29649170488119125, Final Batch Loss: 0.2040983885526657\n",
      "Epoch 3942, Loss: 0.32380275428295135, Final Batch Loss: 0.1434878706932068\n",
      "Epoch 3943, Loss: 0.30027370899915695, Final Batch Loss: 0.18432722985744476\n",
      "Epoch 3944, Loss: 0.22472554445266724, Final Batch Loss: 0.08819112181663513\n",
      "Epoch 3945, Loss: 0.25401437282562256, Final Batch Loss: 0.1131991595029831\n",
      "Epoch 3946, Loss: 0.22103815525770187, Final Batch Loss: 0.10551787912845612\n",
      "Epoch 3947, Loss: 0.20926066488027573, Final Batch Loss: 0.08528247475624084\n",
      "Epoch 3948, Loss: 0.19755910336971283, Final Batch Loss: 0.09973754733800888\n",
      "Epoch 3949, Loss: 0.24401559680700302, Final Batch Loss: 0.11598353832960129\n",
      "Epoch 3950, Loss: 0.19723927229642868, Final Batch Loss: 0.06805259734392166\n",
      "Epoch 3951, Loss: 0.20801465213298798, Final Batch Loss: 0.07226203382015228\n",
      "Epoch 3952, Loss: 0.2698211818933487, Final Batch Loss: 0.12301698327064514\n",
      "Epoch 3953, Loss: 0.23076069355010986, Final Batch Loss: 0.11806891858577728\n",
      "Epoch 3954, Loss: 0.2504693791270256, Final Batch Loss: 0.13896633684635162\n",
      "Epoch 3955, Loss: 0.2722298204898834, Final Batch Loss: 0.14114874601364136\n",
      "Epoch 3956, Loss: 0.22406309843063354, Final Batch Loss: 0.10966187715530396\n",
      "Epoch 3957, Loss: 0.26925550401210785, Final Batch Loss: 0.13827408850193024\n",
      "Epoch 3958, Loss: 0.17710495740175247, Final Batch Loss: 0.07978324592113495\n",
      "Epoch 3959, Loss: 0.20946891605854034, Final Batch Loss: 0.11431270092725754\n",
      "Epoch 3960, Loss: 0.2353869006037712, Final Batch Loss: 0.14053718745708466\n",
      "Epoch 3961, Loss: 0.3070293590426445, Final Batch Loss: 0.21188651025295258\n",
      "Epoch 3962, Loss: 0.20624983310699463, Final Batch Loss: 0.12289789319038391\n",
      "Epoch 3963, Loss: 0.2327379733324051, Final Batch Loss: 0.11189952492713928\n",
      "Epoch 3964, Loss: 0.37582211941480637, Final Batch Loss: 0.26951080560684204\n",
      "Epoch 3965, Loss: 0.22725597769021988, Final Batch Loss: 0.0995379164814949\n",
      "Epoch 3966, Loss: 0.20048175752162933, Final Batch Loss: 0.09601709246635437\n",
      "Epoch 3967, Loss: 0.2905312031507492, Final Batch Loss: 0.14202632009983063\n",
      "Epoch 3968, Loss: 0.24572841078042984, Final Batch Loss: 0.09164796024560928\n",
      "Epoch 3969, Loss: 0.18525845557451248, Final Batch Loss: 0.09172852337360382\n",
      "Epoch 3970, Loss: 0.2546610161662102, Final Batch Loss: 0.13995543122291565\n",
      "Epoch 3971, Loss: 0.26366502046585083, Final Batch Loss: 0.1298014521598816\n",
      "Epoch 3972, Loss: 0.2920849844813347, Final Batch Loss: 0.10644044727087021\n",
      "Epoch 3973, Loss: 0.23112406581640244, Final Batch Loss: 0.11201097071170807\n",
      "Epoch 3974, Loss: 0.31602826714515686, Final Batch Loss: 0.18437016010284424\n",
      "Epoch 3975, Loss: 0.26900362968444824, Final Batch Loss: 0.11868277192115784\n",
      "Epoch 3976, Loss: 0.21943647414445877, Final Batch Loss: 0.09747914969921112\n",
      "Epoch 3977, Loss: 0.30060815811157227, Final Batch Loss: 0.14334984123706818\n",
      "Epoch 3978, Loss: 0.24587725847959518, Final Batch Loss: 0.1283072531223297\n",
      "Epoch 3979, Loss: 0.2724566161632538, Final Batch Loss: 0.13906297087669373\n",
      "Epoch 3980, Loss: 0.2674126923084259, Final Batch Loss: 0.16070514917373657\n",
      "Epoch 3981, Loss: 0.22506769746541977, Final Batch Loss: 0.1035752147436142\n",
      "Epoch 3982, Loss: 0.22096430510282516, Final Batch Loss: 0.09679830819368362\n",
      "Epoch 3983, Loss: 0.20890730619430542, Final Batch Loss: 0.09174545109272003\n",
      "Epoch 3984, Loss: 0.24667882174253464, Final Batch Loss: 0.1352277398109436\n",
      "Epoch 3985, Loss: 0.23063133656978607, Final Batch Loss: 0.09232184290885925\n",
      "Epoch 3986, Loss: 0.24913401901721954, Final Batch Loss: 0.12054415047168732\n",
      "Epoch 3987, Loss: 0.21958741545677185, Final Batch Loss: 0.12070169299840927\n",
      "Epoch 3988, Loss: 0.2571878656744957, Final Batch Loss: 0.12335362285375595\n",
      "Epoch 3989, Loss: 0.3029947578907013, Final Batch Loss: 0.16434510052204132\n",
      "Epoch 3990, Loss: 0.2348511442542076, Final Batch Loss: 0.11849137395620346\n",
      "Epoch 3991, Loss: 0.2505417540669441, Final Batch Loss: 0.11604496091604233\n",
      "Epoch 3992, Loss: 0.23492655158042908, Final Batch Loss: 0.13981369137763977\n",
      "Epoch 3993, Loss: 0.22022084891796112, Final Batch Loss: 0.10819073766469955\n",
      "Epoch 3994, Loss: 0.26712533831596375, Final Batch Loss: 0.1384361982345581\n",
      "Epoch 3995, Loss: 0.22865208983421326, Final Batch Loss: 0.09846752882003784\n",
      "Epoch 3996, Loss: 0.286591537296772, Final Batch Loss: 0.10152847319841385\n",
      "Epoch 3997, Loss: 0.23612043261528015, Final Batch Loss: 0.11485861241817474\n",
      "Epoch 3998, Loss: 0.22726106643676758, Final Batch Loss: 0.10417276620864868\n",
      "Epoch 3999, Loss: 0.2352559193968773, Final Batch Loss: 0.1242256835103035\n",
      "Epoch 4000, Loss: 0.2581371143460274, Final Batch Loss: 0.14687316119670868\n",
      "Epoch 4001, Loss: 0.2262503057718277, Final Batch Loss: 0.1276809722185135\n",
      "Epoch 4002, Loss: 0.28964664041996, Final Batch Loss: 0.13134610652923584\n",
      "Epoch 4003, Loss: 0.2376820594072342, Final Batch Loss: 0.13121233880519867\n",
      "Epoch 4004, Loss: 0.22402285039424896, Final Batch Loss: 0.09157858788967133\n",
      "Epoch 4005, Loss: 0.2730526030063629, Final Batch Loss: 0.14708110690116882\n",
      "Epoch 4006, Loss: 0.2690497860312462, Final Batch Loss: 0.1471102386713028\n",
      "Epoch 4007, Loss: 0.24686159193515778, Final Batch Loss: 0.08228686451911926\n",
      "Epoch 4008, Loss: 0.20610521733760834, Final Batch Loss: 0.0886354148387909\n",
      "Epoch 4009, Loss: 0.18613657355308533, Final Batch Loss: 0.08823683112859726\n",
      "Epoch 4010, Loss: 0.2013637274503708, Final Batch Loss: 0.08847220987081528\n",
      "Epoch 4011, Loss: 0.2500324472784996, Final Batch Loss: 0.1173485592007637\n",
      "Epoch 4012, Loss: 0.21566183120012283, Final Batch Loss: 0.1153874546289444\n",
      "Epoch 4013, Loss: 0.2655915766954422, Final Batch Loss: 0.13724543154239655\n",
      "Epoch 4014, Loss: 0.22527074068784714, Final Batch Loss: 0.14401838183403015\n",
      "Epoch 4015, Loss: 0.28563831746578217, Final Batch Loss: 0.15987713634967804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4016, Loss: 0.2554658576846123, Final Batch Loss: 0.1479330062866211\n",
      "Epoch 4017, Loss: 0.22988516837358475, Final Batch Loss: 0.109443299472332\n",
      "Epoch 4018, Loss: 0.3787308782339096, Final Batch Loss: 0.1678387075662613\n",
      "Epoch 4019, Loss: 0.23780037462711334, Final Batch Loss: 0.13959556818008423\n",
      "Epoch 4020, Loss: 0.21764758974313736, Final Batch Loss: 0.10774360597133636\n",
      "Epoch 4021, Loss: 0.2592066675424576, Final Batch Loss: 0.11839352548122406\n",
      "Epoch 4022, Loss: 0.2582257390022278, Final Batch Loss: 0.11729511618614197\n",
      "Epoch 4023, Loss: 0.23849240690469742, Final Batch Loss: 0.1273168921470642\n",
      "Epoch 4024, Loss: 0.20707577466964722, Final Batch Loss: 0.11805856227874756\n",
      "Epoch 4025, Loss: 0.20453587174415588, Final Batch Loss: 0.08904306590557098\n",
      "Epoch 4026, Loss: 0.23024478554725647, Final Batch Loss: 0.11009274423122406\n",
      "Epoch 4027, Loss: 0.2102455496788025, Final Batch Loss: 0.12352882325649261\n",
      "Epoch 4028, Loss: 0.19148191809654236, Final Batch Loss: 0.10435488075017929\n",
      "Epoch 4029, Loss: 0.21580415219068527, Final Batch Loss: 0.08624308556318283\n",
      "Epoch 4030, Loss: 0.23405780643224716, Final Batch Loss: 0.14698122441768646\n",
      "Epoch 4031, Loss: 0.20660687237977982, Final Batch Loss: 0.09768133610486984\n",
      "Epoch 4032, Loss: 0.25277111679315567, Final Batch Loss: 0.15885725617408752\n",
      "Epoch 4033, Loss: 0.22869795560836792, Final Batch Loss: 0.1326093077659607\n",
      "Epoch 4034, Loss: 0.2714337855577469, Final Batch Loss: 0.13377130031585693\n",
      "Epoch 4035, Loss: 0.2554062530398369, Final Batch Loss: 0.14184144139289856\n",
      "Epoch 4036, Loss: 0.250211700797081, Final Batch Loss: 0.1486080437898636\n",
      "Epoch 4037, Loss: 0.2599648982286453, Final Batch Loss: 0.12295089662075043\n",
      "Epoch 4038, Loss: 0.2183595523238182, Final Batch Loss: 0.11684443801641464\n",
      "Epoch 4039, Loss: 0.28336410969495773, Final Batch Loss: 0.16762365400791168\n",
      "Epoch 4040, Loss: 0.20517228543758392, Final Batch Loss: 0.1279989629983902\n",
      "Epoch 4041, Loss: 0.2595341205596924, Final Batch Loss: 0.16427570581436157\n",
      "Epoch 4042, Loss: 0.2548333406448364, Final Batch Loss: 0.13317613303661346\n",
      "Epoch 4043, Loss: 0.27969275414943695, Final Batch Loss: 0.14330296218395233\n",
      "Epoch 4044, Loss: 0.2518487945199013, Final Batch Loss: 0.1440090388059616\n",
      "Epoch 4045, Loss: 0.22317475080490112, Final Batch Loss: 0.10141079127788544\n",
      "Epoch 4046, Loss: 0.23495308309793472, Final Batch Loss: 0.12985730171203613\n",
      "Epoch 4047, Loss: 0.24874203652143478, Final Batch Loss: 0.1257631927728653\n",
      "Epoch 4048, Loss: 0.22025129944086075, Final Batch Loss: 0.11916176229715347\n",
      "Epoch 4049, Loss: 0.24675662070512772, Final Batch Loss: 0.12907248735427856\n",
      "Epoch 4050, Loss: 0.2681156098842621, Final Batch Loss: 0.1552237719297409\n",
      "Epoch 4051, Loss: 0.20508284866809845, Final Batch Loss: 0.09472812712192535\n",
      "Epoch 4052, Loss: 0.24653200060129166, Final Batch Loss: 0.10565810650587082\n",
      "Epoch 4053, Loss: 0.2387790083885193, Final Batch Loss: 0.11597941070795059\n",
      "Epoch 4054, Loss: 0.2903228998184204, Final Batch Loss: 0.15188775956630707\n",
      "Epoch 4055, Loss: 0.24415535479784012, Final Batch Loss: 0.11743954569101334\n",
      "Epoch 4056, Loss: 0.22961334884166718, Final Batch Loss: 0.12787748873233795\n",
      "Epoch 4057, Loss: 0.22574052959680557, Final Batch Loss: 0.10902804136276245\n",
      "Epoch 4058, Loss: 0.2260749414563179, Final Batch Loss: 0.12586386501789093\n",
      "Epoch 4059, Loss: 0.29034510254859924, Final Batch Loss: 0.18215319514274597\n",
      "Epoch 4060, Loss: 0.21892331540584564, Final Batch Loss: 0.10093338042497635\n",
      "Epoch 4061, Loss: 0.25480763614177704, Final Batch Loss: 0.1467883437871933\n",
      "Epoch 4062, Loss: 0.2168480083346367, Final Batch Loss: 0.10192033648490906\n",
      "Epoch 4063, Loss: 0.25310656428337097, Final Batch Loss: 0.14114223420619965\n",
      "Epoch 4064, Loss: 0.23920266330242157, Final Batch Loss: 0.13839706778526306\n",
      "Epoch 4065, Loss: 0.20572587102651596, Final Batch Loss: 0.11046824604272842\n",
      "Epoch 4066, Loss: 0.2025068998336792, Final Batch Loss: 0.11131440848112106\n",
      "Epoch 4067, Loss: 0.19166100025177002, Final Batch Loss: 0.07044032216072083\n",
      "Epoch 4068, Loss: 0.20430445671081543, Final Batch Loss: 0.10330425947904587\n",
      "Epoch 4069, Loss: 0.23344731330871582, Final Batch Loss: 0.10684815049171448\n",
      "Epoch 4070, Loss: 0.24287867546081543, Final Batch Loss: 0.10723012685775757\n",
      "Epoch 4071, Loss: 0.2445065900683403, Final Batch Loss: 0.08927903324365616\n",
      "Epoch 4072, Loss: 0.16804610192775726, Final Batch Loss: 0.06877532601356506\n",
      "Epoch 4073, Loss: 0.25968679040670395, Final Batch Loss: 0.141994908452034\n",
      "Epoch 4074, Loss: 0.2591284438967705, Final Batch Loss: 0.12394902855157852\n",
      "Epoch 4075, Loss: 0.23053251951932907, Final Batch Loss: 0.10665208846330643\n",
      "Epoch 4076, Loss: 0.22041548043489456, Final Batch Loss: 0.11227349191904068\n",
      "Epoch 4077, Loss: 0.226558655500412, Final Batch Loss: 0.12389690428972244\n",
      "Epoch 4078, Loss: 0.2292959988117218, Final Batch Loss: 0.09725458920001984\n",
      "Epoch 4079, Loss: 0.19348668307065964, Final Batch Loss: 0.09108331054449081\n",
      "Epoch 4080, Loss: 0.2233319729566574, Final Batch Loss: 0.12453673779964447\n",
      "Epoch 4081, Loss: 0.2037724182009697, Final Batch Loss: 0.10672764480113983\n",
      "Epoch 4082, Loss: 0.2301013469696045, Final Batch Loss: 0.13000214099884033\n",
      "Epoch 4083, Loss: 0.22462285310029984, Final Batch Loss: 0.08086258918046951\n",
      "Epoch 4084, Loss: 0.24359311908483505, Final Batch Loss: 0.1301831156015396\n",
      "Epoch 4085, Loss: 0.22102925926446915, Final Batch Loss: 0.11314493417739868\n",
      "Epoch 4086, Loss: 0.20096756517887115, Final Batch Loss: 0.0947885811328888\n",
      "Epoch 4087, Loss: 0.23157650232315063, Final Batch Loss: 0.11013268679380417\n",
      "Epoch 4088, Loss: 0.19310709834098816, Final Batch Loss: 0.10689349472522736\n",
      "Epoch 4089, Loss: 0.26496654003858566, Final Batch Loss: 0.14458422362804413\n",
      "Epoch 4090, Loss: 0.2046549767255783, Final Batch Loss: 0.09146715700626373\n",
      "Epoch 4091, Loss: 0.18827857077121735, Final Batch Loss: 0.08752834051847458\n",
      "Epoch 4092, Loss: 0.2581420987844467, Final Batch Loss: 0.1649717539548874\n",
      "Epoch 4093, Loss: 0.18847835808992386, Final Batch Loss: 0.08522779494524002\n",
      "Epoch 4094, Loss: 0.19250746071338654, Final Batch Loss: 0.07468441873788834\n",
      "Epoch 4095, Loss: 0.19443907588720322, Final Batch Loss: 0.09335263818502426\n",
      "Epoch 4096, Loss: 0.26456014066934586, Final Batch Loss: 0.11392737179994583\n",
      "Epoch 4097, Loss: 0.2318762093782425, Final Batch Loss: 0.15479587018489838\n",
      "Epoch 4098, Loss: 0.1898207888007164, Final Batch Loss: 0.11780474334955215\n",
      "Epoch 4099, Loss: 0.19821222126483917, Final Batch Loss: 0.09683173149824142\n",
      "Epoch 4100, Loss: 0.21859055012464523, Final Batch Loss: 0.0865696594119072\n",
      "Epoch 4101, Loss: 0.21885692328214645, Final Batch Loss: 0.1018109917640686\n",
      "Epoch 4102, Loss: 0.25883905589580536, Final Batch Loss: 0.13626143336296082\n",
      "Epoch 4103, Loss: 0.203032948076725, Final Batch Loss: 0.10945936292409897\n",
      "Epoch 4104, Loss: 0.2114855870604515, Final Batch Loss: 0.12046019732952118\n",
      "Epoch 4105, Loss: 0.2550497129559517, Final Batch Loss: 0.156248539686203\n",
      "Epoch 4106, Loss: 0.28554894030094147, Final Batch Loss: 0.1444205492734909\n",
      "Epoch 4107, Loss: 0.20498599857091904, Final Batch Loss: 0.10107457637786865\n",
      "Epoch 4108, Loss: 0.2578597664833069, Final Batch Loss: 0.13973122835159302\n",
      "Epoch 4109, Loss: 0.22703733295202255, Final Batch Loss: 0.12235261499881744\n",
      "Epoch 4110, Loss: 0.190215066075325, Final Batch Loss: 0.07525027543306351\n",
      "Epoch 4111, Loss: 0.23126907646656036, Final Batch Loss: 0.09683305025100708\n",
      "Epoch 4112, Loss: 0.18931828439235687, Final Batch Loss: 0.09510958939790726\n",
      "Epoch 4113, Loss: 0.24857764691114426, Final Batch Loss: 0.08844087272882462\n",
      "Epoch 4114, Loss: 0.2540042847394943, Final Batch Loss: 0.12664322555065155\n",
      "Epoch 4115, Loss: 0.23284528404474258, Final Batch Loss: 0.12908750772476196\n",
      "Epoch 4116, Loss: 0.24687712639570236, Final Batch Loss: 0.15671485662460327\n",
      "Epoch 4117, Loss: 0.24980022758245468, Final Batch Loss: 0.10186903923749924\n",
      "Epoch 4118, Loss: 0.24067283421754837, Final Batch Loss: 0.09416713565587997\n",
      "Epoch 4119, Loss: 0.23244260996580124, Final Batch Loss: 0.1128959134221077\n",
      "Epoch 4120, Loss: 0.2273365706205368, Final Batch Loss: 0.10830987989902496\n",
      "Epoch 4121, Loss: 0.24612301588058472, Final Batch Loss: 0.13591472804546356\n",
      "Epoch 4122, Loss: 0.18726902455091476, Final Batch Loss: 0.09492705017328262\n",
      "Epoch 4123, Loss: 0.275336854159832, Final Batch Loss: 0.16406051814556122\n",
      "Epoch 4124, Loss: 0.2533929571509361, Final Batch Loss: 0.09682662039995193\n",
      "Epoch 4125, Loss: 0.24032414704561234, Final Batch Loss: 0.08131248503923416\n",
      "Epoch 4126, Loss: 0.2152034118771553, Final Batch Loss: 0.10567811131477356\n",
      "Epoch 4127, Loss: 0.2489708662033081, Final Batch Loss: 0.13253067433834076\n",
      "Epoch 4128, Loss: 0.19878704845905304, Final Batch Loss: 0.11712733656167984\n",
      "Epoch 4129, Loss: 0.24250876158475876, Final Batch Loss: 0.14793270826339722\n",
      "Epoch 4130, Loss: 0.2163688987493515, Final Batch Loss: 0.0814296156167984\n",
      "Epoch 4131, Loss: 0.2234022542834282, Final Batch Loss: 0.14296692609786987\n",
      "Epoch 4132, Loss: 0.1833256408572197, Final Batch Loss: 0.0936620831489563\n",
      "Epoch 4133, Loss: 0.22241556644439697, Final Batch Loss: 0.10160411894321442\n",
      "Epoch 4134, Loss: 0.21520427614450455, Final Batch Loss: 0.12286297976970673\n",
      "Epoch 4135, Loss: 0.19228755682706833, Final Batch Loss: 0.09874451905488968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4136, Loss: 0.27771085500717163, Final Batch Loss: 0.15760906040668488\n",
      "Epoch 4137, Loss: 0.23122037947177887, Final Batch Loss: 0.12475929409265518\n",
      "Epoch 4138, Loss: 0.21326765418052673, Final Batch Loss: 0.0767008364200592\n",
      "Epoch 4139, Loss: 0.20707960426807404, Final Batch Loss: 0.11383291333913803\n",
      "Epoch 4140, Loss: 0.229328915476799, Final Batch Loss: 0.10363352298736572\n",
      "Epoch 4141, Loss: 0.22924859076738358, Final Batch Loss: 0.10918936133384705\n",
      "Epoch 4142, Loss: 0.27010297775268555, Final Batch Loss: 0.12895579636096954\n",
      "Epoch 4143, Loss: 0.2191348895430565, Final Batch Loss: 0.1173199862241745\n",
      "Epoch 4144, Loss: 0.20937743037939072, Final Batch Loss: 0.08769778162240982\n",
      "Epoch 4145, Loss: 0.2151966318488121, Final Batch Loss: 0.1228402853012085\n",
      "Epoch 4146, Loss: 0.2648109048604965, Final Batch Loss: 0.1664266735315323\n",
      "Epoch 4147, Loss: 0.1923736035823822, Final Batch Loss: 0.08958563208580017\n",
      "Epoch 4148, Loss: 0.2517125532031059, Final Batch Loss: 0.15054531395435333\n",
      "Epoch 4149, Loss: 0.20557604730129242, Final Batch Loss: 0.11124175041913986\n",
      "Epoch 4150, Loss: 0.2582021653652191, Final Batch Loss: 0.17581859230995178\n",
      "Epoch 4151, Loss: 0.25995101779699326, Final Batch Loss: 0.117821104824543\n",
      "Epoch 4152, Loss: 0.22227822244167328, Final Batch Loss: 0.0876578837633133\n",
      "Epoch 4153, Loss: 0.2333482876420021, Final Batch Loss: 0.12313546985387802\n",
      "Epoch 4154, Loss: 0.2824948728084564, Final Batch Loss: 0.15590360760688782\n",
      "Epoch 4155, Loss: 0.20788702368736267, Final Batch Loss: 0.057912081480026245\n",
      "Epoch 4156, Loss: 0.18856661021709442, Final Batch Loss: 0.09515492618083954\n",
      "Epoch 4157, Loss: 0.21900532394647598, Final Batch Loss: 0.05976652354001999\n",
      "Epoch 4158, Loss: 0.18862375617027283, Final Batch Loss: 0.06286843121051788\n",
      "Epoch 4159, Loss: 0.2175556793808937, Final Batch Loss: 0.11462874710559845\n",
      "Epoch 4160, Loss: 0.2149452120065689, Final Batch Loss: 0.11396973580121994\n",
      "Epoch 4161, Loss: 0.21615952253341675, Final Batch Loss: 0.09981044381856918\n",
      "Epoch 4162, Loss: 0.18140118569135666, Final Batch Loss: 0.0748995989561081\n",
      "Epoch 4163, Loss: 0.19450753927230835, Final Batch Loss: 0.1197904720902443\n",
      "Epoch 4164, Loss: 0.25378911942243576, Final Batch Loss: 0.1529439091682434\n",
      "Epoch 4165, Loss: 0.25538553297519684, Final Batch Loss: 0.09941615164279938\n",
      "Epoch 4166, Loss: 0.23409376293420792, Final Batch Loss: 0.08510347455739975\n",
      "Epoch 4167, Loss: 0.27675048261880875, Final Batch Loss: 0.1577962040901184\n",
      "Epoch 4168, Loss: 0.1837281435728073, Final Batch Loss: 0.0812121331691742\n",
      "Epoch 4169, Loss: 0.2665092647075653, Final Batch Loss: 0.09865249693393707\n",
      "Epoch 4170, Loss: 0.21703144907951355, Final Batch Loss: 0.11012393981218338\n",
      "Epoch 4171, Loss: 0.2643546685576439, Final Batch Loss: 0.14767523109912872\n",
      "Epoch 4172, Loss: 0.21224399656057358, Final Batch Loss: 0.10163845866918564\n",
      "Epoch 4173, Loss: 0.21495410054922104, Final Batch Loss: 0.08167672902345657\n",
      "Epoch 4174, Loss: 0.19999244809150696, Final Batch Loss: 0.08888576179742813\n",
      "Epoch 4175, Loss: 0.18772904574871063, Final Batch Loss: 0.10751178860664368\n",
      "Epoch 4176, Loss: 0.22593704611063004, Final Batch Loss: 0.13717274367809296\n",
      "Epoch 4177, Loss: 0.19087381660938263, Final Batch Loss: 0.08985711634159088\n",
      "Epoch 4178, Loss: 0.24397823214530945, Final Batch Loss: 0.10504394769668579\n",
      "Epoch 4179, Loss: 0.19319270551204681, Final Batch Loss: 0.09995657205581665\n",
      "Epoch 4180, Loss: 0.2677704766392708, Final Batch Loss: 0.15988096594810486\n",
      "Epoch 4181, Loss: 0.29236697405576706, Final Batch Loss: 0.18416379392147064\n",
      "Epoch 4182, Loss: 0.20173639059066772, Final Batch Loss: 0.10024303942918777\n",
      "Epoch 4183, Loss: 0.23919300734996796, Final Batch Loss: 0.13737338781356812\n",
      "Epoch 4184, Loss: 0.3171096444129944, Final Batch Loss: 0.1571841835975647\n",
      "Epoch 4185, Loss: 0.16341687738895416, Final Batch Loss: 0.09069880098104477\n",
      "Epoch 4186, Loss: 0.24267705529928207, Final Batch Loss: 0.11490253359079361\n",
      "Epoch 4187, Loss: 0.2270425632596016, Final Batch Loss: 0.10610932856798172\n",
      "Epoch 4188, Loss: 0.25554390251636505, Final Batch Loss: 0.11483091115951538\n",
      "Epoch 4189, Loss: 0.18593230843544006, Final Batch Loss: 0.07153024524450302\n",
      "Epoch 4190, Loss: 0.29390136897563934, Final Batch Loss: 0.20474939048290253\n",
      "Epoch 4191, Loss: 0.2790377140045166, Final Batch Loss: 0.1581057608127594\n",
      "Epoch 4192, Loss: 0.26457133889198303, Final Batch Loss: 0.11794479191303253\n",
      "Epoch 4193, Loss: 0.22808970510959625, Final Batch Loss: 0.15452037751674652\n",
      "Epoch 4194, Loss: 0.21939148753881454, Final Batch Loss: 0.09465085715055466\n",
      "Epoch 4195, Loss: 0.19100868701934814, Final Batch Loss: 0.08970957249403\n",
      "Epoch 4196, Loss: 0.20970048010349274, Final Batch Loss: 0.07126329839229584\n",
      "Epoch 4197, Loss: 0.18030519038438797, Final Batch Loss: 0.08253496885299683\n",
      "Epoch 4198, Loss: 0.215710811316967, Final Batch Loss: 0.1119169145822525\n",
      "Epoch 4199, Loss: 0.19480544328689575, Final Batch Loss: 0.0918278694152832\n",
      "Epoch 4200, Loss: 0.16419582813978195, Final Batch Loss: 0.08008591085672379\n",
      "Epoch 4201, Loss: 0.20627224445343018, Final Batch Loss: 0.09733247011899948\n",
      "Epoch 4202, Loss: 0.18908781558275223, Final Batch Loss: 0.088707335293293\n",
      "Epoch 4203, Loss: 0.23806516081094742, Final Batch Loss: 0.1363610476255417\n",
      "Epoch 4204, Loss: 0.2407945990562439, Final Batch Loss: 0.10474689304828644\n",
      "Epoch 4205, Loss: 0.25644711405038834, Final Batch Loss: 0.0937436893582344\n",
      "Epoch 4206, Loss: 0.21398012340068817, Final Batch Loss: 0.1096566915512085\n",
      "Epoch 4207, Loss: 0.2642398700118065, Final Batch Loss: 0.14724746346473694\n",
      "Epoch 4208, Loss: 0.22274406254291534, Final Batch Loss: 0.0855020135641098\n",
      "Epoch 4209, Loss: 0.2169513925909996, Final Batch Loss: 0.11562345921993256\n",
      "Epoch 4210, Loss: 0.22081860154867172, Final Batch Loss: 0.09280969947576523\n",
      "Epoch 4211, Loss: 0.19744233787059784, Final Batch Loss: 0.07068444788455963\n",
      "Epoch 4212, Loss: 0.21864137053489685, Final Batch Loss: 0.1086307168006897\n",
      "Epoch 4213, Loss: 0.2236846759915352, Final Batch Loss: 0.11371780931949615\n",
      "Epoch 4214, Loss: 0.23447386920452118, Final Batch Loss: 0.14036130905151367\n",
      "Epoch 4215, Loss: 0.256027951836586, Final Batch Loss: 0.1290527582168579\n",
      "Epoch 4216, Loss: 0.1750044822692871, Final Batch Loss: 0.09002683311700821\n",
      "Epoch 4217, Loss: 0.17585521191358566, Final Batch Loss: 0.06628745794296265\n",
      "Epoch 4218, Loss: 0.19790606200695038, Final Batch Loss: 0.08353649079799652\n",
      "Epoch 4219, Loss: 0.17610042542219162, Final Batch Loss: 0.09372624009847641\n",
      "Epoch 4220, Loss: 0.19785989820957184, Final Batch Loss: 0.10598884522914886\n",
      "Epoch 4221, Loss: 0.2156415954232216, Final Batch Loss: 0.11546096950769424\n",
      "Epoch 4222, Loss: 0.16773685812950134, Final Batch Loss: 0.0843886062502861\n",
      "Epoch 4223, Loss: 0.2078244537115097, Final Batch Loss: 0.10323001444339752\n",
      "Epoch 4224, Loss: 0.2357703074812889, Final Batch Loss: 0.1514766663312912\n",
      "Epoch 4225, Loss: 0.22846835851669312, Final Batch Loss: 0.12707115709781647\n",
      "Epoch 4226, Loss: 0.222507543861866, Final Batch Loss: 0.14516006410121918\n",
      "Epoch 4227, Loss: 0.2351728156208992, Final Batch Loss: 0.08929244428873062\n",
      "Epoch 4228, Loss: 0.16735310852527618, Final Batch Loss: 0.06865840405225754\n",
      "Epoch 4229, Loss: 0.17031855136156082, Final Batch Loss: 0.06590192019939423\n",
      "Epoch 4230, Loss: 0.29417888820171356, Final Batch Loss: 0.1897767335176468\n",
      "Epoch 4231, Loss: 0.22582396864891052, Final Batch Loss: 0.14449293911457062\n",
      "Epoch 4232, Loss: 0.24296975880861282, Final Batch Loss: 0.1423409879207611\n",
      "Epoch 4233, Loss: 0.18249701708555222, Final Batch Loss: 0.08250650763511658\n",
      "Epoch 4234, Loss: 0.2295394390821457, Final Batch Loss: 0.12004909664392471\n",
      "Epoch 4235, Loss: 0.20756152272224426, Final Batch Loss: 0.09615246206521988\n",
      "Epoch 4236, Loss: 0.18727177381515503, Final Batch Loss: 0.09029481559991837\n",
      "Epoch 4237, Loss: 0.16421549022197723, Final Batch Loss: 0.07484292984008789\n",
      "Epoch 4238, Loss: 0.23131097853183746, Final Batch Loss: 0.14200420677661896\n",
      "Epoch 4239, Loss: 0.2503207251429558, Final Batch Loss: 0.14458070695400238\n",
      "Epoch 4240, Loss: 0.19144999980926514, Final Batch Loss: 0.09685182571411133\n",
      "Epoch 4241, Loss: 0.23330778628587723, Final Batch Loss: 0.11249176412820816\n",
      "Epoch 4242, Loss: 0.21579161286354065, Final Batch Loss: 0.09562177211046219\n",
      "Epoch 4243, Loss: 0.2600075751543045, Final Batch Loss: 0.16890989243984222\n",
      "Epoch 4244, Loss: 0.2001250982284546, Final Batch Loss: 0.08512286841869354\n",
      "Epoch 4245, Loss: 0.30914048850536346, Final Batch Loss: 0.19359980523586273\n",
      "Epoch 4246, Loss: 0.25250354409217834, Final Batch Loss: 0.08710618317127228\n",
      "Epoch 4247, Loss: 0.26647525280714035, Final Batch Loss: 0.15868015587329865\n",
      "Epoch 4248, Loss: 0.1753315106034279, Final Batch Loss: 0.07731109112501144\n",
      "Epoch 4249, Loss: 0.20633581280708313, Final Batch Loss: 0.08865749835968018\n",
      "Epoch 4250, Loss: 0.19445934891700745, Final Batch Loss: 0.08176538348197937\n",
      "Epoch 4251, Loss: 0.1998549848794937, Final Batch Loss: 0.09882581979036331\n",
      "Epoch 4252, Loss: 0.15600348263978958, Final Batch Loss: 0.06059730798006058\n",
      "Epoch 4253, Loss: 0.23347333073616028, Final Batch Loss: 0.1033807098865509\n",
      "Epoch 4254, Loss: 0.2738814055919647, Final Batch Loss: 0.13196919858455658\n",
      "Epoch 4255, Loss: 0.24692415446043015, Final Batch Loss: 0.10378814488649368\n",
      "Epoch 4256, Loss: 0.18261658400297165, Final Batch Loss: 0.0703745111823082\n",
      "Epoch 4257, Loss: 0.2125290408730507, Final Batch Loss: 0.08273086696863174\n",
      "Epoch 4258, Loss: 0.20182830840349197, Final Batch Loss: 0.06356584280729294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4259, Loss: 0.20244323462247849, Final Batch Loss: 0.11580631881952286\n",
      "Epoch 4260, Loss: 0.2463889718055725, Final Batch Loss: 0.1534697413444519\n",
      "Epoch 4261, Loss: 0.21371440589427948, Final Batch Loss: 0.1155875101685524\n",
      "Epoch 4262, Loss: 0.24213211238384247, Final Batch Loss: 0.11502929031848907\n",
      "Epoch 4263, Loss: 0.28274252265691757, Final Batch Loss: 0.11009062081575394\n",
      "Epoch 4264, Loss: 0.21327237784862518, Final Batch Loss: 0.13828547298908234\n",
      "Epoch 4265, Loss: 0.22704342752695084, Final Batch Loss: 0.12396133691072464\n",
      "Epoch 4266, Loss: 0.2751987427473068, Final Batch Loss: 0.1706237941980362\n",
      "Epoch 4267, Loss: 0.19794060289859772, Final Batch Loss: 0.08037125319242477\n",
      "Epoch 4268, Loss: 0.1914989873766899, Final Batch Loss: 0.10772605985403061\n",
      "Epoch 4269, Loss: 0.21743357926607132, Final Batch Loss: 0.08209944516420364\n",
      "Epoch 4270, Loss: 0.2046995609998703, Final Batch Loss: 0.11138629168272018\n",
      "Epoch 4271, Loss: 0.2168354168534279, Final Batch Loss: 0.109248086810112\n",
      "Epoch 4272, Loss: 0.19668348878622055, Final Batch Loss: 0.06861741095781326\n",
      "Epoch 4273, Loss: 0.21322974562644958, Final Batch Loss: 0.08268463611602783\n",
      "Epoch 4274, Loss: 0.24494031071662903, Final Batch Loss: 0.12670868635177612\n",
      "Epoch 4275, Loss: 0.17213762551546097, Final Batch Loss: 0.08210191130638123\n",
      "Epoch 4276, Loss: 0.2545729726552963, Final Batch Loss: 0.15983477234840393\n",
      "Epoch 4277, Loss: 0.2010502889752388, Final Batch Loss: 0.07917951792478561\n",
      "Epoch 4278, Loss: 0.313203826546669, Final Batch Loss: 0.1388741135597229\n",
      "Epoch 4279, Loss: 0.20027440041303635, Final Batch Loss: 0.08635333925485611\n",
      "Epoch 4280, Loss: 0.23170194774866104, Final Batch Loss: 0.10538535565137863\n",
      "Epoch 4281, Loss: 0.21386076509952545, Final Batch Loss: 0.12050018459558487\n",
      "Epoch 4282, Loss: 0.21299511194229126, Final Batch Loss: 0.10975806415081024\n",
      "Epoch 4283, Loss: 0.18887083232402802, Final Batch Loss: 0.08031104505062103\n",
      "Epoch 4284, Loss: 0.22948410362005234, Final Batch Loss: 0.12687915563583374\n",
      "Epoch 4285, Loss: 0.20255649089813232, Final Batch Loss: 0.11017194390296936\n",
      "Epoch 4286, Loss: 0.18454042077064514, Final Batch Loss: 0.08329819142818451\n",
      "Epoch 4287, Loss: 0.2904063016176224, Final Batch Loss: 0.15514995157718658\n",
      "Epoch 4288, Loss: 0.2803996503353119, Final Batch Loss: 0.1373748779296875\n",
      "Epoch 4289, Loss: 0.21458759903907776, Final Batch Loss: 0.1429450362920761\n",
      "Epoch 4290, Loss: 0.28973668813705444, Final Batch Loss: 0.12813974916934967\n",
      "Epoch 4291, Loss: 0.1758093759417534, Final Batch Loss: 0.0716262236237526\n",
      "Epoch 4292, Loss: 0.2081517055630684, Final Batch Loss: 0.11331015825271606\n",
      "Epoch 4293, Loss: 0.1899479776620865, Final Batch Loss: 0.07949666678905487\n",
      "Epoch 4294, Loss: 0.23078911751508713, Final Batch Loss: 0.11270319670438766\n",
      "Epoch 4295, Loss: 0.18933311849832535, Final Batch Loss: 0.10188638418912888\n",
      "Epoch 4296, Loss: 0.21788693964481354, Final Batch Loss: 0.11263123154640198\n",
      "Epoch 4297, Loss: 0.2737986519932747, Final Batch Loss: 0.15903639793395996\n",
      "Epoch 4298, Loss: 0.2592470943927765, Final Batch Loss: 0.17687492072582245\n",
      "Epoch 4299, Loss: 0.20851699262857437, Final Batch Loss: 0.10772024840116501\n",
      "Epoch 4300, Loss: 0.24256819486618042, Final Batch Loss: 0.12358357757329941\n",
      "Epoch 4301, Loss: 0.255842961370945, Final Batch Loss: 0.1618300974369049\n",
      "Epoch 4302, Loss: 0.18681684881448746, Final Batch Loss: 0.10189992189407349\n",
      "Epoch 4303, Loss: 0.23191165924072266, Final Batch Loss: 0.12293247133493423\n",
      "Epoch 4304, Loss: 0.32161232829093933, Final Batch Loss: 0.15726453065872192\n",
      "Epoch 4305, Loss: 0.25269506126642227, Final Batch Loss: 0.1230003610253334\n",
      "Epoch 4306, Loss: 0.176764115691185, Final Batch Loss: 0.09011394530534744\n",
      "Epoch 4307, Loss: 0.36715365946292877, Final Batch Loss: 0.21661239862442017\n",
      "Epoch 4308, Loss: 0.2787284031510353, Final Batch Loss: 0.17711594700813293\n",
      "Epoch 4309, Loss: 0.19842816144227982, Final Batch Loss: 0.09975537657737732\n",
      "Epoch 4310, Loss: 0.21549636870622635, Final Batch Loss: 0.07972235232591629\n",
      "Epoch 4311, Loss: 0.2940244972705841, Final Batch Loss: 0.16289661824703217\n",
      "Epoch 4312, Loss: 0.23476487398147583, Final Batch Loss: 0.13007430732250214\n",
      "Epoch 4313, Loss: 0.24889341741800308, Final Batch Loss: 0.09837641566991806\n",
      "Epoch 4314, Loss: 0.20229054987430573, Final Batch Loss: 0.0888814702630043\n",
      "Epoch 4315, Loss: 0.19958803802728653, Final Batch Loss: 0.07010466605424881\n",
      "Epoch 4316, Loss: 0.19554227590560913, Final Batch Loss: 0.09004926681518555\n",
      "Epoch 4317, Loss: 0.1830061450600624, Final Batch Loss: 0.09070219099521637\n",
      "Epoch 4318, Loss: 0.26658135652542114, Final Batch Loss: 0.1476716846227646\n",
      "Epoch 4319, Loss: 0.20225007832050323, Final Batch Loss: 0.09145475178956985\n",
      "Epoch 4320, Loss: 0.2090475857257843, Final Batch Loss: 0.10198322683572769\n",
      "Epoch 4321, Loss: 0.18314968049526215, Final Batch Loss: 0.061492063105106354\n",
      "Epoch 4322, Loss: 0.23065917193889618, Final Batch Loss: 0.08323194086551666\n",
      "Epoch 4323, Loss: 0.26573240756988525, Final Batch Loss: 0.13409428298473358\n",
      "Epoch 4324, Loss: 0.20890333503484726, Final Batch Loss: 0.08891427516937256\n",
      "Epoch 4325, Loss: 0.187469981610775, Final Batch Loss: 0.10143821686506271\n",
      "Epoch 4326, Loss: 0.26110097020864487, Final Batch Loss: 0.11887424439191818\n",
      "Epoch 4327, Loss: 0.21255499124526978, Final Batch Loss: 0.12174166738986969\n",
      "Epoch 4328, Loss: 0.334573395550251, Final Batch Loss: 0.23651885986328125\n",
      "Epoch 4329, Loss: 0.2596155107021332, Final Batch Loss: 0.12541675567626953\n",
      "Epoch 4330, Loss: 0.2066134661436081, Final Batch Loss: 0.09412286430597305\n",
      "Epoch 4331, Loss: 0.23293058574199677, Final Batch Loss: 0.1154390424489975\n",
      "Epoch 4332, Loss: 0.201516255736351, Final Batch Loss: 0.08869705349206924\n",
      "Epoch 4333, Loss: 0.21156153082847595, Final Batch Loss: 0.1008325070142746\n",
      "Epoch 4334, Loss: 0.21779505163431168, Final Batch Loss: 0.1117534264922142\n",
      "Epoch 4335, Loss: 0.2452774941921234, Final Batch Loss: 0.14203853905200958\n",
      "Epoch 4336, Loss: 0.2045365646481514, Final Batch Loss: 0.09071649610996246\n",
      "Epoch 4337, Loss: 0.1817420944571495, Final Batch Loss: 0.07676352560520172\n",
      "Epoch 4338, Loss: 0.2894740477204323, Final Batch Loss: 0.18407802283763885\n",
      "Epoch 4339, Loss: 0.29222381114959717, Final Batch Loss: 0.1710195243358612\n",
      "Epoch 4340, Loss: 0.2691212370991707, Final Batch Loss: 0.1677376627922058\n",
      "Epoch 4341, Loss: 0.21146252006292343, Final Batch Loss: 0.09365874528884888\n",
      "Epoch 4342, Loss: 0.1543455719947815, Final Batch Loss: 0.06024075299501419\n",
      "Epoch 4343, Loss: 0.20603549480438232, Final Batch Loss: 0.09583275020122528\n",
      "Epoch 4344, Loss: 0.19009990245103836, Final Batch Loss: 0.0884413868188858\n",
      "Epoch 4345, Loss: 0.1904454231262207, Final Batch Loss: 0.08742426335811615\n",
      "Epoch 4346, Loss: 0.17822059243917465, Final Batch Loss: 0.09462117403745651\n",
      "Epoch 4347, Loss: 0.2064048796892166, Final Batch Loss: 0.11562991142272949\n",
      "Epoch 4348, Loss: 0.2057654932141304, Final Batch Loss: 0.0923980176448822\n",
      "Epoch 4349, Loss: 0.15598133951425552, Final Batch Loss: 0.050753481686115265\n",
      "Epoch 4350, Loss: 0.183187335729599, Final Batch Loss: 0.08608079701662064\n",
      "Epoch 4351, Loss: 0.30354834347963333, Final Batch Loss: 0.22050480544567108\n",
      "Epoch 4352, Loss: 0.24285289645195007, Final Batch Loss: 0.10430917143821716\n",
      "Epoch 4353, Loss: 0.20890025794506073, Final Batch Loss: 0.09818129986524582\n",
      "Epoch 4354, Loss: 0.17974471300840378, Final Batch Loss: 0.1014120876789093\n",
      "Epoch 4355, Loss: 0.1877949833869934, Final Batch Loss: 0.09216130524873734\n",
      "Epoch 4356, Loss: 0.22365929186344147, Final Batch Loss: 0.12029724568128586\n",
      "Epoch 4357, Loss: 0.2012588456273079, Final Batch Loss: 0.10814687609672546\n",
      "Epoch 4358, Loss: 0.2471505030989647, Final Batch Loss: 0.13349232077598572\n",
      "Epoch 4359, Loss: 0.210041344165802, Final Batch Loss: 0.09616676717996597\n",
      "Epoch 4360, Loss: 0.18871374428272247, Final Batch Loss: 0.09050101041793823\n",
      "Epoch 4361, Loss: 0.20910514891147614, Final Batch Loss: 0.12443752586841583\n",
      "Epoch 4362, Loss: 0.18565060198307037, Final Batch Loss: 0.08925224840641022\n",
      "Epoch 4363, Loss: 0.17837855219841003, Final Batch Loss: 0.08318833261728287\n",
      "Epoch 4364, Loss: 0.19776686280965805, Final Batch Loss: 0.09526628255844116\n",
      "Epoch 4365, Loss: 0.1983834207057953, Final Batch Loss: 0.1071268767118454\n",
      "Epoch 4366, Loss: 0.2106466442346573, Final Batch Loss: 0.08138181269168854\n",
      "Epoch 4367, Loss: 0.20074524730443954, Final Batch Loss: 0.09594686329364777\n",
      "Epoch 4368, Loss: 0.19303760677576065, Final Batch Loss: 0.11242745816707611\n",
      "Epoch 4369, Loss: 0.1800437569618225, Final Batch Loss: 0.09713137149810791\n",
      "Epoch 4370, Loss: 0.16578510403633118, Final Batch Loss: 0.08809221535921097\n",
      "Epoch 4371, Loss: 0.20766515284776688, Final Batch Loss: 0.09977946430444717\n",
      "Epoch 4372, Loss: 0.24282126128673553, Final Batch Loss: 0.130557581782341\n",
      "Epoch 4373, Loss: 0.13943931087851524, Final Batch Loss: 0.059568483382463455\n",
      "Epoch 4374, Loss: 0.20846664905548096, Final Batch Loss: 0.10538725554943085\n",
      "Epoch 4375, Loss: 0.21058490872383118, Final Batch Loss: 0.08512264490127563\n",
      "Epoch 4376, Loss: 0.1833735629916191, Final Batch Loss: 0.07959959656000137\n",
      "Epoch 4377, Loss: 0.17739490419626236, Final Batch Loss: 0.06812064349651337\n",
      "Epoch 4378, Loss: 0.15201642736792564, Final Batch Loss: 0.042848166078329086\n",
      "Epoch 4379, Loss: 0.1920396164059639, Final Batch Loss: 0.07782687246799469\n",
      "Epoch 4380, Loss: 0.20617395639419556, Final Batch Loss: 0.12615104019641876\n",
      "Epoch 4381, Loss: 0.22348426282405853, Final Batch Loss: 0.12284891307353973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4382, Loss: 0.20281927287578583, Final Batch Loss: 0.10252818465232849\n",
      "Epoch 4383, Loss: 0.2247852310538292, Final Batch Loss: 0.0965595617890358\n",
      "Epoch 4384, Loss: 0.1793593168258667, Final Batch Loss: 0.10977298766374588\n",
      "Epoch 4385, Loss: 0.21605748683214188, Final Batch Loss: 0.11202138662338257\n",
      "Epoch 4386, Loss: 0.2971011772751808, Final Batch Loss: 0.1900947540998459\n",
      "Epoch 4387, Loss: 0.22120112925767899, Final Batch Loss: 0.10898343473672867\n",
      "Epoch 4388, Loss: 0.19494112581014633, Final Batch Loss: 0.07198646664619446\n",
      "Epoch 4389, Loss: 0.21031954884529114, Final Batch Loss: 0.11697831749916077\n",
      "Epoch 4390, Loss: 0.22477582842111588, Final Batch Loss: 0.10955431312322617\n",
      "Epoch 4391, Loss: 0.22171810269355774, Final Batch Loss: 0.10846693068742752\n",
      "Epoch 4392, Loss: 0.18042980879545212, Final Batch Loss: 0.10267377644777298\n",
      "Epoch 4393, Loss: 0.18799052387475967, Final Batch Loss: 0.08198399096727371\n",
      "Epoch 4394, Loss: 0.16985195875167847, Final Batch Loss: 0.06730523705482483\n",
      "Epoch 4395, Loss: 0.2306000366806984, Final Batch Loss: 0.12098599225282669\n",
      "Epoch 4396, Loss: 0.20258479565382004, Final Batch Loss: 0.10172726213932037\n",
      "Epoch 4397, Loss: 0.16164468228816986, Final Batch Loss: 0.08628609031438828\n",
      "Epoch 4398, Loss: 0.14850442111492157, Final Batch Loss: 0.07434773445129395\n",
      "Epoch 4399, Loss: 0.2199413999915123, Final Batch Loss: 0.12490006536245346\n",
      "Epoch 4400, Loss: 0.20944232493638992, Final Batch Loss: 0.12021267414093018\n",
      "Epoch 4401, Loss: 0.23781992495059967, Final Batch Loss: 0.1155063733458519\n",
      "Epoch 4402, Loss: 0.19863124191761017, Final Batch Loss: 0.07324808835983276\n",
      "Epoch 4403, Loss: 0.1911555528640747, Final Batch Loss: 0.1026933565735817\n",
      "Epoch 4404, Loss: 0.26230282336473465, Final Batch Loss: 0.1127643957734108\n",
      "Epoch 4405, Loss: 0.21199075132608414, Final Batch Loss: 0.10712689906358719\n",
      "Epoch 4406, Loss: 0.23995163291692734, Final Batch Loss: 0.13984285295009613\n",
      "Epoch 4407, Loss: 0.22372719645500183, Final Batch Loss: 0.10966221988201141\n",
      "Epoch 4408, Loss: 0.20920470356941223, Final Batch Loss: 0.09269046783447266\n",
      "Epoch 4409, Loss: 0.1989642158150673, Final Batch Loss: 0.11305227875709534\n",
      "Epoch 4410, Loss: 0.15430019795894623, Final Batch Loss: 0.08580758422613144\n",
      "Epoch 4411, Loss: 0.24658389389514923, Final Batch Loss: 0.11638690531253815\n",
      "Epoch 4412, Loss: 0.2546594589948654, Final Batch Loss: 0.13817454874515533\n",
      "Epoch 4413, Loss: 0.16180308908224106, Final Batch Loss: 0.04592539370059967\n",
      "Epoch 4414, Loss: 0.19004277139902115, Final Batch Loss: 0.08627606928348541\n",
      "Epoch 4415, Loss: 0.14914100244641304, Final Batch Loss: 0.05915800854563713\n",
      "Epoch 4416, Loss: 0.1932012289762497, Final Batch Loss: 0.08777154237031937\n",
      "Epoch 4417, Loss: 0.20640771090984344, Final Batch Loss: 0.11795040220022202\n",
      "Epoch 4418, Loss: 0.20032721757888794, Final Batch Loss: 0.08101039379835129\n",
      "Epoch 4419, Loss: 0.27469590306282043, Final Batch Loss: 0.16127656400203705\n",
      "Epoch 4420, Loss: 0.1582069918513298, Final Batch Loss: 0.06849326938390732\n",
      "Epoch 4421, Loss: 0.21125928312540054, Final Batch Loss: 0.0901021882891655\n",
      "Epoch 4422, Loss: 0.1882626786828041, Final Batch Loss: 0.08205098658800125\n",
      "Epoch 4423, Loss: 0.2056032046675682, Final Batch Loss: 0.09345455467700958\n",
      "Epoch 4424, Loss: 0.1612616628408432, Final Batch Loss: 0.06947918236255646\n",
      "Epoch 4425, Loss: 0.2117663472890854, Final Batch Loss: 0.09160381555557251\n",
      "Epoch 4426, Loss: 0.23619084060192108, Final Batch Loss: 0.15286357700824738\n",
      "Epoch 4427, Loss: 0.2236550897359848, Final Batch Loss: 0.13036763668060303\n",
      "Epoch 4428, Loss: 0.2097666785120964, Final Batch Loss: 0.10954120010137558\n",
      "Epoch 4429, Loss: 0.2356320172548294, Final Batch Loss: 0.09095333516597748\n",
      "Epoch 4430, Loss: 0.21125958114862442, Final Batch Loss: 0.10512891411781311\n",
      "Epoch 4431, Loss: 0.21954017877578735, Final Batch Loss: 0.11782528460025787\n",
      "Epoch 4432, Loss: 0.2265382707118988, Final Batch Loss: 0.13190479576587677\n",
      "Epoch 4433, Loss: 0.17038413137197495, Final Batch Loss: 0.08027441054582596\n",
      "Epoch 4434, Loss: 0.23255880177021027, Final Batch Loss: 0.12273149937391281\n",
      "Epoch 4435, Loss: 0.22408661246299744, Final Batch Loss: 0.10266485810279846\n",
      "Epoch 4436, Loss: 0.20992589741945267, Final Batch Loss: 0.09648226201534271\n",
      "Epoch 4437, Loss: 0.2005607858300209, Final Batch Loss: 0.11724818497896194\n",
      "Epoch 4438, Loss: 0.20336732268333435, Final Batch Loss: 0.1219692975282669\n",
      "Epoch 4439, Loss: 0.19256533682346344, Final Batch Loss: 0.111282579600811\n",
      "Epoch 4440, Loss: 0.21492443978786469, Final Batch Loss: 0.09688199311494827\n",
      "Epoch 4441, Loss: 0.23083791136741638, Final Batch Loss: 0.12194432318210602\n",
      "Epoch 4442, Loss: 0.19679927080869675, Final Batch Loss: 0.08047906309366226\n",
      "Epoch 4443, Loss: 0.25575950741767883, Final Batch Loss: 0.10070841014385223\n",
      "Epoch 4444, Loss: 0.2101808786392212, Final Batch Loss: 0.1042308434844017\n",
      "Epoch 4445, Loss: 0.28179530799388885, Final Batch Loss: 0.14182090759277344\n",
      "Epoch 4446, Loss: 0.1716698482632637, Final Batch Loss: 0.08874613791704178\n",
      "Epoch 4447, Loss: 0.22396759688854218, Final Batch Loss: 0.12591637670993805\n",
      "Epoch 4448, Loss: 0.24418984353542328, Final Batch Loss: 0.14852498471736908\n",
      "Epoch 4449, Loss: 0.1891891285777092, Final Batch Loss: 0.09079602360725403\n",
      "Epoch 4450, Loss: 0.17298199981451035, Final Batch Loss: 0.1033974215388298\n",
      "Epoch 4451, Loss: 0.16734693944454193, Final Batch Loss: 0.07574761658906937\n",
      "Epoch 4452, Loss: 0.2501619681715965, Final Batch Loss: 0.12312289327383041\n",
      "Epoch 4453, Loss: 0.21976228058338165, Final Batch Loss: 0.09693882614374161\n",
      "Epoch 4454, Loss: 0.24143758416175842, Final Batch Loss: 0.1412249058485031\n",
      "Epoch 4455, Loss: 0.2297314777970314, Final Batch Loss: 0.12335633486509323\n",
      "Epoch 4456, Loss: 0.21799997240304947, Final Batch Loss: 0.12524689733982086\n",
      "Epoch 4457, Loss: 0.21172966808080673, Final Batch Loss: 0.09333500266075134\n",
      "Epoch 4458, Loss: 0.16294071823358536, Final Batch Loss: 0.08032283931970596\n",
      "Epoch 4459, Loss: 0.1845265030860901, Final Batch Loss: 0.08243236690759659\n",
      "Epoch 4460, Loss: 0.1723141372203827, Final Batch Loss: 0.08731735497713089\n",
      "Epoch 4461, Loss: 0.15188148990273476, Final Batch Loss: 0.054386090487241745\n",
      "Epoch 4462, Loss: 0.25022142380476, Final Batch Loss: 0.15191510319709778\n",
      "Epoch 4463, Loss: 0.1912005916237831, Final Batch Loss: 0.08204178512096405\n",
      "Epoch 4464, Loss: 0.20348001271486282, Final Batch Loss: 0.10513599216938019\n",
      "Epoch 4465, Loss: 0.2104995995759964, Final Batch Loss: 0.09199845790863037\n",
      "Epoch 4466, Loss: 0.1929696723818779, Final Batch Loss: 0.09878161549568176\n",
      "Epoch 4467, Loss: 0.2043875902891159, Final Batch Loss: 0.10554272681474686\n",
      "Epoch 4468, Loss: 0.30061567574739456, Final Batch Loss: 0.20226426422595978\n",
      "Epoch 4469, Loss: 0.16419009864330292, Final Batch Loss: 0.06542275846004486\n",
      "Epoch 4470, Loss: 0.24356862157583237, Final Batch Loss: 0.1320735067129135\n",
      "Epoch 4471, Loss: 0.1721004769206047, Final Batch Loss: 0.0877833142876625\n",
      "Epoch 4472, Loss: 0.2726486623287201, Final Batch Loss: 0.13997085392475128\n",
      "Epoch 4473, Loss: 0.19368822872638702, Final Batch Loss: 0.07461284846067429\n",
      "Epoch 4474, Loss: 0.18298791348934174, Final Batch Loss: 0.07622861117124557\n",
      "Epoch 4475, Loss: 0.2108650878071785, Final Batch Loss: 0.0756031796336174\n",
      "Epoch 4476, Loss: 0.19311246275901794, Final Batch Loss: 0.06792593002319336\n",
      "Epoch 4477, Loss: 0.2814326137304306, Final Batch Loss: 0.1546774059534073\n",
      "Epoch 4478, Loss: 0.1457652896642685, Final Batch Loss: 0.0723920613527298\n",
      "Epoch 4479, Loss: 0.2242950052022934, Final Batch Loss: 0.13592343032360077\n",
      "Epoch 4480, Loss: 0.23808088898658752, Final Batch Loss: 0.14680403470993042\n",
      "Epoch 4481, Loss: 0.18648315966129303, Final Batch Loss: 0.1040058359503746\n",
      "Epoch 4482, Loss: 0.2590150982141495, Final Batch Loss: 0.12609469890594482\n",
      "Epoch 4483, Loss: 0.2410159483551979, Final Batch Loss: 0.1356017291545868\n",
      "Epoch 4484, Loss: 0.22605758905410767, Final Batch Loss: 0.1374356895685196\n",
      "Epoch 4485, Loss: 0.1810579150915146, Final Batch Loss: 0.07097964733839035\n",
      "Epoch 4486, Loss: 0.19226783514022827, Final Batch Loss: 0.12072290480136871\n",
      "Epoch 4487, Loss: 0.216329924762249, Final Batch Loss: 0.06567970663309097\n",
      "Epoch 4488, Loss: 0.222065731883049, Final Batch Loss: 0.09605810046195984\n",
      "Epoch 4489, Loss: 0.20474112778902054, Final Batch Loss: 0.08704081922769547\n",
      "Epoch 4490, Loss: 0.15969537943601608, Final Batch Loss: 0.08934535086154938\n",
      "Epoch 4491, Loss: 0.17689672857522964, Final Batch Loss: 0.10103248059749603\n",
      "Epoch 4492, Loss: 0.19967898726463318, Final Batch Loss: 0.07409921288490295\n",
      "Epoch 4493, Loss: 0.21723104268312454, Final Batch Loss: 0.12547020614147186\n",
      "Epoch 4494, Loss: 0.2053523138165474, Final Batch Loss: 0.10334961861371994\n",
      "Epoch 4495, Loss: 0.1954716518521309, Final Batch Loss: 0.1030958890914917\n",
      "Epoch 4496, Loss: 0.21185344457626343, Final Batch Loss: 0.12163460999727249\n",
      "Epoch 4497, Loss: 0.16772287338972092, Final Batch Loss: 0.08703825622797012\n",
      "Epoch 4498, Loss: 0.23454998433589935, Final Batch Loss: 0.10956820100545883\n",
      "Epoch 4499, Loss: 0.24892644584178925, Final Batch Loss: 0.15259210765361786\n",
      "Epoch 4500, Loss: 0.24211649596691132, Final Batch Loss: 0.1546555459499359\n",
      "Epoch 4501, Loss: 0.19552595913410187, Final Batch Loss: 0.09594103693962097\n",
      "Epoch 4502, Loss: 0.2902465835213661, Final Batch Loss: 0.17410139739513397\n",
      "Epoch 4503, Loss: 0.20949503034353256, Final Batch Loss: 0.10896092653274536\n",
      "Epoch 4504, Loss: 0.19575323909521103, Final Batch Loss: 0.08985082805156708\n",
      "Epoch 4505, Loss: 0.18875723332166672, Final Batch Loss: 0.10006272047758102\n",
      "Epoch 4506, Loss: 0.17858406901359558, Final Batch Loss: 0.10416585206985474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4507, Loss: 0.19016344100236893, Final Batch Loss: 0.07743705809116364\n",
      "Epoch 4508, Loss: 0.18810013681650162, Final Batch Loss: 0.10951566696166992\n",
      "Epoch 4509, Loss: 0.25478334724903107, Final Batch Loss: 0.14881540834903717\n",
      "Epoch 4510, Loss: 0.20884811133146286, Final Batch Loss: 0.06741594523191452\n",
      "Epoch 4511, Loss: 0.15730737149715424, Final Batch Loss: 0.06339757144451141\n",
      "Epoch 4512, Loss: 0.19327954947948456, Final Batch Loss: 0.09452477842569351\n",
      "Epoch 4513, Loss: 0.24932564795017242, Final Batch Loss: 0.12047328054904938\n",
      "Epoch 4514, Loss: 0.16863582283258438, Final Batch Loss: 0.08001817017793655\n",
      "Epoch 4515, Loss: 0.15995769202709198, Final Batch Loss: 0.06170268356800079\n",
      "Epoch 4516, Loss: 0.18986518681049347, Final Batch Loss: 0.08502933382987976\n",
      "Epoch 4517, Loss: 0.2024524286389351, Final Batch Loss: 0.11873185634613037\n",
      "Epoch 4518, Loss: 0.264144591987133, Final Batch Loss: 0.16214756667613983\n",
      "Epoch 4519, Loss: 0.22337186336517334, Final Batch Loss: 0.09528052806854248\n",
      "Epoch 4520, Loss: 0.17974425107240677, Final Batch Loss: 0.09248936176300049\n",
      "Epoch 4521, Loss: 0.3039223924279213, Final Batch Loss: 0.18185092508792877\n",
      "Epoch 4522, Loss: 0.18044311553239822, Final Batch Loss: 0.085853211581707\n",
      "Epoch 4523, Loss: 0.20922349393367767, Final Batch Loss: 0.1229657530784607\n",
      "Epoch 4524, Loss: 0.24044230580329895, Final Batch Loss: 0.15735098719596863\n",
      "Epoch 4525, Loss: 0.14413082227110863, Final Batch Loss: 0.05925177410244942\n",
      "Epoch 4526, Loss: 0.2138267606496811, Final Batch Loss: 0.11108843237161636\n",
      "Epoch 4527, Loss: 0.21091005206108093, Final Batch Loss: 0.08014188706874847\n",
      "Epoch 4528, Loss: 0.2041667476296425, Final Batch Loss: 0.09412985295057297\n",
      "Epoch 4529, Loss: 0.17539291828870773, Final Batch Loss: 0.07615901529788971\n",
      "Epoch 4530, Loss: 0.19855888187885284, Final Batch Loss: 0.10757552087306976\n",
      "Epoch 4531, Loss: 0.21468429267406464, Final Batch Loss: 0.12569111585617065\n",
      "Epoch 4532, Loss: 0.18485460430383682, Final Batch Loss: 0.09164045006036758\n",
      "Epoch 4533, Loss: 0.17184701561927795, Final Batch Loss: 0.06176145374774933\n",
      "Epoch 4534, Loss: 0.16529744863510132, Final Batch Loss: 0.07228998094797134\n",
      "Epoch 4535, Loss: 0.2630274072289467, Final Batch Loss: 0.1764499694108963\n",
      "Epoch 4536, Loss: 0.2021346539258957, Final Batch Loss: 0.10891037434339523\n",
      "Epoch 4537, Loss: 0.15526549518108368, Final Batch Loss: 0.087221659719944\n",
      "Epoch 4538, Loss: 0.18532191216945648, Final Batch Loss: 0.08977176994085312\n",
      "Epoch 4539, Loss: 0.15624989569187164, Final Batch Loss: 0.06764686107635498\n",
      "Epoch 4540, Loss: 0.23838451504707336, Final Batch Loss: 0.1257595717906952\n",
      "Epoch 4541, Loss: 0.19324863702058792, Final Batch Loss: 0.11921975761651993\n",
      "Epoch 4542, Loss: 0.16239828243851662, Final Batch Loss: 0.035837847739458084\n",
      "Epoch 4543, Loss: 0.16527051478624344, Final Batch Loss: 0.07114910334348679\n",
      "Epoch 4544, Loss: 0.179587259888649, Final Batch Loss: 0.10353271663188934\n",
      "Epoch 4545, Loss: 0.1886005997657776, Final Batch Loss: 0.10338465869426727\n",
      "Epoch 4546, Loss: 0.23930901288986206, Final Batch Loss: 0.12395064532756805\n",
      "Epoch 4547, Loss: 0.16181697323918343, Final Batch Loss: 0.059187669306993484\n",
      "Epoch 4548, Loss: 0.17416489124298096, Final Batch Loss: 0.09605682641267776\n",
      "Epoch 4549, Loss: 0.15258873999118805, Final Batch Loss: 0.0831308364868164\n",
      "Epoch 4550, Loss: 0.21243205666542053, Final Batch Loss: 0.09114757925271988\n",
      "Epoch 4551, Loss: 0.195376954972744, Final Batch Loss: 0.08289314806461334\n",
      "Epoch 4552, Loss: 0.3720892667770386, Final Batch Loss: 0.28447115421295166\n",
      "Epoch 4553, Loss: 0.237867534160614, Final Batch Loss: 0.14357110857963562\n",
      "Epoch 4554, Loss: 0.1811986267566681, Final Batch Loss: 0.08462265878915787\n",
      "Epoch 4555, Loss: 0.178676538169384, Final Batch Loss: 0.061547935009002686\n",
      "Epoch 4556, Loss: 0.17842110246419907, Final Batch Loss: 0.06661055237054825\n",
      "Epoch 4557, Loss: 0.20269767940044403, Final Batch Loss: 0.10028079152107239\n",
      "Epoch 4558, Loss: 0.16493485122919083, Final Batch Loss: 0.09911137819290161\n",
      "Epoch 4559, Loss: 0.20642498135566711, Final Batch Loss: 0.09200339764356613\n",
      "Epoch 4560, Loss: 0.1724899560213089, Final Batch Loss: 0.0801437571644783\n",
      "Epoch 4561, Loss: 0.17417830228805542, Final Batch Loss: 0.09613839536905289\n",
      "Epoch 4562, Loss: 0.25250960886478424, Final Batch Loss: 0.17843015491962433\n",
      "Epoch 4563, Loss: 0.237937331199646, Final Batch Loss: 0.11833329498767853\n",
      "Epoch 4564, Loss: 0.22353162616491318, Final Batch Loss: 0.10623953491449356\n",
      "Epoch 4565, Loss: 0.19400005787611008, Final Batch Loss: 0.1073654294013977\n",
      "Epoch 4566, Loss: 0.18910441547632217, Final Batch Loss: 0.08945614844560623\n",
      "Epoch 4567, Loss: 0.21110769361257553, Final Batch Loss: 0.08767665922641754\n",
      "Epoch 4568, Loss: 0.18717004358768463, Final Batch Loss: 0.08643051236867905\n",
      "Epoch 4569, Loss: 0.19045080244541168, Final Batch Loss: 0.09854821115732193\n",
      "Epoch 4570, Loss: 0.17583534121513367, Final Batch Loss: 0.09186611324548721\n",
      "Epoch 4571, Loss: 0.20376557856798172, Final Batch Loss: 0.0971512496471405\n",
      "Epoch 4572, Loss: 0.19427219033241272, Final Batch Loss: 0.10773058980703354\n",
      "Epoch 4573, Loss: 0.20283576846122742, Final Batch Loss: 0.0835181474685669\n",
      "Epoch 4574, Loss: 0.154740609228611, Final Batch Loss: 0.08193190395832062\n",
      "Epoch 4575, Loss: 0.18513173609972, Final Batch Loss: 0.07222811877727509\n",
      "Epoch 4576, Loss: 0.15733271092176437, Final Batch Loss: 0.0717633068561554\n",
      "Epoch 4577, Loss: 0.17127444595098495, Final Batch Loss: 0.07724607735872269\n",
      "Epoch 4578, Loss: 0.20570667833089828, Final Batch Loss: 0.12530776858329773\n",
      "Epoch 4579, Loss: 0.22952455282211304, Final Batch Loss: 0.13638487458229065\n",
      "Epoch 4580, Loss: 0.2090068683028221, Final Batch Loss: 0.12546411156654358\n",
      "Epoch 4581, Loss: 0.263045109808445, Final Batch Loss: 0.16005872189998627\n",
      "Epoch 4582, Loss: 0.18610910326242447, Final Batch Loss: 0.0856812372803688\n",
      "Epoch 4583, Loss: 0.2059699147939682, Final Batch Loss: 0.10979853570461273\n",
      "Epoch 4584, Loss: 0.18324685841798782, Final Batch Loss: 0.08440036326646805\n",
      "Epoch 4585, Loss: 0.1748398244380951, Final Batch Loss: 0.08716606348752975\n",
      "Epoch 4586, Loss: 0.19574303179979324, Final Batch Loss: 0.1358603537082672\n",
      "Epoch 4587, Loss: 0.18206585943698883, Final Batch Loss: 0.09229404479265213\n",
      "Epoch 4588, Loss: 0.18839748948812485, Final Batch Loss: 0.09842396527528763\n",
      "Epoch 4589, Loss: 0.21561012417078018, Final Batch Loss: 0.09213368594646454\n",
      "Epoch 4590, Loss: 0.23493137955665588, Final Batch Loss: 0.14696212112903595\n",
      "Epoch 4591, Loss: 0.19317571818828583, Final Batch Loss: 0.07153518497943878\n",
      "Epoch 4592, Loss: 0.2429567649960518, Final Batch Loss: 0.14019373059272766\n",
      "Epoch 4593, Loss: 0.19317983835935593, Final Batch Loss: 0.09744079411029816\n",
      "Epoch 4594, Loss: 0.20399145781993866, Final Batch Loss: 0.1318996101617813\n",
      "Epoch 4595, Loss: 0.2170298993587494, Final Batch Loss: 0.09778475761413574\n",
      "Epoch 4596, Loss: 0.16968462616205215, Final Batch Loss: 0.08031092584133148\n",
      "Epoch 4597, Loss: 0.18638978153467178, Final Batch Loss: 0.07969161123037338\n",
      "Epoch 4598, Loss: 0.20510084927082062, Final Batch Loss: 0.08848032355308533\n",
      "Epoch 4599, Loss: 0.17268045246601105, Final Batch Loss: 0.08679389208555222\n",
      "Epoch 4600, Loss: 0.20945072919130325, Final Batch Loss: 0.12248754501342773\n",
      "Epoch 4601, Loss: 0.1664671078324318, Final Batch Loss: 0.08439724147319794\n",
      "Epoch 4602, Loss: 0.20609260350465775, Final Batch Loss: 0.10807990282773972\n",
      "Epoch 4603, Loss: 0.19278251379728317, Final Batch Loss: 0.08251391351222992\n",
      "Epoch 4604, Loss: 0.18028173595666885, Final Batch Loss: 0.12070929259061813\n",
      "Epoch 4605, Loss: 0.19907601177692413, Final Batch Loss: 0.10337698459625244\n",
      "Epoch 4606, Loss: 0.21857551485300064, Final Batch Loss: 0.10156383365392685\n",
      "Epoch 4607, Loss: 0.15292301028966904, Final Batch Loss: 0.06351681798696518\n",
      "Epoch 4608, Loss: 0.18073206394910812, Final Batch Loss: 0.0795891061425209\n",
      "Epoch 4609, Loss: 0.16119877249002457, Final Batch Loss: 0.09559223055839539\n",
      "Epoch 4610, Loss: 0.19243793189525604, Final Batch Loss: 0.08878549933433533\n",
      "Epoch 4611, Loss: 0.16203126311302185, Final Batch Loss: 0.06639663875102997\n",
      "Epoch 4612, Loss: 0.18693000078201294, Final Batch Loss: 0.08268824219703674\n",
      "Epoch 4613, Loss: 0.1725587695837021, Final Batch Loss: 0.07685492187738419\n",
      "Epoch 4614, Loss: 0.22165364027023315, Final Batch Loss: 0.12860681116580963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4615, Loss: 0.1885722577571869, Final Batch Loss: 0.07525516301393509\n",
      "Epoch 4616, Loss: 0.2546863630414009, Final Batch Loss: 0.09790585190057755\n",
      "Epoch 4617, Loss: 0.20978309959173203, Final Batch Loss: 0.12162657827138901\n",
      "Epoch 4618, Loss: 0.1472029611468315, Final Batch Loss: 0.057340264320373535\n",
      "Epoch 4619, Loss: 0.2144128605723381, Final Batch Loss: 0.09031303972005844\n",
      "Epoch 4620, Loss: 0.2478773146867752, Final Batch Loss: 0.1416628509759903\n",
      "Epoch 4621, Loss: 0.1821102797985077, Final Batch Loss: 0.07649486511945724\n",
      "Epoch 4622, Loss: 0.219694584608078, Final Batch Loss: 0.09567831456661224\n",
      "Epoch 4623, Loss: 0.21575622260570526, Final Batch Loss: 0.09641988575458527\n",
      "Epoch 4624, Loss: 0.14787491410970688, Final Batch Loss: 0.05477850139141083\n",
      "Epoch 4625, Loss: 0.23710810393095016, Final Batch Loss: 0.11359991878271103\n",
      "Epoch 4626, Loss: 0.17402151226997375, Final Batch Loss: 0.08088640868663788\n",
      "Epoch 4627, Loss: 0.22652918100357056, Final Batch Loss: 0.1181633472442627\n",
      "Epoch 4628, Loss: 0.14670491963624954, Final Batch Loss: 0.06354130804538727\n",
      "Epoch 4629, Loss: 0.27743443101644516, Final Batch Loss: 0.19489504396915436\n",
      "Epoch 4630, Loss: 0.14096039533615112, Final Batch Loss: 0.06881402432918549\n",
      "Epoch 4631, Loss: 0.17586511373519897, Final Batch Loss: 0.08359717577695847\n",
      "Epoch 4632, Loss: 0.17017914354801178, Final Batch Loss: 0.08535409718751907\n",
      "Epoch 4633, Loss: 0.18892059475183487, Final Batch Loss: 0.09351499378681183\n",
      "Epoch 4634, Loss: 0.18408112227916718, Final Batch Loss: 0.09684838354587555\n",
      "Epoch 4635, Loss: 0.25478436797857285, Final Batch Loss: 0.10034175962209702\n",
      "Epoch 4636, Loss: 0.17522116750478745, Final Batch Loss: 0.08718642592430115\n",
      "Epoch 4637, Loss: 0.2234860584139824, Final Batch Loss: 0.14903107285499573\n",
      "Epoch 4638, Loss: 0.19435470551252365, Final Batch Loss: 0.11398102343082428\n",
      "Epoch 4639, Loss: 0.21035036444664001, Final Batch Loss: 0.10978718101978302\n",
      "Epoch 4640, Loss: 0.19406940788030624, Final Batch Loss: 0.11056499183177948\n",
      "Epoch 4641, Loss: 0.19875185936689377, Final Batch Loss: 0.08127085864543915\n",
      "Epoch 4642, Loss: 0.16981841623783112, Final Batch Loss: 0.06934075057506561\n",
      "Epoch 4643, Loss: 0.20138418674468994, Final Batch Loss: 0.06355738639831543\n",
      "Epoch 4644, Loss: 0.31066353619098663, Final Batch Loss: 0.0943637490272522\n",
      "Epoch 4645, Loss: 0.18819411098957062, Final Batch Loss: 0.10892727971076965\n",
      "Epoch 4646, Loss: 0.1689397469162941, Final Batch Loss: 0.08120393753051758\n",
      "Epoch 4647, Loss: 0.1783199980854988, Final Batch Loss: 0.08423852920532227\n",
      "Epoch 4648, Loss: 0.1656884104013443, Final Batch Loss: 0.06473375856876373\n",
      "Epoch 4649, Loss: 0.26782193034887314, Final Batch Loss: 0.09603298455476761\n",
      "Epoch 4650, Loss: 0.23655764758586884, Final Batch Loss: 0.14307010173797607\n",
      "Epoch 4651, Loss: 0.18740200996398926, Final Batch Loss: 0.09904448688030243\n",
      "Epoch 4652, Loss: 0.29513049125671387, Final Batch Loss: 0.16916972398757935\n",
      "Epoch 4653, Loss: 0.15772965550422668, Final Batch Loss: 0.09016139060258865\n",
      "Epoch 4654, Loss: 0.3038954883813858, Final Batch Loss: 0.17513608932495117\n",
      "Epoch 4655, Loss: 0.17318303138017654, Final Batch Loss: 0.090815968811512\n",
      "Epoch 4656, Loss: 0.28246651589870453, Final Batch Loss: 0.16171854734420776\n",
      "Epoch 4657, Loss: 0.1711677424609661, Final Batch Loss: 0.06229695305228233\n",
      "Epoch 4658, Loss: 0.26560716331005096, Final Batch Loss: 0.13954906165599823\n",
      "Epoch 4659, Loss: 0.200506754219532, Final Batch Loss: 0.09635897725820541\n",
      "Epoch 4660, Loss: 0.21910474449396133, Final Batch Loss: 0.10052099823951721\n",
      "Epoch 4661, Loss: 0.2533455491065979, Final Batch Loss: 0.09880198538303375\n",
      "Epoch 4662, Loss: 0.2093690112233162, Final Batch Loss: 0.09041620045900345\n",
      "Epoch 4663, Loss: 0.21620456129312515, Final Batch Loss: 0.13625089824199677\n",
      "Epoch 4664, Loss: 0.211545892059803, Final Batch Loss: 0.1083083227276802\n",
      "Epoch 4665, Loss: 0.20499806851148605, Final Batch Loss: 0.0927199050784111\n",
      "Epoch 4666, Loss: 0.18219897896051407, Final Batch Loss: 0.08604120463132858\n",
      "Epoch 4667, Loss: 0.20312247425317764, Final Batch Loss: 0.1287485957145691\n",
      "Epoch 4668, Loss: 0.23414355516433716, Final Batch Loss: 0.08888286352157593\n",
      "Epoch 4669, Loss: 0.2006993293762207, Final Batch Loss: 0.11591930687427521\n",
      "Epoch 4670, Loss: 0.17797202616930008, Final Batch Loss: 0.09019269794225693\n",
      "Epoch 4671, Loss: 0.19962304830551147, Final Batch Loss: 0.10993518680334091\n",
      "Epoch 4672, Loss: 0.21563050895929337, Final Batch Loss: 0.1630638688802719\n",
      "Epoch 4673, Loss: 0.22787462174892426, Final Batch Loss: 0.1088545024394989\n",
      "Epoch 4674, Loss: 0.22212658822536469, Final Batch Loss: 0.10169284045696259\n",
      "Epoch 4675, Loss: 0.16443538665771484, Final Batch Loss: 0.0876302421092987\n",
      "Epoch 4676, Loss: 0.21202193200588226, Final Batch Loss: 0.09689163416624069\n",
      "Epoch 4677, Loss: 0.1795303300023079, Final Batch Loss: 0.08817363530397415\n",
      "Epoch 4678, Loss: 0.20343106240034103, Final Batch Loss: 0.11033455282449722\n",
      "Epoch 4679, Loss: 0.16565301269292831, Final Batch Loss: 0.09973688423633575\n",
      "Epoch 4680, Loss: 0.2134069874882698, Final Batch Loss: 0.09057711064815521\n",
      "Epoch 4681, Loss: 0.16415490955114365, Final Batch Loss: 0.07925330102443695\n",
      "Epoch 4682, Loss: 0.19540704041719437, Final Batch Loss: 0.08792168647050858\n",
      "Epoch 4683, Loss: 0.1948499158024788, Final Batch Loss: 0.10764514654874802\n",
      "Epoch 4684, Loss: 0.17014948278665543, Final Batch Loss: 0.09009402990341187\n",
      "Epoch 4685, Loss: 0.18782556802034378, Final Batch Loss: 0.10932154208421707\n",
      "Epoch 4686, Loss: 0.1649542674422264, Final Batch Loss: 0.08548969775438309\n",
      "Epoch 4687, Loss: 0.20095427334308624, Final Batch Loss: 0.08976516872644424\n",
      "Epoch 4688, Loss: 0.23304437845945358, Final Batch Loss: 0.09214232116937637\n",
      "Epoch 4689, Loss: 0.12513446435332298, Final Batch Loss: 0.04263613000512123\n",
      "Epoch 4690, Loss: 0.18053819239139557, Final Batch Loss: 0.08959142863750458\n",
      "Epoch 4691, Loss: 0.21074511855840683, Final Batch Loss: 0.13048167526721954\n",
      "Epoch 4692, Loss: 0.17664743214845657, Final Batch Loss: 0.079051174223423\n",
      "Epoch 4693, Loss: 0.22042357176542282, Final Batch Loss: 0.11121147871017456\n",
      "Epoch 4694, Loss: 0.1465238705277443, Final Batch Loss: 0.06896622478961945\n",
      "Epoch 4695, Loss: 0.22244182229042053, Final Batch Loss: 0.14474307000637054\n",
      "Epoch 4696, Loss: 0.2573300376534462, Final Batch Loss: 0.1869717389345169\n",
      "Epoch 4697, Loss: 0.19726133346557617, Final Batch Loss: 0.10060734301805496\n",
      "Epoch 4698, Loss: 0.1689881607890129, Final Batch Loss: 0.09287487715482712\n",
      "Epoch 4699, Loss: 0.2112354040145874, Final Batch Loss: 0.09922526776790619\n",
      "Epoch 4700, Loss: 0.17967329919338226, Final Batch Loss: 0.06632047146558762\n",
      "Epoch 4701, Loss: 0.16454894840717316, Final Batch Loss: 0.057525888085365295\n",
      "Epoch 4702, Loss: 0.2250998318195343, Final Batch Loss: 0.1296534538269043\n",
      "Epoch 4703, Loss: 0.1827705204486847, Final Batch Loss: 0.0910647064447403\n",
      "Epoch 4704, Loss: 0.16446980834007263, Final Batch Loss: 0.05087101459503174\n",
      "Epoch 4705, Loss: 0.19148953258991241, Final Batch Loss: 0.11303989589214325\n",
      "Epoch 4706, Loss: 0.17657162994146347, Final Batch Loss: 0.09535408020019531\n",
      "Epoch 4707, Loss: 0.30170880258083344, Final Batch Loss: 0.20215854048728943\n",
      "Epoch 4708, Loss: 0.1640949249267578, Final Batch Loss: 0.07662826776504517\n",
      "Epoch 4709, Loss: 0.18971531093120575, Final Batch Loss: 0.06304742395877838\n",
      "Epoch 4710, Loss: 0.17361631244421005, Final Batch Loss: 0.10874175280332565\n",
      "Epoch 4711, Loss: 0.20966685563325882, Final Batch Loss: 0.12377675622701645\n",
      "Epoch 4712, Loss: 0.1591142863035202, Final Batch Loss: 0.05784711241722107\n",
      "Epoch 4713, Loss: 0.2161950096487999, Final Batch Loss: 0.1573280394077301\n",
      "Epoch 4714, Loss: 0.15102657675743103, Final Batch Loss: 0.07245612889528275\n",
      "Epoch 4715, Loss: 0.17720592394471169, Final Batch Loss: 0.12252353131771088\n",
      "Epoch 4716, Loss: 0.17781618237495422, Final Batch Loss: 0.07570117712020874\n",
      "Epoch 4717, Loss: 0.16285455599427223, Final Batch Loss: 0.0577702634036541\n",
      "Epoch 4718, Loss: 0.18942933529615402, Final Batch Loss: 0.07395632565021515\n",
      "Epoch 4719, Loss: 0.19719869643449783, Final Batch Loss: 0.09974363446235657\n",
      "Epoch 4720, Loss: 0.2923077419400215, Final Batch Loss: 0.1781948059797287\n",
      "Epoch 4721, Loss: 0.13896138966083527, Final Batch Loss: 0.06632596254348755\n",
      "Epoch 4722, Loss: 0.19195646047592163, Final Batch Loss: 0.09172496944665909\n",
      "Epoch 4723, Loss: 0.16070029139518738, Final Batch Loss: 0.07489778101444244\n",
      "Epoch 4724, Loss: 0.2535899952054024, Final Batch Loss: 0.15046687424182892\n",
      "Epoch 4725, Loss: 0.18878642469644547, Final Batch Loss: 0.08430257439613342\n",
      "Epoch 4726, Loss: 0.20175019651651382, Final Batch Loss: 0.07484287768602371\n",
      "Epoch 4727, Loss: 0.18846258521080017, Final Batch Loss: 0.10581366717815399\n",
      "Epoch 4728, Loss: 0.1781567484140396, Final Batch Loss: 0.09481131285429001\n",
      "Epoch 4729, Loss: 0.1822984665632248, Final Batch Loss: 0.10298151522874832\n",
      "Epoch 4730, Loss: 0.14110974967479706, Final Batch Loss: 0.06557421386241913\n",
      "Epoch 4731, Loss: 0.17827700823545456, Final Batch Loss: 0.08686809986829758\n",
      "Epoch 4732, Loss: 0.20258694142103195, Final Batch Loss: 0.10095389187335968\n",
      "Epoch 4733, Loss: 0.18479935824871063, Final Batch Loss: 0.08474177122116089\n",
      "Epoch 4734, Loss: 0.1733340099453926, Final Batch Loss: 0.075927734375\n",
      "Epoch 4735, Loss: 0.2203976958990097, Final Batch Loss: 0.08606071770191193\n",
      "Epoch 4736, Loss: 0.16795815154910088, Final Batch Loss: 0.10715877264738083\n",
      "Epoch 4737, Loss: 0.17254774272441864, Final Batch Loss: 0.08008251339197159\n",
      "Epoch 4738, Loss: 0.22509314492344856, Final Batch Loss: 0.16626736521720886\n",
      "Epoch 4739, Loss: 0.16804469376802444, Final Batch Loss: 0.05778934806585312\n",
      "Epoch 4740, Loss: 0.1577172949910164, Final Batch Loss: 0.08891715109348297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4741, Loss: 0.1716194450855255, Final Batch Loss: 0.07258529961109161\n",
      "Epoch 4742, Loss: 0.17986281216144562, Final Batch Loss: 0.09452943503856659\n",
      "Epoch 4743, Loss: 0.15536769852042198, Final Batch Loss: 0.05246401205658913\n",
      "Epoch 4744, Loss: 0.1775304526090622, Final Batch Loss: 0.06338175386190414\n",
      "Epoch 4745, Loss: 0.17728357017040253, Final Batch Loss: 0.07856166362762451\n",
      "Epoch 4746, Loss: 0.15817942470312119, Final Batch Loss: 0.06928499788045883\n",
      "Epoch 4747, Loss: 0.15654364973306656, Final Batch Loss: 0.060487352311611176\n",
      "Epoch 4748, Loss: 0.22885124385356903, Final Batch Loss: 0.1228056252002716\n",
      "Epoch 4749, Loss: 0.1781964935362339, Final Batch Loss: 0.05949438735842705\n",
      "Epoch 4750, Loss: 0.2069481983780861, Final Batch Loss: 0.12814415991306305\n",
      "Epoch 4751, Loss: 0.15839208662509918, Final Batch Loss: 0.06392897665500641\n",
      "Epoch 4752, Loss: 0.2864968031644821, Final Batch Loss: 0.2164405882358551\n",
      "Epoch 4753, Loss: 0.1478223130106926, Final Batch Loss: 0.06673088669776917\n",
      "Epoch 4754, Loss: 0.15638138353824615, Final Batch Loss: 0.07114578038454056\n",
      "Epoch 4755, Loss: 0.23527798056602478, Final Batch Loss: 0.15243375301361084\n",
      "Epoch 4756, Loss: 0.1393837071955204, Final Batch Loss: 0.050505947321653366\n",
      "Epoch 4757, Loss: 0.19828757643699646, Final Batch Loss: 0.09427285939455032\n",
      "Epoch 4758, Loss: 0.1722649484872818, Final Batch Loss: 0.09575438499450684\n",
      "Epoch 4759, Loss: 0.1470433995127678, Final Batch Loss: 0.06046251207590103\n",
      "Epoch 4760, Loss: 0.16320834308862686, Final Batch Loss: 0.0899176299571991\n",
      "Epoch 4761, Loss: 0.15999292582273483, Final Batch Loss: 0.08994132280349731\n",
      "Epoch 4762, Loss: 0.14502830058336258, Final Batch Loss: 0.08701178431510925\n",
      "Epoch 4763, Loss: 0.1901824027299881, Final Batch Loss: 0.11533695459365845\n",
      "Epoch 4764, Loss: 0.19162286818027496, Final Batch Loss: 0.07671786099672318\n",
      "Epoch 4765, Loss: 0.20808423310518265, Final Batch Loss: 0.0939975380897522\n",
      "Epoch 4766, Loss: 0.20131732523441315, Final Batch Loss: 0.10163258761167526\n",
      "Epoch 4767, Loss: 0.1824657842516899, Final Batch Loss: 0.09278204292058945\n",
      "Epoch 4768, Loss: 0.16927174478769302, Final Batch Loss: 0.0845240131020546\n",
      "Epoch 4769, Loss: 0.2012750580906868, Final Batch Loss: 0.11745508760213852\n",
      "Epoch 4770, Loss: 0.1936015859246254, Final Batch Loss: 0.09158571064472198\n",
      "Epoch 4771, Loss: 0.2886246144771576, Final Batch Loss: 0.20249347388744354\n",
      "Epoch 4772, Loss: 0.15788203105330467, Final Batch Loss: 0.05661751702427864\n",
      "Epoch 4773, Loss: 0.18424589931964874, Final Batch Loss: 0.06576621532440186\n",
      "Epoch 4774, Loss: 0.1665387526154518, Final Batch Loss: 0.08674031496047974\n",
      "Epoch 4775, Loss: 0.15127455815672874, Final Batch Loss: 0.06235341355204582\n",
      "Epoch 4776, Loss: 0.13603559136390686, Final Batch Loss: 0.05839623510837555\n",
      "Epoch 4777, Loss: 0.1719036102294922, Final Batch Loss: 0.06320909410715103\n",
      "Epoch 4778, Loss: 0.19942378252744675, Final Batch Loss: 0.1037517860531807\n",
      "Epoch 4779, Loss: 0.1807457134127617, Final Batch Loss: 0.10953006893396378\n",
      "Epoch 4780, Loss: 0.19340283423662186, Final Batch Loss: 0.12359192222356796\n",
      "Epoch 4781, Loss: 0.1779307872056961, Final Batch Loss: 0.08550646156072617\n",
      "Epoch 4782, Loss: 0.14356201514601707, Final Batch Loss: 0.0546865351498127\n",
      "Epoch 4783, Loss: 0.16581348702311516, Final Batch Loss: 0.06116432324051857\n",
      "Epoch 4784, Loss: 0.2244403287768364, Final Batch Loss: 0.11491026729345322\n",
      "Epoch 4785, Loss: 0.21360228210687637, Final Batch Loss: 0.12423624098300934\n",
      "Epoch 4786, Loss: 0.16634874790906906, Final Batch Loss: 0.07980313897132874\n",
      "Epoch 4787, Loss: 0.14917657524347305, Final Batch Loss: 0.049662575125694275\n",
      "Epoch 4788, Loss: 0.2709350436925888, Final Batch Loss: 0.10503187775611877\n",
      "Epoch 4789, Loss: 0.15388710796833038, Final Batch Loss: 0.08832430839538574\n",
      "Epoch 4790, Loss: 0.1647188365459442, Final Batch Loss: 0.09157996624708176\n",
      "Epoch 4791, Loss: 0.15345192700624466, Final Batch Loss: 0.07545235753059387\n",
      "Epoch 4792, Loss: 0.1712341159582138, Final Batch Loss: 0.08531013876199722\n",
      "Epoch 4793, Loss: 0.15599409490823746, Final Batch Loss: 0.048607535660266876\n",
      "Epoch 4794, Loss: 0.15824595466256142, Final Batch Loss: 0.05660340562462807\n",
      "Epoch 4795, Loss: 0.1494372934103012, Final Batch Loss: 0.06259162724018097\n",
      "Epoch 4796, Loss: 0.19928430020809174, Final Batch Loss: 0.1057785302400589\n",
      "Epoch 4797, Loss: 0.18533927202224731, Final Batch Loss: 0.07998108863830566\n",
      "Epoch 4798, Loss: 0.16791905462741852, Final Batch Loss: 0.07650107145309448\n",
      "Epoch 4799, Loss: 0.14873894304037094, Final Batch Loss: 0.07816246896982193\n",
      "Epoch 4800, Loss: 0.1653231456875801, Final Batch Loss: 0.05932039022445679\n",
      "Epoch 4801, Loss: 0.20072440057992935, Final Batch Loss: 0.09889236092567444\n",
      "Epoch 4802, Loss: 0.15846475958824158, Final Batch Loss: 0.051353976130485535\n",
      "Epoch 4803, Loss: 0.2031637504696846, Final Batch Loss: 0.083424873650074\n",
      "Epoch 4804, Loss: 0.19025737047195435, Final Batch Loss: 0.09839668869972229\n",
      "Epoch 4805, Loss: 0.20103275775909424, Final Batch Loss: 0.1209416389465332\n",
      "Epoch 4806, Loss: 0.18786989152431488, Final Batch Loss: 0.11196768283843994\n",
      "Epoch 4807, Loss: 0.19735907763242722, Final Batch Loss: 0.07318827509880066\n",
      "Epoch 4808, Loss: 0.18230529874563217, Final Batch Loss: 0.10939335823059082\n",
      "Epoch 4809, Loss: 0.14776837080717087, Final Batch Loss: 0.06786927580833435\n",
      "Epoch 4810, Loss: 0.21026251465082169, Final Batch Loss: 0.11002824455499649\n",
      "Epoch 4811, Loss: 0.17548435926437378, Final Batch Loss: 0.07688312977552414\n",
      "Epoch 4812, Loss: 0.14402491599321365, Final Batch Loss: 0.04699067771434784\n",
      "Epoch 4813, Loss: 0.17954983562231064, Final Batch Loss: 0.0923067107796669\n",
      "Epoch 4814, Loss: 0.21517732739448547, Final Batch Loss: 0.10412884503602982\n",
      "Epoch 4815, Loss: 0.17585787177085876, Final Batch Loss: 0.08084776252508163\n",
      "Epoch 4816, Loss: 0.20290163159370422, Final Batch Loss: 0.11765298247337341\n",
      "Epoch 4817, Loss: 0.1450262889266014, Final Batch Loss: 0.06431753188371658\n",
      "Epoch 4818, Loss: 0.12856106087565422, Final Batch Loss: 0.05136807635426521\n",
      "Epoch 4819, Loss: 0.17464304715394974, Final Batch Loss: 0.0952385738492012\n",
      "Epoch 4820, Loss: 0.16369592398405075, Final Batch Loss: 0.07065436244010925\n",
      "Epoch 4821, Loss: 0.17649086564779282, Final Batch Loss: 0.1054944172501564\n",
      "Epoch 4822, Loss: 0.17586451023817062, Final Batch Loss: 0.07842037081718445\n",
      "Epoch 4823, Loss: 0.26777108013629913, Final Batch Loss: 0.18872462213039398\n",
      "Epoch 4824, Loss: 0.14800115674734116, Final Batch Loss: 0.04303785413503647\n",
      "Epoch 4825, Loss: 0.1644323579967022, Final Batch Loss: 0.05952871963381767\n",
      "Epoch 4826, Loss: 0.20795220881700516, Final Batch Loss: 0.08948379755020142\n",
      "Epoch 4827, Loss: 0.12703018262982368, Final Batch Loss: 0.05059570446610451\n",
      "Epoch 4828, Loss: 0.15531794726848602, Final Batch Loss: 0.07306176424026489\n",
      "Epoch 4829, Loss: 0.2426503375172615, Final Batch Loss: 0.10338438302278519\n",
      "Epoch 4830, Loss: 0.15631316229701042, Final Batch Loss: 0.09677039831876755\n",
      "Epoch 4831, Loss: 0.17822379618883133, Final Batch Loss: 0.09286773949861526\n",
      "Epoch 4832, Loss: 0.23722007125616074, Final Batch Loss: 0.07896896451711655\n",
      "Epoch 4833, Loss: 0.19218622148036957, Final Batch Loss: 0.08344440907239914\n",
      "Epoch 4834, Loss: 0.21352286636829376, Final Batch Loss: 0.11649955809116364\n",
      "Epoch 4835, Loss: 0.2028384506702423, Final Batch Loss: 0.10367623716592789\n",
      "Epoch 4836, Loss: 0.14831150323152542, Final Batch Loss: 0.07633014023303986\n",
      "Epoch 4837, Loss: 0.21679261326789856, Final Batch Loss: 0.11195719987154007\n",
      "Epoch 4838, Loss: 0.15989472717046738, Final Batch Loss: 0.07045134902000427\n",
      "Epoch 4839, Loss: 0.20145191252231598, Final Batch Loss: 0.10849498212337494\n",
      "Epoch 4840, Loss: 0.18950404971837997, Final Batch Loss: 0.07160859555006027\n",
      "Epoch 4841, Loss: 0.16984806954860687, Final Batch Loss: 0.09687812626361847\n",
      "Epoch 4842, Loss: 0.1664770282804966, Final Batch Loss: 0.056205760687589645\n",
      "Epoch 4843, Loss: 0.14399822428822517, Final Batch Loss: 0.0609109066426754\n",
      "Epoch 4844, Loss: 0.17657963931560516, Final Batch Loss: 0.09776810556650162\n",
      "Epoch 4845, Loss: 0.18515607714653015, Final Batch Loss: 0.1156979650259018\n",
      "Epoch 4846, Loss: 0.17144961655139923, Final Batch Loss: 0.0912541002035141\n",
      "Epoch 4847, Loss: 0.18628289550542831, Final Batch Loss: 0.12236722558736801\n",
      "Epoch 4848, Loss: 0.20783531665802002, Final Batch Loss: 0.07824784517288208\n",
      "Epoch 4849, Loss: 0.23358812928199768, Final Batch Loss: 0.11812750995159149\n",
      "Epoch 4850, Loss: 0.26071830838918686, Final Batch Loss: 0.08907411247491837\n",
      "Epoch 4851, Loss: 0.20563318580389023, Final Batch Loss: 0.08220010250806808\n",
      "Epoch 4852, Loss: 0.1711922436952591, Final Batch Loss: 0.09964531660079956\n",
      "Epoch 4853, Loss: 0.16499536484479904, Final Batch Loss: 0.09007548540830612\n",
      "Epoch 4854, Loss: 0.16043943166732788, Final Batch Loss: 0.06268928945064545\n",
      "Epoch 4855, Loss: 0.14959237724542618, Final Batch Loss: 0.05584311485290527\n",
      "Epoch 4856, Loss: 0.18325044214725494, Final Batch Loss: 0.12500035762786865\n",
      "Epoch 4857, Loss: 0.19021523743867874, Final Batch Loss: 0.11074010282754898\n",
      "Epoch 4858, Loss: 0.14888352900743484, Final Batch Loss: 0.06430037319660187\n",
      "Epoch 4859, Loss: 0.2010667398571968, Final Batch Loss: 0.10887429863214493\n",
      "Epoch 4860, Loss: 0.19058261811733246, Final Batch Loss: 0.11384213715791702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4861, Loss: 0.20295438915491104, Final Batch Loss: 0.12180987745523453\n",
      "Epoch 4862, Loss: 0.1704985424876213, Final Batch Loss: 0.09512745589017868\n",
      "Epoch 4863, Loss: 0.1733236312866211, Final Batch Loss: 0.08537114411592484\n",
      "Epoch 4864, Loss: 0.16138514131307602, Final Batch Loss: 0.08772565424442291\n",
      "Epoch 4865, Loss: 0.19300898909568787, Final Batch Loss: 0.10036994516849518\n",
      "Epoch 4866, Loss: 0.2289099395275116, Final Batch Loss: 0.11629708111286163\n",
      "Epoch 4867, Loss: 0.15332218259572983, Final Batch Loss: 0.0408015102148056\n",
      "Epoch 4868, Loss: 0.19450931996107101, Final Batch Loss: 0.09705938398838043\n",
      "Epoch 4869, Loss: 0.11511939018964767, Final Batch Loss: 0.04928753525018692\n",
      "Epoch 4870, Loss: 0.18257181346416473, Final Batch Loss: 0.1021391749382019\n",
      "Epoch 4871, Loss: 0.29161337018013, Final Batch Loss: 0.1731395125389099\n",
      "Epoch 4872, Loss: 0.17138046771287918, Final Batch Loss: 0.10139360278844833\n",
      "Epoch 4873, Loss: 0.16688203811645508, Final Batch Loss: 0.0714954361319542\n",
      "Epoch 4874, Loss: 0.17853406816720963, Final Batch Loss: 0.10596375912427902\n",
      "Epoch 4875, Loss: 0.21810762584209442, Final Batch Loss: 0.10661681741476059\n",
      "Epoch 4876, Loss: 0.18956895917654037, Final Batch Loss: 0.1161755695939064\n",
      "Epoch 4877, Loss: 0.21863756328821182, Final Batch Loss: 0.11777946352958679\n",
      "Epoch 4878, Loss: 0.1369854435324669, Final Batch Loss: 0.04784383624792099\n",
      "Epoch 4879, Loss: 0.136895552277565, Final Batch Loss: 0.07736421376466751\n",
      "Epoch 4880, Loss: 0.16997571289539337, Final Batch Loss: 0.08997111767530441\n",
      "Epoch 4881, Loss: 0.19268091022968292, Final Batch Loss: 0.10648749768733978\n",
      "Epoch 4882, Loss: 0.16382784396409988, Final Batch Loss: 0.07034052163362503\n",
      "Epoch 4883, Loss: 0.22572892904281616, Final Batch Loss: 0.1205560490489006\n",
      "Epoch 4884, Loss: 0.13734504580497742, Final Batch Loss: 0.055464401841163635\n",
      "Epoch 4885, Loss: 0.18881429731845856, Final Batch Loss: 0.12132412195205688\n",
      "Epoch 4886, Loss: 0.1345326565206051, Final Batch Loss: 0.043685432523489\n",
      "Epoch 4887, Loss: 0.12834905087947845, Final Batch Loss: 0.05708959698677063\n",
      "Epoch 4888, Loss: 0.16739089041948318, Final Batch Loss: 0.07643400877714157\n",
      "Epoch 4889, Loss: 0.19380635023117065, Final Batch Loss: 0.1148047149181366\n",
      "Epoch 4890, Loss: 0.16465983167290688, Final Batch Loss: 0.06209099665284157\n",
      "Epoch 4891, Loss: 0.19528426229953766, Final Batch Loss: 0.09992671012878418\n",
      "Epoch 4892, Loss: 0.21726993471384048, Final Batch Loss: 0.08194214850664139\n",
      "Epoch 4893, Loss: 0.24765266850590706, Final Batch Loss: 0.18684081733226776\n",
      "Epoch 4894, Loss: 0.21093031018972397, Final Batch Loss: 0.09661141037940979\n",
      "Epoch 4895, Loss: 0.15438609570264816, Final Batch Loss: 0.08240925520658493\n",
      "Epoch 4896, Loss: 0.19539791345596313, Final Batch Loss: 0.06453998386859894\n",
      "Epoch 4897, Loss: 0.19257359951734543, Final Batch Loss: 0.12305732071399689\n",
      "Epoch 4898, Loss: 0.19981691241264343, Final Batch Loss: 0.0941680446267128\n",
      "Epoch 4899, Loss: 0.23982322216033936, Final Batch Loss: 0.16043631732463837\n",
      "Epoch 4900, Loss: 0.1977742537856102, Final Batch Loss: 0.11047777533531189\n",
      "Epoch 4901, Loss: 0.15524408966302872, Final Batch Loss: 0.07108728587627411\n",
      "Epoch 4902, Loss: 0.13697199523448944, Final Batch Loss: 0.06002361327409744\n",
      "Epoch 4903, Loss: 0.37231292575597763, Final Batch Loss: 0.25018832087516785\n",
      "Epoch 4904, Loss: 0.15751930326223373, Final Batch Loss: 0.06395118683576584\n",
      "Epoch 4905, Loss: 0.20211589336395264, Final Batch Loss: 0.09275640547275543\n",
      "Epoch 4906, Loss: 0.20453787595033646, Final Batch Loss: 0.0943470448255539\n",
      "Epoch 4907, Loss: 0.19437096267938614, Final Batch Loss: 0.09680408984422684\n",
      "Epoch 4908, Loss: 0.16627363115549088, Final Batch Loss: 0.08242709934711456\n",
      "Epoch 4909, Loss: 0.21082664281129837, Final Batch Loss: 0.1265469491481781\n",
      "Epoch 4910, Loss: 0.23592527210712433, Final Batch Loss: 0.15153856575489044\n",
      "Epoch 4911, Loss: 0.16793452203273773, Final Batch Loss: 0.08656888455152512\n",
      "Epoch 4912, Loss: 0.15875766426324844, Final Batch Loss: 0.09075187146663666\n",
      "Epoch 4913, Loss: 0.14918919652700424, Final Batch Loss: 0.07562939822673798\n",
      "Epoch 4914, Loss: 0.17345188558101654, Final Batch Loss: 0.08666959404945374\n",
      "Epoch 4915, Loss: 0.13120851293206215, Final Batch Loss: 0.04449373111128807\n",
      "Epoch 4916, Loss: 0.18063626438379288, Final Batch Loss: 0.09461458027362823\n",
      "Epoch 4917, Loss: 0.17839647829532623, Final Batch Loss: 0.0812300518155098\n",
      "Epoch 4918, Loss: 0.20883147418498993, Final Batch Loss: 0.10168910771608353\n",
      "Epoch 4919, Loss: 0.16075529158115387, Final Batch Loss: 0.07251794636249542\n",
      "Epoch 4920, Loss: 0.14446189999580383, Final Batch Loss: 0.0515546053647995\n",
      "Epoch 4921, Loss: 0.1553271785378456, Final Batch Loss: 0.08231659978628159\n",
      "Epoch 4922, Loss: 0.19814356416463852, Final Batch Loss: 0.10125080496072769\n",
      "Epoch 4923, Loss: 0.14397919178009033, Final Batch Loss: 0.06886550039052963\n",
      "Epoch 4924, Loss: 0.17779172211885452, Final Batch Loss: 0.09461726248264313\n",
      "Epoch 4925, Loss: 0.1956353858113289, Final Batch Loss: 0.09120140969753265\n",
      "Epoch 4926, Loss: 0.18718019872903824, Final Batch Loss: 0.08343883603811264\n",
      "Epoch 4927, Loss: 0.12230779975652695, Final Batch Loss: 0.05470310151576996\n",
      "Epoch 4928, Loss: 0.18658333271741867, Final Batch Loss: 0.08270815759897232\n",
      "Epoch 4929, Loss: 0.13597864285111427, Final Batch Loss: 0.057106394320726395\n",
      "Epoch 4930, Loss: 0.1348753347992897, Final Batch Loss: 0.0664253979921341\n",
      "Epoch 4931, Loss: 0.19009550660848618, Final Batch Loss: 0.12490087002515793\n",
      "Epoch 4932, Loss: 0.16861216723918915, Final Batch Loss: 0.08702787756919861\n",
      "Epoch 4933, Loss: 0.12185840681195259, Final Batch Loss: 0.04853152111172676\n",
      "Epoch 4934, Loss: 0.19088777154684067, Final Batch Loss: 0.1070956438779831\n",
      "Epoch 4935, Loss: 0.20225594192743301, Final Batch Loss: 0.11022087931632996\n",
      "Epoch 4936, Loss: 0.20843255519866943, Final Batch Loss: 0.14227700233459473\n",
      "Epoch 4937, Loss: 0.16605893895030022, Final Batch Loss: 0.06242392584681511\n",
      "Epoch 4938, Loss: 0.17134220153093338, Final Batch Loss: 0.07946773618459702\n",
      "Epoch 4939, Loss: 0.16510341316461563, Final Batch Loss: 0.06447378545999527\n",
      "Epoch 4940, Loss: 0.2460447922348976, Final Batch Loss: 0.11629179865121841\n",
      "Epoch 4941, Loss: 0.261022686958313, Final Batch Loss: 0.1672934591770172\n",
      "Epoch 4942, Loss: 0.13918078690767288, Final Batch Loss: 0.06774865835905075\n",
      "Epoch 4943, Loss: 0.15619846433401108, Final Batch Loss: 0.09110227227210999\n",
      "Epoch 4944, Loss: 0.15926064550876617, Final Batch Loss: 0.06690540164709091\n",
      "Epoch 4945, Loss: 0.14463352411985397, Final Batch Loss: 0.08756306022405624\n",
      "Epoch 4946, Loss: 0.17047908902168274, Final Batch Loss: 0.11043116450309753\n",
      "Epoch 4947, Loss: 0.15053898468613625, Final Batch Loss: 0.08978250622749329\n",
      "Epoch 4948, Loss: 0.18525151163339615, Final Batch Loss: 0.0880369320511818\n",
      "Epoch 4949, Loss: 0.141654621809721, Final Batch Loss: 0.05666589364409447\n",
      "Epoch 4950, Loss: 0.1766834780573845, Final Batch Loss: 0.08799105137586594\n",
      "Epoch 4951, Loss: 0.16563651710748672, Final Batch Loss: 0.06899736821651459\n",
      "Epoch 4952, Loss: 0.186968132853508, Final Batch Loss: 0.06718334555625916\n",
      "Epoch 4953, Loss: 0.16214443743228912, Final Batch Loss: 0.06487900763750076\n",
      "Epoch 4954, Loss: 0.17358960211277008, Final Batch Loss: 0.09728890657424927\n",
      "Epoch 4955, Loss: 0.2076299786567688, Final Batch Loss: 0.10643460601568222\n",
      "Epoch 4956, Loss: 0.16608016937971115, Final Batch Loss: 0.0710039883852005\n",
      "Epoch 4957, Loss: 0.1710907593369484, Final Batch Loss: 0.08015037328004837\n",
      "Epoch 4958, Loss: 0.1261894777417183, Final Batch Loss: 0.06520572304725647\n",
      "Epoch 4959, Loss: 0.13123314455151558, Final Batch Loss: 0.07638810575008392\n",
      "Epoch 4960, Loss: 0.22172582149505615, Final Batch Loss: 0.10509902983903885\n",
      "Epoch 4961, Loss: 0.16620541363954544, Final Batch Loss: 0.07694462686777115\n",
      "Epoch 4962, Loss: 0.13181083649396896, Final Batch Loss: 0.0589972659945488\n",
      "Epoch 4963, Loss: 0.16524356976151466, Final Batch Loss: 0.10599704086780548\n",
      "Epoch 4964, Loss: 0.20027154684066772, Final Batch Loss: 0.10577563941478729\n",
      "Epoch 4965, Loss: 0.1729719638824463, Final Batch Loss: 0.10938329249620438\n",
      "Epoch 4966, Loss: 0.21188130229711533, Final Batch Loss: 0.0738179162144661\n",
      "Epoch 4967, Loss: 0.17128796130418777, Final Batch Loss: 0.06841392070055008\n",
      "Epoch 4968, Loss: 0.20973056554794312, Final Batch Loss: 0.11054124683141708\n",
      "Epoch 4969, Loss: 0.13388397172093391, Final Batch Loss: 0.05724218115210533\n",
      "Epoch 4970, Loss: 0.15529853105545044, Final Batch Loss: 0.09075488895177841\n",
      "Epoch 4971, Loss: 0.1506858617067337, Final Batch Loss: 0.06501888483762741\n",
      "Epoch 4972, Loss: 0.19473148882389069, Final Batch Loss: 0.11390241980552673\n",
      "Epoch 4973, Loss: 0.16126780956983566, Final Batch Loss: 0.07078765332698822\n",
      "Epoch 4974, Loss: 0.16284580528736115, Final Batch Loss: 0.09031493961811066\n",
      "Epoch 4975, Loss: 0.1885462999343872, Final Batch Loss: 0.07910966128110886\n",
      "Epoch 4976, Loss: 0.12531999498605728, Final Batch Loss: 0.05255318433046341\n",
      "Epoch 4977, Loss: 0.13453459739685059, Final Batch Loss: 0.05969636142253876\n",
      "Epoch 4978, Loss: 0.10287866741418839, Final Batch Loss: 0.036346517503261566\n",
      "Epoch 4979, Loss: 0.15171543881297112, Final Batch Loss: 0.06152254715561867\n",
      "Epoch 4980, Loss: 0.17944006621837616, Final Batch Loss: 0.08152640610933304\n",
      "Epoch 4981, Loss: 0.17657141014933586, Final Batch Loss: 0.1196780800819397\n",
      "Epoch 4982, Loss: 0.1530166156589985, Final Batch Loss: 0.09708195924758911\n",
      "Epoch 4983, Loss: 0.13696609437465668, Final Batch Loss: 0.07206729799509048\n",
      "Epoch 4984, Loss: 0.1699945405125618, Final Batch Loss: 0.10613886266946793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4985, Loss: 0.1648438200354576, Final Batch Loss: 0.08567185699939728\n",
      "Epoch 4986, Loss: 0.16471297666430473, Final Batch Loss: 0.10533574223518372\n",
      "Epoch 4987, Loss: 0.14333349838852882, Final Batch Loss: 0.05975921079516411\n",
      "Epoch 4988, Loss: 0.1403031162917614, Final Batch Loss: 0.04058270528912544\n",
      "Epoch 4989, Loss: 0.14046068117022514, Final Batch Loss: 0.05707496777176857\n",
      "Epoch 4990, Loss: 0.20313109457492828, Final Batch Loss: 0.13895587623119354\n",
      "Epoch 4991, Loss: 0.19033481925725937, Final Batch Loss: 0.0815543383359909\n",
      "Epoch 4992, Loss: 0.1747642531991005, Final Batch Loss: 0.09596822410821915\n",
      "Epoch 4993, Loss: 0.14298060908913612, Final Batch Loss: 0.08230508118867874\n",
      "Epoch 4994, Loss: 0.2598283439874649, Final Batch Loss: 0.06997057795524597\n",
      "Epoch 4995, Loss: 0.20758536458015442, Final Batch Loss: 0.08766692131757736\n",
      "Epoch 4996, Loss: 0.1374574936926365, Final Batch Loss: 0.051866818219423294\n",
      "Epoch 4997, Loss: 0.18277210742235184, Final Batch Loss: 0.09567105770111084\n",
      "Epoch 4998, Loss: 0.1775604709982872, Final Batch Loss: 0.10833322256803513\n",
      "Epoch 4999, Loss: 0.17456789314746857, Final Batch Loss: 0.08086103945970535\n",
      "Epoch 5000, Loss: 0.1784922443330288, Final Batch Loss: 0.05711398646235466\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  1  0  0  8  0  0  2]\n",
      " [ 0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  1  1  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        22\n",
      "           1    1.00000   1.00000   1.00000         8\n",
      "           2    0.87500   0.87500   0.87500         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    0.80000   0.72727   0.76190        11\n",
      "           6    0.92308   1.00000   0.96000        12\n",
      "           7    1.00000   1.00000   1.00000        10\n",
      "           8    0.84615   0.84615   0.84615        13\n",
      "\n",
      "    accuracy                        0.94000       100\n",
      "   macro avg    0.93825   0.93871   0.93812       100\n",
      "weighted avg    0.93877   0.94000   0.93901       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_30k_TEST_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = get_noise(len(X_train), 100)\n",
    "act_vectors = get_act_matrix(len(X_train), 3)\n",
    "usr_vectors = get_usr_matrix(len(X_train), 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(len(X_train)):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fake = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fake.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(fake_features)\n",
    "train_labels = torch.tensor(fake_labels)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.437093257904053, Final Batch Loss: 2.2363758087158203\n",
      "Epoch 2, Loss: 4.418625593185425, Final Batch Loss: 2.2023468017578125\n",
      "Epoch 3, Loss: 4.4237470626831055, Final Batch Loss: 2.2248528003692627\n",
      "Epoch 4, Loss: 4.41790509223938, Final Batch Loss: 2.2051563262939453\n",
      "Epoch 5, Loss: 4.414108753204346, Final Batch Loss: 2.2095723152160645\n",
      "Epoch 6, Loss: 4.411884546279907, Final Batch Loss: 2.208794116973877\n",
      "Epoch 7, Loss: 4.406961917877197, Final Batch Loss: 2.2055656909942627\n",
      "Epoch 8, Loss: 4.403103828430176, Final Batch Loss: 2.2030482292175293\n",
      "Epoch 9, Loss: 4.400484323501587, Final Batch Loss: 2.195773124694824\n",
      "Epoch 10, Loss: 4.395890712738037, Final Batch Loss: 2.1942098140716553\n",
      "Epoch 11, Loss: 4.3940818309783936, Final Batch Loss: 2.1958820819854736\n",
      "Epoch 12, Loss: 4.39016580581665, Final Batch Loss: 2.194636583328247\n",
      "Epoch 13, Loss: 4.389580965042114, Final Batch Loss: 2.192624568939209\n",
      "Epoch 14, Loss: 4.386880874633789, Final Batch Loss: 2.1937923431396484\n",
      "Epoch 15, Loss: 4.382494211196899, Final Batch Loss: 2.1950595378875732\n",
      "Epoch 16, Loss: 4.379042625427246, Final Batch Loss: 2.1941096782684326\n",
      "Epoch 17, Loss: 4.372406482696533, Final Batch Loss: 2.179215431213379\n",
      "Epoch 18, Loss: 4.373479604721069, Final Batch Loss: 2.1929190158843994\n",
      "Epoch 19, Loss: 4.364176988601685, Final Batch Loss: 2.1803197860717773\n",
      "Epoch 20, Loss: 4.360289573669434, Final Batch Loss: 2.1757185459136963\n",
      "Epoch 21, Loss: 4.353675842285156, Final Batch Loss: 2.175621271133423\n",
      "Epoch 22, Loss: 4.3531410694122314, Final Batch Loss: 2.181969165802002\n",
      "Epoch 23, Loss: 4.342793941497803, Final Batch Loss: 2.169038772583008\n",
      "Epoch 24, Loss: 4.3381218910217285, Final Batch Loss: 2.171630382537842\n",
      "Epoch 25, Loss: 4.3314855098724365, Final Batch Loss: 2.1692655086517334\n",
      "Epoch 26, Loss: 4.323715686798096, Final Batch Loss: 2.160682201385498\n",
      "Epoch 27, Loss: 4.321960926055908, Final Batch Loss: 2.1690447330474854\n",
      "Epoch 28, Loss: 4.294823169708252, Final Batch Loss: 2.138862371444702\n",
      "Epoch 29, Loss: 4.283146142959595, Final Batch Loss: 2.1351842880249023\n",
      "Epoch 30, Loss: 4.276062965393066, Final Batch Loss: 2.1352362632751465\n",
      "Epoch 31, Loss: 4.263979196548462, Final Batch Loss: 2.132344961166382\n",
      "Epoch 32, Loss: 4.248367071151733, Final Batch Loss: 2.1270782947540283\n",
      "Epoch 33, Loss: 4.237562656402588, Final Batch Loss: 2.1361653804779053\n",
      "Epoch 34, Loss: 4.1995978355407715, Final Batch Loss: 2.0966591835021973\n",
      "Epoch 35, Loss: 4.163450002670288, Final Batch Loss: 2.064577341079712\n",
      "Epoch 36, Loss: 4.127156972885132, Final Batch Loss: 2.066561222076416\n",
      "Epoch 37, Loss: 4.124141693115234, Final Batch Loss: 2.0596938133239746\n",
      "Epoch 38, Loss: 4.075778007507324, Final Batch Loss: 2.0338308811187744\n",
      "Epoch 39, Loss: 4.0020564794540405, Final Batch Loss: 1.9747616052627563\n",
      "Epoch 40, Loss: 4.007022380828857, Final Batch Loss: 1.9949746131896973\n",
      "Epoch 41, Loss: 3.9018189907073975, Final Batch Loss: 1.9321978092193604\n",
      "Epoch 42, Loss: 3.9118584394454956, Final Batch Loss: 1.9612252712249756\n",
      "Epoch 43, Loss: 3.8246777057647705, Final Batch Loss: 1.9016958475112915\n",
      "Epoch 44, Loss: 3.718546152114868, Final Batch Loss: 1.8401117324829102\n",
      "Epoch 45, Loss: 3.6964677572250366, Final Batch Loss: 1.8169690370559692\n",
      "Epoch 46, Loss: 3.6849285364151, Final Batch Loss: 1.8437600135803223\n",
      "Epoch 47, Loss: 3.527771472930908, Final Batch Loss: 1.7226486206054688\n",
      "Epoch 48, Loss: 3.48376727104187, Final Batch Loss: 1.7256876230239868\n",
      "Epoch 49, Loss: 3.4533488750457764, Final Batch Loss: 1.7505991458892822\n",
      "Epoch 50, Loss: 3.360391139984131, Final Batch Loss: 1.6914809942245483\n",
      "Epoch 51, Loss: 3.253261685371399, Final Batch Loss: 1.6212103366851807\n",
      "Epoch 52, Loss: 3.2068761587142944, Final Batch Loss: 1.6041568517684937\n",
      "Epoch 53, Loss: 3.141966700553894, Final Batch Loss: 1.5588382482528687\n",
      "Epoch 54, Loss: 2.98558247089386, Final Batch Loss: 1.4969013929367065\n",
      "Epoch 55, Loss: 2.980608344078064, Final Batch Loss: 1.4854902029037476\n",
      "Epoch 56, Loss: 2.8091875314712524, Final Batch Loss: 1.3687924146652222\n",
      "Epoch 57, Loss: 2.7184027433395386, Final Batch Loss: 1.2911019325256348\n",
      "Epoch 58, Loss: 2.7358014583587646, Final Batch Loss: 1.3803876638412476\n",
      "Epoch 59, Loss: 2.5615155696868896, Final Batch Loss: 1.28250253200531\n",
      "Epoch 60, Loss: 2.576192021369934, Final Batch Loss: 1.2754801511764526\n",
      "Epoch 61, Loss: 2.5100806951522827, Final Batch Loss: 1.254960298538208\n",
      "Epoch 62, Loss: 2.401496410369873, Final Batch Loss: 1.1662522554397583\n",
      "Epoch 63, Loss: 2.4490667581558228, Final Batch Loss: 1.3275103569030762\n",
      "Epoch 64, Loss: 2.3350600004196167, Final Batch Loss: 1.1245137453079224\n",
      "Epoch 65, Loss: 2.2542601823806763, Final Batch Loss: 1.076589584350586\n",
      "Epoch 66, Loss: 2.2484683990478516, Final Batch Loss: 1.0983924865722656\n",
      "Epoch 67, Loss: 2.1604093313217163, Final Batch Loss: 1.065896987915039\n",
      "Epoch 68, Loss: 2.1512978076934814, Final Batch Loss: 1.0768276453018188\n",
      "Epoch 69, Loss: 2.1445614099502563, Final Batch Loss: 1.0635106563568115\n",
      "Epoch 70, Loss: 2.1005845069885254, Final Batch Loss: 0.9934024810791016\n",
      "Epoch 71, Loss: 2.0634620785713196, Final Batch Loss: 1.091713547706604\n",
      "Epoch 72, Loss: 1.8575018644332886, Final Batch Loss: 0.8765445351600647\n",
      "Epoch 73, Loss: 1.868162453174591, Final Batch Loss: 0.9439247846603394\n",
      "Epoch 74, Loss: 1.938188374042511, Final Batch Loss: 0.9589900970458984\n",
      "Epoch 75, Loss: 1.8757411241531372, Final Batch Loss: 0.9410103559494019\n",
      "Epoch 76, Loss: 1.772568702697754, Final Batch Loss: 0.86021488904953\n",
      "Epoch 77, Loss: 1.6920034885406494, Final Batch Loss: 0.7593124508857727\n",
      "Epoch 78, Loss: 1.6583974957466125, Final Batch Loss: 0.7637312412261963\n",
      "Epoch 79, Loss: 1.595365047454834, Final Batch Loss: 0.8329247236251831\n",
      "Epoch 80, Loss: 1.7481836676597595, Final Batch Loss: 0.8763801455497742\n",
      "Epoch 81, Loss: 1.6359601020812988, Final Batch Loss: 0.7697805166244507\n",
      "Epoch 82, Loss: 1.6716801524162292, Final Batch Loss: 0.8559092879295349\n",
      "Epoch 83, Loss: 1.5582561492919922, Final Batch Loss: 0.7425119280815125\n",
      "Epoch 84, Loss: 1.5355122089385986, Final Batch Loss: 0.7738771438598633\n",
      "Epoch 85, Loss: 1.5140647888183594, Final Batch Loss: 0.7532209157943726\n",
      "Epoch 86, Loss: 1.5003201961517334, Final Batch Loss: 0.7445089817047119\n",
      "Epoch 87, Loss: 1.408061146736145, Final Batch Loss: 0.6716870665550232\n",
      "Epoch 88, Loss: 1.4722975492477417, Final Batch Loss: 0.7809788584709167\n",
      "Epoch 89, Loss: 1.457793951034546, Final Batch Loss: 0.7777647376060486\n",
      "Epoch 90, Loss: 1.4425801038742065, Final Batch Loss: 0.7102638483047485\n",
      "Epoch 91, Loss: 1.3541662693023682, Final Batch Loss: 0.6552168726921082\n",
      "Epoch 92, Loss: 1.3606313467025757, Final Batch Loss: 0.7150468230247498\n",
      "Epoch 93, Loss: 1.2836340069770813, Final Batch Loss: 0.6564967632293701\n",
      "Epoch 94, Loss: 1.3356966972351074, Final Batch Loss: 0.7035006284713745\n",
      "Epoch 95, Loss: 1.3304712176322937, Final Batch Loss: 0.7165164351463318\n",
      "Epoch 96, Loss: 1.3762399554252625, Final Batch Loss: 0.6682033538818359\n",
      "Epoch 97, Loss: 1.234836459159851, Final Batch Loss: 0.5782533884048462\n",
      "Epoch 98, Loss: 1.2420880794525146, Final Batch Loss: 0.6622018218040466\n",
      "Epoch 99, Loss: 1.27262943983078, Final Batch Loss: 0.6571246981620789\n",
      "Epoch 100, Loss: 1.2122973203659058, Final Batch Loss: 0.6323574185371399\n",
      "Epoch 101, Loss: 1.1826822757720947, Final Batch Loss: 0.6244997382164001\n",
      "Epoch 102, Loss: 1.2112855911254883, Final Batch Loss: 0.6352362632751465\n",
      "Epoch 103, Loss: 1.2254840731620789, Final Batch Loss: 0.6838374733924866\n",
      "Epoch 104, Loss: 1.0225465595722198, Final Batch Loss: 0.43668028712272644\n",
      "Epoch 105, Loss: 1.1482000946998596, Final Batch Loss: 0.49181073904037476\n",
      "Epoch 106, Loss: 1.1513044834136963, Final Batch Loss: 0.5474629998207092\n",
      "Epoch 107, Loss: 1.1563451290130615, Final Batch Loss: 0.5867578387260437\n",
      "Epoch 108, Loss: 1.157879650592804, Final Batch Loss: 0.5990997552871704\n",
      "Epoch 109, Loss: 1.142856776714325, Final Batch Loss: 0.5556520223617554\n",
      "Epoch 110, Loss: 1.1520448327064514, Final Batch Loss: 0.5943994522094727\n",
      "Epoch 111, Loss: 1.2382166981697083, Final Batch Loss: 0.5726531147956848\n",
      "Epoch 112, Loss: 1.1246054768562317, Final Batch Loss: 0.6036447286605835\n",
      "Epoch 113, Loss: 1.0571404695510864, Final Batch Loss: 0.5022774934768677\n",
      "Epoch 114, Loss: 0.9150000810623169, Final Batch Loss: 0.4321034848690033\n",
      "Epoch 115, Loss: 1.0581587553024292, Final Batch Loss: 0.519879162311554\n",
      "Epoch 116, Loss: 1.0284481942653656, Final Batch Loss: 0.4900458753108978\n",
      "Epoch 117, Loss: 1.09025239944458, Final Batch Loss: 0.5431364178657532\n",
      "Epoch 118, Loss: 1.0185660421848297, Final Batch Loss: 0.4713694751262665\n",
      "Epoch 119, Loss: 1.0841784477233887, Final Batch Loss: 0.6024133563041687\n",
      "Epoch 120, Loss: 0.9272293746471405, Final Batch Loss: 0.4672551453113556\n",
      "Epoch 121, Loss: 1.0241226553916931, Final Batch Loss: 0.5206403732299805\n",
      "Epoch 122, Loss: 0.9796322882175446, Final Batch Loss: 0.4583034813404083\n",
      "Epoch 123, Loss: 0.971002608537674, Final Batch Loss: 0.44795289635658264\n",
      "Epoch 124, Loss: 0.9109480381011963, Final Batch Loss: 0.46540677547454834\n",
      "Epoch 125, Loss: 1.011788159608841, Final Batch Loss: 0.5352594256401062\n",
      "Epoch 126, Loss: 0.8718422949314117, Final Batch Loss: 0.4523216187953949\n",
      "Epoch 127, Loss: 0.9004761576652527, Final Batch Loss: 0.4616142809391022\n",
      "Epoch 128, Loss: 0.9495386779308319, Final Batch Loss: 0.4736892282962799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: 0.9406656324863434, Final Batch Loss: 0.47210320830345154\n",
      "Epoch 130, Loss: 0.9619327783584595, Final Batch Loss: 0.46307358145713806\n",
      "Epoch 131, Loss: 0.8034489452838898, Final Batch Loss: 0.39077162742614746\n",
      "Epoch 132, Loss: 0.855337381362915, Final Batch Loss: 0.48018231987953186\n",
      "Epoch 133, Loss: 0.9213784635066986, Final Batch Loss: 0.4747239053249359\n",
      "Epoch 134, Loss: 0.8959013819694519, Final Batch Loss: 0.4161953628063202\n",
      "Epoch 135, Loss: 0.9485061168670654, Final Batch Loss: 0.4677566587924957\n",
      "Epoch 136, Loss: 0.8675930500030518, Final Batch Loss: 0.4345795512199402\n",
      "Epoch 137, Loss: 0.8804813027381897, Final Batch Loss: 0.43033894896507263\n",
      "Epoch 138, Loss: 0.8636233508586884, Final Batch Loss: 0.399203896522522\n",
      "Epoch 139, Loss: 0.8703757524490356, Final Batch Loss: 0.4119870066642761\n",
      "Epoch 140, Loss: 0.839065283536911, Final Batch Loss: 0.46316108107566833\n",
      "Epoch 141, Loss: 0.8761338293552399, Final Batch Loss: 0.40917378664016724\n",
      "Epoch 142, Loss: 0.8882165551185608, Final Batch Loss: 0.46500808000564575\n",
      "Epoch 143, Loss: 0.8058823347091675, Final Batch Loss: 0.39617475867271423\n",
      "Epoch 144, Loss: 0.814367949962616, Final Batch Loss: 0.4043866693973541\n",
      "Epoch 145, Loss: 0.8150242567062378, Final Batch Loss: 0.3881189823150635\n",
      "Epoch 146, Loss: 0.7871850430965424, Final Batch Loss: 0.41432133316993713\n",
      "Epoch 147, Loss: 0.8150285184383392, Final Batch Loss: 0.42663782835006714\n",
      "Epoch 148, Loss: 0.8440771996974945, Final Batch Loss: 0.4386039674282074\n",
      "Epoch 149, Loss: 0.8706307411193848, Final Batch Loss: 0.4267897307872772\n",
      "Epoch 150, Loss: 0.82866171002388, Final Batch Loss: 0.4395278990268707\n",
      "Epoch 151, Loss: 0.8718304336071014, Final Batch Loss: 0.46342742443084717\n",
      "Epoch 152, Loss: 0.7497456967830658, Final Batch Loss: 0.3973403573036194\n",
      "Epoch 153, Loss: 0.7627979815006256, Final Batch Loss: 0.3369317948818207\n",
      "Epoch 154, Loss: 0.6645033657550812, Final Batch Loss: 0.3412245810031891\n",
      "Epoch 155, Loss: 0.832935631275177, Final Batch Loss: 0.4059511423110962\n",
      "Epoch 156, Loss: 0.8095313012599945, Final Batch Loss: 0.40720662474632263\n",
      "Epoch 157, Loss: 0.7817969024181366, Final Batch Loss: 0.43271535634994507\n",
      "Epoch 158, Loss: 0.7452585995197296, Final Batch Loss: 0.38291051983833313\n",
      "Epoch 159, Loss: 0.7642970979213715, Final Batch Loss: 0.3787047863006592\n",
      "Epoch 160, Loss: 0.8140731155872345, Final Batch Loss: 0.42733773589134216\n",
      "Epoch 161, Loss: 0.7065511047840118, Final Batch Loss: 0.3264567255973816\n",
      "Epoch 162, Loss: 0.7294579148292542, Final Batch Loss: 0.33544665575027466\n",
      "Epoch 163, Loss: 0.7772302329540253, Final Batch Loss: 0.37241652607917786\n",
      "Epoch 164, Loss: 0.7870743870735168, Final Batch Loss: 0.3402489423751831\n",
      "Epoch 165, Loss: 0.7386233508586884, Final Batch Loss: 0.3900379538536072\n",
      "Epoch 166, Loss: 0.7227663695812225, Final Batch Loss: 0.351201593875885\n",
      "Epoch 167, Loss: 0.6307774782180786, Final Batch Loss: 0.30970051884651184\n",
      "Epoch 168, Loss: 0.7197977304458618, Final Batch Loss: 0.40360262989997864\n",
      "Epoch 169, Loss: 0.7511151731014252, Final Batch Loss: 0.38528454303741455\n",
      "Epoch 170, Loss: 0.7157983183860779, Final Batch Loss: 0.3152928352355957\n",
      "Epoch 171, Loss: 0.8214549124240875, Final Batch Loss: 0.4480828642845154\n",
      "Epoch 172, Loss: 0.6945378184318542, Final Batch Loss: 0.4082171320915222\n",
      "Epoch 173, Loss: 0.8116186857223511, Final Batch Loss: 0.40472620725631714\n",
      "Epoch 174, Loss: 0.7553925514221191, Final Batch Loss: 0.3820781707763672\n",
      "Epoch 175, Loss: 0.6603478491306305, Final Batch Loss: 0.31519967317581177\n",
      "Epoch 176, Loss: 0.7691623270511627, Final Batch Loss: 0.37223905324935913\n",
      "Epoch 177, Loss: 0.6203830540180206, Final Batch Loss: 0.31551986932754517\n",
      "Epoch 178, Loss: 0.7031197249889374, Final Batch Loss: 0.3655340075492859\n",
      "Epoch 179, Loss: 0.6957770586013794, Final Batch Loss: 0.3467552661895752\n",
      "Epoch 180, Loss: 0.6399922072887421, Final Batch Loss: 0.3251507878303528\n",
      "Epoch 181, Loss: 0.6352461874485016, Final Batch Loss: 0.3131568133831024\n",
      "Epoch 182, Loss: 0.6556145250797272, Final Batch Loss: 0.34116873145103455\n",
      "Epoch 183, Loss: 0.6295528709888458, Final Batch Loss: 0.27408546209335327\n",
      "Epoch 184, Loss: 0.6570281088352203, Final Batch Loss: 0.2995437681674957\n",
      "Epoch 185, Loss: 0.6884876787662506, Final Batch Loss: 0.33896347880363464\n",
      "Epoch 186, Loss: 0.6712247133255005, Final Batch Loss: 0.34119096398353577\n",
      "Epoch 187, Loss: 0.6353425979614258, Final Batch Loss: 0.37706002593040466\n",
      "Epoch 188, Loss: 0.619053453207016, Final Batch Loss: 0.3471037447452545\n",
      "Epoch 189, Loss: 0.5387389957904816, Final Batch Loss: 0.25053414702415466\n",
      "Epoch 190, Loss: 0.6502947211265564, Final Batch Loss: 0.30014005303382874\n",
      "Epoch 191, Loss: 0.6453771591186523, Final Batch Loss: 0.3548671305179596\n",
      "Epoch 192, Loss: 0.6438943147659302, Final Batch Loss: 0.309221088886261\n",
      "Epoch 193, Loss: 0.6311521828174591, Final Batch Loss: 0.3313162922859192\n",
      "Epoch 194, Loss: 0.6520664095878601, Final Batch Loss: 0.29650136828422546\n",
      "Epoch 195, Loss: 0.5538777709007263, Final Batch Loss: 0.292276531457901\n",
      "Epoch 196, Loss: 0.6779477894306183, Final Batch Loss: 0.35067516565322876\n",
      "Epoch 197, Loss: 0.6369679868221283, Final Batch Loss: 0.3369424045085907\n",
      "Epoch 198, Loss: 0.6218283474445343, Final Batch Loss: 0.306915819644928\n",
      "Epoch 199, Loss: 0.6696334183216095, Final Batch Loss: 0.38772648572921753\n",
      "Epoch 200, Loss: 0.6366427540779114, Final Batch Loss: 0.3554975688457489\n",
      "Epoch 201, Loss: 0.5694714188575745, Final Batch Loss: 0.2599116265773773\n",
      "Epoch 202, Loss: 0.5563079714775085, Final Batch Loss: 0.23412159085273743\n",
      "Epoch 203, Loss: 0.6590343117713928, Final Batch Loss: 0.3334599435329437\n",
      "Epoch 204, Loss: 0.49239425361156464, Final Batch Loss: 0.22662080824375153\n",
      "Epoch 205, Loss: 0.5464974045753479, Final Batch Loss: 0.2613012194633484\n",
      "Epoch 206, Loss: 0.6456860899925232, Final Batch Loss: 0.3335353136062622\n",
      "Epoch 207, Loss: 0.6501194536685944, Final Batch Loss: 0.2787815034389496\n",
      "Epoch 208, Loss: 0.5951981544494629, Final Batch Loss: 0.2785511016845703\n",
      "Epoch 209, Loss: 0.5629470944404602, Final Batch Loss: 0.2998102009296417\n",
      "Epoch 210, Loss: 0.7145180404186249, Final Batch Loss: 0.40999218821525574\n",
      "Epoch 211, Loss: 0.594266265630722, Final Batch Loss: 0.325458824634552\n",
      "Epoch 212, Loss: 0.565136581659317, Final Batch Loss: 0.2526788115501404\n",
      "Epoch 213, Loss: 0.6053057014942169, Final Batch Loss: 0.29852163791656494\n",
      "Epoch 214, Loss: 0.567240759730339, Final Batch Loss: 0.24589748680591583\n",
      "Epoch 215, Loss: 0.6544860601425171, Final Batch Loss: 0.36843180656433105\n",
      "Epoch 216, Loss: 0.6607418954372406, Final Batch Loss: 0.4076354503631592\n",
      "Epoch 217, Loss: 0.52386374771595, Final Batch Loss: 0.24550779163837433\n",
      "Epoch 218, Loss: 0.6612611711025238, Final Batch Loss: 0.35367831587791443\n",
      "Epoch 219, Loss: 0.5376221537590027, Final Batch Loss: 0.24805223941802979\n",
      "Epoch 220, Loss: 0.6182087063789368, Final Batch Loss: 0.30898424983024597\n",
      "Epoch 221, Loss: 0.5041262805461884, Final Batch Loss: 0.20291268825531006\n",
      "Epoch 222, Loss: 0.5779065191745758, Final Batch Loss: 0.27194544672966003\n",
      "Epoch 223, Loss: 0.5783570110797882, Final Batch Loss: 0.30305591225624084\n",
      "Epoch 224, Loss: 0.5005690306425095, Final Batch Loss: 0.23507897555828094\n",
      "Epoch 225, Loss: 0.6013553440570831, Final Batch Loss: 0.33202463388442993\n",
      "Epoch 226, Loss: 0.5411898195743561, Final Batch Loss: 0.250776082277298\n",
      "Epoch 227, Loss: 0.6703887581825256, Final Batch Loss: 0.3333427906036377\n",
      "Epoch 228, Loss: 0.6369108557701111, Final Batch Loss: 0.34122636914253235\n",
      "Epoch 229, Loss: 0.5371448546648026, Final Batch Loss: 0.2422068864107132\n",
      "Epoch 230, Loss: 0.5936360061168671, Final Batch Loss: 0.3290186822414398\n",
      "Epoch 231, Loss: 0.5664833784103394, Final Batch Loss: 0.3018943667411804\n",
      "Epoch 232, Loss: 0.5268653035163879, Final Batch Loss: 0.24970144033432007\n",
      "Epoch 233, Loss: 0.53769451379776, Final Batch Loss: 0.233197420835495\n",
      "Epoch 234, Loss: 0.5570606291294098, Final Batch Loss: 0.2864639461040497\n",
      "Epoch 235, Loss: 0.5108097344636917, Final Batch Loss: 0.23545463383197784\n",
      "Epoch 236, Loss: 0.5463431179523468, Final Batch Loss: 0.2195785641670227\n",
      "Epoch 237, Loss: 0.4685303866863251, Final Batch Loss: 0.22325308620929718\n",
      "Epoch 238, Loss: 0.6245209872722626, Final Batch Loss: 0.30406099557876587\n",
      "Epoch 239, Loss: 0.4834180623292923, Final Batch Loss: 0.21503864228725433\n",
      "Epoch 240, Loss: 0.553248405456543, Final Batch Loss: 0.29975447058677673\n",
      "Epoch 241, Loss: 0.618764191865921, Final Batch Loss: 0.3244417607784271\n",
      "Epoch 242, Loss: 0.6462286412715912, Final Batch Loss: 0.34642842411994934\n",
      "Epoch 243, Loss: 0.5903086364269257, Final Batch Loss: 0.2810349464416504\n",
      "Epoch 244, Loss: 0.6423622965812683, Final Batch Loss: 0.3839770555496216\n",
      "Epoch 245, Loss: 0.5129942148923874, Final Batch Loss: 0.23498483002185822\n",
      "Epoch 246, Loss: 0.4720178693532944, Final Batch Loss: 0.21273858845233917\n",
      "Epoch 247, Loss: 0.5273722112178802, Final Batch Loss: 0.26167818903923035\n",
      "Epoch 248, Loss: 0.5410265624523163, Final Batch Loss: 0.2627755105495453\n",
      "Epoch 249, Loss: 0.6780837774276733, Final Batch Loss: 0.32036811113357544\n",
      "Epoch 250, Loss: 0.6951476037502289, Final Batch Loss: 0.3891293406486511\n",
      "Epoch 251, Loss: 0.5515213012695312, Final Batch Loss: 0.2954908311367035\n",
      "Epoch 252, Loss: 0.6019356846809387, Final Batch Loss: 0.3309314548969269\n",
      "Epoch 253, Loss: 0.5582635402679443, Final Batch Loss: 0.22909802198410034\n",
      "Epoch 254, Loss: 0.5781742334365845, Final Batch Loss: 0.2742864787578583\n",
      "Epoch 255, Loss: 0.5350629091262817, Final Batch Loss: 0.2388058304786682\n",
      "Epoch 256, Loss: 0.46389544010162354, Final Batch Loss: 0.24872098863124847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257, Loss: 0.5272809863090515, Final Batch Loss: 0.2007618546485901\n",
      "Epoch 258, Loss: 0.5767721235752106, Final Batch Loss: 0.3136080205440521\n",
      "Epoch 259, Loss: 0.49096377193927765, Final Batch Loss: 0.25180861353874207\n",
      "Epoch 260, Loss: 0.43819141387939453, Final Batch Loss: 0.17485252022743225\n",
      "Epoch 261, Loss: 0.5044281482696533, Final Batch Loss: 0.3054131865501404\n",
      "Epoch 262, Loss: 0.5229319036006927, Final Batch Loss: 0.25037261843681335\n",
      "Epoch 263, Loss: 0.5012392699718475, Final Batch Loss: 0.23495802283287048\n",
      "Epoch 264, Loss: 0.5028478801250458, Final Batch Loss: 0.2439962923526764\n",
      "Epoch 265, Loss: 0.5042735487222672, Final Batch Loss: 0.21693600714206696\n",
      "Epoch 266, Loss: 0.5502733290195465, Final Batch Loss: 0.2817154824733734\n",
      "Epoch 267, Loss: 0.42216956615448, Final Batch Loss: 0.2214755415916443\n",
      "Epoch 268, Loss: 0.5340834558010101, Final Batch Loss: 0.28564921021461487\n",
      "Epoch 269, Loss: 0.5357418209314346, Final Batch Loss: 0.22642509639263153\n",
      "Epoch 270, Loss: 0.5730417370796204, Final Batch Loss: 0.3139685094356537\n",
      "Epoch 271, Loss: 0.5932213068008423, Final Batch Loss: 0.3435550034046173\n",
      "Epoch 272, Loss: 0.47823505103588104, Final Batch Loss: 0.2040768712759018\n",
      "Epoch 273, Loss: 0.4755677431821823, Final Batch Loss: 0.2638896107673645\n",
      "Epoch 274, Loss: 0.4819190800189972, Final Batch Loss: 0.19758430123329163\n",
      "Epoch 275, Loss: 0.5194145739078522, Final Batch Loss: 0.295264333486557\n",
      "Epoch 276, Loss: 0.48900793492794037, Final Batch Loss: 0.2608861029148102\n",
      "Epoch 277, Loss: 0.4946019947528839, Final Batch Loss: 0.21080437302589417\n",
      "Epoch 278, Loss: 0.4780663251876831, Final Batch Loss: 0.2802082300186157\n",
      "Epoch 279, Loss: 0.46567659080028534, Final Batch Loss: 0.2518787384033203\n",
      "Epoch 280, Loss: 0.41350141167640686, Final Batch Loss: 0.2338384985923767\n",
      "Epoch 281, Loss: 0.607848197221756, Final Batch Loss: 0.3120018541812897\n",
      "Epoch 282, Loss: 0.41918882727622986, Final Batch Loss: 0.21998049318790436\n",
      "Epoch 283, Loss: 0.5783985108137131, Final Batch Loss: 0.2420024424791336\n",
      "Epoch 284, Loss: 0.49013301730155945, Final Batch Loss: 0.2612414062023163\n",
      "Epoch 285, Loss: 0.42403121292591095, Final Batch Loss: 0.17752280831336975\n",
      "Epoch 286, Loss: 0.6020424067974091, Final Batch Loss: 0.32222262024879456\n",
      "Epoch 287, Loss: 0.486722394824028, Final Batch Loss: 0.24951589107513428\n",
      "Epoch 288, Loss: 0.44655391573905945, Final Batch Loss: 0.2359037846326828\n",
      "Epoch 289, Loss: 0.4729778915643692, Final Batch Loss: 0.22949828207492828\n",
      "Epoch 290, Loss: 0.4572545140981674, Final Batch Loss: 0.22504092752933502\n",
      "Epoch 291, Loss: 0.6549533009529114, Final Batch Loss: 0.34011998772621155\n",
      "Epoch 292, Loss: 0.5046886205673218, Final Batch Loss: 0.2865985929965973\n",
      "Epoch 293, Loss: 0.4271513521671295, Final Batch Loss: 0.2267562448978424\n",
      "Epoch 294, Loss: 0.45840637385845184, Final Batch Loss: 0.21945837140083313\n",
      "Epoch 295, Loss: 0.4315166026353836, Final Batch Loss: 0.23579952120780945\n",
      "Epoch 296, Loss: 0.4325481355190277, Final Batch Loss: 0.18807777762413025\n",
      "Epoch 297, Loss: 0.5421256124973297, Final Batch Loss: 0.2564697563648224\n",
      "Epoch 298, Loss: 0.5521581768989563, Final Batch Loss: 0.27947798371315\n",
      "Epoch 299, Loss: 0.5001438111066818, Final Batch Loss: 0.2488095611333847\n",
      "Epoch 300, Loss: 0.47068388760089874, Final Batch Loss: 0.2383766919374466\n",
      "Epoch 301, Loss: 0.42244288325309753, Final Batch Loss: 0.19378097355365753\n",
      "Epoch 302, Loss: 0.40923571586608887, Final Batch Loss: 0.1898886263370514\n",
      "Epoch 303, Loss: 0.45124194025993347, Final Batch Loss: 0.19890078902244568\n",
      "Epoch 304, Loss: 0.502554252743721, Final Batch Loss: 0.2270878404378891\n",
      "Epoch 305, Loss: 0.464085191488266, Final Batch Loss: 0.2767709493637085\n",
      "Epoch 306, Loss: 0.5037390291690826, Final Batch Loss: 0.2628864049911499\n",
      "Epoch 307, Loss: 0.4583277255296707, Final Batch Loss: 0.17965374886989594\n",
      "Epoch 308, Loss: 0.43360668420791626, Final Batch Loss: 0.17455220222473145\n",
      "Epoch 309, Loss: 0.39512164890766144, Final Batch Loss: 0.18112815916538239\n",
      "Epoch 310, Loss: 0.3156561702489853, Final Batch Loss: 0.12192988395690918\n",
      "Epoch 311, Loss: 0.33769941329956055, Final Batch Loss: 0.16597770154476166\n",
      "Epoch 312, Loss: 0.4451783746480942, Final Batch Loss: 0.22042205929756165\n",
      "Epoch 313, Loss: 0.40926672518253326, Final Batch Loss: 0.2058103233575821\n",
      "Epoch 314, Loss: 0.4018974006175995, Final Batch Loss: 0.19101350009441376\n",
      "Epoch 315, Loss: 0.33367517590522766, Final Batch Loss: 0.14908812940120697\n",
      "Epoch 316, Loss: 0.5144010037183762, Final Batch Loss: 0.31396475434303284\n",
      "Epoch 317, Loss: 0.44513265788555145, Final Batch Loss: 0.20808179676532745\n",
      "Epoch 318, Loss: 0.417439728975296, Final Batch Loss: 0.18992508947849274\n",
      "Epoch 319, Loss: 0.5152916312217712, Final Batch Loss: 0.2976360619068146\n",
      "Epoch 320, Loss: 0.48800037801265717, Final Batch Loss: 0.2946200370788574\n",
      "Epoch 321, Loss: 0.5256205648183823, Final Batch Loss: 0.19244073331356049\n",
      "Epoch 322, Loss: 0.3766377866268158, Final Batch Loss: 0.17192228138446808\n",
      "Epoch 323, Loss: 0.45243658125400543, Final Batch Loss: 0.23985837399959564\n",
      "Epoch 324, Loss: 0.5025491118431091, Final Batch Loss: 0.2865007519721985\n",
      "Epoch 325, Loss: 0.4635077565908432, Final Batch Loss: 0.15303806960582733\n",
      "Epoch 326, Loss: 0.5144234746694565, Final Batch Loss: 0.2913241982460022\n",
      "Epoch 327, Loss: 0.4481201767921448, Final Batch Loss: 0.23847810924053192\n",
      "Epoch 328, Loss: 0.46947522461414337, Final Batch Loss: 0.24341635406017303\n",
      "Epoch 329, Loss: 0.3962731510400772, Final Batch Loss: 0.18342795968055725\n",
      "Epoch 330, Loss: 0.51743583381176, Final Batch Loss: 0.28435224294662476\n",
      "Epoch 331, Loss: 0.4164167046546936, Final Batch Loss: 0.17981268465518951\n",
      "Epoch 332, Loss: 0.4334373027086258, Final Batch Loss: 0.18587489426136017\n",
      "Epoch 333, Loss: 0.4764208495616913, Final Batch Loss: 0.2673916220664978\n",
      "Epoch 334, Loss: 0.35016925632953644, Final Batch Loss: 0.17657127976417542\n",
      "Epoch 335, Loss: 0.416378378868103, Final Batch Loss: 0.2271716296672821\n",
      "Epoch 336, Loss: 0.44558240473270416, Final Batch Loss: 0.1801302284002304\n",
      "Epoch 337, Loss: 0.4574883282184601, Final Batch Loss: 0.23747798800468445\n",
      "Epoch 338, Loss: 0.4089759588241577, Final Batch Loss: 0.1959185004234314\n",
      "Epoch 339, Loss: 0.356394499540329, Final Batch Loss: 0.15752309560775757\n",
      "Epoch 340, Loss: 0.4299802631139755, Final Batch Loss: 0.2336413562297821\n",
      "Epoch 341, Loss: 0.4869624823331833, Final Batch Loss: 0.2779265344142914\n",
      "Epoch 342, Loss: 0.33563946187496185, Final Batch Loss: 0.17721803486347198\n",
      "Epoch 343, Loss: 0.46449407935142517, Final Batch Loss: 0.21284517645835876\n",
      "Epoch 344, Loss: 0.45181816816329956, Final Batch Loss: 0.21786029636859894\n",
      "Epoch 345, Loss: 0.4079318940639496, Final Batch Loss: 0.16732695698738098\n",
      "Epoch 346, Loss: 0.5113920420408249, Final Batch Loss: 0.2747329771518707\n",
      "Epoch 347, Loss: 0.3688544034957886, Final Batch Loss: 0.1988552212715149\n",
      "Epoch 348, Loss: 0.35779330134391785, Final Batch Loss: 0.15863767266273499\n",
      "Epoch 349, Loss: 0.3644692152738571, Final Batch Loss: 0.14362403750419617\n",
      "Epoch 350, Loss: 0.3607587665319443, Final Batch Loss: 0.17832998931407928\n",
      "Epoch 351, Loss: 0.4490716755390167, Final Batch Loss: 0.19540181756019592\n",
      "Epoch 352, Loss: 0.3964589834213257, Final Batch Loss: 0.19001515209674835\n",
      "Epoch 353, Loss: 0.36643266677856445, Final Batch Loss: 0.16494323313236237\n",
      "Epoch 354, Loss: 0.4585324674844742, Final Batch Loss: 0.2540258467197418\n",
      "Epoch 355, Loss: 0.4690765291452408, Final Batch Loss: 0.20712845027446747\n",
      "Epoch 356, Loss: 0.46231919527053833, Final Batch Loss: 0.26880234479904175\n",
      "Epoch 357, Loss: 0.3087126165628433, Final Batch Loss: 0.13221564888954163\n",
      "Epoch 358, Loss: 0.36632537841796875, Final Batch Loss: 0.16745047271251678\n",
      "Epoch 359, Loss: 0.35304179787635803, Final Batch Loss: 0.17077812552452087\n",
      "Epoch 360, Loss: 0.46512460708618164, Final Batch Loss: 0.24784111976623535\n",
      "Epoch 361, Loss: 0.37512800097465515, Final Batch Loss: 0.17941822111606598\n",
      "Epoch 362, Loss: 0.31649555265903473, Final Batch Loss: 0.15101030468940735\n",
      "Epoch 363, Loss: 0.4141341298818588, Final Batch Loss: 0.21839137375354767\n",
      "Epoch 364, Loss: 0.4526411443948746, Final Batch Loss: 0.2288169115781784\n",
      "Epoch 365, Loss: 0.367003858089447, Final Batch Loss: 0.1890542358160019\n",
      "Epoch 366, Loss: 0.4556272327899933, Final Batch Loss: 0.1960362195968628\n",
      "Epoch 367, Loss: 0.4326634407043457, Final Batch Loss: 0.20640631020069122\n",
      "Epoch 368, Loss: 0.2674398571252823, Final Batch Loss: 0.1364150494337082\n",
      "Epoch 369, Loss: 0.452133372426033, Final Batch Loss: 0.26261988282203674\n",
      "Epoch 370, Loss: 0.35246455669403076, Final Batch Loss: 0.16102170944213867\n",
      "Epoch 371, Loss: 0.33014678955078125, Final Batch Loss: 0.12618926167488098\n",
      "Epoch 372, Loss: 0.3452602028846741, Final Batch Loss: 0.1998961716890335\n",
      "Epoch 373, Loss: 0.3278578817844391, Final Batch Loss: 0.19289785623550415\n",
      "Epoch 374, Loss: 0.5611070990562439, Final Batch Loss: 0.43328556418418884\n",
      "Epoch 375, Loss: 0.35355161130428314, Final Batch Loss: 0.17230314016342163\n",
      "Epoch 376, Loss: 0.4075043052434921, Final Batch Loss: 0.20017915964126587\n",
      "Epoch 377, Loss: 0.40617384016513824, Final Batch Loss: 0.21420948207378387\n",
      "Epoch 378, Loss: 0.31006160378456116, Final Batch Loss: 0.18062064051628113\n",
      "Epoch 379, Loss: 0.4829597771167755, Final Batch Loss: 0.22035661339759827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380, Loss: 0.32109619677066803, Final Batch Loss: 0.13176365196704865\n",
      "Epoch 381, Loss: 0.45743606984615326, Final Batch Loss: 0.27321016788482666\n",
      "Epoch 382, Loss: 0.3984605520963669, Final Batch Loss: 0.20301203429698944\n",
      "Epoch 383, Loss: 0.403827428817749, Final Batch Loss: 0.21497811377048492\n",
      "Epoch 384, Loss: 0.39774374663829803, Final Batch Loss: 0.17108739912509918\n",
      "Epoch 385, Loss: 0.2928582429885864, Final Batch Loss: 0.1566718965768814\n",
      "Epoch 386, Loss: 0.33111472427845, Final Batch Loss: 0.1552625447511673\n",
      "Epoch 387, Loss: 0.4307364672422409, Final Batch Loss: 0.23544612526893616\n",
      "Epoch 388, Loss: 0.34092067182064056, Final Batch Loss: 0.11965347826480865\n",
      "Epoch 389, Loss: 0.40848036110401154, Final Batch Loss: 0.18975386023521423\n",
      "Epoch 390, Loss: 0.41464921832084656, Final Batch Loss: 0.188362255692482\n",
      "Epoch 391, Loss: 0.4190577566623688, Final Batch Loss: 0.20680972933769226\n",
      "Epoch 392, Loss: 0.2920179218053818, Final Batch Loss: 0.14895601570606232\n",
      "Epoch 393, Loss: 0.3912680447101593, Final Batch Loss: 0.14211609959602356\n",
      "Epoch 394, Loss: 0.378583699464798, Final Batch Loss: 0.15622344613075256\n",
      "Epoch 395, Loss: 0.39225536584854126, Final Batch Loss: 0.22399219870567322\n",
      "Epoch 396, Loss: 0.37591661512851715, Final Batch Loss: 0.20325343310832977\n",
      "Epoch 397, Loss: 0.42998673021793365, Final Batch Loss: 0.1699499636888504\n",
      "Epoch 398, Loss: 0.29124611616134644, Final Batch Loss: 0.15984462201595306\n",
      "Epoch 399, Loss: 0.36506514251232147, Final Batch Loss: 0.21974797546863556\n",
      "Epoch 400, Loss: 0.31945887207984924, Final Batch Loss: 0.13517464697360992\n",
      "Epoch 401, Loss: 0.3965282738208771, Final Batch Loss: 0.22374801337718964\n",
      "Epoch 402, Loss: 0.3612187057733536, Final Batch Loss: 0.13764171302318573\n",
      "Epoch 403, Loss: 0.42661252617836, Final Batch Loss: 0.20944727957248688\n",
      "Epoch 404, Loss: 0.5163403153419495, Final Batch Loss: 0.28686267137527466\n",
      "Epoch 405, Loss: 0.3177112340927124, Final Batch Loss: 0.14174500107765198\n",
      "Epoch 406, Loss: 0.2940591871738434, Final Batch Loss: 0.1314687281847\n",
      "Epoch 407, Loss: 0.34219224750995636, Final Batch Loss: 0.18734656274318695\n",
      "Epoch 408, Loss: 0.431106835603714, Final Batch Loss: 0.23209485411643982\n",
      "Epoch 409, Loss: 0.36494946479797363, Final Batch Loss: 0.17755171656608582\n",
      "Epoch 410, Loss: 0.28866132348775864, Final Batch Loss: 0.11804566532373428\n",
      "Epoch 411, Loss: 0.3552964776754379, Final Batch Loss: 0.18628856539726257\n",
      "Epoch 412, Loss: 0.33977170288562775, Final Batch Loss: 0.18319807946681976\n",
      "Epoch 413, Loss: 0.3020860105752945, Final Batch Loss: 0.13303253054618835\n",
      "Epoch 414, Loss: 0.39037391543388367, Final Batch Loss: 0.2722647488117218\n",
      "Epoch 415, Loss: 0.3552698791027069, Final Batch Loss: 0.11997242271900177\n",
      "Epoch 416, Loss: 0.40781985223293304, Final Batch Loss: 0.23853620886802673\n",
      "Epoch 417, Loss: 0.4383752644062042, Final Batch Loss: 0.2445257306098938\n",
      "Epoch 418, Loss: 0.31643858551979065, Final Batch Loss: 0.19208118319511414\n",
      "Epoch 419, Loss: 0.43300920724868774, Final Batch Loss: 0.21333187818527222\n",
      "Epoch 420, Loss: 0.34598153829574585, Final Batch Loss: 0.19143691658973694\n",
      "Epoch 421, Loss: 0.31326809525489807, Final Batch Loss: 0.1600368469953537\n",
      "Epoch 422, Loss: 0.31785042583942413, Final Batch Loss: 0.1443115919828415\n",
      "Epoch 423, Loss: 0.37118470668792725, Final Batch Loss: 0.19219796359539032\n",
      "Epoch 424, Loss: 0.4176763743162155, Final Batch Loss: 0.22169841825962067\n",
      "Epoch 425, Loss: 0.4481832981109619, Final Batch Loss: 0.22925952076911926\n",
      "Epoch 426, Loss: 0.38607899844646454, Final Batch Loss: 0.24126595258712769\n",
      "Epoch 427, Loss: 0.3188370019197464, Final Batch Loss: 0.1655089259147644\n",
      "Epoch 428, Loss: 0.4553098827600479, Final Batch Loss: 0.23063158988952637\n",
      "Epoch 429, Loss: 0.34738409519195557, Final Batch Loss: 0.20724733173847198\n",
      "Epoch 430, Loss: 0.3897612988948822, Final Batch Loss: 0.22892382740974426\n",
      "Epoch 431, Loss: 0.2862301468849182, Final Batch Loss: 0.1503554731607437\n",
      "Epoch 432, Loss: 0.3780764192342758, Final Batch Loss: 0.2167959362268448\n",
      "Epoch 433, Loss: 0.37737274169921875, Final Batch Loss: 0.22663946449756622\n",
      "Epoch 434, Loss: 0.3321505934000015, Final Batch Loss: 0.19673499464988708\n",
      "Epoch 435, Loss: 0.3449329435825348, Final Batch Loss: 0.17209835350513458\n",
      "Epoch 436, Loss: 0.31082165241241455, Final Batch Loss: 0.11631616950035095\n",
      "Epoch 437, Loss: 0.30967363715171814, Final Batch Loss: 0.17639541625976562\n",
      "Epoch 438, Loss: 0.34790897369384766, Final Batch Loss: 0.17099629342556\n",
      "Epoch 439, Loss: 0.3268785774707794, Final Batch Loss: 0.19951984286308289\n",
      "Epoch 440, Loss: 0.31267833709716797, Final Batch Loss: 0.17107559740543365\n",
      "Epoch 441, Loss: 0.35448548197746277, Final Batch Loss: 0.20260363817214966\n",
      "Epoch 442, Loss: 0.2745710164308548, Final Batch Loss: 0.139521524310112\n",
      "Epoch 443, Loss: 0.3599027544260025, Final Batch Loss: 0.1976185142993927\n",
      "Epoch 444, Loss: 0.2893747240304947, Final Batch Loss: 0.1547277271747589\n",
      "Epoch 445, Loss: 0.35966627299785614, Final Batch Loss: 0.22629687190055847\n",
      "Epoch 446, Loss: 0.34384772181510925, Final Batch Loss: 0.1275898665189743\n",
      "Epoch 447, Loss: 0.3037150949239731, Final Batch Loss: 0.15087705850601196\n",
      "Epoch 448, Loss: 0.3242879807949066, Final Batch Loss: 0.14833964407444\n",
      "Epoch 449, Loss: 0.3599786013364792, Final Batch Loss: 0.18810230493545532\n",
      "Epoch 450, Loss: 0.2880380004644394, Final Batch Loss: 0.1629646122455597\n",
      "Epoch 451, Loss: 0.37207670509815216, Final Batch Loss: 0.20465078949928284\n",
      "Epoch 452, Loss: 0.3158248960971832, Final Batch Loss: 0.131246417760849\n",
      "Epoch 453, Loss: 0.31292879581451416, Final Batch Loss: 0.15843501687049866\n",
      "Epoch 454, Loss: 0.4078855514526367, Final Batch Loss: 0.20960716903209686\n",
      "Epoch 455, Loss: 0.26236193627119064, Final Batch Loss: 0.12491763383150101\n",
      "Epoch 456, Loss: 0.2559962347149849, Final Batch Loss: 0.10647469013929367\n",
      "Epoch 457, Loss: 0.2547186464071274, Final Batch Loss: 0.1336718499660492\n",
      "Epoch 458, Loss: 0.30943824350833893, Final Batch Loss: 0.16762807965278625\n",
      "Epoch 459, Loss: 0.2655143290758133, Final Batch Loss: 0.13823212683200836\n",
      "Epoch 460, Loss: 0.237127847969532, Final Batch Loss: 0.09402082115411758\n",
      "Epoch 461, Loss: 0.3535684496164322, Final Batch Loss: 0.16325463354587555\n",
      "Epoch 462, Loss: 0.33723895251750946, Final Batch Loss: 0.1964818239212036\n",
      "Epoch 463, Loss: 0.37718120217323303, Final Batch Loss: 0.1983242928981781\n",
      "Epoch 464, Loss: 0.33857955038547516, Final Batch Loss: 0.21025700867176056\n",
      "Epoch 465, Loss: 0.32064808905124664, Final Batch Loss: 0.1456838697195053\n",
      "Epoch 466, Loss: 0.3752713203430176, Final Batch Loss: 0.14759458601474762\n",
      "Epoch 467, Loss: 0.3203149735927582, Final Batch Loss: 0.1415378749370575\n",
      "Epoch 468, Loss: 0.29398971796035767, Final Batch Loss: 0.1294858306646347\n",
      "Epoch 469, Loss: 0.4386216104030609, Final Batch Loss: 0.289715051651001\n",
      "Epoch 470, Loss: 0.2946735918521881, Final Batch Loss: 0.1322823017835617\n",
      "Epoch 471, Loss: 0.3270968645811081, Final Batch Loss: 0.1519998162984848\n",
      "Epoch 472, Loss: 0.2867797091603279, Final Batch Loss: 0.1192452535033226\n",
      "Epoch 473, Loss: 0.35198403894901276, Final Batch Loss: 0.1763220578432083\n",
      "Epoch 474, Loss: 0.2666775733232498, Final Batch Loss: 0.1366746425628662\n",
      "Epoch 475, Loss: 0.3628474622964859, Final Batch Loss: 0.1670275777578354\n",
      "Epoch 476, Loss: 0.3683987110853195, Final Batch Loss: 0.1735135167837143\n",
      "Epoch 477, Loss: 0.3151739090681076, Final Batch Loss: 0.16947299242019653\n",
      "Epoch 478, Loss: 0.30732204020023346, Final Batch Loss: 0.19578184187412262\n",
      "Epoch 479, Loss: 0.38970130681991577, Final Batch Loss: 0.1998853236436844\n",
      "Epoch 480, Loss: 0.2794477045536041, Final Batch Loss: 0.18220233917236328\n",
      "Epoch 481, Loss: 0.34016261994838715, Final Batch Loss: 0.1911909282207489\n",
      "Epoch 482, Loss: 0.32329829037189484, Final Batch Loss: 0.14992910623550415\n",
      "Epoch 483, Loss: 0.3550984114408493, Final Batch Loss: 0.20838139951229095\n",
      "Epoch 484, Loss: 0.2800798863172531, Final Batch Loss: 0.14381445944309235\n",
      "Epoch 485, Loss: 0.2862267568707466, Final Batch Loss: 0.1833103597164154\n",
      "Epoch 486, Loss: 0.37156397104263306, Final Batch Loss: 0.16519595682621002\n",
      "Epoch 487, Loss: 0.47725728154182434, Final Batch Loss: 0.25143295526504517\n",
      "Epoch 488, Loss: 0.27763181924819946, Final Batch Loss: 0.13479898869991302\n",
      "Epoch 489, Loss: 0.2601950913667679, Final Batch Loss: 0.13415129482746124\n",
      "Epoch 490, Loss: 0.38597145676612854, Final Batch Loss: 0.1917465180158615\n",
      "Epoch 491, Loss: 0.32475607097148895, Final Batch Loss: 0.149860218167305\n",
      "Epoch 492, Loss: 0.2433638796210289, Final Batch Loss: 0.09636195749044418\n",
      "Epoch 493, Loss: 0.33327600359916687, Final Batch Loss: 0.15938697755336761\n",
      "Epoch 494, Loss: 0.32847197353839874, Final Batch Loss: 0.16023871302604675\n",
      "Epoch 495, Loss: 0.3106362223625183, Final Batch Loss: 0.18459388613700867\n",
      "Epoch 496, Loss: 0.256859727203846, Final Batch Loss: 0.1482449322938919\n",
      "Epoch 497, Loss: 0.29367774724960327, Final Batch Loss: 0.16446223855018616\n",
      "Epoch 498, Loss: 0.30774448812007904, Final Batch Loss: 0.20236165821552277\n",
      "Epoch 499, Loss: 0.21045001596212387, Final Batch Loss: 0.08937019854784012\n",
      "Epoch 500, Loss: 0.2597973495721817, Final Batch Loss: 0.1279563456773758\n",
      "Epoch 501, Loss: 0.3100978434085846, Final Batch Loss: 0.14528995752334595\n",
      "Epoch 502, Loss: 0.18587133288383484, Final Batch Loss: 0.09403595328330994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 503, Loss: 0.3246203362941742, Final Batch Loss: 0.16798636317253113\n",
      "Epoch 504, Loss: 0.25662916898727417, Final Batch Loss: 0.10334569215774536\n",
      "Epoch 505, Loss: 0.269232913851738, Final Batch Loss: 0.07675023376941681\n",
      "Epoch 506, Loss: 0.3106527030467987, Final Batch Loss: 0.1423923671245575\n",
      "Epoch 507, Loss: 0.23076261579990387, Final Batch Loss: 0.10051171481609344\n",
      "Epoch 508, Loss: 0.35851408541202545, Final Batch Loss: 0.19241811335086823\n",
      "Epoch 509, Loss: 0.24164288491010666, Final Batch Loss: 0.11194343119859695\n",
      "Epoch 510, Loss: 0.23790276795625687, Final Batch Loss: 0.13593730330467224\n",
      "Epoch 511, Loss: 0.28615058958530426, Final Batch Loss: 0.1218937337398529\n",
      "Epoch 512, Loss: 0.29719504714012146, Final Batch Loss: 0.15060901641845703\n",
      "Epoch 513, Loss: 0.24665574729442596, Final Batch Loss: 0.13634775578975677\n",
      "Epoch 514, Loss: 0.3123062551021576, Final Batch Loss: 0.13279969990253448\n",
      "Epoch 515, Loss: 0.34186458587646484, Final Batch Loss: 0.1649915724992752\n",
      "Epoch 516, Loss: 0.27224667370319366, Final Batch Loss: 0.15331825613975525\n",
      "Epoch 517, Loss: 0.26794254034757614, Final Batch Loss: 0.12425699084997177\n",
      "Epoch 518, Loss: 0.32775984704494476, Final Batch Loss: 0.18484261631965637\n",
      "Epoch 519, Loss: 0.22501485794782639, Final Batch Loss: 0.12013337761163712\n",
      "Epoch 520, Loss: 0.3412502110004425, Final Batch Loss: 0.19237303733825684\n",
      "Epoch 521, Loss: 0.29041914641857147, Final Batch Loss: 0.1576395034790039\n",
      "Epoch 522, Loss: 0.22354202717542648, Final Batch Loss: 0.12231406569480896\n",
      "Epoch 523, Loss: 0.25553036481142044, Final Batch Loss: 0.10899590700864792\n",
      "Epoch 524, Loss: 0.3349996358156204, Final Batch Loss: 0.15461380779743195\n",
      "Epoch 525, Loss: 0.26421181857585907, Final Batch Loss: 0.10620541870594025\n",
      "Epoch 526, Loss: 0.3020114153623581, Final Batch Loss: 0.16237278282642365\n",
      "Epoch 527, Loss: 0.2635861784219742, Final Batch Loss: 0.13078242540359497\n",
      "Epoch 528, Loss: 0.21698829531669617, Final Batch Loss: 0.1289510875940323\n",
      "Epoch 529, Loss: 0.32634909451007843, Final Batch Loss: 0.1688421219587326\n",
      "Epoch 530, Loss: 0.29392144083976746, Final Batch Loss: 0.17909187078475952\n",
      "Epoch 531, Loss: 0.26469872891902924, Final Batch Loss: 0.16223277151584625\n",
      "Epoch 532, Loss: 0.25357460230588913, Final Batch Loss: 0.12943458557128906\n",
      "Epoch 533, Loss: 0.28663134574890137, Final Batch Loss: 0.14021195471286774\n",
      "Epoch 534, Loss: 0.26249081641435623, Final Batch Loss: 0.10651161521673203\n",
      "Epoch 535, Loss: 0.2608235031366348, Final Batch Loss: 0.07182574272155762\n",
      "Epoch 536, Loss: 0.2090788334608078, Final Batch Loss: 0.0961347371339798\n",
      "Epoch 537, Loss: 0.24853556603193283, Final Batch Loss: 0.10120133310556412\n",
      "Epoch 538, Loss: 0.2474488839507103, Final Batch Loss: 0.10508991032838821\n",
      "Epoch 539, Loss: 0.29385917633771896, Final Batch Loss: 0.1812748908996582\n",
      "Epoch 540, Loss: 0.30944038927555084, Final Batch Loss: 0.1458916962146759\n",
      "Epoch 541, Loss: 0.32726994156837463, Final Batch Loss: 0.13838700950145721\n",
      "Epoch 542, Loss: 0.303410179913044, Final Batch Loss: 0.20144663751125336\n",
      "Epoch 543, Loss: 0.32717978954315186, Final Batch Loss: 0.1590302288532257\n",
      "Epoch 544, Loss: 0.24445171654224396, Final Batch Loss: 0.0918673425912857\n",
      "Epoch 545, Loss: 0.23773193359375, Final Batch Loss: 0.08915285766124725\n",
      "Epoch 546, Loss: 0.2643633037805557, Final Batch Loss: 0.1300947666168213\n",
      "Epoch 547, Loss: 0.26194122433662415, Final Batch Loss: 0.09122830629348755\n",
      "Epoch 548, Loss: 0.2778528481721878, Final Batch Loss: 0.17847593128681183\n",
      "Epoch 549, Loss: 0.25934912264347076, Final Batch Loss: 0.1344478577375412\n",
      "Epoch 550, Loss: 0.3150913864374161, Final Batch Loss: 0.125065416097641\n",
      "Epoch 551, Loss: 0.25920768082141876, Final Batch Loss: 0.07574635744094849\n",
      "Epoch 552, Loss: 0.3334052562713623, Final Batch Loss: 0.13851924240589142\n",
      "Epoch 553, Loss: 0.3058450073003769, Final Batch Loss: 0.13156276941299438\n",
      "Epoch 554, Loss: 0.3669776916503906, Final Batch Loss: 0.14965230226516724\n",
      "Epoch 555, Loss: 0.2757084369659424, Final Batch Loss: 0.09224896132946014\n",
      "Epoch 556, Loss: 0.3628763109445572, Final Batch Loss: 0.23149438202381134\n",
      "Epoch 557, Loss: 0.2688703089952469, Final Batch Loss: 0.10676145553588867\n",
      "Epoch 558, Loss: 0.2797582373023033, Final Batch Loss: 0.09251377731561661\n",
      "Epoch 559, Loss: 0.3290116488933563, Final Batch Loss: 0.17698714137077332\n",
      "Epoch 560, Loss: 0.38261619210243225, Final Batch Loss: 0.20223373174667358\n",
      "Epoch 561, Loss: 0.2318791002035141, Final Batch Loss: 0.08755046129226685\n",
      "Epoch 562, Loss: 0.20100592821836472, Final Batch Loss: 0.08490714430809021\n",
      "Epoch 563, Loss: 0.2300056591629982, Final Batch Loss: 0.1430523693561554\n",
      "Epoch 564, Loss: 0.3230057954788208, Final Batch Loss: 0.19162850081920624\n",
      "Epoch 565, Loss: 0.32810816168785095, Final Batch Loss: 0.17078107595443726\n",
      "Epoch 566, Loss: 0.23530516773462296, Final Batch Loss: 0.13506251573562622\n",
      "Epoch 567, Loss: 0.2956945449113846, Final Batch Loss: 0.13612250983715057\n",
      "Epoch 568, Loss: 0.29739299416542053, Final Batch Loss: 0.14847403764724731\n",
      "Epoch 569, Loss: 0.2736861854791641, Final Batch Loss: 0.14368148148059845\n",
      "Epoch 570, Loss: 0.3122507929801941, Final Batch Loss: 0.19329412281513214\n",
      "Epoch 571, Loss: 0.3428075313568115, Final Batch Loss: 0.18530258536338806\n",
      "Epoch 572, Loss: 0.2743726670742035, Final Batch Loss: 0.1795281320810318\n",
      "Epoch 573, Loss: 0.2772996872663498, Final Batch Loss: 0.13606864213943481\n",
      "Epoch 574, Loss: 0.17890597879886627, Final Batch Loss: 0.0905788466334343\n",
      "Epoch 575, Loss: 0.2460012584924698, Final Batch Loss: 0.11990225315093994\n",
      "Epoch 576, Loss: 0.23560305684804916, Final Batch Loss: 0.13207902014255524\n",
      "Epoch 577, Loss: 0.2512979209423065, Final Batch Loss: 0.1272183358669281\n",
      "Epoch 578, Loss: 0.2834988683462143, Final Batch Loss: 0.13060462474822998\n",
      "Epoch 579, Loss: 0.2677602171897888, Final Batch Loss: 0.13387344777584076\n",
      "Epoch 580, Loss: 0.2608073726296425, Final Batch Loss: 0.14249657094478607\n",
      "Epoch 581, Loss: 0.2518588602542877, Final Batch Loss: 0.12581956386566162\n",
      "Epoch 582, Loss: 0.32935841381549835, Final Batch Loss: 0.20567861199378967\n",
      "Epoch 583, Loss: 0.31216952204704285, Final Batch Loss: 0.14856091141700745\n",
      "Epoch 584, Loss: 0.2818205878138542, Final Batch Loss: 0.11437659710645676\n",
      "Epoch 585, Loss: 0.24542789906263351, Final Batch Loss: 0.09729126840829849\n",
      "Epoch 586, Loss: 0.2697853669524193, Final Batch Loss: 0.1624356359243393\n",
      "Epoch 587, Loss: 0.2801871746778488, Final Batch Loss: 0.15563809871673584\n",
      "Epoch 588, Loss: 0.2628355324268341, Final Batch Loss: 0.11432458460330963\n",
      "Epoch 589, Loss: 0.2426324486732483, Final Batch Loss: 0.14734159409999847\n",
      "Epoch 590, Loss: 0.20730048418045044, Final Batch Loss: 0.09363032877445221\n",
      "Epoch 591, Loss: 0.19514724612236023, Final Batch Loss: 0.08596976101398468\n",
      "Epoch 592, Loss: 0.25292059779167175, Final Batch Loss: 0.10423356294631958\n",
      "Epoch 593, Loss: 0.2247963324189186, Final Batch Loss: 0.13009189069271088\n",
      "Epoch 594, Loss: 0.25147972255945206, Final Batch Loss: 0.13429982960224152\n",
      "Epoch 595, Loss: 0.2819538563489914, Final Batch Loss: 0.15495944023132324\n",
      "Epoch 596, Loss: 0.21820960193872452, Final Batch Loss: 0.0877259150147438\n",
      "Epoch 597, Loss: 0.30546151101589203, Final Batch Loss: 0.1539844125509262\n",
      "Epoch 598, Loss: 0.28265319764614105, Final Batch Loss: 0.1458251029253006\n",
      "Epoch 599, Loss: 0.2236664667725563, Final Batch Loss: 0.11714014410972595\n",
      "Epoch 600, Loss: 0.23624099045991898, Final Batch Loss: 0.10680628567934036\n",
      "Epoch 601, Loss: 0.2500799894332886, Final Batch Loss: 0.16470111906528473\n",
      "Epoch 602, Loss: 0.24329019337892532, Final Batch Loss: 0.10501077026128769\n",
      "Epoch 603, Loss: 0.27500998228788376, Final Batch Loss: 0.15026210248470306\n",
      "Epoch 604, Loss: 0.25976186990737915, Final Batch Loss: 0.10991941392421722\n",
      "Epoch 605, Loss: 0.26321015506982803, Final Batch Loss: 0.11243294924497604\n",
      "Epoch 606, Loss: 0.2657860144972801, Final Batch Loss: 0.1137370839715004\n",
      "Epoch 607, Loss: 0.2461620345711708, Final Batch Loss: 0.1575465351343155\n",
      "Epoch 608, Loss: 0.2911625802516937, Final Batch Loss: 0.12660616636276245\n",
      "Epoch 609, Loss: 0.2844931334257126, Final Batch Loss: 0.15131253004074097\n",
      "Epoch 610, Loss: 0.2748326137661934, Final Batch Loss: 0.15496106445789337\n",
      "Epoch 611, Loss: 0.26047980040311813, Final Batch Loss: 0.15511032938957214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 612, Loss: 0.3000098615884781, Final Batch Loss: 0.19491776823997498\n",
      "Epoch 613, Loss: 0.3223038464784622, Final Batch Loss: 0.1763344407081604\n",
      "Epoch 614, Loss: 0.33694417774677277, Final Batch Loss: 0.16718555986881256\n",
      "Epoch 615, Loss: 0.23927585035562515, Final Batch Loss: 0.09978180378675461\n",
      "Epoch 616, Loss: 0.27215033769607544, Final Batch Loss: 0.11899334192276001\n",
      "Epoch 617, Loss: 0.2538839876651764, Final Batch Loss: 0.10134707391262054\n",
      "Epoch 618, Loss: 0.1809595823287964, Final Batch Loss: 0.09631989896297455\n",
      "Epoch 619, Loss: 0.29099611937999725, Final Batch Loss: 0.1769281029701233\n",
      "Epoch 620, Loss: 0.3099381625652313, Final Batch Loss: 0.19716744124889374\n",
      "Epoch 621, Loss: 0.3132133334875107, Final Batch Loss: 0.13129985332489014\n",
      "Epoch 622, Loss: 0.26257388293743134, Final Batch Loss: 0.12594369053840637\n",
      "Epoch 623, Loss: 0.26355695724487305, Final Batch Loss: 0.13875262439250946\n",
      "Epoch 624, Loss: 0.27228448539972305, Final Batch Loss: 0.1525144875049591\n",
      "Epoch 625, Loss: 0.2865303009748459, Final Batch Loss: 0.16939426958560944\n",
      "Epoch 626, Loss: 0.25478582084178925, Final Batch Loss: 0.12525926530361176\n",
      "Epoch 627, Loss: 0.2763773947954178, Final Batch Loss: 0.16777172684669495\n",
      "Epoch 628, Loss: 0.29410144686698914, Final Batch Loss: 0.1602744609117508\n",
      "Epoch 629, Loss: 0.22654403746128082, Final Batch Loss: 0.12741060554981232\n",
      "Epoch 630, Loss: 0.21953441947698593, Final Batch Loss: 0.10061302036046982\n",
      "Epoch 631, Loss: 0.26598403602838516, Final Batch Loss: 0.1436576396226883\n",
      "Epoch 632, Loss: 0.28236123919487, Final Batch Loss: 0.12094248831272125\n",
      "Epoch 633, Loss: 0.2578194737434387, Final Batch Loss: 0.1319534331560135\n",
      "Epoch 634, Loss: 0.20591292530298233, Final Batch Loss: 0.09577328711748123\n",
      "Epoch 635, Loss: 0.351834774017334, Final Batch Loss: 0.1841549277305603\n",
      "Epoch 636, Loss: 0.24536168575286865, Final Batch Loss: 0.08945934474468231\n",
      "Epoch 637, Loss: 0.3139849305152893, Final Batch Loss: 0.1735507845878601\n",
      "Epoch 638, Loss: 0.25122352689504623, Final Batch Loss: 0.12449900060892105\n",
      "Epoch 639, Loss: 0.22577416896820068, Final Batch Loss: 0.08609852194786072\n",
      "Epoch 640, Loss: 0.25008291751146317, Final Batch Loss: 0.12990164756774902\n",
      "Epoch 641, Loss: 0.23640741407871246, Final Batch Loss: 0.08285254240036011\n",
      "Epoch 642, Loss: 0.30343152582645416, Final Batch Loss: 0.13657689094543457\n",
      "Epoch 643, Loss: 0.22076432406902313, Final Batch Loss: 0.12920600175857544\n",
      "Epoch 644, Loss: 0.3280276283621788, Final Batch Loss: 0.09703225642442703\n",
      "Epoch 645, Loss: 0.245721235871315, Final Batch Loss: 0.13821139931678772\n",
      "Epoch 646, Loss: 0.24688635021448135, Final Batch Loss: 0.1457252949476242\n",
      "Epoch 647, Loss: 0.266777865588665, Final Batch Loss: 0.1604197770357132\n",
      "Epoch 648, Loss: 0.2502068430185318, Final Batch Loss: 0.15546944737434387\n",
      "Epoch 649, Loss: 0.256444588303566, Final Batch Loss: 0.11657637357711792\n",
      "Epoch 650, Loss: 0.280314564704895, Final Batch Loss: 0.12502311170101166\n",
      "Epoch 651, Loss: 0.3230748921632767, Final Batch Loss: 0.18763567507266998\n",
      "Epoch 652, Loss: 0.2822269797325134, Final Batch Loss: 0.08462700247764587\n",
      "Epoch 653, Loss: 0.24700402468442917, Final Batch Loss: 0.137040376663208\n",
      "Epoch 654, Loss: 0.3121398240327835, Final Batch Loss: 0.1864272505044937\n",
      "Epoch 655, Loss: 0.17405438423156738, Final Batch Loss: 0.08566619455814362\n",
      "Epoch 656, Loss: 0.3381742089986801, Final Batch Loss: 0.18259797990322113\n",
      "Epoch 657, Loss: 0.2474180907011032, Final Batch Loss: 0.11215199530124664\n",
      "Epoch 658, Loss: 0.2477107048034668, Final Batch Loss: 0.09404376149177551\n",
      "Epoch 659, Loss: 0.2080928012728691, Final Batch Loss: 0.09099932760000229\n",
      "Epoch 660, Loss: 0.21426963061094284, Final Batch Loss: 0.11147806793451309\n",
      "Epoch 661, Loss: 0.251261368393898, Final Batch Loss: 0.1142549067735672\n",
      "Epoch 662, Loss: 0.2650521695613861, Final Batch Loss: 0.13687095046043396\n",
      "Epoch 663, Loss: 0.20577948540449142, Final Batch Loss: 0.080810546875\n",
      "Epoch 664, Loss: 0.19643782079219818, Final Batch Loss: 0.09013973921537399\n",
      "Epoch 665, Loss: 0.19328093528747559, Final Batch Loss: 0.09120398014783859\n",
      "Epoch 666, Loss: 0.1875520497560501, Final Batch Loss: 0.10256446152925491\n",
      "Epoch 667, Loss: 0.2515665963292122, Final Batch Loss: 0.12661653757095337\n",
      "Epoch 668, Loss: 0.17773404717445374, Final Batch Loss: 0.08405934274196625\n",
      "Epoch 669, Loss: 0.3178614526987076, Final Batch Loss: 0.14925497770309448\n",
      "Epoch 670, Loss: 0.20919016748666763, Final Batch Loss: 0.09645025432109833\n",
      "Epoch 671, Loss: 0.28619294613599777, Final Batch Loss: 0.17453506588935852\n",
      "Epoch 672, Loss: 0.24098002165555954, Final Batch Loss: 0.1069694235920906\n",
      "Epoch 673, Loss: 0.180786594748497, Final Batch Loss: 0.08985617756843567\n",
      "Epoch 674, Loss: 0.2608114033937454, Final Batch Loss: 0.1265842616558075\n",
      "Epoch 675, Loss: 0.18098986148834229, Final Batch Loss: 0.10661502927541733\n",
      "Epoch 676, Loss: 0.2808675691485405, Final Batch Loss: 0.11592485755681992\n",
      "Epoch 677, Loss: 0.24436338245868683, Final Batch Loss: 0.1283327043056488\n",
      "Epoch 678, Loss: 0.19626478105783463, Final Batch Loss: 0.10108975321054459\n",
      "Epoch 679, Loss: 0.18007344752550125, Final Batch Loss: 0.10976731777191162\n",
      "Epoch 680, Loss: 0.26389332115650177, Final Batch Loss: 0.13442647457122803\n",
      "Epoch 681, Loss: 0.24093474447727203, Final Batch Loss: 0.10409986972808838\n",
      "Epoch 682, Loss: 0.21648234128952026, Final Batch Loss: 0.10152389109134674\n",
      "Epoch 683, Loss: 0.27111677825450897, Final Batch Loss: 0.14672613143920898\n",
      "Epoch 684, Loss: 0.26702648401260376, Final Batch Loss: 0.14153623580932617\n",
      "Epoch 685, Loss: 0.2218051701784134, Final Batch Loss: 0.07761082053184509\n",
      "Epoch 686, Loss: 0.24301792681217194, Final Batch Loss: 0.1468668282032013\n",
      "Epoch 687, Loss: 0.2998342365026474, Final Batch Loss: 0.1272098571062088\n",
      "Epoch 688, Loss: 0.19923733919858932, Final Batch Loss: 0.12289215624332428\n",
      "Epoch 689, Loss: 0.22180664539337158, Final Batch Loss: 0.11474061757326126\n",
      "Epoch 690, Loss: 0.21181367337703705, Final Batch Loss: 0.09035909175872803\n",
      "Epoch 691, Loss: 0.25595998018980026, Final Batch Loss: 0.08580809086561203\n",
      "Epoch 692, Loss: 0.21021269261837006, Final Batch Loss: 0.11601313203573227\n",
      "Epoch 693, Loss: 0.21544549614191055, Final Batch Loss: 0.06774815171957016\n",
      "Epoch 694, Loss: 0.2578906714916229, Final Batch Loss: 0.15679390728473663\n",
      "Epoch 695, Loss: 0.20131434500217438, Final Batch Loss: 0.09530506283044815\n",
      "Epoch 696, Loss: 0.2753581032156944, Final Batch Loss: 0.15260261297225952\n",
      "Epoch 697, Loss: 0.20213770121335983, Final Batch Loss: 0.12960167229175568\n",
      "Epoch 698, Loss: 0.28615161776542664, Final Batch Loss: 0.14326666295528412\n",
      "Epoch 699, Loss: 0.24498681724071503, Final Batch Loss: 0.11581893265247345\n",
      "Epoch 700, Loss: 0.22387368232011795, Final Batch Loss: 0.1109025776386261\n",
      "Epoch 701, Loss: 0.25076260417699814, Final Batch Loss: 0.1562134027481079\n",
      "Epoch 702, Loss: 0.261039637029171, Final Batch Loss: 0.16060763597488403\n",
      "Epoch 703, Loss: 0.20906206965446472, Final Batch Loss: 0.11433009803295135\n",
      "Epoch 704, Loss: 0.22496506571769714, Final Batch Loss: 0.12548606097698212\n",
      "Epoch 705, Loss: 0.1916578859090805, Final Batch Loss: 0.09052083641290665\n",
      "Epoch 706, Loss: 0.3068068251013756, Final Batch Loss: 0.20043142139911652\n",
      "Epoch 707, Loss: 0.20015878230333328, Final Batch Loss: 0.09266655147075653\n",
      "Epoch 708, Loss: 0.26929645240306854, Final Batch Loss: 0.11079147458076477\n",
      "Epoch 709, Loss: 0.1868174448609352, Final Batch Loss: 0.08443326503038406\n",
      "Epoch 710, Loss: 0.2218613177537918, Final Batch Loss: 0.09605757892131805\n",
      "Epoch 711, Loss: 0.245916910469532, Final Batch Loss: 0.08649950474500656\n",
      "Epoch 712, Loss: 0.1856667473912239, Final Batch Loss: 0.07238472998142242\n",
      "Epoch 713, Loss: 0.3111112117767334, Final Batch Loss: 0.16152337193489075\n",
      "Epoch 714, Loss: 0.2122790887951851, Final Batch Loss: 0.11785367131233215\n",
      "Epoch 715, Loss: 0.2903736084699631, Final Batch Loss: 0.1475788950920105\n",
      "Epoch 716, Loss: 0.12791582942008972, Final Batch Loss: 0.06108485162258148\n",
      "Epoch 717, Loss: 0.2366083264350891, Final Batch Loss: 0.08008281886577606\n",
      "Epoch 718, Loss: 0.2105005495250225, Final Batch Loss: 0.058198701590299606\n",
      "Epoch 719, Loss: 0.27745313942432404, Final Batch Loss: 0.12989622354507446\n",
      "Epoch 720, Loss: 0.2202555164694786, Final Batch Loss: 0.11190330237150192\n",
      "Epoch 721, Loss: 0.21912255138158798, Final Batch Loss: 0.07626474648714066\n",
      "Epoch 722, Loss: 0.24420907348394394, Final Batch Loss: 0.0995994433760643\n",
      "Epoch 723, Loss: 0.287291519343853, Final Batch Loss: 0.16901473701000214\n",
      "Epoch 724, Loss: 0.20865152776241302, Final Batch Loss: 0.07980956137180328\n",
      "Epoch 725, Loss: 0.3037073165178299, Final Batch Loss: 0.1327778846025467\n",
      "Epoch 726, Loss: 0.22909118980169296, Final Batch Loss: 0.10791254788637161\n",
      "Epoch 727, Loss: 0.2345220372080803, Final Batch Loss: 0.1267269402742386\n",
      "Epoch 728, Loss: 0.19015345722436905, Final Batch Loss: 0.09389154613018036\n",
      "Epoch 729, Loss: 0.16320478543639183, Final Batch Loss: 0.061561550945043564\n",
      "Epoch 730, Loss: 0.21967028081417084, Final Batch Loss: 0.11446709930896759\n",
      "Epoch 731, Loss: 0.23565667867660522, Final Batch Loss: 0.1236284077167511\n",
      "Epoch 732, Loss: 0.19642553478479385, Final Batch Loss: 0.12447815388441086\n",
      "Epoch 733, Loss: 0.30393360555171967, Final Batch Loss: 0.1814572513103485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 734, Loss: 0.17325442284345627, Final Batch Loss: 0.07976745814085007\n",
      "Epoch 735, Loss: 0.29876792430877686, Final Batch Loss: 0.13629664480686188\n",
      "Epoch 736, Loss: 0.2126474529504776, Final Batch Loss: 0.09138350933790207\n",
      "Epoch 737, Loss: 0.2719568684697151, Final Batch Loss: 0.155488520860672\n",
      "Epoch 738, Loss: 0.2490907832980156, Final Batch Loss: 0.1188972219824791\n",
      "Epoch 739, Loss: 0.24484989047050476, Final Batch Loss: 0.12443292140960693\n",
      "Epoch 740, Loss: 0.17040615901350975, Final Batch Loss: 0.04829425737261772\n",
      "Epoch 741, Loss: 0.322550505399704, Final Batch Loss: 0.14847081899642944\n",
      "Epoch 742, Loss: 0.2004633992910385, Final Batch Loss: 0.09516096860170364\n",
      "Epoch 743, Loss: 0.19621700048446655, Final Batch Loss: 0.09235424548387527\n",
      "Epoch 744, Loss: 0.26345761865377426, Final Batch Loss: 0.09589795023202896\n",
      "Epoch 745, Loss: 0.1876908764243126, Final Batch Loss: 0.1228804811835289\n",
      "Epoch 746, Loss: 0.21970124542713165, Final Batch Loss: 0.11138202250003815\n",
      "Epoch 747, Loss: 0.1684320718050003, Final Batch Loss: 0.07851993292570114\n",
      "Epoch 748, Loss: 0.24078410118818283, Final Batch Loss: 0.0907105877995491\n",
      "Epoch 749, Loss: 0.3098001629114151, Final Batch Loss: 0.13874287903308868\n",
      "Epoch 750, Loss: 0.34384361654520035, Final Batch Loss: 0.2572097182273865\n",
      "Epoch 751, Loss: 0.19503547251224518, Final Batch Loss: 0.0805288702249527\n",
      "Epoch 752, Loss: 0.176442489027977, Final Batch Loss: 0.0664510577917099\n",
      "Epoch 753, Loss: 0.20150991529226303, Final Batch Loss: 0.10125534236431122\n",
      "Epoch 754, Loss: 0.23584357649087906, Final Batch Loss: 0.1302051693201065\n",
      "Epoch 755, Loss: 0.21113063395023346, Final Batch Loss: 0.12172555178403854\n",
      "Epoch 756, Loss: 0.21451283246278763, Final Batch Loss: 0.08860743790864944\n",
      "Epoch 757, Loss: 0.19911527633666992, Final Batch Loss: 0.10666083544492722\n",
      "Epoch 758, Loss: 0.304998017847538, Final Batch Loss: 0.22078673541545868\n",
      "Epoch 759, Loss: 0.20182758569717407, Final Batch Loss: 0.11904378980398178\n",
      "Epoch 760, Loss: 0.21725375950336456, Final Batch Loss: 0.08416540920734406\n",
      "Epoch 761, Loss: 0.16623303294181824, Final Batch Loss: 0.06382928043603897\n",
      "Epoch 762, Loss: 0.3509942293167114, Final Batch Loss: 0.1631200611591339\n",
      "Epoch 763, Loss: 0.3050186336040497, Final Batch Loss: 0.15122127532958984\n",
      "Epoch 764, Loss: 0.21284041553735733, Final Batch Loss: 0.08591998368501663\n",
      "Epoch 765, Loss: 0.2263864278793335, Final Batch Loss: 0.13108040392398834\n",
      "Epoch 766, Loss: 0.21646296977996826, Final Batch Loss: 0.12265823036432266\n",
      "Epoch 767, Loss: 0.15358401089906693, Final Batch Loss: 0.06916303932666779\n",
      "Epoch 768, Loss: 0.22330372780561447, Final Batch Loss: 0.11235958337783813\n",
      "Epoch 769, Loss: 0.26967038959264755, Final Batch Loss: 0.11946342140436172\n",
      "Epoch 770, Loss: 0.1921423301100731, Final Batch Loss: 0.11169057339429855\n",
      "Epoch 771, Loss: 0.1964690461754799, Final Batch Loss: 0.12475985288619995\n",
      "Epoch 772, Loss: 0.23489698767662048, Final Batch Loss: 0.11039841920137405\n",
      "Epoch 773, Loss: 0.21935905516147614, Final Batch Loss: 0.10907799750566483\n",
      "Epoch 774, Loss: 0.18476691097021103, Final Batch Loss: 0.0613708570599556\n",
      "Epoch 775, Loss: 0.2539903447031975, Final Batch Loss: 0.11937279254198074\n",
      "Epoch 776, Loss: 0.2140861228108406, Final Batch Loss: 0.07922301441431046\n",
      "Epoch 777, Loss: 0.18285340815782547, Final Batch Loss: 0.06998832523822784\n",
      "Epoch 778, Loss: 0.2662317231297493, Final Batch Loss: 0.11559674888849258\n",
      "Epoch 779, Loss: 0.19788789749145508, Final Batch Loss: 0.0764821246266365\n",
      "Epoch 780, Loss: 0.19269855320453644, Final Batch Loss: 0.10382656753063202\n",
      "Epoch 781, Loss: 0.19054178148508072, Final Batch Loss: 0.11211556196212769\n",
      "Epoch 782, Loss: 0.19469481706619263, Final Batch Loss: 0.08790181577205658\n",
      "Epoch 783, Loss: 0.1638641506433487, Final Batch Loss: 0.10125955939292908\n",
      "Epoch 784, Loss: 0.32141023129224777, Final Batch Loss: 0.21331438422203064\n",
      "Epoch 785, Loss: 0.14382674545049667, Final Batch Loss: 0.06756925582885742\n",
      "Epoch 786, Loss: 0.19181202352046967, Final Batch Loss: 0.08889943361282349\n",
      "Epoch 787, Loss: 0.20935595780611038, Final Batch Loss: 0.13657830655574799\n",
      "Epoch 788, Loss: 0.25395845621824265, Final Batch Loss: 0.11455550044775009\n",
      "Epoch 789, Loss: 0.1920669823884964, Final Batch Loss: 0.08054596185684204\n",
      "Epoch 790, Loss: 0.1431059166789055, Final Batch Loss: 0.049481794238090515\n",
      "Epoch 791, Loss: 0.16212253272533417, Final Batch Loss: 0.06536667048931122\n",
      "Epoch 792, Loss: 0.23716896772384644, Final Batch Loss: 0.12023013830184937\n",
      "Epoch 793, Loss: 0.28799350559711456, Final Batch Loss: 0.1562478095293045\n",
      "Epoch 794, Loss: 0.24329035729169846, Final Batch Loss: 0.1364515721797943\n",
      "Epoch 795, Loss: 0.21264418959617615, Final Batch Loss: 0.10137230902910233\n",
      "Epoch 796, Loss: 0.19741663336753845, Final Batch Loss: 0.0985439121723175\n",
      "Epoch 797, Loss: 0.1544051617383957, Final Batch Loss: 0.0642755851149559\n",
      "Epoch 798, Loss: 0.22754797339439392, Final Batch Loss: 0.09235548973083496\n",
      "Epoch 799, Loss: 0.19125483930110931, Final Batch Loss: 0.08952311426401138\n",
      "Epoch 800, Loss: 0.21769675612449646, Final Batch Loss: 0.09217958152294159\n",
      "Epoch 801, Loss: 0.19850188493728638, Final Batch Loss: 0.10007933527231216\n",
      "Epoch 802, Loss: 0.18026139587163925, Final Batch Loss: 0.06778357923030853\n",
      "Epoch 803, Loss: 0.2013961598277092, Final Batch Loss: 0.09148912131786346\n",
      "Epoch 804, Loss: 0.1850544661283493, Final Batch Loss: 0.0759345293045044\n",
      "Epoch 805, Loss: 0.23254364728927612, Final Batch Loss: 0.10890661925077438\n",
      "Epoch 806, Loss: 0.15231969952583313, Final Batch Loss: 0.08365291357040405\n",
      "Epoch 807, Loss: 0.229984812438488, Final Batch Loss: 0.09757191687822342\n",
      "Epoch 808, Loss: 0.32332636415958405, Final Batch Loss: 0.2200886756181717\n",
      "Epoch 809, Loss: 0.2746809720993042, Final Batch Loss: 0.1490209847688675\n",
      "Epoch 810, Loss: 0.16426627337932587, Final Batch Loss: 0.09123895317316055\n",
      "Epoch 811, Loss: 0.1573793776333332, Final Batch Loss: 0.0612902007997036\n",
      "Epoch 812, Loss: 0.24539864808321, Final Batch Loss: 0.16161476075649261\n",
      "Epoch 813, Loss: 0.16391243040561676, Final Batch Loss: 0.0738472118973732\n",
      "Epoch 814, Loss: 0.24050920456647873, Final Batch Loss: 0.09051544219255447\n",
      "Epoch 815, Loss: 0.18445416539907455, Final Batch Loss: 0.08962690085172653\n",
      "Epoch 816, Loss: 0.19367565214633942, Final Batch Loss: 0.08918450772762299\n",
      "Epoch 817, Loss: 0.19837525486946106, Final Batch Loss: 0.10296247154474258\n",
      "Epoch 818, Loss: 0.24678228795528412, Final Batch Loss: 0.15193070471286774\n",
      "Epoch 819, Loss: 0.2589358985424042, Final Batch Loss: 0.12704291939735413\n",
      "Epoch 820, Loss: 0.2397708296775818, Final Batch Loss: 0.1487528681755066\n",
      "Epoch 821, Loss: 0.2535577788949013, Final Batch Loss: 0.16304102540016174\n",
      "Epoch 822, Loss: 0.24783717095851898, Final Batch Loss: 0.1479451060295105\n",
      "Epoch 823, Loss: 0.2300610989332199, Final Batch Loss: 0.11736626923084259\n",
      "Epoch 824, Loss: 0.18517738580703735, Final Batch Loss: 0.08785127103328705\n",
      "Epoch 825, Loss: 0.16722867637872696, Final Batch Loss: 0.09615011513233185\n",
      "Epoch 826, Loss: 0.20059920847415924, Final Batch Loss: 0.10440681874752045\n",
      "Epoch 827, Loss: 0.13829100504517555, Final Batch Loss: 0.07596313208341599\n",
      "Epoch 828, Loss: 0.2879991978406906, Final Batch Loss: 0.17315810918807983\n",
      "Epoch 829, Loss: 0.18073224276304245, Final Batch Loss: 0.060707345604896545\n",
      "Epoch 830, Loss: 0.2281198874115944, Final Batch Loss: 0.14054539799690247\n",
      "Epoch 831, Loss: 0.16980627924203873, Final Batch Loss: 0.0674765333533287\n",
      "Epoch 832, Loss: 0.1937694102525711, Final Batch Loss: 0.07138320058584213\n",
      "Epoch 833, Loss: 0.19040890783071518, Final Batch Loss: 0.06463297456502914\n",
      "Epoch 834, Loss: 0.2233201265335083, Final Batch Loss: 0.12942259013652802\n",
      "Epoch 835, Loss: 0.1796501949429512, Final Batch Loss: 0.07342065125703812\n",
      "Epoch 836, Loss: 0.1745612844824791, Final Batch Loss: 0.08453105390071869\n",
      "Epoch 837, Loss: 0.22870135307312012, Final Batch Loss: 0.11111325770616531\n",
      "Epoch 838, Loss: 0.24591606855392456, Final Batch Loss: 0.09988602995872498\n",
      "Epoch 839, Loss: 0.13866673782467842, Final Batch Loss: 0.0503530390560627\n",
      "Epoch 840, Loss: 0.2940297797322273, Final Batch Loss: 0.19230958819389343\n",
      "Epoch 841, Loss: 0.1966225504875183, Final Batch Loss: 0.09867554903030396\n",
      "Epoch 842, Loss: 0.2662641108036041, Final Batch Loss: 0.17000490427017212\n",
      "Epoch 843, Loss: 0.22172120213508606, Final Batch Loss: 0.09811551868915558\n",
      "Epoch 844, Loss: 0.21107643842697144, Final Batch Loss: 0.10451538115739822\n",
      "Epoch 845, Loss: 0.21031410992145538, Final Batch Loss: 0.10174601525068283\n",
      "Epoch 846, Loss: 0.18463266640901566, Final Batch Loss: 0.09946859627962112\n",
      "Epoch 847, Loss: 0.19339538365602493, Final Batch Loss: 0.12691031396389008\n",
      "Epoch 848, Loss: 0.22397664934396744, Final Batch Loss: 0.13444434106349945\n",
      "Epoch 849, Loss: 0.25197940319776535, Final Batch Loss: 0.09399222582578659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 850, Loss: 0.1358134113252163, Final Batch Loss: 0.05406845733523369\n",
      "Epoch 851, Loss: 0.1957264170050621, Final Batch Loss: 0.10594194382429123\n",
      "Epoch 852, Loss: 0.19608518481254578, Final Batch Loss: 0.10429996997117996\n",
      "Epoch 853, Loss: 0.16164518147706985, Final Batch Loss: 0.08656645566225052\n",
      "Epoch 854, Loss: 0.1759551465511322, Final Batch Loss: 0.0727996975183487\n",
      "Epoch 855, Loss: 0.20354334264993668, Final Batch Loss: 0.08709311485290527\n",
      "Epoch 856, Loss: 0.24345750361680984, Final Batch Loss: 0.1633339524269104\n",
      "Epoch 857, Loss: 0.1607632264494896, Final Batch Loss: 0.07342837750911713\n",
      "Epoch 858, Loss: 0.21088305860757828, Final Batch Loss: 0.07239430397748947\n",
      "Epoch 859, Loss: 0.2552809864282608, Final Batch Loss: 0.143350288271904\n",
      "Epoch 860, Loss: 0.14307285100221634, Final Batch Loss: 0.04837538301944733\n",
      "Epoch 861, Loss: 0.23771875351667404, Final Batch Loss: 0.1295650154352188\n",
      "Epoch 862, Loss: 0.19611113518476486, Final Batch Loss: 0.09925693273544312\n",
      "Epoch 863, Loss: 0.19369351118803024, Final Batch Loss: 0.07689400762319565\n",
      "Epoch 864, Loss: 0.19528239965438843, Final Batch Loss: 0.09049945324659348\n",
      "Epoch 865, Loss: 0.15674008056521416, Final Batch Loss: 0.04643261805176735\n",
      "Epoch 866, Loss: 0.2363963946700096, Final Batch Loss: 0.10868411511182785\n",
      "Epoch 867, Loss: 0.18975092470645905, Final Batch Loss: 0.08196181803941727\n",
      "Epoch 868, Loss: 0.1816239431500435, Final Batch Loss: 0.10010506212711334\n",
      "Epoch 869, Loss: 0.16251156479120255, Final Batch Loss: 0.0636105164885521\n",
      "Epoch 870, Loss: 0.22331350296735764, Final Batch Loss: 0.1438753604888916\n",
      "Epoch 871, Loss: 0.1667407527565956, Final Batch Loss: 0.06351558119058609\n",
      "Epoch 872, Loss: 0.18776985257863998, Final Batch Loss: 0.10838691890239716\n",
      "Epoch 873, Loss: 0.19881708174943924, Final Batch Loss: 0.10879406332969666\n",
      "Epoch 874, Loss: 0.18935249745845795, Final Batch Loss: 0.11855107545852661\n",
      "Epoch 875, Loss: 0.2023792266845703, Final Batch Loss: 0.13051190972328186\n",
      "Epoch 876, Loss: 0.22056924551725388, Final Batch Loss: 0.14439326524734497\n",
      "Epoch 877, Loss: 0.20008905231952667, Final Batch Loss: 0.09260747581720352\n",
      "Epoch 878, Loss: 0.18808289617300034, Final Batch Loss: 0.09605399519205093\n",
      "Epoch 879, Loss: 0.18445813655853271, Final Batch Loss: 0.08543580025434494\n",
      "Epoch 880, Loss: 0.23635753244161606, Final Batch Loss: 0.09602715820074081\n",
      "Epoch 881, Loss: 0.2585856430232525, Final Batch Loss: 0.20204448699951172\n",
      "Epoch 882, Loss: 0.1862925961613655, Final Batch Loss: 0.10810613632202148\n",
      "Epoch 883, Loss: 0.23478271067142487, Final Batch Loss: 0.10401579737663269\n",
      "Epoch 884, Loss: 0.21287988126277924, Final Batch Loss: 0.0963071659207344\n",
      "Epoch 885, Loss: 0.197028286755085, Final Batch Loss: 0.10043241828680038\n",
      "Epoch 886, Loss: 0.1428174301981926, Final Batch Loss: 0.06485068053007126\n",
      "Epoch 887, Loss: 0.2245255783200264, Final Batch Loss: 0.08222802728414536\n",
      "Epoch 888, Loss: 0.20127728581428528, Final Batch Loss: 0.09157837927341461\n",
      "Epoch 889, Loss: 0.21052736788988113, Final Batch Loss: 0.08243844658136368\n",
      "Epoch 890, Loss: 0.13149751722812653, Final Batch Loss: 0.052163705229759216\n",
      "Epoch 891, Loss: 0.22764480859041214, Final Batch Loss: 0.12096786499023438\n",
      "Epoch 892, Loss: 0.1386898085474968, Final Batch Loss: 0.07423505187034607\n",
      "Epoch 893, Loss: 0.15893323719501495, Final Batch Loss: 0.08654055744409561\n",
      "Epoch 894, Loss: 0.1804996281862259, Final Batch Loss: 0.07392198592424393\n",
      "Epoch 895, Loss: 0.15365275740623474, Final Batch Loss: 0.07361678779125214\n",
      "Epoch 896, Loss: 0.1646866723895073, Final Batch Loss: 0.09182910621166229\n",
      "Epoch 897, Loss: 0.2143433466553688, Final Batch Loss: 0.12114757299423218\n",
      "Epoch 898, Loss: 0.18871374428272247, Final Batch Loss: 0.07606077939271927\n",
      "Epoch 899, Loss: 0.18305256217718124, Final Batch Loss: 0.10567169636487961\n",
      "Epoch 900, Loss: 0.12467431649565697, Final Batch Loss: 0.057486314326524734\n",
      "Epoch 901, Loss: 0.1852373331785202, Final Batch Loss: 0.07594908773899078\n",
      "Epoch 902, Loss: 0.169635608792305, Final Batch Loss: 0.10353372991085052\n",
      "Epoch 903, Loss: 0.1602003052830696, Final Batch Loss: 0.0776260569691658\n",
      "Epoch 904, Loss: 0.2643667086958885, Final Batch Loss: 0.15770433843135834\n",
      "Epoch 905, Loss: 0.17068453133106232, Final Batch Loss: 0.090702585875988\n",
      "Epoch 906, Loss: 0.22558026015758514, Final Batch Loss: 0.14759570360183716\n",
      "Epoch 907, Loss: 0.22144360840320587, Final Batch Loss: 0.10920602083206177\n",
      "Epoch 908, Loss: 0.2327047735452652, Final Batch Loss: 0.12278376519680023\n",
      "Epoch 909, Loss: 0.17495722323656082, Final Batch Loss: 0.08080656826496124\n",
      "Epoch 910, Loss: 0.18422524631023407, Final Batch Loss: 0.10238772630691528\n",
      "Epoch 911, Loss: 0.1417059451341629, Final Batch Loss: 0.06807827204465866\n",
      "Epoch 912, Loss: 0.14907409995794296, Final Batch Loss: 0.07677412778139114\n",
      "Epoch 913, Loss: 0.18988943099975586, Final Batch Loss: 0.11502085626125336\n",
      "Epoch 914, Loss: 0.1989978477358818, Final Batch Loss: 0.11683806777000427\n",
      "Epoch 915, Loss: 0.2182176187634468, Final Batch Loss: 0.06822874397039413\n",
      "Epoch 916, Loss: 0.23616837710142136, Final Batch Loss: 0.08704089373350143\n",
      "Epoch 917, Loss: 0.15164833143353462, Final Batch Loss: 0.060407113283872604\n",
      "Epoch 918, Loss: 0.21803397685289383, Final Batch Loss: 0.08297852426767349\n",
      "Epoch 919, Loss: 0.1538032591342926, Final Batch Loss: 0.05664654076099396\n",
      "Epoch 920, Loss: 0.1778467372059822, Final Batch Loss: 0.09405781328678131\n",
      "Epoch 921, Loss: 0.1568256914615631, Final Batch Loss: 0.0924140065908432\n",
      "Epoch 922, Loss: 0.1755886897444725, Final Batch Loss: 0.0814134031534195\n",
      "Epoch 923, Loss: 0.21060536056756973, Final Batch Loss: 0.14352846145629883\n",
      "Epoch 924, Loss: 0.15099474042654037, Final Batch Loss: 0.06730148196220398\n",
      "Epoch 925, Loss: 0.18532059341669083, Final Batch Loss: 0.08488637208938599\n",
      "Epoch 926, Loss: 0.23391543328762054, Final Batch Loss: 0.1229439526796341\n",
      "Epoch 927, Loss: 0.22807589918375015, Final Batch Loss: 0.0930330827832222\n",
      "Epoch 928, Loss: 0.22854580730199814, Final Batch Loss: 0.11934038996696472\n",
      "Epoch 929, Loss: 0.2923787981271744, Final Batch Loss: 0.18430083990097046\n",
      "Epoch 930, Loss: 0.13170630112290382, Final Batch Loss: 0.07654041796922684\n",
      "Epoch 931, Loss: 0.23094642907381058, Final Batch Loss: 0.13557755947113037\n",
      "Epoch 932, Loss: 0.22657816857099533, Final Batch Loss: 0.11405298858880997\n",
      "Epoch 933, Loss: 0.2177087590098381, Final Batch Loss: 0.13236580789089203\n",
      "Epoch 934, Loss: 0.13042086362838745, Final Batch Loss: 0.07063652575016022\n",
      "Epoch 935, Loss: 0.2048518881201744, Final Batch Loss: 0.10247646272182465\n",
      "Epoch 936, Loss: 0.16054949909448624, Final Batch Loss: 0.05952088534832001\n",
      "Epoch 937, Loss: 0.2039581835269928, Final Batch Loss: 0.1350814700126648\n",
      "Epoch 938, Loss: 0.251111775636673, Final Batch Loss: 0.1712149679660797\n",
      "Epoch 939, Loss: 0.26745152473449707, Final Batch Loss: 0.14511123299598694\n",
      "Epoch 940, Loss: 0.13184114173054695, Final Batch Loss: 0.06223846599459648\n",
      "Epoch 941, Loss: 0.16668077558279037, Final Batch Loss: 0.07752961665391922\n",
      "Epoch 942, Loss: 0.1333211213350296, Final Batch Loss: 0.071835957467556\n",
      "Epoch 943, Loss: 0.1736784391105175, Final Batch Loss: 0.055592622607946396\n",
      "Epoch 944, Loss: 0.24043983221054077, Final Batch Loss: 0.12723888456821442\n",
      "Epoch 945, Loss: 0.15471526980400085, Final Batch Loss: 0.08154769241809845\n",
      "Epoch 946, Loss: 0.1474507451057434, Final Batch Loss: 0.052051231265068054\n",
      "Epoch 947, Loss: 0.1915208026766777, Final Batch Loss: 0.11329962313175201\n",
      "Epoch 948, Loss: 0.13131321966648102, Final Batch Loss: 0.055550068616867065\n",
      "Epoch 949, Loss: 0.17525292187929153, Final Batch Loss: 0.08983229845762253\n",
      "Epoch 950, Loss: 0.18027497828006744, Final Batch Loss: 0.08511583507061005\n",
      "Epoch 951, Loss: 0.1347028762102127, Final Batch Loss: 0.04983416944742203\n",
      "Epoch 952, Loss: 0.14189595356583595, Final Batch Loss: 0.05702177807688713\n",
      "Epoch 953, Loss: 0.16226740181446075, Final Batch Loss: 0.07292795181274414\n",
      "Epoch 954, Loss: 0.10466230288147926, Final Batch Loss: 0.032793838530778885\n",
      "Epoch 955, Loss: 0.16888605058193207, Final Batch Loss: 0.0818147137761116\n",
      "Epoch 956, Loss: 0.14218056946992874, Final Batch Loss: 0.0647057592868805\n",
      "Epoch 957, Loss: 0.23797310143709183, Final Batch Loss: 0.13179855048656464\n",
      "Epoch 958, Loss: 0.12687354907393456, Final Batch Loss: 0.06777603924274445\n",
      "Epoch 959, Loss: 0.19251028448343277, Final Batch Loss: 0.12820419669151306\n",
      "Epoch 960, Loss: 0.20018750429153442, Final Batch Loss: 0.1401207447052002\n",
      "Epoch 961, Loss: 0.1564810797572136, Final Batch Loss: 0.07754438370466232\n",
      "Epoch 962, Loss: 0.17620062083005905, Final Batch Loss: 0.09058500081300735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 963, Loss: 0.21048833429813385, Final Batch Loss: 0.11240924149751663\n",
      "Epoch 964, Loss: 0.24284686893224716, Final Batch Loss: 0.1732015311717987\n",
      "Epoch 965, Loss: 0.23070869594812393, Final Batch Loss: 0.10004479438066483\n",
      "Epoch 966, Loss: 0.14853434637188911, Final Batch Loss: 0.09899444133043289\n",
      "Epoch 967, Loss: 0.17624136805534363, Final Batch Loss: 0.09809914231300354\n",
      "Epoch 968, Loss: 0.13008089736104012, Final Batch Loss: 0.037448082119226456\n",
      "Epoch 969, Loss: 0.20315474271774292, Final Batch Loss: 0.0962674468755722\n",
      "Epoch 970, Loss: 0.1458851397037506, Final Batch Loss: 0.07653201371431351\n",
      "Epoch 971, Loss: 0.19844844937324524, Final Batch Loss: 0.09734854102134705\n",
      "Epoch 972, Loss: 0.17077691107988358, Final Batch Loss: 0.081510029733181\n",
      "Epoch 973, Loss: 0.21666083484888077, Final Batch Loss: 0.12441185861825943\n",
      "Epoch 974, Loss: 0.18836083263158798, Final Batch Loss: 0.06409848481416702\n",
      "Epoch 975, Loss: 0.16544584184885025, Final Batch Loss: 0.08312320709228516\n",
      "Epoch 976, Loss: 0.15329252183437347, Final Batch Loss: 0.05843869596719742\n",
      "Epoch 977, Loss: 0.22477740794420242, Final Batch Loss: 0.14482323825359344\n",
      "Epoch 978, Loss: 0.19100798293948174, Final Batch Loss: 0.13204139471054077\n",
      "Epoch 979, Loss: 0.13725803047418594, Final Batch Loss: 0.0644604042172432\n",
      "Epoch 980, Loss: 0.18170592188835144, Final Batch Loss: 0.0945892259478569\n",
      "Epoch 981, Loss: 0.1658964902162552, Final Batch Loss: 0.059937939047813416\n",
      "Epoch 982, Loss: 0.1876434087753296, Final Batch Loss: 0.10427896678447723\n",
      "Epoch 983, Loss: 0.19224916398525238, Final Batch Loss: 0.10563839226961136\n",
      "Epoch 984, Loss: 0.18076806515455246, Final Batch Loss: 0.08054477721452713\n",
      "Epoch 985, Loss: 0.14614012464880943, Final Batch Loss: 0.06092263385653496\n",
      "Epoch 986, Loss: 0.2544770687818527, Final Batch Loss: 0.13486720621585846\n",
      "Epoch 987, Loss: 0.1525510735809803, Final Batch Loss: 0.06082902476191521\n",
      "Epoch 988, Loss: 0.1470637284219265, Final Batch Loss: 0.09365501254796982\n",
      "Epoch 989, Loss: 0.17287562042474747, Final Batch Loss: 0.08536092191934586\n",
      "Epoch 990, Loss: 0.23207472264766693, Final Batch Loss: 0.11816012114286423\n",
      "Epoch 991, Loss: 0.11334546655416489, Final Batch Loss: 0.03826840966939926\n",
      "Epoch 992, Loss: 0.26458757370710373, Final Batch Loss: 0.14498771727085114\n",
      "Epoch 993, Loss: 0.23086199909448624, Final Batch Loss: 0.11814121156930923\n",
      "Epoch 994, Loss: 0.12714343145489693, Final Batch Loss: 0.06488461792469025\n",
      "Epoch 995, Loss: 0.19025155156850815, Final Batch Loss: 0.09794332087039948\n",
      "Epoch 996, Loss: 0.23307571560144424, Final Batch Loss: 0.12510760128498077\n",
      "Epoch 997, Loss: 0.1607660949230194, Final Batch Loss: 0.06663408130407333\n",
      "Epoch 998, Loss: 0.1511353775858879, Final Batch Loss: 0.06850209832191467\n",
      "Epoch 999, Loss: 0.1691736951470375, Final Batch Loss: 0.0694696307182312\n",
      "Epoch 1000, Loss: 0.12026045843958855, Final Batch Loss: 0.0558941625058651\n",
      "Epoch 1001, Loss: 0.17979253083467484, Final Batch Loss: 0.09569121152162552\n",
      "Epoch 1002, Loss: 0.2324778437614441, Final Batch Loss: 0.10861358791589737\n",
      "Epoch 1003, Loss: 0.16046663373708725, Final Batch Loss: 0.06833622604608536\n",
      "Epoch 1004, Loss: 0.22118458896875381, Final Batch Loss: 0.07451038807630539\n",
      "Epoch 1005, Loss: 0.17449826002120972, Final Batch Loss: 0.08018278330564499\n",
      "Epoch 1006, Loss: 0.1418880671262741, Final Batch Loss: 0.0559079572558403\n",
      "Epoch 1007, Loss: 0.1470629721879959, Final Batch Loss: 0.0663825049996376\n",
      "Epoch 1008, Loss: 0.1092410534620285, Final Batch Loss: 0.053218185901641846\n",
      "Epoch 1009, Loss: 0.11005899310112, Final Batch Loss: 0.05523301288485527\n",
      "Epoch 1010, Loss: 0.18225914984941483, Final Batch Loss: 0.07099025696516037\n",
      "Epoch 1011, Loss: 0.14142216742038727, Final Batch Loss: 0.07036197185516357\n",
      "Epoch 1012, Loss: 0.23282436281442642, Final Batch Loss: 0.1302771419286728\n",
      "Epoch 1013, Loss: 0.30337750911712646, Final Batch Loss: 0.1468530148267746\n",
      "Epoch 1014, Loss: 0.15363039821386337, Final Batch Loss: 0.09280944615602493\n",
      "Epoch 1015, Loss: 0.17701613157987595, Final Batch Loss: 0.11630035936832428\n",
      "Epoch 1016, Loss: 0.18023403733968735, Final Batch Loss: 0.11209190636873245\n",
      "Epoch 1017, Loss: 0.13273891061544418, Final Batch Loss: 0.05775526165962219\n",
      "Epoch 1018, Loss: 0.1234729029238224, Final Batch Loss: 0.06921297311782837\n",
      "Epoch 1019, Loss: 0.18768969178199768, Final Batch Loss: 0.10836437344551086\n",
      "Epoch 1020, Loss: 0.1770949438214302, Final Batch Loss: 0.0827542319893837\n",
      "Epoch 1021, Loss: 0.21312009543180466, Final Batch Loss: 0.09831671416759491\n",
      "Epoch 1022, Loss: 0.1557484082877636, Final Batch Loss: 0.055207181721925735\n",
      "Epoch 1023, Loss: 0.18629800528287888, Final Batch Loss: 0.10757611691951752\n",
      "Epoch 1024, Loss: 0.17968644946813583, Final Batch Loss: 0.09581480920314789\n",
      "Epoch 1025, Loss: 0.17282390594482422, Final Batch Loss: 0.06003916263580322\n",
      "Epoch 1026, Loss: 0.15715096145868301, Final Batch Loss: 0.07320638746023178\n",
      "Epoch 1027, Loss: 0.15901675820350647, Final Batch Loss: 0.07359982281923294\n",
      "Epoch 1028, Loss: 0.15837542712688446, Final Batch Loss: 0.08038503676652908\n",
      "Epoch 1029, Loss: 0.19484588503837585, Final Batch Loss: 0.08463641256093979\n",
      "Epoch 1030, Loss: 0.14150242134928703, Final Batch Loss: 0.055830832570791245\n",
      "Epoch 1031, Loss: 0.20920033007860184, Final Batch Loss: 0.11607003957033157\n",
      "Epoch 1032, Loss: 0.18598727136850357, Final Batch Loss: 0.08478140830993652\n",
      "Epoch 1033, Loss: 0.1878991574048996, Final Batch Loss: 0.05560453236103058\n",
      "Epoch 1034, Loss: 0.2051551342010498, Final Batch Loss: 0.08754461258649826\n",
      "Epoch 1035, Loss: 0.15990295261144638, Final Batch Loss: 0.08234550803899765\n",
      "Epoch 1036, Loss: 0.16130982339382172, Final Batch Loss: 0.08892364799976349\n",
      "Epoch 1037, Loss: 0.15868321806192398, Final Batch Loss: 0.0912170335650444\n",
      "Epoch 1038, Loss: 0.19664156436920166, Final Batch Loss: 0.11541400104761124\n",
      "Epoch 1039, Loss: 0.1395540088415146, Final Batch Loss: 0.06437426060438156\n",
      "Epoch 1040, Loss: 0.12926578894257545, Final Batch Loss: 0.06033281609416008\n",
      "Epoch 1041, Loss: 0.20550084859132767, Final Batch Loss: 0.11227364093065262\n",
      "Epoch 1042, Loss: 0.1608315408229828, Final Batch Loss: 0.07491105794906616\n",
      "Epoch 1043, Loss: 0.11787329614162445, Final Batch Loss: 0.0708751454949379\n",
      "Epoch 1044, Loss: 0.11360426619648933, Final Batch Loss: 0.03338276967406273\n",
      "Epoch 1045, Loss: 0.1319093555212021, Final Batch Loss: 0.07456018030643463\n",
      "Epoch 1046, Loss: 0.1025010421872139, Final Batch Loss: 0.0634908676147461\n",
      "Epoch 1047, Loss: 0.11099238693714142, Final Batch Loss: 0.051687344908714294\n",
      "Epoch 1048, Loss: 0.2033900022506714, Final Batch Loss: 0.09702828526496887\n",
      "Epoch 1049, Loss: 0.15177422016859055, Final Batch Loss: 0.06820672750473022\n",
      "Epoch 1050, Loss: 0.23103293776512146, Final Batch Loss: 0.10108514130115509\n",
      "Epoch 1051, Loss: 0.21150170266628265, Final Batch Loss: 0.06976506114006042\n",
      "Epoch 1052, Loss: 0.13444512709975243, Final Batch Loss: 0.05734105780720711\n",
      "Epoch 1053, Loss: 0.13839998841285706, Final Batch Loss: 0.06498437374830246\n",
      "Epoch 1054, Loss: 0.1112058274447918, Final Batch Loss: 0.02941839024424553\n",
      "Epoch 1055, Loss: 0.18586615473031998, Final Batch Loss: 0.08456555753946304\n",
      "Epoch 1056, Loss: 0.19287659972906113, Final Batch Loss: 0.08183151483535767\n",
      "Epoch 1057, Loss: 0.17459947615861893, Final Batch Loss: 0.0876949280500412\n",
      "Epoch 1058, Loss: 0.09919683448970318, Final Batch Loss: 0.02941647730767727\n",
      "Epoch 1059, Loss: 0.21114499121904373, Final Batch Loss: 0.06454908102750778\n",
      "Epoch 1060, Loss: 0.15186356008052826, Final Batch Loss: 0.08127114921808243\n",
      "Epoch 1061, Loss: 0.13512901961803436, Final Batch Loss: 0.051469363272190094\n",
      "Epoch 1062, Loss: 0.1418711617588997, Final Batch Loss: 0.06400396674871445\n",
      "Epoch 1063, Loss: 0.11534933745861053, Final Batch Loss: 0.05771629139780998\n",
      "Epoch 1064, Loss: 0.1915402188897133, Final Batch Loss: 0.11431515961885452\n",
      "Epoch 1065, Loss: 0.170338436961174, Final Batch Loss: 0.06281396001577377\n",
      "Epoch 1066, Loss: 0.2157193049788475, Final Batch Loss: 0.09682823717594147\n",
      "Epoch 1067, Loss: 0.18694014102220535, Final Batch Loss: 0.08418018370866776\n",
      "Epoch 1068, Loss: 0.15037772059440613, Final Batch Loss: 0.07005353271961212\n",
      "Epoch 1069, Loss: 0.18031343817710876, Final Batch Loss: 0.08606346696615219\n",
      "Epoch 1070, Loss: 0.10847001895308495, Final Batch Loss: 0.05477767065167427\n",
      "Epoch 1071, Loss: 0.22999852150678635, Final Batch Loss: 0.11031554639339447\n",
      "Epoch 1072, Loss: 0.1536208763718605, Final Batch Loss: 0.08729798346757889\n",
      "Epoch 1073, Loss: 0.12294822186231613, Final Batch Loss: 0.07070080935955048\n",
      "Epoch 1074, Loss: 0.16478945687413216, Final Batch Loss: 0.12331481277942657\n",
      "Epoch 1075, Loss: 0.1806708723306656, Final Batch Loss: 0.10034938901662827\n",
      "Epoch 1076, Loss: 0.13940156251192093, Final Batch Loss: 0.06970470398664474\n",
      "Epoch 1077, Loss: 0.12154026329517365, Final Batch Loss: 0.0603700615465641\n",
      "Epoch 1078, Loss: 0.1504131630063057, Final Batch Loss: 0.07070150971412659\n",
      "Epoch 1079, Loss: 0.22970253974199295, Final Batch Loss: 0.0999874547123909\n",
      "Epoch 1080, Loss: 0.1664051078259945, Final Batch Loss: 0.06019725278019905\n",
      "Epoch 1081, Loss: 0.19203513115644455, Final Batch Loss: 0.10458257049322128\n",
      "Epoch 1082, Loss: 0.1601794995367527, Final Batch Loss: 0.10114818066358566\n",
      "Epoch 1083, Loss: 0.19081328064203262, Final Batch Loss: 0.09288786351680756\n",
      "Epoch 1084, Loss: 0.17225056886672974, Final Batch Loss: 0.07792740315198898\n",
      "Epoch 1085, Loss: 0.16260945796966553, Final Batch Loss: 0.08333756029605865\n",
      "Epoch 1086, Loss: 0.11233555898070335, Final Batch Loss: 0.053782474249601364\n",
      "Epoch 1087, Loss: 0.24373334646224976, Final Batch Loss: 0.13936007022857666\n",
      "Epoch 1088, Loss: 0.14530503004789352, Final Batch Loss: 0.07284925132989883\n",
      "Epoch 1089, Loss: 0.19386550784111023, Final Batch Loss: 0.11676177382469177\n",
      "Epoch 1090, Loss: 0.15264685451984406, Final Batch Loss: 0.07485208660364151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1091, Loss: 0.15416382998228073, Final Batch Loss: 0.0683852806687355\n",
      "Epoch 1092, Loss: 0.157597865909338, Final Batch Loss: 0.05910540744662285\n",
      "Epoch 1093, Loss: 0.20123209059238434, Final Batch Loss: 0.11792480200529099\n",
      "Epoch 1094, Loss: 0.13503220304846764, Final Batch Loss: 0.05406014248728752\n",
      "Epoch 1095, Loss: 0.22703102976083755, Final Batch Loss: 0.10964678972959518\n",
      "Epoch 1096, Loss: 0.1753498688340187, Final Batch Loss: 0.08610717952251434\n",
      "Epoch 1097, Loss: 0.14883146435022354, Final Batch Loss: 0.08237379789352417\n",
      "Epoch 1098, Loss: 0.12553801015019417, Final Batch Loss: 0.02843267098069191\n",
      "Epoch 1099, Loss: 0.17283503711223602, Final Batch Loss: 0.09427370131015778\n",
      "Epoch 1100, Loss: 0.09040451422333717, Final Batch Loss: 0.03528229519724846\n",
      "Epoch 1101, Loss: 0.18256831169128418, Final Batch Loss: 0.10282041877508163\n",
      "Epoch 1102, Loss: 0.10421666875481606, Final Batch Loss: 0.05346979945898056\n",
      "Epoch 1103, Loss: 0.11024530977010727, Final Batch Loss: 0.032324954867362976\n",
      "Epoch 1104, Loss: 0.11957382038235664, Final Batch Loss: 0.04256344959139824\n",
      "Epoch 1105, Loss: 0.0893252044916153, Final Batch Loss: 0.034254495054483414\n",
      "Epoch 1106, Loss: 0.1846698857843876, Final Batch Loss: 0.13419610261917114\n",
      "Epoch 1107, Loss: 0.09454462304711342, Final Batch Loss: 0.030671317130327225\n",
      "Epoch 1108, Loss: 0.13223077170550823, Final Batch Loss: 0.030605701729655266\n",
      "Epoch 1109, Loss: 0.1646963357925415, Final Batch Loss: 0.06433675438165665\n",
      "Epoch 1110, Loss: 0.10903216525912285, Final Batch Loss: 0.05136393383145332\n",
      "Epoch 1111, Loss: 0.1234837993979454, Final Batch Loss: 0.07435405999422073\n",
      "Epoch 1112, Loss: 0.17816635221242905, Final Batch Loss: 0.10166425257921219\n",
      "Epoch 1113, Loss: 0.18989978730678558, Final Batch Loss: 0.0777316763997078\n",
      "Epoch 1114, Loss: 0.21476437896490097, Final Batch Loss: 0.14594198763370514\n",
      "Epoch 1115, Loss: 0.14002948626875877, Final Batch Loss: 0.04612204059958458\n",
      "Epoch 1116, Loss: 0.12924830615520477, Final Batch Loss: 0.05611856281757355\n",
      "Epoch 1117, Loss: 0.11547218635678291, Final Batch Loss: 0.0395052395761013\n",
      "Epoch 1118, Loss: 0.11305355280637741, Final Batch Loss: 0.05845969542860985\n",
      "Epoch 1119, Loss: 0.1372692510485649, Final Batch Loss: 0.06680303812026978\n",
      "Epoch 1120, Loss: 0.15782653540372849, Final Batch Loss: 0.07471340894699097\n",
      "Epoch 1121, Loss: 0.15951447188854218, Final Batch Loss: 0.10355214029550552\n",
      "Epoch 1122, Loss: 0.1417088806629181, Final Batch Loss: 0.08833935111761093\n",
      "Epoch 1123, Loss: 0.1350417695939541, Final Batch Loss: 0.04617365822196007\n",
      "Epoch 1124, Loss: 0.12760299071669579, Final Batch Loss: 0.07448073476552963\n",
      "Epoch 1125, Loss: 0.16191872209310532, Final Batch Loss: 0.08609794080257416\n",
      "Epoch 1126, Loss: 0.11769368499517441, Final Batch Loss: 0.04821448773145676\n",
      "Epoch 1127, Loss: 0.11980357393622398, Final Batch Loss: 0.062343500554561615\n",
      "Epoch 1128, Loss: 0.18428101390600204, Final Batch Loss: 0.10631120204925537\n",
      "Epoch 1129, Loss: 0.14885318279266357, Final Batch Loss: 0.07942477613687515\n",
      "Epoch 1130, Loss: 0.159567229449749, Final Batch Loss: 0.08487597107887268\n",
      "Epoch 1131, Loss: 0.14952857792377472, Final Batch Loss: 0.08342219889163971\n",
      "Epoch 1132, Loss: 0.22172169387340546, Final Batch Loss: 0.12281323224306107\n",
      "Epoch 1133, Loss: 0.18379653990268707, Final Batch Loss: 0.09762581437826157\n",
      "Epoch 1134, Loss: 0.10207744687795639, Final Batch Loss: 0.055466294288635254\n",
      "Epoch 1135, Loss: 0.13529492914676666, Final Batch Loss: 0.06809618324041367\n",
      "Epoch 1136, Loss: 0.16504345834255219, Final Batch Loss: 0.08289467543363571\n",
      "Epoch 1137, Loss: 0.16127749532461166, Final Batch Loss: 0.0919514000415802\n",
      "Epoch 1138, Loss: 0.21418553590774536, Final Batch Loss: 0.14019078016281128\n",
      "Epoch 1139, Loss: 0.10345315560698509, Final Batch Loss: 0.05264301970601082\n",
      "Epoch 1140, Loss: 0.1555398479104042, Final Batch Loss: 0.07275035232305527\n",
      "Epoch 1141, Loss: 0.17358054965734482, Final Batch Loss: 0.0738527849316597\n",
      "Epoch 1142, Loss: 0.1455453261733055, Final Batch Loss: 0.04606524854898453\n",
      "Epoch 1143, Loss: 0.16586373001337051, Final Batch Loss: 0.0740685984492302\n",
      "Epoch 1144, Loss: 0.19910375028848648, Final Batch Loss: 0.10393022000789642\n",
      "Epoch 1145, Loss: 0.18567641079425812, Final Batch Loss: 0.0749829113483429\n",
      "Epoch 1146, Loss: 0.22054442018270493, Final Batch Loss: 0.08966533094644547\n",
      "Epoch 1147, Loss: 0.21954049915075302, Final Batch Loss: 0.0981869325041771\n",
      "Epoch 1148, Loss: 0.09455133602023125, Final Batch Loss: 0.0608767494559288\n",
      "Epoch 1149, Loss: 0.21955826878547668, Final Batch Loss: 0.13552670180797577\n",
      "Epoch 1150, Loss: 0.1350952573120594, Final Batch Loss: 0.07579576969146729\n",
      "Epoch 1151, Loss: 0.10845820978283882, Final Batch Loss: 0.051591578871011734\n",
      "Epoch 1152, Loss: 0.10026247054338455, Final Batch Loss: 0.05140335112810135\n",
      "Epoch 1153, Loss: 0.1465827003121376, Final Batch Loss: 0.07786568254232407\n",
      "Epoch 1154, Loss: 0.20559550821781158, Final Batch Loss: 0.08166887611150742\n",
      "Epoch 1155, Loss: 0.1186470165848732, Final Batch Loss: 0.05353161692619324\n",
      "Epoch 1156, Loss: 0.18521635979413986, Final Batch Loss: 0.11895520985126495\n",
      "Epoch 1157, Loss: 0.1170937679708004, Final Batch Loss: 0.0686020478606224\n",
      "Epoch 1158, Loss: 0.19773270934820175, Final Batch Loss: 0.05987834185361862\n",
      "Epoch 1159, Loss: 0.16845107078552246, Final Batch Loss: 0.09611880034208298\n",
      "Epoch 1160, Loss: 0.22978869825601578, Final Batch Loss: 0.15020351111888885\n",
      "Epoch 1161, Loss: 0.11870545148849487, Final Batch Loss: 0.06339883804321289\n",
      "Epoch 1162, Loss: 0.12167760357260704, Final Batch Loss: 0.049248579889535904\n",
      "Epoch 1163, Loss: 0.17098480463027954, Final Batch Loss: 0.08680900931358337\n",
      "Epoch 1164, Loss: 0.16106002777814865, Final Batch Loss: 0.07138395309448242\n",
      "Epoch 1165, Loss: 0.18185047060251236, Final Batch Loss: 0.11785740405321121\n",
      "Epoch 1166, Loss: 0.13885654509067535, Final Batch Loss: 0.06161896884441376\n",
      "Epoch 1167, Loss: 0.20010621845722198, Final Batch Loss: 0.09888897836208344\n",
      "Epoch 1168, Loss: 0.1474677324295044, Final Batch Loss: 0.07535910606384277\n",
      "Epoch 1169, Loss: 0.12028062716126442, Final Batch Loss: 0.059273701161146164\n",
      "Epoch 1170, Loss: 0.09366209805011749, Final Batch Loss: 0.045625608414411545\n",
      "Epoch 1171, Loss: 0.17507709562778473, Final Batch Loss: 0.09224676340818405\n",
      "Epoch 1172, Loss: 0.2002803385257721, Final Batch Loss: 0.11634145677089691\n",
      "Epoch 1173, Loss: 0.14135542511940002, Final Batch Loss: 0.07283728569746017\n",
      "Epoch 1174, Loss: 0.18479078263044357, Final Batch Loss: 0.10531753301620483\n",
      "Epoch 1175, Loss: 0.15242617577314377, Final Batch Loss: 0.08728369325399399\n",
      "Epoch 1176, Loss: 0.1580432392656803, Final Batch Loss: 0.09751205146312714\n",
      "Epoch 1177, Loss: 0.1551324799656868, Final Batch Loss: 0.09112932533025742\n",
      "Epoch 1178, Loss: 0.13150732219219208, Final Batch Loss: 0.07114654034376144\n",
      "Epoch 1179, Loss: 0.17860298603773117, Final Batch Loss: 0.10736456513404846\n",
      "Epoch 1180, Loss: 0.16092151403427124, Final Batch Loss: 0.06430291384458542\n",
      "Epoch 1181, Loss: 0.13772164285182953, Final Batch Loss: 0.06495734304189682\n",
      "Epoch 1182, Loss: 0.1676144078373909, Final Batch Loss: 0.12118682265281677\n",
      "Epoch 1183, Loss: 0.09805694594979286, Final Batch Loss: 0.0599161796271801\n",
      "Epoch 1184, Loss: 0.13580991327762604, Final Batch Loss: 0.06375246495008469\n",
      "Epoch 1185, Loss: 0.17479076981544495, Final Batch Loss: 0.106010302901268\n",
      "Epoch 1186, Loss: 0.1643906757235527, Final Batch Loss: 0.07297404855489731\n",
      "Epoch 1187, Loss: 0.1349496990442276, Final Batch Loss: 0.0674705058336258\n",
      "Epoch 1188, Loss: 0.09963138028979301, Final Batch Loss: 0.04688872769474983\n",
      "Epoch 1189, Loss: 0.12276238575577736, Final Batch Loss: 0.054523590952157974\n",
      "Epoch 1190, Loss: 0.1255466379225254, Final Batch Loss: 0.07153570652008057\n",
      "Epoch 1191, Loss: 0.13733340799808502, Final Batch Loss: 0.08320111036300659\n",
      "Epoch 1192, Loss: 0.160195454955101, Final Batch Loss: 0.058684058487415314\n",
      "Epoch 1193, Loss: 0.15134991332888603, Final Batch Loss: 0.09099123626947403\n",
      "Epoch 1194, Loss: 0.11774082109332085, Final Batch Loss: 0.06301817297935486\n",
      "Epoch 1195, Loss: 0.18997132033109665, Final Batch Loss: 0.09008709341287613\n",
      "Epoch 1196, Loss: 0.16523496061563492, Final Batch Loss: 0.07948862761259079\n",
      "Epoch 1197, Loss: 0.12684889510273933, Final Batch Loss: 0.03361013904213905\n",
      "Epoch 1198, Loss: 0.14117292687296867, Final Batch Loss: 0.09166484326124191\n",
      "Epoch 1199, Loss: 0.07892220094799995, Final Batch Loss: 0.035954032093286514\n",
      "Epoch 1200, Loss: 0.1286652348935604, Final Batch Loss: 0.04484585300087929\n",
      "Epoch 1201, Loss: 0.14784244447946548, Final Batch Loss: 0.08427564799785614\n",
      "Epoch 1202, Loss: 0.15916438400745392, Final Batch Loss: 0.07734433561563492\n",
      "Epoch 1203, Loss: 0.13141818717122078, Final Batch Loss: 0.03354397788643837\n",
      "Epoch 1204, Loss: 0.17919815331697464, Final Batch Loss: 0.10362636297941208\n",
      "Epoch 1205, Loss: 0.1444840431213379, Final Batch Loss: 0.07449363172054291\n",
      "Epoch 1206, Loss: 0.10816388204693794, Final Batch Loss: 0.03263794258236885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1207, Loss: 0.11775967106223106, Final Batch Loss: 0.0682082250714302\n",
      "Epoch 1208, Loss: 0.18444370478391647, Final Batch Loss: 0.10751759260892868\n",
      "Epoch 1209, Loss: 0.11699312180280685, Final Batch Loss: 0.05462116375565529\n",
      "Epoch 1210, Loss: 0.21144820749759674, Final Batch Loss: 0.07734331488609314\n",
      "Epoch 1211, Loss: 0.18392395228147507, Final Batch Loss: 0.08293420076370239\n",
      "Epoch 1212, Loss: 0.12092776596546173, Final Batch Loss: 0.04006832838058472\n",
      "Epoch 1213, Loss: 0.16123861446976662, Final Batch Loss: 0.1007363423705101\n",
      "Epoch 1214, Loss: 0.10527234897017479, Final Batch Loss: 0.059197425842285156\n",
      "Epoch 1215, Loss: 0.1987476572394371, Final Batch Loss: 0.13615542650222778\n",
      "Epoch 1216, Loss: 0.14243047684431076, Final Batch Loss: 0.0532044991850853\n",
      "Epoch 1217, Loss: 0.13623809814453125, Final Batch Loss: 0.07305356115102768\n",
      "Epoch 1218, Loss: 0.10658137500286102, Final Batch Loss: 0.05753310024738312\n",
      "Epoch 1219, Loss: 0.1246015764772892, Final Batch Loss: 0.0530189611017704\n",
      "Epoch 1220, Loss: 0.15063899755477905, Final Batch Loss: 0.0970064252614975\n",
      "Epoch 1221, Loss: 0.11194473132491112, Final Batch Loss: 0.04472174122929573\n",
      "Epoch 1222, Loss: 0.14549580588936806, Final Batch Loss: 0.09157061576843262\n",
      "Epoch 1223, Loss: 0.1316099688410759, Final Batch Loss: 0.07312091439962387\n",
      "Epoch 1224, Loss: 0.17366750538349152, Final Batch Loss: 0.08753843605518341\n",
      "Epoch 1225, Loss: 0.13643083721399307, Final Batch Loss: 0.07268466800451279\n",
      "Epoch 1226, Loss: 0.1559547483921051, Final Batch Loss: 0.08623930811882019\n",
      "Epoch 1227, Loss: 0.1057596355676651, Final Batch Loss: 0.04257563501596451\n",
      "Epoch 1228, Loss: 0.11149655282497406, Final Batch Loss: 0.06925129890441895\n",
      "Epoch 1229, Loss: 0.1490115225315094, Final Batch Loss: 0.04185593128204346\n",
      "Epoch 1230, Loss: 0.11526386812329292, Final Batch Loss: 0.07189056277275085\n",
      "Epoch 1231, Loss: 0.10331304371356964, Final Batch Loss: 0.05549139529466629\n",
      "Epoch 1232, Loss: 0.12835680693387985, Final Batch Loss: 0.06614356487989426\n",
      "Epoch 1233, Loss: 0.15615097433328629, Final Batch Loss: 0.07504702359437943\n",
      "Epoch 1234, Loss: 0.16218915954232216, Final Batch Loss: 0.04607361927628517\n",
      "Epoch 1235, Loss: 0.16809435933828354, Final Batch Loss: 0.08033279329538345\n",
      "Epoch 1236, Loss: 0.1214340589940548, Final Batch Loss: 0.06472117453813553\n",
      "Epoch 1237, Loss: 0.14176693931221962, Final Batch Loss: 0.08245886862277985\n",
      "Epoch 1238, Loss: 0.10451465845108032, Final Batch Loss: 0.04011572152376175\n",
      "Epoch 1239, Loss: 0.13491938263177872, Final Batch Loss: 0.07192402333021164\n",
      "Epoch 1240, Loss: 0.1988631635904312, Final Batch Loss: 0.1311904639005661\n",
      "Epoch 1241, Loss: 0.12500900402665138, Final Batch Loss: 0.04153570905327797\n",
      "Epoch 1242, Loss: 0.11335469782352448, Final Batch Loss: 0.052521757781505585\n",
      "Epoch 1243, Loss: 0.15028738975524902, Final Batch Loss: 0.07059278339147568\n",
      "Epoch 1244, Loss: 0.13581838458776474, Final Batch Loss: 0.038069210946559906\n",
      "Epoch 1245, Loss: 0.12044127285480499, Final Batch Loss: 0.04847174882888794\n",
      "Epoch 1246, Loss: 0.09707614034414291, Final Batch Loss: 0.0468006506562233\n",
      "Epoch 1247, Loss: 0.1697562262415886, Final Batch Loss: 0.08265309780836105\n",
      "Epoch 1248, Loss: 0.1447184681892395, Final Batch Loss: 0.0641404464840889\n",
      "Epoch 1249, Loss: 0.16335900872945786, Final Batch Loss: 0.053645409643650055\n",
      "Epoch 1250, Loss: 0.1219945028424263, Final Batch Loss: 0.057839035987854004\n",
      "Epoch 1251, Loss: 0.1122632585465908, Final Batch Loss: 0.041567590087652206\n",
      "Epoch 1252, Loss: 0.14268623664975166, Final Batch Loss: 0.059959929436445236\n",
      "Epoch 1253, Loss: 0.18240217864513397, Final Batch Loss: 0.10035642981529236\n",
      "Epoch 1254, Loss: 0.10139855369925499, Final Batch Loss: 0.05726417899131775\n",
      "Epoch 1255, Loss: 0.12222494930028915, Final Batch Loss: 0.05465913563966751\n",
      "Epoch 1256, Loss: 0.2371782809495926, Final Batch Loss: 0.09797540307044983\n",
      "Epoch 1257, Loss: 0.12564953044056892, Final Batch Loss: 0.06680173426866531\n",
      "Epoch 1258, Loss: 0.098958570510149, Final Batch Loss: 0.044967420399188995\n",
      "Epoch 1259, Loss: 0.13476240262389183, Final Batch Loss: 0.07998451590538025\n",
      "Epoch 1260, Loss: 0.10030164942145348, Final Batch Loss: 0.062167733907699585\n",
      "Epoch 1261, Loss: 0.1287476308643818, Final Batch Loss: 0.05826520547270775\n",
      "Epoch 1262, Loss: 0.11682262644171715, Final Batch Loss: 0.06850619614124298\n",
      "Epoch 1263, Loss: 0.20473115891218185, Final Batch Loss: 0.11394709348678589\n",
      "Epoch 1264, Loss: 0.12432634457945824, Final Batch Loss: 0.07753122597932816\n",
      "Epoch 1265, Loss: 0.14779752865433693, Final Batch Loss: 0.048972468823194504\n",
      "Epoch 1266, Loss: 0.12790675088763237, Final Batch Loss: 0.08717074245214462\n",
      "Epoch 1267, Loss: 0.17047302424907684, Final Batch Loss: 0.07691416144371033\n",
      "Epoch 1268, Loss: 0.18564274162054062, Final Batch Loss: 0.10809557884931564\n",
      "Epoch 1269, Loss: 0.1197446808218956, Final Batch Loss: 0.0709511861205101\n",
      "Epoch 1270, Loss: 0.11589085310697556, Final Batch Loss: 0.046289898455142975\n",
      "Epoch 1271, Loss: 0.11206846311688423, Final Batch Loss: 0.058716896921396255\n",
      "Epoch 1272, Loss: 0.15730996429920197, Final Batch Loss: 0.08126908540725708\n",
      "Epoch 1273, Loss: 0.07729366887360811, Final Batch Loss: 0.010416937060654163\n",
      "Epoch 1274, Loss: 0.12043509632349014, Final Batch Loss: 0.03953489661216736\n",
      "Epoch 1275, Loss: 0.12384871765971184, Final Batch Loss: 0.0548299141228199\n",
      "Epoch 1276, Loss: 0.12202196940779686, Final Batch Loss: 0.06943430006504059\n",
      "Epoch 1277, Loss: 0.16750789433717728, Final Batch Loss: 0.08859792351722717\n",
      "Epoch 1278, Loss: 0.1886008456349373, Final Batch Loss: 0.084120973944664\n",
      "Epoch 1279, Loss: 0.14662395045161247, Final Batch Loss: 0.05957556143403053\n",
      "Epoch 1280, Loss: 0.17283674329519272, Final Batch Loss: 0.08897781372070312\n",
      "Epoch 1281, Loss: 0.15729964524507523, Final Batch Loss: 0.10577201098203659\n",
      "Epoch 1282, Loss: 0.13692061603069305, Final Batch Loss: 0.0482427254319191\n",
      "Epoch 1283, Loss: 0.11518912017345428, Final Batch Loss: 0.05436743423342705\n",
      "Epoch 1284, Loss: 0.1456419937312603, Final Batch Loss: 0.0850362479686737\n",
      "Epoch 1285, Loss: 0.11802293732762337, Final Batch Loss: 0.05725815147161484\n",
      "Epoch 1286, Loss: 0.06734074093401432, Final Batch Loss: 0.041445616632699966\n",
      "Epoch 1287, Loss: 0.0902178306132555, Final Batch Loss: 0.030716175213456154\n",
      "Epoch 1288, Loss: 0.183639295399189, Final Batch Loss: 0.07269357889890671\n",
      "Epoch 1289, Loss: 0.16829372942447662, Final Batch Loss: 0.09056472778320312\n",
      "Epoch 1290, Loss: 0.1319589726626873, Final Batch Loss: 0.04432177171111107\n",
      "Epoch 1291, Loss: 0.16338331997394562, Final Batch Loss: 0.0741477757692337\n",
      "Epoch 1292, Loss: 0.12313349545001984, Final Batch Loss: 0.04211978614330292\n",
      "Epoch 1293, Loss: 0.13642174750566483, Final Batch Loss: 0.05117198824882507\n",
      "Epoch 1294, Loss: 0.08343750983476639, Final Batch Loss: 0.034310232847929\n",
      "Epoch 1295, Loss: 0.14562289416790009, Final Batch Loss: 0.08075728267431259\n",
      "Epoch 1296, Loss: 0.13840481266379356, Final Batch Loss: 0.08315544575452805\n",
      "Epoch 1297, Loss: 0.1410423219203949, Final Batch Loss: 0.06547759473323822\n",
      "Epoch 1298, Loss: 0.10569249466061592, Final Batch Loss: 0.06570502370595932\n",
      "Epoch 1299, Loss: 0.1665574237704277, Final Batch Loss: 0.0815683901309967\n",
      "Epoch 1300, Loss: 0.11061128601431847, Final Batch Loss: 0.05128885433077812\n",
      "Epoch 1301, Loss: 0.12736180424690247, Final Batch Loss: 0.04170946031808853\n",
      "Epoch 1302, Loss: 0.19945145398378372, Final Batch Loss: 0.1291760951280594\n",
      "Epoch 1303, Loss: 0.16032175719738007, Final Batch Loss: 0.06988748162984848\n",
      "Epoch 1304, Loss: 0.09575983881950378, Final Batch Loss: 0.052748989313840866\n",
      "Epoch 1305, Loss: 0.10039938241243362, Final Batch Loss: 0.051647935062646866\n",
      "Epoch 1306, Loss: 0.11504080891609192, Final Batch Loss: 0.05598502233624458\n",
      "Epoch 1307, Loss: 0.16275449097156525, Final Batch Loss: 0.06887327879667282\n",
      "Epoch 1308, Loss: 0.10416653752326965, Final Batch Loss: 0.049554336816072464\n",
      "Epoch 1309, Loss: 0.10005539655685425, Final Batch Loss: 0.04219560697674751\n",
      "Epoch 1310, Loss: 0.105521684512496, Final Batch Loss: 0.022697580978274345\n",
      "Epoch 1311, Loss: 0.16406430304050446, Final Batch Loss: 0.07592228800058365\n",
      "Epoch 1312, Loss: 0.1486533358693123, Final Batch Loss: 0.05304920673370361\n",
      "Epoch 1313, Loss: 0.11444954574108124, Final Batch Loss: 0.036048851907253265\n",
      "Epoch 1314, Loss: 0.17860047519207, Final Batch Loss: 0.053366586565971375\n",
      "Epoch 1315, Loss: 0.09194541350007057, Final Batch Loss: 0.02843114361166954\n",
      "Epoch 1316, Loss: 0.20223364979028702, Final Batch Loss: 0.13451766967773438\n",
      "Epoch 1317, Loss: 0.13981708884239197, Final Batch Loss: 0.04382765293121338\n",
      "Epoch 1318, Loss: 0.10199429839849472, Final Batch Loss: 0.053764328360557556\n",
      "Epoch 1319, Loss: 0.1487952433526516, Final Batch Loss: 0.05263713374733925\n",
      "Epoch 1320, Loss: 0.13867656886577606, Final Batch Loss: 0.05079880356788635\n",
      "Epoch 1321, Loss: 0.09918652102351189, Final Batch Loss: 0.051455214619636536\n",
      "Epoch 1322, Loss: 0.12435911223292351, Final Batch Loss: 0.039461713284254074\n",
      "Epoch 1323, Loss: 0.11907444149255753, Final Batch Loss: 0.07435598969459534\n",
      "Epoch 1324, Loss: 0.16654670983552933, Final Batch Loss: 0.10023310780525208\n",
      "Epoch 1325, Loss: 0.10101516544818878, Final Batch Loss: 0.040392469614744186\n",
      "Epoch 1326, Loss: 0.08902796730399132, Final Batch Loss: 0.03835762292146683\n",
      "Epoch 1327, Loss: 0.12718479707837105, Final Batch Loss: 0.060512203723192215\n",
      "Epoch 1328, Loss: 0.19502825662493706, Final Batch Loss: 0.13334818184375763\n",
      "Epoch 1329, Loss: 0.15371115505695343, Final Batch Loss: 0.07971633225679398\n",
      "Epoch 1330, Loss: 0.09251683205366135, Final Batch Loss: 0.0469718873500824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331, Loss: 0.16501954570412636, Final Batch Loss: 0.11597006767988205\n",
      "Epoch 1332, Loss: 0.11885184422135353, Final Batch Loss: 0.0723540335893631\n",
      "Epoch 1333, Loss: 0.1020887903869152, Final Batch Loss: 0.05405110865831375\n",
      "Epoch 1334, Loss: 0.16885721310973167, Final Batch Loss: 0.10640887916088104\n",
      "Epoch 1335, Loss: 0.12014085054397583, Final Batch Loss: 0.05615805834531784\n",
      "Epoch 1336, Loss: 0.08618807792663574, Final Batch Loss: 0.03010556846857071\n",
      "Epoch 1337, Loss: 0.11503604054450989, Final Batch Loss: 0.04156923294067383\n",
      "Epoch 1338, Loss: 0.14324738830327988, Final Batch Loss: 0.0738733559846878\n",
      "Epoch 1339, Loss: 0.15471642464399338, Final Batch Loss: 0.08480523526668549\n",
      "Epoch 1340, Loss: 0.14532925188541412, Final Batch Loss: 0.047171153128147125\n",
      "Epoch 1341, Loss: 0.12045816332101822, Final Batch Loss: 0.05107274651527405\n",
      "Epoch 1342, Loss: 0.17906350642442703, Final Batch Loss: 0.08912302553653717\n",
      "Epoch 1343, Loss: 0.153269462287426, Final Batch Loss: 0.05110872536897659\n",
      "Epoch 1344, Loss: 0.12035659700632095, Final Batch Loss: 0.041650839149951935\n",
      "Epoch 1345, Loss: 0.09775800257921219, Final Batch Loss: 0.03774930164217949\n",
      "Epoch 1346, Loss: 0.20154504477977753, Final Batch Loss: 0.13668540120124817\n",
      "Epoch 1347, Loss: 0.12503940239548683, Final Batch Loss: 0.039940234273672104\n",
      "Epoch 1348, Loss: 0.1396225467324257, Final Batch Loss: 0.05621080845594406\n",
      "Epoch 1349, Loss: 0.2271370068192482, Final Batch Loss: 0.1260857731103897\n",
      "Epoch 1350, Loss: 0.12464159354567528, Final Batch Loss: 0.07584184408187866\n",
      "Epoch 1351, Loss: 0.11120365932583809, Final Batch Loss: 0.0563216507434845\n",
      "Epoch 1352, Loss: 0.1009373888373375, Final Batch Loss: 0.026939235627651215\n",
      "Epoch 1353, Loss: 0.1019052192568779, Final Batch Loss: 0.056523557752370834\n",
      "Epoch 1354, Loss: 0.12738215178251266, Final Batch Loss: 0.05397331714630127\n",
      "Epoch 1355, Loss: 0.14814850315451622, Final Batch Loss: 0.10359203815460205\n",
      "Epoch 1356, Loss: 0.11715087667107582, Final Batch Loss: 0.06679516285657883\n",
      "Epoch 1357, Loss: 0.08233850821852684, Final Batch Loss: 0.03908921405673027\n",
      "Epoch 1358, Loss: 0.14420264586806297, Final Batch Loss: 0.055189769715070724\n",
      "Epoch 1359, Loss: 0.1554156318306923, Final Batch Loss: 0.07087067514657974\n",
      "Epoch 1360, Loss: 0.16206833720207214, Final Batch Loss: 0.09231314063072205\n",
      "Epoch 1361, Loss: 0.1010638140141964, Final Batch Loss: 0.030280914157629013\n",
      "Epoch 1362, Loss: 0.1420743428170681, Final Batch Loss: 0.07963433116674423\n",
      "Epoch 1363, Loss: 0.1429409608244896, Final Batch Loss: 0.05575570464134216\n",
      "Epoch 1364, Loss: 0.1970227211713791, Final Batch Loss: 0.09335578978061676\n",
      "Epoch 1365, Loss: 0.11033102869987488, Final Batch Loss: 0.05282605439424515\n",
      "Epoch 1366, Loss: 0.10648798942565918, Final Batch Loss: 0.042523182928562164\n",
      "Epoch 1367, Loss: 0.11111439764499664, Final Batch Loss: 0.045380860567092896\n",
      "Epoch 1368, Loss: 0.10513787716627121, Final Batch Loss: 0.055286556482315063\n",
      "Epoch 1369, Loss: 0.11434243619441986, Final Batch Loss: 0.031007736921310425\n",
      "Epoch 1370, Loss: 0.1545116826891899, Final Batch Loss: 0.09155979007482529\n",
      "Epoch 1371, Loss: 0.11414298787713051, Final Batch Loss: 0.040016014128923416\n",
      "Epoch 1372, Loss: 0.105630774050951, Final Batch Loss: 0.03526238724589348\n",
      "Epoch 1373, Loss: 0.1356995441019535, Final Batch Loss: 0.07781807333230972\n",
      "Epoch 1374, Loss: 0.19396225363016129, Final Batch Loss: 0.10366766899824142\n",
      "Epoch 1375, Loss: 0.16967014595866203, Final Batch Loss: 0.10816449671983719\n",
      "Epoch 1376, Loss: 0.11432185769081116, Final Batch Loss: 0.03843367099761963\n",
      "Epoch 1377, Loss: 0.13046032562851906, Final Batch Loss: 0.05718564614653587\n",
      "Epoch 1378, Loss: 0.1242244578897953, Final Batch Loss: 0.04672560468316078\n",
      "Epoch 1379, Loss: 0.15951506793498993, Final Batch Loss: 0.07079289108514786\n",
      "Epoch 1380, Loss: 0.09177270159125328, Final Batch Loss: 0.055527493357658386\n",
      "Epoch 1381, Loss: 0.07549698650836945, Final Batch Loss: 0.030644241720438004\n",
      "Epoch 1382, Loss: 0.1532091572880745, Final Batch Loss: 0.09043490141630173\n",
      "Epoch 1383, Loss: 0.1406501606106758, Final Batch Loss: 0.07481276243925095\n",
      "Epoch 1384, Loss: 0.14129521697759628, Final Batch Loss: 0.07308092713356018\n",
      "Epoch 1385, Loss: 0.10235436633229256, Final Batch Loss: 0.06699099391698837\n",
      "Epoch 1386, Loss: 0.12444659695029259, Final Batch Loss: 0.07799620926380157\n",
      "Epoch 1387, Loss: 0.09711286239326, Final Batch Loss: 0.025577230378985405\n",
      "Epoch 1388, Loss: 0.08789340034127235, Final Batch Loss: 0.04330623894929886\n",
      "Epoch 1389, Loss: 0.18442627787590027, Final Batch Loss: 0.11233991384506226\n",
      "Epoch 1390, Loss: 0.1417788714170456, Final Batch Loss: 0.07410261780023575\n",
      "Epoch 1391, Loss: 0.13149159401655197, Final Batch Loss: 0.06411755084991455\n",
      "Epoch 1392, Loss: 0.09433488734066486, Final Batch Loss: 0.022079044952988625\n",
      "Epoch 1393, Loss: 0.08733576536178589, Final Batch Loss: 0.03259710222482681\n",
      "Epoch 1394, Loss: 0.15902113541960716, Final Batch Loss: 0.06014169380068779\n",
      "Epoch 1395, Loss: 0.15213830769062042, Final Batch Loss: 0.09948020428419113\n",
      "Epoch 1396, Loss: 0.17566405981779099, Final Batch Loss: 0.08790412545204163\n",
      "Epoch 1397, Loss: 0.10204107314348221, Final Batch Loss: 0.06193771958351135\n",
      "Epoch 1398, Loss: 0.13367794454097748, Final Batch Loss: 0.06374652683734894\n",
      "Epoch 1399, Loss: 0.07586820423603058, Final Batch Loss: 0.041635822504758835\n",
      "Epoch 1400, Loss: 0.09590510465204716, Final Batch Loss: 0.07005854696035385\n",
      "Epoch 1401, Loss: 0.1438195649534464, Final Batch Loss: 0.022053474560379982\n",
      "Epoch 1402, Loss: 0.08311031386256218, Final Batch Loss: 0.03707386553287506\n",
      "Epoch 1403, Loss: 0.19905558228492737, Final Batch Loss: 0.10058466345071793\n",
      "Epoch 1404, Loss: 0.13708212226629257, Final Batch Loss: 0.07398000359535217\n",
      "Epoch 1405, Loss: 0.0710383877158165, Final Batch Loss: 0.031465206295251846\n",
      "Epoch 1406, Loss: 0.09854787960648537, Final Batch Loss: 0.05154845863580704\n",
      "Epoch 1407, Loss: 0.1371864527463913, Final Batch Loss: 0.06355271488428116\n",
      "Epoch 1408, Loss: 0.13183439895510674, Final Batch Loss: 0.09969921410083771\n",
      "Epoch 1409, Loss: 0.15024633705615997, Final Batch Loss: 0.08013670146465302\n",
      "Epoch 1410, Loss: 0.11473497375845909, Final Batch Loss: 0.07172928750514984\n",
      "Epoch 1411, Loss: 0.09787512570619583, Final Batch Loss: 0.061351269483566284\n",
      "Epoch 1412, Loss: 0.13366346433758736, Final Batch Loss: 0.0558328814804554\n",
      "Epoch 1413, Loss: 0.12900575995445251, Final Batch Loss: 0.06111631542444229\n",
      "Epoch 1414, Loss: 0.12780440226197243, Final Batch Loss: 0.03994908556342125\n",
      "Epoch 1415, Loss: 0.09152943268418312, Final Batch Loss: 0.05857999622821808\n",
      "Epoch 1416, Loss: 0.1158563382923603, Final Batch Loss: 0.06513021886348724\n",
      "Epoch 1417, Loss: 0.07094669342041016, Final Batch Loss: 0.029882963746786118\n",
      "Epoch 1418, Loss: 0.15895003080368042, Final Batch Loss: 0.05724715441465378\n",
      "Epoch 1419, Loss: 0.12470493838191032, Final Batch Loss: 0.05407761409878731\n",
      "Epoch 1420, Loss: 0.15379700809717178, Final Batch Loss: 0.05421826243400574\n",
      "Epoch 1421, Loss: 0.10483750700950623, Final Batch Loss: 0.04751196876168251\n",
      "Epoch 1422, Loss: 0.19068624824285507, Final Batch Loss: 0.10763141512870789\n",
      "Epoch 1423, Loss: 0.11976955458521843, Final Batch Loss: 0.03196505829691887\n",
      "Epoch 1424, Loss: 0.12541663646697998, Final Batch Loss: 0.05047237128019333\n",
      "Epoch 1425, Loss: 0.1735779196023941, Final Batch Loss: 0.08979841321706772\n",
      "Epoch 1426, Loss: 0.1495574340224266, Final Batch Loss: 0.07605019956827164\n",
      "Epoch 1427, Loss: 0.0855756364762783, Final Batch Loss: 0.036539044231176376\n",
      "Epoch 1428, Loss: 0.10520779713988304, Final Batch Loss: 0.06384982168674469\n",
      "Epoch 1429, Loss: 0.11032769083976746, Final Batch Loss: 0.06878192722797394\n",
      "Epoch 1430, Loss: 0.1133088544011116, Final Batch Loss: 0.07245137542486191\n",
      "Epoch 1431, Loss: 0.10313496366143227, Final Batch Loss: 0.04486894607543945\n",
      "Epoch 1432, Loss: 0.1262277401983738, Final Batch Loss: 0.05974690988659859\n",
      "Epoch 1433, Loss: 0.09746718592941761, Final Batch Loss: 0.0678548812866211\n",
      "Epoch 1434, Loss: 0.1312207318842411, Final Batch Loss: 0.05381704494357109\n",
      "Epoch 1435, Loss: 0.12065069004893303, Final Batch Loss: 0.06882509589195251\n",
      "Epoch 1436, Loss: 0.0855393335223198, Final Batch Loss: 0.029960084706544876\n",
      "Epoch 1437, Loss: 0.08742571994662285, Final Batch Loss: 0.04202201962471008\n",
      "Epoch 1438, Loss: 0.10411448776721954, Final Batch Loss: 0.05473338067531586\n",
      "Epoch 1439, Loss: 0.10056086629629135, Final Batch Loss: 0.04211227223277092\n",
      "Epoch 1440, Loss: 0.08208563923835754, Final Batch Loss: 0.031331781297922134\n",
      "Epoch 1441, Loss: 0.1217416450381279, Final Batch Loss: 0.07169069349765778\n",
      "Epoch 1442, Loss: 0.10668129846453667, Final Batch Loss: 0.036483798176050186\n",
      "Epoch 1443, Loss: 0.11100609600543976, Final Batch Loss: 0.029892198741436005\n",
      "Epoch 1444, Loss: 0.1207563765347004, Final Batch Loss: 0.062135178595781326\n",
      "Epoch 1445, Loss: 0.08119666576385498, Final Batch Loss: 0.04556385427713394\n",
      "Epoch 1446, Loss: 0.08748093247413635, Final Batch Loss: 0.04317225515842438\n",
      "Epoch 1447, Loss: 0.10421933606266975, Final Batch Loss: 0.026055190712213516\n",
      "Epoch 1448, Loss: 0.19459621980786324, Final Batch Loss: 0.05843605473637581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1449, Loss: 0.08784455806016922, Final Batch Loss: 0.03596455976366997\n",
      "Epoch 1450, Loss: 0.1622716411948204, Final Batch Loss: 0.08801569044589996\n",
      "Epoch 1451, Loss: 0.09795825928449631, Final Batch Loss: 0.05984080582857132\n",
      "Epoch 1452, Loss: 0.1064477264881134, Final Batch Loss: 0.061807431280612946\n",
      "Epoch 1453, Loss: 0.09971151500940323, Final Batch Loss: 0.05321534723043442\n",
      "Epoch 1454, Loss: 0.15256231278181076, Final Batch Loss: 0.0720791444182396\n",
      "Epoch 1455, Loss: 0.10801715776324272, Final Batch Loss: 0.05422297120094299\n",
      "Epoch 1456, Loss: 0.11442632414400578, Final Batch Loss: 0.018858226016163826\n",
      "Epoch 1457, Loss: 0.10841353982686996, Final Batch Loss: 0.04173867404460907\n",
      "Epoch 1458, Loss: 0.15403709560632706, Final Batch Loss: 0.10526210814714432\n",
      "Epoch 1459, Loss: 0.26828162372112274, Final Batch Loss: 0.1907099485397339\n",
      "Epoch 1460, Loss: 0.13144952058792114, Final Batch Loss: 0.07257338613271713\n",
      "Epoch 1461, Loss: 0.08381593599915504, Final Batch Loss: 0.03462798520922661\n",
      "Epoch 1462, Loss: 0.09038949012756348, Final Batch Loss: 0.03523065894842148\n",
      "Epoch 1463, Loss: 0.12585603445768356, Final Batch Loss: 0.03471245616674423\n",
      "Epoch 1464, Loss: 0.15494705364108086, Final Batch Loss: 0.0927862897515297\n",
      "Epoch 1465, Loss: 0.10961326584219933, Final Batch Loss: 0.03634637966752052\n",
      "Epoch 1466, Loss: 0.10689358413219452, Final Batch Loss: 0.06518984586000443\n",
      "Epoch 1467, Loss: 0.07397928461432457, Final Batch Loss: 0.029233325272798538\n",
      "Epoch 1468, Loss: 0.11510425060987473, Final Batch Loss: 0.032878898084163666\n",
      "Epoch 1469, Loss: 0.14742690324783325, Final Batch Loss: 0.1103006899356842\n",
      "Epoch 1470, Loss: 0.05887042544782162, Final Batch Loss: 0.020694663748145103\n",
      "Epoch 1471, Loss: 0.13102465867996216, Final Batch Loss: 0.042510271072387695\n",
      "Epoch 1472, Loss: 0.07758558541536331, Final Batch Loss: 0.03392201289534569\n",
      "Epoch 1473, Loss: 0.09242279827594757, Final Batch Loss: 0.05668562278151512\n",
      "Epoch 1474, Loss: 0.07926860824227333, Final Batch Loss: 0.026172518730163574\n",
      "Epoch 1475, Loss: 0.12179188430309296, Final Batch Loss: 0.036373965442180634\n",
      "Epoch 1476, Loss: 0.12476421520113945, Final Batch Loss: 0.06764933466911316\n",
      "Epoch 1477, Loss: 0.14247307181358337, Final Batch Loss: 0.0917242094874382\n",
      "Epoch 1478, Loss: 0.14898860454559326, Final Batch Loss: 0.09397322684526443\n",
      "Epoch 1479, Loss: 0.10683790221810341, Final Batch Loss: 0.035804618149995804\n",
      "Epoch 1480, Loss: 0.056160890497267246, Final Batch Loss: 0.014520236290991306\n",
      "Epoch 1481, Loss: 0.11671189591288567, Final Batch Loss: 0.06031074747443199\n",
      "Epoch 1482, Loss: 0.12859555333852768, Final Batch Loss: 0.04070273041725159\n",
      "Epoch 1483, Loss: 0.09058909118175507, Final Batch Loss: 0.03554484620690346\n",
      "Epoch 1484, Loss: 0.08149119839072227, Final Batch Loss: 0.03600934520363808\n",
      "Epoch 1485, Loss: 0.12351157329976559, Final Batch Loss: 0.02016761712729931\n",
      "Epoch 1486, Loss: 0.14354144781827927, Final Batch Loss: 0.10140307247638702\n",
      "Epoch 1487, Loss: 0.12410193309187889, Final Batch Loss: 0.059416498988866806\n",
      "Epoch 1488, Loss: 0.12471527978777885, Final Batch Loss: 0.05486396327614784\n",
      "Epoch 1489, Loss: 0.11375987902283669, Final Batch Loss: 0.040159765630960464\n",
      "Epoch 1490, Loss: 0.07361767068505287, Final Batch Loss: 0.04159652441740036\n",
      "Epoch 1491, Loss: 0.13630425184965134, Final Batch Loss: 0.06320327520370483\n",
      "Epoch 1492, Loss: 0.12154482677578926, Final Batch Loss: 0.044030699878931046\n",
      "Epoch 1493, Loss: 0.16242289543151855, Final Batch Loss: 0.10106410831212997\n",
      "Epoch 1494, Loss: 0.09673226624727249, Final Batch Loss: 0.05121953412890434\n",
      "Epoch 1495, Loss: 0.11369087174534798, Final Batch Loss: 0.06629055738449097\n",
      "Epoch 1496, Loss: 0.09041378647089005, Final Batch Loss: 0.033179521560668945\n",
      "Epoch 1497, Loss: 0.21133989840745926, Final Batch Loss: 0.13569152355194092\n",
      "Epoch 1498, Loss: 0.10622644051909447, Final Batch Loss: 0.036642689257860184\n",
      "Epoch 1499, Loss: 0.1000916138291359, Final Batch Loss: 0.04683174937963486\n",
      "Epoch 1500, Loss: 0.13942495360970497, Final Batch Loss: 0.08976402878761292\n",
      "Epoch 1501, Loss: 0.12170975282788277, Final Batch Loss: 0.07133130729198456\n",
      "Epoch 1502, Loss: 0.09576716460287571, Final Batch Loss: 0.029773922637104988\n",
      "Epoch 1503, Loss: 0.10891950502991676, Final Batch Loss: 0.04387902840971947\n",
      "Epoch 1504, Loss: 0.067780751734972, Final Batch Loss: 0.033889323472976685\n",
      "Epoch 1505, Loss: 0.11189846321940422, Final Batch Loss: 0.04750495031476021\n",
      "Epoch 1506, Loss: 0.13953229784965515, Final Batch Loss: 0.07157113403081894\n",
      "Epoch 1507, Loss: 0.10885728150606155, Final Batch Loss: 0.07215312123298645\n",
      "Epoch 1508, Loss: 0.15650679916143417, Final Batch Loss: 0.07663749158382416\n",
      "Epoch 1509, Loss: 0.06870253011584282, Final Batch Loss: 0.028178241103887558\n",
      "Epoch 1510, Loss: 0.1514306627213955, Final Batch Loss: 0.10851135104894638\n",
      "Epoch 1511, Loss: 0.13040821999311447, Final Batch Loss: 0.0475863516330719\n",
      "Epoch 1512, Loss: 0.1228301003575325, Final Batch Loss: 0.045256152749061584\n",
      "Epoch 1513, Loss: 0.11600019037723541, Final Batch Loss: 0.057634055614471436\n",
      "Epoch 1514, Loss: 0.09986745566129684, Final Batch Loss: 0.03808758407831192\n",
      "Epoch 1515, Loss: 0.15737007558345795, Final Batch Loss: 0.088477224111557\n",
      "Epoch 1516, Loss: 0.10622920468449593, Final Batch Loss: 0.05774087458848953\n",
      "Epoch 1517, Loss: 0.11961837857961655, Final Batch Loss: 0.061694610863924026\n",
      "Epoch 1518, Loss: 0.14787785708904266, Final Batch Loss: 0.07798851281404495\n",
      "Epoch 1519, Loss: 0.19884870201349258, Final Batch Loss: 0.07359180599451065\n",
      "Epoch 1520, Loss: 0.13832680881023407, Final Batch Loss: 0.07219042629003525\n",
      "Epoch 1521, Loss: 0.08075018227100372, Final Batch Loss: 0.03268617019057274\n",
      "Epoch 1522, Loss: 0.08370425552129745, Final Batch Loss: 0.024977080523967743\n",
      "Epoch 1523, Loss: 0.12121699750423431, Final Batch Loss: 0.05891262739896774\n",
      "Epoch 1524, Loss: 0.0710929874330759, Final Batch Loss: 0.02686403878033161\n",
      "Epoch 1525, Loss: 0.12129905819892883, Final Batch Loss: 0.06099322810769081\n",
      "Epoch 1526, Loss: 0.16484740749001503, Final Batch Loss: 0.10346119850873947\n",
      "Epoch 1527, Loss: 0.14033402130007744, Final Batch Loss: 0.0922706350684166\n",
      "Epoch 1528, Loss: 0.10281753540039062, Final Batch Loss: 0.07150337845087051\n",
      "Epoch 1529, Loss: 0.12574303895235062, Final Batch Loss: 0.06604240089654922\n",
      "Epoch 1530, Loss: 0.11423920840024948, Final Batch Loss: 0.06611335277557373\n",
      "Epoch 1531, Loss: 0.10765236988663673, Final Batch Loss: 0.05548827350139618\n",
      "Epoch 1532, Loss: 0.1384231597185135, Final Batch Loss: 0.06486152112483978\n",
      "Epoch 1533, Loss: 0.08894157037138939, Final Batch Loss: 0.05559803918004036\n",
      "Epoch 1534, Loss: 0.12996087223291397, Final Batch Loss: 0.06198088824748993\n",
      "Epoch 1535, Loss: 0.11431314423680305, Final Batch Loss: 0.04491739347577095\n",
      "Epoch 1536, Loss: 0.09769752621650696, Final Batch Loss: 0.05389225110411644\n",
      "Epoch 1537, Loss: 0.10352172330021858, Final Batch Loss: 0.07043731212615967\n",
      "Epoch 1538, Loss: 0.10337761789560318, Final Batch Loss: 0.056173525750637054\n",
      "Epoch 1539, Loss: 0.09819450601935387, Final Batch Loss: 0.04648717865347862\n",
      "Epoch 1540, Loss: 0.1206270158290863, Final Batch Loss: 0.0698707327246666\n",
      "Epoch 1541, Loss: 0.12209641933441162, Final Batch Loss: 0.062409620732069016\n",
      "Epoch 1542, Loss: 0.06583328358829021, Final Batch Loss: 0.03531753644347191\n",
      "Epoch 1543, Loss: 0.1131109856069088, Final Batch Loss: 0.041069548577070236\n",
      "Epoch 1544, Loss: 0.17731701582670212, Final Batch Loss: 0.10142264515161514\n",
      "Epoch 1545, Loss: 0.09128012508153915, Final Batch Loss: 0.0456596203148365\n",
      "Epoch 1546, Loss: 0.09281985834240913, Final Batch Loss: 0.043326497077941895\n",
      "Epoch 1547, Loss: 0.09514636546373367, Final Batch Loss: 0.04780280217528343\n",
      "Epoch 1548, Loss: 0.13329945877194405, Final Batch Loss: 0.04606268182396889\n",
      "Epoch 1549, Loss: 0.06890185363590717, Final Batch Loss: 0.028704123571515083\n",
      "Epoch 1550, Loss: 0.1537609063088894, Final Batch Loss: 0.10303334891796112\n",
      "Epoch 1551, Loss: 0.16459956765174866, Final Batch Loss: 0.12255044281482697\n",
      "Epoch 1552, Loss: 0.08389213308691978, Final Batch Loss: 0.05211914703249931\n",
      "Epoch 1553, Loss: 0.15559731796383858, Final Batch Loss: 0.1058589443564415\n",
      "Epoch 1554, Loss: 0.06518993340432644, Final Batch Loss: 0.03859531134366989\n",
      "Epoch 1555, Loss: 0.1500566229224205, Final Batch Loss: 0.08072167634963989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1556, Loss: 0.08690065890550613, Final Batch Loss: 0.04859178885817528\n",
      "Epoch 1557, Loss: 0.14961110055446625, Final Batch Loss: 0.06467951089143753\n",
      "Epoch 1558, Loss: 0.1322946548461914, Final Batch Loss: 0.08842606842517853\n",
      "Epoch 1559, Loss: 0.09035275876522064, Final Batch Loss: 0.04319377988576889\n",
      "Epoch 1560, Loss: 0.12922408431768417, Final Batch Loss: 0.08116951584815979\n",
      "Epoch 1561, Loss: 0.07298551499843597, Final Batch Loss: 0.029392465949058533\n",
      "Epoch 1562, Loss: 0.13376928865909576, Final Batch Loss: 0.03825758397579193\n",
      "Epoch 1563, Loss: 0.10891890898346901, Final Batch Loss: 0.046026136726140976\n",
      "Epoch 1564, Loss: 0.13798511028289795, Final Batch Loss: 0.04222913831472397\n",
      "Epoch 1565, Loss: 0.1340535432100296, Final Batch Loss: 0.0646505355834961\n",
      "Epoch 1566, Loss: 0.15068770200014114, Final Batch Loss: 0.06256764382123947\n",
      "Epoch 1567, Loss: 0.09856214746832848, Final Batch Loss: 0.05408621206879616\n",
      "Epoch 1568, Loss: 0.08495743945240974, Final Batch Loss: 0.048883069306612015\n",
      "Epoch 1569, Loss: 0.07684226892888546, Final Batch Loss: 0.04624409228563309\n",
      "Epoch 1570, Loss: 0.14895424246788025, Final Batch Loss: 0.0784509927034378\n",
      "Epoch 1571, Loss: 0.1517697386443615, Final Batch Loss: 0.10376095026731491\n",
      "Epoch 1572, Loss: 0.06097567826509476, Final Batch Loss: 0.022349487990140915\n",
      "Epoch 1573, Loss: 0.1114196926355362, Final Batch Loss: 0.07913893461227417\n",
      "Epoch 1574, Loss: 0.08608435839414597, Final Batch Loss: 0.04813334718346596\n",
      "Epoch 1575, Loss: 0.06847839802503586, Final Batch Loss: 0.034768205136060715\n",
      "Epoch 1576, Loss: 0.0845004990696907, Final Batch Loss: 0.04575787112116814\n",
      "Epoch 1577, Loss: 0.1328168287873268, Final Batch Loss: 0.053642161190509796\n",
      "Epoch 1578, Loss: 0.12364347279071808, Final Batch Loss: 0.06304524093866348\n",
      "Epoch 1579, Loss: 0.10320782661437988, Final Batch Loss: 0.04352341592311859\n",
      "Epoch 1580, Loss: 0.07553841173648834, Final Batch Loss: 0.04554711654782295\n",
      "Epoch 1581, Loss: 0.11006737872958183, Final Batch Loss: 0.03708840534090996\n",
      "Epoch 1582, Loss: 0.1390875019133091, Final Batch Loss: 0.09567683190107346\n",
      "Epoch 1583, Loss: 0.0898820050060749, Final Batch Loss: 0.04235389456152916\n",
      "Epoch 1584, Loss: 0.10469695553183556, Final Batch Loss: 0.04756946489214897\n",
      "Epoch 1585, Loss: 0.13805484771728516, Final Batch Loss: 0.09457065910100937\n",
      "Epoch 1586, Loss: 0.09621734544634819, Final Batch Loss: 0.022579994052648544\n",
      "Epoch 1587, Loss: 0.0747904758900404, Final Batch Loss: 0.022870780900120735\n",
      "Epoch 1588, Loss: 0.11942920461297035, Final Batch Loss: 0.06313667446374893\n",
      "Epoch 1589, Loss: 0.07970700040459633, Final Batch Loss: 0.03637189418077469\n",
      "Epoch 1590, Loss: 0.12732958421111107, Final Batch Loss: 0.0503435917198658\n",
      "Epoch 1591, Loss: 0.09941163286566734, Final Batch Loss: 0.06480836868286133\n",
      "Epoch 1592, Loss: 0.10258501023054123, Final Batch Loss: 0.02956036478281021\n",
      "Epoch 1593, Loss: 0.12621594220399857, Final Batch Loss: 0.08660805225372314\n",
      "Epoch 1594, Loss: 0.17418643832206726, Final Batch Loss: 0.0932883620262146\n",
      "Epoch 1595, Loss: 0.15749527141451836, Final Batch Loss: 0.12021738290786743\n",
      "Epoch 1596, Loss: 0.07141882739961147, Final Batch Loss: 0.055299121886491776\n",
      "Epoch 1597, Loss: 0.14529336988925934, Final Batch Loss: 0.0794479101896286\n",
      "Epoch 1598, Loss: 0.13084086030721664, Final Batch Loss: 0.0798129215836525\n",
      "Epoch 1599, Loss: 0.09781015664339066, Final Batch Loss: 0.03624100983142853\n",
      "Epoch 1600, Loss: 0.12321171164512634, Final Batch Loss: 0.06331047415733337\n",
      "Epoch 1601, Loss: 0.08610233291983604, Final Batch Loss: 0.048662640154361725\n",
      "Epoch 1602, Loss: 0.0667605847120285, Final Batch Loss: 0.033190760761499405\n",
      "Epoch 1603, Loss: 0.08247175440192223, Final Batch Loss: 0.039829179644584656\n",
      "Epoch 1604, Loss: 0.13992847502231598, Final Batch Loss: 0.07735848426818848\n",
      "Epoch 1605, Loss: 0.1004440151154995, Final Batch Loss: 0.06485389918088913\n",
      "Epoch 1606, Loss: 0.07815822027623653, Final Batch Loss: 0.026971353217959404\n",
      "Epoch 1607, Loss: 0.10655679553747177, Final Batch Loss: 0.04395429044961929\n",
      "Epoch 1608, Loss: 0.12179866060614586, Final Batch Loss: 0.03661840036511421\n",
      "Epoch 1609, Loss: 0.1092071682214737, Final Batch Loss: 0.045047514140605927\n",
      "Epoch 1610, Loss: 0.11104428768157959, Final Batch Loss: 0.03160392493009567\n",
      "Epoch 1611, Loss: 0.0925203263759613, Final Batch Loss: 0.04666246846318245\n",
      "Epoch 1612, Loss: 0.10395796224474907, Final Batch Loss: 0.03833348676562309\n",
      "Epoch 1613, Loss: 0.06308731902390718, Final Batch Loss: 0.014732074923813343\n",
      "Epoch 1614, Loss: 0.08284671977162361, Final Batch Loss: 0.02027030661702156\n",
      "Epoch 1615, Loss: 0.09984777122735977, Final Batch Loss: 0.04847465455532074\n",
      "Epoch 1616, Loss: 0.0949835553765297, Final Batch Loss: 0.059649355709552765\n",
      "Epoch 1617, Loss: 0.22496572136878967, Final Batch Loss: 0.15988384187221527\n",
      "Epoch 1618, Loss: 0.05987887643277645, Final Batch Loss: 0.03592217341065407\n",
      "Epoch 1619, Loss: 0.07731987908482552, Final Batch Loss: 0.0366722047328949\n",
      "Epoch 1620, Loss: 0.1225280687212944, Final Batch Loss: 0.053740695118904114\n",
      "Epoch 1621, Loss: 0.11270394176244736, Final Batch Loss: 0.024935945868492126\n",
      "Epoch 1622, Loss: 0.08202939108014107, Final Batch Loss: 0.021278545260429382\n",
      "Epoch 1623, Loss: 0.08115740679204464, Final Batch Loss: 0.0189743060618639\n",
      "Epoch 1624, Loss: 0.1299685537815094, Final Batch Loss: 0.06339351832866669\n",
      "Epoch 1625, Loss: 0.09398382157087326, Final Batch Loss: 0.058679938316345215\n",
      "Epoch 1626, Loss: 0.14274974167346954, Final Batch Loss: 0.06746891885995865\n",
      "Epoch 1627, Loss: 0.13496513664722443, Final Batch Loss: 0.07043556869029999\n",
      "Epoch 1628, Loss: 0.1413584165275097, Final Batch Loss: 0.0847659781575203\n",
      "Epoch 1629, Loss: 0.11399465426802635, Final Batch Loss: 0.04696546122431755\n",
      "Epoch 1630, Loss: 0.10617685317993164, Final Batch Loss: 0.06997109204530716\n",
      "Epoch 1631, Loss: 0.06908245384693146, Final Batch Loss: 0.03161615505814552\n",
      "Epoch 1632, Loss: 0.1080310121178627, Final Batch Loss: 0.04826652258634567\n",
      "Epoch 1633, Loss: 0.14444148913025856, Final Batch Loss: 0.05651847645640373\n",
      "Epoch 1634, Loss: 0.0830847080796957, Final Batch Loss: 0.05634527653455734\n",
      "Epoch 1635, Loss: 0.07968682795763016, Final Batch Loss: 0.04192226380109787\n",
      "Epoch 1636, Loss: 0.08812804892659187, Final Batch Loss: 0.038781698793172836\n",
      "Epoch 1637, Loss: 0.09539638832211494, Final Batch Loss: 0.04173031449317932\n",
      "Epoch 1638, Loss: 0.18821168690919876, Final Batch Loss: 0.1130320131778717\n",
      "Epoch 1639, Loss: 0.09866200387477875, Final Batch Loss: 0.04366264492273331\n",
      "Epoch 1640, Loss: 0.13398130610585213, Final Batch Loss: 0.1063435971736908\n",
      "Epoch 1641, Loss: 0.06927538849413395, Final Batch Loss: 0.03805197402834892\n",
      "Epoch 1642, Loss: 0.08076021447777748, Final Batch Loss: 0.03423309326171875\n",
      "Epoch 1643, Loss: 0.12296511977910995, Final Batch Loss: 0.052495360374450684\n",
      "Epoch 1644, Loss: 0.10106365755200386, Final Batch Loss: 0.04012465476989746\n",
      "Epoch 1645, Loss: 0.10745120793581009, Final Batch Loss: 0.02617478370666504\n",
      "Epoch 1646, Loss: 0.09264922887086868, Final Batch Loss: 0.04821103438735008\n",
      "Epoch 1647, Loss: 0.17028601467609406, Final Batch Loss: 0.09484027326107025\n",
      "Epoch 1648, Loss: 0.11800828948616982, Final Batch Loss: 0.06855139136314392\n",
      "Epoch 1649, Loss: 0.08929337374866009, Final Batch Loss: 0.024552589282393456\n",
      "Epoch 1650, Loss: 0.15005777031183243, Final Batch Loss: 0.07550359517335892\n",
      "Epoch 1651, Loss: 0.15802550315856934, Final Batch Loss: 0.12008511275053024\n",
      "Epoch 1652, Loss: 0.10084113478660583, Final Batch Loss: 0.05165752023458481\n",
      "Epoch 1653, Loss: 0.10849973931908607, Final Batch Loss: 0.04708373174071312\n",
      "Epoch 1654, Loss: 0.09116700850427151, Final Batch Loss: 0.06167437881231308\n",
      "Epoch 1655, Loss: 0.08460036292672157, Final Batch Loss: 0.035892974585294724\n",
      "Epoch 1656, Loss: 0.0937502533197403, Final Batch Loss: 0.05204220488667488\n",
      "Epoch 1657, Loss: 0.10454461723566055, Final Batch Loss: 0.04590762406587601\n",
      "Epoch 1658, Loss: 0.09752205014228821, Final Batch Loss: 0.03665624186396599\n",
      "Epoch 1659, Loss: 0.13355573266744614, Final Batch Loss: 0.09824560582637787\n",
      "Epoch 1660, Loss: 0.09353888221085072, Final Batch Loss: 0.030292579904198647\n",
      "Epoch 1661, Loss: 0.05770467035472393, Final Batch Loss: 0.017033355310559273\n",
      "Epoch 1662, Loss: 0.15458796918392181, Final Batch Loss: 0.055998288094997406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1663, Loss: 0.07909326627850533, Final Batch Loss: 0.03919553756713867\n",
      "Epoch 1664, Loss: 0.08903363905847073, Final Batch Loss: 0.025662599131464958\n",
      "Epoch 1665, Loss: 0.09780501201748848, Final Batch Loss: 0.033514562994241714\n",
      "Epoch 1666, Loss: 0.15464583039283752, Final Batch Loss: 0.07037919759750366\n",
      "Epoch 1667, Loss: 0.07774613425135612, Final Batch Loss: 0.03350834548473358\n",
      "Epoch 1668, Loss: 0.11731166392564774, Final Batch Loss: 0.04597581923007965\n",
      "Epoch 1669, Loss: 0.11654149368405342, Final Batch Loss: 0.0563143715262413\n",
      "Epoch 1670, Loss: 0.10346502438187599, Final Batch Loss: 0.03994990512728691\n",
      "Epoch 1671, Loss: 0.08679790049791336, Final Batch Loss: 0.04245981201529503\n",
      "Epoch 1672, Loss: 0.07692507654428482, Final Batch Loss: 0.04450661316514015\n",
      "Epoch 1673, Loss: 0.12430683895945549, Final Batch Loss: 0.05124970152974129\n",
      "Epoch 1674, Loss: 0.14090131968259811, Final Batch Loss: 0.03729639947414398\n",
      "Epoch 1675, Loss: 0.10416312143206596, Final Batch Loss: 0.06500507891178131\n",
      "Epoch 1676, Loss: 0.12562448531389236, Final Batch Loss: 0.06127539277076721\n",
      "Epoch 1677, Loss: 0.0848325677216053, Final Batch Loss: 0.057480648159980774\n",
      "Epoch 1678, Loss: 0.10349676012992859, Final Batch Loss: 0.06228655204176903\n",
      "Epoch 1679, Loss: 0.11269382014870644, Final Batch Loss: 0.06248066574335098\n",
      "Epoch 1680, Loss: 0.11044881492853165, Final Batch Loss: 0.07368557900190353\n",
      "Epoch 1681, Loss: 0.08486589230597019, Final Batch Loss: 0.029916727915406227\n",
      "Epoch 1682, Loss: 0.10207623615860939, Final Batch Loss: 0.04648229479789734\n",
      "Epoch 1683, Loss: 0.12746049836277962, Final Batch Loss: 0.07305651903152466\n",
      "Epoch 1684, Loss: 0.07623432576656342, Final Batch Loss: 0.03490794450044632\n",
      "Epoch 1685, Loss: 0.09630133956670761, Final Batch Loss: 0.056203003972768784\n",
      "Epoch 1686, Loss: 0.06524405814707279, Final Batch Loss: 0.029459835961461067\n",
      "Epoch 1687, Loss: 0.09366332367062569, Final Batch Loss: 0.02991742268204689\n",
      "Epoch 1688, Loss: 0.04141637682914734, Final Batch Loss: 0.02213749848306179\n",
      "Epoch 1689, Loss: 0.1346653401851654, Final Batch Loss: 0.06915447115898132\n",
      "Epoch 1690, Loss: 0.11981971561908722, Final Batch Loss: 0.0725061297416687\n",
      "Epoch 1691, Loss: 0.09476618841290474, Final Batch Loss: 0.048158589750528336\n",
      "Epoch 1692, Loss: 0.09232117608189583, Final Batch Loss: 0.05070304870605469\n",
      "Epoch 1693, Loss: 0.10403403639793396, Final Batch Loss: 0.05545131489634514\n",
      "Epoch 1694, Loss: 0.08152508176863194, Final Batch Loss: 0.026071002706885338\n",
      "Epoch 1695, Loss: 0.09117744863033295, Final Batch Loss: 0.05971909686923027\n",
      "Epoch 1696, Loss: 0.08706863224506378, Final Batch Loss: 0.050531160086393356\n",
      "Epoch 1697, Loss: 0.09391989931464195, Final Batch Loss: 0.04431204870343208\n",
      "Epoch 1698, Loss: 0.14892974495887756, Final Batch Loss: 0.09797494113445282\n",
      "Epoch 1699, Loss: 0.07480686716735363, Final Batch Loss: 0.024063052609562874\n",
      "Epoch 1700, Loss: 0.049667829647660255, Final Batch Loss: 0.022536123171448708\n",
      "Epoch 1701, Loss: 0.12533331289887428, Final Batch Loss: 0.06847076863050461\n",
      "Epoch 1702, Loss: 0.08866291306912899, Final Batch Loss: 0.02369118295609951\n",
      "Epoch 1703, Loss: 0.11878638714551926, Final Batch Loss: 0.06701245158910751\n",
      "Epoch 1704, Loss: 0.10400136560201645, Final Batch Loss: 0.056444570422172546\n",
      "Epoch 1705, Loss: 0.09731893241405487, Final Batch Loss: 0.045334625989198685\n",
      "Epoch 1706, Loss: 0.0560007318854332, Final Batch Loss: 0.020328659564256668\n",
      "Epoch 1707, Loss: 0.07347585447132587, Final Batch Loss: 0.04629531875252724\n",
      "Epoch 1708, Loss: 0.1638767048716545, Final Batch Loss: 0.09067849069833755\n",
      "Epoch 1709, Loss: 0.0873719397932291, Final Batch Loss: 0.02085820771753788\n",
      "Epoch 1710, Loss: 0.10055947303771973, Final Batch Loss: 0.05195004865527153\n",
      "Epoch 1711, Loss: 0.062145380303263664, Final Batch Loss: 0.028969554230570793\n",
      "Epoch 1712, Loss: 0.07796906307339668, Final Batch Loss: 0.0328679159283638\n",
      "Epoch 1713, Loss: 0.08684712648391724, Final Batch Loss: 0.05160295590758324\n",
      "Epoch 1714, Loss: 0.06462840549647808, Final Batch Loss: 0.02496344782412052\n",
      "Epoch 1715, Loss: 0.14347077161073685, Final Batch Loss: 0.04402034729719162\n",
      "Epoch 1716, Loss: 0.08410343155264854, Final Batch Loss: 0.04273536428809166\n",
      "Epoch 1717, Loss: 0.07021314557641745, Final Batch Loss: 0.05478370562195778\n",
      "Epoch 1718, Loss: 0.12429098039865494, Final Batch Loss: 0.04948000609874725\n",
      "Epoch 1719, Loss: 0.06720109283924103, Final Batch Loss: 0.03518710657954216\n",
      "Epoch 1720, Loss: 0.07513808459043503, Final Batch Loss: 0.035920556634664536\n",
      "Epoch 1721, Loss: 0.07015904597938061, Final Batch Loss: 0.02765214629471302\n",
      "Epoch 1722, Loss: 0.10952351242303848, Final Batch Loss: 0.07594592869281769\n",
      "Epoch 1723, Loss: 0.09865901619195938, Final Batch Loss: 0.04212965443730354\n",
      "Epoch 1724, Loss: 0.10272328555583954, Final Batch Loss: 0.04725346341729164\n",
      "Epoch 1725, Loss: 0.11790953204035759, Final Batch Loss: 0.03684689477086067\n",
      "Epoch 1726, Loss: 0.10179919749498367, Final Batch Loss: 0.039275139570236206\n",
      "Epoch 1727, Loss: 0.061957426369190216, Final Batch Loss: 0.03303436562418938\n",
      "Epoch 1728, Loss: 0.04563977010548115, Final Batch Loss: 0.017600441351532936\n",
      "Epoch 1729, Loss: 0.09828871488571167, Final Batch Loss: 0.0633997917175293\n",
      "Epoch 1730, Loss: 0.0831366553902626, Final Batch Loss: 0.018664881587028503\n",
      "Epoch 1731, Loss: 0.0750093050301075, Final Batch Loss: 0.043480098247528076\n",
      "Epoch 1732, Loss: 0.0840281005948782, Final Batch Loss: 0.02538381703197956\n",
      "Epoch 1733, Loss: 0.09879188612103462, Final Batch Loss: 0.06387883424758911\n",
      "Epoch 1734, Loss: 0.09159192442893982, Final Batch Loss: 0.041578005999326706\n",
      "Epoch 1735, Loss: 0.06865380331873894, Final Batch Loss: 0.031545910984277725\n",
      "Epoch 1736, Loss: 0.09022561088204384, Final Batch Loss: 0.039899006485939026\n",
      "Epoch 1737, Loss: 0.06746602058410645, Final Batch Loss: 0.03074069321155548\n",
      "Epoch 1738, Loss: 0.17099777981638908, Final Batch Loss: 0.1214887797832489\n",
      "Epoch 1739, Loss: 0.13262172043323517, Final Batch Loss: 0.0686253234744072\n",
      "Epoch 1740, Loss: 0.16763820499181747, Final Batch Loss: 0.13283492624759674\n",
      "Epoch 1741, Loss: 0.08575502038002014, Final Batch Loss: 0.0330413319170475\n",
      "Epoch 1742, Loss: 0.07693585008382797, Final Batch Loss: 0.05331036075949669\n",
      "Epoch 1743, Loss: 0.05223567225039005, Final Batch Loss: 0.024998551234602928\n",
      "Epoch 1744, Loss: 0.09863727167248726, Final Batch Loss: 0.050542473793029785\n",
      "Epoch 1745, Loss: 0.08246789872646332, Final Batch Loss: 0.04937990754842758\n",
      "Epoch 1746, Loss: 0.09325528889894485, Final Batch Loss: 0.030479051172733307\n",
      "Epoch 1747, Loss: 0.0921469759196043, Final Batch Loss: 0.029046697542071342\n",
      "Epoch 1748, Loss: 0.09318544715642929, Final Batch Loss: 0.040872037410736084\n",
      "Epoch 1749, Loss: 0.09058030694723129, Final Batch Loss: 0.04317953810095787\n",
      "Epoch 1750, Loss: 0.05858318880200386, Final Batch Loss: 0.03913978114724159\n",
      "Epoch 1751, Loss: 0.06458079814910889, Final Batch Loss: 0.01387198269367218\n",
      "Epoch 1752, Loss: 0.0789104700088501, Final Batch Loss: 0.05687330290675163\n",
      "Epoch 1753, Loss: 0.11843394860625267, Final Batch Loss: 0.08576676994562149\n",
      "Epoch 1754, Loss: 0.12933196872472763, Final Batch Loss: 0.09393593668937683\n",
      "Epoch 1755, Loss: 0.08522785268723965, Final Batch Loss: 0.024901563301682472\n",
      "Epoch 1756, Loss: 0.08902437798678875, Final Batch Loss: 0.030203474685549736\n",
      "Epoch 1757, Loss: 0.09843772649765015, Final Batch Loss: 0.048711735755205154\n",
      "Epoch 1758, Loss: 0.09862833470106125, Final Batch Loss: 0.023633025586605072\n",
      "Epoch 1759, Loss: 0.09424001350998878, Final Batch Loss: 0.039201151579618454\n",
      "Epoch 1760, Loss: 0.1001990232616663, Final Batch Loss: 0.023814814165234566\n",
      "Epoch 1761, Loss: 0.06709190830588341, Final Batch Loss: 0.030883364379405975\n",
      "Epoch 1762, Loss: 0.08688456565141678, Final Batch Loss: 0.05008741468191147\n",
      "Epoch 1763, Loss: 0.0654977597296238, Final Batch Loss: 0.031642161309719086\n",
      "Epoch 1764, Loss: 0.09752962924540043, Final Batch Loss: 0.06680138409137726\n",
      "Epoch 1765, Loss: 0.07822871208190918, Final Batch Loss: 0.03288073465228081\n",
      "Epoch 1766, Loss: 0.06981691159307957, Final Batch Loss: 0.0279704499989748\n",
      "Epoch 1767, Loss: 0.09067302569746971, Final Batch Loss: 0.039398301392793655\n",
      "Epoch 1768, Loss: 0.11357497051358223, Final Batch Loss: 0.03848423436284065\n",
      "Epoch 1769, Loss: 0.09662879258394241, Final Batch Loss: 0.07087769359350204\n",
      "Epoch 1770, Loss: 0.2183559685945511, Final Batch Loss: 0.08825783431529999\n",
      "Epoch 1771, Loss: 0.060614293441176414, Final Batch Loss: 0.03699535131454468\n",
      "Epoch 1772, Loss: 0.08126672357320786, Final Batch Loss: 0.0351291224360466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1773, Loss: 0.14424842596054077, Final Batch Loss: 0.06720947474241257\n",
      "Epoch 1774, Loss: 0.1281355507671833, Final Batch Loss: 0.09977550059556961\n",
      "Epoch 1775, Loss: 0.1052175834774971, Final Batch Loss: 0.049421392381191254\n",
      "Epoch 1776, Loss: 0.13999641314148903, Final Batch Loss: 0.05411858484148979\n",
      "Epoch 1777, Loss: 0.07042098045349121, Final Batch Loss: 0.018465381115674973\n",
      "Epoch 1778, Loss: 0.09488997235894203, Final Batch Loss: 0.05244370922446251\n",
      "Epoch 1779, Loss: 0.12091131880879402, Final Batch Loss: 0.06448619067668915\n",
      "Epoch 1780, Loss: 0.0832049660384655, Final Batch Loss: 0.05244375020265579\n",
      "Epoch 1781, Loss: 0.14279695227742195, Final Batch Loss: 0.08180952072143555\n",
      "Epoch 1782, Loss: 0.08939900249242783, Final Batch Loss: 0.03439553081989288\n",
      "Epoch 1783, Loss: 0.06602669507265091, Final Batch Loss: 0.02720049023628235\n",
      "Epoch 1784, Loss: 0.12644793465733528, Final Batch Loss: 0.057252150028944016\n",
      "Epoch 1785, Loss: 0.11867363378405571, Final Batch Loss: 0.06179983541369438\n",
      "Epoch 1786, Loss: 0.09768647700548172, Final Batch Loss: 0.03682273253798485\n",
      "Epoch 1787, Loss: 0.12178837135434151, Final Batch Loss: 0.0795368105173111\n",
      "Epoch 1788, Loss: 0.11765002086758614, Final Batch Loss: 0.05582745373249054\n",
      "Epoch 1789, Loss: 0.1033950001001358, Final Batch Loss: 0.06911221891641617\n",
      "Epoch 1790, Loss: 0.0990552194416523, Final Batch Loss: 0.031125400215387344\n",
      "Epoch 1791, Loss: 0.08449984341859818, Final Batch Loss: 0.047748714685440063\n",
      "Epoch 1792, Loss: 0.09085594303905964, Final Batch Loss: 0.029221365228295326\n",
      "Epoch 1793, Loss: 0.07137533091008663, Final Batch Loss: 0.04796283319592476\n",
      "Epoch 1794, Loss: 0.13090892881155014, Final Batch Loss: 0.08942031115293503\n",
      "Epoch 1795, Loss: 0.09479494020342827, Final Batch Loss: 0.04190843924880028\n",
      "Epoch 1796, Loss: 0.13880205899477005, Final Batch Loss: 0.08516912162303925\n",
      "Epoch 1797, Loss: 0.09560878202319145, Final Batch Loss: 0.037055373191833496\n",
      "Epoch 1798, Loss: 0.05666869692504406, Final Batch Loss: 0.0283378716558218\n",
      "Epoch 1799, Loss: 0.07031340710818768, Final Batch Loss: 0.02107529528439045\n",
      "Epoch 1800, Loss: 0.07375459000468254, Final Batch Loss: 0.038965482264757156\n",
      "Epoch 1801, Loss: 0.0702013373374939, Final Batch Loss: 0.04727400839328766\n",
      "Epoch 1802, Loss: 0.12061433494091034, Final Batch Loss: 0.07339515537023544\n",
      "Epoch 1803, Loss: 0.13402387872338295, Final Batch Loss: 0.07160959392786026\n",
      "Epoch 1804, Loss: 0.11209699511528015, Final Batch Loss: 0.0294807031750679\n",
      "Epoch 1805, Loss: 0.14342087507247925, Final Batch Loss: 0.09367858618497849\n",
      "Epoch 1806, Loss: 0.10473491623997688, Final Batch Loss: 0.05225657671689987\n",
      "Epoch 1807, Loss: 0.0635377150028944, Final Batch Loss: 0.045837368816137314\n",
      "Epoch 1808, Loss: 0.08639438450336456, Final Batch Loss: 0.05203855410218239\n",
      "Epoch 1809, Loss: 0.12078288197517395, Final Batch Loss: 0.05535305291414261\n",
      "Epoch 1810, Loss: 0.06877678819000721, Final Batch Loss: 0.028318768367171288\n",
      "Epoch 1811, Loss: 0.10662438720464706, Final Batch Loss: 0.0510040707886219\n",
      "Epoch 1812, Loss: 0.09737937152385712, Final Batch Loss: 0.055456966161727905\n",
      "Epoch 1813, Loss: 0.15077969059348106, Final Batch Loss: 0.11108848452568054\n",
      "Epoch 1814, Loss: 0.12033117562532425, Final Batch Loss: 0.08300098031759262\n",
      "Epoch 1815, Loss: 0.07116946298629045, Final Batch Loss: 0.015057933516800404\n",
      "Epoch 1816, Loss: 0.07670540921390057, Final Batch Loss: 0.030559679493308067\n",
      "Epoch 1817, Loss: 0.09332053363323212, Final Batch Loss: 0.05409751459956169\n",
      "Epoch 1818, Loss: 0.08496425300836563, Final Batch Loss: 0.031752847135066986\n",
      "Epoch 1819, Loss: 0.10874894633889198, Final Batch Loss: 0.032052572816610336\n",
      "Epoch 1820, Loss: 0.0814678817987442, Final Batch Loss: 0.04073535278439522\n",
      "Epoch 1821, Loss: 0.0653254073113203, Final Batch Loss: 0.028975551947951317\n",
      "Epoch 1822, Loss: 0.07966583780944347, Final Batch Loss: 0.05573876574635506\n",
      "Epoch 1823, Loss: 0.0717664323747158, Final Batch Loss: 0.027901723980903625\n",
      "Epoch 1824, Loss: 0.08094359748065472, Final Batch Loss: 0.030034957453608513\n",
      "Epoch 1825, Loss: 0.1113486997783184, Final Batch Loss: 0.07047808170318604\n",
      "Epoch 1826, Loss: 0.09783674404025078, Final Batch Loss: 0.04438178613781929\n",
      "Epoch 1827, Loss: 0.10023192316293716, Final Batch Loss: 0.06329476088285446\n",
      "Epoch 1828, Loss: 0.10185332223773003, Final Batch Loss: 0.031130466610193253\n",
      "Epoch 1829, Loss: 0.06513213366270065, Final Batch Loss: 0.04334120824933052\n",
      "Epoch 1830, Loss: 0.07954386249184608, Final Batch Loss: 0.03855171427130699\n",
      "Epoch 1831, Loss: 0.06429651752114296, Final Batch Loss: 0.02992100641131401\n",
      "Epoch 1832, Loss: 0.07424131035804749, Final Batch Loss: 0.02714468166232109\n",
      "Epoch 1833, Loss: 0.06640460714697838, Final Batch Loss: 0.039830856025218964\n",
      "Epoch 1834, Loss: 0.08157665655016899, Final Batch Loss: 0.018843013793230057\n",
      "Epoch 1835, Loss: 0.09835917130112648, Final Batch Loss: 0.05313653498888016\n",
      "Epoch 1836, Loss: 0.08575671538710594, Final Batch Loss: 0.03508160263299942\n",
      "Epoch 1837, Loss: 0.07937584444880486, Final Batch Loss: 0.03360937535762787\n",
      "Epoch 1838, Loss: 0.069345373660326, Final Batch Loss: 0.027278844267129898\n",
      "Epoch 1839, Loss: 0.060446202754974365, Final Batch Loss: 0.024822130799293518\n",
      "Epoch 1840, Loss: 0.06535732001066208, Final Batch Loss: 0.025356315076351166\n",
      "Epoch 1841, Loss: 0.05836805701255798, Final Batch Loss: 0.026761367917060852\n",
      "Epoch 1842, Loss: 0.06819360516965389, Final Batch Loss: 0.038326773792505264\n",
      "Epoch 1843, Loss: 0.040823498740792274, Final Batch Loss: 0.020685365423560143\n",
      "Epoch 1844, Loss: 0.06463073380291462, Final Batch Loss: 0.023858455941081047\n",
      "Epoch 1845, Loss: 0.10287274047732353, Final Batch Loss: 0.05496736243367195\n",
      "Epoch 1846, Loss: 0.10695945471525192, Final Batch Loss: 0.037498340010643005\n",
      "Epoch 1847, Loss: 0.08598899841308594, Final Batch Loss: 0.022045865654945374\n",
      "Epoch 1848, Loss: 0.05226245895028114, Final Batch Loss: 0.02178548090159893\n",
      "Epoch 1849, Loss: 0.09938597306609154, Final Batch Loss: 0.05753146857023239\n",
      "Epoch 1850, Loss: 0.1877385713160038, Final Batch Loss: 0.1429799497127533\n",
      "Epoch 1851, Loss: 0.13337822258472443, Final Batch Loss: 0.0409613698720932\n",
      "Epoch 1852, Loss: 0.07601403817534447, Final Batch Loss: 0.04163270443677902\n",
      "Epoch 1853, Loss: 0.08798395097255707, Final Batch Loss: 0.05791263282299042\n",
      "Epoch 1854, Loss: 0.11635095626115799, Final Batch Loss: 0.028094366192817688\n",
      "Epoch 1855, Loss: 0.07858579605817795, Final Batch Loss: 0.038973718881607056\n",
      "Epoch 1856, Loss: 0.09519626013934612, Final Batch Loss: 0.029092153534293175\n",
      "Epoch 1857, Loss: 0.08471680618822575, Final Batch Loss: 0.029411232098937035\n",
      "Epoch 1858, Loss: 0.08082398027181625, Final Batch Loss: 0.023998871445655823\n",
      "Epoch 1859, Loss: 0.09271367266774178, Final Batch Loss: 0.03406142443418503\n",
      "Epoch 1860, Loss: 0.0982983410358429, Final Batch Loss: 0.06111720949411392\n",
      "Epoch 1861, Loss: 0.08579965308308601, Final Batch Loss: 0.038638822734355927\n",
      "Epoch 1862, Loss: 0.1120496317744255, Final Batch Loss: 0.08226101100444794\n",
      "Epoch 1863, Loss: 0.09218421205878258, Final Batch Loss: 0.04697608947753906\n",
      "Epoch 1864, Loss: 0.09694797359406948, Final Batch Loss: 0.028319602832198143\n",
      "Epoch 1865, Loss: 0.10319819115102291, Final Batch Loss: 0.021049967035651207\n",
      "Epoch 1866, Loss: 0.04613892175257206, Final Batch Loss: 0.021075237542390823\n",
      "Epoch 1867, Loss: 0.05701873637735844, Final Batch Loss: 0.015578797087073326\n",
      "Epoch 1868, Loss: 0.10084120184183121, Final Batch Loss: 0.040237803012132645\n",
      "Epoch 1869, Loss: 0.09699352458119392, Final Batch Loss: 0.06284400820732117\n",
      "Epoch 1870, Loss: 0.10418455302715302, Final Batch Loss: 0.06296030431985855\n",
      "Epoch 1871, Loss: 0.09108362905681133, Final Batch Loss: 0.02718856744468212\n",
      "Epoch 1872, Loss: 0.09110327437520027, Final Batch Loss: 0.03712575137615204\n",
      "Epoch 1873, Loss: 0.09259286150336266, Final Batch Loss: 0.06514133512973785\n",
      "Epoch 1874, Loss: 0.03696801234036684, Final Batch Loss: 0.015143792144954205\n",
      "Epoch 1875, Loss: 0.0677183996886015, Final Batch Loss: 0.026564614847302437\n",
      "Epoch 1876, Loss: 0.0968495886772871, Final Batch Loss: 0.02238762192428112\n",
      "Epoch 1877, Loss: 0.08878626301884651, Final Batch Loss: 0.04664622247219086\n",
      "Epoch 1878, Loss: 0.0886369813233614, Final Batch Loss: 0.03086251951754093\n",
      "Epoch 1879, Loss: 0.08516787365078926, Final Batch Loss: 0.050323281437158585\n",
      "Epoch 1880, Loss: 0.044982015155255795, Final Batch Loss: 0.009905907325446606\n",
      "Epoch 1881, Loss: 0.11038104817271233, Final Batch Loss: 0.048462241888046265\n",
      "Epoch 1882, Loss: 0.08771462365984917, Final Batch Loss: 0.033501651138067245\n",
      "Epoch 1883, Loss: 0.10712803155183792, Final Batch Loss: 0.06959840655326843\n",
      "Epoch 1884, Loss: 0.06452505104243755, Final Batch Loss: 0.02522297389805317\n",
      "Epoch 1885, Loss: 0.08212394826114178, Final Batch Loss: 0.05234559252858162\n",
      "Epoch 1886, Loss: 0.21337496116757393, Final Batch Loss: 0.16754762828350067\n",
      "Epoch 1887, Loss: 0.1336219273507595, Final Batch Loss: 0.07350221276283264\n",
      "Epoch 1888, Loss: 0.059824682772159576, Final Batch Loss: 0.03622513264417648\n",
      "Epoch 1889, Loss: 0.07009205594658852, Final Batch Loss: 0.037225157022476196\n",
      "Epoch 1890, Loss: 0.0929696075618267, Final Batch Loss: 0.05054536834359169\n",
      "Epoch 1891, Loss: 0.11636478453874588, Final Batch Loss: 0.04425705224275589\n",
      "Epoch 1892, Loss: 0.061263952404260635, Final Batch Loss: 0.016581621021032333\n",
      "Epoch 1893, Loss: 0.07939726114273071, Final Batch Loss: 0.04447783902287483\n",
      "Epoch 1894, Loss: 0.06749511137604713, Final Batch Loss: 0.03014260157942772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1895, Loss: 0.07785734161734581, Final Batch Loss: 0.025454815477132797\n",
      "Epoch 1896, Loss: 0.06873998418450356, Final Batch Loss: 0.03653886541724205\n",
      "Epoch 1897, Loss: 0.07585938926786184, Final Batch Loss: 0.007737710140645504\n",
      "Epoch 1898, Loss: 0.1344517432153225, Final Batch Loss: 0.03737245127558708\n",
      "Epoch 1899, Loss: 0.11454875767230988, Final Batch Loss: 0.047929659485816956\n",
      "Epoch 1900, Loss: 0.05027296207845211, Final Batch Loss: 0.022474810481071472\n",
      "Epoch 1901, Loss: 0.0604968648403883, Final Batch Loss: 0.016696864739060402\n",
      "Epoch 1902, Loss: 0.10683354362845421, Final Batch Loss: 0.047499533742666245\n",
      "Epoch 1903, Loss: 0.07502500154078007, Final Batch Loss: 0.04726671800017357\n",
      "Epoch 1904, Loss: 0.10610702261328697, Final Batch Loss: 0.04886437579989433\n",
      "Epoch 1905, Loss: 0.0716930441558361, Final Batch Loss: 0.03240114077925682\n",
      "Epoch 1906, Loss: 0.08882855623960495, Final Batch Loss: 0.04205888882279396\n",
      "Epoch 1907, Loss: 0.08354301936924458, Final Batch Loss: 0.02802337519824505\n",
      "Epoch 1908, Loss: 0.11948125436902046, Final Batch Loss: 0.05147368833422661\n",
      "Epoch 1909, Loss: 0.08667139336466789, Final Batch Loss: 0.03304934501647949\n",
      "Epoch 1910, Loss: 0.05677649937570095, Final Batch Loss: 0.03110390342772007\n",
      "Epoch 1911, Loss: 0.12162062525749207, Final Batch Loss: 0.05222523212432861\n",
      "Epoch 1912, Loss: 0.057379940524697304, Final Batch Loss: 0.011778822168707848\n",
      "Epoch 1913, Loss: 0.08583839982748032, Final Batch Loss: 0.04512092098593712\n",
      "Epoch 1914, Loss: 0.0763334296643734, Final Batch Loss: 0.035032909363508224\n",
      "Epoch 1915, Loss: 0.04553170036524534, Final Batch Loss: 0.036044783890247345\n",
      "Epoch 1916, Loss: 0.10044078901410103, Final Batch Loss: 0.06703200936317444\n",
      "Epoch 1917, Loss: 0.11537406593561172, Final Batch Loss: 0.05115360766649246\n",
      "Epoch 1918, Loss: 0.08207795023918152, Final Batch Loss: 0.05230242386460304\n",
      "Epoch 1919, Loss: 0.08630171790719032, Final Batch Loss: 0.043734077364206314\n",
      "Epoch 1920, Loss: 0.11339389160275459, Final Batch Loss: 0.06461697071790695\n",
      "Epoch 1921, Loss: 0.16299447044730186, Final Batch Loss: 0.11385475099086761\n",
      "Epoch 1922, Loss: 0.0692178551107645, Final Batch Loss: 0.029862718656659126\n",
      "Epoch 1923, Loss: 0.07902483455836773, Final Batch Loss: 0.04939637705683708\n",
      "Epoch 1924, Loss: 0.05157742649316788, Final Batch Loss: 0.029942907392978668\n",
      "Epoch 1925, Loss: 0.09321065619587898, Final Batch Loss: 0.03143417090177536\n",
      "Epoch 1926, Loss: 0.12267972156405449, Final Batch Loss: 0.049510639160871506\n",
      "Epoch 1927, Loss: 0.11377190798521042, Final Batch Loss: 0.03872821480035782\n",
      "Epoch 1928, Loss: 0.06875914707779884, Final Batch Loss: 0.04006938263773918\n",
      "Epoch 1929, Loss: 0.08082651905715466, Final Batch Loss: 0.05337602645158768\n",
      "Epoch 1930, Loss: 0.09016876295208931, Final Batch Loss: 0.04558268189430237\n",
      "Epoch 1931, Loss: 0.10138271003961563, Final Batch Loss: 0.038068488240242004\n",
      "Epoch 1932, Loss: 0.0661531537771225, Final Batch Loss: 0.036313045769929886\n",
      "Epoch 1933, Loss: 0.06656706146895885, Final Batch Loss: 0.0463348887860775\n",
      "Epoch 1934, Loss: 0.09753284603357315, Final Batch Loss: 0.05254286900162697\n",
      "Epoch 1935, Loss: 0.11412276327610016, Final Batch Loss: 0.07822903990745544\n",
      "Epoch 1936, Loss: 0.06162148714065552, Final Batch Loss: 0.02236941084265709\n",
      "Epoch 1937, Loss: 0.09630433470010757, Final Batch Loss: 0.05229351669549942\n",
      "Epoch 1938, Loss: 0.07307174056768417, Final Batch Loss: 0.04140562564134598\n",
      "Epoch 1939, Loss: 0.08096775412559509, Final Batch Loss: 0.04409557208418846\n",
      "Epoch 1940, Loss: 0.12234754115343094, Final Batch Loss: 0.06429877132177353\n",
      "Epoch 1941, Loss: 0.09582030400633812, Final Batch Loss: 0.03915194794535637\n",
      "Epoch 1942, Loss: 0.09040993079543114, Final Batch Loss: 0.02899155393242836\n",
      "Epoch 1943, Loss: 0.07415728271007538, Final Batch Loss: 0.03415445238351822\n",
      "Epoch 1944, Loss: 0.06916872411966324, Final Batch Loss: 0.03686011582612991\n",
      "Epoch 1945, Loss: 0.08742548897862434, Final Batch Loss: 0.03470253199338913\n",
      "Epoch 1946, Loss: 0.08943778742104769, Final Batch Loss: 0.010953252203762531\n",
      "Epoch 1947, Loss: 0.09486931562423706, Final Batch Loss: 0.04844008386135101\n",
      "Epoch 1948, Loss: 0.11795397475361824, Final Batch Loss: 0.06634984165430069\n",
      "Epoch 1949, Loss: 0.060009412467479706, Final Batch Loss: 0.018834959715604782\n",
      "Epoch 1950, Loss: 0.11044353991746902, Final Batch Loss: 0.0345069020986557\n",
      "Epoch 1951, Loss: 0.13431429862976074, Final Batch Loss: 0.06618911027908325\n",
      "Epoch 1952, Loss: 0.09301915019750595, Final Batch Loss: 0.03360943868756294\n",
      "Epoch 1953, Loss: 0.04897235706448555, Final Batch Loss: 0.02144722267985344\n",
      "Epoch 1954, Loss: 0.10838047228753567, Final Batch Loss: 0.03030630759894848\n",
      "Epoch 1955, Loss: 0.11297183111310005, Final Batch Loss: 0.059372156858444214\n",
      "Epoch 1956, Loss: 0.11484276130795479, Final Batch Loss: 0.08045126497745514\n",
      "Epoch 1957, Loss: 0.09921820648014545, Final Batch Loss: 0.0711980015039444\n",
      "Epoch 1958, Loss: 0.081588851287961, Final Batch Loss: 0.026267794892191887\n",
      "Epoch 1959, Loss: 0.05166434869170189, Final Batch Loss: 0.02839476428925991\n",
      "Epoch 1960, Loss: 0.17297475785017014, Final Batch Loss: 0.07548867911100388\n",
      "Epoch 1961, Loss: 0.11556227877736092, Final Batch Loss: 0.08378244936466217\n",
      "Epoch 1962, Loss: 0.13013820350170135, Final Batch Loss: 0.07839164137840271\n",
      "Epoch 1963, Loss: 0.053809317760169506, Final Batch Loss: 0.012335467152297497\n",
      "Epoch 1964, Loss: 0.07238978892564774, Final Batch Loss: 0.029266629368066788\n",
      "Epoch 1965, Loss: 0.060955263674259186, Final Batch Loss: 0.03311289846897125\n",
      "Epoch 1966, Loss: 0.1410076580941677, Final Batch Loss: 0.10956045240163803\n",
      "Epoch 1967, Loss: 0.08839057199656963, Final Batch Loss: 0.06371176987886429\n",
      "Epoch 1968, Loss: 0.08281335234642029, Final Batch Loss: 0.028241779655218124\n",
      "Epoch 1969, Loss: 0.13696188479661942, Final Batch Loss: 0.05360467731952667\n",
      "Epoch 1970, Loss: 0.08746955916285515, Final Batch Loss: 0.059898462146520615\n",
      "Epoch 1971, Loss: 0.06772337481379509, Final Batch Loss: 0.03211965411901474\n",
      "Epoch 1972, Loss: 0.10804399289190769, Final Batch Loss: 0.07833102345466614\n",
      "Epoch 1973, Loss: 0.07766025327146053, Final Batch Loss: 0.05289904400706291\n",
      "Epoch 1974, Loss: 0.08024102449417114, Final Batch Loss: 0.04066839814186096\n",
      "Epoch 1975, Loss: 0.06924798712134361, Final Batch Loss: 0.03405655175447464\n",
      "Epoch 1976, Loss: 0.12044922634959221, Final Batch Loss: 0.08151953667402267\n",
      "Epoch 1977, Loss: 0.07975808158516884, Final Batch Loss: 0.043130502104759216\n",
      "Epoch 1978, Loss: 0.08275875449180603, Final Batch Loss: 0.04523115232586861\n",
      "Epoch 1979, Loss: 0.044450641609728336, Final Batch Loss: 0.013904725201427937\n",
      "Epoch 1980, Loss: 0.0736762024462223, Final Batch Loss: 0.03603135794401169\n",
      "Epoch 1981, Loss: 0.07061378285288811, Final Batch Loss: 0.03737131133675575\n",
      "Epoch 1982, Loss: 0.0833550225943327, Final Batch Loss: 0.05315055325627327\n",
      "Epoch 1983, Loss: 0.04422551020979881, Final Batch Loss: 0.023913685232400894\n",
      "Epoch 1984, Loss: 0.08523601666092873, Final Batch Loss: 0.03952841833233833\n",
      "Epoch 1985, Loss: 0.04726391285657883, Final Batch Loss: 0.018769387155771255\n",
      "Epoch 1986, Loss: 0.056055087596178055, Final Batch Loss: 0.019897233694791794\n",
      "Epoch 1987, Loss: 0.07536713778972626, Final Batch Loss: 0.0415952205657959\n",
      "Epoch 1988, Loss: 0.10994594171643257, Final Batch Loss: 0.04573821648955345\n",
      "Epoch 1989, Loss: 0.1216297447681427, Final Batch Loss: 0.06387089192867279\n",
      "Epoch 1990, Loss: 0.1435273438692093, Final Batch Loss: 0.07249562442302704\n",
      "Epoch 1991, Loss: 0.07472220622003078, Final Batch Loss: 0.021894587203860283\n",
      "Epoch 1992, Loss: 0.09881285950541496, Final Batch Loss: 0.0653291642665863\n",
      "Epoch 1993, Loss: 0.07691936753690243, Final Batch Loss: 0.030198687687516212\n",
      "Epoch 1994, Loss: 0.09888486564159393, Final Batch Loss: 0.037668365985155106\n",
      "Epoch 1995, Loss: 0.0794285349547863, Final Batch Loss: 0.04483562707901001\n",
      "Epoch 1996, Loss: 0.06769155338406563, Final Batch Loss: 0.03198900446295738\n",
      "Epoch 1997, Loss: 0.06165113113820553, Final Batch Loss: 0.030978403985500336\n",
      "Epoch 1998, Loss: 0.07480038050562143, Final Batch Loss: 0.014790804125368595\n",
      "Epoch 1999, Loss: 0.059102365747094154, Final Batch Loss: 0.029135411605238914\n",
      "Epoch 2000, Loss: 0.06585236825048923, Final Batch Loss: 0.027941269800066948\n",
      "Epoch 2001, Loss: 0.10529560595750809, Final Batch Loss: 0.0610651932656765\n",
      "Epoch 2002, Loss: 0.09375682845711708, Final Batch Loss: 0.06936850398778915\n",
      "Epoch 2003, Loss: 0.046823108568787575, Final Batch Loss: 0.01922096312046051\n",
      "Epoch 2004, Loss: 0.08279100246727467, Final Batch Loss: 0.02755570597946644\n",
      "Epoch 2005, Loss: 0.08849165216088295, Final Batch Loss: 0.0369843989610672\n",
      "Epoch 2006, Loss: 0.09342021122574806, Final Batch Loss: 0.049918003380298615\n",
      "Epoch 2007, Loss: 0.10677335783839226, Final Batch Loss: 0.04012319818139076\n",
      "Epoch 2008, Loss: 0.08569583296775818, Final Batch Loss: 0.03612842038273811\n",
      "Epoch 2009, Loss: 0.05976741574704647, Final Batch Loss: 0.024525662884116173\n",
      "Epoch 2010, Loss: 0.0905061773955822, Final Batch Loss: 0.03529660403728485\n",
      "Epoch 2011, Loss: 0.10053874179720879, Final Batch Loss: 0.04143290966749191\n",
      "Epoch 2012, Loss: 0.07616106979548931, Final Batch Loss: 0.020982587710022926\n",
      "Epoch 2013, Loss: 0.10039371438324451, Final Batch Loss: 0.07415906339883804\n",
      "Epoch 2014, Loss: 0.04836506303399801, Final Batch Loss: 0.033051781356334686\n",
      "Epoch 2015, Loss: 0.08744538575410843, Final Batch Loss: 0.05510164052248001\n",
      "Epoch 2016, Loss: 0.06964599341154099, Final Batch Loss: 0.028965875506401062\n",
      "Epoch 2017, Loss: 0.08896754682064056, Final Batch Loss: 0.03148042783141136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2018, Loss: 0.0667269378900528, Final Batch Loss: 0.04445284232497215\n",
      "Epoch 2019, Loss: 0.08948905393481255, Final Batch Loss: 0.04064388573169708\n",
      "Epoch 2020, Loss: 0.08048668876290321, Final Batch Loss: 0.029789648950099945\n",
      "Epoch 2021, Loss: 0.06447000056505203, Final Batch Loss: 0.03459957614541054\n",
      "Epoch 2022, Loss: 0.08386384882032871, Final Batch Loss: 0.025639815255999565\n",
      "Epoch 2023, Loss: 0.06117798760533333, Final Batch Loss: 0.025579985231161118\n",
      "Epoch 2024, Loss: 0.109762292355299, Final Batch Loss: 0.07654564827680588\n",
      "Epoch 2025, Loss: 0.05067021772265434, Final Batch Loss: 0.022390875965356827\n",
      "Epoch 2026, Loss: 0.0639066006988287, Final Batch Loss: 0.03482656925916672\n",
      "Epoch 2027, Loss: 0.0782509557902813, Final Batch Loss: 0.04912598431110382\n",
      "Epoch 2028, Loss: 0.08066458627581596, Final Batch Loss: 0.05679275095462799\n",
      "Epoch 2029, Loss: 0.12821515649557114, Final Batch Loss: 0.04232413321733475\n",
      "Epoch 2030, Loss: 0.07455013412982225, Final Batch Loss: 0.012186367996037006\n",
      "Epoch 2031, Loss: 0.16361644119024277, Final Batch Loss: 0.07211712002754211\n",
      "Epoch 2032, Loss: 0.04586019925773144, Final Batch Loss: 0.022670762613415718\n",
      "Epoch 2033, Loss: 0.07548468932509422, Final Batch Loss: 0.04506373405456543\n",
      "Epoch 2034, Loss: 0.08337930217385292, Final Batch Loss: 0.061449889093637466\n",
      "Epoch 2035, Loss: 0.14158718287944794, Final Batch Loss: 0.09413186460733414\n",
      "Epoch 2036, Loss: 0.045758405700325966, Final Batch Loss: 0.017722051590681076\n",
      "Epoch 2037, Loss: 0.10694310814142227, Final Batch Loss: 0.06914909929037094\n",
      "Epoch 2038, Loss: 0.08733348175883293, Final Batch Loss: 0.04121485352516174\n",
      "Epoch 2039, Loss: 0.06775987334549427, Final Batch Loss: 0.02872978337109089\n",
      "Epoch 2040, Loss: 0.03372015804052353, Final Batch Loss: 0.015839600935578346\n",
      "Epoch 2041, Loss: 0.07761454954743385, Final Batch Loss: 0.03628527373075485\n",
      "Epoch 2042, Loss: 0.05986739322543144, Final Batch Loss: 0.02693065255880356\n",
      "Epoch 2043, Loss: 0.11966685578227043, Final Batch Loss: 0.05082320049405098\n",
      "Epoch 2044, Loss: 0.07730790041387081, Final Batch Loss: 0.027019700035452843\n",
      "Epoch 2045, Loss: 0.07553618401288986, Final Batch Loss: 0.04865534231066704\n",
      "Epoch 2046, Loss: 0.08376114629209042, Final Batch Loss: 0.05957787111401558\n",
      "Epoch 2047, Loss: 0.07696860656142235, Final Batch Loss: 0.03214558959007263\n",
      "Epoch 2048, Loss: 0.03567343857139349, Final Batch Loss: 0.009995455853641033\n",
      "Epoch 2049, Loss: 0.06116560474038124, Final Batch Loss: 0.03446337580680847\n",
      "Epoch 2050, Loss: 0.06564240530133247, Final Batch Loss: 0.04233983904123306\n",
      "Epoch 2051, Loss: 0.08804892003536224, Final Batch Loss: 0.040026385337114334\n",
      "Epoch 2052, Loss: 0.08888898231089115, Final Batch Loss: 0.027729203924536705\n",
      "Epoch 2053, Loss: 0.09983737580478191, Final Batch Loss: 0.07251252233982086\n",
      "Epoch 2054, Loss: 0.04219079948961735, Final Batch Loss: 0.022548193112015724\n",
      "Epoch 2055, Loss: 0.0844196155667305, Final Batch Loss: 0.03734341636300087\n",
      "Epoch 2056, Loss: 0.15893658250570297, Final Batch Loss: 0.11459563672542572\n",
      "Epoch 2057, Loss: 0.08651284500956535, Final Batch Loss: 0.018411684781312943\n",
      "Epoch 2058, Loss: 0.06884535029530525, Final Batch Loss: 0.0401814766228199\n",
      "Epoch 2059, Loss: 0.07514457032084465, Final Batch Loss: 0.03791939839720726\n",
      "Epoch 2060, Loss: 0.09305433183908463, Final Batch Loss: 0.05233678221702576\n",
      "Epoch 2061, Loss: 0.09096131473779678, Final Batch Loss: 0.052186306565999985\n",
      "Epoch 2062, Loss: 0.08953281305730343, Final Batch Loss: 0.028598042204976082\n",
      "Epoch 2063, Loss: 0.1251002661883831, Final Batch Loss: 0.09040544927120209\n",
      "Epoch 2064, Loss: 0.05473199300467968, Final Batch Loss: 0.029314950108528137\n",
      "Epoch 2065, Loss: 0.04019246622920036, Final Batch Loss: 0.02111419290304184\n",
      "Epoch 2066, Loss: 0.12910506501793861, Final Batch Loss: 0.06859841197729111\n",
      "Epoch 2067, Loss: 0.06807058304548264, Final Batch Loss: 0.04624301940202713\n",
      "Epoch 2068, Loss: 0.07020008191466331, Final Batch Loss: 0.03323734179139137\n",
      "Epoch 2069, Loss: 0.05599716305732727, Final Batch Loss: 0.021158557385206223\n",
      "Epoch 2070, Loss: 0.05689804628491402, Final Batch Loss: 0.018224965780973434\n",
      "Epoch 2071, Loss: 0.10359206423163414, Final Batch Loss: 0.038847845047712326\n",
      "Epoch 2072, Loss: 0.04249081574380398, Final Batch Loss: 0.02368619106709957\n",
      "Epoch 2073, Loss: 0.07119008898735046, Final Batch Loss: 0.04152043163776398\n",
      "Epoch 2074, Loss: 0.07881153002381325, Final Batch Loss: 0.04392421990633011\n",
      "Epoch 2075, Loss: 0.033860393799841404, Final Batch Loss: 0.014124761335551739\n",
      "Epoch 2076, Loss: 0.08658978343009949, Final Batch Loss: 0.046852659434080124\n",
      "Epoch 2077, Loss: 0.04713496379554272, Final Batch Loss: 0.018355563282966614\n",
      "Epoch 2078, Loss: 0.16272931545972824, Final Batch Loss: 0.03169485181570053\n",
      "Epoch 2079, Loss: 0.058782221749424934, Final Batch Loss: 0.03752089664340019\n",
      "Epoch 2080, Loss: 0.12706182897090912, Final Batch Loss: 0.10858561843633652\n",
      "Epoch 2081, Loss: 0.06166938133537769, Final Batch Loss: 0.0378546342253685\n",
      "Epoch 2082, Loss: 0.05674672685563564, Final Batch Loss: 0.022308779880404472\n",
      "Epoch 2083, Loss: 0.06777622923254967, Final Batch Loss: 0.023040417581796646\n",
      "Epoch 2084, Loss: 0.07020503468811512, Final Batch Loss: 0.05027750879526138\n",
      "Epoch 2085, Loss: 0.0880747027695179, Final Batch Loss: 0.05333954840898514\n",
      "Epoch 2086, Loss: 0.08102861233055592, Final Batch Loss: 0.05340448021888733\n",
      "Epoch 2087, Loss: 0.05908597260713577, Final Batch Loss: 0.02744298428297043\n",
      "Epoch 2088, Loss: 0.030578698962926865, Final Batch Loss: 0.0121673084795475\n",
      "Epoch 2089, Loss: 0.07484344765543938, Final Batch Loss: 0.03388378396630287\n",
      "Epoch 2090, Loss: 0.0676582045853138, Final Batch Loss: 0.027466725558042526\n",
      "Epoch 2091, Loss: 0.10328596644103527, Final Batch Loss: 0.07235990464687347\n",
      "Epoch 2092, Loss: 0.06650846358388662, Final Batch Loss: 0.012785117141902447\n",
      "Epoch 2093, Loss: 0.07913796231150627, Final Batch Loss: 0.05657944083213806\n",
      "Epoch 2094, Loss: 0.1097472757101059, Final Batch Loss: 0.05502232536673546\n",
      "Epoch 2095, Loss: 0.08321784064173698, Final Batch Loss: 0.04582187533378601\n",
      "Epoch 2096, Loss: 0.05320507287979126, Final Batch Loss: 0.02631169557571411\n",
      "Epoch 2097, Loss: 0.05101892352104187, Final Batch Loss: 0.03181718289852142\n",
      "Epoch 2098, Loss: 0.07744942605495453, Final Batch Loss: 0.043439917266368866\n",
      "Epoch 2099, Loss: 0.040591126307845116, Final Batch Loss: 0.015308454632759094\n",
      "Epoch 2100, Loss: 0.07624765485525131, Final Batch Loss: 0.03151246905326843\n",
      "Epoch 2101, Loss: 0.075814763084054, Final Batch Loss: 0.02890344150364399\n",
      "Epoch 2102, Loss: 0.06900665909051895, Final Batch Loss: 0.036258403211832047\n",
      "Epoch 2103, Loss: 0.09302658028900623, Final Batch Loss: 0.030385838821530342\n",
      "Epoch 2104, Loss: 0.06382468901574612, Final Batch Loss: 0.03016890026628971\n",
      "Epoch 2105, Loss: 0.052714379504323006, Final Batch Loss: 0.03195744752883911\n",
      "Epoch 2106, Loss: 0.09175131097435951, Final Batch Loss: 0.039146538823843\n",
      "Epoch 2107, Loss: 0.06332113780081272, Final Batch Loss: 0.03374388441443443\n",
      "Epoch 2108, Loss: 0.10919713228940964, Final Batch Loss: 0.07188624143600464\n",
      "Epoch 2109, Loss: 0.0471277441829443, Final Batch Loss: 0.02234436199069023\n",
      "Epoch 2110, Loss: 0.08867554552853107, Final Batch Loss: 0.029105117544531822\n",
      "Epoch 2111, Loss: 0.07895242609083652, Final Batch Loss: 0.030393632128834724\n",
      "Epoch 2112, Loss: 0.10985596105456352, Final Batch Loss: 0.06322947889566422\n",
      "Epoch 2113, Loss: 0.07156779244542122, Final Batch Loss: 0.00935438647866249\n",
      "Epoch 2114, Loss: 0.09467804059386253, Final Batch Loss: 0.033327367156744\n",
      "Epoch 2115, Loss: 0.10840613767504692, Final Batch Loss: 0.05564358830451965\n",
      "Epoch 2116, Loss: 0.10905053094029427, Final Batch Loss: 0.03274368867278099\n",
      "Epoch 2117, Loss: 0.16096748411655426, Final Batch Loss: 0.09989965707063675\n",
      "Epoch 2118, Loss: 0.048628171905875206, Final Batch Loss: 0.029724396765232086\n",
      "Epoch 2119, Loss: 0.08496681228280067, Final Batch Loss: 0.053828004747629166\n",
      "Epoch 2120, Loss: 0.0707833282649517, Final Batch Loss: 0.039753448218107224\n",
      "Epoch 2121, Loss: 0.04167610593140125, Final Batch Loss: 0.016131194308400154\n",
      "Epoch 2122, Loss: 0.05357725918292999, Final Batch Loss: 0.03119882568717003\n",
      "Epoch 2123, Loss: 0.05161640606820583, Final Batch Loss: 0.019550764933228493\n",
      "Epoch 2124, Loss: 0.03823555260896683, Final Batch Loss: 0.015598546713590622\n",
      "Epoch 2125, Loss: 0.07165192067623138, Final Batch Loss: 0.03624405711889267\n",
      "Epoch 2126, Loss: 0.05840551294386387, Final Batch Loss: 0.025567634031176567\n",
      "Epoch 2127, Loss: 0.11189371719956398, Final Batch Loss: 0.049989063292741776\n",
      "Epoch 2128, Loss: 0.08307216316461563, Final Batch Loss: 0.03653593361377716\n",
      "Epoch 2129, Loss: 0.08275119587779045, Final Batch Loss: 0.03446970134973526\n",
      "Epoch 2130, Loss: 0.0681646466255188, Final Batch Loss: 0.02210944890975952\n",
      "Epoch 2131, Loss: 0.04389655031263828, Final Batch Loss: 0.027619680389761925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2132, Loss: 0.059396713972091675, Final Batch Loss: 0.02551049366593361\n",
      "Epoch 2133, Loss: 0.08951153233647346, Final Batch Loss: 0.036691050976514816\n",
      "Epoch 2134, Loss: 0.06667450815439224, Final Batch Loss: 0.03788130357861519\n",
      "Epoch 2135, Loss: 0.06471527181565762, Final Batch Loss: 0.02968146838247776\n",
      "Epoch 2136, Loss: 0.10402382910251617, Final Batch Loss: 0.032471463084220886\n",
      "Epoch 2137, Loss: 0.060108548030257225, Final Batch Loss: 0.0319991409778595\n",
      "Epoch 2138, Loss: 0.06883427873253822, Final Batch Loss: 0.03183434158563614\n",
      "Epoch 2139, Loss: 0.09806306473910809, Final Batch Loss: 0.08216701447963715\n",
      "Epoch 2140, Loss: 0.0432839784771204, Final Batch Loss: 0.015850871801376343\n",
      "Epoch 2141, Loss: 0.04661901108920574, Final Batch Loss: 0.02043677680194378\n",
      "Epoch 2142, Loss: 0.054014209657907486, Final Batch Loss: 0.031058931723237038\n",
      "Epoch 2143, Loss: 0.08835950866341591, Final Batch Loss: 0.04715140163898468\n",
      "Epoch 2144, Loss: 0.027033282443881035, Final Batch Loss: 0.013672974891960621\n",
      "Epoch 2145, Loss: 0.05323621816933155, Final Batch Loss: 0.019299400970339775\n",
      "Epoch 2146, Loss: 0.05907202698290348, Final Batch Loss: 0.040728610008955\n",
      "Epoch 2147, Loss: 0.0866711400449276, Final Batch Loss: 0.02863522619009018\n",
      "Epoch 2148, Loss: 0.08916348591446877, Final Batch Loss: 0.05918615683913231\n",
      "Epoch 2149, Loss: 0.12835635989904404, Final Batch Loss: 0.07888858765363693\n",
      "Epoch 2150, Loss: 0.07651820033788681, Final Batch Loss: 0.03751765564084053\n",
      "Epoch 2151, Loss: 0.07204704359173775, Final Batch Loss: 0.038942601531744\n",
      "Epoch 2152, Loss: 0.060191668570041656, Final Batch Loss: 0.016386061906814575\n",
      "Epoch 2153, Loss: 0.10117997974157333, Final Batch Loss: 0.03332623839378357\n",
      "Epoch 2154, Loss: 0.06093061342835426, Final Batch Loss: 0.022820252925157547\n",
      "Epoch 2155, Loss: 0.08669545873999596, Final Batch Loss: 0.01651838794350624\n",
      "Epoch 2156, Loss: 0.0632775779813528, Final Batch Loss: 0.04178836941719055\n",
      "Epoch 2157, Loss: 0.05840063840150833, Final Batch Loss: 0.028611822053790092\n",
      "Epoch 2158, Loss: 0.08324367925524712, Final Batch Loss: 0.04342206194996834\n",
      "Epoch 2159, Loss: 0.059826914221048355, Final Batch Loss: 0.038087405264377594\n",
      "Epoch 2160, Loss: 0.0719817765057087, Final Batch Loss: 0.03304056078195572\n",
      "Epoch 2161, Loss: 0.04070101119577885, Final Batch Loss: 0.017459826543927193\n",
      "Epoch 2162, Loss: 0.08946099411696196, Final Batch Loss: 0.013088519684970379\n",
      "Epoch 2163, Loss: 0.09206067398190498, Final Batch Loss: 0.03867803141474724\n",
      "Epoch 2164, Loss: 0.052587613463401794, Final Batch Loss: 0.033342160284519196\n",
      "Epoch 2165, Loss: 0.053047144785523415, Final Batch Loss: 0.028655655682086945\n",
      "Epoch 2166, Loss: 0.0776665098965168, Final Batch Loss: 0.03252730146050453\n",
      "Epoch 2167, Loss: 0.04296400211751461, Final Batch Loss: 0.02147841453552246\n",
      "Epoch 2168, Loss: 0.06306303292512894, Final Batch Loss: 0.01929638907313347\n",
      "Epoch 2169, Loss: 0.04843393713235855, Final Batch Loss: 0.02520877867937088\n",
      "Epoch 2170, Loss: 0.05763557739555836, Final Batch Loss: 0.0244490597397089\n",
      "Epoch 2171, Loss: 0.0785453375428915, Final Batch Loss: 0.030959641560912132\n",
      "Epoch 2172, Loss: 0.04730084724724293, Final Batch Loss: 0.01657838001847267\n",
      "Epoch 2173, Loss: 0.07431543432176113, Final Batch Loss: 0.05422302335500717\n",
      "Epoch 2174, Loss: 0.04885702207684517, Final Batch Loss: 0.02282780222594738\n",
      "Epoch 2175, Loss: 0.07051989063620567, Final Batch Loss: 0.033973611891269684\n",
      "Epoch 2176, Loss: 0.056384893134236336, Final Batch Loss: 0.03254399076104164\n",
      "Epoch 2177, Loss: 0.05744817852973938, Final Batch Loss: 0.03173021227121353\n",
      "Epoch 2178, Loss: 0.08783524110913277, Final Batch Loss: 0.03726958483457565\n",
      "Epoch 2179, Loss: 0.10171381570398808, Final Batch Loss: 0.017444150522351265\n",
      "Epoch 2180, Loss: 0.041788757778704166, Final Batch Loss: 0.011008783243596554\n",
      "Epoch 2181, Loss: 0.08821104280650616, Final Batch Loss: 0.021475648507475853\n",
      "Epoch 2182, Loss: 0.11082765460014343, Final Batch Loss: 0.07914600521326065\n",
      "Epoch 2183, Loss: 0.060267629101872444, Final Batch Loss: 0.03170076012611389\n",
      "Epoch 2184, Loss: 0.08968975953757763, Final Batch Loss: 0.015951072797179222\n",
      "Epoch 2185, Loss: 0.08758261986076832, Final Batch Loss: 0.059890229254961014\n",
      "Epoch 2186, Loss: 0.06722920946776867, Final Batch Loss: 0.03964035585522652\n",
      "Epoch 2187, Loss: 0.04930056631565094, Final Batch Loss: 0.01768692582845688\n",
      "Epoch 2188, Loss: 0.05300353467464447, Final Batch Loss: 0.02590414695441723\n",
      "Epoch 2189, Loss: 0.07131903618574142, Final Batch Loss: 0.032570015639066696\n",
      "Epoch 2190, Loss: 0.06868171691894531, Final Batch Loss: 0.04157274588942528\n",
      "Epoch 2191, Loss: 0.06739222444593906, Final Batch Loss: 0.019724441692233086\n",
      "Epoch 2192, Loss: 0.06715666316449642, Final Batch Loss: 0.03907676786184311\n",
      "Epoch 2193, Loss: 0.1581265963613987, Final Batch Loss: 0.12302106618881226\n",
      "Epoch 2194, Loss: 0.07988301292061806, Final Batch Loss: 0.02317153662443161\n",
      "Epoch 2195, Loss: 0.09357163310050964, Final Batch Loss: 0.05355833098292351\n",
      "Epoch 2196, Loss: 0.039008334279060364, Final Batch Loss: 0.0245586559176445\n",
      "Epoch 2197, Loss: 0.08327226527035236, Final Batch Loss: 0.027978507801890373\n",
      "Epoch 2198, Loss: 0.0767747089266777, Final Batch Loss: 0.04101061075925827\n",
      "Epoch 2199, Loss: 0.06486168317496777, Final Batch Loss: 0.0395345464348793\n",
      "Epoch 2200, Loss: 0.07017557322978973, Final Batch Loss: 0.03739745914936066\n",
      "Epoch 2201, Loss: 0.06121481955051422, Final Batch Loss: 0.018562007695436478\n",
      "Epoch 2202, Loss: 0.06765180826187134, Final Batch Loss: 0.03357971832156181\n",
      "Epoch 2203, Loss: 0.03956522420048714, Final Batch Loss: 0.016042200848460197\n",
      "Epoch 2204, Loss: 0.1092914305627346, Final Batch Loss: 0.07706614583730698\n",
      "Epoch 2205, Loss: 0.0655360072851181, Final Batch Loss: 0.032596562057733536\n",
      "Epoch 2206, Loss: 0.05151309631764889, Final Batch Loss: 0.0176627729088068\n",
      "Epoch 2207, Loss: 0.1143139898777008, Final Batch Loss: 0.06538796424865723\n",
      "Epoch 2208, Loss: 0.028053080663084984, Final Batch Loss: 0.015580389648675919\n",
      "Epoch 2209, Loss: 0.09234751760959625, Final Batch Loss: 0.06429804116487503\n",
      "Epoch 2210, Loss: 0.08752430975437164, Final Batch Loss: 0.05638977512717247\n",
      "Epoch 2211, Loss: 0.07792526111006737, Final Batch Loss: 0.023562680929899216\n",
      "Epoch 2212, Loss: 0.05621114932000637, Final Batch Loss: 0.02123579941689968\n",
      "Epoch 2213, Loss: 0.06485509686172009, Final Batch Loss: 0.021677302196621895\n",
      "Epoch 2214, Loss: 0.048953608609735966, Final Batch Loss: 0.01377392653375864\n",
      "Epoch 2215, Loss: 0.12041285820305347, Final Batch Loss: 0.09231756627559662\n",
      "Epoch 2216, Loss: 0.1305365450680256, Final Batch Loss: 0.08726480603218079\n",
      "Epoch 2217, Loss: 0.1011098250746727, Final Batch Loss: 0.032388314604759216\n",
      "Epoch 2218, Loss: 0.05486117862164974, Final Batch Loss: 0.011263975873589516\n",
      "Epoch 2219, Loss: 0.10250976122915745, Final Batch Loss: 0.07392419129610062\n",
      "Epoch 2220, Loss: 0.07247662357985973, Final Batch Loss: 0.028562361374497414\n",
      "Epoch 2221, Loss: 0.05756734684109688, Final Batch Loss: 0.027127515524625778\n",
      "Epoch 2222, Loss: 0.05003690905869007, Final Batch Loss: 0.02574325166642666\n",
      "Epoch 2223, Loss: 0.042814526706933975, Final Batch Loss: 0.011746089905500412\n",
      "Epoch 2224, Loss: 0.10204693675041199, Final Batch Loss: 0.06546919047832489\n",
      "Epoch 2225, Loss: 0.05161401815712452, Final Batch Loss: 0.03394879773259163\n",
      "Epoch 2226, Loss: 0.08021651953458786, Final Batch Loss: 0.04293319955468178\n",
      "Epoch 2227, Loss: 0.07745207659900188, Final Batch Loss: 0.04815998300909996\n",
      "Epoch 2228, Loss: 0.05497513711452484, Final Batch Loss: 0.03099985234439373\n",
      "Epoch 2229, Loss: 0.06885726749897003, Final Batch Loss: 0.04441054165363312\n",
      "Epoch 2230, Loss: 0.0724613219499588, Final Batch Loss: 0.05497375875711441\n",
      "Epoch 2231, Loss: 0.05667056143283844, Final Batch Loss: 0.03141365572810173\n",
      "Epoch 2232, Loss: 0.04565557278692722, Final Batch Loss: 0.03022787906229496\n",
      "Epoch 2233, Loss: 0.07860561087727547, Final Batch Loss: 0.038498036563396454\n",
      "Epoch 2234, Loss: 0.07952362298965454, Final Batch Loss: 0.05233798176050186\n",
      "Epoch 2235, Loss: 0.06290404684841633, Final Batch Loss: 0.015853023156523705\n",
      "Epoch 2236, Loss: 0.04346110485494137, Final Batch Loss: 0.022479332983493805\n",
      "Epoch 2237, Loss: 0.03861173056066036, Final Batch Loss: 0.02235972322523594\n",
      "Epoch 2238, Loss: 0.10672837495803833, Final Batch Loss: 0.07495256513357162\n",
      "Epoch 2239, Loss: 0.06210687384009361, Final Batch Loss: 0.041396137326955795\n",
      "Epoch 2240, Loss: 0.04039732180535793, Final Batch Loss: 0.011095672845840454\n",
      "Epoch 2241, Loss: 0.048165151849389076, Final Batch Loss: 0.029847845435142517\n",
      "Epoch 2242, Loss: 0.06329339556396008, Final Batch Loss: 0.03554382920265198\n",
      "Epoch 2243, Loss: 0.06710211746394634, Final Batch Loss: 0.018183613196015358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2244, Loss: 0.05322570540010929, Final Batch Loss: 0.01952315680682659\n",
      "Epoch 2245, Loss: 0.06820404343307018, Final Batch Loss: 0.041646137833595276\n",
      "Epoch 2246, Loss: 0.0737451333552599, Final Batch Loss: 0.04613521322607994\n",
      "Epoch 2247, Loss: 0.050663061439991, Final Batch Loss: 0.020468050613999367\n",
      "Epoch 2248, Loss: 0.05330749321728945, Final Batch Loss: 0.014040623791515827\n",
      "Epoch 2249, Loss: 0.05715351365506649, Final Batch Loss: 0.026294052600860596\n",
      "Epoch 2250, Loss: 0.05063993111252785, Final Batch Loss: 0.019689099863171577\n",
      "Epoch 2251, Loss: 0.07111812010407448, Final Batch Loss: 0.04322665184736252\n",
      "Epoch 2252, Loss: 0.051222520880401134, Final Batch Loss: 0.03928407281637192\n",
      "Epoch 2253, Loss: 0.06900272890925407, Final Batch Loss: 0.029059462249279022\n",
      "Epoch 2254, Loss: 0.05442915018647909, Final Batch Loss: 0.012650533579289913\n",
      "Epoch 2255, Loss: 0.061044711619615555, Final Batch Loss: 0.03163381665945053\n",
      "Epoch 2256, Loss: 0.09944403544068336, Final Batch Loss: 0.04820401221513748\n",
      "Epoch 2257, Loss: 0.07382932677865028, Final Batch Loss: 0.029768720269203186\n",
      "Epoch 2258, Loss: 0.05330489855259657, Final Batch Loss: 0.014428107999265194\n",
      "Epoch 2259, Loss: 0.061020368710160255, Final Batch Loss: 0.021767331287264824\n",
      "Epoch 2260, Loss: 0.0465999748557806, Final Batch Loss: 0.020346151664853096\n",
      "Epoch 2261, Loss: 0.03669071942567825, Final Batch Loss: 0.01220938190817833\n",
      "Epoch 2262, Loss: 0.08113020285964012, Final Batch Loss: 0.036732662469148636\n",
      "Epoch 2263, Loss: 0.0676028560847044, Final Batch Loss: 0.014565782621502876\n",
      "Epoch 2264, Loss: 0.08926475420594215, Final Batch Loss: 0.024835605174303055\n",
      "Epoch 2265, Loss: 0.04710541293025017, Final Batch Loss: 0.027887877076864243\n",
      "Epoch 2266, Loss: 0.04244901239871979, Final Batch Loss: 0.023786511272192\n",
      "Epoch 2267, Loss: 0.05609430745244026, Final Batch Loss: 0.021271847188472748\n",
      "Epoch 2268, Loss: 0.12044870853424072, Final Batch Loss: 0.07336967438459396\n",
      "Epoch 2269, Loss: 0.05092658847570419, Final Batch Loss: 0.024752149358391762\n",
      "Epoch 2270, Loss: 0.039543911814689636, Final Batch Loss: 0.013105178251862526\n",
      "Epoch 2271, Loss: 0.07547397539019585, Final Batch Loss: 0.04352441430091858\n",
      "Epoch 2272, Loss: 0.07348886504769325, Final Batch Loss: 0.05517144128680229\n",
      "Epoch 2273, Loss: 0.07777716591954231, Final Batch Loss: 0.04621149227023125\n",
      "Epoch 2274, Loss: 0.06337053328752518, Final Batch Loss: 0.037831954658031464\n",
      "Epoch 2275, Loss: 0.06418300606310368, Final Batch Loss: 0.040773771703243256\n",
      "Epoch 2276, Loss: 0.06007624976336956, Final Batch Loss: 0.020490678027272224\n",
      "Epoch 2277, Loss: 0.060977503657341, Final Batch Loss: 0.025171831250190735\n",
      "Epoch 2278, Loss: 0.0795906987041235, Final Batch Loss: 0.02062155492603779\n",
      "Epoch 2279, Loss: 0.0455218181014061, Final Batch Loss: 0.0193667933344841\n",
      "Epoch 2280, Loss: 0.06641192175447941, Final Batch Loss: 0.035782549530267715\n",
      "Epoch 2281, Loss: 0.060530826449394226, Final Batch Loss: 0.030930103734135628\n",
      "Epoch 2282, Loss: 0.05529703199863434, Final Batch Loss: 0.029093660414218903\n",
      "Epoch 2283, Loss: 0.0335371270775795, Final Batch Loss: 0.014597212895751\n",
      "Epoch 2284, Loss: 0.15241585671901703, Final Batch Loss: 0.11265898495912552\n",
      "Epoch 2285, Loss: 0.061973607167601585, Final Batch Loss: 0.031965043395757675\n",
      "Epoch 2286, Loss: 0.05049164779484272, Final Batch Loss: 0.03238553926348686\n",
      "Epoch 2287, Loss: 0.05630719102919102, Final Batch Loss: 0.02673369087278843\n",
      "Epoch 2288, Loss: 0.08278496563434601, Final Batch Loss: 0.03339313715696335\n",
      "Epoch 2289, Loss: 0.06560551002621651, Final Batch Loss: 0.02168237790465355\n",
      "Epoch 2290, Loss: 0.06767209619283676, Final Batch Loss: 0.02692822739481926\n",
      "Epoch 2291, Loss: 0.06476145796477795, Final Batch Loss: 0.029570462182164192\n",
      "Epoch 2292, Loss: 0.09387241303920746, Final Batch Loss: 0.04737843573093414\n",
      "Epoch 2293, Loss: 0.1170522402971983, Final Batch Loss: 0.08881229162216187\n",
      "Epoch 2294, Loss: 0.06062578596174717, Final Batch Loss: 0.03758925199508667\n",
      "Epoch 2295, Loss: 0.04363696090877056, Final Batch Loss: 0.01758124865591526\n",
      "Epoch 2296, Loss: 0.05075695179402828, Final Batch Loss: 0.03458400443196297\n",
      "Epoch 2297, Loss: 0.09512589499354362, Final Batch Loss: 0.05865933373570442\n",
      "Epoch 2298, Loss: 0.05533953756093979, Final Batch Loss: 0.036052338778972626\n",
      "Epoch 2299, Loss: 0.10676135867834091, Final Batch Loss: 0.0494992733001709\n",
      "Epoch 2300, Loss: 0.051145512610673904, Final Batch Loss: 0.02694319374859333\n",
      "Epoch 2301, Loss: 0.047030819579958916, Final Batch Loss: 0.013589153066277504\n",
      "Epoch 2302, Loss: 0.036880649626255035, Final Batch Loss: 0.021638739854097366\n",
      "Epoch 2303, Loss: 0.05762987397611141, Final Batch Loss: 0.04092206433415413\n",
      "Epoch 2304, Loss: 0.09596182405948639, Final Batch Loss: 0.06955435127019882\n",
      "Epoch 2305, Loss: 0.050083102658391, Final Batch Loss: 0.018092917278409004\n",
      "Epoch 2306, Loss: 0.038100589998066425, Final Batch Loss: 0.015016998164355755\n",
      "Epoch 2307, Loss: 0.08538825809955597, Final Batch Loss: 0.061988603323698044\n",
      "Epoch 2308, Loss: 0.05794830620288849, Final Batch Loss: 0.03084336221218109\n",
      "Epoch 2309, Loss: 0.04016777686774731, Final Batch Loss: 0.023625168949365616\n",
      "Epoch 2310, Loss: 0.061513904482126236, Final Batch Loss: 0.034699805080890656\n",
      "Epoch 2311, Loss: 0.0629352517426014, Final Batch Loss: 0.021958615630865097\n",
      "Epoch 2312, Loss: 0.08153300732374191, Final Batch Loss: 0.050116799771785736\n",
      "Epoch 2313, Loss: 0.0637521967291832, Final Batch Loss: 0.041459519416093826\n",
      "Epoch 2314, Loss: 0.051637016236782074, Final Batch Loss: 0.01265224814414978\n",
      "Epoch 2315, Loss: 0.049800289794802666, Final Batch Loss: 0.030679067596793175\n",
      "Epoch 2316, Loss: 0.04915023781359196, Final Batch Loss: 0.027510901913046837\n",
      "Epoch 2317, Loss: 0.09649655595421791, Final Batch Loss: 0.0423123836517334\n",
      "Epoch 2318, Loss: 0.05841984041035175, Final Batch Loss: 0.037578847259283066\n",
      "Epoch 2319, Loss: 0.0408049039542675, Final Batch Loss: 0.013706747442483902\n",
      "Epoch 2320, Loss: 0.12941046059131622, Final Batch Loss: 0.08207353949546814\n",
      "Epoch 2321, Loss: 0.03207249753177166, Final Batch Loss: 0.01639534905552864\n",
      "Epoch 2322, Loss: 0.03954337723553181, Final Batch Loss: 0.024899864569306374\n",
      "Epoch 2323, Loss: 0.07768837735056877, Final Batch Loss: 0.03681991249322891\n",
      "Epoch 2324, Loss: 0.04193194955587387, Final Batch Loss: 0.02065395750105381\n",
      "Epoch 2325, Loss: 0.08004762977361679, Final Batch Loss: 0.04039718583226204\n",
      "Epoch 2326, Loss: 0.06736244075000286, Final Batch Loss: 0.03655210882425308\n",
      "Epoch 2327, Loss: 0.04648379981517792, Final Batch Loss: 0.017294516786932945\n",
      "Epoch 2328, Loss: 0.06847739964723587, Final Batch Loss: 0.01712164655327797\n",
      "Epoch 2329, Loss: 0.06542962230741978, Final Batch Loss: 0.021352572366595268\n",
      "Epoch 2330, Loss: 0.08348043635487556, Final Batch Loss: 0.0398440882563591\n",
      "Epoch 2331, Loss: 0.052186742424964905, Final Batch Loss: 0.01952281966805458\n",
      "Epoch 2332, Loss: 0.045236825942993164, Final Batch Loss: 0.01967167854309082\n",
      "Epoch 2333, Loss: 0.10865575075149536, Final Batch Loss: 0.061964549124240875\n",
      "Epoch 2334, Loss: 0.04872538894414902, Final Batch Loss: 0.012996815145015717\n",
      "Epoch 2335, Loss: 0.04081145394593477, Final Batch Loss: 0.011091277934610844\n",
      "Epoch 2336, Loss: 0.08224300853908062, Final Batch Loss: 0.05510031804442406\n",
      "Epoch 2337, Loss: 0.05865427851676941, Final Batch Loss: 0.04022263363003731\n",
      "Epoch 2338, Loss: 0.030346542596817017, Final Batch Loss: 0.02066243439912796\n",
      "Epoch 2339, Loss: 0.07736369036138058, Final Batch Loss: 0.0230617206543684\n",
      "Epoch 2340, Loss: 0.055553700774908066, Final Batch Loss: 0.029479438439011574\n",
      "Epoch 2341, Loss: 0.0632429039105773, Final Batch Loss: 0.008466425351798534\n",
      "Epoch 2342, Loss: 0.06651170551776886, Final Batch Loss: 0.04111150652170181\n",
      "Epoch 2343, Loss: 0.09353126212954521, Final Batch Loss: 0.04423663392663002\n",
      "Epoch 2344, Loss: 0.0856587179005146, Final Batch Loss: 0.05195267125964165\n",
      "Epoch 2345, Loss: 0.04464663937687874, Final Batch Loss: 0.019613591954112053\n",
      "Epoch 2346, Loss: 0.05550667084753513, Final Batch Loss: 0.024432703852653503\n",
      "Epoch 2347, Loss: 0.10599276795983315, Final Batch Loss: 0.080622598528862\n",
      "Epoch 2348, Loss: 0.0831254180520773, Final Batch Loss: 0.024743525311350822\n",
      "Epoch 2349, Loss: 0.06018118932843208, Final Batch Loss: 0.02147519588470459\n",
      "Epoch 2350, Loss: 0.047952670603990555, Final Batch Loss: 0.02468184381723404\n",
      "Epoch 2351, Loss: 0.10559607669711113, Final Batch Loss: 0.06375756859779358\n",
      "Epoch 2352, Loss: 0.06207348592579365, Final Batch Loss: 0.03242499753832817\n",
      "Epoch 2353, Loss: 0.05854296125471592, Final Batch Loss: 0.03152349218726158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2354, Loss: 0.06095955893397331, Final Batch Loss: 0.03161880746483803\n",
      "Epoch 2355, Loss: 0.09307362139225006, Final Batch Loss: 0.02576296776533127\n",
      "Epoch 2356, Loss: 0.087107228115201, Final Batch Loss: 0.05636834725737572\n",
      "Epoch 2357, Loss: 0.02654851321130991, Final Batch Loss: 0.01291747484356165\n",
      "Epoch 2358, Loss: 0.05625844560563564, Final Batch Loss: 0.034281495958566666\n",
      "Epoch 2359, Loss: 0.05102188512682915, Final Batch Loss: 0.025406723842024803\n",
      "Epoch 2360, Loss: 0.0403964864090085, Final Batch Loss: 0.01488864328712225\n",
      "Epoch 2361, Loss: 0.04119892045855522, Final Batch Loss: 0.024316029623150826\n",
      "Epoch 2362, Loss: 0.0722633097320795, Final Batch Loss: 0.05213422700762749\n",
      "Epoch 2363, Loss: 0.05705799721181393, Final Batch Loss: 0.02391381375491619\n",
      "Epoch 2364, Loss: 0.05945247784256935, Final Batch Loss: 0.017925821244716644\n",
      "Epoch 2365, Loss: 0.07990135252475739, Final Batch Loss: 0.03808274120092392\n",
      "Epoch 2366, Loss: 0.07098557241261005, Final Batch Loss: 0.02052164636552334\n",
      "Epoch 2367, Loss: 0.05933814123272896, Final Batch Loss: 0.03695985674858093\n",
      "Epoch 2368, Loss: 0.06523935496807098, Final Batch Loss: 0.04382802173495293\n",
      "Epoch 2369, Loss: 0.07666578143835068, Final Batch Loss: 0.040110994130373\n",
      "Epoch 2370, Loss: 0.05658126808702946, Final Batch Loss: 0.015121085569262505\n",
      "Epoch 2371, Loss: 0.07900921255350113, Final Batch Loss: 0.046365272253751755\n",
      "Epoch 2372, Loss: 0.0452132485806942, Final Batch Loss: 0.021673351526260376\n",
      "Epoch 2373, Loss: 0.0993204340338707, Final Batch Loss: 0.05973316356539726\n",
      "Epoch 2374, Loss: 0.05246110446751118, Final Batch Loss: 0.030983418226242065\n",
      "Epoch 2375, Loss: 0.03481398429721594, Final Batch Loss: 0.02627716027200222\n",
      "Epoch 2376, Loss: 0.04860332980751991, Final Batch Loss: 0.023410353809595108\n",
      "Epoch 2377, Loss: 0.08004309795796871, Final Batch Loss: 0.019348369911313057\n",
      "Epoch 2378, Loss: 0.09396640211343765, Final Batch Loss: 0.05681011453270912\n",
      "Epoch 2379, Loss: 0.03694002889096737, Final Batch Loss: 0.016618812456727028\n",
      "Epoch 2380, Loss: 0.08828283473849297, Final Batch Loss: 0.04430680349469185\n",
      "Epoch 2381, Loss: 0.06206496246159077, Final Batch Loss: 0.019447294995188713\n",
      "Epoch 2382, Loss: 0.03880291059613228, Final Batch Loss: 0.01761777326464653\n",
      "Epoch 2383, Loss: 0.06000974774360657, Final Batch Loss: 0.018273789435625076\n",
      "Epoch 2384, Loss: 0.05363599490374327, Final Batch Loss: 0.00918492954224348\n",
      "Epoch 2385, Loss: 0.07174626551568508, Final Batch Loss: 0.01955498568713665\n",
      "Epoch 2386, Loss: 0.0693006943911314, Final Batch Loss: 0.017811110243201256\n",
      "Epoch 2387, Loss: 0.06159006990492344, Final Batch Loss: 0.028225554153323174\n",
      "Epoch 2388, Loss: 0.07761995494365692, Final Batch Loss: 0.018307406455278397\n",
      "Epoch 2389, Loss: 0.14565003663301468, Final Batch Loss: 0.12667334079742432\n",
      "Epoch 2390, Loss: 0.08364452794194221, Final Batch Loss: 0.0424371100962162\n",
      "Epoch 2391, Loss: 0.06022149324417114, Final Batch Loss: 0.023895319551229477\n",
      "Epoch 2392, Loss: 0.05895250290632248, Final Batch Loss: 0.03558630496263504\n",
      "Epoch 2393, Loss: 0.0590092483907938, Final Batch Loss: 0.021206943318247795\n",
      "Epoch 2394, Loss: 0.08408444747328758, Final Batch Loss: 0.049924399703741074\n",
      "Epoch 2395, Loss: 0.14372390136122704, Final Batch Loss: 0.05566785857081413\n",
      "Epoch 2396, Loss: 0.0792871005833149, Final Batch Loss: 0.03607649356126785\n",
      "Epoch 2397, Loss: 0.10142812691628933, Final Batch Loss: 0.07428006082773209\n",
      "Epoch 2398, Loss: 0.06413358263671398, Final Batch Loss: 0.022223317995667458\n",
      "Epoch 2399, Loss: 0.07282793521881104, Final Batch Loss: 0.03215532377362251\n",
      "Epoch 2400, Loss: 0.06113061308860779, Final Batch Loss: 0.020535312592983246\n",
      "Epoch 2401, Loss: 0.06751349568367004, Final Batch Loss: 0.032859183847904205\n",
      "Epoch 2402, Loss: 0.026561723090708256, Final Batch Loss: 0.014232303015887737\n",
      "Epoch 2403, Loss: 0.07084328308701515, Final Batch Loss: 0.04442726448178291\n",
      "Epoch 2404, Loss: 0.04714020900428295, Final Batch Loss: 0.011623749509453773\n",
      "Epoch 2405, Loss: 0.05628103204071522, Final Batch Loss: 0.03136337548494339\n",
      "Epoch 2406, Loss: 0.039766425266861916, Final Batch Loss: 0.013779034838080406\n",
      "Epoch 2407, Loss: 0.0768883116543293, Final Batch Loss: 0.04312608018517494\n",
      "Epoch 2408, Loss: 0.06681137531995773, Final Batch Loss: 0.02248762920498848\n",
      "Epoch 2409, Loss: 0.036467598751187325, Final Batch Loss: 0.014564450830221176\n",
      "Epoch 2410, Loss: 0.035735067911446095, Final Batch Loss: 0.027078256011009216\n",
      "Epoch 2411, Loss: 0.06420060060918331, Final Batch Loss: 0.02820410393178463\n",
      "Epoch 2412, Loss: 0.12204773351550102, Final Batch Loss: 0.05987038090825081\n",
      "Epoch 2413, Loss: 0.04877427965402603, Final Batch Loss: 0.014852222055196762\n",
      "Epoch 2414, Loss: 0.07996725663542747, Final Batch Loss: 0.040674831718206406\n",
      "Epoch 2415, Loss: 0.05716606415808201, Final Batch Loss: 0.028741175308823586\n",
      "Epoch 2416, Loss: 0.059618812054395676, Final Batch Loss: 0.02169494330883026\n",
      "Epoch 2417, Loss: 0.04796771518886089, Final Batch Loss: 0.022357989102602005\n",
      "Epoch 2418, Loss: 0.037251521833240986, Final Batch Loss: 0.022359976544976234\n",
      "Epoch 2419, Loss: 0.052293505519628525, Final Batch Loss: 0.02035403624176979\n",
      "Epoch 2420, Loss: 0.04371896665543318, Final Batch Loss: 0.03009537234902382\n",
      "Epoch 2421, Loss: 0.050330743193626404, Final Batch Loss: 0.024037668481469154\n",
      "Epoch 2422, Loss: 0.08069733716547489, Final Batch Loss: 0.03008858673274517\n",
      "Epoch 2423, Loss: 0.04572300612926483, Final Batch Loss: 0.026112450286746025\n",
      "Epoch 2424, Loss: 0.07828421238809824, Final Batch Loss: 0.0641220211982727\n",
      "Epoch 2425, Loss: 0.05890679731965065, Final Batch Loss: 0.03191251680254936\n",
      "Epoch 2426, Loss: 0.05974544957280159, Final Batch Loss: 0.04256901890039444\n",
      "Epoch 2427, Loss: 0.16560395993292332, Final Batch Loss: 0.13876007497310638\n",
      "Epoch 2428, Loss: 0.06877715326845646, Final Batch Loss: 0.03018910624086857\n",
      "Epoch 2429, Loss: 0.05696597136557102, Final Batch Loss: 0.021453412249684334\n",
      "Epoch 2430, Loss: 0.0481235533952713, Final Batch Loss: 0.03205767646431923\n",
      "Epoch 2431, Loss: 0.11990343406796455, Final Batch Loss: 0.061761341989040375\n",
      "Epoch 2432, Loss: 0.07011255994439125, Final Batch Loss: 0.05417467653751373\n",
      "Epoch 2433, Loss: 0.053849393501877785, Final Batch Loss: 0.022423507645726204\n",
      "Epoch 2434, Loss: 0.07020030915737152, Final Batch Loss: 0.03748657926917076\n",
      "Epoch 2435, Loss: 0.0797474067658186, Final Batch Loss: 0.020355219021439552\n",
      "Epoch 2436, Loss: 0.03827719483524561, Final Batch Loss: 0.022779392078518867\n",
      "Epoch 2437, Loss: 0.09162257611751556, Final Batch Loss: 0.03204337880015373\n",
      "Epoch 2438, Loss: 0.05253441445529461, Final Batch Loss: 0.034115102142095566\n",
      "Epoch 2439, Loss: 0.06870950013399124, Final Batch Loss: 0.048707179725170135\n",
      "Epoch 2440, Loss: 0.08014427497982979, Final Batch Loss: 0.037929847836494446\n",
      "Epoch 2441, Loss: 0.05555524677038193, Final Batch Loss: 0.03296031057834625\n",
      "Epoch 2442, Loss: 0.07287724688649178, Final Batch Loss: 0.02452801913022995\n",
      "Epoch 2443, Loss: 0.0534744318574667, Final Batch Loss: 0.019462687894701958\n",
      "Epoch 2444, Loss: 0.042445702478289604, Final Batch Loss: 0.013888850808143616\n",
      "Epoch 2445, Loss: 0.05586386099457741, Final Batch Loss: 0.03667965903878212\n",
      "Epoch 2446, Loss: 0.035417476668953896, Final Batch Loss: 0.008892491459846497\n",
      "Epoch 2447, Loss: 0.05448534153401852, Final Batch Loss: 0.022401900961995125\n",
      "Epoch 2448, Loss: 0.15473778173327446, Final Batch Loss: 0.1044733077287674\n",
      "Epoch 2449, Loss: 0.04367115534842014, Final Batch Loss: 0.018671106547117233\n",
      "Epoch 2450, Loss: 0.08358094468712807, Final Batch Loss: 0.06768453866243362\n",
      "Epoch 2451, Loss: 0.11564686521887779, Final Batch Loss: 0.07975193113088608\n",
      "Epoch 2452, Loss: 0.11570286005735397, Final Batch Loss: 0.036637187004089355\n",
      "Epoch 2453, Loss: 0.061167264357209206, Final Batch Loss: 0.01474490575492382\n",
      "Epoch 2454, Loss: 0.04836398921906948, Final Batch Loss: 0.029744135215878487\n",
      "Epoch 2455, Loss: 0.04510393366217613, Final Batch Loss: 0.012296691536903381\n",
      "Epoch 2456, Loss: 0.048340460285544395, Final Batch Loss: 0.030529817566275597\n",
      "Epoch 2457, Loss: 0.0723345447331667, Final Batch Loss: 0.01795012690126896\n",
      "Epoch 2458, Loss: 0.07154300063848495, Final Batch Loss: 0.043717171996831894\n",
      "Epoch 2459, Loss: 0.05987831391394138, Final Batch Loss: 0.02109946496784687\n",
      "Epoch 2460, Loss: 0.059421394020318985, Final Batch Loss: 0.02489267662167549\n",
      "Epoch 2461, Loss: 0.09478790313005447, Final Batch Loss: 0.027434825897216797\n",
      "Epoch 2462, Loss: 0.03327155113220215, Final Batch Loss: 0.012668229639530182\n",
      "Epoch 2463, Loss: 0.03810236509889364, Final Batch Loss: 0.013715512119233608\n",
      "Epoch 2464, Loss: 0.05962561443448067, Final Batch Loss: 0.01721670851111412\n",
      "Epoch 2465, Loss: 0.03650587610900402, Final Batch Loss: 0.021014852449297905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2466, Loss: 0.06003272347152233, Final Batch Loss: 0.0360712967813015\n",
      "Epoch 2467, Loss: 0.0683122631162405, Final Batch Loss: 0.024066226556897163\n",
      "Epoch 2468, Loss: 0.04056844487786293, Final Batch Loss: 0.01431385800242424\n",
      "Epoch 2469, Loss: 0.09041196666657925, Final Batch Loss: 0.07573069632053375\n",
      "Epoch 2470, Loss: 0.06315759941935539, Final Batch Loss: 0.034946080297231674\n",
      "Epoch 2471, Loss: 0.0332370987161994, Final Batch Loss: 0.020421160385012627\n",
      "Epoch 2472, Loss: 0.06247994862496853, Final Batch Loss: 0.031713467091321945\n",
      "Epoch 2473, Loss: 0.03398781828582287, Final Batch Loss: 0.012257805094122887\n",
      "Epoch 2474, Loss: 0.03692559897899628, Final Batch Loss: 0.012081082910299301\n",
      "Epoch 2475, Loss: 0.055475047789514065, Final Batch Loss: 0.010091344825923443\n",
      "Epoch 2476, Loss: 0.04558388888835907, Final Batch Loss: 0.025277040898799896\n",
      "Epoch 2477, Loss: 0.03542249649763107, Final Batch Loss: 0.010623963549733162\n",
      "Epoch 2478, Loss: 0.13553524389863014, Final Batch Loss: 0.10818643867969513\n",
      "Epoch 2479, Loss: 0.04630155302584171, Final Batch Loss: 0.025352809578180313\n",
      "Epoch 2480, Loss: 0.0750437080860138, Final Batch Loss: 0.03798411041498184\n",
      "Epoch 2481, Loss: 0.060437265783548355, Final Batch Loss: 0.0339832678437233\n",
      "Epoch 2482, Loss: 0.027520560659468174, Final Batch Loss: 0.012763689272105694\n",
      "Epoch 2483, Loss: 0.02867936622351408, Final Batch Loss: 0.014039319008588791\n",
      "Epoch 2484, Loss: 0.04469752870500088, Final Batch Loss: 0.018346186727285385\n",
      "Epoch 2485, Loss: 0.07402213662862778, Final Batch Loss: 0.02331840991973877\n",
      "Epoch 2486, Loss: 0.11505218595266342, Final Batch Loss: 0.06479060649871826\n",
      "Epoch 2487, Loss: 0.04763838090002537, Final Batch Loss: 0.0202798955142498\n",
      "Epoch 2488, Loss: 0.04222026467323303, Final Batch Loss: 0.017262106761336327\n",
      "Epoch 2489, Loss: 0.036574799567461014, Final Batch Loss: 0.02046089805662632\n",
      "Epoch 2490, Loss: 0.0494802538305521, Final Batch Loss: 0.02874951809644699\n",
      "Epoch 2491, Loss: 0.061678383499383926, Final Batch Loss: 0.04379955306649208\n",
      "Epoch 2492, Loss: 0.06780778430402279, Final Batch Loss: 0.044577158987522125\n",
      "Epoch 2493, Loss: 0.0833921879529953, Final Batch Loss: 0.03894272446632385\n",
      "Epoch 2494, Loss: 0.11772807687520981, Final Batch Loss: 0.07895635813474655\n",
      "Epoch 2495, Loss: 0.04785787407308817, Final Batch Loss: 0.009341486729681492\n",
      "Epoch 2496, Loss: 0.03943337965756655, Final Batch Loss: 0.011367385275661945\n",
      "Epoch 2497, Loss: 0.05370307061821222, Final Batch Loss: 0.014723305590450764\n",
      "Epoch 2498, Loss: 0.07213641330599785, Final Batch Loss: 0.022831011563539505\n",
      "Epoch 2499, Loss: 0.0603775754570961, Final Batch Loss: 0.019136372953653336\n",
      "Epoch 2500, Loss: 0.06564797274768353, Final Batch Loss: 0.029720840975642204\n",
      "Epoch 2501, Loss: 0.04520652815699577, Final Batch Loss: 0.026634812355041504\n",
      "Epoch 2502, Loss: 0.06044313497841358, Final Batch Loss: 0.022765392437577248\n",
      "Epoch 2503, Loss: 0.07907405123114586, Final Batch Loss: 0.03571466729044914\n",
      "Epoch 2504, Loss: 0.03282905276864767, Final Batch Loss: 0.010688315145671368\n",
      "Epoch 2505, Loss: 0.04563211835920811, Final Batch Loss: 0.02611107937991619\n",
      "Epoch 2506, Loss: 0.09210095182061195, Final Batch Loss: 0.06291650235652924\n",
      "Epoch 2507, Loss: 0.059032756835222244, Final Batch Loss: 0.03395579382777214\n",
      "Epoch 2508, Loss: 0.049084728583693504, Final Batch Loss: 0.02730581723153591\n",
      "Epoch 2509, Loss: 0.04402519203722477, Final Batch Loss: 0.015603817999362946\n",
      "Epoch 2510, Loss: 0.0577074121683836, Final Batch Loss: 0.019316213205456734\n",
      "Epoch 2511, Loss: 0.05152058228850365, Final Batch Loss: 0.039432212710380554\n",
      "Epoch 2512, Loss: 0.04309152252972126, Final Batch Loss: 0.02129736915230751\n",
      "Epoch 2513, Loss: 0.03460054937750101, Final Batch Loss: 0.01300037745386362\n",
      "Epoch 2514, Loss: 0.07404132559895515, Final Batch Loss: 0.04244860261678696\n",
      "Epoch 2515, Loss: 0.05314746871590614, Final Batch Loss: 0.03306432440876961\n",
      "Epoch 2516, Loss: 0.08719558268785477, Final Batch Loss: 0.05026043951511383\n",
      "Epoch 2517, Loss: 0.04693477973341942, Final Batch Loss: 0.02415143884718418\n",
      "Epoch 2518, Loss: 0.03210079483687878, Final Batch Loss: 0.011845007538795471\n",
      "Epoch 2519, Loss: 0.04903801716864109, Final Batch Loss: 0.025888439267873764\n",
      "Epoch 2520, Loss: 0.07365327887237072, Final Batch Loss: 0.02880897931754589\n",
      "Epoch 2521, Loss: 0.06062197871506214, Final Batch Loss: 0.03824623301625252\n",
      "Epoch 2522, Loss: 0.09130657091736794, Final Batch Loss: 0.025702763348817825\n",
      "Epoch 2523, Loss: 0.08032291941344738, Final Batch Loss: 0.061382975429296494\n",
      "Epoch 2524, Loss: 0.03402697481215, Final Batch Loss: 0.015932289883494377\n",
      "Epoch 2525, Loss: 0.03337719012051821, Final Batch Loss: 0.012596041895449162\n",
      "Epoch 2526, Loss: 0.06434954144060612, Final Batch Loss: 0.049363818019628525\n",
      "Epoch 2527, Loss: 0.0623445026576519, Final Batch Loss: 0.032805509865283966\n",
      "Epoch 2528, Loss: 0.11957793310284615, Final Batch Loss: 0.0607510507106781\n",
      "Epoch 2529, Loss: 0.040535484440624714, Final Batch Loss: 0.025492912158370018\n",
      "Epoch 2530, Loss: 0.03492381423711777, Final Batch Loss: 0.015971694141626358\n",
      "Epoch 2531, Loss: 0.04511144757270813, Final Batch Loss: 0.015298442915081978\n",
      "Epoch 2532, Loss: 0.03774645924568176, Final Batch Loss: 0.021715208888053894\n",
      "Epoch 2533, Loss: 0.0896691270172596, Final Batch Loss: 0.07083820551633835\n",
      "Epoch 2534, Loss: 0.07825391367077827, Final Batch Loss: 0.03557825833559036\n",
      "Epoch 2535, Loss: 0.025001968257129192, Final Batch Loss: 0.012097380124032497\n",
      "Epoch 2536, Loss: 0.03625761158764362, Final Batch Loss: 0.011904845014214516\n",
      "Epoch 2537, Loss: 0.03931261785328388, Final Batch Loss: 0.020789330825209618\n",
      "Epoch 2538, Loss: 0.04628049395978451, Final Batch Loss: 0.029764557257294655\n",
      "Epoch 2539, Loss: 0.06538216024637222, Final Batch Loss: 0.025763805955648422\n",
      "Epoch 2540, Loss: 0.049088018015027046, Final Batch Loss: 0.03090529516339302\n",
      "Epoch 2541, Loss: 0.042525697499513626, Final Batch Loss: 0.01870337873697281\n",
      "Epoch 2542, Loss: 0.04978805407881737, Final Batch Loss: 0.011657260358333588\n",
      "Epoch 2543, Loss: 0.059241900220513344, Final Batch Loss: 0.03160565719008446\n",
      "Epoch 2544, Loss: 0.04880994465202093, Final Batch Loss: 0.011356364004313946\n",
      "Epoch 2545, Loss: 0.08609794825315475, Final Batch Loss: 0.0660327896475792\n",
      "Epoch 2546, Loss: 0.04494866728782654, Final Batch Loss: 0.016259275376796722\n",
      "Epoch 2547, Loss: 0.05459008179605007, Final Batch Loss: 0.024164462462067604\n",
      "Epoch 2548, Loss: 0.03318747133016586, Final Batch Loss: 0.0237704087048769\n",
      "Epoch 2549, Loss: 0.0423014834523201, Final Batch Loss: 0.024970486760139465\n",
      "Epoch 2550, Loss: 0.049706959165632725, Final Batch Loss: 0.015491706319153309\n",
      "Epoch 2551, Loss: 0.07153646275401115, Final Batch Loss: 0.04532899335026741\n",
      "Epoch 2552, Loss: 0.03482687473297119, Final Batch Loss: 0.01745314709842205\n",
      "Epoch 2553, Loss: 0.06531349569559097, Final Batch Loss: 0.016435090452432632\n",
      "Epoch 2554, Loss: 0.03643864579498768, Final Batch Loss: 0.018407978117465973\n",
      "Epoch 2555, Loss: 0.07205671444535255, Final Batch Loss: 0.031917132437229156\n",
      "Epoch 2556, Loss: 0.03879323787987232, Final Batch Loss: 0.01041167601943016\n",
      "Epoch 2557, Loss: 0.13345664367079735, Final Batch Loss: 0.11791998893022537\n",
      "Epoch 2558, Loss: 0.09080092422664165, Final Batch Loss: 0.07061544060707092\n",
      "Epoch 2559, Loss: 0.05856651812791824, Final Batch Loss: 0.011362221091985703\n",
      "Epoch 2560, Loss: 0.029786097817122936, Final Batch Loss: 0.018039533868432045\n",
      "Epoch 2561, Loss: 0.07320534065365791, Final Batch Loss: 0.05330674350261688\n",
      "Epoch 2562, Loss: 0.04295864887535572, Final Batch Loss: 0.022921472787857056\n",
      "Epoch 2563, Loss: 0.0672621801495552, Final Batch Loss: 0.029279939830303192\n",
      "Epoch 2564, Loss: 0.058532098308205605, Final Batch Loss: 0.019444333389401436\n",
      "Epoch 2565, Loss: 0.04552749078720808, Final Batch Loss: 0.031825486570596695\n",
      "Epoch 2566, Loss: 0.05934087373316288, Final Batch Loss: 0.04216662794351578\n",
      "Epoch 2567, Loss: 0.0358852194622159, Final Batch Loss: 0.022142887115478516\n",
      "Epoch 2568, Loss: 0.03667224431410432, Final Batch Loss: 0.005512182135134935\n",
      "Epoch 2569, Loss: 0.1694315830245614, Final Batch Loss: 0.15531617403030396\n",
      "Epoch 2570, Loss: 0.04857087694108486, Final Batch Loss: 0.03053233027458191\n",
      "Epoch 2571, Loss: 0.028008902445435524, Final Batch Loss: 0.014936983585357666\n",
      "Epoch 2572, Loss: 0.06600682437419891, Final Batch Loss: 0.04761888086795807\n",
      "Epoch 2573, Loss: 0.03907237388193607, Final Batch Loss: 0.020284781232476234\n",
      "Epoch 2574, Loss: 0.04611973278224468, Final Batch Loss: 0.029015012085437775\n",
      "Epoch 2575, Loss: 0.0538330739364028, Final Batch Loss: 0.0396466925740242\n",
      "Epoch 2576, Loss: 0.06961722578853369, Final Batch Loss: 0.05439291149377823\n",
      "Epoch 2577, Loss: 0.03395116701722145, Final Batch Loss: 0.017582464963197708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2578, Loss: 0.04862787388265133, Final Batch Loss: 0.015041830018162727\n",
      "Epoch 2579, Loss: 0.023742791265249252, Final Batch Loss: 0.010337204672396183\n",
      "Epoch 2580, Loss: 0.0643342025578022, Final Batch Loss: 0.03266255557537079\n",
      "Epoch 2581, Loss: 0.05260513070970774, Final Batch Loss: 0.03805913031101227\n",
      "Epoch 2582, Loss: 0.0683886669576168, Final Batch Loss: 0.030902311205863953\n",
      "Epoch 2583, Loss: 0.06139295734465122, Final Batch Loss: 0.038332249969244\n",
      "Epoch 2584, Loss: 0.037654053419828415, Final Batch Loss: 0.02055019699037075\n",
      "Epoch 2585, Loss: 0.03627022076398134, Final Batch Loss: 0.02307181991636753\n",
      "Epoch 2586, Loss: 0.05458211526274681, Final Batch Loss: 0.023985248059034348\n",
      "Epoch 2587, Loss: 0.09147572331130505, Final Batch Loss: 0.0714462623000145\n",
      "Epoch 2588, Loss: 0.04024030640721321, Final Batch Loss: 0.018966905772686005\n",
      "Epoch 2589, Loss: 0.05426768586039543, Final Batch Loss: 0.010334964841604233\n",
      "Epoch 2590, Loss: 0.05242925509810448, Final Batch Loss: 0.033918172121047974\n",
      "Epoch 2591, Loss: 0.05541555769741535, Final Batch Loss: 0.01699737273156643\n",
      "Epoch 2592, Loss: 0.03319137915968895, Final Batch Loss: 0.01851150207221508\n",
      "Epoch 2593, Loss: 0.03860699664801359, Final Batch Loss: 0.027407938614487648\n",
      "Epoch 2594, Loss: 0.056785136461257935, Final Batch Loss: 0.015558544546365738\n",
      "Epoch 2595, Loss: 0.0858512669801712, Final Batch Loss: 0.054445791989564896\n",
      "Epoch 2596, Loss: 0.0629017986357212, Final Batch Loss: 0.034890130162239075\n",
      "Epoch 2597, Loss: 0.030066722072660923, Final Batch Loss: 0.014320063404738903\n",
      "Epoch 2598, Loss: 0.02845625765621662, Final Batch Loss: 0.016332710161805153\n",
      "Epoch 2599, Loss: 0.08873788639903069, Final Batch Loss: 0.04600103572010994\n",
      "Epoch 2600, Loss: 0.046837396919727325, Final Batch Loss: 0.023880885913968086\n",
      "Epoch 2601, Loss: 0.03368154913187027, Final Batch Loss: 0.015129489824175835\n",
      "Epoch 2602, Loss: 0.046762495301663876, Final Batch Loss: 0.03418245166540146\n",
      "Epoch 2603, Loss: 0.04386947304010391, Final Batch Loss: 0.021592171862721443\n",
      "Epoch 2604, Loss: 0.06998932920396328, Final Batch Loss: 0.056917715817689896\n",
      "Epoch 2605, Loss: 0.05438031814992428, Final Batch Loss: 0.017656655982136726\n",
      "Epoch 2606, Loss: 0.05778602696955204, Final Batch Loss: 0.04423462226986885\n",
      "Epoch 2607, Loss: 0.10235681012272835, Final Batch Loss: 0.025377411395311356\n",
      "Epoch 2608, Loss: 0.06336334720253944, Final Batch Loss: 0.04405527561903\n",
      "Epoch 2609, Loss: 0.053984394297003746, Final Batch Loss: 0.037856150418519974\n",
      "Epoch 2610, Loss: 0.04984910599887371, Final Batch Loss: 0.028054194524884224\n",
      "Epoch 2611, Loss: 0.05846514366567135, Final Batch Loss: 0.027605699375271797\n",
      "Epoch 2612, Loss: 0.032187944278120995, Final Batch Loss: 0.01992138847708702\n",
      "Epoch 2613, Loss: 0.058806948363780975, Final Batch Loss: 0.01741601526737213\n",
      "Epoch 2614, Loss: 0.04012162797152996, Final Batch Loss: 0.012021413072943687\n",
      "Epoch 2615, Loss: 0.05643143318593502, Final Batch Loss: 0.03347977623343468\n",
      "Epoch 2616, Loss: 0.04402255639433861, Final Batch Loss: 0.02824481390416622\n",
      "Epoch 2617, Loss: 0.0507719200104475, Final Batch Loss: 0.022659914568066597\n",
      "Epoch 2618, Loss: 0.03643299825489521, Final Batch Loss: 0.02239750325679779\n",
      "Epoch 2619, Loss: 0.02066377829760313, Final Batch Loss: 0.009789660573005676\n",
      "Epoch 2620, Loss: 0.02638521883636713, Final Batch Loss: 0.007998759858310223\n",
      "Epoch 2621, Loss: 0.09545954689383507, Final Batch Loss: 0.06749685853719711\n",
      "Epoch 2622, Loss: 0.06753687188029289, Final Batch Loss: 0.03597671538591385\n",
      "Epoch 2623, Loss: 0.07031493168324232, Final Batch Loss: 0.05677812173962593\n",
      "Epoch 2624, Loss: 0.056967636570334435, Final Batch Loss: 0.013308318331837654\n",
      "Epoch 2625, Loss: 0.046484505757689476, Final Batch Loss: 0.012219725176692009\n",
      "Epoch 2626, Loss: 0.1216923538595438, Final Batch Loss: 0.09056255966424942\n",
      "Epoch 2627, Loss: 0.05270059034228325, Final Batch Loss: 0.03365088626742363\n",
      "Epoch 2628, Loss: 0.05376174859702587, Final Batch Loss: 0.01732664369046688\n",
      "Epoch 2629, Loss: 0.023212363943457603, Final Batch Loss: 0.0137019706889987\n",
      "Epoch 2630, Loss: 0.019113773480057716, Final Batch Loss: 0.006504488177597523\n",
      "Epoch 2631, Loss: 0.05522428639233112, Final Batch Loss: 0.02432822622358799\n",
      "Epoch 2632, Loss: 0.11048475652933121, Final Batch Loss: 0.0731881707906723\n",
      "Epoch 2633, Loss: 0.058905934914946556, Final Batch Loss: 0.03086002916097641\n",
      "Epoch 2634, Loss: 0.060449687764048576, Final Batch Loss: 0.01618143729865551\n",
      "Epoch 2635, Loss: 0.020354846492409706, Final Batch Loss: 0.012253720313310623\n",
      "Epoch 2636, Loss: 0.062189336866140366, Final Batch Loss: 0.04350469261407852\n",
      "Epoch 2637, Loss: 0.04568896256387234, Final Batch Loss: 0.016200819984078407\n",
      "Epoch 2638, Loss: 0.043363173957914114, Final Batch Loss: 0.006693725939840078\n",
      "Epoch 2639, Loss: 0.0839436762034893, Final Batch Loss: 0.048721347004175186\n",
      "Epoch 2640, Loss: 0.04008856974542141, Final Batch Loss: 0.016893167048692703\n",
      "Epoch 2641, Loss: 0.0308136735111475, Final Batch Loss: 0.012275073677301407\n",
      "Epoch 2642, Loss: 0.033737736754119396, Final Batch Loss: 0.014828084968030453\n",
      "Epoch 2643, Loss: 0.05841332487761974, Final Batch Loss: 0.032925087958574295\n",
      "Epoch 2644, Loss: 0.032756405882537365, Final Batch Loss: 0.015267728827893734\n",
      "Epoch 2645, Loss: 0.05467682331800461, Final Batch Loss: 0.03230522572994232\n",
      "Epoch 2646, Loss: 0.06250439956784248, Final Batch Loss: 0.04395293816924095\n",
      "Epoch 2647, Loss: 0.0220644474029541, Final Batch Loss: 0.011488559655845165\n",
      "Epoch 2648, Loss: 0.031085087917745113, Final Batch Loss: 0.01879352703690529\n",
      "Epoch 2649, Loss: 0.04402240179479122, Final Batch Loss: 0.01883431151509285\n",
      "Epoch 2650, Loss: 0.058848341926932335, Final Batch Loss: 0.0387377105653286\n",
      "Epoch 2651, Loss: 0.12299811840057373, Final Batch Loss: 0.06380670517683029\n",
      "Epoch 2652, Loss: 0.05936513654887676, Final Batch Loss: 0.021159188821911812\n",
      "Epoch 2653, Loss: 0.04955948516726494, Final Batch Loss: 0.021603453904390335\n",
      "Epoch 2654, Loss: 0.0525969322770834, Final Batch Loss: 0.03540774807333946\n",
      "Epoch 2655, Loss: 0.06319397874176502, Final Batch Loss: 0.028834925964474678\n",
      "Epoch 2656, Loss: 0.17327194660902023, Final Batch Loss: 0.14008797705173492\n",
      "Epoch 2657, Loss: 0.07694204151630402, Final Batch Loss: 0.030609510838985443\n",
      "Epoch 2658, Loss: 0.029607707634568214, Final Batch Loss: 0.015390044078230858\n",
      "Epoch 2659, Loss: 0.03942417353391647, Final Batch Loss: 0.024478044360876083\n",
      "Epoch 2660, Loss: 0.09087187796831131, Final Batch Loss: 0.028133422136306763\n",
      "Epoch 2661, Loss: 0.05304458737373352, Final Batch Loss: 0.01793086528778076\n",
      "Epoch 2662, Loss: 0.055029815062880516, Final Batch Loss: 0.02566514164209366\n",
      "Epoch 2663, Loss: 0.03342677280306816, Final Batch Loss: 0.01674591191112995\n",
      "Epoch 2664, Loss: 0.048969414085149765, Final Batch Loss: 0.02170148678123951\n",
      "Epoch 2665, Loss: 0.06835455261170864, Final Batch Loss: 0.03783472999930382\n",
      "Epoch 2666, Loss: 0.0397636778652668, Final Batch Loss: 0.016602547839283943\n",
      "Epoch 2667, Loss: 0.02743293810635805, Final Batch Loss: 0.007438999600708485\n",
      "Epoch 2668, Loss: 0.027461729012429714, Final Batch Loss: 0.010842069052159786\n",
      "Epoch 2669, Loss: 0.061325499787926674, Final Batch Loss: 0.037594083696603775\n",
      "Epoch 2670, Loss: 0.03450816310942173, Final Batch Loss: 0.009254483506083488\n",
      "Epoch 2671, Loss: 0.09107948839664459, Final Batch Loss: 0.07074043154716492\n",
      "Epoch 2672, Loss: 0.04887612722814083, Final Batch Loss: 0.016661034896969795\n",
      "Epoch 2673, Loss: 0.03311361279338598, Final Batch Loss: 0.018361572176218033\n",
      "Epoch 2674, Loss: 0.043147046118974686, Final Batch Loss: 0.021839147433638573\n",
      "Epoch 2675, Loss: 0.08845789171755314, Final Batch Loss: 0.06944052875041962\n",
      "Epoch 2676, Loss: 0.03787413705140352, Final Batch Loss: 0.012490806169807911\n",
      "Epoch 2677, Loss: 0.05721478909254074, Final Batch Loss: 0.02145770564675331\n",
      "Epoch 2678, Loss: 0.056197237223386765, Final Batch Loss: 0.025596458464860916\n",
      "Epoch 2679, Loss: 0.036393350921571255, Final Batch Loss: 0.02598722279071808\n",
      "Epoch 2680, Loss: 0.07024961896240711, Final Batch Loss: 0.02648398093879223\n",
      "Epoch 2681, Loss: 0.06513568572700024, Final Batch Loss: 0.023999227210879326\n",
      "Epoch 2682, Loss: 0.07190739084035158, Final Batch Loss: 0.011980918236076832\n",
      "Epoch 2683, Loss: 0.0712758544832468, Final Batch Loss: 0.045914143323898315\n",
      "Epoch 2684, Loss: 0.03816087357699871, Final Batch Loss: 0.021733492612838745\n",
      "Epoch 2685, Loss: 0.02523218560963869, Final Batch Loss: 0.012027897872030735\n",
      "Epoch 2686, Loss: 0.0769142210483551, Final Batch Loss: 0.03758646547794342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2687, Loss: 0.05435306578874588, Final Batch Loss: 0.036217235028743744\n",
      "Epoch 2688, Loss: 0.04840697906911373, Final Batch Loss: 0.02668207511305809\n",
      "Epoch 2689, Loss: 0.08543795719742775, Final Batch Loss: 0.04737190157175064\n",
      "Epoch 2690, Loss: 0.07855542190372944, Final Batch Loss: 0.026639098301529884\n",
      "Epoch 2691, Loss: 0.050529686734080315, Final Batch Loss: 0.009511778131127357\n",
      "Epoch 2692, Loss: 0.04907855577766895, Final Batch Loss: 0.032476987689733505\n",
      "Epoch 2693, Loss: 0.10711792297661304, Final Batch Loss: 0.08805225044488907\n",
      "Epoch 2694, Loss: 0.08376221638172865, Final Batch Loss: 0.06834755092859268\n",
      "Epoch 2695, Loss: 0.08657791838049889, Final Batch Loss: 0.008640076965093613\n",
      "Epoch 2696, Loss: 0.039782820269465446, Final Batch Loss: 0.013580702245235443\n",
      "Epoch 2697, Loss: 0.06905055418610573, Final Batch Loss: 0.0316070094704628\n",
      "Epoch 2698, Loss: 0.03900300804525614, Final Batch Loss: 0.011080431751906872\n",
      "Epoch 2699, Loss: 0.05738192517310381, Final Batch Loss: 0.012087234295904636\n",
      "Epoch 2700, Loss: 0.04973236098885536, Final Batch Loss: 0.022071359679102898\n",
      "Epoch 2701, Loss: 0.06032809801399708, Final Batch Loss: 0.010074490681290627\n",
      "Epoch 2702, Loss: 0.04150642268359661, Final Batch Loss: 0.015862587839365005\n",
      "Epoch 2703, Loss: 0.0715402290225029, Final Batch Loss: 0.040109045803546906\n",
      "Epoch 2704, Loss: 0.03389785438776016, Final Batch Loss: 0.017656933516263962\n",
      "Epoch 2705, Loss: 0.047555246856063604, Final Batch Loss: 0.006304373499006033\n",
      "Epoch 2706, Loss: 0.05885181948542595, Final Batch Loss: 0.01679656282067299\n",
      "Epoch 2707, Loss: 0.0405568853020668, Final Batch Loss: 0.016721036285161972\n",
      "Epoch 2708, Loss: 0.055248258635401726, Final Batch Loss: 0.029395226389169693\n",
      "Epoch 2709, Loss: 0.054775871336460114, Final Batch Loss: 0.027230381965637207\n",
      "Epoch 2710, Loss: 0.04608683101832867, Final Batch Loss: 0.019683057442307472\n",
      "Epoch 2711, Loss: 0.062070515006780624, Final Batch Loss: 0.025742381811141968\n",
      "Epoch 2712, Loss: 0.055172963067889214, Final Batch Loss: 0.025208566337823868\n",
      "Epoch 2713, Loss: 0.05237560719251633, Final Batch Loss: 0.0213671512901783\n",
      "Epoch 2714, Loss: 0.06414551846683025, Final Batch Loss: 0.038908109068870544\n",
      "Epoch 2715, Loss: 0.03274425305426121, Final Batch Loss: 0.024614330381155014\n",
      "Epoch 2716, Loss: 0.06304861605167389, Final Batch Loss: 0.019829388707876205\n",
      "Epoch 2717, Loss: 0.05568320490419865, Final Batch Loss: 0.032140035182237625\n",
      "Epoch 2718, Loss: 0.06484930776059628, Final Batch Loss: 0.009548278525471687\n",
      "Epoch 2719, Loss: 0.041651757434010506, Final Batch Loss: 0.019526302814483643\n",
      "Epoch 2720, Loss: 0.03209279850125313, Final Batch Loss: 0.01601123996078968\n",
      "Epoch 2721, Loss: 0.07321372628211975, Final Batch Loss: 0.03475524112582207\n",
      "Epoch 2722, Loss: 0.03971239924430847, Final Batch Loss: 0.01818491332232952\n",
      "Epoch 2723, Loss: 0.0612438078969717, Final Batch Loss: 0.025991985574364662\n",
      "Epoch 2724, Loss: 0.04493172466754913, Final Batch Loss: 0.013194005936384201\n",
      "Epoch 2725, Loss: 0.06561397016048431, Final Batch Loss: 0.03355543315410614\n",
      "Epoch 2726, Loss: 0.025319487787783146, Final Batch Loss: 0.012262956239283085\n",
      "Epoch 2727, Loss: 0.04284762963652611, Final Batch Loss: 0.02245013602077961\n",
      "Epoch 2728, Loss: 0.027976932004094124, Final Batch Loss: 0.009193548932671547\n",
      "Epoch 2729, Loss: 0.057314613834023476, Final Batch Loss: 0.03148550167679787\n",
      "Epoch 2730, Loss: 0.0308615630492568, Final Batch Loss: 0.01780008338391781\n",
      "Epoch 2731, Loss: 0.051008135080337524, Final Batch Loss: 0.03095659613609314\n",
      "Epoch 2732, Loss: 0.03461051546037197, Final Batch Loss: 0.012192511931061745\n",
      "Epoch 2733, Loss: 0.04932740703225136, Final Batch Loss: 0.025628171861171722\n",
      "Epoch 2734, Loss: 0.034102379344403744, Final Batch Loss: 0.020105859264731407\n",
      "Epoch 2735, Loss: 0.04249203950166702, Final Batch Loss: 0.024294059723615646\n",
      "Epoch 2736, Loss: 0.06017375737428665, Final Batch Loss: 0.018425211310386658\n",
      "Epoch 2737, Loss: 0.04175705276429653, Final Batch Loss: 0.020665859803557396\n",
      "Epoch 2738, Loss: 0.1072085089981556, Final Batch Loss: 0.045382075011730194\n",
      "Epoch 2739, Loss: 0.03461112454533577, Final Batch Loss: 0.018890397623181343\n",
      "Epoch 2740, Loss: 0.07641199044883251, Final Batch Loss: 0.01719205640256405\n",
      "Epoch 2741, Loss: 0.07571875676512718, Final Batch Loss: 0.05196192488074303\n",
      "Epoch 2742, Loss: 0.05332597345113754, Final Batch Loss: 0.03520651534199715\n",
      "Epoch 2743, Loss: 0.0716151213273406, Final Batch Loss: 0.013468905352056026\n",
      "Epoch 2744, Loss: 0.10484832152724266, Final Batch Loss: 0.06757844984531403\n",
      "Epoch 2745, Loss: 0.04728733189404011, Final Batch Loss: 0.03163917362689972\n",
      "Epoch 2746, Loss: 0.0339143555611372, Final Batch Loss: 0.019755706191062927\n",
      "Epoch 2747, Loss: 0.04990166425704956, Final Batch Loss: 0.021117569878697395\n",
      "Epoch 2748, Loss: 0.07656102813780308, Final Batch Loss: 0.04594894126057625\n",
      "Epoch 2749, Loss: 0.042614562436938286, Final Batch Loss: 0.018641160801053047\n",
      "Epoch 2750, Loss: 0.08340939320623875, Final Batch Loss: 0.018046515062451363\n",
      "Epoch 2751, Loss: 0.021413758397102356, Final Batch Loss: 0.010199159383773804\n",
      "Epoch 2752, Loss: 0.044439902529120445, Final Batch Loss: 0.03297876566648483\n",
      "Epoch 2753, Loss: 0.03882660157978535, Final Batch Loss: 0.016819046810269356\n",
      "Epoch 2754, Loss: 0.031962305307388306, Final Batch Loss: 0.011319732293486595\n",
      "Epoch 2755, Loss: 0.025105626322329044, Final Batch Loss: 0.00597610417753458\n",
      "Epoch 2756, Loss: 0.07569468021392822, Final Batch Loss: 0.05654556304216385\n",
      "Epoch 2757, Loss: 0.050051331520080566, Final Batch Loss: 0.028088044375181198\n",
      "Epoch 2758, Loss: 0.03247151616960764, Final Batch Loss: 0.00982771534472704\n",
      "Epoch 2759, Loss: 0.04248962923884392, Final Batch Loss: 0.0158074963837862\n",
      "Epoch 2760, Loss: 0.026475931517779827, Final Batch Loss: 0.009155209176242352\n",
      "Epoch 2761, Loss: 0.04276892077177763, Final Batch Loss: 0.01524363737553358\n",
      "Epoch 2762, Loss: 0.061790164560079575, Final Batch Loss: 0.038617849349975586\n",
      "Epoch 2763, Loss: 0.06732597202062607, Final Batch Loss: 0.05013072118163109\n",
      "Epoch 2764, Loss: 0.033065350726246834, Final Batch Loss: 0.013852080330252647\n",
      "Epoch 2765, Loss: 0.037778290919959545, Final Batch Loss: 0.02582678012549877\n",
      "Epoch 2766, Loss: 0.04739256389439106, Final Batch Loss: 0.02928350679576397\n",
      "Epoch 2767, Loss: 0.02901805192232132, Final Batch Loss: 0.017194291576743126\n",
      "Epoch 2768, Loss: 0.062462485395371914, Final Batch Loss: 0.05055927857756615\n",
      "Epoch 2769, Loss: 0.03592265676707029, Final Batch Loss: 0.012108425609767437\n",
      "Epoch 2770, Loss: 0.03914898168295622, Final Batch Loss: 0.014397340826690197\n",
      "Epoch 2771, Loss: 0.05063469335436821, Final Batch Loss: 0.013125251978635788\n",
      "Epoch 2772, Loss: 0.07653968594968319, Final Batch Loss: 0.04986483231186867\n",
      "Epoch 2773, Loss: 0.03206847794353962, Final Batch Loss: 0.017974553629755974\n",
      "Epoch 2774, Loss: 0.024604524485766888, Final Batch Loss: 0.008114821277558804\n",
      "Epoch 2775, Loss: 0.053234443068504333, Final Batch Loss: 0.019084367901086807\n",
      "Epoch 2776, Loss: 0.03890465013682842, Final Batch Loss: 0.017462266609072685\n",
      "Epoch 2777, Loss: 0.049072371795773506, Final Batch Loss: 0.021856822073459625\n",
      "Epoch 2778, Loss: 0.040387703105807304, Final Batch Loss: 0.010985815897583961\n",
      "Epoch 2779, Loss: 0.10581662133336067, Final Batch Loss: 0.016552012413740158\n",
      "Epoch 2780, Loss: 0.035238198935985565, Final Batch Loss: 0.018326139077544212\n",
      "Epoch 2781, Loss: 0.027837544679641724, Final Batch Loss: 0.013662856072187424\n",
      "Epoch 2782, Loss: 0.04207317344844341, Final Batch Loss: 0.02085667848587036\n",
      "Epoch 2783, Loss: 0.032576593570411205, Final Batch Loss: 0.011037907563149929\n",
      "Epoch 2784, Loss: 0.04955673962831497, Final Batch Loss: 0.010541383177042007\n",
      "Epoch 2785, Loss: 0.034661428071558475, Final Batch Loss: 0.01387281809002161\n",
      "Epoch 2786, Loss: 0.05186104867607355, Final Batch Loss: 0.012935620732605457\n",
      "Epoch 2787, Loss: 0.08186998590826988, Final Batch Loss: 0.0700141116976738\n",
      "Epoch 2788, Loss: 0.08121524564921856, Final Batch Loss: 0.027523206546902657\n",
      "Epoch 2789, Loss: 0.034564271569252014, Final Batch Loss: 0.023364659398794174\n",
      "Epoch 2790, Loss: 0.05449014808982611, Final Batch Loss: 0.014689679257571697\n",
      "Epoch 2791, Loss: 0.039364252239465714, Final Batch Loss: 0.0150395967066288\n",
      "Epoch 2792, Loss: 0.07834992371499538, Final Batch Loss: 0.052558306604623795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2793, Loss: 0.060678357258439064, Final Batch Loss: 0.023326566442847252\n",
      "Epoch 2794, Loss: 0.03515333030372858, Final Batch Loss: 0.012408209033310413\n",
      "Epoch 2795, Loss: 0.05997991003096104, Final Batch Loss: 0.039027679711580276\n",
      "Epoch 2796, Loss: 0.07218684814870358, Final Batch Loss: 0.027592768892645836\n",
      "Epoch 2797, Loss: 0.057322864420711994, Final Batch Loss: 0.01314747054129839\n",
      "Epoch 2798, Loss: 0.04034402221441269, Final Batch Loss: 0.01976611837744713\n",
      "Epoch 2799, Loss: 0.0489107733592391, Final Batch Loss: 0.012375100515782833\n",
      "Epoch 2800, Loss: 0.06758689507842064, Final Batch Loss: 0.026771217584609985\n",
      "Epoch 2801, Loss: 0.03064880706369877, Final Batch Loss: 0.012901483103632927\n",
      "Epoch 2802, Loss: 0.04949358478188515, Final Batch Loss: 0.030421698465943336\n",
      "Epoch 2803, Loss: 0.03749179746955633, Final Batch Loss: 0.013258631341159344\n",
      "Epoch 2804, Loss: 0.06929811649024487, Final Batch Loss: 0.03873490169644356\n",
      "Epoch 2805, Loss: 0.03340856730937958, Final Batch Loss: 0.017693527042865753\n",
      "Epoch 2806, Loss: 0.07966882735490799, Final Batch Loss: 0.03864682465791702\n",
      "Epoch 2807, Loss: 0.03065219707787037, Final Batch Loss: 0.011676976457238197\n",
      "Epoch 2808, Loss: 0.036350928246974945, Final Batch Loss: 0.011729411780834198\n",
      "Epoch 2809, Loss: 0.041062296368181705, Final Batch Loss: 0.010163814760744572\n",
      "Epoch 2810, Loss: 0.035842981189489365, Final Batch Loss: 0.014779781922698021\n",
      "Epoch 2811, Loss: 0.08802954852581024, Final Batch Loss: 0.025769289582967758\n",
      "Epoch 2812, Loss: 0.055031195282936096, Final Batch Loss: 0.01779467985033989\n",
      "Epoch 2813, Loss: 0.04055965505540371, Final Batch Loss: 0.019262583926320076\n",
      "Epoch 2814, Loss: 0.055173042230308056, Final Batch Loss: 0.04253418743610382\n",
      "Epoch 2815, Loss: 0.06694872304797173, Final Batch Loss: 0.0292380191385746\n",
      "Epoch 2816, Loss: 0.09843163192272186, Final Batch Loss: 0.07647307217121124\n",
      "Epoch 2817, Loss: 0.029106447473168373, Final Batch Loss: 0.01690834015607834\n",
      "Epoch 2818, Loss: 0.03373643849045038, Final Batch Loss: 0.006945722736418247\n",
      "Epoch 2819, Loss: 0.029197806492447853, Final Batch Loss: 0.013288760557770729\n",
      "Epoch 2820, Loss: 0.08181064017117023, Final Batch Loss: 0.02143576182425022\n",
      "Epoch 2821, Loss: 0.09310554340481758, Final Batch Loss: 0.05131859704852104\n",
      "Epoch 2822, Loss: 0.04623462073504925, Final Batch Loss: 0.03545006364583969\n",
      "Epoch 2823, Loss: 0.030400368385016918, Final Batch Loss: 0.01339233573526144\n",
      "Epoch 2824, Loss: 0.04185262322425842, Final Batch Loss: 0.020770801231265068\n",
      "Epoch 2825, Loss: 0.044249704107642174, Final Batch Loss: 0.026644524186849594\n",
      "Epoch 2826, Loss: 0.03540884982794523, Final Batch Loss: 0.015584898181259632\n",
      "Epoch 2827, Loss: 0.03533891215920448, Final Batch Loss: 0.018398918211460114\n",
      "Epoch 2828, Loss: 0.06343134865164757, Final Batch Loss: 0.02411964163184166\n",
      "Epoch 2829, Loss: 0.05661991238594055, Final Batch Loss: 0.021831396967172623\n",
      "Epoch 2830, Loss: 0.05821884609758854, Final Batch Loss: 0.023855438455939293\n",
      "Epoch 2831, Loss: 0.04986892640590668, Final Batch Loss: 0.03404397889971733\n",
      "Epoch 2832, Loss: 0.04499375820159912, Final Batch Loss: 0.024034932255744934\n",
      "Epoch 2833, Loss: 0.028890782967209816, Final Batch Loss: 0.010434292256832123\n",
      "Epoch 2834, Loss: 0.09000927768647671, Final Batch Loss: 0.06737557798624039\n",
      "Epoch 2835, Loss: 0.017475475557148457, Final Batch Loss: 0.009423420764505863\n",
      "Epoch 2836, Loss: 0.032337188720703125, Final Batch Loss: 0.012361239641904831\n",
      "Epoch 2837, Loss: 0.03459860943257809, Final Batch Loss: 0.012998521327972412\n",
      "Epoch 2838, Loss: 0.043378787115216255, Final Batch Loss: 0.02260367013514042\n",
      "Epoch 2839, Loss: 0.05166631378233433, Final Batch Loss: 0.015285154804587364\n",
      "Epoch 2840, Loss: 0.06301797740161419, Final Batch Loss: 0.0466458722949028\n",
      "Epoch 2841, Loss: 0.02613240946084261, Final Batch Loss: 0.017250556498765945\n",
      "Epoch 2842, Loss: 0.09369294345378876, Final Batch Loss: 0.03622909635305405\n",
      "Epoch 2843, Loss: 0.04879903979599476, Final Batch Loss: 0.018351832404732704\n",
      "Epoch 2844, Loss: 0.039319492876529694, Final Batch Loss: 0.017977943643927574\n",
      "Epoch 2845, Loss: 0.06848492659628391, Final Batch Loss: 0.014036780223250389\n",
      "Epoch 2846, Loss: 0.053985731676220894, Final Batch Loss: 0.04260357841849327\n",
      "Epoch 2847, Loss: 0.05714845284819603, Final Batch Loss: 0.020225167274475098\n",
      "Epoch 2848, Loss: 0.06477462314069271, Final Batch Loss: 0.02389855496585369\n",
      "Epoch 2849, Loss: 0.0409190496429801, Final Batch Loss: 0.00970273744314909\n",
      "Epoch 2850, Loss: 0.1254273597151041, Final Batch Loss: 0.10968892276287079\n",
      "Epoch 2851, Loss: 0.03817478194832802, Final Batch Loss: 0.019317906349897385\n",
      "Epoch 2852, Loss: 0.058894187211990356, Final Batch Loss: 0.01952403038740158\n",
      "Epoch 2853, Loss: 0.064930634573102, Final Batch Loss: 0.04421063885092735\n",
      "Epoch 2854, Loss: 0.04494599252939224, Final Batch Loss: 0.028520619496703148\n",
      "Epoch 2855, Loss: 0.033909356221556664, Final Batch Loss: 0.016026748344302177\n",
      "Epoch 2856, Loss: 0.04471476189792156, Final Batch Loss: 0.025420885533094406\n",
      "Epoch 2857, Loss: 0.12914426811039448, Final Batch Loss: 0.10595311224460602\n",
      "Epoch 2858, Loss: 0.08757039904594421, Final Batch Loss: 0.029804576188325882\n",
      "Epoch 2859, Loss: 0.03236338961869478, Final Batch Loss: 0.02203567512333393\n",
      "Epoch 2860, Loss: 0.052223920822143555, Final Batch Loss: 0.030566319823265076\n",
      "Epoch 2861, Loss: 0.03446375485509634, Final Batch Loss: 0.012004823423922062\n",
      "Epoch 2862, Loss: 0.050221316516399384, Final Batch Loss: 0.028173314407467842\n",
      "Epoch 2863, Loss: 0.025121673941612244, Final Batch Loss: 0.010009893216192722\n",
      "Epoch 2864, Loss: 0.05490802973508835, Final Batch Loss: 0.02654317580163479\n",
      "Epoch 2865, Loss: 0.08174128830432892, Final Batch Loss: 0.049522679299116135\n",
      "Epoch 2866, Loss: 0.05956793203949928, Final Batch Loss: 0.04297899827361107\n",
      "Epoch 2867, Loss: 0.04598459228873253, Final Batch Loss: 0.009550735354423523\n",
      "Epoch 2868, Loss: 0.04708335921168327, Final Batch Loss: 0.022879190742969513\n",
      "Epoch 2869, Loss: 0.039123968221247196, Final Batch Loss: 0.013020521961152554\n",
      "Epoch 2870, Loss: 0.028474006801843643, Final Batch Loss: 0.009165745228528976\n",
      "Epoch 2871, Loss: 0.04736452177166939, Final Batch Loss: 0.024157507345080376\n",
      "Epoch 2872, Loss: 0.0675613097846508, Final Batch Loss: 0.035453204065561295\n",
      "Epoch 2873, Loss: 0.03835920803248882, Final Batch Loss: 0.012794490903615952\n",
      "Epoch 2874, Loss: 0.04746717028319836, Final Batch Loss: 0.02432933822274208\n",
      "Epoch 2875, Loss: 0.027386421337723732, Final Batch Loss: 0.010062327608466148\n",
      "Epoch 2876, Loss: 0.037720488384366035, Final Batch Loss: 0.007461976259946823\n",
      "Epoch 2877, Loss: 0.03165923524647951, Final Batch Loss: 0.018549244850873947\n",
      "Epoch 2878, Loss: 0.01690426468849182, Final Batch Loss: 0.005674680694937706\n",
      "Epoch 2879, Loss: 0.051074596121907234, Final Batch Loss: 0.019737927243113518\n",
      "Epoch 2880, Loss: 0.03950706869363785, Final Batch Loss: 0.015447162091732025\n",
      "Epoch 2881, Loss: 0.02594300266355276, Final Batch Loss: 0.011832023970782757\n",
      "Epoch 2882, Loss: 0.029745806008577347, Final Batch Loss: 0.02130964770913124\n",
      "Epoch 2883, Loss: 0.03289662301540375, Final Batch Loss: 0.012398164719343185\n",
      "Epoch 2884, Loss: 0.02687877230346203, Final Batch Loss: 0.013067174702882767\n",
      "Epoch 2885, Loss: 0.04406964872032404, Final Batch Loss: 0.01005000714212656\n",
      "Epoch 2886, Loss: 0.03740055859088898, Final Batch Loss: 0.017643436789512634\n",
      "Epoch 2887, Loss: 0.05491510033607483, Final Batch Loss: 0.03486371040344238\n",
      "Epoch 2888, Loss: 0.06498228944838047, Final Batch Loss: 0.0434301421046257\n",
      "Epoch 2889, Loss: 0.09176500048488379, Final Batch Loss: 0.08350323885679245\n",
      "Epoch 2890, Loss: 0.04179054778069258, Final Batch Loss: 0.015151693485677242\n",
      "Epoch 2891, Loss: 0.10098570585250854, Final Batch Loss: 0.0323052778840065\n",
      "Epoch 2892, Loss: 0.03846564330160618, Final Batch Loss: 0.028244586661458015\n",
      "Epoch 2893, Loss: 0.03798014484345913, Final Batch Loss: 0.016025058925151825\n",
      "Epoch 2894, Loss: 0.02969751600176096, Final Batch Loss: 0.012776357121765614\n",
      "Epoch 2895, Loss: 0.018702989676967263, Final Batch Loss: 0.0023444911930710077\n",
      "Epoch 2896, Loss: 0.05400473903864622, Final Batch Loss: 0.013923711143434048\n",
      "Epoch 2897, Loss: 0.023548532277345657, Final Batch Loss: 0.008053614757955074\n",
      "Epoch 2898, Loss: 0.0735118049196899, Final Batch Loss: 0.06584993749856949\n",
      "Epoch 2899, Loss: 0.05267384834587574, Final Batch Loss: 0.036240361630916595\n",
      "Epoch 2900, Loss: 0.052772341296076775, Final Batch Loss: 0.025004999712109566\n",
      "Epoch 2901, Loss: 0.08597061783075333, Final Batch Loss: 0.059608448296785355\n",
      "Epoch 2902, Loss: 0.046710480004549026, Final Batch Loss: 0.018588745966553688\n",
      "Epoch 2903, Loss: 0.0480597373098135, Final Batch Loss: 0.023436129093170166\n",
      "Epoch 2904, Loss: 0.025889268144965172, Final Batch Loss: 0.012683560140430927\n",
      "Epoch 2905, Loss: 0.0964134931564331, Final Batch Loss: 0.05852797254920006\n",
      "Epoch 2906, Loss: 0.04509738180786371, Final Batch Loss: 0.011891537345945835\n",
      "Epoch 2907, Loss: 0.044762058183550835, Final Batch Loss: 0.02650320529937744\n",
      "Epoch 2908, Loss: 0.03024969808757305, Final Batch Loss: 0.020150700584053993\n",
      "Epoch 2909, Loss: 0.08275553770363331, Final Batch Loss: 0.05772435665130615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2910, Loss: 0.059636570513248444, Final Batch Loss: 0.025544747710227966\n",
      "Epoch 2911, Loss: 0.047636063769459724, Final Batch Loss: 0.029154611751437187\n",
      "Epoch 2912, Loss: 0.05443352460861206, Final Batch Loss: 0.03834329918026924\n",
      "Epoch 2913, Loss: 0.08597494289278984, Final Batch Loss: 0.01580173149704933\n",
      "Epoch 2914, Loss: 0.13293002918362617, Final Batch Loss: 0.10725139826536179\n",
      "Epoch 2915, Loss: 0.056470222771167755, Final Batch Loss: 0.030485769733786583\n",
      "Epoch 2916, Loss: 0.05397101305425167, Final Batch Loss: 0.01300491951406002\n",
      "Epoch 2917, Loss: 0.06520096585154533, Final Batch Loss: 0.029369179159402847\n",
      "Epoch 2918, Loss: 0.044017938897013664, Final Batch Loss: 0.01650850661098957\n",
      "Epoch 2919, Loss: 0.08630387857556343, Final Batch Loss: 0.0537884458899498\n",
      "Epoch 2920, Loss: 0.0627968180924654, Final Batch Loss: 0.010454079136252403\n",
      "Epoch 2921, Loss: 0.04664405528455973, Final Batch Loss: 0.03128524869680405\n",
      "Epoch 2922, Loss: 0.03209458012133837, Final Batch Loss: 0.016761701554059982\n",
      "Epoch 2923, Loss: 0.06615653075277805, Final Batch Loss: 0.05225446820259094\n",
      "Epoch 2924, Loss: 0.04641051031649113, Final Batch Loss: 0.017799463123083115\n",
      "Epoch 2925, Loss: 0.06341520696878433, Final Batch Loss: 0.019937999546527863\n",
      "Epoch 2926, Loss: 0.06567279808223248, Final Batch Loss: 0.04562543332576752\n",
      "Epoch 2927, Loss: 0.059632232412695885, Final Batch Loss: 0.018153758719563484\n",
      "Epoch 2928, Loss: 0.05358676612377167, Final Batch Loss: 0.022488171234726906\n",
      "Epoch 2929, Loss: 0.020224642474204302, Final Batch Loss: 0.007460748311132193\n",
      "Epoch 2930, Loss: 0.04366660304367542, Final Batch Loss: 0.020906144753098488\n",
      "Epoch 2931, Loss: 0.026144915260374546, Final Batch Loss: 0.016836361959576607\n",
      "Epoch 2932, Loss: 0.04161039739847183, Final Batch Loss: 0.02065330743789673\n",
      "Epoch 2933, Loss: 0.06611867435276508, Final Batch Loss: 0.042713623493909836\n",
      "Epoch 2934, Loss: 0.07713699340820312, Final Batch Loss: 0.033462245017290115\n",
      "Epoch 2935, Loss: 0.04213538020849228, Final Batch Loss: 0.013928063213825226\n",
      "Epoch 2936, Loss: 0.04661986231803894, Final Batch Loss: 0.016796428710222244\n",
      "Epoch 2937, Loss: 0.06621628254652023, Final Batch Loss: 0.012776624411344528\n",
      "Epoch 2938, Loss: 0.050654565915465355, Final Batch Loss: 0.02190028876066208\n",
      "Epoch 2939, Loss: 0.027952912263572216, Final Batch Loss: 0.008648806251585484\n",
      "Epoch 2940, Loss: 0.05652746558189392, Final Batch Loss: 0.036452118307352066\n",
      "Epoch 2941, Loss: 0.036326782777905464, Final Batch Loss: 0.018875394016504288\n",
      "Epoch 2942, Loss: 0.045786937698721886, Final Batch Loss: 0.021434547379612923\n",
      "Epoch 2943, Loss: 0.08758841268718243, Final Batch Loss: 0.07082722336053848\n",
      "Epoch 2944, Loss: 0.023425644263625145, Final Batch Loss: 0.008243749849498272\n",
      "Epoch 2945, Loss: 0.06347088888287544, Final Batch Loss: 0.04599803313612938\n",
      "Epoch 2946, Loss: 0.07188058272004128, Final Batch Loss: 0.03741356357932091\n",
      "Epoch 2947, Loss: 0.03551812097430229, Final Batch Loss: 0.01034989207983017\n",
      "Epoch 2948, Loss: 0.06258914992213249, Final Batch Loss: 0.02931429073214531\n",
      "Epoch 2949, Loss: 0.12370145134627819, Final Batch Loss: 0.10105408728122711\n",
      "Epoch 2950, Loss: 0.04919900465756655, Final Batch Loss: 0.04183664917945862\n",
      "Epoch 2951, Loss: 0.07881248742341995, Final Batch Loss: 0.05224090442061424\n",
      "Epoch 2952, Loss: 0.029629354365170002, Final Batch Loss: 0.018296213820576668\n",
      "Epoch 2953, Loss: 0.052929507568478584, Final Batch Loss: 0.033096496015787125\n",
      "Epoch 2954, Loss: 0.03315091226249933, Final Batch Loss: 0.022120345383882523\n",
      "Epoch 2955, Loss: 0.038416365161538124, Final Batch Loss: 0.021753616631031036\n",
      "Epoch 2956, Loss: 0.0493202768266201, Final Batch Loss: 0.01657852903008461\n",
      "Epoch 2957, Loss: 0.05334207508713007, Final Batch Loss: 0.013281737454235554\n",
      "Epoch 2958, Loss: 0.028145626187324524, Final Batch Loss: 0.018552051857113838\n",
      "Epoch 2959, Loss: 0.05134701542556286, Final Batch Loss: 0.03471551090478897\n",
      "Epoch 2960, Loss: 0.05976514518260956, Final Batch Loss: 0.016081776469945908\n",
      "Epoch 2961, Loss: 0.05615786463022232, Final Batch Loss: 0.0433899350464344\n",
      "Epoch 2962, Loss: 0.04662431217730045, Final Batch Loss: 0.008307671174407005\n",
      "Epoch 2963, Loss: 0.05102891847491264, Final Batch Loss: 0.016561944037675858\n",
      "Epoch 2964, Loss: 0.023103339597582817, Final Batch Loss: 0.015956085175275803\n",
      "Epoch 2965, Loss: 0.036756252869963646, Final Batch Loss: 0.01609683781862259\n",
      "Epoch 2966, Loss: 0.04167001694440842, Final Batch Loss: 0.029471293091773987\n",
      "Epoch 2967, Loss: 0.022269840352237225, Final Batch Loss: 0.010499057359993458\n",
      "Epoch 2968, Loss: 0.04403992090374231, Final Batch Loss: 0.014311899431049824\n",
      "Epoch 2969, Loss: 0.05140185169875622, Final Batch Loss: 0.03012743778526783\n",
      "Epoch 2970, Loss: 0.03677835687994957, Final Batch Loss: 0.015871714800596237\n",
      "Epoch 2971, Loss: 0.03484432026743889, Final Batch Loss: 0.009049871936440468\n",
      "Epoch 2972, Loss: 0.047682542353868484, Final Batch Loss: 0.011261969804763794\n",
      "Epoch 2973, Loss: 0.03192899189889431, Final Batch Loss: 0.01451658271253109\n",
      "Epoch 2974, Loss: 0.023638637736439705, Final Batch Loss: 0.009094872511923313\n",
      "Epoch 2975, Loss: 0.04262026585638523, Final Batch Loss: 0.025716256350278854\n",
      "Epoch 2976, Loss: 0.02414556033909321, Final Batch Loss: 0.007044823840260506\n",
      "Epoch 2977, Loss: 0.05195770226418972, Final Batch Loss: 0.022736860439181328\n",
      "Epoch 2978, Loss: 0.051560189574956894, Final Batch Loss: 0.03227689489722252\n",
      "Epoch 2979, Loss: 0.022911603562533855, Final Batch Loss: 0.014911296777427197\n",
      "Epoch 2980, Loss: 0.022984446492046118, Final Batch Loss: 0.007685832213610411\n",
      "Epoch 2981, Loss: 0.05952131189405918, Final Batch Loss: 0.039838504046201706\n",
      "Epoch 2982, Loss: 0.030752933584153652, Final Batch Loss: 0.014724812470376492\n",
      "Epoch 2983, Loss: 0.02803636435419321, Final Batch Loss: 0.015793990343809128\n",
      "Epoch 2984, Loss: 0.039216102100908756, Final Batch Loss: 0.031828586012125015\n",
      "Epoch 2985, Loss: 0.05827992968261242, Final Batch Loss: 0.02390929125249386\n",
      "Epoch 2986, Loss: 0.09204661101102829, Final Batch Loss: 0.022598080337047577\n",
      "Epoch 2987, Loss: 0.048917029052972794, Final Batch Loss: 0.03607683256268501\n",
      "Epoch 2988, Loss: 0.0223210034891963, Final Batch Loss: 0.011674797162413597\n",
      "Epoch 2989, Loss: 0.0303135197609663, Final Batch Loss: 0.008336460217833519\n",
      "Epoch 2990, Loss: 0.02957312623038888, Final Batch Loss: 0.0056723193265497684\n",
      "Epoch 2991, Loss: 0.05496937222778797, Final Batch Loss: 0.03258530795574188\n",
      "Epoch 2992, Loss: 0.027448800392448902, Final Batch Loss: 0.011209064163267612\n",
      "Epoch 2993, Loss: 0.02198162116110325, Final Batch Loss: 0.013243410736322403\n",
      "Epoch 2994, Loss: 0.03312956914305687, Final Batch Loss: 0.008536996319890022\n",
      "Epoch 2995, Loss: 0.07581406645476818, Final Batch Loss: 0.03068225271999836\n",
      "Epoch 2996, Loss: 0.10799270123243332, Final Batch Loss: 0.08842875063419342\n",
      "Epoch 2997, Loss: 0.023641854524612427, Final Batch Loss: 0.009320986457169056\n",
      "Epoch 2998, Loss: 0.03027412248775363, Final Batch Loss: 0.007653099950402975\n",
      "Epoch 2999, Loss: 0.05978913512080908, Final Batch Loss: 0.04749089851975441\n",
      "Epoch 3000, Loss: 0.06480004265904427, Final Batch Loss: 0.0439753532409668\n",
      "Epoch 3001, Loss: 0.029825182631611824, Final Batch Loss: 0.007672084495425224\n",
      "Epoch 3002, Loss: 0.030029863119125366, Final Batch Loss: 0.010920953005552292\n",
      "Epoch 3003, Loss: 0.04429738037288189, Final Batch Loss: 0.02694355696439743\n",
      "Epoch 3004, Loss: 0.06241907179355621, Final Batch Loss: 0.04451926052570343\n",
      "Epoch 3005, Loss: 0.06229654513299465, Final Batch Loss: 0.04297736659646034\n",
      "Epoch 3006, Loss: 0.04346527345478535, Final Batch Loss: 0.02346264012157917\n",
      "Epoch 3007, Loss: 0.02002359228208661, Final Batch Loss: 0.0076707894913852215\n",
      "Epoch 3008, Loss: 0.04083121754229069, Final Batch Loss: 0.02029942162334919\n",
      "Epoch 3009, Loss: 0.04258280619978905, Final Batch Loss: 0.022636616602540016\n",
      "Epoch 3010, Loss: 0.028281894512474537, Final Batch Loss: 0.011860976926982403\n",
      "Epoch 3011, Loss: 0.07484360225498676, Final Batch Loss: 0.05278412997722626\n",
      "Epoch 3012, Loss: 0.05724705010652542, Final Batch Loss: 0.020799659192562103\n",
      "Epoch 3013, Loss: 0.039205487817525864, Final Batch Loss: 0.013656053692102432\n",
      "Epoch 3014, Loss: 0.022028584964573383, Final Batch Loss: 0.009924685582518578\n",
      "Epoch 3015, Loss: 0.025705267675220966, Final Batch Loss: 0.00992830004543066\n",
      "Epoch 3016, Loss: 0.05493430467322469, Final Batch Loss: 0.0055700247175991535\n",
      "Epoch 3017, Loss: 0.05798551253974438, Final Batch Loss: 0.01662626303732395\n",
      "Epoch 3018, Loss: 0.028644969686865807, Final Batch Loss: 0.010926635935902596\n",
      "Epoch 3019, Loss: 0.0588277131319046, Final Batch Loss: 0.02398781105875969\n",
      "Epoch 3020, Loss: 0.03836006112396717, Final Batch Loss: 0.022129682824015617\n",
      "Epoch 3021, Loss: 0.058037587441504, Final Batch Loss: 0.04352334886789322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3022, Loss: 0.08061434514820576, Final Batch Loss: 0.06277862936258316\n",
      "Epoch 3023, Loss: 0.03106034640222788, Final Batch Loss: 0.017415424808859825\n",
      "Epoch 3024, Loss: 0.048218224197626114, Final Batch Loss: 0.015551883727312088\n",
      "Epoch 3025, Loss: 0.029317553620785475, Final Batch Loss: 0.02420474961400032\n",
      "Epoch 3026, Loss: 0.06170968245714903, Final Batch Loss: 0.009642171673476696\n",
      "Epoch 3027, Loss: 0.08068982046097517, Final Batch Loss: 0.01392564456909895\n",
      "Epoch 3028, Loss: 0.04362465534359217, Final Batch Loss: 0.011809864081442356\n",
      "Epoch 3029, Loss: 0.05663973791524768, Final Batch Loss: 0.007646739017218351\n",
      "Epoch 3030, Loss: 0.07371648214757442, Final Batch Loss: 0.0289046298712492\n",
      "Epoch 3031, Loss: 0.09412004053592682, Final Batch Loss: 0.02168087661266327\n",
      "Epoch 3032, Loss: 0.058945307508111, Final Batch Loss: 0.02918742224574089\n",
      "Epoch 3033, Loss: 0.060976676642894745, Final Batch Loss: 0.02481287345290184\n",
      "Epoch 3034, Loss: 0.05410062521696091, Final Batch Loss: 0.011741377413272858\n",
      "Epoch 3035, Loss: 0.033976005390286446, Final Batch Loss: 0.011212754994630814\n",
      "Epoch 3036, Loss: 0.04809021856635809, Final Batch Loss: 0.014800834469497204\n",
      "Epoch 3037, Loss: 0.030063834972679615, Final Batch Loss: 0.009680279530584812\n",
      "Epoch 3038, Loss: 0.033552165143191814, Final Batch Loss: 0.019715338945388794\n",
      "Epoch 3039, Loss: 0.067969411611557, Final Batch Loss: 0.042059995234012604\n",
      "Epoch 3040, Loss: 0.017851127311587334, Final Batch Loss: 0.008382653817534447\n",
      "Epoch 3041, Loss: 0.08124731481075287, Final Batch Loss: 0.0475081242620945\n",
      "Epoch 3042, Loss: 0.051419822499156, Final Batch Loss: 0.016012316569685936\n",
      "Epoch 3043, Loss: 0.03550043236464262, Final Batch Loss: 0.02464986778795719\n",
      "Epoch 3044, Loss: 0.02353909518569708, Final Batch Loss: 0.007930444553494453\n",
      "Epoch 3045, Loss: 0.05246405489742756, Final Batch Loss: 0.031215300783514977\n",
      "Epoch 3046, Loss: 0.03271472826600075, Final Batch Loss: 0.008340481668710709\n",
      "Epoch 3047, Loss: 0.05020459182560444, Final Batch Loss: 0.019586753100156784\n",
      "Epoch 3048, Loss: 0.0610603429377079, Final Batch Loss: 0.026714399456977844\n",
      "Epoch 3049, Loss: 0.03524591960012913, Final Batch Loss: 0.011145632714033127\n",
      "Epoch 3050, Loss: 0.06994937546551228, Final Batch Loss: 0.05885645002126694\n",
      "Epoch 3051, Loss: 0.052037100307643414, Final Batch Loss: 0.011330022476613522\n",
      "Epoch 3052, Loss: 0.056238146498799324, Final Batch Loss: 0.007102170959115028\n",
      "Epoch 3053, Loss: 0.024661097209900618, Final Batch Loss: 0.007383312564343214\n",
      "Epoch 3054, Loss: 0.04649123176932335, Final Batch Loss: 0.030682900920510292\n",
      "Epoch 3055, Loss: 0.03803633339703083, Final Batch Loss: 0.022382359951734543\n",
      "Epoch 3056, Loss: 0.05244050547480583, Final Batch Loss: 0.022533943876624107\n",
      "Epoch 3057, Loss: 0.10709019377827644, Final Batch Loss: 0.08798561990261078\n",
      "Epoch 3058, Loss: 0.06805049069225788, Final Batch Loss: 0.02146734856069088\n",
      "Epoch 3059, Loss: 0.02252204902470112, Final Batch Loss: 0.007597251795232296\n",
      "Epoch 3060, Loss: 0.03029525652527809, Final Batch Loss: 0.019789695739746094\n",
      "Epoch 3061, Loss: 0.05241638235747814, Final Batch Loss: 0.028014402836561203\n",
      "Epoch 3062, Loss: 0.035411641001701355, Final Batch Loss: 0.01711258664727211\n",
      "Epoch 3063, Loss: 0.050883726216852665, Final Batch Loss: 0.03646494075655937\n",
      "Epoch 3064, Loss: 0.03518908564001322, Final Batch Loss: 0.010186749510467052\n",
      "Epoch 3065, Loss: 0.025579536333680153, Final Batch Loss: 0.01072326023131609\n",
      "Epoch 3066, Loss: 0.045307058840990067, Final Batch Loss: 0.03059418313205242\n",
      "Epoch 3067, Loss: 0.03696250915527344, Final Batch Loss: 0.01809837855398655\n",
      "Epoch 3068, Loss: 0.05925077572464943, Final Batch Loss: 0.02995290234684944\n",
      "Epoch 3069, Loss: 0.03769025672227144, Final Batch Loss: 0.0064269984140992165\n",
      "Epoch 3070, Loss: 0.025798305869102478, Final Batch Loss: 0.01086933072656393\n",
      "Epoch 3071, Loss: 0.021126101724803448, Final Batch Loss: 0.005554147996008396\n",
      "Epoch 3072, Loss: 0.02394549548625946, Final Batch Loss: 0.013473550789058208\n",
      "Epoch 3073, Loss: 0.05774140637367964, Final Batch Loss: 0.007214716635644436\n",
      "Epoch 3074, Loss: 0.04777379520237446, Final Batch Loss: 0.022655129432678223\n",
      "Epoch 3075, Loss: 0.045074633322656155, Final Batch Loss: 0.03542372211813927\n",
      "Epoch 3076, Loss: 0.017663550563156605, Final Batch Loss: 0.00899863988161087\n",
      "Epoch 3077, Loss: 0.05244828015565872, Final Batch Loss: 0.02702673152089119\n",
      "Epoch 3078, Loss: 0.06707051955163479, Final Batch Loss: 0.009109338745474815\n",
      "Epoch 3079, Loss: 0.12311942130327225, Final Batch Loss: 0.034561559557914734\n",
      "Epoch 3080, Loss: 0.03514796122908592, Final Batch Loss: 0.02003052458167076\n",
      "Epoch 3081, Loss: 0.05316927842795849, Final Batch Loss: 0.013019455596804619\n",
      "Epoch 3082, Loss: 0.03139851987361908, Final Batch Loss: 0.013104956597089767\n",
      "Epoch 3083, Loss: 0.0613842299208045, Final Batch Loss: 0.05087846517562866\n",
      "Epoch 3084, Loss: 0.053481973707675934, Final Batch Loss: 0.032479219138622284\n",
      "Epoch 3085, Loss: 0.019859901629388332, Final Batch Loss: 0.01197600644081831\n",
      "Epoch 3086, Loss: 0.03334727883338928, Final Batch Loss: 0.01333274319767952\n",
      "Epoch 3087, Loss: 0.03676187805831432, Final Batch Loss: 0.01737280748784542\n",
      "Epoch 3088, Loss: 0.05492522567510605, Final Batch Loss: 0.03891206532716751\n",
      "Epoch 3089, Loss: 0.04099811799824238, Final Batch Loss: 0.01612847112119198\n",
      "Epoch 3090, Loss: 0.05522985942661762, Final Batch Loss: 0.0302034392952919\n",
      "Epoch 3091, Loss: 0.04176770709455013, Final Batch Loss: 0.01683141104876995\n",
      "Epoch 3092, Loss: 0.051128631457686424, Final Batch Loss: 0.026587197557091713\n",
      "Epoch 3093, Loss: 0.022842172533273697, Final Batch Loss: 0.009629294276237488\n",
      "Epoch 3094, Loss: 0.04069220367819071, Final Batch Loss: 0.010012592189013958\n",
      "Epoch 3095, Loss: 0.04257487691938877, Final Batch Loss: 0.013830358162522316\n",
      "Epoch 3096, Loss: 0.046445295214653015, Final Batch Loss: 0.014772403985261917\n",
      "Epoch 3097, Loss: 0.06139321066439152, Final Batch Loss: 0.03431734815239906\n",
      "Epoch 3098, Loss: 0.0426180437207222, Final Batch Loss: 0.026006557047367096\n",
      "Epoch 3099, Loss: 0.06675871834158897, Final Batch Loss: 0.05652603507041931\n",
      "Epoch 3100, Loss: 0.03292850684374571, Final Batch Loss: 0.0063727376982569695\n",
      "Epoch 3101, Loss: 0.0318385548889637, Final Batch Loss: 0.020162789151072502\n",
      "Epoch 3102, Loss: 0.02641525398939848, Final Batch Loss: 0.015459920279681683\n",
      "Epoch 3103, Loss: 0.023484687320888042, Final Batch Loss: 0.01237641554325819\n",
      "Epoch 3104, Loss: 0.08207333087921143, Final Batch Loss: 0.04161740094423294\n",
      "Epoch 3105, Loss: 0.10862082615494728, Final Batch Loss: 0.06192126125097275\n",
      "Epoch 3106, Loss: 0.04284936003386974, Final Batch Loss: 0.018745655193924904\n",
      "Epoch 3107, Loss: 0.03760824352502823, Final Batch Loss: 0.017919372767210007\n",
      "Epoch 3108, Loss: 0.05245125573128462, Final Batch Loss: 0.008259023539721966\n",
      "Epoch 3109, Loss: 0.027091865427792072, Final Batch Loss: 0.008092052303254604\n",
      "Epoch 3110, Loss: 0.060268012806773186, Final Batch Loss: 0.04010184109210968\n",
      "Epoch 3111, Loss: 0.05201088031753898, Final Batch Loss: 0.04480380937457085\n",
      "Epoch 3112, Loss: 0.04646902158856392, Final Batch Loss: 0.020125608891248703\n",
      "Epoch 3113, Loss: 0.036489490419626236, Final Batch Loss: 0.02772866189479828\n",
      "Epoch 3114, Loss: 0.06555003486573696, Final Batch Loss: 0.045698948204517365\n",
      "Epoch 3115, Loss: 0.015662004239857197, Final Batch Loss: 0.005115083418786526\n",
      "Epoch 3116, Loss: 0.047700485214591026, Final Batch Loss: 0.02663147635757923\n",
      "Epoch 3117, Loss: 0.03488580696284771, Final Batch Loss: 0.017103122547268867\n",
      "Epoch 3118, Loss: 0.10577888321131468, Final Batch Loss: 0.0944075882434845\n",
      "Epoch 3119, Loss: 0.03157621901482344, Final Batch Loss: 0.012924336828291416\n",
      "Epoch 3120, Loss: 0.030299294739961624, Final Batch Loss: 0.013400888070464134\n",
      "Epoch 3121, Loss: 0.0785901639610529, Final Batch Loss: 0.029100777581334114\n",
      "Epoch 3122, Loss: 0.04238632507622242, Final Batch Loss: 0.0322098433971405\n",
      "Epoch 3123, Loss: 0.03996480442583561, Final Batch Loss: 0.017963862046599388\n",
      "Epoch 3124, Loss: 0.039920980110764503, Final Batch Loss: 0.013747714459896088\n",
      "Epoch 3125, Loss: 0.027615592814981937, Final Batch Loss: 0.014624308794736862\n",
      "Epoch 3126, Loss: 0.06837204471230507, Final Batch Loss: 0.03413151949644089\n",
      "Epoch 3127, Loss: 0.041642297990620136, Final Batch Loss: 0.011462430469691753\n",
      "Epoch 3128, Loss: 0.023292868863791227, Final Batch Loss: 0.00669822795316577\n",
      "Epoch 3129, Loss: 0.04409786267206073, Final Batch Loss: 0.03982856124639511\n",
      "Epoch 3130, Loss: 0.06702901795506477, Final Batch Loss: 0.030784737318754196\n",
      "Epoch 3131, Loss: 0.01810988038778305, Final Batch Loss: 0.007417759858071804\n",
      "Epoch 3132, Loss: 0.041617218405008316, Final Batch Loss: 0.017839476466178894\n",
      "Epoch 3133, Loss: 0.07151704095304012, Final Batch Loss: 0.045962657779455185\n",
      "Epoch 3134, Loss: 0.024064346216619015, Final Batch Loss: 0.018681401386857033\n",
      "Epoch 3135, Loss: 0.0396528672426939, Final Batch Loss: 0.011124372482299805\n",
      "Epoch 3136, Loss: 0.0360396821051836, Final Batch Loss: 0.01849915273487568\n",
      "Epoch 3137, Loss: 0.035446091555058956, Final Batch Loss: 0.014544320292770863\n",
      "Epoch 3138, Loss: 0.030251038260757923, Final Batch Loss: 0.013877899385988712\n",
      "Epoch 3139, Loss: 0.019272455014288425, Final Batch Loss: 0.01070628222078085\n",
      "Epoch 3140, Loss: 0.048794468864798546, Final Batch Loss: 0.027189131826162338\n",
      "Epoch 3141, Loss: 0.050277428701519966, Final Batch Loss: 0.032248061150312424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3142, Loss: 0.05365512054413557, Final Batch Loss: 0.009765869937837124\n",
      "Epoch 3143, Loss: 0.026214509271085262, Final Batch Loss: 0.007549787871539593\n",
      "Epoch 3144, Loss: 0.056079491041600704, Final Batch Loss: 0.014464306645095348\n",
      "Epoch 3145, Loss: 0.028741021640598774, Final Batch Loss: 0.011019553057849407\n",
      "Epoch 3146, Loss: 0.032805283553898335, Final Batch Loss: 0.01108876895159483\n",
      "Epoch 3147, Loss: 0.026550388894975185, Final Batch Loss: 0.013010427355766296\n",
      "Epoch 3148, Loss: 0.02777387760579586, Final Batch Loss: 0.016033370047807693\n",
      "Epoch 3149, Loss: 0.04378497414290905, Final Batch Loss: 0.012918112799525261\n",
      "Epoch 3150, Loss: 0.038061161525547504, Final Batch Loss: 0.014865514822304249\n",
      "Epoch 3151, Loss: 0.03126843459904194, Final Batch Loss: 0.01881502754986286\n",
      "Epoch 3152, Loss: 0.14047428034245968, Final Batch Loss: 0.11589286476373672\n",
      "Epoch 3153, Loss: 0.024506225250661373, Final Batch Loss: 0.011164272204041481\n",
      "Epoch 3154, Loss: 0.03162745293229818, Final Batch Loss: 0.007992909289896488\n",
      "Epoch 3155, Loss: 0.048679620027542114, Final Batch Loss: 0.02001490257680416\n",
      "Epoch 3156, Loss: 0.05985300987958908, Final Batch Loss: 0.03996729105710983\n",
      "Epoch 3157, Loss: 0.03302538674324751, Final Batch Loss: 0.013729528523981571\n",
      "Epoch 3158, Loss: 0.04592476040124893, Final Batch Loss: 0.023554274812340736\n",
      "Epoch 3159, Loss: 0.021011127158999443, Final Batch Loss: 0.00963518861681223\n",
      "Epoch 3160, Loss: 0.08216413669288158, Final Batch Loss: 0.0609961673617363\n",
      "Epoch 3161, Loss: 0.03921286016702652, Final Batch Loss: 0.013916809111833572\n",
      "Epoch 3162, Loss: 0.05672103259712458, Final Batch Loss: 0.00863217655569315\n",
      "Epoch 3163, Loss: 0.029402224346995354, Final Batch Loss: 0.011857304722070694\n",
      "Epoch 3164, Loss: 0.03167674783617258, Final Batch Loss: 0.012974037788808346\n",
      "Epoch 3165, Loss: 0.026564248837530613, Final Batch Loss: 0.014048309065401554\n",
      "Epoch 3166, Loss: 0.024281585589051247, Final Batch Loss: 0.008805809542536736\n",
      "Epoch 3167, Loss: 0.03434126079082489, Final Batch Loss: 0.01262325793504715\n",
      "Epoch 3168, Loss: 0.03053181990981102, Final Batch Loss: 0.008280208334326744\n",
      "Epoch 3169, Loss: 0.06724388152360916, Final Batch Loss: 0.043150272220373154\n",
      "Epoch 3170, Loss: 0.01701358426362276, Final Batch Loss: 0.007904674857854843\n",
      "Epoch 3171, Loss: 0.02498927339911461, Final Batch Loss: 0.011623511090874672\n",
      "Epoch 3172, Loss: 0.0702259736135602, Final Batch Loss: 0.06224212050437927\n",
      "Epoch 3173, Loss: 0.035688502714037895, Final Batch Loss: 0.016488946974277496\n",
      "Epoch 3174, Loss: 0.022400072775781155, Final Batch Loss: 0.011325469240546227\n",
      "Epoch 3175, Loss: 0.0249145170673728, Final Batch Loss: 0.008677036501467228\n",
      "Epoch 3176, Loss: 0.03565027751028538, Final Batch Loss: 0.016958003863692284\n",
      "Epoch 3177, Loss: 0.06164528988301754, Final Batch Loss: 0.030303945764899254\n",
      "Epoch 3178, Loss: 0.031644845847040415, Final Batch Loss: 0.006538859102874994\n",
      "Epoch 3179, Loss: 0.04713454842567444, Final Batch Loss: 0.023488063365221024\n",
      "Epoch 3180, Loss: 0.053970424458384514, Final Batch Loss: 0.03549248352646828\n",
      "Epoch 3181, Loss: 0.02235758677124977, Final Batch Loss: 0.01349115651100874\n",
      "Epoch 3182, Loss: 0.038607826456427574, Final Batch Loss: 0.01705230586230755\n",
      "Epoch 3183, Loss: 0.026994099840521812, Final Batch Loss: 0.01470881700515747\n",
      "Epoch 3184, Loss: 0.03891075402498245, Final Batch Loss: 0.013727642595767975\n",
      "Epoch 3185, Loss: 0.07437301240861416, Final Batch Loss: 0.027455540373921394\n",
      "Epoch 3186, Loss: 0.029802745208144188, Final Batch Loss: 0.012362809851765633\n",
      "Epoch 3187, Loss: 0.03368879668414593, Final Batch Loss: 0.010561371222138405\n",
      "Epoch 3188, Loss: 0.033817123621702194, Final Batch Loss: 0.008764978498220444\n",
      "Epoch 3189, Loss: 0.06464249640703201, Final Batch Loss: 0.01015857607126236\n",
      "Epoch 3190, Loss: 0.021720007061958313, Final Batch Loss: 0.010807961225509644\n",
      "Epoch 3191, Loss: 0.017326436936855316, Final Batch Loss: 0.008174384012818336\n",
      "Epoch 3192, Loss: 0.049378303810954094, Final Batch Loss: 0.0137645173817873\n",
      "Epoch 3193, Loss: 0.04194282367825508, Final Batch Loss: 0.022344613447785378\n",
      "Epoch 3194, Loss: 0.04761018417775631, Final Batch Loss: 0.03803539276123047\n",
      "Epoch 3195, Loss: 0.051855891942977905, Final Batch Loss: 0.023991191759705544\n",
      "Epoch 3196, Loss: 0.04501959681510925, Final Batch Loss: 0.025912856683135033\n",
      "Epoch 3197, Loss: 0.02734439494088292, Final Batch Loss: 0.01978994533419609\n",
      "Epoch 3198, Loss: 0.05096040107309818, Final Batch Loss: 0.02739577740430832\n",
      "Epoch 3199, Loss: 0.01853257790207863, Final Batch Loss: 0.004698253236711025\n",
      "Epoch 3200, Loss: 0.026843872852623463, Final Batch Loss: 0.013671156950294971\n",
      "Epoch 3201, Loss: 0.02756774052977562, Final Batch Loss: 0.02001471444964409\n",
      "Epoch 3202, Loss: 0.06648656539618969, Final Batch Loss: 0.04744962602853775\n",
      "Epoch 3203, Loss: 0.03290337976068258, Final Batch Loss: 0.01516884844750166\n",
      "Epoch 3204, Loss: 0.03781729191541672, Final Batch Loss: 0.009636243805289268\n",
      "Epoch 3205, Loss: 0.018455124460160732, Final Batch Loss: 0.007292504422366619\n",
      "Epoch 3206, Loss: 0.03117284830659628, Final Batch Loss: 0.012914848513901234\n",
      "Epoch 3207, Loss: 0.03659496642649174, Final Batch Loss: 0.018871765583753586\n",
      "Epoch 3208, Loss: 0.018295992631465197, Final Batch Loss: 0.010974539443850517\n",
      "Epoch 3209, Loss: 0.042619398795068264, Final Batch Loss: 0.029885953292250633\n",
      "Epoch 3210, Loss: 0.026971835643053055, Final Batch Loss: 0.011431806720793247\n",
      "Epoch 3211, Loss: 0.0361877242103219, Final Batch Loss: 0.024756889790296555\n",
      "Epoch 3212, Loss: 0.027680320665240288, Final Batch Loss: 0.012812450528144836\n",
      "Epoch 3213, Loss: 0.028720706701278687, Final Batch Loss: 0.016464173793792725\n",
      "Epoch 3214, Loss: 0.05853556841611862, Final Batch Loss: 0.024646881967782974\n",
      "Epoch 3215, Loss: 0.07062216103076935, Final Batch Loss: 0.036294758319854736\n",
      "Epoch 3216, Loss: 0.0391080304980278, Final Batch Loss: 0.017767423763871193\n",
      "Epoch 3217, Loss: 0.07814612425863743, Final Batch Loss: 0.027570663020014763\n",
      "Epoch 3218, Loss: 0.021061944775283337, Final Batch Loss: 0.011825422756373882\n",
      "Epoch 3219, Loss: 0.025102113373577595, Final Batch Loss: 0.018726542592048645\n",
      "Epoch 3220, Loss: 0.03875846415758133, Final Batch Loss: 0.02690093219280243\n",
      "Epoch 3221, Loss: 0.035773005336523056, Final Batch Loss: 0.01542726531624794\n",
      "Epoch 3222, Loss: 0.03928487002849579, Final Batch Loss: 0.024552462622523308\n",
      "Epoch 3223, Loss: 0.02947687730193138, Final Batch Loss: 0.016736846417188644\n",
      "Epoch 3224, Loss: 0.04121759720146656, Final Batch Loss: 0.025210294872522354\n",
      "Epoch 3225, Loss: 0.08819529786705971, Final Batch Loss: 0.008812937885522842\n",
      "Epoch 3226, Loss: 0.02669682539999485, Final Batch Loss: 0.010579230263829231\n",
      "Epoch 3227, Loss: 0.06134257838129997, Final Batch Loss: 0.030159031972289085\n",
      "Epoch 3228, Loss: 0.04001497104763985, Final Batch Loss: 0.010845812037587166\n",
      "Epoch 3229, Loss: 0.024487705901265144, Final Batch Loss: 0.015430091880261898\n",
      "Epoch 3230, Loss: 0.030524267815053463, Final Batch Loss: 0.021207068115472794\n",
      "Epoch 3231, Loss: 0.035608273930847645, Final Batch Loss: 0.007363130338490009\n",
      "Epoch 3232, Loss: 0.0203949473798275, Final Batch Loss: 0.011206008493900299\n",
      "Epoch 3233, Loss: 0.02414342388510704, Final Batch Loss: 0.013736195862293243\n",
      "Epoch 3234, Loss: 0.02299052570015192, Final Batch Loss: 0.011539526283740997\n",
      "Epoch 3235, Loss: 0.03183965943753719, Final Batch Loss: 0.024022569879889488\n",
      "Epoch 3236, Loss: 0.09570185467600822, Final Batch Loss: 0.08111890405416489\n",
      "Epoch 3237, Loss: 0.022143876180052757, Final Batch Loss: 0.008739178068935871\n",
      "Epoch 3238, Loss: 0.04290970228612423, Final Batch Loss: 0.01906805858016014\n",
      "Epoch 3239, Loss: 0.04084866866469383, Final Batch Loss: 0.025835124775767326\n",
      "Epoch 3240, Loss: 0.03408279921859503, Final Batch Loss: 0.014025933109223843\n",
      "Epoch 3241, Loss: 0.023413950577378273, Final Batch Loss: 0.008523686788976192\n",
      "Epoch 3242, Loss: 0.025481761433184147, Final Batch Loss: 0.01865079626441002\n",
      "Epoch 3243, Loss: 0.035978433676064014, Final Batch Loss: 0.008320917375385761\n",
      "Epoch 3244, Loss: 0.045937951654195786, Final Batch Loss: 0.016442066058516502\n",
      "Epoch 3245, Loss: 0.05507820472121239, Final Batch Loss: 0.031122548505663872\n",
      "Epoch 3246, Loss: 0.03902645595371723, Final Batch Loss: 0.029456553980708122\n",
      "Epoch 3247, Loss: 0.07319415267556906, Final Batch Loss: 0.06380505859851837\n",
      "Epoch 3248, Loss: 0.04092476703226566, Final Batch Loss: 0.03463643044233322\n",
      "Epoch 3249, Loss: 0.022579736076295376, Final Batch Loss: 0.005512024275958538\n",
      "Epoch 3250, Loss: 0.05077599920332432, Final Batch Loss: 0.043699800968170166\n",
      "Epoch 3251, Loss: 0.03494861163198948, Final Batch Loss: 0.021148694679141045\n",
      "Epoch 3252, Loss: 0.05194536317139864, Final Batch Loss: 0.039088085293769836\n",
      "Epoch 3253, Loss: 0.03675143141299486, Final Batch Loss: 0.03206178918480873\n",
      "Epoch 3254, Loss: 0.01868285471573472, Final Batch Loss: 0.005801765713840723\n",
      "Epoch 3255, Loss: 0.031135610304772854, Final Batch Loss: 0.017068641260266304\n",
      "Epoch 3256, Loss: 0.02161489985883236, Final Batch Loss: 0.008432547561824322\n",
      "Epoch 3257, Loss: 0.021728339605033398, Final Batch Loss: 0.015109947882592678\n",
      "Epoch 3258, Loss: 0.02270331373438239, Final Batch Loss: 0.007509214337915182\n",
      "Epoch 3259, Loss: 0.035728047136217356, Final Batch Loss: 0.004283266607671976\n",
      "Epoch 3260, Loss: 0.02904164558276534, Final Batch Loss: 0.006798270624130964\n",
      "Epoch 3261, Loss: 0.0229268716648221, Final Batch Loss: 0.013870910741388798\n",
      "Epoch 3262, Loss: 0.03999602980911732, Final Batch Loss: 0.016823098063468933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3263, Loss: 0.057688125409185886, Final Batch Loss: 0.04276614263653755\n",
      "Epoch 3264, Loss: 0.021840611472725868, Final Batch Loss: 0.014007159508764744\n",
      "Epoch 3265, Loss: 0.031993803568184376, Final Batch Loss: 0.02217184007167816\n",
      "Epoch 3266, Loss: 0.014513658359646797, Final Batch Loss: 0.00785771757364273\n",
      "Epoch 3267, Loss: 0.04657809063792229, Final Batch Loss: 0.03918997198343277\n",
      "Epoch 3268, Loss: 0.02391691878437996, Final Batch Loss: 0.011759246699512005\n",
      "Epoch 3269, Loss: 0.02561576198786497, Final Batch Loss: 0.016667895019054413\n",
      "Epoch 3270, Loss: 0.03656372241675854, Final Batch Loss: 0.020800955593585968\n",
      "Epoch 3271, Loss: 0.047604577615857124, Final Batch Loss: 0.031638458371162415\n",
      "Epoch 3272, Loss: 0.03419045777991414, Final Batch Loss: 0.00418058643117547\n",
      "Epoch 3273, Loss: 0.044126491993665695, Final Batch Loss: 0.025179000571370125\n",
      "Epoch 3274, Loss: 0.037900397554039955, Final Batch Loss: 0.011092973873019218\n",
      "Epoch 3275, Loss: 0.03484231838956475, Final Batch Loss: 0.028454849496483803\n",
      "Epoch 3276, Loss: 0.040470275562256575, Final Batch Loss: 0.005023125093430281\n",
      "Epoch 3277, Loss: 0.021186907775700092, Final Batch Loss: 0.009534850716590881\n",
      "Epoch 3278, Loss: 0.03876239713281393, Final Batch Loss: 0.02510061301290989\n",
      "Epoch 3279, Loss: 0.018568716011941433, Final Batch Loss: 0.006146825850009918\n",
      "Epoch 3280, Loss: 0.04498342610895634, Final Batch Loss: 0.02661396935582161\n",
      "Epoch 3281, Loss: 0.032269351184368134, Final Batch Loss: 0.025918062776327133\n",
      "Epoch 3282, Loss: 0.020416937302798033, Final Batch Loss: 0.007610166911035776\n",
      "Epoch 3283, Loss: 0.04150285106152296, Final Batch Loss: 0.02889225445687771\n",
      "Epoch 3284, Loss: 0.02822099905461073, Final Batch Loss: 0.00897164735943079\n",
      "Epoch 3285, Loss: 0.020534133538603783, Final Batch Loss: 0.009079036302864552\n",
      "Epoch 3286, Loss: 0.039224433712661266, Final Batch Loss: 0.025247499346733093\n",
      "Epoch 3287, Loss: 0.042797502130270004, Final Batch Loss: 0.017557404935359955\n",
      "Epoch 3288, Loss: 0.023160962387919426, Final Batch Loss: 0.0053650494664907455\n",
      "Epoch 3289, Loss: 0.059354694560170174, Final Batch Loss: 0.02960878238081932\n",
      "Epoch 3290, Loss: 0.029910425655543804, Final Batch Loss: 0.010885818861424923\n",
      "Epoch 3291, Loss: 0.012803107965737581, Final Batch Loss: 0.0065908897668123245\n",
      "Epoch 3292, Loss: 0.02514878287911415, Final Batch Loss: 0.012790312990546227\n",
      "Epoch 3293, Loss: 0.02879069745540619, Final Batch Loss: 0.011837523430585861\n",
      "Epoch 3294, Loss: 0.02030486147850752, Final Batch Loss: 0.009718475863337517\n",
      "Epoch 3295, Loss: 0.03205557819455862, Final Batch Loss: 0.012979072518646717\n",
      "Epoch 3296, Loss: 0.035362870432436466, Final Batch Loss: 0.006473218090832233\n",
      "Epoch 3297, Loss: 0.01564212143421173, Final Batch Loss: 0.006388558074831963\n",
      "Epoch 3298, Loss: 0.046550383791327477, Final Batch Loss: 0.02792304940521717\n",
      "Epoch 3299, Loss: 0.024127352982759476, Final Batch Loss: 0.008457683026790619\n",
      "Epoch 3300, Loss: 0.04963168874382973, Final Batch Loss: 0.018485363572835922\n",
      "Epoch 3301, Loss: 0.07373315095901489, Final Batch Loss: 0.04210837930440903\n",
      "Epoch 3302, Loss: 0.027980592101812363, Final Batch Loss: 0.015979230403900146\n",
      "Epoch 3303, Loss: 0.02423843275755644, Final Batch Loss: 0.010133250616490841\n",
      "Epoch 3304, Loss: 0.09783200174570084, Final Batch Loss: 0.024112604558467865\n",
      "Epoch 3305, Loss: 0.02069141948595643, Final Batch Loss: 0.007038357201963663\n",
      "Epoch 3306, Loss: 0.032361604273319244, Final Batch Loss: 0.023365870118141174\n",
      "Epoch 3307, Loss: 0.050751520320773125, Final Batch Loss: 0.012218261137604713\n",
      "Epoch 3308, Loss: 0.019017554819583893, Final Batch Loss: 0.004995807074010372\n",
      "Epoch 3309, Loss: 0.025600391440093517, Final Batch Loss: 0.017757296562194824\n",
      "Epoch 3310, Loss: 0.04421481303870678, Final Batch Loss: 0.02842731773853302\n",
      "Epoch 3311, Loss: 0.03931730426847935, Final Batch Loss: 0.01834411919116974\n",
      "Epoch 3312, Loss: 0.025011767633259296, Final Batch Loss: 0.012065889313817024\n",
      "Epoch 3313, Loss: 0.04819344636052847, Final Batch Loss: 0.008438839577138424\n",
      "Epoch 3314, Loss: 0.024582463316619396, Final Batch Loss: 0.011057659052312374\n",
      "Epoch 3315, Loss: 0.02452183421701193, Final Batch Loss: 0.008098452351987362\n",
      "Epoch 3316, Loss: 0.034489032812416553, Final Batch Loss: 0.009249250404536724\n",
      "Epoch 3317, Loss: 0.04254269041121006, Final Batch Loss: 0.01704678125679493\n",
      "Epoch 3318, Loss: 0.03731153346598148, Final Batch Loss: 0.022413013502955437\n",
      "Epoch 3319, Loss: 0.059382881969213486, Final Batch Loss: 0.0388481430709362\n",
      "Epoch 3320, Loss: 0.026197880506515503, Final Batch Loss: 0.01584138162434101\n",
      "Epoch 3321, Loss: 0.01862007938325405, Final Batch Loss: 0.003986219875514507\n",
      "Epoch 3322, Loss: 0.015286504290997982, Final Batch Loss: 0.005860060453414917\n",
      "Epoch 3323, Loss: 0.05674376152455807, Final Batch Loss: 0.03158765658736229\n",
      "Epoch 3324, Loss: 0.011719836853444576, Final Batch Loss: 0.007797217462211847\n",
      "Epoch 3325, Loss: 0.019367183558642864, Final Batch Loss: 0.010929920710623264\n",
      "Epoch 3326, Loss: 0.031085847876966, Final Batch Loss: 0.01742737367749214\n",
      "Epoch 3327, Loss: 0.020696619525551796, Final Batch Loss: 0.008037236519157887\n",
      "Epoch 3328, Loss: 0.014040036126971245, Final Batch Loss: 0.005476849153637886\n",
      "Epoch 3329, Loss: 0.05636521056294441, Final Batch Loss: 0.021392345428466797\n",
      "Epoch 3330, Loss: 0.030022566206753254, Final Batch Loss: 0.017731446772813797\n",
      "Epoch 3331, Loss: 0.018627514131367207, Final Batch Loss: 0.00873685721307993\n",
      "Epoch 3332, Loss: 0.06574409641325474, Final Batch Loss: 0.018961085006594658\n",
      "Epoch 3333, Loss: 0.02557494957000017, Final Batch Loss: 0.01134970411658287\n",
      "Epoch 3334, Loss: 0.04464629292488098, Final Batch Loss: 0.012373603880405426\n",
      "Epoch 3335, Loss: 0.011048452463001013, Final Batch Loss: 0.005677524022758007\n",
      "Epoch 3336, Loss: 0.028005444444715977, Final Batch Loss: 0.012285345233976841\n",
      "Epoch 3337, Loss: 0.01575117139145732, Final Batch Loss: 0.005322664510458708\n",
      "Epoch 3338, Loss: 0.030956371687352657, Final Batch Loss: 0.01941876858472824\n",
      "Epoch 3339, Loss: 0.025238950736820698, Final Batch Loss: 0.01627221703529358\n",
      "Epoch 3340, Loss: 0.025601345114409924, Final Batch Loss: 0.012757696211338043\n",
      "Epoch 3341, Loss: 0.04119125846773386, Final Batch Loss: 0.03269587829709053\n",
      "Epoch 3342, Loss: 0.02586680557578802, Final Batch Loss: 0.009040300734341145\n",
      "Epoch 3343, Loss: 0.04573762742802501, Final Batch Loss: 0.03843284770846367\n",
      "Epoch 3344, Loss: 0.015155084896832705, Final Batch Loss: 0.009629791602492332\n",
      "Epoch 3345, Loss: 0.03710534609854221, Final Batch Loss: 0.01421142928302288\n",
      "Epoch 3346, Loss: 0.0509380791336298, Final Batch Loss: 0.043687425553798676\n",
      "Epoch 3347, Loss: 0.026992210187017918, Final Batch Loss: 0.008964651264250278\n",
      "Epoch 3348, Loss: 0.020594904199242592, Final Batch Loss: 0.010732253082096577\n",
      "Epoch 3349, Loss: 0.036041537299752235, Final Batch Loss: 0.013311522081494331\n",
      "Epoch 3350, Loss: 0.03496627323329449, Final Batch Loss: 0.018776167184114456\n",
      "Epoch 3351, Loss: 0.026519915089011192, Final Batch Loss: 0.013666871003806591\n",
      "Epoch 3352, Loss: 0.020462671294808388, Final Batch Loss: 0.008454267866909504\n",
      "Epoch 3353, Loss: 0.03597847931087017, Final Batch Loss: 0.0194667037576437\n",
      "Epoch 3354, Loss: 0.08462903462350368, Final Batch Loss: 0.010636983439326286\n",
      "Epoch 3355, Loss: 0.02448723278939724, Final Batch Loss: 0.01773446425795555\n",
      "Epoch 3356, Loss: 0.04983673617243767, Final Batch Loss: 0.028385989367961884\n",
      "Epoch 3357, Loss: 0.015206830110400915, Final Batch Loss: 0.0068248300813138485\n",
      "Epoch 3358, Loss: 0.05464915093034506, Final Batch Loss: 0.0458531379699707\n",
      "Epoch 3359, Loss: 0.04078507423400879, Final Batch Loss: 0.018589524552226067\n",
      "Epoch 3360, Loss: 0.0293632997199893, Final Batch Loss: 0.014819910749793053\n",
      "Epoch 3361, Loss: 0.05082143563777208, Final Batch Loss: 0.03760676831007004\n",
      "Epoch 3362, Loss: 0.05250310339033604, Final Batch Loss: 0.03879992663860321\n",
      "Epoch 3363, Loss: 0.02455971436575055, Final Batch Loss: 0.006112805102020502\n",
      "Epoch 3364, Loss: 0.044185174629092216, Final Batch Loss: 0.022538254037499428\n",
      "Epoch 3365, Loss: 0.021766590885818005, Final Batch Loss: 0.006360185332596302\n",
      "Epoch 3366, Loss: 0.03192414762452245, Final Batch Loss: 0.026988539844751358\n",
      "Epoch 3367, Loss: 0.02378331869840622, Final Batch Loss: 0.012491903267800808\n",
      "Epoch 3368, Loss: 0.06810676399618387, Final Batch Loss: 0.060179393738508224\n",
      "Epoch 3369, Loss: 0.05445389449596405, Final Batch Loss: 0.021609067916870117\n",
      "Epoch 3370, Loss: 0.015206496696919203, Final Batch Loss: 0.008232065476477146\n",
      "Epoch 3371, Loss: 0.027973411604762077, Final Batch Loss: 0.012619169428944588\n",
      "Epoch 3372, Loss: 0.04818879906088114, Final Batch Loss: 0.037668388336896896\n",
      "Epoch 3373, Loss: 0.02266509458422661, Final Batch Loss: 0.011366769671440125\n",
      "Epoch 3374, Loss: 0.01997426152229309, Final Batch Loss: 0.01069995854049921\n",
      "Epoch 3375, Loss: 0.05576460808515549, Final Batch Loss: 0.010370679199695587\n",
      "Epoch 3376, Loss: 0.031706320121884346, Final Batch Loss: 0.017574701458215714\n",
      "Epoch 3377, Loss: 0.03128189221024513, Final Batch Loss: 0.018721355125308037\n",
      "Epoch 3378, Loss: 0.028415598906576633, Final Batch Loss: 0.01378592662513256\n",
      "Epoch 3379, Loss: 0.05434695445001125, Final Batch Loss: 0.012401284649968147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3380, Loss: 0.07423336245119572, Final Batch Loss: 0.025861898437142372\n",
      "Epoch 3381, Loss: 0.05502695683389902, Final Batch Loss: 0.01530239824205637\n",
      "Epoch 3382, Loss: 0.019422731827944517, Final Batch Loss: 0.005293653812259436\n",
      "Epoch 3383, Loss: 0.055858975276350975, Final Batch Loss: 0.013234445825219154\n",
      "Epoch 3384, Loss: 0.03361666202545166, Final Batch Loss: 0.019228069111704826\n",
      "Epoch 3385, Loss: 0.018942286260426044, Final Batch Loss: 0.011569555848836899\n",
      "Epoch 3386, Loss: 0.026448778808116913, Final Batch Loss: 0.021018197759985924\n",
      "Epoch 3387, Loss: 0.01796353654935956, Final Batch Loss: 0.007556999567896128\n",
      "Epoch 3388, Loss: 0.027533440617844462, Final Batch Loss: 0.003300859360024333\n",
      "Epoch 3389, Loss: 0.04191620834171772, Final Batch Loss: 0.014083260670304298\n",
      "Epoch 3390, Loss: 0.013870466034859419, Final Batch Loss: 0.004409234505146742\n",
      "Epoch 3391, Loss: 0.01446295203641057, Final Batch Loss: 0.005958524066954851\n",
      "Epoch 3392, Loss: 0.020389332436025143, Final Batch Loss: 0.010726278647780418\n",
      "Epoch 3393, Loss: 0.040123395854607224, Final Batch Loss: 0.0038679505232721567\n",
      "Epoch 3394, Loss: 0.063839809037745, Final Batch Loss: 0.052412617951631546\n",
      "Epoch 3395, Loss: 0.02903585694730282, Final Batch Loss: 0.008939770981669426\n",
      "Epoch 3396, Loss: 0.037073431070894, Final Batch Loss: 0.006558939348906279\n",
      "Epoch 3397, Loss: 0.03346129972487688, Final Batch Loss: 0.008475176058709621\n",
      "Epoch 3398, Loss: 0.04385075345635414, Final Batch Loss: 0.00807105004787445\n",
      "Epoch 3399, Loss: 0.019425008911639452, Final Batch Loss: 0.013083134777843952\n",
      "Epoch 3400, Loss: 0.021063098683953285, Final Batch Loss: 0.01683965139091015\n",
      "Epoch 3401, Loss: 0.015903250314295292, Final Batch Loss: 0.007986409589648247\n",
      "Epoch 3402, Loss: 0.03288398124277592, Final Batch Loss: 0.020919641479849815\n",
      "Epoch 3403, Loss: 0.026457559783011675, Final Batch Loss: 0.02124331332743168\n",
      "Epoch 3404, Loss: 0.037597304210066795, Final Batch Loss: 0.023355327546596527\n",
      "Epoch 3405, Loss: 0.04611525870859623, Final Batch Loss: 0.023755988106131554\n",
      "Epoch 3406, Loss: 0.019694476388394833, Final Batch Loss: 0.014081322588026524\n",
      "Epoch 3407, Loss: 0.05833758972585201, Final Batch Loss: 0.05054812505841255\n",
      "Epoch 3408, Loss: 0.013643188867717981, Final Batch Loss: 0.007072136737406254\n",
      "Epoch 3409, Loss: 0.05158006399869919, Final Batch Loss: 0.03272454813122749\n",
      "Epoch 3410, Loss: 0.040129791013896465, Final Batch Loss: 0.014027814380824566\n",
      "Epoch 3411, Loss: 0.03729517012834549, Final Batch Loss: 0.017168328166007996\n",
      "Epoch 3412, Loss: 0.048599278554320335, Final Batch Loss: 0.03849836066365242\n",
      "Epoch 3413, Loss: 0.02151133492588997, Final Batch Loss: 0.012677020393311977\n",
      "Epoch 3414, Loss: 0.06513270549476147, Final Batch Loss: 0.030490057542920113\n",
      "Epoch 3415, Loss: 0.024312566965818405, Final Batch Loss: 0.009818811900913715\n",
      "Epoch 3416, Loss: 0.029537489637732506, Final Batch Loss: 0.016263699159026146\n",
      "Epoch 3417, Loss: 0.01792480144649744, Final Batch Loss: 0.004684781655669212\n",
      "Epoch 3418, Loss: 0.02444085292518139, Final Batch Loss: 0.01370997168123722\n",
      "Epoch 3419, Loss: 0.023262158036231995, Final Batch Loss: 0.013233037665486336\n",
      "Epoch 3420, Loss: 0.03561517223715782, Final Batch Loss: 0.020276151597499847\n",
      "Epoch 3421, Loss: 0.026765468530356884, Final Batch Loss: 0.015198239125311375\n",
      "Epoch 3422, Loss: 0.04459849139675498, Final Batch Loss: 0.007142195012420416\n",
      "Epoch 3423, Loss: 0.013870911672711372, Final Batch Loss: 0.005009307526051998\n",
      "Epoch 3424, Loss: 0.017603649757802486, Final Batch Loss: 0.009646967984735966\n",
      "Epoch 3425, Loss: 0.021786021534353495, Final Batch Loss: 0.005336724687367678\n",
      "Epoch 3426, Loss: 0.08143813349306583, Final Batch Loss: 0.012224758043885231\n",
      "Epoch 3427, Loss: 0.037629541009664536, Final Batch Loss: 0.012800322845578194\n",
      "Epoch 3428, Loss: 0.041478986851871014, Final Batch Loss: 0.010487095452845097\n",
      "Epoch 3429, Loss: 0.013784091453999281, Final Batch Loss: 0.009043804369866848\n",
      "Epoch 3430, Loss: 0.03647671081125736, Final Batch Loss: 0.007684122771024704\n",
      "Epoch 3431, Loss: 0.03729061409831047, Final Batch Loss: 0.018201207742094994\n",
      "Epoch 3432, Loss: 0.03431505709886551, Final Batch Loss: 0.013969335705041885\n",
      "Epoch 3433, Loss: 0.031800420954823494, Final Batch Loss: 0.01034911535680294\n",
      "Epoch 3434, Loss: 0.02423402015119791, Final Batch Loss: 0.011920166201889515\n",
      "Epoch 3435, Loss: 0.041877251118421555, Final Batch Loss: 0.03015635348856449\n",
      "Epoch 3436, Loss: 0.03275309503078461, Final Batch Loss: 0.017768895253539085\n",
      "Epoch 3437, Loss: 0.02969044353812933, Final Batch Loss: 0.014751506969332695\n",
      "Epoch 3438, Loss: 0.020881597883999348, Final Batch Loss: 0.012643183581531048\n",
      "Epoch 3439, Loss: 0.0326728499494493, Final Batch Loss: 0.02633042447268963\n",
      "Epoch 3440, Loss: 0.06899946369230747, Final Batch Loss: 0.02432834915816784\n",
      "Epoch 3441, Loss: 0.028712772764265537, Final Batch Loss: 0.004099120385944843\n",
      "Epoch 3442, Loss: 0.059264855459332466, Final Batch Loss: 0.04242083802819252\n",
      "Epoch 3443, Loss: 0.027930296026170254, Final Batch Loss: 0.01834685355424881\n",
      "Epoch 3444, Loss: 0.06546977907419205, Final Batch Loss: 0.042606402188539505\n",
      "Epoch 3445, Loss: 0.05145267769694328, Final Batch Loss: 0.023247547447681427\n",
      "Epoch 3446, Loss: 0.01866468181833625, Final Batch Loss: 0.012436561286449432\n",
      "Epoch 3447, Loss: 0.018215215764939785, Final Batch Loss: 0.009064230136573315\n",
      "Epoch 3448, Loss: 0.05573602579534054, Final Batch Loss: 0.04507818445563316\n",
      "Epoch 3449, Loss: 0.034033712930977345, Final Batch Loss: 0.012582273222506046\n",
      "Epoch 3450, Loss: 0.024029756896197796, Final Batch Loss: 0.006731781177222729\n",
      "Epoch 3451, Loss: 0.02936227060854435, Final Batch Loss: 0.015866445377469063\n",
      "Epoch 3452, Loss: 0.03728612698614597, Final Batch Loss: 0.016901394352316856\n",
      "Epoch 3453, Loss: 0.09742308408021927, Final Batch Loss: 0.08460967242717743\n",
      "Epoch 3454, Loss: 0.029287423472851515, Final Batch Loss: 0.004898001905530691\n",
      "Epoch 3455, Loss: 0.01900345692411065, Final Batch Loss: 0.014225078746676445\n",
      "Epoch 3456, Loss: 0.05852537415921688, Final Batch Loss: 0.03291362151503563\n",
      "Epoch 3457, Loss: 0.028774861246347427, Final Batch Loss: 0.023558929562568665\n",
      "Epoch 3458, Loss: 0.01665452867746353, Final Batch Loss: 0.010218057781457901\n",
      "Epoch 3459, Loss: 0.032005222514271736, Final Batch Loss: 0.012417953461408615\n",
      "Epoch 3460, Loss: 0.01713349763303995, Final Batch Loss: 0.004629511386156082\n",
      "Epoch 3461, Loss: 0.04457487538456917, Final Batch Loss: 0.03570394217967987\n",
      "Epoch 3462, Loss: 0.05513392202556133, Final Batch Loss: 0.03809270262718201\n",
      "Epoch 3463, Loss: 0.031466925516724586, Final Batch Loss: 0.010058773681521416\n",
      "Epoch 3464, Loss: 0.01981364330276847, Final Batch Loss: 0.01306021399796009\n",
      "Epoch 3465, Loss: 0.01661031786352396, Final Batch Loss: 0.0069092558696866035\n",
      "Epoch 3466, Loss: 0.08208150696009398, Final Batch Loss: 0.06720540672540665\n",
      "Epoch 3467, Loss: 0.038363071624189615, Final Batch Loss: 0.006752012763172388\n",
      "Epoch 3468, Loss: 0.026013178750872612, Final Batch Loss: 0.016639849171042442\n",
      "Epoch 3469, Loss: 0.02562341559678316, Final Batch Loss: 0.009229288436472416\n",
      "Epoch 3470, Loss: 0.024599371943622828, Final Batch Loss: 0.007566933054476976\n",
      "Epoch 3471, Loss: 0.018164414446800947, Final Batch Loss: 0.011518722400069237\n",
      "Epoch 3472, Loss: 0.029192856512963772, Final Batch Loss: 0.011734134517610073\n",
      "Epoch 3473, Loss: 0.012646109331399202, Final Batch Loss: 0.008342699147760868\n",
      "Epoch 3474, Loss: 0.033246626146137714, Final Batch Loss: 0.005940807051956654\n",
      "Epoch 3475, Loss: 0.01637968886643648, Final Batch Loss: 0.004452057182788849\n",
      "Epoch 3476, Loss: 0.017006050795316696, Final Batch Loss: 0.007988336496055126\n",
      "Epoch 3477, Loss: 0.05678354576230049, Final Batch Loss: 0.015563547611236572\n",
      "Epoch 3478, Loss: 0.05237087234854698, Final Batch Loss: 0.024872202426195145\n",
      "Epoch 3479, Loss: 0.028971943072974682, Final Batch Loss: 0.012589621357619762\n",
      "Epoch 3480, Loss: 0.01882337941788137, Final Batch Loss: 0.0033545063342899084\n",
      "Epoch 3481, Loss: 0.03964091744273901, Final Batch Loss: 0.011931260116398335\n",
      "Epoch 3482, Loss: 0.032362532801926136, Final Batch Loss: 0.02218911610543728\n",
      "Epoch 3483, Loss: 0.011685123667120934, Final Batch Loss: 0.005408925469964743\n",
      "Epoch 3484, Loss: 0.07246294897049665, Final Batch Loss: 0.06407412141561508\n",
      "Epoch 3485, Loss: 0.05532117560505867, Final Batch Loss: 0.033498018980026245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3486, Loss: 0.04118038434535265, Final Batch Loss: 0.027736041694879532\n",
      "Epoch 3487, Loss: 0.04209255240857601, Final Batch Loss: 0.01870330609381199\n",
      "Epoch 3488, Loss: 0.03431674838066101, Final Batch Loss: 0.007282990962266922\n",
      "Epoch 3489, Loss: 0.08710379339754581, Final Batch Loss: 0.010567480698227882\n",
      "Epoch 3490, Loss: 0.03766726888716221, Final Batch Loss: 0.026544243097305298\n",
      "Epoch 3491, Loss: 0.03846747521311045, Final Batch Loss: 0.014102273620665073\n",
      "Epoch 3492, Loss: 0.023340962827205658, Final Batch Loss: 0.010374041274189949\n",
      "Epoch 3493, Loss: 0.02486533299088478, Final Batch Loss: 0.016848281025886536\n",
      "Epoch 3494, Loss: 0.032818797044456005, Final Batch Loss: 0.008552917279303074\n",
      "Epoch 3495, Loss: 0.029472838155925274, Final Batch Loss: 0.00861597154289484\n",
      "Epoch 3496, Loss: 0.022173690609633923, Final Batch Loss: 0.010174127295613289\n",
      "Epoch 3497, Loss: 0.10045492555946112, Final Batch Loss: 0.006464949809014797\n",
      "Epoch 3498, Loss: 0.04038880951702595, Final Batch Loss: 0.02798653207719326\n",
      "Epoch 3499, Loss: 0.02273445948958397, Final Batch Loss: 0.009841559454798698\n",
      "Epoch 3500, Loss: 0.017740905284881592, Final Batch Loss: 0.008616440929472446\n",
      "Epoch 3501, Loss: 0.02290385589003563, Final Batch Loss: 0.007851778529584408\n",
      "Epoch 3502, Loss: 0.030409494414925575, Final Batch Loss: 0.004631552845239639\n",
      "Epoch 3503, Loss: 0.02226484939455986, Final Batch Loss: 0.011217523366212845\n",
      "Epoch 3504, Loss: 0.019415786489844322, Final Batch Loss: 0.013601534068584442\n",
      "Epoch 3505, Loss: 0.023637293837964535, Final Batch Loss: 0.009504072368144989\n",
      "Epoch 3506, Loss: 0.04659636691212654, Final Batch Loss: 0.00906481221318245\n",
      "Epoch 3507, Loss: 0.030852927826344967, Final Batch Loss: 0.02028091810643673\n",
      "Epoch 3508, Loss: 0.02751143416389823, Final Batch Loss: 0.021900352090597153\n",
      "Epoch 3509, Loss: 0.0631729494780302, Final Batch Loss: 0.041913874447345734\n",
      "Epoch 3510, Loss: 0.09972451999783516, Final Batch Loss: 0.020229864865541458\n",
      "Epoch 3511, Loss: 0.028659170493483543, Final Batch Loss: 0.015614286996424198\n",
      "Epoch 3512, Loss: 0.04455836862325668, Final Batch Loss: 0.010347194969654083\n",
      "Epoch 3513, Loss: 0.035417208448052406, Final Batch Loss: 0.015604032203555107\n",
      "Epoch 3514, Loss: 0.024000109173357487, Final Batch Loss: 0.005300103686749935\n",
      "Epoch 3515, Loss: 0.02730347216129303, Final Batch Loss: 0.011987803503870964\n",
      "Epoch 3516, Loss: 0.011771991848945618, Final Batch Loss: 0.004929730668663979\n",
      "Epoch 3517, Loss: 0.016511036548763514, Final Batch Loss: 0.010823817923665047\n",
      "Epoch 3518, Loss: 0.020074566826224327, Final Batch Loss: 0.010612755082547665\n",
      "Epoch 3519, Loss: 0.012685414869338274, Final Batch Loss: 0.003326288890093565\n",
      "Epoch 3520, Loss: 0.019248507916927338, Final Batch Loss: 0.010043716058135033\n",
      "Epoch 3521, Loss: 0.02554150950163603, Final Batch Loss: 0.015666121616959572\n",
      "Epoch 3522, Loss: 0.0077522918581962585, Final Batch Loss: 0.0026099481619894505\n",
      "Epoch 3523, Loss: 0.047414630418643355, Final Batch Loss: 0.0033087057527154684\n",
      "Epoch 3524, Loss: 0.027472923509776592, Final Batch Loss: 0.011157705448567867\n",
      "Epoch 3525, Loss: 0.028182084672152996, Final Batch Loss: 0.005545084364712238\n",
      "Epoch 3526, Loss: 0.0767689123749733, Final Batch Loss: 0.013481974601745605\n",
      "Epoch 3527, Loss: 0.06488410569727421, Final Batch Loss: 0.05420774221420288\n",
      "Epoch 3528, Loss: 0.016473849304020405, Final Batch Loss: 0.007913944311439991\n",
      "Epoch 3529, Loss: 0.023024456575512886, Final Batch Loss: 0.009797245264053345\n",
      "Epoch 3530, Loss: 0.014312561135739088, Final Batch Loss: 0.009959613904356956\n",
      "Epoch 3531, Loss: 0.014235141221433878, Final Batch Loss: 0.007845396175980568\n",
      "Epoch 3532, Loss: 0.05325636174529791, Final Batch Loss: 0.0400676354765892\n",
      "Epoch 3533, Loss: 0.03206869587302208, Final Batch Loss: 0.022942179813981056\n",
      "Epoch 3534, Loss: 0.04832375794649124, Final Batch Loss: 0.014124970883131027\n",
      "Epoch 3535, Loss: 0.04227726627141237, Final Batch Loss: 0.027036095038056374\n",
      "Epoch 3536, Loss: 0.04921799153089523, Final Batch Loss: 0.017968837171792984\n",
      "Epoch 3537, Loss: 0.03392440406605601, Final Batch Loss: 0.003947650548070669\n",
      "Epoch 3538, Loss: 0.07155441492795944, Final Batch Loss: 0.0382704921066761\n",
      "Epoch 3539, Loss: 0.025398108176887035, Final Batch Loss: 0.008005903102457523\n",
      "Epoch 3540, Loss: 0.014147807378321886, Final Batch Loss: 0.006593006197363138\n",
      "Epoch 3541, Loss: 0.030731916427612305, Final Batch Loss: 0.01630563475191593\n",
      "Epoch 3542, Loss: 0.021086541935801506, Final Batch Loss: 0.006357621401548386\n",
      "Epoch 3543, Loss: 0.03234492428600788, Final Batch Loss: 0.02113567292690277\n",
      "Epoch 3544, Loss: 0.023141013458371162, Final Batch Loss: 0.012483787722885609\n",
      "Epoch 3545, Loss: 0.011848692316561937, Final Batch Loss: 0.004329896066337824\n",
      "Epoch 3546, Loss: 0.06779422983527184, Final Batch Loss: 0.01732471212744713\n",
      "Epoch 3547, Loss: 0.048897042870521545, Final Batch Loss: 0.02417062222957611\n",
      "Epoch 3548, Loss: 0.051081458106637, Final Batch Loss: 0.02608661912381649\n",
      "Epoch 3549, Loss: 0.011953370412811637, Final Batch Loss: 0.0023721966426819563\n",
      "Epoch 3550, Loss: 0.013741218950599432, Final Batch Loss: 0.004370264243334532\n",
      "Epoch 3551, Loss: 0.02176508493721485, Final Batch Loss: 0.014947145245969296\n",
      "Epoch 3552, Loss: 0.027761994395405054, Final Batch Loss: 0.021207982674241066\n",
      "Epoch 3553, Loss: 0.04888142831623554, Final Batch Loss: 0.021564437076449394\n",
      "Epoch 3554, Loss: 0.01909240009263158, Final Batch Loss: 0.006953354459255934\n",
      "Epoch 3555, Loss: 0.038946257904171944, Final Batch Loss: 0.014506738632917404\n",
      "Epoch 3556, Loss: 0.06398076377809048, Final Batch Loss: 0.023542163893580437\n",
      "Epoch 3557, Loss: 0.048566678538918495, Final Batch Loss: 0.025450561195611954\n",
      "Epoch 3558, Loss: 0.06373004615306854, Final Batch Loss: 0.04453320801258087\n",
      "Epoch 3559, Loss: 0.024321526288986206, Final Batch Loss: 0.008556155487895012\n",
      "Epoch 3560, Loss: 0.0442176703363657, Final Batch Loss: 0.019289668649435043\n",
      "Epoch 3561, Loss: 0.026134232059121132, Final Batch Loss: 0.0065073855221271515\n",
      "Epoch 3562, Loss: 0.03102947724983096, Final Batch Loss: 0.005299785640090704\n",
      "Epoch 3563, Loss: 0.020798871759325266, Final Batch Loss: 0.0145194660872221\n",
      "Epoch 3564, Loss: 0.14439693838357925, Final Batch Loss: 0.12716375291347504\n",
      "Epoch 3565, Loss: 0.04135940223932266, Final Batch Loss: 0.03006208874285221\n",
      "Epoch 3566, Loss: 0.01993180252611637, Final Batch Loss: 0.012059700675308704\n",
      "Epoch 3567, Loss: 0.026661867275834084, Final Batch Loss: 0.01456404384225607\n",
      "Epoch 3568, Loss: 0.055161043535918, Final Batch Loss: 0.04776216298341751\n",
      "Epoch 3569, Loss: 0.0369289992377162, Final Batch Loss: 0.022787241265177727\n",
      "Epoch 3570, Loss: 0.023905495181679726, Final Batch Loss: 0.013835887424647808\n",
      "Epoch 3571, Loss: 0.1301194466650486, Final Batch Loss: 0.1081087589263916\n",
      "Epoch 3572, Loss: 0.016769408714026213, Final Batch Loss: 0.004159809555858374\n",
      "Epoch 3573, Loss: 0.10589612647891045, Final Batch Loss: 0.07370735704898834\n",
      "Epoch 3574, Loss: 0.015467466320842505, Final Batch Loss: 0.004147802945226431\n",
      "Epoch 3575, Loss: 0.014102054294198751, Final Batch Loss: 0.004965008702129126\n",
      "Epoch 3576, Loss: 0.01387697970494628, Final Batch Loss: 0.006727144587785006\n",
      "Epoch 3577, Loss: 0.09221573919057846, Final Batch Loss: 0.04243364930152893\n",
      "Epoch 3578, Loss: 0.01241235388442874, Final Batch Loss: 0.0048246788792312145\n",
      "Epoch 3579, Loss: 0.038285969756543636, Final Batch Loss: 0.023416414856910706\n",
      "Epoch 3580, Loss: 0.015258568339049816, Final Batch Loss: 0.0041747987270355225\n",
      "Epoch 3581, Loss: 0.0160187897272408, Final Batch Loss: 0.007099951151758432\n",
      "Epoch 3582, Loss: 0.014656420331448317, Final Batch Loss: 0.005722608882933855\n",
      "Epoch 3583, Loss: 0.037462868727743626, Final Batch Loss: 0.024007374420762062\n",
      "Epoch 3584, Loss: 0.03408880531787872, Final Batch Loss: 0.007800271734595299\n",
      "Epoch 3585, Loss: 0.05429571866989136, Final Batch Loss: 0.03653820976614952\n",
      "Epoch 3586, Loss: 0.0181989383418113, Final Batch Loss: 0.0035143170971423388\n",
      "Epoch 3587, Loss: 0.0622668843716383, Final Batch Loss: 0.03302369639277458\n",
      "Epoch 3588, Loss: 0.027307522017508745, Final Batch Loss: 0.004658399615436792\n",
      "Epoch 3589, Loss: 0.06642101611942053, Final Batch Loss: 0.0531700924038887\n",
      "Epoch 3590, Loss: 0.04798543360084295, Final Batch Loss: 0.01498312409967184\n",
      "Epoch 3591, Loss: 0.04231778718531132, Final Batch Loss: 0.017310362309217453\n",
      "Epoch 3592, Loss: 0.04297940991818905, Final Batch Loss: 0.015967704355716705\n",
      "Epoch 3593, Loss: 0.018889185041189194, Final Batch Loss: 0.010794993489980698\n",
      "Epoch 3594, Loss: 0.04777685925364494, Final Batch Loss: 0.012475017458200455\n",
      "Epoch 3595, Loss: 0.02544399444013834, Final Batch Loss: 0.01485078688710928\n",
      "Epoch 3596, Loss: 0.05004967004060745, Final Batch Loss: 0.009506270289421082\n",
      "Epoch 3597, Loss: 0.09602603502571583, Final Batch Loss: 0.07684396952390671\n",
      "Epoch 3598, Loss: 0.015470550861209631, Final Batch Loss: 0.006035337690263987\n",
      "Epoch 3599, Loss: 0.04354420490562916, Final Batch Loss: 0.03554709255695343\n",
      "Epoch 3600, Loss: 0.030869870679453015, Final Batch Loss: 0.0038659677375108004\n",
      "Epoch 3601, Loss: 0.014218467753380537, Final Batch Loss: 0.006639221683144569\n",
      "Epoch 3602, Loss: 0.023781505413353443, Final Batch Loss: 0.010978803969919682\n",
      "Epoch 3603, Loss: 0.014293847605586052, Final Batch Loss: 0.006649998482316732\n",
      "Epoch 3604, Loss: 0.14356516301631927, Final Batch Loss: 0.08285409957170486\n",
      "Epoch 3605, Loss: 0.02357962541282177, Final Batch Loss: 0.005888022482395172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3606, Loss: 0.033232745714485645, Final Batch Loss: 0.030061140656471252\n",
      "Epoch 3607, Loss: 0.01605258695781231, Final Batch Loss: 0.007672717794775963\n",
      "Epoch 3608, Loss: 0.04531909432262182, Final Batch Loss: 0.03962242603302002\n",
      "Epoch 3609, Loss: 0.034141465090215206, Final Batch Loss: 0.02268509566783905\n",
      "Epoch 3610, Loss: 0.01262963842600584, Final Batch Loss: 0.006843403913080692\n",
      "Epoch 3611, Loss: 0.05717268958687782, Final Batch Loss: 0.030373193323612213\n",
      "Epoch 3612, Loss: 0.030019206926226616, Final Batch Loss: 0.011553067713975906\n",
      "Epoch 3613, Loss: 0.0550086279399693, Final Batch Loss: 0.005307112354785204\n",
      "Epoch 3614, Loss: 0.017039510421454906, Final Batch Loss: 0.007284039631485939\n",
      "Epoch 3615, Loss: 0.07360664568841457, Final Batch Loss: 0.04859176278114319\n",
      "Epoch 3616, Loss: 0.030007410794496536, Final Batch Loss: 0.006793541833758354\n",
      "Epoch 3617, Loss: 0.022428118623793125, Final Batch Loss: 0.013872015289962292\n",
      "Epoch 3618, Loss: 0.02619303110986948, Final Batch Loss: 0.01883494295179844\n",
      "Epoch 3619, Loss: 0.02218809351325035, Final Batch Loss: 0.005484553053975105\n",
      "Epoch 3620, Loss: 0.01902319584041834, Final Batch Loss: 0.008289854042232037\n",
      "Epoch 3621, Loss: 0.06557011231780052, Final Batch Loss: 0.046551577746868134\n",
      "Epoch 3622, Loss: 0.04291391745209694, Final Batch Loss: 0.019830485805869102\n",
      "Epoch 3623, Loss: 0.01877485355362296, Final Batch Loss: 0.005866324994713068\n",
      "Epoch 3624, Loss: 0.037748679518699646, Final Batch Loss: 0.026598284021019936\n",
      "Epoch 3625, Loss: 0.02339958678930998, Final Batch Loss: 0.011881470680236816\n",
      "Epoch 3626, Loss: 0.05387468822300434, Final Batch Loss: 0.044789236038923264\n",
      "Epoch 3627, Loss: 0.017144208308309317, Final Batch Loss: 0.0062736873514950275\n",
      "Epoch 3628, Loss: 0.030359518714249134, Final Batch Loss: 0.0106770945712924\n",
      "Epoch 3629, Loss: 0.03662765119224787, Final Batch Loss: 0.00910694058984518\n",
      "Epoch 3630, Loss: 0.02241070242598653, Final Batch Loss: 0.0034298826940357685\n",
      "Epoch 3631, Loss: 0.023286327254027128, Final Batch Loss: 0.007160843815654516\n",
      "Epoch 3632, Loss: 0.017656360752880573, Final Batch Loss: 0.002483673393726349\n",
      "Epoch 3633, Loss: 0.04961227951571345, Final Batch Loss: 0.006435986142605543\n",
      "Epoch 3634, Loss: 0.10581825952976942, Final Batch Loss: 0.01050024013966322\n",
      "Epoch 3635, Loss: 0.023700491525232792, Final Batch Loss: 0.013396968133747578\n",
      "Epoch 3636, Loss: 0.021346872206777334, Final Batch Loss: 0.004666443448513746\n",
      "Epoch 3637, Loss: 0.03070190642029047, Final Batch Loss: 0.011800889857113361\n",
      "Epoch 3638, Loss: 0.020627764984965324, Final Batch Loss: 0.0048042405396699905\n",
      "Epoch 3639, Loss: 0.0402571726590395, Final Batch Loss: 0.00985940545797348\n",
      "Epoch 3640, Loss: 0.03498523123562336, Final Batch Loss: 0.02252190373837948\n",
      "Epoch 3641, Loss: 0.040295968763530254, Final Batch Loss: 0.011878798715770245\n",
      "Epoch 3642, Loss: 0.031467100605368614, Final Batch Loss: 0.01751038432121277\n",
      "Epoch 3643, Loss: 0.024297740310430527, Final Batch Loss: 0.012737533077597618\n",
      "Epoch 3644, Loss: 0.058409214951097965, Final Batch Loss: 0.00827812496572733\n",
      "Epoch 3645, Loss: 0.024154890328645706, Final Batch Loss: 0.012326056137681007\n",
      "Epoch 3646, Loss: 0.012368487659841776, Final Batch Loss: 0.005496034864336252\n",
      "Epoch 3647, Loss: 0.014512802939862013, Final Batch Loss: 0.009934108704328537\n",
      "Epoch 3648, Loss: 0.03209793474525213, Final Batch Loss: 0.022649968042969704\n",
      "Epoch 3649, Loss: 0.013624622486531734, Final Batch Loss: 0.005126110278069973\n",
      "Epoch 3650, Loss: 0.010898483917117119, Final Batch Loss: 0.004473363049328327\n",
      "Epoch 3651, Loss: 0.05520261265337467, Final Batch Loss: 0.03572869673371315\n",
      "Epoch 3652, Loss: 0.03836398385465145, Final Batch Loss: 0.00736171193420887\n",
      "Epoch 3653, Loss: 0.03361105639487505, Final Batch Loss: 0.014057138003408909\n",
      "Epoch 3654, Loss: 0.008896149229258299, Final Batch Loss: 0.005171637982130051\n",
      "Epoch 3655, Loss: 0.03476772177964449, Final Batch Loss: 0.008578103967010975\n",
      "Epoch 3656, Loss: 0.011764043243601918, Final Batch Loss: 0.0020083507988601923\n",
      "Epoch 3657, Loss: 0.023298349231481552, Final Batch Loss: 0.009168131276965141\n",
      "Epoch 3658, Loss: 0.059143515303730965, Final Batch Loss: 0.03937385603785515\n",
      "Epoch 3659, Loss: 0.01627777051180601, Final Batch Loss: 0.010744135826826096\n",
      "Epoch 3660, Loss: 0.022953993640840054, Final Batch Loss: 0.014733804389834404\n",
      "Epoch 3661, Loss: 0.016162042506039143, Final Batch Loss: 0.008829156868159771\n",
      "Epoch 3662, Loss: 0.03181345295161009, Final Batch Loss: 0.010433382354676723\n",
      "Epoch 3663, Loss: 0.07232216047123075, Final Batch Loss: 0.06680572032928467\n",
      "Epoch 3664, Loss: 0.035116083920001984, Final Batch Loss: 0.020513402298092842\n",
      "Epoch 3665, Loss: 0.032809055875986814, Final Batch Loss: 0.02682471089065075\n",
      "Epoch 3666, Loss: 0.018100938759744167, Final Batch Loss: 0.0072484007105231285\n",
      "Epoch 3667, Loss: 0.031099767424166203, Final Batch Loss: 0.01900537870824337\n",
      "Epoch 3668, Loss: 0.04376022145152092, Final Batch Loss: 0.021960202604532242\n",
      "Epoch 3669, Loss: 0.01197137450799346, Final Batch Loss: 0.004411754198372364\n",
      "Epoch 3670, Loss: 0.02140949433669448, Final Batch Loss: 0.016527647152543068\n",
      "Epoch 3671, Loss: 0.018113932572305202, Final Batch Loss: 0.010861525312066078\n",
      "Epoch 3672, Loss: 0.02448568493127823, Final Batch Loss: 0.016812775284051895\n",
      "Epoch 3673, Loss: 0.03557816240936518, Final Batch Loss: 0.024265842512249947\n",
      "Epoch 3674, Loss: 0.03747869562357664, Final Batch Loss: 0.024015847593545914\n",
      "Epoch 3675, Loss: 0.042793458327651024, Final Batch Loss: 0.02323284186422825\n",
      "Epoch 3676, Loss: 0.017354595474898815, Final Batch Loss: 0.006467896513640881\n",
      "Epoch 3677, Loss: 0.03433255851268768, Final Batch Loss: 0.00792672112584114\n",
      "Epoch 3678, Loss: 0.029941542074084282, Final Batch Loss: 0.015963012352585793\n",
      "Epoch 3679, Loss: 0.03181005688384175, Final Batch Loss: 0.006409176159650087\n",
      "Epoch 3680, Loss: 0.033987863920629025, Final Batch Loss: 0.01889517717063427\n",
      "Epoch 3681, Loss: 0.028169250581413507, Final Batch Loss: 0.007251630071550608\n",
      "Epoch 3682, Loss: 0.032782516442239285, Final Batch Loss: 0.007481330074369907\n",
      "Epoch 3683, Loss: 0.011739777401089668, Final Batch Loss: 0.0077325026504695415\n",
      "Epoch 3684, Loss: 0.12497814185917377, Final Batch Loss: 0.10450686514377594\n",
      "Epoch 3685, Loss: 0.017287806142121553, Final Batch Loss: 0.010410569608211517\n",
      "Epoch 3686, Loss: 0.02655150182545185, Final Batch Loss: 0.01940358243882656\n",
      "Epoch 3687, Loss: 0.14785383082926273, Final Batch Loss: 0.118381567299366\n",
      "Epoch 3688, Loss: 0.050674895755946636, Final Batch Loss: 0.015300258062779903\n",
      "Epoch 3689, Loss: 0.019607829861342907, Final Batch Loss: 0.007889977656304836\n",
      "Epoch 3690, Loss: 0.03759351372718811, Final Batch Loss: 0.029346540570259094\n",
      "Epoch 3691, Loss: 0.014941707253456116, Final Batch Loss: 0.0029964223504066467\n",
      "Epoch 3692, Loss: 0.049546681344509125, Final Batch Loss: 0.031237469986081123\n",
      "Epoch 3693, Loss: 0.022012650035321712, Final Batch Loss: 0.007587628439068794\n",
      "Epoch 3694, Loss: 0.023409036919474602, Final Batch Loss: 0.012374547310173512\n",
      "Epoch 3695, Loss: 0.026472297497093678, Final Batch Loss: 0.014593491330742836\n",
      "Epoch 3696, Loss: 0.025559297297149897, Final Batch Loss: 0.006225014571100473\n",
      "Epoch 3697, Loss: 0.03444041684269905, Final Batch Loss: 0.020365817472338676\n",
      "Epoch 3698, Loss: 0.0486307293176651, Final Batch Loss: 0.034700680524110794\n",
      "Epoch 3699, Loss: 0.01307354774326086, Final Batch Loss: 0.005552588030695915\n",
      "Epoch 3700, Loss: 0.01621157955378294, Final Batch Loss: 0.008254636079072952\n",
      "Epoch 3701, Loss: 0.01197397755458951, Final Batch Loss: 0.007322786841541529\n",
      "Epoch 3702, Loss: 0.02565799653530121, Final Batch Loss: 0.0205231960862875\n",
      "Epoch 3703, Loss: 0.014772476628422737, Final Batch Loss: 0.0033960752189159393\n",
      "Epoch 3704, Loss: 0.04798092087730765, Final Batch Loss: 0.04139431565999985\n",
      "Epoch 3705, Loss: 0.014035571832209826, Final Batch Loss: 0.004748132545500994\n",
      "Epoch 3706, Loss: 0.028070498257875443, Final Batch Loss: 0.014635770581662655\n",
      "Epoch 3707, Loss: 0.03573542460799217, Final Batch Loss: 0.011085448786616325\n",
      "Epoch 3708, Loss: 0.04950250778347254, Final Batch Loss: 0.03442557156085968\n",
      "Epoch 3709, Loss: 0.013884984888136387, Final Batch Loss: 0.005057210102677345\n",
      "Epoch 3710, Loss: 0.01744560757651925, Final Batch Loss: 0.007709807250648737\n",
      "Epoch 3711, Loss: 0.014882450923323631, Final Batch Loss: 0.004337518475949764\n",
      "Epoch 3712, Loss: 0.041841981932520866, Final Batch Loss: 0.01778254099190235\n",
      "Epoch 3713, Loss: 0.021421296522021294, Final Batch Loss: 0.008490903303027153\n",
      "Epoch 3714, Loss: 0.044671813026070595, Final Batch Loss: 0.027210477739572525\n",
      "Epoch 3715, Loss: 0.012694954173639417, Final Batch Loss: 0.0028592583257704973\n",
      "Epoch 3716, Loss: 0.04953795671463013, Final Batch Loss: 0.04414184018969536\n",
      "Epoch 3717, Loss: 0.031606211327016354, Final Batch Loss: 0.022528542205691338\n",
      "Epoch 3718, Loss: 0.01802541082724929, Final Batch Loss: 0.006388568785041571\n",
      "Epoch 3719, Loss: 0.025646664202213287, Final Batch Loss: 0.013265233486890793\n",
      "Epoch 3720, Loss: 0.020555874332785606, Final Batch Loss: 0.013883855193853378\n",
      "Epoch 3721, Loss: 0.02526221238076687, Final Batch Loss: 0.01413851696997881\n",
      "Epoch 3722, Loss: 0.03418877348303795, Final Batch Loss: 0.015566734597086906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3723, Loss: 0.04718868248164654, Final Batch Loss: 0.027131419628858566\n",
      "Epoch 3724, Loss: 0.03643754031509161, Final Batch Loss: 0.012942702509462833\n",
      "Epoch 3725, Loss: 0.020065629854798317, Final Batch Loss: 0.009203463792800903\n",
      "Epoch 3726, Loss: 0.05785021558403969, Final Batch Loss: 0.048111144453287125\n",
      "Epoch 3727, Loss: 0.03451184928417206, Final Batch Loss: 0.016388030722737312\n",
      "Epoch 3728, Loss: 0.03941503446549177, Final Batch Loss: 0.009041831828653812\n",
      "Epoch 3729, Loss: 0.045397148467600346, Final Batch Loss: 0.008706849999725819\n",
      "Epoch 3730, Loss: 0.04941771924495697, Final Batch Loss: 0.026101134717464447\n",
      "Epoch 3731, Loss: 0.03335412312299013, Final Batch Loss: 0.014038718305528164\n",
      "Epoch 3732, Loss: 0.04085336532443762, Final Batch Loss: 0.015596321783959866\n",
      "Epoch 3733, Loss: 0.03745404630899429, Final Batch Loss: 0.016880430281162262\n",
      "Epoch 3734, Loss: 0.024571348913013935, Final Batch Loss: 0.01473036129027605\n",
      "Epoch 3735, Loss: 0.024381023831665516, Final Batch Loss: 0.015202176757156849\n",
      "Epoch 3736, Loss: 0.026964965276420116, Final Batch Loss: 0.015121427364647388\n",
      "Epoch 3737, Loss: 0.047992367297410965, Final Batch Loss: 0.025221025571227074\n",
      "Epoch 3738, Loss: 0.09326341841369867, Final Batch Loss: 0.081927590072155\n",
      "Epoch 3739, Loss: 0.1012133602052927, Final Batch Loss: 0.07706674933433533\n",
      "Epoch 3740, Loss: 0.03869046829640865, Final Batch Loss: 0.02032311260700226\n",
      "Epoch 3741, Loss: 0.04280945844948292, Final Batch Loss: 0.0256794560700655\n",
      "Epoch 3742, Loss: 0.016937237232923508, Final Batch Loss: 0.009321608580648899\n",
      "Epoch 3743, Loss: 0.015212864615023136, Final Batch Loss: 0.009577542543411255\n",
      "Epoch 3744, Loss: 0.021009867545217276, Final Batch Loss: 0.014589700847864151\n",
      "Epoch 3745, Loss: 0.017270580399781466, Final Batch Loss: 0.00718942703679204\n",
      "Epoch 3746, Loss: 0.01581634720787406, Final Batch Loss: 0.00673110643401742\n",
      "Epoch 3747, Loss: 0.01635239878669381, Final Batch Loss: 0.005803829524666071\n",
      "Epoch 3748, Loss: 0.04884606134146452, Final Batch Loss: 0.04100465402007103\n",
      "Epoch 3749, Loss: 0.014888022560626268, Final Batch Loss: 0.006046871189028025\n",
      "Epoch 3750, Loss: 0.01593464706093073, Final Batch Loss: 0.005823647603392601\n",
      "Epoch 3751, Loss: 0.03015443403273821, Final Batch Loss: 0.010053534992039204\n",
      "Epoch 3752, Loss: 0.015589750371873379, Final Batch Loss: 0.006218522787094116\n",
      "Epoch 3753, Loss: 0.017968772910535336, Final Batch Loss: 0.008317754603922367\n",
      "Epoch 3754, Loss: 0.07341378182172775, Final Batch Loss: 0.027081843465566635\n",
      "Epoch 3755, Loss: 0.019538146210834384, Final Batch Loss: 0.003247693879529834\n",
      "Epoch 3756, Loss: 0.04953215643763542, Final Batch Loss: 0.031105220317840576\n",
      "Epoch 3757, Loss: 0.04061701521277428, Final Batch Loss: 0.023606179282069206\n",
      "Epoch 3758, Loss: 0.01123349228873849, Final Batch Loss: 0.004435001872479916\n",
      "Epoch 3759, Loss: 0.02565935254096985, Final Batch Loss: 0.009254645556211472\n",
      "Epoch 3760, Loss: 0.009471261408179998, Final Batch Loss: 0.004163771402090788\n",
      "Epoch 3761, Loss: 0.018534646835178137, Final Batch Loss: 0.010807334445416927\n",
      "Epoch 3762, Loss: 0.027543934294953942, Final Batch Loss: 0.003137562656775117\n",
      "Epoch 3763, Loss: 0.03570706397294998, Final Batch Loss: 0.017767833545804024\n",
      "Epoch 3764, Loss: 0.020313377492129803, Final Batch Loss: 0.010650121606886387\n",
      "Epoch 3765, Loss: 0.019502114038914442, Final Batch Loss: 0.004171182867139578\n",
      "Epoch 3766, Loss: 0.040898868814110756, Final Batch Loss: 0.019717272371053696\n",
      "Epoch 3767, Loss: 0.03168930020183325, Final Batch Loss: 0.025562768802046776\n",
      "Epoch 3768, Loss: 0.06237453827634454, Final Batch Loss: 0.0070580788888037205\n",
      "Epoch 3769, Loss: 0.02572742011398077, Final Batch Loss: 0.012915072031319141\n",
      "Epoch 3770, Loss: 0.011796930804848671, Final Batch Loss: 0.005696076434105635\n",
      "Epoch 3771, Loss: 0.03206349350512028, Final Batch Loss: 0.020322095602750778\n",
      "Epoch 3772, Loss: 0.05842315964400768, Final Batch Loss: 0.026654662564396858\n",
      "Epoch 3773, Loss: 0.042285805102437735, Final Batch Loss: 0.00778920715674758\n",
      "Epoch 3774, Loss: 0.03155170660465956, Final Batch Loss: 0.011230661533772945\n",
      "Epoch 3775, Loss: 0.05132785625755787, Final Batch Loss: 0.044830527156591415\n",
      "Epoch 3776, Loss: 0.10137728974223137, Final Batch Loss: 0.08090981096029282\n",
      "Epoch 3777, Loss: 0.04089711233973503, Final Batch Loss: 0.006324902176856995\n",
      "Epoch 3778, Loss: 0.02191713359206915, Final Batch Loss: 0.009975673630833626\n",
      "Epoch 3779, Loss: 0.04087202437222004, Final Batch Loss: 0.007935715839266777\n",
      "Epoch 3780, Loss: 0.015414941124618053, Final Batch Loss: 0.007546626031398773\n",
      "Epoch 3781, Loss: 0.01514621777459979, Final Batch Loss: 0.0043449983932077885\n",
      "Epoch 3782, Loss: 0.015616041142493486, Final Batch Loss: 0.007428115699440241\n",
      "Epoch 3783, Loss: 0.02248434256762266, Final Batch Loss: 0.006361260078847408\n",
      "Epoch 3784, Loss: 0.01986131351441145, Final Batch Loss: 0.009328867308795452\n",
      "Epoch 3785, Loss: 0.021593793760985136, Final Batch Loss: 0.014026174321770668\n",
      "Epoch 3786, Loss: 0.015542164910584688, Final Batch Loss: 0.0049282521940767765\n",
      "Epoch 3787, Loss: 0.022276692558079958, Final Batch Loss: 0.01466658990830183\n",
      "Epoch 3788, Loss: 0.01964401174336672, Final Batch Loss: 0.0048272740095853806\n",
      "Epoch 3789, Loss: 0.047290741465985775, Final Batch Loss: 0.004487096332013607\n",
      "Epoch 3790, Loss: 0.025389311369508505, Final Batch Loss: 0.0062566655687987804\n",
      "Epoch 3791, Loss: 0.016699414001777768, Final Batch Loss: 0.012941095978021622\n",
      "Epoch 3792, Loss: 0.020398680586367846, Final Batch Loss: 0.00433973828330636\n",
      "Epoch 3793, Loss: 0.034799966029822826, Final Batch Loss: 0.00879466999322176\n",
      "Epoch 3794, Loss: 0.013162069488316774, Final Batch Loss: 0.006270541809499264\n",
      "Epoch 3795, Loss: 0.009285165462642908, Final Batch Loss: 0.004922664724290371\n",
      "Epoch 3796, Loss: 0.04594232141971588, Final Batch Loss: 0.02932720072567463\n",
      "Epoch 3797, Loss: 0.07383612357079983, Final Batch Loss: 0.008390253409743309\n",
      "Epoch 3798, Loss: 0.01535622077062726, Final Batch Loss: 0.005894377361983061\n",
      "Epoch 3799, Loss: 0.025765318889170885, Final Batch Loss: 0.007709648925811052\n",
      "Epoch 3800, Loss: 0.04514847882091999, Final Batch Loss: 0.018878692761063576\n",
      "Epoch 3801, Loss: 0.03673594631254673, Final Batch Loss: 0.02841918356716633\n",
      "Epoch 3802, Loss: 0.019794268999248743, Final Batch Loss: 0.013123752549290657\n",
      "Epoch 3803, Loss: 0.014207981992512941, Final Batch Loss: 0.007125278934836388\n",
      "Epoch 3804, Loss: 0.026946408674120903, Final Batch Loss: 0.012722652405500412\n",
      "Epoch 3805, Loss: 0.039904169738292694, Final Batch Loss: 0.016403835266828537\n",
      "Epoch 3806, Loss: 0.015947431791573763, Final Batch Loss: 0.004560066852718592\n",
      "Epoch 3807, Loss: 0.021610509604215622, Final Batch Loss: 0.00897880271077156\n",
      "Epoch 3808, Loss: 0.01582418940961361, Final Batch Loss: 0.004990451969206333\n",
      "Epoch 3809, Loss: 0.054157324600964785, Final Batch Loss: 0.0481538251042366\n",
      "Epoch 3810, Loss: 0.008943127933889627, Final Batch Loss: 0.004925867076963186\n",
      "Epoch 3811, Loss: 0.013758649118244648, Final Batch Loss: 0.007765204645693302\n",
      "Epoch 3812, Loss: 0.014146675355732441, Final Batch Loss: 0.007271920796483755\n",
      "Epoch 3813, Loss: 0.02392452908679843, Final Batch Loss: 0.016914626583456993\n",
      "Epoch 3814, Loss: 0.06208813190460205, Final Batch Loss: 0.016509778797626495\n",
      "Epoch 3815, Loss: 0.012540721567347646, Final Batch Loss: 0.0033919198904186487\n",
      "Epoch 3816, Loss: 0.022128992713987827, Final Batch Loss: 0.005299360491335392\n",
      "Epoch 3817, Loss: 0.015478555578738451, Final Batch Loss: 0.005771871190518141\n",
      "Epoch 3818, Loss: 0.04320450499653816, Final Batch Loss: 0.019606411457061768\n",
      "Epoch 3819, Loss: 0.03514429274946451, Final Batch Loss: 0.026394737884402275\n",
      "Epoch 3820, Loss: 0.027113457676023245, Final Batch Loss: 0.020078297704458237\n",
      "Epoch 3821, Loss: 0.01808523526415229, Final Batch Loss: 0.004581867251545191\n",
      "Epoch 3822, Loss: 0.02459673024713993, Final Batch Loss: 0.013250215910375118\n",
      "Epoch 3823, Loss: 0.02736540697515011, Final Batch Loss: 0.0164756141602993\n",
      "Epoch 3824, Loss: 0.021096093580126762, Final Batch Loss: 0.010287167504429817\n",
      "Epoch 3825, Loss: 0.07955087348818779, Final Batch Loss: 0.007502254098653793\n",
      "Epoch 3826, Loss: 0.053683157078921795, Final Batch Loss: 0.01554128434509039\n",
      "Epoch 3827, Loss: 0.012700002640485764, Final Batch Loss: 0.004309164360165596\n",
      "Epoch 3828, Loss: 0.02704044058918953, Final Batch Loss: 0.01834518276154995\n",
      "Epoch 3829, Loss: 0.020344886928796768, Final Batch Loss: 0.00782489962875843\n",
      "Epoch 3830, Loss: 0.04478087276220322, Final Batch Loss: 0.02121107466518879\n",
      "Epoch 3831, Loss: 0.02458197809755802, Final Batch Loss: 0.010714436881244183\n",
      "Epoch 3832, Loss: 0.01360089797526598, Final Batch Loss: 0.006494647823274136\n",
      "Epoch 3833, Loss: 0.02248216699808836, Final Batch Loss: 0.008417632430791855\n",
      "Epoch 3834, Loss: 0.029840396717190742, Final Batch Loss: 0.011889629065990448\n",
      "Epoch 3835, Loss: 0.04783219238743186, Final Batch Loss: 0.006230404134839773\n",
      "Epoch 3836, Loss: 0.018062926828861237, Final Batch Loss: 0.008530670776963234\n",
      "Epoch 3837, Loss: 0.033341614063829184, Final Batch Loss: 0.028471803292632103\n",
      "Epoch 3838, Loss: 0.04596119141206145, Final Batch Loss: 0.005145301576703787\n",
      "Epoch 3839, Loss: 0.07917821034789085, Final Batch Loss: 0.011448543518781662\n",
      "Epoch 3840, Loss: 0.07769672945141792, Final Batch Loss: 0.0323905348777771\n",
      "Epoch 3841, Loss: 0.03180307429283857, Final Batch Loss: 0.016872836276888847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3842, Loss: 0.05939203826710582, Final Batch Loss: 0.005412317346781492\n",
      "Epoch 3843, Loss: 0.030829208437353373, Final Batch Loss: 0.00745795713737607\n",
      "Epoch 3844, Loss: 0.01532843941822648, Final Batch Loss: 0.0042956857942044735\n",
      "Epoch 3845, Loss: 0.02544918842613697, Final Batch Loss: 0.021282924339175224\n",
      "Epoch 3846, Loss: 0.03159604128450155, Final Batch Loss: 0.022103488445281982\n",
      "Epoch 3847, Loss: 0.018256417475640774, Final Batch Loss: 0.007893823087215424\n",
      "Epoch 3848, Loss: 0.05767141282558441, Final Batch Loss: 0.026814943179488182\n",
      "Epoch 3849, Loss: 0.014166063163429499, Final Batch Loss: 0.005404788535088301\n",
      "Epoch 3850, Loss: 0.03267154470086098, Final Batch Loss: 0.012408459559082985\n",
      "Epoch 3851, Loss: 0.02298843953758478, Final Batch Loss: 0.01261565089225769\n",
      "Epoch 3852, Loss: 0.01814712630584836, Final Batch Loss: 0.004631312098354101\n",
      "Epoch 3853, Loss: 0.06530871242284775, Final Batch Loss: 0.04974948614835739\n",
      "Epoch 3854, Loss: 0.01700341678224504, Final Batch Loss: 0.0027877360116690397\n",
      "Epoch 3855, Loss: 0.017490556463599205, Final Batch Loss: 0.008501619100570679\n",
      "Epoch 3856, Loss: 0.012831657659262419, Final Batch Loss: 0.006517280824482441\n",
      "Epoch 3857, Loss: 0.02207133872434497, Final Batch Loss: 0.006815750617533922\n",
      "Epoch 3858, Loss: 0.019926486536860466, Final Batch Loss: 0.007403386756777763\n",
      "Epoch 3859, Loss: 0.03065294399857521, Final Batch Loss: 0.02653801627457142\n",
      "Epoch 3860, Loss: 0.03311010031029582, Final Batch Loss: 0.005284266080707312\n",
      "Epoch 3861, Loss: 0.023668311536312103, Final Batch Loss: 0.014398334547877312\n",
      "Epoch 3862, Loss: 0.030472563579678535, Final Batch Loss: 0.015773670747876167\n",
      "Epoch 3863, Loss: 0.016557903960347176, Final Batch Loss: 0.008186780847609043\n",
      "Epoch 3864, Loss: 0.029291857033967972, Final Batch Loss: 0.017849085852503777\n",
      "Epoch 3865, Loss: 0.014758783858269453, Final Batch Loss: 0.010012847371399403\n",
      "Epoch 3866, Loss: 0.020380007568746805, Final Batch Loss: 0.012930778786540031\n",
      "Epoch 3867, Loss: 0.020301522687077522, Final Batch Loss: 0.010708890855312347\n",
      "Epoch 3868, Loss: 0.04338876064866781, Final Batch Loss: 0.03249487653374672\n",
      "Epoch 3869, Loss: 0.05895647779107094, Final Batch Loss: 0.05066704377532005\n",
      "Epoch 3870, Loss: 0.020586006343364716, Final Batch Loss: 0.008599628694355488\n",
      "Epoch 3871, Loss: 0.01443933555856347, Final Batch Loss: 0.004799691494554281\n",
      "Epoch 3872, Loss: 0.014566471800208092, Final Batch Loss: 0.00684615783393383\n",
      "Epoch 3873, Loss: 0.045577334240078926, Final Batch Loss: 0.020711591467261314\n",
      "Epoch 3874, Loss: 0.018277636030688882, Final Batch Loss: 0.01542861107736826\n",
      "Epoch 3875, Loss: 0.02406102605164051, Final Batch Loss: 0.011379496194422245\n",
      "Epoch 3876, Loss: 0.014949830248951912, Final Batch Loss: 0.006765122525393963\n",
      "Epoch 3877, Loss: 0.044786566868424416, Final Batch Loss: 0.0174106415361166\n",
      "Epoch 3878, Loss: 0.04644820000976324, Final Batch Loss: 0.03227725252509117\n",
      "Epoch 3879, Loss: 0.05223067104816437, Final Batch Loss: 0.036816950887441635\n",
      "Epoch 3880, Loss: 0.04109164886176586, Final Batch Loss: 0.023856787011027336\n",
      "Epoch 3881, Loss: 0.02798189502209425, Final Batch Loss: 0.004251602105796337\n",
      "Epoch 3882, Loss: 0.03720290260389447, Final Batch Loss: 0.03038782626390457\n",
      "Epoch 3883, Loss: 0.010310508776456118, Final Batch Loss: 0.004780145362019539\n",
      "Epoch 3884, Loss: 0.019839772954583168, Final Batch Loss: 0.011279793456196785\n",
      "Epoch 3885, Loss: 0.03316016821190715, Final Batch Loss: 0.026328865438699722\n",
      "Epoch 3886, Loss: 0.01872445782646537, Final Batch Loss: 0.014114522375166416\n",
      "Epoch 3887, Loss: 0.009460929315537214, Final Batch Loss: 0.002504317555576563\n",
      "Epoch 3888, Loss: 0.019251755438745022, Final Batch Loss: 0.003991222009062767\n",
      "Epoch 3889, Loss: 0.04661240801215172, Final Batch Loss: 0.037819959223270416\n",
      "Epoch 3890, Loss: 0.026726144831627607, Final Batch Loss: 0.022340238094329834\n",
      "Epoch 3891, Loss: 0.02936219982802868, Final Batch Loss: 0.008384136483073235\n",
      "Epoch 3892, Loss: 0.016028677579015493, Final Batch Loss: 0.010556964203715324\n",
      "Epoch 3893, Loss: 0.01703317929059267, Final Batch Loss: 0.008944272063672543\n",
      "Epoch 3894, Loss: 0.023125645238906145, Final Batch Loss: 0.006018829066306353\n",
      "Epoch 3895, Loss: 0.030909566208720207, Final Batch Loss: 0.017055517062544823\n",
      "Epoch 3896, Loss: 0.07908825390040874, Final Batch Loss: 0.027437468990683556\n",
      "Epoch 3897, Loss: 0.007941823219880462, Final Batch Loss: 0.0029401632491499186\n",
      "Epoch 3898, Loss: 0.026597706601023674, Final Batch Loss: 0.00879707746207714\n",
      "Epoch 3899, Loss: 0.04989308770745993, Final Batch Loss: 0.04594125598669052\n",
      "Epoch 3900, Loss: 0.051645303145051, Final Batch Loss: 0.03350703418254852\n",
      "Epoch 3901, Loss: 0.018419217318296432, Final Batch Loss: 0.012897779233753681\n",
      "Epoch 3902, Loss: 0.0336587755009532, Final Batch Loss: 0.020601967349648476\n",
      "Epoch 3903, Loss: 0.02439605537801981, Final Batch Loss: 0.003957266919314861\n",
      "Epoch 3904, Loss: 0.019860084634274244, Final Batch Loss: 0.004202756565064192\n",
      "Epoch 3905, Loss: 0.02175424061715603, Final Batch Loss: 0.008353357203304768\n",
      "Epoch 3906, Loss: 0.011181522626429796, Final Batch Loss: 0.004174284171313047\n",
      "Epoch 3907, Loss: 0.013593535171821713, Final Batch Loss: 0.0026291257236152887\n",
      "Epoch 3908, Loss: 0.017745280638337135, Final Batch Loss: 0.011546891182661057\n",
      "Epoch 3909, Loss: 0.028835967183113098, Final Batch Loss: 0.01357939001172781\n",
      "Epoch 3910, Loss: 0.021589473355561495, Final Batch Loss: 0.007745759095996618\n",
      "Epoch 3911, Loss: 0.06901928409934044, Final Batch Loss: 0.0597701221704483\n",
      "Epoch 3912, Loss: 0.021922091953456402, Final Batch Loss: 0.017089467495679855\n",
      "Epoch 3913, Loss: 0.012569054961204529, Final Batch Loss: 0.004606993868947029\n",
      "Epoch 3914, Loss: 0.008980259532108903, Final Batch Loss: 0.0036776999477297068\n",
      "Epoch 3915, Loss: 0.009231573436409235, Final Batch Loss: 0.0042406897991895676\n",
      "Epoch 3916, Loss: 0.029037334956228733, Final Batch Loss: 0.0049887849017977715\n",
      "Epoch 3917, Loss: 0.020055143162608147, Final Batch Loss: 0.005388112738728523\n",
      "Epoch 3918, Loss: 0.022203457541763783, Final Batch Loss: 0.009701462462544441\n",
      "Epoch 3919, Loss: 0.03393755666911602, Final Batch Loss: 0.022120267152786255\n",
      "Epoch 3920, Loss: 0.05032112542539835, Final Batch Loss: 0.03504012152552605\n",
      "Epoch 3921, Loss: 0.05861327052116394, Final Batch Loss: 0.008137978613376617\n",
      "Epoch 3922, Loss: 0.015835984144359827, Final Batch Loss: 0.009040521457791328\n",
      "Epoch 3923, Loss: 0.0381851177662611, Final Batch Loss: 0.0056050848215818405\n",
      "Epoch 3924, Loss: 0.007866585161536932, Final Batch Loss: 0.0035780942998826504\n",
      "Epoch 3925, Loss: 0.014501177705824375, Final Batch Loss: 0.00664665549993515\n",
      "Epoch 3926, Loss: 0.011522490996867418, Final Batch Loss: 0.004772661719471216\n",
      "Epoch 3927, Loss: 0.01701316423714161, Final Batch Loss: 0.011524008587002754\n",
      "Epoch 3928, Loss: 0.029159480705857277, Final Batch Loss: 0.011352837085723877\n",
      "Epoch 3929, Loss: 0.019549875054508448, Final Batch Loss: 0.014542413875460625\n",
      "Epoch 3930, Loss: 0.016052796505391598, Final Batch Loss: 0.008837389759719372\n",
      "Epoch 3931, Loss: 0.02009664522483945, Final Batch Loss: 0.012771894223988056\n",
      "Epoch 3932, Loss: 0.016428713221102953, Final Batch Loss: 0.006688758265227079\n",
      "Epoch 3933, Loss: 0.056056056171655655, Final Batch Loss: 0.0384255014359951\n",
      "Epoch 3934, Loss: 0.026133437640964985, Final Batch Loss: 0.015803731977939606\n",
      "Epoch 3935, Loss: 0.016643684823065996, Final Batch Loss: 0.007513734046369791\n",
      "Epoch 3936, Loss: 0.042743293568491936, Final Batch Loss: 0.026517406105995178\n",
      "Epoch 3937, Loss: 0.011431843508034945, Final Batch Loss: 0.005933315958827734\n",
      "Epoch 3938, Loss: 0.022745562717318535, Final Batch Loss: 0.008274433203041553\n",
      "Epoch 3939, Loss: 0.016047639306634665, Final Batch Loss: 0.009187142364680767\n",
      "Epoch 3940, Loss: 0.06472201365977526, Final Batch Loss: 0.05792465806007385\n",
      "Epoch 3941, Loss: 0.06424864009022713, Final Batch Loss: 0.05709531903266907\n",
      "Epoch 3942, Loss: 0.028745179995894432, Final Batch Loss: 0.008542567491531372\n",
      "Epoch 3943, Loss: 0.024777153506875038, Final Batch Loss: 0.0065217819064855576\n",
      "Epoch 3944, Loss: 0.007398865185678005, Final Batch Loss: 0.0041062720119953156\n",
      "Epoch 3945, Loss: 0.015176466200500727, Final Batch Loss: 0.0035909260623157024\n",
      "Epoch 3946, Loss: 0.027892972342669964, Final Batch Loss: 0.00847563985735178\n",
      "Epoch 3947, Loss: 0.021187241189181805, Final Batch Loss: 0.014203876256942749\n",
      "Epoch 3948, Loss: 0.045516328886151314, Final Batch Loss: 0.020573457702994347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3949, Loss: 0.06165977753698826, Final Batch Loss: 0.04272232949733734\n",
      "Epoch 3950, Loss: 0.02959584491327405, Final Batch Loss: 0.006403420586138964\n",
      "Epoch 3951, Loss: 0.03293080348521471, Final Batch Loss: 0.015468829311430454\n",
      "Epoch 3952, Loss: 0.07929312996566296, Final Batch Loss: 0.05742998793721199\n",
      "Epoch 3953, Loss: 0.02856220304965973, Final Batch Loss: 0.013782820664346218\n",
      "Epoch 3954, Loss: 0.019416979514062405, Final Batch Loss: 0.012318353168666363\n",
      "Epoch 3955, Loss: 0.04433382209390402, Final Batch Loss: 0.01515000406652689\n",
      "Epoch 3956, Loss: 0.020841319113969803, Final Batch Loss: 0.005245533771812916\n",
      "Epoch 3957, Loss: 0.0343288304284215, Final Batch Loss: 0.02466766908764839\n",
      "Epoch 3958, Loss: 0.03723082225769758, Final Batch Loss: 0.02189626730978489\n",
      "Epoch 3959, Loss: 0.027113775722682476, Final Batch Loss: 0.01582365483045578\n",
      "Epoch 3960, Loss: 0.006440979428589344, Final Batch Loss: 0.004523022565990686\n",
      "Epoch 3961, Loss: 0.017619012854993343, Final Batch Loss: 0.008525513112545013\n",
      "Epoch 3962, Loss: 0.011920976685360074, Final Batch Loss: 0.0025910523254424334\n",
      "Epoch 3963, Loss: 0.07292285095900297, Final Batch Loss: 0.06054701283574104\n",
      "Epoch 3964, Loss: 0.011716559063643217, Final Batch Loss: 0.005181103479117155\n",
      "Epoch 3965, Loss: 0.029835605528205633, Final Batch Loss: 0.025468982756137848\n",
      "Epoch 3966, Loss: 0.05303092859685421, Final Batch Loss: 0.04269539564847946\n",
      "Epoch 3967, Loss: 0.01460816734470427, Final Batch Loss: 0.002988554770126939\n",
      "Epoch 3968, Loss: 0.017497790046036243, Final Batch Loss: 0.01028915960341692\n",
      "Epoch 3969, Loss: 0.015773752238601446, Final Batch Loss: 0.006808887701481581\n",
      "Epoch 3970, Loss: 0.06469960929825902, Final Batch Loss: 0.06258158385753632\n",
      "Epoch 3971, Loss: 0.036128709092736244, Final Batch Loss: 0.027675580233335495\n",
      "Epoch 3972, Loss: 0.015060899779200554, Final Batch Loss: 0.0022981464862823486\n",
      "Epoch 3973, Loss: 0.018800142221152782, Final Batch Loss: 0.010730434209108353\n",
      "Epoch 3974, Loss: 0.015573360491544008, Final Batch Loss: 0.007780404295772314\n",
      "Epoch 3975, Loss: 0.01634005969390273, Final Batch Loss: 0.008846381679177284\n",
      "Epoch 3976, Loss: 0.018448193091899157, Final Batch Loss: 0.004899530205875635\n",
      "Epoch 3977, Loss: 0.05028136167675257, Final Batch Loss: 0.015612241812050343\n",
      "Epoch 3978, Loss: 0.009651927975937724, Final Batch Loss: 0.003123275237157941\n",
      "Epoch 3979, Loss: 0.03723660530522466, Final Batch Loss: 0.03171418234705925\n",
      "Epoch 3980, Loss: 0.03181253559887409, Final Batch Loss: 0.007902521640062332\n",
      "Epoch 3981, Loss: 0.022259405348449945, Final Batch Loss: 0.005237577948719263\n",
      "Epoch 3982, Loss: 0.01851419359445572, Final Batch Loss: 0.0068944236263632774\n",
      "Epoch 3983, Loss: 0.01570046227425337, Final Batch Loss: 0.005938565358519554\n",
      "Epoch 3984, Loss: 0.020202506333589554, Final Batch Loss: 0.010765224695205688\n",
      "Epoch 3985, Loss: 0.011122387833893299, Final Batch Loss: 0.005339154042303562\n",
      "Epoch 3986, Loss: 0.04307053983211517, Final Batch Loss: 0.024146249517798424\n",
      "Epoch 3987, Loss: 0.051700386218726635, Final Batch Loss: 0.0045237308368086815\n",
      "Epoch 3988, Loss: 0.018261823803186417, Final Batch Loss: 0.00937225017696619\n",
      "Epoch 3989, Loss: 0.014549130108207464, Final Batch Loss: 0.0067519452422857285\n",
      "Epoch 3990, Loss: 0.021367616020143032, Final Batch Loss: 0.008509100414812565\n",
      "Epoch 3991, Loss: 0.03789098747074604, Final Batch Loss: 0.010839007794857025\n",
      "Epoch 3992, Loss: 0.016380724497139454, Final Batch Loss: 0.009177163243293762\n",
      "Epoch 3993, Loss: 0.024097220972180367, Final Batch Loss: 0.009276922792196274\n",
      "Epoch 3994, Loss: 0.029098910046741366, Final Batch Loss: 0.025449857115745544\n",
      "Epoch 3995, Loss: 0.01772652566432953, Final Batch Loss: 0.0053228819742798805\n",
      "Epoch 3996, Loss: 0.06381847243756056, Final Batch Loss: 0.012200682424008846\n",
      "Epoch 3997, Loss: 0.012867205776274204, Final Batch Loss: 0.004629694856703281\n",
      "Epoch 3998, Loss: 0.00844100909307599, Final Batch Loss: 0.0034410422667860985\n",
      "Epoch 3999, Loss: 0.021397251868620515, Final Batch Loss: 0.0035279972944408655\n",
      "Epoch 4000, Loss: 0.017562424764037132, Final Batch Loss: 0.004946376197040081\n",
      "Epoch 4001, Loss: 0.007558094221167266, Final Batch Loss: 0.0018703242531046271\n",
      "Epoch 4002, Loss: 0.04139568330720067, Final Batch Loss: 0.035417672246694565\n",
      "Epoch 4003, Loss: 0.019221436930820346, Final Batch Loss: 0.0026167079340666533\n",
      "Epoch 4004, Loss: 0.011908922344446182, Final Batch Loss: 0.003387187607586384\n",
      "Epoch 4005, Loss: 0.013678237795829773, Final Batch Loss: 0.004037927836179733\n",
      "Epoch 4006, Loss: 0.02151659596711397, Final Batch Loss: 0.012422065250575542\n",
      "Epoch 4007, Loss: 0.018739566672593355, Final Batch Loss: 0.007055184338241816\n",
      "Epoch 4008, Loss: 0.08747476898133755, Final Batch Loss: 0.06657987833023071\n",
      "Epoch 4009, Loss: 0.013935971539467573, Final Batch Loss: 0.0042980690486729145\n",
      "Epoch 4010, Loss: 0.012089970521628857, Final Batch Loss: 0.0021404288709163666\n",
      "Epoch 4011, Loss: 0.009915412403643131, Final Batch Loss: 0.005989643279463053\n",
      "Epoch 4012, Loss: 0.035852610133588314, Final Batch Loss: 0.012543589808046818\n",
      "Epoch 4013, Loss: 0.018465972039848566, Final Batch Loss: 0.012819354422390461\n",
      "Epoch 4014, Loss: 0.02024213783442974, Final Batch Loss: 0.0028679631650447845\n",
      "Epoch 4015, Loss: 0.026839521247893572, Final Batch Loss: 0.021771037951111794\n",
      "Epoch 4016, Loss: 0.036826071329414845, Final Batch Loss: 0.02416865900158882\n",
      "Epoch 4017, Loss: 0.026310062035918236, Final Batch Loss: 0.007329350337386131\n",
      "Epoch 4018, Loss: 0.026080076582729816, Final Batch Loss: 0.009974823333323002\n",
      "Epoch 4019, Loss: 0.010793054476380348, Final Batch Loss: 0.0063536521047353745\n",
      "Epoch 4020, Loss: 0.013521733693778515, Final Batch Loss: 0.007391717750579119\n",
      "Epoch 4021, Loss: 0.04195280373096466, Final Batch Loss: 0.015584517270326614\n",
      "Epoch 4022, Loss: 0.01104264659807086, Final Batch Loss: 0.004183259792625904\n",
      "Epoch 4023, Loss: 0.02607725467532873, Final Batch Loss: 0.017979513853788376\n",
      "Epoch 4024, Loss: 0.012299506459385157, Final Batch Loss: 0.0056300899013876915\n",
      "Epoch 4025, Loss: 0.01585110928863287, Final Batch Loss: 0.009642336517572403\n",
      "Epoch 4026, Loss: 0.011361183132976294, Final Batch Loss: 0.006020086817443371\n",
      "Epoch 4027, Loss: 0.022577449679374695, Final Batch Loss: 0.010747140273451805\n",
      "Epoch 4028, Loss: 0.03807959333062172, Final Batch Loss: 0.031743526458740234\n",
      "Epoch 4029, Loss: 0.057483406737446785, Final Batch Loss: 0.04054230451583862\n",
      "Epoch 4030, Loss: 0.031404140405356884, Final Batch Loss: 0.026126369833946228\n",
      "Epoch 4031, Loss: 0.023711127461865544, Final Batch Loss: 0.0015834227669984102\n",
      "Epoch 4032, Loss: 0.01428572228178382, Final Batch Loss: 0.005304325837641954\n",
      "Epoch 4033, Loss: 0.026922038290649652, Final Batch Loss: 0.02044721134006977\n",
      "Epoch 4034, Loss: 0.008919216226786375, Final Batch Loss: 0.004542820155620575\n",
      "Epoch 4035, Loss: 0.0125708335544914, Final Batch Loss: 0.0035061126109212637\n",
      "Epoch 4036, Loss: 0.1000194288790226, Final Batch Loss: 0.09293729066848755\n",
      "Epoch 4037, Loss: 0.01843852549791336, Final Batch Loss: 0.007861395366489887\n",
      "Epoch 4038, Loss: 0.027246315963566303, Final Batch Loss: 0.016987470909953117\n",
      "Epoch 4039, Loss: 0.01695612445473671, Final Batch Loss: 0.005765706300735474\n",
      "Epoch 4040, Loss: 0.011280011618509889, Final Batch Loss: 0.008155697956681252\n",
      "Epoch 4041, Loss: 0.013782281894236803, Final Batch Loss: 0.0092900600284338\n",
      "Epoch 4042, Loss: 0.00631228880956769, Final Batch Loss: 0.0030312109738588333\n",
      "Epoch 4043, Loss: 0.011066098930314183, Final Batch Loss: 0.008429588750004768\n",
      "Epoch 4044, Loss: 0.08509200438857079, Final Batch Loss: 0.007907543331384659\n",
      "Epoch 4045, Loss: 0.036660490557551384, Final Batch Loss: 0.017801247537136078\n",
      "Epoch 4046, Loss: 0.025275430642068386, Final Batch Loss: 0.013360819779336452\n",
      "Epoch 4047, Loss: 0.029804672580212355, Final Batch Loss: 0.002387074287980795\n",
      "Epoch 4048, Loss: 0.02216982888057828, Final Batch Loss: 0.004426587838679552\n",
      "Epoch 4049, Loss: 0.018397769890725613, Final Batch Loss: 0.012582211755216122\n",
      "Epoch 4050, Loss: 0.006021019537001848, Final Batch Loss: 0.002264298964291811\n",
      "Epoch 4051, Loss: 0.028587442822754383, Final Batch Loss: 0.024674197658896446\n",
      "Epoch 4052, Loss: 0.027919732965528965, Final Batch Loss: 0.0021102605387568474\n",
      "Epoch 4053, Loss: 0.05691121518611908, Final Batch Loss: 0.042029645293951035\n",
      "Epoch 4054, Loss: 0.012429691385477781, Final Batch Loss: 0.007711338344961405\n",
      "Epoch 4055, Loss: 0.08075732365250587, Final Batch Loss: 0.062034040689468384\n",
      "Epoch 4056, Loss: 0.014741689898073673, Final Batch Loss: 0.006347880698740482\n",
      "Epoch 4057, Loss: 0.01125542214140296, Final Batch Loss: 0.00576914194971323\n",
      "Epoch 4058, Loss: 0.012883218005299568, Final Batch Loss: 0.006012257654219866\n",
      "Epoch 4059, Loss: 0.022362399147823453, Final Batch Loss: 0.0030092813540250063\n",
      "Epoch 4060, Loss: 0.021130915731191635, Final Batch Loss: 0.015079084783792496\n",
      "Epoch 4061, Loss: 0.022740155458450317, Final Batch Loss: 0.01604294218122959\n",
      "Epoch 4062, Loss: 0.03041192516684532, Final Batch Loss: 0.020232347771525383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4063, Loss: 0.026709322817623615, Final Batch Loss: 0.012546190060675144\n",
      "Epoch 4064, Loss: 0.025355597026646137, Final Batch Loss: 0.009115665219724178\n",
      "Epoch 4065, Loss: 0.038994044065475464, Final Batch Loss: 0.017608515918254852\n",
      "Epoch 4066, Loss: 0.02084582718089223, Final Batch Loss: 0.005187558475881815\n",
      "Epoch 4067, Loss: 0.012951591284945607, Final Batch Loss: 0.010336803272366524\n",
      "Epoch 4068, Loss: 0.10873570665717125, Final Batch Loss: 0.009134862571954727\n",
      "Epoch 4069, Loss: 0.024268094915896654, Final Batch Loss: 0.016757287085056305\n",
      "Epoch 4070, Loss: 0.024609140120446682, Final Batch Loss: 0.01481899619102478\n",
      "Epoch 4071, Loss: 0.027350006625056267, Final Batch Loss: 0.016152624040842056\n",
      "Epoch 4072, Loss: 0.018739297054708004, Final Batch Loss: 0.008968944661319256\n",
      "Epoch 4073, Loss: 0.024977107997983694, Final Batch Loss: 0.005152590107172728\n",
      "Epoch 4074, Loss: 0.011391239473596215, Final Batch Loss: 0.003700793953612447\n",
      "Epoch 4075, Loss: 0.06382765248417854, Final Batch Loss: 0.042923588305711746\n",
      "Epoch 4076, Loss: 0.027320628985762596, Final Batch Loss: 0.013497767969965935\n",
      "Epoch 4077, Loss: 0.02165648667141795, Final Batch Loss: 0.01504457090049982\n",
      "Epoch 4078, Loss: 0.015292372554540634, Final Batch Loss: 0.009611047804355621\n",
      "Epoch 4079, Loss: 0.037516454700380564, Final Batch Loss: 0.030511071905493736\n",
      "Epoch 4080, Loss: 0.028412622399628162, Final Batch Loss: 0.01999577321112156\n",
      "Epoch 4081, Loss: 0.025405487045645714, Final Batch Loss: 0.017810631543397903\n",
      "Epoch 4082, Loss: 0.08071564510464668, Final Batch Loss: 0.035832811146974564\n",
      "Epoch 4083, Loss: 0.04073021188378334, Final Batch Loss: 0.021397870033979416\n",
      "Epoch 4084, Loss: 0.04113391041755676, Final Batch Loss: 0.028782915323972702\n",
      "Epoch 4085, Loss: 0.02684759348630905, Final Batch Loss: 0.017132822424173355\n",
      "Epoch 4086, Loss: 0.017934661358594894, Final Batch Loss: 0.009184748865664005\n",
      "Epoch 4087, Loss: 0.0283010876737535, Final Batch Loss: 0.006079090293496847\n",
      "Epoch 4088, Loss: 0.037066031247377396, Final Batch Loss: 0.01218782551586628\n",
      "Epoch 4089, Loss: 0.04799513192847371, Final Batch Loss: 0.04220718517899513\n",
      "Epoch 4090, Loss: 0.05338600371032953, Final Batch Loss: 0.04103083536028862\n",
      "Epoch 4091, Loss: 0.03505912655964494, Final Batch Loss: 0.028866566717624664\n",
      "Epoch 4092, Loss: 0.06962414085865021, Final Batch Loss: 0.05296434089541435\n",
      "Epoch 4093, Loss: 0.025156472343951464, Final Batch Loss: 0.01959066279232502\n",
      "Epoch 4094, Loss: 0.014349801698699594, Final Batch Loss: 0.003483076812699437\n",
      "Epoch 4095, Loss: 0.017591977259144187, Final Batch Loss: 0.0031040438916534185\n",
      "Epoch 4096, Loss: 0.07939522387459874, Final Batch Loss: 0.07331147789955139\n",
      "Epoch 4097, Loss: 0.028515322133898735, Final Batch Loss: 0.01362703274935484\n",
      "Epoch 4098, Loss: 0.008932604687288404, Final Batch Loss: 0.005498339422047138\n",
      "Epoch 4099, Loss: 0.012618557550013065, Final Batch Loss: 0.006571783218532801\n",
      "Epoch 4100, Loss: 0.029006050899624825, Final Batch Loss: 0.015054631046950817\n",
      "Epoch 4101, Loss: 0.01887329574674368, Final Batch Loss: 0.011036068201065063\n",
      "Epoch 4102, Loss: 0.031412684358656406, Final Batch Loss: 0.011171887628734112\n",
      "Epoch 4103, Loss: 0.01633856212720275, Final Batch Loss: 0.0058847046457231045\n",
      "Epoch 4104, Loss: 0.025742354802787304, Final Batch Loss: 0.010479937307536602\n",
      "Epoch 4105, Loss: 0.019607046153396368, Final Batch Loss: 0.007283459883183241\n",
      "Epoch 4106, Loss: 0.01767344307154417, Final Batch Loss: 0.005765584297478199\n",
      "Epoch 4107, Loss: 0.03195728175342083, Final Batch Loss: 0.022105570882558823\n",
      "Epoch 4108, Loss: 0.012532055377960205, Final Batch Loss: 0.009781118482351303\n",
      "Epoch 4109, Loss: 0.017750665545463562, Final Batch Loss: 0.008382600732147694\n",
      "Epoch 4110, Loss: 0.01918863644823432, Final Batch Loss: 0.012027890421450138\n",
      "Epoch 4111, Loss: 0.013326761778444052, Final Batch Loss: 0.004160700831562281\n",
      "Epoch 4112, Loss: 0.017322020139545202, Final Batch Loss: 0.0060739372856915\n",
      "Epoch 4113, Loss: 0.021841217763721943, Final Batch Loss: 0.012984268367290497\n",
      "Epoch 4114, Loss: 0.012547997757792473, Final Batch Loss: 0.006196928676217794\n",
      "Epoch 4115, Loss: 0.02128882147371769, Final Batch Loss: 0.010081559419631958\n",
      "Epoch 4116, Loss: 0.1246770266443491, Final Batch Loss: 0.11652697622776031\n",
      "Epoch 4117, Loss: 0.05064640939235687, Final Batch Loss: 0.04583681374788284\n",
      "Epoch 4118, Loss: 0.019053512485697865, Final Batch Loss: 0.0030621711630374193\n",
      "Epoch 4119, Loss: 0.024036956019699574, Final Batch Loss: 0.011335998773574829\n",
      "Epoch 4120, Loss: 0.016658375971019268, Final Batch Loss: 0.0034211119636893272\n",
      "Epoch 4121, Loss: 0.049910482950508595, Final Batch Loss: 0.004976098425686359\n",
      "Epoch 4122, Loss: 0.02301848866045475, Final Batch Loss: 0.01906622014939785\n",
      "Epoch 4123, Loss: 0.022047285921871662, Final Batch Loss: 0.01327558234333992\n",
      "Epoch 4124, Loss: 0.009806387824937701, Final Batch Loss: 0.0020495003554970026\n",
      "Epoch 4125, Loss: 0.01733411126770079, Final Batch Loss: 0.013433723710477352\n",
      "Epoch 4126, Loss: 0.024116198997944593, Final Batch Loss: 0.0031479480676352978\n",
      "Epoch 4127, Loss: 0.02829921618103981, Final Batch Loss: 0.01694607362151146\n",
      "Epoch 4128, Loss: 0.013640087097883224, Final Batch Loss: 0.0018388647586107254\n",
      "Epoch 4129, Loss: 0.06636717915534973, Final Batch Loss: 0.020307712256908417\n",
      "Epoch 4130, Loss: 0.026730431709438562, Final Batch Loss: 0.002688285429030657\n",
      "Epoch 4131, Loss: 0.04878172371536493, Final Batch Loss: 0.014425461180508137\n",
      "Epoch 4132, Loss: 0.019084510393440723, Final Batch Loss: 0.00897406879812479\n",
      "Epoch 4133, Loss: 0.022124510258436203, Final Batch Loss: 0.006687506102025509\n",
      "Epoch 4134, Loss: 0.014605746138840914, Final Batch Loss: 0.003666460979729891\n",
      "Epoch 4135, Loss: 0.05212622135877609, Final Batch Loss: 0.028940808027982712\n",
      "Epoch 4136, Loss: 0.02574493968859315, Final Batch Loss: 0.021040283143520355\n",
      "Epoch 4137, Loss: 0.019717834889888763, Final Batch Loss: 0.0100868446752429\n",
      "Epoch 4138, Loss: 0.036959932651370764, Final Batch Loss: 0.0061780414544045925\n",
      "Epoch 4139, Loss: 0.01860737008973956, Final Batch Loss: 0.01125531829893589\n",
      "Epoch 4140, Loss: 0.01219509169459343, Final Batch Loss: 0.0053798481822013855\n",
      "Epoch 4141, Loss: 0.010942342691123486, Final Batch Loss: 0.004904306493699551\n",
      "Epoch 4142, Loss: 0.020014094654470682, Final Batch Loss: 0.007668291684240103\n",
      "Epoch 4143, Loss: 0.02223134832456708, Final Batch Loss: 0.01749122515320778\n",
      "Epoch 4144, Loss: 0.020560877863317728, Final Batch Loss: 0.016413651406764984\n",
      "Epoch 4145, Loss: 0.13051431253552437, Final Batch Loss: 0.1154278963804245\n",
      "Epoch 4146, Loss: 0.03066402766853571, Final Batch Loss: 0.008491822518408298\n",
      "Epoch 4147, Loss: 0.014384641777724028, Final Batch Loss: 0.009318338707089424\n",
      "Epoch 4148, Loss: 0.028089499566704035, Final Batch Loss: 0.007680254522711039\n",
      "Epoch 4149, Loss: 0.02037554746493697, Final Batch Loss: 0.004143258091062307\n",
      "Epoch 4150, Loss: 0.00946354866027832, Final Batch Loss: 0.006296088453382254\n",
      "Epoch 4151, Loss: 0.017863430082798004, Final Batch Loss: 0.004265310242772102\n",
      "Epoch 4152, Loss: 0.058838851749897, Final Batch Loss: 0.02560247853398323\n",
      "Epoch 4153, Loss: 0.03646131884306669, Final Batch Loss: 0.021232448518276215\n",
      "Epoch 4154, Loss: 0.010200147749856114, Final Batch Loss: 0.0023462495300918818\n",
      "Epoch 4155, Loss: 0.01992956642061472, Final Batch Loss: 0.011402822099626064\n",
      "Epoch 4156, Loss: 0.02554677240550518, Final Batch Loss: 0.013335300609469414\n",
      "Epoch 4157, Loss: 0.013603625353425741, Final Batch Loss: 0.007541984785348177\n",
      "Epoch 4158, Loss: 0.011545841814950109, Final Batch Loss: 0.0030041199643164873\n",
      "Epoch 4159, Loss: 0.01265232264995575, Final Batch Loss: 0.003953766077756882\n",
      "Epoch 4160, Loss: 0.02887891884893179, Final Batch Loss: 0.0119111193343997\n",
      "Epoch 4161, Loss: 0.038203499279916286, Final Batch Loss: 0.02575497515499592\n",
      "Epoch 4162, Loss: 0.13728874549269676, Final Batch Loss: 0.10069060325622559\n",
      "Epoch 4163, Loss: 0.06102619366720319, Final Batch Loss: 0.0050856140442192554\n",
      "Epoch 4164, Loss: 0.03195622470229864, Final Batch Loss: 0.014216401614248753\n",
      "Epoch 4165, Loss: 0.05550284869968891, Final Batch Loss: 0.03332848474383354\n",
      "Epoch 4166, Loss: 0.010296805063262582, Final Batch Loss: 0.0036314798053354025\n",
      "Epoch 4167, Loss: 0.008141350466758013, Final Batch Loss: 0.0019658240489661694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4168, Loss: 0.015379189513623714, Final Batch Loss: 0.006868287920951843\n",
      "Epoch 4169, Loss: 0.05266460031270981, Final Batch Loss: 0.023186901584267616\n",
      "Epoch 4170, Loss: 0.0297894817776978, Final Batch Loss: 0.006307721603661776\n",
      "Epoch 4171, Loss: 0.03536667302250862, Final Batch Loss: 0.01255757361650467\n",
      "Epoch 4172, Loss: 0.016374163795262575, Final Batch Loss: 0.010231996886432171\n",
      "Epoch 4173, Loss: 0.02025017561390996, Final Batch Loss: 0.006917042192071676\n",
      "Epoch 4174, Loss: 0.013282739790156484, Final Batch Loss: 0.002202554838731885\n",
      "Epoch 4175, Loss: 0.01925741694867611, Final Batch Loss: 0.010920978151261806\n",
      "Epoch 4176, Loss: 0.038826124742627144, Final Batch Loss: 0.019268911331892014\n",
      "Epoch 4177, Loss: 0.02712505217641592, Final Batch Loss: 0.004022871144115925\n",
      "Epoch 4178, Loss: 0.012965754605829716, Final Batch Loss: 0.009457473643124104\n",
      "Epoch 4179, Loss: 0.01308557391166687, Final Batch Loss: 0.0033123679459095\n",
      "Epoch 4180, Loss: 0.051117814145982265, Final Batch Loss: 0.03907877579331398\n",
      "Epoch 4181, Loss: 0.027067061513662338, Final Batch Loss: 0.016675559803843498\n",
      "Epoch 4182, Loss: 0.009498944506049156, Final Batch Loss: 0.0051638176664710045\n",
      "Epoch 4183, Loss: 0.010038912994787097, Final Batch Loss: 0.006895700469613075\n",
      "Epoch 4184, Loss: 0.014080001972615719, Final Batch Loss: 0.007456730119884014\n",
      "Epoch 4185, Loss: 0.017019232735037804, Final Batch Loss: 0.01161864772439003\n",
      "Epoch 4186, Loss: 0.0302850641310215, Final Batch Loss: 0.016365626826882362\n",
      "Epoch 4187, Loss: 0.019683285616338253, Final Batch Loss: 0.014230293221771717\n",
      "Epoch 4188, Loss: 0.028295569121837616, Final Batch Loss: 0.018723661080002785\n",
      "Epoch 4189, Loss: 0.008169243577867746, Final Batch Loss: 0.0023271008394658566\n",
      "Epoch 4190, Loss: 0.01430930895730853, Final Batch Loss: 0.003214332740753889\n",
      "Epoch 4191, Loss: 0.04612861014902592, Final Batch Loss: 0.024194689467549324\n",
      "Epoch 4192, Loss: 0.051180798560380936, Final Batch Loss: 0.01883915811777115\n",
      "Epoch 4193, Loss: 0.009870128706097603, Final Batch Loss: 0.005970667116343975\n",
      "Epoch 4194, Loss: 0.02299312222748995, Final Batch Loss: 0.013641081750392914\n",
      "Epoch 4195, Loss: 0.01591377006843686, Final Batch Loss: 0.0073394677601754665\n",
      "Epoch 4196, Loss: 0.015653162263333797, Final Batch Loss: 0.009592348709702492\n",
      "Epoch 4197, Loss: 0.02075593825429678, Final Batch Loss: 0.01596992276608944\n",
      "Epoch 4198, Loss: 0.010750372894108295, Final Batch Loss: 0.004545098170638084\n",
      "Epoch 4199, Loss: 0.03723821975290775, Final Batch Loss: 0.028556739911437035\n",
      "Epoch 4200, Loss: 0.007636187830939889, Final Batch Loss: 0.0031069449614733458\n",
      "Epoch 4201, Loss: 0.01550666056573391, Final Batch Loss: 0.013168933801352978\n",
      "Epoch 4202, Loss: 0.03568511828780174, Final Batch Loss: 0.018320033326745033\n",
      "Epoch 4203, Loss: 0.015622850973159075, Final Batch Loss: 0.005932689178735018\n",
      "Epoch 4204, Loss: 0.020038573071360588, Final Batch Loss: 0.013281288556754589\n",
      "Epoch 4205, Loss: 0.021143548423424363, Final Batch Loss: 0.003903364995494485\n",
      "Epoch 4206, Loss: 0.019229809753596783, Final Batch Loss: 0.009022897109389305\n",
      "Epoch 4207, Loss: 0.01738544716499746, Final Batch Loss: 0.013757357373833656\n",
      "Epoch 4208, Loss: 0.02892246562987566, Final Batch Loss: 0.004984498955309391\n",
      "Epoch 4209, Loss: 0.01769766630604863, Final Batch Loss: 0.01212579570710659\n",
      "Epoch 4210, Loss: 0.020046072313562036, Final Batch Loss: 0.01645575650036335\n",
      "Epoch 4211, Loss: 0.007905742852017283, Final Batch Loss: 0.00225140736438334\n",
      "Epoch 4212, Loss: 0.006120670819655061, Final Batch Loss: 0.0035193157382309437\n",
      "Epoch 4213, Loss: 0.038297384046018124, Final Batch Loss: 0.025775138288736343\n",
      "Epoch 4214, Loss: 0.014715844765305519, Final Batch Loss: 0.0034622922539711\n",
      "Epoch 4215, Loss: 0.05272509716451168, Final Batch Loss: 0.023167595267295837\n",
      "Epoch 4216, Loss: 0.030772567726671696, Final Batch Loss: 0.018314963206648827\n",
      "Epoch 4217, Loss: 0.00881455559283495, Final Batch Loss: 0.0053994543850421906\n",
      "Epoch 4218, Loss: 0.01834068971220404, Final Batch Loss: 0.001937055611051619\n",
      "Epoch 4219, Loss: 0.013012837851420045, Final Batch Loss: 0.01027982123196125\n",
      "Epoch 4220, Loss: 0.028815974481403828, Final Batch Loss: 0.019632723182439804\n",
      "Epoch 4221, Loss: 0.01393731590360403, Final Batch Loss: 0.009496007114648819\n",
      "Epoch 4222, Loss: 0.01795047614723444, Final Batch Loss: 0.01231076568365097\n",
      "Epoch 4223, Loss: 0.03217614348977804, Final Batch Loss: 0.007433759979903698\n",
      "Epoch 4224, Loss: 0.02579314261674881, Final Batch Loss: 0.008131686598062515\n",
      "Epoch 4225, Loss: 0.011989431688562036, Final Batch Loss: 0.001393751474097371\n",
      "Epoch 4226, Loss: 0.013002958614379168, Final Batch Loss: 0.008602196350693703\n",
      "Epoch 4227, Loss: 0.019356269855052233, Final Batch Loss: 0.00335973734036088\n",
      "Epoch 4228, Loss: 0.005287662264890969, Final Batch Loss: 0.0016799216391518712\n",
      "Epoch 4229, Loss: 0.014210266759619117, Final Batch Loss: 0.0030526749324053526\n",
      "Epoch 4230, Loss: 0.012147356290370226, Final Batch Loss: 0.007165622431784868\n",
      "Epoch 4231, Loss: 0.013386853272095323, Final Batch Loss: 0.009731241501867771\n",
      "Epoch 4232, Loss: 0.023256890941411257, Final Batch Loss: 0.017840737476944923\n",
      "Epoch 4233, Loss: 0.010053877718746662, Final Batch Loss: 0.003464318811893463\n",
      "Epoch 4234, Loss: 0.029290770646184683, Final Batch Loss: 0.0068073938600718975\n",
      "Epoch 4235, Loss: 0.06594309816136956, Final Batch Loss: 0.006317886058241129\n",
      "Epoch 4236, Loss: 0.02922415779903531, Final Batch Loss: 0.003633462358266115\n",
      "Epoch 4237, Loss: 0.011366120772436261, Final Batch Loss: 0.003725196933373809\n",
      "Epoch 4238, Loss: 0.02815506886690855, Final Batch Loss: 0.01138146873563528\n",
      "Epoch 4239, Loss: 0.020939167588949203, Final Batch Loss: 0.015373547561466694\n",
      "Epoch 4240, Loss: 0.013154678978025913, Final Batch Loss: 0.005843374878168106\n",
      "Epoch 4241, Loss: 0.0512509485706687, Final Batch Loss: 0.011755636893212795\n",
      "Epoch 4242, Loss: 0.03692458523437381, Final Batch Loss: 0.004372041206806898\n",
      "Epoch 4243, Loss: 0.028539410326629877, Final Batch Loss: 0.021942678838968277\n",
      "Epoch 4244, Loss: 0.014817551709711552, Final Batch Loss: 0.009089380502700806\n",
      "Epoch 4245, Loss: 0.028792576864361763, Final Batch Loss: 0.015774061903357506\n",
      "Epoch 4246, Loss: 0.045816059689968824, Final Batch Loss: 0.03905298188328743\n",
      "Epoch 4247, Loss: 0.02617960376664996, Final Batch Loss: 0.0051728165708482265\n",
      "Epoch 4248, Loss: 0.024959432892501354, Final Batch Loss: 0.020582368597388268\n",
      "Epoch 4249, Loss: 0.015156771754845977, Final Batch Loss: 0.0030530511867254972\n",
      "Epoch 4250, Loss: 0.010811558226123452, Final Batch Loss: 0.0029794482979923487\n",
      "Epoch 4251, Loss: 0.010953965596854687, Final Batch Loss: 0.006803372409194708\n",
      "Epoch 4252, Loss: 0.014731897506862879, Final Batch Loss: 0.0023203720338642597\n",
      "Epoch 4253, Loss: 0.09372488968074322, Final Batch Loss: 0.07650631666183472\n",
      "Epoch 4254, Loss: 0.025397442746907473, Final Batch Loss: 0.004593868274241686\n",
      "Epoch 4255, Loss: 0.020861678756773472, Final Batch Loss: 0.006588633172214031\n",
      "Epoch 4256, Loss: 0.047229557763785124, Final Batch Loss: 0.007382407318800688\n",
      "Epoch 4257, Loss: 0.0519039211794734, Final Batch Loss: 0.005731436423957348\n",
      "Epoch 4258, Loss: 0.045766524970531464, Final Batch Loss: 0.038348134607076645\n",
      "Epoch 4259, Loss: 0.07663435488939285, Final Batch Loss: 0.05054979398846626\n",
      "Epoch 4260, Loss: 0.03211439214646816, Final Batch Loss: 0.022816315293312073\n",
      "Epoch 4261, Loss: 0.011771691031754017, Final Batch Loss: 0.006212018430233002\n",
      "Epoch 4262, Loss: 0.009215810569003224, Final Batch Loss: 0.0023207662161439657\n",
      "Epoch 4263, Loss: 0.009347484447062016, Final Batch Loss: 0.003316106740385294\n",
      "Epoch 4264, Loss: 0.046018676832318306, Final Batch Loss: 0.03259609639644623\n",
      "Epoch 4265, Loss: 0.02974005602300167, Final Batch Loss: 0.021131426095962524\n",
      "Epoch 4266, Loss: 0.01095369877293706, Final Batch Loss: 0.0058777774684131145\n",
      "Epoch 4267, Loss: 0.009484007023274899, Final Batch Loss: 0.004602986387908459\n",
      "Epoch 4268, Loss: 0.020790571346879005, Final Batch Loss: 0.005973448045551777\n",
      "Epoch 4269, Loss: 0.02120525436475873, Final Batch Loss: 0.00495452294126153\n",
      "Epoch 4270, Loss: 0.02295509446412325, Final Batch Loss: 0.01232701912522316\n",
      "Epoch 4271, Loss: 0.019385938066989183, Final Batch Loss: 0.004511831793934107\n",
      "Epoch 4272, Loss: 0.010816682130098343, Final Batch Loss: 0.004561915528029203\n",
      "Epoch 4273, Loss: 0.021886000409722328, Final Batch Loss: 0.01175826694816351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4274, Loss: 0.03889485448598862, Final Batch Loss: 0.023117726668715477\n",
      "Epoch 4275, Loss: 0.008252025814726949, Final Batch Loss: 0.005073138512670994\n",
      "Epoch 4276, Loss: 0.017284474335610867, Final Batch Loss: 0.006981121376156807\n",
      "Epoch 4277, Loss: 0.01660394063219428, Final Batch Loss: 0.008835092186927795\n",
      "Epoch 4278, Loss: 0.02495200326666236, Final Batch Loss: 0.00402936851605773\n",
      "Epoch 4279, Loss: 0.020068828016519547, Final Batch Loss: 0.008268303237855434\n",
      "Epoch 4280, Loss: 0.022383346688002348, Final Batch Loss: 0.005051766987890005\n",
      "Epoch 4281, Loss: 0.01566502219066024, Final Batch Loss: 0.003795915748924017\n",
      "Epoch 4282, Loss: 0.016528830397874117, Final Batch Loss: 0.009356142953038216\n",
      "Epoch 4283, Loss: 0.022970511112362146, Final Batch Loss: 0.004696934018284082\n",
      "Epoch 4284, Loss: 0.09376921993680298, Final Batch Loss: 0.0898905098438263\n",
      "Epoch 4285, Loss: 0.012480348814278841, Final Batch Loss: 0.006254684645682573\n",
      "Epoch 4286, Loss: 0.026094574481248856, Final Batch Loss: 0.002929752692580223\n",
      "Epoch 4287, Loss: 0.019607293885201216, Final Batch Loss: 0.007555937860161066\n",
      "Epoch 4288, Loss: 0.011771202087402344, Final Batch Loss: 0.006664305459707975\n",
      "Epoch 4289, Loss: 0.024967207573354244, Final Batch Loss: 0.012825354002416134\n",
      "Epoch 4290, Loss: 0.033695048186928034, Final Batch Loss: 0.03130980208516121\n",
      "Epoch 4291, Loss: 0.0344898896291852, Final Batch Loss: 0.020387576892971992\n",
      "Epoch 4292, Loss: 0.04377265740185976, Final Batch Loss: 0.029535362496972084\n",
      "Epoch 4293, Loss: 0.015437540598213673, Final Batch Loss: 0.004646824672818184\n",
      "Epoch 4294, Loss: 0.019914831966161728, Final Batch Loss: 0.012391617521643639\n",
      "Epoch 4295, Loss: 0.008241184055805206, Final Batch Loss: 0.0032673808746039867\n",
      "Epoch 4296, Loss: 0.011800360167399049, Final Batch Loss: 0.0036146186757832766\n",
      "Epoch 4297, Loss: 0.020330950850620866, Final Batch Loss: 0.0033296982292085886\n",
      "Epoch 4298, Loss: 0.030019452795386314, Final Batch Loss: 0.018341297283768654\n",
      "Epoch 4299, Loss: 0.02783151064068079, Final Batch Loss: 0.01107154879719019\n",
      "Epoch 4300, Loss: 0.022732486482709646, Final Batch Loss: 0.006750393193215132\n",
      "Epoch 4301, Loss: 0.017477060202509165, Final Batch Loss: 0.01269963663071394\n",
      "Epoch 4302, Loss: 0.015724836848676205, Final Batch Loss: 0.011618668213486671\n",
      "Epoch 4303, Loss: 0.012597853783518076, Final Batch Loss: 0.004924414679408073\n",
      "Epoch 4304, Loss: 0.030013257637619972, Final Batch Loss: 0.0021470878273248672\n",
      "Epoch 4305, Loss: 0.012397262500599027, Final Batch Loss: 0.009788675233721733\n",
      "Epoch 4306, Loss: 0.034463876858353615, Final Batch Loss: 0.016564171761274338\n",
      "Epoch 4307, Loss: 0.024094339925795794, Final Batch Loss: 0.0034877662546932697\n",
      "Epoch 4308, Loss: 0.025174445006996393, Final Batch Loss: 0.019952990114688873\n",
      "Epoch 4309, Loss: 0.01309344545006752, Final Batch Loss: 0.003896270878612995\n",
      "Epoch 4310, Loss: 0.019955597817897797, Final Batch Loss: 0.005921839736402035\n",
      "Epoch 4311, Loss: 0.06139521533623338, Final Batch Loss: 0.05618700012564659\n",
      "Epoch 4312, Loss: 0.007556587690487504, Final Batch Loss: 0.0038773876149207354\n",
      "Epoch 4313, Loss: 0.030881935730576515, Final Batch Loss: 0.012622945010662079\n",
      "Epoch 4314, Loss: 0.019921100698411465, Final Batch Loss: 0.008262562565505505\n",
      "Epoch 4315, Loss: 0.04231908265501261, Final Batch Loss: 0.015003220178186893\n",
      "Epoch 4316, Loss: 0.020100833848118782, Final Batch Loss: 0.008496908470988274\n",
      "Epoch 4317, Loss: 0.02376120164990425, Final Batch Loss: 0.005774449557065964\n",
      "Epoch 4318, Loss: 0.014100215630605817, Final Batch Loss: 0.0019347539637237787\n",
      "Epoch 4319, Loss: 0.019824027782306075, Final Batch Loss: 0.0031491105910390615\n",
      "Epoch 4320, Loss: 0.03849972411990166, Final Batch Loss: 0.015133144333958626\n",
      "Epoch 4321, Loss: 0.030095805414021015, Final Batch Loss: 0.012206119485199451\n",
      "Epoch 4322, Loss: 0.01624925434589386, Final Batch Loss: 0.004327215254306793\n",
      "Epoch 4323, Loss: 0.12713701277971268, Final Batch Loss: 0.06348447501659393\n",
      "Epoch 4324, Loss: 0.02908687014132738, Final Batch Loss: 0.01702330820262432\n",
      "Epoch 4325, Loss: 0.0143814985640347, Final Batch Loss: 0.00551011273637414\n",
      "Epoch 4326, Loss: 0.04144020564854145, Final Batch Loss: 0.01959565468132496\n",
      "Epoch 4327, Loss: 0.02195431850850582, Final Batch Loss: 0.010940882377326488\n",
      "Epoch 4328, Loss: 0.011809529038146138, Final Batch Loss: 0.008453316055238247\n",
      "Epoch 4329, Loss: 0.020854206755757332, Final Batch Loss: 0.008196373470127583\n",
      "Epoch 4330, Loss: 0.024846072308719158, Final Batch Loss: 0.011008174158632755\n",
      "Epoch 4331, Loss: 0.01591830188408494, Final Batch Loss: 0.008943787775933743\n",
      "Epoch 4332, Loss: 0.10598377697169781, Final Batch Loss: 0.0980505645275116\n",
      "Epoch 4333, Loss: 0.031181745696812868, Final Batch Loss: 0.023384060710668564\n",
      "Epoch 4334, Loss: 0.016584092285484076, Final Batch Loss: 0.009901168756186962\n",
      "Epoch 4335, Loss: 0.018790312577039003, Final Batch Loss: 0.013716104440391064\n",
      "Epoch 4336, Loss: 0.006842594943009317, Final Batch Loss: 0.0011841621017083526\n",
      "Epoch 4337, Loss: 0.032374182250350714, Final Batch Loss: 0.0036362181417644024\n",
      "Epoch 4338, Loss: 0.02104264497756958, Final Batch Loss: 0.011252634227275848\n",
      "Epoch 4339, Loss: 0.010265604592859745, Final Batch Loss: 0.0063712941482663155\n",
      "Epoch 4340, Loss: 0.007021544734016061, Final Batch Loss: 0.00399080291390419\n",
      "Epoch 4341, Loss: 0.011333654867485166, Final Batch Loss: 0.003352517494931817\n",
      "Epoch 4342, Loss: 0.026536382734775543, Final Batch Loss: 0.008473824709653854\n",
      "Epoch 4343, Loss: 0.02618146501481533, Final Batch Loss: 0.009089794009923935\n",
      "Epoch 4344, Loss: 0.007655065739527345, Final Batch Loss: 0.004000855144113302\n",
      "Epoch 4345, Loss: 0.016816487535834312, Final Batch Loss: 0.0036247754469513893\n",
      "Epoch 4346, Loss: 0.04724491713568568, Final Batch Loss: 0.03984440118074417\n",
      "Epoch 4347, Loss: 0.026047512888908386, Final Batch Loss: 0.010060759261250496\n",
      "Epoch 4348, Loss: 0.01790260663256049, Final Batch Loss: 0.011381051503121853\n",
      "Epoch 4349, Loss: 0.021774754859507084, Final Batch Loss: 0.018235791474580765\n",
      "Epoch 4350, Loss: 0.010726649081334472, Final Batch Loss: 0.0026558793615549803\n",
      "Epoch 4351, Loss: 0.08828876912593842, Final Batch Loss: 0.05368650704622269\n",
      "Epoch 4352, Loss: 0.015567073598504066, Final Batch Loss: 0.011244239285588264\n",
      "Epoch 4353, Loss: 0.03345455415546894, Final Batch Loss: 0.009834039956331253\n",
      "Epoch 4354, Loss: 0.02809032704681158, Final Batch Loss: 0.024071933701634407\n",
      "Epoch 4355, Loss: 0.059429706539958715, Final Batch Loss: 0.05625974386930466\n",
      "Epoch 4356, Loss: 0.010653528617694974, Final Batch Loss: 0.006816396955400705\n",
      "Epoch 4357, Loss: 0.011732965242117643, Final Batch Loss: 0.00606732489541173\n",
      "Epoch 4358, Loss: 0.02597935451194644, Final Batch Loss: 0.004598391707986593\n",
      "Epoch 4359, Loss: 0.03485720418393612, Final Batch Loss: 0.025352843105793\n",
      "Epoch 4360, Loss: 0.014466628897935152, Final Batch Loss: 0.005894542206078768\n",
      "Epoch 4361, Loss: 0.01623775064945221, Final Batch Loss: 0.00685604102909565\n",
      "Epoch 4362, Loss: 0.030109746381640434, Final Batch Loss: 0.01845976896584034\n",
      "Epoch 4363, Loss: 0.02778020314872265, Final Batch Loss: 0.019076963886618614\n",
      "Epoch 4364, Loss: 0.01573269534856081, Final Batch Loss: 0.010030987672507763\n",
      "Epoch 4365, Loss: 0.016122817061841488, Final Batch Loss: 0.008953574113547802\n",
      "Epoch 4366, Loss: 0.006987253436818719, Final Batch Loss: 0.002774525200948119\n",
      "Epoch 4367, Loss: 0.008936735335737467, Final Batch Loss: 0.0041641429997980595\n",
      "Epoch 4368, Loss: 0.0278249301481992, Final Batch Loss: 0.0030061465222388506\n",
      "Epoch 4369, Loss: 0.0147296073846519, Final Batch Loss: 0.004253812599927187\n",
      "Epoch 4370, Loss: 0.02094480791129172, Final Batch Loss: 0.003224496031180024\n",
      "Epoch 4371, Loss: 0.006930853007361293, Final Batch Loss: 0.0036617221776396036\n",
      "Epoch 4372, Loss: 0.016540118493139744, Final Batch Loss: 0.010353808291256428\n",
      "Epoch 4373, Loss: 0.021653531584888697, Final Batch Loss: 0.01565464586019516\n",
      "Epoch 4374, Loss: 0.015710619278252125, Final Batch Loss: 0.010139831341803074\n",
      "Epoch 4375, Loss: 0.014480333775281906, Final Batch Loss: 0.007707267999649048\n",
      "Epoch 4376, Loss: 0.010567524936050177, Final Batch Loss: 0.004884755238890648\n",
      "Epoch 4377, Loss: 0.02241226425394416, Final Batch Loss: 0.0034757102839648724\n",
      "Epoch 4378, Loss: 0.010876028332859278, Final Batch Loss: 0.006952165625989437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4379, Loss: 0.021545712370425463, Final Batch Loss: 0.01844274066388607\n",
      "Epoch 4380, Loss: 0.01521533471532166, Final Batch Loss: 0.0030483475420624018\n",
      "Epoch 4381, Loss: 0.01627230178564787, Final Batch Loss: 0.009263589978218079\n",
      "Epoch 4382, Loss: 0.005993582308292389, Final Batch Loss: 0.001965590752661228\n",
      "Epoch 4383, Loss: 0.009967809310182929, Final Batch Loss: 0.0033782029058784246\n",
      "Epoch 4384, Loss: 0.020134303718805313, Final Batch Loss: 0.008064099587500095\n",
      "Epoch 4385, Loss: 0.011542548891156912, Final Batch Loss: 0.002207397948950529\n",
      "Epoch 4386, Loss: 0.010046362178400159, Final Batch Loss: 0.0027121480088680983\n",
      "Epoch 4387, Loss: 0.026611486449837685, Final Batch Loss: 0.023701222613453865\n",
      "Epoch 4388, Loss: 0.012532200897112489, Final Batch Loss: 0.0037929092068225145\n",
      "Epoch 4389, Loss: 0.05446413205936551, Final Batch Loss: 0.051856137812137604\n",
      "Epoch 4390, Loss: 0.011153677012771368, Final Batch Loss: 0.005154966842383146\n",
      "Epoch 4391, Loss: 0.027137411758303642, Final Batch Loss: 0.009772367775440216\n",
      "Epoch 4392, Loss: 0.014162690378725529, Final Batch Loss: 0.009652272798120975\n",
      "Epoch 4393, Loss: 0.04867805098183453, Final Batch Loss: 0.04511455073952675\n",
      "Epoch 4394, Loss: 0.007020036922767758, Final Batch Loss: 0.0034923614002764225\n",
      "Epoch 4395, Loss: 0.010401013307273388, Final Batch Loss: 0.00190800242125988\n",
      "Epoch 4396, Loss: 0.009815075434744358, Final Batch Loss: 0.004154360853135586\n",
      "Epoch 4397, Loss: 0.008831959217786789, Final Batch Loss: 0.003966029733419418\n",
      "Epoch 4398, Loss: 0.0672994451597333, Final Batch Loss: 0.010797121562063694\n",
      "Epoch 4399, Loss: 0.030200250912457705, Final Batch Loss: 0.02328539453446865\n",
      "Epoch 4400, Loss: 0.033436969853937626, Final Batch Loss: 0.028900351375341415\n",
      "Epoch 4401, Loss: 0.009513538796454668, Final Batch Loss: 0.006510699633508921\n",
      "Epoch 4402, Loss: 0.016523172613233328, Final Batch Loss: 0.01301554124802351\n",
      "Epoch 4403, Loss: 0.017679065000265837, Final Batch Loss: 0.012394307181239128\n",
      "Epoch 4404, Loss: 0.029856301844120026, Final Batch Loss: 0.007847895845770836\n",
      "Epoch 4405, Loss: 0.025023168418556452, Final Batch Loss: 0.005933769512921572\n",
      "Epoch 4406, Loss: 0.03023995365947485, Final Batch Loss: 0.019389547407627106\n",
      "Epoch 4407, Loss: 0.013068479485809803, Final Batch Loss: 0.003989545628428459\n",
      "Epoch 4408, Loss: 0.020627439953386784, Final Batch Loss: 0.01387682743370533\n",
      "Epoch 4409, Loss: 0.015906630316749215, Final Batch Loss: 0.013007811270654202\n",
      "Epoch 4410, Loss: 0.012933286838233471, Final Batch Loss: 0.005534087773412466\n",
      "Epoch 4411, Loss: 0.00922849029302597, Final Batch Loss: 0.004338233266025782\n",
      "Epoch 4412, Loss: 0.014324946328997612, Final Batch Loss: 0.0061134397983551025\n",
      "Epoch 4413, Loss: 0.023122980259358883, Final Batch Loss: 0.01844610460102558\n",
      "Epoch 4414, Loss: 0.01448283065110445, Final Batch Loss: 0.005321463569998741\n",
      "Epoch 4415, Loss: 0.05735957249999046, Final Batch Loss: 0.03315165266394615\n",
      "Epoch 4416, Loss: 0.04591505788266659, Final Batch Loss: 0.03447623550891876\n",
      "Epoch 4417, Loss: 0.016175905941054225, Final Batch Loss: 0.003660887246951461\n",
      "Epoch 4418, Loss: 0.023725590202957392, Final Batch Loss: 0.017215346917510033\n",
      "Epoch 4419, Loss: 0.024444203823804855, Final Batch Loss: 0.012423173524439335\n",
      "Epoch 4420, Loss: 0.019621460931375623, Final Batch Loss: 0.0025238723028451204\n",
      "Epoch 4421, Loss: 0.017629205249249935, Final Batch Loss: 0.005979189649224281\n",
      "Epoch 4422, Loss: 0.020505170803517103, Final Batch Loss: 0.015743713825941086\n",
      "Epoch 4423, Loss: 0.03695941623300314, Final Batch Loss: 0.006050444208085537\n",
      "Epoch 4424, Loss: 0.024490900337696075, Final Batch Loss: 0.014539762400090694\n",
      "Epoch 4425, Loss: 0.014420371502637863, Final Batch Loss: 0.008377458900213242\n",
      "Epoch 4426, Loss: 0.03357349638827145, Final Batch Loss: 0.030571093782782555\n",
      "Epoch 4427, Loss: 0.01029718341305852, Final Batch Loss: 0.0060545713640749454\n",
      "Epoch 4428, Loss: 0.018526966217905283, Final Batch Loss: 0.014462910592556\n",
      "Epoch 4429, Loss: 0.01613303367048502, Final Batch Loss: 0.013701568357646465\n",
      "Epoch 4430, Loss: 0.008082774467766285, Final Batch Loss: 0.004475814756006002\n",
      "Epoch 4431, Loss: 0.03293976420536637, Final Batch Loss: 0.005860761273652315\n",
      "Epoch 4432, Loss: 0.011371761560440063, Final Batch Loss: 0.003879938740283251\n",
      "Epoch 4433, Loss: 0.009807786671444774, Final Batch Loss: 0.005918649956583977\n",
      "Epoch 4434, Loss: 0.01675251591950655, Final Batch Loss: 0.0053053200244903564\n",
      "Epoch 4435, Loss: 0.017055923584848642, Final Batch Loss: 0.0054470025934278965\n",
      "Epoch 4436, Loss: 0.00857265992090106, Final Batch Loss: 0.0037684915587306023\n",
      "Epoch 4437, Loss: 0.0039477014215663075, Final Batch Loss: 0.0016654111677780747\n",
      "Epoch 4438, Loss: 0.010819271439686418, Final Batch Loss: 0.0020478556398302317\n",
      "Epoch 4439, Loss: 0.027320864144712687, Final Batch Loss: 0.02541985921561718\n",
      "Epoch 4440, Loss: 0.006590122589841485, Final Batch Loss: 0.0018738571088761091\n",
      "Epoch 4441, Loss: 0.0075409895507618785, Final Batch Loss: 0.001676794490776956\n",
      "Epoch 4442, Loss: 0.0321137789869681, Final Batch Loss: 0.0013150867307558656\n",
      "Epoch 4443, Loss: 0.018484050873667, Final Batch Loss: 0.006712493021041155\n",
      "Epoch 4444, Loss: 0.013944672886282206, Final Batch Loss: 0.006432457361370325\n",
      "Epoch 4445, Loss: 0.037282164208590984, Final Batch Loss: 0.012648737989366055\n",
      "Epoch 4446, Loss: 0.004739190451800823, Final Batch Loss: 0.003003527410328388\n",
      "Epoch 4447, Loss: 0.007886152947321534, Final Batch Loss: 0.0023561271373182535\n",
      "Epoch 4448, Loss: 0.015238706953823566, Final Batch Loss: 0.010473526082932949\n",
      "Epoch 4449, Loss: 0.006491507636383176, Final Batch Loss: 0.0033582081086933613\n",
      "Epoch 4450, Loss: 0.022624678909778595, Final Batch Loss: 0.004235958680510521\n",
      "Epoch 4451, Loss: 0.009127970086410642, Final Batch Loss: 0.0031035274732857943\n",
      "Epoch 4452, Loss: 0.0640242313966155, Final Batch Loss: 0.006147048436105251\n",
      "Epoch 4453, Loss: 0.02186500420793891, Final Batch Loss: 0.00499493395909667\n",
      "Epoch 4454, Loss: 0.01992229768075049, Final Batch Loss: 0.0022749623749405146\n",
      "Epoch 4455, Loss: 0.024047535145655274, Final Batch Loss: 0.002434617141261697\n",
      "Epoch 4456, Loss: 0.005119804758578539, Final Batch Loss: 0.002399833407253027\n",
      "Epoch 4457, Loss: 0.028668932616710663, Final Batch Loss: 0.011077996343374252\n",
      "Epoch 4458, Loss: 0.04101167432963848, Final Batch Loss: 0.024147842079401016\n",
      "Epoch 4459, Loss: 0.008822193602100015, Final Batch Loss: 0.005000317934900522\n",
      "Epoch 4460, Loss: 0.011419656686484814, Final Batch Loss: 0.007481331937015057\n",
      "Epoch 4461, Loss: 0.012553126085549593, Final Batch Loss: 0.007661624811589718\n",
      "Epoch 4462, Loss: 0.013042787555605173, Final Batch Loss: 0.006417023949325085\n",
      "Epoch 4463, Loss: 0.029407141031697392, Final Batch Loss: 0.003794091986492276\n",
      "Epoch 4464, Loss: 0.014474090654402971, Final Batch Loss: 0.005110736470669508\n",
      "Epoch 4465, Loss: 0.011444447096437216, Final Batch Loss: 0.0071399640291929245\n",
      "Epoch 4466, Loss: 0.015548701398074627, Final Batch Loss: 0.010064555332064629\n",
      "Epoch 4467, Loss: 0.018019723240286112, Final Batch Loss: 0.01556810550391674\n",
      "Epoch 4468, Loss: 0.03977831453084946, Final Batch Loss: 0.017016587778925896\n",
      "Epoch 4469, Loss: 0.009410651866346598, Final Batch Loss: 0.004553291480988264\n",
      "Epoch 4470, Loss: 0.009211902506649494, Final Batch Loss: 0.0061545115895569324\n",
      "Epoch 4471, Loss: 0.03720426931977272, Final Batch Loss: 0.013028012588620186\n",
      "Epoch 4472, Loss: 0.008196006063371897, Final Batch Loss: 0.003969643265008926\n",
      "Epoch 4473, Loss: 0.012020721100270748, Final Batch Loss: 0.006804828532040119\n",
      "Epoch 4474, Loss: 0.01628089975565672, Final Batch Loss: 0.005159863270819187\n",
      "Epoch 4475, Loss: 0.013306512963026762, Final Batch Loss: 0.0066946521401405334\n",
      "Epoch 4476, Loss: 0.010266939178109169, Final Batch Loss: 0.004610651638358831\n",
      "Epoch 4477, Loss: 0.014368154108524323, Final Batch Loss: 0.008269630372524261\n",
      "Epoch 4478, Loss: 0.020229764049872756, Final Batch Loss: 0.01670939102768898\n",
      "Epoch 4479, Loss: 0.008858483750373125, Final Batch Loss: 0.002220151014626026\n",
      "Epoch 4480, Loss: 0.009426813572645187, Final Batch Loss: 0.006551190745085478\n",
      "Epoch 4481, Loss: 0.018268614076077938, Final Batch Loss: 0.0034683728590607643\n",
      "Epoch 4482, Loss: 0.007820433005690575, Final Batch Loss: 0.005566301289945841\n",
      "Epoch 4483, Loss: 0.005035854876041412, Final Batch Loss: 0.0030612756963819265\n",
      "Epoch 4484, Loss: 0.08655518712475896, Final Batch Loss: 0.007226255256682634\n",
      "Epoch 4485, Loss: 0.01929323049262166, Final Batch Loss: 0.006176838185638189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4486, Loss: 0.008333724457770586, Final Batch Loss: 0.0031619383953511715\n",
      "Epoch 4487, Loss: 0.015500425361096859, Final Batch Loss: 0.00644429586827755\n",
      "Epoch 4488, Loss: 0.016445515677332878, Final Batch Loss: 0.008565030992031097\n",
      "Epoch 4489, Loss: 0.010623949347063899, Final Batch Loss: 0.002472369698807597\n",
      "Epoch 4490, Loss: 0.012199576827697456, Final Batch Loss: 0.0016285422025248408\n",
      "Epoch 4491, Loss: 0.015333617106080055, Final Batch Loss: 0.007928316481411457\n",
      "Epoch 4492, Loss: 0.024313530884683132, Final Batch Loss: 0.008331202901899815\n",
      "Epoch 4493, Loss: 0.051553234457969666, Final Batch Loss: 0.041500091552734375\n",
      "Epoch 4494, Loss: 0.01286179618909955, Final Batch Loss: 0.0064186048693954945\n",
      "Epoch 4495, Loss: 0.012399639468640089, Final Batch Loss: 0.0025559780187904835\n",
      "Epoch 4496, Loss: 0.014058954315260053, Final Batch Loss: 0.0018956505227833986\n",
      "Epoch 4497, Loss: 0.03393707424402237, Final Batch Loss: 0.006366649642586708\n",
      "Epoch 4498, Loss: 0.02162258792668581, Final Batch Loss: 0.005216327495872974\n",
      "Epoch 4499, Loss: 0.014058094937354326, Final Batch Loss: 0.005904036108404398\n",
      "Epoch 4500, Loss: 0.006400140700861812, Final Batch Loss: 0.0034781252034008503\n",
      "Epoch 4501, Loss: 0.009531971300020814, Final Batch Loss: 0.0021803982090204954\n",
      "Epoch 4502, Loss: 0.020435363985598087, Final Batch Loss: 0.0037987669929862022\n",
      "Epoch 4503, Loss: 0.035353454295545816, Final Batch Loss: 0.006218648049980402\n",
      "Epoch 4504, Loss: 0.0040657371282577515, Final Batch Loss: 0.00242856377735734\n",
      "Epoch 4505, Loss: 0.03900543134659529, Final Batch Loss: 0.03481629863381386\n",
      "Epoch 4506, Loss: 0.008285837946459651, Final Batch Loss: 0.0038588412571698427\n",
      "Epoch 4507, Loss: 0.02256149100139737, Final Batch Loss: 0.017476892098784447\n",
      "Epoch 4508, Loss: 0.04374760668724775, Final Batch Loss: 0.035805169492959976\n",
      "Epoch 4509, Loss: 0.018755455734208226, Final Batch Loss: 0.014905782416462898\n",
      "Epoch 4510, Loss: 0.04988770931959152, Final Batch Loss: 0.016022201627492905\n",
      "Epoch 4511, Loss: 0.04345541913062334, Final Batch Loss: 0.03168746456503868\n",
      "Epoch 4512, Loss: 0.018606103491038084, Final Batch Loss: 0.0032776580192148685\n",
      "Epoch 4513, Loss: 0.01870903791859746, Final Batch Loss: 0.005077734123915434\n",
      "Epoch 4514, Loss: 0.026320546865463257, Final Batch Loss: 0.010188519954681396\n",
      "Epoch 4515, Loss: 0.008389679482206702, Final Batch Loss: 0.003073097439482808\n",
      "Epoch 4516, Loss: 0.021793165942654014, Final Batch Loss: 0.01842481829226017\n",
      "Epoch 4517, Loss: 0.025767656974494457, Final Batch Loss: 0.023425865918397903\n",
      "Epoch 4518, Loss: 0.010813694912940264, Final Batch Loss: 0.004399677738547325\n",
      "Epoch 4519, Loss: 0.013746588490903378, Final Batch Loss: 0.007664422504603863\n",
      "Epoch 4520, Loss: 0.01804037531837821, Final Batch Loss: 0.013682986609637737\n",
      "Epoch 4521, Loss: 0.010303017683327198, Final Batch Loss: 0.002286890521645546\n",
      "Epoch 4522, Loss: 0.04077756777405739, Final Batch Loss: 0.0232005063444376\n",
      "Epoch 4523, Loss: 0.020200671162456274, Final Batch Loss: 0.012978918850421906\n",
      "Epoch 4524, Loss: 0.01753768790513277, Final Batch Loss: 0.007527458481490612\n",
      "Epoch 4525, Loss: 0.04221372678875923, Final Batch Loss: 0.012065745890140533\n",
      "Epoch 4526, Loss: 0.00879387860186398, Final Batch Loss: 0.0063078198581933975\n",
      "Epoch 4527, Loss: 0.08038918301463127, Final Batch Loss: 0.060659512877464294\n",
      "Epoch 4528, Loss: 0.12144842930138111, Final Batch Loss: 0.11069697141647339\n",
      "Epoch 4529, Loss: 0.01989950705319643, Final Batch Loss: 0.008384072221815586\n",
      "Epoch 4530, Loss: 0.03552673198282719, Final Batch Loss: 0.028771255165338516\n",
      "Epoch 4531, Loss: 0.051861266838386655, Final Batch Loss: 0.0023765594232827425\n",
      "Epoch 4532, Loss: 0.02465123124420643, Final Batch Loss: 0.008848614990711212\n",
      "Epoch 4533, Loss: 0.016643310897052288, Final Batch Loss: 0.003758000209927559\n",
      "Epoch 4534, Loss: 0.021563292481005192, Final Batch Loss: 0.007975716143846512\n",
      "Epoch 4535, Loss: 0.01689616171643138, Final Batch Loss: 0.004001956898719072\n",
      "Epoch 4536, Loss: 0.006985305808484554, Final Batch Loss: 0.0024488307535648346\n",
      "Epoch 4537, Loss: 0.009713853942230344, Final Batch Loss: 0.002993594156578183\n",
      "Epoch 4538, Loss: 0.01976026874035597, Final Batch Loss: 0.008248036727309227\n",
      "Epoch 4539, Loss: 0.011283725500106812, Final Batch Loss: 0.006046694703400135\n",
      "Epoch 4540, Loss: 0.019817015388980508, Final Batch Loss: 0.003230583155527711\n",
      "Epoch 4541, Loss: 0.02199724270030856, Final Batch Loss: 0.01834730990231037\n",
      "Epoch 4542, Loss: 0.043357198126614094, Final Batch Loss: 0.010603227652609348\n",
      "Epoch 4543, Loss: 0.013221694622188807, Final Batch Loss: 0.0032436796464025974\n",
      "Epoch 4544, Loss: 0.010431849397718906, Final Batch Loss: 0.0057251364924013615\n",
      "Epoch 4545, Loss: 0.011923925019800663, Final Batch Loss: 0.006420178804546595\n",
      "Epoch 4546, Loss: 0.012530233711004257, Final Batch Loss: 0.007176350802183151\n",
      "Epoch 4547, Loss: 0.03193890117108822, Final Batch Loss: 0.006397344172000885\n",
      "Epoch 4548, Loss: 0.009652119828388095, Final Batch Loss: 0.003584912745282054\n",
      "Epoch 4549, Loss: 0.01047520898282528, Final Batch Loss: 0.0055087567307055\n",
      "Epoch 4550, Loss: 0.007985008647665381, Final Batch Loss: 0.005754914600402117\n",
      "Epoch 4551, Loss: 0.04433002881705761, Final Batch Loss: 0.025117037817835808\n",
      "Epoch 4552, Loss: 0.027702546678483486, Final Batch Loss: 0.012423000298440456\n",
      "Epoch 4553, Loss: 0.010765346232801676, Final Batch Loss: 0.006181512493640184\n",
      "Epoch 4554, Loss: 0.03050759807229042, Final Batch Loss: 0.00860179029405117\n",
      "Epoch 4555, Loss: 0.011061951518058777, Final Batch Loss: 0.0038351817056536674\n",
      "Epoch 4556, Loss: 0.025221744552254677, Final Batch Loss: 0.013459080830216408\n",
      "Epoch 4557, Loss: 0.016046154778450727, Final Batch Loss: 0.013383186422288418\n",
      "Epoch 4558, Loss: 0.010296893306076527, Final Batch Loss: 0.006156135816127062\n",
      "Epoch 4559, Loss: 0.007806930225342512, Final Batch Loss: 0.002977092284709215\n",
      "Epoch 4560, Loss: 0.01787498057819903, Final Batch Loss: 0.0025986407417804003\n",
      "Epoch 4561, Loss: 0.00860415305942297, Final Batch Loss: 0.0018587727099657059\n",
      "Epoch 4562, Loss: 0.012464566621929407, Final Batch Loss: 0.0030340715311467648\n",
      "Epoch 4563, Loss: 0.011082456912845373, Final Batch Loss: 0.0030552628450095654\n",
      "Epoch 4564, Loss: 0.01584909751545638, Final Batch Loss: 0.0013335681287571788\n",
      "Epoch 4565, Loss: 0.013810937758535147, Final Batch Loss: 0.00768639100715518\n",
      "Epoch 4566, Loss: 0.019822878297418356, Final Batch Loss: 0.0029517426155507565\n",
      "Epoch 4567, Loss: 0.044390952214598656, Final Batch Loss: 0.04023699462413788\n",
      "Epoch 4568, Loss: 0.007465902250260115, Final Batch Loss: 0.005075161810964346\n",
      "Epoch 4569, Loss: 0.028916559647768736, Final Batch Loss: 0.00744678033515811\n",
      "Epoch 4570, Loss: 0.011921244440600276, Final Batch Loss: 0.0012702893000096083\n",
      "Epoch 4571, Loss: 0.013666026294231415, Final Batch Loss: 0.0049124304205179214\n",
      "Epoch 4572, Loss: 0.028982531744986773, Final Batch Loss: 0.0057686627842485905\n",
      "Epoch 4573, Loss: 0.01300443522632122, Final Batch Loss: 0.005292312707751989\n",
      "Epoch 4574, Loss: 0.005210745963267982, Final Batch Loss: 0.0019092332804575562\n",
      "Epoch 4575, Loss: 0.12144140154123306, Final Batch Loss: 0.09980060160160065\n",
      "Epoch 4576, Loss: 0.005012563779018819, Final Batch Loss: 0.0014297551242634654\n",
      "Epoch 4577, Loss: 0.01919825468212366, Final Batch Loss: 0.0033431528136134148\n",
      "Epoch 4578, Loss: 0.029146593995392323, Final Batch Loss: 0.013812185265123844\n",
      "Epoch 4579, Loss: 0.022808711044490337, Final Batch Loss: 0.013101441785693169\n",
      "Epoch 4580, Loss: 0.02140953205525875, Final Batch Loss: 0.018881037831306458\n",
      "Epoch 4581, Loss: 0.006955241085961461, Final Batch Loss: 0.0022650950122624636\n",
      "Epoch 4582, Loss: 0.018636082531884313, Final Batch Loss: 0.01486253086477518\n",
      "Epoch 4583, Loss: 0.03531843330711126, Final Batch Loss: 0.025719566270709038\n",
      "Epoch 4584, Loss: 0.01562650827690959, Final Batch Loss: 0.006125304382294416\n",
      "Epoch 4585, Loss: 0.025790425017476082, Final Batch Loss: 0.022584740072488785\n",
      "Epoch 4586, Loss: 0.015193040482699871, Final Batch Loss: 0.006417178548872471\n",
      "Epoch 4587, Loss: 0.013125024270266294, Final Batch Loss: 0.002132595982402563\n",
      "Epoch 4588, Loss: 0.03618696145713329, Final Batch Loss: 0.025878146290779114\n",
      "Epoch 4589, Loss: 0.03549139387905598, Final Batch Loss: 0.010364999994635582\n",
      "Epoch 4590, Loss: 0.020007634535431862, Final Batch Loss: 0.007108407095074654\n",
      "Epoch 4591, Loss: 0.013040500693023205, Final Batch Loss: 0.006851074751466513\n",
      "Epoch 4592, Loss: 0.003552029491402209, Final Batch Loss: 0.0018450268544256687\n",
      "Epoch 4593, Loss: 0.022614366840571165, Final Batch Loss: 0.019956864416599274\n",
      "Epoch 4594, Loss: 0.047016654163599014, Final Batch Loss: 0.023425156250596046\n",
      "Epoch 4595, Loss: 0.06094630481675267, Final Batch Loss: 0.00381052540615201\n",
      "Epoch 4596, Loss: 0.02616695873439312, Final Batch Loss: 0.014606027863919735\n",
      "Epoch 4597, Loss: 0.03647535294294357, Final Batch Loss: 0.025125402957201004\n",
      "Epoch 4598, Loss: 0.046714888885617256, Final Batch Loss: 0.01411089114844799\n",
      "Epoch 4599, Loss: 0.04327971860766411, Final Batch Loss: 0.023880835622549057\n",
      "Epoch 4600, Loss: 0.025983319617807865, Final Batch Loss: 0.012395708821713924\n",
      "Epoch 4601, Loss: 0.010918652173131704, Final Batch Loss: 0.0036162640899419785\n",
      "Epoch 4602, Loss: 0.025742693804204464, Final Batch Loss: 0.011471174657344818\n",
      "Epoch 4603, Loss: 0.01161726051941514, Final Batch Loss: 0.007911814376711845\n",
      "Epoch 4604, Loss: 0.019390281289815903, Final Batch Loss: 0.011526234447956085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4605, Loss: 0.030131075531244278, Final Batch Loss: 0.01573774218559265\n",
      "Epoch 4606, Loss: 0.13035429641604424, Final Batch Loss: 0.1088944673538208\n",
      "Epoch 4607, Loss: 0.016839356161653996, Final Batch Loss: 0.007954698987305164\n",
      "Epoch 4608, Loss: 0.02300336118787527, Final Batch Loss: 0.012823645025491714\n",
      "Epoch 4609, Loss: 0.02039624284952879, Final Batch Loss: 0.013466662727296352\n",
      "Epoch 4610, Loss: 0.019939785357564688, Final Batch Loss: 0.013317347504198551\n",
      "Epoch 4611, Loss: 0.01004459522664547, Final Batch Loss: 0.005695799365639687\n",
      "Epoch 4612, Loss: 0.00822996674105525, Final Batch Loss: 0.004113702103495598\n",
      "Epoch 4613, Loss: 0.01285489578731358, Final Batch Loss: 0.003114700550213456\n",
      "Epoch 4614, Loss: 0.005878589232452214, Final Batch Loss: 0.0039500463753938675\n",
      "Epoch 4615, Loss: 0.02304841112345457, Final Batch Loss: 0.01260290015488863\n",
      "Epoch 4616, Loss: 0.01521255262196064, Final Batch Loss: 0.005932226777076721\n",
      "Epoch 4617, Loss: 0.03866495564579964, Final Batch Loss: 0.02229258231818676\n",
      "Epoch 4618, Loss: 0.010923508554697037, Final Batch Loss: 0.005729396361857653\n",
      "Epoch 4619, Loss: 0.026390289422124624, Final Batch Loss: 0.0011817174963653088\n",
      "Epoch 4620, Loss: 0.024347394704818726, Final Batch Loss: 0.008604133501648903\n",
      "Epoch 4621, Loss: 0.028944646008312702, Final Batch Loss: 0.020198961719870567\n",
      "Epoch 4622, Loss: 0.011381514370441437, Final Batch Loss: 0.0054466030560433865\n",
      "Epoch 4623, Loss: 0.013604219537228346, Final Batch Loss: 0.007338922005146742\n",
      "Epoch 4624, Loss: 0.019552357960492373, Final Batch Loss: 0.014787660911679268\n",
      "Epoch 4625, Loss: 0.0222587869502604, Final Batch Loss: 0.016762780025601387\n",
      "Epoch 4626, Loss: 0.010591580998152494, Final Batch Loss: 0.004128700587898493\n",
      "Epoch 4627, Loss: 0.032497611828148365, Final Batch Loss: 0.017960287630558014\n",
      "Epoch 4628, Loss: 0.026619411073625088, Final Batch Loss: 0.019021689891815186\n",
      "Epoch 4629, Loss: 0.025467824190855026, Final Batch Loss: 0.0157026220113039\n",
      "Epoch 4630, Loss: 0.0178101840429008, Final Batch Loss: 0.005267438944429159\n",
      "Epoch 4631, Loss: 0.008415857795625925, Final Batch Loss: 0.00435229716822505\n",
      "Epoch 4632, Loss: 0.010376434307545424, Final Batch Loss: 0.007163291797041893\n",
      "Epoch 4633, Loss: 0.0363544262945652, Final Batch Loss: 0.026119958609342575\n",
      "Epoch 4634, Loss: 0.00811313884332776, Final Batch Loss: 0.004015085753053427\n",
      "Epoch 4635, Loss: 0.021798995323479176, Final Batch Loss: 0.016792992129921913\n",
      "Epoch 4636, Loss: 0.015409975778311491, Final Batch Loss: 0.006145039107650518\n",
      "Epoch 4637, Loss: 0.018984182737767696, Final Batch Loss: 0.013007977046072483\n",
      "Epoch 4638, Loss: 0.01278098882175982, Final Batch Loss: 0.010063894093036652\n",
      "Epoch 4639, Loss: 0.02531237993389368, Final Batch Loss: 0.002548736520111561\n",
      "Epoch 4640, Loss: 0.011306568048894405, Final Batch Loss: 0.004668762907385826\n",
      "Epoch 4641, Loss: 0.027589519508183002, Final Batch Loss: 0.004265745170414448\n",
      "Epoch 4642, Loss: 0.005944007076323032, Final Batch Loss: 0.003029861720278859\n",
      "Epoch 4643, Loss: 0.03907032497227192, Final Batch Loss: 0.02755226008594036\n",
      "Epoch 4644, Loss: 0.009220271836966276, Final Batch Loss: 0.005210236180573702\n",
      "Epoch 4645, Loss: 0.05700756283476949, Final Batch Loss: 0.052579786628484726\n",
      "Epoch 4646, Loss: 0.022190450225025415, Final Batch Loss: 0.018332986161112785\n",
      "Epoch 4647, Loss: 0.19476608093827963, Final Batch Loss: 0.19032864272594452\n",
      "Epoch 4648, Loss: 0.04103180393576622, Final Batch Loss: 0.02373393625020981\n",
      "Epoch 4649, Loss: 0.03551889676600695, Final Batch Loss: 0.02203182317316532\n",
      "Epoch 4650, Loss: 0.01759975031018257, Final Batch Loss: 0.0034707318991422653\n",
      "Epoch 4651, Loss: 0.03503429377451539, Final Batch Loss: 0.004438458476215601\n",
      "Epoch 4652, Loss: 0.044298733584582806, Final Batch Loss: 0.037028227001428604\n",
      "Epoch 4653, Loss: 0.01334485737606883, Final Batch Loss: 0.005870832595974207\n",
      "Epoch 4654, Loss: 0.020940945483744144, Final Batch Loss: 0.013858246617019176\n",
      "Epoch 4655, Loss: 0.014907009899616241, Final Batch Loss: 0.006159679032862186\n",
      "Epoch 4656, Loss: 0.014085528440773487, Final Batch Loss: 0.00966742541640997\n",
      "Epoch 4657, Loss: 0.011442524380981922, Final Batch Loss: 0.005188523791730404\n",
      "Epoch 4658, Loss: 0.008503398392349482, Final Batch Loss: 0.0036285798996686935\n",
      "Epoch 4659, Loss: 0.06856848252937198, Final Batch Loss: 0.00466476334258914\n",
      "Epoch 4660, Loss: 0.03417558455839753, Final Batch Loss: 0.006844787392765284\n",
      "Epoch 4661, Loss: 0.06283519975841045, Final Batch Loss: 0.03533950075507164\n",
      "Epoch 4662, Loss: 0.048148677218705416, Final Batch Loss: 0.04557187855243683\n",
      "Epoch 4663, Loss: 0.01561808306723833, Final Batch Loss: 0.004245627671480179\n",
      "Epoch 4664, Loss: 0.04958788026124239, Final Batch Loss: 0.03655417636036873\n",
      "Epoch 4665, Loss: 0.01317979022860527, Final Batch Loss: 0.004280512221157551\n",
      "Epoch 4666, Loss: 0.027421055361628532, Final Batch Loss: 0.009745391085743904\n",
      "Epoch 4667, Loss: 0.008058172417804599, Final Batch Loss: 0.0023522095289081335\n",
      "Epoch 4668, Loss: 0.02417751820757985, Final Batch Loss: 0.02066897414624691\n",
      "Epoch 4669, Loss: 0.01643383945338428, Final Batch Loss: 0.003075748449191451\n",
      "Epoch 4670, Loss: 0.01802585879340768, Final Batch Loss: 0.005049180705100298\n",
      "Epoch 4671, Loss: 0.004455359419807792, Final Batch Loss: 0.0022246562875807285\n",
      "Epoch 4672, Loss: 0.017257623374462128, Final Batch Loss: 0.013087245635688305\n",
      "Epoch 4673, Loss: 0.01239710464142263, Final Batch Loss: 0.003538615768775344\n",
      "Epoch 4674, Loss: 0.020065325079485774, Final Batch Loss: 0.01639535091817379\n",
      "Epoch 4675, Loss: 0.05518085369840264, Final Batch Loss: 0.0022719562985002995\n",
      "Epoch 4676, Loss: 0.017640928272157907, Final Batch Loss: 0.0051728724502027035\n",
      "Epoch 4677, Loss: 0.02127030910924077, Final Batch Loss: 0.017047353088855743\n",
      "Epoch 4678, Loss: 0.06329998932778835, Final Batch Loss: 0.02394923008978367\n",
      "Epoch 4679, Loss: 0.014311294071376324, Final Batch Loss: 0.002364729531109333\n",
      "Epoch 4680, Loss: 0.012893903534859419, Final Batch Loss: 0.002883749548345804\n",
      "Epoch 4681, Loss: 0.029536884278059006, Final Batch Loss: 0.006135644391179085\n",
      "Epoch 4682, Loss: 0.017785660922527313, Final Batch Loss: 0.0054953014478087425\n",
      "Epoch 4683, Loss: 0.05427279695868492, Final Batch Loss: 0.046748775988817215\n",
      "Epoch 4684, Loss: 0.013258031569421291, Final Batch Loss: 0.009246223606169224\n",
      "Epoch 4685, Loss: 0.006794278044253588, Final Batch Loss: 0.0022243745625019073\n",
      "Epoch 4686, Loss: 0.008688588161021471, Final Batch Loss: 0.003955301363021135\n",
      "Epoch 4687, Loss: 0.023558596149086952, Final Batch Loss: 0.017792390659451485\n",
      "Epoch 4688, Loss: 0.010591219877824187, Final Batch Loss: 0.003613770706579089\n",
      "Epoch 4689, Loss: 0.01904390286654234, Final Batch Loss: 0.015060885809361935\n",
      "Epoch 4690, Loss: 0.012263021431863308, Final Batch Loss: 0.007036285940557718\n",
      "Epoch 4691, Loss: 0.017179050482809544, Final Batch Loss: 0.010591677390038967\n",
      "Epoch 4692, Loss: 0.011524585541337729, Final Batch Loss: 0.0045239669270813465\n",
      "Epoch 4693, Loss: 0.009486285503953695, Final Batch Loss: 0.00414765439927578\n",
      "Epoch 4694, Loss: 0.007950609549880028, Final Batch Loss: 0.004818043205887079\n",
      "Epoch 4695, Loss: 0.014606192708015442, Final Batch Loss: 0.007813194766640663\n",
      "Epoch 4696, Loss: 0.028060264885425568, Final Batch Loss: 0.014055185951292515\n",
      "Epoch 4697, Loss: 0.019628011621534824, Final Batch Loss: 0.01037744153290987\n",
      "Epoch 4698, Loss: 0.009735925123095512, Final Batch Loss: 0.0020887134596705437\n",
      "Epoch 4699, Loss: 0.01861476991325617, Final Batch Loss: 0.004958009347319603\n",
      "Epoch 4700, Loss: 0.0057506231823936105, Final Batch Loss: 0.0019117380725219846\n",
      "Epoch 4701, Loss: 0.017709597945213318, Final Batch Loss: 0.010772905312478542\n",
      "Epoch 4702, Loss: 0.01617079460993409, Final Batch Loss: 0.011009035632014275\n",
      "Epoch 4703, Loss: 0.009766247356310487, Final Batch Loss: 0.006165876053273678\n",
      "Epoch 4704, Loss: 0.012636480387300253, Final Batch Loss: 0.007737751584500074\n",
      "Epoch 4705, Loss: 0.011288032867014408, Final Batch Loss: 0.003272091969847679\n",
      "Epoch 4706, Loss: 0.01197345182299614, Final Batch Loss: 0.007067419122904539\n",
      "Epoch 4707, Loss: 0.010833185631781816, Final Batch Loss: 0.00628614379093051\n",
      "Epoch 4708, Loss: 0.04076974280178547, Final Batch Loss: 0.03278085216879845\n",
      "Epoch 4709, Loss: 0.012317617889493704, Final Batch Loss: 0.008251870982348919\n",
      "Epoch 4710, Loss: 0.05551888747140765, Final Batch Loss: 0.04995511472225189\n",
      "Epoch 4711, Loss: 0.008933020988479257, Final Batch Loss: 0.005171550903469324\n",
      "Epoch 4712, Loss: 0.01613679062575102, Final Batch Loss: 0.00560977216809988\n",
      "Epoch 4713, Loss: 0.007786653935909271, Final Batch Loss: 0.004357385914772749\n",
      "Epoch 4714, Loss: 0.01588080171495676, Final Batch Loss: 0.008511468768119812\n",
      "Epoch 4715, Loss: 0.010358983185142279, Final Batch Loss: 0.004296071827411652\n",
      "Epoch 4716, Loss: 0.016722741536796093, Final Batch Loss: 0.006297542713582516\n",
      "Epoch 4717, Loss: 0.013391571585088968, Final Batch Loss: 0.00994732603430748\n",
      "Epoch 4718, Loss: 0.04283367097377777, Final Batch Loss: 0.0326293520629406\n",
      "Epoch 4719, Loss: 0.009359793737530708, Final Batch Loss: 0.00521364901214838\n",
      "Epoch 4720, Loss: 0.00970543478615582, Final Batch Loss: 0.0027517506387084723\n",
      "Epoch 4721, Loss: 0.02449803054332733, Final Batch Loss: 0.014536953531205654\n",
      "Epoch 4722, Loss: 0.009828176349401474, Final Batch Loss: 0.004745522513985634\n",
      "Epoch 4723, Loss: 0.016860913950949907, Final Batch Loss: 0.0027172728441655636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4724, Loss: 0.00926003884524107, Final Batch Loss: 0.0021145115606486797\n",
      "Epoch 4725, Loss: 0.022271830588579178, Final Batch Loss: 0.010880975052714348\n",
      "Epoch 4726, Loss: 0.014961233828216791, Final Batch Loss: 0.0058098421432077885\n",
      "Epoch 4727, Loss: 0.01457366906106472, Final Batch Loss: 0.011931739747524261\n",
      "Epoch 4728, Loss: 0.023333203047513962, Final Batch Loss: 0.014307922683656216\n",
      "Epoch 4729, Loss: 0.01162543031387031, Final Batch Loss: 0.0018365930300205946\n",
      "Epoch 4730, Loss: 0.02502665831707418, Final Batch Loss: 0.0029797626193612814\n",
      "Epoch 4731, Loss: 0.013108357321470976, Final Batch Loss: 0.004756383132189512\n",
      "Epoch 4732, Loss: 0.02532186033204198, Final Batch Loss: 0.004027674440294504\n",
      "Epoch 4733, Loss: 0.02167784608900547, Final Batch Loss: 0.004989545792341232\n",
      "Epoch 4734, Loss: 0.01006748341023922, Final Batch Loss: 0.003110571764409542\n",
      "Epoch 4735, Loss: 0.009711305378004909, Final Batch Loss: 0.002151738153770566\n",
      "Epoch 4736, Loss: 0.007200982654467225, Final Batch Loss: 0.003103989874944091\n",
      "Epoch 4737, Loss: 0.01710955798625946, Final Batch Loss: 0.00793955847620964\n",
      "Epoch 4738, Loss: 0.013299835612997413, Final Batch Loss: 0.010366460308432579\n",
      "Epoch 4739, Loss: 0.027720820857211947, Final Batch Loss: 0.002394686220213771\n",
      "Epoch 4740, Loss: 0.025225417455658317, Final Batch Loss: 0.0034584698732942343\n",
      "Epoch 4741, Loss: 0.011860027443617582, Final Batch Loss: 0.0033363462425768375\n",
      "Epoch 4742, Loss: 0.037104046903550625, Final Batch Loss: 0.032317012548446655\n",
      "Epoch 4743, Loss: 0.008544980082660913, Final Batch Loss: 0.0038641439750790596\n",
      "Epoch 4744, Loss: 0.012013284023851156, Final Batch Loss: 0.006035704165697098\n",
      "Epoch 4745, Loss: 0.010394168552011251, Final Batch Loss: 0.007854250259697437\n",
      "Epoch 4746, Loss: 0.010739982826635242, Final Batch Loss: 0.0030620174948126078\n",
      "Epoch 4747, Loss: 0.012804445810616016, Final Batch Loss: 0.00733292568475008\n",
      "Epoch 4748, Loss: 0.011709772516041994, Final Batch Loss: 0.006842104718089104\n",
      "Epoch 4749, Loss: 0.050750247202813625, Final Batch Loss: 0.003012760542333126\n",
      "Epoch 4750, Loss: 0.007595906499773264, Final Batch Loss: 0.003781180363148451\n",
      "Epoch 4751, Loss: 0.008476373041048646, Final Batch Loss: 0.0063072191551327705\n",
      "Epoch 4752, Loss: 0.009169992292299867, Final Batch Loss: 0.0016121088992804289\n",
      "Epoch 4753, Loss: 0.012649862561374903, Final Batch Loss: 0.0029709958471357822\n",
      "Epoch 4754, Loss: 0.013746652752161026, Final Batch Loss: 0.006577834952622652\n",
      "Epoch 4755, Loss: 0.017100530210882425, Final Batch Loss: 0.009583343751728535\n",
      "Epoch 4756, Loss: 0.018229966517537832, Final Batch Loss: 0.006022296380251646\n",
      "Epoch 4757, Loss: 0.00968401413410902, Final Batch Loss: 0.00450022192671895\n",
      "Epoch 4758, Loss: 0.004631204530596733, Final Batch Loss: 0.0019650759641081095\n",
      "Epoch 4759, Loss: 0.015081917634233832, Final Batch Loss: 0.003856208873912692\n",
      "Epoch 4760, Loss: 0.016567850951105356, Final Batch Loss: 0.012636854313313961\n",
      "Epoch 4761, Loss: 0.003952697617933154, Final Batch Loss: 0.001530122710391879\n",
      "Epoch 4762, Loss: 0.03858689870685339, Final Batch Loss: 0.03657061606645584\n",
      "Epoch 4763, Loss: 0.023089286871254444, Final Batch Loss: 0.009509692899882793\n",
      "Epoch 4764, Loss: 0.009471640922129154, Final Batch Loss: 0.003943556919693947\n",
      "Epoch 4765, Loss: 0.009815045166760683, Final Batch Loss: 0.0046631633304059505\n",
      "Epoch 4766, Loss: 0.013036227319389582, Final Batch Loss: 0.004985752981156111\n",
      "Epoch 4767, Loss: 0.009174383827485144, Final Batch Loss: 0.007306108716875315\n",
      "Epoch 4768, Loss: 0.03601687296759337, Final Batch Loss: 0.03441201150417328\n",
      "Epoch 4769, Loss: 0.02160790143534541, Final Batch Loss: 0.005182241555303335\n",
      "Epoch 4770, Loss: 0.009942935314029455, Final Batch Loss: 0.0025991997681558132\n",
      "Epoch 4771, Loss: 0.013950979337096214, Final Batch Loss: 0.004012332297861576\n",
      "Epoch 4772, Loss: 0.024064538069069386, Final Batch Loss: 0.019347572699189186\n",
      "Epoch 4773, Loss: 0.010731118731200695, Final Batch Loss: 0.004978543147444725\n",
      "Epoch 4774, Loss: 0.017874944489449263, Final Batch Loss: 0.015153513289988041\n",
      "Epoch 4775, Loss: 0.0048812448512762785, Final Batch Loss: 0.002748604631051421\n",
      "Epoch 4776, Loss: 0.007479783846065402, Final Batch Loss: 0.005305098369717598\n",
      "Epoch 4777, Loss: 0.03292015753686428, Final Batch Loss: 0.027670616284012794\n",
      "Epoch 4778, Loss: 0.004428525804542005, Final Batch Loss: 0.001138188992626965\n",
      "Epoch 4779, Loss: 0.03777459915727377, Final Batch Loss: 0.004469222389161587\n",
      "Epoch 4780, Loss: 0.0176455220207572, Final Batch Loss: 0.007681920193135738\n",
      "Epoch 4781, Loss: 0.013610264984890819, Final Batch Loss: 0.0037741863634437323\n",
      "Epoch 4782, Loss: 0.018579110503196716, Final Batch Loss: 0.00988941639661789\n",
      "Epoch 4783, Loss: 0.019289751537144184, Final Batch Loss: 0.005326918326318264\n",
      "Epoch 4784, Loss: 0.008148299297317863, Final Batch Loss: 0.0058905938640236855\n",
      "Epoch 4785, Loss: 0.004028272582218051, Final Batch Loss: 0.0011101560667157173\n",
      "Epoch 4786, Loss: 0.03359730262309313, Final Batch Loss: 0.008856668137013912\n",
      "Epoch 4787, Loss: 0.010477542411535978, Final Batch Loss: 0.005984021350741386\n",
      "Epoch 4788, Loss: 0.012414568336680532, Final Batch Loss: 0.008629878051578999\n",
      "Epoch 4789, Loss: 0.007247635163366795, Final Batch Loss: 0.005253884941339493\n",
      "Epoch 4790, Loss: 0.008329069940373302, Final Batch Loss: 0.002008124953135848\n",
      "Epoch 4791, Loss: 0.012982621788978577, Final Batch Loss: 0.007272148039191961\n",
      "Epoch 4792, Loss: 0.022461155662313104, Final Batch Loss: 0.0026759898755699396\n",
      "Epoch 4793, Loss: 0.016371422912925482, Final Batch Loss: 0.006405377294868231\n",
      "Epoch 4794, Loss: 0.03199863899499178, Final Batch Loss: 0.018486039713025093\n",
      "Epoch 4795, Loss: 0.005425872164778411, Final Batch Loss: 0.0035463287495076656\n",
      "Epoch 4796, Loss: 0.010998395271599293, Final Batch Loss: 0.005189978051930666\n",
      "Epoch 4797, Loss: 0.008619595319032669, Final Batch Loss: 0.00398040609434247\n",
      "Epoch 4798, Loss: 0.02795940451323986, Final Batch Loss: 0.015366528183221817\n",
      "Epoch 4799, Loss: 0.007881155121140182, Final Batch Loss: 0.0011415606131777167\n",
      "Epoch 4800, Loss: 0.021645894274115562, Final Batch Loss: 0.004486164078116417\n",
      "Epoch 4801, Loss: 0.006564607028849423, Final Batch Loss: 0.0017446071142330766\n",
      "Epoch 4802, Loss: 0.01152002380695194, Final Batch Loss: 0.009898414835333824\n",
      "Epoch 4803, Loss: 0.023178827483206987, Final Batch Loss: 0.017764830961823463\n",
      "Epoch 4804, Loss: 0.06052360660396516, Final Batch Loss: 0.05669707432389259\n",
      "Epoch 4805, Loss: 0.01064955536276102, Final Batch Loss: 0.0037338361144065857\n",
      "Epoch 4806, Loss: 0.03043440030887723, Final Batch Loss: 0.024867305532097816\n",
      "Epoch 4807, Loss: 0.003890046151354909, Final Batch Loss: 0.0013396688736975193\n",
      "Epoch 4808, Loss: 0.009596066782251, Final Batch Loss: 0.003440974513068795\n",
      "Epoch 4809, Loss: 0.05913012474775314, Final Batch Loss: 0.04878726974129677\n",
      "Epoch 4810, Loss: 0.021386335603892803, Final Batch Loss: 0.012125793844461441\n",
      "Epoch 4811, Loss: 0.007162678753957152, Final Batch Loss: 0.003770011244341731\n",
      "Epoch 4812, Loss: 0.007373047643341124, Final Batch Loss: 0.005880163051187992\n",
      "Epoch 4813, Loss: 0.029019547859206796, Final Batch Loss: 0.0035865327809005976\n",
      "Epoch 4814, Loss: 0.0073362733237445354, Final Batch Loss: 0.0056465002708137035\n",
      "Epoch 4815, Loss: 0.03515619202516973, Final Batch Loss: 0.03173968940973282\n",
      "Epoch 4816, Loss: 0.006486975587904453, Final Batch Loss: 0.002793763531371951\n",
      "Epoch 4817, Loss: 0.009160691173747182, Final Batch Loss: 0.008046068251132965\n",
      "Epoch 4818, Loss: 0.004947717767208815, Final Batch Loss: 0.00222240318544209\n",
      "Epoch 4819, Loss: 0.05291859805583954, Final Batch Loss: 0.03081378899514675\n",
      "Epoch 4820, Loss: 0.013845476321876049, Final Batch Loss: 0.00639620516449213\n",
      "Epoch 4821, Loss: 0.013986502774059772, Final Batch Loss: 0.003961456939578056\n",
      "Epoch 4822, Loss: 0.0259716734290123, Final Batch Loss: 0.0055198073387146\n",
      "Epoch 4823, Loss: 0.006450667744502425, Final Batch Loss: 0.0012461065780371428\n",
      "Epoch 4824, Loss: 0.019702255725860596, Final Batch Loss: 0.008463094010949135\n",
      "Epoch 4825, Loss: 0.027378960512578487, Final Batch Loss: 0.004469799809157848\n",
      "Epoch 4826, Loss: 0.01610733661800623, Final Batch Loss: 0.014006499201059341\n",
      "Epoch 4827, Loss: 0.005128288990817964, Final Batch Loss: 0.001447012065909803\n",
      "Epoch 4828, Loss: 0.021903122309595346, Final Batch Loss: 0.016927357763051987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4829, Loss: 0.032754594925791025, Final Batch Loss: 0.00634810933843255\n",
      "Epoch 4830, Loss: 0.03000161936506629, Final Batch Loss: 0.024787675589323044\n",
      "Epoch 4831, Loss: 0.009775524027645588, Final Batch Loss: 0.0026753144338726997\n",
      "Epoch 4832, Loss: 0.015904540196061134, Final Batch Loss: 0.011646704748272896\n",
      "Epoch 4833, Loss: 0.03407009597867727, Final Batch Loss: 0.02818324975669384\n",
      "Epoch 4834, Loss: 0.0490814158692956, Final Batch Loss: 0.04189791902899742\n",
      "Epoch 4835, Loss: 0.006810522871091962, Final Batch Loss: 0.0019774402026087046\n",
      "Epoch 4836, Loss: 0.009872434195131063, Final Batch Loss: 0.005830601789057255\n",
      "Epoch 4837, Loss: 0.021951002068817616, Final Batch Loss: 0.010725239291787148\n",
      "Epoch 4838, Loss: 0.007693257764913142, Final Batch Loss: 0.001651021302677691\n",
      "Epoch 4839, Loss: 0.05101561173796654, Final Batch Loss: 0.015923328697681427\n",
      "Epoch 4840, Loss: 0.009080541785806417, Final Batch Loss: 0.0023400085046887398\n",
      "Epoch 4841, Loss: 0.053442378994077444, Final Batch Loss: 0.047863855957984924\n",
      "Epoch 4842, Loss: 0.02181454934179783, Final Batch Loss: 0.013100486248731613\n",
      "Epoch 4843, Loss: 0.022415618412196636, Final Batch Loss: 0.0015732673928141594\n",
      "Epoch 4844, Loss: 0.012209234526380897, Final Batch Loss: 0.001551579451188445\n",
      "Epoch 4845, Loss: 0.005399487272370607, Final Batch Loss: 0.0007178679225035012\n",
      "Epoch 4846, Loss: 0.02238158928230405, Final Batch Loss: 0.0009963703341782093\n",
      "Epoch 4847, Loss: 0.013704115059226751, Final Batch Loss: 0.008783883415162563\n",
      "Epoch 4848, Loss: 0.04102740762755275, Final Batch Loss: 0.036091338843107224\n",
      "Epoch 4849, Loss: 0.023926500231027603, Final Batch Loss: 0.012619933113455772\n",
      "Epoch 4850, Loss: 0.008682579267770052, Final Batch Loss: 0.002523395698517561\n",
      "Epoch 4851, Loss: 0.01698947884142399, Final Batch Loss: 0.011257180012762547\n",
      "Epoch 4852, Loss: 0.015050084330141544, Final Batch Loss: 0.009318679571151733\n",
      "Epoch 4853, Loss: 0.030062632635235786, Final Batch Loss: 0.025137007236480713\n",
      "Epoch 4854, Loss: 0.012956250924617052, Final Batch Loss: 0.006938462145626545\n",
      "Epoch 4855, Loss: 0.009540485218167305, Final Batch Loss: 0.0030109588988125324\n",
      "Epoch 4856, Loss: 0.05125895142555237, Final Batch Loss: 0.036752890795469284\n",
      "Epoch 4857, Loss: 0.023893559351563454, Final Batch Loss: 0.012013106606900692\n",
      "Epoch 4858, Loss: 0.03876207140274346, Final Batch Loss: 0.0031156663317233324\n",
      "Epoch 4859, Loss: 0.015263107139617205, Final Batch Loss: 0.0035306750796735287\n",
      "Epoch 4860, Loss: 0.051209104247391224, Final Batch Loss: 0.04683081805706024\n",
      "Epoch 4861, Loss: 0.01751428423449397, Final Batch Loss: 0.00649248855188489\n",
      "Epoch 4862, Loss: 0.021543003618717194, Final Batch Loss: 0.008324855007231236\n",
      "Epoch 4863, Loss: 0.022162489127367735, Final Batch Loss: 0.004035854246467352\n",
      "Epoch 4864, Loss: 0.02979140728712082, Final Batch Loss: 0.013610653579235077\n",
      "Epoch 4865, Loss: 0.019418511539697647, Final Batch Loss: 0.01310846209526062\n",
      "Epoch 4866, Loss: 0.02323898859322071, Final Batch Loss: 0.00795093085616827\n",
      "Epoch 4867, Loss: 0.025922450702637434, Final Batch Loss: 0.006612240802496672\n",
      "Epoch 4868, Loss: 0.010794318513944745, Final Batch Loss: 0.0022006670478731394\n",
      "Epoch 4869, Loss: 0.008168199565261602, Final Batch Loss: 0.0030555743724107742\n",
      "Epoch 4870, Loss: 0.0054148908238857985, Final Batch Loss: 0.001147695118561387\n",
      "Epoch 4871, Loss: 0.012495419709011912, Final Batch Loss: 0.0021221924107521772\n",
      "Epoch 4872, Loss: 0.01159920310601592, Final Batch Loss: 0.005008738487958908\n",
      "Epoch 4873, Loss: 0.010565357282757759, Final Batch Loss: 0.007909630425274372\n",
      "Epoch 4874, Loss: 0.038416389375925064, Final Batch Loss: 0.011562608182430267\n",
      "Epoch 4875, Loss: 0.040275465697050095, Final Batch Loss: 0.03187735378742218\n",
      "Epoch 4876, Loss: 0.019207967445254326, Final Batch Loss: 0.0071295322850346565\n",
      "Epoch 4877, Loss: 0.023028146475553513, Final Batch Loss: 0.014426174573600292\n",
      "Epoch 4878, Loss: 0.00747932936064899, Final Batch Loss: 0.0041976370848715305\n",
      "Epoch 4879, Loss: 0.00999612221494317, Final Batch Loss: 0.005754414480179548\n",
      "Epoch 4880, Loss: 0.006595289567485452, Final Batch Loss: 0.0030788015574216843\n",
      "Epoch 4881, Loss: 0.032798418775200844, Final Batch Loss: 0.008372161537408829\n",
      "Epoch 4882, Loss: 0.0059760387521237135, Final Batch Loss: 0.0030831482727080584\n",
      "Epoch 4883, Loss: 0.011233317200094461, Final Batch Loss: 0.004459375515580177\n",
      "Epoch 4884, Loss: 0.008527756435796618, Final Batch Loss: 0.005780884530395269\n",
      "Epoch 4885, Loss: 0.02289361134171486, Final Batch Loss: 0.017916211858391762\n",
      "Epoch 4886, Loss: 0.024775858968496323, Final Batch Loss: 0.017170732840895653\n",
      "Epoch 4887, Loss: 0.012816553469747305, Final Batch Loss: 0.005223431624472141\n",
      "Epoch 4888, Loss: 0.02618550811894238, Final Batch Loss: 0.022666538134217262\n",
      "Epoch 4889, Loss: 0.02621761802583933, Final Batch Loss: 0.009186486713588238\n",
      "Epoch 4890, Loss: 0.014740744372829795, Final Batch Loss: 0.0038454008754342794\n",
      "Epoch 4891, Loss: 0.012046036310493946, Final Batch Loss: 0.008309461176395416\n",
      "Epoch 4892, Loss: 0.03519375203177333, Final Batch Loss: 0.007624415215104818\n",
      "Epoch 4893, Loss: 0.01698116445913911, Final Batch Loss: 0.011847548186779022\n",
      "Epoch 4894, Loss: 0.02707055490463972, Final Batch Loss: 0.014759947545826435\n",
      "Epoch 4895, Loss: 0.011983185075223446, Final Batch Loss: 0.00690254895016551\n",
      "Epoch 4896, Loss: 0.045671205036342144, Final Batch Loss: 0.042584143579006195\n",
      "Epoch 4897, Loss: 0.010932530334684998, Final Batch Loss: 0.0008503525168634951\n",
      "Epoch 4898, Loss: 0.02224581502377987, Final Batch Loss: 0.005281597375869751\n",
      "Epoch 4899, Loss: 0.008670599316246808, Final Batch Loss: 0.001468993374146521\n",
      "Epoch 4900, Loss: 0.008424649247899652, Final Batch Loss: 0.004694974049925804\n",
      "Epoch 4901, Loss: 0.06702519953250885, Final Batch Loss: 0.05861502140760422\n",
      "Epoch 4902, Loss: 0.02699198992922902, Final Batch Loss: 0.007409167010337114\n",
      "Epoch 4903, Loss: 0.007014380767941475, Final Batch Loss: 0.0021350234746932983\n",
      "Epoch 4904, Loss: 0.012348029529675841, Final Batch Loss: 0.009964880533516407\n",
      "Epoch 4905, Loss: 0.03222683910280466, Final Batch Loss: 0.023314369842410088\n",
      "Epoch 4906, Loss: 0.06983320228755474, Final Batch Loss: 0.0639679804444313\n",
      "Epoch 4907, Loss: 0.015816227765753865, Final Batch Loss: 0.01194668747484684\n",
      "Epoch 4908, Loss: 0.0111442010384053, Final Batch Loss: 0.0026434778701514006\n",
      "Epoch 4909, Loss: 0.011263831751421094, Final Batch Loss: 0.002011166187003255\n",
      "Epoch 4910, Loss: 0.013012413401156664, Final Batch Loss: 0.008541013114154339\n",
      "Epoch 4911, Loss: 0.0104881483130157, Final Batch Loss: 0.004694194532930851\n",
      "Epoch 4912, Loss: 0.017349359579384327, Final Batch Loss: 0.01163034699857235\n",
      "Epoch 4913, Loss: 0.01637300569564104, Final Batch Loss: 0.004165862686932087\n",
      "Epoch 4914, Loss: 0.021956742741167545, Final Batch Loss: 0.010915867052972317\n",
      "Epoch 4915, Loss: 0.007770129479467869, Final Batch Loss: 0.0024261106736958027\n",
      "Epoch 4916, Loss: 0.01160235796123743, Final Batch Loss: 0.0048109120689332485\n",
      "Epoch 4917, Loss: 0.009873722679913044, Final Batch Loss: 0.004113283008337021\n",
      "Epoch 4918, Loss: 0.006368545233272016, Final Batch Loss: 0.0016129164723679423\n",
      "Epoch 4919, Loss: 0.005450791330076754, Final Batch Loss: 0.001037752372212708\n",
      "Epoch 4920, Loss: 0.013091853354126215, Final Batch Loss: 0.003152053337544203\n",
      "Epoch 4921, Loss: 0.004091018228791654, Final Batch Loss: 0.0023337991442531347\n",
      "Epoch 4922, Loss: 0.03758945968002081, Final Batch Loss: 0.011157293803989887\n",
      "Epoch 4923, Loss: 0.03449268592521548, Final Batch Loss: 0.00688954209908843\n",
      "Epoch 4924, Loss: 0.012090038973838091, Final Batch Loss: 0.0027399868704378605\n",
      "Epoch 4925, Loss: 0.01985970651730895, Final Batch Loss: 0.006913124118000269\n",
      "Epoch 4926, Loss: 0.007826415239833295, Final Batch Loss: 0.0011231208918616176\n",
      "Epoch 4927, Loss: 0.004426842089742422, Final Batch Loss: 0.0016702646389603615\n",
      "Epoch 4928, Loss: 0.054715899750590324, Final Batch Loss: 0.03619064763188362\n",
      "Epoch 4929, Loss: 0.025186188519001007, Final Batch Loss: 0.01471454743295908\n",
      "Epoch 4930, Loss: 0.009837044402956963, Final Batch Loss: 0.003972433507442474\n",
      "Epoch 4931, Loss: 0.020027257036417723, Final Batch Loss: 0.006745468359440565\n",
      "Epoch 4932, Loss: 0.08653452270664275, Final Batch Loss: 0.0018546583596616983\n",
      "Epoch 4933, Loss: 0.00528540788218379, Final Batch Loss: 0.0025829675141721964\n",
      "Epoch 4934, Loss: 0.011118180584162474, Final Batch Loss: 0.005009944550693035\n",
      "Epoch 4935, Loss: 0.016290587838739157, Final Batch Loss: 0.010848150588572025\n",
      "Epoch 4936, Loss: 0.00815407233312726, Final Batch Loss: 0.004382806830108166\n",
      "Epoch 4937, Loss: 0.02352449344471097, Final Batch Loss: 0.01940198242664337\n",
      "Epoch 4938, Loss: 0.009261891478672624, Final Batch Loss: 0.0018410116899758577\n",
      "Epoch 4939, Loss: 0.014471405651420355, Final Batch Loss: 0.009816247969865799\n",
      "Epoch 4940, Loss: 0.019146902486681938, Final Batch Loss: 0.01218806765973568\n",
      "Epoch 4941, Loss: 0.012631549965590239, Final Batch Loss: 0.004055732395499945\n",
      "Epoch 4942, Loss: 0.010235160356387496, Final Batch Loss: 0.006629624869674444\n",
      "Epoch 4943, Loss: 0.009109283098950982, Final Batch Loss: 0.003633321961387992\n",
      "Epoch 4944, Loss: 0.007591947913169861, Final Batch Loss: 0.005116789136081934\n",
      "Epoch 4945, Loss: 0.17746993713080883, Final Batch Loss: 0.021307600662112236\n",
      "Epoch 4946, Loss: 0.004774571629241109, Final Batch Loss: 0.0022629692684859037\n",
      "Epoch 4947, Loss: 0.010034954873844981, Final Batch Loss: 0.0031397349666804075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4948, Loss: 0.011447322554886341, Final Batch Loss: 0.002754436805844307\n",
      "Epoch 4949, Loss: 0.014116973616182804, Final Batch Loss: 0.004097593016922474\n",
      "Epoch 4950, Loss: 0.006144608370959759, Final Batch Loss: 0.002910233335569501\n",
      "Epoch 4951, Loss: 0.008224709192290902, Final Batch Loss: 0.002956450218334794\n",
      "Epoch 4952, Loss: 0.02235726686194539, Final Batch Loss: 0.017897235229611397\n",
      "Epoch 4953, Loss: 0.015872520860284567, Final Batch Loss: 0.013526161201298237\n",
      "Epoch 4954, Loss: 0.03556338371708989, Final Batch Loss: 0.006914852652698755\n",
      "Epoch 4955, Loss: 0.014867953956127167, Final Batch Loss: 0.011757683008909225\n",
      "Epoch 4956, Loss: 0.00663796765729785, Final Batch Loss: 0.00152506772428751\n",
      "Epoch 4957, Loss: 0.021442533936351538, Final Batch Loss: 0.005959867965430021\n",
      "Epoch 4958, Loss: 0.050656503066420555, Final Batch Loss: 0.04703480377793312\n",
      "Epoch 4959, Loss: 0.006667634355835617, Final Batch Loss: 0.0017267422517761588\n",
      "Epoch 4960, Loss: 0.02182060107588768, Final Batch Loss: 0.012632664293050766\n",
      "Epoch 4961, Loss: 0.008165800711140037, Final Batch Loss: 0.002270371885970235\n",
      "Epoch 4962, Loss: 0.023023326881229877, Final Batch Loss: 0.007163560949265957\n",
      "Epoch 4963, Loss: 0.017476501408964396, Final Batch Loss: 0.013467530719935894\n",
      "Epoch 4964, Loss: 0.010435814037919044, Final Batch Loss: 0.004906876012682915\n",
      "Epoch 4965, Loss: 0.09065993269905448, Final Batch Loss: 0.08312932401895523\n",
      "Epoch 4966, Loss: 0.018342261668294668, Final Batch Loss: 0.012893459759652615\n",
      "Epoch 4967, Loss: 0.02360443538054824, Final Batch Loss: 0.0027211899869143963\n",
      "Epoch 4968, Loss: 0.006899666506797075, Final Batch Loss: 0.003262157551944256\n",
      "Epoch 4969, Loss: 0.010512372944504023, Final Batch Loss: 0.0034863287582993507\n",
      "Epoch 4970, Loss: 0.01925054518505931, Final Batch Loss: 0.005995336454361677\n",
      "Epoch 4971, Loss: 0.0065843225456774235, Final Batch Loss: 0.0033347224816679955\n",
      "Epoch 4972, Loss: 0.014880596194416285, Final Batch Loss: 0.002955780830234289\n",
      "Epoch 4973, Loss: 0.019051128765568137, Final Batch Loss: 0.0036394575145095587\n",
      "Epoch 4974, Loss: 0.007606593775562942, Final Batch Loss: 0.0010867527453228831\n",
      "Epoch 4975, Loss: 0.03965681418776512, Final Batch Loss: 0.0020067058503627777\n",
      "Epoch 4976, Loss: 0.03627862874418497, Final Batch Loss: 0.03134796395897865\n",
      "Epoch 4977, Loss: 0.028780657798051834, Final Batch Loss: 0.02233930118381977\n",
      "Epoch 4978, Loss: 0.011775124352425337, Final Batch Loss: 0.00697752833366394\n",
      "Epoch 4979, Loss: 0.010333464946597815, Final Batch Loss: 0.0026638885028660297\n",
      "Epoch 4980, Loss: 0.004562426824122667, Final Batch Loss: 0.0021480221766978502\n",
      "Epoch 4981, Loss: 0.00840634573251009, Final Batch Loss: 0.004058797378093004\n",
      "Epoch 4982, Loss: 0.021130916196852922, Final Batch Loss: 0.018058188259601593\n",
      "Epoch 4983, Loss: 0.009455219842493534, Final Batch Loss: 0.004660779144614935\n",
      "Epoch 4984, Loss: 0.009840886807069182, Final Batch Loss: 0.0037566914688795805\n",
      "Epoch 4985, Loss: 0.0190539276227355, Final Batch Loss: 0.008894531987607479\n",
      "Epoch 4986, Loss: 0.0214471323415637, Final Batch Loss: 0.01875709928572178\n",
      "Epoch 4987, Loss: 0.01080101914703846, Final Batch Loss: 0.0045518116094172\n",
      "Epoch 4988, Loss: 0.007910387590527534, Final Batch Loss: 0.00423825578764081\n",
      "Epoch 4989, Loss: 0.009283184073865414, Final Batch Loss: 0.005197341088205576\n",
      "Epoch 4990, Loss: 0.014494097325950861, Final Batch Loss: 0.006046121474355459\n",
      "Epoch 4991, Loss: 0.009805305860936642, Final Batch Loss: 0.007014650851488113\n",
      "Epoch 4992, Loss: 0.017505284398794174, Final Batch Loss: 0.004592676647007465\n",
      "Epoch 4993, Loss: 0.009693074272945523, Final Batch Loss: 0.006298333406448364\n",
      "Epoch 4994, Loss: 0.00803336757235229, Final Batch Loss: 0.003408657154068351\n",
      "Epoch 4995, Loss: 0.007413791725412011, Final Batch Loss: 0.004494084976613522\n",
      "Epoch 4996, Loss: 0.0072826428804546595, Final Batch Loss: 0.0050505781546235085\n",
      "Epoch 4997, Loss: 0.012550491373986006, Final Batch Loss: 0.005540374666452408\n",
      "Epoch 4998, Loss: 0.005669250967912376, Final Batch Loss: 0.0007011046400293708\n",
      "Epoch 4999, Loss: 0.030110303312540054, Final Batch Loss: 0.02596231922507286\n",
      "Epoch 5000, Loss: 0.007824805565178394, Final Batch Loss: 0.005364136770367622\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model_fake(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  2  0  0  0  0  4  0  0]\n",
      " [ 0  6  0  0  1  0  0  1  0]\n",
      " [ 0  0  7  0  0  1  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  5  0  0  1  0  0  5]\n",
      " [ 1  4  0  0  0  0  6  0  1]\n",
      " [ 0  0  0  0  1  0  0  9  0]\n",
      " [ 0  0  8  0  0  0  0  0  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.94118   0.72727   0.82051        22\n",
      "           1    0.50000   0.75000   0.60000         8\n",
      "           2    0.35000   0.87500   0.50000         8\n",
      "           3    1.00000   1.00000   1.00000         8\n",
      "           4    0.80000   1.00000   0.88889         8\n",
      "           5    0.50000   0.09091   0.15385        11\n",
      "           6    0.60000   0.50000   0.54545        12\n",
      "           7    0.90000   0.90000   0.90000        10\n",
      "           8    0.45455   0.38462   0.41667        13\n",
      "\n",
      "    accuracy                        0.66000       100\n",
      "   macro avg    0.67175   0.69198   0.64726       100\n",
      "weighted avg    0.69515   0.66000   0.64617       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model_fake(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
