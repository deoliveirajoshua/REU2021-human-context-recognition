{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  \\\n",
       "0                 -0.976623              -0.976353  ...   \n",
       "1                 -0.989046              -0.989038  ...   \n",
       "2                 -0.993552              -0.994122  ...   \n",
       "3                 -0.992407              -0.993142  ...   \n",
       "4                 -0.992378              -0.992542  ...   \n",
       "...                     ...                    ...  ...   \n",
       "7347               0.084878               0.065142  ...   \n",
       "7348               0.098249               0.091791  ...   \n",
       "7349               0.185902               0.170686  ...   \n",
       "7350               0.190360               0.178939  ...   \n",
       "7351               0.022216              -0.073681  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                    -0.998285        1         5  \n",
       "1                    -0.999472        1         5  \n",
       "2                    -0.999807        1         5  \n",
       "3                    -0.999770        1         5  \n",
       "4                    -0.999873        1         5  \n",
       "...                        ...      ...       ...  \n",
       "7347                 -0.584282       30         2  \n",
       "7348                 -0.632536       30         2  \n",
       "7349                 -0.641170       30         2  \n",
       "7350                 -0.663579       30         2  \n",
       "7351                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "# X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([19, 21, 22])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([19, 21, 22])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 19:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 21:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.203650712966919, Final Batch Loss: 1.098635196685791\n",
      "Epoch 2, Loss: 2.1995517015457153, Final Batch Loss: 1.0991970300674438\n",
      "Epoch 3, Loss: 2.197994828224182, Final Batch Loss: 1.0977168083190918\n",
      "Epoch 4, Loss: 2.1956063508987427, Final Batch Loss: 1.098901391029358\n",
      "Epoch 5, Loss: 2.1923266649246216, Final Batch Loss: 1.096808671951294\n",
      "Epoch 6, Loss: 2.189774513244629, Final Batch Loss: 1.0969038009643555\n",
      "Epoch 7, Loss: 2.186023473739624, Final Batch Loss: 1.0918394327163696\n",
      "Epoch 8, Loss: 2.1847052574157715, Final Batch Loss: 1.0935205221176147\n",
      "Epoch 9, Loss: 2.1851553916931152, Final Batch Loss: 1.0967257022857666\n",
      "Epoch 10, Loss: 2.1736141443252563, Final Batch Loss: 1.0863841772079468\n",
      "Epoch 11, Loss: 2.1663808822631836, Final Batch Loss: 1.0792717933654785\n",
      "Epoch 12, Loss: 2.158864140510559, Final Batch Loss: 1.0712966918945312\n",
      "Epoch 13, Loss: 2.146217107772827, Final Batch Loss: 1.0651086568832397\n",
      "Epoch 14, Loss: 2.149187684059143, Final Batch Loss: 1.063855528831482\n",
      "Epoch 15, Loss: 2.148587703704834, Final Batch Loss: 1.0710862874984741\n",
      "Epoch 16, Loss: 2.1480709314346313, Final Batch Loss: 1.0891233682632446\n",
      "Epoch 17, Loss: 2.1412458419799805, Final Batch Loss: 1.0719385147094727\n",
      "Epoch 18, Loss: 2.1176282167434692, Final Batch Loss: 1.0547035932540894\n",
      "Epoch 19, Loss: 2.1334590911865234, Final Batch Loss: 1.0875452756881714\n",
      "Epoch 20, Loss: 2.111746907234192, Final Batch Loss: 1.0696133375167847\n",
      "Epoch 21, Loss: 2.107575535774231, Final Batch Loss: 1.05317223072052\n",
      "Epoch 22, Loss: 2.0987008810043335, Final Batch Loss: 1.049263596534729\n",
      "Epoch 23, Loss: 2.081001400947571, Final Batch Loss: 1.0189735889434814\n",
      "Epoch 24, Loss: 2.0785902738571167, Final Batch Loss: 1.054435133934021\n",
      "Epoch 25, Loss: 2.076468586921692, Final Batch Loss: 1.0400727987289429\n",
      "Epoch 26, Loss: 2.0507678985595703, Final Batch Loss: 1.0205634832382202\n",
      "Epoch 27, Loss: 2.0452632904052734, Final Batch Loss: 1.0109009742736816\n",
      "Epoch 28, Loss: 2.0496588945388794, Final Batch Loss: 1.0367435216903687\n",
      "Epoch 29, Loss: 2.0348886251449585, Final Batch Loss: 1.0181702375411987\n",
      "Epoch 30, Loss: 2.019848585128784, Final Batch Loss: 1.0046751499176025\n",
      "Epoch 31, Loss: 1.9719963669776917, Final Batch Loss: 0.995819091796875\n",
      "Epoch 32, Loss: 1.9784232378005981, Final Batch Loss: 0.9783320426940918\n",
      "Epoch 33, Loss: 1.9505035281181335, Final Batch Loss: 0.9950931072235107\n",
      "Epoch 34, Loss: 1.9384546279907227, Final Batch Loss: 0.9838509559631348\n",
      "Epoch 35, Loss: 1.922108769416809, Final Batch Loss: 0.9410204887390137\n",
      "Epoch 36, Loss: 1.9001567363739014, Final Batch Loss: 0.9278278350830078\n",
      "Epoch 37, Loss: 1.8820961713790894, Final Batch Loss: 0.9289474487304688\n",
      "Epoch 38, Loss: 1.8651158213615417, Final Batch Loss: 0.9192622303962708\n",
      "Epoch 39, Loss: 1.8012771010398865, Final Batch Loss: 0.8947935700416565\n",
      "Epoch 40, Loss: 1.7897183299064636, Final Batch Loss: 0.8963066935539246\n",
      "Epoch 41, Loss: 1.7678577899932861, Final Batch Loss: 0.9188953042030334\n",
      "Epoch 42, Loss: 1.7305078506469727, Final Batch Loss: 0.8538308143615723\n",
      "Epoch 43, Loss: 1.6685143113136292, Final Batch Loss: 0.8168917894363403\n",
      "Epoch 44, Loss: 1.6207691431045532, Final Batch Loss: 0.7685254216194153\n",
      "Epoch 45, Loss: 1.6273441314697266, Final Batch Loss: 0.8411049246788025\n",
      "Epoch 46, Loss: 1.593936800956726, Final Batch Loss: 0.8172141313552856\n",
      "Epoch 47, Loss: 1.5530484914779663, Final Batch Loss: 0.7663640379905701\n",
      "Epoch 48, Loss: 1.508026897907257, Final Batch Loss: 0.7192863821983337\n",
      "Epoch 49, Loss: 1.483785331249237, Final Batch Loss: 0.7530144453048706\n",
      "Epoch 50, Loss: 1.4815662503242493, Final Batch Loss: 0.756389856338501\n",
      "Epoch 51, Loss: 1.3999468684196472, Final Batch Loss: 0.717257559299469\n",
      "Epoch 52, Loss: 1.379535734653473, Final Batch Loss: 0.6876665353775024\n",
      "Epoch 53, Loss: 1.326591968536377, Final Batch Loss: 0.6422900557518005\n",
      "Epoch 54, Loss: 1.3048138618469238, Final Batch Loss: 0.6435805559158325\n",
      "Epoch 55, Loss: 1.2993156909942627, Final Batch Loss: 0.6722862124443054\n",
      "Epoch 56, Loss: 1.2699548602104187, Final Batch Loss: 0.6358314752578735\n",
      "Epoch 57, Loss: 1.2431530952453613, Final Batch Loss: 0.6678860783576965\n",
      "Epoch 58, Loss: 1.204637587070465, Final Batch Loss: 0.5728205442428589\n",
      "Epoch 59, Loss: 1.1952085494995117, Final Batch Loss: 0.5924729108810425\n",
      "Epoch 60, Loss: 1.1767412424087524, Final Batch Loss: 0.6056334972381592\n",
      "Epoch 61, Loss: 1.154693365097046, Final Batch Loss: 0.5754982829093933\n",
      "Epoch 62, Loss: 1.1430221796035767, Final Batch Loss: 0.5932618379592896\n",
      "Epoch 63, Loss: 1.1640778183937073, Final Batch Loss: 0.5587648153305054\n",
      "Epoch 64, Loss: 1.1091194152832031, Final Batch Loss: 0.5557580590248108\n",
      "Epoch 65, Loss: 1.0842249393463135, Final Batch Loss: 0.553554117679596\n",
      "Epoch 66, Loss: 1.079304277896881, Final Batch Loss: 0.5274876356124878\n",
      "Epoch 67, Loss: 1.1003589630126953, Final Batch Loss: 0.5664270520210266\n",
      "Epoch 68, Loss: 1.0754361152648926, Final Batch Loss: 0.5184986591339111\n",
      "Epoch 69, Loss: 1.0163653492927551, Final Batch Loss: 0.49559497833251953\n",
      "Epoch 70, Loss: 0.9759777188301086, Final Batch Loss: 0.4604853391647339\n",
      "Epoch 71, Loss: 1.0161921679973602, Final Batch Loss: 0.5355390310287476\n",
      "Epoch 72, Loss: 0.98253533244133, Final Batch Loss: 0.47894176840782166\n",
      "Epoch 73, Loss: 0.9564687609672546, Final Batch Loss: 0.47850069403648376\n",
      "Epoch 74, Loss: 0.9516266584396362, Final Batch Loss: 0.46799543499946594\n",
      "Epoch 75, Loss: 0.9769212603569031, Final Batch Loss: 0.48703232407569885\n",
      "Epoch 76, Loss: 0.9808755815029144, Final Batch Loss: 0.5333508253097534\n",
      "Epoch 77, Loss: 0.9584579169750214, Final Batch Loss: 0.5053948760032654\n",
      "Epoch 78, Loss: 0.9292092323303223, Final Batch Loss: 0.45649558305740356\n",
      "Epoch 79, Loss: 0.879597008228302, Final Batch Loss: 0.4143274128437042\n",
      "Epoch 80, Loss: 0.8975987434387207, Final Batch Loss: 0.46137773990631104\n",
      "Epoch 81, Loss: 0.8701837360858917, Final Batch Loss: 0.4101978838443756\n",
      "Epoch 82, Loss: 0.8216388523578644, Final Batch Loss: 0.3814067542552948\n",
      "Epoch 83, Loss: 0.8277332484722137, Final Batch Loss: 0.38452205061912537\n",
      "Epoch 84, Loss: 0.8543499708175659, Final Batch Loss: 0.4121970534324646\n",
      "Epoch 85, Loss: 0.837831050157547, Final Batch Loss: 0.3639114797115326\n",
      "Epoch 86, Loss: 0.8807103037834167, Final Batch Loss: 0.4267566502094269\n",
      "Epoch 87, Loss: 0.8545842468738556, Final Batch Loss: 0.4327118694782257\n",
      "Epoch 88, Loss: 0.8184886872768402, Final Batch Loss: 0.42214685678482056\n",
      "Epoch 89, Loss: 0.8117668032646179, Final Batch Loss: 0.39963260293006897\n",
      "Epoch 90, Loss: 0.8330056071281433, Final Batch Loss: 0.4017103314399719\n",
      "Epoch 91, Loss: 0.7914441823959351, Final Batch Loss: 0.34325724840164185\n",
      "Epoch 92, Loss: 0.8307330012321472, Final Batch Loss: 0.4497562050819397\n",
      "Epoch 93, Loss: 0.8107202053070068, Final Batch Loss: 0.3985755145549774\n",
      "Epoch 94, Loss: 0.788128674030304, Final Batch Loss: 0.3805820941925049\n",
      "Epoch 95, Loss: 0.8108089566230774, Final Batch Loss: 0.37912553548812866\n",
      "Epoch 96, Loss: 0.7782352268695831, Final Batch Loss: 0.37800320982933044\n",
      "Epoch 97, Loss: 0.765862762928009, Final Batch Loss: 0.37119626998901367\n",
      "Epoch 98, Loss: 0.8076562285423279, Final Batch Loss: 0.47121503949165344\n",
      "Epoch 99, Loss: 0.7934229075908661, Final Batch Loss: 0.3539603650569916\n",
      "Epoch 100, Loss: 0.7687624394893646, Final Batch Loss: 0.4050331115722656\n",
      "Epoch 101, Loss: 0.6921760439872742, Final Batch Loss: 0.3045675754547119\n",
      "Epoch 102, Loss: 0.8041689991950989, Final Batch Loss: 0.3940053880214691\n",
      "Epoch 103, Loss: 0.7441767454147339, Final Batch Loss: 0.3477807939052582\n",
      "Epoch 104, Loss: 0.7949541807174683, Final Batch Loss: 0.426986426115036\n",
      "Epoch 105, Loss: 0.7768064737319946, Final Batch Loss: 0.4450051784515381\n",
      "Epoch 106, Loss: 0.7297365963459015, Final Batch Loss: 0.37744566798210144\n",
      "Epoch 107, Loss: 0.741205245256424, Final Batch Loss: 0.3741929233074188\n",
      "Epoch 108, Loss: 0.7506005465984344, Final Batch Loss: 0.33840641379356384\n",
      "Epoch 109, Loss: 0.7142401039600372, Final Batch Loss: 0.31538793444633484\n",
      "Epoch 110, Loss: 0.7455796301364899, Final Batch Loss: 0.3755773603916168\n",
      "Epoch 111, Loss: 0.7172533571720123, Final Batch Loss: 0.3345494866371155\n",
      "Epoch 112, Loss: 0.7224218845367432, Final Batch Loss: 0.35591983795166016\n",
      "Epoch 113, Loss: 0.714800089597702, Final Batch Loss: 0.37544527649879456\n",
      "Epoch 114, Loss: 0.7269926071166992, Final Batch Loss: 0.3506702780723572\n",
      "Epoch 115, Loss: 0.7222439348697662, Final Batch Loss: 0.3921278417110443\n",
      "Epoch 116, Loss: 0.7015590071678162, Final Batch Loss: 0.34100890159606934\n",
      "Epoch 117, Loss: 0.7058177590370178, Final Batch Loss: 0.3734494745731354\n",
      "Epoch 118, Loss: 0.7129076421260834, Final Batch Loss: 0.34542590379714966\n",
      "Epoch 119, Loss: 0.6791352927684784, Final Batch Loss: 0.308878093957901\n",
      "Epoch 120, Loss: 0.6803737580776215, Final Batch Loss: 0.333017498254776\n",
      "Epoch 121, Loss: 0.7101997137069702, Final Batch Loss: 0.36560025811195374\n",
      "Epoch 122, Loss: 0.6573748588562012, Final Batch Loss: 0.3193805515766144\n",
      "Epoch 123, Loss: 0.6763464510440826, Final Batch Loss: 0.35208532214164734\n",
      "Epoch 124, Loss: 0.6759748458862305, Final Batch Loss: 0.33285367488861084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 0.6636723875999451, Final Batch Loss: 0.26965203881263733\n",
      "Epoch 126, Loss: 0.695173442363739, Final Batch Loss: 0.323687881231308\n",
      "Epoch 127, Loss: 0.6850484311580658, Final Batch Loss: 0.3655449450016022\n",
      "Epoch 128, Loss: 0.6380286514759064, Final Batch Loss: 0.3170183300971985\n",
      "Epoch 129, Loss: 0.6619206964969635, Final Batch Loss: 0.3431174159049988\n",
      "Epoch 130, Loss: 0.6504224538803101, Final Batch Loss: 0.32561448216438293\n",
      "Epoch 131, Loss: 0.5956210494041443, Final Batch Loss: 0.27335333824157715\n",
      "Epoch 132, Loss: 0.6249101459980011, Final Batch Loss: 0.260597288608551\n",
      "Epoch 133, Loss: 0.6193743944168091, Final Batch Loss: 0.3452620506286621\n",
      "Epoch 134, Loss: 0.6511350572109222, Final Batch Loss: 0.33085915446281433\n",
      "Epoch 135, Loss: 0.6394690275192261, Final Batch Loss: 0.3064644932746887\n",
      "Epoch 136, Loss: 0.5944555103778839, Final Batch Loss: 0.3051683306694031\n",
      "Epoch 137, Loss: 0.6385283172130585, Final Batch Loss: 0.3452690541744232\n",
      "Epoch 138, Loss: 0.5976553857326508, Final Batch Loss: 0.3008444011211395\n",
      "Epoch 139, Loss: 0.6138617396354675, Final Batch Loss: 0.35523316264152527\n",
      "Epoch 140, Loss: 0.6083292961120605, Final Batch Loss: 0.316150426864624\n",
      "Epoch 141, Loss: 0.6580617129802704, Final Batch Loss: 0.3464171290397644\n",
      "Epoch 142, Loss: 0.5934199094772339, Final Batch Loss: 0.30899375677108765\n",
      "Epoch 143, Loss: 0.5560203194618225, Final Batch Loss: 0.2599028944969177\n",
      "Epoch 144, Loss: 0.6442455649375916, Final Batch Loss: 0.3035411238670349\n",
      "Epoch 145, Loss: 0.5789309442043304, Final Batch Loss: 0.2407378852367401\n",
      "Epoch 146, Loss: 0.530622810125351, Final Batch Loss: 0.24950528144836426\n",
      "Epoch 147, Loss: 0.5729408264160156, Final Batch Loss: 0.29194697737693787\n",
      "Epoch 148, Loss: 0.6486342251300812, Final Batch Loss: 0.3875180780887604\n",
      "Epoch 149, Loss: 0.5712254792451859, Final Batch Loss: 0.3372212052345276\n",
      "Epoch 150, Loss: 0.5472341924905777, Final Batch Loss: 0.244446262717247\n",
      "Epoch 151, Loss: 0.5756109952926636, Final Batch Loss: 0.30429694056510925\n",
      "Epoch 152, Loss: 0.5258602201938629, Final Batch Loss: 0.25652560591697693\n",
      "Epoch 153, Loss: 0.5593425333499908, Final Batch Loss: 0.27529510855674744\n",
      "Epoch 154, Loss: 0.5798037946224213, Final Batch Loss: 0.3143475651741028\n",
      "Epoch 155, Loss: 0.5384515821933746, Final Batch Loss: 0.2703944444656372\n",
      "Epoch 156, Loss: 0.5202285051345825, Final Batch Loss: 0.23319438099861145\n",
      "Epoch 157, Loss: 0.5560895502567291, Final Batch Loss: 0.23959311842918396\n",
      "Epoch 158, Loss: 0.5597472786903381, Final Batch Loss: 0.30266159772872925\n",
      "Epoch 159, Loss: 0.5584740042686462, Final Batch Loss: 0.2760480046272278\n",
      "Epoch 160, Loss: 0.5279781222343445, Final Batch Loss: 0.28878048062324524\n",
      "Epoch 161, Loss: 0.5775113105773926, Final Batch Loss: 0.29510068893432617\n",
      "Epoch 162, Loss: 0.5434281229972839, Final Batch Loss: 0.28773006796836853\n",
      "Epoch 163, Loss: 0.5210956037044525, Final Batch Loss: 0.2650415003299713\n",
      "Epoch 164, Loss: 0.4891798198223114, Final Batch Loss: 0.2431851625442505\n",
      "Epoch 165, Loss: 0.492282509803772, Final Batch Loss: 0.2396538257598877\n",
      "Epoch 166, Loss: 0.509928897023201, Final Batch Loss: 0.28992024064064026\n",
      "Epoch 167, Loss: 0.5038021057844162, Final Batch Loss: 0.2553060054779053\n",
      "Epoch 168, Loss: 0.5222464799880981, Final Batch Loss: 0.2589077651500702\n",
      "Epoch 169, Loss: 0.5213706195354462, Final Batch Loss: 0.27060192823410034\n",
      "Epoch 170, Loss: 0.46902601420879364, Final Batch Loss: 0.23830820620059967\n",
      "Epoch 171, Loss: 0.4843786060810089, Final Batch Loss: 0.25706347823143005\n",
      "Epoch 172, Loss: 0.48987796902656555, Final Batch Loss: 0.22319796681404114\n",
      "Epoch 173, Loss: 0.5408617854118347, Final Batch Loss: 0.2620278000831604\n",
      "Epoch 174, Loss: 0.4916745275259018, Final Batch Loss: 0.24960598349571228\n",
      "Epoch 175, Loss: 0.5274293720722198, Final Batch Loss: 0.3028520941734314\n",
      "Epoch 176, Loss: 0.5231427997350693, Final Batch Loss: 0.24499507248401642\n",
      "Epoch 177, Loss: 0.49196623265743256, Final Batch Loss: 0.2773621678352356\n",
      "Epoch 178, Loss: 0.4797910004854202, Final Batch Loss: 0.22062425315380096\n",
      "Epoch 179, Loss: 0.44675488770008087, Final Batch Loss: 0.21397331357002258\n",
      "Epoch 180, Loss: 0.5015159845352173, Final Batch Loss: 0.25895699858665466\n",
      "Epoch 181, Loss: 0.49320846796035767, Final Batch Loss: 0.23004743456840515\n",
      "Epoch 182, Loss: 0.4985862523317337, Final Batch Loss: 0.21453629434108734\n",
      "Epoch 183, Loss: 0.5161171108484268, Final Batch Loss: 0.29801031947135925\n",
      "Epoch 184, Loss: 0.4524657726287842, Final Batch Loss: 0.21617338061332703\n",
      "Epoch 185, Loss: 0.4605711102485657, Final Batch Loss: 0.2495952695608139\n",
      "Epoch 186, Loss: 0.44008028507232666, Final Batch Loss: 0.18158462643623352\n",
      "Epoch 187, Loss: 0.46092356741428375, Final Batch Loss: 0.24364903569221497\n",
      "Epoch 188, Loss: 0.4378289729356766, Final Batch Loss: 0.20660094916820526\n",
      "Epoch 189, Loss: 0.4900225102901459, Final Batch Loss: 0.26123106479644775\n",
      "Epoch 190, Loss: 0.45046012103557587, Final Batch Loss: 0.20731739699840546\n",
      "Epoch 191, Loss: 0.47349312901496887, Final Batch Loss: 0.21921241283416748\n",
      "Epoch 192, Loss: 0.45789338648319244, Final Batch Loss: 0.21999770402908325\n",
      "Epoch 193, Loss: 0.4425806552171707, Final Batch Loss: 0.21955478191375732\n",
      "Epoch 194, Loss: 0.4331672191619873, Final Batch Loss: 0.20903180539608002\n",
      "Epoch 195, Loss: 0.46486201882362366, Final Batch Loss: 0.2499721199274063\n",
      "Epoch 196, Loss: 0.45941559970378876, Final Batch Loss: 0.2365046739578247\n",
      "Epoch 197, Loss: 0.4105408936738968, Final Batch Loss: 0.1770911067724228\n",
      "Epoch 198, Loss: 0.4191991537809372, Final Batch Loss: 0.22578372061252594\n",
      "Epoch 199, Loss: 0.4680558294057846, Final Batch Loss: 0.23660524189472198\n",
      "Epoch 200, Loss: 0.4657432436943054, Final Batch Loss: 0.23362299799919128\n",
      "Epoch 201, Loss: 0.45408280193805695, Final Batch Loss: 0.2257014811038971\n",
      "Epoch 202, Loss: 0.481855183839798, Final Batch Loss: 0.2527594566345215\n",
      "Epoch 203, Loss: 0.40684323012828827, Final Batch Loss: 0.21158595383167267\n",
      "Epoch 204, Loss: 0.432858407497406, Final Batch Loss: 0.1962706595659256\n",
      "Epoch 205, Loss: 0.40851233899593353, Final Batch Loss: 0.21726776659488678\n",
      "Epoch 206, Loss: 0.44419847428798676, Final Batch Loss: 0.2403162717819214\n",
      "Epoch 207, Loss: 0.46681825816631317, Final Batch Loss: 0.20590980350971222\n",
      "Epoch 208, Loss: 0.4072139114141464, Final Batch Loss: 0.19836679100990295\n",
      "Epoch 209, Loss: 0.3939005732536316, Final Batch Loss: 0.19554828107357025\n",
      "Epoch 210, Loss: 0.4380336105823517, Final Batch Loss: 0.2604192793369293\n",
      "Epoch 211, Loss: 0.4055042564868927, Final Batch Loss: 0.18909527361392975\n",
      "Epoch 212, Loss: 0.3978559225797653, Final Batch Loss: 0.19562707841396332\n",
      "Epoch 213, Loss: 0.4057497978210449, Final Batch Loss: 0.2139989137649536\n",
      "Epoch 214, Loss: 0.40530985593795776, Final Batch Loss: 0.2229933887720108\n",
      "Epoch 215, Loss: 0.39058613777160645, Final Batch Loss: 0.20379289984703064\n",
      "Epoch 216, Loss: 0.43619395792484283, Final Batch Loss: 0.24711468815803528\n",
      "Epoch 217, Loss: 0.4258756786584854, Final Batch Loss: 0.24279449880123138\n",
      "Epoch 218, Loss: 0.36471307277679443, Final Batch Loss: 0.19055017828941345\n",
      "Epoch 219, Loss: 0.394772008061409, Final Batch Loss: 0.22201746702194214\n",
      "Epoch 220, Loss: 0.43500910699367523, Final Batch Loss: 0.17693357169628143\n",
      "Epoch 221, Loss: 0.37480804324150085, Final Batch Loss: 0.18628419935703278\n",
      "Epoch 222, Loss: 0.37224695086479187, Final Batch Loss: 0.19167231023311615\n",
      "Epoch 223, Loss: 0.39338715374469757, Final Batch Loss: 0.20194926857948303\n",
      "Epoch 224, Loss: 0.38542158901691437, Final Batch Loss: 0.22263237833976746\n",
      "Epoch 225, Loss: 0.3983118087053299, Final Batch Loss: 0.19954422116279602\n",
      "Epoch 226, Loss: 0.36506031453609467, Final Batch Loss: 0.16545523703098297\n",
      "Epoch 227, Loss: 0.34533990919589996, Final Batch Loss: 0.15417003631591797\n",
      "Epoch 228, Loss: 0.3523332476615906, Final Batch Loss: 0.17916591465473175\n",
      "Epoch 229, Loss: 0.3695942759513855, Final Batch Loss: 0.19288873672485352\n",
      "Epoch 230, Loss: 0.38932138681411743, Final Batch Loss: 0.21811650693416595\n",
      "Epoch 231, Loss: 0.3233910948038101, Final Batch Loss: 0.1607784628868103\n",
      "Epoch 232, Loss: 0.37517599761486053, Final Batch Loss: 0.20190230011940002\n",
      "Epoch 233, Loss: 0.3534211665391922, Final Batch Loss: 0.17974045872688293\n",
      "Epoch 234, Loss: 0.378545418381691, Final Batch Loss: 0.2099859118461609\n",
      "Epoch 235, Loss: 0.35422155261039734, Final Batch Loss: 0.15657784044742584\n",
      "Epoch 236, Loss: 0.36943165957927704, Final Batch Loss: 0.14967133104801178\n",
      "Epoch 237, Loss: 0.33474528789520264, Final Batch Loss: 0.15487682819366455\n",
      "Epoch 238, Loss: 0.3418494611978531, Final Batch Loss: 0.14123375713825226\n",
      "Epoch 239, Loss: 0.3136650025844574, Final Batch Loss: 0.15285906195640564\n",
      "Epoch 240, Loss: 0.3478991538286209, Final Batch Loss: 0.16445353627204895\n",
      "Epoch 241, Loss: 0.31427010893821716, Final Batch Loss: 0.1541891098022461\n",
      "Epoch 242, Loss: 0.3338489830493927, Final Batch Loss: 0.1775950789451599\n",
      "Epoch 243, Loss: 0.34278400242328644, Final Batch Loss: 0.17627570033073425\n",
      "Epoch 244, Loss: 0.3209543824195862, Final Batch Loss: 0.1787613481283188\n",
      "Epoch 245, Loss: 0.31478090584278107, Final Batch Loss: 0.1698026955127716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246, Loss: 0.3036787211894989, Final Batch Loss: 0.14503031969070435\n",
      "Epoch 247, Loss: 0.2744866758584976, Final Batch Loss: 0.13934583961963654\n",
      "Epoch 248, Loss: 0.32291530072689056, Final Batch Loss: 0.15174327790737152\n",
      "Epoch 249, Loss: 0.32797396183013916, Final Batch Loss: 0.174992173910141\n",
      "Epoch 250, Loss: 0.380146861076355, Final Batch Loss: 0.22904011607170105\n",
      "Epoch 251, Loss: 0.302761510014534, Final Batch Loss: 0.1340029090642929\n",
      "Epoch 252, Loss: 0.35302576422691345, Final Batch Loss: 0.18215207755565643\n",
      "Epoch 253, Loss: 0.3205518424510956, Final Batch Loss: 0.16527612507343292\n",
      "Epoch 254, Loss: 0.34986943006515503, Final Batch Loss: 0.16123056411743164\n",
      "Epoch 255, Loss: 0.30321116745471954, Final Batch Loss: 0.12675778567790985\n",
      "Epoch 256, Loss: 0.2955593913793564, Final Batch Loss: 0.1325184404850006\n",
      "Epoch 257, Loss: 0.2699531316757202, Final Batch Loss: 0.11065037548542023\n",
      "Epoch 258, Loss: 0.36193135380744934, Final Batch Loss: 0.15778356790542603\n",
      "Epoch 259, Loss: 0.30034904181957245, Final Batch Loss: 0.1424044966697693\n",
      "Epoch 260, Loss: 0.3186577558517456, Final Batch Loss: 0.15634015202522278\n",
      "Epoch 261, Loss: 0.34059539437294006, Final Batch Loss: 0.18224039673805237\n",
      "Epoch 262, Loss: 0.3258630037307739, Final Batch Loss: 0.19683805108070374\n",
      "Epoch 263, Loss: 0.3330301493406296, Final Batch Loss: 0.1675524264574051\n",
      "Epoch 264, Loss: 0.27643983066082, Final Batch Loss: 0.11471953988075256\n",
      "Epoch 265, Loss: 0.34896935522556305, Final Batch Loss: 0.206010103225708\n",
      "Epoch 266, Loss: 0.3018498867750168, Final Batch Loss: 0.146189883351326\n",
      "Epoch 267, Loss: 0.2901255190372467, Final Batch Loss: 0.1274694949388504\n",
      "Epoch 268, Loss: 0.29230672121047974, Final Batch Loss: 0.16149543225765228\n",
      "Epoch 269, Loss: 0.32088595628738403, Final Batch Loss: 0.140089750289917\n",
      "Epoch 270, Loss: 0.277413085103035, Final Batch Loss: 0.14230751991271973\n",
      "Epoch 271, Loss: 0.27059172838926315, Final Batch Loss: 0.14655940234661102\n",
      "Epoch 272, Loss: 0.2952568978071213, Final Batch Loss: 0.16782839596271515\n",
      "Epoch 273, Loss: 0.27455440163612366, Final Batch Loss: 0.13347941637039185\n",
      "Epoch 274, Loss: 0.28949038684368134, Final Batch Loss: 0.15751296281814575\n",
      "Epoch 275, Loss: 0.2881558686494827, Final Batch Loss: 0.1586424559354782\n",
      "Epoch 276, Loss: 0.2989809662103653, Final Batch Loss: 0.13128520548343658\n",
      "Epoch 277, Loss: 0.2874465435743332, Final Batch Loss: 0.13334502279758453\n",
      "Epoch 278, Loss: 0.24429301172494888, Final Batch Loss: 0.13346005976200104\n",
      "Epoch 279, Loss: 0.2950028330087662, Final Batch Loss: 0.14096081256866455\n",
      "Epoch 280, Loss: 0.2946242466568947, Final Batch Loss: 0.16964100301265717\n",
      "Epoch 281, Loss: 0.22986633330583572, Final Batch Loss: 0.10356532782316208\n",
      "Epoch 282, Loss: 0.2540077045559883, Final Batch Loss: 0.10919196158647537\n",
      "Epoch 283, Loss: 0.2546726390719414, Final Batch Loss: 0.1237473115324974\n",
      "Epoch 284, Loss: 0.20062453299760818, Final Batch Loss: 0.08479641377925873\n",
      "Epoch 285, Loss: 0.21641721576452255, Final Batch Loss: 0.08682594448328018\n",
      "Epoch 286, Loss: 0.2239833101630211, Final Batch Loss: 0.1121596172451973\n",
      "Epoch 287, Loss: 0.253068283200264, Final Batch Loss: 0.12712234258651733\n",
      "Epoch 288, Loss: 0.2705977186560631, Final Batch Loss: 0.1636907458305359\n",
      "Epoch 289, Loss: 0.3075732886791229, Final Batch Loss: 0.13836529850959778\n",
      "Epoch 290, Loss: 0.2510231286287308, Final Batch Loss: 0.12796922028064728\n",
      "Epoch 291, Loss: 0.20500991493463516, Final Batch Loss: 0.0998774841427803\n",
      "Epoch 292, Loss: 0.23936699330806732, Final Batch Loss: 0.14290085434913635\n",
      "Epoch 293, Loss: 0.27488966286182404, Final Batch Loss: 0.147455096244812\n",
      "Epoch 294, Loss: 0.26175811886787415, Final Batch Loss: 0.12639838457107544\n",
      "Epoch 295, Loss: 0.20213093608617783, Final Batch Loss: 0.09138050675392151\n",
      "Epoch 296, Loss: 0.27833960950374603, Final Batch Loss: 0.12287671864032745\n",
      "Epoch 297, Loss: 0.2793990299105644, Final Batch Loss: 0.17676492035388947\n",
      "Epoch 298, Loss: 0.25904178619384766, Final Batch Loss: 0.12933801114559174\n",
      "Epoch 299, Loss: 0.18602872639894485, Final Batch Loss: 0.08066064864397049\n",
      "Epoch 300, Loss: 0.24912351369857788, Final Batch Loss: 0.08374427258968353\n",
      "Epoch 301, Loss: 0.23071184009313583, Final Batch Loss: 0.10644033551216125\n",
      "Epoch 302, Loss: 0.25779806077480316, Final Batch Loss: 0.11715136468410492\n",
      "Epoch 303, Loss: 0.2393949031829834, Final Batch Loss: 0.14049604535102844\n",
      "Epoch 304, Loss: 0.24897247552871704, Final Batch Loss: 0.1190892904996872\n",
      "Epoch 305, Loss: 0.24888063222169876, Final Batch Loss: 0.12410354614257812\n",
      "Epoch 306, Loss: 0.21781592816114426, Final Batch Loss: 0.09317553043365479\n",
      "Epoch 307, Loss: 0.24749769270420074, Final Batch Loss: 0.11089539527893066\n",
      "Epoch 308, Loss: 0.22580820322036743, Final Batch Loss: 0.1025020107626915\n",
      "Epoch 309, Loss: 0.2426266223192215, Final Batch Loss: 0.11909177899360657\n",
      "Epoch 310, Loss: 0.2547857314348221, Final Batch Loss: 0.12181724607944489\n",
      "Epoch 311, Loss: 0.21269997209310532, Final Batch Loss: 0.1026720479130745\n",
      "Epoch 312, Loss: 0.2138214260339737, Final Batch Loss: 0.10506850481033325\n",
      "Epoch 313, Loss: 0.24288541078567505, Final Batch Loss: 0.13476310670375824\n",
      "Epoch 314, Loss: 0.22972524166107178, Final Batch Loss: 0.08594562113285065\n",
      "Epoch 315, Loss: 0.20778188109397888, Final Batch Loss: 0.09664874523878098\n",
      "Epoch 316, Loss: 0.24539489299058914, Final Batch Loss: 0.12081047147512436\n",
      "Epoch 317, Loss: 0.20852260291576385, Final Batch Loss: 0.09536481648683548\n",
      "Epoch 318, Loss: 0.19143811613321304, Final Batch Loss: 0.12117774039506912\n",
      "Epoch 319, Loss: 0.18971503525972366, Final Batch Loss: 0.10143837332725525\n",
      "Epoch 320, Loss: 0.1970723494887352, Final Batch Loss: 0.08559739589691162\n",
      "Epoch 321, Loss: 0.21134080737829208, Final Batch Loss: 0.1017250120639801\n",
      "Epoch 322, Loss: 0.19302407652139664, Final Batch Loss: 0.10905762016773224\n",
      "Epoch 323, Loss: 0.22809229791164398, Final Batch Loss: 0.12503471970558167\n",
      "Epoch 324, Loss: 0.20951718091964722, Final Batch Loss: 0.10928089916706085\n",
      "Epoch 325, Loss: 0.2618376538157463, Final Batch Loss: 0.12395403534173965\n",
      "Epoch 326, Loss: 0.2500240206718445, Final Batch Loss: 0.12602654099464417\n",
      "Epoch 327, Loss: 0.1962297260761261, Final Batch Loss: 0.08414515852928162\n",
      "Epoch 328, Loss: 0.188689187169075, Final Batch Loss: 0.10307338088750839\n",
      "Epoch 329, Loss: 0.2094253972172737, Final Batch Loss: 0.12120774388313293\n",
      "Epoch 330, Loss: 0.19739103317260742, Final Batch Loss: 0.1104266420006752\n",
      "Epoch 331, Loss: 0.22714018821716309, Final Batch Loss: 0.1250215321779251\n",
      "Epoch 332, Loss: 0.20068421214818954, Final Batch Loss: 0.10238679498434067\n",
      "Epoch 333, Loss: 0.20602455735206604, Final Batch Loss: 0.09351625293493271\n",
      "Epoch 334, Loss: 0.184735968708992, Final Batch Loss: 0.0957750678062439\n",
      "Epoch 335, Loss: 0.20240936428308487, Final Batch Loss: 0.07469133287668228\n",
      "Epoch 336, Loss: 0.1789315864443779, Final Batch Loss: 0.08365753293037415\n",
      "Epoch 337, Loss: 0.2028736099600792, Final Batch Loss: 0.11978501081466675\n",
      "Epoch 338, Loss: 0.2031574845314026, Final Batch Loss: 0.12151087075471878\n",
      "Epoch 339, Loss: 0.21551474183797836, Final Batch Loss: 0.10091456770896912\n",
      "Epoch 340, Loss: 0.20261027663946152, Final Batch Loss: 0.08266536891460419\n",
      "Epoch 341, Loss: 0.21541908383369446, Final Batch Loss: 0.14780014753341675\n",
      "Epoch 342, Loss: 0.21128841489553452, Final Batch Loss: 0.09427142888307571\n",
      "Epoch 343, Loss: 0.22179248929023743, Final Batch Loss: 0.08612795174121857\n",
      "Epoch 344, Loss: 0.22885826975107193, Final Batch Loss: 0.11854703724384308\n",
      "Epoch 345, Loss: 0.1834830865263939, Final Batch Loss: 0.09145193547010422\n",
      "Epoch 346, Loss: 0.20995234698057175, Final Batch Loss: 0.1255381852388382\n",
      "Epoch 347, Loss: 0.21185459941625595, Final Batch Loss: 0.13484326004981995\n",
      "Epoch 348, Loss: 0.2180239036679268, Final Batch Loss: 0.11927801370620728\n",
      "Epoch 349, Loss: 0.14296679571270943, Final Batch Loss: 0.05932942405343056\n",
      "Epoch 350, Loss: 0.2041194662451744, Final Batch Loss: 0.11795365810394287\n",
      "Epoch 351, Loss: 0.21474508941173553, Final Batch Loss: 0.12099060416221619\n",
      "Epoch 352, Loss: 0.19525005668401718, Final Batch Loss: 0.10078222304582596\n",
      "Epoch 353, Loss: 0.17882130295038223, Final Batch Loss: 0.08751114457845688\n",
      "Epoch 354, Loss: 0.14377136528491974, Final Batch Loss: 0.0623767226934433\n",
      "Epoch 355, Loss: 0.1952233612537384, Final Batch Loss: 0.10251596570014954\n",
      "Epoch 356, Loss: 0.1643928810954094, Final Batch Loss: 0.07513800263404846\n",
      "Epoch 357, Loss: 0.18463987112045288, Final Batch Loss: 0.09988706558942795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358, Loss: 0.17118412256240845, Final Batch Loss: 0.10508573800325394\n",
      "Epoch 359, Loss: 0.1842511221766472, Final Batch Loss: 0.10114932060241699\n",
      "Epoch 360, Loss: 0.17165938019752502, Final Batch Loss: 0.09249299019575119\n",
      "Epoch 361, Loss: 0.18787994235754013, Final Batch Loss: 0.11810919642448425\n",
      "Epoch 362, Loss: 0.21141845732927322, Final Batch Loss: 0.11644821614027023\n",
      "Epoch 363, Loss: 0.16977854818105698, Final Batch Loss: 0.10512085258960724\n",
      "Epoch 364, Loss: 0.18914450705051422, Final Batch Loss: 0.08073025196790695\n",
      "Epoch 365, Loss: 0.17141111195087433, Final Batch Loss: 0.07511822879314423\n",
      "Epoch 366, Loss: 0.18029459565877914, Final Batch Loss: 0.06977824866771698\n",
      "Epoch 367, Loss: 0.1960013285279274, Final Batch Loss: 0.09633100032806396\n",
      "Epoch 368, Loss: 0.19307313859462738, Final Batch Loss: 0.10089005529880524\n",
      "Epoch 369, Loss: 0.17076177150011063, Final Batch Loss: 0.07884930074214935\n",
      "Epoch 370, Loss: 0.1663208156824112, Final Batch Loss: 0.0869346559047699\n",
      "Epoch 371, Loss: 0.1928793266415596, Final Batch Loss: 0.08062992990016937\n",
      "Epoch 372, Loss: 0.18956391885876656, Final Batch Loss: 0.05622342601418495\n",
      "Epoch 373, Loss: 0.1447480395436287, Final Batch Loss: 0.07438980787992477\n",
      "Epoch 374, Loss: 0.2231285348534584, Final Batch Loss: 0.1200445145368576\n",
      "Epoch 375, Loss: 0.20460520684719086, Final Batch Loss: 0.12697221338748932\n",
      "Epoch 376, Loss: 0.17726406827569008, Final Batch Loss: 0.0592096783220768\n",
      "Epoch 377, Loss: 0.19090662896633148, Final Batch Loss: 0.10083866119384766\n",
      "Epoch 378, Loss: 0.18988092988729477, Final Batch Loss: 0.11406087130308151\n",
      "Epoch 379, Loss: 0.16111430525779724, Final Batch Loss: 0.07970358431339264\n",
      "Epoch 380, Loss: 0.18386993557214737, Final Batch Loss: 0.09906798601150513\n",
      "Epoch 381, Loss: 0.14535856992006302, Final Batch Loss: 0.06159255653619766\n",
      "Epoch 382, Loss: 0.18680354952812195, Final Batch Loss: 0.1031847819685936\n",
      "Epoch 383, Loss: 0.2059687003493309, Final Batch Loss: 0.1239074319601059\n",
      "Epoch 384, Loss: 0.14879975467920303, Final Batch Loss: 0.0812082290649414\n",
      "Epoch 385, Loss: 0.18954581767320633, Final Batch Loss: 0.11267729848623276\n",
      "Epoch 386, Loss: 0.19937476515769958, Final Batch Loss: 0.11653256416320801\n",
      "Epoch 387, Loss: 0.1392473354935646, Final Batch Loss: 0.07876774668693542\n",
      "Epoch 388, Loss: 0.15348711609840393, Final Batch Loss: 0.07395799458026886\n",
      "Epoch 389, Loss: 0.18479625135660172, Final Batch Loss: 0.09247108548879623\n",
      "Epoch 390, Loss: 0.18134674429893494, Final Batch Loss: 0.09559926390647888\n",
      "Epoch 391, Loss: 0.20023257657885551, Final Batch Loss: 0.14323262870311737\n",
      "Epoch 392, Loss: 0.17641782760620117, Final Batch Loss: 0.10545358061790466\n",
      "Epoch 393, Loss: 0.1446632370352745, Final Batch Loss: 0.06601818650960922\n",
      "Epoch 394, Loss: 0.20080537348985672, Final Batch Loss: 0.09807389229536057\n",
      "Epoch 395, Loss: 0.16596230491995811, Final Batch Loss: 0.11614241451025009\n",
      "Epoch 396, Loss: 0.16223056986927986, Final Batch Loss: 0.10511066019535065\n",
      "Epoch 397, Loss: 0.16133029013872147, Final Batch Loss: 0.0935545563697815\n",
      "Epoch 398, Loss: 0.1826518476009369, Final Batch Loss: 0.11047297716140747\n",
      "Epoch 399, Loss: 0.12927454710006714, Final Batch Loss: 0.08167076855897903\n",
      "Epoch 400, Loss: 0.16059698164463043, Final Batch Loss: 0.058886200189590454\n",
      "Epoch 401, Loss: 0.17284317687153816, Final Batch Loss: 0.11718083918094635\n",
      "Epoch 402, Loss: 0.1883935183286667, Final Batch Loss: 0.11288340389728546\n",
      "Epoch 403, Loss: 0.18047331273555756, Final Batch Loss: 0.09432903677225113\n",
      "Epoch 404, Loss: 0.16612276434898376, Final Batch Loss: 0.08897855132818222\n",
      "Epoch 405, Loss: 0.12690812349319458, Final Batch Loss: 0.06614011526107788\n",
      "Epoch 406, Loss: 0.13372572138905525, Final Batch Loss: 0.05611579492688179\n",
      "Epoch 407, Loss: 0.1395096480846405, Final Batch Loss: 0.07559221982955933\n",
      "Epoch 408, Loss: 0.13601164519786835, Final Batch Loss: 0.07783771306276321\n",
      "Epoch 409, Loss: 0.09280179813504219, Final Batch Loss: 0.03812209144234657\n",
      "Epoch 410, Loss: 0.1784258633852005, Final Batch Loss: 0.10273577272891998\n",
      "Epoch 411, Loss: 0.14051152765750885, Final Batch Loss: 0.07313846796751022\n",
      "Epoch 412, Loss: 0.15194988623261452, Final Batch Loss: 0.09646345674991608\n",
      "Epoch 413, Loss: 0.12842465564608574, Final Batch Loss: 0.04094012454152107\n",
      "Epoch 414, Loss: 0.114055335521698, Final Batch Loss: 0.05868063122034073\n",
      "Epoch 415, Loss: 0.12616483122110367, Final Batch Loss: 0.06079256534576416\n",
      "Epoch 416, Loss: 0.1752757504582405, Final Batch Loss: 0.09323327988386154\n",
      "Epoch 417, Loss: 0.15338633954524994, Final Batch Loss: 0.087162546813488\n",
      "Epoch 418, Loss: 0.1596810147166252, Final Batch Loss: 0.06949695199728012\n",
      "Epoch 419, Loss: 0.15527446940541267, Final Batch Loss: 0.10293964296579361\n",
      "Epoch 420, Loss: 0.14765486121177673, Final Batch Loss: 0.07323776185512543\n",
      "Epoch 421, Loss: 0.17709484696388245, Final Batch Loss: 0.06936182081699371\n",
      "Epoch 422, Loss: 0.1699703484773636, Final Batch Loss: 0.10892394930124283\n",
      "Epoch 423, Loss: 0.1655489057302475, Final Batch Loss: 0.07799383252859116\n",
      "Epoch 424, Loss: 0.13327556848526, Final Batch Loss: 0.046230316162109375\n",
      "Epoch 425, Loss: 0.13749926909804344, Final Batch Loss: 0.0478072427213192\n",
      "Epoch 426, Loss: 0.1604294553399086, Final Batch Loss: 0.07907450199127197\n",
      "Epoch 427, Loss: 0.15298838913440704, Final Batch Loss: 0.06954144686460495\n",
      "Epoch 428, Loss: 0.15396830067038536, Final Batch Loss: 0.051697421818971634\n",
      "Epoch 429, Loss: 0.15754565596580505, Final Batch Loss: 0.10256735980510712\n",
      "Epoch 430, Loss: 0.12252342700958252, Final Batch Loss: 0.05130787193775177\n",
      "Epoch 431, Loss: 0.10782394930720329, Final Batch Loss: 0.053010888397693634\n",
      "Epoch 432, Loss: 0.12682343274354935, Final Batch Loss: 0.0664544552564621\n",
      "Epoch 433, Loss: 0.1240517608821392, Final Batch Loss: 0.06256694346666336\n",
      "Epoch 434, Loss: 0.13299649581313133, Final Batch Loss: 0.0712314322590828\n",
      "Epoch 435, Loss: 0.1788078397512436, Final Batch Loss: 0.09033481031656265\n",
      "Epoch 436, Loss: 0.11504175886511803, Final Batch Loss: 0.06006499379873276\n",
      "Epoch 437, Loss: 0.11868646740913391, Final Batch Loss: 0.07710004597902298\n",
      "Epoch 438, Loss: 0.1265113539993763, Final Batch Loss: 0.06703779846429825\n",
      "Epoch 439, Loss: 0.13941626623272896, Final Batch Loss: 0.0970141664147377\n",
      "Epoch 440, Loss: 0.10093438997864723, Final Batch Loss: 0.04616106301546097\n",
      "Epoch 441, Loss: 0.13059725239872932, Final Batch Loss: 0.055617500096559525\n",
      "Epoch 442, Loss: 0.17370041087269783, Final Batch Loss: 0.1257247030735016\n",
      "Epoch 443, Loss: 0.14951011538505554, Final Batch Loss: 0.07518541067838669\n",
      "Epoch 444, Loss: 0.1398523449897766, Final Batch Loss: 0.08920706063508987\n",
      "Epoch 445, Loss: 0.18758613616228104, Final Batch Loss: 0.11747707426548004\n",
      "Epoch 446, Loss: 0.18026962131261826, Final Batch Loss: 0.10802275687456131\n",
      "Epoch 447, Loss: 0.18815072625875473, Final Batch Loss: 0.12344017624855042\n",
      "Epoch 448, Loss: 0.1321779154241085, Final Batch Loss: 0.07925254106521606\n",
      "Epoch 449, Loss: 0.11304862052202225, Final Batch Loss: 0.05380575358867645\n",
      "Epoch 450, Loss: 0.15820031613111496, Final Batch Loss: 0.09320688247680664\n",
      "Epoch 451, Loss: 0.12303837016224861, Final Batch Loss: 0.06300874799489975\n",
      "Epoch 452, Loss: 0.12686191871762276, Final Batch Loss: 0.08198287338018417\n",
      "Epoch 453, Loss: 0.1316325068473816, Final Batch Loss: 0.059525519609451294\n",
      "Epoch 454, Loss: 0.09229916334152222, Final Batch Loss: 0.05659400299191475\n",
      "Epoch 455, Loss: 0.14646689966320992, Final Batch Loss: 0.08568272739648819\n",
      "Epoch 456, Loss: 0.12169573083519936, Final Batch Loss: 0.04354381933808327\n",
      "Epoch 457, Loss: 0.11729201674461365, Final Batch Loss: 0.07731299102306366\n",
      "Epoch 458, Loss: 0.06700583919882774, Final Batch Loss: 0.03460164740681648\n",
      "Epoch 459, Loss: 0.09468742646276951, Final Batch Loss: 0.029359320178627968\n",
      "Epoch 460, Loss: 0.15894493460655212, Final Batch Loss: 0.09008757770061493\n",
      "Epoch 461, Loss: 0.10370955988764763, Final Batch Loss: 0.05076386407017708\n",
      "Epoch 462, Loss: 0.16728314757347107, Final Batch Loss: 0.09755031019449234\n",
      "Epoch 463, Loss: 0.15639503300189972, Final Batch Loss: 0.09181943535804749\n",
      "Epoch 464, Loss: 0.14625312760472298, Final Batch Loss: 0.04942012205719948\n",
      "Epoch 465, Loss: 0.1407414823770523, Final Batch Loss: 0.06539566069841385\n",
      "Epoch 466, Loss: 0.09938181564211845, Final Batch Loss: 0.04386665299534798\n",
      "Epoch 467, Loss: 0.11784440651535988, Final Batch Loss: 0.08190970867872238\n",
      "Epoch 468, Loss: 0.09135304763913155, Final Batch Loss: 0.03717527911067009\n",
      "Epoch 469, Loss: 0.12165650352835655, Final Batch Loss: 0.06741559505462646\n",
      "Epoch 470, Loss: 0.15497808903455734, Final Batch Loss: 0.08749783039093018\n",
      "Epoch 471, Loss: 0.17745357006788254, Final Batch Loss: 0.11192192137241364\n",
      "Epoch 472, Loss: 0.18366149067878723, Final Batch Loss: 0.08066512644290924\n",
      "Epoch 473, Loss: 0.1189873144030571, Final Batch Loss: 0.046811141073703766\n",
      "Epoch 474, Loss: 0.12045285850763321, Final Batch Loss: 0.06286995112895966\n",
      "Epoch 475, Loss: 0.11328165605664253, Final Batch Loss: 0.07665994763374329\n",
      "Epoch 476, Loss: 0.15032583475112915, Final Batch Loss: 0.042441561818122864\n",
      "Epoch 477, Loss: 0.11943588778376579, Final Batch Loss: 0.04454067721962929\n",
      "Epoch 478, Loss: 0.09321535751223564, Final Batch Loss: 0.04727402329444885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479, Loss: 0.1545640453696251, Final Batch Loss: 0.08755768090486526\n",
      "Epoch 480, Loss: 0.13135544210672379, Final Batch Loss: 0.06684154272079468\n",
      "Epoch 481, Loss: 0.09075845777988434, Final Batch Loss: 0.05079614743590355\n",
      "Epoch 482, Loss: 0.07683978602290154, Final Batch Loss: 0.03831266239285469\n",
      "Epoch 483, Loss: 0.0991436019539833, Final Batch Loss: 0.049428027123212814\n",
      "Epoch 484, Loss: 0.11507945880293846, Final Batch Loss: 0.05845257639884949\n",
      "Epoch 485, Loss: 0.09139559045433998, Final Batch Loss: 0.0329405702650547\n",
      "Epoch 486, Loss: 0.1461644247174263, Final Batch Loss: 0.08116152137517929\n",
      "Epoch 487, Loss: 0.08724986016750336, Final Batch Loss: 0.04536500200629234\n",
      "Epoch 488, Loss: 0.13587478175759315, Final Batch Loss: 0.0568377785384655\n",
      "Epoch 489, Loss: 0.08966339752078056, Final Batch Loss: 0.054710302501916885\n",
      "Epoch 490, Loss: 0.11839132755994797, Final Batch Loss: 0.06566578149795532\n",
      "Epoch 491, Loss: 0.09089955314993858, Final Batch Loss: 0.039013102650642395\n",
      "Epoch 492, Loss: 0.12639867886900902, Final Batch Loss: 0.0923958420753479\n",
      "Epoch 493, Loss: 0.09205663949251175, Final Batch Loss: 0.03270033001899719\n",
      "Epoch 494, Loss: 0.1273212730884552, Final Batch Loss: 0.0586882159113884\n",
      "Epoch 495, Loss: 0.16350765898823738, Final Batch Loss: 0.10176597535610199\n",
      "Epoch 496, Loss: 0.118684571236372, Final Batch Loss: 0.05752265825867653\n",
      "Epoch 497, Loss: 0.16893009468913078, Final Batch Loss: 0.036994051188230515\n",
      "Epoch 498, Loss: 0.07111182250082493, Final Batch Loss: 0.030695730820298195\n",
      "Epoch 499, Loss: 0.11007653176784515, Final Batch Loss: 0.041990943253040314\n",
      "Epoch 500, Loss: 0.17020636051893234, Final Batch Loss: 0.10260899364948273\n",
      "Epoch 501, Loss: 0.11497513949871063, Final Batch Loss: 0.06249922886490822\n",
      "Epoch 502, Loss: 0.1572294645011425, Final Batch Loss: 0.11388746649026871\n",
      "Epoch 503, Loss: 0.1088712178170681, Final Batch Loss: 0.04385083541274071\n",
      "Epoch 504, Loss: 0.11895100772380829, Final Batch Loss: 0.04356231540441513\n",
      "Epoch 505, Loss: 0.1172381304204464, Final Batch Loss: 0.07859771698713303\n",
      "Epoch 506, Loss: 0.13065987080335617, Final Batch Loss: 0.06475618481636047\n",
      "Epoch 507, Loss: 0.08510049432516098, Final Batch Loss: 0.04640727862715721\n",
      "Epoch 508, Loss: 0.10595700889825821, Final Batch Loss: 0.06134142354130745\n",
      "Epoch 509, Loss: 0.13663841784000397, Final Batch Loss: 0.06934390217065811\n",
      "Epoch 510, Loss: 0.1138804443180561, Final Batch Loss: 0.03991883620619774\n",
      "Epoch 511, Loss: 0.09551766514778137, Final Batch Loss: 0.04826999455690384\n",
      "Epoch 512, Loss: 0.08398591354489326, Final Batch Loss: 0.033469729125499725\n",
      "Epoch 513, Loss: 0.09156940132379532, Final Batch Loss: 0.03666416183114052\n",
      "Epoch 514, Loss: 0.12264015898108482, Final Batch Loss: 0.04739316925406456\n",
      "Epoch 515, Loss: 0.09716089814901352, Final Batch Loss: 0.061529677361249924\n",
      "Epoch 516, Loss: 0.10301054641604424, Final Batch Loss: 0.06303165853023529\n",
      "Epoch 517, Loss: 0.07427261024713516, Final Batch Loss: 0.03848899155855179\n",
      "Epoch 518, Loss: 0.08047495037317276, Final Batch Loss: 0.03458307310938835\n",
      "Epoch 519, Loss: 0.06605125591158867, Final Batch Loss: 0.030555859208106995\n",
      "Epoch 520, Loss: 0.167546346783638, Final Batch Loss: 0.07736612856388092\n",
      "Epoch 521, Loss: 0.13083267584443092, Final Batch Loss: 0.04164991155266762\n",
      "Epoch 522, Loss: 0.07529595494270325, Final Batch Loss: 0.03976551070809364\n",
      "Epoch 523, Loss: 0.15326061099767685, Final Batch Loss: 0.12010890245437622\n",
      "Epoch 524, Loss: 0.09133276343345642, Final Batch Loss: 0.046038974076509476\n",
      "Epoch 525, Loss: 0.09044826030731201, Final Batch Loss: 0.054629359394311905\n",
      "Epoch 526, Loss: 0.16034509986639023, Final Batch Loss: 0.0942440927028656\n",
      "Epoch 527, Loss: 0.10408198460936546, Final Batch Loss: 0.04260073974728584\n",
      "Epoch 528, Loss: 0.09276864305138588, Final Batch Loss: 0.03115912154316902\n",
      "Epoch 529, Loss: 0.14453541859984398, Final Batch Loss: 0.083988718688488\n",
      "Epoch 530, Loss: 0.10138413682579994, Final Batch Loss: 0.035742904990911484\n",
      "Epoch 531, Loss: 0.09736761823296547, Final Batch Loss: 0.044895898550748825\n",
      "Epoch 532, Loss: 0.10346845164895058, Final Batch Loss: 0.0626763105392456\n",
      "Epoch 533, Loss: 0.13476981967687607, Final Batch Loss: 0.05976445972919464\n",
      "Epoch 534, Loss: 0.09873206913471222, Final Batch Loss: 0.048404086381196976\n",
      "Epoch 535, Loss: 0.06243624724447727, Final Batch Loss: 0.020877646282315254\n",
      "Epoch 536, Loss: 0.10556817799806595, Final Batch Loss: 0.04413358122110367\n",
      "Epoch 537, Loss: 0.08892388641834259, Final Batch Loss: 0.044200677424669266\n",
      "Epoch 538, Loss: 0.12249073013663292, Final Batch Loss: 0.06621025502681732\n",
      "Epoch 539, Loss: 0.16110287606716156, Final Batch Loss: 0.0943535640835762\n",
      "Epoch 540, Loss: 0.12613700702786446, Final Batch Loss: 0.08835083246231079\n",
      "Epoch 541, Loss: 0.11086367815732956, Final Batch Loss: 0.05570388585329056\n",
      "Epoch 542, Loss: 0.12995361164212227, Final Batch Loss: 0.05427514389157295\n",
      "Epoch 543, Loss: 0.07031474635004997, Final Batch Loss: 0.041680462658405304\n",
      "Epoch 544, Loss: 0.10048535093665123, Final Batch Loss: 0.06474761664867401\n",
      "Epoch 545, Loss: 0.08282960578799248, Final Batch Loss: 0.0391412116587162\n",
      "Epoch 546, Loss: 0.11811968311667442, Final Batch Loss: 0.05054592713713646\n",
      "Epoch 547, Loss: 0.04575561545789242, Final Batch Loss: 0.02672392688691616\n",
      "Epoch 548, Loss: 0.061111025512218475, Final Batch Loss: 0.018621250987052917\n",
      "Epoch 549, Loss: 0.07485488057136536, Final Batch Loss: 0.03697097301483154\n",
      "Epoch 550, Loss: 0.12906622514128685, Final Batch Loss: 0.08706096559762955\n",
      "Epoch 551, Loss: 0.07474983483552933, Final Batch Loss: 0.034356940537691116\n",
      "Epoch 552, Loss: 0.13183315470814705, Final Batch Loss: 0.038825128227472305\n",
      "Epoch 553, Loss: 0.1285959929227829, Final Batch Loss: 0.044386208057403564\n",
      "Epoch 554, Loss: 0.061003515496850014, Final Batch Loss: 0.021973079070448875\n",
      "Epoch 555, Loss: 0.09692104905843735, Final Batch Loss: 0.052863482385873795\n",
      "Epoch 556, Loss: 0.08423607051372528, Final Batch Loss: 0.04779710993170738\n",
      "Epoch 557, Loss: 0.08342061564326286, Final Batch Loss: 0.055322594940662384\n",
      "Epoch 558, Loss: 0.10926448553800583, Final Batch Loss: 0.050426263362169266\n",
      "Epoch 559, Loss: 0.08235157653689384, Final Batch Loss: 0.022257167845964432\n",
      "Epoch 560, Loss: 0.12102017924189568, Final Batch Loss: 0.08123932033777237\n",
      "Epoch 561, Loss: 0.10744613036513329, Final Batch Loss: 0.037587154656648636\n",
      "Epoch 562, Loss: 0.07284414023160934, Final Batch Loss: 0.04377508908510208\n",
      "Epoch 563, Loss: 0.05094076879322529, Final Batch Loss: 0.021557331085205078\n",
      "Epoch 564, Loss: 0.08501824550330639, Final Batch Loss: 0.0547342486679554\n",
      "Epoch 565, Loss: 0.13278106972575188, Final Batch Loss: 0.0764068216085434\n",
      "Epoch 566, Loss: 0.11584332212805748, Final Batch Loss: 0.07547037303447723\n",
      "Epoch 567, Loss: 0.08587881922721863, Final Batch Loss: 0.05477893352508545\n",
      "Epoch 568, Loss: 0.12017476931214333, Final Batch Loss: 0.08721984922885895\n",
      "Epoch 569, Loss: 0.070987269282341, Final Batch Loss: 0.029663439840078354\n",
      "Epoch 570, Loss: 0.07068332843482494, Final Batch Loss: 0.016281889751553535\n",
      "Epoch 571, Loss: 0.05733661912381649, Final Batch Loss: 0.033424776047468185\n",
      "Epoch 572, Loss: 0.08152281306684017, Final Batch Loss: 0.02742328681051731\n",
      "Epoch 573, Loss: 0.13848920539021492, Final Batch Loss: 0.050653669983148575\n",
      "Epoch 574, Loss: 0.09102281183004379, Final Batch Loss: 0.04501037672162056\n",
      "Epoch 575, Loss: 0.08555354550480843, Final Batch Loss: 0.04570305719971657\n",
      "Epoch 576, Loss: 0.062168581411242485, Final Batch Loss: 0.03722720965743065\n",
      "Epoch 577, Loss: 0.047259410843253136, Final Batch Loss: 0.01616496406495571\n",
      "Epoch 578, Loss: 0.09351121261715889, Final Batch Loss: 0.04823934659361839\n",
      "Epoch 579, Loss: 0.06455732509493828, Final Batch Loss: 0.030627388507127762\n",
      "Epoch 580, Loss: 0.12760260701179504, Final Batch Loss: 0.060112908482551575\n",
      "Epoch 581, Loss: 0.10134939849376678, Final Batch Loss: 0.0684567391872406\n",
      "Epoch 582, Loss: 0.11406952142715454, Final Batch Loss: 0.06215775012969971\n",
      "Epoch 583, Loss: 0.06925598718225956, Final Batch Loss: 0.02225336618721485\n",
      "Epoch 584, Loss: 0.06988811865448952, Final Batch Loss: 0.01994585618376732\n",
      "Epoch 585, Loss: 0.044375333935022354, Final Batch Loss: 0.01836174540221691\n",
      "Epoch 586, Loss: 0.11431203410029411, Final Batch Loss: 0.06419108062982559\n",
      "Epoch 587, Loss: 0.09473370760679245, Final Batch Loss: 0.04826191067695618\n",
      "Epoch 588, Loss: 0.07832677289843559, Final Batch Loss: 0.03712746500968933\n",
      "Epoch 589, Loss: 0.05712142586708069, Final Batch Loss: 0.02725835330784321\n",
      "Epoch 590, Loss: 0.124783705919981, Final Batch Loss: 0.07475772500038147\n",
      "Epoch 591, Loss: 0.08750651031732559, Final Batch Loss: 0.042106762528419495\n",
      "Epoch 592, Loss: 0.11686911433935165, Final Batch Loss: 0.041694365441799164\n",
      "Epoch 593, Loss: 0.05935328081250191, Final Batch Loss: 0.025214258581399918\n",
      "Epoch 594, Loss: 0.05249067768454552, Final Batch Loss: 0.014122925698757172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 595, Loss: 0.09091027081012726, Final Batch Loss: 0.05176994577050209\n",
      "Epoch 596, Loss: 0.07042840495705605, Final Batch Loss: 0.03481794521212578\n",
      "Epoch 597, Loss: 0.10658075660467148, Final Batch Loss: 0.05916865915060043\n",
      "Epoch 598, Loss: 0.09265732020139694, Final Batch Loss: 0.03165307268500328\n",
      "Epoch 599, Loss: 0.07752888090908527, Final Batch Loss: 0.02775726281106472\n",
      "Epoch 600, Loss: 0.1112220287322998, Final Batch Loss: 0.07057946920394897\n",
      "Epoch 601, Loss: 0.09591864049434662, Final Batch Loss: 0.039819810539484024\n",
      "Epoch 602, Loss: 0.08485369756817818, Final Batch Loss: 0.04832957312464714\n",
      "Epoch 603, Loss: 0.1502564363181591, Final Batch Loss: 0.04647955670952797\n",
      "Epoch 604, Loss: 0.04541933722794056, Final Batch Loss: 0.017664844170212746\n",
      "Epoch 605, Loss: 0.22931724786758423, Final Batch Loss: 0.17768535017967224\n",
      "Epoch 606, Loss: 0.10715898871421814, Final Batch Loss: 0.03665514290332794\n",
      "Epoch 607, Loss: 0.17105737328529358, Final Batch Loss: 0.09718579798936844\n",
      "Epoch 608, Loss: 0.1250539906322956, Final Batch Loss: 0.04542187228798866\n",
      "Epoch 609, Loss: 0.13809282705187798, Final Batch Loss: 0.05408727005124092\n",
      "Epoch 610, Loss: 0.08744332939386368, Final Batch Loss: 0.027258530259132385\n",
      "Epoch 611, Loss: 0.09023306332528591, Final Batch Loss: 0.017644910141825676\n",
      "Epoch 612, Loss: 0.122514383867383, Final Batch Loss: 0.09845566749572754\n",
      "Epoch 613, Loss: 0.07681277208030224, Final Batch Loss: 0.024265484884381294\n",
      "Epoch 614, Loss: 0.051297398284077644, Final Batch Loss: 0.017323194071650505\n",
      "Epoch 615, Loss: 0.08241106197237968, Final Batch Loss: 0.03410119190812111\n",
      "Epoch 616, Loss: 0.07895069569349289, Final Batch Loss: 0.03245319426059723\n",
      "Epoch 617, Loss: 0.08800099045038223, Final Batch Loss: 0.05124745890498161\n",
      "Epoch 618, Loss: 0.08568492531776428, Final Batch Loss: 0.04473385214805603\n",
      "Epoch 619, Loss: 0.06552740558981895, Final Batch Loss: 0.040893085300922394\n",
      "Epoch 620, Loss: 0.07678218930959702, Final Batch Loss: 0.01817161589860916\n",
      "Epoch 621, Loss: 0.0857950784265995, Final Batch Loss: 0.033663779497146606\n",
      "Epoch 622, Loss: 0.14554327726364136, Final Batch Loss: 0.07341893762350082\n",
      "Epoch 623, Loss: 0.05280244164168835, Final Batch Loss: 0.025800563395023346\n",
      "Epoch 624, Loss: 0.10773244872689247, Final Batch Loss: 0.0662677064538002\n",
      "Epoch 625, Loss: 0.11299660056829453, Final Batch Loss: 0.049976199865341187\n",
      "Epoch 626, Loss: 0.07263902574777603, Final Batch Loss: 0.03871457278728485\n",
      "Epoch 627, Loss: 0.10954147577285767, Final Batch Loss: 0.056908831000328064\n",
      "Epoch 628, Loss: 0.10100425779819489, Final Batch Loss: 0.06317204982042313\n",
      "Epoch 629, Loss: 0.08752777799963951, Final Batch Loss: 0.038520876318216324\n",
      "Epoch 630, Loss: 0.07729236781597137, Final Batch Loss: 0.03568126633763313\n",
      "Epoch 631, Loss: 0.09819431975483894, Final Batch Loss: 0.03750418871641159\n",
      "Epoch 632, Loss: 0.15163277089595795, Final Batch Loss: 0.06419726461172104\n",
      "Epoch 633, Loss: 0.07940367236733437, Final Batch Loss: 0.046530064195394516\n",
      "Epoch 634, Loss: 0.09413447976112366, Final Batch Loss: 0.07295923680067062\n",
      "Epoch 635, Loss: 0.08181518688797951, Final Batch Loss: 0.033275358378887177\n",
      "Epoch 636, Loss: 0.04866994917392731, Final Batch Loss: 0.015428077429533005\n",
      "Epoch 637, Loss: 0.0643887110054493, Final Batch Loss: 0.03187458589673042\n",
      "Epoch 638, Loss: 0.07481454312801361, Final Batch Loss: 0.03481711074709892\n",
      "Epoch 639, Loss: 0.07541786693036556, Final Batch Loss: 0.02405170165002346\n",
      "Epoch 640, Loss: 0.07143097184598446, Final Batch Loss: 0.04049760475754738\n",
      "Epoch 641, Loss: 0.11837384104728699, Final Batch Loss: 0.08145703375339508\n",
      "Epoch 642, Loss: 0.10508587211370468, Final Batch Loss: 0.03832177817821503\n",
      "Epoch 643, Loss: 0.09687453508377075, Final Batch Loss: 0.03548736497759819\n",
      "Epoch 644, Loss: 0.06735369935631752, Final Batch Loss: 0.022685382515192032\n",
      "Epoch 645, Loss: 0.07436450943350792, Final Batch Loss: 0.03293108195066452\n",
      "Epoch 646, Loss: 0.06936537101864815, Final Batch Loss: 0.042649589478969574\n",
      "Epoch 647, Loss: 0.08450167253613472, Final Batch Loss: 0.0400153286755085\n",
      "Epoch 648, Loss: 0.07238875143229961, Final Batch Loss: 0.027457064017653465\n",
      "Epoch 649, Loss: 0.07617098093032837, Final Batch Loss: 0.044432684779167175\n",
      "Epoch 650, Loss: 0.10162705928087234, Final Batch Loss: 0.05830783769488335\n",
      "Epoch 651, Loss: 0.09076548367738724, Final Batch Loss: 0.024243958294391632\n",
      "Epoch 652, Loss: 0.07587723061442375, Final Batch Loss: 0.03455955907702446\n",
      "Epoch 653, Loss: 0.0570272970944643, Final Batch Loss: 0.025565175339579582\n",
      "Epoch 654, Loss: 0.09989944100379944, Final Batch Loss: 0.06465689837932587\n",
      "Epoch 655, Loss: 0.08557313308119774, Final Batch Loss: 0.05408160760998726\n",
      "Epoch 656, Loss: 0.11384212598204613, Final Batch Loss: 0.08376634120941162\n",
      "Epoch 657, Loss: 0.0642344132065773, Final Batch Loss: 0.024562209844589233\n",
      "Epoch 658, Loss: 0.09312235936522484, Final Batch Loss: 0.06421241164207458\n",
      "Epoch 659, Loss: 0.06921840831637383, Final Batch Loss: 0.0364723727107048\n",
      "Epoch 660, Loss: 0.137862890958786, Final Batch Loss: 0.06546654552221298\n",
      "Epoch 661, Loss: 0.0853692851960659, Final Batch Loss: 0.04820018261671066\n",
      "Epoch 662, Loss: 0.08053868636488914, Final Batch Loss: 0.04294689744710922\n",
      "Epoch 663, Loss: 0.09469741582870483, Final Batch Loss: 0.04378942772746086\n",
      "Epoch 664, Loss: 0.09488283097743988, Final Batch Loss: 0.0637742206454277\n",
      "Epoch 665, Loss: 0.0912842359393835, Final Batch Loss: 0.06385243684053421\n",
      "Epoch 666, Loss: 0.06519797630608082, Final Batch Loss: 0.027773888781666756\n",
      "Epoch 667, Loss: 0.11782791838049889, Final Batch Loss: 0.08148512989282608\n",
      "Epoch 668, Loss: 0.08203527703881264, Final Batch Loss: 0.03884720802307129\n",
      "Epoch 669, Loss: 0.08884907141327858, Final Batch Loss: 0.04711797460913658\n",
      "Epoch 670, Loss: 0.07505081593990326, Final Batch Loss: 0.04173742234706879\n",
      "Epoch 671, Loss: 0.08532252721488476, Final Batch Loss: 0.02983062155544758\n",
      "Epoch 672, Loss: 0.05481210723519325, Final Batch Loss: 0.029230976477265358\n",
      "Epoch 673, Loss: 0.0828788485378027, Final Batch Loss: 0.053140535950660706\n",
      "Epoch 674, Loss: 0.06979200057685375, Final Batch Loss: 0.040943581610918045\n",
      "Epoch 675, Loss: 0.053173353895545006, Final Batch Loss: 0.01866104267537594\n",
      "Epoch 676, Loss: 0.10449894517660141, Final Batch Loss: 0.039488278329372406\n",
      "Epoch 677, Loss: 0.047029122710227966, Final Batch Loss: 0.018443994224071503\n",
      "Epoch 678, Loss: 0.05752627179026604, Final Batch Loss: 0.044133033603429794\n",
      "Epoch 679, Loss: 0.13468499667942524, Final Batch Loss: 0.11016211658716202\n",
      "Epoch 680, Loss: 0.053766123950481415, Final Batch Loss: 0.032216500490903854\n",
      "Epoch 681, Loss: 0.14186736568808556, Final Batch Loss: 0.10382909327745438\n",
      "Epoch 682, Loss: 0.05585714802145958, Final Batch Loss: 0.027127929031848907\n",
      "Epoch 683, Loss: 0.17479635775089264, Final Batch Loss: 0.1385856568813324\n",
      "Epoch 684, Loss: 0.06403579469770193, Final Batch Loss: 0.049316663295030594\n",
      "Epoch 685, Loss: 0.09619756788015366, Final Batch Loss: 0.06071750074625015\n",
      "Epoch 686, Loss: 0.06547776237130165, Final Batch Loss: 0.027708187699317932\n",
      "Epoch 687, Loss: 0.07146848365664482, Final Batch Loss: 0.0480138398706913\n",
      "Epoch 688, Loss: 0.07266862131655216, Final Batch Loss: 0.05088028311729431\n",
      "Epoch 689, Loss: 0.09007863886654377, Final Batch Loss: 0.01910022832453251\n",
      "Epoch 690, Loss: 0.056677188724279404, Final Batch Loss: 0.02061844989657402\n",
      "Epoch 691, Loss: 0.10326053202152252, Final Batch Loss: 0.04152807965874672\n",
      "Epoch 692, Loss: 0.06876355409622192, Final Batch Loss: 0.029874037951231003\n",
      "Epoch 693, Loss: 0.10304692387580872, Final Batch Loss: 0.06084170937538147\n",
      "Epoch 694, Loss: 0.07141157239675522, Final Batch Loss: 0.03891531005501747\n",
      "Epoch 695, Loss: 0.07081647589802742, Final Batch Loss: 0.04813534766435623\n",
      "Epoch 696, Loss: 0.09918150305747986, Final Batch Loss: 0.05912327021360397\n",
      "Epoch 697, Loss: 0.10194013267755508, Final Batch Loss: 0.047468286007642746\n",
      "Epoch 698, Loss: 0.08930400013923645, Final Batch Loss: 0.05421743541955948\n",
      "Epoch 699, Loss: 0.07581710442900658, Final Batch Loss: 0.03477724641561508\n",
      "Epoch 700, Loss: 0.0805320143699646, Final Batch Loss: 0.05089740827679634\n",
      "Epoch 701, Loss: 0.11171809211373329, Final Batch Loss: 0.0720534473657608\n",
      "Epoch 702, Loss: 0.11734450608491898, Final Batch Loss: 0.08020097762346268\n",
      "Epoch 703, Loss: 0.0624720323830843, Final Batch Loss: 0.04529328644275665\n",
      "Epoch 704, Loss: 0.0947527214884758, Final Batch Loss: 0.05196944251656532\n",
      "Epoch 705, Loss: 0.06506755575537682, Final Batch Loss: 0.04086827114224434\n",
      "Epoch 706, Loss: 0.08852162398397923, Final Batch Loss: 0.023263761773705482\n",
      "Epoch 707, Loss: 0.0939517468214035, Final Batch Loss: 0.03468899428844452\n",
      "Epoch 708, Loss: 0.10780453309416771, Final Batch Loss: 0.05197284370660782\n",
      "Epoch 709, Loss: 0.06369883380830288, Final Batch Loss: 0.018758738413453102\n",
      "Epoch 710, Loss: 0.09637138806283474, Final Batch Loss: 0.07034395635128021\n",
      "Epoch 711, Loss: 0.08622018247842789, Final Batch Loss: 0.047264017164707184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 712, Loss: 0.06740127131342888, Final Batch Loss: 0.0242026224732399\n",
      "Epoch 713, Loss: 0.09328237920999527, Final Batch Loss: 0.029328681528568268\n",
      "Epoch 714, Loss: 0.08555630594491959, Final Batch Loss: 0.03616473823785782\n",
      "Epoch 715, Loss: 0.07413433864712715, Final Batch Loss: 0.041456688195466995\n",
      "Epoch 716, Loss: 0.03202830534428358, Final Batch Loss: 0.013546210713684559\n",
      "Epoch 717, Loss: 0.06268657930195332, Final Batch Loss: 0.04036492481827736\n",
      "Epoch 718, Loss: 0.08775064535439014, Final Batch Loss: 0.02114122547209263\n",
      "Epoch 719, Loss: 0.02515240851789713, Final Batch Loss: 0.011282267048954964\n",
      "Epoch 720, Loss: 0.07803050987422466, Final Batch Loss: 0.05680811405181885\n",
      "Epoch 721, Loss: 0.06013881973922253, Final Batch Loss: 0.026315784081816673\n",
      "Epoch 722, Loss: 0.05645284429192543, Final Batch Loss: 0.02675710618495941\n",
      "Epoch 723, Loss: 0.05413570813834667, Final Batch Loss: 0.01621383987367153\n",
      "Epoch 724, Loss: 0.04523381590843201, Final Batch Loss: 0.026929611340165138\n",
      "Epoch 725, Loss: 0.06964368373155594, Final Batch Loss: 0.032486286014318466\n",
      "Epoch 726, Loss: 0.05262468568980694, Final Batch Loss: 0.023963140323758125\n",
      "Epoch 727, Loss: 0.049203624948859215, Final Batch Loss: 0.014796877279877663\n",
      "Epoch 728, Loss: 0.05401051603257656, Final Batch Loss: 0.028193846344947815\n",
      "Epoch 729, Loss: 0.0712155681103468, Final Batch Loss: 0.024143828079104424\n",
      "Epoch 730, Loss: 0.0756220631301403, Final Batch Loss: 0.03819356486201286\n",
      "Epoch 731, Loss: 0.11800090968608856, Final Batch Loss: 0.062037110328674316\n",
      "Epoch 732, Loss: 0.0756203942000866, Final Batch Loss: 0.03627653047442436\n",
      "Epoch 733, Loss: 0.08185335993766785, Final Batch Loss: 0.0311790369451046\n",
      "Epoch 734, Loss: 0.07157603465020657, Final Batch Loss: 0.027319790795445442\n",
      "Epoch 735, Loss: 0.05762398801743984, Final Batch Loss: 0.030004912987351418\n",
      "Epoch 736, Loss: 0.054356979206204414, Final Batch Loss: 0.026058293879032135\n",
      "Epoch 737, Loss: 0.09955376014113426, Final Batch Loss: 0.05355510488152504\n",
      "Epoch 738, Loss: 0.08714911714196205, Final Batch Loss: 0.05577225238084793\n",
      "Epoch 739, Loss: 0.0638834573328495, Final Batch Loss: 0.016926106065511703\n",
      "Epoch 740, Loss: 0.05563512444496155, Final Batch Loss: 0.020652666687965393\n",
      "Epoch 741, Loss: 0.06593666598200798, Final Batch Loss: 0.04086773470044136\n",
      "Epoch 742, Loss: 0.07372830808162689, Final Batch Loss: 0.03200791776180267\n",
      "Epoch 743, Loss: 0.06221882253885269, Final Batch Loss: 0.015935607254505157\n",
      "Epoch 744, Loss: 0.08371002972126007, Final Batch Loss: 0.04356861114501953\n",
      "Epoch 745, Loss: 0.0477614039555192, Final Batch Loss: 0.034299496561288834\n",
      "Epoch 746, Loss: 0.04571564681828022, Final Batch Loss: 0.016187522560358047\n",
      "Epoch 747, Loss: 0.08286766335368156, Final Batch Loss: 0.044188242405653\n",
      "Epoch 748, Loss: 0.04657054506242275, Final Batch Loss: 0.026757817715406418\n",
      "Epoch 749, Loss: 0.08364113420248032, Final Batch Loss: 0.04410990700125694\n",
      "Epoch 750, Loss: 0.0898725651204586, Final Batch Loss: 0.031938422471284866\n",
      "Epoch 751, Loss: 0.06248778663575649, Final Batch Loss: 0.02729444019496441\n",
      "Epoch 752, Loss: 0.05281008407473564, Final Batch Loss: 0.03410066291689873\n",
      "Epoch 753, Loss: 0.032094129361212254, Final Batch Loss: 0.014309470541775227\n",
      "Epoch 754, Loss: 0.04600699990987778, Final Batch Loss: 0.018481669947504997\n",
      "Epoch 755, Loss: 0.06981351971626282, Final Batch Loss: 0.04632703214883804\n",
      "Epoch 756, Loss: 0.047603100538253784, Final Batch Loss: 0.022989546880126\n",
      "Epoch 757, Loss: 0.1262727975845337, Final Batch Loss: 0.060694292187690735\n",
      "Epoch 758, Loss: 0.06814444437623024, Final Batch Loss: 0.04148819297552109\n",
      "Epoch 759, Loss: 0.05173046700656414, Final Batch Loss: 0.027551427483558655\n",
      "Epoch 760, Loss: 0.06676340661942959, Final Batch Loss: 0.012796187773346901\n",
      "Epoch 761, Loss: 0.10664493218064308, Final Batch Loss: 0.06019878387451172\n",
      "Epoch 762, Loss: 0.05375538021326065, Final Batch Loss: 0.03386598452925682\n",
      "Epoch 763, Loss: 0.06983942911028862, Final Batch Loss: 0.013896401971578598\n",
      "Epoch 764, Loss: 0.09728004410862923, Final Batch Loss: 0.03453126177191734\n",
      "Epoch 765, Loss: 0.06660281866788864, Final Batch Loss: 0.028379451483488083\n",
      "Epoch 766, Loss: 0.0637986920773983, Final Batch Loss: 0.028856240212917328\n",
      "Epoch 767, Loss: 0.06676258146762848, Final Batch Loss: 0.03308568894863129\n",
      "Epoch 768, Loss: 0.02499214094132185, Final Batch Loss: 0.011946450918912888\n",
      "Epoch 769, Loss: 0.11354738846421242, Final Batch Loss: 0.044573184102773666\n",
      "Epoch 770, Loss: 0.08274335786700249, Final Batch Loss: 0.04583028703927994\n",
      "Epoch 771, Loss: 0.09496454149484634, Final Batch Loss: 0.046219199895858765\n",
      "Epoch 772, Loss: 0.08365178480744362, Final Batch Loss: 0.04966051131486893\n",
      "Epoch 773, Loss: 0.04634217731654644, Final Batch Loss: 0.023517167195677757\n",
      "Epoch 774, Loss: 0.0661853775382042, Final Batch Loss: 0.04435953125357628\n",
      "Epoch 775, Loss: 0.05516833811998367, Final Batch Loss: 0.021780431270599365\n",
      "Epoch 776, Loss: 0.05698896944522858, Final Batch Loss: 0.0358477383852005\n",
      "Epoch 777, Loss: 0.08036734350025654, Final Batch Loss: 0.019043980166316032\n",
      "Epoch 778, Loss: 0.06442736089229584, Final Batch Loss: 0.02024715393781662\n",
      "Epoch 779, Loss: 0.0663267020136118, Final Batch Loss: 0.01685146428644657\n",
      "Epoch 780, Loss: 0.09675507247447968, Final Batch Loss: 0.04314563423395157\n",
      "Epoch 781, Loss: 0.10110925137996674, Final Batch Loss: 0.0318019837141037\n",
      "Epoch 782, Loss: 0.035664536990225315, Final Batch Loss: 0.02338758297264576\n",
      "Epoch 783, Loss: 0.08937805891036987, Final Batch Loss: 0.05829089507460594\n",
      "Epoch 784, Loss: 0.08767300471663475, Final Batch Loss: 0.05657537281513214\n",
      "Epoch 785, Loss: 0.08162369392812252, Final Batch Loss: 0.021468492224812508\n",
      "Epoch 786, Loss: 0.06268961168825626, Final Batch Loss: 0.015245651826262474\n",
      "Epoch 787, Loss: 0.08113643154501915, Final Batch Loss: 0.041146185249090195\n",
      "Epoch 788, Loss: 0.15183531865477562, Final Batch Loss: 0.09157423675060272\n",
      "Epoch 789, Loss: 0.05520501174032688, Final Batch Loss: 0.03353579342365265\n",
      "Epoch 790, Loss: 0.1357211023569107, Final Batch Loss: 0.09628263115882874\n",
      "Epoch 791, Loss: 0.11922423541545868, Final Batch Loss: 0.04023327678442001\n",
      "Epoch 792, Loss: 0.1338699273765087, Final Batch Loss: 0.09505616873502731\n",
      "Epoch 793, Loss: 0.04254495818167925, Final Batch Loss: 0.007731848396360874\n",
      "Epoch 794, Loss: 0.13214312866330147, Final Batch Loss: 0.07600964605808258\n",
      "Epoch 795, Loss: 0.05937865097075701, Final Batch Loss: 0.013140183873474598\n",
      "Epoch 796, Loss: 0.030215508304536343, Final Batch Loss: 0.015460610389709473\n",
      "Epoch 797, Loss: 0.038436586037278175, Final Batch Loss: 0.02066316455602646\n",
      "Epoch 798, Loss: 0.05129815638065338, Final Batch Loss: 0.025394512340426445\n",
      "Epoch 799, Loss: 0.04683423787355423, Final Batch Loss: 0.02491503767669201\n",
      "Epoch 800, Loss: 0.04691961035132408, Final Batch Loss: 0.020329341292381287\n",
      "Epoch 801, Loss: 0.0738578587770462, Final Batch Loss: 0.016986150294542313\n",
      "Epoch 802, Loss: 0.03717373125255108, Final Batch Loss: 0.02333950065076351\n",
      "Epoch 803, Loss: 0.04688941314816475, Final Batch Loss: 0.0316535085439682\n",
      "Epoch 804, Loss: 0.06917784176766872, Final Batch Loss: 0.022851714864373207\n",
      "Epoch 805, Loss: 0.04530807584524155, Final Batch Loss: 0.02182142063975334\n",
      "Epoch 806, Loss: 0.0333127798512578, Final Batch Loss: 0.012401803396642208\n",
      "Epoch 807, Loss: 0.06743263639509678, Final Batch Loss: 0.04206491634249687\n",
      "Epoch 808, Loss: 0.03409366309642792, Final Batch Loss: 0.019485777243971825\n",
      "Epoch 809, Loss: 0.040893769823014736, Final Batch Loss: 0.010122501291334629\n",
      "Epoch 810, Loss: 0.05684701353311539, Final Batch Loss: 0.02063147723674774\n",
      "Epoch 811, Loss: 0.026195542886853218, Final Batch Loss: 0.010055990889668465\n",
      "Epoch 812, Loss: 0.06184016540646553, Final Batch Loss: 0.04298025742173195\n",
      "Epoch 813, Loss: 0.050664788112044334, Final Batch Loss: 0.02948067896068096\n",
      "Epoch 814, Loss: 0.0684861782938242, Final Batch Loss: 0.04368019104003906\n",
      "Epoch 815, Loss: 0.06912041269242764, Final Batch Loss: 0.039796311408281326\n",
      "Epoch 816, Loss: 0.06426653452217579, Final Batch Loss: 0.017873430624604225\n",
      "Epoch 817, Loss: 0.07793259248137474, Final Batch Loss: 0.0531054362654686\n",
      "Epoch 818, Loss: 0.0670563280582428, Final Batch Loss: 0.029766559600830078\n",
      "Epoch 819, Loss: 0.042915137484669685, Final Batch Loss: 0.030527781695127487\n",
      "Epoch 820, Loss: 0.04420061595737934, Final Batch Loss: 0.021590810269117355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 821, Loss: 0.03676396515220404, Final Batch Loss: 0.021193355321884155\n",
      "Epoch 822, Loss: 0.026322217658162117, Final Batch Loss: 0.01654096692800522\n",
      "Epoch 823, Loss: 0.08647313714027405, Final Batch Loss: 0.0355244018137455\n",
      "Epoch 824, Loss: 0.09564613364636898, Final Batch Loss: 0.07304157316684723\n",
      "Epoch 825, Loss: 0.07926823012530804, Final Batch Loss: 0.025824951007962227\n",
      "Epoch 826, Loss: 0.14689616113901138, Final Batch Loss: 0.045205987989902496\n",
      "Epoch 827, Loss: 0.04991702921688557, Final Batch Loss: 0.030119657516479492\n",
      "Epoch 828, Loss: 0.04559854045510292, Final Batch Loss: 0.027369309216737747\n",
      "Epoch 829, Loss: 0.04499755334109068, Final Batch Loss: 0.013945979066193104\n",
      "Epoch 830, Loss: 0.047473592683672905, Final Batch Loss: 0.02999119833111763\n",
      "Epoch 831, Loss: 0.08817896246910095, Final Batch Loss: 0.04014553129673004\n",
      "Epoch 832, Loss: 0.025394759140908718, Final Batch Loss: 0.012234631925821304\n",
      "Epoch 833, Loss: 0.07534599676728249, Final Batch Loss: 0.05813688784837723\n",
      "Epoch 834, Loss: 0.06434210762381554, Final Batch Loss: 0.026537418365478516\n",
      "Epoch 835, Loss: 0.05207139253616333, Final Batch Loss: 0.01642916351556778\n",
      "Epoch 836, Loss: 0.04507841169834137, Final Batch Loss: 0.03183012455701828\n",
      "Epoch 837, Loss: 0.05956864729523659, Final Batch Loss: 0.025100961327552795\n",
      "Epoch 838, Loss: 0.06326375156641006, Final Batch Loss: 0.03605854511260986\n",
      "Epoch 839, Loss: 0.08269806951284409, Final Batch Loss: 0.04067681357264519\n",
      "Epoch 840, Loss: 0.04330804944038391, Final Batch Loss: 0.026348691433668137\n",
      "Epoch 841, Loss: 0.05132358334958553, Final Batch Loss: 0.01698211394250393\n",
      "Epoch 842, Loss: 0.06779848597943783, Final Batch Loss: 0.045086927711963654\n",
      "Epoch 843, Loss: 0.09716321900486946, Final Batch Loss: 0.037497151643037796\n",
      "Epoch 844, Loss: 0.08307026326656342, Final Batch Loss: 0.05985845997929573\n",
      "Epoch 845, Loss: 0.04423329792916775, Final Batch Loss: 0.023639889433979988\n",
      "Epoch 846, Loss: 0.09161742776632309, Final Batch Loss: 0.05268041044473648\n",
      "Epoch 847, Loss: 0.04230379406362772, Final Batch Loss: 0.03209693729877472\n",
      "Epoch 848, Loss: 0.058316124603152275, Final Batch Loss: 0.03750960901379585\n",
      "Epoch 849, Loss: 0.06683971174061298, Final Batch Loss: 0.03070967085659504\n",
      "Epoch 850, Loss: 0.039840937592089176, Final Batch Loss: 0.025817710906267166\n",
      "Epoch 851, Loss: 0.06186544243246317, Final Batch Loss: 0.0130946459248662\n",
      "Epoch 852, Loss: 0.03723306301981211, Final Batch Loss: 0.009704320691525936\n",
      "Epoch 853, Loss: 0.054061874747276306, Final Batch Loss: 0.03480838984251022\n",
      "Epoch 854, Loss: 0.039050642400979996, Final Batch Loss: 0.021735884249210358\n",
      "Epoch 855, Loss: 0.04381164163351059, Final Batch Loss: 0.025218302384018898\n",
      "Epoch 856, Loss: 0.05301815550774336, Final Batch Loss: 0.012650500051677227\n",
      "Epoch 857, Loss: 0.05178153142333031, Final Batch Loss: 0.030437510460615158\n",
      "Epoch 858, Loss: 0.0749522726982832, Final Batch Loss: 0.047286905348300934\n",
      "Epoch 859, Loss: 0.04176120646297932, Final Batch Loss: 0.021890435367822647\n",
      "Epoch 860, Loss: 0.041894782334566116, Final Batch Loss: 0.01196466013789177\n",
      "Epoch 861, Loss: 0.05290014483034611, Final Batch Loss: 0.03588424250483513\n",
      "Epoch 862, Loss: 0.03311500325798988, Final Batch Loss: 0.01231471449136734\n",
      "Epoch 863, Loss: 0.03408869635313749, Final Batch Loss: 0.024620147421956062\n",
      "Epoch 864, Loss: 0.06894498504698277, Final Batch Loss: 0.021542450413107872\n",
      "Epoch 865, Loss: 0.03091378789395094, Final Batch Loss: 0.011174564249813557\n",
      "Epoch 866, Loss: 0.05193813517689705, Final Batch Loss: 0.028307033702731133\n",
      "Epoch 867, Loss: 0.09285563509911299, Final Batch Loss: 0.08031728863716125\n",
      "Epoch 868, Loss: 0.026938559487462044, Final Batch Loss: 0.012825259007513523\n",
      "Epoch 869, Loss: 0.06211456190794706, Final Batch Loss: 0.010584942065179348\n",
      "Epoch 870, Loss: 0.0665403250604868, Final Batch Loss: 0.03803719952702522\n",
      "Epoch 871, Loss: 0.023846271447837353, Final Batch Loss: 0.009910273365676403\n",
      "Epoch 872, Loss: 0.05276082642376423, Final Batch Loss: 0.01684541068971157\n",
      "Epoch 873, Loss: 0.10527775064110756, Final Batch Loss: 0.0693700984120369\n",
      "Epoch 874, Loss: 0.04130527563393116, Final Batch Loss: 0.017454903572797775\n",
      "Epoch 875, Loss: 0.04558982886373997, Final Batch Loss: 0.0208220686763525\n",
      "Epoch 876, Loss: 0.036484694108366966, Final Batch Loss: 0.01251666247844696\n",
      "Epoch 877, Loss: 0.04249373823404312, Final Batch Loss: 0.008406106382608414\n",
      "Epoch 878, Loss: 0.0975363701581955, Final Batch Loss: 0.06726454943418503\n",
      "Epoch 879, Loss: 0.053144458681344986, Final Batch Loss: 0.03313203155994415\n",
      "Epoch 880, Loss: 0.05520852655172348, Final Batch Loss: 0.038612715899944305\n",
      "Epoch 881, Loss: 0.03866150416433811, Final Batch Loss: 0.016059590503573418\n",
      "Epoch 882, Loss: 0.07828674465417862, Final Batch Loss: 0.05976789444684982\n",
      "Epoch 883, Loss: 0.04705282673239708, Final Batch Loss: 0.03372542932629585\n",
      "Epoch 884, Loss: 0.06515627354383469, Final Batch Loss: 0.030568309128284454\n",
      "Epoch 885, Loss: 0.02967122010886669, Final Batch Loss: 0.0179904792457819\n",
      "Epoch 886, Loss: 0.05576062947511673, Final Batch Loss: 0.043563827872276306\n",
      "Epoch 887, Loss: 0.05417716130614281, Final Batch Loss: 0.009887106716632843\n",
      "Epoch 888, Loss: 0.06784138455986977, Final Batch Loss: 0.035487495362758636\n",
      "Epoch 889, Loss: 0.06661790050566196, Final Batch Loss: 0.020929565653204918\n",
      "Epoch 890, Loss: 0.021352280862629414, Final Batch Loss: 0.007745310664176941\n",
      "Epoch 891, Loss: 0.07813985832035542, Final Batch Loss: 0.015443595126271248\n",
      "Epoch 892, Loss: 0.059511659666895866, Final Batch Loss: 0.026655947789549828\n",
      "Epoch 893, Loss: 0.0666845515370369, Final Batch Loss: 0.03532274067401886\n",
      "Epoch 894, Loss: 0.02963863592594862, Final Batch Loss: 0.009628740139305592\n",
      "Epoch 895, Loss: 0.052377983927726746, Final Batch Loss: 0.03131214529275894\n",
      "Epoch 896, Loss: 0.060420501977205276, Final Batch Loss: 0.021058339625597\n",
      "Epoch 897, Loss: 0.0591619573533535, Final Batch Loss: 0.03238338604569435\n",
      "Epoch 898, Loss: 0.03262012358754873, Final Batch Loss: 0.008256497792899609\n",
      "Epoch 899, Loss: 0.050328800454735756, Final Batch Loss: 0.0335371196269989\n",
      "Epoch 900, Loss: 0.07052015513181686, Final Batch Loss: 0.03265916183590889\n",
      "Epoch 901, Loss: 0.022093442268669605, Final Batch Loss: 0.00797155499458313\n",
      "Epoch 902, Loss: 0.06778923980891705, Final Batch Loss: 0.02388727106153965\n",
      "Epoch 903, Loss: 0.05285880155861378, Final Batch Loss: 0.019513526931405067\n",
      "Epoch 904, Loss: 0.06170544773340225, Final Batch Loss: 0.044344015419483185\n",
      "Epoch 905, Loss: 0.053447263315320015, Final Batch Loss: 0.0253006499260664\n",
      "Epoch 906, Loss: 0.04398896545171738, Final Batch Loss: 0.025963762775063515\n",
      "Epoch 907, Loss: 0.10090119764208794, Final Batch Loss: 0.06066640093922615\n",
      "Epoch 908, Loss: 0.036272093653678894, Final Batch Loss: 0.020812533795833588\n",
      "Epoch 909, Loss: 0.02486391831189394, Final Batch Loss: 0.010238605551421642\n",
      "Epoch 910, Loss: 0.04320777393877506, Final Batch Loss: 0.02035817690193653\n",
      "Epoch 911, Loss: 0.0776083804666996, Final Batch Loss: 0.05096530169248581\n",
      "Epoch 912, Loss: 0.0813385508954525, Final Batch Loss: 0.06652631610631943\n",
      "Epoch 913, Loss: 0.11124583333730698, Final Batch Loss: 0.06613855063915253\n",
      "Epoch 914, Loss: 0.09240593202412128, Final Batch Loss: 0.027628982439637184\n",
      "Epoch 915, Loss: 0.10429410822689533, Final Batch Loss: 0.08413787186145782\n",
      "Epoch 916, Loss: 0.07644733414053917, Final Batch Loss: 0.036489494144916534\n",
      "Epoch 917, Loss: 0.05421246774494648, Final Batch Loss: 0.02679099142551422\n",
      "Epoch 918, Loss: 0.07414457574486732, Final Batch Loss: 0.04076404869556427\n",
      "Epoch 919, Loss: 0.05606723390519619, Final Batch Loss: 0.03272947296500206\n",
      "Epoch 920, Loss: 0.073230454698205, Final Batch Loss: 0.020224304869771004\n",
      "Epoch 921, Loss: 0.09184101223945618, Final Batch Loss: 0.03443564474582672\n",
      "Epoch 922, Loss: 0.037791136652231216, Final Batch Loss: 0.02106800489127636\n",
      "Epoch 923, Loss: 0.09489654563367367, Final Batch Loss: 0.06593305617570877\n",
      "Epoch 924, Loss: 0.10037633217871189, Final Batch Loss: 0.030287975445389748\n",
      "Epoch 925, Loss: 0.06779911741614342, Final Batch Loss: 0.036681920289993286\n",
      "Epoch 926, Loss: 0.0936048747971654, Final Batch Loss: 0.013838735409080982\n",
      "Epoch 927, Loss: 0.04158562608063221, Final Batch Loss: 0.015182256698608398\n",
      "Epoch 928, Loss: 0.08703987300395966, Final Batch Loss: 0.03925296291708946\n",
      "Epoch 929, Loss: 0.06691666413098574, Final Batch Loss: 0.051457662135362625\n",
      "Epoch 930, Loss: 0.08478662744164467, Final Batch Loss: 0.0439930222928524\n",
      "Epoch 931, Loss: 0.06283830478787422, Final Batch Loss: 0.019776728004217148\n",
      "Epoch 932, Loss: 0.042520349845290184, Final Batch Loss: 0.02157410979270935\n",
      "Epoch 933, Loss: 0.03135040029883385, Final Batch Loss: 0.01901358924806118\n",
      "Epoch 934, Loss: 0.05373649671673775, Final Batch Loss: 0.018671296536922455\n",
      "Epoch 935, Loss: 0.041639016941189766, Final Batch Loss: 0.019210800528526306\n",
      "Epoch 936, Loss: 0.05797629617154598, Final Batch Loss: 0.04017263278365135\n",
      "Epoch 937, Loss: 0.06436528824269772, Final Batch Loss: 0.015806743875145912\n",
      "Epoch 938, Loss: 0.0816376656293869, Final Batch Loss: 0.06803098320960999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939, Loss: 0.054610466584563255, Final Batch Loss: 0.02878279611468315\n",
      "Epoch 940, Loss: 0.06969497911632061, Final Batch Loss: 0.044498663395643234\n",
      "Epoch 941, Loss: 0.04357705172151327, Final Batch Loss: 0.00793102290481329\n",
      "Epoch 942, Loss: 0.12079139798879623, Final Batch Loss: 0.09212671965360641\n",
      "Epoch 943, Loss: 0.05007347837090492, Final Batch Loss: 0.02946353517472744\n",
      "Epoch 944, Loss: 0.024003533646464348, Final Batch Loss: 0.005756895989179611\n",
      "Epoch 945, Loss: 0.0649226251989603, Final Batch Loss: 0.03434722498059273\n",
      "Epoch 946, Loss: 0.036673445254564285, Final Batch Loss: 0.009724820032715797\n",
      "Epoch 947, Loss: 0.06425481289625168, Final Batch Loss: 0.037336479872465134\n",
      "Epoch 948, Loss: 0.048303812742233276, Final Batch Loss: 0.01420702412724495\n",
      "Epoch 949, Loss: 0.03542737476527691, Final Batch Loss: 0.0255521722137928\n",
      "Epoch 950, Loss: 0.029064340516924858, Final Batch Loss: 0.013267271220684052\n",
      "Epoch 951, Loss: 0.09228083118796349, Final Batch Loss: 0.028290938585996628\n",
      "Epoch 952, Loss: 0.05166769400238991, Final Batch Loss: 0.026544487103819847\n",
      "Epoch 953, Loss: 0.050755999982357025, Final Batch Loss: 0.0170857235789299\n",
      "Epoch 954, Loss: 0.025604779832065105, Final Batch Loss: 0.01721986010670662\n",
      "Epoch 955, Loss: 0.04958254937082529, Final Batch Loss: 0.03705073893070221\n",
      "Epoch 956, Loss: 0.023006166331470013, Final Batch Loss: 0.008781171403825283\n",
      "Epoch 957, Loss: 0.04509818274527788, Final Batch Loss: 0.03261716291308403\n",
      "Epoch 958, Loss: 0.05057787802070379, Final Batch Loss: 0.00823745783418417\n",
      "Epoch 959, Loss: 0.04745395854115486, Final Batch Loss: 0.023714417591691017\n",
      "Epoch 960, Loss: 0.025744669139385223, Final Batch Loss: 0.013545672409236431\n",
      "Epoch 961, Loss: 0.05054513365030289, Final Batch Loss: 0.01859593763947487\n",
      "Epoch 962, Loss: 0.017648459877818823, Final Batch Loss: 0.010613339953124523\n",
      "Epoch 963, Loss: 0.036215640604496, Final Batch Loss: 0.015896832570433617\n",
      "Epoch 964, Loss: 0.10028575919568539, Final Batch Loss: 0.09177052229642868\n",
      "Epoch 965, Loss: 0.0579279912635684, Final Batch Loss: 0.012898839078843594\n",
      "Epoch 966, Loss: 0.0369783965870738, Final Batch Loss: 0.015329948626458645\n",
      "Epoch 967, Loss: 0.03769148886203766, Final Batch Loss: 0.013641001656651497\n",
      "Epoch 968, Loss: 0.05494528450071812, Final Batch Loss: 0.03474913164973259\n",
      "Epoch 969, Loss: 0.04393378086388111, Final Batch Loss: 0.019493889063596725\n",
      "Epoch 970, Loss: 0.016070207115262747, Final Batch Loss: 0.006421391386538744\n",
      "Epoch 971, Loss: 0.05710271466523409, Final Batch Loss: 0.013954737223684788\n",
      "Epoch 972, Loss: 0.03331358637660742, Final Batch Loss: 0.014119482599198818\n",
      "Epoch 973, Loss: 0.03214526455849409, Final Batch Loss: 0.0193791463971138\n",
      "Epoch 974, Loss: 0.11001527309417725, Final Batch Loss: 0.07488991320133209\n",
      "Epoch 975, Loss: 0.058831626549363136, Final Batch Loss: 0.02317444048821926\n",
      "Epoch 976, Loss: 0.03217402892187238, Final Batch Loss: 0.005466532427817583\n",
      "Epoch 977, Loss: 0.05254996195435524, Final Batch Loss: 0.015908513218164444\n",
      "Epoch 978, Loss: 0.03500458411872387, Final Batch Loss: 0.016942108049988747\n",
      "Epoch 979, Loss: 0.06510288128629327, Final Batch Loss: 0.05824630707502365\n",
      "Epoch 980, Loss: 0.02871624380350113, Final Batch Loss: 0.017187131568789482\n",
      "Epoch 981, Loss: 0.03341034986078739, Final Batch Loss: 0.012482970952987671\n",
      "Epoch 982, Loss: 0.02797939721494913, Final Batch Loss: 0.018107648938894272\n",
      "Epoch 983, Loss: 0.046433545649051666, Final Batch Loss: 0.036812569946050644\n",
      "Epoch 984, Loss: 0.13035477697849274, Final Batch Loss: 0.11092586815357208\n",
      "Epoch 985, Loss: 0.030018698424100876, Final Batch Loss: 0.007957158610224724\n",
      "Epoch 986, Loss: 0.030855665914714336, Final Batch Loss: 0.00804870668798685\n",
      "Epoch 987, Loss: 0.0407422948628664, Final Batch Loss: 0.01726626232266426\n",
      "Epoch 988, Loss: 0.04082688130438328, Final Batch Loss: 0.011706942692399025\n",
      "Epoch 989, Loss: 0.05042320489883423, Final Batch Loss: 0.020350268110632896\n",
      "Epoch 990, Loss: 0.07584371790289879, Final Batch Loss: 0.05161217600107193\n",
      "Epoch 991, Loss: 0.03956697881221771, Final Batch Loss: 0.0160206388682127\n",
      "Epoch 992, Loss: 0.05330218840390444, Final Batch Loss: 0.04061218351125717\n",
      "Epoch 993, Loss: 0.04486696980893612, Final Batch Loss: 0.02054467424750328\n",
      "Epoch 994, Loss: 0.08250183332711458, Final Batch Loss: 0.011510019190609455\n",
      "Epoch 995, Loss: 0.027541745454072952, Final Batch Loss: 0.012192901223897934\n",
      "Epoch 996, Loss: 0.06891162134706974, Final Batch Loss: 0.03853382170200348\n",
      "Epoch 997, Loss: 0.052404576912522316, Final Batch Loss: 0.015367770567536354\n",
      "Epoch 998, Loss: 0.1479179859161377, Final Batch Loss: 0.05472991615533829\n",
      "Epoch 999, Loss: 0.047847555950284004, Final Batch Loss: 0.03156418353319168\n",
      "Epoch 1000, Loss: 0.0711139403283596, Final Batch Loss: 0.03274694085121155\n",
      "Epoch 1001, Loss: 0.026437158696353436, Final Batch Loss: 0.020765533670783043\n",
      "Epoch 1002, Loss: 0.10353262722492218, Final Batch Loss: 0.0393681675195694\n",
      "Epoch 1003, Loss: 0.07047700323164463, Final Batch Loss: 0.04239990934729576\n",
      "Epoch 1004, Loss: 0.10589867830276489, Final Batch Loss: 0.041255004703998566\n",
      "Epoch 1005, Loss: 0.14246810972690582, Final Batch Loss: 0.08971928060054779\n",
      "Epoch 1006, Loss: 0.0534763946197927, Final Batch Loss: 0.046604543924331665\n",
      "Epoch 1007, Loss: 0.08562136068940163, Final Batch Loss: 0.040341220796108246\n",
      "Epoch 1008, Loss: 0.0306781604886055, Final Batch Loss: 0.016596289351582527\n",
      "Epoch 1009, Loss: 0.1431041732430458, Final Batch Loss: 0.09369159489870071\n",
      "Epoch 1010, Loss: 0.046007225289940834, Final Batch Loss: 0.023667309433221817\n",
      "Epoch 1011, Loss: 0.06214713118970394, Final Batch Loss: 0.017954399809241295\n",
      "Epoch 1012, Loss: 0.10496310889720917, Final Batch Loss: 0.04654333367943764\n",
      "Epoch 1013, Loss: 0.03907306678593159, Final Batch Loss: 0.012219736352562904\n",
      "Epoch 1014, Loss: 0.1326185017824173, Final Batch Loss: 0.034388571977615356\n",
      "Epoch 1015, Loss: 0.0627654492855072, Final Batch Loss: 0.02401025965809822\n",
      "Epoch 1016, Loss: 0.06156548857688904, Final Batch Loss: 0.039720918983221054\n",
      "Epoch 1017, Loss: 0.08431553468108177, Final Batch Loss: 0.026931211352348328\n",
      "Epoch 1018, Loss: 0.025699985213577747, Final Batch Loss: 0.008430312387645245\n",
      "Epoch 1019, Loss: 0.09837740659713745, Final Batch Loss: 0.05785220488905907\n",
      "Epoch 1020, Loss: 0.03961989842355251, Final Batch Loss: 0.024431193247437477\n",
      "Epoch 1021, Loss: 0.03350060433149338, Final Batch Loss: 0.015601037070155144\n",
      "Epoch 1022, Loss: 0.04066019505262375, Final Batch Loss: 0.019011512398719788\n",
      "Epoch 1023, Loss: 0.04267021641135216, Final Batch Loss: 0.024540726095438004\n",
      "Epoch 1024, Loss: 0.03558754501864314, Final Batch Loss: 0.006913622375577688\n",
      "Epoch 1025, Loss: 0.05453875660896301, Final Batch Loss: 0.016715548932552338\n",
      "Epoch 1026, Loss: 0.04908667225390673, Final Batch Loss: 0.03374659642577171\n",
      "Epoch 1027, Loss: 0.038987984880805016, Final Batch Loss: 0.01943812519311905\n",
      "Epoch 1028, Loss: 0.06563069019466639, Final Batch Loss: 0.05006910860538483\n",
      "Epoch 1029, Loss: 0.024290486704558134, Final Batch Loss: 0.00661813048645854\n",
      "Epoch 1030, Loss: 0.024799278937280178, Final Batch Loss: 0.011561904102563858\n",
      "Epoch 1031, Loss: 0.02531671617180109, Final Batch Loss: 0.01165428850799799\n",
      "Epoch 1032, Loss: 0.038666133768856525, Final Batch Loss: 0.008601156063377857\n",
      "Epoch 1033, Loss: 0.040022388100624084, Final Batch Loss: 0.025790872052311897\n",
      "Epoch 1034, Loss: 0.056743230670690536, Final Batch Loss: 0.008165579289197922\n",
      "Epoch 1035, Loss: 0.05972663685679436, Final Batch Loss: 0.047060638666152954\n",
      "Epoch 1036, Loss: 0.09843924641609192, Final Batch Loss: 0.01782207190990448\n",
      "Epoch 1037, Loss: 0.028316297568380833, Final Batch Loss: 0.012013186700642109\n",
      "Epoch 1038, Loss: 0.06472003646194935, Final Batch Loss: 0.0475563108921051\n",
      "Epoch 1039, Loss: 0.052965374663472176, Final Batch Loss: 0.021043552085757256\n",
      "Epoch 1040, Loss: 0.06187799014151096, Final Batch Loss: 0.03328971192240715\n",
      "Epoch 1041, Loss: 0.0399655569344759, Final Batch Loss: 0.03126643970608711\n",
      "Epoch 1042, Loss: 0.05072090681642294, Final Batch Loss: 0.012309304438531399\n",
      "Epoch 1043, Loss: 0.11042851209640503, Final Batch Loss: 0.09262412786483765\n",
      "Epoch 1044, Loss: 0.03450390603393316, Final Batch Loss: 0.024095777422189713\n",
      "Epoch 1045, Loss: 0.07078281603753567, Final Batch Loss: 0.017060117796063423\n",
      "Epoch 1046, Loss: 0.06706318445503712, Final Batch Loss: 0.020193127915263176\n",
      "Epoch 1047, Loss: 0.04727930948138237, Final Batch Loss: 0.03374342620372772\n",
      "Epoch 1048, Loss: 0.06092623062431812, Final Batch Loss: 0.033577222377061844\n",
      "Epoch 1049, Loss: 0.07622526958584785, Final Batch Loss: 0.04091951623558998\n",
      "Epoch 1050, Loss: 0.03941836580634117, Final Batch Loss: 0.023185042664408684\n",
      "Epoch 1051, Loss: 0.0756272841244936, Final Batch Loss: 0.05324386805295944\n",
      "Epoch 1052, Loss: 0.046455858275294304, Final Batch Loss: 0.027279555797576904\n",
      "Epoch 1053, Loss: 0.03907365910708904, Final Batch Loss: 0.016863375902175903\n",
      "Epoch 1054, Loss: 0.023200764320790768, Final Batch Loss: 0.015191279351711273\n",
      "Epoch 1055, Loss: 0.04000495094805956, Final Batch Loss: 0.01213058177381754\n",
      "Epoch 1056, Loss: 0.028418391942977905, Final Batch Loss: 0.014344045892357826\n",
      "Epoch 1057, Loss: 0.039735681377351284, Final Batch Loss: 0.028091728687286377\n",
      "Epoch 1058, Loss: 0.1313301920890808, Final Batch Loss: 0.1099737212061882\n",
      "Epoch 1059, Loss: 0.11568538844585419, Final Batch Loss: 0.045352086424827576\n",
      "Epoch 1060, Loss: 0.01629426795989275, Final Batch Loss: 0.0050437478348612785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1061, Loss: 0.0239606574177742, Final Batch Loss: 0.009950454346835613\n",
      "Epoch 1062, Loss: 0.06892940774559975, Final Batch Loss: 0.03382755070924759\n",
      "Epoch 1063, Loss: 0.04113776423037052, Final Batch Loss: 0.02605036459863186\n",
      "Epoch 1064, Loss: 0.03214137814939022, Final Batch Loss: 0.023462312296032906\n",
      "Epoch 1065, Loss: 0.0340503454208374, Final Batch Loss: 0.006102593615651131\n",
      "Epoch 1066, Loss: 0.01572444336488843, Final Batch Loss: 0.007396693807095289\n",
      "Epoch 1067, Loss: 0.05330166220664978, Final Batch Loss: 0.02969169244170189\n",
      "Epoch 1068, Loss: 0.04923446290194988, Final Batch Loss: 0.020497813820838928\n",
      "Epoch 1069, Loss: 0.04848623089492321, Final Batch Loss: 0.020504171028733253\n",
      "Epoch 1070, Loss: 0.0479974839836359, Final Batch Loss: 0.020594920963048935\n",
      "Epoch 1071, Loss: 0.039764413610100746, Final Batch Loss: 0.007839715108275414\n",
      "Epoch 1072, Loss: 0.03388478513807058, Final Batch Loss: 0.01961749605834484\n",
      "Epoch 1073, Loss: 0.016549617052078247, Final Batch Loss: 0.00481511652469635\n",
      "Epoch 1074, Loss: 0.030690847896039486, Final Batch Loss: 0.012381392531096935\n",
      "Epoch 1075, Loss: 0.025226596742868423, Final Batch Loss: 0.00516369566321373\n",
      "Epoch 1076, Loss: 0.028771141543984413, Final Batch Loss: 0.008075941354036331\n",
      "Epoch 1077, Loss: 0.030121696181595325, Final Batch Loss: 0.012142623774707317\n",
      "Epoch 1078, Loss: 0.0530319157987833, Final Batch Loss: 0.04066959023475647\n",
      "Epoch 1079, Loss: 0.01791120646521449, Final Batch Loss: 0.0074237859807908535\n",
      "Epoch 1080, Loss: 0.028702599927783012, Final Batch Loss: 0.015496608801186085\n",
      "Epoch 1081, Loss: 0.05209409072995186, Final Batch Loss: 0.03461014851927757\n",
      "Epoch 1082, Loss: 0.0627029500901699, Final Batch Loss: 0.04507032036781311\n",
      "Epoch 1083, Loss: 0.11055164039134979, Final Batch Loss: 0.05833813548088074\n",
      "Epoch 1084, Loss: 0.029965552501380444, Final Batch Loss: 0.011588902212679386\n",
      "Epoch 1085, Loss: 0.037731400690972805, Final Batch Loss: 0.02955380640923977\n",
      "Epoch 1086, Loss: 0.06745007075369358, Final Batch Loss: 0.04200638830661774\n",
      "Epoch 1087, Loss: 0.03163997270166874, Final Batch Loss: 0.022734317928552628\n",
      "Epoch 1088, Loss: 0.030408541206270456, Final Batch Loss: 0.006488958839327097\n",
      "Epoch 1089, Loss: 0.047754185274243355, Final Batch Loss: 0.014914175495505333\n",
      "Epoch 1090, Loss: 0.052177528850734234, Final Batch Loss: 0.012765779159963131\n",
      "Epoch 1091, Loss: 0.04428907576948404, Final Batch Loss: 0.02903558500111103\n",
      "Epoch 1092, Loss: 0.05664213187992573, Final Batch Loss: 0.029932811856269836\n",
      "Epoch 1093, Loss: 0.08661821112036705, Final Batch Loss: 0.042536307126283646\n",
      "Epoch 1094, Loss: 0.0819869339466095, Final Batch Loss: 0.03762103617191315\n",
      "Epoch 1095, Loss: 0.05106307752430439, Final Batch Loss: 0.03415945544838905\n",
      "Epoch 1096, Loss: 0.04668942093849182, Final Batch Loss: 0.029659975320100784\n",
      "Epoch 1097, Loss: 0.08491627033799887, Final Batch Loss: 0.013887195847928524\n",
      "Epoch 1098, Loss: 0.041380634531378746, Final Batch Loss: 0.015105975791811943\n",
      "Epoch 1099, Loss: 0.042927393689751625, Final Batch Loss: 0.018075155094265938\n",
      "Epoch 1100, Loss: 0.049411240965127945, Final Batch Loss: 0.014447726309299469\n",
      "Epoch 1101, Loss: 0.019388833083212376, Final Batch Loss: 0.009957483038306236\n",
      "Epoch 1102, Loss: 0.03575447108596563, Final Batch Loss: 0.007822087965905666\n",
      "Epoch 1103, Loss: 0.03909173607826233, Final Batch Loss: 0.01656360737979412\n",
      "Epoch 1104, Loss: 0.017006910871714354, Final Batch Loss: 0.004312925506383181\n",
      "Epoch 1105, Loss: 0.06509731616824865, Final Batch Loss: 0.008988956920802593\n",
      "Epoch 1106, Loss: 0.05062626302242279, Final Batch Loss: 0.025868326425552368\n",
      "Epoch 1107, Loss: 0.05350350961089134, Final Batch Loss: 0.02196793258190155\n",
      "Epoch 1108, Loss: 0.03561672568321228, Final Batch Loss: 0.013285795226693153\n",
      "Epoch 1109, Loss: 0.01804580446332693, Final Batch Loss: 0.00628905463963747\n",
      "Epoch 1110, Loss: 0.019401622004806995, Final Batch Loss: 0.011875171214342117\n",
      "Epoch 1111, Loss: 0.023215582128614187, Final Batch Loss: 0.00434985151514411\n",
      "Epoch 1112, Loss: 0.05247216485440731, Final Batch Loss: 0.011662444099783897\n",
      "Epoch 1113, Loss: 0.05582841672003269, Final Batch Loss: 0.03602619469165802\n",
      "Epoch 1114, Loss: 0.0224536107853055, Final Batch Loss: 0.011411693878471851\n",
      "Epoch 1115, Loss: 0.020848983898758888, Final Batch Loss: 0.012074828147888184\n",
      "Epoch 1116, Loss: 0.04967587348073721, Final Batch Loss: 0.009634084068238735\n",
      "Epoch 1117, Loss: 0.017842009663581848, Final Batch Loss: 0.008088423870503902\n",
      "Epoch 1118, Loss: 0.029936962760984898, Final Batch Loss: 0.008786794729530811\n",
      "Epoch 1119, Loss: 0.023716277442872524, Final Batch Loss: 0.011324489489197731\n",
      "Epoch 1120, Loss: 0.024652938824146986, Final Batch Loss: 0.003592574503272772\n",
      "Epoch 1121, Loss: 0.04516427591443062, Final Batch Loss: 0.020288415253162384\n",
      "Epoch 1122, Loss: 0.08030296675860882, Final Batch Loss: 0.0671297013759613\n",
      "Epoch 1123, Loss: 0.04523136653006077, Final Batch Loss: 0.023782962933182716\n",
      "Epoch 1124, Loss: 0.031251237727701664, Final Batch Loss: 0.010443544946610928\n",
      "Epoch 1125, Loss: 0.05016201548278332, Final Batch Loss: 0.0265471413731575\n",
      "Epoch 1126, Loss: 0.04381280858069658, Final Batch Loss: 0.029620520770549774\n",
      "Epoch 1127, Loss: 0.043001262471079826, Final Batch Loss: 0.014970935881137848\n",
      "Epoch 1128, Loss: 0.058794924058020115, Final Batch Loss: 0.013600216247141361\n",
      "Epoch 1129, Loss: 0.03168311156332493, Final Batch Loss: 0.010367097333073616\n",
      "Epoch 1130, Loss: 0.09771660342812538, Final Batch Loss: 0.037982478737831116\n",
      "Epoch 1131, Loss: 0.06459028273820877, Final Batch Loss: 0.021127518266439438\n",
      "Epoch 1132, Loss: 0.02772802021354437, Final Batch Loss: 0.013909008353948593\n",
      "Epoch 1133, Loss: 0.05040623061358929, Final Batch Loss: 0.035950470715761185\n",
      "Epoch 1134, Loss: 0.03239059075713158, Final Batch Loss: 0.017588062211871147\n",
      "Epoch 1135, Loss: 0.01943160779774189, Final Batch Loss: 0.00479871965944767\n",
      "Epoch 1136, Loss: 0.05923336371779442, Final Batch Loss: 0.02737344801425934\n",
      "Epoch 1137, Loss: 0.03662144020199776, Final Batch Loss: 0.017389172688126564\n",
      "Epoch 1138, Loss: 0.05306984484195709, Final Batch Loss: 0.03959941118955612\n",
      "Epoch 1139, Loss: 0.039365436881780624, Final Batch Loss: 0.019837379455566406\n",
      "Epoch 1140, Loss: 0.019072837196290493, Final Batch Loss: 0.008513045497238636\n",
      "Epoch 1141, Loss: 0.018063341733068228, Final Batch Loss: 0.011961432173848152\n",
      "Epoch 1142, Loss: 0.03259570524096489, Final Batch Loss: 0.013844817876815796\n",
      "Epoch 1143, Loss: 0.02837179834023118, Final Batch Loss: 0.022412683814764023\n",
      "Epoch 1144, Loss: 0.053272711113095284, Final Batch Loss: 0.03282030671834946\n",
      "Epoch 1145, Loss: 0.04120470955967903, Final Batch Loss: 0.026790153235197067\n",
      "Epoch 1146, Loss: 0.03541506268084049, Final Batch Loss: 0.013775579631328583\n",
      "Epoch 1147, Loss: 0.023551379796117544, Final Batch Loss: 0.004887131508439779\n",
      "Epoch 1148, Loss: 0.029591528698801994, Final Batch Loss: 0.018724989145994186\n",
      "Epoch 1149, Loss: 0.01697859028354287, Final Batch Loss: 0.00564878573641181\n",
      "Epoch 1150, Loss: 0.05106714554131031, Final Batch Loss: 0.015901649370789528\n",
      "Epoch 1151, Loss: 0.03258140292018652, Final Batch Loss: 0.02032935433089733\n",
      "Epoch 1152, Loss: 0.034222048707306385, Final Batch Loss: 0.01010987814515829\n",
      "Epoch 1153, Loss: 0.0425040889531374, Final Batch Loss: 0.004406223073601723\n",
      "Epoch 1154, Loss: 0.04683617502450943, Final Batch Loss: 0.03995547816157341\n",
      "Epoch 1155, Loss: 0.02863349113613367, Final Batch Loss: 0.01037911232560873\n",
      "Epoch 1156, Loss: 0.031847900710999966, Final Batch Loss: 0.02333451248705387\n",
      "Epoch 1157, Loss: 0.035091834142804146, Final Batch Loss: 0.011574817821383476\n",
      "Epoch 1158, Loss: 0.03371012210845947, Final Batch Loss: 0.013222312554717064\n",
      "Epoch 1159, Loss: 0.038953484036028385, Final Batch Loss: 0.013764980249106884\n",
      "Epoch 1160, Loss: 0.029429331421852112, Final Batch Loss: 0.0061601027846336365\n",
      "Epoch 1161, Loss: 0.03415234759449959, Final Batch Loss: 0.01798187755048275\n",
      "Epoch 1162, Loss: 0.03230834659188986, Final Batch Loss: 0.007992382161319256\n",
      "Epoch 1163, Loss: 0.02419705782085657, Final Batch Loss: 0.01599646545946598\n",
      "Epoch 1164, Loss: 0.06106974370777607, Final Batch Loss: 0.01622406207025051\n",
      "Epoch 1165, Loss: 0.018170579569414258, Final Batch Loss: 0.014384874142706394\n",
      "Epoch 1166, Loss: 0.013664917089045048, Final Batch Loss: 0.00817705225199461\n",
      "Epoch 1167, Loss: 0.02373405103571713, Final Batch Loss: 0.0037910316605120897\n",
      "Epoch 1168, Loss: 0.04363938979804516, Final Batch Loss: 0.0178452767431736\n",
      "Epoch 1169, Loss: 0.03036556113511324, Final Batch Loss: 0.012822124175727367\n",
      "Epoch 1170, Loss: 0.03956918139010668, Final Batch Loss: 0.028032653033733368\n",
      "Epoch 1171, Loss: 0.03589764889329672, Final Batch Loss: 0.02945500984787941\n",
      "Epoch 1172, Loss: 0.014020692557096481, Final Batch Loss: 0.008102917112410069\n",
      "Epoch 1173, Loss: 0.04026828473433852, Final Batch Loss: 0.033350005745887756\n",
      "Epoch 1174, Loss: 0.027581256814301014, Final Batch Loss: 0.014307811856269836\n",
      "Epoch 1175, Loss: 0.016096184495836496, Final Batch Loss: 0.009370366111397743\n",
      "Epoch 1176, Loss: 0.03732238803058863, Final Batch Loss: 0.004568607546389103\n",
      "Epoch 1177, Loss: 0.028055901639163494, Final Batch Loss: 0.014243250712752342\n",
      "Epoch 1178, Loss: 0.03169546090066433, Final Batch Loss: 0.020240383222699165\n",
      "Epoch 1179, Loss: 0.06954728439450264, Final Batch Loss: 0.027466636151075363\n",
      "Epoch 1180, Loss: 0.03545417357236147, Final Batch Loss: 0.026742875576019287\n",
      "Epoch 1181, Loss: 0.08100429177284241, Final Batch Loss: 0.028902489691972733\n",
      "Epoch 1182, Loss: 0.03458465449512005, Final Batch Loss: 0.01923901028931141\n",
      "Epoch 1183, Loss: 0.05362595431506634, Final Batch Loss: 0.03202429413795471\n",
      "Epoch 1184, Loss: 0.044087907299399376, Final Batch Loss: 0.006926348432898521\n",
      "Epoch 1185, Loss: 0.03024925198405981, Final Batch Loss: 0.019168298691511154\n",
      "Epoch 1186, Loss: 0.035181799903512, Final Batch Loss: 0.01509421318769455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1187, Loss: 0.061662943102419376, Final Batch Loss: 0.015499857254326344\n",
      "Epoch 1188, Loss: 0.033844590187072754, Final Batch Loss: 0.02672840654850006\n",
      "Epoch 1189, Loss: 0.011507975403219461, Final Batch Loss: 0.007016883697360754\n",
      "Epoch 1190, Loss: 0.09545744955539703, Final Batch Loss: 0.033991336822509766\n",
      "Epoch 1191, Loss: 0.06441696733236313, Final Batch Loss: 0.026679374277591705\n",
      "Epoch 1192, Loss: 0.039992052130401134, Final Batch Loss: 0.008367746137082577\n",
      "Epoch 1193, Loss: 0.03206964582204819, Final Batch Loss: 0.016706541180610657\n",
      "Epoch 1194, Loss: 0.03803992085158825, Final Batch Loss: 0.015239039435982704\n",
      "Epoch 1195, Loss: 0.029306146316230297, Final Batch Loss: 0.004151639528572559\n",
      "Epoch 1196, Loss: 0.04886859189718962, Final Batch Loss: 0.03943585604429245\n",
      "Epoch 1197, Loss: 0.019464447628706694, Final Batch Loss: 0.007492599543184042\n",
      "Epoch 1198, Loss: 0.041263312101364136, Final Batch Loss: 0.019885525107383728\n",
      "Epoch 1199, Loss: 0.01533307391218841, Final Batch Loss: 0.01295263972133398\n",
      "Epoch 1200, Loss: 0.023185156285762787, Final Batch Loss: 0.014880193397402763\n",
      "Epoch 1201, Loss: 0.0467178113758564, Final Batch Loss: 0.02827472798526287\n",
      "Epoch 1202, Loss: 0.017319911159574986, Final Batch Loss: 0.00786430761218071\n",
      "Epoch 1203, Loss: 0.05435374844819307, Final Batch Loss: 0.008020144887268543\n",
      "Epoch 1204, Loss: 0.04802294075489044, Final Batch Loss: 0.02383379265666008\n",
      "Epoch 1205, Loss: 0.03592918161302805, Final Batch Loss: 0.011836222372949123\n",
      "Epoch 1206, Loss: 0.01368239801377058, Final Batch Loss: 0.005981086753308773\n",
      "Epoch 1207, Loss: 0.12235262617468834, Final Batch Loss: 0.06489139795303345\n",
      "Epoch 1208, Loss: 0.0662634875625372, Final Batch Loss: 0.04588058963418007\n",
      "Epoch 1209, Loss: 0.03027220955118537, Final Batch Loss: 0.005991015117615461\n",
      "Epoch 1210, Loss: 0.07400626875460148, Final Batch Loss: 0.04847322776913643\n",
      "Epoch 1211, Loss: 0.021217648871243, Final Batch Loss: 0.012048430740833282\n",
      "Epoch 1212, Loss: 0.017786407843232155, Final Batch Loss: 0.013797501102089882\n",
      "Epoch 1213, Loss: 0.02561012003570795, Final Batch Loss: 0.008967959322035313\n",
      "Epoch 1214, Loss: 0.014903373084962368, Final Batch Loss: 0.009168538264930248\n",
      "Epoch 1215, Loss: 0.02834635227918625, Final Batch Loss: 0.007249150425195694\n",
      "Epoch 1216, Loss: 0.026197252795100212, Final Batch Loss: 0.00971122644841671\n",
      "Epoch 1217, Loss: 0.04738815734162927, Final Batch Loss: 0.005543159786611795\n",
      "Epoch 1218, Loss: 0.034998968709260225, Final Batch Loss: 0.003992448095232248\n",
      "Epoch 1219, Loss: 0.03293688129633665, Final Batch Loss: 0.017402946949005127\n",
      "Epoch 1220, Loss: 0.04011970292776823, Final Batch Loss: 0.02995728887617588\n",
      "Epoch 1221, Loss: 0.053622701205313206, Final Batch Loss: 0.04308617487549782\n",
      "Epoch 1222, Loss: 0.022334625013172626, Final Batch Loss: 0.01406491082161665\n",
      "Epoch 1223, Loss: 0.02254424011334777, Final Batch Loss: 0.015770284458994865\n",
      "Epoch 1224, Loss: 0.03280643094331026, Final Batch Loss: 0.01111863274127245\n",
      "Epoch 1225, Loss: 0.04131350666284561, Final Batch Loss: 0.015673043206334114\n",
      "Epoch 1226, Loss: 0.031057005748152733, Final Batch Loss: 0.017603876069188118\n",
      "Epoch 1227, Loss: 0.019805277697741985, Final Batch Loss: 0.01121518388390541\n",
      "Epoch 1228, Loss: 0.01778608001768589, Final Batch Loss: 0.00794735737144947\n",
      "Epoch 1229, Loss: 0.03288243245333433, Final Batch Loss: 0.02169918268918991\n",
      "Epoch 1230, Loss: 0.02522118203341961, Final Batch Loss: 0.01587534137070179\n",
      "Epoch 1231, Loss: 0.09070280566811562, Final Batch Loss: 0.06676828116178513\n",
      "Epoch 1232, Loss: 0.04214171878993511, Final Batch Loss: 0.024145085364580154\n",
      "Epoch 1233, Loss: 0.03253280185163021, Final Batch Loss: 0.022549109533429146\n",
      "Epoch 1234, Loss: 0.03899918310344219, Final Batch Loss: 0.024049660190939903\n",
      "Epoch 1235, Loss: 0.032682723831385374, Final Batch Loss: 0.005745486821979284\n",
      "Epoch 1236, Loss: 0.07670819666236639, Final Batch Loss: 0.06572089344263077\n",
      "Epoch 1237, Loss: 0.02621716633439064, Final Batch Loss: 0.01493021659553051\n",
      "Epoch 1238, Loss: 0.02367978636175394, Final Batch Loss: 0.010405564680695534\n",
      "Epoch 1239, Loss: 0.0514526292681694, Final Batch Loss: 0.021087562665343285\n",
      "Epoch 1240, Loss: 0.02640976570546627, Final Batch Loss: 0.00611310638487339\n",
      "Epoch 1241, Loss: 0.032267216593027115, Final Batch Loss: 0.02391016110777855\n",
      "Epoch 1242, Loss: 0.014484313316643238, Final Batch Loss: 0.00654600840061903\n",
      "Epoch 1243, Loss: 0.07218441739678383, Final Batch Loss: 0.020459067076444626\n",
      "Epoch 1244, Loss: 0.05458245985209942, Final Batch Loss: 0.024301713332533836\n",
      "Epoch 1245, Loss: 0.06615363992750645, Final Batch Loss: 0.049391452223062515\n",
      "Epoch 1246, Loss: 0.01984820980578661, Final Batch Loss: 0.010646410286426544\n",
      "Epoch 1247, Loss: 0.01512821763753891, Final Batch Loss: 0.010072666220366955\n",
      "Epoch 1248, Loss: 0.05243373475968838, Final Batch Loss: 0.017425337806344032\n",
      "Epoch 1249, Loss: 0.03226045053452253, Final Batch Loss: 0.009985084645450115\n",
      "Epoch 1250, Loss: 0.04083076911047101, Final Batch Loss: 0.035499949008226395\n",
      "Epoch 1251, Loss: 0.05407201126217842, Final Batch Loss: 0.026386596262454987\n",
      "Epoch 1252, Loss: 0.025340246502310038, Final Batch Loss: 0.018231980502605438\n",
      "Epoch 1253, Loss: 0.012963394867256284, Final Batch Loss: 0.009831288829445839\n",
      "Epoch 1254, Loss: 0.0507521484978497, Final Batch Loss: 0.006960772443562746\n",
      "Epoch 1255, Loss: 0.01224652212113142, Final Batch Loss: 0.005615898873656988\n",
      "Epoch 1256, Loss: 0.043664091266691685, Final Batch Loss: 0.011481010355055332\n",
      "Epoch 1257, Loss: 0.05746418982744217, Final Batch Loss: 0.04190089926123619\n",
      "Epoch 1258, Loss: 0.02159472042694688, Final Batch Loss: 0.005113684106618166\n",
      "Epoch 1259, Loss: 0.06258013471961021, Final Batch Loss: 0.013514269143342972\n",
      "Epoch 1260, Loss: 0.06506660766899586, Final Batch Loss: 0.046055637300014496\n",
      "Epoch 1261, Loss: 0.021522320341318846, Final Batch Loss: 0.01455662027001381\n",
      "Epoch 1262, Loss: 0.09636787325143814, Final Batch Loss: 0.06821233034133911\n",
      "Epoch 1263, Loss: 0.03309307433664799, Final Batch Loss: 0.006701325997710228\n",
      "Epoch 1264, Loss: 0.051464613527059555, Final Batch Loss: 0.027332047000527382\n",
      "Epoch 1265, Loss: 0.02365144668146968, Final Batch Loss: 0.007479993160814047\n",
      "Epoch 1266, Loss: 0.054659292101860046, Final Batch Loss: 0.02715299278497696\n",
      "Epoch 1267, Loss: 0.09233440458774567, Final Batch Loss: 0.06653618812561035\n",
      "Epoch 1268, Loss: 0.10137735679745674, Final Batch Loss: 0.04044768214225769\n",
      "Epoch 1269, Loss: 0.034736599773168564, Final Batch Loss: 0.008421679958701134\n",
      "Epoch 1270, Loss: 0.05591203272342682, Final Batch Loss: 0.047043923288583755\n",
      "Epoch 1271, Loss: 0.04666560608893633, Final Batch Loss: 0.012042832560837269\n",
      "Epoch 1272, Loss: 0.030104472301900387, Final Batch Loss: 0.007446856237947941\n",
      "Epoch 1273, Loss: 0.03124737972393632, Final Batch Loss: 0.024361880496144295\n",
      "Epoch 1274, Loss: 0.02914178604260087, Final Batch Loss: 0.007101051975041628\n",
      "Epoch 1275, Loss: 0.05079085985198617, Final Batch Loss: 0.00756077328696847\n",
      "Epoch 1276, Loss: 0.06720144674181938, Final Batch Loss: 0.052385903894901276\n",
      "Epoch 1277, Loss: 0.023490223102271557, Final Batch Loss: 0.015591603703796864\n",
      "Epoch 1278, Loss: 0.015482920687645674, Final Batch Loss: 0.010044757276773453\n",
      "Epoch 1279, Loss: 0.027330893091857433, Final Batch Loss: 0.020405171439051628\n",
      "Epoch 1280, Loss: 0.01665059756487608, Final Batch Loss: 0.009450804442167282\n",
      "Epoch 1281, Loss: 0.02649701666086912, Final Batch Loss: 0.012391074560582638\n",
      "Epoch 1282, Loss: 0.0345603646710515, Final Batch Loss: 0.022043632343411446\n",
      "Epoch 1283, Loss: 0.014950424898415804, Final Batch Loss: 0.0039028353057801723\n",
      "Epoch 1284, Loss: 0.024964007548987865, Final Batch Loss: 0.020877445116639137\n",
      "Epoch 1285, Loss: 0.024038443341851234, Final Batch Loss: 0.014141366817057133\n",
      "Epoch 1286, Loss: 0.059388574212789536, Final Batch Loss: 0.04892802983522415\n",
      "Epoch 1287, Loss: 0.016570447478443384, Final Batch Loss: 0.006608992349356413\n",
      "Epoch 1288, Loss: 0.015287690330296755, Final Batch Loss: 0.007298068609088659\n",
      "Epoch 1289, Loss: 0.03395577659830451, Final Batch Loss: 0.006829993333667517\n",
      "Epoch 1290, Loss: 0.009887655265629292, Final Batch Loss: 0.005351871717721224\n",
      "Epoch 1291, Loss: 0.019065203610807657, Final Batch Loss: 0.012875976040959358\n",
      "Epoch 1292, Loss: 0.008254799293354154, Final Batch Loss: 0.004787303041666746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1293, Loss: 0.07370530720800161, Final Batch Loss: 0.006970404647290707\n",
      "Epoch 1294, Loss: 0.015083003090694547, Final Batch Loss: 0.012777246534824371\n",
      "Epoch 1295, Loss: 0.04814503854140639, Final Batch Loss: 0.006517705973237753\n",
      "Epoch 1296, Loss: 0.04067494720220566, Final Batch Loss: 0.009954562410712242\n",
      "Epoch 1297, Loss: 0.017669836059212685, Final Batch Loss: 0.0068915365263819695\n",
      "Epoch 1298, Loss: 0.035740879364311695, Final Batch Loss: 0.003095773048698902\n",
      "Epoch 1299, Loss: 0.07749783433973789, Final Batch Loss: 0.028171660378575325\n",
      "Epoch 1300, Loss: 0.06242752447724342, Final Batch Loss: 0.03732488676905632\n",
      "Epoch 1301, Loss: 0.041215263307094574, Final Batch Loss: 0.01064310409128666\n",
      "Epoch 1302, Loss: 0.021216594614088535, Final Batch Loss: 0.012316258624196053\n",
      "Epoch 1303, Loss: 0.06293832696974277, Final Batch Loss: 0.033520884811878204\n",
      "Epoch 1304, Loss: 0.027278777677565813, Final Batch Loss: 0.0069122337736189365\n",
      "Epoch 1305, Loss: 0.01728944480419159, Final Batch Loss: 0.00781854148954153\n",
      "Epoch 1306, Loss: 0.02486412599682808, Final Batch Loss: 0.016313904896378517\n",
      "Epoch 1307, Loss: 0.08152662590146065, Final Batch Loss: 0.0034196190536022186\n",
      "Epoch 1308, Loss: 0.05379635654389858, Final Batch Loss: 0.016672084107995033\n",
      "Epoch 1309, Loss: 0.07689259946346283, Final Batch Loss: 0.027819178998470306\n",
      "Epoch 1310, Loss: 0.05213851481676102, Final Batch Loss: 0.03915213793516159\n",
      "Epoch 1311, Loss: 0.02265442768111825, Final Batch Loss: 0.0067967441864311695\n",
      "Epoch 1312, Loss: 0.012744518462568521, Final Batch Loss: 0.005960824899375439\n",
      "Epoch 1313, Loss: 0.01626400020904839, Final Batch Loss: 0.0018330051098018885\n",
      "Epoch 1314, Loss: 0.05524811055511236, Final Batch Loss: 0.045770540833473206\n",
      "Epoch 1315, Loss: 0.028745634481310844, Final Batch Loss: 0.02286490984261036\n",
      "Epoch 1316, Loss: 0.07171451486647129, Final Batch Loss: 0.01939520798623562\n",
      "Epoch 1317, Loss: 0.02442680113017559, Final Batch Loss: 0.011152651160955429\n",
      "Epoch 1318, Loss: 0.028586892411112785, Final Batch Loss: 0.01641579531133175\n",
      "Epoch 1319, Loss: 0.05403115041553974, Final Batch Loss: 0.03794024512171745\n",
      "Epoch 1320, Loss: 0.058327642269432545, Final Batch Loss: 0.004324783571064472\n",
      "Epoch 1321, Loss: 0.11289216624572873, Final Batch Loss: 0.10563990473747253\n",
      "Epoch 1322, Loss: 0.03511156793683767, Final Batch Loss: 0.01156072597950697\n",
      "Epoch 1323, Loss: 0.025280768983066082, Final Batch Loss: 0.018376242369413376\n",
      "Epoch 1324, Loss: 0.12045303732156754, Final Batch Loss: 0.089918352663517\n",
      "Epoch 1325, Loss: 0.03817954193800688, Final Batch Loss: 0.014484421350061893\n",
      "Epoch 1326, Loss: 0.09527360089123249, Final Batch Loss: 0.06891056150197983\n",
      "Epoch 1327, Loss: 0.15260737389326096, Final Batch Loss: 0.020787887275218964\n",
      "Epoch 1328, Loss: 0.05527570191770792, Final Batch Loss: 0.04995622858405113\n",
      "Epoch 1329, Loss: 0.06284065917134285, Final Batch Loss: 0.006937995553016663\n",
      "Epoch 1330, Loss: 0.022674200125038624, Final Batch Loss: 0.012330600991845131\n",
      "Epoch 1331, Loss: 0.03634756896644831, Final Batch Loss: 0.021634463220834732\n",
      "Epoch 1332, Loss: 0.08891680650413036, Final Batch Loss: 0.02463863603770733\n",
      "Epoch 1333, Loss: 0.04783833213150501, Final Batch Loss: 0.016752207651734352\n",
      "Epoch 1334, Loss: 0.08159802854061127, Final Batch Loss: 0.04828358814120293\n",
      "Epoch 1335, Loss: 0.0271616131067276, Final Batch Loss: 0.017239807173609734\n",
      "Epoch 1336, Loss: 0.024602351244539022, Final Batch Loss: 0.007122156675904989\n",
      "Epoch 1337, Loss: 0.0282518295571208, Final Batch Loss: 0.006047415547072887\n",
      "Epoch 1338, Loss: 0.07303814217448235, Final Batch Loss: 0.03164197877049446\n",
      "Epoch 1339, Loss: 0.027574914507567883, Final Batch Loss: 0.010683760978281498\n",
      "Epoch 1340, Loss: 0.05947226472198963, Final Batch Loss: 0.004958679899573326\n",
      "Epoch 1341, Loss: 0.044579213950783014, Final Batch Loss: 0.004181495402008295\n",
      "Epoch 1342, Loss: 0.05680895410478115, Final Batch Loss: 0.03242978826165199\n",
      "Epoch 1343, Loss: 0.03669294714927673, Final Batch Loss: 0.02378886751830578\n",
      "Epoch 1344, Loss: 0.019113927148282528, Final Batch Loss: 0.010471269488334656\n",
      "Epoch 1345, Loss: 0.023609403520822525, Final Batch Loss: 0.004469878971576691\n",
      "Epoch 1346, Loss: 0.02393740974366665, Final Batch Loss: 0.012348813004791737\n",
      "Epoch 1347, Loss: 0.020732063334435225, Final Batch Loss: 0.004463439341634512\n",
      "Epoch 1348, Loss: 0.02059387299232185, Final Batch Loss: 0.0031325414311140776\n",
      "Epoch 1349, Loss: 0.04206813592463732, Final Batch Loss: 0.032145604491233826\n",
      "Epoch 1350, Loss: 0.041141131427139044, Final Batch Loss: 0.003966787364333868\n",
      "Epoch 1351, Loss: 0.03103104722686112, Final Batch Loss: 0.002585307927802205\n",
      "Epoch 1352, Loss: 0.04872564785182476, Final Batch Loss: 0.03275737538933754\n",
      "Epoch 1353, Loss: 0.03792779566720128, Final Batch Loss: 0.005435221362859011\n",
      "Epoch 1354, Loss: 0.01073016133159399, Final Batch Loss: 0.0057481881231069565\n",
      "Epoch 1355, Loss: 0.02091592736542225, Final Batch Loss: 0.005187435075640678\n",
      "Epoch 1356, Loss: 0.019292535725980997, Final Batch Loss: 0.006443919148296118\n",
      "Epoch 1357, Loss: 0.021339083090424538, Final Batch Loss: 0.003047039732336998\n",
      "Epoch 1358, Loss: 0.034483037889003754, Final Batch Loss: 0.014154566451907158\n",
      "Epoch 1359, Loss: 0.03953084070235491, Final Batch Loss: 0.027268562465906143\n",
      "Epoch 1360, Loss: 0.014118575025349855, Final Batch Loss: 0.008800157345831394\n",
      "Epoch 1361, Loss: 0.02742957230657339, Final Batch Loss: 0.009633305482566357\n",
      "Epoch 1362, Loss: 0.020495889708399773, Final Batch Loss: 0.00821479968726635\n",
      "Epoch 1363, Loss: 0.019959777127951384, Final Batch Loss: 0.014437073841691017\n",
      "Epoch 1364, Loss: 0.020005062222480774, Final Batch Loss: 0.014360276982188225\n",
      "Epoch 1365, Loss: 0.017282426822930574, Final Batch Loss: 0.006246411707252264\n",
      "Epoch 1366, Loss: 0.04926552437245846, Final Batch Loss: 0.038823045790195465\n",
      "Epoch 1367, Loss: 0.02001768071204424, Final Batch Loss: 0.01173347420990467\n",
      "Epoch 1368, Loss: 0.013688765000551939, Final Batch Loss: 0.0050077964551746845\n",
      "Epoch 1369, Loss: 0.013592422474175692, Final Batch Loss: 0.00395750580355525\n",
      "Epoch 1370, Loss: 0.04322029743343592, Final Batch Loss: 0.010605317540466785\n",
      "Epoch 1371, Loss: 0.02997317397966981, Final Batch Loss: 0.023181797936558723\n",
      "Epoch 1372, Loss: 0.015037503559142351, Final Batch Loss: 0.00714763393625617\n",
      "Epoch 1373, Loss: 0.05862264521420002, Final Batch Loss: 0.018609648570418358\n",
      "Epoch 1374, Loss: 0.03657856723293662, Final Batch Loss: 0.005827443208545446\n",
      "Epoch 1375, Loss: 0.05061444267630577, Final Batch Loss: 0.03547091782093048\n",
      "Epoch 1376, Loss: 0.014083749148994684, Final Batch Loss: 0.004550484474748373\n",
      "Epoch 1377, Loss: 0.053457675501704216, Final Batch Loss: 0.036337293684482574\n",
      "Epoch 1378, Loss: 0.07000265270471573, Final Batch Loss: 0.025762323290109634\n",
      "Epoch 1379, Loss: 0.027466941624879837, Final Batch Loss: 0.012255375273525715\n",
      "Epoch 1380, Loss: 0.04954964481294155, Final Batch Loss: 0.030874114483594894\n",
      "Epoch 1381, Loss: 0.02817556355148554, Final Batch Loss: 0.02291218563914299\n",
      "Epoch 1382, Loss: 0.06647884007543325, Final Batch Loss: 0.009688212536275387\n",
      "Epoch 1383, Loss: 0.0474267303943634, Final Batch Loss: 0.022772962227463722\n",
      "Epoch 1384, Loss: 0.06031834799796343, Final Batch Loss: 0.00824823323637247\n",
      "Epoch 1385, Loss: 0.022988687036558986, Final Batch Loss: 0.003669512225314975\n",
      "Epoch 1386, Loss: 0.03351473342627287, Final Batch Loss: 0.018344148993492126\n",
      "Epoch 1387, Loss: 0.058456551283597946, Final Batch Loss: 0.03980984538793564\n",
      "Epoch 1388, Loss: 0.04654958378523588, Final Batch Loss: 0.032939884811639786\n",
      "Epoch 1389, Loss: 0.09713718108832836, Final Batch Loss: 0.014687856659293175\n",
      "Epoch 1390, Loss: 0.027281581657007337, Final Batch Loss: 0.002182732569053769\n",
      "Epoch 1391, Loss: 0.03467401769012213, Final Batch Loss: 0.005092780105769634\n",
      "Epoch 1392, Loss: 0.05749603174626827, Final Batch Loss: 0.019806841388344765\n",
      "Epoch 1393, Loss: 0.03049281472340226, Final Batch Loss: 0.007799987215548754\n",
      "Epoch 1394, Loss: 0.021954386262223125, Final Batch Loss: 0.01832893304526806\n",
      "Epoch 1395, Loss: 0.08520082756876945, Final Batch Loss: 0.04098029434680939\n",
      "Epoch 1396, Loss: 0.010224462021142244, Final Batch Loss: 0.0053601618856191635\n",
      "Epoch 1397, Loss: 0.0529429130256176, Final Batch Loss: 0.02952817641198635\n",
      "Epoch 1398, Loss: 0.025055259931832552, Final Batch Loss: 0.007056745234876871\n",
      "Epoch 1399, Loss: 0.024964074604213238, Final Batch Loss: 0.01634066179394722\n",
      "Epoch 1400, Loss: 0.05736213829368353, Final Batch Loss: 0.04263904318213463\n",
      "Epoch 1401, Loss: 0.03185133170336485, Final Batch Loss: 0.02171424590051174\n",
      "Epoch 1402, Loss: 0.04000694118440151, Final Batch Loss: 0.026457425206899643\n",
      "Epoch 1403, Loss: 0.06548880785703659, Final Batch Loss: 0.033992573618888855\n",
      "Epoch 1404, Loss: 0.05261807609349489, Final Batch Loss: 0.03872501850128174\n",
      "Epoch 1405, Loss: 0.04892537323758006, Final Batch Loss: 0.0429423488676548\n",
      "Epoch 1406, Loss: 0.036238882690668106, Final Batch Loss: 0.030069828033447266\n",
      "Epoch 1407, Loss: 0.02210799092426896, Final Batch Loss: 0.005393907893449068\n",
      "Epoch 1408, Loss: 0.06371740577742457, Final Batch Loss: 0.05783182755112648\n",
      "Epoch 1409, Loss: 0.01816026121377945, Final Batch Loss: 0.0068044476211071014\n",
      "Epoch 1410, Loss: 0.118032181635499, Final Batch Loss: 0.02993166632950306\n",
      "Epoch 1411, Loss: 0.043214361649006605, Final Batch Loss: 0.005104694049805403\n",
      "Epoch 1412, Loss: 0.026002665050327778, Final Batch Loss: 0.010174994356930256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1413, Loss: 0.03496068995445967, Final Batch Loss: 0.02165621519088745\n",
      "Epoch 1414, Loss: 0.08641282096505165, Final Batch Loss: 0.026036515831947327\n",
      "Epoch 1415, Loss: 0.033019039779901505, Final Batch Loss: 0.021454034373164177\n",
      "Epoch 1416, Loss: 0.042072657495737076, Final Batch Loss: 0.0253438837826252\n",
      "Epoch 1417, Loss: 0.05363749712705612, Final Batch Loss: 0.006426181644201279\n",
      "Epoch 1418, Loss: 0.061543624848127365, Final Batch Loss: 0.05454447120428085\n",
      "Epoch 1419, Loss: 0.018606973811984062, Final Batch Loss: 0.005714378319680691\n",
      "Epoch 1420, Loss: 0.07039209827780724, Final Batch Loss: 0.04258951544761658\n",
      "Epoch 1421, Loss: 0.06740891933441162, Final Batch Loss: 0.021289244294166565\n",
      "Epoch 1422, Loss: 0.014096553903073072, Final Batch Loss: 0.004558283369988203\n",
      "Epoch 1423, Loss: 0.05516308173537254, Final Batch Loss: 0.046777818351984024\n",
      "Epoch 1424, Loss: 0.02968757040798664, Final Batch Loss: 0.0032734572887420654\n",
      "Epoch 1425, Loss: 0.08965682797133923, Final Batch Loss: 0.06231985241174698\n",
      "Epoch 1426, Loss: 0.0757604893296957, Final Batch Loss: 0.016069678589701653\n",
      "Epoch 1427, Loss: 0.08050446026027203, Final Batch Loss: 0.06773760169744492\n",
      "Epoch 1428, Loss: 0.04427527729421854, Final Batch Loss: 0.03187233954668045\n",
      "Epoch 1429, Loss: 0.025907344184815884, Final Batch Loss: 0.014362221583724022\n",
      "Epoch 1430, Loss: 0.07181121036410332, Final Batch Loss: 0.01693909242749214\n",
      "Epoch 1431, Loss: 0.028550712391734123, Final Batch Loss: 0.00859164446592331\n",
      "Epoch 1432, Loss: 0.028551088646054268, Final Batch Loss: 0.00876428559422493\n",
      "Epoch 1433, Loss: 0.0363909681327641, Final Batch Loss: 0.030571790412068367\n",
      "Epoch 1434, Loss: 0.029848743230104446, Final Batch Loss: 0.013047335669398308\n",
      "Epoch 1435, Loss: 0.08535834029316902, Final Batch Loss: 0.04254878684878349\n",
      "Epoch 1436, Loss: 0.032787872944027185, Final Batch Loss: 0.026253588497638702\n",
      "Epoch 1437, Loss: 0.03568831365555525, Final Batch Loss: 0.023334821686148643\n",
      "Epoch 1438, Loss: 0.04160427721217275, Final Batch Loss: 0.00391425983980298\n",
      "Epoch 1439, Loss: 0.04537207819521427, Final Batch Loss: 0.022713668644428253\n",
      "Epoch 1440, Loss: 0.025594680570065975, Final Batch Loss: 0.012206761166453362\n",
      "Epoch 1441, Loss: 0.024910516571253538, Final Batch Loss: 0.004460190888494253\n",
      "Epoch 1442, Loss: 0.03399539180099964, Final Batch Loss: 0.0210874043405056\n",
      "Epoch 1443, Loss: 0.09881005808711052, Final Batch Loss: 0.06230926513671875\n",
      "Epoch 1444, Loss: 0.032981203868985176, Final Batch Loss: 0.020645977929234505\n",
      "Epoch 1445, Loss: 0.06984852068126202, Final Batch Loss: 0.026480240747332573\n",
      "Epoch 1446, Loss: 0.059486149810254574, Final Batch Loss: 0.01453747134655714\n",
      "Epoch 1447, Loss: 0.05508717708289623, Final Batch Loss: 0.012227585539221764\n",
      "Epoch 1448, Loss: 0.00922231050208211, Final Batch Loss: 0.0044589778408408165\n",
      "Epoch 1449, Loss: 0.08532240986824036, Final Batch Loss: 0.0503254234790802\n",
      "Epoch 1450, Loss: 0.09494626335799694, Final Batch Loss: 0.025139926001429558\n",
      "Epoch 1451, Loss: 0.03292886167764664, Final Batch Loss: 0.015535673126578331\n",
      "Epoch 1452, Loss: 0.03133035358041525, Final Batch Loss: 0.01315514836460352\n",
      "Epoch 1453, Loss: 0.04829167667776346, Final Batch Loss: 0.010582529939711094\n",
      "Epoch 1454, Loss: 0.059642449021339417, Final Batch Loss: 0.02669290453195572\n",
      "Epoch 1455, Loss: 0.044528061524033546, Final Batch Loss: 0.0075052883476018906\n",
      "Epoch 1456, Loss: 0.03228182811290026, Final Batch Loss: 0.01143112126737833\n",
      "Epoch 1457, Loss: 0.08382543548941612, Final Batch Loss: 0.02753002941608429\n",
      "Epoch 1458, Loss: 0.02751485724002123, Final Batch Loss: 0.004959109239280224\n",
      "Epoch 1459, Loss: 0.0093675022944808, Final Batch Loss: 0.004317732062190771\n",
      "Epoch 1460, Loss: 0.043769353069365025, Final Batch Loss: 0.029780643060803413\n",
      "Epoch 1461, Loss: 0.015221217880025506, Final Batch Loss: 0.002631004201248288\n",
      "Epoch 1462, Loss: 0.03298007883131504, Final Batch Loss: 0.012059612199664116\n",
      "Epoch 1463, Loss: 0.0716131953522563, Final Batch Loss: 0.05675101652741432\n",
      "Epoch 1464, Loss: 0.047194380313158035, Final Batch Loss: 0.021050123497843742\n",
      "Epoch 1465, Loss: 0.03466640366241336, Final Batch Loss: 0.0066524832509458065\n",
      "Epoch 1466, Loss: 0.01343339029699564, Final Batch Loss: 0.005054270848631859\n",
      "Epoch 1467, Loss: 0.021173521410673857, Final Batch Loss: 0.016500381752848625\n",
      "Epoch 1468, Loss: 0.02986128441989422, Final Batch Loss: 0.02339343912899494\n",
      "Epoch 1469, Loss: 0.028228914365172386, Final Batch Loss: 0.012386899441480637\n",
      "Epoch 1470, Loss: 0.012160858605057001, Final Batch Loss: 0.004412592854350805\n",
      "Epoch 1471, Loss: 0.037913207430392504, Final Batch Loss: 0.007487407419830561\n",
      "Epoch 1472, Loss: 0.04785145726054907, Final Batch Loss: 0.0071695102378726006\n",
      "Epoch 1473, Loss: 0.03384559787809849, Final Batch Loss: 0.017701316624879837\n",
      "Epoch 1474, Loss: 0.03999051824212074, Final Batch Loss: 0.03114861436188221\n",
      "Epoch 1475, Loss: 0.02819314505904913, Final Batch Loss: 0.009250861592590809\n",
      "Epoch 1476, Loss: 0.03787299105897546, Final Batch Loss: 0.006769527215510607\n",
      "Epoch 1477, Loss: 0.03937231656163931, Final Batch Loss: 0.029156461358070374\n",
      "Epoch 1478, Loss: 0.009034712333232164, Final Batch Loss: 0.002358864527195692\n",
      "Epoch 1479, Loss: 0.011182716116309166, Final Batch Loss: 0.0047092619352042675\n",
      "Epoch 1480, Loss: 0.04158390872180462, Final Batch Loss: 0.029038989916443825\n",
      "Epoch 1481, Loss: 0.019916817545890808, Final Batch Loss: 0.010795089416205883\n",
      "Epoch 1482, Loss: 0.029151231981813908, Final Batch Loss: 0.017039082944393158\n",
      "Epoch 1483, Loss: 0.01816867687739432, Final Batch Loss: 0.003800060832872987\n",
      "Epoch 1484, Loss: 0.04194050654768944, Final Batch Loss: 0.017153626307845116\n",
      "Epoch 1485, Loss: 0.030702280811965466, Final Batch Loss: 0.02512497268617153\n",
      "Epoch 1486, Loss: 0.06248828209936619, Final Batch Loss: 0.045297130942344666\n",
      "Epoch 1487, Loss: 0.04836604278534651, Final Batch Loss: 0.04083620384335518\n",
      "Epoch 1488, Loss: 0.016940725035965443, Final Batch Loss: 0.010263858363032341\n",
      "Epoch 1489, Loss: 0.0680377334356308, Final Batch Loss: 0.03546978160738945\n",
      "Epoch 1490, Loss: 0.012113319244235754, Final Batch Loss: 0.003063920419663191\n",
      "Epoch 1491, Loss: 0.02862961497157812, Final Batch Loss: 0.00631121639162302\n",
      "Epoch 1492, Loss: 0.021619310602545738, Final Batch Loss: 0.010957739315927029\n",
      "Epoch 1493, Loss: 0.030363617464900017, Final Batch Loss: 0.013670584186911583\n",
      "Epoch 1494, Loss: 0.03597540408372879, Final Batch Loss: 0.012314382940530777\n",
      "Epoch 1495, Loss: 0.0873113814741373, Final Batch Loss: 0.08148535341024399\n",
      "Epoch 1496, Loss: 0.022638384718447924, Final Batch Loss: 0.018374601379036903\n",
      "Epoch 1497, Loss: 0.05574346147477627, Final Batch Loss: 0.027540605515241623\n",
      "Epoch 1498, Loss: 0.11149916239082813, Final Batch Loss: 0.031145842745900154\n",
      "Epoch 1499, Loss: 0.09109229501336813, Final Batch Loss: 0.01475511398166418\n",
      "Epoch 1500, Loss: 0.08135216683149338, Final Batch Loss: 0.047932036221027374\n",
      "Epoch 1501, Loss: 0.042670488357543945, Final Batch Loss: 0.01396925374865532\n",
      "Epoch 1502, Loss: 0.045285480096936226, Final Batch Loss: 0.034195274114608765\n",
      "Epoch 1503, Loss: 0.052887046709656715, Final Batch Loss: 0.006973499432206154\n",
      "Epoch 1504, Loss: 0.04292561346665025, Final Batch Loss: 0.03534827381372452\n",
      "Epoch 1505, Loss: 0.03710467182099819, Final Batch Loss: 0.023577304556965828\n",
      "Epoch 1506, Loss: 0.023167427629232407, Final Batch Loss: 0.013835822232067585\n",
      "Epoch 1507, Loss: 0.03218071721494198, Final Batch Loss: 0.016230255365371704\n",
      "Epoch 1508, Loss: 0.01586842257529497, Final Batch Loss: 0.011244448833167553\n",
      "Epoch 1509, Loss: 0.03159330692142248, Final Batch Loss: 0.01754065230488777\n",
      "Epoch 1510, Loss: 0.02259010775014758, Final Batch Loss: 0.005490067880600691\n",
      "Epoch 1511, Loss: 0.035957127809524536, Final Batch Loss: 0.020278610289096832\n",
      "Epoch 1512, Loss: 0.019828523509204388, Final Batch Loss: 0.006475203670561314\n",
      "Epoch 1513, Loss: 0.025526121258735657, Final Batch Loss: 0.01974331960082054\n",
      "Epoch 1514, Loss: 0.027906524017453194, Final Batch Loss: 0.014768552966415882\n",
      "Epoch 1515, Loss: 0.020601521246135235, Final Batch Loss: 0.010121166706085205\n",
      "Epoch 1516, Loss: 0.062022821977734566, Final Batch Loss: 0.045772045850753784\n",
      "Epoch 1517, Loss: 0.09639140777289867, Final Batch Loss: 0.06979893893003464\n",
      "Epoch 1518, Loss: 0.033943804912269115, Final Batch Loss: 0.022187763825058937\n",
      "Epoch 1519, Loss: 0.048463978338986635, Final Batch Loss: 0.04203019291162491\n",
      "Epoch 1520, Loss: 0.026611248962581158, Final Batch Loss: 0.020049307495355606\n",
      "Epoch 1521, Loss: 0.02805429045110941, Final Batch Loss: 0.01241303514689207\n",
      "Epoch 1522, Loss: 0.02546615176834166, Final Batch Loss: 0.0016348494682461023\n",
      "Epoch 1523, Loss: 0.049673729576170444, Final Batch Loss: 0.007145480252802372\n",
      "Epoch 1524, Loss: 0.02706765104085207, Final Batch Loss: 0.01755254901945591\n",
      "Epoch 1525, Loss: 0.012348615564405918, Final Batch Loss: 0.005470470059663057\n",
      "Epoch 1526, Loss: 0.0267976438626647, Final Batch Loss: 0.009790363721549511\n",
      "Epoch 1527, Loss: 0.024683307856321335, Final Batch Loss: 0.014934942126274109\n",
      "Epoch 1528, Loss: 0.011836554855108261, Final Batch Loss: 0.006900439038872719\n",
      "Epoch 1529, Loss: 0.015407247003167868, Final Batch Loss: 0.006214569788426161\n",
      "Epoch 1530, Loss: 0.016990124713629484, Final Batch Loss: 0.007807958405464888\n",
      "Epoch 1531, Loss: 0.034579855389893055, Final Batch Loss: 0.013329206965863705\n",
      "Epoch 1532, Loss: 0.0658017760142684, Final Batch Loss: 0.0517844595015049\n",
      "Epoch 1533, Loss: 0.018366820877417922, Final Batch Loss: 0.015381695702672005\n",
      "Epoch 1534, Loss: 0.03311276435852051, Final Batch Loss: 0.015327304601669312\n",
      "Epoch 1535, Loss: 0.011189292185008526, Final Batch Loss: 0.003928379621356726\n",
      "Epoch 1536, Loss: 0.017508363351225853, Final Batch Loss: 0.008323932997882366\n",
      "Epoch 1537, Loss: 0.04536334425210953, Final Batch Loss: 0.009375900030136108\n",
      "Epoch 1538, Loss: 0.019755234010517597, Final Batch Loss: 0.008976846933364868\n",
      "Epoch 1539, Loss: 0.04254682920873165, Final Batch Loss: 0.02337680011987686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1540, Loss: 0.0189218963496387, Final Batch Loss: 0.0036297286860644817\n",
      "Epoch 1541, Loss: 0.025662047788500786, Final Batch Loss: 0.011910717934370041\n",
      "Epoch 1542, Loss: 0.02512137684971094, Final Batch Loss: 0.006971585564315319\n",
      "Epoch 1543, Loss: 0.01936423499137163, Final Batch Loss: 0.008951195515692234\n",
      "Epoch 1544, Loss: 0.021999293006956577, Final Batch Loss: 0.0027403300628066063\n",
      "Epoch 1545, Loss: 0.021090571768581867, Final Batch Loss: 0.012372144497931004\n",
      "Epoch 1546, Loss: 0.020950220990926027, Final Batch Loss: 0.004129741806536913\n",
      "Epoch 1547, Loss: 0.051734588108956814, Final Batch Loss: 0.008544391952455044\n",
      "Epoch 1548, Loss: 0.020832544192671776, Final Batch Loss: 0.01622934639453888\n",
      "Epoch 1549, Loss: 0.01842393446713686, Final Batch Loss: 0.007075749337673187\n",
      "Epoch 1550, Loss: 0.02501213736832142, Final Batch Loss: 0.010504036210477352\n",
      "Epoch 1551, Loss: 0.04196875588968396, Final Batch Loss: 0.0054201711900532246\n",
      "Epoch 1552, Loss: 0.029586408520117402, Final Batch Loss: 0.0035563039127737284\n",
      "Epoch 1553, Loss: 0.036303951404988766, Final Batch Loss: 0.031242655590176582\n",
      "Epoch 1554, Loss: 0.06822076067328453, Final Batch Loss: 0.02422928437590599\n",
      "Epoch 1555, Loss: 0.032240583561360836, Final Batch Loss: 0.020332731306552887\n",
      "Epoch 1556, Loss: 0.03222709707915783, Final Batch Loss: 0.021339552477002144\n",
      "Epoch 1557, Loss: 0.08999352436512709, Final Batch Loss: 0.011252197436988354\n",
      "Epoch 1558, Loss: 0.021547188982367516, Final Batch Loss: 0.013624553568661213\n",
      "Epoch 1559, Loss: 0.024572419933974743, Final Batch Loss: 0.008043711073696613\n",
      "Epoch 1560, Loss: 0.02984987385571003, Final Batch Loss: 0.013827243819832802\n",
      "Epoch 1561, Loss: 0.03737694304436445, Final Batch Loss: 0.015028209425508976\n",
      "Epoch 1562, Loss: 0.02454477082937956, Final Batch Loss: 0.008502197451889515\n",
      "Epoch 1563, Loss: 0.04788757115602493, Final Batch Loss: 0.026525180786848068\n",
      "Epoch 1564, Loss: 0.01674675615504384, Final Batch Loss: 0.006006084848195314\n",
      "Epoch 1565, Loss: 0.046111416071653366, Final Batch Loss: 0.019788889214396477\n",
      "Epoch 1566, Loss: 0.0368451289832592, Final Batch Loss: 0.01645837537944317\n",
      "Epoch 1567, Loss: 0.009029715321958065, Final Batch Loss: 0.002819109708070755\n",
      "Epoch 1568, Loss: 0.036246951669454575, Final Batch Loss: 0.02117985300719738\n",
      "Epoch 1569, Loss: 0.020079382695257664, Final Batch Loss: 0.005500844679772854\n",
      "Epoch 1570, Loss: 0.05500366725027561, Final Batch Loss: 0.01975790224969387\n",
      "Epoch 1571, Loss: 0.034655038733035326, Final Batch Loss: 0.004826901946216822\n",
      "Epoch 1572, Loss: 0.03318389318883419, Final Batch Loss: 0.017424970865249634\n",
      "Epoch 1573, Loss: 0.029462783597409725, Final Batch Loss: 0.023976661264896393\n",
      "Epoch 1574, Loss: 0.009709065081551671, Final Batch Loss: 0.0026407039258629084\n",
      "Epoch 1575, Loss: 0.017691077664494514, Final Batch Loss: 0.006476718932390213\n",
      "Epoch 1576, Loss: 0.05013539828360081, Final Batch Loss: 0.040853068232536316\n",
      "Epoch 1577, Loss: 0.019916095305234194, Final Batch Loss: 0.007180303800851107\n",
      "Epoch 1578, Loss: 0.014488006941974163, Final Batch Loss: 0.006968682631850243\n",
      "Epoch 1579, Loss: 0.01602671155706048, Final Batch Loss: 0.009519902057945728\n",
      "Epoch 1580, Loss: 0.021328925155103207, Final Batch Loss: 0.008766819722950459\n",
      "Epoch 1581, Loss: 0.028337355703115463, Final Batch Loss: 0.010757507756352425\n",
      "Epoch 1582, Loss: 0.027975975070148706, Final Batch Loss: 0.021469326689839363\n",
      "Epoch 1583, Loss: 0.007141603622585535, Final Batch Loss: 0.003073583822697401\n",
      "Epoch 1584, Loss: 0.01194177009165287, Final Batch Loss: 0.0021771136671304703\n",
      "Epoch 1585, Loss: 0.017733078449964523, Final Batch Loss: 0.00917789712548256\n",
      "Epoch 1586, Loss: 0.014769638888537884, Final Batch Loss: 0.006545377895236015\n",
      "Epoch 1587, Loss: 0.02421197621151805, Final Batch Loss: 0.006153698544949293\n",
      "Epoch 1588, Loss: 0.013789252610877156, Final Batch Loss: 0.00356434122659266\n",
      "Epoch 1589, Loss: 0.05978431738913059, Final Batch Loss: 0.01668606884777546\n",
      "Epoch 1590, Loss: 0.038505337201058865, Final Batch Loss: 0.009447422809898853\n",
      "Epoch 1591, Loss: 0.022977949120104313, Final Batch Loss: 0.01349566224962473\n",
      "Epoch 1592, Loss: 0.013640106189996004, Final Batch Loss: 0.0073663643561303616\n",
      "Epoch 1593, Loss: 0.007262971252202988, Final Batch Loss: 0.0027217986062169075\n",
      "Epoch 1594, Loss: 0.037317602429538965, Final Batch Loss: 0.03173818811774254\n",
      "Epoch 1595, Loss: 0.02572320168837905, Final Batch Loss: 0.0055948770605027676\n",
      "Epoch 1596, Loss: 0.04483840148895979, Final Batch Loss: 0.012788551859557629\n",
      "Epoch 1597, Loss: 0.009393122978508472, Final Batch Loss: 0.006627110298722982\n",
      "Epoch 1598, Loss: 0.021626191213726997, Final Batch Loss: 0.016910124570131302\n",
      "Epoch 1599, Loss: 0.010917142499238253, Final Batch Loss: 0.005560130812227726\n",
      "Epoch 1600, Loss: 0.017407031264156103, Final Batch Loss: 0.014311942271888256\n",
      "Epoch 1601, Loss: 0.013699912000447512, Final Batch Loss: 0.004980883095413446\n",
      "Epoch 1602, Loss: 0.009423432871699333, Final Batch Loss: 0.0046601626090705395\n",
      "Epoch 1603, Loss: 0.025577418506145477, Final Batch Loss: 0.014238481409847736\n",
      "Epoch 1604, Loss: 0.024482869543135166, Final Batch Loss: 0.003293330781161785\n",
      "Epoch 1605, Loss: 0.0354991527274251, Final Batch Loss: 0.0221725981682539\n",
      "Epoch 1606, Loss: 0.022310378961265087, Final Batch Loss: 0.005603025667369366\n",
      "Epoch 1607, Loss: 0.022151712328195572, Final Batch Loss: 0.005899054929614067\n",
      "Epoch 1608, Loss: 0.008730408968403935, Final Batch Loss: 0.00532298581674695\n",
      "Epoch 1609, Loss: 0.013848967850208282, Final Batch Loss: 0.005887574516236782\n",
      "Epoch 1610, Loss: 0.027240620460361242, Final Batch Loss: 0.0055136471055448055\n",
      "Epoch 1611, Loss: 0.027820168528705835, Final Batch Loss: 0.020402703434228897\n",
      "Epoch 1612, Loss: 0.0953121380880475, Final Batch Loss: 0.08150701969861984\n",
      "Epoch 1613, Loss: 0.03984128683805466, Final Batch Loss: 0.010572534054517746\n",
      "Epoch 1614, Loss: 0.012371843215078115, Final Batch Loss: 0.0055456641130149364\n",
      "Epoch 1615, Loss: 0.06121687591075897, Final Batch Loss: 0.023212455213069916\n",
      "Epoch 1616, Loss: 0.07974095642566681, Final Batch Loss: 0.06349867582321167\n",
      "Epoch 1617, Loss: 0.03568748850375414, Final Batch Loss: 0.02232038974761963\n",
      "Epoch 1618, Loss: 0.07923210132867098, Final Batch Loss: 0.06714949756860733\n",
      "Epoch 1619, Loss: 0.045411041006445885, Final Batch Loss: 0.021538691595196724\n",
      "Epoch 1620, Loss: 0.025865210220217705, Final Batch Loss: 0.013814051635563374\n",
      "Epoch 1621, Loss: 0.01610415754839778, Final Batch Loss: 0.006652492564171553\n",
      "Epoch 1622, Loss: 0.046994921285659075, Final Batch Loss: 0.04229182377457619\n",
      "Epoch 1623, Loss: 0.028624491300433874, Final Batch Loss: 0.005617754068225622\n",
      "Epoch 1624, Loss: 0.05865492485463619, Final Batch Loss: 0.019111962988972664\n",
      "Epoch 1625, Loss: 0.10275843366980553, Final Batch Loss: 0.05045802891254425\n",
      "Epoch 1626, Loss: 0.041396912187337875, Final Batch Loss: 0.02013481594622135\n",
      "Epoch 1627, Loss: 0.02393052726984024, Final Batch Loss: 0.013644605875015259\n",
      "Epoch 1628, Loss: 0.06120390258729458, Final Batch Loss: 0.03740055114030838\n",
      "Epoch 1629, Loss: 0.02769174985587597, Final Batch Loss: 0.014059491455554962\n",
      "Epoch 1630, Loss: 0.06549097644165158, Final Batch Loss: 0.058760665357112885\n",
      "Epoch 1631, Loss: 0.03907828126102686, Final Batch Loss: 0.028991445899009705\n",
      "Epoch 1632, Loss: 0.03438183991238475, Final Batch Loss: 0.031043868511915207\n",
      "Epoch 1633, Loss: 0.041284617967903614, Final Batch Loss: 0.015227529220283031\n",
      "Epoch 1634, Loss: 0.01988766761496663, Final Batch Loss: 0.007299074437469244\n",
      "Epoch 1635, Loss: 0.07219024933874607, Final Batch Loss: 0.04240190237760544\n",
      "Epoch 1636, Loss: 0.03338243160396814, Final Batch Loss: 0.01808682084083557\n",
      "Epoch 1637, Loss: 0.036031391471624374, Final Batch Loss: 0.009329011663794518\n",
      "Epoch 1638, Loss: 0.009266125736758113, Final Batch Loss: 0.005517386365681887\n",
      "Epoch 1639, Loss: 0.024458530358970165, Final Batch Loss: 0.01658761315047741\n",
      "Epoch 1640, Loss: 0.055737609043717384, Final Batch Loss: 0.023049136623740196\n",
      "Epoch 1641, Loss: 0.014099271968007088, Final Batch Loss: 0.0037278709933161736\n",
      "Epoch 1642, Loss: 0.03357319161295891, Final Batch Loss: 0.018857672810554504\n",
      "Epoch 1643, Loss: 0.012427447363734245, Final Batch Loss: 0.008326486684381962\n",
      "Epoch 1644, Loss: 0.03174392646178603, Final Batch Loss: 0.0027410495094954967\n",
      "Epoch 1645, Loss: 0.047630857676267624, Final Batch Loss: 0.007568776607513428\n",
      "Epoch 1646, Loss: 0.023599693085998297, Final Batch Loss: 0.004276493098586798\n",
      "Epoch 1647, Loss: 0.06404241919517517, Final Batch Loss: 0.03383488208055496\n",
      "Epoch 1648, Loss: 0.0228196382522583, Final Batch Loss: 0.009816273115575314\n",
      "Epoch 1649, Loss: 0.021449387539178133, Final Batch Loss: 0.015744514763355255\n",
      "Epoch 1650, Loss: 0.023350813426077366, Final Batch Loss: 0.005807067267596722\n",
      "Epoch 1651, Loss: 0.019739343784749508, Final Batch Loss: 0.01130703091621399\n",
      "Epoch 1652, Loss: 0.03567492961883545, Final Batch Loss: 0.017141791060566902\n",
      "Epoch 1653, Loss: 0.032363107427954674, Final Batch Loss: 0.003082234412431717\n",
      "Epoch 1654, Loss: 0.03955528885126114, Final Batch Loss: 0.013165555894374847\n",
      "Epoch 1655, Loss: 0.017464466392993927, Final Batch Loss: 0.008031350560486317\n",
      "Epoch 1656, Loss: 0.08899830095469952, Final Batch Loss: 0.0796470046043396\n",
      "Epoch 1657, Loss: 0.024663215968757868, Final Batch Loss: 0.005989244673401117\n",
      "Epoch 1658, Loss: 0.015658225398510695, Final Batch Loss: 0.0045149014331400394\n",
      "Epoch 1659, Loss: 0.04858684632927179, Final Batch Loss: 0.012003806419670582\n",
      "Epoch 1660, Loss: 0.011024673003703356, Final Batch Loss: 0.005307616665959358\n",
      "Epoch 1661, Loss: 0.014036886859685183, Final Batch Loss: 0.00418845983222127\n",
      "Epoch 1662, Loss: 0.03933235863223672, Final Batch Loss: 0.03487095609307289\n",
      "Epoch 1663, Loss: 0.045199121348559856, Final Batch Loss: 0.037073079496622086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1664, Loss: 0.03156886622309685, Final Batch Loss: 0.0070478543639183044\n",
      "Epoch 1665, Loss: 0.03705801907926798, Final Batch Loss: 0.003956836648285389\n",
      "Epoch 1666, Loss: 0.05962988641113043, Final Batch Loss: 0.044594015926122665\n",
      "Epoch 1667, Loss: 0.017139946576207876, Final Batch Loss: 0.009891846217215061\n",
      "Epoch 1668, Loss: 0.05691461777314544, Final Batch Loss: 0.05189070105552673\n",
      "Epoch 1669, Loss: 0.008061711909249425, Final Batch Loss: 0.004762596450746059\n",
      "Epoch 1670, Loss: 0.010545433266088367, Final Batch Loss: 0.003540022997185588\n",
      "Epoch 1671, Loss: 0.01295130094513297, Final Batch Loss: 0.00499906437471509\n",
      "Epoch 1672, Loss: 0.034888902911916375, Final Batch Loss: 0.0027231855783611536\n",
      "Epoch 1673, Loss: 0.005272924550808966, Final Batch Loss: 0.001493707182817161\n",
      "Epoch 1674, Loss: 0.01798154879361391, Final Batch Loss: 0.012141882441937923\n",
      "Epoch 1675, Loss: 0.009216647129505873, Final Batch Loss: 0.005816134624183178\n",
      "Epoch 1676, Loss: 0.014997594524174929, Final Batch Loss: 0.007427137810736895\n",
      "Epoch 1677, Loss: 0.03088816162198782, Final Batch Loss: 0.012480462901294231\n",
      "Epoch 1678, Loss: 0.012943682726472616, Final Batch Loss: 0.008906586095690727\n",
      "Epoch 1679, Loss: 0.015346861444413662, Final Batch Loss: 0.011078324168920517\n",
      "Epoch 1680, Loss: 0.024243497289717197, Final Batch Loss: 0.015057399868965149\n",
      "Epoch 1681, Loss: 0.009366295766085386, Final Batch Loss: 0.002011618111282587\n",
      "Epoch 1682, Loss: 0.016089594224467874, Final Batch Loss: 0.013169601559638977\n",
      "Epoch 1683, Loss: 0.01824840158224106, Final Batch Loss: 0.005974791012704372\n",
      "Epoch 1684, Loss: 0.021681383717805147, Final Batch Loss: 0.004657023120671511\n",
      "Epoch 1685, Loss: 0.06692611239850521, Final Batch Loss: 0.025116009637713432\n",
      "Epoch 1686, Loss: 0.014106844551861286, Final Batch Loss: 0.002774743363261223\n",
      "Epoch 1687, Loss: 0.0390014979057014, Final Batch Loss: 0.03364453464746475\n",
      "Epoch 1688, Loss: 0.00798208941705525, Final Batch Loss: 0.0021500529255717993\n",
      "Epoch 1689, Loss: 0.0207333629950881, Final Batch Loss: 0.010924139060080051\n",
      "Epoch 1690, Loss: 0.030945793725550175, Final Batch Loss: 0.020992344245314598\n",
      "Epoch 1691, Loss: 0.021439256612211466, Final Batch Loss: 0.003480444196611643\n",
      "Epoch 1692, Loss: 0.02694520028308034, Final Batch Loss: 0.0023319819010794163\n",
      "Epoch 1693, Loss: 0.04406427498906851, Final Batch Loss: 0.038916319608688354\n",
      "Epoch 1694, Loss: 0.039261849131435156, Final Batch Loss: 0.002148290630429983\n",
      "Epoch 1695, Loss: 0.04085539001971483, Final Batch Loss: 0.029817968606948853\n",
      "Epoch 1696, Loss: 0.07688813656568527, Final Batch Loss: 0.06041610240936279\n",
      "Epoch 1697, Loss: 0.02404590114019811, Final Batch Loss: 0.0036779826041311026\n",
      "Epoch 1698, Loss: 0.021210497245192528, Final Batch Loss: 0.015190254896879196\n",
      "Epoch 1699, Loss: 0.041911063715815544, Final Batch Loss: 0.013699186965823174\n",
      "Epoch 1700, Loss: 0.0377412848174572, Final Batch Loss: 0.029871894046664238\n",
      "Epoch 1701, Loss: 0.02813749387860298, Final Batch Loss: 0.01601535640656948\n",
      "Epoch 1702, Loss: 0.06612536404281855, Final Batch Loss: 0.009080511517822742\n",
      "Epoch 1703, Loss: 0.042905992828309536, Final Batch Loss: 0.01333907712250948\n",
      "Epoch 1704, Loss: 0.03644936345517635, Final Batch Loss: 0.014180503785610199\n",
      "Epoch 1705, Loss: 0.052691273391246796, Final Batch Loss: 0.031638678163290024\n",
      "Epoch 1706, Loss: 0.07488154619932175, Final Batch Loss: 0.044791366904973984\n",
      "Epoch 1707, Loss: 0.019972567446529865, Final Batch Loss: 0.010368001647293568\n",
      "Epoch 1708, Loss: 0.019423053599894047, Final Batch Loss: 0.011245381087064743\n",
      "Epoch 1709, Loss: 0.1310667097568512, Final Batch Loss: 0.09525518864393234\n",
      "Epoch 1710, Loss: 0.03060117829591036, Final Batch Loss: 0.0166329313069582\n",
      "Epoch 1711, Loss: 0.06694280356168747, Final Batch Loss: 0.03416299447417259\n",
      "Epoch 1712, Loss: 0.01023122202605009, Final Batch Loss: 0.003506434615701437\n",
      "Epoch 1713, Loss: 0.08522789552807808, Final Batch Loss: 0.05761110782623291\n",
      "Epoch 1714, Loss: 0.04696899792179465, Final Batch Loss: 0.03943417966365814\n",
      "Epoch 1715, Loss: 0.021722355391830206, Final Batch Loss: 0.007713341619819403\n",
      "Epoch 1716, Loss: 0.016392160672694445, Final Batch Loss: 0.005342402961105108\n",
      "Epoch 1717, Loss: 0.015011309646070004, Final Batch Loss: 0.005657866597175598\n",
      "Epoch 1718, Loss: 0.03224334213882685, Final Batch Loss: 0.009920417331159115\n",
      "Epoch 1719, Loss: 0.02762336377054453, Final Batch Loss: 0.017030296847224236\n",
      "Epoch 1720, Loss: 0.01749471598304808, Final Batch Loss: 0.002325834007933736\n",
      "Epoch 1721, Loss: 0.03916762676090002, Final Batch Loss: 0.030466286465525627\n",
      "Epoch 1722, Loss: 0.022446210496127605, Final Batch Loss: 0.010593469254672527\n",
      "Epoch 1723, Loss: 0.05829496402293444, Final Batch Loss: 0.010240421630442142\n",
      "Epoch 1724, Loss: 0.04364571534097195, Final Batch Loss: 0.021351324394345284\n",
      "Epoch 1725, Loss: 0.041207363829016685, Final Batch Loss: 0.034253332763910294\n",
      "Epoch 1726, Loss: 0.0597255639731884, Final Batch Loss: 0.036926914006471634\n",
      "Epoch 1727, Loss: 0.04934812616556883, Final Batch Loss: 0.040497660636901855\n",
      "Epoch 1728, Loss: 0.042188568972051144, Final Batch Loss: 0.03376041725277901\n",
      "Epoch 1729, Loss: 0.021391747053712606, Final Batch Loss: 0.006451692897826433\n",
      "Epoch 1730, Loss: 0.029407154768705368, Final Batch Loss: 0.006699597463011742\n",
      "Epoch 1731, Loss: 0.06344072706997395, Final Batch Loss: 0.016591107472777367\n",
      "Epoch 1732, Loss: 0.027178088203072548, Final Batch Loss: 0.010296011343598366\n",
      "Epoch 1733, Loss: 0.04473470617085695, Final Batch Loss: 0.029848776757717133\n",
      "Epoch 1734, Loss: 0.017239851411432028, Final Batch Loss: 0.012948425486683846\n",
      "Epoch 1735, Loss: 0.04093538224697113, Final Batch Loss: 0.025675563141703606\n",
      "Epoch 1736, Loss: 0.03191510774195194, Final Batch Loss: 0.0159728042781353\n",
      "Epoch 1737, Loss: 0.03089324478060007, Final Batch Loss: 0.00981272105127573\n",
      "Epoch 1738, Loss: 0.038373665418475866, Final Batch Loss: 0.03206614404916763\n",
      "Epoch 1739, Loss: 0.03770637232810259, Final Batch Loss: 0.00930165033787489\n",
      "Epoch 1740, Loss: 0.03775421250611544, Final Batch Loss: 0.008876289241015911\n",
      "Epoch 1741, Loss: 0.011053757509216666, Final Batch Loss: 0.0019366794731467962\n",
      "Epoch 1742, Loss: 0.08173082582652569, Final Batch Loss: 0.07575591653585434\n",
      "Epoch 1743, Loss: 0.05448311101645231, Final Batch Loss: 0.01500657107681036\n",
      "Epoch 1744, Loss: 0.01254450948908925, Final Batch Loss: 0.007314207497984171\n",
      "Epoch 1745, Loss: 0.014838898088783026, Final Batch Loss: 0.010941257700324059\n",
      "Epoch 1746, Loss: 0.04241179581731558, Final Batch Loss: 0.010361871682107449\n",
      "Epoch 1747, Loss: 0.040334342047572136, Final Batch Loss: 0.0165750440210104\n",
      "Epoch 1748, Loss: 0.018613178748637438, Final Batch Loss: 0.0034354780800640583\n",
      "Epoch 1749, Loss: 0.058828460052609444, Final Batch Loss: 0.048282191157341\n",
      "Epoch 1750, Loss: 0.01564347418025136, Final Batch Loss: 0.004881210159510374\n",
      "Epoch 1751, Loss: 0.018295079469680786, Final Batch Loss: 0.005322855897247791\n",
      "Epoch 1752, Loss: 0.026459715329110622, Final Batch Loss: 0.016975151374936104\n",
      "Epoch 1753, Loss: 0.027304437942802906, Final Batch Loss: 0.00913331750780344\n",
      "Epoch 1754, Loss: 0.02767872530966997, Final Batch Loss: 0.011478391475975513\n",
      "Epoch 1755, Loss: 0.014470121823251247, Final Batch Loss: 0.010476106777787209\n",
      "Epoch 1756, Loss: 0.026693248888477683, Final Batch Loss: 0.023856226354837418\n",
      "Epoch 1757, Loss: 0.01826241984963417, Final Batch Loss: 0.008104867301881313\n",
      "Epoch 1758, Loss: 0.04610192961990833, Final Batch Loss: 0.008733374997973442\n",
      "Epoch 1759, Loss: 0.025714285671710968, Final Batch Loss: 0.012551289051771164\n",
      "Epoch 1760, Loss: 0.01735070114955306, Final Batch Loss: 0.012683969922363758\n",
      "Epoch 1761, Loss: 0.0822649309411645, Final Batch Loss: 0.01054647658020258\n",
      "Epoch 1762, Loss: 0.016278259456157684, Final Batch Loss: 0.009492858313024044\n",
      "Epoch 1763, Loss: 0.04821055196225643, Final Batch Loss: 0.025235414505004883\n",
      "Epoch 1764, Loss: 0.017365457955747843, Final Batch Loss: 0.010532258078455925\n",
      "Epoch 1765, Loss: 0.0334757836535573, Final Batch Loss: 0.019992535933852196\n",
      "Epoch 1766, Loss: 0.05780120939016342, Final Batch Loss: 0.05426318570971489\n",
      "Epoch 1767, Loss: 0.056184928864240646, Final Batch Loss: 0.03213219344615936\n",
      "Epoch 1768, Loss: 0.09907673299312592, Final Batch Loss: 0.05990644916892052\n",
      "Epoch 1769, Loss: 0.2633732594549656, Final Batch Loss: 0.21149933338165283\n",
      "Epoch 1770, Loss: 0.08266924321651459, Final Batch Loss: 0.03152989223599434\n",
      "Epoch 1771, Loss: 0.08032397087663412, Final Batch Loss: 0.0718735009431839\n",
      "Epoch 1772, Loss: 0.051933301612734795, Final Batch Loss: 0.012684604153037071\n",
      "Epoch 1773, Loss: 0.011905782856047153, Final Batch Loss: 0.005116453859955072\n",
      "Epoch 1774, Loss: 0.04633386339992285, Final Batch Loss: 0.009556121192872524\n",
      "Epoch 1775, Loss: 0.04638181813061237, Final Batch Loss: 0.018168458715081215\n",
      "Epoch 1776, Loss: 0.06028064154088497, Final Batch Loss: 0.026297161355614662\n",
      "Epoch 1777, Loss: 0.042614512611180544, Final Batch Loss: 0.03611662611365318\n",
      "Epoch 1778, Loss: 0.03280842583626509, Final Batch Loss: 0.013707947917282581\n",
      "Epoch 1779, Loss: 0.0443786708638072, Final Batch Loss: 0.03357578068971634\n",
      "Epoch 1780, Loss: 0.02593533555045724, Final Batch Loss: 0.018169112503528595\n",
      "Epoch 1781, Loss: 0.14620245061814785, Final Batch Loss: 0.12651360034942627\n",
      "Epoch 1782, Loss: 0.0263894684612751, Final Batch Loss: 0.011119259521365166\n",
      "Epoch 1783, Loss: 0.01631876186002046, Final Batch Loss: 0.0010698606492951512\n",
      "Epoch 1784, Loss: 0.017579062841832638, Final Batch Loss: 0.00873483344912529\n",
      "Epoch 1785, Loss: 0.06380926445126534, Final Batch Loss: 0.024186935275793076\n",
      "Epoch 1786, Loss: 0.05474564293399453, Final Batch Loss: 0.049044955521821976\n",
      "Epoch 1787, Loss: 0.08118297718465328, Final Batch Loss: 0.05715997517108917\n",
      "Epoch 1788, Loss: 0.010736197931692004, Final Batch Loss: 0.002061986131593585\n",
      "Epoch 1789, Loss: 0.04411666840314865, Final Batch Loss: 0.01705521158874035\n",
      "Epoch 1790, Loss: 0.012925053015351295, Final Batch Loss: 0.0039260489866137505\n",
      "Epoch 1791, Loss: 0.02787760365754366, Final Batch Loss: 0.01846766658127308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1792, Loss: 0.02004857058636844, Final Batch Loss: 0.003018441377207637\n",
      "Epoch 1793, Loss: 0.03004761692136526, Final Batch Loss: 0.003146323375403881\n",
      "Epoch 1794, Loss: 0.053877461701631546, Final Batch Loss: 0.04181857779622078\n",
      "Epoch 1795, Loss: 0.06183594651520252, Final Batch Loss: 0.01792733557522297\n",
      "Epoch 1796, Loss: 0.05622166022658348, Final Batch Loss: 0.02063806727528572\n",
      "Epoch 1797, Loss: 0.010416049510240555, Final Batch Loss: 0.0047560688108205795\n",
      "Epoch 1798, Loss: 0.020967480493709445, Final Batch Loss: 0.01815464161336422\n",
      "Epoch 1799, Loss: 0.01748341741040349, Final Batch Loss: 0.01186523400247097\n",
      "Epoch 1800, Loss: 0.045271992683410645, Final Batch Loss: 0.007007405161857605\n",
      "Epoch 1801, Loss: 0.018907881807535887, Final Batch Loss: 0.007405070122331381\n",
      "Epoch 1802, Loss: 0.01717644976451993, Final Batch Loss: 0.009504925459623337\n",
      "Epoch 1803, Loss: 0.008398498641327024, Final Batch Loss: 0.0046844277530908585\n",
      "Epoch 1804, Loss: 0.017680399818345904, Final Batch Loss: 0.0036337513010948896\n",
      "Epoch 1805, Loss: 0.023700027726590633, Final Batch Loss: 0.0048304954543709755\n",
      "Epoch 1806, Loss: 0.03250992717221379, Final Batch Loss: 0.026322603225708008\n",
      "Epoch 1807, Loss: 0.012754376977682114, Final Batch Loss: 0.004738452844321728\n",
      "Epoch 1808, Loss: 0.02079698257148266, Final Batch Loss: 0.010559894144535065\n",
      "Epoch 1809, Loss: 0.015486315824091434, Final Batch Loss: 0.004459276795387268\n",
      "Epoch 1810, Loss: 0.011435728520154953, Final Batch Loss: 0.0057567390613257885\n",
      "Epoch 1811, Loss: 0.014031280763447285, Final Batch Loss: 0.006833376828581095\n",
      "Epoch 1812, Loss: 0.023061500396579504, Final Batch Loss: 0.0025247945450246334\n",
      "Epoch 1813, Loss: 0.02163044223561883, Final Batch Loss: 0.0069513781927526\n",
      "Epoch 1814, Loss: 0.059814220294356346, Final Batch Loss: 0.05206146463751793\n",
      "Epoch 1815, Loss: 0.020051108207553625, Final Batch Loss: 0.013068032450973988\n",
      "Epoch 1816, Loss: 0.01381273427978158, Final Batch Loss: 0.006767368409782648\n",
      "Epoch 1817, Loss: 0.017359182704240084, Final Batch Loss: 0.013165833428502083\n",
      "Epoch 1818, Loss: 0.015335137606598437, Final Batch Loss: 0.0014516605297103524\n",
      "Epoch 1819, Loss: 0.015286267269402742, Final Batch Loss: 0.00893582683056593\n",
      "Epoch 1820, Loss: 0.016327001387253404, Final Batch Loss: 0.00343504105694592\n",
      "Epoch 1821, Loss: 0.012050713412463665, Final Batch Loss: 0.004993250593543053\n",
      "Epoch 1822, Loss: 0.013531486969441175, Final Batch Loss: 0.003977614920586348\n",
      "Epoch 1823, Loss: 0.008586128009483218, Final Batch Loss: 0.0014002828393131495\n",
      "Epoch 1824, Loss: 0.029442202299833298, Final Batch Loss: 0.00436432845890522\n",
      "Epoch 1825, Loss: 0.018503803526982665, Final Batch Loss: 0.002317124744877219\n",
      "Epoch 1826, Loss: 0.021757246926426888, Final Batch Loss: 0.0045750997960567474\n",
      "Epoch 1827, Loss: 0.00795633066445589, Final Batch Loss: 0.0029610642232000828\n",
      "Epoch 1828, Loss: 0.007005401654168963, Final Batch Loss: 0.00203972146846354\n",
      "Epoch 1829, Loss: 0.03357366006821394, Final Batch Loss: 0.018258443102240562\n",
      "Epoch 1830, Loss: 0.013787951786071062, Final Batch Loss: 0.004151252564042807\n",
      "Epoch 1831, Loss: 0.014942229259759188, Final Batch Loss: 0.005538859870284796\n",
      "Epoch 1832, Loss: 0.026351865381002426, Final Batch Loss: 0.02162451110780239\n",
      "Epoch 1833, Loss: 0.010804912075400352, Final Batch Loss: 0.005140756256878376\n",
      "Epoch 1834, Loss: 0.010267314035445452, Final Batch Loss: 0.004165904130786657\n",
      "Epoch 1835, Loss: 0.00574907916598022, Final Batch Loss: 0.0032115690410137177\n",
      "Epoch 1836, Loss: 0.020873471163213253, Final Batch Loss: 0.005351254716515541\n",
      "Epoch 1837, Loss: 0.05223500728607178, Final Batch Loss: 0.016551513224840164\n",
      "Epoch 1838, Loss: 0.019610402174293995, Final Batch Loss: 0.012871047481894493\n",
      "Epoch 1839, Loss: 0.01628341432660818, Final Batch Loss: 0.01060999371111393\n",
      "Epoch 1840, Loss: 0.028243392007425427, Final Batch Loss: 0.0013655603397637606\n",
      "Epoch 1841, Loss: 0.014849106781184673, Final Batch Loss: 0.005139957182109356\n",
      "Epoch 1842, Loss: 0.04913119226694107, Final Batch Loss: 0.014516077935695648\n",
      "Epoch 1843, Loss: 0.03412865661084652, Final Batch Loss: 0.027613366022706032\n",
      "Epoch 1844, Loss: 0.01465669646859169, Final Batch Loss: 0.007133404724299908\n",
      "Epoch 1845, Loss: 0.012052277568727732, Final Batch Loss: 0.007578687276691198\n",
      "Epoch 1846, Loss: 0.06617725174874067, Final Batch Loss: 0.007860186509788036\n",
      "Epoch 1847, Loss: 0.010922607034444809, Final Batch Loss: 0.007875417359173298\n",
      "Epoch 1848, Loss: 0.015021277591586113, Final Batch Loss: 0.004586605355143547\n",
      "Epoch 1849, Loss: 0.036431568674743176, Final Batch Loss: 0.010041075758635998\n",
      "Epoch 1850, Loss: 0.029361856169998646, Final Batch Loss: 0.02306954748928547\n",
      "Epoch 1851, Loss: 0.010992597322911024, Final Batch Loss: 0.0037845810875296593\n",
      "Epoch 1852, Loss: 0.009700108552351594, Final Batch Loss: 0.006993747781962156\n",
      "Epoch 1853, Loss: 0.021382679231464863, Final Batch Loss: 0.01596755161881447\n",
      "Epoch 1854, Loss: 0.022117104846984148, Final Batch Loss: 0.01817452535033226\n",
      "Epoch 1855, Loss: 0.008334202691912651, Final Batch Loss: 0.005989249795675278\n",
      "Epoch 1856, Loss: 0.020012406166642904, Final Batch Loss: 0.002601906191557646\n",
      "Epoch 1857, Loss: 0.007124388590455055, Final Batch Loss: 0.00422040605917573\n",
      "Epoch 1858, Loss: 0.008505790261551738, Final Batch Loss: 0.001980663975700736\n",
      "Epoch 1859, Loss: 0.005628169979900122, Final Batch Loss: 0.00276649696752429\n",
      "Epoch 1860, Loss: 0.03839625115506351, Final Batch Loss: 0.003420655382797122\n",
      "Epoch 1861, Loss: 0.021604920155368745, Final Batch Loss: 0.0205826535820961\n",
      "Epoch 1862, Loss: 0.06316074309870601, Final Batch Loss: 0.05912068486213684\n",
      "Epoch 1863, Loss: 0.019942143466323614, Final Batch Loss: 0.016310356557369232\n",
      "Epoch 1864, Loss: 0.09131526295095682, Final Batch Loss: 0.013631471432745457\n",
      "Epoch 1865, Loss: 0.06406014785170555, Final Batch Loss: 0.0343407467007637\n",
      "Epoch 1866, Loss: 0.019765786360949278, Final Batch Loss: 0.004147747065871954\n",
      "Epoch 1867, Loss: 0.05731080286204815, Final Batch Loss: 0.03307328373193741\n",
      "Epoch 1868, Loss: 0.016557476483285427, Final Batch Loss: 0.006273780949413776\n",
      "Epoch 1869, Loss: 0.013232450000941753, Final Batch Loss: 0.0078425956889987\n",
      "Epoch 1870, Loss: 0.03627686947584152, Final Batch Loss: 0.026725703850388527\n",
      "Epoch 1871, Loss: 0.0439913272857666, Final Batch Loss: 0.039179977029561996\n",
      "Epoch 1872, Loss: 0.03047659434378147, Final Batch Loss: 0.005413249135017395\n",
      "Epoch 1873, Loss: 0.03903058264404535, Final Batch Loss: 0.010138149373233318\n",
      "Epoch 1874, Loss: 0.04628483019769192, Final Batch Loss: 0.021096426993608475\n",
      "Epoch 1875, Loss: 0.024827384389936924, Final Batch Loss: 0.014449886977672577\n",
      "Epoch 1876, Loss: 0.02899466920644045, Final Batch Loss: 0.008284046314656734\n",
      "Epoch 1877, Loss: 0.03945138119161129, Final Batch Loss: 0.03385017812252045\n",
      "Epoch 1878, Loss: 0.03601318341679871, Final Batch Loss: 0.0028948199469596148\n",
      "Epoch 1879, Loss: 0.018415370490401983, Final Batch Loss: 0.011378675699234009\n",
      "Epoch 1880, Loss: 0.01914430968463421, Final Batch Loss: 0.003057403489947319\n",
      "Epoch 1881, Loss: 0.050258198752999306, Final Batch Loss: 0.04168844595551491\n",
      "Epoch 1882, Loss: 0.009908075910061598, Final Batch Loss: 0.001879954244941473\n",
      "Epoch 1883, Loss: 0.03389197774231434, Final Batch Loss: 0.02961527556180954\n",
      "Epoch 1884, Loss: 0.04985925927758217, Final Batch Loss: 0.02991640754044056\n",
      "Epoch 1885, Loss: 0.01109811756759882, Final Batch Loss: 0.0030807433649897575\n",
      "Epoch 1886, Loss: 0.043117158114910126, Final Batch Loss: 0.01345095969736576\n",
      "Epoch 1887, Loss: 0.05902467295527458, Final Batch Loss: 0.009349685162305832\n",
      "Epoch 1888, Loss: 0.05578408017754555, Final Batch Loss: 0.03253627568483353\n",
      "Epoch 1889, Loss: 0.0296325096860528, Final Batch Loss: 0.025352466851472855\n",
      "Epoch 1890, Loss: 0.050529008731245995, Final Batch Loss: 0.03641192615032196\n",
      "Epoch 1891, Loss: 0.021017478546127677, Final Batch Loss: 0.0019638778176158667\n",
      "Epoch 1892, Loss: 0.025643096305429935, Final Batch Loss: 0.006940779276192188\n",
      "Epoch 1893, Loss: 0.015076667070388794, Final Batch Loss: 0.012416668236255646\n",
      "Epoch 1894, Loss: 0.07625823933631182, Final Batch Loss: 0.07086022943258286\n",
      "Epoch 1895, Loss: 0.04961265530437231, Final Batch Loss: 0.03698224201798439\n",
      "Epoch 1896, Loss: 0.030178095679730177, Final Batch Loss: 0.004725975450128317\n",
      "Epoch 1897, Loss: 0.020884680096060038, Final Batch Loss: 0.0029294011183083057\n",
      "Epoch 1898, Loss: 0.01286421436816454, Final Batch Loss: 0.00776920048519969\n",
      "Epoch 1899, Loss: 0.01462252950295806, Final Batch Loss: 0.006256380584090948\n",
      "Epoch 1900, Loss: 0.036211030557751656, Final Batch Loss: 0.010176140815019608\n",
      "Epoch 1901, Loss: 0.006948008900508285, Final Batch Loss: 0.003178074723109603\n",
      "Epoch 1902, Loss: 0.011483021546155214, Final Batch Loss: 0.006529899314045906\n",
      "Epoch 1903, Loss: 0.02807956305332482, Final Batch Loss: 0.0021479253191500902\n",
      "Epoch 1904, Loss: 0.02730434574186802, Final Batch Loss: 0.01275964081287384\n",
      "Epoch 1905, Loss: 0.014711516909301281, Final Batch Loss: 0.0019996818155050278\n",
      "Epoch 1906, Loss: 0.018891787622123957, Final Batch Loss: 0.012051564641296864\n",
      "Epoch 1907, Loss: 0.02024330012500286, Final Batch Loss: 0.01317489705979824\n",
      "Epoch 1908, Loss: 0.013172205537557602, Final Batch Loss: 0.00915093906223774\n",
      "Epoch 1909, Loss: 0.025188766652718186, Final Batch Loss: 0.02180837281048298\n",
      "Epoch 1910, Loss: 0.010177104501053691, Final Batch Loss: 0.006797391455620527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1911, Loss: 0.029558937065303326, Final Batch Loss: 0.005572979338467121\n",
      "Epoch 1912, Loss: 0.022914772853255272, Final Batch Loss: 0.019764821976423264\n",
      "Epoch 1913, Loss: 0.011890157591551542, Final Batch Loss: 0.006878554821014404\n",
      "Epoch 1914, Loss: 0.039351750863716006, Final Batch Loss: 0.002656620694324374\n",
      "Epoch 1915, Loss: 0.02941224817186594, Final Batch Loss: 0.003742399625480175\n",
      "Epoch 1916, Loss: 0.015756526496261358, Final Batch Loss: 0.002273996826261282\n",
      "Epoch 1917, Loss: 0.015297581441700459, Final Batch Loss: 0.0033799251541495323\n",
      "Epoch 1918, Loss: 0.011850712820887566, Final Batch Loss: 0.003983421251177788\n",
      "Epoch 1919, Loss: 0.008597513427957892, Final Batch Loss: 0.0016315414104610682\n",
      "Epoch 1920, Loss: 0.055517555214464664, Final Batch Loss: 0.04525245353579521\n",
      "Epoch 1921, Loss: 0.024442262016236782, Final Batch Loss: 0.015147405676543713\n",
      "Epoch 1922, Loss: 0.007520662155002356, Final Batch Loss: 0.0024350243620574474\n",
      "Epoch 1923, Loss: 0.04502866975963116, Final Batch Loss: 0.003176553174853325\n",
      "Epoch 1924, Loss: 0.02810299303382635, Final Batch Loss: 0.01259161438792944\n",
      "Epoch 1925, Loss: 0.017896476201713085, Final Batch Loss: 0.0019880784675478935\n",
      "Epoch 1926, Loss: 0.00752917374484241, Final Batch Loss: 0.004833932500332594\n",
      "Epoch 1927, Loss: 0.00948535231873393, Final Batch Loss: 0.002595631405711174\n",
      "Epoch 1928, Loss: 0.01611068844795227, Final Batch Loss: 0.010782817378640175\n",
      "Epoch 1929, Loss: 0.004668040666729212, Final Batch Loss: 0.0027968958020210266\n",
      "Epoch 1930, Loss: 0.030389085179194808, Final Batch Loss: 0.0025162489619106054\n",
      "Epoch 1931, Loss: 0.008693194016814232, Final Batch Loss: 0.004032038152217865\n",
      "Epoch 1932, Loss: 0.017620092490687966, Final Batch Loss: 0.01479468122124672\n",
      "Epoch 1933, Loss: 0.004665953223593533, Final Batch Loss: 0.0018896755063906312\n",
      "Epoch 1934, Loss: 0.022001937264576554, Final Batch Loss: 0.003887558588758111\n",
      "Epoch 1935, Loss: 0.006589361932128668, Final Batch Loss: 0.003919586073607206\n",
      "Epoch 1936, Loss: 0.0156981130130589, Final Batch Loss: 0.004303032997995615\n",
      "Epoch 1937, Loss: 0.01704824110493064, Final Batch Loss: 0.011607130989432335\n",
      "Epoch 1938, Loss: 0.051587834022939205, Final Batch Loss: 0.012380446307361126\n",
      "Epoch 1939, Loss: 0.03666121745482087, Final Batch Loss: 0.00637649605050683\n",
      "Epoch 1940, Loss: 0.02561535034328699, Final Batch Loss: 0.015513740479946136\n",
      "Epoch 1941, Loss: 0.05907836649566889, Final Batch Loss: 0.04816697910428047\n",
      "Epoch 1942, Loss: 0.012720504775643349, Final Batch Loss: 0.006319883279502392\n",
      "Epoch 1943, Loss: 0.006877061910927296, Final Batch Loss: 0.0024376437067985535\n",
      "Epoch 1944, Loss: 0.038050190545618534, Final Batch Loss: 0.02673446200788021\n",
      "Epoch 1945, Loss: 0.024591269437223673, Final Batch Loss: 0.004683102015405893\n",
      "Epoch 1946, Loss: 0.027520819567143917, Final Batch Loss: 0.02213512733578682\n",
      "Epoch 1947, Loss: 0.049997603520751, Final Batch Loss: 0.032665152102708817\n",
      "Epoch 1948, Loss: 0.02289653941988945, Final Batch Loss: 0.013894679956138134\n",
      "Epoch 1949, Loss: 0.01206759549677372, Final Batch Loss: 0.007814361713826656\n",
      "Epoch 1950, Loss: 0.006538592511788011, Final Batch Loss: 0.002780286828055978\n",
      "Epoch 1951, Loss: 0.023130313493311405, Final Batch Loss: 0.003215962089598179\n",
      "Epoch 1952, Loss: 0.014221809571608901, Final Batch Loss: 0.011192873120307922\n",
      "Epoch 1953, Loss: 0.02904315572232008, Final Batch Loss: 0.00951636303216219\n",
      "Epoch 1954, Loss: 0.009680091869086027, Final Batch Loss: 0.00515565974637866\n",
      "Epoch 1955, Loss: 0.03851561690680683, Final Batch Loss: 0.0033083490561693907\n",
      "Epoch 1956, Loss: 0.02763941790908575, Final Batch Loss: 0.024250641465187073\n",
      "Epoch 1957, Loss: 0.015389970736578107, Final Batch Loss: 0.0018811759073287249\n",
      "Epoch 1958, Loss: 0.0659031793475151, Final Batch Loss: 0.047706373035907745\n",
      "Epoch 1959, Loss: 0.08070888370275497, Final Batch Loss: 0.0381699874997139\n",
      "Epoch 1960, Loss: 0.03690305445343256, Final Batch Loss: 0.004194159992039204\n",
      "Epoch 1961, Loss: 0.01898561604321003, Final Batch Loss: 0.01257949136197567\n",
      "Epoch 1962, Loss: 0.03180082980543375, Final Batch Loss: 0.023813549429178238\n",
      "Epoch 1963, Loss: 0.029236562782898545, Final Batch Loss: 0.0030309876892715693\n",
      "Epoch 1964, Loss: 0.014539113268256187, Final Batch Loss: 0.004436680115759373\n",
      "Epoch 1965, Loss: 0.026203990913927555, Final Batch Loss: 0.009657141752541065\n",
      "Epoch 1966, Loss: 0.0347384549677372, Final Batch Loss: 0.016018927097320557\n",
      "Epoch 1967, Loss: 0.023933681659400463, Final Batch Loss: 0.012422106228768826\n",
      "Epoch 1968, Loss: 0.02869482059031725, Final Batch Loss: 0.016369525343179703\n",
      "Epoch 1969, Loss: 0.03827142110094428, Final Batch Loss: 0.03196611627936363\n",
      "Epoch 1970, Loss: 0.06003284081816673, Final Batch Loss: 0.02320897951722145\n",
      "Epoch 1971, Loss: 0.025181199423968792, Final Batch Loss: 0.015469853766262531\n",
      "Epoch 1972, Loss: 0.037032251711934805, Final Batch Loss: 0.030980389565229416\n",
      "Epoch 1973, Loss: 0.03394899517297745, Final Batch Loss: 0.018129758536815643\n",
      "Epoch 1974, Loss: 0.019909294322133064, Final Batch Loss: 0.01675916649401188\n",
      "Epoch 1975, Loss: 0.036544667556881905, Final Batch Loss: 0.013059288263320923\n",
      "Epoch 1976, Loss: 0.02830872545018792, Final Batch Loss: 0.02098781056702137\n",
      "Epoch 1977, Loss: 0.02520905714482069, Final Batch Loss: 0.017121504992246628\n",
      "Epoch 1978, Loss: 0.010548795573413372, Final Batch Loss: 0.0052237738855183125\n",
      "Epoch 1979, Loss: 0.01526647713035345, Final Batch Loss: 0.006779140792787075\n",
      "Epoch 1980, Loss: 0.019535339204594493, Final Batch Loss: 0.016176583245396614\n",
      "Epoch 1981, Loss: 0.012879912741482258, Final Batch Loss: 0.0049291979521512985\n",
      "Epoch 1982, Loss: 0.03496778756380081, Final Batch Loss: 0.017058376222848892\n",
      "Epoch 1983, Loss: 0.003135975217446685, Final Batch Loss: 0.0019306597532704473\n",
      "Epoch 1984, Loss: 0.037181105464696884, Final Batch Loss: 0.02066076174378395\n",
      "Epoch 1985, Loss: 0.029967340640723705, Final Batch Loss: 0.009538614191114902\n",
      "Epoch 1986, Loss: 0.008399052079766989, Final Batch Loss: 0.003923056181520224\n",
      "Epoch 1987, Loss: 0.027038844767957926, Final Batch Loss: 0.0038250056095421314\n",
      "Epoch 1988, Loss: 0.0243700104765594, Final Batch Loss: 0.0037969755940139294\n",
      "Epoch 1989, Loss: 0.03665763884782791, Final Batch Loss: 0.01604527235031128\n",
      "Epoch 1990, Loss: 0.06356257013976574, Final Batch Loss: 0.03409747779369354\n",
      "Epoch 1991, Loss: 0.02513494435697794, Final Batch Loss: 0.007364819757640362\n",
      "Epoch 1992, Loss: 0.01857517519965768, Final Batch Loss: 0.012058677151799202\n",
      "Epoch 1993, Loss: 0.017720204778015614, Final Batch Loss: 0.00799478217959404\n",
      "Epoch 1994, Loss: 0.02675477322191, Final Batch Loss: 0.010988379828631878\n",
      "Epoch 1995, Loss: 0.015760871348902583, Final Batch Loss: 0.013044430874288082\n",
      "Epoch 1996, Loss: 0.03612498752772808, Final Batch Loss: 0.014561790972948074\n",
      "Epoch 1997, Loss: 0.028114719316363335, Final Batch Loss: 0.021473530679941177\n",
      "Epoch 1998, Loss: 0.038372536189854145, Final Batch Loss: 0.024127431213855743\n",
      "Epoch 1999, Loss: 0.043720917776227, Final Batch Loss: 0.025039825588464737\n",
      "Epoch 2000, Loss: 0.017642814200371504, Final Batch Loss: 0.004602580796927214\n",
      "Epoch 2001, Loss: 0.017630092334002256, Final Batch Loss: 0.006184536498039961\n",
      "Epoch 2002, Loss: 0.020044788718223572, Final Batch Loss: 0.013683782890439034\n",
      "Epoch 2003, Loss: 0.019827671814709902, Final Batch Loss: 0.014287102967500687\n",
      "Epoch 2004, Loss: 0.02634122548624873, Final Batch Loss: 0.005530967842787504\n",
      "Epoch 2005, Loss: 0.030518872663378716, Final Batch Loss: 0.00526249036192894\n",
      "Epoch 2006, Loss: 0.02620410080999136, Final Batch Loss: 0.019325880333781242\n",
      "Epoch 2007, Loss: 0.04130991315469146, Final Batch Loss: 0.005686508025974035\n",
      "Epoch 2008, Loss: 0.06722963694483042, Final Batch Loss: 0.01330492738634348\n",
      "Epoch 2009, Loss: 0.058087123557925224, Final Batch Loss: 0.04621016979217529\n",
      "Epoch 2010, Loss: 0.0078000950161367655, Final Batch Loss: 0.003960413858294487\n",
      "Epoch 2011, Loss: 0.01955633983016014, Final Batch Loss: 0.0039530592039227486\n",
      "Epoch 2012, Loss: 0.020577375777065754, Final Batch Loss: 0.012506436556577682\n",
      "Epoch 2013, Loss: 0.016800443874672055, Final Batch Loss: 0.0034705849830061197\n",
      "Epoch 2014, Loss: 0.023551424965262413, Final Batch Loss: 0.016831394284963608\n",
      "Epoch 2015, Loss: 0.019969348795711994, Final Batch Loss: 0.0062156980857253075\n",
      "Epoch 2016, Loss: 0.0252111554145813, Final Batch Loss: 0.010161561891436577\n",
      "Epoch 2017, Loss: 0.007902236422523856, Final Batch Loss: 0.0026661574374884367\n",
      "Epoch 2018, Loss: 0.03080783598124981, Final Batch Loss: 0.02893987111747265\n",
      "Epoch 2019, Loss: 0.02572328969836235, Final Batch Loss: 0.01677846908569336\n",
      "Epoch 2020, Loss: 0.011399814859032631, Final Batch Loss: 0.006080180872231722\n",
      "Epoch 2021, Loss: 0.04491524584591389, Final Batch Loss: 0.02904306910932064\n",
      "Epoch 2022, Loss: 0.020239680539816618, Final Batch Loss: 0.017646729946136475\n",
      "Epoch 2023, Loss: 0.015491990372538567, Final Batch Loss: 0.00987695250660181\n",
      "Epoch 2024, Loss: 0.03430835483595729, Final Batch Loss: 0.03125123307108879\n",
      "Epoch 2025, Loss: 0.03095266968011856, Final Batch Loss: 0.0160742849111557\n",
      "Epoch 2026, Loss: 0.010229828301817179, Final Batch Loss: 0.0050923931412398815\n",
      "Epoch 2027, Loss: 0.011976972687989473, Final Batch Loss: 0.00767297251150012\n",
      "Epoch 2028, Loss: 0.06339861825108528, Final Batch Loss: 0.019966058433055878\n",
      "Epoch 2029, Loss: 0.02855441626161337, Final Batch Loss: 0.016504019498825073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2030, Loss: 0.02237798972055316, Final Batch Loss: 0.004816861357539892\n",
      "Epoch 2031, Loss: 0.1083388403058052, Final Batch Loss: 0.008880600333213806\n",
      "Epoch 2032, Loss: 0.05209537781774998, Final Batch Loss: 0.04058679938316345\n",
      "Epoch 2033, Loss: 0.027599618770182133, Final Batch Loss: 0.010876129381358624\n",
      "Epoch 2034, Loss: 0.04090170655399561, Final Batch Loss: 0.010265621356666088\n",
      "Epoch 2035, Loss: 0.049291498959064484, Final Batch Loss: 0.026081521064043045\n",
      "Epoch 2036, Loss: 0.04357707500457764, Final Batch Loss: 0.02408728562295437\n",
      "Epoch 2037, Loss: 0.017012684606015682, Final Batch Loss: 0.009694057516753674\n",
      "Epoch 2038, Loss: 0.040560625959187746, Final Batch Loss: 0.03534192219376564\n",
      "Epoch 2039, Loss: 0.011121411342173815, Final Batch Loss: 0.007844250649213791\n",
      "Epoch 2040, Loss: 0.0249938340857625, Final Batch Loss: 0.021492058411240578\n",
      "Epoch 2041, Loss: 0.016903789713978767, Final Batch Loss: 0.004249255172908306\n",
      "Epoch 2042, Loss: 0.016983954701572657, Final Batch Loss: 0.012979088351130486\n",
      "Epoch 2043, Loss: 0.020310747902840376, Final Batch Loss: 0.004971812944859266\n",
      "Epoch 2044, Loss: 0.011237794999033213, Final Batch Loss: 0.004343054257333279\n",
      "Epoch 2045, Loss: 0.029572646133601665, Final Batch Loss: 0.0050211092457175255\n",
      "Epoch 2046, Loss: 0.038709159940481186, Final Batch Loss: 0.03338204696774483\n",
      "Epoch 2047, Loss: 0.021221166476607323, Final Batch Loss: 0.010259361006319523\n",
      "Epoch 2048, Loss: 0.012820106465369463, Final Batch Loss: 0.005567115265876055\n",
      "Epoch 2049, Loss: 0.021168186329305172, Final Batch Loss: 0.013119477778673172\n",
      "Epoch 2050, Loss: 0.017093944363296032, Final Batch Loss: 0.00726084690541029\n",
      "Epoch 2051, Loss: 0.07791930297389627, Final Batch Loss: 0.07405460625886917\n",
      "Epoch 2052, Loss: 0.04550367221236229, Final Batch Loss: 0.009895674884319305\n",
      "Epoch 2053, Loss: 0.04816211713477969, Final Batch Loss: 0.0442488007247448\n",
      "Epoch 2054, Loss: 0.017805817537009716, Final Batch Loss: 0.008410254493355751\n",
      "Epoch 2055, Loss: 0.03452923707664013, Final Batch Loss: 0.0132738146930933\n",
      "Epoch 2056, Loss: 0.08837123215198517, Final Batch Loss: 0.07047399133443832\n",
      "Epoch 2057, Loss: 0.033164539374411106, Final Batch Loss: 0.02317609079182148\n",
      "Epoch 2058, Loss: 0.023865627590566874, Final Batch Loss: 0.005165266338735819\n",
      "Epoch 2059, Loss: 0.0592521196231246, Final Batch Loss: 0.044746145606040955\n",
      "Epoch 2060, Loss: 0.051166314631700516, Final Batch Loss: 0.011559892445802689\n",
      "Epoch 2061, Loss: 0.06212914176285267, Final Batch Loss: 0.04369903355836868\n",
      "Epoch 2062, Loss: 0.06040053069591522, Final Batch Loss: 0.040938686579465866\n",
      "Epoch 2063, Loss: 0.008769776672124863, Final Batch Loss: 0.004151278641074896\n",
      "Epoch 2064, Loss: 0.027819959446787834, Final Batch Loss: 0.017754102125763893\n",
      "Epoch 2065, Loss: 0.13778253179043531, Final Batch Loss: 0.0061719296500086784\n",
      "Epoch 2066, Loss: 0.04777835588902235, Final Batch Loss: 0.008587448857724667\n",
      "Epoch 2067, Loss: 0.09180123172700405, Final Batch Loss: 0.0710228756070137\n",
      "Epoch 2068, Loss: 0.014320970512926579, Final Batch Loss: 0.005682612769305706\n",
      "Epoch 2069, Loss: 0.02549128048121929, Final Batch Loss: 0.01675376296043396\n",
      "Epoch 2070, Loss: 0.021217048168182373, Final Batch Loss: 0.0094708651304245\n",
      "Epoch 2071, Loss: 0.047542758751660585, Final Batch Loss: 0.005114641506224871\n",
      "Epoch 2072, Loss: 0.09096992062404752, Final Batch Loss: 0.08484139293432236\n",
      "Epoch 2073, Loss: 0.02370499726384878, Final Batch Loss: 0.015159736387431622\n",
      "Epoch 2074, Loss: 0.02161006024107337, Final Batch Loss: 0.0070483568124473095\n",
      "Epoch 2075, Loss: 0.04419625364243984, Final Batch Loss: 0.007548121735453606\n",
      "Epoch 2076, Loss: 0.03951733140274882, Final Batch Loss: 0.03373238816857338\n",
      "Epoch 2077, Loss: 0.022564191836863756, Final Batch Loss: 0.018872318789362907\n",
      "Epoch 2078, Loss: 0.031815214082598686, Final Batch Loss: 0.010221675038337708\n",
      "Epoch 2079, Loss: 0.014160658232867718, Final Batch Loss: 0.006985701620578766\n",
      "Epoch 2080, Loss: 0.022381232818588614, Final Batch Loss: 0.0037237757351249456\n",
      "Epoch 2081, Loss: 0.04657991090789437, Final Batch Loss: 0.04153990000486374\n",
      "Epoch 2082, Loss: 0.026056187227368355, Final Batch Loss: 0.004771176725625992\n",
      "Epoch 2083, Loss: 0.0230439358856529, Final Batch Loss: 0.0028994770254939795\n",
      "Epoch 2084, Loss: 0.016140257008373737, Final Batch Loss: 0.010386407375335693\n",
      "Epoch 2085, Loss: 0.018035334069281816, Final Batch Loss: 0.004032570403069258\n",
      "Epoch 2086, Loss: 0.02878729999065399, Final Batch Loss: 0.009614337235689163\n",
      "Epoch 2087, Loss: 0.02371176891028881, Final Batch Loss: 0.003351055085659027\n",
      "Epoch 2088, Loss: 0.038221754133701324, Final Batch Loss: 0.01925770565867424\n",
      "Epoch 2089, Loss: 0.008381686639040709, Final Batch Loss: 0.0041536749340593815\n",
      "Epoch 2090, Loss: 0.03293636767193675, Final Batch Loss: 0.0023177131079137325\n",
      "Epoch 2091, Loss: 0.05661946441978216, Final Batch Loss: 0.00925018172711134\n",
      "Epoch 2092, Loss: 0.010120903374627233, Final Batch Loss: 0.006476455368101597\n",
      "Epoch 2093, Loss: 0.00829626526683569, Final Batch Loss: 0.0025515048764646053\n",
      "Epoch 2094, Loss: 0.013242717832326889, Final Batch Loss: 0.0049637481570243835\n",
      "Epoch 2095, Loss: 0.05073154391720891, Final Batch Loss: 0.005861077923327684\n",
      "Epoch 2096, Loss: 0.019545885268598795, Final Batch Loss: 0.007073416840285063\n",
      "Epoch 2097, Loss: 0.03862851858139038, Final Batch Loss: 0.01079464703798294\n",
      "Epoch 2098, Loss: 0.01948332693427801, Final Batch Loss: 0.007968835532665253\n",
      "Epoch 2099, Loss: 0.013271289179101586, Final Batch Loss: 0.0027362473774701357\n",
      "Epoch 2100, Loss: 0.013048493070527911, Final Batch Loss: 0.009773556143045425\n",
      "Epoch 2101, Loss: 0.007870833273045719, Final Batch Loss: 0.001669936696998775\n",
      "Epoch 2102, Loss: 0.027293079998344183, Final Batch Loss: 0.02211543172597885\n",
      "Epoch 2103, Loss: 0.014010692480951548, Final Batch Loss: 0.010183354839682579\n",
      "Epoch 2104, Loss: 0.039642030373215675, Final Batch Loss: 0.012185975909233093\n",
      "Epoch 2105, Loss: 0.007036334136500955, Final Batch Loss: 0.003240142948925495\n",
      "Epoch 2106, Loss: 0.01004395168274641, Final Batch Loss: 0.0054571437649428844\n",
      "Epoch 2107, Loss: 0.05742851458489895, Final Batch Loss: 0.03646322339773178\n",
      "Epoch 2108, Loss: 0.05177227780222893, Final Batch Loss: 0.03921006992459297\n",
      "Epoch 2109, Loss: 0.02150639146566391, Final Batch Loss: 0.0040779393166303635\n",
      "Epoch 2110, Loss: 0.017425113823264837, Final Batch Loss: 0.005057135131210089\n",
      "Epoch 2111, Loss: 0.01386312348768115, Final Batch Loss: 0.0059967911802232265\n",
      "Epoch 2112, Loss: 0.042215106543153524, Final Batch Loss: 0.005528525914996862\n",
      "Epoch 2113, Loss: 0.03004397079348564, Final Batch Loss: 0.02144867181777954\n",
      "Epoch 2114, Loss: 0.08185815066099167, Final Batch Loss: 0.06345558166503906\n",
      "Epoch 2115, Loss: 0.05284222215414047, Final Batch Loss: 0.03349443897604942\n",
      "Epoch 2116, Loss: 0.022458563558757305, Final Batch Loss: 0.01509851310402155\n",
      "Epoch 2117, Loss: 0.027087482158094645, Final Batch Loss: 0.00437449524179101\n",
      "Epoch 2118, Loss: 0.019708982668817043, Final Batch Loss: 0.010344724170863628\n",
      "Epoch 2119, Loss: 0.027454765513539314, Final Batch Loss: 0.009037008509039879\n",
      "Epoch 2120, Loss: 0.040939542930573225, Final Batch Loss: 0.033589817583560944\n",
      "Epoch 2121, Loss: 0.014349856413900852, Final Batch Loss: 0.008108524605631828\n",
      "Epoch 2122, Loss: 0.028651942498981953, Final Batch Loss: 0.015345086343586445\n",
      "Epoch 2123, Loss: 0.06458211876451969, Final Batch Loss: 0.028476962819695473\n",
      "Epoch 2124, Loss: 0.03337294491939247, Final Batch Loss: 0.0032143115531653166\n",
      "Epoch 2125, Loss: 0.011321333586238325, Final Batch Loss: 0.0018201324855908751\n",
      "Epoch 2126, Loss: 0.047364314552396536, Final Batch Loss: 0.005829860921949148\n",
      "Epoch 2127, Loss: 0.017590951174497604, Final Batch Loss: 0.007999148219823837\n",
      "Epoch 2128, Loss: 0.0664801774546504, Final Batch Loss: 0.06499984860420227\n",
      "Epoch 2129, Loss: 0.052705446258187294, Final Batch Loss: 0.030225325375795364\n",
      "Epoch 2130, Loss: 0.03832431696355343, Final Batch Loss: 0.026773197576403618\n",
      "Epoch 2131, Loss: 0.059803957119584084, Final Batch Loss: 0.047417186200618744\n",
      "Epoch 2132, Loss: 0.06224231794476509, Final Batch Loss: 0.033301472663879395\n",
      "Epoch 2133, Loss: 0.0186148164793849, Final Batch Loss: 0.01285877451300621\n",
      "Epoch 2134, Loss: 0.016728144139051437, Final Batch Loss: 0.0135859539732337\n",
      "Epoch 2135, Loss: 0.030625837855041027, Final Batch Loss: 0.005703923292458057\n",
      "Epoch 2136, Loss: 0.0116609837859869, Final Batch Loss: 0.005239554680883884\n",
      "Epoch 2137, Loss: 0.019159103743731976, Final Batch Loss: 0.010512568056583405\n",
      "Epoch 2138, Loss: 0.020641413051635027, Final Batch Loss: 0.006843530107289553\n",
      "Epoch 2139, Loss: 0.020597069524228573, Final Batch Loss: 0.00692268181592226\n",
      "Epoch 2140, Loss: 0.03384970314800739, Final Batch Loss: 0.008288353681564331\n",
      "Epoch 2141, Loss: 0.04014652129262686, Final Batch Loss: 0.025393741205334663\n",
      "Epoch 2142, Loss: 0.034468747675418854, Final Batch Loss: 0.01738586463034153\n",
      "Epoch 2143, Loss: 0.017487722914665937, Final Batch Loss: 0.013137712143361568\n",
      "Epoch 2144, Loss: 0.02955288253724575, Final Batch Loss: 0.007146650925278664\n",
      "Epoch 2145, Loss: 0.03700077533721924, Final Batch Loss: 0.011083941906690598\n",
      "Epoch 2146, Loss: 0.05498995492234826, Final Batch Loss: 0.0506325401365757\n",
      "Epoch 2147, Loss: 0.009848866378888488, Final Batch Loss: 0.0015785347204655409\n",
      "Epoch 2148, Loss: 0.01122617069631815, Final Batch Loss: 0.0054269577376544476\n",
      "Epoch 2149, Loss: 0.029788291547447443, Final Batch Loss: 0.004273815546184778\n",
      "Epoch 2150, Loss: 0.01010712399147451, Final Batch Loss: 0.0011719891335815191\n",
      "Epoch 2151, Loss: 0.06346813030540943, Final Batch Loss: 0.0186509620398283\n",
      "Epoch 2152, Loss: 0.04302273225039244, Final Batch Loss: 0.01262847613543272\n",
      "Epoch 2153, Loss: 0.07546186540275812, Final Batch Loss: 0.003912151791155338\n",
      "Epoch 2154, Loss: 0.031389166601002216, Final Batch Loss: 0.01704546995460987\n",
      "Epoch 2155, Loss: 0.03264983231201768, Final Batch Loss: 0.02932620979845524\n",
      "Epoch 2156, Loss: 0.0106226010248065, Final Batch Loss: 0.006247997283935547\n",
      "Epoch 2157, Loss: 0.021655275020748377, Final Batch Loss: 0.0161146130412817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2158, Loss: 0.009776663733646274, Final Batch Loss: 0.00370616908185184\n",
      "Epoch 2159, Loss: 0.04556930926628411, Final Batch Loss: 0.04173476621508598\n",
      "Epoch 2160, Loss: 0.011709905229508877, Final Batch Loss: 0.0019116802141070366\n",
      "Epoch 2161, Loss: 0.031014254316687584, Final Batch Loss: 0.011508164927363396\n",
      "Epoch 2162, Loss: 0.011987688252702355, Final Batch Loss: 0.003711179131641984\n",
      "Epoch 2163, Loss: 0.02360659185796976, Final Batch Loss: 0.006507023237645626\n",
      "Epoch 2164, Loss: 0.026977429166436195, Final Batch Loss: 0.015822984278202057\n",
      "Epoch 2165, Loss: 0.032670678570866585, Final Batch Loss: 0.007041623815894127\n",
      "Epoch 2166, Loss: 0.04080317448824644, Final Batch Loss: 0.03310805559158325\n",
      "Epoch 2167, Loss: 0.014121920336037874, Final Batch Loss: 0.004302639048546553\n",
      "Epoch 2168, Loss: 0.007011031382717192, Final Batch Loss: 0.0012674188474193215\n",
      "Epoch 2169, Loss: 0.005110893049277365, Final Batch Loss: 0.001206752727739513\n",
      "Epoch 2170, Loss: 0.018803496845066547, Final Batch Loss: 0.005975713953375816\n",
      "Epoch 2171, Loss: 0.010821308474987745, Final Batch Loss: 0.00715260487049818\n",
      "Epoch 2172, Loss: 0.03645728202536702, Final Batch Loss: 0.029400771483778954\n",
      "Epoch 2173, Loss: 0.044333687517791986, Final Batch Loss: 0.03899066522717476\n",
      "Epoch 2174, Loss: 0.018097090534865856, Final Batch Loss: 0.008265607059001923\n",
      "Epoch 2175, Loss: 0.04191127885133028, Final Batch Loss: 0.008314509876072407\n",
      "Epoch 2176, Loss: 0.023545995354652405, Final Batch Loss: 0.012236213311553001\n",
      "Epoch 2177, Loss: 0.025851473910734057, Final Batch Loss: 0.023743391036987305\n",
      "Epoch 2178, Loss: 0.02125859260559082, Final Batch Loss: 0.010956872254610062\n",
      "Epoch 2179, Loss: 0.05269582197070122, Final Batch Loss: 0.001171700656414032\n",
      "Epoch 2180, Loss: 0.030444287694990635, Final Batch Loss: 0.015994179993867874\n",
      "Epoch 2181, Loss: 0.05693684797734022, Final Batch Loss: 0.04924483597278595\n",
      "Epoch 2182, Loss: 0.03322917688637972, Final Batch Loss: 0.00964249949902296\n",
      "Epoch 2183, Loss: 0.034403933212161064, Final Batch Loss: 0.0051686521619558334\n",
      "Epoch 2184, Loss: 0.033803004771471024, Final Batch Loss: 0.007780846208333969\n",
      "Epoch 2185, Loss: 0.024544376879930496, Final Batch Loss: 0.0201722402125597\n",
      "Epoch 2186, Loss: 0.03397290874272585, Final Batch Loss: 0.02512446977198124\n",
      "Epoch 2187, Loss: 0.00620579719543457, Final Batch Loss: 0.002876312704756856\n",
      "Epoch 2188, Loss: 0.02009871182963252, Final Batch Loss: 0.005123110953718424\n",
      "Epoch 2189, Loss: 0.016923436429351568, Final Batch Loss: 0.00940771959722042\n",
      "Epoch 2190, Loss: 0.01530089252628386, Final Batch Loss: 0.003034785622730851\n",
      "Epoch 2191, Loss: 0.015205968404188752, Final Batch Loss: 0.0021238394547253847\n",
      "Epoch 2192, Loss: 0.008916582446545362, Final Batch Loss: 0.0029091117903590202\n",
      "Epoch 2193, Loss: 0.00571108260191977, Final Batch Loss: 0.003184565110132098\n",
      "Epoch 2194, Loss: 0.020780886057764292, Final Batch Loss: 0.004982775542885065\n",
      "Epoch 2195, Loss: 0.03514218237251043, Final Batch Loss: 0.013331170193850994\n",
      "Epoch 2196, Loss: 0.014502455946058035, Final Batch Loss: 0.011061823926866055\n",
      "Epoch 2197, Loss: 0.02353591425344348, Final Batch Loss: 0.020592011511325836\n",
      "Epoch 2198, Loss: 0.014020706992596388, Final Batch Loss: 0.0020557655952870846\n",
      "Epoch 2199, Loss: 0.031061260029673576, Final Batch Loss: 0.00369332917034626\n",
      "Epoch 2200, Loss: 0.008833714760839939, Final Batch Loss: 0.003355930093675852\n",
      "Epoch 2201, Loss: 0.011495500220917165, Final Batch Loss: 0.0018855895614251494\n",
      "Epoch 2202, Loss: 0.02051689976360649, Final Batch Loss: 0.0017766872188076377\n",
      "Epoch 2203, Loss: 0.06571944244205952, Final Batch Loss: 0.029525207355618477\n",
      "Epoch 2204, Loss: 0.0045331683941185474, Final Batch Loss: 0.0020941030234098434\n",
      "Epoch 2205, Loss: 0.01472553052008152, Final Batch Loss: 0.001984948292374611\n",
      "Epoch 2206, Loss: 0.011339516611769795, Final Batch Loss: 0.0012469824869185686\n",
      "Epoch 2207, Loss: 0.005200255662202835, Final Batch Loss: 0.001662632217630744\n",
      "Epoch 2208, Loss: 0.00872751697897911, Final Batch Loss: 0.0035845120437443256\n",
      "Epoch 2209, Loss: 0.029726884327828884, Final Batch Loss: 0.0071291206404566765\n",
      "Epoch 2210, Loss: 0.07758463267236948, Final Batch Loss: 0.06872406601905823\n",
      "Epoch 2211, Loss: 0.010024013929069042, Final Batch Loss: 0.004820114932954311\n",
      "Epoch 2212, Loss: 0.02361271157860756, Final Batch Loss: 0.020444365218281746\n",
      "Epoch 2213, Loss: 0.05365848168730736, Final Batch Loss: 0.02822194993495941\n",
      "Epoch 2214, Loss: 0.04555410612374544, Final Batch Loss: 0.00735046062618494\n",
      "Epoch 2215, Loss: 0.01863610139116645, Final Batch Loss: 0.012674051336944103\n",
      "Epoch 2216, Loss: 0.036352479830384254, Final Batch Loss: 0.028523830696940422\n",
      "Epoch 2217, Loss: 0.02378293965011835, Final Batch Loss: 0.017634281888604164\n",
      "Epoch 2218, Loss: 0.02460214588791132, Final Batch Loss: 0.0039866892620921135\n",
      "Epoch 2219, Loss: 0.08630367740988731, Final Batch Loss: 0.04993477836251259\n",
      "Epoch 2220, Loss: 0.04721608338877559, Final Batch Loss: 0.005402970593422651\n",
      "Epoch 2221, Loss: 0.03837721701711416, Final Batch Loss: 0.015243406407535076\n",
      "Epoch 2222, Loss: 0.05171947181224823, Final Batch Loss: 0.007098156958818436\n",
      "Epoch 2223, Loss: 0.03759776055812836, Final Batch Loss: 0.03537316620349884\n",
      "Epoch 2224, Loss: 0.024251325987279415, Final Batch Loss: 0.011469715274870396\n",
      "Epoch 2225, Loss: 0.032720424234867096, Final Batch Loss: 0.009562419727444649\n",
      "Epoch 2226, Loss: 0.056436337530612946, Final Batch Loss: 0.03572196140885353\n",
      "Epoch 2227, Loss: 0.00754764536395669, Final Batch Loss: 0.0027754856273531914\n",
      "Epoch 2228, Loss: 0.047411815728992224, Final Batch Loss: 0.040572699159383774\n",
      "Epoch 2229, Loss: 0.03106527030467987, Final Batch Loss: 0.013496581465005875\n",
      "Epoch 2230, Loss: 0.023595346370711923, Final Batch Loss: 0.022744113579392433\n",
      "Epoch 2231, Loss: 0.015886728186160326, Final Batch Loss: 0.010729165747761726\n",
      "Epoch 2232, Loss: 0.02848607930354774, Final Batch Loss: 0.02547970786690712\n",
      "Epoch 2233, Loss: 0.009723112685605884, Final Batch Loss: 0.0028637258801609278\n",
      "Epoch 2234, Loss: 0.0049275882774963975, Final Batch Loss: 0.0008626357885077596\n",
      "Epoch 2235, Loss: 0.015082739293575287, Final Batch Loss: 0.013024558313190937\n",
      "Epoch 2236, Loss: 0.01223947573453188, Final Batch Loss: 0.004473533481359482\n",
      "Epoch 2237, Loss: 0.021068029571324587, Final Batch Loss: 0.004142128396779299\n",
      "Epoch 2238, Loss: 0.019466703291982412, Final Batch Loss: 0.006972404662519693\n",
      "Epoch 2239, Loss: 0.01163088297471404, Final Batch Loss: 0.008113522082567215\n",
      "Epoch 2240, Loss: 0.010514607187360525, Final Batch Loss: 0.0037192623130977154\n",
      "Epoch 2241, Loss: 0.005491818999871612, Final Batch Loss: 0.0038434716407209635\n",
      "Epoch 2242, Loss: 0.02096468023955822, Final Batch Loss: 0.011715325526893139\n",
      "Epoch 2243, Loss: 0.010034949984401464, Final Batch Loss: 0.004157443065196276\n",
      "Epoch 2244, Loss: 0.0212657293304801, Final Batch Loss: 0.015211261808872223\n",
      "Epoch 2245, Loss: 0.004757625749334693, Final Batch Loss: 0.001158004393801093\n",
      "Epoch 2246, Loss: 0.006995182018727064, Final Batch Loss: 0.0014679720625281334\n",
      "Epoch 2247, Loss: 0.0062035564333200455, Final Batch Loss: 0.0024230366107076406\n",
      "Epoch 2248, Loss: 0.00481820001732558, Final Batch Loss: 0.000940105295740068\n",
      "Epoch 2249, Loss: 0.0047324071056209505, Final Batch Loss: 0.0006895916885696352\n",
      "Epoch 2250, Loss: 0.05658949725329876, Final Batch Loss: 0.051629193127155304\n",
      "Epoch 2251, Loss: 0.012274133041501045, Final Batch Loss: 0.0073112607933580875\n",
      "Epoch 2252, Loss: 0.02501109428703785, Final Batch Loss: 0.01300117652863264\n",
      "Epoch 2253, Loss: 0.004906757734715939, Final Batch Loss: 0.001997410086914897\n",
      "Epoch 2254, Loss: 0.03412713366560638, Final Batch Loss: 0.032418835908174515\n",
      "Epoch 2255, Loss: 0.02252301073167473, Final Batch Loss: 0.0015588296810165048\n",
      "Epoch 2256, Loss: 0.04642765782773495, Final Batch Loss: 0.022429658100008965\n",
      "Epoch 2257, Loss: 0.04122474417090416, Final Batch Loss: 0.03316856175661087\n",
      "Epoch 2258, Loss: 0.019682127400301397, Final Batch Loss: 0.01774836890399456\n",
      "Epoch 2259, Loss: 0.012846643337979913, Final Batch Loss: 0.009764221496880054\n",
      "Epoch 2260, Loss: 0.03491028631106019, Final Batch Loss: 0.028069516643881798\n",
      "Epoch 2261, Loss: 0.02537478506565094, Final Batch Loss: 0.014429238624870777\n",
      "Epoch 2262, Loss: 0.02122827060520649, Final Batch Loss: 0.007700452581048012\n",
      "Epoch 2263, Loss: 0.020843189675360918, Final Batch Loss: 0.007181511726230383\n",
      "Epoch 2264, Loss: 0.015581336105242372, Final Batch Loss: 0.0023383975494652987\n",
      "Epoch 2265, Loss: 0.007330937078222632, Final Batch Loss: 0.0030948759522289038\n",
      "Epoch 2266, Loss: 0.0383684653788805, Final Batch Loss: 0.02553783543407917\n",
      "Epoch 2267, Loss: 0.013051417656242847, Final Batch Loss: 0.005407715681940317\n",
      "Epoch 2268, Loss: 0.05862486269325018, Final Batch Loss: 0.05527171865105629\n",
      "Epoch 2269, Loss: 0.010755072114989161, Final Batch Loss: 0.002468485152348876\n",
      "Epoch 2270, Loss: 0.009378015762194991, Final Batch Loss: 0.006518499460071325\n",
      "Epoch 2271, Loss: 0.005682456539943814, Final Batch Loss: 0.003197581972926855\n",
      "Epoch 2272, Loss: 0.005173178622499108, Final Batch Loss: 0.0027669500559568405\n",
      "Epoch 2273, Loss: 0.008314750622957945, Final Batch Loss: 0.005148882977664471\n",
      "Epoch 2274, Loss: 0.019988980144262314, Final Batch Loss: 0.0036446284502744675\n",
      "Epoch 2275, Loss: 0.03681866265833378, Final Batch Loss: 0.02279348112642765\n",
      "Epoch 2276, Loss: 0.02082175575196743, Final Batch Loss: 0.005525141954421997\n",
      "Epoch 2277, Loss: 0.022019403520971537, Final Batch Loss: 0.005600346717983484\n",
      "Epoch 2278, Loss: 0.030035600066184998, Final Batch Loss: 0.007129820063710213\n",
      "Epoch 2279, Loss: 0.008993261959403753, Final Batch Loss: 0.002706027589738369\n",
      "Epoch 2280, Loss: 0.006126920925453305, Final Batch Loss: 0.0028713892679661512\n",
      "Epoch 2281, Loss: 0.01478509558364749, Final Batch Loss: 0.004120758268982172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2282, Loss: 0.004337250837124884, Final Batch Loss: 0.0029438682831823826\n",
      "Epoch 2283, Loss: 0.025220580399036407, Final Batch Loss: 0.008323777467012405\n",
      "Epoch 2284, Loss: 0.06031627766788006, Final Batch Loss: 0.015629755333065987\n",
      "Epoch 2285, Loss: 0.013112685875967145, Final Batch Loss: 0.009433182887732983\n",
      "Epoch 2286, Loss: 0.021523381350561976, Final Batch Loss: 0.0026139586698263884\n",
      "Epoch 2287, Loss: 0.09477772191166878, Final Batch Loss: 0.03467564284801483\n",
      "Epoch 2288, Loss: 0.010910252574831247, Final Batch Loss: 0.002341593150049448\n",
      "Epoch 2289, Loss: 0.02334121474996209, Final Batch Loss: 0.0027577881701290607\n",
      "Epoch 2290, Loss: 0.006950180744752288, Final Batch Loss: 0.004859752021729946\n",
      "Epoch 2291, Loss: 0.014572684653103352, Final Batch Loss: 0.007682474330067635\n",
      "Epoch 2292, Loss: 0.009201463137287647, Final Batch Loss: 0.000896645535249263\n",
      "Epoch 2293, Loss: 0.03057590569369495, Final Batch Loss: 0.027292044833302498\n",
      "Epoch 2294, Loss: 0.04632819816470146, Final Batch Loss: 0.02233046479523182\n",
      "Epoch 2295, Loss: 0.008311673766002059, Final Batch Loss: 0.005543493665754795\n",
      "Epoch 2296, Loss: 0.03527806233614683, Final Batch Loss: 0.012283052317798138\n",
      "Epoch 2297, Loss: 0.012459770310670137, Final Batch Loss: 0.004380386788398027\n",
      "Epoch 2298, Loss: 0.025648855604231358, Final Batch Loss: 0.016830570995807648\n",
      "Epoch 2299, Loss: 0.013181202695704997, Final Batch Loss: 0.0017078478122130036\n",
      "Epoch 2300, Loss: 0.03267559129744768, Final Batch Loss: 0.0018593622371554375\n",
      "Epoch 2301, Loss: 0.0038647906621918082, Final Batch Loss: 0.002494444604963064\n",
      "Epoch 2302, Loss: 0.0085993860848248, Final Batch Loss: 0.004272528924047947\n",
      "Epoch 2303, Loss: 0.04210765240713954, Final Batch Loss: 0.035176996141672134\n",
      "Epoch 2304, Loss: 0.016387843759730458, Final Batch Loss: 0.0020358071196824312\n",
      "Epoch 2305, Loss: 0.03299928014166653, Final Batch Loss: 0.0307003166526556\n",
      "Epoch 2306, Loss: 0.008348891511559486, Final Batch Loss: 0.004873229656368494\n",
      "Epoch 2307, Loss: 0.03480831300839782, Final Batch Loss: 0.0016156821511685848\n",
      "Epoch 2308, Loss: 0.04311358369886875, Final Batch Loss: 0.02163497731089592\n",
      "Epoch 2309, Loss: 0.014187656342983246, Final Batch Loss: 0.012240828014910221\n",
      "Epoch 2310, Loss: 0.027821477968245745, Final Batch Loss: 0.02359492890536785\n",
      "Epoch 2311, Loss: 0.00743937399238348, Final Batch Loss: 0.004340033978223801\n",
      "Epoch 2312, Loss: 0.05574890784919262, Final Batch Loss: 0.03811616078019142\n",
      "Epoch 2313, Loss: 0.00766370166093111, Final Batch Loss: 0.0037999972701072693\n",
      "Epoch 2314, Loss: 0.023265565978363156, Final Batch Loss: 0.021223019808530807\n",
      "Epoch 2315, Loss: 0.01161259040236473, Final Batch Loss: 0.009165589697659016\n",
      "Epoch 2316, Loss: 0.013596357079222798, Final Batch Loss: 0.011495368555188179\n",
      "Epoch 2317, Loss: 0.01567014353349805, Final Batch Loss: 0.009180544875562191\n",
      "Epoch 2318, Loss: 0.006535108317621052, Final Batch Loss: 0.005318847019225359\n",
      "Epoch 2319, Loss: 0.016528730047866702, Final Batch Loss: 0.0032378181349486113\n",
      "Epoch 2320, Loss: 0.050974094308912754, Final Batch Loss: 0.045945361256599426\n",
      "Epoch 2321, Loss: 0.003287576255388558, Final Batch Loss: 0.0014833936002105474\n",
      "Epoch 2322, Loss: 0.012455980642698705, Final Batch Loss: 0.0009972028201445937\n",
      "Epoch 2323, Loss: 0.010336099192500114, Final Batch Loss: 0.004145956132560968\n",
      "Epoch 2324, Loss: 0.008854068000800908, Final Batch Loss: 0.0016650977777317166\n",
      "Epoch 2325, Loss: 0.03573341807350516, Final Batch Loss: 0.029621196910738945\n",
      "Epoch 2326, Loss: 0.026075586676597595, Final Batch Loss: 0.009871233254671097\n",
      "Epoch 2327, Loss: 0.012489675544202328, Final Batch Loss: 0.007059077266603708\n",
      "Epoch 2328, Loss: 0.007450102362781763, Final Batch Loss: 0.00482975086197257\n",
      "Epoch 2329, Loss: 0.025549433194100857, Final Batch Loss: 0.016979312524199486\n",
      "Epoch 2330, Loss: 0.01346500008367002, Final Batch Loss: 0.0033548984210938215\n",
      "Epoch 2331, Loss: 0.01704815635457635, Final Batch Loss: 0.00531742861494422\n",
      "Epoch 2332, Loss: 0.005617865361273289, Final Batch Loss: 0.0032977410592138767\n",
      "Epoch 2333, Loss: 0.0086401361040771, Final Batch Loss: 0.0023162015713751316\n",
      "Epoch 2334, Loss: 0.10626555606722832, Final Batch Loss: 0.005874212831258774\n",
      "Epoch 2335, Loss: 0.027138200646732002, Final Batch Loss: 0.0007171100587584078\n",
      "Epoch 2336, Loss: 0.013134898617863655, Final Batch Loss: 0.005239840596914291\n",
      "Epoch 2337, Loss: 0.013455216074362397, Final Batch Loss: 0.00978977233171463\n",
      "Epoch 2338, Loss: 0.004836814245209098, Final Batch Loss: 0.001936239656060934\n",
      "Epoch 2339, Loss: 0.02544498909264803, Final Batch Loss: 0.004668195731937885\n",
      "Epoch 2340, Loss: 0.005541702616028488, Final Batch Loss: 0.004009020049124956\n",
      "Epoch 2341, Loss: 0.01767263049259782, Final Batch Loss: 0.015835804864764214\n",
      "Epoch 2342, Loss: 0.016782024642452598, Final Batch Loss: 0.0026235615368932486\n",
      "Epoch 2343, Loss: 0.013043172541074455, Final Batch Loss: 0.001583247329108417\n",
      "Epoch 2344, Loss: 0.01951763406395912, Final Batch Loss: 0.011356927454471588\n",
      "Epoch 2345, Loss: 0.008296577259898186, Final Batch Loss: 0.005888872314244509\n",
      "Epoch 2346, Loss: 0.031111340038478374, Final Batch Loss: 0.004692300222814083\n",
      "Epoch 2347, Loss: 0.01378675946034491, Final Batch Loss: 0.002264193492010236\n",
      "Epoch 2348, Loss: 0.01323221018537879, Final Batch Loss: 0.007469285745173693\n",
      "Epoch 2349, Loss: 0.0956888496875763, Final Batch Loss: 0.01166445016860962\n",
      "Epoch 2350, Loss: 0.008690849179401994, Final Batch Loss: 0.006123123224824667\n",
      "Epoch 2351, Loss: 0.014543281635269523, Final Batch Loss: 0.002589349402114749\n",
      "Epoch 2352, Loss: 0.006301283836364746, Final Batch Loss: 0.0029297098517417908\n",
      "Epoch 2353, Loss: 0.0671273386105895, Final Batch Loss: 0.05520102009177208\n",
      "Epoch 2354, Loss: 0.01966045587323606, Final Batch Loss: 0.002211415907368064\n",
      "Epoch 2355, Loss: 0.024861769401468337, Final Batch Loss: 0.0016728617483749986\n",
      "Epoch 2356, Loss: 0.008870570454746485, Final Batch Loss: 0.003868410363793373\n",
      "Epoch 2357, Loss: 0.007093154359608889, Final Batch Loss: 0.004197665955871344\n",
      "Epoch 2358, Loss: 0.07054230477660894, Final Batch Loss: 0.0562623105943203\n",
      "Epoch 2359, Loss: 0.016671030782163143, Final Batch Loss: 0.011049695312976837\n",
      "Epoch 2360, Loss: 0.03269878216087818, Final Batch Loss: 0.007824858650565147\n",
      "Epoch 2361, Loss: 0.034806910902261734, Final Batch Loss: 0.018521113321185112\n",
      "Epoch 2362, Loss: 0.07416219171136618, Final Batch Loss: 0.06029032915830612\n",
      "Epoch 2363, Loss: 0.05307129863649607, Final Batch Loss: 0.013635006733238697\n",
      "Epoch 2364, Loss: 0.012851330451667309, Final Batch Loss: 0.0053996797651052475\n",
      "Epoch 2365, Loss: 0.0322521299822256, Final Batch Loss: 0.00068960792850703\n",
      "Epoch 2366, Loss: 0.017136733047664165, Final Batch Loss: 0.008538035675883293\n",
      "Epoch 2367, Loss: 0.021708990447223186, Final Batch Loss: 0.018041018396615982\n",
      "Epoch 2368, Loss: 0.09557006135582924, Final Batch Loss: 0.046642474830150604\n",
      "Epoch 2369, Loss: 0.03315318515524268, Final Batch Loss: 0.0022109164856374264\n",
      "Epoch 2370, Loss: 0.04058910347521305, Final Batch Loss: 0.012657782062888145\n",
      "Epoch 2371, Loss: 0.009388019796460867, Final Batch Loss: 0.004912428557872772\n",
      "Epoch 2372, Loss: 0.03241713088937104, Final Batch Loss: 0.030153926461935043\n",
      "Epoch 2373, Loss: 0.025686561595648527, Final Batch Loss: 0.01868484728038311\n",
      "Epoch 2374, Loss: 0.045668247155845165, Final Batch Loss: 0.04151429235935211\n",
      "Epoch 2375, Loss: 0.029533184599131346, Final Batch Loss: 0.024040959775447845\n",
      "Epoch 2376, Loss: 0.016652857419103384, Final Batch Loss: 0.009950365871191025\n",
      "Epoch 2377, Loss: 0.01439518528059125, Final Batch Loss: 0.0040360488928854465\n",
      "Epoch 2378, Loss: 0.03286021947860718, Final Batch Loss: 0.015583602711558342\n",
      "Epoch 2379, Loss: 0.00806148792617023, Final Batch Loss: 0.0025734466034919024\n",
      "Epoch 2380, Loss: 0.04025792516767979, Final Batch Loss: 0.014689011499285698\n",
      "Epoch 2381, Loss: 0.022091432474553585, Final Batch Loss: 0.005581763572990894\n",
      "Epoch 2382, Loss: 0.02696111984550953, Final Batch Loss: 0.011014774441719055\n",
      "Epoch 2383, Loss: 0.022227410227060318, Final Batch Loss: 0.015837326645851135\n",
      "Epoch 2384, Loss: 0.03098690463230014, Final Batch Loss: 0.00637842295691371\n",
      "Epoch 2385, Loss: 0.027850619982928038, Final Batch Loss: 0.023710984736680984\n",
      "Epoch 2386, Loss: 0.02415767521597445, Final Batch Loss: 0.0024788437876850367\n",
      "Epoch 2387, Loss: 0.014717967249453068, Final Batch Loss: 0.0035674413666129112\n",
      "Epoch 2388, Loss: 0.033184597035869956, Final Batch Loss: 0.03037123568356037\n",
      "Epoch 2389, Loss: 0.015227568335831165, Final Batch Loss: 0.005330990068614483\n",
      "Epoch 2390, Loss: 0.02256982261314988, Final Batch Loss: 0.006278099957853556\n",
      "Epoch 2391, Loss: 0.016195595962926745, Final Batch Loss: 0.003222608705982566\n",
      "Epoch 2392, Loss: 0.021486750338226557, Final Batch Loss: 0.014523714780807495\n",
      "Epoch 2393, Loss: 0.025604051537811756, Final Batch Loss: 0.007494964636862278\n",
      "Epoch 2394, Loss: 0.013409574399702251, Final Batch Loss: 0.012275876477360725\n",
      "Epoch 2395, Loss: 0.03934321366250515, Final Batch Loss: 0.005586585029959679\n",
      "Epoch 2396, Loss: 0.0881422171369195, Final Batch Loss: 0.011967138387262821\n",
      "Epoch 2397, Loss: 0.01218261499889195, Final Batch Loss: 0.0024579011369496584\n",
      "Epoch 2398, Loss: 0.0994889996945858, Final Batch Loss: 0.09526243805885315\n",
      "Epoch 2399, Loss: 0.016183900646865368, Final Batch Loss: 0.007879769429564476\n",
      "Epoch 2400, Loss: 0.02325258473865688, Final Batch Loss: 0.019555380567908287\n",
      "Epoch 2401, Loss: 0.01482737553305924, Final Batch Loss: 0.0022866923827677965\n",
      "Epoch 2402, Loss: 0.008719298057258129, Final Batch Loss: 0.0061264936812222\n",
      "Epoch 2403, Loss: 0.04697599168866873, Final Batch Loss: 0.0426611565053463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2404, Loss: 0.02698739292100072, Final Batch Loss: 0.021163005381822586\n",
      "Epoch 2405, Loss: 0.019809004850685596, Final Batch Loss: 0.008220143616199493\n",
      "Epoch 2406, Loss: 0.061581049114465714, Final Batch Loss: 0.016400478780269623\n",
      "Epoch 2407, Loss: 0.02835469599813223, Final Batch Loss: 0.021164653822779655\n",
      "Epoch 2408, Loss: 0.03889173443894833, Final Batch Loss: 0.0011691654799506068\n",
      "Epoch 2409, Loss: 0.03432739246636629, Final Batch Loss: 0.02516438625752926\n",
      "Epoch 2410, Loss: 0.024084447417408228, Final Batch Loss: 0.01641254313290119\n",
      "Epoch 2411, Loss: 0.04361917823553085, Final Batch Loss: 0.033555712550878525\n",
      "Epoch 2412, Loss: 0.01124598877504468, Final Batch Loss: 0.005228645633906126\n",
      "Epoch 2413, Loss: 0.07250098884105682, Final Batch Loss: 0.04049103334546089\n",
      "Epoch 2414, Loss: 0.013641832862049341, Final Batch Loss: 0.010971573181450367\n",
      "Epoch 2415, Loss: 0.006298104883171618, Final Batch Loss: 0.005078291054815054\n",
      "Epoch 2416, Loss: 0.022811785340309143, Final Batch Loss: 0.017721189185976982\n",
      "Epoch 2417, Loss: 0.014495460549369454, Final Batch Loss: 0.0023653574753552675\n",
      "Epoch 2418, Loss: 0.027649543713778257, Final Batch Loss: 0.005537464749068022\n",
      "Epoch 2419, Loss: 0.01808083546347916, Final Batch Loss: 0.014976218342781067\n",
      "Epoch 2420, Loss: 0.04428595956414938, Final Batch Loss: 0.01370339933782816\n",
      "Epoch 2421, Loss: 0.07391748065128922, Final Batch Loss: 0.06980966031551361\n",
      "Epoch 2422, Loss: 0.011561131803318858, Final Batch Loss: 0.009298287332057953\n",
      "Epoch 2423, Loss: 0.013926093466579914, Final Batch Loss: 0.00688320305198431\n",
      "Epoch 2424, Loss: 0.03078814223408699, Final Batch Loss: 0.005273854359984398\n",
      "Epoch 2425, Loss: 0.037630668841302395, Final Batch Loss: 0.028313802555203438\n",
      "Epoch 2426, Loss: 0.027072629891335964, Final Batch Loss: 0.013968579471111298\n",
      "Epoch 2427, Loss: 0.09151781816035509, Final Batch Loss: 0.08297298848628998\n",
      "Epoch 2428, Loss: 0.015990173909813166, Final Batch Loss: 0.00824291817843914\n",
      "Epoch 2429, Loss: 0.12743255496025085, Final Batch Loss: 0.041024088859558105\n",
      "Epoch 2430, Loss: 0.027922429144382477, Final Batch Loss: 0.010158907622098923\n",
      "Epoch 2431, Loss: 0.021002407651394606, Final Batch Loss: 0.004972351249307394\n",
      "Epoch 2432, Loss: 0.10706797800958157, Final Batch Loss: 0.0878065750002861\n",
      "Epoch 2433, Loss: 0.12269655615091324, Final Batch Loss: 0.05377555638551712\n",
      "Epoch 2434, Loss: 0.03369162906892598, Final Batch Loss: 0.0036856115330010653\n",
      "Epoch 2435, Loss: 0.041285621002316475, Final Batch Loss: 0.02607804536819458\n",
      "Epoch 2436, Loss: 0.04049639729782939, Final Batch Loss: 0.03431874141097069\n",
      "Epoch 2437, Loss: 0.022798626217991114, Final Batch Loss: 0.007122537586838007\n",
      "Epoch 2438, Loss: 0.05156906694173813, Final Batch Loss: 0.006159588694572449\n",
      "Epoch 2439, Loss: 0.008323796093463898, Final Batch Loss: 0.004249906167387962\n",
      "Epoch 2440, Loss: 0.03471288736909628, Final Batch Loss: 0.02105163224041462\n",
      "Epoch 2441, Loss: 0.10846066102385521, Final Batch Loss: 0.05886184796690941\n",
      "Epoch 2442, Loss: 0.005163182038813829, Final Batch Loss: 0.0028259761165827513\n",
      "Epoch 2443, Loss: 0.035982951521873474, Final Batch Loss: 0.024952635169029236\n",
      "Epoch 2444, Loss: 0.03385428385809064, Final Batch Loss: 0.003117621410638094\n",
      "Epoch 2445, Loss: 0.01843969477340579, Final Batch Loss: 0.011838001199066639\n",
      "Epoch 2446, Loss: 0.051312586292624474, Final Batch Loss: 0.03129534795880318\n",
      "Epoch 2447, Loss: 0.04400147404521704, Final Batch Loss: 0.004076509736478329\n",
      "Epoch 2448, Loss: 0.02046798774972558, Final Batch Loss: 0.005257666576653719\n",
      "Epoch 2449, Loss: 0.02729489398188889, Final Batch Loss: 0.02413918264210224\n",
      "Epoch 2450, Loss: 0.013044258113950491, Final Batch Loss: 0.004726045299321413\n",
      "Epoch 2451, Loss: 0.01588341360911727, Final Batch Loss: 0.009655142202973366\n",
      "Epoch 2452, Loss: 0.029125606175512075, Final Batch Loss: 0.003204397391527891\n",
      "Epoch 2453, Loss: 0.07599694281816483, Final Batch Loss: 0.0619707815349102\n",
      "Epoch 2454, Loss: 0.022748494520783424, Final Batch Loss: 0.020276442170143127\n",
      "Epoch 2455, Loss: 0.007825117791071534, Final Batch Loss: 0.005402209237217903\n",
      "Epoch 2456, Loss: 0.0129397539421916, Final Batch Loss: 0.006066338159143925\n",
      "Epoch 2457, Loss: 0.0326338903978467, Final Batch Loss: 0.02259529009461403\n",
      "Epoch 2458, Loss: 0.028497409541159868, Final Batch Loss: 0.00776659557595849\n",
      "Epoch 2459, Loss: 0.01162272784858942, Final Batch Loss: 0.005641563329845667\n",
      "Epoch 2460, Loss: 0.020809894427657127, Final Batch Loss: 0.007750768214464188\n",
      "Epoch 2461, Loss: 0.026944628916680813, Final Batch Loss: 0.01146200392395258\n",
      "Epoch 2462, Loss: 0.017765455413609743, Final Batch Loss: 0.0032555456273257732\n",
      "Epoch 2463, Loss: 0.03291282430291176, Final Batch Loss: 0.018297338858246803\n",
      "Epoch 2464, Loss: 0.01474771648645401, Final Batch Loss: 0.0031280992552638054\n",
      "Epoch 2465, Loss: 0.004020716063678265, Final Batch Loss: 0.0011693623382598162\n",
      "Epoch 2466, Loss: 0.0308787205722183, Final Batch Loss: 0.0028163527604192495\n",
      "Epoch 2467, Loss: 0.008360623032785952, Final Batch Loss: 0.0018389668548479676\n",
      "Epoch 2468, Loss: 0.04788876045495272, Final Batch Loss: 0.007471674121916294\n",
      "Epoch 2469, Loss: 0.00659144064411521, Final Batch Loss: 0.0040635112673044205\n",
      "Epoch 2470, Loss: 0.007244761916808784, Final Batch Loss: 0.0018881851574406028\n",
      "Epoch 2471, Loss: 0.019564203452318907, Final Batch Loss: 0.016565371304750443\n",
      "Epoch 2472, Loss: 0.01752089080400765, Final Batch Loss: 0.00284138903953135\n",
      "Epoch 2473, Loss: 0.026706534437835217, Final Batch Loss: 0.017209723591804504\n",
      "Epoch 2474, Loss: 0.020227747038006783, Final Batch Loss: 0.015314926393330097\n",
      "Epoch 2475, Loss: 0.014208590146154165, Final Batch Loss: 0.0023519000969827175\n",
      "Epoch 2476, Loss: 0.009821855695918202, Final Batch Loss: 0.0069427951239049435\n",
      "Epoch 2477, Loss: 0.026630843058228493, Final Batch Loss: 0.024307360872626305\n",
      "Epoch 2478, Loss: 0.00989702739752829, Final Batch Loss: 0.0037907951045781374\n",
      "Epoch 2479, Loss: 0.006092429452110082, Final Batch Loss: 0.0008568170596845448\n",
      "Epoch 2480, Loss: 0.03695730306208134, Final Batch Loss: 0.030552208423614502\n",
      "Epoch 2481, Loss: 0.01968132983893156, Final Batch Loss: 0.011559449136257172\n",
      "Epoch 2482, Loss: 0.03731216490268707, Final Batch Loss: 0.010353205725550652\n",
      "Epoch 2483, Loss: 0.007580022094771266, Final Batch Loss: 0.0027787263970822096\n",
      "Epoch 2484, Loss: 0.013515649130567908, Final Batch Loss: 0.010334381833672523\n",
      "Epoch 2485, Loss: 0.04859502054750919, Final Batch Loss: 0.014791229739785194\n",
      "Epoch 2486, Loss: 0.07329667825251818, Final Batch Loss: 0.006554481573402882\n",
      "Epoch 2487, Loss: 0.015136650297790766, Final Batch Loss: 0.009572159498929977\n",
      "Epoch 2488, Loss: 0.02503988053649664, Final Batch Loss: 0.01673349365592003\n",
      "Epoch 2489, Loss: 0.015006431378424168, Final Batch Loss: 0.010316677391529083\n",
      "Epoch 2490, Loss: 0.02957847062498331, Final Batch Loss: 0.01934749446809292\n",
      "Epoch 2491, Loss: 0.006113164126873016, Final Batch Loss: 0.0021826494485139847\n",
      "Epoch 2492, Loss: 0.025920928921550512, Final Batch Loss: 0.02149815298616886\n",
      "Epoch 2493, Loss: 0.06311825104057789, Final Batch Loss: 0.032936930656433105\n",
      "Epoch 2494, Loss: 0.03686945699155331, Final Batch Loss: 0.03286734223365784\n",
      "Epoch 2495, Loss: 0.023526139557361603, Final Batch Loss: 0.01638498529791832\n",
      "Epoch 2496, Loss: 0.021642494946718216, Final Batch Loss: 0.010924160480499268\n",
      "Epoch 2497, Loss: 0.025952578522264957, Final Batch Loss: 0.012797607108950615\n",
      "Epoch 2498, Loss: 0.05153161194175482, Final Batch Loss: 0.004562976770102978\n",
      "Epoch 2499, Loss: 0.0069520846009254456, Final Batch Loss: 0.0034768811892718077\n",
      "Epoch 2500, Loss: 0.0067827594466507435, Final Batch Loss: 0.005046165082603693\n",
      "Epoch 2501, Loss: 0.05424448102712631, Final Batch Loss: 0.011390496045351028\n",
      "Epoch 2502, Loss: 0.028624484781175852, Final Batch Loss: 0.0054571074433624744\n",
      "Epoch 2503, Loss: 0.01991398725658655, Final Batch Loss: 0.00804133340716362\n",
      "Epoch 2504, Loss: 0.007209334755316377, Final Batch Loss: 0.003582930425181985\n",
      "Epoch 2505, Loss: 0.026672878302633762, Final Batch Loss: 0.005303920246660709\n",
      "Epoch 2506, Loss: 0.007555922609753907, Final Batch Loss: 0.005948228761553764\n",
      "Epoch 2507, Loss: 0.016957751475274563, Final Batch Loss: 0.01067882589995861\n",
      "Epoch 2508, Loss: 0.03991690557450056, Final Batch Loss: 0.03127165883779526\n",
      "Epoch 2509, Loss: 0.005083943950012326, Final Batch Loss: 0.001218714751303196\n",
      "Epoch 2510, Loss: 0.020630710758268833, Final Batch Loss: 0.003608222119510174\n",
      "Epoch 2511, Loss: 0.012222561519593, Final Batch Loss: 0.0065710293129086494\n",
      "Epoch 2512, Loss: 0.011460099834948778, Final Batch Loss: 0.0026309662498533726\n",
      "Epoch 2513, Loss: 0.00883798603899777, Final Batch Loss: 0.0014605221804231405\n",
      "Epoch 2514, Loss: 0.01641549332998693, Final Batch Loss: 0.013943232595920563\n",
      "Epoch 2515, Loss: 0.008390638511627913, Final Batch Loss: 0.003953746519982815\n",
      "Epoch 2516, Loss: 0.04540184698998928, Final Batch Loss: 0.020342139527201653\n",
      "Epoch 2517, Loss: 0.011199355125427246, Final Batch Loss: 0.004912284668534994\n",
      "Epoch 2518, Loss: 0.014274937566369772, Final Batch Loss: 0.010981536470353603\n",
      "Epoch 2519, Loss: 0.05253239534795284, Final Batch Loss: 0.01577945239841938\n",
      "Epoch 2520, Loss: 0.03683023015037179, Final Batch Loss: 0.03462088108062744\n",
      "Epoch 2521, Loss: 0.06568394228816032, Final Batch Loss: 0.025607898831367493\n",
      "Epoch 2522, Loss: 0.03993994742631912, Final Batch Loss: 0.0207580104470253\n",
      "Epoch 2523, Loss: 0.024395864456892014, Final Batch Loss: 0.005948079749941826\n",
      "Epoch 2524, Loss: 0.04539143946021795, Final Batch Loss: 0.037536341696977615\n",
      "Epoch 2525, Loss: 0.04745087120682001, Final Batch Loss: 0.005196874029934406\n",
      "Epoch 2526, Loss: 0.006852653343230486, Final Batch Loss: 0.0026614144444465637\n",
      "Epoch 2527, Loss: 0.11694738268852234, Final Batch Loss: 0.08040349930524826\n",
      "Epoch 2528, Loss: 0.03850749507546425, Final Batch Loss: 0.025073217228055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2529, Loss: 0.10390861704945564, Final Batch Loss: 0.08175362646579742\n",
      "Epoch 2530, Loss: 0.055506397038698196, Final Batch Loss: 0.02032267302274704\n",
      "Epoch 2531, Loss: 0.03260860126465559, Final Batch Loss: 0.019810283556580544\n",
      "Epoch 2532, Loss: 0.08382287621498108, Final Batch Loss: 0.04378129914402962\n",
      "Epoch 2533, Loss: 0.02752749342471361, Final Batch Loss: 0.017187470570206642\n",
      "Epoch 2534, Loss: 0.013773493003100157, Final Batch Loss: 0.0024414299987256527\n",
      "Epoch 2535, Loss: 0.011164612602442503, Final Batch Loss: 0.003868520725518465\n",
      "Epoch 2536, Loss: 0.0754376295953989, Final Batch Loss: 0.029442934319376945\n",
      "Epoch 2537, Loss: 0.016371360514312983, Final Batch Loss: 0.012862464413046837\n",
      "Epoch 2538, Loss: 0.023256922140717506, Final Batch Loss: 0.011687833815813065\n",
      "Epoch 2539, Loss: 0.010645710164681077, Final Batch Loss: 0.0030347153078764677\n",
      "Epoch 2540, Loss: 0.05133666004985571, Final Batch Loss: 0.03854962810873985\n",
      "Epoch 2541, Loss: 0.013601580052636564, Final Batch Loss: 0.0011801760410889983\n",
      "Epoch 2542, Loss: 0.02696114405989647, Final Batch Loss: 0.01674722507596016\n",
      "Epoch 2543, Loss: 0.01786684012040496, Final Batch Loss: 0.002614361699670553\n",
      "Epoch 2544, Loss: 0.03470790898427367, Final Batch Loss: 0.00619686720892787\n",
      "Epoch 2545, Loss: 0.012807230930775404, Final Batch Loss: 0.007956658490002155\n",
      "Epoch 2546, Loss: 0.016968972980976105, Final Batch Loss: 0.008792562410235405\n",
      "Epoch 2547, Loss: 0.027417746372520924, Final Batch Loss: 0.014502029865980148\n",
      "Epoch 2548, Loss: 0.043066905811429024, Final Batch Loss: 0.03749125078320503\n",
      "Epoch 2549, Loss: 0.01834399253129959, Final Batch Loss: 0.008259902708232403\n",
      "Epoch 2550, Loss: 0.022464525187388062, Final Batch Loss: 0.0036178643349558115\n",
      "Epoch 2551, Loss: 0.024341840762645006, Final Batch Loss: 0.01900574006140232\n",
      "Epoch 2552, Loss: 0.04444798082113266, Final Batch Loss: 0.029201624915003777\n",
      "Epoch 2553, Loss: 0.08389585139229894, Final Batch Loss: 0.006884042639285326\n",
      "Epoch 2554, Loss: 0.023632268887013197, Final Batch Loss: 0.005580059718340635\n",
      "Epoch 2555, Loss: 0.006804643431678414, Final Batch Loss: 0.0025436177384108305\n",
      "Epoch 2556, Loss: 0.030120511539280415, Final Batch Loss: 0.016829298809170723\n",
      "Epoch 2557, Loss: 0.018613768508657813, Final Batch Loss: 0.016496872529387474\n",
      "Epoch 2558, Loss: 0.031007167417556047, Final Batch Loss: 0.02371157892048359\n",
      "Epoch 2559, Loss: 0.01004139706492424, Final Batch Loss: 0.002700437791645527\n",
      "Epoch 2560, Loss: 0.02743353578262031, Final Batch Loss: 0.023973245173692703\n",
      "Epoch 2561, Loss: 0.009002218255773187, Final Batch Loss: 0.003405065508559346\n",
      "Epoch 2562, Loss: 0.020661712624132633, Final Batch Loss: 0.015090929344296455\n",
      "Epoch 2563, Loss: 0.0429855699185282, Final Batch Loss: 0.002805710071697831\n",
      "Epoch 2564, Loss: 0.011726458091288805, Final Batch Loss: 0.007909819483757019\n",
      "Epoch 2565, Loss: 0.03594874031841755, Final Batch Loss: 0.02051800675690174\n",
      "Epoch 2566, Loss: 0.009031818248331547, Final Batch Loss: 0.004624506458640099\n",
      "Epoch 2567, Loss: 0.024553020484745502, Final Batch Loss: 0.013746214099228382\n",
      "Epoch 2568, Loss: 0.039733766578137875, Final Batch Loss: 0.024338822811841965\n",
      "Epoch 2569, Loss: 0.035696645732969046, Final Batch Loss: 0.004109775181859732\n",
      "Epoch 2570, Loss: 0.03799822460860014, Final Batch Loss: 0.0070165591314435005\n",
      "Epoch 2571, Loss: 0.008798382710665464, Final Batch Loss: 0.0047310288064181805\n",
      "Epoch 2572, Loss: 0.05736125633120537, Final Batch Loss: 0.023578498512506485\n",
      "Epoch 2573, Loss: 0.007893358822911978, Final Batch Loss: 0.002506792079657316\n",
      "Epoch 2574, Loss: 0.006986196618527174, Final Batch Loss: 0.004286848474293947\n",
      "Epoch 2575, Loss: 0.00920947827398777, Final Batch Loss: 0.0043547870591282845\n",
      "Epoch 2576, Loss: 0.024714063853025436, Final Batch Loss: 0.012658169493079185\n",
      "Epoch 2577, Loss: 0.056422400288283825, Final Batch Loss: 0.0063267601653933525\n",
      "Epoch 2578, Loss: 0.022546753520146012, Final Batch Loss: 0.0019524137023836374\n",
      "Epoch 2579, Loss: 0.01377637847326696, Final Batch Loss: 0.011808928102254868\n",
      "Epoch 2580, Loss: 0.009209381067194045, Final Batch Loss: 0.00753798196092248\n",
      "Epoch 2581, Loss: 0.015612982213497162, Final Batch Loss: 0.009814885444939137\n",
      "Epoch 2582, Loss: 0.007764667039737105, Final Batch Loss: 0.0038614629302173853\n",
      "Epoch 2583, Loss: 0.016221160534769297, Final Batch Loss: 0.0059633697383105755\n",
      "Epoch 2584, Loss: 0.017428497318178415, Final Batch Loss: 0.010807164944708347\n",
      "Epoch 2585, Loss: 0.029500643664505333, Final Batch Loss: 0.0006989528774283826\n",
      "Epoch 2586, Loss: 0.016272099688649178, Final Batch Loss: 0.002481491304934025\n",
      "Epoch 2587, Loss: 0.027748160995543003, Final Batch Loss: 0.004429996944963932\n",
      "Epoch 2588, Loss: 0.030199273955076933, Final Batch Loss: 0.022852234542369843\n",
      "Epoch 2589, Loss: 0.03360822983086109, Final Batch Loss: 0.010804733261466026\n",
      "Epoch 2590, Loss: 0.010139639489352703, Final Batch Loss: 0.004870512522757053\n",
      "Epoch 2591, Loss: 0.005348638165742159, Final Batch Loss: 0.0026463745161890984\n",
      "Epoch 2592, Loss: 0.022195530124008656, Final Batch Loss: 0.0070577869191765785\n",
      "Epoch 2593, Loss: 0.04004850098863244, Final Batch Loss: 0.03426195681095123\n",
      "Epoch 2594, Loss: 0.043968431651592255, Final Batch Loss: 0.04045334458351135\n",
      "Epoch 2595, Loss: 0.07825918402522802, Final Batch Loss: 0.002695350907742977\n",
      "Epoch 2596, Loss: 0.009814067743718624, Final Batch Loss: 0.005279596894979477\n",
      "Epoch 2597, Loss: 0.06963411439210176, Final Batch Loss: 0.06600311398506165\n",
      "Epoch 2598, Loss: 0.024581320118159056, Final Batch Loss: 0.004964468535035849\n",
      "Epoch 2599, Loss: 0.043643418699502945, Final Batch Loss: 0.017333460971713066\n",
      "Epoch 2600, Loss: 0.12017431296408176, Final Batch Loss: 0.01513008214533329\n",
      "Epoch 2601, Loss: 0.05979103548452258, Final Batch Loss: 0.05566167086362839\n",
      "Epoch 2602, Loss: 0.22186975553631783, Final Batch Loss: 0.05922255292534828\n",
      "Epoch 2603, Loss: 0.011553661664947867, Final Batch Loss: 0.0038931609597057104\n",
      "Epoch 2604, Loss: 0.10602158214896917, Final Batch Loss: 0.09682902693748474\n",
      "Epoch 2605, Loss: 0.03488961420953274, Final Batch Loss: 0.0172659233212471\n",
      "Epoch 2606, Loss: 0.1204211376607418, Final Batch Loss: 0.10637229681015015\n",
      "Epoch 2607, Loss: 0.06843784917145967, Final Batch Loss: 0.00988831464201212\n",
      "Epoch 2608, Loss: 0.021598684135824442, Final Batch Loss: 0.002919240389019251\n",
      "Epoch 2609, Loss: 0.04620487615466118, Final Batch Loss: 0.03739401698112488\n",
      "Epoch 2610, Loss: 0.04449508339166641, Final Batch Loss: 0.010371696203947067\n",
      "Epoch 2611, Loss: 0.06010626442730427, Final Batch Loss: 0.015336507931351662\n",
      "Epoch 2612, Loss: 0.038490574806928635, Final Batch Loss: 0.024150317534804344\n",
      "Epoch 2613, Loss: 0.08026712946593761, Final Batch Loss: 0.030934737995266914\n",
      "Epoch 2614, Loss: 0.021181484684348106, Final Batch Loss: 0.010529892519116402\n",
      "Epoch 2615, Loss: 0.02363810781389475, Final Batch Loss: 0.018082495778799057\n",
      "Epoch 2616, Loss: 0.053165871649980545, Final Batch Loss: 0.044373247772455215\n",
      "Epoch 2617, Loss: 0.06211505550891161, Final Batch Loss: 0.04815594106912613\n",
      "Epoch 2618, Loss: 0.025320101296529174, Final Batch Loss: 0.003027148311957717\n",
      "Epoch 2619, Loss: 0.019173534587025642, Final Batch Loss: 0.014404342509806156\n",
      "Epoch 2620, Loss: 0.05393911525607109, Final Batch Loss: 0.028713364154100418\n",
      "Epoch 2621, Loss: 0.03488372080028057, Final Batch Loss: 0.007279643788933754\n",
      "Epoch 2622, Loss: 0.012730973307043314, Final Batch Loss: 0.0029730121605098248\n",
      "Epoch 2623, Loss: 0.021411900874227285, Final Batch Loss: 0.01702754758298397\n",
      "Epoch 2624, Loss: 0.04932594671845436, Final Batch Loss: 0.02316206321120262\n",
      "Epoch 2625, Loss: 0.042931877076625824, Final Batch Loss: 0.01632118411362171\n",
      "Epoch 2626, Loss: 0.026570837944746017, Final Batch Loss: 0.0032000020146369934\n",
      "Epoch 2627, Loss: 0.09718501195311546, Final Batch Loss: 0.03555819019675255\n",
      "Epoch 2628, Loss: 0.09276682743802667, Final Batch Loss: 0.08829580247402191\n",
      "Epoch 2629, Loss: 0.012945644557476044, Final Batch Loss: 0.008966607041656971\n",
      "Epoch 2630, Loss: 0.03948553209193051, Final Batch Loss: 0.003394692437723279\n",
      "Epoch 2631, Loss: 0.016758111771196127, Final Batch Loss: 0.013458020985126495\n",
      "Epoch 2632, Loss: 0.0736961318179965, Final Batch Loss: 0.0041782064363360405\n",
      "Epoch 2633, Loss: 0.020194114185869694, Final Batch Loss: 0.01502286922186613\n",
      "Epoch 2634, Loss: 0.019476202316582203, Final Batch Loss: 0.0077749453485012054\n",
      "Epoch 2635, Loss: 0.02953891735523939, Final Batch Loss: 0.022830037400126457\n",
      "Epoch 2636, Loss: 0.02094645146280527, Final Batch Loss: 0.014863241463899612\n",
      "Epoch 2637, Loss: 0.042028602212667465, Final Batch Loss: 0.03845197334885597\n",
      "Epoch 2638, Loss: 0.020993093959987164, Final Batch Loss: 0.016698382794857025\n",
      "Epoch 2639, Loss: 0.04650327656418085, Final Batch Loss: 0.012854711152613163\n",
      "Epoch 2640, Loss: 0.03632016736082733, Final Batch Loss: 0.0031019251327961683\n",
      "Epoch 2641, Loss: 0.03133171331137419, Final Batch Loss: 0.021338026970624924\n",
      "Epoch 2642, Loss: 0.02208973467350006, Final Batch Loss: 0.013269157148897648\n",
      "Epoch 2643, Loss: 0.010740099940449, Final Batch Loss: 0.00505284545943141\n",
      "Epoch 2644, Loss: 0.009433906758204103, Final Batch Loss: 0.00289529818110168\n",
      "Epoch 2645, Loss: 0.027787894010543823, Final Batch Loss: 0.020809896290302277\n",
      "Epoch 2646, Loss: 0.019676311872899532, Final Batch Loss: 0.00982960220426321\n",
      "Epoch 2647, Loss: 0.03889805544167757, Final Batch Loss: 0.027162916958332062\n",
      "Epoch 2648, Loss: 0.025792607106268406, Final Batch Loss: 0.011633924208581448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2649, Loss: 0.029540907591581345, Final Batch Loss: 0.011234130710363388\n",
      "Epoch 2650, Loss: 0.03383803367614746, Final Batch Loss: 0.0183833260089159\n",
      "Epoch 2651, Loss: 0.004600591259077191, Final Batch Loss: 0.0022576656192541122\n",
      "Epoch 2652, Loss: 0.014017248060554266, Final Batch Loss: 0.0048401192761957645\n",
      "Epoch 2653, Loss: 0.009549390757456422, Final Batch Loss: 0.0032545176800340414\n",
      "Epoch 2654, Loss: 0.012861135881394148, Final Batch Loss: 0.003874028567224741\n",
      "Epoch 2655, Loss: 0.06161841191351414, Final Batch Loss: 0.0031965095549821854\n",
      "Epoch 2656, Loss: 0.011273641604930162, Final Batch Loss: 0.004575195722281933\n",
      "Epoch 2657, Loss: 0.02073952555656433, Final Batch Loss: 0.011595181189477444\n",
      "Epoch 2658, Loss: 0.008916831109672785, Final Batch Loss: 0.0029078577645123005\n",
      "Epoch 2659, Loss: 0.010636020451784134, Final Batch Loss: 0.00823771208524704\n",
      "Epoch 2660, Loss: 0.04651890881359577, Final Batch Loss: 0.027359645813703537\n",
      "Epoch 2661, Loss: 0.006378155434504151, Final Batch Loss: 0.003056610468775034\n",
      "Epoch 2662, Loss: 0.016872272826731205, Final Batch Loss: 0.010275939479470253\n",
      "Epoch 2663, Loss: 0.0050917749758809805, Final Batch Loss: 0.0019959306810051203\n",
      "Epoch 2664, Loss: 0.006344037828966975, Final Batch Loss: 0.0020887337159365416\n",
      "Epoch 2665, Loss: 0.02345246123149991, Final Batch Loss: 0.005395457614213228\n",
      "Epoch 2666, Loss: 0.03384387632831931, Final Batch Loss: 0.030399363487958908\n",
      "Epoch 2667, Loss: 0.01103914063423872, Final Batch Loss: 0.005496766418218613\n",
      "Epoch 2668, Loss: 0.01102282851934433, Final Batch Loss: 0.004157467279583216\n",
      "Epoch 2669, Loss: 0.010866355616599321, Final Batch Loss: 0.0045117842964828014\n",
      "Epoch 2670, Loss: 0.02416926622390747, Final Batch Loss: 0.006063347682356834\n",
      "Epoch 2671, Loss: 0.029415569733828306, Final Batch Loss: 0.004827891942113638\n",
      "Epoch 2672, Loss: 0.0069223251193761826, Final Batch Loss: 0.0037081646732985973\n",
      "Epoch 2673, Loss: 0.02565907617099583, Final Batch Loss: 0.003186593996360898\n",
      "Epoch 2674, Loss: 0.014127161353826523, Final Batch Loss: 0.004540850408375263\n",
      "Epoch 2675, Loss: 0.021135562798008323, Final Batch Loss: 0.0013952252920717\n",
      "Epoch 2676, Loss: 0.04392752074636519, Final Batch Loss: 0.040865376591682434\n",
      "Epoch 2677, Loss: 0.04849326005205512, Final Batch Loss: 0.04617017135024071\n",
      "Epoch 2678, Loss: 0.01054210215806961, Final Batch Loss: 0.004479716997593641\n",
      "Epoch 2679, Loss: 0.0055365245789289474, Final Batch Loss: 0.001541870180517435\n",
      "Epoch 2680, Loss: 0.02631292212754488, Final Batch Loss: 0.012653549201786518\n",
      "Epoch 2681, Loss: 0.0048288044054061174, Final Batch Loss: 0.0021733324974775314\n",
      "Epoch 2682, Loss: 0.022425662726163864, Final Batch Loss: 0.005240319296717644\n",
      "Epoch 2683, Loss: 0.0065040732733905315, Final Batch Loss: 0.0012788386084139347\n",
      "Epoch 2684, Loss: 0.00799065176397562, Final Batch Loss: 0.003990075085312128\n",
      "Epoch 2685, Loss: 0.01237497664988041, Final Batch Loss: 0.005030242260545492\n",
      "Epoch 2686, Loss: 0.005069358740001917, Final Batch Loss: 0.0019295010715723038\n",
      "Epoch 2687, Loss: 0.02078584348782897, Final Batch Loss: 0.003340733703225851\n",
      "Epoch 2688, Loss: 0.018055766820907593, Final Batch Loss: 0.007047605700790882\n",
      "Epoch 2689, Loss: 0.017910145223140717, Final Batch Loss: 0.0028473902493715286\n",
      "Epoch 2690, Loss: 0.004041346022859216, Final Batch Loss: 0.0018113546539098024\n",
      "Epoch 2691, Loss: 0.021210383623838425, Final Batch Loss: 0.011272300034761429\n",
      "Epoch 2692, Loss: 0.05442292429506779, Final Batch Loss: 0.04659295827150345\n",
      "Epoch 2693, Loss: 0.017346030566841364, Final Batch Loss: 0.004703246522694826\n",
      "Epoch 2694, Loss: 0.01583171123638749, Final Batch Loss: 0.0024261525832116604\n",
      "Epoch 2695, Loss: 0.014121646294370294, Final Batch Loss: 0.0031119559425860643\n",
      "Epoch 2696, Loss: 0.03046410344541073, Final Batch Loss: 0.003931781277060509\n",
      "Epoch 2697, Loss: 0.021642538718879223, Final Batch Loss: 0.017466789111495018\n",
      "Epoch 2698, Loss: 0.006270571844652295, Final Batch Loss: 0.00362074445001781\n",
      "Epoch 2699, Loss: 0.020425228402018547, Final Batch Loss: 0.011955251917243004\n",
      "Epoch 2700, Loss: 0.012617243686690927, Final Batch Loss: 0.003135677659884095\n",
      "Epoch 2701, Loss: 0.018805827014148235, Final Batch Loss: 0.012107971124351025\n",
      "Epoch 2702, Loss: 0.013182934606447816, Final Batch Loss: 0.003259938443079591\n",
      "Epoch 2703, Loss: 0.018172255717217922, Final Batch Loss: 0.005256324075162411\n",
      "Epoch 2704, Loss: 0.015350486384704709, Final Batch Loss: 0.012039948254823685\n",
      "Epoch 2705, Loss: 0.025593508034944534, Final Batch Loss: 0.007295204326510429\n",
      "Epoch 2706, Loss: 0.010583442635834217, Final Batch Loss: 0.004258647561073303\n",
      "Epoch 2707, Loss: 0.014123842353001237, Final Batch Loss: 0.011626695282757282\n",
      "Epoch 2708, Loss: 0.03305692784488201, Final Batch Loss: 0.01459270529448986\n",
      "Epoch 2709, Loss: 0.014369837939739227, Final Batch Loss: 0.009336252696812153\n",
      "Epoch 2710, Loss: 0.03180293133482337, Final Batch Loss: 0.006859749089926481\n",
      "Epoch 2711, Loss: 0.01223768899217248, Final Batch Loss: 0.006018215790390968\n",
      "Epoch 2712, Loss: 0.00882396288216114, Final Batch Loss: 0.0041832043789327145\n",
      "Epoch 2713, Loss: 0.01935623912140727, Final Batch Loss: 0.004473928827792406\n",
      "Epoch 2714, Loss: 0.008929566014558077, Final Batch Loss: 0.0023676701821386814\n",
      "Epoch 2715, Loss: 0.007786061614751816, Final Batch Loss: 0.0035382984206080437\n",
      "Epoch 2716, Loss: 0.034134541638195515, Final Batch Loss: 0.03075242042541504\n",
      "Epoch 2717, Loss: 0.04702789895236492, Final Batch Loss: 0.02903292328119278\n",
      "Epoch 2718, Loss: 0.007369006751105189, Final Batch Loss: 0.003425904316827655\n",
      "Epoch 2719, Loss: 0.006441370118409395, Final Batch Loss: 0.0022343196906149387\n",
      "Epoch 2720, Loss: 0.02527533902321011, Final Batch Loss: 0.001698015839792788\n",
      "Epoch 2721, Loss: 0.03159901965409517, Final Batch Loss: 0.014711878262460232\n",
      "Epoch 2722, Loss: 0.07590887136757374, Final Batch Loss: 0.06934503465890884\n",
      "Epoch 2723, Loss: 0.006436256226152182, Final Batch Loss: 0.0025382181629538536\n",
      "Epoch 2724, Loss: 0.028272385243326426, Final Batch Loss: 0.02225339412689209\n",
      "Epoch 2725, Loss: 0.007952381856739521, Final Batch Loss: 0.0036255717277526855\n",
      "Epoch 2726, Loss: 0.03622166579589248, Final Batch Loss: 0.03131024166941643\n",
      "Epoch 2727, Loss: 0.011111451545730233, Final Batch Loss: 0.008432772941887379\n",
      "Epoch 2728, Loss: 0.0195197737775743, Final Batch Loss: 0.012985494919121265\n",
      "Epoch 2729, Loss: 0.048971226904541254, Final Batch Loss: 0.0053395614959299564\n",
      "Epoch 2730, Loss: 0.013484674273058772, Final Batch Loss: 0.003229050664231181\n",
      "Epoch 2731, Loss: 0.005765892099589109, Final Batch Loss: 0.0035640536807477474\n",
      "Epoch 2732, Loss: 0.02069263020530343, Final Batch Loss: 0.00326108792796731\n",
      "Epoch 2733, Loss: 0.02090028114616871, Final Batch Loss: 0.00722434651106596\n",
      "Epoch 2734, Loss: 0.036975922994315624, Final Batch Loss: 0.008851570077240467\n",
      "Epoch 2735, Loss: 0.023599373176693916, Final Batch Loss: 0.0198327898979187\n",
      "Epoch 2736, Loss: 0.01516845403239131, Final Batch Loss: 0.0113545972853899\n",
      "Epoch 2737, Loss: 0.05487128533422947, Final Batch Loss: 0.025636345148086548\n",
      "Epoch 2738, Loss: 0.01093876687809825, Final Batch Loss: 0.005008304957300425\n",
      "Epoch 2739, Loss: 0.006161151686683297, Final Batch Loss: 0.0017998798284679651\n",
      "Epoch 2740, Loss: 0.006796952104195952, Final Batch Loss: 0.002694574883207679\n",
      "Epoch 2741, Loss: 0.04386731144040823, Final Batch Loss: 0.028256654739379883\n",
      "Epoch 2742, Loss: 0.05707258824259043, Final Batch Loss: 0.00821539107710123\n",
      "Epoch 2743, Loss: 0.012440061662346125, Final Batch Loss: 0.006025989539921284\n",
      "Epoch 2744, Loss: 0.014863842399790883, Final Batch Loss: 0.003583494806662202\n",
      "Epoch 2745, Loss: 0.05074344761669636, Final Batch Loss: 0.009865464642643929\n",
      "Epoch 2746, Loss: 0.024086156859993935, Final Batch Loss: 0.0027150865644216537\n",
      "Epoch 2747, Loss: 0.03467115107923746, Final Batch Loss: 0.025738388299942017\n",
      "Epoch 2748, Loss: 0.007942654425278306, Final Batch Loss: 0.0027762975078076124\n",
      "Epoch 2749, Loss: 0.07919343188405037, Final Batch Loss: 0.016664620488882065\n",
      "Epoch 2750, Loss: 0.004512950545176864, Final Batch Loss: 0.0017731329426169395\n",
      "Epoch 2751, Loss: 0.0260426034219563, Final Batch Loss: 0.018786797299981117\n",
      "Epoch 2752, Loss: 0.010475557763129473, Final Batch Loss: 0.001334118191152811\n",
      "Epoch 2753, Loss: 0.02744391723535955, Final Batch Loss: 0.0031190754380077124\n",
      "Epoch 2754, Loss: 0.011715945205651224, Final Batch Loss: 0.009984094649553299\n",
      "Epoch 2755, Loss: 0.12605633214116096, Final Batch Loss: 0.07694706320762634\n",
      "Epoch 2756, Loss: 0.03676867764443159, Final Batch Loss: 0.026246512308716774\n",
      "Epoch 2757, Loss: 0.015347552951425314, Final Batch Loss: 0.008578415028750896\n",
      "Epoch 2758, Loss: 0.010056990198791027, Final Batch Loss: 0.007339787669479847\n",
      "Epoch 2759, Loss: 0.008399009006097913, Final Batch Loss: 0.002730173757299781\n",
      "Epoch 2760, Loss: 0.040925958193838596, Final Batch Loss: 0.015560819767415524\n",
      "Epoch 2761, Loss: 0.009915981907397509, Final Batch Loss: 0.005112463608384132\n",
      "Epoch 2762, Loss: 0.05207977443933487, Final Batch Loss: 0.0023877881467342377\n",
      "Epoch 2763, Loss: 0.006301750196143985, Final Batch Loss: 0.0033864956349134445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2764, Loss: 0.013734352309256792, Final Batch Loss: 0.00898043904453516\n",
      "Epoch 2765, Loss: 0.012875620392151177, Final Batch Loss: 0.0017774711595848203\n",
      "Epoch 2766, Loss: 0.04911964014172554, Final Batch Loss: 0.024358145892620087\n",
      "Epoch 2767, Loss: 0.017630680464208126, Final Batch Loss: 0.002753610722720623\n",
      "Epoch 2768, Loss: 0.014216685900464654, Final Batch Loss: 0.011358542367815971\n",
      "Epoch 2769, Loss: 0.01335297292098403, Final Batch Loss: 0.005031795706599951\n",
      "Epoch 2770, Loss: 0.03383570071309805, Final Batch Loss: 0.011043510399758816\n",
      "Epoch 2771, Loss: 0.005457696737721562, Final Batch Loss: 0.0021402991842478514\n",
      "Epoch 2772, Loss: 0.009335831739008427, Final Batch Loss: 0.004769120831042528\n",
      "Epoch 2773, Loss: 0.03812929941341281, Final Batch Loss: 0.005821827333420515\n",
      "Epoch 2774, Loss: 0.015247628092765808, Final Batch Loss: 0.010973680764436722\n",
      "Epoch 2775, Loss: 0.005352570908144116, Final Batch Loss: 0.0025255039799958467\n",
      "Epoch 2776, Loss: 0.028748492244631052, Final Batch Loss: 0.0038566947914659977\n",
      "Epoch 2777, Loss: 0.09418342262506485, Final Batch Loss: 0.04517513886094093\n",
      "Epoch 2778, Loss: 0.02044159430079162, Final Batch Loss: 0.0028843663167208433\n",
      "Epoch 2779, Loss: 0.052848820108920336, Final Batch Loss: 0.05005938187241554\n",
      "Epoch 2780, Loss: 0.03871039766818285, Final Batch Loss: 0.03414516896009445\n",
      "Epoch 2781, Loss: 0.0065044385846704245, Final Batch Loss: 0.0042146495543420315\n",
      "Epoch 2782, Loss: 0.14952785521745682, Final Batch Loss: 0.08385102450847626\n",
      "Epoch 2783, Loss: 0.015140435425564647, Final Batch Loss: 0.0035467862617224455\n",
      "Epoch 2784, Loss: 0.03895304258912802, Final Batch Loss: 0.02592081017792225\n",
      "Epoch 2785, Loss: 0.1094659399241209, Final Batch Loss: 0.09961855411529541\n",
      "Epoch 2786, Loss: 0.02986055496148765, Final Batch Loss: 0.001094964100047946\n",
      "Epoch 2787, Loss: 0.03097985079512, Final Batch Loss: 0.004636900033801794\n",
      "Epoch 2788, Loss: 0.07408648356795311, Final Batch Loss: 0.04454037547111511\n",
      "Epoch 2789, Loss: 0.013350172666832805, Final Batch Loss: 0.010290578939020634\n",
      "Epoch 2790, Loss: 0.0759803936816752, Final Batch Loss: 0.07080844789743423\n",
      "Epoch 2791, Loss: 0.008578525157645345, Final Batch Loss: 0.0058659641072154045\n",
      "Epoch 2792, Loss: 0.02499854564666748, Final Batch Loss: 0.02018030360341072\n",
      "Epoch 2793, Loss: 0.009067527018487453, Final Batch Loss: 0.004754559602588415\n",
      "Epoch 2794, Loss: 0.012385002337396145, Final Batch Loss: 0.004693442955613136\n",
      "Epoch 2795, Loss: 0.016910044010728598, Final Batch Loss: 0.009537024423480034\n",
      "Epoch 2796, Loss: 0.013398527633398771, Final Batch Loss: 0.009496599435806274\n",
      "Epoch 2797, Loss: 0.032766823656857014, Final Batch Loss: 0.02334558591246605\n",
      "Epoch 2798, Loss: 0.05769277922809124, Final Batch Loss: 0.019894583150744438\n",
      "Epoch 2799, Loss: 0.047474308870732784, Final Batch Loss: 0.015014364384114742\n",
      "Epoch 2800, Loss: 0.011117450194433331, Final Batch Loss: 0.009265596978366375\n",
      "Epoch 2801, Loss: 0.05589906871318817, Final Batch Loss: 0.04081209376454353\n",
      "Epoch 2802, Loss: 0.005163982044905424, Final Batch Loss: 0.0009581861086189747\n",
      "Epoch 2803, Loss: 0.009659752948209643, Final Batch Loss: 0.003901624819263816\n",
      "Epoch 2804, Loss: 0.04135163314640522, Final Batch Loss: 0.03113935887813568\n",
      "Epoch 2805, Loss: 0.014316128799691796, Final Batch Loss: 0.0017804654780775309\n",
      "Epoch 2806, Loss: 0.01970518473535776, Final Batch Loss: 0.008212407119572163\n",
      "Epoch 2807, Loss: 0.03553594509139657, Final Batch Loss: 0.03421158343553543\n",
      "Epoch 2808, Loss: 0.006366901099681854, Final Batch Loss: 0.002577744424343109\n",
      "Epoch 2809, Loss: 0.033943924121558666, Final Batch Loss: 0.009222383610904217\n",
      "Epoch 2810, Loss: 0.00577697332482785, Final Batch Loss: 0.001860549091361463\n",
      "Epoch 2811, Loss: 0.00641124602407217, Final Batch Loss: 0.0021675648167729378\n",
      "Epoch 2812, Loss: 0.052971167489886284, Final Batch Loss: 0.011922771111130714\n",
      "Epoch 2813, Loss: 0.018994970247149467, Final Batch Loss: 0.01090551819652319\n",
      "Epoch 2814, Loss: 0.010348995216190815, Final Batch Loss: 0.005411746446043253\n",
      "Epoch 2815, Loss: 0.01558647770434618, Final Batch Loss: 0.006934366188943386\n",
      "Epoch 2816, Loss: 0.031155550852417946, Final Batch Loss: 0.02555834874510765\n",
      "Epoch 2817, Loss: 0.02081856830045581, Final Batch Loss: 0.0065162000246346\n",
      "Epoch 2818, Loss: 0.007179687963798642, Final Batch Loss: 0.00063866819255054\n",
      "Epoch 2819, Loss: 0.011875040829181671, Final Batch Loss: 0.007177538238465786\n",
      "Epoch 2820, Loss: 0.04023092705756426, Final Batch Loss: 0.03020869940519333\n",
      "Epoch 2821, Loss: 0.015190210426226258, Final Batch Loss: 0.011627951636910439\n",
      "Epoch 2822, Loss: 0.011220380198210478, Final Batch Loss: 0.007369528524577618\n",
      "Epoch 2823, Loss: 0.0029413029551506042, Final Batch Loss: 0.0007105686236172915\n",
      "Epoch 2824, Loss: 0.017257655039429665, Final Batch Loss: 0.01194889098405838\n",
      "Epoch 2825, Loss: 0.029542325530201197, Final Batch Loss: 0.004151100758463144\n",
      "Epoch 2826, Loss: 0.07176431245170534, Final Batch Loss: 0.06825859844684601\n",
      "Epoch 2827, Loss: 0.007140264380723238, Final Batch Loss: 0.0029982267878949642\n",
      "Epoch 2828, Loss: 0.012697183527052402, Final Batch Loss: 0.007565680891275406\n",
      "Epoch 2829, Loss: 0.07693865336477757, Final Batch Loss: 0.058669913560152054\n",
      "Epoch 2830, Loss: 0.011939944699406624, Final Batch Loss: 0.003864297643303871\n",
      "Epoch 2831, Loss: 0.012639194959774613, Final Batch Loss: 0.009580138139426708\n",
      "Epoch 2832, Loss: 0.03690232615917921, Final Batch Loss: 0.0237816721200943\n",
      "Epoch 2833, Loss: 0.00990275526419282, Final Batch Loss: 0.0048158676363527775\n",
      "Epoch 2834, Loss: 0.01521662320010364, Final Batch Loss: 0.011873854324221611\n",
      "Epoch 2835, Loss: 0.0037056601140648127, Final Batch Loss: 0.001162966014817357\n",
      "Epoch 2836, Loss: 0.011267766123637557, Final Batch Loss: 0.002111955313012004\n",
      "Epoch 2837, Loss: 0.008753809845075011, Final Batch Loss: 0.0037445526104420424\n",
      "Epoch 2838, Loss: 0.004980942467227578, Final Batch Loss: 0.0028135767206549644\n",
      "Epoch 2839, Loss: 0.01615514955483377, Final Batch Loss: 0.0025888781528919935\n",
      "Epoch 2840, Loss: 0.0056602576514706016, Final Batch Loss: 0.0013917832402512431\n",
      "Epoch 2841, Loss: 0.025659507140517235, Final Batch Loss: 0.021741177886724472\n",
      "Epoch 2842, Loss: 0.004811379709281027, Final Batch Loss: 0.001724731526337564\n",
      "Epoch 2843, Loss: 0.0073087618220597506, Final Batch Loss: 0.0024493082892149687\n",
      "Epoch 2844, Loss: 0.008936455706134439, Final Batch Loss: 0.005452009383589029\n",
      "Epoch 2845, Loss: 0.010111716343089938, Final Batch Loss: 0.00815609097480774\n",
      "Epoch 2846, Loss: 0.023031520191580057, Final Batch Loss: 0.004547013435512781\n",
      "Epoch 2847, Loss: 0.0045470132026821375, Final Batch Loss: 0.002308555180206895\n",
      "Epoch 2848, Loss: 0.012995895463973284, Final Batch Loss: 0.005528117064386606\n",
      "Epoch 2849, Loss: 0.009976420784369111, Final Batch Loss: 0.00233165523968637\n",
      "Epoch 2850, Loss: 0.02614688768517226, Final Batch Loss: 0.024798182770609856\n",
      "Epoch 2851, Loss: 0.004128094296902418, Final Batch Loss: 0.0029455169569700956\n",
      "Epoch 2852, Loss: 0.0027953663957305253, Final Batch Loss: 0.0005714796134270728\n",
      "Epoch 2853, Loss: 0.018212452996522188, Final Batch Loss: 0.0029385467059910297\n",
      "Epoch 2854, Loss: 0.004086406552232802, Final Batch Loss: 0.0011402597883716226\n",
      "Epoch 2855, Loss: 0.026690599974244833, Final Batch Loss: 0.00539817800745368\n",
      "Epoch 2856, Loss: 0.03079311642795801, Final Batch Loss: 0.020238764584064484\n",
      "Epoch 2857, Loss: 0.00411935563897714, Final Batch Loss: 0.0007688910118304193\n",
      "Epoch 2858, Loss: 0.007265565975103527, Final Batch Loss: 0.0008768304833211005\n",
      "Epoch 2859, Loss: 0.014252359513193369, Final Batch Loss: 0.010371350683271885\n",
      "Epoch 2860, Loss: 0.010960108134895563, Final Batch Loss: 0.00411302549764514\n",
      "Epoch 2861, Loss: 0.032011279836297035, Final Batch Loss: 0.0081231240183115\n",
      "Epoch 2862, Loss: 0.008136292221024632, Final Batch Loss: 0.005986582022160292\n",
      "Epoch 2863, Loss: 0.02463814930524677, Final Batch Loss: 0.0016414964338764548\n",
      "Epoch 2864, Loss: 0.014258109033107758, Final Batch Loss: 0.010563415475189686\n",
      "Epoch 2865, Loss: 0.00686586182564497, Final Batch Loss: 0.0021104933694005013\n",
      "Epoch 2866, Loss: 0.05480669601820409, Final Batch Loss: 0.002515768399462104\n",
      "Epoch 2867, Loss: 0.005431221332401037, Final Batch Loss: 0.0016034464351832867\n",
      "Epoch 2868, Loss: 0.05114452913403511, Final Batch Loss: 0.02308446168899536\n",
      "Epoch 2869, Loss: 0.0050390728283673525, Final Batch Loss: 0.0012955274432897568\n",
      "Epoch 2870, Loss: 0.016667043790221214, Final Batch Loss: 0.014030424878001213\n",
      "Epoch 2871, Loss: 0.03385673789307475, Final Batch Loss: 0.03222746029496193\n",
      "Epoch 2872, Loss: 0.0160802697064355, Final Batch Loss: 0.001361260307021439\n",
      "Epoch 2873, Loss: 0.009429555153474212, Final Batch Loss: 0.0056182993575930595\n",
      "Epoch 2874, Loss: 0.03666737023741007, Final Batch Loss: 0.004726589657366276\n",
      "Epoch 2875, Loss: 0.02152542443946004, Final Batch Loss: 0.016402458772063255\n",
      "Epoch 2876, Loss: 0.010149895213544369, Final Batch Loss: 0.004203154239803553\n",
      "Epoch 2877, Loss: 0.022656517568975687, Final Batch Loss: 0.018442420288920403\n",
      "Epoch 2878, Loss: 0.03161122649908066, Final Batch Loss: 0.0033181384205818176\n",
      "Epoch 2879, Loss: 0.002117819676641375, Final Batch Loss: 0.0011472062906250358\n",
      "Epoch 2880, Loss: 0.007132880942663178, Final Batch Loss: 0.00031806211336515844\n",
      "Epoch 2881, Loss: 0.017661004327237606, Final Batch Loss: 0.009191866032779217\n",
      "Epoch 2882, Loss: 0.006205911748111248, Final Batch Loss: 0.002970657544210553\n",
      "Epoch 2883, Loss: 0.025358696933835745, Final Batch Loss: 0.017767056822776794\n",
      "Epoch 2884, Loss: 0.010620599379763007, Final Batch Loss: 0.0019954617600888014\n",
      "Epoch 2885, Loss: 0.017418887931853533, Final Batch Loss: 0.0026656384579837322\n",
      "Epoch 2886, Loss: 0.0353690218180418, Final Batch Loss: 0.028644884005188942\n",
      "Epoch 2887, Loss: 0.010096461977809668, Final Batch Loss: 0.005904998630285263\n",
      "Epoch 2888, Loss: 0.011723492527380586, Final Batch Loss: 0.002785980934277177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2889, Loss: 0.00477311946451664, Final Batch Loss: 0.002304693916812539\n",
      "Epoch 2890, Loss: 0.0038875084137544036, Final Batch Loss: 0.0016123639652505517\n",
      "Epoch 2891, Loss: 0.004427960608154535, Final Batch Loss: 0.002168429084122181\n",
      "Epoch 2892, Loss: 0.004548435448668897, Final Batch Loss: 0.0027218214236199856\n",
      "Epoch 2893, Loss: 0.03718370641581714, Final Batch Loss: 0.0016054518055170774\n",
      "Epoch 2894, Loss: 0.04992902581579983, Final Batch Loss: 0.04680202156305313\n",
      "Epoch 2895, Loss: 0.006909780204296112, Final Batch Loss: 0.002467917278409004\n",
      "Epoch 2896, Loss: 0.005613895715214312, Final Batch Loss: 0.004202426411211491\n",
      "Epoch 2897, Loss: 0.004725913051515818, Final Batch Loss: 0.0031610699370503426\n",
      "Epoch 2898, Loss: 0.011080038268119097, Final Batch Loss: 0.002453214954584837\n",
      "Epoch 2899, Loss: 0.004227035096846521, Final Batch Loss: 0.0015964653575792909\n",
      "Epoch 2900, Loss: 0.0071483575738966465, Final Batch Loss: 0.004739836789667606\n",
      "Epoch 2901, Loss: 0.0048297501634806395, Final Batch Loss: 0.0026328908279538155\n",
      "Epoch 2902, Loss: 0.01587883196771145, Final Batch Loss: 0.00156466756016016\n",
      "Epoch 2903, Loss: 0.006532104918733239, Final Batch Loss: 0.003047115169465542\n",
      "Epoch 2904, Loss: 0.002127493964508176, Final Batch Loss: 0.0006313500925898552\n",
      "Epoch 2905, Loss: 0.018881422467529774, Final Batch Loss: 0.01218473631888628\n",
      "Epoch 2906, Loss: 0.002244476054329425, Final Batch Loss: 0.0012921509332954884\n",
      "Epoch 2907, Loss: 0.003094636951573193, Final Batch Loss: 0.0011080446420237422\n",
      "Epoch 2908, Loss: 0.005034701665863395, Final Batch Loss: 0.002119021490216255\n",
      "Epoch 2909, Loss: 0.007295769639313221, Final Batch Loss: 0.003643557196483016\n",
      "Epoch 2910, Loss: 0.0030326625565066934, Final Batch Loss: 0.0006373078795149922\n",
      "Epoch 2911, Loss: 0.010150029323995113, Final Batch Loss: 0.0031612091697752476\n",
      "Epoch 2912, Loss: 0.005581177887506783, Final Batch Loss: 0.0045718904584646225\n",
      "Epoch 2913, Loss: 0.015027195680886507, Final Batch Loss: 0.008267291821539402\n",
      "Epoch 2914, Loss: 0.006951349321752787, Final Batch Loss: 0.0022919103503227234\n",
      "Epoch 2915, Loss: 0.024051978951320052, Final Batch Loss: 0.021422088146209717\n",
      "Epoch 2916, Loss: 0.0044935590121895075, Final Batch Loss: 0.003330277046188712\n",
      "Epoch 2917, Loss: 0.019501269096508622, Final Batch Loss: 0.015992455184459686\n",
      "Epoch 2918, Loss: 0.0048347651027143, Final Batch Loss: 0.002132108435034752\n",
      "Epoch 2919, Loss: 0.007346763974055648, Final Batch Loss: 0.004186809062957764\n",
      "Epoch 2920, Loss: 0.011665646918118, Final Batch Loss: 0.007251855917274952\n",
      "Epoch 2921, Loss: 0.0053676392417401075, Final Batch Loss: 0.003351485123857856\n",
      "Epoch 2922, Loss: 0.005028060753829777, Final Batch Loss: 0.003286690916866064\n",
      "Epoch 2923, Loss: 0.018106934614479542, Final Batch Loss: 0.002909916453063488\n",
      "Epoch 2924, Loss: 0.0030331903835758567, Final Batch Loss: 0.001460046274587512\n",
      "Epoch 2925, Loss: 0.02632171055302024, Final Batch Loss: 0.004642184358090162\n",
      "Epoch 2926, Loss: 0.022249890957027674, Final Batch Loss: 0.004862579051405191\n",
      "Epoch 2927, Loss: 0.02235768223181367, Final Batch Loss: 0.015710968524217606\n",
      "Epoch 2928, Loss: 0.0033657426247373223, Final Batch Loss: 0.0005126645555719733\n",
      "Epoch 2929, Loss: 0.013555423356592655, Final Batch Loss: 0.008582210168242455\n",
      "Epoch 2930, Loss: 0.004519487847574055, Final Batch Loss: 0.003026143880560994\n",
      "Epoch 2931, Loss: 0.0038520441157743335, Final Batch Loss: 0.0016212834743782878\n",
      "Epoch 2932, Loss: 0.007284760242328048, Final Batch Loss: 0.001861450495198369\n",
      "Epoch 2933, Loss: 0.009296855423599482, Final Batch Loss: 0.004217197187244892\n",
      "Epoch 2934, Loss: 0.02680078404955566, Final Batch Loss: 0.001638780115172267\n",
      "Epoch 2935, Loss: 0.012765897903591394, Final Batch Loss: 0.0018977993167936802\n",
      "Epoch 2936, Loss: 0.0065520553616806865, Final Batch Loss: 0.0016362726455554366\n",
      "Epoch 2937, Loss: 0.010616065934300423, Final Batch Loss: 0.0035105817951261997\n",
      "Epoch 2938, Loss: 0.007041213801130652, Final Batch Loss: 0.0026800844352692366\n",
      "Epoch 2939, Loss: 0.005463071633130312, Final Batch Loss: 0.0033271810971200466\n",
      "Epoch 2940, Loss: 0.028327728621661663, Final Batch Loss: 0.005766109563410282\n",
      "Epoch 2941, Loss: 0.005401199567131698, Final Batch Loss: 0.0016383783658966422\n",
      "Epoch 2942, Loss: 0.036843626759946346, Final Batch Loss: 0.0012582642957568169\n",
      "Epoch 2943, Loss: 0.016491256654262543, Final Batch Loss: 0.0011147195473313332\n",
      "Epoch 2944, Loss: 0.008972140261903405, Final Batch Loss: 0.003652392653748393\n",
      "Epoch 2945, Loss: 0.023743657395243645, Final Batch Loss: 0.003820141777396202\n",
      "Epoch 2946, Loss: 0.05952753033488989, Final Batch Loss: 0.004263262264430523\n",
      "Epoch 2947, Loss: 0.010687294416129589, Final Batch Loss: 0.002000357024371624\n",
      "Epoch 2948, Loss: 0.08071432635188103, Final Batch Loss: 0.06673339009284973\n",
      "Epoch 2949, Loss: 0.026914835441857576, Final Batch Loss: 0.0046525136567652225\n",
      "Epoch 2950, Loss: 0.03556740679778159, Final Batch Loss: 0.002000540727749467\n",
      "Epoch 2951, Loss: 0.0365034444257617, Final Batch Loss: 0.0069413213059306145\n",
      "Epoch 2952, Loss: 0.02567980601452291, Final Batch Loss: 0.023651940748095512\n",
      "Epoch 2953, Loss: 0.03455033712089062, Final Batch Loss: 0.004201605916023254\n",
      "Epoch 2954, Loss: 0.017841660417616367, Final Batch Loss: 0.007421407848596573\n",
      "Epoch 2955, Loss: 0.022794620133936405, Final Batch Loss: 0.011167176999151707\n",
      "Epoch 2956, Loss: 0.01857041334733367, Final Batch Loss: 0.0022978479973971844\n",
      "Epoch 2957, Loss: 0.03535957634449005, Final Batch Loss: 0.014038922265172005\n",
      "Epoch 2958, Loss: 0.04578941687941551, Final Batch Loss: 0.009304791688919067\n",
      "Epoch 2959, Loss: 0.0095473846886307, Final Batch Loss: 0.006256916560232639\n",
      "Epoch 2960, Loss: 0.05283473921008408, Final Batch Loss: 0.05011982098221779\n",
      "Epoch 2961, Loss: 0.02538939961232245, Final Batch Loss: 0.022390983998775482\n",
      "Epoch 2962, Loss: 0.012473480543121696, Final Batch Loss: 0.010427773930132389\n",
      "Epoch 2963, Loss: 0.021516155917197466, Final Batch Loss: 0.014951257966458797\n",
      "Epoch 2964, Loss: 0.015816992381587625, Final Batch Loss: 0.012391718104481697\n",
      "Epoch 2965, Loss: 0.03906312771141529, Final Batch Loss: 0.03550456836819649\n",
      "Epoch 2966, Loss: 0.006939723854884505, Final Batch Loss: 0.004074934870004654\n",
      "Epoch 2967, Loss: 0.018391110934317112, Final Batch Loss: 0.0037881527096033096\n",
      "Epoch 2968, Loss: 0.02437753789126873, Final Batch Loss: 0.01073471549898386\n",
      "Epoch 2969, Loss: 0.015327501576393843, Final Batch Loss: 0.007119597401469946\n",
      "Epoch 2970, Loss: 0.007062401855364442, Final Batch Loss: 0.0038937197532504797\n",
      "Epoch 2971, Loss: 0.03347254730761051, Final Batch Loss: 0.02009044587612152\n",
      "Epoch 2972, Loss: 0.011801990680396557, Final Batch Loss: 0.0006765341386198997\n",
      "Epoch 2973, Loss: 0.022241658996790648, Final Batch Loss: 0.01915225386619568\n",
      "Epoch 2974, Loss: 0.025885735172778368, Final Batch Loss: 0.020571323111653328\n",
      "Epoch 2975, Loss: 0.016029023099690676, Final Batch Loss: 0.004030799027532339\n",
      "Epoch 2976, Loss: 0.03246327582746744, Final Batch Loss: 0.007882245816290379\n",
      "Epoch 2977, Loss: 0.01804564124904573, Final Batch Loss: 0.0006312092300504446\n",
      "Epoch 2978, Loss: 0.01374895207118243, Final Batch Loss: 0.0016310595674440265\n",
      "Epoch 2979, Loss: 0.015303893247619271, Final Batch Loss: 0.0011958798859268427\n",
      "Epoch 2980, Loss: 0.007769941817969084, Final Batch Loss: 0.0008062492124736309\n",
      "Epoch 2981, Loss: 0.007476065307855606, Final Batch Loss: 0.003155541606247425\n",
      "Epoch 2982, Loss: 0.006981338607147336, Final Batch Loss: 0.0049950797110795975\n",
      "Epoch 2983, Loss: 0.03750992473214865, Final Batch Loss: 0.0274012740701437\n",
      "Epoch 2984, Loss: 0.006818653317168355, Final Batch Loss: 0.005098069552332163\n",
      "Epoch 2985, Loss: 0.006947207730263472, Final Batch Loss: 0.0026416373439133167\n",
      "Epoch 2986, Loss: 0.0099826380610466, Final Batch Loss: 0.003029461018741131\n",
      "Epoch 2987, Loss: 0.006974173476919532, Final Batch Loss: 0.0021490042563527822\n",
      "Epoch 2988, Loss: 0.011800720356404781, Final Batch Loss: 0.00997740775346756\n",
      "Epoch 2989, Loss: 0.008371105301193893, Final Batch Loss: 0.0019185090204700828\n",
      "Epoch 2990, Loss: 0.062139729503542185, Final Batch Loss: 0.05874721705913544\n",
      "Epoch 2991, Loss: 0.007704148185439408, Final Batch Loss: 0.0068566096015274525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2992, Loss: 0.011915616691112518, Final Batch Loss: 0.008002148941159248\n",
      "Epoch 2993, Loss: 0.019924418069422245, Final Batch Loss: 0.009226844646036625\n",
      "Epoch 2994, Loss: 0.004274054430425167, Final Batch Loss: 0.0031901123002171516\n",
      "Epoch 2995, Loss: 0.04753789957612753, Final Batch Loss: 0.014911196194589138\n",
      "Epoch 2996, Loss: 0.029210112988948822, Final Batch Loss: 0.004583779722452164\n",
      "Epoch 2997, Loss: 0.011173154693096876, Final Batch Loss: 0.0044356221333146095\n",
      "Epoch 2998, Loss: 0.0027880873531103134, Final Batch Loss: 0.0011553165968507528\n",
      "Epoch 2999, Loss: 0.0036671141861006618, Final Batch Loss: 0.001598252565599978\n",
      "Epoch 3000, Loss: 0.035864146426320076, Final Batch Loss: 0.022477244958281517\n",
      "Epoch 3001, Loss: 0.010314732149709016, Final Batch Loss: 0.000711485103238374\n",
      "Epoch 3002, Loss: 0.003256200347095728, Final Batch Loss: 0.0018798653036355972\n",
      "Epoch 3003, Loss: 0.03420396987348795, Final Batch Loss: 0.0031643761321902275\n",
      "Epoch 3004, Loss: 0.037084844429045916, Final Batch Loss: 0.03270069137215614\n",
      "Epoch 3005, Loss: 0.014216488227248192, Final Batch Loss: 0.004262447357177734\n",
      "Epoch 3006, Loss: 0.003734360565431416, Final Batch Loss: 0.0017849848372861743\n",
      "Epoch 3007, Loss: 0.0024282209342345595, Final Batch Loss: 0.0003616622416302562\n",
      "Epoch 3008, Loss: 0.01883759442716837, Final Batch Loss: 0.014244027435779572\n",
      "Epoch 3009, Loss: 0.022347424644976854, Final Batch Loss: 0.004387818742543459\n",
      "Epoch 3010, Loss: 0.04756878409534693, Final Batch Loss: 0.03290583938360214\n",
      "Epoch 3011, Loss: 0.03162823058664799, Final Batch Loss: 0.014523789286613464\n",
      "Epoch 3012, Loss: 0.009194945450872183, Final Batch Loss: 0.003025018610060215\n",
      "Epoch 3013, Loss: 0.002823674469254911, Final Batch Loss: 0.0008261430775746703\n",
      "Epoch 3014, Loss: 0.011201634304597974, Final Batch Loss: 0.0020936394575983286\n",
      "Epoch 3015, Loss: 0.10858513787388802, Final Batch Loss: 0.01606367900967598\n",
      "Epoch 3016, Loss: 0.012192829512059689, Final Batch Loss: 0.0032754074782133102\n",
      "Epoch 3017, Loss: 0.003172204946167767, Final Batch Loss: 0.0010461279889568686\n",
      "Epoch 3018, Loss: 0.0458095854264684, Final Batch Loss: 0.04493720456957817\n",
      "Epoch 3019, Loss: 0.009852363727986813, Final Batch Loss: 0.0021410901099443436\n",
      "Epoch 3020, Loss: 0.04149780375882983, Final Batch Loss: 0.03731100633740425\n",
      "Epoch 3021, Loss: 0.03599171037785709, Final Batch Loss: 0.003278978867456317\n",
      "Epoch 3022, Loss: 0.011438868008553982, Final Batch Loss: 0.002944955602288246\n",
      "Epoch 3023, Loss: 0.0068977526389062405, Final Batch Loss: 0.0014163372106850147\n",
      "Epoch 3024, Loss: 0.00554245023522526, Final Batch Loss: 0.001439938903786242\n",
      "Epoch 3025, Loss: 0.04712272295728326, Final Batch Loss: 0.0397924929857254\n",
      "Epoch 3026, Loss: 0.009322220925241709, Final Batch Loss: 0.002222614362835884\n",
      "Epoch 3027, Loss: 0.009076273418031633, Final Batch Loss: 0.007798241451382637\n",
      "Epoch 3028, Loss: 0.006299639819189906, Final Batch Loss: 0.001485292101278901\n",
      "Epoch 3029, Loss: 0.005688773817382753, Final Batch Loss: 0.0044418517500162125\n",
      "Epoch 3030, Loss: 0.03618864715099335, Final Batch Loss: 0.021696267649531364\n",
      "Epoch 3031, Loss: 0.0050609257305040956, Final Batch Loss: 0.0017759049078449607\n",
      "Epoch 3032, Loss: 0.028668544255197048, Final Batch Loss: 0.02519983798265457\n",
      "Epoch 3033, Loss: 0.008130074478685856, Final Batch Loss: 0.006164251361042261\n",
      "Epoch 3034, Loss: 0.0196918873116374, Final Batch Loss: 0.01528080552816391\n",
      "Epoch 3035, Loss: 0.015508655225858092, Final Batch Loss: 0.013002530671656132\n",
      "Epoch 3036, Loss: 0.006053573917597532, Final Batch Loss: 0.0024353766348212957\n",
      "Epoch 3037, Loss: 0.02360065747052431, Final Batch Loss: 0.010530847124755383\n",
      "Epoch 3038, Loss: 0.006222392607014626, Final Batch Loss: 0.0006962341140024364\n",
      "Epoch 3039, Loss: 0.00801597343524918, Final Batch Loss: 0.0007522771484218538\n",
      "Epoch 3040, Loss: 0.00399276043754071, Final Batch Loss: 0.0027360040694475174\n",
      "Epoch 3041, Loss: 0.023065301589667797, Final Batch Loss: 0.00598034355789423\n",
      "Epoch 3042, Loss: 0.039369205478578806, Final Batch Loss: 0.004824607167392969\n",
      "Epoch 3043, Loss: 0.024503350257873535, Final Batch Loss: 0.007818134501576424\n",
      "Epoch 3044, Loss: 0.029926378512755036, Final Batch Loss: 0.002649065339937806\n",
      "Epoch 3045, Loss: 0.007469789125025272, Final Batch Loss: 0.003329948056489229\n",
      "Epoch 3046, Loss: 0.006807139376178384, Final Batch Loss: 0.003260199213400483\n",
      "Epoch 3047, Loss: 0.028093202970921993, Final Batch Loss: 0.01376072596758604\n",
      "Epoch 3048, Loss: 0.024928098311647773, Final Batch Loss: 0.0038526912685483694\n",
      "Epoch 3049, Loss: 0.0045909316977486014, Final Batch Loss: 0.002984629711136222\n",
      "Epoch 3050, Loss: 0.0068233892088755965, Final Batch Loss: 0.0007161222165450454\n",
      "Epoch 3051, Loss: 0.054156158585101366, Final Batch Loss: 0.006986361462622881\n",
      "Epoch 3052, Loss: 0.019449047511443496, Final Batch Loss: 0.016316331923007965\n",
      "Epoch 3053, Loss: 0.014101002598181367, Final Batch Loss: 0.003325115190818906\n",
      "Epoch 3054, Loss: 0.0310498233884573, Final Batch Loss: 0.017159948125481606\n",
      "Epoch 3055, Loss: 0.012773836962878704, Final Batch Loss: 0.003253532573580742\n",
      "Epoch 3056, Loss: 0.04127279692329466, Final Batch Loss: 0.0037813822273164988\n",
      "Epoch 3057, Loss: 0.032688227482140064, Final Batch Loss: 0.026735935360193253\n",
      "Epoch 3058, Loss: 0.04677476920187473, Final Batch Loss: 0.00973249040544033\n",
      "Epoch 3059, Loss: 0.0034409890649840236, Final Batch Loss: 0.0024993657134473324\n",
      "Epoch 3060, Loss: 0.007792974705807865, Final Batch Loss: 0.0018643111689016223\n",
      "Epoch 3061, Loss: 0.015851257368922234, Final Batch Loss: 0.007827037014067173\n",
      "Epoch 3062, Loss: 0.015661640791222453, Final Batch Loss: 0.002396800322458148\n",
      "Epoch 3063, Loss: 0.019101195968687534, Final Batch Loss: 0.008937222883105278\n",
      "Epoch 3064, Loss: 0.023239105008542538, Final Batch Loss: 0.02132921665906906\n",
      "Epoch 3065, Loss: 0.03145733056589961, Final Batch Loss: 0.029166746884584427\n",
      "Epoch 3066, Loss: 0.03763270704075694, Final Batch Loss: 0.03354080766439438\n",
      "Epoch 3067, Loss: 0.05684325285255909, Final Batch Loss: 0.03060617484152317\n",
      "Epoch 3068, Loss: 0.0060556207317858934, Final Batch Loss: 0.0022205186542123556\n",
      "Epoch 3069, Loss: 0.014489064458757639, Final Batch Loss: 0.010663785971701145\n",
      "Epoch 3070, Loss: 0.036006094655022025, Final Batch Loss: 0.0024891102220863104\n",
      "Epoch 3071, Loss: 0.16613570228219032, Final Batch Loss: 0.11001893877983093\n",
      "Epoch 3072, Loss: 0.03144450951367617, Final Batch Loss: 0.008602160029113293\n",
      "Epoch 3073, Loss: 0.019001539796590805, Final Batch Loss: 0.0032191891223192215\n",
      "Epoch 3074, Loss: 0.02804288687184453, Final Batch Loss: 0.0029845968820154667\n",
      "Epoch 3075, Loss: 0.004770656232722104, Final Batch Loss: 0.003364126430824399\n",
      "Epoch 3076, Loss: 0.015073070768266916, Final Batch Loss: 0.010876772925257683\n",
      "Epoch 3077, Loss: 0.04823942016810179, Final Batch Loss: 0.014364871196448803\n",
      "Epoch 3078, Loss: 0.011113050859421492, Final Batch Loss: 0.006075900513678789\n",
      "Epoch 3079, Loss: 0.01930796029046178, Final Batch Loss: 0.002052385825663805\n",
      "Epoch 3080, Loss: 0.024125783646013588, Final Batch Loss: 0.023272598162293434\n",
      "Epoch 3081, Loss: 0.015042797662317753, Final Batch Loss: 0.01216092612594366\n",
      "Epoch 3082, Loss: 0.048121954780071974, Final Batch Loss: 0.043894145637750626\n",
      "Epoch 3083, Loss: 0.03788947616703808, Final Batch Loss: 0.002425204264000058\n",
      "Epoch 3084, Loss: 0.043286802247166634, Final Batch Loss: 0.00844063051044941\n",
      "Epoch 3085, Loss: 0.0437626950442791, Final Batch Loss: 0.024371221661567688\n",
      "Epoch 3086, Loss: 0.04940853174775839, Final Batch Loss: 0.044281721115112305\n",
      "Epoch 3087, Loss: 0.018130906857550144, Final Batch Loss: 0.01385132223367691\n",
      "Epoch 3088, Loss: 0.02182791382074356, Final Batch Loss: 0.009222820401191711\n",
      "Epoch 3089, Loss: 0.02613552287220955, Final Batch Loss: 0.007910484448075294\n",
      "Epoch 3090, Loss: 0.014250267820898443, Final Batch Loss: 0.013424452394247055\n",
      "Epoch 3091, Loss: 0.037613688968122005, Final Batch Loss: 0.02850719541311264\n",
      "Epoch 3092, Loss: 0.022636967711150646, Final Batch Loss: 0.008759744465351105\n",
      "Epoch 3093, Loss: 0.03522865287959576, Final Batch Loss: 0.02382420375943184\n",
      "Epoch 3094, Loss: 0.07599331438541412, Final Batch Loss: 0.024320274591445923\n",
      "Epoch 3095, Loss: 0.04096366837620735, Final Batch Loss: 0.02255033329129219\n",
      "Epoch 3096, Loss: 0.035540249198675156, Final Batch Loss: 0.011660043150186539\n",
      "Epoch 3097, Loss: 0.0329255519900471, Final Batch Loss: 0.003597658360376954\n",
      "Epoch 3098, Loss: 0.03994040936231613, Final Batch Loss: 0.023906465619802475\n",
      "Epoch 3099, Loss: 0.03236174397170544, Final Batch Loss: 0.01566920429468155\n",
      "Epoch 3100, Loss: 0.006722174119204283, Final Batch Loss: 0.0019599986262619495\n",
      "Epoch 3101, Loss: 0.006165424361824989, Final Batch Loss: 0.004641049075871706\n",
      "Epoch 3102, Loss: 0.043633613269776106, Final Batch Loss: 0.037701770663261414\n",
      "Epoch 3103, Loss: 0.024669227190315723, Final Batch Loss: 0.018993059173226357\n",
      "Epoch 3104, Loss: 0.004270399804227054, Final Batch Loss: 0.0015425175661221147\n",
      "Epoch 3105, Loss: 0.006816397304646671, Final Batch Loss: 0.0009330244502052665\n",
      "Epoch 3106, Loss: 0.024467117618769407, Final Batch Loss: 0.0049919686280190945\n",
      "Epoch 3107, Loss: 0.027894566184841096, Final Batch Loss: 0.0016267154132947326\n",
      "Epoch 3108, Loss: 0.013954960042610765, Final Batch Loss: 0.010881366208195686\n",
      "Epoch 3109, Loss: 0.005578180658631027, Final Batch Loss: 0.0006237787893041968\n",
      "Epoch 3110, Loss: 0.04759642295539379, Final Batch Loss: 0.02791529707610607\n",
      "Epoch 3111, Loss: 0.0632672025822103, Final Batch Loss: 0.0011586188338696957\n",
      "Epoch 3112, Loss: 0.017258801264688373, Final Batch Loss: 0.003108351258561015\n",
      "Epoch 3113, Loss: 0.010723981307819486, Final Batch Loss: 0.0032182440627366304\n",
      "Epoch 3114, Loss: 0.0412411205470562, Final Batch Loss: 0.027812829241156578\n",
      "Epoch 3115, Loss: 0.0437825140543282, Final Batch Loss: 0.0047729951329529285\n",
      "Epoch 3116, Loss: 0.04033314157277346, Final Batch Loss: 0.03351113200187683\n",
      "Epoch 3117, Loss: 0.009668581886216998, Final Batch Loss: 0.006479022093117237\n",
      "Epoch 3118, Loss: 0.0727880485355854, Final Batch Loss: 0.03689401224255562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3119, Loss: 0.03445859835483134, Final Batch Loss: 0.00259729684330523\n",
      "Epoch 3120, Loss: 0.003738840576261282, Final Batch Loss: 0.001792479190044105\n",
      "Epoch 3121, Loss: 0.06309810653328896, Final Batch Loss: 0.028841983526945114\n",
      "Epoch 3122, Loss: 0.021216516848653555, Final Batch Loss: 0.002334370743483305\n",
      "Epoch 3123, Loss: 0.012998577789403498, Final Batch Loss: 0.001621092320419848\n",
      "Epoch 3124, Loss: 0.004806481767445803, Final Batch Loss: 0.0024980376474559307\n",
      "Epoch 3125, Loss: 0.03418780351057649, Final Batch Loss: 0.006419482175260782\n",
      "Epoch 3126, Loss: 0.01853281445801258, Final Batch Loss: 0.010997192934155464\n",
      "Epoch 3127, Loss: 0.01400976162403822, Final Batch Loss: 0.00738972844555974\n",
      "Epoch 3128, Loss: 0.038438151590526104, Final Batch Loss: 0.011431065388023853\n",
      "Epoch 3129, Loss: 0.058924452401697636, Final Batch Loss: 0.046748775988817215\n",
      "Epoch 3130, Loss: 0.06067012622952461, Final Batch Loss: 0.04695535823702812\n",
      "Epoch 3131, Loss: 0.03045551257673651, Final Batch Loss: 0.02850967086851597\n",
      "Epoch 3132, Loss: 0.04860996454954147, Final Batch Loss: 0.036691244691610336\n",
      "Epoch 3133, Loss: 0.076194042339921, Final Batch Loss: 0.0472395084798336\n",
      "Epoch 3134, Loss: 0.040281510911881924, Final Batch Loss: 0.0387427881360054\n",
      "Epoch 3135, Loss: 0.07272971421480179, Final Batch Loss: 0.0518629252910614\n",
      "Epoch 3136, Loss: 0.01921035721898079, Final Batch Loss: 0.011223225854337215\n",
      "Epoch 3137, Loss: 0.004200969124212861, Final Batch Loss: 0.001934743020683527\n",
      "Epoch 3138, Loss: 0.0031512874411419034, Final Batch Loss: 0.0018300871597602963\n",
      "Epoch 3139, Loss: 0.04706895910203457, Final Batch Loss: 0.004494285210967064\n",
      "Epoch 3140, Loss: 0.02176928147673607, Final Batch Loss: 0.01245401706546545\n",
      "Epoch 3141, Loss: 0.050271693617105484, Final Batch Loss: 0.03338242694735527\n",
      "Epoch 3142, Loss: 0.014753444585949183, Final Batch Loss: 0.008046506904065609\n",
      "Epoch 3143, Loss: 0.011917160358279943, Final Batch Loss: 0.005086462013423443\n",
      "Epoch 3144, Loss: 0.03697721194475889, Final Batch Loss: 0.027361905202269554\n",
      "Epoch 3145, Loss: 0.017405983060598373, Final Batch Loss: 0.01201252918690443\n",
      "Epoch 3146, Loss: 0.010110035305842757, Final Batch Loss: 0.006958864629268646\n",
      "Epoch 3147, Loss: 0.003969381796196103, Final Batch Loss: 0.001764984568580985\n",
      "Epoch 3148, Loss: 0.00835803966037929, Final Batch Loss: 0.0057036494836211205\n",
      "Epoch 3149, Loss: 0.013487626099959016, Final Batch Loss: 0.012317527085542679\n",
      "Epoch 3150, Loss: 0.00863169482909143, Final Batch Loss: 0.0030550321098417044\n",
      "Epoch 3151, Loss: 0.021573799196630716, Final Batch Loss: 0.01787722483277321\n",
      "Epoch 3152, Loss: 0.01734990905970335, Final Batch Loss: 0.010691080242395401\n",
      "Epoch 3153, Loss: 0.004008562304079533, Final Batch Loss: 0.002099825767800212\n",
      "Epoch 3154, Loss: 0.002866590628400445, Final Batch Loss: 0.0015512112295255065\n",
      "Epoch 3155, Loss: 0.012475234689190984, Final Batch Loss: 0.002139744581654668\n",
      "Epoch 3156, Loss: 0.009998754132539034, Final Batch Loss: 0.0032529677264392376\n",
      "Epoch 3157, Loss: 0.00410743965767324, Final Batch Loss: 0.002530390163883567\n",
      "Epoch 3158, Loss: 0.0020361535134725273, Final Batch Loss: 0.0007259882404468954\n",
      "Epoch 3159, Loss: 0.02601572312414646, Final Batch Loss: 0.017248645424842834\n",
      "Epoch 3160, Loss: 0.02224387740716338, Final Batch Loss: 0.0020892652682960033\n",
      "Epoch 3161, Loss: 0.012538725044578314, Final Batch Loss: 0.00494901929050684\n",
      "Epoch 3162, Loss: 0.006357266567647457, Final Batch Loss: 0.0031869206577539444\n",
      "Epoch 3163, Loss: 0.01094061741605401, Final Batch Loss: 0.008382310159504414\n",
      "Epoch 3164, Loss: 0.01776390476152301, Final Batch Loss: 0.014500297605991364\n",
      "Epoch 3165, Loss: 0.009871688671410084, Final Batch Loss: 0.004250968340784311\n",
      "Epoch 3166, Loss: 0.017558382358402014, Final Batch Loss: 0.006709680426865816\n",
      "Epoch 3167, Loss: 0.02207280439324677, Final Batch Loss: 0.0031150944996625185\n",
      "Epoch 3168, Loss: 0.01124326849821955, Final Batch Loss: 0.0009978885063901544\n",
      "Epoch 3169, Loss: 0.013026754837483168, Final Batch Loss: 0.006386419292539358\n",
      "Epoch 3170, Loss: 0.018512056907638907, Final Batch Loss: 0.016378089785575867\n",
      "Epoch 3171, Loss: 0.02751716785132885, Final Batch Loss: 0.02044966071844101\n",
      "Epoch 3172, Loss: 0.013026592729147524, Final Batch Loss: 0.012470735237002373\n",
      "Epoch 3173, Loss: 0.02047631796449423, Final Batch Loss: 0.013009743764996529\n",
      "Epoch 3174, Loss: 0.03233748767524958, Final Batch Loss: 0.007942055352032185\n",
      "Epoch 3175, Loss: 0.028919920325279236, Final Batch Loss: 0.012491375207901001\n",
      "Epoch 3176, Loss: 0.012987098656594753, Final Batch Loss: 0.0025072768330574036\n",
      "Epoch 3177, Loss: 0.020430681761354208, Final Batch Loss: 0.004576790612190962\n",
      "Epoch 3178, Loss: 0.013703724136576056, Final Batch Loss: 0.010428527370095253\n",
      "Epoch 3179, Loss: 0.01558245113119483, Final Batch Loss: 0.004378428217023611\n",
      "Epoch 3180, Loss: 0.05385672487318516, Final Batch Loss: 0.023651430383324623\n",
      "Epoch 3181, Loss: 0.026508714072406292, Final Batch Loss: 0.004167775623500347\n",
      "Epoch 3182, Loss: 0.006946022622287273, Final Batch Loss: 0.004255135077983141\n",
      "Epoch 3183, Loss: 0.008284864947199821, Final Batch Loss: 0.006148177664726973\n",
      "Epoch 3184, Loss: 0.02877989294938743, Final Batch Loss: 0.0033391721080988646\n",
      "Epoch 3185, Loss: 0.005674925399944186, Final Batch Loss: 0.0022843319457024336\n",
      "Epoch 3186, Loss: 0.04749849624931812, Final Batch Loss: 0.010356245562434196\n",
      "Epoch 3187, Loss: 0.008553127525374293, Final Batch Loss: 0.006009836681187153\n",
      "Epoch 3188, Loss: 0.010636389022693038, Final Batch Loss: 0.0024711822625249624\n",
      "Epoch 3189, Loss: 0.004969367990270257, Final Batch Loss: 0.002189742401242256\n",
      "Epoch 3190, Loss: 0.007305719191208482, Final Batch Loss: 0.005172654986381531\n",
      "Epoch 3191, Loss: 0.074973464012146, Final Batch Loss: 0.05882306769490242\n",
      "Epoch 3192, Loss: 0.018664869014173746, Final Batch Loss: 0.014561382122337818\n",
      "Epoch 3193, Loss: 0.00263978325529024, Final Batch Loss: 0.0017863087123259902\n",
      "Epoch 3194, Loss: 0.012121465988457203, Final Batch Loss: 0.0054421620443463326\n",
      "Epoch 3195, Loss: 0.0107875713147223, Final Batch Loss: 0.005655474495142698\n",
      "Epoch 3196, Loss: 0.035542833618819714, Final Batch Loss: 0.007849385030567646\n",
      "Epoch 3197, Loss: 0.007675245637074113, Final Batch Loss: 0.004809879232198\n",
      "Epoch 3198, Loss: 0.01755702029913664, Final Batch Loss: 0.0010462095960974693\n",
      "Epoch 3199, Loss: 0.007381055038422346, Final Batch Loss: 0.0026518255472183228\n",
      "Epoch 3200, Loss: 0.01594468136318028, Final Batch Loss: 0.0019577641505748034\n",
      "Epoch 3201, Loss: 0.03602537326514721, Final Batch Loss: 0.01925484463572502\n",
      "Epoch 3202, Loss: 0.02612324571236968, Final Batch Loss: 0.023074282333254814\n",
      "Epoch 3203, Loss: 0.06992096395697445, Final Batch Loss: 0.001499555422924459\n",
      "Epoch 3204, Loss: 0.008102853316813707, Final Batch Loss: 0.005330802407115698\n",
      "Epoch 3205, Loss: 0.009725028183311224, Final Batch Loss: 0.005180500913411379\n",
      "Epoch 3206, Loss: 0.028351700399070978, Final Batch Loss: 0.02420334704220295\n",
      "Epoch 3207, Loss: 0.01969674602150917, Final Batch Loss: 0.01061235461384058\n",
      "Epoch 3208, Loss: 0.013709316495805979, Final Batch Loss: 0.0067709446884691715\n",
      "Epoch 3209, Loss: 0.019367908826097846, Final Batch Loss: 0.016039099544286728\n",
      "Epoch 3210, Loss: 0.004008407238870859, Final Batch Loss: 0.002043291460722685\n",
      "Epoch 3211, Loss: 0.008884506300091743, Final Batch Loss: 0.003932018298655748\n",
      "Epoch 3212, Loss: 0.007239664439111948, Final Batch Loss: 0.005060227122157812\n",
      "Epoch 3213, Loss: 0.01078493520617485, Final Batch Loss: 0.0025067273527383804\n",
      "Epoch 3214, Loss: 0.005669654347002506, Final Batch Loss: 0.0030510632786899805\n",
      "Epoch 3215, Loss: 0.0035783411003649235, Final Batch Loss: 0.00176318921148777\n",
      "Epoch 3216, Loss: 0.007666800869628787, Final Batch Loss: 0.0025212776381522417\n",
      "Epoch 3217, Loss: 0.008909695548936725, Final Batch Loss: 0.003168635768815875\n",
      "Epoch 3218, Loss: 0.024421427864581347, Final Batch Loss: 0.000615148339420557\n",
      "Epoch 3219, Loss: 0.019079559948295355, Final Batch Loss: 0.01710718870162964\n",
      "Epoch 3220, Loss: 0.0022884182981215417, Final Batch Loss: 0.0006755568902008235\n",
      "Epoch 3221, Loss: 0.0033968553761951625, Final Batch Loss: 0.0005677849403582513\n",
      "Epoch 3222, Loss: 0.017065826570615172, Final Batch Loss: 0.0011260116007179022\n",
      "Epoch 3223, Loss: 0.006005060160532594, Final Batch Loss: 0.0030240535270422697\n",
      "Epoch 3224, Loss: 0.06648318632505834, Final Batch Loss: 0.06406331062316895\n",
      "Epoch 3225, Loss: 0.005390498321503401, Final Batch Loss: 0.0032502305693924427\n",
      "Epoch 3226, Loss: 0.0036300301435403526, Final Batch Loss: 0.0004780680756084621\n",
      "Epoch 3227, Loss: 0.023007294163107872, Final Batch Loss: 0.004729744046926498\n",
      "Epoch 3228, Loss: 0.006100995698943734, Final Batch Loss: 0.0036334532778710127\n",
      "Epoch 3229, Loss: 0.013209433294832706, Final Batch Loss: 0.0046450067311525345\n",
      "Epoch 3230, Loss: 0.003656005021184683, Final Batch Loss: 0.0014648404903709888\n",
      "Epoch 3231, Loss: 0.0054995379177853465, Final Batch Loss: 0.0018049090867862105\n",
      "Epoch 3232, Loss: 0.001452316006179899, Final Batch Loss: 0.00046487030340358615\n",
      "Epoch 3233, Loss: 0.012344760354608297, Final Batch Loss: 0.006086472887545824\n",
      "Epoch 3234, Loss: 0.004226930672302842, Final Batch Loss: 0.0030048051849007607\n",
      "Epoch 3235, Loss: 0.012346574570983648, Final Batch Loss: 0.006772615015506744\n",
      "Epoch 3236, Loss: 0.005454759229905903, Final Batch Loss: 0.004749807994812727\n",
      "Epoch 3237, Loss: 0.00804409570991993, Final Batch Loss: 0.0045269024558365345\n",
      "Epoch 3238, Loss: 0.0057113814400509, Final Batch Loss: 0.004633538890630007\n",
      "Epoch 3239, Loss: 0.003231706446968019, Final Batch Loss: 0.0019644175190478563\n",
      "Epoch 3240, Loss: 0.017685807077214122, Final Batch Loss: 0.002189709572121501\n",
      "Epoch 3241, Loss: 0.009322659345343709, Final Batch Loss: 0.0023317073937505484\n",
      "Epoch 3242, Loss: 0.01911737211048603, Final Batch Loss: 0.002649528905749321\n",
      "Epoch 3243, Loss: 0.012902849353849888, Final Batch Loss: 0.007539232261478901\n",
      "Epoch 3244, Loss: 0.004340618965215981, Final Batch Loss: 0.002757436828687787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3245, Loss: 0.013011951348744333, Final Batch Loss: 0.0017435209592804313\n",
      "Epoch 3246, Loss: 0.004011743236333132, Final Batch Loss: 0.0030346133280545473\n",
      "Epoch 3247, Loss: 0.00475092139095068, Final Batch Loss: 0.0011810767464339733\n",
      "Epoch 3248, Loss: 0.016565771540626884, Final Batch Loss: 0.0010095841716974974\n",
      "Epoch 3249, Loss: 0.0034612967865541577, Final Batch Loss: 0.0015970321837812662\n",
      "Epoch 3250, Loss: 0.024100094102323055, Final Batch Loss: 0.015756960958242416\n",
      "Epoch 3251, Loss: 0.007688749930821359, Final Batch Loss: 0.0013620018726214767\n",
      "Epoch 3252, Loss: 0.004925261018797755, Final Batch Loss: 0.0010630732867866755\n",
      "Epoch 3253, Loss: 0.02191003318876028, Final Batch Loss: 0.008168329484760761\n",
      "Epoch 3254, Loss: 0.019604897184763104, Final Batch Loss: 0.0009456091211177409\n",
      "Epoch 3255, Loss: 0.002106621628627181, Final Batch Loss: 0.000897963996976614\n",
      "Epoch 3256, Loss: 0.007089607650414109, Final Batch Loss: 0.003390319412574172\n",
      "Epoch 3257, Loss: 0.014180888421833515, Final Batch Loss: 0.010020799934864044\n",
      "Epoch 3258, Loss: 0.019332899246364832, Final Batch Loss: 0.0077725606970489025\n",
      "Epoch 3259, Loss: 0.006705561303533614, Final Batch Loss: 0.0013005152577534318\n",
      "Epoch 3260, Loss: 0.008198812371119857, Final Batch Loss: 0.0054553356021642685\n",
      "Epoch 3261, Loss: 0.01556061627343297, Final Batch Loss: 0.007211834657937288\n",
      "Epoch 3262, Loss: 0.0053186004515737295, Final Batch Loss: 0.0023050967138260603\n",
      "Epoch 3263, Loss: 0.016272942535579205, Final Batch Loss: 0.00810482818633318\n",
      "Epoch 3264, Loss: 0.012836977548431605, Final Batch Loss: 0.011876389384269714\n",
      "Epoch 3265, Loss: 0.023735489463433623, Final Batch Loss: 0.002161631127819419\n",
      "Epoch 3266, Loss: 0.004253380175214261, Final Batch Loss: 0.0008063329732976854\n",
      "Epoch 3267, Loss: 0.00456790323369205, Final Batch Loss: 0.0013927218969911337\n",
      "Epoch 3268, Loss: 0.023627069778740406, Final Batch Loss: 0.01114208810031414\n",
      "Epoch 3269, Loss: 0.007800746476277709, Final Batch Loss: 0.004453529603779316\n",
      "Epoch 3270, Loss: 0.0048577552661299706, Final Batch Loss: 0.002139562275260687\n",
      "Epoch 3271, Loss: 0.0075738884042948484, Final Batch Loss: 0.005371960811316967\n",
      "Epoch 3272, Loss: 0.008460459765046835, Final Batch Loss: 0.00505358399823308\n",
      "Epoch 3273, Loss: 0.02013069111853838, Final Batch Loss: 0.012636459432542324\n",
      "Epoch 3274, Loss: 0.007828943664208055, Final Batch Loss: 0.006843545939773321\n",
      "Epoch 3275, Loss: 0.015712068183347583, Final Batch Loss: 0.014960521832108498\n",
      "Epoch 3276, Loss: 0.007411770056933165, Final Batch Loss: 0.006156293209642172\n",
      "Epoch 3277, Loss: 0.004781679366715252, Final Batch Loss: 0.0019190929597243667\n",
      "Epoch 3278, Loss: 0.004220278700813651, Final Batch Loss: 0.0013317714910954237\n",
      "Epoch 3279, Loss: 0.12090346403419971, Final Batch Loss: 0.10625290125608444\n",
      "Epoch 3280, Loss: 0.008005120093002915, Final Batch Loss: 0.0006612746510654688\n",
      "Epoch 3281, Loss: 0.013831801246851683, Final Batch Loss: 0.002213297877460718\n",
      "Epoch 3282, Loss: 0.022564437240362167, Final Batch Loss: 0.009543709456920624\n",
      "Epoch 3283, Loss: 0.022783888620324433, Final Batch Loss: 0.02198711782693863\n",
      "Epoch 3284, Loss: 0.006190337473526597, Final Batch Loss: 0.0029876427724957466\n",
      "Epoch 3285, Loss: 0.02441882179118693, Final Batch Loss: 0.021341808140277863\n",
      "Epoch 3286, Loss: 0.03118927590548992, Final Batch Loss: 0.023190567269921303\n",
      "Epoch 3287, Loss: 0.004987871507182717, Final Batch Loss: 0.0015803989954292774\n",
      "Epoch 3288, Loss: 0.01824789959937334, Final Batch Loss: 0.005730029195547104\n",
      "Epoch 3289, Loss: 0.0020810773130506277, Final Batch Loss: 0.0009299991652369499\n",
      "Epoch 3290, Loss: 0.015940090641379356, Final Batch Loss: 0.013798832893371582\n",
      "Epoch 3291, Loss: 0.003259229240939021, Final Batch Loss: 0.0012443186715245247\n",
      "Epoch 3292, Loss: 0.00485123589169234, Final Batch Loss: 0.003302212106063962\n",
      "Epoch 3293, Loss: 0.014454534190008417, Final Batch Loss: 0.0004382197803352028\n",
      "Epoch 3294, Loss: 0.010016771033406258, Final Batch Loss: 0.0027146772481501102\n",
      "Epoch 3295, Loss: 0.003470732772257179, Final Batch Loss: 0.0028902690391987562\n",
      "Epoch 3296, Loss: 0.015207999385893345, Final Batch Loss: 0.008170392364263535\n",
      "Epoch 3297, Loss: 0.029105954570695758, Final Batch Loss: 0.02553700841963291\n",
      "Epoch 3298, Loss: 0.0060765864327549934, Final Batch Loss: 0.0020309900864958763\n",
      "Epoch 3299, Loss: 0.008208196028135717, Final Batch Loss: 0.006770007777959108\n",
      "Epoch 3300, Loss: 0.009309772402048111, Final Batch Loss: 0.0034702299162745476\n",
      "Epoch 3301, Loss: 0.004915192956104875, Final Batch Loss: 0.0016014648135751486\n",
      "Epoch 3302, Loss: 0.015690880827605724, Final Batch Loss: 0.006823674775660038\n",
      "Epoch 3303, Loss: 0.0030301439110189676, Final Batch Loss: 0.0015765191055834293\n",
      "Epoch 3304, Loss: 0.005587124382145703, Final Batch Loss: 0.001519532990641892\n",
      "Epoch 3305, Loss: 0.007209287490695715, Final Batch Loss: 0.003419385524466634\n",
      "Epoch 3306, Loss: 0.005069419741630554, Final Batch Loss: 0.002144681289792061\n",
      "Epoch 3307, Loss: 0.02299979724921286, Final Batch Loss: 0.002340620616450906\n",
      "Epoch 3308, Loss: 0.01191378734074533, Final Batch Loss: 0.003177648177370429\n",
      "Epoch 3309, Loss: 0.008904136484488845, Final Batch Loss: 0.003455817000940442\n",
      "Epoch 3310, Loss: 0.003025008481927216, Final Batch Loss: 0.0012649816926568747\n",
      "Epoch 3311, Loss: 0.024766028858721256, Final Batch Loss: 0.010340049862861633\n",
      "Epoch 3312, Loss: 0.011564029613509774, Final Batch Loss: 0.002555754268541932\n",
      "Epoch 3313, Loss: 0.017871978227049112, Final Batch Loss: 0.0028537516482174397\n",
      "Epoch 3314, Loss: 0.005210789589909837, Final Batch Loss: 0.00046376066165976226\n",
      "Epoch 3315, Loss: 0.0037376146065071225, Final Batch Loss: 0.0012144799111410975\n",
      "Epoch 3316, Loss: 0.00228004262316972, Final Batch Loss: 0.0011153407394886017\n",
      "Epoch 3317, Loss: 0.007952899672091007, Final Batch Loss: 0.0033984296023845673\n",
      "Epoch 3318, Loss: 0.014924171089660376, Final Batch Loss: 0.0005355185712687671\n",
      "Epoch 3319, Loss: 0.011935770278796554, Final Batch Loss: 0.0021598830353468657\n",
      "Epoch 3320, Loss: 0.018018950358964503, Final Batch Loss: 0.017024492844939232\n",
      "Epoch 3321, Loss: 0.03625149466097355, Final Batch Loss: 0.030702944844961166\n",
      "Epoch 3322, Loss: 0.023653140757232904, Final Batch Loss: 0.0008135070092976093\n",
      "Epoch 3323, Loss: 0.01113676792010665, Final Batch Loss: 0.005496328230947256\n",
      "Epoch 3324, Loss: 0.04617005353793502, Final Batch Loss: 0.004398786928504705\n",
      "Epoch 3325, Loss: 0.010279831942170858, Final Batch Loss: 0.00823376514017582\n",
      "Epoch 3326, Loss: 0.013146868208423257, Final Batch Loss: 0.0012940580490976572\n",
      "Epoch 3327, Loss: 0.004879886284470558, Final Batch Loss: 0.0022248043678700924\n",
      "Epoch 3328, Loss: 0.003890817519277334, Final Batch Loss: 0.0017906073480844498\n",
      "Epoch 3329, Loss: 0.009928283747285604, Final Batch Loss: 0.00420341407880187\n",
      "Epoch 3330, Loss: 0.016216367832385004, Final Batch Loss: 0.014791157096624374\n",
      "Epoch 3331, Loss: 0.02167986403219402, Final Batch Loss: 0.0034757282119244337\n",
      "Epoch 3332, Loss: 0.003538469201885164, Final Batch Loss: 0.002199064241722226\n",
      "Epoch 3333, Loss: 0.011960299219936132, Final Batch Loss: 0.004763569217175245\n",
      "Epoch 3334, Loss: 0.04245062172412872, Final Batch Loss: 0.03346329554915428\n",
      "Epoch 3335, Loss: 0.03696298785507679, Final Batch Loss: 0.004940534010529518\n",
      "Epoch 3336, Loss: 0.006950621725991368, Final Batch Loss: 0.0022883827332407236\n",
      "Epoch 3337, Loss: 0.008719251258298755, Final Batch Loss: 0.0021948551293462515\n",
      "Epoch 3338, Loss: 0.00606394000351429, Final Batch Loss: 0.004184023942798376\n",
      "Epoch 3339, Loss: 0.021602675784379244, Final Batch Loss: 0.017061833292245865\n",
      "Epoch 3340, Loss: 0.03564203158020973, Final Batch Loss: 0.016964951530098915\n",
      "Epoch 3341, Loss: 0.020244146697223186, Final Batch Loss: 0.009011290036141872\n",
      "Epoch 3342, Loss: 0.03165557328611612, Final Batch Loss: 0.027667129412293434\n",
      "Epoch 3343, Loss: 0.01956252194941044, Final Batch Loss: 0.004362002015113831\n",
      "Epoch 3344, Loss: 0.018755147233605385, Final Batch Loss: 0.012457450851798058\n",
      "Epoch 3345, Loss: 0.005132029240485281, Final Batch Loss: 0.0007531209266744554\n",
      "Epoch 3346, Loss: 0.016323026502504945, Final Batch Loss: 0.003871355438604951\n",
      "Epoch 3347, Loss: 0.017868135939352214, Final Batch Loss: 0.0017559555126354098\n",
      "Epoch 3348, Loss: 0.0132229991722852, Final Batch Loss: 0.009920012205839157\n",
      "Epoch 3349, Loss: 0.007670880760997534, Final Batch Loss: 0.004889424424618483\n",
      "Epoch 3350, Loss: 0.006779538467526436, Final Batch Loss: 0.003067453159019351\n",
      "Epoch 3351, Loss: 0.05122754164040089, Final Batch Loss: 0.0355253703892231\n",
      "Epoch 3352, Loss: 0.004906095447950065, Final Batch Loss: 0.0008572315564379096\n",
      "Epoch 3353, Loss: 0.002863915346097201, Final Batch Loss: 0.0008750811102800071\n",
      "Epoch 3354, Loss: 0.006403637118637562, Final Batch Loss: 0.004208957776427269\n",
      "Epoch 3355, Loss: 0.002153892768546939, Final Batch Loss: 0.0010774339316412807\n",
      "Epoch 3356, Loss: 0.018053916050121188, Final Batch Loss: 0.01433895155787468\n",
      "Epoch 3357, Loss: 0.02364715002477169, Final Batch Loss: 0.019993452355265617\n",
      "Epoch 3358, Loss: 0.011369064450263977, Final Batch Loss: 0.006977005861699581\n",
      "Epoch 3359, Loss: 0.0195832047611475, Final Batch Loss: 0.008428377099335194\n",
      "Epoch 3360, Loss: 0.004895745194517076, Final Batch Loss: 0.0013519992353394628\n",
      "Epoch 3361, Loss: 0.0022524070227518678, Final Batch Loss: 0.0006893421523272991\n",
      "Epoch 3362, Loss: 0.015999967232346535, Final Batch Loss: 0.005247451364994049\n",
      "Epoch 3363, Loss: 0.00859513133764267, Final Batch Loss: 0.004456435330212116\n",
      "Epoch 3364, Loss: 0.015452992636710405, Final Batch Loss: 0.005190535914152861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3365, Loss: 0.004475228546652943, Final Batch Loss: 0.00046307180309668183\n",
      "Epoch 3366, Loss: 0.03765355236828327, Final Batch Loss: 0.029415570199489594\n",
      "Epoch 3367, Loss: 0.0031950497068464756, Final Batch Loss: 0.0023309525568038225\n",
      "Epoch 3368, Loss: 0.005211979383602738, Final Batch Loss: 0.0013732325751334429\n",
      "Epoch 3369, Loss: 0.004588184820022434, Final Batch Loss: 0.0008812650921754539\n",
      "Epoch 3370, Loss: 0.009730687830597162, Final Batch Loss: 0.0018431670032441616\n",
      "Epoch 3371, Loss: 0.0067594952415674925, Final Batch Loss: 0.004246596712619066\n",
      "Epoch 3372, Loss: 0.06329890061169863, Final Batch Loss: 0.05935376510024071\n",
      "Epoch 3373, Loss: 0.021258565597236156, Final Batch Loss: 0.009259896352887154\n",
      "Epoch 3374, Loss: 0.004622298409231007, Final Batch Loss: 0.0006610626587644219\n",
      "Epoch 3375, Loss: 0.018141754902899265, Final Batch Loss: 0.005161946639418602\n",
      "Epoch 3376, Loss: 0.004187949118204415, Final Batch Loss: 0.0014533818466588855\n",
      "Epoch 3377, Loss: 0.0075267094653099775, Final Batch Loss: 0.005471323151141405\n",
      "Epoch 3378, Loss: 0.013603503815829754, Final Batch Loss: 0.009404189884662628\n",
      "Epoch 3379, Loss: 0.0053218998946249485, Final Batch Loss: 0.0025980595964938402\n",
      "Epoch 3380, Loss: 0.010535666719079018, Final Batch Loss: 0.004240510519593954\n",
      "Epoch 3381, Loss: 0.006251150625757873, Final Batch Loss: 0.0010419258615002036\n",
      "Epoch 3382, Loss: 0.009237900609150529, Final Batch Loss: 0.0026333711575716734\n",
      "Epoch 3383, Loss: 0.012338807806372643, Final Batch Loss: 0.0023969244211912155\n",
      "Epoch 3384, Loss: 0.004115519695915282, Final Batch Loss: 0.00124174018856138\n",
      "Epoch 3385, Loss: 0.004219422466121614, Final Batch Loss: 0.0012009384809061885\n",
      "Epoch 3386, Loss: 0.019968827720731497, Final Batch Loss: 0.012874918058514595\n",
      "Epoch 3387, Loss: 0.004242190858349204, Final Batch Loss: 0.001809805864468217\n",
      "Epoch 3388, Loss: 0.026957460679113865, Final Batch Loss: 0.022671042010188103\n",
      "Epoch 3389, Loss: 0.012834831140935421, Final Batch Loss: 0.0019464707002043724\n",
      "Epoch 3390, Loss: 0.02508321381174028, Final Batch Loss: 0.023096635937690735\n",
      "Epoch 3391, Loss: 0.06832326948642731, Final Batch Loss: 0.013726968318223953\n",
      "Epoch 3392, Loss: 0.0033295995090156794, Final Batch Loss: 0.0025107995606958866\n",
      "Epoch 3393, Loss: 0.01322368485853076, Final Batch Loss: 0.006796799600124359\n",
      "Epoch 3394, Loss: 0.003765734494663775, Final Batch Loss: 0.001668051234446466\n",
      "Epoch 3395, Loss: 0.014741294784471393, Final Batch Loss: 0.0030369150917977095\n",
      "Epoch 3396, Loss: 0.020542766549624503, Final Batch Loss: 0.01940661482512951\n",
      "Epoch 3397, Loss: 0.0356734711676836, Final Batch Loss: 0.03418063372373581\n",
      "Epoch 3398, Loss: 0.0070134789566509426, Final Batch Loss: 0.0009075265261344612\n",
      "Epoch 3399, Loss: 0.015922016464173794, Final Batch Loss: 0.008030053228139877\n",
      "Epoch 3400, Loss: 0.001812645175959915, Final Batch Loss: 0.0010883418144658208\n",
      "Epoch 3401, Loss: 0.004710062174126506, Final Batch Loss: 0.0020949719473719597\n",
      "Epoch 3402, Loss: 0.016364472452551126, Final Batch Loss: 0.012197605334222317\n",
      "Epoch 3403, Loss: 0.02131388313136995, Final Batch Loss: 0.0030837298836559057\n",
      "Epoch 3404, Loss: 0.00482167536392808, Final Batch Loss: 0.0019967847038060427\n",
      "Epoch 3405, Loss: 0.022213785123312846, Final Batch Loss: 0.0004732405359391123\n",
      "Epoch 3406, Loss: 0.02053192676976323, Final Batch Loss: 0.0038773086853325367\n",
      "Epoch 3407, Loss: 0.07266436330974102, Final Batch Loss: 0.06981708109378815\n",
      "Epoch 3408, Loss: 0.011531997472047806, Final Batch Loss: 0.007557066157460213\n",
      "Epoch 3409, Loss: 0.011569678084924817, Final Batch Loss: 0.0016784353647381067\n",
      "Epoch 3410, Loss: 0.03710922715254128, Final Batch Loss: 0.0028532047290354967\n",
      "Epoch 3411, Loss: 0.0028076188173145056, Final Batch Loss: 0.001436754479072988\n",
      "Epoch 3412, Loss: 0.028624800965189934, Final Batch Loss: 0.008306384086608887\n",
      "Epoch 3413, Loss: 0.012592449318617582, Final Batch Loss: 0.005087338387966156\n",
      "Epoch 3414, Loss: 0.012971332180313766, Final Batch Loss: 0.0013983127428218722\n",
      "Epoch 3415, Loss: 0.05305950157344341, Final Batch Loss: 0.03529772162437439\n",
      "Epoch 3416, Loss: 0.030682916985824704, Final Batch Loss: 0.02736245095729828\n",
      "Epoch 3417, Loss: 0.0033501728903502226, Final Batch Loss: 0.0011415111366659403\n",
      "Epoch 3418, Loss: 0.01478361221961677, Final Batch Loss: 0.0017104314174503088\n",
      "Epoch 3419, Loss: 0.03047305013751611, Final Batch Loss: 0.000912847404833883\n",
      "Epoch 3420, Loss: 0.023175479844212532, Final Batch Loss: 0.01783238910138607\n",
      "Epoch 3421, Loss: 0.005512498319149017, Final Batch Loss: 0.0020823171362280846\n",
      "Epoch 3422, Loss: 0.004083960899151862, Final Batch Loss: 0.0015707450220361352\n",
      "Epoch 3423, Loss: 0.04438929120078683, Final Batch Loss: 0.039962418377399445\n",
      "Epoch 3424, Loss: 0.025736319832503796, Final Batch Loss: 0.013992561027407646\n",
      "Epoch 3425, Loss: 0.004360817722044885, Final Batch Loss: 0.0015106432838365436\n",
      "Epoch 3426, Loss: 0.008913366124033928, Final Batch Loss: 0.004125349689275026\n",
      "Epoch 3427, Loss: 0.013837092323228717, Final Batch Loss: 0.010982917621731758\n",
      "Epoch 3428, Loss: 0.039711094461381435, Final Batch Loss: 0.006174474023282528\n",
      "Epoch 3429, Loss: 0.010815556510351598, Final Batch Loss: 0.0008556846296414733\n",
      "Epoch 3430, Loss: 0.01274650334380567, Final Batch Loss: 0.009423403069376945\n",
      "Epoch 3431, Loss: 0.006314606172963977, Final Batch Loss: 0.0037863573525100946\n",
      "Epoch 3432, Loss: 0.016181739047169685, Final Batch Loss: 0.007657910697162151\n",
      "Epoch 3433, Loss: 0.00506844068877399, Final Batch Loss: 0.0021371394395828247\n",
      "Epoch 3434, Loss: 0.009383104275912046, Final Batch Loss: 0.004046556074172258\n",
      "Epoch 3435, Loss: 0.02916851732879877, Final Batch Loss: 0.021581139415502548\n",
      "Epoch 3436, Loss: 0.09887415310367942, Final Batch Loss: 0.09469996392726898\n",
      "Epoch 3437, Loss: 0.021011787466704845, Final Batch Loss: 0.012096391059458256\n",
      "Epoch 3438, Loss: 0.03537030448205769, Final Batch Loss: 0.0015194409061223269\n",
      "Epoch 3439, Loss: 0.0022296045790426433, Final Batch Loss: 0.00040178868221119046\n",
      "Epoch 3440, Loss: 0.011383301112800837, Final Batch Loss: 0.003923204727470875\n",
      "Epoch 3441, Loss: 0.00981834577396512, Final Batch Loss: 0.007741517852991819\n",
      "Epoch 3442, Loss: 0.010347567964345217, Final Batch Loss: 0.004809485748410225\n",
      "Epoch 3443, Loss: 0.004452024877537042, Final Batch Loss: 0.0006422610604204237\n",
      "Epoch 3444, Loss: 0.007500288309529424, Final Batch Loss: 0.006129327230155468\n",
      "Epoch 3445, Loss: 0.012567253783345222, Final Batch Loss: 0.008067327551543713\n",
      "Epoch 3446, Loss: 0.006302397698163986, Final Batch Loss: 0.004175661131739616\n",
      "Epoch 3447, Loss: 0.015620959689840674, Final Batch Loss: 0.012693669646978378\n",
      "Epoch 3448, Loss: 0.05611817678436637, Final Batch Loss: 0.005691410508006811\n",
      "Epoch 3449, Loss: 0.011370032327249646, Final Batch Loss: 0.0019201722461730242\n",
      "Epoch 3450, Loss: 0.022416546242311597, Final Batch Loss: 0.0020708043593913317\n",
      "Epoch 3451, Loss: 0.006903725443407893, Final Batch Loss: 0.003489502938464284\n",
      "Epoch 3452, Loss: 0.016161880223080516, Final Batch Loss: 0.002444914309307933\n",
      "Epoch 3453, Loss: 0.006815214874222875, Final Batch Loss: 0.004683192819356918\n",
      "Epoch 3454, Loss: 0.05679745972156525, Final Batch Loss: 0.037043482065200806\n",
      "Epoch 3455, Loss: 0.0067238276824355125, Final Batch Loss: 0.003683108137920499\n",
      "Epoch 3456, Loss: 0.017935895826667547, Final Batch Loss: 0.0029224841855466366\n",
      "Epoch 3457, Loss: 0.013080581091344357, Final Batch Loss: 0.007313752546906471\n",
      "Epoch 3458, Loss: 0.0013158565270714462, Final Batch Loss: 0.0003570237895473838\n",
      "Epoch 3459, Loss: 0.011521488195285201, Final Batch Loss: 0.0007344491314142942\n",
      "Epoch 3460, Loss: 0.0037689594319090247, Final Batch Loss: 0.000989544321782887\n",
      "Epoch 3461, Loss: 0.0259232220123522, Final Batch Loss: 0.02506377547979355\n",
      "Epoch 3462, Loss: 0.007733089616522193, Final Batch Loss: 0.0030662037897855043\n",
      "Epoch 3463, Loss: 0.005691627273336053, Final Batch Loss: 0.0037603680975735188\n",
      "Epoch 3464, Loss: 0.0028911750996485353, Final Batch Loss: 0.0016881268238648772\n",
      "Epoch 3465, Loss: 0.012803220190107822, Final Batch Loss: 0.007176991552114487\n",
      "Epoch 3466, Loss: 0.04432840086519718, Final Batch Loss: 0.02246515080332756\n",
      "Epoch 3467, Loss: 0.03848026879131794, Final Batch Loss: 0.01964389532804489\n",
      "Epoch 3468, Loss: 0.019538596272468567, Final Batch Loss: 0.003624696284532547\n",
      "Epoch 3469, Loss: 0.01774029736407101, Final Batch Loss: 0.014336038380861282\n",
      "Epoch 3470, Loss: 0.03624503349419683, Final Batch Loss: 0.0013680014526471496\n",
      "Epoch 3471, Loss: 0.02580100577324629, Final Batch Loss: 0.011364656500518322\n",
      "Epoch 3472, Loss: 0.0109742630738765, Final Batch Loss: 0.002666511805728078\n",
      "Epoch 3473, Loss: 0.02441102429293096, Final Batch Loss: 0.020526431500911713\n",
      "Epoch 3474, Loss: 0.015266308328136802, Final Batch Loss: 0.0012345684226602316\n",
      "Epoch 3475, Loss: 0.01871634181588888, Final Batch Loss: 0.009395355358719826\n",
      "Epoch 3476, Loss: 0.0027614980353973806, Final Batch Loss: 0.0007492694421671331\n",
      "Epoch 3477, Loss: 0.005778879392892122, Final Batch Loss: 0.003770718816667795\n",
      "Epoch 3478, Loss: 0.013217691506724805, Final Batch Loss: 0.0007761429878883064\n",
      "Epoch 3479, Loss: 0.003962297923862934, Final Batch Loss: 0.0017125897575169802\n",
      "Epoch 3480, Loss: 0.004241542890667915, Final Batch Loss: 0.0008545557502657175\n",
      "Epoch 3481, Loss: 0.006737884366884828, Final Batch Loss: 0.0024898408446460962\n",
      "Epoch 3482, Loss: 0.0027559592272154987, Final Batch Loss: 0.0008279851754195988\n",
      "Epoch 3483, Loss: 0.03570899646729231, Final Batch Loss: 0.021799856796860695\n",
      "Epoch 3484, Loss: 0.0169131716247648, Final Batch Loss: 0.00037398538552224636\n",
      "Epoch 3485, Loss: 0.009171338519081473, Final Batch Loss: 0.00191788119263947\n",
      "Epoch 3486, Loss: 0.007554975803941488, Final Batch Loss: 0.00176314078271389\n",
      "Epoch 3487, Loss: 0.012465152714867145, Final Batch Loss: 0.0007802562904544175\n",
      "Epoch 3488, Loss: 0.009382953401654959, Final Batch Loss: 0.007419913075864315\n",
      "Epoch 3489, Loss: 0.0038412725552916527, Final Batch Loss: 0.0025462275370955467\n",
      "Epoch 3490, Loss: 0.013972023501992226, Final Batch Loss: 0.007325656246393919\n",
      "Epoch 3491, Loss: 0.018681650748476386, Final Batch Loss: 0.016381824389100075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3492, Loss: 0.01510820398107171, Final Batch Loss: 0.013720626942813396\n",
      "Epoch 3493, Loss: 0.014888554811477661, Final Batch Loss: 0.007479759398847818\n",
      "Epoch 3494, Loss: 0.010319625027477741, Final Batch Loss: 0.006262298207730055\n",
      "Epoch 3495, Loss: 0.009372221538797021, Final Batch Loss: 0.0061530605889856815\n",
      "Epoch 3496, Loss: 0.0036612413823604584, Final Batch Loss: 0.0017603355227038264\n",
      "Epoch 3497, Loss: 0.006145484163425863, Final Batch Loss: 0.004466307815164328\n",
      "Epoch 3498, Loss: 0.011251927819103003, Final Batch Loss: 0.003937940113246441\n",
      "Epoch 3499, Loss: 0.003143145819194615, Final Batch Loss: 0.0015917429700493813\n",
      "Epoch 3500, Loss: 0.016880128998309374, Final Batch Loss: 0.015542454086244106\n",
      "Epoch 3501, Loss: 0.00922456756234169, Final Batch Loss: 0.0037454869598150253\n",
      "Epoch 3502, Loss: 0.021113091614097357, Final Batch Loss: 0.017427941784262657\n",
      "Epoch 3503, Loss: 0.015303830150514841, Final Batch Loss: 0.010121234692633152\n",
      "Epoch 3504, Loss: 0.048490666784346104, Final Batch Loss: 0.007702448405325413\n",
      "Epoch 3505, Loss: 0.008734727511182427, Final Batch Loss: 0.00587723683565855\n",
      "Epoch 3506, Loss: 0.002495006541721523, Final Batch Loss: 0.0006977207958698273\n",
      "Epoch 3507, Loss: 0.007451256969943643, Final Batch Loss: 0.003981946501880884\n",
      "Epoch 3508, Loss: 0.025615849532186985, Final Batch Loss: 0.010649277828633785\n",
      "Epoch 3509, Loss: 0.014162406791001558, Final Batch Loss: 0.011810235679149628\n",
      "Epoch 3510, Loss: 0.010459975164849311, Final Batch Loss: 0.0005263963830657303\n",
      "Epoch 3511, Loss: 0.003825879073701799, Final Batch Loss: 0.002336891833692789\n",
      "Epoch 3512, Loss: 0.03101510927081108, Final Batch Loss: 0.028156735002994537\n",
      "Epoch 3513, Loss: 0.03337571653537452, Final Batch Loss: 0.03169465437531471\n",
      "Epoch 3514, Loss: 0.008919657906517386, Final Batch Loss: 0.0023085058201104403\n",
      "Epoch 3515, Loss: 0.016189285088330507, Final Batch Loss: 0.01164998672902584\n",
      "Epoch 3516, Loss: 0.011913228314369917, Final Batch Loss: 0.0067499238066375256\n",
      "Epoch 3517, Loss: 0.04029165953397751, Final Batch Loss: 0.01776251196861267\n",
      "Epoch 3518, Loss: 0.01266212877817452, Final Batch Loss: 0.0038516188506036997\n",
      "Epoch 3519, Loss: 0.027933894656598568, Final Batch Loss: 0.009266664274036884\n",
      "Epoch 3520, Loss: 0.005270737281534821, Final Batch Loss: 0.0007510735304094851\n",
      "Epoch 3521, Loss: 0.014435660559684038, Final Batch Loss: 0.008817576803267002\n",
      "Epoch 3522, Loss: 0.012596348416991532, Final Batch Loss: 0.0018551322864368558\n",
      "Epoch 3523, Loss: 0.011077954317443073, Final Batch Loss: 0.0010147186694666743\n",
      "Epoch 3524, Loss: 0.003525501466356218, Final Batch Loss: 0.0015431410865858197\n",
      "Epoch 3525, Loss: 0.010001992341130972, Final Batch Loss: 0.004343721084296703\n",
      "Epoch 3526, Loss: 0.004390813526697457, Final Batch Loss: 0.002659571822732687\n",
      "Epoch 3527, Loss: 0.0037373662926256657, Final Batch Loss: 0.0032463837414979935\n",
      "Epoch 3528, Loss: 0.010737549513578415, Final Batch Loss: 0.005413937848061323\n",
      "Epoch 3529, Loss: 0.013178179040551186, Final Batch Loss: 0.0065416498109698296\n",
      "Epoch 3530, Loss: 0.01885011268313974, Final Batch Loss: 0.0016872159903869033\n",
      "Epoch 3531, Loss: 0.025952570140361786, Final Batch Loss: 0.02100682631134987\n",
      "Epoch 3532, Loss: 0.00904127606190741, Final Batch Loss: 0.0027703631203621626\n",
      "Epoch 3533, Loss: 0.015238697873428464, Final Batch Loss: 0.012774178758263588\n",
      "Epoch 3534, Loss: 0.009340540505945683, Final Batch Loss: 0.0029226653277873993\n",
      "Epoch 3535, Loss: 0.01415214384905994, Final Batch Loss: 0.012433833442628384\n",
      "Epoch 3536, Loss: 0.010600946377962828, Final Batch Loss: 0.005331981927156448\n",
      "Epoch 3537, Loss: 0.03477390552870929, Final Batch Loss: 0.0034258298110216856\n",
      "Epoch 3538, Loss: 0.010989610804244876, Final Batch Loss: 0.00801654439419508\n",
      "Epoch 3539, Loss: 0.040096865966916084, Final Batch Loss: 0.038497358560562134\n",
      "Epoch 3540, Loss: 0.03244793135672808, Final Batch Loss: 0.011843816377222538\n",
      "Epoch 3541, Loss: 0.008409768808633089, Final Batch Loss: 0.004339993931353092\n",
      "Epoch 3542, Loss: 0.00795301841571927, Final Batch Loss: 0.0052914307452738285\n",
      "Epoch 3543, Loss: 0.04310578037984669, Final Batch Loss: 0.001209217356517911\n",
      "Epoch 3544, Loss: 0.0077752950601279736, Final Batch Loss: 0.00271685142070055\n",
      "Epoch 3545, Loss: 0.0033044670708477497, Final Batch Loss: 0.0014878710499033332\n",
      "Epoch 3546, Loss: 0.042253535240888596, Final Batch Loss: 0.037069790065288544\n",
      "Epoch 3547, Loss: 0.00857895566150546, Final Batch Loss: 0.0025589531287550926\n",
      "Epoch 3548, Loss: 0.024150666198693216, Final Batch Loss: 0.001674527651630342\n",
      "Epoch 3549, Loss: 0.00830069137737155, Final Batch Loss: 0.002083773259073496\n",
      "Epoch 3550, Loss: 0.03295885305851698, Final Batch Loss: 0.012190124951303005\n",
      "Epoch 3551, Loss: 0.024173950077965856, Final Batch Loss: 0.02242571860551834\n",
      "Epoch 3552, Loss: 0.021421800687676296, Final Batch Loss: 0.0003490684030111879\n",
      "Epoch 3553, Loss: 0.006676931399852037, Final Batch Loss: 0.004435238428413868\n",
      "Epoch 3554, Loss: 0.08170419652014971, Final Batch Loss: 0.07334497570991516\n",
      "Epoch 3555, Loss: 0.029299435205757618, Final Batch Loss: 0.01054063718765974\n",
      "Epoch 3556, Loss: 0.004769619205035269, Final Batch Loss: 0.0017292419215664268\n",
      "Epoch 3557, Loss: 0.01360356342047453, Final Batch Loss: 0.003926335833966732\n",
      "Epoch 3558, Loss: 0.10393127426505089, Final Batch Loss: 0.09738343209028244\n",
      "Epoch 3559, Loss: 0.03495305310934782, Final Batch Loss: 0.01945277489721775\n",
      "Epoch 3560, Loss: 0.02544423658400774, Final Batch Loss: 0.007165183313190937\n",
      "Epoch 3561, Loss: 0.009899292374029756, Final Batch Loss: 0.0026916067581623793\n",
      "Epoch 3562, Loss: 0.008850031765177846, Final Batch Loss: 0.007493771146982908\n",
      "Epoch 3563, Loss: 0.023624362424016, Final Batch Loss: 0.02228911593556404\n",
      "Epoch 3564, Loss: 0.009001498110592365, Final Batch Loss: 0.004971203859895468\n",
      "Epoch 3565, Loss: 0.11867328127846122, Final Batch Loss: 0.11432336270809174\n",
      "Epoch 3566, Loss: 0.02901562349870801, Final Batch Loss: 0.02226158045232296\n",
      "Epoch 3567, Loss: 0.08523411210626364, Final Batch Loss: 0.07462276518344879\n",
      "Epoch 3568, Loss: 0.005678951973095536, Final Batch Loss: 0.0021379508543759584\n",
      "Epoch 3569, Loss: 0.01670347823528573, Final Batch Loss: 0.0008773479494266212\n",
      "Epoch 3570, Loss: 0.06909660750534385, Final Batch Loss: 0.0017170509090647101\n",
      "Epoch 3571, Loss: 0.020239907316863537, Final Batch Loss: 0.008934956975281239\n",
      "Epoch 3572, Loss: 0.03922045975923538, Final Batch Loss: 0.023766644299030304\n",
      "Epoch 3573, Loss: 0.053021707106381655, Final Batch Loss: 0.006217267829924822\n",
      "Epoch 3574, Loss: 0.006761275231838226, Final Batch Loss: 0.0023033954203128815\n",
      "Epoch 3575, Loss: 0.06378960679285228, Final Batch Loss: 0.061309993267059326\n",
      "Epoch 3576, Loss: 0.0045353054301813245, Final Batch Loss: 0.003384410636499524\n",
      "Epoch 3577, Loss: 0.03045577771263197, Final Batch Loss: 0.0008115734090097249\n",
      "Epoch 3578, Loss: 0.004719780990853906, Final Batch Loss: 0.0026442999951541424\n",
      "Epoch 3579, Loss: 0.007697659777477384, Final Batch Loss: 0.0047508105635643005\n",
      "Epoch 3580, Loss: 0.01913104997947812, Final Batch Loss: 0.016721947118639946\n",
      "Epoch 3581, Loss: 0.00373499293345958, Final Batch Loss: 0.0021425567101687193\n",
      "Epoch 3582, Loss: 0.031027138233184814, Final Batch Loss: 0.02263789251446724\n",
      "Epoch 3583, Loss: 0.013880250975489616, Final Batch Loss: 0.00986512191593647\n",
      "Epoch 3584, Loss: 0.005248027388006449, Final Batch Loss: 0.0021527670323848724\n",
      "Epoch 3585, Loss: 0.011906683444976807, Final Batch Loss: 0.00918970163911581\n",
      "Epoch 3586, Loss: 0.0035180450649932027, Final Batch Loss: 0.0017814096063375473\n",
      "Epoch 3587, Loss: 0.0010652683122316375, Final Batch Loss: 0.0002350329450564459\n",
      "Epoch 3588, Loss: 0.004946643253788352, Final Batch Loss: 0.003001095959916711\n",
      "Epoch 3589, Loss: 0.012210669927299023, Final Batch Loss: 0.010398618876934052\n",
      "Epoch 3590, Loss: 0.004265124327503145, Final Batch Loss: 0.0028417790308594704\n",
      "Epoch 3591, Loss: 0.011413030093535781, Final Batch Loss: 0.008592105470597744\n",
      "Epoch 3592, Loss: 0.009697558358311653, Final Batch Loss: 0.006271179765462875\n",
      "Epoch 3593, Loss: 0.012812101107556373, Final Batch Loss: 0.0006584905204363167\n",
      "Epoch 3594, Loss: 0.011934999842196703, Final Batch Loss: 0.0040670656599104404\n",
      "Epoch 3595, Loss: 0.05687423574272543, Final Batch Loss: 0.0019475013250485063\n",
      "Epoch 3596, Loss: 0.008750451845116913, Final Batch Loss: 0.008047850802540779\n",
      "Epoch 3597, Loss: 0.013510642107576132, Final Batch Loss: 0.010603242553770542\n",
      "Epoch 3598, Loss: 0.017819495871663094, Final Batch Loss: 0.008202789351344109\n",
      "Epoch 3599, Loss: 0.013569612056016922, Final Batch Loss: 0.005294526927173138\n",
      "Epoch 3600, Loss: 0.009520999155938625, Final Batch Loss: 0.004304876085370779\n",
      "Epoch 3601, Loss: 0.004422593745402992, Final Batch Loss: 0.001880231429822743\n",
      "Epoch 3602, Loss: 0.018967469106428325, Final Batch Loss: 0.017690366134047508\n",
      "Epoch 3603, Loss: 0.012263867072761059, Final Batch Loss: 0.009592105634510517\n",
      "Epoch 3604, Loss: 0.014632705831900239, Final Batch Loss: 0.0025088258553296328\n",
      "Epoch 3605, Loss: 0.005396041669882834, Final Batch Loss: 0.003790222806856036\n",
      "Epoch 3606, Loss: 0.0017872349708341062, Final Batch Loss: 0.000531722151208669\n",
      "Epoch 3607, Loss: 0.020910277031362057, Final Batch Loss: 0.0184944961220026\n",
      "Epoch 3608, Loss: 0.0051818640204146504, Final Batch Loss: 0.0010705121094360948\n",
      "Epoch 3609, Loss: 0.03126011253334582, Final Batch Loss: 0.028924500569701195\n",
      "Epoch 3610, Loss: 0.007541935192421079, Final Batch Loss: 0.0019554735627025366\n",
      "Epoch 3611, Loss: 0.010933897690847516, Final Batch Loss: 0.003414407605305314\n",
      "Epoch 3612, Loss: 0.12192181870341301, Final Batch Loss: 0.031025897711515427\n",
      "Epoch 3613, Loss: 0.008941933163441718, Final Batch Loss: 0.007198848761618137\n",
      "Epoch 3614, Loss: 0.04568589013069868, Final Batch Loss: 0.0423772893846035\n",
      "Epoch 3615, Loss: 0.01925501978257671, Final Batch Loss: 0.00041137455264106393\n",
      "Epoch 3616, Loss: 0.014064769726246595, Final Batch Loss: 0.007983849383890629\n",
      "Epoch 3617, Loss: 0.02683294378221035, Final Batch Loss: 0.008829908445477486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3618, Loss: 0.024933126522228122, Final Batch Loss: 0.0030527792405337095\n",
      "Epoch 3619, Loss: 0.050527796149253845, Final Batch Loss: 0.04048610478639603\n",
      "Epoch 3620, Loss: 0.018905302044004202, Final Batch Loss: 0.006390892434865236\n",
      "Epoch 3621, Loss: 0.0432637301273644, Final Batch Loss: 0.036490414291620255\n",
      "Epoch 3622, Loss: 0.03743105044122785, Final Batch Loss: 0.001223240396939218\n",
      "Epoch 3623, Loss: 0.016772175207734108, Final Batch Loss: 0.00474032387137413\n",
      "Epoch 3624, Loss: 0.004088186426088214, Final Batch Loss: 0.0025596481282263994\n",
      "Epoch 3625, Loss: 0.006716854404658079, Final Batch Loss: 0.0012982925400137901\n",
      "Epoch 3626, Loss: 0.024912633933126926, Final Batch Loss: 0.011079087853431702\n",
      "Epoch 3627, Loss: 0.007612891029566526, Final Batch Loss: 0.001697668805718422\n",
      "Epoch 3628, Loss: 0.002732682798523456, Final Batch Loss: 0.001800606376491487\n",
      "Epoch 3629, Loss: 0.008627486880868673, Final Batch Loss: 0.005023445468395948\n",
      "Epoch 3630, Loss: 0.001898567657917738, Final Batch Loss: 0.0004813049454241991\n",
      "Epoch 3631, Loss: 0.006158817850518972, Final Batch Loss: 0.0007862726342864335\n",
      "Epoch 3632, Loss: 0.08758843410760164, Final Batch Loss: 0.013657894916832447\n",
      "Epoch 3633, Loss: 0.02014083508402109, Final Batch Loss: 0.009456224739551544\n",
      "Epoch 3634, Loss: 0.01819085469469428, Final Batch Loss: 0.017341598868370056\n",
      "Epoch 3635, Loss: 0.0064224551897495985, Final Batch Loss: 0.0034367607440799475\n",
      "Epoch 3636, Loss: 0.009647983126342297, Final Batch Loss: 0.00467438530176878\n",
      "Epoch 3637, Loss: 0.023179566022008657, Final Batch Loss: 0.004780856426805258\n",
      "Epoch 3638, Loss: 0.007626754697412252, Final Batch Loss: 0.003730099881067872\n",
      "Epoch 3639, Loss: 0.025230873841792345, Final Batch Loss: 0.018822114914655685\n",
      "Epoch 3640, Loss: 0.010065347887575626, Final Batch Loss: 0.005306834354996681\n",
      "Epoch 3641, Loss: 0.02403384866192937, Final Batch Loss: 0.021350430324673653\n",
      "Epoch 3642, Loss: 0.011160791385918856, Final Batch Loss: 0.008577324450016022\n",
      "Epoch 3643, Loss: 0.006096617493312806, Final Batch Loss: 0.00521851284429431\n",
      "Epoch 3644, Loss: 0.033317700028419495, Final Batch Loss: 0.00574796088039875\n",
      "Epoch 3645, Loss: 0.013210331089794636, Final Batch Loss: 0.0031590592116117477\n",
      "Epoch 3646, Loss: 0.02213138248771429, Final Batch Loss: 0.015694307163357735\n",
      "Epoch 3647, Loss: 0.018227703287266195, Final Batch Loss: 0.0017507480224594474\n",
      "Epoch 3648, Loss: 0.04948256240459159, Final Batch Loss: 0.04856277257204056\n",
      "Epoch 3649, Loss: 0.027378335129469633, Final Batch Loss: 0.024473153054714203\n",
      "Epoch 3650, Loss: 0.022474266588687897, Final Batch Loss: 0.01974351517856121\n",
      "Epoch 3651, Loss: 0.009233939228579402, Final Batch Loss: 0.003880658419802785\n",
      "Epoch 3652, Loss: 0.02168350201100111, Final Batch Loss: 0.013703717850148678\n",
      "Epoch 3653, Loss: 0.005566626321524382, Final Batch Loss: 0.0032402474898844957\n",
      "Epoch 3654, Loss: 0.03221712866798043, Final Batch Loss: 0.004729062784463167\n",
      "Epoch 3655, Loss: 0.03957690577954054, Final Batch Loss: 0.03057604283094406\n",
      "Epoch 3656, Loss: 0.005275328177958727, Final Batch Loss: 0.0025245530996471643\n",
      "Epoch 3657, Loss: 0.004904873785562813, Final Batch Loss: 0.0018633679719641805\n",
      "Epoch 3658, Loss: 0.0020518890232779086, Final Batch Loss: 0.0014162302250042558\n",
      "Epoch 3659, Loss: 0.02500963769853115, Final Batch Loss: 0.00975080393254757\n",
      "Epoch 3660, Loss: 0.031641755951568484, Final Batch Loss: 0.0034192099701613188\n",
      "Epoch 3661, Loss: 0.01073516090400517, Final Batch Loss: 0.0011666857171803713\n",
      "Epoch 3662, Loss: 0.006533971522003412, Final Batch Loss: 0.0043557072058320045\n",
      "Epoch 3663, Loss: 0.021804362768307328, Final Batch Loss: 0.002987306797876954\n",
      "Epoch 3664, Loss: 0.00697441934607923, Final Batch Loss: 0.0043840790167450905\n",
      "Epoch 3665, Loss: 0.06868571415543556, Final Batch Loss: 0.03298264369368553\n",
      "Epoch 3666, Loss: 0.019132042187266052, Final Batch Loss: 0.0009031853405758739\n",
      "Epoch 3667, Loss: 0.03495426615700126, Final Batch Loss: 0.028244145214557648\n",
      "Epoch 3668, Loss: 0.013760820031166077, Final Batch Loss: 0.00798336323350668\n",
      "Epoch 3669, Loss: 0.012418859638273716, Final Batch Loss: 0.0031585898250341415\n",
      "Epoch 3670, Loss: 0.012706730049103498, Final Batch Loss: 0.005797147285193205\n",
      "Epoch 3671, Loss: 0.01178212184458971, Final Batch Loss: 0.0072295451536774635\n",
      "Epoch 3672, Loss: 0.02448184066452086, Final Batch Loss: 0.021075496450066566\n",
      "Epoch 3673, Loss: 0.013618934317491949, Final Batch Loss: 0.0016263587167486548\n",
      "Epoch 3674, Loss: 0.004476176807656884, Final Batch Loss: 0.0009765971917659044\n",
      "Epoch 3675, Loss: 0.003044146636966616, Final Batch Loss: 0.002187507227063179\n",
      "Epoch 3676, Loss: 0.006813933257944882, Final Batch Loss: 0.0011627055937424302\n",
      "Epoch 3677, Loss: 0.009378010756336153, Final Batch Loss: 0.007448413409292698\n",
      "Epoch 3678, Loss: 0.0771907027810812, Final Batch Loss: 0.017692601308226585\n",
      "Epoch 3679, Loss: 0.07954299822449684, Final Batch Loss: 0.04867948219180107\n",
      "Epoch 3680, Loss: 0.08175088919233531, Final Batch Loss: 0.08015289157629013\n",
      "Epoch 3681, Loss: 0.11727787926793098, Final Batch Loss: 0.0770515725016594\n",
      "Epoch 3682, Loss: 0.009742801310494542, Final Batch Loss: 0.003051806939765811\n",
      "Epoch 3683, Loss: 0.019260656321421266, Final Batch Loss: 0.0037851121742278337\n",
      "Epoch 3684, Loss: 0.007972046732902527, Final Batch Loss: 0.005614816676825285\n",
      "Epoch 3685, Loss: 0.023393298499286175, Final Batch Loss: 0.010743164457380772\n",
      "Epoch 3686, Loss: 0.016639333218336105, Final Batch Loss: 0.005041413940489292\n",
      "Epoch 3687, Loss: 0.014942594687454402, Final Batch Loss: 0.0010009490652009845\n",
      "Epoch 3688, Loss: 0.010608389973640442, Final Batch Loss: 0.006621087901294231\n",
      "Epoch 3689, Loss: 0.008077201549895108, Final Batch Loss: 0.0011169897625222802\n",
      "Epoch 3690, Loss: 0.01486241607926786, Final Batch Loss: 0.011744000017642975\n",
      "Epoch 3691, Loss: 0.019403783371672034, Final Batch Loss: 0.003510689130052924\n",
      "Epoch 3692, Loss: 0.027493796427734196, Final Batch Loss: 0.0014106343733146787\n",
      "Epoch 3693, Loss: 0.004347736714407802, Final Batch Loss: 0.002089487388730049\n",
      "Epoch 3694, Loss: 0.005749247851781547, Final Batch Loss: 0.0018497294513508677\n",
      "Epoch 3695, Loss: 0.03138852026313543, Final Batch Loss: 0.017562519758939743\n",
      "Epoch 3696, Loss: 0.03334650723263621, Final Batch Loss: 0.028003206476569176\n",
      "Epoch 3697, Loss: 0.006537769106216729, Final Batch Loss: 0.004654252901673317\n",
      "Epoch 3698, Loss: 0.005460425512865186, Final Batch Loss: 0.0031602669041603804\n",
      "Epoch 3699, Loss: 0.03951525827869773, Final Batch Loss: 0.03471992537379265\n",
      "Epoch 3700, Loss: 0.029281408991664648, Final Batch Loss: 0.0040702312253415585\n",
      "Epoch 3701, Loss: 0.005132887978106737, Final Batch Loss: 0.0020475825294852257\n",
      "Epoch 3702, Loss: 0.1136464811861515, Final Batch Loss: 0.0788593515753746\n",
      "Epoch 3703, Loss: 0.023518073721788824, Final Batch Loss: 0.0008763180812820792\n",
      "Epoch 3704, Loss: 0.05560779385268688, Final Batch Loss: 0.008126037195324898\n",
      "Epoch 3705, Loss: 0.017633316107094288, Final Batch Loss: 0.0011095022782683372\n",
      "Epoch 3706, Loss: 0.011025639018043876, Final Batch Loss: 0.008247165009379387\n",
      "Epoch 3707, Loss: 0.011704141041263938, Final Batch Loss: 0.003358683316037059\n",
      "Epoch 3708, Loss: 0.031980072846636176, Final Batch Loss: 0.0013340075965970755\n",
      "Epoch 3709, Loss: 0.0076555050909519196, Final Batch Loss: 0.0036209668032824993\n",
      "Epoch 3710, Loss: 0.0562660563737154, Final Batch Loss: 0.03209548071026802\n",
      "Epoch 3711, Loss: 0.005524779669940472, Final Batch Loss: 0.0038241567090153694\n",
      "Epoch 3712, Loss: 0.023483948782086372, Final Batch Loss: 0.00944308377802372\n",
      "Epoch 3713, Loss: 0.02056017005816102, Final Batch Loss: 0.006475130561739206\n",
      "Epoch 3714, Loss: 0.004599309992045164, Final Batch Loss: 0.0023521280381828547\n",
      "Epoch 3715, Loss: 0.017619538586586714, Final Batch Loss: 0.003619283903390169\n",
      "Epoch 3716, Loss: 0.010616965708322823, Final Batch Loss: 0.0005658623995259404\n",
      "Epoch 3717, Loss: 0.015265470137819648, Final Batch Loss: 0.0034288724418729544\n",
      "Epoch 3718, Loss: 0.03968021005857736, Final Batch Loss: 0.0015974614070728421\n",
      "Epoch 3719, Loss: 0.00729275681078434, Final Batch Loss: 0.004700431600213051\n",
      "Epoch 3720, Loss: 0.028419259004294872, Final Batch Loss: 0.019323786720633507\n",
      "Epoch 3721, Loss: 0.018563686055131257, Final Batch Loss: 0.016869259998202324\n",
      "Epoch 3722, Loss: 0.027276213746517897, Final Batch Loss: 0.024039672687649727\n",
      "Epoch 3723, Loss: 0.008221294963732362, Final Batch Loss: 0.005966721102595329\n",
      "Epoch 3724, Loss: 0.011536156525835395, Final Batch Loss: 0.008436135947704315\n",
      "Epoch 3725, Loss: 0.021455060923472047, Final Batch Loss: 0.01907825469970703\n",
      "Epoch 3726, Loss: 0.01594208087772131, Final Batch Loss: 0.013482194393873215\n",
      "Epoch 3727, Loss: 0.09385800175368786, Final Batch Loss: 0.01614362560212612\n",
      "Epoch 3728, Loss: 0.018748128786683083, Final Batch Loss: 0.004100663587450981\n",
      "Epoch 3729, Loss: 0.016269479412585497, Final Batch Loss: 0.003875081893056631\n",
      "Epoch 3730, Loss: 0.01624705153517425, Final Batch Loss: 0.0018255587201565504\n",
      "Epoch 3731, Loss: 0.023512946791015565, Final Batch Loss: 0.0013321473961696029\n",
      "Epoch 3732, Loss: 0.027220824966207147, Final Batch Loss: 0.024306653067469597\n",
      "Epoch 3733, Loss: 0.03955300711095333, Final Batch Loss: 0.015057163313031197\n",
      "Epoch 3734, Loss: 0.01697254041209817, Final Batch Loss: 0.0042246063239872456\n",
      "Epoch 3735, Loss: 0.012826342252083123, Final Batch Loss: 0.011395827867090702\n",
      "Epoch 3736, Loss: 0.004056834266521037, Final Batch Loss: 0.0024740020744502544\n",
      "Epoch 3737, Loss: 0.014227594248950481, Final Batch Loss: 0.002140703611075878\n",
      "Epoch 3738, Loss: 0.022630951600149274, Final Batch Loss: 0.0019190881866961718\n",
      "Epoch 3739, Loss: 0.006097110453993082, Final Batch Loss: 0.0041678897105157375\n",
      "Epoch 3740, Loss: 0.0041757633443921804, Final Batch Loss: 0.0029696922283619642\n",
      "Epoch 3741, Loss: 0.004507065401412547, Final Batch Loss: 0.0014946396695449948\n",
      "Epoch 3742, Loss: 0.02582971891388297, Final Batch Loss: 0.022047260776162148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3743, Loss: 0.04054621676914394, Final Batch Loss: 0.003662061644718051\n",
      "Epoch 3744, Loss: 0.041784792207181454, Final Batch Loss: 0.010497207753360271\n",
      "Epoch 3745, Loss: 0.0715092234313488, Final Batch Loss: 0.018680524080991745\n",
      "Epoch 3746, Loss: 0.016093062004074454, Final Batch Loss: 0.002503893105313182\n",
      "Epoch 3747, Loss: 0.04687482863664627, Final Batch Loss: 0.0018375776708126068\n",
      "Epoch 3748, Loss: 0.011238845065236092, Final Batch Loss: 0.00708475336432457\n",
      "Epoch 3749, Loss: 0.013907863292843103, Final Batch Loss: 0.009950327686965466\n",
      "Epoch 3750, Loss: 0.019205784425139427, Final Batch Loss: 0.001032443717122078\n",
      "Epoch 3751, Loss: 0.0570812257938087, Final Batch Loss: 0.002548664342612028\n",
      "Epoch 3752, Loss: 0.03588270582258701, Final Batch Loss: 0.03353742137551308\n",
      "Epoch 3753, Loss: 0.03398897126317024, Final Batch Loss: 0.008499762043356895\n",
      "Epoch 3754, Loss: 0.03911677747964859, Final Batch Loss: 0.028886714950203896\n",
      "Epoch 3755, Loss: 0.017321834340691566, Final Batch Loss: 0.004360298626124859\n",
      "Epoch 3756, Loss: 0.009559320635162294, Final Batch Loss: 0.0010027283569797873\n",
      "Epoch 3757, Loss: 0.017638627905398607, Final Batch Loss: 0.0015821387059986591\n",
      "Epoch 3758, Loss: 0.032098253606818616, Final Batch Loss: 0.030806751921772957\n",
      "Epoch 3759, Loss: 0.012096527963876724, Final Batch Loss: 0.007089854218065739\n",
      "Epoch 3760, Loss: 0.005087803350761533, Final Batch Loss: 0.002638936275616288\n",
      "Epoch 3761, Loss: 0.005041898228228092, Final Batch Loss: 0.0013780086301267147\n",
      "Epoch 3762, Loss: 0.01056535542011261, Final Batch Loss: 0.005461334250867367\n",
      "Epoch 3763, Loss: 0.01656155032105744, Final Batch Loss: 0.013805714435875416\n",
      "Epoch 3764, Loss: 0.03548593679443002, Final Batch Loss: 0.031172893941402435\n",
      "Epoch 3765, Loss: 0.01887278724461794, Final Batch Loss: 0.0078174052760005\n",
      "Epoch 3766, Loss: 0.02115317154675722, Final Batch Loss: 0.009148994460701942\n",
      "Epoch 3767, Loss: 0.010443586157634854, Final Batch Loss: 0.008192096836864948\n",
      "Epoch 3768, Loss: 0.009046262013725936, Final Batch Loss: 0.0013292253715917468\n",
      "Epoch 3769, Loss: 0.005282209720462561, Final Batch Loss: 0.0014547768514603376\n",
      "Epoch 3770, Loss: 0.005619074450805783, Final Batch Loss: 0.002062922576442361\n",
      "Epoch 3771, Loss: 0.010907899588346481, Final Batch Loss: 0.00303537305444479\n",
      "Epoch 3772, Loss: 0.008140594698488712, Final Batch Loss: 0.0060927108861505985\n",
      "Epoch 3773, Loss: 0.008081842679530382, Final Batch Loss: 0.00437577860429883\n",
      "Epoch 3774, Loss: 0.003831131267361343, Final Batch Loss: 0.0014274410204961896\n",
      "Epoch 3775, Loss: 0.017746658995747566, Final Batch Loss: 0.010377020575106144\n",
      "Epoch 3776, Loss: 0.0059955420438200235, Final Batch Loss: 0.004440018441528082\n",
      "Epoch 3777, Loss: 0.008266445016488433, Final Batch Loss: 0.003057225374504924\n",
      "Epoch 3778, Loss: 0.004892924043815583, Final Batch Loss: 0.0007383750635199249\n",
      "Epoch 3779, Loss: 0.007125073461793363, Final Batch Loss: 0.005401096306741238\n",
      "Epoch 3780, Loss: 0.04875585436820984, Final Batch Loss: 0.01599527895450592\n",
      "Epoch 3781, Loss: 0.004315053345635533, Final Batch Loss: 0.0024014643859118223\n",
      "Epoch 3782, Loss: 0.022167859598994255, Final Batch Loss: 0.017891978845000267\n",
      "Epoch 3783, Loss: 0.017526062671095133, Final Batch Loss: 0.01523071713745594\n",
      "Epoch 3784, Loss: 0.04370670858770609, Final Batch Loss: 0.03797410801053047\n",
      "Epoch 3785, Loss: 0.028335422626696527, Final Batch Loss: 0.001844004145823419\n",
      "Epoch 3786, Loss: 0.005197428399696946, Final Batch Loss: 0.0022006460931152105\n",
      "Epoch 3787, Loss: 0.00783132528886199, Final Batch Loss: 0.0011589368805289268\n",
      "Epoch 3788, Loss: 0.006034241057932377, Final Batch Loss: 0.0032594355288892984\n",
      "Epoch 3789, Loss: 0.0044657152611762285, Final Batch Loss: 0.002791674342006445\n",
      "Epoch 3790, Loss: 0.006871317746117711, Final Batch Loss: 0.002905951114371419\n",
      "Epoch 3791, Loss: 0.008270895574241877, Final Batch Loss: 0.006146708969026804\n",
      "Epoch 3792, Loss: 0.01357389078475535, Final Batch Loss: 0.011216510087251663\n",
      "Epoch 3793, Loss: 0.005826832028105855, Final Batch Loss: 0.003362379502505064\n",
      "Epoch 3794, Loss: 0.007647683494724333, Final Batch Loss: 0.005882147699594498\n",
      "Epoch 3795, Loss: 0.07740020263008773, Final Batch Loss: 0.0022934998851269484\n",
      "Epoch 3796, Loss: 0.0220107885543257, Final Batch Loss: 0.018646543845534325\n",
      "Epoch 3797, Loss: 0.006550280726514757, Final Batch Loss: 0.005172590725123882\n",
      "Epoch 3798, Loss: 0.038943166844546795, Final Batch Loss: 0.0351557619869709\n",
      "Epoch 3799, Loss: 0.013397101545706391, Final Batch Loss: 0.010854457505047321\n",
      "Epoch 3800, Loss: 0.014850994339212775, Final Batch Loss: 0.0019586344715207815\n",
      "Epoch 3801, Loss: 0.019610019167885184, Final Batch Loss: 0.0011995097156614065\n",
      "Epoch 3802, Loss: 0.020470575953368098, Final Batch Loss: 0.0006912402459420264\n",
      "Epoch 3803, Loss: 0.004917254322208464, Final Batch Loss: 0.0038601465057581663\n",
      "Epoch 3804, Loss: 0.01170102332253009, Final Batch Loss: 0.0007539057405665517\n",
      "Epoch 3805, Loss: 0.009010865935124457, Final Batch Loss: 0.0017779447371140122\n",
      "Epoch 3806, Loss: 0.004729177453555167, Final Batch Loss: 0.001388360164128244\n",
      "Epoch 3807, Loss: 0.027849666541442275, Final Batch Loss: 0.024946048855781555\n",
      "Epoch 3808, Loss: 0.045863574370741844, Final Batch Loss: 0.027124032378196716\n",
      "Epoch 3809, Loss: 0.026300346944481134, Final Batch Loss: 0.0242462120950222\n",
      "Epoch 3810, Loss: 0.041372669860720634, Final Batch Loss: 0.010117271915078163\n",
      "Epoch 3811, Loss: 0.0071235314244404435, Final Batch Loss: 0.001243192353285849\n",
      "Epoch 3812, Loss: 0.00955642363987863, Final Batch Loss: 0.0026637224946171045\n",
      "Epoch 3813, Loss: 0.021377108991146088, Final Batch Loss: 0.011177042499184608\n",
      "Epoch 3814, Loss: 0.0087893046438694, Final Batch Loss: 0.0036877221427857876\n",
      "Epoch 3815, Loss: 0.008213497698307037, Final Batch Loss: 0.005867983680218458\n",
      "Epoch 3816, Loss: 0.03942491393536329, Final Batch Loss: 0.028536589816212654\n",
      "Epoch 3817, Loss: 0.004297051462344825, Final Batch Loss: 0.0028927938546985388\n",
      "Epoch 3818, Loss: 0.009644055273383856, Final Batch Loss: 0.0017314418219029903\n",
      "Epoch 3819, Loss: 0.03325887257233262, Final Batch Loss: 0.02894538827240467\n",
      "Epoch 3820, Loss: 0.013653554487973452, Final Batch Loss: 0.003963612485677004\n",
      "Epoch 3821, Loss: 0.016518506221473217, Final Batch Loss: 0.014038515277206898\n",
      "Epoch 3822, Loss: 0.01448336080648005, Final Batch Loss: 0.0030370161402970552\n",
      "Epoch 3823, Loss: 0.0027134697884321213, Final Batch Loss: 0.0014829159481450915\n",
      "Epoch 3824, Loss: 0.004973238450475037, Final Batch Loss: 0.0017451898893341422\n",
      "Epoch 3825, Loss: 0.007573606446385384, Final Batch Loss: 0.004120822064578533\n",
      "Epoch 3826, Loss: 0.005043105978984386, Final Batch Loss: 0.0042185913771390915\n",
      "Epoch 3827, Loss: 0.006769494386389852, Final Batch Loss: 0.004789118655025959\n",
      "Epoch 3828, Loss: 0.009908086154609919, Final Batch Loss: 0.005547663662582636\n",
      "Epoch 3829, Loss: 0.003971285012084991, Final Batch Loss: 0.0009761090041138232\n",
      "Epoch 3830, Loss: 0.003833109396509826, Final Batch Loss: 0.0015972667606547475\n",
      "Epoch 3831, Loss: 0.06789070321246982, Final Batch Loss: 0.06585464626550674\n",
      "Epoch 3832, Loss: 0.011733582010492682, Final Batch Loss: 0.009187263436615467\n",
      "Epoch 3833, Loss: 0.007826786721125245, Final Batch Loss: 0.0021496734116226435\n",
      "Epoch 3834, Loss: 0.016721515217795968, Final Batch Loss: 0.003202146152034402\n",
      "Epoch 3835, Loss: 0.004307033959776163, Final Batch Loss: 0.0020357812754809856\n",
      "Epoch 3836, Loss: 0.015294142009224743, Final Batch Loss: 0.0006468099891208112\n",
      "Epoch 3837, Loss: 0.05847885878756642, Final Batch Loss: 0.005323801655322313\n",
      "Epoch 3838, Loss: 0.02691898960620165, Final Batch Loss: 0.002406594343483448\n",
      "Epoch 3839, Loss: 0.004117554955882952, Final Batch Loss: 0.00025470551918260753\n",
      "Epoch 3840, Loss: 0.0022176785860210657, Final Batch Loss: 0.0009617726318538189\n",
      "Epoch 3841, Loss: 0.004165425663813949, Final Batch Loss: 0.002923382446169853\n",
      "Epoch 3842, Loss: 0.02164169494062662, Final Batch Loss: 0.018748532980680466\n",
      "Epoch 3843, Loss: 0.007526623900048435, Final Batch Loss: 0.006516245659440756\n",
      "Epoch 3844, Loss: 0.003768433234654367, Final Batch Loss: 0.002245381474494934\n",
      "Epoch 3845, Loss: 0.002465584606397897, Final Batch Loss: 0.0007341706077568233\n",
      "Epoch 3846, Loss: 0.005232319701462984, Final Batch Loss: 0.0029542725533246994\n",
      "Epoch 3847, Loss: 0.013067688792943954, Final Batch Loss: 0.0036972779780626297\n",
      "Epoch 3848, Loss: 0.0080799232237041, Final Batch Loss: 0.002772074658423662\n",
      "Epoch 3849, Loss: 0.002944106003269553, Final Batch Loss: 0.0015350577887147665\n",
      "Epoch 3850, Loss: 0.011721143964678049, Final Batch Loss: 0.010018602944910526\n",
      "Epoch 3851, Loss: 0.013516163919121027, Final Batch Loss: 0.005828545894473791\n",
      "Epoch 3852, Loss: 0.00471633329289034, Final Batch Loss: 0.004073027521371841\n",
      "Epoch 3853, Loss: 0.0047042371006682515, Final Batch Loss: 0.0011009356239810586\n",
      "Epoch 3854, Loss: 0.004317607264965773, Final Batch Loss: 0.0010680933482944965\n",
      "Epoch 3855, Loss: 0.00669637304963544, Final Batch Loss: 0.00046109018148854375\n",
      "Epoch 3856, Loss: 0.017758161528036, Final Batch Loss: 0.015390281565487385\n",
      "Epoch 3857, Loss: 0.007890991517342627, Final Batch Loss: 0.0067862896248698235\n",
      "Epoch 3858, Loss: 0.004404677310958505, Final Batch Loss: 0.0030586672946810722\n",
      "Epoch 3859, Loss: 0.002099639503285289, Final Batch Loss: 0.0010976368794217706\n",
      "Epoch 3860, Loss: 0.0011941258562728763, Final Batch Loss: 0.0005751862190663815\n",
      "Epoch 3861, Loss: 0.005896067945286632, Final Batch Loss: 0.0022350777871906757\n",
      "Epoch 3862, Loss: 0.004147978615947068, Final Batch Loss: 0.0025954297743737698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3863, Loss: 0.0028374569956213236, Final Batch Loss: 0.0017371773719787598\n",
      "Epoch 3864, Loss: 0.005063570628408343, Final Batch Loss: 0.00024202564964070916\n",
      "Epoch 3865, Loss: 0.01784882473293692, Final Batch Loss: 0.0014775056624785066\n",
      "Epoch 3866, Loss: 0.010210884734988213, Final Batch Loss: 0.0018883952870965004\n",
      "Epoch 3867, Loss: 0.004634919343516231, Final Batch Loss: 0.002571041928604245\n",
      "Epoch 3868, Loss: 0.0030991119565442204, Final Batch Loss: 0.0016422016778960824\n",
      "Epoch 3869, Loss: 0.017469684360548854, Final Batch Loss: 0.0020161063876003027\n",
      "Epoch 3870, Loss: 0.003278853284427896, Final Batch Loss: 0.00018177778110839427\n",
      "Epoch 3871, Loss: 0.006057890481315553, Final Batch Loss: 0.004620295483618975\n",
      "Epoch 3872, Loss: 0.0022885765065439045, Final Batch Loss: 0.0007401599432341754\n",
      "Epoch 3873, Loss: 0.02328930119983852, Final Batch Loss: 0.022186370566487312\n",
      "Epoch 3874, Loss: 0.0011952821223530918, Final Batch Loss: 0.0004779499431606382\n",
      "Epoch 3875, Loss: 0.0018617662135511637, Final Batch Loss: 0.0005950015038251877\n",
      "Epoch 3876, Loss: 0.0024798475205898285, Final Batch Loss: 0.0015323155093938112\n",
      "Epoch 3877, Loss: 0.0061065173940733075, Final Batch Loss: 0.0012921761954203248\n",
      "Epoch 3878, Loss: 0.00635795239941217, Final Batch Loss: 0.000443905038991943\n",
      "Epoch 3879, Loss: 0.0026228849310427904, Final Batch Loss: 0.0005595376715064049\n",
      "Epoch 3880, Loss: 0.0038312956457957625, Final Batch Loss: 0.002278661821037531\n",
      "Epoch 3881, Loss: 0.005562971346080303, Final Batch Loss: 0.0029927340801805258\n",
      "Epoch 3882, Loss: 0.004638932819943875, Final Batch Loss: 0.003887198166921735\n",
      "Epoch 3883, Loss: 0.009385907789692283, Final Batch Loss: 0.0004333618562668562\n",
      "Epoch 3884, Loss: 0.01011232566088438, Final Batch Loss: 0.0033456948585808277\n",
      "Epoch 3885, Loss: 0.0022839122684672475, Final Batch Loss: 0.001274213776923716\n",
      "Epoch 3886, Loss: 0.004769528051838279, Final Batch Loss: 0.0018469151109457016\n",
      "Epoch 3887, Loss: 0.008591438177973032, Final Batch Loss: 0.0025002225302159786\n",
      "Epoch 3888, Loss: 0.017161997850053012, Final Batch Loss: 0.015235739760100842\n",
      "Epoch 3889, Loss: 0.030322742648422718, Final Batch Loss: 0.0019256388768553734\n",
      "Epoch 3890, Loss: 0.027352241799235344, Final Batch Loss: 0.012163233011960983\n",
      "Epoch 3891, Loss: 0.022449938289355487, Final Batch Loss: 0.02168450690805912\n",
      "Epoch 3892, Loss: 0.0033964980393648148, Final Batch Loss: 0.0012251518201082945\n",
      "Epoch 3893, Loss: 0.013243675697594881, Final Batch Loss: 0.003604584839195013\n",
      "Epoch 3894, Loss: 0.0023100952967070043, Final Batch Loss: 0.0004382021143101156\n",
      "Epoch 3895, Loss: 0.014527425169944763, Final Batch Loss: 0.0042800381779670715\n",
      "Epoch 3896, Loss: 0.014812771696597338, Final Batch Loss: 0.010551905259490013\n",
      "Epoch 3897, Loss: 0.0045646370854228735, Final Batch Loss: 0.0021045247558504343\n",
      "Epoch 3898, Loss: 0.08326006680727005, Final Batch Loss: 0.044382013380527496\n",
      "Epoch 3899, Loss: 0.05515228770673275, Final Batch Loss: 0.038903530687093735\n",
      "Epoch 3900, Loss: 0.08667944744229317, Final Batch Loss: 0.0503428690135479\n",
      "Epoch 3901, Loss: 0.011307041626423597, Final Batch Loss: 0.0037857573479413986\n",
      "Epoch 3902, Loss: 0.0032029757276177406, Final Batch Loss: 0.0010154542978852987\n",
      "Epoch 3903, Loss: 0.007480961969122291, Final Batch Loss: 0.006472610402852297\n",
      "Epoch 3904, Loss: 0.007945612072944641, Final Batch Loss: 0.004367286339402199\n",
      "Epoch 3905, Loss: 0.023347286565694958, Final Batch Loss: 0.02264605462551117\n",
      "Epoch 3906, Loss: 0.0073836620431393385, Final Batch Loss: 0.006103991065174341\n",
      "Epoch 3907, Loss: 0.013262600637972355, Final Batch Loss: 0.003979453817009926\n",
      "Epoch 3908, Loss: 0.02009109640493989, Final Batch Loss: 0.0015078713186085224\n",
      "Epoch 3909, Loss: 0.01413999404758215, Final Batch Loss: 0.003922702744603157\n",
      "Epoch 3910, Loss: 0.010895456187427044, Final Batch Loss: 0.004409450571984053\n",
      "Epoch 3911, Loss: 0.009528762893751264, Final Batch Loss: 0.002406935440376401\n",
      "Epoch 3912, Loss: 0.01766730472445488, Final Batch Loss: 0.0033668531104922295\n",
      "Epoch 3913, Loss: 0.005994818871840835, Final Batch Loss: 0.0020407920237630606\n",
      "Epoch 3914, Loss: 0.009199892869219184, Final Batch Loss: 0.0021503607276827097\n",
      "Epoch 3915, Loss: 0.030718084424734116, Final Batch Loss: 0.01864955574274063\n",
      "Epoch 3916, Loss: 0.0059340831357985735, Final Batch Loss: 0.0034897353034466505\n",
      "Epoch 3917, Loss: 0.04248661734163761, Final Batch Loss: 0.014384010806679726\n",
      "Epoch 3918, Loss: 0.014379695523530245, Final Batch Loss: 0.002040689345449209\n",
      "Epoch 3919, Loss: 0.01232504565268755, Final Batch Loss: 0.006261139176785946\n",
      "Epoch 3920, Loss: 0.03331829293165356, Final Batch Loss: 0.0322919599711895\n",
      "Epoch 3921, Loss: 0.0029674957040697336, Final Batch Loss: 0.0013037071330472827\n",
      "Epoch 3922, Loss: 0.006245612632483244, Final Batch Loss: 0.00268358807079494\n",
      "Epoch 3923, Loss: 0.0038930715527385473, Final Batch Loss: 0.0010150943417102098\n",
      "Epoch 3924, Loss: 0.011257980251684785, Final Batch Loss: 0.0017339123878628016\n",
      "Epoch 3925, Loss: 0.011310964822769165, Final Batch Loss: 0.009309330955147743\n",
      "Epoch 3926, Loss: 0.005729835480451584, Final Batch Loss: 0.002034140285104513\n",
      "Epoch 3927, Loss: 0.005679375026375055, Final Batch Loss: 0.0013880007900297642\n",
      "Epoch 3928, Loss: 0.002462139236740768, Final Batch Loss: 0.0011244580382481217\n",
      "Epoch 3929, Loss: 0.006151881301775575, Final Batch Loss: 0.0020677947904914618\n",
      "Epoch 3930, Loss: 0.003651066275779158, Final Batch Loss: 0.0006878730491735041\n",
      "Epoch 3931, Loss: 0.0031839575385674834, Final Batch Loss: 0.0019010471878573298\n",
      "Epoch 3932, Loss: 0.047572061419487, Final Batch Loss: 0.02851446531713009\n",
      "Epoch 3933, Loss: 0.0024820109829306602, Final Batch Loss: 0.0011343585792928934\n",
      "Epoch 3934, Loss: 0.005456012790091336, Final Batch Loss: 0.0013540216023102403\n",
      "Epoch 3935, Loss: 0.003914734232239425, Final Batch Loss: 0.0031261981930583715\n",
      "Epoch 3936, Loss: 0.005092868115752935, Final Batch Loss: 0.002852607751265168\n",
      "Epoch 3937, Loss: 0.058016587514430285, Final Batch Loss: 0.05324174091219902\n",
      "Epoch 3938, Loss: 0.019100897712633014, Final Batch Loss: 0.015721090137958527\n",
      "Epoch 3939, Loss: 0.039433781523257494, Final Batch Loss: 0.002413943875581026\n",
      "Epoch 3940, Loss: 0.009053277201019228, Final Batch Loss: 0.0016292784130200744\n",
      "Epoch 3941, Loss: 0.04699992563109845, Final Batch Loss: 0.0454593226313591\n",
      "Epoch 3942, Loss: 0.006983994739130139, Final Batch Loss: 0.004406219348311424\n",
      "Epoch 3943, Loss: 0.02266921359114349, Final Batch Loss: 0.003602259559556842\n",
      "Epoch 3944, Loss: 0.030235620215535164, Final Batch Loss: 0.003949303179979324\n",
      "Epoch 3945, Loss: 0.06006073299795389, Final Batch Loss: 0.013497577048838139\n",
      "Epoch 3946, Loss: 0.027807465754449368, Final Batch Loss: 0.013254859484732151\n",
      "Epoch 3947, Loss: 0.018655910156667233, Final Batch Loss: 0.007921463809907436\n",
      "Epoch 3948, Loss: 0.023878308478742838, Final Batch Loss: 0.0049244859255850315\n",
      "Epoch 3949, Loss: 0.01105264388024807, Final Batch Loss: 0.0015064086765050888\n",
      "Epoch 3950, Loss: 0.00651598465628922, Final Batch Loss: 0.005128736142069101\n",
      "Epoch 3951, Loss: 0.010834958171471953, Final Batch Loss: 0.002606201684102416\n",
      "Epoch 3952, Loss: 0.02250605239532888, Final Batch Loss: 0.0196270402520895\n",
      "Epoch 3953, Loss: 0.009468341711908579, Final Batch Loss: 0.0064150928519666195\n",
      "Epoch 3954, Loss: 0.015395823400467634, Final Batch Loss: 0.0022520176135003567\n",
      "Epoch 3955, Loss: 0.017710807733237743, Final Batch Loss: 0.009141610935330391\n",
      "Epoch 3956, Loss: 0.0029438522178679705, Final Batch Loss: 0.00026967725716531277\n",
      "Epoch 3957, Loss: 0.0021709719439968467, Final Batch Loss: 0.0006573267746716738\n",
      "Epoch 3958, Loss: 0.016918549314141273, Final Batch Loss: 0.010992540046572685\n",
      "Epoch 3959, Loss: 0.009413394145667553, Final Batch Loss: 0.0047026220709085464\n",
      "Epoch 3960, Loss: 0.008358597289770842, Final Batch Loss: 0.0031096655875444412\n",
      "Epoch 3961, Loss: 0.007119637448340654, Final Batch Loss: 0.0034813371021300554\n",
      "Epoch 3962, Loss: 0.005455612321384251, Final Batch Loss: 0.0009907764615491033\n",
      "Epoch 3963, Loss: 0.01501076587010175, Final Batch Loss: 0.013427607715129852\n",
      "Epoch 3964, Loss: 0.010989702655933797, Final Batch Loss: 0.0011400965740904212\n",
      "Epoch 3965, Loss: 0.005741084925830364, Final Batch Loss: 0.0038052459713071585\n",
      "Epoch 3966, Loss: 0.005464130837935954, Final Batch Loss: 0.0047017671167850494\n",
      "Epoch 3967, Loss: 0.003451338969171047, Final Batch Loss: 0.00085080461576581\n",
      "Epoch 3968, Loss: 0.04067034460604191, Final Batch Loss: 0.023160912096500397\n",
      "Epoch 3969, Loss: 0.011189725366421044, Final Batch Loss: 0.009375795722007751\n",
      "Epoch 3970, Loss: 0.018727846909314394, Final Batch Loss: 0.00635475805029273\n",
      "Epoch 3971, Loss: 0.06985347485169768, Final Batch Loss: 0.06600731611251831\n",
      "Epoch 3972, Loss: 0.004788367077708244, Final Batch Loss: 0.003926650155335665\n",
      "Epoch 3973, Loss: 0.005298712057992816, Final Batch Loss: 0.003886632854118943\n",
      "Epoch 3974, Loss: 0.013826682232320309, Final Batch Loss: 0.005494593642652035\n",
      "Epoch 3975, Loss: 0.024484047200530767, Final Batch Loss: 0.0022860965691506863\n",
      "Epoch 3976, Loss: 0.022031218744814396, Final Batch Loss: 0.009297150187194347\n",
      "Epoch 3977, Loss: 0.007065030513331294, Final Batch Loss: 0.0009015083778649569\n",
      "Epoch 3978, Loss: 0.014207204570993781, Final Batch Loss: 0.011566936038434505\n",
      "Epoch 3979, Loss: 0.02966808620840311, Final Batch Loss: 0.008319900371134281\n",
      "Epoch 3980, Loss: 0.0021666623651981354, Final Batch Loss: 0.0007895466405898333\n",
      "Epoch 3981, Loss: 0.007139584049582481, Final Batch Loss: 0.003294945927336812\n",
      "Epoch 3982, Loss: 0.04502700176090002, Final Batch Loss: 0.030098306015133858\n",
      "Epoch 3983, Loss: 0.010014099068939686, Final Batch Loss: 0.004562366288155317\n",
      "Epoch 3984, Loss: 0.008882195921614766, Final Batch Loss: 0.00662069208920002\n",
      "Epoch 3985, Loss: 0.031697730999439955, Final Batch Loss: 0.02443227916955948\n",
      "Epoch 3986, Loss: 0.002862253342755139, Final Batch Loss: 0.0015551225515082479\n",
      "Epoch 3987, Loss: 0.01426115445792675, Final Batch Loss: 0.013074551708996296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3988, Loss: 0.008109641959890723, Final Batch Loss: 0.0016988108400255442\n",
      "Epoch 3989, Loss: 0.008754896931350231, Final Batch Loss: 0.003146698698401451\n",
      "Epoch 3990, Loss: 0.0038618266116827726, Final Batch Loss: 0.002342440187931061\n",
      "Epoch 3991, Loss: 0.02583371102809906, Final Batch Loss: 0.02381911687552929\n",
      "Epoch 3992, Loss: 0.0038021498476155102, Final Batch Loss: 0.0032774382270872593\n",
      "Epoch 3993, Loss: 0.009496234823018312, Final Batch Loss: 0.00516699580475688\n",
      "Epoch 3994, Loss: 0.007870547124184668, Final Batch Loss: 0.0018265062244608998\n",
      "Epoch 3995, Loss: 0.003460061619989574, Final Batch Loss: 0.0022807784844189882\n",
      "Epoch 3996, Loss: 0.007417958462610841, Final Batch Loss: 0.004963706713169813\n",
      "Epoch 3997, Loss: 0.013031902024522424, Final Batch Loss: 0.001152013661339879\n",
      "Epoch 3998, Loss: 0.04429567605257034, Final Batch Loss: 0.031647760421037674\n",
      "Epoch 3999, Loss: 0.013389018829911947, Final Batch Loss: 0.004827956203371286\n",
      "Epoch 4000, Loss: 0.023238914785906672, Final Batch Loss: 0.002116949064657092\n",
      "Epoch 4001, Loss: 0.05100163072347641, Final Batch Loss: 0.04848567023873329\n",
      "Epoch 4002, Loss: 0.024718349799513817, Final Batch Loss: 0.008320767432451248\n",
      "Epoch 4003, Loss: 0.007157925749197602, Final Batch Loss: 0.0023174972739070654\n",
      "Epoch 4004, Loss: 0.005854383343830705, Final Batch Loss: 0.002222335198894143\n",
      "Epoch 4005, Loss: 0.034889429807662964, Final Batch Loss: 0.017405513674020767\n",
      "Epoch 4006, Loss: 0.0126292675267905, Final Batch Loss: 0.002085579326376319\n",
      "Epoch 4007, Loss: 0.005664705531671643, Final Batch Loss: 0.0030481882859021425\n",
      "Epoch 4008, Loss: 0.03712616953998804, Final Batch Loss: 0.003158961422741413\n",
      "Epoch 4009, Loss: 0.010963383130729198, Final Batch Loss: 0.0015223175287246704\n",
      "Epoch 4010, Loss: 0.06319782696664333, Final Batch Loss: 0.042619261890649796\n",
      "Epoch 4011, Loss: 0.01704924344085157, Final Batch Loss: 0.0023687935899943113\n",
      "Epoch 4012, Loss: 0.014540725387632847, Final Batch Loss: 0.007084080949425697\n",
      "Epoch 4013, Loss: 0.006831855163909495, Final Batch Loss: 0.0004884150112047791\n",
      "Epoch 4014, Loss: 0.042892773635685444, Final Batch Loss: 0.009897378273308277\n",
      "Epoch 4015, Loss: 0.007949989754706621, Final Batch Loss: 0.006040595937520266\n",
      "Epoch 4016, Loss: 0.004906407557427883, Final Batch Loss: 0.0033909082412719727\n",
      "Epoch 4017, Loss: 0.0025298307300545275, Final Batch Loss: 0.0007909527630545199\n",
      "Epoch 4018, Loss: 0.024674082873389125, Final Batch Loss: 0.0011169023346155882\n",
      "Epoch 4019, Loss: 0.0390157587826252, Final Batch Loss: 0.017543133348226547\n",
      "Epoch 4020, Loss: 0.013492220954503864, Final Batch Loss: 0.012645453214645386\n",
      "Epoch 4021, Loss: 0.01366987219080329, Final Batch Loss: 0.006849114317446947\n",
      "Epoch 4022, Loss: 0.0317426691763103, Final Batch Loss: 0.006105340551584959\n",
      "Epoch 4023, Loss: 0.00407999410526827, Final Batch Loss: 0.0009642127552069724\n",
      "Epoch 4024, Loss: 0.021600553765892982, Final Batch Loss: 0.012320209294557571\n",
      "Epoch 4025, Loss: 0.010873194318264723, Final Batch Loss: 0.0066435751505196095\n",
      "Epoch 4026, Loss: 0.058172311168164015, Final Batch Loss: 0.0054255458526313305\n",
      "Epoch 4027, Loss: 0.008680851897224784, Final Batch Loss: 0.0029100493993610144\n",
      "Epoch 4028, Loss: 0.005814543226733804, Final Batch Loss: 0.004813177045434713\n",
      "Epoch 4029, Loss: 0.012438196747098118, Final Batch Loss: 0.0007587653235532343\n",
      "Epoch 4030, Loss: 0.004602206638082862, Final Batch Loss: 0.0031266803853213787\n",
      "Epoch 4031, Loss: 0.042815047316253185, Final Batch Loss: 0.03958098590373993\n",
      "Epoch 4032, Loss: 0.01011620566714555, Final Batch Loss: 0.008209893479943275\n",
      "Epoch 4033, Loss: 0.07739349640905857, Final Batch Loss: 0.048338454216718674\n",
      "Epoch 4034, Loss: 0.0037676445208489895, Final Batch Loss: 0.001682961592450738\n",
      "Epoch 4035, Loss: 0.08071779273450375, Final Batch Loss: 0.020421555265784264\n",
      "Epoch 4036, Loss: 0.016344892792403698, Final Batch Loss: 0.008424405939877033\n",
      "Epoch 4037, Loss: 0.059797557070851326, Final Batch Loss: 0.04273689165711403\n",
      "Epoch 4038, Loss: 0.00953010783996433, Final Batch Loss: 0.008468539454042912\n",
      "Epoch 4039, Loss: 0.023613445460796356, Final Batch Loss: 0.012497689574956894\n",
      "Epoch 4040, Loss: 0.08663287665694952, Final Batch Loss: 0.011272951029241085\n",
      "Epoch 4041, Loss: 0.014421371743083, Final Batch Loss: 0.007022951729595661\n",
      "Epoch 4042, Loss: 0.03687033895403147, Final Batch Loss: 0.007912305183708668\n",
      "Epoch 4043, Loss: 0.03852871619164944, Final Batch Loss: 0.03266870602965355\n",
      "Epoch 4044, Loss: 0.034251551143825054, Final Batch Loss: 0.008439677767455578\n",
      "Epoch 4045, Loss: 0.016508531291037798, Final Batch Loss: 0.004757620859891176\n",
      "Epoch 4046, Loss: 0.014527460909448564, Final Batch Loss: 0.01297229714691639\n",
      "Epoch 4047, Loss: 0.016204968094825745, Final Batch Loss: 0.010367297567427158\n",
      "Epoch 4048, Loss: 0.02307486440986395, Final Batch Loss: 0.011918558739125729\n",
      "Epoch 4049, Loss: 0.03438083059154451, Final Batch Loss: 0.003413144266232848\n",
      "Epoch 4050, Loss: 0.004338387516327202, Final Batch Loss: 0.002615046687424183\n",
      "Epoch 4051, Loss: 0.01938760932534933, Final Batch Loss: 0.01458633504807949\n",
      "Epoch 4052, Loss: 0.04173905774950981, Final Batch Loss: 0.03662342205643654\n",
      "Epoch 4053, Loss: 0.006678109057247639, Final Batch Loss: 0.002965463325381279\n",
      "Epoch 4054, Loss: 0.0031528198160231113, Final Batch Loss: 0.0015087961219251156\n",
      "Epoch 4055, Loss: 0.011788568692281842, Final Batch Loss: 0.002255287254229188\n",
      "Epoch 4056, Loss: 0.008112712297588587, Final Batch Loss: 0.00588561873883009\n",
      "Epoch 4057, Loss: 0.025670252740383148, Final Batch Loss: 0.01911204122006893\n",
      "Epoch 4058, Loss: 0.019700859673321247, Final Batch Loss: 0.004117737524211407\n",
      "Epoch 4059, Loss: 0.009628993459045887, Final Batch Loss: 0.0074092731811106205\n",
      "Epoch 4060, Loss: 0.005389973404817283, Final Batch Loss: 0.0015713825123384595\n",
      "Epoch 4061, Loss: 0.01759151229634881, Final Batch Loss: 0.0022894428111612797\n",
      "Epoch 4062, Loss: 0.0611877366900444, Final Batch Loss: 0.032804012298583984\n",
      "Epoch 4063, Loss: 0.013756323489360511, Final Batch Loss: 0.011874792166054249\n",
      "Epoch 4064, Loss: 0.03455538465641439, Final Batch Loss: 0.0022511796560138464\n",
      "Epoch 4065, Loss: 0.007665284443646669, Final Batch Loss: 0.0018665697425603867\n",
      "Epoch 4066, Loss: 0.008200168493203819, Final Batch Loss: 0.006694667972624302\n",
      "Epoch 4067, Loss: 0.003924825461581349, Final Batch Loss: 0.0018920386210083961\n",
      "Epoch 4068, Loss: 0.0070052599767223, Final Batch Loss: 0.005102281458675861\n",
      "Epoch 4069, Loss: 0.012855688109993935, Final Batch Loss: 0.010092059150338173\n",
      "Epoch 4070, Loss: 0.005967196309939027, Final Batch Loss: 0.0035767056979238987\n",
      "Epoch 4071, Loss: 0.005501509644091129, Final Batch Loss: 0.0016093640588223934\n",
      "Epoch 4072, Loss: 0.005069790291599929, Final Batch Loss: 0.001561069511808455\n",
      "Epoch 4073, Loss: 0.020873337984085083, Final Batch Loss: 0.013001449406147003\n",
      "Epoch 4074, Loss: 0.007876664749346673, Final Batch Loss: 0.001018043956719339\n",
      "Epoch 4075, Loss: 0.010803446639329195, Final Batch Loss: 0.006286228075623512\n",
      "Epoch 4076, Loss: 0.022281506564468145, Final Batch Loss: 0.006293631624430418\n",
      "Epoch 4077, Loss: 0.007912480738013983, Final Batch Loss: 0.0014321133494377136\n",
      "Epoch 4078, Loss: 0.0032190638594329357, Final Batch Loss: 0.002046449575573206\n",
      "Epoch 4079, Loss: 0.05313874175772071, Final Batch Loss: 0.050120119005441666\n",
      "Epoch 4080, Loss: 0.011169377248734236, Final Batch Loss: 0.0043184892274439335\n",
      "Epoch 4081, Loss: 0.0027378828381188214, Final Batch Loss: 0.0009668621351011097\n",
      "Epoch 4082, Loss: 0.01461211172863841, Final Batch Loss: 0.007216508034616709\n",
      "Epoch 4083, Loss: 0.0365800429135561, Final Batch Loss: 0.018640780821442604\n",
      "Epoch 4084, Loss: 0.004187592305243015, Final Batch Loss: 0.0009911386296153069\n",
      "Epoch 4085, Loss: 0.013687725877389312, Final Batch Loss: 0.011552191339433193\n",
      "Epoch 4086, Loss: 0.013801231514662504, Final Batch Loss: 0.008371442556381226\n",
      "Epoch 4087, Loss: 0.016668695025146008, Final Batch Loss: 0.011392692103981972\n",
      "Epoch 4088, Loss: 0.0045613066758960485, Final Batch Loss: 0.003453043522313237\n",
      "Epoch 4089, Loss: 0.005791471805423498, Final Batch Loss: 0.0016604126431047916\n",
      "Epoch 4090, Loss: 0.003596198628656566, Final Batch Loss: 0.0013907694956287742\n",
      "Epoch 4091, Loss: 0.007474732119590044, Final Batch Loss: 0.0019970275461673737\n",
      "Epoch 4092, Loss: 0.00921747146639973, Final Batch Loss: 0.008047865703701973\n",
      "Epoch 4093, Loss: 0.03597110975533724, Final Batch Loss: 0.003508412279188633\n",
      "Epoch 4094, Loss: 0.015091708395630121, Final Batch Loss: 0.003960628528147936\n",
      "Epoch 4095, Loss: 0.08303852193057537, Final Batch Loss: 0.06150073930621147\n",
      "Epoch 4096, Loss: 0.004429096356034279, Final Batch Loss: 0.002050952287390828\n",
      "Epoch 4097, Loss: 0.016923374496400356, Final Batch Loss: 0.011725615710020065\n",
      "Epoch 4098, Loss: 0.0031846235506236553, Final Batch Loss: 0.0008765212260186672\n",
      "Epoch 4099, Loss: 0.003164442256093025, Final Batch Loss: 0.00204805051907897\n",
      "Epoch 4100, Loss: 0.024740319175180048, Final Batch Loss: 0.0008695641881786287\n",
      "Epoch 4101, Loss: 0.010192001005634665, Final Batch Loss: 0.008316905237734318\n",
      "Epoch 4102, Loss: 0.028456565458327532, Final Batch Loss: 0.004168726969510317\n",
      "Epoch 4103, Loss: 0.009421862079761922, Final Batch Loss: 0.0015227211406454444\n",
      "Epoch 4104, Loss: 0.004337501246482134, Final Batch Loss: 0.0024913125671446323\n",
      "Epoch 4105, Loss: 0.03158530127257109, Final Batch Loss: 0.004266382195055485\n",
      "Epoch 4106, Loss: 0.0027393706259317696, Final Batch Loss: 0.0021382314153015614\n",
      "Epoch 4107, Loss: 0.0142967538558878, Final Batch Loss: 0.00028184946859255433\n",
      "Epoch 4108, Loss: 0.019275850150734186, Final Batch Loss: 0.0011158124543726444\n",
      "Epoch 4109, Loss: 0.003059790702536702, Final Batch Loss: 0.0014417503261938691\n",
      "Epoch 4110, Loss: 0.04518561065196991, Final Batch Loss: 0.03786568343639374\n",
      "Epoch 4111, Loss: 0.007263223407790065, Final Batch Loss: 0.0014242681208997965\n",
      "Epoch 4112, Loss: 0.0065335055696778, Final Batch Loss: 0.005710072815418243\n",
      "Epoch 4113, Loss: 0.003960085101425648, Final Batch Loss: 0.0020413987804204226\n",
      "Epoch 4114, Loss: 0.0043489711824804544, Final Batch Loss: 0.002097292570397258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4115, Loss: 0.04281058255583048, Final Batch Loss: 0.029677927494049072\n",
      "Epoch 4116, Loss: 0.03564045505481772, Final Batch Loss: 0.00042983071762137115\n",
      "Epoch 4117, Loss: 0.0801378209143877, Final Batch Loss: 0.07218004763126373\n",
      "Epoch 4118, Loss: 0.017868309980258346, Final Batch Loss: 0.002506140386685729\n",
      "Epoch 4119, Loss: 0.006871558725833893, Final Batch Loss: 0.005426417570561171\n",
      "Epoch 4120, Loss: 0.011045576422475278, Final Batch Loss: 0.001090850098989904\n",
      "Epoch 4121, Loss: 0.03711756505072117, Final Batch Loss: 0.018696391955018044\n",
      "Epoch 4122, Loss: 0.007741890847682953, Final Batch Loss: 0.0028614113107323647\n",
      "Epoch 4123, Loss: 0.0030931782093830407, Final Batch Loss: 0.0024029091000556946\n",
      "Epoch 4124, Loss: 0.004894479410722852, Final Batch Loss: 0.0028926499653607607\n",
      "Epoch 4125, Loss: 0.042035245802253485, Final Batch Loss: 0.039991434663534164\n",
      "Epoch 4126, Loss: 0.05361924134194851, Final Batch Loss: 0.02120254747569561\n",
      "Epoch 4127, Loss: 0.09995253011584282, Final Batch Loss: 0.05199114605784416\n",
      "Epoch 4128, Loss: 0.016696791863068938, Final Batch Loss: 0.002646462759003043\n",
      "Epoch 4129, Loss: 0.10059819463640451, Final Batch Loss: 0.0869155079126358\n",
      "Epoch 4130, Loss: 0.04356402298435569, Final Batch Loss: 0.040764790028333664\n",
      "Epoch 4131, Loss: 0.10391546040773392, Final Batch Loss: 0.04777289927005768\n",
      "Epoch 4132, Loss: 0.04304120736196637, Final Batch Loss: 0.0066155423410236835\n",
      "Epoch 4133, Loss: 0.07815427333116531, Final Batch Loss: 0.04779288172721863\n",
      "Epoch 4134, Loss: 0.030335810966789722, Final Batch Loss: 0.011683831922709942\n",
      "Epoch 4135, Loss: 0.04589967289939523, Final Batch Loss: 0.002565565053373575\n",
      "Epoch 4136, Loss: 0.025972978677600622, Final Batch Loss: 0.021840840578079224\n",
      "Epoch 4137, Loss: 0.021240501664578915, Final Batch Loss: 0.006073763594031334\n",
      "Epoch 4138, Loss: 0.08059314452111721, Final Batch Loss: 0.029789751395583153\n",
      "Epoch 4139, Loss: 0.0515979309566319, Final Batch Loss: 0.004850280005484819\n",
      "Epoch 4140, Loss: 0.09944140166044235, Final Batch Loss: 0.02558460831642151\n",
      "Epoch 4141, Loss: 0.020374535699374974, Final Batch Loss: 0.0012099399464204907\n",
      "Epoch 4142, Loss: 0.003754801698960364, Final Batch Loss: 0.0020200982689857483\n",
      "Epoch 4143, Loss: 0.0479124691337347, Final Batch Loss: 0.035175323486328125\n",
      "Epoch 4144, Loss: 0.015977355185896158, Final Batch Loss: 0.005212150048464537\n",
      "Epoch 4145, Loss: 0.21095842495560646, Final Batch Loss: 0.1600475162267685\n",
      "Epoch 4146, Loss: 0.03221839293837547, Final Batch Loss: 0.023647582158446312\n",
      "Epoch 4147, Loss: 0.060881996527314186, Final Batch Loss: 0.02349627949297428\n",
      "Epoch 4148, Loss: 0.10553574189543724, Final Batch Loss: 0.08156521618366241\n",
      "Epoch 4149, Loss: 0.014004682190716267, Final Batch Loss: 0.00883942935615778\n",
      "Epoch 4150, Loss: 0.01789923384785652, Final Batch Loss: 0.008729534223675728\n",
      "Epoch 4151, Loss: 0.08079947927035391, Final Batch Loss: 0.07900607585906982\n",
      "Epoch 4152, Loss: 0.06101364269852638, Final Batch Loss: 0.03450118377804756\n",
      "Epoch 4153, Loss: 0.04910043766722083, Final Batch Loss: 0.04129745066165924\n",
      "Epoch 4154, Loss: 0.04281536303460598, Final Batch Loss: 0.023607192561030388\n",
      "Epoch 4155, Loss: 0.08108180947601795, Final Batch Loss: 0.06042543798685074\n",
      "Epoch 4156, Loss: 0.07305426604580134, Final Batch Loss: 0.0014761135680601\n",
      "Epoch 4157, Loss: 0.006372998235747218, Final Batch Loss: 0.0017837558407336473\n",
      "Epoch 4158, Loss: 0.1414320096373558, Final Batch Loss: 0.08239328116178513\n",
      "Epoch 4159, Loss: 0.0703014638274908, Final Batch Loss: 0.01686141826212406\n",
      "Epoch 4160, Loss: 0.025017986074090004, Final Batch Loss: 0.0023249033838510513\n",
      "Epoch 4161, Loss: 0.057839302346110344, Final Batch Loss: 0.03932863473892212\n",
      "Epoch 4162, Loss: 0.041718258522450924, Final Batch Loss: 0.01062306109815836\n",
      "Epoch 4163, Loss: 0.023409954737871885, Final Batch Loss: 0.003865912090986967\n",
      "Epoch 4164, Loss: 0.034288257360458374, Final Batch Loss: 0.02503412589430809\n",
      "Epoch 4165, Loss: 0.056544726714491844, Final Batch Loss: 0.05526183173060417\n",
      "Epoch 4166, Loss: 0.011377286864444613, Final Batch Loss: 0.008580049499869347\n",
      "Epoch 4167, Loss: 0.013869387563318014, Final Batch Loss: 0.007725565694272518\n",
      "Epoch 4168, Loss: 0.0037775550736114383, Final Batch Loss: 0.00189419265370816\n",
      "Epoch 4169, Loss: 0.02384946937672794, Final Batch Loss: 0.020130807533860207\n",
      "Epoch 4170, Loss: 0.017016201745718718, Final Batch Loss: 0.0053007532842457294\n",
      "Epoch 4171, Loss: 0.017005198635160923, Final Batch Loss: 0.004229462705552578\n",
      "Epoch 4172, Loss: 0.0073270893190056086, Final Batch Loss: 0.004178876522928476\n",
      "Epoch 4173, Loss: 0.013164845295250416, Final Batch Loss: 0.0048621948808431625\n",
      "Epoch 4174, Loss: 0.009360582800582051, Final Batch Loss: 0.003888242645189166\n",
      "Epoch 4175, Loss: 0.01630630949512124, Final Batch Loss: 0.0060855853371322155\n",
      "Epoch 4176, Loss: 0.004824992618523538, Final Batch Loss: 0.003015964524820447\n",
      "Epoch 4177, Loss: 0.025613649864681065, Final Batch Loss: 0.0008459883974865079\n",
      "Epoch 4178, Loss: 0.006708452478051186, Final Batch Loss: 0.004348010290414095\n",
      "Epoch 4179, Loss: 0.07021701149642467, Final Batch Loss: 0.046185679733753204\n",
      "Epoch 4180, Loss: 0.017658630153164268, Final Batch Loss: 0.0034565057139843702\n",
      "Epoch 4181, Loss: 0.004465972306206822, Final Batch Loss: 0.002957142423838377\n",
      "Epoch 4182, Loss: 0.04374610958620906, Final Batch Loss: 0.0408160574734211\n",
      "Epoch 4183, Loss: 0.01654217322356999, Final Batch Loss: 0.013989932835102081\n",
      "Epoch 4184, Loss: 0.0352131319232285, Final Batch Loss: 0.03090699017047882\n",
      "Epoch 4185, Loss: 0.019763032905757427, Final Batch Loss: 0.006943275220692158\n",
      "Epoch 4186, Loss: 0.01854791259393096, Final Batch Loss: 0.015389307402074337\n",
      "Epoch 4187, Loss: 0.03840395621955395, Final Batch Loss: 0.019530775025486946\n",
      "Epoch 4188, Loss: 0.006857838248834014, Final Batch Loss: 0.0033039238769561052\n",
      "Epoch 4189, Loss: 0.024066177662461996, Final Batch Loss: 0.01937127485871315\n",
      "Epoch 4190, Loss: 0.021690213587135077, Final Batch Loss: 0.018465779721736908\n",
      "Epoch 4191, Loss: 0.08451016526669264, Final Batch Loss: 0.08035427331924438\n",
      "Epoch 4192, Loss: 0.008837851462885737, Final Batch Loss: 0.006544254720211029\n",
      "Epoch 4193, Loss: 0.03621401684358716, Final Batch Loss: 0.005482685286551714\n",
      "Epoch 4194, Loss: 0.004854702157899737, Final Batch Loss: 0.0022775970865041018\n",
      "Epoch 4195, Loss: 0.020192553522065282, Final Batch Loss: 0.003511131973937154\n",
      "Epoch 4196, Loss: 0.02158976998180151, Final Batch Loss: 0.011828728951513767\n",
      "Epoch 4197, Loss: 0.023791381157934666, Final Batch Loss: 0.018727988004684448\n",
      "Epoch 4198, Loss: 0.009491258766502142, Final Batch Loss: 0.0032348018139600754\n",
      "Epoch 4199, Loss: 0.014414402889087796, Final Batch Loss: 0.010679857805371284\n",
      "Epoch 4200, Loss: 0.0075657006818801165, Final Batch Loss: 0.004094655159860849\n",
      "Epoch 4201, Loss: 0.005728808464482427, Final Batch Loss: 0.00319442106410861\n",
      "Epoch 4202, Loss: 0.024577671661973, Final Batch Loss: 0.002493562176823616\n",
      "Epoch 4203, Loss: 0.023242610041052103, Final Batch Loss: 0.0022768662311136723\n",
      "Epoch 4204, Loss: 0.039395605912432075, Final Batch Loss: 0.036961499601602554\n",
      "Epoch 4205, Loss: 0.018545621540397406, Final Batch Loss: 0.014661431312561035\n",
      "Epoch 4206, Loss: 0.010997464880347252, Final Batch Loss: 0.00649022264406085\n",
      "Epoch 4207, Loss: 0.014092959929257631, Final Batch Loss: 0.002264877315610647\n",
      "Epoch 4208, Loss: 0.0047855619341135025, Final Batch Loss: 0.0037027981597930193\n",
      "Epoch 4209, Loss: 0.013727058656513691, Final Batch Loss: 0.010433832183480263\n",
      "Epoch 4210, Loss: 0.005575543502345681, Final Batch Loss: 0.0027690082788467407\n",
      "Epoch 4211, Loss: 0.05028924159705639, Final Batch Loss: 0.04490635171532631\n",
      "Epoch 4212, Loss: 0.007393331499770284, Final Batch Loss: 0.0037646477576345205\n",
      "Epoch 4213, Loss: 0.006824268726631999, Final Batch Loss: 0.0031167659908533096\n",
      "Epoch 4214, Loss: 0.01914831716567278, Final Batch Loss: 0.017711542546749115\n",
      "Epoch 4215, Loss: 0.014584341552108526, Final Batch Loss: 0.004892748314887285\n",
      "Epoch 4216, Loss: 0.006527365650981665, Final Batch Loss: 0.0035397913306951523\n",
      "Epoch 4217, Loss: 0.004562323912978172, Final Batch Loss: 0.00120437890291214\n",
      "Epoch 4218, Loss: 0.0034782185684889555, Final Batch Loss: 0.0005860575474798679\n",
      "Epoch 4219, Loss: 0.012230095453560352, Final Batch Loss: 0.002920733764767647\n",
      "Epoch 4220, Loss: 0.009860046207904816, Final Batch Loss: 0.0018441332504153252\n",
      "Epoch 4221, Loss: 0.006382859661243856, Final Batch Loss: 0.0010342708555981517\n",
      "Epoch 4222, Loss: 0.007283553713932633, Final Batch Loss: 0.0034149116836488247\n",
      "Epoch 4223, Loss: 0.00703152921050787, Final Batch Loss: 0.00466202525421977\n",
      "Epoch 4224, Loss: 0.0033845292637124658, Final Batch Loss: 0.002370049012824893\n",
      "Epoch 4225, Loss: 0.003584853606298566, Final Batch Loss: 0.00095698656514287\n",
      "Epoch 4226, Loss: 0.014907157397828996, Final Batch Loss: 0.0132530452683568\n",
      "Epoch 4227, Loss: 0.0034709214232861996, Final Batch Loss: 0.0023084699641913176\n",
      "Epoch 4228, Loss: 0.008958545397035778, Final Batch Loss: 0.0017782304203137755\n",
      "Epoch 4229, Loss: 0.003245449741370976, Final Batch Loss: 0.0022251668851822615\n",
      "Epoch 4230, Loss: 0.008265973068773746, Final Batch Loss: 0.004256573040038347\n",
      "Epoch 4231, Loss: 0.016159696504473686, Final Batch Loss: 0.012375143356621265\n",
      "Epoch 4232, Loss: 0.005011619534343481, Final Batch Loss: 0.003021923126652837\n",
      "Epoch 4233, Loss: 0.009899957338348031, Final Batch Loss: 0.0007699534762650728\n",
      "Epoch 4234, Loss: 0.0071219520177692175, Final Batch Loss: 0.0037060671020299196\n",
      "Epoch 4235, Loss: 0.03418139414861798, Final Batch Loss: 0.004822347778826952\n",
      "Epoch 4236, Loss: 0.004530543345026672, Final Batch Loss: 0.0026375490706413984\n",
      "Epoch 4237, Loss: 0.00505592068657279, Final Batch Loss: 0.0021118035074323416\n",
      "Epoch 4238, Loss: 0.011419251561164856, Final Batch Loss: 0.001980484463274479\n",
      "Epoch 4239, Loss: 0.0038085985579527915, Final Batch Loss: 0.0005777180776931345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4240, Loss: 0.005629419814795256, Final Batch Loss: 0.0024921572767198086\n",
      "Epoch 4241, Loss: 0.0028311924543231726, Final Batch Loss: 0.001385325682349503\n",
      "Epoch 4242, Loss: 0.010511750937439501, Final Batch Loss: 0.0013431893894448876\n",
      "Epoch 4243, Loss: 0.06917468924075365, Final Batch Loss: 0.06296559423208237\n",
      "Epoch 4244, Loss: 0.022388785146176815, Final Batch Loss: 0.01279218215495348\n",
      "Epoch 4245, Loss: 0.004216576926410198, Final Batch Loss: 0.0018755760975182056\n",
      "Epoch 4246, Loss: 0.006732186069712043, Final Batch Loss: 0.0024924466852098703\n",
      "Epoch 4247, Loss: 0.0037453522672876716, Final Batch Loss: 0.002199774142354727\n",
      "Epoch 4248, Loss: 0.00447794480714947, Final Batch Loss: 0.0014018806396052241\n",
      "Epoch 4249, Loss: 0.0030877836979925632, Final Batch Loss: 0.0018582426710054278\n",
      "Epoch 4250, Loss: 0.007871244219131768, Final Batch Loss: 0.0015555127756670117\n",
      "Epoch 4251, Loss: 0.010955489706248045, Final Batch Loss: 0.005430849734693766\n",
      "Epoch 4252, Loss: 0.006325703579932451, Final Batch Loss: 0.0019342885352671146\n",
      "Epoch 4253, Loss: 0.007395023596473038, Final Batch Loss: 0.0008023871341720223\n",
      "Epoch 4254, Loss: 0.0036576869897544384, Final Batch Loss: 0.0026619445998221636\n",
      "Epoch 4255, Loss: 0.004937191377393901, Final Batch Loss: 0.0032053934410214424\n",
      "Epoch 4256, Loss: 0.0025492611748632044, Final Batch Loss: 0.0004510135331656784\n",
      "Epoch 4257, Loss: 0.0027921819128096104, Final Batch Loss: 0.0014009206788614392\n",
      "Epoch 4258, Loss: 0.010020551504567266, Final Batch Loss: 0.0017790424171835184\n",
      "Epoch 4259, Loss: 0.00897422048728913, Final Batch Loss: 0.0014410641742870212\n",
      "Epoch 4260, Loss: 0.0021715507027693093, Final Batch Loss: 0.00043400650611147285\n",
      "Epoch 4261, Loss: 0.0031383891473524272, Final Batch Loss: 0.0008022363181225955\n",
      "Epoch 4262, Loss: 0.02358562801964581, Final Batch Loss: 0.020053325220942497\n",
      "Epoch 4263, Loss: 0.049154849257320166, Final Batch Loss: 0.044316623359918594\n",
      "Epoch 4264, Loss: 0.0038306425558403134, Final Batch Loss: 0.0024042290169745684\n",
      "Epoch 4265, Loss: 0.010413013864308596, Final Batch Loss: 0.00812133401632309\n",
      "Epoch 4266, Loss: 0.009136140928603709, Final Batch Loss: 0.0011168398195877671\n",
      "Epoch 4267, Loss: 0.0022433253470808268, Final Batch Loss: 0.0010933788726106286\n",
      "Epoch 4268, Loss: 0.005246183951385319, Final Batch Loss: 0.004561031237244606\n",
      "Epoch 4269, Loss: 0.0027336066123098135, Final Batch Loss: 0.001260666991584003\n",
      "Epoch 4270, Loss: 0.005066418438218534, Final Batch Loss: 0.0038350799586623907\n",
      "Epoch 4271, Loss: 0.0033085928298532963, Final Batch Loss: 0.0013885253574699163\n",
      "Epoch 4272, Loss: 0.00252148718573153, Final Batch Loss: 0.001391857280395925\n",
      "Epoch 4273, Loss: 0.0077436931896954775, Final Batch Loss: 0.002308413153514266\n",
      "Epoch 4274, Loss: 0.006496014888398349, Final Batch Loss: 0.0012277247151359916\n",
      "Epoch 4275, Loss: 0.004942524712532759, Final Batch Loss: 0.0009830938652157784\n",
      "Epoch 4276, Loss: 0.025495850713923573, Final Batch Loss: 0.023222442716360092\n",
      "Epoch 4277, Loss: 0.004041086416691542, Final Batch Loss: 0.002194350818172097\n",
      "Epoch 4278, Loss: 0.012495602248236537, Final Batch Loss: 0.001638523070141673\n",
      "Epoch 4279, Loss: 0.009851197595708072, Final Batch Loss: 0.008505064062774181\n",
      "Epoch 4280, Loss: 0.005491208634339273, Final Batch Loss: 0.001092478516511619\n",
      "Epoch 4281, Loss: 0.0040412647649645805, Final Batch Loss: 0.001602238742634654\n",
      "Epoch 4282, Loss: 0.003830718225799501, Final Batch Loss: 0.001286774524487555\n",
      "Epoch 4283, Loss: 0.0053650542395189404, Final Batch Loss: 0.003681139787659049\n",
      "Epoch 4284, Loss: 0.026424968149513006, Final Batch Loss: 0.022266240790486336\n",
      "Epoch 4285, Loss: 0.0020153819932602346, Final Batch Loss: 0.0006385138840414584\n",
      "Epoch 4286, Loss: 0.0027355068596079946, Final Batch Loss: 0.0013457753229886293\n",
      "Epoch 4287, Loss: 0.022491312469355762, Final Batch Loss: 0.020733065903186798\n",
      "Epoch 4288, Loss: 0.02228469110559672, Final Batch Loss: 0.020495887845754623\n",
      "Epoch 4289, Loss: 0.004145940882153809, Final Batch Loss: 0.0030072717927396297\n",
      "Epoch 4290, Loss: 0.033831220120191574, Final Batch Loss: 0.022733211517333984\n",
      "Epoch 4291, Loss: 0.01751778507605195, Final Batch Loss: 0.009868524968624115\n",
      "Epoch 4292, Loss: 0.005950746824964881, Final Batch Loss: 0.0026519394014030695\n",
      "Epoch 4293, Loss: 0.00719372509047389, Final Batch Loss: 0.002149808220565319\n",
      "Epoch 4294, Loss: 0.003760540275834501, Final Batch Loss: 0.001490990282036364\n",
      "Epoch 4295, Loss: 0.011290809605270624, Final Batch Loss: 0.004857379477471113\n",
      "Epoch 4296, Loss: 0.0026087944861501455, Final Batch Loss: 0.0014503390993922949\n",
      "Epoch 4297, Loss: 0.01589828636497259, Final Batch Loss: 0.009405262768268585\n",
      "Epoch 4298, Loss: 0.007769855437800288, Final Batch Loss: 0.0051972754299640656\n",
      "Epoch 4299, Loss: 0.002111846872139722, Final Batch Loss: 0.001417944091372192\n",
      "Epoch 4300, Loss: 0.003840594901703298, Final Batch Loss: 0.002389167668297887\n",
      "Epoch 4301, Loss: 0.006419985322281718, Final Batch Loss: 0.001944300951436162\n",
      "Epoch 4302, Loss: 0.010607364703901112, Final Batch Loss: 0.009353888221085072\n",
      "Epoch 4303, Loss: 0.002884338144212961, Final Batch Loss: 0.0016216406365856528\n",
      "Epoch 4304, Loss: 0.005130314209964126, Final Batch Loss: 0.0005131912766955793\n",
      "Epoch 4305, Loss: 0.002824839437380433, Final Batch Loss: 0.001145555404946208\n",
      "Epoch 4306, Loss: 0.009860745863988996, Final Batch Loss: 0.0077828457579016685\n",
      "Epoch 4307, Loss: 0.004777321417350322, Final Batch Loss: 0.0007543012616224587\n",
      "Epoch 4308, Loss: 0.01022534305229783, Final Batch Loss: 0.002409887034446001\n",
      "Epoch 4309, Loss: 0.010023590992204845, Final Batch Loss: 0.009402597323060036\n",
      "Epoch 4310, Loss: 0.0015386365703307092, Final Batch Loss: 0.0006156559684313834\n",
      "Epoch 4311, Loss: 0.006919128820300102, Final Batch Loss: 0.00493221078068018\n",
      "Epoch 4312, Loss: 0.0018905175966210663, Final Batch Loss: 0.0010109248105436563\n",
      "Epoch 4313, Loss: 0.0035196985118091106, Final Batch Loss: 0.0015112406108528376\n",
      "Epoch 4314, Loss: 0.0016228202730417252, Final Batch Loss: 0.0005437845829874277\n",
      "Epoch 4315, Loss: 0.001491927876486443, Final Batch Loss: 0.00022568424174096435\n",
      "Epoch 4316, Loss: 0.0023375811288133264, Final Batch Loss: 0.0011416581692174077\n",
      "Epoch 4317, Loss: 0.007302020909264684, Final Batch Loss: 0.004874630365520716\n",
      "Epoch 4318, Loss: 0.005135538580361754, Final Batch Loss: 0.0009139688336290419\n",
      "Epoch 4319, Loss: 0.013431318569928408, Final Batch Loss: 0.0057554589584469795\n",
      "Epoch 4320, Loss: 0.0034142628428526223, Final Batch Loss: 0.002712829038500786\n",
      "Epoch 4321, Loss: 0.002908774884417653, Final Batch Loss: 0.001569411251693964\n",
      "Epoch 4322, Loss: 0.0035987478913739324, Final Batch Loss: 0.002645990112796426\n",
      "Epoch 4323, Loss: 0.015126285492442548, Final Batch Loss: 0.013692673295736313\n",
      "Epoch 4324, Loss: 0.06971084629185498, Final Batch Loss: 0.06736914813518524\n",
      "Epoch 4325, Loss: 0.004411656409502029, Final Batch Loss: 0.000841168686747551\n",
      "Epoch 4326, Loss: 0.022212128387764096, Final Batch Loss: 0.0011533827055245638\n",
      "Epoch 4327, Loss: 0.006699261954054236, Final Batch Loss: 0.001342168776318431\n",
      "Epoch 4328, Loss: 0.0018467358313500881, Final Batch Loss: 0.0010225186124444008\n",
      "Epoch 4329, Loss: 0.015239060507155955, Final Batch Loss: 0.0018661912763491273\n",
      "Epoch 4330, Loss: 0.015153020969592035, Final Batch Loss: 0.013610954396426678\n",
      "Epoch 4331, Loss: 0.0032669546781107783, Final Batch Loss: 0.001037083682604134\n",
      "Epoch 4332, Loss: 0.010238946182653308, Final Batch Loss: 0.008160754106938839\n",
      "Epoch 4333, Loss: 0.012372772675007582, Final Batch Loss: 0.008277608081698418\n",
      "Epoch 4334, Loss: 0.004633754258975387, Final Batch Loss: 0.001702088862657547\n",
      "Epoch 4335, Loss: 0.0017898743972182274, Final Batch Loss: 0.0009832089999690652\n",
      "Epoch 4336, Loss: 0.00944259949028492, Final Batch Loss: 0.004731330554932356\n",
      "Epoch 4337, Loss: 0.05169723276048899, Final Batch Loss: 0.050532449036836624\n",
      "Epoch 4338, Loss: 0.014130865456536412, Final Batch Loss: 0.012134363874793053\n",
      "Epoch 4339, Loss: 0.009117957670241594, Final Batch Loss: 0.004223344847559929\n",
      "Epoch 4340, Loss: 0.0033632396953180432, Final Batch Loss: 0.0013798855943605304\n",
      "Epoch 4341, Loss: 0.01125480281189084, Final Batch Loss: 0.007286095526069403\n",
      "Epoch 4342, Loss: 0.050103188725188375, Final Batch Loss: 0.0012804840225726366\n",
      "Epoch 4343, Loss: 0.02704312652349472, Final Batch Loss: 0.008540580049157143\n",
      "Epoch 4344, Loss: 0.00918466251459904, Final Batch Loss: 0.008869785815477371\n",
      "Epoch 4345, Loss: 0.027368835173547268, Final Batch Loss: 0.004319666884839535\n",
      "Epoch 4346, Loss: 0.011266295681707561, Final Batch Loss: 0.009697021916508675\n",
      "Epoch 4347, Loss: 0.011643244186416268, Final Batch Loss: 0.0021188140381127596\n",
      "Epoch 4348, Loss: 0.00795118068344891, Final Batch Loss: 0.0030277117621153593\n",
      "Epoch 4349, Loss: 0.02680247277021408, Final Batch Loss: 0.019021805375814438\n",
      "Epoch 4350, Loss: 0.00877684960141778, Final Batch Loss: 0.0072382367216050625\n",
      "Epoch 4351, Loss: 0.003136388084385544, Final Batch Loss: 0.0007257601828314364\n",
      "Epoch 4352, Loss: 0.007213470991700888, Final Batch Loss: 0.0013169832527637482\n",
      "Epoch 4353, Loss: 0.048301829025149345, Final Batch Loss: 0.021507928147912025\n",
      "Epoch 4354, Loss: 0.00156903185416013, Final Batch Loss: 0.0007994739571586251\n",
      "Epoch 4355, Loss: 0.04552655084989965, Final Batch Loss: 0.04173291102051735\n",
      "Epoch 4356, Loss: 0.011874549207277596, Final Batch Loss: 0.0018765461863949895\n",
      "Epoch 4357, Loss: 0.004798087873496115, Final Batch Loss: 0.0016697078244760633\n",
      "Epoch 4358, Loss: 0.011736395303159952, Final Batch Loss: 0.007758063729852438\n",
      "Epoch 4359, Loss: 0.004621141997631639, Final Batch Loss: 0.0009525836794637144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4360, Loss: 0.027523256139829755, Final Batch Loss: 0.02573637291789055\n",
      "Epoch 4361, Loss: 0.0034768751356750727, Final Batch Loss: 0.002237636363133788\n",
      "Epoch 4362, Loss: 0.0026613475056365132, Final Batch Loss: 0.001875294023193419\n",
      "Epoch 4363, Loss: 0.005260229809209704, Final Batch Loss: 0.00315836351364851\n",
      "Epoch 4364, Loss: 0.00636137614492327, Final Batch Loss: 0.0018287665443494916\n",
      "Epoch 4365, Loss: 0.0021303470130078495, Final Batch Loss: 0.0012141569750383496\n",
      "Epoch 4366, Loss: 0.005190330906771123, Final Batch Loss: 0.001903858152218163\n",
      "Epoch 4367, Loss: 0.005372949060983956, Final Batch Loss: 0.0007015076698735356\n",
      "Epoch 4368, Loss: 0.0048031898913905025, Final Batch Loss: 0.0006386887980625033\n",
      "Epoch 4369, Loss: 0.02722827158868313, Final Batch Loss: 0.006408706307411194\n",
      "Epoch 4370, Loss: 0.0017978550749830902, Final Batch Loss: 0.0009396641980856657\n",
      "Epoch 4371, Loss: 0.003706450865138322, Final Batch Loss: 0.0008153298986144364\n",
      "Epoch 4372, Loss: 0.0028506546514108777, Final Batch Loss: 0.0010726901236921549\n",
      "Epoch 4373, Loss: 0.004141030600294471, Final Batch Loss: 0.002951339352875948\n",
      "Epoch 4374, Loss: 0.010156588745303452, Final Batch Loss: 0.00899113342165947\n",
      "Epoch 4375, Loss: 0.005341887677786872, Final Batch Loss: 0.004910193849354982\n",
      "Epoch 4376, Loss: 0.009093813830986619, Final Batch Loss: 0.0027699132915586233\n",
      "Epoch 4377, Loss: 0.0027711481670849025, Final Batch Loss: 0.0009520036983303726\n",
      "Epoch 4378, Loss: 0.008005335868801922, Final Batch Loss: 0.007400345988571644\n",
      "Epoch 4379, Loss: 0.0039978622226044536, Final Batch Loss: 0.0016487574903294444\n",
      "Epoch 4380, Loss: 0.008685862994752824, Final Batch Loss: 0.007212718483060598\n",
      "Epoch 4381, Loss: 0.006076771533116698, Final Batch Loss: 0.0019984885584563017\n",
      "Epoch 4382, Loss: 0.007196104968897998, Final Batch Loss: 0.0005460114916786551\n",
      "Epoch 4383, Loss: 0.004396908450871706, Final Batch Loss: 0.0026027888525277376\n",
      "Epoch 4384, Loss: 0.015817266423255205, Final Batch Loss: 0.01145061943680048\n",
      "Epoch 4385, Loss: 0.014640723820775747, Final Batch Loss: 0.01154138520359993\n",
      "Epoch 4386, Loss: 0.006152836722321808, Final Batch Loss: 0.0008624644251540303\n",
      "Epoch 4387, Loss: 0.0018948857905343175, Final Batch Loss: 0.0006936384597793221\n",
      "Epoch 4388, Loss: 0.0050086467526853085, Final Batch Loss: 0.002283428329974413\n",
      "Epoch 4389, Loss: 0.0037164106033742428, Final Batch Loss: 0.0009305712301284075\n",
      "Epoch 4390, Loss: 0.005113482708111405, Final Batch Loss: 0.002724897814914584\n",
      "Epoch 4391, Loss: 0.0067282457603141665, Final Batch Loss: 0.0018243686063215137\n",
      "Epoch 4392, Loss: 0.008199341769795865, Final Batch Loss: 0.0007976528140716255\n",
      "Epoch 4393, Loss: 0.016114105936139822, Final Batch Loss: 0.01447329856455326\n",
      "Epoch 4394, Loss: 0.010254372842609882, Final Batch Loss: 0.0042589944787323475\n",
      "Epoch 4395, Loss: 0.003100298228673637, Final Batch Loss: 0.0010187089210376143\n",
      "Epoch 4396, Loss: 0.0021335318742785603, Final Batch Loss: 0.0003642662486527115\n",
      "Epoch 4397, Loss: 0.03542069357354194, Final Batch Loss: 0.03472547605633736\n",
      "Epoch 4398, Loss: 0.0032241992885246873, Final Batch Loss: 0.0010731661459431052\n",
      "Epoch 4399, Loss: 0.003860116674331948, Final Batch Loss: 0.0034116189926862717\n",
      "Epoch 4400, Loss: 0.005237344186753035, Final Batch Loss: 0.0031329805497080088\n",
      "Epoch 4401, Loss: 0.003062262840103358, Final Batch Loss: 0.0022470944095402956\n",
      "Epoch 4402, Loss: 0.012832647771574557, Final Batch Loss: 0.012263171374797821\n",
      "Epoch 4403, Loss: 0.011329100700095296, Final Batch Loss: 0.003543468425050378\n",
      "Epoch 4404, Loss: 0.016587511636316776, Final Batch Loss: 0.006076863035559654\n",
      "Epoch 4405, Loss: 0.00770362582989037, Final Batch Loss: 0.0012306685093790293\n",
      "Epoch 4406, Loss: 0.001527156971860677, Final Batch Loss: 0.0008617244311608374\n",
      "Epoch 4407, Loss: 0.006671570241451263, Final Batch Loss: 0.002589407842606306\n",
      "Epoch 4408, Loss: 0.002268729149363935, Final Batch Loss: 0.001730440417304635\n",
      "Epoch 4409, Loss: 0.0025685167056508362, Final Batch Loss: 0.0017953770002350211\n",
      "Epoch 4410, Loss: 0.004411752335727215, Final Batch Loss: 0.0019693190697580576\n",
      "Epoch 4411, Loss: 0.004591206496115774, Final Batch Loss: 0.00057323178043589\n",
      "Epoch 4412, Loss: 0.0029285000637173653, Final Batch Loss: 0.001285705016925931\n",
      "Epoch 4413, Loss: 0.011665951984468848, Final Batch Loss: 0.0005829050787724555\n",
      "Epoch 4414, Loss: 0.020943726412951946, Final Batch Loss: 0.008429004810750484\n",
      "Epoch 4415, Loss: 0.003275062015745789, Final Batch Loss: 0.0024758854415267706\n",
      "Epoch 4416, Loss: 0.00282905378844589, Final Batch Loss: 0.001044100965373218\n",
      "Epoch 4417, Loss: 0.009800716768950224, Final Batch Loss: 0.006573649123311043\n",
      "Epoch 4418, Loss: 0.004360060207545757, Final Batch Loss: 0.003267652355134487\n",
      "Epoch 4419, Loss: 0.0068399494048208, Final Batch Loss: 0.0038050010334700346\n",
      "Epoch 4420, Loss: 0.004900821048067883, Final Batch Loss: 0.0004020575725007802\n",
      "Epoch 4421, Loss: 0.003214085241779685, Final Batch Loss: 0.0017273720586672425\n",
      "Epoch 4422, Loss: 0.001980900764465332, Final Batch Loss: 0.00108558579813689\n",
      "Epoch 4423, Loss: 0.0030135096749290824, Final Batch Loss: 0.0015785919968038797\n",
      "Epoch 4424, Loss: 0.0023353901924565434, Final Batch Loss: 0.0007517486810684204\n",
      "Epoch 4425, Loss: 0.0020371400751173496, Final Batch Loss: 0.0013199351960793138\n",
      "Epoch 4426, Loss: 0.0024254488525912166, Final Batch Loss: 0.0015997026348486543\n",
      "Epoch 4427, Loss: 0.05387871782295406, Final Batch Loss: 0.0016729689668864012\n",
      "Epoch 4428, Loss: 0.003449886804446578, Final Batch Loss: 0.0024541153106838465\n",
      "Epoch 4429, Loss: 0.005409813791629858, Final Batch Loss: 0.0052320989780128\n",
      "Epoch 4430, Loss: 0.02943543391302228, Final Batch Loss: 0.007552406284958124\n",
      "Epoch 4431, Loss: 0.004078006255440414, Final Batch Loss: 0.0021453406661748886\n",
      "Epoch 4432, Loss: 0.06178532255580649, Final Batch Loss: 0.0009243906824849546\n",
      "Epoch 4433, Loss: 0.004807250807061791, Final Batch Loss: 0.0007713243830949068\n",
      "Epoch 4434, Loss: 0.01579594984650612, Final Batch Loss: 0.014116518199443817\n",
      "Epoch 4435, Loss: 0.024596979608759284, Final Batch Loss: 0.002788936486467719\n",
      "Epoch 4436, Loss: 0.003644253476522863, Final Batch Loss: 0.002147937659174204\n",
      "Epoch 4437, Loss: 0.0029303464107215405, Final Batch Loss: 0.001728559611365199\n",
      "Epoch 4438, Loss: 0.006404470885172486, Final Batch Loss: 0.00042033917270600796\n",
      "Epoch 4439, Loss: 0.02108016615966335, Final Batch Loss: 0.0008876659558154643\n",
      "Epoch 4440, Loss: 0.0035962335823569447, Final Batch Loss: 0.00048076562234200537\n",
      "Epoch 4441, Loss: 0.0021525201154872775, Final Batch Loss: 0.0013476317981258035\n",
      "Epoch 4442, Loss: 0.0085303895175457, Final Batch Loss: 0.004971882328391075\n",
      "Epoch 4443, Loss: 0.0064811111660674214, Final Batch Loss: 0.005035515874624252\n",
      "Epoch 4444, Loss: 0.030592220835387707, Final Batch Loss: 0.005905588157474995\n",
      "Epoch 4445, Loss: 0.0074057161109521985, Final Batch Loss: 0.001016834401525557\n",
      "Epoch 4446, Loss: 0.02534195128828287, Final Batch Loss: 0.005179890431463718\n",
      "Epoch 4447, Loss: 0.004904401837848127, Final Batch Loss: 0.0015760889509692788\n",
      "Epoch 4448, Loss: 0.015529936092207208, Final Batch Loss: 0.0003154686710331589\n",
      "Epoch 4449, Loss: 0.03170936612877995, Final Batch Loss: 0.001330368802882731\n",
      "Epoch 4450, Loss: 0.04357041232287884, Final Batch Loss: 0.019866783171892166\n",
      "Epoch 4451, Loss: 0.00895894120912999, Final Batch Loss: 0.0019426130456849933\n",
      "Epoch 4452, Loss: 0.007351825770456344, Final Batch Loss: 0.0007589508895762265\n",
      "Epoch 4453, Loss: 0.05463813478127122, Final Batch Loss: 0.05253816023468971\n",
      "Epoch 4454, Loss: 0.022811120026744902, Final Batch Loss: 0.001194512122310698\n",
      "Epoch 4455, Loss: 0.011810744239483029, Final Batch Loss: 0.0004655728698708117\n",
      "Epoch 4456, Loss: 0.02076684223720804, Final Batch Loss: 0.019828446209430695\n",
      "Epoch 4457, Loss: 0.001959099085070193, Final Batch Loss: 0.0009997395100072026\n",
      "Epoch 4458, Loss: 0.014053711958695203, Final Batch Loss: 0.013210996054112911\n",
      "Epoch 4459, Loss: 0.003038336057215929, Final Batch Loss: 0.0018082900205627084\n",
      "Epoch 4460, Loss: 0.011191662400960922, Final Batch Loss: 0.0063595641404390335\n",
      "Epoch 4461, Loss: 0.004615643061697483, Final Batch Loss: 0.004272639751434326\n",
      "Epoch 4462, Loss: 0.014681703178212047, Final Batch Loss: 0.0012626766692847013\n",
      "Epoch 4463, Loss: 0.001767785637639463, Final Batch Loss: 0.0007749320939183235\n",
      "Epoch 4464, Loss: 0.002686420571990311, Final Batch Loss: 0.0014910962199792266\n",
      "Epoch 4465, Loss: 0.0629317369312048, Final Batch Loss: 0.037577830255031586\n",
      "Epoch 4466, Loss: 0.016005115117877722, Final Batch Loss: 0.0017723576165735722\n",
      "Epoch 4467, Loss: 0.04022342246025801, Final Batch Loss: 0.011364116333425045\n",
      "Epoch 4468, Loss: 0.014747838024049997, Final Batch Loss: 0.007345349527895451\n",
      "Epoch 4469, Loss: 0.006900446838699281, Final Batch Loss: 0.005766554269939661\n",
      "Epoch 4470, Loss: 0.0058943681651726365, Final Batch Loss: 0.001722736400552094\n",
      "Epoch 4471, Loss: 0.029091164469718933, Final Batch Loss: 0.00608489103615284\n",
      "Epoch 4472, Loss: 0.01247327565215528, Final Batch Loss: 0.0023702874314039946\n",
      "Epoch 4473, Loss: 0.012360894586890936, Final Batch Loss: 0.004977168515324593\n",
      "Epoch 4474, Loss: 0.006483935052528977, Final Batch Loss: 0.0047669424675405025\n",
      "Epoch 4475, Loss: 0.025968274741899222, Final Batch Loss: 0.00068669457687065\n",
      "Epoch 4476, Loss: 0.009597545256838202, Final Batch Loss: 0.001769310561940074\n",
      "Epoch 4477, Loss: 0.0977998304006178, Final Batch Loss: 0.000467360281618312\n",
      "Epoch 4478, Loss: 0.005214201635681093, Final Batch Loss: 0.0037103204522281885\n",
      "Epoch 4479, Loss: 0.0027829284663312137, Final Batch Loss: 0.0020316357258707285\n",
      "Epoch 4480, Loss: 0.017185482662171125, Final Batch Loss: 0.009884905070066452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4481, Loss: 0.01275210059247911, Final Batch Loss: 0.011249157600104809\n",
      "Epoch 4482, Loss: 0.004945660475641489, Final Batch Loss: 0.002627672627568245\n",
      "Epoch 4483, Loss: 0.003828704357147217, Final Batch Loss: 0.0025989802088588476\n",
      "Epoch 4484, Loss: 0.0017317020683549345, Final Batch Loss: 0.0008314821170642972\n",
      "Epoch 4485, Loss: 0.09668353945016861, Final Batch Loss: 0.062464576214551926\n",
      "Epoch 4486, Loss: 0.0213847333798185, Final Batch Loss: 0.02018425613641739\n",
      "Epoch 4487, Loss: 0.017838774947449565, Final Batch Loss: 0.0025973401498049498\n",
      "Epoch 4488, Loss: 0.012104350258596241, Final Batch Loss: 0.00116411701310426\n",
      "Epoch 4489, Loss: 0.025803261436522007, Final Batch Loss: 0.004552139900624752\n",
      "Epoch 4490, Loss: 0.003358485409989953, Final Batch Loss: 0.0010428563691675663\n",
      "Epoch 4491, Loss: 0.019326941575855017, Final Batch Loss: 0.003812000621110201\n",
      "Epoch 4492, Loss: 0.007723934482783079, Final Batch Loss: 0.005179019644856453\n",
      "Epoch 4493, Loss: 0.010510198771953583, Final Batch Loss: 0.0055258446373045444\n",
      "Epoch 4494, Loss: 0.03196116641629487, Final Batch Loss: 0.001610986073501408\n",
      "Epoch 4495, Loss: 0.0150427675107494, Final Batch Loss: 0.013756358064711094\n",
      "Epoch 4496, Loss: 0.037805658066645265, Final Batch Loss: 0.001273442292585969\n",
      "Epoch 4497, Loss: 0.019090892281383276, Final Batch Loss: 0.017065169289708138\n",
      "Epoch 4498, Loss: 0.024083653464913368, Final Batch Loss: 0.006789248436689377\n",
      "Epoch 4499, Loss: 0.009955903282389045, Final Batch Loss: 0.006374556105583906\n",
      "Epoch 4500, Loss: 0.027480012737214565, Final Batch Loss: 0.01743975840508938\n",
      "Epoch 4501, Loss: 0.01613771793199703, Final Batch Loss: 0.0002935069496743381\n",
      "Epoch 4502, Loss: 0.03701256704516709, Final Batch Loss: 0.0013280336279422045\n",
      "Epoch 4503, Loss: 0.0031151333823800087, Final Batch Loss: 0.0013829406816512346\n",
      "Epoch 4504, Loss: 0.09862808138132095, Final Batch Loss: 0.09226591885089874\n",
      "Epoch 4505, Loss: 0.00923305656760931, Final Batch Loss: 0.0056260256096720695\n",
      "Epoch 4506, Loss: 0.20640984922647476, Final Batch Loss: 0.1478927880525589\n",
      "Epoch 4507, Loss: 0.027747812680900097, Final Batch Loss: 0.004767973907291889\n",
      "Epoch 4508, Loss: 0.10416781529784203, Final Batch Loss: 0.059222012758255005\n",
      "Epoch 4509, Loss: 0.06691995821893215, Final Batch Loss: 0.005936319008469582\n",
      "Epoch 4510, Loss: 0.022560288663953543, Final Batch Loss: 0.0050368173979222775\n",
      "Epoch 4511, Loss: 0.027528594713658094, Final Batch Loss: 0.005810791160911322\n",
      "Epoch 4512, Loss: 0.2891833819448948, Final Batch Loss: 0.2579081058502197\n",
      "Epoch 4513, Loss: 0.015344596467912197, Final Batch Loss: 0.008616405539214611\n",
      "Epoch 4514, Loss: 0.2442176267504692, Final Batch Loss: 0.137211874127388\n",
      "Epoch 4515, Loss: 0.02980562439188361, Final Batch Loss: 0.005919880699366331\n",
      "Epoch 4516, Loss: 0.049248768016695976, Final Batch Loss: 0.027555104345083237\n",
      "Epoch 4517, Loss: 0.2986346669495106, Final Batch Loss: 0.2724143862724304\n",
      "Epoch 4518, Loss: 0.03029763838276267, Final Batch Loss: 0.0014138012193143368\n",
      "Epoch 4519, Loss: 0.019195307977497578, Final Batch Loss: 0.010948728770017624\n",
      "Epoch 4520, Loss: 0.21231702342629433, Final Batch Loss: 0.17811928689479828\n",
      "Epoch 4521, Loss: 0.06062560458667576, Final Batch Loss: 0.002689223038032651\n",
      "Epoch 4522, Loss: 0.11669063195586205, Final Batch Loss: 0.04948915168642998\n",
      "Epoch 4523, Loss: 0.07203247025609016, Final Batch Loss: 0.01491519808769226\n",
      "Epoch 4524, Loss: 0.008159829012583941, Final Batch Loss: 0.0008005520212464035\n",
      "Epoch 4525, Loss: 0.06940228119492531, Final Batch Loss: 0.04153012856841087\n",
      "Epoch 4526, Loss: 0.0861838236451149, Final Batch Loss: 0.03589562326669693\n",
      "Epoch 4527, Loss: 0.06902893027290702, Final Batch Loss: 0.006408037152141333\n",
      "Epoch 4528, Loss: 0.010872328421100974, Final Batch Loss: 0.003419986693188548\n",
      "Epoch 4529, Loss: 0.016187738394364715, Final Batch Loss: 0.014075255952775478\n",
      "Epoch 4530, Loss: 0.014845722820609808, Final Batch Loss: 0.011890179477632046\n",
      "Epoch 4531, Loss: 0.04959610756486654, Final Batch Loss: 0.04052742198109627\n",
      "Epoch 4532, Loss: 0.013671552995219827, Final Batch Loss: 0.0034209846053272486\n",
      "Epoch 4533, Loss: 0.010033873375505209, Final Batch Loss: 0.005595474038273096\n",
      "Epoch 4534, Loss: 0.010174511931836605, Final Batch Loss: 0.0008906219154596329\n",
      "Epoch 4535, Loss: 0.007399954134598374, Final Batch Loss: 0.0056681702844798565\n",
      "Epoch 4536, Loss: 0.012532849796116352, Final Batch Loss: 0.004109383560717106\n",
      "Epoch 4537, Loss: 0.01851862110197544, Final Batch Loss: 0.008944930508732796\n",
      "Epoch 4538, Loss: 0.011693200096487999, Final Batch Loss: 0.004674695897847414\n",
      "Epoch 4539, Loss: 0.018967944663017988, Final Batch Loss: 0.006422844249755144\n",
      "Epoch 4540, Loss: 0.00940835615620017, Final Batch Loss: 0.0037786131724715233\n",
      "Epoch 4541, Loss: 0.06794511154294014, Final Batch Loss: 0.012812603265047073\n",
      "Epoch 4542, Loss: 0.013176881708204746, Final Batch Loss: 0.003922753036022186\n",
      "Epoch 4543, Loss: 0.008526722900569439, Final Batch Loss: 0.0029688472859561443\n",
      "Epoch 4544, Loss: 0.0037145372480154037, Final Batch Loss: 0.0028388507198542356\n",
      "Epoch 4545, Loss: 0.01888776198029518, Final Batch Loss: 0.0020598992705345154\n",
      "Epoch 4546, Loss: 0.00877766910707578, Final Batch Loss: 0.0006100707105360925\n",
      "Epoch 4547, Loss: 0.0047769511584192514, Final Batch Loss: 0.003281419165432453\n",
      "Epoch 4548, Loss: 0.006445855833590031, Final Batch Loss: 0.0023024100810289383\n",
      "Epoch 4549, Loss: 0.012790757231414318, Final Batch Loss: 0.004640299826860428\n",
      "Epoch 4550, Loss: 0.02573066484183073, Final Batch Loss: 0.008944428525865078\n",
      "Epoch 4551, Loss: 0.0023908657603897154, Final Batch Loss: 0.0004684264422394335\n",
      "Epoch 4552, Loss: 0.00311165681341663, Final Batch Loss: 0.0008165015024133027\n",
      "Epoch 4553, Loss: 0.004689092515036464, Final Batch Loss: 0.001532452879473567\n",
      "Epoch 4554, Loss: 0.0068734679371118546, Final Batch Loss: 0.001995109487324953\n",
      "Epoch 4555, Loss: 0.011150172678753734, Final Batch Loss: 0.008209088817238808\n",
      "Epoch 4556, Loss: 0.007215739227831364, Final Batch Loss: 0.0036679706536233425\n",
      "Epoch 4557, Loss: 0.00363918743096292, Final Batch Loss: 0.0016137589700520039\n",
      "Epoch 4558, Loss: 0.003515981079544872, Final Batch Loss: 0.0008839393849484622\n",
      "Epoch 4559, Loss: 0.011216110200621188, Final Batch Loss: 0.0011900939280167222\n",
      "Epoch 4560, Loss: 0.00416830217000097, Final Batch Loss: 0.0015089287189766765\n",
      "Epoch 4561, Loss: 0.0149068683385849, Final Batch Loss: 0.008165448904037476\n",
      "Epoch 4562, Loss: 0.0021690501016564667, Final Batch Loss: 0.0007982409442774951\n",
      "Epoch 4563, Loss: 0.004898951854556799, Final Batch Loss: 0.00241286912932992\n",
      "Epoch 4564, Loss: 0.0059193759225308895, Final Batch Loss: 0.003020795062184334\n",
      "Epoch 4565, Loss: 0.012272425461560488, Final Batch Loss: 0.0027585946954786777\n",
      "Epoch 4566, Loss: 0.053119928925298154, Final Batch Loss: 0.0012065073242411017\n",
      "Epoch 4567, Loss: 0.05477850534953177, Final Batch Loss: 0.0032554680947214365\n",
      "Epoch 4568, Loss: 0.010383909102529287, Final Batch Loss: 0.006777009926736355\n",
      "Epoch 4569, Loss: 0.006210162304341793, Final Batch Loss: 0.0028618741780519485\n",
      "Epoch 4570, Loss: 0.027782791759818792, Final Batch Loss: 0.003249614965170622\n",
      "Epoch 4571, Loss: 0.015905762556940317, Final Batch Loss: 0.013206674717366695\n",
      "Epoch 4572, Loss: 0.004013386904262006, Final Batch Loss: 0.001915107131935656\n",
      "Epoch 4573, Loss: 0.02913636714220047, Final Batch Loss: 0.010116130113601685\n",
      "Epoch 4574, Loss: 0.02095871279016137, Final Batch Loss: 0.0160527266561985\n",
      "Epoch 4575, Loss: 0.004767116392031312, Final Batch Loss: 0.0019536656327545643\n",
      "Epoch 4576, Loss: 0.00809245347045362, Final Batch Loss: 0.005175410304218531\n",
      "Epoch 4577, Loss: 0.0058058793656528, Final Batch Loss: 0.002827033633366227\n",
      "Epoch 4578, Loss: 0.007607608917169273, Final Batch Loss: 0.0013870293041691184\n",
      "Epoch 4579, Loss: 0.00599618989508599, Final Batch Loss: 0.001880884519778192\n",
      "Epoch 4580, Loss: 0.01748498296365142, Final Batch Loss: 0.005431136582046747\n",
      "Epoch 4581, Loss: 0.0030744531541131437, Final Batch Loss: 0.0007980884402059019\n",
      "Epoch 4582, Loss: 0.012583580799400806, Final Batch Loss: 0.009546375833451748\n",
      "Epoch 4583, Loss: 0.01688335556536913, Final Batch Loss: 0.00970243476331234\n",
      "Epoch 4584, Loss: 0.02204700466245413, Final Batch Loss: 0.01681709475815296\n",
      "Epoch 4585, Loss: 0.007513621123507619, Final Batch Loss: 0.0027707593981176615\n",
      "Epoch 4586, Loss: 0.010896153515204787, Final Batch Loss: 0.008852599188685417\n",
      "Epoch 4587, Loss: 0.008078054524958134, Final Batch Loss: 0.006770162843167782\n",
      "Epoch 4588, Loss: 0.02568271744530648, Final Batch Loss: 0.0016595701454207301\n",
      "Epoch 4589, Loss: 0.028121553361415863, Final Batch Loss: 0.023614278063178062\n",
      "Epoch 4590, Loss: 0.004152740468271077, Final Batch Loss: 0.0028229288291186094\n",
      "Epoch 4591, Loss: 0.0050150236347690225, Final Batch Loss: 0.0016934912418946624\n",
      "Epoch 4592, Loss: 0.013099204748868942, Final Batch Loss: 0.0027253972366452217\n",
      "Epoch 4593, Loss: 0.006926004076376557, Final Batch Loss: 0.0015872770454734564\n",
      "Epoch 4594, Loss: 0.002966320258565247, Final Batch Loss: 0.0008930539479479194\n",
      "Epoch 4595, Loss: 0.00406692991964519, Final Batch Loss: 0.0027931600343436003\n",
      "Epoch 4596, Loss: 0.015796459279954433, Final Batch Loss: 0.0024609090760350227\n",
      "Epoch 4597, Loss: 0.014185872860252857, Final Batch Loss: 0.007888312451541424\n",
      "Epoch 4598, Loss: 0.015003222972154617, Final Batch Loss: 0.002756909467279911\n",
      "Epoch 4599, Loss: 0.003888049745000899, Final Batch Loss: 0.0018921088194474578\n",
      "Epoch 4600, Loss: 0.005457619903609157, Final Batch Loss: 0.0036514075472950935\n",
      "Epoch 4601, Loss: 0.014906843425706029, Final Batch Loss: 0.0010898637119680643\n",
      "Epoch 4602, Loss: 0.004621729953214526, Final Batch Loss: 0.0032869086135178804\n",
      "Epoch 4603, Loss: 0.009460904635488987, Final Batch Loss: 0.0026279394514858723\n",
      "Epoch 4604, Loss: 0.00406409875722602, Final Batch Loss: 0.000878386024851352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4605, Loss: 0.015819752239622176, Final Batch Loss: 0.001366527401842177\n",
      "Epoch 4606, Loss: 0.007674720836803317, Final Batch Loss: 0.002246404765173793\n",
      "Epoch 4607, Loss: 0.0034628312569111586, Final Batch Loss: 0.001134833786636591\n",
      "Epoch 4608, Loss: 0.003825508989393711, Final Batch Loss: 0.0025350165087729692\n",
      "Epoch 4609, Loss: 0.010104155400767922, Final Batch Loss: 0.0025588537100702524\n",
      "Epoch 4610, Loss: 0.01582816196605563, Final Batch Loss: 0.012564185075461864\n",
      "Epoch 4611, Loss: 0.002353809541091323, Final Batch Loss: 0.000918141333386302\n",
      "Epoch 4612, Loss: 0.0064097834401763976, Final Batch Loss: 0.000965486338827759\n",
      "Epoch 4613, Loss: 0.018858988187275827, Final Batch Loss: 0.001839761040173471\n",
      "Epoch 4614, Loss: 0.0031067655654624104, Final Batch Loss: 0.0010706455213949084\n",
      "Epoch 4615, Loss: 0.016386014816816896, Final Batch Loss: 0.0007394751883111894\n",
      "Epoch 4616, Loss: 0.013769899727776647, Final Batch Loss: 0.011749865487217903\n",
      "Epoch 4617, Loss: 0.028344030492007732, Final Batch Loss: 0.024479255080223083\n",
      "Epoch 4618, Loss: 0.00960668409243226, Final Batch Loss: 0.002178519032895565\n",
      "Epoch 4619, Loss: 0.009845501510426402, Final Batch Loss: 0.008481555618345737\n",
      "Epoch 4620, Loss: 0.004949763882905245, Final Batch Loss: 0.001075552310794592\n",
      "Epoch 4621, Loss: 0.007409540587104857, Final Batch Loss: 0.00125527021009475\n",
      "Epoch 4622, Loss: 0.0018780454993247986, Final Batch Loss: 0.0004423245554789901\n",
      "Epoch 4623, Loss: 0.004841975402086973, Final Batch Loss: 0.0030149249359965324\n",
      "Epoch 4624, Loss: 0.050085253576980904, Final Batch Loss: 0.00040716261719353497\n",
      "Epoch 4625, Loss: 0.0026847950648516417, Final Batch Loss: 0.0007638426031917334\n",
      "Epoch 4626, Loss: 0.011751776095479727, Final Batch Loss: 0.0043119401670992374\n",
      "Epoch 4627, Loss: 0.00451861007604748, Final Batch Loss: 0.0029418838676065207\n",
      "Epoch 4628, Loss: 0.021834233310073614, Final Batch Loss: 0.007579150144010782\n",
      "Epoch 4629, Loss: 0.004647507099434733, Final Batch Loss: 0.0022910742554813623\n",
      "Epoch 4630, Loss: 0.0047737970016896725, Final Batch Loss: 0.002111791865900159\n",
      "Epoch 4631, Loss: 0.023224545642733574, Final Batch Loss: 0.010265182703733444\n",
      "Epoch 4632, Loss: 0.06112530827522278, Final Batch Loss: 0.035842034965753555\n",
      "Epoch 4633, Loss: 0.010608667915221304, Final Batch Loss: 0.009839401580393314\n",
      "Epoch 4634, Loss: 0.0779959037899971, Final Batch Loss: 0.06931202858686447\n",
      "Epoch 4635, Loss: 0.009009144821902737, Final Batch Loss: 0.00046283434494398534\n",
      "Epoch 4636, Loss: 0.004856483661569655, Final Batch Loss: 0.0013996035559102893\n",
      "Epoch 4637, Loss: 0.0024209259427152574, Final Batch Loss: 0.0009664405952207744\n",
      "Epoch 4638, Loss: 0.003619302297011018, Final Batch Loss: 0.0023883057292550802\n",
      "Epoch 4639, Loss: 0.0066144021693617105, Final Batch Loss: 0.003780538449063897\n",
      "Epoch 4640, Loss: 0.006324934656731784, Final Batch Loss: 0.0011450046440586448\n",
      "Epoch 4641, Loss: 0.004673573188483715, Final Batch Loss: 0.0021486561745405197\n",
      "Epoch 4642, Loss: 0.0224264832213521, Final Batch Loss: 0.012224105186760426\n",
      "Epoch 4643, Loss: 0.006372088799253106, Final Batch Loss: 0.0012912817765027285\n",
      "Epoch 4644, Loss: 0.016632420592941344, Final Batch Loss: 0.0011449387529864907\n",
      "Epoch 4645, Loss: 0.0036901552230119705, Final Batch Loss: 0.00226136134006083\n",
      "Epoch 4646, Loss: 0.0029855675529688597, Final Batch Loss: 0.0009008869528770447\n",
      "Epoch 4647, Loss: 0.009453487698920071, Final Batch Loss: 0.0013115165056660771\n",
      "Epoch 4648, Loss: 0.012762455735355616, Final Batch Loss: 0.010535210371017456\n",
      "Epoch 4649, Loss: 0.00682002620305866, Final Batch Loss: 0.001333574648015201\n",
      "Epoch 4650, Loss: 0.004644351778551936, Final Batch Loss: 0.0025790748186409473\n",
      "Epoch 4651, Loss: 0.002470059203915298, Final Batch Loss: 0.0011880036909133196\n",
      "Epoch 4652, Loss: 0.002036683668848127, Final Batch Loss: 0.0008015637868084013\n",
      "Epoch 4653, Loss: 0.00753042520955205, Final Batch Loss: 0.003674889914691448\n",
      "Epoch 4654, Loss: 0.0022972103906795382, Final Batch Loss: 0.0017428011633455753\n",
      "Epoch 4655, Loss: 0.0019914318691007793, Final Batch Loss: 0.0009731887257657945\n",
      "Epoch 4656, Loss: 0.015221923124045134, Final Batch Loss: 0.012650280259549618\n",
      "Epoch 4657, Loss: 0.006995501229539514, Final Batch Loss: 0.004297463223338127\n",
      "Epoch 4658, Loss: 0.005185188434552401, Final Batch Loss: 0.000808554410468787\n",
      "Epoch 4659, Loss: 0.0030329059809446335, Final Batch Loss: 0.0016324871685355902\n",
      "Epoch 4660, Loss: 0.005192507291212678, Final Batch Loss: 0.002877224236726761\n",
      "Epoch 4661, Loss: 0.004752364009618759, Final Batch Loss: 0.002553933532908559\n",
      "Epoch 4662, Loss: 0.020828757958952338, Final Batch Loss: 0.0008307924144901335\n",
      "Epoch 4663, Loss: 0.007834557443857193, Final Batch Loss: 0.001627491321414709\n",
      "Epoch 4664, Loss: 0.01657252013683319, Final Batch Loss: 0.0121475113555789\n",
      "Epoch 4665, Loss: 0.003200811392161995, Final Batch Loss: 0.0022327739279717207\n",
      "Epoch 4666, Loss: 0.01326509437058121, Final Batch Loss: 0.011651197448372841\n",
      "Epoch 4667, Loss: 0.003907246398739517, Final Batch Loss: 0.002252507023513317\n",
      "Epoch 4668, Loss: 0.007097148220054805, Final Batch Loss: 0.001280377502553165\n",
      "Epoch 4669, Loss: 0.002694982453249395, Final Batch Loss: 0.0017124796286225319\n",
      "Epoch 4670, Loss: 0.01353915175423026, Final Batch Loss: 0.00036321813240647316\n",
      "Epoch 4671, Loss: 0.09390642493963242, Final Batch Loss: 0.047238919883966446\n",
      "Epoch 4672, Loss: 0.014109086245298386, Final Batch Loss: 0.007537114899605513\n",
      "Epoch 4673, Loss: 0.01289716106839478, Final Batch Loss: 0.011413585394620895\n",
      "Epoch 4674, Loss: 0.03579398663714528, Final Batch Loss: 0.003341075498610735\n",
      "Epoch 4675, Loss: 0.058887915685772896, Final Batch Loss: 0.049681782722473145\n",
      "Epoch 4676, Loss: 0.03122349875047803, Final Batch Loss: 0.0019531906582415104\n",
      "Epoch 4677, Loss: 0.0058245365507900715, Final Batch Loss: 0.00213805353268981\n",
      "Epoch 4678, Loss: 0.06981908390298486, Final Batch Loss: 0.0644812285900116\n",
      "Epoch 4679, Loss: 0.004888194613158703, Final Batch Loss: 0.0032253293320536613\n",
      "Epoch 4680, Loss: 0.012336462968960404, Final Batch Loss: 0.008677682839334011\n",
      "Epoch 4681, Loss: 0.020696593914180994, Final Batch Loss: 0.0028935004957020283\n",
      "Epoch 4682, Loss: 0.01777763816062361, Final Batch Loss: 0.0013548113638535142\n",
      "Epoch 4683, Loss: 0.004956971388310194, Final Batch Loss: 0.002532664919272065\n",
      "Epoch 4684, Loss: 0.002184625598601997, Final Batch Loss: 0.001098428969271481\n",
      "Epoch 4685, Loss: 0.019311203621327877, Final Batch Loss: 0.0050631677731871605\n",
      "Epoch 4686, Loss: 0.0031064902432262897, Final Batch Loss: 0.0016035656444728374\n",
      "Epoch 4687, Loss: 0.05018667597323656, Final Batch Loss: 0.04429519921541214\n",
      "Epoch 4688, Loss: 0.01613953383639455, Final Batch Loss: 0.00625053932890296\n",
      "Epoch 4689, Loss: 0.019787675701081753, Final Batch Loss: 0.008597862906754017\n",
      "Epoch 4690, Loss: 0.009405900025740266, Final Batch Loss: 0.008021969348192215\n",
      "Epoch 4691, Loss: 0.01280332519672811, Final Batch Loss: 0.011286154389381409\n",
      "Epoch 4692, Loss: 0.019327539019286633, Final Batch Loss: 0.013265029527246952\n",
      "Epoch 4693, Loss: 0.009590951260179281, Final Batch Loss: 0.0044778939336538315\n",
      "Epoch 4694, Loss: 0.0065715613309293985, Final Batch Loss: 0.0053088488057255745\n",
      "Epoch 4695, Loss: 0.008167573832906783, Final Batch Loss: 0.0011794596211984754\n",
      "Epoch 4696, Loss: 0.018655972555279732, Final Batch Loss: 0.00840283278375864\n",
      "Epoch 4697, Loss: 0.0151054747402668, Final Batch Loss: 0.008597324602305889\n",
      "Epoch 4698, Loss: 0.006999202072620392, Final Batch Loss: 0.0021258266642689705\n",
      "Epoch 4699, Loss: 0.003427184303291142, Final Batch Loss: 0.0019143446115776896\n",
      "Epoch 4700, Loss: 0.005957177607342601, Final Batch Loss: 0.0028968907427042723\n",
      "Epoch 4701, Loss: 0.037433821707963943, Final Batch Loss: 0.03326520696282387\n",
      "Epoch 4702, Loss: 0.014403067179955542, Final Batch Loss: 0.0016561975935474038\n",
      "Epoch 4703, Loss: 0.00792430923320353, Final Batch Loss: 0.0027706401888281107\n",
      "Epoch 4704, Loss: 0.0048843828262761235, Final Batch Loss: 0.0031668038573116064\n",
      "Epoch 4705, Loss: 0.013123214826919138, Final Batch Loss: 0.012393715791404247\n",
      "Epoch 4706, Loss: 0.03035245998762548, Final Batch Loss: 0.0011230853851884604\n",
      "Epoch 4707, Loss: 0.04839144088327885, Final Batch Loss: 0.029352838173508644\n",
      "Epoch 4708, Loss: 0.004910581279546022, Final Batch Loss: 0.0025131418369710445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4709, Loss: 0.011886446387507021, Final Batch Loss: 0.010574445128440857\n",
      "Epoch 4710, Loss: 0.05038417223840952, Final Batch Loss: 0.03541187942028046\n",
      "Epoch 4711, Loss: 0.07394225290045142, Final Batch Loss: 0.06966609507799149\n",
      "Epoch 4712, Loss: 0.08452184125781059, Final Batch Loss: 0.04733135551214218\n",
      "Epoch 4713, Loss: 0.008215524721890688, Final Batch Loss: 0.0023216581903398037\n",
      "Epoch 4714, Loss: 0.0027879675617441535, Final Batch Loss: 0.0019420272437855601\n",
      "Epoch 4715, Loss: 0.02929878095164895, Final Batch Loss: 0.006979864556342363\n",
      "Epoch 4716, Loss: 0.0790148638188839, Final Batch Loss: 0.04592682421207428\n",
      "Epoch 4717, Loss: 0.011348548578098416, Final Batch Loss: 0.0010630993638187647\n",
      "Epoch 4718, Loss: 0.02376358024775982, Final Batch Loss: 0.019137123599648476\n",
      "Epoch 4719, Loss: 0.011890414170920849, Final Batch Loss: 0.0043716118671\n",
      "Epoch 4720, Loss: 0.0250325002707541, Final Batch Loss: 0.002439290750771761\n",
      "Epoch 4721, Loss: 0.023548453580588102, Final Batch Loss: 0.004834245424717665\n",
      "Epoch 4722, Loss: 0.0333652263507247, Final Batch Loss: 0.009392504580318928\n",
      "Epoch 4723, Loss: 0.00852191133890301, Final Batch Loss: 0.0013788623036816716\n",
      "Epoch 4724, Loss: 0.02043518889695406, Final Batch Loss: 0.0013197539374232292\n",
      "Epoch 4725, Loss: 0.002725129364989698, Final Batch Loss: 0.0015317890793085098\n",
      "Epoch 4726, Loss: 0.050241315737366676, Final Batch Loss: 0.002045208588242531\n",
      "Epoch 4727, Loss: 0.016663774382323027, Final Batch Loss: 0.003542617429047823\n",
      "Epoch 4728, Loss: 0.004715796327218413, Final Batch Loss: 0.002589837182313204\n",
      "Epoch 4729, Loss: 0.0320159001275897, Final Batch Loss: 0.016706589609384537\n",
      "Epoch 4730, Loss: 0.005765898968093097, Final Batch Loss: 0.0009213819866999984\n",
      "Epoch 4731, Loss: 0.01695098076015711, Final Batch Loss: 0.009555432014167309\n",
      "Epoch 4732, Loss: 0.019655175739899278, Final Batch Loss: 0.0029165709856897593\n",
      "Epoch 4733, Loss: 0.006208006059750915, Final Batch Loss: 0.0009557574521750212\n",
      "Epoch 4734, Loss: 0.008987129898741841, Final Batch Loss: 0.0017968767788261175\n",
      "Epoch 4735, Loss: 0.0081587991444394, Final Batch Loss: 0.0014320019399747252\n",
      "Epoch 4736, Loss: 0.01125994324684143, Final Batch Loss: 0.00641906401142478\n",
      "Epoch 4737, Loss: 0.017493191175162792, Final Batch Loss: 0.006181167438626289\n",
      "Epoch 4738, Loss: 0.009462826768867671, Final Batch Loss: 0.0011455070925876498\n",
      "Epoch 4739, Loss: 0.006813925574533641, Final Batch Loss: 0.004970645532011986\n",
      "Epoch 4740, Loss: 0.018086610361933708, Final Batch Loss: 0.008431934751570225\n",
      "Epoch 4741, Loss: 0.010845537297427654, Final Batch Loss: 0.006524641998112202\n",
      "Epoch 4742, Loss: 0.04346734704449773, Final Batch Loss: 0.004483454395085573\n",
      "Epoch 4743, Loss: 0.0032398434123024344, Final Batch Loss: 0.0021097781136631966\n",
      "Epoch 4744, Loss: 0.003143085283227265, Final Batch Loss: 0.0012206792598590255\n",
      "Epoch 4745, Loss: 0.00240003305952996, Final Batch Loss: 0.0011664631310850382\n",
      "Epoch 4746, Loss: 0.0034745000302791595, Final Batch Loss: 0.0023478111252188683\n",
      "Epoch 4747, Loss: 0.0147225649561733, Final Batch Loss: 0.0034512223210185766\n",
      "Epoch 4748, Loss: 0.014181081322021782, Final Batch Loss: 0.013144545257091522\n",
      "Epoch 4749, Loss: 0.0048420390812680125, Final Batch Loss: 0.003069712081924081\n",
      "Epoch 4750, Loss: 0.009143183939158916, Final Batch Loss: 0.004454280715435743\n",
      "Epoch 4751, Loss: 0.029982194304466248, Final Batch Loss: 0.028859201818704605\n",
      "Epoch 4752, Loss: 0.004401536309160292, Final Batch Loss: 0.0029031243175268173\n",
      "Epoch 4753, Loss: 0.01812402019277215, Final Batch Loss: 0.01389521174132824\n",
      "Epoch 4754, Loss: 0.004045720328576863, Final Batch Loss: 0.0027290068101137877\n",
      "Epoch 4755, Loss: 0.0073562313336879015, Final Batch Loss: 0.002159051364287734\n",
      "Epoch 4756, Loss: 0.011924908962100744, Final Batch Loss: 0.0025635180063545704\n",
      "Epoch 4757, Loss: 0.0070736551424488425, Final Batch Loss: 0.005239884369075298\n",
      "Epoch 4758, Loss: 0.029385332483798265, Final Batch Loss: 0.004243089351803064\n",
      "Epoch 4759, Loss: 0.03156829415820539, Final Batch Loss: 0.0027364918496459723\n",
      "Epoch 4760, Loss: 0.015351553913205862, Final Batch Loss: 0.012044764123857021\n",
      "Epoch 4761, Loss: 0.016177769051864743, Final Batch Loss: 0.013966919854283333\n",
      "Epoch 4762, Loss: 0.0159241973888129, Final Batch Loss: 0.013213925994932652\n",
      "Epoch 4763, Loss: 0.0018059543217532337, Final Batch Loss: 0.0009721358655951917\n",
      "Epoch 4764, Loss: 0.040145836770534515, Final Batch Loss: 0.02613171376287937\n",
      "Epoch 4765, Loss: 0.006627093651331961, Final Batch Loss: 0.0010523704113438725\n",
      "Epoch 4766, Loss: 0.008535443339496851, Final Batch Loss: 0.0023799464106559753\n",
      "Epoch 4767, Loss: 0.03922010259702802, Final Batch Loss: 0.03507299721240997\n",
      "Epoch 4768, Loss: 0.004262454574927688, Final Batch Loss: 0.0021860674023628235\n",
      "Epoch 4769, Loss: 0.008258870104327798, Final Batch Loss: 0.005223982501775026\n",
      "Epoch 4770, Loss: 0.03524787165224552, Final Batch Loss: 0.016388749703764915\n",
      "Epoch 4771, Loss: 0.025671700830571353, Final Batch Loss: 0.02441677451133728\n",
      "Epoch 4772, Loss: 0.01111834798939526, Final Batch Loss: 0.009387404657900333\n",
      "Epoch 4773, Loss: 0.006964902160689235, Final Batch Loss: 0.0037194518372416496\n",
      "Epoch 4774, Loss: 0.00595709856133908, Final Batch Loss: 0.0016212457558140159\n",
      "Epoch 4775, Loss: 0.005795629811473191, Final Batch Loss: 0.003999494481831789\n",
      "Epoch 4776, Loss: 0.00763222249224782, Final Batch Loss: 0.0032294737175107002\n",
      "Epoch 4777, Loss: 0.007122956914827228, Final Batch Loss: 0.003763381624594331\n",
      "Epoch 4778, Loss: 0.00863717938773334, Final Batch Loss: 0.0012755219358950853\n",
      "Epoch 4779, Loss: 0.005602606455795467, Final Batch Loss: 0.0019389464287087321\n",
      "Epoch 4780, Loss: 0.001585913181770593, Final Batch Loss: 0.0004976575146429241\n",
      "Epoch 4781, Loss: 0.00559263676404953, Final Batch Loss: 0.002577204955741763\n",
      "Epoch 4782, Loss: 0.006186598213389516, Final Batch Loss: 0.001196791185066104\n",
      "Epoch 4783, Loss: 0.03838958963751793, Final Batch Loss: 0.00562174990773201\n",
      "Epoch 4784, Loss: 0.04350786842405796, Final Batch Loss: 0.03903175890445709\n",
      "Epoch 4785, Loss: 0.014283114927820861, Final Batch Loss: 0.0016312244115397334\n",
      "Epoch 4786, Loss: 0.024066975340247154, Final Batch Loss: 0.015374933369457722\n",
      "Epoch 4787, Loss: 0.009372238186188042, Final Batch Loss: 0.007761301938444376\n",
      "Epoch 4788, Loss: 0.007346322759985924, Final Batch Loss: 0.0015379181131720543\n",
      "Epoch 4789, Loss: 0.007253695628605783, Final Batch Loss: 0.005817587487399578\n",
      "Epoch 4790, Loss: 0.011299223406240344, Final Batch Loss: 0.0020435431506484747\n",
      "Epoch 4791, Loss: 0.02403067471459508, Final Batch Loss: 0.018075600266456604\n",
      "Epoch 4792, Loss: 0.026063923723995686, Final Batch Loss: 0.00907182414084673\n",
      "Epoch 4793, Loss: 0.01997754629701376, Final Batch Loss: 0.011758416891098022\n",
      "Epoch 4794, Loss: 0.004983058082871139, Final Batch Loss: 0.0035217152908444405\n",
      "Epoch 4795, Loss: 0.012064762297086418, Final Batch Loss: 0.0006227715639397502\n",
      "Epoch 4796, Loss: 0.017114947084337473, Final Batch Loss: 0.015891404822468758\n",
      "Epoch 4797, Loss: 0.002876748680137098, Final Batch Loss: 0.0011581946164369583\n",
      "Epoch 4798, Loss: 0.008751594461500645, Final Batch Loss: 0.0016898452304303646\n",
      "Epoch 4799, Loss: 0.005522308521904051, Final Batch Loss: 0.0041595264337956905\n",
      "Epoch 4800, Loss: 0.010867723380215466, Final Batch Loss: 0.009882310405373573\n",
      "Epoch 4801, Loss: 0.004126843996345997, Final Batch Loss: 0.0019864621572196484\n",
      "Epoch 4802, Loss: 0.003730553900822997, Final Batch Loss: 0.001451343297958374\n",
      "Epoch 4803, Loss: 0.0072520916583016515, Final Batch Loss: 0.00044955092016607523\n",
      "Epoch 4804, Loss: 0.08244690857827663, Final Batch Loss: 0.06465420871973038\n",
      "Epoch 4805, Loss: 0.004265230498276651, Final Batch Loss: 0.0010752106318250299\n",
      "Epoch 4806, Loss: 0.032287027686834335, Final Batch Loss: 0.010220171883702278\n",
      "Epoch 4807, Loss: 0.034334080293774605, Final Batch Loss: 0.019099656492471695\n",
      "Epoch 4808, Loss: 0.02956624235957861, Final Batch Loss: 0.014902690425515175\n",
      "Epoch 4809, Loss: 0.005097070010378957, Final Batch Loss: 0.002876486862078309\n",
      "Epoch 4810, Loss: 0.0030766434501856565, Final Batch Loss: 0.0011128296609967947\n",
      "Epoch 4811, Loss: 0.013843958731740713, Final Batch Loss: 0.005434427876025438\n",
      "Epoch 4812, Loss: 0.007573578739538789, Final Batch Loss: 0.002576634520664811\n",
      "Epoch 4813, Loss: 0.00614826928358525, Final Batch Loss: 0.0010163498809561133\n",
      "Epoch 4814, Loss: 0.008736293297261, Final Batch Loss: 0.0006781355477869511\n",
      "Epoch 4815, Loss: 0.0301140071824193, Final Batch Loss: 0.016407791525125504\n",
      "Epoch 4816, Loss: 0.01822983007878065, Final Batch Loss: 0.006843567825853825\n",
      "Epoch 4817, Loss: 0.05812940839678049, Final Batch Loss: 0.05670568346977234\n",
      "Epoch 4818, Loss: 0.017011430580168962, Final Batch Loss: 0.004531575832515955\n",
      "Epoch 4819, Loss: 0.011665649712085724, Final Batch Loss: 0.004242388065904379\n",
      "Epoch 4820, Loss: 0.014458211604505777, Final Batch Loss: 0.010151592083275318\n",
      "Epoch 4821, Loss: 0.02990622422657907, Final Batch Loss: 0.026239875704050064\n",
      "Epoch 4822, Loss: 0.014953902922570705, Final Batch Loss: 0.011929859407246113\n",
      "Epoch 4823, Loss: 0.01175100659020245, Final Batch Loss: 0.010655674152076244\n",
      "Epoch 4824, Loss: 0.02053700666874647, Final Batch Loss: 0.007412713021039963\n",
      "Epoch 4825, Loss: 0.024196390761062503, Final Batch Loss: 0.021739311516284943\n",
      "Epoch 4826, Loss: 0.008254548301920295, Final Batch Loss: 0.0023195433896034956\n",
      "Epoch 4827, Loss: 0.03654988552443683, Final Batch Loss: 0.0035704306792467833\n",
      "Epoch 4828, Loss: 0.06560788815841079, Final Batch Loss: 0.05795998126268387\n",
      "Epoch 4829, Loss: 0.005779140628874302, Final Batch Loss: 0.003548222128301859\n",
      "Epoch 4830, Loss: 0.006480945972725749, Final Batch Loss: 0.002434786641970277\n",
      "Epoch 4831, Loss: 0.009434222825802863, Final Batch Loss: 0.0013652244815602899\n",
      "Epoch 4832, Loss: 0.0025021699839271605, Final Batch Loss: 0.001527142128907144\n",
      "Epoch 4833, Loss: 0.004557289998047054, Final Batch Loss: 0.001371360500343144\n",
      "Epoch 4834, Loss: 0.00813279952853918, Final Batch Loss: 0.0028894497081637383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4835, Loss: 0.010497935931198299, Final Batch Loss: 0.009072559885680676\n",
      "Epoch 4836, Loss: 0.008906669216230512, Final Batch Loss: 0.003861999372020364\n",
      "Epoch 4837, Loss: 0.0030564445187337697, Final Batch Loss: 0.0005971764330752194\n",
      "Epoch 4838, Loss: 0.010541441151872277, Final Batch Loss: 0.0029269580263644457\n",
      "Epoch 4839, Loss: 0.016099174274131656, Final Batch Loss: 0.0024318580981343985\n",
      "Epoch 4840, Loss: 0.007923091063275933, Final Batch Loss: 0.0036106009501963854\n",
      "Epoch 4841, Loss: 0.004156915238127112, Final Batch Loss: 0.002192365238443017\n",
      "Epoch 4842, Loss: 0.023549543228000402, Final Batch Loss: 0.016658375039696693\n",
      "Epoch 4843, Loss: 0.016579061280936003, Final Batch Loss: 0.003484234679490328\n",
      "Epoch 4844, Loss: 0.026025911793112755, Final Batch Loss: 0.010429710149765015\n",
      "Epoch 4845, Loss: 0.009555435623042285, Final Batch Loss: 0.008458277210593224\n",
      "Epoch 4846, Loss: 0.004248033044859767, Final Batch Loss: 0.0023452627938240767\n",
      "Epoch 4847, Loss: 0.07268673554062843, Final Batch Loss: 0.04807959496974945\n",
      "Epoch 4848, Loss: 0.02085427730344236, Final Batch Loss: 0.017531991004943848\n",
      "Epoch 4849, Loss: 0.007913172710686922, Final Batch Loss: 0.0030922451987862587\n",
      "Epoch 4850, Loss: 0.026759370521176606, Final Batch Loss: 0.000675685063470155\n",
      "Epoch 4851, Loss: 0.004164072568528354, Final Batch Loss: 0.0017407973064109683\n",
      "Epoch 4852, Loss: 0.020903357537463307, Final Batch Loss: 0.0034019232261925936\n",
      "Epoch 4853, Loss: 0.005333338864147663, Final Batch Loss: 0.0033533836249262094\n",
      "Epoch 4854, Loss: 0.004798040725290775, Final Batch Loss: 0.002853367943316698\n",
      "Epoch 4855, Loss: 0.005649384343996644, Final Batch Loss: 0.0025466468650847673\n",
      "Epoch 4856, Loss: 0.012836464331485331, Final Batch Loss: 0.0010610871249809861\n",
      "Epoch 4857, Loss: 0.003585032536648214, Final Batch Loss: 0.0016911241691559553\n",
      "Epoch 4858, Loss: 0.020247460808604956, Final Batch Loss: 0.006090241018682718\n",
      "Epoch 4859, Loss: 0.004515331645961851, Final Batch Loss: 0.0037284507416188717\n",
      "Epoch 4860, Loss: 0.013744996016612276, Final Batch Loss: 0.00041406331001780927\n",
      "Epoch 4861, Loss: 0.0029753331327810884, Final Batch Loss: 0.0018731969175860286\n",
      "Epoch 4862, Loss: 0.013828599127009511, Final Batch Loss: 0.009970788843929768\n",
      "Epoch 4863, Loss: 0.023170022934209555, Final Batch Loss: 0.0007305859471671283\n",
      "Epoch 4864, Loss: 0.007853359216824174, Final Batch Loss: 0.0025145530235022306\n",
      "Epoch 4865, Loss: 0.02030567266047001, Final Batch Loss: 0.004805953241884708\n",
      "Epoch 4866, Loss: 0.012530552165117115, Final Batch Loss: 0.0008104061125777662\n",
      "Epoch 4867, Loss: 0.018503365106880665, Final Batch Loss: 0.0025065774098038673\n",
      "Epoch 4868, Loss: 0.03300847881473601, Final Batch Loss: 0.00116960727609694\n",
      "Epoch 4869, Loss: 0.007288544904440641, Final Batch Loss: 0.0033320412039756775\n",
      "Epoch 4870, Loss: 0.006037565181031823, Final Batch Loss: 0.002371799433603883\n",
      "Epoch 4871, Loss: 0.02406671131029725, Final Batch Loss: 0.002170898485928774\n",
      "Epoch 4872, Loss: 0.08405860885977745, Final Batch Loss: 0.031831685453653336\n",
      "Epoch 4873, Loss: 0.02359272143803537, Final Batch Loss: 0.003359066089615226\n",
      "Epoch 4874, Loss: 0.024311915040016174, Final Batch Loss: 0.01202821172773838\n",
      "Epoch 4875, Loss: 0.00858617341145873, Final Batch Loss: 0.004228333011269569\n",
      "Epoch 4876, Loss: 0.012780865072272718, Final Batch Loss: 0.001561244367621839\n",
      "Epoch 4877, Loss: 0.00845056725665927, Final Batch Loss: 0.0025533149018883705\n",
      "Epoch 4878, Loss: 0.020836170413531363, Final Batch Loss: 0.0011477820808067918\n",
      "Epoch 4879, Loss: 0.003912912798114121, Final Batch Loss: 0.00247400370426476\n",
      "Epoch 4880, Loss: 0.007125334115698934, Final Batch Loss: 0.005566854495555162\n",
      "Epoch 4881, Loss: 0.005389993777498603, Final Batch Loss: 0.0025983043015003204\n",
      "Epoch 4882, Loss: 0.0038565564900636673, Final Batch Loss: 0.0018616768065840006\n",
      "Epoch 4883, Loss: 0.004531048587523401, Final Batch Loss: 0.0012971576070412993\n",
      "Epoch 4884, Loss: 0.022259857039898634, Final Batch Loss: 0.005524841602891684\n",
      "Epoch 4885, Loss: 0.011689923703670502, Final Batch Loss: 0.005586514249444008\n",
      "Epoch 4886, Loss: 0.009129692800343037, Final Batch Loss: 0.004629428964108229\n",
      "Epoch 4887, Loss: 0.005946759949438274, Final Batch Loss: 0.0016036344459280372\n",
      "Epoch 4888, Loss: 0.005425457609817386, Final Batch Loss: 0.0021349673625081778\n",
      "Epoch 4889, Loss: 0.03757905610837042, Final Batch Loss: 0.035625528544187546\n",
      "Epoch 4890, Loss: 0.02219069004058838, Final Batch Loss: 0.016910061240196228\n",
      "Epoch 4891, Loss: 0.017322661238722503, Final Batch Loss: 0.0017066792352125049\n",
      "Epoch 4892, Loss: 0.006071124225854874, Final Batch Loss: 0.003816534299403429\n",
      "Epoch 4893, Loss: 0.05444172117859125, Final Batch Loss: 0.015486174263060093\n",
      "Epoch 4894, Loss: 0.008074329234659672, Final Batch Loss: 0.003996999002993107\n",
      "Epoch 4895, Loss: 0.019098457298241556, Final Batch Loss: 0.017623314633965492\n",
      "Epoch 4896, Loss: 0.0073697937768884, Final Batch Loss: 0.0006919571314938366\n",
      "Epoch 4897, Loss: 0.021574727026745677, Final Batch Loss: 0.01888676919043064\n",
      "Epoch 4898, Loss: 0.008607739116996527, Final Batch Loss: 0.0026249797083437443\n",
      "Epoch 4899, Loss: 0.004224932985380292, Final Batch Loss: 0.0021873621735721827\n",
      "Epoch 4900, Loss: 0.006293825572356582, Final Batch Loss: 0.0038635849487036467\n",
      "Epoch 4901, Loss: 0.033185647800564766, Final Batch Loss: 0.017308389768004417\n",
      "Epoch 4902, Loss: 0.003390201018191874, Final Batch Loss: 0.001472810166887939\n",
      "Epoch 4903, Loss: 0.0035037362831644714, Final Batch Loss: 0.002912734402343631\n",
      "Epoch 4904, Loss: 0.009883892780635506, Final Batch Loss: 0.0007592328474856913\n",
      "Epoch 4905, Loss: 0.012590842321515083, Final Batch Loss: 0.00379033200442791\n",
      "Epoch 4906, Loss: 0.01451734930742532, Final Batch Loss: 0.0013079383643344045\n",
      "Epoch 4907, Loss: 0.008140827878378332, Final Batch Loss: 0.006296197418123484\n",
      "Epoch 4908, Loss: 0.008016365114599466, Final Batch Loss: 0.002239388879388571\n",
      "Epoch 4909, Loss: 0.029445470543578267, Final Batch Loss: 0.0022069469559937716\n",
      "Epoch 4910, Loss: 0.012936356186401099, Final Batch Loss: 0.0007596860523335636\n",
      "Epoch 4911, Loss: 0.011328436899930239, Final Batch Loss: 0.006841870490461588\n",
      "Epoch 4912, Loss: 0.0041831627022475, Final Batch Loss: 0.0020218787249177694\n",
      "Epoch 4913, Loss: 0.016886743949726224, Final Batch Loss: 0.01408233679831028\n",
      "Epoch 4914, Loss: 0.012675655074417591, Final Batch Loss: 0.0026119276881217957\n",
      "Epoch 4915, Loss: 0.02122837328352034, Final Batch Loss: 0.003202501917257905\n",
      "Epoch 4916, Loss: 0.005066699523013085, Final Batch Loss: 0.0006659437785856426\n",
      "Epoch 4917, Loss: 0.011299572885036469, Final Batch Loss: 0.006298725958913565\n",
      "Epoch 4918, Loss: 0.040110101574100554, Final Batch Loss: 0.03842465206980705\n",
      "Epoch 4919, Loss: 0.003887489205226302, Final Batch Loss: 0.0019370629452168941\n",
      "Epoch 4920, Loss: 0.004908536793664098, Final Batch Loss: 0.0026869431603699923\n",
      "Epoch 4921, Loss: 0.0027068087947554886, Final Batch Loss: 0.00043478800216689706\n",
      "Epoch 4922, Loss: 0.002054877928458154, Final Batch Loss: 0.0011862769024446607\n",
      "Epoch 4923, Loss: 0.03178347286302596, Final Batch Loss: 0.030329331755638123\n",
      "Epoch 4924, Loss: 0.08273125812411308, Final Batch Loss: 0.06973518431186676\n",
      "Epoch 4925, Loss: 0.020797044970095158, Final Batch Loss: 0.00910700112581253\n",
      "Epoch 4926, Loss: 0.03883058484643698, Final Batch Loss: 0.0045701852068305016\n",
      "Epoch 4927, Loss: 0.00801938702352345, Final Batch Loss: 0.004521519877016544\n",
      "Epoch 4928, Loss: 0.019948274246416986, Final Batch Loss: 0.0010583068942651153\n",
      "Epoch 4929, Loss: 0.005624218436423689, Final Batch Loss: 0.004891041200608015\n",
      "Epoch 4930, Loss: 0.009461040142923594, Final Batch Loss: 0.005078659392893314\n",
      "Epoch 4931, Loss: 0.005651967832818627, Final Batch Loss: 0.0034046443179249763\n",
      "Epoch 4932, Loss: 0.01942838914692402, Final Batch Loss: 0.007850131019949913\n",
      "Epoch 4933, Loss: 0.00907828169874847, Final Batch Loss: 0.006027401890605688\n",
      "Epoch 4934, Loss: 0.002663187449797988, Final Batch Loss: 0.0013987114652991295\n",
      "Epoch 4935, Loss: 0.0029138055397197604, Final Batch Loss: 0.0011687534861266613\n",
      "Epoch 4936, Loss: 0.007589000626467168, Final Batch Loss: 0.0010674291988834739\n",
      "Epoch 4937, Loss: 0.008434259798377752, Final Batch Loss: 0.004376480355858803\n",
      "Epoch 4938, Loss: 0.005753798934165388, Final Batch Loss: 0.0008237166912294924\n",
      "Epoch 4939, Loss: 0.005201679654419422, Final Batch Loss: 0.001079624518752098\n",
      "Epoch 4940, Loss: 0.005223551299422979, Final Batch Loss: 0.0017970551270991564\n",
      "Epoch 4941, Loss: 0.007897369214333594, Final Batch Loss: 0.0066664679907262325\n",
      "Epoch 4942, Loss: 0.009428585413843393, Final Batch Loss: 0.004780788440257311\n",
      "Epoch 4943, Loss: 0.01053517498075962, Final Batch Loss: 0.00786383356899023\n",
      "Epoch 4944, Loss: 0.0024898393312469125, Final Batch Loss: 0.0011226119240745902\n",
      "Epoch 4945, Loss: 0.023866862058639526, Final Batch Loss: 0.011559296399354935\n",
      "Epoch 4946, Loss: 0.0014813080779276788, Final Batch Loss: 0.0005316251190379262\n",
      "Epoch 4947, Loss: 0.018429655872751027, Final Batch Loss: 0.017747994512319565\n",
      "Epoch 4948, Loss: 0.0054386605334002525, Final Batch Loss: 0.00035315219429321587\n",
      "Epoch 4949, Loss: 0.00290788896381855, Final Batch Loss: 0.0019440061878412962\n",
      "Epoch 4950, Loss: 0.002444023033604026, Final Batch Loss: 0.0008249325910583138\n",
      "Epoch 4951, Loss: 0.033173320814967155, Final Batch Loss: 0.029494144022464752\n",
      "Epoch 4952, Loss: 0.010651741176843643, Final Batch Loss: 0.003553470131009817\n",
      "Epoch 4953, Loss: 0.0022388191428035498, Final Batch Loss: 0.000783278257586062\n",
      "Epoch 4954, Loss: 0.004113340168260038, Final Batch Loss: 0.0016809658845886588\n",
      "Epoch 4955, Loss: 0.0019110744469799101, Final Batch Loss: 0.0011629749787971377\n",
      "Epoch 4956, Loss: 0.0026479095686227083, Final Batch Loss: 0.0007057142211124301\n",
      "Epoch 4957, Loss: 0.005375491571612656, Final Batch Loss: 0.0012815004447475076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4958, Loss: 0.0022973756422288716, Final Batch Loss: 0.0009393763612024486\n",
      "Epoch 4959, Loss: 0.006085410015657544, Final Batch Loss: 0.003839292796328664\n",
      "Epoch 4960, Loss: 0.0034867817303165793, Final Batch Loss: 0.0009213659213855863\n",
      "Epoch 4961, Loss: 0.009051627945154905, Final Batch Loss: 0.004046005662530661\n",
      "Epoch 4962, Loss: 0.0074435732094570994, Final Batch Loss: 0.006060593295842409\n",
      "Epoch 4963, Loss: 0.0023829067358747125, Final Batch Loss: 0.0009699288057163358\n",
      "Epoch 4964, Loss: 0.005969837657175958, Final Batch Loss: 0.0014874831540510058\n",
      "Epoch 4965, Loss: 0.006732366979122162, Final Batch Loss: 0.0012083197943866253\n",
      "Epoch 4966, Loss: 0.0048480661935172975, Final Batch Loss: 0.004145967308431864\n",
      "Epoch 4967, Loss: 0.0015975922287907451, Final Batch Loss: 0.00048795584007166326\n",
      "Epoch 4968, Loss: 0.012646754970774055, Final Batch Loss: 0.010408748872578144\n",
      "Epoch 4969, Loss: 0.005656553665176034, Final Batch Loss: 0.0025641382671892643\n",
      "Epoch 4970, Loss: 0.026885641971603036, Final Batch Loss: 0.0012264421675354242\n",
      "Epoch 4971, Loss: 0.0053187084558885545, Final Batch Loss: 0.0002885090361814946\n",
      "Epoch 4972, Loss: 0.0038836635649204254, Final Batch Loss: 0.0018989513628184795\n",
      "Epoch 4973, Loss: 0.004633000819012523, Final Batch Loss: 0.003300395095720887\n",
      "Epoch 4974, Loss: 0.04392633121460676, Final Batch Loss: 0.03738737106323242\n",
      "Epoch 4975, Loss: 0.0011314585572108626, Final Batch Loss: 0.0005559907294809818\n",
      "Epoch 4976, Loss: 0.0021381282713264227, Final Batch Loss: 0.0009275502525269985\n",
      "Epoch 4977, Loss: 0.0050232550129294395, Final Batch Loss: 0.0024432733189314604\n",
      "Epoch 4978, Loss: 0.00804988550953567, Final Batch Loss: 0.00582620594650507\n",
      "Epoch 4979, Loss: 0.05076869577169418, Final Batch Loss: 0.027512159198522568\n",
      "Epoch 4980, Loss: 0.0047193149803206325, Final Batch Loss: 0.0011326282983645797\n",
      "Epoch 4981, Loss: 0.010449946857988834, Final Batch Loss: 0.0016280943527817726\n",
      "Epoch 4982, Loss: 0.024249084875918925, Final Batch Loss: 0.023096946999430656\n",
      "Epoch 4983, Loss: 0.006527670833747834, Final Batch Loss: 0.0003485476481728256\n",
      "Epoch 4984, Loss: 0.01294196886010468, Final Batch Loss: 0.01117389090359211\n",
      "Epoch 4985, Loss: 0.014786791987717152, Final Batch Loss: 0.002577139064669609\n",
      "Epoch 4986, Loss: 0.011143969837576151, Final Batch Loss: 0.005497548263520002\n",
      "Epoch 4987, Loss: 0.005178514402359724, Final Batch Loss: 0.0036128421779721975\n",
      "Epoch 4988, Loss: 0.0010673982906155288, Final Batch Loss: 0.0004893839941360056\n",
      "Epoch 4989, Loss: 0.004262459697201848, Final Batch Loss: 0.002530130324885249\n",
      "Epoch 4990, Loss: 0.07334110140800476, Final Batch Loss: 0.030812378972768784\n",
      "Epoch 4991, Loss: 0.007159152359236032, Final Batch Loss: 0.0006610232521779835\n",
      "Epoch 4992, Loss: 0.0018355866195634007, Final Batch Loss: 0.0011812589364126325\n",
      "Epoch 4993, Loss: 0.0074697318486869335, Final Batch Loss: 0.0051340688951313496\n",
      "Epoch 4994, Loss: 0.0052662481321021914, Final Batch Loss: 0.0037588311824947596\n",
      "Epoch 4995, Loss: 0.005135398008860648, Final Batch Loss: 0.0037758005782961845\n",
      "Epoch 4996, Loss: 0.0037729141768068075, Final Batch Loss: 0.0014351778663694859\n",
      "Epoch 4997, Loss: 0.00913496594876051, Final Batch Loss: 0.004146241582930088\n",
      "Epoch 4998, Loss: 0.002735845686402172, Final Batch Loss: 0.0008079729159362614\n",
      "Epoch 4999, Loss: 0.008055018726736307, Final Batch Loss: 0.0011156140826642513\n",
      "Epoch 5000, Loss: 0.07207063504029065, Final Batch Loss: 0.0013911790447309613\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  0  0]\n",
      " [ 0 27  0]\n",
      " [ 0  0 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        26\n",
      "           1      1.000     1.000     1.000        27\n",
      "           2      1.000     1.000     1.000        21\n",
      "\n",
      "    accuracy                          1.000        74\n",
      "   macro avg      1.000     1.000     1.000        74\n",
      "weighted avg      1.000     1.000     1.000        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../saved_models/UCI 3 User Classifier Group 4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
