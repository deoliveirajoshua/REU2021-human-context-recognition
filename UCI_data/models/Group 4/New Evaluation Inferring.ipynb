{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A0 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_1 = gen(to_gen).detach().numpy()\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A1 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_2 = gen(to_gen).detach().numpy()\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U0A2 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,0] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_3 = gen(to_gen).detach().numpy()\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A0 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_4 = gen(to_gen).detach().numpy()\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A1 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_5 = gen(to_gen).detach().numpy()\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U1A2 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,1] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_6 = gen(to_gen).detach().numpy()\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A0 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,0] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_7 = gen(to_gen).detach().numpy()\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A1 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,1] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_8 = gen(to_gen).detach().numpy()\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"U2A2 Excluded Group 4_gen.param\")\n",
    "gen.eval()\n",
    "latent_vectors = get_noise(35, 100)\n",
    "act_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "usr_vectors = torch.Tensor(np.zeros((35,3)))\n",
    "for k in range(35):\n",
    "    act_vectors[k,2] = 1\n",
    "    usr_vectors[k,2] = 1\n",
    "to_gen = torch.cat((latent_vectors, act_vectors, usr_vectors), 1)\n",
    "fake_features_9 = gen(to_gen).detach().numpy()\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_test = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_test = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [19, 21, 22]\n",
    "X_train, y_train = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.367840766906738, Final Batch Loss: 2.170567274093628\n",
      "Epoch 2, Loss: 4.365608215332031, Final Batch Loss: 2.182229518890381\n",
      "Epoch 3, Loss: 4.360623836517334, Final Batch Loss: 2.179626226425171\n",
      "Epoch 4, Loss: 4.3532915115356445, Final Batch Loss: 2.1873271465301514\n",
      "Epoch 5, Loss: 4.345149517059326, Final Batch Loss: 2.1684815883636475\n",
      "Epoch 6, Loss: 4.341340780258179, Final Batch Loss: 2.17812180519104\n",
      "Epoch 7, Loss: 4.328272104263306, Final Batch Loss: 2.171015977859497\n",
      "Epoch 8, Loss: 4.31042218208313, Final Batch Loss: 2.1501972675323486\n",
      "Epoch 9, Loss: 4.306570053100586, Final Batch Loss: 2.1569724082946777\n",
      "Epoch 10, Loss: 4.297294855117798, Final Batch Loss: 2.148012638092041\n",
      "Epoch 11, Loss: 4.271338224411011, Final Batch Loss: 2.150373935699463\n",
      "Epoch 12, Loss: 4.2656590938568115, Final Batch Loss: 2.109822988510132\n",
      "Epoch 13, Loss: 4.253783226013184, Final Batch Loss: 2.1315810680389404\n",
      "Epoch 14, Loss: 4.230139970779419, Final Batch Loss: 2.125282049179077\n",
      "Epoch 15, Loss: 4.218689680099487, Final Batch Loss: 2.0973334312438965\n",
      "Epoch 16, Loss: 4.187781572341919, Final Batch Loss: 2.109466791152954\n",
      "Epoch 17, Loss: 4.155842542648315, Final Batch Loss: 2.0635883808135986\n",
      "Epoch 18, Loss: 4.145556688308716, Final Batch Loss: 2.1013917922973633\n",
      "Epoch 19, Loss: 4.133767127990723, Final Batch Loss: 2.063371419906616\n",
      "Epoch 20, Loss: 4.110629081726074, Final Batch Loss: 2.052534341812134\n",
      "Epoch 21, Loss: 4.0770463943481445, Final Batch Loss: 2.0326266288757324\n",
      "Epoch 22, Loss: 4.036081314086914, Final Batch Loss: 2.020217180252075\n",
      "Epoch 23, Loss: 4.016294240951538, Final Batch Loss: 2.0103631019592285\n",
      "Epoch 24, Loss: 4.015030026435852, Final Batch Loss: 1.9836384057998657\n",
      "Epoch 25, Loss: 3.9798473119735718, Final Batch Loss: 1.950820803642273\n",
      "Epoch 26, Loss: 3.958551049232483, Final Batch Loss: 1.999932050704956\n",
      "Epoch 27, Loss: 3.9144469499588013, Final Batch Loss: 1.9134639501571655\n",
      "Epoch 28, Loss: 3.9215370416641235, Final Batch Loss: 1.9423514604568481\n",
      "Epoch 29, Loss: 3.9413236379623413, Final Batch Loss: 1.9536911249160767\n",
      "Epoch 30, Loss: 3.8894307613372803, Final Batch Loss: 1.9666191339492798\n",
      "Epoch 31, Loss: 3.85339617729187, Final Batch Loss: 1.9190183877944946\n",
      "Epoch 32, Loss: 3.8337031602859497, Final Batch Loss: 1.916332483291626\n",
      "Epoch 33, Loss: 3.820362091064453, Final Batch Loss: 1.9318797588348389\n",
      "Epoch 34, Loss: 3.770134925842285, Final Batch Loss: 1.8909592628479004\n",
      "Epoch 35, Loss: 3.750111222267151, Final Batch Loss: 1.9048373699188232\n",
      "Epoch 36, Loss: 3.7474608421325684, Final Batch Loss: 1.8939725160598755\n",
      "Epoch 37, Loss: 3.734593987464905, Final Batch Loss: 1.8881434202194214\n",
      "Epoch 38, Loss: 3.6673197746276855, Final Batch Loss: 1.8473538160324097\n",
      "Epoch 39, Loss: 3.635134220123291, Final Batch Loss: 1.7955793142318726\n",
      "Epoch 40, Loss: 3.620752215385437, Final Batch Loss: 1.793485164642334\n",
      "Epoch 41, Loss: 3.5964068174362183, Final Batch Loss: 1.8021392822265625\n",
      "Epoch 42, Loss: 3.5343490839004517, Final Batch Loss: 1.7526545524597168\n",
      "Epoch 43, Loss: 3.5088560581207275, Final Batch Loss: 1.7420510053634644\n",
      "Epoch 44, Loss: 3.4873785972595215, Final Batch Loss: 1.795997142791748\n",
      "Epoch 45, Loss: 3.426499605178833, Final Batch Loss: 1.6795628070831299\n",
      "Epoch 46, Loss: 3.4307647943496704, Final Batch Loss: 1.7256674766540527\n",
      "Epoch 47, Loss: 3.3339143991470337, Final Batch Loss: 1.6622068881988525\n",
      "Epoch 48, Loss: 3.343091607093811, Final Batch Loss: 1.6791903972625732\n",
      "Epoch 49, Loss: 3.2736759185791016, Final Batch Loss: 1.6191843748092651\n",
      "Epoch 50, Loss: 3.2766939401626587, Final Batch Loss: 1.6174448728561401\n",
      "Epoch 51, Loss: 3.252922773361206, Final Batch Loss: 1.6251981258392334\n",
      "Epoch 52, Loss: 3.179506301879883, Final Batch Loss: 1.6429451704025269\n",
      "Epoch 53, Loss: 3.163951873779297, Final Batch Loss: 1.5969874858856201\n",
      "Epoch 54, Loss: 3.13858163356781, Final Batch Loss: 1.5324474573135376\n",
      "Epoch 55, Loss: 3.0845389366149902, Final Batch Loss: 1.5519201755523682\n",
      "Epoch 56, Loss: 3.0316332578659058, Final Batch Loss: 1.506609559059143\n",
      "Epoch 57, Loss: 2.9989590644836426, Final Batch Loss: 1.5150046348571777\n",
      "Epoch 58, Loss: 2.9653598070144653, Final Batch Loss: 1.456836462020874\n",
      "Epoch 59, Loss: 2.968605399131775, Final Batch Loss: 1.5010162591934204\n",
      "Epoch 60, Loss: 2.908088445663452, Final Batch Loss: 1.4061537981033325\n",
      "Epoch 61, Loss: 2.9204241037368774, Final Batch Loss: 1.4658966064453125\n",
      "Epoch 62, Loss: 2.8537673950195312, Final Batch Loss: 1.4341908693313599\n",
      "Epoch 63, Loss: 2.90681529045105, Final Batch Loss: 1.5163809061050415\n",
      "Epoch 64, Loss: 2.834176778793335, Final Batch Loss: 1.416355848312378\n",
      "Epoch 65, Loss: 2.8309342861175537, Final Batch Loss: 1.4446864128112793\n",
      "Epoch 66, Loss: 2.817657947540283, Final Batch Loss: 1.4470313787460327\n",
      "Epoch 67, Loss: 2.7306429147720337, Final Batch Loss: 1.302269697189331\n",
      "Epoch 68, Loss: 2.7892768383026123, Final Batch Loss: 1.4348280429840088\n",
      "Epoch 69, Loss: 2.8440226316452026, Final Batch Loss: 1.4875876903533936\n",
      "Epoch 70, Loss: 2.7241411209106445, Final Batch Loss: 1.3598698377609253\n",
      "Epoch 71, Loss: 2.711403012275696, Final Batch Loss: 1.3821139335632324\n",
      "Epoch 72, Loss: 2.684721350669861, Final Batch Loss: 1.3250553607940674\n",
      "Epoch 73, Loss: 2.6895229816436768, Final Batch Loss: 1.3446214199066162\n",
      "Epoch 74, Loss: 2.682808041572571, Final Batch Loss: 1.304442048072815\n",
      "Epoch 75, Loss: 2.559794783592224, Final Batch Loss: 1.2493057250976562\n",
      "Epoch 76, Loss: 2.5903791189193726, Final Batch Loss: 1.2624555826187134\n",
      "Epoch 77, Loss: 2.610777735710144, Final Batch Loss: 1.3328841924667358\n",
      "Epoch 78, Loss: 2.594499707221985, Final Batch Loss: 1.3317021131515503\n",
      "Epoch 79, Loss: 2.6033705472946167, Final Batch Loss: 1.3158485889434814\n",
      "Epoch 80, Loss: 2.5427606105804443, Final Batch Loss: 1.220098853111267\n",
      "Epoch 81, Loss: 2.5571197271347046, Final Batch Loss: 1.302965760231018\n",
      "Epoch 82, Loss: 2.515298366546631, Final Batch Loss: 1.2596231698989868\n",
      "Epoch 83, Loss: 2.4718711376190186, Final Batch Loss: 1.2363314628601074\n",
      "Epoch 84, Loss: 2.4418188333511353, Final Batch Loss: 1.2051979303359985\n",
      "Epoch 85, Loss: 2.48781955242157, Final Batch Loss: 1.2879977226257324\n",
      "Epoch 86, Loss: 2.4376367330551147, Final Batch Loss: 1.2509452104568481\n",
      "Epoch 87, Loss: 2.5375781059265137, Final Batch Loss: 1.3211530447006226\n",
      "Epoch 88, Loss: 2.4167033433914185, Final Batch Loss: 1.2485051155090332\n",
      "Epoch 89, Loss: 2.4505434036254883, Final Batch Loss: 1.2694413661956787\n",
      "Epoch 90, Loss: 2.3529900312423706, Final Batch Loss: 1.1881788969039917\n",
      "Epoch 91, Loss: 2.333230137825012, Final Batch Loss: 1.1219372749328613\n",
      "Epoch 92, Loss: 2.301924705505371, Final Batch Loss: 1.07535982131958\n",
      "Epoch 93, Loss: 2.392715334892273, Final Batch Loss: 1.183329463005066\n",
      "Epoch 94, Loss: 2.3859193325042725, Final Batch Loss: 1.1823838949203491\n",
      "Epoch 95, Loss: 2.21008837223053, Final Batch Loss: 1.1062968969345093\n",
      "Epoch 96, Loss: 2.2631806135177612, Final Batch Loss: 1.1268951892852783\n",
      "Epoch 97, Loss: 2.260809302330017, Final Batch Loss: 1.07011878490448\n",
      "Epoch 98, Loss: 2.2958308458328247, Final Batch Loss: 1.1822925806045532\n",
      "Epoch 99, Loss: 2.1340506076812744, Final Batch Loss: 1.0044667720794678\n",
      "Epoch 100, Loss: 2.1755192279815674, Final Batch Loss: 1.0475956201553345\n",
      "Epoch 101, Loss: 2.2439253330230713, Final Batch Loss: 1.1401809453964233\n",
      "Epoch 102, Loss: 2.151878595352173, Final Batch Loss: 1.0400248765945435\n",
      "Epoch 103, Loss: 2.1557148694992065, Final Batch Loss: 1.0370954275131226\n",
      "Epoch 104, Loss: 2.2386064529418945, Final Batch Loss: 1.1589055061340332\n",
      "Epoch 105, Loss: 2.034976601600647, Final Batch Loss: 1.0156598091125488\n",
      "Epoch 106, Loss: 2.1496351957321167, Final Batch Loss: 1.0537234544754028\n",
      "Epoch 107, Loss: 2.1125539541244507, Final Batch Loss: 1.0831434726715088\n",
      "Epoch 108, Loss: 2.0449938774108887, Final Batch Loss: 1.0229394435882568\n",
      "Epoch 109, Loss: 2.0739777088165283, Final Batch Loss: 1.0065635442733765\n",
      "Epoch 110, Loss: 2.127068877220154, Final Batch Loss: 1.0522500276565552\n",
      "Epoch 111, Loss: 2.035880446434021, Final Batch Loss: 1.0346314907073975\n",
      "Epoch 112, Loss: 2.0263118147850037, Final Batch Loss: 0.9851866364479065\n",
      "Epoch 113, Loss: 2.043098568916321, Final Batch Loss: 1.0397030115127563\n",
      "Epoch 114, Loss: 2.077345609664917, Final Batch Loss: 1.0097891092300415\n",
      "Epoch 115, Loss: 1.9585152864456177, Final Batch Loss: 0.9311873912811279\n",
      "Epoch 116, Loss: 2.057448625564575, Final Batch Loss: 1.0307046175003052\n",
      "Epoch 117, Loss: 2.0374823808670044, Final Batch Loss: 1.0611522197723389\n",
      "Epoch 118, Loss: 1.9505380392074585, Final Batch Loss: 0.9710153341293335\n",
      "Epoch 119, Loss: 1.9264716506004333, Final Batch Loss: 0.9679403305053711\n",
      "Epoch 120, Loss: 1.961396038532257, Final Batch Loss: 0.9816781282424927\n",
      "Epoch 121, Loss: 1.9112236499786377, Final Batch Loss: 0.9253169298171997\n",
      "Epoch 122, Loss: 1.9490700960159302, Final Batch Loss: 0.9964666962623596\n",
      "Epoch 123, Loss: 1.9520816206932068, Final Batch Loss: 0.9824100732803345\n",
      "Epoch 124, Loss: 1.8853967189788818, Final Batch Loss: 0.8907859325408936\n",
      "Epoch 125, Loss: 1.8661664128303528, Final Batch Loss: 0.973901093006134\n",
      "Epoch 126, Loss: 1.944342017173767, Final Batch Loss: 0.9506139755249023\n",
      "Epoch 127, Loss: 1.8333362936973572, Final Batch Loss: 0.9102460145950317\n",
      "Epoch 128, Loss: 1.794779360294342, Final Batch Loss: 0.9128553867340088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: 1.8435022234916687, Final Batch Loss: 0.9067776203155518\n",
      "Epoch 130, Loss: 1.8061559796333313, Final Batch Loss: 0.9007671475410461\n",
      "Epoch 131, Loss: 1.7522465586662292, Final Batch Loss: 0.8728840351104736\n",
      "Epoch 132, Loss: 1.7746000289916992, Final Batch Loss: 0.9325474500656128\n",
      "Epoch 133, Loss: 1.86391943693161, Final Batch Loss: 0.9756565093994141\n",
      "Epoch 134, Loss: 1.8310107588768005, Final Batch Loss: 0.8937795162200928\n",
      "Epoch 135, Loss: 1.780493676662445, Final Batch Loss: 0.9036983251571655\n",
      "Epoch 136, Loss: 1.7056443095207214, Final Batch Loss: 0.8608459830284119\n",
      "Epoch 137, Loss: 1.8172557353973389, Final Batch Loss: 0.9373510479927063\n",
      "Epoch 138, Loss: 1.7053506970405579, Final Batch Loss: 0.8564825654029846\n",
      "Epoch 139, Loss: 1.768049716949463, Final Batch Loss: 0.9141610264778137\n",
      "Epoch 140, Loss: 1.69284325838089, Final Batch Loss: 0.789998471736908\n",
      "Epoch 141, Loss: 1.720328688621521, Final Batch Loss: 0.8652228116989136\n",
      "Epoch 142, Loss: 1.6885928511619568, Final Batch Loss: 0.8326266407966614\n",
      "Epoch 143, Loss: 1.6993407607078552, Final Batch Loss: 0.8656113147735596\n",
      "Epoch 144, Loss: 1.7456332445144653, Final Batch Loss: 0.9224435091018677\n",
      "Epoch 145, Loss: 1.7083925604820251, Final Batch Loss: 0.8133686780929565\n",
      "Epoch 146, Loss: 1.6952804923057556, Final Batch Loss: 0.8764151930809021\n",
      "Epoch 147, Loss: 1.741741418838501, Final Batch Loss: 0.8916445374488831\n",
      "Epoch 148, Loss: 1.671026587486267, Final Batch Loss: 0.8345718383789062\n",
      "Epoch 149, Loss: 1.6132425665855408, Final Batch Loss: 0.7886342406272888\n",
      "Epoch 150, Loss: 1.6954290866851807, Final Batch Loss: 0.8243110179901123\n",
      "Epoch 151, Loss: 1.6111700534820557, Final Batch Loss: 0.8356066346168518\n",
      "Epoch 152, Loss: 1.6716623902320862, Final Batch Loss: 0.8385331034660339\n",
      "Epoch 153, Loss: 1.6994824409484863, Final Batch Loss: 0.8637251257896423\n",
      "Epoch 154, Loss: 1.6871183514595032, Final Batch Loss: 0.814428985118866\n",
      "Epoch 155, Loss: 1.6181254982948303, Final Batch Loss: 0.7539814710617065\n",
      "Epoch 156, Loss: 1.5757234692573547, Final Batch Loss: 0.7961978316307068\n",
      "Epoch 157, Loss: 1.606416940689087, Final Batch Loss: 0.8256014585494995\n",
      "Epoch 158, Loss: 1.5990018248558044, Final Batch Loss: 0.7922946810722351\n",
      "Epoch 159, Loss: 1.5988758206367493, Final Batch Loss: 0.7591535449028015\n",
      "Epoch 160, Loss: 1.569170594215393, Final Batch Loss: 0.7745336890220642\n",
      "Epoch 161, Loss: 1.6563648581504822, Final Batch Loss: 0.8196751475334167\n",
      "Epoch 162, Loss: 1.6222280263900757, Final Batch Loss: 0.796210527420044\n",
      "Epoch 163, Loss: 1.6566925644874573, Final Batch Loss: 0.8573826551437378\n",
      "Epoch 164, Loss: 1.5623437762260437, Final Batch Loss: 0.7707891464233398\n",
      "Epoch 165, Loss: 1.569785714149475, Final Batch Loss: 0.7813341021537781\n",
      "Epoch 166, Loss: 1.5172628164291382, Final Batch Loss: 0.749305009841919\n",
      "Epoch 167, Loss: 1.5322601199150085, Final Batch Loss: 0.707686185836792\n",
      "Epoch 168, Loss: 1.597236454486847, Final Batch Loss: 0.7737352848052979\n",
      "Epoch 169, Loss: 1.5887665748596191, Final Batch Loss: 0.7711960077285767\n",
      "Epoch 170, Loss: 1.5832740664482117, Final Batch Loss: 0.7876883745193481\n",
      "Epoch 171, Loss: 1.5715341567993164, Final Batch Loss: 0.790087103843689\n",
      "Epoch 172, Loss: 1.5167364478111267, Final Batch Loss: 0.727750837802887\n",
      "Epoch 173, Loss: 1.5815497040748596, Final Batch Loss: 0.7744640111923218\n",
      "Epoch 174, Loss: 1.4704065322875977, Final Batch Loss: 0.7994235157966614\n",
      "Epoch 175, Loss: 1.5817039012908936, Final Batch Loss: 0.7992956638336182\n",
      "Epoch 176, Loss: 1.5355610847473145, Final Batch Loss: 0.7815396189689636\n",
      "Epoch 177, Loss: 1.485687792301178, Final Batch Loss: 0.7277106046676636\n",
      "Epoch 178, Loss: 1.6208873391151428, Final Batch Loss: 0.8295894265174866\n",
      "Epoch 179, Loss: 1.5461968779563904, Final Batch Loss: 0.7810772657394409\n",
      "Epoch 180, Loss: 1.4456013441085815, Final Batch Loss: 0.7387552857398987\n",
      "Epoch 181, Loss: 1.4641950130462646, Final Batch Loss: 0.6846936345100403\n",
      "Epoch 182, Loss: 1.563489854335785, Final Batch Loss: 0.745715856552124\n",
      "Epoch 183, Loss: 1.5429467558860779, Final Batch Loss: 0.7491168975830078\n",
      "Epoch 184, Loss: 1.5079012513160706, Final Batch Loss: 0.7371941208839417\n",
      "Epoch 185, Loss: 1.4738358855247498, Final Batch Loss: 0.7542620897293091\n",
      "Epoch 186, Loss: 1.5223429799079895, Final Batch Loss: 0.7518308162689209\n",
      "Epoch 187, Loss: 1.507581651210785, Final Batch Loss: 0.8066726326942444\n",
      "Epoch 188, Loss: 1.4920273423194885, Final Batch Loss: 0.7419240474700928\n",
      "Epoch 189, Loss: 1.4703629612922668, Final Batch Loss: 0.7411586046218872\n",
      "Epoch 190, Loss: 1.446704924106598, Final Batch Loss: 0.6976795196533203\n",
      "Epoch 191, Loss: 1.5091938972473145, Final Batch Loss: 0.7564193606376648\n",
      "Epoch 192, Loss: 1.470173180103302, Final Batch Loss: 0.7399904727935791\n",
      "Epoch 193, Loss: 1.5106184482574463, Final Batch Loss: 0.6817837953567505\n",
      "Epoch 194, Loss: 1.4562379717826843, Final Batch Loss: 0.7890980839729309\n",
      "Epoch 195, Loss: 1.4331377148628235, Final Batch Loss: 0.6806278824806213\n",
      "Epoch 196, Loss: 1.407982349395752, Final Batch Loss: 0.6749532222747803\n",
      "Epoch 197, Loss: 1.4558186531066895, Final Batch Loss: 0.7461642622947693\n",
      "Epoch 198, Loss: 1.4217707514762878, Final Batch Loss: 0.7335538864135742\n",
      "Epoch 199, Loss: 1.4474660158157349, Final Batch Loss: 0.7110539674758911\n",
      "Epoch 200, Loss: 1.4459196329116821, Final Batch Loss: 0.7341257333755493\n",
      "Epoch 201, Loss: 1.4158987998962402, Final Batch Loss: 0.7076635360717773\n",
      "Epoch 202, Loss: 1.436294674873352, Final Batch Loss: 0.7450062036514282\n",
      "Epoch 203, Loss: 1.4088865518569946, Final Batch Loss: 0.7653585076332092\n",
      "Epoch 204, Loss: 1.3894453644752502, Final Batch Loss: 0.7055517435073853\n",
      "Epoch 205, Loss: 1.4339395761489868, Final Batch Loss: 0.7117210030555725\n",
      "Epoch 206, Loss: 1.4610637426376343, Final Batch Loss: 0.6906746029853821\n",
      "Epoch 207, Loss: 1.3866062760353088, Final Batch Loss: 0.6532581448554993\n",
      "Epoch 208, Loss: 1.3810389637947083, Final Batch Loss: 0.695667564868927\n",
      "Epoch 209, Loss: 1.4095298051834106, Final Batch Loss: 0.6643551588058472\n",
      "Epoch 210, Loss: 1.4331568479537964, Final Batch Loss: 0.7369347214698792\n",
      "Epoch 211, Loss: 1.4236206412315369, Final Batch Loss: 0.7409477829933167\n",
      "Epoch 212, Loss: 1.3563756942749023, Final Batch Loss: 0.64374178647995\n",
      "Epoch 213, Loss: 1.4076007008552551, Final Batch Loss: 0.6812157034873962\n",
      "Epoch 214, Loss: 1.3876467943191528, Final Batch Loss: 0.7036116719245911\n",
      "Epoch 215, Loss: 1.369770109653473, Final Batch Loss: 0.6972663402557373\n",
      "Epoch 216, Loss: 1.348753809928894, Final Batch Loss: 0.6504315733909607\n",
      "Epoch 217, Loss: 1.3637173771858215, Final Batch Loss: 0.6864132285118103\n",
      "Epoch 218, Loss: 1.3255025744438171, Final Batch Loss: 0.6966344714164734\n",
      "Epoch 219, Loss: 1.2578632831573486, Final Batch Loss: 0.6036626696586609\n",
      "Epoch 220, Loss: 1.3715449571609497, Final Batch Loss: 0.7086209654808044\n",
      "Epoch 221, Loss: 1.3696213960647583, Final Batch Loss: 0.7218151092529297\n",
      "Epoch 222, Loss: 1.3924479484558105, Final Batch Loss: 0.6586157083511353\n",
      "Epoch 223, Loss: 1.2899377942085266, Final Batch Loss: 0.6363891363143921\n",
      "Epoch 224, Loss: 1.2902075052261353, Final Batch Loss: 0.6306736469268799\n",
      "Epoch 225, Loss: 1.3139621019363403, Final Batch Loss: 0.6933819055557251\n",
      "Epoch 226, Loss: 1.2620609402656555, Final Batch Loss: 0.6275321245193481\n",
      "Epoch 227, Loss: 1.278196394443512, Final Batch Loss: 0.6493038535118103\n",
      "Epoch 228, Loss: 1.3241029977798462, Final Batch Loss: 0.6634749174118042\n",
      "Epoch 229, Loss: 1.2592655420303345, Final Batch Loss: 0.6331819891929626\n",
      "Epoch 230, Loss: 1.2604135274887085, Final Batch Loss: 0.5990758538246155\n",
      "Epoch 231, Loss: 1.3314600586891174, Final Batch Loss: 0.681632936000824\n",
      "Epoch 232, Loss: 1.2669035196304321, Final Batch Loss: 0.6148414015769958\n",
      "Epoch 233, Loss: 1.2848676443099976, Final Batch Loss: 0.6239458322525024\n",
      "Epoch 234, Loss: 1.3181313276290894, Final Batch Loss: 0.6228716373443604\n",
      "Epoch 235, Loss: 1.2673847079277039, Final Batch Loss: 0.6208091378211975\n",
      "Epoch 236, Loss: 1.2684590816497803, Final Batch Loss: 0.6237788796424866\n",
      "Epoch 237, Loss: 1.228051781654358, Final Batch Loss: 0.6026108860969543\n",
      "Epoch 238, Loss: 1.2863624691963196, Final Batch Loss: 0.6281890273094177\n",
      "Epoch 239, Loss: 1.304150104522705, Final Batch Loss: 0.6637163758277893\n",
      "Epoch 240, Loss: 1.307444155216217, Final Batch Loss: 0.7152220606803894\n",
      "Epoch 241, Loss: 1.3028931617736816, Final Batch Loss: 0.6435583233833313\n",
      "Epoch 242, Loss: 1.241550326347351, Final Batch Loss: 0.6164738535881042\n",
      "Epoch 243, Loss: 1.327113926410675, Final Batch Loss: 0.6873873472213745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244, Loss: 1.2058669924736023, Final Batch Loss: 0.5826213359832764\n",
      "Epoch 245, Loss: 1.1934117674827576, Final Batch Loss: 0.6464788913726807\n",
      "Epoch 246, Loss: 1.1918118000030518, Final Batch Loss: 0.5681205987930298\n",
      "Epoch 247, Loss: 1.26023268699646, Final Batch Loss: 0.6751707792282104\n",
      "Epoch 248, Loss: 1.2140306234359741, Final Batch Loss: 0.538772702217102\n",
      "Epoch 249, Loss: 1.2125903964042664, Final Batch Loss: 0.5800142884254456\n",
      "Epoch 250, Loss: 1.233731210231781, Final Batch Loss: 0.6305013298988342\n",
      "Epoch 251, Loss: 1.2314144372940063, Final Batch Loss: 0.6735314130783081\n",
      "Epoch 252, Loss: 1.1863495111465454, Final Batch Loss: 0.5585834383964539\n",
      "Epoch 253, Loss: 1.1875471472740173, Final Batch Loss: 0.5617129802703857\n",
      "Epoch 254, Loss: 1.28131902217865, Final Batch Loss: 0.6297898888587952\n",
      "Epoch 255, Loss: 1.2461922764778137, Final Batch Loss: 0.6195780038833618\n",
      "Epoch 256, Loss: 1.2240864634513855, Final Batch Loss: 0.6364778876304626\n",
      "Epoch 257, Loss: 1.1861043572425842, Final Batch Loss: 0.5908997654914856\n",
      "Epoch 258, Loss: 1.2784302830696106, Final Batch Loss: 0.6442598104476929\n",
      "Epoch 259, Loss: 1.2286759614944458, Final Batch Loss: 0.603127121925354\n",
      "Epoch 260, Loss: 1.2669318914413452, Final Batch Loss: 0.6114608645439148\n",
      "Epoch 261, Loss: 1.2409663200378418, Final Batch Loss: 0.6340691447257996\n",
      "Epoch 262, Loss: 1.1466386318206787, Final Batch Loss: 0.5457998514175415\n",
      "Epoch 263, Loss: 1.2389892935752869, Final Batch Loss: 0.6265837550163269\n",
      "Epoch 264, Loss: 1.1936875581741333, Final Batch Loss: 0.6001436114311218\n",
      "Epoch 265, Loss: 1.2286652326583862, Final Batch Loss: 0.6133395433425903\n",
      "Epoch 266, Loss: 1.1580063104629517, Final Batch Loss: 0.5926889777183533\n",
      "Epoch 267, Loss: 1.2514696717262268, Final Batch Loss: 0.6737163066864014\n",
      "Epoch 268, Loss: 1.1431242227554321, Final Batch Loss: 0.5466858148574829\n",
      "Epoch 269, Loss: 1.1881174445152283, Final Batch Loss: 0.5815560817718506\n",
      "Epoch 270, Loss: 1.1767864227294922, Final Batch Loss: 0.6088124513626099\n",
      "Epoch 271, Loss: 1.135077178478241, Final Batch Loss: 0.597001850605011\n",
      "Epoch 272, Loss: 1.2358704209327698, Final Batch Loss: 0.6018149852752686\n",
      "Epoch 273, Loss: 1.1544222235679626, Final Batch Loss: 0.5481745600700378\n",
      "Epoch 274, Loss: 1.221143364906311, Final Batch Loss: 0.6587382555007935\n",
      "Epoch 275, Loss: 1.1969510912895203, Final Batch Loss: 0.6236696839332581\n",
      "Epoch 276, Loss: 1.1446998715400696, Final Batch Loss: 0.5371018052101135\n",
      "Epoch 277, Loss: 1.0903993248939514, Final Batch Loss: 0.55326247215271\n",
      "Epoch 278, Loss: 1.150153934955597, Final Batch Loss: 0.5611003041267395\n",
      "Epoch 279, Loss: 1.1675934195518494, Final Batch Loss: 0.5940024852752686\n",
      "Epoch 280, Loss: 1.185043454170227, Final Batch Loss: 0.586375892162323\n",
      "Epoch 281, Loss: 1.149444580078125, Final Batch Loss: 0.5798635482788086\n",
      "Epoch 282, Loss: 1.1395027041435242, Final Batch Loss: 0.5657727718353271\n",
      "Epoch 283, Loss: 1.1417291760444641, Final Batch Loss: 0.5275712013244629\n",
      "Epoch 284, Loss: 1.1955467462539673, Final Batch Loss: 0.5794950127601624\n",
      "Epoch 285, Loss: 1.1182818412780762, Final Batch Loss: 0.5646935701370239\n",
      "Epoch 286, Loss: 1.1098195314407349, Final Batch Loss: 0.5713415741920471\n",
      "Epoch 287, Loss: 1.0684838891029358, Final Batch Loss: 0.534488320350647\n",
      "Epoch 288, Loss: 1.1403090953826904, Final Batch Loss: 0.5464039444923401\n",
      "Epoch 289, Loss: 1.1190040707588196, Final Batch Loss: 0.5888357162475586\n",
      "Epoch 290, Loss: 1.075696051120758, Final Batch Loss: 0.5118058323860168\n",
      "Epoch 291, Loss: 1.2327954173088074, Final Batch Loss: 0.6113045811653137\n",
      "Epoch 292, Loss: 1.1280502080917358, Final Batch Loss: 0.5348972678184509\n",
      "Epoch 293, Loss: 1.1230064034461975, Final Batch Loss: 0.6225826740264893\n",
      "Epoch 294, Loss: 1.1727768182754517, Final Batch Loss: 0.5546411275863647\n",
      "Epoch 295, Loss: 1.167443871498108, Final Batch Loss: 0.5582758784294128\n",
      "Epoch 296, Loss: 1.140621304512024, Final Batch Loss: 0.5797616839408875\n",
      "Epoch 297, Loss: 1.0689516067504883, Final Batch Loss: 0.562543511390686\n",
      "Epoch 298, Loss: 1.1976104974746704, Final Batch Loss: 0.6058627963066101\n",
      "Epoch 299, Loss: 1.0919208526611328, Final Batch Loss: 0.559054434299469\n",
      "Epoch 300, Loss: 1.117876410484314, Final Batch Loss: 0.5805171728134155\n",
      "Epoch 301, Loss: 1.105746865272522, Final Batch Loss: 0.5294474959373474\n",
      "Epoch 302, Loss: 1.1559616327285767, Final Batch Loss: 0.5689719915390015\n",
      "Epoch 303, Loss: 1.0785463452339172, Final Batch Loss: 0.5682916641235352\n",
      "Epoch 304, Loss: 1.1520220637321472, Final Batch Loss: 0.6042337417602539\n",
      "Epoch 305, Loss: 1.0393946170806885, Final Batch Loss: 0.5174947381019592\n",
      "Epoch 306, Loss: 1.0319957733154297, Final Batch Loss: 0.4857134222984314\n",
      "Epoch 307, Loss: 1.123788595199585, Final Batch Loss: 0.614605724811554\n",
      "Epoch 308, Loss: 1.1121585965156555, Final Batch Loss: 0.5644818544387817\n",
      "Epoch 309, Loss: 1.0744532346725464, Final Batch Loss: 0.538966715335846\n",
      "Epoch 310, Loss: 1.0768413543701172, Final Batch Loss: 0.5202465653419495\n",
      "Epoch 311, Loss: 1.1197407245635986, Final Batch Loss: 0.599454939365387\n",
      "Epoch 312, Loss: 1.1063128113746643, Final Batch Loss: 0.5644665956497192\n",
      "Epoch 313, Loss: 1.0446286797523499, Final Batch Loss: 0.5389164090156555\n",
      "Epoch 314, Loss: 1.096510887145996, Final Batch Loss: 0.5503154993057251\n",
      "Epoch 315, Loss: 1.152486503124237, Final Batch Loss: 0.573059618473053\n",
      "Epoch 316, Loss: 1.0179469585418701, Final Batch Loss: 0.5079723000526428\n",
      "Epoch 317, Loss: 1.118029236793518, Final Batch Loss: 0.5513811707496643\n",
      "Epoch 318, Loss: 1.1300468444824219, Final Batch Loss: 0.5612121820449829\n",
      "Epoch 319, Loss: 1.0312066078186035, Final Batch Loss: 0.5142404437065125\n",
      "Epoch 320, Loss: 1.056772768497467, Final Batch Loss: 0.5534067749977112\n",
      "Epoch 321, Loss: 1.0772354006767273, Final Batch Loss: 0.5116704702377319\n",
      "Epoch 322, Loss: 1.024535357952118, Final Batch Loss: 0.5006328225135803\n",
      "Epoch 323, Loss: 1.1094249486923218, Final Batch Loss: 0.5354896783828735\n",
      "Epoch 324, Loss: 1.0108685195446014, Final Batch Loss: 0.5157344937324524\n",
      "Epoch 325, Loss: 1.0462167263031006, Final Batch Loss: 0.513968825340271\n",
      "Epoch 326, Loss: 1.0333539247512817, Final Batch Loss: 0.5161559581756592\n",
      "Epoch 327, Loss: 1.0971416234970093, Final Batch Loss: 0.5014336109161377\n",
      "Epoch 328, Loss: 1.1350233852863312, Final Batch Loss: 0.652622640132904\n",
      "Epoch 329, Loss: 1.116756021976471, Final Batch Loss: 0.5165320634841919\n",
      "Epoch 330, Loss: 1.0222260355949402, Final Batch Loss: 0.5265220999717712\n",
      "Epoch 331, Loss: 1.0315308570861816, Final Batch Loss: 0.5262045860290527\n",
      "Epoch 332, Loss: 1.0730589628219604, Final Batch Loss: 0.48267900943756104\n",
      "Epoch 333, Loss: 1.0623304843902588, Final Batch Loss: 0.5534732937812805\n",
      "Epoch 334, Loss: 1.1048880219459534, Final Batch Loss: 0.5445775389671326\n",
      "Epoch 335, Loss: 1.131105899810791, Final Batch Loss: 0.548454225063324\n",
      "Epoch 336, Loss: 1.0266913771629333, Final Batch Loss: 0.49150794744491577\n",
      "Epoch 337, Loss: 1.0791288614273071, Final Batch Loss: 0.5866870880126953\n",
      "Epoch 338, Loss: 0.9840549528598785, Final Batch Loss: 0.4625493586063385\n",
      "Epoch 339, Loss: 1.0356733202934265, Final Batch Loss: 0.5330604314804077\n",
      "Epoch 340, Loss: 1.0475682616233826, Final Batch Loss: 0.5335511565208435\n",
      "Epoch 341, Loss: 1.0579399764537811, Final Batch Loss: 0.5744948983192444\n",
      "Epoch 342, Loss: 1.0402724146842957, Final Batch Loss: 0.4748031497001648\n",
      "Epoch 343, Loss: 1.0313738286495209, Final Batch Loss: 0.48416760563850403\n",
      "Epoch 344, Loss: 1.0185407996177673, Final Batch Loss: 0.4988960027694702\n",
      "Epoch 345, Loss: 1.090493083000183, Final Batch Loss: 0.5642939805984497\n",
      "Epoch 346, Loss: 1.070997416973114, Final Batch Loss: 0.5003882646560669\n",
      "Epoch 347, Loss: 1.0213607251644135, Final Batch Loss: 0.4649270474910736\n",
      "Epoch 348, Loss: 1.0114202201366425, Final Batch Loss: 0.49362459778785706\n",
      "Epoch 349, Loss: 1.0578644275665283, Final Batch Loss: 0.529100239276886\n",
      "Epoch 350, Loss: 1.0333069860935211, Final Batch Loss: 0.5556383728981018\n",
      "Epoch 351, Loss: 1.0328733921051025, Final Batch Loss: 0.5242132544517517\n",
      "Epoch 352, Loss: 1.0276464223861694, Final Batch Loss: 0.4892197251319885\n",
      "Epoch 353, Loss: 0.9817692041397095, Final Batch Loss: 0.5129292011260986\n",
      "Epoch 354, Loss: 0.9621562659740448, Final Batch Loss: 0.4978015720844269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355, Loss: 1.0231837630271912, Final Batch Loss: 0.535535991191864\n",
      "Epoch 356, Loss: 0.9690931439399719, Final Batch Loss: 0.5007131099700928\n",
      "Epoch 357, Loss: 0.9507094919681549, Final Batch Loss: 0.5146757960319519\n",
      "Epoch 358, Loss: 1.0000810325145721, Final Batch Loss: 0.50295090675354\n",
      "Epoch 359, Loss: 1.022647500038147, Final Batch Loss: 0.51517254114151\n",
      "Epoch 360, Loss: 0.9680179059505463, Final Batch Loss: 0.4493478834629059\n",
      "Epoch 361, Loss: 0.988116443157196, Final Batch Loss: 0.49528786540031433\n",
      "Epoch 362, Loss: 1.0554065108299255, Final Batch Loss: 0.5592940449714661\n",
      "Epoch 363, Loss: 1.0782727003097534, Final Batch Loss: 0.53895103931427\n",
      "Epoch 364, Loss: 1.0100739300251007, Final Batch Loss: 0.5323212146759033\n",
      "Epoch 365, Loss: 0.9792203307151794, Final Batch Loss: 0.44775068759918213\n",
      "Epoch 366, Loss: 0.9841427206993103, Final Batch Loss: 0.49913322925567627\n",
      "Epoch 367, Loss: 0.9653211534023285, Final Batch Loss: 0.4607555568218231\n",
      "Epoch 368, Loss: 1.0206288695335388, Final Batch Loss: 0.5277539491653442\n",
      "Epoch 369, Loss: 1.0807759761810303, Final Batch Loss: 0.5900903940200806\n",
      "Epoch 370, Loss: 0.9334447085857391, Final Batch Loss: 0.43073466420173645\n",
      "Epoch 371, Loss: 0.9985212087631226, Final Batch Loss: 0.4696466326713562\n",
      "Epoch 372, Loss: 0.9641814827919006, Final Batch Loss: 0.5248247981071472\n",
      "Epoch 373, Loss: 1.0431020557880402, Final Batch Loss: 0.5573705434799194\n",
      "Epoch 374, Loss: 1.061964988708496, Final Batch Loss: 0.5228895545005798\n",
      "Epoch 375, Loss: 1.002626895904541, Final Batch Loss: 0.4869920611381531\n",
      "Epoch 376, Loss: 0.9795881807804108, Final Batch Loss: 0.49132487177848816\n",
      "Epoch 377, Loss: 1.0373615622520447, Final Batch Loss: 0.512319803237915\n",
      "Epoch 378, Loss: 0.9732864797115326, Final Batch Loss: 0.49256888031959534\n",
      "Epoch 379, Loss: 0.9462066888809204, Final Batch Loss: 0.4859582781791687\n",
      "Epoch 380, Loss: 0.9914392828941345, Final Batch Loss: 0.511339008808136\n",
      "Epoch 381, Loss: 0.9729015827178955, Final Batch Loss: 0.47531765699386597\n",
      "Epoch 382, Loss: 1.0143949091434479, Final Batch Loss: 0.46343472599983215\n",
      "Epoch 383, Loss: 0.9597710967063904, Final Batch Loss: 0.4723436236381531\n",
      "Epoch 384, Loss: 1.0133325457572937, Final Batch Loss: 0.5024608969688416\n",
      "Epoch 385, Loss: 0.9580824375152588, Final Batch Loss: 0.5004000067710876\n",
      "Epoch 386, Loss: 1.0198892951011658, Final Batch Loss: 0.5025929808616638\n",
      "Epoch 387, Loss: 0.9711440801620483, Final Batch Loss: 0.5002564787864685\n",
      "Epoch 388, Loss: 0.9936258792877197, Final Batch Loss: 0.480817973613739\n",
      "Epoch 389, Loss: 0.9364838600158691, Final Batch Loss: 0.45771872997283936\n",
      "Epoch 390, Loss: 1.091007113456726, Final Batch Loss: 0.5971516370773315\n",
      "Epoch 391, Loss: 0.9573715925216675, Final Batch Loss: 0.489688903093338\n",
      "Epoch 392, Loss: 0.9594805538654327, Final Batch Loss: 0.5093129277229309\n",
      "Epoch 393, Loss: 0.88755002617836, Final Batch Loss: 0.44887956976890564\n",
      "Epoch 394, Loss: 0.9483574628829956, Final Batch Loss: 0.43493330478668213\n",
      "Epoch 395, Loss: 0.9481775760650635, Final Batch Loss: 0.41566646099090576\n",
      "Epoch 396, Loss: 1.0067937076091766, Final Batch Loss: 0.48330649733543396\n",
      "Epoch 397, Loss: 0.9769165813922882, Final Batch Loss: 0.5267782807350159\n",
      "Epoch 398, Loss: 0.9796863198280334, Final Batch Loss: 0.5047321915626526\n",
      "Epoch 399, Loss: 0.866038590669632, Final Batch Loss: 0.45697489380836487\n",
      "Epoch 400, Loss: 0.9847761988639832, Final Batch Loss: 0.5086852312088013\n",
      "Epoch 401, Loss: 1.013927698135376, Final Batch Loss: 0.5193474292755127\n",
      "Epoch 402, Loss: 1.002629965543747, Final Batch Loss: 0.43674758076667786\n",
      "Epoch 403, Loss: 0.9681574702262878, Final Batch Loss: 0.4750940501689911\n",
      "Epoch 404, Loss: 0.9328504800796509, Final Batch Loss: 0.47681939601898193\n",
      "Epoch 405, Loss: 0.9108159840106964, Final Batch Loss: 0.4468773901462555\n",
      "Epoch 406, Loss: 0.9324627816677094, Final Batch Loss: 0.4724745750427246\n",
      "Epoch 407, Loss: 0.9396972954273224, Final Batch Loss: 0.48816680908203125\n",
      "Epoch 408, Loss: 0.9501875936985016, Final Batch Loss: 0.4885590672492981\n",
      "Epoch 409, Loss: 0.9322375357151031, Final Batch Loss: 0.44838637113571167\n",
      "Epoch 410, Loss: 0.9253265857696533, Final Batch Loss: 0.4006693959236145\n",
      "Epoch 411, Loss: 0.8687488436698914, Final Batch Loss: 0.42661190032958984\n",
      "Epoch 412, Loss: 0.9434904456138611, Final Batch Loss: 0.4426015019416809\n",
      "Epoch 413, Loss: 1.0168807208538055, Final Batch Loss: 0.5460360050201416\n",
      "Epoch 414, Loss: 0.9467743933200836, Final Batch Loss: 0.4716342091560364\n",
      "Epoch 415, Loss: 0.9592507481575012, Final Batch Loss: 0.4959214925765991\n",
      "Epoch 416, Loss: 0.9161295294761658, Final Batch Loss: 0.48368769884109497\n",
      "Epoch 417, Loss: 0.924338310956955, Final Batch Loss: 0.43109872937202454\n",
      "Epoch 418, Loss: 0.9082168638706207, Final Batch Loss: 0.4829420745372772\n",
      "Epoch 419, Loss: 0.905156821012497, Final Batch Loss: 0.46763497591018677\n",
      "Epoch 420, Loss: 0.9430953860282898, Final Batch Loss: 0.47103366255760193\n",
      "Epoch 421, Loss: 0.9022716283798218, Final Batch Loss: 0.4704822301864624\n",
      "Epoch 422, Loss: 0.9942077398300171, Final Batch Loss: 0.5159909129142761\n",
      "Epoch 423, Loss: 0.882358729839325, Final Batch Loss: 0.3908146917819977\n",
      "Epoch 424, Loss: 0.9413363635540009, Final Batch Loss: 0.44157785177230835\n",
      "Epoch 425, Loss: 0.8669266700744629, Final Batch Loss: 0.4513837397098541\n",
      "Epoch 426, Loss: 0.90391606092453, Final Batch Loss: 0.45586851239204407\n",
      "Epoch 427, Loss: 0.8708008527755737, Final Batch Loss: 0.4711751639842987\n",
      "Epoch 428, Loss: 0.9204767644405365, Final Batch Loss: 0.4994741976261139\n",
      "Epoch 429, Loss: 0.9269771575927734, Final Batch Loss: 0.43638312816619873\n",
      "Epoch 430, Loss: 0.9834984838962555, Final Batch Loss: 0.46280238032341003\n",
      "Epoch 431, Loss: 0.9772606790065765, Final Batch Loss: 0.5150259733200073\n",
      "Epoch 432, Loss: 0.9590240716934204, Final Batch Loss: 0.4464913010597229\n",
      "Epoch 433, Loss: 1.0112515091896057, Final Batch Loss: 0.5382753014564514\n",
      "Epoch 434, Loss: 0.9189830422401428, Final Batch Loss: 0.4771227240562439\n",
      "Epoch 435, Loss: 0.9375244677066803, Final Batch Loss: 0.4652884900569916\n",
      "Epoch 436, Loss: 0.8557209372520447, Final Batch Loss: 0.48673397302627563\n",
      "Epoch 437, Loss: 0.9261124134063721, Final Batch Loss: 0.43260636925697327\n",
      "Epoch 438, Loss: 0.8882172703742981, Final Batch Loss: 0.4527706503868103\n",
      "Epoch 439, Loss: 0.9826265573501587, Final Batch Loss: 0.5118011832237244\n",
      "Epoch 440, Loss: 0.873788982629776, Final Batch Loss: 0.41844403743743896\n",
      "Epoch 441, Loss: 0.8856714963912964, Final Batch Loss: 0.45522961020469666\n",
      "Epoch 442, Loss: 0.9427459836006165, Final Batch Loss: 0.4602154493331909\n",
      "Epoch 443, Loss: 0.9107385873794556, Final Batch Loss: 0.4660307466983795\n",
      "Epoch 444, Loss: 0.9001915752887726, Final Batch Loss: 0.4599067270755768\n",
      "Epoch 445, Loss: 0.980823427438736, Final Batch Loss: 0.5425793528556824\n",
      "Epoch 446, Loss: 0.897199958562851, Final Batch Loss: 0.46521154046058655\n",
      "Epoch 447, Loss: 0.8747484087944031, Final Batch Loss: 0.4226551353931427\n",
      "Epoch 448, Loss: 0.8582363724708557, Final Batch Loss: 0.39862746000289917\n",
      "Epoch 449, Loss: 0.8758251667022705, Final Batch Loss: 0.4151400923728943\n",
      "Epoch 450, Loss: 0.9488036632537842, Final Batch Loss: 0.4720599055290222\n",
      "Epoch 451, Loss: 0.8697071671485901, Final Batch Loss: 0.41092315316200256\n",
      "Epoch 452, Loss: 0.8621101379394531, Final Batch Loss: 0.4218117594718933\n",
      "Epoch 453, Loss: 0.945419043302536, Final Batch Loss: 0.4559701383113861\n",
      "Epoch 454, Loss: 0.8985909521579742, Final Batch Loss: 0.4440927505493164\n",
      "Epoch 455, Loss: 0.8691809177398682, Final Batch Loss: 0.427071213722229\n",
      "Epoch 456, Loss: 0.8466850221157074, Final Batch Loss: 0.4403352439403534\n",
      "Epoch 457, Loss: 0.9702842831611633, Final Batch Loss: 0.48919036984443665\n",
      "Epoch 458, Loss: 0.8988389074802399, Final Batch Loss: 0.41514208912849426\n",
      "Epoch 459, Loss: 0.8706434667110443, Final Batch Loss: 0.41116735339164734\n",
      "Epoch 460, Loss: 0.9307973682880402, Final Batch Loss: 0.4656358063220978\n",
      "Epoch 461, Loss: 0.9563986957073212, Final Batch Loss: 0.4531906545162201\n",
      "Epoch 462, Loss: 0.8970196843147278, Final Batch Loss: 0.4080997705459595\n",
      "Epoch 463, Loss: 0.8817808330059052, Final Batch Loss: 0.4721117913722992\n",
      "Epoch 464, Loss: 0.8202807009220123, Final Batch Loss: 0.4382007420063019\n",
      "Epoch 465, Loss: 0.8391379714012146, Final Batch Loss: 0.4276615381240845\n",
      "Epoch 466, Loss: 0.9025356769561768, Final Batch Loss: 0.5089884400367737\n",
      "Epoch 467, Loss: 0.843497633934021, Final Batch Loss: 0.44171714782714844\n",
      "Epoch 468, Loss: 0.934543639421463, Final Batch Loss: 0.44722801446914673\n",
      "Epoch 469, Loss: 0.9242824912071228, Final Batch Loss: 0.4554844796657562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470, Loss: 0.9214863479137421, Final Batch Loss: 0.467721164226532\n",
      "Epoch 471, Loss: 0.8619905412197113, Final Batch Loss: 0.4509972333908081\n",
      "Epoch 472, Loss: 0.8962275683879852, Final Batch Loss: 0.4404943287372589\n",
      "Epoch 473, Loss: 0.8450388014316559, Final Batch Loss: 0.44818374514579773\n",
      "Epoch 474, Loss: 0.8845400214195251, Final Batch Loss: 0.4335733950138092\n",
      "Epoch 475, Loss: 0.8856377005577087, Final Batch Loss: 0.44913631677627563\n",
      "Epoch 476, Loss: 0.9163255393505096, Final Batch Loss: 0.4453997313976288\n",
      "Epoch 477, Loss: 0.8480695486068726, Final Batch Loss: 0.42566829919815063\n",
      "Epoch 478, Loss: 0.9067970514297485, Final Batch Loss: 0.4427529275417328\n",
      "Epoch 479, Loss: 0.8670685887336731, Final Batch Loss: 0.4476960301399231\n",
      "Epoch 480, Loss: 0.8606680929660797, Final Batch Loss: 0.41169965267181396\n",
      "Epoch 481, Loss: 0.8794971704483032, Final Batch Loss: 0.42967337369918823\n",
      "Epoch 482, Loss: 0.831297367811203, Final Batch Loss: 0.4555510878562927\n",
      "Epoch 483, Loss: 0.9379635453224182, Final Batch Loss: 0.47573310136795044\n",
      "Epoch 484, Loss: 0.8987776041030884, Final Batch Loss: 0.4574146270751953\n",
      "Epoch 485, Loss: 0.8331464529037476, Final Batch Loss: 0.38128677010536194\n",
      "Epoch 486, Loss: 0.9380346238613129, Final Batch Loss: 0.47845888137817383\n",
      "Epoch 487, Loss: 0.9152131378650665, Final Batch Loss: 0.49232810735702515\n",
      "Epoch 488, Loss: 0.8577536046504974, Final Batch Loss: 0.4651355445384979\n",
      "Epoch 489, Loss: 0.8674369752407074, Final Batch Loss: 0.42359161376953125\n",
      "Epoch 490, Loss: 0.8635023832321167, Final Batch Loss: 0.42931053042411804\n",
      "Epoch 491, Loss: 0.8603319227695465, Final Batch Loss: 0.4450888931751251\n",
      "Epoch 492, Loss: 0.827824205160141, Final Batch Loss: 0.42517173290252686\n",
      "Epoch 493, Loss: 0.859205573797226, Final Batch Loss: 0.42634767293930054\n",
      "Epoch 494, Loss: 0.9321286678314209, Final Batch Loss: 0.5106128454208374\n",
      "Epoch 495, Loss: 0.8394162654876709, Final Batch Loss: 0.46531766653060913\n",
      "Epoch 496, Loss: 0.8400164842605591, Final Batch Loss: 0.41837674379348755\n",
      "Epoch 497, Loss: 0.8800458312034607, Final Batch Loss: 0.45970290899276733\n",
      "Epoch 498, Loss: 0.8224257528781891, Final Batch Loss: 0.3946242034435272\n",
      "Epoch 499, Loss: 0.873594343662262, Final Batch Loss: 0.4591110646724701\n",
      "Epoch 500, Loss: 0.886205404996872, Final Batch Loss: 0.4349057078361511\n",
      "Epoch 501, Loss: 0.818851113319397, Final Batch Loss: 0.3811192512512207\n",
      "Epoch 502, Loss: 0.8225369453430176, Final Batch Loss: 0.4202098548412323\n",
      "Epoch 503, Loss: 0.8626770675182343, Final Batch Loss: 0.4439857304096222\n",
      "Epoch 504, Loss: 0.8694073259830475, Final Batch Loss: 0.41671818494796753\n",
      "Epoch 505, Loss: 0.837324857711792, Final Batch Loss: 0.39724481105804443\n",
      "Epoch 506, Loss: 0.9221726059913635, Final Batch Loss: 0.4715544581413269\n",
      "Epoch 507, Loss: 0.90767902135849, Final Batch Loss: 0.45001477003097534\n",
      "Epoch 508, Loss: 0.9179063737392426, Final Batch Loss: 0.4743109345436096\n",
      "Epoch 509, Loss: 0.8874129056930542, Final Batch Loss: 0.43837201595306396\n",
      "Epoch 510, Loss: 0.8467237651348114, Final Batch Loss: 0.43304482102394104\n",
      "Epoch 511, Loss: 0.8047534227371216, Final Batch Loss: 0.40224459767341614\n",
      "Epoch 512, Loss: 0.803365021944046, Final Batch Loss: 0.4277772307395935\n",
      "Epoch 513, Loss: 0.8684093952178955, Final Batch Loss: 0.43106809258461\n",
      "Epoch 514, Loss: 0.8064940869808197, Final Batch Loss: 0.36172714829444885\n",
      "Epoch 515, Loss: 0.8339382410049438, Final Batch Loss: 0.460581511259079\n",
      "Epoch 516, Loss: 0.8281253576278687, Final Batch Loss: 0.419100821018219\n",
      "Epoch 517, Loss: 0.890969455242157, Final Batch Loss: 0.490463525056839\n",
      "Epoch 518, Loss: 0.8988797664642334, Final Batch Loss: 0.4752517342567444\n",
      "Epoch 519, Loss: 0.812026172876358, Final Batch Loss: 0.41517728567123413\n",
      "Epoch 520, Loss: 0.8068612813949585, Final Batch Loss: 0.42448779940605164\n",
      "Epoch 521, Loss: 0.865113377571106, Final Batch Loss: 0.4366846978664398\n",
      "Epoch 522, Loss: 0.8294865787029266, Final Batch Loss: 0.4113973379135132\n",
      "Epoch 523, Loss: 0.8271574676036835, Final Batch Loss: 0.3706364631652832\n",
      "Epoch 524, Loss: 0.8044913113117218, Final Batch Loss: 0.4143190085887909\n",
      "Epoch 525, Loss: 0.8528872132301331, Final Batch Loss: 0.40945425629615784\n",
      "Epoch 526, Loss: 0.848848432302475, Final Batch Loss: 0.38748031854629517\n",
      "Epoch 527, Loss: 0.8272183239459991, Final Batch Loss: 0.3878409266471863\n",
      "Epoch 528, Loss: 0.8349160850048065, Final Batch Loss: 0.41900765895843506\n",
      "Epoch 529, Loss: 0.8501860797405243, Final Batch Loss: 0.47045227885246277\n",
      "Epoch 530, Loss: 0.853935182094574, Final Batch Loss: 0.42309173941612244\n",
      "Epoch 531, Loss: 0.8436025977134705, Final Batch Loss: 0.41215479373931885\n",
      "Epoch 532, Loss: 0.7498842775821686, Final Batch Loss: 0.3744485676288605\n",
      "Epoch 533, Loss: 0.7725397348403931, Final Batch Loss: 0.37487250566482544\n",
      "Epoch 534, Loss: 0.7599906623363495, Final Batch Loss: 0.3633171021938324\n",
      "Epoch 535, Loss: 0.8711381554603577, Final Batch Loss: 0.41765889525413513\n",
      "Epoch 536, Loss: 0.8277441263198853, Final Batch Loss: 0.4768688380718231\n",
      "Epoch 537, Loss: 0.8323118388652802, Final Batch Loss: 0.42084425687789917\n",
      "Epoch 538, Loss: 0.8019556105136871, Final Batch Loss: 0.3776029050350189\n",
      "Epoch 539, Loss: 0.8429664075374603, Final Batch Loss: 0.4190348982810974\n",
      "Epoch 540, Loss: 0.8560895621776581, Final Batch Loss: 0.4244319796562195\n",
      "Epoch 541, Loss: 0.8341745138168335, Final Batch Loss: 0.46513524651527405\n",
      "Epoch 542, Loss: 0.8791285455226898, Final Batch Loss: 0.45797762274742126\n",
      "Epoch 543, Loss: 0.8986644148826599, Final Batch Loss: 0.4066959023475647\n",
      "Epoch 544, Loss: 0.814398467540741, Final Batch Loss: 0.44075241684913635\n",
      "Epoch 545, Loss: 0.8143439590930939, Final Batch Loss: 0.4107370674610138\n",
      "Epoch 546, Loss: 0.7891656160354614, Final Batch Loss: 0.38315722346305847\n",
      "Epoch 547, Loss: 0.8970789015293121, Final Batch Loss: 0.45058202743530273\n",
      "Epoch 548, Loss: 0.8620399832725525, Final Batch Loss: 0.40370380878448486\n",
      "Epoch 549, Loss: 0.8113240599632263, Final Batch Loss: 0.46429115533828735\n",
      "Epoch 550, Loss: 0.8441900908946991, Final Batch Loss: 0.40497130155563354\n",
      "Epoch 551, Loss: 0.8145249783992767, Final Batch Loss: 0.4244095981121063\n",
      "Epoch 552, Loss: 0.8947188854217529, Final Batch Loss: 0.4650188982486725\n",
      "Epoch 553, Loss: 0.8134124279022217, Final Batch Loss: 0.41598331928253174\n",
      "Epoch 554, Loss: 0.7689622640609741, Final Batch Loss: 0.3721576929092407\n",
      "Epoch 555, Loss: 0.8262339532375336, Final Batch Loss: 0.3933655917644501\n",
      "Epoch 556, Loss: 0.8270286321640015, Final Batch Loss: 0.42133334279060364\n",
      "Epoch 557, Loss: 0.7671134769916534, Final Batch Loss: 0.39087677001953125\n",
      "Epoch 558, Loss: 0.8267949819564819, Final Batch Loss: 0.38356420397758484\n",
      "Epoch 559, Loss: 0.7212014496326447, Final Batch Loss: 0.3549192547798157\n",
      "Epoch 560, Loss: 0.8112696707248688, Final Batch Loss: 0.40858423709869385\n",
      "Epoch 561, Loss: 0.8286369740962982, Final Batch Loss: 0.4472295641899109\n",
      "Epoch 562, Loss: 0.7399218380451202, Final Batch Loss: 0.3889854848384857\n",
      "Epoch 563, Loss: 0.8483261168003082, Final Batch Loss: 0.377268522977829\n",
      "Epoch 564, Loss: 0.8303621709346771, Final Batch Loss: 0.39857593178749084\n",
      "Epoch 565, Loss: 0.8479707539081573, Final Batch Loss: 0.4635484516620636\n",
      "Epoch 566, Loss: 0.7343507409095764, Final Batch Loss: 0.3324524760246277\n",
      "Epoch 567, Loss: 0.8013487458229065, Final Batch Loss: 0.439293771982193\n",
      "Epoch 568, Loss: 0.7266097664833069, Final Batch Loss: 0.3600574731826782\n",
      "Epoch 569, Loss: 0.8269844949245453, Final Batch Loss: 0.44065970182418823\n",
      "Epoch 570, Loss: 0.8253333568572998, Final Batch Loss: 0.4489791691303253\n",
      "Epoch 571, Loss: 0.8502303957939148, Final Batch Loss: 0.46201640367507935\n",
      "Epoch 572, Loss: 0.7401184141635895, Final Batch Loss: 0.3708220422267914\n",
      "Epoch 573, Loss: 0.7999716401100159, Final Batch Loss: 0.4191758334636688\n",
      "Epoch 574, Loss: 0.8148271143436432, Final Batch Loss: 0.40166860818862915\n",
      "Epoch 575, Loss: 0.8116427659988403, Final Batch Loss: 0.3975668251514435\n",
      "Epoch 576, Loss: 0.8030364811420441, Final Batch Loss: 0.38770729303359985\n",
      "Epoch 577, Loss: 0.906176894903183, Final Batch Loss: 0.3998934328556061\n",
      "Epoch 578, Loss: 0.8014993667602539, Final Batch Loss: 0.4503936767578125\n",
      "Epoch 579, Loss: 0.7774145007133484, Final Batch Loss: 0.38216108083724976\n",
      "Epoch 580, Loss: 0.8001560866832733, Final Batch Loss: 0.37394270300865173\n",
      "Epoch 581, Loss: 0.803363710641861, Final Batch Loss: 0.4226013720035553\n",
      "Epoch 582, Loss: 0.7712573409080505, Final Batch Loss: 0.36482366919517517\n",
      "Epoch 583, Loss: 0.7633166015148163, Final Batch Loss: 0.3391191065311432\n",
      "Epoch 584, Loss: 0.8185699284076691, Final Batch Loss: 0.43760332465171814\n",
      "Epoch 585, Loss: 0.7555687129497528, Final Batch Loss: 0.33347833156585693\n",
      "Epoch 586, Loss: 0.8382478058338165, Final Batch Loss: 0.39998069405555725\n",
      "Epoch 587, Loss: 0.7778134346008301, Final Batch Loss: 0.35451969504356384\n",
      "Epoch 588, Loss: 0.7512390613555908, Final Batch Loss: 0.33996835350990295\n",
      "Epoch 589, Loss: 0.804491400718689, Final Batch Loss: 0.38632968068122864\n",
      "Epoch 590, Loss: 0.7502714693546295, Final Batch Loss: 0.3816956877708435\n",
      "Epoch 591, Loss: 0.7051828503608704, Final Batch Loss: 0.33844321966171265\n",
      "Epoch 592, Loss: 0.7703258693218231, Final Batch Loss: 0.39713573455810547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593, Loss: 0.7490686774253845, Final Batch Loss: 0.354629248380661\n",
      "Epoch 594, Loss: 0.7858822643756866, Final Batch Loss: 0.40095677971839905\n",
      "Epoch 595, Loss: 0.7917907536029816, Final Batch Loss: 0.3827800452709198\n",
      "Epoch 596, Loss: 0.8514958322048187, Final Batch Loss: 0.3988550305366516\n",
      "Epoch 597, Loss: 0.7912272214889526, Final Batch Loss: 0.3881467580795288\n",
      "Epoch 598, Loss: 0.7375532686710358, Final Batch Loss: 0.38700854778289795\n",
      "Epoch 599, Loss: 0.7874840497970581, Final Batch Loss: 0.44584375619888306\n",
      "Epoch 600, Loss: 0.8237842619419098, Final Batch Loss: 0.46530088782310486\n",
      "Epoch 601, Loss: 0.7888162434101105, Final Batch Loss: 0.3798232674598694\n",
      "Epoch 602, Loss: 0.7716768085956573, Final Batch Loss: 0.4263773560523987\n",
      "Epoch 603, Loss: 0.7005947828292847, Final Batch Loss: 0.3207402229309082\n",
      "Epoch 604, Loss: 0.7346574366092682, Final Batch Loss: 0.35350501537323\n",
      "Epoch 605, Loss: 0.8321233689785004, Final Batch Loss: 0.40553194284439087\n",
      "Epoch 606, Loss: 0.7313264906406403, Final Batch Loss: 0.29484447836875916\n",
      "Epoch 607, Loss: 0.8068591356277466, Final Batch Loss: 0.38385453820228577\n",
      "Epoch 608, Loss: 0.7643910646438599, Final Batch Loss: 0.4317956566810608\n",
      "Epoch 609, Loss: 0.7820928394794464, Final Batch Loss: 0.4041966497898102\n",
      "Epoch 610, Loss: 0.7565881609916687, Final Batch Loss: 0.3653445243835449\n",
      "Epoch 611, Loss: 0.7704735100269318, Final Batch Loss: 0.3824840784072876\n",
      "Epoch 612, Loss: 0.7287608981132507, Final Batch Loss: 0.33895236253738403\n",
      "Epoch 613, Loss: 0.7514712810516357, Final Batch Loss: 0.3490111231803894\n",
      "Epoch 614, Loss: 0.8221020698547363, Final Batch Loss: 0.43160396814346313\n",
      "Epoch 615, Loss: 0.7917966246604919, Final Batch Loss: 0.3866601288318634\n",
      "Epoch 616, Loss: 0.8207615315914154, Final Batch Loss: 0.4283086359500885\n",
      "Epoch 617, Loss: 0.8425673842430115, Final Batch Loss: 0.38929998874664307\n",
      "Epoch 618, Loss: 0.7468381822109222, Final Batch Loss: 0.3779221177101135\n",
      "Epoch 619, Loss: 0.789874792098999, Final Batch Loss: 0.41535452008247375\n",
      "Epoch 620, Loss: 0.7649229168891907, Final Batch Loss: 0.4257480800151825\n",
      "Epoch 621, Loss: 0.7732882499694824, Final Batch Loss: 0.3941304087638855\n",
      "Epoch 622, Loss: 0.7483700513839722, Final Batch Loss: 0.3180672824382782\n",
      "Epoch 623, Loss: 0.7772914171218872, Final Batch Loss: 0.3726096451282501\n",
      "Epoch 624, Loss: 0.803752064704895, Final Batch Loss: 0.39471501111984253\n",
      "Epoch 625, Loss: 0.7745228707790375, Final Batch Loss: 0.3796048164367676\n",
      "Epoch 626, Loss: 0.8304823637008667, Final Batch Loss: 0.42979490756988525\n",
      "Epoch 627, Loss: 0.7479999661445618, Final Batch Loss: 0.4174318313598633\n",
      "Epoch 628, Loss: 0.7623827457427979, Final Batch Loss: 0.35170868039131165\n",
      "Epoch 629, Loss: 0.7108542323112488, Final Batch Loss: 0.36165520548820496\n",
      "Epoch 630, Loss: 0.719440221786499, Final Batch Loss: 0.3272581100463867\n",
      "Epoch 631, Loss: 0.7068121433258057, Final Batch Loss: 0.34228160977363586\n",
      "Epoch 632, Loss: 0.7206931710243225, Final Batch Loss: 0.36160358786582947\n",
      "Epoch 633, Loss: 0.7481708526611328, Final Batch Loss: 0.3600161671638489\n",
      "Epoch 634, Loss: 0.7893624901771545, Final Batch Loss: 0.37102797627449036\n",
      "Epoch 635, Loss: 0.743251770734787, Final Batch Loss: 0.3737858235836029\n",
      "Epoch 636, Loss: 0.7322010397911072, Final Batch Loss: 0.4201436936855316\n",
      "Epoch 637, Loss: 0.7651013731956482, Final Batch Loss: 0.3716803193092346\n",
      "Epoch 638, Loss: 0.7178520262241364, Final Batch Loss: 0.4041300117969513\n",
      "Epoch 639, Loss: 0.7600787878036499, Final Batch Loss: 0.3345853388309479\n",
      "Epoch 640, Loss: 0.765337347984314, Final Batch Loss: 0.40406185388565063\n",
      "Epoch 641, Loss: 0.7844056487083435, Final Batch Loss: 0.42709147930145264\n",
      "Epoch 642, Loss: 0.8081998825073242, Final Batch Loss: 0.4018808901309967\n",
      "Epoch 643, Loss: 0.8517596423625946, Final Batch Loss: 0.39163321256637573\n",
      "Epoch 644, Loss: 0.7031847834587097, Final Batch Loss: 0.3179507553577423\n",
      "Epoch 645, Loss: 0.6955229938030243, Final Batch Loss: 0.3286738991737366\n",
      "Epoch 646, Loss: 0.7002057433128357, Final Batch Loss: 0.33882296085357666\n",
      "Epoch 647, Loss: 0.7284547686576843, Final Batch Loss: 0.3393293619155884\n",
      "Epoch 648, Loss: 0.7725198268890381, Final Batch Loss: 0.45152774453163147\n",
      "Epoch 649, Loss: 0.7044008076190948, Final Batch Loss: 0.34465208649635315\n",
      "Epoch 650, Loss: 0.7397211492061615, Final Batch Loss: 0.3581363260746002\n",
      "Epoch 651, Loss: 0.7413470149040222, Final Batch Loss: 0.32622194290161133\n",
      "Epoch 652, Loss: 0.672313928604126, Final Batch Loss: 0.294285386800766\n",
      "Epoch 653, Loss: 0.7216672003269196, Final Batch Loss: 0.3385067582130432\n",
      "Epoch 654, Loss: 0.6926003396511078, Final Batch Loss: 0.39608606696128845\n",
      "Epoch 655, Loss: 0.7477241158485413, Final Batch Loss: 0.3480572998523712\n",
      "Epoch 656, Loss: 0.7415791749954224, Final Batch Loss: 0.4054866135120392\n",
      "Epoch 657, Loss: 0.6815519332885742, Final Batch Loss: 0.3492434322834015\n",
      "Epoch 658, Loss: 0.7069984972476959, Final Batch Loss: 0.36855509877204895\n",
      "Epoch 659, Loss: 0.7552027106285095, Final Batch Loss: 0.33472320437431335\n",
      "Epoch 660, Loss: 0.6696929037570953, Final Batch Loss: 0.3385222852230072\n",
      "Epoch 661, Loss: 0.7002786695957184, Final Batch Loss: 0.34106308221817017\n",
      "Epoch 662, Loss: 0.6811550855636597, Final Batch Loss: 0.3147202432155609\n",
      "Epoch 663, Loss: 0.683454841375351, Final Batch Loss: 0.34116315841674805\n",
      "Epoch 664, Loss: 0.7625027596950531, Final Batch Loss: 0.38703203201293945\n",
      "Epoch 665, Loss: 0.7025400102138519, Final Batch Loss: 0.36257344484329224\n",
      "Epoch 666, Loss: 0.6747389733791351, Final Batch Loss: 0.33044007420539856\n",
      "Epoch 667, Loss: 0.7017209827899933, Final Batch Loss: 0.3876286447048187\n",
      "Epoch 668, Loss: 0.7181035280227661, Final Batch Loss: 0.34334278106689453\n",
      "Epoch 669, Loss: 0.7568888068199158, Final Batch Loss: 0.39655524492263794\n",
      "Epoch 670, Loss: 0.6988218128681183, Final Batch Loss: 0.38034194707870483\n",
      "Epoch 671, Loss: 0.7944543063640594, Final Batch Loss: 0.31580686569213867\n",
      "Epoch 672, Loss: 0.7759731113910675, Final Batch Loss: 0.36510708928108215\n",
      "Epoch 673, Loss: 0.6790377497673035, Final Batch Loss: 0.3561626076698303\n",
      "Epoch 674, Loss: 0.673098087310791, Final Batch Loss: 0.3342917859554291\n",
      "Epoch 675, Loss: 0.7068897187709808, Final Batch Loss: 0.3963245153427124\n",
      "Epoch 676, Loss: 0.6557142436504364, Final Batch Loss: 0.34028714895248413\n",
      "Epoch 677, Loss: 0.7272962927818298, Final Batch Loss: 0.3752668499946594\n",
      "Epoch 678, Loss: 0.6997325122356415, Final Batch Loss: 0.35009869933128357\n",
      "Epoch 679, Loss: 0.6631665229797363, Final Batch Loss: 0.3098802864551544\n",
      "Epoch 680, Loss: 0.6774797737598419, Final Batch Loss: 0.28646212816238403\n",
      "Epoch 681, Loss: 0.7068304419517517, Final Batch Loss: 0.36526232957839966\n",
      "Epoch 682, Loss: 0.5909443497657776, Final Batch Loss: 0.3120923340320587\n",
      "Epoch 683, Loss: 0.7324683964252472, Final Batch Loss: 0.34665507078170776\n",
      "Epoch 684, Loss: 0.6880258917808533, Final Batch Loss: 0.33015352487564087\n",
      "Epoch 685, Loss: 0.5903690755367279, Final Batch Loss: 0.27481964230537415\n",
      "Epoch 686, Loss: 0.6628687679767609, Final Batch Loss: 0.3029269874095917\n",
      "Epoch 687, Loss: 0.7421399652957916, Final Batch Loss: 0.37890222668647766\n",
      "Epoch 688, Loss: 0.6841934025287628, Final Batch Loss: 0.3256491720676422\n",
      "Epoch 689, Loss: 0.6695692241191864, Final Batch Loss: 0.3006766140460968\n",
      "Epoch 690, Loss: 0.6415236294269562, Final Batch Loss: 0.3247207701206207\n",
      "Epoch 691, Loss: 0.6808561384677887, Final Batch Loss: 0.3294057846069336\n",
      "Epoch 692, Loss: 0.6734489798545837, Final Batch Loss: 0.376298725605011\n",
      "Epoch 693, Loss: 0.6958263218402863, Final Batch Loss: 0.3984413743019104\n",
      "Epoch 694, Loss: 0.6840052306652069, Final Batch Loss: 0.3862900137901306\n",
      "Epoch 695, Loss: 0.6784258484840393, Final Batch Loss: 0.31421127915382385\n",
      "Epoch 696, Loss: 0.7007642686367035, Final Batch Loss: 0.3929547965526581\n",
      "Epoch 697, Loss: 0.6796888113021851, Final Batch Loss: 0.3197755217552185\n",
      "Epoch 698, Loss: 0.5947206020355225, Final Batch Loss: 0.28281185030937195\n",
      "Epoch 699, Loss: 0.6184139549732208, Final Batch Loss: 0.2600189745426178\n",
      "Epoch 700, Loss: 0.6746605038642883, Final Batch Loss: 0.3142009377479553\n",
      "Epoch 701, Loss: 0.7261959314346313, Final Batch Loss: 0.4092152416706085\n",
      "Epoch 702, Loss: 0.7198604941368103, Final Batch Loss: 0.4177882969379425\n",
      "Epoch 703, Loss: 0.7162286341190338, Final Batch Loss: 0.30654242634773254\n",
      "Epoch 704, Loss: 0.6636434197425842, Final Batch Loss: 0.2880830466747284\n",
      "Epoch 705, Loss: 0.6993114352226257, Final Batch Loss: 0.314583957195282\n",
      "Epoch 706, Loss: 0.6435053646564484, Final Batch Loss: 0.3182387053966522\n",
      "Epoch 707, Loss: 0.6566774547100067, Final Batch Loss: 0.3660663664340973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708, Loss: 0.6614063084125519, Final Batch Loss: 0.3371778130531311\n",
      "Epoch 709, Loss: 0.6616101861000061, Final Batch Loss: 0.28663015365600586\n",
      "Epoch 710, Loss: 0.687600702047348, Final Batch Loss: 0.38233456015586853\n",
      "Epoch 711, Loss: 0.6157045364379883, Final Batch Loss: 0.29927414655685425\n",
      "Epoch 712, Loss: 0.6523074507713318, Final Batch Loss: 0.3274650275707245\n",
      "Epoch 713, Loss: 0.6538529396057129, Final Batch Loss: 0.3542936444282532\n",
      "Epoch 714, Loss: 0.6285590529441833, Final Batch Loss: 0.34473365545272827\n",
      "Epoch 715, Loss: 0.5737588703632355, Final Batch Loss: 0.29682108759880066\n",
      "Epoch 716, Loss: 0.6312368214130402, Final Batch Loss: 0.34410446882247925\n",
      "Epoch 717, Loss: 0.6607991456985474, Final Batch Loss: 0.32851260900497437\n",
      "Epoch 718, Loss: 0.6135062873363495, Final Batch Loss: 0.2937900125980377\n",
      "Epoch 719, Loss: 0.6302182674407959, Final Batch Loss: 0.32707542181015015\n",
      "Epoch 720, Loss: 0.5819938778877258, Final Batch Loss: 0.2905029356479645\n",
      "Epoch 721, Loss: 0.5880150198936462, Final Batch Loss: 0.2688014805316925\n",
      "Epoch 722, Loss: 0.6628974676132202, Final Batch Loss: 0.36163467168807983\n",
      "Epoch 723, Loss: 0.6855789124965668, Final Batch Loss: 0.3209659457206726\n",
      "Epoch 724, Loss: 0.6427524387836456, Final Batch Loss: 0.3064938187599182\n",
      "Epoch 725, Loss: 0.6158177554607391, Final Batch Loss: 0.3006962537765503\n",
      "Epoch 726, Loss: 0.6075444221496582, Final Batch Loss: 0.2910020053386688\n",
      "Epoch 727, Loss: 0.622939258813858, Final Batch Loss: 0.3221874535083771\n",
      "Epoch 728, Loss: 0.5983717739582062, Final Batch Loss: 0.2933131158351898\n",
      "Epoch 729, Loss: 0.6365751624107361, Final Batch Loss: 0.33571305871009827\n",
      "Epoch 730, Loss: 0.7319431006908417, Final Batch Loss: 0.38111311197280884\n",
      "Epoch 731, Loss: 0.7520055174827576, Final Batch Loss: 0.4163041114807129\n",
      "Epoch 732, Loss: 0.6676652133464813, Final Batch Loss: 0.34705525636672974\n",
      "Epoch 733, Loss: 0.5799941122531891, Final Batch Loss: 0.2947601079940796\n",
      "Epoch 734, Loss: 0.7013828158378601, Final Batch Loss: 0.3226395845413208\n",
      "Epoch 735, Loss: 0.63706374168396, Final Batch Loss: 0.38254213333129883\n",
      "Epoch 736, Loss: 0.5898399353027344, Final Batch Loss: 0.27438849210739136\n",
      "Epoch 737, Loss: 0.6183408200740814, Final Batch Loss: 0.26586440205574036\n",
      "Epoch 738, Loss: 0.6499577164649963, Final Batch Loss: 0.3190101087093353\n",
      "Epoch 739, Loss: 0.6352762877941132, Final Batch Loss: 0.3287639617919922\n",
      "Epoch 740, Loss: 0.6602921187877655, Final Batch Loss: 0.4056127965450287\n",
      "Epoch 741, Loss: 0.6307701468467712, Final Batch Loss: 0.2698764503002167\n",
      "Epoch 742, Loss: 0.6666686236858368, Final Batch Loss: 0.34724897146224976\n",
      "Epoch 743, Loss: 0.6735756695270538, Final Batch Loss: 0.3605841398239136\n",
      "Epoch 744, Loss: 0.5929255485534668, Final Batch Loss: 0.2936916649341583\n",
      "Epoch 745, Loss: 0.6078039705753326, Final Batch Loss: 0.27875426411628723\n",
      "Epoch 746, Loss: 0.6427640318870544, Final Batch Loss: 0.2852468490600586\n",
      "Epoch 747, Loss: 0.6136599183082581, Final Batch Loss: 0.32706958055496216\n",
      "Epoch 748, Loss: 0.6373199820518494, Final Batch Loss: 0.3018447160720825\n",
      "Epoch 749, Loss: 0.6384082734584808, Final Batch Loss: 0.3206029534339905\n",
      "Epoch 750, Loss: 0.6930444240570068, Final Batch Loss: 0.37485572695732117\n",
      "Epoch 751, Loss: 0.59039106965065, Final Batch Loss: 0.2937207818031311\n",
      "Epoch 752, Loss: 0.6914367377758026, Final Batch Loss: 0.31122758984565735\n",
      "Epoch 753, Loss: 0.6053549349308014, Final Batch Loss: 0.32898902893066406\n",
      "Epoch 754, Loss: 0.7008618414402008, Final Batch Loss: 0.37716013193130493\n",
      "Epoch 755, Loss: 0.6400747299194336, Final Batch Loss: 0.27466002106666565\n",
      "Epoch 756, Loss: 0.635514110326767, Final Batch Loss: 0.32390424609184265\n",
      "Epoch 757, Loss: 0.7044702172279358, Final Batch Loss: 0.3676333725452423\n",
      "Epoch 758, Loss: 0.6315019428730011, Final Batch Loss: 0.3085950016975403\n",
      "Epoch 759, Loss: 0.6615391075611115, Final Batch Loss: 0.33083584904670715\n",
      "Epoch 760, Loss: 0.6686139702796936, Final Batch Loss: 0.353919118642807\n",
      "Epoch 761, Loss: 0.6264301240444183, Final Batch Loss: 0.2744085192680359\n",
      "Epoch 762, Loss: 0.6767189502716064, Final Batch Loss: 0.32228991389274597\n",
      "Epoch 763, Loss: 0.6092047095298767, Final Batch Loss: 0.3355509042739868\n",
      "Epoch 764, Loss: 0.6336677074432373, Final Batch Loss: 0.33275023102760315\n",
      "Epoch 765, Loss: 0.5950932800769806, Final Batch Loss: 0.2757071852684021\n",
      "Epoch 766, Loss: 0.6913534998893738, Final Batch Loss: 0.331164687871933\n",
      "Epoch 767, Loss: 0.5838633477687836, Final Batch Loss: 0.29105016589164734\n",
      "Epoch 768, Loss: 0.6985245943069458, Final Batch Loss: 0.3814137578010559\n",
      "Epoch 769, Loss: 0.6388743221759796, Final Batch Loss: 0.3322485685348511\n",
      "Epoch 770, Loss: 0.6325715482234955, Final Batch Loss: 0.2819311320781708\n",
      "Epoch 771, Loss: 0.6701202094554901, Final Batch Loss: 0.3532451391220093\n",
      "Epoch 772, Loss: 0.5992159843444824, Final Batch Loss: 0.309026300907135\n",
      "Epoch 773, Loss: 0.5952928066253662, Final Batch Loss: 0.33551982045173645\n",
      "Epoch 774, Loss: 0.5960126519203186, Final Batch Loss: 0.2909792959690094\n",
      "Epoch 775, Loss: 0.6478397846221924, Final Batch Loss: 0.3396064043045044\n",
      "Epoch 776, Loss: 0.5612014830112457, Final Batch Loss: 0.27393457293510437\n",
      "Epoch 777, Loss: 0.6683081090450287, Final Batch Loss: 0.3331860303878784\n",
      "Epoch 778, Loss: 0.617093950510025, Final Batch Loss: 0.2872923016548157\n",
      "Epoch 779, Loss: 0.5943485498428345, Final Batch Loss: 0.33647844195365906\n",
      "Epoch 780, Loss: 0.6220826804637909, Final Batch Loss: 0.28809091448783875\n",
      "Epoch 781, Loss: 0.6351817846298218, Final Batch Loss: 0.2921726405620575\n",
      "Epoch 782, Loss: 0.6298736035823822, Final Batch Loss: 0.3119707703590393\n",
      "Epoch 783, Loss: 0.6251625716686249, Final Batch Loss: 0.3232843577861786\n",
      "Epoch 784, Loss: 0.613149493932724, Final Batch Loss: 0.2871551215648651\n",
      "Epoch 785, Loss: 0.6507279574871063, Final Batch Loss: 0.3292163610458374\n",
      "Epoch 786, Loss: 0.6129651367664337, Final Batch Loss: 0.30367037653923035\n",
      "Epoch 787, Loss: 0.5574015974998474, Final Batch Loss: 0.2737134099006653\n",
      "Epoch 788, Loss: 0.6027399301528931, Final Batch Loss: 0.2731780409812927\n",
      "Epoch 789, Loss: 0.5829082131385803, Final Batch Loss: 0.30406150221824646\n",
      "Epoch 790, Loss: 0.5740730166435242, Final Batch Loss: 0.25864216685295105\n",
      "Epoch 791, Loss: 0.6918731927871704, Final Batch Loss: 0.30412763357162476\n",
      "Epoch 792, Loss: 0.5514121353626251, Final Batch Loss: 0.3201780319213867\n",
      "Epoch 793, Loss: 0.614973783493042, Final Batch Loss: 0.27650174498558044\n",
      "Epoch 794, Loss: 0.5654279887676239, Final Batch Loss: 0.2837773859500885\n",
      "Epoch 795, Loss: 0.5695081055164337, Final Batch Loss: 0.2867140769958496\n",
      "Epoch 796, Loss: 0.6914296746253967, Final Batch Loss: 0.3554418385028839\n",
      "Epoch 797, Loss: 0.598766416311264, Final Batch Loss: 0.2663407325744629\n",
      "Epoch 798, Loss: 0.5372093021869659, Final Batch Loss: 0.2524484694004059\n",
      "Epoch 799, Loss: 0.5529040694236755, Final Batch Loss: 0.2484445571899414\n",
      "Epoch 800, Loss: 0.6279239058494568, Final Batch Loss: 0.31506022810935974\n",
      "Epoch 801, Loss: 0.6132944524288177, Final Batch Loss: 0.28091922402381897\n",
      "Epoch 802, Loss: 0.5231008380651474, Final Batch Loss: 0.2858286201953888\n",
      "Epoch 803, Loss: 0.55870121717453, Final Batch Loss: 0.2553504705429077\n",
      "Epoch 804, Loss: 0.5988888740539551, Final Batch Loss: 0.3092932403087616\n",
      "Epoch 805, Loss: 0.5866819322109222, Final Batch Loss: 0.33044177293777466\n",
      "Epoch 806, Loss: 0.5913112759590149, Final Batch Loss: 0.32211437821388245\n",
      "Epoch 807, Loss: 0.6045032441616058, Final Batch Loss: 0.26716166734695435\n",
      "Epoch 808, Loss: 0.5410777032375336, Final Batch Loss: 0.27166083455085754\n",
      "Epoch 809, Loss: 0.6068380773067474, Final Batch Loss: 0.2806798815727234\n",
      "Epoch 810, Loss: 0.5902324914932251, Final Batch Loss: 0.3079114556312561\n",
      "Epoch 811, Loss: 0.6517854928970337, Final Batch Loss: 0.346032053232193\n",
      "Epoch 812, Loss: 0.5946277678012848, Final Batch Loss: 0.3298085927963257\n",
      "Epoch 813, Loss: 0.5914547145366669, Final Batch Loss: 0.2849675416946411\n",
      "Epoch 814, Loss: 0.6838652789592743, Final Batch Loss: 0.33926403522491455\n",
      "Epoch 815, Loss: 0.60139599442482, Final Batch Loss: 0.28275439143180847\n",
      "Epoch 816, Loss: 0.64215949177742, Final Batch Loss: 0.3130260705947876\n",
      "Epoch 817, Loss: 0.6734647154808044, Final Batch Loss: 0.3480842411518097\n",
      "Epoch 818, Loss: 0.6645659804344177, Final Batch Loss: 0.313999205827713\n",
      "Epoch 819, Loss: 0.6296921074390411, Final Batch Loss: 0.3162323236465454\n",
      "Epoch 820, Loss: 0.5889093577861786, Final Batch Loss: 0.28284594416618347\n",
      "Epoch 821, Loss: 0.5722966492176056, Final Batch Loss: 0.3163936734199524\n",
      "Epoch 822, Loss: 0.5454850345849991, Final Batch Loss: 0.3054696023464203\n",
      "Epoch 823, Loss: 0.5758118629455566, Final Batch Loss: 0.2914060056209564\n",
      "Epoch 824, Loss: 0.5604920387268066, Final Batch Loss: 0.26274189352989197\n",
      "Epoch 825, Loss: 0.6759057343006134, Final Batch Loss: 0.3955353796482086\n",
      "Epoch 826, Loss: 0.566177487373352, Final Batch Loss: 0.26453444361686707\n",
      "Epoch 827, Loss: 0.5177222788333893, Final Batch Loss: 0.254297137260437\n",
      "Epoch 828, Loss: 0.470812052488327, Final Batch Loss: 0.23335427045822144\n",
      "Epoch 829, Loss: 0.6013537645339966, Final Batch Loss: 0.3414495289325714\n",
      "Epoch 830, Loss: 0.5988457798957825, Final Batch Loss: 0.3204592168331146\n",
      "Epoch 831, Loss: 0.5478285253047943, Final Batch Loss: 0.2770064175128937\n",
      "Epoch 832, Loss: 0.5703971982002258, Final Batch Loss: 0.2852075695991516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 833, Loss: 0.5283081233501434, Final Batch Loss: 0.26822683215141296\n",
      "Epoch 834, Loss: 0.5851269364356995, Final Batch Loss: 0.28559574484825134\n",
      "Epoch 835, Loss: 0.5698719918727875, Final Batch Loss: 0.27425843477249146\n",
      "Epoch 836, Loss: 0.6369323134422302, Final Batch Loss: 0.27345019578933716\n",
      "Epoch 837, Loss: 0.6376436352729797, Final Batch Loss: 0.36534440517425537\n",
      "Epoch 838, Loss: 0.6523850560188293, Final Batch Loss: 0.35061922669410706\n",
      "Epoch 839, Loss: 0.6078445017337799, Final Batch Loss: 0.3004229962825775\n",
      "Epoch 840, Loss: 0.5651942491531372, Final Batch Loss: 0.29726603627204895\n",
      "Epoch 841, Loss: 0.5560847818851471, Final Batch Loss: 0.2641727328300476\n",
      "Epoch 842, Loss: 0.6148736476898193, Final Batch Loss: 0.32761719822883606\n",
      "Epoch 843, Loss: 0.5943799018859863, Final Batch Loss: 0.30372533202171326\n",
      "Epoch 844, Loss: 0.6491903066635132, Final Batch Loss: 0.33191412687301636\n",
      "Epoch 845, Loss: 0.5391066074371338, Final Batch Loss: 0.27485179901123047\n",
      "Epoch 846, Loss: 0.5706838667392731, Final Batch Loss: 0.31319501996040344\n",
      "Epoch 847, Loss: 0.6122088432312012, Final Batch Loss: 0.30605611205101013\n",
      "Epoch 848, Loss: 0.5324487239122391, Final Batch Loss: 0.21149463951587677\n",
      "Epoch 849, Loss: 0.5942343175411224, Final Batch Loss: 0.33214181661605835\n",
      "Epoch 850, Loss: 0.5433680713176727, Final Batch Loss: 0.3028382658958435\n",
      "Epoch 851, Loss: 0.5898474454879761, Final Batch Loss: 0.29803019762039185\n",
      "Epoch 852, Loss: 0.575345367193222, Final Batch Loss: 0.31646305322647095\n",
      "Epoch 853, Loss: 0.5829756855964661, Final Batch Loss: 0.25179386138916016\n",
      "Epoch 854, Loss: 0.5714220702648163, Final Batch Loss: 0.2666805684566498\n",
      "Epoch 855, Loss: 0.5461357086896896, Final Batch Loss: 0.2498743087053299\n",
      "Epoch 856, Loss: 0.5869518518447876, Final Batch Loss: 0.33828848600387573\n",
      "Epoch 857, Loss: 0.5503996163606644, Final Batch Loss: 0.3293413817882538\n",
      "Epoch 858, Loss: 0.5590946972370148, Final Batch Loss: 0.2582312524318695\n",
      "Epoch 859, Loss: 0.5990681946277618, Final Batch Loss: 0.2832554280757904\n",
      "Epoch 860, Loss: 0.51558817923069, Final Batch Loss: 0.2769049108028412\n",
      "Epoch 861, Loss: 0.5928935110569, Final Batch Loss: 0.27885475754737854\n",
      "Epoch 862, Loss: 0.5722140371799469, Final Batch Loss: 0.30867451429367065\n",
      "Epoch 863, Loss: 0.5940331518650055, Final Batch Loss: 0.26639965176582336\n",
      "Epoch 864, Loss: 0.549609512090683, Final Batch Loss: 0.28088581562042236\n",
      "Epoch 865, Loss: 0.48399724066257477, Final Batch Loss: 0.20960070192813873\n",
      "Epoch 866, Loss: 0.5679803192615509, Final Batch Loss: 0.27550068497657776\n",
      "Epoch 867, Loss: 0.5553004145622253, Final Batch Loss: 0.3026845157146454\n",
      "Epoch 868, Loss: 0.5974186658859253, Final Batch Loss: 0.3327532410621643\n",
      "Epoch 869, Loss: 0.5637312531471252, Final Batch Loss: 0.30307042598724365\n",
      "Epoch 870, Loss: 0.5759910345077515, Final Batch Loss: 0.2912912368774414\n",
      "Epoch 871, Loss: 0.6048560738563538, Final Batch Loss: 0.2967672049999237\n",
      "Epoch 872, Loss: 0.5637808442115784, Final Batch Loss: 0.30193623900413513\n",
      "Epoch 873, Loss: 0.649054616689682, Final Batch Loss: 0.30456608533859253\n",
      "Epoch 874, Loss: 0.6035860776901245, Final Batch Loss: 0.26305559277534485\n",
      "Epoch 875, Loss: 0.67035773396492, Final Batch Loss: 0.3788948953151703\n",
      "Epoch 876, Loss: 0.6548243463039398, Final Batch Loss: 0.3169017434120178\n",
      "Epoch 877, Loss: 0.5652927309274673, Final Batch Loss: 0.24978069961071014\n",
      "Epoch 878, Loss: 0.5268767178058624, Final Batch Loss: 0.2638185918331146\n",
      "Epoch 879, Loss: 0.5634672939777374, Final Batch Loss: 0.2504315972328186\n",
      "Epoch 880, Loss: 0.6779023110866547, Final Batch Loss: 0.3808950185775757\n",
      "Epoch 881, Loss: 0.6140212118625641, Final Batch Loss: 0.29457786679267883\n",
      "Epoch 882, Loss: 0.6189850568771362, Final Batch Loss: 0.2713760435581207\n",
      "Epoch 883, Loss: 0.5954959392547607, Final Batch Loss: 0.3440400958061218\n",
      "Epoch 884, Loss: 0.5901341438293457, Final Batch Loss: 0.29743367433547974\n",
      "Epoch 885, Loss: 0.5105559527873993, Final Batch Loss: 0.2493913769721985\n",
      "Epoch 886, Loss: 0.6100569367408752, Final Batch Loss: 0.30421146750450134\n",
      "Epoch 887, Loss: 0.5199551582336426, Final Batch Loss: 0.2836407423019409\n",
      "Epoch 888, Loss: 0.5224735736846924, Final Batch Loss: 0.2515655755996704\n",
      "Epoch 889, Loss: 0.6091717779636383, Final Batch Loss: 0.312734454870224\n",
      "Epoch 890, Loss: 0.5472293049097061, Final Batch Loss: 0.23608453571796417\n",
      "Epoch 891, Loss: 0.5604822337627411, Final Batch Loss: 0.2758532464504242\n",
      "Epoch 892, Loss: 0.5064645856618881, Final Batch Loss: 0.27591848373413086\n",
      "Epoch 893, Loss: 0.5760352611541748, Final Batch Loss: 0.29961279034614563\n",
      "Epoch 894, Loss: 0.5502172410488129, Final Batch Loss: 0.3098467290401459\n",
      "Epoch 895, Loss: 0.5218113958835602, Final Batch Loss: 0.2772611975669861\n",
      "Epoch 896, Loss: 0.632040798664093, Final Batch Loss: 0.2833470106124878\n",
      "Epoch 897, Loss: 0.6183919310569763, Final Batch Loss: 0.3344671428203583\n",
      "Epoch 898, Loss: 0.5393763184547424, Final Batch Loss: 0.26481893658638\n",
      "Epoch 899, Loss: 0.5602615773677826, Final Batch Loss: 0.3104541599750519\n",
      "Epoch 900, Loss: 0.47407232224941254, Final Batch Loss: 0.20752204954624176\n",
      "Epoch 901, Loss: 0.5685133934020996, Final Batch Loss: 0.2803335189819336\n",
      "Epoch 902, Loss: 0.5546625852584839, Final Batch Loss: 0.30015987157821655\n",
      "Epoch 903, Loss: 0.5916192829608917, Final Batch Loss: 0.2785928547382355\n",
      "Epoch 904, Loss: 0.48534244298934937, Final Batch Loss: 0.24221445620059967\n",
      "Epoch 905, Loss: 0.5945044755935669, Final Batch Loss: 0.2783030867576599\n",
      "Epoch 906, Loss: 0.5962150692939758, Final Batch Loss: 0.313864141702652\n",
      "Epoch 907, Loss: 0.5195731818675995, Final Batch Loss: 0.2712772786617279\n",
      "Epoch 908, Loss: 0.5665505528450012, Final Batch Loss: 0.28982341289520264\n",
      "Epoch 909, Loss: 0.5262998342514038, Final Batch Loss: 0.27076953649520874\n",
      "Epoch 910, Loss: 0.5156351923942566, Final Batch Loss: 0.2552248537540436\n",
      "Epoch 911, Loss: 0.5371967852115631, Final Batch Loss: 0.25375989079475403\n",
      "Epoch 912, Loss: 0.5839024782180786, Final Batch Loss: 0.2596069276332855\n",
      "Epoch 913, Loss: 0.594562441110611, Final Batch Loss: 0.2828477919101715\n",
      "Epoch 914, Loss: 0.5107828825712204, Final Batch Loss: 0.24315090477466583\n",
      "Epoch 915, Loss: 0.5264563858509064, Final Batch Loss: 0.27772262692451477\n",
      "Epoch 916, Loss: 0.46292319893836975, Final Batch Loss: 0.259893536567688\n",
      "Epoch 917, Loss: 0.5687883049249649, Final Batch Loss: 0.2293107956647873\n",
      "Epoch 918, Loss: 0.6306875944137573, Final Batch Loss: 0.29522570967674255\n",
      "Epoch 919, Loss: 0.4862552136182785, Final Batch Loss: 0.20940251648426056\n",
      "Epoch 920, Loss: 0.5451074242591858, Final Batch Loss: 0.2695683240890503\n",
      "Epoch 921, Loss: 0.5007089823484421, Final Batch Loss: 0.2625562250614166\n",
      "Epoch 922, Loss: 0.5871377736330032, Final Batch Loss: 0.3387581706047058\n",
      "Epoch 923, Loss: 0.5223633944988251, Final Batch Loss: 0.238070547580719\n",
      "Epoch 924, Loss: 0.567927211523056, Final Batch Loss: 0.3105989098548889\n",
      "Epoch 925, Loss: 0.5825173258781433, Final Batch Loss: 0.30096885561943054\n",
      "Epoch 926, Loss: 0.5791230797767639, Final Batch Loss: 0.29483893513679504\n",
      "Epoch 927, Loss: 0.5693542957305908, Final Batch Loss: 0.30915626883506775\n",
      "Epoch 928, Loss: 0.5958812534809113, Final Batch Loss: 0.3157420754432678\n",
      "Epoch 929, Loss: 0.5621561110019684, Final Batch Loss: 0.2670952081680298\n",
      "Epoch 930, Loss: 0.5543143451213837, Final Batch Loss: 0.28391045331954956\n",
      "Epoch 931, Loss: 0.6240793168544769, Final Batch Loss: 0.3048996031284332\n",
      "Epoch 932, Loss: 0.5776925384998322, Final Batch Loss: 0.28086191415786743\n",
      "Epoch 933, Loss: 0.525568351149559, Final Batch Loss: 0.2951519191265106\n",
      "Epoch 934, Loss: 0.5776734352111816, Final Batch Loss: 0.29383257031440735\n",
      "Epoch 935, Loss: 0.6090439260005951, Final Batch Loss: 0.3458671271800995\n",
      "Epoch 936, Loss: 0.5756305158138275, Final Batch Loss: 0.3228047490119934\n",
      "Epoch 937, Loss: 0.5956265926361084, Final Batch Loss: 0.30160415172576904\n",
      "Epoch 938, Loss: 0.5754339694976807, Final Batch Loss: 0.31133878231048584\n",
      "Epoch 939, Loss: 0.5275871753692627, Final Batch Loss: 0.30115026235580444\n",
      "Epoch 940, Loss: 0.5796037912368774, Final Batch Loss: 0.29738685488700867\n",
      "Epoch 941, Loss: 0.5726751983165741, Final Batch Loss: 0.26458415389060974\n",
      "Epoch 942, Loss: 0.539185181260109, Final Batch Loss: 0.2896369397640228\n",
      "Epoch 943, Loss: 0.5424190759658813, Final Batch Loss: 0.2949903905391693\n",
      "Epoch 944, Loss: 0.6120429039001465, Final Batch Loss: 0.33214041590690613\n",
      "Epoch 945, Loss: 0.6194678544998169, Final Batch Loss: 0.2956084609031677\n",
      "Epoch 946, Loss: 0.6754196584224701, Final Batch Loss: 0.33884134888648987\n",
      "Epoch 947, Loss: 0.4655364602804184, Final Batch Loss: 0.21830250322818756\n",
      "Epoch 948, Loss: 0.5575889497995377, Final Batch Loss: 0.22689242660999298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949, Loss: 0.5237866044044495, Final Batch Loss: 0.2638404071331024\n",
      "Epoch 950, Loss: 0.4876974821090698, Final Batch Loss: 0.2034258246421814\n",
      "Epoch 951, Loss: 0.590483695268631, Final Batch Loss: 0.2758292853832245\n",
      "Epoch 952, Loss: 0.5888565927743912, Final Batch Loss: 0.23570148646831512\n",
      "Epoch 953, Loss: 0.5101666301488876, Final Batch Loss: 0.26200929284095764\n",
      "Epoch 954, Loss: 0.6053750514984131, Final Batch Loss: 0.25472933053970337\n",
      "Epoch 955, Loss: 0.5279992520809174, Final Batch Loss: 0.27298468351364136\n",
      "Epoch 956, Loss: 0.4975192993879318, Final Batch Loss: 0.28444668650627136\n",
      "Epoch 957, Loss: 0.45900706946849823, Final Batch Loss: 0.2057224065065384\n",
      "Epoch 958, Loss: 0.45162682235240936, Final Batch Loss: 0.23772510886192322\n",
      "Epoch 959, Loss: 0.5774610042572021, Final Batch Loss: 0.2579432427883148\n",
      "Epoch 960, Loss: 0.5217676758766174, Final Batch Loss: 0.29795166850090027\n",
      "Epoch 961, Loss: 0.574301540851593, Final Batch Loss: 0.28064611554145813\n",
      "Epoch 962, Loss: 0.5846476852893829, Final Batch Loss: 0.2609943449497223\n",
      "Epoch 963, Loss: 0.5490963011980057, Final Batch Loss: 0.23145581781864166\n",
      "Epoch 964, Loss: 0.5373028516769409, Final Batch Loss: 0.26032182574272156\n",
      "Epoch 965, Loss: 0.5974708050489426, Final Batch Loss: 0.23261122405529022\n",
      "Epoch 966, Loss: 0.5989854037761688, Final Batch Loss: 0.2682352364063263\n",
      "Epoch 967, Loss: 0.5148052275180817, Final Batch Loss: 0.2512688636779785\n",
      "Epoch 968, Loss: 0.5521159619092941, Final Batch Loss: 0.31085509061813354\n",
      "Epoch 969, Loss: 0.5475979745388031, Final Batch Loss: 0.25942060351371765\n",
      "Epoch 970, Loss: 0.5414679050445557, Final Batch Loss: 0.2889251410961151\n",
      "Epoch 971, Loss: 0.6521870791912079, Final Batch Loss: 0.3000010550022125\n",
      "Epoch 972, Loss: 0.4741649776697159, Final Batch Loss: 0.2266094833612442\n",
      "Epoch 973, Loss: 0.5217347592115402, Final Batch Loss: 0.2826589345932007\n",
      "Epoch 974, Loss: 0.5295534431934357, Final Batch Loss: 0.3050818145275116\n",
      "Epoch 975, Loss: 0.5039523094892502, Final Batch Loss: 0.19945462048053741\n",
      "Epoch 976, Loss: 0.5320406556129456, Final Batch Loss: 0.2690110504627228\n",
      "Epoch 977, Loss: 0.5628405213356018, Final Batch Loss: 0.296284019947052\n",
      "Epoch 978, Loss: 0.4906870275735855, Final Batch Loss: 0.21345387399196625\n",
      "Epoch 979, Loss: 0.5334232598543167, Final Batch Loss: 0.22116385400295258\n",
      "Epoch 980, Loss: 0.4774949550628662, Final Batch Loss: 0.2338324934244156\n",
      "Epoch 981, Loss: 0.4792144149541855, Final Batch Loss: 0.2509203851222992\n",
      "Epoch 982, Loss: 0.5681928396224976, Final Batch Loss: 0.28052467107772827\n",
      "Epoch 983, Loss: 0.5178564339876175, Final Batch Loss: 0.24915479123592377\n",
      "Epoch 984, Loss: 0.5347728729248047, Final Batch Loss: 0.2669888138771057\n",
      "Epoch 985, Loss: 0.6024577021598816, Final Batch Loss: 0.3265538513660431\n",
      "Epoch 986, Loss: 0.506990373134613, Final Batch Loss: 0.26513323187828064\n",
      "Epoch 987, Loss: 0.5952830016613007, Final Batch Loss: 0.34012842178344727\n",
      "Epoch 988, Loss: 0.5257781893014908, Final Batch Loss: 0.22685842216014862\n",
      "Epoch 989, Loss: 0.4767385870218277, Final Batch Loss: 0.26075154542922974\n",
      "Epoch 990, Loss: 0.5338567346334457, Final Batch Loss: 0.2435656040906906\n",
      "Epoch 991, Loss: 0.5479349493980408, Final Batch Loss: 0.2559300363063812\n",
      "Epoch 992, Loss: 0.5692866742610931, Final Batch Loss: 0.278402179479599\n",
      "Epoch 993, Loss: 0.5577942728996277, Final Batch Loss: 0.3150539696216583\n",
      "Epoch 994, Loss: 0.47397254407405853, Final Batch Loss: 0.2189512401819229\n",
      "Epoch 995, Loss: 0.5510788261890411, Final Batch Loss: 0.29219141602516174\n",
      "Epoch 996, Loss: 0.5055340081453323, Final Batch Loss: 0.23052869737148285\n",
      "Epoch 997, Loss: 0.5105379521846771, Final Batch Loss: 0.2782965302467346\n",
      "Epoch 998, Loss: 0.47380430996418, Final Batch Loss: 0.26580655574798584\n",
      "Epoch 999, Loss: 0.5440573692321777, Final Batch Loss: 0.2928102910518646\n",
      "Epoch 1000, Loss: 0.5019010305404663, Final Batch Loss: 0.2652164101600647\n",
      "Epoch 1001, Loss: 0.5290301740169525, Final Batch Loss: 0.250688374042511\n",
      "Epoch 1002, Loss: 0.5264292806386948, Final Batch Loss: 0.24629829823970795\n",
      "Epoch 1003, Loss: 0.5948360562324524, Final Batch Loss: 0.3135453462600708\n",
      "Epoch 1004, Loss: 0.5782284140586853, Final Batch Loss: 0.2948395609855652\n",
      "Epoch 1005, Loss: 0.501473680138588, Final Batch Loss: 0.27013882994651794\n",
      "Epoch 1006, Loss: 0.5076299458742142, Final Batch Loss: 0.21154506504535675\n",
      "Epoch 1007, Loss: 0.5784729719161987, Final Batch Loss: 0.3125419318675995\n",
      "Epoch 1008, Loss: 0.4557013660669327, Final Batch Loss: 0.2325880378484726\n",
      "Epoch 1009, Loss: 0.4909535199403763, Final Batch Loss: 0.21795614063739777\n",
      "Epoch 1010, Loss: 0.5136633068323135, Final Batch Loss: 0.23547561466693878\n",
      "Epoch 1011, Loss: 0.5552668869495392, Final Batch Loss: 0.29213273525238037\n",
      "Epoch 1012, Loss: 0.4707479625940323, Final Batch Loss: 0.22291187942028046\n",
      "Epoch 1013, Loss: 0.5059173852205276, Final Batch Loss: 0.2221417874097824\n",
      "Epoch 1014, Loss: 0.5745657980442047, Final Batch Loss: 0.2806342542171478\n",
      "Epoch 1015, Loss: 0.5220305919647217, Final Batch Loss: 0.26736897230148315\n",
      "Epoch 1016, Loss: 0.554754227399826, Final Batch Loss: 0.2825998365879059\n",
      "Epoch 1017, Loss: 0.5621068775653839, Final Batch Loss: 0.2411852478981018\n",
      "Epoch 1018, Loss: 0.47361670434474945, Final Batch Loss: 0.25551122426986694\n",
      "Epoch 1019, Loss: 0.5497522354125977, Final Batch Loss: 0.26063627004623413\n",
      "Epoch 1020, Loss: 0.5389984995126724, Final Batch Loss: 0.3030794858932495\n",
      "Epoch 1021, Loss: 0.5571275353431702, Final Batch Loss: 0.3385956883430481\n",
      "Epoch 1022, Loss: 0.572815477848053, Final Batch Loss: 0.33210429549217224\n",
      "Epoch 1023, Loss: 0.5470913946628571, Final Batch Loss: 0.27899280190467834\n",
      "Epoch 1024, Loss: 0.5715926885604858, Final Batch Loss: 0.2916944921016693\n",
      "Epoch 1025, Loss: 0.4674912542104721, Final Batch Loss: 0.24089577794075012\n",
      "Epoch 1026, Loss: 0.5247802287340164, Final Batch Loss: 0.21417568624019623\n",
      "Epoch 1027, Loss: 0.5958733856678009, Final Batch Loss: 0.28034117817878723\n",
      "Epoch 1028, Loss: 0.5231366753578186, Final Batch Loss: 0.3125773072242737\n",
      "Epoch 1029, Loss: 0.5690232217311859, Final Batch Loss: 0.31140539050102234\n",
      "Epoch 1030, Loss: 0.5091138631105423, Final Batch Loss: 0.215037003159523\n",
      "Epoch 1031, Loss: 0.5755823850631714, Final Batch Loss: 0.26522237062454224\n",
      "Epoch 1032, Loss: 0.48949140310287476, Final Batch Loss: 0.2348574995994568\n",
      "Epoch 1033, Loss: 0.5021435469388962, Final Batch Loss: 0.2699533700942993\n",
      "Epoch 1034, Loss: 0.5083571523427963, Final Batch Loss: 0.24124829471111298\n",
      "Epoch 1035, Loss: 0.5268496870994568, Final Batch Loss: 0.27722054719924927\n",
      "Epoch 1036, Loss: 0.5944603383541107, Final Batch Loss: 0.31409093737602234\n",
      "Epoch 1037, Loss: 0.5784310400485992, Final Batch Loss: 0.28402063250541687\n",
      "Epoch 1038, Loss: 0.5590369999408722, Final Batch Loss: 0.269315630197525\n",
      "Epoch 1039, Loss: 0.5536783337593079, Final Batch Loss: 0.27101194858551025\n",
      "Epoch 1040, Loss: 0.5005521923303604, Final Batch Loss: 0.24728454649448395\n",
      "Epoch 1041, Loss: 0.5466972589492798, Final Batch Loss: 0.3277750313282013\n",
      "Epoch 1042, Loss: 0.5296534895896912, Final Batch Loss: 0.27694204449653625\n",
      "Epoch 1043, Loss: 0.45794162154197693, Final Batch Loss: 0.26280492544174194\n",
      "Epoch 1044, Loss: 0.45859338343143463, Final Batch Loss: 0.23037058115005493\n",
      "Epoch 1045, Loss: 0.547539472579956, Final Batch Loss: 0.25648996233940125\n",
      "Epoch 1046, Loss: 0.5747628808021545, Final Batch Loss: 0.24169176816940308\n",
      "Epoch 1047, Loss: 0.4878878891468048, Final Batch Loss: 0.20992949604988098\n",
      "Epoch 1048, Loss: 0.53739233314991, Final Batch Loss: 0.2980126440525055\n",
      "Epoch 1049, Loss: 0.5683084279298782, Final Batch Loss: 0.23976655304431915\n",
      "Epoch 1050, Loss: 0.48152077198028564, Final Batch Loss: 0.23408907651901245\n",
      "Epoch 1051, Loss: 0.500709056854248, Final Batch Loss: 0.2200087010860443\n",
      "Epoch 1052, Loss: 0.4900898188352585, Final Batch Loss: 0.2510821223258972\n",
      "Epoch 1053, Loss: 0.46621614694595337, Final Batch Loss: 0.23166362941265106\n",
      "Epoch 1054, Loss: 0.5591685473918915, Final Batch Loss: 0.2414916455745697\n",
      "Epoch 1055, Loss: 0.4554566442966461, Final Batch Loss: 0.2309962660074234\n",
      "Epoch 1056, Loss: 0.5644516348838806, Final Batch Loss: 0.3004559278488159\n",
      "Epoch 1057, Loss: 0.5383957028388977, Final Batch Loss: 0.2974262833595276\n",
      "Epoch 1058, Loss: 0.5053236335515976, Final Batch Loss: 0.23831595480442047\n",
      "Epoch 1059, Loss: 0.5624197721481323, Final Batch Loss: 0.22814172506332397\n",
      "Epoch 1060, Loss: 0.5191563367843628, Final Batch Loss: 0.26436910033226013\n",
      "Epoch 1061, Loss: 0.5497207343578339, Final Batch Loss: 0.272206574678421\n",
      "Epoch 1062, Loss: 0.5121676921844482, Final Batch Loss: 0.23920348286628723\n",
      "Epoch 1063, Loss: 0.5853520035743713, Final Batch Loss: 0.3032539188861847\n",
      "Epoch 1064, Loss: 0.5118556320667267, Final Batch Loss: 0.2916736304759979\n",
      "Epoch 1065, Loss: 0.46563056111335754, Final Batch Loss: 0.2549506425857544\n",
      "Epoch 1066, Loss: 0.49873876571655273, Final Batch Loss: 0.21488544344902039\n",
      "Epoch 1067, Loss: 0.43749602138996124, Final Batch Loss: 0.19119980931282043\n",
      "Epoch 1068, Loss: 0.47466738522052765, Final Batch Loss: 0.2559733986854553\n",
      "Epoch 1069, Loss: 0.45053404569625854, Final Batch Loss: 0.210708886384964\n",
      "Epoch 1070, Loss: 0.4448731988668442, Final Batch Loss: 0.1919022649526596\n",
      "Epoch 1071, Loss: 0.4387245923280716, Final Batch Loss: 0.19966363906860352\n",
      "Epoch 1072, Loss: 0.4775322675704956, Final Batch Loss: 0.2202419638633728\n",
      "Epoch 1073, Loss: 0.5146844536066055, Final Batch Loss: 0.2397591918706894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1074, Loss: 0.527577206492424, Final Batch Loss: 0.2820749282836914\n",
      "Epoch 1075, Loss: 0.5832298696041107, Final Batch Loss: 0.2773786187171936\n",
      "Epoch 1076, Loss: 0.48509882390499115, Final Batch Loss: 0.23586882650852203\n",
      "Epoch 1077, Loss: 0.46916840970516205, Final Batch Loss: 0.23527193069458008\n",
      "Epoch 1078, Loss: 0.5241447389125824, Final Batch Loss: 0.2611871063709259\n",
      "Epoch 1079, Loss: 0.6237283945083618, Final Batch Loss: 0.30595874786376953\n",
      "Epoch 1080, Loss: 0.5605679154396057, Final Batch Loss: 0.24665912985801697\n",
      "Epoch 1081, Loss: 0.5549819469451904, Final Batch Loss: 0.2823905050754547\n",
      "Epoch 1082, Loss: 0.5393906831741333, Final Batch Loss: 0.23196309804916382\n",
      "Epoch 1083, Loss: 0.5620582699775696, Final Batch Loss: 0.2929116487503052\n",
      "Epoch 1084, Loss: 0.47077904641628265, Final Batch Loss: 0.2352956384420395\n",
      "Epoch 1085, Loss: 0.46802492439746857, Final Batch Loss: 0.2288130223751068\n",
      "Epoch 1086, Loss: 0.5887750685214996, Final Batch Loss: 0.30412039160728455\n",
      "Epoch 1087, Loss: 0.486725389957428, Final Batch Loss: 0.27152231335639954\n",
      "Epoch 1088, Loss: 0.4901150017976761, Final Batch Loss: 0.24900372326374054\n",
      "Epoch 1089, Loss: 0.5044540613889694, Final Batch Loss: 0.2663254141807556\n",
      "Epoch 1090, Loss: 0.46410948038101196, Final Batch Loss: 0.24332676827907562\n",
      "Epoch 1091, Loss: 0.4812157154083252, Final Batch Loss: 0.18301650881767273\n",
      "Epoch 1092, Loss: 0.5731225311756134, Final Batch Loss: 0.33172136545181274\n",
      "Epoch 1093, Loss: 0.4556800425052643, Final Batch Loss: 0.2616403102874756\n",
      "Epoch 1094, Loss: 0.48820336163043976, Final Batch Loss: 0.24149535596370697\n",
      "Epoch 1095, Loss: 0.5274520218372345, Final Batch Loss: 0.23266983032226562\n",
      "Epoch 1096, Loss: 0.5453663319349289, Final Batch Loss: 0.3145469129085541\n",
      "Epoch 1097, Loss: 0.5492661893367767, Final Batch Loss: 0.2698550820350647\n",
      "Epoch 1098, Loss: 0.5910564661026001, Final Batch Loss: 0.290277898311615\n",
      "Epoch 1099, Loss: 0.47338396310806274, Final Batch Loss: 0.22360166907310486\n",
      "Epoch 1100, Loss: 0.5446435511112213, Final Batch Loss: 0.2884620726108551\n",
      "Epoch 1101, Loss: 0.396693155169487, Final Batch Loss: 0.18347245454788208\n",
      "Epoch 1102, Loss: 0.4851798564195633, Final Batch Loss: 0.23201040923595428\n",
      "Epoch 1103, Loss: 0.5098548084497452, Final Batch Loss: 0.26710447669029236\n",
      "Epoch 1104, Loss: 0.4858066737651825, Final Batch Loss: 0.21207886934280396\n",
      "Epoch 1105, Loss: 0.5211862176656723, Final Batch Loss: 0.3156530261039734\n",
      "Epoch 1106, Loss: 0.5097954720258713, Final Batch Loss: 0.27005016803741455\n",
      "Epoch 1107, Loss: 0.46723075211048126, Final Batch Loss: 0.19007490575313568\n",
      "Epoch 1108, Loss: 0.4307226538658142, Final Batch Loss: 0.23012031614780426\n",
      "Epoch 1109, Loss: 0.4773675203323364, Final Batch Loss: 0.26658326387405396\n",
      "Epoch 1110, Loss: 0.4427456110715866, Final Batch Loss: 0.23256118595600128\n",
      "Epoch 1111, Loss: 0.5044331401586533, Final Batch Loss: 0.2769358456134796\n",
      "Epoch 1112, Loss: 0.5478544235229492, Final Batch Loss: 0.25737234950065613\n",
      "Epoch 1113, Loss: 0.4611385017633438, Final Batch Loss: 0.19932682812213898\n",
      "Epoch 1114, Loss: 0.5353388339281082, Final Batch Loss: 0.23491249978542328\n",
      "Epoch 1115, Loss: 0.5257434546947479, Final Batch Loss: 0.3129706382751465\n",
      "Epoch 1116, Loss: 0.4691191762685776, Final Batch Loss: 0.2564338743686676\n",
      "Epoch 1117, Loss: 0.49788349866867065, Final Batch Loss: 0.253025084733963\n",
      "Epoch 1118, Loss: 0.4690883755683899, Final Batch Loss: 0.20362120866775513\n",
      "Epoch 1119, Loss: 0.461505651473999, Final Batch Loss: 0.2535461187362671\n",
      "Epoch 1120, Loss: 0.5046457201242447, Final Batch Loss: 0.23270414769649506\n",
      "Epoch 1121, Loss: 0.4699130058288574, Final Batch Loss: 0.24068443477153778\n",
      "Epoch 1122, Loss: 0.4635316729545593, Final Batch Loss: 0.2156154066324234\n",
      "Epoch 1123, Loss: 0.4806085079908371, Final Batch Loss: 0.21648018062114716\n",
      "Epoch 1124, Loss: 0.44113439321517944, Final Batch Loss: 0.24731744825839996\n",
      "Epoch 1125, Loss: 0.441263884305954, Final Batch Loss: 0.25918689370155334\n",
      "Epoch 1126, Loss: 0.5320754051208496, Final Batch Loss: 0.2955084443092346\n",
      "Epoch 1127, Loss: 0.5488493591547012, Final Batch Loss: 0.3173271715641022\n",
      "Epoch 1128, Loss: 0.47209395468235016, Final Batch Loss: 0.22534799575805664\n",
      "Epoch 1129, Loss: 0.5099891722202301, Final Batch Loss: 0.2527244985103607\n",
      "Epoch 1130, Loss: 0.5129869878292084, Final Batch Loss: 0.27648720145225525\n",
      "Epoch 1131, Loss: 0.4265580326318741, Final Batch Loss: 0.16402174532413483\n",
      "Epoch 1132, Loss: 0.5231571644544601, Final Batch Loss: 0.27848386764526367\n",
      "Epoch 1133, Loss: 0.49479179084300995, Final Batch Loss: 0.25050267577171326\n",
      "Epoch 1134, Loss: 0.5037139058113098, Final Batch Loss: 0.2347864806652069\n",
      "Epoch 1135, Loss: 0.43386493623256683, Final Batch Loss: 0.21782070398330688\n",
      "Epoch 1136, Loss: 0.4486466348171234, Final Batch Loss: 0.21041983366012573\n",
      "Epoch 1137, Loss: 0.46669213473796844, Final Batch Loss: 0.22394341230392456\n",
      "Epoch 1138, Loss: 0.46288271248340607, Final Batch Loss: 0.25163307785987854\n",
      "Epoch 1139, Loss: 0.4365594834089279, Final Batch Loss: 0.2549862563610077\n",
      "Epoch 1140, Loss: 0.48878994584083557, Final Batch Loss: 0.20328837633132935\n",
      "Epoch 1141, Loss: 0.5109001845121384, Final Batch Loss: 0.23821009695529938\n",
      "Epoch 1142, Loss: 0.4652184545993805, Final Batch Loss: 0.2686989903450012\n",
      "Epoch 1143, Loss: 0.5558280944824219, Final Batch Loss: 0.26729458570480347\n",
      "Epoch 1144, Loss: 0.454126700758934, Final Batch Loss: 0.23832504451274872\n",
      "Epoch 1145, Loss: 0.47759757936000824, Final Batch Loss: 0.2440904676914215\n",
      "Epoch 1146, Loss: 0.4165254682302475, Final Batch Loss: 0.22009028494358063\n",
      "Epoch 1147, Loss: 0.46052615344524384, Final Batch Loss: 0.23866136372089386\n",
      "Epoch 1148, Loss: 0.5399998873472214, Final Batch Loss: 0.29250043630599976\n",
      "Epoch 1149, Loss: 0.42839497327804565, Final Batch Loss: 0.18655173480510712\n",
      "Epoch 1150, Loss: 0.5540003180503845, Final Batch Loss: 0.2651321589946747\n",
      "Epoch 1151, Loss: 0.4664975702762604, Final Batch Loss: 0.21328380703926086\n",
      "Epoch 1152, Loss: 0.427840918302536, Final Batch Loss: 0.21298961341381073\n",
      "Epoch 1153, Loss: 0.46871088445186615, Final Batch Loss: 0.2549625337123871\n",
      "Epoch 1154, Loss: 0.5199026167392731, Final Batch Loss: 0.23917260766029358\n",
      "Epoch 1155, Loss: 0.4735797792673111, Final Batch Loss: 0.23490677773952484\n",
      "Epoch 1156, Loss: 0.5358757078647614, Final Batch Loss: 0.2701343894004822\n",
      "Epoch 1157, Loss: 0.5066803097724915, Final Batch Loss: 0.2727416753768921\n",
      "Epoch 1158, Loss: 0.547543853521347, Final Batch Loss: 0.2845631539821625\n",
      "Epoch 1159, Loss: 0.5187700092792511, Final Batch Loss: 0.27425193786621094\n",
      "Epoch 1160, Loss: 0.4952472895383835, Final Batch Loss: 0.21901173889636993\n",
      "Epoch 1161, Loss: 0.4856465607881546, Final Batch Loss: 0.2486524134874344\n",
      "Epoch 1162, Loss: 0.45388907194137573, Final Batch Loss: 0.25681838393211365\n",
      "Epoch 1163, Loss: 0.5362063646316528, Final Batch Loss: 0.24618402123451233\n",
      "Epoch 1164, Loss: 0.5100557804107666, Final Batch Loss: 0.18759772181510925\n",
      "Epoch 1165, Loss: 0.483526349067688, Final Batch Loss: 0.242689847946167\n",
      "Epoch 1166, Loss: 0.5212142765522003, Final Batch Loss: 0.21806952357292175\n",
      "Epoch 1167, Loss: 0.506389707326889, Final Batch Loss: 0.22910019755363464\n",
      "Epoch 1168, Loss: 0.4162305146455765, Final Batch Loss: 0.19026313722133636\n",
      "Epoch 1169, Loss: 0.47666747868061066, Final Batch Loss: 0.24755190312862396\n",
      "Epoch 1170, Loss: 0.4931465685367584, Final Batch Loss: 0.21975913643836975\n",
      "Epoch 1171, Loss: 0.49453304708004, Final Batch Loss: 0.2555217444896698\n",
      "Epoch 1172, Loss: 0.5099727660417557, Final Batch Loss: 0.2632192075252533\n",
      "Epoch 1173, Loss: 0.5197366029024124, Final Batch Loss: 0.21400104463100433\n",
      "Epoch 1174, Loss: 0.499643936753273, Final Batch Loss: 0.24542216956615448\n",
      "Epoch 1175, Loss: 0.5313436836004257, Final Batch Loss: 0.24592064321041107\n",
      "Epoch 1176, Loss: 0.49479417502880096, Final Batch Loss: 0.2530058026313782\n",
      "Epoch 1177, Loss: 0.5065364837646484, Final Batch Loss: 0.2789163291454315\n",
      "Epoch 1178, Loss: 0.4975038319826126, Final Batch Loss: 0.2224784940481186\n",
      "Epoch 1179, Loss: 0.47778743505477905, Final Batch Loss: 0.21720942854881287\n",
      "Epoch 1180, Loss: 0.4665478616952896, Final Batch Loss: 0.23645970225334167\n",
      "Epoch 1181, Loss: 0.44656115770339966, Final Batch Loss: 0.2037622630596161\n",
      "Epoch 1182, Loss: 0.49750909209251404, Final Batch Loss: 0.24614977836608887\n",
      "Epoch 1183, Loss: 0.4402482211589813, Final Batch Loss: 0.2248721569776535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1184, Loss: 0.49570274353027344, Final Batch Loss: 0.22305351495742798\n",
      "Epoch 1185, Loss: 0.46223802864551544, Final Batch Loss: 0.19989942014217377\n",
      "Epoch 1186, Loss: 0.49916960299015045, Final Batch Loss: 0.25498536229133606\n",
      "Epoch 1187, Loss: 0.462637722492218, Final Batch Loss: 0.21554596722126007\n",
      "Epoch 1188, Loss: 0.5065563023090363, Final Batch Loss: 0.25403639674186707\n",
      "Epoch 1189, Loss: 0.4430457353591919, Final Batch Loss: 0.18510907888412476\n",
      "Epoch 1190, Loss: 0.46376337110996246, Final Batch Loss: 0.2586492598056793\n",
      "Epoch 1191, Loss: 0.40827618539333344, Final Batch Loss: 0.20251867175102234\n",
      "Epoch 1192, Loss: 0.4813409894704819, Final Batch Loss: 0.22653262317180634\n",
      "Epoch 1193, Loss: 0.43346789479255676, Final Batch Loss: 0.20881490409374237\n",
      "Epoch 1194, Loss: 0.46829143166542053, Final Batch Loss: 0.23237532377243042\n",
      "Epoch 1195, Loss: 0.4005869925022125, Final Batch Loss: 0.1956193745136261\n",
      "Epoch 1196, Loss: 0.41671641170978546, Final Batch Loss: 0.21607479453086853\n",
      "Epoch 1197, Loss: 0.48081597685813904, Final Batch Loss: 0.23442716896533966\n",
      "Epoch 1198, Loss: 0.436543807387352, Final Batch Loss: 0.2213154137134552\n",
      "Epoch 1199, Loss: 0.4352637976408005, Final Batch Loss: 0.23732422292232513\n",
      "Epoch 1200, Loss: 0.4809638410806656, Final Batch Loss: 0.22976408898830414\n",
      "Epoch 1201, Loss: 0.4398352801799774, Final Batch Loss: 0.2160363495349884\n",
      "Epoch 1202, Loss: 0.4905324727296829, Final Batch Loss: 0.2513514757156372\n",
      "Epoch 1203, Loss: 0.4357099235057831, Final Batch Loss: 0.21119001507759094\n",
      "Epoch 1204, Loss: 0.41741161048412323, Final Batch Loss: 0.16242511570453644\n",
      "Epoch 1205, Loss: 0.3968345522880554, Final Batch Loss: 0.17560908198356628\n",
      "Epoch 1206, Loss: 0.38549068570137024, Final Batch Loss: 0.1467081606388092\n",
      "Epoch 1207, Loss: 0.5103667080402374, Final Batch Loss: 0.2642531394958496\n",
      "Epoch 1208, Loss: 0.44182030856609344, Final Batch Loss: 0.20495060086250305\n",
      "Epoch 1209, Loss: 0.49177806079387665, Final Batch Loss: 0.25327199697494507\n",
      "Epoch 1210, Loss: 0.4586133360862732, Final Batch Loss: 0.2589402496814728\n",
      "Epoch 1211, Loss: 0.4362918585538864, Final Batch Loss: 0.26306968927383423\n",
      "Epoch 1212, Loss: 0.46242551505565643, Final Batch Loss: 0.20439551770687103\n",
      "Epoch 1213, Loss: 0.4999384582042694, Final Batch Loss: 0.2176937460899353\n",
      "Epoch 1214, Loss: 0.602090448141098, Final Batch Loss: 0.30856651067733765\n",
      "Epoch 1215, Loss: 0.4310864210128784, Final Batch Loss: 0.2338569164276123\n",
      "Epoch 1216, Loss: 0.5074535608291626, Final Batch Loss: 0.27105826139450073\n",
      "Epoch 1217, Loss: 0.4863153100013733, Final Batch Loss: 0.2581569254398346\n",
      "Epoch 1218, Loss: 0.48305386304855347, Final Batch Loss: 0.24616551399230957\n",
      "Epoch 1219, Loss: 0.4775422066450119, Final Batch Loss: 0.22797121107578278\n",
      "Epoch 1220, Loss: 0.5093507617712021, Final Batch Loss: 0.2499808818101883\n",
      "Epoch 1221, Loss: 0.5775884091854095, Final Batch Loss: 0.25248557329177856\n",
      "Epoch 1222, Loss: 0.46666060388088226, Final Batch Loss: 0.2101045399904251\n",
      "Epoch 1223, Loss: 0.5069585889577866, Final Batch Loss: 0.2241472452878952\n",
      "Epoch 1224, Loss: 0.4117048978805542, Final Batch Loss: 0.24018947780132294\n",
      "Epoch 1225, Loss: 0.45176000893116, Final Batch Loss: 0.2132597714662552\n",
      "Epoch 1226, Loss: 0.5371942222118378, Final Batch Loss: 0.23296970129013062\n",
      "Epoch 1227, Loss: 0.46602632105350494, Final Batch Loss: 0.2415512502193451\n",
      "Epoch 1228, Loss: 0.47048236429691315, Final Batch Loss: 0.21965594589710236\n",
      "Epoch 1229, Loss: 0.5102436691522598, Final Batch Loss: 0.2623349130153656\n",
      "Epoch 1230, Loss: 0.4904673099517822, Final Batch Loss: 0.30081576108932495\n",
      "Epoch 1231, Loss: 0.4194949120283127, Final Batch Loss: 0.1934604048728943\n",
      "Epoch 1232, Loss: 0.5031807869672775, Final Batch Loss: 0.2764233946800232\n",
      "Epoch 1233, Loss: 0.5054083615541458, Final Batch Loss: 0.2295997589826584\n",
      "Epoch 1234, Loss: 0.4349505454301834, Final Batch Loss: 0.1948685497045517\n",
      "Epoch 1235, Loss: 0.4496859610080719, Final Batch Loss: 0.24179606139659882\n",
      "Epoch 1236, Loss: 0.4831449091434479, Final Batch Loss: 0.2837706208229065\n",
      "Epoch 1237, Loss: 0.4449394792318344, Final Batch Loss: 0.24815459549427032\n",
      "Epoch 1238, Loss: 0.43619386851787567, Final Batch Loss: 0.22701308131217957\n",
      "Epoch 1239, Loss: 0.45737965404987335, Final Batch Loss: 0.22701363265514374\n",
      "Epoch 1240, Loss: 0.45255835354328156, Final Batch Loss: 0.2138713300228119\n",
      "Epoch 1241, Loss: 0.4576708674430847, Final Batch Loss: 0.26306214928627014\n",
      "Epoch 1242, Loss: 0.45303235948085785, Final Batch Loss: 0.18950967490673065\n",
      "Epoch 1243, Loss: 0.4746783524751663, Final Batch Loss: 0.21123908460140228\n",
      "Epoch 1244, Loss: 0.43344980478286743, Final Batch Loss: 0.20617052912712097\n",
      "Epoch 1245, Loss: 0.4596918374300003, Final Batch Loss: 0.23405607044696808\n",
      "Epoch 1246, Loss: 0.47740134596824646, Final Batch Loss: 0.2548865079879761\n",
      "Epoch 1247, Loss: 0.5069949179887772, Final Batch Loss: 0.3161676526069641\n",
      "Epoch 1248, Loss: 0.5362931787967682, Final Batch Loss: 0.2700728476047516\n",
      "Epoch 1249, Loss: 0.4374275505542755, Final Batch Loss: 0.22770753502845764\n",
      "Epoch 1250, Loss: 0.4474421739578247, Final Batch Loss: 0.1885519027709961\n",
      "Epoch 1251, Loss: 0.45403504371643066, Final Batch Loss: 0.22766925394535065\n",
      "Epoch 1252, Loss: 0.5285563468933105, Final Batch Loss: 0.25925499200820923\n",
      "Epoch 1253, Loss: 0.4380108565092087, Final Batch Loss: 0.19394108653068542\n",
      "Epoch 1254, Loss: 0.4711650609970093, Final Batch Loss: 0.22572489082813263\n",
      "Epoch 1255, Loss: 0.4450410306453705, Final Batch Loss: 0.2273278534412384\n",
      "Epoch 1256, Loss: 0.4219950735569, Final Batch Loss: 0.22092948853969574\n",
      "Epoch 1257, Loss: 0.4315122365951538, Final Batch Loss: 0.19951802492141724\n",
      "Epoch 1258, Loss: 0.44072750210762024, Final Batch Loss: 0.1776411235332489\n",
      "Epoch 1259, Loss: 0.49508367478847504, Final Batch Loss: 0.2084173709154129\n",
      "Epoch 1260, Loss: 0.47535455226898193, Final Batch Loss: 0.1833239495754242\n",
      "Epoch 1261, Loss: 0.42845290899276733, Final Batch Loss: 0.24465836584568024\n",
      "Epoch 1262, Loss: 0.42945531010627747, Final Batch Loss: 0.1966542750597\n",
      "Epoch 1263, Loss: 0.46683143079280853, Final Batch Loss: 0.2482028603553772\n",
      "Epoch 1264, Loss: 0.48133988678455353, Final Batch Loss: 0.2929839789867401\n",
      "Epoch 1265, Loss: 0.49319902062416077, Final Batch Loss: 0.24696636199951172\n",
      "Epoch 1266, Loss: 0.43027137219905853, Final Batch Loss: 0.1992546021938324\n",
      "Epoch 1267, Loss: 0.4571800082921982, Final Batch Loss: 0.2652147710323334\n",
      "Epoch 1268, Loss: 0.424187496304512, Final Batch Loss: 0.2157593071460724\n",
      "Epoch 1269, Loss: 0.41594210267066956, Final Batch Loss: 0.22376340627670288\n",
      "Epoch 1270, Loss: 0.5083214044570923, Final Batch Loss: 0.2346659004688263\n",
      "Epoch 1271, Loss: 0.47342830896377563, Final Batch Loss: 0.23639826476573944\n",
      "Epoch 1272, Loss: 0.4422779679298401, Final Batch Loss: 0.2298211306333542\n",
      "Epoch 1273, Loss: 0.4251193106174469, Final Batch Loss: 0.22552062571048737\n",
      "Epoch 1274, Loss: 0.5012383311986923, Final Batch Loss: 0.2817619740962982\n",
      "Epoch 1275, Loss: 0.49184635281562805, Final Batch Loss: 0.2569611668586731\n",
      "Epoch 1276, Loss: 0.4353843480348587, Final Batch Loss: 0.22534777224063873\n",
      "Epoch 1277, Loss: 0.4784526228904724, Final Batch Loss: 0.26369544863700867\n",
      "Epoch 1278, Loss: 0.5059311836957932, Final Batch Loss: 0.2771572172641754\n",
      "Epoch 1279, Loss: 0.4293251931667328, Final Batch Loss: 0.20341934263706207\n",
      "Epoch 1280, Loss: 0.4128988981246948, Final Batch Loss: 0.1793450564146042\n",
      "Epoch 1281, Loss: 0.45702415704727173, Final Batch Loss: 0.21388743817806244\n",
      "Epoch 1282, Loss: 0.4326667785644531, Final Batch Loss: 0.2275753617286682\n",
      "Epoch 1283, Loss: 0.4071039855480194, Final Batch Loss: 0.20603635907173157\n",
      "Epoch 1284, Loss: 0.508957177400589, Final Batch Loss: 0.23895758390426636\n",
      "Epoch 1285, Loss: 0.4468870460987091, Final Batch Loss: 0.19062861800193787\n",
      "Epoch 1286, Loss: 0.4440144896507263, Final Batch Loss: 0.24696287512779236\n",
      "Epoch 1287, Loss: 0.4603099226951599, Final Batch Loss: 0.20705637335777283\n",
      "Epoch 1288, Loss: 0.5216504633426666, Final Batch Loss: 0.23528143763542175\n",
      "Epoch 1289, Loss: 0.4846641272306442, Final Batch Loss: 0.27348244190216064\n",
      "Epoch 1290, Loss: 0.397495374083519, Final Batch Loss: 0.20987559854984283\n",
      "Epoch 1291, Loss: 0.4388193190097809, Final Batch Loss: 0.26104122400283813\n",
      "Epoch 1292, Loss: 0.42569148540496826, Final Batch Loss: 0.18404830992221832\n",
      "Epoch 1293, Loss: 0.42902979254722595, Final Batch Loss: 0.2017180323600769\n",
      "Epoch 1294, Loss: 0.5346678197383881, Final Batch Loss: 0.26058489084243774\n",
      "Epoch 1295, Loss: 0.35270868241786957, Final Batch Loss: 0.17724065482616425\n",
      "Epoch 1296, Loss: 0.4266775995492935, Final Batch Loss: 0.20728175342082977\n",
      "Epoch 1297, Loss: 0.4614844024181366, Final Batch Loss: 0.2510449290275574\n",
      "Epoch 1298, Loss: 0.5530078709125519, Final Batch Loss: 0.25062867999076843\n",
      "Epoch 1299, Loss: 0.478647843003273, Final Batch Loss: 0.23174746334552765\n",
      "Epoch 1300, Loss: 0.4018601030111313, Final Batch Loss: 0.1707640141248703\n",
      "Epoch 1301, Loss: 0.4403861165046692, Final Batch Loss: 0.22645118832588196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1302, Loss: 0.4126742482185364, Final Batch Loss: 0.25090381503105164\n",
      "Epoch 1303, Loss: 0.39549633860588074, Final Batch Loss: 0.19512538611888885\n",
      "Epoch 1304, Loss: 0.4062929153442383, Final Batch Loss: 0.2324136346578598\n",
      "Epoch 1305, Loss: 0.40896306931972504, Final Batch Loss: 0.2065565437078476\n",
      "Epoch 1306, Loss: 0.41701847314834595, Final Batch Loss: 0.20394016802310944\n",
      "Epoch 1307, Loss: 0.4304271936416626, Final Batch Loss: 0.19509992003440857\n",
      "Epoch 1308, Loss: 0.41312576830387115, Final Batch Loss: 0.22572264075279236\n",
      "Epoch 1309, Loss: 0.4547526687383652, Final Batch Loss: 0.16539011895656586\n",
      "Epoch 1310, Loss: 0.4317004978656769, Final Batch Loss: 0.16711384057998657\n",
      "Epoch 1311, Loss: 0.3874980956315994, Final Batch Loss: 0.17641882598400116\n",
      "Epoch 1312, Loss: 0.44425956904888153, Final Batch Loss: 0.22208388149738312\n",
      "Epoch 1313, Loss: 0.3987579643726349, Final Batch Loss: 0.19842131435871124\n",
      "Epoch 1314, Loss: 0.4392419755458832, Final Batch Loss: 0.22402521967887878\n",
      "Epoch 1315, Loss: 0.34751318395137787, Final Batch Loss: 0.16870519518852234\n",
      "Epoch 1316, Loss: 0.46984362602233887, Final Batch Loss: 0.22696782648563385\n",
      "Epoch 1317, Loss: 0.3954899460077286, Final Batch Loss: 0.20650257170200348\n",
      "Epoch 1318, Loss: 0.43624550104141235, Final Batch Loss: 0.21242673695087433\n",
      "Epoch 1319, Loss: 0.5685964822769165, Final Batch Loss: 0.2724091112613678\n",
      "Epoch 1320, Loss: 0.39607545733451843, Final Batch Loss: 0.19301961362361908\n",
      "Epoch 1321, Loss: 0.5021332204341888, Final Batch Loss: 0.24753674864768982\n",
      "Epoch 1322, Loss: 0.44680044054985046, Final Batch Loss: 0.23185864090919495\n",
      "Epoch 1323, Loss: 0.4356148689985275, Final Batch Loss: 0.274115651845932\n",
      "Epoch 1324, Loss: 0.4267200827598572, Final Batch Loss: 0.20076023042201996\n",
      "Epoch 1325, Loss: 0.49103423953056335, Final Batch Loss: 0.245575413107872\n",
      "Epoch 1326, Loss: 0.3936944901943207, Final Batch Loss: 0.18620485067367554\n",
      "Epoch 1327, Loss: 0.4634089469909668, Final Batch Loss: 0.2243950217962265\n",
      "Epoch 1328, Loss: 0.44385261833667755, Final Batch Loss: 0.1872207075357437\n",
      "Epoch 1329, Loss: 0.372475802898407, Final Batch Loss: 0.19098690152168274\n",
      "Epoch 1330, Loss: 0.5903790444135666, Final Batch Loss: 0.3665241003036499\n",
      "Epoch 1331, Loss: 0.5035383850336075, Final Batch Loss: 0.2441285401582718\n",
      "Epoch 1332, Loss: 0.43089883029460907, Final Batch Loss: 0.2144545167684555\n",
      "Epoch 1333, Loss: 0.4741378277540207, Final Batch Loss: 0.22984109818935394\n",
      "Epoch 1334, Loss: 0.4483373314142227, Final Batch Loss: 0.23675377666950226\n",
      "Epoch 1335, Loss: 0.4192197471857071, Final Batch Loss: 0.18210111558437347\n",
      "Epoch 1336, Loss: 0.4696181118488312, Final Batch Loss: 0.2858008146286011\n",
      "Epoch 1337, Loss: 0.4392690509557724, Final Batch Loss: 0.20469407737255096\n",
      "Epoch 1338, Loss: 0.489850252866745, Final Batch Loss: 0.23413997888565063\n",
      "Epoch 1339, Loss: 0.4546952545642853, Final Batch Loss: 0.2416221648454666\n",
      "Epoch 1340, Loss: 0.4139518290758133, Final Batch Loss: 0.23050564527511597\n",
      "Epoch 1341, Loss: 0.4276715815067291, Final Batch Loss: 0.239778071641922\n",
      "Epoch 1342, Loss: 0.39546625316143036, Final Batch Loss: 0.1875745803117752\n",
      "Epoch 1343, Loss: 0.432878702878952, Final Batch Loss: 0.20497208833694458\n",
      "Epoch 1344, Loss: 0.4059733599424362, Final Batch Loss: 0.17528554797172546\n",
      "Epoch 1345, Loss: 0.43290574848651886, Final Batch Loss: 0.25382888317108154\n",
      "Epoch 1346, Loss: 0.37737374007701874, Final Batch Loss: 0.1875293254852295\n",
      "Epoch 1347, Loss: 0.49304617941379547, Final Batch Loss: 0.26962485909461975\n",
      "Epoch 1348, Loss: 0.3948942869901657, Final Batch Loss: 0.17316636443138123\n",
      "Epoch 1349, Loss: 0.3948374390602112, Final Batch Loss: 0.1988510936498642\n",
      "Epoch 1350, Loss: 0.357973575592041, Final Batch Loss: 0.16065552830696106\n",
      "Epoch 1351, Loss: 0.4385092109441757, Final Batch Loss: 0.19728568196296692\n",
      "Epoch 1352, Loss: 0.45804616808891296, Final Batch Loss: 0.23709923028945923\n",
      "Epoch 1353, Loss: 0.4373198449611664, Final Batch Loss: 0.24451102316379547\n",
      "Epoch 1354, Loss: 0.41242991387844086, Final Batch Loss: 0.2152116298675537\n",
      "Epoch 1355, Loss: 0.45833925902843475, Final Batch Loss: 0.27251842617988586\n",
      "Epoch 1356, Loss: 0.42947153747081757, Final Batch Loss: 0.21818934381008148\n",
      "Epoch 1357, Loss: 0.43898122012615204, Final Batch Loss: 0.20093636214733124\n",
      "Epoch 1358, Loss: 0.4027892202138901, Final Batch Loss: 0.24787579476833344\n",
      "Epoch 1359, Loss: 0.5194133073091507, Final Batch Loss: 0.22372107207775116\n",
      "Epoch 1360, Loss: 0.4747439920902252, Final Batch Loss: 0.19386261701583862\n",
      "Epoch 1361, Loss: 0.43052372336387634, Final Batch Loss: 0.20233651995658875\n",
      "Epoch 1362, Loss: 0.3631894737482071, Final Batch Loss: 0.18462450802326202\n",
      "Epoch 1363, Loss: 0.4063701927661896, Final Batch Loss: 0.18428626656532288\n",
      "Epoch 1364, Loss: 0.3933391869068146, Final Batch Loss: 0.17119576036930084\n",
      "Epoch 1365, Loss: 0.5180551111698151, Final Batch Loss: 0.2999059855937958\n",
      "Epoch 1366, Loss: 0.3957819938659668, Final Batch Loss: 0.22342750430107117\n",
      "Epoch 1367, Loss: 0.4425400644540787, Final Batch Loss: 0.22276915609836578\n",
      "Epoch 1368, Loss: 0.4861217439174652, Final Batch Loss: 0.24397467076778412\n",
      "Epoch 1369, Loss: 0.4822041839361191, Final Batch Loss: 0.21270035207271576\n",
      "Epoch 1370, Loss: 0.47778987884521484, Final Batch Loss: 0.2239273488521576\n",
      "Epoch 1371, Loss: 0.4245203137397766, Final Batch Loss: 0.19521638751029968\n",
      "Epoch 1372, Loss: 0.4612208306789398, Final Batch Loss: 0.2548264265060425\n",
      "Epoch 1373, Loss: 0.3958696126937866, Final Batch Loss: 0.1988026350736618\n",
      "Epoch 1374, Loss: 0.38446715474128723, Final Batch Loss: 0.1787581443786621\n",
      "Epoch 1375, Loss: 0.4875442683696747, Final Batch Loss: 0.24599982798099518\n",
      "Epoch 1376, Loss: 0.44987069070339203, Final Batch Loss: 0.20765571296215057\n",
      "Epoch 1377, Loss: 0.4671853482723236, Final Batch Loss: 0.21691909432411194\n",
      "Epoch 1378, Loss: 0.39790256321430206, Final Batch Loss: 0.19260038435459137\n",
      "Epoch 1379, Loss: 0.4444934278726578, Final Batch Loss: 0.2426287829875946\n",
      "Epoch 1380, Loss: 0.42823341488838196, Final Batch Loss: 0.20952320098876953\n",
      "Epoch 1381, Loss: 0.46631284058094025, Final Batch Loss: 0.20597441494464874\n",
      "Epoch 1382, Loss: 0.45009858906269073, Final Batch Loss: 0.21789006888866425\n",
      "Epoch 1383, Loss: 0.37005893886089325, Final Batch Loss: 0.1986611932516098\n",
      "Epoch 1384, Loss: 0.38786232471466064, Final Batch Loss: 0.17002366483211517\n",
      "Epoch 1385, Loss: 0.39652569591999054, Final Batch Loss: 0.18517640233039856\n",
      "Epoch 1386, Loss: 0.4116888642311096, Final Batch Loss: 0.19654974341392517\n",
      "Epoch 1387, Loss: 0.47557273507118225, Final Batch Loss: 0.2420615404844284\n",
      "Epoch 1388, Loss: 0.4424106627702713, Final Batch Loss: 0.23297670483589172\n",
      "Epoch 1389, Loss: 0.47346334159374237, Final Batch Loss: 0.20868371427059174\n",
      "Epoch 1390, Loss: 0.40433645248413086, Final Batch Loss: 0.1699051707983017\n",
      "Epoch 1391, Loss: 0.3919174522161484, Final Batch Loss: 0.19721120595932007\n",
      "Epoch 1392, Loss: 0.46549344062805176, Final Batch Loss: 0.25814327597618103\n",
      "Epoch 1393, Loss: 0.44615083932876587, Final Batch Loss: 0.21652953326702118\n",
      "Epoch 1394, Loss: 0.4705270230770111, Final Batch Loss: 0.2986350357532501\n",
      "Epoch 1395, Loss: 0.528396874666214, Final Batch Loss: 0.269707590341568\n",
      "Epoch 1396, Loss: 0.42185454070568085, Final Batch Loss: 0.2716931402683258\n",
      "Epoch 1397, Loss: 0.4140187054872513, Final Batch Loss: 0.19219975173473358\n",
      "Epoch 1398, Loss: 0.4661306291818619, Final Batch Loss: 0.2148074060678482\n",
      "Epoch 1399, Loss: 0.4344862103462219, Final Batch Loss: 0.16831880807876587\n",
      "Epoch 1400, Loss: 0.4126933515071869, Final Batch Loss: 0.1936229020357132\n",
      "Epoch 1401, Loss: 0.451569065451622, Final Batch Loss: 0.207351952791214\n",
      "Epoch 1402, Loss: 0.4178384989500046, Final Batch Loss: 0.20755599439144135\n",
      "Epoch 1403, Loss: 0.4538862854242325, Final Batch Loss: 0.2139262706041336\n",
      "Epoch 1404, Loss: 0.4514486491680145, Final Batch Loss: 0.240696981549263\n",
      "Epoch 1405, Loss: 0.4162999093532562, Final Batch Loss: 0.24537567794322968\n",
      "Epoch 1406, Loss: 0.4968282878398895, Final Batch Loss: 0.21691974997520447\n",
      "Epoch 1407, Loss: 0.4293828308582306, Final Batch Loss: 0.21696622669696808\n",
      "Epoch 1408, Loss: 0.4128669500350952, Final Batch Loss: 0.19784101843833923\n",
      "Epoch 1409, Loss: 0.39939582347869873, Final Batch Loss: 0.22642990946769714\n",
      "Epoch 1410, Loss: 0.376582071185112, Final Batch Loss: 0.17228202521800995\n",
      "Epoch 1411, Loss: 0.4325782209634781, Final Batch Loss: 0.2011309266090393\n",
      "Epoch 1412, Loss: 0.45786774158477783, Final Batch Loss: 0.2617061734199524\n",
      "Epoch 1413, Loss: 0.4560156464576721, Final Batch Loss: 0.26219695806503296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1414, Loss: 0.37709152698516846, Final Batch Loss: 0.2065463662147522\n",
      "Epoch 1415, Loss: 0.38974466919898987, Final Batch Loss: 0.2008267492055893\n",
      "Epoch 1416, Loss: 0.4760833829641342, Final Batch Loss: 0.25705257058143616\n",
      "Epoch 1417, Loss: 0.4371345788240433, Final Batch Loss: 0.2165980190038681\n",
      "Epoch 1418, Loss: 0.3486892282962799, Final Batch Loss: 0.1499820053577423\n",
      "Epoch 1419, Loss: 0.47869715094566345, Final Batch Loss: 0.2609080672264099\n",
      "Epoch 1420, Loss: 0.40823936462402344, Final Batch Loss: 0.2143266797065735\n",
      "Epoch 1421, Loss: 0.4217294603586197, Final Batch Loss: 0.21472780406475067\n",
      "Epoch 1422, Loss: 0.4639604389667511, Final Batch Loss: 0.2428373247385025\n",
      "Epoch 1423, Loss: 0.4431701898574829, Final Batch Loss: 0.23039311170578003\n",
      "Epoch 1424, Loss: 0.3699685037136078, Final Batch Loss: 0.1956963688135147\n",
      "Epoch 1425, Loss: 0.4013718217611313, Final Batch Loss: 0.21443717181682587\n",
      "Epoch 1426, Loss: 0.3482586294412613, Final Batch Loss: 0.14508889615535736\n",
      "Epoch 1427, Loss: 0.4222441613674164, Final Batch Loss: 0.21325144171714783\n",
      "Epoch 1428, Loss: 0.41089771687984467, Final Batch Loss: 0.2491450309753418\n",
      "Epoch 1429, Loss: 0.3586377650499344, Final Batch Loss: 0.1665363907814026\n",
      "Epoch 1430, Loss: 0.4559331387281418, Final Batch Loss: 0.21515919268131256\n",
      "Epoch 1431, Loss: 0.4531092196702957, Final Batch Loss: 0.24298931658267975\n",
      "Epoch 1432, Loss: 0.42619624733924866, Final Batch Loss: 0.21075119078159332\n",
      "Epoch 1433, Loss: 0.439243882894516, Final Batch Loss: 0.1811474859714508\n",
      "Epoch 1434, Loss: 0.44510918855667114, Final Batch Loss: 0.25496578216552734\n",
      "Epoch 1435, Loss: 0.4177156537771225, Final Batch Loss: 0.23108820617198944\n",
      "Epoch 1436, Loss: 0.5108081549406052, Final Batch Loss: 0.2468711882829666\n",
      "Epoch 1437, Loss: 0.421321839094162, Final Batch Loss: 0.19062434136867523\n",
      "Epoch 1438, Loss: 0.3480048030614853, Final Batch Loss: 0.19808797538280487\n",
      "Epoch 1439, Loss: 0.4240320175886154, Final Batch Loss: 0.23544548451900482\n",
      "Epoch 1440, Loss: 0.39849039912223816, Final Batch Loss: 0.18818572163581848\n",
      "Epoch 1441, Loss: 0.37799496948719025, Final Batch Loss: 0.17092879116535187\n",
      "Epoch 1442, Loss: 0.3967358022928238, Final Batch Loss: 0.180378720164299\n",
      "Epoch 1443, Loss: 0.4700320065021515, Final Batch Loss: 0.2504967451095581\n",
      "Epoch 1444, Loss: 0.425096794962883, Final Batch Loss: 0.18370024859905243\n",
      "Epoch 1445, Loss: 0.3458656072616577, Final Batch Loss: 0.14488659799098969\n",
      "Epoch 1446, Loss: 0.3478122800588608, Final Batch Loss: 0.1857168823480606\n",
      "Epoch 1447, Loss: 0.4196832925081253, Final Batch Loss: 0.18605495989322662\n",
      "Epoch 1448, Loss: 0.41312897205352783, Final Batch Loss: 0.1784626692533493\n",
      "Epoch 1449, Loss: 0.41920697689056396, Final Batch Loss: 0.2567862272262573\n",
      "Epoch 1450, Loss: 0.4219124913215637, Final Batch Loss: 0.2067621648311615\n",
      "Epoch 1451, Loss: 0.411944180727005, Final Batch Loss: 0.21034184098243713\n",
      "Epoch 1452, Loss: 0.40357138216495514, Final Batch Loss: 0.2071097493171692\n",
      "Epoch 1453, Loss: 0.3959050327539444, Final Batch Loss: 0.20712991058826447\n",
      "Epoch 1454, Loss: 0.380972757935524, Final Batch Loss: 0.19501893222332\n",
      "Epoch 1455, Loss: 0.44080522656440735, Final Batch Loss: 0.24418717622756958\n",
      "Epoch 1456, Loss: 0.37960079312324524, Final Batch Loss: 0.20275172591209412\n",
      "Epoch 1457, Loss: 0.3996708542108536, Final Batch Loss: 0.20723867416381836\n",
      "Epoch 1458, Loss: 0.33950866758823395, Final Batch Loss: 0.16195791959762573\n",
      "Epoch 1459, Loss: 0.39997561275959015, Final Batch Loss: 0.20243647694587708\n",
      "Epoch 1460, Loss: 0.38917218148708344, Final Batch Loss: 0.18907876312732697\n",
      "Epoch 1461, Loss: 0.3524935096502304, Final Batch Loss: 0.17686863243579865\n",
      "Epoch 1462, Loss: 0.4056859314441681, Final Batch Loss: 0.20222590863704681\n",
      "Epoch 1463, Loss: 0.3767836093902588, Final Batch Loss: 0.1511555314064026\n",
      "Epoch 1464, Loss: 0.4345304071903229, Final Batch Loss: 0.22130972146987915\n",
      "Epoch 1465, Loss: 0.48737385869026184, Final Batch Loss: 0.2818135917186737\n",
      "Epoch 1466, Loss: 0.3697848916053772, Final Batch Loss: 0.1772601157426834\n",
      "Epoch 1467, Loss: 0.4150844216346741, Final Batch Loss: 0.19280649721622467\n",
      "Epoch 1468, Loss: 0.38266922533512115, Final Batch Loss: 0.19913332164287567\n",
      "Epoch 1469, Loss: 0.3719422221183777, Final Batch Loss: 0.1427970677614212\n",
      "Epoch 1470, Loss: 0.45619726181030273, Final Batch Loss: 0.2005309760570526\n",
      "Epoch 1471, Loss: 0.3476893752813339, Final Batch Loss: 0.15598097443580627\n",
      "Epoch 1472, Loss: 0.4479801654815674, Final Batch Loss: 0.24056001007556915\n",
      "Epoch 1473, Loss: 0.4293355494737625, Final Batch Loss: 0.2215164303779602\n",
      "Epoch 1474, Loss: 0.39835673570632935, Final Batch Loss: 0.19997452199459076\n",
      "Epoch 1475, Loss: 0.4170169234275818, Final Batch Loss: 0.21355895698070526\n",
      "Epoch 1476, Loss: 0.3883221000432968, Final Batch Loss: 0.2134457528591156\n",
      "Epoch 1477, Loss: 0.4183150380849838, Final Batch Loss: 0.21269692480564117\n",
      "Epoch 1478, Loss: 0.4047599881887436, Final Batch Loss: 0.20475369691848755\n",
      "Epoch 1479, Loss: 0.42056985199451447, Final Batch Loss: 0.2106143981218338\n",
      "Epoch 1480, Loss: 0.4035305678844452, Final Batch Loss: 0.18731792271137238\n",
      "Epoch 1481, Loss: 0.4238673597574234, Final Batch Loss: 0.1955917924642563\n",
      "Epoch 1482, Loss: 0.42983242869377136, Final Batch Loss: 0.20998719334602356\n",
      "Epoch 1483, Loss: 0.42640405893325806, Final Batch Loss: 0.21954911947250366\n",
      "Epoch 1484, Loss: 0.38423818349838257, Final Batch Loss: 0.19084620475769043\n",
      "Epoch 1485, Loss: 0.4009411931037903, Final Batch Loss: 0.26188457012176514\n",
      "Epoch 1486, Loss: 0.40412938594818115, Final Batch Loss: 0.23380763828754425\n",
      "Epoch 1487, Loss: 0.5011794567108154, Final Batch Loss: 0.22189921140670776\n",
      "Epoch 1488, Loss: 0.3659640848636627, Final Batch Loss: 0.1814914047718048\n",
      "Epoch 1489, Loss: 0.3347650319337845, Final Batch Loss: 0.16245128214359283\n",
      "Epoch 1490, Loss: 0.4820266515016556, Final Batch Loss: 0.27864667773246765\n",
      "Epoch 1491, Loss: 0.44023217260837555, Final Batch Loss: 0.21581344306468964\n",
      "Epoch 1492, Loss: 0.35868923366069794, Final Batch Loss: 0.19290883839130402\n",
      "Epoch 1493, Loss: 0.4256374388933182, Final Batch Loss: 0.1952500343322754\n",
      "Epoch 1494, Loss: 0.41882818937301636, Final Batch Loss: 0.1659909188747406\n",
      "Epoch 1495, Loss: 0.43872298300266266, Final Batch Loss: 0.23099441826343536\n",
      "Epoch 1496, Loss: 0.4109973609447479, Final Batch Loss: 0.18706117570400238\n",
      "Epoch 1497, Loss: 0.4564545750617981, Final Batch Loss: 0.2567759156227112\n",
      "Epoch 1498, Loss: 0.47568391263484955, Final Batch Loss: 0.251512348651886\n",
      "Epoch 1499, Loss: 0.33594685792922974, Final Batch Loss: 0.1713222861289978\n",
      "Epoch 1500, Loss: 0.43619248270988464, Final Batch Loss: 0.24395442008972168\n",
      "Epoch 1501, Loss: 0.40430666506290436, Final Batch Loss: 0.21939025819301605\n",
      "Epoch 1502, Loss: 0.40933549404144287, Final Batch Loss: 0.1523776650428772\n",
      "Epoch 1503, Loss: 0.42369914054870605, Final Batch Loss: 0.18046323955059052\n",
      "Epoch 1504, Loss: 0.44732336699962616, Final Batch Loss: 0.23156575858592987\n",
      "Epoch 1505, Loss: 0.43405313789844513, Final Batch Loss: 0.24994225800037384\n",
      "Epoch 1506, Loss: 0.43754734098911285, Final Batch Loss: 0.21272295713424683\n",
      "Epoch 1507, Loss: 0.3798481225967407, Final Batch Loss: 0.19891566038131714\n",
      "Epoch 1508, Loss: 0.44657500088214874, Final Batch Loss: 0.25934359431266785\n",
      "Epoch 1509, Loss: 0.4426162838935852, Final Batch Loss: 0.23567408323287964\n",
      "Epoch 1510, Loss: 0.4306189566850662, Final Batch Loss: 0.23367905616760254\n",
      "Epoch 1511, Loss: 0.43400268256664276, Final Batch Loss: 0.19479377567768097\n",
      "Epoch 1512, Loss: 0.36962802708148956, Final Batch Loss: 0.19259294867515564\n",
      "Epoch 1513, Loss: 0.35568051040172577, Final Batch Loss: 0.17146441340446472\n",
      "Epoch 1514, Loss: 0.4119618088006973, Final Batch Loss: 0.19780130684375763\n",
      "Epoch 1515, Loss: 0.4110678881406784, Final Batch Loss: 0.23065581917762756\n",
      "Epoch 1516, Loss: 0.36272306740283966, Final Batch Loss: 0.18414323031902313\n",
      "Epoch 1517, Loss: 0.35681670904159546, Final Batch Loss: 0.1455734372138977\n",
      "Epoch 1518, Loss: 0.36330416798591614, Final Batch Loss: 0.17033153772354126\n",
      "Epoch 1519, Loss: 0.4400731027126312, Final Batch Loss: 0.2252594530582428\n",
      "Epoch 1520, Loss: 0.35674913227558136, Final Batch Loss: 0.18265680968761444\n",
      "Epoch 1521, Loss: 0.3976999968290329, Final Batch Loss: 0.20386090874671936\n",
      "Epoch 1522, Loss: 0.39508405327796936, Final Batch Loss: 0.23913691937923431\n",
      "Epoch 1523, Loss: 0.37577567994594574, Final Batch Loss: 0.15878050029277802\n",
      "Epoch 1524, Loss: 0.5063155740499496, Final Batch Loss: 0.28388532996177673\n",
      "Epoch 1525, Loss: 0.4550012797117233, Final Batch Loss: 0.20550884306430817\n",
      "Epoch 1526, Loss: 0.37895216047763824, Final Batch Loss: 0.188537135720253\n",
      "Epoch 1527, Loss: 0.33813823014497757, Final Batch Loss: 0.12147731333971024\n",
      "Epoch 1528, Loss: 0.42922575771808624, Final Batch Loss: 0.24468263983726501\n",
      "Epoch 1529, Loss: 0.4210451543331146, Final Batch Loss: 0.1859886795282364\n",
      "Epoch 1530, Loss: 0.4473077058792114, Final Batch Loss: 0.23644037544727325\n",
      "Epoch 1531, Loss: 0.3498260825872421, Final Batch Loss: 0.15773151814937592\n",
      "Epoch 1532, Loss: 0.4482720196247101, Final Batch Loss: 0.255525678396225\n",
      "Epoch 1533, Loss: 0.32835960388183594, Final Batch Loss: 0.16150563955307007\n",
      "Epoch 1534, Loss: 0.4065065383911133, Final Batch Loss: 0.18661397695541382\n",
      "Epoch 1535, Loss: 0.39579659700393677, Final Batch Loss: 0.19949811697006226\n",
      "Epoch 1536, Loss: 0.4837716221809387, Final Batch Loss: 0.22171688079833984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1537, Loss: 0.3816337287425995, Final Batch Loss: 0.23886418342590332\n",
      "Epoch 1538, Loss: 0.3203142285346985, Final Batch Loss: 0.16369204223155975\n",
      "Epoch 1539, Loss: 0.37321479618549347, Final Batch Loss: 0.1952640414237976\n",
      "Epoch 1540, Loss: 0.3900110274553299, Final Batch Loss: 0.17076581716537476\n",
      "Epoch 1541, Loss: 0.48320940136909485, Final Batch Loss: 0.25367167592048645\n",
      "Epoch 1542, Loss: 0.40167365968227386, Final Batch Loss: 0.20803400874137878\n",
      "Epoch 1543, Loss: 0.45727819204330444, Final Batch Loss: 0.21667145192623138\n",
      "Epoch 1544, Loss: 0.3816353529691696, Final Batch Loss: 0.22399522364139557\n",
      "Epoch 1545, Loss: 0.3697872906923294, Final Batch Loss: 0.20692703127861023\n",
      "Epoch 1546, Loss: 0.3944351375102997, Final Batch Loss: 0.1739756315946579\n",
      "Epoch 1547, Loss: 0.4172869026660919, Final Batch Loss: 0.21619342267513275\n",
      "Epoch 1548, Loss: 0.3538977652788162, Final Batch Loss: 0.1779680848121643\n",
      "Epoch 1549, Loss: 0.4167506545782089, Final Batch Loss: 0.23516914248466492\n",
      "Epoch 1550, Loss: 0.4146827310323715, Final Batch Loss: 0.19861777126789093\n",
      "Epoch 1551, Loss: 0.45736271142959595, Final Batch Loss: 0.21825705468654633\n",
      "Epoch 1552, Loss: 0.36200718581676483, Final Batch Loss: 0.18743345141410828\n",
      "Epoch 1553, Loss: 0.457639217376709, Final Batch Loss: 0.19599410891532898\n",
      "Epoch 1554, Loss: 0.4192962348461151, Final Batch Loss: 0.22559325397014618\n",
      "Epoch 1555, Loss: 0.4838126450777054, Final Batch Loss: 0.22241239249706268\n",
      "Epoch 1556, Loss: 0.432660236954689, Final Batch Loss: 0.19670428335666656\n",
      "Epoch 1557, Loss: 0.38618020713329315, Final Batch Loss: 0.1923377364873886\n",
      "Epoch 1558, Loss: 0.34209612011909485, Final Batch Loss: 0.17640601098537445\n",
      "Epoch 1559, Loss: 0.3561408221721649, Final Batch Loss: 0.1854545921087265\n",
      "Epoch 1560, Loss: 0.38252949714660645, Final Batch Loss: 0.18621185421943665\n",
      "Epoch 1561, Loss: 0.40684370696544647, Final Batch Loss: 0.22863048315048218\n",
      "Epoch 1562, Loss: 0.37158775329589844, Final Batch Loss: 0.17434771358966827\n",
      "Epoch 1563, Loss: 0.4417843371629715, Final Batch Loss: 0.22544890642166138\n",
      "Epoch 1564, Loss: 0.4217761904001236, Final Batch Loss: 0.20444223284721375\n",
      "Epoch 1565, Loss: 0.47290295362472534, Final Batch Loss: 0.2377268522977829\n",
      "Epoch 1566, Loss: 0.3268834203481674, Final Batch Loss: 0.16053615510463715\n",
      "Epoch 1567, Loss: 0.3421282321214676, Final Batch Loss: 0.17854008078575134\n",
      "Epoch 1568, Loss: 0.45510129630565643, Final Batch Loss: 0.2197096347808838\n",
      "Epoch 1569, Loss: 0.33473020792007446, Final Batch Loss: 0.1632314771413803\n",
      "Epoch 1570, Loss: 0.3544606864452362, Final Batch Loss: 0.1875191181898117\n",
      "Epoch 1571, Loss: 0.38779695332050323, Final Batch Loss: 0.17910131812095642\n",
      "Epoch 1572, Loss: 0.42000120878219604, Final Batch Loss: 0.20762403309345245\n",
      "Epoch 1573, Loss: 0.3549870550632477, Final Batch Loss: 0.1954140067100525\n",
      "Epoch 1574, Loss: 0.36048761010169983, Final Batch Loss: 0.19201454520225525\n",
      "Epoch 1575, Loss: 0.42232024669647217, Final Batch Loss: 0.20871272683143616\n",
      "Epoch 1576, Loss: 0.41059166193008423, Final Batch Loss: 0.21680928766727448\n",
      "Epoch 1577, Loss: 0.42689795792102814, Final Batch Loss: 0.22756171226501465\n",
      "Epoch 1578, Loss: 0.36954230070114136, Final Batch Loss: 0.19779525697231293\n",
      "Epoch 1579, Loss: 0.38003362715244293, Final Batch Loss: 0.1523449569940567\n",
      "Epoch 1580, Loss: 0.40049730241298676, Final Batch Loss: 0.17236287891864777\n",
      "Epoch 1581, Loss: 0.40600666403770447, Final Batch Loss: 0.18853610754013062\n",
      "Epoch 1582, Loss: 0.44823697209358215, Final Batch Loss: 0.22440074384212494\n",
      "Epoch 1583, Loss: 0.41658104956150055, Final Batch Loss: 0.18984229862689972\n",
      "Epoch 1584, Loss: 0.3250627666711807, Final Batch Loss: 0.19422036409378052\n",
      "Epoch 1585, Loss: 0.3349319100379944, Final Batch Loss: 0.18448810279369354\n",
      "Epoch 1586, Loss: 0.4565088599920273, Final Batch Loss: 0.2520129084587097\n",
      "Epoch 1587, Loss: 0.38476721942424774, Final Batch Loss: 0.2157033085823059\n",
      "Epoch 1588, Loss: 0.3456844389438629, Final Batch Loss: 0.16440919041633606\n",
      "Epoch 1589, Loss: 0.3810140788555145, Final Batch Loss: 0.20291543006896973\n",
      "Epoch 1590, Loss: 0.5042034536600113, Final Batch Loss: 0.2628780007362366\n",
      "Epoch 1591, Loss: 0.3404914140701294, Final Batch Loss: 0.16668158769607544\n",
      "Epoch 1592, Loss: 0.3833204060792923, Final Batch Loss: 0.19408327341079712\n",
      "Epoch 1593, Loss: 0.4339705556631088, Final Batch Loss: 0.2721817195415497\n",
      "Epoch 1594, Loss: 0.33591751754283905, Final Batch Loss: 0.17523673176765442\n",
      "Epoch 1595, Loss: 0.3145485371351242, Final Batch Loss: 0.16673940420150757\n",
      "Epoch 1596, Loss: 0.3427451401948929, Final Batch Loss: 0.1819358766078949\n",
      "Epoch 1597, Loss: 0.31869472563266754, Final Batch Loss: 0.1799934208393097\n",
      "Epoch 1598, Loss: 0.3743521124124527, Final Batch Loss: 0.17873935401439667\n",
      "Epoch 1599, Loss: 0.41209736466407776, Final Batch Loss: 0.24630801379680634\n",
      "Epoch 1600, Loss: 0.4298294931650162, Final Batch Loss: 0.20094795525074005\n",
      "Epoch 1601, Loss: 0.36441606283187866, Final Batch Loss: 0.1885981410741806\n",
      "Epoch 1602, Loss: 0.38646411895751953, Final Batch Loss: 0.19284053146839142\n",
      "Epoch 1603, Loss: 0.31199289858341217, Final Batch Loss: 0.1618107110261917\n",
      "Epoch 1604, Loss: 0.36798615753650665, Final Batch Loss: 0.18710123002529144\n",
      "Epoch 1605, Loss: 0.4674619734287262, Final Batch Loss: 0.22712674736976624\n",
      "Epoch 1606, Loss: 0.3987179845571518, Final Batch Loss: 0.18322911858558655\n",
      "Epoch 1607, Loss: 0.3929121047258377, Final Batch Loss: 0.16567964851856232\n",
      "Epoch 1608, Loss: 0.42086032032966614, Final Batch Loss: 0.19334343075752258\n",
      "Epoch 1609, Loss: 0.3868356943130493, Final Batch Loss: 0.1805538684129715\n",
      "Epoch 1610, Loss: 0.3764003813266754, Final Batch Loss: 0.1765861213207245\n",
      "Epoch 1611, Loss: 0.42245684564113617, Final Batch Loss: 0.21261928975582123\n",
      "Epoch 1612, Loss: 0.40066930651664734, Final Batch Loss: 0.20910456776618958\n",
      "Epoch 1613, Loss: 0.42790262401103973, Final Batch Loss: 0.24520529806613922\n",
      "Epoch 1614, Loss: 0.33577683568000793, Final Batch Loss: 0.16035595536231995\n",
      "Epoch 1615, Loss: 0.4298523813486099, Final Batch Loss: 0.22417579591274261\n",
      "Epoch 1616, Loss: 0.3937681019306183, Final Batch Loss: 0.2189767062664032\n",
      "Epoch 1617, Loss: 0.36843739449977875, Final Batch Loss: 0.18079541623592377\n",
      "Epoch 1618, Loss: 0.3161150962114334, Final Batch Loss: 0.14682340621948242\n",
      "Epoch 1619, Loss: 0.4604480266571045, Final Batch Loss: 0.23593607544898987\n",
      "Epoch 1620, Loss: 0.30911390483379364, Final Batch Loss: 0.1405048370361328\n",
      "Epoch 1621, Loss: 0.3907814472913742, Final Batch Loss: 0.16603359580039978\n",
      "Epoch 1622, Loss: 0.3581908643245697, Final Batch Loss: 0.15990614891052246\n",
      "Epoch 1623, Loss: 0.417969286441803, Final Batch Loss: 0.21822670102119446\n",
      "Epoch 1624, Loss: 0.3377123028039932, Final Batch Loss: 0.14330020546913147\n",
      "Epoch 1625, Loss: 0.43274711072444916, Final Batch Loss: 0.22455202043056488\n",
      "Epoch 1626, Loss: 0.3908454030752182, Final Batch Loss: 0.1740722507238388\n",
      "Epoch 1627, Loss: 0.3113696724176407, Final Batch Loss: 0.1594955176115036\n",
      "Epoch 1628, Loss: 0.3897280842065811, Final Batch Loss: 0.1989736109972\n",
      "Epoch 1629, Loss: 0.32245883345603943, Final Batch Loss: 0.1564483344554901\n",
      "Epoch 1630, Loss: 0.388653427362442, Final Batch Loss: 0.19898860156536102\n",
      "Epoch 1631, Loss: 0.3161391615867615, Final Batch Loss: 0.14508207142353058\n",
      "Epoch 1632, Loss: 0.4153338223695755, Final Batch Loss: 0.20485496520996094\n",
      "Epoch 1633, Loss: 0.32167671620845795, Final Batch Loss: 0.1913238912820816\n",
      "Epoch 1634, Loss: 0.37116411328315735, Final Batch Loss: 0.16209913790225983\n",
      "Epoch 1635, Loss: 0.4361211061477661, Final Batch Loss: 0.20355108380317688\n",
      "Epoch 1636, Loss: 0.4188200682401657, Final Batch Loss: 0.215544655919075\n",
      "Epoch 1637, Loss: 0.32021187990903854, Final Batch Loss: 0.20292696356773376\n",
      "Epoch 1638, Loss: 0.3503793478012085, Final Batch Loss: 0.17363379895687103\n",
      "Epoch 1639, Loss: 0.35040025413036346, Final Batch Loss: 0.15912802517414093\n",
      "Epoch 1640, Loss: 0.39481285214424133, Final Batch Loss: 0.2472812533378601\n",
      "Epoch 1641, Loss: 0.43008068203926086, Final Batch Loss: 0.22907938063144684\n",
      "Epoch 1642, Loss: 0.36734823882579803, Final Batch Loss: 0.195389062166214\n",
      "Epoch 1643, Loss: 0.4303574562072754, Final Batch Loss: 0.2439291775226593\n",
      "Epoch 1644, Loss: 0.40923330187797546, Final Batch Loss: 0.23586364090442657\n",
      "Epoch 1645, Loss: 0.445191890001297, Final Batch Loss: 0.221851646900177\n",
      "Epoch 1646, Loss: 0.4349699765443802, Final Batch Loss: 0.22879700362682343\n",
      "Epoch 1647, Loss: 0.3992757648229599, Final Batch Loss: 0.1801726371049881\n",
      "Epoch 1648, Loss: 0.35619306564331055, Final Batch Loss: 0.18148121237754822\n",
      "Epoch 1649, Loss: 0.42471839487552643, Final Batch Loss: 0.22751393914222717\n",
      "Epoch 1650, Loss: 0.3466244041919708, Final Batch Loss: 0.17212772369384766\n",
      "Epoch 1651, Loss: 0.3965185284614563, Final Batch Loss: 0.23617197573184967\n",
      "Epoch 1652, Loss: 0.40131404995918274, Final Batch Loss: 0.22709737718105316\n",
      "Epoch 1653, Loss: 0.3832572400569916, Final Batch Loss: 0.16507825255393982\n",
      "Epoch 1654, Loss: 0.3558996319770813, Final Batch Loss: 0.19404169917106628\n",
      "Epoch 1655, Loss: 0.33974048495292664, Final Batch Loss: 0.15645018219947815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1656, Loss: 0.33717482537031174, Final Batch Loss: 0.2166900485754013\n",
      "Epoch 1657, Loss: 0.37611041963100433, Final Batch Loss: 0.20438985526561737\n",
      "Epoch 1658, Loss: 0.3603600114583969, Final Batch Loss: 0.18108722567558289\n",
      "Epoch 1659, Loss: 0.3592218607664108, Final Batch Loss: 0.17598393559455872\n",
      "Epoch 1660, Loss: 0.3444943428039551, Final Batch Loss: 0.2417403906583786\n",
      "Epoch 1661, Loss: 0.42780718207359314, Final Batch Loss: 0.20050162076950073\n",
      "Epoch 1662, Loss: 0.4376335144042969, Final Batch Loss: 0.21138402819633484\n",
      "Epoch 1663, Loss: 0.408477321267128, Final Batch Loss: 0.25495004653930664\n",
      "Epoch 1664, Loss: 0.3280358910560608, Final Batch Loss: 0.15624882280826569\n",
      "Epoch 1665, Loss: 0.379445880651474, Final Batch Loss: 0.13696171343326569\n",
      "Epoch 1666, Loss: 0.35301704704761505, Final Batch Loss: 0.18994762003421783\n",
      "Epoch 1667, Loss: 0.40912875533103943, Final Batch Loss: 0.18627335131168365\n",
      "Epoch 1668, Loss: 0.43357376754283905, Final Batch Loss: 0.24578572809696198\n",
      "Epoch 1669, Loss: 0.3262075185775757, Final Batch Loss: 0.17212294042110443\n",
      "Epoch 1670, Loss: 0.34075474739074707, Final Batch Loss: 0.19721974432468414\n",
      "Epoch 1671, Loss: 0.2910134494304657, Final Batch Loss: 0.15023641288280487\n",
      "Epoch 1672, Loss: 0.39867164194583893, Final Batch Loss: 0.18906493484973907\n",
      "Epoch 1673, Loss: 0.3206580728292465, Final Batch Loss: 0.16170954704284668\n",
      "Epoch 1674, Loss: 0.34659726917743683, Final Batch Loss: 0.1542227566242218\n",
      "Epoch 1675, Loss: 0.4212312251329422, Final Batch Loss: 0.2668406665325165\n",
      "Epoch 1676, Loss: 0.38398247957229614, Final Batch Loss: 0.17269976437091827\n",
      "Epoch 1677, Loss: 0.37071317434310913, Final Batch Loss: 0.1859201341867447\n",
      "Epoch 1678, Loss: 0.33137668669223785, Final Batch Loss: 0.18208031356334686\n",
      "Epoch 1679, Loss: 0.4090363681316376, Final Batch Loss: 0.17257343232631683\n",
      "Epoch 1680, Loss: 0.32914187014102936, Final Batch Loss: 0.17892497777938843\n",
      "Epoch 1681, Loss: 0.3987705856561661, Final Batch Loss: 0.2356286346912384\n",
      "Epoch 1682, Loss: 0.3843667656183243, Final Batch Loss: 0.1743374615907669\n",
      "Epoch 1683, Loss: 0.378161296248436, Final Batch Loss: 0.19594822824001312\n",
      "Epoch 1684, Loss: 0.33542250096797943, Final Batch Loss: 0.1569993942975998\n",
      "Epoch 1685, Loss: 0.3601280599832535, Final Batch Loss: 0.20297101140022278\n",
      "Epoch 1686, Loss: 0.3984874188899994, Final Batch Loss: 0.2396322786808014\n",
      "Epoch 1687, Loss: 0.4225659668445587, Final Batch Loss: 0.21101154386997223\n",
      "Epoch 1688, Loss: 0.385041281580925, Final Batch Loss: 0.1442107856273651\n",
      "Epoch 1689, Loss: 0.3518216162919998, Final Batch Loss: 0.1896495521068573\n",
      "Epoch 1690, Loss: 0.3683324307203293, Final Batch Loss: 0.1716393530368805\n",
      "Epoch 1691, Loss: 0.37074074149131775, Final Batch Loss: 0.180420383810997\n",
      "Epoch 1692, Loss: 0.3683691769838333, Final Batch Loss: 0.20339861512184143\n",
      "Epoch 1693, Loss: 0.3625599443912506, Final Batch Loss: 0.12834633886814117\n",
      "Epoch 1694, Loss: 0.34337638318538666, Final Batch Loss: 0.17662422358989716\n",
      "Epoch 1695, Loss: 0.4037995785474777, Final Batch Loss: 0.20843061804771423\n",
      "Epoch 1696, Loss: 0.3185536414384842, Final Batch Loss: 0.18459613621234894\n",
      "Epoch 1697, Loss: 0.3891185522079468, Final Batch Loss: 0.19753609597682953\n",
      "Epoch 1698, Loss: 0.4138592332601547, Final Batch Loss: 0.2147010713815689\n",
      "Epoch 1699, Loss: 0.37969331443309784, Final Batch Loss: 0.18151959776878357\n",
      "Epoch 1700, Loss: 0.32382600009441376, Final Batch Loss: 0.1792566478252411\n",
      "Epoch 1701, Loss: 0.40903380513191223, Final Batch Loss: 0.20568053424358368\n",
      "Epoch 1702, Loss: 0.3230489492416382, Final Batch Loss: 0.1596730649471283\n",
      "Epoch 1703, Loss: 0.38061730563640594, Final Batch Loss: 0.18812867999076843\n",
      "Epoch 1704, Loss: 0.3622779846191406, Final Batch Loss: 0.21463152766227722\n",
      "Epoch 1705, Loss: 0.41502757370471954, Final Batch Loss: 0.19752095639705658\n",
      "Epoch 1706, Loss: 0.3722219318151474, Final Batch Loss: 0.1642579585313797\n",
      "Epoch 1707, Loss: 0.3615396320819855, Final Batch Loss: 0.18521800637245178\n",
      "Epoch 1708, Loss: 0.3749973922967911, Final Batch Loss: 0.20404189825057983\n",
      "Epoch 1709, Loss: 0.3518805056810379, Final Batch Loss: 0.16233277320861816\n",
      "Epoch 1710, Loss: 0.34797269105911255, Final Batch Loss: 0.19282832741737366\n",
      "Epoch 1711, Loss: 0.45157355070114136, Final Batch Loss: 0.26367539167404175\n",
      "Epoch 1712, Loss: 0.4012640565633774, Final Batch Loss: 0.2156660258769989\n",
      "Epoch 1713, Loss: 0.3951602578163147, Final Batch Loss: 0.18331773579120636\n",
      "Epoch 1714, Loss: 0.3801024556159973, Final Batch Loss: 0.1827508509159088\n",
      "Epoch 1715, Loss: 0.4326031357049942, Final Batch Loss: 0.1730930060148239\n",
      "Epoch 1716, Loss: 0.3889899104833603, Final Batch Loss: 0.17662319540977478\n",
      "Epoch 1717, Loss: 0.3356272131204605, Final Batch Loss: 0.1632172167301178\n",
      "Epoch 1718, Loss: 0.3759501874446869, Final Batch Loss: 0.1711195409297943\n",
      "Epoch 1719, Loss: 0.44177624583244324, Final Batch Loss: 0.21462436020374298\n",
      "Epoch 1720, Loss: 0.32011860609054565, Final Batch Loss: 0.14826396107673645\n",
      "Epoch 1721, Loss: 0.2909882441163063, Final Batch Loss: 0.1674676537513733\n",
      "Epoch 1722, Loss: 0.4151894152164459, Final Batch Loss: 0.20432497560977936\n",
      "Epoch 1723, Loss: 0.3441692441701889, Final Batch Loss: 0.18052522838115692\n",
      "Epoch 1724, Loss: 0.28188712894916534, Final Batch Loss: 0.12298642098903656\n",
      "Epoch 1725, Loss: 0.4362219125032425, Final Batch Loss: 0.21614694595336914\n",
      "Epoch 1726, Loss: 0.383623406291008, Final Batch Loss: 0.18271546065807343\n",
      "Epoch 1727, Loss: 0.39913372695446014, Final Batch Loss: 0.21354106068611145\n",
      "Epoch 1728, Loss: 0.37962333858013153, Final Batch Loss: 0.1900636851787567\n",
      "Epoch 1729, Loss: 0.34823112189769745, Final Batch Loss: 0.17993511259555817\n",
      "Epoch 1730, Loss: 0.40222567319869995, Final Batch Loss: 0.2466285079717636\n",
      "Epoch 1731, Loss: 0.34733982384204865, Final Batch Loss: 0.15093868970870972\n",
      "Epoch 1732, Loss: 0.34560948610305786, Final Batch Loss: 0.20861737430095673\n",
      "Epoch 1733, Loss: 0.35283873975276947, Final Batch Loss: 0.17603641748428345\n",
      "Epoch 1734, Loss: 0.4082578867673874, Final Batch Loss: 0.21389032900333405\n",
      "Epoch 1735, Loss: 0.35724666714668274, Final Batch Loss: 0.18408527970314026\n",
      "Epoch 1736, Loss: 0.32858239114284515, Final Batch Loss: 0.15581639111042023\n",
      "Epoch 1737, Loss: 0.3430050015449524, Final Batch Loss: 0.20172543823719025\n",
      "Epoch 1738, Loss: 0.2684558629989624, Final Batch Loss: 0.1420055329799652\n",
      "Epoch 1739, Loss: 0.4218994379043579, Final Batch Loss: 0.1914862096309662\n",
      "Epoch 1740, Loss: 0.4105066508054733, Final Batch Loss: 0.23705044388771057\n",
      "Epoch 1741, Loss: 0.42059022188186646, Final Batch Loss: 0.18927903473377228\n",
      "Epoch 1742, Loss: 0.3130310922861099, Final Batch Loss: 0.17974139750003815\n",
      "Epoch 1743, Loss: 0.363530769944191, Final Batch Loss: 0.17412106692790985\n",
      "Epoch 1744, Loss: 0.4225611537694931, Final Batch Loss: 0.17739027738571167\n",
      "Epoch 1745, Loss: 0.3373069167137146, Final Batch Loss: 0.20351408421993256\n",
      "Epoch 1746, Loss: 0.38505685329437256, Final Batch Loss: 0.2311137467622757\n",
      "Epoch 1747, Loss: 0.3724796622991562, Final Batch Loss: 0.1815144270658493\n",
      "Epoch 1748, Loss: 0.3738126754760742, Final Batch Loss: 0.18659387528896332\n",
      "Epoch 1749, Loss: 0.4212367683649063, Final Batch Loss: 0.20781166851520538\n",
      "Epoch 1750, Loss: 0.4268537759780884, Final Batch Loss: 0.2780487835407257\n",
      "Epoch 1751, Loss: 0.3222477585077286, Final Batch Loss: 0.16640301048755646\n",
      "Epoch 1752, Loss: 0.3780509978532791, Final Batch Loss: 0.18734867870807648\n",
      "Epoch 1753, Loss: 0.4142832010984421, Final Batch Loss: 0.1977488100528717\n",
      "Epoch 1754, Loss: 0.42638877034187317, Final Batch Loss: 0.19220969080924988\n",
      "Epoch 1755, Loss: 0.3165280669927597, Final Batch Loss: 0.14711886644363403\n",
      "Epoch 1756, Loss: 0.3288929611444473, Final Batch Loss: 0.20323996245861053\n",
      "Epoch 1757, Loss: 0.45220349729061127, Final Batch Loss: 0.19677574932575226\n",
      "Epoch 1758, Loss: 0.32745982706546783, Final Batch Loss: 0.17035020887851715\n",
      "Epoch 1759, Loss: 0.370775505900383, Final Batch Loss: 0.14143095910549164\n",
      "Epoch 1760, Loss: 0.3521113097667694, Final Batch Loss: 0.16377240419387817\n",
      "Epoch 1761, Loss: 0.383538618683815, Final Batch Loss: 0.17932787537574768\n",
      "Epoch 1762, Loss: 0.3819601535797119, Final Batch Loss: 0.21001027524471283\n",
      "Epoch 1763, Loss: 0.39471563696861267, Final Batch Loss: 0.20334789156913757\n",
      "Epoch 1764, Loss: 0.35276132822036743, Final Batch Loss: 0.1680653840303421\n",
      "Epoch 1765, Loss: 0.3821873962879181, Final Batch Loss: 0.189365416765213\n",
      "Epoch 1766, Loss: 0.36365365982055664, Final Batch Loss: 0.18285511434078217\n",
      "Epoch 1767, Loss: 0.3709879070520401, Final Batch Loss: 0.1591162383556366\n",
      "Epoch 1768, Loss: 0.33338868618011475, Final Batch Loss: 0.15828576683998108\n",
      "Epoch 1769, Loss: 0.31736503541469574, Final Batch Loss: 0.15957477688789368\n",
      "Epoch 1770, Loss: 0.4425392150878906, Final Batch Loss: 0.24272705614566803\n",
      "Epoch 1771, Loss: 0.37448030710220337, Final Batch Loss: 0.19800491631031036\n",
      "Epoch 1772, Loss: 0.36385567486286163, Final Batch Loss: 0.1911710649728775\n",
      "Epoch 1773, Loss: 0.3244856148958206, Final Batch Loss: 0.1412501186132431\n",
      "Epoch 1774, Loss: 0.3509919047355652, Final Batch Loss: 0.18005719780921936\n",
      "Epoch 1775, Loss: 0.34583616256713867, Final Batch Loss: 0.15924742817878723\n",
      "Epoch 1776, Loss: 0.3911989629268646, Final Batch Loss: 0.16400772333145142\n",
      "Epoch 1777, Loss: 0.38428016006946564, Final Batch Loss: 0.1794731169939041\n",
      "Epoch 1778, Loss: 0.3272119462490082, Final Batch Loss: 0.1527257263660431\n",
      "Epoch 1779, Loss: 0.29741039872169495, Final Batch Loss: 0.1413773000240326\n",
      "Epoch 1780, Loss: 0.341956228017807, Final Batch Loss: 0.18870191276073456\n",
      "Epoch 1781, Loss: 0.37489528954029083, Final Batch Loss: 0.21262337267398834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1782, Loss: 0.375929519534111, Final Batch Loss: 0.16302579641342163\n",
      "Epoch 1783, Loss: 0.33170144259929657, Final Batch Loss: 0.19351772964000702\n",
      "Epoch 1784, Loss: 0.332312673330307, Final Batch Loss: 0.13541381061077118\n",
      "Epoch 1785, Loss: 0.37672725319862366, Final Batch Loss: 0.1991214007139206\n",
      "Epoch 1786, Loss: 0.37113238871097565, Final Batch Loss: 0.17633429169654846\n",
      "Epoch 1787, Loss: 0.3173336684703827, Final Batch Loss: 0.1796284019947052\n",
      "Epoch 1788, Loss: 0.31274086236953735, Final Batch Loss: 0.15471221506595612\n",
      "Epoch 1789, Loss: 0.4107695072889328, Final Batch Loss: 0.26135191321372986\n",
      "Epoch 1790, Loss: 0.297310471534729, Final Batch Loss: 0.13966533541679382\n",
      "Epoch 1791, Loss: 0.35975128412246704, Final Batch Loss: 0.1500387191772461\n",
      "Epoch 1792, Loss: 0.3870723694562912, Final Batch Loss: 0.19781212508678436\n",
      "Epoch 1793, Loss: 0.3467746078968048, Final Batch Loss: 0.17660759389400482\n",
      "Epoch 1794, Loss: 0.35691502690315247, Final Batch Loss: 0.1496165543794632\n",
      "Epoch 1795, Loss: 0.34325166046619415, Final Batch Loss: 0.12683312594890594\n",
      "Epoch 1796, Loss: 0.36546529829502106, Final Batch Loss: 0.1835210621356964\n",
      "Epoch 1797, Loss: 0.3957103341817856, Final Batch Loss: 0.1969558745622635\n",
      "Epoch 1798, Loss: 0.3662816882133484, Final Batch Loss: 0.19366969168186188\n",
      "Epoch 1799, Loss: 0.45719383656978607, Final Batch Loss: 0.2508453130722046\n",
      "Epoch 1800, Loss: 0.41489845514297485, Final Batch Loss: 0.23807474970817566\n",
      "Epoch 1801, Loss: 0.4091433733701706, Final Batch Loss: 0.19980111718177795\n",
      "Epoch 1802, Loss: 0.3562604635953903, Final Batch Loss: 0.171370729804039\n",
      "Epoch 1803, Loss: 0.3070381283760071, Final Batch Loss: 0.1303447037935257\n",
      "Epoch 1804, Loss: 0.35599440336227417, Final Batch Loss: 0.16338281333446503\n",
      "Epoch 1805, Loss: 0.4111984968185425, Final Batch Loss: 0.22870378196239471\n",
      "Epoch 1806, Loss: 0.3121347725391388, Final Batch Loss: 0.17224472761154175\n",
      "Epoch 1807, Loss: 0.33827026188373566, Final Batch Loss: 0.18015803396701813\n",
      "Epoch 1808, Loss: 0.3851161152124405, Final Batch Loss: 0.18703487515449524\n",
      "Epoch 1809, Loss: 0.40917736291885376, Final Batch Loss: 0.1683381050825119\n",
      "Epoch 1810, Loss: 0.3409659415483475, Final Batch Loss: 0.19308090209960938\n",
      "Epoch 1811, Loss: 0.36432787775993347, Final Batch Loss: 0.15100950002670288\n",
      "Epoch 1812, Loss: 0.29615817964076996, Final Batch Loss: 0.16380485892295837\n",
      "Epoch 1813, Loss: 0.361647292971611, Final Batch Loss: 0.16057662665843964\n",
      "Epoch 1814, Loss: 0.3492962568998337, Final Batch Loss: 0.1833883374929428\n",
      "Epoch 1815, Loss: 0.3749953806400299, Final Batch Loss: 0.16924118995666504\n",
      "Epoch 1816, Loss: 0.32449039816856384, Final Batch Loss: 0.16549696028232574\n",
      "Epoch 1817, Loss: 0.35591091215610504, Final Batch Loss: 0.21471692621707916\n",
      "Epoch 1818, Loss: 0.3405633270740509, Final Batch Loss: 0.18698228895664215\n",
      "Epoch 1819, Loss: 0.3400995135307312, Final Batch Loss: 0.1910851001739502\n",
      "Epoch 1820, Loss: 0.35528962314128876, Final Batch Loss: 0.18134810030460358\n",
      "Epoch 1821, Loss: 0.33339254558086395, Final Batch Loss: 0.16222204267978668\n",
      "Epoch 1822, Loss: 0.2887437641620636, Final Batch Loss: 0.14386534690856934\n",
      "Epoch 1823, Loss: 0.386712446808815, Final Batch Loss: 0.19909673929214478\n",
      "Epoch 1824, Loss: 0.337160661816597, Final Batch Loss: 0.16728468239307404\n",
      "Epoch 1825, Loss: 0.3089507967233658, Final Batch Loss: 0.18199843168258667\n",
      "Epoch 1826, Loss: 0.36723843216896057, Final Batch Loss: 0.16193079948425293\n",
      "Epoch 1827, Loss: 0.42664735019207, Final Batch Loss: 0.19929549098014832\n",
      "Epoch 1828, Loss: 0.3024604767560959, Final Batch Loss: 0.14311964809894562\n",
      "Epoch 1829, Loss: 0.3876125365495682, Final Batch Loss: 0.1908426731824875\n",
      "Epoch 1830, Loss: 0.3607294112443924, Final Batch Loss: 0.16886690258979797\n",
      "Epoch 1831, Loss: 0.3498125821352005, Final Batch Loss: 0.1953880488872528\n",
      "Epoch 1832, Loss: 0.2999098151922226, Final Batch Loss: 0.15886647999286652\n",
      "Epoch 1833, Loss: 0.3783607929944992, Final Batch Loss: 0.20800526440143585\n",
      "Epoch 1834, Loss: 0.2746995836496353, Final Batch Loss: 0.12241455912590027\n",
      "Epoch 1835, Loss: 0.33275918662548065, Final Batch Loss: 0.15904489159584045\n",
      "Epoch 1836, Loss: 0.3460479974746704, Final Batch Loss: 0.1601089984178543\n",
      "Epoch 1837, Loss: 0.4125254601240158, Final Batch Loss: 0.22255483269691467\n",
      "Epoch 1838, Loss: 0.2574855238199234, Final Batch Loss: 0.14023225009441376\n",
      "Epoch 1839, Loss: 0.3713586926460266, Final Batch Loss: 0.21800832450389862\n",
      "Epoch 1840, Loss: 0.27263301610946655, Final Batch Loss: 0.11445353925228119\n",
      "Epoch 1841, Loss: 0.3307415693998337, Final Batch Loss: 0.14714272320270538\n",
      "Epoch 1842, Loss: 0.37256065011024475, Final Batch Loss: 0.2158268243074417\n",
      "Epoch 1843, Loss: 0.35348521173000336, Final Batch Loss: 0.1789151281118393\n",
      "Epoch 1844, Loss: 0.2970222383737564, Final Batch Loss: 0.15266846120357513\n",
      "Epoch 1845, Loss: 0.380846306681633, Final Batch Loss: 0.21687118709087372\n",
      "Epoch 1846, Loss: 0.3671991527080536, Final Batch Loss: 0.21394261717796326\n",
      "Epoch 1847, Loss: 0.3651446998119354, Final Batch Loss: 0.14836852252483368\n",
      "Epoch 1848, Loss: 0.3521096706390381, Final Batch Loss: 0.15728314220905304\n",
      "Epoch 1849, Loss: 0.3399188220500946, Final Batch Loss: 0.19815929234027863\n",
      "Epoch 1850, Loss: 0.37773919105529785, Final Batch Loss: 0.19609908759593964\n",
      "Epoch 1851, Loss: 0.30343952775001526, Final Batch Loss: 0.12986384332180023\n",
      "Epoch 1852, Loss: 0.3190618306398392, Final Batch Loss: 0.1581091284751892\n",
      "Epoch 1853, Loss: 0.37993259727954865, Final Batch Loss: 0.178141251206398\n",
      "Epoch 1854, Loss: 0.355065256357193, Final Batch Loss: 0.18677735328674316\n",
      "Epoch 1855, Loss: 0.3749319463968277, Final Batch Loss: 0.15334652364253998\n",
      "Epoch 1856, Loss: 0.3489421010017395, Final Batch Loss: 0.19806262850761414\n",
      "Epoch 1857, Loss: 0.31918027997016907, Final Batch Loss: 0.16790971159934998\n",
      "Epoch 1858, Loss: 0.3274395316839218, Final Batch Loss: 0.1419038623571396\n",
      "Epoch 1859, Loss: 0.38697361946105957, Final Batch Loss: 0.21012148261070251\n",
      "Epoch 1860, Loss: 0.4224205017089844, Final Batch Loss: 0.17944709956645966\n",
      "Epoch 1861, Loss: 0.3795170187950134, Final Batch Loss: 0.20804771780967712\n",
      "Epoch 1862, Loss: 0.3552921265363693, Final Batch Loss: 0.16996710002422333\n",
      "Epoch 1863, Loss: 0.40722431242465973, Final Batch Loss: 0.22309735417366028\n",
      "Epoch 1864, Loss: 0.3714437335729599, Final Batch Loss: 0.2150602787733078\n",
      "Epoch 1865, Loss: 0.3719414323568344, Final Batch Loss: 0.15268097817897797\n",
      "Epoch 1866, Loss: 0.34097079932689667, Final Batch Loss: 0.18460609018802643\n",
      "Epoch 1867, Loss: 0.27107658982276917, Final Batch Loss: 0.1492399126291275\n",
      "Epoch 1868, Loss: 0.2971746474504471, Final Batch Loss: 0.17144663631916046\n",
      "Epoch 1869, Loss: 0.3634200692176819, Final Batch Loss: 0.2338838279247284\n",
      "Epoch 1870, Loss: 0.3500402122735977, Final Batch Loss: 0.1860259622335434\n",
      "Epoch 1871, Loss: 0.3696054518222809, Final Batch Loss: 0.15599830448627472\n",
      "Epoch 1872, Loss: 0.4171243757009506, Final Batch Loss: 0.18901018798351288\n",
      "Epoch 1873, Loss: 0.35359081625938416, Final Batch Loss: 0.1427113115787506\n",
      "Epoch 1874, Loss: 0.3554241359233856, Final Batch Loss: 0.18928295373916626\n",
      "Epoch 1875, Loss: 0.2979908585548401, Final Batch Loss: 0.12893672287464142\n",
      "Epoch 1876, Loss: 0.38398727774620056, Final Batch Loss: 0.2096860110759735\n",
      "Epoch 1877, Loss: 0.43084272742271423, Final Batch Loss: 0.18998517096042633\n",
      "Epoch 1878, Loss: 0.37149836122989655, Final Batch Loss: 0.19924725592136383\n",
      "Epoch 1879, Loss: 0.34560245275497437, Final Batch Loss: 0.20846228301525116\n",
      "Epoch 1880, Loss: 0.3151651471853256, Final Batch Loss: 0.16856500506401062\n",
      "Epoch 1881, Loss: 0.3838920295238495, Final Batch Loss: 0.1840875744819641\n",
      "Epoch 1882, Loss: 0.3403540551662445, Final Batch Loss: 0.1803622543811798\n",
      "Epoch 1883, Loss: 0.30580151081085205, Final Batch Loss: 0.137669637799263\n",
      "Epoch 1884, Loss: 0.3132798969745636, Final Batch Loss: 0.132622629404068\n",
      "Epoch 1885, Loss: 0.37829720973968506, Final Batch Loss: 0.19425611197948456\n",
      "Epoch 1886, Loss: 0.4058356434106827, Final Batch Loss: 0.23883181810379028\n",
      "Epoch 1887, Loss: 0.3133194297552109, Final Batch Loss: 0.1582108438014984\n",
      "Epoch 1888, Loss: 0.3359156399965286, Final Batch Loss: 0.1713665872812271\n",
      "Epoch 1889, Loss: 0.3566698282957077, Final Batch Loss: 0.19191855192184448\n",
      "Epoch 1890, Loss: 0.3589373826980591, Final Batch Loss: 0.15488287806510925\n",
      "Epoch 1891, Loss: 0.4103666692972183, Final Batch Loss: 0.2021750807762146\n",
      "Epoch 1892, Loss: 0.39923126995563507, Final Batch Loss: 0.18003179132938385\n",
      "Epoch 1893, Loss: 0.38473936915397644, Final Batch Loss: 0.1709980070590973\n",
      "Epoch 1894, Loss: 0.32249972224235535, Final Batch Loss: 0.18485718965530396\n",
      "Epoch 1895, Loss: 0.30098526179790497, Final Batch Loss: 0.1524750292301178\n",
      "Epoch 1896, Loss: 0.3098123222589493, Final Batch Loss: 0.15709534287452698\n",
      "Epoch 1897, Loss: 0.3705844283103943, Final Batch Loss: 0.19131149351596832\n",
      "Epoch 1898, Loss: 0.3776962012052536, Final Batch Loss: 0.19678960740566254\n",
      "Epoch 1899, Loss: 0.33653029799461365, Final Batch Loss: 0.16669079661369324\n",
      "Epoch 1900, Loss: 0.2813776731491089, Final Batch Loss: 0.14099277555942535\n",
      "Epoch 1901, Loss: 0.31665003299713135, Final Batch Loss: 0.15959784388542175\n",
      "Epoch 1902, Loss: 0.35915277898311615, Final Batch Loss: 0.16383661329746246\n",
      "Epoch 1903, Loss: 0.278074212372303, Final Batch Loss: 0.1085699275135994\n",
      "Epoch 1904, Loss: 0.2931005358695984, Final Batch Loss: 0.17587924003601074\n",
      "Epoch 1905, Loss: 0.3086070269346237, Final Batch Loss: 0.1577993929386139\n",
      "Epoch 1906, Loss: 0.33511191606521606, Final Batch Loss: 0.16395489871501923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1907, Loss: 0.3064371198415756, Final Batch Loss: 0.161319762468338\n",
      "Epoch 1908, Loss: 0.2731339707970619, Final Batch Loss: 0.09510663896799088\n",
      "Epoch 1909, Loss: 0.3611014783382416, Final Batch Loss: 0.1860567033290863\n",
      "Epoch 1910, Loss: 0.36607831716537476, Final Batch Loss: 0.17822223901748657\n",
      "Epoch 1911, Loss: 0.34708262979984283, Final Batch Loss: 0.17424938082695007\n",
      "Epoch 1912, Loss: 0.3249649107456207, Final Batch Loss: 0.15886828303337097\n",
      "Epoch 1913, Loss: 0.30008719861507416, Final Batch Loss: 0.14786700904369354\n",
      "Epoch 1914, Loss: 0.35467271506786346, Final Batch Loss: 0.17314022779464722\n",
      "Epoch 1915, Loss: 0.39409032464027405, Final Batch Loss: 0.1765451729297638\n",
      "Epoch 1916, Loss: 0.2864300310611725, Final Batch Loss: 0.13701137900352478\n",
      "Epoch 1917, Loss: 0.34297114610671997, Final Batch Loss: 0.17173174023628235\n",
      "Epoch 1918, Loss: 0.3268166035413742, Final Batch Loss: 0.12243609130382538\n",
      "Epoch 1919, Loss: 0.33498725295066833, Final Batch Loss: 0.18585491180419922\n",
      "Epoch 1920, Loss: 0.2796001732349396, Final Batch Loss: 0.1333671361207962\n",
      "Epoch 1921, Loss: 0.33873824775218964, Final Batch Loss: 0.16760514676570892\n",
      "Epoch 1922, Loss: 0.2782549113035202, Final Batch Loss: 0.13885065913200378\n",
      "Epoch 1923, Loss: 0.3794921189546585, Final Batch Loss: 0.14441372454166412\n",
      "Epoch 1924, Loss: 0.30653712153434753, Final Batch Loss: 0.14915111660957336\n",
      "Epoch 1925, Loss: 0.3553525507450104, Final Batch Loss: 0.19369260966777802\n",
      "Epoch 1926, Loss: 0.3462033122777939, Final Batch Loss: 0.16219797730445862\n",
      "Epoch 1927, Loss: 0.33119747042655945, Final Batch Loss: 0.1429337114095688\n",
      "Epoch 1928, Loss: 0.2655436173081398, Final Batch Loss: 0.17564330995082855\n",
      "Epoch 1929, Loss: 0.3410797566175461, Final Batch Loss: 0.19263459742069244\n",
      "Epoch 1930, Loss: 0.3374486714601517, Final Batch Loss: 0.19887863099575043\n",
      "Epoch 1931, Loss: 0.32514508068561554, Final Batch Loss: 0.16300803422927856\n",
      "Epoch 1932, Loss: 0.35470181703567505, Final Batch Loss: 0.17244625091552734\n",
      "Epoch 1933, Loss: 0.2501887008547783, Final Batch Loss: 0.1067500039935112\n",
      "Epoch 1934, Loss: 0.28109055757522583, Final Batch Loss: 0.15083599090576172\n",
      "Epoch 1935, Loss: 0.29252511262893677, Final Batch Loss: 0.14502890408039093\n",
      "Epoch 1936, Loss: 0.3829214423894882, Final Batch Loss: 0.25537654757499695\n",
      "Epoch 1937, Loss: 0.30839361250400543, Final Batch Loss: 0.1540978103876114\n",
      "Epoch 1938, Loss: 0.3447454869747162, Final Batch Loss: 0.17605045437812805\n",
      "Epoch 1939, Loss: 0.3377538323402405, Final Batch Loss: 0.14095231890678406\n",
      "Epoch 1940, Loss: 0.3838464319705963, Final Batch Loss: 0.21191217005252838\n",
      "Epoch 1941, Loss: 0.38836726546287537, Final Batch Loss: 0.17743423581123352\n",
      "Epoch 1942, Loss: 0.332040011882782, Final Batch Loss: 0.1343531310558319\n",
      "Epoch 1943, Loss: 0.374371275305748, Final Batch Loss: 0.24091312289237976\n",
      "Epoch 1944, Loss: 0.36672140657901764, Final Batch Loss: 0.21795931458473206\n",
      "Epoch 1945, Loss: 0.37967805564403534, Final Batch Loss: 0.15786246955394745\n",
      "Epoch 1946, Loss: 0.3649766445159912, Final Batch Loss: 0.20722225308418274\n",
      "Epoch 1947, Loss: 0.3327743113040924, Final Batch Loss: 0.14708417654037476\n",
      "Epoch 1948, Loss: 0.3126427233219147, Final Batch Loss: 0.15987175703048706\n",
      "Epoch 1949, Loss: 0.36473315954208374, Final Batch Loss: 0.16970223188400269\n",
      "Epoch 1950, Loss: 0.25737589597702026, Final Batch Loss: 0.1435582935810089\n",
      "Epoch 1951, Loss: 0.3269151449203491, Final Batch Loss: 0.1636246144771576\n",
      "Epoch 1952, Loss: 0.32374487817287445, Final Batch Loss: 0.1677580624818802\n",
      "Epoch 1953, Loss: 0.3043562173843384, Final Batch Loss: 0.14970092475414276\n",
      "Epoch 1954, Loss: 0.33749212324619293, Final Batch Loss: 0.13754312694072723\n",
      "Epoch 1955, Loss: 0.3693193346261978, Final Batch Loss: 0.20082491636276245\n",
      "Epoch 1956, Loss: 0.3715572655200958, Final Batch Loss: 0.15901730954647064\n",
      "Epoch 1957, Loss: 0.3444741517305374, Final Batch Loss: 0.16908685863018036\n",
      "Epoch 1958, Loss: 0.2691471725702286, Final Batch Loss: 0.13153429329395294\n",
      "Epoch 1959, Loss: 0.3875928670167923, Final Batch Loss: 0.23674286901950836\n",
      "Epoch 1960, Loss: 0.2805320769548416, Final Batch Loss: 0.1342015564441681\n",
      "Epoch 1961, Loss: 0.3147014379501343, Final Batch Loss: 0.13569633662700653\n",
      "Epoch 1962, Loss: 0.3482836186885834, Final Batch Loss: 0.16137942671775818\n",
      "Epoch 1963, Loss: 0.29930464923381805, Final Batch Loss: 0.15430791676044464\n",
      "Epoch 1964, Loss: 0.34564946591854095, Final Batch Loss: 0.22024966776371002\n",
      "Epoch 1965, Loss: 0.3327040374279022, Final Batch Loss: 0.15608708560466766\n",
      "Epoch 1966, Loss: 0.39430004358291626, Final Batch Loss: 0.22235868871212006\n",
      "Epoch 1967, Loss: 0.30938921868801117, Final Batch Loss: 0.16224758327007294\n",
      "Epoch 1968, Loss: 0.3452727198600769, Final Batch Loss: 0.1433209329843521\n",
      "Epoch 1969, Loss: 0.40466587245464325, Final Batch Loss: 0.21846111118793488\n",
      "Epoch 1970, Loss: 0.2804180830717087, Final Batch Loss: 0.14052285254001617\n",
      "Epoch 1971, Loss: 0.31861287355422974, Final Batch Loss: 0.18567852675914764\n",
      "Epoch 1972, Loss: 0.30602966248989105, Final Batch Loss: 0.1363278180360794\n",
      "Epoch 1973, Loss: 0.2790016233921051, Final Batch Loss: 0.13959357142448425\n",
      "Epoch 1974, Loss: 0.3204007148742676, Final Batch Loss: 0.18474292755126953\n",
      "Epoch 1975, Loss: 0.3418818414211273, Final Batch Loss: 0.1720905303955078\n",
      "Epoch 1976, Loss: 0.3222614526748657, Final Batch Loss: 0.16421599686145782\n",
      "Epoch 1977, Loss: 0.36285413801670074, Final Batch Loss: 0.16119122505187988\n",
      "Epoch 1978, Loss: 0.3682580143213272, Final Batch Loss: 0.16077423095703125\n",
      "Epoch 1979, Loss: 0.3777422159910202, Final Batch Loss: 0.1756673902273178\n",
      "Epoch 1980, Loss: 0.3591214567422867, Final Batch Loss: 0.19641612470149994\n",
      "Epoch 1981, Loss: 0.3851780742406845, Final Batch Loss: 0.15561603009700775\n",
      "Epoch 1982, Loss: 0.379448801279068, Final Batch Loss: 0.1872832179069519\n",
      "Epoch 1983, Loss: 0.2669716626405716, Final Batch Loss: 0.1309843510389328\n",
      "Epoch 1984, Loss: 0.3295762836933136, Final Batch Loss: 0.17157743871212006\n",
      "Epoch 1985, Loss: 0.30641578137874603, Final Batch Loss: 0.14241710305213928\n",
      "Epoch 1986, Loss: 0.4136989116668701, Final Batch Loss: 0.1781398355960846\n",
      "Epoch 1987, Loss: 0.3453710973262787, Final Batch Loss: 0.19512106478214264\n",
      "Epoch 1988, Loss: 0.4939539432525635, Final Batch Loss: 0.2307766079902649\n",
      "Epoch 1989, Loss: 0.39879025518894196, Final Batch Loss: 0.21997512876987457\n",
      "Epoch 1990, Loss: 0.27999356389045715, Final Batch Loss: 0.1783863604068756\n",
      "Epoch 1991, Loss: 0.2663406431674957, Final Batch Loss: 0.13598009943962097\n",
      "Epoch 1992, Loss: 0.3214825689792633, Final Batch Loss: 0.16271144151687622\n",
      "Epoch 1993, Loss: 0.29015445709228516, Final Batch Loss: 0.11658278107643127\n",
      "Epoch 1994, Loss: 0.35661134123802185, Final Batch Loss: 0.1932382583618164\n",
      "Epoch 1995, Loss: 0.3582374304533005, Final Batch Loss: 0.21503837406635284\n",
      "Epoch 1996, Loss: 0.29902347922325134, Final Batch Loss: 0.15755122900009155\n",
      "Epoch 1997, Loss: 0.32359422743320465, Final Batch Loss: 0.1541559398174286\n",
      "Epoch 1998, Loss: 0.3471987843513489, Final Batch Loss: 0.15523812174797058\n",
      "Epoch 1999, Loss: 0.3088184595108032, Final Batch Loss: 0.15959861874580383\n",
      "Epoch 2000, Loss: 0.3646051436662674, Final Batch Loss: 0.1725844442844391\n",
      "Epoch 2001, Loss: 0.3328893780708313, Final Batch Loss: 0.13452908396720886\n",
      "Epoch 2002, Loss: 0.32420068979263306, Final Batch Loss: 0.12466326355934143\n",
      "Epoch 2003, Loss: 0.3019157648086548, Final Batch Loss: 0.12114326655864716\n",
      "Epoch 2004, Loss: 0.31084416806697845, Final Batch Loss: 0.16520048677921295\n",
      "Epoch 2005, Loss: 0.2729751914739609, Final Batch Loss: 0.15752317011356354\n",
      "Epoch 2006, Loss: 0.3191527724266052, Final Batch Loss: 0.16714295744895935\n",
      "Epoch 2007, Loss: 0.30023249983787537, Final Batch Loss: 0.18202239274978638\n",
      "Epoch 2008, Loss: 0.3070030212402344, Final Batch Loss: 0.17807608842849731\n",
      "Epoch 2009, Loss: 0.39948730170726776, Final Batch Loss: 0.19927239418029785\n",
      "Epoch 2010, Loss: 0.30120091140270233, Final Batch Loss: 0.16736580431461334\n",
      "Epoch 2011, Loss: 0.34057798981666565, Final Batch Loss: 0.18569861352443695\n",
      "Epoch 2012, Loss: 0.3260418623685837, Final Batch Loss: 0.17276762425899506\n",
      "Epoch 2013, Loss: 0.31395454704761505, Final Batch Loss: 0.13078565895557404\n",
      "Epoch 2014, Loss: 0.29439520835876465, Final Batch Loss: 0.1453809291124344\n",
      "Epoch 2015, Loss: 0.29471056163311005, Final Batch Loss: 0.13385920226573944\n",
      "Epoch 2016, Loss: 0.39452242851257324, Final Batch Loss: 0.12796476483345032\n",
      "Epoch 2017, Loss: 0.2897714227437973, Final Batch Loss: 0.14344555139541626\n",
      "Epoch 2018, Loss: 0.3083278238773346, Final Batch Loss: 0.14178414642810822\n",
      "Epoch 2019, Loss: 0.33359621465206146, Final Batch Loss: 0.15777891874313354\n",
      "Epoch 2020, Loss: 0.31502020359039307, Final Batch Loss: 0.14618709683418274\n",
      "Epoch 2021, Loss: 0.31116022169589996, Final Batch Loss: 0.1833435744047165\n",
      "Epoch 2022, Loss: 0.37821051478385925, Final Batch Loss: 0.22273460030555725\n",
      "Epoch 2023, Loss: 0.31240610778331757, Final Batch Loss: 0.16921788454055786\n",
      "Epoch 2024, Loss: 0.3371180444955826, Final Batch Loss: 0.16250824928283691\n",
      "Epoch 2025, Loss: 0.3871936947107315, Final Batch Loss: 0.1741999238729477\n",
      "Epoch 2026, Loss: 0.30711764097213745, Final Batch Loss: 0.16648930311203003\n",
      "Epoch 2027, Loss: 0.3324842154979706, Final Batch Loss: 0.12986406683921814\n",
      "Epoch 2028, Loss: 0.33539019525051117, Final Batch Loss: 0.20442348718643188\n",
      "Epoch 2029, Loss: 0.3362840414047241, Final Batch Loss: 0.18391373753547668\n",
      "Epoch 2030, Loss: 0.3169201910495758, Final Batch Loss: 0.1494683474302292\n",
      "Epoch 2031, Loss: 0.36257387697696686, Final Batch Loss: 0.1827075183391571\n",
      "Epoch 2032, Loss: 0.30720028281211853, Final Batch Loss: 0.16059759259223938\n",
      "Epoch 2033, Loss: 0.30211856961250305, Final Batch Loss: 0.14421722292900085\n",
      "Epoch 2034, Loss: 0.2569386959075928, Final Batch Loss: 0.12407524883747101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2035, Loss: 0.32940588891506195, Final Batch Loss: 0.1461222767829895\n",
      "Epoch 2036, Loss: 0.29214757680892944, Final Batch Loss: 0.13368257880210876\n",
      "Epoch 2037, Loss: 0.27904242277145386, Final Batch Loss: 0.1405525654554367\n",
      "Epoch 2038, Loss: 0.3166487291455269, Final Batch Loss: 0.12333933264017105\n",
      "Epoch 2039, Loss: 0.2911868393421173, Final Batch Loss: 0.15143097937107086\n",
      "Epoch 2040, Loss: 0.3031676411628723, Final Batch Loss: 0.15703044831752777\n",
      "Epoch 2041, Loss: 0.38493792712688446, Final Batch Loss: 0.19092091917991638\n",
      "Epoch 2042, Loss: 0.27202189713716507, Final Batch Loss: 0.11167425662279129\n",
      "Epoch 2043, Loss: 0.3328659236431122, Final Batch Loss: 0.18502061069011688\n",
      "Epoch 2044, Loss: 0.28812800347805023, Final Batch Loss: 0.1395854502916336\n",
      "Epoch 2045, Loss: 0.341495156288147, Final Batch Loss: 0.20586960017681122\n",
      "Epoch 2046, Loss: 0.32584163546562195, Final Batch Loss: 0.1266152709722519\n",
      "Epoch 2047, Loss: 0.3238156735897064, Final Batch Loss: 0.1632780134677887\n",
      "Epoch 2048, Loss: 0.30946268141269684, Final Batch Loss: 0.17841926217079163\n",
      "Epoch 2049, Loss: 0.26046954840421677, Final Batch Loss: 0.14047646522521973\n",
      "Epoch 2050, Loss: 0.35058435797691345, Final Batch Loss: 0.14386777579784393\n",
      "Epoch 2051, Loss: 0.2958497107028961, Final Batch Loss: 0.13942556083202362\n",
      "Epoch 2052, Loss: 0.3352696895599365, Final Batch Loss: 0.19850066304206848\n",
      "Epoch 2053, Loss: 0.24064647406339645, Final Batch Loss: 0.09918837994337082\n",
      "Epoch 2054, Loss: 0.29401762038469315, Final Batch Loss: 0.11750944703817368\n",
      "Epoch 2055, Loss: 0.35303932428359985, Final Batch Loss: 0.14121781289577484\n",
      "Epoch 2056, Loss: 0.30285756289958954, Final Batch Loss: 0.15659061074256897\n",
      "Epoch 2057, Loss: 0.2575781121850014, Final Batch Loss: 0.14057201147079468\n",
      "Epoch 2058, Loss: 0.28294847905635834, Final Batch Loss: 0.1505659520626068\n",
      "Epoch 2059, Loss: 0.26249607652425766, Final Batch Loss: 0.13929519057273865\n",
      "Epoch 2060, Loss: 0.31545838713645935, Final Batch Loss: 0.17828407883644104\n",
      "Epoch 2061, Loss: 0.3332359492778778, Final Batch Loss: 0.20448127388954163\n",
      "Epoch 2062, Loss: 0.3027106076478958, Final Batch Loss: 0.1702490746974945\n",
      "Epoch 2063, Loss: 0.2442164048552513, Final Batch Loss: 0.12331296503543854\n",
      "Epoch 2064, Loss: 0.31467488408088684, Final Batch Loss: 0.16436167061328888\n",
      "Epoch 2065, Loss: 0.3948912173509598, Final Batch Loss: 0.1890166699886322\n",
      "Epoch 2066, Loss: 0.28770940005779266, Final Batch Loss: 0.1460718959569931\n",
      "Epoch 2067, Loss: 0.3309558182954788, Final Batch Loss: 0.1810714602470398\n",
      "Epoch 2068, Loss: 0.2669270634651184, Final Batch Loss: 0.12445025146007538\n",
      "Epoch 2069, Loss: 0.2778102532029152, Final Batch Loss: 0.1536170095205307\n",
      "Epoch 2070, Loss: 0.23384541273117065, Final Batch Loss: 0.11595190316438675\n",
      "Epoch 2071, Loss: 0.33323322236537933, Final Batch Loss: 0.1870097517967224\n",
      "Epoch 2072, Loss: 0.25838568061590195, Final Batch Loss: 0.13418075442314148\n",
      "Epoch 2073, Loss: 0.28105440735816956, Final Batch Loss: 0.14472174644470215\n",
      "Epoch 2074, Loss: 0.27728933840990067, Final Batch Loss: 0.16111420094966888\n",
      "Epoch 2075, Loss: 0.32866035401821136, Final Batch Loss: 0.14462056756019592\n",
      "Epoch 2076, Loss: 0.3680420517921448, Final Batch Loss: 0.20657363533973694\n",
      "Epoch 2077, Loss: 0.2438313066959381, Final Batch Loss: 0.11450989544391632\n",
      "Epoch 2078, Loss: 0.3074547052383423, Final Batch Loss: 0.1671113669872284\n",
      "Epoch 2079, Loss: 0.31133677065372467, Final Batch Loss: 0.16927501559257507\n",
      "Epoch 2080, Loss: 0.25385582447052, Final Batch Loss: 0.14288680255413055\n",
      "Epoch 2081, Loss: 0.28190945088863373, Final Batch Loss: 0.12943480908870697\n",
      "Epoch 2082, Loss: 0.26818428933620453, Final Batch Loss: 0.15093685686588287\n",
      "Epoch 2083, Loss: 0.334734782576561, Final Batch Loss: 0.19567398726940155\n",
      "Epoch 2084, Loss: 0.3147623836994171, Final Batch Loss: 0.14376872777938843\n",
      "Epoch 2085, Loss: 0.3649727404117584, Final Batch Loss: 0.16326889395713806\n",
      "Epoch 2086, Loss: 0.352019838988781, Final Batch Loss: 0.23358437418937683\n",
      "Epoch 2087, Loss: 0.28231698274612427, Final Batch Loss: 0.11962433159351349\n",
      "Epoch 2088, Loss: 0.296609565615654, Final Batch Loss: 0.1313963383436203\n",
      "Epoch 2089, Loss: 0.3466028571128845, Final Batch Loss: 0.20048239827156067\n",
      "Epoch 2090, Loss: 0.28689979761838913, Final Batch Loss: 0.16892977058887482\n",
      "Epoch 2091, Loss: 0.2936359941959381, Final Batch Loss: 0.13619333505630493\n",
      "Epoch 2092, Loss: 0.30473122000694275, Final Batch Loss: 0.17043502628803253\n",
      "Epoch 2093, Loss: 0.3350004255771637, Final Batch Loss: 0.1663086712360382\n",
      "Epoch 2094, Loss: 0.29440606385469437, Final Batch Loss: 0.1109900251030922\n",
      "Epoch 2095, Loss: 0.29401445388793945, Final Batch Loss: 0.1543310433626175\n",
      "Epoch 2096, Loss: 0.25781747698783875, Final Batch Loss: 0.10996294021606445\n",
      "Epoch 2097, Loss: 0.31543058156967163, Final Batch Loss: 0.15950153768062592\n",
      "Epoch 2098, Loss: 0.27543921023607254, Final Batch Loss: 0.12436201423406601\n",
      "Epoch 2099, Loss: 0.30874761939048767, Final Batch Loss: 0.14378604292869568\n",
      "Epoch 2100, Loss: 0.25194399058818817, Final Batch Loss: 0.1267797201871872\n",
      "Epoch 2101, Loss: 0.27805399894714355, Final Batch Loss: 0.13426336646080017\n",
      "Epoch 2102, Loss: 0.2670516222715378, Final Batch Loss: 0.13958458602428436\n",
      "Epoch 2103, Loss: 0.3154733330011368, Final Batch Loss: 0.2078295648097992\n",
      "Epoch 2104, Loss: 0.326655812561512, Final Batch Loss: 0.11566326767206192\n",
      "Epoch 2105, Loss: 0.30997389554977417, Final Batch Loss: 0.1682441532611847\n",
      "Epoch 2106, Loss: 0.3305738642811775, Final Batch Loss: 0.11759587377309799\n",
      "Epoch 2107, Loss: 0.24217888712882996, Final Batch Loss: 0.13028720021247864\n",
      "Epoch 2108, Loss: 0.2873828262090683, Final Batch Loss: 0.130867138504982\n",
      "Epoch 2109, Loss: 0.28161346167325974, Final Batch Loss: 0.168717160820961\n",
      "Epoch 2110, Loss: 0.26063183695077896, Final Batch Loss: 0.11624561995267868\n",
      "Epoch 2111, Loss: 0.36029186844825745, Final Batch Loss: 0.21244144439697266\n",
      "Epoch 2112, Loss: 0.3318062573671341, Final Batch Loss: 0.13760875165462494\n",
      "Epoch 2113, Loss: 0.30318573117256165, Final Batch Loss: 0.15160322189331055\n",
      "Epoch 2114, Loss: 0.2397938370704651, Final Batch Loss: 0.1355518400669098\n",
      "Epoch 2115, Loss: 0.26128970086574554, Final Batch Loss: 0.11110779643058777\n",
      "Epoch 2116, Loss: 0.2961122840642929, Final Batch Loss: 0.1660696417093277\n",
      "Epoch 2117, Loss: 0.2926340699195862, Final Batch Loss: 0.13625946640968323\n",
      "Epoch 2118, Loss: 0.2976880818605423, Final Batch Loss: 0.13121646642684937\n",
      "Epoch 2119, Loss: 0.38348692655563354, Final Batch Loss: 0.20624947547912598\n",
      "Epoch 2120, Loss: 0.272037997841835, Final Batch Loss: 0.14778372645378113\n",
      "Epoch 2121, Loss: 0.29543545842170715, Final Batch Loss: 0.13532502949237823\n",
      "Epoch 2122, Loss: 0.25035517662763596, Final Batch Loss: 0.1309797465801239\n",
      "Epoch 2123, Loss: 0.3071596473455429, Final Batch Loss: 0.1672104448080063\n",
      "Epoch 2124, Loss: 0.3030131831765175, Final Batch Loss: 0.12334873527288437\n",
      "Epoch 2125, Loss: 0.26526062190532684, Final Batch Loss: 0.12811070680618286\n",
      "Epoch 2126, Loss: 0.29129480570554733, Final Batch Loss: 0.1118432953953743\n",
      "Epoch 2127, Loss: 0.2658975124359131, Final Batch Loss: 0.12695933878421783\n",
      "Epoch 2128, Loss: 0.28349569439888, Final Batch Loss: 0.12542733550071716\n",
      "Epoch 2129, Loss: 0.330607607960701, Final Batch Loss: 0.12713761627674103\n",
      "Epoch 2130, Loss: 0.24766716361045837, Final Batch Loss: 0.13812501728534698\n",
      "Epoch 2131, Loss: 0.29392851889133453, Final Batch Loss: 0.14488019049167633\n",
      "Epoch 2132, Loss: 0.24529068171977997, Final Batch Loss: 0.11209690570831299\n",
      "Epoch 2133, Loss: 0.21004008501768112, Final Batch Loss: 0.12300732731819153\n",
      "Epoch 2134, Loss: 0.25328054279088974, Final Batch Loss: 0.14180172979831696\n",
      "Epoch 2135, Loss: 0.33233942091464996, Final Batch Loss: 0.1649746596813202\n",
      "Epoch 2136, Loss: 0.28684376180171967, Final Batch Loss: 0.13367502391338348\n",
      "Epoch 2137, Loss: 0.2292717695236206, Final Batch Loss: 0.12072113156318665\n",
      "Epoch 2138, Loss: 0.3590348958969116, Final Batch Loss: 0.2056490033864975\n",
      "Epoch 2139, Loss: 0.2230609580874443, Final Batch Loss: 0.09356074780225754\n",
      "Epoch 2140, Loss: 0.25003740936517715, Final Batch Loss: 0.11241000145673752\n",
      "Epoch 2141, Loss: 0.28729061782360077, Final Batch Loss: 0.1413080096244812\n",
      "Epoch 2142, Loss: 0.31475138664245605, Final Batch Loss: 0.17344170808792114\n",
      "Epoch 2143, Loss: 0.26429085433483124, Final Batch Loss: 0.13068744540214539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2144, Loss: 0.23094958066940308, Final Batch Loss: 0.12846161425113678\n",
      "Epoch 2145, Loss: 0.2660750448703766, Final Batch Loss: 0.14267338812351227\n",
      "Epoch 2146, Loss: 0.30750149488449097, Final Batch Loss: 0.1146429032087326\n",
      "Epoch 2147, Loss: 0.3131427764892578, Final Batch Loss: 0.1284155398607254\n",
      "Epoch 2148, Loss: 0.32328303158283234, Final Batch Loss: 0.1758733093738556\n",
      "Epoch 2149, Loss: 0.2813585102558136, Final Batch Loss: 0.10991433262825012\n",
      "Epoch 2150, Loss: 0.2870517075061798, Final Batch Loss: 0.154562309384346\n",
      "Epoch 2151, Loss: 0.26478731632232666, Final Batch Loss: 0.1429230123758316\n",
      "Epoch 2152, Loss: 0.2755330204963684, Final Batch Loss: 0.12129902839660645\n",
      "Epoch 2153, Loss: 0.2628820687532425, Final Batch Loss: 0.11811695992946625\n",
      "Epoch 2154, Loss: 0.3028791844844818, Final Batch Loss: 0.1427917182445526\n",
      "Epoch 2155, Loss: 0.29253247380256653, Final Batch Loss: 0.13798706233501434\n",
      "Epoch 2156, Loss: 0.25163523107767105, Final Batch Loss: 0.1322975754737854\n",
      "Epoch 2157, Loss: 0.2680695056915283, Final Batch Loss: 0.1318524032831192\n",
      "Epoch 2158, Loss: 0.2531537786126137, Final Batch Loss: 0.12321773916482925\n",
      "Epoch 2159, Loss: 0.3419327437877655, Final Batch Loss: 0.17320461571216583\n",
      "Epoch 2160, Loss: 0.3109252154827118, Final Batch Loss: 0.15632079541683197\n",
      "Epoch 2161, Loss: 0.32123975455760956, Final Batch Loss: 0.19434846937656403\n",
      "Epoch 2162, Loss: 0.30532611906528473, Final Batch Loss: 0.15734747052192688\n",
      "Epoch 2163, Loss: 0.2710978016257286, Final Batch Loss: 0.12261126190423965\n",
      "Epoch 2164, Loss: 0.23895350843667984, Final Batch Loss: 0.11727730929851532\n",
      "Epoch 2165, Loss: 0.25716927647590637, Final Batch Loss: 0.12991094589233398\n",
      "Epoch 2166, Loss: 0.3152772784233093, Final Batch Loss: 0.1853129267692566\n",
      "Epoch 2167, Loss: 0.2477053552865982, Final Batch Loss: 0.1256123185157776\n",
      "Epoch 2168, Loss: 0.19916952401399612, Final Batch Loss: 0.11598037928342819\n",
      "Epoch 2169, Loss: 0.23189116269350052, Final Batch Loss: 0.1193191260099411\n",
      "Epoch 2170, Loss: 0.2649524509906769, Final Batch Loss: 0.12636789679527283\n",
      "Epoch 2171, Loss: 0.2621856853365898, Final Batch Loss: 0.14394013583660126\n",
      "Epoch 2172, Loss: 0.37291745841503143, Final Batch Loss: 0.17981664836406708\n",
      "Epoch 2173, Loss: 0.27909715473651886, Final Batch Loss: 0.15448352694511414\n",
      "Epoch 2174, Loss: 0.3885468989610672, Final Batch Loss: 0.20156146585941315\n",
      "Epoch 2175, Loss: 0.28237585723400116, Final Batch Loss: 0.13356278836727142\n",
      "Epoch 2176, Loss: 0.30847203731536865, Final Batch Loss: 0.1470671147108078\n",
      "Epoch 2177, Loss: 0.2878696918487549, Final Batch Loss: 0.10075066983699799\n",
      "Epoch 2178, Loss: 0.3197638839483261, Final Batch Loss: 0.1924007683992386\n",
      "Epoch 2179, Loss: 0.24031712859869003, Final Batch Loss: 0.12711051106452942\n",
      "Epoch 2180, Loss: 0.2959866225719452, Final Batch Loss: 0.16080346703529358\n",
      "Epoch 2181, Loss: 0.34523317217826843, Final Batch Loss: 0.21378682553768158\n",
      "Epoch 2182, Loss: 0.2705691009759903, Final Batch Loss: 0.13492067158222198\n",
      "Epoch 2183, Loss: 0.3154761344194412, Final Batch Loss: 0.16965745389461517\n",
      "Epoch 2184, Loss: 0.2965198755264282, Final Batch Loss: 0.1501506268978119\n",
      "Epoch 2185, Loss: 0.3277427777647972, Final Batch Loss: 0.22074808180332184\n",
      "Epoch 2186, Loss: 0.2857924848794937, Final Batch Loss: 0.12915144860744476\n",
      "Epoch 2187, Loss: 0.3086967021226883, Final Batch Loss: 0.1818496733903885\n",
      "Epoch 2188, Loss: 0.47499358654022217, Final Batch Loss: 0.28777894377708435\n",
      "Epoch 2189, Loss: 0.2889525294303894, Final Batch Loss: 0.13537216186523438\n",
      "Epoch 2190, Loss: 0.2961632311344147, Final Batch Loss: 0.1560821682214737\n",
      "Epoch 2191, Loss: 0.2346474900841713, Final Batch Loss: 0.09823492914438248\n",
      "Epoch 2192, Loss: 0.3093998581171036, Final Batch Loss: 0.17662014067173004\n",
      "Epoch 2193, Loss: 0.2740568295121193, Final Batch Loss: 0.1544916033744812\n",
      "Epoch 2194, Loss: 0.27621689438819885, Final Batch Loss: 0.13671833276748657\n",
      "Epoch 2195, Loss: 0.2510077953338623, Final Batch Loss: 0.13803066313266754\n",
      "Epoch 2196, Loss: 0.31036753207445145, Final Batch Loss: 0.11726375669240952\n",
      "Epoch 2197, Loss: 0.24490106105804443, Final Batch Loss: 0.11321504414081573\n",
      "Epoch 2198, Loss: 0.2558998614549637, Final Batch Loss: 0.12150290608406067\n",
      "Epoch 2199, Loss: 0.3096810281276703, Final Batch Loss: 0.15548208355903625\n",
      "Epoch 2200, Loss: 0.33282679319381714, Final Batch Loss: 0.15564370155334473\n",
      "Epoch 2201, Loss: 0.273627370595932, Final Batch Loss: 0.10016109049320221\n",
      "Epoch 2202, Loss: 0.23776410520076752, Final Batch Loss: 0.11038579046726227\n",
      "Epoch 2203, Loss: 0.26512643694877625, Final Batch Loss: 0.1297188252210617\n",
      "Epoch 2204, Loss: 0.3049257844686508, Final Batch Loss: 0.1369563490152359\n",
      "Epoch 2205, Loss: 0.3718627244234085, Final Batch Loss: 0.18851734697818756\n",
      "Epoch 2206, Loss: 0.29644496738910675, Final Batch Loss: 0.15368963778018951\n",
      "Epoch 2207, Loss: 0.28843364119529724, Final Batch Loss: 0.16802997887134552\n",
      "Epoch 2208, Loss: 0.2816539853811264, Final Batch Loss: 0.15600165724754333\n",
      "Epoch 2209, Loss: 0.2540088966488838, Final Batch Loss: 0.12421711534261703\n",
      "Epoch 2210, Loss: 0.32966773211956024, Final Batch Loss: 0.16909724473953247\n",
      "Epoch 2211, Loss: 0.3022080957889557, Final Batch Loss: 0.15146300196647644\n",
      "Epoch 2212, Loss: 0.23636994510889053, Final Batch Loss: 0.13019776344299316\n",
      "Epoch 2213, Loss: 0.2684910297393799, Final Batch Loss: 0.13050426542758942\n",
      "Epoch 2214, Loss: 0.30192722380161285, Final Batch Loss: 0.15345658361911774\n",
      "Epoch 2215, Loss: 0.2838463634252548, Final Batch Loss: 0.1311686635017395\n",
      "Epoch 2216, Loss: 0.2475532740354538, Final Batch Loss: 0.12443078309297562\n",
      "Epoch 2217, Loss: 0.24984461814165115, Final Batch Loss: 0.11219959706068039\n",
      "Epoch 2218, Loss: 0.34136468172073364, Final Batch Loss: 0.2098451405763626\n",
      "Epoch 2219, Loss: 0.2920555844902992, Final Batch Loss: 0.16758422553539276\n",
      "Epoch 2220, Loss: 0.2756199836730957, Final Batch Loss: 0.1904587596654892\n",
      "Epoch 2221, Loss: 0.30950649082660675, Final Batch Loss: 0.16115859150886536\n",
      "Epoch 2222, Loss: 0.2006889134645462, Final Batch Loss: 0.09022247791290283\n",
      "Epoch 2223, Loss: 0.2780385762453079, Final Batch Loss: 0.10768811404705048\n",
      "Epoch 2224, Loss: 0.2604348137974739, Final Batch Loss: 0.1237393394112587\n",
      "Epoch 2225, Loss: 0.28018899261951447, Final Batch Loss: 0.14856873452663422\n",
      "Epoch 2226, Loss: 0.26882776618003845, Final Batch Loss: 0.09552764892578125\n",
      "Epoch 2227, Loss: 0.2488509565591812, Final Batch Loss: 0.14736494421958923\n",
      "Epoch 2228, Loss: 0.2870582044124603, Final Batch Loss: 0.1069854199886322\n",
      "Epoch 2229, Loss: 0.29301057755947113, Final Batch Loss: 0.1572362631559372\n",
      "Epoch 2230, Loss: 0.25968214869499207, Final Batch Loss: 0.12273263931274414\n",
      "Epoch 2231, Loss: 0.30083365738391876, Final Batch Loss: 0.1642947643995285\n",
      "Epoch 2232, Loss: 0.30643871426582336, Final Batch Loss: 0.1366376280784607\n",
      "Epoch 2233, Loss: 0.3355782479047775, Final Batch Loss: 0.14727343618869781\n",
      "Epoch 2234, Loss: 0.31134895980358124, Final Batch Loss: 0.15002237260341644\n",
      "Epoch 2235, Loss: 0.28778403997421265, Final Batch Loss: 0.14495687186717987\n",
      "Epoch 2236, Loss: 0.2502177804708481, Final Batch Loss: 0.14052286744117737\n",
      "Epoch 2237, Loss: 0.26047681272029877, Final Batch Loss: 0.10950684547424316\n",
      "Epoch 2238, Loss: 0.26810523122549057, Final Batch Loss: 0.12378773838281631\n",
      "Epoch 2239, Loss: 0.2965598404407501, Final Batch Loss: 0.12297394871711731\n",
      "Epoch 2240, Loss: 0.24635840207338333, Final Batch Loss: 0.10590056329965591\n",
      "Epoch 2241, Loss: 0.29168252646923065, Final Batch Loss: 0.15209579467773438\n",
      "Epoch 2242, Loss: 0.28976430743932724, Final Batch Loss: 0.17449934780597687\n",
      "Epoch 2243, Loss: 0.3537108302116394, Final Batch Loss: 0.18013978004455566\n",
      "Epoch 2244, Loss: 0.2578946128487587, Final Batch Loss: 0.13388466835021973\n",
      "Epoch 2245, Loss: 0.23198892921209335, Final Batch Loss: 0.1008106991648674\n",
      "Epoch 2246, Loss: 0.23142920434474945, Final Batch Loss: 0.10942961275577545\n",
      "Epoch 2247, Loss: 0.2803405225276947, Final Batch Loss: 0.13236472010612488\n",
      "Epoch 2248, Loss: 0.32097844779491425, Final Batch Loss: 0.17329095304012299\n",
      "Epoch 2249, Loss: 0.22184379398822784, Final Batch Loss: 0.1334344893693924\n",
      "Epoch 2250, Loss: 0.2656152993440628, Final Batch Loss: 0.14502301812171936\n",
      "Epoch 2251, Loss: 0.28356675803661346, Final Batch Loss: 0.1561998575925827\n",
      "Epoch 2252, Loss: 0.315434992313385, Final Batch Loss: 0.17467905580997467\n",
      "Epoch 2253, Loss: 0.2934664934873581, Final Batch Loss: 0.15649940073490143\n",
      "Epoch 2254, Loss: 0.25944992154836655, Final Batch Loss: 0.14772917330265045\n",
      "Epoch 2255, Loss: 0.3598901480436325, Final Batch Loss: 0.20075605809688568\n",
      "Epoch 2256, Loss: 0.2547122836112976, Final Batch Loss: 0.12049663066864014\n",
      "Epoch 2257, Loss: 0.29225051403045654, Final Batch Loss: 0.10369689762592316\n",
      "Epoch 2258, Loss: 0.2982502728700638, Final Batch Loss: 0.14827169477939606\n",
      "Epoch 2259, Loss: 0.2627960741519928, Final Batch Loss: 0.12464846670627594\n",
      "Epoch 2260, Loss: 0.2852942943572998, Final Batch Loss: 0.13648606836795807\n",
      "Epoch 2261, Loss: 0.27203187346458435, Final Batch Loss: 0.14011023938655853\n",
      "Epoch 2262, Loss: 0.2457442581653595, Final Batch Loss: 0.11407160758972168\n",
      "Epoch 2263, Loss: 0.32438862323760986, Final Batch Loss: 0.13532264530658722\n",
      "Epoch 2264, Loss: 0.30946800112724304, Final Batch Loss: 0.1423863023519516\n",
      "Epoch 2265, Loss: 0.28350329399108887, Final Batch Loss: 0.11133064329624176\n",
      "Epoch 2266, Loss: 0.2330077439546585, Final Batch Loss: 0.15089640021324158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2267, Loss: 0.21555223315954208, Final Batch Loss: 0.13962189853191376\n",
      "Epoch 2268, Loss: 0.3312487155199051, Final Batch Loss: 0.18515683710575104\n",
      "Epoch 2269, Loss: 0.23826853930950165, Final Batch Loss: 0.12230786681175232\n",
      "Epoch 2270, Loss: 0.26858115941286087, Final Batch Loss: 0.15182872116565704\n",
      "Epoch 2271, Loss: 0.35684897005558014, Final Batch Loss: 0.20540912449359894\n",
      "Epoch 2272, Loss: 0.3152964264154434, Final Batch Loss: 0.16827130317687988\n",
      "Epoch 2273, Loss: 0.21657827496528625, Final Batch Loss: 0.0991891399025917\n",
      "Epoch 2274, Loss: 0.2539447247982025, Final Batch Loss: 0.1518876999616623\n",
      "Epoch 2275, Loss: 0.2562779188156128, Final Batch Loss: 0.1336875557899475\n",
      "Epoch 2276, Loss: 0.27818506956100464, Final Batch Loss: 0.1874929517507553\n",
      "Epoch 2277, Loss: 0.2529885917901993, Final Batch Loss: 0.14683739840984344\n",
      "Epoch 2278, Loss: 0.3183612823486328, Final Batch Loss: 0.16697333753108978\n",
      "Epoch 2279, Loss: 0.24534575641155243, Final Batch Loss: 0.1302552968263626\n",
      "Epoch 2280, Loss: 0.2797267585992813, Final Batch Loss: 0.13098298013210297\n",
      "Epoch 2281, Loss: 0.2530512362718582, Final Batch Loss: 0.11310975253582001\n",
      "Epoch 2282, Loss: 0.2734713852405548, Final Batch Loss: 0.14467395842075348\n",
      "Epoch 2283, Loss: 0.313067227602005, Final Batch Loss: 0.16814157366752625\n",
      "Epoch 2284, Loss: 0.2957557663321495, Final Batch Loss: 0.11778544634580612\n",
      "Epoch 2285, Loss: 0.2677765190601349, Final Batch Loss: 0.13437014818191528\n",
      "Epoch 2286, Loss: 0.23093122988939285, Final Batch Loss: 0.08915045112371445\n",
      "Epoch 2287, Loss: 0.24360664933919907, Final Batch Loss: 0.11306463927030563\n",
      "Epoch 2288, Loss: 0.3182520568370819, Final Batch Loss: 0.15230178833007812\n",
      "Epoch 2289, Loss: 0.23704157024621964, Final Batch Loss: 0.1373416632413864\n",
      "Epoch 2290, Loss: 0.17789389938116074, Final Batch Loss: 0.08605816960334778\n",
      "Epoch 2291, Loss: 0.2508537992835045, Final Batch Loss: 0.12183091789484024\n",
      "Epoch 2292, Loss: 0.29685498774051666, Final Batch Loss: 0.11704318225383759\n",
      "Epoch 2293, Loss: 0.25167857855558395, Final Batch Loss: 0.16751985251903534\n",
      "Epoch 2294, Loss: 0.2136540561914444, Final Batch Loss: 0.11259586364030838\n",
      "Epoch 2295, Loss: 0.22871875762939453, Final Batch Loss: 0.12330470979213715\n",
      "Epoch 2296, Loss: 0.25257834792137146, Final Batch Loss: 0.13290123641490936\n",
      "Epoch 2297, Loss: 0.22417566925287247, Final Batch Loss: 0.08146730810403824\n",
      "Epoch 2298, Loss: 0.22716764360666275, Final Batch Loss: 0.0895320251584053\n",
      "Epoch 2299, Loss: 0.2626548260450363, Final Batch Loss: 0.07865293323993683\n",
      "Epoch 2300, Loss: 0.2854219526052475, Final Batch Loss: 0.17082779109477997\n",
      "Epoch 2301, Loss: 0.260757640004158, Final Batch Loss: 0.11533936858177185\n",
      "Epoch 2302, Loss: 0.3140069842338562, Final Batch Loss: 0.15808454155921936\n",
      "Epoch 2303, Loss: 0.206829734146595, Final Batch Loss: 0.10136111080646515\n",
      "Epoch 2304, Loss: 0.21875397861003876, Final Batch Loss: 0.1092999204993248\n",
      "Epoch 2305, Loss: 0.24586323648691177, Final Batch Loss: 0.13689391314983368\n",
      "Epoch 2306, Loss: 0.2033703401684761, Final Batch Loss: 0.1127057895064354\n",
      "Epoch 2307, Loss: 0.31865109503269196, Final Batch Loss: 0.1503782570362091\n",
      "Epoch 2308, Loss: 0.29250118136405945, Final Batch Loss: 0.13784077763557434\n",
      "Epoch 2309, Loss: 0.24092038720846176, Final Batch Loss: 0.0939294770359993\n",
      "Epoch 2310, Loss: 0.29009242355823517, Final Batch Loss: 0.14451326429843903\n",
      "Epoch 2311, Loss: 0.24193263798952103, Final Batch Loss: 0.1350868195295334\n",
      "Epoch 2312, Loss: 0.27306629717350006, Final Batch Loss: 0.11692310869693756\n",
      "Epoch 2313, Loss: 0.2347230762243271, Final Batch Loss: 0.12187519669532776\n",
      "Epoch 2314, Loss: 0.22647803276777267, Final Batch Loss: 0.11554061621427536\n",
      "Epoch 2315, Loss: 0.29158691316843033, Final Batch Loss: 0.11407334357500076\n",
      "Epoch 2316, Loss: 0.18342123925685883, Final Batch Loss: 0.07505675405263901\n",
      "Epoch 2317, Loss: 0.2711523175239563, Final Batch Loss: 0.14625835418701172\n",
      "Epoch 2318, Loss: 0.23370309174060822, Final Batch Loss: 0.12713520228862762\n",
      "Epoch 2319, Loss: 0.2943584620952606, Final Batch Loss: 0.13290442526340485\n",
      "Epoch 2320, Loss: 0.2365434467792511, Final Batch Loss: 0.1087920218706131\n",
      "Epoch 2321, Loss: 0.27307358384132385, Final Batch Loss: 0.08815133571624756\n",
      "Epoch 2322, Loss: 0.23537452518939972, Final Batch Loss: 0.11692582070827484\n",
      "Epoch 2323, Loss: 0.1787361279129982, Final Batch Loss: 0.08299032598733902\n",
      "Epoch 2324, Loss: 0.29580242186784744, Final Batch Loss: 0.1115957573056221\n",
      "Epoch 2325, Loss: 0.21008490025997162, Final Batch Loss: 0.07236960530281067\n",
      "Epoch 2326, Loss: 0.22284943610429764, Final Batch Loss: 0.10660544782876968\n",
      "Epoch 2327, Loss: 0.28849591314792633, Final Batch Loss: 0.14923740923404694\n",
      "Epoch 2328, Loss: 0.27082589268684387, Final Batch Loss: 0.1495702862739563\n",
      "Epoch 2329, Loss: 0.19487621635198593, Final Batch Loss: 0.083643838763237\n",
      "Epoch 2330, Loss: 0.23872561752796173, Final Batch Loss: 0.11956924945116043\n",
      "Epoch 2331, Loss: 0.2058476358652115, Final Batch Loss: 0.10063327848911285\n",
      "Epoch 2332, Loss: 0.25835275650024414, Final Batch Loss: 0.12185728549957275\n",
      "Epoch 2333, Loss: 0.2056879997253418, Final Batch Loss: 0.12297777086496353\n",
      "Epoch 2334, Loss: 0.24040980637073517, Final Batch Loss: 0.11983834952116013\n",
      "Epoch 2335, Loss: 0.30469126999378204, Final Batch Loss: 0.14369937777519226\n",
      "Epoch 2336, Loss: 0.23026234656572342, Final Batch Loss: 0.1282239705324173\n",
      "Epoch 2337, Loss: 0.2758217304944992, Final Batch Loss: 0.16770607233047485\n",
      "Epoch 2338, Loss: 0.2584990784525871, Final Batch Loss: 0.15265998244285583\n",
      "Epoch 2339, Loss: 0.29618852585554123, Final Batch Loss: 0.2014400213956833\n",
      "Epoch 2340, Loss: 0.2422717660665512, Final Batch Loss: 0.09401290118694305\n",
      "Epoch 2341, Loss: 0.2640347480773926, Final Batch Loss: 0.14780238270759583\n",
      "Epoch 2342, Loss: 0.25632087141275406, Final Batch Loss: 0.11813332885503769\n",
      "Epoch 2343, Loss: 0.2993885576725006, Final Batch Loss: 0.1691499501466751\n",
      "Epoch 2344, Loss: 0.26098398119211197, Final Batch Loss: 0.15347035229206085\n",
      "Epoch 2345, Loss: 0.1983228698372841, Final Batch Loss: 0.09574088454246521\n",
      "Epoch 2346, Loss: 0.2678460106253624, Final Batch Loss: 0.09398601204156876\n",
      "Epoch 2347, Loss: 0.26126015186309814, Final Batch Loss: 0.1389167457818985\n",
      "Epoch 2348, Loss: 0.24997980892658234, Final Batch Loss: 0.11176134645938873\n",
      "Epoch 2349, Loss: 0.28914138674736023, Final Batch Loss: 0.16086429357528687\n",
      "Epoch 2350, Loss: 0.2882489562034607, Final Batch Loss: 0.15120548009872437\n",
      "Epoch 2351, Loss: 0.2502090483903885, Final Batch Loss: 0.13680380582809448\n",
      "Epoch 2352, Loss: 0.21872255951166153, Final Batch Loss: 0.10832934826612473\n",
      "Epoch 2353, Loss: 0.3447280079126358, Final Batch Loss: 0.19284969568252563\n",
      "Epoch 2354, Loss: 0.310873843729496, Final Batch Loss: 0.12391401082277298\n",
      "Epoch 2355, Loss: 0.19153931736946106, Final Batch Loss: 0.09077610075473785\n",
      "Epoch 2356, Loss: 0.35724955797195435, Final Batch Loss: 0.15311700105667114\n",
      "Epoch 2357, Loss: 0.21843869239091873, Final Batch Loss: 0.12216154485940933\n",
      "Epoch 2358, Loss: 0.23805704712867737, Final Batch Loss: 0.14027413725852966\n",
      "Epoch 2359, Loss: 0.22608034312725067, Final Batch Loss: 0.14177171885967255\n",
      "Epoch 2360, Loss: 0.2392488270998001, Final Batch Loss: 0.13662657141685486\n",
      "Epoch 2361, Loss: 0.26770609617233276, Final Batch Loss: 0.15572068095207214\n",
      "Epoch 2362, Loss: 0.25742243230342865, Final Batch Loss: 0.14597755670547485\n",
      "Epoch 2363, Loss: 0.2696203291416168, Final Batch Loss: 0.14095349609851837\n",
      "Epoch 2364, Loss: 0.22960150986909866, Final Batch Loss: 0.08920925110578537\n",
      "Epoch 2365, Loss: 0.2966947704553604, Final Batch Loss: 0.16661188006401062\n",
      "Epoch 2366, Loss: 0.2958831638097763, Final Batch Loss: 0.14860877394676208\n",
      "Epoch 2367, Loss: 0.253366157412529, Final Batch Loss: 0.1276622861623764\n",
      "Epoch 2368, Loss: 0.23754878342151642, Final Batch Loss: 0.10392603278160095\n",
      "Epoch 2369, Loss: 0.2704446390271187, Final Batch Loss: 0.15470966696739197\n",
      "Epoch 2370, Loss: 0.26314350962638855, Final Batch Loss: 0.15554313361644745\n",
      "Epoch 2371, Loss: 0.2701267749071121, Final Batch Loss: 0.11816278100013733\n",
      "Epoch 2372, Loss: 0.23009935021400452, Final Batch Loss: 0.12802845239639282\n",
      "Epoch 2373, Loss: 0.30909326672554016, Final Batch Loss: 0.16394349932670593\n",
      "Epoch 2374, Loss: 0.20789466053247452, Final Batch Loss: 0.08021494001150131\n",
      "Epoch 2375, Loss: 0.27568428218364716, Final Batch Loss: 0.12643805146217346\n",
      "Epoch 2376, Loss: 0.20434406399726868, Final Batch Loss: 0.13256748020648956\n",
      "Epoch 2377, Loss: 0.24698374420404434, Final Batch Loss: 0.12467821687459946\n",
      "Epoch 2378, Loss: 0.19199275225400925, Final Batch Loss: 0.1127646192908287\n",
      "Epoch 2379, Loss: 0.3125006929039955, Final Batch Loss: 0.19394856691360474\n",
      "Epoch 2380, Loss: 0.2338930442929268, Final Batch Loss: 0.1277157962322235\n",
      "Epoch 2381, Loss: 0.3508917763829231, Final Batch Loss: 0.24164381623268127\n",
      "Epoch 2382, Loss: 0.2626129984855652, Final Batch Loss: 0.10168950259685516\n",
      "Epoch 2383, Loss: 0.268786683678627, Final Batch Loss: 0.08597417175769806\n",
      "Epoch 2384, Loss: 0.23569555580615997, Final Batch Loss: 0.14443056285381317\n",
      "Epoch 2385, Loss: 0.29405204951763153, Final Batch Loss: 0.15829114615917206\n",
      "Epoch 2386, Loss: 0.20397978276014328, Final Batch Loss: 0.10903286188840866\n",
      "Epoch 2387, Loss: 0.2600085958838463, Final Batch Loss: 0.15658719837665558\n",
      "Epoch 2388, Loss: 0.2516205832362175, Final Batch Loss: 0.11748973280191422\n",
      "Epoch 2389, Loss: 0.22138479351997375, Final Batch Loss: 0.11187051981687546\n",
      "Epoch 2390, Loss: 0.2232062816619873, Final Batch Loss: 0.10701176524162292\n",
      "Epoch 2391, Loss: 0.24961767345666885, Final Batch Loss: 0.10319512337446213\n",
      "Epoch 2392, Loss: 0.20552000403404236, Final Batch Loss: 0.0933285728096962\n",
      "Epoch 2393, Loss: 0.24431947618722916, Final Batch Loss: 0.12118826061487198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2394, Loss: 0.26499422639608383, Final Batch Loss: 0.1428873986005783\n",
      "Epoch 2395, Loss: 0.23399925231933594, Final Batch Loss: 0.12152907997369766\n",
      "Epoch 2396, Loss: 0.23697835952043533, Final Batch Loss: 0.10659774392843246\n",
      "Epoch 2397, Loss: 0.19261553138494492, Final Batch Loss: 0.11596417427062988\n",
      "Epoch 2398, Loss: 0.19257986545562744, Final Batch Loss: 0.09566204249858856\n",
      "Epoch 2399, Loss: 0.21586494892835617, Final Batch Loss: 0.07675530761480331\n",
      "Epoch 2400, Loss: 0.2949175536632538, Final Batch Loss: 0.13813556730747223\n",
      "Epoch 2401, Loss: 0.24097568541765213, Final Batch Loss: 0.10601475089788437\n",
      "Epoch 2402, Loss: 0.22527377307415009, Final Batch Loss: 0.08057549595832825\n",
      "Epoch 2403, Loss: 0.24761315435171127, Final Batch Loss: 0.14801715314388275\n",
      "Epoch 2404, Loss: 0.32960928976535797, Final Batch Loss: 0.12945538759231567\n",
      "Epoch 2405, Loss: 0.25385407358407974, Final Batch Loss: 0.13068094849586487\n",
      "Epoch 2406, Loss: 0.23027841746807098, Final Batch Loss: 0.11482369899749756\n",
      "Epoch 2407, Loss: 0.25056997686624527, Final Batch Loss: 0.15109752118587494\n",
      "Epoch 2408, Loss: 0.26215073466300964, Final Batch Loss: 0.1210416704416275\n",
      "Epoch 2409, Loss: 0.2850135862827301, Final Batch Loss: 0.1584092527627945\n",
      "Epoch 2410, Loss: 0.238775797188282, Final Batch Loss: 0.1468234360218048\n",
      "Epoch 2411, Loss: 0.21775686740875244, Final Batch Loss: 0.11338535696268082\n",
      "Epoch 2412, Loss: 0.3011224716901779, Final Batch Loss: 0.13694290816783905\n",
      "Epoch 2413, Loss: 0.21442509442567825, Final Batch Loss: 0.09309519827365875\n",
      "Epoch 2414, Loss: 0.2526632249355316, Final Batch Loss: 0.11724627017974854\n",
      "Epoch 2415, Loss: 0.23666325956583023, Final Batch Loss: 0.13444870710372925\n",
      "Epoch 2416, Loss: 0.25934794545173645, Final Batch Loss: 0.1324426829814911\n",
      "Epoch 2417, Loss: 0.1967419981956482, Final Batch Loss: 0.10216457396745682\n",
      "Epoch 2418, Loss: 0.2596796825528145, Final Batch Loss: 0.18797720968723297\n",
      "Epoch 2419, Loss: 0.22551243752241135, Final Batch Loss: 0.12488831579685211\n",
      "Epoch 2420, Loss: 0.2225966677069664, Final Batch Loss: 0.11701501905918121\n",
      "Epoch 2421, Loss: 0.284077987074852, Final Batch Loss: 0.1277388334274292\n",
      "Epoch 2422, Loss: 0.2062869817018509, Final Batch Loss: 0.11487666517496109\n",
      "Epoch 2423, Loss: 0.21148083359003067, Final Batch Loss: 0.09103130549192429\n",
      "Epoch 2424, Loss: 0.24949681758880615, Final Batch Loss: 0.10705940425395966\n",
      "Epoch 2425, Loss: 0.3214503824710846, Final Batch Loss: 0.16547729074954987\n",
      "Epoch 2426, Loss: 0.1947942078113556, Final Batch Loss: 0.08123315125703812\n",
      "Epoch 2427, Loss: 0.24399032443761826, Final Batch Loss: 0.13809330761432648\n",
      "Epoch 2428, Loss: 0.27532440423965454, Final Batch Loss: 0.09591303765773773\n",
      "Epoch 2429, Loss: 0.16022440791130066, Final Batch Loss: 0.07348329573869705\n",
      "Epoch 2430, Loss: 0.23267745971679688, Final Batch Loss: 0.10817161202430725\n",
      "Epoch 2431, Loss: 0.33737465739250183, Final Batch Loss: 0.15638849139213562\n",
      "Epoch 2432, Loss: 0.24149248003959656, Final Batch Loss: 0.0916132777929306\n",
      "Epoch 2433, Loss: 0.22305728495121002, Final Batch Loss: 0.1134583130478859\n",
      "Epoch 2434, Loss: 0.16830714792013168, Final Batch Loss: 0.0913403332233429\n",
      "Epoch 2435, Loss: 0.2615208923816681, Final Batch Loss: 0.13737092912197113\n",
      "Epoch 2436, Loss: 0.2165561318397522, Final Batch Loss: 0.10270360112190247\n",
      "Epoch 2437, Loss: 0.18359367549419403, Final Batch Loss: 0.10157280415296555\n",
      "Epoch 2438, Loss: 0.24525850266218185, Final Batch Loss: 0.10920172184705734\n",
      "Epoch 2439, Loss: 0.187113918364048, Final Batch Loss: 0.11630508303642273\n",
      "Epoch 2440, Loss: 0.19640565663576126, Final Batch Loss: 0.11427818983793259\n",
      "Epoch 2441, Loss: 0.27214935421943665, Final Batch Loss: 0.1443447321653366\n",
      "Epoch 2442, Loss: 0.2173805758357048, Final Batch Loss: 0.09243938326835632\n",
      "Epoch 2443, Loss: 0.2620132118463516, Final Batch Loss: 0.13389568030834198\n",
      "Epoch 2444, Loss: 0.27040793001651764, Final Batch Loss: 0.1311686784029007\n",
      "Epoch 2445, Loss: 0.26231419295072556, Final Batch Loss: 0.11479543894529343\n",
      "Epoch 2446, Loss: 0.257736511528492, Final Batch Loss: 0.11631342023611069\n",
      "Epoch 2447, Loss: 0.2571636363863945, Final Batch Loss: 0.08855754882097244\n",
      "Epoch 2448, Loss: 0.31019171327352524, Final Batch Loss: 0.2055804431438446\n",
      "Epoch 2449, Loss: 0.23247861117124557, Final Batch Loss: 0.11722542345523834\n",
      "Epoch 2450, Loss: 0.2183346077799797, Final Batch Loss: 0.1217205822467804\n",
      "Epoch 2451, Loss: 0.25117939710617065, Final Batch Loss: 0.13496004045009613\n",
      "Epoch 2452, Loss: 0.22539978474378586, Final Batch Loss: 0.10293344408273697\n",
      "Epoch 2453, Loss: 0.2613540217280388, Final Batch Loss: 0.14949320256710052\n",
      "Epoch 2454, Loss: 0.18529808521270752, Final Batch Loss: 0.10404682159423828\n",
      "Epoch 2455, Loss: 0.2766650915145874, Final Batch Loss: 0.11153145134449005\n",
      "Epoch 2456, Loss: 0.27823735028505325, Final Batch Loss: 0.1575465351343155\n",
      "Epoch 2457, Loss: 0.2176157608628273, Final Batch Loss: 0.11211511492729187\n",
      "Epoch 2458, Loss: 0.18178690969944, Final Batch Loss: 0.0853394940495491\n",
      "Epoch 2459, Loss: 0.21418868750333786, Final Batch Loss: 0.07138795405626297\n",
      "Epoch 2460, Loss: 0.2574407681822777, Final Batch Loss: 0.12178980559110641\n",
      "Epoch 2461, Loss: 0.210151769220829, Final Batch Loss: 0.10117844492197037\n",
      "Epoch 2462, Loss: 0.1950031816959381, Final Batch Loss: 0.0956168919801712\n",
      "Epoch 2463, Loss: 0.2279983013868332, Final Batch Loss: 0.1028367131948471\n",
      "Epoch 2464, Loss: 0.2500966265797615, Final Batch Loss: 0.07798967510461807\n",
      "Epoch 2465, Loss: 0.24075190722942352, Final Batch Loss: 0.15458297729492188\n",
      "Epoch 2466, Loss: 0.2544821724295616, Final Batch Loss: 0.1008244976401329\n",
      "Epoch 2467, Loss: 0.2809939980506897, Final Batch Loss: 0.1484471708536148\n",
      "Epoch 2468, Loss: 0.2539841905236244, Final Batch Loss: 0.17814823985099792\n",
      "Epoch 2469, Loss: 0.21684493869543076, Final Batch Loss: 0.09986437857151031\n",
      "Epoch 2470, Loss: 0.18845147639513016, Final Batch Loss: 0.08118119090795517\n",
      "Epoch 2471, Loss: 0.19246511161327362, Final Batch Loss: 0.1045728474855423\n",
      "Epoch 2472, Loss: 0.22580531984567642, Final Batch Loss: 0.08708510547876358\n",
      "Epoch 2473, Loss: 0.23259275406599045, Final Batch Loss: 0.11561375111341476\n",
      "Epoch 2474, Loss: 0.13857444375753403, Final Batch Loss: 0.060533054172992706\n",
      "Epoch 2475, Loss: 0.25388313084840775, Final Batch Loss: 0.10117905586957932\n",
      "Epoch 2476, Loss: 0.2131630852818489, Final Batch Loss: 0.10853267461061478\n",
      "Epoch 2477, Loss: 0.20521004498004913, Final Batch Loss: 0.10580155998468399\n",
      "Epoch 2478, Loss: 0.20663169771432877, Final Batch Loss: 0.0912201777100563\n",
      "Epoch 2479, Loss: 0.18017752468585968, Final Batch Loss: 0.09070852398872375\n",
      "Epoch 2480, Loss: 0.21144185960292816, Final Batch Loss: 0.11285696178674698\n",
      "Epoch 2481, Loss: 0.2034890279173851, Final Batch Loss: 0.09906012564897537\n",
      "Epoch 2482, Loss: 0.2919524163007736, Final Batch Loss: 0.1289951652288437\n",
      "Epoch 2483, Loss: 0.2898111715912819, Final Batch Loss: 0.19936658442020416\n",
      "Epoch 2484, Loss: 0.25225985050201416, Final Batch Loss: 0.13911668956279755\n",
      "Epoch 2485, Loss: 0.2352236732840538, Final Batch Loss: 0.12615221738815308\n",
      "Epoch 2486, Loss: 0.21931004524230957, Final Batch Loss: 0.087633416056633\n",
      "Epoch 2487, Loss: 0.191983163356781, Final Batch Loss: 0.08927357196807861\n",
      "Epoch 2488, Loss: 0.3021707162261009, Final Batch Loss: 0.18801376223564148\n",
      "Epoch 2489, Loss: 0.21349631994962692, Final Batch Loss: 0.11186288297176361\n",
      "Epoch 2490, Loss: 0.2138097584247589, Final Batch Loss: 0.09393501281738281\n",
      "Epoch 2491, Loss: 0.2635646387934685, Final Batch Loss: 0.14909768104553223\n",
      "Epoch 2492, Loss: 0.28717753291130066, Final Batch Loss: 0.15429458022117615\n",
      "Epoch 2493, Loss: 0.25335243344306946, Final Batch Loss: 0.11465752124786377\n",
      "Epoch 2494, Loss: 0.23199590295553207, Final Batch Loss: 0.15430477261543274\n",
      "Epoch 2495, Loss: 0.2298314943909645, Final Batch Loss: 0.1401410549879074\n",
      "Epoch 2496, Loss: 0.17021828889846802, Final Batch Loss: 0.09731019288301468\n",
      "Epoch 2497, Loss: 0.1906263828277588, Final Batch Loss: 0.09729395806789398\n",
      "Epoch 2498, Loss: 0.1881626844406128, Final Batch Loss: 0.09255912899971008\n",
      "Epoch 2499, Loss: 0.22825365513563156, Final Batch Loss: 0.15263880789279938\n",
      "Epoch 2500, Loss: 0.19154489785432816, Final Batch Loss: 0.09102156013250351\n",
      "Epoch 2501, Loss: 0.20641335099935532, Final Batch Loss: 0.1048196330666542\n",
      "Epoch 2502, Loss: 0.17685848474502563, Final Batch Loss: 0.11402931809425354\n",
      "Epoch 2503, Loss: 0.1622128039598465, Final Batch Loss: 0.06476660817861557\n",
      "Epoch 2504, Loss: 0.17987531423568726, Final Batch Loss: 0.07605040073394775\n",
      "Epoch 2505, Loss: 0.20437483489513397, Final Batch Loss: 0.09677989035844803\n",
      "Epoch 2506, Loss: 0.22553950548171997, Final Batch Loss: 0.10593122988939285\n",
      "Epoch 2507, Loss: 0.23214323073625565, Final Batch Loss: 0.07838694006204605\n",
      "Epoch 2508, Loss: 0.21977443993091583, Final Batch Loss: 0.11507373303174973\n",
      "Epoch 2509, Loss: 0.2636037617921829, Final Batch Loss: 0.10613429546356201\n",
      "Epoch 2510, Loss: 0.18340111523866653, Final Batch Loss: 0.11924774944782257\n",
      "Epoch 2511, Loss: 0.2500233128666878, Final Batch Loss: 0.1581660956144333\n",
      "Epoch 2512, Loss: 0.29265445470809937, Final Batch Loss: 0.14756891131401062\n",
      "Epoch 2513, Loss: 0.22545621544122696, Final Batch Loss: 0.11347690969705582\n",
      "Epoch 2514, Loss: 0.19662954658269882, Final Batch Loss: 0.10065009444952011\n",
      "Epoch 2515, Loss: 0.28932639211416245, Final Batch Loss: 0.18391412496566772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2516, Loss: 0.19672667235136032, Final Batch Loss: 0.09842241555452347\n",
      "Epoch 2517, Loss: 0.20135368406772614, Final Batch Loss: 0.10952597111463547\n",
      "Epoch 2518, Loss: 0.285544753074646, Final Batch Loss: 0.17617267370224\n",
      "Epoch 2519, Loss: 0.26364942640066147, Final Batch Loss: 0.15832310914993286\n",
      "Epoch 2520, Loss: 0.18387101590633392, Final Batch Loss: 0.08769930154085159\n",
      "Epoch 2521, Loss: 0.24925699830055237, Final Batch Loss: 0.10088470578193665\n",
      "Epoch 2522, Loss: 0.18980804085731506, Final Batch Loss: 0.07893840968608856\n",
      "Epoch 2523, Loss: 0.20771031081676483, Final Batch Loss: 0.13073131442070007\n",
      "Epoch 2524, Loss: 0.2324349507689476, Final Batch Loss: 0.1086706593632698\n",
      "Epoch 2525, Loss: 0.22199168801307678, Final Batch Loss: 0.10703323036432266\n",
      "Epoch 2526, Loss: 0.27001435309648514, Final Batch Loss: 0.18046219646930695\n",
      "Epoch 2527, Loss: 0.2073463648557663, Final Batch Loss: 0.12707243859767914\n",
      "Epoch 2528, Loss: 0.18293433636426926, Final Batch Loss: 0.08394256234169006\n",
      "Epoch 2529, Loss: 0.2934943437576294, Final Batch Loss: 0.11503420770168304\n",
      "Epoch 2530, Loss: 0.23845495283603668, Final Batch Loss: 0.1458573192358017\n",
      "Epoch 2531, Loss: 0.22976066172122955, Final Batch Loss: 0.10995369404554367\n",
      "Epoch 2532, Loss: 0.17672065645456314, Final Batch Loss: 0.10676522552967072\n",
      "Epoch 2533, Loss: 0.17808722704648972, Final Batch Loss: 0.10631587356328964\n",
      "Epoch 2534, Loss: 0.21429533511400223, Final Batch Loss: 0.1262277066707611\n",
      "Epoch 2535, Loss: 0.18748041987419128, Final Batch Loss: 0.1095924898982048\n",
      "Epoch 2536, Loss: 0.19629449397325516, Final Batch Loss: 0.1040550246834755\n",
      "Epoch 2537, Loss: 0.25341491401195526, Final Batch Loss: 0.11407126486301422\n",
      "Epoch 2538, Loss: 0.19772817939519882, Final Batch Loss: 0.09444277733564377\n",
      "Epoch 2539, Loss: 0.2941778004169464, Final Batch Loss: 0.14405103027820587\n",
      "Epoch 2540, Loss: 0.20035212486982346, Final Batch Loss: 0.08899299055337906\n",
      "Epoch 2541, Loss: 0.1938675045967102, Final Batch Loss: 0.10264156758785248\n",
      "Epoch 2542, Loss: 0.20796770602464676, Final Batch Loss: 0.10124113410711288\n",
      "Epoch 2543, Loss: 0.2097371146082878, Final Batch Loss: 0.10267660021781921\n",
      "Epoch 2544, Loss: 0.19421961903572083, Final Batch Loss: 0.10634152591228485\n",
      "Epoch 2545, Loss: 0.19567551463842392, Final Batch Loss: 0.08441051840782166\n",
      "Epoch 2546, Loss: 0.22309353202581406, Final Batch Loss: 0.13686399161815643\n",
      "Epoch 2547, Loss: 0.2764270007610321, Final Batch Loss: 0.1224866658449173\n",
      "Epoch 2548, Loss: 0.24029453098773956, Final Batch Loss: 0.11206860840320587\n",
      "Epoch 2549, Loss: 0.2083389237523079, Final Batch Loss: 0.10230240225791931\n",
      "Epoch 2550, Loss: 0.2149486467242241, Final Batch Loss: 0.10682506114244461\n",
      "Epoch 2551, Loss: 0.2535695806145668, Final Batch Loss: 0.13356365263462067\n",
      "Epoch 2552, Loss: 0.23429736495018005, Final Batch Loss: 0.09104032814502716\n",
      "Epoch 2553, Loss: 0.18578430265188217, Final Batch Loss: 0.0899580791592598\n",
      "Epoch 2554, Loss: 0.14407366514205933, Final Batch Loss: 0.07821052521467209\n",
      "Epoch 2555, Loss: 0.1785554736852646, Final Batch Loss: 0.1102343425154686\n",
      "Epoch 2556, Loss: 0.3007119745016098, Final Batch Loss: 0.1411076933145523\n",
      "Epoch 2557, Loss: 0.26235850155353546, Final Batch Loss: 0.12596745789051056\n",
      "Epoch 2558, Loss: 0.26158201694488525, Final Batch Loss: 0.1267571598291397\n",
      "Epoch 2559, Loss: 0.19577394425868988, Final Batch Loss: 0.09676451981067657\n",
      "Epoch 2560, Loss: 0.1925845965743065, Final Batch Loss: 0.1340487003326416\n",
      "Epoch 2561, Loss: 0.22898053377866745, Final Batch Loss: 0.13186323642730713\n",
      "Epoch 2562, Loss: 0.1744019240140915, Final Batch Loss: 0.06471040844917297\n",
      "Epoch 2563, Loss: 0.19769729673862457, Final Batch Loss: 0.08139315247535706\n",
      "Epoch 2564, Loss: 0.21194805204868317, Final Batch Loss: 0.11104897409677505\n",
      "Epoch 2565, Loss: 0.22001807391643524, Final Batch Loss: 0.10931836068630219\n",
      "Epoch 2566, Loss: 0.18151286244392395, Final Batch Loss: 0.10350339859724045\n",
      "Epoch 2567, Loss: 0.1748906895518303, Final Batch Loss: 0.09313119202852249\n",
      "Epoch 2568, Loss: 0.27055665105581284, Final Batch Loss: 0.11981730908155441\n",
      "Epoch 2569, Loss: 0.20801102370023727, Final Batch Loss: 0.13381601870059967\n",
      "Epoch 2570, Loss: 0.18412703275680542, Final Batch Loss: 0.10080904513597488\n",
      "Epoch 2571, Loss: 0.2171632945537567, Final Batch Loss: 0.14424605667591095\n",
      "Epoch 2572, Loss: 0.23405615985393524, Final Batch Loss: 0.10574814677238464\n",
      "Epoch 2573, Loss: 0.2236645594239235, Final Batch Loss: 0.11955925077199936\n",
      "Epoch 2574, Loss: 0.2325601950287819, Final Batch Loss: 0.12226682156324387\n",
      "Epoch 2575, Loss: 0.30562005937099457, Final Batch Loss: 0.21319445967674255\n",
      "Epoch 2576, Loss: 0.21701518446207047, Final Batch Loss: 0.0923309400677681\n",
      "Epoch 2577, Loss: 0.19926542788743973, Final Batch Loss: 0.11761664599180222\n",
      "Epoch 2578, Loss: 0.19884344935417175, Final Batch Loss: 0.07776210457086563\n",
      "Epoch 2579, Loss: 0.23964467644691467, Final Batch Loss: 0.12589432299137115\n",
      "Epoch 2580, Loss: 0.22733337432146072, Final Batch Loss: 0.10870467871427536\n",
      "Epoch 2581, Loss: 0.2539675906300545, Final Batch Loss: 0.1591331660747528\n",
      "Epoch 2582, Loss: 0.22961611300706863, Final Batch Loss: 0.12582086026668549\n",
      "Epoch 2583, Loss: 0.19730623811483383, Final Batch Loss: 0.09994872659444809\n",
      "Epoch 2584, Loss: 0.22200822085142136, Final Batch Loss: 0.13692478835582733\n",
      "Epoch 2585, Loss: 0.20950397849082947, Final Batch Loss: 0.1494920700788498\n",
      "Epoch 2586, Loss: 0.25808851420879364, Final Batch Loss: 0.1223253458738327\n",
      "Epoch 2587, Loss: 0.2348547875881195, Final Batch Loss: 0.1394818127155304\n",
      "Epoch 2588, Loss: 0.20825731754302979, Final Batch Loss: 0.1147860586643219\n",
      "Epoch 2589, Loss: 0.20648780465126038, Final Batch Loss: 0.05367380380630493\n",
      "Epoch 2590, Loss: 0.181799054145813, Final Batch Loss: 0.10722596198320389\n",
      "Epoch 2591, Loss: 0.1294020228087902, Final Batch Loss: 0.07068175822496414\n",
      "Epoch 2592, Loss: 0.26524414122104645, Final Batch Loss: 0.18139147758483887\n",
      "Epoch 2593, Loss: 0.1751922443509102, Final Batch Loss: 0.09063311666250229\n",
      "Epoch 2594, Loss: 0.21673190593719482, Final Batch Loss: 0.1421755999326706\n",
      "Epoch 2595, Loss: 0.24819988012313843, Final Batch Loss: 0.14995817840099335\n",
      "Epoch 2596, Loss: 0.1459268406033516, Final Batch Loss: 0.07697548717260361\n",
      "Epoch 2597, Loss: 0.26437824219465256, Final Batch Loss: 0.16161450743675232\n",
      "Epoch 2598, Loss: 0.19049955159425735, Final Batch Loss: 0.10290850698947906\n",
      "Epoch 2599, Loss: 0.22242774814367294, Final Batch Loss: 0.11306916922330856\n",
      "Epoch 2600, Loss: 0.15006113052368164, Final Batch Loss: 0.07512263208627701\n",
      "Epoch 2601, Loss: 0.17498594522476196, Final Batch Loss: 0.06345721334218979\n",
      "Epoch 2602, Loss: 0.17348696291446686, Final Batch Loss: 0.08581331372261047\n",
      "Epoch 2603, Loss: 0.1529109925031662, Final Batch Loss: 0.0781300738453865\n",
      "Epoch 2604, Loss: 0.1726519912481308, Final Batch Loss: 0.0688721314072609\n",
      "Epoch 2605, Loss: 0.16238371282815933, Final Batch Loss: 0.09450791776180267\n",
      "Epoch 2606, Loss: 0.26644817739725113, Final Batch Loss: 0.10750622302293777\n",
      "Epoch 2607, Loss: 0.23168287426233292, Final Batch Loss: 0.10164492577314377\n",
      "Epoch 2608, Loss: 0.2004445269703865, Final Batch Loss: 0.11038601398468018\n",
      "Epoch 2609, Loss: 0.17517783492803574, Final Batch Loss: 0.0778881087899208\n",
      "Epoch 2610, Loss: 0.18011656403541565, Final Batch Loss: 0.07786538451910019\n",
      "Epoch 2611, Loss: 0.20220854133367538, Final Batch Loss: 0.105484239757061\n",
      "Epoch 2612, Loss: 0.16457045823335648, Final Batch Loss: 0.0688730999827385\n",
      "Epoch 2613, Loss: 0.2304229587316513, Final Batch Loss: 0.129172682762146\n",
      "Epoch 2614, Loss: 0.19656971096992493, Final Batch Loss: 0.12394991517066956\n",
      "Epoch 2615, Loss: 0.1380949169397354, Final Batch Loss: 0.07206229120492935\n",
      "Epoch 2616, Loss: 0.1736597828567028, Final Batch Loss: 0.061795178800821304\n",
      "Epoch 2617, Loss: 0.1526523344218731, Final Batch Loss: 0.09865414351224899\n",
      "Epoch 2618, Loss: 0.24303646385669708, Final Batch Loss: 0.11325529217720032\n",
      "Epoch 2619, Loss: 0.18528946489095688, Final Batch Loss: 0.07666163146495819\n",
      "Epoch 2620, Loss: 0.15899541229009628, Final Batch Loss: 0.08942960202693939\n",
      "Epoch 2621, Loss: 0.18095311522483826, Final Batch Loss: 0.08464572578668594\n",
      "Epoch 2622, Loss: 0.16119274497032166, Final Batch Loss: 0.10674897581338882\n",
      "Epoch 2623, Loss: 0.1695788949728012, Final Batch Loss: 0.06806612014770508\n",
      "Epoch 2624, Loss: 0.21206311881542206, Final Batch Loss: 0.08590812981128693\n",
      "Epoch 2625, Loss: 0.2390245869755745, Final Batch Loss: 0.12522485852241516\n",
      "Epoch 2626, Loss: 0.21028753370046616, Final Batch Loss: 0.06276986747980118\n",
      "Epoch 2627, Loss: 0.2261558547616005, Final Batch Loss: 0.13018010556697845\n",
      "Epoch 2628, Loss: 0.2167728841304779, Final Batch Loss: 0.08131425082683563\n",
      "Epoch 2629, Loss: 0.15527962148189545, Final Batch Loss: 0.08366715908050537\n",
      "Epoch 2630, Loss: 0.17404931038618088, Final Batch Loss: 0.10589589178562164\n",
      "Epoch 2631, Loss: 0.1133871003985405, Final Batch Loss: 0.05314204841852188\n",
      "Epoch 2632, Loss: 0.17508253455162048, Final Batch Loss: 0.10232612490653992\n",
      "Epoch 2633, Loss: 0.19273480027914047, Final Batch Loss: 0.09168338030576706\n",
      "Epoch 2634, Loss: 0.19791123270988464, Final Batch Loss: 0.10274077206850052\n",
      "Epoch 2635, Loss: 0.17446957528591156, Final Batch Loss: 0.10202369838953018\n",
      "Epoch 2636, Loss: 0.1794864684343338, Final Batch Loss: 0.10678298026323318\n",
      "Epoch 2637, Loss: 0.19197407364845276, Final Batch Loss: 0.08367536962032318\n",
      "Epoch 2638, Loss: 0.20128613710403442, Final Batch Loss: 0.11676327884197235\n",
      "Epoch 2639, Loss: 0.17614275217056274, Final Batch Loss: 0.09188877791166306\n",
      "Epoch 2640, Loss: 0.22123096883296967, Final Batch Loss: 0.11751901358366013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2641, Loss: 0.23353739827871323, Final Batch Loss: 0.10470981150865555\n",
      "Epoch 2642, Loss: 0.24095284938812256, Final Batch Loss: 0.15601272881031036\n",
      "Epoch 2643, Loss: 0.2267623096704483, Final Batch Loss: 0.09412825107574463\n",
      "Epoch 2644, Loss: 0.2084166333079338, Final Batch Loss: 0.11317011713981628\n",
      "Epoch 2645, Loss: 0.1569046713411808, Final Batch Loss: 0.09490275382995605\n",
      "Epoch 2646, Loss: 0.1841570883989334, Final Batch Loss: 0.08676604926586151\n",
      "Epoch 2647, Loss: 0.2567259147763252, Final Batch Loss: 0.14953477680683136\n",
      "Epoch 2648, Loss: 0.22065287828445435, Final Batch Loss: 0.11193089932203293\n",
      "Epoch 2649, Loss: 0.28223513066768646, Final Batch Loss: 0.150832399725914\n",
      "Epoch 2650, Loss: 0.22779597342014313, Final Batch Loss: 0.13795369863510132\n",
      "Epoch 2651, Loss: 0.22626088559627533, Final Batch Loss: 0.1416958123445511\n",
      "Epoch 2652, Loss: 0.18310607224702835, Final Batch Loss: 0.08906050026416779\n",
      "Epoch 2653, Loss: 0.1526830941438675, Final Batch Loss: 0.08436346799135208\n",
      "Epoch 2654, Loss: 0.23520081490278244, Final Batch Loss: 0.10981471091508865\n",
      "Epoch 2655, Loss: 0.21601174026727676, Final Batch Loss: 0.13601098954677582\n",
      "Epoch 2656, Loss: 0.19441485404968262, Final Batch Loss: 0.11580271273851395\n",
      "Epoch 2657, Loss: 0.25679463148117065, Final Batch Loss: 0.12846703827381134\n",
      "Epoch 2658, Loss: 0.14933007955551147, Final Batch Loss: 0.07992135733366013\n",
      "Epoch 2659, Loss: 0.2175580859184265, Final Batch Loss: 0.12160161882638931\n",
      "Epoch 2660, Loss: 0.22066275030374527, Final Batch Loss: 0.09834883362054825\n",
      "Epoch 2661, Loss: 0.21018598973751068, Final Batch Loss: 0.11636772751808167\n",
      "Epoch 2662, Loss: 0.17855685949325562, Final Batch Loss: 0.0836271420121193\n",
      "Epoch 2663, Loss: 0.19084348529577255, Final Batch Loss: 0.09880048781633377\n",
      "Epoch 2664, Loss: 0.2337464988231659, Final Batch Loss: 0.09569071233272552\n",
      "Epoch 2665, Loss: 0.20392326265573502, Final Batch Loss: 0.12233289331197739\n",
      "Epoch 2666, Loss: 0.113437969237566, Final Batch Loss: 0.064255490899086\n",
      "Epoch 2667, Loss: 0.18219057470560074, Final Batch Loss: 0.08786233514547348\n",
      "Epoch 2668, Loss: 0.16819576174020767, Final Batch Loss: 0.09028653800487518\n",
      "Epoch 2669, Loss: 0.18099118769168854, Final Batch Loss: 0.08077457547187805\n",
      "Epoch 2670, Loss: 0.2585197016596794, Final Batch Loss: 0.10357972234487534\n",
      "Epoch 2671, Loss: 0.15087269991636276, Final Batch Loss: 0.07964912801980972\n",
      "Epoch 2672, Loss: 0.19383933395147324, Final Batch Loss: 0.10344060510396957\n",
      "Epoch 2673, Loss: 0.14390929043293, Final Batch Loss: 0.044097498059272766\n",
      "Epoch 2674, Loss: 0.17903776466846466, Final Batch Loss: 0.0944269672036171\n",
      "Epoch 2675, Loss: 0.14909914135932922, Final Batch Loss: 0.09656333923339844\n",
      "Epoch 2676, Loss: 0.1854647845029831, Final Batch Loss: 0.09738824516534805\n",
      "Epoch 2677, Loss: 0.22062192857265472, Final Batch Loss: 0.07982929050922394\n",
      "Epoch 2678, Loss: 0.2059781737625599, Final Batch Loss: 0.14411017298698425\n",
      "Epoch 2679, Loss: 0.1756434366106987, Final Batch Loss: 0.09463394433259964\n",
      "Epoch 2680, Loss: 0.16817015409469604, Final Batch Loss: 0.08154024928808212\n",
      "Epoch 2681, Loss: 0.166858471930027, Final Batch Loss: 0.07392256706953049\n",
      "Epoch 2682, Loss: 0.21389047801494598, Final Batch Loss: 0.141281858086586\n",
      "Epoch 2683, Loss: 0.16901834309101105, Final Batch Loss: 0.07477878034114838\n",
      "Epoch 2684, Loss: 0.1764027550816536, Final Batch Loss: 0.06282058358192444\n",
      "Epoch 2685, Loss: 0.20249272137880325, Final Batch Loss: 0.11380825936794281\n",
      "Epoch 2686, Loss: 0.28843503445386887, Final Batch Loss: 0.10527894645929337\n",
      "Epoch 2687, Loss: 0.16445130482316017, Final Batch Loss: 0.05898401513695717\n",
      "Epoch 2688, Loss: 0.16871241480112076, Final Batch Loss: 0.09511534124612808\n",
      "Epoch 2689, Loss: 0.19773761928081512, Final Batch Loss: 0.12208542227745056\n",
      "Epoch 2690, Loss: 0.1948343962430954, Final Batch Loss: 0.08344382792711258\n",
      "Epoch 2691, Loss: 0.24683012068271637, Final Batch Loss: 0.1413823664188385\n",
      "Epoch 2692, Loss: 0.17269188910722733, Final Batch Loss: 0.08312852680683136\n",
      "Epoch 2693, Loss: 0.23071610927581787, Final Batch Loss: 0.08022205531597137\n",
      "Epoch 2694, Loss: 0.20132694393396378, Final Batch Loss: 0.09979291260242462\n",
      "Epoch 2695, Loss: 0.1828456073999405, Final Batch Loss: 0.08348836749792099\n",
      "Epoch 2696, Loss: 0.19021661579608917, Final Batch Loss: 0.10332249104976654\n",
      "Epoch 2697, Loss: 0.2014085277915001, Final Batch Loss: 0.08520873636007309\n",
      "Epoch 2698, Loss: 0.1877283826470375, Final Batch Loss: 0.10826488584280014\n",
      "Epoch 2699, Loss: 0.18510469794273376, Final Batch Loss: 0.09689339995384216\n",
      "Epoch 2700, Loss: 0.16732138767838478, Final Batch Loss: 0.05813153460621834\n",
      "Epoch 2701, Loss: 0.20622450858354568, Final Batch Loss: 0.07869520038366318\n",
      "Epoch 2702, Loss: 0.1912575513124466, Final Batch Loss: 0.07749274373054504\n",
      "Epoch 2703, Loss: 0.21389438211917877, Final Batch Loss: 0.0994873046875\n",
      "Epoch 2704, Loss: 0.1830277219414711, Final Batch Loss: 0.09072678536176682\n",
      "Epoch 2705, Loss: 0.16891328990459442, Final Batch Loss: 0.08550450950860977\n",
      "Epoch 2706, Loss: 0.26620495319366455, Final Batch Loss: 0.1167549341917038\n",
      "Epoch 2707, Loss: 0.20249366015195847, Final Batch Loss: 0.09489719569683075\n",
      "Epoch 2708, Loss: 0.1410558596253395, Final Batch Loss: 0.06694379448890686\n",
      "Epoch 2709, Loss: 0.1823204681277275, Final Batch Loss: 0.11360880732536316\n",
      "Epoch 2710, Loss: 0.23180392384529114, Final Batch Loss: 0.08012285828590393\n",
      "Epoch 2711, Loss: 0.2173965498805046, Final Batch Loss: 0.11370909959077835\n",
      "Epoch 2712, Loss: 0.21996493637561798, Final Batch Loss: 0.12520067393779755\n",
      "Epoch 2713, Loss: 0.26819565892219543, Final Batch Loss: 0.13476306200027466\n",
      "Epoch 2714, Loss: 0.1796327605843544, Final Batch Loss: 0.11445291340351105\n",
      "Epoch 2715, Loss: 0.19443311542272568, Final Batch Loss: 0.09607705473899841\n",
      "Epoch 2716, Loss: 0.2020307257771492, Final Batch Loss: 0.08868787437677383\n",
      "Epoch 2717, Loss: 0.1820104718208313, Final Batch Loss: 0.09407728165388107\n",
      "Epoch 2718, Loss: 0.1650409772992134, Final Batch Loss: 0.07892210781574249\n",
      "Epoch 2719, Loss: 0.2006748989224434, Final Batch Loss: 0.1143903061747551\n",
      "Epoch 2720, Loss: 0.1760079711675644, Final Batch Loss: 0.07243192940950394\n",
      "Epoch 2721, Loss: 0.09836309775710106, Final Batch Loss: 0.05447226017713547\n",
      "Epoch 2722, Loss: 0.2206059768795967, Final Batch Loss: 0.11581173539161682\n",
      "Epoch 2723, Loss: 0.23511327803134918, Final Batch Loss: 0.11972842365503311\n",
      "Epoch 2724, Loss: 0.2078201249241829, Final Batch Loss: 0.08333954960107803\n",
      "Epoch 2725, Loss: 0.23166900128126144, Final Batch Loss: 0.0986955538392067\n",
      "Epoch 2726, Loss: 0.163263700902462, Final Batch Loss: 0.08019509166479111\n",
      "Epoch 2727, Loss: 0.16255469620227814, Final Batch Loss: 0.08094511926174164\n",
      "Epoch 2728, Loss: 0.2841724380850792, Final Batch Loss: 0.17341943085193634\n",
      "Epoch 2729, Loss: 0.18844638764858246, Final Batch Loss: 0.10920768231153488\n",
      "Epoch 2730, Loss: 0.2060593068599701, Final Batch Loss: 0.09084701538085938\n",
      "Epoch 2731, Loss: 0.1929379180073738, Final Batch Loss: 0.07011302560567856\n",
      "Epoch 2732, Loss: 0.1653299704194069, Final Batch Loss: 0.09974976629018784\n",
      "Epoch 2733, Loss: 0.2620718255639076, Final Batch Loss: 0.10734451562166214\n",
      "Epoch 2734, Loss: 0.19834361225366592, Final Batch Loss: 0.06622304767370224\n",
      "Epoch 2735, Loss: 0.17745546996593475, Final Batch Loss: 0.08659745007753372\n",
      "Epoch 2736, Loss: 0.2335435375571251, Final Batch Loss: 0.09959468990564346\n",
      "Epoch 2737, Loss: 0.23627056181430817, Final Batch Loss: 0.12395581603050232\n",
      "Epoch 2738, Loss: 0.17752088606357574, Final Batch Loss: 0.0802917629480362\n",
      "Epoch 2739, Loss: 0.17537149786949158, Final Batch Loss: 0.07639512419700623\n",
      "Epoch 2740, Loss: 0.263362854719162, Final Batch Loss: 0.12324516475200653\n",
      "Epoch 2741, Loss: 0.1611199900507927, Final Batch Loss: 0.06672783195972443\n",
      "Epoch 2742, Loss: 0.18358229845762253, Final Batch Loss: 0.0730506181716919\n",
      "Epoch 2743, Loss: 0.11197658255696297, Final Batch Loss: 0.05806306377053261\n",
      "Epoch 2744, Loss: 0.23188000172376633, Final Batch Loss: 0.16568109393119812\n",
      "Epoch 2745, Loss: 0.19954940676689148, Final Batch Loss: 0.08524627238512039\n",
      "Epoch 2746, Loss: 0.16747505217790604, Final Batch Loss: 0.08706251531839371\n",
      "Epoch 2747, Loss: 0.1959630250930786, Final Batch Loss: 0.1268165409564972\n",
      "Epoch 2748, Loss: 0.20235782116651535, Final Batch Loss: 0.10303337872028351\n",
      "Epoch 2749, Loss: 0.23402982205152512, Final Batch Loss: 0.14265216886997223\n",
      "Epoch 2750, Loss: 0.1867183968424797, Final Batch Loss: 0.10294544696807861\n",
      "Epoch 2751, Loss: 0.20125901699066162, Final Batch Loss: 0.14423327147960663\n",
      "Epoch 2752, Loss: 0.10648010298609734, Final Batch Loss: 0.05526113510131836\n",
      "Epoch 2753, Loss: 0.18280474841594696, Final Batch Loss: 0.06998161971569061\n",
      "Epoch 2754, Loss: 0.15126599371433258, Final Batch Loss: 0.05898667871952057\n",
      "Epoch 2755, Loss: 0.13989735767245293, Final Batch Loss: 0.08962225914001465\n",
      "Epoch 2756, Loss: 0.1402999311685562, Final Batch Loss: 0.06787167489528656\n",
      "Epoch 2757, Loss: 0.16336758434772491, Final Batch Loss: 0.0682457908987999\n",
      "Epoch 2758, Loss: 0.24429532885551453, Final Batch Loss: 0.13973353803157806\n",
      "Epoch 2759, Loss: 0.16608517244458199, Final Batch Loss: 0.05346835032105446\n",
      "Epoch 2760, Loss: 0.18858180940151215, Final Batch Loss: 0.10937080532312393\n",
      "Epoch 2761, Loss: 0.17515797168016434, Final Batch Loss: 0.07090862095355988\n",
      "Epoch 2762, Loss: 0.15964043885469437, Final Batch Loss: 0.06771819293498993\n",
      "Epoch 2763, Loss: 0.30856455862522125, Final Batch Loss: 0.1309070885181427\n",
      "Epoch 2764, Loss: 0.2143845185637474, Final Batch Loss: 0.11843165010213852\n",
      "Epoch 2765, Loss: 0.1749412938952446, Final Batch Loss: 0.10561804473400116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2766, Loss: 0.2667982280254364, Final Batch Loss: 0.12961570918560028\n",
      "Epoch 2767, Loss: 0.20593543350696564, Final Batch Loss: 0.08028504252433777\n",
      "Epoch 2768, Loss: 0.21295742690563202, Final Batch Loss: 0.10572849214076996\n",
      "Epoch 2769, Loss: 0.2312471568584442, Final Batch Loss: 0.09881915152072906\n",
      "Epoch 2770, Loss: 0.17004133760929108, Final Batch Loss: 0.09832075238227844\n",
      "Epoch 2771, Loss: 0.16295990347862244, Final Batch Loss: 0.08745179325342178\n",
      "Epoch 2772, Loss: 0.1890908107161522, Final Batch Loss: 0.10062912851572037\n",
      "Epoch 2773, Loss: 0.13509121537208557, Final Batch Loss: 0.06850016117095947\n",
      "Epoch 2774, Loss: 0.16457884013652802, Final Batch Loss: 0.06538271903991699\n",
      "Epoch 2775, Loss: 0.17017248272895813, Final Batch Loss: 0.09530262649059296\n",
      "Epoch 2776, Loss: 0.2169087827205658, Final Batch Loss: 0.13265100121498108\n",
      "Epoch 2777, Loss: 0.1849999576807022, Final Batch Loss: 0.1221727654337883\n",
      "Epoch 2778, Loss: 0.2004830613732338, Final Batch Loss: 0.09133175760507584\n",
      "Epoch 2779, Loss: 0.20275569707155228, Final Batch Loss: 0.08084627240896225\n",
      "Epoch 2780, Loss: 0.2597394809126854, Final Batch Loss: 0.1716981679201126\n",
      "Epoch 2781, Loss: 0.1822776272892952, Final Batch Loss: 0.11197944730520248\n",
      "Epoch 2782, Loss: 0.2191956713795662, Final Batch Loss: 0.10865618288516998\n",
      "Epoch 2783, Loss: 0.22598529607057571, Final Batch Loss: 0.09712079912424088\n",
      "Epoch 2784, Loss: 0.14528390765190125, Final Batch Loss: 0.1017741933465004\n",
      "Epoch 2785, Loss: 0.17292360961437225, Final Batch Loss: 0.10640861093997955\n",
      "Epoch 2786, Loss: 0.18085597455501556, Final Batch Loss: 0.03786344826221466\n",
      "Epoch 2787, Loss: 0.19883554428815842, Final Batch Loss: 0.08689139038324356\n",
      "Epoch 2788, Loss: 0.23577085882425308, Final Batch Loss: 0.10975433140993118\n",
      "Epoch 2789, Loss: 0.18927013128995895, Final Batch Loss: 0.09319362044334412\n",
      "Epoch 2790, Loss: 0.20307794958353043, Final Batch Loss: 0.11016629636287689\n",
      "Epoch 2791, Loss: 0.18789035826921463, Final Batch Loss: 0.07937260717153549\n",
      "Epoch 2792, Loss: 0.20276938378810883, Final Batch Loss: 0.1140250489115715\n",
      "Epoch 2793, Loss: 0.19144044816493988, Final Batch Loss: 0.08430901914834976\n",
      "Epoch 2794, Loss: 0.13496478646993637, Final Batch Loss: 0.06366880238056183\n",
      "Epoch 2795, Loss: 0.19734764099121094, Final Batch Loss: 0.09341917186975479\n",
      "Epoch 2796, Loss: 0.21200131624937057, Final Batch Loss: 0.07675791531801224\n",
      "Epoch 2797, Loss: 0.16580280661582947, Final Batch Loss: 0.08713553100824356\n",
      "Epoch 2798, Loss: 0.20642779767513275, Final Batch Loss: 0.09389100968837738\n",
      "Epoch 2799, Loss: 0.1534479483962059, Final Batch Loss: 0.07337164133787155\n",
      "Epoch 2800, Loss: 0.14130482077598572, Final Batch Loss: 0.07116031646728516\n",
      "Epoch 2801, Loss: 0.14356128126382828, Final Batch Loss: 0.06355936825275421\n",
      "Epoch 2802, Loss: 0.1675100103020668, Final Batch Loss: 0.08715283125638962\n",
      "Epoch 2803, Loss: 0.20041023194789886, Final Batch Loss: 0.11525178700685501\n",
      "Epoch 2804, Loss: 0.1431153118610382, Final Batch Loss: 0.060995519161224365\n",
      "Epoch 2805, Loss: 0.18692530691623688, Final Batch Loss: 0.09504140168428421\n",
      "Epoch 2806, Loss: 0.16793179512023926, Final Batch Loss: 0.08583806455135345\n",
      "Epoch 2807, Loss: 0.1947954148054123, Final Batch Loss: 0.08374509960412979\n",
      "Epoch 2808, Loss: 0.14613023400306702, Final Batch Loss: 0.07897888869047165\n",
      "Epoch 2809, Loss: 0.18708287179470062, Final Batch Loss: 0.115453340113163\n",
      "Epoch 2810, Loss: 0.20461264997720718, Final Batch Loss: 0.08278706669807434\n",
      "Epoch 2811, Loss: 0.2048068344593048, Final Batch Loss: 0.09651157259941101\n",
      "Epoch 2812, Loss: 0.23552873730659485, Final Batch Loss: 0.11939571797847748\n",
      "Epoch 2813, Loss: 0.19187498092651367, Final Batch Loss: 0.12815482914447784\n",
      "Epoch 2814, Loss: 0.1432034894824028, Final Batch Loss: 0.0634058341383934\n",
      "Epoch 2815, Loss: 0.1840408369898796, Final Batch Loss: 0.07523366808891296\n",
      "Epoch 2816, Loss: 0.23620616644620895, Final Batch Loss: 0.07480820268392563\n",
      "Epoch 2817, Loss: 0.23994464427232742, Final Batch Loss: 0.13036298751831055\n",
      "Epoch 2818, Loss: 0.1253724955022335, Final Batch Loss: 0.05474324896931648\n",
      "Epoch 2819, Loss: 0.1504031978547573, Final Batch Loss: 0.0476032979786396\n",
      "Epoch 2820, Loss: 0.17802392691373825, Final Batch Loss: 0.08999378979206085\n",
      "Epoch 2821, Loss: 0.18717557191848755, Final Batch Loss: 0.10069064050912857\n",
      "Epoch 2822, Loss: 0.30147044360637665, Final Batch Loss: 0.1860661804676056\n",
      "Epoch 2823, Loss: 0.17814386636018753, Final Batch Loss: 0.07064969837665558\n",
      "Epoch 2824, Loss: 0.20802465826272964, Final Batch Loss: 0.11246342211961746\n",
      "Epoch 2825, Loss: 0.14800553023815155, Final Batch Loss: 0.0567634254693985\n",
      "Epoch 2826, Loss: 0.18744240701198578, Final Batch Loss: 0.07525350153446198\n",
      "Epoch 2827, Loss: 0.17265114188194275, Final Batch Loss: 0.09012472629547119\n",
      "Epoch 2828, Loss: 0.21527349948883057, Final Batch Loss: 0.13870833814144135\n",
      "Epoch 2829, Loss: 0.22910688817501068, Final Batch Loss: 0.11159846931695938\n",
      "Epoch 2830, Loss: 0.1920415386557579, Final Batch Loss: 0.10883437097072601\n",
      "Epoch 2831, Loss: 0.2535395920276642, Final Batch Loss: 0.12619145214557648\n",
      "Epoch 2832, Loss: 0.16576898843050003, Final Batch Loss: 0.07911842316389084\n",
      "Epoch 2833, Loss: 0.17569680511951447, Final Batch Loss: 0.08514485508203506\n",
      "Epoch 2834, Loss: 0.15617874637246132, Final Batch Loss: 0.06007162109017372\n",
      "Epoch 2835, Loss: 0.1868143528699875, Final Batch Loss: 0.11973151564598083\n",
      "Epoch 2836, Loss: 0.1917511597275734, Final Batch Loss: 0.09565267711877823\n",
      "Epoch 2837, Loss: 0.24027863144874573, Final Batch Loss: 0.12750959396362305\n",
      "Epoch 2838, Loss: 0.2576415240764618, Final Batch Loss: 0.16332267224788666\n",
      "Epoch 2839, Loss: 0.1590849831700325, Final Batch Loss: 0.07750679552555084\n",
      "Epoch 2840, Loss: 0.1864919811487198, Final Batch Loss: 0.07451676577329636\n",
      "Epoch 2841, Loss: 0.14696088433265686, Final Batch Loss: 0.07207894325256348\n",
      "Epoch 2842, Loss: 0.24634044617414474, Final Batch Loss: 0.10851416736841202\n",
      "Epoch 2843, Loss: 0.1925867646932602, Final Batch Loss: 0.10224094986915588\n",
      "Epoch 2844, Loss: 0.2062249481678009, Final Batch Loss: 0.11294139176607132\n",
      "Epoch 2845, Loss: 0.21962125599384308, Final Batch Loss: 0.14052525162696838\n",
      "Epoch 2846, Loss: 0.21708619594573975, Final Batch Loss: 0.10489965230226517\n",
      "Epoch 2847, Loss: 0.20580923557281494, Final Batch Loss: 0.07809163630008698\n",
      "Epoch 2848, Loss: 0.19481095671653748, Final Batch Loss: 0.12130936980247498\n",
      "Epoch 2849, Loss: 0.18483053892850876, Final Batch Loss: 0.07423373311758041\n",
      "Epoch 2850, Loss: 0.18453683704137802, Final Batch Loss: 0.09058079123497009\n",
      "Epoch 2851, Loss: 0.1791832521557808, Final Batch Loss: 0.08749867975711823\n",
      "Epoch 2852, Loss: 0.23252729326486588, Final Batch Loss: 0.15203484892845154\n",
      "Epoch 2853, Loss: 0.1707002893090248, Final Batch Loss: 0.09488825500011444\n",
      "Epoch 2854, Loss: 0.1823730394244194, Final Batch Loss: 0.0919908806681633\n",
      "Epoch 2855, Loss: 0.2233513668179512, Final Batch Loss: 0.10553696006536484\n",
      "Epoch 2856, Loss: 0.172815702855587, Final Batch Loss: 0.08832357823848724\n",
      "Epoch 2857, Loss: 0.20343469083309174, Final Batch Loss: 0.08069667965173721\n",
      "Epoch 2858, Loss: 0.19067221879959106, Final Batch Loss: 0.10932183265686035\n",
      "Epoch 2859, Loss: 0.1316898576915264, Final Batch Loss: 0.07535259425640106\n",
      "Epoch 2860, Loss: 0.14080509170889854, Final Batch Loss: 0.05556246265769005\n",
      "Epoch 2861, Loss: 0.12182159349322319, Final Batch Loss: 0.052112992852926254\n",
      "Epoch 2862, Loss: 0.1601652279496193, Final Batch Loss: 0.06835264712572098\n",
      "Epoch 2863, Loss: 0.17035186290740967, Final Batch Loss: 0.10240995138883591\n",
      "Epoch 2864, Loss: 0.18979182839393616, Final Batch Loss: 0.08128153532743454\n",
      "Epoch 2865, Loss: 0.17720159888267517, Final Batch Loss: 0.08376526832580566\n",
      "Epoch 2866, Loss: 0.21493186056613922, Final Batch Loss: 0.11599858105182648\n",
      "Epoch 2867, Loss: 0.16089605540037155, Final Batch Loss: 0.07268352061510086\n",
      "Epoch 2868, Loss: 0.22952920198440552, Final Batch Loss: 0.14279671013355255\n",
      "Epoch 2869, Loss: 0.13266067579388618, Final Batch Loss: 0.06000695750117302\n",
      "Epoch 2870, Loss: 0.20691771060228348, Final Batch Loss: 0.13949234783649445\n",
      "Epoch 2871, Loss: 0.25856655091047287, Final Batch Loss: 0.11199089139699936\n",
      "Epoch 2872, Loss: 0.17212221771478653, Final Batch Loss: 0.06466416269540787\n",
      "Epoch 2873, Loss: 0.1567140892148018, Final Batch Loss: 0.06457656621932983\n",
      "Epoch 2874, Loss: 0.24888325482606888, Final Batch Loss: 0.15136250853538513\n",
      "Epoch 2875, Loss: 0.13377028331160545, Final Batch Loss: 0.059017155319452286\n",
      "Epoch 2876, Loss: 0.21805471926927567, Final Batch Loss: 0.11299765110015869\n",
      "Epoch 2877, Loss: 0.16754446178674698, Final Batch Loss: 0.08757860958576202\n",
      "Epoch 2878, Loss: 0.13543236255645752, Final Batch Loss: 0.08122913539409637\n",
      "Epoch 2879, Loss: 0.21239537000656128, Final Batch Loss: 0.06444016098976135\n",
      "Epoch 2880, Loss: 0.1748541221022606, Final Batch Loss: 0.08926129341125488\n",
      "Epoch 2881, Loss: 0.23166406154632568, Final Batch Loss: 0.11046529561281204\n",
      "Epoch 2882, Loss: 0.1858161762356758, Final Batch Loss: 0.0818483754992485\n",
      "Epoch 2883, Loss: 0.19351840019226074, Final Batch Loss: 0.09932336211204529\n",
      "Epoch 2884, Loss: 0.1781066358089447, Final Batch Loss: 0.09154754132032394\n",
      "Epoch 2885, Loss: 0.13825121894478798, Final Batch Loss: 0.06121956929564476\n",
      "Epoch 2886, Loss: 0.13549374788999557, Final Batch Loss: 0.06229577213525772\n",
      "Epoch 2887, Loss: 0.15543808788061142, Final Batch Loss: 0.07252561300992966\n",
      "Epoch 2888, Loss: 0.13949549943208694, Final Batch Loss: 0.07294784486293793\n",
      "Epoch 2889, Loss: 0.20934978872537613, Final Batch Loss: 0.1144023984670639\n",
      "Epoch 2890, Loss: 0.14749988913536072, Final Batch Loss: 0.09116651862859726\n",
      "Epoch 2891, Loss: 0.1379818543791771, Final Batch Loss: 0.07449838519096375\n",
      "Epoch 2892, Loss: 0.12683885172009468, Final Batch Loss: 0.062256235629320145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2893, Loss: 0.13615088909864426, Final Batch Loss: 0.05568443238735199\n",
      "Epoch 2894, Loss: 0.16224020719528198, Final Batch Loss: 0.09872542321681976\n",
      "Epoch 2895, Loss: 0.16196074336767197, Final Batch Loss: 0.06640278548002243\n",
      "Epoch 2896, Loss: 0.17766683548688889, Final Batch Loss: 0.08600052446126938\n",
      "Epoch 2897, Loss: 0.19180793315172195, Final Batch Loss: 0.07782230526208878\n",
      "Epoch 2898, Loss: 0.21305836737155914, Final Batch Loss: 0.091420017182827\n",
      "Epoch 2899, Loss: 0.17347654700279236, Final Batch Loss: 0.10157876461744308\n",
      "Epoch 2900, Loss: 0.22470927238464355, Final Batch Loss: 0.12036038935184479\n",
      "Epoch 2901, Loss: 0.12145227566361427, Final Batch Loss: 0.05120457336306572\n",
      "Epoch 2902, Loss: 0.1477414220571518, Final Batch Loss: 0.08804643154144287\n",
      "Epoch 2903, Loss: 0.15914634615182877, Final Batch Loss: 0.0651719868183136\n",
      "Epoch 2904, Loss: 0.19143394380807877, Final Batch Loss: 0.08473746478557587\n",
      "Epoch 2905, Loss: 0.12774227559566498, Final Batch Loss: 0.0800657793879509\n",
      "Epoch 2906, Loss: 0.1939350813627243, Final Batch Loss: 0.09186180680990219\n",
      "Epoch 2907, Loss: 0.12713026255369186, Final Batch Loss: 0.07010778039693832\n",
      "Epoch 2908, Loss: 0.16577083989977837, Final Batch Loss: 0.10967586189508438\n",
      "Epoch 2909, Loss: 0.13782279193401337, Final Batch Loss: 0.04588578641414642\n",
      "Epoch 2910, Loss: 0.1817961484193802, Final Batch Loss: 0.0960269346833229\n",
      "Epoch 2911, Loss: 0.14485592767596245, Final Batch Loss: 0.043037500232458115\n",
      "Epoch 2912, Loss: 0.1712232530117035, Final Batch Loss: 0.07679357379674911\n",
      "Epoch 2913, Loss: 0.12044389918446541, Final Batch Loss: 0.05503985658288002\n",
      "Epoch 2914, Loss: 0.15489476174116135, Final Batch Loss: 0.052793048322200775\n",
      "Epoch 2915, Loss: 0.22125115245580673, Final Batch Loss: 0.12687022984027863\n",
      "Epoch 2916, Loss: 0.2230491191148758, Final Batch Loss: 0.11773313581943512\n",
      "Epoch 2917, Loss: 0.1633014753460884, Final Batch Loss: 0.09520784765481949\n",
      "Epoch 2918, Loss: 0.14860539138317108, Final Batch Loss: 0.0677916631102562\n",
      "Epoch 2919, Loss: 0.19514191895723343, Final Batch Loss: 0.06954071670770645\n",
      "Epoch 2920, Loss: 0.12644443660974503, Final Batch Loss: 0.04121016710996628\n",
      "Epoch 2921, Loss: 0.21171130239963531, Final Batch Loss: 0.14089612662792206\n",
      "Epoch 2922, Loss: 0.16488411277532578, Final Batch Loss: 0.07802771031856537\n",
      "Epoch 2923, Loss: 0.16525810211896896, Final Batch Loss: 0.0941958948969841\n",
      "Epoch 2924, Loss: 0.17011910676956177, Final Batch Loss: 0.09274060279130936\n",
      "Epoch 2925, Loss: 0.25012195110321045, Final Batch Loss: 0.09842835366725922\n",
      "Epoch 2926, Loss: 0.1614847406744957, Final Batch Loss: 0.08369667083024979\n",
      "Epoch 2927, Loss: 0.18222921341657639, Final Batch Loss: 0.09763921797275543\n",
      "Epoch 2928, Loss: 0.17225637659430504, Final Batch Loss: 0.12309849262237549\n",
      "Epoch 2929, Loss: 0.19452780857682228, Final Batch Loss: 0.05335574224591255\n",
      "Epoch 2930, Loss: 0.11848478764295578, Final Batch Loss: 0.058805033564567566\n",
      "Epoch 2931, Loss: 0.22292596101760864, Final Batch Loss: 0.13432809710502625\n",
      "Epoch 2932, Loss: 0.18395847082138062, Final Batch Loss: 0.06619325280189514\n",
      "Epoch 2933, Loss: 0.17587580531835556, Final Batch Loss: 0.09191492944955826\n",
      "Epoch 2934, Loss: 0.16500642895698547, Final Batch Loss: 0.07745037972927094\n",
      "Epoch 2935, Loss: 0.19088240712881088, Final Batch Loss: 0.07679544389247894\n",
      "Epoch 2936, Loss: 0.14694733172655106, Final Batch Loss: 0.06670036911964417\n",
      "Epoch 2937, Loss: 0.18872623145580292, Final Batch Loss: 0.08384907990694046\n",
      "Epoch 2938, Loss: 0.22140998393297195, Final Batch Loss: 0.10617326200008392\n",
      "Epoch 2939, Loss: 0.1826065182685852, Final Batch Loss: 0.08715898543596268\n",
      "Epoch 2940, Loss: 0.18511679768562317, Final Batch Loss: 0.10968463122844696\n",
      "Epoch 2941, Loss: 0.1597125306725502, Final Batch Loss: 0.08312413096427917\n",
      "Epoch 2942, Loss: 0.12256419658660889, Final Batch Loss: 0.05955338478088379\n",
      "Epoch 2943, Loss: 0.14004052430391312, Final Batch Loss: 0.09542502462863922\n",
      "Epoch 2944, Loss: 0.16426478326320648, Final Batch Loss: 0.06981722265481949\n",
      "Epoch 2945, Loss: 0.21057797968387604, Final Batch Loss: 0.13237889111042023\n",
      "Epoch 2946, Loss: 0.13529851660132408, Final Batch Loss: 0.08312074840068817\n",
      "Epoch 2947, Loss: 0.1265917867422104, Final Batch Loss: 0.06632161140441895\n",
      "Epoch 2948, Loss: 0.11345406621694565, Final Batch Loss: 0.046881817281246185\n",
      "Epoch 2949, Loss: 0.09494411945343018, Final Batch Loss: 0.050007931888103485\n",
      "Epoch 2950, Loss: 0.19497175514698029, Final Batch Loss: 0.07692794501781464\n",
      "Epoch 2951, Loss: 0.17486285418272018, Final Batch Loss: 0.08207747340202332\n",
      "Epoch 2952, Loss: 0.20255109667778015, Final Batch Loss: 0.1295231729745865\n",
      "Epoch 2953, Loss: 0.12617667019367218, Final Batch Loss: 0.05929764360189438\n",
      "Epoch 2954, Loss: 0.12609083205461502, Final Batch Loss: 0.06339170038700104\n",
      "Epoch 2955, Loss: 0.12365875765681267, Final Batch Loss: 0.0772111713886261\n",
      "Epoch 2956, Loss: 0.15175056084990501, Final Batch Loss: 0.06054007634520531\n",
      "Epoch 2957, Loss: 0.1773693859577179, Final Batch Loss: 0.10646654665470123\n",
      "Epoch 2958, Loss: 0.20470771566033363, Final Batch Loss: 0.05374192073941231\n",
      "Epoch 2959, Loss: 0.12267352640628815, Final Batch Loss: 0.05514474958181381\n",
      "Epoch 2960, Loss: 0.17818181961774826, Final Batch Loss: 0.08030105382204056\n",
      "Epoch 2961, Loss: 0.13037441298365593, Final Batch Loss: 0.07152485102415085\n",
      "Epoch 2962, Loss: 0.15755289793014526, Final Batch Loss: 0.07451602816581726\n",
      "Epoch 2963, Loss: 0.21069860458374023, Final Batch Loss: 0.09075356274843216\n",
      "Epoch 2964, Loss: 0.159246526658535, Final Batch Loss: 0.09224796295166016\n",
      "Epoch 2965, Loss: 0.17684442549943924, Final Batch Loss: 0.10720531642436981\n",
      "Epoch 2966, Loss: 0.17696034163236618, Final Batch Loss: 0.08114133030176163\n",
      "Epoch 2967, Loss: 0.1375073604285717, Final Batch Loss: 0.05437782034277916\n",
      "Epoch 2968, Loss: 0.17155218124389648, Final Batch Loss: 0.08711878955364227\n",
      "Epoch 2969, Loss: 0.18004372715950012, Final Batch Loss: 0.08753974735736847\n",
      "Epoch 2970, Loss: 0.12587643414735794, Final Batch Loss: 0.06930998712778091\n",
      "Epoch 2971, Loss: 0.13961458578705788, Final Batch Loss: 0.08547338098287582\n",
      "Epoch 2972, Loss: 0.16436774283647537, Final Batch Loss: 0.06862957030534744\n",
      "Epoch 2973, Loss: 0.20789922773838043, Final Batch Loss: 0.14066524803638458\n",
      "Epoch 2974, Loss: 0.14300402253866196, Final Batch Loss: 0.07659542560577393\n",
      "Epoch 2975, Loss: 0.16537829488515854, Final Batch Loss: 0.11228950321674347\n",
      "Epoch 2976, Loss: 0.15021830797195435, Final Batch Loss: 0.09328320622444153\n",
      "Epoch 2977, Loss: 0.18763866275548935, Final Batch Loss: 0.12252030521631241\n",
      "Epoch 2978, Loss: 0.17668775841593742, Final Batch Loss: 0.1420280784368515\n",
      "Epoch 2979, Loss: 0.12274628132581711, Final Batch Loss: 0.07010387629270554\n",
      "Epoch 2980, Loss: 0.20195607095956802, Final Batch Loss: 0.11209311336278915\n",
      "Epoch 2981, Loss: 0.14491745829582214, Final Batch Loss: 0.06689926236867905\n",
      "Epoch 2982, Loss: 0.132465448230505, Final Batch Loss: 0.07208466529846191\n",
      "Epoch 2983, Loss: 0.1716279461979866, Final Batch Loss: 0.07333851605653763\n",
      "Epoch 2984, Loss: 0.1886557713150978, Final Batch Loss: 0.08059195429086685\n",
      "Epoch 2985, Loss: 0.11905350908637047, Final Batch Loss: 0.06061822548508644\n",
      "Epoch 2986, Loss: 0.1609928458929062, Final Batch Loss: 0.07615862041711807\n",
      "Epoch 2987, Loss: 0.1531156376004219, Final Batch Loss: 0.10093136876821518\n",
      "Epoch 2988, Loss: 0.20967596024274826, Final Batch Loss: 0.1066981628537178\n",
      "Epoch 2989, Loss: 0.20496400445699692, Final Batch Loss: 0.10978027433156967\n",
      "Epoch 2990, Loss: 0.15773096308112144, Final Batch Loss: 0.1031227856874466\n",
      "Epoch 2991, Loss: 0.23302943259477615, Final Batch Loss: 0.07669491320848465\n",
      "Epoch 2992, Loss: 0.20005839318037033, Final Batch Loss: 0.09107713401317596\n",
      "Epoch 2993, Loss: 0.21099903434515, Final Batch Loss: 0.1018700823187828\n",
      "Epoch 2994, Loss: 0.11986071988940239, Final Batch Loss: 0.06618792563676834\n",
      "Epoch 2995, Loss: 0.2887401059269905, Final Batch Loss: 0.12394734472036362\n",
      "Epoch 2996, Loss: 0.10402321070432663, Final Batch Loss: 0.05492369458079338\n",
      "Epoch 2997, Loss: 0.10848387330770493, Final Batch Loss: 0.03936038911342621\n",
      "Epoch 2998, Loss: 0.20225295424461365, Final Batch Loss: 0.10719301551580429\n",
      "Epoch 2999, Loss: 0.11514680460095406, Final Batch Loss: 0.05727452412247658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000, Loss: 0.12584677711129189, Final Batch Loss: 0.08848735690116882\n",
      "Epoch 3001, Loss: 0.21049435436725616, Final Batch Loss: 0.11062439531087875\n",
      "Epoch 3002, Loss: 0.1723192110657692, Final Batch Loss: 0.10191471874713898\n",
      "Epoch 3003, Loss: 0.2510688379406929, Final Batch Loss: 0.07549306005239487\n",
      "Epoch 3004, Loss: 0.16229809820652008, Final Batch Loss: 0.10872559994459152\n",
      "Epoch 3005, Loss: 0.23649568855762482, Final Batch Loss: 0.16529400646686554\n",
      "Epoch 3006, Loss: 0.16075296700000763, Final Batch Loss: 0.10239484906196594\n",
      "Epoch 3007, Loss: 0.10763686522841454, Final Batch Loss: 0.051813263446092606\n",
      "Epoch 3008, Loss: 0.12491720169782639, Final Batch Loss: 0.042266108095645905\n",
      "Epoch 3009, Loss: 0.126876100897789, Final Batch Loss: 0.07718110084533691\n",
      "Epoch 3010, Loss: 0.14509819447994232, Final Batch Loss: 0.06731776893138885\n",
      "Epoch 3011, Loss: 0.1704997941851616, Final Batch Loss: 0.1039273589849472\n",
      "Epoch 3012, Loss: 0.15113651007413864, Final Batch Loss: 0.08566871285438538\n",
      "Epoch 3013, Loss: 0.128105029463768, Final Batch Loss: 0.06872013211250305\n",
      "Epoch 3014, Loss: 0.1623566560447216, Final Batch Loss: 0.12368682026863098\n",
      "Epoch 3015, Loss: 0.18311332911252975, Final Batch Loss: 0.08341220766305923\n",
      "Epoch 3016, Loss: 0.22083812952041626, Final Batch Loss: 0.14818228781223297\n",
      "Epoch 3017, Loss: 0.09673682972788811, Final Batch Loss: 0.04873981699347496\n",
      "Epoch 3018, Loss: 0.17290092259645462, Final Batch Loss: 0.09528983384370804\n",
      "Epoch 3019, Loss: 0.1383904442191124, Final Batch Loss: 0.07197367399930954\n",
      "Epoch 3020, Loss: 0.1690952256321907, Final Batch Loss: 0.0748731717467308\n",
      "Epoch 3021, Loss: 0.20689863711595535, Final Batch Loss: 0.12244191765785217\n",
      "Epoch 3022, Loss: 0.20482360571622849, Final Batch Loss: 0.14289294183254242\n",
      "Epoch 3023, Loss: 0.17923498153686523, Final Batch Loss: 0.0774541050195694\n",
      "Epoch 3024, Loss: 0.1090259850025177, Final Batch Loss: 0.06532970815896988\n",
      "Epoch 3025, Loss: 0.17178462445735931, Final Batch Loss: 0.1230282261967659\n",
      "Epoch 3026, Loss: 0.15993479639291763, Final Batch Loss: 0.0910564437508583\n",
      "Epoch 3027, Loss: 0.14166023582220078, Final Batch Loss: 0.06807039678096771\n",
      "Epoch 3028, Loss: 0.12019257992506027, Final Batch Loss: 0.07676491886377335\n",
      "Epoch 3029, Loss: 0.19221919775009155, Final Batch Loss: 0.11117375642061234\n",
      "Epoch 3030, Loss: 0.14942418038845062, Final Batch Loss: 0.0818808525800705\n",
      "Epoch 3031, Loss: 0.17953823134303093, Final Batch Loss: 0.04024000093340874\n",
      "Epoch 3032, Loss: 0.18961355090141296, Final Batch Loss: 0.07523654401302338\n",
      "Epoch 3033, Loss: 0.09678748995065689, Final Batch Loss: 0.04321800917387009\n",
      "Epoch 3034, Loss: 0.1208428144454956, Final Batch Loss: 0.06236672401428223\n",
      "Epoch 3035, Loss: 0.16960665583610535, Final Batch Loss: 0.09447000175714493\n",
      "Epoch 3036, Loss: 0.18093789368867874, Final Batch Loss: 0.08921829611063004\n",
      "Epoch 3037, Loss: 0.12508774921298027, Final Batch Loss: 0.06559104472398758\n",
      "Epoch 3038, Loss: 0.1717650592327118, Final Batch Loss: 0.08186240494251251\n",
      "Epoch 3039, Loss: 0.1198267936706543, Final Batch Loss: 0.04663984477519989\n",
      "Epoch 3040, Loss: 0.14842556416988373, Final Batch Loss: 0.06237246096134186\n",
      "Epoch 3041, Loss: 0.13792254030704498, Final Batch Loss: 0.06539132446050644\n",
      "Epoch 3042, Loss: 0.15637215971946716, Final Batch Loss: 0.05282812565565109\n",
      "Epoch 3043, Loss: 0.1794462874531746, Final Batch Loss: 0.10319105535745621\n",
      "Epoch 3044, Loss: 0.13127051293849945, Final Batch Loss: 0.0802520215511322\n",
      "Epoch 3045, Loss: 0.14047318696975708, Final Batch Loss: 0.06950072199106216\n",
      "Epoch 3046, Loss: 0.22934765368700027, Final Batch Loss: 0.1464618742465973\n",
      "Epoch 3047, Loss: 0.1566982939839363, Final Batch Loss: 0.09162446111440659\n",
      "Epoch 3048, Loss: 0.12779494002461433, Final Batch Loss: 0.04830830171704292\n",
      "Epoch 3049, Loss: 0.1593584269285202, Final Batch Loss: 0.08208718150854111\n",
      "Epoch 3050, Loss: 0.11043492704629898, Final Batch Loss: 0.04838141053915024\n",
      "Epoch 3051, Loss: 0.197572223842144, Final Batch Loss: 0.11790209263563156\n",
      "Epoch 3052, Loss: 0.15377496182918549, Final Batch Loss: 0.10709277540445328\n",
      "Epoch 3053, Loss: 0.1478734090924263, Final Batch Loss: 0.09299734234809875\n",
      "Epoch 3054, Loss: 0.1118965670466423, Final Batch Loss: 0.05838565528392792\n",
      "Epoch 3055, Loss: 0.14553048834204674, Final Batch Loss: 0.08523888885974884\n",
      "Epoch 3056, Loss: 0.20954442769289017, Final Batch Loss: 0.09239692240953445\n",
      "Epoch 3057, Loss: 0.17687886953353882, Final Batch Loss: 0.08234365284442902\n",
      "Epoch 3058, Loss: 0.19655802845954895, Final Batch Loss: 0.12063242495059967\n",
      "Epoch 3059, Loss: 0.15957573056221008, Final Batch Loss: 0.07358310371637344\n",
      "Epoch 3060, Loss: 0.1232709288597107, Final Batch Loss: 0.05757517367601395\n",
      "Epoch 3061, Loss: 0.1797301545739174, Final Batch Loss: 0.07842731475830078\n",
      "Epoch 3062, Loss: 0.17150869220495224, Final Batch Loss: 0.11597489565610886\n",
      "Epoch 3063, Loss: 0.13627206534147263, Final Batch Loss: 0.038413144648075104\n",
      "Epoch 3064, Loss: 0.1993280127644539, Final Batch Loss: 0.13260802626609802\n",
      "Epoch 3065, Loss: 0.17287129163742065, Final Batch Loss: 0.08409980684518814\n",
      "Epoch 3066, Loss: 0.14053045213222504, Final Batch Loss: 0.08231339603662491\n",
      "Epoch 3067, Loss: 0.18578870594501495, Final Batch Loss: 0.10004576295614243\n",
      "Epoch 3068, Loss: 0.1874728724360466, Final Batch Loss: 0.11288955807685852\n",
      "Epoch 3069, Loss: 0.1505294218659401, Final Batch Loss: 0.07330296188592911\n",
      "Epoch 3070, Loss: 0.1534094512462616, Final Batch Loss: 0.052651114761829376\n",
      "Epoch 3071, Loss: 0.16464129090309143, Final Batch Loss: 0.07848168164491653\n",
      "Epoch 3072, Loss: 0.11970249563455582, Final Batch Loss: 0.06935571134090424\n",
      "Epoch 3073, Loss: 0.1580657847225666, Final Batch Loss: 0.04368188604712486\n",
      "Epoch 3074, Loss: 0.15352898836135864, Final Batch Loss: 0.09309622645378113\n",
      "Epoch 3075, Loss: 0.1116437315940857, Final Batch Loss: 0.06815667450428009\n",
      "Epoch 3076, Loss: 0.13552769273519516, Final Batch Loss: 0.07272207736968994\n",
      "Epoch 3077, Loss: 0.12507616728544235, Final Batch Loss: 0.05570293217897415\n",
      "Epoch 3078, Loss: 0.14847895130515099, Final Batch Loss: 0.0867353230714798\n",
      "Epoch 3079, Loss: 0.17614047229290009, Final Batch Loss: 0.11821310967206955\n",
      "Epoch 3080, Loss: 0.1829635426402092, Final Batch Loss: 0.10675300657749176\n",
      "Epoch 3081, Loss: 0.10151851177215576, Final Batch Loss: 0.0596650131046772\n",
      "Epoch 3082, Loss: 0.15853743255138397, Final Batch Loss: 0.09520124644041061\n",
      "Epoch 3083, Loss: 0.18175862729549408, Final Batch Loss: 0.09213218837976456\n",
      "Epoch 3084, Loss: 0.13742615282535553, Final Batch Loss: 0.04587747901678085\n",
      "Epoch 3085, Loss: 0.14643677696585655, Final Batch Loss: 0.04903687164187431\n",
      "Epoch 3086, Loss: 0.13575966656208038, Final Batch Loss: 0.05526132881641388\n",
      "Epoch 3087, Loss: 0.2173459604382515, Final Batch Loss: 0.11038850992918015\n",
      "Epoch 3088, Loss: 0.19321273267269135, Final Batch Loss: 0.07263502478599548\n",
      "Epoch 3089, Loss: 0.15988657623529434, Final Batch Loss: 0.06954287737607956\n",
      "Epoch 3090, Loss: 0.15200214833021164, Final Batch Loss: 0.07523214817047119\n",
      "Epoch 3091, Loss: 0.12827961519360542, Final Batch Loss: 0.08558903634548187\n",
      "Epoch 3092, Loss: 0.1394289880990982, Final Batch Loss: 0.06963309645652771\n",
      "Epoch 3093, Loss: 0.1470249593257904, Final Batch Loss: 0.07154471427202225\n",
      "Epoch 3094, Loss: 0.17792873829603195, Final Batch Loss: 0.0822521522641182\n",
      "Epoch 3095, Loss: 0.21102624386548996, Final Batch Loss: 0.11355817317962646\n",
      "Epoch 3096, Loss: 0.1369253657758236, Final Batch Loss: 0.05555820092558861\n",
      "Epoch 3097, Loss: 0.1380794420838356, Final Batch Loss: 0.05423144996166229\n",
      "Epoch 3098, Loss: 0.12103857845067978, Final Batch Loss: 0.053477123379707336\n",
      "Epoch 3099, Loss: 0.2350521981716156, Final Batch Loss: 0.11606445908546448\n",
      "Epoch 3100, Loss: 0.15102408826351166, Final Batch Loss: 0.07582440227270126\n",
      "Epoch 3101, Loss: 0.1914920136332512, Final Batch Loss: 0.07292051613330841\n",
      "Epoch 3102, Loss: 0.12084828689694405, Final Batch Loss: 0.04165363684296608\n",
      "Epoch 3103, Loss: 0.14886275678873062, Final Batch Loss: 0.06177907437086105\n",
      "Epoch 3104, Loss: 0.14846281707286835, Final Batch Loss: 0.06908523291349411\n",
      "Epoch 3105, Loss: 0.14848598092794418, Final Batch Loss: 0.06605712324380875\n",
      "Epoch 3106, Loss: 0.14255325496196747, Final Batch Loss: 0.07989578694105148\n",
      "Epoch 3107, Loss: 0.13001466169953346, Final Batch Loss: 0.04950900748372078\n",
      "Epoch 3108, Loss: 0.08778435736894608, Final Batch Loss: 0.0402766615152359\n",
      "Epoch 3109, Loss: 0.13004876300692558, Final Batch Loss: 0.056142378598451614\n",
      "Epoch 3110, Loss: 0.1356353834271431, Final Batch Loss: 0.062990702688694\n",
      "Epoch 3111, Loss: 0.10412154719233513, Final Batch Loss: 0.045980338007211685\n",
      "Epoch 3112, Loss: 0.1952720768749714, Final Batch Loss: 0.13312633335590363\n",
      "Epoch 3113, Loss: 0.1746876835823059, Final Batch Loss: 0.0802668035030365\n",
      "Epoch 3114, Loss: 0.1501970775425434, Final Batch Loss: 0.048551756888628006\n",
      "Epoch 3115, Loss: 0.16690554469823837, Final Batch Loss: 0.08649715036153793\n",
      "Epoch 3116, Loss: 0.1854931116104126, Final Batch Loss: 0.10436651855707169\n",
      "Epoch 3117, Loss: 0.1805190145969391, Final Batch Loss: 0.10027584433555603\n",
      "Epoch 3118, Loss: 0.15145133063197136, Final Batch Loss: 0.060620736330747604\n",
      "Epoch 3119, Loss: 0.1718917191028595, Final Batch Loss: 0.10514457523822784\n",
      "Epoch 3120, Loss: 0.1840839385986328, Final Batch Loss: 0.10634953528642654\n",
      "Epoch 3121, Loss: 0.17306053638458252, Final Batch Loss: 0.09923005849123001\n",
      "Epoch 3122, Loss: 0.1579558253288269, Final Batch Loss: 0.09032163769006729\n",
      "Epoch 3123, Loss: 0.17418503761291504, Final Batch Loss: 0.06898687779903412\n",
      "Epoch 3124, Loss: 0.20417701452970505, Final Batch Loss: 0.1155686005949974\n",
      "Epoch 3125, Loss: 0.17999544739723206, Final Batch Loss: 0.09688068926334381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3126, Loss: 0.16947200149297714, Final Batch Loss: 0.08911263197660446\n",
      "Epoch 3127, Loss: 0.15146715193986893, Final Batch Loss: 0.07969740033149719\n",
      "Epoch 3128, Loss: 0.14469372481107712, Final Batch Loss: 0.07303273677825928\n",
      "Epoch 3129, Loss: 0.19829683750867844, Final Batch Loss: 0.08915548026561737\n",
      "Epoch 3130, Loss: 0.1478661745786667, Final Batch Loss: 0.06356792151927948\n",
      "Epoch 3131, Loss: 0.21264898777008057, Final Batch Loss: 0.10564079880714417\n",
      "Epoch 3132, Loss: 0.19308995455503464, Final Batch Loss: 0.11714380234479904\n",
      "Epoch 3133, Loss: 0.11092763394117355, Final Batch Loss: 0.04715371131896973\n",
      "Epoch 3134, Loss: 0.18053188174962997, Final Batch Loss: 0.0888490155339241\n",
      "Epoch 3135, Loss: 0.11955657228827477, Final Batch Loss: 0.0740947276353836\n",
      "Epoch 3136, Loss: 0.13198217377066612, Final Batch Loss: 0.05570526048541069\n",
      "Epoch 3137, Loss: 0.1353689283132553, Final Batch Loss: 0.06759089976549149\n",
      "Epoch 3138, Loss: 0.2372107058763504, Final Batch Loss: 0.15345850586891174\n",
      "Epoch 3139, Loss: 0.15029700845479965, Final Batch Loss: 0.08578190207481384\n",
      "Epoch 3140, Loss: 0.13050415366888046, Final Batch Loss: 0.07244466245174408\n",
      "Epoch 3141, Loss: 0.09812136366963387, Final Batch Loss: 0.04384375363588333\n",
      "Epoch 3142, Loss: 0.18891601264476776, Final Batch Loss: 0.08136806637048721\n",
      "Epoch 3143, Loss: 0.15376345068216324, Final Batch Loss: 0.06971614807844162\n",
      "Epoch 3144, Loss: 0.15751153230667114, Final Batch Loss: 0.07166016846895218\n",
      "Epoch 3145, Loss: 0.14029847458004951, Final Batch Loss: 0.06137268617749214\n",
      "Epoch 3146, Loss: 0.15481222420930862, Final Batch Loss: 0.11359144747257233\n",
      "Epoch 3147, Loss: 0.14901510253548622, Final Batch Loss: 0.05792256072163582\n",
      "Epoch 3148, Loss: 0.13797251135110855, Final Batch Loss: 0.06931249797344208\n",
      "Epoch 3149, Loss: 0.09606945514678955, Final Batch Loss: 0.05652330815792084\n",
      "Epoch 3150, Loss: 0.15598738193511963, Final Batch Loss: 0.05771277844905853\n",
      "Epoch 3151, Loss: 0.19195997714996338, Final Batch Loss: 0.129351407289505\n",
      "Epoch 3152, Loss: 0.13260643929243088, Final Batch Loss: 0.07876759022474289\n",
      "Epoch 3153, Loss: 0.14770468324422836, Final Batch Loss: 0.07673346251249313\n",
      "Epoch 3154, Loss: 0.1581733152270317, Final Batch Loss: 0.07810059189796448\n",
      "Epoch 3155, Loss: 0.11774247512221336, Final Batch Loss: 0.05477399751543999\n",
      "Epoch 3156, Loss: 0.15245936810970306, Final Batch Loss: 0.07290022820234299\n",
      "Epoch 3157, Loss: 0.0977160669863224, Final Batch Loss: 0.03909071534872055\n",
      "Epoch 3158, Loss: 0.10334473475813866, Final Batch Loss: 0.054329391568899155\n",
      "Epoch 3159, Loss: 0.11603769659996033, Final Batch Loss: 0.04893370717763901\n",
      "Epoch 3160, Loss: 0.1069788783788681, Final Batch Loss: 0.040703289210796356\n",
      "Epoch 3161, Loss: 0.15721099823713303, Final Batch Loss: 0.059088334441185\n",
      "Epoch 3162, Loss: 0.1611890159547329, Final Batch Loss: 0.05179617926478386\n",
      "Epoch 3163, Loss: 0.13338860496878624, Final Batch Loss: 0.07857993245124817\n",
      "Epoch 3164, Loss: 0.14593879878520966, Final Batch Loss: 0.04144642502069473\n",
      "Epoch 3165, Loss: 0.16190170496702194, Final Batch Loss: 0.08629073202610016\n",
      "Epoch 3166, Loss: 0.12107756361365318, Final Batch Loss: 0.044839587062597275\n",
      "Epoch 3167, Loss: 0.13711748272180557, Final Batch Loss: 0.06220631301403046\n",
      "Epoch 3168, Loss: 0.08963745459914207, Final Batch Loss: 0.052357546985149384\n",
      "Epoch 3169, Loss: 0.1138233058154583, Final Batch Loss: 0.05270995572209358\n",
      "Epoch 3170, Loss: 0.19073662906885147, Final Batch Loss: 0.11432686448097229\n",
      "Epoch 3171, Loss: 0.18601864576339722, Final Batch Loss: 0.08723966032266617\n",
      "Epoch 3172, Loss: 0.12836464494466782, Final Batch Loss: 0.051960840821266174\n",
      "Epoch 3173, Loss: 0.13462037593126297, Final Batch Loss: 0.05016516149044037\n",
      "Epoch 3174, Loss: 0.1668737828731537, Final Batch Loss: 0.1059018224477768\n",
      "Epoch 3175, Loss: 0.14755641669034958, Final Batch Loss: 0.060064129531383514\n",
      "Epoch 3176, Loss: 0.12857333198189735, Final Batch Loss: 0.07685068249702454\n",
      "Epoch 3177, Loss: 0.08953535184264183, Final Batch Loss: 0.050378844141960144\n",
      "Epoch 3178, Loss: 0.15997838973999023, Final Batch Loss: 0.08049634844064713\n",
      "Epoch 3179, Loss: 0.11899270489811897, Final Batch Loss: 0.05282874032855034\n",
      "Epoch 3180, Loss: 0.14730356261134148, Final Batch Loss: 0.04862834885716438\n",
      "Epoch 3181, Loss: 0.12588734552264214, Final Batch Loss: 0.0497150756418705\n",
      "Epoch 3182, Loss: 0.2346859946846962, Final Batch Loss: 0.14567914605140686\n",
      "Epoch 3183, Loss: 0.1301850862801075, Final Batch Loss: 0.09301033616065979\n",
      "Epoch 3184, Loss: 0.13849645107984543, Final Batch Loss: 0.06627563387155533\n",
      "Epoch 3185, Loss: 0.10149036720395088, Final Batch Loss: 0.0458662249147892\n",
      "Epoch 3186, Loss: 0.12662576138973236, Final Batch Loss: 0.04150726646184921\n",
      "Epoch 3187, Loss: 0.14907890558242798, Final Batch Loss: 0.07878224551677704\n",
      "Epoch 3188, Loss: 0.1101173460483551, Final Batch Loss: 0.0587628073990345\n",
      "Epoch 3189, Loss: 0.17387842386960983, Final Batch Loss: 0.0841260775923729\n",
      "Epoch 3190, Loss: 0.11538670212030411, Final Batch Loss: 0.06204766407608986\n",
      "Epoch 3191, Loss: 0.12568259611725807, Final Batch Loss: 0.05030083283782005\n",
      "Epoch 3192, Loss: 0.14597384631633759, Final Batch Loss: 0.0793587788939476\n",
      "Epoch 3193, Loss: 0.12598860263824463, Final Batch Loss: 0.06643076986074448\n",
      "Epoch 3194, Loss: 0.14607811719179153, Final Batch Loss: 0.06967053562402725\n",
      "Epoch 3195, Loss: 0.11372902989387512, Final Batch Loss: 0.04913492500782013\n",
      "Epoch 3196, Loss: 0.1259751357138157, Final Batch Loss: 0.041387733072042465\n",
      "Epoch 3197, Loss: 0.14625149965286255, Final Batch Loss: 0.06864230334758759\n",
      "Epoch 3198, Loss: 0.13881035894155502, Final Batch Loss: 0.0815465897321701\n",
      "Epoch 3199, Loss: 0.1960841566324234, Final Batch Loss: 0.1123344898223877\n",
      "Epoch 3200, Loss: 0.12335457280278206, Final Batch Loss: 0.053264934569597244\n",
      "Epoch 3201, Loss: 0.15420923382043839, Final Batch Loss: 0.06996505707502365\n",
      "Epoch 3202, Loss: 0.1581912562251091, Final Batch Loss: 0.08692353218793869\n",
      "Epoch 3203, Loss: 0.17687023058533669, Final Batch Loss: 0.11525656282901764\n",
      "Epoch 3204, Loss: 0.15861190855503082, Final Batch Loss: 0.07417909055948257\n",
      "Epoch 3205, Loss: 0.16535035520792007, Final Batch Loss: 0.07084998488426208\n",
      "Epoch 3206, Loss: 0.1882220208644867, Final Batch Loss: 0.09314153343439102\n",
      "Epoch 3207, Loss: 0.14051079750061035, Final Batch Loss: 0.06598224490880966\n",
      "Epoch 3208, Loss: 0.16184242069721222, Final Batch Loss: 0.09459991753101349\n",
      "Epoch 3209, Loss: 0.12651167437434196, Final Batch Loss: 0.08804232627153397\n",
      "Epoch 3210, Loss: 0.18352307379245758, Final Batch Loss: 0.10543952882289886\n",
      "Epoch 3211, Loss: 0.15035753697156906, Final Batch Loss: 0.09977234154939651\n",
      "Epoch 3212, Loss: 0.14208207465708256, Final Batch Loss: 0.028377780690789223\n",
      "Epoch 3213, Loss: 0.145530354231596, Final Batch Loss: 0.09430371969938278\n",
      "Epoch 3214, Loss: 0.14925288408994675, Final Batch Loss: 0.07142064720392227\n",
      "Epoch 3215, Loss: 0.1332673393189907, Final Batch Loss: 0.04715776816010475\n",
      "Epoch 3216, Loss: 0.12897533550858498, Final Batch Loss: 0.07140528410673141\n",
      "Epoch 3217, Loss: 0.1100362241268158, Final Batch Loss: 0.04380346089601517\n",
      "Epoch 3218, Loss: 0.18228311836719513, Final Batch Loss: 0.08370792120695114\n",
      "Epoch 3219, Loss: 0.21058912575244904, Final Batch Loss: 0.14686398208141327\n",
      "Epoch 3220, Loss: 0.1082577258348465, Final Batch Loss: 0.051352422684431076\n",
      "Epoch 3221, Loss: 0.17377649992704391, Final Batch Loss: 0.08107364177703857\n",
      "Epoch 3222, Loss: 0.11961726099252701, Final Batch Loss: 0.0572468601167202\n",
      "Epoch 3223, Loss: 0.17639818787574768, Final Batch Loss: 0.06980331987142563\n",
      "Epoch 3224, Loss: 0.23028496652841568, Final Batch Loss: 0.15824291110038757\n",
      "Epoch 3225, Loss: 0.13676730543375015, Final Batch Loss: 0.07276429235935211\n",
      "Epoch 3226, Loss: 0.11188458278775215, Final Batch Loss: 0.0668451339006424\n",
      "Epoch 3227, Loss: 0.12437121570110321, Final Batch Loss: 0.08687572181224823\n",
      "Epoch 3228, Loss: 0.20342261344194412, Final Batch Loss: 0.07793582230806351\n",
      "Epoch 3229, Loss: 0.1726510226726532, Final Batch Loss: 0.0817774087190628\n",
      "Epoch 3230, Loss: 0.10813498497009277, Final Batch Loss: 0.0744168683886528\n",
      "Epoch 3231, Loss: 0.12678145617246628, Final Batch Loss: 0.0462406650185585\n",
      "Epoch 3232, Loss: 0.14130263030529022, Final Batch Loss: 0.0709257647395134\n",
      "Epoch 3233, Loss: 0.10998392105102539, Final Batch Loss: 0.06803575158119202\n",
      "Epoch 3234, Loss: 0.08843999728560448, Final Batch Loss: 0.03986247256398201\n",
      "Epoch 3235, Loss: 0.13423702865839005, Final Batch Loss: 0.08020476251840591\n",
      "Epoch 3236, Loss: 0.15818902477622032, Final Batch Loss: 0.044952016323804855\n",
      "Epoch 3237, Loss: 0.15517503023147583, Final Batch Loss: 0.08178785443305969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3238, Loss: 0.1577913910150528, Final Batch Loss: 0.07853241264820099\n",
      "Epoch 3239, Loss: 0.1587391272187233, Final Batch Loss: 0.10355652868747711\n",
      "Epoch 3240, Loss: 0.17847158014774323, Final Batch Loss: 0.10747572034597397\n",
      "Epoch 3241, Loss: 0.14434276521205902, Final Batch Loss: 0.09248393028974533\n",
      "Epoch 3242, Loss: 0.15758614987134933, Final Batch Loss: 0.09287746995687485\n",
      "Epoch 3243, Loss: 0.1309974566102028, Final Batch Loss: 0.07540608197450638\n",
      "Epoch 3244, Loss: 0.14903301745653152, Final Batch Loss: 0.07647836208343506\n",
      "Epoch 3245, Loss: 0.22866544872522354, Final Batch Loss: 0.13866621255874634\n",
      "Epoch 3246, Loss: 0.14761221036314964, Final Batch Loss: 0.04782279208302498\n",
      "Epoch 3247, Loss: 0.09637892246246338, Final Batch Loss: 0.04096189886331558\n",
      "Epoch 3248, Loss: 0.14414263144135475, Final Batch Loss: 0.08289813995361328\n",
      "Epoch 3249, Loss: 0.14665869623422623, Final Batch Loss: 0.09625948220491409\n",
      "Epoch 3250, Loss: 0.14760583639144897, Final Batch Loss: 0.08997010439634323\n",
      "Epoch 3251, Loss: 0.1651906594634056, Final Batch Loss: 0.11026028543710709\n",
      "Epoch 3252, Loss: 0.11820188537240028, Final Batch Loss: 0.06542950123548508\n",
      "Epoch 3253, Loss: 0.1261209025979042, Final Batch Loss: 0.073805071413517\n",
      "Epoch 3254, Loss: 0.1780904158949852, Final Batch Loss: 0.09147115051746368\n",
      "Epoch 3255, Loss: 0.14031638205051422, Final Batch Loss: 0.04684332758188248\n",
      "Epoch 3256, Loss: 0.17974017560482025, Final Batch Loss: 0.07211584597826004\n",
      "Epoch 3257, Loss: 0.2188284769654274, Final Batch Loss: 0.10948390513658524\n",
      "Epoch 3258, Loss: 0.18626371026039124, Final Batch Loss: 0.08111504465341568\n",
      "Epoch 3259, Loss: 0.08901724591851234, Final Batch Loss: 0.042856477200984955\n",
      "Epoch 3260, Loss: 0.1733221635222435, Final Batch Loss: 0.07941821962594986\n",
      "Epoch 3261, Loss: 0.13635528460144997, Final Batch Loss: 0.05123923346400261\n",
      "Epoch 3262, Loss: 0.15209681913256645, Final Batch Loss: 0.054176319390535355\n",
      "Epoch 3263, Loss: 0.17420637235045433, Final Batch Loss: 0.054879214614629745\n",
      "Epoch 3264, Loss: 0.13606201484799385, Final Batch Loss: 0.06001230701804161\n",
      "Epoch 3265, Loss: 0.10319740325212479, Final Batch Loss: 0.04969406500458717\n",
      "Epoch 3266, Loss: 0.16590363532304764, Final Batch Loss: 0.06344007700681686\n",
      "Epoch 3267, Loss: 0.16458428651094437, Final Batch Loss: 0.08900192379951477\n",
      "Epoch 3268, Loss: 0.1502510830760002, Final Batch Loss: 0.05304030328989029\n",
      "Epoch 3269, Loss: 0.12134016677737236, Final Batch Loss: 0.08545888215303421\n",
      "Epoch 3270, Loss: 0.09405376017093658, Final Batch Loss: 0.0431104376912117\n",
      "Epoch 3271, Loss: 0.15534919500350952, Final Batch Loss: 0.05797283351421356\n",
      "Epoch 3272, Loss: 0.22128894180059433, Final Batch Loss: 0.13063077628612518\n",
      "Epoch 3273, Loss: 0.12699536606669426, Final Batch Loss: 0.09118674695491791\n",
      "Epoch 3274, Loss: 0.172295942902565, Final Batch Loss: 0.1117425486445427\n",
      "Epoch 3275, Loss: 0.11153733730316162, Final Batch Loss: 0.06751351058483124\n",
      "Epoch 3276, Loss: 0.15130870789289474, Final Batch Loss: 0.04883863031864166\n",
      "Epoch 3277, Loss: 0.14067307114601135, Final Batch Loss: 0.07022657245397568\n",
      "Epoch 3278, Loss: 0.1475403718650341, Final Batch Loss: 0.05820325389504433\n",
      "Epoch 3279, Loss: 0.21558385342359543, Final Batch Loss: 0.10376209765672684\n",
      "Epoch 3280, Loss: 0.1552530750632286, Final Batch Loss: 0.08061280101537704\n",
      "Epoch 3281, Loss: 0.1682828590273857, Final Batch Loss: 0.08588865399360657\n",
      "Epoch 3282, Loss: 0.10626299306750298, Final Batch Loss: 0.061297349631786346\n",
      "Epoch 3283, Loss: 0.1355031095445156, Final Batch Loss: 0.09201602637767792\n",
      "Epoch 3284, Loss: 0.10704124346375465, Final Batch Loss: 0.062434807419776917\n",
      "Epoch 3285, Loss: 0.11833243072032928, Final Batch Loss: 0.0750046893954277\n",
      "Epoch 3286, Loss: 0.13636323809623718, Final Batch Loss: 0.04216061532497406\n",
      "Epoch 3287, Loss: 0.136704221367836, Final Batch Loss: 0.04667394608259201\n",
      "Epoch 3288, Loss: 0.1357332020998001, Final Batch Loss: 0.0705554187297821\n",
      "Epoch 3289, Loss: 0.1713661476969719, Final Batch Loss: 0.05306863784790039\n",
      "Epoch 3290, Loss: 0.13606302067637444, Final Batch Loss: 0.07541139423847198\n",
      "Epoch 3291, Loss: 0.17351076751947403, Final Batch Loss: 0.09538517147302628\n",
      "Epoch 3292, Loss: 0.1748015061020851, Final Batch Loss: 0.09308169782161713\n",
      "Epoch 3293, Loss: 0.13875820487737656, Final Batch Loss: 0.06444475054740906\n",
      "Epoch 3294, Loss: 0.1403990238904953, Final Batch Loss: 0.07028503715991974\n",
      "Epoch 3295, Loss: 0.21868829429149628, Final Batch Loss: 0.1107078418135643\n",
      "Epoch 3296, Loss: 0.17015518248081207, Final Batch Loss: 0.08161834627389908\n",
      "Epoch 3297, Loss: 0.1320069618523121, Final Batch Loss: 0.08123249560594559\n",
      "Epoch 3298, Loss: 0.1299889050424099, Final Batch Loss: 0.04905634745955467\n",
      "Epoch 3299, Loss: 0.13068480789661407, Final Batch Loss: 0.049611903727054596\n",
      "Epoch 3300, Loss: 0.11298051103949547, Final Batch Loss: 0.06748300045728683\n",
      "Epoch 3301, Loss: 0.1222841702401638, Final Batch Loss: 0.050124604254961014\n",
      "Epoch 3302, Loss: 0.1222514770925045, Final Batch Loss: 0.058169227093458176\n",
      "Epoch 3303, Loss: 0.14726538956165314, Final Batch Loss: 0.07527467608451843\n",
      "Epoch 3304, Loss: 0.09225178882479668, Final Batch Loss: 0.046991076320409775\n",
      "Epoch 3305, Loss: 0.11228929087519646, Final Batch Loss: 0.055744390934705734\n",
      "Epoch 3306, Loss: 0.12206602841615677, Final Batch Loss: 0.06473718583583832\n",
      "Epoch 3307, Loss: 0.13572100549936295, Final Batch Loss: 0.051713958382606506\n",
      "Epoch 3308, Loss: 0.13897978886961937, Final Batch Loss: 0.030287999659776688\n",
      "Epoch 3309, Loss: 0.14410502091050148, Final Batch Loss: 0.09406079351902008\n",
      "Epoch 3310, Loss: 0.14601349458098412, Final Batch Loss: 0.08783402293920517\n",
      "Epoch 3311, Loss: 0.20703446120023727, Final Batch Loss: 0.131601020693779\n",
      "Epoch 3312, Loss: 0.12270824983716011, Final Batch Loss: 0.03613448515534401\n",
      "Epoch 3313, Loss: 0.16838876903057098, Final Batch Loss: 0.07617926597595215\n",
      "Epoch 3314, Loss: 0.14602961391210556, Final Batch Loss: 0.07544124871492386\n",
      "Epoch 3315, Loss: 0.11730656400322914, Final Batch Loss: 0.05678648501634598\n",
      "Epoch 3316, Loss: 0.11434242874383926, Final Batch Loss: 0.07418079674243927\n",
      "Epoch 3317, Loss: 0.14860019832849503, Final Batch Loss: 0.08444538712501526\n",
      "Epoch 3318, Loss: 0.1449182778596878, Final Batch Loss: 0.04489269107580185\n",
      "Epoch 3319, Loss: 0.12654365226626396, Final Batch Loss: 0.04564585164189339\n",
      "Epoch 3320, Loss: 0.17277337610721588, Final Batch Loss: 0.06528357416391373\n",
      "Epoch 3321, Loss: 0.17893848568201065, Final Batch Loss: 0.10369087010622025\n",
      "Epoch 3322, Loss: 0.13847921788692474, Final Batch Loss: 0.07751915603876114\n",
      "Epoch 3323, Loss: 0.1784072369337082, Final Batch Loss: 0.07787584513425827\n",
      "Epoch 3324, Loss: 0.14372026175260544, Final Batch Loss: 0.06824621558189392\n",
      "Epoch 3325, Loss: 0.1525307223200798, Final Batch Loss: 0.09715361893177032\n",
      "Epoch 3326, Loss: 0.1483563855290413, Final Batch Loss: 0.06797956675291061\n",
      "Epoch 3327, Loss: 0.11484730616211891, Final Batch Loss: 0.0573439747095108\n",
      "Epoch 3328, Loss: 0.08891769871115685, Final Batch Loss: 0.04678408056497574\n",
      "Epoch 3329, Loss: 0.139008030295372, Final Batch Loss: 0.06941141188144684\n",
      "Epoch 3330, Loss: 0.1521657258272171, Final Batch Loss: 0.07889674603939056\n",
      "Epoch 3331, Loss: 0.13692112267017365, Final Batch Loss: 0.06345579028129578\n",
      "Epoch 3332, Loss: 0.15223321691155434, Final Batch Loss: 0.05187765136361122\n",
      "Epoch 3333, Loss: 0.1932758316397667, Final Batch Loss: 0.13673052191734314\n",
      "Epoch 3334, Loss: 0.10051903873682022, Final Batch Loss: 0.056722693145275116\n",
      "Epoch 3335, Loss: 0.14465316012501717, Final Batch Loss: 0.10392025858163834\n",
      "Epoch 3336, Loss: 0.16865522414445877, Final Batch Loss: 0.07237165421247482\n",
      "Epoch 3337, Loss: 0.15521133691072464, Final Batch Loss: 0.07666538655757904\n",
      "Epoch 3338, Loss: 0.1506558172404766, Final Batch Loss: 0.05026127025485039\n",
      "Epoch 3339, Loss: 0.13361920043826103, Final Batch Loss: 0.052260804921388626\n",
      "Epoch 3340, Loss: 0.15888481214642525, Final Batch Loss: 0.0997501090168953\n",
      "Epoch 3341, Loss: 0.12402785569429398, Final Batch Loss: 0.060063354671001434\n",
      "Epoch 3342, Loss: 0.14002028852701187, Final Batch Loss: 0.0763087347149849\n",
      "Epoch 3343, Loss: 0.14086559787392616, Final Batch Loss: 0.09351243823766708\n",
      "Epoch 3344, Loss: 0.15500077605247498, Final Batch Loss: 0.04964902251958847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3345, Loss: 0.13623471930623055, Final Batch Loss: 0.05316967889666557\n",
      "Epoch 3346, Loss: 0.15730276331305504, Final Batch Loss: 0.09817447513341904\n",
      "Epoch 3347, Loss: 0.0799352377653122, Final Batch Loss: 0.03022400289773941\n",
      "Epoch 3348, Loss: 0.12449803575873375, Final Batch Loss: 0.08286436647176743\n",
      "Epoch 3349, Loss: 0.16510549187660217, Final Batch Loss: 0.10782131552696228\n",
      "Epoch 3350, Loss: 0.15668538957834244, Final Batch Loss: 0.06690816581249237\n",
      "Epoch 3351, Loss: 0.11805132031440735, Final Batch Loss: 0.036860391497612\n",
      "Epoch 3352, Loss: 0.1583254337310791, Final Batch Loss: 0.08203800022602081\n",
      "Epoch 3353, Loss: 0.1256748028099537, Final Batch Loss: 0.0365767739713192\n",
      "Epoch 3354, Loss: 0.11698300763964653, Final Batch Loss: 0.04821270331740379\n",
      "Epoch 3355, Loss: 0.0919443890452385, Final Batch Loss: 0.04020453989505768\n",
      "Epoch 3356, Loss: 0.11810659244656563, Final Batch Loss: 0.04378780350089073\n",
      "Epoch 3357, Loss: 0.14147325977683067, Final Batch Loss: 0.0901169627904892\n",
      "Epoch 3358, Loss: 0.18404366075992584, Final Batch Loss: 0.06973668187856674\n",
      "Epoch 3359, Loss: 0.2067696750164032, Final Batch Loss: 0.1410151869058609\n",
      "Epoch 3360, Loss: 0.15968568623065948, Final Batch Loss: 0.09184524416923523\n",
      "Epoch 3361, Loss: 0.1332327164709568, Final Batch Loss: 0.0779922753572464\n",
      "Epoch 3362, Loss: 0.12300526350736618, Final Batch Loss: 0.08001143485307693\n",
      "Epoch 3363, Loss: 0.13142863288521767, Final Batch Loss: 0.0498080812394619\n",
      "Epoch 3364, Loss: 0.13696463778614998, Final Batch Loss: 0.06091432645916939\n",
      "Epoch 3365, Loss: 0.2558297626674175, Final Batch Loss: 0.21956142783164978\n",
      "Epoch 3366, Loss: 0.1338174268603325, Final Batch Loss: 0.09071072190999985\n",
      "Epoch 3367, Loss: 0.17112784087657928, Final Batch Loss: 0.07903619110584259\n",
      "Epoch 3368, Loss: 0.11373236775398254, Final Batch Loss: 0.0691971629858017\n",
      "Epoch 3369, Loss: 0.10888201743364334, Final Batch Loss: 0.05769101157784462\n",
      "Epoch 3370, Loss: 0.09958059340715408, Final Batch Loss: 0.057656027376651764\n",
      "Epoch 3371, Loss: 0.1800851821899414, Final Batch Loss: 0.07014120370149612\n",
      "Epoch 3372, Loss: 0.1747594028711319, Final Batch Loss: 0.10468031466007233\n",
      "Epoch 3373, Loss: 0.1496448665857315, Final Batch Loss: 0.07022357732057571\n",
      "Epoch 3374, Loss: 0.11010486632585526, Final Batch Loss: 0.05891300365328789\n",
      "Epoch 3375, Loss: 0.14461170881986618, Final Batch Loss: 0.06232249736785889\n",
      "Epoch 3376, Loss: 0.12881475687026978, Final Batch Loss: 0.08552464097738266\n",
      "Epoch 3377, Loss: 0.11784423142671585, Final Batch Loss: 0.04625155031681061\n",
      "Epoch 3378, Loss: 0.1354532688856125, Final Batch Loss: 0.06270503997802734\n",
      "Epoch 3379, Loss: 0.16302789002656937, Final Batch Loss: 0.08071526885032654\n",
      "Epoch 3380, Loss: 0.16585645079612732, Final Batch Loss: 0.06391069293022156\n",
      "Epoch 3381, Loss: 0.2163107618689537, Final Batch Loss: 0.10998141020536423\n",
      "Epoch 3382, Loss: 0.10567968711256981, Final Batch Loss: 0.046069443225860596\n",
      "Epoch 3383, Loss: 0.12537012621760368, Final Batch Loss: 0.06458694487810135\n",
      "Epoch 3384, Loss: 0.1357041336596012, Final Batch Loss: 0.0799628347158432\n",
      "Epoch 3385, Loss: 0.14191700518131256, Final Batch Loss: 0.09218166768550873\n",
      "Epoch 3386, Loss: 0.14396146312355995, Final Batch Loss: 0.057686980813741684\n",
      "Epoch 3387, Loss: 0.13776160404086113, Final Batch Loss: 0.056691911071538925\n",
      "Epoch 3388, Loss: 0.1360878124833107, Final Batch Loss: 0.061656974256038666\n",
      "Epoch 3389, Loss: 0.14734584838151932, Final Batch Loss: 0.09031765162944794\n",
      "Epoch 3390, Loss: 0.12570681422948837, Final Batch Loss: 0.06570287048816681\n",
      "Epoch 3391, Loss: 0.13477282226085663, Final Batch Loss: 0.0714416354894638\n",
      "Epoch 3392, Loss: 0.11936305835843086, Final Batch Loss: 0.055643659085035324\n",
      "Epoch 3393, Loss: 0.12931031361222267, Final Batch Loss: 0.050593581050634384\n",
      "Epoch 3394, Loss: 0.10178279504179955, Final Batch Loss: 0.06263228505849838\n",
      "Epoch 3395, Loss: 0.18049675971269608, Final Batch Loss: 0.06368899345397949\n",
      "Epoch 3396, Loss: 0.17036928981542587, Final Batch Loss: 0.10744818300008774\n",
      "Epoch 3397, Loss: 0.11565957590937614, Final Batch Loss: 0.07115619629621506\n",
      "Epoch 3398, Loss: 0.20195075124502182, Final Batch Loss: 0.10619309544563293\n",
      "Epoch 3399, Loss: 0.20234980434179306, Final Batch Loss: 0.06596139818429947\n",
      "Epoch 3400, Loss: 0.12925558909773827, Final Batch Loss: 0.05612652376294136\n",
      "Epoch 3401, Loss: 0.13971315324306488, Final Batch Loss: 0.06430404633283615\n",
      "Epoch 3402, Loss: 0.20505358278751373, Final Batch Loss: 0.1139269694685936\n",
      "Epoch 3403, Loss: 0.1948116421699524, Final Batch Loss: 0.1055140346288681\n",
      "Epoch 3404, Loss: 0.1698690429329872, Final Batch Loss: 0.06328432261943817\n",
      "Epoch 3405, Loss: 0.1353711597621441, Final Batch Loss: 0.0622272752225399\n",
      "Epoch 3406, Loss: 0.20241939276456833, Final Batch Loss: 0.09933457523584366\n",
      "Epoch 3407, Loss: 0.0990789569914341, Final Batch Loss: 0.050137702375650406\n",
      "Epoch 3408, Loss: 0.15326710045337677, Final Batch Loss: 0.09105496108531952\n",
      "Epoch 3409, Loss: 0.2211749032139778, Final Batch Loss: 0.15612855553627014\n",
      "Epoch 3410, Loss: 0.20636329054832458, Final Batch Loss: 0.06957900524139404\n",
      "Epoch 3411, Loss: 0.11950735002756119, Final Batch Loss: 0.07293403148651123\n",
      "Epoch 3412, Loss: 0.19765213876962662, Final Batch Loss: 0.080962173640728\n",
      "Epoch 3413, Loss: 0.13705061003565788, Final Batch Loss: 0.08564414083957672\n",
      "Epoch 3414, Loss: 0.13997294008731842, Final Batch Loss: 0.07387600839138031\n",
      "Epoch 3415, Loss: 0.16901569068431854, Final Batch Loss: 0.0917702168226242\n",
      "Epoch 3416, Loss: 0.12721878290176392, Final Batch Loss: 0.06448711454868317\n",
      "Epoch 3417, Loss: 0.13357914611697197, Final Batch Loss: 0.05357217416167259\n",
      "Epoch 3418, Loss: 0.1347799375653267, Final Batch Loss: 0.04920176416635513\n",
      "Epoch 3419, Loss: 0.18139995634555817, Final Batch Loss: 0.08475837856531143\n",
      "Epoch 3420, Loss: 0.1356634497642517, Final Batch Loss: 0.07461027055978775\n",
      "Epoch 3421, Loss: 0.1604967936873436, Final Batch Loss: 0.10434149205684662\n",
      "Epoch 3422, Loss: 0.14060721546411514, Final Batch Loss: 0.07587079703807831\n",
      "Epoch 3423, Loss: 0.13625530153512955, Final Batch Loss: 0.0689065158367157\n",
      "Epoch 3424, Loss: 0.18295983225107193, Final Batch Loss: 0.10080267488956451\n",
      "Epoch 3425, Loss: 0.145413089543581, Final Batch Loss: 0.08420190960168839\n",
      "Epoch 3426, Loss: 0.2596418187022209, Final Batch Loss: 0.122654028236866\n",
      "Epoch 3427, Loss: 0.17686859518289566, Final Batch Loss: 0.0921594649553299\n",
      "Epoch 3428, Loss: 0.16373750567436218, Final Batch Loss: 0.06347043067216873\n",
      "Epoch 3429, Loss: 0.14269964769482613, Final Batch Loss: 0.0499940924346447\n",
      "Epoch 3430, Loss: 0.09630437195301056, Final Batch Loss: 0.06228657439351082\n",
      "Epoch 3431, Loss: 0.16977455466985703, Final Batch Loss: 0.08918165415525436\n",
      "Epoch 3432, Loss: 0.11046016588807106, Final Batch Loss: 0.05666547268629074\n",
      "Epoch 3433, Loss: 0.1061515100300312, Final Batch Loss: 0.058926355093717575\n",
      "Epoch 3434, Loss: 0.10021806880831718, Final Batch Loss: 0.04176579415798187\n",
      "Epoch 3435, Loss: 0.13838522881269455, Final Batch Loss: 0.08137106150388718\n",
      "Epoch 3436, Loss: 0.1644807755947113, Final Batch Loss: 0.0628262609243393\n",
      "Epoch 3437, Loss: 0.09134767577052116, Final Batch Loss: 0.05654088780283928\n",
      "Epoch 3438, Loss: 0.13322440534830093, Final Batch Loss: 0.0750039666891098\n",
      "Epoch 3439, Loss: 0.16404478251934052, Final Batch Loss: 0.061833456158638\n",
      "Epoch 3440, Loss: 0.12622959539294243, Final Batch Loss: 0.07498291879892349\n",
      "Epoch 3441, Loss: 0.07755795493721962, Final Batch Loss: 0.03640056028962135\n",
      "Epoch 3442, Loss: 0.18047332763671875, Final Batch Loss: 0.07801767438650131\n",
      "Epoch 3443, Loss: 0.13906537741422653, Final Batch Loss: 0.07803015410900116\n",
      "Epoch 3444, Loss: 0.17378971725702286, Final Batch Loss: 0.047772325575351715\n",
      "Epoch 3445, Loss: 0.11964648216962814, Final Batch Loss: 0.02811647206544876\n",
      "Epoch 3446, Loss: 0.11750926449894905, Final Batch Loss: 0.07863970845937729\n",
      "Epoch 3447, Loss: 0.12476075813174248, Final Batch Loss: 0.08385870605707169\n",
      "Epoch 3448, Loss: 0.09230887144804001, Final Batch Loss: 0.04955306276679039\n",
      "Epoch 3449, Loss: 0.1251649409532547, Final Batch Loss: 0.05942528694868088\n",
      "Epoch 3450, Loss: 0.130674596875906, Final Batch Loss: 0.05634505674242973\n",
      "Epoch 3451, Loss: 0.13057468086481094, Final Batch Loss: 0.0635969489812851\n",
      "Epoch 3452, Loss: 0.12619124352931976, Final Batch Loss: 0.06088864058256149\n",
      "Epoch 3453, Loss: 0.12348802387714386, Final Batch Loss: 0.03311612457036972\n",
      "Epoch 3454, Loss: 0.10681688785552979, Final Batch Loss: 0.05991048738360405\n",
      "Epoch 3455, Loss: 0.14630408585071564, Final Batch Loss: 0.09838569164276123\n",
      "Epoch 3456, Loss: 0.11384976655244827, Final Batch Loss: 0.06663159281015396\n",
      "Epoch 3457, Loss: 0.18703413009643555, Final Batch Loss: 0.09726249426603317\n",
      "Epoch 3458, Loss: 0.06997256353497505, Final Batch Loss: 0.03422531113028526\n",
      "Epoch 3459, Loss: 0.11944642663002014, Final Batch Loss: 0.07301373034715652\n",
      "Epoch 3460, Loss: 0.09916066750884056, Final Batch Loss: 0.04692576080560684\n",
      "Epoch 3461, Loss: 0.1267331726849079, Final Batch Loss: 0.050420355051755905\n",
      "Epoch 3462, Loss: 0.0907139703631401, Final Batch Loss: 0.03431876003742218\n",
      "Epoch 3463, Loss: 0.08950509130954742, Final Batch Loss: 0.053052742034196854\n",
      "Epoch 3464, Loss: 0.07467256672680378, Final Batch Loss: 0.029856598004698753\n",
      "Epoch 3465, Loss: 0.14782654121518135, Final Batch Loss: 0.05991065129637718\n",
      "Epoch 3466, Loss: 0.19464509189128876, Final Batch Loss: 0.09726116061210632\n",
      "Epoch 3467, Loss: 0.11837165057659149, Final Batch Loss: 0.06303334981203079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3468, Loss: 0.16738473996520042, Final Batch Loss: 0.11080201715230942\n",
      "Epoch 3469, Loss: 0.13874886184930801, Final Batch Loss: 0.04367319494485855\n",
      "Epoch 3470, Loss: 0.14309538900852203, Final Batch Loss: 0.05634728819131851\n",
      "Epoch 3471, Loss: 0.15199285745620728, Final Batch Loss: 0.05699734389781952\n",
      "Epoch 3472, Loss: 0.1350635178387165, Final Batch Loss: 0.059385497123003006\n",
      "Epoch 3473, Loss: 0.15548460185527802, Final Batch Loss: 0.0996791198849678\n",
      "Epoch 3474, Loss: 0.11156933009624481, Final Batch Loss: 0.06340149790048599\n",
      "Epoch 3475, Loss: 0.15886203944683075, Final Batch Loss: 0.0850568562746048\n",
      "Epoch 3476, Loss: 0.11102214828133583, Final Batch Loss: 0.06726228445768356\n",
      "Epoch 3477, Loss: 0.1185990497469902, Final Batch Loss: 0.07820162177085876\n",
      "Epoch 3478, Loss: 0.11708170920610428, Final Batch Loss: 0.07623851299285889\n",
      "Epoch 3479, Loss: 0.17153703421354294, Final Batch Loss: 0.06794827431440353\n",
      "Epoch 3480, Loss: 0.08486304432153702, Final Batch Loss: 0.04732099920511246\n",
      "Epoch 3481, Loss: 0.1843404695391655, Final Batch Loss: 0.08566639572381973\n",
      "Epoch 3482, Loss: 0.15294311940670013, Final Batch Loss: 0.06488141417503357\n",
      "Epoch 3483, Loss: 0.1385127753019333, Final Batch Loss: 0.05603545904159546\n",
      "Epoch 3484, Loss: 0.11700839549303055, Final Batch Loss: 0.08537985384464264\n",
      "Epoch 3485, Loss: 0.17872802913188934, Final Batch Loss: 0.09738663583993912\n",
      "Epoch 3486, Loss: 0.11701425537467003, Final Batch Loss: 0.04634205624461174\n",
      "Epoch 3487, Loss: 0.14848799630999565, Final Batch Loss: 0.08725323528051376\n",
      "Epoch 3488, Loss: 0.1485690400004387, Final Batch Loss: 0.07566332817077637\n",
      "Epoch 3489, Loss: 0.14428770914673805, Final Batch Loss: 0.06171047315001488\n",
      "Epoch 3490, Loss: 0.13503682985901833, Final Batch Loss: 0.08375570178031921\n",
      "Epoch 3491, Loss: 0.11199083179235458, Final Batch Loss: 0.06104554980993271\n",
      "Epoch 3492, Loss: 0.09073706716299057, Final Batch Loss: 0.06625641137361526\n",
      "Epoch 3493, Loss: 0.14538851752877235, Final Batch Loss: 0.03555409237742424\n",
      "Epoch 3494, Loss: 0.1537126637995243, Final Batch Loss: 0.10046762973070145\n",
      "Epoch 3495, Loss: 0.14457713440060616, Final Batch Loss: 0.08214586228132248\n",
      "Epoch 3496, Loss: 0.1025925949215889, Final Batch Loss: 0.05291645973920822\n",
      "Epoch 3497, Loss: 0.11887204647064209, Final Batch Loss: 0.04643332213163376\n",
      "Epoch 3498, Loss: 0.16073841601610184, Final Batch Loss: 0.07841555029153824\n",
      "Epoch 3499, Loss: 0.11262768879532814, Final Batch Loss: 0.057587627321481705\n",
      "Epoch 3500, Loss: 0.12330196797847748, Final Batch Loss: 0.06702558696269989\n",
      "Epoch 3501, Loss: 0.11868119239807129, Final Batch Loss: 0.047015391290187836\n",
      "Epoch 3502, Loss: 0.12279977649450302, Final Batch Loss: 0.05769522488117218\n",
      "Epoch 3503, Loss: 0.1160859689116478, Final Batch Loss: 0.040921345353126526\n",
      "Epoch 3504, Loss: 0.15132447332143784, Final Batch Loss: 0.08344335108995438\n",
      "Epoch 3505, Loss: 0.14855733141303062, Final Batch Loss: 0.09486477077007294\n",
      "Epoch 3506, Loss: 0.1330896094441414, Final Batch Loss: 0.08520802855491638\n",
      "Epoch 3507, Loss: 0.07820565812289715, Final Batch Loss: 0.05408838018774986\n",
      "Epoch 3508, Loss: 0.11685379222035408, Final Batch Loss: 0.05060870572924614\n",
      "Epoch 3509, Loss: 0.12315258383750916, Final Batch Loss: 0.07746414840221405\n",
      "Epoch 3510, Loss: 0.21326284855604172, Final Batch Loss: 0.11890879273414612\n",
      "Epoch 3511, Loss: 0.17803112417459488, Final Batch Loss: 0.08474303781986237\n",
      "Epoch 3512, Loss: 0.14037777855992317, Final Batch Loss: 0.08353447914123535\n",
      "Epoch 3513, Loss: 0.1430063657462597, Final Batch Loss: 0.05884537473320961\n",
      "Epoch 3514, Loss: 0.09024640545248985, Final Batch Loss: 0.03421635180711746\n",
      "Epoch 3515, Loss: 0.14178872480988503, Final Batch Loss: 0.048909787088632584\n",
      "Epoch 3516, Loss: 0.16139695793390274, Final Batch Loss: 0.08352640271186829\n",
      "Epoch 3517, Loss: 0.19880735874176025, Final Batch Loss: 0.07316528260707855\n",
      "Epoch 3518, Loss: 0.15452110394835472, Final Batch Loss: 0.09580690413713455\n",
      "Epoch 3519, Loss: 0.1501299887895584, Final Batch Loss: 0.08416833728551865\n",
      "Epoch 3520, Loss: 0.12473386153578758, Final Batch Loss: 0.08322717994451523\n",
      "Epoch 3521, Loss: 0.11596507579088211, Final Batch Loss: 0.08175864815711975\n",
      "Epoch 3522, Loss: 0.09739216417074203, Final Batch Loss: 0.022397011518478394\n",
      "Epoch 3523, Loss: 0.14060258120298386, Final Batch Loss: 0.05535435676574707\n",
      "Epoch 3524, Loss: 0.11332093924283981, Final Batch Loss: 0.06937278807163239\n",
      "Epoch 3525, Loss: 0.15599464625120163, Final Batch Loss: 0.07323303073644638\n",
      "Epoch 3526, Loss: 0.122316163033247, Final Batch Loss: 0.070173479616642\n",
      "Epoch 3527, Loss: 0.10401024669408798, Final Batch Loss: 0.05360158160328865\n",
      "Epoch 3528, Loss: 0.16915512830018997, Final Batch Loss: 0.07581746578216553\n",
      "Epoch 3529, Loss: 0.22245702892541885, Final Batch Loss: 0.10504685342311859\n",
      "Epoch 3530, Loss: 0.16512181237339973, Final Batch Loss: 0.04953181371092796\n",
      "Epoch 3531, Loss: 0.11797473579645157, Final Batch Loss: 0.05296510457992554\n",
      "Epoch 3532, Loss: 0.11215119436383247, Final Batch Loss: 0.04662337526679039\n",
      "Epoch 3533, Loss: 0.08639948442578316, Final Batch Loss: 0.04586108401417732\n",
      "Epoch 3534, Loss: 0.1785048171877861, Final Batch Loss: 0.08994017541408539\n",
      "Epoch 3535, Loss: 0.10369575023651123, Final Batch Loss: 0.03145019710063934\n",
      "Epoch 3536, Loss: 0.18439176678657532, Final Batch Loss: 0.0735163763165474\n",
      "Epoch 3537, Loss: 0.16409121081233025, Final Batch Loss: 0.04583447799086571\n",
      "Epoch 3538, Loss: 0.134799562394619, Final Batch Loss: 0.0701267197728157\n",
      "Epoch 3539, Loss: 0.11975019052624702, Final Batch Loss: 0.05664186552166939\n",
      "Epoch 3540, Loss: 0.09064165502786636, Final Batch Loss: 0.029481399804353714\n",
      "Epoch 3541, Loss: 0.1145271509885788, Final Batch Loss: 0.0629100501537323\n",
      "Epoch 3542, Loss: 0.08649707958102226, Final Batch Loss: 0.03182252123951912\n",
      "Epoch 3543, Loss: 0.16220279783010483, Final Batch Loss: 0.08951911330223083\n",
      "Epoch 3544, Loss: 0.1763613298535347, Final Batch Loss: 0.07196302711963654\n",
      "Epoch 3545, Loss: 0.12824680283665657, Final Batch Loss: 0.058148499578237534\n",
      "Epoch 3546, Loss: 0.09783993102610111, Final Batch Loss: 0.023934589698910713\n",
      "Epoch 3547, Loss: 0.12429660931229591, Final Batch Loss: 0.06536058336496353\n",
      "Epoch 3548, Loss: 0.12047051265835762, Final Batch Loss: 0.051173243671655655\n",
      "Epoch 3549, Loss: 0.09134400263428688, Final Batch Loss: 0.03146312013268471\n",
      "Epoch 3550, Loss: 0.1596040427684784, Final Batch Loss: 0.06570513546466827\n",
      "Epoch 3551, Loss: 0.13206304982304573, Final Batch Loss: 0.08181990683078766\n",
      "Epoch 3552, Loss: 0.11855870112776756, Final Batch Loss: 0.05233464762568474\n",
      "Epoch 3553, Loss: 0.07319876737892628, Final Batch Loss: 0.029722070321440697\n",
      "Epoch 3554, Loss: 0.09401555731892586, Final Batch Loss: 0.05742330104112625\n",
      "Epoch 3555, Loss: 0.12028955295681953, Final Batch Loss: 0.060609400272369385\n",
      "Epoch 3556, Loss: 0.0923139825463295, Final Batch Loss: 0.049181919544935226\n",
      "Epoch 3557, Loss: 0.10631483793258667, Final Batch Loss: 0.05749120935797691\n",
      "Epoch 3558, Loss: 0.16820848360657692, Final Batch Loss: 0.12576915323734283\n",
      "Epoch 3559, Loss: 0.16717352718114853, Final Batch Loss: 0.07749404013156891\n",
      "Epoch 3560, Loss: 0.1395997405052185, Final Batch Loss: 0.04639565199613571\n",
      "Epoch 3561, Loss: 0.13256745412945747, Final Batch Loss: 0.07837614417076111\n",
      "Epoch 3562, Loss: 0.11071198433637619, Final Batch Loss: 0.04468988627195358\n",
      "Epoch 3563, Loss: 0.10600355267524719, Final Batch Loss: 0.03388602286577225\n",
      "Epoch 3564, Loss: 0.09410401806235313, Final Batch Loss: 0.04301925003528595\n",
      "Epoch 3565, Loss: 0.09781377576291561, Final Batch Loss: 0.02353661321103573\n",
      "Epoch 3566, Loss: 0.10775230452418327, Final Batch Loss: 0.0701102688908577\n",
      "Epoch 3567, Loss: 0.1063481792807579, Final Batch Loss: 0.06239567697048187\n",
      "Epoch 3568, Loss: 0.14676547423005104, Final Batch Loss: 0.03704163804650307\n",
      "Epoch 3569, Loss: 0.10889391973614693, Final Batch Loss: 0.061334893107414246\n",
      "Epoch 3570, Loss: 0.11036242544651031, Final Batch Loss: 0.057093698531389236\n",
      "Epoch 3571, Loss: 0.1679903343319893, Final Batch Loss: 0.08086525648832321\n",
      "Epoch 3572, Loss: 0.06828627176582813, Final Batch Loss: 0.0382549874484539\n",
      "Epoch 3573, Loss: 0.1568657085299492, Final Batch Loss: 0.0839972272515297\n",
      "Epoch 3574, Loss: 0.14062101021409035, Final Batch Loss: 0.04050910100340843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3575, Loss: 0.1442783921957016, Final Batch Loss: 0.06798035651445389\n",
      "Epoch 3576, Loss: 0.13205163925886154, Final Batch Loss: 0.08302829414606094\n",
      "Epoch 3577, Loss: 0.12160845100879669, Final Batch Loss: 0.04587052762508392\n",
      "Epoch 3578, Loss: 0.17737144976854324, Final Batch Loss: 0.07465826719999313\n",
      "Epoch 3579, Loss: 0.14061876386404037, Final Batch Loss: 0.08805341273546219\n",
      "Epoch 3580, Loss: 0.11693049222230911, Final Batch Loss: 0.06197427213191986\n",
      "Epoch 3581, Loss: 0.13068342953920364, Final Batch Loss: 0.06277921795845032\n",
      "Epoch 3582, Loss: 0.0910528190433979, Final Batch Loss: 0.03743164986371994\n",
      "Epoch 3583, Loss: 0.263642355799675, Final Batch Loss: 0.13252659142017365\n",
      "Epoch 3584, Loss: 0.13124136999249458, Final Batch Loss: 0.07282765954732895\n",
      "Epoch 3585, Loss: 0.14038943499326706, Final Batch Loss: 0.06058957427740097\n",
      "Epoch 3586, Loss: 0.11091967672109604, Final Batch Loss: 0.03732103109359741\n",
      "Epoch 3587, Loss: 0.07810726761817932, Final Batch Loss: 0.028921905905008316\n",
      "Epoch 3588, Loss: 0.11461977288126945, Final Batch Loss: 0.08114795386791229\n",
      "Epoch 3589, Loss: 0.15213855728507042, Final Batch Loss: 0.05321138724684715\n",
      "Epoch 3590, Loss: 0.12321192771196365, Final Batch Loss: 0.08775362372398376\n",
      "Epoch 3591, Loss: 0.15745283663272858, Final Batch Loss: 0.10175161808729172\n",
      "Epoch 3592, Loss: 0.0976068302989006, Final Batch Loss: 0.04734645038843155\n",
      "Epoch 3593, Loss: 0.10048247873783112, Final Batch Loss: 0.053447525948286057\n",
      "Epoch 3594, Loss: 0.15452682599425316, Final Batch Loss: 0.0546569786965847\n",
      "Epoch 3595, Loss: 0.12175318598747253, Final Batch Loss: 0.06399915367364883\n",
      "Epoch 3596, Loss: 0.16954578459262848, Final Batch Loss: 0.06244073808193207\n",
      "Epoch 3597, Loss: 0.11232002824544907, Final Batch Loss: 0.05464698374271393\n",
      "Epoch 3598, Loss: 0.13518871366977692, Final Batch Loss: 0.07195326685905457\n",
      "Epoch 3599, Loss: 0.2052275538444519, Final Batch Loss: 0.1264321506023407\n",
      "Epoch 3600, Loss: 0.13347914442420006, Final Batch Loss: 0.06107836589217186\n",
      "Epoch 3601, Loss: 0.07478593103587627, Final Batch Loss: 0.028984123840928078\n",
      "Epoch 3602, Loss: 0.08310746680945158, Final Batch Loss: 0.01420245599001646\n",
      "Epoch 3603, Loss: 0.17468544095754623, Final Batch Loss: 0.07596983015537262\n",
      "Epoch 3604, Loss: 0.13684367015957832, Final Batch Loss: 0.08274564146995544\n",
      "Epoch 3605, Loss: 0.1232866421341896, Final Batch Loss: 0.05843137949705124\n",
      "Epoch 3606, Loss: 0.08363884314894676, Final Batch Loss: 0.03516112640500069\n",
      "Epoch 3607, Loss: 0.14964528381824493, Final Batch Loss: 0.07271610200405121\n",
      "Epoch 3608, Loss: 0.08295618183910847, Final Batch Loss: 0.05185073986649513\n",
      "Epoch 3609, Loss: 0.11233152449131012, Final Batch Loss: 0.07858315855264664\n",
      "Epoch 3610, Loss: 0.08662104606628418, Final Batch Loss: 0.04821506515145302\n",
      "Epoch 3611, Loss: 0.09038839489221573, Final Batch Loss: 0.04533950611948967\n",
      "Epoch 3612, Loss: 0.10172311589121819, Final Batch Loss: 0.06626696139574051\n",
      "Epoch 3613, Loss: 0.12048941105604172, Final Batch Loss: 0.042037658393383026\n",
      "Epoch 3614, Loss: 0.11304695159196854, Final Batch Loss: 0.06170118227601051\n",
      "Epoch 3615, Loss: 0.09578143432736397, Final Batch Loss: 0.06030100956559181\n",
      "Epoch 3616, Loss: 0.11089049279689789, Final Batch Loss: 0.045968882739543915\n",
      "Epoch 3617, Loss: 0.09918994456529617, Final Batch Loss: 0.05953438580036163\n",
      "Epoch 3618, Loss: 0.10392290353775024, Final Batch Loss: 0.05446204915642738\n",
      "Epoch 3619, Loss: 0.09369298443198204, Final Batch Loss: 0.049697816371917725\n",
      "Epoch 3620, Loss: 0.08528796210885048, Final Batch Loss: 0.03699612244963646\n",
      "Epoch 3621, Loss: 0.13470177352428436, Final Batch Loss: 0.05091007053852081\n",
      "Epoch 3622, Loss: 0.1047133058309555, Final Batch Loss: 0.05374553054571152\n",
      "Epoch 3623, Loss: 0.13804346323013306, Final Batch Loss: 0.08228247612714767\n",
      "Epoch 3624, Loss: 0.14974835142493248, Final Batch Loss: 0.04454335942864418\n",
      "Epoch 3625, Loss: 0.14436624571681023, Final Batch Loss: 0.05715474858880043\n",
      "Epoch 3626, Loss: 0.168901726603508, Final Batch Loss: 0.06559527665376663\n",
      "Epoch 3627, Loss: 0.117830790579319, Final Batch Loss: 0.04893757402896881\n",
      "Epoch 3628, Loss: 0.13068387284874916, Final Batch Loss: 0.03686315938830376\n",
      "Epoch 3629, Loss: 0.10920724645256996, Final Batch Loss: 0.05855265632271767\n",
      "Epoch 3630, Loss: 0.13562120869755745, Final Batch Loss: 0.05397972837090492\n",
      "Epoch 3631, Loss: 0.10506663098931313, Final Batch Loss: 0.05792275443673134\n",
      "Epoch 3632, Loss: 0.13452742621302605, Final Batch Loss: 0.07588841021060944\n",
      "Epoch 3633, Loss: 0.09544496238231659, Final Batch Loss: 0.04320482537150383\n",
      "Epoch 3634, Loss: 0.1238550916314125, Final Batch Loss: 0.06253242492675781\n",
      "Epoch 3635, Loss: 0.10579755902290344, Final Batch Loss: 0.0670316070318222\n",
      "Epoch 3636, Loss: 0.11646286025643349, Final Batch Loss: 0.046851638704538345\n",
      "Epoch 3637, Loss: 0.1874123215675354, Final Batch Loss: 0.07726819813251495\n",
      "Epoch 3638, Loss: 0.09966905415058136, Final Batch Loss: 0.03484965115785599\n",
      "Epoch 3639, Loss: 0.08867844194173813, Final Batch Loss: 0.030725128948688507\n",
      "Epoch 3640, Loss: 0.17178012430667877, Final Batch Loss: 0.06320006400346756\n",
      "Epoch 3641, Loss: 0.11706645414233208, Final Batch Loss: 0.054853394627571106\n",
      "Epoch 3642, Loss: 0.17997300252318382, Final Batch Loss: 0.12918411195278168\n",
      "Epoch 3643, Loss: 0.12195302546024323, Final Batch Loss: 0.07642869651317596\n",
      "Epoch 3644, Loss: 0.12187992408871651, Final Batch Loss: 0.07218776643276215\n",
      "Epoch 3645, Loss: 0.08441388979554176, Final Batch Loss: 0.03726557642221451\n",
      "Epoch 3646, Loss: 0.13242093101143837, Final Batch Loss: 0.09188593924045563\n",
      "Epoch 3647, Loss: 0.12237212061882019, Final Batch Loss: 0.04082215577363968\n",
      "Epoch 3648, Loss: 0.10676905140280724, Final Batch Loss: 0.06507469713687897\n",
      "Epoch 3649, Loss: 0.11021952331066132, Final Batch Loss: 0.0565679669380188\n",
      "Epoch 3650, Loss: 0.1246679350733757, Final Batch Loss: 0.05713125318288803\n",
      "Epoch 3651, Loss: 0.14803703129291534, Final Batch Loss: 0.10163190960884094\n",
      "Epoch 3652, Loss: 0.08645088970661163, Final Batch Loss: 0.05340993031859398\n",
      "Epoch 3653, Loss: 0.12822852283716202, Final Batch Loss: 0.0642176941037178\n",
      "Epoch 3654, Loss: 0.11348829790949821, Final Batch Loss: 0.07694459706544876\n",
      "Epoch 3655, Loss: 0.11260803043842316, Final Batch Loss: 0.054924506694078445\n",
      "Epoch 3656, Loss: 0.07437924481928349, Final Batch Loss: 0.02680640108883381\n",
      "Epoch 3657, Loss: 0.11415711417794228, Final Batch Loss: 0.03896080330014229\n",
      "Epoch 3658, Loss: 0.1271396540105343, Final Batch Loss: 0.08443047851324081\n",
      "Epoch 3659, Loss: 0.11530883237719536, Final Batch Loss: 0.0655125305056572\n",
      "Epoch 3660, Loss: 0.08851855434477329, Final Batch Loss: 0.05931490287184715\n",
      "Epoch 3661, Loss: 0.08210733160376549, Final Batch Loss: 0.05325793847441673\n",
      "Epoch 3662, Loss: 0.09600650332868099, Final Batch Loss: 0.06851992756128311\n",
      "Epoch 3663, Loss: 0.10898373648524284, Final Batch Loss: 0.05450652912259102\n",
      "Epoch 3664, Loss: 0.08020699955523014, Final Batch Loss: 0.030712896957993507\n",
      "Epoch 3665, Loss: 0.17070794478058815, Final Batch Loss: 0.12568528950214386\n",
      "Epoch 3666, Loss: 0.11756707355380058, Final Batch Loss: 0.03762431815266609\n",
      "Epoch 3667, Loss: 0.12277805805206299, Final Batch Loss: 0.09553954005241394\n",
      "Epoch 3668, Loss: 0.09883036464452744, Final Batch Loss: 0.03530879318714142\n",
      "Epoch 3669, Loss: 0.17559880018234253, Final Batch Loss: 0.12182709574699402\n",
      "Epoch 3670, Loss: 0.11116231232881546, Final Batch Loss: 0.03748178482055664\n",
      "Epoch 3671, Loss: 0.08732342720031738, Final Batch Loss: 0.0398152731359005\n",
      "Epoch 3672, Loss: 0.12265923246741295, Final Batch Loss: 0.05047638341784477\n",
      "Epoch 3673, Loss: 0.18076546490192413, Final Batch Loss: 0.08255449682474136\n",
      "Epoch 3674, Loss: 0.13628152012825012, Final Batch Loss: 0.06945013254880905\n",
      "Epoch 3675, Loss: 0.16962626948952675, Final Batch Loss: 0.055600013583898544\n",
      "Epoch 3676, Loss: 0.12680991366505623, Final Batch Loss: 0.07589258998632431\n",
      "Epoch 3677, Loss: 0.0906781330704689, Final Batch Loss: 0.032894156873226166\n",
      "Epoch 3678, Loss: 0.13441234827041626, Final Batch Loss: 0.09443586319684982\n",
      "Epoch 3679, Loss: 0.16592223942279816, Final Batch Loss: 0.09107694029808044\n",
      "Epoch 3680, Loss: 0.14556195214390755, Final Batch Loss: 0.05668977275490761\n",
      "Epoch 3681, Loss: 0.11921432986855507, Final Batch Loss: 0.06412669271230698\n",
      "Epoch 3682, Loss: 0.13404540345072746, Final Batch Loss: 0.07410073280334473\n",
      "Epoch 3683, Loss: 0.14108824729919434, Final Batch Loss: 0.07641102373600006\n",
      "Epoch 3684, Loss: 0.17053968831896782, Final Batch Loss: 0.061820369213819504\n",
      "Epoch 3685, Loss: 0.1347774937748909, Final Batch Loss: 0.07200084626674652\n",
      "Epoch 3686, Loss: 0.1293422318994999, Final Batch Loss: 0.06014900282025337\n",
      "Epoch 3687, Loss: 0.09213535860180855, Final Batch Loss: 0.05057523027062416\n",
      "Epoch 3688, Loss: 0.07112167589366436, Final Batch Loss: 0.018657544627785683\n",
      "Epoch 3689, Loss: 0.10614707693457603, Final Batch Loss: 0.0628829225897789\n",
      "Epoch 3690, Loss: 0.07371510937809944, Final Batch Loss: 0.033393148332834244\n",
      "Epoch 3691, Loss: 0.12503406777977943, Final Batch Loss: 0.06452314555644989\n",
      "Epoch 3692, Loss: 0.07371173053979874, Final Batch Loss: 0.03914203122258186\n",
      "Epoch 3693, Loss: 0.08373217284679413, Final Batch Loss: 0.051847461611032486\n",
      "Epoch 3694, Loss: 0.16554417833685875, Final Batch Loss: 0.11136066168546677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3695, Loss: 0.11567756906151772, Final Batch Loss: 0.06989797949790955\n",
      "Epoch 3696, Loss: 0.1571357399225235, Final Batch Loss: 0.058555081486701965\n",
      "Epoch 3697, Loss: 0.10467898845672607, Final Batch Loss: 0.05563940480351448\n",
      "Epoch 3698, Loss: 0.13265452533960342, Final Batch Loss: 0.054671868681907654\n",
      "Epoch 3699, Loss: 0.13149121403694153, Final Batch Loss: 0.07385000586509705\n",
      "Epoch 3700, Loss: 0.08859038725495338, Final Batch Loss: 0.03520467132329941\n",
      "Epoch 3701, Loss: 0.06004978157579899, Final Batch Loss: 0.022055169567465782\n",
      "Epoch 3702, Loss: 0.09262307360768318, Final Batch Loss: 0.04116198793053627\n",
      "Epoch 3703, Loss: 0.11404998227953911, Final Batch Loss: 0.060648515820503235\n",
      "Epoch 3704, Loss: 0.0977059155702591, Final Batch Loss: 0.049394767731428146\n",
      "Epoch 3705, Loss: 0.11337713524699211, Final Batch Loss: 0.06830909848213196\n",
      "Epoch 3706, Loss: 0.09002832695841789, Final Batch Loss: 0.057096462696790695\n",
      "Epoch 3707, Loss: 0.09997506812214851, Final Batch Loss: 0.03377125784754753\n",
      "Epoch 3708, Loss: 0.13367680087685585, Final Batch Loss: 0.03975323960185051\n",
      "Epoch 3709, Loss: 0.21541325002908707, Final Batch Loss: 0.12043881416320801\n",
      "Epoch 3710, Loss: 0.12438265606760979, Final Batch Loss: 0.055471476167440414\n",
      "Epoch 3711, Loss: 0.09846597909927368, Final Batch Loss: 0.06314624845981598\n",
      "Epoch 3712, Loss: 0.08586988970637321, Final Batch Loss: 0.03490697219967842\n",
      "Epoch 3713, Loss: 0.1705412119626999, Final Batch Loss: 0.09522383660078049\n",
      "Epoch 3714, Loss: 0.15462112054228783, Final Batch Loss: 0.05585357919335365\n",
      "Epoch 3715, Loss: 0.08429281786084175, Final Batch Loss: 0.03561658039689064\n",
      "Epoch 3716, Loss: 0.1088334172964096, Final Batch Loss: 0.030471257865428925\n",
      "Epoch 3717, Loss: 0.1211259737610817, Final Batch Loss: 0.06505066156387329\n",
      "Epoch 3718, Loss: 0.14131926372647285, Final Batch Loss: 0.10611411929130554\n",
      "Epoch 3719, Loss: 0.16260934621095657, Final Batch Loss: 0.08711583912372589\n",
      "Epoch 3720, Loss: 0.1410207375884056, Final Batch Loss: 0.046234600245952606\n",
      "Epoch 3721, Loss: 0.14455607160925865, Final Batch Loss: 0.047984529286623\n",
      "Epoch 3722, Loss: 0.13087450712919235, Final Batch Loss: 0.07232234627008438\n",
      "Epoch 3723, Loss: 0.09021827951073647, Final Batch Loss: 0.04242023453116417\n",
      "Epoch 3724, Loss: 0.15420805290341377, Final Batch Loss: 0.11357363313436508\n",
      "Epoch 3725, Loss: 0.11917825043201447, Final Batch Loss: 0.04512220621109009\n",
      "Epoch 3726, Loss: 0.10815229639410973, Final Batch Loss: 0.07532051205635071\n",
      "Epoch 3727, Loss: 0.11165856197476387, Final Batch Loss: 0.03929433599114418\n",
      "Epoch 3728, Loss: 0.23895765841007233, Final Batch Loss: 0.12881730496883392\n",
      "Epoch 3729, Loss: 0.059854812920093536, Final Batch Loss: 0.028863031417131424\n",
      "Epoch 3730, Loss: 0.09727808088064194, Final Batch Loss: 0.036981645971536636\n",
      "Epoch 3731, Loss: 0.12230731546878815, Final Batch Loss: 0.07307198643684387\n",
      "Epoch 3732, Loss: 0.18580850213766098, Final Batch Loss: 0.04586643725633621\n",
      "Epoch 3733, Loss: 0.1418510153889656, Final Batch Loss: 0.055912554264068604\n",
      "Epoch 3734, Loss: 0.16619357466697693, Final Batch Loss: 0.05994834005832672\n",
      "Epoch 3735, Loss: 0.0965815931558609, Final Batch Loss: 0.05622377619147301\n",
      "Epoch 3736, Loss: 0.15013272315263748, Final Batch Loss: 0.04102610796689987\n",
      "Epoch 3737, Loss: 0.101874228566885, Final Batch Loss: 0.04657783359289169\n",
      "Epoch 3738, Loss: 0.1430191956460476, Final Batch Loss: 0.054996322840452194\n",
      "Epoch 3739, Loss: 0.1790594533085823, Final Batch Loss: 0.10619265586137772\n",
      "Epoch 3740, Loss: 0.15602034330368042, Final Batch Loss: 0.09520015120506287\n",
      "Epoch 3741, Loss: 0.18783237412571907, Final Batch Loss: 0.0593317486345768\n",
      "Epoch 3742, Loss: 0.14149172604084015, Final Batch Loss: 0.06570246070623398\n",
      "Epoch 3743, Loss: 0.14448897913098335, Final Batch Loss: 0.043159451335668564\n",
      "Epoch 3744, Loss: 0.13049978390336037, Final Batch Loss: 0.06160837039351463\n",
      "Epoch 3745, Loss: 0.143061101436615, Final Batch Loss: 0.07278517633676529\n",
      "Epoch 3746, Loss: 0.11349606513977051, Final Batch Loss: 0.05464078485965729\n",
      "Epoch 3747, Loss: 0.13468432426452637, Final Batch Loss: 0.06756571680307388\n",
      "Epoch 3748, Loss: 0.11502067372202873, Final Batch Loss: 0.05890369042754173\n",
      "Epoch 3749, Loss: 0.10097521916031837, Final Batch Loss: 0.06422360986471176\n",
      "Epoch 3750, Loss: 0.065852090716362, Final Batch Loss: 0.030536260455846786\n",
      "Epoch 3751, Loss: 0.1133718229830265, Final Batch Loss: 0.05368095636367798\n",
      "Epoch 3752, Loss: 0.06569520942866802, Final Batch Loss: 0.027467837557196617\n",
      "Epoch 3753, Loss: 0.08673286810517311, Final Batch Loss: 0.04129945486783981\n",
      "Epoch 3754, Loss: 0.14758389443159103, Final Batch Loss: 0.08078312873840332\n",
      "Epoch 3755, Loss: 0.10000969097018242, Final Batch Loss: 0.04017625004053116\n",
      "Epoch 3756, Loss: 0.13832764700055122, Final Batch Loss: 0.04665380343794823\n",
      "Epoch 3757, Loss: 0.12469552084803581, Final Batch Loss: 0.026035215705633163\n",
      "Epoch 3758, Loss: 0.12991410121321678, Final Batch Loss: 0.07151876389980316\n",
      "Epoch 3759, Loss: 0.08937712758779526, Final Batch Loss: 0.05240801349282265\n",
      "Epoch 3760, Loss: 0.12022697553038597, Final Batch Loss: 0.04875067248940468\n",
      "Epoch 3761, Loss: 0.12809320911765099, Final Batch Loss: 0.05147165432572365\n",
      "Epoch 3762, Loss: 0.1283884011209011, Final Batch Loss: 0.10187433660030365\n",
      "Epoch 3763, Loss: 0.10589764639735222, Final Batch Loss: 0.03295961394906044\n",
      "Epoch 3764, Loss: 0.12279291450977325, Final Batch Loss: 0.06631572544574738\n",
      "Epoch 3765, Loss: 0.11704090982675552, Final Batch Loss: 0.05681083723902702\n",
      "Epoch 3766, Loss: 0.20003240555524826, Final Batch Loss: 0.11638308316469193\n",
      "Epoch 3767, Loss: 0.09453372284770012, Final Batch Loss: 0.04829978570342064\n",
      "Epoch 3768, Loss: 0.11645383387804031, Final Batch Loss: 0.04682129621505737\n",
      "Epoch 3769, Loss: 0.08319005742669106, Final Batch Loss: 0.04090113565325737\n",
      "Epoch 3770, Loss: 0.07431997545063496, Final Batch Loss: 0.01937911845743656\n",
      "Epoch 3771, Loss: 0.18874043971300125, Final Batch Loss: 0.09038259088993073\n",
      "Epoch 3772, Loss: 0.1032441109418869, Final Batch Loss: 0.06022705137729645\n",
      "Epoch 3773, Loss: 0.18695276230573654, Final Batch Loss: 0.10327623039484024\n",
      "Epoch 3774, Loss: 0.0924335177987814, Final Batch Loss: 0.030701028183102608\n",
      "Epoch 3775, Loss: 0.15359602123498917, Final Batch Loss: 0.06985891610383987\n",
      "Epoch 3776, Loss: 0.188857926055789, Final Batch Loss: 0.026333538815379143\n",
      "Epoch 3777, Loss: 0.0927375964820385, Final Batch Loss: 0.041602972894907\n",
      "Epoch 3778, Loss: 0.2286374643445015, Final Batch Loss: 0.16569934785366058\n",
      "Epoch 3779, Loss: 0.12453601509332657, Final Batch Loss: 0.06282985955476761\n",
      "Epoch 3780, Loss: 0.13228079676628113, Final Batch Loss: 0.05744355171918869\n",
      "Epoch 3781, Loss: 0.11409513838589191, Final Batch Loss: 0.024764688685536385\n",
      "Epoch 3782, Loss: 0.10209580883383751, Final Batch Loss: 0.049043092876672745\n",
      "Epoch 3783, Loss: 0.13622687757015228, Final Batch Loss: 0.06290445476770401\n",
      "Epoch 3784, Loss: 0.11309635639190674, Final Batch Loss: 0.043340809643268585\n",
      "Epoch 3785, Loss: 0.08659344166517258, Final Batch Loss: 0.050544556230306625\n",
      "Epoch 3786, Loss: 0.11646033078432083, Final Batch Loss: 0.05139823257923126\n",
      "Epoch 3787, Loss: 0.14531728625297546, Final Batch Loss: 0.07195033133029938\n",
      "Epoch 3788, Loss: 0.15657033026218414, Final Batch Loss: 0.07058896124362946\n",
      "Epoch 3789, Loss: 0.09517228975892067, Final Batch Loss: 0.038781628012657166\n",
      "Epoch 3790, Loss: 0.11159643530845642, Final Batch Loss: 0.05989117547869682\n",
      "Epoch 3791, Loss: 0.12970668449997902, Final Batch Loss: 0.07764222472906113\n",
      "Epoch 3792, Loss: 0.10896402969956398, Final Batch Loss: 0.06649217009544373\n",
      "Epoch 3793, Loss: 0.15136820822954178, Final Batch Loss: 0.07352504134178162\n",
      "Epoch 3794, Loss: 0.16535182297229767, Final Batch Loss: 0.08241402357816696\n",
      "Epoch 3795, Loss: 0.12341997399926186, Final Batch Loss: 0.06727976351976395\n",
      "Epoch 3796, Loss: 0.08966675028204918, Final Batch Loss: 0.043303098529577255\n",
      "Epoch 3797, Loss: 0.248163640499115, Final Batch Loss: 0.11833791434764862\n",
      "Epoch 3798, Loss: 0.11110017821192741, Final Batch Loss: 0.061143092811107635\n",
      "Epoch 3799, Loss: 0.12954432889819145, Final Batch Loss: 0.07619139552116394\n",
      "Epoch 3800, Loss: 0.11528664454817772, Final Batch Loss: 0.07097015529870987\n",
      "Epoch 3801, Loss: 0.08679541572928429, Final Batch Loss: 0.032048337161540985\n",
      "Epoch 3802, Loss: 0.12476162612438202, Final Batch Loss: 0.08111034333705902\n",
      "Epoch 3803, Loss: 0.10394521430134773, Final Batch Loss: 0.06081235781311989\n",
      "Epoch 3804, Loss: 0.05522800423204899, Final Batch Loss: 0.02660486288368702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3805, Loss: 0.08094161748886108, Final Batch Loss: 0.034725967794656754\n",
      "Epoch 3806, Loss: 0.14799386262893677, Final Batch Loss: 0.07493636757135391\n",
      "Epoch 3807, Loss: 0.09195606783032417, Final Batch Loss: 0.0404278002679348\n",
      "Epoch 3808, Loss: 0.11152765899896622, Final Batch Loss: 0.07066651433706284\n",
      "Epoch 3809, Loss: 0.1282321996986866, Final Batch Loss: 0.060691941529512405\n",
      "Epoch 3810, Loss: 0.06825298257172108, Final Batch Loss: 0.0303217563778162\n",
      "Epoch 3811, Loss: 0.06446594931185246, Final Batch Loss: 0.03456081449985504\n",
      "Epoch 3812, Loss: 0.1303604170680046, Final Batch Loss: 0.07659311592578888\n",
      "Epoch 3813, Loss: 0.12590371817350388, Final Batch Loss: 0.07919764518737793\n",
      "Epoch 3814, Loss: 0.13996528834104538, Final Batch Loss: 0.08189542591571808\n",
      "Epoch 3815, Loss: 0.11974917352199554, Final Batch Loss: 0.04519076645374298\n",
      "Epoch 3816, Loss: 0.14800648391246796, Final Batch Loss: 0.07862129807472229\n",
      "Epoch 3817, Loss: 0.057794682681560516, Final Batch Loss: 0.022294558584690094\n",
      "Epoch 3818, Loss: 0.1251680888235569, Final Batch Loss: 0.0499856062233448\n",
      "Epoch 3819, Loss: 0.19308841228485107, Final Batch Loss: 0.09043522924184799\n",
      "Epoch 3820, Loss: 0.10028421506285667, Final Batch Loss: 0.04676790162920952\n",
      "Epoch 3821, Loss: 0.15843883901834488, Final Batch Loss: 0.10911492258310318\n",
      "Epoch 3822, Loss: 0.16882174834609032, Final Batch Loss: 0.054340261965990067\n",
      "Epoch 3823, Loss: 0.13853280246257782, Final Batch Loss: 0.0696866363286972\n",
      "Epoch 3824, Loss: 0.09733641892671585, Final Batch Loss: 0.03143487125635147\n",
      "Epoch 3825, Loss: 0.0903199166059494, Final Batch Loss: 0.06348404288291931\n",
      "Epoch 3826, Loss: 0.1751883178949356, Final Batch Loss: 0.09820985049009323\n",
      "Epoch 3827, Loss: 0.09595562517642975, Final Batch Loss: 0.05855901911854744\n",
      "Epoch 3828, Loss: 0.07306537218391895, Final Batch Loss: 0.019709685817360878\n",
      "Epoch 3829, Loss: 0.07448165118694305, Final Batch Loss: 0.042626719921827316\n",
      "Epoch 3830, Loss: 0.10609190911054611, Final Batch Loss: 0.06492292881011963\n",
      "Epoch 3831, Loss: 0.06405757553875446, Final Batch Loss: 0.027609912678599358\n",
      "Epoch 3832, Loss: 0.07740281149744987, Final Batch Loss: 0.022296380251646042\n",
      "Epoch 3833, Loss: 0.10861972719430923, Final Batch Loss: 0.07644876837730408\n",
      "Epoch 3834, Loss: 0.12898285686969757, Final Batch Loss: 0.040590859949588776\n",
      "Epoch 3835, Loss: 0.15379947051405907, Final Batch Loss: 0.09464948624372482\n",
      "Epoch 3836, Loss: 0.12067932263016701, Final Batch Loss: 0.0775931179523468\n",
      "Epoch 3837, Loss: 0.1357615888118744, Final Batch Loss: 0.08041799813508987\n",
      "Epoch 3838, Loss: 0.09093889407813549, Final Batch Loss: 0.029132744297385216\n",
      "Epoch 3839, Loss: 0.06839468702673912, Final Batch Loss: 0.026237402111291885\n",
      "Epoch 3840, Loss: 0.05544215999543667, Final Batch Loss: 0.03417179733514786\n",
      "Epoch 3841, Loss: 0.12226418405771255, Final Batch Loss: 0.0733586922287941\n",
      "Epoch 3842, Loss: 0.10620976239442825, Final Batch Loss: 0.03266692906618118\n",
      "Epoch 3843, Loss: 0.10406918451189995, Final Batch Loss: 0.03468063101172447\n",
      "Epoch 3844, Loss: 0.07495803385972977, Final Batch Loss: 0.0460900217294693\n",
      "Epoch 3845, Loss: 0.27883218973875046, Final Batch Loss: 0.2158980816602707\n",
      "Epoch 3846, Loss: 0.0819467194378376, Final Batch Loss: 0.06179191544651985\n",
      "Epoch 3847, Loss: 0.10938465967774391, Final Batch Loss: 0.04784131422638893\n",
      "Epoch 3848, Loss: 0.11018987745046616, Final Batch Loss: 0.0699915736913681\n",
      "Epoch 3849, Loss: 0.09474363923072815, Final Batch Loss: 0.053338777273893356\n",
      "Epoch 3850, Loss: 0.09944408759474754, Final Batch Loss: 0.05226806551218033\n",
      "Epoch 3851, Loss: 0.13765529543161392, Final Batch Loss: 0.05936376750469208\n",
      "Epoch 3852, Loss: 0.15796170383691788, Final Batch Loss: 0.08976524323225021\n",
      "Epoch 3853, Loss: 0.22496431320905685, Final Batch Loss: 0.09539731591939926\n",
      "Epoch 3854, Loss: 0.09782381728291512, Final Batch Loss: 0.04784274101257324\n",
      "Epoch 3855, Loss: 0.08910309709608555, Final Batch Loss: 0.030585503205657005\n",
      "Epoch 3856, Loss: 0.09563706070184708, Final Batch Loss: 0.05413166433572769\n",
      "Epoch 3857, Loss: 0.15227632224559784, Final Batch Loss: 0.06425179541110992\n",
      "Epoch 3858, Loss: 0.09714454412460327, Final Batch Loss: 0.0425928458571434\n",
      "Epoch 3859, Loss: 0.1170232854783535, Final Batch Loss: 0.06981419771909714\n",
      "Epoch 3860, Loss: 0.08423289656639099, Final Batch Loss: 0.03843541815876961\n",
      "Epoch 3861, Loss: 0.1280624270439148, Final Batch Loss: 0.061454348266124725\n",
      "Epoch 3862, Loss: 0.10842037200927734, Final Batch Loss: 0.050616227090358734\n",
      "Epoch 3863, Loss: 0.09566177800297737, Final Batch Loss: 0.03836759179830551\n",
      "Epoch 3864, Loss: 0.17616797983646393, Final Batch Loss: 0.1187879666686058\n",
      "Epoch 3865, Loss: 0.08265090547502041, Final Batch Loss: 0.026972493156790733\n",
      "Epoch 3866, Loss: 0.11729490011930466, Final Batch Loss: 0.06375204026699066\n",
      "Epoch 3867, Loss: 0.13054703548550606, Final Batch Loss: 0.05551835522055626\n",
      "Epoch 3868, Loss: 0.07273116055876017, Final Batch Loss: 0.013727759011089802\n",
      "Epoch 3869, Loss: 0.1389106847345829, Final Batch Loss: 0.04941577836871147\n",
      "Epoch 3870, Loss: 0.11093752086162567, Final Batch Loss: 0.06857731193304062\n",
      "Epoch 3871, Loss: 0.1609216295182705, Final Batch Loss: 0.05855340138077736\n",
      "Epoch 3872, Loss: 0.1363641358911991, Final Batch Loss: 0.05670240893959999\n",
      "Epoch 3873, Loss: 0.08250055834650993, Final Batch Loss: 0.034836240112781525\n",
      "Epoch 3874, Loss: 0.1622871682047844, Final Batch Loss: 0.05331897735595703\n",
      "Epoch 3875, Loss: 0.13922590017318726, Final Batch Loss: 0.09096561372280121\n",
      "Epoch 3876, Loss: 0.1121920645236969, Final Batch Loss: 0.06457064300775528\n",
      "Epoch 3877, Loss: 0.07676156610250473, Final Batch Loss: 0.03610038012266159\n",
      "Epoch 3878, Loss: 0.11749112978577614, Final Batch Loss: 0.05970532447099686\n",
      "Epoch 3879, Loss: 0.14017442986369133, Final Batch Loss: 0.08152171969413757\n",
      "Epoch 3880, Loss: 0.07061382010579109, Final Batch Loss: 0.03711444139480591\n",
      "Epoch 3881, Loss: 0.09082973375916481, Final Batch Loss: 0.03947843983769417\n",
      "Epoch 3882, Loss: 0.111255943775177, Final Batch Loss: 0.06439969688653946\n",
      "Epoch 3883, Loss: 0.11114055663347244, Final Batch Loss: 0.04019777476787567\n",
      "Epoch 3884, Loss: 0.07739478722214699, Final Batch Loss: 0.04972822964191437\n",
      "Epoch 3885, Loss: 0.11834671348333359, Final Batch Loss: 0.0775865688920021\n",
      "Epoch 3886, Loss: 0.0961274728178978, Final Batch Loss: 0.05263533815741539\n",
      "Epoch 3887, Loss: 0.12994049489498138, Final Batch Loss: 0.04359971731901169\n",
      "Epoch 3888, Loss: 0.1276378631591797, Final Batch Loss: 0.07709789276123047\n",
      "Epoch 3889, Loss: 0.12005356326699257, Final Batch Loss: 0.07235581427812576\n",
      "Epoch 3890, Loss: 0.11374428495764732, Final Batch Loss: 0.05680571123957634\n",
      "Epoch 3891, Loss: 0.09268826991319656, Final Batch Loss: 0.054863471537828445\n",
      "Epoch 3892, Loss: 0.09301768243312836, Final Batch Loss: 0.044666990637779236\n",
      "Epoch 3893, Loss: 0.12313443049788475, Final Batch Loss: 0.08165755122900009\n",
      "Epoch 3894, Loss: 0.08755247294902802, Final Batch Loss: 0.041226182132959366\n",
      "Epoch 3895, Loss: 0.1538819819688797, Final Batch Loss: 0.11085048317909241\n",
      "Epoch 3896, Loss: 0.06783841550350189, Final Batch Loss: 0.03649315610527992\n",
      "Epoch 3897, Loss: 0.11134505644440651, Final Batch Loss: 0.034270886331796646\n",
      "Epoch 3898, Loss: 0.10708104446530342, Final Batch Loss: 0.04469011351466179\n",
      "Epoch 3899, Loss: 0.06616394221782684, Final Batch Loss: 0.029669884592294693\n",
      "Epoch 3900, Loss: 0.07414066791534424, Final Batch Loss: 0.04974057897925377\n",
      "Epoch 3901, Loss: 0.06886713206768036, Final Batch Loss: 0.033963631838560104\n",
      "Epoch 3902, Loss: 0.11230980232357979, Final Batch Loss: 0.06026974692940712\n",
      "Epoch 3903, Loss: 0.18571245297789574, Final Batch Loss: 0.1292608082294464\n",
      "Epoch 3904, Loss: 0.0846983827650547, Final Batch Loss: 0.042972203344106674\n",
      "Epoch 3905, Loss: 0.0661731455475092, Final Batch Loss: 0.035348400473594666\n",
      "Epoch 3906, Loss: 0.10170380026102066, Final Batch Loss: 0.03343760967254639\n",
      "Epoch 3907, Loss: 0.0942150354385376, Final Batch Loss: 0.03477707877755165\n",
      "Epoch 3908, Loss: 0.1418985202908516, Final Batch Loss: 0.0743815153837204\n",
      "Epoch 3909, Loss: 0.09174652397632599, Final Batch Loss: 0.057221993803977966\n",
      "Epoch 3910, Loss: 0.07743191346526146, Final Batch Loss: 0.043556518852710724\n",
      "Epoch 3911, Loss: 0.07186166942119598, Final Batch Loss: 0.03877122700214386\n",
      "Epoch 3912, Loss: 0.13412928581237793, Final Batch Loss: 0.06707455217838287\n",
      "Epoch 3913, Loss: 0.09957782551646233, Final Batch Loss: 0.04290487989783287\n",
      "Epoch 3914, Loss: 0.09583306312561035, Final Batch Loss: 0.05381303280591965\n",
      "Epoch 3915, Loss: 0.1426304243505001, Final Batch Loss: 0.08507262170314789\n",
      "Epoch 3916, Loss: 0.06709329970180988, Final Batch Loss: 0.024195866659283638\n",
      "Epoch 3917, Loss: 0.18241626024246216, Final Batch Loss: 0.12037505954504013\n",
      "Epoch 3918, Loss: 0.11342727020382881, Final Batch Loss: 0.06640665233135223\n",
      "Epoch 3919, Loss: 0.09760700166225433, Final Batch Loss: 0.05330574885010719\n",
      "Epoch 3920, Loss: 0.09481364116072655, Final Batch Loss: 0.04826277121901512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3921, Loss: 0.1806565299630165, Final Batch Loss: 0.11469852179288864\n",
      "Epoch 3922, Loss: 0.0629323199391365, Final Batch Loss: 0.03972391411662102\n",
      "Epoch 3923, Loss: 0.10926748067140579, Final Batch Loss: 0.06389995664358139\n",
      "Epoch 3924, Loss: 0.13517047464847565, Final Batch Loss: 0.04953887313604355\n",
      "Epoch 3925, Loss: 0.17986083775758743, Final Batch Loss: 0.11677821725606918\n",
      "Epoch 3926, Loss: 0.06772374734282494, Final Batch Loss: 0.019699756056070328\n",
      "Epoch 3927, Loss: 0.14799588918685913, Final Batch Loss: 0.04877664893865585\n",
      "Epoch 3928, Loss: 0.08150293305516243, Final Batch Loss: 0.032603610306978226\n",
      "Epoch 3929, Loss: 0.08737313374876976, Final Batch Loss: 0.040671367198228836\n",
      "Epoch 3930, Loss: 0.08601899817585945, Final Batch Loss: 0.034937407821416855\n",
      "Epoch 3931, Loss: 0.10288331285119057, Final Batch Loss: 0.03320181742310524\n",
      "Epoch 3932, Loss: 0.09888934716582298, Final Batch Loss: 0.06503397971391678\n",
      "Epoch 3933, Loss: 0.11893799901008606, Final Batch Loss: 0.06124480813741684\n",
      "Epoch 3934, Loss: 0.09362789243459702, Final Batch Loss: 0.0418974906206131\n",
      "Epoch 3935, Loss: 0.07567628845572472, Final Batch Loss: 0.03425580635666847\n",
      "Epoch 3936, Loss: 0.07735027000308037, Final Batch Loss: 0.04322942718863487\n",
      "Epoch 3937, Loss: 0.0906401239335537, Final Batch Loss: 0.020239274948835373\n",
      "Epoch 3938, Loss: 0.10452992469072342, Final Batch Loss: 0.038046225905418396\n",
      "Epoch 3939, Loss: 0.08762943372130394, Final Batch Loss: 0.03772745281457901\n",
      "Epoch 3940, Loss: 0.12523296475410461, Final Batch Loss: 0.06264267861843109\n",
      "Epoch 3941, Loss: 0.099784255027771, Final Batch Loss: 0.06668029725551605\n",
      "Epoch 3942, Loss: 0.07604365050792694, Final Batch Loss: 0.023777220398187637\n",
      "Epoch 3943, Loss: 0.11976046860218048, Final Batch Loss: 0.038886167109012604\n",
      "Epoch 3944, Loss: 0.09562647342681885, Final Batch Loss: 0.04898666590452194\n",
      "Epoch 3945, Loss: 0.13148733973503113, Final Batch Loss: 0.051575519144535065\n",
      "Epoch 3946, Loss: 0.1501016840338707, Final Batch Loss: 0.0653550922870636\n",
      "Epoch 3947, Loss: 0.1085488349199295, Final Batch Loss: 0.04782241955399513\n",
      "Epoch 3948, Loss: 0.07093756645917892, Final Batch Loss: 0.028471611440181732\n",
      "Epoch 3949, Loss: 0.11549348384141922, Final Batch Loss: 0.07018058001995087\n",
      "Epoch 3950, Loss: 0.09802136570215225, Final Batch Loss: 0.060126207768917084\n",
      "Epoch 3951, Loss: 0.0714094527065754, Final Batch Loss: 0.03560922294855118\n",
      "Epoch 3952, Loss: 0.15707991644740105, Final Batch Loss: 0.09527047723531723\n",
      "Epoch 3953, Loss: 0.08807973563671112, Final Batch Loss: 0.037689290940761566\n",
      "Epoch 3954, Loss: 0.0932611133903265, Final Batch Loss: 0.06945333629846573\n",
      "Epoch 3955, Loss: 0.06960522569715977, Final Batch Loss: 0.0282855574041605\n",
      "Epoch 3956, Loss: 0.1225351057946682, Final Batch Loss: 0.04446006938815117\n",
      "Epoch 3957, Loss: 0.14414671808481216, Final Batch Loss: 0.07686910033226013\n",
      "Epoch 3958, Loss: 0.21702132746577263, Final Batch Loss: 0.17487385869026184\n",
      "Epoch 3959, Loss: 0.12945762649178505, Final Batch Loss: 0.08815160393714905\n",
      "Epoch 3960, Loss: 0.10656442493200302, Final Batch Loss: 0.04971194639801979\n",
      "Epoch 3961, Loss: 0.11471031978726387, Final Batch Loss: 0.04548778012394905\n",
      "Epoch 3962, Loss: 0.16243603080511093, Final Batch Loss: 0.04862123727798462\n",
      "Epoch 3963, Loss: 0.13548164442181587, Final Batch Loss: 0.0525658018887043\n",
      "Epoch 3964, Loss: 0.12651094980537891, Final Batch Loss: 0.029888229444622993\n",
      "Epoch 3965, Loss: 0.09092690795660019, Final Batch Loss: 0.028348810970783234\n",
      "Epoch 3966, Loss: 0.09435871988534927, Final Batch Loss: 0.05802594870328903\n",
      "Epoch 3967, Loss: 0.08463255874812603, Final Batch Loss: 0.02512454055249691\n",
      "Epoch 3968, Loss: 0.11659133434295654, Final Batch Loss: 0.05531303212046623\n",
      "Epoch 3969, Loss: 0.0817938819527626, Final Batch Loss: 0.036051154136657715\n",
      "Epoch 3970, Loss: 0.16827552765607834, Final Batch Loss: 0.09046436846256256\n",
      "Epoch 3971, Loss: 0.10621057078242302, Final Batch Loss: 0.06299970299005508\n",
      "Epoch 3972, Loss: 0.07229138538241386, Final Batch Loss: 0.03557378426194191\n",
      "Epoch 3973, Loss: 0.06743513606488705, Final Batch Loss: 0.028267642483115196\n",
      "Epoch 3974, Loss: 0.09539623558521271, Final Batch Loss: 0.027021944522857666\n",
      "Epoch 3975, Loss: 0.08585236221551895, Final Batch Loss: 0.042922306805849075\n",
      "Epoch 3976, Loss: 0.13029903545975685, Final Batch Loss: 0.07568203657865524\n",
      "Epoch 3977, Loss: 0.10635104775428772, Final Batch Loss: 0.07354749739170074\n",
      "Epoch 3978, Loss: 0.14322391897439957, Final Batch Loss: 0.06171274930238724\n",
      "Epoch 3979, Loss: 0.06257203407585621, Final Batch Loss: 0.0412246398627758\n",
      "Epoch 3980, Loss: 0.09685483947396278, Final Batch Loss: 0.05729810148477554\n",
      "Epoch 3981, Loss: 0.08834938704967499, Final Batch Loss: 0.05767102912068367\n",
      "Epoch 3982, Loss: 0.14875027909874916, Final Batch Loss: 0.059714529663324356\n",
      "Epoch 3983, Loss: 0.06884888373315334, Final Batch Loss: 0.020332036539912224\n",
      "Epoch 3984, Loss: 0.08578669279813766, Final Batch Loss: 0.04354304447770119\n",
      "Epoch 3985, Loss: 0.17571958526968956, Final Batch Loss: 0.049206774681806564\n",
      "Epoch 3986, Loss: 0.08044815436005592, Final Batch Loss: 0.03646247461438179\n",
      "Epoch 3987, Loss: 0.10695236176252365, Final Batch Loss: 0.06594506651163101\n",
      "Epoch 3988, Loss: 0.08966855704784393, Final Batch Loss: 0.04190971329808235\n",
      "Epoch 3989, Loss: 0.13528255000710487, Final Batch Loss: 0.08177065849304199\n",
      "Epoch 3990, Loss: 0.07263573817908764, Final Batch Loss: 0.01962713710963726\n",
      "Epoch 3991, Loss: 0.1006610244512558, Final Batch Loss: 0.06449197232723236\n",
      "Epoch 3992, Loss: 0.09392793662846088, Final Batch Loss: 0.06745345145463943\n",
      "Epoch 3993, Loss: 0.15212103724479675, Final Batch Loss: 0.06963857263326645\n",
      "Epoch 3994, Loss: 0.10047287866473198, Final Batch Loss: 0.03408605977892876\n",
      "Epoch 3995, Loss: 0.09265197813510895, Final Batch Loss: 0.03691554814577103\n",
      "Epoch 3996, Loss: 0.11143682152032852, Final Batch Loss: 0.07299701869487762\n",
      "Epoch 3997, Loss: 0.06922906450927258, Final Batch Loss: 0.025751100853085518\n",
      "Epoch 3998, Loss: 0.08903269842267036, Final Batch Loss: 0.03926147520542145\n",
      "Epoch 3999, Loss: 0.10269192606210709, Final Batch Loss: 0.06385613977909088\n",
      "Epoch 4000, Loss: 0.08344146981835365, Final Batch Loss: 0.05369175598025322\n",
      "Epoch 4001, Loss: 0.10385536029934883, Final Batch Loss: 0.04641449451446533\n",
      "Epoch 4002, Loss: 0.09796960279345512, Final Batch Loss: 0.05172548443078995\n",
      "Epoch 4003, Loss: 0.1250832863152027, Final Batch Loss: 0.05783851817250252\n",
      "Epoch 4004, Loss: 0.10783921554684639, Final Batch Loss: 0.04280490055680275\n",
      "Epoch 4005, Loss: 0.09318770468235016, Final Batch Loss: 0.04566580802202225\n",
      "Epoch 4006, Loss: 0.10711751505732536, Final Batch Loss: 0.05580214038491249\n",
      "Epoch 4007, Loss: 0.10349085927009583, Final Batch Loss: 0.04926769435405731\n",
      "Epoch 4008, Loss: 0.12992866337299347, Final Batch Loss: 0.052145831286907196\n",
      "Epoch 4009, Loss: 0.09382182732224464, Final Batch Loss: 0.04604335129261017\n",
      "Epoch 4010, Loss: 0.11671963706612587, Final Batch Loss: 0.07172954082489014\n",
      "Epoch 4011, Loss: 0.09591368585824966, Final Batch Loss: 0.04285623878240585\n",
      "Epoch 4012, Loss: 0.11250687390565872, Final Batch Loss: 0.05777270719408989\n",
      "Epoch 4013, Loss: 0.09861628338694572, Final Batch Loss: 0.05437368154525757\n",
      "Epoch 4014, Loss: 0.1060575507581234, Final Batch Loss: 0.06426718831062317\n",
      "Epoch 4015, Loss: 0.10138432309031487, Final Batch Loss: 0.03803880885243416\n",
      "Epoch 4016, Loss: 0.14627746120095253, Final Batch Loss: 0.09100044518709183\n",
      "Epoch 4017, Loss: 0.08623946830630302, Final Batch Loss: 0.03371571749448776\n",
      "Epoch 4018, Loss: 0.14169390127062798, Final Batch Loss: 0.09809402376413345\n",
      "Epoch 4019, Loss: 0.12087234482169151, Final Batch Loss: 0.07429833710193634\n",
      "Epoch 4020, Loss: 0.14480476826429367, Final Batch Loss: 0.0675126388669014\n",
      "Epoch 4021, Loss: 0.10386311635375023, Final Batch Loss: 0.05332321301102638\n",
      "Epoch 4022, Loss: 0.09832696244120598, Final Batch Loss: 0.03815148398280144\n",
      "Epoch 4023, Loss: 0.1700989082455635, Final Batch Loss: 0.07466994225978851\n",
      "Epoch 4024, Loss: 0.120209701359272, Final Batch Loss: 0.08109254390001297\n",
      "Epoch 4025, Loss: 0.11405098810791969, Final Batch Loss: 0.06354435533285141\n",
      "Epoch 4026, Loss: 0.16480478644371033, Final Batch Loss: 0.09752314537763596\n",
      "Epoch 4027, Loss: 0.0850021094083786, Final Batch Loss: 0.060067880898714066\n",
      "Epoch 4028, Loss: 0.08261694386601448, Final Batch Loss: 0.03588927537202835\n",
      "Epoch 4029, Loss: 0.07831453531980515, Final Batch Loss: 0.02384195104241371\n",
      "Epoch 4030, Loss: 0.07967840507626534, Final Batch Loss: 0.033511899411678314\n",
      "Epoch 4031, Loss: 0.06595605239272118, Final Batch Loss: 0.0413021594285965\n",
      "Epoch 4032, Loss: 0.09530554711818695, Final Batch Loss: 0.0383080318570137\n",
      "Epoch 4033, Loss: 0.09645819664001465, Final Batch Loss: 0.03632216528058052\n",
      "Epoch 4034, Loss: 0.18689756095409393, Final Batch Loss: 0.1273462325334549\n",
      "Epoch 4035, Loss: 0.12093117088079453, Final Batch Loss: 0.07964980602264404\n",
      "Epoch 4036, Loss: 0.12058031186461449, Final Batch Loss: 0.06969303637742996\n",
      "Epoch 4037, Loss: 0.08658245205879211, Final Batch Loss: 0.03832036629319191\n",
      "Epoch 4038, Loss: 0.12803463637828827, Final Batch Loss: 0.0579695850610733\n",
      "Epoch 4039, Loss: 0.15160363912582397, Final Batch Loss: 0.07690104842185974\n",
      "Epoch 4040, Loss: 0.13287343829870224, Final Batch Loss: 0.056210875511169434\n",
      "Epoch 4041, Loss: 0.15379192680120468, Final Batch Loss: 0.07349806278944016\n",
      "Epoch 4042, Loss: 0.11969617754220963, Final Batch Loss: 0.04310258477926254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4043, Loss: 0.08549483120441437, Final Batch Loss: 0.03372611850500107\n",
      "Epoch 4044, Loss: 0.10333365201950073, Final Batch Loss: 0.04530209302902222\n",
      "Epoch 4045, Loss: 0.08235601708292961, Final Batch Loss: 0.0365302599966526\n",
      "Epoch 4046, Loss: 0.10771259665489197, Final Batch Loss: 0.0819958746433258\n",
      "Epoch 4047, Loss: 0.08226770535111427, Final Batch Loss: 0.03897261247038841\n",
      "Epoch 4048, Loss: 0.07853025756776333, Final Batch Loss: 0.047928858548402786\n",
      "Epoch 4049, Loss: 0.06411752291023731, Final Batch Loss: 0.0415591336786747\n",
      "Epoch 4050, Loss: 0.10414331033825874, Final Batch Loss: 0.060144662857055664\n",
      "Epoch 4051, Loss: 0.12555595859885216, Final Batch Loss: 0.059801410883665085\n",
      "Epoch 4052, Loss: 0.13734273985028267, Final Batch Loss: 0.09917093813419342\n",
      "Epoch 4053, Loss: 0.17646628618240356, Final Batch Loss: 0.1041136160492897\n",
      "Epoch 4054, Loss: 0.03760994412004948, Final Batch Loss: 0.018829505890607834\n",
      "Epoch 4055, Loss: 0.14202721044421196, Final Batch Loss: 0.09085489809513092\n",
      "Epoch 4056, Loss: 0.08761432766914368, Final Batch Loss: 0.04907555505633354\n",
      "Epoch 4057, Loss: 0.09023752063512802, Final Batch Loss: 0.04251391813158989\n",
      "Epoch 4058, Loss: 0.19159714132547379, Final Batch Loss: 0.08857199549674988\n",
      "Epoch 4059, Loss: 0.1627759002149105, Final Batch Loss: 0.1224507987499237\n",
      "Epoch 4060, Loss: 0.10811103880405426, Final Batch Loss: 0.054958414286375046\n",
      "Epoch 4061, Loss: 0.13937725499272346, Final Batch Loss: 0.10318247228860855\n",
      "Epoch 4062, Loss: 0.11561303585767746, Final Batch Loss: 0.041219137609004974\n",
      "Epoch 4063, Loss: 0.08749246597290039, Final Batch Loss: 0.047366466373205185\n",
      "Epoch 4064, Loss: 0.09219472110271454, Final Batch Loss: 0.03636334836483002\n",
      "Epoch 4065, Loss: 0.11815149337053299, Final Batch Loss: 0.04229340702295303\n",
      "Epoch 4066, Loss: 0.11522799357771873, Final Batch Loss: 0.05430752411484718\n",
      "Epoch 4067, Loss: 0.07592962868511677, Final Batch Loss: 0.0531185008585453\n",
      "Epoch 4068, Loss: 0.08846387267112732, Final Batch Loss: 0.041693635284900665\n",
      "Epoch 4069, Loss: 0.08671599626541138, Final Batch Loss: 0.05605398491024971\n",
      "Epoch 4070, Loss: 0.140031099319458, Final Batch Loss: 0.09641174226999283\n",
      "Epoch 4071, Loss: 0.07184598594903946, Final Batch Loss: 0.041882243007421494\n",
      "Epoch 4072, Loss: 0.11657260358333588, Final Batch Loss: 0.04496607929468155\n",
      "Epoch 4073, Loss: 0.09209080412983894, Final Batch Loss: 0.05738243833184242\n",
      "Epoch 4074, Loss: 0.06086398661136627, Final Batch Loss: 0.03706268221139908\n",
      "Epoch 4075, Loss: 0.0895268302410841, Final Batch Loss: 0.02805010788142681\n",
      "Epoch 4076, Loss: 0.1057535894215107, Final Batch Loss: 0.04454876482486725\n",
      "Epoch 4077, Loss: 0.10514497384428978, Final Batch Loss: 0.06170867383480072\n",
      "Epoch 4078, Loss: 0.16895033046603203, Final Batch Loss: 0.12658514082431793\n",
      "Epoch 4079, Loss: 0.11421801149845123, Final Batch Loss: 0.051786527037620544\n",
      "Epoch 4080, Loss: 0.07148732803761959, Final Batch Loss: 0.04977104812860489\n",
      "Epoch 4081, Loss: 0.06579442322254181, Final Batch Loss: 0.04312434419989586\n",
      "Epoch 4082, Loss: 0.11716353893280029, Final Batch Loss: 0.047373853623867035\n",
      "Epoch 4083, Loss: 0.12221143767237663, Final Batch Loss: 0.08746621012687683\n",
      "Epoch 4084, Loss: 0.09309564530849457, Final Batch Loss: 0.036693282425403595\n",
      "Epoch 4085, Loss: 0.1388042215257883, Final Batch Loss: 0.02796710468828678\n",
      "Epoch 4086, Loss: 0.14308255910873413, Final Batch Loss: 0.0715370699763298\n",
      "Epoch 4087, Loss: 0.10073501616716385, Final Batch Loss: 0.03324417769908905\n",
      "Epoch 4088, Loss: 0.14930828660726547, Final Batch Loss: 0.07892764359712601\n",
      "Epoch 4089, Loss: 0.14415939897298813, Final Batch Loss: 0.11312542110681534\n",
      "Epoch 4090, Loss: 0.08372518047690392, Final Batch Loss: 0.03928057849407196\n",
      "Epoch 4091, Loss: 0.10106915980577469, Final Batch Loss: 0.0763261541724205\n",
      "Epoch 4092, Loss: 0.09110157936811447, Final Batch Loss: 0.03132576495409012\n",
      "Epoch 4093, Loss: 0.15638280659914017, Final Batch Loss: 0.08518247306346893\n",
      "Epoch 4094, Loss: 0.10190512239933014, Final Batch Loss: 0.05235552415251732\n",
      "Epoch 4095, Loss: 0.06351620517671108, Final Batch Loss: 0.04270114004611969\n",
      "Epoch 4096, Loss: 0.11567950993776321, Final Batch Loss: 0.06986716389656067\n",
      "Epoch 4097, Loss: 0.08533283695578575, Final Batch Loss: 0.0275055393576622\n",
      "Epoch 4098, Loss: 0.1407928690314293, Final Batch Loss: 0.09155969321727753\n",
      "Epoch 4099, Loss: 0.09686170145869255, Final Batch Loss: 0.050406575202941895\n",
      "Epoch 4100, Loss: 0.10315896943211555, Final Batch Loss: 0.03666302189230919\n",
      "Epoch 4101, Loss: 0.07700205966830254, Final Batch Loss: 0.034791070967912674\n",
      "Epoch 4102, Loss: 0.06480514816939831, Final Batch Loss: 0.038402073085308075\n",
      "Epoch 4103, Loss: 0.12391967698931694, Final Batch Loss: 0.08321361243724823\n",
      "Epoch 4104, Loss: 0.11249316483736038, Final Batch Loss: 0.07260210812091827\n",
      "Epoch 4105, Loss: 0.09243126213550568, Final Batch Loss: 0.06535252183675766\n",
      "Epoch 4106, Loss: 0.18367256224155426, Final Batch Loss: 0.07368231564760208\n",
      "Epoch 4107, Loss: 0.10692558065056801, Final Batch Loss: 0.03351053223013878\n",
      "Epoch 4108, Loss: 0.14583969861268997, Final Batch Loss: 0.059592507779598236\n",
      "Epoch 4109, Loss: 0.11220846325159073, Final Batch Loss: 0.03958086669445038\n",
      "Epoch 4110, Loss: 0.07412449643015862, Final Batch Loss: 0.03659851476550102\n",
      "Epoch 4111, Loss: 0.18287267535924911, Final Batch Loss: 0.052370794117450714\n",
      "Epoch 4112, Loss: 0.09959686174988747, Final Batch Loss: 0.043240271508693695\n",
      "Epoch 4113, Loss: 0.04790412448346615, Final Batch Loss: 0.02011091075837612\n",
      "Epoch 4114, Loss: 0.109916802495718, Final Batch Loss: 0.04448608681559563\n",
      "Epoch 4115, Loss: 0.12183600291609764, Final Batch Loss: 0.0755394697189331\n",
      "Epoch 4116, Loss: 0.14049340039491653, Final Batch Loss: 0.07316155731678009\n",
      "Epoch 4117, Loss: 0.1130230464041233, Final Batch Loss: 0.058652546256780624\n",
      "Epoch 4118, Loss: 0.14330676943063736, Final Batch Loss: 0.06907358020544052\n",
      "Epoch 4119, Loss: 0.08374637737870216, Final Batch Loss: 0.022734813392162323\n",
      "Epoch 4120, Loss: 0.10650505684316158, Final Batch Loss: 0.07819592207670212\n",
      "Epoch 4121, Loss: 0.08584504574537277, Final Batch Loss: 0.05876905471086502\n",
      "Epoch 4122, Loss: 0.10062173567712307, Final Batch Loss: 0.0755486860871315\n",
      "Epoch 4123, Loss: 0.08690296672284603, Final Batch Loss: 0.017186572775244713\n",
      "Epoch 4124, Loss: 0.12749522179365158, Final Batch Loss: 0.06594625860452652\n",
      "Epoch 4125, Loss: 0.1390513926744461, Final Batch Loss: 0.06473402678966522\n",
      "Epoch 4126, Loss: 0.07993746548891068, Final Batch Loss: 0.03766671568155289\n",
      "Epoch 4127, Loss: 0.07501943409442902, Final Batch Loss: 0.045901305973529816\n",
      "Epoch 4128, Loss: 0.1158466674387455, Final Batch Loss: 0.06130488961935043\n",
      "Epoch 4129, Loss: 0.12362248450517654, Final Batch Loss: 0.038451120257377625\n",
      "Epoch 4130, Loss: 0.09820456057786942, Final Batch Loss: 0.04025162756443024\n",
      "Epoch 4131, Loss: 0.057307629846036434, Final Batch Loss: 0.015427338890731335\n",
      "Epoch 4132, Loss: 0.1298028789460659, Final Batch Loss: 0.07715175300836563\n",
      "Epoch 4133, Loss: 0.14246035367250443, Final Batch Loss: 0.07623596489429474\n",
      "Epoch 4134, Loss: 0.08152321353554726, Final Batch Loss: 0.042413339018821716\n",
      "Epoch 4135, Loss: 0.1414550319314003, Final Batch Loss: 0.08096203953027725\n",
      "Epoch 4136, Loss: 0.1759280413389206, Final Batch Loss: 0.06370390206575394\n",
      "Epoch 4137, Loss: 0.0760236531496048, Final Batch Loss: 0.02966758981347084\n",
      "Epoch 4138, Loss: 0.05838454328477383, Final Batch Loss: 0.019530856981873512\n",
      "Epoch 4139, Loss: 0.14325778558850288, Final Batch Loss: 0.054873768240213394\n",
      "Epoch 4140, Loss: 0.11407658830285072, Final Batch Loss: 0.053119998425245285\n",
      "Epoch 4141, Loss: 0.09416930750012398, Final Batch Loss: 0.055486805737018585\n",
      "Epoch 4142, Loss: 0.06371815502643585, Final Batch Loss: 0.030267875641584396\n",
      "Epoch 4143, Loss: 0.09772978350520134, Final Batch Loss: 0.04096424579620361\n",
      "Epoch 4144, Loss: 0.13486528769135475, Final Batch Loss: 0.05855360999703407\n",
      "Epoch 4145, Loss: 0.08356216922402382, Final Batch Loss: 0.04283469542860985\n",
      "Epoch 4146, Loss: 0.0793517418205738, Final Batch Loss: 0.032786838710308075\n",
      "Epoch 4147, Loss: 0.07918144948780537, Final Batch Loss: 0.059420689940452576\n",
      "Epoch 4148, Loss: 0.13299383968114853, Final Batch Loss: 0.055911898612976074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4149, Loss: 0.05987933836877346, Final Batch Loss: 0.0160836149007082\n",
      "Epoch 4150, Loss: 0.07006430439651012, Final Batch Loss: 0.02547864057123661\n",
      "Epoch 4151, Loss: 0.08952654153108597, Final Batch Loss: 0.04508061334490776\n",
      "Epoch 4152, Loss: 0.09620722755789757, Final Batch Loss: 0.041954997926950455\n",
      "Epoch 4153, Loss: 0.0908710565418005, Final Batch Loss: 0.027809379622340202\n",
      "Epoch 4154, Loss: 0.0668832678347826, Final Batch Loss: 0.02944873832166195\n",
      "Epoch 4155, Loss: 0.10035736113786697, Final Batch Loss: 0.07384428381919861\n",
      "Epoch 4156, Loss: 0.12321412190794945, Final Batch Loss: 0.042057935148477554\n",
      "Epoch 4157, Loss: 0.10474575869739056, Final Batch Loss: 0.02558155171573162\n",
      "Epoch 4158, Loss: 0.09624844789505005, Final Batch Loss: 0.022534556686878204\n",
      "Epoch 4159, Loss: 0.07140866667032242, Final Batch Loss: 0.03476369380950928\n",
      "Epoch 4160, Loss: 0.10995286330580711, Final Batch Loss: 0.05758101865649223\n",
      "Epoch 4161, Loss: 0.12048862501978874, Final Batch Loss: 0.05374227091670036\n",
      "Epoch 4162, Loss: 0.07225763611495495, Final Batch Loss: 0.04188735783100128\n",
      "Epoch 4163, Loss: 0.10223094932734966, Final Batch Loss: 0.030520832166075706\n",
      "Epoch 4164, Loss: 0.09371552802622318, Final Batch Loss: 0.06418727338314056\n",
      "Epoch 4165, Loss: 0.10815858095884323, Final Batch Loss: 0.04345500469207764\n",
      "Epoch 4166, Loss: 0.08539564162492752, Final Batch Loss: 0.04959716275334358\n",
      "Epoch 4167, Loss: 0.07157584093511105, Final Batch Loss: 0.04231203719973564\n",
      "Epoch 4168, Loss: 0.08953903056681156, Final Batch Loss: 0.06502975523471832\n",
      "Epoch 4169, Loss: 0.11416739225387573, Final Batch Loss: 0.05527142435312271\n",
      "Epoch 4170, Loss: 0.0978207178413868, Final Batch Loss: 0.051050856709480286\n",
      "Epoch 4171, Loss: 0.08246692083775997, Final Batch Loss: 0.026121893897652626\n",
      "Epoch 4172, Loss: 0.0706371832638979, Final Batch Loss: 0.05269833281636238\n",
      "Epoch 4173, Loss: 0.05031768884509802, Final Batch Loss: 0.01467526238411665\n",
      "Epoch 4174, Loss: 0.09179281257092953, Final Batch Loss: 0.061527833342552185\n",
      "Epoch 4175, Loss: 0.08355023339390755, Final Batch Loss: 0.029961008578538895\n",
      "Epoch 4176, Loss: 0.06690401583909988, Final Batch Loss: 0.03765140473842621\n",
      "Epoch 4177, Loss: 0.18135156482458115, Final Batch Loss: 0.08040960878133774\n",
      "Epoch 4178, Loss: 0.1215986218303442, Final Batch Loss: 0.0931352898478508\n",
      "Epoch 4179, Loss: 0.11899654194712639, Final Batch Loss: 0.055231187492609024\n",
      "Epoch 4180, Loss: 0.1096569076180458, Final Batch Loss: 0.06400390714406967\n",
      "Epoch 4181, Loss: 0.08457401767373085, Final Batch Loss: 0.0354749970138073\n",
      "Epoch 4182, Loss: 0.14904292672872543, Final Batch Loss: 0.06335123628377914\n",
      "Epoch 4183, Loss: 0.10814965888857841, Final Batch Loss: 0.07620624452829361\n",
      "Epoch 4184, Loss: 0.12647595256567, Final Batch Loss: 0.09009040892124176\n",
      "Epoch 4185, Loss: 0.12017470598220825, Final Batch Loss: 0.06086263433098793\n",
      "Epoch 4186, Loss: 0.09283499792218208, Final Batch Loss: 0.04110759124159813\n",
      "Epoch 4187, Loss: 0.09265551902353764, Final Batch Loss: 0.027377130463719368\n",
      "Epoch 4188, Loss: 0.10690108686685562, Final Batch Loss: 0.03666354715824127\n",
      "Epoch 4189, Loss: 0.1360960155725479, Final Batch Loss: 0.07686188071966171\n",
      "Epoch 4190, Loss: 0.0890352874994278, Final Batch Loss: 0.04807558283209801\n",
      "Epoch 4191, Loss: 0.12933490797877312, Final Batch Loss: 0.06241820380091667\n",
      "Epoch 4192, Loss: 0.14022685587406158, Final Batch Loss: 0.07057396322488785\n",
      "Epoch 4193, Loss: 0.10495716705918312, Final Batch Loss: 0.06720919907093048\n",
      "Epoch 4194, Loss: 0.10579948127269745, Final Batch Loss: 0.052213527262210846\n",
      "Epoch 4195, Loss: 0.10165336355566978, Final Batch Loss: 0.058836568146944046\n",
      "Epoch 4196, Loss: 0.1032813098281622, Final Batch Loss: 0.02550959400832653\n",
      "Epoch 4197, Loss: 0.11586270853877068, Final Batch Loss: 0.07275533676147461\n",
      "Epoch 4198, Loss: 0.11581234261393547, Final Batch Loss: 0.06312016397714615\n",
      "Epoch 4199, Loss: 0.06077258102595806, Final Batch Loss: 0.027520330622792244\n",
      "Epoch 4200, Loss: 0.08713543601334095, Final Batch Loss: 0.061996594071388245\n",
      "Epoch 4201, Loss: 0.08803129196166992, Final Batch Loss: 0.04810933396220207\n",
      "Epoch 4202, Loss: 0.24377528578042984, Final Batch Loss: 0.13878895342350006\n",
      "Epoch 4203, Loss: 0.08379446156322956, Final Batch Loss: 0.030551264062523842\n",
      "Epoch 4204, Loss: 0.11052859202027321, Final Batch Loss: 0.07691875100135803\n",
      "Epoch 4205, Loss: 0.14725052192807198, Final Batch Loss: 0.08728287369012833\n",
      "Epoch 4206, Loss: 0.07016146741807461, Final Batch Loss: 0.025467468425631523\n",
      "Epoch 4207, Loss: 0.08331849053502083, Final Batch Loss: 0.04127708449959755\n",
      "Epoch 4208, Loss: 0.07364954240620136, Final Batch Loss: 0.04948016256093979\n",
      "Epoch 4209, Loss: 0.10746621713042259, Final Batch Loss: 0.05543868616223335\n",
      "Epoch 4210, Loss: 0.14098500832915306, Final Batch Loss: 0.050045523792505264\n",
      "Epoch 4211, Loss: 0.051271189004182816, Final Batch Loss: 0.0217757485806942\n",
      "Epoch 4212, Loss: 0.09431824460625648, Final Batch Loss: 0.05524904653429985\n",
      "Epoch 4213, Loss: 0.14255520701408386, Final Batch Loss: 0.06153572350740433\n",
      "Epoch 4214, Loss: 0.06923658028244972, Final Batch Loss: 0.042745817452669144\n",
      "Epoch 4215, Loss: 0.08302514255046844, Final Batch Loss: 0.04692681506276131\n",
      "Epoch 4216, Loss: 0.11156658083200455, Final Batch Loss: 0.07738165557384491\n",
      "Epoch 4217, Loss: 0.13309840112924576, Final Batch Loss: 0.08318276703357697\n",
      "Epoch 4218, Loss: 0.11089160107076168, Final Batch Loss: 0.08350124210119247\n",
      "Epoch 4219, Loss: 0.10210835188627243, Final Batch Loss: 0.04019339382648468\n",
      "Epoch 4220, Loss: 0.1279832199215889, Final Batch Loss: 0.03283413499593735\n",
      "Epoch 4221, Loss: 0.07187514379620552, Final Batch Loss: 0.026992760598659515\n",
      "Epoch 4222, Loss: 0.13031282648444176, Final Batch Loss: 0.0753163993358612\n",
      "Epoch 4223, Loss: 0.1219199001789093, Final Batch Loss: 0.07496742904186249\n",
      "Epoch 4224, Loss: 0.11586780473589897, Final Batch Loss: 0.05828657001256943\n",
      "Epoch 4225, Loss: 0.15167944505810738, Final Batch Loss: 0.09151999652385712\n",
      "Epoch 4226, Loss: 0.07837960869073868, Final Batch Loss: 0.019174743443727493\n",
      "Epoch 4227, Loss: 0.11141914129257202, Final Batch Loss: 0.05077701807022095\n",
      "Epoch 4228, Loss: 0.13013228215277195, Final Batch Loss: 0.027974190190434456\n",
      "Epoch 4229, Loss: 0.0982636995613575, Final Batch Loss: 0.03817625343799591\n",
      "Epoch 4230, Loss: 0.06704816222190857, Final Batch Loss: 0.01777580752968788\n",
      "Epoch 4231, Loss: 0.08744162693619728, Final Batch Loss: 0.04648379608988762\n",
      "Epoch 4232, Loss: 0.08371421322226524, Final Batch Loss: 0.04308072105050087\n",
      "Epoch 4233, Loss: 0.1465446874499321, Final Batch Loss: 0.07400354743003845\n",
      "Epoch 4234, Loss: 0.07410192489624023, Final Batch Loss: 0.03820514306426048\n",
      "Epoch 4235, Loss: 0.05215676501393318, Final Batch Loss: 0.02432466670870781\n",
      "Epoch 4236, Loss: 0.07827219739556313, Final Batch Loss: 0.053934384137392044\n",
      "Epoch 4237, Loss: 0.149917621165514, Final Batch Loss: 0.10569754242897034\n",
      "Epoch 4238, Loss: 0.1246364489197731, Final Batch Loss: 0.05296982079744339\n",
      "Epoch 4239, Loss: 0.08260097354650497, Final Batch Loss: 0.028596047312021255\n",
      "Epoch 4240, Loss: 0.13008572161197662, Final Batch Loss: 0.06304243952035904\n",
      "Epoch 4241, Loss: 0.10096209868788719, Final Batch Loss: 0.04885145276784897\n",
      "Epoch 4242, Loss: 0.12137239053845406, Final Batch Loss: 0.08297817409038544\n",
      "Epoch 4243, Loss: 0.13749698922038078, Final Batch Loss: 0.0852363258600235\n",
      "Epoch 4244, Loss: 0.08439026027917862, Final Batch Loss: 0.017133332788944244\n",
      "Epoch 4245, Loss: 0.11406100168824196, Final Batch Loss: 0.07223924249410629\n",
      "Epoch 4246, Loss: 0.12670890986919403, Final Batch Loss: 0.03666745126247406\n",
      "Epoch 4247, Loss: 0.07974574156105518, Final Batch Loss: 0.026041945442557335\n",
      "Epoch 4248, Loss: 0.10972662270069122, Final Batch Loss: 0.04393835365772247\n",
      "Epoch 4249, Loss: 0.11926703341305256, Final Batch Loss: 0.08873144537210464\n",
      "Epoch 4250, Loss: 0.15876435115933418, Final Batch Loss: 0.060337137430906296\n",
      "Epoch 4251, Loss: 0.07748155482113361, Final Batch Loss: 0.0520695336163044\n",
      "Epoch 4252, Loss: 0.065335001796484, Final Batch Loss: 0.03756379708647728\n",
      "Epoch 4253, Loss: 0.09430734254419804, Final Batch Loss: 0.0259969774633646\n",
      "Epoch 4254, Loss: 0.0899508036673069, Final Batch Loss: 0.05529662221670151\n",
      "Epoch 4255, Loss: 0.1573113426566124, Final Batch Loss: 0.0929582491517067\n",
      "Epoch 4256, Loss: 0.058831311762332916, Final Batch Loss: 0.03478420525789261\n",
      "Epoch 4257, Loss: 0.10243990272283554, Final Batch Loss: 0.05475065857172012\n",
      "Epoch 4258, Loss: 0.11461249366402626, Final Batch Loss: 0.0549389012157917\n",
      "Epoch 4259, Loss: 0.05129593051970005, Final Batch Loss: 0.02954639308154583\n",
      "Epoch 4260, Loss: 0.09966420009732246, Final Batch Loss: 0.05271686613559723\n",
      "Epoch 4261, Loss: 0.10173209756612778, Final Batch Loss: 0.03662373870611191\n",
      "Epoch 4262, Loss: 0.09866442158818245, Final Batch Loss: 0.05860098451375961\n",
      "Epoch 4263, Loss: 0.1108721625059843, Final Batch Loss: 0.03099348209798336\n",
      "Epoch 4264, Loss: 0.07422259077429771, Final Batch Loss: 0.040247127413749695\n",
      "Epoch 4265, Loss: 0.07920118980109692, Final Batch Loss: 0.0214348416775465\n",
      "Epoch 4266, Loss: 0.07863704115152359, Final Batch Loss: 0.011420980095863342\n",
      "Epoch 4267, Loss: 0.15224960446357727, Final Batch Loss: 0.08153517544269562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4268, Loss: 0.14415425807237625, Final Batch Loss: 0.0744442269206047\n",
      "Epoch 4269, Loss: 0.09210357815027237, Final Batch Loss: 0.053868528455495834\n",
      "Epoch 4270, Loss: 0.07220694050192833, Final Batch Loss: 0.04688103869557381\n",
      "Epoch 4271, Loss: 0.10128119587898254, Final Batch Loss: 0.06671501696109772\n",
      "Epoch 4272, Loss: 0.15766635537147522, Final Batch Loss: 0.05263219028711319\n",
      "Epoch 4273, Loss: 0.062175579369068146, Final Batch Loss: 0.04242907464504242\n",
      "Epoch 4274, Loss: 0.12761568278074265, Final Batch Loss: 0.046197935938835144\n",
      "Epoch 4275, Loss: 0.09145252406597137, Final Batch Loss: 0.04334692656993866\n",
      "Epoch 4276, Loss: 0.11462781578302383, Final Batch Loss: 0.08342106640338898\n",
      "Epoch 4277, Loss: 0.10324027761816978, Final Batch Loss: 0.048427220433950424\n",
      "Epoch 4278, Loss: 0.07528405636548996, Final Batch Loss: 0.046526942402124405\n",
      "Epoch 4279, Loss: 0.1044139452278614, Final Batch Loss: 0.05817272514104843\n",
      "Epoch 4280, Loss: 0.05082574486732483, Final Batch Loss: 0.02869902364909649\n",
      "Epoch 4281, Loss: 0.12704277411103249, Final Batch Loss: 0.07944359630346298\n",
      "Epoch 4282, Loss: 0.08443889394402504, Final Batch Loss: 0.05295863002538681\n",
      "Epoch 4283, Loss: 0.06349181104451418, Final Batch Loss: 0.05200811102986336\n",
      "Epoch 4284, Loss: 0.08834830299019814, Final Batch Loss: 0.0349164642393589\n",
      "Epoch 4285, Loss: 0.12692880257964134, Final Batch Loss: 0.05174608156085014\n",
      "Epoch 4286, Loss: 0.12634352222085, Final Batch Loss: 0.0607622005045414\n",
      "Epoch 4287, Loss: 0.10200152918696404, Final Batch Loss: 0.0527808852493763\n",
      "Epoch 4288, Loss: 0.12748432904481888, Final Batch Loss: 0.08019404858350754\n",
      "Epoch 4289, Loss: 0.07993797585368156, Final Batch Loss: 0.03394176438450813\n",
      "Epoch 4290, Loss: 0.10728849843144417, Final Batch Loss: 0.07260595262050629\n",
      "Epoch 4291, Loss: 0.12264037504792213, Final Batch Loss: 0.07297388464212418\n",
      "Epoch 4292, Loss: 0.06725364550948143, Final Batch Loss: 0.02791452780365944\n",
      "Epoch 4293, Loss: 0.1365109570324421, Final Batch Loss: 0.10031265765428543\n",
      "Epoch 4294, Loss: 0.09558606520295143, Final Batch Loss: 0.041794050484895706\n",
      "Epoch 4295, Loss: 0.0879164058715105, Final Batch Loss: 0.02004302479326725\n",
      "Epoch 4296, Loss: 0.09894088841974735, Final Batch Loss: 0.08465779572725296\n",
      "Epoch 4297, Loss: 0.15253791026771069, Final Batch Loss: 0.023656366392970085\n",
      "Epoch 4298, Loss: 0.08163577690720558, Final Batch Loss: 0.04195975512266159\n",
      "Epoch 4299, Loss: 0.16705946996808052, Final Batch Loss: 0.12154896557331085\n",
      "Epoch 4300, Loss: 0.13309093192219734, Final Batch Loss: 0.05727487429976463\n",
      "Epoch 4301, Loss: 0.09433380886912346, Final Batch Loss: 0.03690677881240845\n",
      "Epoch 4302, Loss: 0.15053854882717133, Final Batch Loss: 0.07885681837797165\n",
      "Epoch 4303, Loss: 0.07053679414093494, Final Batch Loss: 0.028950320556759834\n",
      "Epoch 4304, Loss: 0.07671692408621311, Final Batch Loss: 0.0229580570012331\n",
      "Epoch 4305, Loss: 0.07719485089182854, Final Batch Loss: 0.04255092889070511\n",
      "Epoch 4306, Loss: 0.11384127289056778, Final Batch Loss: 0.05070256441831589\n",
      "Epoch 4307, Loss: 0.11687801033258438, Final Batch Loss: 0.03431008756160736\n",
      "Epoch 4308, Loss: 0.07682432606816292, Final Batch Loss: 0.031080543994903564\n",
      "Epoch 4309, Loss: 0.06864259019494057, Final Batch Loss: 0.03426571190357208\n",
      "Epoch 4310, Loss: 0.09959804639220238, Final Batch Loss: 0.06411727517843246\n",
      "Epoch 4311, Loss: 0.10432223230600357, Final Batch Loss: 0.04800849035382271\n",
      "Epoch 4312, Loss: 0.114783626049757, Final Batch Loss: 0.06209111586213112\n",
      "Epoch 4313, Loss: 0.11222954466938972, Final Batch Loss: 0.07640985399484634\n",
      "Epoch 4314, Loss: 0.09525882825255394, Final Batch Loss: 0.04757583141326904\n",
      "Epoch 4315, Loss: 0.11826970800757408, Final Batch Loss: 0.06296616792678833\n",
      "Epoch 4316, Loss: 0.08518299087882042, Final Batch Loss: 0.04417157173156738\n",
      "Epoch 4317, Loss: 0.08954314514994621, Final Batch Loss: 0.039343468844890594\n",
      "Epoch 4318, Loss: 0.06514057144522667, Final Batch Loss: 0.03387762978672981\n",
      "Epoch 4319, Loss: 0.0842695590108633, Final Batch Loss: 0.029319116845726967\n",
      "Epoch 4320, Loss: 0.05131616070866585, Final Batch Loss: 0.03372429683804512\n",
      "Epoch 4321, Loss: 0.12027708441019058, Final Batch Loss: 0.06760308891534805\n",
      "Epoch 4322, Loss: 0.06060926429927349, Final Batch Loss: 0.020933518186211586\n",
      "Epoch 4323, Loss: 0.06503217481076717, Final Batch Loss: 0.040741126984357834\n",
      "Epoch 4324, Loss: 0.13146641850471497, Final Batch Loss: 0.045544080436229706\n",
      "Epoch 4325, Loss: 0.1255529411137104, Final Batch Loss: 0.07172580063343048\n",
      "Epoch 4326, Loss: 0.13385869935154915, Final Batch Loss: 0.09859120100736618\n",
      "Epoch 4327, Loss: 0.057110270485281944, Final Batch Loss: 0.019183015450835228\n",
      "Epoch 4328, Loss: 0.10846647620201111, Final Batch Loss: 0.0451221838593483\n",
      "Epoch 4329, Loss: 0.12271712720394135, Final Batch Loss: 0.044367022812366486\n",
      "Epoch 4330, Loss: 0.08727446757256985, Final Batch Loss: 0.06048880144953728\n",
      "Epoch 4331, Loss: 0.07366461493074894, Final Batch Loss: 0.025506390258669853\n",
      "Epoch 4332, Loss: 0.0609605498611927, Final Batch Loss: 0.03398866951465607\n",
      "Epoch 4333, Loss: 0.09348253533244133, Final Batch Loss: 0.03994132950901985\n",
      "Epoch 4334, Loss: 0.06613482721149921, Final Batch Loss: 0.026774989441037178\n",
      "Epoch 4335, Loss: 0.0981161966919899, Final Batch Loss: 0.03504908084869385\n",
      "Epoch 4336, Loss: 0.10422207787632942, Final Batch Loss: 0.06354017555713654\n",
      "Epoch 4337, Loss: 0.1024029403924942, Final Batch Loss: 0.05482170730829239\n",
      "Epoch 4338, Loss: 0.10813334211707115, Final Batch Loss: 0.07140015810728073\n",
      "Epoch 4339, Loss: 0.08937588334083557, Final Batch Loss: 0.04370981827378273\n",
      "Epoch 4340, Loss: 0.11767896637320518, Final Batch Loss: 0.0734473392367363\n",
      "Epoch 4341, Loss: 0.0831809975206852, Final Batch Loss: 0.040400467813014984\n",
      "Epoch 4342, Loss: 0.10111545771360397, Final Batch Loss: 0.04157961532473564\n",
      "Epoch 4343, Loss: 0.1121925450861454, Final Batch Loss: 0.04451305791735649\n",
      "Epoch 4344, Loss: 0.11904405429959297, Final Batch Loss: 0.06162507086992264\n",
      "Epoch 4345, Loss: 0.07442811504006386, Final Batch Loss: 0.03368188068270683\n",
      "Epoch 4346, Loss: 0.08021314814686775, Final Batch Loss: 0.0422549694776535\n",
      "Epoch 4347, Loss: 0.08173885196447372, Final Batch Loss: 0.03735620528459549\n",
      "Epoch 4348, Loss: 0.0980217270553112, Final Batch Loss: 0.04812087118625641\n",
      "Epoch 4349, Loss: 0.13275935873389244, Final Batch Loss: 0.02916976436972618\n",
      "Epoch 4350, Loss: 0.13729242980480194, Final Batch Loss: 0.06866252422332764\n",
      "Epoch 4351, Loss: 0.099556565284729, Final Batch Loss: 0.04137611389160156\n",
      "Epoch 4352, Loss: 0.13238272815942764, Final Batch Loss: 0.0822765901684761\n",
      "Epoch 4353, Loss: 0.07487332820892334, Final Batch Loss: 0.03559231013059616\n",
      "Epoch 4354, Loss: 0.08871366456151009, Final Batch Loss: 0.06100934371352196\n",
      "Epoch 4355, Loss: 0.18292567133903503, Final Batch Loss: 0.08704604208469391\n",
      "Epoch 4356, Loss: 0.09814578667283058, Final Batch Loss: 0.05219193920493126\n",
      "Epoch 4357, Loss: 0.09949339553713799, Final Batch Loss: 0.033077072352170944\n",
      "Epoch 4358, Loss: 0.189911637455225, Final Batch Loss: 0.060348886996507645\n",
      "Epoch 4359, Loss: 0.1187761202454567, Final Batch Loss: 0.04502786695957184\n",
      "Epoch 4360, Loss: 0.09471962973475456, Final Batch Loss: 0.04687996208667755\n",
      "Epoch 4361, Loss: 0.09833945333957672, Final Batch Loss: 0.033920884132385254\n",
      "Epoch 4362, Loss: 0.15577388182282448, Final Batch Loss: 0.049287017434835434\n",
      "Epoch 4363, Loss: 0.08290922828018665, Final Batch Loss: 0.05526827648282051\n",
      "Epoch 4364, Loss: 0.11190863698720932, Final Batch Loss: 0.04631897062063217\n",
      "Epoch 4365, Loss: 0.16344352066516876, Final Batch Loss: 0.08260182291269302\n",
      "Epoch 4366, Loss: 0.07644977048039436, Final Batch Loss: 0.03522982448339462\n",
      "Epoch 4367, Loss: 0.13932861387729645, Final Batch Loss: 0.10313107073307037\n",
      "Epoch 4368, Loss: 0.09946297854185104, Final Batch Loss: 0.03346157819032669\n",
      "Epoch 4369, Loss: 0.11428306996822357, Final Batch Loss: 0.035093799233436584\n",
      "Epoch 4370, Loss: 0.07649854943156242, Final Batch Loss: 0.03972935304045677\n",
      "Epoch 4371, Loss: 0.24390499666333199, Final Batch Loss: 0.20013520121574402\n",
      "Epoch 4372, Loss: 0.09768503159284592, Final Batch Loss: 0.04654180631041527\n",
      "Epoch 4373, Loss: 0.09024864621460438, Final Batch Loss: 0.02892596833407879\n",
      "Epoch 4374, Loss: 0.09312950074672699, Final Batch Loss: 0.02538028359413147\n",
      "Epoch 4375, Loss: 0.08565849624574184, Final Batch Loss: 0.029877016320824623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4376, Loss: 0.10769161209464073, Final Batch Loss: 0.05307311192154884\n",
      "Epoch 4377, Loss: 0.0753069780766964, Final Batch Loss: 0.03412961587309837\n",
      "Epoch 4378, Loss: 0.10594556853175163, Final Batch Loss: 0.05290830507874489\n",
      "Epoch 4379, Loss: 0.0930994562804699, Final Batch Loss: 0.035338159650564194\n",
      "Epoch 4380, Loss: 0.07923589088022709, Final Batch Loss: 0.03072105534374714\n",
      "Epoch 4381, Loss: 0.07520225644111633, Final Batch Loss: 0.0315408892929554\n",
      "Epoch 4382, Loss: 0.08001633360981941, Final Batch Loss: 0.045421648770570755\n",
      "Epoch 4383, Loss: 0.05947921425104141, Final Batch Loss: 0.025468070060014725\n",
      "Epoch 4384, Loss: 0.11237042024731636, Final Batch Loss: 0.0538971871137619\n",
      "Epoch 4385, Loss: 0.0965370163321495, Final Batch Loss: 0.03944979980587959\n",
      "Epoch 4386, Loss: 0.08671742305159569, Final Batch Loss: 0.05050475895404816\n",
      "Epoch 4387, Loss: 0.08784138225018978, Final Batch Loss: 0.026418307796120644\n",
      "Epoch 4388, Loss: 0.12306519597768784, Final Batch Loss: 0.04145955294370651\n",
      "Epoch 4389, Loss: 0.08685572072863579, Final Batch Loss: 0.03864777833223343\n",
      "Epoch 4390, Loss: 0.09134143218398094, Final Batch Loss: 0.055769361555576324\n",
      "Epoch 4391, Loss: 0.09127610363066196, Final Batch Loss: 0.07187953591346741\n",
      "Epoch 4392, Loss: 0.13214850798249245, Final Batch Loss: 0.07435746490955353\n",
      "Epoch 4393, Loss: 0.1017630472779274, Final Batch Loss: 0.039729852229356766\n",
      "Epoch 4394, Loss: 0.13005903363227844, Final Batch Loss: 0.06706343591213226\n",
      "Epoch 4395, Loss: 0.1583796888589859, Final Batch Loss: 0.108181431889534\n",
      "Epoch 4396, Loss: 0.09932930953800678, Final Batch Loss: 0.024765165522694588\n",
      "Epoch 4397, Loss: 0.10484015475958586, Final Batch Loss: 0.09204158931970596\n",
      "Epoch 4398, Loss: 0.08437824249267578, Final Batch Loss: 0.05950457602739334\n",
      "Epoch 4399, Loss: 0.10508733987808228, Final Batch Loss: 0.041956059634685516\n",
      "Epoch 4400, Loss: 0.05619793199002743, Final Batch Loss: 0.027782496064901352\n",
      "Epoch 4401, Loss: 0.07749022543430328, Final Batch Loss: 0.04996367171406746\n",
      "Epoch 4402, Loss: 0.08265791833400726, Final Batch Loss: 0.05257139727473259\n",
      "Epoch 4403, Loss: 0.15385492146015167, Final Batch Loss: 0.10537002235651016\n",
      "Epoch 4404, Loss: 0.079346664249897, Final Batch Loss: 0.026439540088176727\n",
      "Epoch 4405, Loss: 0.14729950577020645, Final Batch Loss: 0.06907472759485245\n",
      "Epoch 4406, Loss: 0.08809447661042213, Final Batch Loss: 0.035267140716314316\n",
      "Epoch 4407, Loss: 0.13218269869685173, Final Batch Loss: 0.07106909155845642\n",
      "Epoch 4408, Loss: 0.0563623309135437, Final Batch Loss: 0.01573144644498825\n",
      "Epoch 4409, Loss: 0.10343915224075317, Final Batch Loss: 0.04161302372813225\n",
      "Epoch 4410, Loss: 0.0901898443698883, Final Batch Loss: 0.05266151204705238\n",
      "Epoch 4411, Loss: 0.09694565087556839, Final Batch Loss: 0.07755415886640549\n",
      "Epoch 4412, Loss: 0.08900225535035133, Final Batch Loss: 0.038804467767477036\n",
      "Epoch 4413, Loss: 0.09261532500386238, Final Batch Loss: 0.04936407133936882\n",
      "Epoch 4414, Loss: 0.10928241163492203, Final Batch Loss: 0.0617196224629879\n",
      "Epoch 4415, Loss: 0.13727596774697304, Final Batch Loss: 0.08450237661600113\n",
      "Epoch 4416, Loss: 0.08193122409284115, Final Batch Loss: 0.06252381950616837\n",
      "Epoch 4417, Loss: 0.10186158865690231, Final Batch Loss: 0.03376512974500656\n",
      "Epoch 4418, Loss: 0.12764162942767143, Final Batch Loss: 0.018564749509096146\n",
      "Epoch 4419, Loss: 0.11010256037116051, Final Batch Loss: 0.027582142502069473\n",
      "Epoch 4420, Loss: 0.13141799718141556, Final Batch Loss: 0.08027173578739166\n",
      "Epoch 4421, Loss: 0.10347087681293488, Final Batch Loss: 0.05692563205957413\n",
      "Epoch 4422, Loss: 0.13844450190663338, Final Batch Loss: 0.061138417571783066\n",
      "Epoch 4423, Loss: 0.09992125257849693, Final Batch Loss: 0.056933145970106125\n",
      "Epoch 4424, Loss: 0.07960648089647293, Final Batch Loss: 0.04013463109731674\n",
      "Epoch 4425, Loss: 0.09573026560246944, Final Batch Loss: 0.0748906210064888\n",
      "Epoch 4426, Loss: 0.1359366811811924, Final Batch Loss: 0.051556993275880814\n",
      "Epoch 4427, Loss: 0.1339115910232067, Final Batch Loss: 0.05127440020442009\n",
      "Epoch 4428, Loss: 0.10711636766791344, Final Batch Loss: 0.05436750873923302\n",
      "Epoch 4429, Loss: 0.11141610890626907, Final Batch Loss: 0.03565209358930588\n",
      "Epoch 4430, Loss: 0.06267037242650986, Final Batch Loss: 0.03802848607301712\n",
      "Epoch 4431, Loss: 0.09403038024902344, Final Batch Loss: 0.04557208716869354\n",
      "Epoch 4432, Loss: 0.08112997561693192, Final Batch Loss: 0.036560263484716415\n",
      "Epoch 4433, Loss: 0.08584551513195038, Final Batch Loss: 0.04205435514450073\n",
      "Epoch 4434, Loss: 0.05335827358067036, Final Batch Loss: 0.03589972108602524\n",
      "Epoch 4435, Loss: 0.07790250331163406, Final Batch Loss: 0.03857438638806343\n",
      "Epoch 4436, Loss: 0.17423096671700478, Final Batch Loss: 0.13585621118545532\n",
      "Epoch 4437, Loss: 0.07765201479196548, Final Batch Loss: 0.05100352317094803\n",
      "Epoch 4438, Loss: 0.11365601792931557, Final Batch Loss: 0.05660416930913925\n",
      "Epoch 4439, Loss: 0.07444415986537933, Final Batch Loss: 0.04271858185529709\n",
      "Epoch 4440, Loss: 0.10313926637172699, Final Batch Loss: 0.05195217952132225\n",
      "Epoch 4441, Loss: 0.1053098812699318, Final Batch Loss: 0.04838772490620613\n",
      "Epoch 4442, Loss: 0.07050912454724312, Final Batch Loss: 0.03630568087100983\n",
      "Epoch 4443, Loss: 0.14095721021294594, Final Batch Loss: 0.07951688766479492\n",
      "Epoch 4444, Loss: 0.12207737937569618, Final Batch Loss: 0.05956600233912468\n",
      "Epoch 4445, Loss: 0.10232441872358322, Final Batch Loss: 0.0627032145857811\n",
      "Epoch 4446, Loss: 0.07973557151854038, Final Batch Loss: 0.050958696752786636\n",
      "Epoch 4447, Loss: 0.1463576927781105, Final Batch Loss: 0.07616432756185532\n",
      "Epoch 4448, Loss: 0.11322404071688652, Final Batch Loss: 0.05438077449798584\n",
      "Epoch 4449, Loss: 0.07196873053908348, Final Batch Loss: 0.037217285484075546\n",
      "Epoch 4450, Loss: 0.14705216139554977, Final Batch Loss: 0.06418929994106293\n",
      "Epoch 4451, Loss: 0.12103485129773617, Final Batch Loss: 0.091492660343647\n",
      "Epoch 4452, Loss: 0.07541687786579132, Final Batch Loss: 0.05128348618745804\n",
      "Epoch 4453, Loss: 0.07616811990737915, Final Batch Loss: 0.032616015523672104\n",
      "Epoch 4454, Loss: 0.09848329797387123, Final Batch Loss: 0.052927032113075256\n",
      "Epoch 4455, Loss: 0.09418153390288353, Final Batch Loss: 0.047362327575683594\n",
      "Epoch 4456, Loss: 0.08779305592179298, Final Batch Loss: 0.03934313356876373\n",
      "Epoch 4457, Loss: 0.08205851167440414, Final Batch Loss: 0.037021830677986145\n",
      "Epoch 4458, Loss: 0.09835640154778957, Final Batch Loss: 0.024967698380351067\n",
      "Epoch 4459, Loss: 0.09314942732453346, Final Batch Loss: 0.052658386528491974\n",
      "Epoch 4460, Loss: 0.07540939562022686, Final Batch Loss: 0.022867253050208092\n",
      "Epoch 4461, Loss: 0.08890151977539062, Final Batch Loss: 0.039892323315143585\n",
      "Epoch 4462, Loss: 0.1289105974137783, Final Batch Loss: 0.09134338796138763\n",
      "Epoch 4463, Loss: 0.10758019611239433, Final Batch Loss: 0.0536954402923584\n",
      "Epoch 4464, Loss: 0.10681691393256187, Final Batch Loss: 0.04449295252561569\n",
      "Epoch 4465, Loss: 0.05928486958146095, Final Batch Loss: 0.02380862459540367\n",
      "Epoch 4466, Loss: 0.05465891398489475, Final Batch Loss: 0.022409724071621895\n",
      "Epoch 4467, Loss: 0.0890176109969616, Final Batch Loss: 0.05614614486694336\n",
      "Epoch 4468, Loss: 0.11134389042854309, Final Batch Loss: 0.032948195934295654\n",
      "Epoch 4469, Loss: 0.10966294631361961, Final Batch Loss: 0.022843513637781143\n",
      "Epoch 4470, Loss: 0.07964889891445637, Final Batch Loss: 0.05981942266225815\n",
      "Epoch 4471, Loss: 0.11447533406317234, Final Batch Loss: 0.023042062297463417\n",
      "Epoch 4472, Loss: 0.06011023372411728, Final Batch Loss: 0.042014095932245255\n",
      "Epoch 4473, Loss: 0.09754380397498608, Final Batch Loss: 0.024080006405711174\n",
      "Epoch 4474, Loss: 0.08198591880500317, Final Batch Loss: 0.055911336094141006\n",
      "Epoch 4475, Loss: 0.0544032696634531, Final Batch Loss: 0.03350890055298805\n",
      "Epoch 4476, Loss: 0.05524208024144173, Final Batch Loss: 0.0295140128582716\n",
      "Epoch 4477, Loss: 0.06121470779180527, Final Batch Loss: 0.042834557592868805\n",
      "Epoch 4478, Loss: 0.15099453926086426, Final Batch Loss: 0.07936624437570572\n",
      "Epoch 4479, Loss: 0.08824257925152779, Final Batch Loss: 0.06287913024425507\n",
      "Epoch 4480, Loss: 0.0489107109606266, Final Batch Loss: 0.016377829015254974\n",
      "Epoch 4481, Loss: 0.06349049508571625, Final Batch Loss: 0.019407037645578384\n",
      "Epoch 4482, Loss: 0.10252557694911957, Final Batch Loss: 0.05002778768539429\n",
      "Epoch 4483, Loss: 0.1340043656527996, Final Batch Loss: 0.049021560698747635\n",
      "Epoch 4484, Loss: 0.13638905063271523, Final Batch Loss: 0.03726498410105705\n",
      "Epoch 4485, Loss: 0.19964656978845596, Final Batch Loss: 0.08803609013557434\n",
      "Epoch 4486, Loss: 0.18459130078554153, Final Batch Loss: 0.09998492896556854\n",
      "Epoch 4487, Loss: 0.07981854677200317, Final Batch Loss: 0.03324732929468155\n",
      "Epoch 4488, Loss: 0.11999726295471191, Final Batch Loss: 0.05279184877872467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4489, Loss: 0.07736895978450775, Final Batch Loss: 0.04084642976522446\n",
      "Epoch 4490, Loss: 0.1621619611978531, Final Batch Loss: 0.07911788672208786\n",
      "Epoch 4491, Loss: 0.1543722301721573, Final Batch Loss: 0.09222815930843353\n",
      "Epoch 4492, Loss: 0.15944281220436096, Final Batch Loss: 0.10510659217834473\n",
      "Epoch 4493, Loss: 0.115617286413908, Final Batch Loss: 0.0562136173248291\n",
      "Epoch 4494, Loss: 0.10726303234696388, Final Batch Loss: 0.05909566953778267\n",
      "Epoch 4495, Loss: 0.1045031026005745, Final Batch Loss: 0.04378267005085945\n",
      "Epoch 4496, Loss: 0.14427822455763817, Final Batch Loss: 0.03536888584494591\n",
      "Epoch 4497, Loss: 0.11194287985563278, Final Batch Loss: 0.07460428774356842\n",
      "Epoch 4498, Loss: 0.08987709507346153, Final Batch Loss: 0.05273640528321266\n",
      "Epoch 4499, Loss: 0.10653023049235344, Final Batch Loss: 0.06246829777956009\n",
      "Epoch 4500, Loss: 0.13611414283514023, Final Batch Loss: 0.09611384570598602\n",
      "Epoch 4501, Loss: 0.1302863024175167, Final Batch Loss: 0.05017569288611412\n",
      "Epoch 4502, Loss: 0.12195683643221855, Final Batch Loss: 0.06478896737098694\n",
      "Epoch 4503, Loss: 0.10413049906492233, Final Batch Loss: 0.07258757203817368\n",
      "Epoch 4504, Loss: 0.09129435941576958, Final Batch Loss: 0.0369986854493618\n",
      "Epoch 4505, Loss: 0.10960220545530319, Final Batch Loss: 0.06782222539186478\n",
      "Epoch 4506, Loss: 0.1226826086640358, Final Batch Loss: 0.09385478496551514\n",
      "Epoch 4507, Loss: 0.059609225019812584, Final Batch Loss: 0.02597050555050373\n",
      "Epoch 4508, Loss: 0.06076030805706978, Final Batch Loss: 0.036446455866098404\n",
      "Epoch 4509, Loss: 0.0504581555724144, Final Batch Loss: 0.030893120914697647\n",
      "Epoch 4510, Loss: 0.06660659238696098, Final Batch Loss: 0.02735516056418419\n",
      "Epoch 4511, Loss: 0.08096668869256973, Final Batch Loss: 0.049867406487464905\n",
      "Epoch 4512, Loss: 0.10020274296402931, Final Batch Loss: 0.06568220257759094\n",
      "Epoch 4513, Loss: 0.1107308454811573, Final Batch Loss: 0.03467844799160957\n",
      "Epoch 4514, Loss: 0.09186884760856628, Final Batch Loss: 0.04699007421731949\n",
      "Epoch 4515, Loss: 0.07085582986474037, Final Batch Loss: 0.0316350944340229\n",
      "Epoch 4516, Loss: 0.09949303045868874, Final Batch Loss: 0.0709986761212349\n",
      "Epoch 4517, Loss: 0.06393109634518623, Final Batch Loss: 0.03376150503754616\n",
      "Epoch 4518, Loss: 0.1290135160088539, Final Batch Loss: 0.08909258246421814\n",
      "Epoch 4519, Loss: 0.058259088546037674, Final Batch Loss: 0.028792332857847214\n",
      "Epoch 4520, Loss: 0.11699148267507553, Final Batch Loss: 0.0499190017580986\n",
      "Epoch 4521, Loss: 0.11031867191195488, Final Batch Loss: 0.06085895746946335\n",
      "Epoch 4522, Loss: 0.06151071563363075, Final Batch Loss: 0.02761669084429741\n",
      "Epoch 4523, Loss: 0.10102598741650581, Final Batch Loss: 0.040406592190265656\n",
      "Epoch 4524, Loss: 0.04555067792534828, Final Batch Loss: 0.03180749714374542\n",
      "Epoch 4525, Loss: 0.08058047294616699, Final Batch Loss: 0.04021787270903587\n",
      "Epoch 4526, Loss: 0.10499782487750053, Final Batch Loss: 0.0375913642346859\n",
      "Epoch 4527, Loss: 0.11992933973670006, Final Batch Loss: 0.051661740988492966\n",
      "Epoch 4528, Loss: 0.05832750163972378, Final Batch Loss: 0.03907255083322525\n",
      "Epoch 4529, Loss: 0.09291335195302963, Final Batch Loss: 0.0701604038476944\n",
      "Epoch 4530, Loss: 0.09914639964699745, Final Batch Loss: 0.039824046194553375\n",
      "Epoch 4531, Loss: 0.10521149262785912, Final Batch Loss: 0.06189359351992607\n",
      "Epoch 4532, Loss: 0.09292808268219233, Final Batch Loss: 0.07854916900396347\n",
      "Epoch 4533, Loss: 0.05471585877239704, Final Batch Loss: 0.03007209487259388\n",
      "Epoch 4534, Loss: 0.05357588827610016, Final Batch Loss: 0.02893216162919998\n",
      "Epoch 4535, Loss: 0.0820529330521822, Final Batch Loss: 0.030862802639603615\n",
      "Epoch 4536, Loss: 0.0702538713812828, Final Batch Loss: 0.03616400063037872\n",
      "Epoch 4537, Loss: 0.10546821914613247, Final Batch Loss: 0.020742831751704216\n",
      "Epoch 4538, Loss: 0.09812660329043865, Final Batch Loss: 0.018988316878676414\n",
      "Epoch 4539, Loss: 0.06923258304595947, Final Batch Loss: 0.038237206637859344\n",
      "Epoch 4540, Loss: 0.09684460610151291, Final Batch Loss: 0.050640977919101715\n",
      "Epoch 4541, Loss: 0.14314574375748634, Final Batch Loss: 0.0981639176607132\n",
      "Epoch 4542, Loss: 0.12438611313700676, Final Batch Loss: 0.06801723688840866\n",
      "Epoch 4543, Loss: 0.06901412084698677, Final Batch Loss: 0.03525690361857414\n",
      "Epoch 4544, Loss: 0.1051374301314354, Final Batch Loss: 0.04288589581847191\n",
      "Epoch 4545, Loss: 0.06869206018745899, Final Batch Loss: 0.04615779593586922\n",
      "Epoch 4546, Loss: 0.11312262713909149, Final Batch Loss: 0.04213511943817139\n",
      "Epoch 4547, Loss: 0.09595264121890068, Final Batch Loss: 0.06663644313812256\n",
      "Epoch 4548, Loss: 0.09693844802677631, Final Batch Loss: 0.024280110374093056\n",
      "Epoch 4549, Loss: 0.054014893248677254, Final Batch Loss: 0.025025837123394012\n",
      "Epoch 4550, Loss: 0.06130095571279526, Final Batch Loss: 0.027451660484075546\n",
      "Epoch 4551, Loss: 0.05341370776295662, Final Batch Loss: 0.023007985204458237\n",
      "Epoch 4552, Loss: 0.0560007207095623, Final Batch Loss: 0.027370993047952652\n",
      "Epoch 4553, Loss: 0.09907390549778938, Final Batch Loss: 0.0472647100687027\n",
      "Epoch 4554, Loss: 0.07736687548458576, Final Batch Loss: 0.028671426698565483\n",
      "Epoch 4555, Loss: 0.09175793454051018, Final Batch Loss: 0.06593234837055206\n",
      "Epoch 4556, Loss: 0.056271038949489594, Final Batch Loss: 0.03222363069653511\n",
      "Epoch 4557, Loss: 0.12059781327843666, Final Batch Loss: 0.08877585828304291\n",
      "Epoch 4558, Loss: 0.17853540182113647, Final Batch Loss: 0.105030857026577\n",
      "Epoch 4559, Loss: 0.06638661399483681, Final Batch Loss: 0.034102246165275574\n",
      "Epoch 4560, Loss: 0.04306451324373484, Final Batch Loss: 0.029675034806132317\n",
      "Epoch 4561, Loss: 0.09426227957010269, Final Batch Loss: 0.050407540053129196\n",
      "Epoch 4562, Loss: 0.07203156687319279, Final Batch Loss: 0.026251157745718956\n",
      "Epoch 4563, Loss: 0.12921060621738434, Final Batch Loss: 0.05316011607646942\n",
      "Epoch 4564, Loss: 0.0591959897428751, Final Batch Loss: 0.024361716583371162\n",
      "Epoch 4565, Loss: 0.1809731423854828, Final Batch Loss: 0.12026896327733994\n",
      "Epoch 4566, Loss: 0.05963796190917492, Final Batch Loss: 0.026793425902724266\n",
      "Epoch 4567, Loss: 0.05398358963429928, Final Batch Loss: 0.016468456014990807\n",
      "Epoch 4568, Loss: 0.11864840984344482, Final Batch Loss: 0.07227225601673126\n",
      "Epoch 4569, Loss: 0.0818546935915947, Final Batch Loss: 0.04045633226633072\n",
      "Epoch 4570, Loss: 0.07510045357048512, Final Batch Loss: 0.020029926672577858\n",
      "Epoch 4571, Loss: 0.08458627015352249, Final Batch Loss: 0.03819962590932846\n",
      "Epoch 4572, Loss: 0.043454237282276154, Final Batch Loss: 0.01943146623671055\n",
      "Epoch 4573, Loss: 0.10191059298813343, Final Batch Loss: 0.02860991843044758\n",
      "Epoch 4574, Loss: 0.09844160452485085, Final Batch Loss: 0.04277794435620308\n",
      "Epoch 4575, Loss: 0.07271987944841385, Final Batch Loss: 0.034679874777793884\n",
      "Epoch 4576, Loss: 0.07584193907678127, Final Batch Loss: 0.0457431934773922\n",
      "Epoch 4577, Loss: 0.055181071162223816, Final Batch Loss: 0.0302308090031147\n",
      "Epoch 4578, Loss: 0.06432138197124004, Final Batch Loss: 0.024438699707388878\n",
      "Epoch 4579, Loss: 0.0603476595133543, Final Batch Loss: 0.025611037388443947\n",
      "Epoch 4580, Loss: 0.07543150335550308, Final Batch Loss: 0.041740573942661285\n",
      "Epoch 4581, Loss: 0.09079707227647305, Final Batch Loss: 0.02721027098596096\n",
      "Epoch 4582, Loss: 0.057389864698052406, Final Batch Loss: 0.024033354595303535\n",
      "Epoch 4583, Loss: 0.09638988599181175, Final Batch Loss: 0.03371587023139\n",
      "Epoch 4584, Loss: 0.03962921444326639, Final Batch Loss: 0.025195874273777008\n",
      "Epoch 4585, Loss: 0.10239741206169128, Final Batch Loss: 0.061617232859134674\n",
      "Epoch 4586, Loss: 0.09725235402584076, Final Batch Loss: 0.014547109603881836\n",
      "Epoch 4587, Loss: 0.08711208775639534, Final Batch Loss: 0.04947306588292122\n",
      "Epoch 4588, Loss: 0.10121488198637962, Final Batch Loss: 0.03925224021077156\n",
      "Epoch 4589, Loss: 0.11840864270925522, Final Batch Loss: 0.07230070233345032\n",
      "Epoch 4590, Loss: 0.07034576125442982, Final Batch Loss: 0.043690361082553864\n",
      "Epoch 4591, Loss: 0.07287096884101629, Final Batch Loss: 0.014788213185966015\n",
      "Epoch 4592, Loss: 0.06138971447944641, Final Batch Loss: 0.025457900017499924\n",
      "Epoch 4593, Loss: 0.07189103960990906, Final Batch Loss: 0.04077289253473282\n",
      "Epoch 4594, Loss: 0.04944100044667721, Final Batch Loss: 0.025633500888943672\n",
      "Epoch 4595, Loss: 0.08224263414740562, Final Batch Loss: 0.04754823073744774\n",
      "Epoch 4596, Loss: 0.10336202196776867, Final Batch Loss: 0.03066849522292614\n",
      "Epoch 4597, Loss: 0.06503552012145519, Final Batch Loss: 0.02800476737320423\n",
      "Epoch 4598, Loss: 0.1087176613509655, Final Batch Loss: 0.03486914560198784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4599, Loss: 0.0955873541533947, Final Batch Loss: 0.05413024127483368\n",
      "Epoch 4600, Loss: 0.08855589479207993, Final Batch Loss: 0.03832804784178734\n",
      "Epoch 4601, Loss: 0.0855080634355545, Final Batch Loss: 0.04262257367372513\n",
      "Epoch 4602, Loss: 0.07567078992724419, Final Batch Loss: 0.030090246349573135\n",
      "Epoch 4603, Loss: 0.06223695166409016, Final Batch Loss: 0.036467183381319046\n",
      "Epoch 4604, Loss: 0.08513091504573822, Final Batch Loss: 0.048365235328674316\n",
      "Epoch 4605, Loss: 0.07494984194636345, Final Batch Loss: 0.043450310826301575\n",
      "Epoch 4606, Loss: 0.08987156674265862, Final Batch Loss: 0.05006067827343941\n",
      "Epoch 4607, Loss: 0.07459103129804134, Final Batch Loss: 0.026540564373135567\n",
      "Epoch 4608, Loss: 0.09674281626939774, Final Batch Loss: 0.04053495079278946\n",
      "Epoch 4609, Loss: 0.07654804736375809, Final Batch Loss: 0.02503899112343788\n",
      "Epoch 4610, Loss: 0.12757551111280918, Final Batch Loss: 0.10339381545782089\n",
      "Epoch 4611, Loss: 0.11538318917155266, Final Batch Loss: 0.04016988351941109\n",
      "Epoch 4612, Loss: 0.10350564122200012, Final Batch Loss: 0.028732411563396454\n",
      "Epoch 4613, Loss: 0.05440286546945572, Final Batch Loss: 0.02615494653582573\n",
      "Epoch 4614, Loss: 0.10046953335404396, Final Batch Loss: 0.04948483034968376\n",
      "Epoch 4615, Loss: 0.10708359070122242, Final Batch Loss: 0.08909590542316437\n",
      "Epoch 4616, Loss: 0.06290444731712341, Final Batch Loss: 0.013682916760444641\n",
      "Epoch 4617, Loss: 0.08102278411388397, Final Batch Loss: 0.04142552241683006\n",
      "Epoch 4618, Loss: 0.06465381942689419, Final Batch Loss: 0.038733694702386856\n",
      "Epoch 4619, Loss: 0.06356041505932808, Final Batch Loss: 0.04687625169754028\n",
      "Epoch 4620, Loss: 0.08513829857110977, Final Batch Loss: 0.05373590812087059\n",
      "Epoch 4621, Loss: 0.05642823502421379, Final Batch Loss: 0.03479853644967079\n",
      "Epoch 4622, Loss: 0.0888974629342556, Final Batch Loss: 0.07226905971765518\n",
      "Epoch 4623, Loss: 0.043710989877581596, Final Batch Loss: 0.013273278251290321\n",
      "Epoch 4624, Loss: 0.09083297103643417, Final Batch Loss: 0.030634619295597076\n",
      "Epoch 4625, Loss: 0.10207516327500343, Final Batch Loss: 0.046759024262428284\n",
      "Epoch 4626, Loss: 0.06847390159964561, Final Batch Loss: 0.038436081260442734\n",
      "Epoch 4627, Loss: 0.06188955530524254, Final Batch Loss: 0.03361522778868675\n",
      "Epoch 4628, Loss: 0.12730491161346436, Final Batch Loss: 0.08518476039171219\n",
      "Epoch 4629, Loss: 0.06287488900125027, Final Batch Loss: 0.022295376285910606\n",
      "Epoch 4630, Loss: 0.09386244043707848, Final Batch Loss: 0.04460188001394272\n",
      "Epoch 4631, Loss: 0.05636700615286827, Final Batch Loss: 0.026877349242568016\n",
      "Epoch 4632, Loss: 0.0813630186021328, Final Batch Loss: 0.057836417108774185\n",
      "Epoch 4633, Loss: 0.06965521723031998, Final Batch Loss: 0.01847328618168831\n",
      "Epoch 4634, Loss: 0.19288840144872665, Final Batch Loss: 0.13066524267196655\n",
      "Epoch 4635, Loss: 0.061722656711936, Final Batch Loss: 0.029137684032320976\n",
      "Epoch 4636, Loss: 0.04842125345021486, Final Batch Loss: 0.0360497422516346\n",
      "Epoch 4637, Loss: 0.0875403918325901, Final Batch Loss: 0.05073444917798042\n",
      "Epoch 4638, Loss: 0.0814187042415142, Final Batch Loss: 0.03184588626027107\n",
      "Epoch 4639, Loss: 0.06859301216900349, Final Batch Loss: 0.02364807017147541\n",
      "Epoch 4640, Loss: 0.0968262106180191, Final Batch Loss: 0.08622658252716064\n",
      "Epoch 4641, Loss: 0.0897362269461155, Final Batch Loss: 0.05214926227927208\n",
      "Epoch 4642, Loss: 0.19139176607131958, Final Batch Loss: 0.12431110441684723\n",
      "Epoch 4643, Loss: 0.04689614847302437, Final Batch Loss: 0.020147105678915977\n",
      "Epoch 4644, Loss: 0.0682649202644825, Final Batch Loss: 0.03539949283003807\n",
      "Epoch 4645, Loss: 0.08345609158277512, Final Batch Loss: 0.053459156304597855\n",
      "Epoch 4646, Loss: 0.11674141511321068, Final Batch Loss: 0.04503897950053215\n",
      "Epoch 4647, Loss: 0.06614359840750694, Final Batch Loss: 0.03042880818247795\n",
      "Epoch 4648, Loss: 0.09019583463668823, Final Batch Loss: 0.03630978986620903\n",
      "Epoch 4649, Loss: 0.0626046173274517, Final Batch Loss: 0.018452554941177368\n",
      "Epoch 4650, Loss: 0.14476589858531952, Final Batch Loss: 0.08697254955768585\n",
      "Epoch 4651, Loss: 0.036944324150681496, Final Batch Loss: 0.019811464473605156\n",
      "Epoch 4652, Loss: 0.06600498594343662, Final Batch Loss: 0.02315537817776203\n",
      "Epoch 4653, Loss: 0.13054576329886913, Final Batch Loss: 0.013054391369223595\n",
      "Epoch 4654, Loss: 0.08016183227300644, Final Batch Loss: 0.047717202454805374\n",
      "Epoch 4655, Loss: 0.07632875069975853, Final Batch Loss: 0.036059893667697906\n",
      "Epoch 4656, Loss: 0.09333337843418121, Final Batch Loss: 0.04666292294859886\n",
      "Epoch 4657, Loss: 0.07389779016375542, Final Batch Loss: 0.041502781212329865\n",
      "Epoch 4658, Loss: 0.15329638868570328, Final Batch Loss: 0.07970120012760162\n",
      "Epoch 4659, Loss: 0.041275784373283386, Final Batch Loss: 0.01654474064707756\n",
      "Epoch 4660, Loss: 0.1355445645749569, Final Batch Loss: 0.06222333386540413\n",
      "Epoch 4661, Loss: 0.11498654820024967, Final Batch Loss: 0.02574370987713337\n",
      "Epoch 4662, Loss: 0.06919354386627674, Final Batch Loss: 0.020725620910525322\n",
      "Epoch 4663, Loss: 0.05252647027373314, Final Batch Loss: 0.025554819032549858\n",
      "Epoch 4664, Loss: 0.12218070402741432, Final Batch Loss: 0.0561867393553257\n",
      "Epoch 4665, Loss: 0.11336545273661613, Final Batch Loss: 0.058688897639513016\n",
      "Epoch 4666, Loss: 0.13414568454027176, Final Batch Loss: 0.06441400200128555\n",
      "Epoch 4667, Loss: 0.06762212887406349, Final Batch Loss: 0.05283594876527786\n",
      "Epoch 4668, Loss: 0.12094063684344292, Final Batch Loss: 0.08600696921348572\n",
      "Epoch 4669, Loss: 0.0657303836196661, Final Batch Loss: 0.027733338996767998\n",
      "Epoch 4670, Loss: 0.09021705016493797, Final Batch Loss: 0.040398355573415756\n",
      "Epoch 4671, Loss: 0.0949990376830101, Final Batch Loss: 0.044989943504333496\n",
      "Epoch 4672, Loss: 0.04842757806181908, Final Batch Loss: 0.020188672468066216\n",
      "Epoch 4673, Loss: 0.10021290555596352, Final Batch Loss: 0.05842343717813492\n",
      "Epoch 4674, Loss: 0.08675122261047363, Final Batch Loss: 0.04442526400089264\n",
      "Epoch 4675, Loss: 0.08476029708981514, Final Batch Loss: 0.05640210956335068\n",
      "Epoch 4676, Loss: 0.05796312354505062, Final Batch Loss: 0.016636675223708153\n",
      "Epoch 4677, Loss: 0.07195722870528698, Final Batch Loss: 0.04309992864727974\n",
      "Epoch 4678, Loss: 0.08332900144159794, Final Batch Loss: 0.029390910640358925\n",
      "Epoch 4679, Loss: 0.07135747745633125, Final Batch Loss: 0.04613466188311577\n",
      "Epoch 4680, Loss: 0.08205747231841087, Final Batch Loss: 0.03443608805537224\n",
      "Epoch 4681, Loss: 0.04009968228638172, Final Batch Loss: 0.019960492849349976\n",
      "Epoch 4682, Loss: 0.07061594538390636, Final Batch Loss: 0.0419464148581028\n",
      "Epoch 4683, Loss: 0.06340615078806877, Final Batch Loss: 0.03151710703969002\n",
      "Epoch 4684, Loss: 0.08010665886104107, Final Batch Loss: 0.05431118234992027\n",
      "Epoch 4685, Loss: 0.06785980425775051, Final Batch Loss: 0.054119594395160675\n",
      "Epoch 4686, Loss: 0.0435008779168129, Final Batch Loss: 0.022114308550953865\n",
      "Epoch 4687, Loss: 0.12137671932578087, Final Batch Loss: 0.03689232096076012\n",
      "Epoch 4688, Loss: 0.1109394021332264, Final Batch Loss: 0.0420646034181118\n",
      "Epoch 4689, Loss: 0.137894406914711, Final Batch Loss: 0.0664093866944313\n",
      "Epoch 4690, Loss: 0.08443716913461685, Final Batch Loss: 0.04616789147257805\n",
      "Epoch 4691, Loss: 0.1134607195854187, Final Batch Loss: 0.09707941859960556\n",
      "Epoch 4692, Loss: 0.11902068182826042, Final Batch Loss: 0.07057147473096848\n",
      "Epoch 4693, Loss: 0.04593690671026707, Final Batch Loss: 0.02737441100180149\n",
      "Epoch 4694, Loss: 0.1059400886297226, Final Batch Loss: 0.040939390659332275\n",
      "Epoch 4695, Loss: 0.07166997529566288, Final Batch Loss: 0.029052944853901863\n",
      "Epoch 4696, Loss: 0.06099544279277325, Final Batch Loss: 0.021790260449051857\n",
      "Epoch 4697, Loss: 0.049250515177845955, Final Batch Loss: 0.031243430450558662\n",
      "Epoch 4698, Loss: 0.1064034141600132, Final Batch Loss: 0.045461468398571014\n",
      "Epoch 4699, Loss: 0.10758009925484657, Final Batch Loss: 0.05563788488507271\n",
      "Epoch 4700, Loss: 0.12317400425672531, Final Batch Loss: 0.06550149619579315\n",
      "Epoch 4701, Loss: 0.07204743474721909, Final Batch Loss: 0.025751259177923203\n",
      "Epoch 4702, Loss: 0.09296341985464096, Final Batch Loss: 0.04326818883419037\n",
      "Epoch 4703, Loss: 0.07406893745064735, Final Batch Loss: 0.03544934466481209\n",
      "Epoch 4704, Loss: 0.0747983455657959, Final Batch Loss: 0.01927974447607994\n",
      "Epoch 4705, Loss: 0.10588298365473747, Final Batch Loss: 0.058400046080350876\n",
      "Epoch 4706, Loss: 0.0719902366399765, Final Batch Loss: 0.04729926586151123\n",
      "Epoch 4707, Loss: 0.06562546268105507, Final Batch Loss: 0.03745776787400246\n",
      "Epoch 4708, Loss: 0.08800989203155041, Final Batch Loss: 0.029049834236502647\n",
      "Epoch 4709, Loss: 0.12277048453688622, Final Batch Loss: 0.06024528667330742\n",
      "Epoch 4710, Loss: 0.0636382456868887, Final Batch Loss: 0.02830428071320057\n",
      "Epoch 4711, Loss: 0.07664052955806255, Final Batch Loss: 0.025738833472132683\n",
      "Epoch 4712, Loss: 0.19322632253170013, Final Batch Loss: 0.10397542268037796\n",
      "Epoch 4713, Loss: 0.06911201402544975, Final Batch Loss: 0.04983390495181084\n",
      "Epoch 4714, Loss: 0.06537657976150513, Final Batch Loss: 0.042764414101839066\n",
      "Epoch 4715, Loss: 0.12895625829696655, Final Batch Loss: 0.07566889375448227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4716, Loss: 0.07741298899054527, Final Batch Loss: 0.03191947191953659\n",
      "Epoch 4717, Loss: 0.09135516360402107, Final Batch Loss: 0.024492163211107254\n",
      "Epoch 4718, Loss: 0.1013709120452404, Final Batch Loss: 0.0688762441277504\n",
      "Epoch 4719, Loss: 0.1566840596497059, Final Batch Loss: 0.0997655838727951\n",
      "Epoch 4720, Loss: 0.07183442637324333, Final Batch Loss: 0.03826768696308136\n",
      "Epoch 4721, Loss: 0.0639583207666874, Final Batch Loss: 0.027967508882284164\n",
      "Epoch 4722, Loss: 0.06675727292895317, Final Batch Loss: 0.02808409556746483\n",
      "Epoch 4723, Loss: 0.08802482485771179, Final Batch Loss: 0.05511913821101189\n",
      "Epoch 4724, Loss: 0.12876885756850243, Final Batch Loss: 0.08190547674894333\n",
      "Epoch 4725, Loss: 0.10185876116156578, Final Batch Loss: 0.046029992401599884\n",
      "Epoch 4726, Loss: 0.0958356186747551, Final Batch Loss: 0.05076470226049423\n",
      "Epoch 4727, Loss: 0.15286610275506973, Final Batch Loss: 0.059530556201934814\n",
      "Epoch 4728, Loss: 0.059238579124212265, Final Batch Loss: 0.034350596368312836\n",
      "Epoch 4729, Loss: 0.10736619308590889, Final Batch Loss: 0.05257201939821243\n",
      "Epoch 4730, Loss: 0.05256473086774349, Final Batch Loss: 0.02823464199900627\n",
      "Epoch 4731, Loss: 0.07294904254376888, Final Batch Loss: 0.026498591527342796\n",
      "Epoch 4732, Loss: 0.08687891066074371, Final Batch Loss: 0.053049486130476\n",
      "Epoch 4733, Loss: 0.052973587065935135, Final Batch Loss: 0.03607543930411339\n",
      "Epoch 4734, Loss: 0.05557120591402054, Final Batch Loss: 0.02081860974431038\n",
      "Epoch 4735, Loss: 0.05380578711628914, Final Batch Loss: 0.02574787475168705\n",
      "Epoch 4736, Loss: 0.09094560146331787, Final Batch Loss: 0.03144492581486702\n",
      "Epoch 4737, Loss: 0.11191578954458237, Final Batch Loss: 0.04034624993801117\n",
      "Epoch 4738, Loss: 0.08549293875694275, Final Batch Loss: 0.0335356630384922\n",
      "Epoch 4739, Loss: 0.11449255049228668, Final Batch Loss: 0.06034361571073532\n",
      "Epoch 4740, Loss: 0.10038363188505173, Final Batch Loss: 0.0537109449505806\n",
      "Epoch 4741, Loss: 0.05999406985938549, Final Batch Loss: 0.02757742814719677\n",
      "Epoch 4742, Loss: 0.048185208812355995, Final Batch Loss: 0.03223208710551262\n",
      "Epoch 4743, Loss: 0.03420204669237137, Final Batch Loss: 0.01973414234817028\n",
      "Epoch 4744, Loss: 0.062112171202898026, Final Batch Loss: 0.031963326036930084\n",
      "Epoch 4745, Loss: 0.10305261239409447, Final Batch Loss: 0.0781974270939827\n",
      "Epoch 4746, Loss: 0.11130218580365181, Final Batch Loss: 0.06082597002387047\n",
      "Epoch 4747, Loss: 0.1149318628013134, Final Batch Loss: 0.056607961654663086\n",
      "Epoch 4748, Loss: 0.12013040482997894, Final Batch Loss: 0.05245605856180191\n",
      "Epoch 4749, Loss: 0.0618718545883894, Final Batch Loss: 0.019501762464642525\n",
      "Epoch 4750, Loss: 0.0370822474360466, Final Batch Loss: 0.01835070177912712\n",
      "Epoch 4751, Loss: 0.11828666552901268, Final Batch Loss: 0.046404559165239334\n",
      "Epoch 4752, Loss: 0.09633558988571167, Final Batch Loss: 0.054408345371484756\n",
      "Epoch 4753, Loss: 0.10705842077732086, Final Batch Loss: 0.04822305962443352\n",
      "Epoch 4754, Loss: 0.06773781217634678, Final Batch Loss: 0.03655819594860077\n",
      "Epoch 4755, Loss: 0.15238498151302338, Final Batch Loss: 0.0829576924443245\n",
      "Epoch 4756, Loss: 0.1162763386964798, Final Batch Loss: 0.07222029566764832\n",
      "Epoch 4757, Loss: 0.07841044291853905, Final Batch Loss: 0.02413322404026985\n",
      "Epoch 4758, Loss: 0.11746222898364067, Final Batch Loss: 0.07699880748987198\n",
      "Epoch 4759, Loss: 0.14680372178554535, Final Batch Loss: 0.044402480125427246\n",
      "Epoch 4760, Loss: 0.12111363559961319, Final Batch Loss: 0.039738647639751434\n",
      "Epoch 4761, Loss: 0.10164451971650124, Final Batch Loss: 0.059792518615722656\n",
      "Epoch 4762, Loss: 0.12508141994476318, Final Batch Loss: 0.0831240564584732\n",
      "Epoch 4763, Loss: 0.13346197083592415, Final Batch Loss: 0.03213508054614067\n",
      "Epoch 4764, Loss: 0.1500968374311924, Final Batch Loss: 0.04354344680905342\n",
      "Epoch 4765, Loss: 0.06862460263073444, Final Batch Loss: 0.03885304182767868\n",
      "Epoch 4766, Loss: 0.07916481047868729, Final Batch Loss: 0.042101189494132996\n",
      "Epoch 4767, Loss: 0.11614818498492241, Final Batch Loss: 0.06108176335692406\n",
      "Epoch 4768, Loss: 0.08693166822195053, Final Batch Loss: 0.04645566642284393\n",
      "Epoch 4769, Loss: 0.12849334254860878, Final Batch Loss: 0.08771783858537674\n",
      "Epoch 4770, Loss: 0.11667510122060776, Final Batch Loss: 0.06440987437963486\n",
      "Epoch 4771, Loss: 0.08111312240362167, Final Batch Loss: 0.03859281167387962\n",
      "Epoch 4772, Loss: 0.05075412429869175, Final Batch Loss: 0.03047696128487587\n",
      "Epoch 4773, Loss: 0.07212354615330696, Final Batch Loss: 0.04597880691289902\n",
      "Epoch 4774, Loss: 0.1112460307776928, Final Batch Loss: 0.06589541584253311\n",
      "Epoch 4775, Loss: 0.07597186416387558, Final Batch Loss: 0.03125346079468727\n",
      "Epoch 4776, Loss: 0.08922874182462692, Final Batch Loss: 0.0401197224855423\n",
      "Epoch 4777, Loss: 0.10473336279392242, Final Batch Loss: 0.0512051023542881\n",
      "Epoch 4778, Loss: 0.04876652546226978, Final Batch Loss: 0.026169991120696068\n",
      "Epoch 4779, Loss: 0.08696600049734116, Final Batch Loss: 0.05416988208889961\n",
      "Epoch 4780, Loss: 0.0737576112151146, Final Batch Loss: 0.03787289559841156\n",
      "Epoch 4781, Loss: 0.07504035532474518, Final Batch Loss: 0.035176753997802734\n",
      "Epoch 4782, Loss: 0.10840288549661636, Final Batch Loss: 0.05046198517084122\n",
      "Epoch 4783, Loss: 0.10512478277087212, Final Batch Loss: 0.034794580191373825\n",
      "Epoch 4784, Loss: 0.15000862628221512, Final Batch Loss: 0.07129578292369843\n",
      "Epoch 4785, Loss: 0.05979807861149311, Final Batch Loss: 0.023886194452643394\n",
      "Epoch 4786, Loss: 0.09365390241146088, Final Batch Loss: 0.05641818791627884\n",
      "Epoch 4787, Loss: 0.09789875149726868, Final Batch Loss: 0.05398230254650116\n",
      "Epoch 4788, Loss: 0.12407230958342552, Final Batch Loss: 0.06389614939689636\n",
      "Epoch 4789, Loss: 0.06749938242137432, Final Batch Loss: 0.037562910467386246\n",
      "Epoch 4790, Loss: 0.11430652067065239, Final Batch Loss: 0.07725746184587479\n",
      "Epoch 4791, Loss: 0.1058524064719677, Final Batch Loss: 0.06439714878797531\n",
      "Epoch 4792, Loss: 0.060112494975328445, Final Batch Loss: 0.030138831585645676\n",
      "Epoch 4793, Loss: 0.07803434692323208, Final Batch Loss: 0.028602013364434242\n",
      "Epoch 4794, Loss: 0.10528336465358734, Final Batch Loss: 0.050446197390556335\n",
      "Epoch 4795, Loss: 0.09569784253835678, Final Batch Loss: 0.0693175345659256\n",
      "Epoch 4796, Loss: 0.06569117680191994, Final Batch Loss: 0.03187733516097069\n",
      "Epoch 4797, Loss: 0.0743970088660717, Final Batch Loss: 0.03037487342953682\n",
      "Epoch 4798, Loss: 0.07093098014593124, Final Batch Loss: 0.046684276312589645\n",
      "Epoch 4799, Loss: 0.1037675254046917, Final Batch Loss: 0.021994706243276596\n",
      "Epoch 4800, Loss: 0.13024966418743134, Final Batch Loss: 0.06296265125274658\n",
      "Epoch 4801, Loss: 0.07976322248578072, Final Batch Loss: 0.05485299974679947\n",
      "Epoch 4802, Loss: 0.061976587399840355, Final Batch Loss: 0.026413945481181145\n",
      "Epoch 4803, Loss: 0.14642811007797718, Final Batch Loss: 0.025592127814888954\n",
      "Epoch 4804, Loss: 0.12132110446691513, Final Batch Loss: 0.04761713743209839\n",
      "Epoch 4805, Loss: 0.05609460361301899, Final Batch Loss: 0.01582207717001438\n",
      "Epoch 4806, Loss: 0.08008406311273575, Final Batch Loss: 0.056444231420755386\n",
      "Epoch 4807, Loss: 0.08354886248707771, Final Batch Loss: 0.051327452063560486\n",
      "Epoch 4808, Loss: 0.07880071178078651, Final Batch Loss: 0.03369009122252464\n",
      "Epoch 4809, Loss: 0.07944926247000694, Final Batch Loss: 0.023595891892910004\n",
      "Epoch 4810, Loss: 0.10330339148640633, Final Batch Loss: 0.043178196996450424\n",
      "Epoch 4811, Loss: 0.09708670154213905, Final Batch Loss: 0.032768312841653824\n",
      "Epoch 4812, Loss: 0.048813844099640846, Final Batch Loss: 0.01765402965247631\n",
      "Epoch 4813, Loss: 0.14193259179592133, Final Batch Loss: 0.019618481397628784\n",
      "Epoch 4814, Loss: 0.12065951526165009, Final Batch Loss: 0.07211046665906906\n",
      "Epoch 4815, Loss: 0.07485921680927277, Final Batch Loss: 0.037917833775281906\n",
      "Epoch 4816, Loss: 0.13693289831280708, Final Batch Loss: 0.08159837871789932\n",
      "Epoch 4817, Loss: 0.06494828499853611, Final Batch Loss: 0.01655622385442257\n",
      "Epoch 4818, Loss: 0.055832184851169586, Final Batch Loss: 0.02603132836520672\n",
      "Epoch 4819, Loss: 0.0914791626855731, Final Batch Loss: 0.014905723743140697\n",
      "Epoch 4820, Loss: 0.03594487812370062, Final Batch Loss: 0.024943886324763298\n",
      "Epoch 4821, Loss: 0.06882375106215477, Final Batch Loss: 0.03691711649298668\n",
      "Epoch 4822, Loss: 0.07427666522562504, Final Batch Loss: 0.04387256130576134\n",
      "Epoch 4823, Loss: 0.040035758167505264, Final Batch Loss: 0.019558148458600044\n",
      "Epoch 4824, Loss: 0.05254480428993702, Final Batch Loss: 0.024235717952251434\n",
      "Epoch 4825, Loss: 0.049712241627275944, Final Batch Loss: 0.03414876386523247\n",
      "Epoch 4826, Loss: 0.04228249564766884, Final Batch Loss: 0.01652711071074009\n",
      "Epoch 4827, Loss: 0.03131392877548933, Final Batch Loss: 0.01822206936776638\n",
      "Epoch 4828, Loss: 0.08883974701166153, Final Batch Loss: 0.038967378437519073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4829, Loss: 0.049259791150689125, Final Batch Loss: 0.015644105151295662\n",
      "Epoch 4830, Loss: 0.030069585889577866, Final Batch Loss: 0.0222391989082098\n",
      "Epoch 4831, Loss: 0.05698934756219387, Final Batch Loss: 0.026505816727876663\n",
      "Epoch 4832, Loss: 0.0861884243786335, Final Batch Loss: 0.05240318551659584\n",
      "Epoch 4833, Loss: 0.09217053651809692, Final Batch Loss: 0.047973912209272385\n",
      "Epoch 4834, Loss: 0.09023015201091766, Final Batch Loss: 0.0322495698928833\n",
      "Epoch 4835, Loss: 0.1201564334332943, Final Batch Loss: 0.0883469209074974\n",
      "Epoch 4836, Loss: 0.10368507727980614, Final Batch Loss: 0.06799331307411194\n",
      "Epoch 4837, Loss: 0.06656767427921295, Final Batch Loss: 0.029092125594615936\n",
      "Epoch 4838, Loss: 0.0971879381686449, Final Batch Loss: 0.02737797237932682\n",
      "Epoch 4839, Loss: 0.06394220516085625, Final Batch Loss: 0.03744502738118172\n",
      "Epoch 4840, Loss: 0.06435196846723557, Final Batch Loss: 0.03445613384246826\n",
      "Epoch 4841, Loss: 0.04845665115863085, Final Batch Loss: 0.014339336194097996\n",
      "Epoch 4842, Loss: 0.07526973634958267, Final Batch Loss: 0.0331023707985878\n",
      "Epoch 4843, Loss: 0.07149255461990833, Final Batch Loss: 0.02512436918914318\n",
      "Epoch 4844, Loss: 0.08695507422089577, Final Batch Loss: 0.049766574054956436\n",
      "Epoch 4845, Loss: 0.0649874247610569, Final Batch Loss: 0.03398202732205391\n",
      "Epoch 4846, Loss: 0.0580633208155632, Final Batch Loss: 0.03662782907485962\n",
      "Epoch 4847, Loss: 0.12610069662332535, Final Batch Loss: 0.06949642300605774\n",
      "Epoch 4848, Loss: 0.1023451779037714, Final Batch Loss: 0.07470183074474335\n",
      "Epoch 4849, Loss: 0.08773933537304401, Final Batch Loss: 0.02440338023006916\n",
      "Epoch 4850, Loss: 0.06964495405554771, Final Batch Loss: 0.0387069433927536\n",
      "Epoch 4851, Loss: 0.070202911272645, Final Batch Loss: 0.04007076099514961\n",
      "Epoch 4852, Loss: 0.08710446208715439, Final Batch Loss: 0.03146999329328537\n",
      "Epoch 4853, Loss: 0.06502179428935051, Final Batch Loss: 0.028372038155794144\n",
      "Epoch 4854, Loss: 0.13011402264237404, Final Batch Loss: 0.07691982388496399\n",
      "Epoch 4855, Loss: 0.1210126131772995, Final Batch Loss: 0.046251289546489716\n",
      "Epoch 4856, Loss: 0.08711516484618187, Final Batch Loss: 0.039432343095541\n",
      "Epoch 4857, Loss: 0.12665239721536636, Final Batch Loss: 0.09669876098632812\n",
      "Epoch 4858, Loss: 0.05349218472838402, Final Batch Loss: 0.024326052516698837\n",
      "Epoch 4859, Loss: 0.08028483018279076, Final Batch Loss: 0.032988324761390686\n",
      "Epoch 4860, Loss: 0.08208625018596649, Final Batch Loss: 0.03208128362894058\n",
      "Epoch 4861, Loss: 0.07323253154754639, Final Batch Loss: 0.047501131892204285\n",
      "Epoch 4862, Loss: 0.08682901784777641, Final Batch Loss: 0.05162191390991211\n",
      "Epoch 4863, Loss: 0.06088351644575596, Final Batch Loss: 0.02029658667743206\n",
      "Epoch 4864, Loss: 0.0829563457518816, Final Batch Loss: 0.05626559257507324\n",
      "Epoch 4865, Loss: 0.0838621687144041, Final Batch Loss: 0.052945442497730255\n",
      "Epoch 4866, Loss: 0.06593544781208038, Final Batch Loss: 0.0251789353787899\n",
      "Epoch 4867, Loss: 0.10707242041826248, Final Batch Loss: 0.0666438564658165\n",
      "Epoch 4868, Loss: 0.05019889585673809, Final Batch Loss: 0.03178497403860092\n",
      "Epoch 4869, Loss: 0.09150646440684795, Final Batch Loss: 0.016006654128432274\n",
      "Epoch 4870, Loss: 0.043237753212451935, Final Batch Loss: 0.024112727493047714\n",
      "Epoch 4871, Loss: 0.035755244083702564, Final Batch Loss: 0.02180524356663227\n",
      "Epoch 4872, Loss: 0.12867945432662964, Final Batch Loss: 0.0638858750462532\n",
      "Epoch 4873, Loss: 0.08283479511737823, Final Batch Loss: 0.06942642480134964\n",
      "Epoch 4874, Loss: 0.04807674139738083, Final Batch Loss: 0.026813991367816925\n",
      "Epoch 4875, Loss: 0.048155197873711586, Final Batch Loss: 0.021007351577281952\n",
      "Epoch 4876, Loss: 0.06963801756501198, Final Batch Loss: 0.037799954414367676\n",
      "Epoch 4877, Loss: 0.15701914951205254, Final Batch Loss: 0.11853805929422379\n",
      "Epoch 4878, Loss: 0.05731433629989624, Final Batch Loss: 0.029830891638994217\n",
      "Epoch 4879, Loss: 0.05294191278517246, Final Batch Loss: 0.0303515437990427\n",
      "Epoch 4880, Loss: 0.07143914885818958, Final Batch Loss: 0.04614634066820145\n",
      "Epoch 4881, Loss: 0.10455569066107273, Final Batch Loss: 0.02502112276852131\n",
      "Epoch 4882, Loss: 0.06704556383192539, Final Batch Loss: 0.021704038605093956\n",
      "Epoch 4883, Loss: 0.06200913153588772, Final Batch Loss: 0.03587982803583145\n",
      "Epoch 4884, Loss: 0.11292801052331924, Final Batch Loss: 0.052279163151979446\n",
      "Epoch 4885, Loss: 0.08122008107602596, Final Batch Loss: 0.019772397354245186\n",
      "Epoch 4886, Loss: 0.07715015858411789, Final Batch Loss: 0.05116163194179535\n",
      "Epoch 4887, Loss: 0.11030998453497887, Final Batch Loss: 0.04015597328543663\n",
      "Epoch 4888, Loss: 0.136268001049757, Final Batch Loss: 0.08887539058923721\n",
      "Epoch 4889, Loss: 0.07525241188704967, Final Batch Loss: 0.029994195327162743\n",
      "Epoch 4890, Loss: 0.06066734716296196, Final Batch Loss: 0.018672622740268707\n",
      "Epoch 4891, Loss: 0.0840789582580328, Final Batch Loss: 0.057551998645067215\n",
      "Epoch 4892, Loss: 0.14293324947357178, Final Batch Loss: 0.0657738745212555\n",
      "Epoch 4893, Loss: 0.08464349061250687, Final Batch Loss: 0.03004869446158409\n",
      "Epoch 4894, Loss: 0.0731098297983408, Final Batch Loss: 0.021465053781867027\n",
      "Epoch 4895, Loss: 0.10887063294649124, Final Batch Loss: 0.05545138195157051\n",
      "Epoch 4896, Loss: 0.11104423925280571, Final Batch Loss: 0.05315348133444786\n",
      "Epoch 4897, Loss: 0.07780524902045727, Final Batch Loss: 0.02918274886906147\n",
      "Epoch 4898, Loss: 0.07566116750240326, Final Batch Loss: 0.031835488975048065\n",
      "Epoch 4899, Loss: 0.06630059517920017, Final Batch Loss: 0.039688244462013245\n",
      "Epoch 4900, Loss: 0.04506005719304085, Final Batch Loss: 0.015810688957571983\n",
      "Epoch 4901, Loss: 0.09681066311895847, Final Batch Loss: 0.031001562252640724\n",
      "Epoch 4902, Loss: 0.08543188124895096, Final Batch Loss: 0.04257255420088768\n",
      "Epoch 4903, Loss: 0.09892460145056248, Final Batch Loss: 0.02449052967131138\n",
      "Epoch 4904, Loss: 0.08833110705018044, Final Batch Loss: 0.04083350673317909\n",
      "Epoch 4905, Loss: 0.0541263110935688, Final Batch Loss: 0.03461357578635216\n",
      "Epoch 4906, Loss: 0.12993551790714264, Final Batch Loss: 0.06327815353870392\n",
      "Epoch 4907, Loss: 0.0831546038389206, Final Batch Loss: 0.05289673060178757\n",
      "Epoch 4908, Loss: 0.05389733798801899, Final Batch Loss: 0.018954718485474586\n",
      "Epoch 4909, Loss: 0.055974981747567654, Final Batch Loss: 0.01445504929870367\n",
      "Epoch 4910, Loss: 0.0547319520264864, Final Batch Loss: 0.03638304024934769\n",
      "Epoch 4911, Loss: 0.05624336004257202, Final Batch Loss: 0.02120247483253479\n",
      "Epoch 4912, Loss: 0.08987100794911385, Final Batch Loss: 0.0404043011367321\n",
      "Epoch 4913, Loss: 0.06095537170767784, Final Batch Loss: 0.03325899690389633\n",
      "Epoch 4914, Loss: 0.06579937227070332, Final Batch Loss: 0.027270643040537834\n",
      "Epoch 4915, Loss: 0.04199481010437012, Final Batch Loss: 0.02457653172314167\n",
      "Epoch 4916, Loss: 0.06708377227187157, Final Batch Loss: 0.030428718775510788\n",
      "Epoch 4917, Loss: 0.050733717158436775, Final Batch Loss: 0.022290106862783432\n",
      "Epoch 4918, Loss: 0.09425806626677513, Final Batch Loss: 0.04350544884800911\n",
      "Epoch 4919, Loss: 0.05353479087352753, Final Batch Loss: 0.025786524638533592\n",
      "Epoch 4920, Loss: 0.05410924553871155, Final Batch Loss: 0.02991907112300396\n",
      "Epoch 4921, Loss: 0.07903170585632324, Final Batch Loss: 0.027598440647125244\n",
      "Epoch 4922, Loss: 0.07536314614117146, Final Batch Loss: 0.0296670850366354\n",
      "Epoch 4923, Loss: 0.07121289893984795, Final Batch Loss: 0.032212160527706146\n",
      "Epoch 4924, Loss: 0.09665201604366302, Final Batch Loss: 0.04620334133505821\n",
      "Epoch 4925, Loss: 0.06689072772860527, Final Batch Loss: 0.021451324224472046\n",
      "Epoch 4926, Loss: 0.07283768430352211, Final Batch Loss: 0.028478939086198807\n",
      "Epoch 4927, Loss: 0.08440774120390415, Final Batch Loss: 0.06718800216913223\n",
      "Epoch 4928, Loss: 0.05684562399983406, Final Batch Loss: 0.03011741302907467\n",
      "Epoch 4929, Loss: 0.13473188132047653, Final Batch Loss: 0.10191664844751358\n",
      "Epoch 4930, Loss: 0.14432990550994873, Final Batch Loss: 0.06402057409286499\n",
      "Epoch 4931, Loss: 0.08762969821691513, Final Batch Loss: 0.033745940774679184\n",
      "Epoch 4932, Loss: 0.09508445113897324, Final Batch Loss: 0.04359624534845352\n",
      "Epoch 4933, Loss: 0.1516638919711113, Final Batch Loss: 0.0637526586651802\n",
      "Epoch 4934, Loss: 0.06847918033599854, Final Batch Loss: 0.046750959008932114\n",
      "Epoch 4935, Loss: 0.0559678990393877, Final Batch Loss: 0.03201545029878616\n",
      "Epoch 4936, Loss: 0.1038702093064785, Final Batch Loss: 0.039753127843141556\n",
      "Epoch 4937, Loss: 0.11758125387132168, Final Batch Loss: 0.0300781037658453\n",
      "Epoch 4938, Loss: 0.05900649353861809, Final Batch Loss: 0.02001536265015602\n",
      "Epoch 4939, Loss: 0.11597880348563194, Final Batch Loss: 0.0806158110499382\n",
      "Epoch 4940, Loss: 0.08139309659600258, Final Batch Loss: 0.0462973490357399\n",
      "Epoch 4941, Loss: 0.06521408632397652, Final Batch Loss: 0.03289024904370308\n",
      "Epoch 4942, Loss: 0.0964277870953083, Final Batch Loss: 0.0630686953663826\n",
      "Epoch 4943, Loss: 0.13752247020602226, Final Batch Loss: 0.05586186423897743\n",
      "Epoch 4944, Loss: 0.09258202277123928, Final Batch Loss: 0.02807684801518917\n",
      "Epoch 4945, Loss: 0.12935885041952133, Final Batch Loss: 0.06518407166004181\n",
      "Epoch 4946, Loss: 0.06915267556905746, Final Batch Loss: 0.04032210633158684\n",
      "Epoch 4947, Loss: 0.06949323788285255, Final Batch Loss: 0.04143882915377617\n",
      "Epoch 4948, Loss: 0.12332561425864697, Final Batch Loss: 0.02803834341466427\n",
      "Epoch 4949, Loss: 0.07387011684477329, Final Batch Loss: 0.04328860715031624\n",
      "Epoch 4950, Loss: 0.04754596948623657, Final Batch Loss: 0.02626793272793293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4951, Loss: 0.07317297533154488, Final Batch Loss: 0.03288641571998596\n",
      "Epoch 4952, Loss: 0.07979118824005127, Final Batch Loss: 0.035257816314697266\n",
      "Epoch 4953, Loss: 0.12986154109239578, Final Batch Loss: 0.04599081724882126\n",
      "Epoch 4954, Loss: 0.05069077294319868, Final Batch Loss: 0.037577349692583084\n",
      "Epoch 4955, Loss: 0.08713259175419807, Final Batch Loss: 0.05510934814810753\n",
      "Epoch 4956, Loss: 0.06549069285392761, Final Batch Loss: 0.043093547224998474\n",
      "Epoch 4957, Loss: 0.07536744512617588, Final Batch Loss: 0.022392531856894493\n",
      "Epoch 4958, Loss: 0.13777538016438484, Final Batch Loss: 0.05249795690178871\n",
      "Epoch 4959, Loss: 0.0783931165933609, Final Batch Loss: 0.038721341639757156\n",
      "Epoch 4960, Loss: 0.048643749207258224, Final Batch Loss: 0.022084470838308334\n",
      "Epoch 4961, Loss: 0.07036515697836876, Final Batch Loss: 0.03692806139588356\n",
      "Epoch 4962, Loss: 0.062161264941096306, Final Batch Loss: 0.032004568725824356\n",
      "Epoch 4963, Loss: 0.06536144576966763, Final Batch Loss: 0.02677128277719021\n",
      "Epoch 4964, Loss: 0.0792628824710846, Final Batch Loss: 0.02393340691924095\n",
      "Epoch 4965, Loss: 0.06899328343570232, Final Batch Loss: 0.044346850365400314\n",
      "Epoch 4966, Loss: 0.06574864685535431, Final Batch Loss: 0.031040821224451065\n",
      "Epoch 4967, Loss: 0.04822554625570774, Final Batch Loss: 0.018669936805963516\n",
      "Epoch 4968, Loss: 0.13180842250585556, Final Batch Loss: 0.02956271916627884\n",
      "Epoch 4969, Loss: 0.14880561456084251, Final Batch Loss: 0.09368376433849335\n",
      "Epoch 4970, Loss: 0.11161389201879501, Final Batch Loss: 0.05076249688863754\n",
      "Epoch 4971, Loss: 0.04720249492675066, Final Batch Loss: 0.011791312135756016\n",
      "Epoch 4972, Loss: 0.040773022919893265, Final Batch Loss: 0.02102525904774666\n",
      "Epoch 4973, Loss: 0.06223404407501221, Final Batch Loss: 0.03302557021379471\n",
      "Epoch 4974, Loss: 0.05273849703371525, Final Batch Loss: 0.013898095116019249\n",
      "Epoch 4975, Loss: 0.14036378636956215, Final Batch Loss: 0.11296529322862625\n",
      "Epoch 4976, Loss: 0.0971294529736042, Final Batch Loss: 0.04413038492202759\n",
      "Epoch 4977, Loss: 0.09435528889298439, Final Batch Loss: 0.034986790269613266\n",
      "Epoch 4978, Loss: 0.06749068200588226, Final Batch Loss: 0.033999547362327576\n",
      "Epoch 4979, Loss: 0.07244675233960152, Final Batch Loss: 0.03998998925089836\n",
      "Epoch 4980, Loss: 0.07580918446183205, Final Batch Loss: 0.03267117217183113\n",
      "Epoch 4981, Loss: 0.053684903774410486, Final Batch Loss: 0.007019514683634043\n",
      "Epoch 4982, Loss: 0.12996554002165794, Final Batch Loss: 0.06924986094236374\n",
      "Epoch 4983, Loss: 0.08289335668087006, Final Batch Loss: 0.045098885893821716\n",
      "Epoch 4984, Loss: 0.1451745107769966, Final Batch Loss: 0.09053288400173187\n",
      "Epoch 4985, Loss: 0.046365730464458466, Final Batch Loss: 0.023949168622493744\n",
      "Epoch 4986, Loss: 0.07398051954805851, Final Batch Loss: 0.02517126314342022\n",
      "Epoch 4987, Loss: 0.10166497528553009, Final Batch Loss: 0.04263141006231308\n",
      "Epoch 4988, Loss: 0.05972171016037464, Final Batch Loss: 0.029397286474704742\n",
      "Epoch 4989, Loss: 0.088467326015234, Final Batch Loss: 0.04597281292080879\n",
      "Epoch 4990, Loss: 0.07072066701948643, Final Batch Loss: 0.028055796399712563\n",
      "Epoch 4991, Loss: 0.08955884352326393, Final Batch Loss: 0.052876878529787064\n",
      "Epoch 4992, Loss: 0.08480314537882805, Final Batch Loss: 0.04567430913448334\n",
      "Epoch 4993, Loss: 0.030445015989243984, Final Batch Loss: 0.021192975342273712\n",
      "Epoch 4994, Loss: 0.07201432064175606, Final Batch Loss: 0.035127636045217514\n",
      "Epoch 4995, Loss: 0.06849449500441551, Final Batch Loss: 0.02978084236383438\n",
      "Epoch 4996, Loss: 0.050987545400857925, Final Batch Loss: 0.021940868347883224\n",
      "Epoch 4997, Loss: 0.06690820679068565, Final Batch Loss: 0.03339274227619171\n",
      "Epoch 4998, Loss: 0.06335434876382351, Final Batch Loss: 0.02638375200331211\n",
      "Epoch 4999, Loss: 0.0882018692791462, Final Batch Loss: 0.04828175529837608\n",
      "Epoch 5000, Loss: 0.09759548678994179, Final Batch Loss: 0.03958660364151001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0  0  0  1  0  0  0  0]\n",
      " [ 5 17  0  0 13  0  0  0  0]\n",
      " [ 0  0 19  0  0 11  0  0  5]\n",
      " [23  0  0  9  1  2  0  0  0]\n",
      " [ 3  0  0  0 25  0  7  0  0]\n",
      " [ 0  0  0  5  0 30  0  0  0]\n",
      " [11  0  0  1  5  0 18  0  0]\n",
      " [ 0  0  1  0  0  0  0 34  0]\n",
      " [ 0  0 20  0  0  0  0  0 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.44737   0.97143   0.61261        35\n",
      "         1.0    1.00000   0.48571   0.65385        35\n",
      "         2.0    0.47500   0.54286   0.50667        35\n",
      "         3.0    0.60000   0.25714   0.36000        35\n",
      "         4.0    0.55556   0.71429   0.62500        35\n",
      "         5.0    0.69767   0.85714   0.76923        35\n",
      "         6.0    0.72000   0.51429   0.60000        35\n",
      "         7.0    1.00000   0.97143   0.98551        35\n",
      "         8.0    0.75000   0.42857   0.54545        35\n",
      "\n",
      "    accuracy                        0.63810       315\n",
      "   macro avg    0.69396   0.63810   0.62870       315\n",
      "weighted avg    0.69396   0.63810   0.62870       315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7660333333333335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET RID OF 2, 5, 8 for JUST DYNAMIC\n",
    "(0.61261+0.65385+0.35+0.625+0.6+0.98551+0.76923)/6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
