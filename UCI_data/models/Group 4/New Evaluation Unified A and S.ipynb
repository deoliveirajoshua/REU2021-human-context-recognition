{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [19, 21, 22]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.433591365814209, Final Batch Loss: 2.2009265422821045\n",
      "Epoch 2, Loss: 4.43070650100708, Final Batch Loss: 2.2102818489074707\n",
      "Epoch 3, Loss: 4.425504207611084, Final Batch Loss: 2.209519386291504\n",
      "Epoch 4, Loss: 4.432607412338257, Final Batch Loss: 2.224916458129883\n",
      "Epoch 5, Loss: 4.428786993026733, Final Batch Loss: 2.2219576835632324\n",
      "Epoch 6, Loss: 4.411971092224121, Final Batch Loss: 2.1971659660339355\n",
      "Epoch 7, Loss: 4.417679071426392, Final Batch Loss: 2.2170660495758057\n",
      "Epoch 8, Loss: 4.414139032363892, Final Batch Loss: 2.2132935523986816\n",
      "Epoch 9, Loss: 4.395273447036743, Final Batch Loss: 2.190516710281372\n",
      "Epoch 10, Loss: 4.396904706954956, Final Batch Loss: 2.205063581466675\n",
      "Epoch 11, Loss: 4.379371643066406, Final Batch Loss: 2.1775155067443848\n",
      "Epoch 12, Loss: 4.3698976039886475, Final Batch Loss: 2.1799354553222656\n",
      "Epoch 13, Loss: 4.358470916748047, Final Batch Loss: 2.1809964179992676\n",
      "Epoch 14, Loss: 4.341734886169434, Final Batch Loss: 2.16286301612854\n",
      "Epoch 15, Loss: 4.332390546798706, Final Batch Loss: 2.171901226043701\n",
      "Epoch 16, Loss: 4.306464433670044, Final Batch Loss: 2.145651340484619\n",
      "Epoch 17, Loss: 4.291182518005371, Final Batch Loss: 2.1553070545196533\n",
      "Epoch 18, Loss: 4.28350043296814, Final Batch Loss: 2.1238014698028564\n",
      "Epoch 19, Loss: 4.252483606338501, Final Batch Loss: 2.116461992263794\n",
      "Epoch 20, Loss: 4.223773241043091, Final Batch Loss: 2.1014347076416016\n",
      "Epoch 21, Loss: 4.227542877197266, Final Batch Loss: 2.1313323974609375\n",
      "Epoch 22, Loss: 4.176911354064941, Final Batch Loss: 2.070134162902832\n",
      "Epoch 23, Loss: 4.143157243728638, Final Batch Loss: 2.0181632041931152\n",
      "Epoch 24, Loss: 4.10617208480835, Final Batch Loss: 2.0285284519195557\n",
      "Epoch 25, Loss: 4.104789972305298, Final Batch Loss: 2.0429036617279053\n",
      "Epoch 26, Loss: 4.091540813446045, Final Batch Loss: 2.038832902908325\n",
      "Epoch 27, Loss: 4.06679630279541, Final Batch Loss: 2.0390841960906982\n",
      "Epoch 28, Loss: 4.017268896102905, Final Batch Loss: 2.020221710205078\n",
      "Epoch 29, Loss: 3.968507409095764, Final Batch Loss: 1.9831558465957642\n",
      "Epoch 30, Loss: 3.9460967779159546, Final Batch Loss: 1.9599615335464478\n",
      "Epoch 31, Loss: 3.9060468673706055, Final Batch Loss: 1.931797742843628\n",
      "Epoch 32, Loss: 3.917494297027588, Final Batch Loss: 1.9695305824279785\n",
      "Epoch 33, Loss: 3.7990537881851196, Final Batch Loss: 1.860054612159729\n",
      "Epoch 34, Loss: 3.81693172454834, Final Batch Loss: 1.9440674781799316\n",
      "Epoch 35, Loss: 3.8024964332580566, Final Batch Loss: 1.910528302192688\n",
      "Epoch 36, Loss: 3.657287120819092, Final Batch Loss: 1.7816526889801025\n",
      "Epoch 37, Loss: 3.6911109685897827, Final Batch Loss: 1.8559656143188477\n",
      "Epoch 38, Loss: 3.6069835424423218, Final Batch Loss: 1.7682609558105469\n",
      "Epoch 39, Loss: 3.6067975759506226, Final Batch Loss: 1.8422561883926392\n",
      "Epoch 40, Loss: 3.5528576374053955, Final Batch Loss: 1.7708446979522705\n",
      "Epoch 41, Loss: 3.5360500812530518, Final Batch Loss: 1.7657387256622314\n",
      "Epoch 42, Loss: 3.4494857788085938, Final Batch Loss: 1.7198917865753174\n",
      "Epoch 43, Loss: 3.389800190925598, Final Batch Loss: 1.6970359086990356\n",
      "Epoch 44, Loss: 3.3849047422409058, Final Batch Loss: 1.6532245874404907\n",
      "Epoch 45, Loss: 3.366286039352417, Final Batch Loss: 1.6985644102096558\n",
      "Epoch 46, Loss: 3.3176751136779785, Final Batch Loss: 1.6410225629806519\n",
      "Epoch 47, Loss: 3.304524302482605, Final Batch Loss: 1.685010552406311\n",
      "Epoch 48, Loss: 3.2324479818344116, Final Batch Loss: 1.6273964643478394\n",
      "Epoch 49, Loss: 3.179316997528076, Final Batch Loss: 1.5644874572753906\n",
      "Epoch 50, Loss: 3.2047696113586426, Final Batch Loss: 1.6471890211105347\n",
      "Epoch 51, Loss: 3.1335355043411255, Final Batch Loss: 1.551287293434143\n",
      "Epoch 52, Loss: 3.0988121032714844, Final Batch Loss: 1.5601353645324707\n",
      "Epoch 53, Loss: 3.065239906311035, Final Batch Loss: 1.5158627033233643\n",
      "Epoch 54, Loss: 3.133151650428772, Final Batch Loss: 1.6070761680603027\n",
      "Epoch 55, Loss: 3.1102250814437866, Final Batch Loss: 1.5574302673339844\n",
      "Epoch 56, Loss: 3.0343596935272217, Final Batch Loss: 1.4943965673446655\n",
      "Epoch 57, Loss: 2.9882816076278687, Final Batch Loss: 1.482351541519165\n",
      "Epoch 58, Loss: 2.9832961559295654, Final Batch Loss: 1.4865728616714478\n",
      "Epoch 59, Loss: 3.0470551252365112, Final Batch Loss: 1.5227775573730469\n",
      "Epoch 60, Loss: 2.9588884115219116, Final Batch Loss: 1.4805915355682373\n",
      "Epoch 61, Loss: 2.9451645612716675, Final Batch Loss: 1.4707762002944946\n",
      "Epoch 62, Loss: 2.8998302221298218, Final Batch Loss: 1.460707426071167\n",
      "Epoch 63, Loss: 2.887872815132141, Final Batch Loss: 1.4365198612213135\n",
      "Epoch 64, Loss: 2.8201661109924316, Final Batch Loss: 1.4287242889404297\n",
      "Epoch 65, Loss: 2.845542788505554, Final Batch Loss: 1.4420280456542969\n",
      "Epoch 66, Loss: 2.776562452316284, Final Batch Loss: 1.3710129261016846\n",
      "Epoch 67, Loss: 2.8078486919403076, Final Batch Loss: 1.4225279092788696\n",
      "Epoch 68, Loss: 2.7685136795043945, Final Batch Loss: 1.3755613565444946\n",
      "Epoch 69, Loss: 2.7978986501693726, Final Batch Loss: 1.4439902305603027\n",
      "Epoch 70, Loss: 2.753712773323059, Final Batch Loss: 1.4364618062973022\n",
      "Epoch 71, Loss: 2.7222816944122314, Final Batch Loss: 1.447219967842102\n",
      "Epoch 72, Loss: 2.7287888526916504, Final Batch Loss: 1.3237016201019287\n",
      "Epoch 73, Loss: 2.6989234685897827, Final Batch Loss: 1.389696717262268\n",
      "Epoch 74, Loss: 2.6395615339279175, Final Batch Loss: 1.3529911041259766\n",
      "Epoch 75, Loss: 2.626089334487915, Final Batch Loss: 1.3134808540344238\n",
      "Epoch 76, Loss: 2.5514899492263794, Final Batch Loss: 1.2979453802108765\n",
      "Epoch 77, Loss: 2.518177628517151, Final Batch Loss: 1.2518683671951294\n",
      "Epoch 78, Loss: 2.439310073852539, Final Batch Loss: 1.3038039207458496\n",
      "Epoch 79, Loss: 2.4271349906921387, Final Batch Loss: 1.247209906578064\n",
      "Epoch 80, Loss: 2.4724459648132324, Final Batch Loss: 1.249602198600769\n",
      "Epoch 81, Loss: 2.4174455404281616, Final Batch Loss: 1.1840251684188843\n",
      "Epoch 82, Loss: 2.4397261142730713, Final Batch Loss: 1.2555828094482422\n",
      "Epoch 83, Loss: 2.3740732669830322, Final Batch Loss: 1.1839690208435059\n",
      "Epoch 84, Loss: 2.3776992559432983, Final Batch Loss: 1.2066785097122192\n",
      "Epoch 85, Loss: 2.2304623126983643, Final Batch Loss: 1.040689468383789\n",
      "Epoch 86, Loss: 2.2678799629211426, Final Batch Loss: 1.123329758644104\n",
      "Epoch 87, Loss: 2.1298683881759644, Final Batch Loss: 1.0398633480072021\n",
      "Epoch 88, Loss: 2.190820336341858, Final Batch Loss: 1.0773662328720093\n",
      "Epoch 89, Loss: 2.1763317584991455, Final Batch Loss: 1.135887622833252\n",
      "Epoch 90, Loss: 2.1450897455215454, Final Batch Loss: 1.0755046606063843\n",
      "Epoch 91, Loss: 2.085935115814209, Final Batch Loss: 1.0493730306625366\n",
      "Epoch 92, Loss: 2.1522339582443237, Final Batch Loss: 1.0836796760559082\n",
      "Epoch 93, Loss: 2.0622103810310364, Final Batch Loss: 1.0843623876571655\n",
      "Epoch 94, Loss: 2.087126851081848, Final Batch Loss: 1.0806901454925537\n",
      "Epoch 95, Loss: 2.0531871914863586, Final Batch Loss: 0.9552736878395081\n",
      "Epoch 96, Loss: 2.021596670150757, Final Batch Loss: 1.0086435079574585\n",
      "Epoch 97, Loss: 2.037330210208893, Final Batch Loss: 0.9793477654457092\n",
      "Epoch 98, Loss: 2.0169920325279236, Final Batch Loss: 1.0340023040771484\n",
      "Epoch 99, Loss: 2.130221366882324, Final Batch Loss: 1.1201814413070679\n",
      "Epoch 100, Loss: 2.016346037387848, Final Batch Loss: 0.9925183653831482\n",
      "Epoch 101, Loss: 2.012283504009247, Final Batch Loss: 1.0463268756866455\n",
      "Epoch 102, Loss: 1.9643443822860718, Final Batch Loss: 1.001423954963684\n",
      "Epoch 103, Loss: 1.9026715159416199, Final Batch Loss: 0.8923946022987366\n",
      "Epoch 104, Loss: 2.0449925661087036, Final Batch Loss: 1.0269057750701904\n",
      "Epoch 105, Loss: 1.9715267419815063, Final Batch Loss: 0.9735646843910217\n",
      "Epoch 106, Loss: 1.9383611679077148, Final Batch Loss: 0.9597442150115967\n",
      "Epoch 107, Loss: 1.8792833089828491, Final Batch Loss: 0.8909977078437805\n",
      "Epoch 108, Loss: 2.037415027618408, Final Batch Loss: 1.0167524814605713\n",
      "Epoch 109, Loss: 1.9667388796806335, Final Batch Loss: 0.9597813487052917\n",
      "Epoch 110, Loss: 1.8911694884300232, Final Batch Loss: 0.8967598676681519\n",
      "Epoch 111, Loss: 2.0572986006736755, Final Batch Loss: 1.0702780485153198\n",
      "Epoch 112, Loss: 1.8913627862930298, Final Batch Loss: 1.0011234283447266\n",
      "Epoch 113, Loss: 1.8320185542106628, Final Batch Loss: 0.9447693228721619\n",
      "Epoch 114, Loss: 1.8808608651161194, Final Batch Loss: 0.9502504467964172\n",
      "Epoch 115, Loss: 1.8630839586257935, Final Batch Loss: 0.9056009650230408\n",
      "Epoch 116, Loss: 1.763361632823944, Final Batch Loss: 0.8290125727653503\n",
      "Epoch 117, Loss: 1.761709749698639, Final Batch Loss: 0.8679054975509644\n",
      "Epoch 118, Loss: 1.8162055611610413, Final Batch Loss: 0.9597550630569458\n",
      "Epoch 119, Loss: 1.8352220058441162, Final Batch Loss: 0.9073543548583984\n",
      "Epoch 120, Loss: 1.8173942565917969, Final Batch Loss: 0.8832727074623108\n",
      "Epoch 121, Loss: 1.8694657683372498, Final Batch Loss: 0.8822544813156128\n",
      "Epoch 122, Loss: 1.81117182970047, Final Batch Loss: 0.8735640048980713\n",
      "Epoch 123, Loss: 1.8319034576416016, Final Batch Loss: 0.9203616976737976\n",
      "Epoch 124, Loss: 1.80619478225708, Final Batch Loss: 0.924396812915802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 1.7552944421768188, Final Batch Loss: 0.8471106886863708\n",
      "Epoch 126, Loss: 1.8944051265716553, Final Batch Loss: 0.9484320878982544\n",
      "Epoch 127, Loss: 1.7736896872520447, Final Batch Loss: 0.8538005352020264\n",
      "Epoch 128, Loss: 1.7447181940078735, Final Batch Loss: 0.809695839881897\n",
      "Epoch 129, Loss: 1.8987371325492859, Final Batch Loss: 1.0221452713012695\n",
      "Epoch 130, Loss: 1.891821265220642, Final Batch Loss: 0.9676063656806946\n",
      "Epoch 131, Loss: 1.7152700424194336, Final Batch Loss: 0.7861785292625427\n",
      "Epoch 132, Loss: 1.7554535269737244, Final Batch Loss: 0.8675905466079712\n",
      "Epoch 133, Loss: 1.8303781151771545, Final Batch Loss: 0.9667190909385681\n",
      "Epoch 134, Loss: 1.699815273284912, Final Batch Loss: 0.8610339164733887\n",
      "Epoch 135, Loss: 1.8706289529800415, Final Batch Loss: 0.9673155546188354\n",
      "Epoch 136, Loss: 1.8161274790763855, Final Batch Loss: 0.9290751218795776\n",
      "Epoch 137, Loss: 1.7020591497421265, Final Batch Loss: 0.8892269134521484\n",
      "Epoch 138, Loss: 1.8155362606048584, Final Batch Loss: 0.8990786075592041\n",
      "Epoch 139, Loss: 1.77108895778656, Final Batch Loss: 0.868754506111145\n",
      "Epoch 140, Loss: 1.7227489352226257, Final Batch Loss: 0.8176185488700867\n",
      "Epoch 141, Loss: 1.8285813927650452, Final Batch Loss: 0.9681761860847473\n",
      "Epoch 142, Loss: 1.7379059195518494, Final Batch Loss: 0.8259331583976746\n",
      "Epoch 143, Loss: 1.7525445818901062, Final Batch Loss: 0.8816704154014587\n",
      "Epoch 144, Loss: 1.785463571548462, Final Batch Loss: 0.8879317045211792\n",
      "Epoch 145, Loss: 1.6982494592666626, Final Batch Loss: 0.8665236830711365\n",
      "Epoch 146, Loss: 1.7603482604026794, Final Batch Loss: 0.8924350738525391\n",
      "Epoch 147, Loss: 1.6594589948654175, Final Batch Loss: 0.7860915660858154\n",
      "Epoch 148, Loss: 1.6881543397903442, Final Batch Loss: 0.8420321941375732\n",
      "Epoch 149, Loss: 1.6954048871994019, Final Batch Loss: 0.8155815005302429\n",
      "Epoch 150, Loss: 1.7596880793571472, Final Batch Loss: 0.8835023045539856\n",
      "Epoch 151, Loss: 1.6490440964698792, Final Batch Loss: 0.7951304316520691\n",
      "Epoch 152, Loss: 1.7389808297157288, Final Batch Loss: 0.8830122351646423\n",
      "Epoch 153, Loss: 1.7542784214019775, Final Batch Loss: 0.8871336579322815\n",
      "Epoch 154, Loss: 1.7419148683547974, Final Batch Loss: 0.8698228001594543\n",
      "Epoch 155, Loss: 1.6967787742614746, Final Batch Loss: 0.8675305247306824\n",
      "Epoch 156, Loss: 1.612509846687317, Final Batch Loss: 0.7410035729408264\n",
      "Epoch 157, Loss: 1.6542581915855408, Final Batch Loss: 0.7982938885688782\n",
      "Epoch 158, Loss: 1.6578096151351929, Final Batch Loss: 0.8614744544029236\n",
      "Epoch 159, Loss: 1.694963812828064, Final Batch Loss: 0.872631847858429\n",
      "Epoch 160, Loss: 1.7594574689865112, Final Batch Loss: 0.8818027973175049\n",
      "Epoch 161, Loss: 1.6641948819160461, Final Batch Loss: 0.7365220785140991\n",
      "Epoch 162, Loss: 1.6399775743484497, Final Batch Loss: 0.8300921320915222\n",
      "Epoch 163, Loss: 1.7073017954826355, Final Batch Loss: 0.8535341024398804\n",
      "Epoch 164, Loss: 1.71793931722641, Final Batch Loss: 0.7992311120033264\n",
      "Epoch 165, Loss: 1.6247721910476685, Final Batch Loss: 0.8664769530296326\n",
      "Epoch 166, Loss: 1.72770094871521, Final Batch Loss: 0.9005536437034607\n",
      "Epoch 167, Loss: 1.6306933164596558, Final Batch Loss: 0.8425624966621399\n",
      "Epoch 168, Loss: 1.6537132263183594, Final Batch Loss: 0.8267346620559692\n",
      "Epoch 169, Loss: 1.5663374662399292, Final Batch Loss: 0.7594996690750122\n",
      "Epoch 170, Loss: 1.5679185390472412, Final Batch Loss: 0.737693727016449\n",
      "Epoch 171, Loss: 1.6870188117027283, Final Batch Loss: 0.8871615529060364\n",
      "Epoch 172, Loss: 1.595106065273285, Final Batch Loss: 0.7652742862701416\n",
      "Epoch 173, Loss: 1.6377540230751038, Final Batch Loss: 0.8367511034011841\n",
      "Epoch 174, Loss: 1.762794017791748, Final Batch Loss: 0.9289867877960205\n",
      "Epoch 175, Loss: 1.7022054195404053, Final Batch Loss: 0.8372236490249634\n",
      "Epoch 176, Loss: 1.6651489734649658, Final Batch Loss: 0.7934421300888062\n",
      "Epoch 177, Loss: 1.687232792377472, Final Batch Loss: 0.9007768034934998\n",
      "Epoch 178, Loss: 1.6912707090377808, Final Batch Loss: 0.8550156354904175\n",
      "Epoch 179, Loss: 1.5414262413978577, Final Batch Loss: 0.7594037055969238\n",
      "Epoch 180, Loss: 1.5833008289337158, Final Batch Loss: 0.7969461679458618\n",
      "Epoch 181, Loss: 1.6339772939682007, Final Batch Loss: 0.8200687766075134\n",
      "Epoch 182, Loss: 1.6297634840011597, Final Batch Loss: 0.8686410188674927\n",
      "Epoch 183, Loss: 1.72374027967453, Final Batch Loss: 0.9065337777137756\n",
      "Epoch 184, Loss: 1.5661697387695312, Final Batch Loss: 0.7852485179901123\n",
      "Epoch 185, Loss: 1.62049800157547, Final Batch Loss: 0.8012009859085083\n",
      "Epoch 186, Loss: 1.6556660532951355, Final Batch Loss: 0.8213005065917969\n",
      "Epoch 187, Loss: 1.642654538154602, Final Batch Loss: 0.8589795231819153\n",
      "Epoch 188, Loss: 1.6497343182563782, Final Batch Loss: 0.8784814476966858\n",
      "Epoch 189, Loss: 1.5143927335739136, Final Batch Loss: 0.6835760474205017\n",
      "Epoch 190, Loss: 1.6437793970108032, Final Batch Loss: 0.8452814221382141\n",
      "Epoch 191, Loss: 1.5627257823944092, Final Batch Loss: 0.7293869256973267\n",
      "Epoch 192, Loss: 1.5348403453826904, Final Batch Loss: 0.7240151762962341\n",
      "Epoch 193, Loss: 1.5131085515022278, Final Batch Loss: 0.7062264680862427\n",
      "Epoch 194, Loss: 1.5990018844604492, Final Batch Loss: 0.8282259106636047\n",
      "Epoch 195, Loss: 1.5575581789016724, Final Batch Loss: 0.8136113882064819\n",
      "Epoch 196, Loss: 1.5254785418510437, Final Batch Loss: 0.7259010076522827\n",
      "Epoch 197, Loss: 1.6326338052749634, Final Batch Loss: 0.8324368000030518\n",
      "Epoch 198, Loss: 1.5941600799560547, Final Batch Loss: 0.7946206331253052\n",
      "Epoch 199, Loss: 1.6107035875320435, Final Batch Loss: 0.8102781176567078\n",
      "Epoch 200, Loss: 1.5094193816184998, Final Batch Loss: 0.7173895239830017\n",
      "Epoch 201, Loss: 1.6003814935684204, Final Batch Loss: 0.810070276260376\n",
      "Epoch 202, Loss: 1.5164591670036316, Final Batch Loss: 0.782952070236206\n",
      "Epoch 203, Loss: 1.5184041261672974, Final Batch Loss: 0.6798490285873413\n",
      "Epoch 204, Loss: 1.5504963994026184, Final Batch Loss: 0.8483241200447083\n",
      "Epoch 205, Loss: 1.581583857536316, Final Batch Loss: 0.7876628637313843\n",
      "Epoch 206, Loss: 1.5213454365730286, Final Batch Loss: 0.7690619230270386\n",
      "Epoch 207, Loss: 1.4879946112632751, Final Batch Loss: 0.7760377526283264\n",
      "Epoch 208, Loss: 1.5312010049819946, Final Batch Loss: 0.8247244954109192\n",
      "Epoch 209, Loss: 1.5664851069450378, Final Batch Loss: 0.8171830773353577\n",
      "Epoch 210, Loss: 1.5906427502632141, Final Batch Loss: 0.8408730626106262\n",
      "Epoch 211, Loss: 1.423613429069519, Final Batch Loss: 0.6310275197029114\n",
      "Epoch 212, Loss: 1.5262237787246704, Final Batch Loss: 0.7165113687515259\n",
      "Epoch 213, Loss: 1.5574756860733032, Final Batch Loss: 0.7933539748191833\n",
      "Epoch 214, Loss: 1.4474007487297058, Final Batch Loss: 0.6939509510993958\n",
      "Epoch 215, Loss: 1.5175288915634155, Final Batch Loss: 0.7335143685340881\n",
      "Epoch 216, Loss: 1.5309472680091858, Final Batch Loss: 0.7503107786178589\n",
      "Epoch 217, Loss: 1.5635266304016113, Final Batch Loss: 0.7922074198722839\n",
      "Epoch 218, Loss: 1.4929832816123962, Final Batch Loss: 0.7392898797988892\n",
      "Epoch 219, Loss: 1.4727630019187927, Final Batch Loss: 0.7267366051673889\n",
      "Epoch 220, Loss: 1.5487861633300781, Final Batch Loss: 0.753713071346283\n",
      "Epoch 221, Loss: 1.5404502749443054, Final Batch Loss: 0.728418231010437\n",
      "Epoch 222, Loss: 1.4797226786613464, Final Batch Loss: 0.6814767122268677\n",
      "Epoch 223, Loss: 1.4634883403778076, Final Batch Loss: 0.7539488077163696\n",
      "Epoch 224, Loss: 1.590482771396637, Final Batch Loss: 0.8719797134399414\n",
      "Epoch 225, Loss: 1.5345750451087952, Final Batch Loss: 0.7687247395515442\n",
      "Epoch 226, Loss: 1.4604021906852722, Final Batch Loss: 0.6576337814331055\n",
      "Epoch 227, Loss: 1.4518176913261414, Final Batch Loss: 0.7294796109199524\n",
      "Epoch 228, Loss: 1.4572175741195679, Final Batch Loss: 0.7111522555351257\n",
      "Epoch 229, Loss: 1.4841272830963135, Final Batch Loss: 0.7525741457939148\n",
      "Epoch 230, Loss: 1.4728424549102783, Final Batch Loss: 0.7398319244384766\n",
      "Epoch 231, Loss: 1.5389516949653625, Final Batch Loss: 0.7337696552276611\n",
      "Epoch 232, Loss: 1.4735931158065796, Final Batch Loss: 0.7097115516662598\n",
      "Epoch 233, Loss: 1.4849127531051636, Final Batch Loss: 0.7273741364479065\n",
      "Epoch 234, Loss: 1.489880919456482, Final Batch Loss: 0.7345561385154724\n",
      "Epoch 235, Loss: 1.5026346445083618, Final Batch Loss: 0.7481978535652161\n",
      "Epoch 236, Loss: 1.4210294485092163, Final Batch Loss: 0.6955282688140869\n",
      "Epoch 237, Loss: 1.5316333770751953, Final Batch Loss: 0.8347059488296509\n",
      "Epoch 238, Loss: 1.5119403004646301, Final Batch Loss: 0.7639522552490234\n",
      "Epoch 239, Loss: 1.4307888746261597, Final Batch Loss: 0.71872478723526\n",
      "Epoch 240, Loss: 1.4201674461364746, Final Batch Loss: 0.6762060523033142\n",
      "Epoch 241, Loss: 1.440196692943573, Final Batch Loss: 0.6535009741783142\n",
      "Epoch 242, Loss: 1.449506163597107, Final Batch Loss: 0.6785913109779358\n",
      "Epoch 243, Loss: 1.4783982038497925, Final Batch Loss: 0.7605118751525879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244, Loss: 1.5305401682853699, Final Batch Loss: 0.813092052936554\n",
      "Epoch 245, Loss: 1.5052616000175476, Final Batch Loss: 0.7557404637336731\n",
      "Epoch 246, Loss: 1.425437867641449, Final Batch Loss: 0.7495288252830505\n",
      "Epoch 247, Loss: 1.4628217220306396, Final Batch Loss: 0.6886193752288818\n",
      "Epoch 248, Loss: 1.482232928276062, Final Batch Loss: 0.726304829120636\n",
      "Epoch 249, Loss: 1.462340533733368, Final Batch Loss: 0.7488476634025574\n",
      "Epoch 250, Loss: 1.4204230904579163, Final Batch Loss: 0.6527639627456665\n",
      "Epoch 251, Loss: 1.4058527946472168, Final Batch Loss: 0.6668888926506042\n",
      "Epoch 252, Loss: 1.5459265112876892, Final Batch Loss: 0.783629834651947\n",
      "Epoch 253, Loss: 1.3799662590026855, Final Batch Loss: 0.6816758513450623\n",
      "Epoch 254, Loss: 1.405888020992279, Final Batch Loss: 0.6776490211486816\n",
      "Epoch 255, Loss: 1.3598406314849854, Final Batch Loss: 0.6138291954994202\n",
      "Epoch 256, Loss: 1.4239203333854675, Final Batch Loss: 0.7049221396446228\n",
      "Epoch 257, Loss: 1.4366137385368347, Final Batch Loss: 0.7268556356430054\n",
      "Epoch 258, Loss: 1.5310277342796326, Final Batch Loss: 0.7555882334709167\n",
      "Epoch 259, Loss: 1.4604878425598145, Final Batch Loss: 0.7142464518547058\n",
      "Epoch 260, Loss: 1.5124800205230713, Final Batch Loss: 0.8208110332489014\n",
      "Epoch 261, Loss: 1.5019246935844421, Final Batch Loss: 0.7653042674064636\n",
      "Epoch 262, Loss: 1.3828821182250977, Final Batch Loss: 0.7119485139846802\n",
      "Epoch 263, Loss: 1.4092772006988525, Final Batch Loss: 0.7090980410575867\n",
      "Epoch 264, Loss: 1.3987544178962708, Final Batch Loss: 0.6986527442932129\n",
      "Epoch 265, Loss: 1.4043427109718323, Final Batch Loss: 0.6861781477928162\n",
      "Epoch 266, Loss: 1.442575454711914, Final Batch Loss: 0.666120171546936\n",
      "Epoch 267, Loss: 1.4802340865135193, Final Batch Loss: 0.7262668609619141\n",
      "Epoch 268, Loss: 1.4357686042785645, Final Batch Loss: 0.7352707386016846\n",
      "Epoch 269, Loss: 1.456475853919983, Final Batch Loss: 0.7897049784660339\n",
      "Epoch 270, Loss: 1.4374517798423767, Final Batch Loss: 0.710070788860321\n",
      "Epoch 271, Loss: 1.4176045060157776, Final Batch Loss: 0.7522717714309692\n",
      "Epoch 272, Loss: 1.4559685587882996, Final Batch Loss: 0.7006254196166992\n",
      "Epoch 273, Loss: 1.4197773337364197, Final Batch Loss: 0.7500291466712952\n",
      "Epoch 274, Loss: 1.4084762930870056, Final Batch Loss: 0.6905559301376343\n",
      "Epoch 275, Loss: 1.4022231101989746, Final Batch Loss: 0.7167550325393677\n",
      "Epoch 276, Loss: 1.4326670169830322, Final Batch Loss: 0.7426987886428833\n",
      "Epoch 277, Loss: 1.4406672716140747, Final Batch Loss: 0.6992325186729431\n",
      "Epoch 278, Loss: 1.3973420858383179, Final Batch Loss: 0.6571097373962402\n",
      "Epoch 279, Loss: 1.4591871500015259, Final Batch Loss: 0.7127001285552979\n",
      "Epoch 280, Loss: 1.4073076248168945, Final Batch Loss: 0.7119240760803223\n",
      "Epoch 281, Loss: 1.3181900382041931, Final Batch Loss: 0.6459494829177856\n",
      "Epoch 282, Loss: 1.354878842830658, Final Batch Loss: 0.605746328830719\n",
      "Epoch 283, Loss: 1.3564828038215637, Final Batch Loss: 0.6370996236801147\n",
      "Epoch 284, Loss: 1.3559521436691284, Final Batch Loss: 0.6562280058860779\n",
      "Epoch 285, Loss: 1.396971881389618, Final Batch Loss: 0.699783980846405\n",
      "Epoch 286, Loss: 1.4136934280395508, Final Batch Loss: 0.6338527202606201\n",
      "Epoch 287, Loss: 1.447081208229065, Final Batch Loss: 0.7298117280006409\n",
      "Epoch 288, Loss: 1.369120717048645, Final Batch Loss: 0.6831968426704407\n",
      "Epoch 289, Loss: 1.461528480052948, Final Batch Loss: 0.7657342553138733\n",
      "Epoch 290, Loss: 1.3403934240341187, Final Batch Loss: 0.6274611949920654\n",
      "Epoch 291, Loss: 1.3730656504631042, Final Batch Loss: 0.6687831282615662\n",
      "Epoch 292, Loss: 1.3876882195472717, Final Batch Loss: 0.6751616597175598\n",
      "Epoch 293, Loss: 1.3707634806632996, Final Batch Loss: 0.6911891102790833\n",
      "Epoch 294, Loss: 1.384284794330597, Final Batch Loss: 0.7012803554534912\n",
      "Epoch 295, Loss: 1.4316602945327759, Final Batch Loss: 0.7682158946990967\n",
      "Epoch 296, Loss: 1.3858091831207275, Final Batch Loss: 0.6924800872802734\n",
      "Epoch 297, Loss: 1.3635771870613098, Final Batch Loss: 0.7000661492347717\n",
      "Epoch 298, Loss: 1.4204899072647095, Final Batch Loss: 0.7164068222045898\n",
      "Epoch 299, Loss: 1.413400948047638, Final Batch Loss: 0.6884132623672485\n",
      "Epoch 300, Loss: 1.388520896434784, Final Batch Loss: 0.6785839200019836\n",
      "Epoch 301, Loss: 1.3396158814430237, Final Batch Loss: 0.676953136920929\n",
      "Epoch 302, Loss: 1.3551872968673706, Final Batch Loss: 0.65186607837677\n",
      "Epoch 303, Loss: 1.597402274608612, Final Batch Loss: 0.8631600737571716\n",
      "Epoch 304, Loss: 1.3225983381271362, Final Batch Loss: 0.6496454477310181\n",
      "Epoch 305, Loss: 1.396698534488678, Final Batch Loss: 0.6668996810913086\n",
      "Epoch 306, Loss: 1.34270179271698, Final Batch Loss: 0.667668342590332\n",
      "Epoch 307, Loss: 1.3237899541854858, Final Batch Loss: 0.6492084264755249\n",
      "Epoch 308, Loss: 1.3326767086982727, Final Batch Loss: 0.724739670753479\n",
      "Epoch 309, Loss: 1.4753324389457703, Final Batch Loss: 0.7545371651649475\n",
      "Epoch 310, Loss: 1.4121974110603333, Final Batch Loss: 0.7590252757072449\n",
      "Epoch 311, Loss: 1.3167521357536316, Final Batch Loss: 0.6096818447113037\n",
      "Epoch 312, Loss: 1.3446845412254333, Final Batch Loss: 0.7285791039466858\n",
      "Epoch 313, Loss: 1.2686725854873657, Final Batch Loss: 0.594829797744751\n",
      "Epoch 314, Loss: 1.3222302198410034, Final Batch Loss: 0.638343870639801\n",
      "Epoch 315, Loss: 1.3764766454696655, Final Batch Loss: 0.7142153382301331\n",
      "Epoch 316, Loss: 1.3655165433883667, Final Batch Loss: 0.7152726054191589\n",
      "Epoch 317, Loss: 1.4177234172821045, Final Batch Loss: 0.7192291617393494\n",
      "Epoch 318, Loss: 1.2902455925941467, Final Batch Loss: 0.6220386028289795\n",
      "Epoch 319, Loss: 1.3253065347671509, Final Batch Loss: 0.6505581140518188\n",
      "Epoch 320, Loss: 1.3123855590820312, Final Batch Loss: 0.6601584553718567\n",
      "Epoch 321, Loss: 1.3807705640792847, Final Batch Loss: 0.7299991250038147\n",
      "Epoch 322, Loss: 1.3306636810302734, Final Batch Loss: 0.6687325239181519\n",
      "Epoch 323, Loss: 1.2921811938285828, Final Batch Loss: 0.633588969707489\n",
      "Epoch 324, Loss: 1.3232718110084534, Final Batch Loss: 0.6671777367591858\n",
      "Epoch 325, Loss: 1.389586091041565, Final Batch Loss: 0.7576667070388794\n",
      "Epoch 326, Loss: 1.344886064529419, Final Batch Loss: 0.7041723132133484\n",
      "Epoch 327, Loss: 1.355325698852539, Final Batch Loss: 0.7080839276313782\n",
      "Epoch 328, Loss: 1.307249128818512, Final Batch Loss: 0.6040188670158386\n",
      "Epoch 329, Loss: 1.299481451511383, Final Batch Loss: 0.6430976390838623\n",
      "Epoch 330, Loss: 1.3047088384628296, Final Batch Loss: 0.6434698700904846\n",
      "Epoch 331, Loss: 1.3518328666687012, Final Batch Loss: 0.627750039100647\n",
      "Epoch 332, Loss: 1.3975269794464111, Final Batch Loss: 0.7470368146896362\n",
      "Epoch 333, Loss: 1.3561033010482788, Final Batch Loss: 0.6905043125152588\n",
      "Epoch 334, Loss: 1.3823962807655334, Final Batch Loss: 0.7164562344551086\n",
      "Epoch 335, Loss: 1.3795969486236572, Final Batch Loss: 0.6512205600738525\n",
      "Epoch 336, Loss: 1.2842052578926086, Final Batch Loss: 0.6430507898330688\n",
      "Epoch 337, Loss: 1.3385695219039917, Final Batch Loss: 0.6537854075431824\n",
      "Epoch 338, Loss: 1.2561751008033752, Final Batch Loss: 0.6192463040351868\n",
      "Epoch 339, Loss: 1.3574804663658142, Final Batch Loss: 0.6787475347518921\n",
      "Epoch 340, Loss: 1.2759330868721008, Final Batch Loss: 0.6302656531333923\n",
      "Epoch 341, Loss: 1.3137723803520203, Final Batch Loss: 0.7057090997695923\n",
      "Epoch 342, Loss: 1.3009392023086548, Final Batch Loss: 0.6894658803939819\n",
      "Epoch 343, Loss: 1.3312488198280334, Final Batch Loss: 0.6581663489341736\n",
      "Epoch 344, Loss: 1.3149295449256897, Final Batch Loss: 0.6130820512771606\n",
      "Epoch 345, Loss: 1.4055789113044739, Final Batch Loss: 0.7779492735862732\n",
      "Epoch 346, Loss: 1.3755001425743103, Final Batch Loss: 0.7206972241401672\n",
      "Epoch 347, Loss: 1.2946841716766357, Final Batch Loss: 0.6487753987312317\n",
      "Epoch 348, Loss: 1.2652427554130554, Final Batch Loss: 0.5901428461074829\n",
      "Epoch 349, Loss: 1.2377243041992188, Final Batch Loss: 0.6070270538330078\n",
      "Epoch 350, Loss: 1.304232656955719, Final Batch Loss: 0.6835659742355347\n",
      "Epoch 351, Loss: 1.2844017148017883, Final Batch Loss: 0.6137720346450806\n",
      "Epoch 352, Loss: 1.228046953678131, Final Batch Loss: 0.603263258934021\n",
      "Epoch 353, Loss: 1.2999773025512695, Final Batch Loss: 0.6473227739334106\n",
      "Epoch 354, Loss: 1.3520446419715881, Final Batch Loss: 0.6800426840782166\n",
      "Epoch 355, Loss: 1.299791157245636, Final Batch Loss: 0.6563858985900879\n",
      "Epoch 356, Loss: 1.3502461910247803, Final Batch Loss: 0.6528333425521851\n",
      "Epoch 357, Loss: 1.3517881035804749, Final Batch Loss: 0.7112178802490234\n",
      "Epoch 358, Loss: 1.2975095510482788, Final Batch Loss: 0.6879847049713135\n",
      "Epoch 359, Loss: 1.2728655338287354, Final Batch Loss: 0.6566410660743713\n",
      "Epoch 360, Loss: 1.3218323588371277, Final Batch Loss: 0.6624026894569397\n",
      "Epoch 361, Loss: 1.2894943356513977, Final Batch Loss: 0.6508873701095581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362, Loss: 1.2800076603889465, Final Batch Loss: 0.6117081046104431\n",
      "Epoch 363, Loss: 1.2857243418693542, Final Batch Loss: 0.5854585766792297\n",
      "Epoch 364, Loss: 1.2789175510406494, Final Batch Loss: 0.6637121438980103\n",
      "Epoch 365, Loss: 1.3381027579307556, Final Batch Loss: 0.6497116088867188\n",
      "Epoch 366, Loss: 1.1848866939544678, Final Batch Loss: 0.5734273195266724\n",
      "Epoch 367, Loss: 1.3365848064422607, Final Batch Loss: 0.680364191532135\n",
      "Epoch 368, Loss: 1.2783533930778503, Final Batch Loss: 0.644783616065979\n",
      "Epoch 369, Loss: 1.3394861817359924, Final Batch Loss: 0.6917825937271118\n",
      "Epoch 370, Loss: 1.2084366083145142, Final Batch Loss: 0.527201771736145\n",
      "Epoch 371, Loss: 1.2556971907615662, Final Batch Loss: 0.6223173141479492\n",
      "Epoch 372, Loss: 1.2779214978218079, Final Batch Loss: 0.6535667777061462\n",
      "Epoch 373, Loss: 1.2330151796340942, Final Batch Loss: 0.6175636649131775\n",
      "Epoch 374, Loss: 1.2584348320960999, Final Batch Loss: 0.6119707226753235\n",
      "Epoch 375, Loss: 1.293404221534729, Final Batch Loss: 0.6846039295196533\n",
      "Epoch 376, Loss: 1.2195159792900085, Final Batch Loss: 0.6266911625862122\n",
      "Epoch 377, Loss: 1.376039743423462, Final Batch Loss: 0.6408448815345764\n",
      "Epoch 378, Loss: 1.2964253425598145, Final Batch Loss: 0.5943036675453186\n",
      "Epoch 379, Loss: 1.2546331286430359, Final Batch Loss: 0.6597724556922913\n",
      "Epoch 380, Loss: 1.2484318614006042, Final Batch Loss: 0.6476301550865173\n",
      "Epoch 381, Loss: 1.3906612992286682, Final Batch Loss: 0.7271613478660583\n",
      "Epoch 382, Loss: 1.2671504020690918, Final Batch Loss: 0.6522032022476196\n",
      "Epoch 383, Loss: 1.281366765499115, Final Batch Loss: 0.6307987570762634\n",
      "Epoch 384, Loss: 1.3101803064346313, Final Batch Loss: 0.6196066737174988\n",
      "Epoch 385, Loss: 1.2223828434944153, Final Batch Loss: 0.5864424109458923\n",
      "Epoch 386, Loss: 1.2579412460327148, Final Batch Loss: 0.5780223608016968\n",
      "Epoch 387, Loss: 1.2611764669418335, Final Batch Loss: 0.59321129322052\n",
      "Epoch 388, Loss: 1.2626608610153198, Final Batch Loss: 0.6497907042503357\n",
      "Epoch 389, Loss: 1.2998329997062683, Final Batch Loss: 0.6769576072692871\n",
      "Epoch 390, Loss: 1.230850100517273, Final Batch Loss: 0.6233829259872437\n",
      "Epoch 391, Loss: 1.1412362456321716, Final Batch Loss: 0.5493855476379395\n",
      "Epoch 392, Loss: 1.1497460007667542, Final Batch Loss: 0.6041537523269653\n",
      "Epoch 393, Loss: 1.263653039932251, Final Batch Loss: 0.6362699866294861\n",
      "Epoch 394, Loss: 1.2676215767860413, Final Batch Loss: 0.6478511691093445\n",
      "Epoch 395, Loss: 1.2652664184570312, Final Batch Loss: 0.6661065816879272\n",
      "Epoch 396, Loss: 1.1747997999191284, Final Batch Loss: 0.5279433727264404\n",
      "Epoch 397, Loss: 1.2724575400352478, Final Batch Loss: 0.6715673804283142\n",
      "Epoch 398, Loss: 1.1988746523857117, Final Batch Loss: 0.5731115937232971\n",
      "Epoch 399, Loss: 1.144554615020752, Final Batch Loss: 0.5569707155227661\n",
      "Epoch 400, Loss: 1.235339343547821, Final Batch Loss: 0.6315137147903442\n",
      "Epoch 401, Loss: 1.231196641921997, Final Batch Loss: 0.6362701654434204\n",
      "Epoch 402, Loss: 1.2187841534614563, Final Batch Loss: 0.6510647535324097\n",
      "Epoch 403, Loss: 1.2650389075279236, Final Batch Loss: 0.6158149838447571\n",
      "Epoch 404, Loss: 1.210326611995697, Final Batch Loss: 0.6090214252471924\n",
      "Epoch 405, Loss: 1.3106086254119873, Final Batch Loss: 0.6289591789245605\n",
      "Epoch 406, Loss: 1.259229063987732, Final Batch Loss: 0.6736127734184265\n",
      "Epoch 407, Loss: 1.1872745752334595, Final Batch Loss: 0.618453860282898\n",
      "Epoch 408, Loss: 1.2736687660217285, Final Batch Loss: 0.6285174489021301\n",
      "Epoch 409, Loss: 1.2529412508010864, Final Batch Loss: 0.6240788698196411\n",
      "Epoch 410, Loss: 1.2112210988998413, Final Batch Loss: 0.5552188158035278\n",
      "Epoch 411, Loss: 1.2750232219696045, Final Batch Loss: 0.6645815968513489\n",
      "Epoch 412, Loss: 1.1977375149726868, Final Batch Loss: 0.6163987517356873\n",
      "Epoch 413, Loss: 1.230797827243805, Final Batch Loss: 0.6338574290275574\n",
      "Epoch 414, Loss: 1.2581339478492737, Final Batch Loss: 0.6092196702957153\n",
      "Epoch 415, Loss: 1.1838411092758179, Final Batch Loss: 0.5570328235626221\n",
      "Epoch 416, Loss: 1.172932505607605, Final Batch Loss: 0.5483609437942505\n",
      "Epoch 417, Loss: 1.1507412195205688, Final Batch Loss: 0.5713704824447632\n",
      "Epoch 418, Loss: 1.1710588932037354, Final Batch Loss: 0.6083856225013733\n",
      "Epoch 419, Loss: 1.1854466795921326, Final Batch Loss: 0.5887035727500916\n",
      "Epoch 420, Loss: 1.173629641532898, Final Batch Loss: 0.549562931060791\n",
      "Epoch 421, Loss: 1.2028807997703552, Final Batch Loss: 0.6474853754043579\n",
      "Epoch 422, Loss: 1.1600309610366821, Final Batch Loss: 0.5153350830078125\n",
      "Epoch 423, Loss: 1.165722668170929, Final Batch Loss: 0.5728940367698669\n",
      "Epoch 424, Loss: 1.1435757279396057, Final Batch Loss: 0.5610490441322327\n",
      "Epoch 425, Loss: 1.1780665516853333, Final Batch Loss: 0.5701266527175903\n",
      "Epoch 426, Loss: 1.1787045001983643, Final Batch Loss: 0.5713037848472595\n",
      "Epoch 427, Loss: 1.1918296813964844, Final Batch Loss: 0.6413263082504272\n",
      "Epoch 428, Loss: 1.2412657737731934, Final Batch Loss: 0.6328163743019104\n",
      "Epoch 429, Loss: 1.2471104860305786, Final Batch Loss: 0.6194472312927246\n",
      "Epoch 430, Loss: 1.1374844312667847, Final Batch Loss: 0.5493086576461792\n",
      "Epoch 431, Loss: 1.151249885559082, Final Batch Loss: 0.5962115526199341\n",
      "Epoch 432, Loss: 1.1939656734466553, Final Batch Loss: 0.682485044002533\n",
      "Epoch 433, Loss: 1.1913217902183533, Final Batch Loss: 0.592232346534729\n",
      "Epoch 434, Loss: 1.1560661792755127, Final Batch Loss: 0.5867004990577698\n",
      "Epoch 435, Loss: 1.167431116104126, Final Batch Loss: 0.5704599618911743\n",
      "Epoch 436, Loss: 1.2063069343566895, Final Batch Loss: 0.5665346384048462\n",
      "Epoch 437, Loss: 1.192086935043335, Final Batch Loss: 0.6237980127334595\n",
      "Epoch 438, Loss: 1.1641534566879272, Final Batch Loss: 0.5973984599113464\n",
      "Epoch 439, Loss: 1.0721087455749512, Final Batch Loss: 0.4998812675476074\n",
      "Epoch 440, Loss: 1.1972565650939941, Final Batch Loss: 0.6161883473396301\n",
      "Epoch 441, Loss: 1.2315745949745178, Final Batch Loss: 0.6522322297096252\n",
      "Epoch 442, Loss: 1.105757474899292, Final Batch Loss: 0.5451761484146118\n",
      "Epoch 443, Loss: 1.1518558263778687, Final Batch Loss: 0.5957045555114746\n",
      "Epoch 444, Loss: 1.0560200810432434, Final Batch Loss: 0.5340660810470581\n",
      "Epoch 445, Loss: 1.1733899116516113, Final Batch Loss: 0.6209006309509277\n",
      "Epoch 446, Loss: 1.1958571076393127, Final Batch Loss: 0.6327168345451355\n",
      "Epoch 447, Loss: 1.2115976214408875, Final Batch Loss: 0.6069711446762085\n",
      "Epoch 448, Loss: 1.1788129806518555, Final Batch Loss: 0.6391119360923767\n",
      "Epoch 449, Loss: 1.2301082611083984, Final Batch Loss: 0.6492536067962646\n",
      "Epoch 450, Loss: 1.1001773476600647, Final Batch Loss: 0.546349287033081\n",
      "Epoch 451, Loss: 1.19223153591156, Final Batch Loss: 0.658284604549408\n",
      "Epoch 452, Loss: 1.15040922164917, Final Batch Loss: 0.5877294540405273\n",
      "Epoch 453, Loss: 1.0801434516906738, Final Batch Loss: 0.5061639547348022\n",
      "Epoch 454, Loss: 1.1414119601249695, Final Batch Loss: 0.5552812218666077\n",
      "Epoch 455, Loss: 1.2065629363059998, Final Batch Loss: 0.6331120729446411\n",
      "Epoch 456, Loss: 1.1941277384757996, Final Batch Loss: 0.6772626638412476\n",
      "Epoch 457, Loss: 1.2982271909713745, Final Batch Loss: 0.7115064859390259\n",
      "Epoch 458, Loss: 1.06120765209198, Final Batch Loss: 0.5322265625\n",
      "Epoch 459, Loss: 1.1946104168891907, Final Batch Loss: 0.6555832624435425\n",
      "Epoch 460, Loss: 1.1084486246109009, Final Batch Loss: 0.54807448387146\n",
      "Epoch 461, Loss: 1.080263763666153, Final Batch Loss: 0.48105379939079285\n",
      "Epoch 462, Loss: 1.117296814918518, Final Batch Loss: 0.5106645226478577\n",
      "Epoch 463, Loss: 1.0927955508232117, Final Batch Loss: 0.5566317439079285\n",
      "Epoch 464, Loss: 1.1447518467903137, Final Batch Loss: 0.5778603553771973\n",
      "Epoch 465, Loss: 1.2128270268440247, Final Batch Loss: 0.6429203748703003\n",
      "Epoch 466, Loss: 1.14127779006958, Final Batch Loss: 0.6075854301452637\n",
      "Epoch 467, Loss: 1.1391589045524597, Final Batch Loss: 0.6183893084526062\n",
      "Epoch 468, Loss: 1.0628310143947601, Final Batch Loss: 0.4948350489139557\n",
      "Epoch 469, Loss: 1.114972174167633, Final Batch Loss: 0.5316175818443298\n",
      "Epoch 470, Loss: 1.1197436451911926, Final Batch Loss: 0.543489396572113\n",
      "Epoch 471, Loss: 1.1938632726669312, Final Batch Loss: 0.6187008023262024\n",
      "Epoch 472, Loss: 1.099444031715393, Final Batch Loss: 0.5858845114707947\n",
      "Epoch 473, Loss: 1.1722562909126282, Final Batch Loss: 0.5441139936447144\n",
      "Epoch 474, Loss: 1.1123847961425781, Final Batch Loss: 0.5400051474571228\n",
      "Epoch 475, Loss: 1.1424629092216492, Final Batch Loss: 0.5987603664398193\n",
      "Epoch 476, Loss: 1.1659114360809326, Final Batch Loss: 0.5724951028823853\n",
      "Epoch 477, Loss: 1.1438547372817993, Final Batch Loss: 0.5572304129600525\n",
      "Epoch 478, Loss: 1.1558847427368164, Final Batch Loss: 0.5626825094223022\n",
      "Epoch 479, Loss: 1.177668273448944, Final Batch Loss: 0.529910683631897\n",
      "Epoch 480, Loss: 1.1492260098457336, Final Batch Loss: 0.617441713809967\n",
      "Epoch 481, Loss: 1.1340673565864563, Final Batch Loss: 0.502062976360321\n",
      "Epoch 482, Loss: 1.0339242815971375, Final Batch Loss: 0.5121091604232788\n",
      "Epoch 483, Loss: 1.085135281085968, Final Batch Loss: 0.5511530041694641\n",
      "Epoch 484, Loss: 1.0279551148414612, Final Batch Loss: 0.48161429166793823\n",
      "Epoch 485, Loss: 1.1100305914878845, Final Batch Loss: 0.5418536067008972\n",
      "Epoch 486, Loss: 1.0573835372924805, Final Batch Loss: 0.48320335149765015\n",
      "Epoch 487, Loss: 1.068692684173584, Final Batch Loss: 0.507293164730072\n",
      "Epoch 488, Loss: 1.1394272446632385, Final Batch Loss: 0.5200114846229553\n",
      "Epoch 489, Loss: 1.05918550491333, Final Batch Loss: 0.5011855363845825\n",
      "Epoch 490, Loss: 1.112400770187378, Final Batch Loss: 0.5391408205032349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491, Loss: 1.2063273191452026, Final Batch Loss: 0.626448929309845\n",
      "Epoch 492, Loss: 1.1401317715644836, Final Batch Loss: 0.5784950256347656\n",
      "Epoch 493, Loss: 1.0502458810806274, Final Batch Loss: 0.49624407291412354\n",
      "Epoch 494, Loss: 1.0867900252342224, Final Batch Loss: 0.5790758728981018\n",
      "Epoch 495, Loss: 1.0759732127189636, Final Batch Loss: 0.5133183598518372\n",
      "Epoch 496, Loss: 1.1711103916168213, Final Batch Loss: 0.5911213755607605\n",
      "Epoch 497, Loss: 1.1884695291519165, Final Batch Loss: 0.6537407636642456\n",
      "Epoch 498, Loss: 1.0943663120269775, Final Batch Loss: 0.5529540777206421\n",
      "Epoch 499, Loss: 1.0489411354064941, Final Batch Loss: 0.4924048185348511\n",
      "Epoch 500, Loss: 1.096953809261322, Final Batch Loss: 0.5639286041259766\n",
      "Epoch 501, Loss: 1.1232733726501465, Final Batch Loss: 0.6095259785652161\n",
      "Epoch 502, Loss: 1.1077522039413452, Final Batch Loss: 0.5336319804191589\n",
      "Epoch 503, Loss: 1.1250845789909363, Final Batch Loss: 0.5431644320487976\n",
      "Epoch 504, Loss: 1.0763513445854187, Final Batch Loss: 0.5022727847099304\n",
      "Epoch 505, Loss: 1.0885505080223083, Final Batch Loss: 0.607399582862854\n",
      "Epoch 506, Loss: 1.0112809538841248, Final Batch Loss: 0.46375370025634766\n",
      "Epoch 507, Loss: 1.0930684208869934, Final Batch Loss: 0.5569119453430176\n",
      "Epoch 508, Loss: 1.0180513262748718, Final Batch Loss: 0.48227089643478394\n",
      "Epoch 509, Loss: 1.1544513702392578, Final Batch Loss: 0.6388593912124634\n",
      "Epoch 510, Loss: 1.137089192867279, Final Batch Loss: 0.5747969746589661\n",
      "Epoch 511, Loss: 1.0737817883491516, Final Batch Loss: 0.5503472089767456\n",
      "Epoch 512, Loss: 1.1545079350471497, Final Batch Loss: 0.6167577505111694\n",
      "Epoch 513, Loss: 1.0929221510887146, Final Batch Loss: 0.5527162551879883\n",
      "Epoch 514, Loss: 1.0043376684188843, Final Batch Loss: 0.48667919635772705\n",
      "Epoch 515, Loss: 1.0887937545776367, Final Batch Loss: 0.5124350786209106\n",
      "Epoch 516, Loss: 1.0479417741298676, Final Batch Loss: 0.5645667314529419\n",
      "Epoch 517, Loss: 1.0206970870494843, Final Batch Loss: 0.5279890298843384\n",
      "Epoch 518, Loss: 1.0834257006645203, Final Batch Loss: 0.5415328741073608\n",
      "Epoch 519, Loss: 1.0871574878692627, Final Batch Loss: 0.508853018283844\n",
      "Epoch 520, Loss: 0.987122118473053, Final Batch Loss: 0.434712290763855\n",
      "Epoch 521, Loss: 1.0590240359306335, Final Batch Loss: 0.527843177318573\n",
      "Epoch 522, Loss: 0.9691985845565796, Final Batch Loss: 0.4256218671798706\n",
      "Epoch 523, Loss: 1.0539149343967438, Final Batch Loss: 0.5641663670539856\n",
      "Epoch 524, Loss: 1.0770124197006226, Final Batch Loss: 0.5023617148399353\n",
      "Epoch 525, Loss: 1.1067042350769043, Final Batch Loss: 0.5469639301300049\n",
      "Epoch 526, Loss: 1.1123200953006744, Final Batch Loss: 0.6229318976402283\n",
      "Epoch 527, Loss: 1.1006816029548645, Final Batch Loss: 0.5407222509384155\n",
      "Epoch 528, Loss: 1.0081405639648438, Final Batch Loss: 0.505784273147583\n",
      "Epoch 529, Loss: 1.0431699454784393, Final Batch Loss: 0.5578513145446777\n",
      "Epoch 530, Loss: 0.9985132217407227, Final Batch Loss: 0.5162652134895325\n",
      "Epoch 531, Loss: 1.0921072363853455, Final Batch Loss: 0.5623111724853516\n",
      "Epoch 532, Loss: 1.2212562561035156, Final Batch Loss: 0.6236804127693176\n",
      "Epoch 533, Loss: 1.0956734716892242, Final Batch Loss: 0.6146264672279358\n",
      "Epoch 534, Loss: 0.9448078274726868, Final Batch Loss: 0.46492186188697815\n",
      "Epoch 535, Loss: 1.0573236048221588, Final Batch Loss: 0.5586139559745789\n",
      "Epoch 536, Loss: 1.0032422542572021, Final Batch Loss: 0.5073369145393372\n",
      "Epoch 537, Loss: 0.9701844155788422, Final Batch Loss: 0.4619627296924591\n",
      "Epoch 538, Loss: 1.0179103016853333, Final Batch Loss: 0.5520663857460022\n",
      "Epoch 539, Loss: 1.033137559890747, Final Batch Loss: 0.5393446683883667\n",
      "Epoch 540, Loss: 0.9959819614887238, Final Batch Loss: 0.45436891913414\n",
      "Epoch 541, Loss: 1.0456715822219849, Final Batch Loss: 0.5481610298156738\n",
      "Epoch 542, Loss: 1.0776414275169373, Final Batch Loss: 0.554543137550354\n",
      "Epoch 543, Loss: 1.053436666727066, Final Batch Loss: 0.5650531649589539\n",
      "Epoch 544, Loss: 1.1229706406593323, Final Batch Loss: 0.5643866062164307\n",
      "Epoch 545, Loss: 1.052497148513794, Final Batch Loss: 0.5073615312576294\n",
      "Epoch 546, Loss: 1.035025030374527, Final Batch Loss: 0.5671881437301636\n",
      "Epoch 547, Loss: 1.069228172302246, Final Batch Loss: 0.5431947112083435\n",
      "Epoch 548, Loss: 0.9993549287319183, Final Batch Loss: 0.5179941058158875\n",
      "Epoch 549, Loss: 0.9819228649139404, Final Batch Loss: 0.46673858165740967\n",
      "Epoch 550, Loss: 1.109480619430542, Final Batch Loss: 0.5986275672912598\n",
      "Epoch 551, Loss: 1.080235242843628, Final Batch Loss: 0.560915470123291\n",
      "Epoch 552, Loss: 1.0014975666999817, Final Batch Loss: 0.5309995412826538\n",
      "Epoch 553, Loss: 1.046834945678711, Final Batch Loss: 0.5366466045379639\n",
      "Epoch 554, Loss: 0.9878279268741608, Final Batch Loss: 0.4485491216182709\n",
      "Epoch 555, Loss: 0.9239624440670013, Final Batch Loss: 0.4049416482448578\n",
      "Epoch 556, Loss: 0.9732921719551086, Final Batch Loss: 0.5222082734107971\n",
      "Epoch 557, Loss: 0.9811456203460693, Final Batch Loss: 0.5040133595466614\n",
      "Epoch 558, Loss: 0.8969248831272125, Final Batch Loss: 0.44055503606796265\n",
      "Epoch 559, Loss: 1.0164834260940552, Final Batch Loss: 0.4759673476219177\n",
      "Epoch 560, Loss: 0.9913988411426544, Final Batch Loss: 0.5366713404655457\n",
      "Epoch 561, Loss: 1.0094333291053772, Final Batch Loss: 0.5241661071777344\n",
      "Epoch 562, Loss: 0.9786818325519562, Final Batch Loss: 0.44668635725975037\n",
      "Epoch 563, Loss: 0.9786621630191803, Final Batch Loss: 0.5152760744094849\n",
      "Epoch 564, Loss: 1.0746763050556183, Final Batch Loss: 0.5804173350334167\n",
      "Epoch 565, Loss: 0.9722836017608643, Final Batch Loss: 0.4947749972343445\n",
      "Epoch 566, Loss: 0.9785109758377075, Final Batch Loss: 0.5090197324752808\n",
      "Epoch 567, Loss: 1.008099913597107, Final Batch Loss: 0.5352588295936584\n",
      "Epoch 568, Loss: 1.0144518315792084, Final Batch Loss: 0.49123886227607727\n",
      "Epoch 569, Loss: 1.0531225204467773, Final Batch Loss: 0.5687876343727112\n",
      "Epoch 570, Loss: 1.1015929579734802, Final Batch Loss: 0.5699220299720764\n",
      "Epoch 571, Loss: 1.0026796460151672, Final Batch Loss: 0.5225085020065308\n",
      "Epoch 572, Loss: 1.0583248734474182, Final Batch Loss: 0.5222998857498169\n",
      "Epoch 573, Loss: 0.9817842543125153, Final Batch Loss: 0.5309420228004456\n",
      "Epoch 574, Loss: 0.9647373855113983, Final Batch Loss: 0.49485310912132263\n",
      "Epoch 575, Loss: 0.9648684561252594, Final Batch Loss: 0.5086793303489685\n",
      "Epoch 576, Loss: 0.9307165443897247, Final Batch Loss: 0.46143221855163574\n",
      "Epoch 577, Loss: 1.0426136255264282, Final Batch Loss: 0.5184309482574463\n",
      "Epoch 578, Loss: 1.030265986919403, Final Batch Loss: 0.5108512043952942\n",
      "Epoch 579, Loss: 0.9232113063335419, Final Batch Loss: 0.4492385983467102\n",
      "Epoch 580, Loss: 1.0684568583965302, Final Batch Loss: 0.6104167103767395\n",
      "Epoch 581, Loss: 1.035207450389862, Final Batch Loss: 0.5306356549263\n",
      "Epoch 582, Loss: 1.0062069296836853, Final Batch Loss: 0.5218714475631714\n",
      "Epoch 583, Loss: 1.0315857827663422, Final Batch Loss: 0.49519971013069153\n",
      "Epoch 584, Loss: 0.9811815619468689, Final Batch Loss: 0.4897838532924652\n",
      "Epoch 585, Loss: 0.9241761565208435, Final Batch Loss: 0.5031726956367493\n",
      "Epoch 586, Loss: 0.9716005325317383, Final Batch Loss: 0.4591025710105896\n",
      "Epoch 587, Loss: 0.950483649969101, Final Batch Loss: 0.5099928379058838\n",
      "Epoch 588, Loss: 0.8947769105434418, Final Batch Loss: 0.4173959791660309\n",
      "Epoch 589, Loss: 1.0103949308395386, Final Batch Loss: 0.5501523613929749\n",
      "Epoch 590, Loss: 1.0806965827941895, Final Batch Loss: 0.5652260780334473\n",
      "Epoch 591, Loss: 0.9157653450965881, Final Batch Loss: 0.47141146659851074\n",
      "Epoch 592, Loss: 1.0002737641334534, Final Batch Loss: 0.546652615070343\n",
      "Epoch 593, Loss: 1.0265367031097412, Final Batch Loss: 0.5153242945671082\n",
      "Epoch 594, Loss: 1.0619179010391235, Final Batch Loss: 0.5460093021392822\n",
      "Epoch 595, Loss: 0.9677110612392426, Final Batch Loss: 0.47481149435043335\n",
      "Epoch 596, Loss: 0.9597288370132446, Final Batch Loss: 0.4577651619911194\n",
      "Epoch 597, Loss: 0.8638244271278381, Final Batch Loss: 0.3867557644844055\n",
      "Epoch 598, Loss: 1.0081066489219666, Final Batch Loss: 0.487601637840271\n",
      "Epoch 599, Loss: 0.9738104045391083, Final Batch Loss: 0.5146722197532654\n",
      "Epoch 600, Loss: 1.0095194876194, Final Batch Loss: 0.512026846408844\n",
      "Epoch 601, Loss: 1.0110185742378235, Final Batch Loss: 0.5662528276443481\n",
      "Epoch 602, Loss: 0.9052520990371704, Final Batch Loss: 0.44188663363456726\n",
      "Epoch 603, Loss: 0.9306714236736298, Final Batch Loss: 0.4934278726577759\n",
      "Epoch 604, Loss: 0.9578328728675842, Final Batch Loss: 0.5095581412315369\n",
      "Epoch 605, Loss: 0.885290265083313, Final Batch Loss: 0.4692813754081726\n",
      "Epoch 606, Loss: 0.9766131341457367, Final Batch Loss: 0.48305490612983704\n",
      "Epoch 607, Loss: 0.8976031541824341, Final Batch Loss: 0.4143822491168976\n",
      "Epoch 608, Loss: 0.9622021317481995, Final Batch Loss: 0.47044941782951355\n",
      "Epoch 609, Loss: 0.997702419757843, Final Batch Loss: 0.5425844788551331\n",
      "Epoch 610, Loss: 0.9325272142887115, Final Batch Loss: 0.44204452633857727\n",
      "Epoch 611, Loss: 0.9626691043376923, Final Batch Loss: 0.5491124987602234\n",
      "Epoch 612, Loss: 1.0300541818141937, Final Batch Loss: 0.5566391944885254\n",
      "Epoch 613, Loss: 0.9804978668689728, Final Batch Loss: 0.5014243721961975\n",
      "Epoch 614, Loss: 0.9786853194236755, Final Batch Loss: 0.5320994257926941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 615, Loss: 0.9255864918231964, Final Batch Loss: 0.4605366885662079\n",
      "Epoch 616, Loss: 0.9440285861492157, Final Batch Loss: 0.43401578068733215\n",
      "Epoch 617, Loss: 0.9469873011112213, Final Batch Loss: 0.47839081287384033\n",
      "Epoch 618, Loss: 0.8878270983695984, Final Batch Loss: 0.43575090169906616\n",
      "Epoch 619, Loss: 0.9363707304000854, Final Batch Loss: 0.44145435094833374\n",
      "Epoch 620, Loss: 0.9705672562122345, Final Batch Loss: 0.5119156241416931\n",
      "Epoch 621, Loss: 0.9615663290023804, Final Batch Loss: 0.44889479875564575\n",
      "Epoch 622, Loss: 0.9382236003875732, Final Batch Loss: 0.47190362215042114\n",
      "Epoch 623, Loss: 0.9336541593074799, Final Batch Loss: 0.4626496434211731\n",
      "Epoch 624, Loss: 0.9080618619918823, Final Batch Loss: 0.47582384943962097\n",
      "Epoch 625, Loss: 0.9909069240093231, Final Batch Loss: 0.45806577801704407\n",
      "Epoch 626, Loss: 0.9894486665725708, Final Batch Loss: 0.4352741241455078\n",
      "Epoch 627, Loss: 0.8398659527301788, Final Batch Loss: 0.42636650800704956\n",
      "Epoch 628, Loss: 0.9388572573661804, Final Batch Loss: 0.4664618670940399\n",
      "Epoch 629, Loss: 0.9672289192676544, Final Batch Loss: 0.4860609769821167\n",
      "Epoch 630, Loss: 0.8848532438278198, Final Batch Loss: 0.440951406955719\n",
      "Epoch 631, Loss: 1.0093303620815277, Final Batch Loss: 0.5445367693901062\n",
      "Epoch 632, Loss: 0.9371183514595032, Final Batch Loss: 0.4779175817966461\n",
      "Epoch 633, Loss: 0.8559512495994568, Final Batch Loss: 0.39043375849723816\n",
      "Epoch 634, Loss: 1.048986166715622, Final Batch Loss: 0.5706855058670044\n",
      "Epoch 635, Loss: 0.8967251777648926, Final Batch Loss: 0.46463072299957275\n",
      "Epoch 636, Loss: 0.962573766708374, Final Batch Loss: 0.48154541850090027\n",
      "Epoch 637, Loss: 0.9383890926837921, Final Batch Loss: 0.44452741742134094\n",
      "Epoch 638, Loss: 0.9024337530136108, Final Batch Loss: 0.43774017691612244\n",
      "Epoch 639, Loss: 0.8928267955780029, Final Batch Loss: 0.4461005926132202\n",
      "Epoch 640, Loss: 1.06392103433609, Final Batch Loss: 0.5654081702232361\n",
      "Epoch 641, Loss: 0.9161917567253113, Final Batch Loss: 0.4399370551109314\n",
      "Epoch 642, Loss: 0.9621964693069458, Final Batch Loss: 0.4778028130531311\n",
      "Epoch 643, Loss: 0.9710745513439178, Final Batch Loss: 0.5072593092918396\n",
      "Epoch 644, Loss: 0.9495003819465637, Final Batch Loss: 0.5191726684570312\n",
      "Epoch 645, Loss: 0.8948790729045868, Final Batch Loss: 0.4142497181892395\n",
      "Epoch 646, Loss: 0.9602830410003662, Final Batch Loss: 0.5014548301696777\n",
      "Epoch 647, Loss: 0.8746925294399261, Final Batch Loss: 0.4040621519088745\n",
      "Epoch 648, Loss: 0.8208855092525482, Final Batch Loss: 0.40322375297546387\n",
      "Epoch 649, Loss: 1.0432793498039246, Final Batch Loss: 0.5206643342971802\n",
      "Epoch 650, Loss: 0.9718212187290192, Final Batch Loss: 0.4841316044330597\n",
      "Epoch 651, Loss: 0.9556596577167511, Final Batch Loss: 0.4917614758014679\n",
      "Epoch 652, Loss: 0.916051834821701, Final Batch Loss: 0.4490285813808441\n",
      "Epoch 653, Loss: 0.9446412920951843, Final Batch Loss: 0.5129528641700745\n",
      "Epoch 654, Loss: 0.8970281481742859, Final Batch Loss: 0.42283880710601807\n",
      "Epoch 655, Loss: 0.860879123210907, Final Batch Loss: 0.4117637276649475\n",
      "Epoch 656, Loss: 0.8947958946228027, Final Batch Loss: 0.471250981092453\n",
      "Epoch 657, Loss: 0.9555699825286865, Final Batch Loss: 0.5315023064613342\n",
      "Epoch 658, Loss: 0.9427908062934875, Final Batch Loss: 0.47204113006591797\n",
      "Epoch 659, Loss: 0.9187880754470825, Final Batch Loss: 0.4845036268234253\n",
      "Epoch 660, Loss: 0.9983408153057098, Final Batch Loss: 0.49567535519599915\n",
      "Epoch 661, Loss: 0.8838406801223755, Final Batch Loss: 0.43237873911857605\n",
      "Epoch 662, Loss: 0.9905648529529572, Final Batch Loss: 0.5747882723808289\n",
      "Epoch 663, Loss: 0.9339109659194946, Final Batch Loss: 0.5043953061103821\n",
      "Epoch 664, Loss: 0.9530048370361328, Final Batch Loss: 0.49104243516921997\n",
      "Epoch 665, Loss: 0.8879942893981934, Final Batch Loss: 0.4220498502254486\n",
      "Epoch 666, Loss: 0.822625070810318, Final Batch Loss: 0.34830743074417114\n",
      "Epoch 667, Loss: 0.9362287819385529, Final Batch Loss: 0.4963439106941223\n",
      "Epoch 668, Loss: 0.8950232565402985, Final Batch Loss: 0.41870352625846863\n",
      "Epoch 669, Loss: 0.9120277762413025, Final Batch Loss: 0.4699667692184448\n",
      "Epoch 670, Loss: 0.9120294749736786, Final Batch Loss: 0.43730562925338745\n",
      "Epoch 671, Loss: 0.9297614395618439, Final Batch Loss: 0.46683061122894287\n",
      "Epoch 672, Loss: 0.919540137052536, Final Batch Loss: 0.46483197808265686\n",
      "Epoch 673, Loss: 0.9459506869316101, Final Batch Loss: 0.4886741042137146\n",
      "Epoch 674, Loss: 0.9811862409114838, Final Batch Loss: 0.588775634765625\n",
      "Epoch 675, Loss: 0.9223470985889435, Final Batch Loss: 0.4224926233291626\n",
      "Epoch 676, Loss: 0.8621948659420013, Final Batch Loss: 0.41012129187583923\n",
      "Epoch 677, Loss: 0.9099720418453217, Final Batch Loss: 0.44724950194358826\n",
      "Epoch 678, Loss: 0.8798317313194275, Final Batch Loss: 0.4507181942462921\n",
      "Epoch 679, Loss: 0.7886851131916046, Final Batch Loss: 0.3737175464630127\n",
      "Epoch 680, Loss: 0.841983824968338, Final Batch Loss: 0.37544718384742737\n",
      "Epoch 681, Loss: 0.9234860837459564, Final Batch Loss: 0.5156956911087036\n",
      "Epoch 682, Loss: 0.90336874127388, Final Batch Loss: 0.4917204976081848\n",
      "Epoch 683, Loss: 0.840452253818512, Final Batch Loss: 0.4077511727809906\n",
      "Epoch 684, Loss: 0.8360353410243988, Final Batch Loss: 0.3804243505001068\n",
      "Epoch 685, Loss: 0.8553189337253571, Final Batch Loss: 0.3638685345649719\n",
      "Epoch 686, Loss: 0.8457883894443512, Final Batch Loss: 0.42364928126335144\n",
      "Epoch 687, Loss: 0.8206484317779541, Final Batch Loss: 0.3665008842945099\n",
      "Epoch 688, Loss: 0.8895192444324493, Final Batch Loss: 0.46219193935394287\n",
      "Epoch 689, Loss: 0.8783505260944366, Final Batch Loss: 0.5063328146934509\n",
      "Epoch 690, Loss: 0.89592245221138, Final Batch Loss: 0.5088291168212891\n",
      "Epoch 691, Loss: 0.9110530316829681, Final Batch Loss: 0.4362591505050659\n",
      "Epoch 692, Loss: 0.9344159960746765, Final Batch Loss: 0.460679829120636\n",
      "Epoch 693, Loss: 0.8188655078411102, Final Batch Loss: 0.41369137167930603\n",
      "Epoch 694, Loss: 0.921422928571701, Final Batch Loss: 0.4918788969516754\n",
      "Epoch 695, Loss: 0.8585573136806488, Final Batch Loss: 0.4702740013599396\n",
      "Epoch 696, Loss: 0.8544553518295288, Final Batch Loss: 0.44258829951286316\n",
      "Epoch 697, Loss: 0.8203724026679993, Final Batch Loss: 0.3887089788913727\n",
      "Epoch 698, Loss: 0.9597929120063782, Final Batch Loss: 0.4858638644218445\n",
      "Epoch 699, Loss: 0.8120660185813904, Final Batch Loss: 0.3685465455055237\n",
      "Epoch 700, Loss: 0.9497153759002686, Final Batch Loss: 0.49861133098602295\n",
      "Epoch 701, Loss: 0.8710774481296539, Final Batch Loss: 0.4208991229534149\n",
      "Epoch 702, Loss: 0.8686541318893433, Final Batch Loss: 0.41138553619384766\n",
      "Epoch 703, Loss: 0.8418103456497192, Final Batch Loss: 0.4212269186973572\n",
      "Epoch 704, Loss: 0.9234255254268646, Final Batch Loss: 0.46380943059921265\n",
      "Epoch 705, Loss: 0.8589227795600891, Final Batch Loss: 0.48470374941825867\n",
      "Epoch 706, Loss: 0.8411960899829865, Final Batch Loss: 0.43292105197906494\n",
      "Epoch 707, Loss: 0.7739529609680176, Final Batch Loss: 0.3877500593662262\n",
      "Epoch 708, Loss: 0.9113975465297699, Final Batch Loss: 0.4829334616661072\n",
      "Epoch 709, Loss: 0.8273914158344269, Final Batch Loss: 0.4070698916912079\n",
      "Epoch 710, Loss: 0.8293591141700745, Final Batch Loss: 0.43763285875320435\n",
      "Epoch 711, Loss: 0.8259910643100739, Final Batch Loss: 0.4033412039279938\n",
      "Epoch 712, Loss: 0.793502539396286, Final Batch Loss: 0.3987679183483124\n",
      "Epoch 713, Loss: 0.9477829039096832, Final Batch Loss: 0.4513832926750183\n",
      "Epoch 714, Loss: 0.7809549272060394, Final Batch Loss: 0.3812001347541809\n",
      "Epoch 715, Loss: 0.933439165353775, Final Batch Loss: 0.4924997091293335\n",
      "Epoch 716, Loss: 0.9328417181968689, Final Batch Loss: 0.4602050185203552\n",
      "Epoch 717, Loss: 0.8168695569038391, Final Batch Loss: 0.44113773107528687\n",
      "Epoch 718, Loss: 0.890000581741333, Final Batch Loss: 0.5233784317970276\n",
      "Epoch 719, Loss: 0.8464873433113098, Final Batch Loss: 0.4627831280231476\n",
      "Epoch 720, Loss: 0.8062516152858734, Final Batch Loss: 0.45087680220603943\n",
      "Epoch 721, Loss: 0.8425635397434235, Final Batch Loss: 0.3983359634876251\n",
      "Epoch 722, Loss: 0.863533079624176, Final Batch Loss: 0.41135260462760925\n",
      "Epoch 723, Loss: 0.8209860622882843, Final Batch Loss: 0.41519537568092346\n",
      "Epoch 724, Loss: 0.851948618888855, Final Batch Loss: 0.46694445610046387\n",
      "Epoch 725, Loss: 0.8403282463550568, Final Batch Loss: 0.41063055396080017\n",
      "Epoch 726, Loss: 0.862767219543457, Final Batch Loss: 0.37768617272377014\n",
      "Epoch 727, Loss: 0.8214450478553772, Final Batch Loss: 0.45236679911613464\n",
      "Epoch 728, Loss: 0.7957477569580078, Final Batch Loss: 0.4060412049293518\n",
      "Epoch 729, Loss: 0.7545785307884216, Final Batch Loss: 0.36609721183776855\n",
      "Epoch 730, Loss: 0.7541036605834961, Final Batch Loss: 0.3572032153606415\n",
      "Epoch 731, Loss: 0.8323045372962952, Final Batch Loss: 0.4266405999660492\n",
      "Epoch 732, Loss: 0.8696903586387634, Final Batch Loss: 0.4583715498447418\n",
      "Epoch 733, Loss: 0.8647768199443817, Final Batch Loss: 0.4358389377593994\n",
      "Epoch 734, Loss: 0.8002569675445557, Final Batch Loss: 0.3888908624649048\n",
      "Epoch 735, Loss: 0.8425335586071014, Final Batch Loss: 0.4174368977546692\n",
      "Epoch 736, Loss: 0.8029710054397583, Final Batch Loss: 0.3461834192276001\n",
      "Epoch 737, Loss: 0.7741103172302246, Final Batch Loss: 0.42270392179489136\n",
      "Epoch 738, Loss: 0.7749435007572174, Final Batch Loss: 0.3844941258430481\n",
      "Epoch 739, Loss: 0.7783469259738922, Final Batch Loss: 0.4190095067024231\n",
      "Epoch 740, Loss: 0.8206313252449036, Final Batch Loss: 0.42291873693466187\n",
      "Epoch 741, Loss: 0.7736314833164215, Final Batch Loss: 0.395012229681015\n",
      "Epoch 742, Loss: 0.888009786605835, Final Batch Loss: 0.5114673376083374\n",
      "Epoch 743, Loss: 0.8727882504463196, Final Batch Loss: 0.3535187244415283\n",
      "Epoch 744, Loss: 0.8769744336605072, Final Batch Loss: 0.4852026700973511\n",
      "Epoch 745, Loss: 0.7700006365776062, Final Batch Loss: 0.36403658986091614\n",
      "Epoch 746, Loss: 0.8575740158557892, Final Batch Loss: 0.5020689964294434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 747, Loss: 0.7076046466827393, Final Batch Loss: 0.3739457428455353\n",
      "Epoch 748, Loss: 0.828940212726593, Final Batch Loss: 0.40024107694625854\n",
      "Epoch 749, Loss: 0.8206626176834106, Final Batch Loss: 0.42075595259666443\n",
      "Epoch 750, Loss: 0.8374136388301849, Final Batch Loss: 0.4239139258861542\n",
      "Epoch 751, Loss: 0.7766185700893402, Final Batch Loss: 0.42037636041641235\n",
      "Epoch 752, Loss: 0.8424560129642487, Final Batch Loss: 0.4348579943180084\n",
      "Epoch 753, Loss: 0.7847133576869965, Final Batch Loss: 0.40796127915382385\n",
      "Epoch 754, Loss: 0.7774370014667511, Final Batch Loss: 0.34461143612861633\n",
      "Epoch 755, Loss: 0.7643070816993713, Final Batch Loss: 0.37621375918388367\n",
      "Epoch 756, Loss: 0.7878892123699188, Final Batch Loss: 0.41094523668289185\n",
      "Epoch 757, Loss: 0.7773907780647278, Final Batch Loss: 0.38159501552581787\n",
      "Epoch 758, Loss: 0.7519848048686981, Final Batch Loss: 0.4022989273071289\n",
      "Epoch 759, Loss: 0.8486444652080536, Final Batch Loss: 0.4105927348136902\n",
      "Epoch 760, Loss: 0.7649316191673279, Final Batch Loss: 0.41869646310806274\n",
      "Epoch 761, Loss: 0.7370046079158783, Final Batch Loss: 0.33847054839134216\n",
      "Epoch 762, Loss: 0.8150732517242432, Final Batch Loss: 0.41695067286491394\n",
      "Epoch 763, Loss: 0.7299398183822632, Final Batch Loss: 0.34812915325164795\n",
      "Epoch 764, Loss: 0.7872106730937958, Final Batch Loss: 0.36537083983421326\n",
      "Epoch 765, Loss: 0.9447333216667175, Final Batch Loss: 0.49311426281929016\n",
      "Epoch 766, Loss: 0.7156576216220856, Final Batch Loss: 0.357322633266449\n",
      "Epoch 767, Loss: 0.7912037670612335, Final Batch Loss: 0.41684776544570923\n",
      "Epoch 768, Loss: 0.7883081138134003, Final Batch Loss: 0.4461486041545868\n",
      "Epoch 769, Loss: 0.8231299519538879, Final Batch Loss: 0.45432496070861816\n",
      "Epoch 770, Loss: 0.819443553686142, Final Batch Loss: 0.44525617361068726\n",
      "Epoch 771, Loss: 0.7980189919471741, Final Batch Loss: 0.4110512137413025\n",
      "Epoch 772, Loss: 0.7394285202026367, Final Batch Loss: 0.33036109805107117\n",
      "Epoch 773, Loss: 0.8303928077220917, Final Batch Loss: 0.4363080561161041\n",
      "Epoch 774, Loss: 0.7817034423351288, Final Batch Loss: 0.3760581612586975\n",
      "Epoch 775, Loss: 0.724186509847641, Final Batch Loss: 0.31649184226989746\n",
      "Epoch 776, Loss: 0.8053050935268402, Final Batch Loss: 0.3186643123626709\n",
      "Epoch 777, Loss: 0.8131967782974243, Final Batch Loss: 0.39379140734672546\n",
      "Epoch 778, Loss: 0.7791418135166168, Final Batch Loss: 0.3743772804737091\n",
      "Epoch 779, Loss: 0.7929837107658386, Final Batch Loss: 0.4027063548564911\n",
      "Epoch 780, Loss: 0.75008624792099, Final Batch Loss: 0.38662874698638916\n",
      "Epoch 781, Loss: 0.8171495199203491, Final Batch Loss: 0.37526947259902954\n",
      "Epoch 782, Loss: 0.7529310882091522, Final Batch Loss: 0.4255082309246063\n",
      "Epoch 783, Loss: 0.8766583204269409, Final Batch Loss: 0.5001087784767151\n",
      "Epoch 784, Loss: 0.8296203017234802, Final Batch Loss: 0.3996530771255493\n",
      "Epoch 785, Loss: 0.7868028879165649, Final Batch Loss: 0.37307316064834595\n",
      "Epoch 786, Loss: 0.7635814547538757, Final Batch Loss: 0.3933951258659363\n",
      "Epoch 787, Loss: 0.8092895746231079, Final Batch Loss: 0.4024733901023865\n",
      "Epoch 788, Loss: 0.7553972601890564, Final Batch Loss: 0.3881288468837738\n",
      "Epoch 789, Loss: 0.8505825102329254, Final Batch Loss: 0.4439269006252289\n",
      "Epoch 790, Loss: 0.6922364234924316, Final Batch Loss: 0.32003146409988403\n",
      "Epoch 791, Loss: 0.7642180919647217, Final Batch Loss: 0.39462465047836304\n",
      "Epoch 792, Loss: 0.855817973613739, Final Batch Loss: 0.4467407166957855\n",
      "Epoch 793, Loss: 0.7340674698352814, Final Batch Loss: 0.39468398690223694\n",
      "Epoch 794, Loss: 0.7630333304405212, Final Batch Loss: 0.4012523293495178\n",
      "Epoch 795, Loss: 0.7426480352878571, Final Batch Loss: 0.33028122782707214\n",
      "Epoch 796, Loss: 0.7205643057823181, Final Batch Loss: 0.38115373253822327\n",
      "Epoch 797, Loss: 0.7144002020359039, Final Batch Loss: 0.3463827073574066\n",
      "Epoch 798, Loss: 0.6731701493263245, Final Batch Loss: 0.29880550503730774\n",
      "Epoch 799, Loss: 0.7453688979148865, Final Batch Loss: 0.3381219804286957\n",
      "Epoch 800, Loss: 0.6499593555927277, Final Batch Loss: 0.26332297921180725\n",
      "Epoch 801, Loss: 0.8340476751327515, Final Batch Loss: 0.41312575340270996\n",
      "Epoch 802, Loss: 0.7520595788955688, Final Batch Loss: 0.4122730791568756\n",
      "Epoch 803, Loss: 0.8791903257369995, Final Batch Loss: 0.5192391872406006\n",
      "Epoch 804, Loss: 0.69826340675354, Final Batch Loss: 0.3037123382091522\n",
      "Epoch 805, Loss: 0.7859241366386414, Final Batch Loss: 0.4321399927139282\n",
      "Epoch 806, Loss: 0.7052673101425171, Final Batch Loss: 0.33784356713294983\n",
      "Epoch 807, Loss: 0.7860057651996613, Final Batch Loss: 0.3976355195045471\n",
      "Epoch 808, Loss: 0.7462905645370483, Final Batch Loss: 0.3432360589504242\n",
      "Epoch 809, Loss: 0.7509777843952179, Final Batch Loss: 0.34355998039245605\n",
      "Epoch 810, Loss: 0.7451420426368713, Final Batch Loss: 0.3395198881626129\n",
      "Epoch 811, Loss: 0.7723530828952789, Final Batch Loss: 0.3584505319595337\n",
      "Epoch 812, Loss: 0.7248110473155975, Final Batch Loss: 0.3253406882286072\n",
      "Epoch 813, Loss: 0.7918427586555481, Final Batch Loss: 0.3934313654899597\n",
      "Epoch 814, Loss: 0.7149900794029236, Final Batch Loss: 0.3600987195968628\n",
      "Epoch 815, Loss: 0.7308529913425446, Final Batch Loss: 0.34530213475227356\n",
      "Epoch 816, Loss: 0.7902958989143372, Final Batch Loss: 0.3355349004268646\n",
      "Epoch 817, Loss: 0.7009285390377045, Final Batch Loss: 0.32487818598747253\n",
      "Epoch 818, Loss: 0.6737842857837677, Final Batch Loss: 0.32322293519973755\n",
      "Epoch 819, Loss: 0.7646046280860901, Final Batch Loss: 0.3694017231464386\n",
      "Epoch 820, Loss: 0.7425771057605743, Final Batch Loss: 0.35239216685295105\n",
      "Epoch 821, Loss: 0.7880308926105499, Final Batch Loss: 0.39960581064224243\n",
      "Epoch 822, Loss: 0.8276311457157135, Final Batch Loss: 0.4180648624897003\n",
      "Epoch 823, Loss: 0.7210333049297333, Final Batch Loss: 0.38019415736198425\n",
      "Epoch 824, Loss: 0.6977642476558685, Final Batch Loss: 0.30644139647483826\n",
      "Epoch 825, Loss: 0.8008573949337006, Final Batch Loss: 0.39194226264953613\n",
      "Epoch 826, Loss: 0.6931638717651367, Final Batch Loss: 0.3777621388435364\n",
      "Epoch 827, Loss: 0.7643531858921051, Final Batch Loss: 0.39302682876586914\n",
      "Epoch 828, Loss: 0.7376157641410828, Final Batch Loss: 0.39086493849754333\n",
      "Epoch 829, Loss: 0.7312645316123962, Final Batch Loss: 0.33077290654182434\n",
      "Epoch 830, Loss: 0.7786779999732971, Final Batch Loss: 0.3117683231830597\n",
      "Epoch 831, Loss: 0.7764688730239868, Final Batch Loss: 0.4265393614768982\n",
      "Epoch 832, Loss: 0.7221689820289612, Final Batch Loss: 0.365681916475296\n",
      "Epoch 833, Loss: 0.7677858471870422, Final Batch Loss: 0.34362560510635376\n",
      "Epoch 834, Loss: 0.6823506355285645, Final Batch Loss: 0.3346257507801056\n",
      "Epoch 835, Loss: 0.7394395470619202, Final Batch Loss: 0.3954029381275177\n",
      "Epoch 836, Loss: 0.7854157388210297, Final Batch Loss: 0.4316234290599823\n",
      "Epoch 837, Loss: 0.6980327665805817, Final Batch Loss: 0.3491634428501129\n",
      "Epoch 838, Loss: 0.8146661818027496, Final Batch Loss: 0.4309694468975067\n",
      "Epoch 839, Loss: 0.689156711101532, Final Batch Loss: 0.31047073006629944\n",
      "Epoch 840, Loss: 0.7247116565704346, Final Batch Loss: 0.3779178559780121\n",
      "Epoch 841, Loss: 0.7456344366073608, Final Batch Loss: 0.3464488387107849\n",
      "Epoch 842, Loss: 0.7012063264846802, Final Batch Loss: 0.3918106257915497\n",
      "Epoch 843, Loss: 0.7161598205566406, Final Batch Loss: 0.348854660987854\n",
      "Epoch 844, Loss: 0.6758411526679993, Final Batch Loss: 0.31678205728530884\n",
      "Epoch 845, Loss: 0.761063277721405, Final Batch Loss: 0.44249227643013\n",
      "Epoch 846, Loss: 0.8147897124290466, Final Batch Loss: 0.39196598529815674\n",
      "Epoch 847, Loss: 0.6791733503341675, Final Batch Loss: 0.34813088178634644\n",
      "Epoch 848, Loss: 0.7916567027568817, Final Batch Loss: 0.432539701461792\n",
      "Epoch 849, Loss: 0.6786512732505798, Final Batch Loss: 0.3206140100955963\n",
      "Epoch 850, Loss: 0.6800184547901154, Final Batch Loss: 0.3388954699039459\n",
      "Epoch 851, Loss: 0.7475373446941376, Final Batch Loss: 0.34201177954673767\n",
      "Epoch 852, Loss: 0.754524827003479, Final Batch Loss: 0.3803233504295349\n",
      "Epoch 853, Loss: 0.6836185157299042, Final Batch Loss: 0.2898017466068268\n",
      "Epoch 854, Loss: 0.7074370384216309, Final Batch Loss: 0.40860116481781006\n",
      "Epoch 855, Loss: 0.726088285446167, Final Batch Loss: 0.34692543745040894\n",
      "Epoch 856, Loss: 0.7773334681987762, Final Batch Loss: 0.39573389291763306\n",
      "Epoch 857, Loss: 0.7660281658172607, Final Batch Loss: 0.3592529892921448\n",
      "Epoch 858, Loss: 0.681170791387558, Final Batch Loss: 0.3171851336956024\n",
      "Epoch 859, Loss: 0.7174893617630005, Final Batch Loss: 0.34658992290496826\n",
      "Epoch 860, Loss: 0.772313266992569, Final Batch Loss: 0.4065099358558655\n",
      "Epoch 861, Loss: 0.7314448058605194, Final Batch Loss: 0.32909607887268066\n",
      "Epoch 862, Loss: 0.7734582722187042, Final Batch Loss: 0.4030376374721527\n",
      "Epoch 863, Loss: 0.8185070157051086, Final Batch Loss: 0.4131573438644409\n",
      "Epoch 864, Loss: 0.7170853316783905, Final Batch Loss: 0.3769319951534271\n",
      "Epoch 865, Loss: 0.7160663902759552, Final Batch Loss: 0.3234695494174957\n",
      "Epoch 866, Loss: 0.772036999464035, Final Batch Loss: 0.416066974401474\n",
      "Epoch 867, Loss: 0.8011719286441803, Final Batch Loss: 0.29778042435646057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 868, Loss: 0.7512027621269226, Final Batch Loss: 0.40898555517196655\n",
      "Epoch 869, Loss: 0.7040035128593445, Final Batch Loss: 0.3572043776512146\n",
      "Epoch 870, Loss: 0.7174057066440582, Final Batch Loss: 0.35100871324539185\n",
      "Epoch 871, Loss: 0.7276157736778259, Final Batch Loss: 0.37048521637916565\n",
      "Epoch 872, Loss: 0.6561481058597565, Final Batch Loss: 0.2902149260044098\n",
      "Epoch 873, Loss: 0.7045175135135651, Final Batch Loss: 0.3367370367050171\n",
      "Epoch 874, Loss: 0.6959240138530731, Final Batch Loss: 0.31779420375823975\n",
      "Epoch 875, Loss: 0.720603346824646, Final Batch Loss: 0.38395509123802185\n",
      "Epoch 876, Loss: 0.6712263226509094, Final Batch Loss: 0.3277210593223572\n",
      "Epoch 877, Loss: 0.7025265395641327, Final Batch Loss: 0.35779109597206116\n",
      "Epoch 878, Loss: 0.650488406419754, Final Batch Loss: 0.26994091272354126\n",
      "Epoch 879, Loss: 0.6522182524204254, Final Batch Loss: 0.3041064441204071\n",
      "Epoch 880, Loss: 0.6435349583625793, Final Batch Loss: 0.31463658809661865\n",
      "Epoch 881, Loss: 0.6958581209182739, Final Batch Loss: 0.37459680438041687\n",
      "Epoch 882, Loss: 0.6859287619590759, Final Batch Loss: 0.31392768025398254\n",
      "Epoch 883, Loss: 0.736049622297287, Final Batch Loss: 0.3491915166378021\n",
      "Epoch 884, Loss: 0.7939591705799103, Final Batch Loss: 0.4321518838405609\n",
      "Epoch 885, Loss: 0.6601174175739288, Final Batch Loss: 0.31842488050460815\n",
      "Epoch 886, Loss: 0.6823168694972992, Final Batch Loss: 0.25489237904548645\n",
      "Epoch 887, Loss: 0.6332250237464905, Final Batch Loss: 0.3153974711894989\n",
      "Epoch 888, Loss: 0.686803013086319, Final Batch Loss: 0.2712176740169525\n",
      "Epoch 889, Loss: 0.6486182510852814, Final Batch Loss: 0.30528444051742554\n",
      "Epoch 890, Loss: 0.706013947725296, Final Batch Loss: 0.35085585713386536\n",
      "Epoch 891, Loss: 0.761160135269165, Final Batch Loss: 0.428860604763031\n",
      "Epoch 892, Loss: 0.7438322007656097, Final Batch Loss: 0.36808446049690247\n",
      "Epoch 893, Loss: 0.694859653711319, Final Batch Loss: 0.3391519784927368\n",
      "Epoch 894, Loss: 0.5785110592842102, Final Batch Loss: 0.2711619734764099\n",
      "Epoch 895, Loss: 0.7076269686222076, Final Batch Loss: 0.34191587567329407\n",
      "Epoch 896, Loss: 0.6704426109790802, Final Batch Loss: 0.3254167139530182\n",
      "Epoch 897, Loss: 0.547068327665329, Final Batch Loss: 0.26254546642303467\n",
      "Epoch 898, Loss: 0.7227528691291809, Final Batch Loss: 0.40056705474853516\n",
      "Epoch 899, Loss: 0.7371858954429626, Final Batch Loss: 0.4310828745365143\n",
      "Epoch 900, Loss: 0.7324399054050446, Final Batch Loss: 0.37272512912750244\n",
      "Epoch 901, Loss: 0.7177432179450989, Final Batch Loss: 0.3836807906627655\n",
      "Epoch 902, Loss: 0.7522616982460022, Final Batch Loss: 0.3654543161392212\n",
      "Epoch 903, Loss: 0.687039315700531, Final Batch Loss: 0.3155117928981781\n",
      "Epoch 904, Loss: 0.6321414709091187, Final Batch Loss: 0.3029376268386841\n",
      "Epoch 905, Loss: 0.6193512976169586, Final Batch Loss: 0.3133559226989746\n",
      "Epoch 906, Loss: 0.7069880366325378, Final Batch Loss: 0.3460584878921509\n",
      "Epoch 907, Loss: 0.7052852213382721, Final Batch Loss: 0.345525324344635\n",
      "Epoch 908, Loss: 0.6842015981674194, Final Batch Loss: 0.34963640570640564\n",
      "Epoch 909, Loss: 0.6592380106449127, Final Batch Loss: 0.2996257543563843\n",
      "Epoch 910, Loss: 0.6059339344501495, Final Batch Loss: 0.32269924879074097\n",
      "Epoch 911, Loss: 0.59963259100914, Final Batch Loss: 0.2844642698764801\n",
      "Epoch 912, Loss: 0.595156192779541, Final Batch Loss: 0.3286741375923157\n",
      "Epoch 913, Loss: 0.62025386095047, Final Batch Loss: 0.30409297347068787\n",
      "Epoch 914, Loss: 0.7305387556552887, Final Batch Loss: 0.3486028015613556\n",
      "Epoch 915, Loss: 0.6691395938396454, Final Batch Loss: 0.32283341884613037\n",
      "Epoch 916, Loss: 0.6891350746154785, Final Batch Loss: 0.31566235423088074\n",
      "Epoch 917, Loss: 0.6416703462600708, Final Batch Loss: 0.28881239891052246\n",
      "Epoch 918, Loss: 0.6279077231884003, Final Batch Loss: 0.3630751967430115\n",
      "Epoch 919, Loss: 0.6375264823436737, Final Batch Loss: 0.29607412219047546\n",
      "Epoch 920, Loss: 0.6771093606948853, Final Batch Loss: 0.3292950689792633\n",
      "Epoch 921, Loss: 0.5592429935932159, Final Batch Loss: 0.2562822103500366\n",
      "Epoch 922, Loss: 0.5762367248535156, Final Batch Loss: 0.2986340820789337\n",
      "Epoch 923, Loss: 0.6078515648841858, Final Batch Loss: 0.3015708029270172\n",
      "Epoch 924, Loss: 0.6273509562015533, Final Batch Loss: 0.2990744411945343\n",
      "Epoch 925, Loss: 0.6447777450084686, Final Batch Loss: 0.27865105867385864\n",
      "Epoch 926, Loss: 0.6526930332183838, Final Batch Loss: 0.3414207994937897\n",
      "Epoch 927, Loss: 0.7389045059680939, Final Batch Loss: 0.39685964584350586\n",
      "Epoch 928, Loss: 0.5537807941436768, Final Batch Loss: 0.26292186975479126\n",
      "Epoch 929, Loss: 0.6274258494377136, Final Batch Loss: 0.30970966815948486\n",
      "Epoch 930, Loss: 0.6755354106426239, Final Batch Loss: 0.35886096954345703\n",
      "Epoch 931, Loss: 0.648471474647522, Final Batch Loss: 0.32796165347099304\n",
      "Epoch 932, Loss: 0.6307171881198883, Final Batch Loss: 0.3307621479034424\n",
      "Epoch 933, Loss: 0.6452779769897461, Final Batch Loss: 0.3490898013114929\n",
      "Epoch 934, Loss: 0.7303527891635895, Final Batch Loss: 0.36801794171333313\n",
      "Epoch 935, Loss: 0.6015439033508301, Final Batch Loss: 0.3073299825191498\n",
      "Epoch 936, Loss: 0.6622717976570129, Final Batch Loss: 0.3484521806240082\n",
      "Epoch 937, Loss: 0.5885217785835266, Final Batch Loss: 0.2727341651916504\n",
      "Epoch 938, Loss: 0.5421269834041595, Final Batch Loss: 0.2776557505130768\n",
      "Epoch 939, Loss: 0.6843699514865875, Final Batch Loss: 0.3233758211135864\n",
      "Epoch 940, Loss: 0.7005683779716492, Final Batch Loss: 0.2998540699481964\n",
      "Epoch 941, Loss: 0.6678052842617035, Final Batch Loss: 0.3763330578804016\n",
      "Epoch 942, Loss: 0.6970088183879852, Final Batch Loss: 0.39288192987442017\n",
      "Epoch 943, Loss: 0.6841152012348175, Final Batch Loss: 0.3358554244041443\n",
      "Epoch 944, Loss: 0.6396397650241852, Final Batch Loss: 0.31179773807525635\n",
      "Epoch 945, Loss: 0.5987253487110138, Final Batch Loss: 0.3123173713684082\n",
      "Epoch 946, Loss: 0.5744358003139496, Final Batch Loss: 0.2759743630886078\n",
      "Epoch 947, Loss: 0.5997982919216156, Final Batch Loss: 0.3221224248409271\n",
      "Epoch 948, Loss: 0.6592738926410675, Final Batch Loss: 0.33026427030563354\n",
      "Epoch 949, Loss: 0.6117837727069855, Final Batch Loss: 0.30352360010147095\n",
      "Epoch 950, Loss: 0.5732093751430511, Final Batch Loss: 0.2652721107006073\n",
      "Epoch 951, Loss: 0.5501765012741089, Final Batch Loss: 0.24875864386558533\n",
      "Epoch 952, Loss: 0.5802175402641296, Final Batch Loss: 0.2924558222293854\n",
      "Epoch 953, Loss: 0.6944144368171692, Final Batch Loss: 0.4145370423793793\n",
      "Epoch 954, Loss: 0.6432091295719147, Final Batch Loss: 0.3349400758743286\n",
      "Epoch 955, Loss: 0.7198068499565125, Final Batch Loss: 0.35392025113105774\n",
      "Epoch 956, Loss: 0.6885501742362976, Final Batch Loss: 0.3973540961742401\n",
      "Epoch 957, Loss: 0.57123664021492, Final Batch Loss: 0.2847403287887573\n",
      "Epoch 958, Loss: 0.6586633324623108, Final Batch Loss: 0.3189062774181366\n",
      "Epoch 959, Loss: 0.5732848942279816, Final Batch Loss: 0.2901838719844818\n",
      "Epoch 960, Loss: 0.5620577931404114, Final Batch Loss: 0.22424903512001038\n",
      "Epoch 961, Loss: 0.5826318562030792, Final Batch Loss: 0.2503567636013031\n",
      "Epoch 962, Loss: 0.6417628526687622, Final Batch Loss: 0.3091585338115692\n",
      "Epoch 963, Loss: 0.6570960879325867, Final Batch Loss: 0.3374566435813904\n",
      "Epoch 964, Loss: 0.621386855840683, Final Batch Loss: 0.3046663999557495\n",
      "Epoch 965, Loss: 0.6110665798187256, Final Batch Loss: 0.26618605852127075\n",
      "Epoch 966, Loss: 0.6661226153373718, Final Batch Loss: 0.2991514205932617\n",
      "Epoch 967, Loss: 0.553110808134079, Final Batch Loss: 0.2563002407550812\n",
      "Epoch 968, Loss: 0.6943121254444122, Final Batch Loss: 0.2710297703742981\n",
      "Epoch 969, Loss: 0.554849773645401, Final Batch Loss: 0.234108567237854\n",
      "Epoch 970, Loss: 0.7258215844631195, Final Batch Loss: 0.4226295053958893\n",
      "Epoch 971, Loss: 0.6594008207321167, Final Batch Loss: 0.3708336651325226\n",
      "Epoch 972, Loss: 0.7034787535667419, Final Batch Loss: 0.37450358271598816\n",
      "Epoch 973, Loss: 0.6196005642414093, Final Batch Loss: 0.3090327978134155\n",
      "Epoch 974, Loss: 0.7260972857475281, Final Batch Loss: 0.39059504866600037\n",
      "Epoch 975, Loss: 0.5983653962612152, Final Batch Loss: 0.3067138195037842\n",
      "Epoch 976, Loss: 0.674496978521347, Final Batch Loss: 0.3461835980415344\n",
      "Epoch 977, Loss: 0.5727271437644958, Final Batch Loss: 0.2893409729003906\n",
      "Epoch 978, Loss: 0.6269382834434509, Final Batch Loss: 0.3200187087059021\n",
      "Epoch 979, Loss: 0.5608125329017639, Final Batch Loss: 0.29328617453575134\n",
      "Epoch 980, Loss: 0.7281980812549591, Final Batch Loss: 0.43794360756874084\n",
      "Epoch 981, Loss: 0.5785780549049377, Final Batch Loss: 0.2633762061595917\n",
      "Epoch 982, Loss: 0.6265359818935394, Final Batch Loss: 0.31037744879722595\n",
      "Epoch 983, Loss: 0.5947897136211395, Final Batch Loss: 0.25674909353256226\n",
      "Epoch 984, Loss: 0.7305144965648651, Final Batch Loss: 0.3552623391151428\n",
      "Epoch 985, Loss: 0.6531589925289154, Final Batch Loss: 0.3232060968875885\n",
      "Epoch 986, Loss: 0.6175421476364136, Final Batch Loss: 0.3219057619571686\n",
      "Epoch 987, Loss: 0.5751902461051941, Final Batch Loss: 0.28534039855003357\n",
      "Epoch 988, Loss: 0.5735392868518829, Final Batch Loss: 0.2494812309741974\n",
      "Epoch 989, Loss: 0.5855287909507751, Final Batch Loss: 0.20822793245315552\n",
      "Epoch 990, Loss: 0.6973311603069305, Final Batch Loss: 0.3318067193031311\n",
      "Epoch 991, Loss: 0.5783988833427429, Final Batch Loss: 0.2892768085002899\n",
      "Epoch 992, Loss: 0.555733785033226, Final Batch Loss: 0.3205506205558777\n",
      "Epoch 993, Loss: 0.5362928956747055, Final Batch Loss: 0.2464418262243271\n",
      "Epoch 994, Loss: 0.5311117321252823, Final Batch Loss: 0.28968897461891174\n",
      "Epoch 995, Loss: 0.5942836999893188, Final Batch Loss: 0.30317479372024536\n",
      "Epoch 996, Loss: 0.5405911356210709, Final Batch Loss: 0.2399064153432846\n",
      "Epoch 997, Loss: 0.597495511174202, Final Batch Loss: 0.3487068712711334\n",
      "Epoch 998, Loss: 0.5635484308004379, Final Batch Loss: 0.2488647848367691\n",
      "Epoch 999, Loss: 0.6892571449279785, Final Batch Loss: 0.3881928622722626\n",
      "Epoch 1000, Loss: 0.6459928750991821, Final Batch Loss: 0.32644927501678467\n",
      "Epoch 1001, Loss: 0.6103434264659882, Final Batch Loss: 0.3594171106815338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1002, Loss: 0.6056805849075317, Final Batch Loss: 0.275190144777298\n",
      "Epoch 1003, Loss: 0.6614162623882294, Final Batch Loss: 0.3345785439014435\n",
      "Epoch 1004, Loss: 0.5828295946121216, Final Batch Loss: 0.29497992992401123\n",
      "Epoch 1005, Loss: 0.6237919926643372, Final Batch Loss: 0.3354158401489258\n",
      "Epoch 1006, Loss: 0.5662376284599304, Final Batch Loss: 0.23026549816131592\n",
      "Epoch 1007, Loss: 0.6895552575588226, Final Batch Loss: 0.3486635386943817\n",
      "Epoch 1008, Loss: 0.6480421423912048, Final Batch Loss: 0.34636250138282776\n",
      "Epoch 1009, Loss: 0.48869092762470245, Final Batch Loss: 0.2558459937572479\n",
      "Epoch 1010, Loss: 0.663583368062973, Final Batch Loss: 0.3389836549758911\n",
      "Epoch 1011, Loss: 0.5935772359371185, Final Batch Loss: 0.3078036904335022\n",
      "Epoch 1012, Loss: 0.6098277866840363, Final Batch Loss: 0.28117167949676514\n",
      "Epoch 1013, Loss: 0.5633220076560974, Final Batch Loss: 0.2586974799633026\n",
      "Epoch 1014, Loss: 0.553313285112381, Final Batch Loss: 0.2217518389225006\n",
      "Epoch 1015, Loss: 0.5108461081981659, Final Batch Loss: 0.2534182071685791\n",
      "Epoch 1016, Loss: 0.5877620875835419, Final Batch Loss: 0.3092375695705414\n",
      "Epoch 1017, Loss: 0.7022587358951569, Final Batch Loss: 0.29056650400161743\n",
      "Epoch 1018, Loss: 0.5823160707950592, Final Batch Loss: 0.2639411687850952\n",
      "Epoch 1019, Loss: 0.5793387293815613, Final Batch Loss: 0.3026798665523529\n",
      "Epoch 1020, Loss: 0.5727336406707764, Final Batch Loss: 0.27277904748916626\n",
      "Epoch 1021, Loss: 0.5561254769563675, Final Batch Loss: 0.3142058849334717\n",
      "Epoch 1022, Loss: 0.5665262639522552, Final Batch Loss: 0.31758913397789\n",
      "Epoch 1023, Loss: 0.6534685492515564, Final Batch Loss: 0.3263997733592987\n",
      "Epoch 1024, Loss: 0.5689919888973236, Final Batch Loss: 0.25796273350715637\n",
      "Epoch 1025, Loss: 0.6548865437507629, Final Batch Loss: 0.339177668094635\n",
      "Epoch 1026, Loss: 0.5820841491222382, Final Batch Loss: 0.28910383582115173\n",
      "Epoch 1027, Loss: 0.5467214286327362, Final Batch Loss: 0.2921576201915741\n",
      "Epoch 1028, Loss: 0.5757205784320831, Final Batch Loss: 0.2828332185745239\n",
      "Epoch 1029, Loss: 0.5393840670585632, Final Batch Loss: 0.2379603087902069\n",
      "Epoch 1030, Loss: 0.525212287902832, Final Batch Loss: 0.26650696992874146\n",
      "Epoch 1031, Loss: 0.5645480751991272, Final Batch Loss: 0.2446829080581665\n",
      "Epoch 1032, Loss: 0.5409907102584839, Final Batch Loss: 0.24825182557106018\n",
      "Epoch 1033, Loss: 0.503400132060051, Final Batch Loss: 0.24514923989772797\n",
      "Epoch 1034, Loss: 0.5681932270526886, Final Batch Loss: 0.2998163402080536\n",
      "Epoch 1035, Loss: 0.6338647603988647, Final Batch Loss: 0.3685227930545807\n",
      "Epoch 1036, Loss: 0.5287789553403854, Final Batch Loss: 0.30281421542167664\n",
      "Epoch 1037, Loss: 0.5294828712940216, Final Batch Loss: 0.25961071252822876\n",
      "Epoch 1038, Loss: 0.432139128446579, Final Batch Loss: 0.1806023120880127\n",
      "Epoch 1039, Loss: 0.5720778107643127, Final Batch Loss: 0.2736422121524811\n",
      "Epoch 1040, Loss: 0.5456828624010086, Final Batch Loss: 0.2975181043148041\n",
      "Epoch 1041, Loss: 0.4962374120950699, Final Batch Loss: 0.22803784906864166\n",
      "Epoch 1042, Loss: 0.5981336832046509, Final Batch Loss: 0.30108046531677246\n",
      "Epoch 1043, Loss: 0.546227902173996, Final Batch Loss: 0.29672208428382874\n",
      "Epoch 1044, Loss: 0.6627874970436096, Final Batch Loss: 0.38223230838775635\n",
      "Epoch 1045, Loss: 0.4964895248413086, Final Batch Loss: 0.21176958084106445\n",
      "Epoch 1046, Loss: 0.6168001592159271, Final Batch Loss: 0.28758504986763\n",
      "Epoch 1047, Loss: 0.5645093321800232, Final Batch Loss: 0.3041747510433197\n",
      "Epoch 1048, Loss: 0.5486314296722412, Final Batch Loss: 0.26479896903038025\n",
      "Epoch 1049, Loss: 0.5620330274105072, Final Batch Loss: 0.2932818531990051\n",
      "Epoch 1050, Loss: 0.5490847527980804, Final Batch Loss: 0.23600634932518005\n",
      "Epoch 1051, Loss: 0.5609689354896545, Final Batch Loss: 0.2973431646823883\n",
      "Epoch 1052, Loss: 0.5748327672481537, Final Batch Loss: 0.26963794231414795\n",
      "Epoch 1053, Loss: 0.533935159444809, Final Batch Loss: 0.28996741771698\n",
      "Epoch 1054, Loss: 0.617561936378479, Final Batch Loss: 0.25020718574523926\n",
      "Epoch 1055, Loss: 0.5409180223941803, Final Batch Loss: 0.2787911593914032\n",
      "Epoch 1056, Loss: 0.650604784488678, Final Batch Loss: 0.3301287889480591\n",
      "Epoch 1057, Loss: 0.5431161820888519, Final Batch Loss: 0.2274673581123352\n",
      "Epoch 1058, Loss: 0.5819058120250702, Final Batch Loss: 0.2712387442588806\n",
      "Epoch 1059, Loss: 0.5422176420688629, Final Batch Loss: 0.25010931491851807\n",
      "Epoch 1060, Loss: 0.49781182408332825, Final Batch Loss: 0.23925486207008362\n",
      "Epoch 1061, Loss: 0.5280434489250183, Final Batch Loss: 0.2585897445678711\n",
      "Epoch 1062, Loss: 0.6777973175048828, Final Batch Loss: 0.32721421122550964\n",
      "Epoch 1063, Loss: 0.631648451089859, Final Batch Loss: 0.3344203531742096\n",
      "Epoch 1064, Loss: 0.5616416335105896, Final Batch Loss: 0.25620007514953613\n",
      "Epoch 1065, Loss: 0.5425727367401123, Final Batch Loss: 0.2934754192829132\n",
      "Epoch 1066, Loss: 0.595900148153305, Final Batch Loss: 0.31592121720314026\n",
      "Epoch 1067, Loss: 0.5962271094322205, Final Batch Loss: 0.3078770637512207\n",
      "Epoch 1068, Loss: 0.5570614039897919, Final Batch Loss: 0.2708929181098938\n",
      "Epoch 1069, Loss: 0.614102840423584, Final Batch Loss: 0.33594176173210144\n",
      "Epoch 1070, Loss: 0.46475471556186676, Final Batch Loss: 0.1928759664297104\n",
      "Epoch 1071, Loss: 0.535244882106781, Final Batch Loss: 0.29947784543037415\n",
      "Epoch 1072, Loss: 0.5135844051837921, Final Batch Loss: 0.25374138355255127\n",
      "Epoch 1073, Loss: 0.5268753916025162, Final Batch Loss: 0.3021720349788666\n",
      "Epoch 1074, Loss: 0.6122953295707703, Final Batch Loss: 0.2893409729003906\n",
      "Epoch 1075, Loss: 0.6414876580238342, Final Batch Loss: 0.34639427065849304\n",
      "Epoch 1076, Loss: 0.5218074321746826, Final Batch Loss: 0.2555360794067383\n",
      "Epoch 1077, Loss: 0.5112311840057373, Final Batch Loss: 0.2547438442707062\n",
      "Epoch 1078, Loss: 0.5808015465736389, Final Batch Loss: 0.2957437038421631\n",
      "Epoch 1079, Loss: 0.5424017608165741, Final Batch Loss: 0.24822044372558594\n",
      "Epoch 1080, Loss: 0.49403591454029083, Final Batch Loss: 0.2362133413553238\n",
      "Epoch 1081, Loss: 0.568837121129036, Final Batch Loss: 0.2252238541841507\n",
      "Epoch 1082, Loss: 0.5811781138181686, Final Batch Loss: 0.338880330324173\n",
      "Epoch 1083, Loss: 0.5995365977287292, Final Batch Loss: 0.3280414640903473\n",
      "Epoch 1084, Loss: 0.5239731073379517, Final Batch Loss: 0.2665402889251709\n",
      "Epoch 1085, Loss: 0.5108506083488464, Final Batch Loss: 0.2360774576663971\n",
      "Epoch 1086, Loss: 0.5109276324510574, Final Batch Loss: 0.2784738540649414\n",
      "Epoch 1087, Loss: 0.5641505569219589, Final Batch Loss: 0.2485061138868332\n",
      "Epoch 1088, Loss: 0.5749715566635132, Final Batch Loss: 0.29559868574142456\n",
      "Epoch 1089, Loss: 0.4773653447628021, Final Batch Loss: 0.21576762199401855\n",
      "Epoch 1090, Loss: 0.5426902025938034, Final Batch Loss: 0.31821542978286743\n",
      "Epoch 1091, Loss: 0.5451884269714355, Final Batch Loss: 0.25787657499313354\n",
      "Epoch 1092, Loss: 0.5062806457281113, Final Batch Loss: 0.2359372228384018\n",
      "Epoch 1093, Loss: 0.5249371081590652, Final Batch Loss: 0.226328507065773\n",
      "Epoch 1094, Loss: 0.5869495123624802, Final Batch Loss: 0.24450786411762238\n",
      "Epoch 1095, Loss: 0.5209169387817383, Final Batch Loss: 0.27336832880973816\n",
      "Epoch 1096, Loss: 0.5615906864404678, Final Batch Loss: 0.3461616039276123\n",
      "Epoch 1097, Loss: 0.5748082399368286, Final Batch Loss: 0.314274400472641\n",
      "Epoch 1098, Loss: 0.4789341241121292, Final Batch Loss: 0.23773892223834991\n",
      "Epoch 1099, Loss: 0.48312629759311676, Final Batch Loss: 0.2377346158027649\n",
      "Epoch 1100, Loss: 0.6102219223976135, Final Batch Loss: 0.3312927782535553\n",
      "Epoch 1101, Loss: 0.4724514037370682, Final Batch Loss: 0.24039630591869354\n",
      "Epoch 1102, Loss: 0.5338730812072754, Final Batch Loss: 0.2653067409992218\n",
      "Epoch 1103, Loss: 0.5472452193498611, Final Batch Loss: 0.3330530524253845\n",
      "Epoch 1104, Loss: 0.5135424882173538, Final Batch Loss: 0.24148760735988617\n",
      "Epoch 1105, Loss: 0.6404706835746765, Final Batch Loss: 0.3093196153640747\n",
      "Epoch 1106, Loss: 0.5438864678144455, Final Batch Loss: 0.29750367999076843\n",
      "Epoch 1107, Loss: 0.5524817407131195, Final Batch Loss: 0.2920101284980774\n",
      "Epoch 1108, Loss: 0.5128042548894882, Final Batch Loss: 0.2812308669090271\n",
      "Epoch 1109, Loss: 0.5870822072029114, Final Batch Loss: 0.3616805970668793\n",
      "Epoch 1110, Loss: 0.5217189341783524, Final Batch Loss: 0.28855472803115845\n",
      "Epoch 1111, Loss: 0.5931953489780426, Final Batch Loss: 0.3333260416984558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1112, Loss: 0.5592891871929169, Final Batch Loss: 0.3064972460269928\n",
      "Epoch 1113, Loss: 0.5233507752418518, Final Batch Loss: 0.2690184712409973\n",
      "Epoch 1114, Loss: 0.5586002171039581, Final Batch Loss: 0.3066723346710205\n",
      "Epoch 1115, Loss: 0.5202684998512268, Final Batch Loss: 0.253580242395401\n",
      "Epoch 1116, Loss: 0.5107804238796234, Final Batch Loss: 0.2193203568458557\n",
      "Epoch 1117, Loss: 0.4943614602088928, Final Batch Loss: 0.2673189640045166\n",
      "Epoch 1118, Loss: 0.5936861038208008, Final Batch Loss: 0.30143454670906067\n",
      "Epoch 1119, Loss: 0.497151255607605, Final Batch Loss: 0.2929879426956177\n",
      "Epoch 1120, Loss: 0.6343042552471161, Final Batch Loss: 0.3704938292503357\n",
      "Epoch 1121, Loss: 0.5243834853172302, Final Batch Loss: 0.26192352175712585\n",
      "Epoch 1122, Loss: 0.5998826026916504, Final Batch Loss: 0.28521624207496643\n",
      "Epoch 1123, Loss: 0.541927695274353, Final Batch Loss: 0.2739800214767456\n",
      "Epoch 1124, Loss: 0.42744041979312897, Final Batch Loss: 0.21316763758659363\n",
      "Epoch 1125, Loss: 0.49042809009552, Final Batch Loss: 0.19781282544136047\n",
      "Epoch 1126, Loss: 0.5734745264053345, Final Batch Loss: 0.26589491963386536\n",
      "Epoch 1127, Loss: 0.6643387377262115, Final Batch Loss: 0.4037853181362152\n",
      "Epoch 1128, Loss: 0.6529655456542969, Final Batch Loss: 0.36409199237823486\n",
      "Epoch 1129, Loss: 0.5251524746417999, Final Batch Loss: 0.2121601700782776\n",
      "Epoch 1130, Loss: 0.5401988923549652, Final Batch Loss: 0.25327247381210327\n",
      "Epoch 1131, Loss: 0.49766451120376587, Final Batch Loss: 0.2395932972431183\n",
      "Epoch 1132, Loss: 0.4924328178167343, Final Batch Loss: 0.21512876451015472\n",
      "Epoch 1133, Loss: 0.5427922904491425, Final Batch Loss: 0.2726476192474365\n",
      "Epoch 1134, Loss: 0.5007221549749374, Final Batch Loss: 0.22448556125164032\n",
      "Epoch 1135, Loss: 0.5061523169279099, Final Batch Loss: 0.25716087222099304\n",
      "Epoch 1136, Loss: 0.5195477604866028, Final Batch Loss: 0.24334058165550232\n",
      "Epoch 1137, Loss: 0.5108389258384705, Final Batch Loss: 0.2538077235221863\n",
      "Epoch 1138, Loss: 0.5089332014322281, Final Batch Loss: 0.2806381583213806\n",
      "Epoch 1139, Loss: 0.5011430978775024, Final Batch Loss: 0.23355644941329956\n",
      "Epoch 1140, Loss: 0.5908913910388947, Final Batch Loss: 0.28884801268577576\n",
      "Epoch 1141, Loss: 0.45186589658260345, Final Batch Loss: 0.1810968667268753\n",
      "Epoch 1142, Loss: 0.5062903314828873, Final Batch Loss: 0.2687776982784271\n",
      "Epoch 1143, Loss: 0.5088919997215271, Final Batch Loss: 0.21874889731407166\n",
      "Epoch 1144, Loss: 0.5132450610399246, Final Batch Loss: 0.22831769287586212\n",
      "Epoch 1145, Loss: 0.4689996987581253, Final Batch Loss: 0.20261268317699432\n",
      "Epoch 1146, Loss: 0.4996291399002075, Final Batch Loss: 0.30354443192481995\n",
      "Epoch 1147, Loss: 0.5638782978057861, Final Batch Loss: 0.31244102120399475\n",
      "Epoch 1148, Loss: 0.5308886915445328, Final Batch Loss: 0.2954561412334442\n",
      "Epoch 1149, Loss: 0.49307022988796234, Final Batch Loss: 0.24182580411434174\n",
      "Epoch 1150, Loss: 0.5459187924861908, Final Batch Loss: 0.2851116955280304\n",
      "Epoch 1151, Loss: 0.5280723571777344, Final Batch Loss: 0.2562142014503479\n",
      "Epoch 1152, Loss: 0.5164767652750015, Final Batch Loss: 0.2069815844297409\n",
      "Epoch 1153, Loss: 0.5207359790802002, Final Batch Loss: 0.2569755017757416\n",
      "Epoch 1154, Loss: 0.4930124282836914, Final Batch Loss: 0.24998508393764496\n",
      "Epoch 1155, Loss: 0.5034360885620117, Final Batch Loss: 0.2412126064300537\n",
      "Epoch 1156, Loss: 0.5446185171604156, Final Batch Loss: 0.265600323677063\n",
      "Epoch 1157, Loss: 0.48876167833805084, Final Batch Loss: 0.2591378390789032\n",
      "Epoch 1158, Loss: 0.5772146880626678, Final Batch Loss: 0.3214815557003021\n",
      "Epoch 1159, Loss: 0.5971978604793549, Final Batch Loss: 0.2813468277454376\n",
      "Epoch 1160, Loss: 0.4936909079551697, Final Batch Loss: 0.28757137060165405\n",
      "Epoch 1161, Loss: 0.4295421242713928, Final Batch Loss: 0.2486177384853363\n",
      "Epoch 1162, Loss: 0.5366393178701401, Final Batch Loss: 0.29107871651649475\n",
      "Epoch 1163, Loss: 0.5209588557481766, Final Batch Loss: 0.28281840682029724\n",
      "Epoch 1164, Loss: 0.5183005034923553, Final Batch Loss: 0.24028751254081726\n",
      "Epoch 1165, Loss: 0.4550107419490814, Final Batch Loss: 0.17652979493141174\n",
      "Epoch 1166, Loss: 0.5521008670330048, Final Batch Loss: 0.26400959491729736\n",
      "Epoch 1167, Loss: 0.535554364323616, Final Batch Loss: 0.2906099557876587\n",
      "Epoch 1168, Loss: 0.554909348487854, Final Batch Loss: 0.26133862137794495\n",
      "Epoch 1169, Loss: 0.5272413343191147, Final Batch Loss: 0.24357976019382477\n",
      "Epoch 1170, Loss: 0.5349252671003342, Final Batch Loss: 0.23469452559947968\n",
      "Epoch 1171, Loss: 0.5321498662233353, Final Batch Loss: 0.2991264760494232\n",
      "Epoch 1172, Loss: 0.5848364531993866, Final Batch Loss: 0.286590039730072\n",
      "Epoch 1173, Loss: 0.49523119628429413, Final Batch Loss: 0.21863771975040436\n",
      "Epoch 1174, Loss: 0.5088856518268585, Final Batch Loss: 0.2688363790512085\n",
      "Epoch 1175, Loss: 0.5358998328447342, Final Batch Loss: 0.31483063101768494\n",
      "Epoch 1176, Loss: 0.5045601427555084, Final Batch Loss: 0.21980008482933044\n",
      "Epoch 1177, Loss: 0.45545288920402527, Final Batch Loss: 0.17202910780906677\n",
      "Epoch 1178, Loss: 0.46888330578804016, Final Batch Loss: 0.2096821665763855\n",
      "Epoch 1179, Loss: 0.5091626942157745, Final Batch Loss: 0.22220203280448914\n",
      "Epoch 1180, Loss: 0.6909641027450562, Final Batch Loss: 0.3753271996974945\n",
      "Epoch 1181, Loss: 0.4865652620792389, Final Batch Loss: 0.24865789711475372\n",
      "Epoch 1182, Loss: 0.47212380170822144, Final Batch Loss: 0.27733319997787476\n",
      "Epoch 1183, Loss: 0.42977163195610046, Final Batch Loss: 0.2314661592245102\n",
      "Epoch 1184, Loss: 0.46212998032569885, Final Batch Loss: 0.2503497004508972\n",
      "Epoch 1185, Loss: 0.42082037031650543, Final Batch Loss: 0.1537017673254013\n",
      "Epoch 1186, Loss: 0.4662512540817261, Final Batch Loss: 0.22312965989112854\n",
      "Epoch 1187, Loss: 0.6066077351570129, Final Batch Loss: 0.3337017893791199\n",
      "Epoch 1188, Loss: 0.41552096605300903, Final Batch Loss: 0.23732995986938477\n",
      "Epoch 1189, Loss: 0.5343794822692871, Final Batch Loss: 0.29261887073516846\n",
      "Epoch 1190, Loss: 0.559005469083786, Final Batch Loss: 0.31385353207588196\n",
      "Epoch 1191, Loss: 0.44236041605472565, Final Batch Loss: 0.20338074862957\n",
      "Epoch 1192, Loss: 0.5084165930747986, Final Batch Loss: 0.2511294186115265\n",
      "Epoch 1193, Loss: 0.4827026277780533, Final Batch Loss: 0.22930045425891876\n",
      "Epoch 1194, Loss: 0.4706060290336609, Final Batch Loss: 0.23291775584220886\n",
      "Epoch 1195, Loss: 0.46179889142513275, Final Batch Loss: 0.23888881504535675\n",
      "Epoch 1196, Loss: 0.4348474442958832, Final Batch Loss: 0.2496579885482788\n",
      "Epoch 1197, Loss: 0.47685128450393677, Final Batch Loss: 0.20675420761108398\n",
      "Epoch 1198, Loss: 0.44468261301517487, Final Batch Loss: 0.2531156539916992\n",
      "Epoch 1199, Loss: 0.6040257066488266, Final Batch Loss: 0.36529025435447693\n",
      "Epoch 1200, Loss: 0.48883046209812164, Final Batch Loss: 0.26478323340415955\n",
      "Epoch 1201, Loss: 0.5473943054676056, Final Batch Loss: 0.2630901634693146\n",
      "Epoch 1202, Loss: 0.4836377501487732, Final Batch Loss: 0.22007855772972107\n",
      "Epoch 1203, Loss: 0.5056341290473938, Final Batch Loss: 0.2541622519493103\n",
      "Epoch 1204, Loss: 0.4524693340063095, Final Batch Loss: 0.1882263869047165\n",
      "Epoch 1205, Loss: 0.4596179276704788, Final Batch Loss: 0.20623241364955902\n",
      "Epoch 1206, Loss: 0.4664008915424347, Final Batch Loss: 0.24006566405296326\n",
      "Epoch 1207, Loss: 0.47797994315624237, Final Batch Loss: 0.23659826815128326\n",
      "Epoch 1208, Loss: 0.4774857312440872, Final Batch Loss: 0.22279314696788788\n",
      "Epoch 1209, Loss: 0.5178650915622711, Final Batch Loss: 0.26252350211143494\n",
      "Epoch 1210, Loss: 0.48396702110767365, Final Batch Loss: 0.22610436379909515\n",
      "Epoch 1211, Loss: 0.5652069449424744, Final Batch Loss: 0.271768718957901\n",
      "Epoch 1212, Loss: 0.45817552506923676, Final Batch Loss: 0.1885318011045456\n",
      "Epoch 1213, Loss: 0.5096678733825684, Final Batch Loss: 0.25137433409690857\n",
      "Epoch 1214, Loss: 0.5738114267587662, Final Batch Loss: 0.3410104215145111\n",
      "Epoch 1215, Loss: 0.5857778787612915, Final Batch Loss: 0.3041299879550934\n",
      "Epoch 1216, Loss: 0.5469650328159332, Final Batch Loss: 0.26188212633132935\n",
      "Epoch 1217, Loss: 0.36749422550201416, Final Batch Loss: 0.16554562747478485\n",
      "Epoch 1218, Loss: 0.48958389461040497, Final Batch Loss: 0.28903648257255554\n",
      "Epoch 1219, Loss: 0.5477148294448853, Final Batch Loss: 0.28881776332855225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1220, Loss: 0.45665135979652405, Final Batch Loss: 0.2238074243068695\n",
      "Epoch 1221, Loss: 0.5692698359489441, Final Batch Loss: 0.29492810368537903\n",
      "Epoch 1222, Loss: 0.45726436376571655, Final Batch Loss: 0.24482382833957672\n",
      "Epoch 1223, Loss: 0.44713273644447327, Final Batch Loss: 0.20130063593387604\n",
      "Epoch 1224, Loss: 0.4955008327960968, Final Batch Loss: 0.2478623241186142\n",
      "Epoch 1225, Loss: 0.4490302950143814, Final Batch Loss: 0.21961797773838043\n",
      "Epoch 1226, Loss: 0.4556952714920044, Final Batch Loss: 0.2562403082847595\n",
      "Epoch 1227, Loss: 0.4886827915906906, Final Batch Loss: 0.29677310585975647\n",
      "Epoch 1228, Loss: 0.4981559067964554, Final Batch Loss: 0.23815543949604034\n",
      "Epoch 1229, Loss: 0.5791623294353485, Final Batch Loss: 0.27255013585090637\n",
      "Epoch 1230, Loss: 0.42742055654525757, Final Batch Loss: 0.18328621983528137\n",
      "Epoch 1231, Loss: 0.5212880373001099, Final Batch Loss: 0.20799106359481812\n",
      "Epoch 1232, Loss: 0.5629570782184601, Final Batch Loss: 0.25237587094306946\n",
      "Epoch 1233, Loss: 0.4732401967048645, Final Batch Loss: 0.24977657198905945\n",
      "Epoch 1234, Loss: 0.4115907996892929, Final Batch Loss: 0.15515200793743134\n",
      "Epoch 1235, Loss: 0.44368089735507965, Final Batch Loss: 0.2183772623538971\n",
      "Epoch 1236, Loss: 0.4721135348081589, Final Batch Loss: 0.22441451251506805\n",
      "Epoch 1237, Loss: 0.4292367994785309, Final Batch Loss: 0.20455925166606903\n",
      "Epoch 1238, Loss: 0.4300081878900528, Final Batch Loss: 0.18708279728889465\n",
      "Epoch 1239, Loss: 0.512748658657074, Final Batch Loss: 0.3233563303947449\n",
      "Epoch 1240, Loss: 0.5020749568939209, Final Batch Loss: 0.29170486330986023\n",
      "Epoch 1241, Loss: 0.5247660577297211, Final Batch Loss: 0.25247514247894287\n",
      "Epoch 1242, Loss: 0.4644206017255783, Final Batch Loss: 0.19687126576900482\n",
      "Epoch 1243, Loss: 0.4079211354255676, Final Batch Loss: 0.18684938549995422\n",
      "Epoch 1244, Loss: 0.5829582363367081, Final Batch Loss: 0.3409871459007263\n",
      "Epoch 1245, Loss: 0.4330127686262131, Final Batch Loss: 0.2402663379907608\n",
      "Epoch 1246, Loss: 0.47659799456596375, Final Batch Loss: 0.21181726455688477\n",
      "Epoch 1247, Loss: 0.4775315374135971, Final Batch Loss: 0.22364114224910736\n",
      "Epoch 1248, Loss: 0.5456912964582443, Final Batch Loss: 0.24714504182338715\n",
      "Epoch 1249, Loss: 0.45376522839069366, Final Batch Loss: 0.21700289845466614\n",
      "Epoch 1250, Loss: 0.4265799820423126, Final Batch Loss: 0.19421203434467316\n",
      "Epoch 1251, Loss: 0.4655963182449341, Final Batch Loss: 0.2140296995639801\n",
      "Epoch 1252, Loss: 0.4118109494447708, Final Batch Loss: 0.20338796079158783\n",
      "Epoch 1253, Loss: 0.4986868053674698, Final Batch Loss: 0.26851463317871094\n",
      "Epoch 1254, Loss: 0.5248213857412338, Final Batch Loss: 0.29783084988594055\n",
      "Epoch 1255, Loss: 0.5025796890258789, Final Batch Loss: 0.2534931004047394\n",
      "Epoch 1256, Loss: 0.471354216337204, Final Batch Loss: 0.22796542942523956\n",
      "Epoch 1257, Loss: 0.5015367567539215, Final Batch Loss: 0.2971176505088806\n",
      "Epoch 1258, Loss: 0.4080626368522644, Final Batch Loss: 0.23303216695785522\n",
      "Epoch 1259, Loss: 0.48353244364261627, Final Batch Loss: 0.24763673543930054\n",
      "Epoch 1260, Loss: 0.4709306061267853, Final Batch Loss: 0.19651958346366882\n",
      "Epoch 1261, Loss: 0.4362751245498657, Final Batch Loss: 0.17972779273986816\n",
      "Epoch 1262, Loss: 0.49345940351486206, Final Batch Loss: 0.29161885380744934\n",
      "Epoch 1263, Loss: 0.5547150671482086, Final Batch Loss: 0.291503369808197\n",
      "Epoch 1264, Loss: 0.41693463921546936, Final Batch Loss: 0.2116079181432724\n",
      "Epoch 1265, Loss: 0.4396388977766037, Final Batch Loss: 0.24217970669269562\n",
      "Epoch 1266, Loss: 0.475688174366951, Final Batch Loss: 0.22216026484966278\n",
      "Epoch 1267, Loss: 0.47341717779636383, Final Batch Loss: 0.2483004778623581\n",
      "Epoch 1268, Loss: 0.5334589630365372, Final Batch Loss: 0.240437850356102\n",
      "Epoch 1269, Loss: 0.47962844371795654, Final Batch Loss: 0.25333845615386963\n",
      "Epoch 1270, Loss: 0.44915811717510223, Final Batch Loss: 0.2247299998998642\n",
      "Epoch 1271, Loss: 0.4376227855682373, Final Batch Loss: 0.19775736331939697\n",
      "Epoch 1272, Loss: 0.43164093792438507, Final Batch Loss: 0.21876928210258484\n",
      "Epoch 1273, Loss: 0.5254033356904984, Final Batch Loss: 0.22831057012081146\n",
      "Epoch 1274, Loss: 0.4590274840593338, Final Batch Loss: 0.2008133977651596\n",
      "Epoch 1275, Loss: 0.4499177634716034, Final Batch Loss: 0.23475050926208496\n",
      "Epoch 1276, Loss: 0.44016367197036743, Final Batch Loss: 0.2320474237203598\n",
      "Epoch 1277, Loss: 0.43850918114185333, Final Batch Loss: 0.2094229906797409\n",
      "Epoch 1278, Loss: 0.43623022735118866, Final Batch Loss: 0.2376604527235031\n",
      "Epoch 1279, Loss: 0.4479003995656967, Final Batch Loss: 0.25226545333862305\n",
      "Epoch 1280, Loss: 0.4583870470523834, Final Batch Loss: 0.22264502942562103\n",
      "Epoch 1281, Loss: 0.43302810192108154, Final Batch Loss: 0.26992279291152954\n",
      "Epoch 1282, Loss: 0.4432898014783859, Final Batch Loss: 0.22434788942337036\n",
      "Epoch 1283, Loss: 0.4484705477952957, Final Batch Loss: 0.21844272315502167\n",
      "Epoch 1284, Loss: 0.501380205154419, Final Batch Loss: 0.29676374793052673\n",
      "Epoch 1285, Loss: 0.4096943289041519, Final Batch Loss: 0.16419076919555664\n",
      "Epoch 1286, Loss: 0.5229432582855225, Final Batch Loss: 0.28073495626449585\n",
      "Epoch 1287, Loss: 0.4257342964410782, Final Batch Loss: 0.18326862156391144\n",
      "Epoch 1288, Loss: 0.5134264379739761, Final Batch Loss: 0.24172790348529816\n",
      "Epoch 1289, Loss: 0.4518405497074127, Final Batch Loss: 0.2582279443740845\n",
      "Epoch 1290, Loss: 0.4395667165517807, Final Batch Loss: 0.2193833291530609\n",
      "Epoch 1291, Loss: 0.45149487257003784, Final Batch Loss: 0.1816748082637787\n",
      "Epoch 1292, Loss: 0.424394428730011, Final Batch Loss: 0.19010941684246063\n",
      "Epoch 1293, Loss: 0.43243347108364105, Final Batch Loss: 0.2000850886106491\n",
      "Epoch 1294, Loss: 0.40612107515335083, Final Batch Loss: 0.21903236210346222\n",
      "Epoch 1295, Loss: 0.6160738468170166, Final Batch Loss: 0.33148378133773804\n",
      "Epoch 1296, Loss: 0.5219221264123917, Final Batch Loss: 0.30585139989852905\n",
      "Epoch 1297, Loss: 0.4510894864797592, Final Batch Loss: 0.2205827683210373\n",
      "Epoch 1298, Loss: 0.5231905877590179, Final Batch Loss: 0.291029691696167\n",
      "Epoch 1299, Loss: 0.3844374120235443, Final Batch Loss: 0.15145975351333618\n",
      "Epoch 1300, Loss: 0.5577861070632935, Final Batch Loss: 0.285014808177948\n",
      "Epoch 1301, Loss: 0.44218146800994873, Final Batch Loss: 0.2389201521873474\n",
      "Epoch 1302, Loss: 0.4136730283498764, Final Batch Loss: 0.16946257650852203\n",
      "Epoch 1303, Loss: 0.4172067940235138, Final Batch Loss: 0.18518675863742828\n",
      "Epoch 1304, Loss: 0.5335629880428314, Final Batch Loss: 0.2534528970718384\n",
      "Epoch 1305, Loss: 0.40837545692920685, Final Batch Loss: 0.22458770871162415\n",
      "Epoch 1306, Loss: 0.487340047955513, Final Batch Loss: 0.2428407222032547\n",
      "Epoch 1307, Loss: 0.4658023566007614, Final Batch Loss: 0.23566721379756927\n",
      "Epoch 1308, Loss: 0.4573652446269989, Final Batch Loss: 0.15529939532279968\n",
      "Epoch 1309, Loss: 0.48569679260253906, Final Batch Loss: 0.2844702899456024\n",
      "Epoch 1310, Loss: 0.4615572392940521, Final Batch Loss: 0.2406836748123169\n",
      "Epoch 1311, Loss: 0.416770339012146, Final Batch Loss: 0.21073578298091888\n",
      "Epoch 1312, Loss: 0.3685997724533081, Final Batch Loss: 0.16170711815357208\n",
      "Epoch 1313, Loss: 0.5127090811729431, Final Batch Loss: 0.3056039810180664\n",
      "Epoch 1314, Loss: 0.4709123373031616, Final Batch Loss: 0.2311154007911682\n",
      "Epoch 1315, Loss: 0.38863296806812286, Final Batch Loss: 0.18018674850463867\n",
      "Epoch 1316, Loss: 0.40075094997882843, Final Batch Loss: 0.2143663614988327\n",
      "Epoch 1317, Loss: 0.4942278414964676, Final Batch Loss: 0.22004611790180206\n",
      "Epoch 1318, Loss: 0.4351954907178879, Final Batch Loss: 0.21111202239990234\n",
      "Epoch 1319, Loss: 0.5611843019723892, Final Batch Loss: 0.32097241282463074\n",
      "Epoch 1320, Loss: 0.41844893991947174, Final Batch Loss: 0.22600343823432922\n",
      "Epoch 1321, Loss: 0.4568665027618408, Final Batch Loss: 0.1982746720314026\n",
      "Epoch 1322, Loss: 0.4202229827642441, Final Batch Loss: 0.2004055678844452\n",
      "Epoch 1323, Loss: 0.4817514568567276, Final Batch Loss: 0.23878753185272217\n",
      "Epoch 1324, Loss: 0.5858651995658875, Final Batch Loss: 0.3578353524208069\n",
      "Epoch 1325, Loss: 0.4409768432378769, Final Batch Loss: 0.22290116548538208\n",
      "Epoch 1326, Loss: 0.5223978608846664, Final Batch Loss: 0.2757200002670288\n",
      "Epoch 1327, Loss: 0.4792678654193878, Final Batch Loss: 0.22665590047836304\n",
      "Epoch 1328, Loss: 0.6171825081110001, Final Batch Loss: 0.43466824293136597\n",
      "Epoch 1329, Loss: 0.4583563804626465, Final Batch Loss: 0.2303650826215744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1330, Loss: 0.4885738492012024, Final Batch Loss: 0.2571834623813629\n",
      "Epoch 1331, Loss: 0.47321006655693054, Final Batch Loss: 0.22639435529708862\n",
      "Epoch 1332, Loss: 0.4455372393131256, Final Batch Loss: 0.22136902809143066\n",
      "Epoch 1333, Loss: 0.474366158246994, Final Batch Loss: 0.23948542773723602\n",
      "Epoch 1334, Loss: 0.3866482228040695, Final Batch Loss: 0.20821313560009003\n",
      "Epoch 1335, Loss: 0.4812301844358444, Final Batch Loss: 0.2783069908618927\n",
      "Epoch 1336, Loss: 0.44086451828479767, Final Batch Loss: 0.2529008686542511\n",
      "Epoch 1337, Loss: 0.4743238538503647, Final Batch Loss: 0.26191017031669617\n",
      "Epoch 1338, Loss: 0.38844868540763855, Final Batch Loss: 0.20776762068271637\n",
      "Epoch 1339, Loss: 0.5056392401456833, Final Batch Loss: 0.23586662113666534\n",
      "Epoch 1340, Loss: 0.5171569883823395, Final Batch Loss: 0.28280019760131836\n",
      "Epoch 1341, Loss: 0.4128493368625641, Final Batch Loss: 0.18052351474761963\n",
      "Epoch 1342, Loss: 0.5135793387889862, Final Batch Loss: 0.24129411578178406\n",
      "Epoch 1343, Loss: 0.48797133564949036, Final Batch Loss: 0.26617884635925293\n",
      "Epoch 1344, Loss: 0.46746084094047546, Final Batch Loss: 0.21029585599899292\n",
      "Epoch 1345, Loss: 0.4413788169622421, Final Batch Loss: 0.23029698431491852\n",
      "Epoch 1346, Loss: 0.45778822898864746, Final Batch Loss: 0.22022883594036102\n",
      "Epoch 1347, Loss: 0.5467879772186279, Final Batch Loss: 0.3002151846885681\n",
      "Epoch 1348, Loss: 0.49667564034461975, Final Batch Loss: 0.25050896406173706\n",
      "Epoch 1349, Loss: 0.5152865201234818, Final Batch Loss: 0.266781210899353\n",
      "Epoch 1350, Loss: 0.48969267308712006, Final Batch Loss: 0.23558412492275238\n",
      "Epoch 1351, Loss: 0.47227756679058075, Final Batch Loss: 0.23568882048130035\n",
      "Epoch 1352, Loss: 0.3636985421180725, Final Batch Loss: 0.1531791090965271\n",
      "Epoch 1353, Loss: 0.5233577489852905, Final Batch Loss: 0.2560240924358368\n",
      "Epoch 1354, Loss: 0.4743350148200989, Final Batch Loss: 0.2189631462097168\n",
      "Epoch 1355, Loss: 0.43807704746723175, Final Batch Loss: 0.21078699827194214\n",
      "Epoch 1356, Loss: 0.4323092848062515, Final Batch Loss: 0.24030639231204987\n",
      "Epoch 1357, Loss: 0.38913094997406006, Final Batch Loss: 0.18015384674072266\n",
      "Epoch 1358, Loss: 0.4720945507287979, Final Batch Loss: 0.23171204328536987\n",
      "Epoch 1359, Loss: 0.469956710934639, Final Batch Loss: 0.23028390109539032\n",
      "Epoch 1360, Loss: 0.4964381605386734, Final Batch Loss: 0.2573680281639099\n",
      "Epoch 1361, Loss: 0.46861620247364044, Final Batch Loss: 0.22393053770065308\n",
      "Epoch 1362, Loss: 0.48856106400489807, Final Batch Loss: 0.23916615545749664\n",
      "Epoch 1363, Loss: 0.408188059926033, Final Batch Loss: 0.1878558248281479\n",
      "Epoch 1364, Loss: 0.37614545226097107, Final Batch Loss: 0.16535915434360504\n",
      "Epoch 1365, Loss: 0.39580492675304413, Final Batch Loss: 0.18537990748882294\n",
      "Epoch 1366, Loss: 0.5028948038816452, Final Batch Loss: 0.3025026321411133\n",
      "Epoch 1367, Loss: 0.4302375167608261, Final Batch Loss: 0.18615837395191193\n",
      "Epoch 1368, Loss: 0.4158080369234085, Final Batch Loss: 0.22759822010993958\n",
      "Epoch 1369, Loss: 0.4790165275335312, Final Batch Loss: 0.25389012694358826\n",
      "Epoch 1370, Loss: 0.4471108019351959, Final Batch Loss: 0.22962446510791779\n",
      "Epoch 1371, Loss: 0.37132661044597626, Final Batch Loss: 0.1941915899515152\n",
      "Epoch 1372, Loss: 0.4210084229707718, Final Batch Loss: 0.23972217738628387\n",
      "Epoch 1373, Loss: 0.37061309814453125, Final Batch Loss: 0.14738397300243378\n",
      "Epoch 1374, Loss: 0.42391711473464966, Final Batch Loss: 0.2138037085533142\n",
      "Epoch 1375, Loss: 0.37855111062526703, Final Batch Loss: 0.19285383820533752\n",
      "Epoch 1376, Loss: 0.4194061756134033, Final Batch Loss: 0.16536852717399597\n",
      "Epoch 1377, Loss: 0.4352279305458069, Final Batch Loss: 0.21676436066627502\n",
      "Epoch 1378, Loss: 0.4428270757198334, Final Batch Loss: 0.21848073601722717\n",
      "Epoch 1379, Loss: 0.4235978424549103, Final Batch Loss: 0.18306231498718262\n",
      "Epoch 1380, Loss: 0.473297119140625, Final Batch Loss: 0.25879329442977905\n",
      "Epoch 1381, Loss: 0.569023460149765, Final Batch Loss: 0.2650015652179718\n",
      "Epoch 1382, Loss: 0.4844513237476349, Final Batch Loss: 0.2517809569835663\n",
      "Epoch 1383, Loss: 0.44864170253276825, Final Batch Loss: 0.24153421819210052\n",
      "Epoch 1384, Loss: 0.34306153655052185, Final Batch Loss: 0.15780922770500183\n",
      "Epoch 1385, Loss: 0.5042213946580887, Final Batch Loss: 0.23803503811359406\n",
      "Epoch 1386, Loss: 0.40284304320812225, Final Batch Loss: 0.20808471739292145\n",
      "Epoch 1387, Loss: 0.47197334468364716, Final Batch Loss: 0.23110169172286987\n",
      "Epoch 1388, Loss: 0.332326278090477, Final Batch Loss: 0.1788063496351242\n",
      "Epoch 1389, Loss: 0.4778897762298584, Final Batch Loss: 0.22532978653907776\n",
      "Epoch 1390, Loss: 0.5316748321056366, Final Batch Loss: 0.30468329787254333\n",
      "Epoch 1391, Loss: 0.4516516178846359, Final Batch Loss: 0.24941755831241608\n",
      "Epoch 1392, Loss: 0.5622831434011459, Final Batch Loss: 0.3335500657558441\n",
      "Epoch 1393, Loss: 0.4994851350784302, Final Batch Loss: 0.2573205232620239\n",
      "Epoch 1394, Loss: 0.45800793170928955, Final Batch Loss: 0.23415763676166534\n",
      "Epoch 1395, Loss: 0.4106026738882065, Final Batch Loss: 0.21810375154018402\n",
      "Epoch 1396, Loss: 0.40459205210208893, Final Batch Loss: 0.17870493233203888\n",
      "Epoch 1397, Loss: 0.4033810794353485, Final Batch Loss: 0.20699486136436462\n",
      "Epoch 1398, Loss: 0.43680426478385925, Final Batch Loss: 0.14726486802101135\n",
      "Epoch 1399, Loss: 0.43958865106105804, Final Batch Loss: 0.218330517411232\n",
      "Epoch 1400, Loss: 0.3917047530412674, Final Batch Loss: 0.173183411359787\n",
      "Epoch 1401, Loss: 0.39999517798423767, Final Batch Loss: 0.17144496738910675\n",
      "Epoch 1402, Loss: 0.41246363520622253, Final Batch Loss: 0.25775274634361267\n",
      "Epoch 1403, Loss: 0.464126780629158, Final Batch Loss: 0.21580564975738525\n",
      "Epoch 1404, Loss: 0.4418243318796158, Final Batch Loss: 0.21887105703353882\n",
      "Epoch 1405, Loss: 0.45182038843631744, Final Batch Loss: 0.24003399908542633\n",
      "Epoch 1406, Loss: 0.4399734139442444, Final Batch Loss: 0.2578775882720947\n",
      "Epoch 1407, Loss: 0.49051031470298767, Final Batch Loss: 0.2576473653316498\n",
      "Epoch 1408, Loss: 0.44646038115024567, Final Batch Loss: 0.2191077172756195\n",
      "Epoch 1409, Loss: 0.3570125252008438, Final Batch Loss: 0.18953189253807068\n",
      "Epoch 1410, Loss: 0.4681314378976822, Final Batch Loss: 0.2256396859884262\n",
      "Epoch 1411, Loss: 0.38537219166755676, Final Batch Loss: 0.15941211581230164\n",
      "Epoch 1412, Loss: 0.390585795044899, Final Batch Loss: 0.18577508628368378\n",
      "Epoch 1413, Loss: 0.4014980047941208, Final Batch Loss: 0.1857619434595108\n",
      "Epoch 1414, Loss: 0.3881942480802536, Final Batch Loss: 0.2209707796573639\n",
      "Epoch 1415, Loss: 0.3144457936286926, Final Batch Loss: 0.14145182073116302\n",
      "Epoch 1416, Loss: 0.521103173494339, Final Batch Loss: 0.22377893328666687\n",
      "Epoch 1417, Loss: 0.5042736679315567, Final Batch Loss: 0.28417903184890747\n",
      "Epoch 1418, Loss: 0.49353277683258057, Final Batch Loss: 0.21705693006515503\n",
      "Epoch 1419, Loss: 0.4302990585565567, Final Batch Loss: 0.21994684636592865\n",
      "Epoch 1420, Loss: 0.39976245164871216, Final Batch Loss: 0.1991167515516281\n",
      "Epoch 1421, Loss: 0.4566086679697037, Final Batch Loss: 0.19706450402736664\n",
      "Epoch 1422, Loss: 0.4578169584274292, Final Batch Loss: 0.216191366314888\n",
      "Epoch 1423, Loss: 0.37460730969905853, Final Batch Loss: 0.18338672816753387\n",
      "Epoch 1424, Loss: 0.4352586716413498, Final Batch Loss: 0.1574908047914505\n",
      "Epoch 1425, Loss: 0.40823666751384735, Final Batch Loss: 0.1829501837491989\n",
      "Epoch 1426, Loss: 0.46513397991657257, Final Batch Loss: 0.18721427023410797\n",
      "Epoch 1427, Loss: 0.48803988099098206, Final Batch Loss: 0.236465722322464\n",
      "Epoch 1428, Loss: 0.36933988332748413, Final Batch Loss: 0.17441675066947937\n",
      "Epoch 1429, Loss: 0.39350418746471405, Final Batch Loss: 0.1539873629808426\n",
      "Epoch 1430, Loss: 0.44576120376586914, Final Batch Loss: 0.16053375601768494\n",
      "Epoch 1431, Loss: 0.455392062664032, Final Batch Loss: 0.1962202787399292\n",
      "Epoch 1432, Loss: 0.39834776520729065, Final Batch Loss: 0.18350005149841309\n",
      "Epoch 1433, Loss: 0.5017133802175522, Final Batch Loss: 0.28945544362068176\n",
      "Epoch 1434, Loss: 0.4320730119943619, Final Batch Loss: 0.2408779263496399\n",
      "Epoch 1435, Loss: 0.3842196762561798, Final Batch Loss: 0.20662832260131836\n",
      "Epoch 1436, Loss: 0.4473858028650284, Final Batch Loss: 0.20287320017814636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1437, Loss: 0.3971528708934784, Final Batch Loss: 0.18766063451766968\n",
      "Epoch 1438, Loss: 0.4249308854341507, Final Batch Loss: 0.24357473850250244\n",
      "Epoch 1439, Loss: 0.424137145280838, Final Batch Loss: 0.21004317700862885\n",
      "Epoch 1440, Loss: 0.40486280620098114, Final Batch Loss: 0.20135170221328735\n",
      "Epoch 1441, Loss: 0.36175981163978577, Final Batch Loss: 0.14600151777267456\n",
      "Epoch 1442, Loss: 0.344577893614769, Final Batch Loss: 0.213899627327919\n",
      "Epoch 1443, Loss: 0.35600215196609497, Final Batch Loss: 0.18090787529945374\n",
      "Epoch 1444, Loss: 0.48590661585330963, Final Batch Loss: 0.27412378787994385\n",
      "Epoch 1445, Loss: 0.36893436312675476, Final Batch Loss: 0.19266727566719055\n",
      "Epoch 1446, Loss: 0.45219099521636963, Final Batch Loss: 0.27654239535331726\n",
      "Epoch 1447, Loss: 0.5644703507423401, Final Batch Loss: 0.34835970401763916\n",
      "Epoch 1448, Loss: 0.41685423254966736, Final Batch Loss: 0.1755344718694687\n",
      "Epoch 1449, Loss: 0.3690534383058548, Final Batch Loss: 0.17997339367866516\n",
      "Epoch 1450, Loss: 0.42327748239040375, Final Batch Loss: 0.2549154758453369\n",
      "Epoch 1451, Loss: 0.3885095566511154, Final Batch Loss: 0.16855843365192413\n",
      "Epoch 1452, Loss: 0.3863646984100342, Final Batch Loss: 0.19205594062805176\n",
      "Epoch 1453, Loss: 0.39821919798851013, Final Batch Loss: 0.16080604493618011\n",
      "Epoch 1454, Loss: 0.3352105915546417, Final Batch Loss: 0.17525707185268402\n",
      "Epoch 1455, Loss: 0.45563486218452454, Final Batch Loss: 0.24041962623596191\n",
      "Epoch 1456, Loss: 0.4329727590084076, Final Batch Loss: 0.23067109286785126\n",
      "Epoch 1457, Loss: 0.5028678178787231, Final Batch Loss: 0.23366084694862366\n",
      "Epoch 1458, Loss: 0.3903192728757858, Final Batch Loss: 0.20853661000728607\n",
      "Epoch 1459, Loss: 0.42316119372844696, Final Batch Loss: 0.2147478461265564\n",
      "Epoch 1460, Loss: 0.3924097716808319, Final Batch Loss: 0.20189465582370758\n",
      "Epoch 1461, Loss: 0.3923986703157425, Final Batch Loss: 0.209260031580925\n",
      "Epoch 1462, Loss: 0.4939846247434616, Final Batch Loss: 0.20402635633945465\n",
      "Epoch 1463, Loss: 0.5354704856872559, Final Batch Loss: 0.27087530493736267\n",
      "Epoch 1464, Loss: 0.36159850656986237, Final Batch Loss: 0.15104109048843384\n",
      "Epoch 1465, Loss: 0.389320969581604, Final Batch Loss: 0.15568333864212036\n",
      "Epoch 1466, Loss: 0.506353348493576, Final Batch Loss: 0.28316667675971985\n",
      "Epoch 1467, Loss: 0.5042629092931747, Final Batch Loss: 0.2643015384674072\n",
      "Epoch 1468, Loss: 0.36868035793304443, Final Batch Loss: 0.16661794483661652\n",
      "Epoch 1469, Loss: 0.3877636343240738, Final Batch Loss: 0.1864403486251831\n",
      "Epoch 1470, Loss: 0.4985412210226059, Final Batch Loss: 0.2942800521850586\n",
      "Epoch 1471, Loss: 0.39106954634189606, Final Batch Loss: 0.18405593931674957\n",
      "Epoch 1472, Loss: 0.40788574516773224, Final Batch Loss: 0.2165069282054901\n",
      "Epoch 1473, Loss: 0.5120153427124023, Final Batch Loss: 0.3176236152648926\n",
      "Epoch 1474, Loss: 0.4001789838075638, Final Batch Loss: 0.19570420682430267\n",
      "Epoch 1475, Loss: 0.41093580424785614, Final Batch Loss: 0.20259128510951996\n",
      "Epoch 1476, Loss: 0.3619328439235687, Final Batch Loss: 0.1798803210258484\n",
      "Epoch 1477, Loss: 0.3724661320447922, Final Batch Loss: 0.20391444861888885\n",
      "Epoch 1478, Loss: 0.40531980991363525, Final Batch Loss: 0.19103887677192688\n",
      "Epoch 1479, Loss: 0.3821396082639694, Final Batch Loss: 0.13126705586910248\n",
      "Epoch 1480, Loss: 0.4199720025062561, Final Batch Loss: 0.17024193704128265\n",
      "Epoch 1481, Loss: 0.39734336733818054, Final Batch Loss: 0.1453232765197754\n",
      "Epoch 1482, Loss: 0.363677442073822, Final Batch Loss: 0.19167731702327728\n",
      "Epoch 1483, Loss: 0.4413309395313263, Final Batch Loss: 0.24013572931289673\n",
      "Epoch 1484, Loss: 0.4633762836456299, Final Batch Loss: 0.21066367626190186\n",
      "Epoch 1485, Loss: 0.4234355092048645, Final Batch Loss: 0.18216226994991302\n",
      "Epoch 1486, Loss: 0.4600140303373337, Final Batch Loss: 0.19759143888950348\n",
      "Epoch 1487, Loss: 0.3658394366502762, Final Batch Loss: 0.2052605003118515\n",
      "Epoch 1488, Loss: 0.45076784491539, Final Batch Loss: 0.2376651167869568\n",
      "Epoch 1489, Loss: 0.3579181879758835, Final Batch Loss: 0.14645707607269287\n",
      "Epoch 1490, Loss: 0.42072896659374237, Final Batch Loss: 0.23007212579250336\n",
      "Epoch 1491, Loss: 0.37158606946468353, Final Batch Loss: 0.21655555069446564\n",
      "Epoch 1492, Loss: 0.3860485702753067, Final Batch Loss: 0.21382631361484528\n",
      "Epoch 1493, Loss: 0.4390975534915924, Final Batch Loss: 0.20159421861171722\n",
      "Epoch 1494, Loss: 0.3779443949460983, Final Batch Loss: 0.1795351207256317\n",
      "Epoch 1495, Loss: 0.4902281165122986, Final Batch Loss: 0.26985689997673035\n",
      "Epoch 1496, Loss: 0.45348644256591797, Final Batch Loss: 0.31083786487579346\n",
      "Epoch 1497, Loss: 0.3462023288011551, Final Batch Loss: 0.12733976542949677\n",
      "Epoch 1498, Loss: 0.34759777784347534, Final Batch Loss: 0.15888968110084534\n",
      "Epoch 1499, Loss: 0.5195843875408173, Final Batch Loss: 0.3039412200450897\n",
      "Epoch 1500, Loss: 0.3853225111961365, Final Batch Loss: 0.16152799129486084\n",
      "Epoch 1501, Loss: 0.3834409862756729, Final Batch Loss: 0.2139398157596588\n",
      "Epoch 1502, Loss: 0.46078895032405853, Final Batch Loss: 0.27801769971847534\n",
      "Epoch 1503, Loss: 0.41690991818904877, Final Batch Loss: 0.2595166563987732\n",
      "Epoch 1504, Loss: 0.4846856892108917, Final Batch Loss: 0.21560651063919067\n",
      "Epoch 1505, Loss: 0.38719865679740906, Final Batch Loss: 0.20568829774856567\n",
      "Epoch 1506, Loss: 0.4851575046777725, Final Batch Loss: 0.23655925691127777\n",
      "Epoch 1507, Loss: 0.3570418357849121, Final Batch Loss: 0.20812392234802246\n",
      "Epoch 1508, Loss: 0.3552017956972122, Final Batch Loss: 0.14880020916461945\n",
      "Epoch 1509, Loss: 0.43106095492839813, Final Batch Loss: 0.21016038954257965\n",
      "Epoch 1510, Loss: 0.3139409199357033, Final Batch Loss: 0.11861466616392136\n",
      "Epoch 1511, Loss: 0.38148847222328186, Final Batch Loss: 0.16569043695926666\n",
      "Epoch 1512, Loss: 0.36621955037117004, Final Batch Loss: 0.18438100814819336\n",
      "Epoch 1513, Loss: 0.3467051237821579, Final Batch Loss: 0.1640787571668625\n",
      "Epoch 1514, Loss: 0.4220222234725952, Final Batch Loss: 0.18399079144001007\n",
      "Epoch 1515, Loss: 0.3318878561258316, Final Batch Loss: 0.16638578474521637\n",
      "Epoch 1516, Loss: 0.4313107281923294, Final Batch Loss: 0.22213687002658844\n",
      "Epoch 1517, Loss: 0.4914195239543915, Final Batch Loss: 0.23202574253082275\n",
      "Epoch 1518, Loss: 0.36730217933654785, Final Batch Loss: 0.17902381718158722\n",
      "Epoch 1519, Loss: 0.3481782525777817, Final Batch Loss: 0.16724509000778198\n",
      "Epoch 1520, Loss: 0.37160827219486237, Final Batch Loss: 0.19843491911888123\n",
      "Epoch 1521, Loss: 0.37182092666625977, Final Batch Loss: 0.18718016147613525\n",
      "Epoch 1522, Loss: 0.40500394999980927, Final Batch Loss: 0.21142585575580597\n",
      "Epoch 1523, Loss: 0.32435011863708496, Final Batch Loss: 0.13515812158584595\n",
      "Epoch 1524, Loss: 0.29847848415374756, Final Batch Loss: 0.1350651979446411\n",
      "Epoch 1525, Loss: 0.3114722669124603, Final Batch Loss: 0.1274147778749466\n",
      "Epoch 1526, Loss: 0.40700283646583557, Final Batch Loss: 0.20677317678928375\n",
      "Epoch 1527, Loss: 0.3927055895328522, Final Batch Loss: 0.2060057520866394\n",
      "Epoch 1528, Loss: 0.31748856604099274, Final Batch Loss: 0.15166856348514557\n",
      "Epoch 1529, Loss: 0.3449966460466385, Final Batch Loss: 0.12913115322589874\n",
      "Epoch 1530, Loss: 0.3451366722583771, Final Batch Loss: 0.14628610014915466\n",
      "Epoch 1531, Loss: 0.37170593440532684, Final Batch Loss: 0.1181783527135849\n",
      "Epoch 1532, Loss: 0.30101048946380615, Final Batch Loss: 0.1751786470413208\n",
      "Epoch 1533, Loss: 0.3089795708656311, Final Batch Loss: 0.17961889505386353\n",
      "Epoch 1534, Loss: 0.44427211582660675, Final Batch Loss: 0.2654050290584564\n",
      "Epoch 1535, Loss: 0.3108364939689636, Final Batch Loss: 0.16962246596813202\n",
      "Epoch 1536, Loss: 0.36146117746829987, Final Batch Loss: 0.15546678006649017\n",
      "Epoch 1537, Loss: 0.3664706349372864, Final Batch Loss: 0.22939079999923706\n",
      "Epoch 1538, Loss: 0.2981390804052353, Final Batch Loss: 0.1344423145055771\n",
      "Epoch 1539, Loss: 0.35673239827156067, Final Batch Loss: 0.17251548171043396\n",
      "Epoch 1540, Loss: 0.361411452293396, Final Batch Loss: 0.1283358782529831\n",
      "Epoch 1541, Loss: 0.3164767771959305, Final Batch Loss: 0.16193538904190063\n",
      "Epoch 1542, Loss: 0.3605576902627945, Final Batch Loss: 0.14004836976528168\n",
      "Epoch 1543, Loss: 0.4043674021959305, Final Batch Loss: 0.21058903634548187\n",
      "Epoch 1544, Loss: 0.4211513102054596, Final Batch Loss: 0.21769699454307556\n",
      "Epoch 1545, Loss: 0.3951215595006943, Final Batch Loss: 0.19281105697155\n",
      "Epoch 1546, Loss: 0.35857726633548737, Final Batch Loss: 0.1507098525762558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1547, Loss: 0.38727377355098724, Final Batch Loss: 0.16412334144115448\n",
      "Epoch 1548, Loss: 0.34856489300727844, Final Batch Loss: 0.15896578133106232\n",
      "Epoch 1549, Loss: 0.30784958600997925, Final Batch Loss: 0.15678493678569794\n",
      "Epoch 1550, Loss: 0.39857690036296844, Final Batch Loss: 0.2127307653427124\n",
      "Epoch 1551, Loss: 0.30251671373844147, Final Batch Loss: 0.15155567228794098\n",
      "Epoch 1552, Loss: 0.31484371423721313, Final Batch Loss: 0.1195366382598877\n",
      "Epoch 1553, Loss: 0.41330479085445404, Final Batch Loss: 0.17661842703819275\n",
      "Epoch 1554, Loss: 0.31398485600948334, Final Batch Loss: 0.15049982070922852\n",
      "Epoch 1555, Loss: 0.361625000834465, Final Batch Loss: 0.14040373265743256\n",
      "Epoch 1556, Loss: 0.4182482212781906, Final Batch Loss: 0.23343709111213684\n",
      "Epoch 1557, Loss: 0.4378220736980438, Final Batch Loss: 0.20341238379478455\n",
      "Epoch 1558, Loss: 0.36230525374412537, Final Batch Loss: 0.18892137706279755\n",
      "Epoch 1559, Loss: 0.44763602316379547, Final Batch Loss: 0.23167875409126282\n",
      "Epoch 1560, Loss: 0.40375156700611115, Final Batch Loss: 0.22619213163852692\n",
      "Epoch 1561, Loss: 0.4355945289134979, Final Batch Loss: 0.21776998043060303\n",
      "Epoch 1562, Loss: 0.39928658306598663, Final Batch Loss: 0.210008442401886\n",
      "Epoch 1563, Loss: 0.3277417719364166, Final Batch Loss: 0.1664811074733734\n",
      "Epoch 1564, Loss: 0.39402273297309875, Final Batch Loss: 0.1767832189798355\n",
      "Epoch 1565, Loss: 0.49276091158390045, Final Batch Loss: 0.3137374520301819\n",
      "Epoch 1566, Loss: 0.3835974931716919, Final Batch Loss: 0.1347496509552002\n",
      "Epoch 1567, Loss: 0.3932800590991974, Final Batch Loss: 0.21168529987335205\n",
      "Epoch 1568, Loss: 0.41674211621284485, Final Batch Loss: 0.23908059298992157\n",
      "Epoch 1569, Loss: 0.37817786633968353, Final Batch Loss: 0.20800131559371948\n",
      "Epoch 1570, Loss: 0.3671072721481323, Final Batch Loss: 0.21682791411876678\n",
      "Epoch 1571, Loss: 0.3403998613357544, Final Batch Loss: 0.18966051936149597\n",
      "Epoch 1572, Loss: 0.34525246918201447, Final Batch Loss: 0.15504732728004456\n",
      "Epoch 1573, Loss: 0.34987354278564453, Final Batch Loss: 0.13043637573719025\n",
      "Epoch 1574, Loss: 0.3270922005176544, Final Batch Loss: 0.13638935983181\n",
      "Epoch 1575, Loss: 0.346045583486557, Final Batch Loss: 0.1779363453388214\n",
      "Epoch 1576, Loss: 0.39978036284446716, Final Batch Loss: 0.17897754907608032\n",
      "Epoch 1577, Loss: 0.32804732024669647, Final Batch Loss: 0.13814204931259155\n",
      "Epoch 1578, Loss: 0.3809720277786255, Final Batch Loss: 0.1571262776851654\n",
      "Epoch 1579, Loss: 0.45488137006759644, Final Batch Loss: 0.2420983910560608\n",
      "Epoch 1580, Loss: 0.2801624611020088, Final Batch Loss: 0.1156894639134407\n",
      "Epoch 1581, Loss: 0.44571734964847565, Final Batch Loss: 0.1971631795167923\n",
      "Epoch 1582, Loss: 0.345712848007679, Final Batch Loss: 0.12222009152173996\n",
      "Epoch 1583, Loss: 0.3551136404275894, Final Batch Loss: 0.15597005188465118\n",
      "Epoch 1584, Loss: 0.3327796012163162, Final Batch Loss: 0.14831463992595673\n",
      "Epoch 1585, Loss: 0.37684203684329987, Final Batch Loss: 0.18747764825820923\n",
      "Epoch 1586, Loss: 0.4727964997291565, Final Batch Loss: 0.19566595554351807\n",
      "Epoch 1587, Loss: 0.48109494149684906, Final Batch Loss: 0.32675302028656006\n",
      "Epoch 1588, Loss: 0.5562240481376648, Final Batch Loss: 0.25161418318748474\n",
      "Epoch 1589, Loss: 0.3645050972700119, Final Batch Loss: 0.18359777331352234\n",
      "Epoch 1590, Loss: 0.3381318747997284, Final Batch Loss: 0.19453927874565125\n",
      "Epoch 1591, Loss: 0.3773360997438431, Final Batch Loss: 0.17545513808727264\n",
      "Epoch 1592, Loss: 0.37258298695087433, Final Batch Loss: 0.19648011028766632\n",
      "Epoch 1593, Loss: 0.39306385815143585, Final Batch Loss: 0.1801539957523346\n",
      "Epoch 1594, Loss: 0.42680589854717255, Final Batch Loss: 0.24719004333019257\n",
      "Epoch 1595, Loss: 0.352689191699028, Final Batch Loss: 0.18926425278186798\n",
      "Epoch 1596, Loss: 0.3531264066696167, Final Batch Loss: 0.12737303972244263\n",
      "Epoch 1597, Loss: 0.4268297702074051, Final Batch Loss: 0.22944965958595276\n",
      "Epoch 1598, Loss: 0.4364662170410156, Final Batch Loss: 0.2277764528989792\n",
      "Epoch 1599, Loss: 0.38986268639564514, Final Batch Loss: 0.23942385613918304\n",
      "Epoch 1600, Loss: 0.40886619687080383, Final Batch Loss: 0.22222988307476044\n",
      "Epoch 1601, Loss: 0.33811499178409576, Final Batch Loss: 0.15696227550506592\n",
      "Epoch 1602, Loss: 0.3837551325559616, Final Batch Loss: 0.2043280154466629\n",
      "Epoch 1603, Loss: 0.49345844984054565, Final Batch Loss: 0.2684507966041565\n",
      "Epoch 1604, Loss: 0.5894214808940887, Final Batch Loss: 0.31476467847824097\n",
      "Epoch 1605, Loss: 0.4128964841365814, Final Batch Loss: 0.23517589271068573\n",
      "Epoch 1606, Loss: 0.36139631271362305, Final Batch Loss: 0.19383370876312256\n",
      "Epoch 1607, Loss: 0.3767213821411133, Final Batch Loss: 0.20323199033737183\n",
      "Epoch 1608, Loss: 0.3846099227666855, Final Batch Loss: 0.18051818013191223\n",
      "Epoch 1609, Loss: 0.487222820520401, Final Batch Loss: 0.3104036748409271\n",
      "Epoch 1610, Loss: 0.4509843736886978, Final Batch Loss: 0.2626214027404785\n",
      "Epoch 1611, Loss: 0.37570054829120636, Final Batch Loss: 0.18648575246334076\n",
      "Epoch 1612, Loss: 0.42269764840602875, Final Batch Loss: 0.22121569514274597\n",
      "Epoch 1613, Loss: 0.48178093135356903, Final Batch Loss: 0.22873340547084808\n",
      "Epoch 1614, Loss: 0.3706926703453064, Final Batch Loss: 0.191249817609787\n",
      "Epoch 1615, Loss: 0.4291498214006424, Final Batch Loss: 0.23852311074733734\n",
      "Epoch 1616, Loss: 0.47557446360588074, Final Batch Loss: 0.24831247329711914\n",
      "Epoch 1617, Loss: 0.32823166251182556, Final Batch Loss: 0.1475847065448761\n",
      "Epoch 1618, Loss: 0.2919332683086395, Final Batch Loss: 0.1459653526544571\n",
      "Epoch 1619, Loss: 0.40692973136901855, Final Batch Loss: 0.2152400016784668\n",
      "Epoch 1620, Loss: 0.3573824018239975, Final Batch Loss: 0.1465073674917221\n",
      "Epoch 1621, Loss: 0.413558766245842, Final Batch Loss: 0.19689472019672394\n",
      "Epoch 1622, Loss: 0.41297197341918945, Final Batch Loss: 0.17227263748645782\n",
      "Epoch 1623, Loss: 0.4008680284023285, Final Batch Loss: 0.17403389513492584\n",
      "Epoch 1624, Loss: 0.3794182538986206, Final Batch Loss: 0.21114325523376465\n",
      "Epoch 1625, Loss: 0.44355306029319763, Final Batch Loss: 0.18432411551475525\n",
      "Epoch 1626, Loss: 0.3482619971036911, Final Batch Loss: 0.15641780197620392\n",
      "Epoch 1627, Loss: 0.4136669784784317, Final Batch Loss: 0.218865767121315\n",
      "Epoch 1628, Loss: 0.3451062738895416, Final Batch Loss: 0.1793387234210968\n",
      "Epoch 1629, Loss: 0.47079725563526154, Final Batch Loss: 0.2798040807247162\n",
      "Epoch 1630, Loss: 0.3649681955575943, Final Batch Loss: 0.2172558605670929\n",
      "Epoch 1631, Loss: 0.45631206035614014, Final Batch Loss: 0.2597883641719818\n",
      "Epoch 1632, Loss: 0.33263881504535675, Final Batch Loss: 0.17503701150417328\n",
      "Epoch 1633, Loss: 0.35574406385421753, Final Batch Loss: 0.16984422504901886\n",
      "Epoch 1634, Loss: 0.3705720752477646, Final Batch Loss: 0.2106531709432602\n",
      "Epoch 1635, Loss: 0.3758826106786728, Final Batch Loss: 0.21199442446231842\n",
      "Epoch 1636, Loss: 0.39519713819026947, Final Batch Loss: 0.17444241046905518\n",
      "Epoch 1637, Loss: 0.3702622801065445, Final Batch Loss: 0.2070326954126358\n",
      "Epoch 1638, Loss: 0.35381411015987396, Final Batch Loss: 0.1538137048482895\n",
      "Epoch 1639, Loss: 0.37454620003700256, Final Batch Loss: 0.17649796605110168\n",
      "Epoch 1640, Loss: 0.45157040655612946, Final Batch Loss: 0.18884943425655365\n",
      "Epoch 1641, Loss: 0.29286275804042816, Final Batch Loss: 0.17580384016036987\n",
      "Epoch 1642, Loss: 0.3532276451587677, Final Batch Loss: 0.2108093947172165\n",
      "Epoch 1643, Loss: 0.40374812483787537, Final Batch Loss: 0.1794692575931549\n",
      "Epoch 1644, Loss: 0.45326027274131775, Final Batch Loss: 0.21897956728935242\n",
      "Epoch 1645, Loss: 0.3540025055408478, Final Batch Loss: 0.16565674543380737\n",
      "Epoch 1646, Loss: 0.32450316846370697, Final Batch Loss: 0.1490064561367035\n",
      "Epoch 1647, Loss: 0.3772812634706497, Final Batch Loss: 0.2076767385005951\n",
      "Epoch 1648, Loss: 0.3867790400981903, Final Batch Loss: 0.18208438158035278\n",
      "Epoch 1649, Loss: 0.4098328649997711, Final Batch Loss: 0.17378629744052887\n",
      "Epoch 1650, Loss: 0.2889235094189644, Final Batch Loss: 0.09346728771924973\n",
      "Epoch 1651, Loss: 0.29805371165275574, Final Batch Loss: 0.1272631734609604\n",
      "Epoch 1652, Loss: 0.38978971540927887, Final Batch Loss: 0.22556649148464203\n",
      "Epoch 1653, Loss: 0.2962742894887924, Final Batch Loss: 0.14318004250526428\n",
      "Epoch 1654, Loss: 0.33352604508399963, Final Batch Loss: 0.14778311550617218\n",
      "Epoch 1655, Loss: 0.32662881910800934, Final Batch Loss: 0.1731385737657547\n",
      "Epoch 1656, Loss: 0.4921780377626419, Final Batch Loss: 0.20337001979351044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1657, Loss: 0.32796698808670044, Final Batch Loss: 0.15801750123500824\n",
      "Epoch 1658, Loss: 0.3368794918060303, Final Batch Loss: 0.12876853346824646\n",
      "Epoch 1659, Loss: 0.416145995259285, Final Batch Loss: 0.2136792093515396\n",
      "Epoch 1660, Loss: 0.37972690165042877, Final Batch Loss: 0.1746440976858139\n",
      "Epoch 1661, Loss: 0.43865828216075897, Final Batch Loss: 0.19071239233016968\n",
      "Epoch 1662, Loss: 0.37216581404209137, Final Batch Loss: 0.20027253031730652\n",
      "Epoch 1663, Loss: 0.333686038851738, Final Batch Loss: 0.14926832914352417\n",
      "Epoch 1664, Loss: 0.36231572926044464, Final Batch Loss: 0.18924883008003235\n",
      "Epoch 1665, Loss: 0.33005064725875854, Final Batch Loss: 0.16999290883541107\n",
      "Epoch 1666, Loss: 0.294035404920578, Final Batch Loss: 0.1279200166463852\n",
      "Epoch 1667, Loss: 0.4057803153991699, Final Batch Loss: 0.2360149770975113\n",
      "Epoch 1668, Loss: 0.4381922781467438, Final Batch Loss: 0.2045825570821762\n",
      "Epoch 1669, Loss: 0.4763486087322235, Final Batch Loss: 0.2162143886089325\n",
      "Epoch 1670, Loss: 0.3124037832021713, Final Batch Loss: 0.1256246417760849\n",
      "Epoch 1671, Loss: 0.388850599527359, Final Batch Loss: 0.19856193661689758\n",
      "Epoch 1672, Loss: 0.3614816963672638, Final Batch Loss: 0.1525244116783142\n",
      "Epoch 1673, Loss: 0.355184867978096, Final Batch Loss: 0.1523241251707077\n",
      "Epoch 1674, Loss: 0.26655130833387375, Final Batch Loss: 0.11505747586488724\n",
      "Epoch 1675, Loss: 0.28825700283050537, Final Batch Loss: 0.1597668081521988\n",
      "Epoch 1676, Loss: 0.3750302642583847, Final Batch Loss: 0.15972791612148285\n",
      "Epoch 1677, Loss: 0.3605354279279709, Final Batch Loss: 0.20140855014324188\n",
      "Epoch 1678, Loss: 0.32214419543743134, Final Batch Loss: 0.16845254600048065\n",
      "Epoch 1679, Loss: 0.3773738145828247, Final Batch Loss: 0.2022247016429901\n",
      "Epoch 1680, Loss: 0.3717443197965622, Final Batch Loss: 0.1895918846130371\n",
      "Epoch 1681, Loss: 0.28746411204338074, Final Batch Loss: 0.10141150653362274\n",
      "Epoch 1682, Loss: 0.3644096851348877, Final Batch Loss: 0.2293815314769745\n",
      "Epoch 1683, Loss: 0.3218277543783188, Final Batch Loss: 0.1235763281583786\n",
      "Epoch 1684, Loss: 0.4854220002889633, Final Batch Loss: 0.2921534776687622\n",
      "Epoch 1685, Loss: 0.32947681844234467, Final Batch Loss: 0.15834087133407593\n",
      "Epoch 1686, Loss: 0.37537574768066406, Final Batch Loss: 0.23571854829788208\n",
      "Epoch 1687, Loss: 0.38966603577136993, Final Batch Loss: 0.20341645181179047\n",
      "Epoch 1688, Loss: 0.4492451399564743, Final Batch Loss: 0.1908966451883316\n",
      "Epoch 1689, Loss: 0.46605025231838226, Final Batch Loss: 0.22885476052761078\n",
      "Epoch 1690, Loss: 0.2865542620420456, Final Batch Loss: 0.10299666225910187\n",
      "Epoch 1691, Loss: 0.4002448171377182, Final Batch Loss: 0.17633214592933655\n",
      "Epoch 1692, Loss: 0.3896214962005615, Final Batch Loss: 0.14880044758319855\n",
      "Epoch 1693, Loss: 0.3288850784301758, Final Batch Loss: 0.1836509108543396\n",
      "Epoch 1694, Loss: 0.39043259620666504, Final Batch Loss: 0.20350544154644012\n",
      "Epoch 1695, Loss: 0.34292255342006683, Final Batch Loss: 0.15296649932861328\n",
      "Epoch 1696, Loss: 0.3642958998680115, Final Batch Loss: 0.16574272513389587\n",
      "Epoch 1697, Loss: 0.34478959441185, Final Batch Loss: 0.15655869245529175\n",
      "Epoch 1698, Loss: 0.31276535987854004, Final Batch Loss: 0.16750384867191315\n",
      "Epoch 1699, Loss: 0.44073809683322906, Final Batch Loss: 0.2253386676311493\n",
      "Epoch 1700, Loss: 0.3776356875896454, Final Batch Loss: 0.18920837342739105\n",
      "Epoch 1701, Loss: 0.32859718799591064, Final Batch Loss: 0.1733769327402115\n",
      "Epoch 1702, Loss: 0.3533310294151306, Final Batch Loss: 0.14511454105377197\n",
      "Epoch 1703, Loss: 0.28586122393608093, Final Batch Loss: 0.10769349336624146\n",
      "Epoch 1704, Loss: 0.4094378650188446, Final Batch Loss: 0.2508557438850403\n",
      "Epoch 1705, Loss: 0.3455725908279419, Final Batch Loss: 0.16606241464614868\n",
      "Epoch 1706, Loss: 0.3456096947193146, Final Batch Loss: 0.18004491925239563\n",
      "Epoch 1707, Loss: 0.3281503915786743, Final Batch Loss: 0.1987035870552063\n",
      "Epoch 1708, Loss: 0.2745135799050331, Final Batch Loss: 0.11842506378889084\n",
      "Epoch 1709, Loss: 0.3379022777080536, Final Batch Loss: 0.1554984301328659\n",
      "Epoch 1710, Loss: 0.36216430366039276, Final Batch Loss: 0.1498507261276245\n",
      "Epoch 1711, Loss: 0.3586095720529556, Final Batch Loss: 0.18028387427330017\n",
      "Epoch 1712, Loss: 0.3669963926076889, Final Batch Loss: 0.16365690529346466\n",
      "Epoch 1713, Loss: 0.33355192840099335, Final Batch Loss: 0.1367226541042328\n",
      "Epoch 1714, Loss: 0.3211377412080765, Final Batch Loss: 0.15226878225803375\n",
      "Epoch 1715, Loss: 0.40861815214157104, Final Batch Loss: 0.20931564271450043\n",
      "Epoch 1716, Loss: 0.34275293350219727, Final Batch Loss: 0.13829949498176575\n",
      "Epoch 1717, Loss: 0.32767991721630096, Final Batch Loss: 0.1399759203195572\n",
      "Epoch 1718, Loss: 0.4061722159385681, Final Batch Loss: 0.23382461071014404\n",
      "Epoch 1719, Loss: 0.3429289162158966, Final Batch Loss: 0.18043634295463562\n",
      "Epoch 1720, Loss: 0.2993258088827133, Final Batch Loss: 0.14098332822322845\n",
      "Epoch 1721, Loss: 0.38261252641677856, Final Batch Loss: 0.1887856274843216\n",
      "Epoch 1722, Loss: 0.3426872491836548, Final Batch Loss: 0.1506107896566391\n",
      "Epoch 1723, Loss: 0.35461972653865814, Final Batch Loss: 0.18287768959999084\n",
      "Epoch 1724, Loss: 0.28079062700271606, Final Batch Loss: 0.14019642770290375\n",
      "Epoch 1725, Loss: 0.3525249809026718, Final Batch Loss: 0.16755151748657227\n",
      "Epoch 1726, Loss: 0.33006468415260315, Final Batch Loss: 0.1296606808900833\n",
      "Epoch 1727, Loss: 0.36451758444309235, Final Batch Loss: 0.15873324871063232\n",
      "Epoch 1728, Loss: 0.4189246594905853, Final Batch Loss: 0.2314492017030716\n",
      "Epoch 1729, Loss: 0.32175056636333466, Final Batch Loss: 0.17165309190750122\n",
      "Epoch 1730, Loss: 0.3279780149459839, Final Batch Loss: 0.1780659556388855\n",
      "Epoch 1731, Loss: 0.3587799519300461, Final Batch Loss: 0.20819823443889618\n",
      "Epoch 1732, Loss: 0.41221490502357483, Final Batch Loss: 0.2030145823955536\n",
      "Epoch 1733, Loss: 0.34054310619831085, Final Batch Loss: 0.14585991203784943\n",
      "Epoch 1734, Loss: 0.31318889558315277, Final Batch Loss: 0.16783089935779572\n",
      "Epoch 1735, Loss: 0.4055495858192444, Final Batch Loss: 0.16510458290576935\n",
      "Epoch 1736, Loss: 0.3121839910745621, Final Batch Loss: 0.18801249563694\n",
      "Epoch 1737, Loss: 0.33225496113300323, Final Batch Loss: 0.11909675598144531\n",
      "Epoch 1738, Loss: 0.3755699694156647, Final Batch Loss: 0.20573726296424866\n",
      "Epoch 1739, Loss: 0.46501879394054413, Final Batch Loss: 0.2403428852558136\n",
      "Epoch 1740, Loss: 0.3822626918554306, Final Batch Loss: 0.20657101273536682\n",
      "Epoch 1741, Loss: 0.35360829532146454, Final Batch Loss: 0.18762685358524323\n",
      "Epoch 1742, Loss: 0.3863024115562439, Final Batch Loss: 0.2260904461145401\n",
      "Epoch 1743, Loss: 0.3667452782392502, Final Batch Loss: 0.19598683714866638\n",
      "Epoch 1744, Loss: 0.317936435341835, Final Batch Loss: 0.16313299536705017\n",
      "Epoch 1745, Loss: 0.39298857748508453, Final Batch Loss: 0.18359804153442383\n",
      "Epoch 1746, Loss: 0.35716383159160614, Final Batch Loss: 0.19688257575035095\n",
      "Epoch 1747, Loss: 0.3275731950998306, Final Batch Loss: 0.19383399188518524\n",
      "Epoch 1748, Loss: 0.30988216400146484, Final Batch Loss: 0.13557173311710358\n",
      "Epoch 1749, Loss: 0.3173525556921959, Final Batch Loss: 0.19576118886470795\n",
      "Epoch 1750, Loss: 0.44717201590538025, Final Batch Loss: 0.19847124814987183\n",
      "Epoch 1751, Loss: 0.3117668330669403, Final Batch Loss: 0.16498610377311707\n",
      "Epoch 1752, Loss: 0.36638860404491425, Final Batch Loss: 0.19257883727550507\n",
      "Epoch 1753, Loss: 0.352517306804657, Final Batch Loss: 0.12690405547618866\n",
      "Epoch 1754, Loss: 0.42898809909820557, Final Batch Loss: 0.23472899198532104\n",
      "Epoch 1755, Loss: 0.4818506985902786, Final Batch Loss: 0.2660476863384247\n",
      "Epoch 1756, Loss: 0.4666000157594681, Final Batch Loss: 0.19958676397800446\n",
      "Epoch 1757, Loss: 0.3068760335445404, Final Batch Loss: 0.16600368916988373\n",
      "Epoch 1758, Loss: 0.35178643465042114, Final Batch Loss: 0.1255449503660202\n",
      "Epoch 1759, Loss: 0.37750400602817535, Final Batch Loss: 0.23371288180351257\n",
      "Epoch 1760, Loss: 0.33244892954826355, Final Batch Loss: 0.16681882739067078\n",
      "Epoch 1761, Loss: 0.3418385535478592, Final Batch Loss: 0.17208413779735565\n",
      "Epoch 1762, Loss: 0.40076136589050293, Final Batch Loss: 0.24474641680717468\n",
      "Epoch 1763, Loss: 0.2729019373655319, Final Batch Loss: 0.13759149610996246\n",
      "Epoch 1764, Loss: 0.3089904487133026, Final Batch Loss: 0.16274656355381012\n",
      "Epoch 1765, Loss: 0.4025667607784271, Final Batch Loss: 0.2533937990665436\n",
      "Epoch 1766, Loss: 0.36202895641326904, Final Batch Loss: 0.22265079617500305\n",
      "Epoch 1767, Loss: 0.3395906537771225, Final Batch Loss: 0.15226586163043976\n",
      "Epoch 1768, Loss: 0.331791989505291, Final Batch Loss: 0.10379409044981003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1769, Loss: 0.30954307317733765, Final Batch Loss: 0.1751949042081833\n",
      "Epoch 1770, Loss: 0.3565268814563751, Final Batch Loss: 0.19011029601097107\n",
      "Epoch 1771, Loss: 0.3821772634983063, Final Batch Loss: 0.17843443155288696\n",
      "Epoch 1772, Loss: 0.3309261128306389, Final Batch Loss: 0.11225179582834244\n",
      "Epoch 1773, Loss: 0.3126584440469742, Final Batch Loss: 0.1411561816930771\n",
      "Epoch 1774, Loss: 0.3529586046934128, Final Batch Loss: 0.165995791554451\n",
      "Epoch 1775, Loss: 0.3506581038236618, Final Batch Loss: 0.19820338487625122\n",
      "Epoch 1776, Loss: 0.35354311764240265, Final Batch Loss: 0.16005392372608185\n",
      "Epoch 1777, Loss: 0.31365419924259186, Final Batch Loss: 0.1515244096517563\n",
      "Epoch 1778, Loss: 0.33462102711200714, Final Batch Loss: 0.13447648286819458\n",
      "Epoch 1779, Loss: 0.3217228353023529, Final Batch Loss: 0.18756772577762604\n",
      "Epoch 1780, Loss: 0.346930593252182, Final Batch Loss: 0.20113398134708405\n",
      "Epoch 1781, Loss: 0.341028094291687, Final Batch Loss: 0.15852853655815125\n",
      "Epoch 1782, Loss: 0.2608373910188675, Final Batch Loss: 0.15853369235992432\n",
      "Epoch 1783, Loss: 0.3674529790878296, Final Batch Loss: 0.21908245980739594\n",
      "Epoch 1784, Loss: 0.36356812715530396, Final Batch Loss: 0.22100409865379333\n",
      "Epoch 1785, Loss: 0.3362962007522583, Final Batch Loss: 0.1519433557987213\n",
      "Epoch 1786, Loss: 0.3233189135789871, Final Batch Loss: 0.14582978188991547\n",
      "Epoch 1787, Loss: 0.3486969918012619, Final Batch Loss: 0.2024400383234024\n",
      "Epoch 1788, Loss: 0.30193911492824554, Final Batch Loss: 0.15422050654888153\n",
      "Epoch 1789, Loss: 0.24062302708625793, Final Batch Loss: 0.10765644907951355\n",
      "Epoch 1790, Loss: 0.2380489557981491, Final Batch Loss: 0.11365213245153427\n",
      "Epoch 1791, Loss: 0.4433979392051697, Final Batch Loss: 0.2958405315876007\n",
      "Epoch 1792, Loss: 0.41620831191539764, Final Batch Loss: 0.22152076661586761\n",
      "Epoch 1793, Loss: 0.27462373673915863, Final Batch Loss: 0.11875900626182556\n",
      "Epoch 1794, Loss: 0.33041632175445557, Final Batch Loss: 0.15443705022335052\n",
      "Epoch 1795, Loss: 0.26859916746616364, Final Batch Loss: 0.11373303830623627\n",
      "Epoch 1796, Loss: 0.2937098369002342, Final Batch Loss: 0.1081337109208107\n",
      "Epoch 1797, Loss: 0.2850658744573593, Final Batch Loss: 0.15176115930080414\n",
      "Epoch 1798, Loss: 0.4392714127898216, Final Batch Loss: 0.3165830671787262\n",
      "Epoch 1799, Loss: 0.3496088981628418, Final Batch Loss: 0.16501912474632263\n",
      "Epoch 1800, Loss: 0.35758769512176514, Final Batch Loss: 0.20747166872024536\n",
      "Epoch 1801, Loss: 0.3513290733098984, Final Batch Loss: 0.17470672726631165\n",
      "Epoch 1802, Loss: 0.3887694329023361, Final Batch Loss: 0.2577624022960663\n",
      "Epoch 1803, Loss: 0.4064626842737198, Final Batch Loss: 0.23616811633110046\n",
      "Epoch 1804, Loss: 0.31816260516643524, Final Batch Loss: 0.1518811285495758\n",
      "Epoch 1805, Loss: 0.34624433517456055, Final Batch Loss: 0.16172106564044952\n",
      "Epoch 1806, Loss: 0.23920681327581406, Final Batch Loss: 0.07636178284883499\n",
      "Epoch 1807, Loss: 0.29523056745529175, Final Batch Loss: 0.16700442135334015\n",
      "Epoch 1808, Loss: 0.271665021777153, Final Batch Loss: 0.1116982102394104\n",
      "Epoch 1809, Loss: 0.363595649600029, Final Batch Loss: 0.21616898477077484\n",
      "Epoch 1810, Loss: 0.34427040815353394, Final Batch Loss: 0.18954376876354218\n",
      "Epoch 1811, Loss: 0.37171582877635956, Final Batch Loss: 0.11119009554386139\n",
      "Epoch 1812, Loss: 0.33598488569259644, Final Batch Loss: 0.14374558627605438\n",
      "Epoch 1813, Loss: 0.32676661014556885, Final Batch Loss: 0.17073999345302582\n",
      "Epoch 1814, Loss: 0.41098836064338684, Final Batch Loss: 0.21597816050052643\n",
      "Epoch 1815, Loss: 0.25436411798000336, Final Batch Loss: 0.10742682218551636\n",
      "Epoch 1816, Loss: 0.39856721460819244, Final Batch Loss: 0.20759810507297516\n",
      "Epoch 1817, Loss: 0.24192196130752563, Final Batch Loss: 0.1015188992023468\n",
      "Epoch 1818, Loss: 0.26401761174201965, Final Batch Loss: 0.12136226892471313\n",
      "Epoch 1819, Loss: 0.3569837212562561, Final Batch Loss: 0.2046077996492386\n",
      "Epoch 1820, Loss: 0.37186530232429504, Final Batch Loss: 0.21525579690933228\n",
      "Epoch 1821, Loss: 0.3434174507856369, Final Batch Loss: 0.2017020434141159\n",
      "Epoch 1822, Loss: 0.3115335553884506, Final Batch Loss: 0.14663343131542206\n",
      "Epoch 1823, Loss: 0.3300379067659378, Final Batch Loss: 0.15784001350402832\n",
      "Epoch 1824, Loss: 0.22410427778959274, Final Batch Loss: 0.09795617312192917\n",
      "Epoch 1825, Loss: 0.3235195502638817, Final Batch Loss: 0.2010279893875122\n",
      "Epoch 1826, Loss: 0.28836335241794586, Final Batch Loss: 0.14308853447437286\n",
      "Epoch 1827, Loss: 0.34775516390800476, Final Batch Loss: 0.15217013657093048\n",
      "Epoch 1828, Loss: 0.33047275245189667, Final Batch Loss: 0.1439647674560547\n",
      "Epoch 1829, Loss: 0.4138585925102234, Final Batch Loss: 0.20472244918346405\n",
      "Epoch 1830, Loss: 0.29281412065029144, Final Batch Loss: 0.15967011451721191\n",
      "Epoch 1831, Loss: 0.3298717811703682, Final Batch Loss: 0.12374558299779892\n",
      "Epoch 1832, Loss: 0.334103986620903, Final Batch Loss: 0.1885751485824585\n",
      "Epoch 1833, Loss: 0.3836831599473953, Final Batch Loss: 0.20066428184509277\n",
      "Epoch 1834, Loss: 0.3950091153383255, Final Batch Loss: 0.21385736763477325\n",
      "Epoch 1835, Loss: 0.2998265326023102, Final Batch Loss: 0.1574936807155609\n",
      "Epoch 1836, Loss: 0.3564863055944443, Final Batch Loss: 0.24209968745708466\n",
      "Epoch 1837, Loss: 0.37307218462228775, Final Batch Loss: 0.1204524114727974\n",
      "Epoch 1838, Loss: 0.25149957835674286, Final Batch Loss: 0.11703228950500488\n",
      "Epoch 1839, Loss: 0.35432641208171844, Final Batch Loss: 0.1583395153284073\n",
      "Epoch 1840, Loss: 0.4710511714220047, Final Batch Loss: 0.2426203340291977\n",
      "Epoch 1841, Loss: 0.2847653478384018, Final Batch Loss: 0.14980190992355347\n",
      "Epoch 1842, Loss: 0.26479800045490265, Final Batch Loss: 0.17206884920597076\n",
      "Epoch 1843, Loss: 0.3420698642730713, Final Batch Loss: 0.20530328154563904\n",
      "Epoch 1844, Loss: 0.29804518818855286, Final Batch Loss: 0.16292430460453033\n",
      "Epoch 1845, Loss: 0.3339790552854538, Final Batch Loss: 0.17215707898139954\n",
      "Epoch 1846, Loss: 0.3642786145210266, Final Batch Loss: 0.15214022994041443\n",
      "Epoch 1847, Loss: 0.31457409262657166, Final Batch Loss: 0.16751106083393097\n",
      "Epoch 1848, Loss: 0.2683025598526001, Final Batch Loss: 0.13428530097007751\n",
      "Epoch 1849, Loss: 0.34146256744861603, Final Batch Loss: 0.20412732660770416\n",
      "Epoch 1850, Loss: 0.28238557279109955, Final Batch Loss: 0.12139521539211273\n",
      "Epoch 1851, Loss: 0.29673582315444946, Final Batch Loss: 0.11183656752109528\n",
      "Epoch 1852, Loss: 0.2934713661670685, Final Batch Loss: 0.14948052167892456\n",
      "Epoch 1853, Loss: 0.4299905300140381, Final Batch Loss: 0.23332957923412323\n",
      "Epoch 1854, Loss: 0.2560132369399071, Final Batch Loss: 0.1119038388133049\n",
      "Epoch 1855, Loss: 0.3434561938047409, Final Batch Loss: 0.17341119050979614\n",
      "Epoch 1856, Loss: 0.3484485596418381, Final Batch Loss: 0.15867476165294647\n",
      "Epoch 1857, Loss: 0.23824699223041534, Final Batch Loss: 0.11287333071231842\n",
      "Epoch 1858, Loss: 0.23112982511520386, Final Batch Loss: 0.09379284083843231\n",
      "Epoch 1859, Loss: 0.33897584676742554, Final Batch Loss: 0.17111554741859436\n",
      "Epoch 1860, Loss: 0.30816926062107086, Final Batch Loss: 0.17610351741313934\n",
      "Epoch 1861, Loss: 0.2995912879705429, Final Batch Loss: 0.09812821447849274\n",
      "Epoch 1862, Loss: 0.25903816521167755, Final Batch Loss: 0.12614670395851135\n",
      "Epoch 1863, Loss: 0.31155379116535187, Final Batch Loss: 0.1424233764410019\n",
      "Epoch 1864, Loss: 0.34404879808425903, Final Batch Loss: 0.1578405350446701\n",
      "Epoch 1865, Loss: 0.2760712653398514, Final Batch Loss: 0.1265101432800293\n",
      "Epoch 1866, Loss: 0.3116045445203781, Final Batch Loss: 0.16547882556915283\n",
      "Epoch 1867, Loss: 0.5046044141054153, Final Batch Loss: 0.32802703976631165\n",
      "Epoch 1868, Loss: 0.23621928691864014, Final Batch Loss: 0.10553406178951263\n",
      "Epoch 1869, Loss: 0.33514487743377686, Final Batch Loss: 0.13047276437282562\n",
      "Epoch 1870, Loss: 0.3432846665382385, Final Batch Loss: 0.181671142578125\n",
      "Epoch 1871, Loss: 0.2640402913093567, Final Batch Loss: 0.11947780847549438\n",
      "Epoch 1872, Loss: 0.2898828834295273, Final Batch Loss: 0.12748025357723236\n",
      "Epoch 1873, Loss: 0.36747510731220245, Final Batch Loss: 0.17666494846343994\n",
      "Epoch 1874, Loss: 0.3260505795478821, Final Batch Loss: 0.145761638879776\n",
      "Epoch 1875, Loss: 0.3444095253944397, Final Batch Loss: 0.19698123633861542\n",
      "Epoch 1876, Loss: 0.2742721140384674, Final Batch Loss: 0.13173604011535645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1877, Loss: 0.2822287455201149, Final Batch Loss: 0.10708395391702652\n",
      "Epoch 1878, Loss: 0.2967662215232849, Final Batch Loss: 0.11133122444152832\n",
      "Epoch 1879, Loss: 0.26238561421632767, Final Batch Loss: 0.10639359802007675\n",
      "Epoch 1880, Loss: 0.2642236649990082, Final Batch Loss: 0.13685418665409088\n",
      "Epoch 1881, Loss: 0.3108678460121155, Final Batch Loss: 0.1341882050037384\n",
      "Epoch 1882, Loss: 0.30226975679397583, Final Batch Loss: 0.1423501819372177\n",
      "Epoch 1883, Loss: 0.1865571364760399, Final Batch Loss: 0.08566448092460632\n",
      "Epoch 1884, Loss: 0.24606864154338837, Final Batch Loss: 0.12388445436954498\n",
      "Epoch 1885, Loss: 0.25928060710430145, Final Batch Loss: 0.12710976600646973\n",
      "Epoch 1886, Loss: 0.2079319730401039, Final Batch Loss: 0.07442685216665268\n",
      "Epoch 1887, Loss: 0.2985943704843521, Final Batch Loss: 0.12586119771003723\n",
      "Epoch 1888, Loss: 0.2567407563328743, Final Batch Loss: 0.0997028723359108\n",
      "Epoch 1889, Loss: 0.2943733185529709, Final Batch Loss: 0.15506874024868011\n",
      "Epoch 1890, Loss: 0.3183528482913971, Final Batch Loss: 0.16911157965660095\n",
      "Epoch 1891, Loss: 0.2881470024585724, Final Batch Loss: 0.16387039422988892\n",
      "Epoch 1892, Loss: 0.3393213003873825, Final Batch Loss: 0.188221737742424\n",
      "Epoch 1893, Loss: 0.55123271048069, Final Batch Loss: 0.40477511286735535\n",
      "Epoch 1894, Loss: 0.3047540932893753, Final Batch Loss: 0.17622677981853485\n",
      "Epoch 1895, Loss: 0.29418472945690155, Final Batch Loss: 0.1437777578830719\n",
      "Epoch 1896, Loss: 0.3696301579475403, Final Batch Loss: 0.1855054646730423\n",
      "Epoch 1897, Loss: 0.4608577936887741, Final Batch Loss: 0.27881789207458496\n",
      "Epoch 1898, Loss: 0.36473287642002106, Final Batch Loss: 0.16368162631988525\n",
      "Epoch 1899, Loss: 0.21669957041740417, Final Batch Loss: 0.11955834925174713\n",
      "Epoch 1900, Loss: 0.22701279819011688, Final Batch Loss: 0.11377763003110886\n",
      "Epoch 1901, Loss: 0.24766158312559128, Final Batch Loss: 0.12752589583396912\n",
      "Epoch 1902, Loss: 0.29993072152137756, Final Batch Loss: 0.16027069091796875\n",
      "Epoch 1903, Loss: 0.3750239759683609, Final Batch Loss: 0.1554524302482605\n",
      "Epoch 1904, Loss: 0.327826589345932, Final Batch Loss: 0.16578884422779083\n",
      "Epoch 1905, Loss: 0.2478906735777855, Final Batch Loss: 0.13425832986831665\n",
      "Epoch 1906, Loss: 0.3096272051334381, Final Batch Loss: 0.1710951030254364\n",
      "Epoch 1907, Loss: 0.33851517736911774, Final Batch Loss: 0.1530236005783081\n",
      "Epoch 1908, Loss: 0.2852894589304924, Final Batch Loss: 0.10744217783212662\n",
      "Epoch 1909, Loss: 0.4013482332229614, Final Batch Loss: 0.16123290359973907\n",
      "Epoch 1910, Loss: 0.282330758869648, Final Batch Loss: 0.11504972726106644\n",
      "Epoch 1911, Loss: 0.2983102947473526, Final Batch Loss: 0.13770848512649536\n",
      "Epoch 1912, Loss: 0.3976995199918747, Final Batch Loss: 0.20698167383670807\n",
      "Epoch 1913, Loss: 0.2875102236866951, Final Batch Loss: 0.17606444656848907\n",
      "Epoch 1914, Loss: 0.25392844527959824, Final Batch Loss: 0.15575312077999115\n",
      "Epoch 1915, Loss: 0.26391272246837616, Final Batch Loss: 0.12409785389900208\n",
      "Epoch 1916, Loss: 0.26964061707258224, Final Batch Loss: 0.10017438977956772\n",
      "Epoch 1917, Loss: 0.27001797407865524, Final Batch Loss: 0.070950947701931\n",
      "Epoch 1918, Loss: 0.2599095180630684, Final Batch Loss: 0.07681553810834885\n",
      "Epoch 1919, Loss: 0.37491732835769653, Final Batch Loss: 0.17517535388469696\n",
      "Epoch 1920, Loss: 0.29747719317674637, Final Batch Loss: 0.18305279314517975\n",
      "Epoch 1921, Loss: 0.293891578912735, Final Batch Loss: 0.11976242065429688\n",
      "Epoch 1922, Loss: 0.26920391619205475, Final Batch Loss: 0.10314030945301056\n",
      "Epoch 1923, Loss: 0.24583829194307327, Final Batch Loss: 0.10325170308351517\n",
      "Epoch 1924, Loss: 0.3123433291912079, Final Batch Loss: 0.1789504587650299\n",
      "Epoch 1925, Loss: 0.2807718962430954, Final Batch Loss: 0.13648517429828644\n",
      "Epoch 1926, Loss: 0.28024160861968994, Final Batch Loss: 0.13424116373062134\n",
      "Epoch 1927, Loss: 0.316603347659111, Final Batch Loss: 0.16551026701927185\n",
      "Epoch 1928, Loss: 0.255307674407959, Final Batch Loss: 0.10079509019851685\n",
      "Epoch 1929, Loss: 0.35954077541828156, Final Batch Loss: 0.22224141657352448\n",
      "Epoch 1930, Loss: 0.2396043911576271, Final Batch Loss: 0.14637491106987\n",
      "Epoch 1931, Loss: 0.25225497782230377, Final Batch Loss: 0.14584098756313324\n",
      "Epoch 1932, Loss: 0.2553766518831253, Final Batch Loss: 0.10495585203170776\n",
      "Epoch 1933, Loss: 0.27031299471855164, Final Batch Loss: 0.1726008504629135\n",
      "Epoch 1934, Loss: 0.3387945592403412, Final Batch Loss: 0.192289799451828\n",
      "Epoch 1935, Loss: 0.4035996198654175, Final Batch Loss: 0.23655273020267487\n",
      "Epoch 1936, Loss: 0.3177556097507477, Final Batch Loss: 0.18487273156642914\n",
      "Epoch 1937, Loss: 0.2618071362376213, Final Batch Loss: 0.1157984659075737\n",
      "Epoch 1938, Loss: 0.22498244047164917, Final Batch Loss: 0.08045095205307007\n",
      "Epoch 1939, Loss: 0.32497674226760864, Final Batch Loss: 0.15499472618103027\n",
      "Epoch 1940, Loss: 0.24263489991426468, Final Batch Loss: 0.14787067472934723\n",
      "Epoch 1941, Loss: 0.2709996998310089, Final Batch Loss: 0.1173371970653534\n",
      "Epoch 1942, Loss: 0.35466232895851135, Final Batch Loss: 0.20025178790092468\n",
      "Epoch 1943, Loss: 0.27305592596530914, Final Batch Loss: 0.17094291746616364\n",
      "Epoch 1944, Loss: 0.24427375197410583, Final Batch Loss: 0.0978095531463623\n",
      "Epoch 1945, Loss: 0.36418719589710236, Final Batch Loss: 0.20022334158420563\n",
      "Epoch 1946, Loss: 0.2520978972315788, Final Batch Loss: 0.08331901580095291\n",
      "Epoch 1947, Loss: 0.30102962255477905, Final Batch Loss: 0.14607395231723785\n",
      "Epoch 1948, Loss: 0.31655430793762207, Final Batch Loss: 0.19182364642620087\n",
      "Epoch 1949, Loss: 0.334163174033165, Final Batch Loss: 0.19317665696144104\n",
      "Epoch 1950, Loss: 0.2682691961526871, Final Batch Loss: 0.13525423407554626\n",
      "Epoch 1951, Loss: 0.24684826284646988, Final Batch Loss: 0.12112788110971451\n",
      "Epoch 1952, Loss: 0.30136042833328247, Final Batch Loss: 0.1605321764945984\n",
      "Epoch 1953, Loss: 0.2658485025167465, Final Batch Loss: 0.13167569041252136\n",
      "Epoch 1954, Loss: 0.3972925841808319, Final Batch Loss: 0.2161051630973816\n",
      "Epoch 1955, Loss: 0.345173180103302, Final Batch Loss: 0.15292717516422272\n",
      "Epoch 1956, Loss: 0.3022353798151016, Final Batch Loss: 0.19324631989002228\n",
      "Epoch 1957, Loss: 0.2410109043121338, Final Batch Loss: 0.12757398188114166\n",
      "Epoch 1958, Loss: 0.20375321805477142, Final Batch Loss: 0.08852671831846237\n",
      "Epoch 1959, Loss: 0.28434766829013824, Final Batch Loss: 0.15413863956928253\n",
      "Epoch 1960, Loss: 0.2720665782690048, Final Batch Loss: 0.1301829218864441\n",
      "Epoch 1961, Loss: 0.27877090871334076, Final Batch Loss: 0.1240866482257843\n",
      "Epoch 1962, Loss: 0.2397729977965355, Final Batch Loss: 0.09364160150289536\n",
      "Epoch 1963, Loss: 0.3325042426586151, Final Batch Loss: 0.18113529682159424\n",
      "Epoch 1964, Loss: 0.32944440841674805, Final Batch Loss: 0.1685255765914917\n",
      "Epoch 1965, Loss: 0.3288886398077011, Final Batch Loss: 0.21819022297859192\n",
      "Epoch 1966, Loss: 0.28916484862565994, Final Batch Loss: 0.10603874176740646\n",
      "Epoch 1967, Loss: 0.4229109585285187, Final Batch Loss: 0.30525192618370056\n",
      "Epoch 1968, Loss: 0.23196914047002792, Final Batch Loss: 0.15078656375408173\n",
      "Epoch 1969, Loss: 0.3920471668243408, Final Batch Loss: 0.21102280914783478\n",
      "Epoch 1970, Loss: 0.26917803287506104, Final Batch Loss: 0.15873311460018158\n",
      "Epoch 1971, Loss: 0.3172904998064041, Final Batch Loss: 0.07184262573719025\n",
      "Epoch 1972, Loss: 0.2770792171359062, Final Batch Loss: 0.1958891600370407\n",
      "Epoch 1973, Loss: 0.3185545802116394, Final Batch Loss: 0.17748303711414337\n",
      "Epoch 1974, Loss: 0.27647827565670013, Final Batch Loss: 0.1354549080133438\n",
      "Epoch 1975, Loss: 0.36164093017578125, Final Batch Loss: 0.17808525264263153\n",
      "Epoch 1976, Loss: 0.2737518846988678, Final Batch Loss: 0.14010658860206604\n",
      "Epoch 1977, Loss: 0.28341564536094666, Final Batch Loss: 0.12567730247974396\n",
      "Epoch 1978, Loss: 0.25504419952630997, Final Batch Loss: 0.13252319395542145\n",
      "Epoch 1979, Loss: 0.2959425002336502, Final Batch Loss: 0.14782851934432983\n",
      "Epoch 1980, Loss: 0.2630753442645073, Final Batch Loss: 0.1597273051738739\n",
      "Epoch 1981, Loss: 0.355851411819458, Final Batch Loss: 0.15415939688682556\n",
      "Epoch 1982, Loss: 0.320468932390213, Final Batch Loss: 0.19033002853393555\n",
      "Epoch 1983, Loss: 0.30281341075897217, Final Batch Loss: 0.11798393726348877\n",
      "Epoch 1984, Loss: 0.28870992362499237, Final Batch Loss: 0.1341947317123413\n",
      "Epoch 1985, Loss: 0.2494650036096573, Final Batch Loss: 0.1006898581981659\n",
      "Epoch 1986, Loss: 0.25576193630695343, Final Batch Loss: 0.14433076977729797\n",
      "Epoch 1987, Loss: 0.18478307873010635, Final Batch Loss: 0.09350001811981201\n",
      "Epoch 1988, Loss: 0.3496202826499939, Final Batch Loss: 0.21126370131969452\n",
      "Epoch 1989, Loss: 0.18903424590826035, Final Batch Loss: 0.06437567621469498\n",
      "Epoch 1990, Loss: 0.3395605832338333, Final Batch Loss: 0.2092965692281723\n",
      "Epoch 1991, Loss: 0.304172158241272, Final Batch Loss: 0.16229422390460968\n",
      "Epoch 1992, Loss: 0.2257298082113266, Final Batch Loss: 0.1062680333852768\n",
      "Epoch 1993, Loss: 0.2672162428498268, Final Batch Loss: 0.1617325097322464\n",
      "Epoch 1994, Loss: 0.27882125973701477, Final Batch Loss: 0.13447962701320648\n",
      "Epoch 1995, Loss: 0.19212979078292847, Final Batch Loss: 0.07078766077756882\n",
      "Epoch 1996, Loss: 0.32942699640989304, Final Batch Loss: 0.11198476701974869\n",
      "Epoch 1997, Loss: 0.2742534950375557, Final Batch Loss: 0.1661664992570877\n",
      "Epoch 1998, Loss: 0.3379411995410919, Final Batch Loss: 0.19044280052185059\n",
      "Epoch 1999, Loss: 0.3461974263191223, Final Batch Loss: 0.18154022097587585\n",
      "Epoch 2000, Loss: 0.27224138379096985, Final Batch Loss: 0.12684234976768494\n",
      "Epoch 2001, Loss: 0.4113379716873169, Final Batch Loss: 0.28164464235305786\n",
      "Epoch 2002, Loss: 0.2849071994423866, Final Batch Loss: 0.16631610691547394\n",
      "Epoch 2003, Loss: 0.26510000228881836, Final Batch Loss: 0.14409507811069489\n",
      "Epoch 2004, Loss: 0.25262944400310516, Final Batch Loss: 0.1181139349937439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2005, Loss: 0.2954789996147156, Final Batch Loss: 0.14204753935337067\n",
      "Epoch 2006, Loss: 0.3398090749979019, Final Batch Loss: 0.1750602275133133\n",
      "Epoch 2007, Loss: 0.29763199388980865, Final Batch Loss: 0.16678482294082642\n",
      "Epoch 2008, Loss: 0.21465299278497696, Final Batch Loss: 0.10181542485952377\n",
      "Epoch 2009, Loss: 0.23725653439760208, Final Batch Loss: 0.11366410553455353\n",
      "Epoch 2010, Loss: 0.26817139238119125, Final Batch Loss: 0.10320132225751877\n",
      "Epoch 2011, Loss: 0.2219475507736206, Final Batch Loss: 0.09848455339670181\n",
      "Epoch 2012, Loss: 0.2784697785973549, Final Batch Loss: 0.10310748964548111\n",
      "Epoch 2013, Loss: 0.3248290941119194, Final Batch Loss: 0.2024683803319931\n",
      "Epoch 2014, Loss: 0.21514666825532913, Final Batch Loss: 0.09093888849020004\n",
      "Epoch 2015, Loss: 0.25813450664281845, Final Batch Loss: 0.10208650678396225\n",
      "Epoch 2016, Loss: 0.20326239615678787, Final Batch Loss: 0.09215021133422852\n",
      "Epoch 2017, Loss: 0.34547413885593414, Final Batch Loss: 0.17157204449176788\n",
      "Epoch 2018, Loss: 0.269580140709877, Final Batch Loss: 0.13636821508407593\n",
      "Epoch 2019, Loss: 0.20850005745887756, Final Batch Loss: 0.06934714317321777\n",
      "Epoch 2020, Loss: 0.2159276306629181, Final Batch Loss: 0.10115399956703186\n",
      "Epoch 2021, Loss: 0.2800201401114464, Final Batch Loss: 0.11900310963392258\n",
      "Epoch 2022, Loss: 0.3022790849208832, Final Batch Loss: 0.14942225813865662\n",
      "Epoch 2023, Loss: 0.3229643702507019, Final Batch Loss: 0.1823071390390396\n",
      "Epoch 2024, Loss: 0.24629241973161697, Final Batch Loss: 0.08461692184209824\n",
      "Epoch 2025, Loss: 0.2837473154067993, Final Batch Loss: 0.1690780073404312\n",
      "Epoch 2026, Loss: 0.19249920547008514, Final Batch Loss: 0.08198638260364532\n",
      "Epoch 2027, Loss: 0.23538165539503098, Final Batch Loss: 0.13517044484615326\n",
      "Epoch 2028, Loss: 0.3239213675260544, Final Batch Loss: 0.18977023661136627\n",
      "Epoch 2029, Loss: 0.25473179668188095, Final Batch Loss: 0.13466152548789978\n",
      "Epoch 2030, Loss: 0.2647552564740181, Final Batch Loss: 0.17142410576343536\n",
      "Epoch 2031, Loss: 0.29595932364463806, Final Batch Loss: 0.16175447404384613\n",
      "Epoch 2032, Loss: 0.3192354068160057, Final Batch Loss: 0.11462072283029556\n",
      "Epoch 2033, Loss: 0.3098173290491104, Final Batch Loss: 0.16876272857189178\n",
      "Epoch 2034, Loss: 0.2627314105629921, Final Batch Loss: 0.1433221995830536\n",
      "Epoch 2035, Loss: 0.3271830528974533, Final Batch Loss: 0.18577058613300323\n",
      "Epoch 2036, Loss: 0.2896984964609146, Final Batch Loss: 0.13462387025356293\n",
      "Epoch 2037, Loss: 0.3375373035669327, Final Batch Loss: 0.17006294429302216\n",
      "Epoch 2038, Loss: 0.3331403136253357, Final Batch Loss: 0.18631498515605927\n",
      "Epoch 2039, Loss: 0.23763582110404968, Final Batch Loss: 0.1013052761554718\n",
      "Epoch 2040, Loss: 0.37264563143253326, Final Batch Loss: 0.1992851346731186\n",
      "Epoch 2041, Loss: 0.2797071412205696, Final Batch Loss: 0.1636333465576172\n",
      "Epoch 2042, Loss: 0.3086674213409424, Final Batch Loss: 0.16330689191818237\n",
      "Epoch 2043, Loss: 0.29971110820770264, Final Batch Loss: 0.10327881574630737\n",
      "Epoch 2044, Loss: 0.27882954478263855, Final Batch Loss: 0.14741714298725128\n",
      "Epoch 2045, Loss: 0.2656885161995888, Final Batch Loss: 0.1221202090382576\n",
      "Epoch 2046, Loss: 0.2375125139951706, Final Batch Loss: 0.11434172093868256\n",
      "Epoch 2047, Loss: 0.27465370297431946, Final Batch Loss: 0.1112702339887619\n",
      "Epoch 2048, Loss: 0.29756632447242737, Final Batch Loss: 0.14976023137569427\n",
      "Epoch 2049, Loss: 0.3174961507320404, Final Batch Loss: 0.1705627143383026\n",
      "Epoch 2050, Loss: 0.241883784532547, Final Batch Loss: 0.13149315118789673\n",
      "Epoch 2051, Loss: 0.28193774074316025, Final Batch Loss: 0.16627611219882965\n",
      "Epoch 2052, Loss: 0.3106105178594589, Final Batch Loss: 0.11661873757839203\n",
      "Epoch 2053, Loss: 0.2536817416548729, Final Batch Loss: 0.11065831035375595\n",
      "Epoch 2054, Loss: 0.37019744515419006, Final Batch Loss: 0.1617337018251419\n",
      "Epoch 2055, Loss: 0.2623926028609276, Final Batch Loss: 0.11924353986978531\n",
      "Epoch 2056, Loss: 0.22355994582176208, Final Batch Loss: 0.12077958881855011\n",
      "Epoch 2057, Loss: 0.259936660528183, Final Batch Loss: 0.13279004395008087\n",
      "Epoch 2058, Loss: 0.22838278859853745, Final Batch Loss: 0.10157311707735062\n",
      "Epoch 2059, Loss: 0.21001236140727997, Final Batch Loss: 0.0959453284740448\n",
      "Epoch 2060, Loss: 0.2100624367594719, Final Batch Loss: 0.0959700495004654\n",
      "Epoch 2061, Loss: 0.2932521104812622, Final Batch Loss: 0.18998192250728607\n",
      "Epoch 2062, Loss: 0.2523450329899788, Final Batch Loss: 0.09394028037786484\n",
      "Epoch 2063, Loss: 0.303047314286232, Final Batch Loss: 0.1504099816083908\n",
      "Epoch 2064, Loss: 0.28493404388427734, Final Batch Loss: 0.08328934013843536\n",
      "Epoch 2065, Loss: 0.2801305502653122, Final Batch Loss: 0.1475694328546524\n",
      "Epoch 2066, Loss: 0.30311107635498047, Final Batch Loss: 0.16585935652256012\n",
      "Epoch 2067, Loss: 0.3003295958042145, Final Batch Loss: 0.18834245204925537\n",
      "Epoch 2068, Loss: 0.2060309499502182, Final Batch Loss: 0.08227452635765076\n",
      "Epoch 2069, Loss: 0.3474172502756119, Final Batch Loss: 0.13987351953983307\n",
      "Epoch 2070, Loss: 0.28880684077739716, Final Batch Loss: 0.13760438561439514\n",
      "Epoch 2071, Loss: 0.337117537856102, Final Batch Loss: 0.19619278609752655\n",
      "Epoch 2072, Loss: 0.35818442702293396, Final Batch Loss: 0.15871977806091309\n",
      "Epoch 2073, Loss: 0.2637704461812973, Final Batch Loss: 0.12223419547080994\n",
      "Epoch 2074, Loss: 0.30721430480480194, Final Batch Loss: 0.18105262517929077\n",
      "Epoch 2075, Loss: 0.2754300609230995, Final Batch Loss: 0.08575432747602463\n",
      "Epoch 2076, Loss: 0.3315139412879944, Final Batch Loss: 0.18032461404800415\n",
      "Epoch 2077, Loss: 0.31759607791900635, Final Batch Loss: 0.16635720431804657\n",
      "Epoch 2078, Loss: 0.2439163625240326, Final Batch Loss: 0.10433310270309448\n",
      "Epoch 2079, Loss: 0.3712591975927353, Final Batch Loss: 0.1437007337808609\n",
      "Epoch 2080, Loss: 0.38552944362163544, Final Batch Loss: 0.26956233382225037\n",
      "Epoch 2081, Loss: 0.3322601169347763, Final Batch Loss: 0.18122024834156036\n",
      "Epoch 2082, Loss: 0.23989559710025787, Final Batch Loss: 0.13127759099006653\n",
      "Epoch 2083, Loss: 0.2265210896730423, Final Batch Loss: 0.09474290907382965\n",
      "Epoch 2084, Loss: 0.3157510757446289, Final Batch Loss: 0.12494279444217682\n",
      "Epoch 2085, Loss: 0.26618340611457825, Final Batch Loss: 0.1301000565290451\n",
      "Epoch 2086, Loss: 0.2294996678829193, Final Batch Loss: 0.13678714632987976\n",
      "Epoch 2087, Loss: 0.17940174788236618, Final Batch Loss: 0.09223422408103943\n",
      "Epoch 2088, Loss: 0.2491186186671257, Final Batch Loss: 0.11284781247377396\n",
      "Epoch 2089, Loss: 0.22674978524446487, Final Batch Loss: 0.08770263940095901\n",
      "Epoch 2090, Loss: 0.3547156751155853, Final Batch Loss: 0.22230729460716248\n",
      "Epoch 2091, Loss: 0.22486617416143417, Final Batch Loss: 0.13324758410453796\n",
      "Epoch 2092, Loss: 0.2900744155049324, Final Batch Loss: 0.12473180145025253\n",
      "Epoch 2093, Loss: 0.26252373307943344, Final Batch Loss: 0.11995474249124527\n",
      "Epoch 2094, Loss: 0.3425491899251938, Final Batch Loss: 0.2343175709247589\n",
      "Epoch 2095, Loss: 0.25332044064998627, Final Batch Loss: 0.13165536522865295\n",
      "Epoch 2096, Loss: 0.2914550304412842, Final Batch Loss: 0.17303280532360077\n",
      "Epoch 2097, Loss: 0.27291473001241684, Final Batch Loss: 0.16335518658161163\n",
      "Epoch 2098, Loss: 0.2775292620062828, Final Batch Loss: 0.12287188321352005\n",
      "Epoch 2099, Loss: 0.34361833333969116, Final Batch Loss: 0.16283851861953735\n",
      "Epoch 2100, Loss: 0.3169232979416847, Final Batch Loss: 0.11731397360563278\n",
      "Epoch 2101, Loss: 0.41983410716056824, Final Batch Loss: 0.30525892972946167\n",
      "Epoch 2102, Loss: 0.24228505790233612, Final Batch Loss: 0.11984828114509583\n",
      "Epoch 2103, Loss: 0.43380503356456757, Final Batch Loss: 0.25506600737571716\n",
      "Epoch 2104, Loss: 0.22594792395830154, Final Batch Loss: 0.08412303775548935\n",
      "Epoch 2105, Loss: 0.29843755066394806, Final Batch Loss: 0.10519225895404816\n",
      "Epoch 2106, Loss: 0.20867793262004852, Final Batch Loss: 0.08854687958955765\n",
      "Epoch 2107, Loss: 0.2781156972050667, Final Batch Loss: 0.10703057795763016\n",
      "Epoch 2108, Loss: 0.3731238543987274, Final Batch Loss: 0.194546177983284\n",
      "Epoch 2109, Loss: 0.3570321649312973, Final Batch Loss: 0.19671416282653809\n",
      "Epoch 2110, Loss: 0.2532135546207428, Final Batch Loss: 0.10803534090518951\n",
      "Epoch 2111, Loss: 0.2586672604084015, Final Batch Loss: 0.10723473131656647\n",
      "Epoch 2112, Loss: 0.2846289128065109, Final Batch Loss: 0.15475857257843018\n",
      "Epoch 2113, Loss: 0.24572845548391342, Final Batch Loss: 0.13246610760688782\n",
      "Epoch 2114, Loss: 0.21198159456253052, Final Batch Loss: 0.11030222475528717\n",
      "Epoch 2115, Loss: 0.2930149510502815, Final Batch Loss: 0.1727513074874878\n",
      "Epoch 2116, Loss: 0.21620599180459976, Final Batch Loss: 0.10708735138177872\n",
      "Epoch 2117, Loss: 0.24658309668302536, Final Batch Loss: 0.0855511948466301\n",
      "Epoch 2118, Loss: 0.30213940143585205, Final Batch Loss: 0.14150860905647278\n",
      "Epoch 2119, Loss: 0.2495649829506874, Final Batch Loss: 0.13739347457885742\n",
      "Epoch 2120, Loss: 0.30221524834632874, Final Batch Loss: 0.10381059348583221\n",
      "Epoch 2121, Loss: 0.25989876687526703, Final Batch Loss: 0.1397119015455246\n",
      "Epoch 2122, Loss: 0.22077932208776474, Final Batch Loss: 0.12815359234809875\n",
      "Epoch 2123, Loss: 0.22530446201562881, Final Batch Loss: 0.10458327829837799\n",
      "Epoch 2124, Loss: 0.21893376857042313, Final Batch Loss: 0.06741655617952347\n",
      "Epoch 2125, Loss: 0.24359743297100067, Final Batch Loss: 0.11380195617675781\n",
      "Epoch 2126, Loss: 0.20873484015464783, Final Batch Loss: 0.08322258293628693\n",
      "Epoch 2127, Loss: 0.37128231674432755, Final Batch Loss: 0.2577923536300659\n",
      "Epoch 2128, Loss: 0.2234979271888733, Final Batch Loss: 0.12420748174190521\n",
      "Epoch 2129, Loss: 0.25638897716999054, Final Batch Loss: 0.13739895820617676\n",
      "Epoch 2130, Loss: 0.22919953614473343, Final Batch Loss: 0.09144770354032516\n",
      "Epoch 2131, Loss: 0.3101084530353546, Final Batch Loss: 0.1749500334262848\n",
      "Epoch 2132, Loss: 0.2935972809791565, Final Batch Loss: 0.163824662566185\n",
      "Epoch 2133, Loss: 0.24306321889162064, Final Batch Loss: 0.1322193741798401\n",
      "Epoch 2134, Loss: 0.2661207690834999, Final Batch Loss: 0.1604706346988678\n",
      "Epoch 2135, Loss: 0.29898060113191605, Final Batch Loss: 0.11980553716421127\n",
      "Epoch 2136, Loss: 0.2044474482536316, Final Batch Loss: 0.09861767292022705\n",
      "Epoch 2137, Loss: 0.2795342355966568, Final Batch Loss: 0.14778488874435425\n",
      "Epoch 2138, Loss: 0.28722183406352997, Final Batch Loss: 0.11687330901622772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2139, Loss: 0.3130814954638481, Final Batch Loss: 0.22624485194683075\n",
      "Epoch 2140, Loss: 0.21328554302453995, Final Batch Loss: 0.06981203705072403\n",
      "Epoch 2141, Loss: 0.19234582781791687, Final Batch Loss: 0.06686845421791077\n",
      "Epoch 2142, Loss: 0.27993565052747726, Final Batch Loss: 0.1723940670490265\n",
      "Epoch 2143, Loss: 0.3283584415912628, Final Batch Loss: 0.13438282907009125\n",
      "Epoch 2144, Loss: 0.2521689087152481, Final Batch Loss: 0.14121420681476593\n",
      "Epoch 2145, Loss: 0.18266243487596512, Final Batch Loss: 0.08792483061552048\n",
      "Epoch 2146, Loss: 0.21153021603822708, Final Batch Loss: 0.09343210607767105\n",
      "Epoch 2147, Loss: 0.27781759202480316, Final Batch Loss: 0.15822413563728333\n",
      "Epoch 2148, Loss: 0.21544737368822098, Final Batch Loss: 0.12059809267520905\n",
      "Epoch 2149, Loss: 0.21221274882555008, Final Batch Loss: 0.11879958212375641\n",
      "Epoch 2150, Loss: 0.30985963344573975, Final Batch Loss: 0.12812437117099762\n",
      "Epoch 2151, Loss: 0.23465608060359955, Final Batch Loss: 0.12847556173801422\n",
      "Epoch 2152, Loss: 0.3461858928203583, Final Batch Loss: 0.1936221867799759\n",
      "Epoch 2153, Loss: 0.26191629469394684, Final Batch Loss: 0.18557894229888916\n",
      "Epoch 2154, Loss: 0.3307570070028305, Final Batch Loss: 0.20819111168384552\n",
      "Epoch 2155, Loss: 0.29840226471424103, Final Batch Loss: 0.12832964956760406\n",
      "Epoch 2156, Loss: 0.31053435802459717, Final Batch Loss: 0.1735735535621643\n",
      "Epoch 2157, Loss: 0.2867981046438217, Final Batch Loss: 0.13006454706192017\n",
      "Epoch 2158, Loss: 0.27845442295074463, Final Batch Loss: 0.15354612469673157\n",
      "Epoch 2159, Loss: 0.2695455402135849, Final Batch Loss: 0.13441096246242523\n",
      "Epoch 2160, Loss: 0.23457618057727814, Final Batch Loss: 0.1094067394733429\n",
      "Epoch 2161, Loss: 0.19355473667383194, Final Batch Loss: 0.07834962755441666\n",
      "Epoch 2162, Loss: 0.22912118583917618, Final Batch Loss: 0.11852043867111206\n",
      "Epoch 2163, Loss: 0.301550030708313, Final Batch Loss: 0.14425228536128998\n",
      "Epoch 2164, Loss: 0.21186982840299606, Final Batch Loss: 0.08983194082975388\n",
      "Epoch 2165, Loss: 0.22001242637634277, Final Batch Loss: 0.10805078595876694\n",
      "Epoch 2166, Loss: 0.25130026042461395, Final Batch Loss: 0.09280534088611603\n",
      "Epoch 2167, Loss: 0.39272473752498627, Final Batch Loss: 0.2112557590007782\n",
      "Epoch 2168, Loss: 0.28504713624715805, Final Batch Loss: 0.20095551013946533\n",
      "Epoch 2169, Loss: 0.1722741536796093, Final Batch Loss: 0.11069238185882568\n",
      "Epoch 2170, Loss: 0.23488200455904007, Final Batch Loss: 0.10899417847394943\n",
      "Epoch 2171, Loss: 0.2382221594452858, Final Batch Loss: 0.10405700653791428\n",
      "Epoch 2172, Loss: 0.24266087263822556, Final Batch Loss: 0.13285690546035767\n",
      "Epoch 2173, Loss: 0.21256174892187119, Final Batch Loss: 0.09345725923776627\n",
      "Epoch 2174, Loss: 0.2198493331670761, Final Batch Loss: 0.12415589392185211\n",
      "Epoch 2175, Loss: 0.3509664982557297, Final Batch Loss: 0.16924326121807098\n",
      "Epoch 2176, Loss: 0.23265962302684784, Final Batch Loss: 0.09359721839427948\n",
      "Epoch 2177, Loss: 0.2764546424150467, Final Batch Loss: 0.13995365798473358\n",
      "Epoch 2178, Loss: 0.2614222764968872, Final Batch Loss: 0.12791359424591064\n",
      "Epoch 2179, Loss: 0.3197137713432312, Final Batch Loss: 0.1671716719865799\n",
      "Epoch 2180, Loss: 0.29167603701353073, Final Batch Loss: 0.10943172127008438\n",
      "Epoch 2181, Loss: 0.1746034324169159, Final Batch Loss: 0.07543203234672546\n",
      "Epoch 2182, Loss: 0.29364731907844543, Final Batch Loss: 0.17595581710338593\n",
      "Epoch 2183, Loss: 0.2939545810222626, Final Batch Loss: 0.15663011372089386\n",
      "Epoch 2184, Loss: 0.36294734477996826, Final Batch Loss: 0.22680166363716125\n",
      "Epoch 2185, Loss: 0.13160444423556328, Final Batch Loss: 0.062050919979810715\n",
      "Epoch 2186, Loss: 0.2617388069629669, Final Batch Loss: 0.1304907500743866\n",
      "Epoch 2187, Loss: 0.20328062772750854, Final Batch Loss: 0.10631892085075378\n",
      "Epoch 2188, Loss: 0.2707507014274597, Final Batch Loss: 0.12992697954177856\n",
      "Epoch 2189, Loss: 0.28946344554424286, Final Batch Loss: 0.11989569664001465\n",
      "Epoch 2190, Loss: 0.22799205780029297, Final Batch Loss: 0.12738020718097687\n",
      "Epoch 2191, Loss: 0.25085560977458954, Final Batch Loss: 0.08426326513290405\n",
      "Epoch 2192, Loss: 0.2354246824979782, Final Batch Loss: 0.12629905343055725\n",
      "Epoch 2193, Loss: 0.26447515189647675, Final Batch Loss: 0.12859255075454712\n",
      "Epoch 2194, Loss: 0.22737590968608856, Final Batch Loss: 0.09122645854949951\n",
      "Epoch 2195, Loss: 0.19411132484674454, Final Batch Loss: 0.10027045011520386\n",
      "Epoch 2196, Loss: 0.30123601108789444, Final Batch Loss: 0.18639829754829407\n",
      "Epoch 2197, Loss: 0.2978488951921463, Final Batch Loss: 0.16997471451759338\n",
      "Epoch 2198, Loss: 0.20824070274829865, Final Batch Loss: 0.11945027112960815\n",
      "Epoch 2199, Loss: 0.23019444942474365, Final Batch Loss: 0.12937694787979126\n",
      "Epoch 2200, Loss: 0.23405399173498154, Final Batch Loss: 0.11453144997358322\n",
      "Epoch 2201, Loss: 0.2399596869945526, Final Batch Loss: 0.07650361955165863\n",
      "Epoch 2202, Loss: 0.190573338419199, Final Batch Loss: 0.05934228375554085\n",
      "Epoch 2203, Loss: 0.2560465857386589, Final Batch Loss: 0.08273767679929733\n",
      "Epoch 2204, Loss: 0.19105850160121918, Final Batch Loss: 0.09752827137708664\n",
      "Epoch 2205, Loss: 0.26706962287425995, Final Batch Loss: 0.13061515986919403\n",
      "Epoch 2206, Loss: 0.35753753781318665, Final Batch Loss: 0.20111346244812012\n",
      "Epoch 2207, Loss: 0.3237634152173996, Final Batch Loss: 0.16064241528511047\n",
      "Epoch 2208, Loss: 0.26465165615081787, Final Batch Loss: 0.13065870106220245\n",
      "Epoch 2209, Loss: 0.16501671075820923, Final Batch Loss: 0.08718959242105484\n",
      "Epoch 2210, Loss: 0.28439948707818985, Final Batch Loss: 0.19592143595218658\n",
      "Epoch 2211, Loss: 0.2567533329129219, Final Batch Loss: 0.13686923682689667\n",
      "Epoch 2212, Loss: 0.25028716772794724, Final Batch Loss: 0.12319820374250412\n",
      "Epoch 2213, Loss: 0.24762357026338577, Final Batch Loss: 0.1243966743350029\n",
      "Epoch 2214, Loss: 0.2967213839292526, Final Batch Loss: 0.1649407148361206\n",
      "Epoch 2215, Loss: 0.24157557636499405, Final Batch Loss: 0.07348393648862839\n",
      "Epoch 2216, Loss: 0.34116292744874954, Final Batch Loss: 0.21968181431293488\n",
      "Epoch 2217, Loss: 0.22357060760259628, Final Batch Loss: 0.14346006512641907\n",
      "Epoch 2218, Loss: 0.278021976351738, Final Batch Loss: 0.18210671842098236\n",
      "Epoch 2219, Loss: 0.18119683861732483, Final Batch Loss: 0.0767778679728508\n",
      "Epoch 2220, Loss: 0.2899380177259445, Final Batch Loss: 0.16761524975299835\n",
      "Epoch 2221, Loss: 0.19282713532447815, Final Batch Loss: 0.05481159687042236\n",
      "Epoch 2222, Loss: 0.21995387971401215, Final Batch Loss: 0.11815734952688217\n",
      "Epoch 2223, Loss: 0.23305544257164001, Final Batch Loss: 0.11706101149320602\n",
      "Epoch 2224, Loss: 0.3262014389038086, Final Batch Loss: 0.15749791264533997\n",
      "Epoch 2225, Loss: 0.28344373404979706, Final Batch Loss: 0.1368025690317154\n",
      "Epoch 2226, Loss: 0.20544074475765228, Final Batch Loss: 0.10044649988412857\n",
      "Epoch 2227, Loss: 0.24130646884441376, Final Batch Loss: 0.09166890382766724\n",
      "Epoch 2228, Loss: 0.3108099102973938, Final Batch Loss: 0.13784900307655334\n",
      "Epoch 2229, Loss: 0.172984316945076, Final Batch Loss: 0.07424449175596237\n",
      "Epoch 2230, Loss: 0.3319825679063797, Final Batch Loss: 0.1954900324344635\n",
      "Epoch 2231, Loss: 0.25723588466644287, Final Batch Loss: 0.12963348627090454\n",
      "Epoch 2232, Loss: 0.22622676193714142, Final Batch Loss: 0.07662191987037659\n",
      "Epoch 2233, Loss: 0.27635548263788223, Final Batch Loss: 0.11614178866147995\n",
      "Epoch 2234, Loss: 0.35024209320545197, Final Batch Loss: 0.25085917115211487\n",
      "Epoch 2235, Loss: 0.31823354959487915, Final Batch Loss: 0.16328661143779755\n",
      "Epoch 2236, Loss: 0.35939665138721466, Final Batch Loss: 0.22924116253852844\n",
      "Epoch 2237, Loss: 0.3172650933265686, Final Batch Loss: 0.13611239194869995\n",
      "Epoch 2238, Loss: 0.3151993826031685, Final Batch Loss: 0.21997375786304474\n",
      "Epoch 2239, Loss: 0.2151016891002655, Final Batch Loss: 0.09919599443674088\n",
      "Epoch 2240, Loss: 0.24773439764976501, Final Batch Loss: 0.08145771920681\n",
      "Epoch 2241, Loss: 0.25067735463380814, Final Batch Loss: 0.1304192990064621\n",
      "Epoch 2242, Loss: 0.21385729312896729, Final Batch Loss: 0.08057674765586853\n",
      "Epoch 2243, Loss: 0.25428834557533264, Final Batch Loss: 0.10050751268863678\n",
      "Epoch 2244, Loss: 0.19128240644931793, Final Batch Loss: 0.08211743831634521\n",
      "Epoch 2245, Loss: 0.23454246670007706, Final Batch Loss: 0.11649789661169052\n",
      "Epoch 2246, Loss: 0.21617284417152405, Final Batch Loss: 0.12500222027301788\n",
      "Epoch 2247, Loss: 0.25430014729499817, Final Batch Loss: 0.10309113562107086\n",
      "Epoch 2248, Loss: 0.19777227938175201, Final Batch Loss: 0.0751037523150444\n",
      "Epoch 2249, Loss: 0.19696495682001114, Final Batch Loss: 0.1094997301697731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2250, Loss: 0.23096098005771637, Final Batch Loss: 0.07580506801605225\n",
      "Epoch 2251, Loss: 0.27203238010406494, Final Batch Loss: 0.1175067275762558\n",
      "Epoch 2252, Loss: 0.2577603906393051, Final Batch Loss: 0.10394532978534698\n",
      "Epoch 2253, Loss: 0.2854659929871559, Final Batch Loss: 0.1907937079668045\n",
      "Epoch 2254, Loss: 0.3223034143447876, Final Batch Loss: 0.15670904517173767\n",
      "Epoch 2255, Loss: 0.3101450502872467, Final Batch Loss: 0.15028293430805206\n",
      "Epoch 2256, Loss: 0.22303535044193268, Final Batch Loss: 0.12496747076511383\n",
      "Epoch 2257, Loss: 0.2537732757627964, Final Batch Loss: 0.19149596989154816\n",
      "Epoch 2258, Loss: 0.3284597098827362, Final Batch Loss: 0.1598089188337326\n",
      "Epoch 2259, Loss: 0.20927244424819946, Final Batch Loss: 0.09050429612398148\n",
      "Epoch 2260, Loss: 0.2815255969762802, Final Batch Loss: 0.13234932720661163\n",
      "Epoch 2261, Loss: 0.27050428092479706, Final Batch Loss: 0.11298424005508423\n",
      "Epoch 2262, Loss: 0.3512173965573311, Final Batch Loss: 0.2330414205789566\n",
      "Epoch 2263, Loss: 0.2891921103000641, Final Batch Loss: 0.1596204787492752\n",
      "Epoch 2264, Loss: 0.29357946664094925, Final Batch Loss: 0.20965807139873505\n",
      "Epoch 2265, Loss: 0.2945459187030792, Final Batch Loss: 0.14853273332118988\n",
      "Epoch 2266, Loss: 0.18193136900663376, Final Batch Loss: 0.08553148061037064\n",
      "Epoch 2267, Loss: 0.3332021236419678, Final Batch Loss: 0.16431306302547455\n",
      "Epoch 2268, Loss: 0.2524971440434456, Final Batch Loss: 0.12218088656663895\n",
      "Epoch 2269, Loss: 0.19350258260965347, Final Batch Loss: 0.09201409667730331\n",
      "Epoch 2270, Loss: 0.23319519311189651, Final Batch Loss: 0.14439928531646729\n",
      "Epoch 2271, Loss: 0.2495052069425583, Final Batch Loss: 0.13012497127056122\n",
      "Epoch 2272, Loss: 0.20116661489009857, Final Batch Loss: 0.11507242172956467\n",
      "Epoch 2273, Loss: 0.24509235471487045, Final Batch Loss: 0.11203453689813614\n",
      "Epoch 2274, Loss: 0.16791275143623352, Final Batch Loss: 0.06474924087524414\n",
      "Epoch 2275, Loss: 0.22792180627584457, Final Batch Loss: 0.08027992397546768\n",
      "Epoch 2276, Loss: 0.3153049051761627, Final Batch Loss: 0.15173299610614777\n",
      "Epoch 2277, Loss: 0.18595841899514198, Final Batch Loss: 0.0622900165617466\n",
      "Epoch 2278, Loss: 0.2859814316034317, Final Batch Loss: 0.12301065027713776\n",
      "Epoch 2279, Loss: 0.3467274159193039, Final Batch Loss: 0.2072700411081314\n",
      "Epoch 2280, Loss: 0.2671894431114197, Final Batch Loss: 0.16001899540424347\n",
      "Epoch 2281, Loss: 0.32929328083992004, Final Batch Loss: 0.1375724822282791\n",
      "Epoch 2282, Loss: 0.26548823714256287, Final Batch Loss: 0.12623803317546844\n",
      "Epoch 2283, Loss: 0.37330207228660583, Final Batch Loss: 0.15794065594673157\n",
      "Epoch 2284, Loss: 0.39895619451999664, Final Batch Loss: 0.20208141207695007\n",
      "Epoch 2285, Loss: 0.2661973871290684, Final Batch Loss: 0.21084073185920715\n",
      "Epoch 2286, Loss: 0.35483188927173615, Final Batch Loss: 0.17210166156291962\n",
      "Epoch 2287, Loss: 0.22975774854421616, Final Batch Loss: 0.10455776005983353\n",
      "Epoch 2288, Loss: 0.21256282180547714, Final Batch Loss: 0.10080301016569138\n",
      "Epoch 2289, Loss: 0.23742251843214035, Final Batch Loss: 0.13653326034545898\n",
      "Epoch 2290, Loss: 0.20922022312879562, Final Batch Loss: 0.12849198281764984\n",
      "Epoch 2291, Loss: 0.2142915576696396, Final Batch Loss: 0.08966182172298431\n",
      "Epoch 2292, Loss: 0.167744018137455, Final Batch Loss: 0.06707651913166046\n",
      "Epoch 2293, Loss: 0.18468914926052094, Final Batch Loss: 0.06890552490949631\n",
      "Epoch 2294, Loss: 0.17208846658468246, Final Batch Loss: 0.09335622191429138\n",
      "Epoch 2295, Loss: 0.277042455971241, Final Batch Loss: 0.10790766030550003\n",
      "Epoch 2296, Loss: 0.2578697055578232, Final Batch Loss: 0.15934112668037415\n",
      "Epoch 2297, Loss: 0.18046001344919205, Final Batch Loss: 0.07577607035636902\n",
      "Epoch 2298, Loss: 0.22793900221586227, Final Batch Loss: 0.14459441602230072\n",
      "Epoch 2299, Loss: 0.28324559330940247, Final Batch Loss: 0.17106851935386658\n",
      "Epoch 2300, Loss: 0.21938645839691162, Final Batch Loss: 0.1027151495218277\n",
      "Epoch 2301, Loss: 0.3494577258825302, Final Batch Loss: 0.22970618307590485\n",
      "Epoch 2302, Loss: 0.3237302899360657, Final Batch Loss: 0.14149010181427002\n",
      "Epoch 2303, Loss: 0.3754798695445061, Final Batch Loss: 0.11655335873365402\n",
      "Epoch 2304, Loss: 0.25603145360946655, Final Batch Loss: 0.09085544943809509\n",
      "Epoch 2305, Loss: 0.2344149947166443, Final Batch Loss: 0.08154985308647156\n",
      "Epoch 2306, Loss: 0.2618682533502579, Final Batch Loss: 0.12031243741512299\n",
      "Epoch 2307, Loss: 0.19017785042524338, Final Batch Loss: 0.09644033759832382\n",
      "Epoch 2308, Loss: 0.24086235463619232, Final Batch Loss: 0.11445550620555878\n",
      "Epoch 2309, Loss: 0.28377415239810944, Final Batch Loss: 0.10382218658924103\n",
      "Epoch 2310, Loss: 0.17387812584638596, Final Batch Loss: 0.0984792709350586\n",
      "Epoch 2311, Loss: 0.2206723615527153, Final Batch Loss: 0.10253429412841797\n",
      "Epoch 2312, Loss: 0.24808023124933243, Final Batch Loss: 0.161847785115242\n",
      "Epoch 2313, Loss: 0.20117733627557755, Final Batch Loss: 0.08558297157287598\n",
      "Epoch 2314, Loss: 0.17999381572008133, Final Batch Loss: 0.07047593593597412\n",
      "Epoch 2315, Loss: 0.18420013785362244, Final Batch Loss: 0.09399813413619995\n",
      "Epoch 2316, Loss: 0.2719520181417465, Final Batch Loss: 0.14963391423225403\n",
      "Epoch 2317, Loss: 0.26619721949100494, Final Batch Loss: 0.09599792957305908\n",
      "Epoch 2318, Loss: 0.16815951466560364, Final Batch Loss: 0.07490856200456619\n",
      "Epoch 2319, Loss: 0.27526458352804184, Final Batch Loss: 0.12424585968255997\n",
      "Epoch 2320, Loss: 0.246102012693882, Final Batch Loss: 0.08217693120241165\n",
      "Epoch 2321, Loss: 0.18959087133407593, Final Batch Loss: 0.08767174184322357\n",
      "Epoch 2322, Loss: 0.21015770733356476, Final Batch Loss: 0.0985397920012474\n",
      "Epoch 2323, Loss: 0.17727921158075333, Final Batch Loss: 0.08431392163038254\n",
      "Epoch 2324, Loss: 0.20636321604251862, Final Batch Loss: 0.08565616607666016\n",
      "Epoch 2325, Loss: 0.26195061206817627, Final Batch Loss: 0.13545332849025726\n",
      "Epoch 2326, Loss: 0.2422540858387947, Final Batch Loss: 0.1297435313463211\n",
      "Epoch 2327, Loss: 0.24164754152297974, Final Batch Loss: 0.15829917788505554\n",
      "Epoch 2328, Loss: 0.20996133238077164, Final Batch Loss: 0.08664119988679886\n",
      "Epoch 2329, Loss: 0.41187429428100586, Final Batch Loss: 0.2621987760066986\n",
      "Epoch 2330, Loss: 0.2835555523633957, Final Batch Loss: 0.11961852014064789\n",
      "Epoch 2331, Loss: 0.32940860092639923, Final Batch Loss: 0.18602347373962402\n",
      "Epoch 2332, Loss: 0.26418305188417435, Final Batch Loss: 0.15179665386676788\n",
      "Epoch 2333, Loss: 0.472738541662693, Final Batch Loss: 0.37111568450927734\n",
      "Epoch 2334, Loss: 0.24063041806221008, Final Batch Loss: 0.11700700223445892\n",
      "Epoch 2335, Loss: 0.2494712471961975, Final Batch Loss: 0.10829348862171173\n",
      "Epoch 2336, Loss: 0.2067137509584427, Final Batch Loss: 0.09417148679494858\n",
      "Epoch 2337, Loss: 0.2603895664215088, Final Batch Loss: 0.13083870708942413\n",
      "Epoch 2338, Loss: 0.2625107765197754, Final Batch Loss: 0.10198569297790527\n",
      "Epoch 2339, Loss: 0.23595329374074936, Final Batch Loss: 0.10092658549547195\n",
      "Epoch 2340, Loss: 0.29880959540605545, Final Batch Loss: 0.1838156133890152\n",
      "Epoch 2341, Loss: 0.22921942174434662, Final Batch Loss: 0.10182636976242065\n",
      "Epoch 2342, Loss: 0.2499009594321251, Final Batch Loss: 0.1386481076478958\n",
      "Epoch 2343, Loss: 0.22692900151014328, Final Batch Loss: 0.11289621889591217\n",
      "Epoch 2344, Loss: 0.24336744844913483, Final Batch Loss: 0.10106225311756134\n",
      "Epoch 2345, Loss: 0.21160399913787842, Final Batch Loss: 0.11135305464267731\n",
      "Epoch 2346, Loss: 0.20153801888227463, Final Batch Loss: 0.09786864370107651\n",
      "Epoch 2347, Loss: 0.237539142370224, Final Batch Loss: 0.11426589637994766\n",
      "Epoch 2348, Loss: 0.23392537981271744, Final Batch Loss: 0.11346087604761124\n",
      "Epoch 2349, Loss: 0.22462379187345505, Final Batch Loss: 0.1133972629904747\n",
      "Epoch 2350, Loss: 0.32208026200532913, Final Batch Loss: 0.22399739921092987\n",
      "Epoch 2351, Loss: 0.17773699015378952, Final Batch Loss: 0.10187124460935593\n",
      "Epoch 2352, Loss: 0.2973697930574417, Final Batch Loss: 0.2048819214105606\n",
      "Epoch 2353, Loss: 0.25588247925043106, Final Batch Loss: 0.15077386796474457\n",
      "Epoch 2354, Loss: 0.4061884358525276, Final Batch Loss: 0.2975424528121948\n",
      "Epoch 2355, Loss: 0.2537580505013466, Final Batch Loss: 0.10872771590948105\n",
      "Epoch 2356, Loss: 0.16602826863527298, Final Batch Loss: 0.06908461451530457\n",
      "Epoch 2357, Loss: 0.2805093750357628, Final Batch Loss: 0.16221922636032104\n",
      "Epoch 2358, Loss: 0.23247254639863968, Final Batch Loss: 0.10116799920797348\n",
      "Epoch 2359, Loss: 0.20091058313846588, Final Batch Loss: 0.09396576136350632\n",
      "Epoch 2360, Loss: 0.251475490629673, Final Batch Loss: 0.14448390901088715\n",
      "Epoch 2361, Loss: 0.21432103961706161, Final Batch Loss: 0.09399785101413727\n",
      "Epoch 2362, Loss: 0.20361369848251343, Final Batch Loss: 0.08567209541797638\n",
      "Epoch 2363, Loss: 0.26739759743213654, Final Batch Loss: 0.17465795576572418\n",
      "Epoch 2364, Loss: 0.2244047001004219, Final Batch Loss: 0.08829928189516068\n",
      "Epoch 2365, Loss: 0.1870567500591278, Final Batch Loss: 0.09279032796621323\n",
      "Epoch 2366, Loss: 0.1995098814368248, Final Batch Loss: 0.08934804797172546\n",
      "Epoch 2367, Loss: 0.24973513931035995, Final Batch Loss: 0.15243428945541382\n",
      "Epoch 2368, Loss: 0.32736655324697495, Final Batch Loss: 0.10115651041269302\n",
      "Epoch 2369, Loss: 0.2568567097187042, Final Batch Loss: 0.08539356291294098\n",
      "Epoch 2370, Loss: 0.19308629631996155, Final Batch Loss: 0.07348140329122543\n",
      "Epoch 2371, Loss: 0.24053781479597092, Final Batch Loss: 0.11406057327985764\n",
      "Epoch 2372, Loss: 0.18580356985330582, Final Batch Loss: 0.08633360266685486\n",
      "Epoch 2373, Loss: 0.2482762560248375, Final Batch Loss: 0.14815959334373474\n",
      "Epoch 2374, Loss: 0.26154011487960815, Final Batch Loss: 0.137286975979805\n",
      "Epoch 2375, Loss: 0.18555062264204025, Final Batch Loss: 0.07198174297809601\n",
      "Epoch 2376, Loss: 0.24084074795246124, Final Batch Loss: 0.10914264619350433\n",
      "Epoch 2377, Loss: 0.19646859914064407, Final Batch Loss: 0.0677810087800026\n",
      "Epoch 2378, Loss: 0.1993713527917862, Final Batch Loss: 0.07957795262336731\n",
      "Epoch 2379, Loss: 0.17979644984006882, Final Batch Loss: 0.0839463323354721\n",
      "Epoch 2380, Loss: 0.1599433310329914, Final Batch Loss: 0.0579182393848896\n",
      "Epoch 2381, Loss: 0.2786179333925247, Final Batch Loss: 0.16655105352401733\n",
      "Epoch 2382, Loss: 0.21693578362464905, Final Batch Loss: 0.12940385937690735\n",
      "Epoch 2383, Loss: 0.17295435816049576, Final Batch Loss: 0.09724490344524384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2384, Loss: 0.41030458360910416, Final Batch Loss: 0.28605470061302185\n",
      "Epoch 2385, Loss: 0.24901241809129715, Final Batch Loss: 0.10002703219652176\n",
      "Epoch 2386, Loss: 0.2716117948293686, Final Batch Loss: 0.16602037847042084\n",
      "Epoch 2387, Loss: 0.17146199941635132, Final Batch Loss: 0.06195145100355148\n",
      "Epoch 2388, Loss: 0.22453320026397705, Final Batch Loss: 0.10712411999702454\n",
      "Epoch 2389, Loss: 0.2126879170536995, Final Batch Loss: 0.12439389526844025\n",
      "Epoch 2390, Loss: 0.2699959948658943, Final Batch Loss: 0.10749790817499161\n",
      "Epoch 2391, Loss: 0.20765257626771927, Final Batch Loss: 0.08694124221801758\n",
      "Epoch 2392, Loss: 0.23566153645515442, Final Batch Loss: 0.1021941751241684\n",
      "Epoch 2393, Loss: 0.23877034336328506, Final Batch Loss: 0.11515267938375473\n",
      "Epoch 2394, Loss: 0.2866009697318077, Final Batch Loss: 0.1710832417011261\n",
      "Epoch 2395, Loss: 0.2675490453839302, Final Batch Loss: 0.14287851750850677\n",
      "Epoch 2396, Loss: 0.1503819301724434, Final Batch Loss: 0.07044398039579391\n",
      "Epoch 2397, Loss: 0.29413654655218124, Final Batch Loss: 0.22983410954475403\n",
      "Epoch 2398, Loss: 0.183953195810318, Final Batch Loss: 0.10019510984420776\n",
      "Epoch 2399, Loss: 0.16232645139098167, Final Batch Loss: 0.05640406534075737\n",
      "Epoch 2400, Loss: 0.29617177695035934, Final Batch Loss: 0.12464515119791031\n",
      "Epoch 2401, Loss: 0.261052243411541, Final Batch Loss: 0.10509974509477615\n",
      "Epoch 2402, Loss: 0.22409691661596298, Final Batch Loss: 0.08947557955980301\n",
      "Epoch 2403, Loss: 0.2961895316839218, Final Batch Loss: 0.20155709981918335\n",
      "Epoch 2404, Loss: 0.2624482288956642, Final Batch Loss: 0.15240904688835144\n",
      "Epoch 2405, Loss: 0.2121008113026619, Final Batch Loss: 0.08371227234601974\n",
      "Epoch 2406, Loss: 0.22394997626543045, Final Batch Loss: 0.11842431128025055\n",
      "Epoch 2407, Loss: 0.17572782933712006, Final Batch Loss: 0.09686142951250076\n",
      "Epoch 2408, Loss: 0.27454470843076706, Final Batch Loss: 0.11799014359712601\n",
      "Epoch 2409, Loss: 0.3355838656425476, Final Batch Loss: 0.18959280848503113\n",
      "Epoch 2410, Loss: 0.30312924087047577, Final Batch Loss: 0.1832653433084488\n",
      "Epoch 2411, Loss: 0.23853403329849243, Final Batch Loss: 0.13923662900924683\n",
      "Epoch 2412, Loss: 0.27944330871105194, Final Batch Loss: 0.11949948966503143\n",
      "Epoch 2413, Loss: 0.26362035423517227, Final Batch Loss: 0.1425265669822693\n",
      "Epoch 2414, Loss: 0.25019606202840805, Final Batch Loss: 0.11520866304636002\n",
      "Epoch 2415, Loss: 0.2702149450778961, Final Batch Loss: 0.1345502883195877\n",
      "Epoch 2416, Loss: 0.27512212097644806, Final Batch Loss: 0.12789298593997955\n",
      "Epoch 2417, Loss: 0.1768162101507187, Final Batch Loss: 0.1081254631280899\n",
      "Epoch 2418, Loss: 0.18146146833896637, Final Batch Loss: 0.10806222259998322\n",
      "Epoch 2419, Loss: 0.30473779141902924, Final Batch Loss: 0.1640656292438507\n",
      "Epoch 2420, Loss: 0.21010154485702515, Final Batch Loss: 0.08957842737436295\n",
      "Epoch 2421, Loss: 0.3901195824146271, Final Batch Loss: 0.2927498519420624\n",
      "Epoch 2422, Loss: 0.24507582932710648, Final Batch Loss: 0.13722731173038483\n",
      "Epoch 2423, Loss: 0.24446704238653183, Final Batch Loss: 0.1299503743648529\n",
      "Epoch 2424, Loss: 0.22600491344928741, Final Batch Loss: 0.10793990641832352\n",
      "Epoch 2425, Loss: 0.20515179634094238, Final Batch Loss: 0.07119916379451752\n",
      "Epoch 2426, Loss: 0.3530660942196846, Final Batch Loss: 0.11329180747270584\n",
      "Epoch 2427, Loss: 0.30184856057167053, Final Batch Loss: 0.14406609535217285\n",
      "Epoch 2428, Loss: 0.22549369931221008, Final Batch Loss: 0.09559331834316254\n",
      "Epoch 2429, Loss: 0.20418140292167664, Final Batch Loss: 0.09031731635332108\n",
      "Epoch 2430, Loss: 0.20845435559749603, Final Batch Loss: 0.07106007635593414\n",
      "Epoch 2431, Loss: 0.2856057211756706, Final Batch Loss: 0.17220929265022278\n",
      "Epoch 2432, Loss: 0.22741500288248062, Final Batch Loss: 0.10592959821224213\n",
      "Epoch 2433, Loss: 0.14782605320215225, Final Batch Loss: 0.061183035373687744\n",
      "Epoch 2434, Loss: 0.23871571570634842, Final Batch Loss: 0.09208550304174423\n",
      "Epoch 2435, Loss: 0.24391720443964005, Final Batch Loss: 0.12853915989398956\n",
      "Epoch 2436, Loss: 0.28572534769773483, Final Batch Loss: 0.10652147978544235\n",
      "Epoch 2437, Loss: 0.17296205461025238, Final Batch Loss: 0.08273311704397202\n",
      "Epoch 2438, Loss: 0.1925017312169075, Final Batch Loss: 0.08147256821393967\n",
      "Epoch 2439, Loss: 0.2358206883072853, Final Batch Loss: 0.08114730566740036\n",
      "Epoch 2440, Loss: 0.28638264536857605, Final Batch Loss: 0.15660245716571808\n",
      "Epoch 2441, Loss: 0.35875098407268524, Final Batch Loss: 0.25570905208587646\n",
      "Epoch 2442, Loss: 0.3057181239128113, Final Batch Loss: 0.16855749487876892\n",
      "Epoch 2443, Loss: 0.24605678021907806, Final Batch Loss: 0.1085084080696106\n",
      "Epoch 2444, Loss: 0.23939725011587143, Final Batch Loss: 0.11846649646759033\n",
      "Epoch 2445, Loss: 0.2331215739250183, Final Batch Loss: 0.12528565526008606\n",
      "Epoch 2446, Loss: 0.2885420620441437, Final Batch Loss: 0.12612390518188477\n",
      "Epoch 2447, Loss: 0.16909845918416977, Final Batch Loss: 0.06714477390050888\n",
      "Epoch 2448, Loss: 0.24987760186195374, Final Batch Loss: 0.16743534803390503\n",
      "Epoch 2449, Loss: 0.27322834730148315, Final Batch Loss: 0.14568565785884857\n",
      "Epoch 2450, Loss: 0.2304169461131096, Final Batch Loss: 0.07451575249433517\n",
      "Epoch 2451, Loss: 0.2463519349694252, Final Batch Loss: 0.16886256635189056\n",
      "Epoch 2452, Loss: 0.2382909134030342, Final Batch Loss: 0.1091446653008461\n",
      "Epoch 2453, Loss: 0.23905281722545624, Final Batch Loss: 0.13713541626930237\n",
      "Epoch 2454, Loss: 0.22025133669376373, Final Batch Loss: 0.07445290684700012\n",
      "Epoch 2455, Loss: 0.28205692768096924, Final Batch Loss: 0.06237995624542236\n",
      "Epoch 2456, Loss: 0.23642345517873764, Final Batch Loss: 0.12822571396827698\n",
      "Epoch 2457, Loss: 0.20381203293800354, Final Batch Loss: 0.13068729639053345\n",
      "Epoch 2458, Loss: 0.2047562673687935, Final Batch Loss: 0.09448768198490143\n",
      "Epoch 2459, Loss: 0.2938936874270439, Final Batch Loss: 0.0979446992278099\n",
      "Epoch 2460, Loss: 0.20708740502595901, Final Batch Loss: 0.07588621228933334\n",
      "Epoch 2461, Loss: 0.20281747728586197, Final Batch Loss: 0.09517250210046768\n",
      "Epoch 2462, Loss: 0.2575484961271286, Final Batch Loss: 0.08745098114013672\n",
      "Epoch 2463, Loss: 0.24504103511571884, Final Batch Loss: 0.1302749216556549\n",
      "Epoch 2464, Loss: 0.17874570935964584, Final Batch Loss: 0.06521837413311005\n",
      "Epoch 2465, Loss: 0.1868458166718483, Final Batch Loss: 0.08799019455909729\n",
      "Epoch 2466, Loss: 0.28518304973840714, Final Batch Loss: 0.20420919358730316\n",
      "Epoch 2467, Loss: 0.23434274643659592, Final Batch Loss: 0.13686776161193848\n",
      "Epoch 2468, Loss: 0.19147823005914688, Final Batch Loss: 0.10335328429937363\n",
      "Epoch 2469, Loss: 0.27426211535930634, Final Batch Loss: 0.15664145350456238\n",
      "Epoch 2470, Loss: 0.22268445044755936, Final Batch Loss: 0.1383485049009323\n",
      "Epoch 2471, Loss: 0.26830635964870453, Final Batch Loss: 0.17935840785503387\n",
      "Epoch 2472, Loss: 0.31787071377038956, Final Batch Loss: 0.2217741310596466\n",
      "Epoch 2473, Loss: 0.28068868815898895, Final Batch Loss: 0.17244410514831543\n",
      "Epoch 2474, Loss: 0.2008470594882965, Final Batch Loss: 0.0798075869679451\n",
      "Epoch 2475, Loss: 0.20899378508329391, Final Batch Loss: 0.0922681987285614\n",
      "Epoch 2476, Loss: 0.17861872166395187, Final Batch Loss: 0.06824441254138947\n",
      "Epoch 2477, Loss: 0.35050684213638306, Final Batch Loss: 0.13806751370429993\n",
      "Epoch 2478, Loss: 0.2655264139175415, Final Batch Loss: 0.1270190328359604\n",
      "Epoch 2479, Loss: 0.2549576386809349, Final Batch Loss: 0.13484416902065277\n",
      "Epoch 2480, Loss: 0.25714533776044846, Final Batch Loss: 0.12370031327009201\n",
      "Epoch 2481, Loss: 0.23886922001838684, Final Batch Loss: 0.1315132975578308\n",
      "Epoch 2482, Loss: 0.22057390958070755, Final Batch Loss: 0.11244712769985199\n",
      "Epoch 2483, Loss: 0.19259384274482727, Final Batch Loss: 0.10035073012113571\n",
      "Epoch 2484, Loss: 0.2332642450928688, Final Batch Loss: 0.14043028652668\n",
      "Epoch 2485, Loss: 0.23925750702619553, Final Batch Loss: 0.12697990238666534\n",
      "Epoch 2486, Loss: 0.25467459857463837, Final Batch Loss: 0.14888276159763336\n",
      "Epoch 2487, Loss: 0.21718482673168182, Final Batch Loss: 0.07182475924491882\n",
      "Epoch 2488, Loss: 0.27977149933576584, Final Batch Loss: 0.17149165272712708\n",
      "Epoch 2489, Loss: 0.17256832122802734, Final Batch Loss: 0.07357101142406464\n",
      "Epoch 2490, Loss: 0.2252417653799057, Final Batch Loss: 0.1336388885974884\n",
      "Epoch 2491, Loss: 0.20671936869621277, Final Batch Loss: 0.09172698110342026\n",
      "Epoch 2492, Loss: 0.17779509350657463, Final Batch Loss: 0.05895565077662468\n",
      "Epoch 2493, Loss: 0.2523527070879936, Final Batch Loss: 0.13531669974327087\n",
      "Epoch 2494, Loss: 0.3473638892173767, Final Batch Loss: 0.14576199650764465\n",
      "Epoch 2495, Loss: 0.31811466813087463, Final Batch Loss: 0.06628835201263428\n",
      "Epoch 2496, Loss: 0.29392245411872864, Final Batch Loss: 0.13648146390914917\n",
      "Epoch 2497, Loss: 0.2121633142232895, Final Batch Loss: 0.09045165032148361\n",
      "Epoch 2498, Loss: 0.24031221866607666, Final Batch Loss: 0.11154064536094666\n",
      "Epoch 2499, Loss: 0.18619775772094727, Final Batch Loss: 0.08956696838140488\n",
      "Epoch 2500, Loss: 0.1586882546544075, Final Batch Loss: 0.06957883387804031\n",
      "Epoch 2501, Loss: 0.1661594659090042, Final Batch Loss: 0.054806195199489594\n",
      "Epoch 2502, Loss: 0.2601901516318321, Final Batch Loss: 0.12385500222444534\n",
      "Epoch 2503, Loss: 0.21935658901929855, Final Batch Loss: 0.0902244970202446\n",
      "Epoch 2504, Loss: 0.2245987355709076, Final Batch Loss: 0.12622663378715515\n",
      "Epoch 2505, Loss: 0.28717802464962006, Final Batch Loss: 0.1091008186340332\n",
      "Epoch 2506, Loss: 0.2692079544067383, Final Batch Loss: 0.130395770072937\n",
      "Epoch 2507, Loss: 0.23120705783367157, Final Batch Loss: 0.1082020178437233\n",
      "Epoch 2508, Loss: 0.26419026404619217, Final Batch Loss: 0.1184011921286583\n",
      "Epoch 2509, Loss: 0.2162085473537445, Final Batch Loss: 0.09780475497245789\n",
      "Epoch 2510, Loss: 0.18646714836359024, Final Batch Loss: 0.08058799803256989\n",
      "Epoch 2511, Loss: 0.30011649429798126, Final Batch Loss: 0.18266849219799042\n",
      "Epoch 2512, Loss: 0.2732892334461212, Final Batch Loss: 0.1556742638349533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2513, Loss: 0.1805567443370819, Final Batch Loss: 0.11015334725379944\n",
      "Epoch 2514, Loss: 0.1961025521159172, Final Batch Loss: 0.10072626918554306\n",
      "Epoch 2515, Loss: 0.17518243193626404, Final Batch Loss: 0.08030883967876434\n",
      "Epoch 2516, Loss: 0.22451546043157578, Final Batch Loss: 0.1274295300245285\n",
      "Epoch 2517, Loss: 0.13458111137151718, Final Batch Loss: 0.0658193826675415\n",
      "Epoch 2518, Loss: 0.2619802951812744, Final Batch Loss: 0.13452067971229553\n",
      "Epoch 2519, Loss: 0.2562977895140648, Final Batch Loss: 0.15354667603969574\n",
      "Epoch 2520, Loss: 0.2344103306531906, Final Batch Loss: 0.09888766705989838\n",
      "Epoch 2521, Loss: 0.22491998225450516, Final Batch Loss: 0.11177676171064377\n",
      "Epoch 2522, Loss: 0.38835854828357697, Final Batch Loss: 0.20001356303691864\n",
      "Epoch 2523, Loss: 0.39830130338668823, Final Batch Loss: 0.2532084286212921\n",
      "Epoch 2524, Loss: 0.2150319144129753, Final Batch Loss: 0.12375232577323914\n",
      "Epoch 2525, Loss: 0.2339286506175995, Final Batch Loss: 0.08636310696601868\n",
      "Epoch 2526, Loss: 0.20680325478315353, Final Batch Loss: 0.10498924553394318\n",
      "Epoch 2527, Loss: 0.2645270749926567, Final Batch Loss: 0.07286161929368973\n",
      "Epoch 2528, Loss: 0.23922758549451828, Final Batch Loss: 0.11337462812662125\n",
      "Epoch 2529, Loss: 0.20579053461551666, Final Batch Loss: 0.11436622589826584\n",
      "Epoch 2530, Loss: 0.2656862214207649, Final Batch Loss: 0.14851120114326477\n",
      "Epoch 2531, Loss: 0.27949368953704834, Final Batch Loss: 0.1638549417257309\n",
      "Epoch 2532, Loss: 0.2550985589623451, Final Batch Loss: 0.13154424726963043\n",
      "Epoch 2533, Loss: 0.18979406356811523, Final Batch Loss: 0.07696268707513809\n",
      "Epoch 2534, Loss: 0.24565411359071732, Final Batch Loss: 0.10800113528966904\n",
      "Epoch 2535, Loss: 0.21013910323381424, Final Batch Loss: 0.061170242726802826\n",
      "Epoch 2536, Loss: 0.2567334920167923, Final Batch Loss: 0.12098424136638641\n",
      "Epoch 2537, Loss: 0.24718712270259857, Final Batch Loss: 0.1445816159248352\n",
      "Epoch 2538, Loss: 0.27376896142959595, Final Batch Loss: 0.13492925465106964\n",
      "Epoch 2539, Loss: 0.18164795637130737, Final Batch Loss: 0.07894544303417206\n",
      "Epoch 2540, Loss: 0.23370623588562012, Final Batch Loss: 0.09149405360221863\n",
      "Epoch 2541, Loss: 0.19569510966539383, Final Batch Loss: 0.0725615918636322\n",
      "Epoch 2542, Loss: 0.2459755837917328, Final Batch Loss: 0.1363728791475296\n",
      "Epoch 2543, Loss: 0.1912698894739151, Final Batch Loss: 0.10544546693563461\n",
      "Epoch 2544, Loss: 0.26599713414907455, Final Batch Loss: 0.1219206377863884\n",
      "Epoch 2545, Loss: 0.3560820370912552, Final Batch Loss: 0.18150518834590912\n",
      "Epoch 2546, Loss: 0.2039090245962143, Final Batch Loss: 0.1454111784696579\n",
      "Epoch 2547, Loss: 0.22476312518119812, Final Batch Loss: 0.07149498164653778\n",
      "Epoch 2548, Loss: 0.20070874691009521, Final Batch Loss: 0.11508707702159882\n",
      "Epoch 2549, Loss: 0.35005009174346924, Final Batch Loss: 0.10115635395050049\n",
      "Epoch 2550, Loss: 0.16721520572900772, Final Batch Loss: 0.09360002726316452\n",
      "Epoch 2551, Loss: 0.21368330717086792, Final Batch Loss: 0.09006238728761673\n",
      "Epoch 2552, Loss: 0.1696024239063263, Final Batch Loss: 0.10177852213382721\n",
      "Epoch 2553, Loss: 0.19328903406858444, Final Batch Loss: 0.11342105269432068\n",
      "Epoch 2554, Loss: 0.22867294400930405, Final Batch Loss: 0.08634638041257858\n",
      "Epoch 2555, Loss: 0.20358913391828537, Final Batch Loss: 0.0953492522239685\n",
      "Epoch 2556, Loss: 0.22487401217222214, Final Batch Loss: 0.10231250524520874\n",
      "Epoch 2557, Loss: 0.16399303823709488, Final Batch Loss: 0.07754722237586975\n",
      "Epoch 2558, Loss: 0.20231902599334717, Final Batch Loss: 0.07294340431690216\n",
      "Epoch 2559, Loss: 0.19361281394958496, Final Batch Loss: 0.06578357517719269\n",
      "Epoch 2560, Loss: 0.18699589371681213, Final Batch Loss: 0.11237896233797073\n",
      "Epoch 2561, Loss: 0.17474213987588882, Final Batch Loss: 0.053701549768447876\n",
      "Epoch 2562, Loss: 0.2562440261244774, Final Batch Loss: 0.14799687266349792\n",
      "Epoch 2563, Loss: 0.30436472594738007, Final Batch Loss: 0.1839199811220169\n",
      "Epoch 2564, Loss: 0.28076331317424774, Final Batch Loss: 0.1830909103155136\n",
      "Epoch 2565, Loss: 0.2585645541548729, Final Batch Loss: 0.120558001101017\n",
      "Epoch 2566, Loss: 0.15287386626005173, Final Batch Loss: 0.07884237170219421\n",
      "Epoch 2567, Loss: 0.19622378051280975, Final Batch Loss: 0.07694024592638016\n",
      "Epoch 2568, Loss: 0.2593834698200226, Final Batch Loss: 0.09879517555236816\n",
      "Epoch 2569, Loss: 0.20342347025871277, Final Batch Loss: 0.08260144293308258\n",
      "Epoch 2570, Loss: 0.14597270637750626, Final Batch Loss: 0.07114659994840622\n",
      "Epoch 2571, Loss: 0.22013038396835327, Final Batch Loss: 0.09353913366794586\n",
      "Epoch 2572, Loss: 0.19890744239091873, Final Batch Loss: 0.09356410801410675\n",
      "Epoch 2573, Loss: 0.16873273253440857, Final Batch Loss: 0.09107010066509247\n",
      "Epoch 2574, Loss: 0.24699907004833221, Final Batch Loss: 0.09922458231449127\n",
      "Epoch 2575, Loss: 0.16279209405183792, Final Batch Loss: 0.07198876887559891\n",
      "Epoch 2576, Loss: 0.18812159448862076, Final Batch Loss: 0.103216752409935\n",
      "Epoch 2577, Loss: 0.18265602737665176, Final Batch Loss: 0.07410969585180283\n",
      "Epoch 2578, Loss: 0.2850734069943428, Final Batch Loss: 0.19566388428211212\n",
      "Epoch 2579, Loss: 0.2570207193493843, Final Batch Loss: 0.16893579065799713\n",
      "Epoch 2580, Loss: 0.37863290309906006, Final Batch Loss: 0.11577171087265015\n",
      "Epoch 2581, Loss: 0.22508954256772995, Final Batch Loss: 0.10757773369550705\n",
      "Epoch 2582, Loss: 0.25512877851724625, Final Batch Loss: 0.09280136972665787\n",
      "Epoch 2583, Loss: 0.18577411770820618, Final Batch Loss: 0.07491446286439896\n",
      "Epoch 2584, Loss: 0.24192752689123154, Final Batch Loss: 0.12175934761762619\n",
      "Epoch 2585, Loss: 0.20527493208646774, Final Batch Loss: 0.09731604903936386\n",
      "Epoch 2586, Loss: 0.19604700803756714, Final Batch Loss: 0.06788808107376099\n",
      "Epoch 2587, Loss: 0.1497814878821373, Final Batch Loss: 0.06785384565591812\n",
      "Epoch 2588, Loss: 0.20151164382696152, Final Batch Loss: 0.09074052423238754\n",
      "Epoch 2589, Loss: 0.1672680824995041, Final Batch Loss: 0.08740498125553131\n",
      "Epoch 2590, Loss: 0.20204389095306396, Final Batch Loss: 0.10101322084665298\n",
      "Epoch 2591, Loss: 0.20648143440485, Final Batch Loss: 0.08052065223455429\n",
      "Epoch 2592, Loss: 0.21886324882507324, Final Batch Loss: 0.14199501276016235\n",
      "Epoch 2593, Loss: 0.1857798770070076, Final Batch Loss: 0.08926638960838318\n",
      "Epoch 2594, Loss: 0.25799423456192017, Final Batch Loss: 0.12301063537597656\n",
      "Epoch 2595, Loss: 0.2091158777475357, Final Batch Loss: 0.08595003187656403\n",
      "Epoch 2596, Loss: 0.26623691618442535, Final Batch Loss: 0.15182732045650482\n",
      "Epoch 2597, Loss: 0.278515949845314, Final Batch Loss: 0.1673857420682907\n",
      "Epoch 2598, Loss: 0.3001148849725723, Final Batch Loss: 0.15408538281917572\n",
      "Epoch 2599, Loss: 0.24627453088760376, Final Batch Loss: 0.10967765748500824\n",
      "Epoch 2600, Loss: 0.29365570843219757, Final Batch Loss: 0.18624471127986908\n",
      "Epoch 2601, Loss: 0.20175337046384811, Final Batch Loss: 0.1212766170501709\n",
      "Epoch 2602, Loss: 0.2733165845274925, Final Batch Loss: 0.17393074929714203\n",
      "Epoch 2603, Loss: 0.2081906720995903, Final Batch Loss: 0.10168147087097168\n",
      "Epoch 2604, Loss: 0.19035059213638306, Final Batch Loss: 0.08338093757629395\n",
      "Epoch 2605, Loss: 0.23912320286035538, Final Batch Loss: 0.09507530182600021\n",
      "Epoch 2606, Loss: 0.2021266296505928, Final Batch Loss: 0.1250832974910736\n",
      "Epoch 2607, Loss: 0.1850987821817398, Final Batch Loss: 0.06055165082216263\n",
      "Epoch 2608, Loss: 0.1370159387588501, Final Batch Loss: 0.05015435814857483\n",
      "Epoch 2609, Loss: 0.19087204337120056, Final Batch Loss: 0.09653089940547943\n",
      "Epoch 2610, Loss: 0.15245603770017624, Final Batch Loss: 0.07922845333814621\n",
      "Epoch 2611, Loss: 0.21529792994260788, Final Batch Loss: 0.09957097470760345\n",
      "Epoch 2612, Loss: 0.24317695945501328, Final Batch Loss: 0.15250539779663086\n",
      "Epoch 2613, Loss: 0.23011507838964462, Final Batch Loss: 0.10904999822378159\n",
      "Epoch 2614, Loss: 0.21146561950445175, Final Batch Loss: 0.09360503405332565\n",
      "Epoch 2615, Loss: 0.18868105858564377, Final Batch Loss: 0.09119392186403275\n",
      "Epoch 2616, Loss: 0.20237623155117035, Final Batch Loss: 0.10984671860933304\n",
      "Epoch 2617, Loss: 0.19127488881349564, Final Batch Loss: 0.12030728906393051\n",
      "Epoch 2618, Loss: 0.17800545319914818, Final Batch Loss: 0.05632611736655235\n",
      "Epoch 2619, Loss: 0.2541772201657295, Final Batch Loss: 0.06876751035451889\n",
      "Epoch 2620, Loss: 0.2179240733385086, Final Batch Loss: 0.11081100255250931\n",
      "Epoch 2621, Loss: 0.15468914806842804, Final Batch Loss: 0.07019238919019699\n",
      "Epoch 2622, Loss: 0.2488541081547737, Final Batch Loss: 0.1205517128109932\n",
      "Epoch 2623, Loss: 0.20508016645908356, Final Batch Loss: 0.09313429147005081\n",
      "Epoch 2624, Loss: 0.17952131479978561, Final Batch Loss: 0.09015488624572754\n",
      "Epoch 2625, Loss: 0.23626962304115295, Final Batch Loss: 0.10623817145824432\n",
      "Epoch 2626, Loss: 0.21055041998624802, Final Batch Loss: 0.13819144666194916\n",
      "Epoch 2627, Loss: 0.17194786667823792, Final Batch Loss: 0.0652613639831543\n",
      "Epoch 2628, Loss: 0.1845632940530777, Final Batch Loss: 0.06945717334747314\n",
      "Epoch 2629, Loss: 0.17430837452411652, Final Batch Loss: 0.08662909269332886\n",
      "Epoch 2630, Loss: 0.2306525632739067, Final Batch Loss: 0.08835770934820175\n",
      "Epoch 2631, Loss: 0.20236413925886154, Final Batch Loss: 0.11128973960876465\n",
      "Epoch 2632, Loss: 0.19322513043880463, Final Batch Loss: 0.0675865113735199\n",
      "Epoch 2633, Loss: 0.2804586887359619, Final Batch Loss: 0.11115249991416931\n",
      "Epoch 2634, Loss: 0.21697186678647995, Final Batch Loss: 0.11451664566993713\n",
      "Epoch 2635, Loss: 0.28913913667201996, Final Batch Loss: 0.15917688608169556\n",
      "Epoch 2636, Loss: 0.19296609610319138, Final Batch Loss: 0.05237998813390732\n",
      "Epoch 2637, Loss: 0.2806909829378128, Final Batch Loss: 0.13829532265663147\n",
      "Epoch 2638, Loss: 0.18782229721546173, Final Batch Loss: 0.10963402688503265\n",
      "Epoch 2639, Loss: 0.17879635840654373, Final Batch Loss: 0.09711703658103943\n",
      "Epoch 2640, Loss: 0.28336216509342194, Final Batch Loss: 0.15103337168693542\n",
      "Epoch 2641, Loss: 0.20692532509565353, Final Batch Loss: 0.08882424980401993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2642, Loss: 0.19385096430778503, Final Batch Loss: 0.09651415795087814\n",
      "Epoch 2643, Loss: 0.2485039308667183, Final Batch Loss: 0.17772355675697327\n",
      "Epoch 2644, Loss: 0.18816955387592316, Final Batch Loss: 0.09852606803178787\n",
      "Epoch 2645, Loss: 0.3032310903072357, Final Batch Loss: 0.14216288924217224\n",
      "Epoch 2646, Loss: 0.3115006610751152, Final Batch Loss: 0.09721928089857101\n",
      "Epoch 2647, Loss: 0.22025872766971588, Final Batch Loss: 0.14639398455619812\n",
      "Epoch 2648, Loss: 0.17717669904232025, Final Batch Loss: 0.08844488114118576\n",
      "Epoch 2649, Loss: 0.18900080025196075, Final Batch Loss: 0.08469972759485245\n",
      "Epoch 2650, Loss: 0.1950763389468193, Final Batch Loss: 0.10060130059719086\n",
      "Epoch 2651, Loss: 0.1948460340499878, Final Batch Loss: 0.06330828368663788\n",
      "Epoch 2652, Loss: 0.16703563183546066, Final Batch Loss: 0.09672898799180984\n",
      "Epoch 2653, Loss: 0.2923914045095444, Final Batch Loss: 0.20048339664936066\n",
      "Epoch 2654, Loss: 0.24009987711906433, Final Batch Loss: 0.11593467742204666\n",
      "Epoch 2655, Loss: 0.17956314235925674, Final Batch Loss: 0.07414036989212036\n",
      "Epoch 2656, Loss: 0.232541985809803, Final Batch Loss: 0.06936240941286087\n",
      "Epoch 2657, Loss: 0.30495384335517883, Final Batch Loss: 0.157770574092865\n",
      "Epoch 2658, Loss: 0.2543211281299591, Final Batch Loss: 0.14629791676998138\n",
      "Epoch 2659, Loss: 0.19072765111923218, Final Batch Loss: 0.09714671969413757\n",
      "Epoch 2660, Loss: 0.26067857444286346, Final Batch Loss: 0.13372568786144257\n",
      "Epoch 2661, Loss: 0.13985539600253105, Final Batch Loss: 0.056543122977018356\n",
      "Epoch 2662, Loss: 0.2109246551990509, Final Batch Loss: 0.10793834924697876\n",
      "Epoch 2663, Loss: 0.23640119284391403, Final Batch Loss: 0.15334558486938477\n",
      "Epoch 2664, Loss: 0.2180953472852707, Final Batch Loss: 0.07504993677139282\n",
      "Epoch 2665, Loss: 0.21076875925064087, Final Batch Loss: 0.09480276703834534\n",
      "Epoch 2666, Loss: 0.15427659451961517, Final Batch Loss: 0.07554834336042404\n",
      "Epoch 2667, Loss: 0.23920151591300964, Final Batch Loss: 0.13548938930034637\n",
      "Epoch 2668, Loss: 0.21124869585037231, Final Batch Loss: 0.08397848904132843\n",
      "Epoch 2669, Loss: 0.236221045255661, Final Batch Loss: 0.12890025973320007\n",
      "Epoch 2670, Loss: 0.18528533354401588, Final Batch Loss: 0.05740617588162422\n",
      "Epoch 2671, Loss: 0.20571042597293854, Final Batch Loss: 0.1010737195611\n",
      "Epoch 2672, Loss: 0.18897051364183426, Final Batch Loss: 0.08358905464410782\n",
      "Epoch 2673, Loss: 0.23940744996070862, Final Batch Loss: 0.15408404171466827\n",
      "Epoch 2674, Loss: 0.13222768157720566, Final Batch Loss: 0.08420571684837341\n",
      "Epoch 2675, Loss: 0.16756516695022583, Final Batch Loss: 0.09085889905691147\n",
      "Epoch 2676, Loss: 0.13380173966288567, Final Batch Loss: 0.05776866152882576\n",
      "Epoch 2677, Loss: 0.3244139477610588, Final Batch Loss: 0.2571743130683899\n",
      "Epoch 2678, Loss: 0.17549315840005875, Final Batch Loss: 0.049228735268116\n",
      "Epoch 2679, Loss: 0.3359719067811966, Final Batch Loss: 0.1960279643535614\n",
      "Epoch 2680, Loss: 0.22295163571834564, Final Batch Loss: 0.128702774643898\n",
      "Epoch 2681, Loss: 0.21720048785209656, Final Batch Loss: 0.08322012424468994\n",
      "Epoch 2682, Loss: 0.25417812913656235, Final Batch Loss: 0.15357643365859985\n",
      "Epoch 2683, Loss: 0.23367946594953537, Final Batch Loss: 0.1526525616645813\n",
      "Epoch 2684, Loss: 0.20533884316682816, Final Batch Loss: 0.07478589564561844\n",
      "Epoch 2685, Loss: 0.23183325678110123, Final Batch Loss: 0.1506756842136383\n",
      "Epoch 2686, Loss: 0.21885830909013748, Final Batch Loss: 0.10836532711982727\n",
      "Epoch 2687, Loss: 0.27996891736984253, Final Batch Loss: 0.17466266453266144\n",
      "Epoch 2688, Loss: 0.18412183970212936, Final Batch Loss: 0.09644502401351929\n",
      "Epoch 2689, Loss: 0.23473522812128067, Final Batch Loss: 0.15940088033676147\n",
      "Epoch 2690, Loss: 0.31685100495815277, Final Batch Loss: 0.1366516798734665\n",
      "Epoch 2691, Loss: 0.19868631660938263, Final Batch Loss: 0.07791323959827423\n",
      "Epoch 2692, Loss: 0.1862761452794075, Final Batch Loss: 0.08372616022825241\n",
      "Epoch 2693, Loss: 0.2373156026005745, Final Batch Loss: 0.14495012164115906\n",
      "Epoch 2694, Loss: 0.21314087510108948, Final Batch Loss: 0.09791047871112823\n",
      "Epoch 2695, Loss: 0.20296835899353027, Final Batch Loss: 0.10603336244821548\n",
      "Epoch 2696, Loss: 0.16857562214136124, Final Batch Loss: 0.06377054750919342\n",
      "Epoch 2697, Loss: 0.12383095920085907, Final Batch Loss: 0.05897337943315506\n",
      "Epoch 2698, Loss: 0.22079265117645264, Final Batch Loss: 0.11172652244567871\n",
      "Epoch 2699, Loss: 0.16208640486001968, Final Batch Loss: 0.09594646841287613\n",
      "Epoch 2700, Loss: 0.2125122770667076, Final Batch Loss: 0.09551193565130234\n",
      "Epoch 2701, Loss: 0.18231311812996864, Final Batch Loss: 0.054378438740968704\n",
      "Epoch 2702, Loss: 0.304468996822834, Final Batch Loss: 0.18990132212638855\n",
      "Epoch 2703, Loss: 0.2788946330547333, Final Batch Loss: 0.1421753168106079\n",
      "Epoch 2704, Loss: 0.22015322744846344, Final Batch Loss: 0.09185878932476044\n",
      "Epoch 2705, Loss: 0.24571221321821213, Final Batch Loss: 0.08181161433458328\n",
      "Epoch 2706, Loss: 0.16798212379217148, Final Batch Loss: 0.06320712715387344\n",
      "Epoch 2707, Loss: 0.1876218318939209, Final Batch Loss: 0.09744193404912949\n",
      "Epoch 2708, Loss: 0.1474977806210518, Final Batch Loss: 0.06734796613454819\n",
      "Epoch 2709, Loss: 0.21110908687114716, Final Batch Loss: 0.11173249036073685\n",
      "Epoch 2710, Loss: 0.22045842558145523, Final Batch Loss: 0.10246017575263977\n",
      "Epoch 2711, Loss: 0.23189076036214828, Final Batch Loss: 0.14760632812976837\n",
      "Epoch 2712, Loss: 0.22326836735010147, Final Batch Loss: 0.08252065628767014\n",
      "Epoch 2713, Loss: 0.17837241291999817, Final Batch Loss: 0.11030956357717514\n",
      "Epoch 2714, Loss: 0.1620497703552246, Final Batch Loss: 0.07204440981149673\n",
      "Epoch 2715, Loss: 0.1661589965224266, Final Batch Loss: 0.09445798397064209\n",
      "Epoch 2716, Loss: 0.1787063479423523, Final Batch Loss: 0.0943765938282013\n",
      "Epoch 2717, Loss: 0.16459453105926514, Final Batch Loss: 0.0778772160410881\n",
      "Epoch 2718, Loss: 0.1891031190752983, Final Batch Loss: 0.09440524876117706\n",
      "Epoch 2719, Loss: 0.2196640819311142, Final Batch Loss: 0.1252860277891159\n",
      "Epoch 2720, Loss: 0.29397767782211304, Final Batch Loss: 0.17591425776481628\n",
      "Epoch 2721, Loss: 0.30162225663661957, Final Batch Loss: 0.15094803273677826\n",
      "Epoch 2722, Loss: 0.21628975868225098, Final Batch Loss: 0.11300580948591232\n",
      "Epoch 2723, Loss: 0.191118985414505, Final Batch Loss: 0.102175273001194\n",
      "Epoch 2724, Loss: 0.2982359379529953, Final Batch Loss: 0.1709628403186798\n",
      "Epoch 2725, Loss: 0.21884247660636902, Final Batch Loss: 0.1188143640756607\n",
      "Epoch 2726, Loss: 0.16883688047528267, Final Batch Loss: 0.0436534620821476\n",
      "Epoch 2727, Loss: 0.1739151030778885, Final Batch Loss: 0.08953102678060532\n",
      "Epoch 2728, Loss: 0.2558709904551506, Final Batch Loss: 0.15806016325950623\n",
      "Epoch 2729, Loss: 0.2005195915699005, Final Batch Loss: 0.09230271726846695\n",
      "Epoch 2730, Loss: 0.19201140850782394, Final Batch Loss: 0.06483719497919083\n",
      "Epoch 2731, Loss: 0.15470079332590103, Final Batch Loss: 0.04434503614902496\n",
      "Epoch 2732, Loss: 0.2168147787451744, Final Batch Loss: 0.10395625978708267\n",
      "Epoch 2733, Loss: 0.14467619359493256, Final Batch Loss: 0.07575872540473938\n",
      "Epoch 2734, Loss: 0.19816624373197556, Final Batch Loss: 0.07814047485589981\n",
      "Epoch 2735, Loss: 0.20470860600471497, Final Batch Loss: 0.13549065589904785\n",
      "Epoch 2736, Loss: 0.20759418606758118, Final Batch Loss: 0.13541176915168762\n",
      "Epoch 2737, Loss: 0.16137957572937012, Final Batch Loss: 0.08665741980075836\n",
      "Epoch 2738, Loss: 0.19344384968280792, Final Batch Loss: 0.10517868399620056\n",
      "Epoch 2739, Loss: 0.12317653745412827, Final Batch Loss: 0.05784805119037628\n",
      "Epoch 2740, Loss: 0.15457109734416008, Final Batch Loss: 0.06177162006497383\n",
      "Epoch 2741, Loss: 0.1820853278040886, Final Batch Loss: 0.0691390186548233\n",
      "Epoch 2742, Loss: 0.2335420772433281, Final Batch Loss: 0.11007190495729446\n",
      "Epoch 2743, Loss: 0.21304213255643845, Final Batch Loss: 0.13575628399848938\n",
      "Epoch 2744, Loss: 0.19770459830760956, Final Batch Loss: 0.06859919428825378\n",
      "Epoch 2745, Loss: 0.22595272213220596, Final Batch Loss: 0.10797524452209473\n",
      "Epoch 2746, Loss: 0.21287889033555984, Final Batch Loss: 0.08313708752393723\n",
      "Epoch 2747, Loss: 0.18574749678373337, Final Batch Loss: 0.0773996114730835\n",
      "Epoch 2748, Loss: 0.15204081311821938, Final Batch Loss: 0.05608995631337166\n",
      "Epoch 2749, Loss: 0.19522079080343246, Final Batch Loss: 0.08879402279853821\n",
      "Epoch 2750, Loss: 0.22375833243131638, Final Batch Loss: 0.10024671256542206\n",
      "Epoch 2751, Loss: 0.21727679669857025, Final Batch Loss: 0.10809434205293655\n",
      "Epoch 2752, Loss: 0.23721741139888763, Final Batch Loss: 0.10962545871734619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2753, Loss: 0.22958145290613174, Final Batch Loss: 0.11849664151668549\n",
      "Epoch 2754, Loss: 0.15735682472586632, Final Batch Loss: 0.05058600381016731\n",
      "Epoch 2755, Loss: 0.24518724530935287, Final Batch Loss: 0.13382914662361145\n",
      "Epoch 2756, Loss: 0.18341253697872162, Final Batch Loss: 0.08852680772542953\n",
      "Epoch 2757, Loss: 0.18541086465120316, Final Batch Loss: 0.08408351242542267\n",
      "Epoch 2758, Loss: 0.1832670271396637, Final Batch Loss: 0.08706513792276382\n",
      "Epoch 2759, Loss: 0.1924108937382698, Final Batch Loss: 0.08938456326723099\n",
      "Epoch 2760, Loss: 0.2216717153787613, Final Batch Loss: 0.1039653941988945\n",
      "Epoch 2761, Loss: 0.20154964178800583, Final Batch Loss: 0.11583361774682999\n",
      "Epoch 2762, Loss: 0.17542116343975067, Final Batch Loss: 0.06626720726490021\n",
      "Epoch 2763, Loss: 0.1750751957297325, Final Batch Loss: 0.050504133105278015\n",
      "Epoch 2764, Loss: 0.19562352448701859, Final Batch Loss: 0.08719140291213989\n",
      "Epoch 2765, Loss: 0.25061996281147003, Final Batch Loss: 0.12911495566368103\n",
      "Epoch 2766, Loss: 0.31639352440834045, Final Batch Loss: 0.17700475454330444\n",
      "Epoch 2767, Loss: 0.20216279849410057, Final Batch Loss: 0.05370024964213371\n",
      "Epoch 2768, Loss: 0.18993577361106873, Final Batch Loss: 0.09383151680231094\n",
      "Epoch 2769, Loss: 0.210463747382164, Final Batch Loss: 0.1459016352891922\n",
      "Epoch 2770, Loss: 0.16186662763357162, Final Batch Loss: 0.0469680055975914\n",
      "Epoch 2771, Loss: 0.15796370804309845, Final Batch Loss: 0.08740058541297913\n",
      "Epoch 2772, Loss: 0.17612817138433456, Final Batch Loss: 0.09960102289915085\n",
      "Epoch 2773, Loss: 0.18519984930753708, Final Batch Loss: 0.06698068231344223\n",
      "Epoch 2774, Loss: 0.23292816430330276, Final Batch Loss: 0.15199808776378632\n",
      "Epoch 2775, Loss: 0.19331098347902298, Final Batch Loss: 0.10398530215024948\n",
      "Epoch 2776, Loss: 0.12778290361166, Final Batch Loss: 0.06463516503572464\n",
      "Epoch 2777, Loss: 0.1268722079694271, Final Batch Loss: 0.05455309525132179\n",
      "Epoch 2778, Loss: 0.19989646971225739, Final Batch Loss: 0.05487775802612305\n",
      "Epoch 2779, Loss: 0.14329902082681656, Final Batch Loss: 0.07283580303192139\n",
      "Epoch 2780, Loss: 0.17070793360471725, Final Batch Loss: 0.07466544955968857\n",
      "Epoch 2781, Loss: 0.15227489173412323, Final Batch Loss: 0.07731795310974121\n",
      "Epoch 2782, Loss: 0.2034270465373993, Final Batch Loss: 0.10989688336849213\n",
      "Epoch 2783, Loss: 0.29739270359277725, Final Batch Loss: 0.21333836019039154\n",
      "Epoch 2784, Loss: 0.17975497245788574, Final Batch Loss: 0.10137871652841568\n",
      "Epoch 2785, Loss: 0.2315225824713707, Final Batch Loss: 0.08144541829824448\n",
      "Epoch 2786, Loss: 0.17793379724025726, Final Batch Loss: 0.09665060043334961\n",
      "Epoch 2787, Loss: 0.21391186118125916, Final Batch Loss: 0.06777055561542511\n",
      "Epoch 2788, Loss: 0.18552665412425995, Final Batch Loss: 0.06414652615785599\n",
      "Epoch 2789, Loss: 0.18441924452781677, Final Batch Loss: 0.08946222066879272\n",
      "Epoch 2790, Loss: 0.189908005297184, Final Batch Loss: 0.07845180481672287\n",
      "Epoch 2791, Loss: 0.1782088205218315, Final Batch Loss: 0.06534433364868164\n",
      "Epoch 2792, Loss: 0.22673390060663223, Final Batch Loss: 0.07649227231740952\n",
      "Epoch 2793, Loss: 0.20833700895309448, Final Batch Loss: 0.07941898703575134\n",
      "Epoch 2794, Loss: 0.1718621477484703, Final Batch Loss: 0.0825498104095459\n",
      "Epoch 2795, Loss: 0.1606445163488388, Final Batch Loss: 0.07089515030384064\n",
      "Epoch 2796, Loss: 0.15951184183359146, Final Batch Loss: 0.08015848696231842\n",
      "Epoch 2797, Loss: 0.17110909521579742, Final Batch Loss: 0.07889778167009354\n",
      "Epoch 2798, Loss: 0.13929606601595879, Final Batch Loss: 0.0488491989672184\n",
      "Epoch 2799, Loss: 0.1778276041150093, Final Batch Loss: 0.10578560829162598\n",
      "Epoch 2800, Loss: 0.19274388253688812, Final Batch Loss: 0.07166192680597305\n",
      "Epoch 2801, Loss: 0.2097078636288643, Final Batch Loss: 0.13387833535671234\n",
      "Epoch 2802, Loss: 0.1753760762512684, Final Batch Loss: 0.1286768764257431\n",
      "Epoch 2803, Loss: 0.20934250950813293, Final Batch Loss: 0.07901553809642792\n",
      "Epoch 2804, Loss: 0.17394941300153732, Final Batch Loss: 0.09533946216106415\n",
      "Epoch 2805, Loss: 0.1969112902879715, Final Batch Loss: 0.1261964738368988\n",
      "Epoch 2806, Loss: 0.1663949266076088, Final Batch Loss: 0.09095856547355652\n",
      "Epoch 2807, Loss: 0.2389284148812294, Final Batch Loss: 0.12150058150291443\n",
      "Epoch 2808, Loss: 0.2129836454987526, Final Batch Loss: 0.10982595384120941\n",
      "Epoch 2809, Loss: 0.14005503803491592, Final Batch Loss: 0.07192302495241165\n",
      "Epoch 2810, Loss: 0.14680121839046478, Final Batch Loss: 0.06309124082326889\n",
      "Epoch 2811, Loss: 0.1620953530073166, Final Batch Loss: 0.07293543219566345\n",
      "Epoch 2812, Loss: 0.14671014249324799, Final Batch Loss: 0.07718849927186966\n",
      "Epoch 2813, Loss: 0.17720727249979973, Final Batch Loss: 0.11537453532218933\n",
      "Epoch 2814, Loss: 0.1246882863342762, Final Batch Loss: 0.06004978343844414\n",
      "Epoch 2815, Loss: 0.2209072858095169, Final Batch Loss: 0.09062021970748901\n",
      "Epoch 2816, Loss: 0.22016190737485886, Final Batch Loss: 0.10218309611082077\n",
      "Epoch 2817, Loss: 0.17538709193468094, Final Batch Loss: 0.0771990567445755\n",
      "Epoch 2818, Loss: 0.18598107993602753, Final Batch Loss: 0.07605379819869995\n",
      "Epoch 2819, Loss: 0.3244500309228897, Final Batch Loss: 0.13523004949092865\n",
      "Epoch 2820, Loss: 0.17419478297233582, Final Batch Loss: 0.09388599544763565\n",
      "Epoch 2821, Loss: 0.29984554648399353, Final Batch Loss: 0.16546955704689026\n",
      "Epoch 2822, Loss: 0.1675494685769081, Final Batch Loss: 0.07086408138275146\n",
      "Epoch 2823, Loss: 0.27083056420087814, Final Batch Loss: 0.16504770517349243\n",
      "Epoch 2824, Loss: 0.19621047750115395, Final Batch Loss: 0.057680387049913406\n",
      "Epoch 2825, Loss: 0.21791498363018036, Final Batch Loss: 0.12009426951408386\n",
      "Epoch 2826, Loss: 0.2885942608118057, Final Batch Loss: 0.14531053602695465\n",
      "Epoch 2827, Loss: 0.1357930824160576, Final Batch Loss: 0.05096945911645889\n",
      "Epoch 2828, Loss: 0.18964431434869766, Final Batch Loss: 0.12465979158878326\n",
      "Epoch 2829, Loss: 0.17233866453170776, Final Batch Loss: 0.06650003045797348\n",
      "Epoch 2830, Loss: 0.19644758105278015, Final Batch Loss: 0.11727053672075272\n",
      "Epoch 2831, Loss: 0.17880748212337494, Final Batch Loss: 0.0847911536693573\n",
      "Epoch 2832, Loss: 0.24037767946720123, Final Batch Loss: 0.13345535099506378\n",
      "Epoch 2833, Loss: 0.19752179831266403, Final Batch Loss: 0.09257243573665619\n",
      "Epoch 2834, Loss: 0.16927826404571533, Final Batch Loss: 0.10554437339305878\n",
      "Epoch 2835, Loss: 0.1850745752453804, Final Batch Loss: 0.1011321097612381\n",
      "Epoch 2836, Loss: 0.09449243359267712, Final Batch Loss: 0.030482618138194084\n",
      "Epoch 2837, Loss: 0.2553829476237297, Final Batch Loss: 0.1206190213561058\n",
      "Epoch 2838, Loss: 0.20349590480327606, Final Batch Loss: 0.13809555768966675\n",
      "Epoch 2839, Loss: 0.25440704077482224, Final Batch Loss: 0.16653253138065338\n",
      "Epoch 2840, Loss: 0.22931096702814102, Final Batch Loss: 0.13030287623405457\n",
      "Epoch 2841, Loss: 0.1962471753358841, Final Batch Loss: 0.07286730408668518\n",
      "Epoch 2842, Loss: 0.28677958250045776, Final Batch Loss: 0.13429252803325653\n",
      "Epoch 2843, Loss: 0.20754097402095795, Final Batch Loss: 0.09158626943826675\n",
      "Epoch 2844, Loss: 0.1987602338194847, Final Batch Loss: 0.13067659735679626\n",
      "Epoch 2845, Loss: 0.2125905603170395, Final Batch Loss: 0.11686941236257553\n",
      "Epoch 2846, Loss: 0.12582451850175858, Final Batch Loss: 0.03707871586084366\n",
      "Epoch 2847, Loss: 0.22414395958185196, Final Batch Loss: 0.09957055002450943\n",
      "Epoch 2848, Loss: 0.24918071180582047, Final Batch Loss: 0.09261766821146011\n",
      "Epoch 2849, Loss: 0.24443922191858292, Final Batch Loss: 0.14333359897136688\n",
      "Epoch 2850, Loss: 0.15548761934041977, Final Batch Loss: 0.06834748387336731\n",
      "Epoch 2851, Loss: 0.2438422590494156, Final Batch Loss: 0.13114184141159058\n",
      "Epoch 2852, Loss: 0.3022717982530594, Final Batch Loss: 0.15178754925727844\n",
      "Epoch 2853, Loss: 0.21023713052272797, Final Batch Loss: 0.1434939205646515\n",
      "Epoch 2854, Loss: 0.15311678498983383, Final Batch Loss: 0.0767097920179367\n",
      "Epoch 2855, Loss: 0.19808777421712875, Final Batch Loss: 0.06280284374952316\n",
      "Epoch 2856, Loss: 0.14049968868494034, Final Batch Loss: 0.04475269466638565\n",
      "Epoch 2857, Loss: 0.14488434791564941, Final Batch Loss: 0.07391790300607681\n",
      "Epoch 2858, Loss: 0.15206757932901382, Final Batch Loss: 0.07518286257982254\n",
      "Epoch 2859, Loss: 0.29308153688907623, Final Batch Loss: 0.14076930284500122\n",
      "Epoch 2860, Loss: 0.28195253014564514, Final Batch Loss: 0.18869730830192566\n",
      "Epoch 2861, Loss: 0.2308332622051239, Final Batch Loss: 0.14602014422416687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2862, Loss: 0.2042718306183815, Final Batch Loss: 0.07871703058481216\n",
      "Epoch 2863, Loss: 0.2654201537370682, Final Batch Loss: 0.18267932534217834\n",
      "Epoch 2864, Loss: 0.20038864016532898, Final Batch Loss: 0.11302255094051361\n",
      "Epoch 2865, Loss: 0.23005378991365433, Final Batch Loss: 0.13568344712257385\n",
      "Epoch 2866, Loss: 0.2905697897076607, Final Batch Loss: 0.10507900267839432\n",
      "Epoch 2867, Loss: 0.1530928686261177, Final Batch Loss: 0.0680483877658844\n",
      "Epoch 2868, Loss: 0.1487416848540306, Final Batch Loss: 0.089309461414814\n",
      "Epoch 2869, Loss: 0.2124655544757843, Final Batch Loss: 0.0855415016412735\n",
      "Epoch 2870, Loss: 0.12636961415410042, Final Batch Loss: 0.0547197051346302\n",
      "Epoch 2871, Loss: 0.23217150568962097, Final Batch Loss: 0.107761450111866\n",
      "Epoch 2872, Loss: 0.16546116396784782, Final Batch Loss: 0.061301182955503464\n",
      "Epoch 2873, Loss: 0.2681295573711395, Final Batch Loss: 0.20399020612239838\n",
      "Epoch 2874, Loss: 0.2980288937687874, Final Batch Loss: 0.20079649984836578\n",
      "Epoch 2875, Loss: 0.20256299525499344, Final Batch Loss: 0.11008019000291824\n",
      "Epoch 2876, Loss: 0.19499749690294266, Final Batch Loss: 0.07460204511880875\n",
      "Epoch 2877, Loss: 0.26597095280885696, Final Batch Loss: 0.1590435951948166\n",
      "Epoch 2878, Loss: 0.13926350697875023, Final Batch Loss: 0.03918870911002159\n",
      "Epoch 2879, Loss: 0.20985983312129974, Final Batch Loss: 0.13482727110385895\n",
      "Epoch 2880, Loss: 0.1821587160229683, Final Batch Loss: 0.0957319438457489\n",
      "Epoch 2881, Loss: 0.1420125626027584, Final Batch Loss: 0.08170484751462936\n",
      "Epoch 2882, Loss: 0.2219042107462883, Final Batch Loss: 0.12298398464918137\n",
      "Epoch 2883, Loss: 0.1663816124200821, Final Batch Loss: 0.050706081092357635\n",
      "Epoch 2884, Loss: 0.17052330821752548, Final Batch Loss: 0.05028165876865387\n",
      "Epoch 2885, Loss: 0.0905214436352253, Final Batch Loss: 0.031044717878103256\n",
      "Epoch 2886, Loss: 0.2290206179022789, Final Batch Loss: 0.10092215985059738\n",
      "Epoch 2887, Loss: 0.16908229887485504, Final Batch Loss: 0.07121926546096802\n",
      "Epoch 2888, Loss: 0.2532081678509712, Final Batch Loss: 0.08939120918512344\n",
      "Epoch 2889, Loss: 0.2131085991859436, Final Batch Loss: 0.11641838401556015\n",
      "Epoch 2890, Loss: 0.24729204177856445, Final Batch Loss: 0.05343158543109894\n",
      "Epoch 2891, Loss: 0.22093717753887177, Final Batch Loss: 0.11296563595533371\n",
      "Epoch 2892, Loss: 0.1940956450998783, Final Batch Loss: 0.046966973692178726\n",
      "Epoch 2893, Loss: 0.3457192927598953, Final Batch Loss: 0.26527082920074463\n",
      "Epoch 2894, Loss: 0.1988043263554573, Final Batch Loss: 0.126172736287117\n",
      "Epoch 2895, Loss: 0.24783719331026077, Final Batch Loss: 0.07578263431787491\n",
      "Epoch 2896, Loss: 0.15758267045021057, Final Batch Loss: 0.07389219850301743\n",
      "Epoch 2897, Loss: 0.14580102264881134, Final Batch Loss: 0.06155553460121155\n",
      "Epoch 2898, Loss: 0.19700921326875687, Final Batch Loss: 0.06938140839338303\n",
      "Epoch 2899, Loss: 0.1169573962688446, Final Batch Loss: 0.06086075305938721\n",
      "Epoch 2900, Loss: 0.19249435514211655, Final Batch Loss: 0.11423176527023315\n",
      "Epoch 2901, Loss: 0.32647643983364105, Final Batch Loss: 0.132023885846138\n",
      "Epoch 2902, Loss: 0.1827821582555771, Final Batch Loss: 0.09071632474660873\n",
      "Epoch 2903, Loss: 0.2209468111395836, Final Batch Loss: 0.10489233583211899\n",
      "Epoch 2904, Loss: 0.16128867492079735, Final Batch Loss: 0.04710010811686516\n",
      "Epoch 2905, Loss: 0.15364304929971695, Final Batch Loss: 0.06130880117416382\n",
      "Epoch 2906, Loss: 0.16720791161060333, Final Batch Loss: 0.09169475734233856\n",
      "Epoch 2907, Loss: 0.13716663792729378, Final Batch Loss: 0.058853793889284134\n",
      "Epoch 2908, Loss: 0.17274554818868637, Final Batch Loss: 0.09788069874048233\n",
      "Epoch 2909, Loss: 0.2643855884671211, Final Batch Loss: 0.0779774859547615\n",
      "Epoch 2910, Loss: 0.24847400933504105, Final Batch Loss: 0.17857064306735992\n",
      "Epoch 2911, Loss: 0.20295341312885284, Final Batch Loss: 0.11695824563503265\n",
      "Epoch 2912, Loss: 0.2662670835852623, Final Batch Loss: 0.10021024197340012\n",
      "Epoch 2913, Loss: 0.1639508306980133, Final Batch Loss: 0.07659827917814255\n",
      "Epoch 2914, Loss: 0.21136701107025146, Final Batch Loss: 0.10204029083251953\n",
      "Epoch 2915, Loss: 0.22269799560308456, Final Batch Loss: 0.13779500126838684\n",
      "Epoch 2916, Loss: 0.20054128766059875, Final Batch Loss: 0.08603006601333618\n",
      "Epoch 2917, Loss: 0.1765664927661419, Final Batch Loss: 0.06230546906590462\n",
      "Epoch 2918, Loss: 0.16491048783063889, Final Batch Loss: 0.06063693016767502\n",
      "Epoch 2919, Loss: 0.17752088606357574, Final Batch Loss: 0.10499384999275208\n",
      "Epoch 2920, Loss: 0.19440505653619766, Final Batch Loss: 0.08801630884408951\n",
      "Epoch 2921, Loss: 0.21756599098443985, Final Batch Loss: 0.13000932335853577\n",
      "Epoch 2922, Loss: 0.2458547130227089, Final Batch Loss: 0.15772385895252228\n",
      "Epoch 2923, Loss: 0.12370286509394646, Final Batch Loss: 0.07114812731742859\n",
      "Epoch 2924, Loss: 0.2341914400458336, Final Batch Loss: 0.170617938041687\n",
      "Epoch 2925, Loss: 0.16382396966218948, Final Batch Loss: 0.09494289010763168\n",
      "Epoch 2926, Loss: 0.1809842512011528, Final Batch Loss: 0.07740466296672821\n",
      "Epoch 2927, Loss: 0.1817699745297432, Final Batch Loss: 0.06767023354768753\n",
      "Epoch 2928, Loss: 0.1759306788444519, Final Batch Loss: 0.08936040848493576\n",
      "Epoch 2929, Loss: 0.23900028318166733, Final Batch Loss: 0.14613623917102814\n",
      "Epoch 2930, Loss: 0.17375900596380234, Final Batch Loss: 0.08087145537137985\n",
      "Epoch 2931, Loss: 0.24091074988245964, Final Batch Loss: 0.058855991810560226\n",
      "Epoch 2932, Loss: 0.22540895640850067, Final Batch Loss: 0.12301617115736008\n",
      "Epoch 2933, Loss: 0.1752634346485138, Final Batch Loss: 0.06526507437229156\n",
      "Epoch 2934, Loss: 0.2136744260787964, Final Batch Loss: 0.048167526721954346\n",
      "Epoch 2935, Loss: 0.13903066515922546, Final Batch Loss: 0.06268682330846786\n",
      "Epoch 2936, Loss: 0.20926284044981003, Final Batch Loss: 0.11362297087907791\n",
      "Epoch 2937, Loss: 0.17916090786457062, Final Batch Loss: 0.1093008890748024\n",
      "Epoch 2938, Loss: 0.15642785280942917, Final Batch Loss: 0.07064735144376755\n",
      "Epoch 2939, Loss: 0.20417999476194382, Final Batch Loss: 0.11146306991577148\n",
      "Epoch 2940, Loss: 0.17359653115272522, Final Batch Loss: 0.09038935601711273\n",
      "Epoch 2941, Loss: 0.23647065460681915, Final Batch Loss: 0.14249219000339508\n",
      "Epoch 2942, Loss: 0.2047325074672699, Final Batch Loss: 0.09945596009492874\n",
      "Epoch 2943, Loss: 0.15815936774015427, Final Batch Loss: 0.06957241147756577\n",
      "Epoch 2944, Loss: 0.2160385251045227, Final Batch Loss: 0.1518690586090088\n",
      "Epoch 2945, Loss: 0.18384593725204468, Final Batch Loss: 0.08172270655632019\n",
      "Epoch 2946, Loss: 0.14344754815101624, Final Batch Loss: 0.05316093564033508\n",
      "Epoch 2947, Loss: 0.1207999475300312, Final Batch Loss: 0.05571186915040016\n",
      "Epoch 2948, Loss: 0.2378491461277008, Final Batch Loss: 0.14318449795246124\n",
      "Epoch 2949, Loss: 0.13650939241051674, Final Batch Loss: 0.041102755814790726\n",
      "Epoch 2950, Loss: 0.16527080535888672, Final Batch Loss: 0.0828642025589943\n",
      "Epoch 2951, Loss: 0.20563974976539612, Final Batch Loss: 0.11985258758068085\n",
      "Epoch 2952, Loss: 0.1852956935763359, Final Batch Loss: 0.08835223317146301\n",
      "Epoch 2953, Loss: 0.12924280762672424, Final Batch Loss: 0.04774272441864014\n",
      "Epoch 2954, Loss: 0.241801455616951, Final Batch Loss: 0.1304786503314972\n",
      "Epoch 2955, Loss: 0.23157459497451782, Final Batch Loss: 0.11523965746164322\n",
      "Epoch 2956, Loss: 0.23286916315555573, Final Batch Loss: 0.15123017132282257\n",
      "Epoch 2957, Loss: 0.16523931920528412, Final Batch Loss: 0.0956721305847168\n",
      "Epoch 2958, Loss: 0.16384904831647873, Final Batch Loss: 0.08432723581790924\n",
      "Epoch 2959, Loss: 0.18418238312005997, Final Batch Loss: 0.11660398542881012\n",
      "Epoch 2960, Loss: 0.26107580959796906, Final Batch Loss: 0.1879005879163742\n",
      "Epoch 2961, Loss: 0.19046108424663544, Final Batch Loss: 0.11325446516275406\n",
      "Epoch 2962, Loss: 0.15437351912260056, Final Batch Loss: 0.07425578683614731\n",
      "Epoch 2963, Loss: 0.20060303807258606, Final Batch Loss: 0.05711255967617035\n",
      "Epoch 2964, Loss: 0.22951529920101166, Final Batch Loss: 0.07744401693344116\n",
      "Epoch 2965, Loss: 0.19214775413274765, Final Batch Loss: 0.06697515398263931\n",
      "Epoch 2966, Loss: 0.17413880676031113, Final Batch Loss: 0.0939413532614708\n",
      "Epoch 2967, Loss: 0.1649031639099121, Final Batch Loss: 0.06300859898328781\n",
      "Epoch 2968, Loss: 0.1768466830253601, Final Batch Loss: 0.10397234559059143\n",
      "Epoch 2969, Loss: 0.2360132411122322, Final Batch Loss: 0.09954053908586502\n",
      "Epoch 2970, Loss: 0.17826693505048752, Final Batch Loss: 0.11018470674753189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2971, Loss: 0.1481446549296379, Final Batch Loss: 0.07135523855686188\n",
      "Epoch 2972, Loss: 0.2365407571196556, Final Batch Loss: 0.12082411348819733\n",
      "Epoch 2973, Loss: 0.2124728187918663, Final Batch Loss: 0.07653477042913437\n",
      "Epoch 2974, Loss: 0.20792223513126373, Final Batch Loss: 0.07263664901256561\n",
      "Epoch 2975, Loss: 0.13892917707562447, Final Batch Loss: 0.06112796440720558\n",
      "Epoch 2976, Loss: 0.15486838668584824, Final Batch Loss: 0.064334936439991\n",
      "Epoch 2977, Loss: 0.20757224410772324, Final Batch Loss: 0.08117382973432541\n",
      "Epoch 2978, Loss: 0.15506363660097122, Final Batch Loss: 0.06978810578584671\n",
      "Epoch 2979, Loss: 0.18948081880807877, Final Batch Loss: 0.11995096504688263\n",
      "Epoch 2980, Loss: 0.1622920259833336, Final Batch Loss: 0.08683909475803375\n",
      "Epoch 2981, Loss: 0.1880107745528221, Final Batch Loss: 0.07840048521757126\n",
      "Epoch 2982, Loss: 0.25963637232780457, Final Batch Loss: 0.18930748105049133\n",
      "Epoch 2983, Loss: 0.1696927733719349, Final Batch Loss: 0.052121203392744064\n",
      "Epoch 2984, Loss: 0.13804518058896065, Final Batch Loss: 0.045616161078214645\n",
      "Epoch 2985, Loss: 0.3258400708436966, Final Batch Loss: 0.2186613529920578\n",
      "Epoch 2986, Loss: 0.10178618133068085, Final Batch Loss: 0.042849138379096985\n",
      "Epoch 2987, Loss: 0.21104597300291061, Final Batch Loss: 0.07593246549367905\n",
      "Epoch 2988, Loss: 0.24810047447681427, Final Batch Loss: 0.10780215263366699\n",
      "Epoch 2989, Loss: 0.17902407050132751, Final Batch Loss: 0.11168863624334335\n",
      "Epoch 2990, Loss: 0.1456364542245865, Final Batch Loss: 0.07392960041761398\n",
      "Epoch 2991, Loss: 0.413128599524498, Final Batch Loss: 0.19554269313812256\n",
      "Epoch 2992, Loss: 0.21411074697971344, Final Batch Loss: 0.10788464546203613\n",
      "Epoch 2993, Loss: 0.16336098313331604, Final Batch Loss: 0.0558144748210907\n",
      "Epoch 2994, Loss: 0.18066559731960297, Final Batch Loss: 0.10469698905944824\n",
      "Epoch 2995, Loss: 0.1587849110364914, Final Batch Loss: 0.08848188817501068\n",
      "Epoch 2996, Loss: 0.19208822399377823, Final Batch Loss: 0.13466715812683105\n",
      "Epoch 2997, Loss: 0.18322691321372986, Final Batch Loss: 0.08455182611942291\n",
      "Epoch 2998, Loss: 0.1810653731226921, Final Batch Loss: 0.08388853818178177\n",
      "Epoch 2999, Loss: 0.31937018781900406, Final Batch Loss: 0.24320554733276367\n",
      "Epoch 3000, Loss: 0.17880652844905853, Final Batch Loss: 0.09222472459077835\n",
      "Epoch 3001, Loss: 0.21096555888652802, Final Batch Loss: 0.1313779354095459\n",
      "Epoch 3002, Loss: 0.251128114759922, Final Batch Loss: 0.13499288260936737\n",
      "Epoch 3003, Loss: 0.27353397011756897, Final Batch Loss: 0.21700596809387207\n",
      "Epoch 3004, Loss: 0.14076214283704758, Final Batch Loss: 0.050166428089141846\n",
      "Epoch 3005, Loss: 0.1554434895515442, Final Batch Loss: 0.08990411460399628\n",
      "Epoch 3006, Loss: 0.16403980925679207, Final Batch Loss: 0.10470803081989288\n",
      "Epoch 3007, Loss: 0.18157318234443665, Final Batch Loss: 0.11180548369884491\n",
      "Epoch 3008, Loss: 0.20961839705705643, Final Batch Loss: 0.08717062324285507\n",
      "Epoch 3009, Loss: 0.284670814871788, Final Batch Loss: 0.14634954929351807\n",
      "Epoch 3010, Loss: 0.20272058248519897, Final Batch Loss: 0.08348331600427628\n",
      "Epoch 3011, Loss: 0.28705936670303345, Final Batch Loss: 0.17129218578338623\n",
      "Epoch 3012, Loss: 0.11084874719381332, Final Batch Loss: 0.0399133637547493\n",
      "Epoch 3013, Loss: 0.14825081080198288, Final Batch Loss: 0.06349924206733704\n",
      "Epoch 3014, Loss: 0.2596122547984123, Final Batch Loss: 0.14084523916244507\n",
      "Epoch 3015, Loss: 0.16969280689954758, Final Batch Loss: 0.1005120649933815\n",
      "Epoch 3016, Loss: 0.16001763194799423, Final Batch Loss: 0.09558606147766113\n",
      "Epoch 3017, Loss: 0.20672845840454102, Final Batch Loss: 0.07771901786327362\n",
      "Epoch 3018, Loss: 0.18678690493106842, Final Batch Loss: 0.10698559135198593\n",
      "Epoch 3019, Loss: 0.14235486835241318, Final Batch Loss: 0.07581951469182968\n",
      "Epoch 3020, Loss: 0.15731748193502426, Final Batch Loss: 0.06608453392982483\n",
      "Epoch 3021, Loss: 0.1076694168150425, Final Batch Loss: 0.041510146111249924\n",
      "Epoch 3022, Loss: 0.16621705889701843, Final Batch Loss: 0.09189850836992264\n",
      "Epoch 3023, Loss: 0.15614819899201393, Final Batch Loss: 0.05404527857899666\n",
      "Epoch 3024, Loss: 0.29030297696590424, Final Batch Loss: 0.19451183080673218\n",
      "Epoch 3025, Loss: 0.13002606108784676, Final Batch Loss: 0.03781725838780403\n",
      "Epoch 3026, Loss: 0.16456598788499832, Final Batch Loss: 0.08322317898273468\n",
      "Epoch 3027, Loss: 0.14952196925878525, Final Batch Loss: 0.08008973300457001\n",
      "Epoch 3028, Loss: 0.1878143474459648, Final Batch Loss: 0.07159015536308289\n",
      "Epoch 3029, Loss: 0.25232038646936417, Final Batch Loss: 0.08007096499204636\n",
      "Epoch 3030, Loss: 0.25659235566854477, Final Batch Loss: 0.11553210765123367\n",
      "Epoch 3031, Loss: 0.13817090541124344, Final Batch Loss: 0.04952152073383331\n",
      "Epoch 3032, Loss: 0.3292302340269089, Final Batch Loss: 0.17146959900856018\n",
      "Epoch 3033, Loss: 0.2589673846960068, Final Batch Loss: 0.14608468115329742\n",
      "Epoch 3034, Loss: 0.32849375158548355, Final Batch Loss: 0.23419269919395447\n",
      "Epoch 3035, Loss: 0.190758615732193, Final Batch Loss: 0.11388986557722092\n",
      "Epoch 3036, Loss: 0.22358505427837372, Final Batch Loss: 0.10007792711257935\n",
      "Epoch 3037, Loss: 0.1939026415348053, Final Batch Loss: 0.1061989888548851\n",
      "Epoch 3038, Loss: 0.2313833311200142, Final Batch Loss: 0.1576787233352661\n",
      "Epoch 3039, Loss: 0.18579398840665817, Final Batch Loss: 0.0786772295832634\n",
      "Epoch 3040, Loss: 0.18318871408700943, Final Batch Loss: 0.06896738708019257\n",
      "Epoch 3041, Loss: 0.20720379054546356, Final Batch Loss: 0.06705701351165771\n",
      "Epoch 3042, Loss: 0.1851738542318344, Final Batch Loss: 0.1179010346531868\n",
      "Epoch 3043, Loss: 0.2058577463030815, Final Batch Loss: 0.07150211185216904\n",
      "Epoch 3044, Loss: 0.14336850494146347, Final Batch Loss: 0.07804519683122635\n",
      "Epoch 3045, Loss: 0.17558593302965164, Final Batch Loss: 0.07399506121873856\n",
      "Epoch 3046, Loss: 0.1580154076218605, Final Batch Loss: 0.09289470314979553\n",
      "Epoch 3047, Loss: 0.2168482542037964, Final Batch Loss: 0.12668251991271973\n",
      "Epoch 3048, Loss: 0.1566186510026455, Final Batch Loss: 0.054189641028642654\n",
      "Epoch 3049, Loss: 0.18465810269117355, Final Batch Loss: 0.09474975615739822\n",
      "Epoch 3050, Loss: 0.17902594059705734, Final Batch Loss: 0.07835905253887177\n",
      "Epoch 3051, Loss: 0.17043085396289825, Final Batch Loss: 0.06331426650285721\n",
      "Epoch 3052, Loss: 0.19395509362220764, Final Batch Loss: 0.08420997112989426\n",
      "Epoch 3053, Loss: 0.2424556314945221, Final Batch Loss: 0.15915900468826294\n",
      "Epoch 3054, Loss: 0.16288786381483078, Final Batch Loss: 0.0831129401922226\n",
      "Epoch 3055, Loss: 0.1753978282213211, Final Batch Loss: 0.0849766805768013\n",
      "Epoch 3056, Loss: 0.2003762647509575, Final Batch Loss: 0.08461719006299973\n",
      "Epoch 3057, Loss: 0.13910448551177979, Final Batch Loss: 0.06477750092744827\n",
      "Epoch 3058, Loss: 0.22287578135728836, Final Batch Loss: 0.12944597005844116\n",
      "Epoch 3059, Loss: 0.26145564764738083, Final Batch Loss: 0.16474857926368713\n",
      "Epoch 3060, Loss: 0.22945783287286758, Final Batch Loss: 0.11578576266765594\n",
      "Epoch 3061, Loss: 0.14563976228237152, Final Batch Loss: 0.06652698665857315\n",
      "Epoch 3062, Loss: 0.35273242741823196, Final Batch Loss: 0.22929348051548004\n",
      "Epoch 3063, Loss: 0.1867392733693123, Final Batch Loss: 0.10276370495557785\n",
      "Epoch 3064, Loss: 0.17282894253730774, Final Batch Loss: 0.08931953459978104\n",
      "Epoch 3065, Loss: 0.20974625647068024, Final Batch Loss: 0.11762821674346924\n",
      "Epoch 3066, Loss: 0.17172031849622726, Final Batch Loss: 0.06707049161195755\n",
      "Epoch 3067, Loss: 0.24403107538819313, Final Batch Loss: 0.19477713108062744\n",
      "Epoch 3068, Loss: 0.14274141192436218, Final Batch Loss: 0.05898924916982651\n",
      "Epoch 3069, Loss: 0.23408115655183792, Final Batch Loss: 0.11835232377052307\n",
      "Epoch 3070, Loss: 0.24903761595487595, Final Batch Loss: 0.1576908379793167\n",
      "Epoch 3071, Loss: 0.2977996990084648, Final Batch Loss: 0.1841406226158142\n",
      "Epoch 3072, Loss: 0.1903493031859398, Final Batch Loss: 0.0603756383061409\n",
      "Epoch 3073, Loss: 0.23312130570411682, Final Batch Loss: 0.13278017938137054\n",
      "Epoch 3074, Loss: 0.26359201967716217, Final Batch Loss: 0.07976306974887848\n",
      "Epoch 3075, Loss: 0.15611614659428596, Final Batch Loss: 0.040601376444101334\n",
      "Epoch 3076, Loss: 0.13622218370437622, Final Batch Loss: 0.06909853965044022\n",
      "Epoch 3077, Loss: 0.15091850608587265, Final Batch Loss: 0.07719925791025162\n",
      "Epoch 3078, Loss: 0.16238891333341599, Final Batch Loss: 0.05436234176158905\n",
      "Epoch 3079, Loss: 0.13569026440382004, Final Batch Loss: 0.07919314503669739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3080, Loss: 0.21278563141822815, Final Batch Loss: 0.08580657839775085\n",
      "Epoch 3081, Loss: 0.16237682104110718, Final Batch Loss: 0.0692153051495552\n",
      "Epoch 3082, Loss: 0.15765546634793282, Final Batch Loss: 0.04689304903149605\n",
      "Epoch 3083, Loss: 0.14457275718450546, Final Batch Loss: 0.05889994651079178\n",
      "Epoch 3084, Loss: 0.20244816690683365, Final Batch Loss: 0.08387310057878494\n",
      "Epoch 3085, Loss: 0.1190490871667862, Final Batch Loss: 0.08008795976638794\n",
      "Epoch 3086, Loss: 0.1742512434720993, Final Batch Loss: 0.03627823293209076\n",
      "Epoch 3087, Loss: 0.18038944154977798, Final Batch Loss: 0.08086889237165451\n",
      "Epoch 3088, Loss: 0.21125227957963943, Final Batch Loss: 0.11609011888504028\n",
      "Epoch 3089, Loss: 0.16908839344978333, Final Batch Loss: 0.0916086733341217\n",
      "Epoch 3090, Loss: 0.13593460991978645, Final Batch Loss: 0.04709252342581749\n",
      "Epoch 3091, Loss: 0.13942188397049904, Final Batch Loss: 0.06077703461050987\n",
      "Epoch 3092, Loss: 0.13995812460780144, Final Batch Loss: 0.04919992759823799\n",
      "Epoch 3093, Loss: 0.21090706437826157, Final Batch Loss: 0.06161000579595566\n",
      "Epoch 3094, Loss: 0.16719436645507812, Final Batch Loss: 0.08153656125068665\n",
      "Epoch 3095, Loss: 0.16609539836645126, Final Batch Loss: 0.08678018301725388\n",
      "Epoch 3096, Loss: 0.29589758068323135, Final Batch Loss: 0.10469669848680496\n",
      "Epoch 3097, Loss: 0.16653962433338165, Final Batch Loss: 0.13126757740974426\n",
      "Epoch 3098, Loss: 0.2620891183614731, Final Batch Loss: 0.1579277217388153\n",
      "Epoch 3099, Loss: 0.20150621235370636, Final Batch Loss: 0.1060597226023674\n",
      "Epoch 3100, Loss: 0.14777878671884537, Final Batch Loss: 0.058960363268852234\n",
      "Epoch 3101, Loss: 0.14192114397883415, Final Batch Loss: 0.03445054963231087\n",
      "Epoch 3102, Loss: 0.20037907361984253, Final Batch Loss: 0.08899521827697754\n",
      "Epoch 3103, Loss: 0.22329605370759964, Final Batch Loss: 0.09791996330022812\n",
      "Epoch 3104, Loss: 0.13586263731122017, Final Batch Loss: 0.07793598622083664\n",
      "Epoch 3105, Loss: 0.15432273596525192, Final Batch Loss: 0.0773720070719719\n",
      "Epoch 3106, Loss: 0.16978174448013306, Final Batch Loss: 0.06811431795358658\n",
      "Epoch 3107, Loss: 0.18838155269622803, Final Batch Loss: 0.08597934991121292\n",
      "Epoch 3108, Loss: 0.22793110460042953, Final Batch Loss: 0.09310392290353775\n",
      "Epoch 3109, Loss: 0.2385900393128395, Final Batch Loss: 0.05853796750307083\n",
      "Epoch 3110, Loss: 0.23736245185136795, Final Batch Loss: 0.0916040763258934\n",
      "Epoch 3111, Loss: 0.11071375757455826, Final Batch Loss: 0.037743113934993744\n",
      "Epoch 3112, Loss: 0.15154192596673965, Final Batch Loss: 0.08624381572008133\n",
      "Epoch 3113, Loss: 0.19803234934806824, Final Batch Loss: 0.08780431747436523\n",
      "Epoch 3114, Loss: 0.15914951264858246, Final Batch Loss: 0.07418566197156906\n",
      "Epoch 3115, Loss: 0.15289916843175888, Final Batch Loss: 0.08661076426506042\n",
      "Epoch 3116, Loss: 0.13868335261940956, Final Batch Loss: 0.08548206090927124\n",
      "Epoch 3117, Loss: 0.29596538841724396, Final Batch Loss: 0.18761347234249115\n",
      "Epoch 3118, Loss: 0.1303347684442997, Final Batch Loss: 0.07728468626737595\n",
      "Epoch 3119, Loss: 0.1580800637602806, Final Batch Loss: 0.09340766072273254\n",
      "Epoch 3120, Loss: 0.22717425227165222, Final Batch Loss: 0.1690286248922348\n",
      "Epoch 3121, Loss: 0.21255889534950256, Final Batch Loss: 0.11525499075651169\n",
      "Epoch 3122, Loss: 0.25611262768507004, Final Batch Loss: 0.16354647278785706\n",
      "Epoch 3123, Loss: 0.1867922618985176, Final Batch Loss: 0.08568913489580154\n",
      "Epoch 3124, Loss: 0.21198678016662598, Final Batch Loss: 0.07622380554676056\n",
      "Epoch 3125, Loss: 0.12997189164161682, Final Batch Loss: 0.053000494837760925\n",
      "Epoch 3126, Loss: 0.22498707473278046, Final Batch Loss: 0.13441778719425201\n",
      "Epoch 3127, Loss: 0.15893615037202835, Final Batch Loss: 0.0837099552154541\n",
      "Epoch 3128, Loss: 0.2601550444960594, Final Batch Loss: 0.07326645404100418\n",
      "Epoch 3129, Loss: 0.16934110969305038, Final Batch Loss: 0.10633096843957901\n",
      "Epoch 3130, Loss: 0.2989365682005882, Final Batch Loss: 0.22356678545475006\n",
      "Epoch 3131, Loss: 0.19286994636058807, Final Batch Loss: 0.11427678912878036\n",
      "Epoch 3132, Loss: 0.19683395326137543, Final Batch Loss: 0.07476527243852615\n",
      "Epoch 3133, Loss: 0.20619521290063858, Final Batch Loss: 0.11707372963428497\n",
      "Epoch 3134, Loss: 0.17771364748477936, Final Batch Loss: 0.06425858289003372\n",
      "Epoch 3135, Loss: 0.2321546971797943, Final Batch Loss: 0.11861379444599152\n",
      "Epoch 3136, Loss: 0.17442457005381584, Final Batch Loss: 0.06052381917834282\n",
      "Epoch 3137, Loss: 0.22038637846708298, Final Batch Loss: 0.12954211235046387\n",
      "Epoch 3138, Loss: 0.2527056783437729, Final Batch Loss: 0.15457844734191895\n",
      "Epoch 3139, Loss: 0.3532765358686447, Final Batch Loss: 0.21045656502246857\n",
      "Epoch 3140, Loss: 0.2803120091557503, Final Batch Loss: 0.11055343598127365\n",
      "Epoch 3141, Loss: 0.2002587988972664, Final Batch Loss: 0.10982558876276016\n",
      "Epoch 3142, Loss: 0.22007732838392258, Final Batch Loss: 0.12283585965633392\n",
      "Epoch 3143, Loss: 0.14595912396907806, Final Batch Loss: 0.0832587406039238\n",
      "Epoch 3144, Loss: 0.23065035790205002, Final Batch Loss: 0.15737591683864594\n",
      "Epoch 3145, Loss: 0.13775863498449326, Final Batch Loss: 0.07429133355617523\n",
      "Epoch 3146, Loss: 0.161532424390316, Final Batch Loss: 0.06711483746767044\n",
      "Epoch 3147, Loss: 0.19980623573064804, Final Batch Loss: 0.062217049300670624\n",
      "Epoch 3148, Loss: 0.2773786187171936, Final Batch Loss: 0.14632372558116913\n",
      "Epoch 3149, Loss: 0.23539036512374878, Final Batch Loss: 0.16639363765716553\n",
      "Epoch 3150, Loss: 0.16288767755031586, Final Batch Loss: 0.0941377803683281\n",
      "Epoch 3151, Loss: 0.2579383850097656, Final Batch Loss: 0.18744446337223053\n",
      "Epoch 3152, Loss: 0.22355032712221146, Final Batch Loss: 0.08790082484483719\n",
      "Epoch 3153, Loss: 0.12665501236915588, Final Batch Loss: 0.06804639846086502\n",
      "Epoch 3154, Loss: 0.24595708400011063, Final Batch Loss: 0.08350401371717453\n",
      "Epoch 3155, Loss: 0.17355014383792877, Final Batch Loss: 0.08438543230295181\n",
      "Epoch 3156, Loss: 0.2242041528224945, Final Batch Loss: 0.12759864330291748\n",
      "Epoch 3157, Loss: 0.19453415274620056, Final Batch Loss: 0.12866227328777313\n",
      "Epoch 3158, Loss: 0.18258928880095482, Final Batch Loss: 0.051892753690481186\n",
      "Epoch 3159, Loss: 0.15977759286761284, Final Batch Loss: 0.10442384332418442\n",
      "Epoch 3160, Loss: 0.1709567978978157, Final Batch Loss: 0.057871222496032715\n",
      "Epoch 3161, Loss: 0.18439768254756927, Final Batch Loss: 0.10025537759065628\n",
      "Epoch 3162, Loss: 0.1795598268508911, Final Batch Loss: 0.09098131209611893\n",
      "Epoch 3163, Loss: 0.12624337896704674, Final Batch Loss: 0.07310468703508377\n",
      "Epoch 3164, Loss: 0.14029652252793312, Final Batch Loss: 0.060654010623693466\n",
      "Epoch 3165, Loss: 0.15462159365415573, Final Batch Loss: 0.07986806333065033\n",
      "Epoch 3166, Loss: 0.1879505254328251, Final Batch Loss: 0.1306319385766983\n",
      "Epoch 3167, Loss: 0.20063485950231552, Final Batch Loss: 0.09136050194501877\n",
      "Epoch 3168, Loss: 0.2154204547405243, Final Batch Loss: 0.10980189591646194\n",
      "Epoch 3169, Loss: 0.1755979135632515, Final Batch Loss: 0.07307540625333786\n",
      "Epoch 3170, Loss: 0.17316726595163345, Final Batch Loss: 0.09186010807752609\n",
      "Epoch 3171, Loss: 0.17530440539121628, Final Batch Loss: 0.09952691942453384\n",
      "Epoch 3172, Loss: 0.2541268393397331, Final Batch Loss: 0.12941686809062958\n",
      "Epoch 3173, Loss: 0.18431276828050613, Final Batch Loss: 0.10210713744163513\n",
      "Epoch 3174, Loss: 0.13987161219120026, Final Batch Loss: 0.04418797045946121\n",
      "Epoch 3175, Loss: 0.15164797008037567, Final Batch Loss: 0.08764529228210449\n",
      "Epoch 3176, Loss: 0.14896772056818008, Final Batch Loss: 0.07288312911987305\n",
      "Epoch 3177, Loss: 0.2537241578102112, Final Batch Loss: 0.06561069190502167\n",
      "Epoch 3178, Loss: 0.2268732488155365, Final Batch Loss: 0.11503627896308899\n",
      "Epoch 3179, Loss: 0.40874481201171875, Final Batch Loss: 0.27902474999427795\n",
      "Epoch 3180, Loss: 0.1649155467748642, Final Batch Loss: 0.08416197448968887\n",
      "Epoch 3181, Loss: 0.14600654691457748, Final Batch Loss: 0.06900961697101593\n",
      "Epoch 3182, Loss: 0.18310068547725677, Final Batch Loss: 0.07093647867441177\n",
      "Epoch 3183, Loss: 0.12863898277282715, Final Batch Loss: 0.07913153618574142\n",
      "Epoch 3184, Loss: 0.1680171713232994, Final Batch Loss: 0.11848197132349014\n",
      "Epoch 3185, Loss: 0.19619237631559372, Final Batch Loss: 0.1262573003768921\n",
      "Epoch 3186, Loss: 0.2441284954547882, Final Batch Loss: 0.1256094127893448\n",
      "Epoch 3187, Loss: 0.20733606070280075, Final Batch Loss: 0.09491294622421265\n",
      "Epoch 3188, Loss: 0.1854371428489685, Final Batch Loss: 0.09439962357282639\n",
      "Epoch 3189, Loss: 0.21235675364732742, Final Batch Loss: 0.1129838228225708\n",
      "Epoch 3190, Loss: 0.1637011542916298, Final Batch Loss: 0.07380247861146927\n",
      "Epoch 3191, Loss: 0.12664644420146942, Final Batch Loss: 0.06429126113653183\n",
      "Epoch 3192, Loss: 0.18191732466220856, Final Batch Loss: 0.11434148252010345\n",
      "Epoch 3193, Loss: 0.18072359263896942, Final Batch Loss: 0.07684608548879623\n",
      "Epoch 3194, Loss: 0.2537698671221733, Final Batch Loss: 0.1191534772515297\n",
      "Epoch 3195, Loss: 0.10215884819626808, Final Batch Loss: 0.0624656155705452\n",
      "Epoch 3196, Loss: 0.23017724603414536, Final Batch Loss: 0.1675741970539093\n",
      "Epoch 3197, Loss: 0.20339379832148552, Final Batch Loss: 0.147986501455307\n",
      "Epoch 3198, Loss: 0.19096489995718002, Final Batch Loss: 0.0714762806892395\n",
      "Epoch 3199, Loss: 0.2522633671760559, Final Batch Loss: 0.17013603448867798\n",
      "Epoch 3200, Loss: 0.23784568160772324, Final Batch Loss: 0.11616989970207214\n",
      "Epoch 3201, Loss: 0.12408775091171265, Final Batch Loss: 0.06216093525290489\n",
      "Epoch 3202, Loss: 0.1754494570195675, Final Batch Loss: 0.05664610490202904\n",
      "Epoch 3203, Loss: 0.1465315818786621, Final Batch Loss: 0.07192277908325195\n",
      "Epoch 3204, Loss: 0.15265043079853058, Final Batch Loss: 0.087016262114048\n",
      "Epoch 3205, Loss: 0.17829551547765732, Final Batch Loss: 0.11057169735431671\n",
      "Epoch 3206, Loss: 0.20481355488300323, Final Batch Loss: 0.11225304007530212\n",
      "Epoch 3207, Loss: 0.2271360605955124, Final Batch Loss: 0.11894483864307404\n",
      "Epoch 3208, Loss: 0.18729623407125473, Final Batch Loss: 0.08848541229963303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3209, Loss: 0.18515725433826447, Final Batch Loss: 0.09240205585956573\n",
      "Epoch 3210, Loss: 0.13718973472714424, Final Batch Loss: 0.0624704547226429\n",
      "Epoch 3211, Loss: 0.1690632328391075, Final Batch Loss: 0.060648851096630096\n",
      "Epoch 3212, Loss: 0.10790776088833809, Final Batch Loss: 0.05391043424606323\n",
      "Epoch 3213, Loss: 0.14695437997579575, Final Batch Loss: 0.08265272527933121\n",
      "Epoch 3214, Loss: 0.13398144394159317, Final Batch Loss: 0.060245394706726074\n",
      "Epoch 3215, Loss: 0.14248740673065186, Final Batch Loss: 0.07157686352729797\n",
      "Epoch 3216, Loss: 0.1689111813902855, Final Batch Loss: 0.11184179782867432\n",
      "Epoch 3217, Loss: 0.201372392475605, Final Batch Loss: 0.1126609519124031\n",
      "Epoch 3218, Loss: 0.14762960001826286, Final Batch Loss: 0.05954752489924431\n",
      "Epoch 3219, Loss: 0.19742900133132935, Final Batch Loss: 0.09144996106624603\n",
      "Epoch 3220, Loss: 0.1952677257359028, Final Batch Loss: 0.053397152572870255\n",
      "Epoch 3221, Loss: 0.1982172653079033, Final Batch Loss: 0.12185437977313995\n",
      "Epoch 3222, Loss: 0.14824042841792107, Final Batch Loss: 0.05467509850859642\n",
      "Epoch 3223, Loss: 0.13087098300457, Final Batch Loss: 0.04023522883653641\n",
      "Epoch 3224, Loss: 0.16029487177729607, Final Batch Loss: 0.10101930797100067\n",
      "Epoch 3225, Loss: 0.19862063601613045, Final Batch Loss: 0.05528894439339638\n",
      "Epoch 3226, Loss: 0.1291733905673027, Final Batch Loss: 0.06201056391000748\n",
      "Epoch 3227, Loss: 0.34622886776924133, Final Batch Loss: 0.18338604271411896\n",
      "Epoch 3228, Loss: 0.20566340535879135, Final Batch Loss: 0.10714351385831833\n",
      "Epoch 3229, Loss: 0.2011561393737793, Final Batch Loss: 0.1373308002948761\n",
      "Epoch 3230, Loss: 0.22512327134609222, Final Batch Loss: 0.0958937555551529\n",
      "Epoch 3231, Loss: 0.23159971833229065, Final Batch Loss: 0.042254894971847534\n",
      "Epoch 3232, Loss: 0.20411942899227142, Final Batch Loss: 0.06852971017360687\n",
      "Epoch 3233, Loss: 0.21556496620178223, Final Batch Loss: 0.0746031105518341\n",
      "Epoch 3234, Loss: 0.15234902501106262, Final Batch Loss: 0.05251125246286392\n",
      "Epoch 3235, Loss: 0.13108540326356888, Final Batch Loss: 0.06461066752672195\n",
      "Epoch 3236, Loss: 0.14625703543424606, Final Batch Loss: 0.08342401683330536\n",
      "Epoch 3237, Loss: 0.23978976905345917, Final Batch Loss: 0.12930753827095032\n",
      "Epoch 3238, Loss: 0.12615349516272545, Final Batch Loss: 0.04110654816031456\n",
      "Epoch 3239, Loss: 0.14805570989847183, Final Batch Loss: 0.060234881937503815\n",
      "Epoch 3240, Loss: 0.19050481915473938, Final Batch Loss: 0.13252019882202148\n",
      "Epoch 3241, Loss: 0.1699572429060936, Final Batch Loss: 0.0957915335893631\n",
      "Epoch 3242, Loss: 0.3156679719686508, Final Batch Loss: 0.1812690943479538\n",
      "Epoch 3243, Loss: 0.18542034178972244, Final Batch Loss: 0.10902421921491623\n",
      "Epoch 3244, Loss: 0.15418081730604172, Final Batch Loss: 0.07054360210895538\n",
      "Epoch 3245, Loss: 0.15966802090406418, Final Batch Loss: 0.07722800970077515\n",
      "Epoch 3246, Loss: 0.1570488139986992, Final Batch Loss: 0.09336169064044952\n",
      "Epoch 3247, Loss: 0.38330410420894623, Final Batch Loss: 0.226851686835289\n",
      "Epoch 3248, Loss: 0.252843514084816, Final Batch Loss: 0.12055383622646332\n",
      "Epoch 3249, Loss: 0.13353581726551056, Final Batch Loss: 0.07491137087345123\n",
      "Epoch 3250, Loss: 0.15206270664930344, Final Batch Loss: 0.06818471848964691\n",
      "Epoch 3251, Loss: 0.19303212314844131, Final Batch Loss: 0.09165720641613007\n",
      "Epoch 3252, Loss: 0.14370765537023544, Final Batch Loss: 0.0627809539437294\n",
      "Epoch 3253, Loss: 0.2614963725209236, Final Batch Loss: 0.16992920637130737\n",
      "Epoch 3254, Loss: 0.13285716995596886, Final Batch Loss: 0.05981476977467537\n",
      "Epoch 3255, Loss: 0.19309695810079575, Final Batch Loss: 0.0832897424697876\n",
      "Epoch 3256, Loss: 0.1921481415629387, Final Batch Loss: 0.08171774446964264\n",
      "Epoch 3257, Loss: 0.13986676558852196, Final Batch Loss: 0.05394727364182472\n",
      "Epoch 3258, Loss: 0.24209606647491455, Final Batch Loss: 0.08930201828479767\n",
      "Epoch 3259, Loss: 0.18854521960020065, Final Batch Loss: 0.0989987850189209\n",
      "Epoch 3260, Loss: 0.18834802508354187, Final Batch Loss: 0.11627785861492157\n",
      "Epoch 3261, Loss: 0.20193061232566833, Final Batch Loss: 0.11496064066886902\n",
      "Epoch 3262, Loss: 0.2534070089459419, Final Batch Loss: 0.16239292919635773\n",
      "Epoch 3263, Loss: 0.1440340206027031, Final Batch Loss: 0.06263823062181473\n",
      "Epoch 3264, Loss: 0.15498045086860657, Final Batch Loss: 0.08320502936840057\n",
      "Epoch 3265, Loss: 0.15596649050712585, Final Batch Loss: 0.0774669274687767\n",
      "Epoch 3266, Loss: 0.21909181028604507, Final Batch Loss: 0.09252961724996567\n",
      "Epoch 3267, Loss: 0.22546181827783585, Final Batch Loss: 0.13671401143074036\n",
      "Epoch 3268, Loss: 0.20926064997911453, Final Batch Loss: 0.15659748017787933\n",
      "Epoch 3269, Loss: 0.20152034610509872, Final Batch Loss: 0.1291581094264984\n",
      "Epoch 3270, Loss: 0.16404946893453598, Final Batch Loss: 0.07475496083498001\n",
      "Epoch 3271, Loss: 0.24429171532392502, Final Batch Loss: 0.12855549156665802\n",
      "Epoch 3272, Loss: 0.14832377433776855, Final Batch Loss: 0.09030111134052277\n",
      "Epoch 3273, Loss: 0.1684439331293106, Final Batch Loss: 0.08754973858594894\n",
      "Epoch 3274, Loss: 0.12063002213835716, Final Batch Loss: 0.05046259984374046\n",
      "Epoch 3275, Loss: 0.17055334895849228, Final Batch Loss: 0.09073575586080551\n",
      "Epoch 3276, Loss: 0.2523097097873688, Final Batch Loss: 0.09932991862297058\n",
      "Epoch 3277, Loss: 0.1387350782752037, Final Batch Loss: 0.06887144595384598\n",
      "Epoch 3278, Loss: 0.13754098489880562, Final Batch Loss: 0.05173185095191002\n",
      "Epoch 3279, Loss: 0.1560896746814251, Final Batch Loss: 0.06076733395457268\n",
      "Epoch 3280, Loss: 0.12933476269245148, Final Batch Loss: 0.07183269411325455\n",
      "Epoch 3281, Loss: 0.2904285043478012, Final Batch Loss: 0.22421972453594208\n",
      "Epoch 3282, Loss: 0.1635543629527092, Final Batch Loss: 0.08316338062286377\n",
      "Epoch 3283, Loss: 0.21981026977300644, Final Batch Loss: 0.08642122894525528\n",
      "Epoch 3284, Loss: 0.18849478662014008, Final Batch Loss: 0.08198804408311844\n",
      "Epoch 3285, Loss: 0.2461828961968422, Final Batch Loss: 0.1262989491224289\n",
      "Epoch 3286, Loss: 0.2000957876443863, Final Batch Loss: 0.10638199001550674\n",
      "Epoch 3287, Loss: 0.22917351871728897, Final Batch Loss: 0.10204421728849411\n",
      "Epoch 3288, Loss: 0.28454338014125824, Final Batch Loss: 0.19707228243350983\n",
      "Epoch 3289, Loss: 0.25325925648212433, Final Batch Loss: 0.13187190890312195\n",
      "Epoch 3290, Loss: 0.14867650344967842, Final Batch Loss: 0.043899472802877426\n",
      "Epoch 3291, Loss: 0.26912976801395416, Final Batch Loss: 0.1690230369567871\n",
      "Epoch 3292, Loss: 0.31490732729434967, Final Batch Loss: 0.20631156861782074\n",
      "Epoch 3293, Loss: 0.1896783635020256, Final Batch Loss: 0.09163731336593628\n",
      "Epoch 3294, Loss: 0.2031979337334633, Final Batch Loss: 0.10671697556972504\n",
      "Epoch 3295, Loss: 0.24366562813520432, Final Batch Loss: 0.10495608299970627\n",
      "Epoch 3296, Loss: 0.16991271078586578, Final Batch Loss: 0.11125390976667404\n",
      "Epoch 3297, Loss: 0.2596871256828308, Final Batch Loss: 0.19464443624019623\n",
      "Epoch 3298, Loss: 0.18057820573449135, Final Batch Loss: 0.05351436510682106\n",
      "Epoch 3299, Loss: 0.11960111185908318, Final Batch Loss: 0.06917660683393478\n",
      "Epoch 3300, Loss: 0.20159749686717987, Final Batch Loss: 0.1081300675868988\n",
      "Epoch 3301, Loss: 0.26932542771101, Final Batch Loss: 0.15715292096138\n",
      "Epoch 3302, Loss: 0.1566491723060608, Final Batch Loss: 0.06811511516571045\n",
      "Epoch 3303, Loss: 0.18758083879947662, Final Batch Loss: 0.12231221050024033\n",
      "Epoch 3304, Loss: 0.15340906381607056, Final Batch Loss: 0.0671931579709053\n",
      "Epoch 3305, Loss: 0.16917582973837852, Final Batch Loss: 0.03787165507674217\n",
      "Epoch 3306, Loss: 0.320435494184494, Final Batch Loss: 0.21313151717185974\n",
      "Epoch 3307, Loss: 0.1386820711195469, Final Batch Loss: 0.08283250033855438\n",
      "Epoch 3308, Loss: 0.23562097176909447, Final Batch Loss: 0.060509633272886276\n",
      "Epoch 3309, Loss: 0.1552051305770874, Final Batch Loss: 0.042943112552165985\n",
      "Epoch 3310, Loss: 0.20673906058073044, Final Batch Loss: 0.12558718025684357\n",
      "Epoch 3311, Loss: 0.13359630480408669, Final Batch Loss: 0.03650585189461708\n",
      "Epoch 3312, Loss: 0.25847531855106354, Final Batch Loss: 0.12784390151500702\n",
      "Epoch 3313, Loss: 0.15624485537409782, Final Batch Loss: 0.0525088869035244\n",
      "Epoch 3314, Loss: 0.2629418149590492, Final Batch Loss: 0.1428404599428177\n",
      "Epoch 3315, Loss: 0.16784638911485672, Final Batch Loss: 0.07283186912536621\n",
      "Epoch 3316, Loss: 0.18362727761268616, Final Batch Loss: 0.09279404580593109\n",
      "Epoch 3317, Loss: 0.09401869773864746, Final Batch Loss: 0.04107031598687172\n",
      "Epoch 3318, Loss: 0.2823392152786255, Final Batch Loss: 0.1518513262271881\n",
      "Epoch 3319, Loss: 0.16008101403713226, Final Batch Loss: 0.08418435603380203\n",
      "Epoch 3320, Loss: 0.14241307973861694, Final Batch Loss: 0.08119536936283112\n",
      "Epoch 3321, Loss: 0.171431515365839, Final Batch Loss: 0.12001330405473709\n",
      "Epoch 3322, Loss: 0.12731263414025307, Final Batch Loss: 0.07997052371501923\n",
      "Epoch 3323, Loss: 0.20350628346204758, Final Batch Loss: 0.06469578295946121\n",
      "Epoch 3324, Loss: 0.22579605877399445, Final Batch Loss: 0.08133487403392792\n",
      "Epoch 3325, Loss: 0.1805928647518158, Final Batch Loss: 0.0986194834113121\n",
      "Epoch 3326, Loss: 0.15443195402622223, Final Batch Loss: 0.07286877185106277\n",
      "Epoch 3327, Loss: 0.20431381464004517, Final Batch Loss: 0.11836498975753784\n",
      "Epoch 3328, Loss: 0.20046579837799072, Final Batch Loss: 0.09164290875196457\n",
      "Epoch 3329, Loss: 0.10462896153330803, Final Batch Loss: 0.04006997123360634\n",
      "Epoch 3330, Loss: 0.17986323684453964, Final Batch Loss: 0.10892512649297714\n",
      "Epoch 3331, Loss: 0.14769834280014038, Final Batch Loss: 0.07703524827957153\n",
      "Epoch 3332, Loss: 0.20269324630498886, Final Batch Loss: 0.11672929674386978\n",
      "Epoch 3333, Loss: 0.2515634596347809, Final Batch Loss: 0.16992400586605072\n",
      "Epoch 3334, Loss: 0.1373037025332451, Final Batch Loss: 0.07573840767145157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3335, Loss: 0.1644190102815628, Final Batch Loss: 0.07818616926670074\n",
      "Epoch 3336, Loss: 0.17337094992399216, Final Batch Loss: 0.10638774931430817\n",
      "Epoch 3337, Loss: 0.14044585078954697, Final Batch Loss: 0.05961964279413223\n",
      "Epoch 3338, Loss: 0.11335357651114464, Final Batch Loss: 0.037460315972566605\n",
      "Epoch 3339, Loss: 0.23831713199615479, Final Batch Loss: 0.14259225130081177\n",
      "Epoch 3340, Loss: 0.1672540232539177, Final Batch Loss: 0.09230934828519821\n",
      "Epoch 3341, Loss: 0.16494109854102135, Final Batch Loss: 0.047086942940950394\n",
      "Epoch 3342, Loss: 0.31165066361427307, Final Batch Loss: 0.11210991442203522\n",
      "Epoch 3343, Loss: 0.21472777426242828, Final Batch Loss: 0.10446889698505402\n",
      "Epoch 3344, Loss: 0.18085849285125732, Final Batch Loss: 0.10746892541646957\n",
      "Epoch 3345, Loss: 0.1647781953215599, Final Batch Loss: 0.09548894315958023\n",
      "Epoch 3346, Loss: 0.16391775757074356, Final Batch Loss: 0.07765784114599228\n",
      "Epoch 3347, Loss: 0.22591280937194824, Final Batch Loss: 0.13349749147891998\n",
      "Epoch 3348, Loss: 0.13507596403360367, Final Batch Loss: 0.07477989792823792\n",
      "Epoch 3349, Loss: 0.18324525654315948, Final Batch Loss: 0.10169529169797897\n",
      "Epoch 3350, Loss: 0.11094505712389946, Final Batch Loss: 0.04974568635225296\n",
      "Epoch 3351, Loss: 0.27815598994493484, Final Batch Loss: 0.10503847151994705\n",
      "Epoch 3352, Loss: 0.13648536056280136, Final Batch Loss: 0.05546479672193527\n",
      "Epoch 3353, Loss: 0.20170103013515472, Final Batch Loss: 0.12478945404291153\n",
      "Epoch 3354, Loss: 0.10073547065258026, Final Batch Loss: 0.05051323026418686\n",
      "Epoch 3355, Loss: 0.1286969631910324, Final Batch Loss: 0.06265714764595032\n",
      "Epoch 3356, Loss: 0.13363052159547806, Final Batch Loss: 0.041081152856349945\n",
      "Epoch 3357, Loss: 0.14557991176843643, Final Batch Loss: 0.06873570382595062\n",
      "Epoch 3358, Loss: 0.2075909823179245, Final Batch Loss: 0.11738723516464233\n",
      "Epoch 3359, Loss: 0.14971628412604332, Final Batch Loss: 0.10844825208187103\n",
      "Epoch 3360, Loss: 0.16651205718517303, Final Batch Loss: 0.09625161439180374\n",
      "Epoch 3361, Loss: 0.27341940999031067, Final Batch Loss: 0.09411314129829407\n",
      "Epoch 3362, Loss: 0.1838277354836464, Final Batch Loss: 0.08654087781906128\n",
      "Epoch 3363, Loss: 0.11918395757675171, Final Batch Loss: 0.033357180655002594\n",
      "Epoch 3364, Loss: 0.18145770952105522, Final Batch Loss: 0.1303664743900299\n",
      "Epoch 3365, Loss: 0.1878591626882553, Final Batch Loss: 0.07047387957572937\n",
      "Epoch 3366, Loss: 0.3299986720085144, Final Batch Loss: 0.11098919808864594\n",
      "Epoch 3367, Loss: 0.13315078243613243, Final Batch Loss: 0.05191035196185112\n",
      "Epoch 3368, Loss: 0.2262265384197235, Final Batch Loss: 0.1290963739156723\n",
      "Epoch 3369, Loss: 0.2980048209428787, Final Batch Loss: 0.17281381785869598\n",
      "Epoch 3370, Loss: 0.20973850041627884, Final Batch Loss: 0.13038751482963562\n",
      "Epoch 3371, Loss: 0.22701972723007202, Final Batch Loss: 0.13657072186470032\n",
      "Epoch 3372, Loss: 0.13580478355288506, Final Batch Loss: 0.038122277706861496\n",
      "Epoch 3373, Loss: 0.1484696939587593, Final Batch Loss: 0.0491178035736084\n",
      "Epoch 3374, Loss: 0.21167684346437454, Final Batch Loss: 0.14974062144756317\n",
      "Epoch 3375, Loss: 0.1385801061987877, Final Batch Loss: 0.06365208327770233\n",
      "Epoch 3376, Loss: 0.144039336591959, Final Batch Loss: 0.05393734946846962\n",
      "Epoch 3377, Loss: 0.1646893508732319, Final Batch Loss: 0.10357405245304108\n",
      "Epoch 3378, Loss: 0.18855369091033936, Final Batch Loss: 0.10215684771537781\n",
      "Epoch 3379, Loss: 0.2060527577996254, Final Batch Loss: 0.09745313227176666\n",
      "Epoch 3380, Loss: 0.1758192554116249, Final Batch Loss: 0.10136901587247849\n",
      "Epoch 3381, Loss: 0.22448208928108215, Final Batch Loss: 0.06308558583259583\n",
      "Epoch 3382, Loss: 0.18638787791132927, Final Batch Loss: 0.058341946452856064\n",
      "Epoch 3383, Loss: 0.18809954822063446, Final Batch Loss: 0.08952996879816055\n",
      "Epoch 3384, Loss: 0.1671932078897953, Final Batch Loss: 0.11164776980876923\n",
      "Epoch 3385, Loss: 0.20437519252300262, Final Batch Loss: 0.07036131620407104\n",
      "Epoch 3386, Loss: 0.13009704649448395, Final Batch Loss: 0.07555797696113586\n",
      "Epoch 3387, Loss: 0.14521004259586334, Final Batch Loss: 0.06479576230049133\n",
      "Epoch 3388, Loss: 0.1495252549648285, Final Batch Loss: 0.048065900802612305\n",
      "Epoch 3389, Loss: 0.15098755061626434, Final Batch Loss: 0.06788351386785507\n",
      "Epoch 3390, Loss: 0.1138075403869152, Final Batch Loss: 0.04020989313721657\n",
      "Epoch 3391, Loss: 0.15550435334444046, Final Batch Loss: 0.08818303048610687\n",
      "Epoch 3392, Loss: 0.2265671193599701, Final Batch Loss: 0.1340290755033493\n",
      "Epoch 3393, Loss: 0.2612200528383255, Final Batch Loss: 0.15377089381217957\n",
      "Epoch 3394, Loss: 0.20361119508743286, Final Batch Loss: 0.07407844066619873\n",
      "Epoch 3395, Loss: 0.18981365859508514, Final Batch Loss: 0.10505923628807068\n",
      "Epoch 3396, Loss: 0.11035290732979774, Final Batch Loss: 0.063745878636837\n",
      "Epoch 3397, Loss: 0.11709613725543022, Final Batch Loss: 0.06315118074417114\n",
      "Epoch 3398, Loss: 0.15309534966945648, Final Batch Loss: 0.06398182362318039\n",
      "Epoch 3399, Loss: 0.14453456550836563, Final Batch Loss: 0.07545346766710281\n",
      "Epoch 3400, Loss: 0.12339899688959122, Final Batch Loss: 0.07138320803642273\n",
      "Epoch 3401, Loss: 0.15145139396190643, Final Batch Loss: 0.049103714525699615\n",
      "Epoch 3402, Loss: 0.267079159617424, Final Batch Loss: 0.17993144690990448\n",
      "Epoch 3403, Loss: 0.14860467240214348, Final Batch Loss: 0.0936199203133583\n",
      "Epoch 3404, Loss: 0.1924103945493698, Final Batch Loss: 0.09343691915273666\n",
      "Epoch 3405, Loss: 0.1644076630473137, Final Batch Loss: 0.09909944981336594\n",
      "Epoch 3406, Loss: 0.08772008866071701, Final Batch Loss: 0.03542705997824669\n",
      "Epoch 3407, Loss: 0.1397879309952259, Final Batch Loss: 0.09093781560659409\n",
      "Epoch 3408, Loss: 0.26872891932725906, Final Batch Loss: 0.1467646062374115\n",
      "Epoch 3409, Loss: 0.16224878281354904, Final Batch Loss: 0.07221968472003937\n",
      "Epoch 3410, Loss: 0.1145528294146061, Final Batch Loss: 0.05596045032143593\n",
      "Epoch 3411, Loss: 0.1568874828517437, Final Batch Loss: 0.05106668546795845\n",
      "Epoch 3412, Loss: 0.11752312257885933, Final Batch Loss: 0.042551081627607346\n",
      "Epoch 3413, Loss: 0.1877945438027382, Final Batch Loss: 0.05771835893392563\n",
      "Epoch 3414, Loss: 0.14095349609851837, Final Batch Loss: 0.04027414321899414\n",
      "Epoch 3415, Loss: 0.1412954144179821, Final Batch Loss: 0.04158516600728035\n",
      "Epoch 3416, Loss: 0.26044680923223495, Final Batch Loss: 0.09438089281320572\n",
      "Epoch 3417, Loss: 0.14943335950374603, Final Batch Loss: 0.06948069483041763\n",
      "Epoch 3418, Loss: 0.12226101756095886, Final Batch Loss: 0.04454014450311661\n",
      "Epoch 3419, Loss: 0.1809767782688141, Final Batch Loss: 0.10869935899972916\n",
      "Epoch 3420, Loss: 0.483223270624876, Final Batch Loss: 0.434115469455719\n",
      "Epoch 3421, Loss: 0.23689260333776474, Final Batch Loss: 0.10160068422555923\n",
      "Epoch 3422, Loss: 0.17926765233278275, Final Batch Loss: 0.08029678463935852\n",
      "Epoch 3423, Loss: 0.1042962446808815, Final Batch Loss: 0.040632203221321106\n",
      "Epoch 3424, Loss: 0.17081206291913986, Final Batch Loss: 0.08515173196792603\n",
      "Epoch 3425, Loss: 0.10016180202364922, Final Batch Loss: 0.0395793542265892\n",
      "Epoch 3426, Loss: 0.13366732746362686, Final Batch Loss: 0.06805326044559479\n",
      "Epoch 3427, Loss: 0.24803611636161804, Final Batch Loss: 0.18076136708259583\n",
      "Epoch 3428, Loss: 0.144569780677557, Final Batch Loss: 0.052895765751600266\n",
      "Epoch 3429, Loss: 0.2030455470085144, Final Batch Loss: 0.09361454844474792\n",
      "Epoch 3430, Loss: 0.1820441111922264, Final Batch Loss: 0.09500984102487564\n",
      "Epoch 3431, Loss: 0.1293150968849659, Final Batch Loss: 0.06233762577176094\n",
      "Epoch 3432, Loss: 0.14537413418293, Final Batch Loss: 0.05359763652086258\n",
      "Epoch 3433, Loss: 0.1632624864578247, Final Batch Loss: 0.04242425411939621\n",
      "Epoch 3434, Loss: 0.20774351060390472, Final Batch Loss: 0.14014650881290436\n",
      "Epoch 3435, Loss: 0.19625462591648102, Final Batch Loss: 0.04862120747566223\n",
      "Epoch 3436, Loss: 0.19831644743680954, Final Batch Loss: 0.11014559119939804\n",
      "Epoch 3437, Loss: 0.1324003003537655, Final Batch Loss: 0.08596734702587128\n",
      "Epoch 3438, Loss: 0.2014986202120781, Final Batch Loss: 0.09551337361335754\n",
      "Epoch 3439, Loss: 0.12377501651644707, Final Batch Loss: 0.03813784942030907\n",
      "Epoch 3440, Loss: 0.16842477396130562, Final Batch Loss: 0.12200166285037994\n",
      "Epoch 3441, Loss: 0.13091078400611877, Final Batch Loss: 0.05911732465028763\n",
      "Epoch 3442, Loss: 0.15471592545509338, Final Batch Loss: 0.07617847621440887\n",
      "Epoch 3443, Loss: 0.213836207985878, Final Batch Loss: 0.13601765036582947\n",
      "Epoch 3444, Loss: 0.1694481149315834, Final Batch Loss: 0.10161378234624863\n",
      "Epoch 3445, Loss: 0.1664031445980072, Final Batch Loss: 0.040574222803115845\n",
      "Epoch 3446, Loss: 0.13464248925447464, Final Batch Loss: 0.054803185164928436\n",
      "Epoch 3447, Loss: 0.19710759073495865, Final Batch Loss: 0.13797307014465332\n",
      "Epoch 3448, Loss: 0.10516168549656868, Final Batch Loss: 0.060421038419008255\n",
      "Epoch 3449, Loss: 0.19460757076740265, Final Batch Loss: 0.11031793802976608\n",
      "Epoch 3450, Loss: 0.171901173889637, Final Batch Loss: 0.12109199166297913\n",
      "Epoch 3451, Loss: 0.12306641042232513, Final Batch Loss: 0.07185764610767365\n",
      "Epoch 3452, Loss: 0.19861437380313873, Final Batch Loss: 0.06282532215118408\n",
      "Epoch 3453, Loss: 0.31909947097301483, Final Batch Loss: 0.2567877471446991\n",
      "Epoch 3454, Loss: 0.21077217906713486, Final Batch Loss: 0.07878629118204117\n",
      "Epoch 3455, Loss: 0.10303651168942451, Final Batch Loss: 0.04956592619419098\n",
      "Epoch 3456, Loss: 0.21149788796901703, Final Batch Loss: 0.08110268414020538\n",
      "Epoch 3457, Loss: 0.2099171131849289, Final Batch Loss: 0.1467597633600235\n",
      "Epoch 3458, Loss: 0.10871832817792892, Final Batch Loss: 0.045287467539310455\n",
      "Epoch 3459, Loss: 0.2060287594795227, Final Batch Loss: 0.12932126224040985\n",
      "Epoch 3460, Loss: 0.17588023841381073, Final Batch Loss: 0.06521780788898468\n",
      "Epoch 3461, Loss: 0.18070178106427193, Final Batch Loss: 0.04651926830410957\n",
      "Epoch 3462, Loss: 0.1868891566991806, Final Batch Loss: 0.08843477815389633\n",
      "Epoch 3463, Loss: 0.16077778488397598, Final Batch Loss: 0.09841898828744888\n",
      "Epoch 3464, Loss: 0.1710747331380844, Final Batch Loss: 0.10457410663366318\n",
      "Epoch 3465, Loss: 0.1781744360923767, Final Batch Loss: 0.07056975364685059\n",
      "Epoch 3466, Loss: 0.16297977417707443, Final Batch Loss: 0.0838770791888237\n",
      "Epoch 3467, Loss: 0.2540549859404564, Final Batch Loss: 0.16253358125686646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3468, Loss: 0.17071736603975296, Final Batch Loss: 0.06218910962343216\n",
      "Epoch 3469, Loss: 0.1272122710943222, Final Batch Loss: 0.05036914348602295\n",
      "Epoch 3470, Loss: 0.16802900657057762, Final Batch Loss: 0.05010701343417168\n",
      "Epoch 3471, Loss: 0.16915728896856308, Final Batch Loss: 0.1030782163143158\n",
      "Epoch 3472, Loss: 0.17098741978406906, Final Batch Loss: 0.09035307168960571\n",
      "Epoch 3473, Loss: 0.1533203199505806, Final Batch Loss: 0.04306540638208389\n",
      "Epoch 3474, Loss: 0.163131944835186, Final Batch Loss: 0.04061327874660492\n",
      "Epoch 3475, Loss: 0.1363835260272026, Final Batch Loss: 0.06073299050331116\n",
      "Epoch 3476, Loss: 0.13484593108296394, Final Batch Loss: 0.07944969087839127\n",
      "Epoch 3477, Loss: 0.14808426797389984, Final Batch Loss: 0.03381437063217163\n",
      "Epoch 3478, Loss: 0.14708156511187553, Final Batch Loss: 0.09350737929344177\n",
      "Epoch 3479, Loss: 0.11384222283959389, Final Batch Loss: 0.06030208244919777\n",
      "Epoch 3480, Loss: 0.23296545445919037, Final Batch Loss: 0.13613536953926086\n",
      "Epoch 3481, Loss: 0.1137179508805275, Final Batch Loss: 0.07263587415218353\n",
      "Epoch 3482, Loss: 0.1863904893398285, Final Batch Loss: 0.0770900771021843\n",
      "Epoch 3483, Loss: 0.14413581788539886, Final Batch Loss: 0.05517394095659256\n",
      "Epoch 3484, Loss: 0.1779055818915367, Final Batch Loss: 0.10634288936853409\n",
      "Epoch 3485, Loss: 0.13802990689873695, Final Batch Loss: 0.07696721702814102\n",
      "Epoch 3486, Loss: 0.1759156510233879, Final Batch Loss: 0.14836786687374115\n",
      "Epoch 3487, Loss: 0.11698063835501671, Final Batch Loss: 0.049097854644060135\n",
      "Epoch 3488, Loss: 0.16228721290826797, Final Batch Loss: 0.06267362833023071\n",
      "Epoch 3489, Loss: 0.23268918693065643, Final Batch Loss: 0.1425580233335495\n",
      "Epoch 3490, Loss: 0.15624989569187164, Final Batch Loss: 0.08081691712141037\n",
      "Epoch 3491, Loss: 0.16341038048267365, Final Batch Loss: 0.08822570741176605\n",
      "Epoch 3492, Loss: 0.28891970962285995, Final Batch Loss: 0.1760757863521576\n",
      "Epoch 3493, Loss: 0.18463673442602158, Final Batch Loss: 0.07250043749809265\n",
      "Epoch 3494, Loss: 0.16631564870476723, Final Batch Loss: 0.10616151988506317\n",
      "Epoch 3495, Loss: 0.216681107878685, Final Batch Loss: 0.08136814832687378\n",
      "Epoch 3496, Loss: 0.24703335762023926, Final Batch Loss: 0.08988285064697266\n",
      "Epoch 3497, Loss: 0.23280395567417145, Final Batch Loss: 0.08937899768352509\n",
      "Epoch 3498, Loss: 0.23712871223688126, Final Batch Loss: 0.1541188806295395\n",
      "Epoch 3499, Loss: 0.18262412399053574, Final Batch Loss: 0.0914413258433342\n",
      "Epoch 3500, Loss: 0.18896134197711945, Final Batch Loss: 0.06576699018478394\n",
      "Epoch 3501, Loss: 0.16650786995887756, Final Batch Loss: 0.0566372349858284\n",
      "Epoch 3502, Loss: 0.13309414312243462, Final Batch Loss: 0.04283536598086357\n",
      "Epoch 3503, Loss: 0.14286668598651886, Final Batch Loss: 0.06900525838136673\n",
      "Epoch 3504, Loss: 0.22698984295129776, Final Batch Loss: 0.1599992960691452\n",
      "Epoch 3505, Loss: 0.14231937378644943, Final Batch Loss: 0.10201279073953629\n",
      "Epoch 3506, Loss: 0.15132063254714012, Final Batch Loss: 0.09440504759550095\n",
      "Epoch 3507, Loss: 0.1246522106230259, Final Batch Loss: 0.058738406747579575\n",
      "Epoch 3508, Loss: 0.35285286605358124, Final Batch Loss: 0.23741671442985535\n",
      "Epoch 3509, Loss: 0.1669938862323761, Final Batch Loss: 0.10288383811712265\n",
      "Epoch 3510, Loss: 0.15414556115865707, Final Batch Loss: 0.07172621786594391\n",
      "Epoch 3511, Loss: 0.15922081470489502, Final Batch Loss: 0.08081204444169998\n",
      "Epoch 3512, Loss: 0.1419079825282097, Final Batch Loss: 0.07556622475385666\n",
      "Epoch 3513, Loss: 0.19867996126413345, Final Batch Loss: 0.14720110595226288\n",
      "Epoch 3514, Loss: 0.13797049596905708, Final Batch Loss: 0.056018006056547165\n",
      "Epoch 3515, Loss: 0.219317264854908, Final Batch Loss: 0.08781711012125015\n",
      "Epoch 3516, Loss: 0.19015302509069443, Final Batch Loss: 0.10593195259571075\n",
      "Epoch 3517, Loss: 0.17501142621040344, Final Batch Loss: 0.09166193753480911\n",
      "Epoch 3518, Loss: 0.174260213971138, Final Batch Loss: 0.08286269009113312\n",
      "Epoch 3519, Loss: 0.13892878592014313, Final Batch Loss: 0.07851971685886383\n",
      "Epoch 3520, Loss: 0.1368655413389206, Final Batch Loss: 0.053773507475852966\n",
      "Epoch 3521, Loss: 0.2562115713953972, Final Batch Loss: 0.172796368598938\n",
      "Epoch 3522, Loss: 0.17695944011211395, Final Batch Loss: 0.10593296587467194\n",
      "Epoch 3523, Loss: 0.11770124360918999, Final Batch Loss: 0.04190609231591225\n",
      "Epoch 3524, Loss: 0.17119023203849792, Final Batch Loss: 0.10389821976423264\n",
      "Epoch 3525, Loss: 0.13174281641840935, Final Batch Loss: 0.04569011554121971\n",
      "Epoch 3526, Loss: 0.13870538771152496, Final Batch Loss: 0.07278808951377869\n",
      "Epoch 3527, Loss: 0.19886702671647072, Final Batch Loss: 0.04729760065674782\n",
      "Epoch 3528, Loss: 0.12106405571103096, Final Batch Loss: 0.07932659238576889\n",
      "Epoch 3529, Loss: 0.1181485764682293, Final Batch Loss: 0.06471753120422363\n",
      "Epoch 3530, Loss: 0.16420860588550568, Final Batch Loss: 0.08602850884199142\n",
      "Epoch 3531, Loss: 0.11706263571977615, Final Batch Loss: 0.04739063233137131\n",
      "Epoch 3532, Loss: 0.2380921095609665, Final Batch Loss: 0.1372860074043274\n",
      "Epoch 3533, Loss: 0.11297718435525894, Final Batch Loss: 0.037345051765441895\n",
      "Epoch 3534, Loss: 0.17066007480025291, Final Batch Loss: 0.11011087149381638\n",
      "Epoch 3535, Loss: 0.17261485755443573, Final Batch Loss: 0.11067425459623337\n",
      "Epoch 3536, Loss: 0.19648417085409164, Final Batch Loss: 0.0683792307972908\n",
      "Epoch 3537, Loss: 0.20515985786914825, Final Batch Loss: 0.16573750972747803\n",
      "Epoch 3538, Loss: 0.12324226647615433, Final Batch Loss: 0.03853347897529602\n",
      "Epoch 3539, Loss: 0.1558004468679428, Final Batch Loss: 0.06860081106424332\n",
      "Epoch 3540, Loss: 0.1458551399409771, Final Batch Loss: 0.09554998576641083\n",
      "Epoch 3541, Loss: 0.1917346641421318, Final Batch Loss: 0.04802969843149185\n",
      "Epoch 3542, Loss: 0.12195618823170662, Final Batch Loss: 0.05861162021756172\n",
      "Epoch 3543, Loss: 0.13933958113193512, Final Batch Loss: 0.0753469318151474\n",
      "Epoch 3544, Loss: 0.20971989631652832, Final Batch Loss: 0.07218705117702484\n",
      "Epoch 3545, Loss: 0.1414518654346466, Final Batch Loss: 0.047889843583106995\n",
      "Epoch 3546, Loss: 0.10271034017205238, Final Batch Loss: 0.04996469244360924\n",
      "Epoch 3547, Loss: 0.1584710292518139, Final Batch Loss: 0.05151895061135292\n",
      "Epoch 3548, Loss: 0.10201302543282509, Final Batch Loss: 0.03648865595459938\n",
      "Epoch 3549, Loss: 0.06704667769372463, Final Batch Loss: 0.028475532308220863\n",
      "Epoch 3550, Loss: 0.1272234544157982, Final Batch Loss: 0.08117116987705231\n",
      "Epoch 3551, Loss: 0.13970262557268143, Final Batch Loss: 0.07494836300611496\n",
      "Epoch 3552, Loss: 0.15550699084997177, Final Batch Loss: 0.06945452094078064\n",
      "Epoch 3553, Loss: 0.16829940676689148, Final Batch Loss: 0.08566934615373611\n",
      "Epoch 3554, Loss: 0.14561571553349495, Final Batch Loss: 0.09046973288059235\n",
      "Epoch 3555, Loss: 0.29489265382289886, Final Batch Loss: 0.153081476688385\n",
      "Epoch 3556, Loss: 0.22017762809991837, Final Batch Loss: 0.11942794919013977\n",
      "Epoch 3557, Loss: 0.11025284230709076, Final Batch Loss: 0.04961508512496948\n",
      "Epoch 3558, Loss: 0.11764712631702423, Final Batch Loss: 0.05886397510766983\n",
      "Epoch 3559, Loss: 0.1863481104373932, Final Batch Loss: 0.09017504751682281\n",
      "Epoch 3560, Loss: 0.17742746323347092, Final Batch Loss: 0.07864652574062347\n",
      "Epoch 3561, Loss: 0.20222662389278412, Final Batch Loss: 0.08237181603908539\n",
      "Epoch 3562, Loss: 0.19049513339996338, Final Batch Loss: 0.11134497821331024\n",
      "Epoch 3563, Loss: 0.15369508042931557, Final Batch Loss: 0.11275117099285126\n",
      "Epoch 3564, Loss: 0.14731555432081223, Final Batch Loss: 0.0836225375533104\n",
      "Epoch 3565, Loss: 0.12096499279141426, Final Batch Loss: 0.038264598697423935\n",
      "Epoch 3566, Loss: 0.1360662579536438, Final Batch Loss: 0.07177816331386566\n",
      "Epoch 3567, Loss: 0.12974761798977852, Final Batch Loss: 0.056420233100652695\n",
      "Epoch 3568, Loss: 0.15144716575741768, Final Batch Loss: 0.09376464784145355\n",
      "Epoch 3569, Loss: 0.19132184982299805, Final Batch Loss: 0.14909780025482178\n",
      "Epoch 3570, Loss: 0.21916724741458893, Final Batch Loss: 0.12235855311155319\n",
      "Epoch 3571, Loss: 0.1748824566602707, Final Batch Loss: 0.06420132517814636\n",
      "Epoch 3572, Loss: 0.14467129670083523, Final Batch Loss: 0.02975134365260601\n",
      "Epoch 3573, Loss: 0.10041936859488487, Final Batch Loss: 0.02947748824954033\n",
      "Epoch 3574, Loss: 0.12271025776863098, Final Batch Loss: 0.04501953721046448\n",
      "Epoch 3575, Loss: 0.1798393316566944, Final Batch Loss: 0.12372129410505295\n",
      "Epoch 3576, Loss: 0.2292116954922676, Final Batch Loss: 0.07947032898664474\n",
      "Epoch 3577, Loss: 0.15485800430178642, Final Batch Loss: 0.06070481613278389\n",
      "Epoch 3578, Loss: 0.2045634537935257, Final Batch Loss: 0.09772747755050659\n",
      "Epoch 3579, Loss: 0.2084823176264763, Final Batch Loss: 0.10968894511461258\n",
      "Epoch 3580, Loss: 0.28928733617067337, Final Batch Loss: 0.22715705633163452\n",
      "Epoch 3581, Loss: 0.14695413038134575, Final Batch Loss: 0.08956047147512436\n",
      "Epoch 3582, Loss: 0.1631232313811779, Final Batch Loss: 0.04965391382575035\n",
      "Epoch 3583, Loss: 0.20881105214357376, Final Batch Loss: 0.11122976988554001\n",
      "Epoch 3584, Loss: 0.14511607587337494, Final Batch Loss: 0.0637926533818245\n",
      "Epoch 3585, Loss: 0.1716890037059784, Final Batch Loss: 0.10306955128908157\n",
      "Epoch 3586, Loss: 0.1596800684928894, Final Batch Loss: 0.07074355334043503\n",
      "Epoch 3587, Loss: 0.20677568018436432, Final Batch Loss: 0.09046095609664917\n",
      "Epoch 3588, Loss: 0.13939198106527328, Final Batch Loss: 0.08564947545528412\n",
      "Epoch 3589, Loss: 0.17329130321741104, Final Batch Loss: 0.10628331452608109\n",
      "Epoch 3590, Loss: 0.21616079658269882, Final Batch Loss: 0.12394274026155472\n",
      "Epoch 3591, Loss: 0.17175009846687317, Final Batch Loss: 0.1321013867855072\n",
      "Epoch 3592, Loss: 0.13855117559432983, Final Batch Loss: 0.04243472218513489\n",
      "Epoch 3593, Loss: 0.22712649405002594, Final Batch Loss: 0.12051986902952194\n",
      "Epoch 3594, Loss: 0.18141498044133186, Final Batch Loss: 0.13675324618816376\n",
      "Epoch 3595, Loss: 0.15925084054470062, Final Batch Loss: 0.04434376209974289\n",
      "Epoch 3596, Loss: 0.13445641845464706, Final Batch Loss: 0.06654728949069977\n",
      "Epoch 3597, Loss: 0.16724231094121933, Final Batch Loss: 0.09350639581680298\n",
      "Epoch 3598, Loss: 0.13860594108700752, Final Batch Loss: 0.1063542366027832\n",
      "Epoch 3599, Loss: 0.209779754281044, Final Batch Loss: 0.08496741205453873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3600, Loss: 0.21628592908382416, Final Batch Loss: 0.11111590266227722\n",
      "Epoch 3601, Loss: 0.13751941919326782, Final Batch Loss: 0.0682213082909584\n",
      "Epoch 3602, Loss: 0.1377519741654396, Final Batch Loss: 0.0721345990896225\n",
      "Epoch 3603, Loss: 0.16789156943559647, Final Batch Loss: 0.09202264994382858\n",
      "Epoch 3604, Loss: 0.15874332189559937, Final Batch Loss: 0.08393318206071854\n",
      "Epoch 3605, Loss: 0.12386716902256012, Final Batch Loss: 0.04149400442838669\n",
      "Epoch 3606, Loss: 0.14705264195799828, Final Batch Loss: 0.08573149144649506\n",
      "Epoch 3607, Loss: 0.19067057967185974, Final Batch Loss: 0.07641556113958359\n",
      "Epoch 3608, Loss: 0.1807660385966301, Final Batch Loss: 0.08465570211410522\n",
      "Epoch 3609, Loss: 0.17330610752105713, Final Batch Loss: 0.08702471107244492\n",
      "Epoch 3610, Loss: 0.21830733120441437, Final Batch Loss: 0.06939835846424103\n",
      "Epoch 3611, Loss: 0.13764191046357155, Final Batch Loss: 0.05089013651013374\n",
      "Epoch 3612, Loss: 0.10723258182406425, Final Batch Loss: 0.043929796665906906\n",
      "Epoch 3613, Loss: 0.19914717227220535, Final Batch Loss: 0.09824317693710327\n",
      "Epoch 3614, Loss: 0.12337740883231163, Final Batch Loss: 0.04427265003323555\n",
      "Epoch 3615, Loss: 0.08370999246835709, Final Batch Loss: 0.03433007746934891\n",
      "Epoch 3616, Loss: 0.17349279671907425, Final Batch Loss: 0.11504726856946945\n",
      "Epoch 3617, Loss: 0.135986328125, Final Batch Loss: 0.07207091897726059\n",
      "Epoch 3618, Loss: 0.21935617178678513, Final Batch Loss: 0.1196533665060997\n",
      "Epoch 3619, Loss: 0.09722825139760971, Final Batch Loss: 0.042706288397312164\n",
      "Epoch 3620, Loss: 0.13586916401982307, Final Batch Loss: 0.04888409748673439\n",
      "Epoch 3621, Loss: 0.175550177693367, Final Batch Loss: 0.1049775555729866\n",
      "Epoch 3622, Loss: 0.1344025321304798, Final Batch Loss: 0.05768730118870735\n",
      "Epoch 3623, Loss: 0.13179030269384384, Final Batch Loss: 0.05842173844575882\n",
      "Epoch 3624, Loss: 0.10059762373566628, Final Batch Loss: 0.04338723048567772\n",
      "Epoch 3625, Loss: 0.16402298212051392, Final Batch Loss: 0.06789173930883408\n",
      "Epoch 3626, Loss: 0.2612435147166252, Final Batch Loss: 0.11831497400999069\n",
      "Epoch 3627, Loss: 0.18625164777040482, Final Batch Loss: 0.10766127705574036\n",
      "Epoch 3628, Loss: 0.21337071806192398, Final Batch Loss: 0.12592744827270508\n",
      "Epoch 3629, Loss: 0.16347232460975647, Final Batch Loss: 0.08587253093719482\n",
      "Epoch 3630, Loss: 0.17043761909008026, Final Batch Loss: 0.09529780596494675\n",
      "Epoch 3631, Loss: 0.17748326063156128, Final Batch Loss: 0.0851128101348877\n",
      "Epoch 3632, Loss: 0.1336740106344223, Final Batch Loss: 0.08404609560966492\n",
      "Epoch 3633, Loss: 0.21412456780672073, Final Batch Loss: 0.08527322858572006\n",
      "Epoch 3634, Loss: 0.14293550327420235, Final Batch Loss: 0.10035014897584915\n",
      "Epoch 3635, Loss: 0.1692107692360878, Final Batch Loss: 0.08198639750480652\n",
      "Epoch 3636, Loss: 0.1724843978881836, Final Batch Loss: 0.07981117069721222\n",
      "Epoch 3637, Loss: 0.1989496946334839, Final Batch Loss: 0.08146639168262482\n",
      "Epoch 3638, Loss: 0.15045562386512756, Final Batch Loss: 0.06270216405391693\n",
      "Epoch 3639, Loss: 0.13101930916309357, Final Batch Loss: 0.07758656144142151\n",
      "Epoch 3640, Loss: 0.13142089918255806, Final Batch Loss: 0.05024233087897301\n",
      "Epoch 3641, Loss: 0.31886811554431915, Final Batch Loss: 0.2088768184185028\n",
      "Epoch 3642, Loss: 0.23215146362781525, Final Batch Loss: 0.12017372250556946\n",
      "Epoch 3643, Loss: 0.09917333722114563, Final Batch Loss: 0.06115845590829849\n",
      "Epoch 3644, Loss: 0.11714563518762589, Final Batch Loss: 0.06879764795303345\n",
      "Epoch 3645, Loss: 0.14753156900405884, Final Batch Loss: 0.06975798308849335\n",
      "Epoch 3646, Loss: 0.17666403949260712, Final Batch Loss: 0.1312067210674286\n",
      "Epoch 3647, Loss: 0.14150703325867653, Final Batch Loss: 0.03765960410237312\n",
      "Epoch 3648, Loss: 0.10433754324913025, Final Batch Loss: 0.05467086285352707\n",
      "Epoch 3649, Loss: 0.12262452393770218, Final Batch Loss: 0.06069963052868843\n",
      "Epoch 3650, Loss: 0.12440463900566101, Final Batch Loss: 0.0448894128203392\n",
      "Epoch 3651, Loss: 0.11638053506612778, Final Batch Loss: 0.04258742928504944\n",
      "Epoch 3652, Loss: 0.15371086448431015, Final Batch Loss: 0.07066645473241806\n",
      "Epoch 3653, Loss: 0.11927460879087448, Final Batch Loss: 0.07618779689073563\n",
      "Epoch 3654, Loss: 0.13775447756052017, Final Batch Loss: 0.07453674077987671\n",
      "Epoch 3655, Loss: 0.12243694439530373, Final Batch Loss: 0.06705866754055023\n",
      "Epoch 3656, Loss: 0.2106865793466568, Final Batch Loss: 0.10226120799779892\n",
      "Epoch 3657, Loss: 0.07984595187008381, Final Batch Loss: 0.02363474853336811\n",
      "Epoch 3658, Loss: 0.17760328575968742, Final Batch Loss: 0.04849853739142418\n",
      "Epoch 3659, Loss: 0.25095997005701065, Final Batch Loss: 0.13345395028591156\n",
      "Epoch 3660, Loss: 0.14272383600473404, Final Batch Loss: 0.07946448773145676\n",
      "Epoch 3661, Loss: 0.11705856025218964, Final Batch Loss: 0.051127299666404724\n",
      "Epoch 3662, Loss: 0.13755785673856735, Final Batch Loss: 0.07012534141540527\n",
      "Epoch 3663, Loss: 0.3271833807229996, Final Batch Loss: 0.1907382607460022\n",
      "Epoch 3664, Loss: 0.12254087626934052, Final Batch Loss: 0.03727744519710541\n",
      "Epoch 3665, Loss: 0.14229952543973923, Final Batch Loss: 0.06826534122228622\n",
      "Epoch 3666, Loss: 0.11320362985134125, Final Batch Loss: 0.038847289979457855\n",
      "Epoch 3667, Loss: 0.26030485332012177, Final Batch Loss: 0.11533869802951813\n",
      "Epoch 3668, Loss: 0.20019184052944183, Final Batch Loss: 0.12550973892211914\n",
      "Epoch 3669, Loss: 0.12259192019701004, Final Batch Loss: 0.05944618582725525\n",
      "Epoch 3670, Loss: 0.1467021033167839, Final Batch Loss: 0.07355860620737076\n",
      "Epoch 3671, Loss: 0.17275580763816833, Final Batch Loss: 0.0812714621424675\n",
      "Epoch 3672, Loss: 0.1836671456694603, Final Batch Loss: 0.08684408664703369\n",
      "Epoch 3673, Loss: 0.08217446506023407, Final Batch Loss: 0.04577743262052536\n",
      "Epoch 3674, Loss: 0.1251610368490219, Final Batch Loss: 0.0570957288146019\n",
      "Epoch 3675, Loss: 0.1704338639974594, Final Batch Loss: 0.05096927285194397\n",
      "Epoch 3676, Loss: 0.07847980409860611, Final Batch Loss: 0.03848402574658394\n",
      "Epoch 3677, Loss: 0.21348240971565247, Final Batch Loss: 0.1234663873910904\n",
      "Epoch 3678, Loss: 0.1322874929755926, Final Batch Loss: 0.028635112568736076\n",
      "Epoch 3679, Loss: 0.19841264933347702, Final Batch Loss: 0.08411974459886551\n",
      "Epoch 3680, Loss: 0.13149793446063995, Final Batch Loss: 0.08674253523349762\n",
      "Epoch 3681, Loss: 0.1494293324649334, Final Batch Loss: 0.04863424971699715\n",
      "Epoch 3682, Loss: 0.1058754213154316, Final Batch Loss: 0.05456554517149925\n",
      "Epoch 3683, Loss: 0.25522680580616, Final Batch Loss: 0.1755288541316986\n",
      "Epoch 3684, Loss: 0.19185469299554825, Final Batch Loss: 0.14339911937713623\n",
      "Epoch 3685, Loss: 0.16888941079378128, Final Batch Loss: 0.08497775346040726\n",
      "Epoch 3686, Loss: 0.14987516030669212, Final Batch Loss: 0.05609436705708504\n",
      "Epoch 3687, Loss: 0.1221538744866848, Final Batch Loss: 0.04272821173071861\n",
      "Epoch 3688, Loss: 0.18780407682061195, Final Batch Loss: 0.061265502125024796\n",
      "Epoch 3689, Loss: 0.11871333420276642, Final Batch Loss: 0.057467613369226456\n",
      "Epoch 3690, Loss: 0.13084320724010468, Final Batch Loss: 0.0667157992720604\n",
      "Epoch 3691, Loss: 0.26684117317199707, Final Batch Loss: 0.09516991674900055\n",
      "Epoch 3692, Loss: 0.21877803653478622, Final Batch Loss: 0.13474783301353455\n",
      "Epoch 3693, Loss: 0.1233607679605484, Final Batch Loss: 0.06783295422792435\n",
      "Epoch 3694, Loss: 0.12983091548085213, Final Batch Loss: 0.07777088135480881\n",
      "Epoch 3695, Loss: 0.13787510991096497, Final Batch Loss: 0.0704282745718956\n",
      "Epoch 3696, Loss: 0.14418017491698265, Final Batch Loss: 0.09649538993835449\n",
      "Epoch 3697, Loss: 0.14243314787745476, Final Batch Loss: 0.08923494070768356\n",
      "Epoch 3698, Loss: 0.10163398459553719, Final Batch Loss: 0.03950749337673187\n",
      "Epoch 3699, Loss: 0.07285187393426895, Final Batch Loss: 0.03462294861674309\n",
      "Epoch 3700, Loss: 0.17836102843284607, Final Batch Loss: 0.10733527690172195\n",
      "Epoch 3701, Loss: 0.26031768321990967, Final Batch Loss: 0.1815595179796219\n",
      "Epoch 3702, Loss: 0.13285953551530838, Final Batch Loss: 0.04473858326673508\n",
      "Epoch 3703, Loss: 0.10682377964258194, Final Batch Loss: 0.057093363255262375\n",
      "Epoch 3704, Loss: 0.09833382442593575, Final Batch Loss: 0.045033376663923264\n",
      "Epoch 3705, Loss: 0.13734005391597748, Final Batch Loss: 0.09480820596218109\n",
      "Epoch 3706, Loss: 0.2174861803650856, Final Batch Loss: 0.11880142241716385\n",
      "Epoch 3707, Loss: 0.1727115996181965, Final Batch Loss: 0.120847687125206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3708, Loss: 0.09321722015738487, Final Batch Loss: 0.03701116144657135\n",
      "Epoch 3709, Loss: 0.18395020067691803, Final Batch Loss: 0.12918631732463837\n",
      "Epoch 3710, Loss: 0.12000370398163795, Final Batch Loss: 0.033513713628053665\n",
      "Epoch 3711, Loss: 0.16247577965259552, Final Batch Loss: 0.07558043301105499\n",
      "Epoch 3712, Loss: 0.17440977320075035, Final Batch Loss: 0.12475935369729996\n",
      "Epoch 3713, Loss: 0.1167970709502697, Final Batch Loss: 0.04111732915043831\n",
      "Epoch 3714, Loss: 0.11585032939910889, Final Batch Loss: 0.0666014552116394\n",
      "Epoch 3715, Loss: 0.1104516014456749, Final Batch Loss: 0.07193395495414734\n",
      "Epoch 3716, Loss: 0.17657578736543655, Final Batch Loss: 0.09352307766675949\n",
      "Epoch 3717, Loss: 0.11229729279875755, Final Batch Loss: 0.046487946063280106\n",
      "Epoch 3718, Loss: 0.08892003446817398, Final Batch Loss: 0.04936763271689415\n",
      "Epoch 3719, Loss: 0.1942213848233223, Final Batch Loss: 0.10260221362113953\n",
      "Epoch 3720, Loss: 0.12766873836517334, Final Batch Loss: 0.08847921341657639\n",
      "Epoch 3721, Loss: 0.20476917177438736, Final Batch Loss: 0.12163718044757843\n",
      "Epoch 3722, Loss: 0.12863073498010635, Final Batch Loss: 0.06829100102186203\n",
      "Epoch 3723, Loss: 0.15247928351163864, Final Batch Loss: 0.09699634462594986\n",
      "Epoch 3724, Loss: 0.11714661866426468, Final Batch Loss: 0.06914805620908737\n",
      "Epoch 3725, Loss: 0.16138218343257904, Final Batch Loss: 0.11016876250505447\n",
      "Epoch 3726, Loss: 0.1280692219734192, Final Batch Loss: 0.06409218162298203\n",
      "Epoch 3727, Loss: 0.1152641735970974, Final Batch Loss: 0.04407704994082451\n",
      "Epoch 3728, Loss: 0.2174782007932663, Final Batch Loss: 0.09646580368280411\n",
      "Epoch 3729, Loss: 0.1919233277440071, Final Batch Loss: 0.11122223734855652\n",
      "Epoch 3730, Loss: 0.14771392941474915, Final Batch Loss: 0.0982285588979721\n",
      "Epoch 3731, Loss: 0.2446652054786682, Final Batch Loss: 0.08347395062446594\n",
      "Epoch 3732, Loss: 0.10939113423228264, Final Batch Loss: 0.04622185602784157\n",
      "Epoch 3733, Loss: 0.10431942716240883, Final Batch Loss: 0.039808791130781174\n",
      "Epoch 3734, Loss: 0.14387062564492226, Final Batch Loss: 0.05317458137869835\n",
      "Epoch 3735, Loss: 0.18267809599637985, Final Batch Loss: 0.08311009407043457\n",
      "Epoch 3736, Loss: 0.15895191580057144, Final Batch Loss: 0.048064760863780975\n",
      "Epoch 3737, Loss: 0.1259794756770134, Final Batch Loss: 0.05347418785095215\n",
      "Epoch 3738, Loss: 0.1751980409026146, Final Batch Loss: 0.11280254274606705\n",
      "Epoch 3739, Loss: 0.17397886514663696, Final Batch Loss: 0.09924803674221039\n",
      "Epoch 3740, Loss: 0.12881040573120117, Final Batch Loss: 0.05655009299516678\n",
      "Epoch 3741, Loss: 0.0861845500767231, Final Batch Loss: 0.033006809651851654\n",
      "Epoch 3742, Loss: 0.13400724530220032, Final Batch Loss: 0.0667288526892662\n",
      "Epoch 3743, Loss: 0.11347315460443497, Final Batch Loss: 0.07547824829816818\n",
      "Epoch 3744, Loss: 0.10908008739352226, Final Batch Loss: 0.027352508157491684\n",
      "Epoch 3745, Loss: 0.14018888026475906, Final Batch Loss: 0.04763319343328476\n",
      "Epoch 3746, Loss: 0.11968015879392624, Final Batch Loss: 0.09190892428159714\n",
      "Epoch 3747, Loss: 0.21868638321757317, Final Batch Loss: 0.054547395557165146\n",
      "Epoch 3748, Loss: 0.19886945560574532, Final Batch Loss: 0.05645366385579109\n",
      "Epoch 3749, Loss: 0.1236378476023674, Final Batch Loss: 0.06473925709724426\n",
      "Epoch 3750, Loss: 0.12771208211779594, Final Batch Loss: 0.08306585252285004\n",
      "Epoch 3751, Loss: 0.155823964625597, Final Batch Loss: 0.04128300026059151\n",
      "Epoch 3752, Loss: 0.14140914380550385, Final Batch Loss: 0.05683327466249466\n",
      "Epoch 3753, Loss: 0.13310595974326134, Final Batch Loss: 0.08357059210538864\n",
      "Epoch 3754, Loss: 0.19298718124628067, Final Batch Loss: 0.12257073819637299\n",
      "Epoch 3755, Loss: 0.19741354137659073, Final Batch Loss: 0.12370646744966507\n",
      "Epoch 3756, Loss: 0.1254214644432068, Final Batch Loss: 0.03845500946044922\n",
      "Epoch 3757, Loss: 0.14567261934280396, Final Batch Loss: 0.07186271995306015\n",
      "Epoch 3758, Loss: 0.1559443436563015, Final Batch Loss: 0.0998167023062706\n",
      "Epoch 3759, Loss: 0.23386803269386292, Final Batch Loss: 0.08560605347156525\n",
      "Epoch 3760, Loss: 0.17224733531475067, Final Batch Loss: 0.09265433251857758\n",
      "Epoch 3761, Loss: 0.1674238182604313, Final Batch Loss: 0.058216530829668045\n",
      "Epoch 3762, Loss: 0.12207823246717453, Final Batch Loss: 0.05623839050531387\n",
      "Epoch 3763, Loss: 0.1229308620095253, Final Batch Loss: 0.037096984684467316\n",
      "Epoch 3764, Loss: 0.10738384164869785, Final Batch Loss: 0.030134866014122963\n",
      "Epoch 3765, Loss: 0.1529565006494522, Final Batch Loss: 0.062313757836818695\n",
      "Epoch 3766, Loss: 0.11473986133933067, Final Batch Loss: 0.04492070898413658\n",
      "Epoch 3767, Loss: 0.15208683907985687, Final Batch Loss: 0.07732761651277542\n",
      "Epoch 3768, Loss: 0.11411727219820023, Final Batch Loss: 0.05159167945384979\n",
      "Epoch 3769, Loss: 0.21390282735228539, Final Batch Loss: 0.05196944996714592\n",
      "Epoch 3770, Loss: 0.26208165660500526, Final Batch Loss: 0.2036394327878952\n",
      "Epoch 3771, Loss: 0.16589902341365814, Final Batch Loss: 0.1279565989971161\n",
      "Epoch 3772, Loss: 0.17790468409657478, Final Batch Loss: 0.1250866800546646\n",
      "Epoch 3773, Loss: 0.11728062480688095, Final Batch Loss: 0.048264771699905396\n",
      "Epoch 3774, Loss: 0.16128919273614883, Final Batch Loss: 0.07588673382997513\n",
      "Epoch 3775, Loss: 0.1963871717453003, Final Batch Loss: 0.11625032126903534\n",
      "Epoch 3776, Loss: 0.18402204662561417, Final Batch Loss: 0.060812339186668396\n",
      "Epoch 3777, Loss: 0.16611437499523163, Final Batch Loss: 0.08601604402065277\n",
      "Epoch 3778, Loss: 0.09360427781939507, Final Batch Loss: 0.030257422477006912\n",
      "Epoch 3779, Loss: 0.15512178093194962, Final Batch Loss: 0.08968410640954971\n",
      "Epoch 3780, Loss: 0.18598364293575287, Final Batch Loss: 0.060451775789260864\n",
      "Epoch 3781, Loss: 0.12036731839179993, Final Batch Loss: 0.03729464113712311\n",
      "Epoch 3782, Loss: 0.16163639724254608, Final Batch Loss: 0.09234937280416489\n",
      "Epoch 3783, Loss: 0.15027011185884476, Final Batch Loss: 0.07714566588401794\n",
      "Epoch 3784, Loss: 0.22254418581724167, Final Batch Loss: 0.1286657154560089\n",
      "Epoch 3785, Loss: 0.26104736328125, Final Batch Loss: 0.16783088445663452\n",
      "Epoch 3786, Loss: 0.26344626396894455, Final Batch Loss: 0.16072580218315125\n",
      "Epoch 3787, Loss: 0.1289984993636608, Final Batch Loss: 0.04718199744820595\n",
      "Epoch 3788, Loss: 0.1570241004228592, Final Batch Loss: 0.030968427658081055\n",
      "Epoch 3789, Loss: 0.18591633439064026, Final Batch Loss: 0.09395220130681992\n",
      "Epoch 3790, Loss: 0.09294679760932922, Final Batch Loss: 0.04029044136404991\n",
      "Epoch 3791, Loss: 0.28024642169475555, Final Batch Loss: 0.14820300042629242\n",
      "Epoch 3792, Loss: 0.1067921593785286, Final Batch Loss: 0.048554953187704086\n",
      "Epoch 3793, Loss: 0.08791771903634071, Final Batch Loss: 0.0406544990837574\n",
      "Epoch 3794, Loss: 0.13458077609539032, Final Batch Loss: 0.07843639701604843\n",
      "Epoch 3795, Loss: 0.18375588953495026, Final Batch Loss: 0.09476829320192337\n",
      "Epoch 3796, Loss: 0.19712530076503754, Final Batch Loss: 0.07576726377010345\n",
      "Epoch 3797, Loss: 0.1886391043663025, Final Batch Loss: 0.0894225686788559\n",
      "Epoch 3798, Loss: 0.18676193803548813, Final Batch Loss: 0.07388798147439957\n",
      "Epoch 3799, Loss: 0.15063220262527466, Final Batch Loss: 0.07313405722379684\n",
      "Epoch 3800, Loss: 0.15358208492398262, Final Batch Loss: 0.1092950701713562\n",
      "Epoch 3801, Loss: 0.1043430007994175, Final Batch Loss: 0.03766440972685814\n",
      "Epoch 3802, Loss: 0.08292055130004883, Final Batch Loss: 0.03952639177441597\n",
      "Epoch 3803, Loss: 0.33112213015556335, Final Batch Loss: 0.24520087242126465\n",
      "Epoch 3804, Loss: 0.15986519679427147, Final Batch Loss: 0.06092293933033943\n",
      "Epoch 3805, Loss: 0.12030251324176788, Final Batch Loss: 0.07388275116682053\n",
      "Epoch 3806, Loss: 0.11697617545723915, Final Batch Loss: 0.05678871273994446\n",
      "Epoch 3807, Loss: 0.1525944620370865, Final Batch Loss: 0.0692179724574089\n",
      "Epoch 3808, Loss: 0.17690683901309967, Final Batch Loss: 0.06797884404659271\n",
      "Epoch 3809, Loss: 0.16138754040002823, Final Batch Loss: 0.08630567789077759\n",
      "Epoch 3810, Loss: 0.08777691796422005, Final Batch Loss: 0.044946860522031784\n",
      "Epoch 3811, Loss: 0.23866066336631775, Final Batch Loss: 0.1268751323223114\n",
      "Epoch 3812, Loss: 0.12125289440155029, Final Batch Loss: 0.065688855946064\n",
      "Epoch 3813, Loss: 0.18231519684195518, Final Batch Loss: 0.060316052287817\n",
      "Epoch 3814, Loss: 0.2172929346561432, Final Batch Loss: 0.12838999927043915\n",
      "Epoch 3815, Loss: 0.17387107014656067, Final Batch Loss: 0.08105024695396423\n",
      "Epoch 3816, Loss: 0.21335164457559586, Final Batch Loss: 0.11078162491321564\n",
      "Epoch 3817, Loss: 0.2746216431260109, Final Batch Loss: 0.2026159167289734\n",
      "Epoch 3818, Loss: 0.13263369724154472, Final Batch Loss: 0.0877656564116478\n",
      "Epoch 3819, Loss: 0.17713405936956406, Final Batch Loss: 0.10480106621980667\n",
      "Epoch 3820, Loss: 0.13731906563043594, Final Batch Loss: 0.04205518960952759\n",
      "Epoch 3821, Loss: 0.1398938037455082, Final Batch Loss: 0.04820756986737251\n",
      "Epoch 3822, Loss: 0.16051575541496277, Final Batch Loss: 0.0784514844417572\n",
      "Epoch 3823, Loss: 0.11626160144805908, Final Batch Loss: 0.06380429863929749\n",
      "Epoch 3824, Loss: 0.13925329595804214, Final Batch Loss: 0.07381902635097504\n",
      "Epoch 3825, Loss: 0.1152142621576786, Final Batch Loss: 0.05487614870071411\n",
      "Epoch 3826, Loss: 0.1779692843556404, Final Batch Loss: 0.1453980654478073\n",
      "Epoch 3827, Loss: 0.13917556405067444, Final Batch Loss: 0.07505916804075241\n",
      "Epoch 3828, Loss: 0.10646744817495346, Final Batch Loss: 0.04304368793964386\n",
      "Epoch 3829, Loss: 0.084571223706007, Final Batch Loss: 0.03405001759529114\n",
      "Epoch 3830, Loss: 0.15549145638942719, Final Batch Loss: 0.07858104258775711\n",
      "Epoch 3831, Loss: 0.17689886316657066, Final Batch Loss: 0.04734848067164421\n",
      "Epoch 3832, Loss: 0.1842527687549591, Final Batch Loss: 0.07496437430381775\n",
      "Epoch 3833, Loss: 0.18361981585621834, Final Batch Loss: 0.058088261634111404\n",
      "Epoch 3834, Loss: 0.12541243247687817, Final Batch Loss: 0.029598942026495934\n",
      "Epoch 3835, Loss: 0.16347292065620422, Final Batch Loss: 0.07226312160491943\n",
      "Epoch 3836, Loss: 0.14455842971801758, Final Batch Loss: 0.06910310685634613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3837, Loss: 0.19234450161457062, Final Batch Loss: 0.06995376199483871\n",
      "Epoch 3838, Loss: 0.1458861529827118, Final Batch Loss: 0.08713407069444656\n",
      "Epoch 3839, Loss: 0.13731266371905804, Final Batch Loss: 0.03052574209868908\n",
      "Epoch 3840, Loss: 0.22023680061101913, Final Batch Loss: 0.12485054135322571\n",
      "Epoch 3841, Loss: 0.12681973725557327, Final Batch Loss: 0.04463275521993637\n",
      "Epoch 3842, Loss: 0.22096817195415497, Final Batch Loss: 0.09820748120546341\n",
      "Epoch 3843, Loss: 0.14143014326691628, Final Batch Loss: 0.0880826860666275\n",
      "Epoch 3844, Loss: 0.13070569932460785, Final Batch Loss: 0.043562501668930054\n",
      "Epoch 3845, Loss: 0.13024476543068886, Final Batch Loss: 0.057299111038446426\n",
      "Epoch 3846, Loss: 0.08967933431267738, Final Batch Loss: 0.035589683800935745\n",
      "Epoch 3847, Loss: 0.16084380447864532, Final Batch Loss: 0.05892910063266754\n",
      "Epoch 3848, Loss: 0.169778972864151, Final Batch Loss: 0.08306176960468292\n",
      "Epoch 3849, Loss: 0.12492040544748306, Final Batch Loss: 0.04139561951160431\n",
      "Epoch 3850, Loss: 0.08608430996537209, Final Batch Loss: 0.04103003069758415\n",
      "Epoch 3851, Loss: 0.16301807761192322, Final Batch Loss: 0.0662350058555603\n",
      "Epoch 3852, Loss: 0.17262066155672073, Final Batch Loss: 0.11306687444448471\n",
      "Epoch 3853, Loss: 0.12841793149709702, Final Batch Loss: 0.05946800112724304\n",
      "Epoch 3854, Loss: 0.10673642344772816, Final Batch Loss: 0.029369981959462166\n",
      "Epoch 3855, Loss: 0.2538582310080528, Final Batch Loss: 0.11357846111059189\n",
      "Epoch 3856, Loss: 0.2303735390305519, Final Batch Loss: 0.1178276315331459\n",
      "Epoch 3857, Loss: 0.15963981673121452, Final Batch Loss: 0.05746522918343544\n",
      "Epoch 3858, Loss: 0.11489676684141159, Final Batch Loss: 0.028526030480861664\n",
      "Epoch 3859, Loss: 0.18010659143328667, Final Batch Loss: 0.03308872506022453\n",
      "Epoch 3860, Loss: 0.15753988176584244, Final Batch Loss: 0.03856007009744644\n",
      "Epoch 3861, Loss: 0.19199631363153458, Final Batch Loss: 0.12460273504257202\n",
      "Epoch 3862, Loss: 0.09615310281515121, Final Batch Loss: 0.033543191850185394\n",
      "Epoch 3863, Loss: 0.2030365988612175, Final Batch Loss: 0.1361353099346161\n",
      "Epoch 3864, Loss: 0.1503661423921585, Final Batch Loss: 0.057713642716407776\n",
      "Epoch 3865, Loss: 0.150178462266922, Final Batch Loss: 0.08690761774778366\n",
      "Epoch 3866, Loss: 0.22795478999614716, Final Batch Loss: 0.11677798628807068\n",
      "Epoch 3867, Loss: 0.13303064927458763, Final Batch Loss: 0.06038908287882805\n",
      "Epoch 3868, Loss: 0.12539493665099144, Final Batch Loss: 0.0787639170885086\n",
      "Epoch 3869, Loss: 0.1789223626255989, Final Batch Loss: 0.10302364826202393\n",
      "Epoch 3870, Loss: 0.11702675372362137, Final Batch Loss: 0.034455545246601105\n",
      "Epoch 3871, Loss: 0.20936381816864014, Final Batch Loss: 0.08477145433425903\n",
      "Epoch 3872, Loss: 0.24289697408676147, Final Batch Loss: 0.09854491055011749\n",
      "Epoch 3873, Loss: 0.18447217345237732, Final Batch Loss: 0.07369843870401382\n",
      "Epoch 3874, Loss: 0.19587856531143188, Final Batch Loss: 0.11986521631479263\n",
      "Epoch 3875, Loss: 0.1864243820309639, Final Batch Loss: 0.0654209703207016\n",
      "Epoch 3876, Loss: 0.15247761458158493, Final Batch Loss: 0.0763004794716835\n",
      "Epoch 3877, Loss: 0.18913574516773224, Final Batch Loss: 0.08110295236110687\n",
      "Epoch 3878, Loss: 0.1670311614871025, Final Batch Loss: 0.07966546714305878\n",
      "Epoch 3879, Loss: 0.13869690522551537, Final Batch Loss: 0.05823936685919762\n",
      "Epoch 3880, Loss: 0.2844684273004532, Final Batch Loss: 0.15821531414985657\n",
      "Epoch 3881, Loss: 0.14441122114658356, Final Batch Loss: 0.07682699710130692\n",
      "Epoch 3882, Loss: 0.10283735394477844, Final Batch Loss: 0.05902990698814392\n",
      "Epoch 3883, Loss: 0.1594463437795639, Final Batch Loss: 0.06555557250976562\n",
      "Epoch 3884, Loss: 0.1134317722171545, Final Batch Loss: 0.025263553485274315\n",
      "Epoch 3885, Loss: 0.1220276914536953, Final Batch Loss: 0.034364376217126846\n",
      "Epoch 3886, Loss: 0.19719741493463516, Final Batch Loss: 0.09821595251560211\n",
      "Epoch 3887, Loss: 0.14024348929524422, Final Batch Loss: 0.07779964059591293\n",
      "Epoch 3888, Loss: 0.26497094333171844, Final Batch Loss: 0.1061316579580307\n",
      "Epoch 3889, Loss: 0.11604771018028259, Final Batch Loss: 0.054712824523448944\n",
      "Epoch 3890, Loss: 0.21687545254826546, Final Batch Loss: 0.04692249372601509\n",
      "Epoch 3891, Loss: 0.12086175754666328, Final Batch Loss: 0.04751350358128548\n",
      "Epoch 3892, Loss: 0.16970079392194748, Final Batch Loss: 0.04381362348794937\n",
      "Epoch 3893, Loss: 0.14266613125801086, Final Batch Loss: 0.07426810264587402\n",
      "Epoch 3894, Loss: 0.14480024576187134, Final Batch Loss: 0.06449025124311447\n",
      "Epoch 3895, Loss: 0.16579820215702057, Final Batch Loss: 0.06938090920448303\n",
      "Epoch 3896, Loss: 0.14354343712329865, Final Batch Loss: 0.0881197601556778\n",
      "Epoch 3897, Loss: 0.14333929866552353, Final Batch Loss: 0.07600387930870056\n",
      "Epoch 3898, Loss: 0.0967592615634203, Final Batch Loss: 0.02770884521305561\n",
      "Epoch 3899, Loss: 0.15009672194719315, Final Batch Loss: 0.055989474058151245\n",
      "Epoch 3900, Loss: 0.16671745479106903, Final Batch Loss: 0.08510826528072357\n",
      "Epoch 3901, Loss: 0.16292743384838104, Final Batch Loss: 0.09601567685604095\n",
      "Epoch 3902, Loss: 0.14943332970142365, Final Batch Loss: 0.08132707327604294\n",
      "Epoch 3903, Loss: 0.14909225702285767, Final Batch Loss: 0.06690056622028351\n",
      "Epoch 3904, Loss: 0.14452244341373444, Final Batch Loss: 0.0684548020362854\n",
      "Epoch 3905, Loss: 0.15854861587285995, Final Batch Loss: 0.07998582720756531\n",
      "Epoch 3906, Loss: 0.11087108217179775, Final Batch Loss: 0.025738811120390892\n",
      "Epoch 3907, Loss: 0.15422773733735085, Final Batch Loss: 0.09422015398740768\n",
      "Epoch 3908, Loss: 0.12948698922991753, Final Batch Loss: 0.08038023859262466\n",
      "Epoch 3909, Loss: 0.12900589406490326, Final Batch Loss: 0.05196834355592728\n",
      "Epoch 3910, Loss: 0.1287052221596241, Final Batch Loss: 0.05882302299141884\n",
      "Epoch 3911, Loss: 0.146442212164402, Final Batch Loss: 0.05513828247785568\n",
      "Epoch 3912, Loss: 0.08470863476395607, Final Batch Loss: 0.051177434623241425\n",
      "Epoch 3913, Loss: 0.08083139732480049, Final Batch Loss: 0.03162810206413269\n",
      "Epoch 3914, Loss: 0.08786177262663841, Final Batch Loss: 0.03348216041922569\n",
      "Epoch 3915, Loss: 0.14218397065997124, Final Batch Loss: 0.062043238431215286\n",
      "Epoch 3916, Loss: 0.16086672991514206, Final Batch Loss: 0.07484636455774307\n",
      "Epoch 3917, Loss: 0.12275614961981773, Final Batch Loss: 0.039735231548547745\n",
      "Epoch 3918, Loss: 0.13241539523005486, Final Batch Loss: 0.034798938781023026\n",
      "Epoch 3919, Loss: 0.1516408808529377, Final Batch Loss: 0.02819327637553215\n",
      "Epoch 3920, Loss: 0.12095409631729126, Final Batch Loss: 0.07447560131549835\n",
      "Epoch 3921, Loss: 0.142074815928936, Final Batch Loss: 0.06939170509576797\n",
      "Epoch 3922, Loss: 0.19148093089461327, Final Batch Loss: 0.04478175565600395\n",
      "Epoch 3923, Loss: 0.21561499685049057, Final Batch Loss: 0.09117816388607025\n",
      "Epoch 3924, Loss: 0.17287738621234894, Final Batch Loss: 0.08964656293392181\n",
      "Epoch 3925, Loss: 0.25994180142879486, Final Batch Loss: 0.13106979429721832\n",
      "Epoch 3926, Loss: 0.1339302957057953, Final Batch Loss: 0.0391293540596962\n",
      "Epoch 3927, Loss: 0.21614430472254753, Final Batch Loss: 0.060776736587285995\n",
      "Epoch 3928, Loss: 0.06886208057403564, Final Batch Loss: 0.02265169471502304\n",
      "Epoch 3929, Loss: 0.2274857684969902, Final Batch Loss: 0.07597335427999496\n",
      "Epoch 3930, Loss: 0.2259937934577465, Final Batch Loss: 0.1818339079618454\n",
      "Epoch 3931, Loss: 0.1327585130929947, Final Batch Loss: 0.04710710048675537\n",
      "Epoch 3932, Loss: 0.14521199464797974, Final Batch Loss: 0.07586193084716797\n",
      "Epoch 3933, Loss: 0.16209951043128967, Final Batch Loss: 0.03897669166326523\n",
      "Epoch 3934, Loss: 0.09483266621828079, Final Batch Loss: 0.0446430966258049\n",
      "Epoch 3935, Loss: 0.3257693201303482, Final Batch Loss: 0.2102486491203308\n",
      "Epoch 3936, Loss: 0.1270424984395504, Final Batch Loss: 0.03841470554471016\n",
      "Epoch 3937, Loss: 0.1649240031838417, Final Batch Loss: 0.07095923274755478\n",
      "Epoch 3938, Loss: 0.09297104924917221, Final Batch Loss: 0.03453684598207474\n",
      "Epoch 3939, Loss: 0.09812918305397034, Final Batch Loss: 0.048214979469776154\n",
      "Epoch 3940, Loss: 0.15853997319936752, Final Batch Loss: 0.07931500673294067\n",
      "Epoch 3941, Loss: 0.13831568509340286, Final Batch Loss: 0.08638076484203339\n",
      "Epoch 3942, Loss: 0.17020612582564354, Final Batch Loss: 0.12788142263889313\n",
      "Epoch 3943, Loss: 0.14235756173729897, Final Batch Loss: 0.06121882423758507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3944, Loss: 0.10491423681378365, Final Batch Loss: 0.05008644238114357\n",
      "Epoch 3945, Loss: 0.08270983770489693, Final Batch Loss: 0.03249994292855263\n",
      "Epoch 3946, Loss: 0.2496931031346321, Final Batch Loss: 0.1269330531358719\n",
      "Epoch 3947, Loss: 0.1573338657617569, Final Batch Loss: 0.083116315305233\n",
      "Epoch 3948, Loss: 0.12553761526942253, Final Batch Loss: 0.06839440762996674\n",
      "Epoch 3949, Loss: 0.15430527925491333, Final Batch Loss: 0.07915449887514114\n",
      "Epoch 3950, Loss: 0.15853050351142883, Final Batch Loss: 0.07758829742670059\n",
      "Epoch 3951, Loss: 0.138985738158226, Final Batch Loss: 0.05978911370038986\n",
      "Epoch 3952, Loss: 0.09656162932515144, Final Batch Loss: 0.04646781086921692\n",
      "Epoch 3953, Loss: 0.1458689123392105, Final Batch Loss: 0.08892501890659332\n",
      "Epoch 3954, Loss: 0.15503987669944763, Final Batch Loss: 0.06638418883085251\n",
      "Epoch 3955, Loss: 0.2845214009284973, Final Batch Loss: 0.15448097884655\n",
      "Epoch 3956, Loss: 0.1525067575275898, Final Batch Loss: 0.09337049722671509\n",
      "Epoch 3957, Loss: 0.2118636667728424, Final Batch Loss: 0.12280268222093582\n",
      "Epoch 3958, Loss: 0.15331797301769257, Final Batch Loss: 0.1216585785150528\n",
      "Epoch 3959, Loss: 0.2263116091489792, Final Batch Loss: 0.11810163408517838\n",
      "Epoch 3960, Loss: 0.2470422089099884, Final Batch Loss: 0.13636858761310577\n",
      "Epoch 3961, Loss: 0.12946667522192, Final Batch Loss: 0.07971104979515076\n",
      "Epoch 3962, Loss: 0.2315809205174446, Final Batch Loss: 0.13247564435005188\n",
      "Epoch 3963, Loss: 0.20919786393642426, Final Batch Loss: 0.09434198588132858\n",
      "Epoch 3964, Loss: 0.14450837671756744, Final Batch Loss: 0.0279884934425354\n",
      "Epoch 3965, Loss: 0.10396366193890572, Final Batch Loss: 0.04894380271434784\n",
      "Epoch 3966, Loss: 0.18823345750570297, Final Batch Loss: 0.07829714566469193\n",
      "Epoch 3967, Loss: 0.08494093269109726, Final Batch Loss: 0.03186653181910515\n",
      "Epoch 3968, Loss: 0.10929058119654655, Final Batch Loss: 0.05359959974884987\n",
      "Epoch 3969, Loss: 0.10614857077598572, Final Batch Loss: 0.05223541706800461\n",
      "Epoch 3970, Loss: 0.10370279476046562, Final Batch Loss: 0.044459130614995956\n",
      "Epoch 3971, Loss: 0.14049157127738, Final Batch Loss: 0.05939391627907753\n",
      "Epoch 3972, Loss: 0.11642786487936974, Final Batch Loss: 0.03377803787589073\n",
      "Epoch 3973, Loss: 0.20694087073206902, Final Batch Loss: 0.057068031281232834\n",
      "Epoch 3974, Loss: 0.12792977318167686, Final Batch Loss: 0.049626562744379044\n",
      "Epoch 3975, Loss: 0.1695849671959877, Final Batch Loss: 0.0962463915348053\n",
      "Epoch 3976, Loss: 0.11962372064590454, Final Batch Loss: 0.06507488340139389\n",
      "Epoch 3977, Loss: 0.30827905237674713, Final Batch Loss: 0.2565881609916687\n",
      "Epoch 3978, Loss: 0.2585035711526871, Final Batch Loss: 0.1620330959558487\n",
      "Epoch 3979, Loss: 0.07962000370025635, Final Batch Loss: 0.03680192679166794\n",
      "Epoch 3980, Loss: 0.14781391993165016, Final Batch Loss: 0.0926498994231224\n",
      "Epoch 3981, Loss: 0.12160493433475494, Final Batch Loss: 0.046184130012989044\n",
      "Epoch 3982, Loss: 0.21059014648199081, Final Batch Loss: 0.08130546659231186\n",
      "Epoch 3983, Loss: 0.15863117575645447, Final Batch Loss: 0.0873367041349411\n",
      "Epoch 3984, Loss: 0.18065809458494186, Final Batch Loss: 0.06533756852149963\n",
      "Epoch 3985, Loss: 0.13761042803525925, Final Batch Loss: 0.049947433173656464\n",
      "Epoch 3986, Loss: 0.1060233972966671, Final Batch Loss: 0.0554998517036438\n",
      "Epoch 3987, Loss: 0.14626246690750122, Final Batch Loss: 0.06107693165540695\n",
      "Epoch 3988, Loss: 0.14904120564460754, Final Batch Loss: 0.08267685025930405\n",
      "Epoch 3989, Loss: 0.21095800399780273, Final Batch Loss: 0.07458949089050293\n",
      "Epoch 3990, Loss: 0.16584816202521324, Final Batch Loss: 0.058224644511938095\n",
      "Epoch 3991, Loss: 0.15287474542856216, Final Batch Loss: 0.07140985876321793\n",
      "Epoch 3992, Loss: 0.12205333262681961, Final Batch Loss: 0.07114110141992569\n",
      "Epoch 3993, Loss: 0.14554572477936745, Final Batch Loss: 0.0543196015059948\n",
      "Epoch 3994, Loss: 0.18989311903715134, Final Batch Loss: 0.07924681901931763\n",
      "Epoch 3995, Loss: 0.1013740748167038, Final Batch Loss: 0.041572973132133484\n",
      "Epoch 3996, Loss: 0.1105869971215725, Final Batch Loss: 0.05853133648633957\n",
      "Epoch 3997, Loss: 0.09945156425237656, Final Batch Loss: 0.05013339966535568\n",
      "Epoch 3998, Loss: 0.15592166036367416, Final Batch Loss: 0.08618978410959244\n",
      "Epoch 3999, Loss: 0.1910167746245861, Final Batch Loss: 0.15784749388694763\n",
      "Epoch 4000, Loss: 0.15109317004680634, Final Batch Loss: 0.072149358689785\n",
      "Epoch 4001, Loss: 0.15283016860485077, Final Batch Loss: 0.08134754747152328\n",
      "Epoch 4002, Loss: 0.12840325012803078, Final Batch Loss: 0.03651760146021843\n",
      "Epoch 4003, Loss: 0.13075900822877884, Final Batch Loss: 0.03444202244281769\n",
      "Epoch 4004, Loss: 0.13554950058460236, Final Batch Loss: 0.06743615120649338\n",
      "Epoch 4005, Loss: 0.09916284680366516, Final Batch Loss: 0.04216812551021576\n",
      "Epoch 4006, Loss: 0.3113114684820175, Final Batch Loss: 0.24351036548614502\n",
      "Epoch 4007, Loss: 0.15451505035161972, Final Batch Loss: 0.03416236490011215\n",
      "Epoch 4008, Loss: 0.11654838174581528, Final Batch Loss: 0.06335189193487167\n",
      "Epoch 4009, Loss: 0.2246536761522293, Final Batch Loss: 0.0980340838432312\n",
      "Epoch 4010, Loss: 0.1649705395102501, Final Batch Loss: 0.07641442120075226\n",
      "Epoch 4011, Loss: 0.19238325208425522, Final Batch Loss: 0.0938049852848053\n",
      "Epoch 4012, Loss: 0.22788464277982712, Final Batch Loss: 0.07862960547208786\n",
      "Epoch 4013, Loss: 0.1430284082889557, Final Batch Loss: 0.10068199038505554\n",
      "Epoch 4014, Loss: 0.12460316717624664, Final Batch Loss: 0.06564460694789886\n",
      "Epoch 4015, Loss: 0.22394581884145737, Final Batch Loss: 0.12072305381298065\n",
      "Epoch 4016, Loss: 0.17630917578935623, Final Batch Loss: 0.08884693682193756\n",
      "Epoch 4017, Loss: 0.1157325953245163, Final Batch Loss: 0.04845995455980301\n",
      "Epoch 4018, Loss: 0.10368726775050163, Final Batch Loss: 0.04384957253932953\n",
      "Epoch 4019, Loss: 0.1878308355808258, Final Batch Loss: 0.12692663073539734\n",
      "Epoch 4020, Loss: 0.09656016528606415, Final Batch Loss: 0.0589742511510849\n",
      "Epoch 4021, Loss: 0.13074908405542374, Final Batch Loss: 0.03612174093723297\n",
      "Epoch 4022, Loss: 0.12559761106967926, Final Batch Loss: 0.0699508860707283\n",
      "Epoch 4023, Loss: 0.15332959592342377, Final Batch Loss: 0.09582319855690002\n",
      "Epoch 4024, Loss: 0.1887267306447029, Final Batch Loss: 0.10235678404569626\n",
      "Epoch 4025, Loss: 0.17902957648038864, Final Batch Loss: 0.10264106839895248\n",
      "Epoch 4026, Loss: 0.1530656199902296, Final Batch Loss: 0.12501811981201172\n",
      "Epoch 4027, Loss: 0.14529628679156303, Final Batch Loss: 0.09033560752868652\n",
      "Epoch 4028, Loss: 0.21799803525209427, Final Batch Loss: 0.12640824913978577\n",
      "Epoch 4029, Loss: 0.11768821999430656, Final Batch Loss: 0.06750893592834473\n",
      "Epoch 4030, Loss: 0.12482568621635437, Final Batch Loss: 0.04622873663902283\n",
      "Epoch 4031, Loss: 0.1411661058664322, Final Batch Loss: 0.07431409507989883\n",
      "Epoch 4032, Loss: 0.1836588978767395, Final Batch Loss: 0.11211366951465607\n",
      "Epoch 4033, Loss: 0.12129631265997887, Final Batch Loss: 0.04933277890086174\n",
      "Epoch 4034, Loss: 0.13195548951625824, Final Batch Loss: 0.05874837189912796\n",
      "Epoch 4035, Loss: 0.15901901945471764, Final Batch Loss: 0.05659453943371773\n",
      "Epoch 4036, Loss: 0.14686862751841545, Final Batch Loss: 0.029733385890722275\n",
      "Epoch 4037, Loss: 0.1582837626338005, Final Batch Loss: 0.11023946851491928\n",
      "Epoch 4038, Loss: 0.24626565724611282, Final Batch Loss: 0.12270447611808777\n",
      "Epoch 4039, Loss: 0.13968612998723984, Final Batch Loss: 0.03492265194654465\n",
      "Epoch 4040, Loss: 0.14269211888313293, Final Batch Loss: 0.07763437181711197\n",
      "Epoch 4041, Loss: 0.1439523622393608, Final Batch Loss: 0.05060096085071564\n",
      "Epoch 4042, Loss: 0.12402469292283058, Final Batch Loss: 0.041711654514074326\n",
      "Epoch 4043, Loss: 0.11988428607583046, Final Batch Loss: 0.06746190786361694\n",
      "Epoch 4044, Loss: 0.18603552505373955, Final Batch Loss: 0.06017960235476494\n",
      "Epoch 4045, Loss: 0.14306996390223503, Final Batch Loss: 0.05624033883213997\n",
      "Epoch 4046, Loss: 0.11174987629055977, Final Batch Loss: 0.055216047912836075\n",
      "Epoch 4047, Loss: 0.12208395451307297, Final Batch Loss: 0.07421284914016724\n",
      "Epoch 4048, Loss: 0.10075913369655609, Final Batch Loss: 0.04256357625126839\n",
      "Epoch 4049, Loss: 0.15011343359947205, Final Batch Loss: 0.05794278532266617\n",
      "Epoch 4050, Loss: 0.1406385600566864, Final Batch Loss: 0.08811686187982559\n",
      "Epoch 4051, Loss: 0.07845858857035637, Final Batch Loss: 0.02466949075460434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4052, Loss: 0.10270240157842636, Final Batch Loss: 0.055200763046741486\n",
      "Epoch 4053, Loss: 0.1827782690525055, Final Batch Loss: 0.07207715511322021\n",
      "Epoch 4054, Loss: 0.1707582250237465, Final Batch Loss: 0.11077924072742462\n",
      "Epoch 4055, Loss: 0.17748946323990822, Final Batch Loss: 0.11976548284292221\n",
      "Epoch 4056, Loss: 0.185099259018898, Final Batch Loss: 0.07527392357587814\n",
      "Epoch 4057, Loss: 0.08259952440857887, Final Batch Loss: 0.0368022657930851\n",
      "Epoch 4058, Loss: 0.11801182478666306, Final Batch Loss: 0.03971007466316223\n",
      "Epoch 4059, Loss: 0.14976021647453308, Final Batch Loss: 0.07887707650661469\n",
      "Epoch 4060, Loss: 0.2105318233370781, Final Batch Loss: 0.11909779161214828\n",
      "Epoch 4061, Loss: 0.12078068777918816, Final Batch Loss: 0.047367606312036514\n",
      "Epoch 4062, Loss: 0.1053338535130024, Final Batch Loss: 0.05884154513478279\n",
      "Epoch 4063, Loss: 0.07388781383633614, Final Batch Loss: 0.04127666354179382\n",
      "Epoch 4064, Loss: 0.11580700799822807, Final Batch Loss: 0.04803701862692833\n",
      "Epoch 4065, Loss: 0.1740330085158348, Final Batch Loss: 0.08802560716867447\n",
      "Epoch 4066, Loss: 0.08596126735210419, Final Batch Loss: 0.04803219810128212\n",
      "Epoch 4067, Loss: 0.13168931007385254, Final Batch Loss: 0.08596669137477875\n",
      "Epoch 4068, Loss: 0.2311372384428978, Final Batch Loss: 0.10532376915216446\n",
      "Epoch 4069, Loss: 0.17476660013198853, Final Batch Loss: 0.0717381164431572\n",
      "Epoch 4070, Loss: 0.10150210186839104, Final Batch Loss: 0.030887220054864883\n",
      "Epoch 4071, Loss: 0.2849001958966255, Final Batch Loss: 0.21994122862815857\n",
      "Epoch 4072, Loss: 0.15535445511341095, Final Batch Loss: 0.10178502649068832\n",
      "Epoch 4073, Loss: 0.06842253915965557, Final Batch Loss: 0.024034148082137108\n",
      "Epoch 4074, Loss: 0.13641491532325745, Final Batch Loss: 0.08045915514230728\n",
      "Epoch 4075, Loss: 0.09382057562470436, Final Batch Loss: 0.036622293293476105\n",
      "Epoch 4076, Loss: 0.1652325838804245, Final Batch Loss: 0.06323912739753723\n",
      "Epoch 4077, Loss: 0.19815188646316528, Final Batch Loss: 0.13128980994224548\n",
      "Epoch 4078, Loss: 0.1789454147219658, Final Batch Loss: 0.03711990267038345\n",
      "Epoch 4079, Loss: 0.18264850229024887, Final Batch Loss: 0.0756165087223053\n",
      "Epoch 4080, Loss: 0.11607673019170761, Final Batch Loss: 0.0683133527636528\n",
      "Epoch 4081, Loss: 0.13967815786600113, Final Batch Loss: 0.05652521550655365\n",
      "Epoch 4082, Loss: 0.1737539917230606, Final Batch Loss: 0.06530574709177017\n",
      "Epoch 4083, Loss: 0.18692833185195923, Final Batch Loss: 0.11925407499074936\n",
      "Epoch 4084, Loss: 0.12307292222976685, Final Batch Loss: 0.03409860283136368\n",
      "Epoch 4085, Loss: 0.10285470262169838, Final Batch Loss: 0.04362208768725395\n",
      "Epoch 4086, Loss: 0.1350621096789837, Final Batch Loss: 0.03160784766077995\n",
      "Epoch 4087, Loss: 0.26220567524433136, Final Batch Loss: 0.1517917662858963\n",
      "Epoch 4088, Loss: 0.15658356994390488, Final Batch Loss: 0.08551166951656342\n",
      "Epoch 4089, Loss: 0.13508399575948715, Final Batch Loss: 0.06334232538938522\n",
      "Epoch 4090, Loss: 0.12631896138191223, Final Batch Loss: 0.055750541388988495\n",
      "Epoch 4091, Loss: 0.11985254287719727, Final Batch Loss: 0.08066264539957047\n",
      "Epoch 4092, Loss: 0.09644041955471039, Final Batch Loss: 0.03158470243215561\n",
      "Epoch 4093, Loss: 0.1785506010055542, Final Batch Loss: 0.07363956421613693\n",
      "Epoch 4094, Loss: 0.2662437930703163, Final Batch Loss: 0.15623268485069275\n",
      "Epoch 4095, Loss: 0.127996988594532, Final Batch Loss: 0.04834657162427902\n",
      "Epoch 4096, Loss: 0.1364898905158043, Final Batch Loss: 0.055363550782203674\n",
      "Epoch 4097, Loss: 0.19290205091238022, Final Batch Loss: 0.09312708675861359\n",
      "Epoch 4098, Loss: 0.09668165445327759, Final Batch Loss: 0.052555058151483536\n",
      "Epoch 4099, Loss: 0.2029433362185955, Final Batch Loss: 0.05664650723338127\n",
      "Epoch 4100, Loss: 0.1495274007320404, Final Batch Loss: 0.08530860394239426\n",
      "Epoch 4101, Loss: 0.09902912192046642, Final Batch Loss: 0.02852088026702404\n",
      "Epoch 4102, Loss: 0.15167078375816345, Final Batch Loss: 0.06866125762462616\n",
      "Epoch 4103, Loss: 0.19719260185956955, Final Batch Loss: 0.10339038819074631\n",
      "Epoch 4104, Loss: 0.1791660040616989, Final Batch Loss: 0.0632937103509903\n",
      "Epoch 4105, Loss: 0.07698029279708862, Final Batch Loss: 0.023948881775140762\n",
      "Epoch 4106, Loss: 0.13112732768058777, Final Batch Loss: 0.04905731976032257\n",
      "Epoch 4107, Loss: 0.13370363414287567, Final Batch Loss: 0.06256728619337082\n",
      "Epoch 4108, Loss: 0.13249646127223969, Final Batch Loss: 0.06304048001766205\n",
      "Epoch 4109, Loss: 0.14620206505060196, Final Batch Loss: 0.08080407232046127\n",
      "Epoch 4110, Loss: 0.10665865615010262, Final Batch Loss: 0.05673717334866524\n",
      "Epoch 4111, Loss: 0.15703939646482468, Final Batch Loss: 0.08620838820934296\n",
      "Epoch 4112, Loss: 0.08376674726605415, Final Batch Loss: 0.017209451645612717\n",
      "Epoch 4113, Loss: 0.11577542126178741, Final Batch Loss: 0.05374821648001671\n",
      "Epoch 4114, Loss: 0.1637830026447773, Final Batch Loss: 0.10268799215555191\n",
      "Epoch 4115, Loss: 0.1456875428557396, Final Batch Loss: 0.0947347953915596\n",
      "Epoch 4116, Loss: 0.15790288522839546, Final Batch Loss: 0.05198177322745323\n",
      "Epoch 4117, Loss: 0.15054034441709518, Final Batch Loss: 0.06157886981964111\n",
      "Epoch 4118, Loss: 0.14250147715210915, Final Batch Loss: 0.09626711159944534\n",
      "Epoch 4119, Loss: 0.08546563237905502, Final Batch Loss: 0.039251018315553665\n",
      "Epoch 4120, Loss: 0.23086317628622055, Final Batch Loss: 0.09164728969335556\n",
      "Epoch 4121, Loss: 0.1700635775923729, Final Batch Loss: 0.1097559928894043\n",
      "Epoch 4122, Loss: 0.17943736910820007, Final Batch Loss: 0.09412302821874619\n",
      "Epoch 4123, Loss: 0.09938236325979233, Final Batch Loss: 0.05176100507378578\n",
      "Epoch 4124, Loss: 0.10831642895936966, Final Batch Loss: 0.051190804690122604\n",
      "Epoch 4125, Loss: 0.14795206114649773, Final Batch Loss: 0.05654959753155708\n",
      "Epoch 4126, Loss: 0.3029739074409008, Final Batch Loss: 0.05902191624045372\n",
      "Epoch 4127, Loss: 0.14166346937417984, Final Batch Loss: 0.06504461169242859\n",
      "Epoch 4128, Loss: 0.09499631822109222, Final Batch Loss: 0.03166068345308304\n",
      "Epoch 4129, Loss: 0.13195521384477615, Final Batch Loss: 0.08946361392736435\n",
      "Epoch 4130, Loss: 0.17046662420034409, Final Batch Loss: 0.06579601019620895\n",
      "Epoch 4131, Loss: 0.19078988581895828, Final Batch Loss: 0.09974175691604614\n",
      "Epoch 4132, Loss: 0.12294803932309151, Final Batch Loss: 0.08388682454824448\n",
      "Epoch 4133, Loss: 0.07178514637053013, Final Batch Loss: 0.04503961279988289\n",
      "Epoch 4134, Loss: 0.16071176528930664, Final Batch Loss: 0.11069770902395248\n",
      "Epoch 4135, Loss: 0.15917759388685226, Final Batch Loss: 0.0926479920744896\n",
      "Epoch 4136, Loss: 0.13667477667331696, Final Batch Loss: 0.08644528687000275\n",
      "Epoch 4137, Loss: 0.12176856584846973, Final Batch Loss: 0.02097233571112156\n",
      "Epoch 4138, Loss: 0.15951241925358772, Final Batch Loss: 0.09741798788309097\n",
      "Epoch 4139, Loss: 0.11027386412024498, Final Batch Loss: 0.05121364817023277\n",
      "Epoch 4140, Loss: 0.1037272959947586, Final Batch Loss: 0.04519667476415634\n",
      "Epoch 4141, Loss: 0.06888220831751823, Final Batch Loss: 0.022044837474822998\n",
      "Epoch 4142, Loss: 0.20014406368136406, Final Batch Loss: 0.05933743342757225\n",
      "Epoch 4143, Loss: 0.11776647344231606, Final Batch Loss: 0.07532745599746704\n",
      "Epoch 4144, Loss: 0.10571746900677681, Final Batch Loss: 0.026758532971143723\n",
      "Epoch 4145, Loss: 0.12889079749584198, Final Batch Loss: 0.06171683222055435\n",
      "Epoch 4146, Loss: 0.0944841131567955, Final Batch Loss: 0.04888417199254036\n",
      "Epoch 4147, Loss: 0.08452898263931274, Final Batch Loss: 0.03782202675938606\n",
      "Epoch 4148, Loss: 0.12692326679825783, Final Batch Loss: 0.03956400230526924\n",
      "Epoch 4149, Loss: 0.14186788722872734, Final Batch Loss: 0.03829563036561012\n",
      "Epoch 4150, Loss: 0.08496137708425522, Final Batch Loss: 0.029302965849637985\n",
      "Epoch 4151, Loss: 0.12730109319090843, Final Batch Loss: 0.06944569945335388\n",
      "Epoch 4152, Loss: 0.10227342694997787, Final Batch Loss: 0.05682015046477318\n",
      "Epoch 4153, Loss: 0.14116549119353294, Final Batch Loss: 0.05203177407383919\n",
      "Epoch 4154, Loss: 0.11438800767064095, Final Batch Loss: 0.03723134472966194\n",
      "Epoch 4155, Loss: 0.14822030812501907, Final Batch Loss: 0.08135445415973663\n",
      "Epoch 4156, Loss: 0.15115810930728912, Final Batch Loss: 0.05112870782613754\n",
      "Epoch 4157, Loss: 0.1293332874774933, Final Batch Loss: 0.07199497520923615\n",
      "Epoch 4158, Loss: 0.15412604063749313, Final Batch Loss: 0.0934825986623764\n",
      "Epoch 4159, Loss: 0.17272963747382164, Final Batch Loss: 0.03486565873026848\n",
      "Epoch 4160, Loss: 0.09020145796239376, Final Batch Loss: 0.02613317035138607\n",
      "Epoch 4161, Loss: 0.38628847897052765, Final Batch Loss: 0.3279055058956146\n",
      "Epoch 4162, Loss: 0.121281698346138, Final Batch Loss: 0.03461587429046631\n",
      "Epoch 4163, Loss: 0.10576609894633293, Final Batch Loss: 0.05275866761803627\n",
      "Epoch 4164, Loss: 0.12141397595405579, Final Batch Loss: 0.06459037959575653\n",
      "Epoch 4165, Loss: 0.16649270057678223, Final Batch Loss: 0.08233554661273956\n",
      "Epoch 4166, Loss: 0.12175458297133446, Final Batch Loss: 0.045385878533124924\n",
      "Epoch 4167, Loss: 0.0816768016666174, Final Batch Loss: 0.026967225596308708\n",
      "Epoch 4168, Loss: 0.32638119161129, Final Batch Loss: 0.15701061487197876\n",
      "Epoch 4169, Loss: 0.09806359186768532, Final Batch Loss: 0.04423071816563606\n",
      "Epoch 4170, Loss: 0.09718584269285202, Final Batch Loss: 0.04375258833169937\n",
      "Epoch 4171, Loss: 0.13253239169716835, Final Batch Loss: 0.04808250442147255\n",
      "Epoch 4172, Loss: 0.10355003550648689, Final Batch Loss: 0.0437641367316246\n",
      "Epoch 4173, Loss: 0.1357985995709896, Final Batch Loss: 0.061332736164331436\n",
      "Epoch 4174, Loss: 0.16221793740987778, Final Batch Loss: 0.061978742480278015\n",
      "Epoch 4175, Loss: 0.14031092077493668, Final Batch Loss: 0.05749601125717163\n",
      "Epoch 4176, Loss: 0.14537815749645233, Final Batch Loss: 0.08053383976221085\n",
      "Epoch 4177, Loss: 0.08798648044466972, Final Batch Loss: 0.03965704143047333\n",
      "Epoch 4178, Loss: 0.1339065097272396, Final Batch Loss: 0.08214376121759415\n",
      "Epoch 4179, Loss: 0.17788384854793549, Final Batch Loss: 0.12614652514457703\n",
      "Epoch 4180, Loss: 0.24279003590345383, Final Batch Loss: 0.11132944375276566\n",
      "Epoch 4181, Loss: 0.1648949459195137, Final Batch Loss: 0.09753252565860748\n",
      "Epoch 4182, Loss: 0.15732184052467346, Final Batch Loss: 0.10654506087303162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4183, Loss: 0.14343509078025818, Final Batch Loss: 0.07024127244949341\n",
      "Epoch 4184, Loss: 0.13152218237519264, Final Batch Loss: 0.07949263602495193\n",
      "Epoch 4185, Loss: 0.11872884258627892, Final Batch Loss: 0.05725284293293953\n",
      "Epoch 4186, Loss: 0.19420567154884338, Final Batch Loss: 0.06395953893661499\n",
      "Epoch 4187, Loss: 0.08661473542451859, Final Batch Loss: 0.036740414798259735\n",
      "Epoch 4188, Loss: 0.11707624793052673, Final Batch Loss: 0.06943684071302414\n",
      "Epoch 4189, Loss: 0.14685231819748878, Final Batch Loss: 0.0467696376144886\n",
      "Epoch 4190, Loss: 0.13652432709932327, Final Batch Loss: 0.07961726188659668\n",
      "Epoch 4191, Loss: 0.15256568044424057, Final Batch Loss: 0.11537495255470276\n",
      "Epoch 4192, Loss: 0.1041409969329834, Final Batch Loss: 0.049692921340465546\n",
      "Epoch 4193, Loss: 0.22194069251418114, Final Batch Loss: 0.16402016580104828\n",
      "Epoch 4194, Loss: 0.15300023555755615, Final Batch Loss: 0.06626613438129425\n",
      "Epoch 4195, Loss: 0.20658549666404724, Final Batch Loss: 0.15253664553165436\n",
      "Epoch 4196, Loss: 0.09166842326521873, Final Batch Loss: 0.027543436735868454\n",
      "Epoch 4197, Loss: 0.13335413485765457, Final Batch Loss: 0.07206865400075912\n",
      "Epoch 4198, Loss: 0.07557159662246704, Final Batch Loss: 0.04131164774298668\n",
      "Epoch 4199, Loss: 0.10605637915432453, Final Batch Loss: 0.030997904017567635\n",
      "Epoch 4200, Loss: 0.13051143288612366, Final Batch Loss: 0.027994126081466675\n",
      "Epoch 4201, Loss: 0.08808572217822075, Final Batch Loss: 0.050105974078178406\n",
      "Epoch 4202, Loss: 0.09730935655534267, Final Batch Loss: 0.027076540514826775\n",
      "Epoch 4203, Loss: 0.10267099738121033, Final Batch Loss: 0.031295403838157654\n",
      "Epoch 4204, Loss: 0.25547291338443756, Final Batch Loss: 0.14668932557106018\n",
      "Epoch 4205, Loss: 0.1291603185236454, Final Batch Loss: 0.08723238110542297\n",
      "Epoch 4206, Loss: 0.14148889482021332, Final Batch Loss: 0.07223273068666458\n",
      "Epoch 4207, Loss: 0.10450150445103645, Final Batch Loss: 0.0538313128054142\n",
      "Epoch 4208, Loss: 0.20845221728086472, Final Batch Loss: 0.08510465919971466\n",
      "Epoch 4209, Loss: 0.2596905529499054, Final Batch Loss: 0.10320714116096497\n",
      "Epoch 4210, Loss: 0.21388942003250122, Final Batch Loss: 0.12427209317684174\n",
      "Epoch 4211, Loss: 0.1918259561061859, Final Batch Loss: 0.10803335160017014\n",
      "Epoch 4212, Loss: 0.2614063620567322, Final Batch Loss: 0.18855352699756622\n",
      "Epoch 4213, Loss: 0.12037833407521248, Final Batch Loss: 0.06619875133037567\n",
      "Epoch 4214, Loss: 0.11566542088985443, Final Batch Loss: 0.0533958375453949\n",
      "Epoch 4215, Loss: 0.1285517793148756, Final Batch Loss: 0.028962431475520134\n",
      "Epoch 4216, Loss: 0.09317474439740181, Final Batch Loss: 0.033374421298503876\n",
      "Epoch 4217, Loss: 0.18547941744327545, Final Batch Loss: 0.07331177592277527\n",
      "Epoch 4218, Loss: 0.14633060991764069, Final Batch Loss: 0.06297510862350464\n",
      "Epoch 4219, Loss: 0.0989808589220047, Final Batch Loss: 0.05680215358734131\n",
      "Epoch 4220, Loss: 0.20532428473234177, Final Batch Loss: 0.0494517907500267\n",
      "Epoch 4221, Loss: 0.12528768926858902, Final Batch Loss: 0.06234562397003174\n",
      "Epoch 4222, Loss: 0.19128935039043427, Final Batch Loss: 0.08116095513105392\n",
      "Epoch 4223, Loss: 0.06286776810884476, Final Batch Loss: 0.023523494601249695\n",
      "Epoch 4224, Loss: 0.1274697110056877, Final Batch Loss: 0.07077131420373917\n",
      "Epoch 4225, Loss: 0.09885115921497345, Final Batch Loss: 0.03408217430114746\n",
      "Epoch 4226, Loss: 0.06934252753853798, Final Batch Loss: 0.02958611398935318\n",
      "Epoch 4227, Loss: 0.20580727979540825, Final Batch Loss: 0.05110844597220421\n",
      "Epoch 4228, Loss: 0.1099221371114254, Final Batch Loss: 0.05607198178768158\n",
      "Epoch 4229, Loss: 0.11102452874183655, Final Batch Loss: 0.05900697410106659\n",
      "Epoch 4230, Loss: 0.11613350920379162, Final Batch Loss: 0.02162632904946804\n",
      "Epoch 4231, Loss: 0.11548703536391258, Final Batch Loss: 0.053063519299030304\n",
      "Epoch 4232, Loss: 0.17109785228967667, Final Batch Loss: 0.056930750608444214\n",
      "Epoch 4233, Loss: 0.180612251162529, Final Batch Loss: 0.11303557455539703\n",
      "Epoch 4234, Loss: 0.11619842797517776, Final Batch Loss: 0.04680421203374863\n",
      "Epoch 4235, Loss: 0.12441477924585342, Final Batch Loss: 0.052008748054504395\n",
      "Epoch 4236, Loss: 0.2507145181298256, Final Batch Loss: 0.12392012029886246\n",
      "Epoch 4237, Loss: 0.15205446630716324, Final Batch Loss: 0.06301774084568024\n",
      "Epoch 4238, Loss: 0.1934218592941761, Final Batch Loss: 0.059242088347673416\n",
      "Epoch 4239, Loss: 0.10144127160310745, Final Batch Loss: 0.040953438729047775\n",
      "Epoch 4240, Loss: 0.20296288654208183, Final Batch Loss: 0.15016280114650726\n",
      "Epoch 4241, Loss: 0.18819128721952438, Final Batch Loss: 0.11499401926994324\n",
      "Epoch 4242, Loss: 0.15195710957050323, Final Batch Loss: 0.07266663014888763\n",
      "Epoch 4243, Loss: 0.11107358708977699, Final Batch Loss: 0.048375632613897324\n",
      "Epoch 4244, Loss: 0.12981010600924492, Final Batch Loss: 0.058421988040208817\n",
      "Epoch 4245, Loss: 0.1072334535419941, Final Batch Loss: 0.06221659108996391\n",
      "Epoch 4246, Loss: 0.2311234176158905, Final Batch Loss: 0.13388971984386444\n",
      "Epoch 4247, Loss: 0.148364819586277, Final Batch Loss: 0.06883538514375687\n",
      "Epoch 4248, Loss: 0.18574578315019608, Final Batch Loss: 0.09263492375612259\n",
      "Epoch 4249, Loss: 0.17824877053499222, Final Batch Loss: 0.06617822498083115\n",
      "Epoch 4250, Loss: 0.10191987454891205, Final Batch Loss: 0.035131506621837616\n",
      "Epoch 4251, Loss: 0.10598194971680641, Final Batch Loss: 0.05778483301401138\n",
      "Epoch 4252, Loss: 0.10706016421318054, Final Batch Loss: 0.05728168785572052\n",
      "Epoch 4253, Loss: 0.15849139541387558, Final Batch Loss: 0.05109873414039612\n",
      "Epoch 4254, Loss: 0.1533026248216629, Final Batch Loss: 0.0812360867857933\n",
      "Epoch 4255, Loss: 0.13364297151565552, Final Batch Loss: 0.04582078754901886\n",
      "Epoch 4256, Loss: 0.18184512108564377, Final Batch Loss: 0.09741263836622238\n",
      "Epoch 4257, Loss: 0.16881044954061508, Final Batch Loss: 0.09045111387968063\n",
      "Epoch 4258, Loss: 0.1304897516965866, Final Batch Loss: 0.08431176096200943\n",
      "Epoch 4259, Loss: 0.1745052933692932, Final Batch Loss: 0.09652557969093323\n",
      "Epoch 4260, Loss: 0.14752063155174255, Final Batch Loss: 0.07459838688373566\n",
      "Epoch 4261, Loss: 0.08148367330431938, Final Batch Loss: 0.015005465596914291\n",
      "Epoch 4262, Loss: 0.14686887338757515, Final Batch Loss: 0.05931961163878441\n",
      "Epoch 4263, Loss: 0.10135924443602562, Final Batch Loss: 0.04322412237524986\n",
      "Epoch 4264, Loss: 0.11929507926106453, Final Batch Loss: 0.0485650934278965\n",
      "Epoch 4265, Loss: 0.22085420787334442, Final Batch Loss: 0.10152554512023926\n",
      "Epoch 4266, Loss: 0.13580472767353058, Final Batch Loss: 0.03777962177991867\n",
      "Epoch 4267, Loss: 0.11384151503443718, Final Batch Loss: 0.034191351383924484\n",
      "Epoch 4268, Loss: 0.09497318789362907, Final Batch Loss: 0.04718582704663277\n",
      "Epoch 4269, Loss: 0.15926867723464966, Final Batch Loss: 0.11389626562595367\n",
      "Epoch 4270, Loss: 0.15348225459456444, Final Batch Loss: 0.03169778361916542\n",
      "Epoch 4271, Loss: 0.17201092839241028, Final Batch Loss: 0.06760252267122269\n",
      "Epoch 4272, Loss: 0.10111752152442932, Final Batch Loss: 0.05671507865190506\n",
      "Epoch 4273, Loss: 0.14184106141328812, Final Batch Loss: 0.07381495088338852\n",
      "Epoch 4274, Loss: 0.26139985769987106, Final Batch Loss: 0.16999417543411255\n",
      "Epoch 4275, Loss: 0.08944975957274437, Final Batch Loss: 0.034082140773534775\n",
      "Epoch 4276, Loss: 0.17216087877750397, Final Batch Loss: 0.10199552029371262\n",
      "Epoch 4277, Loss: 0.24900027364492416, Final Batch Loss: 0.10197620838880539\n",
      "Epoch 4278, Loss: 0.2258172482252121, Final Batch Loss: 0.1315799206495285\n",
      "Epoch 4279, Loss: 0.07421410083770752, Final Batch Loss: 0.0336887389421463\n",
      "Epoch 4280, Loss: 0.23800989985466003, Final Batch Loss: 0.13920360803604126\n",
      "Epoch 4281, Loss: 0.14292608574032784, Final Batch Loss: 0.08412809669971466\n",
      "Epoch 4282, Loss: 0.1705615594983101, Final Batch Loss: 0.11217450350522995\n",
      "Epoch 4283, Loss: 0.06254362314939499, Final Batch Loss: 0.022995688021183014\n",
      "Epoch 4284, Loss: 0.12529874220490456, Final Batch Loss: 0.06296725571155548\n",
      "Epoch 4285, Loss: 0.18517020344734192, Final Batch Loss: 0.10151185095310211\n",
      "Epoch 4286, Loss: 0.15311884135007858, Final Batch Loss: 0.10524637252092361\n",
      "Epoch 4287, Loss: 0.09411051124334335, Final Batch Loss: 0.0331888422369957\n",
      "Epoch 4288, Loss: 0.11587485671043396, Final Batch Loss: 0.053688421845436096\n",
      "Epoch 4289, Loss: 0.13833057507872581, Final Batch Loss: 0.08692008256912231\n",
      "Epoch 4290, Loss: 0.2353873997926712, Final Batch Loss: 0.07279461622238159\n",
      "Epoch 4291, Loss: 0.17966965213418007, Final Batch Loss: 0.05686816945672035\n",
      "Epoch 4292, Loss: 0.11881431192159653, Final Batch Loss: 0.07658087462186813\n",
      "Epoch 4293, Loss: 0.16591781377792358, Final Batch Loss: 0.053456321358680725\n",
      "Epoch 4294, Loss: 0.11824261769652367, Final Batch Loss: 0.05578242614865303\n",
      "Epoch 4295, Loss: 0.17516570538282394, Final Batch Loss: 0.13215522468090057\n",
      "Epoch 4296, Loss: 0.07229077816009521, Final Batch Loss: 0.039201200008392334\n",
      "Epoch 4297, Loss: 0.11826298013329506, Final Batch Loss: 0.06464511901140213\n",
      "Epoch 4298, Loss: 0.09736105613410473, Final Batch Loss: 0.022516420111060143\n",
      "Epoch 4299, Loss: 0.09172596409916878, Final Batch Loss: 0.025700975209474564\n",
      "Epoch 4300, Loss: 0.08725421130657196, Final Batch Loss: 0.04177926480770111\n",
      "Epoch 4301, Loss: 0.10069688782095909, Final Batch Loss: 0.04203419387340546\n",
      "Epoch 4302, Loss: 0.0986734926700592, Final Batch Loss: 0.04830397292971611\n",
      "Epoch 4303, Loss: 0.13095645606517792, Final Batch Loss: 0.0993017628788948\n",
      "Epoch 4304, Loss: 0.17875050753355026, Final Batch Loss: 0.12899532914161682\n",
      "Epoch 4305, Loss: 0.11939531564712524, Final Batch Loss: 0.07828790694475174\n",
      "Epoch 4306, Loss: 0.20071659982204437, Final Batch Loss: 0.09395156055688858\n",
      "Epoch 4307, Loss: 0.14064686372876167, Final Batch Loss: 0.0927518904209137\n",
      "Epoch 4308, Loss: 0.09058498963713646, Final Batch Loss: 0.021449793130159378\n",
      "Epoch 4309, Loss: 0.0963365025818348, Final Batch Loss: 0.055488910526037216\n",
      "Epoch 4310, Loss: 0.1200682520866394, Final Batch Loss: 0.06026092544198036\n",
      "Epoch 4311, Loss: 0.086335688829422, Final Batch Loss: 0.04055805131793022\n",
      "Epoch 4312, Loss: 0.12731775641441345, Final Batch Loss: 0.058006078004837036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4313, Loss: 0.07874132506549358, Final Batch Loss: 0.0278316717594862\n",
      "Epoch 4314, Loss: 0.11769098043441772, Final Batch Loss: 0.03617525100708008\n",
      "Epoch 4315, Loss: 0.1747402511537075, Final Batch Loss: 0.059030238538980484\n",
      "Epoch 4316, Loss: 0.10318755730986595, Final Batch Loss: 0.04872783645987511\n",
      "Epoch 4317, Loss: 0.1446773298084736, Final Batch Loss: 0.09590282291173935\n",
      "Epoch 4318, Loss: 0.14265213161706924, Final Batch Loss: 0.06071275472640991\n",
      "Epoch 4319, Loss: 0.12152929604053497, Final Batch Loss: 0.03711419552564621\n",
      "Epoch 4320, Loss: 0.07606743276119232, Final Batch Loss: 0.031191378831863403\n",
      "Epoch 4321, Loss: 0.12498960644006729, Final Batch Loss: 0.07475630193948746\n",
      "Epoch 4322, Loss: 0.13604862242937088, Final Batch Loss: 0.051018938422203064\n",
      "Epoch 4323, Loss: 0.1959959790110588, Final Batch Loss: 0.0762123242020607\n",
      "Epoch 4324, Loss: 0.1890343390405178, Final Batch Loss: 0.1406179666519165\n",
      "Epoch 4325, Loss: 0.09083176776766777, Final Batch Loss: 0.03904227539896965\n",
      "Epoch 4326, Loss: 0.1412632241845131, Final Batch Loss: 0.06455963104963303\n",
      "Epoch 4327, Loss: 0.1563364863395691, Final Batch Loss: 0.07903297245502472\n",
      "Epoch 4328, Loss: 0.1433427333831787, Final Batch Loss: 0.08962198346853256\n",
      "Epoch 4329, Loss: 0.1747068539261818, Final Batch Loss: 0.10351990908384323\n",
      "Epoch 4330, Loss: 0.1407139003276825, Final Batch Loss: 0.07294175028800964\n",
      "Epoch 4331, Loss: 0.09691163152456284, Final Batch Loss: 0.06092597916722298\n",
      "Epoch 4332, Loss: 0.13743527233600616, Final Batch Loss: 0.06004032492637634\n",
      "Epoch 4333, Loss: 0.13074709847569466, Final Batch Loss: 0.052566047757864\n",
      "Epoch 4334, Loss: 0.11112241074442863, Final Batch Loss: 0.048125993460416794\n",
      "Epoch 4335, Loss: 0.1819840595126152, Final Batch Loss: 0.0982276201248169\n",
      "Epoch 4336, Loss: 0.1700330562889576, Final Batch Loss: 0.11107132583856583\n",
      "Epoch 4337, Loss: 0.14954669773578644, Final Batch Loss: 0.10451310873031616\n",
      "Epoch 4338, Loss: 0.34012646973133087, Final Batch Loss: 0.18107371032238007\n",
      "Epoch 4339, Loss: 0.2037830501794815, Final Batch Loss: 0.05833020806312561\n",
      "Epoch 4340, Loss: 0.20598170906305313, Final Batch Loss: 0.0655740424990654\n",
      "Epoch 4341, Loss: 0.1895088404417038, Final Batch Loss: 0.11358563601970673\n",
      "Epoch 4342, Loss: 0.2545372322201729, Final Batch Loss: 0.1456884741783142\n",
      "Epoch 4343, Loss: 0.11690603196620941, Final Batch Loss: 0.05055627226829529\n",
      "Epoch 4344, Loss: 0.14568911492824554, Final Batch Loss: 0.06764063239097595\n",
      "Epoch 4345, Loss: 0.1666768714785576, Final Batch Loss: 0.0890020877122879\n",
      "Epoch 4346, Loss: 0.08771513402462006, Final Batch Loss: 0.04133574664592743\n",
      "Epoch 4347, Loss: 0.12145133689045906, Final Batch Loss: 0.04165508225560188\n",
      "Epoch 4348, Loss: 0.15943393111228943, Final Batch Loss: 0.062071792781353\n",
      "Epoch 4349, Loss: 0.08559044823050499, Final Batch Loss: 0.046965353190898895\n",
      "Epoch 4350, Loss: 0.12621762603521347, Final Batch Loss: 0.07496581971645355\n",
      "Epoch 4351, Loss: 0.11755667254328728, Final Batch Loss: 0.04514642432332039\n",
      "Epoch 4352, Loss: 0.10278168320655823, Final Batch Loss: 0.05016900226473808\n",
      "Epoch 4353, Loss: 0.1413552239537239, Final Batch Loss: 0.07592408359050751\n",
      "Epoch 4354, Loss: 0.13281002640724182, Final Batch Loss: 0.06418336927890778\n",
      "Epoch 4355, Loss: 0.11106593534350395, Final Batch Loss: 0.05043806508183479\n",
      "Epoch 4356, Loss: 0.0834839902818203, Final Batch Loss: 0.050171732902526855\n",
      "Epoch 4357, Loss: 0.11278106644749641, Final Batch Loss: 0.0656239315867424\n",
      "Epoch 4358, Loss: 0.12156148627400398, Final Batch Loss: 0.06414557993412018\n",
      "Epoch 4359, Loss: 0.0725853368639946, Final Batch Loss: 0.028165306895971298\n",
      "Epoch 4360, Loss: 0.1232835166156292, Final Batch Loss: 0.0712427943944931\n",
      "Epoch 4361, Loss: 0.1128648892045021, Final Batch Loss: 0.03330802917480469\n",
      "Epoch 4362, Loss: 0.08776155114173889, Final Batch Loss: 0.043599363416433334\n",
      "Epoch 4363, Loss: 0.13351716846227646, Final Batch Loss: 0.07314059138298035\n",
      "Epoch 4364, Loss: 0.1009608842432499, Final Batch Loss: 0.047129493206739426\n",
      "Epoch 4365, Loss: 0.188181322067976, Final Batch Loss: 0.04828214272856712\n",
      "Epoch 4366, Loss: 0.18962638825178146, Final Batch Loss: 0.1211358979344368\n",
      "Epoch 4367, Loss: 0.1229800134897232, Final Batch Loss: 0.04493703693151474\n",
      "Epoch 4368, Loss: 0.15200531855225563, Final Batch Loss: 0.05914158746600151\n",
      "Epoch 4369, Loss: 0.08953463658690453, Final Batch Loss: 0.03165170177817345\n",
      "Epoch 4370, Loss: 0.15837480500340462, Final Batch Loss: 0.11680947989225388\n",
      "Epoch 4371, Loss: 0.1875256523489952, Final Batch Loss: 0.054189182817935944\n",
      "Epoch 4372, Loss: 0.22037089616060257, Final Batch Loss: 0.13331235945224762\n",
      "Epoch 4373, Loss: 0.10334368795156479, Final Batch Loss: 0.03463616222143173\n",
      "Epoch 4374, Loss: 0.10077466070652008, Final Batch Loss: 0.030092380940914154\n",
      "Epoch 4375, Loss: 0.1382192112505436, Final Batch Loss: 0.03591391071677208\n",
      "Epoch 4376, Loss: 0.09806627593934536, Final Batch Loss: 0.02503620646893978\n",
      "Epoch 4377, Loss: 0.1147720143198967, Final Batch Loss: 0.06003010272979736\n",
      "Epoch 4378, Loss: 0.1538231149315834, Final Batch Loss: 0.07258593291044235\n",
      "Epoch 4379, Loss: 0.21051140874624252, Final Batch Loss: 0.11400291323661804\n",
      "Epoch 4380, Loss: 0.16561774164438248, Final Batch Loss: 0.06723485887050629\n",
      "Epoch 4381, Loss: 0.1778789609670639, Final Batch Loss: 0.07757831364870071\n",
      "Epoch 4382, Loss: 0.09532533586025238, Final Batch Loss: 0.03647603467106819\n",
      "Epoch 4383, Loss: 0.1881457418203354, Final Batch Loss: 0.11621607840061188\n",
      "Epoch 4384, Loss: 0.11796870455145836, Final Batch Loss: 0.052149008959531784\n",
      "Epoch 4385, Loss: 0.08793787285685539, Final Batch Loss: 0.018733743578195572\n",
      "Epoch 4386, Loss: 0.15488158911466599, Final Batch Loss: 0.06786855310201645\n",
      "Epoch 4387, Loss: 0.12278489768505096, Final Batch Loss: 0.06019362062215805\n",
      "Epoch 4388, Loss: 0.12385162711143494, Final Batch Loss: 0.08673614263534546\n",
      "Epoch 4389, Loss: 0.16591088473796844, Final Batch Loss: 0.09646309912204742\n",
      "Epoch 4390, Loss: 0.08527453616261482, Final Batch Loss: 0.027274996042251587\n",
      "Epoch 4391, Loss: 0.13482771068811417, Final Batch Loss: 0.03364752233028412\n",
      "Epoch 4392, Loss: 0.09656974673271179, Final Batch Loss: 0.06648741662502289\n",
      "Epoch 4393, Loss: 0.076506401412189, Final Batch Loss: 0.014695274643599987\n",
      "Epoch 4394, Loss: 0.11706361919641495, Final Batch Loss: 0.0721287801861763\n",
      "Epoch 4395, Loss: 0.10713379085063934, Final Batch Loss: 0.0549466647207737\n",
      "Epoch 4396, Loss: 0.08569153398275375, Final Batch Loss: 0.05254106968641281\n",
      "Epoch 4397, Loss: 0.09380518272519112, Final Batch Loss: 0.053849752992391586\n",
      "Epoch 4398, Loss: 0.1207888051867485, Final Batch Loss: 0.06743502616882324\n",
      "Epoch 4399, Loss: 0.13968464359641075, Final Batch Loss: 0.09767242521047592\n",
      "Epoch 4400, Loss: 0.14565323293209076, Final Batch Loss: 0.06271066516637802\n",
      "Epoch 4401, Loss: 0.15395720303058624, Final Batch Loss: 0.08583037555217743\n",
      "Epoch 4402, Loss: 0.09224197641015053, Final Batch Loss: 0.04325329512357712\n",
      "Epoch 4403, Loss: 0.10116129741072655, Final Batch Loss: 0.054659612476825714\n",
      "Epoch 4404, Loss: 0.08824291452765465, Final Batch Loss: 0.030644185841083527\n",
      "Epoch 4405, Loss: 0.10071375221014023, Final Batch Loss: 0.029342278838157654\n",
      "Epoch 4406, Loss: 0.09030336141586304, Final Batch Loss: 0.01774972677230835\n",
      "Epoch 4407, Loss: 0.16261369735002518, Final Batch Loss: 0.07704757153987885\n",
      "Epoch 4408, Loss: 0.09044583328068256, Final Batch Loss: 0.029174206778407097\n",
      "Epoch 4409, Loss: 0.13591792806982994, Final Batch Loss: 0.060402851551771164\n",
      "Epoch 4410, Loss: 0.09639965184032917, Final Batch Loss: 0.06574948132038116\n",
      "Epoch 4411, Loss: 0.1632455810904503, Final Batch Loss: 0.0711597427725792\n",
      "Epoch 4412, Loss: 0.1469656229019165, Final Batch Loss: 0.06779997795820236\n",
      "Epoch 4413, Loss: 0.0673608835786581, Final Batch Loss: 0.025491809472441673\n",
      "Epoch 4414, Loss: 0.09252757206559181, Final Batch Loss: 0.031665463000535965\n",
      "Epoch 4415, Loss: 0.0835289005190134, Final Batch Loss: 0.028988583013415337\n",
      "Epoch 4416, Loss: 0.13206129521131516, Final Batch Loss: 0.056646622717380524\n",
      "Epoch 4417, Loss: 0.09983693808317184, Final Batch Loss: 0.03576117753982544\n",
      "Epoch 4418, Loss: 0.07171337306499481, Final Batch Loss: 0.03134353831410408\n",
      "Epoch 4419, Loss: 0.1260311771184206, Final Batch Loss: 0.0949535220861435\n",
      "Epoch 4420, Loss: 0.10460007935762405, Final Batch Loss: 0.04035681486129761\n",
      "Epoch 4421, Loss: 0.1242227852344513, Final Batch Loss: 0.07783588767051697\n",
      "Epoch 4422, Loss: 0.12367753684520721, Final Batch Loss: 0.05659417062997818\n",
      "Epoch 4423, Loss: 0.11478209868073463, Final Batch Loss: 0.057973794639110565\n",
      "Epoch 4424, Loss: 0.10819889791309834, Final Batch Loss: 0.07835721969604492\n",
      "Epoch 4425, Loss: 0.13438426703214645, Final Batch Loss: 0.07008649408817291\n",
      "Epoch 4426, Loss: 0.10366037487983704, Final Batch Loss: 0.04569435864686966\n",
      "Epoch 4427, Loss: 0.050237495452165604, Final Batch Loss: 0.02393379807472229\n",
      "Epoch 4428, Loss: 0.13798020035028458, Final Batch Loss: 0.04376044124364853\n",
      "Epoch 4429, Loss: 0.14343660697340965, Final Batch Loss: 0.08241420239210129\n",
      "Epoch 4430, Loss: 0.1515851616859436, Final Batch Loss: 0.08468944579362869\n",
      "Epoch 4431, Loss: 0.1601584404706955, Final Batch Loss: 0.07660651206970215\n",
      "Epoch 4432, Loss: 0.1942935511469841, Final Batch Loss: 0.09477490186691284\n",
      "Epoch 4433, Loss: 0.15940043330192566, Final Batch Loss: 0.06963976472616196\n",
      "Epoch 4434, Loss: 0.13167111948132515, Final Batch Loss: 0.08251234143972397\n",
      "Epoch 4435, Loss: 0.0682340357452631, Final Batch Loss: 0.024690916761755943\n",
      "Epoch 4436, Loss: 0.12497011944651604, Final Batch Loss: 0.06737606227397919\n",
      "Epoch 4437, Loss: 0.09624961763620377, Final Batch Loss: 0.04240873083472252\n",
      "Epoch 4438, Loss: 0.16864990815520287, Final Batch Loss: 0.10962166637182236\n",
      "Epoch 4439, Loss: 0.0747308898717165, Final Batch Loss: 0.04506115987896919\n",
      "Epoch 4440, Loss: 0.12918401882052422, Final Batch Loss: 0.05152658000588417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4441, Loss: 0.06850023753941059, Final Batch Loss: 0.027676189318299294\n",
      "Epoch 4442, Loss: 0.1408853828907013, Final Batch Loss: 0.07564923912286758\n",
      "Epoch 4443, Loss: 0.12711875513195992, Final Batch Loss: 0.07225389778614044\n",
      "Epoch 4444, Loss: 0.13668370246887207, Final Batch Loss: 0.07736250758171082\n",
      "Epoch 4445, Loss: 0.12323680147528648, Final Batch Loss: 0.0847555473446846\n",
      "Epoch 4446, Loss: 0.08203722536563873, Final Batch Loss: 0.04311709851026535\n",
      "Epoch 4447, Loss: 0.09721450880169868, Final Batch Loss: 0.057347215712070465\n",
      "Epoch 4448, Loss: 0.12281909212470055, Final Batch Loss: 0.0633118748664856\n",
      "Epoch 4449, Loss: 0.09057934395968914, Final Batch Loss: 0.06013968214392662\n",
      "Epoch 4450, Loss: 0.1247241422533989, Final Batch Loss: 0.04987107217311859\n",
      "Epoch 4451, Loss: 0.18187323585152626, Final Batch Loss: 0.13560818135738373\n",
      "Epoch 4452, Loss: 0.18908321857452393, Final Batch Loss: 0.08605930954217911\n",
      "Epoch 4453, Loss: 0.08852588757872581, Final Batch Loss: 0.04907640442252159\n",
      "Epoch 4454, Loss: 0.09970973804593086, Final Batch Loss: 0.042080000042915344\n",
      "Epoch 4455, Loss: 0.16674964874982834, Final Batch Loss: 0.07951239496469498\n",
      "Epoch 4456, Loss: 0.09743458032608032, Final Batch Loss: 0.04736833646893501\n",
      "Epoch 4457, Loss: 0.10807837545871735, Final Batch Loss: 0.07249347865581512\n",
      "Epoch 4458, Loss: 0.07621568813920021, Final Batch Loss: 0.03562448173761368\n",
      "Epoch 4459, Loss: 0.09156091511249542, Final Batch Loss: 0.058573588728904724\n",
      "Epoch 4460, Loss: 0.11655208468437195, Final Batch Loss: 0.07953828573226929\n",
      "Epoch 4461, Loss: 0.14152372628450394, Final Batch Loss: 0.07024393230676651\n",
      "Epoch 4462, Loss: 0.09290256723761559, Final Batch Loss: 0.018923018127679825\n",
      "Epoch 4463, Loss: 0.16650620847940445, Final Batch Loss: 0.048924222588539124\n",
      "Epoch 4464, Loss: 0.1431107148528099, Final Batch Loss: 0.09226614236831665\n",
      "Epoch 4465, Loss: 0.09797737002372742, Final Batch Loss: 0.03899917006492615\n",
      "Epoch 4466, Loss: 0.07464109733700752, Final Batch Loss: 0.03369883447885513\n",
      "Epoch 4467, Loss: 0.10538304969668388, Final Batch Loss: 0.03839773312211037\n",
      "Epoch 4468, Loss: 0.1746004819869995, Final Batch Loss: 0.07251901924610138\n",
      "Epoch 4469, Loss: 0.11990561708807945, Final Batch Loss: 0.05337497964501381\n",
      "Epoch 4470, Loss: 0.12187840789556503, Final Batch Loss: 0.0349663645029068\n",
      "Epoch 4471, Loss: 0.10403376445174217, Final Batch Loss: 0.051438361406326294\n",
      "Epoch 4472, Loss: 0.11648641899228096, Final Batch Loss: 0.06612319499254227\n",
      "Epoch 4473, Loss: 0.11088832095265388, Final Batch Loss: 0.06400880962610245\n",
      "Epoch 4474, Loss: 0.11760017275810242, Final Batch Loss: 0.03598073869943619\n",
      "Epoch 4475, Loss: 0.1076272763311863, Final Batch Loss: 0.03848030045628548\n",
      "Epoch 4476, Loss: 0.21983309090137482, Final Batch Loss: 0.06586076319217682\n",
      "Epoch 4477, Loss: 0.1406785249710083, Final Batch Loss: 0.042425185441970825\n",
      "Epoch 4478, Loss: 0.09535269066691399, Final Batch Loss: 0.041346464306116104\n",
      "Epoch 4479, Loss: 0.18557775765657425, Final Batch Loss: 0.09630361199378967\n",
      "Epoch 4480, Loss: 0.11479073762893677, Final Batch Loss: 0.08223129063844681\n",
      "Epoch 4481, Loss: 0.10555532574653625, Final Batch Loss: 0.055168814957141876\n",
      "Epoch 4482, Loss: 0.16266046836972237, Final Batch Loss: 0.043714653700590134\n",
      "Epoch 4483, Loss: 0.12152682617306709, Final Batch Loss: 0.06796476244926453\n",
      "Epoch 4484, Loss: 0.15895065665245056, Final Batch Loss: 0.06610918790102005\n",
      "Epoch 4485, Loss: 0.23777000606060028, Final Batch Loss: 0.18621458113193512\n",
      "Epoch 4486, Loss: 0.14985375106334686, Final Batch Loss: 0.08744362741708755\n",
      "Epoch 4487, Loss: 0.20253430679440498, Final Batch Loss: 0.14966130256652832\n",
      "Epoch 4488, Loss: 0.1685328334569931, Final Batch Loss: 0.0814780592918396\n",
      "Epoch 4489, Loss: 0.14906687289476395, Final Batch Loss: 0.05663394182920456\n",
      "Epoch 4490, Loss: 0.0942838303744793, Final Batch Loss: 0.04424912855029106\n",
      "Epoch 4491, Loss: 0.3744472563266754, Final Batch Loss: 0.1444530487060547\n",
      "Epoch 4492, Loss: 0.059735774993896484, Final Batch Loss: 0.03264499828219414\n",
      "Epoch 4493, Loss: 0.10436177998781204, Final Batch Loss: 0.05289991945028305\n",
      "Epoch 4494, Loss: 0.0961553193628788, Final Batch Loss: 0.04576550051569939\n",
      "Epoch 4495, Loss: 0.15777835994958878, Final Batch Loss: 0.06655482947826385\n",
      "Epoch 4496, Loss: 0.18875883519649506, Final Batch Loss: 0.1161070391535759\n",
      "Epoch 4497, Loss: 0.09530841559171677, Final Batch Loss: 0.04355622082948685\n",
      "Epoch 4498, Loss: 0.10766152665019035, Final Batch Loss: 0.060851216316223145\n",
      "Epoch 4499, Loss: 0.12558859586715698, Final Batch Loss: 0.07143066078424454\n",
      "Epoch 4500, Loss: 0.30712661147117615, Final Batch Loss: 0.10293878614902496\n",
      "Epoch 4501, Loss: 0.1270848773419857, Final Batch Loss: 0.0691155195236206\n",
      "Epoch 4502, Loss: 0.07168873772025108, Final Batch Loss: 0.025237731635570526\n",
      "Epoch 4503, Loss: 0.1271621286869049, Final Batch Loss: 0.07363767921924591\n",
      "Epoch 4504, Loss: 0.09161003306508064, Final Batch Loss: 0.044481974095106125\n",
      "Epoch 4505, Loss: 0.12121174111962318, Final Batch Loss: 0.05902568995952606\n",
      "Epoch 4506, Loss: 0.1520121470093727, Final Batch Loss: 0.09369184076786041\n",
      "Epoch 4507, Loss: 0.08194036968052387, Final Batch Loss: 0.051536381244659424\n",
      "Epoch 4508, Loss: 0.08491814322769642, Final Batch Loss: 0.028658611699938774\n",
      "Epoch 4509, Loss: 0.11319941282272339, Final Batch Loss: 0.06121436133980751\n",
      "Epoch 4510, Loss: 0.2056548371911049, Final Batch Loss: 0.12945975363254547\n",
      "Epoch 4511, Loss: 0.09107688441872597, Final Batch Loss: 0.049803994596004486\n",
      "Epoch 4512, Loss: 0.11441197246313095, Final Batch Loss: 0.046874143183231354\n",
      "Epoch 4513, Loss: 0.13808263465762138, Final Batch Loss: 0.07573018968105316\n",
      "Epoch 4514, Loss: 0.12094736471772194, Final Batch Loss: 0.06589395552873611\n",
      "Epoch 4515, Loss: 0.1257631815969944, Final Batch Loss: 0.06029787287116051\n",
      "Epoch 4516, Loss: 0.0933687575161457, Final Batch Loss: 0.04295606538653374\n",
      "Epoch 4517, Loss: 0.09964673966169357, Final Batch Loss: 0.038580264896154404\n",
      "Epoch 4518, Loss: 0.12878981977701187, Final Batch Loss: 0.08012514561414719\n",
      "Epoch 4519, Loss: 0.19996526092290878, Final Batch Loss: 0.1307540386915207\n",
      "Epoch 4520, Loss: 0.08949259668588638, Final Batch Loss: 0.040925532579422\n",
      "Epoch 4521, Loss: 0.10834617540240288, Final Batch Loss: 0.05584380775690079\n",
      "Epoch 4522, Loss: 0.16818322241306305, Final Batch Loss: 0.09005804359912872\n",
      "Epoch 4523, Loss: 0.160371333360672, Final Batch Loss: 0.09752193093299866\n",
      "Epoch 4524, Loss: 0.1290086843073368, Final Batch Loss: 0.03967538848519325\n",
      "Epoch 4525, Loss: 0.13795268163084984, Final Batch Loss: 0.09994512051343918\n",
      "Epoch 4526, Loss: 0.09420976042747498, Final Batch Loss: 0.05185200646519661\n",
      "Epoch 4527, Loss: 0.07842797040939331, Final Batch Loss: 0.03707284480333328\n",
      "Epoch 4528, Loss: 0.11340022832155228, Final Batch Loss: 0.056801937520504\n",
      "Epoch 4529, Loss: 0.16839062795042992, Final Batch Loss: 0.12541396915912628\n",
      "Epoch 4530, Loss: 0.16900695115327835, Final Batch Loss: 0.06970509886741638\n",
      "Epoch 4531, Loss: 0.08398686349391937, Final Batch Loss: 0.032697804272174835\n",
      "Epoch 4532, Loss: 0.12446052953600883, Final Batch Loss: 0.056458648294210434\n",
      "Epoch 4533, Loss: 0.05994753539562225, Final Batch Loss: 0.030859224498271942\n",
      "Epoch 4534, Loss: 0.17570598423480988, Final Batch Loss: 0.09219537675380707\n",
      "Epoch 4535, Loss: 0.10671982914209366, Final Batch Loss: 0.054970234632492065\n",
      "Epoch 4536, Loss: 0.10092176869511604, Final Batch Loss: 0.05343989282846451\n",
      "Epoch 4537, Loss: 0.17017873004078865, Final Batch Loss: 0.12988698482513428\n",
      "Epoch 4538, Loss: 0.145442396402359, Final Batch Loss: 0.10995124280452728\n",
      "Epoch 4539, Loss: 0.13062454015016556, Final Batch Loss: 0.040744297206401825\n",
      "Epoch 4540, Loss: 0.13041069358587265, Final Batch Loss: 0.051593512296676636\n",
      "Epoch 4541, Loss: 0.251261405646801, Final Batch Loss: 0.21267306804656982\n",
      "Epoch 4542, Loss: 0.12073874101042747, Final Batch Loss: 0.072147898375988\n",
      "Epoch 4543, Loss: 0.15721555054187775, Final Batch Loss: 0.04534706473350525\n",
      "Epoch 4544, Loss: 0.08568848669528961, Final Batch Loss: 0.015947580337524414\n",
      "Epoch 4545, Loss: 0.17333078756928444, Final Batch Loss: 0.12053069472312927\n",
      "Epoch 4546, Loss: 0.13821806013584137, Final Batch Loss: 0.0700756162405014\n",
      "Epoch 4547, Loss: 0.22588038444519043, Final Batch Loss: 0.11820358037948608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4548, Loss: 0.1299164853990078, Final Batch Loss: 0.05264822766184807\n",
      "Epoch 4549, Loss: 0.10769033804535866, Final Batch Loss: 0.06004346162080765\n",
      "Epoch 4550, Loss: 0.08229002729058266, Final Batch Loss: 0.04990306496620178\n",
      "Epoch 4551, Loss: 0.17719152197241783, Final Batch Loss: 0.11726468056440353\n",
      "Epoch 4552, Loss: 0.1386481672525406, Final Batch Loss: 0.08513427525758743\n",
      "Epoch 4553, Loss: 0.12124048918485641, Final Batch Loss: 0.06690182536840439\n",
      "Epoch 4554, Loss: 0.098529152572155, Final Batch Loss: 0.037217818200588226\n",
      "Epoch 4555, Loss: 0.12956316024065018, Final Batch Loss: 0.06507398933172226\n",
      "Epoch 4556, Loss: 0.1247534267604351, Final Batch Loss: 0.05795018747448921\n",
      "Epoch 4557, Loss: 0.11194708943367004, Final Batch Loss: 0.0613892637193203\n",
      "Epoch 4558, Loss: 0.17292149737477303, Final Batch Loss: 0.11555246263742447\n",
      "Epoch 4559, Loss: 0.17945250868797302, Final Batch Loss: 0.11821209639310837\n",
      "Epoch 4560, Loss: 0.08131186291575432, Final Batch Loss: 0.038506846874952316\n",
      "Epoch 4561, Loss: 0.15459686517715454, Final Batch Loss: 0.06541217863559723\n",
      "Epoch 4562, Loss: 0.14460389316082, Final Batch Loss: 0.07173832505941391\n",
      "Epoch 4563, Loss: 0.19335978850722313, Final Batch Loss: 0.13993103802204132\n",
      "Epoch 4564, Loss: 0.13072915747761726, Final Batch Loss: 0.08208784461021423\n",
      "Epoch 4565, Loss: 0.18026351928710938, Final Batch Loss: 0.11210668832063675\n",
      "Epoch 4566, Loss: 0.1395592764019966, Final Batch Loss: 0.092839315533638\n",
      "Epoch 4567, Loss: 0.14469090849161148, Final Batch Loss: 0.051340118050575256\n",
      "Epoch 4568, Loss: 0.1452707201242447, Final Batch Loss: 0.08388646692037582\n",
      "Epoch 4569, Loss: 0.12494779378175735, Final Batch Loss: 0.07051917910575867\n",
      "Epoch 4570, Loss: 0.16289684176445007, Final Batch Loss: 0.08471929281949997\n",
      "Epoch 4571, Loss: 0.09339236095547676, Final Batch Loss: 0.026164930313825607\n",
      "Epoch 4572, Loss: 0.0970909483730793, Final Batch Loss: 0.03493531793355942\n",
      "Epoch 4573, Loss: 0.08241666480898857, Final Batch Loss: 0.03965382277965546\n",
      "Epoch 4574, Loss: 0.10419047623872757, Final Batch Loss: 0.031534381210803986\n",
      "Epoch 4575, Loss: 0.2118557170033455, Final Batch Loss: 0.1013035699725151\n",
      "Epoch 4576, Loss: 0.1398376002907753, Final Batch Loss: 0.09136330336332321\n",
      "Epoch 4577, Loss: 0.15366390347480774, Final Batch Loss: 0.0757080614566803\n",
      "Epoch 4578, Loss: 0.14096837863326073, Final Batch Loss: 0.07895299792289734\n",
      "Epoch 4579, Loss: 0.17387231439352036, Final Batch Loss: 0.12115482240915298\n",
      "Epoch 4580, Loss: 0.17766664922237396, Final Batch Loss: 0.09588251262903214\n",
      "Epoch 4581, Loss: 0.178982425481081, Final Batch Loss: 0.04163256660103798\n",
      "Epoch 4582, Loss: 0.23250598460435867, Final Batch Loss: 0.10843782871961594\n",
      "Epoch 4583, Loss: 0.2120332345366478, Final Batch Loss: 0.08450529724359512\n",
      "Epoch 4584, Loss: 0.16022172570228577, Final Batch Loss: 0.06392648816108704\n",
      "Epoch 4585, Loss: 0.11957651749253273, Final Batch Loss: 0.07462773472070694\n",
      "Epoch 4586, Loss: 0.15185227990150452, Final Batch Loss: 0.054313741624355316\n",
      "Epoch 4587, Loss: 0.09886278957128525, Final Batch Loss: 0.03541058301925659\n",
      "Epoch 4588, Loss: 0.15482963621616364, Final Batch Loss: 0.04467085003852844\n",
      "Epoch 4589, Loss: 0.07685766741633415, Final Batch Loss: 0.02404826134443283\n",
      "Epoch 4590, Loss: 0.08447151631116867, Final Batch Loss: 0.0435805581510067\n",
      "Epoch 4591, Loss: 0.10396401956677437, Final Batch Loss: 0.043053049594163895\n",
      "Epoch 4592, Loss: 0.058724988251924515, Final Batch Loss: 0.029752936214208603\n",
      "Epoch 4593, Loss: 0.09592436254024506, Final Batch Loss: 0.06922795623540878\n",
      "Epoch 4594, Loss: 0.08423730731010437, Final Batch Loss: 0.044318437576293945\n",
      "Epoch 4595, Loss: 0.11341746151447296, Final Batch Loss: 0.06324419379234314\n",
      "Epoch 4596, Loss: 0.07925191335380077, Final Batch Loss: 0.02920948900282383\n",
      "Epoch 4597, Loss: 0.16373610123991966, Final Batch Loss: 0.11120138317346573\n",
      "Epoch 4598, Loss: 0.09140029549598694, Final Batch Loss: 0.043771564960479736\n",
      "Epoch 4599, Loss: 0.11927123926579952, Final Batch Loss: 0.08908302336931229\n",
      "Epoch 4600, Loss: 0.1213354803621769, Final Batch Loss: 0.03242282196879387\n",
      "Epoch 4601, Loss: 0.06949296779930592, Final Batch Loss: 0.0276672150939703\n",
      "Epoch 4602, Loss: 0.11136189475655556, Final Batch Loss: 0.03695588931441307\n",
      "Epoch 4603, Loss: 0.17644386738538742, Final Batch Loss: 0.03291774541139603\n",
      "Epoch 4604, Loss: 0.11777117475867271, Final Batch Loss: 0.042546626180410385\n",
      "Epoch 4605, Loss: 0.1139390878379345, Final Batch Loss: 0.03436950966715813\n",
      "Epoch 4606, Loss: 0.1876506730914116, Final Batch Loss: 0.11047004908323288\n",
      "Epoch 4607, Loss: 0.11634355038404465, Final Batch Loss: 0.04390377551317215\n",
      "Epoch 4608, Loss: 0.1431041769683361, Final Batch Loss: 0.10764484107494354\n",
      "Epoch 4609, Loss: 0.07584498450160027, Final Batch Loss: 0.03651214390993118\n",
      "Epoch 4610, Loss: 0.10833613947033882, Final Batch Loss: 0.07475493848323822\n",
      "Epoch 4611, Loss: 0.07734652236104012, Final Batch Loss: 0.03760654479265213\n",
      "Epoch 4612, Loss: 0.17854522913694382, Final Batch Loss: 0.09600196778774261\n",
      "Epoch 4613, Loss: 0.142935648560524, Final Batch Loss: 0.08296765387058258\n",
      "Epoch 4614, Loss: 0.11592015251517296, Final Batch Loss: 0.07332583516836166\n",
      "Epoch 4615, Loss: 0.18874696642160416, Final Batch Loss: 0.12003854662179947\n",
      "Epoch 4616, Loss: 0.07762719132006168, Final Batch Loss: 0.021169276908040047\n",
      "Epoch 4617, Loss: 0.17282452434301376, Final Batch Loss: 0.11194558441638947\n",
      "Epoch 4618, Loss: 0.08594987168908119, Final Batch Loss: 0.04008464515209198\n",
      "Epoch 4619, Loss: 0.1368047371506691, Final Batch Loss: 0.09065195173025131\n",
      "Epoch 4620, Loss: 0.14803476631641388, Final Batch Loss: 0.06032036244869232\n",
      "Epoch 4621, Loss: 0.16076860576868057, Final Batch Loss: 0.09535840898752213\n",
      "Epoch 4622, Loss: 0.09748456999659538, Final Batch Loss: 0.04839105159044266\n",
      "Epoch 4623, Loss: 0.14564137160778046, Final Batch Loss: 0.06342805922031403\n",
      "Epoch 4624, Loss: 0.16670648008584976, Final Batch Loss: 0.07957985997200012\n",
      "Epoch 4625, Loss: 0.16983646899461746, Final Batch Loss: 0.06288202852010727\n",
      "Epoch 4626, Loss: 0.08124004118144512, Final Batch Loss: 0.017877062782645226\n",
      "Epoch 4627, Loss: 0.07007321529090405, Final Batch Loss: 0.027967216446995735\n",
      "Epoch 4628, Loss: 0.08423102647066116, Final Batch Loss: 0.04134616255760193\n",
      "Epoch 4629, Loss: 0.11021218448877335, Final Batch Loss: 0.06535424292087555\n",
      "Epoch 4630, Loss: 0.08529406040906906, Final Batch Loss: 0.04062121734023094\n",
      "Epoch 4631, Loss: 0.09898637980222702, Final Batch Loss: 0.03444284945726395\n",
      "Epoch 4632, Loss: 0.10455457493662834, Final Batch Loss: 0.04555635154247284\n",
      "Epoch 4633, Loss: 0.16389631107449532, Final Batch Loss: 0.10302291065454483\n",
      "Epoch 4634, Loss: 0.10020870715379715, Final Batch Loss: 0.06602742522954941\n",
      "Epoch 4635, Loss: 0.13879189267754555, Final Batch Loss: 0.025492358952760696\n",
      "Epoch 4636, Loss: 0.14051801711320877, Final Batch Loss: 0.020817413926124573\n",
      "Epoch 4637, Loss: 0.11433422937989235, Final Batch Loss: 0.04223835840821266\n",
      "Epoch 4638, Loss: 0.13348711654543877, Final Batch Loss: 0.08430629223585129\n",
      "Epoch 4639, Loss: 0.11545605957508087, Final Batch Loss: 0.054600805044174194\n",
      "Epoch 4640, Loss: 0.13594022765755653, Final Batch Loss: 0.04243525490164757\n",
      "Epoch 4641, Loss: 0.2201453372836113, Final Batch Loss: 0.14910192787647247\n",
      "Epoch 4642, Loss: 0.12643036246299744, Final Batch Loss: 0.0578421950340271\n",
      "Epoch 4643, Loss: 0.11185233667492867, Final Batch Loss: 0.026569757610559464\n",
      "Epoch 4644, Loss: 0.15591150522232056, Final Batch Loss: 0.08671405166387558\n",
      "Epoch 4645, Loss: 0.08992671221494675, Final Batch Loss: 0.02527032047510147\n",
      "Epoch 4646, Loss: 0.06797163747251034, Final Batch Loss: 0.027188139036297798\n",
      "Epoch 4647, Loss: 0.18403327465057373, Final Batch Loss: 0.09331812709569931\n",
      "Epoch 4648, Loss: 0.10645464062690735, Final Batch Loss: 0.037789784371852875\n",
      "Epoch 4649, Loss: 0.14486441761255264, Final Batch Loss: 0.07243453711271286\n",
      "Epoch 4650, Loss: 0.09705894812941551, Final Batch Loss: 0.026240970939397812\n",
      "Epoch 4651, Loss: 0.17256756126880646, Final Batch Loss: 0.11721664667129517\n",
      "Epoch 4652, Loss: 0.11865486204624176, Final Batch Loss: 0.06636089831590652\n",
      "Epoch 4653, Loss: 0.1096404381096363, Final Batch Loss: 0.06703799962997437\n",
      "Epoch 4654, Loss: 0.1489478275179863, Final Batch Loss: 0.052036598324775696\n",
      "Epoch 4655, Loss: 0.28319647908210754, Final Batch Loss: 0.14651577174663544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4656, Loss: 0.1631958931684494, Final Batch Loss: 0.07170692086219788\n",
      "Epoch 4657, Loss: 0.09755600243806839, Final Batch Loss: 0.05504989251494408\n",
      "Epoch 4658, Loss: 0.1773962303996086, Final Batch Loss: 0.07182681560516357\n",
      "Epoch 4659, Loss: 0.11827294528484344, Final Batch Loss: 0.041700705885887146\n",
      "Epoch 4660, Loss: 0.12603683397173882, Final Batch Loss: 0.04033717140555382\n",
      "Epoch 4661, Loss: 0.2252514362335205, Final Batch Loss: 0.12149370461702347\n",
      "Epoch 4662, Loss: 0.3614032417535782, Final Batch Loss: 0.18983802199363708\n",
      "Epoch 4663, Loss: 0.2421749234199524, Final Batch Loss: 0.15336620807647705\n",
      "Epoch 4664, Loss: 0.14026493951678276, Final Batch Loss: 0.1002122014760971\n",
      "Epoch 4665, Loss: 0.1445833072066307, Final Batch Loss: 0.05990472435951233\n",
      "Epoch 4666, Loss: 0.16405098140239716, Final Batch Loss: 0.0840064287185669\n",
      "Epoch 4667, Loss: 0.10001537576317787, Final Batch Loss: 0.04586424678564072\n",
      "Epoch 4668, Loss: 0.08159039169549942, Final Batch Loss: 0.05094984918832779\n",
      "Epoch 4669, Loss: 0.1018022894859314, Final Batch Loss: 0.06379818171262741\n",
      "Epoch 4670, Loss: 0.11890345811843872, Final Batch Loss: 0.04947632551193237\n",
      "Epoch 4671, Loss: 0.11047160252928734, Final Batch Loss: 0.07041685283184052\n",
      "Epoch 4672, Loss: 0.13798216357827187, Final Batch Loss: 0.10376633703708649\n",
      "Epoch 4673, Loss: 0.10454162023961544, Final Batch Loss: 0.02140396647155285\n",
      "Epoch 4674, Loss: 0.14691218733787537, Final Batch Loss: 0.06879137456417084\n",
      "Epoch 4675, Loss: 0.1669710949063301, Final Batch Loss: 0.05725174397230148\n",
      "Epoch 4676, Loss: 0.07550403475761414, Final Batch Loss: 0.03850627318024635\n",
      "Epoch 4677, Loss: 0.17641142755746841, Final Batch Loss: 0.07215248793363571\n",
      "Epoch 4678, Loss: 0.12968289852142334, Final Batch Loss: 0.07904258370399475\n",
      "Epoch 4679, Loss: 0.1222449503839016, Final Batch Loss: 0.056547317653894424\n",
      "Epoch 4680, Loss: 0.16647351160645485, Final Batch Loss: 0.10929547250270844\n",
      "Epoch 4681, Loss: 0.15753672271966934, Final Batch Loss: 0.06821116805076599\n",
      "Epoch 4682, Loss: 0.3267972879111767, Final Batch Loss: 0.2949196994304657\n",
      "Epoch 4683, Loss: 0.0840463787317276, Final Batch Loss: 0.042049307376146317\n",
      "Epoch 4684, Loss: 0.12244859337806702, Final Batch Loss: 0.06455504894256592\n",
      "Epoch 4685, Loss: 0.13891853392124176, Final Batch Loss: 0.060040414333343506\n",
      "Epoch 4686, Loss: 0.09107174724340439, Final Batch Loss: 0.0451868511736393\n",
      "Epoch 4687, Loss: 0.08415044844150543, Final Batch Loss: 0.03534628823399544\n",
      "Epoch 4688, Loss: 0.1441735178232193, Final Batch Loss: 0.06042862683534622\n",
      "Epoch 4689, Loss: 0.11106530204415321, Final Batch Loss: 0.04172777757048607\n",
      "Epoch 4690, Loss: 0.08932983130216599, Final Batch Loss: 0.055602844804525375\n",
      "Epoch 4691, Loss: 0.13546147570014, Final Batch Loss: 0.08072537183761597\n",
      "Epoch 4692, Loss: 0.1376044861972332, Final Batch Loss: 0.05386486276984215\n",
      "Epoch 4693, Loss: 0.1326836235821247, Final Batch Loss: 0.09956566244363785\n",
      "Epoch 4694, Loss: 0.10437854006886482, Final Batch Loss: 0.05638539046049118\n",
      "Epoch 4695, Loss: 0.13682181388139725, Final Batch Loss: 0.027232594788074493\n",
      "Epoch 4696, Loss: 0.07006263360381126, Final Batch Loss: 0.03210587799549103\n",
      "Epoch 4697, Loss: 0.11524912714958191, Final Batch Loss: 0.05224648118019104\n",
      "Epoch 4698, Loss: 0.14769308269023895, Final Batch Loss: 0.08266736567020416\n",
      "Epoch 4699, Loss: 0.13369329273700714, Final Batch Loss: 0.05621541291475296\n",
      "Epoch 4700, Loss: 0.18897584080696106, Final Batch Loss: 0.09724796563386917\n",
      "Epoch 4701, Loss: 0.15527906641364098, Final Batch Loss: 0.050921905785799026\n",
      "Epoch 4702, Loss: 0.11029040068387985, Final Batch Loss: 0.0480661503970623\n",
      "Epoch 4703, Loss: 0.06874405965209007, Final Batch Loss: 0.03654101863503456\n",
      "Epoch 4704, Loss: 0.14418471604585648, Final Batch Loss: 0.08448558300733566\n",
      "Epoch 4705, Loss: 0.09999524429440498, Final Batch Loss: 0.04509102925658226\n",
      "Epoch 4706, Loss: 0.15307565033435822, Final Batch Loss: 0.06466033309698105\n",
      "Epoch 4707, Loss: 0.10400049015879631, Final Batch Loss: 0.05186222121119499\n",
      "Epoch 4708, Loss: 0.12840403243899345, Final Batch Loss: 0.05070197954773903\n",
      "Epoch 4709, Loss: 0.09096569567918777, Final Batch Loss: 0.03984824940562248\n",
      "Epoch 4710, Loss: 0.17224831134080887, Final Batch Loss: 0.10886712372303009\n",
      "Epoch 4711, Loss: 0.2434110827744007, Final Batch Loss: 0.05153908208012581\n",
      "Epoch 4712, Loss: 0.18321489542722702, Final Batch Loss: 0.0813862755894661\n",
      "Epoch 4713, Loss: 0.11050418764352798, Final Batch Loss: 0.07009335607290268\n",
      "Epoch 4714, Loss: 0.1671573519706726, Final Batch Loss: 0.09234302490949631\n",
      "Epoch 4715, Loss: 0.12677082046866417, Final Batch Loss: 0.048193324357271194\n",
      "Epoch 4716, Loss: 0.1199440248310566, Final Batch Loss: 0.07113100588321686\n",
      "Epoch 4717, Loss: 0.12509867921471596, Final Batch Loss: 0.07382698357105255\n",
      "Epoch 4718, Loss: 0.1070789098739624, Final Batch Loss: 0.04525766894221306\n",
      "Epoch 4719, Loss: 0.6776492856442928, Final Batch Loss: 0.6359395980834961\n",
      "Epoch 4720, Loss: 0.07332638837397099, Final Batch Loss: 0.030063169077038765\n",
      "Epoch 4721, Loss: 0.1304655522108078, Final Batch Loss: 0.06445745378732681\n",
      "Epoch 4722, Loss: 0.14698417484760284, Final Batch Loss: 0.06806547939777374\n",
      "Epoch 4723, Loss: 0.18039897829294205, Final Batch Loss: 0.11995740979909897\n",
      "Epoch 4724, Loss: 0.07984421215951443, Final Batch Loss: 0.05091087147593498\n",
      "Epoch 4725, Loss: 0.12095111608505249, Final Batch Loss: 0.04234748333692551\n",
      "Epoch 4726, Loss: 0.13854046911001205, Final Batch Loss: 0.0791524350643158\n",
      "Epoch 4727, Loss: 0.15968076884746552, Final Batch Loss: 0.1154307946562767\n",
      "Epoch 4728, Loss: 0.11863632872700691, Final Batch Loss: 0.05179348960518837\n",
      "Epoch 4729, Loss: 0.11304229125380516, Final Batch Loss: 0.031039264053106308\n",
      "Epoch 4730, Loss: 0.11305242404341698, Final Batch Loss: 0.06501706689596176\n",
      "Epoch 4731, Loss: 0.17573979124426842, Final Batch Loss: 0.061754260212183\n",
      "Epoch 4732, Loss: 0.24079807847738266, Final Batch Loss: 0.14502838253974915\n",
      "Epoch 4733, Loss: 0.085275087505579, Final Batch Loss: 0.062320057302713394\n",
      "Epoch 4734, Loss: 0.07863036543130875, Final Batch Loss: 0.028545990586280823\n",
      "Epoch 4735, Loss: 0.1727975755929947, Final Batch Loss: 0.08111676573753357\n",
      "Epoch 4736, Loss: 0.07962557300925255, Final Batch Loss: 0.031632181257009506\n",
      "Epoch 4737, Loss: 0.10387516766786575, Final Batch Loss: 0.030685633420944214\n",
      "Epoch 4738, Loss: 0.11598332971334457, Final Batch Loss: 0.04020743817090988\n",
      "Epoch 4739, Loss: 0.09196285158395767, Final Batch Loss: 0.049600519239902496\n",
      "Epoch 4740, Loss: 0.10269070044159889, Final Batch Loss: 0.04065771400928497\n",
      "Epoch 4741, Loss: 0.1381249986588955, Final Batch Loss: 0.04442327097058296\n",
      "Epoch 4742, Loss: 0.1114545464515686, Final Batch Loss: 0.04647653549909592\n",
      "Epoch 4743, Loss: 0.12244846671819687, Final Batch Loss: 0.0796719491481781\n",
      "Epoch 4744, Loss: 0.08278564363718033, Final Batch Loss: 0.022270753979682922\n",
      "Epoch 4745, Loss: 0.18235857784748077, Final Batch Loss: 0.10798240453004837\n",
      "Epoch 4746, Loss: 0.08039148524403572, Final Batch Loss: 0.04911993071436882\n",
      "Epoch 4747, Loss: 0.09671267494559288, Final Batch Loss: 0.04097088798880577\n",
      "Epoch 4748, Loss: 0.11026506870985031, Final Batch Loss: 0.05715751647949219\n",
      "Epoch 4749, Loss: 0.08296609297394753, Final Batch Loss: 0.03891577944159508\n",
      "Epoch 4750, Loss: 0.09709754586219788, Final Batch Loss: 0.04587605223059654\n",
      "Epoch 4751, Loss: 0.10115385986864567, Final Batch Loss: 0.02989470399916172\n",
      "Epoch 4752, Loss: 0.11940626800060272, Final Batch Loss: 0.038143470883369446\n",
      "Epoch 4753, Loss: 0.1632094606757164, Final Batch Loss: 0.09181742370128632\n",
      "Epoch 4754, Loss: 0.12416132166981697, Final Batch Loss: 0.0688859075307846\n",
      "Epoch 4755, Loss: 0.13461077585816383, Final Batch Loss: 0.09114520996809006\n",
      "Epoch 4756, Loss: 0.13517558574676514, Final Batch Loss: 0.09717052429914474\n",
      "Epoch 4757, Loss: 0.1624516025185585, Final Batch Loss: 0.12275372445583344\n",
      "Epoch 4758, Loss: 0.12284684926271439, Final Batch Loss: 0.03253859281539917\n",
      "Epoch 4759, Loss: 0.14862366393208504, Final Batch Loss: 0.09390178322792053\n",
      "Epoch 4760, Loss: 0.09776192530989647, Final Batch Loss: 0.05401846766471863\n",
      "Epoch 4761, Loss: 0.07425975054502487, Final Batch Loss: 0.024834752082824707\n",
      "Epoch 4762, Loss: 0.08270368725061417, Final Batch Loss: 0.042518988251686096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4763, Loss: 0.11248119547963142, Final Batch Loss: 0.07633287459611893\n",
      "Epoch 4764, Loss: 0.17906765639781952, Final Batch Loss: 0.11246348172426224\n",
      "Epoch 4765, Loss: 0.12102796137332916, Final Batch Loss: 0.07611288875341415\n",
      "Epoch 4766, Loss: 0.08751779980957508, Final Batch Loss: 0.023853057995438576\n",
      "Epoch 4767, Loss: 0.19969023019075394, Final Batch Loss: 0.107805997133255\n",
      "Epoch 4768, Loss: 0.13920331746339798, Final Batch Loss: 0.07198568433523178\n",
      "Epoch 4769, Loss: 0.14373083971440792, Final Batch Loss: 0.029804853722453117\n",
      "Epoch 4770, Loss: 0.10096589103341103, Final Batch Loss: 0.049945611506700516\n",
      "Epoch 4771, Loss: 0.1157248467206955, Final Batch Loss: 0.061862386763095856\n",
      "Epoch 4772, Loss: 0.07256957702338696, Final Batch Loss: 0.02583123929798603\n",
      "Epoch 4773, Loss: 0.10354046523571014, Final Batch Loss: 0.05393044278025627\n",
      "Epoch 4774, Loss: 0.2214820235967636, Final Batch Loss: 0.12892284989356995\n",
      "Epoch 4775, Loss: 0.07939108461141586, Final Batch Loss: 0.044821009039878845\n",
      "Epoch 4776, Loss: 0.17351019009947777, Final Batch Loss: 0.11816521733999252\n",
      "Epoch 4777, Loss: 0.09014570713043213, Final Batch Loss: 0.03579464927315712\n",
      "Epoch 4778, Loss: 0.08588363789021969, Final Batch Loss: 0.05976260453462601\n",
      "Epoch 4779, Loss: 0.19167130440473557, Final Batch Loss: 0.10797947645187378\n",
      "Epoch 4780, Loss: 0.06993205472826958, Final Batch Loss: 0.04032476618885994\n",
      "Epoch 4781, Loss: 0.0845787525177002, Final Batch Loss: 0.04055243358016014\n",
      "Epoch 4782, Loss: 0.2195155844092369, Final Batch Loss: 0.09661518037319183\n",
      "Epoch 4783, Loss: 0.10357875376939774, Final Batch Loss: 0.04224414750933647\n",
      "Epoch 4784, Loss: 0.18858323246240616, Final Batch Loss: 0.11541303247213364\n",
      "Epoch 4785, Loss: 0.14256318658590317, Final Batch Loss: 0.05904007703065872\n",
      "Epoch 4786, Loss: 0.15314605832099915, Final Batch Loss: 0.06420605629682541\n",
      "Epoch 4787, Loss: 0.13908644765615463, Final Batch Loss: 0.051197350025177\n",
      "Epoch 4788, Loss: 0.08305545523762703, Final Batch Loss: 0.03844250366091728\n",
      "Epoch 4789, Loss: 0.0954015702009201, Final Batch Loss: 0.040119364857673645\n",
      "Epoch 4790, Loss: 0.1176546961069107, Final Batch Loss: 0.0844004899263382\n",
      "Epoch 4791, Loss: 0.10308146104216576, Final Batch Loss: 0.05910227820277214\n",
      "Epoch 4792, Loss: 0.0774993821978569, Final Batch Loss: 0.0344051867723465\n",
      "Epoch 4793, Loss: 0.14162538200616837, Final Batch Loss: 0.06650225073099136\n",
      "Epoch 4794, Loss: 0.12865754589438438, Final Batch Loss: 0.09725126624107361\n",
      "Epoch 4795, Loss: 0.1214972659945488, Final Batch Loss: 0.08588061481714249\n",
      "Epoch 4796, Loss: 0.05501783266663551, Final Batch Loss: 0.026074938476085663\n",
      "Epoch 4797, Loss: 0.21897884458303452, Final Batch Loss: 0.029713667929172516\n",
      "Epoch 4798, Loss: 0.1430436410009861, Final Batch Loss: 0.06092448905110359\n",
      "Epoch 4799, Loss: 0.1606748253107071, Final Batch Loss: 0.12626685202121735\n",
      "Epoch 4800, Loss: 0.14952421933412552, Final Batch Loss: 0.07961355149745941\n",
      "Epoch 4801, Loss: 0.11134817823767662, Final Batch Loss: 0.05496298521757126\n",
      "Epoch 4802, Loss: 0.08370739966630936, Final Batch Loss: 0.02344547212123871\n",
      "Epoch 4803, Loss: 0.09058630838990211, Final Batch Loss: 0.05596960708498955\n",
      "Epoch 4804, Loss: 0.1394545491784811, Final Batch Loss: 0.022778144106268883\n",
      "Epoch 4805, Loss: 0.17938541248440742, Final Batch Loss: 0.05745699629187584\n",
      "Epoch 4806, Loss: 0.11877407878637314, Final Batch Loss: 0.061913833022117615\n",
      "Epoch 4807, Loss: 0.14802685379981995, Final Batch Loss: 0.06255435943603516\n",
      "Epoch 4808, Loss: 0.15474940463900566, Final Batch Loss: 0.10680218040943146\n",
      "Epoch 4809, Loss: 0.11231563240289688, Final Batch Loss: 0.036656565964221954\n",
      "Epoch 4810, Loss: 0.13124949857592583, Final Batch Loss: 0.04757716879248619\n",
      "Epoch 4811, Loss: 0.10212231427431107, Final Batch Loss: 0.0414390042424202\n",
      "Epoch 4812, Loss: 0.10442422330379486, Final Batch Loss: 0.03421231359243393\n",
      "Epoch 4813, Loss: 0.23508134484291077, Final Batch Loss: 0.1400509476661682\n",
      "Epoch 4814, Loss: 0.199489526450634, Final Batch Loss: 0.04506973177194595\n",
      "Epoch 4815, Loss: 0.09641161561012268, Final Batch Loss: 0.06088005006313324\n",
      "Epoch 4816, Loss: 0.10099408403038979, Final Batch Loss: 0.059278521686792374\n",
      "Epoch 4817, Loss: 0.20559778809547424, Final Batch Loss: 0.1448526233434677\n",
      "Epoch 4818, Loss: 0.19071542471647263, Final Batch Loss: 0.07500812411308289\n",
      "Epoch 4819, Loss: 0.139556422829628, Final Batch Loss: 0.0718889907002449\n",
      "Epoch 4820, Loss: 0.25197525322437286, Final Batch Loss: 0.17792119085788727\n",
      "Epoch 4821, Loss: 0.11559010669589043, Final Batch Loss: 0.04360922798514366\n",
      "Epoch 4822, Loss: 0.10099256783723831, Final Batch Loss: 0.03462214767932892\n",
      "Epoch 4823, Loss: 0.1357601396739483, Final Batch Loss: 0.045292679220438004\n",
      "Epoch 4824, Loss: 0.11526670306921005, Final Batch Loss: 0.026065848767757416\n",
      "Epoch 4825, Loss: 0.10141226276755333, Final Batch Loss: 0.04996073618531227\n",
      "Epoch 4826, Loss: 0.18403175473213196, Final Batch Loss: 0.08103978633880615\n",
      "Epoch 4827, Loss: 0.17470067739486694, Final Batch Loss: 0.11050736159086227\n",
      "Epoch 4828, Loss: 0.1337892971932888, Final Batch Loss: 0.04168796166777611\n",
      "Epoch 4829, Loss: 0.13702696189284325, Final Batch Loss: 0.03771045431494713\n",
      "Epoch 4830, Loss: 0.10237518325448036, Final Batch Loss: 0.032573144882917404\n",
      "Epoch 4831, Loss: 0.1811782792210579, Final Batch Loss: 0.13173457980155945\n",
      "Epoch 4832, Loss: 0.16158221289515495, Final Batch Loss: 0.09945563226938248\n",
      "Epoch 4833, Loss: 0.0961567647755146, Final Batch Loss: 0.04567641764879227\n",
      "Epoch 4834, Loss: 0.10080131515860558, Final Batch Loss: 0.06346432864665985\n",
      "Epoch 4835, Loss: 0.05346675589680672, Final Batch Loss: 0.016650930047035217\n",
      "Epoch 4836, Loss: 0.07516039535403252, Final Batch Loss: 0.02933662384748459\n",
      "Epoch 4837, Loss: 0.13654840737581253, Final Batch Loss: 0.07433526962995529\n",
      "Epoch 4838, Loss: 0.17011581361293793, Final Batch Loss: 0.10628286749124527\n",
      "Epoch 4839, Loss: 0.11458608880639076, Final Batch Loss: 0.08013129979372025\n",
      "Epoch 4840, Loss: 0.08877481520175934, Final Batch Loss: 0.04000898450613022\n",
      "Epoch 4841, Loss: 0.09756128117442131, Final Batch Loss: 0.031958047300577164\n",
      "Epoch 4842, Loss: 0.26221341639757156, Final Batch Loss: 0.18809375166893005\n",
      "Epoch 4843, Loss: 0.14009402692317963, Final Batch Loss: 0.030176401138305664\n",
      "Epoch 4844, Loss: 0.1447112038731575, Final Batch Loss: 0.0859525203704834\n",
      "Epoch 4845, Loss: 0.11641919985413551, Final Batch Loss: 0.06468510627746582\n",
      "Epoch 4846, Loss: 0.16147971898317337, Final Batch Loss: 0.11082237958908081\n",
      "Epoch 4847, Loss: 0.17350894585251808, Final Batch Loss: 0.12443647533655167\n",
      "Epoch 4848, Loss: 0.09594520181417465, Final Batch Loss: 0.04138728231191635\n",
      "Epoch 4849, Loss: 0.11889073252677917, Final Batch Loss: 0.04504945129156113\n",
      "Epoch 4850, Loss: 0.10727985762059689, Final Batch Loss: 0.07815108448266983\n",
      "Epoch 4851, Loss: 0.16130077838897705, Final Batch Loss: 0.05123187601566315\n",
      "Epoch 4852, Loss: 0.09236634336411953, Final Batch Loss: 0.028651012107729912\n",
      "Epoch 4853, Loss: 0.07646051421761513, Final Batch Loss: 0.03238503262400627\n",
      "Epoch 4854, Loss: 0.0891232006251812, Final Batch Loss: 0.04778468608856201\n",
      "Epoch 4855, Loss: 0.13273882120847702, Final Batch Loss: 0.06605328619480133\n",
      "Epoch 4856, Loss: 0.11135878786444664, Final Batch Loss: 0.06483783572912216\n",
      "Epoch 4857, Loss: 0.10903093963861465, Final Batch Loss: 0.03175090253353119\n",
      "Epoch 4858, Loss: 0.07131759822368622, Final Batch Loss: 0.038312967866659164\n",
      "Epoch 4859, Loss: 0.14182039722800255, Final Batch Loss: 0.09342312812805176\n",
      "Epoch 4860, Loss: 0.1165454275906086, Final Batch Loss: 0.0648605152964592\n",
      "Epoch 4861, Loss: 0.1451001614332199, Final Batch Loss: 0.08180857449769974\n",
      "Epoch 4862, Loss: 0.1230028010904789, Final Batch Loss: 0.0713181421160698\n",
      "Epoch 4863, Loss: 0.09331564232707024, Final Batch Loss: 0.03909732773900032\n",
      "Epoch 4864, Loss: 0.11278574541211128, Final Batch Loss: 0.03927690163254738\n",
      "Epoch 4865, Loss: 0.14728713780641556, Final Batch Loss: 0.07397152483463287\n",
      "Epoch 4866, Loss: 0.09587076678872108, Final Batch Loss: 0.0612076036632061\n",
      "Epoch 4867, Loss: 0.11646862328052521, Final Batch Loss: 0.05976775661110878\n",
      "Epoch 4868, Loss: 0.14489905163645744, Final Batch Loss: 0.05754733458161354\n",
      "Epoch 4869, Loss: 0.24158868193626404, Final Batch Loss: 0.14728689193725586\n",
      "Epoch 4870, Loss: 0.09913354739546776, Final Batch Loss: 0.03273875638842583\n",
      "Epoch 4871, Loss: 0.09356840699911118, Final Batch Loss: 0.035489145666360855\n",
      "Epoch 4872, Loss: 0.06708399951457977, Final Batch Loss: 0.03177100419998169\n",
      "Epoch 4873, Loss: 0.10635052248835564, Final Batch Loss: 0.03717425838112831\n",
      "Epoch 4874, Loss: 0.30857081711292267, Final Batch Loss: 0.1992643028497696\n",
      "Epoch 4875, Loss: 0.26251621544361115, Final Batch Loss: 0.09301863610744476\n",
      "Epoch 4876, Loss: 0.5643913596868515, Final Batch Loss: 0.4385508894920349\n",
      "Epoch 4877, Loss: 0.1247429046779871, Final Batch Loss: 0.025412218645215034\n",
      "Epoch 4878, Loss: 0.13105381652712822, Final Batch Loss: 0.06043599918484688\n",
      "Epoch 4879, Loss: 0.19534973800182343, Final Batch Loss: 0.11830828338861465\n",
      "Epoch 4880, Loss: 0.14745568856596947, Final Batch Loss: 0.034922633320093155\n",
      "Epoch 4881, Loss: 0.12887920066714287, Final Batch Loss: 0.061221856623888016\n",
      "Epoch 4882, Loss: 0.12926164641976357, Final Batch Loss: 0.05126190558075905\n",
      "Epoch 4883, Loss: 0.10077927261590958, Final Batch Loss: 0.04406797140836716\n",
      "Epoch 4884, Loss: 0.15731140226125717, Final Batch Loss: 0.08361070603132248\n",
      "Epoch 4885, Loss: 0.12341755628585815, Final Batch Loss: 0.06757213175296783\n",
      "Epoch 4886, Loss: 0.10415924340486526, Final Batch Loss: 0.0686892420053482\n",
      "Epoch 4887, Loss: 0.11573968082666397, Final Batch Loss: 0.031839534640312195\n",
      "Epoch 4888, Loss: 0.2136428952217102, Final Batch Loss: 0.09506964683532715\n",
      "Epoch 4889, Loss: 0.15268903598189354, Final Batch Loss: 0.02291223034262657\n",
      "Epoch 4890, Loss: 0.06016846001148224, Final Batch Loss: 0.03224281594157219\n",
      "Epoch 4891, Loss: 0.087525624781847, Final Batch Loss: 0.043366413563489914\n",
      "Epoch 4892, Loss: 0.14730387181043625, Final Batch Loss: 0.06880846619606018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4893, Loss: 0.08509528450667858, Final Batch Loss: 0.02904581092298031\n",
      "Epoch 4894, Loss: 0.09453810751438141, Final Batch Loss: 0.03513104468584061\n",
      "Epoch 4895, Loss: 0.08950230106711388, Final Batch Loss: 0.05073972046375275\n",
      "Epoch 4896, Loss: 0.1327267736196518, Final Batch Loss: 0.06191408634185791\n",
      "Epoch 4897, Loss: 0.10691731609404087, Final Batch Loss: 0.08149157464504242\n",
      "Epoch 4898, Loss: 0.12072166986763477, Final Batch Loss: 0.026139961555600166\n",
      "Epoch 4899, Loss: 0.14260655641555786, Final Batch Loss: 0.05509069561958313\n",
      "Epoch 4900, Loss: 0.12336918711662292, Final Batch Loss: 0.06542263925075531\n",
      "Epoch 4901, Loss: 0.093668632209301, Final Batch Loss: 0.048576463013887405\n",
      "Epoch 4902, Loss: 0.12604627013206482, Final Batch Loss: 0.034256719052791595\n",
      "Epoch 4903, Loss: 0.08821828663349152, Final Batch Loss: 0.033640939742326736\n",
      "Epoch 4904, Loss: 0.09120085835456848, Final Batch Loss: 0.05890621989965439\n",
      "Epoch 4905, Loss: 0.1265173964202404, Final Batch Loss: 0.0617072768509388\n",
      "Epoch 4906, Loss: 0.15027151256799698, Final Batch Loss: 0.09155873209238052\n",
      "Epoch 4907, Loss: 0.1275612823665142, Final Batch Loss: 0.04847494140267372\n",
      "Epoch 4908, Loss: 0.1326989233493805, Final Batch Loss: 0.11100482195615768\n",
      "Epoch 4909, Loss: 0.1470932960510254, Final Batch Loss: 0.07050371915102005\n",
      "Epoch 4910, Loss: 0.0850716307759285, Final Batch Loss: 0.04844001680612564\n",
      "Epoch 4911, Loss: 0.1006716825067997, Final Batch Loss: 0.061442382633686066\n",
      "Epoch 4912, Loss: 0.0793498307466507, Final Batch Loss: 0.05290766432881355\n",
      "Epoch 4913, Loss: 0.07885535806417465, Final Batch Loss: 0.0429360456764698\n",
      "Epoch 4914, Loss: 0.1450367346405983, Final Batch Loss: 0.0903162807226181\n",
      "Epoch 4915, Loss: 0.14104576781392097, Final Batch Loss: 0.08526474982500076\n",
      "Epoch 4916, Loss: 0.13302664831280708, Final Batch Loss: 0.04823450371623039\n",
      "Epoch 4917, Loss: 0.11423136480152607, Final Batch Loss: 0.0862121433019638\n",
      "Epoch 4918, Loss: 0.08537101000547409, Final Batch Loss: 0.043244242668151855\n",
      "Epoch 4919, Loss: 0.09031082130968571, Final Batch Loss: 0.02895963005721569\n",
      "Epoch 4920, Loss: 0.10583728551864624, Final Batch Loss: 0.03833895921707153\n",
      "Epoch 4921, Loss: 0.10156070441007614, Final Batch Loss: 0.028159648180007935\n",
      "Epoch 4922, Loss: 0.0654295515269041, Final Batch Loss: 0.022249048575758934\n",
      "Epoch 4923, Loss: 0.08941454812884331, Final Batch Loss: 0.033970292657613754\n",
      "Epoch 4924, Loss: 0.13227388262748718, Final Batch Loss: 0.03307565301656723\n",
      "Epoch 4925, Loss: 0.10022595152258873, Final Batch Loss: 0.046981293708086014\n",
      "Epoch 4926, Loss: 0.12398712709546089, Final Batch Loss: 0.06261817365884781\n",
      "Epoch 4927, Loss: 0.20269828289747238, Final Batch Loss: 0.12462477385997772\n",
      "Epoch 4928, Loss: 0.17937351763248444, Final Batch Loss: 0.10455133020877838\n",
      "Epoch 4929, Loss: 0.12734658643603325, Final Batch Loss: 0.09021759778261185\n",
      "Epoch 4930, Loss: 0.09834069758653641, Final Batch Loss: 0.04783657193183899\n",
      "Epoch 4931, Loss: 0.16249098628759384, Final Batch Loss: 0.07896774262189865\n",
      "Epoch 4932, Loss: 0.07899442687630653, Final Batch Loss: 0.024268928915262222\n",
      "Epoch 4933, Loss: 0.13147829845547676, Final Batch Loss: 0.04051605239510536\n",
      "Epoch 4934, Loss: 0.12461484968662262, Final Batch Loss: 0.06950993835926056\n",
      "Epoch 4935, Loss: 0.1389293149113655, Final Batch Loss: 0.09074009954929352\n",
      "Epoch 4936, Loss: 0.14147478342056274, Final Batch Loss: 0.10790569335222244\n",
      "Epoch 4937, Loss: 0.15136929228901863, Final Batch Loss: 0.04077433422207832\n",
      "Epoch 4938, Loss: 0.11953506991267204, Final Batch Loss: 0.023888375610113144\n",
      "Epoch 4939, Loss: 0.09289517253637314, Final Batch Loss: 0.045218244194984436\n",
      "Epoch 4940, Loss: 0.12695719860494137, Final Batch Loss: 0.028317561373114586\n",
      "Epoch 4941, Loss: 0.14202041551470757, Final Batch Loss: 0.08136202394962311\n",
      "Epoch 4942, Loss: 0.19906890392303467, Final Batch Loss: 0.12696963548660278\n",
      "Epoch 4943, Loss: 0.10340028256177902, Final Batch Loss: 0.05813607946038246\n",
      "Epoch 4944, Loss: 0.11314110457897186, Final Batch Loss: 0.054615236818790436\n",
      "Epoch 4945, Loss: 0.08189488388597965, Final Batch Loss: 0.021285666152834892\n",
      "Epoch 4946, Loss: 0.1102384626865387, Final Batch Loss: 0.03649307042360306\n",
      "Epoch 4947, Loss: 0.09542166069149971, Final Batch Loss: 0.0501069650053978\n",
      "Epoch 4948, Loss: 0.11062367632985115, Final Batch Loss: 0.07516910880804062\n",
      "Epoch 4949, Loss: 0.10168909281492233, Final Batch Loss: 0.04714281111955643\n",
      "Epoch 4950, Loss: 0.065452940762043, Final Batch Loss: 0.024164479225873947\n",
      "Epoch 4951, Loss: 0.07708762027323246, Final Batch Loss: 0.0272508654743433\n",
      "Epoch 4952, Loss: 0.13948259875178337, Final Batch Loss: 0.07897619158029556\n",
      "Epoch 4953, Loss: 0.12351258471608162, Final Batch Loss: 0.052641820162534714\n",
      "Epoch 4954, Loss: 0.06180325150489807, Final Batch Loss: 0.02533591538667679\n",
      "Epoch 4955, Loss: 0.0646548718214035, Final Batch Loss: 0.034108858555555344\n",
      "Epoch 4956, Loss: 0.08166569098830223, Final Batch Loss: 0.036561187356710434\n",
      "Epoch 4957, Loss: 0.1010817177593708, Final Batch Loss: 0.0748554915189743\n",
      "Epoch 4958, Loss: 0.16685371845960617, Final Batch Loss: 0.053414277732372284\n",
      "Epoch 4959, Loss: 0.07156193628907204, Final Batch Loss: 0.03785590082406998\n",
      "Epoch 4960, Loss: 0.2739674746990204, Final Batch Loss: 0.1357208490371704\n",
      "Epoch 4961, Loss: 0.06413992308080196, Final Batch Loss: 0.027147075161337852\n",
      "Epoch 4962, Loss: 0.10991700366139412, Final Batch Loss: 0.04431998357176781\n",
      "Epoch 4963, Loss: 0.09857822954654694, Final Batch Loss: 0.046392060816287994\n",
      "Epoch 4964, Loss: 0.10231244191527367, Final Batch Loss: 0.04710988327860832\n",
      "Epoch 4965, Loss: 0.10165192186832428, Final Batch Loss: 0.0351753830909729\n",
      "Epoch 4966, Loss: 0.4480067156255245, Final Batch Loss: 0.42020559310913086\n",
      "Epoch 4967, Loss: 0.1098511628806591, Final Batch Loss: 0.05445912852883339\n",
      "Epoch 4968, Loss: 0.16322336345911026, Final Batch Loss: 0.09643366932868958\n",
      "Epoch 4969, Loss: 0.10749662667512894, Final Batch Loss: 0.03161793202161789\n",
      "Epoch 4970, Loss: 0.1196659654378891, Final Batch Loss: 0.05535612255334854\n",
      "Epoch 4971, Loss: 0.1349751502275467, Final Batch Loss: 0.06270074844360352\n",
      "Epoch 4972, Loss: 0.08518553897738457, Final Batch Loss: 0.037534572184085846\n",
      "Epoch 4973, Loss: 0.21840687841176987, Final Batch Loss: 0.10947031527757645\n",
      "Epoch 4974, Loss: 0.10912346839904785, Final Batch Loss: 0.04773318022489548\n",
      "Epoch 4975, Loss: 0.13867353647947311, Final Batch Loss: 0.06716076284646988\n",
      "Epoch 4976, Loss: 0.12138290330767632, Final Batch Loss: 0.04820426180958748\n",
      "Epoch 4977, Loss: 0.10046089440584183, Final Batch Loss: 0.03935513272881508\n",
      "Epoch 4978, Loss: 0.09401644766330719, Final Batch Loss: 0.04284905269742012\n",
      "Epoch 4979, Loss: 0.24775590002536774, Final Batch Loss: 0.06579446792602539\n",
      "Epoch 4980, Loss: 0.16268054768443108, Final Batch Loss: 0.10627466440200806\n",
      "Epoch 4981, Loss: 0.09105310216546059, Final Batch Loss: 0.020179014652967453\n",
      "Epoch 4982, Loss: 0.1595693603157997, Final Batch Loss: 0.0856894701719284\n",
      "Epoch 4983, Loss: 0.09075576439499855, Final Batch Loss: 0.04091499000787735\n",
      "Epoch 4984, Loss: 0.12575072795152664, Final Batch Loss: 0.05075472593307495\n",
      "Epoch 4985, Loss: 0.09224214404821396, Final Batch Loss: 0.042786356061697006\n",
      "Epoch 4986, Loss: 0.09503641352057457, Final Batch Loss: 0.031420979648828506\n",
      "Epoch 4987, Loss: 0.1940801963210106, Final Batch Loss: 0.0975487157702446\n",
      "Epoch 4988, Loss: 0.1893925964832306, Final Batch Loss: 0.08639156073331833\n",
      "Epoch 4989, Loss: 0.08395708166062832, Final Batch Loss: 0.029368141666054726\n",
      "Epoch 4990, Loss: 0.11656155064702034, Final Batch Loss: 0.04260915890336037\n",
      "Epoch 4991, Loss: 0.08824052661657333, Final Batch Loss: 0.03492506593465805\n",
      "Epoch 4992, Loss: 0.15170682966709137, Final Batch Loss: 0.09297119826078415\n",
      "Epoch 4993, Loss: 0.07816783711314201, Final Batch Loss: 0.035129014402627945\n",
      "Epoch 4994, Loss: 0.17946696281433105, Final Batch Loss: 0.10666552186012268\n",
      "Epoch 4995, Loss: 0.14881925657391548, Final Batch Loss: 0.04819665476679802\n",
      "Epoch 4996, Loss: 0.111307043582201, Final Batch Loss: 0.04633982852101326\n",
      "Epoch 4997, Loss: 0.10810404643416405, Final Batch Loss: 0.07111731916666031\n",
      "Epoch 4998, Loss: 0.1809840314090252, Final Batch Loss: 0.136867493391037\n",
      "Epoch 4999, Loss: 0.12375937402248383, Final Batch Loss: 0.06246987357735634\n",
      "Epoch 5000, Loss: 0.1239040195941925, Final Batch Loss: 0.040008313953876495\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  0  1  0]\n",
      " [ 0  0 12  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  1  0  0 21  0  0  0]\n",
      " [ 0  0  0  0  0  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  0  0  1  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        12\n",
      "           1    1.00000   0.88889   0.94118         9\n",
      "           2    0.92308   1.00000   0.96000        12\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         8\n",
      "           5    1.00000   0.95455   0.97674        22\n",
      "           6    1.00000   1.00000   1.00000        12\n",
      "           7    0.75000   1.00000   0.85714         6\n",
      "           8    1.00000   0.87500   0.93333         8\n",
      "\n",
      "    accuracy                        0.96939        98\n",
      "   macro avg    0.96368   0.96871   0.96316        98\n",
      "weighted avg    0.97527   0.96939   0.97029        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.train()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Fake Test on Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20,000 epochs DEFAULT PARAMETERS FOR THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_Group_4_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(X_test)\n",
    "latent_vectors = get_noise(size, 100)\n",
    "act_vectors = get_act_matrix(size, 3)\n",
    "usr_vectors = get_usr_matrix(size, 3)\n",
    "\n",
    "fake_labels = []\n",
    "\n",
    "for k in range(size):\n",
    "    if act_vectors[0][k] == 0 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(0)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(1)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 0:\n",
    "        fake_labels.append(2)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(3)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(4)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 1:\n",
    "        fake_labels.append(5)\n",
    "    elif act_vectors[0][k] == 0 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(6)\n",
    "    elif act_vectors[0][k] == 1 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(7)\n",
    "    elif act_vectors[0][k] == 2 and usr_vectors[0][k] == 2:\n",
    "        fake_labels.append(8)\n",
    "\n",
    "fake_labels = np.asarray(fake_labels)\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach()\n",
    "#fake_features = gen(to_gen).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  4  0  0  2]\n",
      " [ 0  0  0 17  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  0  0  0  9  0  0  0]\n",
      " [ 1  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0 12  0]\n",
      " [ 0  0  1  0  0  0  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.92308   1.00000   0.96000        12\n",
      "           1    1.00000   1.00000   1.00000         4\n",
      "           2    0.66667   0.25000   0.36364         8\n",
      "           3    1.00000   1.00000   1.00000        17\n",
      "           4    1.00000   1.00000   1.00000        12\n",
      "           5    0.69231   1.00000   0.81818         9\n",
      "           6    1.00000   0.91667   0.95652        12\n",
      "           7    1.00000   1.00000   1.00000        12\n",
      "           8    0.84615   0.91667   0.88000        12\n",
      "\n",
      "    accuracy                        0.91837        98\n",
      "   macro avg    0.90313   0.89815   0.88648        98\n",
      "weighted avg    0.91627   0.91837   0.90644        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
