{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 19) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 21) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 22) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [19, 21, 22]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.4217212200164795, Final Batch Loss: 2.2083535194396973\n",
      "Epoch 2, Loss: 4.409052610397339, Final Batch Loss: 2.186685562133789\n",
      "Epoch 3, Loss: 4.411114692687988, Final Batch Loss: 2.1969480514526367\n",
      "Epoch 4, Loss: 4.417100191116333, Final Batch Loss: 2.221562147140503\n",
      "Epoch 5, Loss: 4.409917593002319, Final Batch Loss: 2.204122543334961\n",
      "Epoch 6, Loss: 4.4084694385528564, Final Batch Loss: 2.2106213569641113\n",
      "Epoch 7, Loss: 4.3950605392456055, Final Batch Loss: 2.190225601196289\n",
      "Epoch 8, Loss: 4.394351243972778, Final Batch Loss: 2.1950175762176514\n",
      "Epoch 9, Loss: 4.391976356506348, Final Batch Loss: 2.195054054260254\n",
      "Epoch 10, Loss: 4.389969348907471, Final Batch Loss: 2.19486141204834\n",
      "Epoch 11, Loss: 4.384737968444824, Final Batch Loss: 2.194596767425537\n",
      "Epoch 12, Loss: 4.38344407081604, Final Batch Loss: 2.1904377937316895\n",
      "Epoch 13, Loss: 4.373279571533203, Final Batch Loss: 2.1845626831054688\n",
      "Epoch 14, Loss: 4.373605728149414, Final Batch Loss: 2.1899847984313965\n",
      "Epoch 15, Loss: 4.370562791824341, Final Batch Loss: 2.1898131370544434\n",
      "Epoch 16, Loss: 4.36200475692749, Final Batch Loss: 2.181206226348877\n",
      "Epoch 17, Loss: 4.359982013702393, Final Batch Loss: 2.1840627193450928\n",
      "Epoch 18, Loss: 4.340437650680542, Final Batch Loss: 2.163956880569458\n",
      "Epoch 19, Loss: 4.333345174789429, Final Batch Loss: 2.1616196632385254\n",
      "Epoch 20, Loss: 4.32716703414917, Final Batch Loss: 2.158233404159546\n",
      "Epoch 21, Loss: 4.3144590854644775, Final Batch Loss: 2.153140068054199\n",
      "Epoch 22, Loss: 4.3006510734558105, Final Batch Loss: 2.139514446258545\n",
      "Epoch 23, Loss: 4.290163993835449, Final Batch Loss: 2.138509511947632\n",
      "Epoch 24, Loss: 4.279723405838013, Final Batch Loss: 2.13928484916687\n",
      "Epoch 25, Loss: 4.257751941680908, Final Batch Loss: 2.1308517456054688\n",
      "Epoch 26, Loss: 4.237509727478027, Final Batch Loss: 2.1078577041625977\n",
      "Epoch 27, Loss: 4.221562623977661, Final Batch Loss: 2.110903739929199\n",
      "Epoch 28, Loss: 4.2041120529174805, Final Batch Loss: 2.1080007553100586\n",
      "Epoch 29, Loss: 4.164401054382324, Final Batch Loss: 2.069204092025757\n",
      "Epoch 30, Loss: 4.176754951477051, Final Batch Loss: 2.1108627319335938\n",
      "Epoch 31, Loss: 4.124889135360718, Final Batch Loss: 2.063145875930786\n",
      "Epoch 32, Loss: 4.071638822555542, Final Batch Loss: 2.0252935886383057\n",
      "Epoch 33, Loss: 4.052990436553955, Final Batch Loss: 2.008852243423462\n",
      "Epoch 34, Loss: 4.012616276741028, Final Batch Loss: 2.021872043609619\n",
      "Epoch 35, Loss: 3.953799605369568, Final Batch Loss: 1.9872260093688965\n",
      "Epoch 36, Loss: 3.9338510036468506, Final Batch Loss: 1.9595178365707397\n",
      "Epoch 37, Loss: 3.8880252838134766, Final Batch Loss: 1.9490545988082886\n",
      "Epoch 38, Loss: 3.862813353538513, Final Batch Loss: 1.9345413446426392\n",
      "Epoch 39, Loss: 3.8015379905700684, Final Batch Loss: 1.9174823760986328\n",
      "Epoch 40, Loss: 3.8014028072357178, Final Batch Loss: 1.9368553161621094\n",
      "Epoch 41, Loss: 3.713297724723816, Final Batch Loss: 1.8169841766357422\n",
      "Epoch 42, Loss: 3.744192957878113, Final Batch Loss: 1.8975886106491089\n",
      "Epoch 43, Loss: 3.6716995239257812, Final Batch Loss: 1.818594217300415\n",
      "Epoch 44, Loss: 3.6344048976898193, Final Batch Loss: 1.8137410879135132\n",
      "Epoch 45, Loss: 3.6122162342071533, Final Batch Loss: 1.7654783725738525\n",
      "Epoch 46, Loss: 3.5923086404800415, Final Batch Loss: 1.7932401895523071\n",
      "Epoch 47, Loss: 3.579355239868164, Final Batch Loss: 1.7825413942337036\n",
      "Epoch 48, Loss: 3.5822837352752686, Final Batch Loss: 1.8163613080978394\n",
      "Epoch 49, Loss: 3.5720248222351074, Final Batch Loss: 1.8096541166305542\n",
      "Epoch 50, Loss: 3.4889566898345947, Final Batch Loss: 1.7496706247329712\n",
      "Epoch 51, Loss: 3.5300426483154297, Final Batch Loss: 1.831485629081726\n",
      "Epoch 52, Loss: 3.4614332914352417, Final Batch Loss: 1.725068211555481\n",
      "Epoch 53, Loss: 3.4211363792419434, Final Batch Loss: 1.6998318433761597\n",
      "Epoch 54, Loss: 3.4643718004226685, Final Batch Loss: 1.777618646621704\n",
      "Epoch 55, Loss: 3.3315346240997314, Final Batch Loss: 1.6634495258331299\n",
      "Epoch 56, Loss: 3.3541241884231567, Final Batch Loss: 1.6614152193069458\n",
      "Epoch 57, Loss: 3.302963614463806, Final Batch Loss: 1.6576354503631592\n",
      "Epoch 58, Loss: 3.3043519258499146, Final Batch Loss: 1.6340620517730713\n",
      "Epoch 59, Loss: 3.2366678714752197, Final Batch Loss: 1.5927908420562744\n",
      "Epoch 60, Loss: 3.2425293922424316, Final Batch Loss: 1.6374857425689697\n",
      "Epoch 61, Loss: 3.24654483795166, Final Batch Loss: 1.645712971687317\n",
      "Epoch 62, Loss: 3.2019792795181274, Final Batch Loss: 1.5795042514801025\n",
      "Epoch 63, Loss: 3.171028256416321, Final Batch Loss: 1.6105380058288574\n",
      "Epoch 64, Loss: 3.150750160217285, Final Batch Loss: 1.6142019033432007\n",
      "Epoch 65, Loss: 3.071603298187256, Final Batch Loss: 1.5515073537826538\n",
      "Epoch 66, Loss: 3.132589101791382, Final Batch Loss: 1.6166397333145142\n",
      "Epoch 67, Loss: 3.0759856700897217, Final Batch Loss: 1.5463615655899048\n",
      "Epoch 68, Loss: 3.0837903022766113, Final Batch Loss: 1.6018874645233154\n",
      "Epoch 69, Loss: 2.973536491394043, Final Batch Loss: 1.496594786643982\n",
      "Epoch 70, Loss: 2.960014581680298, Final Batch Loss: 1.491047978401184\n",
      "Epoch 71, Loss: 2.9518516063690186, Final Batch Loss: 1.4770426750183105\n",
      "Epoch 72, Loss: 2.8576689958572388, Final Batch Loss: 1.362666368484497\n",
      "Epoch 73, Loss: 2.887534499168396, Final Batch Loss: 1.4087268114089966\n",
      "Epoch 74, Loss: 2.8712509870529175, Final Batch Loss: 1.4502372741699219\n",
      "Epoch 75, Loss: 2.86728036403656, Final Batch Loss: 1.5021241903305054\n",
      "Epoch 76, Loss: 2.7429782152175903, Final Batch Loss: 1.3400589227676392\n",
      "Epoch 77, Loss: 2.755176544189453, Final Batch Loss: 1.3292182683944702\n",
      "Epoch 78, Loss: 2.8195087909698486, Final Batch Loss: 1.3956273794174194\n",
      "Epoch 79, Loss: 2.708914279937744, Final Batch Loss: 1.2923780679702759\n",
      "Epoch 80, Loss: 2.7395613193511963, Final Batch Loss: 1.4098989963531494\n",
      "Epoch 81, Loss: 2.7028998136520386, Final Batch Loss: 1.3023840188980103\n",
      "Epoch 82, Loss: 2.6555533409118652, Final Batch Loss: 1.346477746963501\n",
      "Epoch 83, Loss: 2.6730847358703613, Final Batch Loss: 1.3230836391448975\n",
      "Epoch 84, Loss: 2.655498743057251, Final Batch Loss: 1.3857964277267456\n",
      "Epoch 85, Loss: 2.6009968519210815, Final Batch Loss: 1.2898534536361694\n",
      "Epoch 86, Loss: 2.6552385091781616, Final Batch Loss: 1.350527286529541\n",
      "Epoch 87, Loss: 2.544459104537964, Final Batch Loss: 1.2623177766799927\n",
      "Epoch 88, Loss: 2.556777238845825, Final Batch Loss: 1.254676342010498\n",
      "Epoch 89, Loss: 2.5508155822753906, Final Batch Loss: 1.2068411111831665\n",
      "Epoch 90, Loss: 2.492666244506836, Final Batch Loss: 1.2060270309448242\n",
      "Epoch 91, Loss: 2.478379011154175, Final Batch Loss: 1.2588441371917725\n",
      "Epoch 92, Loss: 2.4805370569229126, Final Batch Loss: 1.2708674669265747\n",
      "Epoch 93, Loss: 2.4102925062179565, Final Batch Loss: 1.2429945468902588\n",
      "Epoch 94, Loss: 2.4679605960845947, Final Batch Loss: 1.2203941345214844\n",
      "Epoch 95, Loss: 2.4955960512161255, Final Batch Loss: 1.2555909156799316\n",
      "Epoch 96, Loss: 2.476424813270569, Final Batch Loss: 1.2766783237457275\n",
      "Epoch 97, Loss: 2.3506683111190796, Final Batch Loss: 1.1559181213378906\n",
      "Epoch 98, Loss: 2.364243745803833, Final Batch Loss: 1.1458966732025146\n",
      "Epoch 99, Loss: 2.3124988079071045, Final Batch Loss: 1.1143449544906616\n",
      "Epoch 100, Loss: 2.345327138900757, Final Batch Loss: 1.1929548978805542\n",
      "Epoch 101, Loss: 2.3203662633895874, Final Batch Loss: 1.1884909868240356\n",
      "Epoch 102, Loss: 2.2704875469207764, Final Batch Loss: 1.1629713773727417\n",
      "Epoch 103, Loss: 2.239319920539856, Final Batch Loss: 1.120020866394043\n",
      "Epoch 104, Loss: 2.1929564476013184, Final Batch Loss: 1.0672802925109863\n",
      "Epoch 105, Loss: 2.217192769050598, Final Batch Loss: 1.0915683507919312\n",
      "Epoch 106, Loss: 2.2070547342300415, Final Batch Loss: 1.0881341695785522\n",
      "Epoch 107, Loss: 2.2129846811294556, Final Batch Loss: 1.1237634420394897\n",
      "Epoch 108, Loss: 2.1781749725341797, Final Batch Loss: 1.082032322883606\n",
      "Epoch 109, Loss: 2.273571014404297, Final Batch Loss: 1.1143913269042969\n",
      "Epoch 110, Loss: 2.2201929092407227, Final Batch Loss: 1.101401686668396\n",
      "Epoch 111, Loss: 2.188867211341858, Final Batch Loss: 1.102513074874878\n",
      "Epoch 112, Loss: 2.1895638704299927, Final Batch Loss: 1.0949586629867554\n",
      "Epoch 113, Loss: 2.231865167617798, Final Batch Loss: 1.0854747295379639\n",
      "Epoch 114, Loss: 2.159812092781067, Final Batch Loss: 1.0710172653198242\n",
      "Epoch 115, Loss: 2.2003872394561768, Final Batch Loss: 1.1189846992492676\n",
      "Epoch 116, Loss: 2.1794209480285645, Final Batch Loss: 1.0511938333511353\n",
      "Epoch 117, Loss: 2.153024911880493, Final Batch Loss: 1.0952967405319214\n",
      "Epoch 118, Loss: 2.10606849193573, Final Batch Loss: 1.04443359375\n",
      "Epoch 119, Loss: 2.1653528213500977, Final Batch Loss: 1.0980099439620972\n",
      "Epoch 120, Loss: 2.208424925804138, Final Batch Loss: 1.0923576354980469\n",
      "Epoch 121, Loss: 2.1284903287887573, Final Batch Loss: 1.0323188304901123\n",
      "Epoch 122, Loss: 2.1491445302963257, Final Batch Loss: 1.0972813367843628\n",
      "Epoch 123, Loss: 2.1633973121643066, Final Batch Loss: 1.0778217315673828\n",
      "Epoch 124, Loss: 2.090956211090088, Final Batch Loss: 1.0252209901809692\n",
      "Epoch 125, Loss: 2.121357798576355, Final Batch Loss: 1.044476866722107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, Loss: 2.2304598093032837, Final Batch Loss: 1.1741867065429688\n",
      "Epoch 127, Loss: 2.1091023683547974, Final Batch Loss: 1.0579761266708374\n",
      "Epoch 128, Loss: 2.1731066703796387, Final Batch Loss: 1.0851101875305176\n",
      "Epoch 129, Loss: 2.1036295890808105, Final Batch Loss: 1.0302783250808716\n",
      "Epoch 130, Loss: 2.064855456352234, Final Batch Loss: 1.0093806982040405\n",
      "Epoch 131, Loss: 2.0547025203704834, Final Batch Loss: 1.0241062641143799\n",
      "Epoch 132, Loss: 2.046659469604492, Final Batch Loss: 1.0364782810211182\n",
      "Epoch 133, Loss: 2.1520577669143677, Final Batch Loss: 1.0806849002838135\n",
      "Epoch 134, Loss: 2.0777041912078857, Final Batch Loss: 1.0684646368026733\n",
      "Epoch 135, Loss: 2.058832883834839, Final Batch Loss: 1.047139286994934\n",
      "Epoch 136, Loss: 2.0314130783081055, Final Batch Loss: 1.0106778144836426\n",
      "Epoch 137, Loss: 2.0559924840927124, Final Batch Loss: 1.012351393699646\n",
      "Epoch 138, Loss: 2.040438234806061, Final Batch Loss: 0.9998776316642761\n",
      "Epoch 139, Loss: 2.0649231672286987, Final Batch Loss: 1.06600022315979\n",
      "Epoch 140, Loss: 2.0015698075294495, Final Batch Loss: 0.9914500117301941\n",
      "Epoch 141, Loss: 2.0559550523757935, Final Batch Loss: 1.068505048751831\n",
      "Epoch 142, Loss: 2.032842516899109, Final Batch Loss: 1.0254926681518555\n",
      "Epoch 143, Loss: 2.040698766708374, Final Batch Loss: 1.0233542919158936\n",
      "Epoch 144, Loss: 2.021779775619507, Final Batch Loss: 1.0170007944107056\n",
      "Epoch 145, Loss: 1.9616010189056396, Final Batch Loss: 0.9841188192367554\n",
      "Epoch 146, Loss: 2.0459119081497192, Final Batch Loss: 1.0451812744140625\n",
      "Epoch 147, Loss: 2.0485196113586426, Final Batch Loss: 1.0242979526519775\n",
      "Epoch 148, Loss: 2.0930397510528564, Final Batch Loss: 1.0933769941329956\n",
      "Epoch 149, Loss: 1.9416843056678772, Final Batch Loss: 0.9186169505119324\n",
      "Epoch 150, Loss: 1.9795618057250977, Final Batch Loss: 0.9722412824630737\n",
      "Epoch 151, Loss: 1.9378057718276978, Final Batch Loss: 0.8946874141693115\n",
      "Epoch 152, Loss: 2.033583164215088, Final Batch Loss: 1.0032843351364136\n",
      "Epoch 153, Loss: 1.9087347388267517, Final Batch Loss: 0.9549824595451355\n",
      "Epoch 154, Loss: 2.0860769748687744, Final Batch Loss: 1.0656754970550537\n",
      "Epoch 155, Loss: 2.0493838787078857, Final Batch Loss: 1.071104884147644\n",
      "Epoch 156, Loss: 1.921625792980194, Final Batch Loss: 0.9286727905273438\n",
      "Epoch 157, Loss: 1.926809847354889, Final Batch Loss: 0.9416280388832092\n",
      "Epoch 158, Loss: 1.8908650875091553, Final Batch Loss: 0.9412696957588196\n",
      "Epoch 159, Loss: 1.868273675441742, Final Batch Loss: 0.8937318325042725\n",
      "Epoch 160, Loss: 1.9351782202720642, Final Batch Loss: 0.9868439435958862\n",
      "Epoch 161, Loss: 1.8973607420921326, Final Batch Loss: 0.991804838180542\n",
      "Epoch 162, Loss: 1.9329217672348022, Final Batch Loss: 0.9928322434425354\n",
      "Epoch 163, Loss: 1.9502658247947693, Final Batch Loss: 1.0103617906570435\n",
      "Epoch 164, Loss: 1.9560852646827698, Final Batch Loss: 0.9925379157066345\n",
      "Epoch 165, Loss: 1.9520844221115112, Final Batch Loss: 0.9815921783447266\n",
      "Epoch 166, Loss: 1.8468874096870422, Final Batch Loss: 0.8903349041938782\n",
      "Epoch 167, Loss: 1.92147296667099, Final Batch Loss: 1.002449631690979\n",
      "Epoch 168, Loss: 1.8895501494407654, Final Batch Loss: 0.9222354292869568\n",
      "Epoch 169, Loss: 1.9453012943267822, Final Batch Loss: 0.9881144762039185\n",
      "Epoch 170, Loss: 1.8720863461494446, Final Batch Loss: 0.9314343333244324\n",
      "Epoch 171, Loss: 1.9493480920791626, Final Batch Loss: 0.9632421135902405\n",
      "Epoch 172, Loss: 1.9375091791152954, Final Batch Loss: 0.9652212262153625\n",
      "Epoch 173, Loss: 1.9483426213264465, Final Batch Loss: 0.9675184488296509\n",
      "Epoch 174, Loss: 1.895508050918579, Final Batch Loss: 0.9532924294471741\n",
      "Epoch 175, Loss: 1.897740125656128, Final Batch Loss: 0.9618958830833435\n",
      "Epoch 176, Loss: 1.9007554054260254, Final Batch Loss: 0.965665340423584\n",
      "Epoch 177, Loss: 1.8687896132469177, Final Batch Loss: 0.9747623801231384\n",
      "Epoch 178, Loss: 1.8031737208366394, Final Batch Loss: 0.9226177334785461\n",
      "Epoch 179, Loss: 1.8525066375732422, Final Batch Loss: 0.9327353835105896\n",
      "Epoch 180, Loss: 1.8035383820533752, Final Batch Loss: 0.8651595711708069\n",
      "Epoch 181, Loss: 1.91741281747818, Final Batch Loss: 1.0297828912734985\n",
      "Epoch 182, Loss: 1.8278725743293762, Final Batch Loss: 0.9302902221679688\n",
      "Epoch 183, Loss: 1.8177948594093323, Final Batch Loss: 0.9377613067626953\n",
      "Epoch 184, Loss: 1.786048173904419, Final Batch Loss: 0.8514251112937927\n",
      "Epoch 185, Loss: 1.801612675189972, Final Batch Loss: 0.8966747522354126\n",
      "Epoch 186, Loss: 1.7017337679862976, Final Batch Loss: 0.8765847086906433\n",
      "Epoch 187, Loss: 1.7302681803703308, Final Batch Loss: 0.8741496205329895\n",
      "Epoch 188, Loss: 1.8152093291282654, Final Batch Loss: 0.9286695718765259\n",
      "Epoch 189, Loss: 1.8384438753128052, Final Batch Loss: 0.9458240866661072\n",
      "Epoch 190, Loss: 1.857882559299469, Final Batch Loss: 1.0113312005996704\n",
      "Epoch 191, Loss: 1.8042826056480408, Final Batch Loss: 0.9831371307373047\n",
      "Epoch 192, Loss: 1.7994397282600403, Final Batch Loss: 0.8852847218513489\n",
      "Epoch 193, Loss: 1.7892239689826965, Final Batch Loss: 0.9369009137153625\n",
      "Epoch 194, Loss: 1.7903167605400085, Final Batch Loss: 0.8557819724082947\n",
      "Epoch 195, Loss: 1.8480013012886047, Final Batch Loss: 0.910612165927887\n",
      "Epoch 196, Loss: 1.7358713746070862, Final Batch Loss: 0.8814882040023804\n",
      "Epoch 197, Loss: 1.7575898170471191, Final Batch Loss: 0.8638501167297363\n",
      "Epoch 198, Loss: 1.7919904589653015, Final Batch Loss: 0.8743901252746582\n",
      "Epoch 199, Loss: 1.8465948104858398, Final Batch Loss: 1.0002901554107666\n",
      "Epoch 200, Loss: 1.7649089097976685, Final Batch Loss: 0.8959465622901917\n",
      "Epoch 201, Loss: 1.777924358844757, Final Batch Loss: 0.8939107060432434\n",
      "Epoch 202, Loss: 1.7652008533477783, Final Batch Loss: 0.9208124876022339\n",
      "Epoch 203, Loss: 1.7056671380996704, Final Batch Loss: 0.8356070518493652\n",
      "Epoch 204, Loss: 1.7833558917045593, Final Batch Loss: 0.924594521522522\n",
      "Epoch 205, Loss: 1.7117554545402527, Final Batch Loss: 0.8858118653297424\n",
      "Epoch 206, Loss: 1.693901777267456, Final Batch Loss: 0.8012779355049133\n",
      "Epoch 207, Loss: 1.6669850945472717, Final Batch Loss: 0.7722102403640747\n",
      "Epoch 208, Loss: 1.7210888862609863, Final Batch Loss: 0.8287698030471802\n",
      "Epoch 209, Loss: 1.6237295866012573, Final Batch Loss: 0.8243924379348755\n",
      "Epoch 210, Loss: 1.7116159200668335, Final Batch Loss: 0.8615248799324036\n",
      "Epoch 211, Loss: 1.7599826455116272, Final Batch Loss: 0.9327666759490967\n",
      "Epoch 212, Loss: 1.6682495474815369, Final Batch Loss: 0.8023757934570312\n",
      "Epoch 213, Loss: 1.703943133354187, Final Batch Loss: 0.8793456554412842\n",
      "Epoch 214, Loss: 1.604606568813324, Final Batch Loss: 0.7374695539474487\n",
      "Epoch 215, Loss: 1.7663771510124207, Final Batch Loss: 0.8830699324607849\n",
      "Epoch 216, Loss: 1.6868505477905273, Final Batch Loss: 0.8934991359710693\n",
      "Epoch 217, Loss: 1.6195858716964722, Final Batch Loss: 0.776663601398468\n",
      "Epoch 218, Loss: 1.6367029547691345, Final Batch Loss: 0.8000702857971191\n",
      "Epoch 219, Loss: 1.688810408115387, Final Batch Loss: 0.8584638833999634\n",
      "Epoch 220, Loss: 1.665280282497406, Final Batch Loss: 0.8023973703384399\n",
      "Epoch 221, Loss: 1.6355149745941162, Final Batch Loss: 0.804938018321991\n",
      "Epoch 222, Loss: 1.5518841743469238, Final Batch Loss: 0.7195208668708801\n",
      "Epoch 223, Loss: 1.6331949830055237, Final Batch Loss: 0.8441174626350403\n",
      "Epoch 224, Loss: 1.7156336307525635, Final Batch Loss: 0.8658547401428223\n",
      "Epoch 225, Loss: 1.6647899150848389, Final Batch Loss: 0.804512083530426\n",
      "Epoch 226, Loss: 1.6765170097351074, Final Batch Loss: 0.8571894764900208\n",
      "Epoch 227, Loss: 1.6396857500076294, Final Batch Loss: 0.8382543921470642\n",
      "Epoch 228, Loss: 1.5989471673965454, Final Batch Loss: 0.7480749487876892\n",
      "Epoch 229, Loss: 1.6281274557113647, Final Batch Loss: 0.8247212171554565\n",
      "Epoch 230, Loss: 1.6523149609565735, Final Batch Loss: 0.8305360674858093\n",
      "Epoch 231, Loss: 1.549094557762146, Final Batch Loss: 0.7355108261108398\n",
      "Epoch 232, Loss: 1.5918896794319153, Final Batch Loss: 0.7931758165359497\n",
      "Epoch 233, Loss: 1.712462306022644, Final Batch Loss: 0.906582236289978\n",
      "Epoch 234, Loss: 1.6258994936943054, Final Batch Loss: 0.8712288737297058\n",
      "Epoch 235, Loss: 1.6027987599372864, Final Batch Loss: 0.8180614709854126\n",
      "Epoch 236, Loss: 1.6601357460021973, Final Batch Loss: 0.9151668548583984\n",
      "Epoch 237, Loss: 1.61410254240036, Final Batch Loss: 0.8181602358818054\n",
      "Epoch 238, Loss: 1.524007260799408, Final Batch Loss: 0.7181854248046875\n",
      "Epoch 239, Loss: 1.5337725281715393, Final Batch Loss: 0.7454811930656433\n",
      "Epoch 240, Loss: 1.5820992588996887, Final Batch Loss: 0.7469162344932556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241, Loss: 1.6046043038368225, Final Batch Loss: 0.7859209775924683\n",
      "Epoch 242, Loss: 1.522018313407898, Final Batch Loss: 0.7694717049598694\n",
      "Epoch 243, Loss: 1.5391921401023865, Final Batch Loss: 0.7533233761787415\n",
      "Epoch 244, Loss: 1.5696105360984802, Final Batch Loss: 0.7997401356697083\n",
      "Epoch 245, Loss: 1.5462550520896912, Final Batch Loss: 0.8008005023002625\n",
      "Epoch 246, Loss: 1.492937445640564, Final Batch Loss: 0.716549277305603\n",
      "Epoch 247, Loss: 1.549312949180603, Final Batch Loss: 0.8093105554580688\n",
      "Epoch 248, Loss: 1.4511915445327759, Final Batch Loss: 0.681766927242279\n",
      "Epoch 249, Loss: 1.5022034049034119, Final Batch Loss: 0.7732785940170288\n",
      "Epoch 250, Loss: 1.4475057721138, Final Batch Loss: 0.6978992223739624\n",
      "Epoch 251, Loss: 1.517107367515564, Final Batch Loss: 0.7614275813102722\n",
      "Epoch 252, Loss: 1.558955192565918, Final Batch Loss: 0.8005619645118713\n",
      "Epoch 253, Loss: 1.393333375453949, Final Batch Loss: 0.673588752746582\n",
      "Epoch 254, Loss: 1.461604356765747, Final Batch Loss: 0.7521212100982666\n",
      "Epoch 255, Loss: 1.463630199432373, Final Batch Loss: 0.7153641581535339\n",
      "Epoch 256, Loss: 1.4507696032524109, Final Batch Loss: 0.7688465118408203\n",
      "Epoch 257, Loss: 1.4124925136566162, Final Batch Loss: 0.7159848213195801\n",
      "Epoch 258, Loss: 1.4465010166168213, Final Batch Loss: 0.6932803392410278\n",
      "Epoch 259, Loss: 1.4562174677848816, Final Batch Loss: 0.720772385597229\n",
      "Epoch 260, Loss: 1.433925449848175, Final Batch Loss: 0.6981515288352966\n",
      "Epoch 261, Loss: 1.4396294355392456, Final Batch Loss: 0.7233636379241943\n",
      "Epoch 262, Loss: 1.4410368204116821, Final Batch Loss: 0.6928604245185852\n",
      "Epoch 263, Loss: 1.5511650443077087, Final Batch Loss: 0.8515512347221375\n",
      "Epoch 264, Loss: 1.4668492078781128, Final Batch Loss: 0.7079901099205017\n",
      "Epoch 265, Loss: 1.4772503972053528, Final Batch Loss: 0.6974446177482605\n",
      "Epoch 266, Loss: 1.408801794052124, Final Batch Loss: 0.7106226682662964\n",
      "Epoch 267, Loss: 1.425801396369934, Final Batch Loss: 0.732917845249176\n",
      "Epoch 268, Loss: 1.4254558682441711, Final Batch Loss: 0.7504494190216064\n",
      "Epoch 269, Loss: 1.369842767715454, Final Batch Loss: 0.6460384130477905\n",
      "Epoch 270, Loss: 1.3759132623672485, Final Batch Loss: 0.6930727958679199\n",
      "Epoch 271, Loss: 1.3849281668663025, Final Batch Loss: 0.6718392968177795\n",
      "Epoch 272, Loss: 1.3542689681053162, Final Batch Loss: 0.6472890377044678\n",
      "Epoch 273, Loss: 1.374777376651764, Final Batch Loss: 0.6528530120849609\n",
      "Epoch 274, Loss: 1.5005642175674438, Final Batch Loss: 0.8026919960975647\n",
      "Epoch 275, Loss: 1.3698270320892334, Final Batch Loss: 0.6310549974441528\n",
      "Epoch 276, Loss: 1.3456992506980896, Final Batch Loss: 0.677452802658081\n",
      "Epoch 277, Loss: 1.365280270576477, Final Batch Loss: 0.6548104882240295\n",
      "Epoch 278, Loss: 1.33620023727417, Final Batch Loss: 0.7017523050308228\n",
      "Epoch 279, Loss: 1.2879359722137451, Final Batch Loss: 0.6413598656654358\n",
      "Epoch 280, Loss: 1.4116724133491516, Final Batch Loss: 0.6743175983428955\n",
      "Epoch 281, Loss: 1.30758935213089, Final Batch Loss: 0.5722996592521667\n",
      "Epoch 282, Loss: 1.3103923797607422, Final Batch Loss: 0.6744406819343567\n",
      "Epoch 283, Loss: 1.2475088834762573, Final Batch Loss: 0.5813151001930237\n",
      "Epoch 284, Loss: 1.2949125170707703, Final Batch Loss: 0.6537969708442688\n",
      "Epoch 285, Loss: 1.3198184370994568, Final Batch Loss: 0.6852195262908936\n",
      "Epoch 286, Loss: 1.3084821701049805, Final Batch Loss: 0.6399977803230286\n",
      "Epoch 287, Loss: 1.444700002670288, Final Batch Loss: 0.7845279574394226\n",
      "Epoch 288, Loss: 1.3365495800971985, Final Batch Loss: 0.6699580550193787\n",
      "Epoch 289, Loss: 1.3197156190872192, Final Batch Loss: 0.6536944508552551\n",
      "Epoch 290, Loss: 1.3127428889274597, Final Batch Loss: 0.7334728240966797\n",
      "Epoch 291, Loss: 1.3163275718688965, Final Batch Loss: 0.6716582179069519\n",
      "Epoch 292, Loss: 1.2889254093170166, Final Batch Loss: 0.6821233630180359\n",
      "Epoch 293, Loss: 1.2900378704071045, Final Batch Loss: 0.6717959642410278\n",
      "Epoch 294, Loss: 1.2540160417556763, Final Batch Loss: 0.6290209889411926\n",
      "Epoch 295, Loss: 1.2560073733329773, Final Batch Loss: 0.6038323640823364\n",
      "Epoch 296, Loss: 1.258136510848999, Final Batch Loss: 0.6121655106544495\n",
      "Epoch 297, Loss: 1.232020616531372, Final Batch Loss: 0.6174936294555664\n",
      "Epoch 298, Loss: 1.2073944211006165, Final Batch Loss: 0.5908449292182922\n",
      "Epoch 299, Loss: 1.3066970705986023, Final Batch Loss: 0.621906578540802\n",
      "Epoch 300, Loss: 1.2224720120429993, Final Batch Loss: 0.587331235408783\n",
      "Epoch 301, Loss: 1.2901660799980164, Final Batch Loss: 0.6497928500175476\n",
      "Epoch 302, Loss: 1.3068046569824219, Final Batch Loss: 0.6474156975746155\n",
      "Epoch 303, Loss: 1.267501950263977, Final Batch Loss: 0.6616348624229431\n",
      "Epoch 304, Loss: 1.2663531303405762, Final Batch Loss: 0.6663410663604736\n",
      "Epoch 305, Loss: 1.1558045148849487, Final Batch Loss: 0.5705143213272095\n",
      "Epoch 306, Loss: 1.1751326322555542, Final Batch Loss: 0.5750669836997986\n",
      "Epoch 307, Loss: 1.2699136137962341, Final Batch Loss: 0.5742484927177429\n",
      "Epoch 308, Loss: 1.1636604070663452, Final Batch Loss: 0.5883044600486755\n",
      "Epoch 309, Loss: 1.155841588973999, Final Batch Loss: 0.5135210752487183\n",
      "Epoch 310, Loss: 1.1531953811645508, Final Batch Loss: 0.5682389140129089\n",
      "Epoch 311, Loss: 1.2669758200645447, Final Batch Loss: 0.5868484377861023\n",
      "Epoch 312, Loss: 1.226014494895935, Final Batch Loss: 0.6189160943031311\n",
      "Epoch 313, Loss: 1.2744587659835815, Final Batch Loss: 0.6581279039382935\n",
      "Epoch 314, Loss: 1.1367006599903107, Final Batch Loss: 0.4662701189517975\n",
      "Epoch 315, Loss: 1.149739921092987, Final Batch Loss: 0.5588980317115784\n",
      "Epoch 316, Loss: 1.1325905323028564, Final Batch Loss: 0.5255985856056213\n",
      "Epoch 317, Loss: 1.11986643075943, Final Batch Loss: 0.5565636157989502\n",
      "Epoch 318, Loss: 1.1488654017448425, Final Batch Loss: 0.5717751383781433\n",
      "Epoch 319, Loss: 1.119294285774231, Final Batch Loss: 0.5946053266525269\n",
      "Epoch 320, Loss: 1.1190851628780365, Final Batch Loss: 0.49909618496894836\n",
      "Epoch 321, Loss: 1.0813006162643433, Final Batch Loss: 0.554783046245575\n",
      "Epoch 322, Loss: 1.0604286193847656, Final Batch Loss: 0.535639226436615\n",
      "Epoch 323, Loss: 1.042528748512268, Final Batch Loss: 0.5033172965049744\n",
      "Epoch 324, Loss: 1.10203617811203, Final Batch Loss: 0.5131256580352783\n",
      "Epoch 325, Loss: 1.0576645731925964, Final Batch Loss: 0.5362812876701355\n",
      "Epoch 326, Loss: 1.07197767496109, Final Batch Loss: 0.4901466965675354\n",
      "Epoch 327, Loss: 1.0756614804267883, Final Batch Loss: 0.5032337307929993\n",
      "Epoch 328, Loss: 1.0588017702102661, Final Batch Loss: 0.5349256992340088\n",
      "Epoch 329, Loss: 1.0005249083042145, Final Batch Loss: 0.47527655959129333\n",
      "Epoch 330, Loss: 1.0378570556640625, Final Batch Loss: 0.495217502117157\n",
      "Epoch 331, Loss: 1.0715199708938599, Final Batch Loss: 0.5593940019607544\n",
      "Epoch 332, Loss: 1.0297067761421204, Final Batch Loss: 0.4996456503868103\n",
      "Epoch 333, Loss: 1.0428733825683594, Final Batch Loss: 0.5670042634010315\n",
      "Epoch 334, Loss: 1.0661835074424744, Final Batch Loss: 0.5575728416442871\n",
      "Epoch 335, Loss: 1.142428696155548, Final Batch Loss: 0.6401663422584534\n",
      "Epoch 336, Loss: 1.021248996257782, Final Batch Loss: 0.48791176080703735\n",
      "Epoch 337, Loss: 1.1952101588249207, Final Batch Loss: 0.5627657771110535\n",
      "Epoch 338, Loss: 1.026087909936905, Final Batch Loss: 0.49362048506736755\n",
      "Epoch 339, Loss: 1.0302531719207764, Final Batch Loss: 0.5126162767410278\n",
      "Epoch 340, Loss: 1.0497603118419647, Final Batch Loss: 0.5941158533096313\n",
      "Epoch 341, Loss: 0.9634054899215698, Final Batch Loss: 0.48980626463890076\n",
      "Epoch 342, Loss: 1.045894831418991, Final Batch Loss: 0.4865753948688507\n",
      "Epoch 343, Loss: 1.0117246806621552, Final Batch Loss: 0.5363332629203796\n",
      "Epoch 344, Loss: 1.0559715628623962, Final Batch Loss: 0.5703482031822205\n",
      "Epoch 345, Loss: 0.9800848960876465, Final Batch Loss: 0.5055280327796936\n",
      "Epoch 346, Loss: 0.9967481195926666, Final Batch Loss: 0.49327871203422546\n",
      "Epoch 347, Loss: 1.0546672940254211, Final Batch Loss: 0.5179023742675781\n",
      "Epoch 348, Loss: 0.9420557022094727, Final Batch Loss: 0.4701341986656189\n",
      "Epoch 349, Loss: 0.9262422919273376, Final Batch Loss: 0.4678548276424408\n",
      "Epoch 350, Loss: 1.0005795359611511, Final Batch Loss: 0.49950671195983887\n",
      "Epoch 351, Loss: 1.0904516577720642, Final Batch Loss: 0.5711014270782471\n",
      "Epoch 352, Loss: 0.9672519266605377, Final Batch Loss: 0.5325408577919006\n",
      "Epoch 353, Loss: 0.9438590407371521, Final Batch Loss: 0.467436820268631\n",
      "Epoch 354, Loss: 0.9831655025482178, Final Batch Loss: 0.5036606192588806\n",
      "Epoch 355, Loss: 0.9917597770690918, Final Batch Loss: 0.4344062805175781\n",
      "Epoch 356, Loss: 1.0257127285003662, Final Batch Loss: 0.5048721432685852\n",
      "Epoch 357, Loss: 0.9894895851612091, Final Batch Loss: 0.49229177832603455\n",
      "Epoch 358, Loss: 1.0473952889442444, Final Batch Loss: 0.5592612028121948\n",
      "Epoch 359, Loss: 0.9360156953334808, Final Batch Loss: 0.45815449953079224\n",
      "Epoch 360, Loss: 1.0464383363723755, Final Batch Loss: 0.603900671005249\n",
      "Epoch 361, Loss: 0.9318388402462006, Final Batch Loss: 0.3994346559047699\n",
      "Epoch 362, Loss: 0.990254282951355, Final Batch Loss: 0.48804086446762085\n",
      "Epoch 363, Loss: 0.9295044541358948, Final Batch Loss: 0.39323514699935913\n",
      "Epoch 364, Loss: 0.9926528632640839, Final Batch Loss: 0.4689424932003021\n",
      "Epoch 365, Loss: 1.000148206949234, Final Batch Loss: 0.4987587630748749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366, Loss: 0.9640918970108032, Final Batch Loss: 0.44292300939559937\n",
      "Epoch 367, Loss: 0.9760082066059113, Final Batch Loss: 0.5093141198158264\n",
      "Epoch 368, Loss: 1.0299992561340332, Final Batch Loss: 0.5135700106620789\n",
      "Epoch 369, Loss: 0.9453832805156708, Final Batch Loss: 0.47586339712142944\n",
      "Epoch 370, Loss: 0.9596563875675201, Final Batch Loss: 0.5285910964012146\n",
      "Epoch 371, Loss: 0.8666249513626099, Final Batch Loss: 0.40904244780540466\n",
      "Epoch 372, Loss: 1.0144612491130829, Final Batch Loss: 0.4669744074344635\n",
      "Epoch 373, Loss: 0.9490786790847778, Final Batch Loss: 0.4515553414821625\n",
      "Epoch 374, Loss: 0.9779604971408844, Final Batch Loss: 0.5234300494194031\n",
      "Epoch 375, Loss: 0.9324468374252319, Final Batch Loss: 0.44953107833862305\n",
      "Epoch 376, Loss: 0.9945903122425079, Final Batch Loss: 0.5169528126716614\n",
      "Epoch 377, Loss: 0.8958385586738586, Final Batch Loss: 0.40139731764793396\n",
      "Epoch 378, Loss: 0.9323466420173645, Final Batch Loss: 0.5111324787139893\n",
      "Epoch 379, Loss: 0.9596839249134064, Final Batch Loss: 0.47924163937568665\n",
      "Epoch 380, Loss: 0.8757035136222839, Final Batch Loss: 0.40995046496391296\n",
      "Epoch 381, Loss: 0.907829076051712, Final Batch Loss: 0.44194498658180237\n",
      "Epoch 382, Loss: 0.9397691786289215, Final Batch Loss: 0.47535115480422974\n",
      "Epoch 383, Loss: 1.0640296936035156, Final Batch Loss: 0.5362616181373596\n",
      "Epoch 384, Loss: 0.9362994432449341, Final Batch Loss: 0.49637073278427124\n",
      "Epoch 385, Loss: 0.9674675464630127, Final Batch Loss: 0.4421372413635254\n",
      "Epoch 386, Loss: 0.9445432126522064, Final Batch Loss: 0.4565536677837372\n",
      "Epoch 387, Loss: 1.023218035697937, Final Batch Loss: 0.5140148997306824\n",
      "Epoch 388, Loss: 0.8973850309848785, Final Batch Loss: 0.4340150058269501\n",
      "Epoch 389, Loss: 0.9355619549751282, Final Batch Loss: 0.4675168991088867\n",
      "Epoch 390, Loss: 0.9198144972324371, Final Batch Loss: 0.46546268463134766\n",
      "Epoch 391, Loss: 0.965834379196167, Final Batch Loss: 0.4660995900630951\n",
      "Epoch 392, Loss: 0.9150408804416656, Final Batch Loss: 0.4338323473930359\n",
      "Epoch 393, Loss: 0.9523729383945465, Final Batch Loss: 0.4983519911766052\n",
      "Epoch 394, Loss: 0.9327290952205658, Final Batch Loss: 0.4476204514503479\n",
      "Epoch 395, Loss: 0.9761562347412109, Final Batch Loss: 0.5298357009887695\n",
      "Epoch 396, Loss: 0.9128394722938538, Final Batch Loss: 0.47944653034210205\n",
      "Epoch 397, Loss: 0.8940068185329437, Final Batch Loss: 0.4152275025844574\n",
      "Epoch 398, Loss: 0.9712089002132416, Final Batch Loss: 0.4884189963340759\n",
      "Epoch 399, Loss: 0.9329590797424316, Final Batch Loss: 0.48030585050582886\n",
      "Epoch 400, Loss: 0.9330777525901794, Final Batch Loss: 0.49404653906822205\n",
      "Epoch 401, Loss: 0.8343294262886047, Final Batch Loss: 0.36261284351348877\n",
      "Epoch 402, Loss: 0.9141995906829834, Final Batch Loss: 0.48262694478034973\n",
      "Epoch 403, Loss: 0.9625574946403503, Final Batch Loss: 0.48417943716049194\n",
      "Epoch 404, Loss: 0.918274849653244, Final Batch Loss: 0.4814569652080536\n",
      "Epoch 405, Loss: 0.9470162987709045, Final Batch Loss: 0.4890819489955902\n",
      "Epoch 406, Loss: 0.8973263502120972, Final Batch Loss: 0.4320108890533447\n",
      "Epoch 407, Loss: 0.9468604922294617, Final Batch Loss: 0.4556199610233307\n",
      "Epoch 408, Loss: 0.8361943066120148, Final Batch Loss: 0.40373215079307556\n",
      "Epoch 409, Loss: 0.8792127370834351, Final Batch Loss: 0.42252662777900696\n",
      "Epoch 410, Loss: 0.9241042733192444, Final Batch Loss: 0.4444340169429779\n",
      "Epoch 411, Loss: 0.9796133637428284, Final Batch Loss: 0.5363876819610596\n",
      "Epoch 412, Loss: 0.9961740076541901, Final Batch Loss: 0.5533913373947144\n",
      "Epoch 413, Loss: 0.9573738276958466, Final Batch Loss: 0.5022321343421936\n",
      "Epoch 414, Loss: 0.8147130012512207, Final Batch Loss: 0.4042932093143463\n",
      "Epoch 415, Loss: 0.9675325751304626, Final Batch Loss: 0.559059739112854\n",
      "Epoch 416, Loss: 0.9812629818916321, Final Batch Loss: 0.48689162731170654\n",
      "Epoch 417, Loss: 0.8804776072502136, Final Batch Loss: 0.41612011194229126\n",
      "Epoch 418, Loss: 1.0583946704864502, Final Batch Loss: 0.6024327278137207\n",
      "Epoch 419, Loss: 0.9372131824493408, Final Batch Loss: 0.4986376166343689\n",
      "Epoch 420, Loss: 0.923679769039154, Final Batch Loss: 0.5096293091773987\n",
      "Epoch 421, Loss: 0.903321385383606, Final Batch Loss: 0.45644405484199524\n",
      "Epoch 422, Loss: 0.9198996722698212, Final Batch Loss: 0.4446914494037628\n",
      "Epoch 423, Loss: 0.9871717989444733, Final Batch Loss: 0.4861753284931183\n",
      "Epoch 424, Loss: 0.9198855757713318, Final Batch Loss: 0.48020967841148376\n",
      "Epoch 425, Loss: 0.8738148808479309, Final Batch Loss: 0.4838380813598633\n",
      "Epoch 426, Loss: 0.8418483734130859, Final Batch Loss: 0.4321516454219818\n",
      "Epoch 427, Loss: 0.848529189825058, Final Batch Loss: 0.4443971514701843\n",
      "Epoch 428, Loss: 0.8179558217525482, Final Batch Loss: 0.3783031105995178\n",
      "Epoch 429, Loss: 0.8423977494239807, Final Batch Loss: 0.37218987941741943\n",
      "Epoch 430, Loss: 0.9329863488674164, Final Batch Loss: 0.48318079113960266\n",
      "Epoch 431, Loss: 0.9553333222866058, Final Batch Loss: 0.4989543855190277\n",
      "Epoch 432, Loss: 0.9242907762527466, Final Batch Loss: 0.5107100605964661\n",
      "Epoch 433, Loss: 0.9419239461421967, Final Batch Loss: 0.4906838536262512\n",
      "Epoch 434, Loss: 0.8729483783245087, Final Batch Loss: 0.4550653398036957\n",
      "Epoch 435, Loss: 0.8898527026176453, Final Batch Loss: 0.4304959177970886\n",
      "Epoch 436, Loss: 0.9041557908058167, Final Batch Loss: 0.39766252040863037\n",
      "Epoch 437, Loss: 0.8708333075046539, Final Batch Loss: 0.3844132125377655\n",
      "Epoch 438, Loss: 0.8992834091186523, Final Batch Loss: 0.45809033513069153\n",
      "Epoch 439, Loss: 0.9691524803638458, Final Batch Loss: 0.5359163284301758\n",
      "Epoch 440, Loss: 0.9471752345561981, Final Batch Loss: 0.5051850080490112\n",
      "Epoch 441, Loss: 0.9071830809116364, Final Batch Loss: 0.5085723996162415\n",
      "Epoch 442, Loss: 0.9655784964561462, Final Batch Loss: 0.5153530836105347\n",
      "Epoch 443, Loss: 0.8743659853935242, Final Batch Loss: 0.4369470179080963\n",
      "Epoch 444, Loss: 0.8946504890918732, Final Batch Loss: 0.4842052161693573\n",
      "Epoch 445, Loss: 0.8793658912181854, Final Batch Loss: 0.45903295278549194\n",
      "Epoch 446, Loss: 0.941264808177948, Final Batch Loss: 0.499384343624115\n",
      "Epoch 447, Loss: 0.8119330704212189, Final Batch Loss: 0.34437984228134155\n",
      "Epoch 448, Loss: 0.8623019158840179, Final Batch Loss: 0.3979296088218689\n",
      "Epoch 449, Loss: 0.8572137653827667, Final Batch Loss: 0.4662313461303711\n",
      "Epoch 450, Loss: 0.7443336844444275, Final Batch Loss: 0.34355294704437256\n",
      "Epoch 451, Loss: 0.8740882575511932, Final Batch Loss: 0.44470030069351196\n",
      "Epoch 452, Loss: 0.9468714892864227, Final Batch Loss: 0.49030765891075134\n",
      "Epoch 453, Loss: 0.8345147967338562, Final Batch Loss: 0.37272247672080994\n",
      "Epoch 454, Loss: 0.9847407639026642, Final Batch Loss: 0.5047100186347961\n",
      "Epoch 455, Loss: 0.8019745349884033, Final Batch Loss: 0.41808444261550903\n",
      "Epoch 456, Loss: 0.8040194511413574, Final Batch Loss: 0.3719387650489807\n",
      "Epoch 457, Loss: 0.8794121742248535, Final Batch Loss: 0.44267746806144714\n",
      "Epoch 458, Loss: 0.8258680999279022, Final Batch Loss: 0.39439335465431213\n",
      "Epoch 459, Loss: 0.8030973076820374, Final Batch Loss: 0.39349764585494995\n",
      "Epoch 460, Loss: 0.8534143567085266, Final Batch Loss: 0.4433240294456482\n",
      "Epoch 461, Loss: 0.8780996203422546, Final Batch Loss: 0.44582808017730713\n",
      "Epoch 462, Loss: 0.8729584813117981, Final Batch Loss: 0.4749990701675415\n",
      "Epoch 463, Loss: 0.8791000843048096, Final Batch Loss: 0.40588656067848206\n",
      "Epoch 464, Loss: 0.859654039144516, Final Batch Loss: 0.4196847081184387\n",
      "Epoch 465, Loss: 0.768924355506897, Final Batch Loss: 0.3809448182582855\n",
      "Epoch 466, Loss: 0.817338764667511, Final Batch Loss: 0.40971389412879944\n",
      "Epoch 467, Loss: 0.886608898639679, Final Batch Loss: 0.42411836981773376\n",
      "Epoch 468, Loss: 0.896611750125885, Final Batch Loss: 0.4975394010543823\n",
      "Epoch 469, Loss: 0.7964547276496887, Final Batch Loss: 0.37191376090049744\n",
      "Epoch 470, Loss: 0.7959990501403809, Final Batch Loss: 0.3842369616031647\n",
      "Epoch 471, Loss: 0.8178922533988953, Final Batch Loss: 0.4308294653892517\n",
      "Epoch 472, Loss: 0.8687337338924408, Final Batch Loss: 0.4305630326271057\n",
      "Epoch 473, Loss: 0.7819648087024689, Final Batch Loss: 0.37862464785575867\n",
      "Epoch 474, Loss: 0.7905435264110565, Final Batch Loss: 0.3722216784954071\n",
      "Epoch 475, Loss: 0.834263265132904, Final Batch Loss: 0.4365805387496948\n",
      "Epoch 476, Loss: 0.8648991882801056, Final Batch Loss: 0.47323334217071533\n",
      "Epoch 477, Loss: 0.8389439880847931, Final Batch Loss: 0.4164748787879944\n",
      "Epoch 478, Loss: 0.8903665244579315, Final Batch Loss: 0.4228766858577728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479, Loss: 0.770186185836792, Final Batch Loss: 0.39059028029441833\n",
      "Epoch 480, Loss: 0.8966652154922485, Final Batch Loss: 0.4426078498363495\n",
      "Epoch 481, Loss: 0.8234943449497223, Final Batch Loss: 0.44507360458374023\n",
      "Epoch 482, Loss: 0.865465372800827, Final Batch Loss: 0.4662838876247406\n",
      "Epoch 483, Loss: 0.7649184167385101, Final Batch Loss: 0.3412923514842987\n",
      "Epoch 484, Loss: 0.8135092854499817, Final Batch Loss: 0.37560123205184937\n",
      "Epoch 485, Loss: 0.7975502610206604, Final Batch Loss: 0.38393616676330566\n",
      "Epoch 486, Loss: 0.8853224813938141, Final Batch Loss: 0.4478004574775696\n",
      "Epoch 487, Loss: 0.88153937458992, Final Batch Loss: 0.43003013730049133\n",
      "Epoch 488, Loss: 0.8089353740215302, Final Batch Loss: 0.35755395889282227\n",
      "Epoch 489, Loss: 0.7714698910713196, Final Batch Loss: 0.3977786600589752\n",
      "Epoch 490, Loss: 0.8518183529376984, Final Batch Loss: 0.4202996790409088\n",
      "Epoch 491, Loss: 0.8509711623191833, Final Batch Loss: 0.4244367182254791\n",
      "Epoch 492, Loss: 0.8121241629123688, Final Batch Loss: 0.40661510825157166\n",
      "Epoch 493, Loss: 0.7938558161258698, Final Batch Loss: 0.37911587953567505\n",
      "Epoch 494, Loss: 0.904968798160553, Final Batch Loss: 0.47844675183296204\n",
      "Epoch 495, Loss: 0.8472062647342682, Final Batch Loss: 0.40379980206489563\n",
      "Epoch 496, Loss: 0.8024623692035675, Final Batch Loss: 0.4091395139694214\n",
      "Epoch 497, Loss: 0.8942431211471558, Final Batch Loss: 0.4921188950538635\n",
      "Epoch 498, Loss: 0.855400800704956, Final Batch Loss: 0.4020098149776459\n",
      "Epoch 499, Loss: 0.8540383875370026, Final Batch Loss: 0.41424980759620667\n",
      "Epoch 500, Loss: 0.7774579524993896, Final Batch Loss: 0.40994754433631897\n",
      "Epoch 501, Loss: 0.8760224282741547, Final Batch Loss: 0.40455931425094604\n",
      "Epoch 502, Loss: 0.8808930516242981, Final Batch Loss: 0.48146089911460876\n",
      "Epoch 503, Loss: 0.7909734547138214, Final Batch Loss: 0.34327566623687744\n",
      "Epoch 504, Loss: 0.878206878900528, Final Batch Loss: 0.4293398857116699\n",
      "Epoch 505, Loss: 0.8347934782505035, Final Batch Loss: 0.44582945108413696\n",
      "Epoch 506, Loss: 0.8275466561317444, Final Batch Loss: 0.40882182121276855\n",
      "Epoch 507, Loss: 0.7640913426876068, Final Batch Loss: 0.3771461248397827\n",
      "Epoch 508, Loss: 0.8103064596652985, Final Batch Loss: 0.37744706869125366\n",
      "Epoch 509, Loss: 0.8833514451980591, Final Batch Loss: 0.447325736284256\n",
      "Epoch 510, Loss: 0.9097238183021545, Final Batch Loss: 0.4954833984375\n",
      "Epoch 511, Loss: 0.7731831967830658, Final Batch Loss: 0.42998990416526794\n",
      "Epoch 512, Loss: 0.7511902153491974, Final Batch Loss: 0.3801715075969696\n",
      "Epoch 513, Loss: 0.8602050244808197, Final Batch Loss: 0.48559802770614624\n",
      "Epoch 514, Loss: 0.8561374247074127, Final Batch Loss: 0.4413154721260071\n",
      "Epoch 515, Loss: 0.7687097787857056, Final Batch Loss: 0.3647063374519348\n",
      "Epoch 516, Loss: 0.7758961617946625, Final Batch Loss: 0.380319744348526\n",
      "Epoch 517, Loss: 0.7791851460933685, Final Batch Loss: 0.40594083070755005\n",
      "Epoch 518, Loss: 0.7722300291061401, Final Batch Loss: 0.3304212987422943\n",
      "Epoch 519, Loss: 0.7980198264122009, Final Batch Loss: 0.37556347250938416\n",
      "Epoch 520, Loss: 0.8476213216781616, Final Batch Loss: 0.4168461859226227\n",
      "Epoch 521, Loss: 0.8491060137748718, Final Batch Loss: 0.4219876825809479\n",
      "Epoch 522, Loss: 0.8114413022994995, Final Batch Loss: 0.396683931350708\n",
      "Epoch 523, Loss: 0.7535025477409363, Final Batch Loss: 0.38605940341949463\n",
      "Epoch 524, Loss: 0.7661042511463165, Final Batch Loss: 0.34299933910369873\n",
      "Epoch 525, Loss: 0.9085228145122528, Final Batch Loss: 0.43494898080825806\n",
      "Epoch 526, Loss: 0.8387338221073151, Final Batch Loss: 0.41431301832199097\n",
      "Epoch 527, Loss: 0.7371287047863007, Final Batch Loss: 0.344276487827301\n",
      "Epoch 528, Loss: 0.7906972169876099, Final Batch Loss: 0.37393203377723694\n",
      "Epoch 529, Loss: 0.7729499042034149, Final Batch Loss: 0.39936742186546326\n",
      "Epoch 530, Loss: 0.7277912795543671, Final Batch Loss: 0.3062750995159149\n",
      "Epoch 531, Loss: 0.7669028341770172, Final Batch Loss: 0.3773652911186218\n",
      "Epoch 532, Loss: 0.7636562883853912, Final Batch Loss: 0.4227016568183899\n",
      "Epoch 533, Loss: 0.7840790152549744, Final Batch Loss: 0.38914236426353455\n",
      "Epoch 534, Loss: 0.8886443674564362, Final Batch Loss: 0.46011030673980713\n",
      "Epoch 535, Loss: 0.8336172699928284, Final Batch Loss: 0.3571912348270416\n",
      "Epoch 536, Loss: 0.8017489612102509, Final Batch Loss: 0.4110593795776367\n",
      "Epoch 537, Loss: 0.7617250382900238, Final Batch Loss: 0.3431072533130646\n",
      "Epoch 538, Loss: 0.7851451933383942, Final Batch Loss: 0.3877921998500824\n",
      "Epoch 539, Loss: 0.752146989107132, Final Batch Loss: 0.34450986981391907\n",
      "Epoch 540, Loss: 0.87590491771698, Final Batch Loss: 0.39819854497909546\n",
      "Epoch 541, Loss: 0.8772446513175964, Final Batch Loss: 0.43368998169898987\n",
      "Epoch 542, Loss: 0.7790873050689697, Final Batch Loss: 0.3565863370895386\n",
      "Epoch 543, Loss: 0.7607371211051941, Final Batch Loss: 0.3366260230541229\n",
      "Epoch 544, Loss: 0.9728051126003265, Final Batch Loss: 0.5221694111824036\n",
      "Epoch 545, Loss: 0.7753913998603821, Final Batch Loss: 0.33776405453681946\n",
      "Epoch 546, Loss: 0.7584450244903564, Final Batch Loss: 0.29123619198799133\n",
      "Epoch 547, Loss: 0.7586454451084137, Final Batch Loss: 0.36422455310821533\n",
      "Epoch 548, Loss: 0.7827059626579285, Final Batch Loss: 0.4199869930744171\n",
      "Epoch 549, Loss: 0.886448323726654, Final Batch Loss: 0.5199460983276367\n",
      "Epoch 550, Loss: 0.8071720898151398, Final Batch Loss: 0.42503821849823\n",
      "Epoch 551, Loss: 0.7085608541965485, Final Batch Loss: 0.3017246127128601\n",
      "Epoch 552, Loss: 0.8714517652988434, Final Batch Loss: 0.5093228220939636\n",
      "Epoch 553, Loss: 0.7872109413146973, Final Batch Loss: 0.3689527213573456\n",
      "Epoch 554, Loss: 0.7288108766078949, Final Batch Loss: 0.35531988739967346\n",
      "Epoch 555, Loss: 0.80784472823143, Final Batch Loss: 0.38337692618370056\n",
      "Epoch 556, Loss: 0.7842645943164825, Final Batch Loss: 0.3700748085975647\n",
      "Epoch 557, Loss: 0.813605785369873, Final Batch Loss: 0.4329662024974823\n",
      "Epoch 558, Loss: 0.8007698357105255, Final Batch Loss: 0.391195684671402\n",
      "Epoch 559, Loss: 0.7686547040939331, Final Batch Loss: 0.42176899313926697\n",
      "Epoch 560, Loss: 0.7668989598751068, Final Batch Loss: 0.34294751286506653\n",
      "Epoch 561, Loss: 0.8112660348415375, Final Batch Loss: 0.41504520177841187\n",
      "Epoch 562, Loss: 0.8727840781211853, Final Batch Loss: 0.456832617521286\n",
      "Epoch 563, Loss: 0.737749844789505, Final Batch Loss: 0.3987794816493988\n",
      "Epoch 564, Loss: 0.8045549988746643, Final Batch Loss: 0.4075513184070587\n",
      "Epoch 565, Loss: 0.8027556240558624, Final Batch Loss: 0.39734604954719543\n",
      "Epoch 566, Loss: 0.8047813177108765, Final Batch Loss: 0.3696698546409607\n",
      "Epoch 567, Loss: 0.7762554287910461, Final Batch Loss: 0.39031651616096497\n",
      "Epoch 568, Loss: 0.7809492647647858, Final Batch Loss: 0.388121098279953\n",
      "Epoch 569, Loss: 0.7496697008609772, Final Batch Loss: 0.3627181649208069\n",
      "Epoch 570, Loss: 0.7281766831874847, Final Batch Loss: 0.268235981464386\n",
      "Epoch 571, Loss: 0.8037569522857666, Final Batch Loss: 0.4281974732875824\n",
      "Epoch 572, Loss: 0.7833597362041473, Final Batch Loss: 0.4026535451412201\n",
      "Epoch 573, Loss: 0.7543151378631592, Final Batch Loss: 0.37958046793937683\n",
      "Epoch 574, Loss: 0.7519038021564484, Final Batch Loss: 0.36764585971832275\n",
      "Epoch 575, Loss: 0.8284699618816376, Final Batch Loss: 0.4245781898498535\n",
      "Epoch 576, Loss: 0.8631643354892731, Final Batch Loss: 0.45579662919044495\n",
      "Epoch 577, Loss: 0.7328553199768066, Final Batch Loss: 0.3294146955013275\n",
      "Epoch 578, Loss: 0.7852429449558258, Final Batch Loss: 0.42948806285858154\n",
      "Epoch 579, Loss: 0.7713717520236969, Final Batch Loss: 0.3662063479423523\n",
      "Epoch 580, Loss: 0.7745658159255981, Final Batch Loss: 0.35571548342704773\n",
      "Epoch 581, Loss: 0.7126113176345825, Final Batch Loss: 0.3395703136920929\n",
      "Epoch 582, Loss: 0.69211146235466, Final Batch Loss: 0.30541113018989563\n",
      "Epoch 583, Loss: 0.7591665089130402, Final Batch Loss: 0.37852606177330017\n",
      "Epoch 584, Loss: 0.7669196724891663, Final Batch Loss: 0.4255841076374054\n",
      "Epoch 585, Loss: 0.7939803898334503, Final Batch Loss: 0.3598034679889679\n",
      "Epoch 586, Loss: 0.7490312159061432, Final Batch Loss: 0.3371359407901764\n",
      "Epoch 587, Loss: 0.7198928594589233, Final Batch Loss: 0.39073172211647034\n",
      "Epoch 588, Loss: 0.839168906211853, Final Batch Loss: 0.46572983264923096\n",
      "Epoch 589, Loss: 0.7466638088226318, Final Batch Loss: 0.33783426880836487\n",
      "Epoch 590, Loss: 0.7749392986297607, Final Batch Loss: 0.3884461224079132\n",
      "Epoch 591, Loss: 0.8200678527355194, Final Batch Loss: 0.4336942732334137\n",
      "Epoch 592, Loss: 0.8118841350078583, Final Batch Loss: 0.4587169289588928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593, Loss: 0.8415067493915558, Final Batch Loss: 0.4164694547653198\n",
      "Epoch 594, Loss: 0.8136181831359863, Final Batch Loss: 0.45862022042274475\n",
      "Epoch 595, Loss: 0.8980422914028168, Final Batch Loss: 0.49535128474235535\n",
      "Epoch 596, Loss: 0.7794988751411438, Final Batch Loss: 0.41857197880744934\n",
      "Epoch 597, Loss: 0.8090509176254272, Final Batch Loss: 0.3822033405303955\n",
      "Epoch 598, Loss: 0.7560441792011261, Final Batch Loss: 0.39558765292167664\n",
      "Epoch 599, Loss: 0.8193781673908234, Final Batch Loss: 0.4136984348297119\n",
      "Epoch 600, Loss: 0.7913048565387726, Final Batch Loss: 0.43205851316452026\n",
      "Epoch 601, Loss: 0.6724242568016052, Final Batch Loss: 0.3573130667209625\n",
      "Epoch 602, Loss: 0.7001360058784485, Final Batch Loss: 0.346735417842865\n",
      "Epoch 603, Loss: 0.7101022303104401, Final Batch Loss: 0.3320026695728302\n",
      "Epoch 604, Loss: 0.7574052214622498, Final Batch Loss: 0.3616221845149994\n",
      "Epoch 605, Loss: 0.7947902679443359, Final Batch Loss: 0.3517603576183319\n",
      "Epoch 606, Loss: 0.756356418132782, Final Batch Loss: 0.3704167604446411\n",
      "Epoch 607, Loss: 0.7998852431774139, Final Batch Loss: 0.4341413378715515\n",
      "Epoch 608, Loss: 0.7824602723121643, Final Batch Loss: 0.3920271396636963\n",
      "Epoch 609, Loss: 0.7256690561771393, Final Batch Loss: 0.33009740710258484\n",
      "Epoch 610, Loss: 0.8342549502849579, Final Batch Loss: 0.46375393867492676\n",
      "Epoch 611, Loss: 0.7646214067935944, Final Batch Loss: 0.35753530263900757\n",
      "Epoch 612, Loss: 0.738211989402771, Final Batch Loss: 0.36634546518325806\n",
      "Epoch 613, Loss: 0.7703574597835541, Final Batch Loss: 0.38049009442329407\n",
      "Epoch 614, Loss: 0.7855333685874939, Final Batch Loss: 0.3996488153934479\n",
      "Epoch 615, Loss: 0.802789568901062, Final Batch Loss: 0.39193424582481384\n",
      "Epoch 616, Loss: 0.8046590089797974, Final Batch Loss: 0.38978111743927\n",
      "Epoch 617, Loss: 0.7868868410587311, Final Batch Loss: 0.3716312050819397\n",
      "Epoch 618, Loss: 0.815547913312912, Final Batch Loss: 0.40692824125289917\n",
      "Epoch 619, Loss: 0.7779547870159149, Final Batch Loss: 0.3783671259880066\n",
      "Epoch 620, Loss: 0.7534758746623993, Final Batch Loss: 0.3775782585144043\n",
      "Epoch 621, Loss: 0.772966593503952, Final Batch Loss: 0.35793471336364746\n",
      "Epoch 622, Loss: 0.7728943824768066, Final Batch Loss: 0.34252288937568665\n",
      "Epoch 623, Loss: 0.8088787496089935, Final Batch Loss: 0.41875889897346497\n",
      "Epoch 624, Loss: 0.7742242515087128, Final Batch Loss: 0.39500007033348083\n",
      "Epoch 625, Loss: 0.7514966130256653, Final Batch Loss: 0.36269307136535645\n",
      "Epoch 626, Loss: 0.7863881289958954, Final Batch Loss: 0.42409995198249817\n",
      "Epoch 627, Loss: 0.7772803902626038, Final Batch Loss: 0.426561176776886\n",
      "Epoch 628, Loss: 0.7762021720409393, Final Batch Loss: 0.4263489544391632\n",
      "Epoch 629, Loss: 0.7621978223323822, Final Batch Loss: 0.38306742906570435\n",
      "Epoch 630, Loss: 0.7878562808036804, Final Batch Loss: 0.3747764825820923\n",
      "Epoch 631, Loss: 0.7417334318161011, Final Batch Loss: 0.39534375071525574\n",
      "Epoch 632, Loss: 0.7884557545185089, Final Batch Loss: 0.36218196153640747\n",
      "Epoch 633, Loss: 0.7552152574062347, Final Batch Loss: 0.36079859733581543\n",
      "Epoch 634, Loss: 0.7159087359905243, Final Batch Loss: 0.3138015568256378\n",
      "Epoch 635, Loss: 0.7452476620674133, Final Batch Loss: 0.3251068592071533\n",
      "Epoch 636, Loss: 0.7396109998226166, Final Batch Loss: 0.34340304136276245\n",
      "Epoch 637, Loss: 0.7990168035030365, Final Batch Loss: 0.4082726240158081\n",
      "Epoch 638, Loss: 0.745967447757721, Final Batch Loss: 0.39336344599723816\n",
      "Epoch 639, Loss: 0.7826913893222809, Final Batch Loss: 0.4311477839946747\n",
      "Epoch 640, Loss: 0.779029130935669, Final Batch Loss: 0.3754175305366516\n",
      "Epoch 641, Loss: 0.7774258852005005, Final Batch Loss: 0.433383047580719\n",
      "Epoch 642, Loss: 0.7216819822788239, Final Batch Loss: 0.3867565095424652\n",
      "Epoch 643, Loss: 0.7426639199256897, Final Batch Loss: 0.3707737922668457\n",
      "Epoch 644, Loss: 0.7753987312316895, Final Batch Loss: 0.34607890248298645\n",
      "Epoch 645, Loss: 0.7419831156730652, Final Batch Loss: 0.3147922158241272\n",
      "Epoch 646, Loss: 0.7967365682125092, Final Batch Loss: 0.40787601470947266\n",
      "Epoch 647, Loss: 0.813957542181015, Final Batch Loss: 0.4111410081386566\n",
      "Epoch 648, Loss: 0.735976368188858, Final Batch Loss: 0.30217286944389343\n",
      "Epoch 649, Loss: 0.7276260852813721, Final Batch Loss: 0.36218953132629395\n",
      "Epoch 650, Loss: 0.7710833847522736, Final Batch Loss: 0.3862359821796417\n",
      "Epoch 651, Loss: 0.7799925208091736, Final Batch Loss: 0.36036813259124756\n",
      "Epoch 652, Loss: 0.7909789681434631, Final Batch Loss: 0.38852769136428833\n",
      "Epoch 653, Loss: 0.7638779878616333, Final Batch Loss: 0.4105823040008545\n",
      "Epoch 654, Loss: 0.7824053168296814, Final Batch Loss: 0.4349619448184967\n",
      "Epoch 655, Loss: 0.7547497749328613, Final Batch Loss: 0.40146517753601074\n",
      "Epoch 656, Loss: 0.7729825973510742, Final Batch Loss: 0.398004412651062\n",
      "Epoch 657, Loss: 0.7092555463314056, Final Batch Loss: 0.3414074182510376\n",
      "Epoch 658, Loss: 0.8243059515953064, Final Batch Loss: 0.43306124210357666\n",
      "Epoch 659, Loss: 0.6833019554615021, Final Batch Loss: 0.3456580340862274\n",
      "Epoch 660, Loss: 0.760742574930191, Final Batch Loss: 0.37772393226623535\n",
      "Epoch 661, Loss: 0.7000672221183777, Final Batch Loss: 0.30361106991767883\n",
      "Epoch 662, Loss: 0.7380951046943665, Final Batch Loss: 0.372060626745224\n",
      "Epoch 663, Loss: 0.7720066606998444, Final Batch Loss: 0.3545571565628052\n",
      "Epoch 664, Loss: 0.7561901211738586, Final Batch Loss: 0.3991059958934784\n",
      "Epoch 665, Loss: 0.6871726512908936, Final Batch Loss: 0.36141371726989746\n",
      "Epoch 666, Loss: 0.7822339832782745, Final Batch Loss: 0.44660425186157227\n",
      "Epoch 667, Loss: 0.734011322259903, Final Batch Loss: 0.36945003271102905\n",
      "Epoch 668, Loss: 0.7561422884464264, Final Batch Loss: 0.3746599853038788\n",
      "Epoch 669, Loss: 0.6682014465332031, Final Batch Loss: 0.29371377825737\n",
      "Epoch 670, Loss: 0.7467220723628998, Final Batch Loss: 0.36370885372161865\n",
      "Epoch 671, Loss: 0.7122849225997925, Final Batch Loss: 0.3601958155632019\n",
      "Epoch 672, Loss: 0.8451277315616608, Final Batch Loss: 0.4347771406173706\n",
      "Epoch 673, Loss: 0.7701795697212219, Final Batch Loss: 0.3458479344844818\n",
      "Epoch 674, Loss: 0.7630199790000916, Final Batch Loss: 0.3729622960090637\n",
      "Epoch 675, Loss: 0.7846692800521851, Final Batch Loss: 0.45555850863456726\n",
      "Epoch 676, Loss: 0.7398800849914551, Final Batch Loss: 0.34875136613845825\n",
      "Epoch 677, Loss: 0.746929794549942, Final Batch Loss: 0.38350898027420044\n",
      "Epoch 678, Loss: 0.7003577053546906, Final Batch Loss: 0.35142651200294495\n",
      "Epoch 679, Loss: 0.7432421147823334, Final Batch Loss: 0.3584749102592468\n",
      "Epoch 680, Loss: 0.738088846206665, Final Batch Loss: 0.36282879114151\n",
      "Epoch 681, Loss: 0.8426151871681213, Final Batch Loss: 0.4680907428264618\n",
      "Epoch 682, Loss: 0.7333191335201263, Final Batch Loss: 0.41299545764923096\n",
      "Epoch 683, Loss: 0.7389193773269653, Final Batch Loss: 0.3812839984893799\n",
      "Epoch 684, Loss: 0.7430470883846283, Final Batch Loss: 0.35513317584991455\n",
      "Epoch 685, Loss: 0.7649684250354767, Final Batch Loss: 0.4292466342449188\n",
      "Epoch 686, Loss: 0.7287544310092926, Final Batch Loss: 0.366970419883728\n",
      "Epoch 687, Loss: 0.7689036726951599, Final Batch Loss: 0.43104925751686096\n",
      "Epoch 688, Loss: 0.7197869122028351, Final Batch Loss: 0.3709414601325989\n",
      "Epoch 689, Loss: 0.7697723507881165, Final Batch Loss: 0.3247271180152893\n",
      "Epoch 690, Loss: 0.7064864635467529, Final Batch Loss: 0.36981001496315\n",
      "Epoch 691, Loss: 0.696864664554596, Final Batch Loss: 0.30984601378440857\n",
      "Epoch 692, Loss: 0.774880051612854, Final Batch Loss: 0.4294663071632385\n",
      "Epoch 693, Loss: 0.7368514835834503, Final Batch Loss: 0.3772190511226654\n",
      "Epoch 694, Loss: 0.7345041930675507, Final Batch Loss: 0.3706650733947754\n",
      "Epoch 695, Loss: 0.7449179291725159, Final Batch Loss: 0.3939463794231415\n",
      "Epoch 696, Loss: 0.6990848481655121, Final Batch Loss: 0.299255907535553\n",
      "Epoch 697, Loss: 0.7251230776309967, Final Batch Loss: 0.3447363078594208\n",
      "Epoch 698, Loss: 0.7196300327777863, Final Batch Loss: 0.3790822923183441\n",
      "Epoch 699, Loss: 0.6871166527271271, Final Batch Loss: 0.33770042657852173\n",
      "Epoch 700, Loss: 0.7527408599853516, Final Batch Loss: 0.35164874792099\n",
      "Epoch 701, Loss: 0.763775110244751, Final Batch Loss: 0.40011388063430786\n",
      "Epoch 702, Loss: 0.8072310388088226, Final Batch Loss: 0.48756515979766846\n",
      "Epoch 703, Loss: 0.787058413028717, Final Batch Loss: 0.406260222196579\n",
      "Epoch 704, Loss: 0.7129527628421783, Final Batch Loss: 0.3604242205619812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 705, Loss: 0.681135505437851, Final Batch Loss: 0.3445151448249817\n",
      "Epoch 706, Loss: 0.660040020942688, Final Batch Loss: 0.3065389096736908\n",
      "Epoch 707, Loss: 0.7074105739593506, Final Batch Loss: 0.33673566579818726\n",
      "Epoch 708, Loss: 0.7425706684589386, Final Batch Loss: 0.35216984152793884\n",
      "Epoch 709, Loss: 0.7070924639701843, Final Batch Loss: 0.3415854573249817\n",
      "Epoch 710, Loss: 0.6996234655380249, Final Batch Loss: 0.3671576976776123\n",
      "Epoch 711, Loss: 0.7089950740337372, Final Batch Loss: 0.3240838050842285\n",
      "Epoch 712, Loss: 0.7247504591941833, Final Batch Loss: 0.3630538582801819\n",
      "Epoch 713, Loss: 0.7427970170974731, Final Batch Loss: 0.3903168737888336\n",
      "Epoch 714, Loss: 0.8230545520782471, Final Batch Loss: 0.4549509584903717\n",
      "Epoch 715, Loss: 0.7449900507926941, Final Batch Loss: 0.3847830891609192\n",
      "Epoch 716, Loss: 0.8395368456840515, Final Batch Loss: 0.48604270815849304\n",
      "Epoch 717, Loss: 0.812636137008667, Final Batch Loss: 0.41564100980758667\n",
      "Epoch 718, Loss: 0.724166601896286, Final Batch Loss: 0.33222320675849915\n",
      "Epoch 719, Loss: 0.7629508376121521, Final Batch Loss: 0.4191094934940338\n",
      "Epoch 720, Loss: 0.7576975226402283, Final Batch Loss: 0.3434160351753235\n",
      "Epoch 721, Loss: 0.7586389780044556, Final Batch Loss: 0.40604835748672485\n",
      "Epoch 722, Loss: 0.7011795341968536, Final Batch Loss: 0.3044034540653229\n",
      "Epoch 723, Loss: 0.7154086232185364, Final Batch Loss: 0.3174334764480591\n",
      "Epoch 724, Loss: 0.7316234409809113, Final Batch Loss: 0.38193607330322266\n",
      "Epoch 725, Loss: 0.7043441832065582, Final Batch Loss: 0.3433026969432831\n",
      "Epoch 726, Loss: 0.6798726320266724, Final Batch Loss: 0.2858467698097229\n",
      "Epoch 727, Loss: 0.749517410993576, Final Batch Loss: 0.4099486470222473\n",
      "Epoch 728, Loss: 0.7255821526050568, Final Batch Loss: 0.4189877212047577\n",
      "Epoch 729, Loss: 0.77463498711586, Final Batch Loss: 0.39812493324279785\n",
      "Epoch 730, Loss: 0.6900707185268402, Final Batch Loss: 0.33833834528923035\n",
      "Epoch 731, Loss: 0.7163902819156647, Final Batch Loss: 0.36531567573547363\n",
      "Epoch 732, Loss: 0.7498772740364075, Final Batch Loss: 0.4086460769176483\n",
      "Epoch 733, Loss: 0.7637238502502441, Final Batch Loss: 0.3715294301509857\n",
      "Epoch 734, Loss: 0.7360664308071136, Final Batch Loss: 0.4086896479129791\n",
      "Epoch 735, Loss: 0.6989753246307373, Final Batch Loss: 0.34774303436279297\n",
      "Epoch 736, Loss: 0.7733364105224609, Final Batch Loss: 0.3967602550983429\n",
      "Epoch 737, Loss: 0.7305599749088287, Final Batch Loss: 0.3835979104042053\n",
      "Epoch 738, Loss: 0.745132327079773, Final Batch Loss: 0.39978596568107605\n",
      "Epoch 739, Loss: 0.6732794642448425, Final Batch Loss: 0.37660568952560425\n",
      "Epoch 740, Loss: 0.7631393373012543, Final Batch Loss: 0.4086822271347046\n",
      "Epoch 741, Loss: 0.7170878946781158, Final Batch Loss: 0.35406431555747986\n",
      "Epoch 742, Loss: 0.7435908317565918, Final Batch Loss: 0.402940034866333\n",
      "Epoch 743, Loss: 0.6993224918842316, Final Batch Loss: 0.34221938252449036\n",
      "Epoch 744, Loss: 0.7300511002540588, Final Batch Loss: 0.3482109308242798\n",
      "Epoch 745, Loss: 0.6530396938323975, Final Batch Loss: 0.29685813188552856\n",
      "Epoch 746, Loss: 0.713595986366272, Final Batch Loss: 0.37308865785598755\n",
      "Epoch 747, Loss: 0.657590389251709, Final Batch Loss: 0.2955953776836395\n",
      "Epoch 748, Loss: 0.6996647119522095, Final Batch Loss: 0.36202412843704224\n",
      "Epoch 749, Loss: 0.7326209545135498, Final Batch Loss: 0.35474029183387756\n",
      "Epoch 750, Loss: 0.7137986719608307, Final Batch Loss: 0.349714457988739\n",
      "Epoch 751, Loss: 0.7295532822608948, Final Batch Loss: 0.423186719417572\n",
      "Epoch 752, Loss: 0.7538082003593445, Final Batch Loss: 0.40676605701446533\n",
      "Epoch 753, Loss: 0.6929151117801666, Final Batch Loss: 0.33497780561447144\n",
      "Epoch 754, Loss: 0.7393814921379089, Final Batch Loss: 0.3929777443408966\n",
      "Epoch 755, Loss: 0.6647409200668335, Final Batch Loss: 0.2893466353416443\n",
      "Epoch 756, Loss: 0.7358739674091339, Final Batch Loss: 0.37452733516693115\n",
      "Epoch 757, Loss: 0.7581076920032501, Final Batch Loss: 0.35977503657341003\n",
      "Epoch 758, Loss: 0.7619518935680389, Final Batch Loss: 0.41934600472450256\n",
      "Epoch 759, Loss: 0.7265692353248596, Final Batch Loss: 0.3660995364189148\n",
      "Epoch 760, Loss: 0.6569347381591797, Final Batch Loss: 0.31443601846694946\n",
      "Epoch 761, Loss: 0.6774592101573944, Final Batch Loss: 0.34786826372146606\n",
      "Epoch 762, Loss: 0.6696662306785583, Final Batch Loss: 0.36370617151260376\n",
      "Epoch 763, Loss: 0.7080104649066925, Final Batch Loss: 0.3201940059661865\n",
      "Epoch 764, Loss: 0.6719181835651398, Final Batch Loss: 0.3285193145275116\n",
      "Epoch 765, Loss: 0.7134069800376892, Final Batch Loss: 0.41155925393104553\n",
      "Epoch 766, Loss: 0.6942545473575592, Final Batch Loss: 0.32728061079978943\n",
      "Epoch 767, Loss: 0.6836694777011871, Final Batch Loss: 0.32764625549316406\n",
      "Epoch 768, Loss: 0.7461376190185547, Final Batch Loss: 0.3661464750766754\n",
      "Epoch 769, Loss: 0.7163532972335815, Final Batch Loss: 0.3552078902721405\n",
      "Epoch 770, Loss: 0.7007753849029541, Final Batch Loss: 0.36103132367134094\n",
      "Epoch 771, Loss: 0.6938667297363281, Final Batch Loss: 0.3197965621948242\n",
      "Epoch 772, Loss: 0.7588812410831451, Final Batch Loss: 0.3451595902442932\n",
      "Epoch 773, Loss: 0.657708615064621, Final Batch Loss: 0.32371756434440613\n",
      "Epoch 774, Loss: 0.6454491913318634, Final Batch Loss: 0.3284252882003784\n",
      "Epoch 775, Loss: 0.6605224907398224, Final Batch Loss: 0.3501124382019043\n",
      "Epoch 776, Loss: 0.6912448406219482, Final Batch Loss: 0.3225996494293213\n",
      "Epoch 777, Loss: 0.6723518073558807, Final Batch Loss: 0.30518078804016113\n",
      "Epoch 778, Loss: 0.7043227553367615, Final Batch Loss: 0.35015538334846497\n",
      "Epoch 779, Loss: 0.7567747235298157, Final Batch Loss: 0.3586392402648926\n",
      "Epoch 780, Loss: 0.7430010437965393, Final Batch Loss: 0.35868537425994873\n",
      "Epoch 781, Loss: 0.7249633073806763, Final Batch Loss: 0.3895006477832794\n",
      "Epoch 782, Loss: 0.6992049515247345, Final Batch Loss: 0.33797338604927063\n",
      "Epoch 783, Loss: 0.6736234724521637, Final Batch Loss: 0.3262213468551636\n",
      "Epoch 784, Loss: 0.6909312605857849, Final Batch Loss: 0.3099508583545685\n",
      "Epoch 785, Loss: 0.6719276905059814, Final Batch Loss: 0.3279978930950165\n",
      "Epoch 786, Loss: 0.7414472699165344, Final Batch Loss: 0.4010004997253418\n",
      "Epoch 787, Loss: 0.7328811585903168, Final Batch Loss: 0.40010353922843933\n",
      "Epoch 788, Loss: 0.6967295706272125, Final Batch Loss: 0.3300536274909973\n",
      "Epoch 789, Loss: 0.7096002697944641, Final Batch Loss: 0.34168264269828796\n",
      "Epoch 790, Loss: 0.6755344867706299, Final Batch Loss: 0.25341370701789856\n",
      "Epoch 791, Loss: 0.8068404197692871, Final Batch Loss: 0.41695496439933777\n",
      "Epoch 792, Loss: 0.7044641375541687, Final Batch Loss: 0.3252605199813843\n",
      "Epoch 793, Loss: 0.7058000862598419, Final Batch Loss: 0.3153965473175049\n",
      "Epoch 794, Loss: 0.690034419298172, Final Batch Loss: 0.36329084634780884\n",
      "Epoch 795, Loss: 0.6613688468933105, Final Batch Loss: 0.2827668786048889\n",
      "Epoch 796, Loss: 0.7057153284549713, Final Batch Loss: 0.39831462502479553\n",
      "Epoch 797, Loss: 0.6833208799362183, Final Batch Loss: 0.35070258378982544\n",
      "Epoch 798, Loss: 0.7074877321720123, Final Batch Loss: 0.31237223744392395\n",
      "Epoch 799, Loss: 0.7100464105606079, Final Batch Loss: 0.38693851232528687\n",
      "Epoch 800, Loss: 0.6876610815525055, Final Batch Loss: 0.35228073596954346\n",
      "Epoch 801, Loss: 0.7103125154972076, Final Batch Loss: 0.3231680393218994\n",
      "Epoch 802, Loss: 0.6478457152843475, Final Batch Loss: 0.3025587201118469\n",
      "Epoch 803, Loss: 0.6677240133285522, Final Batch Loss: 0.295619934797287\n",
      "Epoch 804, Loss: 0.6777368783950806, Final Batch Loss: 0.3469136953353882\n",
      "Epoch 805, Loss: 0.6468273997306824, Final Batch Loss: 0.299997478723526\n",
      "Epoch 806, Loss: 0.695536196231842, Final Batch Loss: 0.32754743099212646\n",
      "Epoch 807, Loss: 0.69821897149086, Final Batch Loss: 0.4171617925167084\n",
      "Epoch 808, Loss: 0.6866166293621063, Final Batch Loss: 0.3949403464794159\n",
      "Epoch 809, Loss: 0.7183641791343689, Final Batch Loss: 0.39742136001586914\n",
      "Epoch 810, Loss: 0.6981057524681091, Final Batch Loss: 0.3431774973869324\n",
      "Epoch 811, Loss: 0.6897042095661163, Final Batch Loss: 0.3307127058506012\n",
      "Epoch 812, Loss: 0.6645738184452057, Final Batch Loss: 0.3348277807235718\n",
      "Epoch 813, Loss: 0.6576572358608246, Final Batch Loss: 0.3320096731185913\n",
      "Epoch 814, Loss: 0.7755485773086548, Final Batch Loss: 0.44636070728302\n",
      "Epoch 815, Loss: 0.707379549741745, Final Batch Loss: 0.3668201267719269\n",
      "Epoch 816, Loss: 0.7272728681564331, Final Batch Loss: 0.40054839849472046\n",
      "Epoch 817, Loss: 0.6593340337276459, Final Batch Loss: 0.34261786937713623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 818, Loss: 0.6266365945339203, Final Batch Loss: 0.29303452372550964\n",
      "Epoch 819, Loss: 0.6905685365200043, Final Batch Loss: 0.385069876909256\n",
      "Epoch 820, Loss: 0.6983789801597595, Final Batch Loss: 0.33080992102622986\n",
      "Epoch 821, Loss: 0.6684352159500122, Final Batch Loss: 0.3196975886821747\n",
      "Epoch 822, Loss: 0.7068727612495422, Final Batch Loss: 0.29102829098701477\n",
      "Epoch 823, Loss: 0.6946342289447784, Final Batch Loss: 0.3427988886833191\n",
      "Epoch 824, Loss: 0.7215750813484192, Final Batch Loss: 0.3946335017681122\n",
      "Epoch 825, Loss: 0.7835502624511719, Final Batch Loss: 0.38271376490592957\n",
      "Epoch 826, Loss: 0.70679971575737, Final Batch Loss: 0.3349730670452118\n",
      "Epoch 827, Loss: 0.6849078834056854, Final Batch Loss: 0.32551971077919006\n",
      "Epoch 828, Loss: 0.7341454923152924, Final Batch Loss: 0.38050076365470886\n",
      "Epoch 829, Loss: 0.7152083516120911, Final Batch Loss: 0.3842468857765198\n",
      "Epoch 830, Loss: 0.7201827466487885, Final Batch Loss: 0.3677566349506378\n",
      "Epoch 831, Loss: 0.6717739999294281, Final Batch Loss: 0.2965357303619385\n",
      "Epoch 832, Loss: 0.7954121828079224, Final Batch Loss: 0.45049309730529785\n",
      "Epoch 833, Loss: 0.707395613193512, Final Batch Loss: 0.3719926178455353\n",
      "Epoch 834, Loss: 0.6450603604316711, Final Batch Loss: 0.33159977197647095\n",
      "Epoch 835, Loss: 0.6350395381450653, Final Batch Loss: 0.29308968782424927\n",
      "Epoch 836, Loss: 0.6728427410125732, Final Batch Loss: 0.28637242317199707\n",
      "Epoch 837, Loss: 0.7579418420791626, Final Batch Loss: 0.39240121841430664\n",
      "Epoch 838, Loss: 0.7063464224338531, Final Batch Loss: 0.3443632125854492\n",
      "Epoch 839, Loss: 0.6443380117416382, Final Batch Loss: 0.3135303556919098\n",
      "Epoch 840, Loss: 0.7048361301422119, Final Batch Loss: 0.3859613537788391\n",
      "Epoch 841, Loss: 0.633745014667511, Final Batch Loss: 0.2715624272823334\n",
      "Epoch 842, Loss: 0.6790516078472137, Final Batch Loss: 0.33831995725631714\n",
      "Epoch 843, Loss: 0.6969299018383026, Final Batch Loss: 0.3487725853919983\n",
      "Epoch 844, Loss: 0.6817202568054199, Final Batch Loss: 0.3822019696235657\n",
      "Epoch 845, Loss: 0.6707203090190887, Final Batch Loss: 0.3016339838504791\n",
      "Epoch 846, Loss: 0.6586963832378387, Final Batch Loss: 0.3096580505371094\n",
      "Epoch 847, Loss: 0.6921206116676331, Final Batch Loss: 0.3416731357574463\n",
      "Epoch 848, Loss: 0.7439508438110352, Final Batch Loss: 0.42915910482406616\n",
      "Epoch 849, Loss: 0.7024625837802887, Final Batch Loss: 0.32588836550712585\n",
      "Epoch 850, Loss: 0.6928743124008179, Final Batch Loss: 0.3492142856121063\n",
      "Epoch 851, Loss: 0.6971206068992615, Final Batch Loss: 0.34450820088386536\n",
      "Epoch 852, Loss: 0.8288840651512146, Final Batch Loss: 0.4968521296977997\n",
      "Epoch 853, Loss: 0.7503493130207062, Final Batch Loss: 0.38713639974594116\n",
      "Epoch 854, Loss: 0.6501517593860626, Final Batch Loss: 0.315603643655777\n",
      "Epoch 855, Loss: 0.7137853801250458, Final Batch Loss: 0.36711999773979187\n",
      "Epoch 856, Loss: 0.6900359094142914, Final Batch Loss: 0.37313300371170044\n",
      "Epoch 857, Loss: 0.7162296772003174, Final Batch Loss: 0.3344719707965851\n",
      "Epoch 858, Loss: 0.7189492881298065, Final Batch Loss: 0.38715070486068726\n",
      "Epoch 859, Loss: 0.7044688165187836, Final Batch Loss: 0.3868905007839203\n",
      "Epoch 860, Loss: 0.6760306656360626, Final Batch Loss: 0.3471534848213196\n",
      "Epoch 861, Loss: 0.6575609147548676, Final Batch Loss: 0.3102245032787323\n",
      "Epoch 862, Loss: 0.668895035982132, Final Batch Loss: 0.3349827527999878\n",
      "Epoch 863, Loss: 0.6763982474803925, Final Batch Loss: 0.31009259819984436\n",
      "Epoch 864, Loss: 0.6787629127502441, Final Batch Loss: 0.3350421190261841\n",
      "Epoch 865, Loss: 0.7137043178081512, Final Batch Loss: 0.32779356837272644\n",
      "Epoch 866, Loss: 0.7161099314689636, Final Batch Loss: 0.35816970467567444\n",
      "Epoch 867, Loss: 0.6801818311214447, Final Batch Loss: 0.33654358983039856\n",
      "Epoch 868, Loss: 0.7182496786117554, Final Batch Loss: 0.3315727412700653\n",
      "Epoch 869, Loss: 0.6872342228889465, Final Batch Loss: 0.3299039900302887\n",
      "Epoch 870, Loss: 0.6482677757740021, Final Batch Loss: 0.2762630879878998\n",
      "Epoch 871, Loss: 0.6853657364845276, Final Batch Loss: 0.37347108125686646\n",
      "Epoch 872, Loss: 0.7208357453346252, Final Batch Loss: 0.35511845350265503\n",
      "Epoch 873, Loss: 0.6633915603160858, Final Batch Loss: 0.3402843177318573\n",
      "Epoch 874, Loss: 0.6855978071689606, Final Batch Loss: 0.32252174615859985\n",
      "Epoch 875, Loss: 0.6612871289253235, Final Batch Loss: 0.28137949109077454\n",
      "Epoch 876, Loss: 0.6381984651088715, Final Batch Loss: 0.30219927430152893\n",
      "Epoch 877, Loss: 0.6490653455257416, Final Batch Loss: 0.30131933093070984\n",
      "Epoch 878, Loss: 0.6723143756389618, Final Batch Loss: 0.38463038206100464\n",
      "Epoch 879, Loss: 0.6290405690670013, Final Batch Loss: 0.2935298979282379\n",
      "Epoch 880, Loss: 0.6297773122787476, Final Batch Loss: 0.2864931523799896\n",
      "Epoch 881, Loss: 0.6895800530910492, Final Batch Loss: 0.3110608160495758\n",
      "Epoch 882, Loss: 0.71865114569664, Final Batch Loss: 0.37769848108291626\n",
      "Epoch 883, Loss: 0.6714127659797668, Final Batch Loss: 0.32673177123069763\n",
      "Epoch 884, Loss: 0.6631610691547394, Final Batch Loss: 0.34546345472335815\n",
      "Epoch 885, Loss: 0.6539797186851501, Final Batch Loss: 0.317727267742157\n",
      "Epoch 886, Loss: 0.6480060517787933, Final Batch Loss: 0.2888888120651245\n",
      "Epoch 887, Loss: 0.6278957426548004, Final Batch Loss: 0.28392091393470764\n",
      "Epoch 888, Loss: 0.7095699906349182, Final Batch Loss: 0.40286704897880554\n",
      "Epoch 889, Loss: 0.6776101291179657, Final Batch Loss: 0.35612577199935913\n",
      "Epoch 890, Loss: 0.650188684463501, Final Batch Loss: 0.32675862312316895\n",
      "Epoch 891, Loss: 0.77824667096138, Final Batch Loss: 0.48723945021629333\n",
      "Epoch 892, Loss: 0.6170145273208618, Final Batch Loss: 0.3221108019351959\n",
      "Epoch 893, Loss: 0.6582587659358978, Final Batch Loss: 0.3222672939300537\n",
      "Epoch 894, Loss: 0.6066106855869293, Final Batch Loss: 0.3058784604072571\n",
      "Epoch 895, Loss: 0.6304259300231934, Final Batch Loss: 0.31176793575286865\n",
      "Epoch 896, Loss: 0.6335710287094116, Final Batch Loss: 0.27866873145103455\n",
      "Epoch 897, Loss: 0.6266313195228577, Final Batch Loss: 0.30595171451568604\n",
      "Epoch 898, Loss: 0.6724453270435333, Final Batch Loss: 0.3128969669342041\n",
      "Epoch 899, Loss: 0.6333563327789307, Final Batch Loss: 0.2866917848587036\n",
      "Epoch 900, Loss: 0.6511830985546112, Final Batch Loss: 0.32363462448120117\n",
      "Epoch 901, Loss: 0.6708455979824066, Final Batch Loss: 0.3523784577846527\n",
      "Epoch 902, Loss: 0.8350112736225128, Final Batch Loss: 0.4705975353717804\n",
      "Epoch 903, Loss: 0.7051581740379333, Final Batch Loss: 0.36794760823249817\n",
      "Epoch 904, Loss: 0.6386395394802094, Final Batch Loss: 0.33145871758461\n",
      "Epoch 905, Loss: 0.7344194948673248, Final Batch Loss: 0.37158724665641785\n",
      "Epoch 906, Loss: 0.7317571640014648, Final Batch Loss: 0.3884445130825043\n",
      "Epoch 907, Loss: 0.7233633995056152, Final Batch Loss: 0.37519562244415283\n",
      "Epoch 908, Loss: 0.7367230951786041, Final Batch Loss: 0.3729036748409271\n",
      "Epoch 909, Loss: 0.6390165090560913, Final Batch Loss: 0.32346194982528687\n",
      "Epoch 910, Loss: 0.6832828223705292, Final Batch Loss: 0.34615185856819153\n",
      "Epoch 911, Loss: 0.6499637961387634, Final Batch Loss: 0.3019029200077057\n",
      "Epoch 912, Loss: 0.6712444126605988, Final Batch Loss: 0.3381528854370117\n",
      "Epoch 913, Loss: 0.6993127763271332, Final Batch Loss: 0.36587974429130554\n",
      "Epoch 914, Loss: 0.6408677399158478, Final Batch Loss: 0.30798888206481934\n",
      "Epoch 915, Loss: 0.6636706292629242, Final Batch Loss: 0.28507524728775024\n",
      "Epoch 916, Loss: 0.6406826972961426, Final Batch Loss: 0.30240580439567566\n",
      "Epoch 917, Loss: 0.6566295921802521, Final Batch Loss: 0.31676656007766724\n",
      "Epoch 918, Loss: 0.6903355717658997, Final Batch Loss: 0.36992692947387695\n",
      "Epoch 919, Loss: 0.6581419706344604, Final Batch Loss: 0.3533580005168915\n",
      "Epoch 920, Loss: 0.6816620826721191, Final Batch Loss: 0.324433296918869\n",
      "Epoch 921, Loss: 0.6735579073429108, Final Batch Loss: 0.343929648399353\n",
      "Epoch 922, Loss: 0.6728954017162323, Final Batch Loss: 0.32376062870025635\n",
      "Epoch 923, Loss: 0.667791336774826, Final Batch Loss: 0.33702728152275085\n",
      "Epoch 924, Loss: 0.6688762009143829, Final Batch Loss: 0.2801525592803955\n",
      "Epoch 925, Loss: 0.6226898729801178, Final Batch Loss: 0.2943436801433563\n",
      "Epoch 926, Loss: 0.7081415951251984, Final Batch Loss: 0.39456844329833984\n",
      "Epoch 927, Loss: 0.6793636977672577, Final Batch Loss: 0.37465211749076843\n",
      "Epoch 928, Loss: 0.7662352621555328, Final Batch Loss: 0.4793359339237213\n",
      "Epoch 929, Loss: 0.6860218644142151, Final Batch Loss: 0.3379099369049072\n",
      "Epoch 930, Loss: 0.7169957458972931, Final Batch Loss: 0.4138844609260559\n",
      "Epoch 931, Loss: 0.7320694327354431, Final Batch Loss: 0.40332868695259094\n",
      "Epoch 932, Loss: 0.6349274814128876, Final Batch Loss: 0.2623618543148041\n",
      "Epoch 933, Loss: 0.7278026044368744, Final Batch Loss: 0.36315685510635376\n",
      "Epoch 934, Loss: 0.712588369846344, Final Batch Loss: 0.3169107735157013\n",
      "Epoch 935, Loss: 0.7302488684654236, Final Batch Loss: 0.3848033845424652\n",
      "Epoch 936, Loss: 0.7318853735923767, Final Batch Loss: 0.3652804493904114\n",
      "Epoch 937, Loss: 0.721100926399231, Final Batch Loss: 0.3813197910785675\n",
      "Epoch 938, Loss: 0.6986672282218933, Final Batch Loss: 0.3683512806892395\n",
      "Epoch 939, Loss: 0.6614207327365875, Final Batch Loss: 0.3102900981903076\n",
      "Epoch 940, Loss: 0.7124304473400116, Final Batch Loss: 0.4050859808921814\n",
      "Epoch 941, Loss: 0.7010245025157928, Final Batch Loss: 0.29187023639678955\n",
      "Epoch 942, Loss: 0.6651839911937714, Final Batch Loss: 0.32027021050453186\n",
      "Epoch 943, Loss: 0.619064450263977, Final Batch Loss: 0.28525668382644653\n",
      "Epoch 944, Loss: 0.7862437963485718, Final Batch Loss: 0.4494532644748688\n",
      "Epoch 945, Loss: 0.6443423926830292, Final Batch Loss: 0.3121764361858368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 946, Loss: 0.6775752305984497, Final Batch Loss: 0.32060426473617554\n",
      "Epoch 947, Loss: 0.6924028694629669, Final Batch Loss: 0.3839365243911743\n",
      "Epoch 948, Loss: 0.6306663751602173, Final Batch Loss: 0.31751173734664917\n",
      "Epoch 949, Loss: 0.6426114439964294, Final Batch Loss: 0.32999181747436523\n",
      "Epoch 950, Loss: 0.6723125576972961, Final Batch Loss: 0.35480180382728577\n",
      "Epoch 951, Loss: 0.6582023203372955, Final Batch Loss: 0.36010193824768066\n",
      "Epoch 952, Loss: 0.6231851279735565, Final Batch Loss: 0.27999407052993774\n",
      "Epoch 953, Loss: 0.6963502168655396, Final Batch Loss: 0.35578951239585876\n",
      "Epoch 954, Loss: 0.7052066326141357, Final Batch Loss: 0.407478392124176\n",
      "Epoch 955, Loss: 0.6562367081642151, Final Batch Loss: 0.31573763489723206\n",
      "Epoch 956, Loss: 0.6698783040046692, Final Batch Loss: 0.3460857570171356\n",
      "Epoch 957, Loss: 0.6788318753242493, Final Batch Loss: 0.3660314083099365\n",
      "Epoch 958, Loss: 0.6451083421707153, Final Batch Loss: 0.3169465959072113\n",
      "Epoch 959, Loss: 0.654466837644577, Final Batch Loss: 0.27787545323371887\n",
      "Epoch 960, Loss: 0.6701540350914001, Final Batch Loss: 0.35248157382011414\n",
      "Epoch 961, Loss: 0.7754956781864166, Final Batch Loss: 0.44155487418174744\n",
      "Epoch 962, Loss: 0.6124874651432037, Final Batch Loss: 0.25514841079711914\n",
      "Epoch 963, Loss: 0.6171576976776123, Final Batch Loss: 0.2712970972061157\n",
      "Epoch 964, Loss: 0.725384920835495, Final Batch Loss: 0.39669954776763916\n",
      "Epoch 965, Loss: 0.6519414782524109, Final Batch Loss: 0.264972448348999\n",
      "Epoch 966, Loss: 0.5949714779853821, Final Batch Loss: 0.2718130350112915\n",
      "Epoch 967, Loss: 0.7547336220741272, Final Batch Loss: 0.4111124277114868\n",
      "Epoch 968, Loss: 0.6586589217185974, Final Batch Loss: 0.366717129945755\n",
      "Epoch 969, Loss: 0.6814927160739899, Final Batch Loss: 0.3775288164615631\n",
      "Epoch 970, Loss: 0.6627279818058014, Final Batch Loss: 0.34073904156684875\n",
      "Epoch 971, Loss: 0.6475490629673004, Final Batch Loss: 0.3105930685997009\n",
      "Epoch 972, Loss: 0.6325477957725525, Final Batch Loss: 0.3321061134338379\n",
      "Epoch 973, Loss: 0.6560057103633881, Final Batch Loss: 0.30774205923080444\n",
      "Epoch 974, Loss: 0.6232452690601349, Final Batch Loss: 0.3286733329296112\n",
      "Epoch 975, Loss: 0.6332695484161377, Final Batch Loss: 0.3548893928527832\n",
      "Epoch 976, Loss: 0.6613024771213531, Final Batch Loss: 0.3301875591278076\n",
      "Epoch 977, Loss: 0.652768075466156, Final Batch Loss: 0.3147069811820984\n",
      "Epoch 978, Loss: 0.6662710309028625, Final Batch Loss: 0.3411634862422943\n",
      "Epoch 979, Loss: 0.655783474445343, Final Batch Loss: 0.36307913064956665\n",
      "Epoch 980, Loss: 0.7545310258865356, Final Batch Loss: 0.41721111536026\n",
      "Epoch 981, Loss: 0.6113499402999878, Final Batch Loss: 0.2918510437011719\n",
      "Epoch 982, Loss: 0.7018282413482666, Final Batch Loss: 0.4075217843055725\n",
      "Epoch 983, Loss: 0.6490243673324585, Final Batch Loss: 0.3031185269355774\n",
      "Epoch 984, Loss: 0.6280244588851929, Final Batch Loss: 0.30894574522972107\n",
      "Epoch 985, Loss: 0.5897201597690582, Final Batch Loss: 0.29808440804481506\n",
      "Epoch 986, Loss: 0.652334064245224, Final Batch Loss: 0.33583348989486694\n",
      "Epoch 987, Loss: 0.774875670671463, Final Batch Loss: 0.4389556348323822\n",
      "Epoch 988, Loss: 0.7408261001110077, Final Batch Loss: 0.4004833996295929\n",
      "Epoch 989, Loss: 0.6407634913921356, Final Batch Loss: 0.3186478614807129\n",
      "Epoch 990, Loss: 0.696426510810852, Final Batch Loss: 0.36874493956565857\n",
      "Epoch 991, Loss: 0.6467398703098297, Final Batch Loss: 0.3290769159793854\n",
      "Epoch 992, Loss: 0.6443360447883606, Final Batch Loss: 0.3699389696121216\n",
      "Epoch 993, Loss: 0.6414287686347961, Final Batch Loss: 0.2873351275920868\n",
      "Epoch 994, Loss: 0.6660754680633545, Final Batch Loss: 0.3691590130329132\n",
      "Epoch 995, Loss: 0.6876333057880402, Final Batch Loss: 0.3458491265773773\n",
      "Epoch 996, Loss: 0.6165145933628082, Final Batch Loss: 0.27830758690834045\n",
      "Epoch 997, Loss: 0.702356606721878, Final Batch Loss: 0.3683492839336395\n",
      "Epoch 998, Loss: 0.6256829798221588, Final Batch Loss: 0.30784380435943604\n",
      "Epoch 999, Loss: 0.6681943833827972, Final Batch Loss: 0.32934147119522095\n",
      "Epoch 1000, Loss: 0.6912625730037689, Final Batch Loss: 0.3696056008338928\n",
      "Epoch 1001, Loss: 0.6731279492378235, Final Batch Loss: 0.3693119287490845\n",
      "Epoch 1002, Loss: 0.684129923582077, Final Batch Loss: 0.3219236433506012\n",
      "Epoch 1003, Loss: 0.7050281465053558, Final Batch Loss: 0.3176850974559784\n",
      "Epoch 1004, Loss: 0.7422947883605957, Final Batch Loss: 0.3796473443508148\n",
      "Epoch 1005, Loss: 0.6504124701023102, Final Batch Loss: 0.30527517199516296\n",
      "Epoch 1006, Loss: 0.5671285837888718, Final Batch Loss: 0.2377026528120041\n",
      "Epoch 1007, Loss: 0.6761960983276367, Final Batch Loss: 0.36980730295181274\n",
      "Epoch 1008, Loss: 0.6956189572811127, Final Batch Loss: 0.38050222396850586\n",
      "Epoch 1009, Loss: 0.6473637819290161, Final Batch Loss: 0.28959909081459045\n",
      "Epoch 1010, Loss: 0.6179315447807312, Final Batch Loss: 0.3326061964035034\n",
      "Epoch 1011, Loss: 0.6890142858028412, Final Batch Loss: 0.38477542996406555\n",
      "Epoch 1012, Loss: 0.6898389458656311, Final Batch Loss: 0.38153916597366333\n",
      "Epoch 1013, Loss: 0.7191691100597382, Final Batch Loss: 0.36882737278938293\n",
      "Epoch 1014, Loss: 0.6674591898918152, Final Batch Loss: 0.368581622838974\n",
      "Epoch 1015, Loss: 0.6684154868125916, Final Batch Loss: 0.31919047236442566\n",
      "Epoch 1016, Loss: 0.6521187424659729, Final Batch Loss: 0.34065553545951843\n",
      "Epoch 1017, Loss: 0.6004143357276917, Final Batch Loss: 0.2966109812259674\n",
      "Epoch 1018, Loss: 0.6333666741847992, Final Batch Loss: 0.3026905655860901\n",
      "Epoch 1019, Loss: 0.6272487640380859, Final Batch Loss: 0.322551965713501\n",
      "Epoch 1020, Loss: 0.6737593412399292, Final Batch Loss: 0.3359954059123993\n",
      "Epoch 1021, Loss: 0.6099656820297241, Final Batch Loss: 0.30242404341697693\n",
      "Epoch 1022, Loss: 0.6065906286239624, Final Batch Loss: 0.27008339762687683\n",
      "Epoch 1023, Loss: 0.6479888558387756, Final Batch Loss: 0.2881103456020355\n",
      "Epoch 1024, Loss: 0.6425378024578094, Final Batch Loss: 0.31539469957351685\n",
      "Epoch 1025, Loss: 0.5911865830421448, Final Batch Loss: 0.29146477580070496\n",
      "Epoch 1026, Loss: 0.7071385383605957, Final Batch Loss: 0.34937599301338196\n",
      "Epoch 1027, Loss: 0.6900924444198608, Final Batch Loss: 0.35334229469299316\n",
      "Epoch 1028, Loss: 0.6102170944213867, Final Batch Loss: 0.270367294549942\n",
      "Epoch 1029, Loss: 0.6550886631011963, Final Batch Loss: 0.34313514828681946\n",
      "Epoch 1030, Loss: 0.5930991470813751, Final Batch Loss: 0.2722903788089752\n",
      "Epoch 1031, Loss: 0.6513331830501556, Final Batch Loss: 0.3475601375102997\n",
      "Epoch 1032, Loss: 0.6939211487770081, Final Batch Loss: 0.3599567115306854\n",
      "Epoch 1033, Loss: 0.6953369081020355, Final Batch Loss: 0.34289059042930603\n",
      "Epoch 1034, Loss: 0.6246033608913422, Final Batch Loss: 0.3271011412143707\n",
      "Epoch 1035, Loss: 0.6535861194133759, Final Batch Loss: 0.3059122860431671\n",
      "Epoch 1036, Loss: 0.6182484030723572, Final Batch Loss: 0.2800169587135315\n",
      "Epoch 1037, Loss: 0.6418020129203796, Final Batch Loss: 0.38181278109550476\n",
      "Epoch 1038, Loss: 0.6805084645748138, Final Batch Loss: 0.3560321033000946\n",
      "Epoch 1039, Loss: 0.6576264202594757, Final Batch Loss: 0.3323533236980438\n",
      "Epoch 1040, Loss: 0.6976060271263123, Final Batch Loss: 0.3545733690261841\n",
      "Epoch 1041, Loss: 0.6605595052242279, Final Batch Loss: 0.33671072125434875\n",
      "Epoch 1042, Loss: 0.6209047138690948, Final Batch Loss: 0.28318771719932556\n",
      "Epoch 1043, Loss: 0.6907398700714111, Final Batch Loss: 0.3537323772907257\n",
      "Epoch 1044, Loss: 0.6195350885391235, Final Batch Loss: 0.27360084652900696\n",
      "Epoch 1045, Loss: 0.6555491983890533, Final Batch Loss: 0.31464964151382446\n",
      "Epoch 1046, Loss: 0.6578143537044525, Final Batch Loss: 0.27889901399612427\n",
      "Epoch 1047, Loss: 0.6483736038208008, Final Batch Loss: 0.31432726979255676\n",
      "Epoch 1048, Loss: 0.6761609315872192, Final Batch Loss: 0.36729171872138977\n",
      "Epoch 1049, Loss: 0.6767171025276184, Final Batch Loss: 0.3481687009334564\n",
      "Epoch 1050, Loss: 0.6729389727115631, Final Batch Loss: 0.34848466515541077\n",
      "Epoch 1051, Loss: 0.6651938259601593, Final Batch Loss: 0.322000652551651\n",
      "Epoch 1052, Loss: 0.6189824640750885, Final Batch Loss: 0.3244006335735321\n",
      "Epoch 1053, Loss: 0.6365963220596313, Final Batch Loss: 0.28842100501060486\n",
      "Epoch 1054, Loss: 0.6336172223091125, Final Batch Loss: 0.3203657269477844\n",
      "Epoch 1055, Loss: 0.5885430872440338, Final Batch Loss: 0.31257355213165283\n",
      "Epoch 1056, Loss: 0.6977783143520355, Final Batch Loss: 0.36010944843292236\n",
      "Epoch 1057, Loss: 0.619341641664505, Final Batch Loss: 0.3004549443721771\n",
      "Epoch 1058, Loss: 0.6340815722942352, Final Batch Loss: 0.36304786801338196\n",
      "Epoch 1059, Loss: 0.6616717576980591, Final Batch Loss: 0.3490268290042877\n",
      "Epoch 1060, Loss: 0.6146517693996429, Final Batch Loss: 0.29161033034324646\n",
      "Epoch 1061, Loss: 0.6790414154529572, Final Batch Loss: 0.3608008027076721\n",
      "Epoch 1062, Loss: 0.6386538445949554, Final Batch Loss: 0.34308379888534546\n",
      "Epoch 1063, Loss: 0.6632204949855804, Final Batch Loss: 0.35987383127212524\n",
      "Epoch 1064, Loss: 0.7370381355285645, Final Batch Loss: 0.4142218828201294\n",
      "Epoch 1065, Loss: 0.5933986902236938, Final Batch Loss: 0.2491702437400818\n",
      "Epoch 1066, Loss: 0.633891761302948, Final Batch Loss: 0.27763041853904724\n",
      "Epoch 1067, Loss: 0.5819604992866516, Final Batch Loss: 0.26947054266929626\n",
      "Epoch 1068, Loss: 0.6142051517963409, Final Batch Loss: 0.3105771541595459\n",
      "Epoch 1069, Loss: 0.5927548408508301, Final Batch Loss: 0.278230756521225\n",
      "Epoch 1070, Loss: 0.6537367403507233, Final Batch Loss: 0.3387991786003113\n",
      "Epoch 1071, Loss: 0.6597040593624115, Final Batch Loss: 0.3018381595611572\n",
      "Epoch 1072, Loss: 0.6051879823207855, Final Batch Loss: 0.2902676463127136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1073, Loss: 0.7171725332736969, Final Batch Loss: 0.35984060168266296\n",
      "Epoch 1074, Loss: 0.6091141104698181, Final Batch Loss: 0.3089253008365631\n",
      "Epoch 1075, Loss: 0.6570810079574585, Final Batch Loss: 0.3077525496482849\n",
      "Epoch 1076, Loss: 0.6954168677330017, Final Batch Loss: 0.35334548354148865\n",
      "Epoch 1077, Loss: 0.6495550572872162, Final Batch Loss: 0.379940390586853\n",
      "Epoch 1078, Loss: 0.6618276238441467, Final Batch Loss: 0.34673383831977844\n",
      "Epoch 1079, Loss: 0.6869058310985565, Final Batch Loss: 0.3616822063922882\n",
      "Epoch 1080, Loss: 0.6544753611087799, Final Batch Loss: 0.30082133412361145\n",
      "Epoch 1081, Loss: 0.6303110420703888, Final Batch Loss: 0.2856333553791046\n",
      "Epoch 1082, Loss: 0.6427687704563141, Final Batch Loss: 0.33903762698173523\n",
      "Epoch 1083, Loss: 0.593149721622467, Final Batch Loss: 0.2897409200668335\n",
      "Epoch 1084, Loss: 0.6309480667114258, Final Batch Loss: 0.3123461902141571\n",
      "Epoch 1085, Loss: 0.6767080128192902, Final Batch Loss: 0.3619094789028168\n",
      "Epoch 1086, Loss: 0.6627353727817535, Final Batch Loss: 0.3478325605392456\n",
      "Epoch 1087, Loss: 0.6086668372154236, Final Batch Loss: 0.31741949915885925\n",
      "Epoch 1088, Loss: 0.5970119535923004, Final Batch Loss: 0.2809620201587677\n",
      "Epoch 1089, Loss: 0.6147380173206329, Final Batch Loss: 0.3122883141040802\n",
      "Epoch 1090, Loss: 0.6819447576999664, Final Batch Loss: 0.34231218695640564\n",
      "Epoch 1091, Loss: 0.6286025941371918, Final Batch Loss: 0.2881641983985901\n",
      "Epoch 1092, Loss: 0.6879321336746216, Final Batch Loss: 0.3768934905529022\n",
      "Epoch 1093, Loss: 0.6332288682460785, Final Batch Loss: 0.3657112121582031\n",
      "Epoch 1094, Loss: 0.6648011803627014, Final Batch Loss: 0.3533672094345093\n",
      "Epoch 1095, Loss: 0.5917655825614929, Final Batch Loss: 0.3029942512512207\n",
      "Epoch 1096, Loss: 0.6376402974128723, Final Batch Loss: 0.3307364881038666\n",
      "Epoch 1097, Loss: 0.6880704164505005, Final Batch Loss: 0.38709768652915955\n",
      "Epoch 1098, Loss: 0.5967294871807098, Final Batch Loss: 0.2799839973449707\n",
      "Epoch 1099, Loss: 0.6325966715812683, Final Batch Loss: 0.2733152210712433\n",
      "Epoch 1100, Loss: 0.6145758032798767, Final Batch Loss: 0.27867212891578674\n",
      "Epoch 1101, Loss: 0.6102664470672607, Final Batch Loss: 0.29322609305381775\n",
      "Epoch 1102, Loss: 0.6367519497871399, Final Batch Loss: 0.3387029469013214\n",
      "Epoch 1103, Loss: 0.5716255903244019, Final Batch Loss: 0.2669247090816498\n",
      "Epoch 1104, Loss: 0.6938612461090088, Final Batch Loss: 0.37365004420280457\n",
      "Epoch 1105, Loss: 0.6373942494392395, Final Batch Loss: 0.3490707576274872\n",
      "Epoch 1106, Loss: 0.6025469899177551, Final Batch Loss: 0.29090285301208496\n",
      "Epoch 1107, Loss: 0.6042433977127075, Final Batch Loss: 0.29800504446029663\n",
      "Epoch 1108, Loss: 0.6515049040317535, Final Batch Loss: 0.3299047648906708\n",
      "Epoch 1109, Loss: 0.6222609281539917, Final Batch Loss: 0.3061133325099945\n",
      "Epoch 1110, Loss: 0.6327355802059174, Final Batch Loss: 0.34578752517700195\n",
      "Epoch 1111, Loss: 0.6439405381679535, Final Batch Loss: 0.34358271956443787\n",
      "Epoch 1112, Loss: 0.5609671026468277, Final Batch Loss: 0.2192930430173874\n",
      "Epoch 1113, Loss: 0.6557077169418335, Final Batch Loss: 0.35415300726890564\n",
      "Epoch 1114, Loss: 0.6497463583946228, Final Batch Loss: 0.3334046006202698\n",
      "Epoch 1115, Loss: 0.5992515087127686, Final Batch Loss: 0.2861559987068176\n",
      "Epoch 1116, Loss: 0.6976945698261261, Final Batch Loss: 0.3994475305080414\n",
      "Epoch 1117, Loss: 0.6115389764308929, Final Batch Loss: 0.2726721167564392\n",
      "Epoch 1118, Loss: 0.6253362596035004, Final Batch Loss: 0.3177519738674164\n",
      "Epoch 1119, Loss: 0.5844250321388245, Final Batch Loss: 0.2539723217487335\n",
      "Epoch 1120, Loss: 0.6309646666049957, Final Batch Loss: 0.3212750256061554\n",
      "Epoch 1121, Loss: 0.6471035778522491, Final Batch Loss: 0.3305974006652832\n",
      "Epoch 1122, Loss: 0.6568714082241058, Final Batch Loss: 0.3359562158584595\n",
      "Epoch 1123, Loss: 0.6018803417682648, Final Batch Loss: 0.2919057011604309\n",
      "Epoch 1124, Loss: 0.677817702293396, Final Batch Loss: 0.37401989102363586\n",
      "Epoch 1125, Loss: 0.5714839994907379, Final Batch Loss: 0.2596758306026459\n",
      "Epoch 1126, Loss: 0.6521508097648621, Final Batch Loss: 0.3277996778488159\n",
      "Epoch 1127, Loss: 0.5687884986400604, Final Batch Loss: 0.26401594281196594\n",
      "Epoch 1128, Loss: 0.6124888956546783, Final Batch Loss: 0.35369741916656494\n",
      "Epoch 1129, Loss: 0.5728847980499268, Final Batch Loss: 0.3063535690307617\n",
      "Epoch 1130, Loss: 0.577053040266037, Final Batch Loss: 0.3056004047393799\n",
      "Epoch 1131, Loss: 0.6109226644039154, Final Batch Loss: 0.25018221139907837\n",
      "Epoch 1132, Loss: 0.589649498462677, Final Batch Loss: 0.30342960357666016\n",
      "Epoch 1133, Loss: 0.6097800433635712, Final Batch Loss: 0.31617313623428345\n",
      "Epoch 1134, Loss: 0.59235018491745, Final Batch Loss: 0.2959233820438385\n",
      "Epoch 1135, Loss: 0.6251299679279327, Final Batch Loss: 0.32068827748298645\n",
      "Epoch 1136, Loss: 0.6294163465499878, Final Batch Loss: 0.31581345200538635\n",
      "Epoch 1137, Loss: 0.6054533421993256, Final Batch Loss: 0.2817002534866333\n",
      "Epoch 1138, Loss: 0.5901777148246765, Final Batch Loss: 0.3047729730606079\n",
      "Epoch 1139, Loss: 0.6306229829788208, Final Batch Loss: 0.32316407561302185\n",
      "Epoch 1140, Loss: 0.6150223612785339, Final Batch Loss: 0.29350510239601135\n",
      "Epoch 1141, Loss: 0.6251784265041351, Final Batch Loss: 0.29031917452812195\n",
      "Epoch 1142, Loss: 0.6443334817886353, Final Batch Loss: 0.31814688444137573\n",
      "Epoch 1143, Loss: 0.5725408792495728, Final Batch Loss: 0.2896241843700409\n",
      "Epoch 1144, Loss: 0.5716677606105804, Final Batch Loss: 0.24201557040214539\n",
      "Epoch 1145, Loss: 0.609021782875061, Final Batch Loss: 0.316068559885025\n",
      "Epoch 1146, Loss: 0.6119118928909302, Final Batch Loss: 0.34515121579170227\n",
      "Epoch 1147, Loss: 0.611181378364563, Final Batch Loss: 0.3072333335876465\n",
      "Epoch 1148, Loss: 0.6188103258609772, Final Batch Loss: 0.3232528865337372\n",
      "Epoch 1149, Loss: 0.6012986898422241, Final Batch Loss: 0.2859867215156555\n",
      "Epoch 1150, Loss: 0.589597076177597, Final Batch Loss: 0.33169788122177124\n",
      "Epoch 1151, Loss: 0.5715756714344025, Final Batch Loss: 0.3022439777851105\n",
      "Epoch 1152, Loss: 0.6643640398979187, Final Batch Loss: 0.3381664752960205\n",
      "Epoch 1153, Loss: 0.671898752450943, Final Batch Loss: 0.34315699338912964\n",
      "Epoch 1154, Loss: 0.6453935205936432, Final Batch Loss: 0.39414453506469727\n",
      "Epoch 1155, Loss: 0.5820994079113007, Final Batch Loss: 0.2536682188510895\n",
      "Epoch 1156, Loss: 0.6864085495471954, Final Batch Loss: 0.3499182462692261\n",
      "Epoch 1157, Loss: 0.6863082349300385, Final Batch Loss: 0.3958814740180969\n",
      "Epoch 1158, Loss: 0.6244005858898163, Final Batch Loss: 0.3347174823284149\n",
      "Epoch 1159, Loss: 0.6075846552848816, Final Batch Loss: 0.2896283268928528\n",
      "Epoch 1160, Loss: 0.5951374769210815, Final Batch Loss: 0.24651747941970825\n",
      "Epoch 1161, Loss: 0.6506173610687256, Final Batch Loss: 0.32166045904159546\n",
      "Epoch 1162, Loss: 0.6145790815353394, Final Batch Loss: 0.3472694754600525\n",
      "Epoch 1163, Loss: 0.6489103138446808, Final Batch Loss: 0.3355880081653595\n",
      "Epoch 1164, Loss: 0.5808620452880859, Final Batch Loss: 0.2563443183898926\n",
      "Epoch 1165, Loss: 0.5951237082481384, Final Batch Loss: 0.2934487462043762\n",
      "Epoch 1166, Loss: 0.6104546785354614, Final Batch Loss: 0.3330751657485962\n",
      "Epoch 1167, Loss: 0.6212242245674133, Final Batch Loss: 0.32632753252983093\n",
      "Epoch 1168, Loss: 0.6275598108768463, Final Batch Loss: 0.33302828669548035\n",
      "Epoch 1169, Loss: 0.6622031033039093, Final Batch Loss: 0.3313485383987427\n",
      "Epoch 1170, Loss: 0.5969701409339905, Final Batch Loss: 0.2848833203315735\n",
      "Epoch 1171, Loss: 0.6484579145908356, Final Batch Loss: 0.3382234573364258\n",
      "Epoch 1172, Loss: 0.642239898443222, Final Batch Loss: 0.34377285838127136\n",
      "Epoch 1173, Loss: 0.6597774922847748, Final Batch Loss: 0.25686508417129517\n",
      "Epoch 1174, Loss: 0.6533523797988892, Final Batch Loss: 0.3280262351036072\n",
      "Epoch 1175, Loss: 0.6426057815551758, Final Batch Loss: 0.3207293748855591\n",
      "Epoch 1176, Loss: 0.6097078323364258, Final Batch Loss: 0.31875306367874146\n",
      "Epoch 1177, Loss: 0.6528862416744232, Final Batch Loss: 0.3728299140930176\n",
      "Epoch 1178, Loss: 0.5555708557367325, Final Batch Loss: 0.22641490399837494\n",
      "Epoch 1179, Loss: 0.573752224445343, Final Batch Loss: 0.28652423620224\n",
      "Epoch 1180, Loss: 0.6982041001319885, Final Batch Loss: 0.4213375449180603\n",
      "Epoch 1181, Loss: 0.581823468208313, Final Batch Loss: 0.2602270543575287\n",
      "Epoch 1182, Loss: 0.5866718590259552, Final Batch Loss: 0.25802159309387207\n",
      "Epoch 1183, Loss: 0.6186327338218689, Final Batch Loss: 0.3360874056816101\n",
      "Epoch 1184, Loss: 0.5816473364830017, Final Batch Loss: 0.25474029779434204\n",
      "Epoch 1185, Loss: 0.5478620529174805, Final Batch Loss: 0.2725136876106262\n",
      "Epoch 1186, Loss: 0.6019247472286224, Final Batch Loss: 0.31288960576057434\n",
      "Epoch 1187, Loss: 0.684587687253952, Final Batch Loss: 0.41503557562828064\n",
      "Epoch 1188, Loss: 0.6623950004577637, Final Batch Loss: 0.35142916440963745\n",
      "Epoch 1189, Loss: 0.5968169867992401, Final Batch Loss: 0.2806378901004791\n",
      "Epoch 1190, Loss: 0.609896183013916, Final Batch Loss: 0.30611225962638855\n",
      "Epoch 1191, Loss: 0.591837614774704, Final Batch Loss: 0.28764382004737854\n",
      "Epoch 1192, Loss: 0.4948173761367798, Final Batch Loss: 0.1722736656665802\n",
      "Epoch 1193, Loss: 0.564258873462677, Final Batch Loss: 0.27022379636764526\n",
      "Epoch 1194, Loss: 0.6347077190876007, Final Batch Loss: 0.35123780369758606\n",
      "Epoch 1195, Loss: 0.6301951706409454, Final Batch Loss: 0.3330121636390686\n",
      "Epoch 1196, Loss: 0.6001092195510864, Final Batch Loss: 0.27831125259399414\n",
      "Epoch 1197, Loss: 0.5594351291656494, Final Batch Loss: 0.28127384185791016\n",
      "Epoch 1198, Loss: 0.5894540548324585, Final Batch Loss: 0.2849119007587433\n",
      "Epoch 1199, Loss: 0.634139746427536, Final Batch Loss: 0.302412211894989\n",
      "Epoch 1200, Loss: 0.5516097247600555, Final Batch Loss: 0.28307589888572693\n",
      "Epoch 1201, Loss: 0.6169525384902954, Final Batch Loss: 0.36066052317619324\n",
      "Epoch 1202, Loss: 0.6409621834754944, Final Batch Loss: 0.3394243121147156\n",
      "Epoch 1203, Loss: 0.5175647288560867, Final Batch Loss: 0.231079563498497\n",
      "Epoch 1204, Loss: 0.6118579208850861, Final Batch Loss: 0.3432334065437317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1205, Loss: 0.5691348612308502, Final Batch Loss: 0.2797747552394867\n",
      "Epoch 1206, Loss: 0.6337013244628906, Final Batch Loss: 0.32844147086143494\n",
      "Epoch 1207, Loss: 0.607966959476471, Final Batch Loss: 0.33895599842071533\n",
      "Epoch 1208, Loss: 0.5789573490619659, Final Batch Loss: 0.27284500002861023\n",
      "Epoch 1209, Loss: 0.5378921926021576, Final Batch Loss: 0.2688891589641571\n",
      "Epoch 1210, Loss: 0.6033991873264313, Final Batch Loss: 0.29728108644485474\n",
      "Epoch 1211, Loss: 0.5746386349201202, Final Batch Loss: 0.29522401094436646\n",
      "Epoch 1212, Loss: 0.5880067646503448, Final Batch Loss: 0.3189554810523987\n",
      "Epoch 1213, Loss: 0.6014097332954407, Final Batch Loss: 0.3330841064453125\n",
      "Epoch 1214, Loss: 0.55830118060112, Final Batch Loss: 0.28154146671295166\n",
      "Epoch 1215, Loss: 0.6013971865177155, Final Batch Loss: 0.2704852223396301\n",
      "Epoch 1216, Loss: 0.5387031584978104, Final Batch Loss: 0.24651919305324554\n",
      "Epoch 1217, Loss: 0.619349479675293, Final Batch Loss: 0.3436535894870758\n",
      "Epoch 1218, Loss: 0.6138565540313721, Final Batch Loss: 0.3388676941394806\n",
      "Epoch 1219, Loss: 0.5580971240997314, Final Batch Loss: 0.25506719946861267\n",
      "Epoch 1220, Loss: 0.5874777734279633, Final Batch Loss: 0.2706819474697113\n",
      "Epoch 1221, Loss: 0.5884432792663574, Final Batch Loss: 0.33160877227783203\n",
      "Epoch 1222, Loss: 0.5515577346086502, Final Batch Loss: 0.2401282638311386\n",
      "Epoch 1223, Loss: 0.5717664361000061, Final Batch Loss: 0.28905120491981506\n",
      "Epoch 1224, Loss: 0.6601797938346863, Final Batch Loss: 0.3749163746833801\n",
      "Epoch 1225, Loss: 0.568291962146759, Final Batch Loss: 0.2702917158603668\n",
      "Epoch 1226, Loss: 0.5558727383613586, Final Batch Loss: 0.27591046690940857\n",
      "Epoch 1227, Loss: 0.5782553404569626, Final Batch Loss: 0.22846587002277374\n",
      "Epoch 1228, Loss: 0.6224980652332306, Final Batch Loss: 0.35509368777275085\n",
      "Epoch 1229, Loss: 0.6090473830699921, Final Batch Loss: 0.28926318883895874\n",
      "Epoch 1230, Loss: 0.5758128762245178, Final Batch Loss: 0.326953649520874\n",
      "Epoch 1231, Loss: 0.5776135623455048, Final Batch Loss: 0.2927555739879608\n",
      "Epoch 1232, Loss: 0.7034511566162109, Final Batch Loss: 0.35426563024520874\n",
      "Epoch 1233, Loss: 0.5865463614463806, Final Batch Loss: 0.29947569966316223\n",
      "Epoch 1234, Loss: 0.6571613848209381, Final Batch Loss: 0.35876986384391785\n",
      "Epoch 1235, Loss: 0.5853776037693024, Final Batch Loss: 0.3190535008907318\n",
      "Epoch 1236, Loss: 0.5815518498420715, Final Batch Loss: 0.3042880892753601\n",
      "Epoch 1237, Loss: 0.6245354115962982, Final Batch Loss: 0.34648555517196655\n",
      "Epoch 1238, Loss: 0.5572012662887573, Final Batch Loss: 0.27276572585105896\n",
      "Epoch 1239, Loss: 0.6354292631149292, Final Batch Loss: 0.3397611677646637\n",
      "Epoch 1240, Loss: 0.5782498270273209, Final Batch Loss: 0.24972735345363617\n",
      "Epoch 1241, Loss: 0.5877866744995117, Final Batch Loss: 0.3316652476787567\n",
      "Epoch 1242, Loss: 0.6371098458766937, Final Batch Loss: 0.3407664895057678\n",
      "Epoch 1243, Loss: 0.663219541311264, Final Batch Loss: 0.3464828431606293\n",
      "Epoch 1244, Loss: 0.5987610816955566, Final Batch Loss: 0.31501320004463196\n",
      "Epoch 1245, Loss: 0.5379069149494171, Final Batch Loss: 0.27407535910606384\n",
      "Epoch 1246, Loss: 0.6122955977916718, Final Batch Loss: 0.3034633994102478\n",
      "Epoch 1247, Loss: 0.5450007170438766, Final Batch Loss: 0.3072548806667328\n",
      "Epoch 1248, Loss: 0.5904844701290131, Final Batch Loss: 0.3019508421421051\n",
      "Epoch 1249, Loss: 0.6831488013267517, Final Batch Loss: 0.31676989793777466\n",
      "Epoch 1250, Loss: 0.5867097228765488, Final Batch Loss: 0.23927398025989532\n",
      "Epoch 1251, Loss: 0.6209479570388794, Final Batch Loss: 0.34701523184776306\n",
      "Epoch 1252, Loss: 0.5930039882659912, Final Batch Loss: 0.2934512197971344\n",
      "Epoch 1253, Loss: 0.53505077958107, Final Batch Loss: 0.2731780707836151\n",
      "Epoch 1254, Loss: 0.57045117020607, Final Batch Loss: 0.2541753947734833\n",
      "Epoch 1255, Loss: 0.5477492958307266, Final Batch Loss: 0.24145416915416718\n",
      "Epoch 1256, Loss: 0.5339195728302002, Final Batch Loss: 0.2629656195640564\n",
      "Epoch 1257, Loss: 0.5548458099365234, Final Batch Loss: 0.27693623304367065\n",
      "Epoch 1258, Loss: 0.6113858819007874, Final Batch Loss: 0.34912317991256714\n",
      "Epoch 1259, Loss: 0.5685862004756927, Final Batch Loss: 0.2808848023414612\n",
      "Epoch 1260, Loss: 0.6143746376037598, Final Batch Loss: 0.3201066553592682\n",
      "Epoch 1261, Loss: 0.5645580738782883, Final Batch Loss: 0.23357070982456207\n",
      "Epoch 1262, Loss: 0.5360760986804962, Final Batch Loss: 0.2663806080818176\n",
      "Epoch 1263, Loss: 0.555877685546875, Final Batch Loss: 0.291498601436615\n",
      "Epoch 1264, Loss: 0.564480260014534, Final Batch Loss: 0.31685835123062134\n",
      "Epoch 1265, Loss: 0.5704362690448761, Final Batch Loss: 0.2709454298019409\n",
      "Epoch 1266, Loss: 0.6228877305984497, Final Batch Loss: 0.35709595680236816\n",
      "Epoch 1267, Loss: 0.5403287410736084, Final Batch Loss: 0.2594403624534607\n",
      "Epoch 1268, Loss: 0.5648706257343292, Final Batch Loss: 0.23033690452575684\n",
      "Epoch 1269, Loss: 0.5568725168704987, Final Batch Loss: 0.289523184299469\n",
      "Epoch 1270, Loss: 0.6036274135112762, Final Batch Loss: 0.2862776219844818\n",
      "Epoch 1271, Loss: 0.5606261491775513, Final Batch Loss: 0.25101757049560547\n",
      "Epoch 1272, Loss: 0.5369895696640015, Final Batch Loss: 0.2577633857727051\n",
      "Epoch 1273, Loss: 0.5946587920188904, Final Batch Loss: 0.3101862668991089\n",
      "Epoch 1274, Loss: 0.5451130867004395, Final Batch Loss: 0.2687968313694\n",
      "Epoch 1275, Loss: 0.6674221158027649, Final Batch Loss: 0.3349123001098633\n",
      "Epoch 1276, Loss: 0.5905413031578064, Final Batch Loss: 0.3456479012966156\n",
      "Epoch 1277, Loss: 0.5347262918949127, Final Batch Loss: 0.26350829005241394\n",
      "Epoch 1278, Loss: 0.7392095625400543, Final Batch Loss: 0.41421598196029663\n",
      "Epoch 1279, Loss: 0.6040636599063873, Final Batch Loss: 0.3168831169605255\n",
      "Epoch 1280, Loss: 0.569802314043045, Final Batch Loss: 0.26059240102767944\n",
      "Epoch 1281, Loss: 0.6261151731014252, Final Batch Loss: 0.36395519971847534\n",
      "Epoch 1282, Loss: 0.5435287654399872, Final Batch Loss: 0.302476704120636\n",
      "Epoch 1283, Loss: 0.5895721018314362, Final Batch Loss: 0.3354451358318329\n",
      "Epoch 1284, Loss: 0.5846749246120453, Final Batch Loss: 0.27635669708251953\n",
      "Epoch 1285, Loss: 0.5709506571292877, Final Batch Loss: 0.2885660231113434\n",
      "Epoch 1286, Loss: 0.5930972993373871, Final Batch Loss: 0.31613773107528687\n",
      "Epoch 1287, Loss: 0.594184935092926, Final Batch Loss: 0.3148914575576782\n",
      "Epoch 1288, Loss: 0.5411269515752792, Final Batch Loss: 0.3020772933959961\n",
      "Epoch 1289, Loss: 0.5818047821521759, Final Batch Loss: 0.23635289072990417\n",
      "Epoch 1290, Loss: 0.5818409621715546, Final Batch Loss: 0.3123227059841156\n",
      "Epoch 1291, Loss: 0.5309444665908813, Final Batch Loss: 0.2599802315235138\n",
      "Epoch 1292, Loss: 0.54841148853302, Final Batch Loss: 0.2718888819217682\n",
      "Epoch 1293, Loss: 0.5427136719226837, Final Batch Loss: 0.2783670425415039\n",
      "Epoch 1294, Loss: 0.5638912618160248, Final Batch Loss: 0.25790250301361084\n",
      "Epoch 1295, Loss: 0.5575461983680725, Final Batch Loss: 0.3050262928009033\n",
      "Epoch 1296, Loss: 0.5717959702014923, Final Batch Loss: 0.30499178171157837\n",
      "Epoch 1297, Loss: 0.571390837430954, Final Batch Loss: 0.2717767655849457\n",
      "Epoch 1298, Loss: 0.5548751652240753, Final Batch Loss: 0.19198864698410034\n",
      "Epoch 1299, Loss: 0.5328588783740997, Final Batch Loss: 0.2859428822994232\n",
      "Epoch 1300, Loss: 0.5472134351730347, Final Batch Loss: 0.26229944825172424\n",
      "Epoch 1301, Loss: 0.5263199806213379, Final Batch Loss: 0.25862354040145874\n",
      "Epoch 1302, Loss: 0.5867807269096375, Final Batch Loss: 0.3053824305534363\n",
      "Epoch 1303, Loss: 0.558268129825592, Final Batch Loss: 0.27550995349884033\n",
      "Epoch 1304, Loss: 0.5336706340312958, Final Batch Loss: 0.22575217485427856\n",
      "Epoch 1305, Loss: 0.4945748448371887, Final Batch Loss: 0.23844259977340698\n",
      "Epoch 1306, Loss: 0.6664358377456665, Final Batch Loss: 0.348223477602005\n",
      "Epoch 1307, Loss: 0.5742358714342117, Final Batch Loss: 0.23536615073680878\n",
      "Epoch 1308, Loss: 0.5098755657672882, Final Batch Loss: 0.25609928369522095\n",
      "Epoch 1309, Loss: 0.6276851892471313, Final Batch Loss: 0.34999462962150574\n",
      "Epoch 1310, Loss: 0.5259231477975845, Final Batch Loss: 0.19219432771205902\n",
      "Epoch 1311, Loss: 0.5669054836034775, Final Batch Loss: 0.24975459277629852\n",
      "Epoch 1312, Loss: 0.5682876408100128, Final Batch Loss: 0.31579598784446716\n",
      "Epoch 1313, Loss: 0.528223067522049, Final Batch Loss: 0.28509455919265747\n",
      "Epoch 1314, Loss: 0.5974155962467194, Final Batch Loss: 0.2782495617866516\n",
      "Epoch 1315, Loss: 0.579628199338913, Final Batch Loss: 0.2815971076488495\n",
      "Epoch 1316, Loss: 0.5749364197254181, Final Batch Loss: 0.2827160656452179\n",
      "Epoch 1317, Loss: 0.5486529767513275, Final Batch Loss: 0.22740495204925537\n",
      "Epoch 1318, Loss: 0.5024124085903168, Final Batch Loss: 0.25097501277923584\n",
      "Epoch 1319, Loss: 0.5606876313686371, Final Batch Loss: 0.2795046269893646\n",
      "Epoch 1320, Loss: 0.5231100022792816, Final Batch Loss: 0.25714725255966187\n",
      "Epoch 1321, Loss: 0.5826437771320343, Final Batch Loss: 0.3101958930492401\n",
      "Epoch 1322, Loss: 0.5951843857765198, Final Batch Loss: 0.3062579333782196\n",
      "Epoch 1323, Loss: 0.5724989473819733, Final Batch Loss: 0.3020436763763428\n",
      "Epoch 1324, Loss: 0.6026092767715454, Final Batch Loss: 0.36093392968177795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1325, Loss: 0.5087450593709946, Final Batch Loss: 0.2593848407268524\n",
      "Epoch 1326, Loss: 0.49906373023986816, Final Batch Loss: 0.23379811644554138\n",
      "Epoch 1327, Loss: 0.5648517906665802, Final Batch Loss: 0.28105252981185913\n",
      "Epoch 1328, Loss: 0.5216459631919861, Final Batch Loss: 0.2519734501838684\n",
      "Epoch 1329, Loss: 0.5192695260047913, Final Batch Loss: 0.23825648427009583\n",
      "Epoch 1330, Loss: 0.4959630072116852, Final Batch Loss: 0.24856232106685638\n",
      "Epoch 1331, Loss: 0.494183674454689, Final Batch Loss: 0.22980813682079315\n",
      "Epoch 1332, Loss: 0.5677741318941116, Final Batch Loss: 0.3303121328353882\n",
      "Epoch 1333, Loss: 0.49670541286468506, Final Batch Loss: 0.2479923665523529\n",
      "Epoch 1334, Loss: 0.634182333946228, Final Batch Loss: 0.3425733149051666\n",
      "Epoch 1335, Loss: 0.6355490982532501, Final Batch Loss: 0.30228328704833984\n",
      "Epoch 1336, Loss: 0.5262584537267685, Final Batch Loss: 0.2929439842700958\n",
      "Epoch 1337, Loss: 0.6534213125705719, Final Batch Loss: 0.3095516562461853\n",
      "Epoch 1338, Loss: 0.5747253000736237, Final Batch Loss: 0.3161531090736389\n",
      "Epoch 1339, Loss: 0.49815474450588226, Final Batch Loss: 0.191899374127388\n",
      "Epoch 1340, Loss: 0.5461335480213165, Final Batch Loss: 0.2459961473941803\n",
      "Epoch 1341, Loss: 0.5483198016881943, Final Batch Loss: 0.30563777685165405\n",
      "Epoch 1342, Loss: 0.6247127652168274, Final Batch Loss: 0.2817809283733368\n",
      "Epoch 1343, Loss: 0.6207119226455688, Final Batch Loss: 0.3307703733444214\n",
      "Epoch 1344, Loss: 0.5495735108852386, Final Batch Loss: 0.28993889689445496\n",
      "Epoch 1345, Loss: 0.5505553930997849, Final Batch Loss: 0.31376326084136963\n",
      "Epoch 1346, Loss: 0.6087690889835358, Final Batch Loss: 0.31609904766082764\n",
      "Epoch 1347, Loss: 0.579277515411377, Final Batch Loss: 0.28905490040779114\n",
      "Epoch 1348, Loss: 0.5482409298419952, Final Batch Loss: 0.25676199793815613\n",
      "Epoch 1349, Loss: 0.5346049666404724, Final Batch Loss: 0.26267561316490173\n",
      "Epoch 1350, Loss: 0.4865350276231766, Final Batch Loss: 0.23888351023197174\n",
      "Epoch 1351, Loss: 0.5647038221359253, Final Batch Loss: 0.2778669595718384\n",
      "Epoch 1352, Loss: 0.5796069502830505, Final Batch Loss: 0.28947457671165466\n",
      "Epoch 1353, Loss: 0.5449925363063812, Final Batch Loss: 0.26700451970100403\n",
      "Epoch 1354, Loss: 0.46988604962825775, Final Batch Loss: 0.22385312616825104\n",
      "Epoch 1355, Loss: 0.5879271626472473, Final Batch Loss: 0.26044633984565735\n",
      "Epoch 1356, Loss: 0.5519055128097534, Final Batch Loss: 0.27435436844825745\n",
      "Epoch 1357, Loss: 0.5328159928321838, Final Batch Loss: 0.27220419049263\n",
      "Epoch 1358, Loss: 0.58673095703125, Final Batch Loss: 0.3161320686340332\n",
      "Epoch 1359, Loss: 0.5392533987760544, Final Batch Loss: 0.24872516095638275\n",
      "Epoch 1360, Loss: 0.5291020572185516, Final Batch Loss: 0.2547358274459839\n",
      "Epoch 1361, Loss: 0.5821034908294678, Final Batch Loss: 0.2898634374141693\n",
      "Epoch 1362, Loss: 0.5101468414068222, Final Batch Loss: 0.24177615344524384\n",
      "Epoch 1363, Loss: 0.5053750425577164, Final Batch Loss: 0.26113903522491455\n",
      "Epoch 1364, Loss: 0.5727452337741852, Final Batch Loss: 0.3023994266986847\n",
      "Epoch 1365, Loss: 0.5565027594566345, Final Batch Loss: 0.3024734556674957\n",
      "Epoch 1366, Loss: 0.6139949262142181, Final Batch Loss: 0.34733447432518005\n",
      "Epoch 1367, Loss: 0.5455147176980972, Final Batch Loss: 0.3023768663406372\n",
      "Epoch 1368, Loss: 0.528407022356987, Final Batch Loss: 0.24879147112369537\n",
      "Epoch 1369, Loss: 0.5299921035766602, Final Batch Loss: 0.2711622714996338\n",
      "Epoch 1370, Loss: 0.4858499616384506, Final Batch Loss: 0.23501606285572052\n",
      "Epoch 1371, Loss: 0.4922705590724945, Final Batch Loss: 0.2674766480922699\n",
      "Epoch 1372, Loss: 0.5060720443725586, Final Batch Loss: 0.25584790110588074\n",
      "Epoch 1373, Loss: 0.5065035820007324, Final Batch Loss: 0.2693856358528137\n",
      "Epoch 1374, Loss: 0.542537659406662, Final Batch Loss: 0.2684996724128723\n",
      "Epoch 1375, Loss: 0.5040205121040344, Final Batch Loss: 0.2599972188472748\n",
      "Epoch 1376, Loss: 0.5003129839897156, Final Batch Loss: 0.24724817276000977\n",
      "Epoch 1377, Loss: 0.5048680305480957, Final Batch Loss: 0.25143393874168396\n",
      "Epoch 1378, Loss: 0.5394884496927261, Final Batch Loss: 0.22168834507465363\n",
      "Epoch 1379, Loss: 0.587012380361557, Final Batch Loss: 0.31123170256614685\n",
      "Epoch 1380, Loss: 0.5148941576480865, Final Batch Loss: 0.24382644891738892\n",
      "Epoch 1381, Loss: 0.5701630711555481, Final Batch Loss: 0.2785903513431549\n",
      "Epoch 1382, Loss: 0.5186336189508438, Final Batch Loss: 0.2962624728679657\n",
      "Epoch 1383, Loss: 0.5223160535097122, Final Batch Loss: 0.2834552228450775\n",
      "Epoch 1384, Loss: 0.5800663828849792, Final Batch Loss: 0.25021618604660034\n",
      "Epoch 1385, Loss: 0.4561811089515686, Final Batch Loss: 0.20997408032417297\n",
      "Epoch 1386, Loss: 0.4978001266717911, Final Batch Loss: 0.23435108363628387\n",
      "Epoch 1387, Loss: 0.5362720489501953, Final Batch Loss: 0.28409698605537415\n",
      "Epoch 1388, Loss: 0.5304398238658905, Final Batch Loss: 0.2561551332473755\n",
      "Epoch 1389, Loss: 0.4935074895620346, Final Batch Loss: 0.2576102018356323\n",
      "Epoch 1390, Loss: 0.5225108563899994, Final Batch Loss: 0.2577627897262573\n",
      "Epoch 1391, Loss: 0.5100238025188446, Final Batch Loss: 0.2610638439655304\n",
      "Epoch 1392, Loss: 0.47611014544963837, Final Batch Loss: 0.19827301800251007\n",
      "Epoch 1393, Loss: 0.4928651452064514, Final Batch Loss: 0.24274829030036926\n",
      "Epoch 1394, Loss: 0.5021867603063583, Final Batch Loss: 0.253134161233902\n",
      "Epoch 1395, Loss: 0.5424528419971466, Final Batch Loss: 0.2897753417491913\n",
      "Epoch 1396, Loss: 0.49726149439811707, Final Batch Loss: 0.24530023336410522\n",
      "Epoch 1397, Loss: 0.5273555964231491, Final Batch Loss: 0.28666558861732483\n",
      "Epoch 1398, Loss: 0.45722879469394684, Final Batch Loss: 0.22600778937339783\n",
      "Epoch 1399, Loss: 0.485037162899971, Final Batch Loss: 0.21722812950611115\n",
      "Epoch 1400, Loss: 0.6099632084369659, Final Batch Loss: 0.3184003531932831\n",
      "Epoch 1401, Loss: 0.49457019567489624, Final Batch Loss: 0.2840593755245209\n",
      "Epoch 1402, Loss: 0.5010145455598831, Final Batch Loss: 0.26629748940467834\n",
      "Epoch 1403, Loss: 0.48381416499614716, Final Batch Loss: 0.2590712010860443\n",
      "Epoch 1404, Loss: 0.5894539952278137, Final Batch Loss: 0.3174004852771759\n",
      "Epoch 1405, Loss: 0.47992001473903656, Final Batch Loss: 0.22421298921108246\n",
      "Epoch 1406, Loss: 0.5523330569267273, Final Batch Loss: 0.2868458926677704\n",
      "Epoch 1407, Loss: 0.515446349978447, Final Batch Loss: 0.2731073498725891\n",
      "Epoch 1408, Loss: 0.5323890894651413, Final Batch Loss: 0.29750728607177734\n",
      "Epoch 1409, Loss: 0.6667667627334595, Final Batch Loss: 0.38575252890586853\n",
      "Epoch 1410, Loss: 0.5162305980920792, Final Batch Loss: 0.27914658188819885\n",
      "Epoch 1411, Loss: 0.6069629192352295, Final Batch Loss: 0.35325512290000916\n",
      "Epoch 1412, Loss: 0.5321540236473083, Final Batch Loss: 0.3069879412651062\n",
      "Epoch 1413, Loss: 0.4933953285217285, Final Batch Loss: 0.25772327184677124\n",
      "Epoch 1414, Loss: 0.5511951148509979, Final Batch Loss: 0.29257357120513916\n",
      "Epoch 1415, Loss: 0.47670985758304596, Final Batch Loss: 0.2240530103445053\n",
      "Epoch 1416, Loss: 0.4898550659418106, Final Batch Loss: 0.24596498906612396\n",
      "Epoch 1417, Loss: 0.5720781236886978, Final Batch Loss: 0.3422269821166992\n",
      "Epoch 1418, Loss: 0.4923398792743683, Final Batch Loss: 0.21735501289367676\n",
      "Epoch 1419, Loss: 0.5851781070232391, Final Batch Loss: 0.3274286389350891\n",
      "Epoch 1420, Loss: 0.43806491792201996, Final Batch Loss: 0.21710702776908875\n",
      "Epoch 1421, Loss: 0.4726265072822571, Final Batch Loss: 0.20443084836006165\n",
      "Epoch 1422, Loss: 0.527912974357605, Final Batch Loss: 0.2467961609363556\n",
      "Epoch 1423, Loss: 0.4749370664358139, Final Batch Loss: 0.22700510919094086\n",
      "Epoch 1424, Loss: 0.5079385340213776, Final Batch Loss: 0.2411562204360962\n",
      "Epoch 1425, Loss: 0.4839719980955124, Final Batch Loss: 0.20800362527370453\n",
      "Epoch 1426, Loss: 0.5085538923740387, Final Batch Loss: 0.2973264455795288\n",
      "Epoch 1427, Loss: 0.5052122473716736, Final Batch Loss: 0.23002925515174866\n",
      "Epoch 1428, Loss: 0.47062310576438904, Final Batch Loss: 0.21468865871429443\n",
      "Epoch 1429, Loss: 0.5448000431060791, Final Batch Loss: 0.2946738600730896\n",
      "Epoch 1430, Loss: 0.46028052270412445, Final Batch Loss: 0.24181024730205536\n",
      "Epoch 1431, Loss: 0.48545853793621063, Final Batch Loss: 0.2043391913175583\n",
      "Epoch 1432, Loss: 0.5736647546291351, Final Batch Loss: 0.32463163137435913\n",
      "Epoch 1433, Loss: 0.4968033879995346, Final Batch Loss: 0.2766121029853821\n",
      "Epoch 1434, Loss: 0.520696371793747, Final Batch Loss: 0.2910299301147461\n",
      "Epoch 1435, Loss: 0.4736226797103882, Final Batch Loss: 0.20867997407913208\n",
      "Epoch 1436, Loss: 0.48414336144924164, Final Batch Loss: 0.23591740429401398\n",
      "Epoch 1437, Loss: 0.5154497474431992, Final Batch Loss: 0.28790393471717834\n",
      "Epoch 1438, Loss: 0.4918662756681442, Final Batch Loss: 0.23443420231342316\n",
      "Epoch 1439, Loss: 0.4936259388923645, Final Batch Loss: 0.21000897884368896\n",
      "Epoch 1440, Loss: 0.5003616362810135, Final Batch Loss: 0.27652618288993835\n",
      "Epoch 1441, Loss: 0.475669264793396, Final Batch Loss: 0.23171062767505646\n",
      "Epoch 1442, Loss: 0.5323353260755539, Final Batch Loss: 0.2883990705013275\n",
      "Epoch 1443, Loss: 0.497121199965477, Final Batch Loss: 0.24332331120967865\n",
      "Epoch 1444, Loss: 0.46593908965587616, Final Batch Loss: 0.2141217738389969\n",
      "Epoch 1445, Loss: 0.4452875852584839, Final Batch Loss: 0.2179335057735443\n",
      "Epoch 1446, Loss: 0.4781419634819031, Final Batch Loss: 0.22243118286132812\n",
      "Epoch 1447, Loss: 0.5271436274051666, Final Batch Loss: 0.2541303038597107\n",
      "Epoch 1448, Loss: 0.4702105224132538, Final Batch Loss: 0.16254976391792297\n",
      "Epoch 1449, Loss: 0.44893547892570496, Final Batch Loss: 0.2427586317062378\n",
      "Epoch 1450, Loss: 0.49505703151226044, Final Batch Loss: 0.23439617455005646\n",
      "Epoch 1451, Loss: 0.560461238026619, Final Batch Loss: 0.3262391984462738\n",
      "Epoch 1452, Loss: 0.504891574382782, Final Batch Loss: 0.2718578577041626\n",
      "Epoch 1453, Loss: 0.462324395775795, Final Batch Loss: 0.22591912746429443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1454, Loss: 0.4481908679008484, Final Batch Loss: 0.2118474841117859\n",
      "Epoch 1455, Loss: 0.46477943658828735, Final Batch Loss: 0.25835078954696655\n",
      "Epoch 1456, Loss: 0.46762946248054504, Final Batch Loss: 0.24839940667152405\n",
      "Epoch 1457, Loss: 0.45241494476795197, Final Batch Loss: 0.18704544007778168\n",
      "Epoch 1458, Loss: 0.49613435566425323, Final Batch Loss: 0.23995624482631683\n",
      "Epoch 1459, Loss: 0.5035723596811295, Final Batch Loss: 0.25892794132232666\n",
      "Epoch 1460, Loss: 0.4675304740667343, Final Batch Loss: 0.249015212059021\n",
      "Epoch 1461, Loss: 0.4464304447174072, Final Batch Loss: 0.2068185806274414\n",
      "Epoch 1462, Loss: 0.49689894914627075, Final Batch Loss: 0.2209809124469757\n",
      "Epoch 1463, Loss: 0.4275908023118973, Final Batch Loss: 0.20788425207138062\n",
      "Epoch 1464, Loss: 0.4782394468784332, Final Batch Loss: 0.2524467706680298\n",
      "Epoch 1465, Loss: 0.49459511041641235, Final Batch Loss: 0.24827773869037628\n",
      "Epoch 1466, Loss: 0.46771635115146637, Final Batch Loss: 0.24808508157730103\n",
      "Epoch 1467, Loss: 0.5081726908683777, Final Batch Loss: 0.2797248959541321\n",
      "Epoch 1468, Loss: 0.4236825704574585, Final Batch Loss: 0.1970096230506897\n",
      "Epoch 1469, Loss: 0.4497123658657074, Final Batch Loss: 0.20607611536979675\n",
      "Epoch 1470, Loss: 0.4357197731733322, Final Batch Loss: 0.1984463632106781\n",
      "Epoch 1471, Loss: 0.5158678591251373, Final Batch Loss: 0.26538851857185364\n",
      "Epoch 1472, Loss: 0.496941402554512, Final Batch Loss: 0.2808030843734741\n",
      "Epoch 1473, Loss: 0.43788938224315643, Final Batch Loss: 0.21248920261859894\n",
      "Epoch 1474, Loss: 0.46601784229278564, Final Batch Loss: 0.21715569496154785\n",
      "Epoch 1475, Loss: 0.500794306397438, Final Batch Loss: 0.2467731088399887\n",
      "Epoch 1476, Loss: 0.4367053955793381, Final Batch Loss: 0.1915048211812973\n",
      "Epoch 1477, Loss: 0.4729515165090561, Final Batch Loss: 0.25778928399086\n",
      "Epoch 1478, Loss: 0.4510684907436371, Final Batch Loss: 0.21308410167694092\n",
      "Epoch 1479, Loss: 0.4119001030921936, Final Batch Loss: 0.2196200042963028\n",
      "Epoch 1480, Loss: 0.45224498212337494, Final Batch Loss: 0.23641034960746765\n",
      "Epoch 1481, Loss: 0.4379587918519974, Final Batch Loss: 0.19634875655174255\n",
      "Epoch 1482, Loss: 0.4542858749628067, Final Batch Loss: 0.2075255960226059\n",
      "Epoch 1483, Loss: 0.5016224980354309, Final Batch Loss: 0.269712895154953\n",
      "Epoch 1484, Loss: 0.4296714663505554, Final Batch Loss: 0.22510460019111633\n",
      "Epoch 1485, Loss: 0.4782378822565079, Final Batch Loss: 0.25729963183403015\n",
      "Epoch 1486, Loss: 0.4346904307603836, Final Batch Loss: 0.18982400000095367\n",
      "Epoch 1487, Loss: 0.46634626388549805, Final Batch Loss: 0.23171676695346832\n",
      "Epoch 1488, Loss: 0.452955424785614, Final Batch Loss: 0.1897355616092682\n",
      "Epoch 1489, Loss: 0.4043964147567749, Final Batch Loss: 0.16290327906608582\n",
      "Epoch 1490, Loss: 0.4252752661705017, Final Batch Loss: 0.17881877720355988\n",
      "Epoch 1491, Loss: 0.523627907037735, Final Batch Loss: 0.3466549515724182\n",
      "Epoch 1492, Loss: 0.45961374044418335, Final Batch Loss: 0.23933805525302887\n",
      "Epoch 1493, Loss: 0.5072196274995804, Final Batch Loss: 0.2394358068704605\n",
      "Epoch 1494, Loss: 0.4905785769224167, Final Batch Loss: 0.2673204839229584\n",
      "Epoch 1495, Loss: 0.5217972695827484, Final Batch Loss: 0.26617053151130676\n",
      "Epoch 1496, Loss: 0.4462343901395798, Final Batch Loss: 0.21648424863815308\n",
      "Epoch 1497, Loss: 0.5231478363275528, Final Batch Loss: 0.2941616475582123\n",
      "Epoch 1498, Loss: 0.44339145720005035, Final Batch Loss: 0.24223144352436066\n",
      "Epoch 1499, Loss: 0.4601926803588867, Final Batch Loss: 0.2362402230501175\n",
      "Epoch 1500, Loss: 0.467117115855217, Final Batch Loss: 0.20481158792972565\n",
      "Epoch 1501, Loss: 0.42070576548576355, Final Batch Loss: 0.1912592500448227\n",
      "Epoch 1502, Loss: 0.5609633922576904, Final Batch Loss: 0.30387893319129944\n",
      "Epoch 1503, Loss: 0.464243546128273, Final Batch Loss: 0.2258952558040619\n",
      "Epoch 1504, Loss: 0.4671972692012787, Final Batch Loss: 0.23775625228881836\n",
      "Epoch 1505, Loss: 0.4743213355541229, Final Batch Loss: 0.26800253987312317\n",
      "Epoch 1506, Loss: 0.3691966235637665, Final Batch Loss: 0.160210981965065\n",
      "Epoch 1507, Loss: 0.40498849749565125, Final Batch Loss: 0.21417857706546783\n",
      "Epoch 1508, Loss: 0.5470500886440277, Final Batch Loss: 0.2614648938179016\n",
      "Epoch 1509, Loss: 0.41422349214553833, Final Batch Loss: 0.20367708802223206\n",
      "Epoch 1510, Loss: 0.4419911801815033, Final Batch Loss: 0.21523012220859528\n",
      "Epoch 1511, Loss: 0.48207633197307587, Final Batch Loss: 0.25276604294776917\n",
      "Epoch 1512, Loss: 0.4714342802762985, Final Batch Loss: 0.21268139779567719\n",
      "Epoch 1513, Loss: 0.4530313014984131, Final Batch Loss: 0.19030871987342834\n",
      "Epoch 1514, Loss: 0.4452560991048813, Final Batch Loss: 0.22094202041625977\n",
      "Epoch 1515, Loss: 0.42710956931114197, Final Batch Loss: 0.18340201675891876\n",
      "Epoch 1516, Loss: 0.43753664195537567, Final Batch Loss: 0.23720556497573853\n",
      "Epoch 1517, Loss: 0.3836195170879364, Final Batch Loss: 0.18437804281711578\n",
      "Epoch 1518, Loss: 0.46261075139045715, Final Batch Loss: 0.26238393783569336\n",
      "Epoch 1519, Loss: 0.46636319160461426, Final Batch Loss: 0.1937403380870819\n",
      "Epoch 1520, Loss: 0.5405739545822144, Final Batch Loss: 0.31193527579307556\n",
      "Epoch 1521, Loss: 0.5084512531757355, Final Batch Loss: 0.3017989993095398\n",
      "Epoch 1522, Loss: 0.42775700986385345, Final Batch Loss: 0.21771304309368134\n",
      "Epoch 1523, Loss: 0.4518420398235321, Final Batch Loss: 0.22546714544296265\n",
      "Epoch 1524, Loss: 0.4819566309452057, Final Batch Loss: 0.2665732204914093\n",
      "Epoch 1525, Loss: 0.49300552904605865, Final Batch Loss: 0.24102680385112762\n",
      "Epoch 1526, Loss: 0.4917045384645462, Final Batch Loss: 0.22410215437412262\n",
      "Epoch 1527, Loss: 0.41382473707199097, Final Batch Loss: 0.18278615176677704\n",
      "Epoch 1528, Loss: 0.43072351813316345, Final Batch Loss: 0.22390355169773102\n",
      "Epoch 1529, Loss: 0.39394180476665497, Final Batch Loss: 0.1827489137649536\n",
      "Epoch 1530, Loss: 0.48908422887325287, Final Batch Loss: 0.2597619593143463\n",
      "Epoch 1531, Loss: 0.48048198223114014, Final Batch Loss: 0.2625332474708557\n",
      "Epoch 1532, Loss: 0.45236313343048096, Final Batch Loss: 0.22422464191913605\n",
      "Epoch 1533, Loss: 0.4567117244005203, Final Batch Loss: 0.18254078924655914\n",
      "Epoch 1534, Loss: 0.4899868667125702, Final Batch Loss: 0.31419551372528076\n",
      "Epoch 1535, Loss: 0.3897946923971176, Final Batch Loss: 0.15157638490200043\n",
      "Epoch 1536, Loss: 0.5089283883571625, Final Batch Loss: 0.25522857904434204\n",
      "Epoch 1537, Loss: 0.43814539909362793, Final Batch Loss: 0.19560639560222626\n",
      "Epoch 1538, Loss: 0.4375981390476227, Final Batch Loss: 0.23660613596439362\n",
      "Epoch 1539, Loss: 0.4556884467601776, Final Batch Loss: 0.21259242296218872\n",
      "Epoch 1540, Loss: 0.45316626131534576, Final Batch Loss: 0.2392483353614807\n",
      "Epoch 1541, Loss: 0.4339672029018402, Final Batch Loss: 0.21155700087547302\n",
      "Epoch 1542, Loss: 0.4643506556749344, Final Batch Loss: 0.2326332926750183\n",
      "Epoch 1543, Loss: 0.4568822979927063, Final Batch Loss: 0.2039131224155426\n",
      "Epoch 1544, Loss: 0.46632014214992523, Final Batch Loss: 0.24842406809329987\n",
      "Epoch 1545, Loss: 0.45118448138237, Final Batch Loss: 0.24330802261829376\n",
      "Epoch 1546, Loss: 0.5564852952957153, Final Batch Loss: 0.2813659906387329\n",
      "Epoch 1547, Loss: 0.4463510513305664, Final Batch Loss: 0.2567307949066162\n",
      "Epoch 1548, Loss: 0.5461029708385468, Final Batch Loss: 0.292972594499588\n",
      "Epoch 1549, Loss: 0.492217481136322, Final Batch Loss: 0.295450359582901\n",
      "Epoch 1550, Loss: 0.47142599523067474, Final Batch Loss: 0.25058290362358093\n",
      "Epoch 1551, Loss: 0.4371170550584793, Final Batch Loss: 0.2300347089767456\n",
      "Epoch 1552, Loss: 0.4319080263376236, Final Batch Loss: 0.16997595131397247\n",
      "Epoch 1553, Loss: 0.39859795570373535, Final Batch Loss: 0.18257582187652588\n",
      "Epoch 1554, Loss: 0.4626256078481674, Final Batch Loss: 0.1930304616689682\n",
      "Epoch 1555, Loss: 0.4179326593875885, Final Batch Loss: 0.20858614146709442\n",
      "Epoch 1556, Loss: 0.4475891888141632, Final Batch Loss: 0.2375047206878662\n",
      "Epoch 1557, Loss: 0.4478779286146164, Final Batch Loss: 0.18974749743938446\n",
      "Epoch 1558, Loss: 0.415757492184639, Final Batch Loss: 0.198793426156044\n",
      "Epoch 1559, Loss: 0.4497684985399246, Final Batch Loss: 0.21717619895935059\n",
      "Epoch 1560, Loss: 0.41075754165649414, Final Batch Loss: 0.17635583877563477\n",
      "Epoch 1561, Loss: 0.47349461913108826, Final Batch Loss: 0.21268174052238464\n",
      "Epoch 1562, Loss: 0.4496630132198334, Final Batch Loss: 0.2075526863336563\n",
      "Epoch 1563, Loss: 0.42762158811092377, Final Batch Loss: 0.2314012497663498\n",
      "Epoch 1564, Loss: 0.4588983505964279, Final Batch Loss: 0.27857139706611633\n",
      "Epoch 1565, Loss: 0.44402937591075897, Final Batch Loss: 0.2464764267206192\n",
      "Epoch 1566, Loss: 0.5061756074428558, Final Batch Loss: 0.30491456389427185\n",
      "Epoch 1567, Loss: 0.4557202458381653, Final Batch Loss: 0.21436065435409546\n",
      "Epoch 1568, Loss: 0.43291617929935455, Final Batch Loss: 0.22810423374176025\n",
      "Epoch 1569, Loss: 0.4548543244600296, Final Batch Loss: 0.2105829417705536\n",
      "Epoch 1570, Loss: 0.4794028550386429, Final Batch Loss: 0.21465615928173065\n",
      "Epoch 1571, Loss: 0.3954283446073532, Final Batch Loss: 0.1223328560590744\n",
      "Epoch 1572, Loss: 0.4523470997810364, Final Batch Loss: 0.22294041514396667\n",
      "Epoch 1573, Loss: 0.4520326554775238, Final Batch Loss: 0.2501407265663147\n",
      "Epoch 1574, Loss: 0.4157632738351822, Final Batch Loss: 0.19776210188865662\n",
      "Epoch 1575, Loss: 0.4723852872848511, Final Batch Loss: 0.2570834159851074\n",
      "Epoch 1576, Loss: 0.41505345702171326, Final Batch Loss: 0.20009705424308777\n",
      "Epoch 1577, Loss: 0.513987585902214, Final Batch Loss: 0.2803787887096405\n",
      "Epoch 1578, Loss: 0.4371434599161148, Final Batch Loss: 0.2121461033821106\n",
      "Epoch 1579, Loss: 0.5045308172702789, Final Batch Loss: 0.2687888741493225\n",
      "Epoch 1580, Loss: 0.4946759045124054, Final Batch Loss: 0.2699947655200958\n",
      "Epoch 1581, Loss: 0.44905634224414825, Final Batch Loss: 0.21236427128314972\n",
      "Epoch 1582, Loss: 0.4720255136489868, Final Batch Loss: 0.2543603181838989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1583, Loss: 0.41696973145008087, Final Batch Loss: 0.1732514649629593\n",
      "Epoch 1584, Loss: 0.4272180050611496, Final Batch Loss: 0.19016452133655548\n",
      "Epoch 1585, Loss: 0.4257003664970398, Final Batch Loss: 0.1672506332397461\n",
      "Epoch 1586, Loss: 0.36415979266166687, Final Batch Loss: 0.2020382136106491\n",
      "Epoch 1587, Loss: 0.4809466600418091, Final Batch Loss: 0.2402569204568863\n",
      "Epoch 1588, Loss: 0.4095514118671417, Final Batch Loss: 0.2120194435119629\n",
      "Epoch 1589, Loss: 0.4268077313899994, Final Batch Loss: 0.21630702912807465\n",
      "Epoch 1590, Loss: 0.4875059127807617, Final Batch Loss: 0.30249810218811035\n",
      "Epoch 1591, Loss: 0.43278391659259796, Final Batch Loss: 0.1970416009426117\n",
      "Epoch 1592, Loss: 0.41437116265296936, Final Batch Loss: 0.19751495122909546\n",
      "Epoch 1593, Loss: 0.42061613500118256, Final Batch Loss: 0.24270670115947723\n",
      "Epoch 1594, Loss: 0.4039228856563568, Final Batch Loss: 0.2266426682472229\n",
      "Epoch 1595, Loss: 0.4110741913318634, Final Batch Loss: 0.23754571378231049\n",
      "Epoch 1596, Loss: 0.4038518965244293, Final Batch Loss: 0.1551162749528885\n",
      "Epoch 1597, Loss: 0.3960306793451309, Final Batch Loss: 0.18291300535202026\n",
      "Epoch 1598, Loss: 0.4025627225637436, Final Batch Loss: 0.1928032785654068\n",
      "Epoch 1599, Loss: 0.5013996362686157, Final Batch Loss: 0.21922284364700317\n",
      "Epoch 1600, Loss: 0.4292561560869217, Final Batch Loss: 0.22056126594543457\n",
      "Epoch 1601, Loss: 0.46520349383354187, Final Batch Loss: 0.2068488597869873\n",
      "Epoch 1602, Loss: 0.39375679194927216, Final Batch Loss: 0.18578730523586273\n",
      "Epoch 1603, Loss: 0.46602143347263336, Final Batch Loss: 0.24288642406463623\n",
      "Epoch 1604, Loss: 0.3545563519001007, Final Batch Loss: 0.15192729234695435\n",
      "Epoch 1605, Loss: 0.4374874383211136, Final Batch Loss: 0.23641368746757507\n",
      "Epoch 1606, Loss: 0.3643126040697098, Final Batch Loss: 0.19078414142131805\n",
      "Epoch 1607, Loss: 0.43277157843112946, Final Batch Loss: 0.25557205080986023\n",
      "Epoch 1608, Loss: 0.4599315822124481, Final Batch Loss: 0.2719581425189972\n",
      "Epoch 1609, Loss: 0.3814852088689804, Final Batch Loss: 0.15019497275352478\n",
      "Epoch 1610, Loss: 0.36375191807746887, Final Batch Loss: 0.12828262150287628\n",
      "Epoch 1611, Loss: 0.4296879321336746, Final Batch Loss: 0.2340208739042282\n",
      "Epoch 1612, Loss: 0.4291973263025284, Final Batch Loss: 0.1830863654613495\n",
      "Epoch 1613, Loss: 0.46732760965824127, Final Batch Loss: 0.24798841774463654\n",
      "Epoch 1614, Loss: 0.45828329026699066, Final Batch Loss: 0.22204846143722534\n",
      "Epoch 1615, Loss: 0.4484587460756302, Final Batch Loss: 0.23646411299705505\n",
      "Epoch 1616, Loss: 0.4573505073785782, Final Batch Loss: 0.20640383660793304\n",
      "Epoch 1617, Loss: 0.4238825589418411, Final Batch Loss: 0.22708135843276978\n",
      "Epoch 1618, Loss: 0.39563722908496857, Final Batch Loss: 0.19717320799827576\n",
      "Epoch 1619, Loss: 0.4912070333957672, Final Batch Loss: 0.2587962746620178\n",
      "Epoch 1620, Loss: 0.4128837436437607, Final Batch Loss: 0.18971224129199982\n",
      "Epoch 1621, Loss: 0.35693077743053436, Final Batch Loss: 0.14216266572475433\n",
      "Epoch 1622, Loss: 0.41269107162952423, Final Batch Loss: 0.2632395923137665\n",
      "Epoch 1623, Loss: 0.43593868613243103, Final Batch Loss: 0.20541904866695404\n",
      "Epoch 1624, Loss: 0.49101340770721436, Final Batch Loss: 0.2708333730697632\n",
      "Epoch 1625, Loss: 0.4106797128915787, Final Batch Loss: 0.2070358693599701\n",
      "Epoch 1626, Loss: 0.423642173409462, Final Batch Loss: 0.18990223109722137\n",
      "Epoch 1627, Loss: 0.43378958106040955, Final Batch Loss: 0.22933150827884674\n",
      "Epoch 1628, Loss: 0.5372956991195679, Final Batch Loss: 0.3351312279701233\n",
      "Epoch 1629, Loss: 0.4598920941352844, Final Batch Loss: 0.27998074889183044\n",
      "Epoch 1630, Loss: 0.3932361453771591, Final Batch Loss: 0.1998717337846756\n",
      "Epoch 1631, Loss: 0.3890099823474884, Final Batch Loss: 0.21498094499111176\n",
      "Epoch 1632, Loss: 0.48669469356536865, Final Batch Loss: 0.264437735080719\n",
      "Epoch 1633, Loss: 0.3946976661682129, Final Batch Loss: 0.2098468393087387\n",
      "Epoch 1634, Loss: 0.4347396641969681, Final Batch Loss: 0.23237088322639465\n",
      "Epoch 1635, Loss: 0.4232494980096817, Final Batch Loss: 0.20995253324508667\n",
      "Epoch 1636, Loss: 0.41248540580272675, Final Batch Loss: 0.2338750809431076\n",
      "Epoch 1637, Loss: 0.4381273686885834, Final Batch Loss: 0.23363113403320312\n",
      "Epoch 1638, Loss: 0.4363666772842407, Final Batch Loss: 0.23921746015548706\n",
      "Epoch 1639, Loss: 0.4312477111816406, Final Batch Loss: 0.20342308282852173\n",
      "Epoch 1640, Loss: 0.4119112342596054, Final Batch Loss: 0.21374493837356567\n",
      "Epoch 1641, Loss: 0.3973247557878494, Final Batch Loss: 0.21510984003543854\n",
      "Epoch 1642, Loss: 0.4452913701534271, Final Batch Loss: 0.23223261535167694\n",
      "Epoch 1643, Loss: 0.41784535348415375, Final Batch Loss: 0.19564832746982574\n",
      "Epoch 1644, Loss: 0.39749377965927124, Final Batch Loss: 0.19054897129535675\n",
      "Epoch 1645, Loss: 0.38885945081710815, Final Batch Loss: 0.22902265191078186\n",
      "Epoch 1646, Loss: 0.43512603640556335, Final Batch Loss: 0.22288990020751953\n",
      "Epoch 1647, Loss: 0.4532609134912491, Final Batch Loss: 0.23431727290153503\n",
      "Epoch 1648, Loss: 0.4619661420583725, Final Batch Loss: 0.2567083239555359\n",
      "Epoch 1649, Loss: 0.38718901574611664, Final Batch Loss: 0.20208655297756195\n",
      "Epoch 1650, Loss: 0.36040346324443817, Final Batch Loss: 0.17735986411571503\n",
      "Epoch 1651, Loss: 0.34511204063892365, Final Batch Loss: 0.15248382091522217\n",
      "Epoch 1652, Loss: 0.4068087637424469, Final Batch Loss: 0.23506172001361847\n",
      "Epoch 1653, Loss: 0.3480230122804642, Final Batch Loss: 0.15840259194374084\n",
      "Epoch 1654, Loss: 0.40378738939762115, Final Batch Loss: 0.1754673719406128\n",
      "Epoch 1655, Loss: 0.3995671719312668, Final Batch Loss: 0.21316850185394287\n",
      "Epoch 1656, Loss: 0.37495240569114685, Final Batch Loss: 0.20066344738006592\n",
      "Epoch 1657, Loss: 0.3775048851966858, Final Batch Loss: 0.17075566947460175\n",
      "Epoch 1658, Loss: 0.39766186475753784, Final Batch Loss: 0.20491905510425568\n",
      "Epoch 1659, Loss: 0.34879450500011444, Final Batch Loss: 0.18127459287643433\n",
      "Epoch 1660, Loss: 0.3357701897621155, Final Batch Loss: 0.12617160379886627\n",
      "Epoch 1661, Loss: 0.40938709676265717, Final Batch Loss: 0.2062283605337143\n",
      "Epoch 1662, Loss: 0.40070725977420807, Final Batch Loss: 0.2198326140642166\n",
      "Epoch 1663, Loss: 0.37941135466098785, Final Batch Loss: 0.18549707531929016\n",
      "Epoch 1664, Loss: 0.44560979306697845, Final Batch Loss: 0.20879527926445007\n",
      "Epoch 1665, Loss: 0.3825872093439102, Final Batch Loss: 0.14670290052890778\n",
      "Epoch 1666, Loss: 0.3571620434522629, Final Batch Loss: 0.14750386774539948\n",
      "Epoch 1667, Loss: 0.41237546503543854, Final Batch Loss: 0.20439408719539642\n",
      "Epoch 1668, Loss: 0.39434465765953064, Final Batch Loss: 0.17354613542556763\n",
      "Epoch 1669, Loss: 0.4557855427265167, Final Batch Loss: 0.2198365330696106\n",
      "Epoch 1670, Loss: 0.39657898247241974, Final Batch Loss: 0.18030507862567902\n",
      "Epoch 1671, Loss: 0.34681668877601624, Final Batch Loss: 0.15918800234794617\n",
      "Epoch 1672, Loss: 0.36599791049957275, Final Batch Loss: 0.14816634356975555\n",
      "Epoch 1673, Loss: 0.4081260859966278, Final Batch Loss: 0.19029021263122559\n",
      "Epoch 1674, Loss: 0.32199111580848694, Final Batch Loss: 0.13210229575634003\n",
      "Epoch 1675, Loss: 0.41042986512184143, Final Batch Loss: 0.17757776379585266\n",
      "Epoch 1676, Loss: 0.37079551815986633, Final Batch Loss: 0.19820909202098846\n",
      "Epoch 1677, Loss: 0.3915072828531265, Final Batch Loss: 0.17721417546272278\n",
      "Epoch 1678, Loss: 0.4107102304697037, Final Batch Loss: 0.2028859704732895\n",
      "Epoch 1679, Loss: 0.40658049285411835, Final Batch Loss: 0.21460062265396118\n",
      "Epoch 1680, Loss: 0.4527982026338577, Final Batch Loss: 0.19409646093845367\n",
      "Epoch 1681, Loss: 0.36639687418937683, Final Batch Loss: 0.15688353776931763\n",
      "Epoch 1682, Loss: 0.3389989584684372, Final Batch Loss: 0.1463249921798706\n",
      "Epoch 1683, Loss: 0.42811471223831177, Final Batch Loss: 0.21027599275112152\n",
      "Epoch 1684, Loss: 0.42190584540367126, Final Batch Loss: 0.2121715247631073\n",
      "Epoch 1685, Loss: 0.36209629476070404, Final Batch Loss: 0.1818997859954834\n",
      "Epoch 1686, Loss: 0.45930133759975433, Final Batch Loss: 0.25449466705322266\n",
      "Epoch 1687, Loss: 0.3662862330675125, Final Batch Loss: 0.1550288051366806\n",
      "Epoch 1688, Loss: 0.41481998562812805, Final Batch Loss: 0.2046438753604889\n",
      "Epoch 1689, Loss: 0.40805281698703766, Final Batch Loss: 0.21293041110038757\n",
      "Epoch 1690, Loss: 0.4254312217235565, Final Batch Loss: 0.19683927297592163\n",
      "Epoch 1691, Loss: 0.4317708760499954, Final Batch Loss: 0.21138125658035278\n",
      "Epoch 1692, Loss: 0.3978448808193207, Final Batch Loss: 0.19768811762332916\n",
      "Epoch 1693, Loss: 0.4132038950920105, Final Batch Loss: 0.14533445239067078\n",
      "Epoch 1694, Loss: 0.4028516411781311, Final Batch Loss: 0.2266758680343628\n",
      "Epoch 1695, Loss: 0.3648061752319336, Final Batch Loss: 0.17986740171909332\n",
      "Epoch 1696, Loss: 0.4187175780534744, Final Batch Loss: 0.24369242787361145\n",
      "Epoch 1697, Loss: 0.39204147458076477, Final Batch Loss: 0.1849215179681778\n",
      "Epoch 1698, Loss: 0.4283972978591919, Final Batch Loss: 0.18539033830165863\n",
      "Epoch 1699, Loss: 0.4201459735631943, Final Batch Loss: 0.1778392493724823\n",
      "Epoch 1700, Loss: 0.31751735508441925, Final Batch Loss: 0.15425853431224823\n",
      "Epoch 1701, Loss: 0.38382716476917267, Final Batch Loss: 0.2123400866985321\n",
      "Epoch 1702, Loss: 0.4292529374361038, Final Batch Loss: 0.21961857378482819\n",
      "Epoch 1703, Loss: 0.44367098808288574, Final Batch Loss: 0.2718375623226166\n",
      "Epoch 1704, Loss: 0.384737029671669, Final Batch Loss: 0.2093418389558792\n",
      "Epoch 1705, Loss: 0.4420025199651718, Final Batch Loss: 0.20594006776809692\n",
      "Epoch 1706, Loss: 0.40892042219638824, Final Batch Loss: 0.19546537101268768\n",
      "Epoch 1707, Loss: 0.35592564940452576, Final Batch Loss: 0.16488952934741974\n",
      "Epoch 1708, Loss: 0.3907683342695236, Final Batch Loss: 0.1550954282283783\n",
      "Epoch 1709, Loss: 0.3619510382413864, Final Batch Loss: 0.16493330895900726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1710, Loss: 0.43704281747341156, Final Batch Loss: 0.2472631186246872\n",
      "Epoch 1711, Loss: 0.35615232586860657, Final Batch Loss: 0.176978200674057\n",
      "Epoch 1712, Loss: 0.4999144971370697, Final Batch Loss: 0.321929395198822\n",
      "Epoch 1713, Loss: 0.37113718688488007, Final Batch Loss: 0.1911323219537735\n",
      "Epoch 1714, Loss: 0.4022214263677597, Final Batch Loss: 0.21648608148097992\n",
      "Epoch 1715, Loss: 0.404131755232811, Final Batch Loss: 0.21016645431518555\n",
      "Epoch 1716, Loss: 0.43164438009262085, Final Batch Loss: 0.1938742846250534\n",
      "Epoch 1717, Loss: 0.3471345901489258, Final Batch Loss: 0.19911165535449982\n",
      "Epoch 1718, Loss: 0.4175596982240677, Final Batch Loss: 0.2290859818458557\n",
      "Epoch 1719, Loss: 0.40827442705631256, Final Batch Loss: 0.24146990478038788\n",
      "Epoch 1720, Loss: 0.31227368116378784, Final Batch Loss: 0.13816531002521515\n",
      "Epoch 1721, Loss: 0.3052809089422226, Final Batch Loss: 0.1319272220134735\n",
      "Epoch 1722, Loss: 0.3560236394405365, Final Batch Loss: 0.15937159955501556\n",
      "Epoch 1723, Loss: 0.3640533834695816, Final Batch Loss: 0.20033952593803406\n",
      "Epoch 1724, Loss: 0.4124491214752197, Final Batch Loss: 0.19239187240600586\n",
      "Epoch 1725, Loss: 0.39542169868946075, Final Batch Loss: 0.21798527240753174\n",
      "Epoch 1726, Loss: 0.3886449933052063, Final Batch Loss: 0.20306426286697388\n",
      "Epoch 1727, Loss: 0.37022432684898376, Final Batch Loss: 0.18763360381126404\n",
      "Epoch 1728, Loss: 0.36807678639888763, Final Batch Loss: 0.20306947827339172\n",
      "Epoch 1729, Loss: 0.37985408306121826, Final Batch Loss: 0.20120488107204437\n",
      "Epoch 1730, Loss: 0.3498907536268234, Final Batch Loss: 0.1473078727722168\n",
      "Epoch 1731, Loss: 0.32742057740688324, Final Batch Loss: 0.17093859612941742\n",
      "Epoch 1732, Loss: 0.3452056348323822, Final Batch Loss: 0.15739305317401886\n",
      "Epoch 1733, Loss: 0.41928327083587646, Final Batch Loss: 0.1809660643339157\n",
      "Epoch 1734, Loss: 0.4454580843448639, Final Batch Loss: 0.22154201567173004\n",
      "Epoch 1735, Loss: 0.386306568980217, Final Batch Loss: 0.2115263193845749\n",
      "Epoch 1736, Loss: 0.359786793589592, Final Batch Loss: 0.17556120455265045\n",
      "Epoch 1737, Loss: 0.40738095343112946, Final Batch Loss: 0.2067279815673828\n",
      "Epoch 1738, Loss: 0.367316797375679, Final Batch Loss: 0.20324838161468506\n",
      "Epoch 1739, Loss: 0.4329954981803894, Final Batch Loss: 0.21148018538951874\n",
      "Epoch 1740, Loss: 0.425633043050766, Final Batch Loss: 0.19556838274002075\n",
      "Epoch 1741, Loss: 0.42830075323581696, Final Batch Loss: 0.22117935121059418\n",
      "Epoch 1742, Loss: 0.3490389436483383, Final Batch Loss: 0.17528770864009857\n",
      "Epoch 1743, Loss: 0.3091880977153778, Final Batch Loss: 0.1703258901834488\n",
      "Epoch 1744, Loss: 0.3561195731163025, Final Batch Loss: 0.17243272066116333\n",
      "Epoch 1745, Loss: 0.3894183486700058, Final Batch Loss: 0.1779075562953949\n",
      "Epoch 1746, Loss: 0.34943653643131256, Final Batch Loss: 0.1540658175945282\n",
      "Epoch 1747, Loss: 0.3974977731704712, Final Batch Loss: 0.2089761644601822\n",
      "Epoch 1748, Loss: 0.41166846454143524, Final Batch Loss: 0.21889741718769073\n",
      "Epoch 1749, Loss: 0.4239021837711334, Final Batch Loss: 0.18340973556041718\n",
      "Epoch 1750, Loss: 0.3681907057762146, Final Batch Loss: 0.17528219521045685\n",
      "Epoch 1751, Loss: 0.4118073135614395, Final Batch Loss: 0.19093072414398193\n",
      "Epoch 1752, Loss: 0.34246742725372314, Final Batch Loss: 0.15652070939540863\n",
      "Epoch 1753, Loss: 0.39108866453170776, Final Batch Loss: 0.2139817178249359\n",
      "Epoch 1754, Loss: 0.39943763613700867, Final Batch Loss: 0.2083524763584137\n",
      "Epoch 1755, Loss: 0.3896976262331009, Final Batch Loss: 0.1846924126148224\n",
      "Epoch 1756, Loss: 0.38308772444725037, Final Batch Loss: 0.18237154185771942\n",
      "Epoch 1757, Loss: 0.36673974990844727, Final Batch Loss: 0.18324807286262512\n",
      "Epoch 1758, Loss: 0.3598286509513855, Final Batch Loss: 0.16949348151683807\n",
      "Epoch 1759, Loss: 0.3079306557774544, Final Batch Loss: 0.09988219290971756\n",
      "Epoch 1760, Loss: 0.373882457613945, Final Batch Loss: 0.19021980464458466\n",
      "Epoch 1761, Loss: 0.34264667332172394, Final Batch Loss: 0.12045276165008545\n",
      "Epoch 1762, Loss: 0.4249168038368225, Final Batch Loss: 0.2403692901134491\n",
      "Epoch 1763, Loss: 0.333853542804718, Final Batch Loss: 0.15285766124725342\n",
      "Epoch 1764, Loss: 0.3867105096578598, Final Batch Loss: 0.1949862390756607\n",
      "Epoch 1765, Loss: 0.4551699012517929, Final Batch Loss: 0.2244042456150055\n",
      "Epoch 1766, Loss: 0.3898498862981796, Final Batch Loss: 0.19633041322231293\n",
      "Epoch 1767, Loss: 0.31124891340732574, Final Batch Loss: 0.15688671171665192\n",
      "Epoch 1768, Loss: 0.3639325201511383, Final Batch Loss: 0.17940041422843933\n",
      "Epoch 1769, Loss: 0.3506022244691849, Final Batch Loss: 0.18219473958015442\n",
      "Epoch 1770, Loss: 0.34871654212474823, Final Batch Loss: 0.17366647720336914\n",
      "Epoch 1771, Loss: 0.46577292680740356, Final Batch Loss: 0.2697834372520447\n",
      "Epoch 1772, Loss: 0.323395274579525, Final Batch Loss: 0.12434621900320053\n",
      "Epoch 1773, Loss: 0.40005044639110565, Final Batch Loss: 0.2007356435060501\n",
      "Epoch 1774, Loss: 0.3771144300699234, Final Batch Loss: 0.15388700366020203\n",
      "Epoch 1775, Loss: 0.37276405096054077, Final Batch Loss: 0.20187874138355255\n",
      "Epoch 1776, Loss: 0.3591387867927551, Final Batch Loss: 0.19258004426956177\n",
      "Epoch 1777, Loss: 0.3119051307439804, Final Batch Loss: 0.16133898496627808\n",
      "Epoch 1778, Loss: 0.3921729475259781, Final Batch Loss: 0.2019035667181015\n",
      "Epoch 1779, Loss: 0.4494747817516327, Final Batch Loss: 0.23927858471870422\n",
      "Epoch 1780, Loss: 0.33545978367328644, Final Batch Loss: 0.16141100227832794\n",
      "Epoch 1781, Loss: 0.34709496796131134, Final Batch Loss: 0.17372065782546997\n",
      "Epoch 1782, Loss: 0.3601234704256058, Final Batch Loss: 0.16381515562534332\n",
      "Epoch 1783, Loss: 0.3579590916633606, Final Batch Loss: 0.14912690222263336\n",
      "Epoch 1784, Loss: 0.39972586929798126, Final Batch Loss: 0.22663357853889465\n",
      "Epoch 1785, Loss: 0.43686410784721375, Final Batch Loss: 0.25764232873916626\n",
      "Epoch 1786, Loss: 0.3840303421020508, Final Batch Loss: 0.19184064865112305\n",
      "Epoch 1787, Loss: 0.36920279264450073, Final Batch Loss: 0.16711494326591492\n",
      "Epoch 1788, Loss: 0.3957102298736572, Final Batch Loss: 0.20579370856285095\n",
      "Epoch 1789, Loss: 0.35745958983898163, Final Batch Loss: 0.15546268224716187\n",
      "Epoch 1790, Loss: 0.3522781729698181, Final Batch Loss: 0.18744896352291107\n",
      "Epoch 1791, Loss: 0.37156249582767487, Final Batch Loss: 0.1685788631439209\n",
      "Epoch 1792, Loss: 0.3567066639661789, Final Batch Loss: 0.15047316253185272\n",
      "Epoch 1793, Loss: 0.45015381276607513, Final Batch Loss: 0.23886196315288544\n",
      "Epoch 1794, Loss: 0.39132682979106903, Final Batch Loss: 0.22168441116809845\n",
      "Epoch 1795, Loss: 0.42283159494400024, Final Batch Loss: 0.2152276486158371\n",
      "Epoch 1796, Loss: 0.31710971891880035, Final Batch Loss: 0.1660498082637787\n",
      "Epoch 1797, Loss: 0.3480566442012787, Final Batch Loss: 0.14726603031158447\n",
      "Epoch 1798, Loss: 0.3903851956129074, Final Batch Loss: 0.2311229407787323\n",
      "Epoch 1799, Loss: 0.35449108481407166, Final Batch Loss: 0.1450650990009308\n",
      "Epoch 1800, Loss: 0.4469311386346817, Final Batch Loss: 0.24887122213840485\n",
      "Epoch 1801, Loss: 0.3427312821149826, Final Batch Loss: 0.16183055937290192\n",
      "Epoch 1802, Loss: 0.3362889736890793, Final Batch Loss: 0.18464738130569458\n",
      "Epoch 1803, Loss: 0.40599240362644196, Final Batch Loss: 0.23061996698379517\n",
      "Epoch 1804, Loss: 0.40775133669376373, Final Batch Loss: 0.18859556317329407\n",
      "Epoch 1805, Loss: 0.32522687315940857, Final Batch Loss: 0.1478799432516098\n",
      "Epoch 1806, Loss: 0.3551543056964874, Final Batch Loss: 0.184158593416214\n",
      "Epoch 1807, Loss: 0.43151645362377167, Final Batch Loss: 0.19155055284500122\n",
      "Epoch 1808, Loss: 0.38663993775844574, Final Batch Loss: 0.16745811700820923\n",
      "Epoch 1809, Loss: 0.3523392230272293, Final Batch Loss: 0.16855855286121368\n",
      "Epoch 1810, Loss: 0.30739542841911316, Final Batch Loss: 0.14046938717365265\n",
      "Epoch 1811, Loss: 0.41206927597522736, Final Batch Loss: 0.2356949895620346\n",
      "Epoch 1812, Loss: 0.520010843873024, Final Batch Loss: 0.1713804453611374\n",
      "Epoch 1813, Loss: 0.33354564011096954, Final Batch Loss: 0.14028845727443695\n",
      "Epoch 1814, Loss: 0.36884060502052307, Final Batch Loss: 0.16443198919296265\n",
      "Epoch 1815, Loss: 0.31590692698955536, Final Batch Loss: 0.1617683470249176\n",
      "Epoch 1816, Loss: 0.3510695546865463, Final Batch Loss: 0.1367330104112625\n",
      "Epoch 1817, Loss: 0.35172197222709656, Final Batch Loss: 0.17255476117134094\n",
      "Epoch 1818, Loss: 0.33601465821266174, Final Batch Loss: 0.1662682443857193\n",
      "Epoch 1819, Loss: 0.37648704648017883, Final Batch Loss: 0.2078040987253189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1820, Loss: 0.40354472398757935, Final Batch Loss: 0.21622468531131744\n",
      "Epoch 1821, Loss: 0.35126282274723053, Final Batch Loss: 0.1898331493139267\n",
      "Epoch 1822, Loss: 0.34393079578876495, Final Batch Loss: 0.18260376155376434\n",
      "Epoch 1823, Loss: 0.3329489827156067, Final Batch Loss: 0.18583005666732788\n",
      "Epoch 1824, Loss: 0.48041124641895294, Final Batch Loss: 0.2905656397342682\n",
      "Epoch 1825, Loss: 0.37399105727672577, Final Batch Loss: 0.18856415152549744\n",
      "Epoch 1826, Loss: 0.31742703914642334, Final Batch Loss: 0.1553351879119873\n",
      "Epoch 1827, Loss: 0.36449161171913147, Final Batch Loss: 0.17734049260616302\n",
      "Epoch 1828, Loss: 0.3345533460378647, Final Batch Loss: 0.14938335120677948\n",
      "Epoch 1829, Loss: 0.30901558697223663, Final Batch Loss: 0.16766920685768127\n",
      "Epoch 1830, Loss: 0.4077242910861969, Final Batch Loss: 0.23039938509464264\n",
      "Epoch 1831, Loss: 0.3540305644273758, Final Batch Loss: 0.16561047732830048\n",
      "Epoch 1832, Loss: 0.38871505856513977, Final Batch Loss: 0.2300015538930893\n",
      "Epoch 1833, Loss: 0.39246146380901337, Final Batch Loss: 0.2168113887310028\n",
      "Epoch 1834, Loss: 0.3171745836734772, Final Batch Loss: 0.1796862632036209\n",
      "Epoch 1835, Loss: 0.3581126183271408, Final Batch Loss: 0.18401691317558289\n",
      "Epoch 1836, Loss: 0.32521621882915497, Final Batch Loss: 0.16327592730522156\n",
      "Epoch 1837, Loss: 0.32114721834659576, Final Batch Loss: 0.15450724959373474\n",
      "Epoch 1838, Loss: 0.4247523099184036, Final Batch Loss: 0.24799814820289612\n",
      "Epoch 1839, Loss: 0.33707645535469055, Final Batch Loss: 0.146316796541214\n",
      "Epoch 1840, Loss: 0.3554515242576599, Final Batch Loss: 0.1788419783115387\n",
      "Epoch 1841, Loss: 0.4199265390634537, Final Batch Loss: 0.2679610848426819\n",
      "Epoch 1842, Loss: 0.3539484292268753, Final Batch Loss: 0.16674894094467163\n",
      "Epoch 1843, Loss: 0.39316563308238983, Final Batch Loss: 0.21313779056072235\n",
      "Epoch 1844, Loss: 0.34021246433258057, Final Batch Loss: 0.17487217485904694\n",
      "Epoch 1845, Loss: 0.31755003333091736, Final Batch Loss: 0.1754470318555832\n",
      "Epoch 1846, Loss: 0.396004319190979, Final Batch Loss: 0.23305438458919525\n",
      "Epoch 1847, Loss: 0.3097120076417923, Final Batch Loss: 0.13133014738559723\n",
      "Epoch 1848, Loss: 0.37675364315509796, Final Batch Loss: 0.19562554359436035\n",
      "Epoch 1849, Loss: 0.29630429297685623, Final Batch Loss: 0.11093803495168686\n",
      "Epoch 1850, Loss: 0.33956559002399445, Final Batch Loss: 0.1316705048084259\n",
      "Epoch 1851, Loss: 0.34129704535007477, Final Batch Loss: 0.1899740993976593\n",
      "Epoch 1852, Loss: 0.42092442512512207, Final Batch Loss: 0.22813643515110016\n",
      "Epoch 1853, Loss: 0.32234445214271545, Final Batch Loss: 0.1567399948835373\n",
      "Epoch 1854, Loss: 0.3627067655324936, Final Batch Loss: 0.18857313692569733\n",
      "Epoch 1855, Loss: 0.3699546456336975, Final Batch Loss: 0.1852508932352066\n",
      "Epoch 1856, Loss: 0.4080192297697067, Final Batch Loss: 0.14018477499485016\n",
      "Epoch 1857, Loss: 0.3135196268558502, Final Batch Loss: 0.10759201645851135\n",
      "Epoch 1858, Loss: 0.3366602882742882, Final Batch Loss: 0.10636664181947708\n",
      "Epoch 1859, Loss: 0.3465753048658371, Final Batch Loss: 0.16527031362056732\n",
      "Epoch 1860, Loss: 0.34829922020435333, Final Batch Loss: 0.18068496882915497\n",
      "Epoch 1861, Loss: 0.3799230605363846, Final Batch Loss: 0.20922142267227173\n",
      "Epoch 1862, Loss: 0.34640800952911377, Final Batch Loss: 0.18008233606815338\n",
      "Epoch 1863, Loss: 0.33415912091732025, Final Batch Loss: 0.1779165416955948\n",
      "Epoch 1864, Loss: 0.38142213225364685, Final Batch Loss: 0.2139900028705597\n",
      "Epoch 1865, Loss: 0.41864389181137085, Final Batch Loss: 0.2167157232761383\n",
      "Epoch 1866, Loss: 0.41724807024002075, Final Batch Loss: 0.3091602623462677\n",
      "Epoch 1867, Loss: 0.38027241826057434, Final Batch Loss: 0.18350283801555634\n",
      "Epoch 1868, Loss: 0.3217739909887314, Final Batch Loss: 0.14243784546852112\n",
      "Epoch 1869, Loss: 0.4500616192817688, Final Batch Loss: 0.256694495677948\n",
      "Epoch 1870, Loss: 0.31269578635692596, Final Batch Loss: 0.15679046511650085\n",
      "Epoch 1871, Loss: 0.40787218511104584, Final Batch Loss: 0.23400095105171204\n",
      "Epoch 1872, Loss: 0.419802725315094, Final Batch Loss: 0.21933792531490326\n",
      "Epoch 1873, Loss: 0.3014163672924042, Final Batch Loss: 0.0992831140756607\n",
      "Epoch 1874, Loss: 0.3340761363506317, Final Batch Loss: 0.14965467154979706\n",
      "Epoch 1875, Loss: 0.36186981201171875, Final Batch Loss: 0.15955787897109985\n",
      "Epoch 1876, Loss: 0.43679825961589813, Final Batch Loss: 0.21365854144096375\n",
      "Epoch 1877, Loss: 0.31354278326034546, Final Batch Loss: 0.17680624127388\n",
      "Epoch 1878, Loss: 0.31779295206069946, Final Batch Loss: 0.16884374618530273\n",
      "Epoch 1879, Loss: 0.35430608689785004, Final Batch Loss: 0.15166303515434265\n",
      "Epoch 1880, Loss: 0.3771990090608597, Final Batch Loss: 0.20434482395648956\n",
      "Epoch 1881, Loss: 0.3662155717611313, Final Batch Loss: 0.22377626597881317\n",
      "Epoch 1882, Loss: 0.39442557096481323, Final Batch Loss: 0.1915290355682373\n",
      "Epoch 1883, Loss: 0.3747803121805191, Final Batch Loss: 0.16385920345783234\n",
      "Epoch 1884, Loss: 0.348461776971817, Final Batch Loss: 0.19626805186271667\n",
      "Epoch 1885, Loss: 0.3538431376218796, Final Batch Loss: 0.18054595589637756\n",
      "Epoch 1886, Loss: 0.3422336280345917, Final Batch Loss: 0.17265886068344116\n",
      "Epoch 1887, Loss: 0.32697726786136627, Final Batch Loss: 0.17318487167358398\n",
      "Epoch 1888, Loss: 0.285774290561676, Final Batch Loss: 0.14431637525558472\n",
      "Epoch 1889, Loss: 0.38658301532268524, Final Batch Loss: 0.22805742919445038\n",
      "Epoch 1890, Loss: 0.37774112820625305, Final Batch Loss: 0.208880215883255\n",
      "Epoch 1891, Loss: 0.32402123510837555, Final Batch Loss: 0.16075542569160461\n",
      "Epoch 1892, Loss: 0.30746492743492126, Final Batch Loss: 0.164638951420784\n",
      "Epoch 1893, Loss: 0.3930214196443558, Final Batch Loss: 0.18839377164840698\n",
      "Epoch 1894, Loss: 0.35398687422275543, Final Batch Loss: 0.20363546907901764\n",
      "Epoch 1895, Loss: 0.3180142045021057, Final Batch Loss: 0.1629236787557602\n",
      "Epoch 1896, Loss: 0.41853608191013336, Final Batch Loss: 0.1997165083885193\n",
      "Epoch 1897, Loss: 0.39054451882839203, Final Batch Loss: 0.17924447357654572\n",
      "Epoch 1898, Loss: 0.4383244216442108, Final Batch Loss: 0.24451099336147308\n",
      "Epoch 1899, Loss: 0.359618604183197, Final Batch Loss: 0.14749862253665924\n",
      "Epoch 1900, Loss: 0.30679063498973846, Final Batch Loss: 0.1718340665102005\n",
      "Epoch 1901, Loss: 0.3232979029417038, Final Batch Loss: 0.15909090638160706\n",
      "Epoch 1902, Loss: 0.384296715259552, Final Batch Loss: 0.1863216757774353\n",
      "Epoch 1903, Loss: 0.3361339718103409, Final Batch Loss: 0.15006938576698303\n",
      "Epoch 1904, Loss: 0.3540445566177368, Final Batch Loss: 0.14701774716377258\n",
      "Epoch 1905, Loss: 0.25416169315576553, Final Batch Loss: 0.08953844755887985\n",
      "Epoch 1906, Loss: 0.35984043776988983, Final Batch Loss: 0.15817596018314362\n",
      "Epoch 1907, Loss: 0.3150477260351181, Final Batch Loss: 0.18024134635925293\n",
      "Epoch 1908, Loss: 0.3257417529821396, Final Batch Loss: 0.16368915140628815\n",
      "Epoch 1909, Loss: 0.3116017282009125, Final Batch Loss: 0.13294647634029388\n",
      "Epoch 1910, Loss: 0.24624957889318466, Final Batch Loss: 0.0754256471991539\n",
      "Epoch 1911, Loss: 0.33294832706451416, Final Batch Loss: 0.15041062235832214\n",
      "Epoch 1912, Loss: 0.29685112833976746, Final Batch Loss: 0.13387668132781982\n",
      "Epoch 1913, Loss: 0.3929968327283859, Final Batch Loss: 0.2344079315662384\n",
      "Epoch 1914, Loss: 0.3787267208099365, Final Batch Loss: 0.19374698400497437\n",
      "Epoch 1915, Loss: 0.35929298400878906, Final Batch Loss: 0.13709191977977753\n",
      "Epoch 1916, Loss: 0.34969934821128845, Final Batch Loss: 0.17511437833309174\n",
      "Epoch 1917, Loss: 0.3219698518514633, Final Batch Loss: 0.164738267660141\n",
      "Epoch 1918, Loss: 0.374380961060524, Final Batch Loss: 0.21571893990039825\n",
      "Epoch 1919, Loss: 0.2853458523750305, Final Batch Loss: 0.12747812271118164\n",
      "Epoch 1920, Loss: 0.2924862504005432, Final Batch Loss: 0.1188461035490036\n",
      "Epoch 1921, Loss: 0.2715841010212898, Final Batch Loss: 0.12033549696207047\n",
      "Epoch 1922, Loss: 0.3146601915359497, Final Batch Loss: 0.14966852962970734\n",
      "Epoch 1923, Loss: 0.31412652134895325, Final Batch Loss: 0.13661570847034454\n",
      "Epoch 1924, Loss: 0.32997263967990875, Final Batch Loss: 0.1821291744709015\n",
      "Epoch 1925, Loss: 0.2756957858800888, Final Batch Loss: 0.13269993662834167\n",
      "Epoch 1926, Loss: 0.30042168498039246, Final Batch Loss: 0.13274803757667542\n",
      "Epoch 1927, Loss: 0.32748444378376007, Final Batch Loss: 0.15478108823299408\n",
      "Epoch 1928, Loss: 0.3505867123603821, Final Batch Loss: 0.22754284739494324\n",
      "Epoch 1929, Loss: 0.3182806968688965, Final Batch Loss: 0.15425899624824524\n",
      "Epoch 1930, Loss: 0.37706276774406433, Final Batch Loss: 0.1893760710954666\n",
      "Epoch 1931, Loss: 0.3734019100666046, Final Batch Loss: 0.2032441943883896\n",
      "Epoch 1932, Loss: 0.30327296257019043, Final Batch Loss: 0.12573450803756714\n",
      "Epoch 1933, Loss: 0.3462742418050766, Final Batch Loss: 0.20510761439800262\n",
      "Epoch 1934, Loss: 0.30733226239681244, Final Batch Loss: 0.19499486684799194\n",
      "Epoch 1935, Loss: 0.2947548031806946, Final Batch Loss: 0.16709129512310028\n",
      "Epoch 1936, Loss: 0.3316568434238434, Final Batch Loss: 0.15777286887168884\n",
      "Epoch 1937, Loss: 0.33260662853717804, Final Batch Loss: 0.14981213212013245\n",
      "Epoch 1938, Loss: 0.26266874372959137, Final Batch Loss: 0.12737642228603363\n",
      "Epoch 1939, Loss: 0.3670920431613922, Final Batch Loss: 0.17856229841709137\n",
      "Epoch 1940, Loss: 0.313905730843544, Final Batch Loss: 0.13187728822231293\n",
      "Epoch 1941, Loss: 0.2820494920015335, Final Batch Loss: 0.11963678896427155\n",
      "Epoch 1942, Loss: 0.39954225718975067, Final Batch Loss: 0.24066239595413208\n",
      "Epoch 1943, Loss: 0.36633650958538055, Final Batch Loss: 0.21345753967761993\n",
      "Epoch 1944, Loss: 0.4260547012090683, Final Batch Loss: 0.21224388480186462\n",
      "Epoch 1945, Loss: 0.2773999944329262, Final Batch Loss: 0.11615566164255142\n",
      "Epoch 1946, Loss: 0.34475719928741455, Final Batch Loss: 0.2079630196094513\n",
      "Epoch 1947, Loss: 0.36597175896167755, Final Batch Loss: 0.1959826499223709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1948, Loss: 0.32278425991535187, Final Batch Loss: 0.1796172708272934\n",
      "Epoch 1949, Loss: 0.31180740147829056, Final Batch Loss: 0.12337540835142136\n",
      "Epoch 1950, Loss: 0.2922643721103668, Final Batch Loss: 0.13329996168613434\n",
      "Epoch 1951, Loss: 0.27171916514635086, Final Batch Loss: 0.1239098533987999\n",
      "Epoch 1952, Loss: 0.37706707417964935, Final Batch Loss: 0.17402538657188416\n",
      "Epoch 1953, Loss: 0.4370146244764328, Final Batch Loss: 0.21126703917980194\n",
      "Epoch 1954, Loss: 0.30927032232284546, Final Batch Loss: 0.160072460770607\n",
      "Epoch 1955, Loss: 0.29509516060352325, Final Batch Loss: 0.14962875843048096\n",
      "Epoch 1956, Loss: 0.30789215862751007, Final Batch Loss: 0.16035158932209015\n",
      "Epoch 1957, Loss: 0.3465922176837921, Final Batch Loss: 0.18801169097423553\n",
      "Epoch 1958, Loss: 0.33212487399578094, Final Batch Loss: 0.1713460236787796\n",
      "Epoch 1959, Loss: 0.4035222977399826, Final Batch Loss: 0.2155698984861374\n",
      "Epoch 1960, Loss: 0.30663633346557617, Final Batch Loss: 0.15267640352249146\n",
      "Epoch 1961, Loss: 0.30748450756073, Final Batch Loss: 0.1668986678123474\n",
      "Epoch 1962, Loss: 0.32400651276111603, Final Batch Loss: 0.1796378791332245\n",
      "Epoch 1963, Loss: 0.41724589467048645, Final Batch Loss: 0.23409700393676758\n",
      "Epoch 1964, Loss: 0.30284830927848816, Final Batch Loss: 0.15570281445980072\n",
      "Epoch 1965, Loss: 0.3400464504957199, Final Batch Loss: 0.1477929949760437\n",
      "Epoch 1966, Loss: 0.24689649045467377, Final Batch Loss: 0.1091250628232956\n",
      "Epoch 1967, Loss: 0.3672040104866028, Final Batch Loss: 0.20780134201049805\n",
      "Epoch 1968, Loss: 0.3122473508119583, Final Batch Loss: 0.173538938164711\n",
      "Epoch 1969, Loss: 0.29458993673324585, Final Batch Loss: 0.14146603643894196\n",
      "Epoch 1970, Loss: 0.35671141743659973, Final Batch Loss: 0.21007803082466125\n",
      "Epoch 1971, Loss: 0.408428892493248, Final Batch Loss: 0.202973410487175\n",
      "Epoch 1972, Loss: 0.365762323141098, Final Batch Loss: 0.18475687503814697\n",
      "Epoch 1973, Loss: 0.28967177867889404, Final Batch Loss: 0.10749176144599915\n",
      "Epoch 1974, Loss: 0.3568214178085327, Final Batch Loss: 0.19547729194164276\n",
      "Epoch 1975, Loss: 0.3175310790538788, Final Batch Loss: 0.17465174198150635\n",
      "Epoch 1976, Loss: 0.3453374654054642, Final Batch Loss: 0.17032206058502197\n",
      "Epoch 1977, Loss: 0.2787068784236908, Final Batch Loss: 0.10597598552703857\n",
      "Epoch 1978, Loss: 0.30705103278160095, Final Batch Loss: 0.16417522728443146\n",
      "Epoch 1979, Loss: 0.2982423007488251, Final Batch Loss: 0.14285749197006226\n",
      "Epoch 1980, Loss: 0.3006334751844406, Final Batch Loss: 0.12565673887729645\n",
      "Epoch 1981, Loss: 0.3500567078590393, Final Batch Loss: 0.14936766028404236\n",
      "Epoch 1982, Loss: 0.37228550016880035, Final Batch Loss: 0.21218951046466827\n",
      "Epoch 1983, Loss: 0.28846584260463715, Final Batch Loss: 0.10777829587459564\n",
      "Epoch 1984, Loss: 0.33046503365039825, Final Batch Loss: 0.16411161422729492\n",
      "Epoch 1985, Loss: 0.28630296140909195, Final Batch Loss: 0.11051595956087112\n",
      "Epoch 1986, Loss: 0.3369624614715576, Final Batch Loss: 0.19668938219547272\n",
      "Epoch 1987, Loss: 0.3766159564256668, Final Batch Loss: 0.22182554006576538\n",
      "Epoch 1988, Loss: 0.32692544162273407, Final Batch Loss: 0.15978007018566132\n",
      "Epoch 1989, Loss: 0.36889348924160004, Final Batch Loss: 0.1799401342868805\n",
      "Epoch 1990, Loss: 0.2725851535797119, Final Batch Loss: 0.14804737269878387\n",
      "Epoch 1991, Loss: 0.28290805220603943, Final Batch Loss: 0.131532222032547\n",
      "Epoch 1992, Loss: 0.2858625054359436, Final Batch Loss: 0.13718023896217346\n",
      "Epoch 1993, Loss: 0.3204754441976547, Final Batch Loss: 0.18802018463611603\n",
      "Epoch 1994, Loss: 0.3548067957162857, Final Batch Loss: 0.19670628011226654\n",
      "Epoch 1995, Loss: 0.31978945434093475, Final Batch Loss: 0.15523217618465424\n",
      "Epoch 1996, Loss: 0.3380204290151596, Final Batch Loss: 0.16564644873142242\n",
      "Epoch 1997, Loss: 0.3392045348882675, Final Batch Loss: 0.19877028465270996\n",
      "Epoch 1998, Loss: 0.3454870879650116, Final Batch Loss: 0.18246452510356903\n",
      "Epoch 1999, Loss: 0.30411726236343384, Final Batch Loss: 0.14502167701721191\n",
      "Epoch 2000, Loss: 0.31921079754829407, Final Batch Loss: 0.17330585420131683\n",
      "Epoch 2001, Loss: 0.3678569048643112, Final Batch Loss: 0.17383743822574615\n",
      "Epoch 2002, Loss: 0.3515677750110626, Final Batch Loss: 0.17615900933742523\n",
      "Epoch 2003, Loss: 0.3209381550550461, Final Batch Loss: 0.19158469140529633\n",
      "Epoch 2004, Loss: 0.31927940249443054, Final Batch Loss: 0.17215530574321747\n",
      "Epoch 2005, Loss: 0.32502083480358124, Final Batch Loss: 0.1488095074892044\n",
      "Epoch 2006, Loss: 0.3463062644004822, Final Batch Loss: 0.16554348170757294\n",
      "Epoch 2007, Loss: 0.3296907991170883, Final Batch Loss: 0.13020603358745575\n",
      "Epoch 2008, Loss: 0.37004712224006653, Final Batch Loss: 0.21655802428722382\n",
      "Epoch 2009, Loss: 0.28731054067611694, Final Batch Loss: 0.15410934388637543\n",
      "Epoch 2010, Loss: 0.2885161489248276, Final Batch Loss: 0.14075221121311188\n",
      "Epoch 2011, Loss: 0.3826068788766861, Final Batch Loss: 0.21320943534374237\n",
      "Epoch 2012, Loss: 0.26769670099020004, Final Batch Loss: 0.10003875941038132\n",
      "Epoch 2013, Loss: 0.28389444202184677, Final Batch Loss: 0.10100821405649185\n",
      "Epoch 2014, Loss: 0.34626708924770355, Final Batch Loss: 0.1872902363538742\n",
      "Epoch 2015, Loss: 0.2951066717505455, Final Batch Loss: 0.1002546176314354\n",
      "Epoch 2016, Loss: 0.3422809988260269, Final Batch Loss: 0.18877053260803223\n",
      "Epoch 2017, Loss: 0.3636787682771683, Final Batch Loss: 0.17611785233020782\n",
      "Epoch 2018, Loss: 0.35628941655158997, Final Batch Loss: 0.21054378151893616\n",
      "Epoch 2019, Loss: 0.26445240527391434, Final Batch Loss: 0.09534620493650436\n",
      "Epoch 2020, Loss: 0.2918410003185272, Final Batch Loss: 0.12636211514472961\n",
      "Epoch 2021, Loss: 0.29260221868753433, Final Batch Loss: 0.11892912536859512\n",
      "Epoch 2022, Loss: 0.38994963467121124, Final Batch Loss: 0.20937475562095642\n",
      "Epoch 2023, Loss: 0.34114016592502594, Final Batch Loss: 0.1809237152338028\n",
      "Epoch 2024, Loss: 0.33059997856616974, Final Batch Loss: 0.1375836730003357\n",
      "Epoch 2025, Loss: 0.24097881466150284, Final Batch Loss: 0.12198464572429657\n",
      "Epoch 2026, Loss: 0.29070010781288147, Final Batch Loss: 0.17148299515247345\n",
      "Epoch 2027, Loss: 0.2482004016637802, Final Batch Loss: 0.10966245830059052\n",
      "Epoch 2028, Loss: 0.25443168729543686, Final Batch Loss: 0.10925751179456711\n",
      "Epoch 2029, Loss: 0.26572270691394806, Final Batch Loss: 0.12716273963451385\n",
      "Epoch 2030, Loss: 0.3176862299442291, Final Batch Loss: 0.1491735279560089\n",
      "Epoch 2031, Loss: 0.3223877549171448, Final Batch Loss: 0.15094003081321716\n",
      "Epoch 2032, Loss: 0.2865738049149513, Final Batch Loss: 0.1796313226222992\n",
      "Epoch 2033, Loss: 0.2966877296566963, Final Batch Loss: 0.11794818192720413\n",
      "Epoch 2034, Loss: 0.27794621884822845, Final Batch Loss: 0.09499768912792206\n",
      "Epoch 2035, Loss: 0.35717469453811646, Final Batch Loss: 0.1773102879524231\n",
      "Epoch 2036, Loss: 0.37082235515117645, Final Batch Loss: 0.16237463057041168\n",
      "Epoch 2037, Loss: 0.32670898735523224, Final Batch Loss: 0.16014185547828674\n",
      "Epoch 2038, Loss: 0.3682063966989517, Final Batch Loss: 0.1832314282655716\n",
      "Epoch 2039, Loss: 0.39295218884944916, Final Batch Loss: 0.21784234046936035\n",
      "Epoch 2040, Loss: 0.33358265459537506, Final Batch Loss: 0.18266965448856354\n",
      "Epoch 2041, Loss: 0.3206849843263626, Final Batch Loss: 0.1686159074306488\n",
      "Epoch 2042, Loss: 0.3206498250365257, Final Batch Loss: 0.20431426167488098\n",
      "Epoch 2043, Loss: 0.29469597339630127, Final Batch Loss: 0.15523025393486023\n",
      "Epoch 2044, Loss: 0.35824722051620483, Final Batch Loss: 0.1652352660894394\n",
      "Epoch 2045, Loss: 0.38100041449069977, Final Batch Loss: 0.23754149675369263\n",
      "Epoch 2046, Loss: 0.3278568834066391, Final Batch Loss: 0.12611572444438934\n",
      "Epoch 2047, Loss: 0.3286895304918289, Final Batch Loss: 0.19725343585014343\n",
      "Epoch 2048, Loss: 0.27900271117687225, Final Batch Loss: 0.11375989019870758\n",
      "Epoch 2049, Loss: 0.34714533388614655, Final Batch Loss: 0.18634362518787384\n",
      "Epoch 2050, Loss: 0.3290751427412033, Final Batch Loss: 0.18653051555156708\n",
      "Epoch 2051, Loss: 0.31312571465969086, Final Batch Loss: 0.144845649600029\n",
      "Epoch 2052, Loss: 0.28781843185424805, Final Batch Loss: 0.1318703144788742\n",
      "Epoch 2053, Loss: 0.28469377756118774, Final Batch Loss: 0.15677034854888916\n",
      "Epoch 2054, Loss: 0.35314780473709106, Final Batch Loss: 0.1980111449956894\n",
      "Epoch 2055, Loss: 0.29321742057800293, Final Batch Loss: 0.1209900826215744\n",
      "Epoch 2056, Loss: 0.2740190774202347, Final Batch Loss: 0.11497700214385986\n",
      "Epoch 2057, Loss: 0.28271034359931946, Final Batch Loss: 0.10944487154483795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2058, Loss: 0.31487925350666046, Final Batch Loss: 0.14073528349399567\n",
      "Epoch 2059, Loss: 0.2955971211194992, Final Batch Loss: 0.1581663191318512\n",
      "Epoch 2060, Loss: 0.3815572261810303, Final Batch Loss: 0.22258904576301575\n",
      "Epoch 2061, Loss: 0.30582818388938904, Final Batch Loss: 0.15737922489643097\n",
      "Epoch 2062, Loss: 0.2574034407734871, Final Batch Loss: 0.14013805985450745\n",
      "Epoch 2063, Loss: 0.30021122097969055, Final Batch Loss: 0.1667717695236206\n",
      "Epoch 2064, Loss: 0.3687071353197098, Final Batch Loss: 0.154888316988945\n",
      "Epoch 2065, Loss: 0.370034396648407, Final Batch Loss: 0.20587128400802612\n",
      "Epoch 2066, Loss: 0.31755152344703674, Final Batch Loss: 0.17480731010437012\n",
      "Epoch 2067, Loss: 0.343373566865921, Final Batch Loss: 0.17709487676620483\n",
      "Epoch 2068, Loss: 0.25767478346824646, Final Batch Loss: 0.12807784974575043\n",
      "Epoch 2069, Loss: 0.32126501202583313, Final Batch Loss: 0.1370815932750702\n",
      "Epoch 2070, Loss: 0.3425438106060028, Final Batch Loss: 0.22547708451747894\n",
      "Epoch 2071, Loss: 0.3062705248594284, Final Batch Loss: 0.12959636747837067\n",
      "Epoch 2072, Loss: 0.3233189433813095, Final Batch Loss: 0.15086247026920319\n",
      "Epoch 2073, Loss: 0.28422385454177856, Final Batch Loss: 0.13506075739860535\n",
      "Epoch 2074, Loss: 0.3423605114221573, Final Batch Loss: 0.16568174958229065\n",
      "Epoch 2075, Loss: 0.2934991866350174, Final Batch Loss: 0.12788458168506622\n",
      "Epoch 2076, Loss: 0.26208943873643875, Final Batch Loss: 0.10659023374319077\n",
      "Epoch 2077, Loss: 0.3506171554327011, Final Batch Loss: 0.16725534200668335\n",
      "Epoch 2078, Loss: 0.29383139312267303, Final Batch Loss: 0.16392400860786438\n",
      "Epoch 2079, Loss: 0.3284965753555298, Final Batch Loss: 0.17495569586753845\n",
      "Epoch 2080, Loss: 0.3205704838037491, Final Batch Loss: 0.1495298594236374\n",
      "Epoch 2081, Loss: 0.3517059236764908, Final Batch Loss: 0.1626497358083725\n",
      "Epoch 2082, Loss: 0.3276677131652832, Final Batch Loss: 0.11462223529815674\n",
      "Epoch 2083, Loss: 0.2433340921998024, Final Batch Loss: 0.10211645811796188\n",
      "Epoch 2084, Loss: 0.3678768426179886, Final Batch Loss: 0.20619384944438934\n",
      "Epoch 2085, Loss: 0.27403587847948074, Final Batch Loss: 0.10237766057252884\n",
      "Epoch 2086, Loss: 0.4285064786672592, Final Batch Loss: 0.289806604385376\n",
      "Epoch 2087, Loss: 0.2661905586719513, Final Batch Loss: 0.12823353707790375\n",
      "Epoch 2088, Loss: 0.30038096010684967, Final Batch Loss: 0.1353350430727005\n",
      "Epoch 2089, Loss: 0.4210241436958313, Final Batch Loss: 0.24132280051708221\n",
      "Epoch 2090, Loss: 0.33773113787174225, Final Batch Loss: 0.14620892703533173\n",
      "Epoch 2091, Loss: 0.24632641673088074, Final Batch Loss: 0.13454939424991608\n",
      "Epoch 2092, Loss: 0.2635961025953293, Final Batch Loss: 0.10054680705070496\n",
      "Epoch 2093, Loss: 0.3077815771102905, Final Batch Loss: 0.17091989517211914\n",
      "Epoch 2094, Loss: 0.2625584006309509, Final Batch Loss: 0.07898746430873871\n",
      "Epoch 2095, Loss: 0.31108132004737854, Final Batch Loss: 0.14797045290470123\n",
      "Epoch 2096, Loss: 0.32279521226882935, Final Batch Loss: 0.12427891790866852\n",
      "Epoch 2097, Loss: 0.27659688889980316, Final Batch Loss: 0.14019586145877838\n",
      "Epoch 2098, Loss: 0.281245693564415, Final Batch Loss: 0.1318989396095276\n",
      "Epoch 2099, Loss: 0.2942584156990051, Final Batch Loss: 0.14651064574718475\n",
      "Epoch 2100, Loss: 0.3476465493440628, Final Batch Loss: 0.18983660638332367\n",
      "Epoch 2101, Loss: 0.32743002474308014, Final Batch Loss: 0.15790298581123352\n",
      "Epoch 2102, Loss: 0.31008297204971313, Final Batch Loss: 0.13652724027633667\n",
      "Epoch 2103, Loss: 0.3547275811433792, Final Batch Loss: 0.21713198721408844\n",
      "Epoch 2104, Loss: 0.2546026408672333, Final Batch Loss: 0.11789605021476746\n",
      "Epoch 2105, Loss: 0.36695343255996704, Final Batch Loss: 0.18363580107688904\n",
      "Epoch 2106, Loss: 0.2533160299062729, Final Batch Loss: 0.10874968767166138\n",
      "Epoch 2107, Loss: 0.28921957314014435, Final Batch Loss: 0.15469327569007874\n",
      "Epoch 2108, Loss: 0.24919011443853378, Final Batch Loss: 0.12194446474313736\n",
      "Epoch 2109, Loss: 0.34117811918258667, Final Batch Loss: 0.16294531524181366\n",
      "Epoch 2110, Loss: 0.3121839612722397, Final Batch Loss: 0.1923244297504425\n",
      "Epoch 2111, Loss: 0.3187858760356903, Final Batch Loss: 0.18343572318553925\n",
      "Epoch 2112, Loss: 0.2873419225215912, Final Batch Loss: 0.12780898809432983\n",
      "Epoch 2113, Loss: 0.2380809187889099, Final Batch Loss: 0.11685779690742493\n",
      "Epoch 2114, Loss: 0.3445514440536499, Final Batch Loss: 0.196454718708992\n",
      "Epoch 2115, Loss: 0.35378630459308624, Final Batch Loss: 0.15142115950584412\n",
      "Epoch 2116, Loss: 0.25828733295202255, Final Batch Loss: 0.0952843502163887\n",
      "Epoch 2117, Loss: 0.24050744622945786, Final Batch Loss: 0.11930316686630249\n",
      "Epoch 2118, Loss: 0.3030937761068344, Final Batch Loss: 0.13619865477085114\n",
      "Epoch 2119, Loss: 0.3056693822145462, Final Batch Loss: 0.13035733997821808\n",
      "Epoch 2120, Loss: 0.31905679404735565, Final Batch Loss: 0.15712352097034454\n",
      "Epoch 2121, Loss: 0.3615887388586998, Final Batch Loss: 0.24591533839702606\n",
      "Epoch 2122, Loss: 0.32286886870861053, Final Batch Loss: 0.14527584612369537\n",
      "Epoch 2123, Loss: 0.2721414342522621, Final Batch Loss: 0.11215534061193466\n",
      "Epoch 2124, Loss: 0.26445870846509933, Final Batch Loss: 0.11571525782346725\n",
      "Epoch 2125, Loss: 0.48347027599811554, Final Batch Loss: 0.2796761393547058\n",
      "Epoch 2126, Loss: 0.2652328237891197, Final Batch Loss: 0.16387112438678741\n",
      "Epoch 2127, Loss: 0.23664255440235138, Final Batch Loss: 0.1178220808506012\n",
      "Epoch 2128, Loss: 0.38583114743232727, Final Batch Loss: 0.22202594578266144\n",
      "Epoch 2129, Loss: 0.3218553215265274, Final Batch Loss: 0.190394327044487\n",
      "Epoch 2130, Loss: 0.2835549861192703, Final Batch Loss: 0.12459985911846161\n",
      "Epoch 2131, Loss: 0.2822866886854172, Final Batch Loss: 0.12637510895729065\n",
      "Epoch 2132, Loss: 0.26386359333992004, Final Batch Loss: 0.14633989334106445\n",
      "Epoch 2133, Loss: 0.24738804250955582, Final Batch Loss: 0.12741944193840027\n",
      "Epoch 2134, Loss: 0.22249147295951843, Final Batch Loss: 0.11326947063207626\n",
      "Epoch 2135, Loss: 0.4226082116365433, Final Batch Loss: 0.24200262129306793\n",
      "Epoch 2136, Loss: 0.2754293829202652, Final Batch Loss: 0.1439976841211319\n",
      "Epoch 2137, Loss: 0.2601175010204315, Final Batch Loss: 0.1254452019929886\n",
      "Epoch 2138, Loss: 0.25203631073236465, Final Batch Loss: 0.12964116036891937\n",
      "Epoch 2139, Loss: 0.30948784202337265, Final Batch Loss: 0.18687832355499268\n",
      "Epoch 2140, Loss: 0.28946617245674133, Final Batch Loss: 0.1443554311990738\n",
      "Epoch 2141, Loss: 0.3467206656932831, Final Batch Loss: 0.20267237722873688\n",
      "Epoch 2142, Loss: 0.28968124091625214, Final Batch Loss: 0.1323327273130417\n",
      "Epoch 2143, Loss: 0.27590498328208923, Final Batch Loss: 0.1426677405834198\n",
      "Epoch 2144, Loss: 0.30091218650341034, Final Batch Loss: 0.14987418055534363\n",
      "Epoch 2145, Loss: 0.2591417282819748, Final Batch Loss: 0.13289354741573334\n",
      "Epoch 2146, Loss: 0.2963237911462784, Final Batch Loss: 0.16900937259197235\n",
      "Epoch 2147, Loss: 0.23548126965761185, Final Batch Loss: 0.13391360640525818\n",
      "Epoch 2148, Loss: 0.33848363161087036, Final Batch Loss: 0.19097721576690674\n",
      "Epoch 2149, Loss: 0.2931276857852936, Final Batch Loss: 0.1514466404914856\n",
      "Epoch 2150, Loss: 0.2649120092391968, Final Batch Loss: 0.1504388302564621\n",
      "Epoch 2151, Loss: 0.22766858339309692, Final Batch Loss: 0.08683165907859802\n",
      "Epoch 2152, Loss: 0.2931017279624939, Final Batch Loss: 0.13346019387245178\n",
      "Epoch 2153, Loss: 0.3401508033275604, Final Batch Loss: 0.1787634640932083\n",
      "Epoch 2154, Loss: 0.25768403708934784, Final Batch Loss: 0.10816819965839386\n",
      "Epoch 2155, Loss: 0.26409436017274857, Final Batch Loss: 0.15151533484458923\n",
      "Epoch 2156, Loss: 0.23448120057582855, Final Batch Loss: 0.0899803638458252\n",
      "Epoch 2157, Loss: 0.28511596471071243, Final Batch Loss: 0.12054737657308578\n",
      "Epoch 2158, Loss: 0.2702220529317856, Final Batch Loss: 0.13048875331878662\n",
      "Epoch 2159, Loss: 0.2779975011944771, Final Batch Loss: 0.16291984915733337\n",
      "Epoch 2160, Loss: 0.28529907763004303, Final Batch Loss: 0.14633223414421082\n",
      "Epoch 2161, Loss: 0.3690032958984375, Final Batch Loss: 0.22149397432804108\n",
      "Epoch 2162, Loss: 0.2982695400714874, Final Batch Loss: 0.1648273468017578\n",
      "Epoch 2163, Loss: 0.3542468100786209, Final Batch Loss: 0.16764891147613525\n",
      "Epoch 2164, Loss: 0.3099483251571655, Final Batch Loss: 0.1352607011795044\n",
      "Epoch 2165, Loss: 0.27629804611206055, Final Batch Loss: 0.14335529506206512\n",
      "Epoch 2166, Loss: 0.26462556421756744, Final Batch Loss: 0.15027832984924316\n",
      "Epoch 2167, Loss: 0.2858830466866493, Final Batch Loss: 0.1682283580303192\n",
      "Epoch 2168, Loss: 0.2714371532201767, Final Batch Loss: 0.13338439166545868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2169, Loss: 0.24887855350971222, Final Batch Loss: 0.13397735357284546\n",
      "Epoch 2170, Loss: 0.34313876926898956, Final Batch Loss: 0.20340894162654877\n",
      "Epoch 2171, Loss: 0.26777131855487823, Final Batch Loss: 0.16274340450763702\n",
      "Epoch 2172, Loss: 0.26234252750873566, Final Batch Loss: 0.1383163034915924\n",
      "Epoch 2173, Loss: 0.25637389719486237, Final Batch Loss: 0.12244521081447601\n",
      "Epoch 2174, Loss: 0.4115205705165863, Final Batch Loss: 0.21635624766349792\n",
      "Epoch 2175, Loss: 0.26410531997680664, Final Batch Loss: 0.1308794468641281\n",
      "Epoch 2176, Loss: 0.29385389387607574, Final Batch Loss: 0.12842048704624176\n",
      "Epoch 2177, Loss: 0.27935172617435455, Final Batch Loss: 0.09604139626026154\n",
      "Epoch 2178, Loss: 0.27303264290094376, Final Batch Loss: 0.12469000369310379\n",
      "Epoch 2179, Loss: 0.24821096658706665, Final Batch Loss: 0.1217978447675705\n",
      "Epoch 2180, Loss: 0.22037797421216965, Final Batch Loss: 0.0846211239695549\n",
      "Epoch 2181, Loss: 0.3045318275690079, Final Batch Loss: 0.1507071703672409\n",
      "Epoch 2182, Loss: 0.3019804507493973, Final Batch Loss: 0.1465984284877777\n",
      "Epoch 2183, Loss: 0.277458131313324, Final Batch Loss: 0.17047396302223206\n",
      "Epoch 2184, Loss: 0.32640574872493744, Final Batch Loss: 0.15069875121116638\n",
      "Epoch 2185, Loss: 0.3043265789747238, Final Batch Loss: 0.1429876983165741\n",
      "Epoch 2186, Loss: 0.3058491349220276, Final Batch Loss: 0.15044108033180237\n",
      "Epoch 2187, Loss: 0.3712761253118515, Final Batch Loss: 0.21358156204223633\n",
      "Epoch 2188, Loss: 0.2950260788202286, Final Batch Loss: 0.15171170234680176\n",
      "Epoch 2189, Loss: 0.2918599843978882, Final Batch Loss: 0.12127362191677094\n",
      "Epoch 2190, Loss: 0.2613123804330826, Final Batch Loss: 0.12358471751213074\n",
      "Epoch 2191, Loss: 0.2998955100774765, Final Batch Loss: 0.14044736325740814\n",
      "Epoch 2192, Loss: 0.29144829511642456, Final Batch Loss: 0.12587925791740417\n",
      "Epoch 2193, Loss: 0.20773793011903763, Final Batch Loss: 0.08253424614667892\n",
      "Epoch 2194, Loss: 0.4150940328836441, Final Batch Loss: 0.24208076298236847\n",
      "Epoch 2195, Loss: 0.2833150327205658, Final Batch Loss: 0.14702165126800537\n",
      "Epoch 2196, Loss: 0.230641171336174, Final Batch Loss: 0.10090705752372742\n",
      "Epoch 2197, Loss: 0.293573722243309, Final Batch Loss: 0.11550873517990112\n",
      "Epoch 2198, Loss: 0.30536694824695587, Final Batch Loss: 0.146291121840477\n",
      "Epoch 2199, Loss: 0.29481588304042816, Final Batch Loss: 0.1680050492286682\n",
      "Epoch 2200, Loss: 0.32258474826812744, Final Batch Loss: 0.14972911775112152\n",
      "Epoch 2201, Loss: 0.2500985339283943, Final Batch Loss: 0.12586985528469086\n",
      "Epoch 2202, Loss: 0.2903580963611603, Final Batch Loss: 0.13283205032348633\n",
      "Epoch 2203, Loss: 0.28488051891326904, Final Batch Loss: 0.13694441318511963\n",
      "Epoch 2204, Loss: 0.34337857365608215, Final Batch Loss: 0.19666165113449097\n",
      "Epoch 2205, Loss: 0.2738983482122421, Final Batch Loss: 0.1295691728591919\n",
      "Epoch 2206, Loss: 0.2799789011478424, Final Batch Loss: 0.10415217280387878\n",
      "Epoch 2207, Loss: 0.22266144305467606, Final Batch Loss: 0.11298897117376328\n",
      "Epoch 2208, Loss: 0.19465170800685883, Final Batch Loss: 0.11348693817853928\n",
      "Epoch 2209, Loss: 0.2947588562965393, Final Batch Loss: 0.15764109790325165\n",
      "Epoch 2210, Loss: 0.34222638607025146, Final Batch Loss: 0.1967051923274994\n",
      "Epoch 2211, Loss: 0.26436975598335266, Final Batch Loss: 0.16402322053909302\n",
      "Epoch 2212, Loss: 0.30317775160074234, Final Batch Loss: 0.12011613696813583\n",
      "Epoch 2213, Loss: 0.20968889445066452, Final Batch Loss: 0.11415901780128479\n",
      "Epoch 2214, Loss: 0.33519919216632843, Final Batch Loss: 0.1713748276233673\n",
      "Epoch 2215, Loss: 0.2061072513461113, Final Batch Loss: 0.0939357802271843\n",
      "Epoch 2216, Loss: 0.26517948508262634, Final Batch Loss: 0.13794280588626862\n",
      "Epoch 2217, Loss: 0.2534617632627487, Final Batch Loss: 0.1107889860868454\n",
      "Epoch 2218, Loss: 0.2301122397184372, Final Batch Loss: 0.11907079070806503\n",
      "Epoch 2219, Loss: 0.2643066272139549, Final Batch Loss: 0.12344963103532791\n",
      "Epoch 2220, Loss: 0.3434530198574066, Final Batch Loss: 0.16233982145786285\n",
      "Epoch 2221, Loss: 0.2853059321641922, Final Batch Loss: 0.16987061500549316\n",
      "Epoch 2222, Loss: 0.3105523884296417, Final Batch Loss: 0.157928928732872\n",
      "Epoch 2223, Loss: 0.2827468663454056, Final Batch Loss: 0.15604612231254578\n",
      "Epoch 2224, Loss: 0.3671697974205017, Final Batch Loss: 0.21170106530189514\n",
      "Epoch 2225, Loss: 0.25924544036388397, Final Batch Loss: 0.11023518443107605\n",
      "Epoch 2226, Loss: 0.2908245697617531, Final Batch Loss: 0.10985135287046432\n",
      "Epoch 2227, Loss: 0.29701289534568787, Final Batch Loss: 0.14375297725200653\n",
      "Epoch 2228, Loss: 0.27303051948547363, Final Batch Loss: 0.13991127908229828\n",
      "Epoch 2229, Loss: 0.31100766360759735, Final Batch Loss: 0.15704642236232758\n",
      "Epoch 2230, Loss: 0.30539676547050476, Final Batch Loss: 0.16836656630039215\n",
      "Epoch 2231, Loss: 0.27561844140291214, Final Batch Loss: 0.15209101140499115\n",
      "Epoch 2232, Loss: 0.32770581543445587, Final Batch Loss: 0.15300601720809937\n",
      "Epoch 2233, Loss: 0.28270819783210754, Final Batch Loss: 0.12572692334651947\n",
      "Epoch 2234, Loss: 0.2856450527906418, Final Batch Loss: 0.1337965577840805\n",
      "Epoch 2235, Loss: 0.28999804705381393, Final Batch Loss: 0.17369645833969116\n",
      "Epoch 2236, Loss: 0.3617652952671051, Final Batch Loss: 0.21767491102218628\n",
      "Epoch 2237, Loss: 0.3027000427246094, Final Batch Loss: 0.16301964223384857\n",
      "Epoch 2238, Loss: 0.2300109937787056, Final Batch Loss: 0.08090243488550186\n",
      "Epoch 2239, Loss: 0.2247518077492714, Final Batch Loss: 0.11959167569875717\n",
      "Epoch 2240, Loss: 0.37382030487060547, Final Batch Loss: 0.2478494495153427\n",
      "Epoch 2241, Loss: 0.34201250970363617, Final Batch Loss: 0.16474021971225739\n",
      "Epoch 2242, Loss: 0.2360614687204361, Final Batch Loss: 0.11655191332101822\n",
      "Epoch 2243, Loss: 0.39153577387332916, Final Batch Loss: 0.19544783234596252\n",
      "Epoch 2244, Loss: 0.31418754160404205, Final Batch Loss: 0.15034635365009308\n",
      "Epoch 2245, Loss: 0.2697457820177078, Final Batch Loss: 0.1305515468120575\n",
      "Epoch 2246, Loss: 0.22778689116239548, Final Batch Loss: 0.10547716170549393\n",
      "Epoch 2247, Loss: 0.2728525251150131, Final Batch Loss: 0.12877830862998962\n",
      "Epoch 2248, Loss: 0.2853732407093048, Final Batch Loss: 0.1061638742685318\n",
      "Epoch 2249, Loss: 0.3462652862071991, Final Batch Loss: 0.20058007538318634\n",
      "Epoch 2250, Loss: 0.28978271782398224, Final Batch Loss: 0.14612719416618347\n",
      "Epoch 2251, Loss: 0.3830692321062088, Final Batch Loss: 0.23623433709144592\n",
      "Epoch 2252, Loss: 0.2714487612247467, Final Batch Loss: 0.1355094462633133\n",
      "Epoch 2253, Loss: 0.3139118552207947, Final Batch Loss: 0.16748832166194916\n",
      "Epoch 2254, Loss: 0.2377888336777687, Final Batch Loss: 0.10422392934560776\n",
      "Epoch 2255, Loss: 0.2665485143661499, Final Batch Loss: 0.139639213681221\n",
      "Epoch 2256, Loss: 0.3914853632450104, Final Batch Loss: 0.24299629032611847\n",
      "Epoch 2257, Loss: 0.28634242713451385, Final Batch Loss: 0.13105320930480957\n",
      "Epoch 2258, Loss: 0.21451900154352188, Final Batch Loss: 0.12016010284423828\n",
      "Epoch 2259, Loss: 0.2814851850271225, Final Batch Loss: 0.09780468046665192\n",
      "Epoch 2260, Loss: 0.3270204961299896, Final Batch Loss: 0.1686972975730896\n",
      "Epoch 2261, Loss: 0.266850009560585, Final Batch Loss: 0.1371479034423828\n",
      "Epoch 2262, Loss: 0.29622218757867813, Final Batch Loss: 0.17452232539653778\n",
      "Epoch 2263, Loss: 0.31618259847164154, Final Batch Loss: 0.15028734505176544\n",
      "Epoch 2264, Loss: 0.2798171937465668, Final Batch Loss: 0.13978034257888794\n",
      "Epoch 2265, Loss: 0.2636762037873268, Final Batch Loss: 0.14621782302856445\n",
      "Epoch 2266, Loss: 0.34661799669265747, Final Batch Loss: 0.19930978119373322\n",
      "Epoch 2267, Loss: 0.2702226862311363, Final Batch Loss: 0.15826699137687683\n",
      "Epoch 2268, Loss: 0.3407449275255203, Final Batch Loss: 0.21667173504829407\n",
      "Epoch 2269, Loss: 0.22109904885292053, Final Batch Loss: 0.08894181251525879\n",
      "Epoch 2270, Loss: 0.3093351870775223, Final Batch Loss: 0.15812361240386963\n",
      "Epoch 2271, Loss: 0.3267405927181244, Final Batch Loss: 0.17829124629497528\n",
      "Epoch 2272, Loss: 0.31951285898685455, Final Batch Loss: 0.1814604550600052\n",
      "Epoch 2273, Loss: 0.2656247541308403, Final Batch Loss: 0.10375245660543442\n",
      "Epoch 2274, Loss: 0.21347034722566605, Final Batch Loss: 0.10456633567810059\n",
      "Epoch 2275, Loss: 0.23829878866672516, Final Batch Loss: 0.10455265641212463\n",
      "Epoch 2276, Loss: 0.3949628323316574, Final Batch Loss: 0.20161034166812897\n",
      "Epoch 2277, Loss: 0.3103325664997101, Final Batch Loss: 0.15660494565963745\n",
      "Epoch 2278, Loss: 0.24464260041713715, Final Batch Loss: 0.13002124428749084\n",
      "Epoch 2279, Loss: 0.2769617885351181, Final Batch Loss: 0.11768950521945953\n",
      "Epoch 2280, Loss: 0.27922211587429047, Final Batch Loss: 0.10976310074329376\n",
      "Epoch 2281, Loss: 0.34020158648490906, Final Batch Loss: 0.17103974521160126\n",
      "Epoch 2282, Loss: 0.33730094134807587, Final Batch Loss: 0.152536079287529\n",
      "Epoch 2283, Loss: 0.24934203922748566, Final Batch Loss: 0.1443236619234085\n",
      "Epoch 2284, Loss: 0.2992663085460663, Final Batch Loss: 0.16260510683059692\n",
      "Epoch 2285, Loss: 0.32824376225471497, Final Batch Loss: 0.1886376589536667\n",
      "Epoch 2286, Loss: 0.2455645054578781, Final Batch Loss: 0.11372837424278259\n",
      "Epoch 2287, Loss: 0.3254469782114029, Final Batch Loss: 0.12813548743724823\n",
      "Epoch 2288, Loss: 0.22858230769634247, Final Batch Loss: 0.08295708894729614\n",
      "Epoch 2289, Loss: 0.31144117563962936, Final Batch Loss: 0.12275270372629166\n",
      "Epoch 2290, Loss: 0.3790424317121506, Final Batch Loss: 0.2507622539997101\n",
      "Epoch 2291, Loss: 0.2565094083547592, Final Batch Loss: 0.14753273129463196\n",
      "Epoch 2292, Loss: 0.22281818091869354, Final Batch Loss: 0.08930464088916779\n",
      "Epoch 2293, Loss: 0.2802103981375694, Final Batch Loss: 0.12018563598394394\n",
      "Epoch 2294, Loss: 0.30544623732566833, Final Batch Loss: 0.16013260185718536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2295, Loss: 0.34453926980495453, Final Batch Loss: 0.13824833929538727\n",
      "Epoch 2296, Loss: 0.19007416814565659, Final Batch Loss: 0.06881041079759598\n",
      "Epoch 2297, Loss: 0.3256434500217438, Final Batch Loss: 0.1699754148721695\n",
      "Epoch 2298, Loss: 0.23713141679763794, Final Batch Loss: 0.12597784399986267\n",
      "Epoch 2299, Loss: 0.3068309873342514, Final Batch Loss: 0.17927569150924683\n",
      "Epoch 2300, Loss: 0.35025791823863983, Final Batch Loss: 0.1628911793231964\n",
      "Epoch 2301, Loss: 0.3469020575284958, Final Batch Loss: 0.17787222564220428\n",
      "Epoch 2302, Loss: 0.2886764407157898, Final Batch Loss: 0.12799127399921417\n",
      "Epoch 2303, Loss: 0.2317199930548668, Final Batch Loss: 0.1285744607448578\n",
      "Epoch 2304, Loss: 0.2743988186120987, Final Batch Loss: 0.16798433661460876\n",
      "Epoch 2305, Loss: 0.304387167096138, Final Batch Loss: 0.15120476484298706\n",
      "Epoch 2306, Loss: 0.23528047651052475, Final Batch Loss: 0.09297821670770645\n",
      "Epoch 2307, Loss: 0.3093544691801071, Final Batch Loss: 0.1266329139471054\n",
      "Epoch 2308, Loss: 0.24702215939760208, Final Batch Loss: 0.10027693957090378\n",
      "Epoch 2309, Loss: 0.35765351355075836, Final Batch Loss: 0.13654758036136627\n",
      "Epoch 2310, Loss: 0.33353108167648315, Final Batch Loss: 0.15271520614624023\n",
      "Epoch 2311, Loss: 0.2622592970728874, Final Batch Loss: 0.14173468947410583\n",
      "Epoch 2312, Loss: 0.2817283123731613, Final Batch Loss: 0.1464281678199768\n",
      "Epoch 2313, Loss: 0.2194298580288887, Final Batch Loss: 0.1021551713347435\n",
      "Epoch 2314, Loss: 0.2803577706217766, Final Batch Loss: 0.16136889159679413\n",
      "Epoch 2315, Loss: 0.33017124980688095, Final Batch Loss: 0.2153387814760208\n",
      "Epoch 2316, Loss: 0.26694003492593765, Final Batch Loss: 0.10681625455617905\n",
      "Epoch 2317, Loss: 0.31350068747997284, Final Batch Loss: 0.1770983785390854\n",
      "Epoch 2318, Loss: 0.25574634969234467, Final Batch Loss: 0.11026641726493835\n",
      "Epoch 2319, Loss: 0.307166188955307, Final Batch Loss: 0.17837224900722504\n",
      "Epoch 2320, Loss: 0.30598922073841095, Final Batch Loss: 0.1384117156267166\n",
      "Epoch 2321, Loss: 0.28932347148656845, Final Batch Loss: 0.11188261955976486\n",
      "Epoch 2322, Loss: 0.26660382747650146, Final Batch Loss: 0.11170542240142822\n",
      "Epoch 2323, Loss: 0.2991037964820862, Final Batch Loss: 0.1725519299507141\n",
      "Epoch 2324, Loss: 0.2818317413330078, Final Batch Loss: 0.11468052864074707\n",
      "Epoch 2325, Loss: 0.2581767812371254, Final Batch Loss: 0.11787309497594833\n",
      "Epoch 2326, Loss: 0.2912415787577629, Final Batch Loss: 0.17021721601486206\n",
      "Epoch 2327, Loss: 0.23635704070329666, Final Batch Loss: 0.09176938980817795\n",
      "Epoch 2328, Loss: 0.27081602066755295, Final Batch Loss: 0.15638461709022522\n",
      "Epoch 2329, Loss: 0.26763689517974854, Final Batch Loss: 0.134421706199646\n",
      "Epoch 2330, Loss: 0.22927195578813553, Final Batch Loss: 0.10560734570026398\n",
      "Epoch 2331, Loss: 0.28563179820775986, Final Batch Loss: 0.17275121808052063\n",
      "Epoch 2332, Loss: 0.3043948858976364, Final Batch Loss: 0.1657276749610901\n",
      "Epoch 2333, Loss: 0.31151075661182404, Final Batch Loss: 0.16979461908340454\n",
      "Epoch 2334, Loss: 0.2819660007953644, Final Batch Loss: 0.1282363086938858\n",
      "Epoch 2335, Loss: 0.37419393658638, Final Batch Loss: 0.22526322305202484\n",
      "Epoch 2336, Loss: 0.31863050162792206, Final Batch Loss: 0.14922529458999634\n",
      "Epoch 2337, Loss: 0.2696748375892639, Final Batch Loss: 0.15096241235733032\n",
      "Epoch 2338, Loss: 0.3236751854419708, Final Batch Loss: 0.1770162135362625\n",
      "Epoch 2339, Loss: 0.2984718978404999, Final Batch Loss: 0.13398633897304535\n",
      "Epoch 2340, Loss: 0.2620670273900032, Final Batch Loss: 0.12223043292760849\n",
      "Epoch 2341, Loss: 0.2115107849240303, Final Batch Loss: 0.11471373587846756\n",
      "Epoch 2342, Loss: 0.4162513017654419, Final Batch Loss: 0.22059379518032074\n",
      "Epoch 2343, Loss: 0.2168024629354477, Final Batch Loss: 0.1260487139225006\n",
      "Epoch 2344, Loss: 0.35923178493976593, Final Batch Loss: 0.18481236696243286\n",
      "Epoch 2345, Loss: 0.2641947269439697, Final Batch Loss: 0.12229956686496735\n",
      "Epoch 2346, Loss: 0.24557562917470932, Final Batch Loss: 0.11500202864408493\n",
      "Epoch 2347, Loss: 0.3528035134077072, Final Batch Loss: 0.18696947395801544\n",
      "Epoch 2348, Loss: 0.41418224573135376, Final Batch Loss: 0.25003430247306824\n",
      "Epoch 2349, Loss: 0.22087255865335464, Final Batch Loss: 0.09388627856969833\n",
      "Epoch 2350, Loss: 0.3670956641435623, Final Batch Loss: 0.2157563418149948\n",
      "Epoch 2351, Loss: 0.2698805630207062, Final Batch Loss: 0.129569411277771\n",
      "Epoch 2352, Loss: 0.30452804267406464, Final Batch Loss: 0.1474500596523285\n",
      "Epoch 2353, Loss: 0.3103305399417877, Final Batch Loss: 0.15962208807468414\n",
      "Epoch 2354, Loss: 0.22783996909856796, Final Batch Loss: 0.07795845717191696\n",
      "Epoch 2355, Loss: 0.2178913876414299, Final Batch Loss: 0.09704028069972992\n",
      "Epoch 2356, Loss: 0.22603297233581543, Final Batch Loss: 0.10075657069683075\n",
      "Epoch 2357, Loss: 0.25582273304462433, Final Batch Loss: 0.08085910975933075\n",
      "Epoch 2358, Loss: 0.2855347916483879, Final Batch Loss: 0.10879462212324142\n",
      "Epoch 2359, Loss: 0.27567338198423386, Final Batch Loss: 0.15598560869693756\n",
      "Epoch 2360, Loss: 0.3000660836696625, Final Batch Loss: 0.1220761388540268\n",
      "Epoch 2361, Loss: 0.24198106676340103, Final Batch Loss: 0.12316592037677765\n",
      "Epoch 2362, Loss: 0.24257318675518036, Final Batch Loss: 0.09528757631778717\n",
      "Epoch 2363, Loss: 0.285839319229126, Final Batch Loss: 0.15745162963867188\n",
      "Epoch 2364, Loss: 0.27800440043210983, Final Batch Loss: 0.1601579636335373\n",
      "Epoch 2365, Loss: 0.2790347635746002, Final Batch Loss: 0.16113732755184174\n",
      "Epoch 2366, Loss: 0.2413889318704605, Final Batch Loss: 0.15020501613616943\n",
      "Epoch 2367, Loss: 0.2534375339746475, Final Batch Loss: 0.11286678910255432\n",
      "Epoch 2368, Loss: 0.31030675768852234, Final Batch Loss: 0.1544899046421051\n",
      "Epoch 2369, Loss: 0.27682340145111084, Final Batch Loss: 0.10966433584690094\n",
      "Epoch 2370, Loss: 0.2379203364253044, Final Batch Loss: 0.1063774898648262\n",
      "Epoch 2371, Loss: 0.26502829790115356, Final Batch Loss: 0.1451241374015808\n",
      "Epoch 2372, Loss: 0.38027825951576233, Final Batch Loss: 0.17110586166381836\n",
      "Epoch 2373, Loss: 0.28522399067878723, Final Batch Loss: 0.15323007106781006\n",
      "Epoch 2374, Loss: 0.31675685942173004, Final Batch Loss: 0.19779770076274872\n",
      "Epoch 2375, Loss: 0.30147457122802734, Final Batch Loss: 0.15294325351715088\n",
      "Epoch 2376, Loss: 0.22926703095436096, Final Batch Loss: 0.10013361275196075\n",
      "Epoch 2377, Loss: 0.2770976722240448, Final Batch Loss: 0.1798386126756668\n",
      "Epoch 2378, Loss: 0.36562376469373703, Final Batch Loss: 0.2413385957479477\n",
      "Epoch 2379, Loss: 0.2442467212677002, Final Batch Loss: 0.11669182777404785\n",
      "Epoch 2380, Loss: 0.2596929669380188, Final Batch Loss: 0.13158588111400604\n",
      "Epoch 2381, Loss: 0.2471981644630432, Final Batch Loss: 0.12965045869350433\n",
      "Epoch 2382, Loss: 0.2734588086605072, Final Batch Loss: 0.1264237016439438\n",
      "Epoch 2383, Loss: 0.21728989481925964, Final Batch Loss: 0.0950932577252388\n",
      "Epoch 2384, Loss: 0.27147648483514786, Final Batch Loss: 0.11322133988142014\n",
      "Epoch 2385, Loss: 0.30020205676555634, Final Batch Loss: 0.1436067372560501\n",
      "Epoch 2386, Loss: 0.31898611783981323, Final Batch Loss: 0.16934292018413544\n",
      "Epoch 2387, Loss: 0.22488364577293396, Final Batch Loss: 0.1327519416809082\n",
      "Epoch 2388, Loss: 0.23019126057624817, Final Batch Loss: 0.13134250044822693\n",
      "Epoch 2389, Loss: 0.2656856179237366, Final Batch Loss: 0.09122008085250854\n",
      "Epoch 2390, Loss: 0.3129650503396988, Final Batch Loss: 0.16219697892665863\n",
      "Epoch 2391, Loss: 0.25248581916093826, Final Batch Loss: 0.1222115233540535\n",
      "Epoch 2392, Loss: 0.20714589208364487, Final Batch Loss: 0.07183104008436203\n",
      "Epoch 2393, Loss: 0.32279811799526215, Final Batch Loss: 0.16378574073314667\n",
      "Epoch 2394, Loss: 0.2116728350520134, Final Batch Loss: 0.1064557358622551\n",
      "Epoch 2395, Loss: 0.2907593548297882, Final Batch Loss: 0.19449445605278015\n",
      "Epoch 2396, Loss: 0.2336643934249878, Final Batch Loss: 0.1301165223121643\n",
      "Epoch 2397, Loss: 0.2676065117120743, Final Batch Loss: 0.11471477150917053\n",
      "Epoch 2398, Loss: 0.20376716554164886, Final Batch Loss: 0.09196870774030685\n",
      "Epoch 2399, Loss: 0.25131501257419586, Final Batch Loss: 0.15797267854213715\n",
      "Epoch 2400, Loss: 0.22463230043649673, Final Batch Loss: 0.12398619204759598\n",
      "Epoch 2401, Loss: 0.2547323703765869, Final Batch Loss: 0.13534212112426758\n",
      "Epoch 2402, Loss: 0.29263511300086975, Final Batch Loss: 0.13635815680027008\n",
      "Epoch 2403, Loss: 0.33456017076969147, Final Batch Loss: 0.20181633532047272\n",
      "Epoch 2404, Loss: 0.23600134253501892, Final Batch Loss: 0.12533500790596008\n",
      "Epoch 2405, Loss: 0.286233052611351, Final Batch Loss: 0.10736958682537079\n",
      "Epoch 2406, Loss: 0.18734990805387497, Final Batch Loss: 0.07527268677949905\n",
      "Epoch 2407, Loss: 0.25535865873098373, Final Batch Loss: 0.12274285405874252\n",
      "Epoch 2408, Loss: 0.2364284247159958, Final Batch Loss: 0.13203316926956177\n",
      "Epoch 2409, Loss: 0.19925599545240402, Final Batch Loss: 0.0787518098950386\n",
      "Epoch 2410, Loss: 0.31454966962337494, Final Batch Loss: 0.2013433277606964\n",
      "Epoch 2411, Loss: 0.25740615278482437, Final Batch Loss: 0.11463432759046555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2412, Loss: 0.2771252691745758, Final Batch Loss: 0.16119012236595154\n",
      "Epoch 2413, Loss: 0.2741687074303627, Final Batch Loss: 0.12273765355348587\n",
      "Epoch 2414, Loss: 0.2783080190420151, Final Batch Loss: 0.12004455924034119\n",
      "Epoch 2415, Loss: 0.2640484794974327, Final Batch Loss: 0.11848180741071701\n",
      "Epoch 2416, Loss: 0.2945750057697296, Final Batch Loss: 0.13943518698215485\n",
      "Epoch 2417, Loss: 0.20162005722522736, Final Batch Loss: 0.109474316239357\n",
      "Epoch 2418, Loss: 0.26242849975824356, Final Batch Loss: 0.10028903931379318\n",
      "Epoch 2419, Loss: 0.22521360963582993, Final Batch Loss: 0.10669849067926407\n",
      "Epoch 2420, Loss: 0.27969712018966675, Final Batch Loss: 0.11301696300506592\n",
      "Epoch 2421, Loss: 0.27943408489227295, Final Batch Loss: 0.12127821147441864\n",
      "Epoch 2422, Loss: 0.26774953305721283, Final Batch Loss: 0.14144867658615112\n",
      "Epoch 2423, Loss: 0.26283296942710876, Final Batch Loss: 0.11381395161151886\n",
      "Epoch 2424, Loss: 0.30450163781642914, Final Batch Loss: 0.2069149762392044\n",
      "Epoch 2425, Loss: 0.28775301575660706, Final Batch Loss: 0.16884925961494446\n",
      "Epoch 2426, Loss: 0.2113288938999176, Final Batch Loss: 0.12055210024118423\n",
      "Epoch 2427, Loss: 0.3089245557785034, Final Batch Loss: 0.1490553915500641\n",
      "Epoch 2428, Loss: 0.26185545325279236, Final Batch Loss: 0.17455463111400604\n",
      "Epoch 2429, Loss: 0.2401401847600937, Final Batch Loss: 0.10617430508136749\n",
      "Epoch 2430, Loss: 0.3347669094800949, Final Batch Loss: 0.14230214059352875\n",
      "Epoch 2431, Loss: 0.24090729653835297, Final Batch Loss: 0.11157877743244171\n",
      "Epoch 2432, Loss: 0.1992589682340622, Final Batch Loss: 0.10011885315179825\n",
      "Epoch 2433, Loss: 0.22015757858753204, Final Batch Loss: 0.11661313474178314\n",
      "Epoch 2434, Loss: 0.23794707655906677, Final Batch Loss: 0.1396780014038086\n",
      "Epoch 2435, Loss: 0.20798409730196, Final Batch Loss: 0.0975165143609047\n",
      "Epoch 2436, Loss: 0.24059949815273285, Final Batch Loss: 0.12913274765014648\n",
      "Epoch 2437, Loss: 0.2499694749712944, Final Batch Loss: 0.14895236492156982\n",
      "Epoch 2438, Loss: 0.21098075807094574, Final Batch Loss: 0.10560718178749084\n",
      "Epoch 2439, Loss: 0.2399834617972374, Final Batch Loss: 0.10507769137620926\n",
      "Epoch 2440, Loss: 0.2061893418431282, Final Batch Loss: 0.0748869851231575\n",
      "Epoch 2441, Loss: 0.2392043098807335, Final Batch Loss: 0.08732060343027115\n",
      "Epoch 2442, Loss: 0.262138731777668, Final Batch Loss: 0.12407202273607254\n",
      "Epoch 2443, Loss: 0.2142503410577774, Final Batch Loss: 0.11005976051092148\n",
      "Epoch 2444, Loss: 0.3375036418437958, Final Batch Loss: 0.17491401731967926\n",
      "Epoch 2445, Loss: 0.2812666893005371, Final Batch Loss: 0.16584545373916626\n",
      "Epoch 2446, Loss: 0.26151812821626663, Final Batch Loss: 0.10967240482568741\n",
      "Epoch 2447, Loss: 0.30047233402729034, Final Batch Loss: 0.17630533874034882\n",
      "Epoch 2448, Loss: 0.3286338448524475, Final Batch Loss: 0.09204517304897308\n",
      "Epoch 2449, Loss: 0.24061858654022217, Final Batch Loss: 0.11657964438199997\n",
      "Epoch 2450, Loss: 0.23345492035150528, Final Batch Loss: 0.1127130314707756\n",
      "Epoch 2451, Loss: 0.2389528527855873, Final Batch Loss: 0.12382366508245468\n",
      "Epoch 2452, Loss: 0.21801526099443436, Final Batch Loss: 0.10670693218708038\n",
      "Epoch 2453, Loss: 0.29965248703956604, Final Batch Loss: 0.13752718269824982\n",
      "Epoch 2454, Loss: 0.26538509875535965, Final Batch Loss: 0.12154068797826767\n",
      "Epoch 2455, Loss: 0.23400990664958954, Final Batch Loss: 0.1422256976366043\n",
      "Epoch 2456, Loss: 0.2540452107787132, Final Batch Loss: 0.09675353020429611\n",
      "Epoch 2457, Loss: 0.24900846183300018, Final Batch Loss: 0.10856758058071136\n",
      "Epoch 2458, Loss: 0.23764998465776443, Final Batch Loss: 0.1022338941693306\n",
      "Epoch 2459, Loss: 0.28953659534454346, Final Batch Loss: 0.12894956767559052\n",
      "Epoch 2460, Loss: 0.21541433781385422, Final Batch Loss: 0.1070273220539093\n",
      "Epoch 2461, Loss: 0.24295058846473694, Final Batch Loss: 0.1093064695596695\n",
      "Epoch 2462, Loss: 0.2787574455142021, Final Batch Loss: 0.1751071661710739\n",
      "Epoch 2463, Loss: 0.28631381690502167, Final Batch Loss: 0.12859682738780975\n",
      "Epoch 2464, Loss: 0.26937005668878555, Final Batch Loss: 0.10598846524953842\n",
      "Epoch 2465, Loss: 0.24667344987392426, Final Batch Loss: 0.15445344150066376\n",
      "Epoch 2466, Loss: 0.28612881898880005, Final Batch Loss: 0.16628190875053406\n",
      "Epoch 2467, Loss: 0.3189620226621628, Final Batch Loss: 0.14219588041305542\n",
      "Epoch 2468, Loss: 0.2642752081155777, Final Batch Loss: 0.1769452542066574\n",
      "Epoch 2469, Loss: 0.2836418002843857, Final Batch Loss: 0.15656284987926483\n",
      "Epoch 2470, Loss: 0.2410028651356697, Final Batch Loss: 0.12378562241792679\n",
      "Epoch 2471, Loss: 0.25961513817310333, Final Batch Loss: 0.1062650978565216\n",
      "Epoch 2472, Loss: 0.23811593651771545, Final Batch Loss: 0.13579022884368896\n",
      "Epoch 2473, Loss: 0.2529616877436638, Final Batch Loss: 0.12097161263227463\n",
      "Epoch 2474, Loss: 0.3231049180030823, Final Batch Loss: 0.18315957486629486\n",
      "Epoch 2475, Loss: 0.31828998029232025, Final Batch Loss: 0.17223013937473297\n",
      "Epoch 2476, Loss: 0.2903652489185333, Final Batch Loss: 0.1573985069990158\n",
      "Epoch 2477, Loss: 0.20478541404008865, Final Batch Loss: 0.08718284219503403\n",
      "Epoch 2478, Loss: 0.28474926948547363, Final Batch Loss: 0.13660605251789093\n",
      "Epoch 2479, Loss: 0.1967192068696022, Final Batch Loss: 0.0878543108701706\n",
      "Epoch 2480, Loss: 0.3229743242263794, Final Batch Loss: 0.1705162078142166\n",
      "Epoch 2481, Loss: 0.22680021077394485, Final Batch Loss: 0.11759398132562637\n",
      "Epoch 2482, Loss: 0.25697679072618484, Final Batch Loss: 0.14120140671730042\n",
      "Epoch 2483, Loss: 0.23326906561851501, Final Batch Loss: 0.08726516366004944\n",
      "Epoch 2484, Loss: 0.21279502660036087, Final Batch Loss: 0.1082058697938919\n",
      "Epoch 2485, Loss: 0.22076315432786942, Final Batch Loss: 0.11139190196990967\n",
      "Epoch 2486, Loss: 0.2882235571742058, Final Batch Loss: 0.19076862931251526\n",
      "Epoch 2487, Loss: 0.18848081678152084, Final Batch Loss: 0.08525549620389938\n",
      "Epoch 2488, Loss: 0.23879238963127136, Final Batch Loss: 0.10365369915962219\n",
      "Epoch 2489, Loss: 0.21888487040996552, Final Batch Loss: 0.06570741534233093\n",
      "Epoch 2490, Loss: 0.2310657873749733, Final Batch Loss: 0.11768389493227005\n",
      "Epoch 2491, Loss: 0.24750100076198578, Final Batch Loss: 0.12825165688991547\n",
      "Epoch 2492, Loss: 0.24729911237955093, Final Batch Loss: 0.08234221488237381\n",
      "Epoch 2493, Loss: 0.20977474749088287, Final Batch Loss: 0.10444845259189606\n",
      "Epoch 2494, Loss: 0.22912298887968063, Final Batch Loss: 0.06867068260908127\n",
      "Epoch 2495, Loss: 0.30155111849308014, Final Batch Loss: 0.1548612415790558\n",
      "Epoch 2496, Loss: 0.2017771229147911, Final Batch Loss: 0.0942823588848114\n",
      "Epoch 2497, Loss: 0.2096448466181755, Final Batch Loss: 0.11188971996307373\n",
      "Epoch 2498, Loss: 0.2669614627957344, Final Batch Loss: 0.15201498568058014\n",
      "Epoch 2499, Loss: 0.24342697113752365, Final Batch Loss: 0.1007961854338646\n",
      "Epoch 2500, Loss: 0.27523861825466156, Final Batch Loss: 0.16161920130252838\n",
      "Epoch 2501, Loss: 0.27213679999113083, Final Batch Loss: 0.15159454941749573\n",
      "Epoch 2502, Loss: 0.2705422043800354, Final Batch Loss: 0.10741458833217621\n",
      "Epoch 2503, Loss: 0.2231859192252159, Final Batch Loss: 0.13355384767055511\n",
      "Epoch 2504, Loss: 0.24036409705877304, Final Batch Loss: 0.10872570425271988\n",
      "Epoch 2505, Loss: 0.23707684874534607, Final Batch Loss: 0.11544165760278702\n",
      "Epoch 2506, Loss: 0.3002360165119171, Final Batch Loss: 0.20077072083950043\n",
      "Epoch 2507, Loss: 0.20426901429891586, Final Batch Loss: 0.10464542359113693\n",
      "Epoch 2508, Loss: 0.2536102458834648, Final Batch Loss: 0.12990084290504456\n",
      "Epoch 2509, Loss: 0.23351547122001648, Final Batch Loss: 0.11788853257894516\n",
      "Epoch 2510, Loss: 0.21444525569677353, Final Batch Loss: 0.07363490015268326\n",
      "Epoch 2511, Loss: 0.2682703584432602, Final Batch Loss: 0.14203806221485138\n",
      "Epoch 2512, Loss: 0.2224775105714798, Final Batch Loss: 0.13662531971931458\n",
      "Epoch 2513, Loss: 0.3082660064101219, Final Batch Loss: 0.1869695484638214\n",
      "Epoch 2514, Loss: 0.3067226707935333, Final Batch Loss: 0.1683400422334671\n",
      "Epoch 2515, Loss: 0.2778283730149269, Final Batch Loss: 0.16572239995002747\n",
      "Epoch 2516, Loss: 0.23597702383995056, Final Batch Loss: 0.14147479832172394\n",
      "Epoch 2517, Loss: 0.21551312506198883, Final Batch Loss: 0.08275580406188965\n",
      "Epoch 2518, Loss: 0.2676598057150841, Final Batch Loss: 0.08937818557024002\n",
      "Epoch 2519, Loss: 0.36959385871887207, Final Batch Loss: 0.21691027283668518\n",
      "Epoch 2520, Loss: 0.2850895896553993, Final Batch Loss: 0.17812272906303406\n",
      "Epoch 2521, Loss: 0.2906193807721138, Final Batch Loss: 0.17661282420158386\n",
      "Epoch 2522, Loss: 0.1627069190144539, Final Batch Loss: 0.05673079937696457\n",
      "Epoch 2523, Loss: 0.19836199283599854, Final Batch Loss: 0.08319190889596939\n",
      "Epoch 2524, Loss: 0.19748494029045105, Final Batch Loss: 0.08477158844470978\n",
      "Epoch 2525, Loss: 0.22697840631008148, Final Batch Loss: 0.1205597072839737\n",
      "Epoch 2526, Loss: 0.2027849704027176, Final Batch Loss: 0.06942138075828552\n",
      "Epoch 2527, Loss: 0.18540877848863602, Final Batch Loss: 0.0747128576040268\n",
      "Epoch 2528, Loss: 0.2520405948162079, Final Batch Loss: 0.07772649824619293\n",
      "Epoch 2529, Loss: 0.3071965277194977, Final Batch Loss: 0.14986345171928406\n",
      "Epoch 2530, Loss: 0.23854102194309235, Final Batch Loss: 0.11886923015117645\n",
      "Epoch 2531, Loss: 0.25490622222423553, Final Batch Loss: 0.14423777163028717\n",
      "Epoch 2532, Loss: 0.27675894647836685, Final Batch Loss: 0.16656005382537842\n",
      "Epoch 2533, Loss: 0.21840617805719376, Final Batch Loss: 0.09813082963228226\n",
      "Epoch 2534, Loss: 0.24211352318525314, Final Batch Loss: 0.13654235005378723\n",
      "Epoch 2535, Loss: 0.25164399296045303, Final Batch Loss: 0.14138881862163544\n",
      "Epoch 2536, Loss: 0.23304424434900284, Final Batch Loss: 0.10735789686441422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2537, Loss: 0.22250860184431076, Final Batch Loss: 0.12727029621601105\n",
      "Epoch 2538, Loss: 0.23434118926525116, Final Batch Loss: 0.08907394111156464\n",
      "Epoch 2539, Loss: 0.22982129454612732, Final Batch Loss: 0.1461707204580307\n",
      "Epoch 2540, Loss: 0.2055581510066986, Final Batch Loss: 0.10767295211553574\n",
      "Epoch 2541, Loss: 0.21873556077480316, Final Batch Loss: 0.10511106997728348\n",
      "Epoch 2542, Loss: 0.20591823756694794, Final Batch Loss: 0.0954640582203865\n",
      "Epoch 2543, Loss: 0.3118419945240021, Final Batch Loss: 0.17335285246372223\n",
      "Epoch 2544, Loss: 0.26729851961135864, Final Batch Loss: 0.15464583039283752\n",
      "Epoch 2545, Loss: 0.2941563352942467, Final Batch Loss: 0.12291083484888077\n",
      "Epoch 2546, Loss: 0.2856341451406479, Final Batch Loss: 0.15911561250686646\n",
      "Epoch 2547, Loss: 0.2122887745499611, Final Batch Loss: 0.10331614315509796\n",
      "Epoch 2548, Loss: 0.2640695720911026, Final Batch Loss: 0.12441237270832062\n",
      "Epoch 2549, Loss: 0.2653815299272537, Final Batch Loss: 0.09196659922599792\n",
      "Epoch 2550, Loss: 0.23078224807977676, Final Batch Loss: 0.12108246982097626\n",
      "Epoch 2551, Loss: 0.3727472722530365, Final Batch Loss: 0.25195661187171936\n",
      "Epoch 2552, Loss: 0.22272734344005585, Final Batch Loss: 0.11599428951740265\n",
      "Epoch 2553, Loss: 0.2581546977162361, Final Batch Loss: 0.1666320562362671\n",
      "Epoch 2554, Loss: 0.19201018661260605, Final Batch Loss: 0.08187229186296463\n",
      "Epoch 2555, Loss: 0.27042508870363235, Final Batch Loss: 0.17132563889026642\n",
      "Epoch 2556, Loss: 0.1832519769668579, Final Batch Loss: 0.07925806939601898\n",
      "Epoch 2557, Loss: 0.3115476220846176, Final Batch Loss: 0.15462937951087952\n",
      "Epoch 2558, Loss: 0.19290386885404587, Final Batch Loss: 0.06511148065328598\n",
      "Epoch 2559, Loss: 0.30417604744434357, Final Batch Loss: 0.16072426736354828\n",
      "Epoch 2560, Loss: 0.257367342710495, Final Batch Loss: 0.1347530633211136\n",
      "Epoch 2561, Loss: 0.2765282988548279, Final Batch Loss: 0.12939271330833435\n",
      "Epoch 2562, Loss: 0.21066972613334656, Final Batch Loss: 0.10040129721164703\n",
      "Epoch 2563, Loss: 0.21272064000368118, Final Batch Loss: 0.12042764574289322\n",
      "Epoch 2564, Loss: 0.2819281071424484, Final Batch Loss: 0.16639144718647003\n",
      "Epoch 2565, Loss: 0.26046789437532425, Final Batch Loss: 0.10594194382429123\n",
      "Epoch 2566, Loss: 0.17694149166345596, Final Batch Loss: 0.08131777495145798\n",
      "Epoch 2567, Loss: 0.2736073210835457, Final Batch Loss: 0.16563940048217773\n",
      "Epoch 2568, Loss: 0.27612491697072983, Final Batch Loss: 0.1691868156194687\n",
      "Epoch 2569, Loss: 0.19677891582250595, Final Batch Loss: 0.08581868559122086\n",
      "Epoch 2570, Loss: 0.29046104848384857, Final Batch Loss: 0.10687965154647827\n",
      "Epoch 2571, Loss: 0.24690360575914383, Final Batch Loss: 0.1513315588235855\n",
      "Epoch 2572, Loss: 0.2913299649953842, Final Batch Loss: 0.14100925624370575\n",
      "Epoch 2573, Loss: 0.20389611274003983, Final Batch Loss: 0.0936705693602562\n",
      "Epoch 2574, Loss: 0.25728418678045273, Final Batch Loss: 0.13272641599178314\n",
      "Epoch 2575, Loss: 0.28632887452840805, Final Batch Loss: 0.17741705477237701\n",
      "Epoch 2576, Loss: 0.20448587834835052, Final Batch Loss: 0.11320119351148605\n",
      "Epoch 2577, Loss: 0.26315921545028687, Final Batch Loss: 0.15123926103115082\n",
      "Epoch 2578, Loss: 0.23330270498991013, Final Batch Loss: 0.08084913343191147\n",
      "Epoch 2579, Loss: 0.23587408661842346, Final Batch Loss: 0.14365437626838684\n",
      "Epoch 2580, Loss: 0.3005122095346451, Final Batch Loss: 0.1750219464302063\n",
      "Epoch 2581, Loss: 0.1956956535577774, Final Batch Loss: 0.07670929282903671\n",
      "Epoch 2582, Loss: 0.2632284015417099, Final Batch Loss: 0.09736786782741547\n",
      "Epoch 2583, Loss: 0.2289520427584648, Final Batch Loss: 0.08976639062166214\n",
      "Epoch 2584, Loss: 0.28397759795188904, Final Batch Loss: 0.12474946677684784\n",
      "Epoch 2585, Loss: 0.3017849922180176, Final Batch Loss: 0.1322212666273117\n",
      "Epoch 2586, Loss: 0.23799297213554382, Final Batch Loss: 0.09737536311149597\n",
      "Epoch 2587, Loss: 0.3020037114620209, Final Batch Loss: 0.18089351058006287\n",
      "Epoch 2588, Loss: 0.25314630568027496, Final Batch Loss: 0.1515698879957199\n",
      "Epoch 2589, Loss: 0.2259928584098816, Final Batch Loss: 0.12641076743602753\n",
      "Epoch 2590, Loss: 0.2198789045214653, Final Batch Loss: 0.09044656902551651\n",
      "Epoch 2591, Loss: 0.2409144639968872, Final Batch Loss: 0.10273423790931702\n",
      "Epoch 2592, Loss: 0.21592910587787628, Final Batch Loss: 0.12448880076408386\n",
      "Epoch 2593, Loss: 0.2646951675415039, Final Batch Loss: 0.1533820927143097\n",
      "Epoch 2594, Loss: 0.3180793523788452, Final Batch Loss: 0.13288520276546478\n",
      "Epoch 2595, Loss: 0.3360249549150467, Final Batch Loss: 0.1861800104379654\n",
      "Epoch 2596, Loss: 0.2722727060317993, Final Batch Loss: 0.1514316350221634\n",
      "Epoch 2597, Loss: 0.2043963074684143, Final Batch Loss: 0.10212145745754242\n",
      "Epoch 2598, Loss: 0.2520907297730446, Final Batch Loss: 0.12879334390163422\n",
      "Epoch 2599, Loss: 0.2065477930009365, Final Batch Loss: 0.06001884862780571\n",
      "Epoch 2600, Loss: 0.2104066088795662, Final Batch Loss: 0.08757592737674713\n",
      "Epoch 2601, Loss: 0.2832276523113251, Final Batch Loss: 0.17691250145435333\n",
      "Epoch 2602, Loss: 0.2656329497694969, Final Batch Loss: 0.11532001942396164\n",
      "Epoch 2603, Loss: 0.29153895378112793, Final Batch Loss: 0.1644100546836853\n",
      "Epoch 2604, Loss: 0.22716812789440155, Final Batch Loss: 0.12301543354988098\n",
      "Epoch 2605, Loss: 0.25069326162338257, Final Batch Loss: 0.12229064106941223\n",
      "Epoch 2606, Loss: 0.25353606045246124, Final Batch Loss: 0.13081957399845123\n",
      "Epoch 2607, Loss: 0.20623765140771866, Final Batch Loss: 0.1098458394408226\n",
      "Epoch 2608, Loss: 0.30541475117206573, Final Batch Loss: 0.18614119291305542\n",
      "Epoch 2609, Loss: 0.1983315721154213, Final Batch Loss: 0.08102894574403763\n",
      "Epoch 2610, Loss: 0.2387719824910164, Final Batch Loss: 0.11632462590932846\n",
      "Epoch 2611, Loss: 0.28603363037109375, Final Batch Loss: 0.1582092046737671\n",
      "Epoch 2612, Loss: 0.24768607318401337, Final Batch Loss: 0.10544024407863617\n",
      "Epoch 2613, Loss: 0.24363894015550613, Final Batch Loss: 0.12849867343902588\n",
      "Epoch 2614, Loss: 0.27165840566158295, Final Batch Loss: 0.10493333637714386\n",
      "Epoch 2615, Loss: 0.1812126338481903, Final Batch Loss: 0.0772080272436142\n",
      "Epoch 2616, Loss: 0.25504426658153534, Final Batch Loss: 0.07726335525512695\n",
      "Epoch 2617, Loss: 0.23797999322414398, Final Batch Loss: 0.07501265406608582\n",
      "Epoch 2618, Loss: 0.23338466882705688, Final Batch Loss: 0.12131394445896149\n",
      "Epoch 2619, Loss: 0.18374786153435707, Final Batch Loss: 0.051497090607881546\n",
      "Epoch 2620, Loss: 0.16016259789466858, Final Batch Loss: 0.08061355352401733\n",
      "Epoch 2621, Loss: 0.24486056715250015, Final Batch Loss: 0.14404070377349854\n",
      "Epoch 2622, Loss: 0.23358578979969025, Final Batch Loss: 0.12967552244663239\n",
      "Epoch 2623, Loss: 0.2634427696466446, Final Batch Loss: 0.14986084401607513\n",
      "Epoch 2624, Loss: 0.27934107184410095, Final Batch Loss: 0.15444572269916534\n",
      "Epoch 2625, Loss: 0.29252366721630096, Final Batch Loss: 0.15077048540115356\n",
      "Epoch 2626, Loss: 0.35697294771671295, Final Batch Loss: 0.2090112417936325\n",
      "Epoch 2627, Loss: 0.2521055340766907, Final Batch Loss: 0.1378680318593979\n",
      "Epoch 2628, Loss: 0.24587475508451462, Final Batch Loss: 0.13639603555202484\n",
      "Epoch 2629, Loss: 0.23138059675693512, Final Batch Loss: 0.13823343813419342\n",
      "Epoch 2630, Loss: 0.23534565418958664, Final Batch Loss: 0.12825259566307068\n",
      "Epoch 2631, Loss: 0.24894414097070694, Final Batch Loss: 0.11171480268239975\n",
      "Epoch 2632, Loss: 0.23022185266017914, Final Batch Loss: 0.09430330991744995\n",
      "Epoch 2633, Loss: 0.2297419160604477, Final Batch Loss: 0.10985612869262695\n",
      "Epoch 2634, Loss: 0.16894998401403427, Final Batch Loss: 0.06578897684812546\n",
      "Epoch 2635, Loss: 0.2983419597148895, Final Batch Loss: 0.17391209304332733\n",
      "Epoch 2636, Loss: 0.23101038485765457, Final Batch Loss: 0.12466651946306229\n",
      "Epoch 2637, Loss: 0.25577642768621445, Final Batch Loss: 0.117242731153965\n",
      "Epoch 2638, Loss: 0.23071042448282242, Final Batch Loss: 0.08702326565980911\n",
      "Epoch 2639, Loss: 0.2165820524096489, Final Batch Loss: 0.05802338570356369\n",
      "Epoch 2640, Loss: 0.25813809782266617, Final Batch Loss: 0.13815920054912567\n",
      "Epoch 2641, Loss: 0.22877862304449081, Final Batch Loss: 0.12738732993602753\n",
      "Epoch 2642, Loss: 0.2042638286948204, Final Batch Loss: 0.09598395228385925\n",
      "Epoch 2643, Loss: 0.24656247347593307, Final Batch Loss: 0.10822122544050217\n",
      "Epoch 2644, Loss: 0.20999681949615479, Final Batch Loss: 0.10183071345090866\n",
      "Epoch 2645, Loss: 0.2304227650165558, Final Batch Loss: 0.08998407423496246\n",
      "Epoch 2646, Loss: 0.24594607204198837, Final Batch Loss: 0.08713342994451523\n",
      "Epoch 2647, Loss: 0.22736182063817978, Final Batch Loss: 0.10210221260786057\n",
      "Epoch 2648, Loss: 0.31806375831365585, Final Batch Loss: 0.20920327305793762\n",
      "Epoch 2649, Loss: 0.227020263671875, Final Batch Loss: 0.1242009848356247\n",
      "Epoch 2650, Loss: 0.2076910361647606, Final Batch Loss: 0.07159515470266342\n",
      "Epoch 2651, Loss: 0.20470087975263596, Final Batch Loss: 0.1020941287279129\n",
      "Epoch 2652, Loss: 0.24230331927537918, Final Batch Loss: 0.06799619644880295\n",
      "Epoch 2653, Loss: 0.2548742964863777, Final Batch Loss: 0.10339102894067764\n",
      "Epoch 2654, Loss: 0.26415371894836426, Final Batch Loss: 0.17157739400863647\n",
      "Epoch 2655, Loss: 0.2961304038763046, Final Batch Loss: 0.11731965839862823\n",
      "Epoch 2656, Loss: 0.20947609841823578, Final Batch Loss: 0.0758112221956253\n",
      "Epoch 2657, Loss: 0.2893061935901642, Final Batch Loss: 0.15504299104213715\n",
      "Epoch 2658, Loss: 0.16407105326652527, Final Batch Loss: 0.07396367192268372\n",
      "Epoch 2659, Loss: 0.23933156579732895, Final Batch Loss: 0.11311418563127518\n",
      "Epoch 2660, Loss: 0.2616720274090767, Final Batch Loss: 0.10316013544797897\n",
      "Epoch 2661, Loss: 0.2032264918088913, Final Batch Loss: 0.13357770442962646\n",
      "Epoch 2662, Loss: 0.20435470342636108, Final Batch Loss: 0.11247359216213226\n",
      "Epoch 2663, Loss: 0.2335437536239624, Final Batch Loss: 0.10180892050266266\n",
      "Epoch 2664, Loss: 0.25214315205812454, Final Batch Loss: 0.1490088254213333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2665, Loss: 0.22595594078302383, Final Batch Loss: 0.12537316977977753\n",
      "Epoch 2666, Loss: 0.1791461482644081, Final Batch Loss: 0.09776031970977783\n",
      "Epoch 2667, Loss: 0.24413549900054932, Final Batch Loss: 0.09613801538944244\n",
      "Epoch 2668, Loss: 0.2413918450474739, Final Batch Loss: 0.11600136011838913\n",
      "Epoch 2669, Loss: 0.236783929169178, Final Batch Loss: 0.14632181823253632\n",
      "Epoch 2670, Loss: 0.24139366298913956, Final Batch Loss: 0.1485147923231125\n",
      "Epoch 2671, Loss: 0.2626398876309395, Final Batch Loss: 0.15716540813446045\n",
      "Epoch 2672, Loss: 0.19860266149044037, Final Batch Loss: 0.08729209005832672\n",
      "Epoch 2673, Loss: 0.2548842132091522, Final Batch Loss: 0.13492168486118317\n",
      "Epoch 2674, Loss: 0.3158997744321823, Final Batch Loss: 0.17813298106193542\n",
      "Epoch 2675, Loss: 0.22758152335882187, Final Batch Loss: 0.09470320492982864\n",
      "Epoch 2676, Loss: 0.2082185298204422, Final Batch Loss: 0.07886508107185364\n",
      "Epoch 2677, Loss: 0.2663027048110962, Final Batch Loss: 0.09330278635025024\n",
      "Epoch 2678, Loss: 0.16137590259313583, Final Batch Loss: 0.0741700753569603\n",
      "Epoch 2679, Loss: 0.2321787327528, Final Batch Loss: 0.11615817993879318\n",
      "Epoch 2680, Loss: 0.23313258588314056, Final Batch Loss: 0.13593123853206635\n",
      "Epoch 2681, Loss: 0.2513849660754204, Final Batch Loss: 0.1213185116648674\n",
      "Epoch 2682, Loss: 0.2297215834259987, Final Batch Loss: 0.08413694053888321\n",
      "Epoch 2683, Loss: 0.20798522233963013, Final Batch Loss: 0.11346746981143951\n",
      "Epoch 2684, Loss: 0.2939436063170433, Final Batch Loss: 0.10820458084344864\n",
      "Epoch 2685, Loss: 0.19814186543226242, Final Batch Loss: 0.11018511652946472\n",
      "Epoch 2686, Loss: 0.19213728606700897, Final Batch Loss: 0.08927524834871292\n",
      "Epoch 2687, Loss: 0.2993868440389633, Final Batch Loss: 0.13982903957366943\n",
      "Epoch 2688, Loss: 0.2679859846830368, Final Batch Loss: 0.1612747311592102\n",
      "Epoch 2689, Loss: 0.18822970986366272, Final Batch Loss: 0.09027794748544693\n",
      "Epoch 2690, Loss: 0.22958854585886002, Final Batch Loss: 0.11501825600862503\n",
      "Epoch 2691, Loss: 0.21954109519720078, Final Batch Loss: 0.07571694999933243\n",
      "Epoch 2692, Loss: 0.22611558437347412, Final Batch Loss: 0.09110313653945923\n",
      "Epoch 2693, Loss: 0.2502041980624199, Final Batch Loss: 0.10938725620508194\n",
      "Epoch 2694, Loss: 0.20622168481349945, Final Batch Loss: 0.08779462426900864\n",
      "Epoch 2695, Loss: 0.27489957958459854, Final Batch Loss: 0.1128382608294487\n",
      "Epoch 2696, Loss: 0.237382210791111, Final Batch Loss: 0.11874319612979889\n",
      "Epoch 2697, Loss: 0.2487223595380783, Final Batch Loss: 0.12780815362930298\n",
      "Epoch 2698, Loss: 0.18525000661611557, Final Batch Loss: 0.0637381449341774\n",
      "Epoch 2699, Loss: 0.2522628828883171, Final Batch Loss: 0.1276509016752243\n",
      "Epoch 2700, Loss: 0.22481585294008255, Final Batch Loss: 0.10974031686782837\n",
      "Epoch 2701, Loss: 0.2788122147321701, Final Batch Loss: 0.14898928999900818\n",
      "Epoch 2702, Loss: 0.23027662187814713, Final Batch Loss: 0.09298492223024368\n",
      "Epoch 2703, Loss: 0.2106916755437851, Final Batch Loss: 0.12454598397016525\n",
      "Epoch 2704, Loss: 0.22250034660100937, Final Batch Loss: 0.10568497329950333\n",
      "Epoch 2705, Loss: 0.26519371569156647, Final Batch Loss: 0.1873098611831665\n",
      "Epoch 2706, Loss: 0.22658182680606842, Final Batch Loss: 0.08473896980285645\n",
      "Epoch 2707, Loss: 0.1724913939833641, Final Batch Loss: 0.08586180955171585\n",
      "Epoch 2708, Loss: 0.2842407524585724, Final Batch Loss: 0.1693856567144394\n",
      "Epoch 2709, Loss: 0.20137137919664383, Final Batch Loss: 0.07749734073877335\n",
      "Epoch 2710, Loss: 0.22960317879915237, Final Batch Loss: 0.10622972995042801\n",
      "Epoch 2711, Loss: 0.20165422558784485, Final Batch Loss: 0.10974860936403275\n",
      "Epoch 2712, Loss: 0.16296479478478432, Final Batch Loss: 0.055996913462877274\n",
      "Epoch 2713, Loss: 0.20232917368412018, Final Batch Loss: 0.12918484210968018\n",
      "Epoch 2714, Loss: 0.21631290763616562, Final Batch Loss: 0.11279882490634918\n",
      "Epoch 2715, Loss: 0.18577339500188828, Final Batch Loss: 0.08886892348527908\n",
      "Epoch 2716, Loss: 0.2138536423444748, Final Batch Loss: 0.117111437022686\n",
      "Epoch 2717, Loss: 0.29145989939570427, Final Batch Loss: 0.06202171370387077\n",
      "Epoch 2718, Loss: 0.20880642533302307, Final Batch Loss: 0.08619819581508636\n",
      "Epoch 2719, Loss: 0.21176983416080475, Final Batch Loss: 0.10026862472295761\n",
      "Epoch 2720, Loss: 0.20322832465171814, Final Batch Loss: 0.07692481577396393\n",
      "Epoch 2721, Loss: 0.23273959755897522, Final Batch Loss: 0.12803716957569122\n",
      "Epoch 2722, Loss: 0.15848969668149948, Final Batch Loss: 0.07352915406227112\n",
      "Epoch 2723, Loss: 0.21784177422523499, Final Batch Loss: 0.08250455558300018\n",
      "Epoch 2724, Loss: 0.2459097057580948, Final Batch Loss: 0.13505522906780243\n",
      "Epoch 2725, Loss: 0.2183779552578926, Final Batch Loss: 0.11223267763853073\n",
      "Epoch 2726, Loss: 0.27250613272190094, Final Batch Loss: 0.15384231507778168\n",
      "Epoch 2727, Loss: 0.18986667692661285, Final Batch Loss: 0.09644317626953125\n",
      "Epoch 2728, Loss: 0.24281392246484756, Final Batch Loss: 0.11889325827360153\n",
      "Epoch 2729, Loss: 0.2061196267604828, Final Batch Loss: 0.10406366735696793\n",
      "Epoch 2730, Loss: 0.23689443618059158, Final Batch Loss: 0.10306695848703384\n",
      "Epoch 2731, Loss: 0.18416202813386917, Final Batch Loss: 0.09169930219650269\n",
      "Epoch 2732, Loss: 0.2736824154853821, Final Batch Loss: 0.1480908840894699\n",
      "Epoch 2733, Loss: 0.2247278094291687, Final Batch Loss: 0.12074428796768188\n",
      "Epoch 2734, Loss: 0.1890757456421852, Final Batch Loss: 0.11255308240652084\n",
      "Epoch 2735, Loss: 0.1838613748550415, Final Batch Loss: 0.10171929001808167\n",
      "Epoch 2736, Loss: 0.24098747223615646, Final Batch Loss: 0.11407787352800369\n",
      "Epoch 2737, Loss: 0.15717115998268127, Final Batch Loss: 0.06648024916648865\n",
      "Epoch 2738, Loss: 0.2611125409603119, Final Batch Loss: 0.082032710313797\n",
      "Epoch 2739, Loss: 0.2583649456501007, Final Batch Loss: 0.14132285118103027\n",
      "Epoch 2740, Loss: 0.28108833730220795, Final Batch Loss: 0.1061326116323471\n",
      "Epoch 2741, Loss: 0.20692472159862518, Final Batch Loss: 0.08856017142534256\n",
      "Epoch 2742, Loss: 0.2515825405716896, Final Batch Loss: 0.1186019703745842\n",
      "Epoch 2743, Loss: 0.19579192996025085, Final Batch Loss: 0.09439495205879211\n",
      "Epoch 2744, Loss: 0.17523793876171112, Final Batch Loss: 0.0940929725766182\n",
      "Epoch 2745, Loss: 0.22731241583824158, Final Batch Loss: 0.12023383378982544\n",
      "Epoch 2746, Loss: 0.2195289582014084, Final Batch Loss: 0.09879618883132935\n",
      "Epoch 2747, Loss: 0.26775893568992615, Final Batch Loss: 0.13516320288181305\n",
      "Epoch 2748, Loss: 0.26413583010435104, Final Batch Loss: 0.10715996474027634\n",
      "Epoch 2749, Loss: 0.17331267893314362, Final Batch Loss: 0.07212495058774948\n",
      "Epoch 2750, Loss: 0.2310674488544464, Final Batch Loss: 0.1172262504696846\n",
      "Epoch 2751, Loss: 0.2407490238547325, Final Batch Loss: 0.1548771858215332\n",
      "Epoch 2752, Loss: 0.245658777654171, Final Batch Loss: 0.15752466022968292\n",
      "Epoch 2753, Loss: 0.2138391137123108, Final Batch Loss: 0.09186485409736633\n",
      "Epoch 2754, Loss: 0.2194008231163025, Final Batch Loss: 0.10766056925058365\n",
      "Epoch 2755, Loss: 0.22408118844032288, Final Batch Loss: 0.11662664264440536\n",
      "Epoch 2756, Loss: 0.21512552350759506, Final Batch Loss: 0.11244998127222061\n",
      "Epoch 2757, Loss: 0.22020892053842545, Final Batch Loss: 0.09600980579853058\n",
      "Epoch 2758, Loss: 0.2711522802710533, Final Batch Loss: 0.1696118265390396\n",
      "Epoch 2759, Loss: 0.22412626445293427, Final Batch Loss: 0.11751724034547806\n",
      "Epoch 2760, Loss: 0.20913639664649963, Final Batch Loss: 0.07079504430294037\n",
      "Epoch 2761, Loss: 0.23493141680955887, Final Batch Loss: 0.1283598691225052\n",
      "Epoch 2762, Loss: 0.16544120013713837, Final Batch Loss: 0.06480423361063004\n",
      "Epoch 2763, Loss: 0.18833549320697784, Final Batch Loss: 0.09037705510854721\n",
      "Epoch 2764, Loss: 0.1794588640332222, Final Batch Loss: 0.10226170718669891\n",
      "Epoch 2765, Loss: 0.23557166010141373, Final Batch Loss: 0.12124543637037277\n",
      "Epoch 2766, Loss: 0.22795119136571884, Final Batch Loss: 0.12493143230676651\n",
      "Epoch 2767, Loss: 0.22375255078077316, Final Batch Loss: 0.12402526289224625\n",
      "Epoch 2768, Loss: 0.23739361017942429, Final Batch Loss: 0.12655460834503174\n",
      "Epoch 2769, Loss: 0.2027120664715767, Final Batch Loss: 0.11051704734563828\n",
      "Epoch 2770, Loss: 0.17632117122411728, Final Batch Loss: 0.07653822004795074\n",
      "Epoch 2771, Loss: 0.22101280093193054, Final Batch Loss: 0.09022541344165802\n",
      "Epoch 2772, Loss: 0.1633601114153862, Final Batch Loss: 0.06205461919307709\n",
      "Epoch 2773, Loss: 0.2207225114107132, Final Batch Loss: 0.10605362802743912\n",
      "Epoch 2774, Loss: 0.1565614566206932, Final Batch Loss: 0.07825560867786407\n",
      "Epoch 2775, Loss: 0.19392968714237213, Final Batch Loss: 0.0920398086309433\n",
      "Epoch 2776, Loss: 0.2566932663321495, Final Batch Loss: 0.13329431414604187\n",
      "Epoch 2777, Loss: 0.16920559480786324, Final Batch Loss: 0.06173084303736687\n",
      "Epoch 2778, Loss: 0.2512590289115906, Final Batch Loss: 0.1709541380405426\n",
      "Epoch 2779, Loss: 0.2402963489294052, Final Batch Loss: 0.1489560604095459\n",
      "Epoch 2780, Loss: 0.24630694091320038, Final Batch Loss: 0.12132507562637329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2781, Loss: 0.15701613575220108, Final Batch Loss: 0.061573632061481476\n",
      "Epoch 2782, Loss: 0.2228817567229271, Final Batch Loss: 0.1410219520330429\n",
      "Epoch 2783, Loss: 0.20527034252882004, Final Batch Loss: 0.07487868517637253\n",
      "Epoch 2784, Loss: 0.17967956513166428, Final Batch Loss: 0.10325729101896286\n",
      "Epoch 2785, Loss: 0.21058030426502228, Final Batch Loss: 0.1201065257191658\n",
      "Epoch 2786, Loss: 0.18817655742168427, Final Batch Loss: 0.06775899231433868\n",
      "Epoch 2787, Loss: 0.24746518582105637, Final Batch Loss: 0.1299738883972168\n",
      "Epoch 2788, Loss: 0.20727583765983582, Final Batch Loss: 0.11195087432861328\n",
      "Epoch 2789, Loss: 0.2527381330728531, Final Batch Loss: 0.14809684455394745\n",
      "Epoch 2790, Loss: 0.20737511664628983, Final Batch Loss: 0.09964390844106674\n",
      "Epoch 2791, Loss: 0.19917254894971848, Final Batch Loss: 0.13644729554653168\n",
      "Epoch 2792, Loss: 0.16816995292901993, Final Batch Loss: 0.09814554452896118\n",
      "Epoch 2793, Loss: 0.1905406266450882, Final Batch Loss: 0.07317212969064713\n",
      "Epoch 2794, Loss: 0.1947610005736351, Final Batch Loss: 0.0753459557890892\n",
      "Epoch 2795, Loss: 0.25018659234046936, Final Batch Loss: 0.10877285897731781\n",
      "Epoch 2796, Loss: 0.21183551847934723, Final Batch Loss: 0.111920066177845\n",
      "Epoch 2797, Loss: 0.3087046146392822, Final Batch Loss: 0.19943632185459137\n",
      "Epoch 2798, Loss: 0.1585068143904209, Final Batch Loss: 0.05282596871256828\n",
      "Epoch 2799, Loss: 0.19374939799308777, Final Batch Loss: 0.09633317589759827\n",
      "Epoch 2800, Loss: 0.22607577592134476, Final Batch Loss: 0.102173812687397\n",
      "Epoch 2801, Loss: 0.25186995416879654, Final Batch Loss: 0.12199818342924118\n",
      "Epoch 2802, Loss: 0.19943900406360626, Final Batch Loss: 0.08583167940378189\n",
      "Epoch 2803, Loss: 0.18742811679840088, Final Batch Loss: 0.09367894381284714\n",
      "Epoch 2804, Loss: 0.12576165050268173, Final Batch Loss: 0.052048683166503906\n",
      "Epoch 2805, Loss: 0.2095247507095337, Final Batch Loss: 0.09249559044837952\n",
      "Epoch 2806, Loss: 0.23082126677036285, Final Batch Loss: 0.09648752212524414\n",
      "Epoch 2807, Loss: 0.19733309000730515, Final Batch Loss: 0.13165518641471863\n",
      "Epoch 2808, Loss: 0.36033302545547485, Final Batch Loss: 0.2215145230293274\n",
      "Epoch 2809, Loss: 0.21115965396165848, Final Batch Loss: 0.12275806069374084\n",
      "Epoch 2810, Loss: 0.22777428478002548, Final Batch Loss: 0.13568592071533203\n",
      "Epoch 2811, Loss: 0.24682965874671936, Final Batch Loss: 0.14101052284240723\n",
      "Epoch 2812, Loss: 0.21643169224262238, Final Batch Loss: 0.12544406950473785\n",
      "Epoch 2813, Loss: 0.253459133207798, Final Batch Loss: 0.1376616358757019\n",
      "Epoch 2814, Loss: 0.19341912865638733, Final Batch Loss: 0.07401776313781738\n",
      "Epoch 2815, Loss: 0.2122795507311821, Final Batch Loss: 0.11030957847833633\n",
      "Epoch 2816, Loss: 0.20753271877765656, Final Batch Loss: 0.1133270263671875\n",
      "Epoch 2817, Loss: 0.17404410243034363, Final Batch Loss: 0.06758429110050201\n",
      "Epoch 2818, Loss: 0.23516830801963806, Final Batch Loss: 0.1573190689086914\n",
      "Epoch 2819, Loss: 0.20241279155015945, Final Batch Loss: 0.10840611904859543\n",
      "Epoch 2820, Loss: 0.2104043811559677, Final Batch Loss: 0.12344765663146973\n",
      "Epoch 2821, Loss: 0.19056832045316696, Final Batch Loss: 0.08374709635972977\n",
      "Epoch 2822, Loss: 0.26205652207136154, Final Batch Loss: 0.16813555359840393\n",
      "Epoch 2823, Loss: 0.16982922703027725, Final Batch Loss: 0.07988415658473969\n",
      "Epoch 2824, Loss: 0.19866346567869186, Final Batch Loss: 0.1207091212272644\n",
      "Epoch 2825, Loss: 0.26544127613306046, Final Batch Loss: 0.16684021055698395\n",
      "Epoch 2826, Loss: 0.2098996564745903, Final Batch Loss: 0.09929630905389786\n",
      "Epoch 2827, Loss: 0.19567014276981354, Final Batch Loss: 0.07993755489587784\n",
      "Epoch 2828, Loss: 0.29044413566589355, Final Batch Loss: 0.12577871978282928\n",
      "Epoch 2829, Loss: 0.22897528856992722, Final Batch Loss: 0.08617978543043137\n",
      "Epoch 2830, Loss: 0.16735754162073135, Final Batch Loss: 0.10034342855215073\n",
      "Epoch 2831, Loss: 0.26052437722682953, Final Batch Loss: 0.13533444702625275\n",
      "Epoch 2832, Loss: 0.2930430397391319, Final Batch Loss: 0.1773681342601776\n",
      "Epoch 2833, Loss: 0.20707771182060242, Final Batch Loss: 0.08243480324745178\n",
      "Epoch 2834, Loss: 0.2712603360414505, Final Batch Loss: 0.13425806164741516\n",
      "Epoch 2835, Loss: 0.2363521084189415, Final Batch Loss: 0.12324736267328262\n",
      "Epoch 2836, Loss: 0.15791105479002, Final Batch Loss: 0.08316884189844131\n",
      "Epoch 2837, Loss: 0.2489476278424263, Final Batch Loss: 0.15113426744937897\n",
      "Epoch 2838, Loss: 0.27760645002126694, Final Batch Loss: 0.18805618584156036\n",
      "Epoch 2839, Loss: 0.23234664648771286, Final Batch Loss: 0.11579650640487671\n",
      "Epoch 2840, Loss: 0.215338334441185, Final Batch Loss: 0.09466185420751572\n",
      "Epoch 2841, Loss: 0.19931256026029587, Final Batch Loss: 0.07385782152414322\n",
      "Epoch 2842, Loss: 0.19352026656270027, Final Batch Loss: 0.055145952850580215\n",
      "Epoch 2843, Loss: 0.24483320116996765, Final Batch Loss: 0.12127146869897842\n",
      "Epoch 2844, Loss: 0.1748650260269642, Final Batch Loss: 0.11242107301950455\n",
      "Epoch 2845, Loss: 0.21617838740348816, Final Batch Loss: 0.12340013682842255\n",
      "Epoch 2846, Loss: 0.18214581161737442, Final Batch Loss: 0.06295561790466309\n",
      "Epoch 2847, Loss: 0.26035600900650024, Final Batch Loss: 0.12513448297977448\n",
      "Epoch 2848, Loss: 0.22541923075914383, Final Batch Loss: 0.12763777375221252\n",
      "Epoch 2849, Loss: 0.21037037670612335, Final Batch Loss: 0.12098011374473572\n",
      "Epoch 2850, Loss: 0.21849172562360764, Final Batch Loss: 0.08404276520013809\n",
      "Epoch 2851, Loss: 0.2174321413040161, Final Batch Loss: 0.12160372734069824\n",
      "Epoch 2852, Loss: 0.1517566777765751, Final Batch Loss: 0.059694040566682816\n",
      "Epoch 2853, Loss: 0.18766792118549347, Final Batch Loss: 0.08813883364200592\n",
      "Epoch 2854, Loss: 0.22180382907390594, Final Batch Loss: 0.1009761244058609\n",
      "Epoch 2855, Loss: 0.24883096665143967, Final Batch Loss: 0.11025495082139969\n",
      "Epoch 2856, Loss: 0.15311798453330994, Final Batch Loss: 0.0595986545085907\n",
      "Epoch 2857, Loss: 0.182573601603508, Final Batch Loss: 0.08487821370363235\n",
      "Epoch 2858, Loss: 0.19900045543909073, Final Batch Loss: 0.11065094918012619\n",
      "Epoch 2859, Loss: 0.2113083079457283, Final Batch Loss: 0.0869775339961052\n",
      "Epoch 2860, Loss: 0.2081936150789261, Final Batch Loss: 0.1097944900393486\n",
      "Epoch 2861, Loss: 0.26829414814710617, Final Batch Loss: 0.1519882082939148\n",
      "Epoch 2862, Loss: 0.2681235149502754, Final Batch Loss: 0.12451993674039841\n",
      "Epoch 2863, Loss: 0.24923089146614075, Final Batch Loss: 0.11317618191242218\n",
      "Epoch 2864, Loss: 0.3740169033408165, Final Batch Loss: 0.264925092458725\n",
      "Epoch 2865, Loss: 0.1931721791625023, Final Batch Loss: 0.1031256765127182\n",
      "Epoch 2866, Loss: 0.1664343699812889, Final Batch Loss: 0.09889183938503265\n",
      "Epoch 2867, Loss: 0.15755760669708252, Final Batch Loss: 0.0643560141324997\n",
      "Epoch 2868, Loss: 0.22517278045415878, Final Batch Loss: 0.12104474753141403\n",
      "Epoch 2869, Loss: 0.25209812074899673, Final Batch Loss: 0.1313309520483017\n",
      "Epoch 2870, Loss: 0.1977997124195099, Final Batch Loss: 0.08618463575839996\n",
      "Epoch 2871, Loss: 0.21669133752584457, Final Batch Loss: 0.10980658233165741\n",
      "Epoch 2872, Loss: 0.256458081305027, Final Batch Loss: 0.153995081782341\n",
      "Epoch 2873, Loss: 0.2148541361093521, Final Batch Loss: 0.0943983793258667\n",
      "Epoch 2874, Loss: 0.2571006119251251, Final Batch Loss: 0.1262422502040863\n",
      "Epoch 2875, Loss: 0.2772577553987503, Final Batch Loss: 0.15219062566757202\n",
      "Epoch 2876, Loss: 0.15206437557935715, Final Batch Loss: 0.058906957507133484\n",
      "Epoch 2877, Loss: 0.24301236867904663, Final Batch Loss: 0.12807796895503998\n",
      "Epoch 2878, Loss: 0.23673993349075317, Final Batch Loss: 0.08772054314613342\n",
      "Epoch 2879, Loss: 0.20736271142959595, Final Batch Loss: 0.10935556888580322\n",
      "Epoch 2880, Loss: 0.23912163078784943, Final Batch Loss: 0.12732449173927307\n",
      "Epoch 2881, Loss: 0.2571553662419319, Final Batch Loss: 0.098238505423069\n",
      "Epoch 2882, Loss: 0.2224387302994728, Final Batch Loss: 0.13574931025505066\n",
      "Epoch 2883, Loss: 0.20436836034059525, Final Batch Loss: 0.10204160213470459\n",
      "Epoch 2884, Loss: 0.22865097224712372, Final Batch Loss: 0.13988927006721497\n",
      "Epoch 2885, Loss: 0.23273077607154846, Final Batch Loss: 0.09180007874965668\n",
      "Epoch 2886, Loss: 0.1859264224767685, Final Batch Loss: 0.07215352356433868\n",
      "Epoch 2887, Loss: 0.314969077706337, Final Batch Loss: 0.15479117631912231\n",
      "Epoch 2888, Loss: 0.1906607449054718, Final Batch Loss: 0.11245445162057877\n",
      "Epoch 2889, Loss: 0.22122038155794144, Final Batch Loss: 0.09138099104166031\n",
      "Epoch 2890, Loss: 0.17730654776096344, Final Batch Loss: 0.09997542202472687\n",
      "Epoch 2891, Loss: 0.22281605750322342, Final Batch Loss: 0.11054907739162445\n",
      "Epoch 2892, Loss: 0.22365044057369232, Final Batch Loss: 0.15366949141025543\n",
      "Epoch 2893, Loss: 0.2743289992213249, Final Batch Loss: 0.1565493941307068\n",
      "Epoch 2894, Loss: 0.25869227945804596, Final Batch Loss: 0.14091770350933075\n",
      "Epoch 2895, Loss: 0.19737575948238373, Final Batch Loss: 0.0798569768667221\n",
      "Epoch 2896, Loss: 0.2781970202922821, Final Batch Loss: 0.12276306748390198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2897, Loss: 0.28409552574157715, Final Batch Loss: 0.14746461808681488\n",
      "Epoch 2898, Loss: 0.211666502058506, Final Batch Loss: 0.07497882097959518\n",
      "Epoch 2899, Loss: 0.2512062340974808, Final Batch Loss: 0.1154002845287323\n",
      "Epoch 2900, Loss: 0.17005926370620728, Final Batch Loss: 0.08033791929483414\n",
      "Epoch 2901, Loss: 0.19795097410678864, Final Batch Loss: 0.08506211638450623\n",
      "Epoch 2902, Loss: 0.1737472340464592, Final Batch Loss: 0.10191917419433594\n",
      "Epoch 2903, Loss: 0.2859172597527504, Final Batch Loss: 0.16097411513328552\n",
      "Epoch 2904, Loss: 0.4136098548769951, Final Batch Loss: 0.11675827950239182\n",
      "Epoch 2905, Loss: 0.2559818923473358, Final Batch Loss: 0.14647136628627777\n",
      "Epoch 2906, Loss: 0.3025871068239212, Final Batch Loss: 0.17513975501060486\n",
      "Epoch 2907, Loss: 0.2449629306793213, Final Batch Loss: 0.13303503394126892\n",
      "Epoch 2908, Loss: 0.18101979792118073, Final Batch Loss: 0.04736706614494324\n",
      "Epoch 2909, Loss: 0.1811530813574791, Final Batch Loss: 0.08172052353620529\n",
      "Epoch 2910, Loss: 0.18634767830371857, Final Batch Loss: 0.09239929169416428\n",
      "Epoch 2911, Loss: 0.20602820068597794, Final Batch Loss: 0.07599148899316788\n",
      "Epoch 2912, Loss: 0.3081270009279251, Final Batch Loss: 0.19438143074512482\n",
      "Epoch 2913, Loss: 0.2638106122612953, Final Batch Loss: 0.14163614809513092\n",
      "Epoch 2914, Loss: 0.23945047706365585, Final Batch Loss: 0.12264043837785721\n",
      "Epoch 2915, Loss: 0.1525900922715664, Final Batch Loss: 0.05966978892683983\n",
      "Epoch 2916, Loss: 0.2792775258421898, Final Batch Loss: 0.15736551582813263\n",
      "Epoch 2917, Loss: 0.2580517455935478, Final Batch Loss: 0.17640602588653564\n",
      "Epoch 2918, Loss: 0.14886751398444176, Final Batch Loss: 0.0532233901321888\n",
      "Epoch 2919, Loss: 0.22669751942157745, Final Batch Loss: 0.11882930994033813\n",
      "Epoch 2920, Loss: 0.23166199028491974, Final Batch Loss: 0.09103474020957947\n",
      "Epoch 2921, Loss: 0.18048644810914993, Final Batch Loss: 0.09466415643692017\n",
      "Epoch 2922, Loss: 0.23368490487337112, Final Batch Loss: 0.10168551653623581\n",
      "Epoch 2923, Loss: 0.19311925023794174, Final Batch Loss: 0.08101701736450195\n",
      "Epoch 2924, Loss: 0.3245360255241394, Final Batch Loss: 0.17286317050457\n",
      "Epoch 2925, Loss: 0.25145363062620163, Final Batch Loss: 0.08664638549089432\n",
      "Epoch 2926, Loss: 0.20374340564012527, Final Batch Loss: 0.08358574658632278\n",
      "Epoch 2927, Loss: 0.2005717158317566, Final Batch Loss: 0.08235848695039749\n",
      "Epoch 2928, Loss: 0.20623835921287537, Final Batch Loss: 0.08943091332912445\n",
      "Epoch 2929, Loss: 0.2416924387216568, Final Batch Loss: 0.1388697624206543\n",
      "Epoch 2930, Loss: 0.2183224856853485, Final Batch Loss: 0.0964396744966507\n",
      "Epoch 2931, Loss: 0.17451315373182297, Final Batch Loss: 0.10015714913606644\n",
      "Epoch 2932, Loss: 0.2448568195104599, Final Batch Loss: 0.13052624464035034\n",
      "Epoch 2933, Loss: 0.25847500562667847, Final Batch Loss: 0.13130469620227814\n",
      "Epoch 2934, Loss: 0.20480762422084808, Final Batch Loss: 0.08045346289873123\n",
      "Epoch 2935, Loss: 0.22921013832092285, Final Batch Loss: 0.11523053050041199\n",
      "Epoch 2936, Loss: 0.25302714109420776, Final Batch Loss: 0.12910299003124237\n",
      "Epoch 2937, Loss: 0.22222300618886948, Final Batch Loss: 0.13403283059597015\n",
      "Epoch 2938, Loss: 0.2548223212361336, Final Batch Loss: 0.17405842244625092\n",
      "Epoch 2939, Loss: 0.14708811044692993, Final Batch Loss: 0.0744461640715599\n",
      "Epoch 2940, Loss: 0.16062210127711296, Final Batch Loss: 0.10354753583669662\n",
      "Epoch 2941, Loss: 0.16083062067627907, Final Batch Loss: 0.05098584666848183\n",
      "Epoch 2942, Loss: 0.2960912361741066, Final Batch Loss: 0.11048518866300583\n",
      "Epoch 2943, Loss: 0.19705957919359207, Final Batch Loss: 0.09864451736211777\n",
      "Epoch 2944, Loss: 0.20379254966974258, Final Batch Loss: 0.10267903655767441\n",
      "Epoch 2945, Loss: 0.18585702031850815, Final Batch Loss: 0.08544763177633286\n",
      "Epoch 2946, Loss: 0.21693050116300583, Final Batch Loss: 0.09734554588794708\n",
      "Epoch 2947, Loss: 0.22427967190742493, Final Batch Loss: 0.12577201426029205\n",
      "Epoch 2948, Loss: 0.26020269095897675, Final Batch Loss: 0.12667229771614075\n",
      "Epoch 2949, Loss: 0.2411307841539383, Final Batch Loss: 0.0863562822341919\n",
      "Epoch 2950, Loss: 0.15247558429837227, Final Batch Loss: 0.06238158419728279\n",
      "Epoch 2951, Loss: 0.22033242881298065, Final Batch Loss: 0.11840701848268509\n",
      "Epoch 2952, Loss: 0.19332030415534973, Final Batch Loss: 0.10350383818149567\n",
      "Epoch 2953, Loss: 0.22880396246910095, Final Batch Loss: 0.1184854507446289\n",
      "Epoch 2954, Loss: 0.19613465666770935, Final Batch Loss: 0.10847331583499908\n",
      "Epoch 2955, Loss: 0.2251923531293869, Final Batch Loss: 0.15589819848537445\n",
      "Epoch 2956, Loss: 0.27029503881931305, Final Batch Loss: 0.13993914425373077\n",
      "Epoch 2957, Loss: 0.1726570427417755, Final Batch Loss: 0.08694067597389221\n",
      "Epoch 2958, Loss: 0.24753617495298386, Final Batch Loss: 0.16101393103599548\n",
      "Epoch 2959, Loss: 0.2538764551281929, Final Batch Loss: 0.14617960155010223\n",
      "Epoch 2960, Loss: 0.21203284710645676, Final Batch Loss: 0.0838695541024208\n",
      "Epoch 2961, Loss: 0.24113167077302933, Final Batch Loss: 0.12646451592445374\n",
      "Epoch 2962, Loss: 0.1988075003027916, Final Batch Loss: 0.1204184964299202\n",
      "Epoch 2963, Loss: 0.19425877928733826, Final Batch Loss: 0.08705475181341171\n",
      "Epoch 2964, Loss: 0.1972993239760399, Final Batch Loss: 0.08866480737924576\n",
      "Epoch 2965, Loss: 0.24219117313623428, Final Batch Loss: 0.11804936826229095\n",
      "Epoch 2966, Loss: 0.21453098952770233, Final Batch Loss: 0.12397851049900055\n",
      "Epoch 2967, Loss: 0.23609548807144165, Final Batch Loss: 0.09734155237674713\n",
      "Epoch 2968, Loss: 0.19713477045297623, Final Batch Loss: 0.08394152671098709\n",
      "Epoch 2969, Loss: 0.1983473151922226, Final Batch Loss: 0.10032369941473007\n",
      "Epoch 2970, Loss: 0.1780591383576393, Final Batch Loss: 0.09513139724731445\n",
      "Epoch 2971, Loss: 0.22066059708595276, Final Batch Loss: 0.1346444934606552\n",
      "Epoch 2972, Loss: 0.17997901141643524, Final Batch Loss: 0.10520769655704498\n",
      "Epoch 2973, Loss: 0.17068801820278168, Final Batch Loss: 0.09561119973659515\n",
      "Epoch 2974, Loss: 0.15970392525196075, Final Batch Loss: 0.06442153453826904\n",
      "Epoch 2975, Loss: 0.19923553615808487, Final Batch Loss: 0.11789954453706741\n",
      "Epoch 2976, Loss: 0.17620033770799637, Final Batch Loss: 0.07252185791730881\n",
      "Epoch 2977, Loss: 0.14514310657978058, Final Batch Loss: 0.0673515796661377\n",
      "Epoch 2978, Loss: 0.21979358047246933, Final Batch Loss: 0.1253732442855835\n",
      "Epoch 2979, Loss: 0.1901363581418991, Final Batch Loss: 0.07741720974445343\n",
      "Epoch 2980, Loss: 0.26119012385606766, Final Batch Loss: 0.15272025763988495\n",
      "Epoch 2981, Loss: 0.2090238481760025, Final Batch Loss: 0.11334475874900818\n",
      "Epoch 2982, Loss: 0.1780197024345398, Final Batch Loss: 0.05199228227138519\n",
      "Epoch 2983, Loss: 0.18563973903656006, Final Batch Loss: 0.08788443356752396\n",
      "Epoch 2984, Loss: 0.16131430864334106, Final Batch Loss: 0.08506976813077927\n",
      "Epoch 2985, Loss: 0.2756778746843338, Final Batch Loss: 0.12362195551395416\n",
      "Epoch 2986, Loss: 0.1775651052594185, Final Batch Loss: 0.07029601186513901\n",
      "Epoch 2987, Loss: 0.25670622289180756, Final Batch Loss: 0.12859627604484558\n",
      "Epoch 2988, Loss: 0.20161284506320953, Final Batch Loss: 0.11642272025346756\n",
      "Epoch 2989, Loss: 0.26463454216718674, Final Batch Loss: 0.09601403027772903\n",
      "Epoch 2990, Loss: 0.17593283206224442, Final Batch Loss: 0.10155058652162552\n",
      "Epoch 2991, Loss: 0.2397371008992195, Final Batch Loss: 0.09774021059274673\n",
      "Epoch 2992, Loss: 0.27455370128154755, Final Batch Loss: 0.16483716666698456\n",
      "Epoch 2993, Loss: 0.14504404366016388, Final Batch Loss: 0.06737712025642395\n",
      "Epoch 2994, Loss: 0.17594046890735626, Final Batch Loss: 0.09080620855093002\n",
      "Epoch 2995, Loss: 0.2421741932630539, Final Batch Loss: 0.14445598423480988\n",
      "Epoch 2996, Loss: 0.22427349537611008, Final Batch Loss: 0.09364815801382065\n",
      "Epoch 2997, Loss: 0.24808640033006668, Final Batch Loss: 0.14087386429309845\n",
      "Epoch 2998, Loss: 0.18236618489027023, Final Batch Loss: 0.1060633510351181\n",
      "Epoch 2999, Loss: 0.22100122272968292, Final Batch Loss: 0.11710314452648163\n",
      "Epoch 3000, Loss: 0.1581854447722435, Final Batch Loss: 0.07705415040254593\n",
      "Epoch 3001, Loss: 0.21637611836194992, Final Batch Loss: 0.13626742362976074\n",
      "Epoch 3002, Loss: 0.19967781007289886, Final Batch Loss: 0.09226048737764359\n",
      "Epoch 3003, Loss: 0.2061052992939949, Final Batch Loss: 0.09187633544206619\n",
      "Epoch 3004, Loss: 0.24615531414747238, Final Batch Loss: 0.10456565767526627\n",
      "Epoch 3005, Loss: 0.18597108125686646, Final Batch Loss: 0.11463938653469086\n",
      "Epoch 3006, Loss: 0.19604434818029404, Final Batch Loss: 0.1090240329504013\n",
      "Epoch 3007, Loss: 0.18406205624341965, Final Batch Loss: 0.10785075277090073\n",
      "Epoch 3008, Loss: 0.19716491550207138, Final Batch Loss: 0.12897686660289764\n",
      "Epoch 3009, Loss: 0.23722343891859055, Final Batch Loss: 0.13832861185073853\n",
      "Epoch 3010, Loss: 0.18292220681905746, Final Batch Loss: 0.09617317467927933\n",
      "Epoch 3011, Loss: 0.22401046752929688, Final Batch Loss: 0.11185072362422943\n",
      "Epoch 3012, Loss: 0.17209983617067337, Final Batch Loss: 0.06309399753808975\n",
      "Epoch 3013, Loss: 0.1719806045293808, Final Batch Loss: 0.08689112216234207\n",
      "Epoch 3014, Loss: 0.1740494668483734, Final Batch Loss: 0.08815298974514008\n",
      "Epoch 3015, Loss: 0.22796863317489624, Final Batch Loss: 0.11793364584445953\n",
      "Epoch 3016, Loss: 0.19090129435062408, Final Batch Loss: 0.10374326258897781\n",
      "Epoch 3017, Loss: 0.22521809488534927, Final Batch Loss: 0.1142745092511177\n",
      "Epoch 3018, Loss: 0.22944822907447815, Final Batch Loss: 0.11904125660657883\n",
      "Epoch 3019, Loss: 0.19308440387248993, Final Batch Loss: 0.07922688126564026\n",
      "Epoch 3020, Loss: 0.22809229791164398, Final Batch Loss: 0.13171148300170898\n",
      "Epoch 3021, Loss: 0.22136910259723663, Final Batch Loss: 0.11857713758945465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3022, Loss: 0.19264289736747742, Final Batch Loss: 0.1112588495016098\n",
      "Epoch 3023, Loss: 0.22923104465007782, Final Batch Loss: 0.08531178534030914\n",
      "Epoch 3024, Loss: 0.25477275252342224, Final Batch Loss: 0.13131870329380035\n",
      "Epoch 3025, Loss: 0.30000628530979156, Final Batch Loss: 0.21881389617919922\n",
      "Epoch 3026, Loss: 0.23416195064783096, Final Batch Loss: 0.13887405395507812\n",
      "Epoch 3027, Loss: 0.17198847979307175, Final Batch Loss: 0.07094717770814896\n",
      "Epoch 3028, Loss: 0.2641167715191841, Final Batch Loss: 0.16241294145584106\n",
      "Epoch 3029, Loss: 0.13061460107564926, Final Batch Loss: 0.04415452480316162\n",
      "Epoch 3030, Loss: 0.20908628404140472, Final Batch Loss: 0.11363107711076736\n",
      "Epoch 3031, Loss: 0.2518361806869507, Final Batch Loss: 0.07423290610313416\n",
      "Epoch 3032, Loss: 0.2146076261997223, Final Batch Loss: 0.10054923593997955\n",
      "Epoch 3033, Loss: 0.20395982265472412, Final Batch Loss: 0.08580218255519867\n",
      "Epoch 3034, Loss: 0.1853797286748886, Final Batch Loss: 0.07648041844367981\n",
      "Epoch 3035, Loss: 0.17729070782661438, Final Batch Loss: 0.09320606291294098\n",
      "Epoch 3036, Loss: 0.22396788746118546, Final Batch Loss: 0.12551124393939972\n",
      "Epoch 3037, Loss: 0.1893683522939682, Final Batch Loss: 0.10275781154632568\n",
      "Epoch 3038, Loss: 0.2272024303674698, Final Batch Loss: 0.1660672277212143\n",
      "Epoch 3039, Loss: 0.1675844043493271, Final Batch Loss: 0.0760684609413147\n",
      "Epoch 3040, Loss: 0.24759319424629211, Final Batch Loss: 0.1278495043516159\n",
      "Epoch 3041, Loss: 0.21693110466003418, Final Batch Loss: 0.10405509173870087\n",
      "Epoch 3042, Loss: 0.1992490515112877, Final Batch Loss: 0.12035831809043884\n",
      "Epoch 3043, Loss: 0.17105986177921295, Final Batch Loss: 0.054573483765125275\n",
      "Epoch 3044, Loss: 0.22238635271787643, Final Batch Loss: 0.13659845292568207\n",
      "Epoch 3045, Loss: 0.18794800341129303, Final Batch Loss: 0.08490978926420212\n",
      "Epoch 3046, Loss: 0.2221435010433197, Final Batch Loss: 0.09926024079322815\n",
      "Epoch 3047, Loss: 0.19285789877176285, Final Batch Loss: 0.09769769012928009\n",
      "Epoch 3048, Loss: 0.21667186170816422, Final Batch Loss: 0.11896602064371109\n",
      "Epoch 3049, Loss: 0.20300788432359695, Final Batch Loss: 0.10578641295433044\n",
      "Epoch 3050, Loss: 0.18749305605888367, Final Batch Loss: 0.12173976004123688\n",
      "Epoch 3051, Loss: 0.14141279458999634, Final Batch Loss: 0.05046704411506653\n",
      "Epoch 3052, Loss: 0.2551795393228531, Final Batch Loss: 0.14742083847522736\n",
      "Epoch 3053, Loss: 0.24583178013563156, Final Batch Loss: 0.1492123156785965\n",
      "Epoch 3054, Loss: 0.16442668437957764, Final Batch Loss: 0.07781147211790085\n",
      "Epoch 3055, Loss: 0.20470326393842697, Final Batch Loss: 0.10114736109972\n",
      "Epoch 3056, Loss: 0.17562450468540192, Final Batch Loss: 0.09283439069986343\n",
      "Epoch 3057, Loss: 0.24765217304229736, Final Batch Loss: 0.11687536537647247\n",
      "Epoch 3058, Loss: 0.2576071694493294, Final Batch Loss: 0.12313469499349594\n",
      "Epoch 3059, Loss: 0.21513722836971283, Final Batch Loss: 0.08032464981079102\n",
      "Epoch 3060, Loss: 0.2386745661497116, Final Batch Loss: 0.12455404549837112\n",
      "Epoch 3061, Loss: 0.17547796666622162, Final Batch Loss: 0.07533935457468033\n",
      "Epoch 3062, Loss: 0.1933480203151703, Final Batch Loss: 0.10494864732027054\n",
      "Epoch 3063, Loss: 0.23409336060285568, Final Batch Loss: 0.11057593673467636\n",
      "Epoch 3064, Loss: 0.17283155024051666, Final Batch Loss: 0.06956249475479126\n",
      "Epoch 3065, Loss: 0.23440705984830856, Final Batch Loss: 0.10702451318502426\n",
      "Epoch 3066, Loss: 0.19087010622024536, Final Batch Loss: 0.09501963108778\n",
      "Epoch 3067, Loss: 0.26110459864139557, Final Batch Loss: 0.13382069766521454\n",
      "Epoch 3068, Loss: 0.19692346453666687, Final Batch Loss: 0.08954497426748276\n",
      "Epoch 3069, Loss: 0.17974171787500381, Final Batch Loss: 0.0881209522485733\n",
      "Epoch 3070, Loss: 0.16911863163113594, Final Batch Loss: 0.05691687390208244\n",
      "Epoch 3071, Loss: 0.146625354886055, Final Batch Loss: 0.06958018243312836\n",
      "Epoch 3072, Loss: 0.18830721825361252, Final Batch Loss: 0.10049906373023987\n",
      "Epoch 3073, Loss: 0.2282506451010704, Final Batch Loss: 0.11977771669626236\n",
      "Epoch 3074, Loss: 0.20809299498796463, Final Batch Loss: 0.09789077937602997\n",
      "Epoch 3075, Loss: 0.2050849124789238, Final Batch Loss: 0.09151876717805862\n",
      "Epoch 3076, Loss: 0.16489202901721, Final Batch Loss: 0.05938355252146721\n",
      "Epoch 3077, Loss: 0.32454125583171844, Final Batch Loss: 0.23782041668891907\n",
      "Epoch 3078, Loss: 0.2049366757273674, Final Batch Loss: 0.11551696807146072\n",
      "Epoch 3079, Loss: 0.23019401729106903, Final Batch Loss: 0.07662180066108704\n",
      "Epoch 3080, Loss: 0.2762778252363205, Final Batch Loss: 0.17676793038845062\n",
      "Epoch 3081, Loss: 0.25473176687955856, Final Batch Loss: 0.12226096540689468\n",
      "Epoch 3082, Loss: 0.23010793328285217, Final Batch Loss: 0.13685517013072968\n",
      "Epoch 3083, Loss: 0.19529783353209496, Final Batch Loss: 0.1361222267150879\n",
      "Epoch 3084, Loss: 0.2147849276661873, Final Batch Loss: 0.11313360184431076\n",
      "Epoch 3085, Loss: 0.22250252962112427, Final Batch Loss: 0.09131434559822083\n",
      "Epoch 3086, Loss: 0.24891343712806702, Final Batch Loss: 0.13587376475334167\n",
      "Epoch 3087, Loss: 0.22506006807088852, Final Batch Loss: 0.10818367451429367\n",
      "Epoch 3088, Loss: 0.22702179849147797, Final Batch Loss: 0.132797971367836\n",
      "Epoch 3089, Loss: 0.17126696556806564, Final Batch Loss: 0.07176246494054794\n",
      "Epoch 3090, Loss: 0.1985049992799759, Final Batch Loss: 0.10663679242134094\n",
      "Epoch 3091, Loss: 0.1582464650273323, Final Batch Loss: 0.06164366006851196\n",
      "Epoch 3092, Loss: 0.21192875504493713, Final Batch Loss: 0.09217127412557602\n",
      "Epoch 3093, Loss: 0.26731593906879425, Final Batch Loss: 0.13814280927181244\n",
      "Epoch 3094, Loss: 0.2693393975496292, Final Batch Loss: 0.1573089212179184\n",
      "Epoch 3095, Loss: 0.2437959834933281, Final Batch Loss: 0.1130792424082756\n",
      "Epoch 3096, Loss: 0.14535818994045258, Final Batch Loss: 0.0495857372879982\n",
      "Epoch 3097, Loss: 0.13459044322371483, Final Batch Loss: 0.07278746366500854\n",
      "Epoch 3098, Loss: 0.2024722471833229, Final Batch Loss: 0.13628844916820526\n",
      "Epoch 3099, Loss: 0.20852067321538925, Final Batch Loss: 0.109552763402462\n",
      "Epoch 3100, Loss: 0.15501084178686142, Final Batch Loss: 0.06894787400960922\n",
      "Epoch 3101, Loss: 0.27446189522743225, Final Batch Loss: 0.18848927319049835\n",
      "Epoch 3102, Loss: 0.23409095406532288, Final Batch Loss: 0.14627568423748016\n",
      "Epoch 3103, Loss: 0.1827184110879898, Final Batch Loss: 0.08845075964927673\n",
      "Epoch 3104, Loss: 0.19742970541119576, Final Batch Loss: 0.13830873370170593\n",
      "Epoch 3105, Loss: 0.2028006911277771, Final Batch Loss: 0.09700369834899902\n",
      "Epoch 3106, Loss: 0.19966824352741241, Final Batch Loss: 0.09415312856435776\n",
      "Epoch 3107, Loss: 0.1781584918498993, Final Batch Loss: 0.08561400324106216\n",
      "Epoch 3108, Loss: 0.13960779458284378, Final Batch Loss: 0.06807205080986023\n",
      "Epoch 3109, Loss: 0.20850174874067307, Final Batch Loss: 0.07854344695806503\n",
      "Epoch 3110, Loss: 0.19467918574810028, Final Batch Loss: 0.0741596668958664\n",
      "Epoch 3111, Loss: 0.13043469935655594, Final Batch Loss: 0.0759015828371048\n",
      "Epoch 3112, Loss: 0.17260663211345673, Final Batch Loss: 0.09064972400665283\n",
      "Epoch 3113, Loss: 0.26901766657829285, Final Batch Loss: 0.1899353712797165\n",
      "Epoch 3114, Loss: 0.1918150708079338, Final Batch Loss: 0.0819888487458229\n",
      "Epoch 3115, Loss: 0.22284366190433502, Final Batch Loss: 0.13820461928844452\n",
      "Epoch 3116, Loss: 0.2058359459042549, Final Batch Loss: 0.09820903837680817\n",
      "Epoch 3117, Loss: 0.2309032380580902, Final Batch Loss: 0.0757371038198471\n",
      "Epoch 3118, Loss: 0.2061593011021614, Final Batch Loss: 0.09996359050273895\n",
      "Epoch 3119, Loss: 0.14699828624725342, Final Batch Loss: 0.07929178327322006\n",
      "Epoch 3120, Loss: 0.15731249749660492, Final Batch Loss: 0.07671856880187988\n",
      "Epoch 3121, Loss: 0.16696815192699432, Final Batch Loss: 0.0481201633810997\n",
      "Epoch 3122, Loss: 0.2611052989959717, Final Batch Loss: 0.13205182552337646\n",
      "Epoch 3123, Loss: 0.15616969019174576, Final Batch Loss: 0.08444585651159286\n",
      "Epoch 3124, Loss: 0.2663237228989601, Final Batch Loss: 0.12348607927560806\n",
      "Epoch 3125, Loss: 0.2053472325205803, Final Batch Loss: 0.09796922653913498\n",
      "Epoch 3126, Loss: 0.18880097568035126, Final Batch Loss: 0.06956162303686142\n",
      "Epoch 3127, Loss: 0.22732770442962646, Final Batch Loss: 0.11421544849872589\n",
      "Epoch 3128, Loss: 0.16119755804538727, Final Batch Loss: 0.09822050482034683\n",
      "Epoch 3129, Loss: 0.21042855829000473, Final Batch Loss: 0.09424949437379837\n",
      "Epoch 3130, Loss: 0.1617180034518242, Final Batch Loss: 0.07123685628175735\n",
      "Epoch 3131, Loss: 0.23927809298038483, Final Batch Loss: 0.1270623356103897\n",
      "Epoch 3132, Loss: 0.27951668202877045, Final Batch Loss: 0.13996361196041107\n",
      "Epoch 3133, Loss: 0.14831069484353065, Final Batch Loss: 0.05528580769896507\n",
      "Epoch 3134, Loss: 0.2519262209534645, Final Batch Loss: 0.10086382180452347\n",
      "Epoch 3135, Loss: 0.21388991177082062, Final Batch Loss: 0.12965241074562073\n",
      "Epoch 3136, Loss: 0.23513583838939667, Final Batch Loss: 0.1352136880159378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3137, Loss: 0.22653505206108093, Final Batch Loss: 0.1195748969912529\n",
      "Epoch 3138, Loss: 0.3344023674726486, Final Batch Loss: 0.1537022590637207\n",
      "Epoch 3139, Loss: 0.21034224331378937, Final Batch Loss: 0.09526623785495758\n",
      "Epoch 3140, Loss: 0.19029556959867477, Final Batch Loss: 0.09865956008434296\n",
      "Epoch 3141, Loss: 0.18681690841913223, Final Batch Loss: 0.07777578383684158\n",
      "Epoch 3142, Loss: 0.19368531554937363, Final Batch Loss: 0.1175631433725357\n",
      "Epoch 3143, Loss: 0.29922744631767273, Final Batch Loss: 0.15453863143920898\n",
      "Epoch 3144, Loss: 0.1371566466987133, Final Batch Loss: 0.04742995277047157\n",
      "Epoch 3145, Loss: 0.21931439638137817, Final Batch Loss: 0.13558432459831238\n",
      "Epoch 3146, Loss: 0.1930800974369049, Final Batch Loss: 0.07083314657211304\n",
      "Epoch 3147, Loss: 0.17881430685520172, Final Batch Loss: 0.09818460792303085\n",
      "Epoch 3148, Loss: 0.22416744381189346, Final Batch Loss: 0.11333055794239044\n",
      "Epoch 3149, Loss: 0.16795026510953903, Final Batch Loss: 0.08109741657972336\n",
      "Epoch 3150, Loss: 0.13380509614944458, Final Batch Loss: 0.06284550577402115\n",
      "Epoch 3151, Loss: 0.18047960847616196, Final Batch Loss: 0.06636086851358414\n",
      "Epoch 3152, Loss: 0.1870170682668686, Final Batch Loss: 0.08841247111558914\n",
      "Epoch 3153, Loss: 0.14449284598231316, Final Batch Loss: 0.057251062244176865\n",
      "Epoch 3154, Loss: 0.3542046695947647, Final Batch Loss: 0.22332048416137695\n",
      "Epoch 3155, Loss: 0.17360761016607285, Final Batch Loss: 0.07835368067026138\n",
      "Epoch 3156, Loss: 0.19655879586935043, Final Batch Loss: 0.12153257429599762\n",
      "Epoch 3157, Loss: 0.1522037833929062, Final Batch Loss: 0.08446528762578964\n",
      "Epoch 3158, Loss: 0.2350383996963501, Final Batch Loss: 0.1482708752155304\n",
      "Epoch 3159, Loss: 0.17129891365766525, Final Batch Loss: 0.09370040148496628\n",
      "Epoch 3160, Loss: 0.18770326673984528, Final Batch Loss: 0.09946167469024658\n",
      "Epoch 3161, Loss: 0.1954967938363552, Final Batch Loss: 0.05657840892672539\n",
      "Epoch 3162, Loss: 0.2298099473118782, Final Batch Loss: 0.11912108212709427\n",
      "Epoch 3163, Loss: 0.1535913646221161, Final Batch Loss: 0.06578667461872101\n",
      "Epoch 3164, Loss: 0.16560854762792587, Final Batch Loss: 0.08638737350702286\n",
      "Epoch 3165, Loss: 0.20495739579200745, Final Batch Loss: 0.10556551814079285\n",
      "Epoch 3166, Loss: 0.21789910644292831, Final Batch Loss: 0.14411741495132446\n",
      "Epoch 3167, Loss: 0.21356871724128723, Final Batch Loss: 0.1033644750714302\n",
      "Epoch 3168, Loss: 0.14648842439055443, Final Batch Loss: 0.053252849727869034\n",
      "Epoch 3169, Loss: 0.14799317717552185, Final Batch Loss: 0.06698454171419144\n",
      "Epoch 3170, Loss: 0.1770206168293953, Final Batch Loss: 0.1056920737028122\n",
      "Epoch 3171, Loss: 0.2093525230884552, Final Batch Loss: 0.11022386699914932\n",
      "Epoch 3172, Loss: 0.2587529867887497, Final Batch Loss: 0.16419392824172974\n",
      "Epoch 3173, Loss: 0.26227695494890213, Final Batch Loss: 0.16528549790382385\n",
      "Epoch 3174, Loss: 0.1970440074801445, Final Batch Loss: 0.11468564718961716\n",
      "Epoch 3175, Loss: 0.16780898720026016, Final Batch Loss: 0.10111608356237411\n",
      "Epoch 3176, Loss: 0.19291214644908905, Final Batch Loss: 0.10834700614213943\n",
      "Epoch 3177, Loss: 0.14602825418114662, Final Batch Loss: 0.0918029174208641\n",
      "Epoch 3178, Loss: 0.12821386009454727, Final Batch Loss: 0.04926437884569168\n",
      "Epoch 3179, Loss: 0.23590123653411865, Final Batch Loss: 0.15192672610282898\n",
      "Epoch 3180, Loss: 0.2288995012640953, Final Batch Loss: 0.1203494444489479\n",
      "Epoch 3181, Loss: 0.1774957776069641, Final Batch Loss: 0.09403684735298157\n",
      "Epoch 3182, Loss: 0.2605976313352585, Final Batch Loss: 0.12878401577472687\n",
      "Epoch 3183, Loss: 0.13431035727262497, Final Batch Loss: 0.06191881746053696\n",
      "Epoch 3184, Loss: 0.18990418314933777, Final Batch Loss: 0.10079976916313171\n",
      "Epoch 3185, Loss: 0.1740664839744568, Final Batch Loss: 0.09156069159507751\n",
      "Epoch 3186, Loss: 0.18367813527584076, Final Batch Loss: 0.07202009856700897\n",
      "Epoch 3187, Loss: 0.21384048461914062, Final Batch Loss: 0.11968503892421722\n",
      "Epoch 3188, Loss: 0.12570266798138618, Final Batch Loss: 0.052002694457769394\n",
      "Epoch 3189, Loss: 0.2437688410282135, Final Batch Loss: 0.11458669602870941\n",
      "Epoch 3190, Loss: 0.13146062567830086, Final Batch Loss: 0.04914617910981178\n",
      "Epoch 3191, Loss: 0.16291673481464386, Final Batch Loss: 0.07505325227975845\n",
      "Epoch 3192, Loss: 0.1448054015636444, Final Batch Loss: 0.08088697493076324\n",
      "Epoch 3193, Loss: 0.20680323988199234, Final Batch Loss: 0.1148165836930275\n",
      "Epoch 3194, Loss: 0.19313745200634003, Final Batch Loss: 0.0822967141866684\n",
      "Epoch 3195, Loss: 0.12161072343587875, Final Batch Loss: 0.05772925168275833\n",
      "Epoch 3196, Loss: 0.18171922862529755, Final Batch Loss: 0.07630150765180588\n",
      "Epoch 3197, Loss: 0.17421654611825943, Final Batch Loss: 0.1166730523109436\n",
      "Epoch 3198, Loss: 0.2513761892914772, Final Batch Loss: 0.17171157896518707\n",
      "Epoch 3199, Loss: 0.13490097224712372, Final Batch Loss: 0.04600421339273453\n",
      "Epoch 3200, Loss: 0.1895451918244362, Final Batch Loss: 0.11176048964262009\n",
      "Epoch 3201, Loss: 0.20879070460796356, Final Batch Loss: 0.09666983038187027\n",
      "Epoch 3202, Loss: 0.19277582317590714, Final Batch Loss: 0.06217718869447708\n",
      "Epoch 3203, Loss: 0.1905551627278328, Final Batch Loss: 0.060641758143901825\n",
      "Epoch 3204, Loss: 0.2126368284225464, Final Batch Loss: 0.11505711078643799\n",
      "Epoch 3205, Loss: 0.20368105173110962, Final Batch Loss: 0.0807412639260292\n",
      "Epoch 3206, Loss: 0.14381100237369537, Final Batch Loss: 0.05637939274311066\n",
      "Epoch 3207, Loss: 0.21185820549726486, Final Batch Loss: 0.10306376218795776\n",
      "Epoch 3208, Loss: 0.13219375535845757, Final Batch Loss: 0.05803860351443291\n",
      "Epoch 3209, Loss: 0.17510973662137985, Final Batch Loss: 0.05550604313611984\n",
      "Epoch 3210, Loss: 0.18037323653697968, Final Batch Loss: 0.0687926635146141\n",
      "Epoch 3211, Loss: 0.14420929178595543, Final Batch Loss: 0.050610799342393875\n",
      "Epoch 3212, Loss: 0.15051879733800888, Final Batch Loss: 0.061307892203330994\n",
      "Epoch 3213, Loss: 0.2264922633767128, Final Batch Loss: 0.15515725314617157\n",
      "Epoch 3214, Loss: 0.18100212514400482, Final Batch Loss: 0.08456441015005112\n",
      "Epoch 3215, Loss: 0.18173376843333244, Final Batch Loss: 0.12614761292934418\n",
      "Epoch 3216, Loss: 0.17063766717910767, Final Batch Loss: 0.096394382417202\n",
      "Epoch 3217, Loss: 0.23781375586986542, Final Batch Loss: 0.09935128688812256\n",
      "Epoch 3218, Loss: 0.1380748674273491, Final Batch Loss: 0.07996026426553726\n",
      "Epoch 3219, Loss: 0.22077953070402145, Final Batch Loss: 0.1497221440076828\n",
      "Epoch 3220, Loss: 0.1075613722205162, Final Batch Loss: 0.034864723682403564\n",
      "Epoch 3221, Loss: 0.19282198697328568, Final Batch Loss: 0.10658609867095947\n",
      "Epoch 3222, Loss: 0.22266190499067307, Final Batch Loss: 0.09767964482307434\n",
      "Epoch 3223, Loss: 0.16560780256986618, Final Batch Loss: 0.0752292349934578\n",
      "Epoch 3224, Loss: 0.19237257540225983, Final Batch Loss: 0.09261149913072586\n",
      "Epoch 3225, Loss: 0.23024340718984604, Final Batch Loss: 0.07914527505636215\n",
      "Epoch 3226, Loss: 0.13244713097810745, Final Batch Loss: 0.038091614842414856\n",
      "Epoch 3227, Loss: 0.1427309326827526, Final Batch Loss: 0.06100570037961006\n",
      "Epoch 3228, Loss: 0.1729239523410797, Final Batch Loss: 0.09176825731992722\n",
      "Epoch 3229, Loss: 0.17117252573370934, Final Batch Loss: 0.04151761159300804\n",
      "Epoch 3230, Loss: 0.17597510665655136, Final Batch Loss: 0.09802006185054779\n",
      "Epoch 3231, Loss: 0.19782309979200363, Final Batch Loss: 0.11283650249242783\n",
      "Epoch 3232, Loss: 0.17812873423099518, Final Batch Loss: 0.09605833142995834\n",
      "Epoch 3233, Loss: 0.2248610556125641, Final Batch Loss: 0.09281660616397858\n",
      "Epoch 3234, Loss: 0.25549817085266113, Final Batch Loss: 0.08279812335968018\n",
      "Epoch 3235, Loss: 0.18941838294267654, Final Batch Loss: 0.07670576125383377\n",
      "Epoch 3236, Loss: 0.23134121298789978, Final Batch Loss: 0.09047599136829376\n",
      "Epoch 3237, Loss: 0.24348679929971695, Final Batch Loss: 0.1575220227241516\n",
      "Epoch 3238, Loss: 0.2637491077184677, Final Batch Loss: 0.15625683963298798\n",
      "Epoch 3239, Loss: 0.25653471797704697, Final Batch Loss: 0.1574706733226776\n",
      "Epoch 3240, Loss: 0.2592066302895546, Final Batch Loss: 0.15686871111392975\n",
      "Epoch 3241, Loss: 0.18221525102853775, Final Batch Loss: 0.05662038177251816\n",
      "Epoch 3242, Loss: 0.17660237103700638, Final Batch Loss: 0.07614527642726898\n",
      "Epoch 3243, Loss: 0.12692305073142052, Final Batch Loss: 0.05064292624592781\n",
      "Epoch 3244, Loss: 0.17294370383024216, Final Batch Loss: 0.07475082576274872\n",
      "Epoch 3245, Loss: 0.21272090077400208, Final Batch Loss: 0.07798503339290619\n",
      "Epoch 3246, Loss: 0.2555980384349823, Final Batch Loss: 0.15901286900043488\n",
      "Epoch 3247, Loss: 0.15898696333169937, Final Batch Loss: 0.09458225220441818\n",
      "Epoch 3248, Loss: 0.16941208392381668, Final Batch Loss: 0.0821734145283699\n",
      "Epoch 3249, Loss: 0.202815480530262, Final Batch Loss: 0.1121310368180275\n",
      "Epoch 3250, Loss: 0.2573748305439949, Final Batch Loss: 0.12272336333990097\n",
      "Epoch 3251, Loss: 0.17125551402568817, Final Batch Loss: 0.08800407499074936\n",
      "Epoch 3252, Loss: 0.21718992292881012, Final Batch Loss: 0.1402457058429718\n",
      "Epoch 3253, Loss: 0.15706366300582886, Final Batch Loss: 0.07709731161594391\n",
      "Epoch 3254, Loss: 0.26867033541202545, Final Batch Loss: 0.10491055250167847\n",
      "Epoch 3255, Loss: 0.18688151240348816, Final Batch Loss: 0.0774608626961708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3256, Loss: 0.18723484873771667, Final Batch Loss: 0.08930539339780807\n",
      "Epoch 3257, Loss: 0.2101874127984047, Final Batch Loss: 0.09976084530353546\n",
      "Epoch 3258, Loss: 0.18166671693325043, Final Batch Loss: 0.08616025745868683\n",
      "Epoch 3259, Loss: 0.23471424728631973, Final Batch Loss: 0.13954423367977142\n",
      "Epoch 3260, Loss: 0.217307411134243, Final Batch Loss: 0.10416138917207718\n",
      "Epoch 3261, Loss: 0.14677158743143082, Final Batch Loss: 0.0835152268409729\n",
      "Epoch 3262, Loss: 0.13981114700436592, Final Batch Loss: 0.08663476258516312\n",
      "Epoch 3263, Loss: 0.12718045338988304, Final Batch Loss: 0.07127247750759125\n",
      "Epoch 3264, Loss: 0.1549450233578682, Final Batch Loss: 0.07397642731666565\n",
      "Epoch 3265, Loss: 0.19103537499904633, Final Batch Loss: 0.1135551854968071\n",
      "Epoch 3266, Loss: 0.15751167014241219, Final Batch Loss: 0.056556593626737595\n",
      "Epoch 3267, Loss: 0.1261417306959629, Final Batch Loss: 0.05243411287665367\n",
      "Epoch 3268, Loss: 0.10072671622037888, Final Batch Loss: 0.044292110949754715\n",
      "Epoch 3269, Loss: 0.1376243494451046, Final Batch Loss: 0.08621387928724289\n",
      "Epoch 3270, Loss: 0.2960110604763031, Final Batch Loss: 0.10185393691062927\n",
      "Epoch 3271, Loss: 0.13609080016613007, Final Batch Loss: 0.06787116080522537\n",
      "Epoch 3272, Loss: 0.13248944468796253, Final Batch Loss: 0.023857327178120613\n",
      "Epoch 3273, Loss: 0.1315605416893959, Final Batch Loss: 0.07022929191589355\n",
      "Epoch 3274, Loss: 0.26433065533638, Final Batch Loss: 0.12972792983055115\n",
      "Epoch 3275, Loss: 0.13758335635066032, Final Batch Loss: 0.05771198496222496\n",
      "Epoch 3276, Loss: 0.2592470496892929, Final Batch Loss: 0.13210941851139069\n",
      "Epoch 3277, Loss: 0.15853028744459152, Final Batch Loss: 0.07835009694099426\n",
      "Epoch 3278, Loss: 0.13570895791053772, Final Batch Loss: 0.0528365895152092\n",
      "Epoch 3279, Loss: 0.17692048847675323, Final Batch Loss: 0.06753895431756973\n",
      "Epoch 3280, Loss: 0.18231955543160439, Final Batch Loss: 0.12327825278043747\n",
      "Epoch 3281, Loss: 0.20864808559417725, Final Batch Loss: 0.11100007593631744\n",
      "Epoch 3282, Loss: 0.14222565293312073, Final Batch Loss: 0.07588300108909607\n",
      "Epoch 3283, Loss: 0.17867201566696167, Final Batch Loss: 0.09595642238855362\n",
      "Epoch 3284, Loss: 0.2379554957151413, Final Batch Loss: 0.15857984125614166\n",
      "Epoch 3285, Loss: 0.18634707108139992, Final Batch Loss: 0.06009567156434059\n",
      "Epoch 3286, Loss: 0.1540171504020691, Final Batch Loss: 0.07906283438205719\n",
      "Epoch 3287, Loss: 0.29691843688488007, Final Batch Loss: 0.2205665558576584\n",
      "Epoch 3288, Loss: 0.17475023120641708, Final Batch Loss: 0.09701352566480637\n",
      "Epoch 3289, Loss: 0.1760786846280098, Final Batch Loss: 0.09802072495222092\n",
      "Epoch 3290, Loss: 0.22437379509210587, Final Batch Loss: 0.10962092131376266\n",
      "Epoch 3291, Loss: 0.22634827345609665, Final Batch Loss: 0.16604092717170715\n",
      "Epoch 3292, Loss: 0.15010680258274078, Final Batch Loss: 0.06515558063983917\n",
      "Epoch 3293, Loss: 0.1899646259844303, Final Batch Loss: 0.13738536834716797\n",
      "Epoch 3294, Loss: 0.1969519630074501, Final Batch Loss: 0.08375375717878342\n",
      "Epoch 3295, Loss: 0.26084865629673004, Final Batch Loss: 0.12443268299102783\n",
      "Epoch 3296, Loss: 0.20008114725351334, Final Batch Loss: 0.09367837011814117\n",
      "Epoch 3297, Loss: 0.2622883841395378, Final Batch Loss: 0.11805879324674606\n",
      "Epoch 3298, Loss: 0.1451178565621376, Final Batch Loss: 0.07634624093770981\n",
      "Epoch 3299, Loss: 0.21592482924461365, Final Batch Loss: 0.11110638827085495\n",
      "Epoch 3300, Loss: 0.19953066110610962, Final Batch Loss: 0.08260834962129593\n",
      "Epoch 3301, Loss: 0.20899758487939835, Final Batch Loss: 0.06820901483297348\n",
      "Epoch 3302, Loss: 0.15119611099362373, Final Batch Loss: 0.05128951743245125\n",
      "Epoch 3303, Loss: 0.20647405833005905, Final Batch Loss: 0.12138941884040833\n",
      "Epoch 3304, Loss: 0.14904017001390457, Final Batch Loss: 0.0597408264875412\n",
      "Epoch 3305, Loss: 0.14791972190141678, Final Batch Loss: 0.07318931818008423\n",
      "Epoch 3306, Loss: 0.15636080875992775, Final Batch Loss: 0.0606725849211216\n",
      "Epoch 3307, Loss: 0.21325799822807312, Final Batch Loss: 0.12150140851736069\n",
      "Epoch 3308, Loss: 0.15761782228946686, Final Batch Loss: 0.07216916233301163\n",
      "Epoch 3309, Loss: 0.1468605287373066, Final Batch Loss: 0.08469317108392715\n",
      "Epoch 3310, Loss: 0.20947765558958054, Final Batch Loss: 0.11322832852602005\n",
      "Epoch 3311, Loss: 0.23273274302482605, Final Batch Loss: 0.12029917538166046\n",
      "Epoch 3312, Loss: 0.16882076859474182, Final Batch Loss: 0.08415313810110092\n",
      "Epoch 3313, Loss: 0.25586917996406555, Final Batch Loss: 0.1542358696460724\n",
      "Epoch 3314, Loss: 0.18968646973371506, Final Batch Loss: 0.0683257132768631\n",
      "Epoch 3315, Loss: 0.1387900710105896, Final Batch Loss: 0.055498696863651276\n",
      "Epoch 3316, Loss: 0.24335049837827682, Final Batch Loss: 0.11933788657188416\n",
      "Epoch 3317, Loss: 0.1882806196808815, Final Batch Loss: 0.10416624695062637\n",
      "Epoch 3318, Loss: 0.13279062509536743, Final Batch Loss: 0.06187205761671066\n",
      "Epoch 3319, Loss: 0.14449647814035416, Final Batch Loss: 0.06791692227125168\n",
      "Epoch 3320, Loss: 0.13943574577569962, Final Batch Loss: 0.07802114635705948\n",
      "Epoch 3321, Loss: 0.21071425825357437, Final Batch Loss: 0.11765602231025696\n",
      "Epoch 3322, Loss: 0.17226486653089523, Final Batch Loss: 0.06996852904558182\n",
      "Epoch 3323, Loss: 0.19912699609994888, Final Batch Loss: 0.08911367505788803\n",
      "Epoch 3324, Loss: 0.1604236662387848, Final Batch Loss: 0.0944899469614029\n",
      "Epoch 3325, Loss: 0.12933048233389854, Final Batch Loss: 0.052021194249391556\n",
      "Epoch 3326, Loss: 0.1853775605559349, Final Batch Loss: 0.1005026251077652\n",
      "Epoch 3327, Loss: 0.17218337208032608, Final Batch Loss: 0.0785246416926384\n",
      "Epoch 3328, Loss: 0.1820473074913025, Final Batch Loss: 0.07873429358005524\n",
      "Epoch 3329, Loss: 0.18619702756404877, Final Batch Loss: 0.07505989074707031\n",
      "Epoch 3330, Loss: 0.1760665774345398, Final Batch Loss: 0.10330328345298767\n",
      "Epoch 3331, Loss: 0.22573606669902802, Final Batch Loss: 0.11279484629631042\n",
      "Epoch 3332, Loss: 0.20531167834997177, Final Batch Loss: 0.09634128957986832\n",
      "Epoch 3333, Loss: 0.1753297969698906, Final Batch Loss: 0.08629875630140305\n",
      "Epoch 3334, Loss: 0.13115616142749786, Final Batch Loss: 0.0668228343129158\n",
      "Epoch 3335, Loss: 0.16206124797463417, Final Batch Loss: 0.1025233119726181\n",
      "Epoch 3336, Loss: 0.2043418511748314, Final Batch Loss: 0.07591385394334793\n",
      "Epoch 3337, Loss: 0.16915831342339516, Final Batch Loss: 0.12555482983589172\n",
      "Epoch 3338, Loss: 0.1657750904560089, Final Batch Loss: 0.07428349554538727\n",
      "Epoch 3339, Loss: 0.1804591305553913, Final Batch Loss: 0.11873245239257812\n",
      "Epoch 3340, Loss: 0.12109963968396187, Final Batch Loss: 0.05263669416308403\n",
      "Epoch 3341, Loss: 0.13989101350307465, Final Batch Loss: 0.07676952332258224\n",
      "Epoch 3342, Loss: 0.1442629173398018, Final Batch Loss: 0.07667212933301926\n",
      "Epoch 3343, Loss: 0.22338104248046875, Final Batch Loss: 0.08438530564308167\n",
      "Epoch 3344, Loss: 0.24266260862350464, Final Batch Loss: 0.10759411752223969\n",
      "Epoch 3345, Loss: 0.18324405699968338, Final Batch Loss: 0.08784541487693787\n",
      "Epoch 3346, Loss: 0.15388182550668716, Final Batch Loss: 0.09397980570793152\n",
      "Epoch 3347, Loss: 0.17974402755498886, Final Batch Loss: 0.11706379055976868\n",
      "Epoch 3348, Loss: 0.2911614179611206, Final Batch Loss: 0.15244081616401672\n",
      "Epoch 3349, Loss: 0.1662459820508957, Final Batch Loss: 0.0709788054227829\n",
      "Epoch 3350, Loss: 0.13849417120218277, Final Batch Loss: 0.09159234911203384\n",
      "Epoch 3351, Loss: 0.17075976729393005, Final Batch Loss: 0.07332323491573334\n",
      "Epoch 3352, Loss: 0.24260025471448898, Final Batch Loss: 0.13857688009738922\n",
      "Epoch 3353, Loss: 0.17595099657773972, Final Batch Loss: 0.06748279929161072\n",
      "Epoch 3354, Loss: 0.1607995741069317, Final Batch Loss: 0.1071261465549469\n",
      "Epoch 3355, Loss: 0.19354552030563354, Final Batch Loss: 0.09331980347633362\n",
      "Epoch 3356, Loss: 0.23655712604522705, Final Batch Loss: 0.11799507588148117\n",
      "Epoch 3357, Loss: 0.17511799186468124, Final Batch Loss: 0.10907933861017227\n",
      "Epoch 3358, Loss: 0.18713529780507088, Final Batch Loss: 0.13267964124679565\n",
      "Epoch 3359, Loss: 0.21018684655427933, Final Batch Loss: 0.09926135838031769\n",
      "Epoch 3360, Loss: 0.18377672880887985, Final Batch Loss: 0.11345968395471573\n",
      "Epoch 3361, Loss: 0.1829972192645073, Final Batch Loss: 0.07729218900203705\n",
      "Epoch 3362, Loss: 0.2011673003435135, Final Batch Loss: 0.0942482203245163\n",
      "Epoch 3363, Loss: 0.1741768941283226, Final Batch Loss: 0.09278212487697601\n",
      "Epoch 3364, Loss: 0.2100396379828453, Final Batch Loss: 0.0688629224896431\n",
      "Epoch 3365, Loss: 0.19037172198295593, Final Batch Loss: 0.10796371102333069\n",
      "Epoch 3366, Loss: 0.23531538248062134, Final Batch Loss: 0.0756838321685791\n",
      "Epoch 3367, Loss: 0.13509400933980942, Final Batch Loss: 0.08527848869562149\n",
      "Epoch 3368, Loss: 0.17479439824819565, Final Batch Loss: 0.09214084595441818\n",
      "Epoch 3369, Loss: 0.19293628633022308, Final Batch Loss: 0.10554156452417374\n",
      "Epoch 3370, Loss: 0.1564839892089367, Final Batch Loss: 0.05884480103850365\n",
      "Epoch 3371, Loss: 0.1969570815563202, Final Batch Loss: 0.09056121110916138\n",
      "Epoch 3372, Loss: 0.15920805931091309, Final Batch Loss: 0.04928208142518997\n",
      "Epoch 3373, Loss: 0.15924646705389023, Final Batch Loss: 0.08746045082807541\n",
      "Epoch 3374, Loss: 0.14916011318564415, Final Batch Loss: 0.08813799172639847\n",
      "Epoch 3375, Loss: 0.18263547122478485, Final Batch Loss: 0.10577566176652908\n",
      "Epoch 3376, Loss: 0.10126258432865143, Final Batch Loss: 0.04914932698011398\n",
      "Epoch 3377, Loss: 0.16129787266254425, Final Batch Loss: 0.08356639742851257\n",
      "Epoch 3378, Loss: 0.13013409078121185, Final Batch Loss: 0.05663953721523285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3379, Loss: 0.23223716020584106, Final Batch Loss: 0.12889321148395538\n",
      "Epoch 3380, Loss: 0.17237882316112518, Final Batch Loss: 0.07386001944541931\n",
      "Epoch 3381, Loss: 0.18850234150886536, Final Batch Loss: 0.09549204260110855\n",
      "Epoch 3382, Loss: 0.15966276079416275, Final Batch Loss: 0.08271802961826324\n",
      "Epoch 3383, Loss: 0.25621388107538223, Final Batch Loss: 0.1354280710220337\n",
      "Epoch 3384, Loss: 0.23892276734113693, Final Batch Loss: 0.13811397552490234\n",
      "Epoch 3385, Loss: 0.19004064798355103, Final Batch Loss: 0.10998272895812988\n",
      "Epoch 3386, Loss: 0.15615350753068924, Final Batch Loss: 0.0712633728981018\n",
      "Epoch 3387, Loss: 0.21959135681390762, Final Batch Loss: 0.11807753145694733\n",
      "Epoch 3388, Loss: 0.15500280633568764, Final Batch Loss: 0.05394678935408592\n",
      "Epoch 3389, Loss: 0.18947111070156097, Final Batch Loss: 0.1134573295712471\n",
      "Epoch 3390, Loss: 0.19247747212648392, Final Batch Loss: 0.086236372590065\n",
      "Epoch 3391, Loss: 0.19712864607572556, Final Batch Loss: 0.11557132005691528\n",
      "Epoch 3392, Loss: 0.17825164273381233, Final Batch Loss: 0.052871573716402054\n",
      "Epoch 3393, Loss: 0.2095644772052765, Final Batch Loss: 0.07629123330116272\n",
      "Epoch 3394, Loss: 0.21377906948328018, Final Batch Loss: 0.10730080306529999\n",
      "Epoch 3395, Loss: 0.22232258319854736, Final Batch Loss: 0.10075648874044418\n",
      "Epoch 3396, Loss: 0.15995988249778748, Final Batch Loss: 0.07376337796449661\n",
      "Epoch 3397, Loss: 0.20212199538946152, Final Batch Loss: 0.07628262788057327\n",
      "Epoch 3398, Loss: 0.23676719516515732, Final Batch Loss: 0.08834230154752731\n",
      "Epoch 3399, Loss: 0.17059080302715302, Final Batch Loss: 0.06874032318592072\n",
      "Epoch 3400, Loss: 0.16001404076814651, Final Batch Loss: 0.06298999488353729\n",
      "Epoch 3401, Loss: 0.10328535549342632, Final Batch Loss: 0.017768947407603264\n",
      "Epoch 3402, Loss: 0.2684023976325989, Final Batch Loss: 0.165139839053154\n",
      "Epoch 3403, Loss: 0.16296256333589554, Final Batch Loss: 0.07506503164768219\n",
      "Epoch 3404, Loss: 0.1697191372513771, Final Batch Loss: 0.0657443106174469\n",
      "Epoch 3405, Loss: 0.16989432647824287, Final Batch Loss: 0.055703554302453995\n",
      "Epoch 3406, Loss: 0.15691757202148438, Final Batch Loss: 0.07438621670007706\n",
      "Epoch 3407, Loss: 0.18068289756774902, Final Batch Loss: 0.11052611470222473\n",
      "Epoch 3408, Loss: 0.1405034177005291, Final Batch Loss: 0.08203588426113129\n",
      "Epoch 3409, Loss: 0.15416481345891953, Final Batch Loss: 0.07800497859716415\n",
      "Epoch 3410, Loss: 0.14090290665626526, Final Batch Loss: 0.06447618454694748\n",
      "Epoch 3411, Loss: 0.1316119059920311, Final Batch Loss: 0.06716907024383545\n",
      "Epoch 3412, Loss: 0.188456229865551, Final Batch Loss: 0.09936495870351791\n",
      "Epoch 3413, Loss: 0.15779976174235344, Final Batch Loss: 0.049936164170503616\n",
      "Epoch 3414, Loss: 0.16573651880025864, Final Batch Loss: 0.08129949122667313\n",
      "Epoch 3415, Loss: 0.19704579561948776, Final Batch Loss: 0.13341563940048218\n",
      "Epoch 3416, Loss: 0.20195356756448746, Final Batch Loss: 0.13165222108364105\n",
      "Epoch 3417, Loss: 0.155622661113739, Final Batch Loss: 0.08264117687940598\n",
      "Epoch 3418, Loss: 0.22850867360830307, Final Batch Loss: 0.09157156199216843\n",
      "Epoch 3419, Loss: 0.15007617324590683, Final Batch Loss: 0.06722422689199448\n",
      "Epoch 3420, Loss: 0.1871332824230194, Final Batch Loss: 0.09375964105129242\n",
      "Epoch 3421, Loss: 0.18158914148807526, Final Batch Loss: 0.10470922291278839\n",
      "Epoch 3422, Loss: 0.20164692401885986, Final Batch Loss: 0.09527432173490524\n",
      "Epoch 3423, Loss: 0.17427868396043777, Final Batch Loss: 0.10087868571281433\n",
      "Epoch 3424, Loss: 0.12210556492209435, Final Batch Loss: 0.044593241065740585\n",
      "Epoch 3425, Loss: 0.17958549410104752, Final Batch Loss: 0.07342715561389923\n",
      "Epoch 3426, Loss: 0.24931783974170685, Final Batch Loss: 0.11528229713439941\n",
      "Epoch 3427, Loss: 0.15329250693321228, Final Batch Loss: 0.06408892571926117\n",
      "Epoch 3428, Loss: 0.19094981998205185, Final Batch Loss: 0.09735661000013351\n",
      "Epoch 3429, Loss: 0.1212000735104084, Final Batch Loss: 0.05647655203938484\n",
      "Epoch 3430, Loss: 0.21861818432807922, Final Batch Loss: 0.1280796378850937\n",
      "Epoch 3431, Loss: 0.22030819952487946, Final Batch Loss: 0.08671729266643524\n",
      "Epoch 3432, Loss: 0.12792519852519035, Final Batch Loss: 0.05771941319108009\n",
      "Epoch 3433, Loss: 0.13064904883503914, Final Batch Loss: 0.07021119445562363\n",
      "Epoch 3434, Loss: 0.1395084373652935, Final Batch Loss: 0.0968707874417305\n",
      "Epoch 3435, Loss: 0.24530798196792603, Final Batch Loss: 0.10951942205429077\n",
      "Epoch 3436, Loss: 0.18691294640302658, Final Batch Loss: 0.06932979822158813\n",
      "Epoch 3437, Loss: 0.16744736954569817, Final Batch Loss: 0.05502031370997429\n",
      "Epoch 3438, Loss: 0.13306913152337074, Final Batch Loss: 0.036989014595746994\n",
      "Epoch 3439, Loss: 0.22488654404878616, Final Batch Loss: 0.128194198012352\n",
      "Epoch 3440, Loss: 0.20633608847856522, Final Batch Loss: 0.11999189108610153\n",
      "Epoch 3441, Loss: 0.14503027871251106, Final Batch Loss: 0.0829303190112114\n",
      "Epoch 3442, Loss: 0.22424788028001785, Final Batch Loss: 0.11996565014123917\n",
      "Epoch 3443, Loss: 0.15837941318750381, Final Batch Loss: 0.04460802674293518\n",
      "Epoch 3444, Loss: 0.19907547533512115, Final Batch Loss: 0.10249988734722137\n",
      "Epoch 3445, Loss: 0.15261349081993103, Final Batch Loss: 0.08887024968862534\n",
      "Epoch 3446, Loss: 0.13412849605083466, Final Batch Loss: 0.067191481590271\n",
      "Epoch 3447, Loss: 0.1701052412390709, Final Batch Loss: 0.09137722104787827\n",
      "Epoch 3448, Loss: 0.1312488317489624, Final Batch Loss: 0.06778079271316528\n",
      "Epoch 3449, Loss: 0.1685970351099968, Final Batch Loss: 0.0945175290107727\n",
      "Epoch 3450, Loss: 0.13978124409914017, Final Batch Loss: 0.05386054515838623\n",
      "Epoch 3451, Loss: 0.11822640523314476, Final Batch Loss: 0.06809055805206299\n",
      "Epoch 3452, Loss: 0.15390922874212265, Final Batch Loss: 0.06078182905912399\n",
      "Epoch 3453, Loss: 0.1776650846004486, Final Batch Loss: 0.10139752179384232\n",
      "Epoch 3454, Loss: 0.13574274629354477, Final Batch Loss: 0.07131355255842209\n",
      "Epoch 3455, Loss: 0.21686940640211105, Final Batch Loss: 0.12415355443954468\n",
      "Epoch 3456, Loss: 0.15640952438116074, Final Batch Loss: 0.06134388595819473\n",
      "Epoch 3457, Loss: 0.18026121705770493, Final Batch Loss: 0.08755228668451309\n",
      "Epoch 3458, Loss: 0.14437617361545563, Final Batch Loss: 0.07254761457443237\n",
      "Epoch 3459, Loss: 0.16551928594708443, Final Batch Loss: 0.11327683180570602\n",
      "Epoch 3460, Loss: 0.17090178281068802, Final Batch Loss: 0.07303526252508163\n",
      "Epoch 3461, Loss: 0.1952093318104744, Final Batch Loss: 0.11011194437742233\n",
      "Epoch 3462, Loss: 0.14300795644521713, Final Batch Loss: 0.0662543997168541\n",
      "Epoch 3463, Loss: 0.17446641623973846, Final Batch Loss: 0.09927617758512497\n",
      "Epoch 3464, Loss: 0.16965673118829727, Final Batch Loss: 0.10627525299787521\n",
      "Epoch 3465, Loss: 0.2653748244047165, Final Batch Loss: 0.12632566690444946\n",
      "Epoch 3466, Loss: 0.2504740133881569, Final Batch Loss: 0.08392467349767685\n",
      "Epoch 3467, Loss: 0.22195222973823547, Final Batch Loss: 0.13468803465366364\n",
      "Epoch 3468, Loss: 0.20460449159145355, Final Batch Loss: 0.12537117302417755\n",
      "Epoch 3469, Loss: 0.16350319981575012, Final Batch Loss: 0.10672137886285782\n",
      "Epoch 3470, Loss: 0.13705723360180855, Final Batch Loss: 0.05535029247403145\n",
      "Epoch 3471, Loss: 0.19525718688964844, Final Batch Loss: 0.09596189111471176\n",
      "Epoch 3472, Loss: 0.1592642068862915, Final Batch Loss: 0.07536087930202484\n",
      "Epoch 3473, Loss: 0.16790375858545303, Final Batch Loss: 0.08147164434194565\n",
      "Epoch 3474, Loss: 0.11613970622420311, Final Batch Loss: 0.037380363792181015\n",
      "Epoch 3475, Loss: 0.44676095992326736, Final Batch Loss: 0.3492388427257538\n",
      "Epoch 3476, Loss: 0.10408954694867134, Final Batch Loss: 0.05288724601268768\n",
      "Epoch 3477, Loss: 0.20224572345614433, Final Batch Loss: 0.1411716490983963\n",
      "Epoch 3478, Loss: 0.17509566247463226, Final Batch Loss: 0.0710659995675087\n",
      "Epoch 3479, Loss: 0.17614257335662842, Final Batch Loss: 0.0949617549777031\n",
      "Epoch 3480, Loss: 0.1416521705687046, Final Batch Loss: 0.08477812260389328\n",
      "Epoch 3481, Loss: 0.14283151924610138, Final Batch Loss: 0.06318578869104385\n",
      "Epoch 3482, Loss: 0.17690147459506989, Final Batch Loss: 0.08711353689432144\n",
      "Epoch 3483, Loss: 0.1646314337849617, Final Batch Loss: 0.06714407354593277\n",
      "Epoch 3484, Loss: 0.17458274215459824, Final Batch Loss: 0.08242122828960419\n",
      "Epoch 3485, Loss: 0.13051022589206696, Final Batch Loss: 0.06497924774885178\n",
      "Epoch 3486, Loss: 0.16346269473433495, Final Batch Loss: 0.1128695011138916\n",
      "Epoch 3487, Loss: 0.15313059836626053, Final Batch Loss: 0.07535386085510254\n",
      "Epoch 3488, Loss: 0.24549948424100876, Final Batch Loss: 0.1169954463839531\n",
      "Epoch 3489, Loss: 0.19597391411662102, Final Batch Loss: 0.054594364017248154\n",
      "Epoch 3490, Loss: 0.14501870423555374, Final Batch Loss: 0.05399896204471588\n",
      "Epoch 3491, Loss: 0.21527982503175735, Final Batch Loss: 0.1380460560321808\n",
      "Epoch 3492, Loss: 0.1922985166311264, Final Batch Loss: 0.08911173790693283\n",
      "Epoch 3493, Loss: 0.12467757612466812, Final Batch Loss: 0.04035168141126633\n",
      "Epoch 3494, Loss: 0.1595223769545555, Final Batch Loss: 0.08226541429758072\n",
      "Epoch 3495, Loss: 0.17026248574256897, Final Batch Loss: 0.10282722115516663\n",
      "Epoch 3496, Loss: 0.16156569123268127, Final Batch Loss: 0.08747878670692444\n",
      "Epoch 3497, Loss: 0.11677245423197746, Final Batch Loss: 0.04916898533701897\n",
      "Epoch 3498, Loss: 0.11419187113642693, Final Batch Loss: 0.04861835762858391\n",
      "Epoch 3499, Loss: 0.09930852800607681, Final Batch Loss: 0.043114837259054184\n",
      "Epoch 3500, Loss: 0.23970989882946014, Final Batch Loss: 0.129740372300148\n",
      "Epoch 3501, Loss: 0.14840810745954514, Final Batch Loss: 0.0856686532497406\n",
      "Epoch 3502, Loss: 0.1371883675456047, Final Batch Loss: 0.07199649512767792\n",
      "Epoch 3503, Loss: 0.13638170063495636, Final Batch Loss: 0.06863556057214737\n",
      "Epoch 3504, Loss: 0.19669681042432785, Final Batch Loss: 0.12167010456323624\n",
      "Epoch 3505, Loss: 0.15738282352685928, Final Batch Loss: 0.07047288864850998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3506, Loss: 0.1303522139787674, Final Batch Loss: 0.06036096066236496\n",
      "Epoch 3507, Loss: 0.1458199918270111, Final Batch Loss: 0.052296511828899384\n",
      "Epoch 3508, Loss: 0.20201023668050766, Final Batch Loss: 0.098079614341259\n",
      "Epoch 3509, Loss: 0.1459503248333931, Final Batch Loss: 0.0694764032959938\n",
      "Epoch 3510, Loss: 0.16772741079330444, Final Batch Loss: 0.08800572156906128\n",
      "Epoch 3511, Loss: 0.15262629836797714, Final Batch Loss: 0.0654202252626419\n",
      "Epoch 3512, Loss: 0.16416680067777634, Final Batch Loss: 0.08013173937797546\n",
      "Epoch 3513, Loss: 0.1999773196876049, Final Batch Loss: 0.15327073633670807\n",
      "Epoch 3514, Loss: 0.18567035347223282, Final Batch Loss: 0.08300921320915222\n",
      "Epoch 3515, Loss: 0.19014212489128113, Final Batch Loss: 0.10116459429264069\n",
      "Epoch 3516, Loss: 0.17343279719352722, Final Batch Loss: 0.08733243495225906\n",
      "Epoch 3517, Loss: 0.1727466806769371, Final Batch Loss: 0.07203645259141922\n",
      "Epoch 3518, Loss: 0.14696739614009857, Final Batch Loss: 0.08228600025177002\n",
      "Epoch 3519, Loss: 0.19110337644815445, Final Batch Loss: 0.09066523611545563\n",
      "Epoch 3520, Loss: 0.14357176423072815, Final Batch Loss: 0.08156972378492355\n",
      "Epoch 3521, Loss: 0.15650999918580055, Final Batch Loss: 0.0353638119995594\n",
      "Epoch 3522, Loss: 0.08915815874934196, Final Batch Loss: 0.05829684063792229\n",
      "Epoch 3523, Loss: 0.16768541559576988, Final Batch Loss: 0.1054999902844429\n",
      "Epoch 3524, Loss: 0.19101960957050323, Final Batch Loss: 0.11268585920333862\n",
      "Epoch 3525, Loss: 0.16131078451871872, Final Batch Loss: 0.06824498623609543\n",
      "Epoch 3526, Loss: 0.14353735744953156, Final Batch Loss: 0.0662367194890976\n",
      "Epoch 3527, Loss: 0.27087465673685074, Final Batch Loss: 0.20019979774951935\n",
      "Epoch 3528, Loss: 0.22841381281614304, Final Batch Loss: 0.11034371703863144\n",
      "Epoch 3529, Loss: 0.16152796894311905, Final Batch Loss: 0.06923326104879379\n",
      "Epoch 3530, Loss: 0.18220987915992737, Final Batch Loss: 0.10377901047468185\n",
      "Epoch 3531, Loss: 0.19702402502298355, Final Batch Loss: 0.07774078845977783\n",
      "Epoch 3532, Loss: 0.18032418191432953, Final Batch Loss: 0.10819067060947418\n",
      "Epoch 3533, Loss: 0.12454098463058472, Final Batch Loss: 0.08314140886068344\n",
      "Epoch 3534, Loss: 0.13971316069364548, Final Batch Loss: 0.06896430999040604\n",
      "Epoch 3535, Loss: 0.1755274459719658, Final Batch Loss: 0.10079169273376465\n",
      "Epoch 3536, Loss: 0.10035232827067375, Final Batch Loss: 0.03473673388361931\n",
      "Epoch 3537, Loss: 0.15528018400073051, Final Batch Loss: 0.0949549525976181\n",
      "Epoch 3538, Loss: 0.1823330968618393, Final Batch Loss: 0.09804059565067291\n",
      "Epoch 3539, Loss: 0.1071188934147358, Final Batch Loss: 0.07300814986228943\n",
      "Epoch 3540, Loss: 0.15610774606466293, Final Batch Loss: 0.09802740067243576\n",
      "Epoch 3541, Loss: 0.112788625061512, Final Batch Loss: 0.04384563863277435\n",
      "Epoch 3542, Loss: 0.1378016099333763, Final Batch Loss: 0.0822078213095665\n",
      "Epoch 3543, Loss: 0.15553658455610275, Final Batch Loss: 0.07230839878320694\n",
      "Epoch 3544, Loss: 0.2221355140209198, Final Batch Loss: 0.09031853079795837\n",
      "Epoch 3545, Loss: 0.17231196910142899, Final Batch Loss: 0.08057922124862671\n",
      "Epoch 3546, Loss: 0.12042692676186562, Final Batch Loss: 0.04941929504275322\n",
      "Epoch 3547, Loss: 0.22261812537908554, Final Batch Loss: 0.08820321410894394\n",
      "Epoch 3548, Loss: 0.20146974921226501, Final Batch Loss: 0.11005350947380066\n",
      "Epoch 3549, Loss: 0.27231648564338684, Final Batch Loss: 0.14248088002204895\n",
      "Epoch 3550, Loss: 0.15721138566732407, Final Batch Loss: 0.0543135404586792\n",
      "Epoch 3551, Loss: 0.11133686453104019, Final Batch Loss: 0.05395841225981712\n",
      "Epoch 3552, Loss: 0.15789073705673218, Final Batch Loss: 0.08252937346696854\n",
      "Epoch 3553, Loss: 0.20367154479026794, Final Batch Loss: 0.06323108077049255\n",
      "Epoch 3554, Loss: 0.1378794088959694, Final Batch Loss: 0.06996387988328934\n",
      "Epoch 3555, Loss: 0.21057777851819992, Final Batch Loss: 0.12258778512477875\n",
      "Epoch 3556, Loss: 0.13989081978797913, Final Batch Loss: 0.07148254662752151\n",
      "Epoch 3557, Loss: 0.15920033305883408, Final Batch Loss: 0.07662401348352432\n",
      "Epoch 3558, Loss: 0.1283801980316639, Final Batch Loss: 0.057898152619600296\n",
      "Epoch 3559, Loss: 0.11711923032999039, Final Batch Loss: 0.07952366024255753\n",
      "Epoch 3560, Loss: 0.13944268971681595, Final Batch Loss: 0.050563447177410126\n",
      "Epoch 3561, Loss: 0.16560842469334602, Final Batch Loss: 0.11117587238550186\n",
      "Epoch 3562, Loss: 0.13474318012595177, Final Batch Loss: 0.04627055302262306\n",
      "Epoch 3563, Loss: 0.10631929337978363, Final Batch Loss: 0.05765300244092941\n",
      "Epoch 3564, Loss: 0.10409104079008102, Final Batch Loss: 0.045863986015319824\n",
      "Epoch 3565, Loss: 0.12507471814751625, Final Batch Loss: 0.07156098634004593\n",
      "Epoch 3566, Loss: 0.11365039646625519, Final Batch Loss: 0.057097937911748886\n",
      "Epoch 3567, Loss: 0.10824909806251526, Final Batch Loss: 0.038163602352142334\n",
      "Epoch 3568, Loss: 0.12974988669157028, Final Batch Loss: 0.08302279561758041\n",
      "Epoch 3569, Loss: 0.10126132145524025, Final Batch Loss: 0.043468669056892395\n",
      "Epoch 3570, Loss: 0.10449837148189545, Final Batch Loss: 0.030160970985889435\n",
      "Epoch 3571, Loss: 0.11550454422831535, Final Batch Loss: 0.05419375002384186\n",
      "Epoch 3572, Loss: 0.15014944225549698, Final Batch Loss: 0.06255470216274261\n",
      "Epoch 3573, Loss: 0.1599837839603424, Final Batch Loss: 0.06680187582969666\n",
      "Epoch 3574, Loss: 0.2150111123919487, Final Batch Loss: 0.1286374181509018\n",
      "Epoch 3575, Loss: 0.13472722470760345, Final Batch Loss: 0.07009686529636383\n",
      "Epoch 3576, Loss: 0.21037355065345764, Final Batch Loss: 0.13905808329582214\n",
      "Epoch 3577, Loss: 0.14126472920179367, Final Batch Loss: 0.07745157927274704\n",
      "Epoch 3578, Loss: 0.14820096641778946, Final Batch Loss: 0.06518294662237167\n",
      "Epoch 3579, Loss: 0.14270637929439545, Final Batch Loss: 0.07656756788492203\n",
      "Epoch 3580, Loss: 0.18795077502727509, Final Batch Loss: 0.12476363778114319\n",
      "Epoch 3581, Loss: 0.1436510607600212, Final Batch Loss: 0.07365165650844574\n",
      "Epoch 3582, Loss: 0.13426685333251953, Final Batch Loss: 0.0568573921918869\n",
      "Epoch 3583, Loss: 0.1347803995013237, Final Batch Loss: 0.04083853214979172\n",
      "Epoch 3584, Loss: 0.1947646364569664, Final Batch Loss: 0.08144007623195648\n",
      "Epoch 3585, Loss: 0.1272871494293213, Final Batch Loss: 0.048425354063510895\n",
      "Epoch 3586, Loss: 0.14255886897444725, Final Batch Loss: 0.04914771392941475\n",
      "Epoch 3587, Loss: 0.09644133225083351, Final Batch Loss: 0.03453473001718521\n",
      "Epoch 3588, Loss: 0.2152472510933876, Final Batch Loss: 0.08207183331251144\n",
      "Epoch 3589, Loss: 0.12755651026964188, Final Batch Loss: 0.05847509205341339\n",
      "Epoch 3590, Loss: 0.2597701624035835, Final Batch Loss: 0.20772676169872284\n",
      "Epoch 3591, Loss: 0.2171180173754692, Final Batch Loss: 0.10736652463674545\n",
      "Epoch 3592, Loss: 0.11362992227077484, Final Batch Loss: 0.045263513922691345\n",
      "Epoch 3593, Loss: 0.14328492060303688, Final Batch Loss: 0.05584293231368065\n",
      "Epoch 3594, Loss: 0.1693652868270874, Final Batch Loss: 0.10048774629831314\n",
      "Epoch 3595, Loss: 0.1715216040611267, Final Batch Loss: 0.09595464915037155\n",
      "Epoch 3596, Loss: 0.14600732922554016, Final Batch Loss: 0.11275146156549454\n",
      "Epoch 3597, Loss: 0.15720544755458832, Final Batch Loss: 0.08150507509708405\n",
      "Epoch 3598, Loss: 0.13882112503051758, Final Batch Loss: 0.07619016617536545\n",
      "Epoch 3599, Loss: 0.16401751339435577, Final Batch Loss: 0.11465994268655777\n",
      "Epoch 3600, Loss: 0.13587484136223793, Final Batch Loss: 0.05262872949242592\n",
      "Epoch 3601, Loss: 0.15586750954389572, Final Batch Loss: 0.06376249343156815\n",
      "Epoch 3602, Loss: 0.14070680364966393, Final Batch Loss: 0.0499243400990963\n",
      "Epoch 3603, Loss: 0.1626053750514984, Final Batch Loss: 0.08300469070672989\n",
      "Epoch 3604, Loss: 0.16653507202863693, Final Batch Loss: 0.07667799293994904\n",
      "Epoch 3605, Loss: 0.1192941851913929, Final Batch Loss: 0.036167073994874954\n",
      "Epoch 3606, Loss: 0.16485071927309036, Final Batch Loss: 0.0670710876584053\n",
      "Epoch 3607, Loss: 0.15824778005480766, Final Batch Loss: 0.06238512322306633\n",
      "Epoch 3608, Loss: 0.17308977991342545, Final Batch Loss: 0.0659780502319336\n",
      "Epoch 3609, Loss: 0.12739012390375137, Final Batch Loss: 0.05908536911010742\n",
      "Epoch 3610, Loss: 0.1689954549074173, Final Batch Loss: 0.10276468098163605\n",
      "Epoch 3611, Loss: 0.10343634337186813, Final Batch Loss: 0.04658818989992142\n",
      "Epoch 3612, Loss: 0.10671343468129635, Final Batch Loss: 0.03047017566859722\n",
      "Epoch 3613, Loss: 0.18320417776703835, Final Batch Loss: 0.12572979927062988\n",
      "Epoch 3614, Loss: 0.1469568982720375, Final Batch Loss: 0.07053784281015396\n",
      "Epoch 3615, Loss: 0.11765408515930176, Final Batch Loss: 0.0373791828751564\n",
      "Epoch 3616, Loss: 0.1739298403263092, Final Batch Loss: 0.08283284306526184\n",
      "Epoch 3617, Loss: 0.10767267271876335, Final Batch Loss: 0.05078929662704468\n",
      "Epoch 3618, Loss: 0.14370401203632355, Final Batch Loss: 0.07983680814504623\n",
      "Epoch 3619, Loss: 0.12274712324142456, Final Batch Loss: 0.05845913290977478\n",
      "Epoch 3620, Loss: 0.20520026981830597, Final Batch Loss: 0.1193477064371109\n",
      "Epoch 3621, Loss: 0.10663450509309769, Final Batch Loss: 0.04077238589525223\n",
      "Epoch 3622, Loss: 0.1843855157494545, Final Batch Loss: 0.0992160364985466\n",
      "Epoch 3623, Loss: 0.17249243706464767, Final Batch Loss: 0.0669788271188736\n",
      "Epoch 3624, Loss: 0.11116865277290344, Final Batch Loss: 0.06110234931111336\n",
      "Epoch 3625, Loss: 0.07229379191994667, Final Batch Loss: 0.028412163257598877\n",
      "Epoch 3626, Loss: 0.16045966744422913, Final Batch Loss: 0.09783841669559479\n",
      "Epoch 3627, Loss: 0.16553424298763275, Final Batch Loss: 0.11361948400735855\n",
      "Epoch 3628, Loss: 0.1969800591468811, Final Batch Loss: 0.082155741751194\n",
      "Epoch 3629, Loss: 0.24637480452656746, Final Batch Loss: 0.1839180588722229\n",
      "Epoch 3630, Loss: 0.25134992972016335, Final Batch Loss: 0.06127335503697395\n",
      "Epoch 3631, Loss: 0.14489326998591423, Final Batch Loss: 0.0581776462495327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3632, Loss: 0.19866763800382614, Final Batch Loss: 0.12676097452640533\n",
      "Epoch 3633, Loss: 0.2509000822901726, Final Batch Loss: 0.11984234303236008\n",
      "Epoch 3634, Loss: 0.1607550084590912, Final Batch Loss: 0.07442185282707214\n",
      "Epoch 3635, Loss: 0.22047198563814163, Final Batch Loss: 0.10358263552188873\n",
      "Epoch 3636, Loss: 0.20426301658153534, Final Batch Loss: 0.1357278674840927\n",
      "Epoch 3637, Loss: 0.1466537043452263, Final Batch Loss: 0.0627618357539177\n",
      "Epoch 3638, Loss: 0.1975402608513832, Final Batch Loss: 0.08820923417806625\n",
      "Epoch 3639, Loss: 0.10826744139194489, Final Batch Loss: 0.05074271187186241\n",
      "Epoch 3640, Loss: 0.21706444025039673, Final Batch Loss: 0.0983034148812294\n",
      "Epoch 3641, Loss: 0.24765943735837936, Final Batch Loss: 0.09553829580545425\n",
      "Epoch 3642, Loss: 0.15463878214359283, Final Batch Loss: 0.07460848987102509\n",
      "Epoch 3643, Loss: 0.1913653388619423, Final Batch Loss: 0.07834819704294205\n",
      "Epoch 3644, Loss: 0.18416988104581833, Final Batch Loss: 0.10061132907867432\n",
      "Epoch 3645, Loss: 0.344165176153183, Final Batch Loss: 0.2417355179786682\n",
      "Epoch 3646, Loss: 0.16625605523586273, Final Batch Loss: 0.09634300321340561\n",
      "Epoch 3647, Loss: 0.15787652134895325, Final Batch Loss: 0.09718544781208038\n",
      "Epoch 3648, Loss: 0.21869438886642456, Final Batch Loss: 0.14846540987491608\n",
      "Epoch 3649, Loss: 0.20435918122529984, Final Batch Loss: 0.11046826094388962\n",
      "Epoch 3650, Loss: 0.1160392239689827, Final Batch Loss: 0.039445661008358\n",
      "Epoch 3651, Loss: 0.372415691614151, Final Batch Loss: 0.30161166191101074\n",
      "Epoch 3652, Loss: 0.15654312074184418, Final Batch Loss: 0.07482437044382095\n",
      "Epoch 3653, Loss: 0.13817444071173668, Final Batch Loss: 0.050738293677568436\n",
      "Epoch 3654, Loss: 0.14330120012164116, Final Batch Loss: 0.10622259229421616\n",
      "Epoch 3655, Loss: 0.21650538593530655, Final Batch Loss: 0.13310888409614563\n",
      "Epoch 3656, Loss: 0.16586299240589142, Final Batch Loss: 0.0857442170381546\n",
      "Epoch 3657, Loss: 0.20244468748569489, Final Batch Loss: 0.12336315959692001\n",
      "Epoch 3658, Loss: 0.12541129440069199, Final Batch Loss: 0.07328705489635468\n",
      "Epoch 3659, Loss: 0.18199633061885834, Final Batch Loss: 0.10366838425397873\n",
      "Epoch 3660, Loss: 0.18100440502166748, Final Batch Loss: 0.07977703958749771\n",
      "Epoch 3661, Loss: 0.17653264850378036, Final Batch Loss: 0.0741443857550621\n",
      "Epoch 3662, Loss: 0.14678985252976418, Final Batch Loss: 0.05284074321389198\n",
      "Epoch 3663, Loss: 0.12905044853687286, Final Batch Loss: 0.06515669077634811\n",
      "Epoch 3664, Loss: 0.10082794539630413, Final Batch Loss: 0.029849400743842125\n",
      "Epoch 3665, Loss: 0.1907934620976448, Final Batch Loss: 0.08281446993350983\n",
      "Epoch 3666, Loss: 0.1439497172832489, Final Batch Loss: 0.06913723796606064\n",
      "Epoch 3667, Loss: 0.15303725749254227, Final Batch Loss: 0.04234771430492401\n",
      "Epoch 3668, Loss: 0.18110907077789307, Final Batch Loss: 0.12681443989276886\n",
      "Epoch 3669, Loss: 0.15509849786758423, Final Batch Loss: 0.06310603767633438\n",
      "Epoch 3670, Loss: 0.2073279544711113, Final Batch Loss: 0.15945659577846527\n",
      "Epoch 3671, Loss: 0.1424715891480446, Final Batch Loss: 0.06449700891971588\n",
      "Epoch 3672, Loss: 0.260232076048851, Final Batch Loss: 0.17321859300136566\n",
      "Epoch 3673, Loss: 0.1517578735947609, Final Batch Loss: 0.07504592835903168\n",
      "Epoch 3674, Loss: 0.10077421367168427, Final Batch Loss: 0.059872157871723175\n",
      "Epoch 3675, Loss: 0.20924429595470428, Final Batch Loss: 0.11342556029558182\n",
      "Epoch 3676, Loss: 0.16987920552492142, Final Batch Loss: 0.11814294755458832\n",
      "Epoch 3677, Loss: 0.17142336815595627, Final Batch Loss: 0.12365682423114777\n",
      "Epoch 3678, Loss: 0.17896749079227448, Final Batch Loss: 0.09082634001970291\n",
      "Epoch 3679, Loss: 0.16747333109378815, Final Batch Loss: 0.1005936712026596\n",
      "Epoch 3680, Loss: 0.1016909908503294, Final Batch Loss: 0.02931871823966503\n",
      "Epoch 3681, Loss: 0.15266374498605728, Final Batch Loss: 0.07604412734508514\n",
      "Epoch 3682, Loss: 0.15228620544075966, Final Batch Loss: 0.09224354475736618\n",
      "Epoch 3683, Loss: 0.14477402716875076, Final Batch Loss: 0.0797988772392273\n",
      "Epoch 3684, Loss: 0.16783329844474792, Final Batch Loss: 0.08310949057340622\n",
      "Epoch 3685, Loss: 0.17855331301689148, Final Batch Loss: 0.08478467166423798\n",
      "Epoch 3686, Loss: 0.21503590792417526, Final Batch Loss: 0.1267084926366806\n",
      "Epoch 3687, Loss: 0.15627024322748184, Final Batch Loss: 0.08214671164751053\n",
      "Epoch 3688, Loss: 0.12840153649449348, Final Batch Loss: 0.06605065613985062\n",
      "Epoch 3689, Loss: 0.1205594539642334, Final Batch Loss: 0.0493927076458931\n",
      "Epoch 3690, Loss: 0.14948180317878723, Final Batch Loss: 0.056661248207092285\n",
      "Epoch 3691, Loss: 0.17428631335496902, Final Batch Loss: 0.10506517440080643\n",
      "Epoch 3692, Loss: 0.1543372943997383, Final Batch Loss: 0.07742919027805328\n",
      "Epoch 3693, Loss: 0.18914713710546494, Final Batch Loss: 0.04838404804468155\n",
      "Epoch 3694, Loss: 0.1385577917098999, Final Batch Loss: 0.06299490481615067\n",
      "Epoch 3695, Loss: 0.1320614516735077, Final Batch Loss: 0.07475705444812775\n",
      "Epoch 3696, Loss: 0.19275090098381042, Final Batch Loss: 0.10166524350643158\n",
      "Epoch 3697, Loss: 0.16597403585910797, Final Batch Loss: 0.056270286440849304\n",
      "Epoch 3698, Loss: 0.15121237188577652, Final Batch Loss: 0.09030470997095108\n",
      "Epoch 3699, Loss: 0.1422378532588482, Final Batch Loss: 0.049119021743535995\n",
      "Epoch 3700, Loss: 0.17211388051509857, Final Batch Loss: 0.08768216520547867\n",
      "Epoch 3701, Loss: 0.14662090688943863, Final Batch Loss: 0.07960674166679382\n",
      "Epoch 3702, Loss: 0.15745530277490616, Final Batch Loss: 0.089842289686203\n",
      "Epoch 3703, Loss: 0.19908589869737625, Final Batch Loss: 0.09334253519773483\n",
      "Epoch 3704, Loss: 0.14026999473571777, Final Batch Loss: 0.06835673004388809\n",
      "Epoch 3705, Loss: 0.21855147182941437, Final Batch Loss: 0.1300777941942215\n",
      "Epoch 3706, Loss: 0.1768106073141098, Final Batch Loss: 0.09764372557401657\n",
      "Epoch 3707, Loss: 0.10796630755066872, Final Batch Loss: 0.053268831223249435\n",
      "Epoch 3708, Loss: 0.1697673760354519, Final Batch Loss: 0.10886699706315994\n",
      "Epoch 3709, Loss: 0.16915006190538406, Final Batch Loss: 0.1151733249425888\n",
      "Epoch 3710, Loss: 0.11302032321691513, Final Batch Loss: 0.03722788393497467\n",
      "Epoch 3711, Loss: 0.13171841204166412, Final Batch Loss: 0.051037684082984924\n",
      "Epoch 3712, Loss: 0.12865618988871574, Final Batch Loss: 0.04967338219285011\n",
      "Epoch 3713, Loss: 0.10759564861655235, Final Batch Loss: 0.04962414130568504\n",
      "Epoch 3714, Loss: 0.22199136763811111, Final Batch Loss: 0.11236380785703659\n",
      "Epoch 3715, Loss: 0.10353187844157219, Final Batch Loss: 0.037417713552713394\n",
      "Epoch 3716, Loss: 0.17344532907009125, Final Batch Loss: 0.06541556864976883\n",
      "Epoch 3717, Loss: 0.12912631407380104, Final Batch Loss: 0.028519053012132645\n",
      "Epoch 3718, Loss: 0.23856893926858902, Final Batch Loss: 0.1385972648859024\n",
      "Epoch 3719, Loss: 0.16719583794474602, Final Batch Loss: 0.05906061455607414\n",
      "Epoch 3720, Loss: 0.2408756986260414, Final Batch Loss: 0.15526559948921204\n",
      "Epoch 3721, Loss: 0.12834061309695244, Final Batch Loss: 0.04679105803370476\n",
      "Epoch 3722, Loss: 0.16242854297161102, Final Batch Loss: 0.050789542496204376\n",
      "Epoch 3723, Loss: 0.20291416347026825, Final Batch Loss: 0.0569593608379364\n",
      "Epoch 3724, Loss: 0.20504680275917053, Final Batch Loss: 0.10432299226522446\n",
      "Epoch 3725, Loss: 0.10274666920304298, Final Batch Loss: 0.037198152393102646\n",
      "Epoch 3726, Loss: 0.1866856813430786, Final Batch Loss: 0.10046804696321487\n",
      "Epoch 3727, Loss: 0.12441033124923706, Final Batch Loss: 0.04244278371334076\n",
      "Epoch 3728, Loss: 0.16039035469293594, Final Batch Loss: 0.09830872714519501\n",
      "Epoch 3729, Loss: 0.13928377628326416, Final Batch Loss: 0.05367843806743622\n",
      "Epoch 3730, Loss: 0.1507701277732849, Final Batch Loss: 0.07349953800439835\n",
      "Epoch 3731, Loss: 0.13707371428608894, Final Batch Loss: 0.04505006596446037\n",
      "Epoch 3732, Loss: 0.15729523450136185, Final Batch Loss: 0.09357470273971558\n",
      "Epoch 3733, Loss: 0.1594744585454464, Final Batch Loss: 0.060743313282728195\n",
      "Epoch 3734, Loss: 0.1900932937860489, Final Batch Loss: 0.08560813963413239\n",
      "Epoch 3735, Loss: 0.12770849838852882, Final Batch Loss: 0.036109160631895065\n",
      "Epoch 3736, Loss: 0.1285388246178627, Final Batch Loss: 0.06346460431814194\n",
      "Epoch 3737, Loss: 0.1754111498594284, Final Batch Loss: 0.09612363576889038\n",
      "Epoch 3738, Loss: 0.1461600884795189, Final Batch Loss: 0.055505067110061646\n",
      "Epoch 3739, Loss: 0.12344898283481598, Final Batch Loss: 0.06469893455505371\n",
      "Epoch 3740, Loss: 0.20220335572957993, Final Batch Loss: 0.08834787458181381\n",
      "Epoch 3741, Loss: 0.2288336157798767, Final Batch Loss: 0.10567938536405563\n",
      "Epoch 3742, Loss: 0.1857609823346138, Final Batch Loss: 0.11451391875743866\n",
      "Epoch 3743, Loss: 0.13047408685088158, Final Batch Loss: 0.06120261177420616\n",
      "Epoch 3744, Loss: 0.30008216202259064, Final Batch Loss: 0.19876115024089813\n",
      "Epoch 3745, Loss: 0.1332029104232788, Final Batch Loss: 0.06841635704040527\n",
      "Epoch 3746, Loss: 0.15061746165156364, Final Batch Loss: 0.05810986086726189\n",
      "Epoch 3747, Loss: 0.14350567758083344, Final Batch Loss: 0.07983389496803284\n",
      "Epoch 3748, Loss: 0.1870468482375145, Final Batch Loss: 0.11665564775466919\n",
      "Epoch 3749, Loss: 0.13021739199757576, Final Batch Loss: 0.03993377462029457\n",
      "Epoch 3750, Loss: 0.23797140270471573, Final Batch Loss: 0.12643370032310486\n",
      "Epoch 3751, Loss: 0.20073410123586655, Final Batch Loss: 0.07853863388299942\n",
      "Epoch 3752, Loss: 0.12319755181670189, Final Batch Loss: 0.05310362949967384\n",
      "Epoch 3753, Loss: 0.21815041452646255, Final Batch Loss: 0.13552597165107727\n",
      "Epoch 3754, Loss: 0.18785879388451576, Final Batch Loss: 0.13067416846752167\n",
      "Epoch 3755, Loss: 0.1745433583855629, Final Batch Loss: 0.07917128503322601\n",
      "Epoch 3756, Loss: 0.14325093105435371, Final Batch Loss: 0.09888086467981339\n",
      "Epoch 3757, Loss: 0.13197726756334305, Final Batch Loss: 0.06330674886703491\n",
      "Epoch 3758, Loss: 0.1333334855735302, Final Batch Loss: 0.0784316435456276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3759, Loss: 0.19120418280363083, Final Batch Loss: 0.10350532829761505\n",
      "Epoch 3760, Loss: 0.1444409415125847, Final Batch Loss: 0.06766379624605179\n",
      "Epoch 3761, Loss: 0.20035604387521744, Final Batch Loss: 0.1215529665350914\n",
      "Epoch 3762, Loss: 0.16803551092743874, Final Batch Loss: 0.1069578230381012\n",
      "Epoch 3763, Loss: 0.17611294239759445, Final Batch Loss: 0.07367931306362152\n",
      "Epoch 3764, Loss: 0.15262188762426376, Final Batch Loss: 0.06333915889263153\n",
      "Epoch 3765, Loss: 0.10349995270371437, Final Batch Loss: 0.03280385956168175\n",
      "Epoch 3766, Loss: 0.19357094168663025, Final Batch Loss: 0.09533122926950455\n",
      "Epoch 3767, Loss: 0.18112928420305252, Final Batch Loss: 0.12802432477474213\n",
      "Epoch 3768, Loss: 0.27710718661546707, Final Batch Loss: 0.10816580802202225\n",
      "Epoch 3769, Loss: 0.12890131771564484, Final Batch Loss: 0.06428517401218414\n",
      "Epoch 3770, Loss: 0.17046993225812912, Final Batch Loss: 0.06337761878967285\n",
      "Epoch 3771, Loss: 0.17179521545767784, Final Batch Loss: 0.11112117022275925\n",
      "Epoch 3772, Loss: 0.14306816458702087, Final Batch Loss: 0.05474500358104706\n",
      "Epoch 3773, Loss: 0.16246749088168144, Final Batch Loss: 0.10370311886072159\n",
      "Epoch 3774, Loss: 0.20036426559090614, Final Batch Loss: 0.13894248008728027\n",
      "Epoch 3775, Loss: 0.1279671974480152, Final Batch Loss: 0.05146199092268944\n",
      "Epoch 3776, Loss: 0.22952774912118912, Final Batch Loss: 0.17490747570991516\n",
      "Epoch 3777, Loss: 0.16048963740468025, Final Batch Loss: 0.10128246992826462\n",
      "Epoch 3778, Loss: 0.09959594905376434, Final Batch Loss: 0.031055524945259094\n",
      "Epoch 3779, Loss: 0.1673816442489624, Final Batch Loss: 0.07545452564954758\n",
      "Epoch 3780, Loss: 0.1288103125989437, Final Batch Loss: 0.08003117889165878\n",
      "Epoch 3781, Loss: 0.1887870877981186, Final Batch Loss: 0.10657281428575516\n",
      "Epoch 3782, Loss: 0.16066370531916618, Final Batch Loss: 0.09986114501953125\n",
      "Epoch 3783, Loss: 0.1516958326101303, Final Batch Loss: 0.06741379201412201\n",
      "Epoch 3784, Loss: 0.1369110718369484, Final Batch Loss: 0.04076022654771805\n",
      "Epoch 3785, Loss: 0.09571363218128681, Final Batch Loss: 0.029722848907113075\n",
      "Epoch 3786, Loss: 0.18293330073356628, Final Batch Loss: 0.10775211453437805\n",
      "Epoch 3787, Loss: 0.12196386978030205, Final Batch Loss: 0.05337461456656456\n",
      "Epoch 3788, Loss: 0.12370714172720909, Final Batch Loss: 0.054327864199876785\n",
      "Epoch 3789, Loss: 0.17630849033594131, Final Batch Loss: 0.08710635453462601\n",
      "Epoch 3790, Loss: 0.14245767146348953, Final Batch Loss: 0.04305169731378555\n",
      "Epoch 3791, Loss: 0.11287886276841164, Final Batch Loss: 0.05592335760593414\n",
      "Epoch 3792, Loss: 0.16902930289506912, Final Batch Loss: 0.07259570062160492\n",
      "Epoch 3793, Loss: 0.17351354658603668, Final Batch Loss: 0.07373670488595963\n",
      "Epoch 3794, Loss: 0.24998789280653, Final Batch Loss: 0.12069325894117355\n",
      "Epoch 3795, Loss: 0.119638592004776, Final Batch Loss: 0.0575871616601944\n",
      "Epoch 3796, Loss: 0.1465233787894249, Final Batch Loss: 0.08623312413692474\n",
      "Epoch 3797, Loss: 0.16007202118635178, Final Batch Loss: 0.09504818916320801\n",
      "Epoch 3798, Loss: 0.14458059519529343, Final Batch Loss: 0.07021266967058182\n",
      "Epoch 3799, Loss: 0.08370544016361237, Final Batch Loss: 0.03903545066714287\n",
      "Epoch 3800, Loss: 0.10404008999466896, Final Batch Loss: 0.04875601828098297\n",
      "Epoch 3801, Loss: 0.1306394524872303, Final Batch Loss: 0.05641600862145424\n",
      "Epoch 3802, Loss: 0.1048310324549675, Final Batch Loss: 0.03618227690458298\n",
      "Epoch 3803, Loss: 0.17681027203798294, Final Batch Loss: 0.10078985244035721\n",
      "Epoch 3804, Loss: 0.10327811166644096, Final Batch Loss: 0.060824476182460785\n",
      "Epoch 3805, Loss: 0.15094800293445587, Final Batch Loss: 0.07952501624822617\n",
      "Epoch 3806, Loss: 0.14318743348121643, Final Batch Loss: 0.07978225499391556\n",
      "Epoch 3807, Loss: 0.16067923605442047, Final Batch Loss: 0.08676747232675552\n",
      "Epoch 3808, Loss: 0.18284955620765686, Final Batch Loss: 0.09814083576202393\n",
      "Epoch 3809, Loss: 0.19804471731185913, Final Batch Loss: 0.10886204987764359\n",
      "Epoch 3810, Loss: 0.09996739029884338, Final Batch Loss: 0.05883949622511864\n",
      "Epoch 3811, Loss: 0.17959312349557877, Final Batch Loss: 0.10708854347467422\n",
      "Epoch 3812, Loss: 0.19028662890195847, Final Batch Loss: 0.14524391293525696\n",
      "Epoch 3813, Loss: 0.23601650446653366, Final Batch Loss: 0.14529483020305634\n",
      "Epoch 3814, Loss: 0.15297169983386993, Final Batch Loss: 0.0860908105969429\n",
      "Epoch 3815, Loss: 0.09272738546133041, Final Batch Loss: 0.04632408544421196\n",
      "Epoch 3816, Loss: 0.10195667296648026, Final Batch Loss: 0.037343963980674744\n",
      "Epoch 3817, Loss: 0.1431409828364849, Final Batch Loss: 0.05915488675236702\n",
      "Epoch 3818, Loss: 0.15031947195529938, Final Batch Loss: 0.08110928535461426\n",
      "Epoch 3819, Loss: 0.0948153156787157, Final Batch Loss: 0.03063500113785267\n",
      "Epoch 3820, Loss: 0.12372481822967529, Final Batch Loss: 0.05202675610780716\n",
      "Epoch 3821, Loss: 0.176335908472538, Final Batch Loss: 0.06691278517246246\n",
      "Epoch 3822, Loss: 0.18894943594932556, Final Batch Loss: 0.10677705705165863\n",
      "Epoch 3823, Loss: 0.13612667471170425, Final Batch Loss: 0.054290711879730225\n",
      "Epoch 3824, Loss: 0.12374849617481232, Final Batch Loss: 0.053740233182907104\n",
      "Epoch 3825, Loss: 0.19212698191404343, Final Batch Loss: 0.11598488688468933\n",
      "Epoch 3826, Loss: 0.23728075623512268, Final Batch Loss: 0.1384386271238327\n",
      "Epoch 3827, Loss: 0.13014721870422363, Final Batch Loss: 0.06996891647577286\n",
      "Epoch 3828, Loss: 0.149456474930048, Final Batch Loss: 0.05163827911019325\n",
      "Epoch 3829, Loss: 0.12009478360414505, Final Batch Loss: 0.06737476587295532\n",
      "Epoch 3830, Loss: 0.16292136907577515, Final Batch Loss: 0.09610570222139359\n",
      "Epoch 3831, Loss: 0.17033466324210167, Final Batch Loss: 0.059732045978307724\n",
      "Epoch 3832, Loss: 0.12472077831625938, Final Batch Loss: 0.044761773198843\n",
      "Epoch 3833, Loss: 0.16059929132461548, Final Batch Loss: 0.06586894392967224\n",
      "Epoch 3834, Loss: 0.11070127785205841, Final Batch Loss: 0.03267556428909302\n",
      "Epoch 3835, Loss: 0.1336725689470768, Final Batch Loss: 0.04992957040667534\n",
      "Epoch 3836, Loss: 0.1469736434519291, Final Batch Loss: 0.0865592360496521\n",
      "Epoch 3837, Loss: 0.10710879042744637, Final Batch Loss: 0.06443223357200623\n",
      "Epoch 3838, Loss: 0.11269961297512054, Final Batch Loss: 0.06222117692232132\n",
      "Epoch 3839, Loss: 0.10395019128918648, Final Batch Loss: 0.06664659082889557\n",
      "Epoch 3840, Loss: 0.11695238202810287, Final Batch Loss: 0.06775963306427002\n",
      "Epoch 3841, Loss: 0.17659476399421692, Final Batch Loss: 0.09803805500268936\n",
      "Epoch 3842, Loss: 0.10703349113464355, Final Batch Loss: 0.05476165935397148\n",
      "Epoch 3843, Loss: 0.20238562673330307, Final Batch Loss: 0.13131190836429596\n",
      "Epoch 3844, Loss: 0.11159757152199745, Final Batch Loss: 0.07250676304101944\n",
      "Epoch 3845, Loss: 0.13176371902227402, Final Batch Loss: 0.06762398779392242\n",
      "Epoch 3846, Loss: 0.125522930175066, Final Batch Loss: 0.08304081112146378\n",
      "Epoch 3847, Loss: 0.15478390455245972, Final Batch Loss: 0.06014692783355713\n",
      "Epoch 3848, Loss: 0.1583823524415493, Final Batch Loss: 0.05592990294098854\n",
      "Epoch 3849, Loss: 0.13747501000761986, Final Batch Loss: 0.08196111023426056\n",
      "Epoch 3850, Loss: 0.12801995873451233, Final Batch Loss: 0.04232922941446304\n",
      "Epoch 3851, Loss: 0.17148477584123611, Final Batch Loss: 0.09617560356855392\n",
      "Epoch 3852, Loss: 0.10440871864557266, Final Batch Loss: 0.04599543660879135\n",
      "Epoch 3853, Loss: 0.10086326114833355, Final Batch Loss: 0.030971644446253777\n",
      "Epoch 3854, Loss: 0.12403320521116257, Final Batch Loss: 0.06179993599653244\n",
      "Epoch 3855, Loss: 0.11900831758975983, Final Batch Loss: 0.035484492778778076\n",
      "Epoch 3856, Loss: 0.18375474214553833, Final Batch Loss: 0.08124037832021713\n",
      "Epoch 3857, Loss: 0.12393269315361977, Final Batch Loss: 0.039027947932481766\n",
      "Epoch 3858, Loss: 0.08547836542129517, Final Batch Loss: 0.05751457437872887\n",
      "Epoch 3859, Loss: 0.15015005320310593, Final Batch Loss: 0.07029759138822556\n",
      "Epoch 3860, Loss: 0.1450476124882698, Final Batch Loss: 0.0519116073846817\n",
      "Epoch 3861, Loss: 0.18376727402210236, Final Batch Loss: 0.11299538612365723\n",
      "Epoch 3862, Loss: 0.12678682804107666, Final Batch Loss: 0.0898183062672615\n",
      "Epoch 3863, Loss: 0.11910984292626381, Final Batch Loss: 0.059247300028800964\n",
      "Epoch 3864, Loss: 0.15898912400007248, Final Batch Loss: 0.058109477162361145\n",
      "Epoch 3865, Loss: 0.11539656296372414, Final Batch Loss: 0.060899518430233\n",
      "Epoch 3866, Loss: 0.15829849988222122, Final Batch Loss: 0.08360794931650162\n",
      "Epoch 3867, Loss: 0.18038877099752426, Final Batch Loss: 0.07334569841623306\n",
      "Epoch 3868, Loss: 0.07871976867318153, Final Batch Loss: 0.04335527494549751\n",
      "Epoch 3869, Loss: 0.09204566478729248, Final Batch Loss: 0.03189743682742119\n",
      "Epoch 3870, Loss: 0.15726349502801895, Final Batch Loss: 0.10205531120300293\n",
      "Epoch 3871, Loss: 0.1839095503091812, Final Batch Loss: 0.10045139491558075\n",
      "Epoch 3872, Loss: 0.10319450125098228, Final Batch Loss: 0.041305966675281525\n",
      "Epoch 3873, Loss: 0.08239646255970001, Final Batch Loss: 0.030179116874933243\n",
      "Epoch 3874, Loss: 0.1177855134010315, Final Batch Loss: 0.04296806454658508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3875, Loss: 0.2165883183479309, Final Batch Loss: 0.03205820918083191\n",
      "Epoch 3876, Loss: 0.15645932406187057, Final Batch Loss: 0.09941085427999496\n",
      "Epoch 3877, Loss: 0.13056981936097145, Final Batch Loss: 0.03642040118575096\n",
      "Epoch 3878, Loss: 0.09197570011019707, Final Batch Loss: 0.02930549904704094\n",
      "Epoch 3879, Loss: 0.1244530975818634, Final Batch Loss: 0.07171981036663055\n",
      "Epoch 3880, Loss: 0.1388002559542656, Final Batch Loss: 0.08761506527662277\n",
      "Epoch 3881, Loss: 0.1338387057185173, Final Batch Loss: 0.0692092701792717\n",
      "Epoch 3882, Loss: 0.146256435662508, Final Batch Loss: 0.08718018978834152\n",
      "Epoch 3883, Loss: 0.19556687772274017, Final Batch Loss: 0.11972375959157944\n",
      "Epoch 3884, Loss: 0.0990302599966526, Final Batch Loss: 0.06028065085411072\n",
      "Epoch 3885, Loss: 0.15566609054803848, Final Batch Loss: 0.08268570154905319\n",
      "Epoch 3886, Loss: 0.1431683488190174, Final Batch Loss: 0.0514356903731823\n",
      "Epoch 3887, Loss: 0.14819544181227684, Final Batch Loss: 0.05643929913640022\n",
      "Epoch 3888, Loss: 0.14760016649961472, Final Batch Loss: 0.10784907639026642\n",
      "Epoch 3889, Loss: 0.1214892640709877, Final Batch Loss: 0.04141770303249359\n",
      "Epoch 3890, Loss: 0.11103816702961922, Final Batch Loss: 0.051452137529850006\n",
      "Epoch 3891, Loss: 0.14558695256710052, Final Batch Loss: 0.07732884585857391\n",
      "Epoch 3892, Loss: 0.1408432014286518, Final Batch Loss: 0.08086181432008743\n",
      "Epoch 3893, Loss: 0.1162169761955738, Final Batch Loss: 0.031903449445962906\n",
      "Epoch 3894, Loss: 0.20517630875110626, Final Batch Loss: 0.0888051986694336\n",
      "Epoch 3895, Loss: 0.18508575856685638, Final Batch Loss: 0.12091193348169327\n",
      "Epoch 3896, Loss: 0.12599586322903633, Final Batch Loss: 0.0524880476295948\n",
      "Epoch 3897, Loss: 0.26009687781333923, Final Batch Loss: 0.18585224449634552\n",
      "Epoch 3898, Loss: 0.22642555832862854, Final Batch Loss: 0.13301941752433777\n",
      "Epoch 3899, Loss: 0.1189851351082325, Final Batch Loss: 0.054124657064676285\n",
      "Epoch 3900, Loss: 0.16693924739956856, Final Batch Loss: 0.05890728160738945\n",
      "Epoch 3901, Loss: 0.15878158062696457, Final Batch Loss: 0.08903224766254425\n",
      "Epoch 3902, Loss: 0.18231474980711937, Final Batch Loss: 0.1214173436164856\n",
      "Epoch 3903, Loss: 0.1572970449924469, Final Batch Loss: 0.06639650464057922\n",
      "Epoch 3904, Loss: 0.20180641114711761, Final Batch Loss: 0.12479925900697708\n",
      "Epoch 3905, Loss: 0.14344066381454468, Final Batch Loss: 0.07468350231647491\n",
      "Epoch 3906, Loss: 0.16017529740929604, Final Batch Loss: 0.04315831884741783\n",
      "Epoch 3907, Loss: 0.16161701828241348, Final Batch Loss: 0.08410335332155228\n",
      "Epoch 3908, Loss: 0.17455346882343292, Final Batch Loss: 0.11185234785079956\n",
      "Epoch 3909, Loss: 0.15830838680267334, Final Batch Loss: 0.08132733404636383\n",
      "Epoch 3910, Loss: 0.14400462061166763, Final Batch Loss: 0.07478892058134079\n",
      "Epoch 3911, Loss: 0.13104387372732162, Final Batch Loss: 0.07150888442993164\n",
      "Epoch 3912, Loss: 0.09310568496584892, Final Batch Loss: 0.04513837769627571\n",
      "Epoch 3913, Loss: 0.16715113818645477, Final Batch Loss: 0.09346971660852432\n",
      "Epoch 3914, Loss: 0.11351199820637703, Final Batch Loss: 0.05692776292562485\n",
      "Epoch 3915, Loss: 0.18561238795518875, Final Batch Loss: 0.10302287340164185\n",
      "Epoch 3916, Loss: 0.15816643834114075, Final Batch Loss: 0.09529811143875122\n",
      "Epoch 3917, Loss: 0.16105686500668526, Final Batch Loss: 0.10218141227960587\n",
      "Epoch 3918, Loss: 0.10286956280469894, Final Batch Loss: 0.05904684588313103\n",
      "Epoch 3919, Loss: 0.14622370898723602, Final Batch Loss: 0.06187561899423599\n",
      "Epoch 3920, Loss: 0.12039249390363693, Final Batch Loss: 0.05995679274201393\n",
      "Epoch 3921, Loss: 0.15609459578990936, Final Batch Loss: 0.05368358641862869\n",
      "Epoch 3922, Loss: 0.08619442582130432, Final Batch Loss: 0.03785698115825653\n",
      "Epoch 3923, Loss: 0.1597515270113945, Final Batch Loss: 0.08399848639965057\n",
      "Epoch 3924, Loss: 0.16129367798566818, Final Batch Loss: 0.06987658888101578\n",
      "Epoch 3925, Loss: 0.14358866959810257, Final Batch Loss: 0.06460278481245041\n",
      "Epoch 3926, Loss: 0.11938787624239922, Final Batch Loss: 0.042118389159440994\n",
      "Epoch 3927, Loss: 0.17853911221027374, Final Batch Loss: 0.07710335403680801\n",
      "Epoch 3928, Loss: 0.28334352374076843, Final Batch Loss: 0.16567197442054749\n",
      "Epoch 3929, Loss: 0.1394229643046856, Final Batch Loss: 0.09619146585464478\n",
      "Epoch 3930, Loss: 0.16854839026927948, Final Batch Loss: 0.03409296274185181\n",
      "Epoch 3931, Loss: 0.12764987722039223, Final Batch Loss: 0.016375135630369186\n",
      "Epoch 3932, Loss: 0.13428425043821335, Final Batch Loss: 0.06760292500257492\n",
      "Epoch 3933, Loss: 0.1714705303311348, Final Batch Loss: 0.09670267254114151\n",
      "Epoch 3934, Loss: 0.10008712857961655, Final Batch Loss: 0.05820412561297417\n",
      "Epoch 3935, Loss: 0.15687070786952972, Final Batch Loss: 0.05508451908826828\n",
      "Epoch 3936, Loss: 0.1379793956875801, Final Batch Loss: 0.03695071488618851\n",
      "Epoch 3937, Loss: 0.18854517489671707, Final Batch Loss: 0.06392118334770203\n",
      "Epoch 3938, Loss: 0.1361374594271183, Final Batch Loss: 0.03483910486102104\n",
      "Epoch 3939, Loss: 0.12989327311515808, Final Batch Loss: 0.04663587361574173\n",
      "Epoch 3940, Loss: 0.10461020097136497, Final Batch Loss: 0.0490560419857502\n",
      "Epoch 3941, Loss: 0.10444457456469536, Final Batch Loss: 0.07191482931375504\n",
      "Epoch 3942, Loss: 0.19173309206962585, Final Batch Loss: 0.07013748586177826\n",
      "Epoch 3943, Loss: 0.13085084781050682, Final Batch Loss: 0.057316068559885025\n",
      "Epoch 3944, Loss: 0.18649805337190628, Final Batch Loss: 0.08697623014450073\n",
      "Epoch 3945, Loss: 0.127314493060112, Final Batch Loss: 0.054196134209632874\n",
      "Epoch 3946, Loss: 0.12131765857338905, Final Batch Loss: 0.0398482121527195\n",
      "Epoch 3947, Loss: 0.13264552876353264, Final Batch Loss: 0.09229802340269089\n",
      "Epoch 3948, Loss: 0.17626243084669113, Final Batch Loss: 0.06516133248806\n",
      "Epoch 3949, Loss: 0.14741387963294983, Final Batch Loss: 0.08085299283266068\n",
      "Epoch 3950, Loss: 0.1347065530717373, Final Batch Loss: 0.07860472053289413\n",
      "Epoch 3951, Loss: 0.16234365105628967, Final Batch Loss: 0.07808244228363037\n",
      "Epoch 3952, Loss: 0.13510441407561302, Final Batch Loss: 0.04608495905995369\n",
      "Epoch 3953, Loss: 0.11809397116303444, Final Batch Loss: 0.07415832579135895\n",
      "Epoch 3954, Loss: 0.15629178285598755, Final Batch Loss: 0.09329681098461151\n",
      "Epoch 3955, Loss: 0.106313930824399, Final Batch Loss: 0.02445620484650135\n",
      "Epoch 3956, Loss: 0.1258975751698017, Final Batch Loss: 0.07816527783870697\n",
      "Epoch 3957, Loss: 0.13748347014188766, Final Batch Loss: 0.056154265999794006\n",
      "Epoch 3958, Loss: 0.14123886078596115, Final Batch Loss: 0.06166908144950867\n",
      "Epoch 3959, Loss: 0.21867205947637558, Final Batch Loss: 0.09052055329084396\n",
      "Epoch 3960, Loss: 0.22850898653268814, Final Batch Loss: 0.08941123634576797\n",
      "Epoch 3961, Loss: 0.12955886125564575, Final Batch Loss: 0.06925798207521439\n",
      "Epoch 3962, Loss: 0.10025542974472046, Final Batch Loss: 0.05510033667087555\n",
      "Epoch 3963, Loss: 0.18011103570461273, Final Batch Loss: 0.09902756661176682\n",
      "Epoch 3964, Loss: 0.13732557743787766, Final Batch Loss: 0.05858183652162552\n",
      "Epoch 3965, Loss: 0.19339915364980698, Final Batch Loss: 0.12718519568443298\n",
      "Epoch 3966, Loss: 0.13961407914757729, Final Batch Loss: 0.046033840626478195\n",
      "Epoch 3967, Loss: 0.0885999146848917, Final Batch Loss: 0.02586769498884678\n",
      "Epoch 3968, Loss: 0.1743619367480278, Final Batch Loss: 0.08787564933300018\n",
      "Epoch 3969, Loss: 0.07607368752360344, Final Batch Loss: 0.03111441060900688\n",
      "Epoch 3970, Loss: 0.12440495751798153, Final Batch Loss: 0.09511467814445496\n",
      "Epoch 3971, Loss: 0.12011517211794853, Final Batch Loss: 0.0501822791993618\n",
      "Epoch 3972, Loss: 0.10254725813865662, Final Batch Loss: 0.04416999593377113\n",
      "Epoch 3973, Loss: 0.12863553315401077, Final Batch Loss: 0.07490047812461853\n",
      "Epoch 3974, Loss: 0.17667710781097412, Final Batch Loss: 0.10315296053886414\n",
      "Epoch 3975, Loss: 0.09496322646737099, Final Batch Loss: 0.04984010010957718\n",
      "Epoch 3976, Loss: 0.08716507256031036, Final Batch Loss: 0.028691567480564117\n",
      "Epoch 3977, Loss: 0.2087768092751503, Final Batch Loss: 0.11012937873601913\n",
      "Epoch 3978, Loss: 0.12764490395784378, Final Batch Loss: 0.039403632283210754\n",
      "Epoch 3979, Loss: 0.12104598432779312, Final Batch Loss: 0.05093245208263397\n",
      "Epoch 3980, Loss: 0.22282540053129196, Final Batch Loss: 0.14125019311904907\n",
      "Epoch 3981, Loss: 0.15415003895759583, Final Batch Loss: 0.08011694997549057\n",
      "Epoch 3982, Loss: 0.11902375891804695, Final Batch Loss: 0.06687090545892715\n",
      "Epoch 3983, Loss: 0.13505485653877258, Final Batch Loss: 0.07255659252405167\n",
      "Epoch 3984, Loss: 0.13549987599253654, Final Batch Loss: 0.08591742068529129\n",
      "Epoch 3985, Loss: 0.21720527857542038, Final Batch Loss: 0.11073289066553116\n",
      "Epoch 3986, Loss: 0.11852778308093548, Final Batch Loss: 0.02418598346412182\n",
      "Epoch 3987, Loss: 0.10789413005113602, Final Batch Loss: 0.03643439710140228\n",
      "Epoch 3988, Loss: 0.11233160272240639, Final Batch Loss: 0.046183254569768906\n",
      "Epoch 3989, Loss: 0.23774276673793793, Final Batch Loss: 0.13789525628089905\n",
      "Epoch 3990, Loss: 0.11931287869811058, Final Batch Loss: 0.056709397584199905\n",
      "Epoch 3991, Loss: 0.1381407454609871, Final Batch Loss: 0.07117880880832672\n",
      "Epoch 3992, Loss: 0.16664212942123413, Final Batch Loss: 0.06837772578001022\n",
      "Epoch 3993, Loss: 0.1729249581694603, Final Batch Loss: 0.09747610986232758\n",
      "Epoch 3994, Loss: 0.15778189897537231, Final Batch Loss: 0.07542142271995544\n",
      "Epoch 3995, Loss: 0.1737813949584961, Final Batch Loss: 0.10164710879325867\n",
      "Epoch 3996, Loss: 0.10854309424757957, Final Batch Loss: 0.05831696465611458\n",
      "Epoch 3997, Loss: 0.11072906106710434, Final Batch Loss: 0.04517544060945511\n",
      "Epoch 3998, Loss: 0.21780794858932495, Final Batch Loss: 0.06380566954612732\n",
      "Epoch 3999, Loss: 0.16721270233392715, Final Batch Loss: 0.06866129487752914\n",
      "Epoch 4000, Loss: 0.10711667872965336, Final Batch Loss: 0.028968924656510353\n",
      "Epoch 4001, Loss: 0.13626480847597122, Final Batch Loss: 0.06792236119508743\n",
      "Epoch 4002, Loss: 0.11178455874323845, Final Batch Loss: 0.026409488171339035\n",
      "Epoch 4003, Loss: 0.164597786962986, Final Batch Loss: 0.06527185440063477\n",
      "Epoch 4004, Loss: 0.09666533023118973, Final Batch Loss: 0.03517867624759674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4005, Loss: 0.15820517390966415, Final Batch Loss: 0.07810351997613907\n",
      "Epoch 4006, Loss: 0.1524864137172699, Final Batch Loss: 0.06160879135131836\n",
      "Epoch 4007, Loss: 0.15909675508737564, Final Batch Loss: 0.07802749425172806\n",
      "Epoch 4008, Loss: 0.19310884177684784, Final Batch Loss: 0.1222989410161972\n",
      "Epoch 4009, Loss: 0.126062773168087, Final Batch Loss: 0.04696459323167801\n",
      "Epoch 4010, Loss: 0.1515125259757042, Final Batch Loss: 0.07847536355257034\n",
      "Epoch 4011, Loss: 0.18265469372272491, Final Batch Loss: 0.11222634464502335\n",
      "Epoch 4012, Loss: 0.1801767796278, Final Batch Loss: 0.11217605322599411\n",
      "Epoch 4013, Loss: 0.15525958687067032, Final Batch Loss: 0.0675172433257103\n",
      "Epoch 4014, Loss: 0.15180481225252151, Final Batch Loss: 0.08201879262924194\n",
      "Epoch 4015, Loss: 0.11837507411837578, Final Batch Loss: 0.03260360285639763\n",
      "Epoch 4016, Loss: 0.16000576317310333, Final Batch Loss: 0.07369450479745865\n",
      "Epoch 4017, Loss: 0.1105443648993969, Final Batch Loss: 0.04222714528441429\n",
      "Epoch 4018, Loss: 0.14630621671676636, Final Batch Loss: 0.0728357806801796\n",
      "Epoch 4019, Loss: 0.17546329647302628, Final Batch Loss: 0.0803118646144867\n",
      "Epoch 4020, Loss: 0.14717771857976913, Final Batch Loss: 0.08599413931369781\n",
      "Epoch 4021, Loss: 0.07121479511260986, Final Batch Loss: 0.029892031103372574\n",
      "Epoch 4022, Loss: 0.1914951130747795, Final Batch Loss: 0.07536295056343079\n",
      "Epoch 4023, Loss: 0.14585331082344055, Final Batch Loss: 0.056324705481529236\n",
      "Epoch 4024, Loss: 0.11621543392539024, Final Batch Loss: 0.06401532888412476\n",
      "Epoch 4025, Loss: 0.12604381516575813, Final Batch Loss: 0.05832401290535927\n",
      "Epoch 4026, Loss: 0.11158565804362297, Final Batch Loss: 0.07008613646030426\n",
      "Epoch 4027, Loss: 0.12718985229730606, Final Batch Loss: 0.06562939286231995\n",
      "Epoch 4028, Loss: 0.13267896324396133, Final Batch Loss: 0.06600900739431381\n",
      "Epoch 4029, Loss: 0.09596587717533112, Final Batch Loss: 0.043329447507858276\n",
      "Epoch 4030, Loss: 0.11456179991364479, Final Batch Loss: 0.0895153135061264\n",
      "Epoch 4031, Loss: 0.1666656881570816, Final Batch Loss: 0.10850685089826584\n",
      "Epoch 4032, Loss: 0.19550160691142082, Final Batch Loss: 0.14988163113594055\n",
      "Epoch 4033, Loss: 0.12614530324935913, Final Batch Loss: 0.05023105442523956\n",
      "Epoch 4034, Loss: 0.11025910452008247, Final Batch Loss: 0.0681973546743393\n",
      "Epoch 4035, Loss: 0.18395760655403137, Final Batch Loss: 0.12001769989728928\n",
      "Epoch 4036, Loss: 0.1126459650695324, Final Batch Loss: 0.0639740452170372\n",
      "Epoch 4037, Loss: 0.19564448297023773, Final Batch Loss: 0.1198577880859375\n",
      "Epoch 4038, Loss: 0.10351039469242096, Final Batch Loss: 0.032576046884059906\n",
      "Epoch 4039, Loss: 0.11294601485133171, Final Batch Loss: 0.046750884503126144\n",
      "Epoch 4040, Loss: 0.12016141042113304, Final Batch Loss: 0.04745924100279808\n",
      "Epoch 4041, Loss: 0.2277381420135498, Final Batch Loss: 0.11140740662813187\n",
      "Epoch 4042, Loss: 0.18218322843313217, Final Batch Loss: 0.12517689168453217\n",
      "Epoch 4043, Loss: 0.12067742645740509, Final Batch Loss: 0.05575483292341232\n",
      "Epoch 4044, Loss: 0.13443919271230698, Final Batch Loss: 0.0683068335056305\n",
      "Epoch 4045, Loss: 0.14035268872976303, Final Batch Loss: 0.050341203808784485\n",
      "Epoch 4046, Loss: 0.2587551698088646, Final Batch Loss: 0.03661898523569107\n",
      "Epoch 4047, Loss: 0.17280369251966476, Final Batch Loss: 0.06253981590270996\n",
      "Epoch 4048, Loss: 0.1252359338104725, Final Batch Loss: 0.06827493011951447\n",
      "Epoch 4049, Loss: 0.12591201439499855, Final Batch Loss: 0.06401188671588898\n",
      "Epoch 4050, Loss: 0.12658213078975677, Final Batch Loss: 0.05171167850494385\n",
      "Epoch 4051, Loss: 0.15414458140730858, Final Batch Loss: 0.05595078691840172\n",
      "Epoch 4052, Loss: 0.12850654870271683, Final Batch Loss: 0.05911821126937866\n",
      "Epoch 4053, Loss: 0.16589155793190002, Final Batch Loss: 0.1097676083445549\n",
      "Epoch 4054, Loss: 0.13641344383358955, Final Batch Loss: 0.0804494246840477\n",
      "Epoch 4055, Loss: 0.08981949836015701, Final Batch Loss: 0.04374905675649643\n",
      "Epoch 4056, Loss: 0.10892636328935623, Final Batch Loss: 0.041896939277648926\n",
      "Epoch 4057, Loss: 0.12986258044838905, Final Batch Loss: 0.07292628288269043\n",
      "Epoch 4058, Loss: 0.13048382475972176, Final Batch Loss: 0.0728956088423729\n",
      "Epoch 4059, Loss: 0.17876802012324333, Final Batch Loss: 0.11817659437656403\n",
      "Epoch 4060, Loss: 0.11054398491978645, Final Batch Loss: 0.03812773898243904\n",
      "Epoch 4061, Loss: 0.16330773383378983, Final Batch Loss: 0.06642724573612213\n",
      "Epoch 4062, Loss: 0.09811685606837273, Final Batch Loss: 0.046174950897693634\n",
      "Epoch 4063, Loss: 0.18030370026826859, Final Batch Loss: 0.11429641395807266\n",
      "Epoch 4064, Loss: 0.12428947165608406, Final Batch Loss: 0.06654199212789536\n",
      "Epoch 4065, Loss: 0.1510903462767601, Final Batch Loss: 0.07323834300041199\n",
      "Epoch 4066, Loss: 0.13831457123160362, Final Batch Loss: 0.05352244898676872\n",
      "Epoch 4067, Loss: 0.1135367676615715, Final Batch Loss: 0.03354577720165253\n",
      "Epoch 4068, Loss: 0.1477845124900341, Final Batch Loss: 0.10622259229421616\n",
      "Epoch 4069, Loss: 0.1850425899028778, Final Batch Loss: 0.11013997346162796\n",
      "Epoch 4070, Loss: 0.18540547788143158, Final Batch Loss: 0.07837691903114319\n",
      "Epoch 4071, Loss: 0.29791347682476044, Final Batch Loss: 0.19233158230781555\n",
      "Epoch 4072, Loss: 0.12883605808019638, Final Batch Loss: 0.05496925860643387\n",
      "Epoch 4073, Loss: 0.12343820929527283, Final Batch Loss: 0.05815398693084717\n",
      "Epoch 4074, Loss: 0.12357957288622856, Final Batch Loss: 0.05587311461567879\n",
      "Epoch 4075, Loss: 0.1868724599480629, Final Batch Loss: 0.09768469631671906\n",
      "Epoch 4076, Loss: 0.11811219900846481, Final Batch Loss: 0.05379294604063034\n",
      "Epoch 4077, Loss: 0.12180079147219658, Final Batch Loss: 0.06722754240036011\n",
      "Epoch 4078, Loss: 0.11063789948821068, Final Batch Loss: 0.055801715701818466\n",
      "Epoch 4079, Loss: 0.10299751907587051, Final Batch Loss: 0.028152942657470703\n",
      "Epoch 4080, Loss: 0.09883715957403183, Final Batch Loss: 0.062131620943546295\n",
      "Epoch 4081, Loss: 0.32161710411310196, Final Batch Loss: 0.23811514675617218\n",
      "Epoch 4082, Loss: 0.10396192967891693, Final Batch Loss: 0.06599225103855133\n",
      "Epoch 4083, Loss: 0.17550505697727203, Final Batch Loss: 0.09367590397596359\n",
      "Epoch 4084, Loss: 0.1944897547364235, Final Batch Loss: 0.11254221200942993\n",
      "Epoch 4085, Loss: 0.12927497923374176, Final Batch Loss: 0.04822906106710434\n",
      "Epoch 4086, Loss: 0.1573253646492958, Final Batch Loss: 0.04129675030708313\n",
      "Epoch 4087, Loss: 0.11305418610572815, Final Batch Loss: 0.03785543888807297\n",
      "Epoch 4088, Loss: 0.18963470682501793, Final Batch Loss: 0.05896947905421257\n",
      "Epoch 4089, Loss: 0.14542629569768906, Final Batch Loss: 0.07656875997781754\n",
      "Epoch 4090, Loss: 0.13260971382260323, Final Batch Loss: 0.05022070184350014\n",
      "Epoch 4091, Loss: 0.14121932163834572, Final Batch Loss: 0.058975059539079666\n",
      "Epoch 4092, Loss: 0.1035781241953373, Final Batch Loss: 0.047409288585186005\n",
      "Epoch 4093, Loss: 0.15421586483716965, Final Batch Loss: 0.03918479382991791\n",
      "Epoch 4094, Loss: 0.14072883874177933, Final Batch Loss: 0.04301249980926514\n",
      "Epoch 4095, Loss: 0.15955646336078644, Final Batch Loss: 0.07753656059503555\n",
      "Epoch 4096, Loss: 0.10079210624098778, Final Batch Loss: 0.05231868103146553\n",
      "Epoch 4097, Loss: 0.12675373256206512, Final Batch Loss: 0.04759164899587631\n",
      "Epoch 4098, Loss: 0.18620944768190384, Final Batch Loss: 0.1047140434384346\n",
      "Epoch 4099, Loss: 0.14298194274306297, Final Batch Loss: 0.05872172489762306\n",
      "Epoch 4100, Loss: 0.18315497785806656, Final Batch Loss: 0.11360123008489609\n",
      "Epoch 4101, Loss: 0.10936714336276054, Final Batch Loss: 0.04714837297797203\n",
      "Epoch 4102, Loss: 0.07715622894465923, Final Batch Loss: 0.024354634806513786\n",
      "Epoch 4103, Loss: 0.11044352874159813, Final Batch Loss: 0.049593426287174225\n",
      "Epoch 4104, Loss: 0.10965605266392231, Final Batch Loss: 0.028904007747769356\n",
      "Epoch 4105, Loss: 0.12198679149150848, Final Batch Loss: 0.07168759405612946\n",
      "Epoch 4106, Loss: 0.13283368945121765, Final Batch Loss: 0.047343336045742035\n",
      "Epoch 4107, Loss: 0.11804340034723282, Final Batch Loss: 0.06542854011058807\n",
      "Epoch 4108, Loss: 0.21597129106521606, Final Batch Loss: 0.14027439057826996\n",
      "Epoch 4109, Loss: 0.09632317163050175, Final Batch Loss: 0.02316717989742756\n",
      "Epoch 4110, Loss: 0.13367508351802826, Final Batch Loss: 0.0691971406340599\n",
      "Epoch 4111, Loss: 0.16684016585350037, Final Batch Loss: 0.05783264338970184\n",
      "Epoch 4112, Loss: 0.12130216881632805, Final Batch Loss: 0.07943133264780045\n",
      "Epoch 4113, Loss: 0.12287021055817604, Final Batch Loss: 0.05880938842892647\n",
      "Epoch 4114, Loss: 0.09827043488621712, Final Batch Loss: 0.03937917947769165\n",
      "Epoch 4115, Loss: 0.16709307581186295, Final Batch Loss: 0.10815434157848358\n",
      "Epoch 4116, Loss: 0.06854649819433689, Final Batch Loss: 0.030151618644595146\n",
      "Epoch 4117, Loss: 0.195592500269413, Final Batch Loss: 0.1284649670124054\n",
      "Epoch 4118, Loss: 0.11699866130948067, Final Batch Loss: 0.05820276960730553\n",
      "Epoch 4119, Loss: 0.10897115617990494, Final Batch Loss: 0.03711889684200287\n",
      "Epoch 4120, Loss: 0.07892592810094357, Final Batch Loss: 0.03053463064134121\n",
      "Epoch 4121, Loss: 0.13166223093867302, Final Batch Loss: 0.08221765607595444\n",
      "Epoch 4122, Loss: 0.10156898945569992, Final Batch Loss: 0.042437274008989334\n",
      "Epoch 4123, Loss: 0.21436581015586853, Final Batch Loss: 0.11389446258544922\n",
      "Epoch 4124, Loss: 0.10109565034508705, Final Batch Loss: 0.058159518986940384\n",
      "Epoch 4125, Loss: 0.13019108772277832, Final Batch Loss: 0.037952713668346405\n",
      "Epoch 4126, Loss: 0.16681037843227386, Final Batch Loss: 0.109441377222538\n",
      "Epoch 4127, Loss: 0.057376474142074585, Final Batch Loss: 0.03398721292614937\n",
      "Epoch 4128, Loss: 0.15974210947752, Final Batch Loss: 0.031277887523174286\n",
      "Epoch 4129, Loss: 0.10508498549461365, Final Batch Loss: 0.047488048672676086\n",
      "Epoch 4130, Loss: 0.07704341597855091, Final Batch Loss: 0.02911163680255413\n",
      "Epoch 4131, Loss: 0.10125233419239521, Final Batch Loss: 0.024785416200757027\n",
      "Epoch 4132, Loss: 0.18608856201171875, Final Batch Loss: 0.12991340458393097\n",
      "Epoch 4133, Loss: 0.08911796472966671, Final Batch Loss: 0.02346932701766491\n",
      "Epoch 4134, Loss: 0.21320325136184692, Final Batch Loss: 0.10171493142843246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4135, Loss: 0.12843376398086548, Final Batch Loss: 0.04931911826133728\n",
      "Epoch 4136, Loss: 0.1339862421154976, Final Batch Loss: 0.06307365745306015\n",
      "Epoch 4137, Loss: 0.102508295327425, Final Batch Loss: 0.07013386487960815\n",
      "Epoch 4138, Loss: 0.15825636312365532, Final Batch Loss: 0.12308070808649063\n",
      "Epoch 4139, Loss: 0.14319170266389847, Final Batch Loss: 0.05230312794446945\n",
      "Epoch 4140, Loss: 0.09221681579947472, Final Batch Loss: 0.022036071866750717\n",
      "Epoch 4141, Loss: 0.1601381003856659, Final Batch Loss: 0.040924496948719025\n",
      "Epoch 4142, Loss: 0.14465761184692383, Final Batch Loss: 0.06792052090167999\n",
      "Epoch 4143, Loss: 0.1027560643851757, Final Batch Loss: 0.03762156143784523\n",
      "Epoch 4144, Loss: 0.16774972528219223, Final Batch Loss: 0.08759719878435135\n",
      "Epoch 4145, Loss: 0.19538035243749619, Final Batch Loss: 0.11360374093055725\n",
      "Epoch 4146, Loss: 0.08836624212563038, Final Batch Loss: 0.026159225031733513\n",
      "Epoch 4147, Loss: 0.16615837812423706, Final Batch Loss: 0.08357273787260056\n",
      "Epoch 4148, Loss: 0.1737193502485752, Final Batch Loss: 0.04785792902112007\n",
      "Epoch 4149, Loss: 0.17323538661003113, Final Batch Loss: 0.06354741007089615\n",
      "Epoch 4150, Loss: 0.13529328256845474, Final Batch Loss: 0.08944596350193024\n",
      "Epoch 4151, Loss: 0.1588243469595909, Final Batch Loss: 0.10878786444664001\n",
      "Epoch 4152, Loss: 0.11749547347426414, Final Batch Loss: 0.0478864349424839\n",
      "Epoch 4153, Loss: 0.11184726655483246, Final Batch Loss: 0.038296736776828766\n",
      "Epoch 4154, Loss: 0.12970570474863052, Final Batch Loss: 0.048898182809352875\n",
      "Epoch 4155, Loss: 0.23530418425798416, Final Batch Loss: 0.14003309607505798\n",
      "Epoch 4156, Loss: 0.12528196722269058, Final Batch Loss: 0.04452157020568848\n",
      "Epoch 4157, Loss: 0.13041401654481888, Final Batch Loss: 0.0513082891702652\n",
      "Epoch 4158, Loss: 0.11059787683188915, Final Batch Loss: 0.016092343255877495\n",
      "Epoch 4159, Loss: 0.1664568930864334, Final Batch Loss: 0.0727137103676796\n",
      "Epoch 4160, Loss: 0.12102752551436424, Final Batch Loss: 0.04088534042239189\n",
      "Epoch 4161, Loss: 0.13445781916379929, Final Batch Loss: 0.06110081821680069\n",
      "Epoch 4162, Loss: 0.19182828813791275, Final Batch Loss: 0.11777488142251968\n",
      "Epoch 4163, Loss: 0.14406391233205795, Final Batch Loss: 0.06935626268386841\n",
      "Epoch 4164, Loss: 0.3388945423066616, Final Batch Loss: 0.27818796038627625\n",
      "Epoch 4165, Loss: 0.15498050674796104, Final Batch Loss: 0.11556839942932129\n",
      "Epoch 4166, Loss: 0.18060433864593506, Final Batch Loss: 0.11587841063737869\n",
      "Epoch 4167, Loss: 0.15896516665816307, Final Batch Loss: 0.10457993298768997\n",
      "Epoch 4168, Loss: 0.10417523980140686, Final Batch Loss: 0.06177794188261032\n",
      "Epoch 4169, Loss: 0.15515094622969627, Final Batch Loss: 0.04645414277911186\n",
      "Epoch 4170, Loss: 0.12124945595860481, Final Batch Loss: 0.05812142416834831\n",
      "Epoch 4171, Loss: 0.12720796093344688, Final Batch Loss: 0.05840494856238365\n",
      "Epoch 4172, Loss: 0.14900565147399902, Final Batch Loss: 0.08713069558143616\n",
      "Epoch 4173, Loss: 0.14729707688093185, Final Batch Loss: 0.08064388483762741\n",
      "Epoch 4174, Loss: 0.14520321041345596, Final Batch Loss: 0.07277349382638931\n",
      "Epoch 4175, Loss: 0.14151816815137863, Final Batch Loss: 0.07187519967556\n",
      "Epoch 4176, Loss: 0.1865382231771946, Final Batch Loss: 0.05269386246800423\n",
      "Epoch 4177, Loss: 0.17320246994495392, Final Batch Loss: 0.08382105827331543\n",
      "Epoch 4178, Loss: 0.12057574465870857, Final Batch Loss: 0.040277522057294846\n",
      "Epoch 4179, Loss: 0.15784941986203194, Final Batch Loss: 0.05660654976963997\n",
      "Epoch 4180, Loss: 0.1508164145052433, Final Batch Loss: 0.038651805371046066\n",
      "Epoch 4181, Loss: 0.12231700122356415, Final Batch Loss: 0.04360102862119675\n",
      "Epoch 4182, Loss: 0.11521806567907333, Final Batch Loss: 0.04915910214185715\n",
      "Epoch 4183, Loss: 0.10799703001976013, Final Batch Loss: 0.06064604967832565\n",
      "Epoch 4184, Loss: 0.1821308173239231, Final Batch Loss: 0.06076210364699364\n",
      "Epoch 4185, Loss: 0.20728890597820282, Final Batch Loss: 0.12762877345085144\n",
      "Epoch 4186, Loss: 0.12949787825345993, Final Batch Loss: 0.08827226608991623\n",
      "Epoch 4187, Loss: 0.10535963997244835, Final Batch Loss: 0.0498163066804409\n",
      "Epoch 4188, Loss: 0.13237829506397247, Final Batch Loss: 0.07243926078081131\n",
      "Epoch 4189, Loss: 0.13865570351481438, Final Batch Loss: 0.04959188029170036\n",
      "Epoch 4190, Loss: 0.20333721488714218, Final Batch Loss: 0.10414523631334305\n",
      "Epoch 4191, Loss: 0.08826761692762375, Final Batch Loss: 0.03576195240020752\n",
      "Epoch 4192, Loss: 0.1115054078400135, Final Batch Loss: 0.036308255046606064\n",
      "Epoch 4193, Loss: 0.1768656075000763, Final Batch Loss: 0.10159227252006531\n",
      "Epoch 4194, Loss: 0.11488272249698639, Final Batch Loss: 0.056234121322631836\n",
      "Epoch 4195, Loss: 0.1865333840250969, Final Batch Loss: 0.09018263965845108\n",
      "Epoch 4196, Loss: 0.09726875089108944, Final Batch Loss: 0.029131432995200157\n",
      "Epoch 4197, Loss: 0.13303790241479874, Final Batch Loss: 0.04120182245969772\n",
      "Epoch 4198, Loss: 0.14509008824825287, Final Batch Loss: 0.08305982500314713\n",
      "Epoch 4199, Loss: 0.09122826531529427, Final Batch Loss: 0.05077732726931572\n",
      "Epoch 4200, Loss: 0.12818549200892448, Final Batch Loss: 0.038055311888456345\n",
      "Epoch 4201, Loss: 0.29809484258294106, Final Batch Loss: 0.24671567976474762\n",
      "Epoch 4202, Loss: 0.12806153297424316, Final Batch Loss: 0.05623095482587814\n",
      "Epoch 4203, Loss: 0.1158309280872345, Final Batch Loss: 0.07187148928642273\n",
      "Epoch 4204, Loss: 0.13838014751672745, Final Batch Loss: 0.07427307963371277\n",
      "Epoch 4205, Loss: 0.08806278929114342, Final Batch Loss: 0.04546070098876953\n",
      "Epoch 4206, Loss: 0.13545779511332512, Final Batch Loss: 0.07777127623558044\n",
      "Epoch 4207, Loss: 0.12057456374168396, Final Batch Loss: 0.04430919140577316\n",
      "Epoch 4208, Loss: 0.1467648521065712, Final Batch Loss: 0.08240988105535507\n",
      "Epoch 4209, Loss: 0.170019943267107, Final Batch Loss: 0.048439111560583115\n",
      "Epoch 4210, Loss: 0.11289107799530029, Final Batch Loss: 0.0697009339928627\n",
      "Epoch 4211, Loss: 0.12702949345111847, Final Batch Loss: 0.045252010226249695\n",
      "Epoch 4212, Loss: 0.1255178041756153, Final Batch Loss: 0.04576971009373665\n",
      "Epoch 4213, Loss: 0.08967959880828857, Final Batch Loss: 0.03187379240989685\n",
      "Epoch 4214, Loss: 0.1586926057934761, Final Batch Loss: 0.11326995491981506\n",
      "Epoch 4215, Loss: 0.12815919890999794, Final Batch Loss: 0.07934748381376266\n",
      "Epoch 4216, Loss: 0.12209652736783028, Final Batch Loss: 0.05643029883503914\n",
      "Epoch 4217, Loss: 0.1309291049838066, Final Batch Loss: 0.08861568570137024\n",
      "Epoch 4218, Loss: 0.1298329122364521, Final Batch Loss: 0.05471697822213173\n",
      "Epoch 4219, Loss: 0.09614592045545578, Final Batch Loss: 0.05572402477264404\n",
      "Epoch 4220, Loss: 0.1459243781864643, Final Batch Loss: 0.08353149145841599\n",
      "Epoch 4221, Loss: 0.13370832055807114, Final Batch Loss: 0.056739531457424164\n",
      "Epoch 4222, Loss: 0.12257007509469986, Final Batch Loss: 0.05993957817554474\n",
      "Epoch 4223, Loss: 0.16646812111139297, Final Batch Loss: 0.0958661362528801\n",
      "Epoch 4224, Loss: 0.12017498165369034, Final Batch Loss: 0.08807910978794098\n",
      "Epoch 4225, Loss: 0.14633045345544815, Final Batch Loss: 0.07172135263681412\n",
      "Epoch 4226, Loss: 0.1019061952829361, Final Batch Loss: 0.03972635045647621\n",
      "Epoch 4227, Loss: 0.18578395247459412, Final Batch Loss: 0.12953326106071472\n",
      "Epoch 4228, Loss: 0.13204151019454002, Final Batch Loss: 0.08931070566177368\n",
      "Epoch 4229, Loss: 0.08566232025623322, Final Batch Loss: 0.03668530285358429\n",
      "Epoch 4230, Loss: 0.16622799634933472, Final Batch Loss: 0.088809534907341\n",
      "Epoch 4231, Loss: 0.3096645250916481, Final Batch Loss: 0.24246634542942047\n",
      "Epoch 4232, Loss: 0.09563617780804634, Final Batch Loss: 0.054668355733156204\n",
      "Epoch 4233, Loss: 0.15934612601995468, Final Batch Loss: 0.07956385612487793\n",
      "Epoch 4234, Loss: 0.16994629800319672, Final Batch Loss: 0.08711037039756775\n",
      "Epoch 4235, Loss: 0.10257969424128532, Final Batch Loss: 0.05756070464849472\n",
      "Epoch 4236, Loss: 0.23248396068811417, Final Batch Loss: 0.1677117794752121\n",
      "Epoch 4237, Loss: 0.12061222642660141, Final Batch Loss: 0.053894415497779846\n",
      "Epoch 4238, Loss: 0.09932381846010685, Final Batch Loss: 0.06898558139801025\n",
      "Epoch 4239, Loss: 0.13993562385439873, Final Batch Loss: 0.10554947704076767\n",
      "Epoch 4240, Loss: 0.18643620610237122, Final Batch Loss: 0.06991717219352722\n",
      "Epoch 4241, Loss: 0.19995025545358658, Final Batch Loss: 0.15389946103096008\n",
      "Epoch 4242, Loss: 0.12211647629737854, Final Batch Loss: 0.06750885397195816\n",
      "Epoch 4243, Loss: 0.10262810438871384, Final Batch Loss: 0.04672487452626228\n",
      "Epoch 4244, Loss: 0.0919378288090229, Final Batch Loss: 0.03292021527886391\n",
      "Epoch 4245, Loss: 0.15292633324861526, Final Batch Loss: 0.09851125627756119\n",
      "Epoch 4246, Loss: 0.12933507934212685, Final Batch Loss: 0.0510719008743763\n",
      "Epoch 4247, Loss: 0.19494226574897766, Final Batch Loss: 0.06338205933570862\n",
      "Epoch 4248, Loss: 0.13347331061959267, Final Batch Loss: 0.05758151039481163\n",
      "Epoch 4249, Loss: 0.14247941970825195, Final Batch Loss: 0.06501476466655731\n",
      "Epoch 4250, Loss: 0.12668785825371742, Final Batch Loss: 0.06681084632873535\n",
      "Epoch 4251, Loss: 0.1514630988240242, Final Batch Loss: 0.07763518393039703\n",
      "Epoch 4252, Loss: 0.0962851494550705, Final Batch Loss: 0.04183776304125786\n",
      "Epoch 4253, Loss: 0.1349269486963749, Final Batch Loss: 0.0793333500623703\n",
      "Epoch 4254, Loss: 0.09246095828711987, Final Batch Loss: 0.018194114789366722\n",
      "Epoch 4255, Loss: 0.08177337236702442, Final Batch Loss: 0.020675653591752052\n",
      "Epoch 4256, Loss: 0.10157812386751175, Final Batch Loss: 0.05749135464429855\n",
      "Epoch 4257, Loss: 0.16718295216560364, Final Batch Loss: 0.06439395248889923\n",
      "Epoch 4258, Loss: 0.14125363901257515, Final Batch Loss: 0.05772193893790245\n",
      "Epoch 4259, Loss: 0.13919086381793022, Final Batch Loss: 0.05199030414223671\n",
      "Epoch 4260, Loss: 0.18286094814538956, Final Batch Loss: 0.06845398992300034\n",
      "Epoch 4261, Loss: 0.1636096090078354, Final Batch Loss: 0.048971399664878845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4262, Loss: 0.150733333081007, Final Batch Loss: 0.09592311829328537\n",
      "Epoch 4263, Loss: 0.13882098719477654, Final Batch Loss: 0.054998572915792465\n",
      "Epoch 4264, Loss: 0.1363036260008812, Final Batch Loss: 0.0659613385796547\n",
      "Epoch 4265, Loss: 0.13450662046670914, Final Batch Loss: 0.08444678783416748\n",
      "Epoch 4266, Loss: 0.1067238487303257, Final Batch Loss: 0.049975503236055374\n",
      "Epoch 4267, Loss: 0.1166100800037384, Final Batch Loss: 0.0627618134021759\n",
      "Epoch 4268, Loss: 0.19087933748960495, Final Batch Loss: 0.10632545500993729\n",
      "Epoch 4269, Loss: 0.10188188031315804, Final Batch Loss: 0.04004769027233124\n",
      "Epoch 4270, Loss: 0.18255972117185593, Final Batch Loss: 0.07206244021654129\n",
      "Epoch 4271, Loss: 0.11748482286930084, Final Batch Loss: 0.06872446089982986\n",
      "Epoch 4272, Loss: 0.1546078622341156, Final Batch Loss: 0.08731729537248611\n",
      "Epoch 4273, Loss: 0.19407152384519577, Final Batch Loss: 0.11062480509281158\n",
      "Epoch 4274, Loss: 0.10269956663250923, Final Batch Loss: 0.03334791585803032\n",
      "Epoch 4275, Loss: 0.17240020632743835, Final Batch Loss: 0.10231205075979233\n",
      "Epoch 4276, Loss: 0.1419120505452156, Final Batch Loss: 0.07786738872528076\n",
      "Epoch 4277, Loss: 0.05710577592253685, Final Batch Loss: 0.02420937269926071\n",
      "Epoch 4278, Loss: 0.13919230923056602, Final Batch Loss: 0.08297982811927795\n",
      "Epoch 4279, Loss: 0.13281705975532532, Final Batch Loss: 0.06645370274782181\n",
      "Epoch 4280, Loss: 0.13535594567656517, Final Batch Loss: 0.03633776679635048\n",
      "Epoch 4281, Loss: 0.14935918897390366, Final Batch Loss: 0.08603978157043457\n",
      "Epoch 4282, Loss: 0.11363708227872849, Final Batch Loss: 0.0858289897441864\n",
      "Epoch 4283, Loss: 0.061010949313640594, Final Batch Loss: 0.020812872797250748\n",
      "Epoch 4284, Loss: 0.10231969505548477, Final Batch Loss: 0.0439775176346302\n",
      "Epoch 4285, Loss: 0.1294923648238182, Final Batch Loss: 0.0735589861869812\n",
      "Epoch 4286, Loss: 0.1218964233994484, Final Batch Loss: 0.04399317502975464\n",
      "Epoch 4287, Loss: 0.10191422328352928, Final Batch Loss: 0.06649544090032578\n",
      "Epoch 4288, Loss: 0.2213403880596161, Final Batch Loss: 0.14488697052001953\n",
      "Epoch 4289, Loss: 0.06747907027602196, Final Batch Loss: 0.0365152582526207\n",
      "Epoch 4290, Loss: 0.10919176787137985, Final Batch Loss: 0.061054036021232605\n",
      "Epoch 4291, Loss: 0.13268223777413368, Final Batch Loss: 0.05258786305785179\n",
      "Epoch 4292, Loss: 0.11072688549757004, Final Batch Loss: 0.04750856012105942\n",
      "Epoch 4293, Loss: 0.12209348380565643, Final Batch Loss: 0.08137282729148865\n",
      "Epoch 4294, Loss: 0.14790864661335945, Final Batch Loss: 0.09680984914302826\n",
      "Epoch 4295, Loss: 0.13878702744841576, Final Batch Loss: 0.08117662370204926\n",
      "Epoch 4296, Loss: 0.14743687957525253, Final Batch Loss: 0.0697333812713623\n",
      "Epoch 4297, Loss: 0.1049305833876133, Final Batch Loss: 0.05224230885505676\n",
      "Epoch 4298, Loss: 0.07391991466283798, Final Batch Loss: 0.03038543462753296\n",
      "Epoch 4299, Loss: 0.1603808030486107, Final Batch Loss: 0.0851566269993782\n",
      "Epoch 4300, Loss: 0.07572504505515099, Final Batch Loss: 0.026139818131923676\n",
      "Epoch 4301, Loss: 0.15399324893951416, Final Batch Loss: 0.07887598872184753\n",
      "Epoch 4302, Loss: 0.17999045550823212, Final Batch Loss: 0.09708239138126373\n",
      "Epoch 4303, Loss: 0.12576797604560852, Final Batch Loss: 0.05742678791284561\n",
      "Epoch 4304, Loss: 0.08880050852894783, Final Batch Loss: 0.040130339562892914\n",
      "Epoch 4305, Loss: 0.11421651393175125, Final Batch Loss: 0.041870974004268646\n",
      "Epoch 4306, Loss: 0.10914513096213341, Final Batch Loss: 0.04421369358897209\n",
      "Epoch 4307, Loss: 0.10274342820048332, Final Batch Loss: 0.03619803860783577\n",
      "Epoch 4308, Loss: 0.12996454536914825, Final Batch Loss: 0.07114986330270767\n",
      "Epoch 4309, Loss: 0.191376231610775, Final Batch Loss: 0.07654780894517899\n",
      "Epoch 4310, Loss: 0.11149344220757484, Final Batch Loss: 0.054213136434555054\n",
      "Epoch 4311, Loss: 0.152169331908226, Final Batch Loss: 0.04240763187408447\n",
      "Epoch 4312, Loss: 0.12053515948355198, Final Batch Loss: 0.08959514647722244\n",
      "Epoch 4313, Loss: 0.19406863301992416, Final Batch Loss: 0.08958326280117035\n",
      "Epoch 4314, Loss: 0.10740339383482933, Final Batch Loss: 0.04879603162407875\n",
      "Epoch 4315, Loss: 0.19574099779129028, Final Batch Loss: 0.1224355399608612\n",
      "Epoch 4316, Loss: 0.1565304845571518, Final Batch Loss: 0.12116209417581558\n",
      "Epoch 4317, Loss: 0.1345897614955902, Final Batch Loss: 0.06417207419872284\n",
      "Epoch 4318, Loss: 0.08236393705010414, Final Batch Loss: 0.02950909361243248\n",
      "Epoch 4319, Loss: 0.08752445876598358, Final Batch Loss: 0.042126696556806564\n",
      "Epoch 4320, Loss: 0.09984253719449043, Final Batch Loss: 0.055352263152599335\n",
      "Epoch 4321, Loss: 0.12003651633858681, Final Batch Loss: 0.05396350845694542\n",
      "Epoch 4322, Loss: 0.19796141237020493, Final Batch Loss: 0.08729671686887741\n",
      "Epoch 4323, Loss: 0.27646489813923836, Final Batch Loss: 0.22639717161655426\n",
      "Epoch 4324, Loss: 0.09837638959288597, Final Batch Loss: 0.04532913491129875\n",
      "Epoch 4325, Loss: 0.134163960814476, Final Batch Loss: 0.0621674507856369\n",
      "Epoch 4326, Loss: 0.187592051923275, Final Batch Loss: 0.10263584554195404\n",
      "Epoch 4327, Loss: 0.13697369396686554, Final Batch Loss: 0.08256510645151138\n",
      "Epoch 4328, Loss: 0.12302847765386105, Final Batch Loss: 0.09293916076421738\n",
      "Epoch 4329, Loss: 0.23481552302837372, Final Batch Loss: 0.09522436559200287\n",
      "Epoch 4330, Loss: 0.10409708693623543, Final Batch Loss: 0.03378893807530403\n",
      "Epoch 4331, Loss: 0.1241435669362545, Final Batch Loss: 0.07676846534013748\n",
      "Epoch 4332, Loss: 0.12780652567744255, Final Batch Loss: 0.05750630423426628\n",
      "Epoch 4333, Loss: 0.13178743049502373, Final Batch Loss: 0.0694756954908371\n",
      "Epoch 4334, Loss: 0.13281548768281937, Final Batch Loss: 0.06814581155776978\n",
      "Epoch 4335, Loss: 0.12684601917862892, Final Batch Loss: 0.047522496432065964\n",
      "Epoch 4336, Loss: 0.12579458579421043, Final Batch Loss: 0.05647069588303566\n",
      "Epoch 4337, Loss: 0.09912188723683357, Final Batch Loss: 0.04447593912482262\n",
      "Epoch 4338, Loss: 0.11773891374468803, Final Batch Loss: 0.05635853856801987\n",
      "Epoch 4339, Loss: 0.09537005424499512, Final Batch Loss: 0.06597404181957245\n",
      "Epoch 4340, Loss: 0.07858776487410069, Final Batch Loss: 0.029266444966197014\n",
      "Epoch 4341, Loss: 0.1423533484339714, Final Batch Loss: 0.07780037075281143\n",
      "Epoch 4342, Loss: 0.14524315670132637, Final Batch Loss: 0.10538588464260101\n",
      "Epoch 4343, Loss: 0.07117977179586887, Final Batch Loss: 0.021338237449526787\n",
      "Epoch 4344, Loss: 0.13848888128995895, Final Batch Loss: 0.05950595438480377\n",
      "Epoch 4345, Loss: 0.14910153299570084, Final Batch Loss: 0.08630978316068649\n",
      "Epoch 4346, Loss: 0.10919464752078056, Final Batch Loss: 0.07582840323448181\n",
      "Epoch 4347, Loss: 0.1750524416565895, Final Batch Loss: 0.1350868046283722\n",
      "Epoch 4348, Loss: 0.14062562957406044, Final Batch Loss: 0.08134615421295166\n",
      "Epoch 4349, Loss: 0.08932794071733952, Final Batch Loss: 0.030858805403113365\n",
      "Epoch 4350, Loss: 0.1783483773469925, Final Batch Loss: 0.11583802103996277\n",
      "Epoch 4351, Loss: 0.1183108314871788, Final Batch Loss: 0.040799111127853394\n",
      "Epoch 4352, Loss: 0.12335777655243874, Final Batch Loss: 0.06959625333547592\n",
      "Epoch 4353, Loss: 0.1923707202076912, Final Batch Loss: 0.14268140494823456\n",
      "Epoch 4354, Loss: 0.10750208422541618, Final Batch Loss: 0.06495554000139236\n",
      "Epoch 4355, Loss: 0.14891772717237473, Final Batch Loss: 0.07610320299863815\n",
      "Epoch 4356, Loss: 0.1248910017311573, Final Batch Loss: 0.06691784411668777\n",
      "Epoch 4357, Loss: 0.13251813128590584, Final Batch Loss: 0.09619646519422531\n",
      "Epoch 4358, Loss: 0.11262565106153488, Final Batch Loss: 0.058694805949926376\n",
      "Epoch 4359, Loss: 0.10548432916402817, Final Batch Loss: 0.04439757391810417\n",
      "Epoch 4360, Loss: 0.1284344643354416, Final Batch Loss: 0.06843174993991852\n",
      "Epoch 4361, Loss: 0.13498132675886154, Final Batch Loss: 0.07086202502250671\n",
      "Epoch 4362, Loss: 0.12298363074660301, Final Batch Loss: 0.05693379417061806\n",
      "Epoch 4363, Loss: 0.11876585707068443, Final Batch Loss: 0.07318289577960968\n",
      "Epoch 4364, Loss: 0.13085009530186653, Final Batch Loss: 0.05514276400208473\n",
      "Epoch 4365, Loss: 0.16460274904966354, Final Batch Loss: 0.0719408392906189\n",
      "Epoch 4366, Loss: 0.09162623435258865, Final Batch Loss: 0.03875836357474327\n",
      "Epoch 4367, Loss: 0.17664051055908203, Final Batch Loss: 0.09747175872325897\n",
      "Epoch 4368, Loss: 0.12884555757045746, Final Batch Loss: 0.03913331776857376\n",
      "Epoch 4369, Loss: 0.13360712677240372, Final Batch Loss: 0.06319870799779892\n",
      "Epoch 4370, Loss: 0.12998395785689354, Final Batch Loss: 0.046857912093400955\n",
      "Epoch 4371, Loss: 0.19213610887527466, Final Batch Loss: 0.12332775443792343\n",
      "Epoch 4372, Loss: 0.12891154363751411, Final Batch Loss: 0.07879669219255447\n",
      "Epoch 4373, Loss: 0.0961017794907093, Final Batch Loss: 0.03801347687840462\n",
      "Epoch 4374, Loss: 0.08357921428978443, Final Batch Loss: 0.02455856464803219\n",
      "Epoch 4375, Loss: 0.14326557144522667, Final Batch Loss: 0.08499468863010406\n",
      "Epoch 4376, Loss: 0.15477777272462845, Final Batch Loss: 0.06439179182052612\n",
      "Epoch 4377, Loss: 0.15417035296559334, Final Batch Loss: 0.09464585781097412\n",
      "Epoch 4378, Loss: 0.14584685862064362, Final Batch Loss: 0.0362582802772522\n",
      "Epoch 4379, Loss: 0.11685669422149658, Final Batch Loss: 0.0448654443025589\n",
      "Epoch 4380, Loss: 0.13582991808652878, Final Batch Loss: 0.04312196373939514\n",
      "Epoch 4381, Loss: 0.10377389937639236, Final Batch Loss: 0.04928260296583176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4382, Loss: 0.0870155617594719, Final Batch Loss: 0.04168699309229851\n",
      "Epoch 4383, Loss: 0.09312053397297859, Final Batch Loss: 0.038846999406814575\n",
      "Epoch 4384, Loss: 0.13138822838664055, Final Batch Loss: 0.05416871979832649\n",
      "Epoch 4385, Loss: 0.16813641041517258, Final Batch Loss: 0.07557296752929688\n",
      "Epoch 4386, Loss: 0.16524217277765274, Final Batch Loss: 0.08530335873365402\n",
      "Epoch 4387, Loss: 0.16260935366153717, Final Batch Loss: 0.08146771788597107\n",
      "Epoch 4388, Loss: 0.15984025597572327, Final Batch Loss: 0.06401950865983963\n",
      "Epoch 4389, Loss: 0.07367968373000622, Final Batch Loss: 0.04524883255362511\n",
      "Epoch 4390, Loss: 0.09869382530450821, Final Batch Loss: 0.052393097430467606\n",
      "Epoch 4391, Loss: 0.12364529445767403, Final Batch Loss: 0.07821059226989746\n",
      "Epoch 4392, Loss: 0.12106902524828911, Final Batch Loss: 0.052993129938840866\n",
      "Epoch 4393, Loss: 0.14693660289049149, Final Batch Loss: 0.04206923395395279\n",
      "Epoch 4394, Loss: 0.19557953625917435, Final Batch Loss: 0.0990258976817131\n",
      "Epoch 4395, Loss: 0.1143639013171196, Final Batch Loss: 0.0678316280245781\n",
      "Epoch 4396, Loss: 0.11702219396829605, Final Batch Loss: 0.08059860020875931\n",
      "Epoch 4397, Loss: 0.10507543385028839, Final Batch Loss: 0.0578145794570446\n",
      "Epoch 4398, Loss: 0.21308274567127228, Final Batch Loss: 0.1422284096479416\n",
      "Epoch 4399, Loss: 0.09032078087329865, Final Batch Loss: 0.03282225504517555\n",
      "Epoch 4400, Loss: 0.08184989169239998, Final Batch Loss: 0.032083842903375626\n",
      "Epoch 4401, Loss: 0.08875532262027264, Final Batch Loss: 0.02844560705125332\n",
      "Epoch 4402, Loss: 0.12427445873618126, Final Batch Loss: 0.0592971108853817\n",
      "Epoch 4403, Loss: 0.16344086453318596, Final Batch Loss: 0.040957625955343246\n",
      "Epoch 4404, Loss: 0.10006748512387276, Final Batch Loss: 0.03959500789642334\n",
      "Epoch 4405, Loss: 0.15450546890497208, Final Batch Loss: 0.0748501867055893\n",
      "Epoch 4406, Loss: 0.09985808283090591, Final Batch Loss: 0.04196539521217346\n",
      "Epoch 4407, Loss: 0.14137312024831772, Final Batch Loss: 0.08167808502912521\n",
      "Epoch 4408, Loss: 0.1446678340435028, Final Batch Loss: 0.05873987823724747\n",
      "Epoch 4409, Loss: 0.17431703954935074, Final Batch Loss: 0.07905404269695282\n",
      "Epoch 4410, Loss: 0.1098029688000679, Final Batch Loss: 0.08321576565504074\n",
      "Epoch 4411, Loss: 0.15168184414505959, Final Batch Loss: 0.10261863470077515\n",
      "Epoch 4412, Loss: 0.1379293091595173, Final Batch Loss: 0.044655654579401016\n",
      "Epoch 4413, Loss: 0.1378323994576931, Final Batch Loss: 0.08285129070281982\n",
      "Epoch 4414, Loss: 0.17278922721743584, Final Batch Loss: 0.11411885917186737\n",
      "Epoch 4415, Loss: 0.13128849864006042, Final Batch Loss: 0.0626494437456131\n",
      "Epoch 4416, Loss: 0.0995543785393238, Final Batch Loss: 0.040606789290905\n",
      "Epoch 4417, Loss: 0.12624285742640495, Final Batch Loss: 0.046081092208623886\n",
      "Epoch 4418, Loss: 0.13257429748773575, Final Batch Loss: 0.06385216861963272\n",
      "Epoch 4419, Loss: 0.17010913416743279, Final Batch Loss: 0.06147531047463417\n",
      "Epoch 4420, Loss: 0.12809475511312485, Final Batch Loss: 0.06494149565696716\n",
      "Epoch 4421, Loss: 0.04965170845389366, Final Batch Loss: 0.020746327936649323\n",
      "Epoch 4422, Loss: 0.14718502014875412, Final Batch Loss: 0.08715453743934631\n",
      "Epoch 4423, Loss: 0.17008833587169647, Final Batch Loss: 0.10343018174171448\n",
      "Epoch 4424, Loss: 0.1192147471010685, Final Batch Loss: 0.03751007094979286\n",
      "Epoch 4425, Loss: 0.11996846832334995, Final Batch Loss: 0.022958705201745033\n",
      "Epoch 4426, Loss: 0.09433552995324135, Final Batch Loss: 0.05704304948449135\n",
      "Epoch 4427, Loss: 0.12419009953737259, Final Batch Loss: 0.04656460881233215\n",
      "Epoch 4428, Loss: 0.15490420162677765, Final Batch Loss: 0.05998815596103668\n",
      "Epoch 4429, Loss: 0.16183088719844818, Final Batch Loss: 0.07475930452346802\n",
      "Epoch 4430, Loss: 0.28085561841726303, Final Batch Loss: 0.1227366253733635\n",
      "Epoch 4431, Loss: 0.08966565132141113, Final Batch Loss: 0.04708817973732948\n",
      "Epoch 4432, Loss: 0.10679763555526733, Final Batch Loss: 0.06103947386145592\n",
      "Epoch 4433, Loss: 0.16236477717757225, Final Batch Loss: 0.10723106563091278\n",
      "Epoch 4434, Loss: 0.08347711525857449, Final Batch Loss: 0.055290479212999344\n",
      "Epoch 4435, Loss: 0.08146464452147484, Final Batch Loss: 0.03937303274869919\n",
      "Epoch 4436, Loss: 0.11228178441524506, Final Batch Loss: 0.052400875836610794\n",
      "Epoch 4437, Loss: 0.097958754748106, Final Batch Loss: 0.045443274080753326\n",
      "Epoch 4438, Loss: 0.07070112973451614, Final Batch Loss: 0.021321259438991547\n",
      "Epoch 4439, Loss: 0.11697778478264809, Final Batch Loss: 0.03560559079051018\n",
      "Epoch 4440, Loss: 0.1500827856361866, Final Batch Loss: 0.10210563987493515\n",
      "Epoch 4441, Loss: 0.20722756534814835, Final Batch Loss: 0.11652608215808868\n",
      "Epoch 4442, Loss: 0.15940838307142258, Final Batch Loss: 0.0776202455163002\n",
      "Epoch 4443, Loss: 0.09455295652151108, Final Batch Loss: 0.05777344852685928\n",
      "Epoch 4444, Loss: 0.15171771869063377, Final Batch Loss: 0.10121533274650574\n",
      "Epoch 4445, Loss: 0.17173348367214203, Final Batch Loss: 0.10533857345581055\n",
      "Epoch 4446, Loss: 0.22106290608644485, Final Batch Loss: 0.09053129702806473\n",
      "Epoch 4447, Loss: 0.09995213523507118, Final Batch Loss: 0.042548149824142456\n",
      "Epoch 4448, Loss: 0.18819129467010498, Final Batch Loss: 0.09512079507112503\n",
      "Epoch 4449, Loss: 0.15769001841545105, Final Batch Loss: 0.07340816408395767\n",
      "Epoch 4450, Loss: 0.1898590736091137, Final Batch Loss: 0.05410188063979149\n",
      "Epoch 4451, Loss: 0.1039382852613926, Final Batch Loss: 0.05006534606218338\n",
      "Epoch 4452, Loss: 0.3250589817762375, Final Batch Loss: 0.08933967351913452\n",
      "Epoch 4453, Loss: 0.12976086139678955, Final Batch Loss: 0.060997769236564636\n",
      "Epoch 4454, Loss: 0.12189637124538422, Final Batch Loss: 0.03330831974744797\n",
      "Epoch 4455, Loss: 0.08901858516037464, Final Batch Loss: 0.029285555705428123\n",
      "Epoch 4456, Loss: 0.1009751632809639, Final Batch Loss: 0.043135836720466614\n",
      "Epoch 4457, Loss: 0.11743180826306343, Final Batch Loss: 0.03513186797499657\n",
      "Epoch 4458, Loss: 0.15830459259450436, Final Batch Loss: 0.030625930055975914\n",
      "Epoch 4459, Loss: 0.17406396567821503, Final Batch Loss: 0.10872701555490494\n",
      "Epoch 4460, Loss: 0.17348948866128922, Final Batch Loss: 0.10128885507583618\n",
      "Epoch 4461, Loss: 0.098781518638134, Final Batch Loss: 0.05827029421925545\n",
      "Epoch 4462, Loss: 0.11640748754143715, Final Batch Loss: 0.0568307489156723\n",
      "Epoch 4463, Loss: 0.12394649162888527, Final Batch Loss: 0.05517527088522911\n",
      "Epoch 4464, Loss: 0.14909517019987106, Final Batch Loss: 0.09414162486791611\n",
      "Epoch 4465, Loss: 0.1709575019776821, Final Batch Loss: 0.060682546347379684\n",
      "Epoch 4466, Loss: 0.1160140410065651, Final Batch Loss: 0.05975452437996864\n",
      "Epoch 4467, Loss: 0.14837031811475754, Final Batch Loss: 0.0816570371389389\n",
      "Epoch 4468, Loss: 0.13461661338806152, Final Batch Loss: 0.08234989643096924\n",
      "Epoch 4469, Loss: 0.1158641017973423, Final Batch Loss: 0.08199082314968109\n",
      "Epoch 4470, Loss: 0.09839535504579544, Final Batch Loss: 0.03583661466836929\n",
      "Epoch 4471, Loss: 0.08586573228240013, Final Batch Loss: 0.04470675438642502\n",
      "Epoch 4472, Loss: 0.0799201987683773, Final Batch Loss: 0.03685469180345535\n",
      "Epoch 4473, Loss: 0.13212594389915466, Final Batch Loss: 0.07222224026918411\n",
      "Epoch 4474, Loss: 0.1729869395494461, Final Batch Loss: 0.08084700256586075\n",
      "Epoch 4475, Loss: 0.13102798536419868, Final Batch Loss: 0.0613715834915638\n",
      "Epoch 4476, Loss: 0.12506888061761856, Final Batch Loss: 0.04549471288919449\n",
      "Epoch 4477, Loss: 0.14911755919456482, Final Batch Loss: 0.05007509887218475\n",
      "Epoch 4478, Loss: 0.10431164130568504, Final Batch Loss: 0.06728898733854294\n",
      "Epoch 4479, Loss: 0.10185036063194275, Final Batch Loss: 0.031752824783325195\n",
      "Epoch 4480, Loss: 0.14169646799564362, Final Batch Loss: 0.06609489023685455\n",
      "Epoch 4481, Loss: 0.10598845407366753, Final Batch Loss: 0.041436705738306046\n",
      "Epoch 4482, Loss: 0.12011902406811714, Final Batch Loss: 0.06757910549640656\n",
      "Epoch 4483, Loss: 0.12802011519670486, Final Batch Loss: 0.07399792224168777\n",
      "Epoch 4484, Loss: 0.11447127163410187, Final Batch Loss: 0.07021204382181168\n",
      "Epoch 4485, Loss: 0.20161640644073486, Final Batch Loss: 0.14231239259243011\n",
      "Epoch 4486, Loss: 0.0978257916867733, Final Batch Loss: 0.03957945480942726\n",
      "Epoch 4487, Loss: 0.13223107904195786, Final Batch Loss: 0.0680147260427475\n",
      "Epoch 4488, Loss: 0.16233180463314056, Final Batch Loss: 0.08357568085193634\n",
      "Epoch 4489, Loss: 0.08087739162147045, Final Batch Loss: 0.050585124641656876\n",
      "Epoch 4490, Loss: 0.14416134357452393, Final Batch Loss: 0.09541616588830948\n",
      "Epoch 4491, Loss: 0.0903688482940197, Final Batch Loss: 0.039249204099178314\n",
      "Epoch 4492, Loss: 0.1637284755706787, Final Batch Loss: 0.09218892455101013\n",
      "Epoch 4493, Loss: 0.17693202197551727, Final Batch Loss: 0.06687785685062408\n",
      "Epoch 4494, Loss: 0.20184465125203133, Final Batch Loss: 0.14287148416042328\n",
      "Epoch 4495, Loss: 0.09771821275353432, Final Batch Loss: 0.04407982528209686\n",
      "Epoch 4496, Loss: 0.11979517340660095, Final Batch Loss: 0.0801054909825325\n",
      "Epoch 4497, Loss: 0.07349551655352116, Final Batch Loss: 0.02721596322953701\n",
      "Epoch 4498, Loss: 0.12183892354369164, Final Batch Loss: 0.03888731822371483\n",
      "Epoch 4499, Loss: 0.13292361423373222, Final Batch Loss: 0.07917557656764984\n",
      "Epoch 4500, Loss: 0.22856847941875458, Final Batch Loss: 0.12978900969028473\n",
      "Epoch 4501, Loss: 0.09621851146221161, Final Batch Loss: 0.04289233684539795\n",
      "Epoch 4502, Loss: 0.20756980031728745, Final Batch Loss: 0.11272497475147247\n",
      "Epoch 4503, Loss: 0.12916075810790062, Final Batch Loss: 0.08320478349924088\n",
      "Epoch 4504, Loss: 0.13072225078940392, Final Batch Loss: 0.050090935081243515\n",
      "Epoch 4505, Loss: 0.09484691545367241, Final Batch Loss: 0.04612497240304947\n",
      "Epoch 4506, Loss: 0.11936761066317558, Final Batch Loss: 0.04618633911013603\n",
      "Epoch 4507, Loss: 0.22283939272165298, Final Batch Loss: 0.15813922882080078\n",
      "Epoch 4508, Loss: 0.09988903254270554, Final Batch Loss: 0.044298503547906876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4509, Loss: 0.1304229199886322, Final Batch Loss: 0.06520397216081619\n",
      "Epoch 4510, Loss: 0.12729106657207012, Final Batch Loss: 0.1062355488538742\n",
      "Epoch 4511, Loss: 0.07752816379070282, Final Batch Loss: 0.036833506077528\n",
      "Epoch 4512, Loss: 0.09127287194132805, Final Batch Loss: 0.058437395840883255\n",
      "Epoch 4513, Loss: 0.1407515723258257, Final Batch Loss: 0.027871938422322273\n",
      "Epoch 4514, Loss: 0.1453636772930622, Final Batch Loss: 0.09532269090414047\n",
      "Epoch 4515, Loss: 0.10439564287662506, Final Batch Loss: 0.06142214685678482\n",
      "Epoch 4516, Loss: 0.10882596299052238, Final Batch Loss: 0.05641237273812294\n",
      "Epoch 4517, Loss: 0.10376625694334507, Final Batch Loss: 0.027148576453328133\n",
      "Epoch 4518, Loss: 0.09083124995231628, Final Batch Loss: 0.03191058337688446\n",
      "Epoch 4519, Loss: 0.09456628747284412, Final Batch Loss: 0.06523953378200531\n",
      "Epoch 4520, Loss: 0.11796989105641842, Final Batch Loss: 0.022253310307860374\n",
      "Epoch 4521, Loss: 0.06367211975157261, Final Batch Loss: 0.028059719130396843\n",
      "Epoch 4522, Loss: 0.08218204602599144, Final Batch Loss: 0.03648485243320465\n",
      "Epoch 4523, Loss: 0.135835450142622, Final Batch Loss: 0.059075530618429184\n",
      "Epoch 4524, Loss: 0.0990401990711689, Final Batch Loss: 0.051053740084171295\n",
      "Epoch 4525, Loss: 0.10680309683084488, Final Batch Loss: 0.05009860917925835\n",
      "Epoch 4526, Loss: 0.07288890331983566, Final Batch Loss: 0.02385028451681137\n",
      "Epoch 4527, Loss: 0.19890037924051285, Final Batch Loss: 0.12979894876480103\n",
      "Epoch 4528, Loss: 0.07947762124240398, Final Batch Loss: 0.026904692873358727\n",
      "Epoch 4529, Loss: 0.10946624353528023, Final Batch Loss: 0.06558772176504135\n",
      "Epoch 4530, Loss: 0.15037499740719795, Final Batch Loss: 0.09990420192480087\n",
      "Epoch 4531, Loss: 0.10381973907351494, Final Batch Loss: 0.07347790896892548\n",
      "Epoch 4532, Loss: 0.09636034443974495, Final Batch Loss: 0.044106874614953995\n",
      "Epoch 4533, Loss: 0.12630775198340416, Final Batch Loss: 0.08363673090934753\n",
      "Epoch 4534, Loss: 0.10898144170641899, Final Batch Loss: 0.06369832903146744\n",
      "Epoch 4535, Loss: 0.1257900670170784, Final Batch Loss: 0.061513133347034454\n",
      "Epoch 4536, Loss: 0.10185332968831062, Final Batch Loss: 0.049323685467243195\n",
      "Epoch 4537, Loss: 0.10939813405275345, Final Batch Loss: 0.056201133877038956\n",
      "Epoch 4538, Loss: 0.09490572288632393, Final Batch Loss: 0.044828251004219055\n",
      "Epoch 4539, Loss: 0.10326402261853218, Final Batch Loss: 0.07093287259340286\n",
      "Epoch 4540, Loss: 0.16634734719991684, Final Batch Loss: 0.0992194190621376\n",
      "Epoch 4541, Loss: 0.07246780395507812, Final Batch Loss: 0.0379306823015213\n",
      "Epoch 4542, Loss: 0.3013977110385895, Final Batch Loss: 0.09626461565494537\n",
      "Epoch 4543, Loss: 0.094609584659338, Final Batch Loss: 0.05980769544839859\n",
      "Epoch 4544, Loss: 0.16058406233787537, Final Batch Loss: 0.08436478674411774\n",
      "Epoch 4545, Loss: 0.15784044936299324, Final Batch Loss: 0.11551005393266678\n",
      "Epoch 4546, Loss: 0.1462559662759304, Final Batch Loss: 0.08674216270446777\n",
      "Epoch 4547, Loss: 0.11182839423418045, Final Batch Loss: 0.05661052465438843\n",
      "Epoch 4548, Loss: 0.13425251841545105, Final Batch Loss: 0.09706410020589828\n",
      "Epoch 4549, Loss: 0.15482936799526215, Final Batch Loss: 0.06313162297010422\n",
      "Epoch 4550, Loss: 0.11996505036950111, Final Batch Loss: 0.06766384840011597\n",
      "Epoch 4551, Loss: 0.15881943330168724, Final Batch Loss: 0.05487934127449989\n",
      "Epoch 4552, Loss: 0.2094268575310707, Final Batch Loss: 0.09585464000701904\n",
      "Epoch 4553, Loss: 0.09283652901649475, Final Batch Loss: 0.030303992331027985\n",
      "Epoch 4554, Loss: 0.1726725473999977, Final Batch Loss: 0.08174040913581848\n",
      "Epoch 4555, Loss: 0.17837007716298103, Final Batch Loss: 0.12169412523508072\n",
      "Epoch 4556, Loss: 0.1084088385105133, Final Batch Loss: 0.05707334354519844\n",
      "Epoch 4557, Loss: 0.07792071625590324, Final Batch Loss: 0.025344815105199814\n",
      "Epoch 4558, Loss: 0.10093814879655838, Final Batch Loss: 0.04484139755368233\n",
      "Epoch 4559, Loss: 0.0841900035738945, Final Batch Loss: 0.025099415332078934\n",
      "Epoch 4560, Loss: 0.12808706238865852, Final Batch Loss: 0.05057239159941673\n",
      "Epoch 4561, Loss: 0.21346497535705566, Final Batch Loss: 0.10585465282201767\n",
      "Epoch 4562, Loss: 0.10550021752715111, Final Batch Loss: 0.05098773539066315\n",
      "Epoch 4563, Loss: 0.1624545007944107, Final Batch Loss: 0.09644147008657455\n",
      "Epoch 4564, Loss: 0.0853733941912651, Final Batch Loss: 0.04140380769968033\n",
      "Epoch 4565, Loss: 0.10461267828941345, Final Batch Loss: 0.04628601297736168\n",
      "Epoch 4566, Loss: 0.1413869857788086, Final Batch Loss: 0.03225073963403702\n",
      "Epoch 4567, Loss: 0.10738268494606018, Final Batch Loss: 0.024986281991004944\n",
      "Epoch 4568, Loss: 0.20607558637857437, Final Batch Loss: 0.12658503651618958\n",
      "Epoch 4569, Loss: 0.0870290994644165, Final Batch Loss: 0.05325711518526077\n",
      "Epoch 4570, Loss: 0.08079955726861954, Final Batch Loss: 0.03174000233411789\n",
      "Epoch 4571, Loss: 0.10562606900930405, Final Batch Loss: 0.06156943365931511\n",
      "Epoch 4572, Loss: 0.16397428512573242, Final Batch Loss: 0.09885001927614212\n",
      "Epoch 4573, Loss: 0.11229874938726425, Final Batch Loss: 0.06219534948468208\n",
      "Epoch 4574, Loss: 0.10936946421861649, Final Batch Loss: 0.07437393814325333\n",
      "Epoch 4575, Loss: 0.10264421626925468, Final Batch Loss: 0.0457744263112545\n",
      "Epoch 4576, Loss: 0.11208143457770348, Final Batch Loss: 0.06320267170667648\n",
      "Epoch 4577, Loss: 0.1265571191906929, Final Batch Loss: 0.03894320875406265\n",
      "Epoch 4578, Loss: 0.19635003060102463, Final Batch Loss: 0.08009374886751175\n",
      "Epoch 4579, Loss: 0.24290060997009277, Final Batch Loss: 0.12376496195793152\n",
      "Epoch 4580, Loss: 0.12163300439715385, Final Batch Loss: 0.06175646185874939\n",
      "Epoch 4581, Loss: 0.12279294431209564, Final Batch Loss: 0.05253499746322632\n",
      "Epoch 4582, Loss: 0.15983986482024193, Final Batch Loss: 0.1126740574836731\n",
      "Epoch 4583, Loss: 0.18580162525177002, Final Batch Loss: 0.10684342682361603\n",
      "Epoch 4584, Loss: 0.15965889766812325, Final Batch Loss: 0.09891212731599808\n",
      "Epoch 4585, Loss: 0.13971802592277527, Final Batch Loss: 0.06945262104272842\n",
      "Epoch 4586, Loss: 0.1860571913421154, Final Batch Loss: 0.13034607470035553\n",
      "Epoch 4587, Loss: 0.10963715426623821, Final Batch Loss: 0.08667778223752975\n",
      "Epoch 4588, Loss: 0.11509428173303604, Final Batch Loss: 0.0437091663479805\n",
      "Epoch 4589, Loss: 0.18004325777292252, Final Batch Loss: 0.08872901648283005\n",
      "Epoch 4590, Loss: 0.12967753037810326, Final Batch Loss: 0.03588493540883064\n",
      "Epoch 4591, Loss: 0.12554923072457314, Final Batch Loss: 0.03054353967308998\n",
      "Epoch 4592, Loss: 0.13338471576571465, Final Batch Loss: 0.04228593036532402\n",
      "Epoch 4593, Loss: 0.17546342313289642, Final Batch Loss: 0.05951138585805893\n",
      "Epoch 4594, Loss: 0.15113867819309235, Final Batch Loss: 0.05784328281879425\n",
      "Epoch 4595, Loss: 0.10500901564955711, Final Batch Loss: 0.042898740619421005\n",
      "Epoch 4596, Loss: 0.15227867662906647, Final Batch Loss: 0.062382958829402924\n",
      "Epoch 4597, Loss: 0.1471567340195179, Final Batch Loss: 0.08620741218328476\n",
      "Epoch 4598, Loss: 0.11812910437583923, Final Batch Loss: 0.08330653607845306\n",
      "Epoch 4599, Loss: 0.17006777226924896, Final Batch Loss: 0.09384124726057053\n",
      "Epoch 4600, Loss: 0.311468742787838, Final Batch Loss: 0.22233834862709045\n",
      "Epoch 4601, Loss: 0.16568424552679062, Final Batch Loss: 0.0767725333571434\n",
      "Epoch 4602, Loss: 0.10515597835183144, Final Batch Loss: 0.03864317759871483\n",
      "Epoch 4603, Loss: 0.11150844767689705, Final Batch Loss: 0.043715741485357285\n",
      "Epoch 4604, Loss: 0.17726732790470123, Final Batch Loss: 0.09168770909309387\n",
      "Epoch 4605, Loss: 0.08276422694325447, Final Batch Loss: 0.033551085740327835\n",
      "Epoch 4606, Loss: 0.12460067495703697, Final Batch Loss: 0.07160242646932602\n",
      "Epoch 4607, Loss: 0.10827578976750374, Final Batch Loss: 0.0550653375685215\n",
      "Epoch 4608, Loss: 0.13144918903708458, Final Batch Loss: 0.051303695887327194\n",
      "Epoch 4609, Loss: 0.10816613957285881, Final Batch Loss: 0.05286889523267746\n",
      "Epoch 4610, Loss: 0.12385696545243263, Final Batch Loss: 0.03821742162108421\n",
      "Epoch 4611, Loss: 0.1819833517074585, Final Batch Loss: 0.11430427432060242\n",
      "Epoch 4612, Loss: 0.124434569850564, Final Batch Loss: 0.019842790439724922\n",
      "Epoch 4613, Loss: 0.0845600999891758, Final Batch Loss: 0.03851306438446045\n",
      "Epoch 4614, Loss: 0.12863025069236755, Final Batch Loss: 0.04835513234138489\n",
      "Epoch 4615, Loss: 0.15835437923669815, Final Batch Loss: 0.08244737982749939\n",
      "Epoch 4616, Loss: 0.09600422158837318, Final Batch Loss: 0.051705483347177505\n",
      "Epoch 4617, Loss: 0.11526666954159737, Final Batch Loss: 0.06324826180934906\n",
      "Epoch 4618, Loss: 0.13835162669420242, Final Batch Loss: 0.07100840657949448\n",
      "Epoch 4619, Loss: 0.11952748149633408, Final Batch Loss: 0.06507585942745209\n",
      "Epoch 4620, Loss: 0.2515670321881771, Final Batch Loss: 0.03270527347922325\n",
      "Epoch 4621, Loss: 0.1149744875729084, Final Batch Loss: 0.04874412342905998\n",
      "Epoch 4622, Loss: 0.16750550270080566, Final Batch Loss: 0.08127429336309433\n",
      "Epoch 4623, Loss: 0.07804341614246368, Final Batch Loss: 0.030697345733642578\n",
      "Epoch 4624, Loss: 0.10273217409849167, Final Batch Loss: 0.041373949497938156\n",
      "Epoch 4625, Loss: 0.08153752982616425, Final Batch Loss: 0.0490826815366745\n",
      "Epoch 4626, Loss: 0.11086321994662285, Final Batch Loss: 0.059802934527397156\n",
      "Epoch 4627, Loss: 0.13236146420240402, Final Batch Loss: 0.0617985799908638\n",
      "Epoch 4628, Loss: 0.0698711909353733, Final Batch Loss: 0.02099129930138588\n",
      "Epoch 4629, Loss: 0.10134968906641006, Final Batch Loss: 0.06097349524497986\n",
      "Epoch 4630, Loss: 0.11989373713731766, Final Batch Loss: 0.06824158132076263\n",
      "Epoch 4631, Loss: 0.16082006692886353, Final Batch Loss: 0.11047632247209549\n",
      "Epoch 4632, Loss: 0.10119727626442909, Final Batch Loss: 0.04510585963726044\n",
      "Epoch 4633, Loss: 0.22180267423391342, Final Batch Loss: 0.12362152338027954\n",
      "Epoch 4634, Loss: 0.13157104700803757, Final Batch Loss: 0.09372124820947647\n",
      "Epoch 4635, Loss: 0.1319633461534977, Final Batch Loss: 0.07942253351211548\n",
      "Epoch 4636, Loss: 0.0993729941546917, Final Batch Loss: 0.040270108729600906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4637, Loss: 0.17006366327404976, Final Batch Loss: 0.1167977824807167\n",
      "Epoch 4638, Loss: 0.13113868609070778, Final Batch Loss: 0.07461695373058319\n",
      "Epoch 4639, Loss: 0.10599104315042496, Final Batch Loss: 0.042222410440444946\n",
      "Epoch 4640, Loss: 0.11417146027088165, Final Batch Loss: 0.06379832327365875\n",
      "Epoch 4641, Loss: 0.08904274925589561, Final Batch Loss: 0.028914954513311386\n",
      "Epoch 4642, Loss: 0.11680889502167702, Final Batch Loss: 0.06164427846670151\n",
      "Epoch 4643, Loss: 0.11652570031583309, Final Batch Loss: 0.02060638926923275\n",
      "Epoch 4644, Loss: 0.16615453362464905, Final Batch Loss: 0.09430646896362305\n",
      "Epoch 4645, Loss: 0.1297839730978012, Final Batch Loss: 0.06408414989709854\n",
      "Epoch 4646, Loss: 0.14825913682579994, Final Batch Loss: 0.037742730230093\n",
      "Epoch 4647, Loss: 0.19413278251886368, Final Batch Loss: 0.12317360937595367\n",
      "Epoch 4648, Loss: 0.14181414619088173, Final Batch Loss: 0.0926349088549614\n",
      "Epoch 4649, Loss: 0.10377468913793564, Final Batch Loss: 0.052712954580783844\n",
      "Epoch 4650, Loss: 0.10231079906225204, Final Batch Loss: 0.045455195009708405\n",
      "Epoch 4651, Loss: 0.12406773492693901, Final Batch Loss: 0.07247396558523178\n",
      "Epoch 4652, Loss: 0.09071093425154686, Final Batch Loss: 0.0431482158601284\n",
      "Epoch 4653, Loss: 0.14630775898694992, Final Batch Loss: 0.08027292788028717\n",
      "Epoch 4654, Loss: 0.111886166036129, Final Batch Loss: 0.03575613349676132\n",
      "Epoch 4655, Loss: 0.12927720695734024, Final Batch Loss: 0.04206649959087372\n",
      "Epoch 4656, Loss: 0.1405913196504116, Final Batch Loss: 0.09159959107637405\n",
      "Epoch 4657, Loss: 0.10778256878256798, Final Batch Loss: 0.03266659006476402\n",
      "Epoch 4658, Loss: 0.13873808830976486, Final Batch Loss: 0.08663558214902878\n",
      "Epoch 4659, Loss: 0.16287997364997864, Final Batch Loss: 0.10945044457912445\n",
      "Epoch 4660, Loss: 0.14594625681638718, Final Batch Loss: 0.11358232796192169\n",
      "Epoch 4661, Loss: 0.15122468024492264, Final Batch Loss: 0.06678502261638641\n",
      "Epoch 4662, Loss: 0.11339449137449265, Final Batch Loss: 0.033628687262535095\n",
      "Epoch 4663, Loss: 0.12967360019683838, Final Batch Loss: 0.07231168448925018\n",
      "Epoch 4664, Loss: 0.11243641003966331, Final Batch Loss: 0.023612897843122482\n",
      "Epoch 4665, Loss: 0.16143985465168953, Final Batch Loss: 0.11247528344392776\n",
      "Epoch 4666, Loss: 0.1164349764585495, Final Batch Loss: 0.09260173887014389\n",
      "Epoch 4667, Loss: 0.14377006515860558, Final Batch Loss: 0.08926141262054443\n",
      "Epoch 4668, Loss: 0.12568792328238487, Final Batch Loss: 0.04721934720873833\n",
      "Epoch 4669, Loss: 0.07836686260998249, Final Batch Loss: 0.01871440000832081\n",
      "Epoch 4670, Loss: 0.08826881647109985, Final Batch Loss: 0.042525023221969604\n",
      "Epoch 4671, Loss: 0.12173033133149147, Final Batch Loss: 0.04175272956490517\n",
      "Epoch 4672, Loss: 0.1104942075908184, Final Batch Loss: 0.055219464004039764\n",
      "Epoch 4673, Loss: 0.1621895581483841, Final Batch Loss: 0.0872742086648941\n",
      "Epoch 4674, Loss: 0.10491198673844337, Final Batch Loss: 0.054953254759311676\n",
      "Epoch 4675, Loss: 0.2768760547041893, Final Batch Loss: 0.21032088994979858\n",
      "Epoch 4676, Loss: 0.22789568454027176, Final Batch Loss: 0.12667661905288696\n",
      "Epoch 4677, Loss: 0.14490696415305138, Final Batch Loss: 0.0541783906519413\n",
      "Epoch 4678, Loss: 0.17159312218427658, Final Batch Loss: 0.07699993997812271\n",
      "Epoch 4679, Loss: 0.14872781187295914, Final Batch Loss: 0.09132354706525803\n",
      "Epoch 4680, Loss: 0.14588484168052673, Final Batch Loss: 0.09616963565349579\n",
      "Epoch 4681, Loss: 0.1537260115146637, Final Batch Loss: 0.07665786892175674\n",
      "Epoch 4682, Loss: 0.17412206903100014, Final Batch Loss: 0.12127700448036194\n",
      "Epoch 4683, Loss: 0.14408887922763824, Final Batch Loss: 0.09226489812135696\n",
      "Epoch 4684, Loss: 0.18188918009400368, Final Batch Loss: 0.1226360946893692\n",
      "Epoch 4685, Loss: 0.13964669778943062, Final Batch Loss: 0.07934591919183731\n",
      "Epoch 4686, Loss: 0.15838027745485306, Final Batch Loss: 0.06930456310510635\n",
      "Epoch 4687, Loss: 0.0962974838912487, Final Batch Loss: 0.05708431079983711\n",
      "Epoch 4688, Loss: 0.08160899206995964, Final Batch Loss: 0.04349052160978317\n",
      "Epoch 4689, Loss: 0.10588518530130386, Final Batch Loss: 0.05455724522471428\n",
      "Epoch 4690, Loss: 0.1294674500823021, Final Batch Loss: 0.08825653791427612\n",
      "Epoch 4691, Loss: 0.17823927849531174, Final Batch Loss: 0.0760861486196518\n",
      "Epoch 4692, Loss: 0.06091172806918621, Final Batch Loss: 0.023185761645436287\n",
      "Epoch 4693, Loss: 0.10519761964678764, Final Batch Loss: 0.045095186680555344\n",
      "Epoch 4694, Loss: 0.1281038224697113, Final Batch Loss: 0.06374099105596542\n",
      "Epoch 4695, Loss: 0.09789779782295227, Final Batch Loss: 0.06311850249767303\n",
      "Epoch 4696, Loss: 0.09444670751690865, Final Batch Loss: 0.0756942555308342\n",
      "Epoch 4697, Loss: 0.1398455873131752, Final Batch Loss: 0.06897047907114029\n",
      "Epoch 4698, Loss: 0.14581377059221268, Final Batch Loss: 0.07460479438304901\n",
      "Epoch 4699, Loss: 0.07547578774392605, Final Batch Loss: 0.028081906959414482\n",
      "Epoch 4700, Loss: 0.23577889800071716, Final Batch Loss: 0.17387862503528595\n",
      "Epoch 4701, Loss: 0.08342373743653297, Final Batch Loss: 0.04637979343533516\n",
      "Epoch 4702, Loss: 0.11203898675739765, Final Batch Loss: 0.026683414354920387\n",
      "Epoch 4703, Loss: 0.19467195123434067, Final Batch Loss: 0.0859052985906601\n",
      "Epoch 4704, Loss: 0.13963043689727783, Final Batch Loss: 0.07934127002954483\n",
      "Epoch 4705, Loss: 0.12017710134387016, Final Batch Loss: 0.08628148585557938\n",
      "Epoch 4706, Loss: 0.09832162037491798, Final Batch Loss: 0.048074450343847275\n",
      "Epoch 4707, Loss: 0.11346148699522018, Final Batch Loss: 0.038518138229846954\n",
      "Epoch 4708, Loss: 0.06654242053627968, Final Batch Loss: 0.025700300931930542\n",
      "Epoch 4709, Loss: 0.1366252899169922, Final Batch Loss: 0.0658823773264885\n",
      "Epoch 4710, Loss: 0.10463893041014671, Final Batch Loss: 0.03845817968249321\n",
      "Epoch 4711, Loss: 0.07984569668769836, Final Batch Loss: 0.048433445394039154\n",
      "Epoch 4712, Loss: 0.1697802171111107, Final Batch Loss: 0.1215282529592514\n",
      "Epoch 4713, Loss: 0.12815380468964577, Final Batch Loss: 0.05567130073904991\n",
      "Epoch 4714, Loss: 0.20128577202558517, Final Batch Loss: 0.13273242115974426\n",
      "Epoch 4715, Loss: 0.12081785872578621, Final Batch Loss: 0.06579544395208359\n",
      "Epoch 4716, Loss: 0.14367426559329033, Final Batch Loss: 0.09050028026103973\n",
      "Epoch 4717, Loss: 0.0910598672926426, Final Batch Loss: 0.02837163582444191\n",
      "Epoch 4718, Loss: 0.08643735200166702, Final Batch Loss: 0.049613695591688156\n",
      "Epoch 4719, Loss: 0.09594595059752464, Final Batch Loss: 0.045125216245651245\n",
      "Epoch 4720, Loss: 0.0748637244105339, Final Batch Loss: 0.041866179555654526\n",
      "Epoch 4721, Loss: 0.10800761356949806, Final Batch Loss: 0.04394291713833809\n",
      "Epoch 4722, Loss: 0.0725910272449255, Final Batch Loss: 0.0461801178753376\n",
      "Epoch 4723, Loss: 0.11433350294828415, Final Batch Loss: 0.04300781339406967\n",
      "Epoch 4724, Loss: 0.08805256336927414, Final Batch Loss: 0.02484273910522461\n",
      "Epoch 4725, Loss: 0.10794772766530514, Final Batch Loss: 0.0782235786318779\n",
      "Epoch 4726, Loss: 0.10197988897562027, Final Batch Loss: 0.04039458930492401\n",
      "Epoch 4727, Loss: 0.14055000245571136, Final Batch Loss: 0.07048016041517258\n",
      "Epoch 4728, Loss: 0.141011081635952, Final Batch Loss: 0.08031784743070602\n",
      "Epoch 4729, Loss: 0.0752926804125309, Final Batch Loss: 0.0379004068672657\n",
      "Epoch 4730, Loss: 0.0797249935567379, Final Batch Loss: 0.03240954130887985\n",
      "Epoch 4731, Loss: 0.09215663000941277, Final Batch Loss: 0.0408928245306015\n",
      "Epoch 4732, Loss: 0.12138795852661133, Final Batch Loss: 0.03252261132001877\n",
      "Epoch 4733, Loss: 0.10389729589223862, Final Batch Loss: 0.04269104450941086\n",
      "Epoch 4734, Loss: 0.15660522133111954, Final Batch Loss: 0.06468599289655685\n",
      "Epoch 4735, Loss: 0.07159978523850441, Final Batch Loss: 0.0241774320602417\n",
      "Epoch 4736, Loss: 0.09481481835246086, Final Batch Loss: 0.05527443811297417\n",
      "Epoch 4737, Loss: 0.0995078943669796, Final Batch Loss: 0.057917676866054535\n",
      "Epoch 4738, Loss: 0.15423951297998428, Final Batch Loss: 0.07476916164159775\n",
      "Epoch 4739, Loss: 0.10002380982041359, Final Batch Loss: 0.03198902681469917\n",
      "Epoch 4740, Loss: 0.13256932608783245, Final Batch Loss: 0.10811407119035721\n",
      "Epoch 4741, Loss: 0.1887052059173584, Final Batch Loss: 0.09303253889083862\n",
      "Epoch 4742, Loss: 0.10773888602852821, Final Batch Loss: 0.034274425357580185\n",
      "Epoch 4743, Loss: 0.11258105933666229, Final Batch Loss: 0.05722269043326378\n",
      "Epoch 4744, Loss: 0.09024832583963871, Final Batch Loss: 0.06863610446453094\n",
      "Epoch 4745, Loss: 0.11587836593389511, Final Batch Loss: 0.04651261121034622\n",
      "Epoch 4746, Loss: 0.13946735113859177, Final Batch Loss: 0.04960141330957413\n",
      "Epoch 4747, Loss: 0.08329209312796593, Final Batch Loss: 0.05737580731511116\n",
      "Epoch 4748, Loss: 0.09033872373402119, Final Batch Loss: 0.06487412750720978\n",
      "Epoch 4749, Loss: 0.08021798357367516, Final Batch Loss: 0.037236228585243225\n",
      "Epoch 4750, Loss: 0.2972898595035076, Final Batch Loss: 0.2384968400001526\n",
      "Epoch 4751, Loss: 0.07327401638031006, Final Batch Loss: 0.034873440861701965\n",
      "Epoch 4752, Loss: 0.06807402335107327, Final Batch Loss: 0.027104264125227928\n",
      "Epoch 4753, Loss: 0.14463738724589348, Final Batch Loss: 0.09736098349094391\n",
      "Epoch 4754, Loss: 0.09716062620282173, Final Batch Loss: 0.03338175639510155\n",
      "Epoch 4755, Loss: 0.10856125876307487, Final Batch Loss: 0.03901049867272377\n",
      "Epoch 4756, Loss: 0.10370520129799843, Final Batch Loss: 0.05934229865670204\n",
      "Epoch 4757, Loss: 0.1295791082084179, Final Batch Loss: 0.045557428151369095\n",
      "Epoch 4758, Loss: 0.16149619221687317, Final Batch Loss: 0.03143298625946045\n",
      "Epoch 4759, Loss: 0.08424478769302368, Final Batch Loss: 0.04052348434925079\n",
      "Epoch 4760, Loss: 0.12163062766194344, Final Batch Loss: 0.043640438467264175\n",
      "Epoch 4761, Loss: 0.15994663536548615, Final Batch Loss: 0.09715069830417633\n",
      "Epoch 4762, Loss: 0.1399739868938923, Final Batch Loss: 0.0400882326066494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4763, Loss: 0.1324627697467804, Final Batch Loss: 0.04913416504859924\n",
      "Epoch 4764, Loss: 0.1632384192198515, Final Batch Loss: 0.13298329710960388\n",
      "Epoch 4765, Loss: 0.14703261107206345, Final Batch Loss: 0.08538659662008286\n",
      "Epoch 4766, Loss: 0.12654755264520645, Final Batch Loss: 0.04768978804349899\n",
      "Epoch 4767, Loss: 0.10827579349279404, Final Batch Loss: 0.05454976111650467\n",
      "Epoch 4768, Loss: 0.051414771005511284, Final Batch Loss: 0.022061076015233994\n",
      "Epoch 4769, Loss: 0.11331448704004288, Final Batch Loss: 0.01947396993637085\n",
      "Epoch 4770, Loss: 0.08380824699997902, Final Batch Loss: 0.04007108882069588\n",
      "Epoch 4771, Loss: 0.0648596528917551, Final Batch Loss: 0.027989068999886513\n",
      "Epoch 4772, Loss: 0.09930180385708809, Final Batch Loss: 0.04708491638302803\n",
      "Epoch 4773, Loss: 0.17604220658540726, Final Batch Loss: 0.09268316626548767\n",
      "Epoch 4774, Loss: 0.12681452557444572, Final Batch Loss: 0.09280478954315186\n",
      "Epoch 4775, Loss: 0.13216469064354897, Final Batch Loss: 0.08029874414205551\n",
      "Epoch 4776, Loss: 0.10770672559738159, Final Batch Loss: 0.04347670078277588\n",
      "Epoch 4777, Loss: 0.09998027235269547, Final Batch Loss: 0.05469171330332756\n",
      "Epoch 4778, Loss: 0.12031805887818336, Final Batch Loss: 0.059917423874139786\n",
      "Epoch 4779, Loss: 0.1256858929991722, Final Batch Loss: 0.0641336739063263\n",
      "Epoch 4780, Loss: 0.14023087546229362, Final Batch Loss: 0.10078419744968414\n",
      "Epoch 4781, Loss: 0.11744875833392143, Final Batch Loss: 0.06651291251182556\n",
      "Epoch 4782, Loss: 0.07719080150127411, Final Batch Loss: 0.023529600352048874\n",
      "Epoch 4783, Loss: 0.13123955577611923, Final Batch Loss: 0.06451112776994705\n",
      "Epoch 4784, Loss: 0.1093129813671112, Final Batch Loss: 0.021791696548461914\n",
      "Epoch 4785, Loss: 0.05375235714018345, Final Batch Loss: 0.025583701208233833\n",
      "Epoch 4786, Loss: 0.12055684253573418, Final Batch Loss: 0.04290955886244774\n",
      "Epoch 4787, Loss: 0.0883348286151886, Final Batch Loss: 0.03728922829031944\n",
      "Epoch 4788, Loss: 0.07596088200807571, Final Batch Loss: 0.029730699956417084\n",
      "Epoch 4789, Loss: 0.11406166106462479, Final Batch Loss: 0.06537804752588272\n",
      "Epoch 4790, Loss: 0.10057616978883743, Final Batch Loss: 0.04167123883962631\n",
      "Epoch 4791, Loss: 0.104500662535429, Final Batch Loss: 0.061823852360248566\n",
      "Epoch 4792, Loss: 0.08861081674695015, Final Batch Loss: 0.051238052546978\n",
      "Epoch 4793, Loss: 0.13855931535363197, Final Batch Loss: 0.07837551832199097\n",
      "Epoch 4794, Loss: 0.1962960734963417, Final Batch Loss: 0.11423298716545105\n",
      "Epoch 4795, Loss: 0.10078026726841927, Final Batch Loss: 0.05256359279155731\n",
      "Epoch 4796, Loss: 0.1341933310031891, Final Batch Loss: 0.053819604218006134\n",
      "Epoch 4797, Loss: 0.09934869781136513, Final Batch Loss: 0.037063077092170715\n",
      "Epoch 4798, Loss: 0.07530747354030609, Final Batch Loss: 0.03562265634536743\n",
      "Epoch 4799, Loss: 0.1263984702527523, Final Batch Loss: 0.04910992458462715\n",
      "Epoch 4800, Loss: 0.09425030648708344, Final Batch Loss: 0.04682297259569168\n",
      "Epoch 4801, Loss: 0.11022042483091354, Final Batch Loss: 0.06297046691179276\n",
      "Epoch 4802, Loss: 0.16234628856182098, Final Batch Loss: 0.0672198086977005\n",
      "Epoch 4803, Loss: 0.0568816252052784, Final Batch Loss: 0.021351099014282227\n",
      "Epoch 4804, Loss: 0.07603887096047401, Final Batch Loss: 0.017836961895227432\n",
      "Epoch 4805, Loss: 0.18641509860754013, Final Batch Loss: 0.07141207903623581\n",
      "Epoch 4806, Loss: 0.04132881574332714, Final Batch Loss: 0.012624448165297508\n",
      "Epoch 4807, Loss: 0.14837729558348656, Final Batch Loss: 0.11626940965652466\n",
      "Epoch 4808, Loss: 0.1127166599035263, Final Batch Loss: 0.0398392453789711\n",
      "Epoch 4809, Loss: 0.09045812115073204, Final Batch Loss: 0.03719130530953407\n",
      "Epoch 4810, Loss: 0.05096855852752924, Final Batch Loss: 0.015009612776339054\n",
      "Epoch 4811, Loss: 0.14482355117797852, Final Batch Loss: 0.1021481454372406\n",
      "Epoch 4812, Loss: 0.09776345826685429, Final Batch Loss: 0.07261039316654205\n",
      "Epoch 4813, Loss: 0.09467463195323944, Final Batch Loss: 0.04576348513364792\n",
      "Epoch 4814, Loss: 0.09783092513680458, Final Batch Loss: 0.04215625301003456\n",
      "Epoch 4815, Loss: 0.10336080193519592, Final Batch Loss: 0.02234247326850891\n",
      "Epoch 4816, Loss: 0.09260662645101547, Final Batch Loss: 0.04789786785840988\n",
      "Epoch 4817, Loss: 0.10184856131672859, Final Batch Loss: 0.03932617977261543\n",
      "Epoch 4818, Loss: 0.09292205795645714, Final Batch Loss: 0.04289107769727707\n",
      "Epoch 4819, Loss: 0.12128803879022598, Final Batch Loss: 0.0772627517580986\n",
      "Epoch 4820, Loss: 0.15921572595834732, Final Batch Loss: 0.052398018538951874\n",
      "Epoch 4821, Loss: 0.13610392063856125, Final Batch Loss: 0.08602635562419891\n",
      "Epoch 4822, Loss: 0.17633496969938278, Final Batch Loss: 0.08214500546455383\n",
      "Epoch 4823, Loss: 0.20485837757587433, Final Batch Loss: 0.08673380315303802\n",
      "Epoch 4824, Loss: 0.20885071903467178, Final Batch Loss: 0.12073525041341782\n",
      "Epoch 4825, Loss: 0.10012238472700119, Final Batch Loss: 0.03708837181329727\n",
      "Epoch 4826, Loss: 0.1213562935590744, Final Batch Loss: 0.08243898302316666\n",
      "Epoch 4827, Loss: 0.11255927011370659, Final Batch Loss: 0.03417207673192024\n",
      "Epoch 4828, Loss: 0.10845135897397995, Final Batch Loss: 0.05248922482132912\n",
      "Epoch 4829, Loss: 0.1260814629495144, Final Batch Loss: 0.06631732732057571\n",
      "Epoch 4830, Loss: 0.1707371212542057, Final Batch Loss: 0.13731984794139862\n",
      "Epoch 4831, Loss: 0.09548189118504524, Final Batch Loss: 0.04291924089193344\n",
      "Epoch 4832, Loss: 0.15260958671569824, Final Batch Loss: 0.0674009919166565\n",
      "Epoch 4833, Loss: 0.15868200361728668, Final Batch Loss: 0.09735649079084396\n",
      "Epoch 4834, Loss: 0.09746663272380829, Final Batch Loss: 0.03552164137363434\n",
      "Epoch 4835, Loss: 0.14532345905900002, Final Batch Loss: 0.04959828034043312\n",
      "Epoch 4836, Loss: 0.151827834546566, Final Batch Loss: 0.0454370453953743\n",
      "Epoch 4837, Loss: 0.10138630494475365, Final Batch Loss: 0.07151719182729721\n",
      "Epoch 4838, Loss: 0.12158840522170067, Final Batch Loss: 0.08386681973934174\n",
      "Epoch 4839, Loss: 0.08296078070998192, Final Batch Loss: 0.04112634435296059\n",
      "Epoch 4840, Loss: 0.08807095140218735, Final Batch Loss: 0.0499323271214962\n",
      "Epoch 4841, Loss: 0.21967971324920654, Final Batch Loss: 0.1280842125415802\n",
      "Epoch 4842, Loss: 0.11667117476463318, Final Batch Loss: 0.06792628020048141\n",
      "Epoch 4843, Loss: 0.14545784890651703, Final Batch Loss: 0.08776988834142685\n",
      "Epoch 4844, Loss: 0.264782153069973, Final Batch Loss: 0.23186077177524567\n",
      "Epoch 4845, Loss: 0.09799937158823013, Final Batch Loss: 0.04500643536448479\n",
      "Epoch 4846, Loss: 0.22485437989234924, Final Batch Loss: 0.14513641595840454\n",
      "Epoch 4847, Loss: 0.14620128646492958, Final Batch Loss: 0.09793191403150558\n",
      "Epoch 4848, Loss: 0.09339159168303013, Final Batch Loss: 0.023839952424168587\n",
      "Epoch 4849, Loss: 0.12503347545862198, Final Batch Loss: 0.04734252393245697\n",
      "Epoch 4850, Loss: 0.23711200803518295, Final Batch Loss: 0.09044379740953445\n",
      "Epoch 4851, Loss: 0.12470894306898117, Final Batch Loss: 0.06874244660139084\n",
      "Epoch 4852, Loss: 0.11552102863788605, Final Batch Loss: 0.04569600522518158\n",
      "Epoch 4853, Loss: 0.1044430211186409, Final Batch Loss: 0.05279603973031044\n",
      "Epoch 4854, Loss: 0.0918053574860096, Final Batch Loss: 0.043621473014354706\n",
      "Epoch 4855, Loss: 0.10665210336446762, Final Batch Loss: 0.05704443156719208\n",
      "Epoch 4856, Loss: 0.08581987395882607, Final Batch Loss: 0.04881326109170914\n",
      "Epoch 4857, Loss: 0.13173433020710945, Final Batch Loss: 0.08910518139600754\n",
      "Epoch 4858, Loss: 0.14727721735835075, Final Batch Loss: 0.09144323319196701\n",
      "Epoch 4859, Loss: 0.09050586260855198, Final Batch Loss: 0.026944288983941078\n",
      "Epoch 4860, Loss: 0.08692380040884018, Final Batch Loss: 0.04555331543087959\n",
      "Epoch 4861, Loss: 0.17695453017950058, Final Batch Loss: 0.10345391929149628\n",
      "Epoch 4862, Loss: 0.1551840901374817, Final Batch Loss: 0.08668450266122818\n",
      "Epoch 4863, Loss: 0.11013191193342209, Final Batch Loss: 0.04859866201877594\n",
      "Epoch 4864, Loss: 0.18703920766711235, Final Batch Loss: 0.14368997514247894\n",
      "Epoch 4865, Loss: 0.10244832187891006, Final Batch Loss: 0.04575186222791672\n",
      "Epoch 4866, Loss: 0.13988689705729485, Final Batch Loss: 0.08825517445802689\n",
      "Epoch 4867, Loss: 0.12048226222395897, Final Batch Loss: 0.05272791162133217\n",
      "Epoch 4868, Loss: 0.09376828745007515, Final Batch Loss: 0.037683386355638504\n",
      "Epoch 4869, Loss: 0.12121079117059708, Final Batch Loss: 0.07235634326934814\n",
      "Epoch 4870, Loss: 0.08471369743347168, Final Batch Loss: 0.04676426202058792\n",
      "Epoch 4871, Loss: 0.24338661134243011, Final Batch Loss: 0.1689172089099884\n",
      "Epoch 4872, Loss: 0.07807416096329689, Final Batch Loss: 0.04099244996905327\n",
      "Epoch 4873, Loss: 0.12475810199975967, Final Batch Loss: 0.03209204971790314\n",
      "Epoch 4874, Loss: 0.13219856843352318, Final Batch Loss: 0.08982062339782715\n",
      "Epoch 4875, Loss: 0.12236740067601204, Final Batch Loss: 0.07513395696878433\n",
      "Epoch 4876, Loss: 0.08526620641350746, Final Batch Loss: 0.036305949091911316\n",
      "Epoch 4877, Loss: 0.11497915536165237, Final Batch Loss: 0.059123024344444275\n",
      "Epoch 4878, Loss: 0.17246725410223007, Final Batch Loss: 0.08177164942026138\n",
      "Epoch 4879, Loss: 0.1727372258901596, Final Batch Loss: 0.08735480904579163\n",
      "Epoch 4880, Loss: 0.12266812473535538, Final Batch Loss: 0.05991137772798538\n",
      "Epoch 4881, Loss: 0.17955166101455688, Final Batch Loss: 0.05511055886745453\n",
      "Epoch 4882, Loss: 0.08655331842601299, Final Batch Loss: 0.05955176427960396\n",
      "Epoch 4883, Loss: 0.14181926846504211, Final Batch Loss: 0.04289594292640686\n",
      "Epoch 4884, Loss: 0.12433290481567383, Final Batch Loss: 0.04473286122083664\n",
      "Epoch 4885, Loss: 0.1372540146112442, Final Batch Loss: 0.0730014219880104\n",
      "Epoch 4886, Loss: 0.13150761649012566, Final Batch Loss: 0.09973952174186707\n",
      "Epoch 4887, Loss: 0.13262475654482841, Final Batch Loss: 0.037508685141801834\n",
      "Epoch 4888, Loss: 0.09146768227219582, Final Batch Loss: 0.05377417430281639\n",
      "Epoch 4889, Loss: 0.09654540941119194, Final Batch Loss: 0.03988149017095566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4890, Loss: 0.1095849834382534, Final Batch Loss: 0.06956765055656433\n",
      "Epoch 4891, Loss: 0.09727250784635544, Final Batch Loss: 0.05441095307469368\n",
      "Epoch 4892, Loss: 0.20107436925172806, Final Batch Loss: 0.09297315031290054\n",
      "Epoch 4893, Loss: 0.07688553258776665, Final Batch Loss: 0.03787930682301521\n",
      "Epoch 4894, Loss: 0.24729527905583382, Final Batch Loss: 0.20651330053806305\n",
      "Epoch 4895, Loss: 0.12688320502638817, Final Batch Loss: 0.07122060656547546\n",
      "Epoch 4896, Loss: 0.14646849408745766, Final Batch Loss: 0.10242016613483429\n",
      "Epoch 4897, Loss: 0.13290095701813698, Final Batch Loss: 0.05843358114361763\n",
      "Epoch 4898, Loss: 0.07686680182814598, Final Batch Loss: 0.03517032414674759\n",
      "Epoch 4899, Loss: 0.1162835955619812, Final Batch Loss: 0.03211510181427002\n",
      "Epoch 4900, Loss: 0.11669163405895233, Final Batch Loss: 0.05713307857513428\n",
      "Epoch 4901, Loss: 0.19042429327964783, Final Batch Loss: 0.10048935562372208\n",
      "Epoch 4902, Loss: 0.05121302045881748, Final Batch Loss: 0.02148929052054882\n",
      "Epoch 4903, Loss: 0.092266745865345, Final Batch Loss: 0.04707770794630051\n",
      "Epoch 4904, Loss: 0.12546927854418755, Final Batch Loss: 0.07731223106384277\n",
      "Epoch 4905, Loss: 0.0873912125825882, Final Batch Loss: 0.05275683477520943\n",
      "Epoch 4906, Loss: 0.19843273609876633, Final Batch Loss: 0.0847613513469696\n",
      "Epoch 4907, Loss: 0.11623470857739449, Final Batch Loss: 0.047751929610967636\n",
      "Epoch 4908, Loss: 0.11014562100172043, Final Batch Loss: 0.04942580312490463\n",
      "Epoch 4909, Loss: 0.1680271252989769, Final Batch Loss: 0.0469498410820961\n",
      "Epoch 4910, Loss: 0.11529017612338066, Final Batch Loss: 0.040667880326509476\n",
      "Epoch 4911, Loss: 0.09023911319673061, Final Batch Loss: 0.06386203318834305\n",
      "Epoch 4912, Loss: 0.09532219171524048, Final Batch Loss: 0.06334833800792694\n",
      "Epoch 4913, Loss: 0.12046850845217705, Final Batch Loss: 0.03385431692004204\n",
      "Epoch 4914, Loss: 0.07562530413269997, Final Batch Loss: 0.026489026844501495\n",
      "Epoch 4915, Loss: 0.09096682257950306, Final Batch Loss: 0.06037963181734085\n",
      "Epoch 4916, Loss: 0.13863826170563698, Final Batch Loss: 0.04437321797013283\n",
      "Epoch 4917, Loss: 0.07300876080989838, Final Batch Loss: 0.04164957255125046\n",
      "Epoch 4918, Loss: 0.1687537133693695, Final Batch Loss: 0.09623560309410095\n",
      "Epoch 4919, Loss: 0.12845966964960098, Final Batch Loss: 0.054485023021698\n",
      "Epoch 4920, Loss: 0.09456210397183895, Final Batch Loss: 0.02432098053395748\n",
      "Epoch 4921, Loss: 0.11494273692369461, Final Batch Loss: 0.0201910138130188\n",
      "Epoch 4922, Loss: 0.1170884482562542, Final Batch Loss: 0.06181487813591957\n",
      "Epoch 4923, Loss: 0.11468499153852463, Final Batch Loss: 0.048833899199962616\n",
      "Epoch 4924, Loss: 0.10919786617159843, Final Batch Loss: 0.05307893455028534\n",
      "Epoch 4925, Loss: 0.13158809393644333, Final Batch Loss: 0.06958640366792679\n",
      "Epoch 4926, Loss: 0.1786605417728424, Final Batch Loss: 0.06236825883388519\n",
      "Epoch 4927, Loss: 0.17315831780433655, Final Batch Loss: 0.04988747090101242\n",
      "Epoch 4928, Loss: 0.10574188455939293, Final Batch Loss: 0.04498256370425224\n",
      "Epoch 4929, Loss: 0.13533950597047806, Final Batch Loss: 0.051206350326538086\n",
      "Epoch 4930, Loss: 0.14205822348594666, Final Batch Loss: 0.07894241809844971\n",
      "Epoch 4931, Loss: 0.09841891378164291, Final Batch Loss: 0.06125430017709732\n",
      "Epoch 4932, Loss: 0.16271279752254486, Final Batch Loss: 0.08848654478788376\n",
      "Epoch 4933, Loss: 0.12064467743039131, Final Batch Loss: 0.07003388553857803\n",
      "Epoch 4934, Loss: 0.1617528200149536, Final Batch Loss: 0.12326061725616455\n",
      "Epoch 4935, Loss: 0.09756230935454369, Final Batch Loss: 0.036303456872701645\n",
      "Epoch 4936, Loss: 0.08626754581928253, Final Batch Loss: 0.04887785017490387\n",
      "Epoch 4937, Loss: 0.10105568915605545, Final Batch Loss: 0.032446540892124176\n",
      "Epoch 4938, Loss: 0.07914477214217186, Final Batch Loss: 0.033363226801157\n",
      "Epoch 4939, Loss: 0.11049218662083149, Final Batch Loss: 0.08067384362220764\n",
      "Epoch 4940, Loss: 0.10692034289240837, Final Batch Loss: 0.03267490491271019\n",
      "Epoch 4941, Loss: 0.13664089143276215, Final Batch Loss: 0.05536369979381561\n",
      "Epoch 4942, Loss: 0.18073716759681702, Final Batch Loss: 0.09973911195993423\n",
      "Epoch 4943, Loss: 0.15100550651550293, Final Batch Loss: 0.10729368031024933\n",
      "Epoch 4944, Loss: 0.07600115239620209, Final Batch Loss: 0.02644459903240204\n",
      "Epoch 4945, Loss: 0.14989878609776497, Final Batch Loss: 0.04253363981842995\n",
      "Epoch 4946, Loss: 0.1533045619726181, Final Batch Loss: 0.09519162029027939\n",
      "Epoch 4947, Loss: 0.1535128764808178, Final Batch Loss: 0.09127361327409744\n",
      "Epoch 4948, Loss: 0.1606295444071293, Final Batch Loss: 0.05717198923230171\n",
      "Epoch 4949, Loss: 0.1306680589914322, Final Batch Loss: 0.05212782323360443\n",
      "Epoch 4950, Loss: 0.20000217109918594, Final Batch Loss: 0.0834120661020279\n",
      "Epoch 4951, Loss: 0.12932368740439415, Final Batch Loss: 0.09575402736663818\n",
      "Epoch 4952, Loss: 0.16015782952308655, Final Batch Loss: 0.06422284245491028\n",
      "Epoch 4953, Loss: 0.1168716587126255, Final Batch Loss: 0.0694955587387085\n",
      "Epoch 4954, Loss: 0.10493456572294235, Final Batch Loss: 0.057964324951171875\n",
      "Epoch 4955, Loss: 0.12268668040633202, Final Batch Loss: 0.04047832265496254\n",
      "Epoch 4956, Loss: 0.08715405315160751, Final Batch Loss: 0.03171588480472565\n",
      "Epoch 4957, Loss: 0.14194697886705399, Final Batch Loss: 0.06552872806787491\n",
      "Epoch 4958, Loss: 0.08223471790552139, Final Batch Loss: 0.04641331732273102\n",
      "Epoch 4959, Loss: 0.1455504521727562, Final Batch Loss: 0.07434725761413574\n",
      "Epoch 4960, Loss: 0.1210956908762455, Final Batch Loss: 0.03451645001769066\n",
      "Epoch 4961, Loss: 0.11879532039165497, Final Batch Loss: 0.059574227780103683\n",
      "Epoch 4962, Loss: 0.15667788684368134, Final Batch Loss: 0.10825978219509125\n",
      "Epoch 4963, Loss: 0.11276421323418617, Final Batch Loss: 0.07624687999486923\n",
      "Epoch 4964, Loss: 0.0899626575410366, Final Batch Loss: 0.035708341747522354\n",
      "Epoch 4965, Loss: 0.11521224305033684, Final Batch Loss: 0.05699099972844124\n",
      "Epoch 4966, Loss: 0.1418137140572071, Final Batch Loss: 0.06225213035941124\n",
      "Epoch 4967, Loss: 0.10924748331308365, Final Batch Loss: 0.055716171860694885\n",
      "Epoch 4968, Loss: 0.19052473455667496, Final Batch Loss: 0.11428252607584\n",
      "Epoch 4969, Loss: 0.14738012477755547, Final Batch Loss: 0.05551924929022789\n",
      "Epoch 4970, Loss: 0.12083769962191582, Final Batch Loss: 0.053309325128793716\n",
      "Epoch 4971, Loss: 0.1439782977104187, Final Batch Loss: 0.08346842974424362\n",
      "Epoch 4972, Loss: 0.1247458141297102, Final Batch Loss: 0.028643770143389702\n",
      "Epoch 4973, Loss: 0.1442158743739128, Final Batch Loss: 0.10057256370782852\n",
      "Epoch 4974, Loss: 0.09487762302160263, Final Batch Loss: 0.04385114088654518\n",
      "Epoch 4975, Loss: 0.14574820548295975, Final Batch Loss: 0.07180468738079071\n",
      "Epoch 4976, Loss: 0.09041257575154305, Final Batch Loss: 0.024411644786596298\n",
      "Epoch 4977, Loss: 0.09667197056114674, Final Batch Loss: 0.029385270550847054\n",
      "Epoch 4978, Loss: 0.06740042567253113, Final Batch Loss: 0.02585475891828537\n",
      "Epoch 4979, Loss: 0.09415068477392197, Final Batch Loss: 0.04818836227059364\n",
      "Epoch 4980, Loss: 0.13596445322036743, Final Batch Loss: 0.06841441988945007\n",
      "Epoch 4981, Loss: 0.14578135311603546, Final Batch Loss: 0.06528282910585403\n",
      "Epoch 4982, Loss: 0.11063773185014725, Final Batch Loss: 0.07347887754440308\n",
      "Epoch 4983, Loss: 0.11914476752281189, Final Batch Loss: 0.06372300535440445\n",
      "Epoch 4984, Loss: 0.09875371679663658, Final Batch Loss: 0.0581287145614624\n",
      "Epoch 4985, Loss: 0.05303534772247076, Final Batch Loss: 0.01415578555315733\n",
      "Epoch 4986, Loss: 0.1283552348613739, Final Batch Loss: 0.05339977890253067\n",
      "Epoch 4987, Loss: 0.13125135004520416, Final Batch Loss: 0.0756511315703392\n",
      "Epoch 4988, Loss: 0.21594055369496346, Final Batch Loss: 0.060009587556123734\n",
      "Epoch 4989, Loss: 0.1019752249121666, Final Batch Loss: 0.03645110875368118\n",
      "Epoch 4990, Loss: 0.09428998455405235, Final Batch Loss: 0.02230698987841606\n",
      "Epoch 4991, Loss: 0.1836526170372963, Final Batch Loss: 0.10460501164197922\n",
      "Epoch 4992, Loss: 0.11302327923476696, Final Batch Loss: 0.08670340478420258\n",
      "Epoch 4993, Loss: 0.10754789784550667, Final Batch Loss: 0.04427233710885048\n",
      "Epoch 4994, Loss: 0.1616903580725193, Final Batch Loss: 0.11174679547548294\n",
      "Epoch 4995, Loss: 0.13500016182661057, Final Batch Loss: 0.05616001784801483\n",
      "Epoch 4996, Loss: 0.08762418106198311, Final Batch Loss: 0.04645648971199989\n",
      "Epoch 4997, Loss: 0.08526775613427162, Final Batch Loss: 0.05137169361114502\n",
      "Epoch 4998, Loss: 0.09734344482421875, Final Batch Loss: 0.04441705718636513\n",
      "Epoch 4999, Loss: 0.23453513346612453, Final Batch Loss: 0.021661167964339256\n",
      "Epoch 5000, Loss: 0.14956629276275635, Final Batch Loss: 0.07568538188934326\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  0  0]\n",
      " [ 0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0]\n",
      " [ 0  0  1  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  0  0]\n",
      " [ 0  2  0  0  0  0  0  8  0]\n",
      " [ 0  0  0  0  0  0  0  0 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000        11\n",
      "           1    0.71429   1.00000   0.83333         5\n",
      "           2    0.92857   1.00000   0.96296        13\n",
      "           3    1.00000   1.00000   1.00000         9\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    1.00000   0.94737   0.97297        19\n",
      "           6    1.00000   1.00000   1.00000         6\n",
      "           7    1.00000   0.80000   0.88889        10\n",
      "           8    1.00000   1.00000   1.00000        16\n",
      "\n",
      "    accuracy                        0.96939        98\n",
      "   macro avg    0.96032   0.97193   0.96202        98\n",
      "weighted avg    0.97595   0.96939   0.97001        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Group 4_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0]\n",
      " [ 0 19  0  0  0  0  0  1  0]\n",
      " [ 0  0 17  0  0  1  0  0  2]\n",
      " [ 0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0]\n",
      " [ 0  0  2  3  0 11  0  0  4]\n",
      " [ 0  0  0  0  0  0 12  8  0]\n",
      " [ 0  0  0  0  7  0  0 13  0]\n",
      " [ 0  0  0  0  0  9  0  0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    1.00000   0.95000   0.97436        20\n",
      "         2.0    0.89474   0.85000   0.87179        20\n",
      "         3.0    0.86957   1.00000   0.93023        20\n",
      "         4.0    0.74074   1.00000   0.85106        20\n",
      "         5.0    0.52381   0.55000   0.53659        20\n",
      "         6.0    1.00000   0.60000   0.75000        20\n",
      "         7.0    0.59091   0.65000   0.61905        20\n",
      "         8.0    0.64706   0.55000   0.59459        20\n",
      "\n",
      "    accuracy                        0.79444       180\n",
      "   macro avg    0.80742   0.79444   0.79196       180\n",
      "weighted avg    0.80742   0.79444   0.79196       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
