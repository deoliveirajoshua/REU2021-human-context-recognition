{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_features = ['43 tGravityAcc-mean()-Z',\n",
    "#  '52 tGravityAcc-max()-Z',\n",
    "#  '54 tGravityAcc-min()-Y',\n",
    "#  '55 tGravityAcc-min()-Z',\n",
    "#  '56 tGravityAcc-sma()',\n",
    "#  '59 tGravityAcc-energy()-Z',\n",
    "#  '125 tBodyGyro-std()-Y',\n",
    "#  '128 tBodyGyro-mad()-Y',\n",
    "#  '138 tBodyGyro-energy()-Y',\n",
    "#  '425 fBodyGyro-mean()-Y',\n",
    "#  '441 fBodyGyro-energy()-Y',\n",
    "#  '475 fBodyGyro-bandsEnergy()-1,8',\n",
    "#  '483 fBodyGyro-bandsEnergy()-1,16',\n",
    "#  '559 angle(X,gravityMean)',\n",
    "#  '561 angle(Z,gravityMean)']\n",
    "\n",
    "# act_features = ['4 tBodyAcc-std()-X',\n",
    "#  '10 tBodyAcc-max()-X',\n",
    "#  '202 tBodyAccMag-std()',\n",
    "#  '215 tGravityAccMag-std()',\n",
    "#  '269 fBodyAcc-std()-X',\n",
    "#  '282 fBodyAcc-energy()-X',\n",
    "#  '303 fBodyAcc-bandsEnergy()-1,8',\n",
    "#  '311 fBodyAcc-bandsEnergy()-1,16',\n",
    "#  '315 fBodyAcc-bandsEnergy()-1,24',\n",
    "#  '504 fBodyAccMag-std()',\n",
    "#  '505 fBodyAccMag-mad()',\n",
    "#  '509 fBodyAccMag-energy()']\n",
    "\n",
    "# input_shape = len(act_features) + len(sub_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>275 fBodyAcc-max()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>-0.976353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993756</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>-0.989038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999372</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>-0.994122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998158</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>-0.993142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997404</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>-0.992542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999277</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.065142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318185</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332146</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>0.170686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160368</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>0.178939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147421</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>-0.073681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417612</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                   0.056483             -0.375426                  -0.975510   \n",
       "1                   0.102764             -0.383430                  -0.978500   \n",
       "2                   0.102764             -0.401602                  -0.981672   \n",
       "3                   0.095753             -0.400278                  -0.982420   \n",
       "4                   0.094059             -0.400477                  -0.984363   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.995193   \n",
       "7348               -0.068919             -0.358934                  -0.995151   \n",
       "7349               -0.068919             -0.377025                  -0.995450   \n",
       "7350               -0.040009             -0.440050                  -0.998824   \n",
       "7351               -0.047491             -0.432003                  -0.998144   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  275 fBodyAcc-max()-X  \\\n",
       "0                 -0.976623              -0.976353  ...             -0.993756   \n",
       "1                 -0.989046              -0.989038  ...             -0.999372   \n",
       "2                 -0.993552              -0.994122  ...             -0.998158   \n",
       "3                 -0.992407              -0.993142  ...             -0.997404   \n",
       "4                 -0.992378              -0.992542  ...             -0.999277   \n",
       "...                     ...                    ...  ...                   ...   \n",
       "7347               0.084878               0.065142  ...             -0.318185   \n",
       "7348               0.098249               0.091791  ...             -0.332146   \n",
       "7349               0.185902               0.170686  ...             -0.160368   \n",
       "7350               0.190360               0.178939  ...             -0.147421   \n",
       "7351               0.022216              -0.073681  ...             -0.417612   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.956134              -0.948870              -0.974321   \n",
       "1                 -0.975866              -0.975777              -0.978226   \n",
       "2                 -0.989015              -0.985594              -0.993062   \n",
       "3                 -0.986742              -0.983524              -0.990230   \n",
       "4                 -0.990063              -0.992324              -0.990506   \n",
       "...                     ...                    ...                    ...   \n",
       "7347              -0.232600              -0.007392              -0.401674   \n",
       "7348              -0.275373              -0.172448              -0.410577   \n",
       "7349              -0.220288              -0.216074              -0.362904   \n",
       "7350              -0.234539              -0.220443              -0.397687   \n",
       "7351              -0.342670              -0.146649              -0.620014   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Activity  \n",
       "0                    -0.998285         5  \n",
       "1                    -0.999472         5  \n",
       "2                    -0.999807         5  \n",
       "3                    -0.999770         5  \n",
       "4                    -0.999873         5  \n",
       "...                        ...       ...  \n",
       "7347                 -0.584282         2  \n",
       "7348                 -0.632536         2  \n",
       "7349                 -0.641170         2  \n",
       "7350                 -0.663579         2  \n",
       "7351                 -0.698087         2  \n",
       "\n",
       "[7352 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "y_train = pd.read_csv('../data/y_train.txt', header = None)\n",
    "y_train.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[(X_train['Activity'] == 1) | (X_train['Activity'] == 3) | (X_train['Activity'] == 4)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Activity'] == 1) | (y_train['Activity'] == 3) | (y_train['Activity'] == 4)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subjects = pd.read_csv('UCI/subject_train.txt', header = None)\n",
    "# train_subjects.columns = ['Subject']\n",
    "# train_subjects = train_subjects.values\n",
    "# train_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>128 tBodyGyro-mad()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>275 fBodyAcc-max()-X</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.282719</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>-0.279244</td>\n",
       "      <td>0.152895</td>\n",
       "      <td>-0.262160</td>\n",
       "      <td>-0.076162</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.967795</td>\n",
       "      <td>-0.816164</td>\n",
       "      <td>-0.857801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.968424</td>\n",
       "      <td>-0.997844</td>\n",
       "      <td>-0.998506</td>\n",
       "      <td>-0.998204</td>\n",
       "      <td>-0.998020</td>\n",
       "      <td>-0.711074</td>\n",
       "      <td>-0.726707</td>\n",
       "      <td>-0.777697</td>\n",
       "      <td>-0.953984</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.289215</td>\n",
       "      <td>0.152568</td>\n",
       "      <td>-0.304870</td>\n",
       "      <td>0.152895</td>\n",
       "      <td>-0.262160</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>-0.957240</td>\n",
       "      <td>-0.929599</td>\n",
       "      <td>-0.950025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.993051</td>\n",
       "      <td>-0.999592</td>\n",
       "      <td>-0.999850</td>\n",
       "      <td>-0.999760</td>\n",
       "      <td>-0.999687</td>\n",
       "      <td>-0.959746</td>\n",
       "      <td>-0.960680</td>\n",
       "      <td>-0.968667</td>\n",
       "      <td>-0.998476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.287513</td>\n",
       "      <td>0.146086</td>\n",
       "      <td>-0.304870</td>\n",
       "      <td>0.139454</td>\n",
       "      <td>-0.261661</td>\n",
       "      <td>0.144969</td>\n",
       "      <td>0.040561</td>\n",
       "      <td>-0.960965</td>\n",
       "      <td>-0.978511</td>\n",
       "      <td>-0.980558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.995082</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-0.999962</td>\n",
       "      <td>-0.999958</td>\n",
       "      <td>-0.983784</td>\n",
       "      <td>-0.977176</td>\n",
       "      <td>-0.991908</td>\n",
       "      <td>-0.999570</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.293396</td>\n",
       "      <td>0.142926</td>\n",
       "      <td>-0.305101</td>\n",
       "      <td>0.136124</td>\n",
       "      <td>-0.272916</td>\n",
       "      <td>0.142107</td>\n",
       "      <td>0.046106</td>\n",
       "      <td>-0.962713</td>\n",
       "      <td>-0.975134</td>\n",
       "      <td>-0.973915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997495</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.982120</td>\n",
       "      <td>-0.976796</td>\n",
       "      <td>-0.988398</td>\n",
       "      <td>-0.999504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.302961</td>\n",
       "      <td>0.138307</td>\n",
       "      <td>-0.312552</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>-0.279190</td>\n",
       "      <td>0.130931</td>\n",
       "      <td>0.055351</td>\n",
       "      <td>-0.965137</td>\n",
       "      <td>-0.977951</td>\n",
       "      <td>-0.979507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.995932</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>-0.999960</td>\n",
       "      <td>-0.999957</td>\n",
       "      <td>-0.978838</td>\n",
       "      <td>-0.975706</td>\n",
       "      <td>-0.981305</td>\n",
       "      <td>-0.999500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>-0.276718</td>\n",
       "      <td>-0.231594</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>-0.226640</td>\n",
       "      <td>-0.272203</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>0.194878</td>\n",
       "      <td>-0.888599</td>\n",
       "      <td>-0.526855</td>\n",
       "      <td>-0.575937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381610</td>\n",
       "      <td>-0.744467</td>\n",
       "      <td>-0.734651</td>\n",
       "      <td>-0.724505</td>\n",
       "      <td>-0.739414</td>\n",
       "      <td>-0.332141</td>\n",
       "      <td>-0.202661</td>\n",
       "      <td>-0.550923</td>\n",
       "      <td>-0.702110</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>-0.274976</td>\n",
       "      <td>-0.228050</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>-0.220590</td>\n",
       "      <td>-0.268172</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>0.181283</td>\n",
       "      <td>-0.891822</td>\n",
       "      <td>-0.518149</td>\n",
       "      <td>-0.571732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.540880</td>\n",
       "      <td>-0.756815</td>\n",
       "      <td>-0.747251</td>\n",
       "      <td>-0.746326</td>\n",
       "      <td>-0.753166</td>\n",
       "      <td>-0.316954</td>\n",
       "      <td>-0.196060</td>\n",
       "      <td>-0.569485</td>\n",
       "      <td>-0.674032</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>-0.276165</td>\n",
       "      <td>-0.226256</td>\n",
       "      <td>-0.273818</td>\n",
       "      <td>-0.220590</td>\n",
       "      <td>-0.268172</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>0.178977</td>\n",
       "      <td>-0.893504</td>\n",
       "      <td>-0.557059</td>\n",
       "      <td>-0.576087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340887</td>\n",
       "      <td>-0.773399</td>\n",
       "      <td>-0.762837</td>\n",
       "      <td>-0.764571</td>\n",
       "      <td>-0.769811</td>\n",
       "      <td>-0.377240</td>\n",
       "      <td>-0.208208</td>\n",
       "      <td>-0.567270</td>\n",
       "      <td>-0.715711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>-0.262356</td>\n",
       "      <td>-0.235108</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>-0.231517</td>\n",
       "      <td>-0.244744</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>-0.885275</td>\n",
       "      <td>-0.555166</td>\n",
       "      <td>-0.580453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.310583</td>\n",
       "      <td>-0.768993</td>\n",
       "      <td>-0.751525</td>\n",
       "      <td>-0.755248</td>\n",
       "      <td>-0.765049</td>\n",
       "      <td>-0.390201</td>\n",
       "      <td>-0.259605</td>\n",
       "      <td>-0.654450</td>\n",
       "      <td>-0.745225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>-0.261407</td>\n",
       "      <td>-0.236112</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>-0.233821</td>\n",
       "      <td>-0.242839</td>\n",
       "      <td>-0.245060</td>\n",
       "      <td>0.171842</td>\n",
       "      <td>-0.884370</td>\n",
       "      <td>-0.508557</td>\n",
       "      <td>-0.544531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413622</td>\n",
       "      <td>-0.773668</td>\n",
       "      <td>-0.755378</td>\n",
       "      <td>-0.754754</td>\n",
       "      <td>-0.768236</td>\n",
       "      <td>-0.362598</td>\n",
       "      <td>-0.231349</td>\n",
       "      <td>-0.605015</td>\n",
       "      <td>-0.723287</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.282719                 0.115288   \n",
       "1                   -0.289215                 0.152568   \n",
       "2                   -0.287513                 0.146086   \n",
       "3                   -0.293396                 0.142926   \n",
       "4                   -0.302961                 0.138307   \n",
       "...                       ...                      ...   \n",
       "2942                -0.276718                -0.231594   \n",
       "2943                -0.274976                -0.228050   \n",
       "2944                -0.276165                -0.226256   \n",
       "2945                -0.262356                -0.235108   \n",
       "2946                -0.261407                -0.236112   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.279244                0.152895               -0.262160   \n",
       "1                  -0.304870                0.152895               -0.262160   \n",
       "2                  -0.304870                0.139454               -0.261661   \n",
       "3                  -0.305101                0.136124               -0.272916   \n",
       "4                  -0.312552                0.133541               -0.279190   \n",
       "...                      ...                     ...                     ...   \n",
       "2942               -0.278442               -0.226640               -0.272203   \n",
       "2943               -0.278442               -0.220590               -0.268172   \n",
       "2944               -0.273818               -0.220590               -0.268172   \n",
       "2945               -0.272785               -0.231517               -0.244744   \n",
       "2946               -0.272785               -0.233821               -0.242839   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  59 tGravityAcc-energy()-Z  \\\n",
       "0                  -0.076162             -0.017827                  -0.967795   \n",
       "1                   0.149013              0.057676                  -0.957240   \n",
       "2                   0.144969              0.040561                  -0.960965   \n",
       "3                   0.142107              0.046106                  -0.962713   \n",
       "4                   0.130931              0.055351                  -0.965137   \n",
       "...                      ...                   ...                        ...   \n",
       "2942               -0.243507              0.194878                  -0.888599   \n",
       "2943               -0.243507              0.181283                  -0.891822   \n",
       "2944               -0.245178              0.178977                  -0.893504   \n",
       "2945               -0.245178              0.168668                  -0.885275   \n",
       "2946               -0.245060              0.171842                  -0.884370   \n",
       "\n",
       "      125 tBodyGyro-std()-Y  128 tBodyGyro-mad()-Y  ...  275 fBodyAcc-max()-X  \\\n",
       "0                 -0.816164              -0.857801  ...             -0.968424   \n",
       "1                 -0.929599              -0.950025  ...             -0.993051   \n",
       "2                 -0.978511              -0.980558  ...             -0.995082   \n",
       "3                 -0.975134              -0.973915  ...             -0.997495   \n",
       "4                 -0.977951              -0.979507  ...             -0.995932   \n",
       "...                     ...                    ...  ...                   ...   \n",
       "2942              -0.526855              -0.575937  ...             -0.381610   \n",
       "2943              -0.518149              -0.571732  ...             -0.540880   \n",
       "2944              -0.557059              -0.576087  ...             -0.340887   \n",
       "2945              -0.555166              -0.580453  ...             -0.310583   \n",
       "2946              -0.508557              -0.544531  ...             -0.413622   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.997844                       -0.998506   \n",
       "1                   -0.999592                       -0.999850   \n",
       "2                   -0.999954                       -0.999976   \n",
       "3                   -0.999963                       -0.999983   \n",
       "4                   -0.999954                       -0.999961   \n",
       "...                       ...                             ...   \n",
       "2942                -0.744467                       -0.734651   \n",
       "2943                -0.756815                       -0.747251   \n",
       "2944                -0.773399                       -0.762837   \n",
       "2945                -0.768993                       -0.751525   \n",
       "2946                -0.773668                       -0.755378   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.998204                        -0.998020   \n",
       "1                           -0.999760                        -0.999687   \n",
       "2                           -0.999962                        -0.999958   \n",
       "3                           -0.999971                        -0.999966   \n",
       "4                           -0.999960                        -0.999957   \n",
       "...                               ...                              ...   \n",
       "2942                        -0.724505                        -0.739414   \n",
       "2943                        -0.746326                        -0.753166   \n",
       "2944                        -0.764571                        -0.769811   \n",
       "2945                        -0.755248                        -0.765049   \n",
       "2946                        -0.754754                        -0.768236   \n",
       "\n",
       "      504 fBodyAccMag-std()  505 fBodyAccMag-mad()  506 fBodyAccMag-max()  \\\n",
       "0                 -0.711074              -0.726707              -0.777697   \n",
       "1                 -0.959746              -0.960680              -0.968667   \n",
       "2                 -0.983784              -0.977176              -0.991908   \n",
       "3                 -0.982120              -0.976796              -0.988398   \n",
       "4                 -0.978838              -0.975706              -0.981305   \n",
       "...                     ...                    ...                    ...   \n",
       "2942              -0.332141              -0.202661              -0.550923   \n",
       "2943              -0.316954              -0.196060              -0.569485   \n",
       "2944              -0.377240              -0.208208              -0.567270   \n",
       "2945              -0.390201              -0.259605              -0.654450   \n",
       "2946              -0.362598              -0.231349              -0.605015   \n",
       "\n",
       "      509 fBodyAccMag-energy()  Activity  \n",
       "0                    -0.953984         5  \n",
       "1                    -0.998476         5  \n",
       "2                    -0.999570         5  \n",
       "3                    -0.999504         5  \n",
       "4                    -0.999500         5  \n",
       "...                        ...       ...  \n",
       "2942                 -0.702110         2  \n",
       "2943                 -0.674032         2  \n",
       "2944                 -0.715711         2  \n",
       "2945                 -0.745225         2  \n",
       "2946                 -0.723287         2  \n",
       "\n",
       "[2947 rows x 47 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "test_column_names = test_names.values.tolist()\n",
    "test_column_names = [k for row in test_column_names for k in row]\n",
    "\n",
    "test_data = pd.read_csv('../data/X_test.txt', delim_whitespace = True, header = None)\n",
    "test_data.columns = test_column_names\n",
    "\n",
    "y_test = pd.read_csv('../data/y_test.txt', header = None)\n",
    "y_test.columns = ['Activity']\n",
    "\n",
    "X_test_1 = test_data[sub_features]\n",
    "X_test_2 = test_data[act_features]\n",
    "\n",
    "# X_test_1 = test_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_test_2 = test_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "X_test = pd.concat([X_test_1, X_test_2], axis = 1)\n",
    "\n",
    "X_test = pd.concat([X_test, y_test], axis = 1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[(X_test['Activity'] == 1) | (X_test['Activity'] == 3) | (X_test['Activity'] == 4)]\n",
    "X_test = X_test.iloc[:,:-1].values\n",
    "\n",
    "y_test = y_test[(y_test['Activity'] == 1) | (y_test['Activity'] == 3) | (y_test['Activity'] == 4)]\n",
    "y_test = y_test.values\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2\n",
    "        \n",
    "for k in range(len(y_test)):\n",
    "    if y_test[k] == 1:\n",
    "        y_test[k] = 0\n",
    "    elif y_test[k] == 3:\n",
    "        y_test[k] = 1\n",
    "    else:\n",
    "        y_test[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.474869132041931, Final Batch Loss: 1.0978004932403564\n",
      "Epoch 2, Loss: 14.986499786376953, Final Batch Loss: 1.0335304737091064\n",
      "Epoch 3, Loss: 13.823860883712769, Final Batch Loss: 0.9623686075210571\n",
      "Epoch 4, Loss: 12.485234797000885, Final Batch Loss: 0.7861201167106628\n",
      "Epoch 5, Loss: 11.015812993049622, Final Batch Loss: 0.7972857356071472\n",
      "Epoch 6, Loss: 9.53484183549881, Final Batch Loss: 0.6344114542007446\n",
      "Epoch 7, Loss: 8.326738774776459, Final Batch Loss: 0.5426145792007446\n",
      "Epoch 8, Loss: 7.014031231403351, Final Batch Loss: 0.43976712226867676\n",
      "Epoch 9, Loss: 5.40009468793869, Final Batch Loss: 0.30076080560684204\n",
      "Epoch 10, Loss: 4.233738109469414, Final Batch Loss: 0.3300112187862396\n",
      "Epoch 11, Loss: 3.5599109530448914, Final Batch Loss: 0.30280601978302\n",
      "Epoch 12, Loss: 3.323524847626686, Final Batch Loss: 0.1801648586988449\n",
      "Epoch 13, Loss: 3.1567269265651703, Final Batch Loss: 0.26541078090667725\n",
      "Epoch 14, Loss: 2.9227994829416275, Final Batch Loss: 0.26109954714775085\n",
      "Epoch 15, Loss: 2.78709477186203, Final Batch Loss: 0.17229613661766052\n",
      "Epoch 16, Loss: 2.930135115981102, Final Batch Loss: 0.23542051017284393\n",
      "Epoch 17, Loss: 2.63286454975605, Final Batch Loss: 0.16829417645931244\n",
      "Epoch 18, Loss: 2.546446144580841, Final Batch Loss: 0.2199035882949829\n",
      "Epoch 19, Loss: 2.4508580714464188, Final Batch Loss: 0.14440765976905823\n",
      "Epoch 20, Loss: 2.46609303355217, Final Batch Loss: 0.13669206202030182\n",
      "Epoch 21, Loss: 2.4189722687005997, Final Batch Loss: 0.1720581352710724\n",
      "Epoch 22, Loss: 2.1563126742839813, Final Batch Loss: 0.1681644469499588\n",
      "Epoch 23, Loss: 2.258757211267948, Final Batch Loss: 0.16467425227165222\n",
      "Epoch 24, Loss: 2.155977286398411, Final Batch Loss: 0.11185067892074585\n",
      "Epoch 25, Loss: 2.0770039409399033, Final Batch Loss: 0.16248257458209991\n",
      "Epoch 26, Loss: 2.0947302281856537, Final Batch Loss: 0.1615646630525589\n",
      "Epoch 27, Loss: 2.0336435437202454, Final Batch Loss: 0.09395153820514679\n",
      "Epoch 28, Loss: 1.9849703460931778, Final Batch Loss: 0.1448887288570404\n",
      "Epoch 29, Loss: 1.8691601902246475, Final Batch Loss: 0.19617760181427002\n",
      "Epoch 30, Loss: 1.8404674902558327, Final Batch Loss: 0.13331946730613708\n",
      "Epoch 31, Loss: 1.808461032807827, Final Batch Loss: 0.13956549763679504\n",
      "Epoch 32, Loss: 1.6875687018036842, Final Batch Loss: 0.15125007927417755\n",
      "Epoch 33, Loss: 1.8025180399417877, Final Batch Loss: 0.11133227497339249\n",
      "Epoch 34, Loss: 1.7517684027552605, Final Batch Loss: 0.11701218038797379\n",
      "Epoch 35, Loss: 1.6361399963498116, Final Batch Loss: 0.15830330550670624\n",
      "Epoch 36, Loss: 1.5928685143589973, Final Batch Loss: 0.1249113380908966\n",
      "Epoch 37, Loss: 1.546783909201622, Final Batch Loss: 0.11161085218191147\n",
      "Epoch 38, Loss: 1.5469433665275574, Final Batch Loss: 0.07082771509885788\n",
      "Epoch 39, Loss: 1.4616194888949394, Final Batch Loss: 0.1130642518401146\n",
      "Epoch 40, Loss: 1.3938450515270233, Final Batch Loss: 0.10600176453590393\n",
      "Epoch 41, Loss: 1.4392342530190945, Final Batch Loss: 0.10528318583965302\n",
      "Epoch 42, Loss: 1.333686277270317, Final Batch Loss: 0.10218674689531326\n",
      "Epoch 43, Loss: 1.4437761902809143, Final Batch Loss: 0.05760791152715683\n",
      "Epoch 44, Loss: 1.3470751903951168, Final Batch Loss: 0.06623949855566025\n",
      "Epoch 45, Loss: 1.2830494977533817, Final Batch Loss: 0.1034654900431633\n",
      "Epoch 46, Loss: 1.333434984087944, Final Batch Loss: 0.07812868058681488\n",
      "Epoch 47, Loss: 1.2624939121305943, Final Batch Loss: 0.05294867232441902\n",
      "Epoch 48, Loss: 1.1551519185304642, Final Batch Loss: 0.06908217072486877\n",
      "Epoch 49, Loss: 1.1936096381396055, Final Batch Loss: 0.11176177114248276\n",
      "Epoch 50, Loss: 1.23456771671772, Final Batch Loss: 0.07151336967945099\n",
      "Epoch 51, Loss: 1.1243652887642384, Final Batch Loss: 0.08596906065940857\n",
      "Epoch 52, Loss: 1.0987987667322159, Final Batch Loss: 0.08553899824619293\n",
      "Epoch 53, Loss: 1.0869825631380081, Final Batch Loss: 0.0704236775636673\n",
      "Epoch 54, Loss: 1.1663266569375992, Final Batch Loss: 0.0652252584695816\n",
      "Epoch 55, Loss: 1.0853372775018215, Final Batch Loss: 0.10788404196500778\n",
      "Epoch 56, Loss: 1.0854111276566982, Final Batch Loss: 0.07636482268571854\n",
      "Epoch 57, Loss: 1.109003759920597, Final Batch Loss: 0.07468368113040924\n",
      "Epoch 58, Loss: 1.1164974458515644, Final Batch Loss: 0.10976321250200272\n",
      "Epoch 59, Loss: 1.0341392271220684, Final Batch Loss: 0.1638294756412506\n",
      "Epoch 60, Loss: 1.0051597841084003, Final Batch Loss: 0.10631579160690308\n",
      "Epoch 61, Loss: 0.9878775663673878, Final Batch Loss: 0.10018301010131836\n",
      "Epoch 62, Loss: 0.904639471322298, Final Batch Loss: 0.04557133838534355\n",
      "Epoch 63, Loss: 0.8941758275032043, Final Batch Loss: 0.06494980305433273\n",
      "Epoch 64, Loss: 0.9205814227461815, Final Batch Loss: 0.07986919581890106\n",
      "Epoch 65, Loss: 0.9650449939072132, Final Batch Loss: 0.11130651086568832\n",
      "Epoch 66, Loss: 0.8487216215580702, Final Batch Loss: 0.029520820826292038\n",
      "Epoch 67, Loss: 0.9829759504646063, Final Batch Loss: 0.0968838706612587\n",
      "Epoch 68, Loss: 0.9449480958282948, Final Batch Loss: 0.05945735052227974\n",
      "Epoch 69, Loss: 0.9401440303772688, Final Batch Loss: 0.02375686727464199\n",
      "Epoch 70, Loss: 0.9679210633039474, Final Batch Loss: 0.12329429388046265\n",
      "Epoch 71, Loss: 0.9525853767991066, Final Batch Loss: 0.08078579604625702\n",
      "Epoch 72, Loss: 0.8664789833128452, Final Batch Loss: 0.08000218123197556\n",
      "Epoch 73, Loss: 0.898478090763092, Final Batch Loss: 0.05067354813218117\n",
      "Epoch 74, Loss: 0.8497209418565035, Final Batch Loss: 0.0535590797662735\n",
      "Epoch 75, Loss: 0.850438330322504, Final Batch Loss: 0.05455132946372032\n",
      "Epoch 76, Loss: 0.8345291092991829, Final Batch Loss: 0.050158895552158356\n",
      "Epoch 77, Loss: 0.9139539413154125, Final Batch Loss: 0.04731457307934761\n",
      "Epoch 78, Loss: 0.8420493602752686, Final Batch Loss: 0.04437515139579773\n",
      "Epoch 79, Loss: 0.850284855812788, Final Batch Loss: 0.04871264472603798\n",
      "Epoch 80, Loss: 0.8554630130529404, Final Batch Loss: 0.05199892818927765\n",
      "Epoch 81, Loss: 0.8199157230556011, Final Batch Loss: 0.04535842686891556\n",
      "Epoch 82, Loss: 0.8414839711040258, Final Batch Loss: 0.043687134981155396\n",
      "Epoch 83, Loss: 0.8092093486338854, Final Batch Loss: 0.06352747231721878\n",
      "Epoch 84, Loss: 0.8475504089146852, Final Batch Loss: 0.10151699185371399\n",
      "Epoch 85, Loss: 0.8384813666343689, Final Batch Loss: 0.05693269520998001\n",
      "Epoch 86, Loss: 0.791214857250452, Final Batch Loss: 0.08483106642961502\n",
      "Epoch 87, Loss: 0.7707887962460518, Final Batch Loss: 0.03484652191400528\n",
      "Epoch 88, Loss: 0.7899846527725458, Final Batch Loss: 0.0663522332906723\n",
      "Epoch 89, Loss: 0.8173736110329628, Final Batch Loss: 0.08322938531637192\n",
      "Epoch 90, Loss: 0.8201884292066097, Final Batch Loss: 0.0519503578543663\n",
      "Epoch 91, Loss: 0.8276889137923717, Final Batch Loss: 0.0800039991736412\n",
      "Epoch 92, Loss: 0.8034660629928112, Final Batch Loss: 0.0380547009408474\n",
      "Epoch 93, Loss: 0.6672643311321735, Final Batch Loss: 0.05660468712449074\n",
      "Epoch 94, Loss: 0.6853733863681555, Final Batch Loss: 0.05554164946079254\n",
      "Epoch 95, Loss: 0.7382162995636463, Final Batch Loss: 0.048261888325214386\n",
      "Epoch 96, Loss: 0.6961452849209309, Final Batch Loss: 0.06331413239240646\n",
      "Epoch 97, Loss: 0.7556485701352358, Final Batch Loss: 0.07766079157590866\n",
      "Epoch 98, Loss: 0.7390092350542545, Final Batch Loss: 0.01852000318467617\n",
      "Epoch 99, Loss: 0.7745947428047657, Final Batch Loss: 0.02940288931131363\n",
      "Epoch 100, Loss: 0.7591668218374252, Final Batch Loss: 0.05159668251872063\n",
      "Epoch 101, Loss: 0.7889076415449381, Final Batch Loss: 0.07141192257404327\n",
      "Epoch 102, Loss: 0.792306512594223, Final Batch Loss: 0.06786768138408661\n",
      "Epoch 103, Loss: 0.7815953735262156, Final Batch Loss: 0.06917943060398102\n",
      "Epoch 104, Loss: 0.7081989496946335, Final Batch Loss: 0.025744296610355377\n",
      "Epoch 105, Loss: 0.7538645751774311, Final Batch Loss: 0.04772884398698807\n",
      "Epoch 106, Loss: 0.7089532259851694, Final Batch Loss: 0.04892369359731674\n",
      "Epoch 107, Loss: 0.6815974079072475, Final Batch Loss: 0.06677236407995224\n",
      "Epoch 108, Loss: 0.6743598338216543, Final Batch Loss: 0.04661690816283226\n",
      "Epoch 109, Loss: 0.7689179740846157, Final Batch Loss: 0.07042921334505081\n",
      "Epoch 110, Loss: 0.699856735765934, Final Batch Loss: 0.028466299176216125\n",
      "Epoch 111, Loss: 0.6711106169968843, Final Batch Loss: 0.039115309715270996\n",
      "Epoch 112, Loss: 0.649714881554246, Final Batch Loss: 0.03210361301898956\n",
      "Epoch 113, Loss: 0.7184205856174231, Final Batch Loss: 0.07246562838554382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, Loss: 0.7041281759738922, Final Batch Loss: 0.045352790504693985\n",
      "Epoch 115, Loss: 0.7027668710798025, Final Batch Loss: 0.052419573068618774\n",
      "Epoch 116, Loss: 0.6357897482812405, Final Batch Loss: 0.07156728208065033\n",
      "Epoch 117, Loss: 0.6528017278760672, Final Batch Loss: 0.04371368885040283\n",
      "Epoch 118, Loss: 0.6197579707950354, Final Batch Loss: 0.039263445883989334\n",
      "Epoch 119, Loss: 0.6556687373667955, Final Batch Loss: 0.08120273798704147\n",
      "Epoch 120, Loss: 0.649245873093605, Final Batch Loss: 0.03289755433797836\n",
      "Epoch 121, Loss: 0.6755289100110531, Final Batch Loss: 0.07878477871417999\n",
      "Epoch 122, Loss: 0.718861497938633, Final Batch Loss: 0.02716485969722271\n",
      "Epoch 123, Loss: 0.6712809298187494, Final Batch Loss: 0.0478314571082592\n",
      "Epoch 124, Loss: 0.6922695729881525, Final Batch Loss: 0.08214841783046722\n",
      "Epoch 125, Loss: 0.6065613906830549, Final Batch Loss: 0.04910200089216232\n",
      "Epoch 126, Loss: 0.6150676012039185, Final Batch Loss: 0.035274554044008255\n",
      "Epoch 127, Loss: 0.6272314246743917, Final Batch Loss: 0.07275757938623428\n",
      "Epoch 128, Loss: 0.7052969485521317, Final Batch Loss: 0.05259613320231438\n",
      "Epoch 129, Loss: 0.6594341825693846, Final Batch Loss: 0.0542999766767025\n",
      "Epoch 130, Loss: 0.6510744411498308, Final Batch Loss: 0.043253377079963684\n",
      "Epoch 131, Loss: 0.6639388520270586, Final Batch Loss: 0.08067136257886887\n",
      "Epoch 132, Loss: 0.6001511421054602, Final Batch Loss: 0.03880123421549797\n",
      "Epoch 133, Loss: 0.6085966471582651, Final Batch Loss: 0.032098013907670975\n",
      "Epoch 134, Loss: 0.5870567932724953, Final Batch Loss: 0.04838678240776062\n",
      "Epoch 135, Loss: 0.5872906427830458, Final Batch Loss: 0.027114495635032654\n",
      "Epoch 136, Loss: 0.629878357052803, Final Batch Loss: 0.0788600742816925\n",
      "Epoch 137, Loss: 0.5887916218489408, Final Batch Loss: 0.0864977017045021\n",
      "Epoch 138, Loss: 0.6196918245404959, Final Batch Loss: 0.027147211134433746\n",
      "Epoch 139, Loss: 0.574210612103343, Final Batch Loss: 0.030167583376169205\n",
      "Epoch 140, Loss: 0.5760134924203157, Final Batch Loss: 0.015471097081899643\n",
      "Epoch 141, Loss: 0.5402623005211353, Final Batch Loss: 0.022963829338550568\n",
      "Epoch 142, Loss: 0.54629095364362, Final Batch Loss: 0.04595458507537842\n",
      "Epoch 143, Loss: 0.5943720769137144, Final Batch Loss: 0.05399112030863762\n",
      "Epoch 144, Loss: 0.5823367163538933, Final Batch Loss: 0.05742855370044708\n",
      "Epoch 145, Loss: 0.5604006573557854, Final Batch Loss: 0.05105998367071152\n",
      "Epoch 146, Loss: 0.5880965944379568, Final Batch Loss: 0.024212708696722984\n",
      "Epoch 147, Loss: 0.5808492712676525, Final Batch Loss: 0.03603730350732803\n",
      "Epoch 148, Loss: 0.586090438067913, Final Batch Loss: 0.0411306768655777\n",
      "Epoch 149, Loss: 0.5267048496752977, Final Batch Loss: 0.018935129046440125\n",
      "Epoch 150, Loss: 0.5581118911504745, Final Batch Loss: 0.0757131353020668\n",
      "Epoch 151, Loss: 0.5570603585802019, Final Batch Loss: 0.06245427578687668\n",
      "Epoch 152, Loss: 0.5201056078076363, Final Batch Loss: 0.04028793424367905\n",
      "Epoch 153, Loss: 0.5793374385684729, Final Batch Loss: 0.03877345845103264\n",
      "Epoch 154, Loss: 0.5708326157182455, Final Batch Loss: 0.019836293533444405\n",
      "Epoch 155, Loss: 0.5199949704110622, Final Batch Loss: 0.03544139489531517\n",
      "Epoch 156, Loss: 0.5410802643746138, Final Batch Loss: 0.05210931971669197\n",
      "Epoch 157, Loss: 0.5386383533477783, Final Batch Loss: 0.03943076357245445\n",
      "Epoch 158, Loss: 0.5183754554018378, Final Batch Loss: 0.027374692261219025\n",
      "Epoch 159, Loss: 0.5291666686534882, Final Batch Loss: 0.07590996474027634\n",
      "Epoch 160, Loss: 0.5519378567114472, Final Batch Loss: 0.025140922516584396\n",
      "Epoch 161, Loss: 0.5900034885853529, Final Batch Loss: 0.03845623508095741\n",
      "Epoch 162, Loss: 0.4898492880165577, Final Batch Loss: 0.06487054377794266\n",
      "Epoch 163, Loss: 0.5275712497532368, Final Batch Loss: 0.030361786484718323\n",
      "Epoch 164, Loss: 0.49716232623904943, Final Batch Loss: 0.055742714554071426\n",
      "Epoch 165, Loss: 0.46057223435491323, Final Batch Loss: 0.023605888709425926\n",
      "Epoch 166, Loss: 0.45439083874225616, Final Batch Loss: 0.012180354446172714\n",
      "Epoch 167, Loss: 0.591731081251055, Final Batch Loss: 0.08281387388706207\n",
      "Epoch 168, Loss: 0.5148553205654025, Final Batch Loss: 0.05970609933137894\n",
      "Epoch 169, Loss: 0.4878603769466281, Final Batch Loss: 0.04031515493988991\n",
      "Epoch 170, Loss: 0.5058804731816053, Final Batch Loss: 0.04250691086053848\n",
      "Epoch 171, Loss: 0.5262086493894458, Final Batch Loss: 0.05066492781043053\n",
      "Epoch 172, Loss: 0.5409402158111334, Final Batch Loss: 0.06979953497648239\n",
      "Epoch 173, Loss: 0.5464643212035298, Final Batch Loss: 0.07228560000658035\n",
      "Epoch 174, Loss: 0.4967063902877271, Final Batch Loss: 0.020021088421344757\n",
      "Epoch 175, Loss: 0.5181866753846407, Final Batch Loss: 0.01966693252325058\n",
      "Epoch 176, Loss: 0.4761203220114112, Final Batch Loss: 0.0396265983581543\n",
      "Epoch 177, Loss: 0.5067704152315855, Final Batch Loss: 0.03704390674829483\n",
      "Epoch 178, Loss: 0.5136009166017175, Final Batch Loss: 0.05292164161801338\n",
      "Epoch 179, Loss: 0.47420996986329556, Final Batch Loss: 0.04128774628043175\n",
      "Epoch 180, Loss: 0.4616677314043045, Final Batch Loss: 0.02525436505675316\n",
      "Epoch 181, Loss: 0.49487319961190224, Final Batch Loss: 0.022633902728557587\n",
      "Epoch 182, Loss: 0.45249967370182276, Final Batch Loss: 0.052308034151792526\n",
      "Epoch 183, Loss: 0.6173818670213223, Final Batch Loss: 0.066082663834095\n",
      "Epoch 184, Loss: 0.48293173871934414, Final Batch Loss: 0.048119623214006424\n",
      "Epoch 185, Loss: 0.48674857150763273, Final Batch Loss: 0.01969841495156288\n",
      "Epoch 186, Loss: 0.4540367294102907, Final Batch Loss: 0.011150339618325233\n",
      "Epoch 187, Loss: 0.503654579166323, Final Batch Loss: 0.01873600110411644\n",
      "Epoch 188, Loss: 0.5442562606185675, Final Batch Loss: 0.014508160762488842\n",
      "Epoch 189, Loss: 0.4367384444922209, Final Batch Loss: 0.026038454845547676\n",
      "Epoch 190, Loss: 0.49978806637227535, Final Batch Loss: 0.02984575554728508\n",
      "Epoch 191, Loss: 0.48106945119798183, Final Batch Loss: 0.04940903186798096\n",
      "Epoch 192, Loss: 0.33841834776103497, Final Batch Loss: 0.00930434837937355\n",
      "Epoch 193, Loss: 0.507986394688487, Final Batch Loss: 0.021924138069152832\n",
      "Epoch 194, Loss: 0.445019967854023, Final Batch Loss: 0.03690437972545624\n",
      "Epoch 195, Loss: 0.4980552550405264, Final Batch Loss: 0.05545505881309509\n",
      "Epoch 196, Loss: 0.47383673675358295, Final Batch Loss: 0.023431958630681038\n",
      "Epoch 197, Loss: 0.46994698233902454, Final Batch Loss: 0.02028970792889595\n",
      "Epoch 198, Loss: 0.4551909836009145, Final Batch Loss: 0.01743239350616932\n",
      "Epoch 199, Loss: 0.4325756411999464, Final Batch Loss: 0.030838046222925186\n",
      "Epoch 200, Loss: 0.4317320715636015, Final Batch Loss: 0.043213311582803726\n",
      "Epoch 201, Loss: 0.38997880555689335, Final Batch Loss: 0.015573927201330662\n",
      "Epoch 202, Loss: 0.4463020768016577, Final Batch Loss: 0.03562784194946289\n",
      "Epoch 203, Loss: 0.4457439947873354, Final Batch Loss: 0.01052830833941698\n",
      "Epoch 204, Loss: 0.4796847552061081, Final Batch Loss: 0.027757832780480385\n",
      "Epoch 205, Loss: 0.4470200762152672, Final Batch Loss: 0.0559452623128891\n",
      "Epoch 206, Loss: 0.4614500901661813, Final Batch Loss: 0.013885854743421078\n",
      "Epoch 207, Loss: 0.41317435912787914, Final Batch Loss: 0.02455216459929943\n",
      "Epoch 208, Loss: 0.3954629022628069, Final Batch Loss: 0.029475310817360878\n",
      "Epoch 209, Loss: 0.36687563452869654, Final Batch Loss: 0.013137857429683208\n",
      "Epoch 210, Loss: 0.4541999101638794, Final Batch Loss: 0.0389605313539505\n",
      "Epoch 211, Loss: 0.4567346042022109, Final Batch Loss: 0.03682410344481468\n",
      "Epoch 212, Loss: 0.43358658347278833, Final Batch Loss: 0.02812548726797104\n",
      "Epoch 213, Loss: 0.4177441606298089, Final Batch Loss: 0.024608030915260315\n",
      "Epoch 214, Loss: 0.4020995330065489, Final Batch Loss: 0.00904876459389925\n",
      "Epoch 215, Loss: 0.4773546466603875, Final Batch Loss: 0.04032948240637779\n",
      "Epoch 216, Loss: 0.4044007994234562, Final Batch Loss: 0.024122614413499832\n",
      "Epoch 217, Loss: 0.384297339245677, Final Batch Loss: 0.055661872029304504\n",
      "Epoch 218, Loss: 0.40565422270447016, Final Batch Loss: 0.04255539923906326\n",
      "Epoch 219, Loss: 0.41253646183758974, Final Batch Loss: 0.04278305545449257\n",
      "Epoch 220, Loss: 0.42417011223733425, Final Batch Loss: 0.05161773040890694\n",
      "Epoch 221, Loss: 0.45531717082485557, Final Batch Loss: 0.07600471377372742\n",
      "Epoch 222, Loss: 0.44408181589096785, Final Batch Loss: 0.030948303639888763\n",
      "Epoch 223, Loss: 0.448868241161108, Final Batch Loss: 0.04352576658129692\n",
      "Epoch 224, Loss: 0.3968125665560365, Final Batch Loss: 0.02538933977484703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, Loss: 0.4793469337746501, Final Batch Loss: 0.03652609512209892\n",
      "Epoch 226, Loss: 0.3934916816651821, Final Batch Loss: 0.022187400609254837\n",
      "Epoch 227, Loss: 0.4688768247142434, Final Batch Loss: 0.013921079225838184\n",
      "Epoch 228, Loss: 0.40311766136437654, Final Batch Loss: 0.05126822739839554\n",
      "Epoch 229, Loss: 0.38868068251758814, Final Batch Loss: 0.034565240144729614\n",
      "Epoch 230, Loss: 0.39365222584456205, Final Batch Loss: 0.017920909449458122\n",
      "Epoch 231, Loss: 0.3647366529330611, Final Batch Loss: 0.03526473045349121\n",
      "Epoch 232, Loss: 0.4072349313646555, Final Batch Loss: 0.010858066380023956\n",
      "Epoch 233, Loss: 0.3938763430342078, Final Batch Loss: 0.035507477819919586\n",
      "Epoch 234, Loss: 0.4081809939816594, Final Batch Loss: 0.008706812746822834\n",
      "Epoch 235, Loss: 0.35342438519001007, Final Batch Loss: 0.016771037131547928\n",
      "Epoch 236, Loss: 0.34922651667147875, Final Batch Loss: 0.024738885462284088\n",
      "Epoch 237, Loss: 0.4360911836847663, Final Batch Loss: 0.03143675625324249\n",
      "Epoch 238, Loss: 0.3967343526892364, Final Batch Loss: 0.05481363460421562\n",
      "Epoch 239, Loss: 0.3632901096716523, Final Batch Loss: 0.05096164345741272\n",
      "Epoch 240, Loss: 0.4148664358071983, Final Batch Loss: 0.033672261983156204\n",
      "Epoch 241, Loss: 0.3880981020629406, Final Batch Loss: 0.039677660912275314\n",
      "Epoch 242, Loss: 0.36602906323969364, Final Batch Loss: 0.027166198939085007\n",
      "Epoch 243, Loss: 0.3835207400843501, Final Batch Loss: 0.045605771243572235\n",
      "Epoch 244, Loss: 0.36886493442580104, Final Batch Loss: 0.04958232864737511\n",
      "Epoch 245, Loss: 0.3183794878423214, Final Batch Loss: 0.01849733293056488\n",
      "Epoch 246, Loss: 0.3745941687375307, Final Batch Loss: 0.012866316363215446\n",
      "Epoch 247, Loss: 0.38815496768802404, Final Batch Loss: 0.048756543546915054\n",
      "Epoch 248, Loss: 0.3690176857635379, Final Batch Loss: 0.02931654453277588\n",
      "Epoch 249, Loss: 0.40910813165828586, Final Batch Loss: 0.014527360908687115\n",
      "Epoch 250, Loss: 0.37887630611658096, Final Batch Loss: 0.03031524457037449\n",
      "Epoch 251, Loss: 0.365824768319726, Final Batch Loss: 0.011417875997722149\n",
      "Epoch 252, Loss: 0.35437951795756817, Final Batch Loss: 0.024313993752002716\n",
      "Epoch 253, Loss: 0.3410103842616081, Final Batch Loss: 0.0289255790412426\n",
      "Epoch 254, Loss: 0.3087692237459123, Final Batch Loss: 0.01918959990143776\n",
      "Epoch 255, Loss: 0.36180325457826257, Final Batch Loss: 0.0479477234184742\n",
      "Epoch 256, Loss: 0.3654714375734329, Final Batch Loss: 0.021514620631933212\n",
      "Epoch 257, Loss: 0.3556317933835089, Final Batch Loss: 0.0641859844326973\n",
      "Epoch 258, Loss: 0.370872899889946, Final Batch Loss: 0.020917858928442\n",
      "Epoch 259, Loss: 0.2823549583554268, Final Batch Loss: 0.0158925112336874\n",
      "Epoch 260, Loss: 0.25733067467808723, Final Batch Loss: 0.01151344459503889\n",
      "Epoch 261, Loss: 0.29839154332876205, Final Batch Loss: 0.025370432063937187\n",
      "Epoch 262, Loss: 0.3457336910068989, Final Batch Loss: 0.05568203330039978\n",
      "Epoch 263, Loss: 0.32995582092553377, Final Batch Loss: 0.009717271663248539\n",
      "Epoch 264, Loss: 0.39136880403384566, Final Batch Loss: 0.05522974580526352\n",
      "Epoch 265, Loss: 0.3398651503957808, Final Batch Loss: 0.030248690396547318\n",
      "Epoch 266, Loss: 0.3258440624922514, Final Batch Loss: 0.022396093234419823\n",
      "Epoch 267, Loss: 0.35871818382292986, Final Batch Loss: 0.01697535440325737\n",
      "Epoch 268, Loss: 0.36086432728916407, Final Batch Loss: 0.03370867669582367\n",
      "Epoch 269, Loss: 0.3080640882253647, Final Batch Loss: 0.021024370566010475\n",
      "Epoch 270, Loss: 0.36924554593861103, Final Batch Loss: 0.021785367280244827\n",
      "Epoch 271, Loss: 0.2686584214679897, Final Batch Loss: 0.030803367495536804\n",
      "Epoch 272, Loss: 0.30089944414794445, Final Batch Loss: 0.019439343363046646\n",
      "Epoch 273, Loss: 0.29927358124405146, Final Batch Loss: 0.03256826847791672\n",
      "Epoch 274, Loss: 0.3154870215803385, Final Batch Loss: 0.017903149127960205\n",
      "Epoch 275, Loss: 0.29836773267015815, Final Batch Loss: 0.016742197796702385\n",
      "Epoch 276, Loss: 0.3692106492817402, Final Batch Loss: 0.04003206640481949\n",
      "Epoch 277, Loss: 0.37065902166068554, Final Batch Loss: 0.03245522826910019\n",
      "Epoch 278, Loss: 0.3092026780359447, Final Batch Loss: 0.028862234205007553\n",
      "Epoch 279, Loss: 0.3208894543349743, Final Batch Loss: 0.024697937071323395\n",
      "Epoch 280, Loss: 0.2728035943582654, Final Batch Loss: 0.052781909704208374\n",
      "Epoch 281, Loss: 0.2895768927410245, Final Batch Loss: 0.012372513301670551\n",
      "Epoch 282, Loss: 0.28604558086954057, Final Batch Loss: 0.04708942025899887\n",
      "Epoch 283, Loss: 0.34281174652278423, Final Batch Loss: 0.014130971394479275\n",
      "Epoch 284, Loss: 0.3717912221327424, Final Batch Loss: 0.04392626881599426\n",
      "Epoch 285, Loss: 0.371793064288795, Final Batch Loss: 0.014939152635633945\n",
      "Epoch 286, Loss: 0.32816241402179, Final Batch Loss: 0.019513022154569626\n",
      "Epoch 287, Loss: 0.302042284514755, Final Batch Loss: 0.013478356413543224\n",
      "Epoch 288, Loss: 0.2680874918587506, Final Batch Loss: 0.037440769374370575\n",
      "Epoch 289, Loss: 0.25412669498473406, Final Batch Loss: 0.025683408603072166\n",
      "Epoch 290, Loss: 0.337577011436224, Final Batch Loss: 0.06115765497088432\n",
      "Epoch 291, Loss: 0.3338828035630286, Final Batch Loss: 0.01875348389148712\n",
      "Epoch 292, Loss: 0.31037627067416906, Final Batch Loss: 0.01475150603801012\n",
      "Epoch 293, Loss: 0.2730141174979508, Final Batch Loss: 0.020959990099072456\n",
      "Epoch 294, Loss: 0.2932721287943423, Final Batch Loss: 0.011123579926788807\n",
      "Epoch 295, Loss: 0.2175021697767079, Final Batch Loss: 0.033259838819503784\n",
      "Epoch 296, Loss: 0.2953920227009803, Final Batch Loss: 0.023176055401563644\n",
      "Epoch 297, Loss: 0.2978324992582202, Final Batch Loss: 0.011683141812682152\n",
      "Epoch 298, Loss: 0.2701577441766858, Final Batch Loss: 0.025141125544905663\n",
      "Epoch 299, Loss: 0.31952420715242624, Final Batch Loss: 0.044586535543203354\n",
      "Epoch 300, Loss: 0.28638959350064397, Final Batch Loss: 0.011329235509037971\n",
      "Epoch 301, Loss: 0.24524571327492595, Final Batch Loss: 0.007101522292941809\n",
      "Epoch 302, Loss: 0.32240537367761135, Final Batch Loss: 0.02490316703915596\n",
      "Epoch 303, Loss: 0.27882074005901814, Final Batch Loss: 0.015070549212396145\n",
      "Epoch 304, Loss: 0.339692265028134, Final Batch Loss: 0.02129334770143032\n",
      "Epoch 305, Loss: 0.2863411447033286, Final Batch Loss: 0.02770225889980793\n",
      "Epoch 306, Loss: 0.2844224749132991, Final Batch Loss: 0.014845212921500206\n",
      "Epoch 307, Loss: 0.2668067142367363, Final Batch Loss: 0.027604632079601288\n",
      "Epoch 308, Loss: 0.3237266577780247, Final Batch Loss: 0.018633630126714706\n",
      "Epoch 309, Loss: 0.30912332609295845, Final Batch Loss: 0.0167732834815979\n",
      "Epoch 310, Loss: 0.359983473084867, Final Batch Loss: 0.006338892504572868\n",
      "Epoch 311, Loss: 0.24421510472893715, Final Batch Loss: 0.003920855466276407\n",
      "Epoch 312, Loss: 0.2231895555742085, Final Batch Loss: 0.014268212951719761\n",
      "Epoch 313, Loss: 0.2739249812439084, Final Batch Loss: 0.015551330521702766\n",
      "Epoch 314, Loss: 0.310807928442955, Final Batch Loss: 0.0253896564245224\n",
      "Epoch 315, Loss: 0.2676885756663978, Final Batch Loss: 0.033539142459630966\n",
      "Epoch 316, Loss: 0.27664480707608163, Final Batch Loss: 0.03368746116757393\n",
      "Epoch 317, Loss: 0.2575121943373233, Final Batch Loss: 0.021652143448591232\n",
      "Epoch 318, Loss: 0.22752294456586242, Final Batch Loss: 0.02836763858795166\n",
      "Epoch 319, Loss: 0.27651725290343165, Final Batch Loss: 0.009263994172215462\n",
      "Epoch 320, Loss: 0.18950607860460877, Final Batch Loss: 0.006346146110445261\n",
      "Epoch 321, Loss: 0.3234964129514992, Final Batch Loss: 0.07060413807630539\n",
      "Epoch 322, Loss: 0.3043978901114315, Final Batch Loss: 0.024001531302928925\n",
      "Epoch 323, Loss: 0.2739799255505204, Final Batch Loss: 0.04149279370903969\n",
      "Epoch 324, Loss: 0.26709320303052664, Final Batch Loss: 0.020081833004951477\n",
      "Epoch 325, Loss: 0.2925975266844034, Final Batch Loss: 0.013208024203777313\n",
      "Epoch 326, Loss: 0.2750782510265708, Final Batch Loss: 0.009420989081263542\n",
      "Epoch 327, Loss: 0.2593908382114023, Final Batch Loss: 0.009034127928316593\n",
      "Epoch 328, Loss: 0.3719264264218509, Final Batch Loss: 0.021223068237304688\n",
      "Epoch 329, Loss: 0.2603959059342742, Final Batch Loss: 0.0373283214867115\n",
      "Epoch 330, Loss: 0.31974907452240586, Final Batch Loss: 0.018165135756134987\n",
      "Epoch 331, Loss: 0.21502333274111152, Final Batch Loss: 0.012887157499790192\n",
      "Epoch 332, Loss: 0.23588899243623018, Final Batch Loss: 0.02801915630698204\n",
      "Epoch 333, Loss: 0.34473872277885675, Final Batch Loss: 0.003720514941960573\n",
      "Epoch 334, Loss: 0.21275472873821855, Final Batch Loss: 0.015962425619363785\n",
      "Epoch 335, Loss: 0.23044575843960047, Final Batch Loss: 0.010857787914574146\n",
      "Epoch 336, Loss: 0.2646715436130762, Final Batch Loss: 0.05328274518251419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337, Loss: 0.29784372728317976, Final Batch Loss: 0.026177851483225822\n",
      "Epoch 338, Loss: 0.20952410902827978, Final Batch Loss: 0.00731697678565979\n",
      "Epoch 339, Loss: 0.25185062550008297, Final Batch Loss: 0.02403826266527176\n",
      "Epoch 340, Loss: 0.3113536648452282, Final Batch Loss: 0.008398673497140408\n",
      "Epoch 341, Loss: 0.284505401737988, Final Batch Loss: 0.009161986410617828\n",
      "Epoch 342, Loss: 0.27982030948624015, Final Batch Loss: 0.03142743930220604\n",
      "Epoch 343, Loss: 0.25137674156576395, Final Batch Loss: 0.008183863013982773\n",
      "Epoch 344, Loss: 0.2730562067590654, Final Batch Loss: 0.00368196377530694\n",
      "Epoch 345, Loss: 0.2233854488003999, Final Batch Loss: 0.004573514685034752\n",
      "Epoch 346, Loss: 0.24431674927473068, Final Batch Loss: 0.00453987717628479\n",
      "Epoch 347, Loss: 0.2544239591807127, Final Batch Loss: 0.006838454864919186\n",
      "Epoch 348, Loss: 0.2426796522922814, Final Batch Loss: 0.009471525438129902\n",
      "Epoch 349, Loss: 0.23202848248183727, Final Batch Loss: 0.02435396984219551\n",
      "Epoch 350, Loss: 0.1347262696363032, Final Batch Loss: 0.013972904533147812\n",
      "Epoch 351, Loss: 0.23120193975046277, Final Batch Loss: 0.026320595294237137\n",
      "Epoch 352, Loss: 0.21497857512440532, Final Batch Loss: 0.007593294605612755\n",
      "Epoch 353, Loss: 0.27098466153256595, Final Batch Loss: 0.038019970059394836\n",
      "Epoch 354, Loss: 0.2848555133678019, Final Batch Loss: 0.013132714666426182\n",
      "Epoch 355, Loss: 0.23209134675562382, Final Batch Loss: 0.034094806760549545\n",
      "Epoch 356, Loss: 0.2960379214491695, Final Batch Loss: 0.003212251467630267\n",
      "Epoch 357, Loss: 0.3314993157982826, Final Batch Loss: 0.038716431707143784\n",
      "Epoch 358, Loss: 0.2564642406068742, Final Batch Loss: 0.013141398318111897\n",
      "Epoch 359, Loss: 0.19857262424193323, Final Batch Loss: 0.023345567286014557\n",
      "Epoch 360, Loss: 0.25889126444235444, Final Batch Loss: 0.026344245299696922\n",
      "Epoch 361, Loss: 0.2349197354633361, Final Batch Loss: 0.01128670759499073\n",
      "Epoch 362, Loss: 0.19998382753692567, Final Batch Loss: 0.005665089935064316\n",
      "Epoch 363, Loss: 0.22324768523685634, Final Batch Loss: 0.01604546792805195\n",
      "Epoch 364, Loss: 0.2220103549771011, Final Batch Loss: 0.0035086623392999172\n",
      "Epoch 365, Loss: 0.22968359082005918, Final Batch Loss: 0.02748662792146206\n",
      "Epoch 366, Loss: 0.26626686519011855, Final Batch Loss: 0.0030019409023225307\n",
      "Epoch 367, Loss: 0.23812352307140827, Final Batch Loss: 0.01675804890692234\n",
      "Epoch 368, Loss: 0.2510045077651739, Final Batch Loss: 0.005350993014872074\n",
      "Epoch 369, Loss: 0.182938480284065, Final Batch Loss: 0.005470840726047754\n",
      "Epoch 370, Loss: 0.2512304612901062, Final Batch Loss: 0.03429150953888893\n",
      "Epoch 371, Loss: 0.24271504813805223, Final Batch Loss: 0.01574600115418434\n",
      "Epoch 372, Loss: 0.2311850714031607, Final Batch Loss: 0.012365687638521194\n",
      "Epoch 373, Loss: 0.17847599321976304, Final Batch Loss: 0.009483732283115387\n",
      "Epoch 374, Loss: 0.279861971270293, Final Batch Loss: 0.010855124332010746\n",
      "Epoch 375, Loss: 0.17081451416015625, Final Batch Loss: 0.015837831422686577\n",
      "Epoch 376, Loss: 0.2191263004206121, Final Batch Loss: 0.012429635971784592\n",
      "Epoch 377, Loss: 0.22101459465920925, Final Batch Loss: 0.01198373083025217\n",
      "Epoch 378, Loss: 0.2672048443928361, Final Batch Loss: 0.014915774576365948\n",
      "Epoch 379, Loss: 0.28308144491165876, Final Batch Loss: 0.026117337867617607\n",
      "Epoch 380, Loss: 0.26653157360851765, Final Batch Loss: 0.029444586485624313\n",
      "Epoch 381, Loss: 0.2376714632846415, Final Batch Loss: 0.02287096157670021\n",
      "Epoch 382, Loss: 0.22229864820837975, Final Batch Loss: 0.02969602681696415\n",
      "Epoch 383, Loss: 0.217612667940557, Final Batch Loss: 0.006729111075401306\n",
      "Epoch 384, Loss: 0.18425796856172383, Final Batch Loss: 0.024138087406754494\n",
      "Epoch 385, Loss: 0.20183694502338767, Final Batch Loss: 0.010974487289786339\n",
      "Epoch 386, Loss: 0.16729460982605815, Final Batch Loss: 0.016969185322523117\n",
      "Epoch 387, Loss: 0.1637790068052709, Final Batch Loss: 0.007289730943739414\n",
      "Epoch 388, Loss: 0.2931641358882189, Final Batch Loss: 0.04745416343212128\n",
      "Epoch 389, Loss: 0.17014652909711003, Final Batch Loss: 0.005450728815048933\n",
      "Epoch 390, Loss: 0.1664649131707847, Final Batch Loss: 0.014766525477170944\n",
      "Epoch 391, Loss: 0.19104273850098252, Final Batch Loss: 0.005081827286630869\n",
      "Epoch 392, Loss: 0.2611210523173213, Final Batch Loss: 0.0061266254633665085\n",
      "Epoch 393, Loss: 0.257821784587577, Final Batch Loss: 0.028561241924762726\n",
      "Epoch 394, Loss: 0.21862633805721998, Final Batch Loss: 0.03269125893712044\n",
      "Epoch 395, Loss: 0.23953571543097496, Final Batch Loss: 0.02040620520710945\n",
      "Epoch 396, Loss: 0.22043445706367493, Final Batch Loss: 0.017232229933142662\n",
      "Epoch 397, Loss: 0.17078591138124466, Final Batch Loss: 0.021850431337952614\n",
      "Epoch 398, Loss: 0.16871168045327067, Final Batch Loss: 0.014902346767485142\n",
      "Epoch 399, Loss: 0.2194246812723577, Final Batch Loss: 0.004621339496225119\n",
      "Epoch 400, Loss: 0.23860542429611087, Final Batch Loss: 0.0018230746500194073\n",
      "Epoch 401, Loss: 0.20497772353701293, Final Batch Loss: 0.037656985223293304\n",
      "Epoch 402, Loss: 0.12972449592780322, Final Batch Loss: 0.006089392583817244\n",
      "Epoch 403, Loss: 0.2111426261253655, Final Batch Loss: 0.007258626166731119\n",
      "Epoch 404, Loss: 0.1532909784000367, Final Batch Loss: 0.008579222485423088\n",
      "Epoch 405, Loss: 0.21167322481051087, Final Batch Loss: 0.013422618620097637\n",
      "Epoch 406, Loss: 0.21300733229145408, Final Batch Loss: 0.012183708138763905\n",
      "Epoch 407, Loss: 0.2693845620378852, Final Batch Loss: 0.030220527201890945\n",
      "Epoch 408, Loss: 0.27840242232196033, Final Batch Loss: 0.02368570864200592\n",
      "Epoch 409, Loss: 0.2628794680349529, Final Batch Loss: 0.018962563946843147\n",
      "Epoch 410, Loss: 0.22673425637185574, Final Batch Loss: 0.005583812948316336\n",
      "Epoch 411, Loss: 0.19856773503124714, Final Batch Loss: 0.023345772176980972\n",
      "Epoch 412, Loss: 0.20175152458250523, Final Batch Loss: 0.021176835522055626\n",
      "Epoch 413, Loss: 0.20110680907964706, Final Batch Loss: 0.011336732655763626\n",
      "Epoch 414, Loss: 0.22094573080539703, Final Batch Loss: 0.04197731241583824\n",
      "Epoch 415, Loss: 0.17854368151165545, Final Batch Loss: 0.007186387199908495\n",
      "Epoch 416, Loss: 0.19672373589128256, Final Batch Loss: 0.00957314483821392\n",
      "Epoch 417, Loss: 0.19210564717650414, Final Batch Loss: 0.011334317736327648\n",
      "Epoch 418, Loss: 0.2294068243354559, Final Batch Loss: 0.010719849728047848\n",
      "Epoch 419, Loss: 0.19205798453185707, Final Batch Loss: 0.019404102116823196\n",
      "Epoch 420, Loss: 0.18099279701709747, Final Batch Loss: 0.004029932431876659\n",
      "Epoch 421, Loss: 0.20286988839507103, Final Batch Loss: 0.014561337418854237\n",
      "Epoch 422, Loss: 0.17779567348770797, Final Batch Loss: 0.01738807186484337\n",
      "Epoch 423, Loss: 0.16969718411564827, Final Batch Loss: 0.01761547103524208\n",
      "Epoch 424, Loss: 0.17403931240551174, Final Batch Loss: 0.011877019889652729\n",
      "Epoch 425, Loss: 0.24262548051774502, Final Batch Loss: 0.019493266940116882\n",
      "Epoch 426, Loss: 0.24027835112065077, Final Batch Loss: 0.016424760222434998\n",
      "Epoch 427, Loss: 0.18213894357904792, Final Batch Loss: 0.016724081709980965\n",
      "Epoch 428, Loss: 0.18641987559385598, Final Batch Loss: 0.025113364681601524\n",
      "Epoch 429, Loss: 0.20717874844558537, Final Batch Loss: 0.01555869821459055\n",
      "Epoch 430, Loss: 0.2118966872803867, Final Batch Loss: 0.015757275745272636\n",
      "Epoch 431, Loss: 0.17255846422631294, Final Batch Loss: 0.013640348799526691\n",
      "Epoch 432, Loss: 0.18324393080547452, Final Batch Loss: 0.009685863740742207\n",
      "Epoch 433, Loss: 0.24746158486232162, Final Batch Loss: 0.004459924064576626\n",
      "Epoch 434, Loss: 0.197590634226799, Final Batch Loss: 0.03560273349285126\n",
      "Epoch 435, Loss: 0.21638461470138282, Final Batch Loss: 0.008114582858979702\n",
      "Epoch 436, Loss: 0.1536122802644968, Final Batch Loss: 0.021907655522227287\n",
      "Epoch 437, Loss: 0.24124522996135056, Final Batch Loss: 0.014653878286480904\n",
      "Epoch 438, Loss: 0.17074487823992968, Final Batch Loss: 0.00911574624478817\n",
      "Epoch 439, Loss: 0.1693761902861297, Final Batch Loss: 0.01139918714761734\n",
      "Epoch 440, Loss: 0.13676112238317728, Final Batch Loss: 0.015746187418699265\n",
      "Epoch 441, Loss: 0.2175982128828764, Final Batch Loss: 0.008745362050831318\n",
      "Epoch 442, Loss: 0.18163272948004305, Final Batch Loss: 0.0075025977566838264\n",
      "Epoch 443, Loss: 0.18812860874459147, Final Batch Loss: 0.0030007052700966597\n",
      "Epoch 444, Loss: 0.21052252815570682, Final Batch Loss: 0.011164937168359756\n",
      "Epoch 445, Loss: 0.2020372413098812, Final Batch Loss: 0.013797583989799023\n",
      "Epoch 446, Loss: 0.19672222074586898, Final Batch Loss: 0.025713469833135605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447, Loss: 0.17987821134738624, Final Batch Loss: 0.0015912705566734076\n",
      "Epoch 448, Loss: 0.1632100404240191, Final Batch Loss: 0.006904422305524349\n",
      "Epoch 449, Loss: 0.20816663186997175, Final Batch Loss: 0.029897203668951988\n",
      "Epoch 450, Loss: 0.21600188314914703, Final Batch Loss: 0.004039506893604994\n",
      "Epoch 451, Loss: 0.18467458593659103, Final Batch Loss: 0.007583577651530504\n",
      "Epoch 452, Loss: 0.14856074610725045, Final Batch Loss: 0.00793714914470911\n",
      "Epoch 453, Loss: 0.20461898180656135, Final Batch Loss: 0.0036956125404685736\n",
      "Epoch 454, Loss: 0.19287533592432737, Final Batch Loss: 0.011874810792505741\n",
      "Epoch 455, Loss: 0.1826697364449501, Final Batch Loss: 0.03105435147881508\n",
      "Epoch 456, Loss: 0.20163544546812773, Final Batch Loss: 0.009589954279363155\n",
      "Epoch 457, Loss: 0.15917332912795246, Final Batch Loss: 0.012791751883924007\n",
      "Epoch 458, Loss: 0.20446632930543274, Final Batch Loss: 0.010237743146717548\n",
      "Epoch 459, Loss: 0.14791608834639192, Final Batch Loss: 0.006557636894285679\n",
      "Epoch 460, Loss: 0.13461128994822502, Final Batch Loss: 0.013743847608566284\n",
      "Epoch 461, Loss: 0.2076265229843557, Final Batch Loss: 0.020540272817015648\n",
      "Epoch 462, Loss: 0.2214578811544925, Final Batch Loss: 0.003277623327448964\n",
      "Epoch 463, Loss: 0.15736308018676937, Final Batch Loss: 0.011912914924323559\n",
      "Epoch 464, Loss: 0.18351846444420516, Final Batch Loss: 0.003275697585195303\n",
      "Epoch 465, Loss: 0.18066085875034332, Final Batch Loss: 0.010299996472895145\n",
      "Epoch 466, Loss: 0.15249967528507113, Final Batch Loss: 0.002094954252243042\n",
      "Epoch 467, Loss: 0.13483793218620121, Final Batch Loss: 0.014784607104957104\n",
      "Epoch 468, Loss: 0.16621170158032328, Final Batch Loss: 0.007435716222971678\n",
      "Epoch 469, Loss: 0.22262189583852887, Final Batch Loss: 0.03444501385092735\n",
      "Epoch 470, Loss: 0.16175475355703384, Final Batch Loss: 0.005116597283631563\n",
      "Epoch 471, Loss: 0.1589091089554131, Final Batch Loss: 0.015356766059994698\n",
      "Epoch 472, Loss: 0.135766796534881, Final Batch Loss: 0.016014497727155685\n",
      "Epoch 473, Loss: 0.16518034576438367, Final Batch Loss: 0.01381759438663721\n",
      "Epoch 474, Loss: 0.15080002648755908, Final Batch Loss: 0.011928783729672432\n",
      "Epoch 475, Loss: 0.1445408510044217, Final Batch Loss: 0.018068531528115273\n",
      "Epoch 476, Loss: 0.15668191039003432, Final Batch Loss: 0.008911630138754845\n",
      "Epoch 477, Loss: 0.15986241155769676, Final Batch Loss: 0.007824080064892769\n",
      "Epoch 478, Loss: 0.19258249551057816, Final Batch Loss: 0.020955756306648254\n",
      "Epoch 479, Loss: 0.22283423063345253, Final Batch Loss: 0.01873616687953472\n",
      "Epoch 480, Loss: 0.1840866815764457, Final Batch Loss: 0.004393482580780983\n",
      "Epoch 481, Loss: 0.11397164152003825, Final Batch Loss: 0.022976968437433243\n",
      "Epoch 482, Loss: 0.14888011943548918, Final Batch Loss: 0.013875531032681465\n",
      "Epoch 483, Loss: 0.17371100932359695, Final Batch Loss: 0.009411284700036049\n",
      "Epoch 484, Loss: 0.16515666898339987, Final Batch Loss: 0.005560044199228287\n",
      "Epoch 485, Loss: 0.1417733491398394, Final Batch Loss: 0.0009693820029497147\n",
      "Epoch 486, Loss: 0.19678167975507677, Final Batch Loss: 0.006866488140076399\n",
      "Epoch 487, Loss: 0.22409262508153915, Final Batch Loss: 0.018234102055430412\n",
      "Epoch 488, Loss: 0.15703980694524944, Final Batch Loss: 0.017100663855671883\n",
      "Epoch 489, Loss: 0.1563601151574403, Final Batch Loss: 0.006947655230760574\n",
      "Epoch 490, Loss: 0.1020368475583382, Final Batch Loss: 0.0068595996126532555\n",
      "Epoch 491, Loss: 0.19313180272001773, Final Batch Loss: 0.0031265681609511375\n",
      "Epoch 492, Loss: 0.1498818842228502, Final Batch Loss: 0.007390057668089867\n",
      "Epoch 493, Loss: 0.2389559232397005, Final Batch Loss: 0.02428491599857807\n",
      "Epoch 494, Loss: 0.24175553827080876, Final Batch Loss: 0.0036737690679728985\n",
      "Epoch 495, Loss: 0.1894213838968426, Final Batch Loss: 0.014317190274596214\n",
      "Epoch 496, Loss: 0.13612007023766637, Final Batch Loss: 0.01233568787574768\n",
      "Epoch 497, Loss: 0.24185054702684283, Final Batch Loss: 0.015834718942642212\n",
      "Epoch 498, Loss: 0.15319250733591616, Final Batch Loss: 0.007023399230092764\n",
      "Epoch 499, Loss: 0.1341419203672558, Final Batch Loss: 0.0008293476421386003\n",
      "Epoch 500, Loss: 0.14166462770663202, Final Batch Loss: 0.011197375133633614\n",
      "Epoch 501, Loss: 0.1538865438196808, Final Batch Loss: 0.003300378331914544\n",
      "Epoch 502, Loss: 0.161158760311082, Final Batch Loss: 0.020294589921832085\n",
      "Epoch 503, Loss: 0.1251240421552211, Final Batch Loss: 0.005905181635171175\n",
      "Epoch 504, Loss: 0.15661723678931594, Final Batch Loss: 0.003606576705351472\n",
      "Epoch 505, Loss: 0.10703460965305567, Final Batch Loss: 0.0031061896588653326\n",
      "Epoch 506, Loss: 0.120268834521994, Final Batch Loss: 0.009767354466021061\n",
      "Epoch 507, Loss: 0.1166492885677144, Final Batch Loss: 0.007043035700917244\n",
      "Epoch 508, Loss: 0.10628774150973186, Final Batch Loss: 0.000772686384152621\n",
      "Epoch 509, Loss: 0.14069776143878698, Final Batch Loss: 0.005618622060865164\n",
      "Epoch 510, Loss: 0.12831095326691866, Final Batch Loss: 0.0047778417356312275\n",
      "Epoch 511, Loss: 0.1307330303825438, Final Batch Loss: 0.008190711960196495\n",
      "Epoch 512, Loss: 0.1544479897711426, Final Batch Loss: 0.009142998605966568\n",
      "Epoch 513, Loss: 0.10411119135096669, Final Batch Loss: 0.012579401023685932\n",
      "Epoch 514, Loss: 0.18073444929905236, Final Batch Loss: 0.03384431079030037\n",
      "Epoch 515, Loss: 0.1642599154729396, Final Batch Loss: 0.009795679710805416\n",
      "Epoch 516, Loss: 0.11787405412178487, Final Batch Loss: 0.0016294157830998302\n",
      "Epoch 517, Loss: 0.09727089386433363, Final Batch Loss: 0.0034077498130500317\n",
      "Epoch 518, Loss: 0.1267327715177089, Final Batch Loss: 0.006229300983250141\n",
      "Epoch 519, Loss: 0.11764640719047748, Final Batch Loss: 0.015475200489163399\n",
      "Epoch 520, Loss: 0.10685941521660425, Final Batch Loss: 0.002000371692702174\n",
      "Epoch 521, Loss: 0.19100761599838734, Final Batch Loss: 0.0042844852432608604\n",
      "Epoch 522, Loss: 0.13780385933932848, Final Batch Loss: 0.03789658471941948\n",
      "Epoch 523, Loss: 0.15802646899828687, Final Batch Loss: 0.009296261705458164\n",
      "Epoch 524, Loss: 0.14165694871917367, Final Batch Loss: 0.012395796366035938\n",
      "Epoch 525, Loss: 0.10153178393375129, Final Batch Loss: 0.0046188803389668465\n",
      "Epoch 526, Loss: 0.09700097906170413, Final Batch Loss: 0.005140332505106926\n",
      "Epoch 527, Loss: 0.1985820069676265, Final Batch Loss: 0.003584339050576091\n",
      "Epoch 528, Loss: 0.12918085569981486, Final Batch Loss: 0.005493202246725559\n",
      "Epoch 529, Loss: 0.15223506384063512, Final Batch Loss: 0.009368089959025383\n",
      "Epoch 530, Loss: 0.12121131748426706, Final Batch Loss: 0.009153930470347404\n",
      "Epoch 531, Loss: 0.16141208523185924, Final Batch Loss: 0.008971653878688812\n",
      "Epoch 532, Loss: 0.17821809835731983, Final Batch Loss: 0.018329260870814323\n",
      "Epoch 533, Loss: 0.25094355759210885, Final Batch Loss: 0.0062513090670108795\n",
      "Epoch 534, Loss: 0.1036862759210635, Final Batch Loss: 0.008089704439043999\n",
      "Epoch 535, Loss: 0.17852347146254033, Final Batch Loss: 0.016602592542767525\n",
      "Epoch 536, Loss: 0.18445343733765185, Final Batch Loss: 0.015388578176498413\n",
      "Epoch 537, Loss: 0.1858079752419144, Final Batch Loss: 0.009861056692898273\n",
      "Epoch 538, Loss: 0.22802441788371652, Final Batch Loss: 0.0033864991273730993\n",
      "Epoch 539, Loss: 0.21680234908126295, Final Batch Loss: 0.011142765171825886\n",
      "Epoch 540, Loss: 0.11237131943926215, Final Batch Loss: 0.007253445219248533\n",
      "Epoch 541, Loss: 0.12657481676433235, Final Batch Loss: 0.002466550562530756\n",
      "Epoch 542, Loss: 0.12589587934780866, Final Batch Loss: 0.011830886825919151\n",
      "Epoch 543, Loss: 0.24373286333866417, Final Batch Loss: 0.04602396860718727\n",
      "Epoch 544, Loss: 0.2576340097002685, Final Batch Loss: 0.013926023617386818\n",
      "Epoch 545, Loss: 0.15287029510363936, Final Batch Loss: 0.008027424104511738\n",
      "Epoch 546, Loss: 0.14235574146732688, Final Batch Loss: 0.018593795597553253\n",
      "Epoch 547, Loss: 0.22808706248179078, Final Batch Loss: 0.0049428814090788364\n",
      "Epoch 548, Loss: 0.11369723337702453, Final Batch Loss: 0.009848363697528839\n",
      "Epoch 549, Loss: 0.1465682860580273, Final Batch Loss: 0.013350158929824829\n",
      "Epoch 550, Loss: 0.12573887943290174, Final Batch Loss: 0.009662874042987823\n",
      "Epoch 551, Loss: 0.1269560302607715, Final Batch Loss: 0.005724057089537382\n",
      "Epoch 552, Loss: 0.1246385124977678, Final Batch Loss: 0.011305528692901134\n",
      "Epoch 553, Loss: 0.10617163521237671, Final Batch Loss: 0.0036122191231697798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 554, Loss: 0.08463211357593536, Final Batch Loss: 0.010273228399455547\n",
      "Epoch 555, Loss: 0.09986745630158111, Final Batch Loss: 0.0004205748555250466\n",
      "Epoch 556, Loss: 0.11849803748191334, Final Batch Loss: 0.017059169709682465\n",
      "Epoch 557, Loss: 0.1388671591412276, Final Batch Loss: 0.0071612028405070305\n",
      "Epoch 558, Loss: 0.15040050019160844, Final Batch Loss: 0.014373213052749634\n",
      "Epoch 559, Loss: 0.16877198813017458, Final Batch Loss: 0.003545020939782262\n",
      "Epoch 560, Loss: 0.14165755757130682, Final Batch Loss: 0.009639916941523552\n",
      "Epoch 561, Loss: 0.1204929961822927, Final Batch Loss: 0.004533813335001469\n",
      "Epoch 562, Loss: 0.17906507151201367, Final Batch Loss: 0.01225319318473339\n",
      "Epoch 563, Loss: 0.12794425408355892, Final Batch Loss: 0.003600355004891753\n",
      "Epoch 564, Loss: 0.15765186573844403, Final Batch Loss: 0.0184941366314888\n",
      "Epoch 565, Loss: 0.12440583563875407, Final Batch Loss: 0.02139505371451378\n",
      "Epoch 566, Loss: 0.20195670519024134, Final Batch Loss: 0.02533155307173729\n",
      "Epoch 567, Loss: 0.1396832847967744, Final Batch Loss: 0.007063691038638353\n",
      "Epoch 568, Loss: 0.13379171187989414, Final Batch Loss: 0.012842625379562378\n",
      "Epoch 569, Loss: 0.16337090614251792, Final Batch Loss: 0.0030279525090008974\n",
      "Epoch 570, Loss: 0.17737564258277416, Final Batch Loss: 0.027499660849571228\n",
      "Epoch 571, Loss: 0.17376067911391146, Final Batch Loss: 0.0009275042684748769\n",
      "Epoch 572, Loss: 0.13762439871788956, Final Batch Loss: 0.015775032341480255\n",
      "Epoch 573, Loss: 0.16683272283989936, Final Batch Loss: 0.0007355303969234228\n",
      "Epoch 574, Loss: 0.1499470446142368, Final Batch Loss: 0.0027254722081124783\n",
      "Epoch 575, Loss: 0.1497580863069743, Final Batch Loss: 0.008086668327450752\n",
      "Epoch 576, Loss: 0.12169086578069255, Final Batch Loss: 0.004353727214038372\n",
      "Epoch 577, Loss: 0.14776155742583796, Final Batch Loss: 0.004484876524657011\n",
      "Epoch 578, Loss: 0.17737767985090613, Final Batch Loss: 0.018327685073018074\n",
      "Epoch 579, Loss: 0.11803303891792893, Final Batch Loss: 0.019810909405350685\n",
      "Epoch 580, Loss: 0.1666977343847975, Final Batch Loss: 0.004087739624083042\n",
      "Epoch 581, Loss: 0.15810166590381414, Final Batch Loss: 0.012452255003154278\n",
      "Epoch 582, Loss: 0.1927693117177114, Final Batch Loss: 0.004045349080115557\n",
      "Epoch 583, Loss: 0.12238050473388284, Final Batch Loss: 0.0012606052914634347\n",
      "Epoch 584, Loss: 0.20629006437957287, Final Batch Loss: 0.00528282904997468\n",
      "Epoch 585, Loss: 0.11624207976274192, Final Batch Loss: 0.008697633631527424\n",
      "Epoch 586, Loss: 0.09900376643054187, Final Batch Loss: 0.009574254974722862\n",
      "Epoch 587, Loss: 0.12390024459455162, Final Batch Loss: 0.0008701662882231176\n",
      "Epoch 588, Loss: 0.08280731094419025, Final Batch Loss: 0.004623251501470804\n",
      "Epoch 589, Loss: 0.08475840894971043, Final Batch Loss: 0.010824227705597878\n",
      "Epoch 590, Loss: 0.1264216760318959, Final Batch Loss: 0.004960934165865183\n",
      "Epoch 591, Loss: 0.1410469645052217, Final Batch Loss: 0.007931405678391457\n",
      "Epoch 592, Loss: 0.11586816841736436, Final Batch Loss: 0.012582148425281048\n",
      "Epoch 593, Loss: 0.08677019237075001, Final Batch Loss: 0.006975984666496515\n",
      "Epoch 594, Loss: 0.10758789425017312, Final Batch Loss: 0.003585041733458638\n",
      "Epoch 595, Loss: 0.1905038165859878, Final Batch Loss: 0.02563696913421154\n",
      "Epoch 596, Loss: 0.1673099465551786, Final Batch Loss: 0.0003767308662645519\n",
      "Epoch 597, Loss: 0.19777652411721647, Final Batch Loss: 0.013988240621984005\n",
      "Epoch 598, Loss: 0.21305197710171342, Final Batch Loss: 0.006247349549084902\n",
      "Epoch 599, Loss: 0.1590403257869184, Final Batch Loss: 0.017316389828920364\n",
      "Epoch 600, Loss: 0.09687591309193522, Final Batch Loss: 0.006884906440973282\n",
      "Epoch 601, Loss: 0.11966913100332022, Final Batch Loss: 0.013739902526140213\n",
      "Epoch 602, Loss: 0.1382882617181167, Final Batch Loss: 0.005775516852736473\n",
      "Epoch 603, Loss: 0.18654286896344274, Final Batch Loss: 0.0336252897977829\n",
      "Epoch 604, Loss: 0.11775778932496905, Final Batch Loss: 0.004872347693890333\n",
      "Epoch 605, Loss: 0.08236425870563835, Final Batch Loss: 0.002673867391422391\n",
      "Epoch 606, Loss: 0.11587805912131444, Final Batch Loss: 0.008773554116487503\n",
      "Epoch 607, Loss: 0.12080545537173748, Final Batch Loss: 0.0028838017024099827\n",
      "Epoch 608, Loss: 0.11223995103500783, Final Batch Loss: 0.0028658073861151934\n",
      "Epoch 609, Loss: 0.12058789120055735, Final Batch Loss: 0.00563843734562397\n",
      "Epoch 610, Loss: 0.07691448595141992, Final Batch Loss: 0.005965296179056168\n",
      "Epoch 611, Loss: 0.11821442598011345, Final Batch Loss: 0.004545959644019604\n",
      "Epoch 612, Loss: 0.13524756720289588, Final Batch Loss: 0.0054609584622085094\n",
      "Epoch 613, Loss: 0.09018722205655649, Final Batch Loss: 0.020770104601979256\n",
      "Epoch 614, Loss: 0.11799343628808856, Final Batch Loss: 0.03600343316793442\n",
      "Epoch 615, Loss: 0.10327942715957761, Final Batch Loss: 0.0032410165295004845\n",
      "Epoch 616, Loss: 0.14461513835703954, Final Batch Loss: 0.0009303428814746439\n",
      "Epoch 617, Loss: 0.14462026057299227, Final Batch Loss: 0.013713299296796322\n",
      "Epoch 618, Loss: 0.12610302469693124, Final Batch Loss: 0.007786602713167667\n",
      "Epoch 619, Loss: 0.108521563350223, Final Batch Loss: 0.00363190402276814\n",
      "Epoch 620, Loss: 0.14577040844596922, Final Batch Loss: 0.009947596117854118\n",
      "Epoch 621, Loss: 0.26212967932224274, Final Batch Loss: 0.011043991893529892\n",
      "Epoch 622, Loss: 0.26471000583842397, Final Batch Loss: 0.012705564498901367\n",
      "Epoch 623, Loss: 0.31117431819438934, Final Batch Loss: 0.0024179299362003803\n",
      "Epoch 624, Loss: 0.18063368392176926, Final Batch Loss: 0.0028666371945291758\n",
      "Epoch 625, Loss: 0.19956828211434186, Final Batch Loss: 0.007028949912637472\n",
      "Epoch 626, Loss: 0.12260646349750459, Final Batch Loss: 0.002169270534068346\n",
      "Epoch 627, Loss: 0.1854073569411412, Final Batch Loss: 0.002710120053961873\n",
      "Epoch 628, Loss: 0.14060613652691245, Final Batch Loss: 0.002124861581251025\n",
      "Epoch 629, Loss: 0.21675864816643298, Final Batch Loss: 0.008082312531769276\n",
      "Epoch 630, Loss: 0.10010818229056895, Final Batch Loss: 0.005217098630964756\n",
      "Epoch 631, Loss: 0.1582897324115038, Final Batch Loss: 0.021646587178111076\n",
      "Epoch 632, Loss: 0.09035179042257369, Final Batch Loss: 0.009460470639169216\n",
      "Epoch 633, Loss: 0.13566872873343527, Final Batch Loss: 0.004879913758486509\n",
      "Epoch 634, Loss: 0.10933959943940863, Final Batch Loss: 0.003409696277230978\n",
      "Epoch 635, Loss: 0.09972342324908823, Final Batch Loss: 0.028752122074365616\n",
      "Epoch 636, Loss: 0.10325449157971889, Final Batch Loss: 0.0033713511656969786\n",
      "Epoch 637, Loss: 0.11273799964692444, Final Batch Loss: 0.02709222584962845\n",
      "Epoch 638, Loss: 0.13147927052341402, Final Batch Loss: 0.03225162625312805\n",
      "Epoch 639, Loss: 0.12445846816990525, Final Batch Loss: 0.02237166278064251\n",
      "Epoch 640, Loss: 0.18160223285667598, Final Batch Loss: 0.002621857449412346\n",
      "Epoch 641, Loss: 0.14267586637288332, Final Batch Loss: 0.008222764357924461\n",
      "Epoch 642, Loss: 0.13220664451364428, Final Batch Loss: 0.005599065683782101\n",
      "Epoch 643, Loss: 0.09736086206976324, Final Batch Loss: 0.002145556965842843\n",
      "Epoch 644, Loss: 0.11788020769017749, Final Batch Loss: 0.0005279504111967981\n",
      "Epoch 645, Loss: 0.09489191428292543, Final Batch Loss: 0.001796544180251658\n",
      "Epoch 646, Loss: 0.1219378846581094, Final Batch Loss: 0.0020642965100705624\n",
      "Epoch 647, Loss: 0.11942430844646879, Final Batch Loss: 0.008223035372793674\n",
      "Epoch 648, Loss: 0.14834714401513338, Final Batch Loss: 0.006066455971449614\n",
      "Epoch 649, Loss: 0.11097361316205934, Final Batch Loss: 0.018243011087179184\n",
      "Epoch 650, Loss: 0.11429404525551945, Final Batch Loss: 0.00523116160184145\n",
      "Epoch 651, Loss: 0.11023715769988485, Final Batch Loss: 0.008573949337005615\n",
      "Epoch 652, Loss: 0.11424752522725612, Final Batch Loss: 0.014232154935598373\n",
      "Epoch 653, Loss: 0.1004058662801981, Final Batch Loss: 0.0020233618561178446\n",
      "Epoch 654, Loss: 0.13132760577718727, Final Batch Loss: 0.007328391075134277\n",
      "Epoch 655, Loss: 0.10523484949953854, Final Batch Loss: 0.01389151718467474\n",
      "Epoch 656, Loss: 0.11937522975495085, Final Batch Loss: 0.013248669914901257\n",
      "Epoch 657, Loss: 0.13568252208642662, Final Batch Loss: 0.0066968114115297794\n",
      "Epoch 658, Loss: 0.0802975888364017, Final Batch Loss: 0.006262247450649738\n",
      "Epoch 659, Loss: 0.06973293441114947, Final Batch Loss: 0.0034048473462462425\n",
      "Epoch 660, Loss: 0.16705804743105546, Final Batch Loss: 0.004455950576812029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661, Loss: 0.1984925446449779, Final Batch Loss: 0.036806635558605194\n",
      "Epoch 662, Loss: 0.1443906065542251, Final Batch Loss: 0.008081905543804169\n",
      "Epoch 663, Loss: 0.16798251448199153, Final Batch Loss: 0.0010498748160898685\n",
      "Epoch 664, Loss: 0.0828221112024039, Final Batch Loss: 0.004067573230713606\n",
      "Epoch 665, Loss: 0.0919848081539385, Final Batch Loss: 0.020059842616319656\n",
      "Epoch 666, Loss: 0.0810166071751155, Final Batch Loss: 0.00644485792145133\n",
      "Epoch 667, Loss: 0.08920262253377587, Final Batch Loss: 0.0044740354642271996\n",
      "Epoch 668, Loss: 0.11350819759536535, Final Batch Loss: 0.01092907227575779\n",
      "Epoch 669, Loss: 0.08062035054899752, Final Batch Loss: 0.003387472126632929\n",
      "Epoch 670, Loss: 0.09185187926050276, Final Batch Loss: 0.004687890410423279\n",
      "Epoch 671, Loss: 0.05877906546811573, Final Batch Loss: 0.008230370469391346\n",
      "Epoch 672, Loss: 0.10505349622690119, Final Batch Loss: 0.0024966381024569273\n",
      "Epoch 673, Loss: 0.17069850536063313, Final Batch Loss: 0.017559748142957687\n",
      "Epoch 674, Loss: 0.14816188672557473, Final Batch Loss: 0.003648461075499654\n",
      "Epoch 675, Loss: 0.08751969563309103, Final Batch Loss: 0.0010680994018912315\n",
      "Epoch 676, Loss: 0.06587473271065392, Final Batch Loss: 0.002181422198191285\n",
      "Epoch 677, Loss: 0.08051221886125859, Final Batch Loss: 0.005365246906876564\n",
      "Epoch 678, Loss: 0.05514292448060587, Final Batch Loss: 0.004468394443392754\n",
      "Epoch 679, Loss: 0.07404312877042685, Final Batch Loss: 0.0036542187444865704\n",
      "Epoch 680, Loss: 0.14626670686993748, Final Batch Loss: 0.006348127964884043\n",
      "Epoch 681, Loss: 0.12777389204711653, Final Batch Loss: 0.0170599315315485\n",
      "Epoch 682, Loss: 0.34351193089969456, Final Batch Loss: 0.022496411576867104\n",
      "Epoch 683, Loss: 0.3388414749642834, Final Batch Loss: 0.00346749578602612\n",
      "Epoch 684, Loss: 0.14730343013070524, Final Batch Loss: 0.005810077302157879\n",
      "Epoch 685, Loss: 0.12990411138162017, Final Batch Loss: 0.01538446731865406\n",
      "Epoch 686, Loss: 0.1538798059336841, Final Batch Loss: 0.006664272863417864\n",
      "Epoch 687, Loss: 0.19395277521107346, Final Batch Loss: 0.01575695350766182\n",
      "Epoch 688, Loss: 0.0978792158421129, Final Batch Loss: 0.006854040082544088\n",
      "Epoch 689, Loss: 0.14409930049441755, Final Batch Loss: 0.01680268533527851\n",
      "Epoch 690, Loss: 0.11312333506066352, Final Batch Loss: 0.0048246062360703945\n",
      "Epoch 691, Loss: 0.11646815913263708, Final Batch Loss: 0.0031112025026232004\n",
      "Epoch 692, Loss: 0.09997473430121318, Final Batch Loss: 0.02232808992266655\n",
      "Epoch 693, Loss: 0.105300385679584, Final Batch Loss: 0.012997905723750591\n",
      "Epoch 694, Loss: 0.08797760680317879, Final Batch Loss: 0.002870669588446617\n",
      "Epoch 695, Loss: 0.07889294205233455, Final Batch Loss: 0.004716396797448397\n",
      "Epoch 696, Loss: 0.10068962644436397, Final Batch Loss: 0.010762196034193039\n",
      "Epoch 697, Loss: 0.11797863128595054, Final Batch Loss: 0.0066285450011491776\n",
      "Epoch 698, Loss: 0.10299716843292117, Final Batch Loss: 0.0020958310924470425\n",
      "Epoch 699, Loss: 0.06665538216475397, Final Batch Loss: 0.0005252243718132377\n",
      "Epoch 700, Loss: 0.09947100043063983, Final Batch Loss: 0.009131532162427902\n",
      "Epoch 701, Loss: 0.1460916688083671, Final Batch Loss: 0.004978565964847803\n",
      "Epoch 702, Loss: 0.12019608519040048, Final Batch Loss: 0.011051815003156662\n",
      "Epoch 703, Loss: 0.09976807571365498, Final Batch Loss: 0.0025757583789527416\n",
      "Epoch 704, Loss: 0.07064190396340564, Final Batch Loss: 0.0011739881010726094\n",
      "Epoch 705, Loss: 0.11414516682270914, Final Batch Loss: 0.0038851266726851463\n",
      "Epoch 706, Loss: 0.09355225940817036, Final Batch Loss: 0.002720387652516365\n",
      "Epoch 707, Loss: 0.08870750735513866, Final Batch Loss: 0.000853509409353137\n",
      "Epoch 708, Loss: 0.1178682534082327, Final Batch Loss: 0.012043147347867489\n",
      "Epoch 709, Loss: 0.08714911673450842, Final Batch Loss: 0.0015153372660279274\n",
      "Epoch 710, Loss: 0.04870944906724617, Final Batch Loss: 0.011466175317764282\n",
      "Epoch 711, Loss: 0.059395600197603926, Final Batch Loss: 0.010312020778656006\n",
      "Epoch 712, Loss: 0.07503093522973359, Final Batch Loss: 0.0009292580070905387\n",
      "Epoch 713, Loss: 0.09878029633546248, Final Batch Loss: 0.0092198820784688\n",
      "Epoch 714, Loss: 0.12281952321063727, Final Batch Loss: 0.02938186004757881\n",
      "Epoch 715, Loss: 0.07878443825757131, Final Batch Loss: 0.003198377089574933\n",
      "Epoch 716, Loss: 0.15118265725323, Final Batch Loss: 0.008563651703298092\n",
      "Epoch 717, Loss: 0.10690654534846544, Final Batch Loss: 0.0072247060015797615\n",
      "Epoch 718, Loss: 0.10104936076095328, Final Batch Loss: 0.004689519759267569\n",
      "Epoch 719, Loss: 0.07721046736696735, Final Batch Loss: 0.005446684081107378\n",
      "Epoch 720, Loss: 0.1470845589064993, Final Batch Loss: 0.02430802956223488\n",
      "Epoch 721, Loss: 0.10234201024286449, Final Batch Loss: 0.004221935290843248\n",
      "Epoch 722, Loss: 0.11528697854373604, Final Batch Loss: 0.00853577721863985\n",
      "Epoch 723, Loss: 0.10767305002082139, Final Batch Loss: 0.0011484355200082064\n",
      "Epoch 724, Loss: 0.10317647524061613, Final Batch Loss: 0.004853765480220318\n",
      "Epoch 725, Loss: 0.12366240832488984, Final Batch Loss: 0.004897817969322205\n",
      "Epoch 726, Loss: 0.06218730233376846, Final Batch Loss: 0.0013695344096049666\n",
      "Epoch 727, Loss: 0.09068725467659533, Final Batch Loss: 0.0016515894094482064\n",
      "Epoch 728, Loss: 0.06575349441118306, Final Batch Loss: 0.024331634864211082\n",
      "Epoch 729, Loss: 0.09385290800128132, Final Batch Loss: 0.008009614422917366\n",
      "Epoch 730, Loss: 0.07918314053677022, Final Batch Loss: 0.0059265452437102795\n",
      "Epoch 731, Loss: 0.11108910560142249, Final Batch Loss: 0.004161095712333918\n",
      "Epoch 732, Loss: 0.09866052510915324, Final Batch Loss: 0.0034319227561354637\n",
      "Epoch 733, Loss: 0.1415298543870449, Final Batch Loss: 0.00041132455226033926\n",
      "Epoch 734, Loss: 0.1397650089347735, Final Batch Loss: 0.027998359873890877\n",
      "Epoch 735, Loss: 0.11342318635433912, Final Batch Loss: 0.008364254608750343\n",
      "Epoch 736, Loss: 0.07167258410481736, Final Batch Loss: 0.0036635270807892084\n",
      "Epoch 737, Loss: 0.12375825666822493, Final Batch Loss: 0.007448185235261917\n",
      "Epoch 738, Loss: 0.07426787202712148, Final Batch Loss: 0.0016725934110581875\n",
      "Epoch 739, Loss: 0.15595340041909367, Final Batch Loss: 0.001547164167277515\n",
      "Epoch 740, Loss: 0.11044553597457707, Final Batch Loss: 0.006150324363261461\n",
      "Epoch 741, Loss: 0.11976241620141082, Final Batch Loss: 0.007105221040546894\n",
      "Epoch 742, Loss: 0.11699762346688658, Final Batch Loss: 0.0022749060299247503\n",
      "Epoch 743, Loss: 0.10363042185781524, Final Batch Loss: 0.022518178448081017\n",
      "Epoch 744, Loss: 0.1185128364013508, Final Batch Loss: 0.004544045310467482\n",
      "Epoch 745, Loss: 0.13355214678449556, Final Batch Loss: 0.0006808211328461766\n",
      "Epoch 746, Loss: 0.09496015281183645, Final Batch Loss: 0.013156403787434101\n",
      "Epoch 747, Loss: 0.09722201386466622, Final Batch Loss: 0.009940273128449917\n",
      "Epoch 748, Loss: 0.10813056805636734, Final Batch Loss: 0.010067502036690712\n",
      "Epoch 749, Loss: 0.11947760684415698, Final Batch Loss: 0.006343580316752195\n",
      "Epoch 750, Loss: 0.0958020206890069, Final Batch Loss: 0.010737689211964607\n",
      "Epoch 751, Loss: 0.0843300896231085, Final Batch Loss: 0.004383270628750324\n",
      "Epoch 752, Loss: 0.09923287508718204, Final Batch Loss: 0.003180041443556547\n",
      "Epoch 753, Loss: 0.07263003260595724, Final Batch Loss: 0.0014156088000163436\n",
      "Epoch 754, Loss: 0.1267829515854828, Final Batch Loss: 0.027415035292506218\n",
      "Epoch 755, Loss: 0.1004788936697878, Final Batch Loss: 0.001839740900322795\n",
      "Epoch 756, Loss: 0.11788734147557989, Final Batch Loss: 0.020007848739624023\n",
      "Epoch 757, Loss: 0.13909580634208396, Final Batch Loss: 0.0030288214329630136\n",
      "Epoch 758, Loss: 0.11730911117047071, Final Batch Loss: 0.003993472084403038\n",
      "Epoch 759, Loss: 0.12369874713476747, Final Batch Loss: 0.008705215528607368\n",
      "Epoch 760, Loss: 0.0869721362250857, Final Batch Loss: 0.003979573026299477\n",
      "Epoch 761, Loss: 0.08884073578519747, Final Batch Loss: 0.001028028898872435\n",
      "Epoch 762, Loss: 0.10918274114374071, Final Batch Loss: 0.002767485100775957\n",
      "Epoch 763, Loss: 0.14451606548391283, Final Batch Loss: 0.010240004397928715\n",
      "Epoch 764, Loss: 0.0846666470170021, Final Batch Loss: 0.01231600996106863\n",
      "Epoch 765, Loss: 0.12863120270776562, Final Batch Loss: 0.024069208651781082\n",
      "Epoch 766, Loss: 0.11174083355581388, Final Batch Loss: 0.05133931711316109\n",
      "Epoch 767, Loss: 0.07607698178617284, Final Batch Loss: 0.009697189554572105\n",
      "Epoch 768, Loss: 0.09909469680860639, Final Batch Loss: 0.018808001652359962\n",
      "Epoch 769, Loss: 0.07207977038342506, Final Batch Loss: 0.003557757707312703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770, Loss: 0.07392123434692621, Final Batch Loss: 0.004451398737728596\n",
      "Epoch 771, Loss: 0.07490908188628964, Final Batch Loss: 0.0008231858373619616\n",
      "Epoch 772, Loss: 0.09579466335708275, Final Batch Loss: 0.00038881797809153795\n",
      "Epoch 773, Loss: 0.24554058734793216, Final Batch Loss: 0.010788791812956333\n",
      "Epoch 774, Loss: 0.14843457855749875, Final Batch Loss: 0.02578512392938137\n",
      "Epoch 775, Loss: 0.1519126077182591, Final Batch Loss: 0.005799037404358387\n",
      "Epoch 776, Loss: 0.15993835718836635, Final Batch Loss: 0.004394011106342077\n",
      "Epoch 777, Loss: 0.18979774857871234, Final Batch Loss: 0.011078010313212872\n",
      "Epoch 778, Loss: 0.1557821648893878, Final Batch Loss: 0.015292163006961346\n",
      "Epoch 779, Loss: 0.0941096052993089, Final Batch Loss: 0.005928842816501856\n",
      "Epoch 780, Loss: 0.07313511468237266, Final Batch Loss: 0.003354904940351844\n",
      "Epoch 781, Loss: 0.0763334646471776, Final Batch Loss: 0.0031431070528924465\n",
      "Epoch 782, Loss: 0.07406050802092068, Final Batch Loss: 0.000908116577193141\n",
      "Epoch 783, Loss: 0.05157776968553662, Final Batch Loss: 0.004779360722750425\n",
      "Epoch 784, Loss: 0.03773724256461719, Final Batch Loss: 8.511798660038039e-05\n",
      "Epoch 785, Loss: 0.09371428858139552, Final Batch Loss: 0.031638003885746\n",
      "Epoch 786, Loss: 0.0882586551742861, Final Batch Loss: 0.00842108204960823\n",
      "Epoch 787, Loss: 0.1981085466541117, Final Batch Loss: 0.0019466414814814925\n",
      "Epoch 788, Loss: 0.06749779602978379, Final Batch Loss: 0.001645914395339787\n",
      "Epoch 789, Loss: 0.05237043740635272, Final Batch Loss: 0.005577011965215206\n",
      "Epoch 790, Loss: 0.08268212428083643, Final Batch Loss: 0.0025653548073023558\n",
      "Epoch 791, Loss: 0.0795317079173401, Final Batch Loss: 0.002322122221812606\n",
      "Epoch 792, Loss: 0.07965860533295199, Final Batch Loss: 0.012562309391796589\n",
      "Epoch 793, Loss: 0.07713319075992331, Final Batch Loss: 0.0009818654507398605\n",
      "Epoch 794, Loss: 0.062065365607850254, Final Batch Loss: 0.0032371997367590666\n",
      "Epoch 795, Loss: 0.15142529783770442, Final Batch Loss: 0.02769368514418602\n",
      "Epoch 796, Loss: 0.15652787814178737, Final Batch Loss: 0.0028451248072087765\n",
      "Epoch 797, Loss: 0.08902220579329878, Final Batch Loss: 0.004700121004134417\n",
      "Epoch 798, Loss: 0.10690098197665066, Final Batch Loss: 0.013619723729789257\n",
      "Epoch 799, Loss: 0.10434927706955932, Final Batch Loss: 0.022525591775774956\n",
      "Epoch 800, Loss: 0.06964264076668769, Final Batch Loss: 0.0022450729738920927\n",
      "Epoch 801, Loss: 0.08138486335519701, Final Batch Loss: 0.003096280386671424\n",
      "Epoch 802, Loss: 0.08256815141066909, Final Batch Loss: 0.008085164241492748\n",
      "Epoch 803, Loss: 0.10953537531895563, Final Batch Loss: 0.02098206989467144\n",
      "Epoch 804, Loss: 0.10643812015769072, Final Batch Loss: 0.009687053970992565\n",
      "Epoch 805, Loss: 0.08909499540459365, Final Batch Loss: 0.0018200338818132877\n",
      "Epoch 806, Loss: 0.10893184202723205, Final Batch Loss: 0.0024927137419581413\n",
      "Epoch 807, Loss: 0.15249592089094222, Final Batch Loss: 0.004321932327002287\n",
      "Epoch 808, Loss: 0.09917737380601466, Final Batch Loss: 0.021669548004865646\n",
      "Epoch 809, Loss: 0.09690482565201819, Final Batch Loss: 0.012807263992726803\n",
      "Epoch 810, Loss: 0.16407265095040202, Final Batch Loss: 0.006907645147293806\n",
      "Epoch 811, Loss: 0.2886974950088188, Final Batch Loss: 0.004038723651319742\n",
      "Epoch 812, Loss: 0.2323940391652286, Final Batch Loss: 0.0016404693014919758\n",
      "Epoch 813, Loss: 0.158302511961665, Final Batch Loss: 0.06855469942092896\n",
      "Epoch 814, Loss: 0.11799157178029418, Final Batch Loss: 0.009508827701210976\n",
      "Epoch 815, Loss: 0.1503297514282167, Final Batch Loss: 0.011440425179898739\n",
      "Epoch 816, Loss: 0.10741023387527093, Final Batch Loss: 0.0010992237366735935\n",
      "Epoch 817, Loss: 0.1011038813740015, Final Batch Loss: 0.03798400238156319\n",
      "Epoch 818, Loss: 0.07713855482870713, Final Batch Loss: 0.003907275851815939\n",
      "Epoch 819, Loss: 0.06256284023402259, Final Batch Loss: 0.00627899868413806\n",
      "Epoch 820, Loss: 0.07625887505128048, Final Batch Loss: 0.000986085506156087\n",
      "Epoch 821, Loss: 0.09184058819664642, Final Batch Loss: 0.009360079653561115\n",
      "Epoch 822, Loss: 0.16280931551591493, Final Batch Loss: 0.00027764710830524564\n",
      "Epoch 823, Loss: 0.15278343885438517, Final Batch Loss: 0.0015592455165460706\n",
      "Epoch 824, Loss: 0.07054486707784235, Final Batch Loss: 0.0029104414861649275\n",
      "Epoch 825, Loss: 0.024779963248874992, Final Batch Loss: 0.0026758317835628986\n",
      "Epoch 826, Loss: 0.082321935085929, Final Batch Loss: 0.0009469507494941354\n",
      "Epoch 827, Loss: 0.0655286253313534, Final Batch Loss: 0.0016030187252908945\n",
      "Epoch 828, Loss: 0.20798557074158452, Final Batch Loss: 0.0021382460836321115\n",
      "Epoch 829, Loss: 0.1595965248416178, Final Batch Loss: 0.0022153689060360193\n",
      "Epoch 830, Loss: 0.18029897776432335, Final Batch Loss: 0.008178730495274067\n",
      "Epoch 831, Loss: 0.12492597138043493, Final Batch Loss: 0.007460002321749926\n",
      "Epoch 832, Loss: 0.12852285231929272, Final Batch Loss: 0.002718296367675066\n",
      "Epoch 833, Loss: 0.04937601421261206, Final Batch Loss: 0.0014826180413365364\n",
      "Epoch 834, Loss: 0.07371726771816611, Final Batch Loss: 0.009330925531685352\n",
      "Epoch 835, Loss: 0.08701310824835673, Final Batch Loss: 0.0022218001540750265\n",
      "Epoch 836, Loss: 0.06933815527008846, Final Batch Loss: 0.0014631578233093023\n",
      "Epoch 837, Loss: 0.052871151943691075, Final Batch Loss: 0.005995616316795349\n",
      "Epoch 838, Loss: 0.07691794575657696, Final Batch Loss: 0.0026269075460731983\n",
      "Epoch 839, Loss: 0.08550766340340488, Final Batch Loss: 0.0017255826387554407\n",
      "Epoch 840, Loss: 0.10186428483575583, Final Batch Loss: 0.002664955100044608\n",
      "Epoch 841, Loss: 0.06251934333704412, Final Batch Loss: 0.0009783707791939378\n",
      "Epoch 842, Loss: 0.16923003483680077, Final Batch Loss: 0.008194245398044586\n",
      "Epoch 843, Loss: 0.08605034020729363, Final Batch Loss: 0.005791692063212395\n",
      "Epoch 844, Loss: 0.08217803633306175, Final Batch Loss: 0.0028261691331863403\n",
      "Epoch 845, Loss: 0.06827419839100912, Final Batch Loss: 0.00496884249150753\n",
      "Epoch 846, Loss: 0.0878173167584464, Final Batch Loss: 0.019719546660780907\n",
      "Epoch 847, Loss: 0.05547161589493044, Final Batch Loss: 0.002838939195498824\n",
      "Epoch 848, Loss: 0.08572935464326292, Final Batch Loss: 0.017597001045942307\n",
      "Epoch 849, Loss: 0.07237710119807161, Final Batch Loss: 0.008034473285079002\n",
      "Epoch 850, Loss: 0.08181055827299133, Final Batch Loss: 0.009552577510476112\n",
      "Epoch 851, Loss: 0.06280768782016821, Final Batch Loss: 0.006661189254373312\n",
      "Epoch 852, Loss: 0.10343492950778455, Final Batch Loss: 0.015908261761069298\n",
      "Epoch 853, Loss: 0.08956449484685436, Final Batch Loss: 0.0020792752038687468\n",
      "Epoch 854, Loss: 0.11264620983274654, Final Batch Loss: 0.007243447471410036\n",
      "Epoch 855, Loss: 0.059867227311769966, Final Batch Loss: 0.001363790244795382\n",
      "Epoch 856, Loss: 0.037398961329017766, Final Batch Loss: 0.0002457145310472697\n",
      "Epoch 857, Loss: 0.08107551134889945, Final Batch Loss: 0.0012931256787851453\n",
      "Epoch 858, Loss: 0.09070079353114124, Final Batch Loss: 0.009391683153808117\n",
      "Epoch 859, Loss: 0.07191591348964721, Final Batch Loss: 0.02281847782433033\n",
      "Epoch 860, Loss: 0.05190158702316694, Final Batch Loss: 0.007013986352831125\n",
      "Epoch 861, Loss: 0.08823985478375107, Final Batch Loss: 0.0017520332476124167\n",
      "Epoch 862, Loss: 0.13572286709677428, Final Batch Loss: 0.0040212064050138\n",
      "Epoch 863, Loss: 0.16406936294515617, Final Batch Loss: 0.012405135668814182\n",
      "Epoch 864, Loss: 0.10706766520161182, Final Batch Loss: 0.003579420503228903\n",
      "Epoch 865, Loss: 0.10997924432740547, Final Batch Loss: 0.003066319040954113\n",
      "Epoch 866, Loss: 0.11572353675728664, Final Batch Loss: 0.003782969433814287\n",
      "Epoch 867, Loss: 0.114679487189278, Final Batch Loss: 0.008188198320567608\n",
      "Epoch 868, Loss: 0.11852475500199944, Final Batch Loss: 0.003397216321900487\n",
      "Epoch 869, Loss: 0.09701343823689967, Final Batch Loss: 0.001645382377319038\n",
      "Epoch 870, Loss: 0.06871220213361084, Final Batch Loss: 0.010620541870594025\n",
      "Epoch 871, Loss: 0.07237649208400398, Final Batch Loss: 0.005868146196007729\n",
      "Epoch 872, Loss: 0.09334560221759602, Final Batch Loss: 0.0017075567739084363\n",
      "Epoch 873, Loss: 0.07939347151841503, Final Batch Loss: 0.013614521361887455\n",
      "Epoch 874, Loss: 0.05686221149517223, Final Batch Loss: 0.006430125329643488\n",
      "Epoch 875, Loss: 0.07452289486536756, Final Batch Loss: 0.0007165379938669503\n",
      "Epoch 876, Loss: 0.06217388066579588, Final Batch Loss: 0.0017628020141273737\n",
      "Epoch 877, Loss: 0.061683077714405954, Final Batch Loss: 0.005006656050682068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 878, Loss: 0.058979538443963975, Final Batch Loss: 0.005426539108157158\n",
      "Epoch 879, Loss: 0.059492923253856134, Final Batch Loss: 0.002485893666744232\n",
      "Epoch 880, Loss: 0.09485783875788911, Final Batch Loss: 0.005706785712391138\n",
      "Epoch 881, Loss: 0.07006251433631405, Final Batch Loss: 0.0026987188030034304\n",
      "Epoch 882, Loss: 0.05739422785700299, Final Batch Loss: 0.0015407042810693383\n",
      "Epoch 883, Loss: 0.04462339179008268, Final Batch Loss: 0.006850606761872768\n",
      "Epoch 884, Loss: 0.16384126141201705, Final Batch Loss: 0.005690836347639561\n",
      "Epoch 885, Loss: 0.10435145441442728, Final Batch Loss: 0.011804097332060337\n",
      "Epoch 886, Loss: 0.054668792043230496, Final Batch Loss: 0.007777355145663023\n",
      "Epoch 887, Loss: 0.0952283729420742, Final Batch Loss: 0.0010157774668186903\n",
      "Epoch 888, Loss: 0.06663755618501455, Final Batch Loss: 0.001262895530089736\n",
      "Epoch 889, Loss: 0.0536381799611263, Final Batch Loss: 0.002818202367052436\n",
      "Epoch 890, Loss: 0.08571526175364852, Final Batch Loss: 0.004477354232221842\n",
      "Epoch 891, Loss: 0.08416188342380337, Final Batch Loss: 0.001708649331703782\n",
      "Epoch 892, Loss: 0.08248018205631524, Final Batch Loss: 0.011517229489982128\n",
      "Epoch 893, Loss: 0.05395341891562566, Final Batch Loss: 0.008890043012797832\n",
      "Epoch 894, Loss: 0.04719940316863358, Final Batch Loss: 0.0007679345435462892\n",
      "Epoch 895, Loss: 0.08303166477708146, Final Batch Loss: 0.001536256750114262\n",
      "Epoch 896, Loss: 0.07500192074803635, Final Batch Loss: 0.0019018467282876372\n",
      "Epoch 897, Loss: 0.0780476849176921, Final Batch Loss: 0.02185099385678768\n",
      "Epoch 898, Loss: 0.06974043400259688, Final Batch Loss: 0.007167281117290258\n",
      "Epoch 899, Loss: 0.08041518117533997, Final Batch Loss: 0.007399705238640308\n",
      "Epoch 900, Loss: 0.08188579775742255, Final Batch Loss: 0.0035666690673679113\n",
      "Epoch 901, Loss: 0.04992596426745877, Final Batch Loss: 0.0063279471360147\n",
      "Epoch 902, Loss: 0.0878078764944803, Final Batch Loss: 0.01417712401598692\n",
      "Epoch 903, Loss: 0.07478969142539427, Final Batch Loss: 0.004087245557457209\n",
      "Epoch 904, Loss: 0.08633198801544495, Final Batch Loss: 0.008379512466490269\n",
      "Epoch 905, Loss: 0.05256725111394189, Final Batch Loss: 0.0002671034599188715\n",
      "Epoch 906, Loss: 0.07020149230083916, Final Batch Loss: 0.004240159876644611\n",
      "Epoch 907, Loss: 0.04503036121604964, Final Batch Loss: 0.0046194144524633884\n",
      "Epoch 908, Loss: 0.10932011768454686, Final Batch Loss: 0.028927192091941833\n",
      "Epoch 909, Loss: 0.14587903849314898, Final Batch Loss: 0.04389726743102074\n",
      "Epoch 910, Loss: 0.12532512596226297, Final Batch Loss: 0.004185133147984743\n",
      "Epoch 911, Loss: 0.08189944725017995, Final Batch Loss: 0.025960739701986313\n",
      "Epoch 912, Loss: 0.08451003354275599, Final Batch Loss: 0.0100011071190238\n",
      "Epoch 913, Loss: 0.059344928711652756, Final Batch Loss: 0.008758718147873878\n",
      "Epoch 914, Loss: 0.15300837997347116, Final Batch Loss: 0.0013097845949232578\n",
      "Epoch 915, Loss: 0.09609896398615092, Final Batch Loss: 0.0020699354354292154\n",
      "Epoch 916, Loss: 0.09663380333222449, Final Batch Loss: 0.011005785316228867\n",
      "Epoch 917, Loss: 0.08496961212949827, Final Batch Loss: 0.001973801990970969\n",
      "Epoch 918, Loss: 0.14729320633341558, Final Batch Loss: 0.003535958705469966\n",
      "Epoch 919, Loss: 0.15773091511800885, Final Batch Loss: 0.005650260020047426\n",
      "Epoch 920, Loss: 0.19215017231181264, Final Batch Loss: 0.035597193986177444\n",
      "Epoch 921, Loss: 0.07603631203528494, Final Batch Loss: 0.0023224963806569576\n",
      "Epoch 922, Loss: 0.1518598431139253, Final Batch Loss: 0.038144681602716446\n",
      "Epoch 923, Loss: 0.06779753317823634, Final Batch Loss: 0.0007996128406375647\n",
      "Epoch 924, Loss: 0.06416265791631304, Final Batch Loss: 0.0009566962835378945\n",
      "Epoch 925, Loss: 0.09299311460927129, Final Batch Loss: 0.004114073701202869\n",
      "Epoch 926, Loss: 0.06169292054255493, Final Batch Loss: 0.00025068430113606155\n",
      "Epoch 927, Loss: 0.059177644725423306, Final Batch Loss: 0.005981331691145897\n",
      "Epoch 928, Loss: 0.0771790133730974, Final Batch Loss: 0.0028714260552078485\n",
      "Epoch 929, Loss: 0.052149600291159004, Final Batch Loss: 0.001997237093746662\n",
      "Epoch 930, Loss: 0.07960520358756185, Final Batch Loss: 0.01300390437245369\n",
      "Epoch 931, Loss: 0.08758820535149425, Final Batch Loss: 0.00849201064556837\n",
      "Epoch 932, Loss: 0.05779037845786661, Final Batch Loss: 0.0020873560570180416\n",
      "Epoch 933, Loss: 0.03620116805541329, Final Batch Loss: 0.0014528365572914481\n",
      "Epoch 934, Loss: 0.02382549397589173, Final Batch Loss: 0.0005558684351854026\n",
      "Epoch 935, Loss: 0.07450653566047549, Final Batch Loss: 0.012970310635864735\n",
      "Epoch 936, Loss: 0.07177752297138795, Final Batch Loss: 0.000553292571566999\n",
      "Epoch 937, Loss: 0.08122235897462815, Final Batch Loss: 0.002230643993243575\n",
      "Epoch 938, Loss: 0.13875388843007386, Final Batch Loss: 0.0017953630303964019\n",
      "Epoch 939, Loss: 0.08936178886506241, Final Batch Loss: 0.015539532527327538\n",
      "Epoch 940, Loss: 0.09627092094160616, Final Batch Loss: 0.003477758029475808\n",
      "Epoch 941, Loss: 0.07225377613212913, Final Batch Loss: 0.005835819058120251\n",
      "Epoch 942, Loss: 0.07472780428361148, Final Batch Loss: 0.0037716461811214685\n",
      "Epoch 943, Loss: 0.08905367192346603, Final Batch Loss: 0.013927150517702103\n",
      "Epoch 944, Loss: 0.08382614806760103, Final Batch Loss: 0.004555474501103163\n",
      "Epoch 945, Loss: 0.05354878326761536, Final Batch Loss: 0.011125524528324604\n",
      "Epoch 946, Loss: 0.03984762435720768, Final Batch Loss: 0.0020961984992027283\n",
      "Epoch 947, Loss: 0.03324687329586595, Final Batch Loss: 0.0037666168063879013\n",
      "Epoch 948, Loss: 0.04139047194621526, Final Batch Loss: 0.0015385139267891645\n",
      "Epoch 949, Loss: 0.06487597976229154, Final Batch Loss: 0.004962192848324776\n",
      "Epoch 950, Loss: 0.041818719255388714, Final Batch Loss: 0.0008567490731365979\n",
      "Epoch 951, Loss: 0.06834331668505911, Final Batch Loss: 0.0031954199075698853\n",
      "Epoch 952, Loss: 0.045744701637886465, Final Batch Loss: 0.006486859172582626\n",
      "Epoch 953, Loss: 0.11641534851514734, Final Batch Loss: 0.0022258488461375237\n",
      "Epoch 954, Loss: 0.06644938522367738, Final Batch Loss: 0.005242904648184776\n",
      "Epoch 955, Loss: 0.10151092786691152, Final Batch Loss: 0.0014022613177075982\n",
      "Epoch 956, Loss: 0.05904311155609321, Final Batch Loss: 0.015651701018214226\n",
      "Epoch 957, Loss: 0.09977097122464329, Final Batch Loss: 0.002500753616914153\n",
      "Epoch 958, Loss: 0.07477061557437992, Final Batch Loss: 0.0019736525136977434\n",
      "Epoch 959, Loss: 0.0921090868068859, Final Batch Loss: 0.008396098390221596\n",
      "Epoch 960, Loss: 0.09512823139084503, Final Batch Loss: 0.02290787734091282\n",
      "Epoch 961, Loss: 0.09838198428042233, Final Batch Loss: 0.007617687340825796\n",
      "Epoch 962, Loss: 0.08389431113027968, Final Batch Loss: 0.023145321756601334\n",
      "Epoch 963, Loss: 0.08585054715513252, Final Batch Loss: 0.000784456729888916\n",
      "Epoch 964, Loss: 0.05220042676955927, Final Batch Loss: 0.013456838205456734\n",
      "Epoch 965, Loss: 0.06877419890952297, Final Batch Loss: 0.0006522677722387016\n",
      "Epoch 966, Loss: 0.09355613404477481, Final Batch Loss: 0.011622380465269089\n",
      "Epoch 967, Loss: 0.035909412283217534, Final Batch Loss: 0.0003266655548941344\n",
      "Epoch 968, Loss: 0.0375194864536752, Final Batch Loss: 0.0008169628563337028\n",
      "Epoch 969, Loss: 0.1474109429109376, Final Batch Loss: 0.058754075318574905\n",
      "Epoch 970, Loss: 0.09676228166790679, Final Batch Loss: 0.017483215779066086\n",
      "Epoch 971, Loss: 0.17836300213821232, Final Batch Loss: 0.012273918837308884\n",
      "Epoch 972, Loss: 0.2544382317864802, Final Batch Loss: 0.008233076892793179\n",
      "Epoch 973, Loss: 0.1581686029676348, Final Batch Loss: 0.007226152811199427\n",
      "Epoch 974, Loss: 0.09992362197954208, Final Batch Loss: 0.006494791712611914\n",
      "Epoch 975, Loss: 0.11078751925379038, Final Batch Loss: 0.0057017989456653595\n",
      "Epoch 976, Loss: 0.10812692402396351, Final Batch Loss: 0.0066401041112840176\n",
      "Epoch 977, Loss: 0.11610262910835445, Final Batch Loss: 0.006021101027727127\n",
      "Epoch 978, Loss: 0.1358650347101502, Final Batch Loss: 0.0006880228756926954\n",
      "Epoch 979, Loss: 0.11020565056242049, Final Batch Loss: 0.009546080604195595\n",
      "Epoch 980, Loss: 0.07204778864979744, Final Batch Loss: 0.006671386305242777\n",
      "Epoch 981, Loss: 0.0769958701566793, Final Batch Loss: 0.005692663136869669\n",
      "Epoch 982, Loss: 0.04465149118914269, Final Batch Loss: 0.0025981878861784935\n",
      "Epoch 983, Loss: 0.06540248583769426, Final Batch Loss: 0.002348851878196001\n",
      "Epoch 984, Loss: 0.06641413090983406, Final Batch Loss: 0.016032200306653976\n",
      "Epoch 985, Loss: 0.10897813632618636, Final Batch Loss: 0.004467553924769163\n",
      "Epoch 986, Loss: 0.07570932453381829, Final Batch Loss: 0.000270134856691584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 987, Loss: 0.055769789876649156, Final Batch Loss: 0.004398389253765345\n",
      "Epoch 988, Loss: 0.04790842247894034, Final Batch Loss: 0.0014243555488064885\n",
      "Epoch 989, Loss: 0.0725984048913233, Final Batch Loss: 0.0052698226645588875\n",
      "Epoch 990, Loss: 0.11784791824175045, Final Batch Loss: 0.0004903565859422088\n",
      "Epoch 991, Loss: 0.14862954326963518, Final Batch Loss: 0.007207825779914856\n",
      "Epoch 992, Loss: 0.04859885253245011, Final Batch Loss: 0.0006974958232603967\n",
      "Epoch 993, Loss: 0.08797011524438858, Final Batch Loss: 0.0023940273094922304\n",
      "Epoch 994, Loss: 0.11408263701014221, Final Batch Loss: 0.004241019953042269\n",
      "Epoch 995, Loss: 0.07911173620959744, Final Batch Loss: 0.0011216012062504888\n",
      "Epoch 996, Loss: 0.08795224333880469, Final Batch Loss: 0.003908333834260702\n",
      "Epoch 997, Loss: 0.07680068002082407, Final Batch Loss: 0.015587197616696358\n",
      "Epoch 998, Loss: 0.05375249605276622, Final Batch Loss: 0.003161284839734435\n",
      "Epoch 999, Loss: 0.05199203146912623, Final Batch Loss: 0.006635230500251055\n",
      "Epoch 1000, Loss: 0.04704047983977944, Final Batch Loss: 0.001897938083857298\n",
      "Epoch 1001, Loss: 0.06463457108475268, Final Batch Loss: 0.006368781439960003\n",
      "Epoch 1002, Loss: 0.08452026007580571, Final Batch Loss: 0.0026164459995925426\n",
      "Epoch 1003, Loss: 0.07740951920277439, Final Batch Loss: 0.004883781541138887\n",
      "Epoch 1004, Loss: 0.0572056011587847, Final Batch Loss: 0.0017535032238811255\n",
      "Epoch 1005, Loss: 0.07381219443050213, Final Batch Loss: 0.00499569671228528\n",
      "Epoch 1006, Loss: 0.03816241284948774, Final Batch Loss: 0.01508557703346014\n",
      "Epoch 1007, Loss: 0.046745719970203936, Final Batch Loss: 0.0036108167842030525\n",
      "Epoch 1008, Loss: 0.03126621205592528, Final Batch Loss: 0.0025556161999702454\n",
      "Epoch 1009, Loss: 0.10498943441052688, Final Batch Loss: 0.0017379196360707283\n",
      "Epoch 1010, Loss: 0.028940628384589218, Final Batch Loss: 0.0002999472781084478\n",
      "Epoch 1011, Loss: 0.03900757794326637, Final Batch Loss: 0.000485697208205238\n",
      "Epoch 1012, Loss: 0.10099183380953036, Final Batch Loss: 0.008579985238611698\n",
      "Epoch 1013, Loss: 0.05645638710484491, Final Batch Loss: 0.006792664062231779\n",
      "Epoch 1014, Loss: 0.07480619534180732, Final Batch Loss: 0.0012983588967472315\n",
      "Epoch 1015, Loss: 0.05487563507631421, Final Batch Loss: 0.004338367376476526\n",
      "Epoch 1016, Loss: 0.1270923018164467, Final Batch Loss: 0.00415123300626874\n",
      "Epoch 1017, Loss: 0.1285440887149889, Final Batch Loss: 0.001607258222065866\n",
      "Epoch 1018, Loss: 0.06629795290064067, Final Batch Loss: 0.005739804357290268\n",
      "Epoch 1019, Loss: 0.04397868895466672, Final Batch Loss: 0.0017990759806707501\n",
      "Epoch 1020, Loss: 0.08965639816597104, Final Batch Loss: 0.0015560865867882967\n",
      "Epoch 1021, Loss: 0.03796444908948615, Final Batch Loss: 0.0026314398273825645\n",
      "Epoch 1022, Loss: 0.09549762363894843, Final Batch Loss: 0.029138140380382538\n",
      "Epoch 1023, Loss: 0.14118783044978045, Final Batch Loss: 0.02498478628695011\n",
      "Epoch 1024, Loss: 0.09618793886329513, Final Batch Loss: 0.006095221731811762\n",
      "Epoch 1025, Loss: 0.09764254206675105, Final Batch Loss: 0.002249474171549082\n",
      "Epoch 1026, Loss: 0.09262393662356772, Final Batch Loss: 0.01731542870402336\n",
      "Epoch 1027, Loss: 0.04781074938364327, Final Batch Loss: 0.002560068154707551\n",
      "Epoch 1028, Loss: 0.037806042353622615, Final Batch Loss: 0.0029217104893177748\n",
      "Epoch 1029, Loss: 0.041745007678400725, Final Batch Loss: 0.0013646228471770883\n",
      "Epoch 1030, Loss: 0.07231221182155423, Final Batch Loss: 0.001648087170906365\n",
      "Epoch 1031, Loss: 0.042314992286264896, Final Batch Loss: 0.0003743084380403161\n",
      "Epoch 1032, Loss: 0.05501090190955438, Final Batch Loss: 0.008877860382199287\n",
      "Epoch 1033, Loss: 0.042293001897633076, Final Batch Loss: 0.004460288677364588\n",
      "Epoch 1034, Loss: 0.05393826897488907, Final Batch Loss: 0.014970668591558933\n",
      "Epoch 1035, Loss: 0.10434702291968279, Final Batch Loss: 0.0076442682184278965\n",
      "Epoch 1036, Loss: 0.028865006694104522, Final Batch Loss: 0.0007731038494966924\n",
      "Epoch 1037, Loss: 0.0906546870683087, Final Batch Loss: 0.0014124662848189473\n",
      "Epoch 1038, Loss: 0.09920154986320995, Final Batch Loss: 0.010023721493780613\n",
      "Epoch 1039, Loss: 0.19896362340659834, Final Batch Loss: 0.02745473012328148\n",
      "Epoch 1040, Loss: 0.12117740011308342, Final Batch Loss: 0.011370520107448101\n",
      "Epoch 1041, Loss: 0.06406026653712615, Final Batch Loss: 0.005358551628887653\n",
      "Epoch 1042, Loss: 0.052133693592622876, Final Batch Loss: 0.016195785254240036\n",
      "Epoch 1043, Loss: 0.049046913380152546, Final Batch Loss: 0.0023303984198719263\n",
      "Epoch 1044, Loss: 0.10251598288596142, Final Batch Loss: 0.0011959013063460588\n",
      "Epoch 1045, Loss: 0.13276667773607187, Final Batch Loss: 0.012973040342330933\n",
      "Epoch 1046, Loss: 0.07280900602927431, Final Batch Loss: 0.004779762588441372\n",
      "Epoch 1047, Loss: 0.05582240427611396, Final Batch Loss: 0.0012087184004485607\n",
      "Epoch 1048, Loss: 0.07107881607953459, Final Batch Loss: 0.00729437405243516\n",
      "Epoch 1049, Loss: 0.039603729848749936, Final Batch Loss: 0.000699831813108176\n",
      "Epoch 1050, Loss: 0.029122053223545663, Final Batch Loss: 0.002987438812851906\n",
      "Epoch 1051, Loss: 0.1235468726081308, Final Batch Loss: 0.0011404849356040359\n",
      "Epoch 1052, Loss: 0.08703817633795552, Final Batch Loss: 0.002371528185904026\n",
      "Epoch 1053, Loss: 0.056972578167915344, Final Batch Loss: 0.002226965269073844\n",
      "Epoch 1054, Loss: 0.07051395050075371, Final Batch Loss: 0.004970733076334\n",
      "Epoch 1055, Loss: 0.13415511150378734, Final Batch Loss: 0.009253575466573238\n",
      "Epoch 1056, Loss: 0.049631849891738966, Final Batch Loss: 0.001989613752812147\n",
      "Epoch 1057, Loss: 0.03267362047336064, Final Batch Loss: 0.0015691620064899325\n",
      "Epoch 1058, Loss: 0.025722952937940136, Final Batch Loss: 0.00036154998815618455\n",
      "Epoch 1059, Loss: 0.05132751903147437, Final Batch Loss: 0.0020522919949144125\n",
      "Epoch 1060, Loss: 0.09911774475767743, Final Batch Loss: 0.002271926263347268\n",
      "Epoch 1061, Loss: 0.09117886312742485, Final Batch Loss: 0.0033737467601895332\n",
      "Epoch 1062, Loss: 0.11323627084493637, Final Batch Loss: 0.0013378009898588061\n",
      "Epoch 1063, Loss: 0.07487877336097881, Final Batch Loss: 0.004418996162712574\n",
      "Epoch 1064, Loss: 0.10435671993764117, Final Batch Loss: 0.002855681348592043\n",
      "Epoch 1065, Loss: 0.05391607619822025, Final Batch Loss: 0.001116291736252606\n",
      "Epoch 1066, Loss: 0.07521482650190592, Final Batch Loss: 0.0006851617363281548\n",
      "Epoch 1067, Loss: 0.04913371772272512, Final Batch Loss: 0.0044585950672626495\n",
      "Epoch 1068, Loss: 0.024623308301670477, Final Batch Loss: 0.0010195680661126971\n",
      "Epoch 1069, Loss: 0.036621426610508934, Final Batch Loss: 0.0017246122006326914\n",
      "Epoch 1070, Loss: 0.09824905823916197, Final Batch Loss: 0.0016443929634988308\n",
      "Epoch 1071, Loss: 0.07485891718533821, Final Batch Loss: 0.0015916177071630955\n",
      "Epoch 1072, Loss: 0.060630134103121236, Final Batch Loss: 0.014835142530500889\n",
      "Epoch 1073, Loss: 0.06600949779385701, Final Batch Loss: 0.004601276479661465\n",
      "Epoch 1074, Loss: 0.032927030842984095, Final Batch Loss: 0.003014221554622054\n",
      "Epoch 1075, Loss: 0.04131224727461813, Final Batch Loss: 0.018352216109633446\n",
      "Epoch 1076, Loss: 0.05255094688618556, Final Batch Loss: 0.0017572634387761354\n",
      "Epoch 1077, Loss: 0.12243079958716407, Final Batch Loss: 0.010403183288872242\n",
      "Epoch 1078, Loss: 0.05275461979908869, Final Batch Loss: 0.005358836147934198\n",
      "Epoch 1079, Loss: 0.04585571424104273, Final Batch Loss: 0.0006913872202858329\n",
      "Epoch 1080, Loss: 0.04111007123719901, Final Batch Loss: 0.0009757814696058631\n",
      "Epoch 1081, Loss: 0.11989686382003129, Final Batch Loss: 0.004415401257574558\n",
      "Epoch 1082, Loss: 0.03268311175634153, Final Batch Loss: 0.00777960242703557\n",
      "Epoch 1083, Loss: 0.02947999400930712, Final Batch Loss: 0.002958071418106556\n",
      "Epoch 1084, Loss: 0.05075321671029087, Final Batch Loss: 0.0002615833654999733\n",
      "Epoch 1085, Loss: 0.06686701149737928, Final Batch Loss: 0.0016198111698031425\n",
      "Epoch 1086, Loss: 0.05750491068465635, Final Batch Loss: 0.010553094558417797\n",
      "Epoch 1087, Loss: 0.07122518197866157, Final Batch Loss: 0.0018169371178373694\n",
      "Epoch 1088, Loss: 0.055432824505260214, Final Batch Loss: 0.0006195474416017532\n",
      "Epoch 1089, Loss: 0.08974477558513172, Final Batch Loss: 0.0015833971556276083\n",
      "Epoch 1090, Loss: 0.08774333648034371, Final Batch Loss: 0.008097516372799873\n",
      "Epoch 1091, Loss: 0.0739362693966541, Final Batch Loss: 0.0019802467431873083\n",
      "Epoch 1092, Loss: 0.04146509480779059, Final Batch Loss: 0.005264466628432274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1093, Loss: 0.11994862736901268, Final Batch Loss: 0.007469893433153629\n",
      "Epoch 1094, Loss: 0.07872987541486509, Final Batch Loss: 0.001072201645001769\n",
      "Epoch 1095, Loss: 0.06484789540991187, Final Batch Loss: 0.0010742644080892205\n",
      "Epoch 1096, Loss: 0.051657240052009, Final Batch Loss: 0.0002206357748946175\n",
      "Epoch 1097, Loss: 0.08679044083692133, Final Batch Loss: 0.00046116410521790385\n",
      "Epoch 1098, Loss: 0.06464226721436717, Final Batch Loss: 0.0007692953222431242\n",
      "Epoch 1099, Loss: 0.04937289847293869, Final Batch Loss: 0.0020122353453189135\n",
      "Epoch 1100, Loss: 0.0393231043999549, Final Batch Loss: 0.0007781570893712342\n",
      "Epoch 1101, Loss: 0.04126661055488512, Final Batch Loss: 0.0024576818104833364\n",
      "Epoch 1102, Loss: 0.04567912267521024, Final Batch Loss: 0.008377635851502419\n",
      "Epoch 1103, Loss: 0.0314909942826489, Final Batch Loss: 0.0005523586296476424\n",
      "Epoch 1104, Loss: 0.034713464530796045, Final Batch Loss: 0.0015135969733819366\n",
      "Epoch 1105, Loss: 0.08691500170971267, Final Batch Loss: 0.02205735817551613\n",
      "Epoch 1106, Loss: 0.13330907990166452, Final Batch Loss: 0.003699944820255041\n",
      "Epoch 1107, Loss: 0.10750935280520935, Final Batch Loss: 0.0011399069335311651\n",
      "Epoch 1108, Loss: 0.04554048928548582, Final Batch Loss: 0.0011298430617898703\n",
      "Epoch 1109, Loss: 0.09817621315596625, Final Batch Loss: 0.004576345440000296\n",
      "Epoch 1110, Loss: 0.10189989424543455, Final Batch Loss: 0.0022482480853796005\n",
      "Epoch 1111, Loss: 0.050760354060912505, Final Batch Loss: 0.0009313355549238622\n",
      "Epoch 1112, Loss: 0.057820060930680484, Final Batch Loss: 0.016205715015530586\n",
      "Epoch 1113, Loss: 0.10132601521763718, Final Batch Loss: 0.00027877415413968265\n",
      "Epoch 1114, Loss: 0.08032142614683835, Final Batch Loss: 0.0006839946727268398\n",
      "Epoch 1115, Loss: 0.0736379106529057, Final Batch Loss: 0.008847364224493504\n",
      "Epoch 1116, Loss: 0.03577767818933353, Final Batch Loss: 0.0013260765699669719\n",
      "Epoch 1117, Loss: 0.04526163308764808, Final Batch Loss: 0.002631146227940917\n",
      "Epoch 1118, Loss: 0.04179360959096812, Final Batch Loss: 0.0049041626043617725\n",
      "Epoch 1119, Loss: 0.05775635024474468, Final Batch Loss: 0.004861801862716675\n",
      "Epoch 1120, Loss: 0.02022959569876548, Final Batch Loss: 0.0011477421503514051\n",
      "Epoch 1121, Loss: 0.04683796075551072, Final Batch Loss: 0.00015657856420148164\n",
      "Epoch 1122, Loss: 0.03669932320190128, Final Batch Loss: 0.0021034677047282457\n",
      "Epoch 1123, Loss: 0.04410557053051889, Final Batch Loss: 0.004527419339865446\n",
      "Epoch 1124, Loss: 0.02435241805505939, Final Batch Loss: 0.00329382810741663\n",
      "Epoch 1125, Loss: 0.06974563575931825, Final Batch Loss: 0.0015704453689977527\n",
      "Epoch 1126, Loss: 0.04336757653800305, Final Batch Loss: 0.0001900600764201954\n",
      "Epoch 1127, Loss: 0.10304475680459291, Final Batch Loss: 0.011504274792969227\n",
      "Epoch 1128, Loss: 0.1123603771411581, Final Batch Loss: 0.005392939783632755\n",
      "Epoch 1129, Loss: 0.09030728008656297, Final Batch Loss: 0.0019579152576625347\n",
      "Epoch 1130, Loss: 0.08027115828008391, Final Batch Loss: 0.005174850579351187\n",
      "Epoch 1131, Loss: 0.0659792756778188, Final Batch Loss: 0.0006635917234234512\n",
      "Epoch 1132, Loss: 0.04128059594950173, Final Batch Loss: 0.004694812465459108\n",
      "Epoch 1133, Loss: 0.06986005480575841, Final Batch Loss: 0.00016302075528074056\n",
      "Epoch 1134, Loss: 0.11534932821086841, Final Batch Loss: 0.03355740010738373\n",
      "Epoch 1135, Loss: 0.06470048008486629, Final Batch Loss: 0.0020210626535117626\n",
      "Epoch 1136, Loss: 0.0694849633728154, Final Batch Loss: 0.0007876101299189031\n",
      "Epoch 1137, Loss: 0.07933986189891584, Final Batch Loss: 0.008705484680831432\n",
      "Epoch 1138, Loss: 0.07176350118243136, Final Batch Loss: 0.008105815388262272\n",
      "Epoch 1139, Loss: 0.03716715465998277, Final Batch Loss: 0.002108089393004775\n",
      "Epoch 1140, Loss: 0.029873025036067702, Final Batch Loss: 0.0009238175116479397\n",
      "Epoch 1141, Loss: 0.046868720739439595, Final Batch Loss: 0.0046252356842160225\n",
      "Epoch 1142, Loss: 0.06664622723474167, Final Batch Loss: 0.0010057027684524655\n",
      "Epoch 1143, Loss: 0.040743111370829865, Final Batch Loss: 0.00011026908759959042\n",
      "Epoch 1144, Loss: 0.050376747516565956, Final Batch Loss: 0.007575016934424639\n",
      "Epoch 1145, Loss: 0.1504360515391454, Final Batch Loss: 0.0027903676964342594\n",
      "Epoch 1146, Loss: 0.06074880369124003, Final Batch Loss: 0.0029796850867569447\n",
      "Epoch 1147, Loss: 0.09286462818272412, Final Batch Loss: 0.018913041800260544\n",
      "Epoch 1148, Loss: 0.129542792274151, Final Batch Loss: 0.0033001224510371685\n",
      "Epoch 1149, Loss: 0.13228976703976514, Final Batch Loss: 5.099749978398904e-05\n",
      "Epoch 1150, Loss: 0.10271695439587347, Final Batch Loss: 0.0036222110502421856\n",
      "Epoch 1151, Loss: 0.039234850752109196, Final Batch Loss: 0.0009436428663320839\n",
      "Epoch 1152, Loss: 0.11171388841466978, Final Batch Loss: 0.007609729655086994\n",
      "Epoch 1153, Loss: 0.05706701590679586, Final Batch Loss: 0.0032372442074120045\n",
      "Epoch 1154, Loss: 0.05661855384823866, Final Batch Loss: 0.008638609200716019\n",
      "Epoch 1155, Loss: 0.04632027605839539, Final Batch Loss: 0.0011027681175619364\n",
      "Epoch 1156, Loss: 0.028166935706394725, Final Batch Loss: 0.0003015822439920157\n",
      "Epoch 1157, Loss: 0.06427822398109129, Final Batch Loss: 0.0014657389838248491\n",
      "Epoch 1158, Loss: 0.07205369525763672, Final Batch Loss: 0.004470553249120712\n",
      "Epoch 1159, Loss: 0.08157776629377622, Final Batch Loss: 0.0031252456828951836\n",
      "Epoch 1160, Loss: 0.1322887894930318, Final Batch Loss: 0.004145663231611252\n",
      "Epoch 1161, Loss: 0.10448422489571385, Final Batch Loss: 0.00025848738732747734\n",
      "Epoch 1162, Loss: 0.07548865821445361, Final Batch Loss: 0.0027035134844481945\n",
      "Epoch 1163, Loss: 0.17202351996093057, Final Batch Loss: 0.012712331488728523\n",
      "Epoch 1164, Loss: 0.05274086823919788, Final Batch Loss: 0.0006287997821345925\n",
      "Epoch 1165, Loss: 0.05751510533809778, Final Batch Loss: 0.0023860333021730185\n",
      "Epoch 1166, Loss: 0.06907773387501948, Final Batch Loss: 0.0066195460967719555\n",
      "Epoch 1167, Loss: 0.07511074464127887, Final Batch Loss: 0.00033031273051165044\n",
      "Epoch 1168, Loss: 0.06923385367554147, Final Batch Loss: 0.0032484112307429314\n",
      "Epoch 1169, Loss: 0.04492495031445287, Final Batch Loss: 0.016505908221006393\n",
      "Epoch 1170, Loss: 0.06893408542964607, Final Batch Loss: 0.016992297023534775\n",
      "Epoch 1171, Loss: 0.034245645758346654, Final Batch Loss: 0.0009212535223923624\n",
      "Epoch 1172, Loss: 0.034518493310315534, Final Batch Loss: 0.0015976232243701816\n",
      "Epoch 1173, Loss: 0.04309924802510068, Final Batch Loss: 0.005048485938459635\n",
      "Epoch 1174, Loss: 0.03196449803726864, Final Batch Loss: 0.00011295093281660229\n",
      "Epoch 1175, Loss: 0.03739102921099402, Final Batch Loss: 0.0002615767589304596\n",
      "Epoch 1176, Loss: 0.033267146987782326, Final Batch Loss: 0.003151574404910207\n",
      "Epoch 1177, Loss: 0.06955003505572677, Final Batch Loss: 0.0007142177782952785\n",
      "Epoch 1178, Loss: 0.05299963240031502, Final Batch Loss: 0.002391533926129341\n",
      "Epoch 1179, Loss: 0.05473285971675068, Final Batch Loss: 0.0039425985887646675\n",
      "Epoch 1180, Loss: 0.05297706611600006, Final Batch Loss: 0.0015494163380935788\n",
      "Epoch 1181, Loss: 0.04504098270263057, Final Batch Loss: 0.004634479060769081\n",
      "Epoch 1182, Loss: 0.04715984060021583, Final Batch Loss: 0.00018871358770411462\n",
      "Epoch 1183, Loss: 0.07984203282103408, Final Batch Loss: 0.00017684967315290123\n",
      "Epoch 1184, Loss: 0.10056863256613724, Final Batch Loss: 0.0033298719208687544\n",
      "Epoch 1185, Loss: 0.08216496439126786, Final Batch Loss: 0.007832512259483337\n",
      "Epoch 1186, Loss: 0.10630448837764561, Final Batch Loss: 0.0010532712331041694\n",
      "Epoch 1187, Loss: 0.05140462137933355, Final Batch Loss: 0.0003395297098904848\n",
      "Epoch 1188, Loss: 0.06942299983347766, Final Batch Loss: 0.004206829704344273\n",
      "Epoch 1189, Loss: 0.06435123906703666, Final Batch Loss: 0.0013091154396533966\n",
      "Epoch 1190, Loss: 0.05057761611533351, Final Batch Loss: 0.0050003114156425\n",
      "Epoch 1191, Loss: 0.08976203660131432, Final Batch Loss: 0.012787668965756893\n",
      "Epoch 1192, Loss: 0.11159488491830416, Final Batch Loss: 0.002089955611154437\n",
      "Epoch 1193, Loss: 0.1236726610804908, Final Batch Loss: 0.0013395450077950954\n",
      "Epoch 1194, Loss: 0.07220400049118325, Final Batch Loss: 0.0019982117228209972\n",
      "Epoch 1195, Loss: 0.09828610546537675, Final Batch Loss: 0.001030772808007896\n",
      "Epoch 1196, Loss: 0.07431325397919863, Final Batch Loss: 0.0015903515741229057\n",
      "Epoch 1197, Loss: 0.06166506512090564, Final Batch Loss: 0.0008471562759950757\n",
      "Epoch 1198, Loss: 0.08455907128518447, Final Batch Loss: 0.0018321290845051408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1199, Loss: 0.05511201804620214, Final Batch Loss: 0.000749767292290926\n",
      "Epoch 1200, Loss: 0.04711804608814418, Final Batch Loss: 0.002846665447577834\n",
      "Epoch 1201, Loss: 0.04552806989522651, Final Batch Loss: 0.004469409119337797\n",
      "Epoch 1202, Loss: 0.047475905739702284, Final Batch Loss: 0.0009793235221877694\n",
      "Epoch 1203, Loss: 0.07560782041400671, Final Batch Loss: 0.004135360941290855\n",
      "Epoch 1204, Loss: 0.0426930527028162, Final Batch Loss: 0.0019023246131837368\n",
      "Epoch 1205, Loss: 0.04093371815542923, Final Batch Loss: 0.019950218498706818\n",
      "Epoch 1206, Loss: 0.07231364425388165, Final Batch Loss: 0.0006166587700136006\n",
      "Epoch 1207, Loss: 0.021211025828961283, Final Batch Loss: 0.0007187497103586793\n",
      "Epoch 1208, Loss: 0.022998274816927733, Final Batch Loss: 0.0014707747614011168\n",
      "Epoch 1209, Loss: 0.050769458553986624, Final Batch Loss: 0.002634966280311346\n",
      "Epoch 1210, Loss: 0.060421211550419684, Final Batch Loss: 0.0006174236768856645\n",
      "Epoch 1211, Loss: 0.07688715035328642, Final Batch Loss: 0.00015558526501990855\n",
      "Epoch 1212, Loss: 0.05678994845948182, Final Batch Loss: 0.0012607126263901591\n",
      "Epoch 1213, Loss: 0.028283285413635895, Final Batch Loss: 0.00034043710911646485\n",
      "Epoch 1214, Loss: 0.0815138451725943, Final Batch Loss: 0.005728061310946941\n",
      "Epoch 1215, Loss: 0.02877786428143736, Final Batch Loss: 0.0018878633854910731\n",
      "Epoch 1216, Loss: 0.04988065741781611, Final Batch Loss: 0.0005779498023912311\n",
      "Epoch 1217, Loss: 0.02798400366009446, Final Batch Loss: 0.0019400137243792415\n",
      "Epoch 1218, Loss: 0.027333176436513895, Final Batch Loss: 0.0002448026498313993\n",
      "Epoch 1219, Loss: 0.07333226373157231, Final Batch Loss: 0.0006237272173166275\n",
      "Epoch 1220, Loss: 0.13278850220376626, Final Batch Loss: 0.0067508951760828495\n",
      "Epoch 1221, Loss: 0.10269540386434528, Final Batch Loss: 0.0039278180338442326\n",
      "Epoch 1222, Loss: 0.08350063511170447, Final Batch Loss: 0.0008710967958904803\n",
      "Epoch 1223, Loss: 0.05299576700781472, Final Batch Loss: 0.007228259462863207\n",
      "Epoch 1224, Loss: 0.04859204495733138, Final Batch Loss: 0.001209993613883853\n",
      "Epoch 1225, Loss: 0.03619256509409752, Final Batch Loss: 0.0007208561291918159\n",
      "Epoch 1226, Loss: 0.04039148864103481, Final Batch Loss: 0.0017142684664577246\n",
      "Epoch 1227, Loss: 0.023037372280668933, Final Batch Loss: 0.0002449697640258819\n",
      "Epoch 1228, Loss: 0.03122618919587694, Final Batch Loss: 0.00783836655318737\n",
      "Epoch 1229, Loss: 0.074762665804883, Final Batch Loss: 0.019394371658563614\n",
      "Epoch 1230, Loss: 0.032227712697931565, Final Batch Loss: 0.00875745341181755\n",
      "Epoch 1231, Loss: 0.05373660936311353, Final Batch Loss: 0.0013700519921258092\n",
      "Epoch 1232, Loss: 0.08534967976811458, Final Batch Loss: 0.00917021743953228\n",
      "Epoch 1233, Loss: 0.06162963610404404, Final Batch Loss: 0.002241768641397357\n",
      "Epoch 1234, Loss: 0.0557442862627795, Final Batch Loss: 0.0038675363175570965\n",
      "Epoch 1235, Loss: 0.04887694953504251, Final Batch Loss: 0.0022458131425082684\n",
      "Epoch 1236, Loss: 0.06328633084194735, Final Batch Loss: 0.0003492992545943707\n",
      "Epoch 1237, Loss: 0.03855553481116658, Final Batch Loss: 0.002453232416883111\n",
      "Epoch 1238, Loss: 0.046452969923848286, Final Batch Loss: 0.002619602484628558\n",
      "Epoch 1239, Loss: 0.03366769287822535, Final Batch Loss: 0.0006048026843927801\n",
      "Epoch 1240, Loss: 0.09390327209257521, Final Batch Loss: 0.0011950883781537414\n",
      "Epoch 1241, Loss: 0.03737891732089338, Final Batch Loss: 0.000366443331586197\n",
      "Epoch 1242, Loss: 0.023191630505607463, Final Batch Loss: 0.0032833744771778584\n",
      "Epoch 1243, Loss: 0.06619718440924771, Final Batch Loss: 0.00011255004210397601\n",
      "Epoch 1244, Loss: 0.05113929192884825, Final Batch Loss: 0.0008366504334844649\n",
      "Epoch 1245, Loss: 0.03457322945178021, Final Batch Loss: 0.0003109352255705744\n",
      "Epoch 1246, Loss: 0.04283361781563144, Final Batch Loss: 0.006373628042638302\n",
      "Epoch 1247, Loss: 0.046415445569437, Final Batch Loss: 0.0003378715773578733\n",
      "Epoch 1248, Loss: 0.03171545649820473, Final Batch Loss: 0.003912727814167738\n",
      "Epoch 1249, Loss: 0.015536551778495777, Final Batch Loss: 0.00018240406643599272\n",
      "Epoch 1250, Loss: 0.08343711301131407, Final Batch Loss: 0.003842476289719343\n",
      "Epoch 1251, Loss: 0.0833954424233525, Final Batch Loss: 0.0012829757761210203\n",
      "Epoch 1252, Loss: 0.14367876315372996, Final Batch Loss: 0.015207505784928799\n",
      "Epoch 1253, Loss: 0.08794525027042255, Final Batch Loss: 0.05041022226214409\n",
      "Epoch 1254, Loss: 0.09546647680690512, Final Batch Loss: 0.003096506930887699\n",
      "Epoch 1255, Loss: 0.045598820433951914, Final Batch Loss: 0.008615994825959206\n",
      "Epoch 1256, Loss: 0.05345185000624042, Final Batch Loss: 0.00031070286058820784\n",
      "Epoch 1257, Loss: 0.05243601289112121, Final Batch Loss: 0.009315826930105686\n",
      "Epoch 1258, Loss: 0.1455859143898124, Final Batch Loss: 0.006550576537847519\n",
      "Epoch 1259, Loss: 0.19575782731408253, Final Batch Loss: 0.0003502040635794401\n",
      "Epoch 1260, Loss: 0.05590363417286426, Final Batch Loss: 0.006752661895006895\n",
      "Epoch 1261, Loss: 0.0536878383718431, Final Batch Loss: 0.007815264165401459\n",
      "Epoch 1262, Loss: 0.05898498877650127, Final Batch Loss: 0.0006512808613479137\n",
      "Epoch 1263, Loss: 0.07461097027407959, Final Batch Loss: 0.005404687486588955\n",
      "Epoch 1264, Loss: 0.059541806287597865, Final Batch Loss: 0.0014483380364254117\n",
      "Epoch 1265, Loss: 0.07812204651418142, Final Batch Loss: 0.0020580796990543604\n",
      "Epoch 1266, Loss: 0.061126591463107616, Final Batch Loss: 0.006874917075037956\n",
      "Epoch 1267, Loss: 0.05672828855313128, Final Batch Loss: 0.001476552220992744\n",
      "Epoch 1268, Loss: 0.15745716716628522, Final Batch Loss: 0.0014417789643630385\n",
      "Epoch 1269, Loss: 0.05034891460672952, Final Batch Loss: 0.0022304137237370014\n",
      "Epoch 1270, Loss: 0.03122954508580733, Final Batch Loss: 0.000510569429025054\n",
      "Epoch 1271, Loss: 0.027283874835120514, Final Batch Loss: 0.0010214295471087098\n",
      "Epoch 1272, Loss: 0.0922742533730343, Final Batch Loss: 0.0033989448565989733\n",
      "Epoch 1273, Loss: 0.09751940931892022, Final Batch Loss: 0.0021116072311997414\n",
      "Epoch 1274, Loss: 0.07526735382270999, Final Batch Loss: 0.02404390461742878\n",
      "Epoch 1275, Loss: 0.07406571286264807, Final Batch Loss: 0.005644452758133411\n",
      "Epoch 1276, Loss: 0.06900098774349317, Final Batch Loss: 0.0007943816599436104\n",
      "Epoch 1277, Loss: 0.10660785823711194, Final Batch Loss: 0.01725527085363865\n",
      "Epoch 1278, Loss: 0.05695553768600803, Final Batch Loss: 0.003992458805441856\n",
      "Epoch 1279, Loss: 0.043642574310069904, Final Batch Loss: 0.003217324148863554\n",
      "Epoch 1280, Loss: 0.04458571022405522, Final Batch Loss: 6.842935545137152e-05\n",
      "Epoch 1281, Loss: 0.027004873467376456, Final Batch Loss: 0.0006647963309660554\n",
      "Epoch 1282, Loss: 0.04563292947932496, Final Batch Loss: 8.490881737088785e-05\n",
      "Epoch 1283, Loss: 0.05595199574599974, Final Batch Loss: 0.00048521943972446024\n",
      "Epoch 1284, Loss: 0.09754547165357508, Final Batch Loss: 0.0002681076293811202\n",
      "Epoch 1285, Loss: 0.04100573970936239, Final Batch Loss: 0.0008144069579429924\n",
      "Epoch 1286, Loss: 0.05190371788921766, Final Batch Loss: 0.0016495328163728118\n",
      "Epoch 1287, Loss: 0.027459032251499593, Final Batch Loss: 0.001779958140105009\n",
      "Epoch 1288, Loss: 0.0460755037056515, Final Batch Loss: 0.0027401025872677565\n",
      "Epoch 1289, Loss: 0.020309785482822917, Final Batch Loss: 0.00023269928351510316\n",
      "Epoch 1290, Loss: 0.045519691844674526, Final Batch Loss: 0.00012367426825221628\n",
      "Epoch 1291, Loss: 0.05140517567633651, Final Batch Loss: 0.005068088881671429\n",
      "Epoch 1292, Loss: 0.09802778720040806, Final Batch Loss: 0.00021639576880261302\n",
      "Epoch 1293, Loss: 0.0722186844504904, Final Batch Loss: 0.005830456502735615\n",
      "Epoch 1294, Loss: 0.08740856175427325, Final Batch Loss: 0.0026572977658361197\n",
      "Epoch 1295, Loss: 0.059314418176654726, Final Batch Loss: 0.001541761215776205\n",
      "Epoch 1296, Loss: 0.04983267611532938, Final Batch Loss: 0.002738810610026121\n",
      "Epoch 1297, Loss: 0.0661112999659963, Final Batch Loss: 0.0009228932904079556\n",
      "Epoch 1298, Loss: 0.04743661830434576, Final Batch Loss: 0.0010513446759432554\n",
      "Epoch 1299, Loss: 0.05294574027357157, Final Batch Loss: 0.005136324092745781\n",
      "Epoch 1300, Loss: 0.07184083829633892, Final Batch Loss: 0.0022007839288562536\n",
      "Epoch 1301, Loss: 0.05929405428469181, Final Batch Loss: 0.0006842984585091472\n",
      "Epoch 1302, Loss: 0.028893645423522685, Final Batch Loss: 0.0013339364668354392\n",
      "Epoch 1303, Loss: 0.021541743590205442, Final Batch Loss: 7.084263052092865e-05\n",
      "Epoch 1304, Loss: 0.023885870752565097, Final Batch Loss: 0.0003173569275531918\n",
      "Epoch 1305, Loss: 0.054622155977995135, Final Batch Loss: 0.003669208148494363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1306, Loss: 0.02870838195667602, Final Batch Loss: 0.0032875221222639084\n",
      "Epoch 1307, Loss: 0.02958161520655267, Final Batch Loss: 0.0007278304547071457\n",
      "Epoch 1308, Loss: 0.030891036614775658, Final Batch Loss: 0.00019667911692522466\n",
      "Epoch 1309, Loss: 0.017301678879448446, Final Batch Loss: 0.002457135822623968\n",
      "Epoch 1310, Loss: 0.031121185773372417, Final Batch Loss: 0.0011404285905882716\n",
      "Epoch 1311, Loss: 0.09543650084015098, Final Batch Loss: 0.0033251619897782803\n",
      "Epoch 1312, Loss: 0.1715843988204142, Final Batch Loss: 0.0054483539424836636\n",
      "Epoch 1313, Loss: 0.11926498638786143, Final Batch Loss: 0.0011458551743999124\n",
      "Epoch 1314, Loss: 0.0506441623729188, Final Batch Loss: 0.0011449767043814063\n",
      "Epoch 1315, Loss: 0.07483006405527703, Final Batch Loss: 0.0006957401055842638\n",
      "Epoch 1316, Loss: 0.06016352231381461, Final Batch Loss: 0.0359659269452095\n",
      "Epoch 1317, Loss: 0.05355992945260368, Final Batch Loss: 0.001567524392157793\n",
      "Epoch 1318, Loss: 0.03931308801838895, Final Batch Loss: 0.0011673597618937492\n",
      "Epoch 1319, Loss: 0.030663999837997835, Final Batch Loss: 0.00020638217392843217\n",
      "Epoch 1320, Loss: 0.04605996403552126, Final Batch Loss: 0.00021095613192301244\n",
      "Epoch 1321, Loss: 0.029745972322416492, Final Batch Loss: 0.0006625415408052504\n",
      "Epoch 1322, Loss: 0.03204224914225051, Final Batch Loss: 5.898054951103404e-05\n",
      "Epoch 1323, Loss: 0.02244931361201452, Final Batch Loss: 0.0016067237593233585\n",
      "Epoch 1324, Loss: 0.09643740634783171, Final Batch Loss: 0.00046884972834959626\n",
      "Epoch 1325, Loss: 0.045510024661780335, Final Batch Loss: 0.003101811045780778\n",
      "Epoch 1326, Loss: 0.011078188617830165, Final Batch Loss: 8.192696259357035e-05\n",
      "Epoch 1327, Loss: 0.0798076658975333, Final Batch Loss: 0.000879292783793062\n",
      "Epoch 1328, Loss: 0.05452997719112318, Final Batch Loss: 0.014197754673659801\n",
      "Epoch 1329, Loss: 0.03080843877251027, Final Batch Loss: 8.788570266915485e-05\n",
      "Epoch 1330, Loss: 0.04217740791500546, Final Batch Loss: 0.00035842604120261967\n",
      "Epoch 1331, Loss: 0.06684337789192796, Final Batch Loss: 0.00815282016992569\n",
      "Epoch 1332, Loss: 0.05917077403864823, Final Batch Loss: 0.005569932051002979\n",
      "Epoch 1333, Loss: 0.077403488161508, Final Batch Loss: 0.0010701316641643643\n",
      "Epoch 1334, Loss: 0.048997704339853954, Final Batch Loss: 0.0006160691846162081\n",
      "Epoch 1335, Loss: 0.06453481133576133, Final Batch Loss: 0.0021837675012648106\n",
      "Epoch 1336, Loss: 0.026819503804290434, Final Batch Loss: 0.0009174435399472713\n",
      "Epoch 1337, Loss: 0.02114306003204547, Final Batch Loss: 0.00022920157061889768\n",
      "Epoch 1338, Loss: 0.03564353067486081, Final Batch Loss: 0.0012182905338704586\n",
      "Epoch 1339, Loss: 0.06105110695352778, Final Batch Loss: 0.0015171768609434366\n",
      "Epoch 1340, Loss: 0.03501558006973937, Final Batch Loss: 0.009668329730629921\n",
      "Epoch 1341, Loss: 0.06319326984521467, Final Batch Loss: 0.03380538895726204\n",
      "Epoch 1342, Loss: 0.3348287447588518, Final Batch Loss: 0.002452248241752386\n",
      "Epoch 1343, Loss: 0.2099228995211888, Final Batch Loss: 0.023313768208026886\n",
      "Epoch 1344, Loss: 0.1082236608635867, Final Batch Loss: 0.004763016011565924\n",
      "Epoch 1345, Loss: 0.07929335907101631, Final Batch Loss: 0.005553457420319319\n",
      "Epoch 1346, Loss: 0.08136147991172038, Final Batch Loss: 0.0003887327329721302\n",
      "Epoch 1347, Loss: 0.046123036212520674, Final Batch Loss: 0.00030227910610847175\n",
      "Epoch 1348, Loss: 0.058701234782347456, Final Batch Loss: 0.023760570213198662\n",
      "Epoch 1349, Loss: 0.0483352379524149, Final Batch Loss: 0.0004848690878134221\n",
      "Epoch 1350, Loss: 0.056035449204500765, Final Batch Loss: 0.00471156882122159\n",
      "Epoch 1351, Loss: 0.054761399660492316, Final Batch Loss: 0.022178856655955315\n",
      "Epoch 1352, Loss: 0.06013308998080902, Final Batch Loss: 0.005813133902847767\n",
      "Epoch 1353, Loss: 0.047015693242428824, Final Batch Loss: 0.0044610328041017056\n",
      "Epoch 1354, Loss: 0.04939013672264991, Final Batch Loss: 0.002210306702181697\n",
      "Epoch 1355, Loss: 0.04269956212374382, Final Batch Loss: 0.0008954532677307725\n",
      "Epoch 1356, Loss: 0.06083452879101969, Final Batch Loss: 0.0007671455387026072\n",
      "Epoch 1357, Loss: 0.08270654482475948, Final Batch Loss: 0.0002221501781605184\n",
      "Epoch 1358, Loss: 0.042636239442799706, Final Batch Loss: 0.002688939915969968\n",
      "Epoch 1359, Loss: 0.046766505809500813, Final Batch Loss: 0.014200408011674881\n",
      "Epoch 1360, Loss: 0.02155013739684364, Final Batch Loss: 0.0029567163437604904\n",
      "Epoch 1361, Loss: 0.04196502961713122, Final Batch Loss: 3.9276528696063906e-05\n",
      "Epoch 1362, Loss: 0.026519555380218662, Final Batch Loss: 0.004275474697351456\n",
      "Epoch 1363, Loss: 0.03923063660477055, Final Batch Loss: 0.01587424799799919\n",
      "Epoch 1364, Loss: 0.06660003087745281, Final Batch Loss: 0.003898471826687455\n",
      "Epoch 1365, Loss: 0.17098589846864343, Final Batch Loss: 0.0479700043797493\n",
      "Epoch 1366, Loss: 0.12271233073261101, Final Batch Loss: 0.00033383400295861065\n",
      "Epoch 1367, Loss: 0.11106681011733599, Final Batch Loss: 0.016266806051135063\n",
      "Epoch 1368, Loss: 0.22545709856785834, Final Batch Loss: 0.009431185200810432\n",
      "Epoch 1369, Loss: 0.13366854478954338, Final Batch Loss: 0.005863585975021124\n",
      "Epoch 1370, Loss: 0.10232544812606648, Final Batch Loss: 0.0031598838977515697\n",
      "Epoch 1371, Loss: 0.029859174363082275, Final Batch Loss: 0.00326463277451694\n",
      "Epoch 1372, Loss: 0.1066311635368038, Final Batch Loss: 0.0020209637004882097\n",
      "Epoch 1373, Loss: 0.09212348237633705, Final Batch Loss: 0.006739490665495396\n",
      "Epoch 1374, Loss: 0.1117058977833949, Final Batch Loss: 0.04009800776839256\n",
      "Epoch 1375, Loss: 0.06389383223722689, Final Batch Loss: 0.000495821819640696\n",
      "Epoch 1376, Loss: 0.046910020617360715, Final Batch Loss: 0.0031167513225227594\n",
      "Epoch 1377, Loss: 0.04694753534568008, Final Batch Loss: 0.0001454335724702105\n",
      "Epoch 1378, Loss: 0.034208607859909534, Final Batch Loss: 0.0005851381574757397\n",
      "Epoch 1379, Loss: 0.04040874360362068, Final Batch Loss: 0.0003215952019672841\n",
      "Epoch 1380, Loss: 0.056268041487783194, Final Batch Loss: 0.003385871183127165\n",
      "Epoch 1381, Loss: 0.030223803274566308, Final Batch Loss: 0.003880911273881793\n",
      "Epoch 1382, Loss: 0.05505496478872374, Final Batch Loss: 0.00015546564827673137\n",
      "Epoch 1383, Loss: 0.04673376327264123, Final Batch Loss: 0.0008953826618380845\n",
      "Epoch 1384, Loss: 0.013107091363053769, Final Batch Loss: 0.001274025533348322\n",
      "Epoch 1385, Loss: 0.04492570580623578, Final Batch Loss: 0.00041783417691476643\n",
      "Epoch 1386, Loss: 0.05139556280482793, Final Batch Loss: 0.005505586043000221\n",
      "Epoch 1387, Loss: 0.0724351319076959, Final Batch Loss: 0.002667903434485197\n",
      "Epoch 1388, Loss: 0.05072141101118177, Final Batch Loss: 0.009868631139397621\n",
      "Epoch 1389, Loss: 0.031282150695915334, Final Batch Loss: 0.002599897561594844\n",
      "Epoch 1390, Loss: 0.04553041132749058, Final Batch Loss: 0.003559581935405731\n",
      "Epoch 1391, Loss: 0.027588487617322244, Final Batch Loss: 0.0020968958269804716\n",
      "Epoch 1392, Loss: 0.02637683306238614, Final Batch Loss: 0.0013626629952341318\n",
      "Epoch 1393, Loss: 0.02120856624969747, Final Batch Loss: 0.001042160321958363\n",
      "Epoch 1394, Loss: 0.0300601053313585, Final Batch Loss: 5.358853013603948e-05\n",
      "Epoch 1395, Loss: 0.031602070754161105, Final Batch Loss: 0.00031551558640785515\n",
      "Epoch 1396, Loss: 0.02963271154658287, Final Batch Loss: 0.002557486528530717\n",
      "Epoch 1397, Loss: 0.05872684268979356, Final Batch Loss: 0.0031859518494457006\n",
      "Epoch 1398, Loss: 0.02200956654996844, Final Batch Loss: 0.0027696052566170692\n",
      "Epoch 1399, Loss: 0.06091942774310155, Final Batch Loss: 0.0014562695287168026\n",
      "Epoch 1400, Loss: 0.04475355628528632, Final Batch Loss: 0.0038438928313553333\n",
      "Epoch 1401, Loss: 0.08245164810796268, Final Batch Loss: 0.003035071073099971\n",
      "Epoch 1402, Loss: 0.04237749014282599, Final Batch Loss: 0.0026432161685079336\n",
      "Epoch 1403, Loss: 0.07098277077602688, Final Batch Loss: 0.012839611619710922\n",
      "Epoch 1404, Loss: 0.03130509040784091, Final Batch Loss: 0.0001487446133978665\n",
      "Epoch 1405, Loss: 0.017241286019270774, Final Batch Loss: 0.001696702092885971\n",
      "Epoch 1406, Loss: 0.08214860485531972, Final Batch Loss: 0.0041387868113815784\n",
      "Epoch 1407, Loss: 0.0767098976939451, Final Batch Loss: 0.0019609162118285894\n",
      "Epoch 1408, Loss: 0.03659331202652538, Final Batch Loss: 0.010387696325778961\n",
      "Epoch 1409, Loss: 0.028901520319777774, Final Batch Loss: 0.0013635692885145545\n",
      "Epoch 1410, Loss: 0.0461380862070655, Final Batch Loss: 0.0003345902659930289\n",
      "Epoch 1411, Loss: 0.070363639290008, Final Batch Loss: 0.0031727964524179697\n",
      "Epoch 1412, Loss: 0.03970230861887103, Final Batch Loss: 0.0006503518088720739\n",
      "Epoch 1413, Loss: 0.04518689873657422, Final Batch Loss: 0.0004397683951538056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1414, Loss: 0.04953619600564707, Final Batch Loss: 0.002161456970497966\n",
      "Epoch 1415, Loss: 0.09417233767817379, Final Batch Loss: 0.003512423951178789\n",
      "Epoch 1416, Loss: 0.02561581474583363, Final Batch Loss: 0.0009433659724891186\n",
      "Epoch 1417, Loss: 0.029153554627555422, Final Batch Loss: 0.001350997481495142\n",
      "Epoch 1418, Loss: 0.04292263450770406, Final Batch Loss: 0.0007082577794790268\n",
      "Epoch 1419, Loss: 0.03842211792289163, Final Batch Loss: 0.0006291078170761466\n",
      "Epoch 1420, Loss: 0.09164335078094155, Final Batch Loss: 0.0006778387469239533\n",
      "Epoch 1421, Loss: 0.07768974819191499, Final Batch Loss: 0.0011365206446498632\n",
      "Epoch 1422, Loss: 0.0567062103364151, Final Batch Loss: 0.011924191378057003\n",
      "Epoch 1423, Loss: 0.0400725208746735, Final Batch Loss: 0.0029546699952334166\n",
      "Epoch 1424, Loss: 0.05850657158589456, Final Batch Loss: 0.002258541062474251\n",
      "Epoch 1425, Loss: 0.025759088035556488, Final Batch Loss: 8.813907334115356e-05\n",
      "Epoch 1426, Loss: 0.05549294743104838, Final Batch Loss: 0.012046006508171558\n",
      "Epoch 1427, Loss: 0.044213026769284625, Final Batch Loss: 0.004560867324471474\n",
      "Epoch 1428, Loss: 0.06602484857285162, Final Batch Loss: 0.0016641287365928292\n",
      "Epoch 1429, Loss: 0.06126253404363524, Final Batch Loss: 8.472077752230689e-05\n",
      "Epoch 1430, Loss: 0.04031733371084556, Final Batch Loss: 0.0008238967275246978\n",
      "Epoch 1431, Loss: 0.05034348506887909, Final Batch Loss: 0.00018073343380820006\n",
      "Epoch 1432, Loss: 0.035178184323740425, Final Batch Loss: 0.004667588975280523\n",
      "Epoch 1433, Loss: 0.07258835708125844, Final Batch Loss: 0.005923429504036903\n",
      "Epoch 1434, Loss: 0.05423310118931113, Final Batch Loss: 0.0008519238326698542\n",
      "Epoch 1435, Loss: 0.08091149014944676, Final Batch Loss: 0.006459853146225214\n",
      "Epoch 1436, Loss: 0.02686425952560967, Final Batch Loss: 0.00017225663759745657\n",
      "Epoch 1437, Loss: 0.020760949788382277, Final Batch Loss: 0.0022855326533317566\n",
      "Epoch 1438, Loss: 0.030793947204074357, Final Batch Loss: 0.00037099982728250325\n",
      "Epoch 1439, Loss: 0.014365746843395755, Final Batch Loss: 0.00029430430731736124\n",
      "Epoch 1440, Loss: 0.012368100651656277, Final Batch Loss: 0.0007030369597487152\n",
      "Epoch 1441, Loss: 0.06671991075927508, Final Batch Loss: 0.004624753724783659\n",
      "Epoch 1442, Loss: 0.036103775579249486, Final Batch Loss: 0.0002797612687572837\n",
      "Epoch 1443, Loss: 0.03975892184462282, Final Batch Loss: 0.005307995714247227\n",
      "Epoch 1444, Loss: 0.02096264305873774, Final Batch Loss: 0.0042429170571267605\n",
      "Epoch 1445, Loss: 0.0846478182793362, Final Batch Loss: 0.00021592948178295046\n",
      "Epoch 1446, Loss: 0.07577642456453759, Final Batch Loss: 0.000811574689578265\n",
      "Epoch 1447, Loss: 0.01406707846763311, Final Batch Loss: 0.0005168701172806323\n",
      "Epoch 1448, Loss: 0.049213534250156954, Final Batch Loss: 0.004366316366940737\n",
      "Epoch 1449, Loss: 0.09464575367746875, Final Batch Loss: 0.0020716742146760225\n",
      "Epoch 1450, Loss: 0.07797760495304829, Final Batch Loss: 0.00020384757954161614\n",
      "Epoch 1451, Loss: 0.03542899947206024, Final Batch Loss: 0.006464092526584864\n",
      "Epoch 1452, Loss: 0.012830260384362191, Final Batch Loss: 0.000168141647009179\n",
      "Epoch 1453, Loss: 0.04091344943299191, Final Batch Loss: 0.0005136691615916789\n",
      "Epoch 1454, Loss: 0.0151418310997542, Final Batch Loss: 0.0003203068918082863\n",
      "Epoch 1455, Loss: 0.052757634955924004, Final Batch Loss: 0.007206892129033804\n",
      "Epoch 1456, Loss: 0.05040478432783857, Final Batch Loss: 0.016709299758076668\n",
      "Epoch 1457, Loss: 0.041537035460351035, Final Batch Loss: 0.0006166989332996309\n",
      "Epoch 1458, Loss: 0.01999220030484139, Final Batch Loss: 9.090799721889198e-05\n",
      "Epoch 1459, Loss: 0.02757628790277522, Final Batch Loss: 0.0024145503994077444\n",
      "Epoch 1460, Loss: 0.02165039983810857, Final Batch Loss: 0.0020214335527271032\n",
      "Epoch 1461, Loss: 0.11016850838495884, Final Batch Loss: 0.08186513185501099\n",
      "Epoch 1462, Loss: 0.09377052212948911, Final Batch Loss: 0.00035779946483671665\n",
      "Epoch 1463, Loss: 0.03343821949965786, Final Batch Loss: 0.0003598030307330191\n",
      "Epoch 1464, Loss: 0.019532166785211302, Final Batch Loss: 0.0015986626967787743\n",
      "Epoch 1465, Loss: 0.020696945721283555, Final Batch Loss: 0.00016766627959441394\n",
      "Epoch 1466, Loss: 0.0741477873234544, Final Batch Loss: 0.001357778673991561\n",
      "Epoch 1467, Loss: 0.0638389682280831, Final Batch Loss: 0.007616366259753704\n",
      "Epoch 1468, Loss: 0.03314530148054473, Final Batch Loss: 0.002472964581102133\n",
      "Epoch 1469, Loss: 0.06926741532515734, Final Batch Loss: 0.0011779948836192489\n",
      "Epoch 1470, Loss: 0.03328413004055619, Final Batch Loss: 0.000631017959676683\n",
      "Epoch 1471, Loss: 0.027627094386843964, Final Batch Loss: 0.0019452179549261928\n",
      "Epoch 1472, Loss: 0.020815053358091973, Final Batch Loss: 0.00010534129978623241\n",
      "Epoch 1473, Loss: 0.045605343097122386, Final Batch Loss: 0.0002442951372358948\n",
      "Epoch 1474, Loss: 0.027624715337879024, Final Batch Loss: 0.007811822462826967\n",
      "Epoch 1475, Loss: 0.0300961694738362, Final Batch Loss: 0.00016760232392698526\n",
      "Epoch 1476, Loss: 0.017821728659328073, Final Batch Loss: 0.0007780363084748387\n",
      "Epoch 1477, Loss: 0.033168079709867015, Final Batch Loss: 0.00014744199870619923\n",
      "Epoch 1478, Loss: 0.04966247757693054, Final Batch Loss: 0.004058207385241985\n",
      "Epoch 1479, Loss: 0.03383375301200431, Final Batch Loss: 0.0040987166576087475\n",
      "Epoch 1480, Loss: 0.06422407604259206, Final Batch Loss: 0.010067871771752834\n",
      "Epoch 1481, Loss: 0.07870830639149062, Final Batch Loss: 0.006356789730489254\n",
      "Epoch 1482, Loss: 0.04351729816698935, Final Batch Loss: 0.0032057722564786673\n",
      "Epoch 1483, Loss: 0.03055581051739864, Final Batch Loss: 0.0003013313689734787\n",
      "Epoch 1484, Loss: 0.054264097052509896, Final Batch Loss: 0.0007331370725296438\n",
      "Epoch 1485, Loss: 0.06959060978988418, Final Batch Loss: 0.01167271938174963\n",
      "Epoch 1486, Loss: 0.0679048226447776, Final Batch Loss: 0.00452576857060194\n",
      "Epoch 1487, Loss: 0.08600961488264147, Final Batch Loss: 0.0010059651685878634\n",
      "Epoch 1488, Loss: 0.04447624471504241, Final Batch Loss: 0.0009086019126698375\n",
      "Epoch 1489, Loss: 0.04693837134982459, Final Batch Loss: 0.00035483454121276736\n",
      "Epoch 1490, Loss: 0.04964307363843545, Final Batch Loss: 0.004091558046638966\n",
      "Epoch 1491, Loss: 0.13233840788598172, Final Batch Loss: 0.07540225982666016\n",
      "Epoch 1492, Loss: 0.04348383875912987, Final Batch Loss: 0.005171600263565779\n",
      "Epoch 1493, Loss: 0.1431497737648897, Final Batch Loss: 0.0007380241295322776\n",
      "Epoch 1494, Loss: 0.09108508960343897, Final Batch Loss: 0.0014019445516169071\n",
      "Epoch 1495, Loss: 0.0556176569807576, Final Batch Loss: 0.001248829998075962\n",
      "Epoch 1496, Loss: 0.04283989202303928, Final Batch Loss: 0.0008367773261852562\n",
      "Epoch 1497, Loss: 0.07900405599502847, Final Batch Loss: 0.010448286309838295\n",
      "Epoch 1498, Loss: 0.04051150943269022, Final Batch Loss: 0.00014949802425689995\n",
      "Epoch 1499, Loss: 0.04320700393873267, Final Batch Loss: 0.0006447631167247891\n",
      "Epoch 1500, Loss: 0.07463020865543513, Final Batch Loss: 0.0021085951011627913\n",
      "Epoch 1501, Loss: 0.05214429742045468, Final Batch Loss: 0.0002081253769574687\n",
      "Epoch 1502, Loss: 0.06159969937289134, Final Batch Loss: 0.002249122830107808\n",
      "Epoch 1503, Loss: 0.04331166244810447, Final Batch Loss: 0.0022416163701564074\n",
      "Epoch 1504, Loss: 0.06905307850684039, Final Batch Loss: 0.005323085002601147\n",
      "Epoch 1505, Loss: 0.09694287034653826, Final Batch Loss: 0.007190825883299112\n",
      "Epoch 1506, Loss: 0.058668950892752036, Final Batch Loss: 0.008037031628191471\n",
      "Epoch 1507, Loss: 0.3703960373532027, Final Batch Loss: 0.025569817051291466\n",
      "Epoch 1508, Loss: 0.22940205922350287, Final Batch Loss: 0.019253401085734367\n",
      "Epoch 1509, Loss: 0.16404472896829247, Final Batch Loss: 0.010872984305024147\n",
      "Epoch 1510, Loss: 0.09914113336708397, Final Batch Loss: 0.0022798904683440924\n",
      "Epoch 1511, Loss: 0.07173850986873731, Final Batch Loss: 0.012757289223372936\n",
      "Epoch 1512, Loss: 0.03368062630761415, Final Batch Loss: 0.0008260784088633955\n",
      "Epoch 1513, Loss: 0.06580033944919705, Final Batch Loss: 0.002215205691754818\n",
      "Epoch 1514, Loss: 0.026291613787179813, Final Batch Loss: 0.0014857453061267734\n",
      "Epoch 1515, Loss: 0.06823297290247865, Final Batch Loss: 0.0006257969071157277\n",
      "Epoch 1516, Loss: 0.051985903875902295, Final Batch Loss: 0.015213304199278355\n",
      "Epoch 1517, Loss: 0.05628317503578728, Final Batch Loss: 0.0024295863695442677\n",
      "Epoch 1518, Loss: 0.13832195394206792, Final Batch Loss: 0.008366981521248817\n",
      "Epoch 1519, Loss: 0.06212071538902819, Final Batch Loss: 0.011781619861721992\n",
      "Epoch 1520, Loss: 0.049578041944187135, Final Batch Loss: 0.005896890535950661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1521, Loss: 0.02581916375493165, Final Batch Loss: 0.0010432770941406488\n",
      "Epoch 1522, Loss: 0.06394522020127624, Final Batch Loss: 0.002576870145276189\n",
      "Epoch 1523, Loss: 0.03227061458164826, Final Batch Loss: 0.002132915426045656\n",
      "Epoch 1524, Loss: 0.051894265416194685, Final Batch Loss: 0.0009963340125977993\n",
      "Epoch 1525, Loss: 0.04411345795961097, Final Batch Loss: 0.00393923232331872\n",
      "Epoch 1526, Loss: 0.0761615634910413, Final Batch Loss: 0.0024246308021247387\n",
      "Epoch 1527, Loss: 0.046185564176994376, Final Batch Loss: 0.0015078781871125102\n",
      "Epoch 1528, Loss: 0.04666129985707812, Final Batch Loss: 0.0017996603855863214\n",
      "Epoch 1529, Loss: 0.08203391613642452, Final Batch Loss: 0.002160872332751751\n",
      "Epoch 1530, Loss: 0.05068234002101235, Final Batch Loss: 0.016438772901892662\n",
      "Epoch 1531, Loss: 0.07632743299473077, Final Batch Loss: 0.0035062043461948633\n",
      "Epoch 1532, Loss: 0.025331914199341554, Final Batch Loss: 8.264531788881868e-05\n",
      "Epoch 1533, Loss: 0.041481534211925464, Final Batch Loss: 5.488683018484153e-05\n",
      "Epoch 1534, Loss: 0.07029813043482136, Final Batch Loss: 0.00038217895780690014\n",
      "Epoch 1535, Loss: 0.06980910831771325, Final Batch Loss: 0.0002932797360699624\n",
      "Epoch 1536, Loss: 0.05609822801488917, Final Batch Loss: 0.00042266157106496394\n",
      "Epoch 1537, Loss: 0.015370306908152997, Final Batch Loss: 0.0011076912051066756\n",
      "Epoch 1538, Loss: 0.02733541902853176, Final Batch Loss: 0.00282708159647882\n",
      "Epoch 1539, Loss: 0.06732601077237632, Final Batch Loss: 0.0015240219654515386\n",
      "Epoch 1540, Loss: 0.06785110775672365, Final Batch Loss: 0.013030556961894035\n",
      "Epoch 1541, Loss: 0.03553546337207081, Final Batch Loss: 0.0004993525217287242\n",
      "Epoch 1542, Loss: 0.04020923504140228, Final Batch Loss: 0.005508022848516703\n",
      "Epoch 1543, Loss: 0.03921993385301903, Final Batch Loss: 0.0006478014402091503\n",
      "Epoch 1544, Loss: 0.032224100243183784, Final Batch Loss: 0.004376295022666454\n",
      "Epoch 1545, Loss: 0.013253051896754187, Final Batch Loss: 0.00044932120363228023\n",
      "Epoch 1546, Loss: 0.029821336866007186, Final Batch Loss: 0.00020087356097064912\n",
      "Epoch 1547, Loss: 0.09325343661475927, Final Batch Loss: 0.005197079852223396\n",
      "Epoch 1548, Loss: 0.06736997529515065, Final Batch Loss: 0.00019500141206663102\n",
      "Epoch 1549, Loss: 0.09770490330265602, Final Batch Loss: 0.0029148205649107695\n",
      "Epoch 1550, Loss: 0.07653083548211725, Final Batch Loss: 0.002745832782238722\n",
      "Epoch 1551, Loss: 0.09519883448956534, Final Batch Loss: 0.0008861137321218848\n",
      "Epoch 1552, Loss: 0.08309037782601081, Final Batch Loss: 0.004275113344192505\n",
      "Epoch 1553, Loss: 0.11816869030008093, Final Batch Loss: 0.014241308905184269\n",
      "Epoch 1554, Loss: 0.06183685385622084, Final Batch Loss: 0.0031656427308917046\n",
      "Epoch 1555, Loss: 0.0702879530726932, Final Batch Loss: 0.007764679845422506\n",
      "Epoch 1556, Loss: 0.031519644588115625, Final Batch Loss: 0.007155187893658876\n",
      "Epoch 1557, Loss: 0.08380847652733792, Final Batch Loss: 0.011187562718987465\n",
      "Epoch 1558, Loss: 0.09230727091198787, Final Batch Loss: 0.00033666304079815745\n",
      "Epoch 1559, Loss: 0.040834890969563276, Final Batch Loss: 0.003274257993325591\n",
      "Epoch 1560, Loss: 0.04219461319735274, Final Batch Loss: 0.0016652141930535436\n",
      "Epoch 1561, Loss: 0.016014981316402555, Final Batch Loss: 0.0004990929155610502\n",
      "Epoch 1562, Loss: 0.027398576232371852, Final Batch Loss: 0.0012854380765929818\n",
      "Epoch 1563, Loss: 0.06786620542698074, Final Batch Loss: 0.0005101327551528811\n",
      "Epoch 1564, Loss: 0.016112910358060617, Final Batch Loss: 9.058515570359305e-05\n",
      "Epoch 1565, Loss: 0.025727178417582763, Final Batch Loss: 0.00013496722385752946\n",
      "Epoch 1566, Loss: 0.0330430218600668, Final Batch Loss: 0.00012833470827899873\n",
      "Epoch 1567, Loss: 0.02700500605715206, Final Batch Loss: 0.0005483974236994982\n",
      "Epoch 1568, Loss: 0.06684406384010799, Final Batch Loss: 0.0020067866425961256\n",
      "Epoch 1569, Loss: 0.055923341467860155, Final Batch Loss: 0.0006615886813960969\n",
      "Epoch 1570, Loss: 0.028346284318104153, Final Batch Loss: 0.003267106367275119\n",
      "Epoch 1571, Loss: 0.014192725302564213, Final Batch Loss: 0.0006188862025737762\n",
      "Epoch 1572, Loss: 0.03174202841546503, Final Batch Loss: 2.7787205908680335e-05\n",
      "Epoch 1573, Loss: 0.024012731253606034, Final Batch Loss: 0.00010752425441751257\n",
      "Epoch 1574, Loss: 0.06392126813261712, Final Batch Loss: 0.0002718303876463324\n",
      "Epoch 1575, Loss: 0.16222093591932207, Final Batch Loss: 0.010359306819736958\n",
      "Epoch 1576, Loss: 0.11391690146410838, Final Batch Loss: 0.020298907533288002\n",
      "Epoch 1577, Loss: 0.04564243872300722, Final Batch Loss: 0.0019239955581724644\n",
      "Epoch 1578, Loss: 0.03554373748193029, Final Batch Loss: 0.0015099847223609686\n",
      "Epoch 1579, Loss: 0.03151417016488267, Final Batch Loss: 0.003872770117595792\n",
      "Epoch 1580, Loss: 0.018817884469171986, Final Batch Loss: 0.001807920983992517\n",
      "Epoch 1581, Loss: 0.02342292717366945, Final Batch Loss: 0.0003147246898151934\n",
      "Epoch 1582, Loss: 0.027515883892192505, Final Batch Loss: 0.00011654679838102311\n",
      "Epoch 1583, Loss: 0.033379322034306824, Final Batch Loss: 0.0008556662942282856\n",
      "Epoch 1584, Loss: 0.014878992071317043, Final Batch Loss: 0.0026233855169266462\n",
      "Epoch 1585, Loss: 0.034771910424751695, Final Batch Loss: 0.0015404607402160764\n",
      "Epoch 1586, Loss: 0.013904165462008677, Final Batch Loss: 0.00016538873023819178\n",
      "Epoch 1587, Loss: 0.024154217699106084, Final Batch Loss: 0.00013061407662462443\n",
      "Epoch 1588, Loss: 0.022119925415609032, Final Batch Loss: 0.00021549206576310098\n",
      "Epoch 1589, Loss: 0.04738997182539606, Final Batch Loss: 9.967287769541144e-05\n",
      "Epoch 1590, Loss: 0.027805994090158492, Final Batch Loss: 0.011926181614398956\n",
      "Epoch 1591, Loss: 0.05269070177746471, Final Batch Loss: 0.003812913317233324\n",
      "Epoch 1592, Loss: 0.029004118565353565, Final Batch Loss: 0.003051029285416007\n",
      "Epoch 1593, Loss: 0.06348906116909347, Final Batch Loss: 0.000735754962079227\n",
      "Epoch 1594, Loss: 0.024357802238228032, Final Batch Loss: 0.0006785528385080397\n",
      "Epoch 1595, Loss: 0.06041150890814606, Final Batch Loss: 0.0035086898133158684\n",
      "Epoch 1596, Loss: 0.03832396224788681, Final Batch Loss: 0.001800987054593861\n",
      "Epoch 1597, Loss: 0.021436848313896917, Final Batch Loss: 0.0005186456837691367\n",
      "Epoch 1598, Loss: 0.0223417615670769, Final Batch Loss: 0.008348798379302025\n",
      "Epoch 1599, Loss: 0.012007685007120017, Final Batch Loss: 0.00031790565117262304\n",
      "Epoch 1600, Loss: 0.02138433383515803, Final Batch Loss: 0.0002610782685223967\n",
      "Epoch 1601, Loss: 0.0465491287104669, Final Batch Loss: 0.0024666946846991777\n",
      "Epoch 1602, Loss: 0.07649574004608439, Final Batch Loss: 0.008156545460224152\n",
      "Epoch 1603, Loss: 0.09521161766315345, Final Batch Loss: 0.011400742456316948\n",
      "Epoch 1604, Loss: 0.06915516136359656, Final Batch Loss: 0.0002732246648520231\n",
      "Epoch 1605, Loss: 0.132985730713699, Final Batch Loss: 0.024252118542790413\n",
      "Epoch 1606, Loss: 0.05441008650086587, Final Batch Loss: 0.003737204009667039\n",
      "Epoch 1607, Loss: 0.04719880601624027, Final Batch Loss: 0.0005543241859413683\n",
      "Epoch 1608, Loss: 0.021929548551270273, Final Batch Loss: 0.0002547132025938481\n",
      "Epoch 1609, Loss: 0.021913876316830283, Final Batch Loss: 0.00025626091519370675\n",
      "Epoch 1610, Loss: 0.03498019448306877, Final Batch Loss: 0.0005521331331692636\n",
      "Epoch 1611, Loss: 0.08162841445300728, Final Batch Loss: 0.0005171329248696566\n",
      "Epoch 1612, Loss: 0.013684090543392813, Final Batch Loss: 0.00218915194272995\n",
      "Epoch 1613, Loss: 0.015899504178378265, Final Batch Loss: 0.005986510775983334\n",
      "Epoch 1614, Loss: 0.03244594403804513, Final Batch Loss: 0.0001694134552963078\n",
      "Epoch 1615, Loss: 0.04221765469992533, Final Batch Loss: 0.004787551239132881\n",
      "Epoch 1616, Loss: 0.06601265264907852, Final Batch Loss: 0.004060956183820963\n",
      "Epoch 1617, Loss: 0.061977062359801494, Final Batch Loss: 0.003124489914625883\n",
      "Epoch 1618, Loss: 0.01837247284129262, Final Batch Loss: 0.007816777564585209\n",
      "Epoch 1619, Loss: 0.03569581300325808, Final Batch Loss: 0.0026670361403375864\n",
      "Epoch 1620, Loss: 0.046640695254609454, Final Batch Loss: 0.00024384484277106822\n",
      "Epoch 1621, Loss: 0.03712321387138218, Final Batch Loss: 0.004427269101142883\n",
      "Epoch 1622, Loss: 0.028851102862972766, Final Batch Loss: 0.0002492586791049689\n",
      "Epoch 1623, Loss: 0.013859532766218763, Final Batch Loss: 0.000148092454764992\n",
      "Epoch 1624, Loss: 0.009036171832121909, Final Batch Loss: 0.00026311189867556095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1625, Loss: 0.054162548753083684, Final Batch Loss: 0.0019586628768593073\n",
      "Epoch 1626, Loss: 0.036342720057291444, Final Batch Loss: 0.00048266310477629304\n",
      "Epoch 1627, Loss: 0.01847512071253732, Final Batch Loss: 0.0008057663799263537\n",
      "Epoch 1628, Loss: 0.022544826490047853, Final Batch Loss: 0.0018856757087633014\n",
      "Epoch 1629, Loss: 0.03870075887971325, Final Batch Loss: 5.373272142605856e-05\n",
      "Epoch 1630, Loss: 0.019880655425367877, Final Batch Loss: 0.00023937106016092002\n",
      "Epoch 1631, Loss: 0.06884498250110482, Final Batch Loss: 0.001605622353963554\n",
      "Epoch 1632, Loss: 0.010772022604214726, Final Batch Loss: 0.00017588127229828387\n",
      "Epoch 1633, Loss: 0.012836474998039193, Final Batch Loss: 0.0003328849852550775\n",
      "Epoch 1634, Loss: 0.049724157426680904, Final Batch Loss: 0.006640439387410879\n",
      "Epoch 1635, Loss: 0.05004904023189738, Final Batch Loss: 0.03772934153676033\n",
      "Epoch 1636, Loss: 0.031764358609507326, Final Batch Loss: 0.001533767906948924\n",
      "Epoch 1637, Loss: 0.02040374098578468, Final Batch Loss: 0.0024325719568878412\n",
      "Epoch 1638, Loss: 0.028930655316798948, Final Batch Loss: 0.00012663219240494072\n",
      "Epoch 1639, Loss: 0.021581766588496976, Final Batch Loss: 0.0010384516790509224\n",
      "Epoch 1640, Loss: 0.011035672330763191, Final Batch Loss: 9.88244210020639e-05\n",
      "Epoch 1641, Loss: 0.01125324319673382, Final Batch Loss: 0.003570323111489415\n",
      "Epoch 1642, Loss: 0.0298827312017238, Final Batch Loss: 0.0002843832189682871\n",
      "Epoch 1643, Loss: 0.03228552620203118, Final Batch Loss: 0.0010616250801831484\n",
      "Epoch 1644, Loss: 0.018526318061049096, Final Batch Loss: 0.0015006510075181723\n",
      "Epoch 1645, Loss: 0.040018572653934825, Final Batch Loss: 0.013232589699327946\n",
      "Epoch 1646, Loss: 0.049774919756600866, Final Batch Loss: 0.002272269455716014\n",
      "Epoch 1647, Loss: 0.04050340270623565, Final Batch Loss: 0.0009400980197824538\n",
      "Epoch 1648, Loss: 0.016383244306780398, Final Batch Loss: 0.0003147910174448043\n",
      "Epoch 1649, Loss: 0.04615996622305829, Final Batch Loss: 0.02452138438820839\n",
      "Epoch 1650, Loss: 0.017647508491791086, Final Batch Loss: 0.002375719603151083\n",
      "Epoch 1651, Loss: 0.06246163019750384, Final Batch Loss: 0.00015225616516545415\n",
      "Epoch 1652, Loss: 0.06033632610342465, Final Batch Loss: 0.011275707744061947\n",
      "Epoch 1653, Loss: 0.08577171046636067, Final Batch Loss: 0.013486131094396114\n",
      "Epoch 1654, Loss: 0.10170418524648994, Final Batch Loss: 0.008162646554410458\n",
      "Epoch 1655, Loss: 0.1396595794649329, Final Batch Loss: 0.002947197062894702\n",
      "Epoch 1656, Loss: 0.06574593319237465, Final Batch Loss: 0.012683992274105549\n",
      "Epoch 1657, Loss: 0.03971043464844115, Final Batch Loss: 0.0004535969055723399\n",
      "Epoch 1658, Loss: 0.11047166097159788, Final Batch Loss: 0.005980188027024269\n",
      "Epoch 1659, Loss: 0.15686700571677648, Final Batch Loss: 0.0010674581862986088\n",
      "Epoch 1660, Loss: 0.026708525670983363, Final Batch Loss: 0.0005205842899158597\n",
      "Epoch 1661, Loss: 0.04295424731390085, Final Batch Loss: 0.0008937229868024588\n",
      "Epoch 1662, Loss: 0.015325262924307026, Final Batch Loss: 0.0008075522491708398\n",
      "Epoch 1663, Loss: 0.07383763557299972, Final Batch Loss: 0.0002512875071261078\n",
      "Epoch 1664, Loss: 0.030527942333719693, Final Batch Loss: 0.00014593542437069118\n",
      "Epoch 1665, Loss: 0.01847822640411323, Final Batch Loss: 0.004286611452698708\n",
      "Epoch 1666, Loss: 0.013900530851969961, Final Batch Loss: 8.396177872782573e-05\n",
      "Epoch 1667, Loss: 0.008767488168814452, Final Batch Loss: 7.95695377746597e-05\n",
      "Epoch 1668, Loss: 0.014331581638543867, Final Batch Loss: 0.0009945117635652423\n",
      "Epoch 1669, Loss: 0.017746333578543272, Final Batch Loss: 0.00596721563488245\n",
      "Epoch 1670, Loss: 0.014053003229491878, Final Batch Loss: 0.006234809290617704\n",
      "Epoch 1671, Loss: 0.051241051325632725, Final Batch Loss: 0.0001602428819751367\n",
      "Epoch 1672, Loss: 0.03946595988236368, Final Batch Loss: 0.00013197428779676557\n",
      "Epoch 1673, Loss: 0.054938782086537685, Final Batch Loss: 3.091911639785394e-05\n",
      "Epoch 1674, Loss: 0.09051250152697321, Final Batch Loss: 0.03405765816569328\n",
      "Epoch 1675, Loss: 0.018970300763612613, Final Batch Loss: 0.0025222445838153362\n",
      "Epoch 1676, Loss: 0.020127948049776023, Final Batch Loss: 0.005696752108633518\n",
      "Epoch 1677, Loss: 0.05815177408658201, Final Batch Loss: 0.0015515134437009692\n",
      "Epoch 1678, Loss: 0.05565480007498991, Final Batch Loss: 0.010841128416359425\n",
      "Epoch 1679, Loss: 0.025394865166163072, Final Batch Loss: 9.142887574853376e-05\n",
      "Epoch 1680, Loss: 0.02712653346679872, Final Batch Loss: 8.412887837039307e-05\n",
      "Epoch 1681, Loss: 0.09613719949993538, Final Batch Loss: 0.00012055961269652471\n",
      "Epoch 1682, Loss: 0.029113157121173572, Final Batch Loss: 7.70685073803179e-05\n",
      "Epoch 1683, Loss: 0.059347029262426076, Final Batch Loss: 0.00038225174648687243\n",
      "Epoch 1684, Loss: 0.06949219794478267, Final Batch Loss: 0.00023734911519568413\n",
      "Epoch 1685, Loss: 0.09327563853003085, Final Batch Loss: 0.0013372970279306173\n",
      "Epoch 1686, Loss: 0.028226757087395526, Final Batch Loss: 0.00020777733880095184\n",
      "Epoch 1687, Loss: 0.054479560451, Final Batch Loss: 0.0008288510725833476\n",
      "Epoch 1688, Loss: 0.0320722330652643, Final Batch Loss: 0.0005231417599134147\n",
      "Epoch 1689, Loss: 0.014463834311754908, Final Batch Loss: 0.00012113815319025889\n",
      "Epoch 1690, Loss: 0.02039809278358007, Final Batch Loss: 0.0015874807722866535\n",
      "Epoch 1691, Loss: 0.07133743573649554, Final Batch Loss: 0.010015284642577171\n",
      "Epoch 1692, Loss: 0.05075111621408723, Final Batch Loss: 0.00039389857556670904\n",
      "Epoch 1693, Loss: 0.04728157407953404, Final Batch Loss: 0.0002807557175401598\n",
      "Epoch 1694, Loss: 0.05360220974398544, Final Batch Loss: 5.667102232109755e-05\n",
      "Epoch 1695, Loss: 0.029210288405010942, Final Batch Loss: 0.0028566347900778055\n",
      "Epoch 1696, Loss: 0.031059283413924277, Final Batch Loss: 0.0002674682473298162\n",
      "Epoch 1697, Loss: 0.06633131561102346, Final Batch Loss: 0.005003540776669979\n",
      "Epoch 1698, Loss: 0.05438268541911384, Final Batch Loss: 0.0034658790100365877\n",
      "Epoch 1699, Loss: 0.017905339860590175, Final Batch Loss: 0.001681094174273312\n",
      "Epoch 1700, Loss: 0.024256043252535164, Final Batch Loss: 0.007515678647905588\n",
      "Epoch 1701, Loss: 0.05109051532053854, Final Batch Loss: 0.0009220021893270314\n",
      "Epoch 1702, Loss: 0.049032428389182314, Final Batch Loss: 0.018422631546854973\n",
      "Epoch 1703, Loss: 0.07161142550467048, Final Batch Loss: 6.836631655460224e-05\n",
      "Epoch 1704, Loss: 0.03305206545337569, Final Batch Loss: 0.004859826993197203\n",
      "Epoch 1705, Loss: 0.03508166426036041, Final Batch Loss: 0.0020951894111931324\n",
      "Epoch 1706, Loss: 0.04045799083542079, Final Batch Loss: 0.004615578800439835\n",
      "Epoch 1707, Loss: 0.0317394961748505, Final Batch Loss: 0.0002924298751167953\n",
      "Epoch 1708, Loss: 0.02090692969795782, Final Batch Loss: 0.00043765336158685386\n",
      "Epoch 1709, Loss: 0.03995388331532013, Final Batch Loss: 0.000821528141386807\n",
      "Epoch 1710, Loss: 0.06551258582840092, Final Batch Loss: 0.0002989340282510966\n",
      "Epoch 1711, Loss: 0.05947150788779254, Final Batch Loss: 0.0008341820794157684\n",
      "Epoch 1712, Loss: 0.04748117814597208, Final Batch Loss: 0.0027466733008623123\n",
      "Epoch 1713, Loss: 0.03599074650264811, Final Batch Loss: 0.00026274044648744166\n",
      "Epoch 1714, Loss: 0.06047892045171466, Final Batch Loss: 0.0012135733850300312\n",
      "Epoch 1715, Loss: 0.023008657022728585, Final Batch Loss: 0.0067840213887393475\n",
      "Epoch 1716, Loss: 0.07614088750415249, Final Batch Loss: 0.0013158242218196392\n",
      "Epoch 1717, Loss: 0.06171443334460491, Final Batch Loss: 0.0012247395934537053\n",
      "Epoch 1718, Loss: 0.019559723768907133, Final Batch Loss: 0.001140320091508329\n",
      "Epoch 1719, Loss: 0.05255581476376392, Final Batch Loss: 0.0026408564299345016\n",
      "Epoch 1720, Loss: 0.03149441856658086, Final Batch Loss: 0.00022782462474424392\n",
      "Epoch 1721, Loss: 0.04139930710516637, Final Batch Loss: 0.0033983353059738874\n",
      "Epoch 1722, Loss: 0.01777362902794266, Final Batch Loss: 0.001859411597251892\n",
      "Epoch 1723, Loss: 0.10302693013181852, Final Batch Loss: 0.000140299103804864\n",
      "Epoch 1724, Loss: 0.024683327806997113, Final Batch Loss: 0.0005011284374631941\n",
      "Epoch 1725, Loss: 0.026554856871371157, Final Batch Loss: 0.0001475387398386374\n",
      "Epoch 1726, Loss: 0.02492694481043145, Final Batch Loss: 0.0023456381168216467\n",
      "Epoch 1727, Loss: 0.043011850444599986, Final Batch Loss: 0.00036958500277251005\n",
      "Epoch 1728, Loss: 0.03044631431112066, Final Batch Loss: 0.0007314693066291511\n",
      "Epoch 1729, Loss: 0.04567174454132328, Final Batch Loss: 0.003839862300083041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1730, Loss: 0.07240079759503715, Final Batch Loss: 0.004633769392967224\n",
      "Epoch 1731, Loss: 0.057792115228949115, Final Batch Loss: 0.00213807076215744\n",
      "Epoch 1732, Loss: 0.03737178409937769, Final Batch Loss: 0.0012349896132946014\n",
      "Epoch 1733, Loss: 0.023988251909031533, Final Batch Loss: 0.0007662441348657012\n",
      "Epoch 1734, Loss: 0.0328849058423657, Final Batch Loss: 0.002048313617706299\n",
      "Epoch 1735, Loss: 0.045679660066525685, Final Batch Loss: 0.0011824218090623617\n",
      "Epoch 1736, Loss: 0.044281342226895504, Final Batch Loss: 0.0020434323232620955\n",
      "Epoch 1737, Loss: 0.0337358393589966, Final Batch Loss: 0.0017900681123137474\n",
      "Epoch 1738, Loss: 0.014994265566201648, Final Batch Loss: 0.0001206569213536568\n",
      "Epoch 1739, Loss: 0.03540626368885569, Final Batch Loss: 0.00025852237013168633\n",
      "Epoch 1740, Loss: 0.04076996432195301, Final Batch Loss: 0.000719719915650785\n",
      "Epoch 1741, Loss: 0.024530811235308647, Final Batch Loss: 0.000313017750158906\n",
      "Epoch 1742, Loss: 0.012284502219699789, Final Batch Loss: 0.00017590448260307312\n",
      "Epoch 1743, Loss: 0.04071081274014432, Final Batch Loss: 0.0006007833871990442\n",
      "Epoch 1744, Loss: 0.09135855394197279, Final Batch Loss: 0.00040789504419080913\n",
      "Epoch 1745, Loss: 0.021049006874818588, Final Batch Loss: 0.0012654142919927835\n",
      "Epoch 1746, Loss: 0.021157948169275187, Final Batch Loss: 0.0004567163123283535\n",
      "Epoch 1747, Loss: 0.036977956791815814, Final Batch Loss: 7.156080391723663e-05\n",
      "Epoch 1748, Loss: 0.04520310628868174, Final Batch Loss: 0.021985715255141258\n",
      "Epoch 1749, Loss: 0.01638389143772656, Final Batch Loss: 0.00013136485358700156\n",
      "Epoch 1750, Loss: 0.07127452701388393, Final Batch Loss: 0.02033347450196743\n",
      "Epoch 1751, Loss: 0.04968151605135063, Final Batch Loss: 0.0055845691822469234\n",
      "Epoch 1752, Loss: 0.044957742386031896, Final Batch Loss: 0.00010933029989246279\n",
      "Epoch 1753, Loss: 0.03802208698471077, Final Batch Loss: 0.000980221084319055\n",
      "Epoch 1754, Loss: 0.024457845276629087, Final Batch Loss: 0.002766871592029929\n",
      "Epoch 1755, Loss: 0.006431217338104034, Final Batch Loss: 0.00014492039917968214\n",
      "Epoch 1756, Loss: 0.025308398751803907, Final Batch Loss: 0.0018727704882621765\n",
      "Epoch 1757, Loss: 0.020634301457903348, Final Batch Loss: 0.007320036180317402\n",
      "Epoch 1758, Loss: 0.020227163837262196, Final Batch Loss: 0.00033141180756501853\n",
      "Epoch 1759, Loss: 0.027953311069722986, Final Batch Loss: 5.0348517106613144e-05\n",
      "Epoch 1760, Loss: 0.015793656817550072, Final Batch Loss: 5.472373231896199e-05\n",
      "Epoch 1761, Loss: 0.10460185287229251, Final Batch Loss: 0.005348955746740103\n",
      "Epoch 1762, Loss: 0.10332138994999696, Final Batch Loss: 0.00994403287768364\n",
      "Epoch 1763, Loss: 0.13801552784570958, Final Batch Loss: 0.00020997402316424996\n",
      "Epoch 1764, Loss: 0.058790404989849776, Final Batch Loss: 0.0023487305734306574\n",
      "Epoch 1765, Loss: 0.0749467700443347, Final Batch Loss: 0.003676110180094838\n",
      "Epoch 1766, Loss: 0.04020657649380155, Final Batch Loss: 0.004765234421938658\n",
      "Epoch 1767, Loss: 0.06733211240498349, Final Batch Loss: 0.007948679849505424\n",
      "Epoch 1768, Loss: 0.03740675993321929, Final Batch Loss: 9.766647417563945e-05\n",
      "Epoch 1769, Loss: 0.024582752877904568, Final Batch Loss: 0.00033972878009080887\n",
      "Epoch 1770, Loss: 0.0570982247882057, Final Batch Loss: 0.02514471486210823\n",
      "Epoch 1771, Loss: 0.027734645547752734, Final Batch Loss: 0.0008996247197501361\n",
      "Epoch 1772, Loss: 0.03966979957476724, Final Batch Loss: 0.0014956550439819694\n",
      "Epoch 1773, Loss: 0.04337655870767776, Final Batch Loss: 0.0002776933542918414\n",
      "Epoch 1774, Loss: 0.07601293548214016, Final Batch Loss: 0.0050559635274112225\n",
      "Epoch 1775, Loss: 0.0450670713908039, Final Batch Loss: 0.0004590534372255206\n",
      "Epoch 1776, Loss: 0.05621441185940057, Final Batch Loss: 0.0013220641994848847\n",
      "Epoch 1777, Loss: 0.034453452222805936, Final Batch Loss: 4.342148167779669e-05\n",
      "Epoch 1778, Loss: 0.03331962450465653, Final Batch Loss: 0.0038384655490517616\n",
      "Epoch 1779, Loss: 0.015282294581993483, Final Batch Loss: 0.00017957088130060583\n",
      "Epoch 1780, Loss: 0.04325134027931199, Final Batch Loss: 0.0010158238001167774\n",
      "Epoch 1781, Loss: 0.01973520064348122, Final Batch Loss: 0.0032433154992759228\n",
      "Epoch 1782, Loss: 0.051051728158199694, Final Batch Loss: 0.00010519281931919977\n",
      "Epoch 1783, Loss: 0.02939013054856332, Final Batch Loss: 0.001168995164334774\n",
      "Epoch 1784, Loss: 0.03725110361847328, Final Batch Loss: 0.0013075940078124404\n",
      "Epoch 1785, Loss: 0.04695000772335334, Final Batch Loss: 6.433077942347154e-05\n",
      "Epoch 1786, Loss: 0.0884844985048403, Final Batch Loss: 0.024832824245095253\n",
      "Epoch 1787, Loss: 0.09338262940582354, Final Batch Loss: 0.012546156533062458\n",
      "Epoch 1788, Loss: 0.05617386879748665, Final Batch Loss: 0.005784070119261742\n",
      "Epoch 1789, Loss: 0.05098183380323462, Final Batch Loss: 0.0012656819308176637\n",
      "Epoch 1790, Loss: 0.0916194572346285, Final Batch Loss: 0.002062816871330142\n",
      "Epoch 1791, Loss: 0.03698033133696299, Final Batch Loss: 0.0002280065236845985\n",
      "Epoch 1792, Loss: 0.03408029064303264, Final Batch Loss: 0.006042351480573416\n",
      "Epoch 1793, Loss: 0.05187508546805475, Final Batch Loss: 0.00402085343375802\n",
      "Epoch 1794, Loss: 0.03941997295623878, Final Batch Loss: 0.004524403251707554\n",
      "Epoch 1795, Loss: 0.05218472214983194, Final Batch Loss: 0.005011121742427349\n",
      "Epoch 1796, Loss: 0.05106898001395166, Final Batch Loss: 0.003651822218671441\n",
      "Epoch 1797, Loss: 0.04870280291652307, Final Batch Loss: 0.0013619762612506747\n",
      "Epoch 1798, Loss: 0.02316135285946075, Final Batch Loss: 0.0023849159479141235\n",
      "Epoch 1799, Loss: 0.04949721211596625, Final Batch Loss: 0.005588101223111153\n",
      "Epoch 1800, Loss: 0.04695688062929548, Final Batch Loss: 0.0015314341289922595\n",
      "Epoch 1801, Loss: 0.07594921483905637, Final Batch Loss: 0.005033145193010569\n",
      "Epoch 1802, Loss: 0.07406938450003508, Final Batch Loss: 0.0004776302957907319\n",
      "Epoch 1803, Loss: 0.10044839282636531, Final Batch Loss: 0.009874371811747551\n",
      "Epoch 1804, Loss: 0.05480109708150849, Final Batch Loss: 0.002448002342134714\n",
      "Epoch 1805, Loss: 0.07406324875046266, Final Batch Loss: 0.00048289637197740376\n",
      "Epoch 1806, Loss: 0.06138638219999848, Final Batch Loss: 0.00268920767121017\n",
      "Epoch 1807, Loss: 0.03070200834190473, Final Batch Loss: 0.0001787392538972199\n",
      "Epoch 1808, Loss: 0.027194915419386234, Final Batch Loss: 0.002369840629398823\n",
      "Epoch 1809, Loss: 0.04956710824626498, Final Batch Loss: 0.0021838615648448467\n",
      "Epoch 1810, Loss: 0.07128309048130177, Final Batch Loss: 0.0053067984990775585\n",
      "Epoch 1811, Loss: 0.044060491360141896, Final Batch Loss: 0.0005107034230604768\n",
      "Epoch 1812, Loss: 0.048533242246776354, Final Batch Loss: 0.001160606974735856\n",
      "Epoch 1813, Loss: 0.01615830346054281, Final Batch Loss: 0.000557903025764972\n",
      "Epoch 1814, Loss: 0.06290514690772397, Final Batch Loss: 0.0017013989854604006\n",
      "Epoch 1815, Loss: 0.04403306220774539, Final Batch Loss: 0.0019780483562499285\n",
      "Epoch 1816, Loss: 0.07241653472010512, Final Batch Loss: 0.006523985881358385\n",
      "Epoch 1817, Loss: 0.05825961101800203, Final Batch Loss: 0.02575051784515381\n",
      "Epoch 1818, Loss: 0.0751715990336379, Final Batch Loss: 0.0070913201197981834\n",
      "Epoch 1819, Loss: 0.060028871812392026, Final Batch Loss: 0.0032410004641860723\n",
      "Epoch 1820, Loss: 0.023261054258910008, Final Batch Loss: 0.0018595211440697312\n",
      "Epoch 1821, Loss: 0.04638858856924344, Final Batch Loss: 0.004020035732537508\n",
      "Epoch 1822, Loss: 0.031159195248619653, Final Batch Loss: 0.0019052033312618732\n",
      "Epoch 1823, Loss: 0.02494679412484402, Final Batch Loss: 0.0003115244908258319\n",
      "Epoch 1824, Loss: 0.03635200559801888, Final Batch Loss: 0.002238284330815077\n",
      "Epoch 1825, Loss: 0.05844291683933989, Final Batch Loss: 0.0019561105873435736\n",
      "Epoch 1826, Loss: 0.07049685053061694, Final Batch Loss: 0.025085097178816795\n",
      "Epoch 1827, Loss: 0.03136646087659756, Final Batch Loss: 6.138324533822015e-05\n",
      "Epoch 1828, Loss: 0.10830753574191476, Final Batch Loss: 0.00023369835980702192\n",
      "Epoch 1829, Loss: 0.04452886915532872, Final Batch Loss: 0.004573476035147905\n",
      "Epoch 1830, Loss: 0.07931169377843617, Final Batch Loss: 0.0024822154082357883\n",
      "Epoch 1831, Loss: 0.020316030415415298, Final Batch Loss: 0.00018147037189919502\n",
      "Epoch 1832, Loss: 0.01790199904644396, Final Batch Loss: 0.0007692541112191975\n",
      "Epoch 1833, Loss: 0.045178962291174685, Final Batch Loss: 0.011389242485165596\n",
      "Epoch 1834, Loss: 0.024997743821586482, Final Batch Loss: 0.0006254567997530103\n",
      "Epoch 1835, Loss: 0.0345473788693198, Final Batch Loss: 0.0014864515978842974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1836, Loss: 0.04295398019712593, Final Batch Loss: 0.00017612104420550168\n",
      "Epoch 1837, Loss: 0.02818580675375415, Final Batch Loss: 0.00036516852560453117\n",
      "Epoch 1838, Loss: 0.04369170525751542, Final Batch Loss: 0.016726959496736526\n",
      "Epoch 1839, Loss: 0.08964117654977599, Final Batch Loss: 0.0077049811370670795\n",
      "Epoch 1840, Loss: 0.014846162979665678, Final Batch Loss: 0.0003150264674331993\n",
      "Epoch 1841, Loss: 0.015874330514634494, Final Batch Loss: 0.0007333934190683067\n",
      "Epoch 1842, Loss: 0.02627077423312585, Final Batch Loss: 0.003876413218677044\n",
      "Epoch 1843, Loss: 0.01367111529543763, Final Batch Loss: 0.0013172054896131158\n",
      "Epoch 1844, Loss: 0.030344674589287024, Final Batch Loss: 0.002176971873268485\n",
      "Epoch 1845, Loss: 0.04042864182702033, Final Batch Loss: 0.0029851230792701244\n",
      "Epoch 1846, Loss: 0.06382600808137795, Final Batch Loss: 0.00010967102571157739\n",
      "Epoch 1847, Loss: 0.08252432289918943, Final Batch Loss: 0.0014417318161576986\n",
      "Epoch 1848, Loss: 0.04449918417230947, Final Batch Loss: 0.0009266470442526042\n",
      "Epoch 1849, Loss: 0.02778670818952378, Final Batch Loss: 0.00014714343706145883\n",
      "Epoch 1850, Loss: 0.01985049010545481, Final Batch Loss: 0.00047294783871620893\n",
      "Epoch 1851, Loss: 0.027638047027721768, Final Batch Loss: 0.00030341395176947117\n",
      "Epoch 1852, Loss: 0.007041571903755539, Final Batch Loss: 0.00021399759862106293\n",
      "Epoch 1853, Loss: 0.014423523243749514, Final Batch Loss: 0.0002639139711391181\n",
      "Epoch 1854, Loss: 0.030689743223774713, Final Batch Loss: 0.005084208212792873\n",
      "Epoch 1855, Loss: 0.03959690685587702, Final Batch Loss: 0.0006370117189362645\n",
      "Epoch 1856, Loss: 0.025415059848455712, Final Batch Loss: 0.000771704304497689\n",
      "Epoch 1857, Loss: 0.03423306223703548, Final Batch Loss: 0.0023532421328127384\n",
      "Epoch 1858, Loss: 0.01804998022998916, Final Batch Loss: 0.002385692438110709\n",
      "Epoch 1859, Loss: 0.012661787819524761, Final Batch Loss: 0.00016778438293840736\n",
      "Epoch 1860, Loss: 0.012479530218115542, Final Batch Loss: 0.0011574459495022893\n",
      "Epoch 1861, Loss: 0.009662284479418304, Final Batch Loss: 0.0014460202073678374\n",
      "Epoch 1862, Loss: 0.035304611854371615, Final Batch Loss: 0.00012679302017204463\n",
      "Epoch 1863, Loss: 0.030990245708380826, Final Batch Loss: 6.071592724765651e-05\n",
      "Epoch 1864, Loss: 0.035725017289223615, Final Batch Loss: 0.013051481917500496\n",
      "Epoch 1865, Loss: 0.10045693533174926, Final Batch Loss: 0.007833094336092472\n",
      "Epoch 1866, Loss: 0.11553765684948303, Final Batch Loss: 0.0032930211164057255\n",
      "Epoch 1867, Loss: 0.030354974514921196, Final Batch Loss: 0.0005814339383505285\n",
      "Epoch 1868, Loss: 0.1212712082779035, Final Batch Loss: 0.00774505315348506\n",
      "Epoch 1869, Loss: 0.05087158831884153, Final Batch Loss: 0.0006063202163204551\n",
      "Epoch 1870, Loss: 0.1303799551569682, Final Batch Loss: 0.009477321989834309\n",
      "Epoch 1871, Loss: 0.15489058689126978, Final Batch Loss: 0.009326490573585033\n",
      "Epoch 1872, Loss: 0.11033671846962534, Final Batch Loss: 0.002659992314875126\n",
      "Epoch 1873, Loss: 0.05604834125551861, Final Batch Loss: 0.020847667008638382\n",
      "Epoch 1874, Loss: 0.13650624237197917, Final Batch Loss: 0.017681406810879707\n",
      "Epoch 1875, Loss: 0.04599411238450557, Final Batch Loss: 0.0029351774137467146\n",
      "Epoch 1876, Loss: 0.030968948762165383, Final Batch Loss: 0.0033011867199093103\n",
      "Epoch 1877, Loss: 0.020766834742971696, Final Batch Loss: 0.00215216469950974\n",
      "Epoch 1878, Loss: 0.026907938517979346, Final Batch Loss: 0.002079030964523554\n",
      "Epoch 1879, Loss: 0.03321091085308581, Final Batch Loss: 0.0028507965616881847\n",
      "Epoch 1880, Loss: 0.04508121608523652, Final Batch Loss: 0.002023016568273306\n",
      "Epoch 1881, Loss: 0.019549629625544185, Final Batch Loss: 0.0004084606480319053\n",
      "Epoch 1882, Loss: 0.056453507975675166, Final Batch Loss: 0.004446324892342091\n",
      "Epoch 1883, Loss: 0.03605297583271749, Final Batch Loss: 0.0014846248086541891\n",
      "Epoch 1884, Loss: 0.03296712790324818, Final Batch Loss: 0.0059878034517169\n",
      "Epoch 1885, Loss: 0.01731252792524174, Final Batch Loss: 9.351193148177117e-05\n",
      "Epoch 1886, Loss: 0.029796430510032224, Final Batch Loss: 3.7908332160441205e-05\n",
      "Epoch 1887, Loss: 0.011598335331655107, Final Batch Loss: 0.0003014013636857271\n",
      "Epoch 1888, Loss: 0.031565998537189444, Final Batch Loss: 0.004089038819074631\n",
      "Epoch 1889, Loss: 0.04319906546152197, Final Batch Loss: 5.29067765455693e-05\n",
      "Epoch 1890, Loss: 0.041677385081129614, Final Batch Loss: 0.0002299458865309134\n",
      "Epoch 1891, Loss: 0.02364268326346064, Final Batch Loss: 0.00014173520321492106\n",
      "Epoch 1892, Loss: 0.012960305670276284, Final Batch Loss: 0.0001905691169667989\n",
      "Epoch 1893, Loss: 0.015155630106164608, Final Batch Loss: 0.00029535486828535795\n",
      "Epoch 1894, Loss: 0.030327284999657422, Final Batch Loss: 0.0003457005077507347\n",
      "Epoch 1895, Loss: 0.014951886481867405, Final Batch Loss: 0.0012673881137743592\n",
      "Epoch 1896, Loss: 0.008432152750174282, Final Batch Loss: 0.0003486974164843559\n",
      "Epoch 1897, Loss: 0.02595663173997309, Final Batch Loss: 6.370168557623401e-05\n",
      "Epoch 1898, Loss: 0.008070395813774667, Final Batch Loss: 6.464444595621899e-05\n",
      "Epoch 1899, Loss: 0.01935628974206338, Final Batch Loss: 2.5564562747604214e-05\n",
      "Epoch 1900, Loss: 0.021306294421265193, Final Batch Loss: 0.0005942372954450548\n",
      "Epoch 1901, Loss: 0.08293596533621894, Final Batch Loss: 0.01570131443440914\n",
      "Epoch 1902, Loss: 0.06158892057283083, Final Batch Loss: 0.0001960916561074555\n",
      "Epoch 1903, Loss: 0.061088241538527654, Final Batch Loss: 7.260420534294099e-05\n",
      "Epoch 1904, Loss: 0.035813958806102164, Final Batch Loss: 0.00042013550410047174\n",
      "Epoch 1905, Loss: 0.03730349896068219, Final Batch Loss: 0.0005233351257629693\n",
      "Epoch 1906, Loss: 0.018019709845248144, Final Batch Loss: 0.00018255844770465046\n",
      "Epoch 1907, Loss: 0.02311791116881068, Final Batch Loss: 0.00012704818800557405\n",
      "Epoch 1908, Loss: 0.006778049064450897, Final Batch Loss: 8.823178359307349e-05\n",
      "Epoch 1909, Loss: 0.014237443119782256, Final Batch Loss: 5.84839035582263e-05\n",
      "Epoch 1910, Loss: 0.013507744610251393, Final Batch Loss: 0.00015278029604814947\n",
      "Epoch 1911, Loss: 0.0617921041266527, Final Batch Loss: 0.0019737265538424253\n",
      "Epoch 1912, Loss: 0.011973136615779367, Final Batch Loss: 0.0004787900543306023\n",
      "Epoch 1913, Loss: 0.008507188485964434, Final Batch Loss: 3.923971962649375e-05\n",
      "Epoch 1914, Loss: 0.011995529517662362, Final Batch Loss: 0.0019885608926415443\n",
      "Epoch 1915, Loss: 0.037434082507388666, Final Batch Loss: 0.000695539521984756\n",
      "Epoch 1916, Loss: 0.014700156341859838, Final Batch Loss: 0.0004298407584428787\n",
      "Epoch 1917, Loss: 0.022567255684407428, Final Batch Loss: 7.907274994067848e-05\n",
      "Epoch 1918, Loss: 0.017879229999380186, Final Batch Loss: 0.00414314167574048\n",
      "Epoch 1919, Loss: 0.04443369573891687, Final Batch Loss: 0.0013683958677574992\n",
      "Epoch 1920, Loss: 0.03251251148321899, Final Batch Loss: 0.0006273769540712237\n",
      "Epoch 1921, Loss: 0.02869850537445018, Final Batch Loss: 1.462395357521018e-05\n",
      "Epoch 1922, Loss: 0.052482179749858915, Final Batch Loss: 0.00010145040869247168\n",
      "Epoch 1923, Loss: 0.03436923667322844, Final Batch Loss: 6.36522236163728e-05\n",
      "Epoch 1924, Loss: 0.020270681710826466, Final Batch Loss: 0.00012133376731071621\n",
      "Epoch 1925, Loss: 0.03474204078884213, Final Batch Loss: 0.0019294539233669639\n",
      "Epoch 1926, Loss: 0.016989494950394146, Final Batch Loss: 0.0015763220144435763\n",
      "Epoch 1927, Loss: 0.015272107048076577, Final Batch Loss: 0.0002609765506349504\n",
      "Epoch 1928, Loss: 0.006823686620919034, Final Batch Loss: 0.0002727900864556432\n",
      "Epoch 1929, Loss: 0.015227233809127938, Final Batch Loss: 0.004109160043299198\n",
      "Epoch 1930, Loss: 0.03537584301375318, Final Batch Loss: 0.0011647675419226289\n",
      "Epoch 1931, Loss: 0.05144384022423765, Final Batch Loss: 0.0008082989370450377\n",
      "Epoch 1932, Loss: 0.04551670843648026, Final Batch Loss: 6.717026553815231e-05\n",
      "Epoch 1933, Loss: 0.03809093807285535, Final Batch Loss: 0.0002131231449311599\n",
      "Epoch 1934, Loss: 0.013657639852681314, Final Batch Loss: 4.605914364219643e-05\n",
      "Epoch 1935, Loss: 0.012583822579472326, Final Batch Loss: 0.00025636309874244034\n",
      "Epoch 1936, Loss: 0.017621133345528506, Final Batch Loss: 0.002756746718659997\n",
      "Epoch 1937, Loss: 0.04914113139966503, Final Batch Loss: 0.0007158901426009834\n",
      "Epoch 1938, Loss: 0.08753376725508133, Final Batch Loss: 0.0014805023092776537\n",
      "Epoch 1939, Loss: 0.018286117941897828, Final Batch Loss: 0.0002572226512711495\n",
      "Epoch 1940, Loss: 0.06342211936134845, Final Batch Loss: 0.0004668537003453821\n",
      "Epoch 1941, Loss: 0.15394877388462191, Final Batch Loss: 0.0029259873554110527\n",
      "Epoch 1942, Loss: 0.11490615411457838, Final Batch Loss: 0.030662618577480316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1943, Loss: 0.07554271047411021, Final Batch Loss: 0.0251499991863966\n",
      "Epoch 1944, Loss: 0.14438974834047258, Final Batch Loss: 0.005253847222775221\n",
      "Epoch 1945, Loss: 0.07043357239308534, Final Batch Loss: 0.00152311148121953\n",
      "Epoch 1946, Loss: 0.08745386070222594, Final Batch Loss: 0.0002931337512563914\n",
      "Epoch 1947, Loss: 0.07077162843779661, Final Batch Loss: 0.0006244095275178552\n",
      "Epoch 1948, Loss: 0.09668760234490037, Final Batch Loss: 0.016246603801846504\n",
      "Epoch 1949, Loss: 0.04430870780197438, Final Batch Loss: 0.0031861222814768553\n",
      "Epoch 1950, Loss: 0.05198608338832855, Final Batch Loss: 0.004430799745023251\n",
      "Epoch 1951, Loss: 0.09163139437441714, Final Batch Loss: 0.003320074640214443\n",
      "Epoch 1952, Loss: 0.06977771091624163, Final Batch Loss: 0.0015726012643426657\n",
      "Epoch 1953, Loss: 0.03107230925161275, Final Batch Loss: 0.0009947501821443439\n",
      "Epoch 1954, Loss: 0.017440199953853153, Final Batch Loss: 0.001971609191969037\n",
      "Epoch 1955, Loss: 0.021248662807920482, Final Batch Loss: 0.00021491797815542668\n",
      "Epoch 1956, Loss: 0.030926217914384324, Final Batch Loss: 0.0007714230450801551\n",
      "Epoch 1957, Loss: 0.02697847802483011, Final Batch Loss: 0.0005364300450310111\n",
      "Epoch 1958, Loss: 0.0150257299101213, Final Batch Loss: 0.0002101973950630054\n",
      "Epoch 1959, Loss: 0.02734834421244159, Final Batch Loss: 0.0019542824011296034\n",
      "Epoch 1960, Loss: 0.020581971693900414, Final Batch Loss: 0.002147957216948271\n",
      "Epoch 1961, Loss: 0.016432691969384905, Final Batch Loss: 0.0001816421136027202\n",
      "Epoch 1962, Loss: 0.01490618801835808, Final Batch Loss: 0.00041871509165503085\n",
      "Epoch 1963, Loss: 0.02234920139744645, Final Batch Loss: 0.0002820083755068481\n",
      "Epoch 1964, Loss: 0.03340708982068463, Final Batch Loss: 0.0016094435704872012\n",
      "Epoch 1965, Loss: 0.030308471352327615, Final Batch Loss: 0.00026648162747733295\n",
      "Epoch 1966, Loss: 0.05104502768153907, Final Batch Loss: 9.150772530119866e-05\n",
      "Epoch 1967, Loss: 0.02686463847203413, Final Batch Loss: 0.0003595041634980589\n",
      "Epoch 1968, Loss: 0.05188271754741436, Final Batch Loss: 0.0006748721352778375\n",
      "Epoch 1969, Loss: 0.03818413162662182, Final Batch Loss: 0.0016416542930528522\n",
      "Epoch 1970, Loss: 0.015261850832757773, Final Batch Loss: 0.00017059150559362024\n",
      "Epoch 1971, Loss: 0.04552903477451764, Final Batch Loss: 0.00020418754138518125\n",
      "Epoch 1972, Loss: 0.07737465636455454, Final Batch Loss: 0.006393559742718935\n",
      "Epoch 1973, Loss: 0.16132357661263086, Final Batch Loss: 0.01969265379011631\n",
      "Epoch 1974, Loss: 0.08046990909497254, Final Batch Loss: 0.0035711529199033976\n",
      "Epoch 1975, Loss: 0.10037374834064394, Final Batch Loss: 0.025709694251418114\n",
      "Epoch 1976, Loss: 0.039217949379235506, Final Batch Loss: 0.003074787789955735\n",
      "Epoch 1977, Loss: 0.031238949377438985, Final Batch Loss: 0.00016907531244214624\n",
      "Epoch 1978, Loss: 0.10348231051466428, Final Batch Loss: 0.008682613261044025\n",
      "Epoch 1979, Loss: 0.036263801594031975, Final Batch Loss: 0.0020798665937036276\n",
      "Epoch 1980, Loss: 0.032966320068226196, Final Batch Loss: 0.0016965587856248021\n",
      "Epoch 1981, Loss: 0.0195115556489327, Final Batch Loss: 0.0014033351326361299\n",
      "Epoch 1982, Loss: 0.018726959875493776, Final Batch Loss: 0.00011486755829537287\n",
      "Epoch 1983, Loss: 0.02525302770663984, Final Batch Loss: 0.001455034245736897\n",
      "Epoch 1984, Loss: 0.02741913392674178, Final Batch Loss: 0.00023028124996926636\n",
      "Epoch 1985, Loss: 0.052974465739680454, Final Batch Loss: 0.0026102005504071712\n",
      "Epoch 1986, Loss: 0.0783677435683785, Final Batch Loss: 0.002084774198010564\n",
      "Epoch 1987, Loss: 0.06206907649175264, Final Batch Loss: 0.000292690412607044\n",
      "Epoch 1988, Loss: 0.0904841058945749, Final Batch Loss: 0.004268527962267399\n",
      "Epoch 1989, Loss: 0.07002214345993707, Final Batch Loss: 0.004938858095556498\n",
      "Epoch 1990, Loss: 0.11034660710720345, Final Batch Loss: 0.005264041479676962\n",
      "Epoch 1991, Loss: 0.06602167541132076, Final Batch Loss: 0.0032896853517740965\n",
      "Epoch 1992, Loss: 0.03413814639498014, Final Batch Loss: 0.00027302870876155794\n",
      "Epoch 1993, Loss: 0.022194566772668622, Final Batch Loss: 0.002117944648489356\n",
      "Epoch 1994, Loss: 0.017363131846650504, Final Batch Loss: 0.000703081488609314\n",
      "Epoch 1995, Loss: 0.02366382709442405, Final Batch Loss: 4.4167540181661025e-05\n",
      "Epoch 1996, Loss: 0.0230198152421508, Final Batch Loss: 0.0014892097096890211\n",
      "Epoch 1997, Loss: 0.02943124717057799, Final Batch Loss: 0.004237429238855839\n",
      "Epoch 1998, Loss: 0.03965064888689085, Final Batch Loss: 0.02664637193083763\n",
      "Epoch 1999, Loss: 0.022022341603587847, Final Batch Loss: 0.0009079410810954869\n",
      "Epoch 2000, Loss: 0.014411508534976747, Final Batch Loss: 0.000288849143544212\n",
      "Epoch 2001, Loss: 0.017660341520240763, Final Batch Loss: 0.0001835065195336938\n",
      "Epoch 2002, Loss: 0.01320408760511782, Final Batch Loss: 0.007856487296521664\n",
      "Epoch 2003, Loss: 0.034044855179672595, Final Batch Loss: 0.01956906169652939\n",
      "Epoch 2004, Loss: 0.04428079721037648, Final Batch Loss: 0.0014380567008629441\n",
      "Epoch 2005, Loss: 0.040788538244669326, Final Batch Loss: 0.007621067110449076\n",
      "Epoch 2006, Loss: 0.10943870768096531, Final Batch Loss: 0.00027200087788514793\n",
      "Epoch 2007, Loss: 0.062491930890246294, Final Batch Loss: 0.0029675317928195\n",
      "Epoch 2008, Loss: 0.08322872495045885, Final Batch Loss: 0.03969622403383255\n",
      "Epoch 2009, Loss: 0.06705448217689991, Final Batch Loss: 0.00013057001342531294\n",
      "Epoch 2010, Loss: 0.07138470649078954, Final Batch Loss: 0.004733305424451828\n",
      "Epoch 2011, Loss: 0.0910049573321885, Final Batch Loss: 0.0036661981139332056\n",
      "Epoch 2012, Loss: 0.13006370769289788, Final Batch Loss: 0.003364244708791375\n",
      "Epoch 2013, Loss: 0.10353438806487247, Final Batch Loss: 0.0042842235416173935\n",
      "Epoch 2014, Loss: 0.039692734921118245, Final Batch Loss: 0.0002779167552944273\n",
      "Epoch 2015, Loss: 0.018196133882156573, Final Batch Loss: 0.0007782423053868115\n",
      "Epoch 2016, Loss: 0.12637034626095556, Final Batch Loss: 0.00022795252152718604\n",
      "Epoch 2017, Loss: 0.023643520718906075, Final Batch Loss: 0.00010158408258575946\n",
      "Epoch 2018, Loss: 0.031588175072101876, Final Batch Loss: 0.0002128322666976601\n",
      "Epoch 2019, Loss: 0.06011985232908046, Final Batch Loss: 0.025717830285429955\n",
      "Epoch 2020, Loss: 0.017218392895301804, Final Batch Loss: 0.0010920355562120676\n",
      "Epoch 2021, Loss: 0.02418323371966835, Final Batch Loss: 0.008551524020731449\n",
      "Epoch 2022, Loss: 0.05433061944495421, Final Batch Loss: 0.0002086704334942624\n",
      "Epoch 2023, Loss: 0.03747025132543058, Final Batch Loss: 0.0006555105210281909\n",
      "Epoch 2024, Loss: 0.04169663385982858, Final Batch Loss: 0.0010635756189003587\n",
      "Epoch 2025, Loss: 0.03965415903076064, Final Batch Loss: 0.026157984510064125\n",
      "Epoch 2026, Loss: 0.04070852630684385, Final Batch Loss: 0.0007371496176347136\n",
      "Epoch 2027, Loss: 0.02154813320885296, Final Batch Loss: 0.0012555898865684867\n",
      "Epoch 2028, Loss: 0.02535929568693973, Final Batch Loss: 0.0009755989885888994\n",
      "Epoch 2029, Loss: 0.013301708218932617, Final Batch Loss: 0.00014111465134192258\n",
      "Epoch 2030, Loss: 0.05512780170101905, Final Batch Loss: 0.0003321513067930937\n",
      "Epoch 2031, Loss: 0.021370054215367418, Final Batch Loss: 0.00012306112330406904\n",
      "Epoch 2032, Loss: 0.016953525242570322, Final Batch Loss: 5.92171709286049e-05\n",
      "Epoch 2033, Loss: 0.028894893868709914, Final Batch Loss: 0.0005518215475603938\n",
      "Epoch 2034, Loss: 0.021176020294660702, Final Batch Loss: 0.012584589421749115\n",
      "Epoch 2035, Loss: 0.021471578656928614, Final Batch Loss: 7.826232467778027e-05\n",
      "Epoch 2036, Loss: 0.05828721131911152, Final Batch Loss: 0.0003884002799168229\n",
      "Epoch 2037, Loss: 0.021775769761006813, Final Batch Loss: 0.00013263427536003292\n",
      "Epoch 2038, Loss: 0.03835136019188212, Final Batch Loss: 0.006610468961298466\n",
      "Epoch 2039, Loss: 0.01854805077164201, Final Batch Loss: 0.00031997368205338717\n",
      "Epoch 2040, Loss: 0.025852669321466237, Final Batch Loss: 0.0011839309008792043\n",
      "Epoch 2041, Loss: 0.01766162977401109, Final Batch Loss: 0.0009574332507327199\n",
      "Epoch 2042, Loss: 0.05853465986729134, Final Batch Loss: 0.0008395040058530867\n",
      "Epoch 2043, Loss: 0.02015521313296631, Final Batch Loss: 0.0007900726050138474\n",
      "Epoch 2044, Loss: 0.022726419163518585, Final Batch Loss: 0.0027276810724288225\n",
      "Epoch 2045, Loss: 0.016593140520853922, Final Batch Loss: 0.001617081230506301\n",
      "Epoch 2046, Loss: 0.01663762038515415, Final Batch Loss: 0.0005666525685228407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2047, Loss: 0.006717632772051729, Final Batch Loss: 6.507024227175862e-05\n",
      "Epoch 2048, Loss: 0.055113620721385814, Final Batch Loss: 0.035842414945364\n",
      "Epoch 2049, Loss: 0.04484384572970157, Final Batch Loss: 0.0005841205711476505\n",
      "Epoch 2050, Loss: 0.08504529388665105, Final Batch Loss: 0.0004485448298510164\n",
      "Epoch 2051, Loss: 0.07531931231642375, Final Batch Loss: 4.5646003854926676e-05\n",
      "Epoch 2052, Loss: 0.08255613944493234, Final Batch Loss: 0.0007402546470984817\n",
      "Epoch 2053, Loss: 0.036357548015075736, Final Batch Loss: 0.0021729853469878435\n",
      "Epoch 2054, Loss: 0.024768414376012515, Final Batch Loss: 0.008264326490461826\n",
      "Epoch 2055, Loss: 0.027743909591663396, Final Batch Loss: 0.00501426262781024\n",
      "Epoch 2056, Loss: 0.03961171975242905, Final Batch Loss: 0.00020889990264549851\n",
      "Epoch 2057, Loss: 0.041471250382528524, Final Batch Loss: 0.00043245108099654317\n",
      "Epoch 2058, Loss: 0.029673486591491383, Final Batch Loss: 0.0026187545154243708\n",
      "Epoch 2059, Loss: 0.0167105883883778, Final Batch Loss: 0.00023433750902768224\n",
      "Epoch 2060, Loss: 0.01865344234465738, Final Batch Loss: 0.0027475410606712103\n",
      "Epoch 2061, Loss: 0.00959959375904873, Final Batch Loss: 0.0018269055290147662\n",
      "Epoch 2062, Loss: 0.024731150537263602, Final Batch Loss: 0.0029809479601681232\n",
      "Epoch 2063, Loss: 0.02868310222402215, Final Batch Loss: 0.00011430862650740892\n",
      "Epoch 2064, Loss: 0.007450049875842524, Final Batch Loss: 0.00011675238056341186\n",
      "Epoch 2065, Loss: 0.005660650534991873, Final Batch Loss: 0.00027055744430981576\n",
      "Epoch 2066, Loss: 0.024339717911971093, Final Batch Loss: 0.00016957795014604926\n",
      "Epoch 2067, Loss: 0.009018318032758543, Final Batch Loss: 6.693180330330506e-05\n",
      "Epoch 2068, Loss: 0.011642367999229464, Final Batch Loss: 0.00017967309395316988\n",
      "Epoch 2069, Loss: 0.027867379852978047, Final Batch Loss: 0.008406870998442173\n",
      "Epoch 2070, Loss: 0.008848511974065332, Final Batch Loss: 3.977339656557888e-05\n",
      "Epoch 2071, Loss: 0.02473010355606675, Final Batch Loss: 0.0007972128805704415\n",
      "Epoch 2072, Loss: 0.010082777349452954, Final Batch Loss: 0.0005146597977727652\n",
      "Epoch 2073, Loss: 0.05966576275386615, Final Batch Loss: 0.016315894201397896\n",
      "Epoch 2074, Loss: 0.04665825506162946, Final Batch Loss: 0.000783214287366718\n",
      "Epoch 2075, Loss: 0.01763466787087964, Final Batch Loss: 0.0009662554366514087\n",
      "Epoch 2076, Loss: 0.028533737935504178, Final Batch Loss: 3.3018059184541926e-05\n",
      "Epoch 2077, Loss: 0.012267288329894654, Final Batch Loss: 0.00012318183144088835\n",
      "Epoch 2078, Loss: 0.014220672248484334, Final Batch Loss: 0.006782683078199625\n",
      "Epoch 2079, Loss: 0.027735739000490867, Final Batch Loss: 0.009320592507719994\n",
      "Epoch 2080, Loss: 0.07643684145295992, Final Batch Loss: 0.0002240501344203949\n",
      "Epoch 2081, Loss: 0.014715883778990246, Final Batch Loss: 0.0035407296381890774\n",
      "Epoch 2082, Loss: 0.015933455866615986, Final Batch Loss: 0.0002120796707458794\n",
      "Epoch 2083, Loss: 0.023891302840638673, Final Batch Loss: 0.000493550905957818\n",
      "Epoch 2084, Loss: 0.06349048306583427, Final Batch Loss: 0.0003945567295886576\n",
      "Epoch 2085, Loss: 0.01782517400715733, Final Batch Loss: 0.00013330650108400732\n",
      "Epoch 2086, Loss: 0.11729760534944944, Final Batch Loss: 0.020131194964051247\n",
      "Epoch 2087, Loss: 0.06720641459833132, Final Batch Loss: 0.0005942868301644921\n",
      "Epoch 2088, Loss: 0.03420098866263288, Final Batch Loss: 0.011975351721048355\n",
      "Epoch 2089, Loss: 0.017434185414458625, Final Batch Loss: 0.002222983632236719\n",
      "Epoch 2090, Loss: 0.030427880032220855, Final Batch Loss: 0.0003722347319126129\n",
      "Epoch 2091, Loss: 0.00865532762145449, Final Batch Loss: 0.00016577879432588816\n",
      "Epoch 2092, Loss: 0.007557925448054448, Final Batch Loss: 0.0011854969197884202\n",
      "Epoch 2093, Loss: 0.020908215039526112, Final Batch Loss: 0.0037744815926998854\n",
      "Epoch 2094, Loss: 0.12454421546499361, Final Batch Loss: 4.666090535465628e-05\n",
      "Epoch 2095, Loss: 0.07337194819410797, Final Batch Loss: 0.0016844826750457287\n",
      "Epoch 2096, Loss: 0.11455729772569612, Final Batch Loss: 0.013035719282925129\n",
      "Epoch 2097, Loss: 0.03209626310490421, Final Batch Loss: 0.0002450333267915994\n",
      "Epoch 2098, Loss: 0.02653679841023404, Final Batch Loss: 0.0012269937433302402\n",
      "Epoch 2099, Loss: 0.028465812236390775, Final Batch Loss: 0.0002906918234657496\n",
      "Epoch 2100, Loss: 0.045232749049318954, Final Batch Loss: 0.00020845730614382774\n",
      "Epoch 2101, Loss: 0.03316986923891818, Final Batch Loss: 0.00021923996973782778\n",
      "Epoch 2102, Loss: 0.07960531294520479, Final Batch Loss: 0.0006766938604414463\n",
      "Epoch 2103, Loss: 0.04690806708094897, Final Batch Loss: 0.0005707892123609781\n",
      "Epoch 2104, Loss: 0.05588126066868426, Final Batch Loss: 0.005096916109323502\n",
      "Epoch 2105, Loss: 0.01538731931213988, Final Batch Loss: 0.003970579709857702\n",
      "Epoch 2106, Loss: 0.023245148768182844, Final Batch Loss: 0.009810727089643478\n",
      "Epoch 2107, Loss: 0.021844389906618744, Final Batch Loss: 0.0010034017032012343\n",
      "Epoch 2108, Loss: 0.022770489333197474, Final Batch Loss: 0.0007810978568159044\n",
      "Epoch 2109, Loss: 0.011322428894345649, Final Batch Loss: 0.0006171754212118685\n",
      "Epoch 2110, Loss: 0.01715565261474694, Final Batch Loss: 0.00024325316189788282\n",
      "Epoch 2111, Loss: 0.029071539116557688, Final Batch Loss: 0.00025144772371277213\n",
      "Epoch 2112, Loss: 0.043963279246781894, Final Batch Loss: 0.003519604215398431\n",
      "Epoch 2113, Loss: 0.018487918009668647, Final Batch Loss: 0.0005755226011388004\n",
      "Epoch 2114, Loss: 0.057808587847830495, Final Batch Loss: 0.0006028604693710804\n",
      "Epoch 2115, Loss: 0.041889595668180846, Final Batch Loss: 0.002158490475267172\n",
      "Epoch 2116, Loss: 0.019812922346318373, Final Batch Loss: 0.0005180756561458111\n",
      "Epoch 2117, Loss: 0.023438354044628795, Final Batch Loss: 0.00018239658675156534\n",
      "Epoch 2118, Loss: 0.010677787540771533, Final Batch Loss: 8.146536129061133e-05\n",
      "Epoch 2119, Loss: 0.03894140061674989, Final Batch Loss: 0.00011006036947946995\n",
      "Epoch 2120, Loss: 0.005074245269497624, Final Batch Loss: 0.00016105329268611968\n",
      "Epoch 2121, Loss: 0.005206460786212119, Final Batch Loss: 0.00018802238628268242\n",
      "Epoch 2122, Loss: 0.051185884318329045, Final Batch Loss: 0.0005579126882366836\n",
      "Epoch 2123, Loss: 0.01427238769701944, Final Batch Loss: 0.001714771264232695\n",
      "Epoch 2124, Loss: 0.026397701425594278, Final Batch Loss: 3.7297715607564896e-05\n",
      "Epoch 2125, Loss: 0.05169747760010068, Final Batch Loss: 0.0057563758455216885\n",
      "Epoch 2126, Loss: 0.03296057957049925, Final Batch Loss: 0.023708991706371307\n",
      "Epoch 2127, Loss: 0.03216268427786417, Final Batch Loss: 0.001909935032017529\n",
      "Epoch 2128, Loss: 0.0707552964813658, Final Batch Loss: 0.002069452777504921\n",
      "Epoch 2129, Loss: 0.04691233759513125, Final Batch Loss: 0.00024061955627985299\n",
      "Epoch 2130, Loss: 0.022453237834270112, Final Batch Loss: 0.0051842317916452885\n",
      "Epoch 2131, Loss: 0.028361284064885695, Final Batch Loss: 0.00038338429294526577\n",
      "Epoch 2132, Loss: 0.04240856459364295, Final Batch Loss: 0.00016490121197421104\n",
      "Epoch 2133, Loss: 0.06325387150718598, Final Batch Loss: 0.00749207753688097\n",
      "Epoch 2134, Loss: 0.07849677685953793, Final Batch Loss: 0.0027218915056437254\n",
      "Epoch 2135, Loss: 0.07251720549538732, Final Batch Loss: 0.0006477638380602002\n",
      "Epoch 2136, Loss: 0.05420074331050273, Final Batch Loss: 0.00017153182125184685\n",
      "Epoch 2137, Loss: 0.021556471212534234, Final Batch Loss: 0.0011841548839583993\n",
      "Epoch 2138, Loss: 0.014865269360598177, Final Batch Loss: 0.0019559331703931093\n",
      "Epoch 2139, Loss: 0.0327717272011796, Final Batch Loss: 0.00023135828087106347\n",
      "Epoch 2140, Loss: 0.029334746999666095, Final Batch Loss: 0.00479263486340642\n",
      "Epoch 2141, Loss: 0.01665755944122793, Final Batch Loss: 0.0012357401428744197\n",
      "Epoch 2142, Loss: 0.03737865190487355, Final Batch Loss: 0.0009192513534799218\n",
      "Epoch 2143, Loss: 0.02414917709029396, Final Batch Loss: 0.0016619028756394982\n",
      "Epoch 2144, Loss: 0.04509945350946509, Final Batch Loss: 0.0036575128324329853\n",
      "Epoch 2145, Loss: 0.07275623906753026, Final Batch Loss: 0.0016866582445800304\n",
      "Epoch 2146, Loss: 0.014820017138845287, Final Batch Loss: 0.000217832435737364\n",
      "Epoch 2147, Loss: 0.011800934349594172, Final Batch Loss: 0.0023821608629077673\n",
      "Epoch 2148, Loss: 0.01083557320998807, Final Batch Loss: 0.0019490736303851008\n",
      "Epoch 2149, Loss: 0.011962193260842469, Final Batch Loss: 6.381006824085489e-05\n",
      "Epoch 2150, Loss: 0.008356337726581842, Final Batch Loss: 0.002405292121693492\n",
      "Epoch 2151, Loss: 0.018510418152800412, Final Batch Loss: 0.0004917861078865826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2152, Loss: 0.03835651188637712, Final Batch Loss: 0.00395034858956933\n",
      "Epoch 2153, Loss: 0.025627498751418898, Final Batch Loss: 0.004892691969871521\n",
      "Epoch 2154, Loss: 0.03094099695317709, Final Batch Loss: 0.00020199560094624758\n",
      "Epoch 2155, Loss: 0.017420205476810224, Final Batch Loss: 0.00032234092941507697\n",
      "Epoch 2156, Loss: 0.030030718738998985, Final Batch Loss: 0.00015496808919124305\n",
      "Epoch 2157, Loss: 0.012221610822962248, Final Batch Loss: 0.0006240696529857814\n",
      "Epoch 2158, Loss: 0.045460455658030696, Final Batch Loss: 4.873682701145299e-05\n",
      "Epoch 2159, Loss: 0.01502535842882935, Final Batch Loss: 0.00022653615451417863\n",
      "Epoch 2160, Loss: 0.005496222835063236, Final Batch Loss: 6.871711229905486e-05\n",
      "Epoch 2161, Loss: 0.01622804472935968, Final Batch Loss: 0.0014097027014940977\n",
      "Epoch 2162, Loss: 0.09098035759234335, Final Batch Loss: 0.0013985676923766732\n",
      "Epoch 2163, Loss: 0.034475179487344576, Final Batch Loss: 0.0011060084216296673\n",
      "Epoch 2164, Loss: 0.1561341259512119, Final Batch Loss: 0.004367389716207981\n",
      "Epoch 2165, Loss: 0.021448225455969805, Final Batch Loss: 0.00023078786034602672\n",
      "Epoch 2166, Loss: 0.11663972713722615, Final Batch Loss: 7.715696847299114e-05\n",
      "Epoch 2167, Loss: 0.018305578661966138, Final Batch Loss: 0.0004188714374322444\n",
      "Epoch 2168, Loss: 0.01697727826103801, Final Batch Loss: 0.0008827230776660144\n",
      "Epoch 2169, Loss: 0.06844394397194264, Final Batch Loss: 0.003992313519120216\n",
      "Epoch 2170, Loss: 0.03196089888660936, Final Batch Loss: 0.0008948643808253109\n",
      "Epoch 2171, Loss: 0.013693523149413522, Final Batch Loss: 0.0004371727118268609\n",
      "Epoch 2172, Loss: 0.038511977254529484, Final Batch Loss: 0.0001623211137484759\n",
      "Epoch 2173, Loss: 0.008511999563779682, Final Batch Loss: 0.003337002359330654\n",
      "Epoch 2174, Loss: 0.004970061621861532, Final Batch Loss: 0.001616142806597054\n",
      "Epoch 2175, Loss: 0.00587592648298596, Final Batch Loss: 4.196267400402576e-05\n",
      "Epoch 2176, Loss: 0.004894254430837464, Final Batch Loss: 6.271806341828778e-05\n",
      "Epoch 2177, Loss: 0.010803407820276334, Final Batch Loss: 0.0002701238845475018\n",
      "Epoch 2178, Loss: 0.013488264798070304, Final Batch Loss: 4.7861798520898446e-05\n",
      "Epoch 2179, Loss: 0.03261495650440338, Final Batch Loss: 0.0008098621619865298\n",
      "Epoch 2180, Loss: 0.00734941635892028, Final Batch Loss: 0.0009000067948363721\n",
      "Epoch 2181, Loss: 0.010966780864691827, Final Batch Loss: 0.000643066770862788\n",
      "Epoch 2182, Loss: 0.010230946898445836, Final Batch Loss: 0.00018049319623969495\n",
      "Epoch 2183, Loss: 0.011544347922608722, Final Batch Loss: 6.92418179824017e-05\n",
      "Epoch 2184, Loss: 0.01018165034474805, Final Batch Loss: 5.731138662667945e-05\n",
      "Epoch 2185, Loss: 0.012112897598854033, Final Batch Loss: 0.0004415700677782297\n",
      "Epoch 2186, Loss: 0.02464940857680631, Final Batch Loss: 0.00034804680035449564\n",
      "Epoch 2187, Loss: 0.09365952883672435, Final Batch Loss: 0.0001489219139330089\n",
      "Epoch 2188, Loss: 0.0522561433390365, Final Batch Loss: 0.0018154144054278731\n",
      "Epoch 2189, Loss: 0.024176101011107676, Final Batch Loss: 0.00014163977175485343\n",
      "Epoch 2190, Loss: 0.036879950308502885, Final Batch Loss: 0.00032307911897078156\n",
      "Epoch 2191, Loss: 0.009667934536992107, Final Batch Loss: 0.0007697343826293945\n",
      "Epoch 2192, Loss: 0.0162688136624638, Final Batch Loss: 0.0007833766285330057\n",
      "Epoch 2193, Loss: 0.03203368853428401, Final Batch Loss: 0.00045402071555145085\n",
      "Epoch 2194, Loss: 0.009691741510323482, Final Batch Loss: 5.7378918427275494e-05\n",
      "Epoch 2195, Loss: 0.010603555056150071, Final Batch Loss: 0.0003225197724532336\n",
      "Epoch 2196, Loss: 0.014151544492051471, Final Batch Loss: 0.00028306254534982145\n",
      "Epoch 2197, Loss: 0.007848170844226843, Final Batch Loss: 0.00015235705359373242\n",
      "Epoch 2198, Loss: 0.032793719810797484, Final Batch Loss: 0.0001999269297812134\n",
      "Epoch 2199, Loss: 0.018489471294742543, Final Batch Loss: 0.00023693544790148735\n",
      "Epoch 2200, Loss: 0.042744719357870053, Final Batch Loss: 0.002353029092773795\n",
      "Epoch 2201, Loss: 0.013827908933308208, Final Batch Loss: 0.004534825682640076\n",
      "Epoch 2202, Loss: 0.1099778250536474, Final Batch Loss: 0.0018011338543146849\n",
      "Epoch 2203, Loss: 0.10942235769289255, Final Batch Loss: 8.645331399748102e-05\n",
      "Epoch 2204, Loss: 0.07706416968721896, Final Batch Loss: 0.0014826558763161302\n",
      "Epoch 2205, Loss: 0.04212940073921345, Final Batch Loss: 0.0006372974603436887\n",
      "Epoch 2206, Loss: 0.041859703065711074, Final Batch Loss: 0.004201375879347324\n",
      "Epoch 2207, Loss: 0.05160178140795324, Final Batch Loss: 0.0025549011770635843\n",
      "Epoch 2208, Loss: 0.019842186069581658, Final Batch Loss: 0.00027350784512236714\n",
      "Epoch 2209, Loss: 0.041579821991035715, Final Batch Loss: 0.00034568493720144033\n",
      "Epoch 2210, Loss: 0.036422895318537485, Final Batch Loss: 0.0007642614655196667\n",
      "Epoch 2211, Loss: 0.03526592184061883, Final Batch Loss: 0.0002696299343369901\n",
      "Epoch 2212, Loss: 0.02033944369031815, Final Batch Loss: 0.00010772988753160462\n",
      "Epoch 2213, Loss: 0.013488545155269094, Final Batch Loss: 0.000589641451369971\n",
      "Epoch 2214, Loss: 0.037328842086935765, Final Batch Loss: 0.004685893654823303\n",
      "Epoch 2215, Loss: 0.02955789643965545, Final Batch Loss: 0.015514683909714222\n",
      "Epoch 2216, Loss: 0.015703282406320795, Final Batch Loss: 0.0038433996960520744\n",
      "Epoch 2217, Loss: 0.01784733956446871, Final Batch Loss: 0.0001540889061288908\n",
      "Epoch 2218, Loss: 0.028616295392566826, Final Batch Loss: 0.0006534311687573791\n",
      "Epoch 2219, Loss: 0.02621247567003593, Final Batch Loss: 0.006011819001287222\n",
      "Epoch 2220, Loss: 0.1865824665856053, Final Batch Loss: 0.034611254930496216\n",
      "Epoch 2221, Loss: 0.22934924039873295, Final Batch Loss: 0.00043878634460270405\n",
      "Epoch 2222, Loss: 0.13464944588486105, Final Batch Loss: 0.034522611647844315\n",
      "Epoch 2223, Loss: 0.04195238443207927, Final Batch Loss: 0.00708278501406312\n",
      "Epoch 2224, Loss: 0.038766598590882495, Final Batch Loss: 0.0012657113838940859\n",
      "Epoch 2225, Loss: 0.07401781680528075, Final Batch Loss: 0.0006998391472734511\n",
      "Epoch 2226, Loss: 0.10942045113188215, Final Batch Loss: 0.0024489322677254677\n",
      "Epoch 2227, Loss: 0.04168919750372879, Final Batch Loss: 0.0016973131569102407\n",
      "Epoch 2228, Loss: 0.020576769900799263, Final Batch Loss: 0.00011433692998252809\n",
      "Epoch 2229, Loss: 0.06618002874893136, Final Batch Loss: 0.001120624947361648\n",
      "Epoch 2230, Loss: 0.047658743962529115, Final Batch Loss: 0.004950127098709345\n",
      "Epoch 2231, Loss: 0.02844896107853856, Final Batch Loss: 0.00381207256577909\n",
      "Epoch 2232, Loss: 0.032715453700802755, Final Batch Loss: 0.0001635662338230759\n",
      "Epoch 2233, Loss: 0.009646882543165702, Final Batch Loss: 0.00043361340067349374\n",
      "Epoch 2234, Loss: 0.036271006872993894, Final Batch Loss: 0.0013792294776067138\n",
      "Epoch 2235, Loss: 0.2217704729628167, Final Batch Loss: 0.0008204595069400966\n",
      "Epoch 2236, Loss: 0.03839251906174468, Final Batch Loss: 0.00030122004682198167\n",
      "Epoch 2237, Loss: 0.05777148243942065, Final Batch Loss: 0.029539452865719795\n",
      "Epoch 2238, Loss: 0.04808239359408617, Final Batch Loss: 0.0001290983345825225\n",
      "Epoch 2239, Loss: 0.03029598308785353, Final Batch Loss: 0.00031364348251372576\n",
      "Epoch 2240, Loss: 0.03066739792120643, Final Batch Loss: 0.00017714971909299493\n",
      "Epoch 2241, Loss: 0.013762500922894105, Final Batch Loss: 0.000833305879496038\n",
      "Epoch 2242, Loss: 0.04691145817923825, Final Batch Loss: 0.00012448169582057744\n",
      "Epoch 2243, Loss: 0.03531207963533234, Final Batch Loss: 0.00011208852083655074\n",
      "Epoch 2244, Loss: 0.04473055310518248, Final Batch Loss: 0.0008081973064690828\n",
      "Epoch 2245, Loss: 0.028616219649848063, Final Batch Loss: 0.0032722498290240765\n",
      "Epoch 2246, Loss: 0.010586487453110749, Final Batch Loss: 0.0015718971844762564\n",
      "Epoch 2247, Loss: 0.020798768113309052, Final Batch Loss: 0.00014178987476043403\n",
      "Epoch 2248, Loss: 0.01163942358471104, Final Batch Loss: 0.0002352690207771957\n",
      "Epoch 2249, Loss: 0.009679586364654824, Final Batch Loss: 4.981400343240239e-05\n",
      "Epoch 2250, Loss: 0.04580052522578626, Final Batch Loss: 0.0007421483751386404\n",
      "Epoch 2251, Loss: 0.059305744365701685, Final Batch Loss: 0.0001325272605754435\n",
      "Epoch 2252, Loss: 0.05028108613623772, Final Batch Loss: 0.003623571712523699\n",
      "Epoch 2253, Loss: 0.021780526520160493, Final Batch Loss: 0.0038817147724330425\n",
      "Epoch 2254, Loss: 0.011084086367191048, Final Batch Loss: 3.731830292963423e-05\n",
      "Epoch 2255, Loss: 0.03765944821861922, Final Batch Loss: 0.023738635703921318\n",
      "Epoch 2256, Loss: 0.01488204913039226, Final Batch Loss: 0.00017517770174890757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2257, Loss: 0.01912161606378504, Final Batch Loss: 8.979755511973053e-05\n",
      "Epoch 2258, Loss: 0.03746444677744876, Final Batch Loss: 0.0001571761240484193\n",
      "Epoch 2259, Loss: 0.052834885333140846, Final Batch Loss: 0.0006859455606900156\n",
      "Epoch 2260, Loss: 0.013038944798609009, Final Batch Loss: 0.0005764178931713104\n",
      "Epoch 2261, Loss: 0.04822703257377725, Final Batch Loss: 0.0018653905717656016\n",
      "Epoch 2262, Loss: 0.011277026162133552, Final Batch Loss: 0.002184180775657296\n",
      "Epoch 2263, Loss: 0.018336913006351097, Final Batch Loss: 0.0008503861026838422\n",
      "Epoch 2264, Loss: 0.01221071687177755, Final Batch Loss: 0.00011016809003194794\n",
      "Epoch 2265, Loss: 0.00417959269543644, Final Batch Loss: 0.00022784463362768292\n",
      "Epoch 2266, Loss: 0.03247854331857525, Final Batch Loss: 0.00037049033562652767\n",
      "Epoch 2267, Loss: 0.01794402130326489, Final Batch Loss: 0.0005251529510132968\n",
      "Epoch 2268, Loss: 0.009629081541788764, Final Batch Loss: 0.00012065791088389233\n",
      "Epoch 2269, Loss: 0.007196758480858989, Final Batch Loss: 0.0001005214944598265\n",
      "Epoch 2270, Loss: 0.008242513871664414, Final Batch Loss: 0.0009425570024177432\n",
      "Epoch 2271, Loss: 0.005543941255382379, Final Batch Loss: 2.4359405870200135e-05\n",
      "Epoch 2272, Loss: 0.006377365189109696, Final Batch Loss: 0.00019603321561589837\n",
      "Epoch 2273, Loss: 0.002823764321874478, Final Batch Loss: 0.0006738564115948975\n",
      "Epoch 2274, Loss: 0.015735178280010587, Final Batch Loss: 0.007990031503140926\n",
      "Epoch 2275, Loss: 0.012784217522494146, Final Batch Loss: 0.00026988860918208957\n",
      "Epoch 2276, Loss: 0.014689589117551805, Final Batch Loss: 6.720064993714914e-05\n",
      "Epoch 2277, Loss: 0.014208235188561957, Final Batch Loss: 0.000347721332218498\n",
      "Epoch 2278, Loss: 0.040065774472168414, Final Batch Loss: 0.002070304937660694\n",
      "Epoch 2279, Loss: 0.034733461627183715, Final Batch Loss: 0.00603838637471199\n",
      "Epoch 2280, Loss: 0.01657700615032809, Final Batch Loss: 0.0016356646083295345\n",
      "Epoch 2281, Loss: 0.04212895423552254, Final Batch Loss: 0.0008185600745491683\n",
      "Epoch 2282, Loss: 0.015933067945297807, Final Batch Loss: 0.005371388979256153\n",
      "Epoch 2283, Loss: 0.016821410034026485, Final Batch Loss: 0.00026102011906914413\n",
      "Epoch 2284, Loss: 0.06942049950157525, Final Batch Loss: 0.00015527536743320525\n",
      "Epoch 2285, Loss: 0.03933802320898394, Final Batch Loss: 0.002454482950270176\n",
      "Epoch 2286, Loss: 0.02087393822876038, Final Batch Loss: 0.00010067499533761293\n",
      "Epoch 2287, Loss: 0.014200499856087845, Final Batch Loss: 7.57234520278871e-05\n",
      "Epoch 2288, Loss: 0.028751416204613633, Final Batch Loss: 0.00039239961188286543\n",
      "Epoch 2289, Loss: 0.016722012471291237, Final Batch Loss: 0.0003331509360577911\n",
      "Epoch 2290, Loss: 0.030306481450679712, Final Batch Loss: 0.00020404800307005644\n",
      "Epoch 2291, Loss: 0.013957631002995186, Final Batch Loss: 0.0020199164282530546\n",
      "Epoch 2292, Loss: 0.04246217843683553, Final Batch Loss: 0.00019567811978049576\n",
      "Epoch 2293, Loss: 0.005803757008834509, Final Batch Loss: 0.0012493279064074159\n",
      "Epoch 2294, Loss: 0.022278988088146434, Final Batch Loss: 0.0008758300682529807\n",
      "Epoch 2295, Loss: 0.060049159023037646, Final Batch Loss: 1.9021405023522675e-05\n",
      "Epoch 2296, Loss: 0.08889632709906437, Final Batch Loss: 0.00043401471339166164\n",
      "Epoch 2297, Loss: 0.060803179985668976, Final Batch Loss: 4.050398274557665e-05\n",
      "Epoch 2298, Loss: 0.03729499055771157, Final Batch Loss: 0.0004648334579542279\n",
      "Epoch 2299, Loss: 0.023582478301250376, Final Batch Loss: 0.0020801706705242395\n",
      "Epoch 2300, Loss: 0.03930869348914712, Final Batch Loss: 0.0011323143262416124\n",
      "Epoch 2301, Loss: 0.038493483953061514, Final Batch Loss: 0.00026163950678892434\n",
      "Epoch 2302, Loss: 0.046668608498293906, Final Batch Loss: 0.00033694502781145275\n",
      "Epoch 2303, Loss: 0.0448032881540712, Final Batch Loss: 0.006643096450716257\n",
      "Epoch 2304, Loss: 0.04606885589964804, Final Batch Loss: 0.02156265266239643\n",
      "Epoch 2305, Loss: 0.03639459944679402, Final Batch Loss: 0.0038431608118116856\n",
      "Epoch 2306, Loss: 0.02409709589119302, Final Batch Loss: 8.223985059885308e-05\n",
      "Epoch 2307, Loss: 0.02072884612789494, Final Batch Loss: 0.0004848264798056334\n",
      "Epoch 2308, Loss: 0.023289215358090587, Final Batch Loss: 0.0012954870471730828\n",
      "Epoch 2309, Loss: 0.050799683085642755, Final Batch Loss: 0.0004367835645098239\n",
      "Epoch 2310, Loss: 0.08076944077038206, Final Batch Loss: 0.0004691492940764874\n",
      "Epoch 2311, Loss: 0.04239803760719951, Final Batch Loss: 0.01398853026330471\n",
      "Epoch 2312, Loss: 0.035749686023336835, Final Batch Loss: 0.00024481918080709875\n",
      "Epoch 2313, Loss: 0.027331320896337274, Final Batch Loss: 0.008342449553310871\n",
      "Epoch 2314, Loss: 0.07038176372407179, Final Batch Loss: 2.4740458684391342e-05\n",
      "Epoch 2315, Loss: 0.012727360197459348, Final Batch Loss: 0.00012158651952631772\n",
      "Epoch 2316, Loss: 0.02204440865898505, Final Batch Loss: 0.00014689663657918572\n",
      "Epoch 2317, Loss: 0.01790196672300226, Final Batch Loss: 0.0012368616880849004\n",
      "Epoch 2318, Loss: 0.009901962170260958, Final Batch Loss: 4.040947897010483e-05\n",
      "Epoch 2319, Loss: 0.05828309407661436, Final Batch Loss: 6.258391658775508e-05\n",
      "Epoch 2320, Loss: 0.037344200543884654, Final Batch Loss: 0.0008568227058276534\n",
      "Epoch 2321, Loss: 0.03448780594044365, Final Batch Loss: 9.037145355250686e-05\n",
      "Epoch 2322, Loss: 0.02761255530276685, Final Batch Loss: 7.02652832842432e-05\n",
      "Epoch 2323, Loss: 0.008697613326148712, Final Batch Loss: 0.000439005991211161\n",
      "Epoch 2324, Loss: 0.004966056381817907, Final Batch Loss: 0.0001610331964911893\n",
      "Epoch 2325, Loss: 0.013040219087997684, Final Batch Loss: 0.0001804633066058159\n",
      "Epoch 2326, Loss: 0.02479093606962124, Final Batch Loss: 0.013344413600862026\n",
      "Epoch 2327, Loss: 0.027714271307559102, Final Batch Loss: 0.0030511582735925913\n",
      "Epoch 2328, Loss: 0.011509234385812306, Final Batch Loss: 2.8122778530814685e-05\n",
      "Epoch 2329, Loss: 0.07064246508525684, Final Batch Loss: 5.201916064834222e-05\n",
      "Epoch 2330, Loss: 0.08679462005966343, Final Batch Loss: 0.0053791324608027935\n",
      "Epoch 2331, Loss: 0.05158645965275355, Final Batch Loss: 0.011429228819906712\n",
      "Epoch 2332, Loss: 0.07244976771471556, Final Batch Loss: 0.0002332956501049921\n",
      "Epoch 2333, Loss: 0.025075987017771695, Final Batch Loss: 0.00017701848992146552\n",
      "Epoch 2334, Loss: 0.10049641445220914, Final Batch Loss: 0.0001713212695904076\n",
      "Epoch 2335, Loss: 0.042627287228242494, Final Batch Loss: 0.00016614112246315926\n",
      "Epoch 2336, Loss: 0.015687687176978216, Final Batch Loss: 0.00024958394351415336\n",
      "Epoch 2337, Loss: 0.026174803817411885, Final Batch Loss: 0.0012287716381251812\n",
      "Epoch 2338, Loss: 0.026929494299110956, Final Batch Loss: 0.0003031001251656562\n",
      "Epoch 2339, Loss: 0.011369682113581803, Final Batch Loss: 0.002245695563033223\n",
      "Epoch 2340, Loss: 0.01927445055480348, Final Batch Loss: 0.000665151106659323\n",
      "Epoch 2341, Loss: 0.03220452206733171, Final Batch Loss: 0.0019420675234869123\n",
      "Epoch 2342, Loss: 0.12098424100258853, Final Batch Loss: 0.0011957647511735559\n",
      "Epoch 2343, Loss: 0.10773883373622084, Final Batch Loss: 0.000127365390653722\n",
      "Epoch 2344, Loss: 0.0682784055970842, Final Batch Loss: 0.00032586732413619757\n",
      "Epoch 2345, Loss: 0.046972229465609416, Final Batch Loss: 0.016834700480103493\n",
      "Epoch 2346, Loss: 0.05962750577600673, Final Batch Loss: 0.0002086930617224425\n",
      "Epoch 2347, Loss: 0.035800774916424416, Final Batch Loss: 0.00019612019241321832\n",
      "Epoch 2348, Loss: 0.023100689373677596, Final Batch Loss: 0.0009247796842828393\n",
      "Epoch 2349, Loss: 0.016295674584398512, Final Batch Loss: 0.0016501087229698896\n",
      "Epoch 2350, Loss: 0.01081950119623798, Final Batch Loss: 0.0012116316938772798\n",
      "Epoch 2351, Loss: 0.006580294524610508, Final Batch Loss: 0.00034603968379087746\n",
      "Epoch 2352, Loss: 0.0036808740769629367, Final Batch Loss: 6.487876817118376e-05\n",
      "Epoch 2353, Loss: 0.007128229855879908, Final Batch Loss: 5.2400857384782284e-05\n",
      "Epoch 2354, Loss: 0.016450240567792207, Final Batch Loss: 0.00015802914276719093\n",
      "Epoch 2355, Loss: 0.020680966103100218, Final Batch Loss: 0.0002555087849032134\n",
      "Epoch 2356, Loss: 0.022771406180254417, Final Batch Loss: 5.457464430946857e-05\n",
      "Epoch 2357, Loss: 0.020768297268659808, Final Batch Loss: 0.00018259177159052342\n",
      "Epoch 2358, Loss: 0.010377953363786219, Final Batch Loss: 0.001670013996772468\n",
      "Epoch 2359, Loss: 0.03712714111679816, Final Batch Loss: 0.0002604816691018641\n",
      "Epoch 2360, Loss: 0.04350443079601973, Final Batch Loss: 0.00015856405661907047\n",
      "Epoch 2361, Loss: 0.04255595904396614, Final Batch Loss: 0.008418189361691475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2362, Loss: 0.015174591033428442, Final Batch Loss: 0.00011963336146436632\n",
      "Epoch 2363, Loss: 0.04034489844343625, Final Batch Loss: 0.016811428591609\n",
      "Epoch 2364, Loss: 0.018338486523134634, Final Batch Loss: 0.0003583691723179072\n",
      "Epoch 2365, Loss: 0.01478441961990029, Final Batch Loss: 0.00012567604426294565\n",
      "Epoch 2366, Loss: 0.05054811613808852, Final Batch Loss: 0.00012215386959724128\n",
      "Epoch 2367, Loss: 0.06023136363000958, Final Batch Loss: 0.0022466795053333044\n",
      "Epoch 2368, Loss: 0.03902079878935183, Final Batch Loss: 0.0009242362575605512\n",
      "Epoch 2369, Loss: 0.008968366313638398, Final Batch Loss: 0.0005495530785992742\n",
      "Epoch 2370, Loss: 0.010045352171800914, Final Batch Loss: 1.5538038496742956e-05\n",
      "Epoch 2371, Loss: 0.015452107429155149, Final Batch Loss: 0.0009079719893634319\n",
      "Epoch 2372, Loss: 0.013072390292109048, Final Batch Loss: 1.415240785718197e-05\n",
      "Epoch 2373, Loss: 0.004216768062178744, Final Batch Loss: 0.0004991257446818054\n",
      "Epoch 2374, Loss: 0.015955041435063322, Final Batch Loss: 0.0002841791429091245\n",
      "Epoch 2375, Loss: 0.0033500472036394058, Final Batch Loss: 0.0001403415371896699\n",
      "Epoch 2376, Loss: 0.03252564107606304, Final Batch Loss: 2.9328688469831832e-05\n",
      "Epoch 2377, Loss: 0.014092594112298684, Final Batch Loss: 0.00019607644935604185\n",
      "Epoch 2378, Loss: 0.01392538958680234, Final Batch Loss: 5.539649646379985e-05\n",
      "Epoch 2379, Loss: 0.009658343333285302, Final Batch Loss: 0.00036609277594834566\n",
      "Epoch 2380, Loss: 0.02239339417883457, Final Batch Loss: 0.00031078472966328263\n",
      "Epoch 2381, Loss: 0.03679805172941997, Final Batch Loss: 0.0007353777764365077\n",
      "Epoch 2382, Loss: 0.05535874119414075, Final Batch Loss: 0.00017133734945673496\n",
      "Epoch 2383, Loss: 0.023842069120291853, Final Batch Loss: 4.5436001528287306e-05\n",
      "Epoch 2384, Loss: 0.028852325762272812, Final Batch Loss: 0.0028107757680118084\n",
      "Epoch 2385, Loss: 0.012976984624401666, Final Batch Loss: 0.000122224009828642\n",
      "Epoch 2386, Loss: 0.01701797798887128, Final Batch Loss: 0.00015322765102609992\n",
      "Epoch 2387, Loss: 0.007252725677972194, Final Batch Loss: 4.093911047675647e-05\n",
      "Epoch 2388, Loss: 0.014259915564252879, Final Batch Loss: 0.0017690621316432953\n",
      "Epoch 2389, Loss: 0.02286447056030738, Final Batch Loss: 0.003646017285063863\n",
      "Epoch 2390, Loss: 0.027030169374484103, Final Batch Loss: 0.002651339629665017\n",
      "Epoch 2391, Loss: 0.06072401784877002, Final Batch Loss: 9.26456632441841e-05\n",
      "Epoch 2392, Loss: 0.1589747089674347, Final Batch Loss: 0.0034210183657705784\n",
      "Epoch 2393, Loss: 0.03423674293480872, Final Batch Loss: 0.0027822109404951334\n",
      "Epoch 2394, Loss: 0.04590901781193679, Final Batch Loss: 0.0022199179511517286\n",
      "Epoch 2395, Loss: 0.014342467133246828, Final Batch Loss: 0.00028231856413185596\n",
      "Epoch 2396, Loss: 0.03233924167579971, Final Batch Loss: 0.0018763217376545072\n",
      "Epoch 2397, Loss: 0.01497927813034039, Final Batch Loss: 0.0005442700930871069\n",
      "Epoch 2398, Loss: 0.03717646970471833, Final Batch Loss: 0.00023576428066007793\n",
      "Epoch 2399, Loss: 0.018191793100413634, Final Batch Loss: 0.0014344669179990888\n",
      "Epoch 2400, Loss: 0.014490262445178814, Final Batch Loss: 0.00022422107576858252\n",
      "Epoch 2401, Loss: 0.00751099776243791, Final Batch Loss: 0.00015930106746964157\n",
      "Epoch 2402, Loss: 0.025209308023477206, Final Batch Loss: 0.00037459240411408246\n",
      "Epoch 2403, Loss: 0.026577440919936635, Final Batch Loss: 0.002627758076414466\n",
      "Epoch 2404, Loss: 0.03821496746968478, Final Batch Loss: 0.00023545126896351576\n",
      "Epoch 2405, Loss: 0.016789982917543966, Final Batch Loss: 0.0046717026270926\n",
      "Epoch 2406, Loss: 0.009987298715714132, Final Batch Loss: 0.00018229178385809064\n",
      "Epoch 2407, Loss: 0.005043239278165856, Final Batch Loss: 0.00034964067162945867\n",
      "Epoch 2408, Loss: 0.003082544913922902, Final Batch Loss: 0.0003541082260198891\n",
      "Epoch 2409, Loss: 0.011683237757097231, Final Batch Loss: 4.109046858502552e-05\n",
      "Epoch 2410, Loss: 0.05386232743421715, Final Batch Loss: 0.004989675246179104\n",
      "Epoch 2411, Loss: 0.024343628083443036, Final Batch Loss: 0.00042335010948590934\n",
      "Epoch 2412, Loss: 0.1222947589121759, Final Batch Loss: 0.00016273409710265696\n",
      "Epoch 2413, Loss: 0.05588426511530997, Final Batch Loss: 0.0007764198235236108\n",
      "Epoch 2414, Loss: 0.029950899159302935, Final Batch Loss: 0.0007923604571260512\n",
      "Epoch 2415, Loss: 0.024404090232565068, Final Batch Loss: 0.0002842220419552177\n",
      "Epoch 2416, Loss: 0.010770255510578863, Final Batch Loss: 0.0007288711494766176\n",
      "Epoch 2417, Loss: 0.12312977833789773, Final Batch Loss: 0.021979210898280144\n",
      "Epoch 2418, Loss: 0.0851265552919358, Final Batch Loss: 0.0016258214600384235\n",
      "Epoch 2419, Loss: 0.05561246335855685, Final Batch Loss: 0.0008134791860356927\n",
      "Epoch 2420, Loss: 0.037906254328845534, Final Batch Loss: 0.00306952022947371\n",
      "Epoch 2421, Loss: 0.0686265277909115, Final Batch Loss: 0.0031687626615166664\n",
      "Epoch 2422, Loss: 0.0335869153495878, Final Batch Loss: 0.00142312899697572\n",
      "Epoch 2423, Loss: 0.06729042904771632, Final Batch Loss: 0.001171316485852003\n",
      "Epoch 2424, Loss: 0.0802369812518009, Final Batch Loss: 0.0013424676144495606\n",
      "Epoch 2425, Loss: 0.1755524296531803, Final Batch Loss: 0.002340549835935235\n",
      "Epoch 2426, Loss: 0.032150151033420116, Final Batch Loss: 0.0004187679442111403\n",
      "Epoch 2427, Loss: 0.03008209212566726, Final Batch Loss: 0.00042987093911506236\n",
      "Epoch 2428, Loss: 0.05019526381511241, Final Batch Loss: 0.0028282322455197573\n",
      "Epoch 2429, Loss: 0.08958967785292771, Final Batch Loss: 0.00020935542124789208\n",
      "Epoch 2430, Loss: 0.02198875873000361, Final Batch Loss: 0.0018209165427833796\n",
      "Epoch 2431, Loss: 0.019221762235247297, Final Batch Loss: 0.0034923271741718054\n",
      "Epoch 2432, Loss: 0.032921776226430666, Final Batch Loss: 0.00014890162856318057\n",
      "Epoch 2433, Loss: 0.015519989967287984, Final Batch Loss: 0.0002003158297156915\n",
      "Epoch 2434, Loss: 0.02119692796986783, Final Batch Loss: 0.0002632303221616894\n",
      "Epoch 2435, Loss: 0.011629994769464247, Final Batch Loss: 0.0007164106937125325\n",
      "Epoch 2436, Loss: 0.023048150185786653, Final Batch Loss: 0.0010658563114702702\n",
      "Epoch 2437, Loss: 0.007730725461442489, Final Batch Loss: 0.00025544347590766847\n",
      "Epoch 2438, Loss: 0.007481957356503699, Final Batch Loss: 0.00023996303207241\n",
      "Epoch 2439, Loss: 0.013749688789175707, Final Batch Loss: 0.0007544812397100031\n",
      "Epoch 2440, Loss: 0.003644368269306142, Final Batch Loss: 0.00013557641068473458\n",
      "Epoch 2441, Loss: 0.008636896003736183, Final Batch Loss: 0.00022691763297189027\n",
      "Epoch 2442, Loss: 0.0068489670411509, Final Batch Loss: 0.00021034860401414335\n",
      "Epoch 2443, Loss: 0.07108681354657165, Final Batch Loss: 0.013379847630858421\n",
      "Epoch 2444, Loss: 0.019765441546041984, Final Batch Loss: 5.878796218894422e-05\n",
      "Epoch 2445, Loss: 0.0617687468220538, Final Batch Loss: 0.0027935768011957407\n",
      "Epoch 2446, Loss: 0.023717492396826856, Final Batch Loss: 0.009010782465338707\n",
      "Epoch 2447, Loss: 0.09019512949453201, Final Batch Loss: 0.01228013914078474\n",
      "Epoch 2448, Loss: 0.010605673614918487, Final Batch Loss: 0.00020041209063492715\n",
      "Epoch 2449, Loss: 0.05439950839354424, Final Batch Loss: 0.0002741410571616143\n",
      "Epoch 2450, Loss: 0.026825743312656414, Final Batch Loss: 0.009968155063688755\n",
      "Epoch 2451, Loss: 0.02280531568248989, Final Batch Loss: 0.00010233910143142566\n",
      "Epoch 2452, Loss: 0.05077836465352448, Final Batch Loss: 0.00010218456736765802\n",
      "Epoch 2453, Loss: 0.02125563061417779, Final Batch Loss: 0.00013980230141896755\n",
      "Epoch 2454, Loss: 0.015971432552760234, Final Batch Loss: 0.0011972313513979316\n",
      "Epoch 2455, Loss: 0.019064283813349903, Final Batch Loss: 0.0003085907956119627\n",
      "Epoch 2456, Loss: 0.011862924198794644, Final Batch Loss: 0.00017088586173485965\n",
      "Epoch 2457, Loss: 0.01439124479657039, Final Batch Loss: 8.84258552105166e-05\n",
      "Epoch 2458, Loss: 0.015663187034078874, Final Batch Loss: 0.002415548777207732\n",
      "Epoch 2459, Loss: 0.045002484734141035, Final Batch Loss: 0.0001747670758049935\n",
      "Epoch 2460, Loss: 0.02423366462608101, Final Batch Loss: 0.0005028837476857007\n",
      "Epoch 2461, Loss: 0.05476826418453129, Final Batch Loss: 0.03262580558657646\n",
      "Epoch 2462, Loss: 0.011154697116580792, Final Batch Loss: 0.00020609985222108662\n",
      "Epoch 2463, Loss: 0.012233727664352045, Final Batch Loss: 0.00017881642270367593\n",
      "Epoch 2464, Loss: 0.04759006820677314, Final Batch Loss: 0.0005029226304031909\n",
      "Epoch 2465, Loss: 0.012070315355231287, Final Batch Loss: 0.00045482299174182117\n",
      "Epoch 2466, Loss: 0.013523155685106758, Final Batch Loss: 0.0001262452860828489\n",
      "Epoch 2467, Loss: 0.009042312041856349, Final Batch Loss: 0.00038082306855358183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2468, Loss: 0.03242306744323287, Final Batch Loss: 0.00080634537152946\n",
      "Epoch 2469, Loss: 0.05517040077756974, Final Batch Loss: 0.00010465538071002811\n",
      "Epoch 2470, Loss: 0.01769010050338693, Final Batch Loss: 0.006950277369469404\n",
      "Epoch 2471, Loss: 0.07297581240345608, Final Batch Loss: 0.00023998144024517387\n",
      "Epoch 2472, Loss: 0.04936565329262521, Final Batch Loss: 0.0020043354015797377\n",
      "Epoch 2473, Loss: 0.028197968498716364, Final Batch Loss: 0.006162234582006931\n",
      "Epoch 2474, Loss: 0.02279836911475286, Final Batch Loss: 0.0031115859746932983\n",
      "Epoch 2475, Loss: 0.015547818865343288, Final Batch Loss: 0.0002480729599483311\n",
      "Epoch 2476, Loss: 0.018730019004578935, Final Batch Loss: 0.0003763720451388508\n",
      "Epoch 2477, Loss: 0.048100241940119304, Final Batch Loss: 5.065193909103982e-05\n",
      "Epoch 2478, Loss: 0.009599672019248828, Final Batch Loss: 0.00015793106285855174\n",
      "Epoch 2479, Loss: 0.029144303105567815, Final Batch Loss: 0.0021531868260353804\n",
      "Epoch 2480, Loss: 0.022405487990909023, Final Batch Loss: 0.0011362334480509162\n",
      "Epoch 2481, Loss: 0.01966276148777979, Final Batch Loss: 0.00040777528192847967\n",
      "Epoch 2482, Loss: 0.046271854982478544, Final Batch Loss: 0.00024846356245689094\n",
      "Epoch 2483, Loss: 0.01771944587562757, Final Batch Loss: 0.002674194984138012\n",
      "Epoch 2484, Loss: 0.026043480431326316, Final Batch Loss: 7.505708344979212e-05\n",
      "Epoch 2485, Loss: 0.02681949286488816, Final Batch Loss: 0.001019085175357759\n",
      "Epoch 2486, Loss: 0.07262534934852738, Final Batch Loss: 0.0019661334808915854\n",
      "Epoch 2487, Loss: 0.19630109649369842, Final Batch Loss: 0.03814857825636864\n",
      "Epoch 2488, Loss: 0.05582896805572091, Final Batch Loss: 8.689791866345331e-05\n",
      "Epoch 2489, Loss: 0.04014331891085021, Final Batch Loss: 0.0021133963018655777\n",
      "Epoch 2490, Loss: 0.02257711548008956, Final Batch Loss: 0.0004082784871570766\n",
      "Epoch 2491, Loss: 0.013080542947136564, Final Batch Loss: 0.00011532501957844943\n",
      "Epoch 2492, Loss: 0.009114471260545542, Final Batch Loss: 0.00012492846872191876\n",
      "Epoch 2493, Loss: 0.011636987639576546, Final Batch Loss: 8.360960782738402e-05\n",
      "Epoch 2494, Loss: 0.018210736921901116, Final Batch Loss: 0.0007087921840138733\n",
      "Epoch 2495, Loss: 0.013727173849474639, Final Batch Loss: 0.006197168491780758\n",
      "Epoch 2496, Loss: 0.004980833295121556, Final Batch Loss: 2.5263925635954365e-05\n",
      "Epoch 2497, Loss: 0.009356062422739342, Final Batch Loss: 0.0001567143335705623\n",
      "Epoch 2498, Loss: 0.013725418906687992, Final Batch Loss: 4.67181671410799e-05\n",
      "Epoch 2499, Loss: 0.0068326988475746475, Final Batch Loss: 0.00010124690743396059\n",
      "Epoch 2500, Loss: 0.010274950062466814, Final Batch Loss: 0.0024923630990087986\n",
      "Epoch 2501, Loss: 0.012338759246631525, Final Batch Loss: 0.00477449269965291\n",
      "Epoch 2502, Loss: 0.012144750383413339, Final Batch Loss: 0.0001425442169420421\n",
      "Epoch 2503, Loss: 0.016129194944369374, Final Batch Loss: 0.0013227233430370688\n",
      "Epoch 2504, Loss: 0.03511810772761237, Final Batch Loss: 0.0004941844963468611\n",
      "Epoch 2505, Loss: 0.004262943820322107, Final Batch Loss: 0.00017852296878118068\n",
      "Epoch 2506, Loss: 0.008533159054422867, Final Batch Loss: 0.001991001423448324\n",
      "Epoch 2507, Loss: 0.021128773521922994, Final Batch Loss: 3.404239396331832e-05\n",
      "Epoch 2508, Loss: 0.050476954216719605, Final Batch Loss: 6.699898221995682e-05\n",
      "Epoch 2509, Loss: 0.01229726881319948, Final Batch Loss: 0.0018530491506680846\n",
      "Epoch 2510, Loss: 0.04401116012741113, Final Batch Loss: 0.00040570914279669523\n",
      "Epoch 2511, Loss: 0.052133218880044296, Final Batch Loss: 4.045992682222277e-05\n",
      "Epoch 2512, Loss: 0.0676733449618041, Final Batch Loss: 0.0016089630080386996\n",
      "Epoch 2513, Loss: 0.04085750655212905, Final Batch Loss: 0.00015731496387161314\n",
      "Epoch 2514, Loss: 0.02514117785904091, Final Batch Loss: 0.0006538840825669467\n",
      "Epoch 2515, Loss: 0.024278915581817273, Final Batch Loss: 0.00041239013080485165\n",
      "Epoch 2516, Loss: 0.06085464049101574, Final Batch Loss: 0.0010693127987906337\n",
      "Epoch 2517, Loss: 0.06941356504830765, Final Batch Loss: 0.00016509702254552394\n",
      "Epoch 2518, Loss: 0.09356935850519221, Final Batch Loss: 6.305834540398791e-05\n",
      "Epoch 2519, Loss: 0.04154358852247242, Final Batch Loss: 0.00020163127919659019\n",
      "Epoch 2520, Loss: 0.025529389327857643, Final Batch Loss: 0.0002806978882290423\n",
      "Epoch 2521, Loss: 0.058888673496767296, Final Batch Loss: 0.007311758119612932\n",
      "Epoch 2522, Loss: 0.02245242818025872, Final Batch Loss: 0.0022259706165641546\n",
      "Epoch 2523, Loss: 0.03627843692083843, Final Batch Loss: 0.0019371351227164268\n",
      "Epoch 2524, Loss: 0.05095294129932881, Final Batch Loss: 0.003037048038095236\n",
      "Epoch 2525, Loss: 0.032938299502347945, Final Batch Loss: 0.0008321495843119919\n",
      "Epoch 2526, Loss: 0.023591071494593052, Final Batch Loss: 0.0008922545821405947\n",
      "Epoch 2527, Loss: 0.009909647760650842, Final Batch Loss: 0.0002593345125205815\n",
      "Epoch 2528, Loss: 0.013608126109829755, Final Batch Loss: 6.894641410326585e-05\n",
      "Epoch 2529, Loss: 0.007926462094474118, Final Batch Loss: 0.0010339536238461733\n",
      "Epoch 2530, Loss: 0.01232946291929693, Final Batch Loss: 0.000303674372844398\n",
      "Epoch 2531, Loss: 0.005262282918920391, Final Batch Loss: 0.00021993686095811427\n",
      "Epoch 2532, Loss: 0.01695094885144499, Final Batch Loss: 2.7560921807889827e-05\n",
      "Epoch 2533, Loss: 0.010348805864850874, Final Batch Loss: 0.0001288714847760275\n",
      "Epoch 2534, Loss: 0.0022929297429072903, Final Batch Loss: 0.00026859488571062684\n",
      "Epoch 2535, Loss: 0.0016470985428895801, Final Batch Loss: 2.61739878624212e-05\n",
      "Epoch 2536, Loss: 0.009311114619777072, Final Batch Loss: 0.0001574836642248556\n",
      "Epoch 2537, Loss: 0.014064075575333845, Final Batch Loss: 0.001715760095976293\n",
      "Epoch 2538, Loss: 0.04958091963544575, Final Batch Loss: 0.023764386773109436\n",
      "Epoch 2539, Loss: 0.048854140572075266, Final Batch Loss: 0.0010364832123741508\n",
      "Epoch 2540, Loss: 0.02656269964245439, Final Batch Loss: 0.0002959456469397992\n",
      "Epoch 2541, Loss: 0.028136371503933333, Final Batch Loss: 0.0015981568722054362\n",
      "Epoch 2542, Loss: 0.03798482441015949, Final Batch Loss: 0.0037887010257691145\n",
      "Epoch 2543, Loss: 0.050368295262160245, Final Batch Loss: 2.0024712284794077e-05\n",
      "Epoch 2544, Loss: 0.02299869406306243, Final Batch Loss: 1.5345076462836005e-05\n",
      "Epoch 2545, Loss: 0.03541903539735358, Final Batch Loss: 0.00042523903539404273\n",
      "Epoch 2546, Loss: 0.018094218419719255, Final Batch Loss: 4.2555817344691604e-05\n",
      "Epoch 2547, Loss: 0.02047630551169277, Final Batch Loss: 0.002591519383713603\n",
      "Epoch 2548, Loss: 0.048678556875529466, Final Batch Loss: 0.0019423626363277435\n",
      "Epoch 2549, Loss: 0.01562855225711246, Final Batch Loss: 0.005600344389677048\n",
      "Epoch 2550, Loss: 0.009738065433339216, Final Batch Loss: 0.0005035094218328595\n",
      "Epoch 2551, Loss: 0.029307061597137363, Final Batch Loss: 0.0001699513231869787\n",
      "Epoch 2552, Loss: 0.0793956584384432, Final Batch Loss: 0.0014102959539741278\n",
      "Epoch 2553, Loss: 0.03590146730130073, Final Batch Loss: 0.0005082926945760846\n",
      "Epoch 2554, Loss: 0.025149802670057397, Final Batch Loss: 0.0003122122725471854\n",
      "Epoch 2555, Loss: 0.015462561725144042, Final Batch Loss: 0.0029087092261761427\n",
      "Epoch 2556, Loss: 0.06837897078003152, Final Batch Loss: 0.00014683215704280883\n",
      "Epoch 2557, Loss: 0.018263219182699686, Final Batch Loss: 0.0007327629718929529\n",
      "Epoch 2558, Loss: 0.022977744334639283, Final Batch Loss: 0.001221607206389308\n",
      "Epoch 2559, Loss: 0.01951456734968815, Final Batch Loss: 4.5638262236025184e-05\n",
      "Epoch 2560, Loss: 0.0034069490648107603, Final Batch Loss: 0.00019250299374107271\n",
      "Epoch 2561, Loss: 0.031121716783673037, Final Batch Loss: 0.0005559686105698347\n",
      "Epoch 2562, Loss: 0.05666226752509829, Final Batch Loss: 0.0008622694294899702\n",
      "Epoch 2563, Loss: 0.02864230578416027, Final Batch Loss: 0.00027528515784069896\n",
      "Epoch 2564, Loss: 0.042792934269527905, Final Batch Loss: 0.0010764753678813577\n",
      "Epoch 2565, Loss: 0.02439314220828237, Final Batch Loss: 0.0007817386649549007\n",
      "Epoch 2566, Loss: 0.04165770138206426, Final Batch Loss: 0.0019192545441910625\n",
      "Epoch 2567, Loss: 0.008336423583386932, Final Batch Loss: 8.47422270453535e-05\n",
      "Epoch 2568, Loss: 0.0044632186563831056, Final Batch Loss: 3.0172262995620258e-05\n",
      "Epoch 2569, Loss: 0.011415687786211492, Final Batch Loss: 0.00010102240048581734\n",
      "Epoch 2570, Loss: 0.01299109578212665, Final Batch Loss: 5.003052865504287e-05\n",
      "Epoch 2571, Loss: 0.03948608737118775, Final Batch Loss: 0.0012269958388060331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2572, Loss: 0.05677786974047194, Final Batch Loss: 0.03869831934571266\n",
      "Epoch 2573, Loss: 0.01927539778444043, Final Batch Loss: 0.00016847880033310503\n",
      "Epoch 2574, Loss: 0.010692891999497078, Final Batch Loss: 0.0012884284369647503\n",
      "Epoch 2575, Loss: 0.01405903498562111, Final Batch Loss: 1.805837564461399e-05\n",
      "Epoch 2576, Loss: 0.024851808546372922, Final Batch Loss: 2.5985116735682823e-05\n",
      "Epoch 2577, Loss: 0.02609500261314679, Final Batch Loss: 0.00013749452773481607\n",
      "Epoch 2578, Loss: 0.025985605247115018, Final Batch Loss: 0.01956813782453537\n",
      "Epoch 2579, Loss: 0.007202833652627305, Final Batch Loss: 0.0024385298602283\n",
      "Epoch 2580, Loss: 0.0358407111143606, Final Batch Loss: 5.6517059420002624e-05\n",
      "Epoch 2581, Loss: 0.03635604010014504, Final Batch Loss: 0.012807442806661129\n",
      "Epoch 2582, Loss: 0.017386752211677958, Final Batch Loss: 0.0006959915044717491\n",
      "Epoch 2583, Loss: 0.008991629760203068, Final Batch Loss: 0.00016647482698317617\n",
      "Epoch 2584, Loss: 0.04009511641561403, Final Batch Loss: 6.526290235342458e-05\n",
      "Epoch 2585, Loss: 0.09025651609408669, Final Batch Loss: 0.00024296532501466572\n",
      "Epoch 2586, Loss: 0.13404426618944854, Final Batch Loss: 0.0010712741641327739\n",
      "Epoch 2587, Loss: 0.06926474660576787, Final Batch Loss: 0.011049221269786358\n",
      "Epoch 2588, Loss: 0.049577620447962545, Final Batch Loss: 0.00019414021517150104\n",
      "Epoch 2589, Loss: 0.017175438151753042, Final Batch Loss: 0.00017194476095028222\n",
      "Epoch 2590, Loss: 0.017063303268514574, Final Batch Loss: 0.0016515214229002595\n",
      "Epoch 2591, Loss: 0.011139041911519598, Final Batch Loss: 0.0004923777305521071\n",
      "Epoch 2592, Loss: 0.02295711077749729, Final Batch Loss: 0.0019752534572035074\n",
      "Epoch 2593, Loss: 0.008303175831315457, Final Batch Loss: 0.004032069351524115\n",
      "Epoch 2594, Loss: 0.11677842514109216, Final Batch Loss: 0.0038796199951320887\n",
      "Epoch 2595, Loss: 0.026957479989505373, Final Batch Loss: 0.00035516798379831016\n",
      "Epoch 2596, Loss: 0.029843208234524354, Final Batch Loss: 0.0015849960036575794\n",
      "Epoch 2597, Loss: 0.01396828093129443, Final Batch Loss: 0.00020521253463812172\n",
      "Epoch 2598, Loss: 0.0475835577599355, Final Batch Loss: 0.00020472347387112677\n",
      "Epoch 2599, Loss: 0.06056003909907304, Final Batch Loss: 0.0006162468926049769\n",
      "Epoch 2600, Loss: 0.026680082082748413, Final Batch Loss: 0.00046408583875745535\n",
      "Epoch 2601, Loss: 0.0420428817596985, Final Batch Loss: 0.003649309277534485\n",
      "Epoch 2602, Loss: 0.07733808273042087, Final Batch Loss: 0.0026919683441519737\n",
      "Epoch 2603, Loss: 0.015225078175717499, Final Batch Loss: 0.006030599121004343\n",
      "Epoch 2604, Loss: 0.015641517456970178, Final Batch Loss: 0.006945344619452953\n",
      "Epoch 2605, Loss: 0.0121426767691446, Final Batch Loss: 0.0018560118041932583\n",
      "Epoch 2606, Loss: 0.010058391810161993, Final Batch Loss: 0.00010266190656693652\n",
      "Epoch 2607, Loss: 0.007120937667423277, Final Batch Loss: 6.85339400661178e-05\n",
      "Epoch 2608, Loss: 0.009512912823993247, Final Batch Loss: 3.770471812458709e-05\n",
      "Epoch 2609, Loss: 0.006748222360329237, Final Batch Loss: 0.000446680816821754\n",
      "Epoch 2610, Loss: 0.007569154415250523, Final Batch Loss: 8.054522186284885e-05\n",
      "Epoch 2611, Loss: 0.016873821397894062, Final Batch Loss: 0.00013867481902707368\n",
      "Epoch 2612, Loss: 0.007326355724217137, Final Batch Loss: 8.79741637618281e-05\n",
      "Epoch 2613, Loss: 0.019373189616089803, Final Batch Loss: 0.00014224622282199562\n",
      "Epoch 2614, Loss: 0.024998445509481826, Final Batch Loss: 2.3715316274319775e-05\n",
      "Epoch 2615, Loss: 0.024860382392944302, Final Batch Loss: 0.00025288385222665966\n",
      "Epoch 2616, Loss: 0.02938720905876835, Final Batch Loss: 3.9886293961899355e-05\n",
      "Epoch 2617, Loss: 0.013789240845653694, Final Batch Loss: 0.0010469956323504448\n",
      "Epoch 2618, Loss: 0.013694008506718092, Final Batch Loss: 7.865657971706241e-05\n",
      "Epoch 2619, Loss: 0.01355308244274056, Final Batch Loss: 0.002535529201850295\n",
      "Epoch 2620, Loss: 0.006082440293539548, Final Batch Loss: 5.359922579373233e-05\n",
      "Epoch 2621, Loss: 0.01850367984297918, Final Batch Loss: 0.0004781386523973197\n",
      "Epoch 2622, Loss: 0.050379517964756815, Final Batch Loss: 8.092212374322116e-05\n",
      "Epoch 2623, Loss: 0.05345160486467648, Final Batch Loss: 0.0007513535674661398\n",
      "Epoch 2624, Loss: 0.09871611396374647, Final Batch Loss: 0.0005436342325992882\n",
      "Epoch 2625, Loss: 0.09462142829215736, Final Batch Loss: 0.024050651118159294\n",
      "Epoch 2626, Loss: 0.03627109593253408, Final Batch Loss: 0.0001724073226796463\n",
      "Epoch 2627, Loss: 0.056870491986046545, Final Batch Loss: 0.0013695323141291738\n",
      "Epoch 2628, Loss: 0.035800821184238885, Final Batch Loss: 0.020611969754099846\n",
      "Epoch 2629, Loss: 0.029429337806504918, Final Batch Loss: 0.008976365439593792\n",
      "Epoch 2630, Loss: 0.05034516687737778, Final Batch Loss: 0.00011496448132675141\n",
      "Epoch 2631, Loss: 0.04745303513482213, Final Batch Loss: 0.0021043855231255293\n",
      "Epoch 2632, Loss: 0.032532004384847824, Final Batch Loss: 9.60067700361833e-05\n",
      "Epoch 2633, Loss: 0.06296722474507987, Final Batch Loss: 0.00024286618281621486\n",
      "Epoch 2634, Loss: 0.10822673766233493, Final Batch Loss: 0.00021913088858127594\n",
      "Epoch 2635, Loss: 0.023037627688609064, Final Batch Loss: 0.00019220670219510794\n",
      "Epoch 2636, Loss: 0.025702813145471737, Final Batch Loss: 0.00022154681209940463\n",
      "Epoch 2637, Loss: 0.06337443101438112, Final Batch Loss: 0.0006444577011279762\n",
      "Epoch 2638, Loss: 0.19107880842057057, Final Batch Loss: 0.006893492769449949\n",
      "Epoch 2639, Loss: 0.03371724126918707, Final Batch Loss: 0.0008140027639456093\n",
      "Epoch 2640, Loss: 0.03538992381072603, Final Batch Loss: 0.0003905824851244688\n",
      "Epoch 2641, Loss: 0.04553450133971637, Final Batch Loss: 0.0017101650591939688\n",
      "Epoch 2642, Loss: 0.03157602276041871, Final Batch Loss: 0.0012247058330103755\n",
      "Epoch 2643, Loss: 0.02618070631797309, Final Batch Loss: 0.00032838762854225934\n",
      "Epoch 2644, Loss: 0.019720454067282844, Final Batch Loss: 0.0028489173855632544\n",
      "Epoch 2645, Loss: 0.0205386568850372, Final Batch Loss: 0.0005857247742824256\n",
      "Epoch 2646, Loss: 0.03806449428520864, Final Batch Loss: 0.005559975281357765\n",
      "Epoch 2647, Loss: 0.04749850685038837, Final Batch Loss: 0.016649888828396797\n",
      "Epoch 2648, Loss: 0.048144275206141174, Final Batch Loss: 0.007697499357163906\n",
      "Epoch 2649, Loss: 0.02300503743754234, Final Batch Loss: 0.0033135521225631237\n",
      "Epoch 2650, Loss: 0.03090144715315546, Final Batch Loss: 0.00118931382894516\n",
      "Epoch 2651, Loss: 0.03758785700483713, Final Batch Loss: 0.000633839110378176\n",
      "Epoch 2652, Loss: 0.07873729080165504, Final Batch Loss: 0.0042645581997931\n",
      "Epoch 2653, Loss: 0.02736308178282343, Final Batch Loss: 0.00016367222997359931\n",
      "Epoch 2654, Loss: 0.03407123592478456, Final Batch Loss: 0.0002677416487131268\n",
      "Epoch 2655, Loss: 0.020646629178372677, Final Batch Loss: 0.0002915545483119786\n",
      "Epoch 2656, Loss: 0.0563131350063486, Final Batch Loss: 0.0005095128435641527\n",
      "Epoch 2657, Loss: 0.04978971238961094, Final Batch Loss: 7.3443770816084e-05\n",
      "Epoch 2658, Loss: 0.0216418556010467, Final Batch Loss: 0.00013016941375099123\n",
      "Epoch 2659, Loss: 0.0138784112787107, Final Batch Loss: 0.006007401738315821\n",
      "Epoch 2660, Loss: 0.04325206966313999, Final Batch Loss: 0.0006626735557802022\n",
      "Epoch 2661, Loss: 0.03517311263567535, Final Batch Loss: 0.0008744046790525317\n",
      "Epoch 2662, Loss: 0.02834197629999835, Final Batch Loss: 0.01223811600357294\n",
      "Epoch 2663, Loss: 0.02529638285341207, Final Batch Loss: 4.0149934648070484e-05\n",
      "Epoch 2664, Loss: 0.024411018915998284, Final Batch Loss: 0.004684354644268751\n",
      "Epoch 2665, Loss: 0.04477354887785623, Final Batch Loss: 0.01510635670274496\n",
      "Epoch 2666, Loss: 0.018052264873404056, Final Batch Loss: 0.006885647773742676\n",
      "Epoch 2667, Loss: 0.0224810146537493, Final Batch Loss: 3.320193718536757e-05\n",
      "Epoch 2668, Loss: 0.012650632663280703, Final Batch Loss: 0.00012774055358022451\n",
      "Epoch 2669, Loss: 0.02173649319593096, Final Batch Loss: 0.0002595927217043936\n",
      "Epoch 2670, Loss: 0.014364950751769356, Final Batch Loss: 0.0004669808258768171\n",
      "Epoch 2671, Loss: 0.028780028089386178, Final Batch Loss: 0.0001555867784190923\n",
      "Epoch 2672, Loss: 0.026510056672123028, Final Batch Loss: 0.00038407871033996344\n",
      "Epoch 2673, Loss: 0.022814482290414162, Final Batch Loss: 0.0002472557534929365\n",
      "Epoch 2674, Loss: 0.03668261163693387, Final Batch Loss: 0.00023278730805031955\n",
      "Epoch 2675, Loss: 0.014074664322833996, Final Batch Loss: 0.001486077904701233\n",
      "Epoch 2676, Loss: 0.011386842721549328, Final Batch Loss: 0.00011162942973896861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2677, Loss: 0.019250918881880352, Final Batch Loss: 5.701218105969019e-05\n",
      "Epoch 2678, Loss: 0.0075232154013065156, Final Batch Loss: 0.00010252272477373481\n",
      "Epoch 2679, Loss: 0.008321646026161034, Final Batch Loss: 0.0018778209341689944\n",
      "Epoch 2680, Loss: 0.01172348919135402, Final Batch Loss: 3.00689171126578e-05\n",
      "Epoch 2681, Loss: 0.015933566533931298, Final Batch Loss: 0.0005187075585126877\n",
      "Epoch 2682, Loss: 0.00971317177391029, Final Batch Loss: 0.0005723901558667421\n",
      "Epoch 2683, Loss: 0.0066446159089537105, Final Batch Loss: 0.0008661248139105737\n",
      "Epoch 2684, Loss: 0.01173803458277689, Final Batch Loss: 9.420327114639804e-05\n",
      "Epoch 2685, Loss: 0.05154624074566527, Final Batch Loss: 0.0012811098713427782\n",
      "Epoch 2686, Loss: 0.06787984595575836, Final Batch Loss: 0.03309975191950798\n",
      "Epoch 2687, Loss: 0.06204955252542277, Final Batch Loss: 0.002666928106918931\n",
      "Epoch 2688, Loss: 0.04921226840815507, Final Batch Loss: 0.000990539905615151\n",
      "Epoch 2689, Loss: 0.01578402974701021, Final Batch Loss: 0.0003497563302516937\n",
      "Epoch 2690, Loss: 0.013129826122167287, Final Batch Loss: 0.000416601455071941\n",
      "Epoch 2691, Loss: 0.052563010385711095, Final Batch Loss: 0.0031090690754354\n",
      "Epoch 2692, Loss: 0.0705771265347721, Final Batch Loss: 0.019936155527830124\n",
      "Epoch 2693, Loss: 0.0581475419094204, Final Batch Loss: 0.00040288735181093216\n",
      "Epoch 2694, Loss: 0.04594707155774813, Final Batch Loss: 0.0001954774634214118\n",
      "Epoch 2695, Loss: 0.025403956620721146, Final Batch Loss: 0.0007546727429144084\n",
      "Epoch 2696, Loss: 0.013413090166068287, Final Batch Loss: 2.4237653633463196e-05\n",
      "Epoch 2697, Loss: 0.015034614130854607, Final Batch Loss: 0.0006549180252477527\n",
      "Epoch 2698, Loss: 0.05609251707937801, Final Batch Loss: 0.03861239179968834\n",
      "Epoch 2699, Loss: 0.0634769218886504, Final Batch Loss: 0.0014527684543281794\n",
      "Epoch 2700, Loss: 0.0907448799116537, Final Batch Loss: 0.007109494414180517\n",
      "Epoch 2701, Loss: 0.07038880305481143, Final Batch Loss: 0.022849835455417633\n",
      "Epoch 2702, Loss: 0.15664523595478386, Final Batch Loss: 0.007255664095282555\n",
      "Epoch 2703, Loss: 0.07034335705975536, Final Batch Loss: 0.006091943942010403\n",
      "Epoch 2704, Loss: 0.02952962635390577, Final Batch Loss: 0.0006848492776043713\n",
      "Epoch 2705, Loss: 0.07407547469483688, Final Batch Loss: 0.0004899080959148705\n",
      "Epoch 2706, Loss: 0.018254441551107448, Final Batch Loss: 0.003596874885261059\n",
      "Epoch 2707, Loss: 0.02359502198487462, Final Batch Loss: 0.0022461130283772945\n",
      "Epoch 2708, Loss: 0.010396257319371216, Final Batch Loss: 0.0011287693632766604\n",
      "Epoch 2709, Loss: 0.021012000335758785, Final Batch Loss: 0.0005662749754264951\n",
      "Epoch 2710, Loss: 0.010094719276821706, Final Batch Loss: 0.00021372767514549196\n",
      "Epoch 2711, Loss: 0.04337750153354136, Final Batch Loss: 0.00040940908365882933\n",
      "Epoch 2712, Loss: 0.01079038954776479, Final Batch Loss: 4.396770236780867e-05\n",
      "Epoch 2713, Loss: 0.013521218919777311, Final Batch Loss: 0.002411272143945098\n",
      "Epoch 2714, Loss: 0.017517066742584575, Final Batch Loss: 0.006481607910245657\n",
      "Epoch 2715, Loss: 0.019622625808551675, Final Batch Loss: 0.00017431993910577148\n",
      "Epoch 2716, Loss: 0.023835101550503168, Final Batch Loss: 0.0004471019492484629\n",
      "Epoch 2717, Loss: 0.07868768717162311, Final Batch Loss: 0.003928393591195345\n",
      "Epoch 2718, Loss: 0.037395006726001156, Final Batch Loss: 0.00532092060893774\n",
      "Epoch 2719, Loss: 0.035244882688857615, Final Batch Loss: 0.0001392498961649835\n",
      "Epoch 2720, Loss: 0.05111480597406626, Final Batch Loss: 0.0003196455363649875\n",
      "Epoch 2721, Loss: 0.022797345351136755, Final Batch Loss: 0.00028050647233612835\n",
      "Epoch 2722, Loss: 0.025971266164560802, Final Batch Loss: 0.0008769896812736988\n",
      "Epoch 2723, Loss: 0.015974286652635783, Final Batch Loss: 0.00030822004191577435\n",
      "Epoch 2724, Loss: 0.041424002913117874, Final Batch Loss: 0.0003148066462017596\n",
      "Epoch 2725, Loss: 0.051003566179133486, Final Batch Loss: 0.008488637395203114\n",
      "Epoch 2726, Loss: 0.0281698809703812, Final Batch Loss: 0.00019881427579093724\n",
      "Epoch 2727, Loss: 0.09768428262759699, Final Batch Loss: 0.003890616586431861\n",
      "Epoch 2728, Loss: 0.04013647323154146, Final Batch Loss: 8.356162288691849e-05\n",
      "Epoch 2729, Loss: 0.07369234457928542, Final Batch Loss: 0.026709552854299545\n",
      "Epoch 2730, Loss: 0.011672077111143153, Final Batch Loss: 0.00014454885968007147\n",
      "Epoch 2731, Loss: 0.019048445217777044, Final Batch Loss: 0.0013208870077505708\n",
      "Epoch 2732, Loss: 0.009619518623367185, Final Batch Loss: 0.0008357163751497865\n",
      "Epoch 2733, Loss: 0.027141495404066518, Final Batch Loss: 6.909581134095788e-05\n",
      "Epoch 2734, Loss: 0.01923012854240369, Final Batch Loss: 0.0029796857852488756\n",
      "Epoch 2735, Loss: 0.039303532754274784, Final Batch Loss: 0.0261206217110157\n",
      "Epoch 2736, Loss: 0.009714591080410173, Final Batch Loss: 8.266990334959701e-05\n",
      "Epoch 2737, Loss: 0.02657312162045855, Final Batch Loss: 0.005321540869772434\n",
      "Epoch 2738, Loss: 0.015319664911658037, Final Batch Loss: 0.0003691966412588954\n",
      "Epoch 2739, Loss: 0.023756313930789474, Final Batch Loss: 3.633729284047149e-05\n",
      "Epoch 2740, Loss: 0.05722496454654902, Final Batch Loss: 0.002764460863545537\n",
      "Epoch 2741, Loss: 0.027436045613285387, Final Batch Loss: 0.0007369245286099613\n",
      "Epoch 2742, Loss: 0.1134767131843546, Final Batch Loss: 0.00032872945303097367\n",
      "Epoch 2743, Loss: 0.030251997799496166, Final Batch Loss: 0.00019496273307595402\n",
      "Epoch 2744, Loss: 0.02716827926633414, Final Batch Loss: 0.0005214322591200471\n",
      "Epoch 2745, Loss: 0.016118180830744677, Final Batch Loss: 0.0011710632825270295\n",
      "Epoch 2746, Loss: 0.024887834246328566, Final Batch Loss: 0.0005582370213232934\n",
      "Epoch 2747, Loss: 0.042491723273997195, Final Batch Loss: 0.01367189735174179\n",
      "Epoch 2748, Loss: 0.0733895807134104, Final Batch Loss: 0.003950847778469324\n",
      "Epoch 2749, Loss: 0.01290074315329548, Final Batch Loss: 0.0002813304017763585\n",
      "Epoch 2750, Loss: 0.07141944995964877, Final Batch Loss: 0.03821239247918129\n",
      "Epoch 2751, Loss: 0.08138231005068519, Final Batch Loss: 0.0013869343092665076\n",
      "Epoch 2752, Loss: 0.07060079465736635, Final Batch Loss: 0.005894411355257034\n",
      "Epoch 2753, Loss: 0.03191953548230231, Final Batch Loss: 0.0004656438541132957\n",
      "Epoch 2754, Loss: 0.03561942184023792, Final Batch Loss: 0.002523638540878892\n",
      "Epoch 2755, Loss: 0.020151005599473137, Final Batch Loss: 0.0034967379178851843\n",
      "Epoch 2756, Loss: 0.008384097098314669, Final Batch Loss: 0.001882408745586872\n",
      "Epoch 2757, Loss: 0.04361334084023838, Final Batch Loss: 0.0035442481748759747\n",
      "Epoch 2758, Loss: 0.013141716670361347, Final Batch Loss: 0.0002154893591068685\n",
      "Epoch 2759, Loss: 0.034244610211317195, Final Batch Loss: 0.00013880945334676653\n",
      "Epoch 2760, Loss: 0.016696734011929948, Final Batch Loss: 4.477201582631096e-05\n",
      "Epoch 2761, Loss: 0.018617649033330963, Final Batch Loss: 0.0010605822317302227\n",
      "Epoch 2762, Loss: 0.010329255459510023, Final Batch Loss: 9.11503448151052e-05\n",
      "Epoch 2763, Loss: 0.014570951659152342, Final Batch Loss: 0.003894527442753315\n",
      "Epoch 2764, Loss: 0.05503972465521656, Final Batch Loss: 0.003466600552201271\n",
      "Epoch 2765, Loss: 0.01511456806838396, Final Batch Loss: 0.00037066428922116756\n",
      "Epoch 2766, Loss: 0.01613967502635205, Final Batch Loss: 0.0005913630593568087\n",
      "Epoch 2767, Loss: 0.013745384327194188, Final Batch Loss: 0.004323915578424931\n",
      "Epoch 2768, Loss: 0.02046385067751544, Final Batch Loss: 0.0030118811409920454\n",
      "Epoch 2769, Loss: 0.010775645037938375, Final Batch Loss: 0.0021047291811555624\n",
      "Epoch 2770, Loss: 0.01327733773541695, Final Batch Loss: 2.8424074116628617e-05\n",
      "Epoch 2771, Loss: 0.00833441156646586, Final Batch Loss: 3.111878322670236e-05\n",
      "Epoch 2772, Loss: 0.011865002936247038, Final Batch Loss: 3.3213502319995314e-05\n",
      "Epoch 2773, Loss: 0.007959792597830528, Final Batch Loss: 0.00011823986278614029\n",
      "Epoch 2774, Loss: 0.006167040942727908, Final Batch Loss: 7.602788627991686e-06\n",
      "Epoch 2775, Loss: 0.007898448652667867, Final Batch Loss: 0.0008657599682919681\n",
      "Epoch 2776, Loss: 0.010346854600356892, Final Batch Loss: 0.0029879629146307707\n",
      "Epoch 2777, Loss: 0.003979517704465252, Final Batch Loss: 0.00043560488848015666\n",
      "Epoch 2778, Loss: 0.01519579903833801, Final Batch Loss: 0.0008918883977457881\n",
      "Epoch 2779, Loss: 0.011079826063905784, Final Batch Loss: 5.892513581784442e-05\n",
      "Epoch 2780, Loss: 0.032223076331320044, Final Batch Loss: 0.00038577101076953113\n",
      "Epoch 2781, Loss: 0.038156360995344585, Final Batch Loss: 0.0005774486344307661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2782, Loss: 0.06262593905921676, Final Batch Loss: 5.4813881433801726e-05\n",
      "Epoch 2783, Loss: 0.05869704085489502, Final Batch Loss: 0.026303058490157127\n",
      "Epoch 2784, Loss: 0.011664516074233688, Final Batch Loss: 0.00018125564383808523\n",
      "Epoch 2785, Loss: 0.02432354979828233, Final Batch Loss: 0.0005554357194341719\n",
      "Epoch 2786, Loss: 0.04784086003382981, Final Batch Loss: 2.78986608464038e-05\n",
      "Epoch 2787, Loss: 0.01900751494758879, Final Batch Loss: 0.0035066904965788126\n",
      "Epoch 2788, Loss: 0.024744542788539547, Final Batch Loss: 0.00030220445478335023\n",
      "Epoch 2789, Loss: 0.05505049817293184, Final Batch Loss: 0.00019338546553626657\n",
      "Epoch 2790, Loss: 0.008695121738128364, Final Batch Loss: 0.0022898446768522263\n",
      "Epoch 2791, Loss: 0.08022787678419263, Final Batch Loss: 0.0106275649741292\n",
      "Epoch 2792, Loss: 0.029990239345352165, Final Batch Loss: 0.0007029191474430263\n",
      "Epoch 2793, Loss: 0.04609826175510534, Final Batch Loss: 0.00012547886581160128\n",
      "Epoch 2794, Loss: 0.012294180760363815, Final Batch Loss: 5.147121919435449e-05\n",
      "Epoch 2795, Loss: 0.005247157900157617, Final Batch Loss: 0.001390327699482441\n",
      "Epoch 2796, Loss: 0.09805947935092263, Final Batch Loss: 2.6102550691575743e-05\n",
      "Epoch 2797, Loss: 0.02516084491071524, Final Batch Loss: 0.0028753310907632113\n",
      "Epoch 2798, Loss: 0.030070884833548917, Final Batch Loss: 8.599512511864305e-05\n",
      "Epoch 2799, Loss: 0.01706883813312743, Final Batch Loss: 0.002263633068650961\n",
      "Epoch 2800, Loss: 0.021008344258007128, Final Batch Loss: 0.010485393926501274\n",
      "Epoch 2801, Loss: 0.012505173350291443, Final Batch Loss: 0.0033728356938809156\n",
      "Epoch 2802, Loss: 0.017214162384334486, Final Batch Loss: 0.002651626942679286\n",
      "Epoch 2803, Loss: 0.011223564397369046, Final Batch Loss: 3.984021532232873e-05\n",
      "Epoch 2804, Loss: 0.010778508079965832, Final Batch Loss: 0.0019874616991728544\n",
      "Epoch 2805, Loss: 0.007240453755912313, Final Batch Loss: 4.810014070244506e-05\n",
      "Epoch 2806, Loss: 0.01528130449287346, Final Batch Loss: 0.000697498966474086\n",
      "Epoch 2807, Loss: 0.006187904127727961, Final Batch Loss: 1.224786501552444e-05\n",
      "Epoch 2808, Loss: 0.004171024427705561, Final Batch Loss: 0.00204628799110651\n",
      "Epoch 2809, Loss: 0.007727034981144243, Final Batch Loss: 0.00023883972608018667\n",
      "Epoch 2810, Loss: 0.009248682839825051, Final Batch Loss: 5.830585359944962e-05\n",
      "Epoch 2811, Loss: 0.005993314134684624, Final Batch Loss: 0.0002508599136490375\n",
      "Epoch 2812, Loss: 0.006516839886899106, Final Batch Loss: 6.944074993953109e-05\n",
      "Epoch 2813, Loss: 0.0054559185955440626, Final Batch Loss: 0.0001888816914288327\n",
      "Epoch 2814, Loss: 0.004381619954983762, Final Batch Loss: 0.00170644442550838\n",
      "Epoch 2815, Loss: 0.006658028964011464, Final Batch Loss: 0.00013397684961091727\n",
      "Epoch 2816, Loss: 0.0046721906442144245, Final Batch Loss: 1.889521263365168e-05\n",
      "Epoch 2817, Loss: 0.022924546088688658, Final Batch Loss: 6.184289668453857e-05\n",
      "Epoch 2818, Loss: 0.030121094605419785, Final Batch Loss: 0.00020262856560293585\n",
      "Epoch 2819, Loss: 0.03190895967782126, Final Batch Loss: 0.001402009162120521\n",
      "Epoch 2820, Loss: 0.02248613406663935, Final Batch Loss: 0.006921119522303343\n",
      "Epoch 2821, Loss: 0.07400298501761426, Final Batch Loss: 9.169499207928311e-06\n",
      "Epoch 2822, Loss: 0.012560110275444458, Final Batch Loss: 0.00038142138510011137\n",
      "Epoch 2823, Loss: 0.009052691460965434, Final Batch Loss: 6.462123565142974e-05\n",
      "Epoch 2824, Loss: 0.03352128552887734, Final Batch Loss: 0.01844199001789093\n",
      "Epoch 2825, Loss: 0.020047731790327816, Final Batch Loss: 6.20744249317795e-05\n",
      "Epoch 2826, Loss: 0.01175582638643391, Final Batch Loss: 0.00756288506090641\n",
      "Epoch 2827, Loss: 0.00955266413075151, Final Batch Loss: 0.0003251486923545599\n",
      "Epoch 2828, Loss: 0.018851851844374323, Final Batch Loss: 2.533045699237846e-05\n",
      "Epoch 2829, Loss: 0.026526462634137715, Final Batch Loss: 0.0011507931631058455\n",
      "Epoch 2830, Loss: 0.06513101586460834, Final Batch Loss: 0.020957186818122864\n",
      "Epoch 2831, Loss: 0.06952406531490851, Final Batch Loss: 0.00782997626811266\n",
      "Epoch 2832, Loss: 0.028617210380616598, Final Batch Loss: 0.003509863745421171\n",
      "Epoch 2833, Loss: 0.01037992488272721, Final Batch Loss: 0.0010312022641301155\n",
      "Epoch 2834, Loss: 0.012559568196593318, Final Batch Loss: 0.0009519330342300236\n",
      "Epoch 2835, Loss: 0.018251732984936098, Final Batch Loss: 3.57713652192615e-05\n",
      "Epoch 2836, Loss: 0.0017914638665388338, Final Batch Loss: 0.00022020700271241367\n",
      "Epoch 2837, Loss: 0.03437839535581588, Final Batch Loss: 0.0010345445480197668\n",
      "Epoch 2838, Loss: 0.004151778579398524, Final Batch Loss: 1.3155022315913811e-05\n",
      "Epoch 2839, Loss: 0.02315294509025989, Final Batch Loss: 0.0004216199740767479\n",
      "Epoch 2840, Loss: 0.09533181968799909, Final Batch Loss: 6.913410470588133e-05\n",
      "Epoch 2841, Loss: 0.033985198937443784, Final Batch Loss: 0.0007242250721901655\n",
      "Epoch 2842, Loss: 0.03441152517189039, Final Batch Loss: 0.00042698049219325185\n",
      "Epoch 2843, Loss: 0.02159756658693368, Final Batch Loss: 0.00022476720914710313\n",
      "Epoch 2844, Loss: 0.016663697419062373, Final Batch Loss: 0.0007624034769833088\n",
      "Epoch 2845, Loss: 0.009204408277582843, Final Batch Loss: 0.0015512672252953053\n",
      "Epoch 2846, Loss: 0.014246920587538625, Final Batch Loss: 0.0019920729100704193\n",
      "Epoch 2847, Loss: 0.005063557599441992, Final Batch Loss: 3.561313860700466e-05\n",
      "Epoch 2848, Loss: 0.049717117712134495, Final Batch Loss: 0.0036541151348501444\n",
      "Epoch 2849, Loss: 0.007087485300871776, Final Batch Loss: 0.0004378113953862339\n",
      "Epoch 2850, Loss: 0.010501451090931369, Final Batch Loss: 0.0001479164493503049\n",
      "Epoch 2851, Loss: 0.02862424072009162, Final Batch Loss: 0.00027599584427662194\n",
      "Epoch 2852, Loss: 0.07183982100468711, Final Batch Loss: 0.0007933730375953019\n",
      "Epoch 2853, Loss: 0.013338439461222151, Final Batch Loss: 0.0002014537894865498\n",
      "Epoch 2854, Loss: 0.0196338365713018, Final Batch Loss: 0.000657214957755059\n",
      "Epoch 2855, Loss: 0.02126878619492345, Final Batch Loss: 0.00022516924946103245\n",
      "Epoch 2856, Loss: 0.030860782338095305, Final Batch Loss: 0.0020709747914224863\n",
      "Epoch 2857, Loss: 0.005312983328622067, Final Batch Loss: 0.00012171961134299636\n",
      "Epoch 2858, Loss: 0.0186273061317479, Final Batch Loss: 2.5391722374479286e-05\n",
      "Epoch 2859, Loss: 0.00917603670313838, Final Batch Loss: 0.00013360974844545126\n",
      "Epoch 2860, Loss: 0.022168709901961847, Final Batch Loss: 0.0001957816130015999\n",
      "Epoch 2861, Loss: 0.0032039758898463333, Final Batch Loss: 2.4717093765502796e-05\n",
      "Epoch 2862, Loss: 0.01803028291260489, Final Batch Loss: 2.1575509890681133e-05\n",
      "Epoch 2863, Loss: 0.006020092350809136, Final Batch Loss: 4.4656841055257246e-05\n",
      "Epoch 2864, Loss: 0.0924933101687202, Final Batch Loss: 1.715535472612828e-05\n",
      "Epoch 2865, Loss: 0.003058272298403608, Final Batch Loss: 0.0009063291945494711\n",
      "Epoch 2866, Loss: 0.038553944883460645, Final Batch Loss: 0.00010667353490134701\n",
      "Epoch 2867, Loss: 0.01900261402806791, Final Batch Loss: 0.0008012823527678847\n",
      "Epoch 2868, Loss: 0.0344102173767169, Final Batch Loss: 0.00019923242507502437\n",
      "Epoch 2869, Loss: 0.035500711304848664, Final Batch Loss: 4.1198691178578883e-05\n",
      "Epoch 2870, Loss: 0.04720452337824099, Final Batch Loss: 4.5973003580002114e-05\n",
      "Epoch 2871, Loss: 0.011222746938074124, Final Batch Loss: 0.0003286875144112855\n",
      "Epoch 2872, Loss: 0.014722263867952279, Final Batch Loss: 0.0001989670272450894\n",
      "Epoch 2873, Loss: 0.013316171472979477, Final Batch Loss: 0.00022304053709376603\n",
      "Epoch 2874, Loss: 0.021111320635100128, Final Batch Loss: 0.00021783096599392593\n",
      "Epoch 2875, Loss: 0.0033542880473760306, Final Batch Loss: 4.6367433242267e-05\n",
      "Epoch 2876, Loss: 0.008991429674097162, Final Batch Loss: 9.39187448238954e-05\n",
      "Epoch 2877, Loss: 0.008447582657026942, Final Batch Loss: 0.00020820011559408158\n",
      "Epoch 2878, Loss: 0.010377706268627662, Final Batch Loss: 0.00135751289781183\n",
      "Epoch 2879, Loss: 0.009923026380420197, Final Batch Loss: 0.00011612998059717938\n",
      "Epoch 2880, Loss: 0.005773436221716111, Final Batch Loss: 0.000136765229399316\n",
      "Epoch 2881, Loss: 0.00523902395980258, Final Batch Loss: 0.003084789263084531\n",
      "Epoch 2882, Loss: 0.004513814808888128, Final Batch Loss: 5.156363840796985e-05\n",
      "Epoch 2883, Loss: 0.012365622990728298, Final Batch Loss: 0.0025209642481058836\n",
      "Epoch 2884, Loss: 0.008495988615777605, Final Batch Loss: 0.00018918170826509595\n",
      "Epoch 2885, Loss: 0.020587063988386944, Final Batch Loss: 0.00019745374447666109\n",
      "Epoch 2886, Loss: 0.026488674580832594, Final Batch Loss: 0.0002618957369122654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2887, Loss: 0.014061065133319062, Final Batch Loss: 0.0005683351191692054\n",
      "Epoch 2888, Loss: 0.007746901055725175, Final Batch Loss: 0.0001621177652850747\n",
      "Epoch 2889, Loss: 0.007834722315237741, Final Batch Loss: 0.0034523189533501863\n",
      "Epoch 2890, Loss: 0.003451593769568717, Final Batch Loss: 0.00015972554683685303\n",
      "Epoch 2891, Loss: 0.020722431831018184, Final Batch Loss: 0.0007143152179196477\n",
      "Epoch 2892, Loss: 0.0053224754592520185, Final Batch Loss: 0.00033335419720970094\n",
      "Epoch 2893, Loss: 0.006770960914764146, Final Batch Loss: 0.0002312446158612147\n",
      "Epoch 2894, Loss: 0.014233249686185445, Final Batch Loss: 3.1312058126786724e-05\n",
      "Epoch 2895, Loss: 0.03427964441834774, Final Batch Loss: 0.00016315019456669688\n",
      "Epoch 2896, Loss: 0.10401709834150097, Final Batch Loss: 0.00010369256051490083\n",
      "Epoch 2897, Loss: 0.06306304303871002, Final Batch Loss: 0.0008395669865421951\n",
      "Epoch 2898, Loss: 0.06483547472998907, Final Batch Loss: 2.9400382118183188e-05\n",
      "Epoch 2899, Loss: 0.047880184847599594, Final Batch Loss: 0.008300341665744781\n",
      "Epoch 2900, Loss: 0.02054430331918411, Final Batch Loss: 0.00015891714429017156\n",
      "Epoch 2901, Loss: 0.03675532025954453, Final Batch Loss: 0.0055305445566773415\n",
      "Epoch 2902, Loss: 0.020084688076167367, Final Batch Loss: 0.00037825206527486444\n",
      "Epoch 2903, Loss: 0.012818022758438019, Final Batch Loss: 0.0008830124861560762\n",
      "Epoch 2904, Loss: 0.00498612266528653, Final Batch Loss: 0.0004027496324852109\n",
      "Epoch 2905, Loss: 0.008326454368216218, Final Batch Loss: 5.597064227913506e-05\n",
      "Epoch 2906, Loss: 0.04407728378100728, Final Batch Loss: 0.004101441707462072\n",
      "Epoch 2907, Loss: 0.02793189366639126, Final Batch Loss: 2.951718124677427e-05\n",
      "Epoch 2908, Loss: 0.031929731700074626, Final Batch Loss: 0.0004926236579194665\n",
      "Epoch 2909, Loss: 0.04290343167667743, Final Batch Loss: 0.0036556359846144915\n",
      "Epoch 2910, Loss: 0.028848273041148786, Final Batch Loss: 0.0002453835040796548\n",
      "Epoch 2911, Loss: 0.037640838760125916, Final Batch Loss: 0.0008237186702899635\n",
      "Epoch 2912, Loss: 0.02536971562585677, Final Batch Loss: 0.002327674301341176\n",
      "Epoch 2913, Loss: 0.01583495980958105, Final Batch Loss: 4.4159682147437707e-05\n",
      "Epoch 2914, Loss: 0.013491481433447916, Final Batch Loss: 0.0007249399786815047\n",
      "Epoch 2915, Loss: 0.02970965933491243, Final Batch Loss: 0.00019699250697158277\n",
      "Epoch 2916, Loss: 0.011373406068742042, Final Batch Loss: 0.00019540097855497152\n",
      "Epoch 2917, Loss: 0.07386244216104387, Final Batch Loss: 9.107751247938722e-05\n",
      "Epoch 2918, Loss: 0.031139227289713745, Final Batch Loss: 0.0038510209415107965\n",
      "Epoch 2919, Loss: 0.03229422891672584, Final Batch Loss: 0.0011682769982144237\n",
      "Epoch 2920, Loss: 0.0319569419225445, Final Batch Loss: 0.00042172044049948454\n",
      "Epoch 2921, Loss: 0.04193959313488449, Final Batch Loss: 9.185165981762111e-05\n",
      "Epoch 2922, Loss: 0.06586048399913125, Final Batch Loss: 0.018937045708298683\n",
      "Epoch 2923, Loss: 0.06131717584503349, Final Batch Loss: 0.0007925804820843041\n",
      "Epoch 2924, Loss: 0.039115723833674565, Final Batch Loss: 0.0007621207623742521\n",
      "Epoch 2925, Loss: 0.01927585309749702, Final Batch Loss: 0.00022054009605199099\n",
      "Epoch 2926, Loss: 0.017978649797441904, Final Batch Loss: 0.0004178572853561491\n",
      "Epoch 2927, Loss: 0.07358963249862427, Final Batch Loss: 0.05942217633128166\n",
      "Epoch 2928, Loss: 0.024230411712778732, Final Batch Loss: 0.0007815244025550783\n",
      "Epoch 2929, Loss: 0.019878699706168845, Final Batch Loss: 0.0010924619855359197\n",
      "Epoch 2930, Loss: 0.01678768677629705, Final Batch Loss: 1.7584685338078998e-05\n",
      "Epoch 2931, Loss: 0.022319401263303007, Final Batch Loss: 0.0004177808586973697\n",
      "Epoch 2932, Loss: 0.017022949487000005, Final Batch Loss: 0.005677934270352125\n",
      "Epoch 2933, Loss: 0.046145841533871135, Final Batch Loss: 0.008672255091369152\n",
      "Epoch 2934, Loss: 0.051705564801522996, Final Batch Loss: 0.042399149388074875\n",
      "Epoch 2935, Loss: 0.004517104589467635, Final Batch Loss: 0.00013277085963636637\n",
      "Epoch 2936, Loss: 0.003391287789781927, Final Batch Loss: 5.576247349381447e-05\n",
      "Epoch 2937, Loss: 0.006322921048194985, Final Batch Loss: 5.621652235276997e-05\n",
      "Epoch 2938, Loss: 0.0072833703488868196, Final Batch Loss: 5.99855266045779e-05\n",
      "Epoch 2939, Loss: 0.023912297965580365, Final Batch Loss: 6.261382804950699e-05\n",
      "Epoch 2940, Loss: 0.04006017080973834, Final Batch Loss: 0.01037838775664568\n",
      "Epoch 2941, Loss: 0.10463580707710207, Final Batch Loss: 0.0006030055810697377\n",
      "Epoch 2942, Loss: 0.05468231095437659, Final Batch Loss: 4.4688400521408767e-05\n",
      "Epoch 2943, Loss: 0.034334774121816736, Final Batch Loss: 0.004270218778401613\n",
      "Epoch 2944, Loss: 0.03962505637900904, Final Batch Loss: 0.00030800080276094377\n",
      "Epoch 2945, Loss: 0.025373844749992713, Final Batch Loss: 0.0009603508515283465\n",
      "Epoch 2946, Loss: 0.03684028832867625, Final Batch Loss: 0.0003870025393553078\n",
      "Epoch 2947, Loss: 0.020218428231601138, Final Batch Loss: 0.00012557000445667654\n",
      "Epoch 2948, Loss: 0.020543658818496624, Final Batch Loss: 0.00016344961477443576\n",
      "Epoch 2949, Loss: 0.01391861483170942, Final Batch Loss: 0.00014748508692719042\n",
      "Epoch 2950, Loss: 0.02376348841426079, Final Batch Loss: 0.015644758939743042\n",
      "Epoch 2951, Loss: 0.05219952530023875, Final Batch Loss: 0.00013024664076510817\n",
      "Epoch 2952, Loss: 0.08573187456931919, Final Batch Loss: 0.0022512548603117466\n",
      "Epoch 2953, Loss: 0.04415734258509474, Final Batch Loss: 0.008258012123405933\n",
      "Epoch 2954, Loss: 0.06315279170667054, Final Batch Loss: 0.00227705598808825\n",
      "Epoch 2955, Loss: 0.03944751255039591, Final Batch Loss: 4.902385990135372e-05\n",
      "Epoch 2956, Loss: 0.030412370339035988, Final Batch Loss: 0.0017148004844784737\n",
      "Epoch 2957, Loss: 0.016964915061180363, Final Batch Loss: 0.00039061589632183313\n",
      "Epoch 2958, Loss: 0.05169174199545523, Final Batch Loss: 0.001535570714622736\n",
      "Epoch 2959, Loss: 0.013989875587867573, Final Batch Loss: 6.618898623855785e-05\n",
      "Epoch 2960, Loss: 0.024433484933979344, Final Batch Loss: 0.0003283923724666238\n",
      "Epoch 2961, Loss: 0.034706953971181065, Final Batch Loss: 0.010781476274132729\n",
      "Epoch 2962, Loss: 0.07831883169274079, Final Batch Loss: 0.005269799381494522\n",
      "Epoch 2963, Loss: 0.05282421406081994, Final Batch Loss: 0.00022488326067104936\n",
      "Epoch 2964, Loss: 0.0595157677817042, Final Batch Loss: 9.668319398770109e-05\n",
      "Epoch 2965, Loss: 0.15259398082707776, Final Batch Loss: 0.011237824335694313\n",
      "Epoch 2966, Loss: 0.12243860539820162, Final Batch Loss: 0.0010369656374678016\n",
      "Epoch 2967, Loss: 0.09196212635288248, Final Batch Loss: 0.005403800867497921\n",
      "Epoch 2968, Loss: 0.045915300768683665, Final Batch Loss: 0.0011086971499025822\n",
      "Epoch 2969, Loss: 0.03095442586345598, Final Batch Loss: 0.000947440043091774\n",
      "Epoch 2970, Loss: 0.03059257345739752, Final Batch Loss: 0.0011780901113525033\n",
      "Epoch 2971, Loss: 0.019054209711612202, Final Batch Loss: 0.0020227141212671995\n",
      "Epoch 2972, Loss: 0.03477662324439734, Final Batch Loss: 0.00031099055195227265\n",
      "Epoch 2973, Loss: 0.014305680204415694, Final Batch Loss: 0.0007916188915260136\n",
      "Epoch 2974, Loss: 0.0322559027117677, Final Batch Loss: 0.0036656376905739307\n",
      "Epoch 2975, Loss: 0.014002695821545785, Final Batch Loss: 0.0010365284979343414\n",
      "Epoch 2976, Loss: 0.014736638526301249, Final Batch Loss: 6.697113713016734e-05\n",
      "Epoch 2977, Loss: 0.013674188434379175, Final Batch Loss: 0.0001180250765173696\n",
      "Epoch 2978, Loss: 0.03438504707082757, Final Batch Loss: 0.0022168925497680902\n",
      "Epoch 2979, Loss: 0.04395346311139292, Final Batch Loss: 0.0007683068979531527\n",
      "Epoch 2980, Loss: 0.035024578268348705, Final Batch Loss: 0.00027194636641070247\n",
      "Epoch 2981, Loss: 0.024265837848361116, Final Batch Loss: 0.0030564090702682734\n",
      "Epoch 2982, Loss: 0.009325473434728337, Final Batch Loss: 0.00023400067584589124\n",
      "Epoch 2983, Loss: 0.017289734787482303, Final Batch Loss: 0.004637163132429123\n",
      "Epoch 2984, Loss: 0.00981217226217268, Final Batch Loss: 0.0003028973296750337\n",
      "Epoch 2985, Loss: 0.010488274176168488, Final Batch Loss: 0.00017772387946024537\n",
      "Epoch 2986, Loss: 0.007377991318207933, Final Batch Loss: 0.0002232403348898515\n",
      "Epoch 2987, Loss: 0.02962941688929277, Final Batch Loss: 0.00015727670688647777\n",
      "Epoch 2988, Loss: 0.011071808494307334, Final Batch Loss: 0.00012515121488831937\n",
      "Epoch 2989, Loss: 0.00934976192729664, Final Batch Loss: 0.004616292659193277\n",
      "Epoch 2990, Loss: 0.02421113906075334, Final Batch Loss: 9.786086593521759e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2991, Loss: 0.015222954680211842, Final Batch Loss: 8.67187773110345e-05\n",
      "Epoch 2992, Loss: 0.010202161098277429, Final Batch Loss: 0.0002751117863226682\n",
      "Epoch 2993, Loss: 0.012365599490294699, Final Batch Loss: 4.435172741068527e-05\n",
      "Epoch 2994, Loss: 0.008534767523087794, Final Batch Loss: 7.583683327538893e-05\n",
      "Epoch 2995, Loss: 0.032420769457530696, Final Batch Loss: 0.026814769953489304\n",
      "Epoch 2996, Loss: 0.03220284526469186, Final Batch Loss: 9.322115511167794e-05\n",
      "Epoch 2997, Loss: 0.07853553295899474, Final Batch Loss: 0.03462892025709152\n",
      "Epoch 2998, Loss: 0.04301550085438066, Final Batch Loss: 0.0009624733938835561\n",
      "Epoch 2999, Loss: 0.03276694731175667, Final Batch Loss: 0.0011589222121983767\n",
      "Epoch 3000, Loss: 0.022944773194467416, Final Batch Loss: 3.356937304488383e-05\n",
      "Epoch 3001, Loss: 0.038966253319813404, Final Batch Loss: 9.097431757254526e-05\n",
      "Epoch 3002, Loss: 0.0382466226874385, Final Batch Loss: 0.00017662382742855698\n",
      "Epoch 3003, Loss: 0.04553746114834212, Final Batch Loss: 0.00016681573470123112\n",
      "Epoch 3004, Loss: 0.039029369512718404, Final Batch Loss: 0.0002704193175304681\n",
      "Epoch 3005, Loss: 0.048050388097180985, Final Batch Loss: 0.003735464299097657\n",
      "Epoch 3006, Loss: 0.03345278138294816, Final Batch Loss: 0.00030173728009685874\n",
      "Epoch 3007, Loss: 0.05899346432852326, Final Batch Loss: 0.01634831912815571\n",
      "Epoch 3008, Loss: 0.05113998070373782, Final Batch Loss: 0.0005116955144330859\n",
      "Epoch 3009, Loss: 0.053906901404843666, Final Batch Loss: 0.0003482770116534084\n",
      "Epoch 3010, Loss: 0.026953066280839266, Final Batch Loss: 0.0004249208141118288\n",
      "Epoch 3011, Loss: 0.10375893934178748, Final Batch Loss: 0.03257105126976967\n",
      "Epoch 3012, Loss: 0.09168888967542443, Final Batch Loss: 0.0014558671973645687\n",
      "Epoch 3013, Loss: 0.04436717056523776, Final Batch Loss: 0.0017908407608047128\n",
      "Epoch 3014, Loss: 0.008720061534404522, Final Batch Loss: 0.0002292732533533126\n",
      "Epoch 3015, Loss: 0.008292625709145796, Final Batch Loss: 0.0015056285774335265\n",
      "Epoch 3016, Loss: 0.026399354421300814, Final Batch Loss: 0.00016723691078368574\n",
      "Epoch 3017, Loss: 0.05025067256792681, Final Batch Loss: 0.0008843089453876019\n",
      "Epoch 3018, Loss: 0.017150854884675937, Final Batch Loss: 0.00041484867688268423\n",
      "Epoch 3019, Loss: 0.013365194823563797, Final Batch Loss: 0.00018378911772742867\n",
      "Epoch 3020, Loss: 0.016610513639534474, Final Batch Loss: 0.0017086645821109414\n",
      "Epoch 3021, Loss: 0.013664447060364182, Final Batch Loss: 7.862800703151152e-05\n",
      "Epoch 3022, Loss: 0.0037754048953502206, Final Batch Loss: 2.9518922019633465e-05\n",
      "Epoch 3023, Loss: 0.00758620608576166, Final Batch Loss: 3.011146873177495e-05\n",
      "Epoch 3024, Loss: 0.016668936987116467, Final Batch Loss: 0.0003512148396112025\n",
      "Epoch 3025, Loss: 0.010639986394380685, Final Batch Loss: 9.700041118776426e-05\n",
      "Epoch 3026, Loss: 0.008201925554203626, Final Batch Loss: 0.00045626150676980615\n",
      "Epoch 3027, Loss: 0.008515457343946764, Final Batch Loss: 0.0009185721282847226\n",
      "Epoch 3028, Loss: 0.028810263591367402, Final Batch Loss: 2.8517062673927285e-05\n",
      "Epoch 3029, Loss: 0.09013779513770714, Final Batch Loss: 0.00250135431997478\n",
      "Epoch 3030, Loss: 0.008516684101778083, Final Batch Loss: 0.0010629144962877035\n",
      "Epoch 3031, Loss: 0.01525222656346159, Final Batch Loss: 0.0005981246940791607\n",
      "Epoch 3032, Loss: 0.0093296664817899, Final Batch Loss: 0.00264205364510417\n",
      "Epoch 3033, Loss: 0.008130668160447385, Final Batch Loss: 2.0911675164825283e-05\n",
      "Epoch 3034, Loss: 0.036731790078192716, Final Batch Loss: 0.00084689847426489\n",
      "Epoch 3035, Loss: 0.050045803174725734, Final Batch Loss: 0.00017599150305613875\n",
      "Epoch 3036, Loss: 0.03349306868403801, Final Batch Loss: 0.0021680742502212524\n",
      "Epoch 3037, Loss: 0.02887773026668583, Final Batch Loss: 0.020491063594818115\n",
      "Epoch 3038, Loss: 0.017021641699102474, Final Batch Loss: 9.00835293577984e-05\n",
      "Epoch 3039, Loss: 0.03539934823857038, Final Batch Loss: 6.168925756355748e-05\n",
      "Epoch 3040, Loss: 0.0408678547355521, Final Batch Loss: 6.220211798790842e-05\n",
      "Epoch 3041, Loss: 0.05103316731765517, Final Batch Loss: 0.0007338601280935109\n",
      "Epoch 3042, Loss: 0.034913918549136724, Final Batch Loss: 7.139441004255787e-05\n",
      "Epoch 3043, Loss: 0.01913512428291142, Final Batch Loss: 0.0018594455905258656\n",
      "Epoch 3044, Loss: 0.007545493353973143, Final Batch Loss: 0.0005605147453024983\n",
      "Epoch 3045, Loss: 0.005248138710157946, Final Batch Loss: 0.0001349631347693503\n",
      "Epoch 3046, Loss: 0.008517402544384822, Final Batch Loss: 3.2957566872937605e-05\n",
      "Epoch 3047, Loss: 0.021377042015046754, Final Batch Loss: 0.0002556467952672392\n",
      "Epoch 3048, Loss: 0.019060043807257898, Final Batch Loss: 9.55387731664814e-05\n",
      "Epoch 3049, Loss: 0.008764174710449879, Final Batch Loss: 0.0006023604655638337\n",
      "Epoch 3050, Loss: 0.007875033476011595, Final Batch Loss: 5.122315633343533e-05\n",
      "Epoch 3051, Loss: 0.00993537232716335, Final Batch Loss: 0.0005892471526749432\n",
      "Epoch 3052, Loss: 0.03005111407219374, Final Batch Loss: 0.014831963926553726\n",
      "Epoch 3053, Loss: 0.0321561720684258, Final Batch Loss: 0.0018155727302655578\n",
      "Epoch 3054, Loss: 0.12459411771123996, Final Batch Loss: 0.0014181941514834762\n",
      "Epoch 3055, Loss: 0.056243278882902814, Final Batch Loss: 0.014221022836863995\n",
      "Epoch 3056, Loss: 0.0394648840156151, Final Batch Loss: 0.0002909430186264217\n",
      "Epoch 3057, Loss: 0.09182135846640449, Final Batch Loss: 0.0011758789187297225\n",
      "Epoch 3058, Loss: 0.14280256477650255, Final Batch Loss: 0.008442328311502934\n",
      "Epoch 3059, Loss: 0.04477091121952981, Final Batch Loss: 0.000657754426356405\n",
      "Epoch 3060, Loss: 0.01592253168928437, Final Batch Loss: 0.0001645806187298149\n",
      "Epoch 3061, Loss: 0.013617538672406226, Final Batch Loss: 0.002556823892518878\n",
      "Epoch 3062, Loss: 0.008663564382004552, Final Batch Loss: 8.175258699338883e-05\n",
      "Epoch 3063, Loss: 0.026931231237540487, Final Batch Loss: 0.013402257114648819\n",
      "Epoch 3064, Loss: 0.02997391821554629, Final Batch Loss: 6.740640674252063e-05\n",
      "Epoch 3065, Loss: 0.015286601928892196, Final Batch Loss: 0.0002556840772740543\n",
      "Epoch 3066, Loss: 0.04774333110981388, Final Batch Loss: 0.0016160017112269998\n",
      "Epoch 3067, Loss: 0.02415246567761642, Final Batch Loss: 0.012819805182516575\n",
      "Epoch 3068, Loss: 0.013697754009626806, Final Batch Loss: 0.00019938401237595826\n",
      "Epoch 3069, Loss: 0.04640312193077989, Final Batch Loss: 0.001939128851518035\n",
      "Epoch 3070, Loss: 0.007413656885546516, Final Batch Loss: 0.0014287926023826003\n",
      "Epoch 3071, Loss: 0.010849772676010616, Final Batch Loss: 0.0001456203026464209\n",
      "Epoch 3072, Loss: 0.01284397829658701, Final Batch Loss: 0.0005214196862652898\n",
      "Epoch 3073, Loss: 0.012415303914167453, Final Batch Loss: 0.00019057646568398923\n",
      "Epoch 3074, Loss: 0.022314639361866284, Final Batch Loss: 0.0006924124318175018\n",
      "Epoch 3075, Loss: 0.012324278166488511, Final Batch Loss: 0.00013749733625445515\n",
      "Epoch 3076, Loss: 0.015673820496886037, Final Batch Loss: 9.696329652797431e-05\n",
      "Epoch 3077, Loss: 0.10352459663408808, Final Batch Loss: 0.007983732968568802\n",
      "Epoch 3078, Loss: 0.15545251018193085, Final Batch Loss: 0.0007447329116985202\n",
      "Epoch 3079, Loss: 0.05539558293821756, Final Batch Loss: 0.0067970422096550465\n",
      "Epoch 3080, Loss: 0.04728108348353999, Final Batch Loss: 0.00026977062225341797\n",
      "Epoch 3081, Loss: 0.028167753349407576, Final Batch Loss: 0.0045360601507127285\n",
      "Epoch 3082, Loss: 0.03478052064747317, Final Batch Loss: 0.0004094724135939032\n",
      "Epoch 3083, Loss: 0.01914921166462591, Final Batch Loss: 0.00039800701779313385\n",
      "Epoch 3084, Loss: 0.018865633664063353, Final Batch Loss: 0.0029384393710643053\n",
      "Epoch 3085, Loss: 0.015991245294571854, Final Batch Loss: 0.00241806055419147\n",
      "Epoch 3086, Loss: 0.007838086905394448, Final Batch Loss: 0.0022118648048490286\n",
      "Epoch 3087, Loss: 0.03169185786828166, Final Batch Loss: 4.677886681747623e-05\n",
      "Epoch 3088, Loss: 0.01635500111115107, Final Batch Loss: 0.00016006798250600696\n",
      "Epoch 3089, Loss: 0.007118139275917201, Final Batch Loss: 2.7144023988512345e-05\n",
      "Epoch 3090, Loss: 0.00822654971943848, Final Batch Loss: 6.296845094766468e-05\n",
      "Epoch 3091, Loss: 0.02247044820978772, Final Batch Loss: 0.0007929879357106984\n",
      "Epoch 3092, Loss: 0.007359265833656536, Final Batch Loss: 0.00039931852370500565\n",
      "Epoch 3093, Loss: 0.017614691681956174, Final Batch Loss: 0.0007372818072326481\n",
      "Epoch 3094, Loss: 0.025629919446146232, Final Batch Loss: 0.0001588308223290369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3095, Loss: 0.018283046265423764, Final Batch Loss: 0.00018773881311062723\n",
      "Epoch 3096, Loss: 0.03183759940293385, Final Batch Loss: 6.622150249313563e-05\n",
      "Epoch 3097, Loss: 0.025811632996919798, Final Batch Loss: 0.0016106507973745465\n",
      "Epoch 3098, Loss: 0.00586362337708124, Final Batch Loss: 2.5856310458038934e-05\n",
      "Epoch 3099, Loss: 0.004415768178660073, Final Batch Loss: 5.0920716603286564e-05\n",
      "Epoch 3100, Loss: 0.005325326385900553, Final Batch Loss: 0.00023200662690214813\n",
      "Epoch 3101, Loss: 0.0038683246511936886, Final Batch Loss: 0.0005336352623999119\n",
      "Epoch 3102, Loss: 0.002513518998057407, Final Batch Loss: 5.65539303352125e-05\n",
      "Epoch 3103, Loss: 0.004750641765895125, Final Batch Loss: 0.0001619805407244712\n",
      "Epoch 3104, Loss: 0.03942056153391604, Final Batch Loss: 7.494322926504537e-05\n",
      "Epoch 3105, Loss: 0.030197541071174783, Final Batch Loss: 0.00035405985545367\n",
      "Epoch 3106, Loss: 0.02305588605668163, Final Batch Loss: 0.0001176933292299509\n",
      "Epoch 3107, Loss: 0.025273355044191703, Final Batch Loss: 0.00011450644524302334\n",
      "Epoch 3108, Loss: 0.01069180931426672, Final Batch Loss: 0.0023381849750876427\n",
      "Epoch 3109, Loss: 0.008610703931481112, Final Batch Loss: 0.000192477906239219\n",
      "Epoch 3110, Loss: 0.00835209212891641, Final Batch Loss: 4.137166979489848e-05\n",
      "Epoch 3111, Loss: 0.009292226994148223, Final Batch Loss: 0.00036286539398133755\n",
      "Epoch 3112, Loss: 0.026929598545393674, Final Batch Loss: 3.2335119612980634e-05\n",
      "Epoch 3113, Loss: 0.0141488603567268, Final Batch Loss: 0.002906246343627572\n",
      "Epoch 3114, Loss: 0.005789866339910077, Final Batch Loss: 0.00011002823885064572\n",
      "Epoch 3115, Loss: 0.009698371401100303, Final Batch Loss: 0.003525099251419306\n",
      "Epoch 3116, Loss: 0.0064685129764257, Final Batch Loss: 0.0002559487766120583\n",
      "Epoch 3117, Loss: 0.011952122036746005, Final Batch Loss: 2.157394374080468e-05\n",
      "Epoch 3118, Loss: 0.004179467307039886, Final Batch Loss: 0.00020510723697952926\n",
      "Epoch 3119, Loss: 0.005063219443400158, Final Batch Loss: 7.509579154429957e-05\n",
      "Epoch 3120, Loss: 0.005920257644902449, Final Batch Loss: 0.00018847527098841965\n",
      "Epoch 3121, Loss: 0.003034285558896954, Final Batch Loss: 0.00014210572408046573\n",
      "Epoch 3122, Loss: 0.04894818927277811, Final Batch Loss: 5.316215174389072e-05\n",
      "Epoch 3123, Loss: 0.005348925053112907, Final Batch Loss: 3.088316952926107e-05\n",
      "Epoch 3124, Loss: 0.004079488915522234, Final Batch Loss: 5.2511873946059495e-05\n",
      "Epoch 3125, Loss: 0.008223454046856205, Final Batch Loss: 0.0007610173779539764\n",
      "Epoch 3126, Loss: 0.0026704285737650935, Final Batch Loss: 9.10580056370236e-05\n",
      "Epoch 3127, Loss: 0.0036429265446713543, Final Batch Loss: 2.8053274945705198e-05\n",
      "Epoch 3128, Loss: 0.00250560513632081, Final Batch Loss: 2.053109164990019e-05\n",
      "Epoch 3129, Loss: 0.014371596091223182, Final Batch Loss: 0.0005244839121587574\n",
      "Epoch 3130, Loss: 0.0632085104853104, Final Batch Loss: 0.000469639606308192\n",
      "Epoch 3131, Loss: 0.025828359783190535, Final Batch Loss: 4.430927583598532e-05\n",
      "Epoch 3132, Loss: 0.0313896192201355, Final Batch Loss: 0.009405274875462055\n",
      "Epoch 3133, Loss: 0.03408028665580787, Final Batch Loss: 0.0022516490425914526\n",
      "Epoch 3134, Loss: 0.015437938363902504, Final Batch Loss: 0.0018493833485990763\n",
      "Epoch 3135, Loss: 0.028353407147733378, Final Batch Loss: 0.0007156108040362597\n",
      "Epoch 3136, Loss: 0.056520856476709014, Final Batch Loss: 0.01049952581524849\n",
      "Epoch 3137, Loss: 0.043142404156242264, Final Batch Loss: 0.0015080422163009644\n",
      "Epoch 3138, Loss: 0.012399120354530169, Final Batch Loss: 0.001023729331791401\n",
      "Epoch 3139, Loss: 0.06710006062348839, Final Batch Loss: 0.00033591085230000317\n",
      "Epoch 3140, Loss: 0.056857578880226356, Final Batch Loss: 0.00011932144843740389\n",
      "Epoch 3141, Loss: 0.03745245347090531, Final Batch Loss: 0.0010187298757955432\n",
      "Epoch 3142, Loss: 0.04856794687657384, Final Batch Loss: 0.00029072948382236063\n",
      "Epoch 3143, Loss: 0.015393316576592042, Final Batch Loss: 0.00010504639794817194\n",
      "Epoch 3144, Loss: 0.038232995830185246, Final Batch Loss: 0.0005660062306560576\n",
      "Epoch 3145, Loss: 0.025019042424901272, Final Batch Loss: 0.0005595236434601247\n",
      "Epoch 3146, Loss: 0.023889866126410197, Final Batch Loss: 0.0004632684285752475\n",
      "Epoch 3147, Loss: 0.012527047703770222, Final Batch Loss: 3.690035009640269e-05\n",
      "Epoch 3148, Loss: 0.0045476189679902745, Final Batch Loss: 8.488045568810776e-05\n",
      "Epoch 3149, Loss: 0.07271271406534652, Final Batch Loss: 0.0277956984937191\n",
      "Epoch 3150, Loss: 0.02270437477818632, Final Batch Loss: 0.001304138801060617\n",
      "Epoch 3151, Loss: 0.04661798897359404, Final Batch Loss: 0.0031920182518661022\n",
      "Epoch 3152, Loss: 0.0378608822611568, Final Batch Loss: 9.605538070900366e-05\n",
      "Epoch 3153, Loss: 0.04909229547411087, Final Batch Loss: 4.928772978018969e-05\n",
      "Epoch 3154, Loss: 0.01993582586510456, Final Batch Loss: 0.0001235598319908604\n",
      "Epoch 3155, Loss: 0.10257877856201958, Final Batch Loss: 0.0002246276126243174\n",
      "Epoch 3156, Loss: 0.030150201593642123, Final Batch Loss: 0.0002453896449878812\n",
      "Epoch 3157, Loss: 0.0182335031349794, Final Batch Loss: 0.0003953935229219496\n",
      "Epoch 3158, Loss: 0.08864718733821064, Final Batch Loss: 0.0008043984998948872\n",
      "Epoch 3159, Loss: 0.03351811760512646, Final Batch Loss: 0.0006285997806116939\n",
      "Epoch 3160, Loss: 0.02204151832847856, Final Batch Loss: 0.0072755999863147736\n",
      "Epoch 3161, Loss: 0.020441537017177325, Final Batch Loss: 8.293626888189465e-05\n",
      "Epoch 3162, Loss: 0.02166679645597469, Final Batch Loss: 0.0011489784810692072\n",
      "Epoch 3163, Loss: 0.04273059367733367, Final Batch Loss: 0.0004024047520942986\n",
      "Epoch 3164, Loss: 0.03316683966841083, Final Batch Loss: 0.0012073105899617076\n",
      "Epoch 3165, Loss: 0.056051332107017515, Final Batch Loss: 0.00011363848170731217\n",
      "Epoch 3166, Loss: 0.032405684347395436, Final Batch Loss: 0.0002538658445701003\n",
      "Epoch 3167, Loss: 0.023538812361948658, Final Batch Loss: 0.00563660217449069\n",
      "Epoch 3168, Loss: 0.01578559865083662, Final Batch Loss: 5.787875488749705e-05\n",
      "Epoch 3169, Loss: 0.016628187608148437, Final Batch Loss: 0.0009646622347645462\n",
      "Epoch 3170, Loss: 0.011737138856915408, Final Batch Loss: 0.0012601056369021535\n",
      "Epoch 3171, Loss: 0.006684203999611782, Final Batch Loss: 0.0003824424056801945\n",
      "Epoch 3172, Loss: 0.021581989400146995, Final Batch Loss: 3.2203122827922925e-05\n",
      "Epoch 3173, Loss: 0.01667703501880169, Final Batch Loss: 0.00016477139433845878\n",
      "Epoch 3174, Loss: 0.00881190341533511, Final Batch Loss: 3.7253110349411145e-05\n",
      "Epoch 3175, Loss: 0.010065121125080623, Final Batch Loss: 5.470921314554289e-05\n",
      "Epoch 3176, Loss: 0.018588342745715636, Final Batch Loss: 0.00015619552868884057\n",
      "Epoch 3177, Loss: 0.04538780800794484, Final Batch Loss: 8.294927829410881e-05\n",
      "Epoch 3178, Loss: 0.02540597891129437, Final Batch Loss: 0.00010781685705296695\n",
      "Epoch 3179, Loss: 0.03880121135443915, Final Batch Loss: 0.00015547906514257193\n",
      "Epoch 3180, Loss: 0.028616472591238562, Final Batch Loss: 5.974203668301925e-05\n",
      "Epoch 3181, Loss: 0.009307797423389275, Final Batch Loss: 8.951523341238499e-05\n",
      "Epoch 3182, Loss: 0.05923232590430416, Final Batch Loss: 0.0012392167700454593\n",
      "Epoch 3183, Loss: 0.036202946732373675, Final Batch Loss: 0.0022007229272276163\n",
      "Epoch 3184, Loss: 0.015460170136066154, Final Batch Loss: 0.00038668361958116293\n",
      "Epoch 3185, Loss: 0.0251737503313052, Final Batch Loss: 0.0008474233327433467\n",
      "Epoch 3186, Loss: 0.016248588884991477, Final Batch Loss: 0.00048821562086232007\n",
      "Epoch 3187, Loss: 0.042411234722749214, Final Batch Loss: 7.396427827188745e-05\n",
      "Epoch 3188, Loss: 0.029693236494495068, Final Batch Loss: 0.001725823967717588\n",
      "Epoch 3189, Loss: 0.006513422338684904, Final Batch Loss: 0.00022003805497661233\n",
      "Epoch 3190, Loss: 0.02198337843583431, Final Batch Loss: 0.00021950743393972516\n",
      "Epoch 3191, Loss: 0.02919861624832265, Final Batch Loss: 0.00021381446276791394\n",
      "Epoch 3192, Loss: 0.016653716225846438, Final Batch Loss: 0.0007770610391162336\n",
      "Epoch 3193, Loss: 0.005861987985554151, Final Batch Loss: 9.16176795726642e-05\n",
      "Epoch 3194, Loss: 0.0026490506425034255, Final Batch Loss: 0.00010971583833452314\n",
      "Epoch 3195, Loss: 0.003556533749360824, Final Batch Loss: 8.48381532705389e-05\n",
      "Epoch 3196, Loss: 0.00695712032757001, Final Batch Loss: 0.0006326683796942234\n",
      "Epoch 3197, Loss: 0.010989418307872256, Final Batch Loss: 0.0006412186194211245\n",
      "Epoch 3198, Loss: 0.0641338561044904, Final Batch Loss: 0.0023853499442338943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3199, Loss: 0.03536130060092546, Final Batch Loss: 0.0007616245420649648\n",
      "Epoch 3200, Loss: 0.019827338146569673, Final Batch Loss: 0.0006669164285995066\n",
      "Epoch 3201, Loss: 0.026655214037418773, Final Batch Loss: 9.86457453109324e-05\n",
      "Epoch 3202, Loss: 0.02323487675130309, Final Batch Loss: 7.709365308983251e-05\n",
      "Epoch 3203, Loss: 0.0253223039671866, Final Batch Loss: 0.0063531845808029175\n",
      "Epoch 3204, Loss: 0.008327053863467881, Final Batch Loss: 5.4184616601560265e-05\n",
      "Epoch 3205, Loss: 0.04093148138417746, Final Batch Loss: 9.85136503004469e-05\n",
      "Epoch 3206, Loss: 0.02460722188698128, Final Batch Loss: 0.000810471479780972\n",
      "Epoch 3207, Loss: 0.036621393983296, Final Batch Loss: 0.0007284630555659533\n",
      "Epoch 3208, Loss: 0.014675449223432224, Final Batch Loss: 0.007815210148692131\n",
      "Epoch 3209, Loss: 0.014855680139589822, Final Batch Loss: 0.0018673369195312262\n",
      "Epoch 3210, Loss: 0.0071779753525333945, Final Batch Loss: 2.2608113795286044e-05\n",
      "Epoch 3211, Loss: 0.0050791332869266625, Final Batch Loss: 0.0011632675305008888\n",
      "Epoch 3212, Loss: 0.010537627075336786, Final Batch Loss: 5.595512629952282e-05\n",
      "Epoch 3213, Loss: 0.013543654025852447, Final Batch Loss: 4.0129005355993286e-05\n",
      "Epoch 3214, Loss: 0.0060620905678661074, Final Batch Loss: 0.0006742225377820432\n",
      "Epoch 3215, Loss: 0.009827624602621654, Final Batch Loss: 3.0196422812878154e-05\n",
      "Epoch 3216, Loss: 0.003844579670840176, Final Batch Loss: 0.0010042699286714196\n",
      "Epoch 3217, Loss: 0.005451645431094221, Final Batch Loss: 0.00014847885177005082\n",
      "Epoch 3218, Loss: 0.020582576241395145, Final Batch Loss: 0.00015470728976652026\n",
      "Epoch 3219, Loss: 0.025173088959945744, Final Batch Loss: 0.0004380882892291993\n",
      "Epoch 3220, Loss: 0.010433210751216393, Final Batch Loss: 0.0008020343375392258\n",
      "Epoch 3221, Loss: 0.03899986510259623, Final Batch Loss: 0.00018698727944865823\n",
      "Epoch 3222, Loss: 0.06539339011942502, Final Batch Loss: 0.004513011779636145\n",
      "Epoch 3223, Loss: 0.0361536095442716, Final Batch Loss: 0.00028146806289441884\n",
      "Epoch 3224, Loss: 0.03283388119234587, Final Batch Loss: 0.0002898920502047986\n",
      "Epoch 3225, Loss: 0.012990286988497246, Final Batch Loss: 0.00011317877215333283\n",
      "Epoch 3226, Loss: 0.018556388356955722, Final Batch Loss: 0.00020744527864735574\n",
      "Epoch 3227, Loss: 0.008970343822511495, Final Batch Loss: 7.822910265531391e-05\n",
      "Epoch 3228, Loss: 0.010657528431693208, Final Batch Loss: 2.6917037757812068e-05\n",
      "Epoch 3229, Loss: 0.024835452608385822, Final Batch Loss: 0.00044963438995182514\n",
      "Epoch 3230, Loss: 0.01895212754880049, Final Batch Loss: 1.4909122000972275e-05\n",
      "Epoch 3231, Loss: 0.02381830078229541, Final Batch Loss: 2.7967927962890826e-05\n",
      "Epoch 3232, Loss: 0.020089237059437437, Final Batch Loss: 0.003918866161257029\n",
      "Epoch 3233, Loss: 0.016628126584691927, Final Batch Loss: 0.00011100224946858361\n",
      "Epoch 3234, Loss: 0.012769327404384967, Final Batch Loss: 3.4744803997455165e-05\n",
      "Epoch 3235, Loss: 0.00684005185848946, Final Batch Loss: 8.893173799151555e-05\n",
      "Epoch 3236, Loss: 0.09054286215177854, Final Batch Loss: 0.0010002154158428311\n",
      "Epoch 3237, Loss: 0.03814328417502111, Final Batch Loss: 0.0017274615820497274\n",
      "Epoch 3238, Loss: 0.011203705098523642, Final Batch Loss: 0.004007497802376747\n",
      "Epoch 3239, Loss: 0.011224719008168904, Final Batch Loss: 0.005276941694319248\n",
      "Epoch 3240, Loss: 0.00523947970214067, Final Batch Loss: 0.0001201034538098611\n",
      "Epoch 3241, Loss: 0.019771478238908458, Final Batch Loss: 6.622233195230365e-05\n",
      "Epoch 3242, Loss: 0.0647428931461036, Final Batch Loss: 0.0016048025572672486\n",
      "Epoch 3243, Loss: 0.03220253821928054, Final Batch Loss: 0.0035187238827347755\n",
      "Epoch 3244, Loss: 0.055537029933475424, Final Batch Loss: 0.00022789996000938118\n",
      "Epoch 3245, Loss: 0.03834870633090759, Final Batch Loss: 0.0001700790598988533\n",
      "Epoch 3246, Loss: 0.05241047465824522, Final Batch Loss: 3.368491161381826e-05\n",
      "Epoch 3247, Loss: 0.009654184352257289, Final Batch Loss: 0.00028139882488176227\n",
      "Epoch 3248, Loss: 0.043758791660366114, Final Batch Loss: 0.000413647125242278\n",
      "Epoch 3249, Loss: 0.019064629479544237, Final Batch Loss: 0.0009006895706988871\n",
      "Epoch 3250, Loss: 0.059974099523969926, Final Batch Loss: 0.0011814783792942762\n",
      "Epoch 3251, Loss: 0.016220035174228542, Final Batch Loss: 8.388669812120497e-05\n",
      "Epoch 3252, Loss: 0.017761242073902395, Final Batch Loss: 3.3835025533335283e-05\n",
      "Epoch 3253, Loss: 0.018034244800219312, Final Batch Loss: 0.0003388870391063392\n",
      "Epoch 3254, Loss: 0.024078910932075814, Final Batch Loss: 0.0002638229343574494\n",
      "Epoch 3255, Loss: 0.12726661929264083, Final Batch Loss: 0.006365553475916386\n",
      "Epoch 3256, Loss: 0.13867661051335745, Final Batch Loss: 0.02786143310368061\n",
      "Epoch 3257, Loss: 0.2174273320561042, Final Batch Loss: 0.003538532881066203\n",
      "Epoch 3258, Loss: 0.1373469831814873, Final Batch Loss: 0.00053317443234846\n",
      "Epoch 3259, Loss: 0.05061998877499718, Final Batch Loss: 0.004187857266515493\n",
      "Epoch 3260, Loss: 0.04904611234087497, Final Batch Loss: 0.004329972434788942\n",
      "Epoch 3261, Loss: 0.02210221948917024, Final Batch Loss: 0.00027588364901021123\n",
      "Epoch 3262, Loss: 0.024536018096114276, Final Batch Loss: 0.004606260918080807\n",
      "Epoch 3263, Loss: 0.0412512101320317, Final Batch Loss: 0.0008374612079933286\n",
      "Epoch 3264, Loss: 0.06171680240368005, Final Batch Loss: 0.010473954491317272\n",
      "Epoch 3265, Loss: 0.02456510014599189, Final Batch Loss: 0.00012737732322420925\n",
      "Epoch 3266, Loss: 0.040518379222703516, Final Batch Loss: 8.567419718019664e-05\n",
      "Epoch 3267, Loss: 0.04919242665346246, Final Batch Loss: 0.0013417437439784408\n",
      "Epoch 3268, Loss: 0.017439846633351408, Final Batch Loss: 0.0004685584281105548\n",
      "Epoch 3269, Loss: 0.008687945373822004, Final Batch Loss: 0.002561484230682254\n",
      "Epoch 3270, Loss: 0.01363361073890701, Final Batch Loss: 3.251814632676542e-05\n",
      "Epoch 3271, Loss: 0.009283059436711483, Final Batch Loss: 0.00019609968876466155\n",
      "Epoch 3272, Loss: 0.008400073667871766, Final Batch Loss: 0.0007173598278313875\n",
      "Epoch 3273, Loss: 0.061095661789295264, Final Batch Loss: 0.011520493775606155\n",
      "Epoch 3274, Loss: 0.03476298978057457, Final Batch Loss: 0.00631405645981431\n",
      "Epoch 3275, Loss: 0.044222496515430976, Final Batch Loss: 2.4421129637630656e-05\n",
      "Epoch 3276, Loss: 0.02556455909507349, Final Batch Loss: 0.00394779397174716\n",
      "Epoch 3277, Loss: 0.010287982302543242, Final Batch Loss: 0.00018323610129300505\n",
      "Epoch 3278, Loss: 0.03221911725268001, Final Batch Loss: 0.0002741515636444092\n",
      "Epoch 3279, Loss: 0.07921301342503284, Final Batch Loss: 0.0012619307963177562\n",
      "Epoch 3280, Loss: 0.07111014169640839, Final Batch Loss: 6.271081656450406e-05\n",
      "Epoch 3281, Loss: 0.09666613546869485, Final Batch Loss: 0.0006853880477137864\n",
      "Epoch 3282, Loss: 0.041844368606689386, Final Batch Loss: 0.0002523640578147024\n",
      "Epoch 3283, Loss: 0.018796837292029522, Final Batch Loss: 0.0011065127328038216\n",
      "Epoch 3284, Loss: 0.028265630338864867, Final Batch Loss: 0.0023903525434434414\n",
      "Epoch 3285, Loss: 0.022579790005693212, Final Batch Loss: 5.3180279792286456e-05\n",
      "Epoch 3286, Loss: 0.014274088320235023, Final Batch Loss: 0.00015987803635653108\n",
      "Epoch 3287, Loss: 0.017275969683396397, Final Batch Loss: 0.0017005042172968388\n",
      "Epoch 3288, Loss: 0.029926738989161095, Final Batch Loss: 0.0005982688162475824\n",
      "Epoch 3289, Loss: 0.03925389477444696, Final Batch Loss: 0.0017437315545976162\n",
      "Epoch 3290, Loss: 0.030088859573879745, Final Batch Loss: 0.00099936465267092\n",
      "Epoch 3291, Loss: 0.02264829222986009, Final Batch Loss: 0.00010493939771549776\n",
      "Epoch 3292, Loss: 0.009899027079882217, Final Batch Loss: 0.002648519352078438\n",
      "Epoch 3293, Loss: 0.039237896977283526, Final Batch Loss: 0.0002074012445518747\n",
      "Epoch 3294, Loss: 0.006771762436073914, Final Batch Loss: 6.0177058912813663e-05\n",
      "Epoch 3295, Loss: 0.0032956034738163, Final Batch Loss: 0.00013573352771345526\n",
      "Epoch 3296, Loss: 0.009132063894867315, Final Batch Loss: 3.635167740867473e-05\n",
      "Epoch 3297, Loss: 0.009768565702870546, Final Batch Loss: 0.0011395564069971442\n",
      "Epoch 3298, Loss: 0.0060145460556668695, Final Batch Loss: 0.0009753255289979279\n",
      "Epoch 3299, Loss: 0.009217896567861317, Final Batch Loss: 0.0013293684460222721\n",
      "Epoch 3300, Loss: 0.017202944948621735, Final Batch Loss: 0.0011003619292750955\n",
      "Epoch 3301, Loss: 0.031590570978551114, Final Batch Loss: 0.007586232852190733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3302, Loss: 0.045896286646893714, Final Batch Loss: 0.0001310671359533444\n",
      "Epoch 3303, Loss: 0.027583252098338562, Final Batch Loss: 0.000932388415094465\n",
      "Epoch 3304, Loss: 0.005661156230416964, Final Batch Loss: 0.00019112162408418953\n",
      "Epoch 3305, Loss: 0.012504745798651129, Final Batch Loss: 0.0015620760386809707\n",
      "Epoch 3306, Loss: 0.010323460335712298, Final Batch Loss: 0.00020080208196304739\n",
      "Epoch 3307, Loss: 0.010714654456023709, Final Batch Loss: 7.302292215172201e-05\n",
      "Epoch 3308, Loss: 0.010659401148586767, Final Batch Loss: 0.0012906413758173585\n",
      "Epoch 3309, Loss: 0.015942816952247085, Final Batch Loss: 0.001309345941990614\n",
      "Epoch 3310, Loss: 0.007663530489480763, Final Batch Loss: 0.00010527040285523981\n",
      "Epoch 3311, Loss: 0.004347636409875122, Final Batch Loss: 0.0008933760109357536\n",
      "Epoch 3312, Loss: 0.01167945674569637, Final Batch Loss: 6.0249698435654864e-05\n",
      "Epoch 3313, Loss: 0.0037686979230784345, Final Batch Loss: 0.00024413336359430104\n",
      "Epoch 3314, Loss: 0.008794296113592281, Final Batch Loss: 0.004844316281378269\n",
      "Epoch 3315, Loss: 0.004759602253216144, Final Batch Loss: 0.000793111277744174\n",
      "Epoch 3316, Loss: 0.01628709421493113, Final Batch Loss: 5.405443152994849e-05\n",
      "Epoch 3317, Loss: 0.0036102651756664272, Final Batch Loss: 4.261002686689608e-05\n",
      "Epoch 3318, Loss: 0.05641567920747548, Final Batch Loss: 0.00038868558476679027\n",
      "Epoch 3319, Loss: 0.011704775222824537, Final Batch Loss: 3.3796572097344324e-05\n",
      "Epoch 3320, Loss: 0.006305598191829631, Final Batch Loss: 8.776329195825383e-05\n",
      "Epoch 3321, Loss: 0.004919395118122338, Final Batch Loss: 2.2420086679630913e-05\n",
      "Epoch 3322, Loss: 0.007677944749048038, Final Batch Loss: 0.0009139243047684431\n",
      "Epoch 3323, Loss: 0.04045336287299506, Final Batch Loss: 0.021397506818175316\n",
      "Epoch 3324, Loss: 0.019375782130737207, Final Batch Loss: 8.934854849940166e-05\n",
      "Epoch 3325, Loss: 0.04512357995281491, Final Batch Loss: 0.00040974075091071427\n",
      "Epoch 3326, Loss: 0.04277868392455275, Final Batch Loss: 0.0005104569136165082\n",
      "Epoch 3327, Loss: 0.029025957133853808, Final Batch Loss: 0.0007067532278597355\n",
      "Epoch 3328, Loss: 0.02579715799220139, Final Batch Loss: 0.00047491546138189733\n",
      "Epoch 3329, Loss: 0.0041624184377724305, Final Batch Loss: 0.00041062835953198373\n",
      "Epoch 3330, Loss: 0.023292695041163824, Final Batch Loss: 4.8732003051554784e-05\n",
      "Epoch 3331, Loss: 0.0068336201475176495, Final Batch Loss: 0.00048604258336126804\n",
      "Epoch 3332, Loss: 0.02554211259484873, Final Batch Loss: 0.0001110626180889085\n",
      "Epoch 3333, Loss: 0.04454857229575282, Final Batch Loss: 5.6244811275973916e-05\n",
      "Epoch 3334, Loss: 0.037122950114280684, Final Batch Loss: 0.0004214471555314958\n",
      "Epoch 3335, Loss: 0.013453602254230645, Final Batch Loss: 0.0005024696583859622\n",
      "Epoch 3336, Loss: 0.007525629649535404, Final Batch Loss: 3.192290023434907e-05\n",
      "Epoch 3337, Loss: 0.044307103285973426, Final Batch Loss: 0.00011949849431402981\n",
      "Epoch 3338, Loss: 0.04460976654445403, Final Batch Loss: 0.00046733496128581464\n",
      "Epoch 3339, Loss: 0.006603264726436464, Final Batch Loss: 0.002609644317999482\n",
      "Epoch 3340, Loss: 0.02142002045366098, Final Batch Loss: 0.0001701986911939457\n",
      "Epoch 3341, Loss: 0.005939257573118084, Final Batch Loss: 3.300744356238283e-05\n",
      "Epoch 3342, Loss: 0.04851193298782164, Final Batch Loss: 0.0002659001329448074\n",
      "Epoch 3343, Loss: 0.011167276679771021, Final Batch Loss: 0.006323669105768204\n",
      "Epoch 3344, Loss: 0.008619327700216672, Final Batch Loss: 8.546956087229773e-05\n",
      "Epoch 3345, Loss: 0.007490155960113043, Final Batch Loss: 8.234104461735114e-05\n",
      "Epoch 3346, Loss: 0.010309670149581507, Final Batch Loss: 0.00036698480835184455\n",
      "Epoch 3347, Loss: 0.007776116872264538, Final Batch Loss: 0.0003565835068002343\n",
      "Epoch 3348, Loss: 0.054044587563112145, Final Batch Loss: 0.043104853481054306\n",
      "Epoch 3349, Loss: 0.04743947407769156, Final Batch Loss: 0.0001927414705278352\n",
      "Epoch 3350, Loss: 0.00900431056288653, Final Batch Loss: 0.0003030193329323083\n",
      "Epoch 3351, Loss: 0.006651818803220522, Final Batch Loss: 0.0004117503995075822\n",
      "Epoch 3352, Loss: 0.013714673252252396, Final Batch Loss: 0.0009293604525737464\n",
      "Epoch 3353, Loss: 0.02703307306728675, Final Batch Loss: 0.0008511132909916341\n",
      "Epoch 3354, Loss: 0.04195596685713099, Final Batch Loss: 0.0022784941829741\n",
      "Epoch 3355, Loss: 0.021124270644577336, Final Batch Loss: 8.748067921260372e-05\n",
      "Epoch 3356, Loss: 0.011479400820462615, Final Batch Loss: 8.179520955309272e-05\n",
      "Epoch 3357, Loss: 0.005313082350767218, Final Batch Loss: 0.00010995854245265946\n",
      "Epoch 3358, Loss: 0.06094866376770369, Final Batch Loss: 0.0013356991112232208\n",
      "Epoch 3359, Loss: 0.013935352472799423, Final Batch Loss: 6.283882248681039e-05\n",
      "Epoch 3360, Loss: 0.03229898503741424, Final Batch Loss: 0.0005786496913060546\n",
      "Epoch 3361, Loss: 0.096651055093389, Final Batch Loss: 0.001987002557143569\n",
      "Epoch 3362, Loss: 0.04248559931147611, Final Batch Loss: 0.0009368247701786458\n",
      "Epoch 3363, Loss: 0.012273079006263288, Final Batch Loss: 0.0006557486485689878\n",
      "Epoch 3364, Loss: 0.008579445706345723, Final Batch Loss: 6.849560304544866e-05\n",
      "Epoch 3365, Loss: 0.031666402266637306, Final Batch Loss: 0.0008294752915389836\n",
      "Epoch 3366, Loss: 0.02746233029392897, Final Batch Loss: 0.022068321704864502\n",
      "Epoch 3367, Loss: 0.051535003491153475, Final Batch Loss: 0.007233640179038048\n",
      "Epoch 3368, Loss: 0.030843735337839462, Final Batch Loss: 0.000260265136603266\n",
      "Epoch 3369, Loss: 0.037365154013969004, Final Batch Loss: 0.0008325441740453243\n",
      "Epoch 3370, Loss: 0.02088328394529526, Final Batch Loss: 0.00012731595779769123\n",
      "Epoch 3371, Loss: 0.1297902602418617, Final Batch Loss: 0.0028284324798732996\n",
      "Epoch 3372, Loss: 0.04163290281758236, Final Batch Loss: 0.000319689919706434\n",
      "Epoch 3373, Loss: 0.05885754063183413, Final Batch Loss: 0.00024359856615774333\n",
      "Epoch 3374, Loss: 0.017214976045579533, Final Batch Loss: 0.0028245137073099613\n",
      "Epoch 3375, Loss: 0.008259210242613335, Final Batch Loss: 0.00046100973850116134\n",
      "Epoch 3376, Loss: 0.01789958099288924, Final Batch Loss: 0.000579095387365669\n",
      "Epoch 3377, Loss: 0.05165729232976446, Final Batch Loss: 0.007607337087392807\n",
      "Epoch 3378, Loss: 0.06253417681728024, Final Batch Loss: 0.0002749893465079367\n",
      "Epoch 3379, Loss: 0.07282691183354473, Final Batch Loss: 0.0011008103610947728\n",
      "Epoch 3380, Loss: 0.033939957909751683, Final Batch Loss: 0.00027209502877667546\n",
      "Epoch 3381, Loss: 0.016738110907681403, Final Batch Loss: 3.134540020255372e-05\n",
      "Epoch 3382, Loss: 0.02832605220100959, Final Batch Loss: 8.037607040023431e-05\n",
      "Epoch 3383, Loss: 0.007656289380975068, Final Batch Loss: 0.00024886129540391266\n",
      "Epoch 3384, Loss: 0.02176969555875985, Final Batch Loss: 0.00021592668781522661\n",
      "Epoch 3385, Loss: 0.015068848631926812, Final Batch Loss: 0.0021135483402758837\n",
      "Epoch 3386, Loss: 0.009965840328732156, Final Batch Loss: 6.673486495856196e-05\n",
      "Epoch 3387, Loss: 0.019919428870707634, Final Batch Loss: 0.0001747228525346145\n",
      "Epoch 3388, Loss: 0.015690738260673243, Final Batch Loss: 0.00018960180750582367\n",
      "Epoch 3389, Loss: 0.028399661767252837, Final Batch Loss: 0.0066360789351165295\n",
      "Epoch 3390, Loss: 0.012145688180680736, Final Batch Loss: 0.001059488276951015\n",
      "Epoch 3391, Loss: 0.022450662616392947, Final Batch Loss: 1.565810634929221e-05\n",
      "Epoch 3392, Loss: 0.005574857769261143, Final Batch Loss: 7.385920616798103e-05\n",
      "Epoch 3393, Loss: 0.015565503426842042, Final Batch Loss: 0.002082904800772667\n",
      "Epoch 3394, Loss: 0.009242817339327303, Final Batch Loss: 0.00012334923667367548\n",
      "Epoch 3395, Loss: 0.013480883882948547, Final Batch Loss: 0.00021917956473771483\n",
      "Epoch 3396, Loss: 0.012525334154815937, Final Batch Loss: 0.0014140417333692312\n",
      "Epoch 3397, Loss: 0.010582964110653847, Final Batch Loss: 2.4841368940542452e-05\n",
      "Epoch 3398, Loss: 0.010899240584421932, Final Batch Loss: 0.0014241012977436185\n",
      "Epoch 3399, Loss: 0.007960213897604262, Final Batch Loss: 0.00030055802199058235\n",
      "Epoch 3400, Loss: 0.0032197161235671956, Final Batch Loss: 0.0001153274133685045\n",
      "Epoch 3401, Loss: 0.014031773365786648, Final Batch Loss: 1.7808495613280684e-05\n",
      "Epoch 3402, Loss: 0.015069987548486097, Final Batch Loss: 0.00013576142373494804\n",
      "Epoch 3403, Loss: 0.01574728061677888, Final Batch Loss: 0.0002677653101272881\n",
      "Epoch 3404, Loss: 0.022988679778791266, Final Batch Loss: 9.775728540262207e-05\n",
      "Epoch 3405, Loss: 0.052637333999882685, Final Batch Loss: 0.008702977560460567\n",
      "Epoch 3406, Loss: 0.009379855291626882, Final Batch Loss: 7.555423508165404e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3407, Loss: 0.04026112163774087, Final Batch Loss: 5.507282185135409e-05\n",
      "Epoch 3408, Loss: 0.004297298492019763, Final Batch Loss: 0.0016455890145152807\n",
      "Epoch 3409, Loss: 0.01755193386998144, Final Batch Loss: 0.00361196999438107\n",
      "Epoch 3410, Loss: 0.006846626303740777, Final Batch Loss: 0.00024055931135080755\n",
      "Epoch 3411, Loss: 0.06836351216679759, Final Batch Loss: 0.00021771262981928885\n",
      "Epoch 3412, Loss: 0.017290929573391622, Final Batch Loss: 0.00011281838669674471\n",
      "Epoch 3413, Loss: 0.01510094405421114, Final Batch Loss: 0.0002627551439218223\n",
      "Epoch 3414, Loss: 0.008442528298473917, Final Batch Loss: 0.0009913378162309527\n",
      "Epoch 3415, Loss: 0.009300765796069754, Final Batch Loss: 0.0067279404029250145\n",
      "Epoch 3416, Loss: 0.04964933420524176, Final Batch Loss: 1.8097791325999424e-05\n",
      "Epoch 3417, Loss: 0.0063674671728222165, Final Batch Loss: 0.00014320715854410082\n",
      "Epoch 3418, Loss: 0.016303855900332564, Final Batch Loss: 0.00023030459124129266\n",
      "Epoch 3419, Loss: 0.021514339812711114, Final Batch Loss: 0.0004516027693171054\n",
      "Epoch 3420, Loss: 0.009014626941279857, Final Batch Loss: 0.0006091538816690445\n",
      "Epoch 3421, Loss: 0.05471738107371493, Final Batch Loss: 0.0017578977858647704\n",
      "Epoch 3422, Loss: 0.02034672078298172, Final Batch Loss: 0.0022892681881785393\n",
      "Epoch 3423, Loss: 0.011106618938356405, Final Batch Loss: 0.00036147068021818995\n",
      "Epoch 3424, Loss: 0.023977498037311307, Final Batch Loss: 0.008079893887043\n",
      "Epoch 3425, Loss: 0.007671361263419385, Final Batch Loss: 0.00013676946400664747\n",
      "Epoch 3426, Loss: 0.0018403814046905609, Final Batch Loss: 0.00014631821250077337\n",
      "Epoch 3427, Loss: 0.0231331033155584, Final Batch Loss: 0.004594604019075632\n",
      "Epoch 3428, Loss: 0.00780702516931342, Final Batch Loss: 0.00011057252413593233\n",
      "Epoch 3429, Loss: 0.004872626119322376, Final Batch Loss: 0.0002063820866169408\n",
      "Epoch 3430, Loss: 0.0088647988413868, Final Batch Loss: 0.00043696488137356937\n",
      "Epoch 3431, Loss: 0.00657007397057896, Final Batch Loss: 2.5848430595942773e-05\n",
      "Epoch 3432, Loss: 0.005928484173637116, Final Batch Loss: 3.8372618291759863e-05\n",
      "Epoch 3433, Loss: 0.04153947868235264, Final Batch Loss: 5.151552250026725e-05\n",
      "Epoch 3434, Loss: 0.02612080192193389, Final Batch Loss: 0.000645766849629581\n",
      "Epoch 3435, Loss: 0.02390075664879987, Final Batch Loss: 9.98416289803572e-05\n",
      "Epoch 3436, Loss: 0.026218174045425258, Final Batch Loss: 0.00013012130511924624\n",
      "Epoch 3437, Loss: 0.009002169744235289, Final Batch Loss: 0.00010451296111568809\n",
      "Epoch 3438, Loss: 0.005380187785704038, Final Batch Loss: 0.0002199644222855568\n",
      "Epoch 3439, Loss: 0.01777303007474984, Final Batch Loss: 7.159048254834488e-05\n",
      "Epoch 3440, Loss: 0.039424892369424924, Final Batch Loss: 0.002921855077147484\n",
      "Epoch 3441, Loss: 0.022587264857065747, Final Batch Loss: 0.00018827663734555244\n",
      "Epoch 3442, Loss: 0.04904721337334195, Final Batch Loss: 0.00011890774476341903\n",
      "Epoch 3443, Loss: 0.0469859411696234, Final Batch Loss: 0.00024344872508663684\n",
      "Epoch 3444, Loss: 0.006270909754675813, Final Batch Loss: 2.5258694222429767e-05\n",
      "Epoch 3445, Loss: 0.014363782221153087, Final Batch Loss: 0.00018915566033683717\n",
      "Epoch 3446, Loss: 0.013208332024078118, Final Batch Loss: 0.0002475939108990133\n",
      "Epoch 3447, Loss: 0.018839536306586524, Final Batch Loss: 0.0008646928472444415\n",
      "Epoch 3448, Loss: 0.017611291843422805, Final Batch Loss: 5.789661008748226e-05\n",
      "Epoch 3449, Loss: 0.03543091981191537, Final Batch Loss: 0.0032207982148975134\n",
      "Epoch 3450, Loss: 0.020751353673404083, Final Batch Loss: 0.0009259848739020526\n",
      "Epoch 3451, Loss: 0.01999089946366439, Final Batch Loss: 0.005040435120463371\n",
      "Epoch 3452, Loss: 0.07991077120095724, Final Batch Loss: 6.243986717890948e-05\n",
      "Epoch 3453, Loss: 0.08084457021323033, Final Batch Loss: 0.005906706675887108\n",
      "Epoch 3454, Loss: 0.057318406195918215, Final Batch Loss: 1.7115899026975967e-05\n",
      "Epoch 3455, Loss: 0.1876458447513869, Final Batch Loss: 0.00032035261392593384\n",
      "Epoch 3456, Loss: 0.14687439790577628, Final Batch Loss: 0.002506591146811843\n",
      "Epoch 3457, Loss: 0.12916518293786794, Final Batch Loss: 0.013031129725277424\n",
      "Epoch 3458, Loss: 0.05025374429533258, Final Batch Loss: 0.005318944342434406\n",
      "Epoch 3459, Loss: 0.025972017465392128, Final Batch Loss: 0.0006900355219841003\n",
      "Epoch 3460, Loss: 0.02760089945513755, Final Batch Loss: 0.004540249705314636\n",
      "Epoch 3461, Loss: 0.02347854757681489, Final Batch Loss: 0.00038677448173984885\n",
      "Epoch 3462, Loss: 0.010593144659651443, Final Batch Loss: 0.001925371354445815\n",
      "Epoch 3463, Loss: 0.014214627331966767, Final Batch Loss: 0.0007005478255450726\n",
      "Epoch 3464, Loss: 0.017389357592037413, Final Batch Loss: 0.007432738319039345\n",
      "Epoch 3465, Loss: 0.010052972780613345, Final Batch Loss: 0.00023381666687782854\n",
      "Epoch 3466, Loss: 0.006965642511204351, Final Batch Loss: 0.00028912644484080374\n",
      "Epoch 3467, Loss: 0.017135036690888228, Final Batch Loss: 0.00042499735718593\n",
      "Epoch 3468, Loss: 0.022047776692488696, Final Batch Loss: 0.0012982807820662856\n",
      "Epoch 3469, Loss: 0.003207722409570124, Final Batch Loss: 2.5522211217321455e-05\n",
      "Epoch 3470, Loss: 0.01343156737220852, Final Batch Loss: 4.675029049394652e-05\n",
      "Epoch 3471, Loss: 0.00435258270954364, Final Batch Loss: 0.0004920614301227033\n",
      "Epoch 3472, Loss: 0.016600330814981135, Final Batch Loss: 0.00018662271031644195\n",
      "Epoch 3473, Loss: 0.0167087590398296, Final Batch Loss: 0.0006317411898635328\n",
      "Epoch 3474, Loss: 0.015946532832458615, Final Batch Loss: 0.00013795128325000405\n",
      "Epoch 3475, Loss: 0.006483923237283307, Final Batch Loss: 1.4677941180707421e-05\n",
      "Epoch 3476, Loss: 0.003333169865072705, Final Batch Loss: 0.0002562767476774752\n",
      "Epoch 3477, Loss: 0.02485804986099538, Final Batch Loss: 0.0032213437370955944\n",
      "Epoch 3478, Loss: 0.023810072271771787, Final Batch Loss: 3.4558244806248695e-05\n",
      "Epoch 3479, Loss: 0.031819255712434824, Final Batch Loss: 0.000821904162876308\n",
      "Epoch 3480, Loss: 0.013605626892967848, Final Batch Loss: 0.006020112428814173\n",
      "Epoch 3481, Loss: 0.009629234482417814, Final Batch Loss: 0.00013265195593703538\n",
      "Epoch 3482, Loss: 0.0100762643305643, Final Batch Loss: 1.7353995644953102e-05\n",
      "Epoch 3483, Loss: 0.03213612706622371, Final Batch Loss: 0.0004614781937561929\n",
      "Epoch 3484, Loss: 0.03152730595320463, Final Batch Loss: 0.0018337937071919441\n",
      "Epoch 3485, Loss: 0.022182205982971936, Final Batch Loss: 0.0032410151325166225\n",
      "Epoch 3486, Loss: 0.031237305025570095, Final Batch Loss: 0.009712912142276764\n",
      "Epoch 3487, Loss: 0.006201695643540006, Final Batch Loss: 0.002202426316216588\n",
      "Epoch 3488, Loss: 0.004557641997962492, Final Batch Loss: 8.237893780460581e-05\n",
      "Epoch 3489, Loss: 0.05198523975741409, Final Batch Loss: 6.0992555518168956e-05\n",
      "Epoch 3490, Loss: 0.01678445132347406, Final Batch Loss: 0.0011835533659905195\n",
      "Epoch 3491, Loss: 0.03183049911217495, Final Batch Loss: 2.029390998359304e-05\n",
      "Epoch 3492, Loss: 0.04241462285426678, Final Batch Loss: 0.0013496491592377424\n",
      "Epoch 3493, Loss: 0.018658552795841388, Final Batch Loss: 9.492372191743925e-05\n",
      "Epoch 3494, Loss: 0.027395308804898377, Final Batch Loss: 0.00020570933702401817\n",
      "Epoch 3495, Loss: 0.05224735958745441, Final Batch Loss: 0.0019529379205778241\n",
      "Epoch 3496, Loss: 0.01310331992317515, Final Batch Loss: 0.004000596236437559\n",
      "Epoch 3497, Loss: 0.08184522899955482, Final Batch Loss: 0.020818421617150307\n",
      "Epoch 3498, Loss: 0.017369680099363904, Final Batch Loss: 0.00014588775229640305\n",
      "Epoch 3499, Loss: 0.03687239192549896, Final Batch Loss: 0.004217072855681181\n",
      "Epoch 3500, Loss: 0.064136523002162, Final Batch Loss: 0.00040328592876903713\n",
      "Epoch 3501, Loss: 0.015778122364281444, Final Batch Loss: 0.00012528695515356958\n",
      "Epoch 3502, Loss: 0.016923799663345562, Final Batch Loss: 0.00017879760707728565\n",
      "Epoch 3503, Loss: 0.013783062671791413, Final Batch Loss: 9.363420758745633e-06\n",
      "Epoch 3504, Loss: 0.0062707923407288035, Final Batch Loss: 0.00015839982370380312\n",
      "Epoch 3505, Loss: 0.006271336113059078, Final Batch Loss: 0.000704806880094111\n",
      "Epoch 3506, Loss: 0.008681588341460156, Final Batch Loss: 5.9037749451817945e-05\n",
      "Epoch 3507, Loss: 0.047100403163312876, Final Batch Loss: 0.0001515726326033473\n",
      "Epoch 3508, Loss: 0.021393695889855735, Final Batch Loss: 7.274789095390588e-05\n",
      "Epoch 3509, Loss: 0.060180629798196605, Final Batch Loss: 0.00032982297125272453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3510, Loss: 0.010087793581533333, Final Batch Loss: 7.127150456653908e-05\n",
      "Epoch 3511, Loss: 0.03358555742670433, Final Batch Loss: 0.0032341794576495886\n",
      "Epoch 3512, Loss: 0.03877004568857956, Final Batch Loss: 0.0011867102002725005\n",
      "Epoch 3513, Loss: 0.020095292959922517, Final Batch Loss: 3.8361144106602296e-05\n",
      "Epoch 3514, Loss: 0.010872945253140642, Final Batch Loss: 4.3715215724660084e-05\n",
      "Epoch 3515, Loss: 0.026948984688715427, Final Batch Loss: 4.4438671466195956e-05\n",
      "Epoch 3516, Loss: 0.028799193540180568, Final Batch Loss: 0.0008166244369931519\n",
      "Epoch 3517, Loss: 0.049368336076440755, Final Batch Loss: 4.731936860480346e-05\n",
      "Epoch 3518, Loss: 0.014421972235140856, Final Batch Loss: 0.0002512220817152411\n",
      "Epoch 3519, Loss: 0.011318462780764094, Final Batch Loss: 0.002452543703839183\n",
      "Epoch 3520, Loss: 0.006263192413825891, Final Batch Loss: 0.0006422289879992604\n",
      "Epoch 3521, Loss: 0.010507069660889101, Final Batch Loss: 0.00017439763178117573\n",
      "Epoch 3522, Loss: 0.014543387762387283, Final Batch Loss: 0.00045049647451378405\n",
      "Epoch 3523, Loss: 0.02433325955462351, Final Batch Loss: 7.489779818570241e-05\n",
      "Epoch 3524, Loss: 0.04662979534623446, Final Batch Loss: 0.0009828588226810098\n",
      "Epoch 3525, Loss: 0.05282657753559761, Final Batch Loss: 0.0003146604576613754\n",
      "Epoch 3526, Loss: 0.0715611428649936, Final Batch Loss: 0.048265136778354645\n",
      "Epoch 3527, Loss: 0.029413299096631818, Final Batch Loss: 0.00015052207163535058\n",
      "Epoch 3528, Loss: 0.036190210390486754, Final Batch Loss: 0.00028952161665074527\n",
      "Epoch 3529, Loss: 0.062452249108901015, Final Batch Loss: 0.0012509020743891597\n",
      "Epoch 3530, Loss: 0.012309478363022208, Final Batch Loss: 0.00021261612710077316\n",
      "Epoch 3531, Loss: 0.0225854907912435, Final Batch Loss: 0.0028547372203320265\n",
      "Epoch 3532, Loss: 0.01683532847528113, Final Batch Loss: 0.0008173003443516791\n",
      "Epoch 3533, Loss: 0.053258155104231264, Final Batch Loss: 5.306955790729262e-05\n",
      "Epoch 3534, Loss: 0.010682735628506634, Final Batch Loss: 0.0005298673058860004\n",
      "Epoch 3535, Loss: 0.02194252555273124, Final Batch Loss: 3.796915916609578e-05\n",
      "Epoch 3536, Loss: 0.013090659260342363, Final Batch Loss: 0.00012198040349176154\n",
      "Epoch 3537, Loss: 0.02253321118223539, Final Batch Loss: 0.014415767043828964\n",
      "Epoch 3538, Loss: 0.01639849805360427, Final Batch Loss: 3.91739777114708e-05\n",
      "Epoch 3539, Loss: 0.11610045026372973, Final Batch Loss: 0.002290502190589905\n",
      "Epoch 3540, Loss: 0.025890040491503896, Final Batch Loss: 0.0027715021278709173\n",
      "Epoch 3541, Loss: 0.011820452200481668, Final Batch Loss: 0.00045786250848323107\n",
      "Epoch 3542, Loss: 0.039823728966439376, Final Batch Loss: 3.983810529462062e-05\n",
      "Epoch 3543, Loss: 0.017097586751333438, Final Batch Loss: 0.0006736244540661573\n",
      "Epoch 3544, Loss: 0.018422004231979372, Final Batch Loss: 0.0002782668743748218\n",
      "Epoch 3545, Loss: 0.032196295678659226, Final Batch Loss: 0.0012937982100993395\n",
      "Epoch 3546, Loss: 0.03431355794782576, Final Batch Loss: 0.0006620050990022719\n",
      "Epoch 3547, Loss: 0.05943287536865682, Final Batch Loss: 0.0002953283837996423\n",
      "Epoch 3548, Loss: 0.021949501242488623, Final Batch Loss: 0.0005528375622816384\n",
      "Epoch 3549, Loss: 0.017474642019806197, Final Batch Loss: 9.624286030884832e-05\n",
      "Epoch 3550, Loss: 0.0112600527027098, Final Batch Loss: 0.0001407801901223138\n",
      "Epoch 3551, Loss: 0.006221235986231477, Final Batch Loss: 0.0004970303270965815\n",
      "Epoch 3552, Loss: 0.015087901349033928, Final Batch Loss: 3.445490074227564e-05\n",
      "Epoch 3553, Loss: 0.009766814120666822, Final Batch Loss: 0.0006267005810514092\n",
      "Epoch 3554, Loss: 0.00603717202648113, Final Batch Loss: 0.00013902773207519203\n",
      "Epoch 3555, Loss: 0.010537412748817587, Final Batch Loss: 8.927242015488446e-05\n",
      "Epoch 3556, Loss: 0.025665939048849395, Final Batch Loss: 0.00039887664024718106\n",
      "Epoch 3557, Loss: 0.039091964848921634, Final Batch Loss: 0.0012540466850623488\n",
      "Epoch 3558, Loss: 0.011791786978392338, Final Batch Loss: 0.00031666879658587277\n",
      "Epoch 3559, Loss: 0.047057855226739775, Final Batch Loss: 7.238257239805534e-05\n",
      "Epoch 3560, Loss: 0.031895479707600316, Final Batch Loss: 0.0007875559385865927\n",
      "Epoch 3561, Loss: 0.028890405850688694, Final Batch Loss: 0.00013516523176804185\n",
      "Epoch 3562, Loss: 0.009620127482776297, Final Batch Loss: 0.0029104051645845175\n",
      "Epoch 3563, Loss: 0.036221195528924, Final Batch Loss: 0.009780021384358406\n",
      "Epoch 3564, Loss: 0.024537498804420466, Final Batch Loss: 0.001390910940244794\n",
      "Epoch 3565, Loss: 0.013829300114593934, Final Batch Loss: 0.003812949638813734\n",
      "Epoch 3566, Loss: 0.037580967151370714, Final Batch Loss: 2.0106492229388095e-05\n",
      "Epoch 3567, Loss: 0.019336951823788695, Final Batch Loss: 0.00017334513540845364\n",
      "Epoch 3568, Loss: 0.024876623985619517, Final Batch Loss: 0.00012516519927885383\n",
      "Epoch 3569, Loss: 0.03443301679180877, Final Batch Loss: 5.2483243052847683e-05\n",
      "Epoch 3570, Loss: 0.008832186083964189, Final Batch Loss: 0.0007330219959840178\n",
      "Epoch 3571, Loss: 0.00715994110214524, Final Batch Loss: 0.00010980325896525756\n",
      "Epoch 3572, Loss: 0.04359630665749137, Final Batch Loss: 0.00019794476975221187\n",
      "Epoch 3573, Loss: 0.014172582675655576, Final Batch Loss: 7.038105104584247e-05\n",
      "Epoch 3574, Loss: 0.030521858290740056, Final Batch Loss: 0.004494515247642994\n",
      "Epoch 3575, Loss: 0.024145210479218804, Final Batch Loss: 0.0022183761466294527\n",
      "Epoch 3576, Loss: 0.05234539691264217, Final Batch Loss: 0.002627934329211712\n",
      "Epoch 3577, Loss: 0.03452799619844882, Final Batch Loss: 0.0014535614755004644\n",
      "Epoch 3578, Loss: 0.014066000880120555, Final Batch Loss: 0.0011085948208346963\n",
      "Epoch 3579, Loss: 0.010131375267519616, Final Batch Loss: 0.00013084994861856103\n",
      "Epoch 3580, Loss: 0.043329100594746706, Final Batch Loss: 0.0012533616973087192\n",
      "Epoch 3581, Loss: 0.017647165444486745, Final Batch Loss: 0.0001788506197044626\n",
      "Epoch 3582, Loss: 0.03905113841938146, Final Batch Loss: 0.0019279608968645334\n",
      "Epoch 3583, Loss: 0.005826710706969607, Final Batch Loss: 0.00021497321722563356\n",
      "Epoch 3584, Loss: 0.007259608365529857, Final Batch Loss: 0.00038782673073001206\n",
      "Epoch 3585, Loss: 0.03136297944274702, Final Batch Loss: 0.027897903695702553\n",
      "Epoch 3586, Loss: 0.033811927132774144, Final Batch Loss: 0.0007773302495479584\n",
      "Epoch 3587, Loss: 0.04286176784808049, Final Batch Loss: 0.0004910726565867662\n",
      "Epoch 3588, Loss: 0.03474553257547086, Final Batch Loss: 0.00017728348029777408\n",
      "Epoch 3589, Loss: 0.01883054310019361, Final Batch Loss: 0.0023814274463802576\n",
      "Epoch 3590, Loss: 0.022265507155680098, Final Batch Loss: 0.0006097417790442705\n",
      "Epoch 3591, Loss: 0.010411954379378585, Final Batch Loss: 8.238008740590885e-05\n",
      "Epoch 3592, Loss: 0.008780143416515784, Final Batch Loss: 0.00011440857633715495\n",
      "Epoch 3593, Loss: 0.012923710943141486, Final Batch Loss: 6.279502849793062e-05\n",
      "Epoch 3594, Loss: 0.005600951757514849, Final Batch Loss: 5.950663398834877e-05\n",
      "Epoch 3595, Loss: 0.0067368955478741555, Final Batch Loss: 2.760114693955984e-05\n",
      "Epoch 3596, Loss: 0.010708182755479356, Final Batch Loss: 7.672997162444517e-05\n",
      "Epoch 3597, Loss: 0.032198389797486016, Final Batch Loss: 5.908366438234225e-05\n",
      "Epoch 3598, Loss: 0.014693842607812257, Final Batch Loss: 0.0004802905605174601\n",
      "Epoch 3599, Loss: 0.01635892611102463, Final Batch Loss: 0.00015075957344379276\n",
      "Epoch 3600, Loss: 0.027389325368858408, Final Batch Loss: 6.138203025329858e-05\n",
      "Epoch 3601, Loss: 0.011485056245874148, Final Batch Loss: 5.342573786037974e-05\n",
      "Epoch 3602, Loss: 0.030497865845973138, Final Batch Loss: 0.00028456124709919095\n",
      "Epoch 3603, Loss: 0.029987049332703464, Final Batch Loss: 0.006308665964752436\n",
      "Epoch 3604, Loss: 0.010094437855514116, Final Batch Loss: 2.392116039118264e-05\n",
      "Epoch 3605, Loss: 0.013746870645263698, Final Batch Loss: 0.002983879530802369\n",
      "Epoch 3606, Loss: 0.022668947773127, Final Batch Loss: 1.1794580132118426e-05\n",
      "Epoch 3607, Loss: 0.030598901730627404, Final Batch Loss: 0.00018472100782673806\n",
      "Epoch 3608, Loss: 0.01378904287412297, Final Batch Loss: 0.005777368787676096\n",
      "Epoch 3609, Loss: 0.019892357842763886, Final Batch Loss: 0.00040829871431924403\n",
      "Epoch 3610, Loss: 0.04305959520570468, Final Batch Loss: 0.00032942480174824595\n",
      "Epoch 3611, Loss: 0.020102927948755678, Final Batch Loss: 0.005411508493125439\n",
      "Epoch 3612, Loss: 0.026744417828012956, Final Batch Loss: 0.00018057739362120628\n",
      "Epoch 3613, Loss: 0.043177335068321554, Final Batch Loss: 0.023854000493884087\n",
      "Epoch 3614, Loss: 0.040038981474026514, Final Batch Loss: 1.8271255612489767e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3615, Loss: 0.009907104134981637, Final Batch Loss: 0.00043750330223701894\n",
      "Epoch 3616, Loss: 0.013823448709445074, Final Batch Loss: 0.0013911445857957006\n",
      "Epoch 3617, Loss: 0.031027599161461694, Final Batch Loss: 0.001514974981546402\n",
      "Epoch 3618, Loss: 0.013615469291835325, Final Batch Loss: 0.0020747387316077948\n",
      "Epoch 3619, Loss: 0.04071308668972051, Final Batch Loss: 0.00029124057618901134\n",
      "Epoch 3620, Loss: 0.022360724435202428, Final Batch Loss: 2.9527936931117438e-05\n",
      "Epoch 3621, Loss: 0.006007111120197806, Final Batch Loss: 0.0006354221259243786\n",
      "Epoch 3622, Loss: 0.015478176461328985, Final Batch Loss: 0.0022444662172347307\n",
      "Epoch 3623, Loss: 0.014389403606401174, Final Batch Loss: 6.923884939169511e-05\n",
      "Epoch 3624, Loss: 0.023358275472673995, Final Batch Loss: 0.0035490859299898148\n",
      "Epoch 3625, Loss: 0.0020882415901724016, Final Batch Loss: 0.00021666350949089974\n",
      "Epoch 3626, Loss: 0.005657322766637662, Final Batch Loss: 2.3699150915490463e-05\n",
      "Epoch 3627, Loss: 0.05055067682405934, Final Batch Loss: 5.1719540351768956e-05\n",
      "Epoch 3628, Loss: 0.006019825115799904, Final Batch Loss: 0.0002695696894079447\n",
      "Epoch 3629, Loss: 0.008526441737558343, Final Batch Loss: 0.00010308533092029393\n",
      "Epoch 3630, Loss: 0.00793749008244049, Final Batch Loss: 7.671651110285893e-05\n",
      "Epoch 3631, Loss: 0.026420142286042392, Final Batch Loss: 1.698650885373354e-05\n",
      "Epoch 3632, Loss: 0.012726982353342464, Final Batch Loss: 0.004547927062958479\n",
      "Epoch 3633, Loss: 0.009492820087871223, Final Batch Loss: 0.00263622566126287\n",
      "Epoch 3634, Loss: 0.006321746569483366, Final Batch Loss: 0.0028038297314196825\n",
      "Epoch 3635, Loss: 0.052344051930504065, Final Batch Loss: 2.4952611056505702e-05\n",
      "Epoch 3636, Loss: 0.0264815099808402, Final Batch Loss: 5.171777229406871e-05\n",
      "Epoch 3637, Loss: 0.03243047981595737, Final Batch Loss: 3.7319809052860364e-05\n",
      "Epoch 3638, Loss: 0.0372405973671448, Final Batch Loss: 0.0003798625257331878\n",
      "Epoch 3639, Loss: 0.0385224303281575, Final Batch Loss: 0.00010719428973970935\n",
      "Epoch 3640, Loss: 0.026293241726307315, Final Batch Loss: 7.21760225133039e-05\n",
      "Epoch 3641, Loss: 0.010756596366263693, Final Batch Loss: 0.0009011333459056914\n",
      "Epoch 3642, Loss: 0.016695888398317038, Final Batch Loss: 6.709615263389423e-05\n",
      "Epoch 3643, Loss: 0.01667230103703332, Final Batch Loss: 0.0005077458918094635\n",
      "Epoch 3644, Loss: 0.00811269207042642, Final Batch Loss: 0.004076559096574783\n",
      "Epoch 3645, Loss: 0.03141937637337833, Final Batch Loss: 0.00914275087416172\n",
      "Epoch 3646, Loss: 0.03581880199271836, Final Batch Loss: 0.0020905265118926764\n",
      "Epoch 3647, Loss: 0.007685678248890326, Final Batch Loss: 0.0002668833185452968\n",
      "Epoch 3648, Loss: 0.006897686123920721, Final Batch Loss: 0.0012280537048354745\n",
      "Epoch 3649, Loss: 0.04794674484310235, Final Batch Loss: 0.016734210774302483\n",
      "Epoch 3650, Loss: 0.03686132366237871, Final Batch Loss: 0.0005662469775415957\n",
      "Epoch 3651, Loss: 0.022216115743503906, Final Batch Loss: 0.0006186112877912819\n",
      "Epoch 3652, Loss: 0.014075258382945322, Final Batch Loss: 0.0002333974844077602\n",
      "Epoch 3653, Loss: 0.03401324782316806, Final Batch Loss: 0.001900063594803214\n",
      "Epoch 3654, Loss: 0.022521165519719943, Final Batch Loss: 0.0031234887428581715\n",
      "Epoch 3655, Loss: 0.011515198730194243, Final Batch Loss: 0.0009048711508512497\n",
      "Epoch 3656, Loss: 0.02431812652866938, Final Batch Loss: 0.00011691590771079063\n",
      "Epoch 3657, Loss: 0.04334591565930168, Final Batch Loss: 0.007167001720517874\n",
      "Epoch 3658, Loss: 0.021130485576577485, Final Batch Loss: 0.0007540618535131216\n",
      "Epoch 3659, Loss: 0.011439833735494176, Final Batch Loss: 0.0006563924835063517\n",
      "Epoch 3660, Loss: 0.008228332200815203, Final Batch Loss: 0.0011658193543553352\n",
      "Epoch 3661, Loss: 0.013292966832523234, Final Batch Loss: 0.003955697175115347\n",
      "Epoch 3662, Loss: 0.06252398830110906, Final Batch Loss: 0.009429384022951126\n",
      "Epoch 3663, Loss: 0.025798604205192532, Final Batch Loss: 0.0015347818844020367\n",
      "Epoch 3664, Loss: 0.038574077977500565, Final Batch Loss: 0.00017301175103057176\n",
      "Epoch 3665, Loss: 0.023845508101658197, Final Batch Loss: 0.0024155674036592245\n",
      "Epoch 3666, Loss: 0.013834594014042523, Final Batch Loss: 0.0015736515633761883\n",
      "Epoch 3667, Loss: 0.05063192274974426, Final Batch Loss: 0.0013437292072921991\n",
      "Epoch 3668, Loss: 0.03183577917297953, Final Batch Loss: 0.0004267750773578882\n",
      "Epoch 3669, Loss: 0.014858931135677267, Final Batch Loss: 0.0018906091572716832\n",
      "Epoch 3670, Loss: 0.010225310905298102, Final Batch Loss: 2.2510996132041328e-05\n",
      "Epoch 3671, Loss: 0.01841682827216573, Final Batch Loss: 0.003008649218827486\n",
      "Epoch 3672, Loss: 0.05356955770912464, Final Batch Loss: 0.0030898533295840025\n",
      "Epoch 3673, Loss: 0.012714571988908574, Final Batch Loss: 0.0008369694114662707\n",
      "Epoch 3674, Loss: 0.01618715421136585, Final Batch Loss: 0.00022276125673670322\n",
      "Epoch 3675, Loss: 0.025171693127049366, Final Batch Loss: 4.6111214032862335e-05\n",
      "Epoch 3676, Loss: 0.09333128138678148, Final Batch Loss: 0.00042340788058936596\n",
      "Epoch 3677, Loss: 0.01500367440894479, Final Batch Loss: 0.00018648931290954351\n",
      "Epoch 3678, Loss: 0.013727286016546714, Final Batch Loss: 5.528490510187112e-05\n",
      "Epoch 3679, Loss: 0.004988303640857339, Final Batch Loss: 0.00014433883188758045\n",
      "Epoch 3680, Loss: 0.009607087296899408, Final Batch Loss: 8.683950727572665e-05\n",
      "Epoch 3681, Loss: 0.02025773546210985, Final Batch Loss: 1.3120590665494092e-05\n",
      "Epoch 3682, Loss: 0.04494837249058037, Final Batch Loss: 0.00014502972771879286\n",
      "Epoch 3683, Loss: 0.0683397600587341, Final Batch Loss: 0.000351312686689198\n",
      "Epoch 3684, Loss: 0.022909983723366167, Final Batch Loss: 7.573253242298961e-05\n",
      "Epoch 3685, Loss: 0.033116117830104486, Final Batch Loss: 0.00019026191148441285\n",
      "Epoch 3686, Loss: 0.031087082254089182, Final Batch Loss: 0.000534950231667608\n",
      "Epoch 3687, Loss: 0.01558417838123205, Final Batch Loss: 0.0006169119733385742\n",
      "Epoch 3688, Loss: 0.02032614098789054, Final Batch Loss: 0.00033674106816761196\n",
      "Epoch 3689, Loss: 0.024893857373172068, Final Batch Loss: 0.0026462427340447903\n",
      "Epoch 3690, Loss: 0.027933946928897058, Final Batch Loss: 0.000619704311247915\n",
      "Epoch 3691, Loss: 0.018021161282376852, Final Batch Loss: 0.00041656725807115436\n",
      "Epoch 3692, Loss: 0.016248169134996715, Final Batch Loss: 0.004038974642753601\n",
      "Epoch 3693, Loss: 0.08616307791089639, Final Batch Loss: 0.003778084646910429\n",
      "Epoch 3694, Loss: 0.020851750857218576, Final Batch Loss: 0.0012479893630370498\n",
      "Epoch 3695, Loss: 0.08623882657047943, Final Batch Loss: 0.001125403679907322\n",
      "Epoch 3696, Loss: 0.04229790215322282, Final Batch Loss: 0.007628119550645351\n",
      "Epoch 3697, Loss: 0.01751951604092028, Final Batch Loss: 8.599343709647655e-05\n",
      "Epoch 3698, Loss: 0.022871120296258596, Final Batch Loss: 1.2008795238216408e-05\n",
      "Epoch 3699, Loss: 0.009457375854253769, Final Batch Loss: 0.0009928139625117183\n",
      "Epoch 3700, Loss: 0.006778235540878086, Final Batch Loss: 0.0002602203458081931\n",
      "Epoch 3701, Loss: 0.004188491126114968, Final Batch Loss: 0.0004689594206865877\n",
      "Epoch 3702, Loss: 0.03544952447373362, Final Batch Loss: 0.02228880673646927\n",
      "Epoch 3703, Loss: 0.027653643565827224, Final Batch Loss: 8.710868314665277e-06\n",
      "Epoch 3704, Loss: 0.005177866320082103, Final Batch Loss: 1.3650223991135135e-05\n",
      "Epoch 3705, Loss: 0.008758085092722467, Final Batch Loss: 0.0008365738322027028\n",
      "Epoch 3706, Loss: 0.0053066441032569855, Final Batch Loss: 0.0001775229611666873\n",
      "Epoch 3707, Loss: 0.003820334430656658, Final Batch Loss: 0.00019976588373538107\n",
      "Epoch 3708, Loss: 0.0019527855201886268, Final Batch Loss: 0.0008316437597386539\n",
      "Epoch 3709, Loss: 0.025764025125226908, Final Batch Loss: 0.00023541520931757987\n",
      "Epoch 3710, Loss: 0.0029863318477509893, Final Batch Loss: 0.0007138167275115848\n",
      "Epoch 3711, Loss: 0.028864343199529685, Final Batch Loss: 0.000994137255474925\n",
      "Epoch 3712, Loss: 0.04474841872433899, Final Batch Loss: 0.004559858236461878\n",
      "Epoch 3713, Loss: 0.021119906024978263, Final Batch Loss: 4.9863439926411957e-05\n",
      "Epoch 3714, Loss: 0.0044869625262435875, Final Batch Loss: 0.0005334352608770132\n",
      "Epoch 3715, Loss: 0.010550960008458787, Final Batch Loss: 0.0002279982581967488\n",
      "Epoch 3716, Loss: 0.004384564905194566, Final Batch Loss: 2.9960057872813195e-05\n",
      "Epoch 3717, Loss: 0.005122085914536001, Final Batch Loss: 0.00018111898680217564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3718, Loss: 0.010282396623551904, Final Batch Loss: 0.0001244246232090518\n",
      "Epoch 3719, Loss: 0.04058975317275326, Final Batch Loss: 0.0003186209069099277\n",
      "Epoch 3720, Loss: 0.030318664399601403, Final Batch Loss: 4.002416608273052e-05\n",
      "Epoch 3721, Loss: 0.006593651779439824, Final Batch Loss: 0.0010954602621495724\n",
      "Epoch 3722, Loss: 0.04623779066332645, Final Batch Loss: 0.009486964903771877\n",
      "Epoch 3723, Loss: 0.03118151291346294, Final Batch Loss: 0.012537175789475441\n",
      "Epoch 3724, Loss: 0.014935061802134442, Final Batch Loss: 0.008844345808029175\n",
      "Epoch 3725, Loss: 0.0128052753098018, Final Batch Loss: 0.005423825234174728\n",
      "Epoch 3726, Loss: 0.005151009537257778, Final Batch Loss: 4.458241892280057e-05\n",
      "Epoch 3727, Loss: 0.02106388047832297, Final Batch Loss: 0.006507586687803268\n",
      "Epoch 3728, Loss: 0.009121956144554133, Final Batch Loss: 5.19913446623832e-05\n",
      "Epoch 3729, Loss: 0.017820942721300526, Final Batch Loss: 0.0003698098589666188\n",
      "Epoch 3730, Loss: 0.06669965691980906, Final Batch Loss: 0.0007068981067277491\n",
      "Epoch 3731, Loss: 0.055595876034203684, Final Batch Loss: 0.006542495917528868\n",
      "Epoch 3732, Loss: 0.03791731177261681, Final Batch Loss: 0.010898410342633724\n",
      "Epoch 3733, Loss: 0.03816617221036722, Final Batch Loss: 9.364036668557674e-05\n",
      "Epoch 3734, Loss: 0.06267158161699626, Final Batch Loss: 9.421168942935765e-05\n",
      "Epoch 3735, Loss: 0.025868196884403005, Final Batch Loss: 5.3346375352703035e-05\n",
      "Epoch 3736, Loss: 0.016073244507424533, Final Batch Loss: 7.645680307177827e-05\n",
      "Epoch 3737, Loss: 0.012751887566992082, Final Batch Loss: 0.00023019214859232306\n",
      "Epoch 3738, Loss: 0.002426908356937929, Final Batch Loss: 2.038226739387028e-05\n",
      "Epoch 3739, Loss: 0.016693079988726822, Final Batch Loss: 0.0004029086558148265\n",
      "Epoch 3740, Loss: 0.005560013273225195, Final Batch Loss: 0.00011503064160933718\n",
      "Epoch 3741, Loss: 0.025208567102708912, Final Batch Loss: 0.001248156069777906\n",
      "Epoch 3742, Loss: 0.003591397698073706, Final Batch Loss: 8.269849058706313e-05\n",
      "Epoch 3743, Loss: 0.009895543366837956, Final Batch Loss: 0.00022650175378657877\n",
      "Epoch 3744, Loss: 0.013885430851587444, Final Batch Loss: 9.265793778467923e-05\n",
      "Epoch 3745, Loss: 0.0022504300250147935, Final Batch Loss: 2.8676740839728154e-05\n",
      "Epoch 3746, Loss: 0.006133539329312043, Final Batch Loss: 2.335040335310623e-05\n",
      "Epoch 3747, Loss: 0.0021316584585520104, Final Batch Loss: 1.7785709133022465e-05\n",
      "Epoch 3748, Loss: 0.01627067588333375, Final Batch Loss: 0.004874711856245995\n",
      "Epoch 3749, Loss: 0.031130762498833064, Final Batch Loss: 9.068083454621956e-05\n",
      "Epoch 3750, Loss: 0.04419636875081778, Final Batch Loss: 0.004694591276347637\n",
      "Epoch 3751, Loss: 0.05077400567824952, Final Batch Loss: 0.00018309538427274674\n",
      "Epoch 3752, Loss: 0.04215812088295934, Final Batch Loss: 0.0011578791309148073\n",
      "Epoch 3753, Loss: 0.06705327385134296, Final Batch Loss: 4.842791531700641e-05\n",
      "Epoch 3754, Loss: 0.029741430464127916, Final Batch Loss: 6.992847920628265e-05\n",
      "Epoch 3755, Loss: 0.037973497246639454, Final Batch Loss: 0.0005855576018802822\n",
      "Epoch 3756, Loss: 0.005986458818824758, Final Batch Loss: 0.0009478380088694394\n",
      "Epoch 3757, Loss: 0.01630929112616286, Final Batch Loss: 0.00017945098807103932\n",
      "Epoch 3758, Loss: 0.006052074873878155, Final Batch Loss: 7.343976176343858e-05\n",
      "Epoch 3759, Loss: 0.005066214156613569, Final Batch Loss: 0.00027024216251447797\n",
      "Epoch 3760, Loss: 0.01342138241034263, Final Batch Loss: 0.0022896567825227976\n",
      "Epoch 3761, Loss: 0.002993882186274277, Final Batch Loss: 8.711000555194914e-05\n",
      "Epoch 3762, Loss: 0.002590632109786384, Final Batch Loss: 0.00018662691581994295\n",
      "Epoch 3763, Loss: 0.006314596809261275, Final Batch Loss: 4.5490090997191146e-05\n",
      "Epoch 3764, Loss: 0.007397774587843742, Final Batch Loss: 0.0002650420065037906\n",
      "Epoch 3765, Loss: 0.001916336005251651, Final Batch Loss: 0.0002640447346493602\n",
      "Epoch 3766, Loss: 0.03406671880338763, Final Batch Loss: 4.3917749280808493e-05\n",
      "Epoch 3767, Loss: 0.01220312663099321, Final Batch Loss: 0.008487919345498085\n",
      "Epoch 3768, Loss: 0.0772546327807504, Final Batch Loss: 1.3541806765715592e-05\n",
      "Epoch 3769, Loss: 0.0524858199951268, Final Batch Loss: 2.0210714865243062e-05\n",
      "Epoch 3770, Loss: 0.020308685205236543, Final Batch Loss: 0.005171847064048052\n",
      "Epoch 3771, Loss: 0.01490601665500435, Final Batch Loss: 0.0052827345207333565\n",
      "Epoch 3772, Loss: 0.04544879266541102, Final Batch Loss: 0.003765014698728919\n",
      "Epoch 3773, Loss: 0.016389172442359268, Final Batch Loss: 0.002190284663811326\n",
      "Epoch 3774, Loss: 0.02209333191422047, Final Batch Loss: 0.00018438413098920137\n",
      "Epoch 3775, Loss: 0.05546166246494977, Final Batch Loss: 0.0038588806055486202\n",
      "Epoch 3776, Loss: 0.048861812669201754, Final Batch Loss: 0.00020110720652155578\n",
      "Epoch 3777, Loss: 0.017531538109324174, Final Batch Loss: 0.0004336574056651443\n",
      "Epoch 3778, Loss: 0.008258077332357061, Final Batch Loss: 0.00039861106779426336\n",
      "Epoch 3779, Loss: 0.027306876891088905, Final Batch Loss: 0.00017603272863198072\n",
      "Epoch 3780, Loss: 0.01341932442619509, Final Batch Loss: 0.00457719573751092\n",
      "Epoch 3781, Loss: 0.0036636389950217563, Final Batch Loss: 2.772473089862615e-05\n",
      "Epoch 3782, Loss: 0.011582209369407792, Final Batch Loss: 0.0005670539685525\n",
      "Epoch 3783, Loss: 0.034563273774665504, Final Batch Loss: 0.00034002630854956806\n",
      "Epoch 3784, Loss: 0.025148968641588, Final Batch Loss: 0.0003597612667363137\n",
      "Epoch 3785, Loss: 0.018737965991022065, Final Batch Loss: 2.1398394892457873e-05\n",
      "Epoch 3786, Loss: 0.015858331587878638, Final Batch Loss: 0.0036303692031651735\n",
      "Epoch 3787, Loss: 0.02373515881322419, Final Batch Loss: 1.1865897249663249e-05\n",
      "Epoch 3788, Loss: 0.04478474429015478, Final Batch Loss: 0.0013879463076591492\n",
      "Epoch 3789, Loss: 0.06481989448366221, Final Batch Loss: 0.00017692202527541667\n",
      "Epoch 3790, Loss: 0.03868819551280467, Final Batch Loss: 0.0007976106717251241\n",
      "Epoch 3791, Loss: 0.06623940151985153, Final Batch Loss: 0.0045389775186777115\n",
      "Epoch 3792, Loss: 0.010012950557211298, Final Batch Loss: 0.0004896830068901181\n",
      "Epoch 3793, Loss: 0.012329270593909314, Final Batch Loss: 0.0001767642388585955\n",
      "Epoch 3794, Loss: 0.028627757797949016, Final Batch Loss: 0.0005666891229338944\n",
      "Epoch 3795, Loss: 0.01134512551698208, Final Batch Loss: 0.0001917638728627935\n",
      "Epoch 3796, Loss: 0.005731814642786048, Final Batch Loss: 0.00016144911933224648\n",
      "Epoch 3797, Loss: 0.02508078295068117, Final Batch Loss: 1.852651621447876e-05\n",
      "Epoch 3798, Loss: 0.0055295829188253265, Final Batch Loss: 0.00029957020888105035\n",
      "Epoch 3799, Loss: 0.07547145561875368, Final Batch Loss: 9.017538104671985e-05\n",
      "Epoch 3800, Loss: 0.027563086008740356, Final Batch Loss: 5.28096170455683e-05\n",
      "Epoch 3801, Loss: 0.007618193241796689, Final Batch Loss: 0.0003418338892515749\n",
      "Epoch 3802, Loss: 0.0540811275723172, Final Batch Loss: 0.003053505439311266\n",
      "Epoch 3803, Loss: 0.005094141721201595, Final Batch Loss: 0.0003939195303246379\n",
      "Epoch 3804, Loss: 0.030486416006169748, Final Batch Loss: 9.323097765445709e-05\n",
      "Epoch 3805, Loss: 0.02799620511541434, Final Batch Loss: 0.00034345986205153167\n",
      "Epoch 3806, Loss: 0.0322200273276394, Final Batch Loss: 0.00019842619076371193\n",
      "Epoch 3807, Loss: 0.026293324013749952, Final Batch Loss: 0.0005139013519510627\n",
      "Epoch 3808, Loss: 0.022927949406948755, Final Batch Loss: 0.008544581942260265\n",
      "Epoch 3809, Loss: 0.02628038369493879, Final Batch Loss: 0.00039596162969246507\n",
      "Epoch 3810, Loss: 0.045813657612598035, Final Batch Loss: 0.0011887536384165287\n",
      "Epoch 3811, Loss: 0.041781604080824764, Final Batch Loss: 0.00019398743461351842\n",
      "Epoch 3812, Loss: 0.025092546549785766, Final Batch Loss: 3.590995765989646e-05\n",
      "Epoch 3813, Loss: 0.008490473303027102, Final Batch Loss: 0.00025977095356211066\n",
      "Epoch 3814, Loss: 0.014129845840216149, Final Batch Loss: 0.00023235510161612183\n",
      "Epoch 3815, Loss: 0.036254824030038435, Final Batch Loss: 0.0001864656078396365\n",
      "Epoch 3816, Loss: 0.010512609105717274, Final Batch Loss: 0.0017527558375149965\n",
      "Epoch 3817, Loss: 0.0048816870566952275, Final Batch Loss: 0.0001723978784866631\n",
      "Epoch 3818, Loss: 0.013494543562046601, Final Batch Loss: 0.0005317626637406647\n",
      "Epoch 3819, Loss: 0.0027695338867488317, Final Batch Loss: 7.642636774107814e-05\n",
      "Epoch 3820, Loss: 0.0189403513313664, Final Batch Loss: 3.839820419671014e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3821, Loss: 0.020013375080452533, Final Batch Loss: 0.00817183032631874\n",
      "Epoch 3822, Loss: 0.03596827699584537, Final Batch Loss: 0.0004914046730846167\n",
      "Epoch 3823, Loss: 0.005245997177553363, Final Batch Loss: 0.0004700478748418391\n",
      "Epoch 3824, Loss: 0.11928874774457654, Final Batch Loss: 0.029590807855129242\n",
      "Epoch 3825, Loss: 0.044538535075844266, Final Batch Loss: 0.0001020902709569782\n",
      "Epoch 3826, Loss: 0.03961186019296292, Final Batch Loss: 0.00018904411990661174\n",
      "Epoch 3827, Loss: 0.017916292483278085, Final Batch Loss: 0.005557800643146038\n",
      "Epoch 3828, Loss: 0.02832665825189906, Final Batch Loss: 0.0011190942022949457\n",
      "Epoch 3829, Loss: 0.04635228282859316, Final Batch Loss: 0.0029440801590681076\n",
      "Epoch 3830, Loss: 0.04096010654757265, Final Batch Loss: 0.002097778022289276\n",
      "Epoch 3831, Loss: 0.0808899068942992, Final Batch Loss: 0.006848395336419344\n",
      "Epoch 3832, Loss: 0.021414159011328593, Final Batch Loss: 0.0025238364469259977\n",
      "Epoch 3833, Loss: 0.016867362826815224, Final Batch Loss: 7.856188312871382e-05\n",
      "Epoch 3834, Loss: 0.019000517659151228, Final Batch Loss: 0.00023487723956350237\n",
      "Epoch 3835, Loss: 0.014504207527352264, Final Batch Loss: 0.0014277343871071935\n",
      "Epoch 3836, Loss: 0.004687270640715724, Final Batch Loss: 0.0008755465387366712\n",
      "Epoch 3837, Loss: 0.02468306478476734, Final Batch Loss: 0.0012103571789339185\n",
      "Epoch 3838, Loss: 0.03421880802125088, Final Batch Loss: 0.0003827032051049173\n",
      "Epoch 3839, Loss: 0.049117838894744636, Final Batch Loss: 0.0020670094527304173\n",
      "Epoch 3840, Loss: 0.010218695335424854, Final Batch Loss: 0.0001787143701221794\n",
      "Epoch 3841, Loss: 0.005472216372822913, Final Batch Loss: 7.56173103582114e-05\n",
      "Epoch 3842, Loss: 0.023560838082630653, Final Batch Loss: 0.002099516335874796\n",
      "Epoch 3843, Loss: 0.013612572448437277, Final Batch Loss: 6.748567102476954e-05\n",
      "Epoch 3844, Loss: 0.009139140654951916, Final Batch Loss: 0.0004450367414392531\n",
      "Epoch 3845, Loss: 0.05047279497375712, Final Batch Loss: 0.0007413518032990396\n",
      "Epoch 3846, Loss: 0.06540921315172454, Final Batch Loss: 7.045247912174091e-05\n",
      "Epoch 3847, Loss: 0.023240960837938474, Final Batch Loss: 0.007595920003950596\n",
      "Epoch 3848, Loss: 0.015016831586763146, Final Batch Loss: 3.587438550312072e-05\n",
      "Epoch 3849, Loss: 0.0787784404528793, Final Batch Loss: 0.004076089709997177\n",
      "Epoch 3850, Loss: 0.032393273128036526, Final Batch Loss: 9.110158134717494e-05\n",
      "Epoch 3851, Loss: 0.04177443556909566, Final Batch Loss: 0.00012101675383746624\n",
      "Epoch 3852, Loss: 0.046596490843512584, Final Batch Loss: 0.004047126974910498\n",
      "Epoch 3853, Loss: 0.07672868338704575, Final Batch Loss: 0.0009330098400823772\n",
      "Epoch 3854, Loss: 0.04382359024748439, Final Batch Loss: 0.00398102356120944\n",
      "Epoch 3855, Loss: 0.04655625305167632, Final Batch Loss: 0.00019494909793138504\n",
      "Epoch 3856, Loss: 0.03516008327642339, Final Batch Loss: 7.18134906492196e-05\n",
      "Epoch 3857, Loss: 0.024888247673516162, Final Batch Loss: 0.00014847272541373968\n",
      "Epoch 3858, Loss: 0.02556978041684488, Final Batch Loss: 0.00012435478856787086\n",
      "Epoch 3859, Loss: 0.017703904160953243, Final Batch Loss: 0.0009125411743298173\n",
      "Epoch 3860, Loss: 0.00826073771895608, Final Batch Loss: 0.00017553259385749698\n",
      "Epoch 3861, Loss: 0.01681349999307713, Final Batch Loss: 0.00048339652130380273\n",
      "Epoch 3862, Loss: 0.009999291360145435, Final Batch Loss: 0.0004423338104970753\n",
      "Epoch 3863, Loss: 0.005470640218845801, Final Batch Loss: 7.62566487537697e-05\n",
      "Epoch 3864, Loss: 0.003454293516369944, Final Batch Loss: 0.0004929458955302835\n",
      "Epoch 3865, Loss: 0.0048875959546421655, Final Batch Loss: 0.0004645303706638515\n",
      "Epoch 3866, Loss: 0.007863764182729938, Final Batch Loss: 0.00012637309555429965\n",
      "Epoch 3867, Loss: 0.00787926393422822, Final Batch Loss: 0.000564213318284601\n",
      "Epoch 3868, Loss: 0.002443905954805814, Final Batch Loss: 5.202912871027365e-05\n",
      "Epoch 3869, Loss: 0.0026280886208951415, Final Batch Loss: 0.00021403867867775261\n",
      "Epoch 3870, Loss: 0.00105739445598374, Final Batch Loss: 2.566850344010163e-05\n",
      "Epoch 3871, Loss: 0.02710769357463505, Final Batch Loss: 0.002664668718352914\n",
      "Epoch 3872, Loss: 0.0021753181990789017, Final Batch Loss: 0.00039925076998770237\n",
      "Epoch 3873, Loss: 0.011535030570939853, Final Batch Loss: 0.0001390799880027771\n",
      "Epoch 3874, Loss: 0.026295464219401765, Final Batch Loss: 0.0003097975568380207\n",
      "Epoch 3875, Loss: 0.07560267171356827, Final Batch Loss: 0.00568809499964118\n",
      "Epoch 3876, Loss: 0.03168063946395705, Final Batch Loss: 0.00392855703830719\n",
      "Epoch 3877, Loss: 0.032049698624177836, Final Batch Loss: 0.0007989938603714108\n",
      "Epoch 3878, Loss: 0.011873346470565593, Final Batch Loss: 0.0009185108356177807\n",
      "Epoch 3879, Loss: 0.012346847267508565, Final Batch Loss: 0.0010958120692521334\n",
      "Epoch 3880, Loss: 0.0020590921994880773, Final Batch Loss: 0.00023521570255979896\n",
      "Epoch 3881, Loss: 0.021369864465668797, Final Batch Loss: 0.003061046591028571\n",
      "Epoch 3882, Loss: 0.005827696808864857, Final Batch Loss: 4.106267078896053e-05\n",
      "Epoch 3883, Loss: 0.012786035486669789, Final Batch Loss: 3.2312748317053774e-06\n",
      "Epoch 3884, Loss: 0.016012558978218294, Final Batch Loss: 0.009549777023494244\n",
      "Epoch 3885, Loss: 0.02126832283101976, Final Batch Loss: 0.000877968268468976\n",
      "Epoch 3886, Loss: 0.024677150873685605, Final Batch Loss: 0.013548506423830986\n",
      "Epoch 3887, Loss: 0.015430488204401627, Final Batch Loss: 0.0010048751719295979\n",
      "Epoch 3888, Loss: 0.0118704457381682, Final Batch Loss: 0.00027932372177019715\n",
      "Epoch 3889, Loss: 0.022944615048800188, Final Batch Loss: 0.00012945303751621395\n",
      "Epoch 3890, Loss: 0.021943149543858453, Final Batch Loss: 6.370385563059244e-06\n",
      "Epoch 3891, Loss: 0.028244864155567484, Final Batch Loss: 8.557153341826051e-05\n",
      "Epoch 3892, Loss: 0.00993995197404729, Final Batch Loss: 0.0007015242008492351\n",
      "Epoch 3893, Loss: 0.011920364812795015, Final Batch Loss: 4.409768007462844e-05\n",
      "Epoch 3894, Loss: 0.024510810079391376, Final Batch Loss: 0.0027921514119952917\n",
      "Epoch 3895, Loss: 0.00317395325328107, Final Batch Loss: 0.00030363068799488246\n",
      "Epoch 3896, Loss: 0.0018125382166545023, Final Batch Loss: 0.0001622444688109681\n",
      "Epoch 3897, Loss: 0.009903944424422662, Final Batch Loss: 0.001189540489576757\n",
      "Epoch 3898, Loss: 0.040463003549803034, Final Batch Loss: 0.00042051999480463564\n",
      "Epoch 3899, Loss: 0.010656017473479551, Final Batch Loss: 4.22891098423861e-05\n",
      "Epoch 3900, Loss: 0.009364866971054653, Final Batch Loss: 0.0008633077959530056\n",
      "Epoch 3901, Loss: 0.0025970902410108465, Final Batch Loss: 7.884861588536296e-06\n",
      "Epoch 3902, Loss: 0.05303254500950061, Final Batch Loss: 0.004436416085809469\n",
      "Epoch 3903, Loss: 0.11192928490618215, Final Batch Loss: 1.4914873645466287e-05\n",
      "Epoch 3904, Loss: 0.03567461718739651, Final Batch Loss: 0.000258761050645262\n",
      "Epoch 3905, Loss: 0.025762088405826944, Final Batch Loss: 4.6628563723061234e-05\n",
      "Epoch 3906, Loss: 0.053579404440824874, Final Batch Loss: 0.004345578141510487\n",
      "Epoch 3907, Loss: 0.039718964952953684, Final Batch Loss: 7.832539995433763e-05\n",
      "Epoch 3908, Loss: 0.0840646243741503, Final Batch Loss: 0.013665401376783848\n",
      "Epoch 3909, Loss: 0.0654974622411828, Final Batch Loss: 0.0007939522038213909\n",
      "Epoch 3910, Loss: 0.08119992362844641, Final Batch Loss: 0.0019500759663060308\n",
      "Epoch 3911, Loss: 0.03980669478914933, Final Batch Loss: 0.0012924819020554423\n",
      "Epoch 3912, Loss: 0.025719805460539646, Final Batch Loss: 0.00015965708007570356\n",
      "Epoch 3913, Loss: 0.01661167847487377, Final Batch Loss: 0.0010563646210357547\n",
      "Epoch 3914, Loss: 0.04068796811588982, Final Batch Loss: 0.004699159413576126\n",
      "Epoch 3915, Loss: 0.02124201149945293, Final Batch Loss: 0.0001524873950984329\n",
      "Epoch 3916, Loss: 0.01277570553793339, Final Batch Loss: 0.0003536585718393326\n",
      "Epoch 3917, Loss: 0.006621493965212721, Final Batch Loss: 0.0008529565529897809\n",
      "Epoch 3918, Loss: 0.008070589661656413, Final Batch Loss: 9.331309411209077e-05\n",
      "Epoch 3919, Loss: 0.005868141390237724, Final Batch Loss: 0.00036241585621610284\n",
      "Epoch 3920, Loss: 0.006367274252625066, Final Batch Loss: 0.0017484292620792985\n",
      "Epoch 3921, Loss: 0.005934445602179039, Final Batch Loss: 4.502004594542086e-05\n",
      "Epoch 3922, Loss: 0.0037993783080310095, Final Batch Loss: 5.387921919464134e-05\n",
      "Epoch 3923, Loss: 0.029688043703572475, Final Batch Loss: 0.02106836624443531\n",
      "Epoch 3924, Loss: 0.00483248159980576, Final Batch Loss: 0.0002846040006261319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3925, Loss: 0.018161519859859254, Final Batch Loss: 7.745967741357163e-05\n",
      "Epoch 3926, Loss: 0.011949301610002294, Final Batch Loss: 0.0005070230690762401\n",
      "Epoch 3927, Loss: 0.024059770887106424, Final Batch Loss: 0.00035983248380944133\n",
      "Epoch 3928, Loss: 0.011945826361625222, Final Batch Loss: 5.861285899300128e-05\n",
      "Epoch 3929, Loss: 0.007224155673611676, Final Batch Loss: 6.85101913404651e-05\n",
      "Epoch 3930, Loss: 0.014914357319867122, Final Batch Loss: 0.000240212757489644\n",
      "Epoch 3931, Loss: 0.0024487715254508657, Final Batch Loss: 1.2839414011978079e-05\n",
      "Epoch 3932, Loss: 0.006896959173900541, Final Batch Loss: 0.00047266934416256845\n",
      "Epoch 3933, Loss: 0.0023161498556873994, Final Batch Loss: 3.243293394916691e-05\n",
      "Epoch 3934, Loss: 0.029212646284577204, Final Batch Loss: 1.9597011487348936e-05\n",
      "Epoch 3935, Loss: 0.009544269168145547, Final Batch Loss: 0.008291791193187237\n",
      "Epoch 3936, Loss: 0.009858203985459113, Final Batch Loss: 0.006139735225588083\n",
      "Epoch 3937, Loss: 0.021586779067547468, Final Batch Loss: 0.011227731592953205\n",
      "Epoch 3938, Loss: 0.05762126966874348, Final Batch Loss: 0.00035995221696794033\n",
      "Epoch 3939, Loss: 0.04964426411243039, Final Batch Loss: 4.420605910127051e-05\n",
      "Epoch 3940, Loss: 0.02902241786068771, Final Batch Loss: 0.0014354451559484005\n",
      "Epoch 3941, Loss: 0.026133230530831497, Final Batch Loss: 0.00895015150308609\n",
      "Epoch 3942, Loss: 0.007853007849917049, Final Batch Loss: 8.299609908135608e-05\n",
      "Epoch 3943, Loss: 0.010558867051258858, Final Batch Loss: 2.2463444111053832e-05\n",
      "Epoch 3944, Loss: 0.01656333532082499, Final Batch Loss: 0.004962313920259476\n",
      "Epoch 3945, Loss: 0.025454471577177173, Final Batch Loss: 0.0016829889500513673\n",
      "Epoch 3946, Loss: 0.004352927897343761, Final Batch Loss: 0.0007821453618817031\n",
      "Epoch 3947, Loss: 0.014264431491028517, Final Batch Loss: 0.0031651535537093878\n",
      "Epoch 3948, Loss: 0.02224423225288774, Final Batch Loss: 0.000342436513165012\n",
      "Epoch 3949, Loss: 0.014293191543401917, Final Batch Loss: 0.0031696236692368984\n",
      "Epoch 3950, Loss: 0.02444636978179915, Final Batch Loss: 0.002600588835775852\n",
      "Epoch 3951, Loss: 0.013045944350778882, Final Batch Loss: 0.0001837902091210708\n",
      "Epoch 3952, Loss: 0.05821022795680619, Final Batch Loss: 6.812480569351465e-05\n",
      "Epoch 3953, Loss: 0.02761768534037401, Final Batch Loss: 0.004441550001502037\n",
      "Epoch 3954, Loss: 0.0307957001559771, Final Batch Loss: 0.00014848000137135386\n",
      "Epoch 3955, Loss: 0.025033660844201222, Final Batch Loss: 1.8564940546639264e-05\n",
      "Epoch 3956, Loss: 0.0264727971789398, Final Batch Loss: 1.1415453627705574e-05\n",
      "Epoch 3957, Loss: 0.06823990112934553, Final Batch Loss: 0.021233858540654182\n",
      "Epoch 3958, Loss: 0.08963038965703163, Final Batch Loss: 3.446573464316316e-05\n",
      "Epoch 3959, Loss: 0.05550927156582475, Final Batch Loss: 0.0018429058836773038\n",
      "Epoch 3960, Loss: 0.03251045966317179, Final Batch Loss: 0.012451130896806717\n",
      "Epoch 3961, Loss: 0.021202568021180923, Final Batch Loss: 0.000848036608658731\n",
      "Epoch 3962, Loss: 0.020600739371730015, Final Batch Loss: 0.0028894918505102396\n",
      "Epoch 3963, Loss: 0.01046875608881237, Final Batch Loss: 0.0004136960196774453\n",
      "Epoch 3964, Loss: 0.01011669108629576, Final Batch Loss: 0.00028761246358044446\n",
      "Epoch 3965, Loss: 0.011659335264994297, Final Batch Loss: 0.001022111508063972\n",
      "Epoch 3966, Loss: 0.006269455428991932, Final Batch Loss: 0.0014271814143285155\n",
      "Epoch 3967, Loss: 0.0031198064207274, Final Batch Loss: 0.0007142613758333027\n",
      "Epoch 3968, Loss: 0.010958873126583057, Final Batch Loss: 0.0008499771356582642\n",
      "Epoch 3969, Loss: 0.003524446934534353, Final Batch Loss: 0.00013693503569811583\n",
      "Epoch 3970, Loss: 0.00666463348261459, Final Batch Loss: 4.804981654160656e-05\n",
      "Epoch 3971, Loss: 0.06045451150748704, Final Batch Loss: 0.0006770382169634104\n",
      "Epoch 3972, Loss: 0.011650109693619015, Final Batch Loss: 4.8656846047379076e-05\n",
      "Epoch 3973, Loss: 0.01264564482607966, Final Batch Loss: 0.00020513663184829056\n",
      "Epoch 3974, Loss: 0.008747856352783856, Final Batch Loss: 2.188411235692911e-05\n",
      "Epoch 3975, Loss: 0.02548416005083709, Final Batch Loss: 4.679223638959229e-05\n",
      "Epoch 3976, Loss: 0.030518691512043006, Final Batch Loss: 1.4592904335586354e-05\n",
      "Epoch 3977, Loss: 0.01797334820003016, Final Batch Loss: 4.425013321451843e-05\n",
      "Epoch 3978, Loss: 0.03833449657395249, Final Batch Loss: 0.0046559888869524\n",
      "Epoch 3979, Loss: 0.06089217272710812, Final Batch Loss: 0.00011698716116370633\n",
      "Epoch 3980, Loss: 0.09007924124125566, Final Batch Loss: 0.020817402750253677\n",
      "Epoch 3981, Loss: 0.16993817361071706, Final Batch Loss: 0.0010943893576040864\n",
      "Epoch 3982, Loss: 0.14521902152046096, Final Batch Loss: 0.0003101431066170335\n",
      "Epoch 3983, Loss: 0.024046939455729444, Final Batch Loss: 0.0017155862879008055\n",
      "Epoch 3984, Loss: 0.017689315009192796, Final Batch Loss: 0.0023436013143509626\n",
      "Epoch 3985, Loss: 0.03617725808726391, Final Batch Loss: 0.005816113669425249\n",
      "Epoch 3986, Loss: 0.0716460966032173, Final Batch Loss: 5.083441283204593e-05\n",
      "Epoch 3987, Loss: 0.09968236537679331, Final Batch Loss: 0.020122982561588287\n",
      "Epoch 3988, Loss: 0.0730678864492802, Final Batch Loss: 0.009565724059939384\n",
      "Epoch 3989, Loss: 0.03322100650257198, Final Batch Loss: 0.002871146658435464\n",
      "Epoch 3990, Loss: 0.10759099443384912, Final Batch Loss: 0.005446359980851412\n",
      "Epoch 3991, Loss: 0.03426995805784827, Final Batch Loss: 0.000528026488609612\n",
      "Epoch 3992, Loss: 0.014238591946195811, Final Batch Loss: 4.643250576918945e-05\n",
      "Epoch 3993, Loss: 0.01053020398830995, Final Batch Loss: 0.0007731578079983592\n",
      "Epoch 3994, Loss: 0.004653227040762431, Final Batch Loss: 0.00010435959120513871\n",
      "Epoch 3995, Loss: 0.013497282957359857, Final Batch Loss: 0.00025719398399814963\n",
      "Epoch 3996, Loss: 0.01578778111070278, Final Batch Loss: 6.445108738262206e-05\n",
      "Epoch 3997, Loss: 0.025141065827483544, Final Batch Loss: 0.00031903412309475243\n",
      "Epoch 3998, Loss: 0.0054684791357431095, Final Batch Loss: 0.0003733012708835304\n",
      "Epoch 3999, Loss: 0.007314996306376997, Final Batch Loss: 0.00034951334237121046\n",
      "Epoch 4000, Loss: 0.008201549559089472, Final Batch Loss: 3.529602327034809e-05\n",
      "Epoch 4001, Loss: 0.005252835416285961, Final Batch Loss: 0.00017300249601248652\n",
      "Epoch 4002, Loss: 0.0121955161466758, Final Batch Loss: 1.331041130470112e-05\n",
      "Epoch 4003, Loss: 0.00875531406927621, Final Batch Loss: 0.0002137484698323533\n",
      "Epoch 4004, Loss: 0.031344858351076255, Final Batch Loss: 0.00021397635282482952\n",
      "Epoch 4005, Loss: 0.005954232540716475, Final Batch Loss: 0.00012346671428531408\n",
      "Epoch 4006, Loss: 0.012118096276026336, Final Batch Loss: 0.010441352613270283\n",
      "Epoch 4007, Loss: 0.06279418551639537, Final Batch Loss: 0.019103311002254486\n",
      "Epoch 4008, Loss: 0.0258755927789025, Final Batch Loss: 4.050786083098501e-05\n",
      "Epoch 4009, Loss: 0.04610758029593853, Final Batch Loss: 0.00012965424684807658\n",
      "Epoch 4010, Loss: 0.02126064165349817, Final Batch Loss: 0.00011709339742083102\n",
      "Epoch 4011, Loss: 0.012766296960762702, Final Batch Loss: 0.0019153694156557322\n",
      "Epoch 4012, Loss: 0.009500399784883484, Final Batch Loss: 6.86581406625919e-05\n",
      "Epoch 4013, Loss: 0.029773997914162464, Final Batch Loss: 0.0007111850427463651\n",
      "Epoch 4014, Loss: 0.007297781905435841, Final Batch Loss: 7.065218960633501e-05\n",
      "Epoch 4015, Loss: 0.016053196686698357, Final Batch Loss: 0.0002973846276290715\n",
      "Epoch 4016, Loss: 0.01148400007514283, Final Batch Loss: 0.004015597514808178\n",
      "Epoch 4017, Loss: 0.006603657122468576, Final Batch Loss: 0.00013748096534982324\n",
      "Epoch 4018, Loss: 0.02149933002783655, Final Batch Loss: 6.017916166456416e-05\n",
      "Epoch 4019, Loss: 0.030787628624239005, Final Batch Loss: 9.1383139078971e-05\n",
      "Epoch 4020, Loss: 0.02323426486509561, Final Batch Loss: 5.4474061471410096e-05\n",
      "Epoch 4021, Loss: 0.011450773234173539, Final Batch Loss: 0.0002035556681221351\n",
      "Epoch 4022, Loss: 0.0203486722712114, Final Batch Loss: 0.012699257582426071\n",
      "Epoch 4023, Loss: 0.005157037365279393, Final Batch Loss: 7.92917053331621e-05\n",
      "Epoch 4024, Loss: 0.01410971535005956, Final Batch Loss: 0.0006957653677091002\n",
      "Epoch 4025, Loss: 0.007549236252089031, Final Batch Loss: 0.00016667545423842967\n",
      "Epoch 4026, Loss: 0.012417375128279673, Final Batch Loss: 4.781024472322315e-05\n",
      "Epoch 4027, Loss: 0.011019691117326147, Final Batch Loss: 0.0019672547932714224\n",
      "Epoch 4028, Loss: 0.006868962749649654, Final Batch Loss: 5.3060080972500145e-05\n",
      "Epoch 4029, Loss: 0.008914786580135114, Final Batch Loss: 0.0006428838241845369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4030, Loss: 0.002046109628281556, Final Batch Loss: 0.00010997442586813122\n",
      "Epoch 4031, Loss: 0.016468775142129743, Final Batch Loss: 0.0006271419115364552\n",
      "Epoch 4032, Loss: 0.01150034818056156, Final Batch Loss: 0.0003392290964256972\n",
      "Epoch 4033, Loss: 0.002019857778577716, Final Batch Loss: 0.00029261564486660063\n",
      "Epoch 4034, Loss: 0.03481362726597581, Final Batch Loss: 8.98127764230594e-05\n",
      "Epoch 4035, Loss: 0.011035723286113353, Final Batch Loss: 0.00040062778862193227\n",
      "Epoch 4036, Loss: 0.0161191940169374, Final Batch Loss: 0.0005368696874938905\n",
      "Epoch 4037, Loss: 0.009542598872940289, Final Batch Loss: 0.00037543580401688814\n",
      "Epoch 4038, Loss: 0.06268508172797738, Final Batch Loss: 0.0006402916624210775\n",
      "Epoch 4039, Loss: 0.008064532220487308, Final Batch Loss: 0.0017520919209346175\n",
      "Epoch 4040, Loss: 0.016297872465656837, Final Batch Loss: 8.325489761773497e-05\n",
      "Epoch 4041, Loss: 0.005312181907356717, Final Batch Loss: 3.847589687211439e-05\n",
      "Epoch 4042, Loss: 0.008082632324658334, Final Batch Loss: 0.0031649454031139612\n",
      "Epoch 4043, Loss: 0.02171294199251861, Final Batch Loss: 0.007189523428678513\n",
      "Epoch 4044, Loss: 0.00433062623051228, Final Batch Loss: 0.0003280264209024608\n",
      "Epoch 4045, Loss: 0.011740998694222071, Final Batch Loss: 0.009641321375966072\n",
      "Epoch 4046, Loss: 0.04794570279773325, Final Batch Loss: 8.660968887852505e-06\n",
      "Epoch 4047, Loss: 0.18678728343365947, Final Batch Loss: 0.004071928095072508\n",
      "Epoch 4048, Loss: 0.03961625273404934, Final Batch Loss: 0.0019711118657141924\n",
      "Epoch 4049, Loss: 0.02680619001512241, Final Batch Loss: 9.356603186461143e-06\n",
      "Epoch 4050, Loss: 0.025085828761802986, Final Batch Loss: 0.00014883883704897016\n",
      "Epoch 4051, Loss: 0.039160561605967814, Final Batch Loss: 0.000737341761123389\n",
      "Epoch 4052, Loss: 0.03657538504921831, Final Batch Loss: 0.0002458525123074651\n",
      "Epoch 4053, Loss: 0.04055651852468145, Final Batch Loss: 7.861485210014507e-05\n",
      "Epoch 4054, Loss: 0.02025672103263787, Final Batch Loss: 0.00021018655388616025\n",
      "Epoch 4055, Loss: 0.048389117984697805, Final Batch Loss: 0.00015965789498295635\n",
      "Epoch 4056, Loss: 0.014006355722813169, Final Batch Loss: 0.0004000586050096899\n",
      "Epoch 4057, Loss: 0.004799357273441274, Final Batch Loss: 0.0006455317488871515\n",
      "Epoch 4058, Loss: 0.01291968139958044, Final Batch Loss: 8.817277557682246e-05\n",
      "Epoch 4059, Loss: 0.01192062759946566, Final Batch Loss: 0.0026924684643745422\n",
      "Epoch 4060, Loss: 0.005992553167743608, Final Batch Loss: 0.0003641528601292521\n",
      "Epoch 4061, Loss: 0.005442934367238195, Final Batch Loss: 1.371790494886227e-05\n",
      "Epoch 4062, Loss: 0.006758495061149006, Final Batch Loss: 3.0513127057929523e-05\n",
      "Epoch 4063, Loss: 0.02234717763712979, Final Batch Loss: 0.0001623865100555122\n",
      "Epoch 4064, Loss: 0.013800027905745083, Final Batch Loss: 0.004585284739732742\n",
      "Epoch 4065, Loss: 0.014627013761128183, Final Batch Loss: 4.15497888752725e-05\n",
      "Epoch 4066, Loss: 0.02133069362025708, Final Batch Loss: 0.0004503591335378587\n",
      "Epoch 4067, Loss: 0.012836555210924416, Final Batch Loss: 0.00040986359817907214\n",
      "Epoch 4068, Loss: 0.006718250802805414, Final Batch Loss: 0.00017902605759445578\n",
      "Epoch 4069, Loss: 0.05423617061023833, Final Batch Loss: 0.0007237840327434242\n",
      "Epoch 4070, Loss: 0.005950446364295203, Final Batch Loss: 0.00015996949514374137\n",
      "Epoch 4071, Loss: 0.011730885094948462, Final Batch Loss: 0.002265961142256856\n",
      "Epoch 4072, Loss: 0.017429497315788467, Final Batch Loss: 0.0007508752751164138\n",
      "Epoch 4073, Loss: 0.009021102121550939, Final Batch Loss: 1.9616292775026523e-05\n",
      "Epoch 4074, Loss: 0.0032511212489225727, Final Batch Loss: 0.00020224822219461203\n",
      "Epoch 4075, Loss: 0.00468548176013428, Final Batch Loss: 0.0005001341924071312\n",
      "Epoch 4076, Loss: 0.024376930041398737, Final Batch Loss: 0.00020684242190327495\n",
      "Epoch 4077, Loss: 0.004715056353234104, Final Batch Loss: 6.311653123702854e-05\n",
      "Epoch 4078, Loss: 0.008547221190383425, Final Batch Loss: 0.00021348331938497722\n",
      "Epoch 4079, Loss: 0.024871843367691326, Final Batch Loss: 0.0005398479988798499\n",
      "Epoch 4080, Loss: 0.03750342385683325, Final Batch Loss: 0.0024797115474939346\n",
      "Epoch 4081, Loss: 0.010721229344198946, Final Batch Loss: 0.0002302522916579619\n",
      "Epoch 4082, Loss: 0.02873487853548795, Final Batch Loss: 0.004649094305932522\n",
      "Epoch 4083, Loss: 0.017708058750940836, Final Batch Loss: 2.793047679006122e-05\n",
      "Epoch 4084, Loss: 0.012601759124663658, Final Batch Loss: 0.000687724445015192\n",
      "Epoch 4085, Loss: 0.004157967532592011, Final Batch Loss: 0.00018210517009720206\n",
      "Epoch 4086, Loss: 0.02470534766325727, Final Batch Loss: 0.00028744767769239843\n",
      "Epoch 4087, Loss: 0.017832474106398877, Final Batch Loss: 4.1120256355497986e-05\n",
      "Epoch 4088, Loss: 0.0043034550271841, Final Batch Loss: 0.0006133883725851774\n",
      "Epoch 4089, Loss: 0.004572569473566546, Final Batch Loss: 0.0001236290845554322\n",
      "Epoch 4090, Loss: 0.007786058560668607, Final Batch Loss: 0.00021923656458966434\n",
      "Epoch 4091, Loss: 0.007916155341717968, Final Batch Loss: 6.04283422944718e-06\n",
      "Epoch 4092, Loss: 0.006571155101482873, Final Batch Loss: 4.514155079959892e-06\n",
      "Epoch 4093, Loss: 0.004136093154556875, Final Batch Loss: 1.02547755886917e-05\n",
      "Epoch 4094, Loss: 0.0026211949034404824, Final Batch Loss: 0.00023832604347262532\n",
      "Epoch 4095, Loss: 0.029443858712738802, Final Batch Loss: 0.00012439844431355596\n",
      "Epoch 4096, Loss: 0.013246944878119393, Final Batch Loss: 0.0019242274574935436\n",
      "Epoch 4097, Loss: 0.012698250744506367, Final Batch Loss: 6.136662705102935e-05\n",
      "Epoch 4098, Loss: 0.022511100412884844, Final Batch Loss: 0.0027396432124078274\n",
      "Epoch 4099, Loss: 0.0046443355868177605, Final Batch Loss: 0.00022659434762317687\n",
      "Epoch 4100, Loss: 0.010535616678680526, Final Batch Loss: 0.0002102965081576258\n",
      "Epoch 4101, Loss: 0.007456684010776371, Final Batch Loss: 0.0007412100676447153\n",
      "Epoch 4102, Loss: 0.016179130025193444, Final Batch Loss: 0.002675944473594427\n",
      "Epoch 4103, Loss: 0.006126461496023694, Final Batch Loss: 0.00011281407932983711\n",
      "Epoch 4104, Loss: 0.02798550778425124, Final Batch Loss: 3.0254959710873663e-05\n",
      "Epoch 4105, Loss: 0.030873699416133604, Final Batch Loss: 9.019762364914641e-05\n",
      "Epoch 4106, Loss: 0.04249729806906544, Final Batch Loss: 0.0001670882193138823\n",
      "Epoch 4107, Loss: 0.04912337107452913, Final Batch Loss: 0.00010549429862294346\n",
      "Epoch 4108, Loss: 0.056972387039422756, Final Batch Loss: 0.0016017620218917727\n",
      "Epoch 4109, Loss: 0.01933867215848295, Final Batch Loss: 0.0004851029079873115\n",
      "Epoch 4110, Loss: 0.0575631451101799, Final Batch Loss: 4.336689380579628e-05\n",
      "Epoch 4111, Loss: 0.03792614393023541, Final Batch Loss: 1.5124565834412351e-05\n",
      "Epoch 4112, Loss: 0.010441219052154338, Final Batch Loss: 4.607355731423013e-05\n",
      "Epoch 4113, Loss: 0.011908854838111438, Final Batch Loss: 3.776680023293011e-05\n",
      "Epoch 4114, Loss: 0.01205793456392712, Final Batch Loss: 0.0005330338608473539\n",
      "Epoch 4115, Loss: 0.008685793801305408, Final Batch Loss: 0.0001436188176739961\n",
      "Epoch 4116, Loss: 0.016448283840873046, Final Batch Loss: 6.138680328149348e-05\n",
      "Epoch 4117, Loss: 0.02578334731879295, Final Batch Loss: 4.2563660826999694e-05\n",
      "Epoch 4118, Loss: 0.005933838556302362, Final Batch Loss: 0.00017729977844282985\n",
      "Epoch 4119, Loss: 0.05978426481306087, Final Batch Loss: 0.0001985382696148008\n",
      "Epoch 4120, Loss: 0.06323810894173221, Final Batch Loss: 0.0001494106400059536\n",
      "Epoch 4121, Loss: 0.04770980212924769, Final Batch Loss: 0.0019294096855446696\n",
      "Epoch 4122, Loss: 0.024302556237671524, Final Batch Loss: 0.00047946517588570714\n",
      "Epoch 4123, Loss: 0.01697310383315198, Final Batch Loss: 0.0010672395583242178\n",
      "Epoch 4124, Loss: 0.029468407308741007, Final Batch Loss: 0.0035612136125564575\n",
      "Epoch 4125, Loss: 0.00970705534928129, Final Batch Loss: 0.000129763619042933\n",
      "Epoch 4126, Loss: 0.031222939731378574, Final Batch Loss: 0.014502279460430145\n",
      "Epoch 4127, Loss: 0.060055157422539196, Final Batch Loss: 6.996050069574267e-05\n",
      "Epoch 4128, Loss: 0.04649387344761635, Final Batch Loss: 0.00032130646286532283\n",
      "Epoch 4129, Loss: 0.035863164681359194, Final Batch Loss: 5.539105404750444e-05\n",
      "Epoch 4130, Loss: 0.056457450533343945, Final Batch Loss: 0.014926214702427387\n",
      "Epoch 4131, Loss: 0.021490697385161184, Final Batch Loss: 0.00016793239046819508\n",
      "Epoch 4132, Loss: 0.011309386929497123, Final Batch Loss: 0.0015932288952171803\n",
      "Epoch 4133, Loss: 0.016068665574493934, Final Batch Loss: 0.0007239409605972469\n",
      "Epoch 4134, Loss: 0.021433526973851258, Final Batch Loss: 0.000554250436834991\n",
      "Epoch 4135, Loss: 0.016148012095072772, Final Batch Loss: 0.00011709214595612139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4136, Loss: 0.003509025461426063, Final Batch Loss: 0.00024136193678714335\n",
      "Epoch 4137, Loss: 0.017629433905312908, Final Batch Loss: 2.6930008971248753e-05\n",
      "Epoch 4138, Loss: 0.007756842893286375, Final Batch Loss: 0.001313579035922885\n",
      "Epoch 4139, Loss: 0.04277557283057831, Final Batch Loss: 0.02890249341726303\n",
      "Epoch 4140, Loss: 0.008975414544693194, Final Batch Loss: 0.0006221110816113651\n",
      "Epoch 4141, Loss: 0.006379343281878391, Final Batch Loss: 0.00010975266923196614\n",
      "Epoch 4142, Loss: 0.007698640343733132, Final Batch Loss: 0.00017983745783567429\n",
      "Epoch 4143, Loss: 0.0037457620192071772, Final Batch Loss: 2.1478534108609892e-05\n",
      "Epoch 4144, Loss: 0.021457697406731313, Final Batch Loss: 0.0013298639096319675\n",
      "Epoch 4145, Loss: 0.026460923052582075, Final Batch Loss: 4.6623772504972294e-05\n",
      "Epoch 4146, Loss: 0.01989450608380139, Final Batch Loss: 0.00012281272211112082\n",
      "Epoch 4147, Loss: 0.04645261895711883, Final Batch Loss: 0.0005780425854027271\n",
      "Epoch 4148, Loss: 0.00876512114336947, Final Batch Loss: 0.0008042767294682562\n",
      "Epoch 4149, Loss: 0.041109473877440905, Final Batch Loss: 0.0005516866804100573\n",
      "Epoch 4150, Loss: 0.004392926706714206, Final Batch Loss: 0.0007146965363062918\n",
      "Epoch 4151, Loss: 0.011310647574646282, Final Batch Loss: 0.00023570755729451776\n",
      "Epoch 4152, Loss: 0.03555716077971738, Final Batch Loss: 7.812546391505748e-05\n",
      "Epoch 4153, Loss: 0.0054054248712418485, Final Batch Loss: 0.0011745915981009603\n",
      "Epoch 4154, Loss: 0.019637231265733135, Final Batch Loss: 4.5935852540424094e-05\n",
      "Epoch 4155, Loss: 0.016790822149232554, Final Batch Loss: 0.00017560752166900784\n",
      "Epoch 4156, Loss: 0.005888592985684227, Final Batch Loss: 1.1607978194660973e-05\n",
      "Epoch 4157, Loss: 0.011124714066681918, Final Batch Loss: 0.0012533694971352816\n",
      "Epoch 4158, Loss: 0.017471903691330226, Final Batch Loss: 0.0015746469143778086\n",
      "Epoch 4159, Loss: 0.014936929032046464, Final Batch Loss: 9.298415534431115e-06\n",
      "Epoch 4160, Loss: 0.003201555823579838, Final Batch Loss: 0.00014883308904245496\n",
      "Epoch 4161, Loss: 0.009413348841007974, Final Batch Loss: 0.00012654274178203195\n",
      "Epoch 4162, Loss: 0.027360824269635486, Final Batch Loss: 9.958964074030519e-05\n",
      "Epoch 4163, Loss: 0.020144439828982286, Final Batch Loss: 0.0004006344825029373\n",
      "Epoch 4164, Loss: 0.08677466848712356, Final Batch Loss: 0.03183980658650398\n",
      "Epoch 4165, Loss: 0.030511990600302852, Final Batch Loss: 5.6201847655756865e-06\n",
      "Epoch 4166, Loss: 0.02198348572346731, Final Batch Loss: 4.538707798928954e-05\n",
      "Epoch 4167, Loss: 0.008548891697046201, Final Batch Loss: 7.586555966554442e-06\n",
      "Epoch 4168, Loss: 0.054321277349117736, Final Batch Loss: 6.740802291460568e-06\n",
      "Epoch 4169, Loss: 0.03490676603951215, Final Batch Loss: 0.0006976292934268713\n",
      "Epoch 4170, Loss: 0.015186125303443987, Final Batch Loss: 2.21245973079931e-05\n",
      "Epoch 4171, Loss: 0.009500817466687295, Final Batch Loss: 2.1127620129846036e-05\n",
      "Epoch 4172, Loss: 0.004593432176079659, Final Batch Loss: 5.863794649485499e-05\n",
      "Epoch 4173, Loss: 0.005292407118759002, Final Batch Loss: 1.4044811905478127e-05\n",
      "Epoch 4174, Loss: 0.005639371752295119, Final Batch Loss: 0.001621709787286818\n",
      "Epoch 4175, Loss: 0.009341539710021607, Final Batch Loss: 8.863196126185358e-05\n",
      "Epoch 4176, Loss: 0.007607606796682376, Final Batch Loss: 5.390870228438871e-06\n",
      "Epoch 4177, Loss: 0.00817200830897491, Final Batch Loss: 0.00013956525071989745\n",
      "Epoch 4178, Loss: 0.011791380200520507, Final Batch Loss: 0.00016803023754619062\n",
      "Epoch 4179, Loss: 0.005967261344267172, Final Batch Loss: 0.00024052827211562544\n",
      "Epoch 4180, Loss: 0.016993974281376723, Final Batch Loss: 0.0037795505486428738\n",
      "Epoch 4181, Loss: 0.0175236186132679, Final Batch Loss: 8.246329343819525e-06\n",
      "Epoch 4182, Loss: 0.012822147757105995, Final Batch Loss: 0.002285860013216734\n",
      "Epoch 4183, Loss: 0.04256098374935391, Final Batch Loss: 9.312206384493038e-05\n",
      "Epoch 4184, Loss: 0.04707974265829762, Final Batch Loss: 1.1044799066439737e-05\n",
      "Epoch 4185, Loss: 0.02004871134431596, Final Batch Loss: 0.0011955893132835627\n",
      "Epoch 4186, Loss: 0.00509199540647387, Final Batch Loss: 0.00016365796909667552\n",
      "Epoch 4187, Loss: 0.008066544683060783, Final Batch Loss: 4.967178938386496e-06\n",
      "Epoch 4188, Loss: 0.047587129482963064, Final Batch Loss: 4.438951509655453e-05\n",
      "Epoch 4189, Loss: 0.0031193659515338368, Final Batch Loss: 3.373661093064584e-05\n",
      "Epoch 4190, Loss: 0.013725100826377457, Final Batch Loss: 0.008851020596921444\n",
      "Epoch 4191, Loss: 0.004898466081613151, Final Batch Loss: 8.02214526629541e-06\n",
      "Epoch 4192, Loss: 0.01011292852126644, Final Batch Loss: 0.0005242897314019501\n",
      "Epoch 4193, Loss: 0.01001219631280037, Final Batch Loss: 0.0009095498826354742\n",
      "Epoch 4194, Loss: 0.00649143741975422, Final Batch Loss: 0.0003403003793209791\n",
      "Epoch 4195, Loss: 0.0038957948290772038, Final Batch Loss: 0.00020335905719548464\n",
      "Epoch 4196, Loss: 0.0013448423328554782, Final Batch Loss: 3.917676804121584e-05\n",
      "Epoch 4197, Loss: 0.0018089834748025169, Final Batch Loss: 0.0001772891409927979\n",
      "Epoch 4198, Loss: 0.02248032696024893, Final Batch Loss: 2.052183663181495e-05\n",
      "Epoch 4199, Loss: 0.006610640852613869, Final Batch Loss: 3.317258597235195e-05\n",
      "Epoch 4200, Loss: 0.004345003335401998, Final Batch Loss: 1.8523363905842416e-05\n",
      "Epoch 4201, Loss: 0.0063370949160344026, Final Batch Loss: 0.00013902359933126718\n",
      "Epoch 4202, Loss: 0.0390753012670757, Final Batch Loss: 2.7269548809272237e-05\n",
      "Epoch 4203, Loss: 0.011608498743498785, Final Batch Loss: 0.001955107320100069\n",
      "Epoch 4204, Loss: 0.023844465791626135, Final Batch Loss: 1.3548809874919243e-05\n",
      "Epoch 4205, Loss: 0.015657532532827645, Final Batch Loss: 0.00047027485561557114\n",
      "Epoch 4206, Loss: 0.043710839559935266, Final Batch Loss: 8.143242666847073e-06\n",
      "Epoch 4207, Loss: 0.006657910561443714, Final Batch Loss: 6.504532211693004e-05\n",
      "Epoch 4208, Loss: 0.021001241084377398, Final Batch Loss: 3.815755917457864e-05\n",
      "Epoch 4209, Loss: 0.17530194953360478, Final Batch Loss: 0.045058052986860275\n",
      "Epoch 4210, Loss: 0.0818001520656253, Final Batch Loss: 3.2059593650046736e-05\n",
      "Epoch 4211, Loss: 0.025159842897210183, Final Batch Loss: 3.998404736194061e-06\n",
      "Epoch 4212, Loss: 0.02703497865513782, Final Batch Loss: 0.0003189824928995222\n",
      "Epoch 4213, Loss: 0.008897609487121372, Final Batch Loss: 7.482015644200146e-05\n",
      "Epoch 4214, Loss: 0.006324865558781312, Final Batch Loss: 0.00040511752013117075\n",
      "Epoch 4215, Loss: 0.006783012499909091, Final Batch Loss: 0.0020028618164360523\n",
      "Epoch 4216, Loss: 0.004996008394300588, Final Batch Loss: 1.7462894902564585e-05\n",
      "Epoch 4217, Loss: 0.0036006279335651925, Final Batch Loss: 4.465817619347945e-05\n",
      "Epoch 4218, Loss: 0.005869082938716019, Final Batch Loss: 9.335359209217131e-05\n",
      "Epoch 4219, Loss: 0.0028236075850145426, Final Batch Loss: 0.00011680958414217457\n",
      "Epoch 4220, Loss: 0.04609926598277525, Final Batch Loss: 0.0004952858434990048\n",
      "Epoch 4221, Loss: 0.02619353320369555, Final Batch Loss: 0.0006800380651839077\n",
      "Epoch 4222, Loss: 0.0034670159957386204, Final Batch Loss: 0.0009288734290748835\n",
      "Epoch 4223, Loss: 0.13155927579123272, Final Batch Loss: 0.045251574367284775\n",
      "Epoch 4224, Loss: 0.046416348960519827, Final Batch Loss: 0.0012681751977652311\n",
      "Epoch 4225, Loss: 0.012670029931541649, Final Batch Loss: 0.0002443321864120662\n",
      "Epoch 4226, Loss: 0.029629060765728354, Final Batch Loss: 6.990976544329897e-05\n",
      "Epoch 4227, Loss: 0.02130537978882785, Final Batch Loss: 5.9346883062971756e-05\n",
      "Epoch 4228, Loss: 0.008121883756757597, Final Batch Loss: 0.00336765986867249\n",
      "Epoch 4229, Loss: 0.006548093366291141, Final Batch Loss: 4.132351386942901e-05\n",
      "Epoch 4230, Loss: 0.004994101531337947, Final Batch Loss: 2.079288060485851e-05\n",
      "Epoch 4231, Loss: 0.011857332005092758, Final Batch Loss: 0.00047017671749927104\n",
      "Epoch 4232, Loss: 0.009189751315716421, Final Batch Loss: 4.154932321398519e-05\n",
      "Epoch 4233, Loss: 0.013372513623835403, Final Batch Loss: 8.645313209854066e-05\n",
      "Epoch 4234, Loss: 0.005782779606306576, Final Batch Loss: 0.000632728508207947\n",
      "Epoch 4235, Loss: 0.008113333011351642, Final Batch Loss: 8.095219527604058e-05\n",
      "Epoch 4236, Loss: 0.004992591637346777, Final Batch Loss: 4.264926246833056e-05\n",
      "Epoch 4237, Loss: 0.06498376951640239, Final Batch Loss: 0.0007984651601873338\n",
      "Epoch 4238, Loss: 0.015619427716956125, Final Batch Loss: 0.0008001672686077654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4239, Loss: 0.005590185988694429, Final Batch Loss: 4.283566522644833e-05\n",
      "Epoch 4240, Loss: 0.010869512323552044, Final Batch Loss: 0.00013182824477553368\n",
      "Epoch 4241, Loss: 0.005429079796158476, Final Batch Loss: 8.252429324784316e-06\n",
      "Epoch 4242, Loss: 0.008112951998555218, Final Batch Loss: 7.933114829938859e-05\n",
      "Epoch 4243, Loss: 0.010779273575280968, Final Batch Loss: 1.3326646694622468e-05\n",
      "Epoch 4244, Loss: 0.0031977197077139863, Final Batch Loss: 0.0005768911796621978\n",
      "Epoch 4245, Loss: 0.040154093912860844, Final Batch Loss: 0.0002874426427297294\n",
      "Epoch 4246, Loss: 0.012070325172317098, Final Batch Loss: 1.8907727280748077e-05\n",
      "Epoch 4247, Loss: 0.010491728393390076, Final Batch Loss: 4.2578521970426664e-05\n",
      "Epoch 4248, Loss: 0.021086068129989144, Final Batch Loss: 0.013683920726180077\n",
      "Epoch 4249, Loss: 0.010479798792403017, Final Batch Loss: 0.0004634313518181443\n",
      "Epoch 4250, Loss: 0.0053124773112358525, Final Batch Loss: 0.00020807656983379275\n",
      "Epoch 4251, Loss: 0.003957831911975518, Final Batch Loss: 0.0014319196343421936\n",
      "Epoch 4252, Loss: 0.008823300428048242, Final Batch Loss: 3.4114760637748986e-05\n",
      "Epoch 4253, Loss: 0.004178368349130324, Final Batch Loss: 2.066781780740712e-05\n",
      "Epoch 4254, Loss: 0.019712346909727785, Final Batch Loss: 4.4670639908872545e-05\n",
      "Epoch 4255, Loss: 0.012562047038954915, Final Batch Loss: 0.00034631512244232\n",
      "Epoch 4256, Loss: 0.005333613265065651, Final Batch Loss: 3.5777997254626825e-05\n",
      "Epoch 4257, Loss: 0.0035935882556259457, Final Batch Loss: 7.054061370581621e-06\n",
      "Epoch 4258, Loss: 0.006442110797706846, Final Batch Loss: 2.6626063117873855e-05\n",
      "Epoch 4259, Loss: 0.006410014197626879, Final Batch Loss: 3.44559557561297e-05\n",
      "Epoch 4260, Loss: 0.010264735356940946, Final Batch Loss: 5.489892646437511e-05\n",
      "Epoch 4261, Loss: 0.01304687808806193, Final Batch Loss: 0.00022290223569143564\n",
      "Epoch 4262, Loss: 0.034512468137108954, Final Batch Loss: 0.0008977280231192708\n",
      "Epoch 4263, Loss: 0.008864103379892185, Final Batch Loss: 0.0004899735213257372\n",
      "Epoch 4264, Loss: 0.007990016664734867, Final Batch Loss: 1.3249127732706256e-05\n",
      "Epoch 4265, Loss: 0.015427373420607182, Final Batch Loss: 6.331166514428332e-05\n",
      "Epoch 4266, Loss: 0.015515312499701395, Final Batch Loss: 0.0009314410272054374\n",
      "Epoch 4267, Loss: 0.009509474937658524, Final Batch Loss: 1.9466158846626058e-05\n",
      "Epoch 4268, Loss: 0.020492385169291083, Final Batch Loss: 0.015506991185247898\n",
      "Epoch 4269, Loss: 0.011038488319627504, Final Batch Loss: 8.398480713367462e-05\n",
      "Epoch 4270, Loss: 0.0031457173331546073, Final Batch Loss: 1.9011857148143463e-05\n",
      "Epoch 4271, Loss: 0.002408935994935746, Final Batch Loss: 0.0006001009605824947\n",
      "Epoch 4272, Loss: 0.004616401175553619, Final Batch Loss: 2.1425177692435682e-05\n",
      "Epoch 4273, Loss: 0.005371455522436008, Final Batch Loss: 9.251981828128919e-05\n",
      "Epoch 4274, Loss: 0.003483863588371605, Final Batch Loss: 0.0008768446859903634\n",
      "Epoch 4275, Loss: 0.008583636588355148, Final Batch Loss: 4.250705387676135e-05\n",
      "Epoch 4276, Loss: 0.002275512367305055, Final Batch Loss: 2.5901752451318316e-05\n",
      "Epoch 4277, Loss: 0.004728352921119949, Final Batch Loss: 5.85101151955314e-05\n",
      "Epoch 4278, Loss: 0.006038461540811113, Final Batch Loss: 0.0004609195457305759\n",
      "Epoch 4279, Loss: 0.005391564560341067, Final Batch Loss: 0.0005654016858898103\n",
      "Epoch 4280, Loss: 0.004373316505734692, Final Batch Loss: 0.0013103786623105407\n",
      "Epoch 4281, Loss: 0.0033748763144103577, Final Batch Loss: 2.4326942366315052e-05\n",
      "Epoch 4282, Loss: 0.0019412249994275044, Final Batch Loss: 3.107920565526001e-05\n",
      "Epoch 4283, Loss: 0.0037826148685553562, Final Batch Loss: 1.0330703844374511e-05\n",
      "Epoch 4284, Loss: 0.004813959137209167, Final Batch Loss: 0.0005810350994579494\n",
      "Epoch 4285, Loss: 0.010961492135720619, Final Batch Loss: 2.433652844047174e-05\n",
      "Epoch 4286, Loss: 0.006195308764631591, Final Batch Loss: 3.732737968675792e-05\n",
      "Epoch 4287, Loss: 0.02252638534446305, Final Batch Loss: 2.4368860977119766e-05\n",
      "Epoch 4288, Loss: 0.0441505110475191, Final Batch Loss: 1.0267896868754178e-05\n",
      "Epoch 4289, Loss: 0.05494559283624767, Final Batch Loss: 4.762114713230403e-06\n",
      "Epoch 4290, Loss: 0.03816925077035194, Final Batch Loss: 0.014602924697101116\n",
      "Epoch 4291, Loss: 0.0736656521318082, Final Batch Loss: 0.0001650297926971689\n",
      "Epoch 4292, Loss: 0.08372372779740545, Final Batch Loss: 0.004891957622021437\n",
      "Epoch 4293, Loss: 0.03878851503327496, Final Batch Loss: 0.0012682181550189853\n",
      "Epoch 4294, Loss: 0.07646525826021389, Final Batch Loss: 0.0008269120007753372\n",
      "Epoch 4295, Loss: 0.17295555537566543, Final Batch Loss: 0.025544529780745506\n",
      "Epoch 4296, Loss: 0.06228443820873508, Final Batch Loss: 0.0001045262033585459\n",
      "Epoch 4297, Loss: 0.09838609001599252, Final Batch Loss: 0.03458814322948456\n",
      "Epoch 4298, Loss: 0.05206803500186652, Final Batch Loss: 0.00019270282064098865\n",
      "Epoch 4299, Loss: 0.01695408245359431, Final Batch Loss: 0.00621386943385005\n",
      "Epoch 4300, Loss: 0.026817459360245266, Final Batch Loss: 0.00023877159401308745\n",
      "Epoch 4301, Loss: 0.02704599641583627, Final Batch Loss: 0.0007593694608658552\n",
      "Epoch 4302, Loss: 0.013127936654200312, Final Batch Loss: 0.0004249278281349689\n",
      "Epoch 4303, Loss: 0.016343137556759757, Final Batch Loss: 2.6292984330211766e-05\n",
      "Epoch 4304, Loss: 0.01168525685716304, Final Batch Loss: 0.0001256867399206385\n",
      "Epoch 4305, Loss: 0.00906022193703393, Final Batch Loss: 0.00014086482406128198\n",
      "Epoch 4306, Loss: 0.032214779700552754, Final Batch Loss: 2.7092508389614522e-05\n",
      "Epoch 4307, Loss: 0.019677902786497725, Final Batch Loss: 0.0002919884864240885\n",
      "Epoch 4308, Loss: 0.00625876456115293, Final Batch Loss: 0.0004232971405144781\n",
      "Epoch 4309, Loss: 0.03139097544590186, Final Batch Loss: 0.009304668754339218\n",
      "Epoch 4310, Loss: 0.01267297472077189, Final Batch Loss: 0.0013927503023296595\n",
      "Epoch 4311, Loss: 0.004302222656406229, Final Batch Loss: 0.0001946759148268029\n",
      "Epoch 4312, Loss: 0.01395942181625287, Final Batch Loss: 0.00048231807886622846\n",
      "Epoch 4313, Loss: 0.005969429673314153, Final Batch Loss: 5.69477015233133e-05\n",
      "Epoch 4314, Loss: 0.010949020108455443, Final Batch Loss: 0.0027676341123878956\n",
      "Epoch 4315, Loss: 0.0031382499510073103, Final Batch Loss: 0.00029124633874744177\n",
      "Epoch 4316, Loss: 0.005218261505433475, Final Batch Loss: 6.280646630330011e-05\n",
      "Epoch 4317, Loss: 0.0029639249296451453, Final Batch Loss: 5.5497737776022404e-05\n",
      "Epoch 4318, Loss: 0.00843640126322498, Final Batch Loss: 2.8816402846132405e-05\n",
      "Epoch 4319, Loss: 0.012633649247163703, Final Batch Loss: 0.00028396700508892536\n",
      "Epoch 4320, Loss: 0.003566128389138612, Final Batch Loss: 4.327906572143547e-05\n",
      "Epoch 4321, Loss: 0.006736479201663315, Final Batch Loss: 0.0006963255000300705\n",
      "Epoch 4322, Loss: 0.006059273633582052, Final Batch Loss: 1.1082540368079208e-05\n",
      "Epoch 4323, Loss: 0.019721557840057358, Final Batch Loss: 2.7890984711120836e-05\n",
      "Epoch 4324, Loss: 0.031839082372243865, Final Batch Loss: 0.00014124307199381292\n",
      "Epoch 4325, Loss: 0.004969327020262426, Final Batch Loss: 0.0001259033742826432\n",
      "Epoch 4326, Loss: 0.009343994857772486, Final Batch Loss: 0.00012617769243661314\n",
      "Epoch 4327, Loss: 0.012619015111795306, Final Batch Loss: 5.379608410294168e-05\n",
      "Epoch 4328, Loss: 0.012751434345773305, Final Batch Loss: 0.0001510031142970547\n",
      "Epoch 4329, Loss: 0.008867337904575834, Final Batch Loss: 1.7178903362946585e-05\n",
      "Epoch 4330, Loss: 0.046112551162877935, Final Batch Loss: 0.0012087947688996792\n",
      "Epoch 4331, Loss: 0.00443296333651233, Final Batch Loss: 0.000653865048661828\n",
      "Epoch 4332, Loss: 0.027678409090185596, Final Batch Loss: 0.013572297990322113\n",
      "Epoch 4333, Loss: 0.027264606329936214, Final Batch Loss: 8.360989340872038e-06\n",
      "Epoch 4334, Loss: 0.038288536553864105, Final Batch Loss: 7.011465186224086e-06\n",
      "Epoch 4335, Loss: 0.008022443691515946, Final Batch Loss: 0.00022018385061528534\n",
      "Epoch 4336, Loss: 0.00760201230605162, Final Batch Loss: 0.00020863434474449605\n",
      "Epoch 4337, Loss: 0.007695536023675231, Final Batch Loss: 1.9959801647928543e-05\n",
      "Epoch 4338, Loss: 0.002037691930127039, Final Batch Loss: 3.470961019047536e-05\n",
      "Epoch 4339, Loss: 0.005240451549980207, Final Batch Loss: 1.5027173503767699e-05\n",
      "Epoch 4340, Loss: 0.035139408508257475, Final Batch Loss: 7.996426575118676e-05\n",
      "Epoch 4341, Loss: 0.01482798238430405, Final Batch Loss: 0.005589500535279512\n",
      "Epoch 4342, Loss: 0.00820109999676788, Final Batch Loss: 0.00021434399241115898\n",
      "Epoch 4343, Loss: 0.04393791626443999, Final Batch Loss: 0.00018869794439524412\n",
      "Epoch 4344, Loss: 0.0023166468258750683, Final Batch Loss: 1.3347851563594304e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4345, Loss: 0.0029232586994112353, Final Batch Loss: 8.174219692591578e-05\n",
      "Epoch 4346, Loss: 0.004327692703100183, Final Batch Loss: 2.2906320737092756e-05\n",
      "Epoch 4347, Loss: 0.005349189595108328, Final Batch Loss: 6.543462950503454e-05\n",
      "Epoch 4348, Loss: 0.0053371647632047825, Final Batch Loss: 7.4935719567292836e-06\n",
      "Epoch 4349, Loss: 0.0011499486072352738, Final Batch Loss: 2.809550460369792e-05\n",
      "Epoch 4350, Loss: 0.004171997787125292, Final Batch Loss: 1.601001713424921e-05\n",
      "Epoch 4351, Loss: 0.006373380903369252, Final Batch Loss: 3.584262594813481e-05\n",
      "Epoch 4352, Loss: 0.0017429369518140447, Final Batch Loss: 0.00010538835340412334\n",
      "Epoch 4353, Loss: 0.027419617112173, Final Batch Loss: 0.00041700838482938707\n",
      "Epoch 4354, Loss: 0.28651395605993457, Final Batch Loss: 0.014056766405701637\n",
      "Epoch 4355, Loss: 0.0748600635852199, Final Batch Loss: 0.00937064178287983\n",
      "Epoch 4356, Loss: 0.04383361719465029, Final Batch Loss: 0.0009221616201102734\n",
      "Epoch 4357, Loss: 0.12391993376149912, Final Batch Loss: 0.011671064421534538\n",
      "Epoch 4358, Loss: 0.11216278265055735, Final Batch Loss: 0.0015745413256809115\n",
      "Epoch 4359, Loss: 0.027725412510335445, Final Batch Loss: 0.0005235557910054922\n",
      "Epoch 4360, Loss: 0.017443877724872436, Final Batch Loss: 8.729797991691157e-05\n",
      "Epoch 4361, Loss: 0.010528766779316356, Final Batch Loss: 0.00011607020132942125\n",
      "Epoch 4362, Loss: 0.03303506558404479, Final Batch Loss: 0.00033164327032864094\n",
      "Epoch 4363, Loss: 0.005204070951549511, Final Batch Loss: 0.001201301347464323\n",
      "Epoch 4364, Loss: 0.01406565048819175, Final Batch Loss: 6.775001384085044e-05\n",
      "Epoch 4365, Loss: 0.027684024751579273, Final Batch Loss: 0.0004646117449738085\n",
      "Epoch 4366, Loss: 0.011306116685773304, Final Batch Loss: 0.001696682651527226\n",
      "Epoch 4367, Loss: 0.009478516513809154, Final Batch Loss: 0.0002655759162735194\n",
      "Epoch 4368, Loss: 0.009422560744496877, Final Batch Loss: 6.6583656007424e-05\n",
      "Epoch 4369, Loss: 0.012032036480377428, Final Batch Loss: 0.0009068382787518203\n",
      "Epoch 4370, Loss: 0.015134745438444952, Final Batch Loss: 0.0006497123977169394\n",
      "Epoch 4371, Loss: 0.04099913915342768, Final Batch Loss: 0.00028247429872862995\n",
      "Epoch 4372, Loss: 0.05486432280304143, Final Batch Loss: 0.00022201240062713623\n",
      "Epoch 4373, Loss: 0.041721521181898424, Final Batch Loss: 5.4674219427397475e-05\n",
      "Epoch 4374, Loss: 0.05461313204432372, Final Batch Loss: 0.009304446168243885\n",
      "Epoch 4375, Loss: 0.023047298625897383, Final Batch Loss: 3.747668597497977e-05\n",
      "Epoch 4376, Loss: 0.025322410318040056, Final Batch Loss: 0.009650728665292263\n",
      "Epoch 4377, Loss: 0.03737342701606394, Final Batch Loss: 0.004622262436896563\n",
      "Epoch 4378, Loss: 0.02352558721031528, Final Batch Loss: 0.00015306458226405084\n",
      "Epoch 4379, Loss: 0.0346847517612332, Final Batch Loss: 5.829667134094052e-05\n",
      "Epoch 4380, Loss: 0.01878518579633237, Final Batch Loss: 8.33626909297891e-05\n",
      "Epoch 4381, Loss: 0.021537974462262355, Final Batch Loss: 7.045964593999088e-05\n",
      "Epoch 4382, Loss: 0.024818113806759357, Final Batch Loss: 0.0011042191181331873\n",
      "Epoch 4383, Loss: 0.003971540500060655, Final Batch Loss: 0.00011054012429667637\n",
      "Epoch 4384, Loss: 0.013689709077880252, Final Batch Loss: 0.0018014414235949516\n",
      "Epoch 4385, Loss: 0.0031859326336416416, Final Batch Loss: 0.0012584945652633905\n",
      "Epoch 4386, Loss: 0.018394577204162488, Final Batch Loss: 6.747044244548306e-05\n",
      "Epoch 4387, Loss: 0.03482619886381144, Final Batch Loss: 3.617012407630682e-05\n",
      "Epoch 4388, Loss: 0.002089369229906879, Final Batch Loss: 0.0001342189498245716\n",
      "Epoch 4389, Loss: 0.011242378057431779, Final Batch Loss: 4.831171463592909e-05\n",
      "Epoch 4390, Loss: 0.005248166646197205, Final Batch Loss: 0.0006304638227447867\n",
      "Epoch 4391, Loss: 0.016078454483249516, Final Batch Loss: 5.4988995543681085e-05\n",
      "Epoch 4392, Loss: 0.0064761426529003074, Final Batch Loss: 5.991970101604238e-05\n",
      "Epoch 4393, Loss: 0.03972612978395773, Final Batch Loss: 0.0010375637793913484\n",
      "Epoch 4394, Loss: 0.014824220699665602, Final Batch Loss: 0.003229390364140272\n",
      "Epoch 4395, Loss: 0.007368772587142303, Final Batch Loss: 6.515688437502831e-05\n",
      "Epoch 4396, Loss: 0.004740035619761329, Final Batch Loss: 0.0028490317054092884\n",
      "Epoch 4397, Loss: 0.026479282189029618, Final Batch Loss: 3.1779680284671485e-05\n",
      "Epoch 4398, Loss: 0.019241269135818584, Final Batch Loss: 7.557829667348415e-05\n",
      "Epoch 4399, Loss: 0.031005467904833495, Final Batch Loss: 0.0004001344204880297\n",
      "Epoch 4400, Loss: 0.01339309225386387, Final Batch Loss: 0.0036392253823578358\n",
      "Epoch 4401, Loss: 0.012331198427091294, Final Batch Loss: 0.0038916764315217733\n",
      "Epoch 4402, Loss: 0.020384171901241643, Final Batch Loss: 0.0015519096050411463\n",
      "Epoch 4403, Loss: 0.00942281022798852, Final Batch Loss: 3.078341978834942e-05\n",
      "Epoch 4404, Loss: 0.006332685123197734, Final Batch Loss: 0.0008629882941022515\n",
      "Epoch 4405, Loss: 0.056209779118944425, Final Batch Loss: 0.018318993970751762\n",
      "Epoch 4406, Loss: 0.03320415927373688, Final Batch Loss: 4.113171235076152e-05\n",
      "Epoch 4407, Loss: 0.023611920782059315, Final Batch Loss: 5.7910336181521416e-05\n",
      "Epoch 4408, Loss: 0.007695629738009302, Final Batch Loss: 0.0004702989826910198\n",
      "Epoch 4409, Loss: 0.003010033342434326, Final Batch Loss: 0.00038427283288910985\n",
      "Epoch 4410, Loss: 0.005237663274783699, Final Batch Loss: 0.002196340821683407\n",
      "Epoch 4411, Loss: 0.009997542299970519, Final Batch Loss: 1.5172729035839438e-05\n",
      "Epoch 4412, Loss: 0.006297956735579646, Final Batch Loss: 9.15840792004019e-05\n",
      "Epoch 4413, Loss: 0.004149210400100856, Final Batch Loss: 1.2892690392618533e-05\n",
      "Epoch 4414, Loss: 0.0289518840609162, Final Batch Loss: 0.004997543524950743\n",
      "Epoch 4415, Loss: 0.020664975045292522, Final Batch Loss: 0.0018647453980520368\n",
      "Epoch 4416, Loss: 0.018971947185491445, Final Batch Loss: 0.0004680334823206067\n",
      "Epoch 4417, Loss: 0.0051345122847124, Final Batch Loss: 0.00011611668742261827\n",
      "Epoch 4418, Loss: 0.00453943942557089, Final Batch Loss: 0.00018592838023323566\n",
      "Epoch 4419, Loss: 0.0021487144322236418, Final Batch Loss: 3.103165727225132e-05\n",
      "Epoch 4420, Loss: 0.002365260356782528, Final Batch Loss: 0.0001232202776009217\n",
      "Epoch 4421, Loss: 0.035198624554141134, Final Batch Loss: 0.0002516017993912101\n",
      "Epoch 4422, Loss: 0.0031746324111736612, Final Batch Loss: 1.0885270057769958e-05\n",
      "Epoch 4423, Loss: 0.009084429811991868, Final Batch Loss: 0.00010931239376077428\n",
      "Epoch 4424, Loss: 0.00515106896227735, Final Batch Loss: 5.40579276275821e-05\n",
      "Epoch 4425, Loss: 0.002832481597579317, Final Batch Loss: 0.00035628199111670256\n",
      "Epoch 4426, Loss: 0.009797288220397604, Final Batch Loss: 0.00015939396689645946\n",
      "Epoch 4427, Loss: 0.010739790571278718, Final Batch Loss: 1.5572772099403664e-05\n",
      "Epoch 4428, Loss: 0.008274147698102752, Final Batch Loss: 0.0001838953176047653\n",
      "Epoch 4429, Loss: 0.006601543803299137, Final Batch Loss: 4.419640026753768e-05\n",
      "Epoch 4430, Loss: 0.009086008091799158, Final Batch Loss: 0.0004010890261270106\n",
      "Epoch 4431, Loss: 0.009930630461894907, Final Batch Loss: 1.1398927199479658e-05\n",
      "Epoch 4432, Loss: 0.028640000759878603, Final Batch Loss: 0.0007357046706601977\n",
      "Epoch 4433, Loss: 0.011942633806029335, Final Batch Loss: 0.00015742814866825938\n",
      "Epoch 4434, Loss: 0.01094310943290111, Final Batch Loss: 2.5175460905302316e-05\n",
      "Epoch 4435, Loss: 0.013681571825145511, Final Batch Loss: 0.00011578691919567063\n",
      "Epoch 4436, Loss: 0.00801847802949851, Final Batch Loss: 2.5997731427196413e-05\n",
      "Epoch 4437, Loss: 0.0023750524592287547, Final Batch Loss: 0.0001699271088000387\n",
      "Epoch 4438, Loss: 0.0019137405822675646, Final Batch Loss: 3.070401362492703e-05\n",
      "Epoch 4439, Loss: 0.007086257292030496, Final Batch Loss: 0.002437595510855317\n",
      "Epoch 4440, Loss: 0.0033877454291086906, Final Batch Loss: 7.6035980782762635e-06\n",
      "Epoch 4441, Loss: 0.003912611611212924, Final Batch Loss: 0.0008045010035857558\n",
      "Epoch 4442, Loss: 0.0012939054806793138, Final Batch Loss: 1.064219077306916e-06\n",
      "Epoch 4443, Loss: 0.015353660468917951, Final Batch Loss: 0.00023553409846499562\n",
      "Epoch 4444, Loss: 0.024331463630915096, Final Batch Loss: 9.096711437450722e-05\n",
      "Epoch 4445, Loss: 0.07426298887367011, Final Batch Loss: 0.00013644741557072848\n",
      "Epoch 4446, Loss: 0.07314739255252789, Final Batch Loss: 0.0004649815382435918\n",
      "Epoch 4447, Loss: 0.0997283950146084, Final Batch Loss: 0.012340008281171322\n",
      "Epoch 4448, Loss: 0.2203788519691443, Final Batch Loss: 0.010729853995144367\n",
      "Epoch 4449, Loss: 0.1939412412175443, Final Batch Loss: 0.015142729505896568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4450, Loss: 0.10208917074487545, Final Batch Loss: 0.003693557344377041\n",
      "Epoch 4451, Loss: 0.07357409001269843, Final Batch Loss: 0.003391884732991457\n",
      "Epoch 4452, Loss: 0.04440137420169776, Final Batch Loss: 0.020597342401742935\n",
      "Epoch 4453, Loss: 0.04059844007133506, Final Batch Loss: 0.0003934925189241767\n",
      "Epoch 4454, Loss: 0.023826882061257493, Final Batch Loss: 0.00034227780997753143\n",
      "Epoch 4455, Loss: 0.0637638532789424, Final Batch Loss: 0.001384509028866887\n",
      "Epoch 4456, Loss: 0.05842544973711483, Final Batch Loss: 0.0006467532366514206\n",
      "Epoch 4457, Loss: 0.015798902186361374, Final Batch Loss: 0.0008462807745672762\n",
      "Epoch 4458, Loss: 0.018573027235106565, Final Batch Loss: 0.0005933461361564696\n",
      "Epoch 4459, Loss: 0.018517502474423964, Final Batch Loss: 0.0036807162687182426\n",
      "Epoch 4460, Loss: 0.04763632552931085, Final Batch Loss: 0.004148061387240887\n",
      "Epoch 4461, Loss: 0.02069588563972502, Final Batch Loss: 5.92754477111157e-05\n",
      "Epoch 4462, Loss: 0.03740801678941352, Final Batch Loss: 0.0006226529367268085\n",
      "Epoch 4463, Loss: 0.03941093842877308, Final Batch Loss: 0.002128806198015809\n",
      "Epoch 4464, Loss: 0.06405219095904613, Final Batch Loss: 0.0003053766558878124\n",
      "Epoch 4465, Loss: 0.00925788113818271, Final Batch Loss: 8.897873340174556e-05\n",
      "Epoch 4466, Loss: 0.014583364016289124, Final Batch Loss: 0.0006049242801964283\n",
      "Epoch 4467, Loss: 0.015728650585515425, Final Batch Loss: 0.0004874130245298147\n",
      "Epoch 4468, Loss: 0.038461380692751845, Final Batch Loss: 0.019378190860152245\n",
      "Epoch 4469, Loss: 0.018141744047170505, Final Batch Loss: 0.0004127275897189975\n",
      "Epoch 4470, Loss: 0.031536909053102136, Final Batch Loss: 0.0012449135538190603\n",
      "Epoch 4471, Loss: 0.058912880822390434, Final Batch Loss: 0.0038845050148665905\n",
      "Epoch 4472, Loss: 0.016585078243224416, Final Batch Loss: 0.002889263676479459\n",
      "Epoch 4473, Loss: 0.03999191345064901, Final Batch Loss: 0.001704906695522368\n",
      "Epoch 4474, Loss: 0.007521580695538432, Final Batch Loss: 0.00016799899458419532\n",
      "Epoch 4475, Loss: 0.006160809421999147, Final Batch Loss: 0.0008969272021204233\n",
      "Epoch 4476, Loss: 0.007997751639777562, Final Batch Loss: 0.0003538586897775531\n",
      "Epoch 4477, Loss: 0.016692701572083024, Final Batch Loss: 0.0009928162908181548\n",
      "Epoch 4478, Loss: 0.0016747860208852217, Final Batch Loss: 0.0004842066846322268\n",
      "Epoch 4479, Loss: 0.01679752574818849, Final Batch Loss: 0.000552409968804568\n",
      "Epoch 4480, Loss: 0.026337709576182533, Final Batch Loss: 4.895664096693508e-05\n",
      "Epoch 4481, Loss: 0.010512380735235638, Final Batch Loss: 0.0003451979428064078\n",
      "Epoch 4482, Loss: 0.003947968343709363, Final Batch Loss: 0.0006958547746762633\n",
      "Epoch 4483, Loss: 0.016147869180713315, Final Batch Loss: 0.00023721928300801665\n",
      "Epoch 4484, Loss: 0.08730325368378544, Final Batch Loss: 5.118727494846098e-05\n",
      "Epoch 4485, Loss: 0.007399635178444441, Final Batch Loss: 0.0005342194344848394\n",
      "Epoch 4486, Loss: 0.015327465313021094, Final Batch Loss: 6.52127419016324e-05\n",
      "Epoch 4487, Loss: 0.013961544975245488, Final Batch Loss: 0.0011959115508943796\n",
      "Epoch 4488, Loss: 0.003848077802103944, Final Batch Loss: 7.345169433392584e-05\n",
      "Epoch 4489, Loss: 0.008766257838033198, Final Batch Loss: 7.202100823633373e-05\n",
      "Epoch 4490, Loss: 0.004375900127342902, Final Batch Loss: 0.00020173257507849485\n",
      "Epoch 4491, Loss: 0.010886862690313137, Final Batch Loss: 0.001837932039052248\n",
      "Epoch 4492, Loss: 0.021031008975114673, Final Batch Loss: 2.4357650545425713e-05\n",
      "Epoch 4493, Loss: 0.017615949973333045, Final Batch Loss: 3.087129516643472e-05\n",
      "Epoch 4494, Loss: 0.015165968690780574, Final Batch Loss: 0.0037317732349038124\n",
      "Epoch 4495, Loss: 0.0028910141863889294, Final Batch Loss: 8.992488437797874e-05\n",
      "Epoch 4496, Loss: 0.012210066478473891, Final Batch Loss: 0.002858993597328663\n",
      "Epoch 4497, Loss: 0.010521429798245663, Final Batch Loss: 8.661652100272477e-05\n",
      "Epoch 4498, Loss: 0.009568180457790731, Final Batch Loss: 0.00016533295274712145\n",
      "Epoch 4499, Loss: 0.0041046871801881935, Final Batch Loss: 9.088780643651262e-05\n",
      "Epoch 4500, Loss: 0.004025307372103271, Final Batch Loss: 2.0043373297085054e-05\n",
      "Epoch 4501, Loss: 0.06670504349267503, Final Batch Loss: 1.8563991034170613e-05\n",
      "Epoch 4502, Loss: 0.15134188680713123, Final Batch Loss: 0.0017789568519219756\n",
      "Epoch 4503, Loss: 0.014848555674689123, Final Batch Loss: 0.00040879088919609785\n",
      "Epoch 4504, Loss: 0.007804136749655299, Final Batch Loss: 0.002298041945323348\n",
      "Epoch 4505, Loss: 0.007889151964263874, Final Batch Loss: 1.0021262824011501e-05\n",
      "Epoch 4506, Loss: 0.014513129908664268, Final Batch Loss: 0.0016713137738406658\n",
      "Epoch 4507, Loss: 0.07436645243433304, Final Batch Loss: 0.027270417660474777\n",
      "Epoch 4508, Loss: 0.01847808230922965, Final Batch Loss: 0.0013431735569611192\n",
      "Epoch 4509, Loss: 0.0316974635279621, Final Batch Loss: 0.001577231683768332\n",
      "Epoch 4510, Loss: 0.018438411552779144, Final Batch Loss: 0.0005818564677610993\n",
      "Epoch 4511, Loss: 0.018183186344685964, Final Batch Loss: 0.0009312629117630422\n",
      "Epoch 4512, Loss: 0.010858119687327417, Final Batch Loss: 4.6937682782299817e-05\n",
      "Epoch 4513, Loss: 0.03035850182368449, Final Batch Loss: 0.00025981321232393384\n",
      "Epoch 4514, Loss: 0.007908357383712428, Final Batch Loss: 3.404848030186258e-05\n",
      "Epoch 4515, Loss: 0.00606605318171205, Final Batch Loss: 0.00011313332652207464\n",
      "Epoch 4516, Loss: 0.0067712672735069646, Final Batch Loss: 0.00013455553562380373\n",
      "Epoch 4517, Loss: 0.01316106704143749, Final Batch Loss: 2.027799746429082e-05\n",
      "Epoch 4518, Loss: 0.005428289772680728, Final Batch Loss: 0.0003170548880007118\n",
      "Epoch 4519, Loss: 0.06643182985317253, Final Batch Loss: 0.00018551797256805003\n",
      "Epoch 4520, Loss: 0.018747895213891752, Final Batch Loss: 0.00019802879251074046\n",
      "Epoch 4521, Loss: 0.008153503698849818, Final Batch Loss: 0.0004020484338980168\n",
      "Epoch 4522, Loss: 0.007006382264080457, Final Batch Loss: 2.6926551072392613e-05\n",
      "Epoch 4523, Loss: 0.014667886880488368, Final Batch Loss: 0.0002788706624414772\n",
      "Epoch 4524, Loss: 0.02030505275251926, Final Batch Loss: 0.0001145277637988329\n",
      "Epoch 4525, Loss: 0.015463364215975162, Final Batch Loss: 0.0006074660923331976\n",
      "Epoch 4526, Loss: 0.009624533053283812, Final Batch Loss: 8.194184192689136e-05\n",
      "Epoch 4527, Loss: 0.019431843630627554, Final Batch Loss: 0.0002102454745909199\n",
      "Epoch 4528, Loss: 0.008282538382445637, Final Batch Loss: 0.00011880403326358646\n",
      "Epoch 4529, Loss: 0.016660496019540005, Final Batch Loss: 0.0024325691629201174\n",
      "Epoch 4530, Loss: 0.0060612485776800895, Final Batch Loss: 0.0011354634771123528\n",
      "Epoch 4531, Loss: 0.017811272801736777, Final Batch Loss: 0.0013535127509385347\n",
      "Epoch 4532, Loss: 0.012890555171907181, Final Batch Loss: 0.0011754970764741302\n",
      "Epoch 4533, Loss: 0.011467756452475442, Final Batch Loss: 4.9771635531215e-05\n",
      "Epoch 4534, Loss: 0.030925753382689436, Final Batch Loss: 1.6559310097363777e-05\n",
      "Epoch 4535, Loss: 0.0044721442773152376, Final Batch Loss: 4.6234701585490257e-05\n",
      "Epoch 4536, Loss: 0.0015159638433033251, Final Batch Loss: 0.00012857888941653073\n",
      "Epoch 4537, Loss: 0.012209586934659455, Final Batch Loss: 0.0030323516111820936\n",
      "Epoch 4538, Loss: 0.0018604588376547326, Final Batch Loss: 8.214128683903255e-06\n",
      "Epoch 4539, Loss: 0.006410114695427183, Final Batch Loss: 1.2912893907923717e-05\n",
      "Epoch 4540, Loss: 0.005153963606971956, Final Batch Loss: 0.00012697219790425152\n",
      "Epoch 4541, Loss: 0.0067158264482714, Final Batch Loss: 6.0346192185534164e-05\n",
      "Epoch 4542, Loss: 0.002010999805861502, Final Batch Loss: 2.084288644255139e-05\n",
      "Epoch 4543, Loss: 0.013473154014718602, Final Batch Loss: 0.0006805052398703992\n",
      "Epoch 4544, Loss: 0.002797915672090312, Final Batch Loss: 0.0003950729442294687\n",
      "Epoch 4545, Loss: 0.009729487746881205, Final Batch Loss: 3.988909884355962e-05\n",
      "Epoch 4546, Loss: 0.010936995862721233, Final Batch Loss: 1.5260824511642568e-05\n",
      "Epoch 4547, Loss: 0.013120258824983466, Final Batch Loss: 1.883178083517123e-05\n",
      "Epoch 4548, Loss: 0.0151801839529071, Final Batch Loss: 9.360178228234872e-05\n",
      "Epoch 4549, Loss: 0.036936857795808464, Final Batch Loss: 9.359052637591958e-05\n",
      "Epoch 4550, Loss: 0.05176609409681987, Final Batch Loss: 0.0028169513680040836\n",
      "Epoch 4551, Loss: 0.029508912266464904, Final Batch Loss: 5.421965033747256e-05\n",
      "Epoch 4552, Loss: 0.05323965814750409, Final Batch Loss: 5.336343383532949e-05\n",
      "Epoch 4553, Loss: 0.026612559599016095, Final Batch Loss: 0.0008626410854049027\n",
      "Epoch 4554, Loss: 0.22616885377647122, Final Batch Loss: 0.004296866245567799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4555, Loss: 0.123621795210056, Final Batch Loss: 0.0002538859553169459\n",
      "Epoch 4556, Loss: 0.037941261420201045, Final Batch Loss: 0.0002976547693833709\n",
      "Epoch 4557, Loss: 0.04237061468302272, Final Batch Loss: 0.0004544283729046583\n",
      "Epoch 4558, Loss: 0.023578817086672643, Final Batch Loss: 0.004718516021966934\n",
      "Epoch 4559, Loss: 0.02501222857972607, Final Batch Loss: 0.001215203432366252\n",
      "Epoch 4560, Loss: 0.019214711788663408, Final Batch Loss: 0.00013654741633217782\n",
      "Epoch 4561, Loss: 0.028118305952375522, Final Batch Loss: 0.0005870829336345196\n",
      "Epoch 4562, Loss: 0.03617964832483267, Final Batch Loss: 0.0006655353936366737\n",
      "Epoch 4563, Loss: 0.00957046504572645, Final Batch Loss: 0.0005593819660134614\n",
      "Epoch 4564, Loss: 0.015404560543174739, Final Batch Loss: 0.0006299126544035971\n",
      "Epoch 4565, Loss: 0.020402460609147965, Final Batch Loss: 0.0002550834324210882\n",
      "Epoch 4566, Loss: 0.0029696988904106547, Final Batch Loss: 0.00029896374326199293\n",
      "Epoch 4567, Loss: 0.026322857597733673, Final Batch Loss: 0.000369956367649138\n",
      "Epoch 4568, Loss: 0.01858379432815127, Final Batch Loss: 0.0004038072074763477\n",
      "Epoch 4569, Loss: 0.018840992292098235, Final Batch Loss: 0.0028706274461001158\n",
      "Epoch 4570, Loss: 0.023084434054908343, Final Batch Loss: 0.00029219812131486833\n",
      "Epoch 4571, Loss: 0.010438926357892342, Final Batch Loss: 0.0007628747262060642\n",
      "Epoch 4572, Loss: 0.02836316055527277, Final Batch Loss: 1.4911595826561097e-05\n",
      "Epoch 4573, Loss: 0.01458949804873555, Final Batch Loss: 0.0005154355894774199\n",
      "Epoch 4574, Loss: 0.0101316194395622, Final Batch Loss: 4.020985579700209e-05\n",
      "Epoch 4575, Loss: 0.017449013004807057, Final Batch Loss: 0.0004029541742056608\n",
      "Epoch 4576, Loss: 0.07192575392036815, Final Batch Loss: 0.0012656109174713492\n",
      "Epoch 4577, Loss: 0.025585732320905663, Final Batch Loss: 0.0019806649070233107\n",
      "Epoch 4578, Loss: 0.009616498628020054, Final Batch Loss: 0.0029195493552833796\n",
      "Epoch 4579, Loss: 0.010414127092190029, Final Batch Loss: 0.0002999431162606925\n",
      "Epoch 4580, Loss: 0.008901349237930845, Final Batch Loss: 0.00047807127702981234\n",
      "Epoch 4581, Loss: 0.018566120325886004, Final Batch Loss: 4.070678187417798e-05\n",
      "Epoch 4582, Loss: 0.03159676802351896, Final Batch Loss: 0.0002596160920802504\n",
      "Epoch 4583, Loss: 0.021456229509567493, Final Batch Loss: 0.00019542833615560085\n",
      "Epoch 4584, Loss: 0.0113170668132625, Final Batch Loss: 5.376286935643293e-05\n",
      "Epoch 4585, Loss: 0.0071090311621446745, Final Batch Loss: 6.463551108026877e-05\n",
      "Epoch 4586, Loss: 0.012968441553312005, Final Batch Loss: 1.4495079085463658e-05\n",
      "Epoch 4587, Loss: 0.021820068461238407, Final Batch Loss: 0.0009911453817039728\n",
      "Epoch 4588, Loss: 0.0023796478108124575, Final Batch Loss: 0.0003401416470296681\n",
      "Epoch 4589, Loss: 0.0031494038848904893, Final Batch Loss: 0.0004921244690194726\n",
      "Epoch 4590, Loss: 0.013192553887165559, Final Batch Loss: 2.0906609279336408e-05\n",
      "Epoch 4591, Loss: 0.005234738706349162, Final Batch Loss: 0.00015655647439416498\n",
      "Epoch 4592, Loss: 0.004005668207355484, Final Batch Loss: 8.324025839101523e-05\n",
      "Epoch 4593, Loss: 0.008048173805946135, Final Batch Loss: 1.6548956409678794e-05\n",
      "Epoch 4594, Loss: 0.007578844799354556, Final Batch Loss: 0.00037358293775469065\n",
      "Epoch 4595, Loss: 0.004644769097012613, Final Batch Loss: 0.0005127631593495607\n",
      "Epoch 4596, Loss: 0.003654882457340136, Final Batch Loss: 0.00024255314201582223\n",
      "Epoch 4597, Loss: 0.0049124522606689425, Final Batch Loss: 2.3533661078545265e-05\n",
      "Epoch 4598, Loss: 0.007523028338255244, Final Batch Loss: 0.000141813769005239\n",
      "Epoch 4599, Loss: 0.004896376260603574, Final Batch Loss: 6.7696951191464905e-06\n",
      "Epoch 4600, Loss: 0.0181961193807183, Final Batch Loss: 2.5110910428338684e-05\n",
      "Epoch 4601, Loss: 0.003918834721844178, Final Batch Loss: 2.4659177142893896e-05\n",
      "Epoch 4602, Loss: 0.0031816384616831783, Final Batch Loss: 8.547733159502968e-05\n",
      "Epoch 4603, Loss: 0.01424885342566995, Final Batch Loss: 8.828675345284864e-05\n",
      "Epoch 4604, Loss: 0.00313411964793886, Final Batch Loss: 0.0011380132054910064\n",
      "Epoch 4605, Loss: 0.0025091334646276664, Final Batch Loss: 1.9745557438000105e-05\n",
      "Epoch 4606, Loss: 0.009846031868164573, Final Batch Loss: 0.0006704838015139103\n",
      "Epoch 4607, Loss: 0.01503313856665045, Final Batch Loss: 1.761470593919512e-05\n",
      "Epoch 4608, Loss: 0.04690161495545908, Final Batch Loss: 0.00038272966048680246\n",
      "Epoch 4609, Loss: 0.008467856504466909, Final Batch Loss: 0.00040456114220432937\n",
      "Epoch 4610, Loss: 0.029918102175997774, Final Batch Loss: 0.0009206775575876236\n",
      "Epoch 4611, Loss: 0.004331464134338603, Final Batch Loss: 6.156219751574099e-05\n",
      "Epoch 4612, Loss: 0.03095764544696067, Final Batch Loss: 6.574882718268782e-05\n",
      "Epoch 4613, Loss: 0.017424385845515644, Final Batch Loss: 7.876479503465816e-05\n",
      "Epoch 4614, Loss: 0.011854609918373171, Final Batch Loss: 3.476123310974799e-05\n",
      "Epoch 4615, Loss: 0.06822528357224655, Final Batch Loss: 0.00011077486851718277\n",
      "Epoch 4616, Loss: 0.009801071657420835, Final Batch Loss: 0.0013886712258681655\n",
      "Epoch 4617, Loss: 0.07600535405890696, Final Batch Loss: 0.005491519812494516\n",
      "Epoch 4618, Loss: 0.11984497929006466, Final Batch Loss: 0.004425358027219772\n",
      "Epoch 4619, Loss: 0.03740606077190023, Final Batch Loss: 0.0001923955715028569\n",
      "Epoch 4620, Loss: 0.046582964932895266, Final Batch Loss: 0.018233774229884148\n",
      "Epoch 4621, Loss: 0.020373163795738947, Final Batch Loss: 0.0001732131204335019\n",
      "Epoch 4622, Loss: 0.01485681407211814, Final Batch Loss: 0.004187763202935457\n",
      "Epoch 4623, Loss: 0.021561241359449923, Final Batch Loss: 0.00043508497765287757\n",
      "Epoch 4624, Loss: 0.037229112880595494, Final Batch Loss: 0.0004092140879947692\n",
      "Epoch 4625, Loss: 0.02900001351372339, Final Batch Loss: 0.0005137977423146367\n",
      "Epoch 4626, Loss: 0.01989452169800643, Final Batch Loss: 0.001520833931863308\n",
      "Epoch 4627, Loss: 0.009210334817908006, Final Batch Loss: 2.144706013496034e-05\n",
      "Epoch 4628, Loss: 0.05001579377858434, Final Batch Loss: 0.019819067791104317\n",
      "Epoch 4629, Loss: 0.04824940853177395, Final Batch Loss: 0.0006065857596695423\n",
      "Epoch 4630, Loss: 0.011978940725384746, Final Batch Loss: 0.0025711646303534508\n",
      "Epoch 4631, Loss: 0.034502005195463425, Final Batch Loss: 5.345771205611527e-05\n",
      "Epoch 4632, Loss: 0.005437739804619923, Final Batch Loss: 0.0003809001646004617\n",
      "Epoch 4633, Loss: 0.00782810202872497, Final Batch Loss: 0.0011001164093613625\n",
      "Epoch 4634, Loss: 0.011577187417060486, Final Batch Loss: 0.000689737731590867\n",
      "Epoch 4635, Loss: 0.015738130579848075, Final Batch Loss: 0.00042271707206964493\n",
      "Epoch 4636, Loss: 0.01783426794281695, Final Batch Loss: 1.2297690773266368e-05\n",
      "Epoch 4637, Loss: 0.016084421704363194, Final Batch Loss: 0.002650826470926404\n",
      "Epoch 4638, Loss: 0.00735417726355081, Final Batch Loss: 0.0007281522266566753\n",
      "Epoch 4639, Loss: 0.028893296625028597, Final Batch Loss: 0.0003858655400108546\n",
      "Epoch 4640, Loss: 0.014663028359791497, Final Batch Loss: 0.00011893767077708617\n",
      "Epoch 4641, Loss: 0.00988219656937872, Final Batch Loss: 9.146412048721686e-05\n",
      "Epoch 4642, Loss: 0.007362416654359549, Final Batch Loss: 0.00010299921268597245\n",
      "Epoch 4643, Loss: 0.01874936321291898, Final Batch Loss: 0.0026016440242528915\n",
      "Epoch 4644, Loss: 0.00770434142987142, Final Batch Loss: 0.0006659110076725483\n",
      "Epoch 4645, Loss: 0.006174105305035482, Final Batch Loss: 0.0005848174332641065\n",
      "Epoch 4646, Loss: 0.0035732685732909886, Final Batch Loss: 1.8537075447966345e-05\n",
      "Epoch 4647, Loss: 0.0028580227403836034, Final Batch Loss: 1.7603893866180442e-05\n",
      "Epoch 4648, Loss: 0.008205974187148968, Final Batch Loss: 0.0003651641309261322\n",
      "Epoch 4649, Loss: 0.03523021922137559, Final Batch Loss: 0.024443207308650017\n",
      "Epoch 4650, Loss: 0.0015274876459443476, Final Batch Loss: 0.00015692545275669545\n",
      "Epoch 4651, Loss: 0.006888871237606509, Final Batch Loss: 0.00015161630290094763\n",
      "Epoch 4652, Loss: 0.03514843465291051, Final Batch Loss: 0.007251514587551355\n",
      "Epoch 4653, Loss: 0.003969838262491976, Final Batch Loss: 3.909547012881376e-05\n",
      "Epoch 4654, Loss: 0.005027913432058995, Final Batch Loss: 0.00035810022382065654\n",
      "Epoch 4655, Loss: 0.00854802082903916, Final Batch Loss: 0.001128391013480723\n",
      "Epoch 4656, Loss: 0.0061040607834002, Final Batch Loss: 0.0021644574590027332\n",
      "Epoch 4657, Loss: 0.06124830173666851, Final Batch Loss: 0.023171572014689445\n",
      "Epoch 4658, Loss: 0.05036066911270609, Final Batch Loss: 0.0006862711161375046\n",
      "Epoch 4659, Loss: 0.04866911888075265, Final Batch Loss: 0.0011129827471449971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4660, Loss: 0.02918877343472559, Final Batch Loss: 0.00028475155704654753\n",
      "Epoch 4661, Loss: 0.06365156757237855, Final Batch Loss: 0.000991867040283978\n",
      "Epoch 4662, Loss: 0.02380766086935182, Final Batch Loss: 0.013655407354235649\n",
      "Epoch 4663, Loss: 0.00956163337923499, Final Batch Loss: 0.0010252476204186678\n",
      "Epoch 4664, Loss: 0.039652058214414865, Final Batch Loss: 2.074024814646691e-05\n",
      "Epoch 4665, Loss: 0.00826716886513168, Final Batch Loss: 8.264771167887375e-05\n",
      "Epoch 4666, Loss: 0.007091119820870517, Final Batch Loss: 0.00012413588410709053\n",
      "Epoch 4667, Loss: 0.058697123029560316, Final Batch Loss: 0.0007673309883102775\n",
      "Epoch 4668, Loss: 0.012320220495894318, Final Batch Loss: 0.0009513049735687673\n",
      "Epoch 4669, Loss: 0.02771994543763867, Final Batch Loss: 9.249156573787332e-05\n",
      "Epoch 4670, Loss: 0.010771496778033907, Final Batch Loss: 6.579429464181885e-05\n",
      "Epoch 4671, Loss: 0.040153990289582, Final Batch Loss: 1.615632754692342e-05\n",
      "Epoch 4672, Loss: 0.010150643127417425, Final Batch Loss: 2.2060359697206877e-05\n",
      "Epoch 4673, Loss: 0.012225042422869592, Final Batch Loss: 4.525575786828995e-05\n",
      "Epoch 4674, Loss: 0.010794025010909536, Final Batch Loss: 4.9975209549302235e-05\n",
      "Epoch 4675, Loss: 0.03723665167126455, Final Batch Loss: 2.1841122361365706e-05\n",
      "Epoch 4676, Loss: 0.016631314871119685, Final Batch Loss: 4.071011790074408e-05\n",
      "Epoch 4677, Loss: 0.04279819526709616, Final Batch Loss: 0.030918098986148834\n",
      "Epoch 4678, Loss: 0.010588079114313587, Final Batch Loss: 0.0038451957516372204\n",
      "Epoch 4679, Loss: 0.009142086079918954, Final Batch Loss: 9.894781214825343e-06\n",
      "Epoch 4680, Loss: 0.024371543059260148, Final Batch Loss: 0.004548325669020414\n",
      "Epoch 4681, Loss: 0.018471432395926968, Final Batch Loss: 0.00019289871852379292\n",
      "Epoch 4682, Loss: 0.06243111864387174, Final Batch Loss: 0.00030150465318001807\n",
      "Epoch 4683, Loss: 0.05336414425255498, Final Batch Loss: 0.00553957698866725\n",
      "Epoch 4684, Loss: 0.03389537235671014, Final Batch Loss: 0.0002896453079301864\n",
      "Epoch 4685, Loss: 0.014162798201141413, Final Batch Loss: 0.0002183877950301394\n",
      "Epoch 4686, Loss: 0.00577053705274011, Final Batch Loss: 6.379770638886839e-05\n",
      "Epoch 4687, Loss: 0.014011458715685876, Final Batch Loss: 0.00018028277554549277\n",
      "Epoch 4688, Loss: 0.010684676446544472, Final Batch Loss: 5.9406687796581537e-05\n",
      "Epoch 4689, Loss: 0.009673095275502419, Final Batch Loss: 3.636771361925639e-05\n",
      "Epoch 4690, Loss: 0.00765384047281259, Final Batch Loss: 1.0734430361480918e-05\n",
      "Epoch 4691, Loss: 0.004762800721437088, Final Batch Loss: 2.1426521925604902e-05\n",
      "Epoch 4692, Loss: 0.013326849003533425, Final Batch Loss: 0.002542070345953107\n",
      "Epoch 4693, Loss: 0.009886044796076021, Final Batch Loss: 0.00023025312111712992\n",
      "Epoch 4694, Loss: 0.0028078795357942, Final Batch Loss: 0.00023039730149321258\n",
      "Epoch 4695, Loss: 0.016279265175398905, Final Batch Loss: 4.937213452649303e-05\n",
      "Epoch 4696, Loss: 0.008626826680028898, Final Batch Loss: 0.0004327558563090861\n",
      "Epoch 4697, Loss: 0.0030218249194149394, Final Batch Loss: 9.04196931514889e-05\n",
      "Epoch 4698, Loss: 0.005591882612861809, Final Batch Loss: 3.800060221692547e-05\n",
      "Epoch 4699, Loss: 0.030651992957245966, Final Batch Loss: 0.0001749372750055045\n",
      "Epoch 4700, Loss: 0.005180049449791113, Final Batch Loss: 0.0007244389271363616\n",
      "Epoch 4701, Loss: 0.002678904937056359, Final Batch Loss: 6.338892853818834e-05\n",
      "Epoch 4702, Loss: 0.017099128919653594, Final Batch Loss: 0.0001967587013496086\n",
      "Epoch 4703, Loss: 0.005795666423637158, Final Batch Loss: 6.4419596128573176e-06\n",
      "Epoch 4704, Loss: 0.02827897975112137, Final Batch Loss: 0.0001634623622521758\n",
      "Epoch 4705, Loss: 0.017114545444201212, Final Batch Loss: 0.00037901903851889074\n",
      "Epoch 4706, Loss: 0.028336643601505784, Final Batch Loss: 0.0074301185086369514\n",
      "Epoch 4707, Loss: 0.013137984693457838, Final Batch Loss: 0.00046280439710244536\n",
      "Epoch 4708, Loss: 0.028014671319397166, Final Batch Loss: 0.00017664555343799293\n",
      "Epoch 4709, Loss: 0.008061982774961507, Final Batch Loss: 0.00010796356946229935\n",
      "Epoch 4710, Loss: 0.01194196455526253, Final Batch Loss: 0.0002525897871237248\n",
      "Epoch 4711, Loss: 0.0046819711751595605, Final Batch Loss: 0.0005087208119221032\n",
      "Epoch 4712, Loss: 0.009380248506204225, Final Batch Loss: 6.57480995869264e-05\n",
      "Epoch 4713, Loss: 0.02542566180045469, Final Batch Loss: 5.9291494835633785e-05\n",
      "Epoch 4714, Loss: 0.029425434837321518, Final Batch Loss: 0.0007694584201090038\n",
      "Epoch 4715, Loss: 0.016925848398386734, Final Batch Loss: 0.0007937794434837997\n",
      "Epoch 4716, Loss: 0.044398097472367226, Final Batch Loss: 0.00017126019520219415\n",
      "Epoch 4717, Loss: 0.06381948977650609, Final Batch Loss: 0.002847452647984028\n",
      "Epoch 4718, Loss: 0.05818749165700865, Final Batch Loss: 0.00031019546440802515\n",
      "Epoch 4719, Loss: 0.06801656633615494, Final Batch Loss: 0.004309173673391342\n",
      "Epoch 4720, Loss: 0.08121860379105783, Final Batch Loss: 0.000612783944234252\n",
      "Epoch 4721, Loss: 0.020811588383367052, Final Batch Loss: 0.0010728914057835937\n",
      "Epoch 4722, Loss: 0.01434622242959449, Final Batch Loss: 0.0003372392093297094\n",
      "Epoch 4723, Loss: 0.02721259675308829, Final Batch Loss: 0.0010328853968530893\n",
      "Epoch 4724, Loss: 0.018276477450854145, Final Batch Loss: 0.0020600645802915096\n",
      "Epoch 4725, Loss: 0.007338251576584298, Final Batch Loss: 0.0010733074741438031\n",
      "Epoch 4726, Loss: 0.008046624174312456, Final Batch Loss: 0.00025709698093123734\n",
      "Epoch 4727, Loss: 0.005398754483394441, Final Batch Loss: 0.00014318402099888772\n",
      "Epoch 4728, Loss: 0.03405807855506282, Final Batch Loss: 5.978051922284067e-05\n",
      "Epoch 4729, Loss: 0.030750319638173096, Final Batch Loss: 0.0011864895932376385\n",
      "Epoch 4730, Loss: 0.09003378613488167, Final Batch Loss: 0.0001579734671395272\n",
      "Epoch 4731, Loss: 0.0871872677889769, Final Batch Loss: 0.00018450524657964706\n",
      "Epoch 4732, Loss: 0.06456811669340823, Final Batch Loss: 0.0043515972793102264\n",
      "Epoch 4733, Loss: 0.04569835451547988, Final Batch Loss: 5.1610262744361535e-05\n",
      "Epoch 4734, Loss: 0.028613159578526393, Final Batch Loss: 0.0015560039319097996\n",
      "Epoch 4735, Loss: 0.01855186609463999, Final Batch Loss: 0.0006843652809038758\n",
      "Epoch 4736, Loss: 0.011295204878479126, Final Batch Loss: 0.005288781598210335\n",
      "Epoch 4737, Loss: 0.016558074385102373, Final Batch Loss: 7.497564365621656e-05\n",
      "Epoch 4738, Loss: 0.009776650749699911, Final Batch Loss: 7.426557567669079e-05\n",
      "Epoch 4739, Loss: 0.00852646567091142, Final Batch Loss: 0.0011672748951241374\n",
      "Epoch 4740, Loss: 0.009531560623145197, Final Batch Loss: 0.00031784106977283955\n",
      "Epoch 4741, Loss: 0.005636880876409123, Final Batch Loss: 4.0973514842335135e-05\n",
      "Epoch 4742, Loss: 0.014560841393176815, Final Batch Loss: 2.2445010472438298e-05\n",
      "Epoch 4743, Loss: 0.018491021008230746, Final Batch Loss: 0.0031010417733341455\n",
      "Epoch 4744, Loss: 0.037122682262634044, Final Batch Loss: 0.0005958364927209914\n",
      "Epoch 4745, Loss: 0.024414927458565217, Final Batch Loss: 0.00015615351730957627\n",
      "Epoch 4746, Loss: 0.011394994846341433, Final Batch Loss: 0.0004814958374481648\n",
      "Epoch 4747, Loss: 0.016280552132229786, Final Batch Loss: 0.011165554635226727\n",
      "Epoch 4748, Loss: 0.01782863391963474, Final Batch Loss: 0.00024045298050623387\n",
      "Epoch 4749, Loss: 0.015249357141783548, Final Batch Loss: 0.00012178741599200293\n",
      "Epoch 4750, Loss: 0.02418411727740022, Final Batch Loss: 3.184686283930205e-05\n",
      "Epoch 4751, Loss: 0.02111406981930486, Final Batch Loss: 0.00013306608889251947\n",
      "Epoch 4752, Loss: 0.0306988968341102, Final Batch Loss: 0.0021490927319973707\n",
      "Epoch 4753, Loss: 0.015767622069688514, Final Batch Loss: 0.0012279150541871786\n",
      "Epoch 4754, Loss: 0.009483766689299955, Final Batch Loss: 6.398809637175873e-05\n",
      "Epoch 4755, Loss: 0.010933073933983906, Final Batch Loss: 0.0005717805470339954\n",
      "Epoch 4756, Loss: 0.023051272196425998, Final Batch Loss: 0.0034745042212307453\n",
      "Epoch 4757, Loss: 0.043258219920971896, Final Batch Loss: 0.00016867055092006922\n",
      "Epoch 4758, Loss: 0.010103739584337745, Final Batch Loss: 0.0009223651140928268\n",
      "Epoch 4759, Loss: 0.007644283606168756, Final Batch Loss: 0.00479811429977417\n",
      "Epoch 4760, Loss: 0.013731326922425069, Final Batch Loss: 2.1431185814435594e-05\n",
      "Epoch 4761, Loss: 0.006941441645267332, Final Batch Loss: 0.00011908298620255664\n",
      "Epoch 4762, Loss: 0.011566224851776497, Final Batch Loss: 7.483583613066003e-05\n",
      "Epoch 4763, Loss: 0.027687038812473475, Final Batch Loss: 0.00018521306628827006\n",
      "Epoch 4764, Loss: 0.0034088698048435617, Final Batch Loss: 1.29453064801055e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4765, Loss: 0.004917745635793835, Final Batch Loss: 0.00025366831687279046\n",
      "Epoch 4766, Loss: 0.009271599916246487, Final Batch Loss: 0.0004072768206242472\n",
      "Epoch 4767, Loss: 0.0021398402604972944, Final Batch Loss: 3.8010355638107285e-05\n",
      "Epoch 4768, Loss: 0.004957979278515268, Final Batch Loss: 0.00023992106434889138\n",
      "Epoch 4769, Loss: 0.011864396128203225, Final Batch Loss: 5.727559710066998e-06\n",
      "Epoch 4770, Loss: 0.009659090208515408, Final Batch Loss: 6.376450619427487e-05\n",
      "Epoch 4771, Loss: 0.013231805157374765, Final Batch Loss: 0.00022781996813137084\n",
      "Epoch 4772, Loss: 0.014193507916843373, Final Batch Loss: 0.0003356367815285921\n",
      "Epoch 4773, Loss: 0.0404830423212843, Final Batch Loss: 0.001664857380092144\n",
      "Epoch 4774, Loss: 0.04899399405621807, Final Batch Loss: 0.00012125051580369473\n",
      "Epoch 4775, Loss: 0.026268941361195175, Final Batch Loss: 0.006187442224472761\n",
      "Epoch 4776, Loss: 0.02714237172040157, Final Batch Loss: 3.0372444598469883e-05\n",
      "Epoch 4777, Loss: 0.011724253371539817, Final Batch Loss: 0.00038563646376132965\n",
      "Epoch 4778, Loss: 0.02235014076745756, Final Batch Loss: 0.0011547030881047249\n",
      "Epoch 4779, Loss: 0.024940160472169737, Final Batch Loss: 0.006339604500681162\n",
      "Epoch 4780, Loss: 0.009430152006643766, Final Batch Loss: 0.00017855098121799529\n",
      "Epoch 4781, Loss: 0.004263602641003672, Final Batch Loss: 0.0012037069536745548\n",
      "Epoch 4782, Loss: 0.0067092123163092765, Final Batch Loss: 3.518117591738701e-05\n",
      "Epoch 4783, Loss: 0.0012456147328521183, Final Batch Loss: 4.5324071834329516e-05\n",
      "Epoch 4784, Loss: 0.004934132200105523, Final Batch Loss: 3.655620457720943e-05\n",
      "Epoch 4785, Loss: 0.001689655591690098, Final Batch Loss: 0.00026754086138680577\n",
      "Epoch 4786, Loss: 0.0031551891825074563, Final Batch Loss: 6.972174742259085e-05\n",
      "Epoch 4787, Loss: 0.0010385614177721436, Final Batch Loss: 7.166017894633114e-05\n",
      "Epoch 4788, Loss: 0.009417851655598497, Final Batch Loss: 4.132348476559855e-05\n",
      "Epoch 4789, Loss: 0.004669237943289772, Final Batch Loss: 0.00014715790166519582\n",
      "Epoch 4790, Loss: 0.04483742951606473, Final Batch Loss: 0.0013247844763100147\n",
      "Epoch 4791, Loss: 0.07432950294241891, Final Batch Loss: 5.5565462389495224e-05\n",
      "Epoch 4792, Loss: 0.01905543613975169, Final Batch Loss: 6.29376809229143e-05\n",
      "Epoch 4793, Loss: 0.03446207209981367, Final Batch Loss: 3.2546809961786494e-05\n",
      "Epoch 4794, Loss: 0.005907830802243552, Final Batch Loss: 0.00291453767567873\n",
      "Epoch 4795, Loss: 0.05183997037966037, Final Batch Loss: 0.00045882328413426876\n",
      "Epoch 4796, Loss: 0.04616278428966325, Final Batch Loss: 8.138040720950812e-05\n",
      "Epoch 4797, Loss: 0.034158875674620504, Final Batch Loss: 0.00024720572400838137\n",
      "Epoch 4798, Loss: 0.007656655267055612, Final Batch Loss: 4.609239476849325e-05\n",
      "Epoch 4799, Loss: 0.0646841833495273, Final Batch Loss: 0.003081640461459756\n",
      "Epoch 4800, Loss: 0.07547466676260228, Final Batch Loss: 0.0018121149623766541\n",
      "Epoch 4801, Loss: 0.08879610727490217, Final Batch Loss: 0.01614810712635517\n",
      "Epoch 4802, Loss: 0.01137495503644459, Final Batch Loss: 0.00035997593658976257\n",
      "Epoch 4803, Loss: 0.01484050528597436, Final Batch Loss: 0.00013601459795609117\n",
      "Epoch 4804, Loss: 0.021802666602525278, Final Batch Loss: 0.013019328005611897\n",
      "Epoch 4805, Loss: 0.04625027481233701, Final Batch Loss: 0.0001418731699232012\n",
      "Epoch 4806, Loss: 0.04478058484164649, Final Batch Loss: 0.016991587355732918\n",
      "Epoch 4807, Loss: 0.008498675379087217, Final Batch Loss: 5.567225889535621e-05\n",
      "Epoch 4808, Loss: 0.15043580092969933, Final Batch Loss: 0.0026884449180215597\n",
      "Epoch 4809, Loss: 0.02215432615048485, Final Batch Loss: 0.0006245789700187743\n",
      "Epoch 4810, Loss: 0.005685541109414771, Final Batch Loss: 0.00016480540216434747\n",
      "Epoch 4811, Loss: 0.008693860490893712, Final Batch Loss: 0.0007126192795112729\n",
      "Epoch 4812, Loss: 0.025172397259666468, Final Batch Loss: 2.2073445506975986e-05\n",
      "Epoch 4813, Loss: 0.014550860683812061, Final Batch Loss: 0.002216365886852145\n",
      "Epoch 4814, Loss: 0.01618056521692779, Final Batch Loss: 0.0004847777890972793\n",
      "Epoch 4815, Loss: 0.020067562289113994, Final Batch Loss: 4.122475002077408e-05\n",
      "Epoch 4816, Loss: 0.009862091917966609, Final Batch Loss: 0.00025333743542432785\n",
      "Epoch 4817, Loss: 0.006198294297064422, Final Batch Loss: 0.00020975271763745695\n",
      "Epoch 4818, Loss: 0.02119351092733268, Final Batch Loss: 1.2991347830393352e-05\n",
      "Epoch 4819, Loss: 0.09189049704218633, Final Batch Loss: 0.00010038296022685245\n",
      "Epoch 4820, Loss: 0.012247390565789829, Final Batch Loss: 0.0008985886815935373\n",
      "Epoch 4821, Loss: 0.019398628017370356, Final Batch Loss: 2.1346295397961512e-05\n",
      "Epoch 4822, Loss: 0.030663076122436905, Final Batch Loss: 0.0005407636053860188\n",
      "Epoch 4823, Loss: 0.0422853222626145, Final Batch Loss: 0.007896329276263714\n",
      "Epoch 4824, Loss: 0.015420539340993855, Final Batch Loss: 0.0006286432617343962\n",
      "Epoch 4825, Loss: 0.013388921618570748, Final Batch Loss: 0.0031249383464455605\n",
      "Epoch 4826, Loss: 0.024346906757273246, Final Batch Loss: 2.1032321456004865e-05\n",
      "Epoch 4827, Loss: 0.008425388388786814, Final Batch Loss: 0.00032022781670093536\n",
      "Epoch 4828, Loss: 0.015511391755353543, Final Batch Loss: 0.0003820314595941454\n",
      "Epoch 4829, Loss: 0.011670791982396622, Final Batch Loss: 2.914600736403372e-05\n",
      "Epoch 4830, Loss: 0.007505857363867108, Final Batch Loss: 1.6368010619771667e-05\n",
      "Epoch 4831, Loss: 0.004211484161714907, Final Batch Loss: 0.0006170909036882222\n",
      "Epoch 4832, Loss: 0.002336486980311747, Final Batch Loss: 6.2712118960917e-05\n",
      "Epoch 4833, Loss: 0.003679503511193616, Final Batch Loss: 4.4769862142857164e-05\n",
      "Epoch 4834, Loss: 0.0036687110186903737, Final Batch Loss: 0.0003176830941811204\n",
      "Epoch 4835, Loss: 0.006342694927297998, Final Batch Loss: 0.002814410487189889\n",
      "Epoch 4836, Loss: 0.005865221442945767, Final Batch Loss: 3.0638671887572855e-05\n",
      "Epoch 4837, Loss: 0.012258060997282882, Final Batch Loss: 3.400920468266122e-05\n",
      "Epoch 4838, Loss: 0.03520985802879295, Final Batch Loss: 0.0006581670604646206\n",
      "Epoch 4839, Loss: 0.01502374454139499, Final Batch Loss: 3.077415021834895e-05\n",
      "Epoch 4840, Loss: 0.020953858616849175, Final Batch Loss: 0.0003942318435292691\n",
      "Epoch 4841, Loss: 0.004910662763904838, Final Batch Loss: 0.00012611776764970273\n",
      "Epoch 4842, Loss: 0.0244331622034224, Final Batch Loss: 0.0016557391500100493\n",
      "Epoch 4843, Loss: 0.005090361262773513, Final Batch Loss: 1.904445707623381e-05\n",
      "Epoch 4844, Loss: 0.008585398209106643, Final Batch Loss: 0.00042247719829902053\n",
      "Epoch 4845, Loss: 0.034919813729175075, Final Batch Loss: 1.5077623174875043e-05\n",
      "Epoch 4846, Loss: 0.008132342980388785, Final Batch Loss: 0.0005740385386161506\n",
      "Epoch 4847, Loss: 0.008243420711551153, Final Batch Loss: 2.002791552513372e-05\n",
      "Epoch 4848, Loss: 0.01385360274798586, Final Batch Loss: 2.8032352929585613e-05\n",
      "Epoch 4849, Loss: 0.0044334909189274185, Final Batch Loss: 0.002242270391434431\n",
      "Epoch 4850, Loss: 0.0038980927597549453, Final Batch Loss: 0.0002634095726534724\n",
      "Epoch 4851, Loss: 0.010983954098264803, Final Batch Loss: 4.841651752940379e-05\n",
      "Epoch 4852, Loss: 0.01608005724119721, Final Batch Loss: 0.00017030726303346455\n",
      "Epoch 4853, Loss: 0.011545664823643165, Final Batch Loss: 6.1030783399473876e-05\n",
      "Epoch 4854, Loss: 0.007010977113168337, Final Batch Loss: 0.0006219144561327994\n",
      "Epoch 4855, Loss: 0.0028607871527128736, Final Batch Loss: 5.6566983403172344e-05\n",
      "Epoch 4856, Loss: 0.0069368548965940136, Final Batch Loss: 3.748954623006284e-05\n",
      "Epoch 4857, Loss: 0.005222421769758512, Final Batch Loss: 1.5653158698114567e-05\n",
      "Epoch 4858, Loss: 0.007348742100475647, Final Batch Loss: 0.00013083804515190423\n",
      "Epoch 4859, Loss: 0.018882315088376345, Final Batch Loss: 0.0035564147401601076\n",
      "Epoch 4860, Loss: 0.029944359646833618, Final Batch Loss: 2.1770620151073672e-05\n",
      "Epoch 4861, Loss: 0.017719313932047953, Final Batch Loss: 0.0004038145416416228\n",
      "Epoch 4862, Loss: 0.006349148520712333, Final Batch Loss: 0.0028754458762705326\n",
      "Epoch 4863, Loss: 0.059069808939966606, Final Batch Loss: 5.09077035530936e-05\n",
      "Epoch 4864, Loss: 0.00740011931702611, Final Batch Loss: 0.0011536020319908857\n",
      "Epoch 4865, Loss: 0.05627508178440621, Final Batch Loss: 0.02087409235537052\n",
      "Epoch 4866, Loss: 0.02432596203561843, Final Batch Loss: 0.0007620996329933405\n",
      "Epoch 4867, Loss: 0.018273144738486735, Final Batch Loss: 0.001369168167002499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4868, Loss: 0.003980189230787801, Final Batch Loss: 4.220819391775876e-05\n",
      "Epoch 4869, Loss: 0.006810093714193499, Final Batch Loss: 9.459557077207137e-06\n",
      "Epoch 4870, Loss: 0.004988504309949349, Final Batch Loss: 0.000466349272755906\n",
      "Epoch 4871, Loss: 0.003568565372916055, Final Batch Loss: 2.5423420083825476e-05\n",
      "Epoch 4872, Loss: 0.021248446012577915, Final Batch Loss: 6.301521352725103e-05\n",
      "Epoch 4873, Loss: 0.004490497423830675, Final Batch Loss: 6.290483725024387e-05\n",
      "Epoch 4874, Loss: 0.01684387932618847, Final Batch Loss: 5.317278191796504e-05\n",
      "Epoch 4875, Loss: 0.009367130782266031, Final Batch Loss: 2.1888194169150665e-05\n",
      "Epoch 4876, Loss: 0.009339724030724028, Final Batch Loss: 4.866775998380035e-05\n",
      "Epoch 4877, Loss: 0.019944977362683858, Final Batch Loss: 0.0045457929372787476\n",
      "Epoch 4878, Loss: 0.003361958518780739, Final Batch Loss: 0.0005395205807872117\n",
      "Epoch 4879, Loss: 0.007535255110269645, Final Batch Loss: 0.000205658667255193\n",
      "Epoch 4880, Loss: 0.044172790889206226, Final Batch Loss: 5.900540418224409e-05\n",
      "Epoch 4881, Loss: 0.006470645506851724, Final Batch Loss: 1.699698441370856e-05\n",
      "Epoch 4882, Loss: 0.030575110770769243, Final Batch Loss: 0.0004485437530092895\n",
      "Epoch 4883, Loss: 0.004015625287138391, Final Batch Loss: 0.0010185966966673732\n",
      "Epoch 4884, Loss: 0.027455867748358287, Final Batch Loss: 0.0005312269786372781\n",
      "Epoch 4885, Loss: 0.05754734300717246, Final Batch Loss: 0.0022890723776072264\n",
      "Epoch 4886, Loss: 0.0447792613049387, Final Batch Loss: 0.0052842372097074986\n",
      "Epoch 4887, Loss: 0.015468399524252163, Final Batch Loss: 0.0009459314751438797\n",
      "Epoch 4888, Loss: 0.017259626817121898, Final Batch Loss: 0.0036638074088841677\n",
      "Epoch 4889, Loss: 0.005262408307316946, Final Batch Loss: 8.300909394165501e-05\n",
      "Epoch 4890, Loss: 0.007914600577350939, Final Batch Loss: 0.0008322896319441497\n",
      "Epoch 4891, Loss: 0.03619593054827419, Final Batch Loss: 5.5966549552977085e-05\n",
      "Epoch 4892, Loss: 0.004047368327519507, Final Batch Loss: 0.00033159571466967463\n",
      "Epoch 4893, Loss: 0.004087274773155514, Final Batch Loss: 0.0011997437104582787\n",
      "Epoch 4894, Loss: 0.043128385974341654, Final Batch Loss: 7.843722414691001e-05\n",
      "Epoch 4895, Loss: 0.029860656715754885, Final Batch Loss: 0.0016389443771913648\n",
      "Epoch 4896, Loss: 0.018081718604662456, Final Batch Loss: 0.00014159674174152315\n",
      "Epoch 4897, Loss: 0.018125918668374652, Final Batch Loss: 0.0006029286305420101\n",
      "Epoch 4898, Loss: 0.009716942902741721, Final Batch Loss: 5.298080577631481e-05\n",
      "Epoch 4899, Loss: 0.008192472085283953, Final Batch Loss: 0.001988672884181142\n",
      "Epoch 4900, Loss: 0.008829788610228206, Final Batch Loss: 7.185083086369559e-05\n",
      "Epoch 4901, Loss: 0.0053438590734913305, Final Batch Loss: 0.00018097346764989197\n",
      "Epoch 4902, Loss: 0.011546856627319357, Final Batch Loss: 0.003523697145283222\n",
      "Epoch 4903, Loss: 0.006358470858685905, Final Batch Loss: 0.0030121977906674147\n",
      "Epoch 4904, Loss: 0.0232268321178708, Final Batch Loss: 2.30897894653026e-05\n",
      "Epoch 4905, Loss: 0.010229041041384335, Final Batch Loss: 0.0008945464505814016\n",
      "Epoch 4906, Loss: 0.02500046453064897, Final Batch Loss: 3.3525240723975e-05\n",
      "Epoch 4907, Loss: 0.006342751136799052, Final Batch Loss: 0.00048569179489277303\n",
      "Epoch 4908, Loss: 0.0060724808545273845, Final Batch Loss: 0.0001389296812703833\n",
      "Epoch 4909, Loss: 0.011660104283691908, Final Batch Loss: 0.001151873148046434\n",
      "Epoch 4910, Loss: 0.0048334303714909765, Final Batch Loss: 0.0010302263544872403\n",
      "Epoch 4911, Loss: 0.005094256079019033, Final Batch Loss: 7.143954007915454e-06\n",
      "Epoch 4912, Loss: 0.003229477627428423, Final Batch Loss: 1.7191687220474705e-05\n",
      "Epoch 4913, Loss: 0.027259265285465517, Final Batch Loss: 5.129303826834075e-05\n",
      "Epoch 4914, Loss: 0.11751175979770778, Final Batch Loss: 0.011576930992305279\n",
      "Epoch 4915, Loss: 0.10625428350067523, Final Batch Loss: 0.00024694035528227687\n",
      "Epoch 4916, Loss: 0.046065914647442696, Final Batch Loss: 1.5018696103652474e-05\n",
      "Epoch 4917, Loss: 0.07289033160122926, Final Batch Loss: 4.648156027542427e-05\n",
      "Epoch 4918, Loss: 0.017846645409008488, Final Batch Loss: 0.00020828560809604824\n",
      "Epoch 4919, Loss: 0.011352281664585462, Final Batch Loss: 0.00011889239976881072\n",
      "Epoch 4920, Loss: 0.010701567407522816, Final Batch Loss: 0.003909607883542776\n",
      "Epoch 4921, Loss: 0.008050108346651541, Final Batch Loss: 0.00029103661654517055\n",
      "Epoch 4922, Loss: 0.0033871073319460265, Final Batch Loss: 0.00022774543322157115\n",
      "Epoch 4923, Loss: 0.02995091326556576, Final Batch Loss: 0.0001958799985004589\n",
      "Epoch 4924, Loss: 0.12687359153824218, Final Batch Loss: 0.0002657052536960691\n",
      "Epoch 4925, Loss: 0.1167143123493588, Final Batch Loss: 0.029181918129324913\n",
      "Epoch 4926, Loss: 0.07493668120878283, Final Batch Loss: 0.0001936708140419796\n",
      "Epoch 4927, Loss: 0.05110211722785607, Final Batch Loss: 0.002977200085297227\n",
      "Epoch 4928, Loss: 0.02042148562031798, Final Batch Loss: 0.00014599115820601583\n",
      "Epoch 4929, Loss: 0.02811920774729515, Final Batch Loss: 2.1808658857480623e-05\n",
      "Epoch 4930, Loss: 0.019189502532753977, Final Batch Loss: 0.001136092352680862\n",
      "Epoch 4931, Loss: 0.01694420981584699, Final Batch Loss: 7.603584526805207e-05\n",
      "Epoch 4932, Loss: 0.023316069340580725, Final Batch Loss: 0.0008274180581793189\n",
      "Epoch 4933, Loss: 0.011218544475923409, Final Batch Loss: 6.617139297304675e-05\n",
      "Epoch 4934, Loss: 0.006030468013705104, Final Batch Loss: 5.839718505740166e-05\n",
      "Epoch 4935, Loss: 0.02920074113899318, Final Batch Loss: 3.411913348827511e-05\n",
      "Epoch 4936, Loss: 0.022711800247634528, Final Batch Loss: 0.00035945899435319006\n",
      "Epoch 4937, Loss: 0.0036567685965565033, Final Batch Loss: 0.0001764592743711546\n",
      "Epoch 4938, Loss: 0.004703726121078944, Final Batch Loss: 0.0010863611241802573\n",
      "Epoch 4939, Loss: 0.004653080208299798, Final Batch Loss: 0.0003436084953136742\n",
      "Epoch 4940, Loss: 0.011575953498322633, Final Batch Loss: 0.00015695221372880042\n",
      "Epoch 4941, Loss: 0.003129006085146102, Final Batch Loss: 0.00018882939184550196\n",
      "Epoch 4942, Loss: 0.0054727894566894975, Final Batch Loss: 2.785451761155855e-05\n",
      "Epoch 4943, Loss: 0.008012691935618932, Final Batch Loss: 6.180775380926207e-05\n",
      "Epoch 4944, Loss: 0.01393743618245935, Final Batch Loss: 0.005244577303528786\n",
      "Epoch 4945, Loss: 0.010461440861035953, Final Batch Loss: 0.0005428262520581484\n",
      "Epoch 4946, Loss: 0.011403171976780868, Final Batch Loss: 0.003550534835085273\n",
      "Epoch 4947, Loss: 0.0197503554445575, Final Batch Loss: 0.00021425832528620958\n",
      "Epoch 4948, Loss: 0.008257027704530628, Final Batch Loss: 0.0003616961184889078\n",
      "Epoch 4949, Loss: 0.023110697077754594, Final Batch Loss: 0.00012718845391646028\n",
      "Epoch 4950, Loss: 0.007475979702576296, Final Batch Loss: 0.00038697916897945106\n",
      "Epoch 4951, Loss: 0.004111040903808316, Final Batch Loss: 5.8433415688341483e-05\n",
      "Epoch 4952, Loss: 0.004695625492786348, Final Batch Loss: 1.572283508721739e-05\n",
      "Epoch 4953, Loss: 0.0057963741273852065, Final Batch Loss: 0.0032856955658644438\n",
      "Epoch 4954, Loss: 0.004391585395751463, Final Batch Loss: 4.409772373037413e-05\n",
      "Epoch 4955, Loss: 0.003223329970751365, Final Batch Loss: 3.159399057039991e-05\n",
      "Epoch 4956, Loss: 0.004393144844470953, Final Batch Loss: 5.139222776051611e-05\n",
      "Epoch 4957, Loss: 0.006458602552811499, Final Batch Loss: 1.0771436791401356e-05\n",
      "Epoch 4958, Loss: 0.0018484603688193602, Final Batch Loss: 1.9433173292782158e-05\n",
      "Epoch 4959, Loss: 0.0016882518038983108, Final Batch Loss: 1.0488489351700991e-05\n",
      "Epoch 4960, Loss: 0.007950850952965993, Final Batch Loss: 0.00019363968749530613\n",
      "Epoch 4961, Loss: 0.010547201754889102, Final Batch Loss: 3.627902697189711e-05\n",
      "Epoch 4962, Loss: 0.0690491560526425, Final Batch Loss: 0.0011333657894283533\n",
      "Epoch 4963, Loss: 0.026068132096042973, Final Batch Loss: 0.0005427675787359476\n",
      "Epoch 4964, Loss: 0.0263862946703739, Final Batch Loss: 6.340698018902913e-05\n",
      "Epoch 4965, Loss: 0.011263016622251598, Final Batch Loss: 9.271512681152672e-05\n",
      "Epoch 4966, Loss: 0.016677932784659788, Final Batch Loss: 7.482500950573012e-05\n",
      "Epoch 4967, Loss: 0.013322113094545784, Final Batch Loss: 0.00010215471411356702\n",
      "Epoch 4968, Loss: 0.011439114611675905, Final Batch Loss: 4.9101116019301116e-05\n",
      "Epoch 4969, Loss: 0.009467394560488174, Final Batch Loss: 2.3678780053160153e-05\n",
      "Epoch 4970, Loss: 0.004510045673669083, Final Batch Loss: 2.735968701017555e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4971, Loss: 0.012102753553335788, Final Batch Loss: 0.0003861026489175856\n",
      "Epoch 4972, Loss: 0.0016941902322287206, Final Batch Loss: 4.690002970164642e-05\n",
      "Epoch 4973, Loss: 0.0023399401898132055, Final Batch Loss: 3.645061224233359e-05\n",
      "Epoch 4974, Loss: 0.010296847653535224, Final Batch Loss: 6.09176313446369e-05\n",
      "Epoch 4975, Loss: 0.001993589125959261, Final Batch Loss: 8.491563676216174e-06\n",
      "Epoch 4976, Loss: 0.014028276465978706, Final Batch Loss: 0.0007079422357492149\n",
      "Epoch 4977, Loss: 0.007797622522957681, Final Batch Loss: 1.2133369637012947e-05\n",
      "Epoch 4978, Loss: 0.0035982566587335896, Final Batch Loss: 4.85991986352019e-05\n",
      "Epoch 4979, Loss: 0.021802299461342045, Final Batch Loss: 9.194802260026336e-05\n",
      "Epoch 4980, Loss: 0.002572436404079781, Final Batch Loss: 2.5853785700746812e-05\n",
      "Epoch 4981, Loss: 0.023760447080348968, Final Batch Loss: 0.0003468900395091623\n",
      "Epoch 4982, Loss: 0.016935818472575193, Final Batch Loss: 0.010722661390900612\n",
      "Epoch 4983, Loss: 0.004285500279365806, Final Batch Loss: 0.00024368819140363485\n",
      "Epoch 4984, Loss: 0.006083291707909666, Final Batch Loss: 0.0001514741888968274\n",
      "Epoch 4985, Loss: 0.002703124137951818, Final Batch Loss: 0.0001449727569706738\n",
      "Epoch 4986, Loss: 0.005843486685080279, Final Batch Loss: 0.0002010771568166092\n",
      "Epoch 4987, Loss: 0.016227485655690543, Final Batch Loss: 5.1660899771377444e-05\n",
      "Epoch 4988, Loss: 0.04727305756387068, Final Batch Loss: 0.00012546716607175767\n",
      "Epoch 4989, Loss: 0.06191266233508941, Final Batch Loss: 0.00182728364598006\n",
      "Epoch 4990, Loss: 0.009055655278643826, Final Batch Loss: 0.0005934582441113889\n",
      "Epoch 4991, Loss: 0.026658000340830768, Final Batch Loss: 2.284437869093381e-05\n",
      "Epoch 4992, Loss: 0.02891963498586847, Final Batch Loss: 0.00035285952617414296\n",
      "Epoch 4993, Loss: 0.015644030831026612, Final Batch Loss: 0.0003721764078363776\n",
      "Epoch 4994, Loss: 0.05223178843880305, Final Batch Loss: 1.8820415789377876e-05\n",
      "Epoch 4995, Loss: 0.09997940281755291, Final Batch Loss: 0.011600019410252571\n",
      "Epoch 4996, Loss: 0.1215375409083208, Final Batch Loss: 4.652165443985723e-05\n",
      "Epoch 4997, Loss: 0.02616858539840905, Final Batch Loss: 8.951496420195326e-05\n",
      "Epoch 4998, Loss: 0.10259564066655003, Final Batch Loss: 0.010313685052096844\n",
      "Epoch 4999, Loss: 0.05935813893665909, Final Batch Loss: 0.0003795719821937382\n",
      "Epoch 5000, Loss: 0.01659506486612372, Final Batch Loss: 3.2160969567485154e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[484  12   0]\n",
      " [ 42 378   0]\n",
      " [  0   0 491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.920     0.976     0.947       496\n",
      "           1      0.969     0.900     0.933       420\n",
      "           2      1.000     1.000     1.000       491\n",
      "\n",
      "    accuracy                          0.962      1407\n",
      "   macro avg      0.963     0.959     0.960      1407\n",
      "weighted avg      0.963     0.962     0.961      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 Label Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
