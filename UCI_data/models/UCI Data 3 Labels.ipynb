{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '484 fBodyGyro-bandsEnergy()-17,32',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_features = ['58 tGravityAcc-energy()-Y', '59 tGravityAcc-energy()-Z', '104 tBodyAccJerk-entropy()-Y', '125 tBodyGyro-std()-Y',\n",
    "#  '128 tBodyGyro-mad()-Y', '132 tBodyGyro-max()-Z', '134 tBodyGyro-min()-Y','138 tBodyGyro-energy()-Y', '141 tBodyGyro-iqr()-Y',\n",
    "#  '167 tBodyGyroJerk-mad()-X','168 tBodyGyroJerk-mad()-Y','177 tBodyGyroJerk-energy()-X', '181 tBodyGyroJerk-iqr()-Y',\n",
    "#  '475 fBodyGyro-bandsEnergy()-1,8', '484 fBodyGyro-bandsEnergy()-17,32','487 fBodyGyro-bandsEnergy()-1,24']\n",
    "\n",
    "# act_features = ['4 tBodyAcc-std()-X', '7 tBodyAcc-mad()-X', '10 tBodyAcc-max()-X', '17 tBodyAcc-energy()-X', '202 tBodyAccMag-std()',\n",
    "#  '204 tBodyAccMag-max()', '215 tGravityAccMag-std()', '217 tGravityAccMag-max()', '269 fBodyAcc-std()-X', '275 fBodyAcc-max()-X',\n",
    "#  '282 fBodyAcc-energy()-X', '286 fBodyAcc-iqr()-Y', '303 fBodyAcc-bandsEnergy()-1,8', '315 fBodyAcc-bandsEnergy()-1,24',\n",
    "#  '368 fBodyAccJerk-entropy()-Y', '390 fBodyAccJerk-bandsEnergy()-1,16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = len(act_features) + len(sub_features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.976623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.974321</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.989046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.978226</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.993552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.993062</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.990230</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.992378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.990506</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.401674</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.410577</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.185902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.362904</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.190360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.397687</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>0.022216</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.620014</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  125 tBodyGyro-std()-Y  ...  \\\n",
       "0                     -0.975510              -0.976623  ...   \n",
       "1                     -0.978500              -0.989046  ...   \n",
       "2                     -0.981672              -0.993552  ...   \n",
       "3                     -0.982420              -0.992407  ...   \n",
       "4                     -0.984363              -0.992378  ...   \n",
       "...                         ...                    ...  ...   \n",
       "7347                  -0.995193               0.084878  ...   \n",
       "7348                  -0.995151               0.098249  ...   \n",
       "7349                  -0.995450               0.185902  ...   \n",
       "7350                  -0.998824               0.190360  ...   \n",
       "7351                  -0.998144               0.022216  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  506 fBodyAccMag-max()  509 fBodyAccMag-energy()  \\\n",
       "0                 -0.948870              -0.974321                 -0.998285   \n",
       "1                 -0.975777              -0.978226                 -0.999472   \n",
       "2                 -0.985594              -0.993062                 -0.999807   \n",
       "3                 -0.983524              -0.990230                 -0.999770   \n",
       "4                 -0.992324              -0.990506                 -0.999873   \n",
       "...                     ...                    ...                       ...   \n",
       "7347              -0.007392              -0.401674                 -0.584282   \n",
       "7348              -0.172448              -0.410577                 -0.632536   \n",
       "7349              -0.216074              -0.362904                 -0.641170   \n",
       "7350              -0.220443              -0.397687                 -0.663579   \n",
       "7351              -0.146649              -0.620014                 -0.698087   \n",
       "\n",
       "      Activity  \n",
       "0            5  \n",
       "1            5  \n",
       "2            5  \n",
       "3            5  \n",
       "4            5  \n",
       "...        ...  \n",
       "7347         2  \n",
       "7348         2  \n",
       "7349         2  \n",
       "7350         2  \n",
       "7351         2  \n",
       "\n",
       "[7352 rows x 46 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "y_train = pd.read_csv('../data/y_train.txt', header = None)\n",
    "y_train.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "# X_train_1 = train_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_train_2 = train_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[(X_train['Activity'] == 1) | (X_train['Activity'] == 3) | (X_train['Activity'] == 4)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Activity'] == 1) | (y_train['Activity'] == 3) | (y_train['Activity'] == 4)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subjects = pd.read_csv('UCI/subject_train.txt', header = None)\n",
    "# train_subjects.columns = ['Subject']\n",
    "# train_subjects = train_subjects.values\n",
    "# train_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>125 tBodyGyro-std()-Y</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>506 fBodyAccMag-max()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.282719</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>-0.279244</td>\n",
       "      <td>0.152895</td>\n",
       "      <td>-0.262160</td>\n",
       "      <td>-0.076162</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.864621</td>\n",
       "      <td>-0.967795</td>\n",
       "      <td>-0.816164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997844</td>\n",
       "      <td>-0.998506</td>\n",
       "      <td>-0.998204</td>\n",
       "      <td>-0.998020</td>\n",
       "      <td>-0.999011</td>\n",
       "      <td>-0.711074</td>\n",
       "      <td>-0.726707</td>\n",
       "      <td>-0.777697</td>\n",
       "      <td>-0.953984</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.289215</td>\n",
       "      <td>0.152568</td>\n",
       "      <td>-0.304870</td>\n",
       "      <td>0.152895</td>\n",
       "      <td>-0.262160</td>\n",
       "      <td>0.149013</td>\n",
       "      <td>0.057676</td>\n",
       "      <td>-0.858163</td>\n",
       "      <td>-0.957240</td>\n",
       "      <td>-0.929599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999592</td>\n",
       "      <td>-0.999850</td>\n",
       "      <td>-0.999760</td>\n",
       "      <td>-0.999687</td>\n",
       "      <td>-0.999918</td>\n",
       "      <td>-0.959746</td>\n",
       "      <td>-0.960680</td>\n",
       "      <td>-0.968667</td>\n",
       "      <td>-0.998476</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.287513</td>\n",
       "      <td>0.146086</td>\n",
       "      <td>-0.304870</td>\n",
       "      <td>0.139454</td>\n",
       "      <td>-0.261661</td>\n",
       "      <td>0.144969</td>\n",
       "      <td>0.040561</td>\n",
       "      <td>-0.859947</td>\n",
       "      <td>-0.960965</td>\n",
       "      <td>-0.978511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-0.999962</td>\n",
       "      <td>-0.999958</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.983784</td>\n",
       "      <td>-0.977176</td>\n",
       "      <td>-0.991908</td>\n",
       "      <td>-0.999570</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.293396</td>\n",
       "      <td>0.142926</td>\n",
       "      <td>-0.305101</td>\n",
       "      <td>0.136124</td>\n",
       "      <td>-0.272916</td>\n",
       "      <td>0.142107</td>\n",
       "      <td>0.046106</td>\n",
       "      <td>-0.853713</td>\n",
       "      <td>-0.962713</td>\n",
       "      <td>-0.975134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.982120</td>\n",
       "      <td>-0.976796</td>\n",
       "      <td>-0.988398</td>\n",
       "      <td>-0.999504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.302961</td>\n",
       "      <td>0.138307</td>\n",
       "      <td>-0.312552</td>\n",
       "      <td>0.133541</td>\n",
       "      <td>-0.279190</td>\n",
       "      <td>0.130931</td>\n",
       "      <td>0.055351</td>\n",
       "      <td>-0.843378</td>\n",
       "      <td>-0.965137</td>\n",
       "      <td>-0.977951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-0.999961</td>\n",
       "      <td>-0.999960</td>\n",
       "      <td>-0.999957</td>\n",
       "      <td>-0.999960</td>\n",
       "      <td>-0.978838</td>\n",
       "      <td>-0.975706</td>\n",
       "      <td>-0.981305</td>\n",
       "      <td>-0.999500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>-0.276718</td>\n",
       "      <td>-0.231594</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>-0.226640</td>\n",
       "      <td>-0.272203</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>0.194878</td>\n",
       "      <td>-0.870515</td>\n",
       "      <td>-0.888599</td>\n",
       "      <td>-0.526855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.744467</td>\n",
       "      <td>-0.734651</td>\n",
       "      <td>-0.724505</td>\n",
       "      <td>-0.739414</td>\n",
       "      <td>-0.857784</td>\n",
       "      <td>-0.332141</td>\n",
       "      <td>-0.202661</td>\n",
       "      <td>-0.550923</td>\n",
       "      <td>-0.702110</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>-0.274976</td>\n",
       "      <td>-0.228050</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>-0.220590</td>\n",
       "      <td>-0.268172</td>\n",
       "      <td>-0.243507</td>\n",
       "      <td>0.181283</td>\n",
       "      <td>-0.872342</td>\n",
       "      <td>-0.891822</td>\n",
       "      <td>-0.518149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756815</td>\n",
       "      <td>-0.747251</td>\n",
       "      <td>-0.746326</td>\n",
       "      <td>-0.753166</td>\n",
       "      <td>-0.846479</td>\n",
       "      <td>-0.316954</td>\n",
       "      <td>-0.196060</td>\n",
       "      <td>-0.569485</td>\n",
       "      <td>-0.674032</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>-0.276165</td>\n",
       "      <td>-0.226256</td>\n",
       "      <td>-0.273818</td>\n",
       "      <td>-0.220590</td>\n",
       "      <td>-0.268172</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>0.178977</td>\n",
       "      <td>-0.871197</td>\n",
       "      <td>-0.893504</td>\n",
       "      <td>-0.557059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773399</td>\n",
       "      <td>-0.762837</td>\n",
       "      <td>-0.764571</td>\n",
       "      <td>-0.769811</td>\n",
       "      <td>-0.867931</td>\n",
       "      <td>-0.377240</td>\n",
       "      <td>-0.208208</td>\n",
       "      <td>-0.567270</td>\n",
       "      <td>-0.715711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>-0.262356</td>\n",
       "      <td>-0.235108</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>-0.231517</td>\n",
       "      <td>-0.244744</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>-0.884788</td>\n",
       "      <td>-0.885275</td>\n",
       "      <td>-0.555166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.768993</td>\n",
       "      <td>-0.751525</td>\n",
       "      <td>-0.755248</td>\n",
       "      <td>-0.765049</td>\n",
       "      <td>-0.880328</td>\n",
       "      <td>-0.390201</td>\n",
       "      <td>-0.259605</td>\n",
       "      <td>-0.654450</td>\n",
       "      <td>-0.745225</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>-0.261407</td>\n",
       "      <td>-0.236112</td>\n",
       "      <td>-0.272785</td>\n",
       "      <td>-0.233821</td>\n",
       "      <td>-0.242839</td>\n",
       "      <td>-0.245060</td>\n",
       "      <td>0.171842</td>\n",
       "      <td>-0.885692</td>\n",
       "      <td>-0.884370</td>\n",
       "      <td>-0.508557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773668</td>\n",
       "      <td>-0.755378</td>\n",
       "      <td>-0.754754</td>\n",
       "      <td>-0.768236</td>\n",
       "      <td>-0.884639</td>\n",
       "      <td>-0.362598</td>\n",
       "      <td>-0.231349</td>\n",
       "      <td>-0.605015</td>\n",
       "      <td>-0.723287</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.282719                 0.115288   \n",
       "1                   -0.289215                 0.152568   \n",
       "2                   -0.287513                 0.146086   \n",
       "3                   -0.293396                 0.142926   \n",
       "4                   -0.302961                 0.138307   \n",
       "...                       ...                      ...   \n",
       "2942                -0.276718                -0.231594   \n",
       "2943                -0.274976                -0.228050   \n",
       "2944                -0.276165                -0.226256   \n",
       "2945                -0.262356                -0.235108   \n",
       "2946                -0.261407                -0.236112   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.279244                0.152895               -0.262160   \n",
       "1                  -0.304870                0.152895               -0.262160   \n",
       "2                  -0.304870                0.139454               -0.261661   \n",
       "3                  -0.305101                0.136124               -0.272916   \n",
       "4                  -0.312552                0.133541               -0.279190   \n",
       "...                      ...                     ...                     ...   \n",
       "2942               -0.278442               -0.226640               -0.272203   \n",
       "2943               -0.278442               -0.220590               -0.268172   \n",
       "2944               -0.273818               -0.220590               -0.268172   \n",
       "2945               -0.272785               -0.231517               -0.244744   \n",
       "2946               -0.272785               -0.233821               -0.242839   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                  -0.076162             -0.017827                  -0.864621   \n",
       "1                   0.149013              0.057676                  -0.858163   \n",
       "2                   0.144969              0.040561                  -0.859947   \n",
       "3                   0.142107              0.046106                  -0.853713   \n",
       "4                   0.130931              0.055351                  -0.843378   \n",
       "...                      ...                   ...                        ...   \n",
       "2942               -0.243507              0.194878                  -0.870515   \n",
       "2943               -0.243507              0.181283                  -0.872342   \n",
       "2944               -0.245178              0.178977                  -0.871197   \n",
       "2945               -0.245178              0.168668                  -0.884788   \n",
       "2946               -0.245060              0.171842                  -0.885692   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  125 tBodyGyro-std()-Y  ...  \\\n",
       "0                     -0.967795              -0.816164  ...   \n",
       "1                     -0.957240              -0.929599  ...   \n",
       "2                     -0.960965              -0.978511  ...   \n",
       "3                     -0.962713              -0.975134  ...   \n",
       "4                     -0.965137              -0.977951  ...   \n",
       "...                         ...                    ...  ...   \n",
       "2942                  -0.888599              -0.526855  ...   \n",
       "2943                  -0.891822              -0.518149  ...   \n",
       "2944                  -0.893504              -0.557059  ...   \n",
       "2945                  -0.885275              -0.555166  ...   \n",
       "2946                  -0.884370              -0.508557  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.997844                       -0.998506   \n",
       "1                   -0.999592                       -0.999850   \n",
       "2                   -0.999954                       -0.999976   \n",
       "3                   -0.999963                       -0.999983   \n",
       "4                   -0.999954                       -0.999961   \n",
       "...                       ...                             ...   \n",
       "2942                -0.744467                       -0.734651   \n",
       "2943                -0.756815                       -0.747251   \n",
       "2944                -0.773399                       -0.762837   \n",
       "2945                -0.768993                       -0.751525   \n",
       "2946                -0.773668                       -0.755378   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.998204                        -0.998020   \n",
       "1                           -0.999760                        -0.999687   \n",
       "2                           -0.999962                        -0.999958   \n",
       "3                           -0.999971                        -0.999966   \n",
       "4                           -0.999960                        -0.999957   \n",
       "...                               ...                              ...   \n",
       "2942                        -0.724505                        -0.739414   \n",
       "2943                        -0.746326                        -0.753166   \n",
       "2944                        -0.764571                        -0.769811   \n",
       "2945                        -0.755248                        -0.765049   \n",
       "2946                        -0.754754                        -0.768236   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999011              -0.711074   \n",
       "1                              -0.999918              -0.959746   \n",
       "2                              -0.999996              -0.983784   \n",
       "3                              -0.999987              -0.982120   \n",
       "4                              -0.999960              -0.978838   \n",
       "...                                  ...                    ...   \n",
       "2942                           -0.857784              -0.332141   \n",
       "2943                           -0.846479              -0.316954   \n",
       "2944                           -0.867931              -0.377240   \n",
       "2945                           -0.880328              -0.390201   \n",
       "2946                           -0.884639              -0.362598   \n",
       "\n",
       "      505 fBodyAccMag-mad()  506 fBodyAccMag-max()  509 fBodyAccMag-energy()  \\\n",
       "0                 -0.726707              -0.777697                 -0.953984   \n",
       "1                 -0.960680              -0.968667                 -0.998476   \n",
       "2                 -0.977176              -0.991908                 -0.999570   \n",
       "3                 -0.976796              -0.988398                 -0.999504   \n",
       "4                 -0.975706              -0.981305                 -0.999500   \n",
       "...                     ...                    ...                       ...   \n",
       "2942              -0.202661              -0.550923                 -0.702110   \n",
       "2943              -0.196060              -0.569485                 -0.674032   \n",
       "2944              -0.208208              -0.567270                 -0.715711   \n",
       "2945              -0.259605              -0.654450                 -0.745225   \n",
       "2946              -0.231349              -0.605015                 -0.723287   \n",
       "\n",
       "      Activity  \n",
       "0            5  \n",
       "1            5  \n",
       "2            5  \n",
       "3            5  \n",
       "4            5  \n",
       "...        ...  \n",
       "2942         2  \n",
       "2943         2  \n",
       "2944         2  \n",
       "2945         2  \n",
       "2946         2  \n",
       "\n",
       "[2947 rows x 46 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "test_column_names = test_names.values.tolist()\n",
    "test_column_names = [k for row in test_column_names for k in row]\n",
    "\n",
    "test_data = pd.read_csv('../data/X_test.txt', delim_whitespace = True, header = None)\n",
    "test_data.columns = test_column_names\n",
    "\n",
    "y_test = pd.read_csv('../data/y_test.txt', header = None)\n",
    "y_test.columns = ['Activity']\n",
    "\n",
    "X_test_1 = test_data[sub_features]\n",
    "X_test_2 = test_data[act_features]\n",
    "\n",
    "# X_test_1 = test_data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "# X_test_2 = test_data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "X_test = pd.concat([X_test_1, X_test_2], axis = 1)\n",
    "\n",
    "X_test = pd.concat([X_test, y_test], axis = 1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[(X_test['Activity'] == 1) | (X_test['Activity'] == 3) | (X_test['Activity'] == 4)]\n",
    "X_test = X_test.iloc[:,:-1].values\n",
    "\n",
    "y_test = y_test[(y_test['Activity'] == 1) | (y_test['Activity'] == 3) | (y_test['Activity'] == 4)]\n",
    "y_test = y_test.values\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2\n",
    "        \n",
    "for k in range(len(y_test)):\n",
    "    if y_test[k] == 1:\n",
    "        y_test[k] = 0\n",
    "    elif y_test[k] == 3:\n",
    "        y_test[k] = 1\n",
    "    else:\n",
    "        y_test[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subjects = pd.read_csv('UCI/subject_test.txt', header = None)\n",
    "# test_subjects.columns = ['Subject']\n",
    "# test_subjects = test_subjects.values\n",
    "# test_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classifier_block(input_dim, output_dim):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Linear(input_dim, output_dim),\n",
    "#         nn.Dropout(0.1),\n",
    "#         nn.LeakyReLU(0.05)\n",
    "#     )\n",
    "\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, feature_dim = 40):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.network = nn.Sequential(\n",
    "#             classifier_block(feature_dim, 30),\n",
    "#             classifier_block(30, 25),\n",
    "#             classifier_block(25, 20),\n",
    "#             classifier_block(20, 10),\n",
    "#             nn.Linear(10, 3)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.29106891155243, Final Batch Loss: 1.0808331966400146\n",
      "Epoch 2, Loss: 15.098232746124268, Final Batch Loss: 1.0686311721801758\n",
      "Epoch 3, Loss: 14.698987007141113, Final Batch Loss: 1.0197675228118896\n",
      "Epoch 4, Loss: 13.311752438545227, Final Batch Loss: 0.8621904850006104\n",
      "Epoch 5, Loss: 10.74896103143692, Final Batch Loss: 0.6466216444969177\n",
      "Epoch 6, Loss: 8.136133462190628, Final Batch Loss: 0.5281177759170532\n",
      "Epoch 7, Loss: 6.363782912492752, Final Batch Loss: 0.3847540616989136\n",
      "Epoch 8, Loss: 5.052074879407883, Final Batch Loss: 0.3295777440071106\n",
      "Epoch 9, Loss: 4.171806275844574, Final Batch Loss: 0.3079298734664917\n",
      "Epoch 10, Loss: 3.5669468343257904, Final Batch Loss: 0.267760694026947\n",
      "Epoch 11, Loss: 3.3433051109313965, Final Batch Loss: 0.2914182245731354\n",
      "Epoch 12, Loss: 2.963102251291275, Final Batch Loss: 0.1823025941848755\n",
      "Epoch 13, Loss: 2.7892411053180695, Final Batch Loss: 0.2075517624616623\n",
      "Epoch 14, Loss: 2.6920008212327957, Final Batch Loss: 0.21902455389499664\n",
      "Epoch 15, Loss: 2.581010028719902, Final Batch Loss: 0.19153207540512085\n",
      "Epoch 16, Loss: 2.5453051179647446, Final Batch Loss: 0.1983768492937088\n",
      "Epoch 17, Loss: 2.348486691713333, Final Batch Loss: 0.1447262018918991\n",
      "Epoch 18, Loss: 2.2707182243466377, Final Batch Loss: 0.15100164711475372\n",
      "Epoch 19, Loss: 2.2604960426688194, Final Batch Loss: 0.15391620993614197\n",
      "Epoch 20, Loss: 2.229001507163048, Final Batch Loss: 0.13710671663284302\n",
      "Epoch 21, Loss: 2.0663425251841545, Final Batch Loss: 0.14248979091644287\n",
      "Epoch 22, Loss: 2.095502883195877, Final Batch Loss: 0.1759394109249115\n",
      "Epoch 23, Loss: 1.9341736286878586, Final Batch Loss: 0.12075024098157883\n",
      "Epoch 24, Loss: 1.8250266537070274, Final Batch Loss: 0.12592047452926636\n",
      "Epoch 25, Loss: 1.7315977737307549, Final Batch Loss: 0.11736200004816055\n",
      "Epoch 26, Loss: 1.8631296306848526, Final Batch Loss: 0.09602473676204681\n",
      "Epoch 27, Loss: 1.8525708466768265, Final Batch Loss: 0.08400457352399826\n",
      "Epoch 28, Loss: 1.7267846763134003, Final Batch Loss: 0.09943855553865433\n",
      "Epoch 29, Loss: 1.6451324298977852, Final Batch Loss: 0.12286888808012009\n",
      "Epoch 30, Loss: 1.6267022266983986, Final Batch Loss: 0.11636374890804291\n",
      "Epoch 31, Loss: 1.6248476207256317, Final Batch Loss: 0.07347247749567032\n",
      "Epoch 32, Loss: 1.6263361759483814, Final Batch Loss: 0.12588846683502197\n",
      "Epoch 33, Loss: 1.5705892443656921, Final Batch Loss: 0.15056569874286652\n",
      "Epoch 34, Loss: 1.5712340250611305, Final Batch Loss: 0.11190488189458847\n",
      "Epoch 35, Loss: 1.561768777668476, Final Batch Loss: 0.11582019925117493\n",
      "Epoch 36, Loss: 1.3852540254592896, Final Batch Loss: 0.0787346139550209\n",
      "Epoch 37, Loss: 1.4069876298308372, Final Batch Loss: 0.1230797991156578\n",
      "Epoch 38, Loss: 1.2888411469757557, Final Batch Loss: 0.09043882042169571\n",
      "Epoch 39, Loss: 1.428037591278553, Final Batch Loss: 0.14333182573318481\n",
      "Epoch 40, Loss: 1.3312548398971558, Final Batch Loss: 0.11958398669958115\n",
      "Epoch 41, Loss: 1.2958280593156815, Final Batch Loss: 0.11242103576660156\n",
      "Epoch 42, Loss: 1.1668330691754818, Final Batch Loss: 0.0750621110200882\n",
      "Epoch 43, Loss: 1.353132225573063, Final Batch Loss: 0.061207227408885956\n",
      "Epoch 44, Loss: 1.1327133402228355, Final Batch Loss: 0.08421450108289719\n",
      "Epoch 45, Loss: 1.1101225279271603, Final Batch Loss: 0.0848388746380806\n",
      "Epoch 46, Loss: 1.1246261037886143, Final Batch Loss: 0.09850922226905823\n",
      "Epoch 47, Loss: 1.1481884941458702, Final Batch Loss: 0.06880486756563187\n",
      "Epoch 48, Loss: 1.1418257504701614, Final Batch Loss: 0.09018922597169876\n",
      "Epoch 49, Loss: 1.1047629937529564, Final Batch Loss: 0.06799102574586868\n",
      "Epoch 50, Loss: 1.055599708110094, Final Batch Loss: 0.10006240010261536\n",
      "Epoch 51, Loss: 1.0428365916013718, Final Batch Loss: 0.08298426866531372\n",
      "Epoch 52, Loss: 0.9169794097542763, Final Batch Loss: 0.049333084374666214\n",
      "Epoch 53, Loss: 1.0189689919352531, Final Batch Loss: 0.08487054705619812\n",
      "Epoch 54, Loss: 0.9744509942829609, Final Batch Loss: 0.028031405061483383\n",
      "Epoch 55, Loss: 1.054108615964651, Final Batch Loss: 0.06963104754686356\n",
      "Epoch 56, Loss: 0.984370643272996, Final Batch Loss: 0.029349392279982567\n",
      "Epoch 57, Loss: 0.9991503171622753, Final Batch Loss: 0.06745220720767975\n",
      "Epoch 58, Loss: 0.9884436950087547, Final Batch Loss: 0.07103092968463898\n",
      "Epoch 59, Loss: 0.8762276396155357, Final Batch Loss: 0.05870405212044716\n",
      "Epoch 60, Loss: 1.0107819810509682, Final Batch Loss: 0.04851997271180153\n",
      "Epoch 61, Loss: 0.9152087736874819, Final Batch Loss: 0.04562167078256607\n",
      "Epoch 62, Loss: 0.9378655515611172, Final Batch Loss: 0.03491196781396866\n",
      "Epoch 63, Loss: 0.9283582493662834, Final Batch Loss: 0.07707937806844711\n",
      "Epoch 64, Loss: 0.8412456922233105, Final Batch Loss: 0.046872224658727646\n",
      "Epoch 65, Loss: 0.8819661717861891, Final Batch Loss: 0.05628177896142006\n",
      "Epoch 66, Loss: 0.8389232996851206, Final Batch Loss: 0.07872313261032104\n",
      "Epoch 67, Loss: 0.8636858034878969, Final Batch Loss: 0.05772123485803604\n",
      "Epoch 68, Loss: 0.9466050826013088, Final Batch Loss: 0.07314290851354599\n",
      "Epoch 69, Loss: 0.8072751443833113, Final Batch Loss: 0.05558712035417557\n",
      "Epoch 70, Loss: 0.8229117225855589, Final Batch Loss: 0.0384938083589077\n",
      "Epoch 71, Loss: 0.7814615741372108, Final Batch Loss: 0.09873875975608826\n",
      "Epoch 72, Loss: 0.8973151743412018, Final Batch Loss: 0.054277729243040085\n",
      "Epoch 73, Loss: 0.8642585128545761, Final Batch Loss: 0.04565388336777687\n",
      "Epoch 74, Loss: 0.8134562261402607, Final Batch Loss: 0.04731808230280876\n",
      "Epoch 75, Loss: 0.7949009593576193, Final Batch Loss: 0.05629565939307213\n",
      "Epoch 76, Loss: 0.836035717278719, Final Batch Loss: 0.0914565920829773\n",
      "Epoch 77, Loss: 0.8338511101901531, Final Batch Loss: 0.05058426782488823\n",
      "Epoch 78, Loss: 0.8012329079210758, Final Batch Loss: 0.06103895977139473\n",
      "Epoch 79, Loss: 0.815301064401865, Final Batch Loss: 0.03707336634397507\n",
      "Epoch 80, Loss: 0.7843762673437595, Final Batch Loss: 0.05084504559636116\n",
      "Epoch 81, Loss: 0.7753033265471458, Final Batch Loss: 0.05683860555291176\n",
      "Epoch 82, Loss: 0.8241556771099567, Final Batch Loss: 0.04095039516687393\n",
      "Epoch 83, Loss: 0.8025482706725597, Final Batch Loss: 0.0703074112534523\n",
      "Epoch 84, Loss: 0.7318687122315168, Final Batch Loss: 0.06250376254320145\n",
      "Epoch 85, Loss: 0.8024810198694468, Final Batch Loss: 0.04531703144311905\n",
      "Epoch 86, Loss: 0.7839039377868176, Final Batch Loss: 0.04991531744599342\n",
      "Epoch 87, Loss: 0.7346260286867619, Final Batch Loss: 0.08709291368722916\n",
      "Epoch 88, Loss: 0.8313529323786497, Final Batch Loss: 0.059450507164001465\n",
      "Epoch 89, Loss: 0.8310477957129478, Final Batch Loss: 0.13459770381450653\n",
      "Epoch 90, Loss: 0.7311383988708258, Final Batch Loss: 0.051747433841228485\n",
      "Epoch 91, Loss: 0.6728843282908201, Final Batch Loss: 0.04177873954176903\n",
      "Epoch 92, Loss: 0.7106669470667839, Final Batch Loss: 0.05969299376010895\n",
      "Epoch 93, Loss: 0.6936917491257191, Final Batch Loss: 0.06330154091119766\n",
      "Epoch 94, Loss: 0.7541533727198839, Final Batch Loss: 0.052904415875673294\n",
      "Epoch 95, Loss: 0.7145106196403503, Final Batch Loss: 0.02775747887790203\n",
      "Epoch 96, Loss: 0.6831160895526409, Final Batch Loss: 0.04334179311990738\n",
      "Epoch 97, Loss: 0.7129782326519489, Final Batch Loss: 0.056312911212444305\n",
      "Epoch 98, Loss: 0.6819803863763809, Final Batch Loss: 0.08138132840394974\n",
      "Epoch 99, Loss: 0.738953048363328, Final Batch Loss: 0.04309093579649925\n",
      "Epoch 100, Loss: 0.731536578387022, Final Batch Loss: 0.09907878190279007\n",
      "Epoch 101, Loss: 0.6916812341660261, Final Batch Loss: 0.03939899429678917\n",
      "Epoch 102, Loss: 0.6863620709627867, Final Batch Loss: 0.033196501433849335\n",
      "Epoch 103, Loss: 0.7152627091854811, Final Batch Loss: 0.02434932254254818\n",
      "Epoch 104, Loss: 0.6839122008532286, Final Batch Loss: 0.040364351123571396\n",
      "Epoch 105, Loss: 0.649178309366107, Final Batch Loss: 0.04717659577727318\n",
      "Epoch 106, Loss: 0.649615840986371, Final Batch Loss: 0.043936606496572495\n",
      "Epoch 107, Loss: 0.7546935342252254, Final Batch Loss: 0.042882487177848816\n",
      "Epoch 108, Loss: 0.6832429002970457, Final Batch Loss: 0.039035290479660034\n",
      "Epoch 109, Loss: 0.6051331534981728, Final Batch Loss: 0.031676024198532104\n",
      "Epoch 110, Loss: 0.7297021243721247, Final Batch Loss: 0.04637084901332855\n",
      "Epoch 111, Loss: 0.6263123974204063, Final Batch Loss: 0.033956438302993774\n",
      "Epoch 112, Loss: 0.6352819427847862, Final Batch Loss: 0.030928846448659897\n",
      "Epoch 113, Loss: 0.6886265985667706, Final Batch Loss: 0.031831540167331696\n",
      "Epoch 114, Loss: 0.6507298517972231, Final Batch Loss: 0.032905444502830505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115, Loss: 0.7054593041539192, Final Batch Loss: 0.04940754920244217\n",
      "Epoch 116, Loss: 0.6899261455982924, Final Batch Loss: 0.016885818913578987\n",
      "Epoch 117, Loss: 0.6067795194685459, Final Batch Loss: 0.04960978776216507\n",
      "Epoch 118, Loss: 0.6230927985161543, Final Batch Loss: 0.02488337829709053\n",
      "Epoch 119, Loss: 0.6674100011587143, Final Batch Loss: 0.04740270972251892\n",
      "Epoch 120, Loss: 0.6576861254870892, Final Batch Loss: 0.04878877103328705\n",
      "Epoch 121, Loss: 0.6448315102607012, Final Batch Loss: 0.06715258210897446\n",
      "Epoch 122, Loss: 0.6485944781452417, Final Batch Loss: 0.02565825544297695\n",
      "Epoch 123, Loss: 0.6149389464408159, Final Batch Loss: 0.04008781537413597\n",
      "Epoch 124, Loss: 0.6286736335605383, Final Batch Loss: 0.030923254787921906\n",
      "Epoch 125, Loss: 0.6076521705836058, Final Batch Loss: 0.04374115169048309\n",
      "Epoch 126, Loss: 0.6278554536402225, Final Batch Loss: 0.04534181207418442\n",
      "Epoch 127, Loss: 0.5708386618643999, Final Batch Loss: 0.04384032264351845\n",
      "Epoch 128, Loss: 0.600736390799284, Final Batch Loss: 0.029577774927020073\n",
      "Epoch 129, Loss: 0.572658964432776, Final Batch Loss: 0.04059987887740135\n",
      "Epoch 130, Loss: 0.5704768300056458, Final Batch Loss: 0.053889088332653046\n",
      "Epoch 131, Loss: 0.6391574870795012, Final Batch Loss: 0.08059719204902649\n",
      "Epoch 132, Loss: 0.5666632568463683, Final Batch Loss: 0.007211240939795971\n",
      "Epoch 133, Loss: 0.6139887403696775, Final Batch Loss: 0.05699106305837631\n",
      "Epoch 134, Loss: 0.5543513279408216, Final Batch Loss: 0.054070744663476944\n",
      "Epoch 135, Loss: 0.5611713025718927, Final Batch Loss: 0.022424690425395966\n",
      "Epoch 136, Loss: 0.5955172218382359, Final Batch Loss: 0.03228389844298363\n",
      "Epoch 137, Loss: 0.6031906269490719, Final Batch Loss: 0.04361889138817787\n",
      "Epoch 138, Loss: 0.5688375048339367, Final Batch Loss: 0.04148104786872864\n",
      "Epoch 139, Loss: 0.5331670716404915, Final Batch Loss: 0.02775530517101288\n",
      "Epoch 140, Loss: 0.5603460799902678, Final Batch Loss: 0.052106454968452454\n",
      "Epoch 141, Loss: 0.5928964708000422, Final Batch Loss: 0.03447822853922844\n",
      "Epoch 142, Loss: 0.5233031064271927, Final Batch Loss: 0.05301491916179657\n",
      "Epoch 143, Loss: 0.5536543074995279, Final Batch Loss: 0.028983429074287415\n",
      "Epoch 144, Loss: 0.6113929282873869, Final Batch Loss: 0.03245477378368378\n",
      "Epoch 145, Loss: 0.5301584908738732, Final Batch Loss: 0.049516256898641586\n",
      "Epoch 146, Loss: 0.49904876574873924, Final Batch Loss: 0.044427815824747086\n",
      "Epoch 147, Loss: 0.528932474553585, Final Batch Loss: 0.041913386434316635\n",
      "Epoch 148, Loss: 0.5615208875387907, Final Batch Loss: 0.04137559235095978\n",
      "Epoch 149, Loss: 0.5421588886529207, Final Batch Loss: 0.020554108545184135\n",
      "Epoch 150, Loss: 0.5741481650620699, Final Batch Loss: 0.07287900894880295\n",
      "Epoch 151, Loss: 0.5814763195812702, Final Batch Loss: 0.04039696231484413\n",
      "Epoch 152, Loss: 0.5760277900844812, Final Batch Loss: 0.03399166837334633\n",
      "Epoch 153, Loss: 0.5623391326516867, Final Batch Loss: 0.023852579295635223\n",
      "Epoch 154, Loss: 0.5007152315229177, Final Batch Loss: 0.025922764092683792\n",
      "Epoch 155, Loss: 0.5250625517219305, Final Batch Loss: 0.0296329353004694\n",
      "Epoch 156, Loss: 0.5277296556159854, Final Batch Loss: 0.048377908766269684\n",
      "Epoch 157, Loss: 0.6176814213395119, Final Batch Loss: 0.03917515650391579\n",
      "Epoch 158, Loss: 0.549381235614419, Final Batch Loss: 0.022850623354315758\n",
      "Epoch 159, Loss: 0.5567654371261597, Final Batch Loss: 0.05142810940742493\n",
      "Epoch 160, Loss: 0.5779467895627022, Final Batch Loss: 0.05059182271361351\n",
      "Epoch 161, Loss: 0.4716626238077879, Final Batch Loss: 0.050728145986795425\n",
      "Epoch 162, Loss: 0.4737995471805334, Final Batch Loss: 0.04605735093355179\n",
      "Epoch 163, Loss: 0.5337494453415275, Final Batch Loss: 0.03594360873103142\n",
      "Epoch 164, Loss: 0.5714948084205389, Final Batch Loss: 0.029184186831116676\n",
      "Epoch 165, Loss: 0.5169088169932365, Final Batch Loss: 0.06687205284833908\n",
      "Epoch 166, Loss: 0.5086867157369852, Final Batch Loss: 0.025765592232346535\n",
      "Epoch 167, Loss: 0.5344274155795574, Final Batch Loss: 0.03286752849817276\n",
      "Epoch 168, Loss: 0.46501487493515015, Final Batch Loss: 0.026388267055153847\n",
      "Epoch 169, Loss: 0.47506659012287855, Final Batch Loss: 0.03629818558692932\n",
      "Epoch 170, Loss: 0.49066995549947023, Final Batch Loss: 0.028459414839744568\n",
      "Epoch 171, Loss: 0.4279538122937083, Final Batch Loss: 0.024218285456299782\n",
      "Epoch 172, Loss: 0.48335964046418667, Final Batch Loss: 0.029032018035650253\n",
      "Epoch 173, Loss: 0.49850259721279144, Final Batch Loss: 0.03201397508382797\n",
      "Epoch 174, Loss: 0.5244693104177713, Final Batch Loss: 0.04609787091612816\n",
      "Epoch 175, Loss: 0.48876719549298286, Final Batch Loss: 0.056801971048116684\n",
      "Epoch 176, Loss: 0.442258189432323, Final Batch Loss: 0.022733481600880623\n",
      "Epoch 177, Loss: 0.45857859309762716, Final Batch Loss: 0.04082823917269707\n",
      "Epoch 178, Loss: 0.43784107454121113, Final Batch Loss: 0.03285493329167366\n",
      "Epoch 179, Loss: 0.50775779876858, Final Batch Loss: 0.038489606231451035\n",
      "Epoch 180, Loss: 0.4905797364190221, Final Batch Loss: 0.03021029755473137\n",
      "Epoch 181, Loss: 0.4450601125136018, Final Batch Loss: 0.014398897998034954\n",
      "Epoch 182, Loss: 0.4844319261610508, Final Batch Loss: 0.03313007205724716\n",
      "Epoch 183, Loss: 0.43060354702174664, Final Batch Loss: 0.01899082213640213\n",
      "Epoch 184, Loss: 0.38801224948838353, Final Batch Loss: 0.02948581427335739\n",
      "Epoch 185, Loss: 0.4055222515016794, Final Batch Loss: 0.00827764067798853\n",
      "Epoch 186, Loss: 0.44503151066601276, Final Batch Loss: 0.011011214926838875\n",
      "Epoch 187, Loss: 0.4414228145033121, Final Batch Loss: 0.030105315148830414\n",
      "Epoch 188, Loss: 0.42616137489676476, Final Batch Loss: 0.06351389735937119\n",
      "Epoch 189, Loss: 0.512176988646388, Final Batch Loss: 0.012742480263113976\n",
      "Epoch 190, Loss: 0.4186760722659528, Final Batch Loss: 0.026714127510786057\n",
      "Epoch 191, Loss: 0.4310163324698806, Final Batch Loss: 0.0452764518558979\n",
      "Epoch 192, Loss: 0.42122494569048285, Final Batch Loss: 0.016265390440821648\n",
      "Epoch 193, Loss: 0.46658495906740427, Final Batch Loss: 0.016962410882115364\n",
      "Epoch 194, Loss: 0.4426467502489686, Final Batch Loss: 0.023728663101792336\n",
      "Epoch 195, Loss: 0.4421340227127075, Final Batch Loss: 0.011088568717241287\n",
      "Epoch 196, Loss: 0.4308018395677209, Final Batch Loss: 0.04384922236204147\n",
      "Epoch 197, Loss: 0.42601345106959343, Final Batch Loss: 0.03773760050535202\n",
      "Epoch 198, Loss: 0.48181536002084613, Final Batch Loss: 0.04325465112924576\n",
      "Epoch 199, Loss: 0.5367973865941167, Final Batch Loss: 0.04028302803635597\n",
      "Epoch 200, Loss: 0.37857527006417513, Final Batch Loss: 0.014768612571060658\n",
      "Epoch 201, Loss: 0.4457474695518613, Final Batch Loss: 0.015877369791269302\n",
      "Epoch 202, Loss: 0.4214189210906625, Final Batch Loss: 0.01922275498509407\n",
      "Epoch 203, Loss: 0.4129453394562006, Final Batch Loss: 0.02457318641245365\n",
      "Epoch 204, Loss: 0.38303994247689843, Final Batch Loss: 0.07371518015861511\n",
      "Epoch 205, Loss: 0.44878296460956335, Final Batch Loss: 0.023801397532224655\n",
      "Epoch 206, Loss: 0.4102132534608245, Final Batch Loss: 0.03777124360203743\n",
      "Epoch 207, Loss: 0.43495370261371136, Final Batch Loss: 0.015214530751109123\n",
      "Epoch 208, Loss: 0.4026540694758296, Final Batch Loss: 0.03771723806858063\n",
      "Epoch 209, Loss: 0.44933465775102377, Final Batch Loss: 0.04984046518802643\n",
      "Epoch 210, Loss: 0.380073138512671, Final Batch Loss: 0.05050650238990784\n",
      "Epoch 211, Loss: 0.3854354349896312, Final Batch Loss: 0.011506478302180767\n",
      "Epoch 212, Loss: 0.44624477066099644, Final Batch Loss: 0.04627545550465584\n",
      "Epoch 213, Loss: 0.39094716869294643, Final Batch Loss: 0.03188582509756088\n",
      "Epoch 214, Loss: 0.48101002257317305, Final Batch Loss: 0.04662086442112923\n",
      "Epoch 215, Loss: 0.4191735778003931, Final Batch Loss: 0.01775018684566021\n",
      "Epoch 216, Loss: 0.40039240568876266, Final Batch Loss: 0.02334480918943882\n",
      "Epoch 217, Loss: 0.39194334857165813, Final Batch Loss: 0.025242531672120094\n",
      "Epoch 218, Loss: 0.4075520485639572, Final Batch Loss: 0.051630210131406784\n",
      "Epoch 219, Loss: 0.4175293883308768, Final Batch Loss: 0.042062874883413315\n",
      "Epoch 220, Loss: 0.37941462732851505, Final Batch Loss: 0.014011568389832973\n",
      "Epoch 221, Loss: 0.36100822035223246, Final Batch Loss: 0.015620049089193344\n",
      "Epoch 222, Loss: 0.3819065922871232, Final Batch Loss: 0.013286566361784935\n",
      "Epoch 223, Loss: 0.36381434043869376, Final Batch Loss: 0.014497488737106323\n",
      "Epoch 224, Loss: 0.3356000832282007, Final Batch Loss: 0.006211666855961084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, Loss: 0.4164561121724546, Final Batch Loss: 0.06815048307180405\n",
      "Epoch 226, Loss: 0.36340733617544174, Final Batch Loss: 0.01503700390458107\n",
      "Epoch 227, Loss: 0.3857622523792088, Final Batch Loss: 0.04041430726647377\n",
      "Epoch 228, Loss: 0.35214892448857427, Final Batch Loss: 0.016153031960129738\n",
      "Epoch 229, Loss: 0.37867173459380865, Final Batch Loss: 0.0354386605322361\n",
      "Epoch 230, Loss: 0.34856665041297674, Final Batch Loss: 0.03719938546419144\n",
      "Epoch 231, Loss: 0.33633842412382364, Final Batch Loss: 0.01575358770787716\n",
      "Epoch 232, Loss: 0.3308615107089281, Final Batch Loss: 0.02738410048186779\n",
      "Epoch 233, Loss: 0.4523741761222482, Final Batch Loss: 0.03625209629535675\n",
      "Epoch 234, Loss: 0.3891340550035238, Final Batch Loss: 0.04062829539179802\n",
      "Epoch 235, Loss: 0.3553395811468363, Final Batch Loss: 0.03677539527416229\n",
      "Epoch 236, Loss: 0.2867755671031773, Final Batch Loss: 0.011795802041888237\n",
      "Epoch 237, Loss: 0.31142577482387424, Final Batch Loss: 0.026131780818104744\n",
      "Epoch 238, Loss: 0.36007324885576963, Final Batch Loss: 0.013607997447252274\n",
      "Epoch 239, Loss: 0.3962534321472049, Final Batch Loss: 0.018891435116529465\n",
      "Epoch 240, Loss: 0.3742097467184067, Final Batch Loss: 0.019685709848999977\n",
      "Epoch 241, Loss: 0.38614162243902683, Final Batch Loss: 0.014282915741205215\n",
      "Epoch 242, Loss: 0.3492765724658966, Final Batch Loss: 0.016150711104273796\n",
      "Epoch 243, Loss: 0.43788887560367584, Final Batch Loss: 0.04456949234008789\n",
      "Epoch 244, Loss: 0.4368145167827606, Final Batch Loss: 0.026484472677111626\n",
      "Epoch 245, Loss: 0.4507450172677636, Final Batch Loss: 0.013963227160274982\n",
      "Epoch 246, Loss: 0.44758465327322483, Final Batch Loss: 0.02972705103456974\n",
      "Epoch 247, Loss: 0.36266345297917724, Final Batch Loss: 0.014477227814495564\n",
      "Epoch 248, Loss: 0.3498331690207124, Final Batch Loss: 0.01327587477862835\n",
      "Epoch 249, Loss: 0.4273664429783821, Final Batch Loss: 0.053244300186634064\n",
      "Epoch 250, Loss: 0.2958425907418132, Final Batch Loss: 0.030159203335642815\n",
      "Epoch 251, Loss: 0.33696401910856366, Final Batch Loss: 0.04040975496172905\n",
      "Epoch 252, Loss: 0.35644002305343747, Final Batch Loss: 0.03295166790485382\n",
      "Epoch 253, Loss: 0.3391358917579055, Final Batch Loss: 0.02768816240131855\n",
      "Epoch 254, Loss: 0.2875499716028571, Final Batch Loss: 0.007426011376082897\n",
      "Epoch 255, Loss: 0.3707701903767884, Final Batch Loss: 0.004724831786006689\n",
      "Epoch 256, Loss: 0.2863665563054383, Final Batch Loss: 0.008445636369287968\n",
      "Epoch 257, Loss: 0.4082318712025881, Final Batch Loss: 0.0506146103143692\n",
      "Epoch 258, Loss: 0.3393727019429207, Final Batch Loss: 0.011550375260412693\n",
      "Epoch 259, Loss: 0.32002200465649366, Final Batch Loss: 0.014908097684383392\n",
      "Epoch 260, Loss: 0.27591794775798917, Final Batch Loss: 0.01152912899851799\n",
      "Epoch 261, Loss: 0.31096924119628966, Final Batch Loss: 0.0026895555201917887\n",
      "Epoch 262, Loss: 0.2899324265308678, Final Batch Loss: 0.02035093680024147\n",
      "Epoch 263, Loss: 0.30333756748586893, Final Batch Loss: 0.028729796409606934\n",
      "Epoch 264, Loss: 0.3399994792416692, Final Batch Loss: 0.022575609385967255\n",
      "Epoch 265, Loss: 0.34781299671158195, Final Batch Loss: 0.06809570640325546\n",
      "Epoch 266, Loss: 0.3175770281814039, Final Batch Loss: 0.006032630801200867\n",
      "Epoch 267, Loss: 0.3002896225079894, Final Batch Loss: 0.022978533059358597\n",
      "Epoch 268, Loss: 0.3487972407601774, Final Batch Loss: 0.03253554180264473\n",
      "Epoch 269, Loss: 0.28648423636332154, Final Batch Loss: 0.0377529114484787\n",
      "Epoch 270, Loss: 0.32682755729183555, Final Batch Loss: 0.007151823025196791\n",
      "Epoch 271, Loss: 0.32134858099743724, Final Batch Loss: 0.02352091856300831\n",
      "Epoch 272, Loss: 0.3086216622032225, Final Batch Loss: 0.02432512864470482\n",
      "Epoch 273, Loss: 0.30541198793798685, Final Batch Loss: 0.01193622313439846\n",
      "Epoch 274, Loss: 0.29087860556319356, Final Batch Loss: 0.03413550928235054\n",
      "Epoch 275, Loss: 0.3189640655182302, Final Batch Loss: 0.0065734549425542355\n",
      "Epoch 276, Loss: 0.31536453729495406, Final Batch Loss: 0.043337732553482056\n",
      "Epoch 277, Loss: 0.2902330672368407, Final Batch Loss: 0.05998687073588371\n",
      "Epoch 278, Loss: 0.2818527282215655, Final Batch Loss: 0.019319480285048485\n",
      "Epoch 279, Loss: 0.28531228145584464, Final Batch Loss: 0.010811304673552513\n",
      "Epoch 280, Loss: 0.30517905578017235, Final Batch Loss: 0.04062194749712944\n",
      "Epoch 281, Loss: 0.2873502243310213, Final Batch Loss: 0.0337112694978714\n",
      "Epoch 282, Loss: 0.2982038534246385, Final Batch Loss: 0.01878293603658676\n",
      "Epoch 283, Loss: 0.3045396385714412, Final Batch Loss: 0.014417136088013649\n",
      "Epoch 284, Loss: 0.25870552263222635, Final Batch Loss: 0.011178463697433472\n",
      "Epoch 285, Loss: 0.30468421895056963, Final Batch Loss: 0.007751014083623886\n",
      "Epoch 286, Loss: 0.2967211529612541, Final Batch Loss: 0.03241266310214996\n",
      "Epoch 287, Loss: 0.2964339475147426, Final Batch Loss: 0.03373336046934128\n",
      "Epoch 288, Loss: 0.25893942220136523, Final Batch Loss: 0.011444183997809887\n",
      "Epoch 289, Loss: 0.3194455113261938, Final Batch Loss: 0.02897948957979679\n",
      "Epoch 290, Loss: 0.26868321979418397, Final Batch Loss: 0.006942043546587229\n",
      "Epoch 291, Loss: 0.28889952693134546, Final Batch Loss: 0.013310318812727928\n",
      "Epoch 292, Loss: 0.2612548586912453, Final Batch Loss: 0.023039797320961952\n",
      "Epoch 293, Loss: 0.26854015700519085, Final Batch Loss: 0.017520098015666008\n",
      "Epoch 294, Loss: 0.3703081086277962, Final Batch Loss: 0.027085063979029655\n",
      "Epoch 295, Loss: 0.3264479166828096, Final Batch Loss: 0.010098733007907867\n",
      "Epoch 296, Loss: 0.26432490767911077, Final Batch Loss: 0.03384013473987579\n",
      "Epoch 297, Loss: 0.3398668263107538, Final Batch Loss: 0.0101606585085392\n",
      "Epoch 298, Loss: 0.30900950147770345, Final Batch Loss: 0.00945157092064619\n",
      "Epoch 299, Loss: 0.3175518582575023, Final Batch Loss: 0.023697257041931152\n",
      "Epoch 300, Loss: 0.27529086149297655, Final Batch Loss: 0.008237965404987335\n",
      "Epoch 301, Loss: 0.22823419282212853, Final Batch Loss: 0.019055083394050598\n",
      "Epoch 302, Loss: 0.29729441134259105, Final Batch Loss: 0.013065262697637081\n",
      "Epoch 303, Loss: 0.30195634020492435, Final Batch Loss: 0.06452125310897827\n",
      "Epoch 304, Loss: 0.2652097805403173, Final Batch Loss: 0.03616737574338913\n",
      "Epoch 305, Loss: 0.24546601390466094, Final Batch Loss: 0.03468392416834831\n",
      "Epoch 306, Loss: 0.23967272136360407, Final Batch Loss: 0.014922555536031723\n",
      "Epoch 307, Loss: 0.2832058952189982, Final Batch Loss: 0.0076261600479483604\n",
      "Epoch 308, Loss: 0.29101995658129454, Final Batch Loss: 0.027696892619132996\n",
      "Epoch 309, Loss: 0.27408170886337757, Final Batch Loss: 0.014115653932094574\n",
      "Epoch 310, Loss: 0.27413703966885805, Final Batch Loss: 0.034473638981580734\n",
      "Epoch 311, Loss: 0.3277107570320368, Final Batch Loss: 0.05188310891389847\n",
      "Epoch 312, Loss: 0.28912258648779243, Final Batch Loss: 0.03115721605718136\n",
      "Epoch 313, Loss: 0.31349338917061687, Final Batch Loss: 0.025133047252893448\n",
      "Epoch 314, Loss: 0.2754871407523751, Final Batch Loss: 0.02591979317367077\n",
      "Epoch 315, Loss: 0.2490103670861572, Final Batch Loss: 0.012087632901966572\n",
      "Epoch 316, Loss: 0.30953595926985145, Final Batch Loss: 0.016808252781629562\n",
      "Epoch 317, Loss: 0.2331876028329134, Final Batch Loss: 0.007324667647480965\n",
      "Epoch 318, Loss: 0.21234695566818118, Final Batch Loss: 0.009165491908788681\n",
      "Epoch 319, Loss: 0.25008619064465165, Final Batch Loss: 0.016585221514105797\n",
      "Epoch 320, Loss: 0.25229237135499716, Final Batch Loss: 0.011423655785620213\n",
      "Epoch 321, Loss: 0.2641100902110338, Final Batch Loss: 0.027088675647974014\n",
      "Epoch 322, Loss: 0.22650549607351422, Final Batch Loss: 0.004365591332316399\n",
      "Epoch 323, Loss: 0.23098381678573787, Final Batch Loss: 0.002036935882642865\n",
      "Epoch 324, Loss: 0.23827766790054739, Final Batch Loss: 0.015950363129377365\n",
      "Epoch 325, Loss: 0.25176118966192007, Final Batch Loss: 0.02343606762588024\n",
      "Epoch 326, Loss: 0.30094742914661765, Final Batch Loss: 0.01843409240245819\n",
      "Epoch 327, Loss: 0.2508938889950514, Final Batch Loss: 0.022585554048419\n",
      "Epoch 328, Loss: 0.32374209142290056, Final Batch Loss: 0.0516853854060173\n",
      "Epoch 329, Loss: 0.308063936419785, Final Batch Loss: 0.018386242911219597\n",
      "Epoch 330, Loss: 0.27687367983162403, Final Batch Loss: 0.0043518515303730965\n",
      "Epoch 331, Loss: 0.24878589576110244, Final Batch Loss: 0.010837431065738201\n",
      "Epoch 332, Loss: 0.31867976672947407, Final Batch Loss: 0.021853329613804817\n",
      "Epoch 333, Loss: 0.2726271082647145, Final Batch Loss: 0.009312947280704975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334, Loss: 0.2752971462905407, Final Batch Loss: 0.018069179728627205\n",
      "Epoch 335, Loss: 0.28922103508375585, Final Batch Loss: 0.026564665138721466\n",
      "Epoch 336, Loss: 0.19688929803669453, Final Batch Loss: 0.007434515748172998\n",
      "Epoch 337, Loss: 0.22667224425822496, Final Batch Loss: 0.006237865891307592\n",
      "Epoch 338, Loss: 0.2825573771260679, Final Batch Loss: 0.03125246986746788\n",
      "Epoch 339, Loss: 0.23578450176864862, Final Batch Loss: 0.02594168670475483\n",
      "Epoch 340, Loss: 0.20376799535006285, Final Batch Loss: 0.003944967407733202\n",
      "Epoch 341, Loss: 0.3289576633833349, Final Batch Loss: 0.03259928151965141\n",
      "Epoch 342, Loss: 0.27501583890989423, Final Batch Loss: 0.013008306734263897\n",
      "Epoch 343, Loss: 0.25063737854361534, Final Batch Loss: 0.007521327584981918\n",
      "Epoch 344, Loss: 0.2476529753766954, Final Batch Loss: 0.008993634954094887\n",
      "Epoch 345, Loss: 0.22956378478556871, Final Batch Loss: 0.018870139494538307\n",
      "Epoch 346, Loss: 0.24664456094615161, Final Batch Loss: 0.01015126146376133\n",
      "Epoch 347, Loss: 0.20548682054504752, Final Batch Loss: 0.025626366958022118\n",
      "Epoch 348, Loss: 0.2503882674500346, Final Batch Loss: 0.008162998594343662\n",
      "Epoch 349, Loss: 0.2865196692291647, Final Batch Loss: 0.025957772508263588\n",
      "Epoch 350, Loss: 0.25241966755129397, Final Batch Loss: 0.006856787949800491\n",
      "Epoch 351, Loss: 0.2073080614209175, Final Batch Loss: 0.017800183966755867\n",
      "Epoch 352, Loss: 0.25304737919941545, Final Batch Loss: 0.010350431315600872\n",
      "Epoch 353, Loss: 0.2223550626076758, Final Batch Loss: 0.03583637997508049\n",
      "Epoch 354, Loss: 0.22709697531536222, Final Batch Loss: 0.01735761947929859\n",
      "Epoch 355, Loss: 0.1988962860777974, Final Batch Loss: 0.011158274486660957\n",
      "Epoch 356, Loss: 0.1915364417945966, Final Batch Loss: 0.04195920377969742\n",
      "Epoch 357, Loss: 0.2858562981709838, Final Batch Loss: 0.010262691415846348\n",
      "Epoch 358, Loss: 0.2042231229133904, Final Batch Loss: 0.016776584088802338\n",
      "Epoch 359, Loss: 0.21960144257172942, Final Batch Loss: 0.006439701188355684\n",
      "Epoch 360, Loss: 0.18375039170496166, Final Batch Loss: 0.009711168706417084\n",
      "Epoch 361, Loss: 0.1898475829511881, Final Batch Loss: 0.005168470088392496\n",
      "Epoch 362, Loss: 0.20613793097436428, Final Batch Loss: 0.006520028691738844\n",
      "Epoch 363, Loss: 0.2670955832581967, Final Batch Loss: 0.01837475411593914\n",
      "Epoch 364, Loss: 0.20877522230148315, Final Batch Loss: 0.029788604006171227\n",
      "Epoch 365, Loss: 0.21409450646024197, Final Batch Loss: 0.008852782659232616\n",
      "Epoch 366, Loss: 0.2167356009595096, Final Batch Loss: 0.027515670284628868\n",
      "Epoch 367, Loss: 0.15985092776827514, Final Batch Loss: 0.003708969336003065\n",
      "Epoch 368, Loss: 0.24685454787686467, Final Batch Loss: 0.020892120897769928\n",
      "Epoch 369, Loss: 0.21256497036665678, Final Batch Loss: 0.004597717430442572\n",
      "Epoch 370, Loss: 0.21210581227205694, Final Batch Loss: 0.024913672357797623\n",
      "Epoch 371, Loss: 0.2214537423569709, Final Batch Loss: 0.010027129203081131\n",
      "Epoch 372, Loss: 0.20547514781355858, Final Batch Loss: 0.01134833786636591\n",
      "Epoch 373, Loss: 0.2564328978769481, Final Batch Loss: 0.021752463653683662\n",
      "Epoch 374, Loss: 0.23528056824579835, Final Batch Loss: 0.004860152490437031\n",
      "Epoch 375, Loss: 0.242497913306579, Final Batch Loss: 0.01646471582353115\n",
      "Epoch 376, Loss: 0.1684101044666022, Final Batch Loss: 0.006892115343362093\n",
      "Epoch 377, Loss: 0.16663237451575696, Final Batch Loss: 0.006998683791607618\n",
      "Epoch 378, Loss: 0.19601682072971016, Final Batch Loss: 0.01421578973531723\n",
      "Epoch 379, Loss: 0.20088489074259996, Final Batch Loss: 0.003592906752601266\n",
      "Epoch 380, Loss: 0.22779268980957568, Final Batch Loss: 0.02179287187755108\n",
      "Epoch 381, Loss: 0.19071029033511877, Final Batch Loss: 0.006740736309438944\n",
      "Epoch 382, Loss: 0.15793971228413284, Final Batch Loss: 0.02039853110909462\n",
      "Epoch 383, Loss: 0.19602969964034855, Final Batch Loss: 0.008878076449036598\n",
      "Epoch 384, Loss: 0.19658668898046017, Final Batch Loss: 0.005468173883855343\n",
      "Epoch 385, Loss: 0.16604527435265481, Final Batch Loss: 0.0034502309281378984\n",
      "Epoch 386, Loss: 0.20016164821572602, Final Batch Loss: 0.021030623465776443\n",
      "Epoch 387, Loss: 0.24430690868757665, Final Batch Loss: 0.020008957013487816\n",
      "Epoch 388, Loss: 0.19970870879478753, Final Batch Loss: 0.015972858294844627\n",
      "Epoch 389, Loss: 0.20106126251630485, Final Batch Loss: 0.023672496899962425\n",
      "Epoch 390, Loss: 0.2001633131876588, Final Batch Loss: 0.009140979498624802\n",
      "Epoch 391, Loss: 0.19347586762160063, Final Batch Loss: 0.021114937961101532\n",
      "Epoch 392, Loss: 0.17608179803937674, Final Batch Loss: 0.020197000354528427\n",
      "Epoch 393, Loss: 0.17912999563850462, Final Batch Loss: 0.02531249448657036\n",
      "Epoch 394, Loss: 0.16571784298866987, Final Batch Loss: 0.0072575779631733894\n",
      "Epoch 395, Loss: 0.20563683938235044, Final Batch Loss: 0.00989479012787342\n",
      "Epoch 396, Loss: 0.13968027546070516, Final Batch Loss: 0.014651634730398655\n",
      "Epoch 397, Loss: 0.2066018267069012, Final Batch Loss: 0.018088242039084435\n",
      "Epoch 398, Loss: 0.15296559478156269, Final Batch Loss: 0.009313119575381279\n",
      "Epoch 399, Loss: 0.21532685752026737, Final Batch Loss: 0.03358210623264313\n",
      "Epoch 400, Loss: 0.18328431900590658, Final Batch Loss: 0.005980913992971182\n",
      "Epoch 401, Loss: 0.1227308651432395, Final Batch Loss: 0.015199191868305206\n",
      "Epoch 402, Loss: 0.15802802168764174, Final Batch Loss: 0.005507859867066145\n",
      "Epoch 403, Loss: 0.157699859701097, Final Batch Loss: 0.014506511390209198\n",
      "Epoch 404, Loss: 0.16655063536018133, Final Batch Loss: 0.0053750076331198215\n",
      "Epoch 405, Loss: 0.1658414602279663, Final Batch Loss: 0.010595747269690037\n",
      "Epoch 406, Loss: 0.22459326731041074, Final Batch Loss: 0.008322418667376041\n",
      "Epoch 407, Loss: 0.2614700247067958, Final Batch Loss: 0.015655579045414925\n",
      "Epoch 408, Loss: 0.1940822135657072, Final Batch Loss: 0.01064436323940754\n",
      "Epoch 409, Loss: 0.19396858965046704, Final Batch Loss: 0.006292824167758226\n",
      "Epoch 410, Loss: 0.1554223537677899, Final Batch Loss: 0.0074404203332960606\n",
      "Epoch 411, Loss: 0.16441233223304152, Final Batch Loss: 0.012716216035187244\n",
      "Epoch 412, Loss: 0.1689251835923642, Final Batch Loss: 0.0037302158307284117\n",
      "Epoch 413, Loss: 0.15799001418054104, Final Batch Loss: 0.005055641755461693\n",
      "Epoch 414, Loss: 0.1890865167370066, Final Batch Loss: 0.008162116631865501\n",
      "Epoch 415, Loss: 0.1836089575663209, Final Batch Loss: 0.010251719504594803\n",
      "Epoch 416, Loss: 0.19479929120279849, Final Batch Loss: 0.0034825943876057863\n",
      "Epoch 417, Loss: 0.15660327509976923, Final Batch Loss: 0.010921585373580456\n",
      "Epoch 418, Loss: 0.15101692522875965, Final Batch Loss: 0.008231420069932938\n",
      "Epoch 419, Loss: 0.14972592890262604, Final Batch Loss: 0.010112868621945381\n",
      "Epoch 420, Loss: 0.2083286631386727, Final Batch Loss: 0.012229138053953648\n",
      "Epoch 421, Loss: 0.15351345692761242, Final Batch Loss: 0.00839158520102501\n",
      "Epoch 422, Loss: 0.20582487399224192, Final Batch Loss: 0.007071861531585455\n",
      "Epoch 423, Loss: 0.11936885607428849, Final Batch Loss: 0.008331592194736004\n",
      "Epoch 424, Loss: 0.18127447785809636, Final Batch Loss: 0.014864555560052395\n",
      "Epoch 425, Loss: 0.1736194574041292, Final Batch Loss: 0.006437357980757952\n",
      "Epoch 426, Loss: 0.16805243585258722, Final Batch Loss: 0.0072463802061975\n",
      "Epoch 427, Loss: 0.14592118433211, Final Batch Loss: 0.0006841444410383701\n",
      "Epoch 428, Loss: 0.1619737723376602, Final Batch Loss: 0.019132113084197044\n",
      "Epoch 429, Loss: 0.19977962714619935, Final Batch Loss: 0.0070753335021436214\n",
      "Epoch 430, Loss: 0.17782179883215576, Final Batch Loss: 0.017709825187921524\n",
      "Epoch 431, Loss: 0.17252008244395256, Final Batch Loss: 0.01807510480284691\n",
      "Epoch 432, Loss: 0.14786709554027766, Final Batch Loss: 0.015020414255559444\n",
      "Epoch 433, Loss: 0.18115503597073257, Final Batch Loss: 0.030116135254502296\n",
      "Epoch 434, Loss: 0.19264906720491126, Final Batch Loss: 0.020211350172758102\n",
      "Epoch 435, Loss: 0.18772862711921334, Final Batch Loss: 0.030732396990060806\n",
      "Epoch 436, Loss: 0.1925120297819376, Final Batch Loss: 0.022846581414341927\n",
      "Epoch 437, Loss: 0.19359611184336245, Final Batch Loss: 0.02607801742851734\n",
      "Epoch 438, Loss: 0.20284244394861162, Final Batch Loss: 0.012494504451751709\n",
      "Epoch 439, Loss: 0.18531194515526295, Final Batch Loss: 0.006568179000169039\n",
      "Epoch 440, Loss: 0.1392495078034699, Final Batch Loss: 0.013364276848733425\n",
      "Epoch 441, Loss: 0.21939853380899876, Final Batch Loss: 0.02137848362326622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442, Loss: 0.16594321536831558, Final Batch Loss: 0.01076118741184473\n",
      "Epoch 443, Loss: 0.15550936828367412, Final Batch Loss: 0.019616059958934784\n",
      "Epoch 444, Loss: 0.14655143325217068, Final Batch Loss: 0.0036216676235198975\n",
      "Epoch 445, Loss: 0.1682801895076409, Final Batch Loss: 0.015491495840251446\n",
      "Epoch 446, Loss: 0.1456345629412681, Final Batch Loss: 0.023507723584771156\n",
      "Epoch 447, Loss: 0.12786860077176243, Final Batch Loss: 0.005154394078999758\n",
      "Epoch 448, Loss: 0.18636222602799535, Final Batch Loss: 0.019533514976501465\n",
      "Epoch 449, Loss: 0.1431477958103642, Final Batch Loss: 0.001992697361856699\n",
      "Epoch 450, Loss: 0.20479339372832328, Final Batch Loss: 0.021840129047632217\n",
      "Epoch 451, Loss: 0.1381737650372088, Final Batch Loss: 0.024462217465043068\n",
      "Epoch 452, Loss: 0.16926000406965613, Final Batch Loss: 0.01307404413819313\n",
      "Epoch 453, Loss: 0.1627341469284147, Final Batch Loss: 0.0033087909687310457\n",
      "Epoch 454, Loss: 0.1708221547305584, Final Batch Loss: 0.0030200560577213764\n",
      "Epoch 455, Loss: 0.1789371846243739, Final Batch Loss: 0.022267933934926987\n",
      "Epoch 456, Loss: 0.09954794822260737, Final Batch Loss: 0.0037269219756126404\n",
      "Epoch 457, Loss: 0.144110445282422, Final Batch Loss: 0.04065099358558655\n",
      "Epoch 458, Loss: 0.12726293387822807, Final Batch Loss: 0.004821621812880039\n",
      "Epoch 459, Loss: 0.12598008767236024, Final Batch Loss: 0.0042083472944796085\n",
      "Epoch 460, Loss: 0.16847363754641265, Final Batch Loss: 0.01645239256322384\n",
      "Epoch 461, Loss: 0.13646567659452558, Final Batch Loss: 0.009173963218927383\n",
      "Epoch 462, Loss: 0.14404264756012708, Final Batch Loss: 0.012268668040633202\n",
      "Epoch 463, Loss: 0.13129958556964993, Final Batch Loss: 0.006785138975828886\n",
      "Epoch 464, Loss: 0.14591629104688764, Final Batch Loss: 0.005413516890257597\n",
      "Epoch 465, Loss: 0.19358736684080213, Final Batch Loss: 0.014510060660541058\n",
      "Epoch 466, Loss: 0.21909136604517698, Final Batch Loss: 0.013000953011214733\n",
      "Epoch 467, Loss: 0.12543914280831814, Final Batch Loss: 0.039120472967624664\n",
      "Epoch 468, Loss: 0.16311453375965357, Final Batch Loss: 0.0027504011522978544\n",
      "Epoch 469, Loss: 0.11314743175171316, Final Batch Loss: 0.012099857442080975\n",
      "Epoch 470, Loss: 0.14189575740601867, Final Batch Loss: 0.031010007485747337\n",
      "Epoch 471, Loss: 0.18764016404747963, Final Batch Loss: 0.03703693673014641\n",
      "Epoch 472, Loss: 0.11871380574302748, Final Batch Loss: 0.000927928660530597\n",
      "Epoch 473, Loss: 0.13625645684078336, Final Batch Loss: 0.011348018422722816\n",
      "Epoch 474, Loss: 0.15725283045321703, Final Batch Loss: 0.016743037849664688\n",
      "Epoch 475, Loss: 0.11932398797944188, Final Batch Loss: 0.0027091007214039564\n",
      "Epoch 476, Loss: 0.16389216529205441, Final Batch Loss: 0.004543667659163475\n",
      "Epoch 477, Loss: 0.10492759710177779, Final Batch Loss: 0.007671387866139412\n",
      "Epoch 478, Loss: 0.1541517243022099, Final Batch Loss: 0.0029771390836685896\n",
      "Epoch 479, Loss: 0.15343645412940532, Final Batch Loss: 0.013290383853018284\n",
      "Epoch 480, Loss: 0.117811628151685, Final Batch Loss: 0.01608966290950775\n",
      "Epoch 481, Loss: 0.15213289787061512, Final Batch Loss: 0.0010700131533667445\n",
      "Epoch 482, Loss: 0.13397722225636244, Final Batch Loss: 0.0025146957486867905\n",
      "Epoch 483, Loss: 0.11306707514449954, Final Batch Loss: 0.015003625303506851\n",
      "Epoch 484, Loss: 0.13401608797721565, Final Batch Loss: 0.002716245362535119\n",
      "Epoch 485, Loss: 0.1536857821047306, Final Batch Loss: 0.010742096230387688\n",
      "Epoch 486, Loss: 0.13971063285134733, Final Batch Loss: 0.0033263424411416054\n",
      "Epoch 487, Loss: 0.14758465555496514, Final Batch Loss: 0.005190529394894838\n",
      "Epoch 488, Loss: 0.17017178726382554, Final Batch Loss: 0.006287289317697287\n",
      "Epoch 489, Loss: 0.12904751987662166, Final Batch Loss: 0.00813313852995634\n",
      "Epoch 490, Loss: 0.13975122955162078, Final Batch Loss: 0.013423902913928032\n",
      "Epoch 491, Loss: 0.15502525214105844, Final Batch Loss: 0.018187591806054115\n",
      "Epoch 492, Loss: 0.28037944447714835, Final Batch Loss: 0.00444504851475358\n",
      "Epoch 493, Loss: 0.14270104584284127, Final Batch Loss: 0.00936209037899971\n",
      "Epoch 494, Loss: 0.15122154541313648, Final Batch Loss: 0.015207333490252495\n",
      "Epoch 495, Loss: 0.16782772121950984, Final Batch Loss: 0.0026652920059859753\n",
      "Epoch 496, Loss: 0.13979580905288458, Final Batch Loss: 0.005161720793694258\n",
      "Epoch 497, Loss: 0.13359451084397733, Final Batch Loss: 0.0014772875001654029\n",
      "Epoch 498, Loss: 0.0882066625636071, Final Batch Loss: 0.006347172427922487\n",
      "Epoch 499, Loss: 0.13518830010434613, Final Batch Loss: 0.01682678982615471\n",
      "Epoch 500, Loss: 0.13347020780202, Final Batch Loss: 0.008363482542335987\n",
      "Epoch 501, Loss: 0.10138325626030564, Final Batch Loss: 0.0007970449514687061\n",
      "Epoch 502, Loss: 0.19194462185259908, Final Batch Loss: 0.01475169975310564\n",
      "Epoch 503, Loss: 0.15875883761327714, Final Batch Loss: 0.016599474474787712\n",
      "Epoch 504, Loss: 0.11576814553700387, Final Batch Loss: 0.0022841368336230516\n",
      "Epoch 505, Loss: 0.1289336204645224, Final Batch Loss: 0.007430719211697578\n",
      "Epoch 506, Loss: 0.10191287100315094, Final Batch Loss: 0.008237434551119804\n",
      "Epoch 507, Loss: 0.09877214283915237, Final Batch Loss: 0.011108051054179668\n",
      "Epoch 508, Loss: 0.12421615282073617, Final Batch Loss: 0.015472895465791225\n",
      "Epoch 509, Loss: 0.12363605247810483, Final Batch Loss: 0.012985511682927608\n",
      "Epoch 510, Loss: 0.13039100356400013, Final Batch Loss: 0.009873810224235058\n",
      "Epoch 511, Loss: 0.16197302733780816, Final Batch Loss: 0.0025117257609963417\n",
      "Epoch 512, Loss: 0.15702555811731145, Final Batch Loss: 0.03081287071108818\n",
      "Epoch 513, Loss: 0.10946213896386325, Final Batch Loss: 0.0017591603100299835\n",
      "Epoch 514, Loss: 0.1387038677930832, Final Batch Loss: 0.005206967703998089\n",
      "Epoch 515, Loss: 0.12344074551947415, Final Batch Loss: 0.007709111087024212\n",
      "Epoch 516, Loss: 0.13844920240808278, Final Batch Loss: 0.02271079830825329\n",
      "Epoch 517, Loss: 0.13219285872764885, Final Batch Loss: 0.010159495286643505\n",
      "Epoch 518, Loss: 0.09707874886225909, Final Batch Loss: 0.0012955993879586458\n",
      "Epoch 519, Loss: 0.1029261841904372, Final Batch Loss: 0.0013238186948001385\n",
      "Epoch 520, Loss: 0.1213888778584078, Final Batch Loss: 0.003635644679889083\n",
      "Epoch 521, Loss: 0.14207608974538743, Final Batch Loss: 0.0039788647554814816\n",
      "Epoch 522, Loss: 0.24326016556005925, Final Batch Loss: 0.02004498802125454\n",
      "Epoch 523, Loss: 0.22166264825500548, Final Batch Loss: 0.008872739039361477\n",
      "Epoch 524, Loss: 0.2053598507773131, Final Batch Loss: 0.0135294608771801\n",
      "Epoch 525, Loss: 0.20888698543421924, Final Batch Loss: 0.005814704112708569\n",
      "Epoch 526, Loss: 0.1375319667859003, Final Batch Loss: 0.022397106513381004\n",
      "Epoch 527, Loss: 0.1465974000748247, Final Batch Loss: 0.003952362108975649\n",
      "Epoch 528, Loss: 0.09864299790933728, Final Batch Loss: 0.005109461955726147\n",
      "Epoch 529, Loss: 0.10434666136279702, Final Batch Loss: 0.0020975936204195023\n",
      "Epoch 530, Loss: 0.17481031082570553, Final Batch Loss: 0.01748192124068737\n",
      "Epoch 531, Loss: 0.0957748331129551, Final Batch Loss: 0.0038868882693350315\n",
      "Epoch 532, Loss: 0.11668375437147915, Final Batch Loss: 0.02853999473154545\n",
      "Epoch 533, Loss: 0.15383880771696568, Final Batch Loss: 0.006198590621352196\n",
      "Epoch 534, Loss: 0.09479359502438456, Final Batch Loss: 0.005967140197753906\n",
      "Epoch 535, Loss: 0.10368142905645072, Final Batch Loss: 0.002772251144051552\n",
      "Epoch 536, Loss: 0.13615873851813376, Final Batch Loss: 0.0037877007853239775\n",
      "Epoch 537, Loss: 0.14053862099535763, Final Batch Loss: 0.006713277194648981\n",
      "Epoch 538, Loss: 0.11302052257815376, Final Batch Loss: 0.014620200730860233\n",
      "Epoch 539, Loss: 0.09833957615774125, Final Batch Loss: 0.017416080459952354\n",
      "Epoch 540, Loss: 0.13648726209066808, Final Batch Loss: 0.004807746037840843\n",
      "Epoch 541, Loss: 0.08774438791442662, Final Batch Loss: 0.006475495640188456\n",
      "Epoch 542, Loss: 0.07201439084019512, Final Batch Loss: 0.0008441621903330088\n",
      "Epoch 543, Loss: 0.12435295735485852, Final Batch Loss: 0.0028283928986638784\n",
      "Epoch 544, Loss: 0.08394868316827342, Final Batch Loss: 0.01625313051044941\n",
      "Epoch 545, Loss: 0.08842681092210114, Final Batch Loss: 0.0038920133374631405\n",
      "Epoch 546, Loss: 0.078781278047245, Final Batch Loss: 0.0014627825003117323\n",
      "Epoch 547, Loss: 0.0728444330743514, Final Batch Loss: 0.00248991628177464\n",
      "Epoch 548, Loss: 0.09170645684935153, Final Batch Loss: 0.0015014703385531902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 549, Loss: 0.13244976493297145, Final Batch Loss: 0.012414439581334591\n",
      "Epoch 550, Loss: 0.10412844328675419, Final Batch Loss: 0.0022656205110251904\n",
      "Epoch 551, Loss: 0.07132919155992568, Final Batch Loss: 0.00131330662406981\n",
      "Epoch 552, Loss: 0.15527109516551718, Final Batch Loss: 0.04508652910590172\n",
      "Epoch 553, Loss: 0.15584124205633998, Final Batch Loss: 0.03297945484519005\n",
      "Epoch 554, Loss: 0.1838402898865752, Final Batch Loss: 0.009590797126293182\n",
      "Epoch 555, Loss: 0.09861295658629388, Final Batch Loss: 0.027375269681215286\n",
      "Epoch 556, Loss: 0.0991155457450077, Final Batch Loss: 0.011927014216780663\n",
      "Epoch 557, Loss: 0.08593152218963951, Final Batch Loss: 0.009809932671487331\n",
      "Epoch 558, Loss: 0.13448890368454158, Final Batch Loss: 0.0046937367878854275\n",
      "Epoch 559, Loss: 0.14330698223784566, Final Batch Loss: 0.0060778651386499405\n",
      "Epoch 560, Loss: 0.08805978036252782, Final Batch Loss: 0.0023424471728503704\n",
      "Epoch 561, Loss: 0.15718622982967645, Final Batch Loss: 0.0012239768402650952\n",
      "Epoch 562, Loss: 0.0968574169673957, Final Batch Loss: 0.003800094360485673\n",
      "Epoch 563, Loss: 0.08942160004517063, Final Batch Loss: 0.00492852134630084\n",
      "Epoch 564, Loss: 0.12621721951290965, Final Batch Loss: 0.011257912032306194\n",
      "Epoch 565, Loss: 0.1289917856338434, Final Batch Loss: 0.0008593734237365425\n",
      "Epoch 566, Loss: 0.24400058924220502, Final Batch Loss: 0.0017756664892658591\n",
      "Epoch 567, Loss: 0.14371348451822996, Final Batch Loss: 0.024777088314294815\n",
      "Epoch 568, Loss: 0.19693044672021642, Final Batch Loss: 0.010156866163015366\n",
      "Epoch 569, Loss: 0.13802437245612964, Final Batch Loss: 0.008231009356677532\n",
      "Epoch 570, Loss: 0.10575349279679358, Final Batch Loss: 0.0016168627189472318\n",
      "Epoch 571, Loss: 0.12490323686506599, Final Batch Loss: 0.04294135421514511\n",
      "Epoch 572, Loss: 0.10468152310932055, Final Batch Loss: 0.003657039487734437\n",
      "Epoch 573, Loss: 0.09480553097091615, Final Batch Loss: 0.007953592576086521\n",
      "Epoch 574, Loss: 0.07691854005679488, Final Batch Loss: 0.001601765281520784\n",
      "Epoch 575, Loss: 0.15073491935618222, Final Batch Loss: 0.004366167820990086\n",
      "Epoch 576, Loss: 0.0922925315098837, Final Batch Loss: 0.00817799475044012\n",
      "Epoch 577, Loss: 0.07745979935862124, Final Batch Loss: 0.011713520623743534\n",
      "Epoch 578, Loss: 0.17362424364546314, Final Batch Loss: 0.04298150911927223\n",
      "Epoch 579, Loss: 0.1034277479047887, Final Batch Loss: 0.014229485765099525\n",
      "Epoch 580, Loss: 0.053225826704874635, Final Batch Loss: 0.0023002249654382467\n",
      "Epoch 581, Loss: 0.10401508241193369, Final Batch Loss: 0.004757229704409838\n",
      "Epoch 582, Loss: 0.1381460021948442, Final Batch Loss: 0.019152961671352386\n",
      "Epoch 583, Loss: 0.11367687326855958, Final Batch Loss: 0.005115353502333164\n",
      "Epoch 584, Loss: 0.10894210613332689, Final Batch Loss: 0.03332670032978058\n",
      "Epoch 585, Loss: 0.14961225411389023, Final Batch Loss: 0.01171272061765194\n",
      "Epoch 586, Loss: 0.18151897855568677, Final Batch Loss: 0.0015483618481084704\n",
      "Epoch 587, Loss: 0.11624642158858478, Final Batch Loss: 0.015650823712348938\n",
      "Epoch 588, Loss: 0.1137403214816004, Final Batch Loss: 0.04236938804388046\n",
      "Epoch 589, Loss: 0.14705874671926722, Final Batch Loss: 0.005464527755975723\n",
      "Epoch 590, Loss: 0.06780730956234038, Final Batch Loss: 0.009531857445836067\n",
      "Epoch 591, Loss: 0.13939899671822786, Final Batch Loss: 0.00161168968770653\n",
      "Epoch 592, Loss: 0.13670023169834167, Final Batch Loss: 0.005840962287038565\n",
      "Epoch 593, Loss: 0.08125554851721972, Final Batch Loss: 0.003381231566891074\n",
      "Epoch 594, Loss: 0.20944672013865784, Final Batch Loss: 0.02622099034488201\n",
      "Epoch 595, Loss: 0.1484144366113469, Final Batch Loss: 0.001894699060358107\n",
      "Epoch 596, Loss: 0.09115941496565938, Final Batch Loss: 0.009373054839670658\n",
      "Epoch 597, Loss: 0.08090186218032613, Final Batch Loss: 0.011052301153540611\n",
      "Epoch 598, Loss: 0.0892727006576024, Final Batch Loss: 0.0010308207711204886\n",
      "Epoch 599, Loss: 0.08147790317889303, Final Batch Loss: 0.005331291817128658\n",
      "Epoch 600, Loss: 0.0955612322431989, Final Batch Loss: 0.009471048600971699\n",
      "Epoch 601, Loss: 0.1457224132027477, Final Batch Loss: 0.005033787805587053\n",
      "Epoch 602, Loss: 0.1610668555367738, Final Batch Loss: 0.029062418267130852\n",
      "Epoch 603, Loss: 0.07061730185523629, Final Batch Loss: 0.012779716402292252\n",
      "Epoch 604, Loss: 0.08980872866231948, Final Batch Loss: 0.00411924347281456\n",
      "Epoch 605, Loss: 0.15217682358343154, Final Batch Loss: 0.00459595350548625\n",
      "Epoch 606, Loss: 0.11601937422528863, Final Batch Loss: 0.0031865835189819336\n",
      "Epoch 607, Loss: 0.12311725108884275, Final Batch Loss: 0.008140329271554947\n",
      "Epoch 608, Loss: 0.10505355731584132, Final Batch Loss: 0.013304944150149822\n",
      "Epoch 609, Loss: 0.13086675049271435, Final Batch Loss: 0.01711236499249935\n",
      "Epoch 610, Loss: 0.17179844470229, Final Batch Loss: 0.007542283274233341\n",
      "Epoch 611, Loss: 0.07629451854154468, Final Batch Loss: 0.0070726266130805016\n",
      "Epoch 612, Loss: 0.07968264713417739, Final Batch Loss: 0.001048178062774241\n",
      "Epoch 613, Loss: 0.11447189608588815, Final Batch Loss: 0.006869591306895018\n",
      "Epoch 614, Loss: 0.11590298160444945, Final Batch Loss: 0.0011726949596777558\n",
      "Epoch 615, Loss: 0.1293222225503996, Final Batch Loss: 0.0014761937782168388\n",
      "Epoch 616, Loss: 0.14107118354877457, Final Batch Loss: 0.018851513043045998\n",
      "Epoch 617, Loss: 0.10920539044309407, Final Batch Loss: 0.0028720751870423555\n",
      "Epoch 618, Loss: 0.10222908656578511, Final Batch Loss: 0.002687999280169606\n",
      "Epoch 619, Loss: 0.0932231618789956, Final Batch Loss: 0.013683675788342953\n",
      "Epoch 620, Loss: 0.13199996872572228, Final Batch Loss: 0.0069823130033910275\n",
      "Epoch 621, Loss: 0.09624654971412383, Final Batch Loss: 0.0061823227442801\n",
      "Epoch 622, Loss: 0.09022874420043081, Final Batch Loss: 0.006023053079843521\n",
      "Epoch 623, Loss: 0.05995696323225275, Final Batch Loss: 0.0005352111184038222\n",
      "Epoch 624, Loss: 0.12888284152722917, Final Batch Loss: 0.017271731048822403\n",
      "Epoch 625, Loss: 0.1623932413640432, Final Batch Loss: 0.010755177587270737\n",
      "Epoch 626, Loss: 0.09467041841708124, Final Batch Loss: 0.0009656048496253788\n",
      "Epoch 627, Loss: 0.12698423932306468, Final Batch Loss: 0.04873180016875267\n",
      "Epoch 628, Loss: 0.11517308419570327, Final Batch Loss: 0.002148813335224986\n",
      "Epoch 629, Loss: 0.09534191503189504, Final Batch Loss: 0.035398125648498535\n",
      "Epoch 630, Loss: 0.09204219753155485, Final Batch Loss: 0.003051463980227709\n",
      "Epoch 631, Loss: 0.10075929167214781, Final Batch Loss: 0.006056905724108219\n",
      "Epoch 632, Loss: 0.08379880682332441, Final Batch Loss: 0.009665177203714848\n",
      "Epoch 633, Loss: 0.07655814406462014, Final Batch Loss: 0.0008183576865121722\n",
      "Epoch 634, Loss: 0.04060332750668749, Final Batch Loss: 0.002426189137622714\n",
      "Epoch 635, Loss: 0.25828398566227406, Final Batch Loss: 0.002200472867116332\n",
      "Epoch 636, Loss: 0.0827325306890998, Final Batch Loss: 0.01032842043787241\n",
      "Epoch 637, Loss: 0.07236761177773587, Final Batch Loss: 0.02761140652000904\n",
      "Epoch 638, Loss: 0.05691444082185626, Final Batch Loss: 0.0011945109581574798\n",
      "Epoch 639, Loss: 0.0803970877896063, Final Batch Loss: 0.011262032203376293\n",
      "Epoch 640, Loss: 0.14787358767352998, Final Batch Loss: 0.011892378330230713\n",
      "Epoch 641, Loss: 0.06830899714259431, Final Batch Loss: 0.001351712504401803\n",
      "Epoch 642, Loss: 0.17204000044148415, Final Batch Loss: 0.00187686737626791\n",
      "Epoch 643, Loss: 0.10007685329765081, Final Batch Loss: 0.007748753298074007\n",
      "Epoch 644, Loss: 0.11708322598133236, Final Batch Loss: 0.0034605059772729874\n",
      "Epoch 645, Loss: 0.12594684725627303, Final Batch Loss: 0.0010500848293304443\n",
      "Epoch 646, Loss: 0.10319435887504369, Final Batch Loss: 0.0008312257123179734\n",
      "Epoch 647, Loss: 0.08287972467951477, Final Batch Loss: 0.006176401861011982\n",
      "Epoch 648, Loss: 0.07738454709760845, Final Batch Loss: 0.0018295145127922297\n",
      "Epoch 649, Loss: 0.08960349456174299, Final Batch Loss: 0.01322986651211977\n",
      "Epoch 650, Loss: 0.11521122581325471, Final Batch Loss: 0.0023939386010169983\n",
      "Epoch 651, Loss: 0.13572069397196174, Final Batch Loss: 0.0035894932225346565\n",
      "Epoch 652, Loss: 0.15232889726758003, Final Batch Loss: 0.0013821219326928258\n",
      "Epoch 653, Loss: 0.08505119342589751, Final Batch Loss: 0.007313217036426067\n",
      "Epoch 654, Loss: 0.14810559758916497, Final Batch Loss: 0.0009070745436474681\n",
      "Epoch 655, Loss: 0.07531088090036064, Final Batch Loss: 0.015122446231544018\n",
      "Epoch 656, Loss: 0.055159470182843506, Final Batch Loss: 0.004561825189739466\n",
      "Epoch 657, Loss: 0.07289790798677132, Final Batch Loss: 0.0035399876069277525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658, Loss: 0.1006667417823337, Final Batch Loss: 0.015047862194478512\n",
      "Epoch 659, Loss: 0.11841042246669531, Final Batch Loss: 0.0014068444725126028\n",
      "Epoch 660, Loss: 0.06260093045420945, Final Batch Loss: 0.013657997362315655\n",
      "Epoch 661, Loss: 0.0317719595041126, Final Batch Loss: 0.001277826027944684\n",
      "Epoch 662, Loss: 0.05661782546667382, Final Batch Loss: 0.009259742684662342\n",
      "Epoch 663, Loss: 0.06647362664807588, Final Batch Loss: 0.0008324821246787906\n",
      "Epoch 664, Loss: 0.07879333998425864, Final Batch Loss: 0.0013669459149241447\n",
      "Epoch 665, Loss: 0.062175306549761444, Final Batch Loss: 0.0028560557402670383\n",
      "Epoch 666, Loss: 0.06233813508879393, Final Batch Loss: 0.0022138345520943403\n",
      "Epoch 667, Loss: 0.07330989598995075, Final Batch Loss: 0.002035114448517561\n",
      "Epoch 668, Loss: 0.150062303931918, Final Batch Loss: 0.0072605363093316555\n",
      "Epoch 669, Loss: 0.05199325902503915, Final Batch Loss: 0.002172193955630064\n",
      "Epoch 670, Loss: 0.10391561279539019, Final Batch Loss: 0.019891628995537758\n",
      "Epoch 671, Loss: 0.09479373705107719, Final Batch Loss: 0.0005674065905623138\n",
      "Epoch 672, Loss: 0.05809039215091616, Final Batch Loss: 0.0012316963402554393\n",
      "Epoch 673, Loss: 0.0970623770263046, Final Batch Loss: 0.004495284985750914\n",
      "Epoch 674, Loss: 0.11327930027619004, Final Batch Loss: 0.011930164881050587\n",
      "Epoch 675, Loss: 0.12201094249030575, Final Batch Loss: 0.0005999432760290802\n",
      "Epoch 676, Loss: 0.09495336381951347, Final Batch Loss: 0.0013475676532834768\n",
      "Epoch 677, Loss: 0.11995219171512872, Final Batch Loss: 0.00926552340388298\n",
      "Epoch 678, Loss: 0.09338215686148033, Final Batch Loss: 0.004595702979713678\n",
      "Epoch 679, Loss: 0.07878522702958435, Final Batch Loss: 0.007408737670630217\n",
      "Epoch 680, Loss: 0.10729622605140321, Final Batch Loss: 0.002882962813600898\n",
      "Epoch 681, Loss: 0.08933437027735636, Final Batch Loss: 0.0003632281441241503\n",
      "Epoch 682, Loss: 0.1030929358676076, Final Batch Loss: 0.0023682357277721167\n",
      "Epoch 683, Loss: 0.07372725894674659, Final Batch Loss: 0.0026552381459623575\n",
      "Epoch 684, Loss: 0.057185282988939434, Final Batch Loss: 0.005347765516489744\n",
      "Epoch 685, Loss: 0.030400661460589617, Final Batch Loss: 0.0030961278825998306\n",
      "Epoch 686, Loss: 0.05081038811476901, Final Batch Loss: 0.005152962636202574\n",
      "Epoch 687, Loss: 0.0991704540210776, Final Batch Loss: 0.00024525434128008783\n",
      "Epoch 688, Loss: 0.10921861068345606, Final Batch Loss: 0.006060193292796612\n",
      "Epoch 689, Loss: 0.15417118216282688, Final Batch Loss: 0.015524364076554775\n",
      "Epoch 690, Loss: 0.1302345803414937, Final Batch Loss: 0.002887304639443755\n",
      "Epoch 691, Loss: 0.08097071840893477, Final Batch Loss: 0.0034331222996115685\n",
      "Epoch 692, Loss: 0.0663474008324556, Final Batch Loss: 0.0019266270101070404\n",
      "Epoch 693, Loss: 0.08644849545089528, Final Batch Loss: 0.0032596446108072996\n",
      "Epoch 694, Loss: 0.04673966245900374, Final Batch Loss: 0.003181417239829898\n",
      "Epoch 695, Loss: 0.0882996825966984, Final Batch Loss: 0.010278372094035149\n",
      "Epoch 696, Loss: 0.12002074575866573, Final Batch Loss: 0.013208640739321709\n",
      "Epoch 697, Loss: 0.1275078175822273, Final Batch Loss: 0.004528597928583622\n",
      "Epoch 698, Loss: 0.072227357479278, Final Batch Loss: 0.008289649151265621\n",
      "Epoch 699, Loss: 0.15183233260177076, Final Batch Loss: 0.023265885189175606\n",
      "Epoch 700, Loss: 0.11895212915260345, Final Batch Loss: 0.008839610032737255\n",
      "Epoch 701, Loss: 0.05923032687860541, Final Batch Loss: 0.003689560806378722\n",
      "Epoch 702, Loss: 0.07152004982344806, Final Batch Loss: 0.0026368945837020874\n",
      "Epoch 703, Loss: 0.06360326986759901, Final Batch Loss: 0.006556384731084108\n",
      "Epoch 704, Loss: 0.06759519857587293, Final Batch Loss: 0.009528358466923237\n",
      "Epoch 705, Loss: 0.09161478697205894, Final Batch Loss: 0.026423420757055283\n",
      "Epoch 706, Loss: 0.049577685131225735, Final Batch Loss: 0.0028384001925587654\n",
      "Epoch 707, Loss: 0.11739370971918106, Final Batch Loss: 0.0020643952302634716\n",
      "Epoch 708, Loss: 0.0657933784823399, Final Batch Loss: 0.006455868482589722\n",
      "Epoch 709, Loss: 0.04060032055713236, Final Batch Loss: 0.015450013801455498\n",
      "Epoch 710, Loss: 0.0687032971472945, Final Batch Loss: 0.0034413046669214964\n",
      "Epoch 711, Loss: 0.04894315919955261, Final Batch Loss: 0.00028638067306019366\n",
      "Epoch 712, Loss: 0.056225687032565475, Final Batch Loss: 0.0035504279658198357\n",
      "Epoch 713, Loss: 0.12367837561760098, Final Batch Loss: 0.004817006643861532\n",
      "Epoch 714, Loss: 0.0605230959772598, Final Batch Loss: 0.003992910962551832\n",
      "Epoch 715, Loss: 0.0500250612385571, Final Batch Loss: 0.0065054199658334255\n",
      "Epoch 716, Loss: 0.1517359713325277, Final Batch Loss: 0.0018587219528853893\n",
      "Epoch 717, Loss: 0.06095489556901157, Final Batch Loss: 0.006825895048677921\n",
      "Epoch 718, Loss: 0.10215379775036126, Final Batch Loss: 0.0322311706840992\n",
      "Epoch 719, Loss: 0.058134768216405064, Final Batch Loss: 0.0005028775776736438\n",
      "Epoch 720, Loss: 0.10371599058271386, Final Batch Loss: 0.006790725979954004\n",
      "Epoch 721, Loss: 0.07250747864600271, Final Batch Loss: 0.015087006613612175\n",
      "Epoch 722, Loss: 0.051211845537181944, Final Batch Loss: 0.0016720916610211134\n",
      "Epoch 723, Loss: 0.07072988006984815, Final Batch Loss: 0.0047716801054775715\n",
      "Epoch 724, Loss: 0.12193535751430318, Final Batch Loss: 0.001688566873781383\n",
      "Epoch 725, Loss: 0.09546061852597632, Final Batch Loss: 0.0009567313827574253\n",
      "Epoch 726, Loss: 0.11266943963710219, Final Batch Loss: 0.005377080757170916\n",
      "Epoch 727, Loss: 0.08507668651873246, Final Batch Loss: 0.006842543371021748\n",
      "Epoch 728, Loss: 0.12178450671490282, Final Batch Loss: 0.006534806918352842\n",
      "Epoch 729, Loss: 0.06173413267242722, Final Batch Loss: 0.0030036428943276405\n",
      "Epoch 730, Loss: 0.11482754087774083, Final Batch Loss: 0.026735585182905197\n",
      "Epoch 731, Loss: 0.0880265835148748, Final Batch Loss: 0.0008787735714577138\n",
      "Epoch 732, Loss: 0.10631878970889375, Final Batch Loss: 0.04566211998462677\n",
      "Epoch 733, Loss: 0.06683134048944339, Final Batch Loss: 0.0008883055415935814\n",
      "Epoch 734, Loss: 0.06691433995729312, Final Batch Loss: 0.0011810287833213806\n",
      "Epoch 735, Loss: 0.06231489684432745, Final Batch Loss: 0.0025066290982067585\n",
      "Epoch 736, Loss: 0.06280003560823388, Final Batch Loss: 0.0003770303737837821\n",
      "Epoch 737, Loss: 0.04757126420736313, Final Batch Loss: 0.0013146963901817799\n",
      "Epoch 738, Loss: 0.07913499389542267, Final Batch Loss: 0.0038874943275004625\n",
      "Epoch 739, Loss: 0.07684653726755641, Final Batch Loss: 0.0057726167142391205\n",
      "Epoch 740, Loss: 0.08871108858147636, Final Batch Loss: 0.005336728412657976\n",
      "Epoch 741, Loss: 0.1268299950461369, Final Batch Loss: 0.04322400316596031\n",
      "Epoch 742, Loss: 0.15961705427616835, Final Batch Loss: 0.0050353966653347015\n",
      "Epoch 743, Loss: 0.05438845721073449, Final Batch Loss: 0.0020423848181962967\n",
      "Epoch 744, Loss: 0.1379841358284466, Final Batch Loss: 0.011374706402420998\n",
      "Epoch 745, Loss: 0.058313627319876105, Final Batch Loss: 0.004277978558093309\n",
      "Epoch 746, Loss: 0.05712954979389906, Final Batch Loss: 0.0031170302536338568\n",
      "Epoch 747, Loss: 0.14998046102118678, Final Batch Loss: 0.008798650465905666\n",
      "Epoch 748, Loss: 0.15960069419816136, Final Batch Loss: 0.003909725695848465\n",
      "Epoch 749, Loss: 0.13990276353433728, Final Batch Loss: 0.0019558898638933897\n",
      "Epoch 750, Loss: 0.04882174517842941, Final Batch Loss: 0.006733375135809183\n",
      "Epoch 751, Loss: 0.042862089758273214, Final Batch Loss: 0.0006395250093191862\n",
      "Epoch 752, Loss: 0.043551354377996176, Final Batch Loss: 0.0027512642554938793\n",
      "Epoch 753, Loss: 0.043946432502707466, Final Batch Loss: 0.00047884800005704165\n",
      "Epoch 754, Loss: 0.05483882047701627, Final Batch Loss: 0.0015844882000237703\n",
      "Epoch 755, Loss: 0.10471397833316587, Final Batch Loss: 0.04861203953623772\n",
      "Epoch 756, Loss: 0.14874336763750762, Final Batch Loss: 0.007391332183033228\n",
      "Epoch 757, Loss: 0.0923526355472859, Final Batch Loss: 0.0014614646788686514\n",
      "Epoch 758, Loss: 0.059671539871487767, Final Batch Loss: 0.006591074634343386\n",
      "Epoch 759, Loss: 0.08137668325798586, Final Batch Loss: 0.0009681828669272363\n",
      "Epoch 760, Loss: 0.062224755325587466, Final Batch Loss: 0.0055847433395683765\n",
      "Epoch 761, Loss: 0.115939237002749, Final Batch Loss: 0.002525179646909237\n",
      "Epoch 762, Loss: 0.09790676587726921, Final Batch Loss: 0.0010217806557193398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763, Loss: 0.07596595003269613, Final Batch Loss: 0.001053989166393876\n",
      "Epoch 764, Loss: 0.07902942736109253, Final Batch Loss: 0.01911124400794506\n",
      "Epoch 765, Loss: 0.13028805830981582, Final Batch Loss: 0.019305597990751266\n",
      "Epoch 766, Loss: 0.07383843319257721, Final Batch Loss: 0.0005020155222155154\n",
      "Epoch 767, Loss: 0.09702006762381643, Final Batch Loss: 0.002640356542542577\n",
      "Epoch 768, Loss: 0.08552463154774159, Final Batch Loss: 0.008756224997341633\n",
      "Epoch 769, Loss: 0.059955927601549774, Final Batch Loss: 0.004437172319740057\n",
      "Epoch 770, Loss: 0.048109072900842875, Final Batch Loss: 0.0003989048709627241\n",
      "Epoch 771, Loss: 0.09958647648454644, Final Batch Loss: 0.0031969640403985977\n",
      "Epoch 772, Loss: 0.06760079431114718, Final Batch Loss: 0.01090490072965622\n",
      "Epoch 773, Loss: 0.0863781608059071, Final Batch Loss: 0.0005468208692036569\n",
      "Epoch 774, Loss: 0.06344947154866531, Final Batch Loss: 0.0026944917626678944\n",
      "Epoch 775, Loss: 0.09809758776100352, Final Batch Loss: 0.0014030002057552338\n",
      "Epoch 776, Loss: 0.12172648165142164, Final Batch Loss: 0.00330103631131351\n",
      "Epoch 777, Loss: 0.07612993428483605, Final Batch Loss: 0.017819302156567574\n",
      "Epoch 778, Loss: 0.06242565994034521, Final Batch Loss: 0.004390561953186989\n",
      "Epoch 779, Loss: 0.04964813042897731, Final Batch Loss: 0.0010275754611939192\n",
      "Epoch 780, Loss: 0.05141858337447047, Final Batch Loss: 0.0028670632746070623\n",
      "Epoch 781, Loss: 0.0906508089392446, Final Batch Loss: 0.0008556957473047078\n",
      "Epoch 782, Loss: 0.046196154376957566, Final Batch Loss: 0.0008596587576903403\n",
      "Epoch 783, Loss: 0.042300443950807676, Final Batch Loss: 0.0002770308929029852\n",
      "Epoch 784, Loss: 0.13818511558929458, Final Batch Loss: 0.0008837535860948265\n",
      "Epoch 785, Loss: 0.08481201887479983, Final Batch Loss: 0.01951863430440426\n",
      "Epoch 786, Loss: 0.07024365046527237, Final Batch Loss: 0.00517655536532402\n",
      "Epoch 787, Loss: 0.10096452990546823, Final Batch Loss: 0.011107861064374447\n",
      "Epoch 788, Loss: 0.1208881932834629, Final Batch Loss: 0.00568158645182848\n",
      "Epoch 789, Loss: 0.06152512994594872, Final Batch Loss: 0.00564471585676074\n",
      "Epoch 790, Loss: 0.08020990807563066, Final Batch Loss: 0.009263885207474232\n",
      "Epoch 791, Loss: 0.06907763562048785, Final Batch Loss: 0.0010905701201409101\n",
      "Epoch 792, Loss: 0.10717029997613281, Final Batch Loss: 0.0009890258079394698\n",
      "Epoch 793, Loss: 0.08112325202091597, Final Batch Loss: 0.018876850605010986\n",
      "Epoch 794, Loss: 0.1168218384264037, Final Batch Loss: 0.021989459171891212\n",
      "Epoch 795, Loss: 0.05529554147506133, Final Batch Loss: 0.0031678483355790377\n",
      "Epoch 796, Loss: 0.07765608112094924, Final Batch Loss: 0.0008590327342972159\n",
      "Epoch 797, Loss: 0.10495644964976236, Final Batch Loss: 0.018737081438302994\n",
      "Epoch 798, Loss: 0.0576891606033314, Final Batch Loss: 0.0035149427130818367\n",
      "Epoch 799, Loss: 0.051043905841652304, Final Batch Loss: 0.0018920316360890865\n",
      "Epoch 800, Loss: 0.06442876213259296, Final Batch Loss: 0.005336390342563391\n",
      "Epoch 801, Loss: 0.06036971270805225, Final Batch Loss: 0.0003005509788636118\n",
      "Epoch 802, Loss: 0.04104109219042584, Final Batch Loss: 0.0018494564574211836\n",
      "Epoch 803, Loss: 0.0647106098185759, Final Batch Loss: 0.01337967999279499\n",
      "Epoch 804, Loss: 0.041156311010126956, Final Batch Loss: 0.001057290006428957\n",
      "Epoch 805, Loss: 0.1973013230599463, Final Batch Loss: 0.0017787620890885592\n",
      "Epoch 806, Loss: 0.048354366270359606, Final Batch Loss: 0.00025546643882989883\n",
      "Epoch 807, Loss: 0.09554384378134273, Final Batch Loss: 0.001790213631466031\n",
      "Epoch 808, Loss: 0.10662780981510878, Final Batch Loss: 0.012729260139167309\n",
      "Epoch 809, Loss: 0.09821631503291428, Final Batch Loss: 0.001930885948240757\n",
      "Epoch 810, Loss: 0.07309642745531164, Final Batch Loss: 0.0014892632607370615\n",
      "Epoch 811, Loss: 0.08541747252456844, Final Batch Loss: 0.0033279629424214363\n",
      "Epoch 812, Loss: 0.05160803729086183, Final Batch Loss: 0.0035464384127408266\n",
      "Epoch 813, Loss: 0.048993581847753376, Final Batch Loss: 0.003390081925317645\n",
      "Epoch 814, Loss: 0.060195684869540855, Final Batch Loss: 0.0017580566927790642\n",
      "Epoch 815, Loss: 0.052759289275854826, Final Batch Loss: 0.0023668184876441956\n",
      "Epoch 816, Loss: 0.02433551725698635, Final Batch Loss: 0.0014764310326427221\n",
      "Epoch 817, Loss: 0.03509985262644477, Final Batch Loss: 0.000986495055258274\n",
      "Epoch 818, Loss: 0.04669481870951131, Final Batch Loss: 0.0002419876109343022\n",
      "Epoch 819, Loss: 0.03389698831597343, Final Batch Loss: 0.0013772088568657637\n",
      "Epoch 820, Loss: 0.05021326639689505, Final Batch Loss: 0.001151102245785296\n",
      "Epoch 821, Loss: 0.054711884033167735, Final Batch Loss: 0.0024528224021196365\n",
      "Epoch 822, Loss: 0.06265056796837598, Final Batch Loss: 0.0005102345603518188\n",
      "Epoch 823, Loss: 0.03958776974468492, Final Batch Loss: 0.0010158554650843143\n",
      "Epoch 824, Loss: 0.07551337988115847, Final Batch Loss: 0.0008312306017614901\n",
      "Epoch 825, Loss: 0.12244161812122911, Final Batch Loss: 0.023053286597132683\n",
      "Epoch 826, Loss: 0.10696627263678238, Final Batch Loss: 0.024042770266532898\n",
      "Epoch 827, Loss: 0.07015018176753074, Final Batch Loss: 0.0017430959269404411\n",
      "Epoch 828, Loss: 0.03753363488067407, Final Batch Loss: 0.0012059983564540744\n",
      "Epoch 829, Loss: 0.12967856239993125, Final Batch Loss: 0.0004981897072866559\n",
      "Epoch 830, Loss: 0.08385888690827414, Final Batch Loss: 0.0010728099150583148\n",
      "Epoch 831, Loss: 0.061495808651670814, Final Batch Loss: 0.005269299726933241\n",
      "Epoch 832, Loss: 0.10442858224269003, Final Batch Loss: 0.012399791739881039\n",
      "Epoch 833, Loss: 0.07216465484816581, Final Batch Loss: 0.0009302299586124718\n",
      "Epoch 834, Loss: 0.04830983222927898, Final Batch Loss: 0.0016347422497346997\n",
      "Epoch 835, Loss: 0.0868297473207349, Final Batch Loss: 0.004215332679450512\n",
      "Epoch 836, Loss: 0.032144470926141366, Final Batch Loss: 0.0020720167085528374\n",
      "Epoch 837, Loss: 0.09549218066968024, Final Batch Loss: 0.031122108921408653\n",
      "Epoch 838, Loss: 0.10847309895325452, Final Batch Loss: 0.014835822395980358\n",
      "Epoch 839, Loss: 0.07368250182480551, Final Batch Loss: 0.0002726120001170784\n",
      "Epoch 840, Loss: 0.051993537694215775, Final Batch Loss: 0.0016329878708347678\n",
      "Epoch 841, Loss: 0.08639993306132965, Final Batch Loss: 0.03705279901623726\n",
      "Epoch 842, Loss: 0.0737195681722369, Final Batch Loss: 0.0006523950723931193\n",
      "Epoch 843, Loss: 0.05623438413022086, Final Batch Loss: 0.00048678283928893507\n",
      "Epoch 844, Loss: 0.0459274940949399, Final Batch Loss: 0.000609574664849788\n",
      "Epoch 845, Loss: 0.12132932609529234, Final Batch Loss: 0.05210762470960617\n",
      "Epoch 846, Loss: 0.07833294797455892, Final Batch Loss: 0.0009230480645783246\n",
      "Epoch 847, Loss: 0.08329643949400634, Final Batch Loss: 0.0027315740007907152\n",
      "Epoch 848, Loss: 0.031119640276301652, Final Batch Loss: 0.0015410949708893895\n",
      "Epoch 849, Loss: 0.03742465242976323, Final Batch Loss: 0.0025939219631254673\n",
      "Epoch 850, Loss: 0.053605665860231966, Final Batch Loss: 0.0017283931374549866\n",
      "Epoch 851, Loss: 0.04920987761579454, Final Batch Loss: 0.007680553011596203\n",
      "Epoch 852, Loss: 0.09360472433036193, Final Batch Loss: 0.0011025049025192857\n",
      "Epoch 853, Loss: 0.09148861964058597, Final Batch Loss: 0.034629564732313156\n",
      "Epoch 854, Loss: 0.09405144504853524, Final Batch Loss: 0.014802848920226097\n",
      "Epoch 855, Loss: 0.08417161725810729, Final Batch Loss: 0.0037705921567976475\n",
      "Epoch 856, Loss: 0.04115827255009208, Final Batch Loss: 0.000747479556594044\n",
      "Epoch 857, Loss: 0.04450313907000236, Final Batch Loss: 0.0019597664941102266\n",
      "Epoch 858, Loss: 0.04288711995468475, Final Batch Loss: 0.0004603768466040492\n",
      "Epoch 859, Loss: 0.08601649093907326, Final Batch Loss: 0.018506722524762154\n",
      "Epoch 860, Loss: 0.09401243434695061, Final Batch Loss: 0.00023159924603533\n",
      "Epoch 861, Loss: 0.050711887364741415, Final Batch Loss: 0.01642463728785515\n",
      "Epoch 862, Loss: 0.0658594892884139, Final Batch Loss: 0.004725665785372257\n",
      "Epoch 863, Loss: 0.04320651383022778, Final Batch Loss: 0.0010883350623771548\n",
      "Epoch 864, Loss: 0.06889468152076006, Final Batch Loss: 0.0018740080995485187\n",
      "Epoch 865, Loss: 0.062320177734363824, Final Batch Loss: 0.004323560744524002\n",
      "Epoch 866, Loss: 0.09426445892313495, Final Batch Loss: 0.018620718270540237\n",
      "Epoch 867, Loss: 0.06950605010933941, Final Batch Loss: 0.0024596003349870443\n",
      "Epoch 868, Loss: 0.07880693601327948, Final Batch Loss: 0.004220983944833279\n",
      "Epoch 869, Loss: 0.05869331871508621, Final Batch Loss: 0.003752683522179723\n",
      "Epoch 870, Loss: 0.07003897819959093, Final Batch Loss: 0.004709325265139341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 871, Loss: 0.07294510197243653, Final Batch Loss: 0.022033853456377983\n",
      "Epoch 872, Loss: 0.13368596459622495, Final Batch Loss: 0.00035619415575638413\n",
      "Epoch 873, Loss: 0.14415523438947275, Final Batch Loss: 0.004968536086380482\n",
      "Epoch 874, Loss: 0.0719841169193387, Final Batch Loss: 0.0014132509240880609\n",
      "Epoch 875, Loss: 0.06663415610091761, Final Batch Loss: 0.002183795440942049\n",
      "Epoch 876, Loss: 0.08597311002085917, Final Batch Loss: 0.011946250684559345\n",
      "Epoch 877, Loss: 0.10518789514026139, Final Batch Loss: 0.00021537191059906036\n",
      "Epoch 878, Loss: 0.07691492192680016, Final Batch Loss: 0.02046610601246357\n",
      "Epoch 879, Loss: 0.07443239458370954, Final Batch Loss: 0.0026655951514840126\n",
      "Epoch 880, Loss: 0.03646527563978452, Final Batch Loss: 0.007957199588418007\n",
      "Epoch 881, Loss: 0.036320650309789926, Final Batch Loss: 0.0009125283104367554\n",
      "Epoch 882, Loss: 0.036864942565443926, Final Batch Loss: 0.000964042148552835\n",
      "Epoch 883, Loss: 0.030422955693211406, Final Batch Loss: 0.00980173610150814\n",
      "Epoch 884, Loss: 0.030212843310437165, Final Batch Loss: 0.0011501755798235536\n",
      "Epoch 885, Loss: 0.040638302540173754, Final Batch Loss: 0.003843868849799037\n",
      "Epoch 886, Loss: 0.1111321646894794, Final Batch Loss: 0.00771710928529501\n",
      "Epoch 887, Loss: 0.053984461788786575, Final Batch Loss: 0.0023812009021639824\n",
      "Epoch 888, Loss: 0.03501459147082642, Final Batch Loss: 0.0003740752872545272\n",
      "Epoch 889, Loss: 0.09607524119201116, Final Batch Loss: 0.0003545528161339462\n",
      "Epoch 890, Loss: 0.12046587158693, Final Batch Loss: 0.007884986698627472\n",
      "Epoch 891, Loss: 0.08620027956203558, Final Batch Loss: 0.00562823424115777\n",
      "Epoch 892, Loss: 0.05227762111462653, Final Batch Loss: 0.0018076084088534117\n",
      "Epoch 893, Loss: 0.052744946646271273, Final Batch Loss: 0.002648384077474475\n",
      "Epoch 894, Loss: 0.09872450816328637, Final Batch Loss: 0.0003858617565128952\n",
      "Epoch 895, Loss: 0.12461951395380311, Final Batch Loss: 0.007454117760062218\n",
      "Epoch 896, Loss: 0.061372183059575036, Final Batch Loss: 0.0006221565417945385\n",
      "Epoch 897, Loss: 0.09476005408214405, Final Batch Loss: 0.023361334577202797\n",
      "Epoch 898, Loss: 0.11081781395478174, Final Batch Loss: 0.0018753992626443505\n",
      "Epoch 899, Loss: 0.062039502736297436, Final Batch Loss: 0.0020460481755435467\n",
      "Epoch 900, Loss: 0.055597236860194243, Final Batch Loss: 0.00045952366781421006\n",
      "Epoch 901, Loss: 0.032644291073665954, Final Batch Loss: 0.0005911909975111485\n",
      "Epoch 902, Loss: 0.0693635037750937, Final Batch Loss: 0.005394738167524338\n",
      "Epoch 903, Loss: 0.0808594078553142, Final Batch Loss: 0.023329567164182663\n",
      "Epoch 904, Loss: 0.0648732794506941, Final Batch Loss: 0.00021353067131713033\n",
      "Epoch 905, Loss: 0.07471343968063593, Final Batch Loss: 0.02237003855407238\n",
      "Epoch 906, Loss: 0.10618114785756916, Final Batch Loss: 0.005694198422133923\n",
      "Epoch 907, Loss: 0.07355922390706837, Final Batch Loss: 0.0027386141009628773\n",
      "Epoch 908, Loss: 0.08756568557873834, Final Batch Loss: 0.000289786490611732\n",
      "Epoch 909, Loss: 0.11140691876062192, Final Batch Loss: 0.01900191605091095\n",
      "Epoch 910, Loss: 0.03605591224186355, Final Batch Loss: 0.0004542506067082286\n",
      "Epoch 911, Loss: 0.03636751297744922, Final Batch Loss: 0.007409439422190189\n",
      "Epoch 912, Loss: 0.038561567846045364, Final Batch Loss: 0.0037328365724533796\n",
      "Epoch 913, Loss: 0.03942550020292401, Final Batch Loss: 0.003469678107649088\n",
      "Epoch 914, Loss: 0.0474243123899214, Final Batch Loss: 0.0002925566805060953\n",
      "Epoch 915, Loss: 0.046869142301147804, Final Batch Loss: 0.0008059290703386068\n",
      "Epoch 916, Loss: 0.050687299968558364, Final Batch Loss: 0.0008966596215032041\n",
      "Epoch 917, Loss: 0.019964499784691725, Final Batch Loss: 0.000686303130351007\n",
      "Epoch 918, Loss: 0.029852126026526093, Final Batch Loss: 0.0017101902049034834\n",
      "Epoch 919, Loss: 0.09543470548669575, Final Batch Loss: 0.023874862119555473\n",
      "Epoch 920, Loss: 0.08822864860121626, Final Batch Loss: 0.007213935721665621\n",
      "Epoch 921, Loss: 0.04686397282057442, Final Batch Loss: 0.010722055099904537\n",
      "Epoch 922, Loss: 0.034619635611306876, Final Batch Loss: 0.0008312485297210515\n",
      "Epoch 923, Loss: 0.04628439543012064, Final Batch Loss: 0.003842958714812994\n",
      "Epoch 924, Loss: 0.08113971602870151, Final Batch Loss: 0.0005098565015941858\n",
      "Epoch 925, Loss: 0.0392559265892487, Final Batch Loss: 0.0006367427995428443\n",
      "Epoch 926, Loss: 0.051734596214373596, Final Batch Loss: 0.013904081657528877\n",
      "Epoch 927, Loss: 0.08957958857354242, Final Batch Loss: 0.0021840708795934916\n",
      "Epoch 928, Loss: 0.11618412949610502, Final Batch Loss: 0.0072116544470191\n",
      "Epoch 929, Loss: 0.0657209315977525, Final Batch Loss: 0.00026379714836366475\n",
      "Epoch 930, Loss: 0.09377785242395476, Final Batch Loss: 0.010545081458985806\n",
      "Epoch 931, Loss: 0.06459965783869848, Final Batch Loss: 0.0017337739700451493\n",
      "Epoch 932, Loss: 0.04204171634046361, Final Batch Loss: 0.00528003741055727\n",
      "Epoch 933, Loss: 0.03975654862006195, Final Batch Loss: 0.0017006585840135813\n",
      "Epoch 934, Loss: 0.1093008893658407, Final Batch Loss: 0.012705792672932148\n",
      "Epoch 935, Loss: 0.09313661241321824, Final Batch Loss: 0.0063469260931015015\n",
      "Epoch 936, Loss: 0.07605494841118343, Final Batch Loss: 0.00048548070481047034\n",
      "Epoch 937, Loss: 0.054213439754676074, Final Batch Loss: 0.0003710446762852371\n",
      "Epoch 938, Loss: 0.046040239722060505, Final Batch Loss: 0.001129048177972436\n",
      "Epoch 939, Loss: 0.08840508680441417, Final Batch Loss: 0.0012268269201740623\n",
      "Epoch 940, Loss: 0.07468100878759287, Final Batch Loss: 0.0017230326775461435\n",
      "Epoch 941, Loss: 0.058467263530474156, Final Batch Loss: 0.0026253743562847376\n",
      "Epoch 942, Loss: 0.04841054166899994, Final Batch Loss: 0.0023914072662591934\n",
      "Epoch 943, Loss: 0.029366721078986302, Final Batch Loss: 0.0010761504527181387\n",
      "Epoch 944, Loss: 0.08103272806329187, Final Batch Loss: 0.0009649680578149855\n",
      "Epoch 945, Loss: 0.06758846034063026, Final Batch Loss: 0.0005715502775274217\n",
      "Epoch 946, Loss: 0.038090109766926616, Final Batch Loss: 0.0003225329564884305\n",
      "Epoch 947, Loss: 0.032546896996791475, Final Batch Loss: 0.0011125633027404547\n",
      "Epoch 948, Loss: 0.018926168777397834, Final Batch Loss: 0.0005319065530784428\n",
      "Epoch 949, Loss: 0.051165597804356366, Final Batch Loss: 0.0008926003938540816\n",
      "Epoch 950, Loss: 0.09000405858387239, Final Batch Loss: 0.00047307732165791094\n",
      "Epoch 951, Loss: 0.128211876435671, Final Batch Loss: 0.006200973875820637\n",
      "Epoch 952, Loss: 0.053880576699157245, Final Batch Loss: 0.0009120721952058375\n",
      "Epoch 953, Loss: 0.03961628276738338, Final Batch Loss: 0.006604779977351427\n",
      "Epoch 954, Loss: 0.0414883552439278, Final Batch Loss: 0.0004953558673150837\n",
      "Epoch 955, Loss: 0.10513462199014612, Final Batch Loss: 0.0007125757983885705\n",
      "Epoch 956, Loss: 0.0858255325583741, Final Batch Loss: 0.010932219214737415\n",
      "Epoch 957, Loss: 0.08170853505725972, Final Batch Loss: 0.006515707820653915\n",
      "Epoch 958, Loss: 0.09723676316207275, Final Batch Loss: 0.004386934917420149\n",
      "Epoch 959, Loss: 0.03845473635010421, Final Batch Loss: 0.0003770490875467658\n",
      "Epoch 960, Loss: 0.048624279734212905, Final Batch Loss: 0.001120983506552875\n",
      "Epoch 961, Loss: 0.02227417098765727, Final Batch Loss: 0.00039978770655579865\n",
      "Epoch 962, Loss: 0.03220533111016266, Final Batch Loss: 0.0007365901255980134\n",
      "Epoch 963, Loss: 0.052461540610238444, Final Batch Loss: 0.00019383624021429569\n",
      "Epoch 964, Loss: 0.037263117359543685, Final Batch Loss: 0.001486623426899314\n",
      "Epoch 965, Loss: 0.03992229694267735, Final Batch Loss: 0.007409417070448399\n",
      "Epoch 966, Loss: 0.06867150793550536, Final Batch Loss: 0.0007114686886779964\n",
      "Epoch 967, Loss: 0.07725203799782321, Final Batch Loss: 0.008326034992933273\n",
      "Epoch 968, Loss: 0.04745022236602381, Final Batch Loss: 0.0003187597612850368\n",
      "Epoch 969, Loss: 0.07153745485993568, Final Batch Loss: 0.0021413045469671488\n",
      "Epoch 970, Loss: 0.09752817527623847, Final Batch Loss: 0.0013170010643079877\n",
      "Epoch 971, Loss: 0.03271298548497725, Final Batch Loss: 0.0005295766750350595\n",
      "Epoch 972, Loss: 0.12326967442641035, Final Batch Loss: 0.005678103771060705\n",
      "Epoch 973, Loss: 0.09205320884939283, Final Batch Loss: 0.0043978700414299965\n",
      "Epoch 974, Loss: 0.1914453343488276, Final Batch Loss: 0.003154585836455226\n",
      "Epoch 975, Loss: 0.15180659090401605, Final Batch Loss: 0.002101319143548608\n",
      "Epoch 976, Loss: 0.07407062069978565, Final Batch Loss: 0.004988361150026321\n",
      "Epoch 977, Loss: 0.040653645890415646, Final Batch Loss: 0.00015944828919600695\n",
      "Epoch 978, Loss: 0.04381336399819702, Final Batch Loss: 0.0010573716135695577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 979, Loss: 0.05813857927569188, Final Batch Loss: 0.009127330034971237\n",
      "Epoch 980, Loss: 0.050072174228262156, Final Batch Loss: 0.0015337589429691434\n",
      "Epoch 981, Loss: 0.033975465572439134, Final Batch Loss: 0.007218284998089075\n",
      "Epoch 982, Loss: 0.0587858942017192, Final Batch Loss: 0.003574795089662075\n",
      "Epoch 983, Loss: 0.04612700195866637, Final Batch Loss: 0.0010464877123013139\n",
      "Epoch 984, Loss: 0.0685721923073288, Final Batch Loss: 0.0002849116863217205\n",
      "Epoch 985, Loss: 0.048354693382862024, Final Batch Loss: 0.00027229971601627767\n",
      "Epoch 986, Loss: 0.05891462374711409, Final Batch Loss: 0.001113450271077454\n",
      "Epoch 987, Loss: 0.06127600179752335, Final Batch Loss: 0.008374368771910667\n",
      "Epoch 988, Loss: 0.09694893143023364, Final Batch Loss: 0.020117612555623055\n",
      "Epoch 989, Loss: 0.03622471887501888, Final Batch Loss: 0.00016963756934273988\n",
      "Epoch 990, Loss: 0.0517238855100004, Final Batch Loss: 0.012816098518669605\n",
      "Epoch 991, Loss: 0.05002864009293262, Final Batch Loss: 0.0017112980131059885\n",
      "Epoch 992, Loss: 0.06278422411560314, Final Batch Loss: 0.000505792791955173\n",
      "Epoch 993, Loss: 0.031143698884989135, Final Batch Loss: 0.005930750630795956\n",
      "Epoch 994, Loss: 0.055814025065046735, Final Batch Loss: 0.001349214930087328\n",
      "Epoch 995, Loss: 0.08412422779656481, Final Batch Loss: 0.024654502049088478\n",
      "Epoch 996, Loss: 0.05244618403958157, Final Batch Loss: 0.00023536322987638414\n",
      "Epoch 997, Loss: 0.036609741422580555, Final Batch Loss: 0.005357903894037008\n",
      "Epoch 998, Loss: 0.03779825734090991, Final Batch Loss: 0.0014731023693457246\n",
      "Epoch 999, Loss: 0.04898826085263863, Final Batch Loss: 0.0072853174060583115\n",
      "Epoch 1000, Loss: 0.0159083261678461, Final Batch Loss: 0.00014317418390419334\n",
      "Epoch 1001, Loss: 0.08096737157029565, Final Batch Loss: 0.0006756280199624598\n",
      "Epoch 1002, Loss: 0.049216575716855004, Final Batch Loss: 0.013630924746394157\n",
      "Epoch 1003, Loss: 0.03318835514073726, Final Batch Loss: 0.0005453488556668162\n",
      "Epoch 1004, Loss: 0.037265500795911066, Final Batch Loss: 0.00029394758166745305\n",
      "Epoch 1005, Loss: 0.07188667950686067, Final Batch Loss: 0.00022196174541022629\n",
      "Epoch 1006, Loss: 0.05088112175872084, Final Batch Loss: 0.001061879564076662\n",
      "Epoch 1007, Loss: 0.02828785874589812, Final Batch Loss: 0.0010030614212155342\n",
      "Epoch 1008, Loss: 0.0965660916917841, Final Batch Loss: 0.0022897240705788136\n",
      "Epoch 1009, Loss: 0.026464150912943296, Final Batch Loss: 0.005910055246204138\n",
      "Epoch 1010, Loss: 0.029492655812646262, Final Batch Loss: 0.00487550487741828\n",
      "Epoch 1011, Loss: 0.03915389742178377, Final Batch Loss: 0.0038193254731595516\n",
      "Epoch 1012, Loss: 0.04208563704742119, Final Batch Loss: 0.0006740190437994897\n",
      "Epoch 1013, Loss: 0.052737331847311, Final Batch Loss: 0.004222800023853779\n",
      "Epoch 1014, Loss: 0.10115709312958643, Final Batch Loss: 0.0018431775970384479\n",
      "Epoch 1015, Loss: 0.07019556203158572, Final Batch Loss: 0.018335115164518356\n",
      "Epoch 1016, Loss: 0.0826938803656958, Final Batch Loss: 0.0010631311452016234\n",
      "Epoch 1017, Loss: 0.07327349908882752, Final Batch Loss: 0.009661461226642132\n",
      "Epoch 1018, Loss: 0.08084698731545359, Final Batch Loss: 0.0029475197661668062\n",
      "Epoch 1019, Loss: 0.0570816783583723, Final Batch Loss: 0.021685566753149033\n",
      "Epoch 1020, Loss: 0.06824090180452913, Final Batch Loss: 0.0005979606648907065\n",
      "Epoch 1021, Loss: 0.060862035024911165, Final Batch Loss: 0.005321747623383999\n",
      "Epoch 1022, Loss: 0.10019155667396262, Final Batch Loss: 0.0027922773733735085\n",
      "Epoch 1023, Loss: 0.0816699598217383, Final Batch Loss: 0.006324589252471924\n",
      "Epoch 1024, Loss: 0.10022879345342517, Final Batch Loss: 0.0012104737106710672\n",
      "Epoch 1025, Loss: 0.09330071249860339, Final Batch Loss: 0.005616036709398031\n",
      "Epoch 1026, Loss: 0.07999755002674647, Final Batch Loss: 0.00092937215231359\n",
      "Epoch 1027, Loss: 0.0424816181184724, Final Batch Loss: 0.016285181045532227\n",
      "Epoch 1028, Loss: 0.08996916533214971, Final Batch Loss: 0.010724276304244995\n",
      "Epoch 1029, Loss: 0.06291370165126864, Final Batch Loss: 0.008180704899132252\n",
      "Epoch 1030, Loss: 0.03746433649212122, Final Batch Loss: 0.005161082837730646\n",
      "Epoch 1031, Loss: 0.12457024723698851, Final Batch Loss: 0.018250951543450356\n",
      "Epoch 1032, Loss: 0.06675373335019685, Final Batch Loss: 0.00353543390519917\n",
      "Epoch 1033, Loss: 0.02607214126328472, Final Batch Loss: 0.00018519659352023154\n",
      "Epoch 1034, Loss: 0.029029689212620724, Final Batch Loss: 0.00043025866034440696\n",
      "Epoch 1035, Loss: 0.055719113544910215, Final Batch Loss: 0.0007559399236924946\n",
      "Epoch 1036, Loss: 0.03145878275972791, Final Batch Loss: 0.005084678530693054\n",
      "Epoch 1037, Loss: 0.03782476607011631, Final Batch Loss: 0.00013445215881802142\n",
      "Epoch 1038, Loss: 0.04873804475937504, Final Batch Loss: 0.00018886728503275663\n",
      "Epoch 1039, Loss: 0.07585616475262213, Final Batch Loss: 0.00046571105485782027\n",
      "Epoch 1040, Loss: 0.05973409168655053, Final Batch Loss: 0.0013836832949891686\n",
      "Epoch 1041, Loss: 0.12624139222316444, Final Batch Loss: 0.028722230345010757\n",
      "Epoch 1042, Loss: 0.06132188047922682, Final Batch Loss: 0.00020753037824761122\n",
      "Epoch 1043, Loss: 0.0614278283319436, Final Batch Loss: 0.0029501363169401884\n",
      "Epoch 1044, Loss: 0.05277351848781109, Final Batch Loss: 0.000828527903649956\n",
      "Epoch 1045, Loss: 0.09502008184790611, Final Batch Loss: 0.004779889713972807\n",
      "Epoch 1046, Loss: 0.05356661236146465, Final Batch Loss: 0.003595308866351843\n",
      "Epoch 1047, Loss: 0.021137756702955812, Final Batch Loss: 0.001491841278038919\n",
      "Epoch 1048, Loss: 0.026287092390703037, Final Batch Loss: 0.00034805163159035146\n",
      "Epoch 1049, Loss: 0.0675210143381264, Final Batch Loss: 0.014753127470612526\n",
      "Epoch 1050, Loss: 0.09711938488180749, Final Batch Loss: 0.017189644277095795\n",
      "Epoch 1051, Loss: 0.05163563048699871, Final Batch Loss: 0.0054226587526500225\n",
      "Epoch 1052, Loss: 0.020794810236111516, Final Batch Loss: 0.0015081416349858046\n",
      "Epoch 1053, Loss: 0.1252401102829026, Final Batch Loss: 0.0003429602656979114\n",
      "Epoch 1054, Loss: 0.17270664748502895, Final Batch Loss: 0.02371387928724289\n",
      "Epoch 1055, Loss: 0.1545280443970114, Final Batch Loss: 0.0031084269285202026\n",
      "Epoch 1056, Loss: 0.11092020565411076, Final Batch Loss: 0.0032233144156634808\n",
      "Epoch 1057, Loss: 0.04871852684300393, Final Batch Loss: 0.004461344331502914\n",
      "Epoch 1058, Loss: 0.049701611045747995, Final Batch Loss: 0.0006091124378144741\n",
      "Epoch 1059, Loss: 0.07615198817802593, Final Batch Loss: 0.0005557068507187068\n",
      "Epoch 1060, Loss: 0.04620982875349, Final Batch Loss: 0.0005346638499759138\n",
      "Epoch 1061, Loss: 0.033572177577298135, Final Batch Loss: 0.0016270747873932123\n",
      "Epoch 1062, Loss: 0.05782551549782511, Final Batch Loss: 0.008545923978090286\n",
      "Epoch 1063, Loss: 0.09669335445505567, Final Batch Loss: 0.0003600096679292619\n",
      "Epoch 1064, Loss: 0.035156614205334336, Final Batch Loss: 0.0018214137526229024\n",
      "Epoch 1065, Loss: 0.08298268963699229, Final Batch Loss: 0.00042786975973285735\n",
      "Epoch 1066, Loss: 0.048079600783239584, Final Batch Loss: 0.0005080507253296673\n",
      "Epoch 1067, Loss: 0.03428623778745532, Final Batch Loss: 0.001784421387128532\n",
      "Epoch 1068, Loss: 0.08889359293971211, Final Batch Loss: 0.010782228782773018\n",
      "Epoch 1069, Loss: 0.0699275402148487, Final Batch Loss: 0.00034070032415911555\n",
      "Epoch 1070, Loss: 0.04432716759038158, Final Batch Loss: 0.0031034161802381277\n",
      "Epoch 1071, Loss: 0.02754752821783768, Final Batch Loss: 0.00860521662980318\n",
      "Epoch 1072, Loss: 0.07304431195370853, Final Batch Loss: 0.0028325943276286125\n",
      "Epoch 1073, Loss: 0.04660310677718371, Final Batch Loss: 0.00015837213140912354\n",
      "Epoch 1074, Loss: 0.1026880744175287, Final Batch Loss: 0.016434481367468834\n",
      "Epoch 1075, Loss: 0.06850847390887793, Final Batch Loss: 0.0002714789879973978\n",
      "Epoch 1076, Loss: 0.03446185318171047, Final Batch Loss: 0.00341798341833055\n",
      "Epoch 1077, Loss: 0.021557262472924776, Final Batch Loss: 0.00026875740149989724\n",
      "Epoch 1078, Loss: 0.025552024599164724, Final Batch Loss: 0.00146301940549165\n",
      "Epoch 1079, Loss: 0.03816975190420635, Final Batch Loss: 0.0033643976785242558\n",
      "Epoch 1080, Loss: 0.019728847284568474, Final Batch Loss: 0.0014507322339341044\n",
      "Epoch 1081, Loss: 0.021826913463883102, Final Batch Loss: 0.00016411278920713812\n",
      "Epoch 1082, Loss: 0.02318767942051636, Final Batch Loss: 0.0023542025592178106\n",
      "Epoch 1083, Loss: 0.0332311292149825, Final Batch Loss: 0.006993772927671671\n",
      "Epoch 1084, Loss: 0.06279055225240882, Final Batch Loss: 0.0009297109791077673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1085, Loss: 0.04438550508348271, Final Batch Loss: 0.0009406866156496108\n",
      "Epoch 1086, Loss: 0.05943041291902773, Final Batch Loss: 0.004363815765827894\n",
      "Epoch 1087, Loss: 0.06002201881346991, Final Batch Loss: 0.029536636546254158\n",
      "Epoch 1088, Loss: 0.047685045006801374, Final Batch Loss: 0.0017088635358959436\n",
      "Epoch 1089, Loss: 0.038188720820471644, Final Batch Loss: 0.00016714792582206428\n",
      "Epoch 1090, Loss: 0.02116741449572146, Final Batch Loss: 0.00020971198682673275\n",
      "Epoch 1091, Loss: 0.042018277788884006, Final Batch Loss: 0.0001748902432154864\n",
      "Epoch 1092, Loss: 0.045351423032116145, Final Batch Loss: 0.004280205350369215\n",
      "Epoch 1093, Loss: 0.08276377449510619, Final Batch Loss: 0.00032737755100242794\n",
      "Epoch 1094, Loss: 0.11366881220601499, Final Batch Loss: 0.00044503534445539117\n",
      "Epoch 1095, Loss: 0.10602899867808446, Final Batch Loss: 0.0022266891319304705\n",
      "Epoch 1096, Loss: 0.08066132862586528, Final Batch Loss: 0.007003066595643759\n",
      "Epoch 1097, Loss: 0.10238026769366115, Final Batch Loss: 0.002880699699744582\n",
      "Epoch 1098, Loss: 0.07230867243197281, Final Batch Loss: 0.00039922219002619386\n",
      "Epoch 1099, Loss: 0.03257601124641951, Final Batch Loss: 0.004613212775439024\n",
      "Epoch 1100, Loss: 0.05331812216900289, Final Batch Loss: 0.0013873400166630745\n",
      "Epoch 1101, Loss: 0.04612214851658791, Final Batch Loss: 0.006253979168832302\n",
      "Epoch 1102, Loss: 0.040721379351452924, Final Batch Loss: 0.0015830624615773559\n",
      "Epoch 1103, Loss: 0.01743075213744305, Final Batch Loss: 0.0014766580425202847\n",
      "Epoch 1104, Loss: 0.05022470398398582, Final Batch Loss: 0.003707235911861062\n",
      "Epoch 1105, Loss: 0.05890779611945618, Final Batch Loss: 0.0046686045825481415\n",
      "Epoch 1106, Loss: 0.06048251506581437, Final Batch Loss: 0.008412720635533333\n",
      "Epoch 1107, Loss: 0.07597752258880064, Final Batch Loss: 0.004102008882910013\n",
      "Epoch 1108, Loss: 0.06284784372837748, Final Batch Loss: 0.001828172942623496\n",
      "Epoch 1109, Loss: 0.08749274411820807, Final Batch Loss: 0.00045445363502949476\n",
      "Epoch 1110, Loss: 0.09491148544475436, Final Batch Loss: 0.0009145017247647047\n",
      "Epoch 1111, Loss: 0.09909964515827596, Final Batch Loss: 0.0013034823350608349\n",
      "Epoch 1112, Loss: 0.03840457958722254, Final Batch Loss: 0.007150543853640556\n",
      "Epoch 1113, Loss: 0.058714224709547125, Final Batch Loss: 0.0006492816610261798\n",
      "Epoch 1114, Loss: 0.0865987031138502, Final Batch Loss: 0.0006825598538853228\n",
      "Epoch 1115, Loss: 0.04703263589181006, Final Batch Loss: 0.0018052784726023674\n",
      "Epoch 1116, Loss: 0.03746958586998517, Final Batch Loss: 0.0009435246465727687\n",
      "Epoch 1117, Loss: 0.08665285700408276, Final Batch Loss: 0.0017774237785488367\n",
      "Epoch 1118, Loss: 0.04545093115302734, Final Batch Loss: 0.0003076240827795118\n",
      "Epoch 1119, Loss: 0.03158072629958042, Final Batch Loss: 0.00038181428681127727\n",
      "Epoch 1120, Loss: 0.1686621522239875, Final Batch Loss: 0.02285543642938137\n",
      "Epoch 1121, Loss: 0.05502376862568781, Final Batch Loss: 0.002244059694930911\n",
      "Epoch 1122, Loss: 0.06728943966299994, Final Batch Loss: 0.008824915625154972\n",
      "Epoch 1123, Loss: 0.05353250270127319, Final Batch Loss: 0.0005599291762337089\n",
      "Epoch 1124, Loss: 0.06115432373189833, Final Batch Loss: 0.009014410898089409\n",
      "Epoch 1125, Loss: 0.133514738176018, Final Batch Loss: 0.008436518721282482\n",
      "Epoch 1126, Loss: 0.17157033658077125, Final Batch Loss: 0.03234391659498215\n",
      "Epoch 1127, Loss: 0.10884535341756418, Final Batch Loss: 0.011864911764860153\n",
      "Epoch 1128, Loss: 0.04684164279024117, Final Batch Loss: 0.0025708614848554134\n",
      "Epoch 1129, Loss: 0.047380889707710594, Final Batch Loss: 0.0013800646411255002\n",
      "Epoch 1130, Loss: 0.06859136396087706, Final Batch Loss: 0.02703317627310753\n",
      "Epoch 1131, Loss: 0.043096559078549035, Final Batch Loss: 0.00015190262638498098\n",
      "Epoch 1132, Loss: 0.05579200395732187, Final Batch Loss: 0.0005510484334081411\n",
      "Epoch 1133, Loss: 0.04294244283664739, Final Batch Loss: 0.0017029403243213892\n",
      "Epoch 1134, Loss: 0.04445231305726338, Final Batch Loss: 0.007176460698246956\n",
      "Epoch 1135, Loss: 0.08828934555640444, Final Batch Loss: 0.0007829672540538013\n",
      "Epoch 1136, Loss: 0.05979941374971531, Final Batch Loss: 0.018422629684209824\n",
      "Epoch 1137, Loss: 0.05529964528250275, Final Batch Loss: 0.004320797510445118\n",
      "Epoch 1138, Loss: 0.07747924556315411, Final Batch Loss: 0.004147429950535297\n",
      "Epoch 1139, Loss: 0.12304861377924681, Final Batch Loss: 0.0037321855779737234\n",
      "Epoch 1140, Loss: 0.08409507758915424, Final Batch Loss: 0.003018307965248823\n",
      "Epoch 1141, Loss: 0.047062982484931126, Final Batch Loss: 0.0003147688985336572\n",
      "Epoch 1142, Loss: 0.07117042725440115, Final Batch Loss: 0.0017664531478658319\n",
      "Epoch 1143, Loss: 0.07700369175290689, Final Batch Loss: 0.0008849116857163608\n",
      "Epoch 1144, Loss: 0.06495865975739434, Final Batch Loss: 0.0273158997297287\n",
      "Epoch 1145, Loss: 0.06017553634592332, Final Batch Loss: 0.0003665358235593885\n",
      "Epoch 1146, Loss: 0.07098503829911351, Final Batch Loss: 0.0009900745935738087\n",
      "Epoch 1147, Loss: 0.043499895225977525, Final Batch Loss: 0.006289293058216572\n",
      "Epoch 1148, Loss: 0.03954550466733053, Final Batch Loss: 0.0002569176140241325\n",
      "Epoch 1149, Loss: 0.05436082259984687, Final Batch Loss: 0.0034388243220746517\n",
      "Epoch 1150, Loss: 0.03166615928057581, Final Batch Loss: 0.0023834563326090574\n",
      "Epoch 1151, Loss: 0.02830585723859258, Final Batch Loss: 0.0009287702850997448\n",
      "Epoch 1152, Loss: 0.027870637248270214, Final Batch Loss: 0.0021809800527989864\n",
      "Epoch 1153, Loss: 0.02178948765504174, Final Batch Loss: 0.0009035330731421709\n",
      "Epoch 1154, Loss: 0.06102363125683041, Final Batch Loss: 0.0004476903413888067\n",
      "Epoch 1155, Loss: 0.05007276588003151, Final Batch Loss: 0.0005075581138953567\n",
      "Epoch 1156, Loss: 0.04211119819956366, Final Batch Loss: 0.002958426484838128\n",
      "Epoch 1157, Loss: 0.04190648107032757, Final Batch Loss: 0.003021672135218978\n",
      "Epoch 1158, Loss: 0.07062723628769163, Final Batch Loss: 0.0002922214916907251\n",
      "Epoch 1159, Loss: 0.1126690497767413, Final Batch Loss: 0.007111386861652136\n",
      "Epoch 1160, Loss: 0.016944566799793392, Final Batch Loss: 0.0035099578090012074\n",
      "Epoch 1161, Loss: 0.049414739944040775, Final Batch Loss: 0.0005328547558747232\n",
      "Epoch 1162, Loss: 0.05342187505448237, Final Batch Loss: 0.0005616527050733566\n",
      "Epoch 1163, Loss: 0.09326470726227853, Final Batch Loss: 0.002725259168073535\n",
      "Epoch 1164, Loss: 0.053772522893268615, Final Batch Loss: 0.0006006601033732295\n",
      "Epoch 1165, Loss: 0.10559020732034696, Final Batch Loss: 0.0018282198579981923\n",
      "Epoch 1166, Loss: 0.07835628162138164, Final Batch Loss: 0.0022050405386835337\n",
      "Epoch 1167, Loss: 0.08153642027173191, Final Batch Loss: 0.012923283502459526\n",
      "Epoch 1168, Loss: 0.03386980699724518, Final Batch Loss: 0.00229239952750504\n",
      "Epoch 1169, Loss: 0.036992376524722204, Final Batch Loss: 0.0038823201321065426\n",
      "Epoch 1170, Loss: 0.027439459270681255, Final Batch Loss: 0.00023125814914237708\n",
      "Epoch 1171, Loss: 0.047628087384509854, Final Batch Loss: 0.006846197415143251\n",
      "Epoch 1172, Loss: 0.026974406602676027, Final Batch Loss: 0.0004446972452569753\n",
      "Epoch 1173, Loss: 0.025320145214209333, Final Batch Loss: 0.0005401636008173227\n",
      "Epoch 1174, Loss: 0.021516829416214023, Final Batch Loss: 0.0008180812001228333\n",
      "Epoch 1175, Loss: 0.018367910815868527, Final Batch Loss: 0.00016751504153944552\n",
      "Epoch 1176, Loss: 0.12013224185284344, Final Batch Loss: 0.0003577534225769341\n",
      "Epoch 1177, Loss: 0.037450346841069404, Final Batch Loss: 0.0003464642504695803\n",
      "Epoch 1178, Loss: 0.0346353384302347, Final Batch Loss: 0.004146638326346874\n",
      "Epoch 1179, Loss: 0.04487370277638547, Final Batch Loss: 0.0023192139342427254\n",
      "Epoch 1180, Loss: 0.021376994322054088, Final Batch Loss: 0.0008371791336685419\n",
      "Epoch 1181, Loss: 0.04511640746204648, Final Batch Loss: 0.004553481470793486\n",
      "Epoch 1182, Loss: 0.030288254842162132, Final Batch Loss: 0.000612447620369494\n",
      "Epoch 1183, Loss: 0.04819759665406309, Final Batch Loss: 0.004998701624572277\n",
      "Epoch 1184, Loss: 0.038330314062477555, Final Batch Loss: 0.0017213415121659636\n",
      "Epoch 1185, Loss: 0.05998446900048293, Final Batch Loss: 0.017072515562176704\n",
      "Epoch 1186, Loss: 0.049772301892517135, Final Batch Loss: 0.007369466591626406\n",
      "Epoch 1187, Loss: 0.06917922971479129, Final Batch Loss: 0.0012400689302012324\n",
      "Epoch 1188, Loss: 0.09234204760286957, Final Batch Loss: 0.0038894929457455873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1189, Loss: 0.05484189037815668, Final Batch Loss: 0.0007712783990427852\n",
      "Epoch 1190, Loss: 0.05175406456692144, Final Batch Loss: 0.00011764009104808792\n",
      "Epoch 1191, Loss: 0.055571421777131036, Final Batch Loss: 0.0021035890094935894\n",
      "Epoch 1192, Loss: 0.053872201096965, Final Batch Loss: 0.0001886355603346601\n",
      "Epoch 1193, Loss: 0.07497723825508729, Final Batch Loss: 0.04136916995048523\n",
      "Epoch 1194, Loss: 0.060782006999943405, Final Batch Loss: 0.0008955187513493001\n",
      "Epoch 1195, Loss: 0.04119183213333599, Final Batch Loss: 0.00033411537879146636\n",
      "Epoch 1196, Loss: 0.02595396102697123, Final Batch Loss: 0.0002000264939852059\n",
      "Epoch 1197, Loss: 0.03558882008655928, Final Batch Loss: 0.0024000403936952353\n",
      "Epoch 1198, Loss: 0.06440062915498856, Final Batch Loss: 0.004886864218860865\n",
      "Epoch 1199, Loss: 0.07414391991187586, Final Batch Loss: 0.03880063444375992\n",
      "Epoch 1200, Loss: 0.24460833439661656, Final Batch Loss: 0.14849337935447693\n",
      "Epoch 1201, Loss: 0.22306855162605643, Final Batch Loss: 0.0014567605685442686\n",
      "Epoch 1202, Loss: 0.09506812787731178, Final Batch Loss: 0.007868134416639805\n",
      "Epoch 1203, Loss: 0.09000680258031934, Final Batch Loss: 0.006223376374691725\n",
      "Epoch 1204, Loss: 0.04070872362353839, Final Batch Loss: 0.010350369848310947\n",
      "Epoch 1205, Loss: 0.03934660011145752, Final Batch Loss: 0.01184109691530466\n",
      "Epoch 1206, Loss: 0.056069806989398785, Final Batch Loss: 0.004360811319202185\n",
      "Epoch 1207, Loss: 0.030943832454795483, Final Batch Loss: 9.922689787345007e-05\n",
      "Epoch 1208, Loss: 0.03825128183962079, Final Batch Loss: 0.00010370932432124391\n",
      "Epoch 1209, Loss: 0.05168631169362925, Final Batch Loss: 0.002348194131627679\n",
      "Epoch 1210, Loss: 0.03614212563843466, Final Batch Loss: 0.0005547462496906519\n",
      "Epoch 1211, Loss: 0.0700714635750046, Final Batch Loss: 0.003698880085721612\n",
      "Epoch 1212, Loss: 0.0440425635460997, Final Batch Loss: 0.003517505247145891\n",
      "Epoch 1213, Loss: 0.032198875218455214, Final Batch Loss: 0.0011458368971943855\n",
      "Epoch 1214, Loss: 0.0815900040033739, Final Batch Loss: 0.0007885418017394841\n",
      "Epoch 1215, Loss: 0.024781605054158717, Final Batch Loss: 0.0007859590696170926\n",
      "Epoch 1216, Loss: 0.049728047917596996, Final Batch Loss: 0.0017534135840833187\n",
      "Epoch 1217, Loss: 0.05679227091604844, Final Batch Loss: 0.003195987083017826\n",
      "Epoch 1218, Loss: 0.027974670978437643, Final Batch Loss: 0.00030084795434959233\n",
      "Epoch 1219, Loss: 0.09489144466351718, Final Batch Loss: 0.0009810993215069175\n",
      "Epoch 1220, Loss: 0.04396948352223262, Final Batch Loss: 0.0003007430350407958\n",
      "Epoch 1221, Loss: 0.05315832010819577, Final Batch Loss: 0.003358810441568494\n",
      "Epoch 1222, Loss: 0.038697017764206976, Final Batch Loss: 0.00035568795283325016\n",
      "Epoch 1223, Loss: 0.11467279114003759, Final Batch Loss: 0.0021513630636036396\n",
      "Epoch 1224, Loss: 0.0530788589749136, Final Batch Loss: 0.0017635638359934092\n",
      "Epoch 1225, Loss: 0.03958673088345677, Final Batch Loss: 0.0008623332832939923\n",
      "Epoch 1226, Loss: 0.03893269361287821, Final Batch Loss: 0.0012018091510981321\n",
      "Epoch 1227, Loss: 0.023679374789935537, Final Batch Loss: 0.0012833119835704565\n",
      "Epoch 1228, Loss: 0.02776048237865325, Final Batch Loss: 0.00939498282968998\n",
      "Epoch 1229, Loss: 0.05115134146763012, Final Batch Loss: 0.014186374843120575\n",
      "Epoch 1230, Loss: 0.11780200822977349, Final Batch Loss: 0.0021820717956870794\n",
      "Epoch 1231, Loss: 0.10442559001967311, Final Batch Loss: 0.004142931196838617\n",
      "Epoch 1232, Loss: 0.09436145739164203, Final Batch Loss: 0.004143666010349989\n",
      "Epoch 1233, Loss: 0.03892207078752108, Final Batch Loss: 0.005080629605799913\n",
      "Epoch 1234, Loss: 0.04865251510636881, Final Batch Loss: 0.0011601642472669482\n",
      "Epoch 1235, Loss: 0.02524533317773603, Final Batch Loss: 0.00039244969957508147\n",
      "Epoch 1236, Loss: 0.09945416061964352, Final Batch Loss: 0.04731476679444313\n",
      "Epoch 1237, Loss: 0.07133681219420396, Final Batch Loss: 0.006882914807647467\n",
      "Epoch 1238, Loss: 0.09195085738610942, Final Batch Loss: 0.0014278359012678266\n",
      "Epoch 1239, Loss: 0.0648628390772501, Final Batch Loss: 0.008875735104084015\n",
      "Epoch 1240, Loss: 0.08132624250720255, Final Batch Loss: 0.006956439930945635\n",
      "Epoch 1241, Loss: 0.041753798970603384, Final Batch Loss: 0.0019431666005402803\n",
      "Epoch 1242, Loss: 0.02225412598636467, Final Batch Loss: 0.0009029789944179356\n",
      "Epoch 1243, Loss: 0.021969750683638267, Final Batch Loss: 0.0002850663149729371\n",
      "Epoch 1244, Loss: 0.059888267394853756, Final Batch Loss: 0.0004970186855643988\n",
      "Epoch 1245, Loss: 0.05845611291442765, Final Batch Loss: 0.0012291877064853907\n",
      "Epoch 1246, Loss: 0.11863036919385195, Final Batch Loss: 0.0014946027658879757\n",
      "Epoch 1247, Loss: 0.22148518112953752, Final Batch Loss: 0.01045753713697195\n",
      "Epoch 1248, Loss: 0.08330529020167887, Final Batch Loss: 0.0021677992772310972\n",
      "Epoch 1249, Loss: 0.04807569328113459, Final Batch Loss: 0.0036002194974571466\n",
      "Epoch 1250, Loss: 0.04515833896584809, Final Batch Loss: 0.0009208882111124694\n",
      "Epoch 1251, Loss: 0.04987072436779272, Final Batch Loss: 0.0017915373900905252\n",
      "Epoch 1252, Loss: 0.038044416083721444, Final Batch Loss: 0.004782920703291893\n",
      "Epoch 1253, Loss: 0.05598175572231412, Final Batch Loss: 0.0009631455177441239\n",
      "Epoch 1254, Loss: 0.024223909291322343, Final Batch Loss: 0.00015722594980616122\n",
      "Epoch 1255, Loss: 0.05115880122320959, Final Batch Loss: 0.0003282950783614069\n",
      "Epoch 1256, Loss: 0.04046652698889375, Final Batch Loss: 0.0008344569359906018\n",
      "Epoch 1257, Loss: 0.04702137354252045, Final Batch Loss: 0.0003087049990426749\n",
      "Epoch 1258, Loss: 0.07268245158775244, Final Batch Loss: 0.00037986814277246594\n",
      "Epoch 1259, Loss: 0.0369464629329741, Final Batch Loss: 0.0021500480361282825\n",
      "Epoch 1260, Loss: 0.03862932576157618, Final Batch Loss: 0.004532902967184782\n",
      "Epoch 1261, Loss: 0.04205624005408026, Final Batch Loss: 0.0005325272213667631\n",
      "Epoch 1262, Loss: 0.02559296556864865, Final Batch Loss: 0.0006843458977527916\n",
      "Epoch 1263, Loss: 0.03910307569458382, Final Batch Loss: 0.00017968060274142772\n",
      "Epoch 1264, Loss: 0.028228375711478293, Final Batch Loss: 0.00017496320651844144\n",
      "Epoch 1265, Loss: 0.02167653820652049, Final Batch Loss: 0.00038843549555167556\n",
      "Epoch 1266, Loss: 0.04268220035737613, Final Batch Loss: 0.0016849545063450933\n",
      "Epoch 1267, Loss: 0.035533766087610275, Final Batch Loss: 0.007459976710379124\n",
      "Epoch 1268, Loss: 0.011378199211321771, Final Batch Loss: 0.00011955421359743923\n",
      "Epoch 1269, Loss: 0.07218702775571728, Final Batch Loss: 0.0015515947015956044\n",
      "Epoch 1270, Loss: 0.02321208463399671, Final Batch Loss: 0.0009917470160871744\n",
      "Epoch 1271, Loss: 0.01888744362804573, Final Batch Loss: 7.6486830948852e-05\n",
      "Epoch 1272, Loss: 0.025472339199041016, Final Batch Loss: 0.0005924762226641178\n",
      "Epoch 1273, Loss: 0.02728462290542666, Final Batch Loss: 0.0003377061802893877\n",
      "Epoch 1274, Loss: 0.01903921607299708, Final Batch Loss: 0.0005126544856466353\n",
      "Epoch 1275, Loss: 0.030148074409225956, Final Batch Loss: 0.0024722458329051733\n",
      "Epoch 1276, Loss: 0.0748583626555046, Final Batch Loss: 0.00026957603404298425\n",
      "Epoch 1277, Loss: 0.04826943585067056, Final Batch Loss: 0.0006238975911401212\n",
      "Epoch 1278, Loss: 0.04341189986735117, Final Batch Loss: 0.005639806389808655\n",
      "Epoch 1279, Loss: 0.04329045292979572, Final Batch Loss: 0.0005777321639470756\n",
      "Epoch 1280, Loss: 0.04927146516274661, Final Batch Loss: 0.0001196687517222017\n",
      "Epoch 1281, Loss: 0.0424110570056655, Final Batch Loss: 3.934910273528658e-05\n",
      "Epoch 1282, Loss: 0.016833959805808263, Final Batch Loss: 0.0006994875730015337\n",
      "Epoch 1283, Loss: 0.011455654097517254, Final Batch Loss: 0.00024060547002591193\n",
      "Epoch 1284, Loss: 0.01854023001578753, Final Batch Loss: 0.0011726273223757744\n",
      "Epoch 1285, Loss: 0.03489434165385319, Final Batch Loss: 8.863847324391827e-05\n",
      "Epoch 1286, Loss: 0.017412468127076863, Final Batch Loss: 0.00044644606532528996\n",
      "Epoch 1287, Loss: 0.012237428445587284, Final Batch Loss: 0.001332564977928996\n",
      "Epoch 1288, Loss: 0.05748006336216349, Final Batch Loss: 0.002085787011310458\n",
      "Epoch 1289, Loss: 0.049552278404007666, Final Batch Loss: 0.00011444282426964492\n",
      "Epoch 1290, Loss: 0.030099694457021542, Final Batch Loss: 0.00032281081075780094\n",
      "Epoch 1291, Loss: 0.058911369342240505, Final Batch Loss: 0.006645853165537119\n",
      "Epoch 1292, Loss: 0.0890654889808502, Final Batch Loss: 0.007894166745245457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1293, Loss: 0.025751386914635077, Final Batch Loss: 0.0030827459413558245\n",
      "Epoch 1294, Loss: 0.02845808246638626, Final Batch Loss: 0.004724455066025257\n",
      "Epoch 1295, Loss: 0.0241968042391818, Final Batch Loss: 0.00013502771616913378\n",
      "Epoch 1296, Loss: 0.03604226536117494, Final Batch Loss: 0.0033190737012773752\n",
      "Epoch 1297, Loss: 0.014568382510333322, Final Batch Loss: 0.00034425314515829086\n",
      "Epoch 1298, Loss: 0.03920370286505204, Final Batch Loss: 0.0031836829148232937\n",
      "Epoch 1299, Loss: 0.03601451359281782, Final Batch Loss: 0.003820423735305667\n",
      "Epoch 1300, Loss: 0.011159726840560324, Final Batch Loss: 0.0005609220243059099\n",
      "Epoch 1301, Loss: 0.016107086557894945, Final Batch Loss: 0.00020589650375768542\n",
      "Epoch 1302, Loss: 0.06911163201039017, Final Batch Loss: 0.00027797301299870014\n",
      "Epoch 1303, Loss: 0.01528002563281916, Final Batch Loss: 0.0005828352877870202\n",
      "Epoch 1304, Loss: 0.042662849387852475, Final Batch Loss: 0.00032000994542613626\n",
      "Epoch 1305, Loss: 0.027515491368831135, Final Batch Loss: 0.00019706129387486726\n",
      "Epoch 1306, Loss: 0.03145052095351275, Final Batch Loss: 0.01182884257286787\n",
      "Epoch 1307, Loss: 0.007704071944317548, Final Batch Loss: 0.0002399344666628167\n",
      "Epoch 1308, Loss: 0.03574210034639691, Final Batch Loss: 0.00042178784497082233\n",
      "Epoch 1309, Loss: 0.05428255548758898, Final Batch Loss: 0.0006562069756910205\n",
      "Epoch 1310, Loss: 0.15055243019014597, Final Batch Loss: 0.009001449681818485\n",
      "Epoch 1311, Loss: 0.1683126387797529, Final Batch Loss: 0.001432370743714273\n",
      "Epoch 1312, Loss: 0.1366562852053903, Final Batch Loss: 0.0013574345503002405\n",
      "Epoch 1313, Loss: 0.05285544785147067, Final Batch Loss: 0.00551906181499362\n",
      "Epoch 1314, Loss: 0.0709404255321715, Final Batch Loss: 0.0025183677207678556\n",
      "Epoch 1315, Loss: 0.02828682783001568, Final Batch Loss: 0.00640134559944272\n",
      "Epoch 1316, Loss: 0.08967978990403935, Final Batch Loss: 0.007970837876200676\n",
      "Epoch 1317, Loss: 0.04126541793812066, Final Batch Loss: 0.0012153171701356769\n",
      "Epoch 1318, Loss: 0.0890536366350716, Final Batch Loss: 0.04525730386376381\n",
      "Epoch 1319, Loss: 0.041267415799666196, Final Batch Loss: 0.0022753248922526836\n",
      "Epoch 1320, Loss: 0.043495804522535764, Final Batch Loss: 0.0005856907810084522\n",
      "Epoch 1321, Loss: 0.025116136675933376, Final Batch Loss: 0.00023528948077000678\n",
      "Epoch 1322, Loss: 0.021457473514601588, Final Batch Loss: 0.0003473233082331717\n",
      "Epoch 1323, Loss: 0.012304833391681314, Final Batch Loss: 0.00020361893984954804\n",
      "Epoch 1324, Loss: 0.010953760822303593, Final Batch Loss: 0.00023748542298562825\n",
      "Epoch 1325, Loss: 0.038372488277673256, Final Batch Loss: 0.0030486308969557285\n",
      "Epoch 1326, Loss: 0.05612163413024973, Final Batch Loss: 0.0008771427092142403\n",
      "Epoch 1327, Loss: 0.03259139123838395, Final Batch Loss: 0.004394848830997944\n",
      "Epoch 1328, Loss: 0.022652726918749977, Final Batch Loss: 0.0006953160627745092\n",
      "Epoch 1329, Loss: 0.21948821510886773, Final Batch Loss: 0.1575905680656433\n",
      "Epoch 1330, Loss: 0.051703232908039354, Final Batch Loss: 0.003747486975044012\n",
      "Epoch 1331, Loss: 0.08759669115534052, Final Batch Loss: 0.0023401028010994196\n",
      "Epoch 1332, Loss: 0.042899264430161566, Final Batch Loss: 0.0014476721407845616\n",
      "Epoch 1333, Loss: 0.06673781701829284, Final Batch Loss: 0.013863341882824898\n",
      "Epoch 1334, Loss: 0.04052739105827641, Final Batch Loss: 0.00033786657149903476\n",
      "Epoch 1335, Loss: 0.0491842801857274, Final Batch Loss: 0.000507407879922539\n",
      "Epoch 1336, Loss: 0.03546345244103577, Final Batch Loss: 0.00035455109900794923\n",
      "Epoch 1337, Loss: 0.11731108048115857, Final Batch Loss: 0.014661162160336971\n",
      "Epoch 1338, Loss: 0.037833954105735756, Final Batch Loss: 0.00021773292974103242\n",
      "Epoch 1339, Loss: 0.025494066576357, Final Batch Loss: 0.0002363013190915808\n",
      "Epoch 1340, Loss: 0.058346879966848064, Final Batch Loss: 0.012896748259663582\n",
      "Epoch 1341, Loss: 0.023837716013076715, Final Batch Loss: 0.0001473095762776211\n",
      "Epoch 1342, Loss: 0.01989891449920833, Final Batch Loss: 0.002298048697412014\n",
      "Epoch 1343, Loss: 0.039180479587230366, Final Batch Loss: 0.0011638730065897107\n",
      "Epoch 1344, Loss: 0.06208434622385539, Final Batch Loss: 0.00039910184568725526\n",
      "Epoch 1345, Loss: 0.03587528670323081, Final Batch Loss: 0.0003149195690639317\n",
      "Epoch 1346, Loss: 0.037227280874503776, Final Batch Loss: 0.0003657496126834303\n",
      "Epoch 1347, Loss: 0.03616930094722193, Final Batch Loss: 0.0004899982595816255\n",
      "Epoch 1348, Loss: 0.01255496062367456, Final Batch Loss: 0.0004989132867194712\n",
      "Epoch 1349, Loss: 0.025420635458431207, Final Batch Loss: 0.002557559870183468\n",
      "Epoch 1350, Loss: 0.06595414038019953, Final Batch Loss: 0.00027564619085751474\n",
      "Epoch 1351, Loss: 0.02199004129943205, Final Batch Loss: 0.0018626933451741934\n",
      "Epoch 1352, Loss: 0.019708252082637046, Final Batch Loss: 6.915438279975206e-05\n",
      "Epoch 1353, Loss: 0.020164165034657344, Final Batch Loss: 0.0002467441954649985\n",
      "Epoch 1354, Loss: 0.02954030600085389, Final Batch Loss: 0.010888134129345417\n",
      "Epoch 1355, Loss: 0.040024145229835995, Final Batch Loss: 0.00035505887353792787\n",
      "Epoch 1356, Loss: 0.032279921404551715, Final Batch Loss: 0.00854638684540987\n",
      "Epoch 1357, Loss: 0.03897952145780437, Final Batch Loss: 0.0013851147377863526\n",
      "Epoch 1358, Loss: 0.08922231323231244, Final Batch Loss: 0.002742769429460168\n",
      "Epoch 1359, Loss: 0.10902710416849004, Final Batch Loss: 0.0028834203258156776\n",
      "Epoch 1360, Loss: 0.06366407171299215, Final Batch Loss: 0.005182291381061077\n",
      "Epoch 1361, Loss: 0.054322263749781996, Final Batch Loss: 0.0015263977693393826\n",
      "Epoch 1362, Loss: 0.03759124968200922, Final Batch Loss: 0.0010655110236257315\n",
      "Epoch 1363, Loss: 0.05257523000182118, Final Batch Loss: 0.001680948887951672\n",
      "Epoch 1364, Loss: 0.10382747696712613, Final Batch Loss: 0.0004247777978889644\n",
      "Epoch 1365, Loss: 0.0453233617299702, Final Batch Loss: 0.0003896913549397141\n",
      "Epoch 1366, Loss: 0.030185487688868307, Final Batch Loss: 0.0038699053693562746\n",
      "Epoch 1367, Loss: 0.07846190493364702, Final Batch Loss: 0.00020041050447616726\n",
      "Epoch 1368, Loss: 0.03190982373780571, Final Batch Loss: 0.002073385287076235\n",
      "Epoch 1369, Loss: 0.023240982096467633, Final Batch Loss: 0.0014187580673024058\n",
      "Epoch 1370, Loss: 0.05835724249482155, Final Batch Loss: 0.010424863547086716\n",
      "Epoch 1371, Loss: 0.016136148275109008, Final Batch Loss: 0.000295970676233992\n",
      "Epoch 1372, Loss: 0.013546951675380114, Final Batch Loss: 8.753594011068344e-05\n",
      "Epoch 1373, Loss: 0.03874276713031577, Final Batch Loss: 0.004075504373759031\n",
      "Epoch 1374, Loss: 0.062168978329282254, Final Batch Loss: 0.022165408357977867\n",
      "Epoch 1375, Loss: 0.057413104514125735, Final Batch Loss: 0.00781380757689476\n",
      "Epoch 1376, Loss: 0.09419777983566746, Final Batch Loss: 0.005816305056214333\n",
      "Epoch 1377, Loss: 0.05972025827213656, Final Batch Loss: 0.038375262171030045\n",
      "Epoch 1378, Loss: 0.08539760956773534, Final Batch Loss: 0.00018678884953260422\n",
      "Epoch 1379, Loss: 0.0787582453340292, Final Batch Loss: 0.004088844638317823\n",
      "Epoch 1380, Loss: 0.04247617986402474, Final Batch Loss: 0.0008070060866884887\n",
      "Epoch 1381, Loss: 0.09321361553156748, Final Batch Loss: 0.006083331536501646\n",
      "Epoch 1382, Loss: 0.06727853085612878, Final Batch Loss: 0.002152304630726576\n",
      "Epoch 1383, Loss: 0.05935679243702907, Final Batch Loss: 0.0012378999963402748\n",
      "Epoch 1384, Loss: 0.07251040416304022, Final Batch Loss: 0.0018890394130721688\n",
      "Epoch 1385, Loss: 0.07190505065955222, Final Batch Loss: 0.0038621528074145317\n",
      "Epoch 1386, Loss: 0.07514460326638073, Final Batch Loss: 0.009461029432713985\n",
      "Epoch 1387, Loss: 0.06909052598348353, Final Batch Loss: 0.0008810822619125247\n",
      "Epoch 1388, Loss: 0.04663724926649593, Final Batch Loss: 0.0005255924770608544\n",
      "Epoch 1389, Loss: 0.07196648704120889, Final Batch Loss: 0.011750803329050541\n",
      "Epoch 1390, Loss: 0.07046219220501371, Final Batch Loss: 0.0008323599467985332\n",
      "Epoch 1391, Loss: 0.021629669310641475, Final Batch Loss: 0.0005988586344756186\n",
      "Epoch 1392, Loss: 0.024265059386380017, Final Batch Loss: 0.0021976896096020937\n",
      "Epoch 1393, Loss: 0.026856970573135186, Final Batch Loss: 0.0004322103632148355\n",
      "Epoch 1394, Loss: 0.07643816444033291, Final Batch Loss: 0.03263338655233383\n",
      "Epoch 1395, Loss: 0.04494484164752066, Final Batch Loss: 0.003381412010639906\n",
      "Epoch 1396, Loss: 0.024343600089196116, Final Batch Loss: 0.0008213788387365639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1397, Loss: 0.0498090130276978, Final Batch Loss: 0.0006154446746222675\n",
      "Epoch 1398, Loss: 0.03799259882725892, Final Batch Loss: 3.801031562034041e-05\n",
      "Epoch 1399, Loss: 0.028860786151199136, Final Batch Loss: 0.002575801219791174\n",
      "Epoch 1400, Loss: 0.026752743320685113, Final Batch Loss: 0.00017912985640577972\n",
      "Epoch 1401, Loss: 0.035100627657811856, Final Batch Loss: 0.00014386553084477782\n",
      "Epoch 1402, Loss: 0.018295984249562025, Final Batch Loss: 0.001987610710784793\n",
      "Epoch 1403, Loss: 0.02850015192598221, Final Batch Loss: 0.0012631992576643825\n",
      "Epoch 1404, Loss: 0.056758017104584724, Final Batch Loss: 0.0017845683032646775\n",
      "Epoch 1405, Loss: 0.0309975247400871, Final Batch Loss: 0.0011388075072318316\n",
      "Epoch 1406, Loss: 0.023881617802544497, Final Batch Loss: 0.0026410154532641172\n",
      "Epoch 1407, Loss: 0.02785046715143835, Final Batch Loss: 0.00010662160639185458\n",
      "Epoch 1408, Loss: 0.03156775527168065, Final Batch Loss: 0.0050654541701078415\n",
      "Epoch 1409, Loss: 0.032311032176949084, Final Batch Loss: 0.0011875465279445052\n",
      "Epoch 1410, Loss: 0.05216458484210307, Final Batch Loss: 0.0006209706189110875\n",
      "Epoch 1411, Loss: 0.017307628171693068, Final Batch Loss: 5.008744483347982e-05\n",
      "Epoch 1412, Loss: 0.038724733458366245, Final Batch Loss: 0.0003712983161676675\n",
      "Epoch 1413, Loss: 0.04141531354980543, Final Batch Loss: 0.001790800946764648\n",
      "Epoch 1414, Loss: 0.025480453012278304, Final Batch Loss: 0.00028450190438888967\n",
      "Epoch 1415, Loss: 0.023857527165091597, Final Batch Loss: 0.001167637063190341\n",
      "Epoch 1416, Loss: 0.014998240221757442, Final Batch Loss: 0.0008322533103637397\n",
      "Epoch 1417, Loss: 0.05770284601021558, Final Batch Loss: 0.0012162550119683146\n",
      "Epoch 1418, Loss: 0.017287299684539903, Final Batch Loss: 0.005136176478117704\n",
      "Epoch 1419, Loss: 0.06349474528906285, Final Batch Loss: 0.004124624654650688\n",
      "Epoch 1420, Loss: 0.05391573546512518, Final Batch Loss: 0.0004187009471934289\n",
      "Epoch 1421, Loss: 0.02660522604128346, Final Batch Loss: 0.0002446566941216588\n",
      "Epoch 1422, Loss: 0.03364412538212491, Final Batch Loss: 0.00019224915013182908\n",
      "Epoch 1423, Loss: 0.059024105674325256, Final Batch Loss: 0.0001875733578344807\n",
      "Epoch 1424, Loss: 0.03419404929809389, Final Batch Loss: 3.369826663401909e-05\n",
      "Epoch 1425, Loss: 0.07628559262957424, Final Batch Loss: 0.003344786586239934\n",
      "Epoch 1426, Loss: 0.1159881001804024, Final Batch Loss: 0.001095025916583836\n",
      "Epoch 1427, Loss: 0.05425596868735738, Final Batch Loss: 0.005470249801874161\n",
      "Epoch 1428, Loss: 0.031626226467778906, Final Batch Loss: 0.002191303065046668\n",
      "Epoch 1429, Loss: 0.031038885208545253, Final Batch Loss: 0.00024124523042701185\n",
      "Epoch 1430, Loss: 0.06839558297360782, Final Batch Loss: 0.0001469927083235234\n",
      "Epoch 1431, Loss: 0.03610184253193438, Final Batch Loss: 0.0012613375438377261\n",
      "Epoch 1432, Loss: 0.03615056125272531, Final Batch Loss: 0.0029268055222928524\n",
      "Epoch 1433, Loss: 0.06488952691506711, Final Batch Loss: 0.0002935604134108871\n",
      "Epoch 1434, Loss: 0.0248999088362325, Final Batch Loss: 0.000564647139981389\n",
      "Epoch 1435, Loss: 0.010018795706855599, Final Batch Loss: 0.0006057627033442259\n",
      "Epoch 1436, Loss: 0.034775422027450986, Final Batch Loss: 0.00011760750930989161\n",
      "Epoch 1437, Loss: 0.02562107202174957, Final Batch Loss: 0.002658235374838114\n",
      "Epoch 1438, Loss: 0.06004230468170135, Final Batch Loss: 0.0019158339127898216\n",
      "Epoch 1439, Loss: 0.04404299256930244, Final Batch Loss: 0.0003315797366667539\n",
      "Epoch 1440, Loss: 0.03165280154644279, Final Batch Loss: 0.0002807894197758287\n",
      "Epoch 1441, Loss: 0.016200658148591174, Final Batch Loss: 0.00026274460833519697\n",
      "Epoch 1442, Loss: 0.02217108317563543, Final Batch Loss: 0.000156394176883623\n",
      "Epoch 1443, Loss: 0.04805177664820803, Final Batch Loss: 0.00012946894275955856\n",
      "Epoch 1444, Loss: 0.03194663221074734, Final Batch Loss: 0.0005620300071313977\n",
      "Epoch 1445, Loss: 0.012911184217955451, Final Batch Loss: 0.001360408728942275\n",
      "Epoch 1446, Loss: 0.06909290031762794, Final Batch Loss: 0.034952595829963684\n",
      "Epoch 1447, Loss: 0.01620699016348226, Final Batch Loss: 0.0016213329508900642\n",
      "Epoch 1448, Loss: 0.04593651807590504, Final Batch Loss: 0.002017579274252057\n",
      "Epoch 1449, Loss: 0.027383622553315945, Final Batch Loss: 0.0016215532086789608\n",
      "Epoch 1450, Loss: 0.09104027084322297, Final Batch Loss: 0.0008093869546428323\n",
      "Epoch 1451, Loss: 0.052101020177360624, Final Batch Loss: 0.006900955457240343\n",
      "Epoch 1452, Loss: 0.07207107328576967, Final Batch Loss: 0.0031048040837049484\n",
      "Epoch 1453, Loss: 0.052920387068297714, Final Batch Loss: 0.0013686703750863671\n",
      "Epoch 1454, Loss: 0.043341067706933245, Final Batch Loss: 0.007964943535625935\n",
      "Epoch 1455, Loss: 0.11405989502964076, Final Batch Loss: 0.006272799335420132\n",
      "Epoch 1456, Loss: 0.027050318909459747, Final Batch Loss: 0.0010567593853920698\n",
      "Epoch 1457, Loss: 0.038158723633387126, Final Batch Loss: 0.0005383712705224752\n",
      "Epoch 1458, Loss: 0.04469825228807167, Final Batch Loss: 0.0030236435122787952\n",
      "Epoch 1459, Loss: 0.06432391570706386, Final Batch Loss: 0.0024988590739667416\n",
      "Epoch 1460, Loss: 0.03762489957443904, Final Batch Loss: 0.00049734947970137\n",
      "Epoch 1461, Loss: 0.03283238675794564, Final Batch Loss: 0.0011461961548775434\n",
      "Epoch 1462, Loss: 0.04844293861242477, Final Batch Loss: 0.0004635327204596251\n",
      "Epoch 1463, Loss: 0.045618267518875655, Final Batch Loss: 0.0004147669533267617\n",
      "Epoch 1464, Loss: 0.045300034078536555, Final Batch Loss: 0.007515476085245609\n",
      "Epoch 1465, Loss: 0.04706454570259666, Final Batch Loss: 0.00018723655375652015\n",
      "Epoch 1466, Loss: 0.0715145572903566, Final Batch Loss: 0.0002645475324243307\n",
      "Epoch 1467, Loss: 0.03856845852715196, Final Batch Loss: 0.008082975633442402\n",
      "Epoch 1468, Loss: 0.0718434306254494, Final Batch Loss: 0.009559939615428448\n",
      "Epoch 1469, Loss: 0.0409603013395099, Final Batch Loss: 0.0006193637964315712\n",
      "Epoch 1470, Loss: 0.05109258193988353, Final Batch Loss: 8.860306115821004e-05\n",
      "Epoch 1471, Loss: 0.05170448858552845, Final Batch Loss: 0.0016376035055145621\n",
      "Epoch 1472, Loss: 0.06923619574081386, Final Batch Loss: 0.032558176666498184\n",
      "Epoch 1473, Loss: 0.08705540654773358, Final Batch Loss: 0.0036189057864248753\n",
      "Epoch 1474, Loss: 0.045278345147380605, Final Batch Loss: 0.0003623946104198694\n",
      "Epoch 1475, Loss: 0.05145327834179625, Final Batch Loss: 0.013179427944123745\n",
      "Epoch 1476, Loss: 0.07439056580187753, Final Batch Loss: 0.018432116135954857\n",
      "Epoch 1477, Loss: 0.08729557276819833, Final Batch Loss: 0.0029900732915848494\n",
      "Epoch 1478, Loss: 0.10200561072997516, Final Batch Loss: 0.0032589512411504984\n",
      "Epoch 1479, Loss: 0.11417519810856902, Final Batch Loss: 0.004516660235822201\n",
      "Epoch 1480, Loss: 0.05086839364957996, Final Batch Loss: 0.0012581928167492151\n",
      "Epoch 1481, Loss: 0.0945462580275489, Final Batch Loss: 0.0007175499340519309\n",
      "Epoch 1482, Loss: 0.057817546869046055, Final Batch Loss: 0.0001829849643399939\n",
      "Epoch 1483, Loss: 0.05964576484984718, Final Batch Loss: 0.0005847191205248237\n",
      "Epoch 1484, Loss: 0.02829027740517631, Final Batch Loss: 0.0014574057422578335\n",
      "Epoch 1485, Loss: 0.03667621382919606, Final Batch Loss: 0.0034807429183274508\n",
      "Epoch 1486, Loss: 0.0962961826735409, Final Batch Loss: 0.00018895264656748623\n",
      "Epoch 1487, Loss: 0.08214290575415362, Final Batch Loss: 0.0005637549329549074\n",
      "Epoch 1488, Loss: 0.13944315206026658, Final Batch Loss: 0.002098802011460066\n",
      "Epoch 1489, Loss: 0.027031447942135856, Final Batch Loss: 0.0006089831585995853\n",
      "Epoch 1490, Loss: 0.03321539398166351, Final Batch Loss: 0.00041821066406555474\n",
      "Epoch 1491, Loss: 0.021610650066577364, Final Batch Loss: 0.0001075422260328196\n",
      "Epoch 1492, Loss: 0.016367371503292816, Final Batch Loss: 3.58680663339328e-05\n",
      "Epoch 1493, Loss: 0.08572262496454641, Final Batch Loss: 0.003556622890755534\n",
      "Epoch 1494, Loss: 0.02898806349548977, Final Batch Loss: 0.005053218919783831\n",
      "Epoch 1495, Loss: 0.04556065240467433, Final Batch Loss: 0.00853662844747305\n",
      "Epoch 1496, Loss: 0.038096719050372485, Final Batch Loss: 3.3837764931377023e-05\n",
      "Epoch 1497, Loss: 0.03779460034274962, Final Batch Loss: 0.014637093991041183\n",
      "Epoch 1498, Loss: 0.033189514142577536, Final Batch Loss: 0.0008027457515709102\n",
      "Epoch 1499, Loss: 0.02505234362615738, Final Batch Loss: 0.0019511753926053643\n",
      "Epoch 1500, Loss: 0.012523869052529335, Final Batch Loss: 0.0012461768928915262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1501, Loss: 0.018460564308043104, Final Batch Loss: 0.0030623122584074736\n",
      "Epoch 1502, Loss: 0.015694022382376716, Final Batch Loss: 0.0043199267238378525\n",
      "Epoch 1503, Loss: 0.15270822978345677, Final Batch Loss: 0.0004388508095871657\n",
      "Epoch 1504, Loss: 0.13351519574644044, Final Batch Loss: 0.0032006059773266315\n",
      "Epoch 1505, Loss: 0.03951915011566598, Final Batch Loss: 0.00027628373936749995\n",
      "Epoch 1506, Loss: 0.1667972861760063, Final Batch Loss: 0.00112175103276968\n",
      "Epoch 1507, Loss: 0.09764388232724741, Final Batch Loss: 0.01969761960208416\n",
      "Epoch 1508, Loss: 0.11447647848399356, Final Batch Loss: 0.0006278532091528177\n",
      "Epoch 1509, Loss: 0.06010150260408409, Final Batch Loss: 0.001117830746807158\n",
      "Epoch 1510, Loss: 0.047345160419354215, Final Batch Loss: 0.0006699439254589379\n",
      "Epoch 1511, Loss: 0.02706518574268557, Final Batch Loss: 0.0020892778411507607\n",
      "Epoch 1512, Loss: 0.028778371241060086, Final Batch Loss: 0.0016189540037885308\n",
      "Epoch 1513, Loss: 0.02201702458114596, Final Batch Loss: 0.004159178119152784\n",
      "Epoch 1514, Loss: 0.02112210597988451, Final Batch Loss: 0.00126161752268672\n",
      "Epoch 1515, Loss: 0.023993055310711497, Final Batch Loss: 0.0004903280059807003\n",
      "Epoch 1516, Loss: 0.08452948111516889, Final Batch Loss: 0.0035020066425204277\n",
      "Epoch 1517, Loss: 0.08779613129445352, Final Batch Loss: 0.0005750120617449284\n",
      "Epoch 1518, Loss: 0.06515506573487073, Final Batch Loss: 0.0019059099722653627\n",
      "Epoch 1519, Loss: 0.019783260933763813, Final Batch Loss: 0.0002933249925263226\n",
      "Epoch 1520, Loss: 0.04207864413911011, Final Batch Loss: 0.0004464134981390089\n",
      "Epoch 1521, Loss: 0.07650146179366857, Final Batch Loss: 0.0004714324022643268\n",
      "Epoch 1522, Loss: 0.06012912551523186, Final Batch Loss: 0.0015732207102701068\n",
      "Epoch 1523, Loss: 0.046659504776471294, Final Batch Loss: 0.00203575543127954\n",
      "Epoch 1524, Loss: 0.06071407234412618, Final Batch Loss: 0.0007069605053402483\n",
      "Epoch 1525, Loss: 0.02881042568333214, Final Batch Loss: 0.006011261139065027\n",
      "Epoch 1526, Loss: 0.06479238726751646, Final Batch Loss: 0.00021484638273250312\n",
      "Epoch 1527, Loss: 0.03561836379230954, Final Batch Loss: 0.00013178387598600239\n",
      "Epoch 1528, Loss: 0.08730674900289159, Final Batch Loss: 0.00038144615245983005\n",
      "Epoch 1529, Loss: 0.08861326347687282, Final Batch Loss: 0.0016121109947562218\n",
      "Epoch 1530, Loss: 0.07751418893167283, Final Batch Loss: 0.0020857779309153557\n",
      "Epoch 1531, Loss: 0.07277250233164523, Final Batch Loss: 0.0005698085878975689\n",
      "Epoch 1532, Loss: 0.05306042323354632, Final Batch Loss: 0.008663860149681568\n",
      "Epoch 1533, Loss: 0.03766945253300946, Final Batch Loss: 0.012297897599637508\n",
      "Epoch 1534, Loss: 0.10296265991928522, Final Batch Loss: 0.0013720330316573381\n",
      "Epoch 1535, Loss: 0.040634403656440554, Final Batch Loss: 4.792665640707128e-05\n",
      "Epoch 1536, Loss: 0.031867006509855855, Final Batch Loss: 0.0021764892153441906\n",
      "Epoch 1537, Loss: 0.029229311920062173, Final Batch Loss: 0.0013714346569031477\n",
      "Epoch 1538, Loss: 0.02239054226083681, Final Batch Loss: 0.00016606519056949764\n",
      "Epoch 1539, Loss: 0.04250905463413801, Final Batch Loss: 0.0012758788652718067\n",
      "Epoch 1540, Loss: 0.054915522792725824, Final Batch Loss: 0.0006296411156654358\n",
      "Epoch 1541, Loss: 0.024245497064839583, Final Batch Loss: 0.0003999582550022751\n",
      "Epoch 1542, Loss: 0.08112295746104792, Final Batch Loss: 0.002227477263659239\n",
      "Epoch 1543, Loss: 0.041964475181885064, Final Batch Loss: 0.0010951308067888021\n",
      "Epoch 1544, Loss: 0.09977780719054863, Final Batch Loss: 0.002590128919109702\n",
      "Epoch 1545, Loss: 0.025356936719617806, Final Batch Loss: 0.00036377779906615615\n",
      "Epoch 1546, Loss: 0.05944708448078018, Final Batch Loss: 0.0017557521350681782\n",
      "Epoch 1547, Loss: 0.01681475481018424, Final Batch Loss: 0.0005063057178631425\n",
      "Epoch 1548, Loss: 0.04846131516387686, Final Batch Loss: 0.00026731850812211633\n",
      "Epoch 1549, Loss: 0.024351159529032884, Final Batch Loss: 0.00011040075332857668\n",
      "Epoch 1550, Loss: 0.017216388863744214, Final Batch Loss: 0.0012699736980721354\n",
      "Epoch 1551, Loss: 0.010188357675360749, Final Batch Loss: 0.00023788248654454947\n",
      "Epoch 1552, Loss: 0.03217655728076352, Final Batch Loss: 0.0001921123912325129\n",
      "Epoch 1553, Loss: 0.013522655368433334, Final Batch Loss: 0.003080799011513591\n",
      "Epoch 1554, Loss: 0.02601265122211771, Final Batch Loss: 0.006474844180047512\n",
      "Epoch 1555, Loss: 0.020534560666419566, Final Batch Loss: 0.00013625599967781454\n",
      "Epoch 1556, Loss: 0.01950873765599681, Final Batch Loss: 0.006522569339722395\n",
      "Epoch 1557, Loss: 0.02828033789410256, Final Batch Loss: 0.005546431988477707\n",
      "Epoch 1558, Loss: 0.016727811147575267, Final Batch Loss: 0.00037158254417590797\n",
      "Epoch 1559, Loss: 0.00919845917087514, Final Batch Loss: 0.0009100003517232835\n",
      "Epoch 1560, Loss: 0.0440189583805477, Final Batch Loss: 0.00058523821644485\n",
      "Epoch 1561, Loss: 0.037735038858954795, Final Batch Loss: 0.0020994553342461586\n",
      "Epoch 1562, Loss: 0.05759965942706913, Final Batch Loss: 0.0033397760707885027\n",
      "Epoch 1563, Loss: 0.03504503505246248, Final Batch Loss: 0.00013157649664208293\n",
      "Epoch 1564, Loss: 0.027179100492503494, Final Batch Loss: 0.00013118181959725916\n",
      "Epoch 1565, Loss: 0.05611004208913073, Final Batch Loss: 0.0005544836749322712\n",
      "Epoch 1566, Loss: 0.032967174454825, Final Batch Loss: 4.8301040806109086e-05\n",
      "Epoch 1567, Loss: 0.057848691518302076, Final Batch Loss: 0.000726105528883636\n",
      "Epoch 1568, Loss: 0.03129286790499464, Final Batch Loss: 0.006565973628312349\n",
      "Epoch 1569, Loss: 0.027865042902703863, Final Batch Loss: 0.0006373300566338003\n",
      "Epoch 1570, Loss: 0.007519620568928076, Final Batch Loss: 4.723878009826876e-05\n",
      "Epoch 1571, Loss: 0.020293089153710753, Final Batch Loss: 0.00020178746490273625\n",
      "Epoch 1572, Loss: 0.00908460009668488, Final Batch Loss: 0.0004293849924579263\n",
      "Epoch 1573, Loss: 0.04242294940922875, Final Batch Loss: 0.00021003696019761264\n",
      "Epoch 1574, Loss: 0.05565600012050709, Final Batch Loss: 0.02525494620203972\n",
      "Epoch 1575, Loss: 0.030137567522615427, Final Batch Loss: 0.0024277460761368275\n",
      "Epoch 1576, Loss: 0.04125135024514748, Final Batch Loss: 0.0005227543879300356\n",
      "Epoch 1577, Loss: 0.059103129380673636, Final Batch Loss: 0.0006917650462128222\n",
      "Epoch 1578, Loss: 0.022130817575089168, Final Batch Loss: 0.0019391431706026196\n",
      "Epoch 1579, Loss: 0.11476040935667697, Final Batch Loss: 0.0002229569508926943\n",
      "Epoch 1580, Loss: 0.02940677035076078, Final Batch Loss: 0.0007057895418256521\n",
      "Epoch 1581, Loss: 0.08651498780818656, Final Batch Loss: 0.0004041438514832407\n",
      "Epoch 1582, Loss: 0.05292847266537137, Final Batch Loss: 0.007072740234434605\n",
      "Epoch 1583, Loss: 0.08219777388148941, Final Batch Loss: 0.007675719913095236\n",
      "Epoch 1584, Loss: 0.05469610877480591, Final Batch Loss: 0.001071845879778266\n",
      "Epoch 1585, Loss: 0.03539342632575426, Final Batch Loss: 0.0009642644436098635\n",
      "Epoch 1586, Loss: 0.0646131118410267, Final Batch Loss: 0.03369788080453873\n",
      "Epoch 1587, Loss: 0.02786722793644003, Final Batch Loss: 0.0023917900398373604\n",
      "Epoch 1588, Loss: 0.014366625968250446, Final Batch Loss: 0.00020811904687434435\n",
      "Epoch 1589, Loss: 0.010718367528170347, Final Batch Loss: 0.00010922936053248122\n",
      "Epoch 1590, Loss: 0.02607379067922011, Final Batch Loss: 0.0009492491954006255\n",
      "Epoch 1591, Loss: 0.01757677109344513, Final Batch Loss: 0.00097707100212574\n",
      "Epoch 1592, Loss: 0.016197512712096795, Final Batch Loss: 0.00029453812749125063\n",
      "Epoch 1593, Loss: 0.03291453988640569, Final Batch Loss: 0.00016534252790734172\n",
      "Epoch 1594, Loss: 0.02178201376227662, Final Batch Loss: 0.0003612325235735625\n",
      "Epoch 1595, Loss: 0.01854872178955702, Final Batch Loss: 0.0010652018245309591\n",
      "Epoch 1596, Loss: 0.06968605880683754, Final Batch Loss: 0.0011524332221597433\n",
      "Epoch 1597, Loss: 0.0464654336537933, Final Batch Loss: 0.038105983287096024\n",
      "Epoch 1598, Loss: 0.017397946612618398, Final Batch Loss: 0.0018577690934762359\n",
      "Epoch 1599, Loss: 0.01927665470429929, Final Batch Loss: 0.0003738182713277638\n",
      "Epoch 1600, Loss: 0.07049161801842274, Final Batch Loss: 0.0003361967974342406\n",
      "Epoch 1601, Loss: 0.023079683029209264, Final Batch Loss: 0.00016090851568151265\n",
      "Epoch 1602, Loss: 0.012019663336104713, Final Batch Loss: 0.0014512534253299236\n",
      "Epoch 1603, Loss: 0.035767771641985746, Final Batch Loss: 4.9985963414656e-05\n",
      "Epoch 1604, Loss: 0.08598452204023488, Final Batch Loss: 0.007255570963025093\n",
      "Epoch 1605, Loss: 0.13346891754190437, Final Batch Loss: 0.024376075714826584\n",
      "Epoch 1606, Loss: 0.06478015948232496, Final Batch Loss: 0.0001210283298860304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1607, Loss: 0.05314718389126938, Final Batch Loss: 0.0010398074518889189\n",
      "Epoch 1608, Loss: 0.0545693139283685, Final Batch Loss: 0.004519973881542683\n",
      "Epoch 1609, Loss: 0.05988031737797428, Final Batch Loss: 0.00013856604346074164\n",
      "Epoch 1610, Loss: 0.03589852899312973, Final Batch Loss: 0.0034235313069075346\n",
      "Epoch 1611, Loss: 0.03501715084712487, Final Batch Loss: 0.0001803548220777884\n",
      "Epoch 1612, Loss: 0.024223551139584742, Final Batch Loss: 6.692534952890128e-05\n",
      "Epoch 1613, Loss: 0.030199957538570743, Final Batch Loss: 0.00816331896930933\n",
      "Epoch 1614, Loss: 0.0535167836715118, Final Batch Loss: 6.601632048841566e-05\n",
      "Epoch 1615, Loss: 0.040867957766749896, Final Batch Loss: 0.00693499855697155\n",
      "Epoch 1616, Loss: 0.077565905143274, Final Batch Loss: 0.0007498074555769563\n",
      "Epoch 1617, Loss: 0.04511523342807777, Final Batch Loss: 0.0004216803645249456\n",
      "Epoch 1618, Loss: 0.049748869430914056, Final Batch Loss: 0.0008502114797011018\n",
      "Epoch 1619, Loss: 0.07594001243705861, Final Batch Loss: 0.000716204522177577\n",
      "Epoch 1620, Loss: 0.024966111392132007, Final Batch Loss: 0.007168010342866182\n",
      "Epoch 1621, Loss: 0.04072781218565069, Final Batch Loss: 0.002841801615431905\n",
      "Epoch 1622, Loss: 0.02096533281292068, Final Batch Loss: 0.0003238975477870554\n",
      "Epoch 1623, Loss: 0.031259335490176454, Final Batch Loss: 0.0006145228981040418\n",
      "Epoch 1624, Loss: 0.013756746862782165, Final Batch Loss: 0.0014192141825333238\n",
      "Epoch 1625, Loss: 0.011782461006077938, Final Batch Loss: 0.0006009828648529947\n",
      "Epoch 1626, Loss: 0.049795663144323044, Final Batch Loss: 0.002128500957041979\n",
      "Epoch 1627, Loss: 0.06679102168709505, Final Batch Loss: 0.007773206103593111\n",
      "Epoch 1628, Loss: 0.04432611370793893, Final Batch Loss: 0.0009551802068017423\n",
      "Epoch 1629, Loss: 0.03687444292154396, Final Batch Loss: 0.0011008569272235036\n",
      "Epoch 1630, Loss: 0.045311530484468676, Final Batch Loss: 0.0014145051827654243\n",
      "Epoch 1631, Loss: 0.019795824198808987, Final Batch Loss: 6.952493276912719e-05\n",
      "Epoch 1632, Loss: 0.02026299097633455, Final Batch Loss: 0.000642049650195986\n",
      "Epoch 1633, Loss: 0.03577772849530447, Final Batch Loss: 8.456774230580777e-05\n",
      "Epoch 1634, Loss: 0.016127135444548912, Final Batch Loss: 0.0015829171752557158\n",
      "Epoch 1635, Loss: 0.033145111294288654, Final Batch Loss: 0.014090277254581451\n",
      "Epoch 1636, Loss: 0.021847312520549167, Final Batch Loss: 0.0004624757857527584\n",
      "Epoch 1637, Loss: 0.07472809053433593, Final Batch Loss: 0.00012420509301591665\n",
      "Epoch 1638, Loss: 0.06994852477509994, Final Batch Loss: 0.0004157159710302949\n",
      "Epoch 1639, Loss: 0.04285383985552471, Final Batch Loss: 0.000669736007694155\n",
      "Epoch 1640, Loss: 0.063165268576995, Final Batch Loss: 0.0014795329188928008\n",
      "Epoch 1641, Loss: 0.046175382027286105, Final Batch Loss: 0.0006657859776169062\n",
      "Epoch 1642, Loss: 0.08164601217140444, Final Batch Loss: 0.0004776613204739988\n",
      "Epoch 1643, Loss: 0.035813479120406555, Final Batch Loss: 0.00014739198377355933\n",
      "Epoch 1644, Loss: 0.05495625389448833, Final Batch Loss: 0.00631727185100317\n",
      "Epoch 1645, Loss: 0.04098905681166798, Final Batch Loss: 0.0035013852175325155\n",
      "Epoch 1646, Loss: 0.040994166243763175, Final Batch Loss: 0.000384423736250028\n",
      "Epoch 1647, Loss: 0.022219000427867286, Final Batch Loss: 0.0019282931461930275\n",
      "Epoch 1648, Loss: 0.03538540667796042, Final Batch Loss: 7.804868801031262e-05\n",
      "Epoch 1649, Loss: 0.02256953503820114, Final Batch Loss: 6.141367339296266e-05\n",
      "Epoch 1650, Loss: 0.023945598986756522, Final Batch Loss: 0.00284754135645926\n",
      "Epoch 1651, Loss: 0.03719327186263399, Final Batch Loss: 0.0002347588015254587\n",
      "Epoch 1652, Loss: 0.03990191717457492, Final Batch Loss: 0.021154150366783142\n",
      "Epoch 1653, Loss: 0.04970871809928212, Final Batch Loss: 0.0048668272793293\n",
      "Epoch 1654, Loss: 0.07892851912765764, Final Batch Loss: 0.0007803059997968376\n",
      "Epoch 1655, Loss: 0.08405448569828877, Final Batch Loss: 0.003115501720458269\n",
      "Epoch 1656, Loss: 0.04015849206189159, Final Batch Loss: 0.0015924065373837948\n",
      "Epoch 1657, Loss: 0.10258799133589491, Final Batch Loss: 0.006130025256425142\n",
      "Epoch 1658, Loss: 0.02372711357747903, Final Batch Loss: 0.001614054897800088\n",
      "Epoch 1659, Loss: 0.02155856043100357, Final Batch Loss: 0.0021518704015761614\n",
      "Epoch 1660, Loss: 0.03320575827092398, Final Batch Loss: 0.00017710334213916212\n",
      "Epoch 1661, Loss: 0.0275703712941322, Final Batch Loss: 0.007951785810291767\n",
      "Epoch 1662, Loss: 0.05358942999737337, Final Batch Loss: 0.0005677267909049988\n",
      "Epoch 1663, Loss: 0.0782154914631974, Final Batch Loss: 0.0003686503041535616\n",
      "Epoch 1664, Loss: 0.05026552538402029, Final Batch Loss: 5.685998985427432e-05\n",
      "Epoch 1665, Loss: 0.013821185209963005, Final Batch Loss: 0.00030345184495672584\n",
      "Epoch 1666, Loss: 0.11120607773955271, Final Batch Loss: 0.0026066675782203674\n",
      "Epoch 1667, Loss: 0.2676211786165368, Final Batch Loss: 0.02004009671509266\n",
      "Epoch 1668, Loss: 0.19489953987067565, Final Batch Loss: 0.013228815980255604\n",
      "Epoch 1669, Loss: 0.041204320805263706, Final Batch Loss: 0.0010671182535588741\n",
      "Epoch 1670, Loss: 0.07367041370889638, Final Batch Loss: 0.004273726139217615\n",
      "Epoch 1671, Loss: 0.0268715384445386, Final Batch Loss: 0.0011085235746577382\n",
      "Epoch 1672, Loss: 0.025902628527546767, Final Batch Loss: 0.00852477177977562\n",
      "Epoch 1673, Loss: 0.013101491727866232, Final Batch Loss: 0.0005162302404642105\n",
      "Epoch 1674, Loss: 0.13898350749514066, Final Batch Loss: 0.01365501806139946\n",
      "Epoch 1675, Loss: 0.06983252262580208, Final Batch Loss: 0.00198353361338377\n",
      "Epoch 1676, Loss: 0.05436041817301884, Final Batch Loss: 0.0006014678510837257\n",
      "Epoch 1677, Loss: 0.06463228075881489, Final Batch Loss: 0.0034958780743181705\n",
      "Epoch 1678, Loss: 0.02822650520829484, Final Batch Loss: 0.0007489132112823427\n",
      "Epoch 1679, Loss: 0.03576514015730936, Final Batch Loss: 0.0018673085141927004\n",
      "Epoch 1680, Loss: 0.03952252760063857, Final Batch Loss: 0.0003929594240617007\n",
      "Epoch 1681, Loss: 0.021116664007422514, Final Batch Loss: 0.0010037156753242016\n",
      "Epoch 1682, Loss: 0.01289451648335671, Final Batch Loss: 0.001997285755351186\n",
      "Epoch 1683, Loss: 0.014203513543179724, Final Batch Loss: 0.0009628549451008439\n",
      "Epoch 1684, Loss: 0.03427000008377945, Final Batch Loss: 0.001207661465741694\n",
      "Epoch 1685, Loss: 0.015597378966049291, Final Batch Loss: 0.00011451639147708192\n",
      "Epoch 1686, Loss: 0.016392910089052748, Final Batch Loss: 0.0031706090085208416\n",
      "Epoch 1687, Loss: 0.049242698652960826, Final Batch Loss: 0.0007054191664792597\n",
      "Epoch 1688, Loss: 0.10146394677576609, Final Batch Loss: 0.0026628724299371243\n",
      "Epoch 1689, Loss: 0.029406135581666604, Final Batch Loss: 0.0006861616857349873\n",
      "Epoch 1690, Loss: 0.05463092602440156, Final Batch Loss: 0.0027654962614178658\n",
      "Epoch 1691, Loss: 0.05368885613279417, Final Batch Loss: 0.0026984482537955046\n",
      "Epoch 1692, Loss: 0.0454862565457006, Final Batch Loss: 0.0005706751835532486\n",
      "Epoch 1693, Loss: 0.08013717185531277, Final Batch Loss: 0.02425008825957775\n",
      "Epoch 1694, Loss: 0.028088753249903675, Final Batch Loss: 0.0002170592633774504\n",
      "Epoch 1695, Loss: 0.023163499048678204, Final Batch Loss: 0.006370525807142258\n",
      "Epoch 1696, Loss: 0.019281709755887277, Final Batch Loss: 0.00034427802893333137\n",
      "Epoch 1697, Loss: 0.012655207858188078, Final Batch Loss: 0.0024956970009952784\n",
      "Epoch 1698, Loss: 0.022414236555050593, Final Batch Loss: 0.0006463913014158607\n",
      "Epoch 1699, Loss: 0.015682978759286925, Final Batch Loss: 0.00013846950605511665\n",
      "Epoch 1700, Loss: 0.051474323045113124, Final Batch Loss: 0.00024867174215614796\n",
      "Epoch 1701, Loss: 0.014892551873344928, Final Batch Loss: 0.0010239934781566262\n",
      "Epoch 1702, Loss: 0.04289392754435539, Final Batch Loss: 0.0007322461460717022\n",
      "Epoch 1703, Loss: 0.17018513144284952, Final Batch Loss: 0.055160894989967346\n",
      "Epoch 1704, Loss: 0.048248689927277155, Final Batch Loss: 0.00039889864274300635\n",
      "Epoch 1705, Loss: 0.09111879300326109, Final Batch Loss: 0.0018181022023782134\n",
      "Epoch 1706, Loss: 0.10250868636649102, Final Batch Loss: 0.0044229780323803425\n",
      "Epoch 1707, Loss: 0.0535092348291073, Final Batch Loss: 0.003264696104452014\n",
      "Epoch 1708, Loss: 0.07758143548562657, Final Batch Loss: 0.004009820055216551\n",
      "Epoch 1709, Loss: 0.05147920665331185, Final Batch Loss: 0.0003715607454068959\n",
      "Epoch 1710, Loss: 0.027507596678333357, Final Batch Loss: 0.0006055435515008867\n",
      "Epoch 1711, Loss: 0.06210048089269549, Final Batch Loss: 0.002740476978942752\n",
      "Epoch 1712, Loss: 0.05854760581860319, Final Batch Loss: 0.00013467198004946113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1713, Loss: 0.04121018649311736, Final Batch Loss: 0.00039000058313831687\n",
      "Epoch 1714, Loss: 0.07123531171237119, Final Batch Loss: 0.0009986187797039747\n",
      "Epoch 1715, Loss: 0.030626864478108473, Final Batch Loss: 0.0025701357517391443\n",
      "Epoch 1716, Loss: 0.035623689836938865, Final Batch Loss: 0.0058837635442614555\n",
      "Epoch 1717, Loss: 0.03536511218408123, Final Batch Loss: 0.0009157861350104213\n",
      "Epoch 1718, Loss: 0.04485076604032656, Final Batch Loss: 0.00030100042931735516\n",
      "Epoch 1719, Loss: 0.07706630040775053, Final Batch Loss: 0.007567453198134899\n",
      "Epoch 1720, Loss: 0.0426184020616347, Final Batch Loss: 5.175491969566792e-05\n",
      "Epoch 1721, Loss: 0.02509799892141018, Final Batch Loss: 0.000286559370579198\n",
      "Epoch 1722, Loss: 0.015294509887098684, Final Batch Loss: 2.4861516067176126e-05\n",
      "Epoch 1723, Loss: 0.04594717703002971, Final Batch Loss: 0.027620315551757812\n",
      "Epoch 1724, Loss: 0.017520170207717456, Final Batch Loss: 0.0025131329894065857\n",
      "Epoch 1725, Loss: 0.04554155559890205, Final Batch Loss: 0.008891887962818146\n",
      "Epoch 1726, Loss: 0.024459675565594807, Final Batch Loss: 5.844649058417417e-05\n",
      "Epoch 1727, Loss: 0.04848760246022721, Final Batch Loss: 0.00016504146333318204\n",
      "Epoch 1728, Loss: 0.015773711704241578, Final Batch Loss: 0.0005228809895925224\n",
      "Epoch 1729, Loss: 0.03855232970090583, Final Batch Loss: 0.0008120435522869229\n",
      "Epoch 1730, Loss: 0.03570899686019402, Final Batch Loss: 0.0003493371477816254\n",
      "Epoch 1731, Loss: 0.01289260745397769, Final Batch Loss: 0.0024891039356589317\n",
      "Epoch 1732, Loss: 0.02589319774415344, Final Batch Loss: 0.00011264564818702638\n",
      "Epoch 1733, Loss: 0.019956446158175822, Final Batch Loss: 0.005308112129569054\n",
      "Epoch 1734, Loss: 0.022660100603388855, Final Batch Loss: 0.001908282283693552\n",
      "Epoch 1735, Loss: 0.010968860544380732, Final Batch Loss: 0.0008406956912949681\n",
      "Epoch 1736, Loss: 0.009457695574383251, Final Batch Loss: 0.0007812678813934326\n",
      "Epoch 1737, Loss: 0.05650188643267029, Final Batch Loss: 0.028036709874868393\n",
      "Epoch 1738, Loss: 0.01478833329747431, Final Batch Loss: 8.738719770917669e-05\n",
      "Epoch 1739, Loss: 0.021206077450187877, Final Batch Loss: 0.00028861890314146876\n",
      "Epoch 1740, Loss: 0.024739731969020795, Final Batch Loss: 0.0015446720644831657\n",
      "Epoch 1741, Loss: 0.05491689165137359, Final Batch Loss: 0.016995981335639954\n",
      "Epoch 1742, Loss: 0.06381735780814779, Final Batch Loss: 0.035613853484392166\n",
      "Epoch 1743, Loss: 0.03550588917278219, Final Batch Loss: 8.696165605215356e-05\n",
      "Epoch 1744, Loss: 0.022484278575575445, Final Batch Loss: 0.0005285665392875671\n",
      "Epoch 1745, Loss: 0.052835157213849016, Final Batch Loss: 0.007842260412871838\n",
      "Epoch 1746, Loss: 0.08349934089346789, Final Batch Loss: 0.0118833864107728\n",
      "Epoch 1747, Loss: 0.05706315525458194, Final Batch Loss: 0.0036368274595588446\n",
      "Epoch 1748, Loss: 0.10512916416337248, Final Batch Loss: 0.0035916143096983433\n",
      "Epoch 1749, Loss: 0.059381544662755914, Final Batch Loss: 0.0024057202972471714\n",
      "Epoch 1750, Loss: 0.11264132887299638, Final Batch Loss: 0.008598103187978268\n",
      "Epoch 1751, Loss: 0.13492986405617557, Final Batch Loss: 0.0032089347951114178\n",
      "Epoch 1752, Loss: 0.1183246377040632, Final Batch Loss: 0.00026819153572432697\n",
      "Epoch 1753, Loss: 0.08333398308604956, Final Batch Loss: 0.0008091968484222889\n",
      "Epoch 1754, Loss: 0.043192916025873274, Final Batch Loss: 0.0010651185875758529\n",
      "Epoch 1755, Loss: 0.11129052059550304, Final Batch Loss: 0.0002289487310918048\n",
      "Epoch 1756, Loss: 0.030086066079093143, Final Batch Loss: 0.0005538866971619427\n",
      "Epoch 1757, Loss: 0.020059188929735683, Final Batch Loss: 0.00018394437211100012\n",
      "Epoch 1758, Loss: 0.009493495832430199, Final Batch Loss: 0.00028088141698390245\n",
      "Epoch 1759, Loss: 0.010529376420890912, Final Batch Loss: 0.00029598482069559395\n",
      "Epoch 1760, Loss: 0.028694358203210868, Final Batch Loss: 0.005433880724012852\n",
      "Epoch 1761, Loss: 0.047124780852755066, Final Batch Loss: 0.0010622642002999783\n",
      "Epoch 1762, Loss: 0.023855455750890542, Final Batch Loss: 0.009936508722603321\n",
      "Epoch 1763, Loss: 0.01670812175507308, Final Batch Loss: 0.0003207219415344298\n",
      "Epoch 1764, Loss: 0.04843641039769864, Final Batch Loss: 9.633530135033652e-05\n",
      "Epoch 1765, Loss: 0.039435498823877424, Final Batch Loss: 0.0006787445745430887\n",
      "Epoch 1766, Loss: 0.02134419372305274, Final Batch Loss: 0.00014256630674935877\n",
      "Epoch 1767, Loss: 0.009390379003889393, Final Batch Loss: 0.0004960803780704737\n",
      "Epoch 1768, Loss: 0.027787874525529332, Final Batch Loss: 0.00029170827474445105\n",
      "Epoch 1769, Loss: 0.022142872301628813, Final Batch Loss: 0.0005643776967190206\n",
      "Epoch 1770, Loss: 0.051346391719562234, Final Batch Loss: 8.195225382223725e-05\n",
      "Epoch 1771, Loss: 0.05062000785255805, Final Batch Loss: 0.0009253864409402013\n",
      "Epoch 1772, Loss: 0.03286867593124043, Final Batch Loss: 0.00036649368121288717\n",
      "Epoch 1773, Loss: 0.03734756568883313, Final Batch Loss: 0.00048615242121741176\n",
      "Epoch 1774, Loss: 0.07477947121515172, Final Batch Loss: 0.0013110738946124911\n",
      "Epoch 1775, Loss: 0.051392745939665474, Final Batch Loss: 0.0005769167910329998\n",
      "Epoch 1776, Loss: 0.011002489321981557, Final Batch Loss: 0.00025680119870230556\n",
      "Epoch 1777, Loss: 0.033048714154574554, Final Batch Loss: 0.004128547385334969\n",
      "Epoch 1778, Loss: 0.014151967596262693, Final Batch Loss: 0.00017999041301663965\n",
      "Epoch 1779, Loss: 0.05614463405800052, Final Batch Loss: 0.00011200542212463915\n",
      "Epoch 1780, Loss: 0.030518093321006745, Final Batch Loss: 0.0019441189942881465\n",
      "Epoch 1781, Loss: 0.09815464737766888, Final Batch Loss: 0.0007121875532902777\n",
      "Epoch 1782, Loss: 0.06334738313307753, Final Batch Loss: 0.00012825283920392394\n",
      "Epoch 1783, Loss: 0.0704634772555437, Final Batch Loss: 0.00016386306378990412\n",
      "Epoch 1784, Loss: 0.03785140842956025, Final Batch Loss: 0.0001855836744653061\n",
      "Epoch 1785, Loss: 0.04655685122997966, Final Batch Loss: 0.0004480493953451514\n",
      "Epoch 1786, Loss: 0.0709044989926042, Final Batch Loss: 0.005331365391612053\n",
      "Epoch 1787, Loss: 0.07782021976890974, Final Batch Loss: 0.0008785374229773879\n",
      "Epoch 1788, Loss: 0.04268645492265932, Final Batch Loss: 0.0021238024346530437\n",
      "Epoch 1789, Loss: 0.06293851410737261, Final Batch Loss: 0.00013236042286735028\n",
      "Epoch 1790, Loss: 0.09141342680959497, Final Batch Loss: 0.0030866432934999466\n",
      "Epoch 1791, Loss: 0.06286825039569521, Final Batch Loss: 3.522328188410029e-05\n",
      "Epoch 1792, Loss: 0.039093669038265944, Final Batch Loss: 0.0009769888129085302\n",
      "Epoch 1793, Loss: 0.04678077880089404, Final Batch Loss: 0.003488721325993538\n",
      "Epoch 1794, Loss: 0.05631519902453874, Final Batch Loss: 4.6891425881767645e-05\n",
      "Epoch 1795, Loss: 0.019854840898915427, Final Batch Loss: 0.0008538023103028536\n",
      "Epoch 1796, Loss: 0.025029208740306785, Final Batch Loss: 0.0003000947181135416\n",
      "Epoch 1797, Loss: 0.0161583752851584, Final Batch Loss: 0.0002039222454186529\n",
      "Epoch 1798, Loss: 0.09784027891873848, Final Batch Loss: 0.0008348271949216723\n",
      "Epoch 1799, Loss: 0.009724196421302622, Final Batch Loss: 0.0007476311293430626\n",
      "Epoch 1800, Loss: 0.09205916266910208, Final Batch Loss: 0.0009950549574568868\n",
      "Epoch 1801, Loss: 0.04528703822870739, Final Batch Loss: 0.0026004358660429716\n",
      "Epoch 1802, Loss: 0.019695863655215362, Final Batch Loss: 0.0007896500756032765\n",
      "Epoch 1803, Loss: 0.03201206411176827, Final Batch Loss: 0.0011970257619395852\n",
      "Epoch 1804, Loss: 0.0720502388139721, Final Batch Loss: 0.0008688539965078235\n",
      "Epoch 1805, Loss: 0.039704777096631005, Final Batch Loss: 0.0055020353756845\n",
      "Epoch 1806, Loss: 0.04620415834142477, Final Batch Loss: 0.0010409504175186157\n",
      "Epoch 1807, Loss: 0.02677321742521599, Final Batch Loss: 0.0016005911165848374\n",
      "Epoch 1808, Loss: 0.040709170294576325, Final Batch Loss: 0.02407093159854412\n",
      "Epoch 1809, Loss: 0.014194557734299451, Final Batch Loss: 0.00012671331933233887\n",
      "Epoch 1810, Loss: 0.00909239704196807, Final Batch Loss: 0.0007112661260180175\n",
      "Epoch 1811, Loss: 0.012563069438328966, Final Batch Loss: 0.002218538662418723\n",
      "Epoch 1812, Loss: 0.04810171794088092, Final Batch Loss: 0.0029939974192529917\n",
      "Epoch 1813, Loss: 0.04584248032188043, Final Batch Loss: 0.0026220253203064203\n",
      "Epoch 1814, Loss: 0.031584346274030395, Final Batch Loss: 0.0004290320794098079\n",
      "Epoch 1815, Loss: 0.04225965387013275, Final Batch Loss: 0.0002475301444064826\n",
      "Epoch 1816, Loss: 0.04962145437457366, Final Batch Loss: 0.0007761574233882129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1817, Loss: 0.02812112867832184, Final Batch Loss: 0.0005471982294693589\n",
      "Epoch 1818, Loss: 0.03567756196935079, Final Batch Loss: 0.00149451510515064\n",
      "Epoch 1819, Loss: 0.04176965984515846, Final Batch Loss: 0.00035411654971539974\n",
      "Epoch 1820, Loss: 0.02819510029439698, Final Batch Loss: 0.008512240834534168\n",
      "Epoch 1821, Loss: 0.0518786773973261, Final Batch Loss: 0.00011710936814779416\n",
      "Epoch 1822, Loss: 0.03856754042499233, Final Batch Loss: 0.00019135253387503326\n",
      "Epoch 1823, Loss: 0.02403691568179056, Final Batch Loss: 0.0028026518411934376\n",
      "Epoch 1824, Loss: 0.026548504742095247, Final Batch Loss: 0.00028709214529953897\n",
      "Epoch 1825, Loss: 0.012019371628412046, Final Batch Loss: 0.00041918226634152234\n",
      "Epoch 1826, Loss: 0.016550351880141534, Final Batch Loss: 0.00031661870889365673\n",
      "Epoch 1827, Loss: 0.02492441318827332, Final Batch Loss: 0.0002771810395643115\n",
      "Epoch 1828, Loss: 0.01657743291798397, Final Batch Loss: 0.0002257775340694934\n",
      "Epoch 1829, Loss: 0.01730409057927318, Final Batch Loss: 0.0002470607578288764\n",
      "Epoch 1830, Loss: 0.015493470375076868, Final Batch Loss: 0.0007458674954250455\n",
      "Epoch 1831, Loss: 0.027435341933596646, Final Batch Loss: 3.5518674849299714e-05\n",
      "Epoch 1832, Loss: 0.006874048624013085, Final Batch Loss: 4.199012619210407e-05\n",
      "Epoch 1833, Loss: 0.02154048472220893, Final Batch Loss: 0.0019771710503846407\n",
      "Epoch 1834, Loss: 0.007094273125403561, Final Batch Loss: 0.0002236818545497954\n",
      "Epoch 1835, Loss: 0.04441154362575617, Final Batch Loss: 0.0044962395913898945\n",
      "Epoch 1836, Loss: 0.010523143915634137, Final Batch Loss: 0.0005389047437347472\n",
      "Epoch 1837, Loss: 0.0478946047660429, Final Batch Loss: 0.00034810072975233197\n",
      "Epoch 1838, Loss: 0.024275678159028757, Final Batch Loss: 4.5303306251298636e-05\n",
      "Epoch 1839, Loss: 0.01624805930077855, Final Batch Loss: 0.0002912687778007239\n",
      "Epoch 1840, Loss: 0.020615157305655885, Final Batch Loss: 0.00046217316412366927\n",
      "Epoch 1841, Loss: 0.01342490714523592, Final Batch Loss: 3.30085567838978e-05\n",
      "Epoch 1842, Loss: 0.0410148526316334, Final Batch Loss: 0.012098127044737339\n",
      "Epoch 1843, Loss: 0.030095039342995733, Final Batch Loss: 0.001898489659652114\n",
      "Epoch 1844, Loss: 0.005834278676047688, Final Batch Loss: 0.0002749200211837888\n",
      "Epoch 1845, Loss: 0.009446955908060772, Final Batch Loss: 4.848673052038066e-05\n",
      "Epoch 1846, Loss: 0.01773606829556229, Final Batch Loss: 7.22447075531818e-05\n",
      "Epoch 1847, Loss: 0.05202344799909042, Final Batch Loss: 0.0001431866257917136\n",
      "Epoch 1848, Loss: 0.04479960740718525, Final Batch Loss: 0.0004979415098205209\n",
      "Epoch 1849, Loss: 0.013641681944136508, Final Batch Loss: 0.0006659565842710435\n",
      "Epoch 1850, Loss: 0.009068542705790605, Final Batch Loss: 0.00030395109206438065\n",
      "Epoch 1851, Loss: 0.01714843251829734, Final Batch Loss: 0.00019040692131966352\n",
      "Epoch 1852, Loss: 0.03213976416736841, Final Batch Loss: 0.0027769736479967833\n",
      "Epoch 1853, Loss: 0.03496485798314097, Final Batch Loss: 0.0001808828819775954\n",
      "Epoch 1854, Loss: 0.3044086968220654, Final Batch Loss: 0.00134518020786345\n",
      "Epoch 1855, Loss: 0.164200616593007, Final Batch Loss: 0.013293927535414696\n",
      "Epoch 1856, Loss: 0.15394690580433235, Final Batch Loss: 0.0028257572557777166\n",
      "Epoch 1857, Loss: 0.06145580943120876, Final Batch Loss: 0.0016976916231215\n",
      "Epoch 1858, Loss: 0.03726272357744165, Final Batch Loss: 0.0004998372751288116\n",
      "Epoch 1859, Loss: 0.05853595484222751, Final Batch Loss: 0.002016836777329445\n",
      "Epoch 1860, Loss: 0.0485973351096618, Final Batch Loss: 7.577629730803892e-05\n",
      "Epoch 1861, Loss: 0.023506077253841795, Final Batch Loss: 0.0017025661654770374\n",
      "Epoch 1862, Loss: 0.047912409747368656, Final Batch Loss: 0.0016394557897001505\n",
      "Epoch 1863, Loss: 0.06130912745720707, Final Batch Loss: 0.002998112002387643\n",
      "Epoch 1864, Loss: 0.02354862601896457, Final Batch Loss: 0.00018070719670504332\n",
      "Epoch 1865, Loss: 0.04312134606880136, Final Batch Loss: 0.006129527930170298\n",
      "Epoch 1866, Loss: 0.019687476189574227, Final Batch Loss: 7.016982999630272e-05\n",
      "Epoch 1867, Loss: 0.07061586435156642, Final Batch Loss: 8.723475184524432e-05\n",
      "Epoch 1868, Loss: 0.025362243555719033, Final Batch Loss: 0.00043922747136093676\n",
      "Epoch 1869, Loss: 0.03025991885806434, Final Batch Loss: 0.002782072639092803\n",
      "Epoch 1870, Loss: 0.06131794009706937, Final Batch Loss: 0.0016205382999032736\n",
      "Epoch 1871, Loss: 0.018952383987198118, Final Batch Loss: 0.00014259599265642464\n",
      "Epoch 1872, Loss: 0.047701017116196454, Final Batch Loss: 0.007347479462623596\n",
      "Epoch 1873, Loss: 0.058303149868152104, Final Batch Loss: 0.0005599610740318894\n",
      "Epoch 1874, Loss: 0.03728466003667563, Final Batch Loss: 0.00044113429612480104\n",
      "Epoch 1875, Loss: 0.12850699329283088, Final Batch Loss: 0.018970036879181862\n",
      "Epoch 1876, Loss: 0.09121363773010671, Final Batch Loss: 0.0020744868088513613\n",
      "Epoch 1877, Loss: 0.05556321878975723, Final Batch Loss: 0.013501239009201527\n",
      "Epoch 1878, Loss: 0.024326500832103193, Final Batch Loss: 0.0008406214765273035\n",
      "Epoch 1879, Loss: 0.08172820383333601, Final Batch Loss: 0.00014178828860167414\n",
      "Epoch 1880, Loss: 0.023351613603153965, Final Batch Loss: 0.0004753035609610379\n",
      "Epoch 1881, Loss: 0.029428132402244955, Final Batch Loss: 0.003372705075889826\n",
      "Epoch 1882, Loss: 0.031197597229038365, Final Batch Loss: 0.0016699263360351324\n",
      "Epoch 1883, Loss: 0.02698646292265039, Final Batch Loss: 0.005568327382206917\n",
      "Epoch 1884, Loss: 0.05390139624432777, Final Batch Loss: 0.0003125945804640651\n",
      "Epoch 1885, Loss: 0.07221958222362446, Final Batch Loss: 0.0019537056796252728\n",
      "Epoch 1886, Loss: 0.04345031867705984, Final Batch Loss: 0.00039057558751665056\n",
      "Epoch 1887, Loss: 0.0395728259900352, Final Batch Loss: 0.0019359126454219222\n",
      "Epoch 1888, Loss: 0.13591573933081236, Final Batch Loss: 0.006583697162568569\n",
      "Epoch 1889, Loss: 0.04060500820924062, Final Batch Loss: 0.00016209228488150984\n",
      "Epoch 1890, Loss: 0.05936859504436143, Final Batch Loss: 0.0007089987047947943\n",
      "Epoch 1891, Loss: 0.032841281361470465, Final Batch Loss: 0.00017118052346631885\n",
      "Epoch 1892, Loss: 0.021514389183721505, Final Batch Loss: 0.001346190576441586\n",
      "Epoch 1893, Loss: 0.03578825151635101, Final Batch Loss: 0.0033894155640155077\n",
      "Epoch 1894, Loss: 0.017066121254174504, Final Batch Loss: 0.00018970922974403948\n",
      "Epoch 1895, Loss: 0.04556324541408685, Final Batch Loss: 4.4061864173272625e-05\n",
      "Epoch 1896, Loss: 0.01828479062533006, Final Batch Loss: 0.010499288327991962\n",
      "Epoch 1897, Loss: 0.008652169795823283, Final Batch Loss: 0.0006266876007430255\n",
      "Epoch 1898, Loss: 0.007541606217273511, Final Batch Loss: 0.0003962316841352731\n",
      "Epoch 1899, Loss: 0.006303924044914311, Final Batch Loss: 0.0010258867405354977\n",
      "Epoch 1900, Loss: 0.009337205388874281, Final Batch Loss: 0.0004658680409193039\n",
      "Epoch 1901, Loss: 0.015741110506496625, Final Batch Loss: 0.0003502283070702106\n",
      "Epoch 1902, Loss: 0.00419498179689981, Final Batch Loss: 0.0004213018692098558\n",
      "Epoch 1903, Loss: 0.02270219204001478, Final Batch Loss: 0.0001728670613374561\n",
      "Epoch 1904, Loss: 0.031036175198096316, Final Batch Loss: 0.0002484174619894475\n",
      "Epoch 1905, Loss: 0.017166809764603386, Final Batch Loss: 0.00011348360567353666\n",
      "Epoch 1906, Loss: 0.023365760174783645, Final Batch Loss: 0.0032694439869374037\n",
      "Epoch 1907, Loss: 0.02822666363135795, Final Batch Loss: 0.0030018207617104053\n",
      "Epoch 1908, Loss: 0.09297303433413617, Final Batch Loss: 0.004033478442579508\n",
      "Epoch 1909, Loss: 0.04853078552696388, Final Batch Loss: 0.0009494214318692684\n",
      "Epoch 1910, Loss: 0.03428444475866854, Final Batch Loss: 0.0002271981065860018\n",
      "Epoch 1911, Loss: 0.03257274899078766, Final Batch Loss: 0.00028741249116137624\n",
      "Epoch 1912, Loss: 0.013159395719412714, Final Batch Loss: 0.004495875909924507\n",
      "Epoch 1913, Loss: 0.014880663347867085, Final Batch Loss: 4.665357482735999e-05\n",
      "Epoch 1914, Loss: 0.05778164251023554, Final Batch Loss: 0.00038774675340391695\n",
      "Epoch 1915, Loss: 0.05218715362570947, Final Batch Loss: 0.0024795993231236935\n",
      "Epoch 1916, Loss: 0.03509568951631081, Final Batch Loss: 0.0009024736937135458\n",
      "Epoch 1917, Loss: 0.038685965904733166, Final Batch Loss: 0.000331184797687456\n",
      "Epoch 1918, Loss: 0.01220448596359347, Final Batch Loss: 0.00010282942093908787\n",
      "Epoch 1919, Loss: 0.023795758810592815, Final Batch Loss: 0.00036906360764987767\n",
      "Epoch 1920, Loss: 0.025958761136280373, Final Batch Loss: 0.019510064274072647\n",
      "Epoch 1921, Loss: 0.028661053969699424, Final Batch Loss: 8.672693365952e-05\n",
      "Epoch 1922, Loss: 0.009321656096290099, Final Batch Loss: 0.000425588310463354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1923, Loss: 0.006167251645820215, Final Batch Loss: 0.00046963978093117476\n",
      "Epoch 1924, Loss: 0.0052097431871516164, Final Batch Loss: 0.001545129925943911\n",
      "Epoch 1925, Loss: 0.021307917762896977, Final Batch Loss: 0.0007604279671795666\n",
      "Epoch 1926, Loss: 0.015830941600142978, Final Batch Loss: 7.892862049629912e-05\n",
      "Epoch 1927, Loss: 0.06916663073570817, Final Batch Loss: 0.0006605056114494801\n",
      "Epoch 1928, Loss: 0.012726956898404751, Final Batch Loss: 0.0066453213803470135\n",
      "Epoch 1929, Loss: 0.03212878425256349, Final Batch Loss: 0.0010882308706641197\n",
      "Epoch 1930, Loss: 0.032312100745912176, Final Batch Loss: 0.0011318341130390763\n",
      "Epoch 1931, Loss: 0.02664121189445723, Final Batch Loss: 0.00043312658090144396\n",
      "Epoch 1932, Loss: 0.03763665451697307, Final Batch Loss: 0.0024860927369445562\n",
      "Epoch 1933, Loss: 0.02493733320989122, Final Batch Loss: 0.00015199353219941258\n",
      "Epoch 1934, Loss: 0.052652318536274834, Final Batch Loss: 0.04032350704073906\n",
      "Epoch 1935, Loss: 0.01445135416724952, Final Batch Loss: 0.00559845007956028\n",
      "Epoch 1936, Loss: 0.03850965037418064, Final Batch Loss: 0.00039830553578212857\n",
      "Epoch 1937, Loss: 0.036699807897093706, Final Batch Loss: 0.009413632564246655\n",
      "Epoch 1938, Loss: 0.04676472318533342, Final Batch Loss: 0.0012698168866336346\n",
      "Epoch 1939, Loss: 0.024608281746623106, Final Batch Loss: 0.00013595132622867823\n",
      "Epoch 1940, Loss: 0.1110815705069399, Final Batch Loss: 0.0004963030805811286\n",
      "Epoch 1941, Loss: 0.07045321904297452, Final Batch Loss: 0.0031887581571936607\n",
      "Epoch 1942, Loss: 0.13149891298962757, Final Batch Loss: 0.05016518756747246\n",
      "Epoch 1943, Loss: 0.07268236610980239, Final Batch Loss: 0.014972871169447899\n",
      "Epoch 1944, Loss: 0.0607775327807758, Final Batch Loss: 0.008599067106842995\n",
      "Epoch 1945, Loss: 0.0746833992889151, Final Batch Loss: 0.000886638939846307\n",
      "Epoch 1946, Loss: 0.049904027706361376, Final Batch Loss: 0.0006657031481154263\n",
      "Epoch 1947, Loss: 0.07190387179434765, Final Batch Loss: 0.002874775091186166\n",
      "Epoch 1948, Loss: 0.05848043660807889, Final Batch Loss: 0.0001997294748434797\n",
      "Epoch 1949, Loss: 0.022220459548407234, Final Batch Loss: 0.0010371372336521745\n",
      "Epoch 1950, Loss: 0.051230140365078114, Final Batch Loss: 0.0003910759405698627\n",
      "Epoch 1951, Loss: 0.015604241329128854, Final Batch Loss: 0.0009854109957814217\n",
      "Epoch 1952, Loss: 0.0073088238314085174, Final Batch Loss: 0.001422354718670249\n",
      "Epoch 1953, Loss: 0.03142890862363856, Final Batch Loss: 0.0007772890385240316\n",
      "Epoch 1954, Loss: 0.007923317774839234, Final Batch Loss: 0.0002552699588704854\n",
      "Epoch 1955, Loss: 0.02800504227343481, Final Batch Loss: 0.0004616204823832959\n",
      "Epoch 1956, Loss: 0.04951918023289181, Final Batch Loss: 0.001543459016829729\n",
      "Epoch 1957, Loss: 0.02209959276660811, Final Batch Loss: 0.0014760796912014484\n",
      "Epoch 1958, Loss: 0.016419713210780174, Final Batch Loss: 0.0026142997667193413\n",
      "Epoch 1959, Loss: 0.007781080377753824, Final Batch Loss: 0.00039742348599247634\n",
      "Epoch 1960, Loss: 0.007568615663331002, Final Batch Loss: 0.001142200082540512\n",
      "Epoch 1961, Loss: 0.02839502973802155, Final Batch Loss: 9.294040501117706e-05\n",
      "Epoch 1962, Loss: 0.009379361985338619, Final Batch Loss: 0.0003151863347738981\n",
      "Epoch 1963, Loss: 0.03044906428476679, Final Batch Loss: 4.061567597091198e-05\n",
      "Epoch 1964, Loss: 0.009826433080888819, Final Batch Loss: 3.762389678740874e-05\n",
      "Epoch 1965, Loss: 0.02915979950739711, Final Batch Loss: 0.0008380199433304369\n",
      "Epoch 1966, Loss: 0.014902475853887154, Final Batch Loss: 3.811712304013781e-05\n",
      "Epoch 1967, Loss: 0.04305447108163207, Final Batch Loss: 0.007432590238749981\n",
      "Epoch 1968, Loss: 0.01559036268372438, Final Batch Loss: 0.0022551207803189754\n",
      "Epoch 1969, Loss: 0.01926033120980719, Final Batch Loss: 4.2661391489673406e-05\n",
      "Epoch 1970, Loss: 0.013687624197700643, Final Batch Loss: 7.939268834888935e-05\n",
      "Epoch 1971, Loss: 0.006368685351844761, Final Batch Loss: 5.8529531088424847e-05\n",
      "Epoch 1972, Loss: 0.0576957175253483, Final Batch Loss: 4.4112250179750845e-05\n",
      "Epoch 1973, Loss: 0.0584313201779878, Final Batch Loss: 0.03512417525053024\n",
      "Epoch 1974, Loss: 0.04799165991062182, Final Batch Loss: 0.0003449146752245724\n",
      "Epoch 1975, Loss: 0.01630781939456938, Final Batch Loss: 7.018349424470216e-05\n",
      "Epoch 1976, Loss: 0.031019214351545088, Final Batch Loss: 0.0003434390528127551\n",
      "Epoch 1977, Loss: 0.02178602990898071, Final Batch Loss: 0.00013263963046483696\n",
      "Epoch 1978, Loss: 0.012361386434349697, Final Batch Loss: 0.00029185158200562\n",
      "Epoch 1979, Loss: 0.012410210549205658, Final Batch Loss: 7.859782635932788e-05\n",
      "Epoch 1980, Loss: 0.008952527987275971, Final Batch Loss: 0.0011026363354176283\n",
      "Epoch 1981, Loss: 0.030887411263393005, Final Batch Loss: 0.0002958887198474258\n",
      "Epoch 1982, Loss: 0.04702199884195579, Final Batch Loss: 0.0011350390268489718\n",
      "Epoch 1983, Loss: 0.16074173100059852, Final Batch Loss: 0.0005532295908778906\n",
      "Epoch 1984, Loss: 0.23111392917053308, Final Batch Loss: 0.0437578447163105\n",
      "Epoch 1985, Loss: 0.08956227160524577, Final Batch Loss: 0.0006162954960018396\n",
      "Epoch 1986, Loss: 0.14987666450906545, Final Batch Loss: 0.0063260337337851524\n",
      "Epoch 1987, Loss: 0.089453152671922, Final Batch Loss: 0.005129231605678797\n",
      "Epoch 1988, Loss: 0.06949321401771158, Final Batch Loss: 0.0018999935127794743\n",
      "Epoch 1989, Loss: 0.04244074622693006, Final Batch Loss: 0.004033759701997042\n",
      "Epoch 1990, Loss: 0.050998107850318775, Final Batch Loss: 0.0013355191331356764\n",
      "Epoch 1991, Loss: 0.08531821629730985, Final Batch Loss: 0.0016534703318029642\n",
      "Epoch 1992, Loss: 0.03584671144199092, Final Batch Loss: 0.015575367026031017\n",
      "Epoch 1993, Loss: 0.032157070440007374, Final Batch Loss: 0.0006138436147011817\n",
      "Epoch 1994, Loss: 0.02968273649457842, Final Batch Loss: 0.0034651802852749825\n",
      "Epoch 1995, Loss: 0.05120383278699592, Final Batch Loss: 0.013617807067930698\n",
      "Epoch 1996, Loss: 0.012126358924433589, Final Batch Loss: 0.0005318750627338886\n",
      "Epoch 1997, Loss: 0.014364622296852758, Final Batch Loss: 7.792580436216667e-05\n",
      "Epoch 1998, Loss: 0.0127804725743772, Final Batch Loss: 4.653288851841353e-05\n",
      "Epoch 1999, Loss: 0.03082212603476364, Final Batch Loss: 0.002543352311477065\n",
      "Epoch 2000, Loss: 0.02703962520172354, Final Batch Loss: 0.014959772117435932\n",
      "Epoch 2001, Loss: 0.01006852532736957, Final Batch Loss: 6.601792847504839e-05\n",
      "Epoch 2002, Loss: 0.025683691772428574, Final Batch Loss: 0.0008067821618169546\n",
      "Epoch 2003, Loss: 0.06938458317017648, Final Batch Loss: 0.004233561921864748\n",
      "Epoch 2004, Loss: 0.07257402983668726, Final Batch Loss: 0.000706095015630126\n",
      "Epoch 2005, Loss: 0.04471423114591744, Final Batch Loss: 0.008109957911074162\n",
      "Epoch 2006, Loss: 0.02004831407975871, Final Batch Loss: 0.00444438960403204\n",
      "Epoch 2007, Loss: 0.021548233944486128, Final Batch Loss: 0.0010106610134243965\n",
      "Epoch 2008, Loss: 0.011329293265589513, Final Batch Loss: 9.958224109141156e-05\n",
      "Epoch 2009, Loss: 0.014744500493179657, Final Batch Loss: 0.005202374886721373\n",
      "Epoch 2010, Loss: 0.026412934268591926, Final Batch Loss: 0.0032480256631970406\n",
      "Epoch 2011, Loss: 0.005537314329558285, Final Batch Loss: 8.256277942564338e-05\n",
      "Epoch 2012, Loss: 0.022368566424120218, Final Batch Loss: 0.0008308076066896319\n",
      "Epoch 2013, Loss: 0.0072032665157166775, Final Batch Loss: 0.003029761603102088\n",
      "Epoch 2014, Loss: 0.008683026260769111, Final Batch Loss: 6.249627767829224e-05\n",
      "Epoch 2015, Loss: 0.01217274448936223, Final Batch Loss: 0.00048132363008335233\n",
      "Epoch 2016, Loss: 0.014078194355533924, Final Batch Loss: 0.0002359676145715639\n",
      "Epoch 2017, Loss: 0.0343829076809925, Final Batch Loss: 0.00010674512304831296\n",
      "Epoch 2018, Loss: 0.0855794005728967, Final Batch Loss: 0.008183356374502182\n",
      "Epoch 2019, Loss: 0.10678155010100454, Final Batch Loss: 0.00027387781301513314\n",
      "Epoch 2020, Loss: 0.03290243479204946, Final Batch Loss: 0.00810127891600132\n",
      "Epoch 2021, Loss: 0.025453187663515564, Final Batch Loss: 0.0006109087844379246\n",
      "Epoch 2022, Loss: 0.017517668442451395, Final Batch Loss: 0.0003947838849853724\n",
      "Epoch 2023, Loss: 0.018231648977234727, Final Batch Loss: 0.0002916540252044797\n",
      "Epoch 2024, Loss: 0.028257327634491958, Final Batch Loss: 0.0040028584189713\n",
      "Epoch 2025, Loss: 0.0219142375899537, Final Batch Loss: 0.0002217982109868899\n",
      "Epoch 2026, Loss: 0.01778203914727783, Final Batch Loss: 0.0003572727437131107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2027, Loss: 0.0053379541996037005, Final Batch Loss: 0.0002537247200962156\n",
      "Epoch 2028, Loss: 0.01388446552118694, Final Batch Loss: 0.0003712849284056574\n",
      "Epoch 2029, Loss: 0.020310457235609647, Final Batch Loss: 0.0002567583869677037\n",
      "Epoch 2030, Loss: 0.0466751449930598, Final Batch Loss: 0.00038345513166859746\n",
      "Epoch 2031, Loss: 0.050800727200112306, Final Batch Loss: 0.0009192035649903119\n",
      "Epoch 2032, Loss: 0.05926137974165613, Final Batch Loss: 2.4926019250415266e-05\n",
      "Epoch 2033, Loss: 0.025716929249028908, Final Batch Loss: 5.5491796956630424e-05\n",
      "Epoch 2034, Loss: 0.01766448344278615, Final Batch Loss: 0.001170719275251031\n",
      "Epoch 2035, Loss: 0.04939897419353656, Final Batch Loss: 0.00016136639169417322\n",
      "Epoch 2036, Loss: 0.023541541529993992, Final Batch Loss: 0.0006921493913978338\n",
      "Epoch 2037, Loss: 0.03497761888138484, Final Batch Loss: 2.2321553842630237e-05\n",
      "Epoch 2038, Loss: 0.023516966690294794, Final Batch Loss: 0.0005135859828442335\n",
      "Epoch 2039, Loss: 0.05038231313028518, Final Batch Loss: 0.0004451711429283023\n",
      "Epoch 2040, Loss: 0.037598511118630995, Final Batch Loss: 0.024574974551796913\n",
      "Epoch 2041, Loss: 0.01593135204348073, Final Batch Loss: 0.000825823750346899\n",
      "Epoch 2042, Loss: 0.04000513105529535, Final Batch Loss: 0.0011360399657860398\n",
      "Epoch 2043, Loss: 0.1062968260703201, Final Batch Loss: 0.03111056238412857\n",
      "Epoch 2044, Loss: 0.08477691516600316, Final Batch Loss: 0.00010660959378583357\n",
      "Epoch 2045, Loss: 0.01836023712530732, Final Batch Loss: 0.00023168894404079765\n",
      "Epoch 2046, Loss: 0.011982332202023827, Final Batch Loss: 0.002397951204329729\n",
      "Epoch 2047, Loss: 0.034619197647771216, Final Batch Loss: 0.004662869498133659\n",
      "Epoch 2048, Loss: 0.038039772342017386, Final Batch Loss: 0.0012822470162063837\n",
      "Epoch 2049, Loss: 0.06209786213730695, Final Batch Loss: 0.00010561887756921351\n",
      "Epoch 2050, Loss: 0.011270270828390494, Final Batch Loss: 0.0005464526475407183\n",
      "Epoch 2051, Loss: 0.009716193810163531, Final Batch Loss: 0.00048079792759381235\n",
      "Epoch 2052, Loss: 0.008398110345297027, Final Batch Loss: 0.0001634872314753011\n",
      "Epoch 2053, Loss: 0.06546633462130558, Final Batch Loss: 0.00499316630885005\n",
      "Epoch 2054, Loss: 0.031102933644433506, Final Batch Loss: 0.00254620797932148\n",
      "Epoch 2055, Loss: 0.027508622581081, Final Batch Loss: 0.01803751289844513\n",
      "Epoch 2056, Loss: 0.020454602490644902, Final Batch Loss: 0.0006048170616850257\n",
      "Epoch 2057, Loss: 0.047237737533578184, Final Batch Loss: 0.010507248342037201\n",
      "Epoch 2058, Loss: 0.03550474980875151, Final Batch Loss: 4.7730754886288196e-05\n",
      "Epoch 2059, Loss: 0.03979589698064956, Final Batch Loss: 0.00044191721826791763\n",
      "Epoch 2060, Loss: 0.008389688286115415, Final Batch Loss: 0.0005293006543070078\n",
      "Epoch 2061, Loss: 0.013149345671990886, Final Batch Loss: 0.0013738167472183704\n",
      "Epoch 2062, Loss: 0.0321600339775614, Final Batch Loss: 0.00024364569981116802\n",
      "Epoch 2063, Loss: 0.04763329931665794, Final Batch Loss: 0.00016454637807328254\n",
      "Epoch 2064, Loss: 0.0533465733169578, Final Batch Loss: 0.0008435926865786314\n",
      "Epoch 2065, Loss: 0.027785202721133828, Final Batch Loss: 0.003984296228736639\n",
      "Epoch 2066, Loss: 0.02189199584245216, Final Batch Loss: 0.007577606476843357\n",
      "Epoch 2067, Loss: 0.019068232024437748, Final Batch Loss: 0.00021382831619121134\n",
      "Epoch 2068, Loss: 0.039247804448677925, Final Batch Loss: 0.0013216480147093534\n",
      "Epoch 2069, Loss: 0.04734328491031192, Final Batch Loss: 0.0003645671240519732\n",
      "Epoch 2070, Loss: 0.048645731763826916, Final Batch Loss: 0.0009370030602440238\n",
      "Epoch 2071, Loss: 0.01646727233492129, Final Batch Loss: 0.00021294222096912563\n",
      "Epoch 2072, Loss: 0.052578879418433644, Final Batch Loss: 0.006632772274315357\n",
      "Epoch 2073, Loss: 0.06470882386929588, Final Batch Loss: 0.005201265215873718\n",
      "Epoch 2074, Loss: 0.051459259251714684, Final Batch Loss: 0.0004965860862284899\n",
      "Epoch 2075, Loss: 0.03078728201580816, Final Batch Loss: 0.009238515980541706\n",
      "Epoch 2076, Loss: 0.024535953540180344, Final Batch Loss: 0.0028169197030365467\n",
      "Epoch 2077, Loss: 0.021430690336273983, Final Batch Loss: 0.0006653942400589585\n",
      "Epoch 2078, Loss: 0.03195423879515147, Final Batch Loss: 0.01190117560327053\n",
      "Epoch 2079, Loss: 0.09817089848183969, Final Batch Loss: 0.0039047864265739918\n",
      "Epoch 2080, Loss: 0.05029862909577787, Final Batch Loss: 0.004004525952041149\n",
      "Epoch 2081, Loss: 0.19908922402828466, Final Batch Loss: 0.011657715775072575\n",
      "Epoch 2082, Loss: 0.2086100988672115, Final Batch Loss: 0.001630301820114255\n",
      "Epoch 2083, Loss: 0.1221065959543921, Final Batch Loss: 0.003762465203180909\n",
      "Epoch 2084, Loss: 0.08122937002917752, Final Batch Loss: 0.0280302707105875\n",
      "Epoch 2085, Loss: 0.05451707627798896, Final Batch Loss: 0.007026609033346176\n",
      "Epoch 2086, Loss: 0.016998316772514954, Final Batch Loss: 0.0005310550332069397\n",
      "Epoch 2087, Loss: 0.05071093389415182, Final Batch Loss: 0.001539051765576005\n",
      "Epoch 2088, Loss: 0.032876701196073554, Final Batch Loss: 0.00022149491996970028\n",
      "Epoch 2089, Loss: 0.06652528028644156, Final Batch Loss: 0.0003479622246231884\n",
      "Epoch 2090, Loss: 0.02416525664011715, Final Batch Loss: 0.0015472997911274433\n",
      "Epoch 2091, Loss: 0.03669180631550262, Final Batch Loss: 0.00019925602828152478\n",
      "Epoch 2092, Loss: 0.01983620705868816, Final Batch Loss: 0.002622151980176568\n",
      "Epoch 2093, Loss: 0.14759934188987245, Final Batch Loss: 0.00015468701894860715\n",
      "Epoch 2094, Loss: 0.0353636717190966, Final Batch Loss: 0.0024153394624590874\n",
      "Epoch 2095, Loss: 0.072376375695967, Final Batch Loss: 0.0003254118491895497\n",
      "Epoch 2096, Loss: 0.032500304623681586, Final Batch Loss: 0.0003901547461282462\n",
      "Epoch 2097, Loss: 0.01546349261479918, Final Batch Loss: 0.000332418130710721\n",
      "Epoch 2098, Loss: 0.054530790032004006, Final Batch Loss: 0.0019788502249866724\n",
      "Epoch 2099, Loss: 0.013237061924883164, Final Batch Loss: 0.0002893337805289775\n",
      "Epoch 2100, Loss: 0.025161659650621004, Final Batch Loss: 0.00041855877498164773\n",
      "Epoch 2101, Loss: 0.04991276218788698, Final Batch Loss: 0.00035652704536914825\n",
      "Epoch 2102, Loss: 0.03425071176025085, Final Batch Loss: 0.010724429041147232\n",
      "Epoch 2103, Loss: 0.09822863261797465, Final Batch Loss: 0.009426305070519447\n",
      "Epoch 2104, Loss: 0.02633969883027021, Final Batch Loss: 0.0007913883309811354\n",
      "Epoch 2105, Loss: 0.10453402250277577, Final Batch Loss: 9.161264460999519e-05\n",
      "Epoch 2106, Loss: 0.039178057879325934, Final Batch Loss: 0.005483285989612341\n",
      "Epoch 2107, Loss: 0.03399745425849687, Final Batch Loss: 0.0025020544417202473\n",
      "Epoch 2108, Loss: 0.016560819873120636, Final Batch Loss: 0.0008716866723261774\n",
      "Epoch 2109, Loss: 0.015900506266916636, Final Batch Loss: 0.0017546765739098191\n",
      "Epoch 2110, Loss: 0.02688935439073248, Final Batch Loss: 0.0003853059315588325\n",
      "Epoch 2111, Loss: 0.0470996438380098, Final Batch Loss: 0.021584894508123398\n",
      "Epoch 2112, Loss: 0.007938793511129916, Final Batch Loss: 0.0005173538811504841\n",
      "Epoch 2113, Loss: 0.03500488261488499, Final Batch Loss: 0.020176449790596962\n",
      "Epoch 2114, Loss: 0.05202298735821387, Final Batch Loss: 0.021404001861810684\n",
      "Epoch 2115, Loss: 0.06170260011276696, Final Batch Loss: 0.0008818551432341337\n",
      "Epoch 2116, Loss: 0.04598970700317295, Final Batch Loss: 0.0006096170982345939\n",
      "Epoch 2117, Loss: 0.04227374693437014, Final Batch Loss: 0.00017907404981087893\n",
      "Epoch 2118, Loss: 0.01990288514934946, Final Batch Loss: 6.2974460888654e-05\n",
      "Epoch 2119, Loss: 0.028784222737158416, Final Batch Loss: 0.0002110214118147269\n",
      "Epoch 2120, Loss: 0.05107610368577298, Final Batch Loss: 0.0008777005132287741\n",
      "Epoch 2121, Loss: 0.027534488079254515, Final Batch Loss: 0.00025408153305761516\n",
      "Epoch 2122, Loss: 0.029800995442201383, Final Batch Loss: 0.0021157357841730118\n",
      "Epoch 2123, Loss: 0.014986851761932485, Final Batch Loss: 0.005703268572688103\n",
      "Epoch 2124, Loss: 0.020775131706614047, Final Batch Loss: 0.00025526381796225905\n",
      "Epoch 2125, Loss: 0.031757481727254344, Final Batch Loss: 0.0001568556035635993\n",
      "Epoch 2126, Loss: 0.0468177397378895, Final Batch Loss: 0.00048501393757760525\n",
      "Epoch 2127, Loss: 0.013788522563118022, Final Batch Loss: 0.0041519817896187305\n",
      "Epoch 2128, Loss: 0.0054477081866934896, Final Batch Loss: 4.75006308988668e-05\n",
      "Epoch 2129, Loss: 0.03665985660336446, Final Batch Loss: 0.0001758221333147958\n",
      "Epoch 2130, Loss: 0.041570376823074184, Final Batch Loss: 0.0029533342458307743\n",
      "Epoch 2131, Loss: 0.019031017938687, Final Batch Loss: 0.00010914945596596226\n",
      "Epoch 2132, Loss: 0.007809003254806157, Final Batch Loss: 0.000545861606951803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2133, Loss: 0.0462167953082826, Final Batch Loss: 0.006838130299001932\n",
      "Epoch 2134, Loss: 0.04635802994744154, Final Batch Loss: 0.0017979891272261739\n",
      "Epoch 2135, Loss: 0.018406401497486513, Final Batch Loss: 0.0001963773393072188\n",
      "Epoch 2136, Loss: 0.030446653719991446, Final Batch Loss: 0.00019477176829241216\n",
      "Epoch 2137, Loss: 0.012663990836699668, Final Batch Loss: 0.00023935182252898812\n",
      "Epoch 2138, Loss: 0.013705969675356755, Final Batch Loss: 0.0001637441455386579\n",
      "Epoch 2139, Loss: 0.01065450281748781, Final Batch Loss: 0.0002451241889502853\n",
      "Epoch 2140, Loss: 0.04021480308256287, Final Batch Loss: 0.0015100230230018497\n",
      "Epoch 2141, Loss: 0.05659247428593517, Final Batch Loss: 0.0016107462579384446\n",
      "Epoch 2142, Loss: 0.01562859605473932, Final Batch Loss: 0.00036208273377269506\n",
      "Epoch 2143, Loss: 0.049458799287094735, Final Batch Loss: 0.0032848478294909\n",
      "Epoch 2144, Loss: 0.08048872562358156, Final Batch Loss: 0.00031582106021232903\n",
      "Epoch 2145, Loss: 0.02159650313842576, Final Batch Loss: 0.0029448627028614283\n",
      "Epoch 2146, Loss: 0.022374440855855937, Final Batch Loss: 2.0151912394794635e-05\n",
      "Epoch 2147, Loss: 0.020506913102508406, Final Batch Loss: 0.004023215267807245\n",
      "Epoch 2148, Loss: 0.054012577907997184, Final Batch Loss: 0.00013228831812739372\n",
      "Epoch 2149, Loss: 0.008916285779378086, Final Batch Loss: 0.0003698507498484105\n",
      "Epoch 2150, Loss: 0.021180000909225782, Final Batch Loss: 0.0015513109974563122\n",
      "Epoch 2151, Loss: 0.02284344886720646, Final Batch Loss: 0.0001665392192080617\n",
      "Epoch 2152, Loss: 0.030886582389939576, Final Batch Loss: 0.0024553134571760893\n",
      "Epoch 2153, Loss: 0.09156539663672447, Final Batch Loss: 0.0010266590397804976\n",
      "Epoch 2154, Loss: 0.06613445196126122, Final Batch Loss: 0.0032962041441351175\n",
      "Epoch 2155, Loss: 0.02541635471061454, Final Batch Loss: 0.001245122286491096\n",
      "Epoch 2156, Loss: 0.012047344287566375, Final Batch Loss: 0.0017481977120041847\n",
      "Epoch 2157, Loss: 0.006049652867659461, Final Batch Loss: 0.00017446796118747443\n",
      "Epoch 2158, Loss: 0.011665383499348536, Final Batch Loss: 0.0010103237582370639\n",
      "Epoch 2159, Loss: 0.0188573521672879, Final Batch Loss: 7.271079084603116e-05\n",
      "Epoch 2160, Loss: 0.009724720322992653, Final Batch Loss: 7.609419844811782e-05\n",
      "Epoch 2161, Loss: 0.009265630043955753, Final Batch Loss: 0.0027379689272493124\n",
      "Epoch 2162, Loss: 0.007220164443424437, Final Batch Loss: 3.121071131317876e-05\n",
      "Epoch 2163, Loss: 0.013371576627832837, Final Batch Loss: 0.002125578001141548\n",
      "Epoch 2164, Loss: 0.011614581606409047, Final Batch Loss: 3.582626231946051e-05\n",
      "Epoch 2165, Loss: 0.058859581000433536, Final Batch Loss: 3.864342943415977e-05\n",
      "Epoch 2166, Loss: 0.010791505457746098, Final Batch Loss: 0.0004208509053569287\n",
      "Epoch 2167, Loss: 0.07210358259908389, Final Batch Loss: 0.00028979688067920506\n",
      "Epoch 2168, Loss: 0.05389648404889158, Final Batch Loss: 0.0033348246943205595\n",
      "Epoch 2169, Loss: 0.04611273479895317, Final Batch Loss: 0.0010492565343156457\n",
      "Epoch 2170, Loss: 0.08036137915041763, Final Batch Loss: 0.000226390446186997\n",
      "Epoch 2171, Loss: 0.055622748972382396, Final Batch Loss: 0.010620386339724064\n",
      "Epoch 2172, Loss: 0.06323735787009355, Final Batch Loss: 0.00015660053759347647\n",
      "Epoch 2173, Loss: 0.030086374266829807, Final Batch Loss: 0.00011420874943723902\n",
      "Epoch 2174, Loss: 0.025487012098892592, Final Batch Loss: 0.0002822224923875183\n",
      "Epoch 2175, Loss: 0.06978291212726617, Final Batch Loss: 0.00024143920745700598\n",
      "Epoch 2176, Loss: 0.032313031813828275, Final Batch Loss: 0.0002055795630440116\n",
      "Epoch 2177, Loss: 0.09274348163307877, Final Batch Loss: 0.00012036183034069836\n",
      "Epoch 2178, Loss: 0.03215667009499157, Final Batch Loss: 0.0010131022427231073\n",
      "Epoch 2179, Loss: 0.03749557267292403, Final Batch Loss: 0.0016963251400738955\n",
      "Epoch 2180, Loss: 0.013417944450338837, Final Batch Loss: 0.0011407063575461507\n",
      "Epoch 2181, Loss: 0.05709049676806899, Final Batch Loss: 0.0036781164817512035\n",
      "Epoch 2182, Loss: 0.0173806747643539, Final Batch Loss: 0.0006907173665240407\n",
      "Epoch 2183, Loss: 0.033579655799258035, Final Batch Loss: 0.0008746148669160903\n",
      "Epoch 2184, Loss: 0.03462808065523859, Final Batch Loss: 0.00034216209314763546\n",
      "Epoch 2185, Loss: 0.007985940679645864, Final Batch Loss: 0.0003918120637536049\n",
      "Epoch 2186, Loss: 0.0949842574482318, Final Batch Loss: 0.001672833925113082\n",
      "Epoch 2187, Loss: 0.06676548838731833, Final Batch Loss: 0.002596923615783453\n",
      "Epoch 2188, Loss: 0.037207370092801284, Final Batch Loss: 0.005477991886436939\n",
      "Epoch 2189, Loss: 0.03499468177324161, Final Batch Loss: 0.000516472035087645\n",
      "Epoch 2190, Loss: 0.03559593021054752, Final Batch Loss: 0.025347666814923286\n",
      "Epoch 2191, Loss: 0.08116693029296584, Final Batch Loss: 0.018159570172429085\n",
      "Epoch 2192, Loss: 0.06165949861315312, Final Batch Loss: 0.0001883444347186014\n",
      "Epoch 2193, Loss: 0.04336632428021403, Final Batch Loss: 0.004047484137117863\n",
      "Epoch 2194, Loss: 0.016339906231223722, Final Batch Loss: 0.0043895249255001545\n",
      "Epoch 2195, Loss: 0.04771128275751835, Final Batch Loss: 0.00018251764413435012\n",
      "Epoch 2196, Loss: 0.008207486553146737, Final Batch Loss: 0.0008787669357843697\n",
      "Epoch 2197, Loss: 0.019744120007089805, Final Batch Loss: 0.003276234492659569\n",
      "Epoch 2198, Loss: 0.02830632081895601, Final Batch Loss: 0.0003320964169688523\n",
      "Epoch 2199, Loss: 0.012927317671710625, Final Batch Loss: 0.0001219103141920641\n",
      "Epoch 2200, Loss: 0.041727522289875196, Final Batch Loss: 0.0005615722038783133\n",
      "Epoch 2201, Loss: 0.028348734173050616, Final Batch Loss: 0.0003925907949451357\n",
      "Epoch 2202, Loss: 0.07903862712191767, Final Batch Loss: 0.050610028207302094\n",
      "Epoch 2203, Loss: 0.033732228286680765, Final Batch Loss: 5.855887138750404e-05\n",
      "Epoch 2204, Loss: 0.03299303169478662, Final Batch Loss: 0.0005174201796762645\n",
      "Epoch 2205, Loss: 0.013701018768188078, Final Batch Loss: 0.00014261082105804235\n",
      "Epoch 2206, Loss: 0.03701745926809963, Final Batch Loss: 0.00733675854280591\n",
      "Epoch 2207, Loss: 0.062041851357207634, Final Batch Loss: 0.004199868068099022\n",
      "Epoch 2208, Loss: 0.03075635730056092, Final Batch Loss: 0.0015599426114931703\n",
      "Epoch 2209, Loss: 0.04820466923411004, Final Batch Loss: 0.0030214085709303617\n",
      "Epoch 2210, Loss: 0.11947544058784842, Final Batch Loss: 0.014120818115770817\n",
      "Epoch 2211, Loss: 0.027881983420229517, Final Batch Loss: 0.0023319371975958347\n",
      "Epoch 2212, Loss: 0.043370594445150346, Final Batch Loss: 0.000154764304170385\n",
      "Epoch 2213, Loss: 0.04046168031345587, Final Batch Loss: 0.02036687172949314\n",
      "Epoch 2214, Loss: 0.0166638149676146, Final Batch Loss: 0.00038749960367567837\n",
      "Epoch 2215, Loss: 0.02268115214246791, Final Batch Loss: 0.000614620978012681\n",
      "Epoch 2216, Loss: 0.054506342588865664, Final Batch Loss: 0.0006228424608707428\n",
      "Epoch 2217, Loss: 0.05064730186131783, Final Batch Loss: 0.01416398212313652\n",
      "Epoch 2218, Loss: 0.017436467263905797, Final Batch Loss: 0.00037865410558879375\n",
      "Epoch 2219, Loss: 0.02874568051629467, Final Batch Loss: 0.0007721498259343207\n",
      "Epoch 2220, Loss: 0.02016896971690585, Final Batch Loss: 0.00025534926680848\n",
      "Epoch 2221, Loss: 0.015195638676232193, Final Batch Loss: 0.009996947832405567\n",
      "Epoch 2222, Loss: 0.011364707637767424, Final Batch Loss: 0.0006976992590352893\n",
      "Epoch 2223, Loss: 0.009093598517210921, Final Batch Loss: 0.00018375966465100646\n",
      "Epoch 2224, Loss: 0.007574077575554838, Final Batch Loss: 7.44932985980995e-05\n",
      "Epoch 2225, Loss: 0.004741860688227462, Final Batch Loss: 0.00019889030954800546\n",
      "Epoch 2226, Loss: 0.02802073634666158, Final Batch Loss: 0.00013190583558753133\n",
      "Epoch 2227, Loss: 0.010314945058780722, Final Batch Loss: 0.0007733942475169897\n",
      "Epoch 2228, Loss: 0.008382997679291293, Final Batch Loss: 0.0034043192863464355\n",
      "Epoch 2229, Loss: 0.037537953598075546, Final Batch Loss: 0.00015178901958279312\n",
      "Epoch 2230, Loss: 0.027983969723209157, Final Batch Loss: 0.00011733932478819042\n",
      "Epoch 2231, Loss: 0.016246339262579568, Final Batch Loss: 0.002297386061400175\n",
      "Epoch 2232, Loss: 0.007372220414254116, Final Batch Loss: 0.00045963298180140555\n",
      "Epoch 2233, Loss: 0.07911302208231064, Final Batch Loss: 0.006541768554598093\n",
      "Epoch 2234, Loss: 0.11793194951314945, Final Batch Loss: 0.00024106113414745778\n",
      "Epoch 2235, Loss: 0.05560308501299005, Final Batch Loss: 0.002904460998252034\n",
      "Epoch 2236, Loss: 0.04208594145893585, Final Batch Loss: 0.0004344341577962041\n",
      "Epoch 2237, Loss: 0.01415051367803244, Final Batch Loss: 0.00019880970648955554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2238, Loss: 0.006775713685783558, Final Batch Loss: 0.0006196942995302379\n",
      "Epoch 2239, Loss: 0.006343784676573705, Final Batch Loss: 0.0001549338921904564\n",
      "Epoch 2240, Loss: 0.025263510509830667, Final Batch Loss: 0.00010545886470936239\n",
      "Epoch 2241, Loss: 0.006454495669458993, Final Batch Loss: 5.7822897360892966e-05\n",
      "Epoch 2242, Loss: 0.05055893422104418, Final Batch Loss: 0.0018088817596435547\n",
      "Epoch 2243, Loss: 0.03350109194434481, Final Batch Loss: 0.00018151979020331055\n",
      "Epoch 2244, Loss: 0.014690478714328492, Final Batch Loss: 5.7993183872895315e-05\n",
      "Epoch 2245, Loss: 0.012573373460327275, Final Batch Loss: 0.0005178521969355643\n",
      "Epoch 2246, Loss: 0.011170670193678234, Final Batch Loss: 5.925786172156222e-05\n",
      "Epoch 2247, Loss: 0.011721731303623528, Final Batch Loss: 0.0004020292544737458\n",
      "Epoch 2248, Loss: 0.04038729139210773, Final Batch Loss: 4.3799838749691844e-05\n",
      "Epoch 2249, Loss: 0.041205200453987345, Final Batch Loss: 0.00019040305051021278\n",
      "Epoch 2250, Loss: 0.034136721296817996, Final Batch Loss: 0.0003753547207452357\n",
      "Epoch 2251, Loss: 0.030930880457162857, Final Batch Loss: 0.0005096564418636262\n",
      "Epoch 2252, Loss: 0.0687127462515491, Final Batch Loss: 0.01666765660047531\n",
      "Epoch 2253, Loss: 0.06138632624060847, Final Batch Loss: 0.004779171198606491\n",
      "Epoch 2254, Loss: 0.054789630623417906, Final Batch Loss: 0.0030516129918396473\n",
      "Epoch 2255, Loss: 0.0897832467671833, Final Batch Loss: 0.004066163673996925\n",
      "Epoch 2256, Loss: 0.19059061992447823, Final Batch Loss: 0.00515282666310668\n",
      "Epoch 2257, Loss: 0.09314028681546915, Final Batch Loss: 0.0016225759172812104\n",
      "Epoch 2258, Loss: 0.034224550967337564, Final Batch Loss: 0.0008820000803098083\n",
      "Epoch 2259, Loss: 0.021553627084358595, Final Batch Loss: 0.003817315213382244\n",
      "Epoch 2260, Loss: 0.026419713227369357, Final Batch Loss: 0.00046846939949318767\n",
      "Epoch 2261, Loss: 0.0073624827127787285, Final Batch Loss: 0.0006390762864612043\n",
      "Epoch 2262, Loss: 0.025509791666991077, Final Batch Loss: 0.0005798802594654262\n",
      "Epoch 2263, Loss: 0.02639376871229615, Final Batch Loss: 0.0002746638492681086\n",
      "Epoch 2264, Loss: 0.032265296151308576, Final Batch Loss: 0.0005412257742136717\n",
      "Epoch 2265, Loss: 0.00823086477612378, Final Batch Loss: 0.001265945378690958\n",
      "Epoch 2266, Loss: 0.01567623774826643, Final Batch Loss: 3.847402331302874e-05\n",
      "Epoch 2267, Loss: 0.018290002597495914, Final Batch Loss: 0.000604974920861423\n",
      "Epoch 2268, Loss: 0.01821759535232559, Final Batch Loss: 0.00022353118401952088\n",
      "Epoch 2269, Loss: 0.031515076761934324, Final Batch Loss: 0.0009774797363206744\n",
      "Epoch 2270, Loss: 0.052463338997768005, Final Batch Loss: 6.755244976375252e-05\n",
      "Epoch 2271, Loss: 0.009579379175193026, Final Batch Loss: 0.00023127670283429325\n",
      "Epoch 2272, Loss: 0.010883535509492503, Final Batch Loss: 0.00030267389956861734\n",
      "Epoch 2273, Loss: 0.012150950453360565, Final Batch Loss: 0.00018388756143394858\n",
      "Epoch 2274, Loss: 0.007846285894629546, Final Batch Loss: 0.0005371163133531809\n",
      "Epoch 2275, Loss: 0.005216631328949006, Final Batch Loss: 0.0007352212560363114\n",
      "Epoch 2276, Loss: 0.02069701356595033, Final Batch Loss: 0.00026950420578941703\n",
      "Epoch 2277, Loss: 0.06279880769216106, Final Batch Loss: 0.00020512011542450637\n",
      "Epoch 2278, Loss: 0.06400359221879626, Final Batch Loss: 0.00013111137377563864\n",
      "Epoch 2279, Loss: 0.03387092447519535, Final Batch Loss: 0.00018527060456108302\n",
      "Epoch 2280, Loss: 0.02587104477061075, Final Batch Loss: 0.00025902202469296753\n",
      "Epoch 2281, Loss: 0.029858741894713603, Final Batch Loss: 0.00023990341287571937\n",
      "Epoch 2282, Loss: 0.012048695663906983, Final Batch Loss: 0.0009154820581898093\n",
      "Epoch 2283, Loss: 0.005851569691003533, Final Batch Loss: 7.147678843466565e-05\n",
      "Epoch 2284, Loss: 0.013544864690629765, Final Batch Loss: 0.002353196032345295\n",
      "Epoch 2285, Loss: 0.03299988571961876, Final Batch Loss: 0.014207495376467705\n",
      "Epoch 2286, Loss: 0.014350594796269434, Final Batch Loss: 0.0026971588376909494\n",
      "Epoch 2287, Loss: 0.02936237600806635, Final Batch Loss: 0.00015408631588798016\n",
      "Epoch 2288, Loss: 0.020858872946519114, Final Batch Loss: 0.00014508835738524795\n",
      "Epoch 2289, Loss: 0.0048900552465056535, Final Batch Loss: 0.0002116075629601255\n",
      "Epoch 2290, Loss: 0.009707007513497956, Final Batch Loss: 6.742545519955456e-05\n",
      "Epoch 2291, Loss: 0.010424231932120165, Final Batch Loss: 0.0005805817199870944\n",
      "Epoch 2292, Loss: 0.052712788336066296, Final Batch Loss: 9.46679210755974e-05\n",
      "Epoch 2293, Loss: 0.01985196901659947, Final Batch Loss: 0.001545415259897709\n",
      "Epoch 2294, Loss: 0.013147571876288566, Final Batch Loss: 8.001656533451751e-05\n",
      "Epoch 2295, Loss: 0.0118453016220883, Final Batch Loss: 5.937113019172102e-05\n",
      "Epoch 2296, Loss: 0.010940920023131184, Final Batch Loss: 0.0001336776331299916\n",
      "Epoch 2297, Loss: 0.007948112339363433, Final Batch Loss: 0.0019740727730095387\n",
      "Epoch 2298, Loss: 0.013002026073081652, Final Batch Loss: 0.0004533140454441309\n",
      "Epoch 2299, Loss: 0.02707191978697665, Final Batch Loss: 0.0007256907992996275\n",
      "Epoch 2300, Loss: 0.027999164993161685, Final Batch Loss: 0.007505031768232584\n",
      "Epoch 2301, Loss: 0.021724549565988127, Final Batch Loss: 0.0031817357521504164\n",
      "Epoch 2302, Loss: 0.011188749564098543, Final Batch Loss: 0.005063313990831375\n",
      "Epoch 2303, Loss: 0.07039231590169948, Final Batch Loss: 0.009469673968851566\n",
      "Epoch 2304, Loss: 0.040407930842775386, Final Batch Loss: 0.018652554601430893\n",
      "Epoch 2305, Loss: 0.037640997808921384, Final Batch Loss: 0.0017897046636790037\n",
      "Epoch 2306, Loss: 0.039047354403010104, Final Batch Loss: 8.590133074903861e-05\n",
      "Epoch 2307, Loss: 0.08290610523545183, Final Batch Loss: 0.0004768984508700669\n",
      "Epoch 2308, Loss: 0.029999208672961686, Final Batch Loss: 0.004189047031104565\n",
      "Epoch 2309, Loss: 0.017385714421834564, Final Batch Loss: 0.00030392667395062745\n",
      "Epoch 2310, Loss: 0.011022566133760847, Final Batch Loss: 6.642434891546145e-05\n",
      "Epoch 2311, Loss: 0.020347048237454146, Final Batch Loss: 0.00010869961988646537\n",
      "Epoch 2312, Loss: 0.06288161018892424, Final Batch Loss: 0.005119860637933016\n",
      "Epoch 2313, Loss: 0.046380747691728175, Final Batch Loss: 0.0035546347498893738\n",
      "Epoch 2314, Loss: 0.07560497357917484, Final Batch Loss: 0.02945040725171566\n",
      "Epoch 2315, Loss: 0.07194648204676923, Final Batch Loss: 0.016936609521508217\n",
      "Epoch 2316, Loss: 0.057468788138066884, Final Batch Loss: 0.00019771901133935899\n",
      "Epoch 2317, Loss: 0.1501422199071385, Final Batch Loss: 0.001157425926066935\n",
      "Epoch 2318, Loss: 0.07973069333820604, Final Batch Loss: 0.005390147212892771\n",
      "Epoch 2319, Loss: 0.10805929973139428, Final Batch Loss: 0.001256256946362555\n",
      "Epoch 2320, Loss: 0.027971625720965676, Final Batch Loss: 0.008344351314008236\n",
      "Epoch 2321, Loss: 0.05128319701907458, Final Batch Loss: 0.0006307177245616913\n",
      "Epoch 2322, Loss: 0.031187141285045072, Final Batch Loss: 0.001359247718937695\n",
      "Epoch 2323, Loss: 0.01905849664035486, Final Batch Loss: 0.0055295187048614025\n",
      "Epoch 2324, Loss: 0.05856089544977294, Final Batch Loss: 0.0005575221730396152\n",
      "Epoch 2325, Loss: 0.041325214253447484, Final Batch Loss: 0.00011744539369828999\n",
      "Epoch 2326, Loss: 0.04260402754880488, Final Batch Loss: 0.002078915713354945\n",
      "Epoch 2327, Loss: 0.02534258260857314, Final Batch Loss: 0.0026237606070935726\n",
      "Epoch 2328, Loss: 0.06437351416388992, Final Batch Loss: 0.0008717936580069363\n",
      "Epoch 2329, Loss: 0.035567487560911104, Final Batch Loss: 0.0006280257366597652\n",
      "Epoch 2330, Loss: 0.04778780914057279, Final Batch Loss: 0.001211897935718298\n",
      "Epoch 2331, Loss: 0.018027157697360963, Final Batch Loss: 0.0008266628137789667\n",
      "Epoch 2332, Loss: 0.018495664015063085, Final Batch Loss: 0.0004375344724394381\n",
      "Epoch 2333, Loss: 0.03746647178923013, Final Batch Loss: 0.0023032205644994974\n",
      "Epoch 2334, Loss: 0.10689988616650226, Final Batch Loss: 0.006385770160704851\n",
      "Epoch 2335, Loss: 0.02913254032318946, Final Batch Loss: 0.011577771976590157\n",
      "Epoch 2336, Loss: 0.029359633757849224, Final Batch Loss: 0.0021490880753844976\n",
      "Epoch 2337, Loss: 0.02734420059277909, Final Batch Loss: 0.0009585947846062481\n",
      "Epoch 2338, Loss: 0.025291761263360968, Final Batch Loss: 0.001255153096280992\n",
      "Epoch 2339, Loss: 0.0281696050951723, Final Batch Loss: 0.00014440725499298424\n",
      "Epoch 2340, Loss: 0.02280327456537634, Final Batch Loss: 0.0011916720541194081\n",
      "Epoch 2341, Loss: 0.0198977438849397, Final Batch Loss: 0.00014408539573196322\n",
      "Epoch 2342, Loss: 0.011483170561405132, Final Batch Loss: 3.549123721313663e-05\n",
      "Epoch 2343, Loss: 0.00783491841139039, Final Batch Loss: 0.0005464437417685986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2344, Loss: 0.005788296009995975, Final Batch Loss: 0.0006099467864260077\n",
      "Epoch 2345, Loss: 0.02237033376513864, Final Batch Loss: 0.00012126233923481777\n",
      "Epoch 2346, Loss: 0.007506101355829742, Final Batch Loss: 4.5114033127902076e-05\n",
      "Epoch 2347, Loss: 0.0331366198406613, Final Batch Loss: 0.00017158400441985577\n",
      "Epoch 2348, Loss: 0.012969134855666198, Final Batch Loss: 0.00038276510895229876\n",
      "Epoch 2349, Loss: 0.014602273982745828, Final Batch Loss: 0.0011656790738925338\n",
      "Epoch 2350, Loss: 0.033792644084314816, Final Batch Loss: 0.002009384334087372\n",
      "Epoch 2351, Loss: 0.10218068957874493, Final Batch Loss: 0.0003398754051886499\n",
      "Epoch 2352, Loss: 0.03493423532199813, Final Batch Loss: 9.99739277176559e-05\n",
      "Epoch 2353, Loss: 0.06824797778972425, Final Batch Loss: 0.0003739513049367815\n",
      "Epoch 2354, Loss: 0.03878460542182438, Final Batch Loss: 0.00015156817971728742\n",
      "Epoch 2355, Loss: 0.02003845046419883, Final Batch Loss: 0.0004930772702209651\n",
      "Epoch 2356, Loss: 0.034423150107613765, Final Batch Loss: 0.0006887776544317603\n",
      "Epoch 2357, Loss: 0.01962130098036141, Final Batch Loss: 1.794937634258531e-05\n",
      "Epoch 2358, Loss: 0.018172325701016234, Final Batch Loss: 0.007049643900245428\n",
      "Epoch 2359, Loss: 0.013656650069606258, Final Batch Loss: 6.334888166747987e-05\n",
      "Epoch 2360, Loss: 0.12910842245037202, Final Batch Loss: 0.00012156854063505307\n",
      "Epoch 2361, Loss: 0.07699196025714627, Final Batch Loss: 0.0031549083068966866\n",
      "Epoch 2362, Loss: 0.05367546930210665, Final Batch Loss: 0.0019377422286197543\n",
      "Epoch 2363, Loss: 0.05960385060461704, Final Batch Loss: 0.020224986597895622\n",
      "Epoch 2364, Loss: 0.03299971669912338, Final Batch Loss: 0.007314953953027725\n",
      "Epoch 2365, Loss: 0.030826861555397045, Final Batch Loss: 0.0047862352803349495\n",
      "Epoch 2366, Loss: 0.03268353663588641, Final Batch Loss: 0.011559821665287018\n",
      "Epoch 2367, Loss: 0.033149018379845074, Final Batch Loss: 0.010925749316811562\n",
      "Epoch 2368, Loss: 0.055420291384507436, Final Batch Loss: 0.0016225000144913793\n",
      "Epoch 2369, Loss: 0.017770379163266625, Final Batch Loss: 0.0001065681062755175\n",
      "Epoch 2370, Loss: 0.10180267739633564, Final Batch Loss: 0.002883968409150839\n",
      "Epoch 2371, Loss: 0.07801362150348723, Final Batch Loss: 0.00029598205583170056\n",
      "Epoch 2372, Loss: 0.04108532787358854, Final Batch Loss: 0.0003212361189071089\n",
      "Epoch 2373, Loss: 0.019157514441758394, Final Batch Loss: 0.0011584324529394507\n",
      "Epoch 2374, Loss: 0.014299963244411629, Final Batch Loss: 0.001301668817177415\n",
      "Epoch 2375, Loss: 0.052710990807099734, Final Batch Loss: 0.002961576683446765\n",
      "Epoch 2376, Loss: 0.05310143485257868, Final Batch Loss: 0.003832161659374833\n",
      "Epoch 2377, Loss: 0.018971062163473107, Final Batch Loss: 0.0002508795296307653\n",
      "Epoch 2378, Loss: 0.018966972766065737, Final Batch Loss: 0.0005083742435090244\n",
      "Epoch 2379, Loss: 0.04225536724698031, Final Batch Loss: 0.0035549078602343798\n",
      "Epoch 2380, Loss: 0.018165175184549298, Final Batch Loss: 0.0018919279100373387\n",
      "Epoch 2381, Loss: 0.05230003661199589, Final Batch Loss: 0.0191601924598217\n",
      "Epoch 2382, Loss: 0.01522371561077307, Final Batch Loss: 0.00022360186267178506\n",
      "Epoch 2383, Loss: 0.03992228877541493, Final Batch Loss: 0.00019618342048488557\n",
      "Epoch 2384, Loss: 0.01352875063821557, Final Batch Loss: 0.0020446698181331158\n",
      "Epoch 2385, Loss: 0.03011507420706039, Final Batch Loss: 0.009183837100863457\n",
      "Epoch 2386, Loss: 0.018210846174042672, Final Batch Loss: 0.0008180904551409185\n",
      "Epoch 2387, Loss: 0.04700105862139026, Final Batch Loss: 0.00035646691685542464\n",
      "Epoch 2388, Loss: 0.031165388340014033, Final Batch Loss: 0.00017953096539713442\n",
      "Epoch 2389, Loss: 0.021475533474585973, Final Batch Loss: 0.0005337783368304372\n",
      "Epoch 2390, Loss: 0.02239422437196481, Final Batch Loss: 0.0046780710108578205\n",
      "Epoch 2391, Loss: 0.040864286209398415, Final Batch Loss: 0.0002584643370937556\n",
      "Epoch 2392, Loss: 0.012532838218248799, Final Batch Loss: 0.000220566987991333\n",
      "Epoch 2393, Loss: 0.00915984870516695, Final Batch Loss: 4.3475567508721724e-05\n",
      "Epoch 2394, Loss: 0.007528621197707253, Final Batch Loss: 0.0006348627503030002\n",
      "Epoch 2395, Loss: 0.01101403683423996, Final Batch Loss: 0.00011459581583039835\n",
      "Epoch 2396, Loss: 0.037454934157722164, Final Batch Loss: 0.0018010608619078994\n",
      "Epoch 2397, Loss: 0.022545231582626002, Final Batch Loss: 0.0001896275789476931\n",
      "Epoch 2398, Loss: 0.035412082808761625, Final Batch Loss: 8.885704301064834e-05\n",
      "Epoch 2399, Loss: 0.023943337619130034, Final Batch Loss: 0.0006032463861629367\n",
      "Epoch 2400, Loss: 0.04266074186671176, Final Batch Loss: 0.00043602759251371026\n",
      "Epoch 2401, Loss: 0.028093546475247422, Final Batch Loss: 0.0005473437486216426\n",
      "Epoch 2402, Loss: 0.02704404207179323, Final Batch Loss: 0.0001765239576343447\n",
      "Epoch 2403, Loss: 0.019249034328822745, Final Batch Loss: 0.00015809785691089928\n",
      "Epoch 2404, Loss: 0.031149928150625783, Final Batch Loss: 0.020637594163417816\n",
      "Epoch 2405, Loss: 0.03915985957428347, Final Batch Loss: 0.00036965528852306306\n",
      "Epoch 2406, Loss: 0.04020398319335072, Final Batch Loss: 0.00020245567429810762\n",
      "Epoch 2407, Loss: 0.060981749469647184, Final Batch Loss: 0.000513387902174145\n",
      "Epoch 2408, Loss: 0.0093728173578711, Final Batch Loss: 0.0008266590884886682\n",
      "Epoch 2409, Loss: 0.056624923070558, Final Batch Loss: 2.5585863113519736e-05\n",
      "Epoch 2410, Loss: 0.04826905354275368, Final Batch Loss: 0.013315894640982151\n",
      "Epoch 2411, Loss: 0.11487552910693921, Final Batch Loss: 0.004169984254986048\n",
      "Epoch 2412, Loss: 0.04339294720921316, Final Batch Loss: 0.00778972776606679\n",
      "Epoch 2413, Loss: 0.019763306321692653, Final Batch Loss: 0.00038905697874724865\n",
      "Epoch 2414, Loss: 0.025340004736790434, Final Batch Loss: 0.0008193526300601661\n",
      "Epoch 2415, Loss: 0.03734409693606722, Final Batch Loss: 0.003937908448278904\n",
      "Epoch 2416, Loss: 0.030054629198275506, Final Batch Loss: 9.152512211585417e-05\n",
      "Epoch 2417, Loss: 0.04998569345480064, Final Batch Loss: 0.0025782224256545305\n",
      "Epoch 2418, Loss: 0.016849446255946532, Final Batch Loss: 0.00014059903332963586\n",
      "Epoch 2419, Loss: 0.059256830310914665, Final Batch Loss: 0.00017984247824642807\n",
      "Epoch 2420, Loss: 0.022692605722113512, Final Batch Loss: 0.001402157242409885\n",
      "Epoch 2421, Loss: 0.031491034120335826, Final Batch Loss: 0.0005379388458095491\n",
      "Epoch 2422, Loss: 0.011539611521584447, Final Batch Loss: 0.00015260539657901973\n",
      "Epoch 2423, Loss: 0.02035186397824873, Final Batch Loss: 0.004444912541657686\n",
      "Epoch 2424, Loss: 0.026301300844352227, Final Batch Loss: 0.000608097470831126\n",
      "Epoch 2425, Loss: 0.21451659058220685, Final Batch Loss: 0.0026309972163289785\n",
      "Epoch 2426, Loss: 0.0971651405270677, Final Batch Loss: 0.0024325742851942778\n",
      "Epoch 2427, Loss: 0.027580185866099782, Final Batch Loss: 0.0011389394057914615\n",
      "Epoch 2428, Loss: 0.07055949448840693, Final Batch Loss: 0.00014901801478117704\n",
      "Epoch 2429, Loss: 0.04588287365913857, Final Batch Loss: 0.005546078085899353\n",
      "Epoch 2430, Loss: 0.032703794557164656, Final Batch Loss: 0.0013887016102671623\n",
      "Epoch 2431, Loss: 0.01880753859586548, Final Batch Loss: 0.0012482667807489634\n",
      "Epoch 2432, Loss: 0.011873732088133693, Final Batch Loss: 9.076111746253446e-05\n",
      "Epoch 2433, Loss: 0.015447365312866168, Final Batch Loss: 0.0012003198498860002\n",
      "Epoch 2434, Loss: 0.007282670907443389, Final Batch Loss: 0.0003671955200843513\n",
      "Epoch 2435, Loss: 0.006925566765858093, Final Batch Loss: 0.0001337240100838244\n",
      "Epoch 2436, Loss: 0.003729031906914315, Final Batch Loss: 3.5236833355156705e-05\n",
      "Epoch 2437, Loss: 0.004970406596839894, Final Batch Loss: 8.584528404753655e-05\n",
      "Epoch 2438, Loss: 0.013417428500360984, Final Batch Loss: 0.002166305435821414\n",
      "Epoch 2439, Loss: 0.006361808016663417, Final Batch Loss: 6.961759208934382e-05\n",
      "Epoch 2440, Loss: 0.009724454224851797, Final Batch Loss: 0.007315648254007101\n",
      "Epoch 2441, Loss: 0.03157776977241156, Final Batch Loss: 0.0001186841691378504\n",
      "Epoch 2442, Loss: 0.016265182370261755, Final Batch Loss: 0.0036364183761179447\n",
      "Epoch 2443, Loss: 0.018033446174740675, Final Batch Loss: 0.0003774623037315905\n",
      "Epoch 2444, Loss: 0.006458000545535469, Final Batch Loss: 0.00088659388711676\n",
      "Epoch 2445, Loss: 0.015895350989012513, Final Batch Loss: 0.010568920522928238\n",
      "Epoch 2446, Loss: 0.07683501486826572, Final Batch Loss: 0.00013310583017300814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2447, Loss: 0.06743715974880615, Final Batch Loss: 0.0015606002416461706\n",
      "Epoch 2448, Loss: 0.015011013354524039, Final Batch Loss: 0.0002386838459642604\n",
      "Epoch 2449, Loss: 0.009963175403754576, Final Batch Loss: 1.8513677787268534e-05\n",
      "Epoch 2450, Loss: 0.019089064246145426, Final Batch Loss: 6.580207264050841e-05\n",
      "Epoch 2451, Loss: 0.03850103711374686, Final Batch Loss: 0.00017256938735954463\n",
      "Epoch 2452, Loss: 0.016170052670076984, Final Batch Loss: 6.733292684657499e-05\n",
      "Epoch 2453, Loss: 0.01040055413977825, Final Batch Loss: 0.004014084581285715\n",
      "Epoch 2454, Loss: 0.0288144519290654, Final Batch Loss: 3.131612538709305e-05\n",
      "Epoch 2455, Loss: 0.024714657667573192, Final Batch Loss: 3.351896521053277e-05\n",
      "Epoch 2456, Loss: 0.008409945598032209, Final Batch Loss: 1.6894087821128778e-05\n",
      "Epoch 2457, Loss: 0.014574733554582053, Final Batch Loss: 0.0006201608339324594\n",
      "Epoch 2458, Loss: 0.10574701073892356, Final Batch Loss: 0.0024045032914727926\n",
      "Epoch 2459, Loss: 0.07160390675198869, Final Batch Loss: 0.0004306374175939709\n",
      "Epoch 2460, Loss: 0.02016366389034374, Final Batch Loss: 5.472969860420562e-05\n",
      "Epoch 2461, Loss: 0.07716610551506164, Final Batch Loss: 0.012188977561891079\n",
      "Epoch 2462, Loss: 0.06504582514753565, Final Batch Loss: 0.013127408921718597\n",
      "Epoch 2463, Loss: 0.012329944176599383, Final Batch Loss: 0.002830156125128269\n",
      "Epoch 2464, Loss: 0.01301555420650402, Final Batch Loss: 0.0028840135782957077\n",
      "Epoch 2465, Loss: 0.011379637726349756, Final Batch Loss: 0.00032371922861784697\n",
      "Epoch 2466, Loss: 0.016888187728909543, Final Batch Loss: 0.001173867261968553\n",
      "Epoch 2467, Loss: 0.008932124012062559, Final Batch Loss: 0.0006454290123656392\n",
      "Epoch 2468, Loss: 0.022858684380480554, Final Batch Loss: 0.010775716975331306\n",
      "Epoch 2469, Loss: 0.031038192599226022, Final Batch Loss: 0.0010905542876571417\n",
      "Epoch 2470, Loss: 0.02523280213426915, Final Batch Loss: 6.960684549994767e-05\n",
      "Epoch 2471, Loss: 0.0193540350683179, Final Batch Loss: 0.00015776722284499556\n",
      "Epoch 2472, Loss: 0.07330427380475157, Final Batch Loss: 0.0020601802971214056\n",
      "Epoch 2473, Loss: 0.0714908678055508, Final Batch Loss: 0.00021576711151283234\n",
      "Epoch 2474, Loss: 0.10949780402734177, Final Batch Loss: 0.0018760573584586382\n",
      "Epoch 2475, Loss: 0.12064672864653403, Final Batch Loss: 0.02329663559794426\n",
      "Epoch 2476, Loss: 0.038427730702096596, Final Batch Loss: 0.021028779447078705\n",
      "Epoch 2477, Loss: 0.11869644887337927, Final Batch Loss: 0.00011827929120045155\n",
      "Epoch 2478, Loss: 0.06680657286779024, Final Batch Loss: 0.0005870740860700607\n",
      "Epoch 2479, Loss: 0.0391147145855939, Final Batch Loss: 0.0012536486610770226\n",
      "Epoch 2480, Loss: 0.02226030123711098, Final Batch Loss: 0.003986227326095104\n",
      "Epoch 2481, Loss: 0.02382533747004345, Final Batch Loss: 0.0001282871380681172\n",
      "Epoch 2482, Loss: 0.01643854360736441, Final Batch Loss: 0.00010712252696976066\n",
      "Epoch 2483, Loss: 0.03684592823992716, Final Batch Loss: 0.00019980594515800476\n",
      "Epoch 2484, Loss: 0.013413787244644482, Final Batch Loss: 0.001516751479357481\n",
      "Epoch 2485, Loss: 0.015020244773040758, Final Batch Loss: 0.000528869975823909\n",
      "Epoch 2486, Loss: 0.03954892899491824, Final Batch Loss: 0.01002955250442028\n",
      "Epoch 2487, Loss: 0.016436153775430284, Final Batch Loss: 0.0004659827973227948\n",
      "Epoch 2488, Loss: 0.022989542936556973, Final Batch Loss: 0.006315440405160189\n",
      "Epoch 2489, Loss: 0.028417010198609205, Final Batch Loss: 0.00016833421250339597\n",
      "Epoch 2490, Loss: 0.01887031258229399, Final Batch Loss: 0.00018743032705970109\n",
      "Epoch 2491, Loss: 0.022227654691960197, Final Batch Loss: 0.00021663287770934403\n",
      "Epoch 2492, Loss: 0.029915773920947686, Final Batch Loss: 9.815792145673186e-05\n",
      "Epoch 2493, Loss: 0.023600485295901308, Final Batch Loss: 0.00022347804042510688\n",
      "Epoch 2494, Loss: 0.08903579570323927, Final Batch Loss: 0.0024845015723258257\n",
      "Epoch 2495, Loss: 0.06984329049737426, Final Batch Loss: 0.0005654694396071136\n",
      "Epoch 2496, Loss: 0.013084927610179875, Final Batch Loss: 0.00018379789253231138\n",
      "Epoch 2497, Loss: 0.021982413392834133, Final Batch Loss: 0.0004915432655252516\n",
      "Epoch 2498, Loss: 0.02618501532560913, Final Batch Loss: 0.0003310257161501795\n",
      "Epoch 2499, Loss: 0.013171027221687837, Final Batch Loss: 0.0003652634914033115\n",
      "Epoch 2500, Loss: 0.007476930280972738, Final Batch Loss: 0.00024061498697847128\n",
      "Epoch 2501, Loss: 0.026915533395367675, Final Batch Loss: 9.597329335520044e-05\n",
      "Epoch 2502, Loss: 0.03648737665571389, Final Batch Loss: 0.0002547355543356389\n",
      "Epoch 2503, Loss: 0.05289011914283037, Final Batch Loss: 0.0004031601711176336\n",
      "Epoch 2504, Loss: 0.013377600986132165, Final Batch Loss: 0.002997876377776265\n",
      "Epoch 2505, Loss: 0.026465248316526413, Final Batch Loss: 0.0018727137940004468\n",
      "Epoch 2506, Loss: 0.032301363586157095, Final Batch Loss: 0.0006075148703530431\n",
      "Epoch 2507, Loss: 0.02806270683868206, Final Batch Loss: 0.0004524076066445559\n",
      "Epoch 2508, Loss: 0.021848963489901507, Final Batch Loss: 0.0003617893089540303\n",
      "Epoch 2509, Loss: 0.01068211009260267, Final Batch Loss: 0.00012872206571046263\n",
      "Epoch 2510, Loss: 0.03545040719473036, Final Batch Loss: 0.011013399809598923\n",
      "Epoch 2511, Loss: 0.020573493482515914, Final Batch Loss: 8.31968936836347e-05\n",
      "Epoch 2512, Loss: 0.019046218378207413, Final Batch Loss: 0.0006583361537195742\n",
      "Epoch 2513, Loss: 0.03229947175714187, Final Batch Loss: 9.552303527016193e-05\n",
      "Epoch 2514, Loss: 0.035614731881651096, Final Batch Loss: 0.000825808965601027\n",
      "Epoch 2515, Loss: 0.04585295078140916, Final Batch Loss: 0.0003016687114723027\n",
      "Epoch 2516, Loss: 0.01186896024591988, Final Batch Loss: 0.00013248666073195636\n",
      "Epoch 2517, Loss: 0.011155630403663963, Final Batch Loss: 0.0013548682909458876\n",
      "Epoch 2518, Loss: 0.009600599434634205, Final Batch Loss: 0.00046656609629280865\n",
      "Epoch 2519, Loss: 0.06481418762996327, Final Batch Loss: 9.841851715464145e-05\n",
      "Epoch 2520, Loss: 0.03931496857694583, Final Batch Loss: 0.0036161900497972965\n",
      "Epoch 2521, Loss: 0.061060760272084735, Final Batch Loss: 0.0019890612456947565\n",
      "Epoch 2522, Loss: 0.025078651706280652, Final Batch Loss: 0.0007445499650202692\n",
      "Epoch 2523, Loss: 0.02143146440903365, Final Batch Loss: 0.0018012052169069648\n",
      "Epoch 2524, Loss: 0.04030379396863282, Final Batch Loss: 0.004922877065837383\n",
      "Epoch 2525, Loss: 0.018520467765483772, Final Batch Loss: 0.0006914186524227262\n",
      "Epoch 2526, Loss: 0.020711979574116413, Final Batch Loss: 0.0006257961504161358\n",
      "Epoch 2527, Loss: 0.024640463903779164, Final Batch Loss: 0.0013897443423047662\n",
      "Epoch 2528, Loss: 0.03783691606440698, Final Batch Loss: 0.0001308922510361299\n",
      "Epoch 2529, Loss: 0.028524038369141635, Final Batch Loss: 0.00010780597222037613\n",
      "Epoch 2530, Loss: 0.04926568132941611, Final Batch Loss: 0.0009295060881413519\n",
      "Epoch 2531, Loss: 0.027669594419421628, Final Batch Loss: 0.0007892547291703522\n",
      "Epoch 2532, Loss: 0.1341816526837647, Final Batch Loss: 0.0014356049941852689\n",
      "Epoch 2533, Loss: 0.05340354981308337, Final Batch Loss: 0.00917598232626915\n",
      "Epoch 2534, Loss: 0.05832928321615327, Final Batch Loss: 0.0008565027383156121\n",
      "Epoch 2535, Loss: 0.04422600804537069, Final Batch Loss: 0.00773607986047864\n",
      "Epoch 2536, Loss: 0.03660505741572706, Final Batch Loss: 0.01198441069573164\n",
      "Epoch 2537, Loss: 0.02417400102422107, Final Batch Loss: 0.0009749115561135113\n",
      "Epoch 2538, Loss: 0.03775987238623202, Final Batch Loss: 0.019070938229560852\n",
      "Epoch 2539, Loss: 0.03778116351895733, Final Batch Loss: 0.0015209319535642862\n",
      "Epoch 2540, Loss: 0.10954943222168367, Final Batch Loss: 0.016081618145108223\n",
      "Epoch 2541, Loss: 0.09711937988322461, Final Batch Loss: 9.80720724328421e-05\n",
      "Epoch 2542, Loss: 0.09365271966635191, Final Batch Loss: 0.015440008603036404\n",
      "Epoch 2543, Loss: 0.12868560604692902, Final Batch Loss: 0.009763127192854881\n",
      "Epoch 2544, Loss: 0.09617094030545559, Final Batch Loss: 0.01252707652747631\n",
      "Epoch 2545, Loss: 0.03569874675304163, Final Batch Loss: 0.00014442410611081868\n",
      "Epoch 2546, Loss: 0.045046814339002594, Final Batch Loss: 0.006098731886595488\n",
      "Epoch 2547, Loss: 0.028738246073771734, Final Batch Loss: 0.0006143952487036586\n",
      "Epoch 2548, Loss: 0.028140756716311444, Final Batch Loss: 0.00345738441683352\n",
      "Epoch 2549, Loss: 0.039250830246601254, Final Batch Loss: 0.0015506603522226214\n",
      "Epoch 2550, Loss: 0.00849455605930416, Final Batch Loss: 0.0013797886203974485\n",
      "Epoch 2551, Loss: 0.020876684506220045, Final Batch Loss: 9.965193021344021e-05\n",
      "Epoch 2552, Loss: 0.021444073718157597, Final Batch Loss: 0.0005487207672558725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2553, Loss: 0.008619066455139546, Final Batch Loss: 0.0004878882609773427\n",
      "Epoch 2554, Loss: 0.011062808967835736, Final Batch Loss: 0.0011384108802303672\n",
      "Epoch 2555, Loss: 0.03469398610468488, Final Batch Loss: 8.590649667894468e-05\n",
      "Epoch 2556, Loss: 0.017539708533149678, Final Batch Loss: 0.004897539969533682\n",
      "Epoch 2557, Loss: 0.06396444574420457, Final Batch Loss: 0.0003062458708882332\n",
      "Epoch 2558, Loss: 0.011426754339481704, Final Batch Loss: 0.0021319733932614326\n",
      "Epoch 2559, Loss: 0.01232288483879529, Final Batch Loss: 0.0007624946301802993\n",
      "Epoch 2560, Loss: 0.07105419315485051, Final Batch Loss: 0.0004035117744933814\n",
      "Epoch 2561, Loss: 0.0379829483517824, Final Batch Loss: 0.0002510217309463769\n",
      "Epoch 2562, Loss: 0.054809470057080034, Final Batch Loss: 0.010565574280917645\n",
      "Epoch 2563, Loss: 0.04850509613606846, Final Batch Loss: 0.0023852032609283924\n",
      "Epoch 2564, Loss: 0.04352211334116873, Final Batch Loss: 0.0013304308522492647\n",
      "Epoch 2565, Loss: 0.05158895626664162, Final Batch Loss: 0.00409625843167305\n",
      "Epoch 2566, Loss: 0.039457183818740305, Final Batch Loss: 0.00016518187476322055\n",
      "Epoch 2567, Loss: 0.058855219453107566, Final Batch Loss: 0.009573310613632202\n",
      "Epoch 2568, Loss: 0.03909772714541759, Final Batch Loss: 0.00030248280381783843\n",
      "Epoch 2569, Loss: 0.05799772717364249, Final Batch Loss: 0.0002643170300871134\n",
      "Epoch 2570, Loss: 0.06684391063390649, Final Batch Loss: 0.00887969508767128\n",
      "Epoch 2571, Loss: 0.03589217268017819, Final Batch Loss: 0.00022461502521764487\n",
      "Epoch 2572, Loss: 0.03405933150497731, Final Batch Loss: 0.00012894682004116476\n",
      "Epoch 2573, Loss: 0.10847708662186051, Final Batch Loss: 0.005334389861673117\n",
      "Epoch 2574, Loss: 0.02625214153158595, Final Batch Loss: 0.0013604392297565937\n",
      "Epoch 2575, Loss: 0.06274787100846879, Final Batch Loss: 0.004390996880829334\n",
      "Epoch 2576, Loss: 0.016807796881039394, Final Batch Loss: 0.00029041836387477815\n",
      "Epoch 2577, Loss: 0.00641964848182397, Final Batch Loss: 0.00016770751972217113\n",
      "Epoch 2578, Loss: 0.04713153818738647, Final Batch Loss: 0.000631069706287235\n",
      "Epoch 2579, Loss: 0.01801506026822608, Final Batch Loss: 0.00021316777565516531\n",
      "Epoch 2580, Loss: 0.01718404521670891, Final Batch Loss: 0.0008648228831589222\n",
      "Epoch 2581, Loss: 0.02736176578400773, Final Batch Loss: 0.0001357112778350711\n",
      "Epoch 2582, Loss: 0.014121924610662973, Final Batch Loss: 0.0016816994175314903\n",
      "Epoch 2583, Loss: 0.012094204566892586, Final Batch Loss: 5.880304524907842e-05\n",
      "Epoch 2584, Loss: 0.044132010956673184, Final Batch Loss: 3.574609581846744e-05\n",
      "Epoch 2585, Loss: 0.04953029508033069, Final Batch Loss: 0.0036596208810806274\n",
      "Epoch 2586, Loss: 0.1345225223849411, Final Batch Loss: 0.00012358138337731361\n",
      "Epoch 2587, Loss: 0.02408343083152431, Final Batch Loss: 0.006289239972829819\n",
      "Epoch 2588, Loss: 0.021775417131721042, Final Batch Loss: 0.00012292245810385793\n",
      "Epoch 2589, Loss: 0.06420853028976126, Final Batch Loss: 0.00045537890400737524\n",
      "Epoch 2590, Loss: 0.015323877283663023, Final Batch Loss: 0.00033003161661326885\n",
      "Epoch 2591, Loss: 0.02030906074651284, Final Batch Loss: 0.0004770000814460218\n",
      "Epoch 2592, Loss: 0.009394788175995927, Final Batch Loss: 3.3016585803125054e-05\n",
      "Epoch 2593, Loss: 0.004613723864167696, Final Batch Loss: 0.00023043176042847335\n",
      "Epoch 2594, Loss: 0.027668257993354928, Final Batch Loss: 0.0006668296409770846\n",
      "Epoch 2595, Loss: 0.04443114815512672, Final Batch Loss: 0.0028503078501671553\n",
      "Epoch 2596, Loss: 0.011482143389002886, Final Batch Loss: 0.00012849342601839453\n",
      "Epoch 2597, Loss: 0.008226504978665616, Final Batch Loss: 0.0006197136244736612\n",
      "Epoch 2598, Loss: 0.017000023175569368, Final Batch Loss: 0.0006039083236828446\n",
      "Epoch 2599, Loss: 0.030006878760104883, Final Batch Loss: 5.096091263112612e-05\n",
      "Epoch 2600, Loss: 0.02076568338816287, Final Batch Loss: 0.00014181241567712277\n",
      "Epoch 2601, Loss: 0.01754527935918304, Final Batch Loss: 0.00018027190526481718\n",
      "Epoch 2602, Loss: 0.01336133146105567, Final Batch Loss: 0.0014172878582030535\n",
      "Epoch 2603, Loss: 0.004542669674265198, Final Batch Loss: 0.0006377222598530352\n",
      "Epoch 2604, Loss: 0.006727747077093227, Final Batch Loss: 8.835278276819736e-05\n",
      "Epoch 2605, Loss: 0.0037188903406786267, Final Batch Loss: 0.00012397007958497852\n",
      "Epoch 2606, Loss: 0.00541736727427633, Final Batch Loss: 7.07136932760477e-05\n",
      "Epoch 2607, Loss: 0.013701459642106784, Final Batch Loss: 1.2430051356204785e-05\n",
      "Epoch 2608, Loss: 0.01784137687900511, Final Batch Loss: 0.0006191061693243682\n",
      "Epoch 2609, Loss: 0.05357185224238492, Final Batch Loss: 0.040667373687028885\n",
      "Epoch 2610, Loss: 0.006026466158800758, Final Batch Loss: 0.00016750510258134454\n",
      "Epoch 2611, Loss: 0.01149883483776648, Final Batch Loss: 0.0002627996145747602\n",
      "Epoch 2612, Loss: 0.0217004201549571, Final Batch Loss: 0.0012195592280477285\n",
      "Epoch 2613, Loss: 0.01105908887257101, Final Batch Loss: 0.00019458486349321902\n",
      "Epoch 2614, Loss: 0.01890018436824903, Final Batch Loss: 7.276383257703856e-05\n",
      "Epoch 2615, Loss: 0.03591311966010835, Final Batch Loss: 0.000416335416957736\n",
      "Epoch 2616, Loss: 0.025112785977398744, Final Batch Loss: 0.0008340691565535963\n",
      "Epoch 2617, Loss: 0.017320551263765083, Final Batch Loss: 0.00016163768304977566\n",
      "Epoch 2618, Loss: 0.04109301014977973, Final Batch Loss: 0.00043016759445890784\n",
      "Epoch 2619, Loss: 0.08332727482411428, Final Batch Loss: 0.0004186149744782597\n",
      "Epoch 2620, Loss: 0.05351154562231386, Final Batch Loss: 0.005747002083808184\n",
      "Epoch 2621, Loss: 0.033514698341605254, Final Batch Loss: 0.0012450816575437784\n",
      "Epoch 2622, Loss: 0.019317337650136324, Final Batch Loss: 0.0004160311946179718\n",
      "Epoch 2623, Loss: 0.013198149128584191, Final Batch Loss: 0.0030798674561083317\n",
      "Epoch 2624, Loss: 0.011285901532573916, Final Batch Loss: 5.8449539210414514e-05\n",
      "Epoch 2625, Loss: 0.06539985055860598, Final Batch Loss: 0.05094848573207855\n",
      "Epoch 2626, Loss: 0.03143457879559719, Final Batch Loss: 0.0008938151877373457\n",
      "Epoch 2627, Loss: 0.05077164078102214, Final Batch Loss: 0.015841351822018623\n",
      "Epoch 2628, Loss: 0.01652912419012864, Final Batch Loss: 0.002302431734278798\n",
      "Epoch 2629, Loss: 0.016929090230405563, Final Batch Loss: 0.0029435812029987574\n",
      "Epoch 2630, Loss: 0.012975766063391347, Final Batch Loss: 8.832124876789749e-05\n",
      "Epoch 2631, Loss: 0.00689963917420755, Final Batch Loss: 2.901414336520247e-05\n",
      "Epoch 2632, Loss: 0.011100295277628902, Final Batch Loss: 0.0008105505839921534\n",
      "Epoch 2633, Loss: 0.010831731557118474, Final Batch Loss: 7.190970063675195e-05\n",
      "Epoch 2634, Loss: 0.014978271239669994, Final Batch Loss: 5.7919743994716555e-05\n",
      "Epoch 2635, Loss: 0.01298103472981893, Final Batch Loss: 2.9353994250413962e-05\n",
      "Epoch 2636, Loss: 0.03255823404106195, Final Batch Loss: 0.0001928084238898009\n",
      "Epoch 2637, Loss: 0.03527276189561235, Final Batch Loss: 0.00029297807486727834\n",
      "Epoch 2638, Loss: 0.06147443691952503, Final Batch Loss: 0.00017722000484354794\n",
      "Epoch 2639, Loss: 0.0074180282827001065, Final Batch Loss: 0.00010450955596752465\n",
      "Epoch 2640, Loss: 0.021021429423853988, Final Batch Loss: 0.0003075201530009508\n",
      "Epoch 2641, Loss: 0.07473164158363943, Final Batch Loss: 0.00021742227545473725\n",
      "Epoch 2642, Loss: 0.21024281460267957, Final Batch Loss: 0.06408343464136124\n",
      "Epoch 2643, Loss: 0.16821981890825555, Final Batch Loss: 0.0005347708938643336\n",
      "Epoch 2644, Loss: 0.12321263912599534, Final Batch Loss: 0.012704846449196339\n",
      "Epoch 2645, Loss: 0.08243037543434184, Final Batch Loss: 0.023418201133608818\n",
      "Epoch 2646, Loss: 0.060890345310326666, Final Batch Loss: 0.015561452135443687\n",
      "Epoch 2647, Loss: 0.032337851866032, Final Batch Loss: 0.0007688040495850146\n",
      "Epoch 2648, Loss: 0.022171877702930942, Final Batch Loss: 0.00020715888240374625\n",
      "Epoch 2649, Loss: 0.03600369695050176, Final Batch Loss: 0.004039608873426914\n",
      "Epoch 2650, Loss: 0.04415687749860808, Final Batch Loss: 0.00258434540592134\n",
      "Epoch 2651, Loss: 0.022952533938223496, Final Batch Loss: 0.0005354687455110252\n",
      "Epoch 2652, Loss: 0.02444986588670872, Final Batch Loss: 0.0003804219886660576\n",
      "Epoch 2653, Loss: 0.03805875914986245, Final Batch Loss: 0.025782136246562004\n",
      "Epoch 2654, Loss: 0.02883650593139464, Final Batch Loss: 0.0004020802734885365\n",
      "Epoch 2655, Loss: 0.021355350654630456, Final Batch Loss: 0.0002705624501686543\n",
      "Epoch 2656, Loss: 0.03927232010573789, Final Batch Loss: 0.0005022208206355572\n",
      "Epoch 2657, Loss: 0.016919465218961705, Final Batch Loss: 0.0004059856291860342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2658, Loss: 0.04506825302087236, Final Batch Loss: 0.0016607760917395353\n",
      "Epoch 2659, Loss: 0.020976452171453275, Final Batch Loss: 0.008657610975205898\n",
      "Epoch 2660, Loss: 0.028498233536083717, Final Batch Loss: 0.00043219889630563557\n",
      "Epoch 2661, Loss: 0.09873908589543134, Final Batch Loss: 2.7860312911798246e-05\n",
      "Epoch 2662, Loss: 0.07032596349017695, Final Batch Loss: 0.00023212477390188724\n",
      "Epoch 2663, Loss: 0.10020810405330849, Final Batch Loss: 0.005837710108608007\n",
      "Epoch 2664, Loss: 0.09069827923667617, Final Batch Loss: 0.0003490158705972135\n",
      "Epoch 2665, Loss: 0.02891731876297854, Final Batch Loss: 0.00022231203911360353\n",
      "Epoch 2666, Loss: 0.03995776278316043, Final Batch Loss: 0.00044536477071233094\n",
      "Epoch 2667, Loss: 0.020660743655753322, Final Batch Loss: 0.0035684602335095406\n",
      "Epoch 2668, Loss: 0.0440177143245819, Final Batch Loss: 0.010878494009375572\n",
      "Epoch 2669, Loss: 0.007271927763213171, Final Batch Loss: 0.0002493474166840315\n",
      "Epoch 2670, Loss: 0.02675618912326172, Final Batch Loss: 0.006716819480061531\n",
      "Epoch 2671, Loss: 0.01518352978018811, Final Batch Loss: 0.005691487342119217\n",
      "Epoch 2672, Loss: 0.012086229013220873, Final Batch Loss: 0.0004301964654587209\n",
      "Epoch 2673, Loss: 0.0559960814389342, Final Batch Loss: 9.813689393922687e-05\n",
      "Epoch 2674, Loss: 0.02776281653495971, Final Batch Loss: 0.00012848600454162806\n",
      "Epoch 2675, Loss: 0.03411822275666054, Final Batch Loss: 0.00025004567578434944\n",
      "Epoch 2676, Loss: 0.01785670597018907, Final Batch Loss: 0.0010404142085462809\n",
      "Epoch 2677, Loss: 0.031178570046904497, Final Batch Loss: 0.008635787293314934\n",
      "Epoch 2678, Loss: 0.02441984784672968, Final Batch Loss: 0.012553221546113491\n",
      "Epoch 2679, Loss: 0.02210814591671806, Final Batch Loss: 0.0040352861396968365\n",
      "Epoch 2680, Loss: 0.10520445992005989, Final Batch Loss: 6.268312426982448e-05\n",
      "Epoch 2681, Loss: 0.12645710702054203, Final Batch Loss: 0.00041952921310439706\n",
      "Epoch 2682, Loss: 0.10114722599973902, Final Batch Loss: 0.0012362069683149457\n",
      "Epoch 2683, Loss: 0.038866652495926246, Final Batch Loss: 0.0003433460369706154\n",
      "Epoch 2684, Loss: 0.028945861326064914, Final Batch Loss: 0.0025871030520647764\n",
      "Epoch 2685, Loss: 0.019157755639753304, Final Batch Loss: 0.0003243484243284911\n",
      "Epoch 2686, Loss: 0.0607237889598764, Final Batch Loss: 0.014769076369702816\n",
      "Epoch 2687, Loss: 0.07996089896187186, Final Batch Loss: 0.006050249096006155\n",
      "Epoch 2688, Loss: 0.05165282136294991, Final Batch Loss: 0.00012925703777000308\n",
      "Epoch 2689, Loss: 0.07141079044959042, Final Batch Loss: 0.0007033247966319323\n",
      "Epoch 2690, Loss: 0.019496432854793966, Final Batch Loss: 0.0025793865788728\n",
      "Epoch 2691, Loss: 0.03523579418833833, Final Batch Loss: 0.009208381175994873\n",
      "Epoch 2692, Loss: 0.01901214363897452, Final Batch Loss: 0.0023728644009679556\n",
      "Epoch 2693, Loss: 0.03613335589761846, Final Batch Loss: 0.00029192661168053746\n",
      "Epoch 2694, Loss: 0.017676143819699064, Final Batch Loss: 0.0009131304686889052\n",
      "Epoch 2695, Loss: 0.044097267389588524, Final Batch Loss: 0.00043332946370355785\n",
      "Epoch 2696, Loss: 0.021311553311534226, Final Batch Loss: 0.0008645695052109659\n",
      "Epoch 2697, Loss: 0.04605756318414933, Final Batch Loss: 0.0014572642976418138\n",
      "Epoch 2698, Loss: 0.02518020737079496, Final Batch Loss: 8.814044849714264e-05\n",
      "Epoch 2699, Loss: 0.027105265467980644, Final Batch Loss: 1.6764952306402847e-05\n",
      "Epoch 2700, Loss: 0.022644644050160423, Final Batch Loss: 0.003529100213199854\n",
      "Epoch 2701, Loss: 0.0271818698856805, Final Batch Loss: 0.0015802255365997553\n",
      "Epoch 2702, Loss: 0.015847186048631556, Final Batch Loss: 7.311173976631835e-05\n",
      "Epoch 2703, Loss: 0.019708975243702298, Final Batch Loss: 0.00033657305175438523\n",
      "Epoch 2704, Loss: 0.03154869207901356, Final Batch Loss: 0.001662084017880261\n",
      "Epoch 2705, Loss: 0.027782133876826265, Final Batch Loss: 0.00021667506371159106\n",
      "Epoch 2706, Loss: 0.012401743977534352, Final Batch Loss: 0.0017123157158493996\n",
      "Epoch 2707, Loss: 0.00519474595785141, Final Batch Loss: 9.842129657045007e-05\n",
      "Epoch 2708, Loss: 0.018554259851043753, Final Batch Loss: 0.0014740383485332131\n",
      "Epoch 2709, Loss: 0.0704906003375072, Final Batch Loss: 0.013812186196446419\n",
      "Epoch 2710, Loss: 0.0159316275803576, Final Batch Loss: 0.00032253560493700206\n",
      "Epoch 2711, Loss: 0.02881410282861907, Final Batch Loss: 0.0004968306748196483\n",
      "Epoch 2712, Loss: 0.06933631846186472, Final Batch Loss: 0.0016547514824196696\n",
      "Epoch 2713, Loss: 0.01876042040203174, Final Batch Loss: 9.753173799254e-05\n",
      "Epoch 2714, Loss: 0.012098249215341639, Final Batch Loss: 0.0003888435603585094\n",
      "Epoch 2715, Loss: 0.009543259389829473, Final Batch Loss: 0.00042363620013929904\n",
      "Epoch 2716, Loss: 0.04595213149150368, Final Batch Loss: 0.0002005187125178054\n",
      "Epoch 2717, Loss: 0.03443440789123997, Final Batch Loss: 0.0006922417087480426\n",
      "Epoch 2718, Loss: 0.00891573559056269, Final Batch Loss: 0.0010569001315161586\n",
      "Epoch 2719, Loss: 0.027652376586047467, Final Batch Loss: 0.00038118797237984836\n",
      "Epoch 2720, Loss: 0.022333401575451717, Final Batch Loss: 0.0007563459221273661\n",
      "Epoch 2721, Loss: 0.06567823696241248, Final Batch Loss: 0.00011694676504703239\n",
      "Epoch 2722, Loss: 0.05546546774166927, Final Batch Loss: 0.0048961034044623375\n",
      "Epoch 2723, Loss: 0.015593728585372446, Final Batch Loss: 3.247332278988324e-05\n",
      "Epoch 2724, Loss: 0.008409351747104665, Final Batch Loss: 0.0001470357528887689\n",
      "Epoch 2725, Loss: 0.016344694939107285, Final Batch Loss: 0.0006505369092337787\n",
      "Epoch 2726, Loss: 0.019769725377045688, Final Batch Loss: 0.00028528147959150374\n",
      "Epoch 2727, Loss: 0.06066285424867601, Final Batch Loss: 4.738105781143531e-05\n",
      "Epoch 2728, Loss: 0.008914685611671302, Final Batch Loss: 0.00025252398336306214\n",
      "Epoch 2729, Loss: 0.03361865340411896, Final Batch Loss: 0.00010554324398981407\n",
      "Epoch 2730, Loss: 0.08082425121392589, Final Batch Loss: 0.012341255322098732\n",
      "Epoch 2731, Loss: 0.01068543334986316, Final Batch Loss: 0.00032210160861723125\n",
      "Epoch 2732, Loss: 0.013364379154154449, Final Batch Loss: 0.002959842560812831\n",
      "Epoch 2733, Loss: 0.03854872743249871, Final Batch Loss: 0.00022887719387654215\n",
      "Epoch 2734, Loss: 0.04168249936265056, Final Batch Loss: 0.002095225965604186\n",
      "Epoch 2735, Loss: 0.08844679353933316, Final Batch Loss: 0.00019544166570995003\n",
      "Epoch 2736, Loss: 0.009871206857496873, Final Batch Loss: 0.0007791483076289296\n",
      "Epoch 2737, Loss: 0.027934257639572024, Final Batch Loss: 0.0015192435821518302\n",
      "Epoch 2738, Loss: 0.04896366943285102, Final Batch Loss: 0.002838206710293889\n",
      "Epoch 2739, Loss: 0.0936477373688831, Final Batch Loss: 0.004528057761490345\n",
      "Epoch 2740, Loss: 0.03049579684739001, Final Batch Loss: 0.0005324070225469768\n",
      "Epoch 2741, Loss: 0.04437406315264525, Final Batch Loss: 0.0013546111294999719\n",
      "Epoch 2742, Loss: 0.02572596118261572, Final Batch Loss: 0.00031980962376110256\n",
      "Epoch 2743, Loss: 0.008126852728310041, Final Batch Loss: 0.0009009037166833878\n",
      "Epoch 2744, Loss: 0.015369361499324441, Final Batch Loss: 0.000724672747310251\n",
      "Epoch 2745, Loss: 0.02617615020062658, Final Batch Loss: 0.0034439146984368563\n",
      "Epoch 2746, Loss: 0.045540364199041505, Final Batch Loss: 0.001961679896339774\n",
      "Epoch 2747, Loss: 0.005638303147861734, Final Batch Loss: 0.0002743042423389852\n",
      "Epoch 2748, Loss: 0.01185141706628201, Final Batch Loss: 0.001166761969216168\n",
      "Epoch 2749, Loss: 0.004560224308079341, Final Batch Loss: 0.0005014439229853451\n",
      "Epoch 2750, Loss: 0.006862375876153237, Final Batch Loss: 6.923740147612989e-05\n",
      "Epoch 2751, Loss: 0.04079378728602023, Final Batch Loss: 1.981115019589197e-05\n",
      "Epoch 2752, Loss: 0.003650878137705149, Final Batch Loss: 0.0003102457267232239\n",
      "Epoch 2753, Loss: 0.0256121337388322, Final Batch Loss: 0.0011431619059294462\n",
      "Epoch 2754, Loss: 0.035048799887590576, Final Batch Loss: 0.003109447890892625\n",
      "Epoch 2755, Loss: 0.00868760717821715, Final Batch Loss: 0.0001413019053870812\n",
      "Epoch 2756, Loss: 0.028059355307050282, Final Batch Loss: 0.0002390661247773096\n",
      "Epoch 2757, Loss: 0.013222822080933838, Final Batch Loss: 0.0011429140577092767\n",
      "Epoch 2758, Loss: 0.020455399419006426, Final Batch Loss: 3.062723408220336e-05\n",
      "Epoch 2759, Loss: 0.015936155439703725, Final Batch Loss: 0.0004726248444058001\n",
      "Epoch 2760, Loss: 0.012392620325044845, Final Batch Loss: 0.002986216451972723\n",
      "Epoch 2761, Loss: 0.002512988985472475, Final Batch Loss: 2.6964795324602164e-05\n",
      "Epoch 2762, Loss: 0.004959555877576349, Final Batch Loss: 2.6052744942717254e-05\n",
      "Epoch 2763, Loss: 0.003680210391394212, Final Batch Loss: 0.00017530855257064104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2764, Loss: 0.007799243820045376, Final Batch Loss: 0.00012131585390307009\n",
      "Epoch 2765, Loss: 0.013191592232033145, Final Batch Loss: 5.602558303507976e-05\n",
      "Epoch 2766, Loss: 0.00807278203592432, Final Batch Loss: 0.001157143502496183\n",
      "Epoch 2767, Loss: 0.0028667931737800245, Final Batch Loss: 3.149577241856605e-05\n",
      "Epoch 2768, Loss: 0.0030940953402023297, Final Batch Loss: 0.0003118194581475109\n",
      "Epoch 2769, Loss: 0.010512700982872047, Final Batch Loss: 7.653318789380137e-06\n",
      "Epoch 2770, Loss: 0.002980639974339283, Final Batch Loss: 0.00040896894643083215\n",
      "Epoch 2771, Loss: 0.07409834084501199, Final Batch Loss: 0.0003805438173003495\n",
      "Epoch 2772, Loss: 0.0386949656658544, Final Batch Loss: 0.00012385871377773583\n",
      "Epoch 2773, Loss: 0.01336013303080108, Final Batch Loss: 0.008094459772109985\n",
      "Epoch 2774, Loss: 0.012743631610646844, Final Batch Loss: 0.00011868772708112374\n",
      "Epoch 2775, Loss: 0.04576662104955176, Final Batch Loss: 3.515840944601223e-05\n",
      "Epoch 2776, Loss: 0.08146000363194617, Final Batch Loss: 0.00016828732623253018\n",
      "Epoch 2777, Loss: 0.04102382052951725, Final Batch Loss: 0.0006866363692097366\n",
      "Epoch 2778, Loss: 0.011485409173474181, Final Batch Loss: 0.000147543047205545\n",
      "Epoch 2779, Loss: 0.0324706263418193, Final Batch Loss: 0.0013410409446805716\n",
      "Epoch 2780, Loss: 0.024068631795671536, Final Batch Loss: 0.0015468965284526348\n",
      "Epoch 2781, Loss: 0.02209266906720586, Final Batch Loss: 0.00045817167847417295\n",
      "Epoch 2782, Loss: 0.024645956214953912, Final Batch Loss: 3.62962564395275e-05\n",
      "Epoch 2783, Loss: 0.02789449773990782, Final Batch Loss: 0.0003422473091632128\n",
      "Epoch 2784, Loss: 0.027531228042789735, Final Batch Loss: 0.0010258370311930776\n",
      "Epoch 2785, Loss: 0.05930316645753919, Final Batch Loss: 0.004786285106092691\n",
      "Epoch 2786, Loss: 0.06968370467802742, Final Batch Loss: 0.011901898309588432\n",
      "Epoch 2787, Loss: 0.12224681686348049, Final Batch Loss: 0.0052449991926550865\n",
      "Epoch 2788, Loss: 0.13839632128656376, Final Batch Loss: 0.002671588445082307\n",
      "Epoch 2789, Loss: 0.029380735009908676, Final Batch Loss: 0.0003456818521954119\n",
      "Epoch 2790, Loss: 0.08728847491147462, Final Batch Loss: 0.007109660189598799\n",
      "Epoch 2791, Loss: 0.043568078690441325, Final Batch Loss: 0.002792306710034609\n",
      "Epoch 2792, Loss: 0.054668791621224955, Final Batch Loss: 0.0018148107919842005\n",
      "Epoch 2793, Loss: 0.01160744947992498, Final Batch Loss: 0.0012442173901945353\n",
      "Epoch 2794, Loss: 0.03066695239249384, Final Batch Loss: 0.00044525950215756893\n",
      "Epoch 2795, Loss: 0.015576913556287764, Final Batch Loss: 9.820017294259742e-05\n",
      "Epoch 2796, Loss: 0.0060249414964346215, Final Batch Loss: 7.532698509749025e-05\n",
      "Epoch 2797, Loss: 0.004912177893857006, Final Batch Loss: 0.00033881323179230094\n",
      "Epoch 2798, Loss: 0.039509029702458065, Final Batch Loss: 6.32941082585603e-05\n",
      "Epoch 2799, Loss: 0.053721021133242175, Final Batch Loss: 0.0005473451456055045\n",
      "Epoch 2800, Loss: 0.019695974773640046, Final Batch Loss: 0.00010497954644961283\n",
      "Epoch 2801, Loss: 0.01162239842597046, Final Batch Loss: 0.00012415267701726407\n",
      "Epoch 2802, Loss: 0.0445732108419179, Final Batch Loss: 0.00047787674702703953\n",
      "Epoch 2803, Loss: 0.020598446895746747, Final Batch Loss: 5.200035957386717e-05\n",
      "Epoch 2804, Loss: 0.04775772132416023, Final Batch Loss: 0.0005098855472169816\n",
      "Epoch 2805, Loss: 0.025377584486705018, Final Batch Loss: 0.00012714890181086957\n",
      "Epoch 2806, Loss: 0.015478346002055332, Final Batch Loss: 0.0018748054280877113\n",
      "Epoch 2807, Loss: 0.004633350059521035, Final Batch Loss: 1.7791191567084752e-05\n",
      "Epoch 2808, Loss: 0.00995728568341292, Final Batch Loss: 0.0030393830966204405\n",
      "Epoch 2809, Loss: 0.007633704221007065, Final Batch Loss: 0.0015433283988386393\n",
      "Epoch 2810, Loss: 0.015078383990839939, Final Batch Loss: 0.0007994542829692364\n",
      "Epoch 2811, Loss: 0.015717705562565243, Final Batch Loss: 0.00015854390221647918\n",
      "Epoch 2812, Loss: 0.011015344487532275, Final Batch Loss: 0.001707022893242538\n",
      "Epoch 2813, Loss: 0.006794666425776086, Final Batch Loss: 0.0003404189774300903\n",
      "Epoch 2814, Loss: 0.016906606888369424, Final Batch Loss: 0.001852969522587955\n",
      "Epoch 2815, Loss: 0.011866108798130881, Final Batch Loss: 0.00010010660480475053\n",
      "Epoch 2816, Loss: 0.017478082692832686, Final Batch Loss: 0.0002824717666953802\n",
      "Epoch 2817, Loss: 0.01564127832170925, Final Batch Loss: 0.0002683395578060299\n",
      "Epoch 2818, Loss: 0.009860481026407797, Final Batch Loss: 0.0001129577576648444\n",
      "Epoch 2819, Loss: 0.010548819838732015, Final Batch Loss: 5.8172539866063744e-05\n",
      "Epoch 2820, Loss: 0.013271943589643342, Final Batch Loss: 0.0005048633902333677\n",
      "Epoch 2821, Loss: 0.021650328337273095, Final Batch Loss: 5.288879037834704e-05\n",
      "Epoch 2822, Loss: 0.02765455980988918, Final Batch Loss: 0.00291795888915658\n",
      "Epoch 2823, Loss: 0.006675490372799686, Final Batch Loss: 3.477983773336746e-05\n",
      "Epoch 2824, Loss: 0.024543325416743755, Final Batch Loss: 5.009381129639223e-05\n",
      "Epoch 2825, Loss: 0.01574940550199244, Final Batch Loss: 0.002402120502665639\n",
      "Epoch 2826, Loss: 0.004613983794115484, Final Batch Loss: 0.00012869964120909572\n",
      "Epoch 2827, Loss: 0.009452849595618318, Final Batch Loss: 0.0006858434062451124\n",
      "Epoch 2828, Loss: 0.027140132116983295, Final Batch Loss: 6.740077515132725e-05\n",
      "Epoch 2829, Loss: 0.016424486176219943, Final Batch Loss: 0.0019874507561326027\n",
      "Epoch 2830, Loss: 0.022389553200810042, Final Batch Loss: 7.626701699336991e-05\n",
      "Epoch 2831, Loss: 0.03164468093746109, Final Batch Loss: 0.0010960956569761038\n",
      "Epoch 2832, Loss: 0.07251798591460101, Final Batch Loss: 0.0005222074105404317\n",
      "Epoch 2833, Loss: 0.03931695672144997, Final Batch Loss: 0.0011400760849937797\n",
      "Epoch 2834, Loss: 0.016139867046149448, Final Batch Loss: 4.1069470171350986e-05\n",
      "Epoch 2835, Loss: 0.006946568716557522, Final Batch Loss: 1.203904776048148e-05\n",
      "Epoch 2836, Loss: 0.013845951662915468, Final Batch Loss: 0.0026203510351479053\n",
      "Epoch 2837, Loss: 0.008332809600688051, Final Batch Loss: 0.00026603342848829925\n",
      "Epoch 2838, Loss: 0.017485055312135955, Final Batch Loss: 0.00012625592353288084\n",
      "Epoch 2839, Loss: 0.02844466009264579, Final Batch Loss: 0.0002707912935875356\n",
      "Epoch 2840, Loss: 0.05394326074201672, Final Batch Loss: 0.0003600410418584943\n",
      "Epoch 2841, Loss: 0.034230310084240045, Final Batch Loss: 0.0014258783776313066\n",
      "Epoch 2842, Loss: 0.011751662095775828, Final Batch Loss: 0.0001675250823609531\n",
      "Epoch 2843, Loss: 0.010886399230003008, Final Batch Loss: 9.375219087814912e-05\n",
      "Epoch 2844, Loss: 0.03673397343391116, Final Batch Loss: 0.001142643392086029\n",
      "Epoch 2845, Loss: 0.030573598975024652, Final Batch Loss: 0.00036459905095398426\n",
      "Epoch 2846, Loss: 0.04963597362257133, Final Batch Loss: 0.03612169250845909\n",
      "Epoch 2847, Loss: 0.05270806655607885, Final Batch Loss: 5.9155234339414164e-05\n",
      "Epoch 2848, Loss: 0.07144142231118167, Final Batch Loss: 0.0010489366250112653\n",
      "Epoch 2849, Loss: 0.044223106211575214, Final Batch Loss: 0.0001862568169599399\n",
      "Epoch 2850, Loss: 0.034515267367169145, Final Batch Loss: 0.0028079228941351175\n",
      "Epoch 2851, Loss: 0.056286300474312156, Final Batch Loss: 0.004061281215399504\n",
      "Epoch 2852, Loss: 0.055678540011285804, Final Batch Loss: 0.008640259504318237\n",
      "Epoch 2853, Loss: 0.034038011315715266, Final Batch Loss: 0.0009128943784162402\n",
      "Epoch 2854, Loss: 0.026717275788541883, Final Batch Loss: 0.0012269973522052169\n",
      "Epoch 2855, Loss: 0.03514086407813011, Final Batch Loss: 0.001902818912640214\n",
      "Epoch 2856, Loss: 0.01392314023541985, Final Batch Loss: 0.003814981784671545\n",
      "Epoch 2857, Loss: 0.011386512138415128, Final Batch Loss: 0.005623174365609884\n",
      "Epoch 2858, Loss: 0.036035474249729305, Final Batch Loss: 0.002439109142869711\n",
      "Epoch 2859, Loss: 0.03265403142904688, Final Batch Loss: 0.0003318929811939597\n",
      "Epoch 2860, Loss: 0.05470750865788432, Final Batch Loss: 0.0006308252341113985\n",
      "Epoch 2861, Loss: 0.06439819216211617, Final Batch Loss: 8.635190170025453e-05\n",
      "Epoch 2862, Loss: 0.015099331489182077, Final Batch Loss: 0.0009194487356580794\n",
      "Epoch 2863, Loss: 0.005034158384660259, Final Batch Loss: 6.1286729760468e-05\n",
      "Epoch 2864, Loss: 0.004717361589428037, Final Batch Loss: 0.00030695469467900693\n",
      "Epoch 2865, Loss: 0.032691396267182427, Final Batch Loss: 0.0002580673899501562\n",
      "Epoch 2866, Loss: 0.00926286643880303, Final Batch Loss: 3.20593171636574e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2867, Loss: 0.002972208025312284, Final Batch Loss: 2.9049946533632465e-05\n",
      "Epoch 2868, Loss: 0.02258950888426625, Final Batch Loss: 0.002443929435685277\n",
      "Epoch 2869, Loss: 0.013979176383145386, Final Batch Loss: 0.00031874876003712416\n",
      "Epoch 2870, Loss: 0.005710204377464834, Final Batch Loss: 0.00253102695569396\n",
      "Epoch 2871, Loss: 0.014790979912504554, Final Batch Loss: 0.00572072621434927\n",
      "Epoch 2872, Loss: 0.02506643528067798, Final Batch Loss: 0.000422251207055524\n",
      "Epoch 2873, Loss: 0.00751561441575177, Final Batch Loss: 3.6600038583856076e-05\n",
      "Epoch 2874, Loss: 0.010029198710071796, Final Batch Loss: 7.008403190411627e-05\n",
      "Epoch 2875, Loss: 0.028792026241717394, Final Batch Loss: 0.0017781847855076194\n",
      "Epoch 2876, Loss: 0.045322211777602206, Final Batch Loss: 0.0005758072948083282\n",
      "Epoch 2877, Loss: 0.05117368656647159, Final Batch Loss: 0.002432578708976507\n",
      "Epoch 2878, Loss: 0.01738646630110452, Final Batch Loss: 0.0006237109773792326\n",
      "Epoch 2879, Loss: 0.012051741148752626, Final Batch Loss: 6.820030830567703e-05\n",
      "Epoch 2880, Loss: 0.0181969655714056, Final Batch Loss: 3.5809058317681774e-05\n",
      "Epoch 2881, Loss: 0.04099118217709474, Final Batch Loss: 5.149492062628269e-05\n",
      "Epoch 2882, Loss: 0.027395163735491224, Final Batch Loss: 0.012807266786694527\n",
      "Epoch 2883, Loss: 0.07206807253533043, Final Batch Loss: 0.0011383394012227654\n",
      "Epoch 2884, Loss: 0.020928356583681307, Final Batch Loss: 0.0006129684625193477\n",
      "Epoch 2885, Loss: 0.031953899131622165, Final Batch Loss: 0.00011335675662849098\n",
      "Epoch 2886, Loss: 0.029395019380899612, Final Batch Loss: 0.00011850104783661664\n",
      "Epoch 2887, Loss: 0.023332313241553493, Final Batch Loss: 0.0009320980752818286\n",
      "Epoch 2888, Loss: 0.012167221011623042, Final Batch Loss: 0.001545842387713492\n",
      "Epoch 2889, Loss: 0.024140336830896558, Final Batch Loss: 3.379182453500107e-05\n",
      "Epoch 2890, Loss: 0.022792861207562964, Final Batch Loss: 0.00011452058970462531\n",
      "Epoch 2891, Loss: 0.030159861882566474, Final Batch Loss: 6.307083094725385e-05\n",
      "Epoch 2892, Loss: 0.021459279967530165, Final Batch Loss: 0.000111195637146011\n",
      "Epoch 2893, Loss: 0.029570224243798293, Final Batch Loss: 0.011739428155124187\n",
      "Epoch 2894, Loss: 0.025765248406969476, Final Batch Loss: 0.00026546212029643357\n",
      "Epoch 2895, Loss: 0.0608350363545469, Final Batch Loss: 0.0053002857603132725\n",
      "Epoch 2896, Loss: 0.021251944697723957, Final Batch Loss: 0.011965632438659668\n",
      "Epoch 2897, Loss: 0.029705554643442156, Final Batch Loss: 0.011839508078992367\n",
      "Epoch 2898, Loss: 0.06631672599178273, Final Batch Loss: 0.013114262372255325\n",
      "Epoch 2899, Loss: 0.052575178971892456, Final Batch Loss: 0.0001314549590460956\n",
      "Epoch 2900, Loss: 0.015508006699747057, Final Batch Loss: 0.00023020413937047124\n",
      "Epoch 2901, Loss: 0.037823834627488395, Final Batch Loss: 0.00012068884825566784\n",
      "Epoch 2902, Loss: 0.07371860779312556, Final Batch Loss: 0.006707179360091686\n",
      "Epoch 2903, Loss: 0.0427303564210888, Final Batch Loss: 0.0001307171187363565\n",
      "Epoch 2904, Loss: 0.02744379555224441, Final Batch Loss: 0.0019670051988214254\n",
      "Epoch 2905, Loss: 0.027082368309493177, Final Batch Loss: 0.0001111980018322356\n",
      "Epoch 2906, Loss: 0.03907290611459757, Final Batch Loss: 0.001064273645170033\n",
      "Epoch 2907, Loss: 0.03927161586125294, Final Batch Loss: 0.006697497330605984\n",
      "Epoch 2908, Loss: 0.026566400469164364, Final Batch Loss: 0.0057693831622600555\n",
      "Epoch 2909, Loss: 0.05994060978628113, Final Batch Loss: 0.0007134061888791621\n",
      "Epoch 2910, Loss: 0.02488479728344828, Final Batch Loss: 0.0020400697831064463\n",
      "Epoch 2911, Loss: 0.01062272779745399, Final Batch Loss: 0.003959935624152422\n",
      "Epoch 2912, Loss: 0.05994462416128954, Final Batch Loss: 0.0003778529935516417\n",
      "Epoch 2913, Loss: 0.022311757298666635, Final Batch Loss: 0.0013983133248984814\n",
      "Epoch 2914, Loss: 0.02968992271598836, Final Batch Loss: 0.00010439534526085481\n",
      "Epoch 2915, Loss: 0.036105302580835996, Final Batch Loss: 8.969953341875225e-05\n",
      "Epoch 2916, Loss: 0.0361125389499648, Final Batch Loss: 0.0006767402519471943\n",
      "Epoch 2917, Loss: 0.008921520142394002, Final Batch Loss: 0.00022906313824933022\n",
      "Epoch 2918, Loss: 0.01276904667065537, Final Batch Loss: 4.53159591415897e-05\n",
      "Epoch 2919, Loss: 0.006923143335370696, Final Batch Loss: 0.00027281997608952224\n",
      "Epoch 2920, Loss: 0.009545022995553154, Final Batch Loss: 2.7152715119882487e-05\n",
      "Epoch 2921, Loss: 0.0389172002351188, Final Batch Loss: 0.0001350880484096706\n",
      "Epoch 2922, Loss: 0.06709271562795038, Final Batch Loss: 0.00038559373933821917\n",
      "Epoch 2923, Loss: 0.01113067390906508, Final Batch Loss: 0.0001021186399157159\n",
      "Epoch 2924, Loss: 0.009588750093826093, Final Batch Loss: 0.00016488699475303292\n",
      "Epoch 2925, Loss: 0.004700706535004429, Final Batch Loss: 0.00015386073209811002\n",
      "Epoch 2926, Loss: 0.023084940607077442, Final Batch Loss: 3.618753908085637e-05\n",
      "Epoch 2927, Loss: 0.013044908726442372, Final Batch Loss: 0.0002963887236546725\n",
      "Epoch 2928, Loss: 0.006322946250293171, Final Batch Loss: 2.7442411010270007e-05\n",
      "Epoch 2929, Loss: 0.023422151291015325, Final Batch Loss: 0.0001366715005133301\n",
      "Epoch 2930, Loss: 0.0030895479903847445, Final Batch Loss: 0.00025295012164860964\n",
      "Epoch 2931, Loss: 0.016889175305550452, Final Batch Loss: 0.001068524201400578\n",
      "Epoch 2932, Loss: 0.014539507457811851, Final Batch Loss: 0.0036070586647838354\n",
      "Epoch 2933, Loss: 0.018172110794694163, Final Batch Loss: 0.000684463360812515\n",
      "Epoch 2934, Loss: 0.008012917201995151, Final Batch Loss: 0.002698672702535987\n",
      "Epoch 2935, Loss: 0.009811579519009683, Final Batch Loss: 0.0003339470422361046\n",
      "Epoch 2936, Loss: 0.08465731992328074, Final Batch Loss: 0.051445309072732925\n",
      "Epoch 2937, Loss: 0.010451565618495806, Final Batch Loss: 0.00031527233659289777\n",
      "Epoch 2938, Loss: 0.018238360302348156, Final Batch Loss: 0.002716887043789029\n",
      "Epoch 2939, Loss: 0.04948861398588633, Final Batch Loss: 0.0013873422285541892\n",
      "Epoch 2940, Loss: 0.02183616309775971, Final Batch Loss: 0.0006889816140756011\n",
      "Epoch 2941, Loss: 0.02657613878545817, Final Batch Loss: 0.00022895511938259006\n",
      "Epoch 2942, Loss: 0.023080690933056758, Final Batch Loss: 0.00015745151904411614\n",
      "Epoch 2943, Loss: 0.06781409770337632, Final Batch Loss: 0.001142469816841185\n",
      "Epoch 2944, Loss: 0.013871146449673688, Final Batch Loss: 8.688127854838967e-05\n",
      "Epoch 2945, Loss: 0.009307028994953725, Final Batch Loss: 2.2628895749221556e-05\n",
      "Epoch 2946, Loss: 0.025908076197083574, Final Batch Loss: 9.303151455242187e-05\n",
      "Epoch 2947, Loss: 0.020058281865203753, Final Batch Loss: 0.0034789014607667923\n",
      "Epoch 2948, Loss: 0.03510431630638777, Final Batch Loss: 0.0005411255988292396\n",
      "Epoch 2949, Loss: 0.021294554837368196, Final Batch Loss: 7.539249054389074e-05\n",
      "Epoch 2950, Loss: 0.0291663832613267, Final Batch Loss: 0.010487052612006664\n",
      "Epoch 2951, Loss: 0.009164705043076538, Final Batch Loss: 0.00013733103696722537\n",
      "Epoch 2952, Loss: 0.02197689700551564, Final Batch Loss: 5.321529897628352e-05\n",
      "Epoch 2953, Loss: 0.021106310832692543, Final Batch Loss: 0.0008020993554964662\n",
      "Epoch 2954, Loss: 0.016567413324082736, Final Batch Loss: 0.0002008301526075229\n",
      "Epoch 2955, Loss: 0.026839160436793463, Final Batch Loss: 0.02153927832841873\n",
      "Epoch 2956, Loss: 0.010147438482817961, Final Batch Loss: 0.0003473317192401737\n",
      "Epoch 2957, Loss: 0.06544259863585467, Final Batch Loss: 8.897281077224761e-05\n",
      "Epoch 2958, Loss: 0.00836894045551162, Final Batch Loss: 7.925347017589957e-05\n",
      "Epoch 2959, Loss: 0.004206254627206363, Final Batch Loss: 7.133738836273551e-05\n",
      "Epoch 2960, Loss: 0.044318099524389254, Final Batch Loss: 0.000645634310785681\n",
      "Epoch 2961, Loss: 0.0101086991126067, Final Batch Loss: 0.0005531586357392371\n",
      "Epoch 2962, Loss: 0.0030387000551854726, Final Batch Loss: 2.2787393390899524e-05\n",
      "Epoch 2963, Loss: 0.003768572571061668, Final Batch Loss: 2.183883589168545e-05\n",
      "Epoch 2964, Loss: 0.020725839600345353, Final Batch Loss: 0.00017223239410668612\n",
      "Epoch 2965, Loss: 0.015780416593770497, Final Batch Loss: 0.004709418397396803\n",
      "Epoch 2966, Loss: 0.021262353930069366, Final Batch Loss: 0.001257696538232267\n",
      "Epoch 2967, Loss: 0.01425074660073733, Final Batch Loss: 2.2206115318113007e-05\n",
      "Epoch 2968, Loss: 0.025136067808489315, Final Batch Loss: 0.00033450062619522214\n",
      "Epoch 2969, Loss: 0.020368217632494634, Final Batch Loss: 3.3842465199995786e-05\n",
      "Epoch 2970, Loss: 0.049677855477057165, Final Batch Loss: 0.00038020272040739655\n",
      "Epoch 2971, Loss: 0.030900384135748027, Final Batch Loss: 0.0043183257803320885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2972, Loss: 0.0840695249971759, Final Batch Loss: 0.005649973638355732\n",
      "Epoch 2973, Loss: 0.031037514680065215, Final Batch Loss: 0.002164293546229601\n",
      "Epoch 2974, Loss: 0.10404027773620328, Final Batch Loss: 0.006843253038823605\n",
      "Epoch 2975, Loss: 0.059539726564253215, Final Batch Loss: 0.00021895459212828428\n",
      "Epoch 2976, Loss: 0.055885206787934294, Final Batch Loss: 0.0038348797243088484\n",
      "Epoch 2977, Loss: 0.049699436785886064, Final Batch Loss: 0.00019093196897301823\n",
      "Epoch 2978, Loss: 0.07242039953416679, Final Batch Loss: 0.0064313956536352634\n",
      "Epoch 2979, Loss: 0.044311049867246766, Final Batch Loss: 0.009958683513104916\n",
      "Epoch 2980, Loss: 0.06899521164450562, Final Batch Loss: 0.002173739718273282\n",
      "Epoch 2981, Loss: 0.03983113517460879, Final Batch Loss: 0.0012972239637747407\n",
      "Epoch 2982, Loss: 0.07571405121416319, Final Batch Loss: 0.015766676515340805\n",
      "Epoch 2983, Loss: 0.06733480087859789, Final Batch Loss: 0.020396869629621506\n",
      "Epoch 2984, Loss: 0.05004908578848699, Final Batch Loss: 0.015990514308214188\n",
      "Epoch 2985, Loss: 0.06392162114207167, Final Batch Loss: 0.0009587291278876364\n",
      "Epoch 2986, Loss: 0.10814559398568235, Final Batch Loss: 0.016880951821804047\n",
      "Epoch 2987, Loss: 0.07294848586025182, Final Batch Loss: 0.016208605840802193\n",
      "Epoch 2988, Loss: 0.022938469002838247, Final Batch Loss: 0.00028743327129632235\n",
      "Epoch 2989, Loss: 0.014021845432580449, Final Batch Loss: 0.00019490529666654766\n",
      "Epoch 2990, Loss: 0.025739353259268682, Final Batch Loss: 0.0005719714099541306\n",
      "Epoch 2991, Loss: 0.036382425132615026, Final Batch Loss: 6.48344139335677e-05\n",
      "Epoch 2992, Loss: 0.11613030648004496, Final Batch Loss: 0.00427843676880002\n",
      "Epoch 2993, Loss: 0.21770100107823964, Final Batch Loss: 0.007797664031386375\n",
      "Epoch 2994, Loss: 0.0569700595078757, Final Batch Loss: 0.0014320436166599393\n",
      "Epoch 2995, Loss: 0.06671420371276326, Final Batch Loss: 0.003347297664731741\n",
      "Epoch 2996, Loss: 0.04492978710914031, Final Batch Loss: 0.0006521700997836888\n",
      "Epoch 2997, Loss: 0.024581968958955258, Final Batch Loss: 0.0006937672733329237\n",
      "Epoch 2998, Loss: 0.03302315298060421, Final Batch Loss: 0.007217041216790676\n",
      "Epoch 2999, Loss: 0.016103140798804816, Final Batch Loss: 0.0007335901609621942\n",
      "Epoch 3000, Loss: 0.04821432535391068, Final Batch Loss: 0.0006956100114621222\n",
      "Epoch 3001, Loss: 0.04674402358796215, Final Batch Loss: 0.008574813604354858\n",
      "Epoch 3002, Loss: 0.02913877454921021, Final Batch Loss: 0.018980134278535843\n",
      "Epoch 3003, Loss: 0.03626352987339487, Final Batch Loss: 0.000314929784508422\n",
      "Epoch 3004, Loss: 0.013550834933994338, Final Batch Loss: 0.00022047493257559836\n",
      "Epoch 3005, Loss: 0.02470019863540074, Final Batch Loss: 8.886479918146506e-05\n",
      "Epoch 3006, Loss: 0.051865833418560214, Final Batch Loss: 0.0004006838717032224\n",
      "Epoch 3007, Loss: 0.04081031997338869, Final Batch Loss: 0.0008039837121032178\n",
      "Epoch 3008, Loss: 0.04250311912619509, Final Batch Loss: 0.0007606361177749932\n",
      "Epoch 3009, Loss: 0.014159316240693443, Final Batch Loss: 0.0005888493615202606\n",
      "Epoch 3010, Loss: 0.04366383682645392, Final Batch Loss: 0.0010269444901496172\n",
      "Epoch 3011, Loss: 0.0494766101546702, Final Batch Loss: 0.00010063123772852123\n",
      "Epoch 3012, Loss: 0.05934976898788591, Final Batch Loss: 0.0020831965375691652\n",
      "Epoch 3013, Loss: 0.017828793927037623, Final Batch Loss: 8.889037417247891e-05\n",
      "Epoch 3014, Loss: 0.018680221466638613, Final Batch Loss: 0.0002849151787813753\n",
      "Epoch 3015, Loss: 0.011460869027359877, Final Batch Loss: 0.0028649428859353065\n",
      "Epoch 3016, Loss: 0.0064068427454913035, Final Batch Loss: 0.0004788835358340293\n",
      "Epoch 3017, Loss: 0.04918621487013297, Final Batch Loss: 0.0001392965205013752\n",
      "Epoch 3018, Loss: 0.01320870197014301, Final Batch Loss: 0.000466158555354923\n",
      "Epoch 3019, Loss: 0.045498005783883855, Final Batch Loss: 0.0023601893335580826\n",
      "Epoch 3020, Loss: 0.03265068951805006, Final Batch Loss: 4.900491694570519e-05\n",
      "Epoch 3021, Loss: 0.043000792342354544, Final Batch Loss: 0.0004880766209680587\n",
      "Epoch 3022, Loss: 0.020834095281315967, Final Batch Loss: 0.00018773980264086276\n",
      "Epoch 3023, Loss: 0.05558434971317183, Final Batch Loss: 0.0020888682920485735\n",
      "Epoch 3024, Loss: 0.0443081653866102, Final Batch Loss: 0.0002035279176197946\n",
      "Epoch 3025, Loss: 0.04759452685539145, Final Batch Loss: 8.645681373309344e-05\n",
      "Epoch 3026, Loss: 0.0281720088532893, Final Batch Loss: 0.0042031616903841496\n",
      "Epoch 3027, Loss: 0.02586687996517867, Final Batch Loss: 0.0003324564313516021\n",
      "Epoch 3028, Loss: 0.020045663084601983, Final Batch Loss: 0.0005226587527431548\n",
      "Epoch 3029, Loss: 0.032266835929476656, Final Batch Loss: 0.0041657183319330215\n",
      "Epoch 3030, Loss: 0.01474546793178888, Final Batch Loss: 0.004467214923352003\n",
      "Epoch 3031, Loss: 0.013292130941408686, Final Batch Loss: 0.00018235474999528378\n",
      "Epoch 3032, Loss: 0.020057439443917247, Final Batch Loss: 6.154930451884866e-05\n",
      "Epoch 3033, Loss: 0.017278971445193747, Final Batch Loss: 0.0005375061882659793\n",
      "Epoch 3034, Loss: 0.037307663460524054, Final Batch Loss: 0.000393225927837193\n",
      "Epoch 3035, Loss: 0.06720766573562287, Final Batch Loss: 0.003289120737463236\n",
      "Epoch 3036, Loss: 0.0361761929889326, Final Batch Loss: 0.0003562750353012234\n",
      "Epoch 3037, Loss: 0.024737657455261797, Final Batch Loss: 0.002475735731422901\n",
      "Epoch 3038, Loss: 0.03359339776216075, Final Batch Loss: 0.0033648686949163675\n",
      "Epoch 3039, Loss: 0.03644344145868672, Final Batch Loss: 0.00024468530318699777\n",
      "Epoch 3040, Loss: 0.022614067478571087, Final Batch Loss: 0.005115312524139881\n",
      "Epoch 3041, Loss: 0.01957801424578065, Final Batch Loss: 0.00022928752878215164\n",
      "Epoch 3042, Loss: 0.02107808939035749, Final Batch Loss: 0.0018535401904955506\n",
      "Epoch 3043, Loss: 0.03348692643339746, Final Batch Loss: 0.008788621984422207\n",
      "Epoch 3044, Loss: 0.016117634124384494, Final Batch Loss: 0.0028954786248505116\n",
      "Epoch 3045, Loss: 0.033684140791592654, Final Batch Loss: 0.003865488339215517\n",
      "Epoch 3046, Loss: 0.061172632511443226, Final Batch Loss: 0.004398819524794817\n",
      "Epoch 3047, Loss: 0.04880182584747672, Final Batch Loss: 0.004625021014362574\n",
      "Epoch 3048, Loss: 0.013874483072868316, Final Batch Loss: 2.057124765997287e-05\n",
      "Epoch 3049, Loss: 0.010437231032483396, Final Batch Loss: 0.00010764347825897858\n",
      "Epoch 3050, Loss: 0.013360208620724734, Final Batch Loss: 0.0011072939960286021\n",
      "Epoch 3051, Loss: 0.022268859938776586, Final Batch Loss: 0.002196288201957941\n",
      "Epoch 3052, Loss: 0.01753198664118827, Final Batch Loss: 0.00010200537508353591\n",
      "Epoch 3053, Loss: 0.010591903585009277, Final Batch Loss: 0.0004450493142940104\n",
      "Epoch 3054, Loss: 0.05556080698443111, Final Batch Loss: 0.003867530496791005\n",
      "Epoch 3055, Loss: 0.05076230539998505, Final Batch Loss: 0.040227700024843216\n",
      "Epoch 3056, Loss: 0.008382877786061727, Final Batch Loss: 0.0014321546768769622\n",
      "Epoch 3057, Loss: 0.020235220570612, Final Batch Loss: 0.0018947855569422245\n",
      "Epoch 3058, Loss: 0.035380103974603117, Final Batch Loss: 0.0006748108426108956\n",
      "Epoch 3059, Loss: 0.007876502786530182, Final Batch Loss: 0.0002853639889508486\n",
      "Epoch 3060, Loss: 0.009247337302440428, Final Batch Loss: 4.1196351958205923e-05\n",
      "Epoch 3061, Loss: 0.008698375851963647, Final Batch Loss: 0.00019310766947455704\n",
      "Epoch 3062, Loss: 0.037390226403658744, Final Batch Loss: 0.03233282268047333\n",
      "Epoch 3063, Loss: 0.010597039350614068, Final Batch Loss: 4.7645618906244636e-05\n",
      "Epoch 3064, Loss: 0.036278468589443946, Final Batch Loss: 0.0004188735329080373\n",
      "Epoch 3065, Loss: 0.00816626272717258, Final Batch Loss: 0.0004117024363949895\n",
      "Epoch 3066, Loss: 0.009492007577136974, Final Batch Loss: 0.0003587700193747878\n",
      "Epoch 3067, Loss: 0.009380977106957289, Final Batch Loss: 0.00028208483126945794\n",
      "Epoch 3068, Loss: 0.06328216929978225, Final Batch Loss: 0.0025350747164338827\n",
      "Epoch 3069, Loss: 0.035589496490501915, Final Batch Loss: 6.730433233315125e-05\n",
      "Epoch 3070, Loss: 0.04779420811973978, Final Batch Loss: 0.004497844260185957\n",
      "Epoch 3071, Loss: 0.03835575075208908, Final Batch Loss: 0.00011731098493328318\n",
      "Epoch 3072, Loss: 0.01574351802264573, Final Batch Loss: 0.0030648179817944765\n",
      "Epoch 3073, Loss: 0.03167386336281197, Final Batch Loss: 0.007972289808094501\n",
      "Epoch 3074, Loss: 0.020756424168212106, Final Batch Loss: 0.00018384205759502947\n",
      "Epoch 3075, Loss: 0.02124599883245537, Final Batch Loss: 0.000138385861646384\n",
      "Epoch 3076, Loss: 0.014067807449464453, Final Batch Loss: 4.0171562432078645e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3077, Loss: 0.05131503756638267, Final Batch Loss: 6.806659075664356e-05\n",
      "Epoch 3078, Loss: 0.054403360445576254, Final Batch Loss: 0.00034353818045929074\n",
      "Epoch 3079, Loss: 0.028636124872718938, Final Batch Loss: 0.002072915667667985\n",
      "Epoch 3080, Loss: 0.02663319668499753, Final Batch Loss: 0.00017026998102664948\n",
      "Epoch 3081, Loss: 0.014153458016153309, Final Batch Loss: 0.00011355465539963916\n",
      "Epoch 3082, Loss: 0.010971050116495462, Final Batch Loss: 0.0033649932593107224\n",
      "Epoch 3083, Loss: 0.02542640210413083, Final Batch Loss: 0.013370933011174202\n",
      "Epoch 3084, Loss: 0.0073084855157503625, Final Batch Loss: 0.0003614829620346427\n",
      "Epoch 3085, Loss: 0.026328527954319725, Final Batch Loss: 0.00661985669285059\n",
      "Epoch 3086, Loss: 0.07625911270588404, Final Batch Loss: 0.016382034868001938\n",
      "Epoch 3087, Loss: 0.04339767509918602, Final Batch Loss: 0.0013804291374981403\n",
      "Epoch 3088, Loss: 0.03848296589058009, Final Batch Loss: 0.00038334057899191976\n",
      "Epoch 3089, Loss: 0.02105462771578459, Final Batch Loss: 0.00010056855535367504\n",
      "Epoch 3090, Loss: 0.039976813799512456, Final Batch Loss: 5.767734182882123e-05\n",
      "Epoch 3091, Loss: 0.06853608066012384, Final Batch Loss: 0.00382179650478065\n",
      "Epoch 3092, Loss: 0.04310900825294084, Final Batch Loss: 0.0001071306542144157\n",
      "Epoch 3093, Loss: 0.018182763080403674, Final Batch Loss: 0.00017088967433664948\n",
      "Epoch 3094, Loss: 0.019286609891423723, Final Batch Loss: 0.0035067906137555838\n",
      "Epoch 3095, Loss: 0.016023222357034683, Final Batch Loss: 0.00021758308866992593\n",
      "Epoch 3096, Loss: 0.062304546603627387, Final Batch Loss: 7.747805648250505e-05\n",
      "Epoch 3097, Loss: 0.011815660782303894, Final Batch Loss: 0.00020715134451165795\n",
      "Epoch 3098, Loss: 0.014784609164053109, Final Batch Loss: 0.0010353790130466223\n",
      "Epoch 3099, Loss: 0.04622570076753618, Final Batch Loss: 0.0007853260030969977\n",
      "Epoch 3100, Loss: 0.040874608101148624, Final Batch Loss: 0.008310998789966106\n",
      "Epoch 3101, Loss: 0.023010012228041887, Final Batch Loss: 0.00020739689352922142\n",
      "Epoch 3102, Loss: 0.0555636235003476, Final Batch Loss: 0.0004428211832419038\n",
      "Epoch 3103, Loss: 0.03605746870016446, Final Batch Loss: 0.00011988682672381401\n",
      "Epoch 3104, Loss: 0.012972036893188488, Final Batch Loss: 0.00020950748876202852\n",
      "Epoch 3105, Loss: 0.021126205880136695, Final Batch Loss: 0.0014662911416962743\n",
      "Epoch 3106, Loss: 0.007747834326437442, Final Batch Loss: 2.1754964109277353e-05\n",
      "Epoch 3107, Loss: 0.012279077523999149, Final Batch Loss: 0.00012819597031921148\n",
      "Epoch 3108, Loss: 0.013880440083084977, Final Batch Loss: 4.064632594236173e-05\n",
      "Epoch 3109, Loss: 0.006349253158987267, Final Batch Loss: 0.00274319713935256\n",
      "Epoch 3110, Loss: 0.004481970275264757, Final Batch Loss: 3.300247044535354e-05\n",
      "Epoch 3111, Loss: 0.011118571506813169, Final Batch Loss: 0.001057992223650217\n",
      "Epoch 3112, Loss: 0.01027953053562669, Final Batch Loss: 6.557608139701188e-05\n",
      "Epoch 3113, Loss: 0.026591481673676753, Final Batch Loss: 0.000182721036253497\n",
      "Epoch 3114, Loss: 0.014415194858884206, Final Batch Loss: 4.4466003600973636e-05\n",
      "Epoch 3115, Loss: 0.06958958234372403, Final Batch Loss: 9.948846127372235e-05\n",
      "Epoch 3116, Loss: 0.012569432161399163, Final Batch Loss: 5.4712072596885264e-05\n",
      "Epoch 3117, Loss: 0.02927176171033352, Final Batch Loss: 0.000524314062204212\n",
      "Epoch 3118, Loss: 0.11443276202408015, Final Batch Loss: 0.0018437010003253818\n",
      "Epoch 3119, Loss: 0.13579397594367038, Final Batch Loss: 0.02217705547809601\n",
      "Epoch 3120, Loss: 0.05147156401653774, Final Batch Loss: 0.007574471179395914\n",
      "Epoch 3121, Loss: 0.06758972778334282, Final Batch Loss: 0.0017053576884791255\n",
      "Epoch 3122, Loss: 0.0767719901741657, Final Batch Loss: 0.000300085375783965\n",
      "Epoch 3123, Loss: 0.06639774664654396, Final Batch Loss: 0.009411009028553963\n",
      "Epoch 3124, Loss: 0.07151939769391902, Final Batch Loss: 0.0009301788522861898\n",
      "Epoch 3125, Loss: 0.029390571668045595, Final Batch Loss: 0.00030054262606427073\n",
      "Epoch 3126, Loss: 0.02419503793134936, Final Batch Loss: 0.001899633789435029\n",
      "Epoch 3127, Loss: 0.021930425960817956, Final Batch Loss: 0.008648136630654335\n",
      "Epoch 3128, Loss: 0.030499753389449324, Final Batch Loss: 0.0003040438168682158\n",
      "Epoch 3129, Loss: 0.030497320731228683, Final Batch Loss: 0.0027776185888797045\n",
      "Epoch 3130, Loss: 0.07076489783867146, Final Batch Loss: 0.001267178333364427\n",
      "Epoch 3131, Loss: 0.018370841557043605, Final Batch Loss: 0.0002999786811415106\n",
      "Epoch 3132, Loss: 0.007925533842353616, Final Batch Loss: 0.00031232566107064486\n",
      "Epoch 3133, Loss: 0.005958218527666759, Final Batch Loss: 0.0001739360741339624\n",
      "Epoch 3134, Loss: 0.023999956025363645, Final Batch Loss: 0.003715688129886985\n",
      "Epoch 3135, Loss: 0.028669084338616813, Final Batch Loss: 0.00011679965246003121\n",
      "Epoch 3136, Loss: 0.036516034037049394, Final Batch Loss: 0.0008225964265875518\n",
      "Epoch 3137, Loss: 0.009823296517424751, Final Batch Loss: 7.019619806669652e-05\n",
      "Epoch 3138, Loss: 0.016852293878400815, Final Batch Loss: 0.000365830201189965\n",
      "Epoch 3139, Loss: 0.009108268932322972, Final Batch Loss: 0.000216249653021805\n",
      "Epoch 3140, Loss: 0.02272984874434769, Final Batch Loss: 0.00014154113887343556\n",
      "Epoch 3141, Loss: 0.0075532846840360435, Final Batch Loss: 0.0014865213306620717\n",
      "Epoch 3142, Loss: 0.010242485998787743, Final Batch Loss: 2.0755265722982585e-05\n",
      "Epoch 3143, Loss: 0.006945990426174831, Final Batch Loss: 0.001492384704761207\n",
      "Epoch 3144, Loss: 0.004613481183696422, Final Batch Loss: 0.0002100673009408638\n",
      "Epoch 3145, Loss: 0.0025336552989756456, Final Batch Loss: 0.0001531049783807248\n",
      "Epoch 3146, Loss: 0.01790425253147987, Final Batch Loss: 1.9366170818102546e-05\n",
      "Epoch 3147, Loss: 0.0312725417134061, Final Batch Loss: 8.226952195400372e-05\n",
      "Epoch 3148, Loss: 0.01562894795279135, Final Batch Loss: 0.004010392818599939\n",
      "Epoch 3149, Loss: 0.0074403920325494255, Final Batch Loss: 0.0007537059136666358\n",
      "Epoch 3150, Loss: 0.0168096420384245, Final Batch Loss: 0.0008623117464594543\n",
      "Epoch 3151, Loss: 0.024944265994236048, Final Batch Loss: 5.530672933673486e-05\n",
      "Epoch 3152, Loss: 0.007519803983996098, Final Batch Loss: 0.0006538914749398828\n",
      "Epoch 3153, Loss: 0.01007762530753098, Final Batch Loss: 0.000964897102676332\n",
      "Epoch 3154, Loss: 0.019829701357593876, Final Batch Loss: 0.005528686568140984\n",
      "Epoch 3155, Loss: 0.006955648732400732, Final Batch Loss: 8.010157034732401e-05\n",
      "Epoch 3156, Loss: 0.011922146943106782, Final Batch Loss: 0.003862203797325492\n",
      "Epoch 3157, Loss: 0.039383105271554086, Final Batch Loss: 0.0065188477747142315\n",
      "Epoch 3158, Loss: 0.24943443944539467, Final Batch Loss: 0.012013239786028862\n",
      "Epoch 3159, Loss: 0.17350138736219378, Final Batch Loss: 0.007627903018146753\n",
      "Epoch 3160, Loss: 0.236258591117803, Final Batch Loss: 0.005733456928282976\n",
      "Epoch 3161, Loss: 0.14361017249029828, Final Batch Loss: 0.0009989484678953886\n",
      "Epoch 3162, Loss: 0.17403059219941497, Final Batch Loss: 0.021765150129795074\n",
      "Epoch 3163, Loss: 0.11877930368063971, Final Batch Loss: 0.0032079676166176796\n",
      "Epoch 3164, Loss: 0.05177984168403782, Final Batch Loss: 0.0016745327739045024\n",
      "Epoch 3165, Loss: 0.05444143613567576, Final Batch Loss: 0.016716107726097107\n",
      "Epoch 3166, Loss: 0.06090479274280369, Final Batch Loss: 0.0002929155307356268\n",
      "Epoch 3167, Loss: 0.04604085422033677, Final Batch Loss: 0.0007655116496607661\n",
      "Epoch 3168, Loss: 0.03727620925201336, Final Batch Loss: 0.0011772419093176723\n",
      "Epoch 3169, Loss: 0.037711488621425815, Final Batch Loss: 9.77877716650255e-05\n",
      "Epoch 3170, Loss: 0.04361643644369906, Final Batch Loss: 0.0015061474405229092\n",
      "Epoch 3171, Loss: 0.04215879579714965, Final Batch Loss: 0.0001949786237673834\n",
      "Epoch 3172, Loss: 0.07419618071435252, Final Batch Loss: 0.0037193438038229942\n",
      "Epoch 3173, Loss: 0.019869258252583677, Final Batch Loss: 0.0002762818767223507\n",
      "Epoch 3174, Loss: 0.03132741883746348, Final Batch Loss: 0.00046020143781788647\n",
      "Epoch 3175, Loss: 0.041531132941599935, Final Batch Loss: 0.006303898524492979\n",
      "Epoch 3176, Loss: 0.02737704036553623, Final Batch Loss: 0.0005456473445519805\n",
      "Epoch 3177, Loss: 0.020937419358233456, Final Batch Loss: 0.0027216665912419558\n",
      "Epoch 3178, Loss: 0.023128535438445397, Final Batch Loss: 0.007214711047708988\n",
      "Epoch 3179, Loss: 0.020164185283647384, Final Batch Loss: 0.0023771501146256924\n",
      "Epoch 3180, Loss: 0.017545250582770677, Final Batch Loss: 0.00010131184535566717\n",
      "Epoch 3181, Loss: 0.01843952929993975, Final Batch Loss: 0.002170909894630313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3182, Loss: 0.06148304321686737, Final Batch Loss: 0.000271118275122717\n",
      "Epoch 3183, Loss: 0.02053742795033031, Final Batch Loss: 0.0002809377037920058\n",
      "Epoch 3184, Loss: 0.03092228293826338, Final Batch Loss: 0.003547239350154996\n",
      "Epoch 3185, Loss: 0.03610361692699371, Final Batch Loss: 0.004815598484128714\n",
      "Epoch 3186, Loss: 0.0201674266718328, Final Batch Loss: 0.0012553875567391515\n",
      "Epoch 3187, Loss: 0.023114906427508686, Final Batch Loss: 0.0012852189829573035\n",
      "Epoch 3188, Loss: 0.03156518348259851, Final Batch Loss: 0.0023990687914192677\n",
      "Epoch 3189, Loss: 0.022674376756185666, Final Batch Loss: 0.00663989782333374\n",
      "Epoch 3190, Loss: 0.019684416711243102, Final Batch Loss: 4.3896256102016196e-05\n",
      "Epoch 3191, Loss: 0.018769508689729264, Final Batch Loss: 0.00021039682906121016\n",
      "Epoch 3192, Loss: 0.05629395523283165, Final Batch Loss: 0.002923259511590004\n",
      "Epoch 3193, Loss: 0.06647298982716165, Final Batch Loss: 0.0003577582829166204\n",
      "Epoch 3194, Loss: 0.025920951578882523, Final Batch Loss: 0.0029408601112663746\n",
      "Epoch 3195, Loss: 0.05648687667417107, Final Batch Loss: 0.00020953411876689643\n",
      "Epoch 3196, Loss: 0.0765084205668245, Final Batch Loss: 0.0001115713530452922\n",
      "Epoch 3197, Loss: 0.013947286730399355, Final Batch Loss: 0.0006340891704894602\n",
      "Epoch 3198, Loss: 0.010351521177653922, Final Batch Loss: 0.00019627774599939585\n",
      "Epoch 3199, Loss: 0.05907310626935214, Final Batch Loss: 0.005443971138447523\n",
      "Epoch 3200, Loss: 0.04834081954322755, Final Batch Loss: 0.0026371567510068417\n",
      "Epoch 3201, Loss: 0.033105951129982714, Final Batch Loss: 0.004782627336680889\n",
      "Epoch 3202, Loss: 0.04567092681827489, Final Batch Loss: 0.0007804322522133589\n",
      "Epoch 3203, Loss: 0.07743036611645948, Final Batch Loss: 0.0030066920444369316\n",
      "Epoch 3204, Loss: 0.032748050056397915, Final Batch Loss: 9.660129580879584e-05\n",
      "Epoch 3205, Loss: 0.03866834544169251, Final Batch Loss: 0.0010415480937808752\n",
      "Epoch 3206, Loss: 0.02015796844716533, Final Batch Loss: 6.613801815547049e-05\n",
      "Epoch 3207, Loss: 0.015483433613553643, Final Batch Loss: 0.003066211473196745\n",
      "Epoch 3208, Loss: 0.010424642285215668, Final Batch Loss: 0.0002335947792744264\n",
      "Epoch 3209, Loss: 0.005959678848739713, Final Batch Loss: 0.00021419806580524892\n",
      "Epoch 3210, Loss: 0.00794793928071158, Final Batch Loss: 0.001068663434125483\n",
      "Epoch 3211, Loss: 0.013963576619062223, Final Batch Loss: 5.9486446843948215e-05\n",
      "Epoch 3212, Loss: 0.007458302683517104, Final Batch Loss: 0.002030383562669158\n",
      "Epoch 3213, Loss: 0.01348411298749852, Final Batch Loss: 7.616179209435359e-05\n",
      "Epoch 3214, Loss: 0.019721590313565684, Final Batch Loss: 0.00029592198552563787\n",
      "Epoch 3215, Loss: 0.0444487365311943, Final Batch Loss: 0.004921729676425457\n",
      "Epoch 3216, Loss: 0.016709723982785363, Final Batch Loss: 3.887732600560412e-05\n",
      "Epoch 3217, Loss: 0.06278637264767895, Final Batch Loss: 0.0031138756312429905\n",
      "Epoch 3218, Loss: 0.03417433019421878, Final Batch Loss: 0.005673564039170742\n",
      "Epoch 3219, Loss: 0.013273041513457429, Final Batch Loss: 0.0015603824285790324\n",
      "Epoch 3220, Loss: 0.036786991855478846, Final Batch Loss: 0.00017080291581805795\n",
      "Epoch 3221, Loss: 0.010220572232356062, Final Batch Loss: 0.0015623678918927908\n",
      "Epoch 3222, Loss: 0.044933104094525333, Final Batch Loss: 0.00027353636687621474\n",
      "Epoch 3223, Loss: 0.01913594226789428, Final Batch Loss: 0.002305262256413698\n",
      "Epoch 3224, Loss: 0.013278483307658462, Final Batch Loss: 0.00036602115142159164\n",
      "Epoch 3225, Loss: 0.018207709112175507, Final Batch Loss: 0.00018850533524528146\n",
      "Epoch 3226, Loss: 0.034762040831992636, Final Batch Loss: 0.0016587871359661222\n",
      "Epoch 3227, Loss: 0.013028575598582393, Final Batch Loss: 0.00011378826457075775\n",
      "Epoch 3228, Loss: 0.03409340045254794, Final Batch Loss: 0.00027233996661379933\n",
      "Epoch 3229, Loss: 0.014268291888583917, Final Batch Loss: 0.00018305603589396924\n",
      "Epoch 3230, Loss: 0.013486318399372976, Final Batch Loss: 0.00016640378453303128\n",
      "Epoch 3231, Loss: 0.012318851320742397, Final Batch Loss: 0.003238369943574071\n",
      "Epoch 3232, Loss: 0.034823560459699365, Final Batch Loss: 0.0005040586111135781\n",
      "Epoch 3233, Loss: 0.0245354095222865, Final Batch Loss: 2.9984797947690822e-05\n",
      "Epoch 3234, Loss: 0.028860805854492355, Final Batch Loss: 6.2111510487739e-05\n",
      "Epoch 3235, Loss: 0.06896832265920239, Final Batch Loss: 0.004083373583853245\n",
      "Epoch 3236, Loss: 0.01247975074511487, Final Batch Loss: 0.0006378194666467607\n",
      "Epoch 3237, Loss: 0.046140238824591506, Final Batch Loss: 4.592324694385752e-05\n",
      "Epoch 3238, Loss: 0.011441975999332499, Final Batch Loss: 6.645902612945065e-05\n",
      "Epoch 3239, Loss: 0.01255636384121317, Final Batch Loss: 6.588874384760857e-05\n",
      "Epoch 3240, Loss: 0.01549506984247273, Final Batch Loss: 0.00022557596093975008\n",
      "Epoch 3241, Loss: 0.020066978580871364, Final Batch Loss: 8.076689118752256e-05\n",
      "Epoch 3242, Loss: 0.020333770122306305, Final Batch Loss: 0.004122550133615732\n",
      "Epoch 3243, Loss: 0.013164803596737329, Final Batch Loss: 0.001642091665416956\n",
      "Epoch 3244, Loss: 0.05835596819451894, Final Batch Loss: 0.019786007702350616\n",
      "Epoch 3245, Loss: 0.013479583747539436, Final Batch Loss: 0.0004836803418584168\n",
      "Epoch 3246, Loss: 0.030135442913888255, Final Batch Loss: 0.0008025828865356743\n",
      "Epoch 3247, Loss: 0.009016885567689314, Final Batch Loss: 0.00013859380851499736\n",
      "Epoch 3248, Loss: 0.014210089053449337, Final Batch Loss: 3.317357914056629e-05\n",
      "Epoch 3249, Loss: 0.021503789677808527, Final Batch Loss: 8.858381625032052e-05\n",
      "Epoch 3250, Loss: 0.005642455353154219, Final Batch Loss: 3.316393485874869e-05\n",
      "Epoch 3251, Loss: 0.029299760912181227, Final Batch Loss: 0.0028668653685599566\n",
      "Epoch 3252, Loss: 0.010506537297260365, Final Batch Loss: 0.0008188617066480219\n",
      "Epoch 3253, Loss: 0.03890753945825054, Final Batch Loss: 3.622364965849556e-05\n",
      "Epoch 3254, Loss: 0.02316510486525658, Final Batch Loss: 0.012845351360738277\n",
      "Epoch 3255, Loss: 0.01236204152519349, Final Batch Loss: 8.679281017975882e-05\n",
      "Epoch 3256, Loss: 0.03419584552102606, Final Batch Loss: 1.966419949894771e-05\n",
      "Epoch 3257, Loss: 0.03184746262195404, Final Batch Loss: 0.005098218098282814\n",
      "Epoch 3258, Loss: 0.011266833935223985, Final Batch Loss: 0.00028709141770377755\n",
      "Epoch 3259, Loss: 0.01195663541511749, Final Batch Loss: 0.004155363421887159\n",
      "Epoch 3260, Loss: 0.024316775077750208, Final Batch Loss: 5.742149005527608e-05\n",
      "Epoch 3261, Loss: 0.033964344526793866, Final Batch Loss: 0.019253015518188477\n",
      "Epoch 3262, Loss: 0.007407155319924641, Final Batch Loss: 2.0345731172710657e-05\n",
      "Epoch 3263, Loss: 0.027868360139109427, Final Batch Loss: 0.00012299077934585512\n",
      "Epoch 3264, Loss: 0.00689042077101476, Final Batch Loss: 0.0002549308119341731\n",
      "Epoch 3265, Loss: 0.018349649193623918, Final Batch Loss: 4.553322651190683e-05\n",
      "Epoch 3266, Loss: 0.036939951933163684, Final Batch Loss: 0.0010416365694254637\n",
      "Epoch 3267, Loss: 0.012716458431896172, Final Batch Loss: 0.0007655794615857303\n",
      "Epoch 3268, Loss: 0.008849175363138784, Final Batch Loss: 0.0018690989818423986\n",
      "Epoch 3269, Loss: 0.02852482312664506, Final Batch Loss: 0.000564146030228585\n",
      "Epoch 3270, Loss: 0.010152939339604927, Final Batch Loss: 7.262521830853075e-05\n",
      "Epoch 3271, Loss: 0.059756927847047336, Final Batch Loss: 0.0010264045558869839\n",
      "Epoch 3272, Loss: 0.020565589060424827, Final Batch Loss: 0.0009803198045119643\n",
      "Epoch 3273, Loss: 0.06290504365824745, Final Batch Loss: 0.0005374965257942677\n",
      "Epoch 3274, Loss: 0.020388868124427972, Final Batch Loss: 0.0001484848471591249\n",
      "Epoch 3275, Loss: 0.02655580946156988, Final Batch Loss: 0.0011824012035503983\n",
      "Epoch 3276, Loss: 0.024311262386618182, Final Batch Loss: 0.00045033832429908216\n",
      "Epoch 3277, Loss: 0.016116456703457516, Final Batch Loss: 0.001143939676694572\n",
      "Epoch 3278, Loss: 0.012169750125394785, Final Batch Loss: 0.00039645080687478185\n",
      "Epoch 3279, Loss: 0.00988510458410019, Final Batch Loss: 0.0001264885941054672\n",
      "Epoch 3280, Loss: 0.010357553577705403, Final Batch Loss: 0.0002929076144937426\n",
      "Epoch 3281, Loss: 0.029974473388392653, Final Batch Loss: 0.025532377883791924\n",
      "Epoch 3282, Loss: 0.011052863988879835, Final Batch Loss: 0.0026675881817936897\n",
      "Epoch 3283, Loss: 0.025921962166648882, Final Batch Loss: 0.004512464161962271\n",
      "Epoch 3284, Loss: 0.012743220024276525, Final Batch Loss: 0.00022608635481446981\n",
      "Epoch 3285, Loss: 0.003052561984077329, Final Batch Loss: 3.5463810490909964e-05\n",
      "Epoch 3286, Loss: 0.00873931332716893, Final Batch Loss: 2.2224983695195988e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3287, Loss: 0.0022816063901700545, Final Batch Loss: 7.434967119479552e-05\n",
      "Epoch 3288, Loss: 0.008540834160157829, Final Batch Loss: 0.0016522143268957734\n",
      "Epoch 3289, Loss: 0.010501364831725368, Final Batch Loss: 0.001611631247214973\n",
      "Epoch 3290, Loss: 0.0035531176672520814, Final Batch Loss: 6.303611007751897e-05\n",
      "Epoch 3291, Loss: 0.007100946659193141, Final Batch Loss: 0.0010582907125353813\n",
      "Epoch 3292, Loss: 0.326451156528492, Final Batch Loss: 0.008279424160718918\n",
      "Epoch 3293, Loss: 0.08224802970289602, Final Batch Loss: 0.009661298245191574\n",
      "Epoch 3294, Loss: 0.07281574039188854, Final Batch Loss: 0.029577508568763733\n",
      "Epoch 3295, Loss: 0.0442420189792756, Final Batch Loss: 0.0006364609580487013\n",
      "Epoch 3296, Loss: 0.0375594144261413, Final Batch Loss: 6.0327754908939824e-05\n",
      "Epoch 3297, Loss: 0.050203693223011214, Final Batch Loss: 0.0005490534822456539\n",
      "Epoch 3298, Loss: 0.02101240738556953, Final Batch Loss: 0.00045615845010615885\n",
      "Epoch 3299, Loss: 0.03876352451334242, Final Batch Loss: 0.000506184296682477\n",
      "Epoch 3300, Loss: 0.035691747201781254, Final Batch Loss: 0.00011366231046849862\n",
      "Epoch 3301, Loss: 0.025667039066320285, Final Batch Loss: 0.0004093758761882782\n",
      "Epoch 3302, Loss: 0.02587868972477736, Final Batch Loss: 0.006511088460683823\n",
      "Epoch 3303, Loss: 0.010311866652045865, Final Batch Loss: 6.878277054056525e-05\n",
      "Epoch 3304, Loss: 0.026591235469823005, Final Batch Loss: 4.368865484138951e-05\n",
      "Epoch 3305, Loss: 0.009942540851625381, Final Batch Loss: 0.00023189107014331967\n",
      "Epoch 3306, Loss: 0.02279315560554096, Final Batch Loss: 7.34970744815655e-05\n",
      "Epoch 3307, Loss: 0.012050307908793911, Final Batch Loss: 0.0027072844095528126\n",
      "Epoch 3308, Loss: 0.0694150902563706, Final Batch Loss: 0.0027938527055084705\n",
      "Epoch 3309, Loss: 0.007822162431693869, Final Batch Loss: 0.000707116152625531\n",
      "Epoch 3310, Loss: 0.012044929928379133, Final Batch Loss: 0.00039677254972048104\n",
      "Epoch 3311, Loss: 0.010065421622130089, Final Batch Loss: 0.0001972860482055694\n",
      "Epoch 3312, Loss: 0.00976569706472219, Final Batch Loss: 0.0028548319824039936\n",
      "Epoch 3313, Loss: 0.00723697584362526, Final Batch Loss: 0.00019646331202238798\n",
      "Epoch 3314, Loss: 0.011926763960218523, Final Batch Loss: 8.423870167462155e-05\n",
      "Epoch 3315, Loss: 0.005452363158838125, Final Batch Loss: 0.00031130557181313634\n",
      "Epoch 3316, Loss: 0.013067181651422288, Final Batch Loss: 0.00039727037074044347\n",
      "Epoch 3317, Loss: 0.014964555368351284, Final Batch Loss: 0.0006100840982981026\n",
      "Epoch 3318, Loss: 0.008938962731917854, Final Batch Loss: 2.606621455925051e-05\n",
      "Epoch 3319, Loss: 0.011559199565454037, Final Batch Loss: 0.00024329991720151156\n",
      "Epoch 3320, Loss: 0.008064430941885803, Final Batch Loss: 0.00027861204580403864\n",
      "Epoch 3321, Loss: 0.009455806146434043, Final Batch Loss: 0.0011257565347477794\n",
      "Epoch 3322, Loss: 0.049535476906385156, Final Batch Loss: 0.012108778581023216\n",
      "Epoch 3323, Loss: 0.0034594554235809483, Final Batch Loss: 0.00014992627257015556\n",
      "Epoch 3324, Loss: 0.012412639645845047, Final Batch Loss: 0.0003355294174980372\n",
      "Epoch 3325, Loss: 0.01170166110023274, Final Batch Loss: 0.002604551613330841\n",
      "Epoch 3326, Loss: 0.005160768090718193, Final Batch Loss: 0.0005040974356234074\n",
      "Epoch 3327, Loss: 0.003274299176155182, Final Batch Loss: 0.0005789058050140738\n",
      "Epoch 3328, Loss: 0.01558734167883813, Final Batch Loss: 9.815429075388238e-05\n",
      "Epoch 3329, Loss: 0.0094592844870931, Final Batch Loss: 0.0002126007020706311\n",
      "Epoch 3330, Loss: 0.033215717816347023, Final Batch Loss: 0.0026660775765776634\n",
      "Epoch 3331, Loss: 0.08063767387466214, Final Batch Loss: 0.007597557734698057\n",
      "Epoch 3332, Loss: 0.004748704595840536, Final Batch Loss: 0.0020225392654538155\n",
      "Epoch 3333, Loss: 0.017318293497737614, Final Batch Loss: 5.408195283962414e-05\n",
      "Epoch 3334, Loss: 0.018171877421082172, Final Batch Loss: 0.000551107048522681\n",
      "Epoch 3335, Loss: 0.014610347100642684, Final Batch Loss: 4.714526221505366e-05\n",
      "Epoch 3336, Loss: 0.005396041855419753, Final Batch Loss: 0.00019254579092375934\n",
      "Epoch 3337, Loss: 0.0077772764443579945, Final Batch Loss: 5.4798369092168286e-05\n",
      "Epoch 3338, Loss: 0.011655332362352055, Final Batch Loss: 0.0010606636060401797\n",
      "Epoch 3339, Loss: 0.03406639386048482, Final Batch Loss: 2.570876313257031e-05\n",
      "Epoch 3340, Loss: 0.03641531697439859, Final Batch Loss: 7.094795819284627e-06\n",
      "Epoch 3341, Loss: 0.05147635144930973, Final Batch Loss: 0.00029544183053076267\n",
      "Epoch 3342, Loss: 0.05973793014345574, Final Batch Loss: 0.011816139332950115\n",
      "Epoch 3343, Loss: 0.037798075973114464, Final Batch Loss: 0.00041035734466277063\n",
      "Epoch 3344, Loss: 0.03010579120928014, Final Batch Loss: 0.001115658669732511\n",
      "Epoch 3345, Loss: 0.09720990652385808, Final Batch Loss: 0.001166979200206697\n",
      "Epoch 3346, Loss: 0.01664089368568966, Final Batch Loss: 0.0006941769970580935\n",
      "Epoch 3347, Loss: 0.05333898735261755, Final Batch Loss: 0.001237116171978414\n",
      "Epoch 3348, Loss: 0.018007787430178723, Final Batch Loss: 0.000199061309103854\n",
      "Epoch 3349, Loss: 0.018884001563492347, Final Batch Loss: 4.5738019252894446e-05\n",
      "Epoch 3350, Loss: 0.016215789055422647, Final Batch Loss: 0.0005980293499305844\n",
      "Epoch 3351, Loss: 0.0038861941065988503, Final Batch Loss: 6.320833199424669e-05\n",
      "Epoch 3352, Loss: 0.005357635141990613, Final Batch Loss: 0.0008803833043202758\n",
      "Epoch 3353, Loss: 0.003984888575359946, Final Batch Loss: 8.504137804266065e-05\n",
      "Epoch 3354, Loss: 0.009006950859657081, Final Batch Loss: 0.0017915762728080153\n",
      "Epoch 3355, Loss: 0.053126340613744105, Final Batch Loss: 9.347421291749924e-05\n",
      "Epoch 3356, Loss: 0.012927334404594149, Final Batch Loss: 7.13848348823376e-05\n",
      "Epoch 3357, Loss: 0.05376644177158596, Final Batch Loss: 0.0006571541889570653\n",
      "Epoch 3358, Loss: 0.013402133226918522, Final Batch Loss: 0.00024602579651400447\n",
      "Epoch 3359, Loss: 0.0443882253675838, Final Batch Loss: 0.00028890129760839045\n",
      "Epoch 3360, Loss: 0.04118860028938798, Final Batch Loss: 0.0003985244547948241\n",
      "Epoch 3361, Loss: 0.02440550670144148, Final Batch Loss: 0.00020424596732482314\n",
      "Epoch 3362, Loss: 0.014656406339781824, Final Batch Loss: 4.883059227722697e-05\n",
      "Epoch 3363, Loss: 0.05966746915510157, Final Batch Loss: 0.0031561325304210186\n",
      "Epoch 3364, Loss: 0.01782490982350282, Final Batch Loss: 0.0038772020488977432\n",
      "Epoch 3365, Loss: 0.03812188266601879, Final Batch Loss: 0.001795989810489118\n",
      "Epoch 3366, Loss: 0.049635121918981895, Final Batch Loss: 0.0014169260393828154\n",
      "Epoch 3367, Loss: 0.020700920082163066, Final Batch Loss: 0.0037443065084517\n",
      "Epoch 3368, Loss: 0.01663319580256939, Final Batch Loss: 0.00018287787679582834\n",
      "Epoch 3369, Loss: 0.015532807103227242, Final Batch Loss: 0.0007708277553319931\n",
      "Epoch 3370, Loss: 0.02073964775991044, Final Batch Loss: 7.126129639800638e-05\n",
      "Epoch 3371, Loss: 0.04431085837131832, Final Batch Loss: 6.999255856499076e-05\n",
      "Epoch 3372, Loss: 0.015859635263041127, Final Batch Loss: 8.139426063280553e-05\n",
      "Epoch 3373, Loss: 0.034315532582695596, Final Batch Loss: 8.380528743145987e-05\n",
      "Epoch 3374, Loss: 0.08897658572095679, Final Batch Loss: 0.045706622302532196\n",
      "Epoch 3375, Loss: 0.08861039013572736, Final Batch Loss: 7.362973701674491e-05\n",
      "Epoch 3376, Loss: 0.034995747955690604, Final Batch Loss: 0.0016706881579011679\n",
      "Epoch 3377, Loss: 0.021231699691270478, Final Batch Loss: 0.00012512780085671693\n",
      "Epoch 3378, Loss: 0.009063067416718695, Final Batch Loss: 0.000501683505717665\n",
      "Epoch 3379, Loss: 0.006141346912045265, Final Batch Loss: 0.00016734206292312592\n",
      "Epoch 3380, Loss: 0.011696774685333367, Final Batch Loss: 4.122407335671596e-05\n",
      "Epoch 3381, Loss: 0.009872080794593785, Final Batch Loss: 0.0004516718618106097\n",
      "Epoch 3382, Loss: 0.013923056354542496, Final Batch Loss: 3.996771920355968e-05\n",
      "Epoch 3383, Loss: 0.01348388401675038, Final Batch Loss: 0.001114238053560257\n",
      "Epoch 3384, Loss: 0.004904404042463284, Final Batch Loss: 0.00043861009180545807\n",
      "Epoch 3385, Loss: 0.004708331373876717, Final Batch Loss: 0.0013798000290989876\n",
      "Epoch 3386, Loss: 0.05893223920975288, Final Batch Loss: 0.05365309119224548\n",
      "Epoch 3387, Loss: 0.008332067363880924, Final Batch Loss: 1.8650242054718547e-05\n",
      "Epoch 3388, Loss: 0.033862353579024784, Final Batch Loss: 0.000424337136792019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3389, Loss: 0.013883293540857267, Final Batch Loss: 0.0007031484856270254\n",
      "Epoch 3390, Loss: 0.03724787963801646, Final Batch Loss: 0.0006171917775645852\n",
      "Epoch 3391, Loss: 0.011180003395566018, Final Batch Loss: 0.003917062655091286\n",
      "Epoch 3392, Loss: 0.011216937957215123, Final Batch Loss: 7.075245230225846e-05\n",
      "Epoch 3393, Loss: 0.016666058294504182, Final Batch Loss: 0.00014681472384836525\n",
      "Epoch 3394, Loss: 0.010366133721618098, Final Batch Loss: 0.0013148884754627943\n",
      "Epoch 3395, Loss: 0.028100698573325644, Final Batch Loss: 0.0004629183968063444\n",
      "Epoch 3396, Loss: 0.02765077619915246, Final Batch Loss: 6.373719952534884e-05\n",
      "Epoch 3397, Loss: 0.02038474650180433, Final Batch Loss: 0.006623079534620047\n",
      "Epoch 3398, Loss: 0.013536337341065519, Final Batch Loss: 0.00095762824639678\n",
      "Epoch 3399, Loss: 0.040044444547675084, Final Batch Loss: 3.435974940657616e-05\n",
      "Epoch 3400, Loss: 0.018503297698771348, Final Batch Loss: 3.6485180316958576e-05\n",
      "Epoch 3401, Loss: 0.010576568578471779, Final Batch Loss: 0.00040082342457026243\n",
      "Epoch 3402, Loss: 0.012361452318145894, Final Batch Loss: 0.0010150086600333452\n",
      "Epoch 3403, Loss: 0.027944701245360193, Final Batch Loss: 3.786856177612208e-05\n",
      "Epoch 3404, Loss: 0.020272568170184968, Final Batch Loss: 3.5376247979002073e-05\n",
      "Epoch 3405, Loss: 0.06925719112223305, Final Batch Loss: 0.006994846276938915\n",
      "Epoch 3406, Loss: 0.06893515933779781, Final Batch Loss: 0.000277375103905797\n",
      "Epoch 3407, Loss: 0.08733932889299467, Final Batch Loss: 4.5220484025776386e-05\n",
      "Epoch 3408, Loss: 0.10107083775073988, Final Batch Loss: 0.002135738031938672\n",
      "Epoch 3409, Loss: 0.02930277894847677, Final Batch Loss: 0.004583260975778103\n",
      "Epoch 3410, Loss: 0.010367039576522075, Final Batch Loss: 0.00017154953093267977\n",
      "Epoch 3411, Loss: 0.027532498177606612, Final Batch Loss: 4.5400163799058646e-05\n",
      "Epoch 3412, Loss: 0.017844632086053025, Final Batch Loss: 9.142472845269367e-05\n",
      "Epoch 3413, Loss: 0.007052489003399387, Final Batch Loss: 0.00016812214744277298\n",
      "Epoch 3414, Loss: 0.05826050315226894, Final Batch Loss: 0.00042350689182057977\n",
      "Epoch 3415, Loss: 0.020607849965017522, Final Batch Loss: 0.0004149879387114197\n",
      "Epoch 3416, Loss: 0.019134276612021495, Final Batch Loss: 0.0007430413388647139\n",
      "Epoch 3417, Loss: 0.020287955594540108, Final Batch Loss: 0.0005033990019001067\n",
      "Epoch 3418, Loss: 0.03595462905650493, Final Batch Loss: 0.00010930447024293244\n",
      "Epoch 3419, Loss: 0.02702714470706269, Final Batch Loss: 3.3120108128059655e-05\n",
      "Epoch 3420, Loss: 0.006468151586886961, Final Batch Loss: 0.00010589271550998092\n",
      "Epoch 3421, Loss: 0.04534188703109976, Final Batch Loss: 0.0021122372709214687\n",
      "Epoch 3422, Loss: 0.03292514057829976, Final Batch Loss: 0.0001367959484923631\n",
      "Epoch 3423, Loss: 0.017355186013446655, Final Batch Loss: 0.01064587477594614\n",
      "Epoch 3424, Loss: 0.014508806983940303, Final Batch Loss: 0.0001728265779092908\n",
      "Epoch 3425, Loss: 0.0072427298946422525, Final Batch Loss: 0.004323416389524937\n",
      "Epoch 3426, Loss: 0.007544190408225404, Final Batch Loss: 0.0004437111783772707\n",
      "Epoch 3427, Loss: 0.012585794351252844, Final Batch Loss: 0.002259730128571391\n",
      "Epoch 3428, Loss: 0.006283910541242221, Final Batch Loss: 0.003984668757766485\n",
      "Epoch 3429, Loss: 0.06087628088789643, Final Batch Loss: 9.524475899524987e-05\n",
      "Epoch 3430, Loss: 0.006290206001722254, Final Batch Loss: 0.00015709480794612318\n",
      "Epoch 3431, Loss: 0.03684631405485561, Final Batch Loss: 0.0009750511380843818\n",
      "Epoch 3432, Loss: 0.033690673091768986, Final Batch Loss: 8.412735041929409e-05\n",
      "Epoch 3433, Loss: 0.020262611767975613, Final Batch Loss: 4.69472688564565e-05\n",
      "Epoch 3434, Loss: 0.004706622135927319, Final Batch Loss: 0.0015205804957076907\n",
      "Epoch 3435, Loss: 0.012104164849006338, Final Batch Loss: 7.50427134335041e-05\n",
      "Epoch 3436, Loss: 0.007015120530923014, Final Batch Loss: 0.0007882912177592516\n",
      "Epoch 3437, Loss: 0.02969979740009876, Final Batch Loss: 5.3614930948242545e-05\n",
      "Epoch 3438, Loss: 0.009673592510807794, Final Batch Loss: 0.0037332656793296337\n",
      "Epoch 3439, Loss: 0.004786844707268756, Final Batch Loss: 5.466833681566641e-05\n",
      "Epoch 3440, Loss: 0.053650094425393036, Final Batch Loss: 0.0089021697640419\n",
      "Epoch 3441, Loss: 0.029829403745679883, Final Batch Loss: 0.00028493875288404524\n",
      "Epoch 3442, Loss: 0.020181746363959974, Final Batch Loss: 0.0002789207792375237\n",
      "Epoch 3443, Loss: 0.03362511132581858, Final Batch Loss: 0.00013218603271525353\n",
      "Epoch 3444, Loss: 0.016271822671114933, Final Batch Loss: 0.004125004634261131\n",
      "Epoch 3445, Loss: 0.012101049007469555, Final Batch Loss: 0.0038805734366178513\n",
      "Epoch 3446, Loss: 0.025833952568063978, Final Batch Loss: 0.014943231828510761\n",
      "Epoch 3447, Loss: 0.024383016738283914, Final Batch Loss: 3.2565196306677535e-05\n",
      "Epoch 3448, Loss: 0.048007740020693745, Final Batch Loss: 0.0002112864312948659\n",
      "Epoch 3449, Loss: 0.061444214603397995, Final Batch Loss: 0.0008166714105755091\n",
      "Epoch 3450, Loss: 0.010423503976198845, Final Batch Loss: 0.00031416831188835204\n",
      "Epoch 3451, Loss: 0.010716911643612548, Final Batch Loss: 0.0006948161753825843\n",
      "Epoch 3452, Loss: 0.019998247791590984, Final Batch Loss: 8.512678323313594e-05\n",
      "Epoch 3453, Loss: 0.013158471858332632, Final Batch Loss: 0.00031899011810310185\n",
      "Epoch 3454, Loss: 0.014288748487160774, Final Batch Loss: 3.6130262742517516e-05\n",
      "Epoch 3455, Loss: 0.011828599039290566, Final Batch Loss: 3.3675707527436316e-05\n",
      "Epoch 3456, Loss: 0.013096744529320858, Final Batch Loss: 9.140708425547928e-05\n",
      "Epoch 3457, Loss: 0.02292508669415838, Final Batch Loss: 0.001061848015524447\n",
      "Epoch 3458, Loss: 0.02917186776539893, Final Batch Loss: 0.0005404907278716564\n",
      "Epoch 3459, Loss: 0.012605599476955831, Final Batch Loss: 0.000205659176572226\n",
      "Epoch 3460, Loss: 0.02403524184410344, Final Batch Loss: 0.00035060025402344763\n",
      "Epoch 3461, Loss: 0.03669332699610095, Final Batch Loss: 0.00014868877769913524\n",
      "Epoch 3462, Loss: 0.06880092560459161, Final Batch Loss: 5.755124584538862e-05\n",
      "Epoch 3463, Loss: 0.07076572981168283, Final Batch Loss: 0.008244850672781467\n",
      "Epoch 3464, Loss: 0.04739007974785636, Final Batch Loss: 0.0007456129533238709\n",
      "Epoch 3465, Loss: 0.12880425577168353, Final Batch Loss: 0.0012466962216421962\n",
      "Epoch 3466, Loss: 0.036309556555352174, Final Batch Loss: 0.0017617428675293922\n",
      "Epoch 3467, Loss: 0.11659807317482773, Final Batch Loss: 0.002204174641519785\n",
      "Epoch 3468, Loss: 0.03723426159922383, Final Batch Loss: 0.0031303020659834146\n",
      "Epoch 3469, Loss: 0.04384662747179391, Final Batch Loss: 0.008844977244734764\n",
      "Epoch 3470, Loss: 0.04718526501528686, Final Batch Loss: 0.01183563843369484\n",
      "Epoch 3471, Loss: 0.0355036423788988, Final Batch Loss: 0.0002153276582248509\n",
      "Epoch 3472, Loss: 0.013950630127510522, Final Batch Loss: 0.0001487171684857458\n",
      "Epoch 3473, Loss: 0.030376556747796712, Final Batch Loss: 7.26348880562e-05\n",
      "Epoch 3474, Loss: 0.007967714962433092, Final Batch Loss: 0.000760091410484165\n",
      "Epoch 3475, Loss: 0.013238926767371595, Final Batch Loss: 0.00020285436767153442\n",
      "Epoch 3476, Loss: 0.009835467524681007, Final Batch Loss: 0.00031603925162926316\n",
      "Epoch 3477, Loss: 0.014241307972042705, Final Batch Loss: 0.00037705033901147544\n",
      "Epoch 3478, Loss: 0.01278759798151441, Final Batch Loss: 0.0017103510908782482\n",
      "Epoch 3479, Loss: 0.005380137412430486, Final Batch Loss: 5.48599346075207e-05\n",
      "Epoch 3480, Loss: 0.013832722250299412, Final Batch Loss: 0.0001457374746678397\n",
      "Epoch 3481, Loss: 0.03259178869666357, Final Batch Loss: 0.0001079921712516807\n",
      "Epoch 3482, Loss: 0.012354003090877086, Final Batch Loss: 0.005710154306143522\n",
      "Epoch 3483, Loss: 0.041927835929527646, Final Batch Loss: 0.006771265063434839\n",
      "Epoch 3484, Loss: 0.12174579516067752, Final Batch Loss: 0.003051036735996604\n",
      "Epoch 3485, Loss: 0.16189035074785352, Final Batch Loss: 0.0015169269172474742\n",
      "Epoch 3486, Loss: 0.1243079887208296, Final Batch Loss: 0.02359013818204403\n",
      "Epoch 3487, Loss: 0.0893659653593204, Final Batch Loss: 0.007253097835928202\n",
      "Epoch 3488, Loss: 0.04190911703335587, Final Batch Loss: 0.0028699517715722322\n",
      "Epoch 3489, Loss: 0.02960307608009316, Final Batch Loss: 0.0005427188007161021\n",
      "Epoch 3490, Loss: 0.015297791120246984, Final Batch Loss: 0.00031488071545027196\n",
      "Epoch 3491, Loss: 0.0318445820157649, Final Batch Loss: 0.0002002072287723422\n",
      "Epoch 3492, Loss: 0.023633390992472414, Final Batch Loss: 6.366255547618493e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3493, Loss: 0.011057365127271623, Final Batch Loss: 0.00024592591216787696\n",
      "Epoch 3494, Loss: 0.015045550604554592, Final Batch Loss: 0.0004948188434354961\n",
      "Epoch 3495, Loss: 0.01183243920240784, Final Batch Loss: 9.539509483147413e-05\n",
      "Epoch 3496, Loss: 0.032997231846820796, Final Batch Loss: 0.0024597106967121363\n",
      "Epoch 3497, Loss: 0.004693796570791164, Final Batch Loss: 9.895175026031211e-06\n",
      "Epoch 3498, Loss: 0.008243610584031558, Final Batch Loss: 0.0009141450864262879\n",
      "Epoch 3499, Loss: 0.016522292861736787, Final Batch Loss: 0.0005887639126740396\n",
      "Epoch 3500, Loss: 0.008178836711522308, Final Batch Loss: 0.00018363968410994858\n",
      "Epoch 3501, Loss: 0.006012960486259544, Final Batch Loss: 0.0006949188536964357\n",
      "Epoch 3502, Loss: 0.0057996075406663294, Final Batch Loss: 0.00018696257029660046\n",
      "Epoch 3503, Loss: 0.007752824954877724, Final Batch Loss: 0.0016065968666225672\n",
      "Epoch 3504, Loss: 0.007462661359568301, Final Batch Loss: 0.0011687534861266613\n",
      "Epoch 3505, Loss: 0.09578004944887653, Final Batch Loss: 0.008995454758405685\n",
      "Epoch 3506, Loss: 0.07936113278083212, Final Batch Loss: 0.000812426267657429\n",
      "Epoch 3507, Loss: 0.03156363664311357, Final Batch Loss: 0.0001605144061613828\n",
      "Epoch 3508, Loss: 0.053222637812723406, Final Batch Loss: 6.813300569774583e-05\n",
      "Epoch 3509, Loss: 0.04450308321065677, Final Batch Loss: 8.883803093340248e-05\n",
      "Epoch 3510, Loss: 0.03845650337461848, Final Batch Loss: 0.008002257905900478\n",
      "Epoch 3511, Loss: 0.017918067627761047, Final Batch Loss: 0.008712062612175941\n",
      "Epoch 3512, Loss: 0.019814045695966342, Final Batch Loss: 5.754049925599247e-05\n",
      "Epoch 3513, Loss: 0.016361964764655568, Final Batch Loss: 6.730638415319845e-05\n",
      "Epoch 3514, Loss: 0.006798193589929724, Final Batch Loss: 0.0003364247386343777\n",
      "Epoch 3515, Loss: 0.01815026271469833, Final Batch Loss: 0.00404449412599206\n",
      "Epoch 3516, Loss: 0.006597981902814354, Final Batch Loss: 3.78913500753697e-05\n",
      "Epoch 3517, Loss: 0.00743065193091752, Final Batch Loss: 3.496000135783106e-05\n",
      "Epoch 3518, Loss: 0.010723424678872107, Final Batch Loss: 0.005764540284872055\n",
      "Epoch 3519, Loss: 0.012952930655956152, Final Batch Loss: 0.0005896114162169397\n",
      "Epoch 3520, Loss: 0.009229245171809453, Final Batch Loss: 0.00023703949409537017\n",
      "Epoch 3521, Loss: 0.004615269024725421, Final Batch Loss: 0.0005112538347020745\n",
      "Epoch 3522, Loss: 0.02542810217710212, Final Batch Loss: 0.00017744234355632216\n",
      "Epoch 3523, Loss: 0.01211146014429687, Final Batch Loss: 0.0006025493494234979\n",
      "Epoch 3524, Loss: 0.010993779562340933, Final Batch Loss: 0.00024635231238789856\n",
      "Epoch 3525, Loss: 0.0032192867456615204, Final Batch Loss: 0.0006764896679669619\n",
      "Epoch 3526, Loss: 0.026640775031410158, Final Batch Loss: 0.0016081976937130094\n",
      "Epoch 3527, Loss: 0.018709599233261542, Final Batch Loss: 0.00015192294085863978\n",
      "Epoch 3528, Loss: 0.026041125052870484, Final Batch Loss: 4.159594755037688e-05\n",
      "Epoch 3529, Loss: 0.013499024119482783, Final Batch Loss: 0.000130522035760805\n",
      "Epoch 3530, Loss: 0.004321749822338461, Final Batch Loss: 8.587312186136842e-05\n",
      "Epoch 3531, Loss: 0.02930632933293964, Final Batch Loss: 0.00021153675334062427\n",
      "Epoch 3532, Loss: 0.01125523702648934, Final Batch Loss: 4.787087527802214e-05\n",
      "Epoch 3533, Loss: 0.014376317353708146, Final Batch Loss: 1.0863933312066365e-05\n",
      "Epoch 3534, Loss: 0.014225340053599211, Final Batch Loss: 0.0003708890581037849\n",
      "Epoch 3535, Loss: 0.006337009974231478, Final Batch Loss: 7.627838203916326e-05\n",
      "Epoch 3536, Loss: 0.005940656885286444, Final Batch Loss: 2.6138928660657257e-05\n",
      "Epoch 3537, Loss: 0.018637618066350115, Final Batch Loss: 0.0022155081387609243\n",
      "Epoch 3538, Loss: 0.010223422019180362, Final Batch Loss: 4.4957720092497766e-05\n",
      "Epoch 3539, Loss: 0.0146999372927894, Final Batch Loss: 0.00987945031374693\n",
      "Epoch 3540, Loss: 0.016015444415643287, Final Batch Loss: 3.258829747210257e-05\n",
      "Epoch 3541, Loss: 0.05192413742042845, Final Batch Loss: 0.009274021722376347\n",
      "Epoch 3542, Loss: 0.013290187602251535, Final Batch Loss: 6.841793219791725e-05\n",
      "Epoch 3543, Loss: 0.007958902206155472, Final Batch Loss: 0.00010422670311527327\n",
      "Epoch 3544, Loss: 0.02188449517416302, Final Batch Loss: 0.004584121517837048\n",
      "Epoch 3545, Loss: 0.013934788710685098, Final Batch Loss: 0.0014618654968217015\n",
      "Epoch 3546, Loss: 0.10380294537753798, Final Batch Loss: 0.000546008872333914\n",
      "Epoch 3547, Loss: 0.12125934001232963, Final Batch Loss: 0.0011805626563727856\n",
      "Epoch 3548, Loss: 0.05562238971469924, Final Batch Loss: 0.0027870233170688152\n",
      "Epoch 3549, Loss: 0.024117984054100816, Final Batch Loss: 2.327515721844975e-05\n",
      "Epoch 3550, Loss: 0.058020449461764656, Final Batch Loss: 0.004197052214294672\n",
      "Epoch 3551, Loss: 0.01600228474853793, Final Batch Loss: 0.00122408929746598\n",
      "Epoch 3552, Loss: 0.03225465170544339, Final Batch Loss: 0.0001184482971439138\n",
      "Epoch 3553, Loss: 0.03852569550508633, Final Batch Loss: 0.00012502289609983563\n",
      "Epoch 3554, Loss: 0.011208706273464486, Final Batch Loss: 0.0011876319767907262\n",
      "Epoch 3555, Loss: 0.041452222314546816, Final Batch Loss: 0.0011771220015361905\n",
      "Epoch 3556, Loss: 0.01236833812436089, Final Batch Loss: 0.0017067191656678915\n",
      "Epoch 3557, Loss: 0.03163898469210835, Final Batch Loss: 0.00019426070502959192\n",
      "Epoch 3558, Loss: 0.019917117191653233, Final Batch Loss: 0.0007155865314416587\n",
      "Epoch 3559, Loss: 0.007390992390355677, Final Batch Loss: 0.0013855097349733114\n",
      "Epoch 3560, Loss: 0.017495321902970318, Final Batch Loss: 7.994061888894066e-05\n",
      "Epoch 3561, Loss: 0.02687656885245815, Final Batch Loss: 0.002752740867435932\n",
      "Epoch 3562, Loss: 0.014083325891988352, Final Batch Loss: 0.0006530455430038273\n",
      "Epoch 3563, Loss: 0.02065933800622588, Final Batch Loss: 0.0001131385870394297\n",
      "Epoch 3564, Loss: 0.020737525206641294, Final Batch Loss: 0.0022090706042945385\n",
      "Epoch 3565, Loss: 0.030967623574724712, Final Batch Loss: 0.0016257959650829434\n",
      "Epoch 3566, Loss: 0.015599389764247462, Final Batch Loss: 0.0024030187632888556\n",
      "Epoch 3567, Loss: 0.016146361391292885, Final Batch Loss: 0.00021682078659068793\n",
      "Epoch 3568, Loss: 0.021524332478293218, Final Batch Loss: 7.124757394194603e-05\n",
      "Epoch 3569, Loss: 0.004993529997591395, Final Batch Loss: 8.990886271931231e-05\n",
      "Epoch 3570, Loss: 0.020528223536530277, Final Batch Loss: 0.0010741588193923235\n",
      "Epoch 3571, Loss: 0.011010168469510972, Final Batch Loss: 0.00019394682021811604\n",
      "Epoch 3572, Loss: 0.014080425528845808, Final Batch Loss: 0.0011667710496112704\n",
      "Epoch 3573, Loss: 0.012141208624598221, Final Batch Loss: 2.443834273435641e-05\n",
      "Epoch 3574, Loss: 0.011172104685101658, Final Batch Loss: 1.978691761905793e-05\n",
      "Epoch 3575, Loss: 0.03800283815871808, Final Batch Loss: 0.00016040567425079644\n",
      "Epoch 3576, Loss: 0.013000741168070817, Final Batch Loss: 5.3157487855060026e-05\n",
      "Epoch 3577, Loss: 0.035464789510115224, Final Batch Loss: 0.0006156493909657001\n",
      "Epoch 3578, Loss: 0.003081585460677161, Final Batch Loss: 0.00029947940493002534\n",
      "Epoch 3579, Loss: 0.008315208600834012, Final Batch Loss: 0.0011679966701194644\n",
      "Epoch 3580, Loss: 0.01655935882445192, Final Batch Loss: 0.00011163658200530335\n",
      "Epoch 3581, Loss: 0.007064271153467416, Final Batch Loss: 0.0013980920193716884\n",
      "Epoch 3582, Loss: 0.005927566421632946, Final Batch Loss: 0.002251533791422844\n",
      "Epoch 3583, Loss: 0.0037384019560704473, Final Batch Loss: 0.0002651963150128722\n",
      "Epoch 3584, Loss: 0.0031977334419934778, Final Batch Loss: 6.508653314085677e-05\n",
      "Epoch 3585, Loss: 0.013232592356871464, Final Batch Loss: 0.00021053610544186085\n",
      "Epoch 3586, Loss: 0.039763992243024404, Final Batch Loss: 0.005471175070852041\n",
      "Epoch 3587, Loss: 0.010184328593823011, Final Batch Loss: 5.717317253584042e-05\n",
      "Epoch 3588, Loss: 0.021044752034867997, Final Batch Loss: 0.0005809920839965343\n",
      "Epoch 3589, Loss: 0.029071757537167287, Final Batch Loss: 8.547618926968426e-05\n",
      "Epoch 3590, Loss: 0.008711711452633608, Final Batch Loss: 3.235137046431191e-05\n",
      "Epoch 3591, Loss: 0.014124219183031528, Final Batch Loss: 5.288775355438702e-05\n",
      "Epoch 3592, Loss: 0.00968714840200846, Final Batch Loss: 0.0002551288635004312\n",
      "Epoch 3593, Loss: 0.01703019799606409, Final Batch Loss: 9.121391485678032e-05\n",
      "Epoch 3594, Loss: 0.009928291035976144, Final Batch Loss: 1.1731312042684294e-05\n",
      "Epoch 3595, Loss: 0.005049238950050494, Final Batch Loss: 1.3048297660134267e-05\n",
      "Epoch 3596, Loss: 0.011959392841163208, Final Batch Loss: 5.7207744248444214e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3597, Loss: 0.0626940722031577, Final Batch Loss: 2.8519965781015344e-05\n",
      "Epoch 3598, Loss: 0.044805295667174505, Final Batch Loss: 0.00027959683211520314\n",
      "Epoch 3599, Loss: 0.00667554857864161, Final Batch Loss: 0.0004025875823572278\n",
      "Epoch 3600, Loss: 0.005672612172929803, Final Batch Loss: 0.0008965586312115192\n",
      "Epoch 3601, Loss: 0.008577358188631479, Final Batch Loss: 0.000887300877366215\n",
      "Epoch 3602, Loss: 0.028178012818898424, Final Batch Loss: 0.00035076815402135253\n",
      "Epoch 3603, Loss: 0.010026609686974552, Final Batch Loss: 0.0008552975486963987\n",
      "Epoch 3604, Loss: 0.006469546354310296, Final Batch Loss: 2.288508949277457e-05\n",
      "Epoch 3605, Loss: 0.0072272253783012275, Final Batch Loss: 0.0001390579855069518\n",
      "Epoch 3606, Loss: 0.0028606962396224844, Final Batch Loss: 3.163412111462094e-05\n",
      "Epoch 3607, Loss: 0.020120465545915067, Final Batch Loss: 6.717768701491877e-05\n",
      "Epoch 3608, Loss: 0.015436042358487612, Final Batch Loss: 0.008576400578022003\n",
      "Epoch 3609, Loss: 0.026072947648572153, Final Batch Loss: 0.0012210223358124495\n",
      "Epoch 3610, Loss: 0.02526004269020632, Final Batch Loss: 0.0002458106027916074\n",
      "Epoch 3611, Loss: 0.0498291791700467, Final Batch Loss: 3.501391984173097e-05\n",
      "Epoch 3612, Loss: 0.041678459603645024, Final Batch Loss: 0.00013551191659644246\n",
      "Epoch 3613, Loss: 0.011447086315456545, Final Batch Loss: 0.0004419020551722497\n",
      "Epoch 3614, Loss: 0.009218201521434821, Final Batch Loss: 7.60695620556362e-05\n",
      "Epoch 3615, Loss: 0.010382783259046846, Final Batch Loss: 2.1439615011331625e-05\n",
      "Epoch 3616, Loss: 0.0021029269410064444, Final Batch Loss: 0.00023510849860031158\n",
      "Epoch 3617, Loss: 0.012795576286407595, Final Batch Loss: 0.0001093177343136631\n",
      "Epoch 3618, Loss: 0.002996276669364306, Final Batch Loss: 6.078955266275443e-05\n",
      "Epoch 3619, Loss: 0.024112210521252564, Final Batch Loss: 0.009380003437399864\n",
      "Epoch 3620, Loss: 0.010234500099613797, Final Batch Loss: 0.0007943164673633873\n",
      "Epoch 3621, Loss: 0.01126612370080693, Final Batch Loss: 0.0015675561735406518\n",
      "Epoch 3622, Loss: 0.007163486514400574, Final Batch Loss: 2.5146362531813793e-05\n",
      "Epoch 3623, Loss: 0.020216865463225986, Final Batch Loss: 0.00016990331641864032\n",
      "Epoch 3624, Loss: 0.007386150213278597, Final Batch Loss: 0.0006597162573598325\n",
      "Epoch 3625, Loss: 0.035900603546906495, Final Batch Loss: 9.860648424364626e-05\n",
      "Epoch 3626, Loss: 0.010211947868810967, Final Batch Loss: 0.00012246060941834003\n",
      "Epoch 3627, Loss: 0.07138463621595292, Final Batch Loss: 0.00016301289724651724\n",
      "Epoch 3628, Loss: 0.03197050681410474, Final Batch Loss: 5.634640183416195e-05\n",
      "Epoch 3629, Loss: 0.02847071832366055, Final Batch Loss: 8.280814654426649e-05\n",
      "Epoch 3630, Loss: 0.05103450591195724, Final Batch Loss: 0.02433607541024685\n",
      "Epoch 3631, Loss: 0.0068129305545880925, Final Batch Loss: 0.00018360618560109288\n",
      "Epoch 3632, Loss: 0.01503543905710103, Final Batch Loss: 0.00018798193195834756\n",
      "Epoch 3633, Loss: 0.004543126184216817, Final Batch Loss: 0.00018765007553156465\n",
      "Epoch 3634, Loss: 0.015456373172128224, Final Batch Loss: 0.00011050937609979883\n",
      "Epoch 3635, Loss: 0.009664548137152451, Final Batch Loss: 0.0009293715702369809\n",
      "Epoch 3636, Loss: 0.005467277880597976, Final Batch Loss: 1.6301904906868003e-05\n",
      "Epoch 3637, Loss: 0.006031106477166759, Final Batch Loss: 7.730754441581666e-05\n",
      "Epoch 3638, Loss: 0.005684040641426691, Final Batch Loss: 2.075957672786899e-05\n",
      "Epoch 3639, Loss: 0.009988541303755483, Final Batch Loss: 2.0999279513489455e-05\n",
      "Epoch 3640, Loss: 0.013952217611858941, Final Batch Loss: 3.892248423653655e-05\n",
      "Epoch 3641, Loss: 0.00788467948223115, Final Batch Loss: 0.00020165696332696825\n",
      "Epoch 3642, Loss: 0.0014981435369918472, Final Batch Loss: 1.0816452231665608e-05\n",
      "Epoch 3643, Loss: 0.002400458831289143, Final Batch Loss: 0.0002626933273859322\n",
      "Epoch 3644, Loss: 0.001121979617892066, Final Batch Loss: 1.0546553312451579e-05\n",
      "Epoch 3645, Loss: 0.0020391975403981633, Final Batch Loss: 2.93677621812094e-05\n",
      "Epoch 3646, Loss: 0.0012246691721884417, Final Batch Loss: 5.692522972822189e-05\n",
      "Epoch 3647, Loss: 0.014270863897763775, Final Batch Loss: 0.00024282715457957238\n",
      "Epoch 3648, Loss: 0.09787296344484275, Final Batch Loss: 2.3937086552905384e-06\n",
      "Epoch 3649, Loss: 0.04195752052100943, Final Batch Loss: 8.455038187094033e-05\n",
      "Epoch 3650, Loss: 0.08236745310750848, Final Batch Loss: 0.02227633073925972\n",
      "Epoch 3651, Loss: 0.07768711545941187, Final Batch Loss: 0.039788078516721725\n",
      "Epoch 3652, Loss: 0.08926166356104659, Final Batch Loss: 0.00033305527176707983\n",
      "Epoch 3653, Loss: 0.021226098251645453, Final Batch Loss: 0.00021300456137396395\n",
      "Epoch 3654, Loss: 0.014546598482411355, Final Batch Loss: 0.0005731653072871268\n",
      "Epoch 3655, Loss: 0.03293869445042219, Final Batch Loss: 0.00012267858255654573\n",
      "Epoch 3656, Loss: 0.024719059128983645, Final Batch Loss: 0.00011846199777210131\n",
      "Epoch 3657, Loss: 0.014321420909254812, Final Batch Loss: 0.00025396523415111005\n",
      "Epoch 3658, Loss: 0.010694110347685637, Final Batch Loss: 0.00010692610521800816\n",
      "Epoch 3659, Loss: 0.003151255057673552, Final Batch Loss: 0.000115713439299725\n",
      "Epoch 3660, Loss: 0.018835562151252816, Final Batch Loss: 0.0001250509376404807\n",
      "Epoch 3661, Loss: 0.02286162298241834, Final Batch Loss: 3.438011117395945e-05\n",
      "Epoch 3662, Loss: 0.035658560154843144, Final Batch Loss: 0.008549302816390991\n",
      "Epoch 3663, Loss: 0.08969693598919548, Final Batch Loss: 0.00011650384840322658\n",
      "Epoch 3664, Loss: 0.04251793623552658, Final Batch Loss: 0.0013376440620049834\n",
      "Epoch 3665, Loss: 0.09053325746572227, Final Batch Loss: 0.0006465062615461648\n",
      "Epoch 3666, Loss: 0.023290072556847008, Final Batch Loss: 0.007463434711098671\n",
      "Epoch 3667, Loss: 0.008851894324834575, Final Batch Loss: 2.726116326812189e-05\n",
      "Epoch 3668, Loss: 0.01829543632629793, Final Batch Loss: 4.593245830619708e-05\n",
      "Epoch 3669, Loss: 0.03695622042141622, Final Batch Loss: 0.00017529455362819135\n",
      "Epoch 3670, Loss: 0.0481134647707222, Final Batch Loss: 0.0001387319789500907\n",
      "Epoch 3671, Loss: 0.020364305398288707, Final Batch Loss: 0.00042929279152303934\n",
      "Epoch 3672, Loss: 0.032412603173725074, Final Batch Loss: 0.00010185674182139337\n",
      "Epoch 3673, Loss: 0.018963901904498925, Final Batch Loss: 0.00025006983196362853\n",
      "Epoch 3674, Loss: 0.029408748538116924, Final Batch Loss: 0.019839975982904434\n",
      "Epoch 3675, Loss: 0.048044809602288296, Final Batch Loss: 5.4266824008664116e-05\n",
      "Epoch 3676, Loss: 0.029731605854976806, Final Batch Loss: 0.0003408755292184651\n",
      "Epoch 3677, Loss: 0.06353012486943044, Final Batch Loss: 5.130606587044895e-05\n",
      "Epoch 3678, Loss: 0.05842742067761719, Final Batch Loss: 0.0006508241640403867\n",
      "Epoch 3679, Loss: 0.03737984197505284, Final Batch Loss: 0.00018365001596976072\n",
      "Epoch 3680, Loss: 0.05111468883114867, Final Batch Loss: 0.008239852264523506\n",
      "Epoch 3681, Loss: 0.00523253816209035, Final Batch Loss: 0.0005137943662703037\n",
      "Epoch 3682, Loss: 0.00816880233833217, Final Batch Loss: 8.062100096140057e-05\n",
      "Epoch 3683, Loss: 0.010301327107299585, Final Batch Loss: 0.00013258392573334277\n",
      "Epoch 3684, Loss: 0.015622633745806525, Final Batch Loss: 0.00017847874551080167\n",
      "Epoch 3685, Loss: 0.033190212736371905, Final Batch Loss: 8.919691026676446e-05\n",
      "Epoch 3686, Loss: 0.01733442139811814, Final Batch Loss: 9.247178968507797e-05\n",
      "Epoch 3687, Loss: 0.016922860086197034, Final Batch Loss: 0.0074196429923176765\n",
      "Epoch 3688, Loss: 0.02271270050914609, Final Batch Loss: 9.472107922192663e-05\n",
      "Epoch 3689, Loss: 0.04673279105554684, Final Batch Loss: 0.0019478442845866084\n",
      "Epoch 3690, Loss: 0.023559774130262667, Final Batch Loss: 0.0028315214440226555\n",
      "Epoch 3691, Loss: 0.021635571605656878, Final Batch Loss: 1.7476262655691244e-05\n",
      "Epoch 3692, Loss: 0.017552992081618868, Final Batch Loss: 0.0040778121910989285\n",
      "Epoch 3693, Loss: 0.013190958085033344, Final Batch Loss: 1.9521812646416947e-05\n",
      "Epoch 3694, Loss: 0.029540161367549445, Final Batch Loss: 0.002555407118052244\n",
      "Epoch 3695, Loss: 0.02971521226936602, Final Batch Loss: 0.0002667670196387917\n",
      "Epoch 3696, Loss: 0.00905681273980008, Final Batch Loss: 5.970946222078055e-05\n",
      "Epoch 3697, Loss: 0.010516829434891406, Final Batch Loss: 9.53889757511206e-05\n",
      "Epoch 3698, Loss: 0.012387503615627793, Final Batch Loss: 0.003587846178561449\n",
      "Epoch 3699, Loss: 0.005217161813561688, Final Batch Loss: 3.6190594983054325e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3700, Loss: 0.007692186463827966, Final Batch Loss: 0.00019120103388559073\n",
      "Epoch 3701, Loss: 0.015990473561942053, Final Batch Loss: 0.0006096221623010933\n",
      "Epoch 3702, Loss: 0.018296774200280197, Final Batch Loss: 6.463802128564566e-05\n",
      "Epoch 3703, Loss: 0.047563587464537704, Final Batch Loss: 2.808192584780045e-05\n",
      "Epoch 3704, Loss: 0.03720286190036859, Final Batch Loss: 0.00016629553283564746\n",
      "Epoch 3705, Loss: 0.028803786475691595, Final Batch Loss: 0.00033625285141170025\n",
      "Epoch 3706, Loss: 0.02563720960642968, Final Batch Loss: 0.00010113183088833466\n",
      "Epoch 3707, Loss: 0.016167284572475182, Final Batch Loss: 0.010272713378071785\n",
      "Epoch 3708, Loss: 0.008150132969603874, Final Batch Loss: 0.00013009402027819306\n",
      "Epoch 3709, Loss: 0.004507442215981428, Final Batch Loss: 1.774226075212937e-05\n",
      "Epoch 3710, Loss: 0.00864365329289285, Final Batch Loss: 4.852877827943303e-05\n",
      "Epoch 3711, Loss: 0.017018656248183106, Final Batch Loss: 5.828654320794158e-05\n",
      "Epoch 3712, Loss: 0.024858412965841126, Final Batch Loss: 4.317859566072002e-05\n",
      "Epoch 3713, Loss: 0.004580457036354346, Final Batch Loss: 0.0001525336701888591\n",
      "Epoch 3714, Loss: 0.03052503908475046, Final Batch Loss: 0.017204299569129944\n",
      "Epoch 3715, Loss: 0.03067718114652962, Final Batch Loss: 0.0014324260409921408\n",
      "Epoch 3716, Loss: 0.03546652422301122, Final Batch Loss: 9.082056203624234e-05\n",
      "Epoch 3717, Loss: 0.02837759308567911, Final Batch Loss: 0.00017741489864420146\n",
      "Epoch 3718, Loss: 0.012275468550797086, Final Batch Loss: 0.001418198342435062\n",
      "Epoch 3719, Loss: 0.032551050706388196, Final Batch Loss: 0.0028304627630859613\n",
      "Epoch 3720, Loss: 0.010378373292041942, Final Batch Loss: 4.348106085672043e-05\n",
      "Epoch 3721, Loss: 0.05316574494872839, Final Batch Loss: 0.024526583030819893\n",
      "Epoch 3722, Loss: 0.031182558326690923, Final Batch Loss: 0.0012803033459931612\n",
      "Epoch 3723, Loss: 0.03002218826804892, Final Batch Loss: 3.057955109397881e-05\n",
      "Epoch 3724, Loss: 0.02228294465749059, Final Batch Loss: 0.0002538455300964415\n",
      "Epoch 3725, Loss: 0.049592343810218154, Final Batch Loss: 0.0005700881592929363\n",
      "Epoch 3726, Loss: 0.023193990949948784, Final Batch Loss: 0.00022766832262277603\n",
      "Epoch 3727, Loss: 0.014875356457196176, Final Batch Loss: 4.50554107374046e-05\n",
      "Epoch 3728, Loss: 0.0231061553768086, Final Batch Loss: 0.0019456532318145037\n",
      "Epoch 3729, Loss: 0.011632099133748852, Final Batch Loss: 0.0006083321641199291\n",
      "Epoch 3730, Loss: 0.009891130754112964, Final Batch Loss: 0.0001640550617594272\n",
      "Epoch 3731, Loss: 0.005167964493921318, Final Batch Loss: 0.0018028023187071085\n",
      "Epoch 3732, Loss: 0.0025745120183273684, Final Batch Loss: 1.7685368220554665e-05\n",
      "Epoch 3733, Loss: 0.027434606347924273, Final Batch Loss: 1.2155168406025041e-05\n",
      "Epoch 3734, Loss: 0.01651877072708885, Final Batch Loss: 0.008973270654678345\n",
      "Epoch 3735, Loss: 0.007635628866410116, Final Batch Loss: 4.793255720869638e-05\n",
      "Epoch 3736, Loss: 0.035975897264506784, Final Batch Loss: 0.00012639164924621582\n",
      "Epoch 3737, Loss: 0.010084084179197816, Final Batch Loss: 3.370208287378773e-05\n",
      "Epoch 3738, Loss: 0.0024114729785651434, Final Batch Loss: 0.0001009386105579324\n",
      "Epoch 3739, Loss: 0.005849331350873399, Final Batch Loss: 1.9463222997728735e-05\n",
      "Epoch 3740, Loss: 0.012367881986392604, Final Batch Loss: 1.5022723346191924e-05\n",
      "Epoch 3741, Loss: 0.01572077695163898, Final Batch Loss: 0.000817000400274992\n",
      "Epoch 3742, Loss: 0.04769213168765418, Final Batch Loss: 0.0003196979814674705\n",
      "Epoch 3743, Loss: 0.012885688147434848, Final Batch Loss: 6.256921187741682e-05\n",
      "Epoch 3744, Loss: 0.005017113688154495, Final Batch Loss: 0.00016332439554389566\n",
      "Epoch 3745, Loss: 0.0158940634883038, Final Batch Loss: 5.050442632636987e-05\n",
      "Epoch 3746, Loss: 0.019435009236985934, Final Batch Loss: 7.100241055013612e-05\n",
      "Epoch 3747, Loss: 0.005590982942521805, Final Batch Loss: 0.00026915871421806514\n",
      "Epoch 3748, Loss: 0.00717829826498928, Final Batch Loss: 3.577415191102773e-05\n",
      "Epoch 3749, Loss: 0.02469891482815001, Final Batch Loss: 0.00012163527571829036\n",
      "Epoch 3750, Loss: 0.005286144937599602, Final Batch Loss: 0.00015104511112440377\n",
      "Epoch 3751, Loss: 0.006389368339114299, Final Batch Loss: 5.369808059185743e-05\n",
      "Epoch 3752, Loss: 0.012617690495517309, Final Batch Loss: 0.0009057415882125497\n",
      "Epoch 3753, Loss: 0.008799717361398507, Final Batch Loss: 5.065357981948182e-05\n",
      "Epoch 3754, Loss: 0.006624177687626798, Final Batch Loss: 0.0003517431323416531\n",
      "Epoch 3755, Loss: 0.005542361330071799, Final Batch Loss: 1.049596266966546e-05\n",
      "Epoch 3756, Loss: 0.0027706547843990847, Final Batch Loss: 5.192892422201112e-05\n",
      "Epoch 3757, Loss: 0.0026333435409924277, Final Batch Loss: 0.000335744145559147\n",
      "Epoch 3758, Loss: 0.011997481209618854, Final Batch Loss: 5.668916492140852e-05\n",
      "Epoch 3759, Loss: 0.007814672744643758, Final Batch Loss: 0.00015817346866242588\n",
      "Epoch 3760, Loss: 0.006431626075936947, Final Batch Loss: 0.00015305109263863415\n",
      "Epoch 3761, Loss: 0.0020584115736710373, Final Batch Loss: 0.00012620164488907903\n",
      "Epoch 3762, Loss: 0.04329610150671215, Final Batch Loss: 8.424259431194514e-05\n",
      "Epoch 3763, Loss: 0.07763619199613458, Final Batch Loss: 0.0046397615224123\n",
      "Epoch 3764, Loss: 0.020354522948764497, Final Batch Loss: 0.00108135596383363\n",
      "Epoch 3765, Loss: 0.018194125970694586, Final Batch Loss: 0.00019453864661045372\n",
      "Epoch 3766, Loss: 0.006549929403263377, Final Batch Loss: 5.7941659179050475e-05\n",
      "Epoch 3767, Loss: 0.020218175814079586, Final Batch Loss: 0.00411345437169075\n",
      "Epoch 3768, Loss: 0.028460984009598178, Final Batch Loss: 2.100211349898018e-05\n",
      "Epoch 3769, Loss: 0.01515157519497734, Final Batch Loss: 0.0007682282011955976\n",
      "Epoch 3770, Loss: 0.006423064554837765, Final Batch Loss: 0.0011788130505010486\n",
      "Epoch 3771, Loss: 0.00491499913186999, Final Batch Loss: 0.00026037360657937825\n",
      "Epoch 3772, Loss: 0.012148174286267022, Final Batch Loss: 0.0010199137032032013\n",
      "Epoch 3773, Loss: 0.02353984335604764, Final Batch Loss: 1.72970012499718e-05\n",
      "Epoch 3774, Loss: 0.022290608382718347, Final Batch Loss: 0.0015689053107053041\n",
      "Epoch 3775, Loss: 0.036038072601513704, Final Batch Loss: 6.697586650261655e-05\n",
      "Epoch 3776, Loss: 0.017066949278159882, Final Batch Loss: 0.00022359441209118813\n",
      "Epoch 3777, Loss: 0.03431826519954484, Final Batch Loss: 0.00015128779341466725\n",
      "Epoch 3778, Loss: 0.003839764784970612, Final Batch Loss: 0.00037536470335908234\n",
      "Epoch 3779, Loss: 0.05847583754803054, Final Batch Loss: 0.0010048217372968793\n",
      "Epoch 3780, Loss: 0.029864436732168542, Final Batch Loss: 0.003942825831472874\n",
      "Epoch 3781, Loss: 0.02526335422953707, Final Batch Loss: 0.0028774181846529245\n",
      "Epoch 3782, Loss: 0.012742797096507275, Final Batch Loss: 2.2080159396864474e-05\n",
      "Epoch 3783, Loss: 0.0031702396809123456, Final Batch Loss: 3.789864058489911e-05\n",
      "Epoch 3784, Loss: 0.0038722557237633737, Final Batch Loss: 4.632936179405078e-05\n",
      "Epoch 3785, Loss: 0.010856679249627632, Final Batch Loss: 2.3474256522604264e-05\n",
      "Epoch 3786, Loss: 0.0031798956442798954, Final Batch Loss: 0.0004975562915205956\n",
      "Epoch 3787, Loss: 0.020313831590101472, Final Batch Loss: 4.099125726497732e-05\n",
      "Epoch 3788, Loss: 0.01726500557288091, Final Batch Loss: 0.00012766491272486746\n",
      "Epoch 3789, Loss: 0.005018429377741995, Final Batch Loss: 0.00020005385158583522\n",
      "Epoch 3790, Loss: 0.04515277957852959, Final Batch Loss: 0.00012727417924907058\n",
      "Epoch 3791, Loss: 0.049615327752690064, Final Batch Loss: 0.00017354043666273355\n",
      "Epoch 3792, Loss: 0.016060485555499326, Final Batch Loss: 4.498791531659663e-05\n",
      "Epoch 3793, Loss: 0.031706399237009464, Final Batch Loss: 0.0002303968503838405\n",
      "Epoch 3794, Loss: 0.011873895116877975, Final Batch Loss: 0.0002637950237840414\n",
      "Epoch 3795, Loss: 0.006521786548546515, Final Batch Loss: 1.0307428055966739e-05\n",
      "Epoch 3796, Loss: 0.03122050205274718, Final Batch Loss: 0.0007141439127735794\n",
      "Epoch 3797, Loss: 0.01967493391202879, Final Batch Loss: 8.134826930472627e-05\n",
      "Epoch 3798, Loss: 0.03383421348553384, Final Batch Loss: 4.132841058890335e-05\n",
      "Epoch 3799, Loss: 0.011501878630951978, Final Batch Loss: 0.002021295949816704\n",
      "Epoch 3800, Loss: 0.028000796061860456, Final Batch Loss: 0.00013611563190352172\n",
      "Epoch 3801, Loss: 0.05945937626529485, Final Batch Loss: 0.00037176720798015594\n",
      "Epoch 3802, Loss: 0.03937122321076458, Final Batch Loss: 0.00410302123054862\n",
      "Epoch 3803, Loss: 0.0835745603480973, Final Batch Loss: 0.006273087579756975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3804, Loss: 0.02166943982592784, Final Batch Loss: 6.590894918190315e-05\n",
      "Epoch 3805, Loss: 0.006185990212543402, Final Batch Loss: 2.9582613933598623e-05\n",
      "Epoch 3806, Loss: 0.029971187506816932, Final Batch Loss: 0.007553430739790201\n",
      "Epoch 3807, Loss: 0.006899763633555267, Final Batch Loss: 0.0005541405407711864\n",
      "Epoch 3808, Loss: 0.05234468412163551, Final Batch Loss: 0.0012052697129547596\n",
      "Epoch 3809, Loss: 0.026560183705441887, Final Batch Loss: 0.00021865181042812765\n",
      "Epoch 3810, Loss: 0.0632965039408191, Final Batch Loss: 0.025319762527942657\n",
      "Epoch 3811, Loss: 0.011646288134215865, Final Batch Loss: 0.004274406936019659\n",
      "Epoch 3812, Loss: 0.02152020675566746, Final Batch Loss: 4.081002407474443e-05\n",
      "Epoch 3813, Loss: 0.04184392950082838, Final Batch Loss: 0.0006809559999965131\n",
      "Epoch 3814, Loss: 0.007255489488670719, Final Batch Loss: 6.0027359722880647e-05\n",
      "Epoch 3815, Loss: 0.026528593002694834, Final Batch Loss: 6.798547838116065e-05\n",
      "Epoch 3816, Loss: 0.010082258802867727, Final Batch Loss: 3.719155938597396e-05\n",
      "Epoch 3817, Loss: 0.03187228355091065, Final Batch Loss: 0.0006463450263254344\n",
      "Epoch 3818, Loss: 0.02330840750801144, Final Batch Loss: 0.0025191395543515682\n",
      "Epoch 3819, Loss: 0.04120078363484936, Final Batch Loss: 0.00027766183484345675\n",
      "Epoch 3820, Loss: 0.02372934700542828, Final Batch Loss: 0.0002594470279291272\n",
      "Epoch 3821, Loss: 0.07755941424693447, Final Batch Loss: 9.261927334591746e-05\n",
      "Epoch 3822, Loss: 0.007778949242492672, Final Batch Loss: 0.0007346696220338345\n",
      "Epoch 3823, Loss: 0.03702906040052767, Final Batch Loss: 0.007429428864270449\n",
      "Epoch 3824, Loss: 0.065172016104043, Final Batch Loss: 0.0005778228514827788\n",
      "Epoch 3825, Loss: 0.04730918631230452, Final Batch Loss: 1.2404615517880302e-05\n",
      "Epoch 3826, Loss: 0.06370315736421617, Final Batch Loss: 0.03685341402888298\n",
      "Epoch 3827, Loss: 0.027784897512901807, Final Batch Loss: 0.0008502398850396276\n",
      "Epoch 3828, Loss: 0.07032068179978523, Final Batch Loss: 0.0013122378150001168\n",
      "Epoch 3829, Loss: 0.07958416434121318, Final Batch Loss: 0.0287588220089674\n",
      "Epoch 3830, Loss: 0.06935397948018363, Final Batch Loss: 0.018210355192422867\n",
      "Epoch 3831, Loss: 0.06543037845403887, Final Batch Loss: 0.0014691378455609083\n",
      "Epoch 3832, Loss: 0.011467572156107053, Final Batch Loss: 0.0007148351287469268\n",
      "Epoch 3833, Loss: 0.04040322299260879, Final Batch Loss: 0.021412575617432594\n",
      "Epoch 3834, Loss: 0.024526199413230643, Final Batch Loss: 0.0005674269632436335\n",
      "Epoch 3835, Loss: 0.014812830770097207, Final Batch Loss: 0.0005189558141864836\n",
      "Epoch 3836, Loss: 0.0387886191601865, Final Batch Loss: 0.00024110429512802511\n",
      "Epoch 3837, Loss: 0.02138140980969183, Final Batch Loss: 0.0002318825136171654\n",
      "Epoch 3838, Loss: 0.05248485634365352, Final Batch Loss: 0.00030611330294050276\n",
      "Epoch 3839, Loss: 0.016821632329083513, Final Batch Loss: 8.474456262774765e-05\n",
      "Epoch 3840, Loss: 0.010880692505452316, Final Batch Loss: 0.0010706812608987093\n",
      "Epoch 3841, Loss: 0.0307367175410036, Final Batch Loss: 0.0002919717808254063\n",
      "Epoch 3842, Loss: 0.0299573506017623, Final Batch Loss: 6.672917515970767e-05\n",
      "Epoch 3843, Loss: 0.033788498236390296, Final Batch Loss: 0.00014701718464493752\n",
      "Epoch 3844, Loss: 0.03728280304494547, Final Batch Loss: 6.611505523324013e-05\n",
      "Epoch 3845, Loss: 0.01864062575623393, Final Batch Loss: 0.000345153413945809\n",
      "Epoch 3846, Loss: 0.03371853396674851, Final Batch Loss: 0.0005518635152839124\n",
      "Epoch 3847, Loss: 0.01959170751797501, Final Batch Loss: 0.00026106651057489216\n",
      "Epoch 3848, Loss: 0.02046948809402238, Final Batch Loss: 8.077480015344918e-05\n",
      "Epoch 3849, Loss: 0.044033762700564694, Final Batch Loss: 0.0002796852495521307\n",
      "Epoch 3850, Loss: 0.007909807594842277, Final Batch Loss: 0.0007294833776541054\n",
      "Epoch 3851, Loss: 0.03760431431874167, Final Batch Loss: 0.000131788503495045\n",
      "Epoch 3852, Loss: 0.007214316894533113, Final Batch Loss: 0.00011708957026712596\n",
      "Epoch 3853, Loss: 0.021840562138095265, Final Batch Loss: 0.00019078479090239853\n",
      "Epoch 3854, Loss: 0.007853750799768022, Final Batch Loss: 0.00011804700625361875\n",
      "Epoch 3855, Loss: 0.02785285203208332, Final Batch Loss: 0.0013986745616421103\n",
      "Epoch 3856, Loss: 0.022745585258235224, Final Batch Loss: 0.002603036118671298\n",
      "Epoch 3857, Loss: 0.030026885640836554, Final Batch Loss: 0.00015056786651257426\n",
      "Epoch 3858, Loss: 0.014307649857073557, Final Batch Loss: 0.0002803104871418327\n",
      "Epoch 3859, Loss: 0.037772429721371736, Final Batch Loss: 2.2940264898352325e-05\n",
      "Epoch 3860, Loss: 0.010866587566852104, Final Batch Loss: 5.614803376374766e-05\n",
      "Epoch 3861, Loss: 0.005179372472412069, Final Batch Loss: 0.0002245007490273565\n",
      "Epoch 3862, Loss: 0.04761551704905287, Final Batch Loss: 0.015113112516701221\n",
      "Epoch 3863, Loss: 0.015179723683104385, Final Batch Loss: 0.0003605570818763226\n",
      "Epoch 3864, Loss: 0.028033800521370722, Final Batch Loss: 0.00034797083935700357\n",
      "Epoch 3865, Loss: 0.04638006287132157, Final Batch Loss: 0.03348930552601814\n",
      "Epoch 3866, Loss: 0.04722237836904242, Final Batch Loss: 0.0004369283851701766\n",
      "Epoch 3867, Loss: 0.17574859814340016, Final Batch Loss: 0.003921581897884607\n",
      "Epoch 3868, Loss: 0.24922122244606726, Final Batch Loss: 0.004247957840561867\n",
      "Epoch 3869, Loss: 0.07905164463591063, Final Batch Loss: 0.019847644492983818\n",
      "Epoch 3870, Loss: 0.06432831376878312, Final Batch Loss: 0.00012170919217169285\n",
      "Epoch 3871, Loss: 0.014805761922616512, Final Batch Loss: 0.0014476424548774958\n",
      "Epoch 3872, Loss: 0.020087173317733686, Final Batch Loss: 6.253259925870225e-05\n",
      "Epoch 3873, Loss: 0.011944841950025875, Final Batch Loss: 0.0001739089348120615\n",
      "Epoch 3874, Loss: 0.007041237313387683, Final Batch Loss: 3.491644383757375e-05\n",
      "Epoch 3875, Loss: 0.006994484210736118, Final Batch Loss: 4.5734392188023776e-05\n",
      "Epoch 3876, Loss: 0.0030243786259234184, Final Batch Loss: 6.045989357517101e-05\n",
      "Epoch 3877, Loss: 0.017552375786181074, Final Batch Loss: 0.00010419688624097034\n",
      "Epoch 3878, Loss: 0.011980990140727954, Final Batch Loss: 5.2206352847861126e-05\n",
      "Epoch 3879, Loss: 0.008310640629133559, Final Batch Loss: 2.1412286514532752e-05\n",
      "Epoch 3880, Loss: 0.010313326334653539, Final Batch Loss: 0.00033591684768907726\n",
      "Epoch 3881, Loss: 0.01537236735202896, Final Batch Loss: 0.0001654150546528399\n",
      "Epoch 3882, Loss: 0.006616432072405587, Final Batch Loss: 4.4868880650028586e-05\n",
      "Epoch 3883, Loss: 0.006827191238699015, Final Batch Loss: 0.00025963695952668786\n",
      "Epoch 3884, Loss: 0.022975464707997162, Final Batch Loss: 9.077585127670318e-05\n",
      "Epoch 3885, Loss: 0.01621069340762915, Final Batch Loss: 0.0002671217080205679\n",
      "Epoch 3886, Loss: 0.013851203308149707, Final Batch Loss: 0.0072460053488612175\n",
      "Epoch 3887, Loss: 0.029134851718481514, Final Batch Loss: 0.0009808385511860251\n",
      "Epoch 3888, Loss: 0.01653256843565032, Final Batch Loss: 0.00012003334268229082\n",
      "Epoch 3889, Loss: 0.01528583341405465, Final Batch Loss: 0.00027042298461310565\n",
      "Epoch 3890, Loss: 0.011823867054772563, Final Batch Loss: 0.00042464997386559844\n",
      "Epoch 3891, Loss: 0.0067545948368206155, Final Batch Loss: 0.0018066067714244127\n",
      "Epoch 3892, Loss: 0.019148148043313995, Final Batch Loss: 3.1277744710678235e-05\n",
      "Epoch 3893, Loss: 0.006405092825843894, Final Batch Loss: 2.0586476239259355e-05\n",
      "Epoch 3894, Loss: 0.005540870253753383, Final Batch Loss: 5.743938163504936e-05\n",
      "Epoch 3895, Loss: 0.006340416741295485, Final Batch Loss: 0.0020793084986507893\n",
      "Epoch 3896, Loss: 0.0022434636748585035, Final Batch Loss: 4.935782271786593e-05\n",
      "Epoch 3897, Loss: 0.003412722560824477, Final Batch Loss: 0.00019791418162640184\n",
      "Epoch 3898, Loss: 0.0023155190178840712, Final Batch Loss: 5.222646450420143e-06\n",
      "Epoch 3899, Loss: 0.0028245239964235225, Final Batch Loss: 1.973963480850216e-05\n",
      "Epoch 3900, Loss: 0.008034914592940368, Final Batch Loss: 0.00023123243590816855\n",
      "Epoch 3901, Loss: 0.009368360138068965, Final Batch Loss: 7.529062713729218e-05\n",
      "Epoch 3902, Loss: 0.02416401633308851, Final Batch Loss: 0.00029047453426755965\n",
      "Epoch 3903, Loss: 0.011296961686639406, Final Batch Loss: 4.15816975873895e-06\n",
      "Epoch 3904, Loss: 0.024657506846779143, Final Batch Loss: 0.017241017892956734\n",
      "Epoch 3905, Loss: 0.0016753426070863497, Final Batch Loss: 4.045628156745806e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3906, Loss: 0.004833971365769685, Final Batch Loss: 0.0003045314224436879\n",
      "Epoch 3907, Loss: 0.03599778460102243, Final Batch Loss: 3.78907352569513e-05\n",
      "Epoch 3908, Loss: 0.042692047920354526, Final Batch Loss: 0.003096346976235509\n",
      "Epoch 3909, Loss: 0.0040000293606681225, Final Batch Loss: 7.080217619659379e-05\n",
      "Epoch 3910, Loss: 0.003400471706299868, Final Batch Loss: 1.994521517190151e-05\n",
      "Epoch 3911, Loss: 0.0038576433253183495, Final Batch Loss: 2.298987419635523e-05\n",
      "Epoch 3912, Loss: 0.01218408448039554, Final Batch Loss: 0.0001495686883572489\n",
      "Epoch 3913, Loss: 0.019136653972964268, Final Batch Loss: 0.006642939988523722\n",
      "Epoch 3914, Loss: 0.0036357407243485795, Final Batch Loss: 0.0001361966278636828\n",
      "Epoch 3915, Loss: 0.042977830806194106, Final Batch Loss: 9.631372813601047e-05\n",
      "Epoch 3916, Loss: 0.020541853924441966, Final Batch Loss: 0.013428122736513615\n",
      "Epoch 3917, Loss: 0.0350511096057744, Final Batch Loss: 0.003237553406506777\n",
      "Epoch 3918, Loss: 0.026691269795264816, Final Batch Loss: 0.0024479059502482414\n",
      "Epoch 3919, Loss: 0.011013266499503516, Final Batch Loss: 4.838517270400189e-05\n",
      "Epoch 3920, Loss: 0.009186795282857929, Final Batch Loss: 5.605186288448749e-06\n",
      "Epoch 3921, Loss: 0.006274503757140337, Final Batch Loss: 4.2139417928410694e-05\n",
      "Epoch 3922, Loss: 0.009914047510392265, Final Batch Loss: 0.00017785457021091133\n",
      "Epoch 3923, Loss: 0.0063611475452489685, Final Batch Loss: 8.747352694626898e-06\n",
      "Epoch 3924, Loss: 0.0015843486980884336, Final Batch Loss: 1.6762851373641752e-05\n",
      "Epoch 3925, Loss: 0.02120334439587168, Final Batch Loss: 0.00022590113803744316\n",
      "Epoch 3926, Loss: 0.05634345713406219, Final Batch Loss: 0.0003284884733147919\n",
      "Epoch 3927, Loss: 0.021080379596241983, Final Batch Loss: 3.840091812890023e-05\n",
      "Epoch 3928, Loss: 0.09949951529597456, Final Batch Loss: 0.02371872588992119\n",
      "Epoch 3929, Loss: 0.08396686297055567, Final Batch Loss: 0.0125214122235775\n",
      "Epoch 3930, Loss: 0.05900295460014604, Final Batch Loss: 0.004254516214132309\n",
      "Epoch 3931, Loss: 0.03560129855759442, Final Batch Loss: 0.0004590405442286283\n",
      "Epoch 3932, Loss: 0.011761583809857257, Final Batch Loss: 0.00028583488892763853\n",
      "Epoch 3933, Loss: 0.02276636553233402, Final Batch Loss: 0.0005217120633460581\n",
      "Epoch 3934, Loss: 0.004302169500988384, Final Batch Loss: 4.395262658363208e-05\n",
      "Epoch 3935, Loss: 0.01238259088495397, Final Batch Loss: 5.318982221069746e-05\n",
      "Epoch 3936, Loss: 0.03471428810553334, Final Batch Loss: 0.00022590947628486902\n",
      "Epoch 3937, Loss: 0.031105275735171745, Final Batch Loss: 9.852220682660118e-05\n",
      "Epoch 3938, Loss: 0.015046010521473363, Final Batch Loss: 0.00024396127264481038\n",
      "Epoch 3939, Loss: 0.02473743356495106, Final Batch Loss: 0.0010324940085411072\n",
      "Epoch 3940, Loss: 0.021435561819089344, Final Batch Loss: 0.006443006917834282\n",
      "Epoch 3941, Loss: 0.025401139573659748, Final Batch Loss: 0.0015482277376577258\n",
      "Epoch 3942, Loss: 0.01712461272109067, Final Batch Loss: 0.00012432625226210803\n",
      "Epoch 3943, Loss: 0.01007560773723526, Final Batch Loss: 0.0003182275395374745\n",
      "Epoch 3944, Loss: 0.004620146214620036, Final Batch Loss: 0.00033701048232614994\n",
      "Epoch 3945, Loss: 0.03225080131232971, Final Batch Loss: 0.0008258108282461762\n",
      "Epoch 3946, Loss: 0.00271074285137729, Final Batch Loss: 0.0004687283362727612\n",
      "Epoch 3947, Loss: 0.07615969806647627, Final Batch Loss: 0.00024646532256156206\n",
      "Epoch 3948, Loss: 0.010505565347557422, Final Batch Loss: 0.0008906577131710947\n",
      "Epoch 3949, Loss: 0.027650869647914078, Final Batch Loss: 0.0009003272862173617\n",
      "Epoch 3950, Loss: 0.020375429468913353, Final Batch Loss: 0.0019359508296474814\n",
      "Epoch 3951, Loss: 0.013512855704448157, Final Batch Loss: 0.0005370806320570409\n",
      "Epoch 3952, Loss: 0.00455602000329236, Final Batch Loss: 4.541933230939321e-05\n",
      "Epoch 3953, Loss: 0.01035540710654459, Final Batch Loss: 0.0006487193750217557\n",
      "Epoch 3954, Loss: 0.0033650105260676355, Final Batch Loss: 1.7165340977953747e-05\n",
      "Epoch 3955, Loss: 0.003332092887831095, Final Batch Loss: 0.0003191135183442384\n",
      "Epoch 3956, Loss: 0.008507200323037978, Final Batch Loss: 0.002324796048924327\n",
      "Epoch 3957, Loss: 0.0029897549611632712, Final Batch Loss: 9.958165173884481e-05\n",
      "Epoch 3958, Loss: 0.0094865537307669, Final Batch Loss: 6.190956628415734e-05\n",
      "Epoch 3959, Loss: 0.0014178482651914237, Final Batch Loss: 6.575416773557663e-05\n",
      "Epoch 3960, Loss: 0.006834319295194291, Final Batch Loss: 1.4721445040777326e-05\n",
      "Epoch 3961, Loss: 0.0032537821543883183, Final Batch Loss: 0.00012109644740121439\n",
      "Epoch 3962, Loss: 0.007048335521176341, Final Batch Loss: 2.581820444902405e-05\n",
      "Epoch 3963, Loss: 0.005263428320176899, Final Batch Loss: 4.159939271630719e-05\n",
      "Epoch 3964, Loss: 0.006993421214929185, Final Batch Loss: 3.0364186386577785e-05\n",
      "Epoch 3965, Loss: 0.010535228275330155, Final Batch Loss: 6.1188948166091e-05\n",
      "Epoch 3966, Loss: 0.0918711735839679, Final Batch Loss: 4.1841489291982725e-05\n",
      "Epoch 3967, Loss: 0.020223345550220984, Final Batch Loss: 0.00043265882413834333\n",
      "Epoch 3968, Loss: 0.009051552543951402, Final Batch Loss: 5.6756292906356975e-05\n",
      "Epoch 3969, Loss: 0.04126601895859494, Final Batch Loss: 0.00017958448734134436\n",
      "Epoch 3970, Loss: 0.01657930907094851, Final Batch Loss: 0.004844070412218571\n",
      "Epoch 3971, Loss: 0.02415241693961434, Final Batch Loss: 0.004257370252162218\n",
      "Epoch 3972, Loss: 0.04767357980290399, Final Batch Loss: 2.7733925890061073e-05\n",
      "Epoch 3973, Loss: 0.03617346178543812, Final Batch Loss: 3.1575094908475876e-05\n",
      "Epoch 3974, Loss: 0.0076252630660746945, Final Batch Loss: 0.002142915967851877\n",
      "Epoch 3975, Loss: 0.007542205072240904, Final Batch Loss: 0.0003411268990021199\n",
      "Epoch 3976, Loss: 0.0034721091524261283, Final Batch Loss: 5.3166982979746535e-05\n",
      "Epoch 3977, Loss: 0.004783999078426859, Final Batch Loss: 6.436952389776707e-05\n",
      "Epoch 3978, Loss: 0.023024555564916227, Final Batch Loss: 0.00025466587976552546\n",
      "Epoch 3979, Loss: 0.005685588266715058, Final Batch Loss: 0.0003125772054772824\n",
      "Epoch 3980, Loss: 0.0045364336256170645, Final Batch Loss: 0.0006456252303905785\n",
      "Epoch 3981, Loss: 0.010341555815102765, Final Batch Loss: 0.0020085503347218037\n",
      "Epoch 3982, Loss: 0.03396847225667443, Final Batch Loss: 9.769593816599809e-06\n",
      "Epoch 3983, Loss: 0.05239857246306201, Final Batch Loss: 0.0009563277708366513\n",
      "Epoch 3984, Loss: 0.03371827458249754, Final Batch Loss: 0.0005845552659593523\n",
      "Epoch 3985, Loss: 0.084225408319071, Final Batch Loss: 0.015468529425561428\n",
      "Epoch 3986, Loss: 0.1920133718595025, Final Batch Loss: 2.5339997591800056e-05\n",
      "Epoch 3987, Loss: 0.09715652358863736, Final Batch Loss: 0.0016078047920018435\n",
      "Epoch 3988, Loss: 0.05901210331285256, Final Batch Loss: 0.00026096621877513826\n",
      "Epoch 3989, Loss: 0.06610813508450519, Final Batch Loss: 0.0019965863320976496\n",
      "Epoch 3990, Loss: 0.04963436798425391, Final Batch Loss: 0.0017131834756582975\n",
      "Epoch 3991, Loss: 0.026732740388979437, Final Batch Loss: 3.270950764999725e-05\n",
      "Epoch 3992, Loss: 0.027931172447097197, Final Batch Loss: 0.00037381923175416887\n",
      "Epoch 3993, Loss: 0.010436889606353361, Final Batch Loss: 0.00026236544363200665\n",
      "Epoch 3994, Loss: 0.022425134709919803, Final Batch Loss: 0.00040496676228940487\n",
      "Epoch 3995, Loss: 0.01897460037162091, Final Batch Loss: 0.00016335089458152652\n",
      "Epoch 3996, Loss: 0.03992929763626307, Final Batch Loss: 0.0001230767520610243\n",
      "Epoch 3997, Loss: 0.013770220961305313, Final Batch Loss: 0.00013642857084050775\n",
      "Epoch 3998, Loss: 0.020171102425592835, Final Batch Loss: 0.008314789272844791\n",
      "Epoch 3999, Loss: 0.020208869314956246, Final Batch Loss: 0.0013358548749238253\n",
      "Epoch 4000, Loss: 0.04319478241086472, Final Batch Loss: 0.0008360011852346361\n",
      "Epoch 4001, Loss: 0.0028043837992299814, Final Batch Loss: 7.783508044667542e-05\n",
      "Epoch 4002, Loss: 0.011721307602783781, Final Batch Loss: 0.0052399951964616776\n",
      "Epoch 4003, Loss: 0.02264353434657096, Final Batch Loss: 0.00017377713811583817\n",
      "Epoch 4004, Loss: 0.026740044264442986, Final Batch Loss: 0.0007262807921506464\n",
      "Epoch 4005, Loss: 0.00608769816608401, Final Batch Loss: 0.001148850191384554\n",
      "Epoch 4006, Loss: 0.020404581315233372, Final Batch Loss: 0.008862349204719067\n",
      "Epoch 4007, Loss: 0.012173294830063242, Final Batch Loss: 3.9720798667985946e-05\n",
      "Epoch 4008, Loss: 0.006937888275388104, Final Batch Loss: 0.0006223396630957723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4009, Loss: 0.014538221828843234, Final Batch Loss: 8.617514686193317e-05\n",
      "Epoch 4010, Loss: 0.007335610604059184, Final Batch Loss: 0.00025074920267798007\n",
      "Epoch 4011, Loss: 0.006500224833871471, Final Batch Loss: 2.6827996407519095e-05\n",
      "Epoch 4012, Loss: 0.006772269547582255, Final Batch Loss: 0.0005317598697729409\n",
      "Epoch 4013, Loss: 0.013211680429776607, Final Batch Loss: 5.351095023797825e-05\n",
      "Epoch 4014, Loss: 0.005504975975782145, Final Batch Loss: 9.3087597633712e-05\n",
      "Epoch 4015, Loss: 0.004916590834909584, Final Batch Loss: 0.0020002098754048347\n",
      "Epoch 4016, Loss: 0.006830479554992053, Final Batch Loss: 1.1880255442520138e-05\n",
      "Epoch 4017, Loss: 0.017360454450681573, Final Batch Loss: 0.00018605677178129554\n",
      "Epoch 4018, Loss: 0.012766177376761334, Final Batch Loss: 0.00012328720185905695\n",
      "Epoch 4019, Loss: 0.005996129272716644, Final Batch Loss: 2.0793635485460982e-05\n",
      "Epoch 4020, Loss: 0.006552175561864715, Final Batch Loss: 0.0002979156852234155\n",
      "Epoch 4021, Loss: 0.003818819624484604, Final Batch Loss: 0.00013781923917122185\n",
      "Epoch 4022, Loss: 0.011033796575247834, Final Batch Loss: 0.0005128923803567886\n",
      "Epoch 4023, Loss: 0.02093747753133357, Final Batch Loss: 0.0004092234594281763\n",
      "Epoch 4024, Loss: 0.0016330084763467312, Final Batch Loss: 0.0002623890177346766\n",
      "Epoch 4025, Loss: 0.014542640230502002, Final Batch Loss: 9.533836418995634e-05\n",
      "Epoch 4026, Loss: 0.003018279256139067, Final Batch Loss: 0.0006973272538743913\n",
      "Epoch 4027, Loss: 0.013967038522423536, Final Batch Loss: 0.0002046500740107149\n",
      "Epoch 4028, Loss: 0.008685643029821222, Final Batch Loss: 8.222023097914644e-06\n",
      "Epoch 4029, Loss: 0.004144767925936321, Final Batch Loss: 0.0002475625369697809\n",
      "Epoch 4030, Loss: 0.008116851305203454, Final Batch Loss: 0.00011365901445969939\n",
      "Epoch 4031, Loss: 0.006621734587497485, Final Batch Loss: 9.673010936239734e-05\n",
      "Epoch 4032, Loss: 0.004327754073528922, Final Batch Loss: 2.0214985852362588e-05\n",
      "Epoch 4033, Loss: 0.0062093475098663475, Final Batch Loss: 0.0001614616485312581\n",
      "Epoch 4034, Loss: 0.014735153703895776, Final Batch Loss: 0.010119188576936722\n",
      "Epoch 4035, Loss: 0.008834490276058204, Final Batch Loss: 0.001470885588787496\n",
      "Epoch 4036, Loss: 0.03457022345173755, Final Batch Loss: 0.00043699826346710324\n",
      "Epoch 4037, Loss: 0.021867661069336464, Final Batch Loss: 0.00174430210608989\n",
      "Epoch 4038, Loss: 0.06115648247941863, Final Batch Loss: 0.0011244920315220952\n",
      "Epoch 4039, Loss: 0.11568501060992276, Final Batch Loss: 0.004555973224341869\n",
      "Epoch 4040, Loss: 0.05211753782532469, Final Batch Loss: 0.00020772461721207947\n",
      "Epoch 4041, Loss: 0.1430187434380059, Final Batch Loss: 8.935825462685898e-05\n",
      "Epoch 4042, Loss: 0.039652922336244956, Final Batch Loss: 0.00042750383727252483\n",
      "Epoch 4043, Loss: 0.02978912841354031, Final Batch Loss: 0.0003782723215408623\n",
      "Epoch 4044, Loss: 0.11038999645825243, Final Batch Loss: 0.029143430292606354\n",
      "Epoch 4045, Loss: 0.14378107219818048, Final Batch Loss: 0.04023446887731552\n",
      "Epoch 4046, Loss: 0.09872953733429313, Final Batch Loss: 0.014280293136835098\n",
      "Epoch 4047, Loss: 0.02911666342697572, Final Batch Loss: 0.0005317016621120274\n",
      "Epoch 4048, Loss: 0.08321120729669929, Final Batch Loss: 0.0022096592001616955\n",
      "Epoch 4049, Loss: 0.04945508381933905, Final Batch Loss: 0.0008212222601287067\n",
      "Epoch 4050, Loss: 0.029562501862528734, Final Batch Loss: 0.00021041555737610906\n",
      "Epoch 4051, Loss: 0.024791953546809964, Final Batch Loss: 0.0017087680753320456\n",
      "Epoch 4052, Loss: 0.02521053001692053, Final Batch Loss: 0.01585466042160988\n",
      "Epoch 4053, Loss: 0.010937392627056397, Final Batch Loss: 0.0018968433141708374\n",
      "Epoch 4054, Loss: 0.025321390729004634, Final Batch Loss: 0.0004234584921505302\n",
      "Epoch 4055, Loss: 0.007806046141922707, Final Batch Loss: 8.434997289441526e-05\n",
      "Epoch 4056, Loss: 0.027799031671747798, Final Batch Loss: 0.0005331346183083951\n",
      "Epoch 4057, Loss: 0.04405921844590921, Final Batch Loss: 0.0008482278790324926\n",
      "Epoch 4058, Loss: 0.023676848635659553, Final Batch Loss: 0.00037781050195917487\n",
      "Epoch 4059, Loss: 0.010485958095159731, Final Batch Loss: 1.9303091903566383e-05\n",
      "Epoch 4060, Loss: 0.007634840036189416, Final Batch Loss: 3.248062785132788e-05\n",
      "Epoch 4061, Loss: 0.018292573595317663, Final Batch Loss: 0.0007804802735336125\n",
      "Epoch 4062, Loss: 0.0448925862476699, Final Batch Loss: 0.00013931076682638377\n",
      "Epoch 4063, Loss: 0.02174817603736301, Final Batch Loss: 0.0041035558097064495\n",
      "Epoch 4064, Loss: 0.020441064261831343, Final Batch Loss: 0.00019055363372899592\n",
      "Epoch 4065, Loss: 0.040181765682064, Final Batch Loss: 0.0004734839603770524\n",
      "Epoch 4066, Loss: 0.009113178097322816, Final Batch Loss: 0.00011188136704731733\n",
      "Epoch 4067, Loss: 0.01549816701663076, Final Batch Loss: 2.9960105166537687e-05\n",
      "Epoch 4068, Loss: 0.012367442552204011, Final Batch Loss: 0.0007054274319671094\n",
      "Epoch 4069, Loss: 0.03287787941371789, Final Batch Loss: 0.0010551913874223828\n",
      "Epoch 4070, Loss: 0.025694357591419248, Final Batch Loss: 0.0006528597441501915\n",
      "Epoch 4071, Loss: 0.012555270586744882, Final Batch Loss: 0.0003574633155949414\n",
      "Epoch 4072, Loss: 0.008818232432531659, Final Batch Loss: 0.00021184833894949406\n",
      "Epoch 4073, Loss: 0.019288192159820028, Final Batch Loss: 5.9938371123280376e-05\n",
      "Epoch 4074, Loss: 0.010235913998258184, Final Batch Loss: 9.378063987242058e-05\n",
      "Epoch 4075, Loss: 0.008195288673960022, Final Batch Loss: 5.920546027482487e-05\n",
      "Epoch 4076, Loss: 0.003677666840303573, Final Batch Loss: 5.2308889280539006e-05\n",
      "Epoch 4077, Loss: 0.00513205521110649, Final Batch Loss: 2.669053355930373e-05\n",
      "Epoch 4078, Loss: 0.00289354864435154, Final Batch Loss: 7.399125024676323e-05\n",
      "Epoch 4079, Loss: 0.007381908870229381, Final Batch Loss: 5.519434125744738e-05\n",
      "Epoch 4080, Loss: 0.005497663668393216, Final Batch Loss: 0.0004811614053323865\n",
      "Epoch 4081, Loss: 0.026965812893877228, Final Batch Loss: 0.0016683434369042516\n",
      "Epoch 4082, Loss: 0.0176576918183855, Final Batch Loss: 0.0012381428387016058\n",
      "Epoch 4083, Loss: 0.006780070572858676, Final Batch Loss: 3.086914512095973e-05\n",
      "Epoch 4084, Loss: 0.006918910063177464, Final Batch Loss: 0.00026252877432852983\n",
      "Epoch 4085, Loss: 0.009886515230391524, Final Batch Loss: 6.337281229207292e-05\n",
      "Epoch 4086, Loss: 0.006936517589565483, Final Batch Loss: 0.0008067705202847719\n",
      "Epoch 4087, Loss: 0.0031426185687450925, Final Batch Loss: 0.0005505569279193878\n",
      "Epoch 4088, Loss: 0.023554839952339535, Final Batch Loss: 0.01219352800399065\n",
      "Epoch 4089, Loss: 0.01170853927033022, Final Batch Loss: 0.0017296383157372475\n",
      "Epoch 4090, Loss: 0.01539177435779493, Final Batch Loss: 9.228869021171704e-05\n",
      "Epoch 4091, Loss: 0.030308190773212118, Final Batch Loss: 6.172323628561571e-05\n",
      "Epoch 4092, Loss: 0.0073635130993352504, Final Batch Loss: 7.81340932007879e-05\n",
      "Epoch 4093, Loss: 0.06674736949571525, Final Batch Loss: 0.024171695113182068\n",
      "Epoch 4094, Loss: 0.08510992155061103, Final Batch Loss: 0.004192488268017769\n",
      "Epoch 4095, Loss: 0.08129405326326378, Final Batch Loss: 0.0006602983339689672\n",
      "Epoch 4096, Loss: 0.049579781840293435, Final Batch Loss: 0.000867774710059166\n",
      "Epoch 4097, Loss: 0.037748764245407074, Final Batch Loss: 0.00012132206029491499\n",
      "Epoch 4098, Loss: 0.02042060018357006, Final Batch Loss: 0.0042698620818555355\n",
      "Epoch 4099, Loss: 0.07109638497786364, Final Batch Loss: 0.0024717815686017275\n",
      "Epoch 4100, Loss: 0.2370285434321886, Final Batch Loss: 0.111538827419281\n",
      "Epoch 4101, Loss: 0.179275319664157, Final Batch Loss: 0.0013463867362588644\n",
      "Epoch 4102, Loss: 0.08721764409710886, Final Batch Loss: 0.006919722072780132\n",
      "Epoch 4103, Loss: 0.03586771627306007, Final Batch Loss: 0.000964385224506259\n",
      "Epoch 4104, Loss: 0.05881720464094542, Final Batch Loss: 0.0002587950730230659\n",
      "Epoch 4105, Loss: 0.02826916608319152, Final Batch Loss: 0.0023507261648774147\n",
      "Epoch 4106, Loss: 0.025374816832481883, Final Batch Loss: 0.0005262405029498041\n",
      "Epoch 4107, Loss: 0.014440199316595681, Final Batch Loss: 0.0013967074919492006\n",
      "Epoch 4108, Loss: 0.022644181619398296, Final Batch Loss: 0.012556381523609161\n",
      "Epoch 4109, Loss: 0.012635439401492476, Final Batch Loss: 0.00013365736231207848\n",
      "Epoch 4110, Loss: 0.009242491189070279, Final Batch Loss: 0.00012203386722831056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4111, Loss: 0.013476946860464523, Final Batch Loss: 0.00035568542080000043\n",
      "Epoch 4112, Loss: 0.025197002687491477, Final Batch Loss: 0.0032192084472626448\n",
      "Epoch 4113, Loss: 0.023035136466205586, Final Batch Loss: 0.01040046475827694\n",
      "Epoch 4114, Loss: 0.011606840496824589, Final Batch Loss: 0.0001489816204411909\n",
      "Epoch 4115, Loss: 0.007428906996210571, Final Batch Loss: 6.866107287351042e-05\n",
      "Epoch 4116, Loss: 0.0032893652278289665, Final Batch Loss: 4.0603659726912156e-05\n",
      "Epoch 4117, Loss: 0.01960431613770197, Final Batch Loss: 0.0005916886148042977\n",
      "Epoch 4118, Loss: 0.007874273745983373, Final Batch Loss: 0.0005384268006309867\n",
      "Epoch 4119, Loss: 0.03636059949531045, Final Batch Loss: 0.00030177118605934083\n",
      "Epoch 4120, Loss: 0.03322548028154415, Final Batch Loss: 0.001685654278844595\n",
      "Epoch 4121, Loss: 0.014543535076882108, Final Batch Loss: 0.0005590281798504293\n",
      "Epoch 4122, Loss: 0.014721985113283154, Final Batch Loss: 8.082271233433858e-05\n",
      "Epoch 4123, Loss: 0.010718231585087779, Final Batch Loss: 0.0008465854334644973\n",
      "Epoch 4124, Loss: 0.011197947545952047, Final Batch Loss: 0.0004229999613016844\n",
      "Epoch 4125, Loss: 0.03892497029301012, Final Batch Loss: 0.00037423305911943316\n",
      "Epoch 4126, Loss: 0.004404960220199428, Final Batch Loss: 0.00023492416949011385\n",
      "Epoch 4127, Loss: 0.017743954740581103, Final Batch Loss: 0.004382454324513674\n",
      "Epoch 4128, Loss: 0.0263599555073597, Final Batch Loss: 0.0011496735969558358\n",
      "Epoch 4129, Loss: 0.004803561223525321, Final Batch Loss: 0.0009558610036037862\n",
      "Epoch 4130, Loss: 0.001928830719407415, Final Batch Loss: 7.462943176506087e-05\n",
      "Epoch 4131, Loss: 0.0046411953517235816, Final Batch Loss: 0.00023618667910341173\n",
      "Epoch 4132, Loss: 0.0040481226169504225, Final Batch Loss: 7.549637666670606e-05\n",
      "Epoch 4133, Loss: 0.01228503160746186, Final Batch Loss: 0.00018067931523546576\n",
      "Epoch 4134, Loss: 0.02663735930218536, Final Batch Loss: 4.307985000195913e-05\n",
      "Epoch 4135, Loss: 0.006993869676080067, Final Batch Loss: 0.0007556204218417406\n",
      "Epoch 4136, Loss: 0.011846570275338308, Final Batch Loss: 0.0021258085034787655\n",
      "Epoch 4137, Loss: 0.003790880859014578, Final Batch Loss: 0.0014864428667351604\n",
      "Epoch 4138, Loss: 0.01785739998376812, Final Batch Loss: 0.00023598007101099938\n",
      "Epoch 4139, Loss: 0.004902909024167457, Final Batch Loss: 7.214236393338069e-05\n",
      "Epoch 4140, Loss: 0.007760483073070645, Final Batch Loss: 0.000912437099032104\n",
      "Epoch 4141, Loss: 0.005344361546121945, Final Batch Loss: 2.0311694242991507e-05\n",
      "Epoch 4142, Loss: 0.003269219780122512, Final Batch Loss: 0.0013602280523627996\n",
      "Epoch 4143, Loss: 0.003868570654958603, Final Batch Loss: 0.0002517516550142318\n",
      "Epoch 4144, Loss: 0.04687772157922154, Final Batch Loss: 4.393170092953369e-05\n",
      "Epoch 4145, Loss: 0.04695915885895374, Final Batch Loss: 2.948350265796762e-05\n",
      "Epoch 4146, Loss: 0.0030290341910585994, Final Batch Loss: 0.00013580548693425953\n",
      "Epoch 4147, Loss: 0.0009171545657409297, Final Batch Loss: 4.150004679104313e-05\n",
      "Epoch 4148, Loss: 0.007241272465762449, Final Batch Loss: 0.00011032481415895745\n",
      "Epoch 4149, Loss: 0.002147311018234177, Final Batch Loss: 5.8855643146671355e-05\n",
      "Epoch 4150, Loss: 0.0009689713201623817, Final Batch Loss: 0.00019674206851050258\n",
      "Epoch 4151, Loss: 0.06305744835117366, Final Batch Loss: 5.82731154281646e-05\n",
      "Epoch 4152, Loss: 0.021330770477106853, Final Batch Loss: 2.0915114873787388e-05\n",
      "Epoch 4153, Loss: 0.022981249501754064, Final Batch Loss: 2.8444859708542936e-05\n",
      "Epoch 4154, Loss: 0.01865153285325505, Final Batch Loss: 0.006058056373149157\n",
      "Epoch 4155, Loss: 0.00600108730395732, Final Batch Loss: 7.339434523601085e-05\n",
      "Epoch 4156, Loss: 0.028427691455362947, Final Batch Loss: 0.004694449715316296\n",
      "Epoch 4157, Loss: 0.014857157755614026, Final Batch Loss: 0.007920227013528347\n",
      "Epoch 4158, Loss: 0.014487220680166502, Final Batch Loss: 0.0003539038880262524\n",
      "Epoch 4159, Loss: 0.019535668619937496, Final Batch Loss: 3.542069680406712e-05\n",
      "Epoch 4160, Loss: 0.011488514603115618, Final Batch Loss: 6.845438474556431e-05\n",
      "Epoch 4161, Loss: 0.017477788609539857, Final Batch Loss: 0.00021601446496788412\n",
      "Epoch 4162, Loss: 0.005874690533687499, Final Batch Loss: 1.0699525773816276e-05\n",
      "Epoch 4163, Loss: 0.004548161326965783, Final Batch Loss: 2.6129064281121828e-05\n",
      "Epoch 4164, Loss: 0.0034053565209433145, Final Batch Loss: 0.0013723886804655194\n",
      "Epoch 4165, Loss: 0.02871233613041113, Final Batch Loss: 1.5471030565095134e-05\n",
      "Epoch 4166, Loss: 0.020281192902984913, Final Batch Loss: 3.9019319956423715e-05\n",
      "Epoch 4167, Loss: 0.02574868892588711, Final Batch Loss: 0.00019273206999059767\n",
      "Epoch 4168, Loss: 0.004927413385303225, Final Batch Loss: 0.0001641422713873908\n",
      "Epoch 4169, Loss: 0.018836363446098403, Final Batch Loss: 4.0376333345193416e-05\n",
      "Epoch 4170, Loss: 0.013736774926655926, Final Batch Loss: 0.0007925161044113338\n",
      "Epoch 4171, Loss: 0.03232621202914743, Final Batch Loss: 0.000250309647526592\n",
      "Epoch 4172, Loss: 0.09512968330818694, Final Batch Loss: 0.00034787211916409433\n",
      "Epoch 4173, Loss: 0.13374940187713946, Final Batch Loss: 0.04431711882352829\n",
      "Epoch 4174, Loss: 0.12017494618339697, Final Batch Loss: 0.012849324382841587\n",
      "Epoch 4175, Loss: 0.03618398068647366, Final Batch Loss: 0.00015610922127962112\n",
      "Epoch 4176, Loss: 0.021696408541174605, Final Batch Loss: 0.00018343963893130422\n",
      "Epoch 4177, Loss: 0.03646665834094165, Final Batch Loss: 0.0008683530031703413\n",
      "Epoch 4178, Loss: 0.024882522600819357, Final Batch Loss: 0.0003171577991452068\n",
      "Epoch 4179, Loss: 0.017808160519052763, Final Batch Loss: 0.004537312313914299\n",
      "Epoch 4180, Loss: 0.018040564651528257, Final Batch Loss: 0.0015614146832376719\n",
      "Epoch 4181, Loss: 0.005692572289262898, Final Batch Loss: 0.00015661878569517285\n",
      "Epoch 4182, Loss: 0.015190014775726013, Final Batch Loss: 0.0016618689987808466\n",
      "Epoch 4183, Loss: 0.03098609054359258, Final Batch Loss: 0.00022740461281500757\n",
      "Epoch 4184, Loss: 0.01704697125023813, Final Batch Loss: 0.0028572806622833014\n",
      "Epoch 4185, Loss: 0.007606180515722372, Final Batch Loss: 4.7460885980399325e-05\n",
      "Epoch 4186, Loss: 0.0060214182012714446, Final Batch Loss: 0.0011022069957107306\n",
      "Epoch 4187, Loss: 0.011773218353482662, Final Batch Loss: 7.449716940755025e-05\n",
      "Epoch 4188, Loss: 0.013690534591660253, Final Batch Loss: 0.0001585038407938555\n",
      "Epoch 4189, Loss: 0.02320614100244711, Final Batch Loss: 0.00023112849157769233\n",
      "Epoch 4190, Loss: 0.01680604467037483, Final Batch Loss: 0.00015699352661613375\n",
      "Epoch 4191, Loss: 0.05492263318228652, Final Batch Loss: 8.608758071204647e-05\n",
      "Epoch 4192, Loss: 0.0382106074684998, Final Batch Loss: 0.0031479874160140753\n",
      "Epoch 4193, Loss: 0.026464341484825127, Final Batch Loss: 0.00011287976667517796\n",
      "Epoch 4194, Loss: 0.130831226677401, Final Batch Loss: 0.0012793957721441984\n",
      "Epoch 4195, Loss: 0.03977675474015996, Final Batch Loss: 0.0012003477895632386\n",
      "Epoch 4196, Loss: 0.016477921861223876, Final Batch Loss: 0.00016582001990173012\n",
      "Epoch 4197, Loss: 0.024539115773222875, Final Batch Loss: 0.00017265860515180975\n",
      "Epoch 4198, Loss: 0.033415803030948155, Final Batch Loss: 0.022022061049938202\n",
      "Epoch 4199, Loss: 0.0576616025846306, Final Batch Loss: 0.0001355339918518439\n",
      "Epoch 4200, Loss: 0.010993620697263395, Final Batch Loss: 4.585615170071833e-05\n",
      "Epoch 4201, Loss: 0.02336733620359155, Final Batch Loss: 0.0010258174734190106\n",
      "Epoch 4202, Loss: 0.009080486506718444, Final Batch Loss: 0.00047334886039607227\n",
      "Epoch 4203, Loss: 0.012337519336142577, Final Batch Loss: 8.51330187288113e-05\n",
      "Epoch 4204, Loss: 0.010546942914515967, Final Batch Loss: 0.0002656391297932714\n",
      "Epoch 4205, Loss: 0.011361099046553136, Final Batch Loss: 0.0006117450539022684\n",
      "Epoch 4206, Loss: 0.14411640481921495, Final Batch Loss: 8.091633208096027e-05\n",
      "Epoch 4207, Loss: 0.03890717413742095, Final Batch Loss: 9.943247277988121e-05\n",
      "Epoch 4208, Loss: 0.022651405801298097, Final Batch Loss: 0.0012724139960482717\n",
      "Epoch 4209, Loss: 0.028987806028453633, Final Batch Loss: 0.008926286362111568\n",
      "Epoch 4210, Loss: 0.01016446303401608, Final Batch Loss: 0.0002265790681121871\n",
      "Epoch 4211, Loss: 0.0054540619312319905, Final Batch Loss: 5.987413533148356e-05\n",
      "Epoch 4212, Loss: 0.011051994551962707, Final Batch Loss: 0.002940161619335413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4213, Loss: 0.013013866633627913, Final Batch Loss: 5.2322211558930576e-05\n",
      "Epoch 4214, Loss: 0.00848091700754594, Final Batch Loss: 0.000316107296384871\n",
      "Epoch 4215, Loss: 0.020962325081200106, Final Batch Loss: 0.0003456426493357867\n",
      "Epoch 4216, Loss: 0.04392297270169365, Final Batch Loss: 0.013185698539018631\n",
      "Epoch 4217, Loss: 0.032606145899990224, Final Batch Loss: 4.8907964810496196e-05\n",
      "Epoch 4218, Loss: 0.015189042111160234, Final Batch Loss: 0.00018701575754676014\n",
      "Epoch 4219, Loss: 0.018241779253003187, Final Batch Loss: 0.006400784943252802\n",
      "Epoch 4220, Loss: 0.02032214539940469, Final Batch Loss: 4.31504231528379e-05\n",
      "Epoch 4221, Loss: 0.025678741596493637, Final Batch Loss: 0.00018061480659525841\n",
      "Epoch 4222, Loss: 0.013400425184954656, Final Batch Loss: 9.476999548496678e-05\n",
      "Epoch 4223, Loss: 0.023038508188619744, Final Batch Loss: 9.66109219007194e-05\n",
      "Epoch 4224, Loss: 0.06782254560675938, Final Batch Loss: 0.0005382009549066424\n",
      "Epoch 4225, Loss: 0.13894876624908647, Final Batch Loss: 0.023363031446933746\n",
      "Epoch 4226, Loss: 0.11996142415955546, Final Batch Loss: 0.00014948136231396347\n",
      "Epoch 4227, Loss: 0.07997547629202018, Final Batch Loss: 0.0004323774774093181\n",
      "Epoch 4228, Loss: 0.00884542540734401, Final Batch Loss: 0.0009824188891798258\n",
      "Epoch 4229, Loss: 0.013829142350004986, Final Batch Loss: 0.0013566372217610478\n",
      "Epoch 4230, Loss: 0.06390154534165049, Final Batch Loss: 0.0003215040487702936\n",
      "Epoch 4231, Loss: 0.009704765499918722, Final Batch Loss: 0.00029178769909776747\n",
      "Epoch 4232, Loss: 0.007978835466929013, Final Batch Loss: 0.001438251230865717\n",
      "Epoch 4233, Loss: 0.019698539086675737, Final Batch Loss: 0.00014471171016339213\n",
      "Epoch 4234, Loss: 0.006109561880293768, Final Batch Loss: 0.0005612650420516729\n",
      "Epoch 4235, Loss: 0.010257536501740105, Final Batch Loss: 0.00026013306342065334\n",
      "Epoch 4236, Loss: 0.019734801890081144, Final Batch Loss: 0.00021186255617067218\n",
      "Epoch 4237, Loss: 0.016832405439345166, Final Batch Loss: 6.888261850690469e-05\n",
      "Epoch 4238, Loss: 0.016837388582644053, Final Batch Loss: 0.0004050637944601476\n",
      "Epoch 4239, Loss: 0.021483596319740172, Final Batch Loss: 0.0010531558655202389\n",
      "Epoch 4240, Loss: 0.012367913595880964, Final Batch Loss: 0.00015465568867512047\n",
      "Epoch 4241, Loss: 0.027792655819212086, Final Batch Loss: 0.0005454026395455003\n",
      "Epoch 4242, Loss: 0.005514621458132751, Final Batch Loss: 0.00016309412603732198\n",
      "Epoch 4243, Loss: 0.011293767500319518, Final Batch Loss: 0.00017913180636242032\n",
      "Epoch 4244, Loss: 0.003399821905986755, Final Batch Loss: 5.23573107784614e-05\n",
      "Epoch 4245, Loss: 0.0014114311852608807, Final Batch Loss: 9.790997864911333e-05\n",
      "Epoch 4246, Loss: 0.013694328793462773, Final Batch Loss: 2.1604062567348592e-05\n",
      "Epoch 4247, Loss: 0.013549903553212062, Final Batch Loss: 0.00025834343978203833\n",
      "Epoch 4248, Loss: 0.0036595903438865207, Final Batch Loss: 4.236039239913225e-05\n",
      "Epoch 4249, Loss: 0.002217028809354815, Final Batch Loss: 0.00010180543176829815\n",
      "Epoch 4250, Loss: 0.018956561613777012, Final Batch Loss: 0.000324935739627108\n",
      "Epoch 4251, Loss: 0.004434839964233106, Final Batch Loss: 7.839306636014953e-05\n",
      "Epoch 4252, Loss: 0.00235394019728119, Final Batch Loss: 2.593664794403594e-05\n",
      "Epoch 4253, Loss: 0.01435676889741444, Final Batch Loss: 0.00033298012567684054\n",
      "Epoch 4254, Loss: 0.00804140373111295, Final Batch Loss: 3.523181294440292e-05\n",
      "Epoch 4255, Loss: 0.00499792469690874, Final Batch Loss: 3.920343078789301e-05\n",
      "Epoch 4256, Loss: 0.006052039047972357, Final Batch Loss: 2.762239455478266e-05\n",
      "Epoch 4257, Loss: 0.012244381486198108, Final Batch Loss: 9.543457417748868e-05\n",
      "Epoch 4258, Loss: 0.005264815743430518, Final Batch Loss: 1.127673931478057e-05\n",
      "Epoch 4259, Loss: 0.005743221177908708, Final Batch Loss: 6.761621625628322e-05\n",
      "Epoch 4260, Loss: 0.017789128067306592, Final Batch Loss: 0.00017434694746043533\n",
      "Epoch 4261, Loss: 0.007156653153288062, Final Batch Loss: 8.048843301367015e-05\n",
      "Epoch 4262, Loss: 0.009441843518743553, Final Batch Loss: 4.2501476855250075e-05\n",
      "Epoch 4263, Loss: 0.0010659442468750058, Final Batch Loss: 0.0001598381786607206\n",
      "Epoch 4264, Loss: 0.003471164758593659, Final Batch Loss: 0.0001226979511557147\n",
      "Epoch 4265, Loss: 0.020988635829780833, Final Batch Loss: 5.062637137598358e-05\n",
      "Epoch 4266, Loss: 0.011872517478877853, Final Batch Loss: 6.99093216098845e-05\n",
      "Epoch 4267, Loss: 0.008149220205268648, Final Batch Loss: 5.0793660193448886e-05\n",
      "Epoch 4268, Loss: 0.009460041500460648, Final Batch Loss: 0.0023359241895377636\n",
      "Epoch 4269, Loss: 0.014646820981397468, Final Batch Loss: 7.678703696001321e-05\n",
      "Epoch 4270, Loss: 0.02999571949658275, Final Batch Loss: 1.8153441487811506e-05\n",
      "Epoch 4271, Loss: 0.02854714889326715, Final Batch Loss: 0.0004818735469598323\n",
      "Epoch 4272, Loss: 0.009258169955501216, Final Batch Loss: 0.00018383582937531173\n",
      "Epoch 4273, Loss: 0.013791266974294558, Final Batch Loss: 0.00027365516871213913\n",
      "Epoch 4274, Loss: 0.00988507622605539, Final Batch Loss: 6.064473564038053e-05\n",
      "Epoch 4275, Loss: 0.010215241239166062, Final Batch Loss: 6.19030834059231e-05\n",
      "Epoch 4276, Loss: 0.005822967868880369, Final Batch Loss: 0.00011980213457718492\n",
      "Epoch 4277, Loss: 0.004897331372831104, Final Batch Loss: 3.638393172877841e-05\n",
      "Epoch 4278, Loss: 0.0010840045792974706, Final Batch Loss: 8.459759555989876e-06\n",
      "Epoch 4279, Loss: 0.0037370346835814416, Final Batch Loss: 0.00015979005547706038\n",
      "Epoch 4280, Loss: 0.01936099605700292, Final Batch Loss: 0.0019145191181451082\n",
      "Epoch 4281, Loss: 0.0032514195208932506, Final Batch Loss: 4.692822403740138e-05\n",
      "Epoch 4282, Loss: 0.0012433502906787908, Final Batch Loss: 4.81287352158688e-05\n",
      "Epoch 4283, Loss: 0.019262089153926354, Final Batch Loss: 8.47500778036192e-05\n",
      "Epoch 4284, Loss: 0.026665675741242012, Final Batch Loss: 0.0020665007177740335\n",
      "Epoch 4285, Loss: 0.0387260255520232, Final Batch Loss: 0.01969367265701294\n",
      "Epoch 4286, Loss: 0.05859160830550536, Final Batch Loss: 0.0003494518168736249\n",
      "Epoch 4287, Loss: 0.022624605562668876, Final Batch Loss: 0.0007598065421916544\n",
      "Epoch 4288, Loss: 0.011521410968271084, Final Batch Loss: 9.76580849965103e-05\n",
      "Epoch 4289, Loss: 0.02001243129689101, Final Batch Loss: 0.00011977852409472689\n",
      "Epoch 4290, Loss: 0.006545080629621225, Final Batch Loss: 0.0019589646253734827\n",
      "Epoch 4291, Loss: 0.0038531535583388177, Final Batch Loss: 1.518617500551045e-05\n",
      "Epoch 4292, Loss: 0.00758541792674805, Final Batch Loss: 5.0342110625933856e-05\n",
      "Epoch 4293, Loss: 0.025271315851568943, Final Batch Loss: 4.2404044506838545e-05\n",
      "Epoch 4294, Loss: 0.03221644863879192, Final Batch Loss: 3.7813362723682076e-05\n",
      "Epoch 4295, Loss: 0.03999820170838575, Final Batch Loss: 0.01680564694106579\n",
      "Epoch 4296, Loss: 0.010703136809752323, Final Batch Loss: 2.3836064428905956e-05\n",
      "Epoch 4297, Loss: 0.0026502507134864572, Final Batch Loss: 5.738900290452875e-05\n",
      "Epoch 4298, Loss: 0.027246195762018033, Final Batch Loss: 0.00010229461622657254\n",
      "Epoch 4299, Loss: 0.013408581012299692, Final Batch Loss: 0.0001525681436760351\n",
      "Epoch 4300, Loss: 0.005184386143810116, Final Batch Loss: 0.0013422477059066296\n",
      "Epoch 4301, Loss: 0.013892527958887513, Final Batch Loss: 0.0022444347850978374\n",
      "Epoch 4302, Loss: 0.02135378814273281, Final Batch Loss: 0.00011422319221310318\n",
      "Epoch 4303, Loss: 0.003812560394180764, Final Batch Loss: 0.0003468494687695056\n",
      "Epoch 4304, Loss: 0.004592694378516171, Final Batch Loss: 0.00015041192818898708\n",
      "Epoch 4305, Loss: 0.003281008119301987, Final Batch Loss: 3.9512473449576646e-05\n",
      "Epoch 4306, Loss: 0.002122642539688968, Final Batch Loss: 1.9362070815986954e-05\n",
      "Epoch 4307, Loss: 0.003259457555031986, Final Batch Loss: 0.0005437053623609245\n",
      "Epoch 4308, Loss: 0.0029768056228931528, Final Batch Loss: 0.0010215440997853875\n",
      "Epoch 4309, Loss: 0.002847629690222675, Final Batch Loss: 8.899116437532939e-06\n",
      "Epoch 4310, Loss: 0.005271539967907302, Final Batch Loss: 0.00014845716941636056\n",
      "Epoch 4311, Loss: 0.0031243873122548393, Final Batch Loss: 0.00013490827404893935\n",
      "Epoch 4312, Loss: 0.020457696352423227, Final Batch Loss: 0.0034331241622567177\n",
      "Epoch 4313, Loss: 0.1255543994011532, Final Batch Loss: 0.004459094721823931\n",
      "Epoch 4314, Loss: 0.013380766315094661, Final Batch Loss: 0.0001628565660212189\n",
      "Epoch 4315, Loss: 0.01079391048369871, Final Batch Loss: 0.0005688382079824805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4316, Loss: 0.011724812195097911, Final Batch Loss: 0.00011361460201442242\n",
      "Epoch 4317, Loss: 0.005880999686269206, Final Batch Loss: 0.0014176898403093219\n",
      "Epoch 4318, Loss: 0.015962931196554564, Final Batch Loss: 0.0009205987444147468\n",
      "Epoch 4319, Loss: 0.019282558341728873, Final Batch Loss: 1.2006657925667241e-05\n",
      "Epoch 4320, Loss: 0.002964911596791353, Final Batch Loss: 0.00013205921277403831\n",
      "Epoch 4321, Loss: 0.0062778784395050025, Final Batch Loss: 3.880658914567903e-05\n",
      "Epoch 4322, Loss: 0.006505244570689683, Final Batch Loss: 2.176857196900528e-05\n",
      "Epoch 4323, Loss: 0.0073965717065220815, Final Batch Loss: 1.4478170669462997e-05\n",
      "Epoch 4324, Loss: 0.001678560383425065, Final Batch Loss: 7.141836704249727e-06\n",
      "Epoch 4325, Loss: 0.005161809667697526, Final Batch Loss: 6.375418888637796e-05\n",
      "Epoch 4326, Loss: 0.009726676364152809, Final Batch Loss: 0.00013239892723504454\n",
      "Epoch 4327, Loss: 0.005851959089341108, Final Batch Loss: 0.0024689645506441593\n",
      "Epoch 4328, Loss: 0.004967088820194476, Final Batch Loss: 0.00011644457845250145\n",
      "Epoch 4329, Loss: 0.010548153862146137, Final Batch Loss: 0.0004892787546850741\n",
      "Epoch 4330, Loss: 0.028612213569431333, Final Batch Loss: 0.000762968382332474\n",
      "Epoch 4331, Loss: 0.0034850272932089865, Final Batch Loss: 0.00017454230692237616\n",
      "Epoch 4332, Loss: 0.0193330711354065, Final Batch Loss: 1.2643312402360607e-05\n",
      "Epoch 4333, Loss: 0.012718823537852586, Final Batch Loss: 5.7583078159950674e-05\n",
      "Epoch 4334, Loss: 0.082115567944129, Final Batch Loss: 9.788665920495987e-05\n",
      "Epoch 4335, Loss: 0.0760925743234111, Final Batch Loss: 0.00023586832685396075\n",
      "Epoch 4336, Loss: 0.043540689104702324, Final Batch Loss: 0.020921999588608742\n",
      "Epoch 4337, Loss: 0.03344348049722612, Final Batch Loss: 8.053302008192986e-05\n",
      "Epoch 4338, Loss: 0.01568859172402881, Final Batch Loss: 0.0010173094924539328\n",
      "Epoch 4339, Loss: 0.12658635505795246, Final Batch Loss: 0.00013192348706070334\n",
      "Epoch 4340, Loss: 0.10129014875383291, Final Batch Loss: 0.0001728297065710649\n",
      "Epoch 4341, Loss: 0.13132441166817443, Final Batch Loss: 0.0010769786313176155\n",
      "Epoch 4342, Loss: 0.11901827098336071, Final Batch Loss: 0.0005928517784923315\n",
      "Epoch 4343, Loss: 0.07209069409873337, Final Batch Loss: 0.016125839203596115\n",
      "Epoch 4344, Loss: 0.07099178075077361, Final Batch Loss: 0.0013129805447533727\n",
      "Epoch 4345, Loss: 0.05959574430016801, Final Batch Loss: 0.016347277909517288\n",
      "Epoch 4346, Loss: 0.04651009867666289, Final Batch Loss: 0.007445679046213627\n",
      "Epoch 4347, Loss: 0.06230709304509219, Final Batch Loss: 9.708601282909513e-05\n",
      "Epoch 4348, Loss: 0.06917740359494928, Final Batch Loss: 0.002250971272587776\n",
      "Epoch 4349, Loss: 0.049682577620842494, Final Batch Loss: 0.0010492438450455666\n",
      "Epoch 4350, Loss: 0.04321125100250356, Final Batch Loss: 0.00017812858277466148\n",
      "Epoch 4351, Loss: 0.04501505362713942, Final Batch Loss: 0.0008284926880151033\n",
      "Epoch 4352, Loss: 0.012745922649628483, Final Batch Loss: 0.00023012765450403094\n",
      "Epoch 4353, Loss: 0.022810175665654242, Final Batch Loss: 0.0004913783632218838\n",
      "Epoch 4354, Loss: 0.014643142458226066, Final Batch Loss: 4.941989027429372e-05\n",
      "Epoch 4355, Loss: 0.007348219063715078, Final Batch Loss: 0.00019607039575930685\n",
      "Epoch 4356, Loss: 0.011159274821693543, Final Batch Loss: 0.0006393133080564439\n",
      "Epoch 4357, Loss: 0.04932785325945588, Final Batch Loss: 0.00026608744519762695\n",
      "Epoch 4358, Loss: 0.022252199156355346, Final Batch Loss: 0.0025868515949696302\n",
      "Epoch 4359, Loss: 0.015572802236420102, Final Batch Loss: 0.00033215913572348654\n",
      "Epoch 4360, Loss: 0.02891353100494598, Final Batch Loss: 0.00017867454153019935\n",
      "Epoch 4361, Loss: 0.032355540119169746, Final Batch Loss: 0.0005057523376308382\n",
      "Epoch 4362, Loss: 0.012668745541304816, Final Batch Loss: 0.0017186910845339298\n",
      "Epoch 4363, Loss: 0.018394034399534576, Final Batch Loss: 9.920246520778164e-05\n",
      "Epoch 4364, Loss: 0.031024790143419523, Final Batch Loss: 0.0008055923390202224\n",
      "Epoch 4365, Loss: 0.020936711751346593, Final Batch Loss: 0.0006746824947185814\n",
      "Epoch 4366, Loss: 0.016578249716985738, Final Batch Loss: 0.0006705895066261292\n",
      "Epoch 4367, Loss: 0.004593035300786141, Final Batch Loss: 0.0002481300907675177\n",
      "Epoch 4368, Loss: 0.015962679266522173, Final Batch Loss: 0.0001222109276568517\n",
      "Epoch 4369, Loss: 0.020333744025265332, Final Batch Loss: 0.00023032492026686668\n",
      "Epoch 4370, Loss: 0.012826952674004133, Final Batch Loss: 0.00023452320601791143\n",
      "Epoch 4371, Loss: 0.010581717055174522, Final Batch Loss: 0.002285221591591835\n",
      "Epoch 4372, Loss: 0.005289275331051613, Final Batch Loss: 0.0002726153179537505\n",
      "Epoch 4373, Loss: 0.03221545552696625, Final Batch Loss: 4.587507282849401e-05\n",
      "Epoch 4374, Loss: 0.009686761168268276, Final Batch Loss: 0.0025472817942500114\n",
      "Epoch 4375, Loss: 0.01534072727736202, Final Batch Loss: 0.0005070898914709687\n",
      "Epoch 4376, Loss: 0.01456093503293232, Final Batch Loss: 0.00026756501756608486\n",
      "Epoch 4377, Loss: 0.06535243218058895, Final Batch Loss: 9.6548130386509e-05\n",
      "Epoch 4378, Loss: 0.006469650337749044, Final Batch Loss: 5.84511217311956e-05\n",
      "Epoch 4379, Loss: 0.011374478352081496, Final Batch Loss: 0.00044778373558074236\n",
      "Epoch 4380, Loss: 0.009279327596232179, Final Batch Loss: 7.287951302714646e-05\n",
      "Epoch 4381, Loss: 0.006405283020285424, Final Batch Loss: 0.00011353453737683594\n",
      "Epoch 4382, Loss: 0.002153206430193677, Final Batch Loss: 4.052920121466741e-05\n",
      "Epoch 4383, Loss: 0.012249286171936546, Final Batch Loss: 3.4416578273521736e-05\n",
      "Epoch 4384, Loss: 0.013161257431420381, Final Batch Loss: 0.00025881914189085364\n",
      "Epoch 4385, Loss: 0.025116590473771794, Final Batch Loss: 0.00035340519389137626\n",
      "Epoch 4386, Loss: 0.016578304861468496, Final Batch Loss: 0.0011088919127359986\n",
      "Epoch 4387, Loss: 0.10938677271769848, Final Batch Loss: 0.00952516682446003\n",
      "Epoch 4388, Loss: 0.5959614498715382, Final Batch Loss: 0.07922746241092682\n",
      "Epoch 4389, Loss: 0.19659048286848702, Final Batch Loss: 0.017277652397751808\n",
      "Epoch 4390, Loss: 0.06217079650377855, Final Batch Loss: 0.002087235916405916\n",
      "Epoch 4391, Loss: 0.06266178755322471, Final Batch Loss: 0.001789974281564355\n",
      "Epoch 4392, Loss: 0.03247582045150921, Final Batch Loss: 0.0034173179883509874\n",
      "Epoch 4393, Loss: 0.0358285648162564, Final Batch Loss: 0.007105870172381401\n",
      "Epoch 4394, Loss: 0.01133782604301814, Final Batch Loss: 0.0005901831318624318\n",
      "Epoch 4395, Loss: 0.03429525210231077, Final Batch Loss: 0.0011358453193679452\n",
      "Epoch 4396, Loss: 0.04098736125979485, Final Batch Loss: 0.000363066210411489\n",
      "Epoch 4397, Loss: 0.018514252282329835, Final Batch Loss: 0.00015523562615271658\n",
      "Epoch 4398, Loss: 0.018570691157947294, Final Batch Loss: 0.0007737584528513253\n",
      "Epoch 4399, Loss: 0.03464494156287401, Final Batch Loss: 0.006035290192812681\n",
      "Epoch 4400, Loss: 0.021108483600983163, Final Batch Loss: 0.00015738946967758238\n",
      "Epoch 4401, Loss: 0.024570732977736043, Final Batch Loss: 0.0002904050634242594\n",
      "Epoch 4402, Loss: 0.030231657903641462, Final Batch Loss: 0.00033316927147097886\n",
      "Epoch 4403, Loss: 0.009583727714925772, Final Batch Loss: 0.0001232400827575475\n",
      "Epoch 4404, Loss: 0.02220730949920835, Final Batch Loss: 0.001145625370554626\n",
      "Epoch 4405, Loss: 0.015183642095507821, Final Batch Loss: 5.4429103329312056e-05\n",
      "Epoch 4406, Loss: 0.05554002081407816, Final Batch Loss: 0.0003288564330432564\n",
      "Epoch 4407, Loss: 0.010086442191095557, Final Batch Loss: 0.00035554339410737157\n",
      "Epoch 4408, Loss: 0.011320947374770185, Final Batch Loss: 0.00012266715930309147\n",
      "Epoch 4409, Loss: 0.03470485776051646, Final Batch Loss: 0.0008971968782134354\n",
      "Epoch 4410, Loss: 0.04292497638380155, Final Batch Loss: 0.0011057675583288074\n",
      "Epoch 4411, Loss: 0.012649433525439235, Final Batch Loss: 0.0019748155027627945\n",
      "Epoch 4412, Loss: 0.021437203933601268, Final Batch Loss: 0.0006177283357828856\n",
      "Epoch 4413, Loss: 0.016801675024908036, Final Batch Loss: 0.00028814550023525953\n",
      "Epoch 4414, Loss: 0.03072250490731676, Final Batch Loss: 0.0004641219275072217\n",
      "Epoch 4415, Loss: 0.031509926568105584, Final Batch Loss: 0.005101734772324562\n",
      "Epoch 4416, Loss: 0.0054192459319892805, Final Batch Loss: 4.214871296426281e-05\n",
      "Epoch 4417, Loss: 0.008766451239353046, Final Batch Loss: 0.003888777457177639\n",
      "Epoch 4418, Loss: 0.018378695920546306, Final Batch Loss: 0.0007053947192616761\n",
      "Epoch 4419, Loss: 0.00594453248049831, Final Batch Loss: 0.00019104110833723098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4420, Loss: 0.02607447256013984, Final Batch Loss: 0.0004588442388921976\n",
      "Epoch 4421, Loss: 0.017169745678984327, Final Batch Loss: 0.00018163835920859128\n",
      "Epoch 4422, Loss: 0.012691609430476092, Final Batch Loss: 0.00010592783655738458\n",
      "Epoch 4423, Loss: 0.038919383761822246, Final Batch Loss: 0.000930889742448926\n",
      "Epoch 4424, Loss: 0.00920545712688181, Final Batch Loss: 5.780404171673581e-05\n",
      "Epoch 4425, Loss: 0.004111315754016687, Final Batch Loss: 0.00046875554835423827\n",
      "Epoch 4426, Loss: 0.0467553923590458, Final Batch Loss: 0.0007761045708321035\n",
      "Epoch 4427, Loss: 0.010246923313388834, Final Batch Loss: 5.91003117733635e-05\n",
      "Epoch 4428, Loss: 0.009988883905862167, Final Batch Loss: 0.00029778669704683125\n",
      "Epoch 4429, Loss: 0.006191345546540106, Final Batch Loss: 0.00011477294901851565\n",
      "Epoch 4430, Loss: 0.042411777747474844, Final Batch Loss: 0.0015847324393689632\n",
      "Epoch 4431, Loss: 0.013870841085008578, Final Batch Loss: 9.631537977838889e-05\n",
      "Epoch 4432, Loss: 0.004913339296763297, Final Batch Loss: 0.0003122819180134684\n",
      "Epoch 4433, Loss: 0.006562864347870345, Final Batch Loss: 0.0006167962565086782\n",
      "Epoch 4434, Loss: 0.005102963557874318, Final Batch Loss: 0.00010963033855659887\n",
      "Epoch 4435, Loss: 0.042261551359843, Final Batch Loss: 0.0007649277686141431\n",
      "Epoch 4436, Loss: 0.007360250102465216, Final Batch Loss: 2.3279228116734885e-05\n",
      "Epoch 4437, Loss: 0.022873227781929018, Final Batch Loss: 6.895702244946733e-05\n",
      "Epoch 4438, Loss: 0.058768037455593, Final Batch Loss: 5.3183593990979716e-05\n",
      "Epoch 4439, Loss: 0.026369271261501126, Final Batch Loss: 6.537661829497665e-05\n",
      "Epoch 4440, Loss: 0.11507512354728533, Final Batch Loss: 0.0003519579186104238\n",
      "Epoch 4441, Loss: 0.02607235672621755, Final Batch Loss: 0.0008492432534694672\n",
      "Epoch 4442, Loss: 0.03590139034349704, Final Batch Loss: 0.001462149666622281\n",
      "Epoch 4443, Loss: 0.06268518519027566, Final Batch Loss: 0.00024247250985354185\n",
      "Epoch 4444, Loss: 0.008720368784906896, Final Batch Loss: 0.0039545390754938126\n",
      "Epoch 4445, Loss: 0.01238110435224371, Final Batch Loss: 0.0006294584600254893\n",
      "Epoch 4446, Loss: 0.006136105417681392, Final Batch Loss: 0.0004416683514136821\n",
      "Epoch 4447, Loss: 0.0074637667057686485, Final Batch Loss: 0.00021731655579060316\n",
      "Epoch 4448, Loss: 0.006193827777678962, Final Batch Loss: 7.744605682091787e-05\n",
      "Epoch 4449, Loss: 0.012678942937782267, Final Batch Loss: 0.0007290128851309419\n",
      "Epoch 4450, Loss: 0.003013263357388496, Final Batch Loss: 0.00019685979350470006\n",
      "Epoch 4451, Loss: 0.018825194252713118, Final Batch Loss: 4.005886148661375e-05\n",
      "Epoch 4452, Loss: 0.0060064050085202325, Final Batch Loss: 0.00018867303151637316\n",
      "Epoch 4453, Loss: 0.008207657065213425, Final Batch Loss: 0.001246469677425921\n",
      "Epoch 4454, Loss: 0.0056298651925317245, Final Batch Loss: 8.414062904193997e-05\n",
      "Epoch 4455, Loss: 0.017031382169079734, Final Batch Loss: 0.0001169799143099226\n",
      "Epoch 4456, Loss: 0.013929253625974525, Final Batch Loss: 8.808652637526393e-05\n",
      "Epoch 4457, Loss: 0.0038767175810789922, Final Batch Loss: 0.00028976707835681736\n",
      "Epoch 4458, Loss: 0.011346267624503525, Final Batch Loss: 0.00015523438923992217\n",
      "Epoch 4459, Loss: 0.0029732748935202835, Final Batch Loss: 7.489831477869302e-05\n",
      "Epoch 4460, Loss: 0.004538147012681293, Final Batch Loss: 0.00010515987378312275\n",
      "Epoch 4461, Loss: 0.009803324231597799, Final Batch Loss: 0.00010937096521956846\n",
      "Epoch 4462, Loss: 0.002975675081870577, Final Batch Loss: 0.0008451819303445518\n",
      "Epoch 4463, Loss: 0.0023964227880242106, Final Batch Loss: 5.765176410932327e-06\n",
      "Epoch 4464, Loss: 0.03278797789334931, Final Batch Loss: 4.5380274968920276e-05\n",
      "Epoch 4465, Loss: 0.014897324269441015, Final Batch Loss: 2.3311182303586975e-05\n",
      "Epoch 4466, Loss: 0.05130465690763231, Final Batch Loss: 0.0003761736152227968\n",
      "Epoch 4467, Loss: 0.004552135902486043, Final Batch Loss: 0.0004490430583246052\n",
      "Epoch 4468, Loss: 0.012827794857003028, Final Batch Loss: 4.338999860920012e-05\n",
      "Epoch 4469, Loss: 0.018886168870267284, Final Batch Loss: 0.0004164043057244271\n",
      "Epoch 4470, Loss: 0.02061512517684605, Final Batch Loss: 0.0030871371272951365\n",
      "Epoch 4471, Loss: 0.0179004623551009, Final Batch Loss: 3.854827446048148e-05\n",
      "Epoch 4472, Loss: 0.007173842793235963, Final Batch Loss: 7.599080709042028e-05\n",
      "Epoch 4473, Loss: 0.004955713638992165, Final Batch Loss: 4.548734432319179e-05\n",
      "Epoch 4474, Loss: 0.009298499852775421, Final Batch Loss: 0.0003974060236942023\n",
      "Epoch 4475, Loss: 0.005541720168366737, Final Batch Loss: 7.294333772733808e-05\n",
      "Epoch 4476, Loss: 0.007373097295385378, Final Batch Loss: 2.7787884391727857e-05\n",
      "Epoch 4477, Loss: 0.0011045705605283729, Final Batch Loss: 3.79038683604449e-05\n",
      "Epoch 4478, Loss: 0.0167090628187907, Final Batch Loss: 3.4994245652342215e-05\n",
      "Epoch 4479, Loss: 0.0878062552492338, Final Batch Loss: 0.0011026376159861684\n",
      "Epoch 4480, Loss: 0.01179827676332934, Final Batch Loss: 8.172080561053008e-05\n",
      "Epoch 4481, Loss: 0.04207357184350258, Final Batch Loss: 0.02425425313413143\n",
      "Epoch 4482, Loss: 0.013301806127856253, Final Batch Loss: 0.00019444590725470334\n",
      "Epoch 4483, Loss: 0.054052409628639, Final Batch Loss: 0.00020047528960276395\n",
      "Epoch 4484, Loss: 0.033951362704101484, Final Batch Loss: 0.00028934472356922925\n",
      "Epoch 4485, Loss: 0.007147182044718647, Final Batch Loss: 0.002837574575096369\n",
      "Epoch 4486, Loss: 0.011397473754186649, Final Batch Loss: 0.00012406831956468523\n",
      "Epoch 4487, Loss: 0.018183297988116465, Final Batch Loss: 0.00854306947439909\n",
      "Epoch 4488, Loss: 0.010951879601634573, Final Batch Loss: 0.00017602949810680002\n",
      "Epoch 4489, Loss: 0.01068523637150065, Final Batch Loss: 0.0009183411602862179\n",
      "Epoch 4490, Loss: 0.010859609432372963, Final Batch Loss: 0.00040983816143125296\n",
      "Epoch 4491, Loss: 0.007912749131719465, Final Batch Loss: 0.0005209581577219069\n",
      "Epoch 4492, Loss: 0.0036521725492093537, Final Batch Loss: 0.00010873106657527387\n",
      "Epoch 4493, Loss: 0.011692054994455248, Final Batch Loss: 1.2509457519627176e-05\n",
      "Epoch 4494, Loss: 0.01582294068793999, Final Batch Loss: 0.00041670611244626343\n",
      "Epoch 4495, Loss: 0.014190024152412661, Final Batch Loss: 0.0001974459592020139\n",
      "Epoch 4496, Loss: 0.025279900393798016, Final Batch Loss: 0.00027423526626080275\n",
      "Epoch 4497, Loss: 0.002539451177653973, Final Batch Loss: 8.99469232535921e-05\n",
      "Epoch 4498, Loss: 0.018987781068517506, Final Batch Loss: 1.645253178139683e-05\n",
      "Epoch 4499, Loss: 0.008687699616530153, Final Batch Loss: 0.0003484421467874199\n",
      "Epoch 4500, Loss: 0.0053433556349773426, Final Batch Loss: 2.7026682801079005e-05\n",
      "Epoch 4501, Loss: 0.005711406939553854, Final Batch Loss: 3.9674065192230046e-05\n",
      "Epoch 4502, Loss: 0.007938725149870152, Final Batch Loss: 0.0015223519876599312\n",
      "Epoch 4503, Loss: 0.016199221610349923, Final Batch Loss: 2.3430795408785343e-05\n",
      "Epoch 4504, Loss: 0.058105291992433195, Final Batch Loss: 1.9004399291588925e-05\n",
      "Epoch 4505, Loss: 0.0078083705629978795, Final Batch Loss: 8.630737283965573e-05\n",
      "Epoch 4506, Loss: 0.018257255605021783, Final Batch Loss: 0.00011285243817837909\n",
      "Epoch 4507, Loss: 0.035738165061047766, Final Batch Loss: 0.0027228076942265034\n",
      "Epoch 4508, Loss: 0.015276864353836572, Final Batch Loss: 2.3118411263567396e-05\n",
      "Epoch 4509, Loss: 0.011453653702574229, Final Batch Loss: 7.313634705496952e-05\n",
      "Epoch 4510, Loss: 0.012038449578540167, Final Batch Loss: 0.003091089893132448\n",
      "Epoch 4511, Loss: 0.012837357003263605, Final Batch Loss: 0.00024171265249606222\n",
      "Epoch 4512, Loss: 0.02938811511876338, Final Batch Loss: 0.0035393170546740294\n",
      "Epoch 4513, Loss: 0.012834250826017524, Final Batch Loss: 0.007940244860947132\n",
      "Epoch 4514, Loss: 0.02176248259638669, Final Batch Loss: 0.011343750171363354\n",
      "Epoch 4515, Loss: 0.08997972033102997, Final Batch Loss: 0.0036028483882546425\n",
      "Epoch 4516, Loss: 0.12818764885741984, Final Batch Loss: 0.00012777885422110558\n",
      "Epoch 4517, Loss: 0.04167267722368706, Final Batch Loss: 0.00019893451826646924\n",
      "Epoch 4518, Loss: 0.005180744008612237, Final Batch Loss: 0.0006772007909603417\n",
      "Epoch 4519, Loss: 0.015733802021713927, Final Batch Loss: 0.0015296682249754667\n",
      "Epoch 4520, Loss: 0.025174095731927082, Final Batch Loss: 0.00022576432093046606\n",
      "Epoch 4521, Loss: 0.007657907419343246, Final Batch Loss: 0.00029727807850576937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4522, Loss: 0.03324606861679058, Final Batch Loss: 0.0002513130020815879\n",
      "Epoch 4523, Loss: 0.028391803217346023, Final Batch Loss: 7.422528142342344e-05\n",
      "Epoch 4524, Loss: 0.020489244705458987, Final Batch Loss: 3.512753755785525e-05\n",
      "Epoch 4525, Loss: 0.0055202144872055214, Final Batch Loss: 0.0020214717369526625\n",
      "Epoch 4526, Loss: 0.009689787673778483, Final Batch Loss: 0.00042582867899909616\n",
      "Epoch 4527, Loss: 0.012478485800784256, Final Batch Loss: 1.1880622878379654e-05\n",
      "Epoch 4528, Loss: 0.021493026216376165, Final Batch Loss: 0.005376626271754503\n",
      "Epoch 4529, Loss: 0.020356710478154127, Final Batch Loss: 1.5553023331449367e-05\n",
      "Epoch 4530, Loss: 0.044078387963963905, Final Batch Loss: 0.0003414083330426365\n",
      "Epoch 4531, Loss: 0.03593547734635649, Final Batch Loss: 0.0015513019170612097\n",
      "Epoch 4532, Loss: 0.019924290088056296, Final Batch Loss: 3.51344897353556e-05\n",
      "Epoch 4533, Loss: 0.0027935295838688035, Final Batch Loss: 1.8020864445134066e-05\n",
      "Epoch 4534, Loss: 0.042156432658885024, Final Batch Loss: 0.0025490233674645424\n",
      "Epoch 4535, Loss: 0.030890274653302185, Final Batch Loss: 0.00010588949953671545\n",
      "Epoch 4536, Loss: 0.01762094527202862, Final Batch Loss: 2.6782185159390792e-05\n",
      "Epoch 4537, Loss: 0.05866844823322026, Final Batch Loss: 9.572556882631034e-05\n",
      "Epoch 4538, Loss: 0.0350447976779833, Final Batch Loss: 0.0014215395785868168\n",
      "Epoch 4539, Loss: 0.008323128568008542, Final Batch Loss: 3.724979251273908e-05\n",
      "Epoch 4540, Loss: 0.015438915008417098, Final Batch Loss: 0.00048328653792850673\n",
      "Epoch 4541, Loss: 0.041395012048269564, Final Batch Loss: 4.451125278137624e-05\n",
      "Epoch 4542, Loss: 0.015163639720412903, Final Batch Loss: 0.00963972695171833\n",
      "Epoch 4543, Loss: 0.02018737143225735, Final Batch Loss: 0.0019754215609282255\n",
      "Epoch 4544, Loss: 0.058887601018795976, Final Batch Loss: 0.0003484284388832748\n",
      "Epoch 4545, Loss: 0.03329501340340357, Final Batch Loss: 0.0002989096683450043\n",
      "Epoch 4546, Loss: 0.008722927639610134, Final Batch Loss: 0.001162007451057434\n",
      "Epoch 4547, Loss: 0.018901003426435636, Final Batch Loss: 0.00017373351147398353\n",
      "Epoch 4548, Loss: 0.01449987426167354, Final Batch Loss: 0.0002857463841792196\n",
      "Epoch 4549, Loss: 0.010168972570681944, Final Batch Loss: 9.321698598796502e-05\n",
      "Epoch 4550, Loss: 0.004349929749878356, Final Batch Loss: 2.2314687157631852e-05\n",
      "Epoch 4551, Loss: 0.011180889396200655, Final Batch Loss: 9.003404557006434e-05\n",
      "Epoch 4552, Loss: 0.04435518043101183, Final Batch Loss: 0.0001523203100077808\n",
      "Epoch 4553, Loss: 0.030547339367331006, Final Batch Loss: 0.007959234528243542\n",
      "Epoch 4554, Loss: 0.02223626128397882, Final Batch Loss: 0.00041952566243708134\n",
      "Epoch 4555, Loss: 0.017181103336042725, Final Batch Loss: 3.737065344466828e-05\n",
      "Epoch 4556, Loss: 0.00414375539367029, Final Batch Loss: 8.172675006790087e-05\n",
      "Epoch 4557, Loss: 0.004473478338695713, Final Batch Loss: 0.0022616961505264044\n",
      "Epoch 4558, Loss: 0.022461460564954905, Final Batch Loss: 0.00011594545503612608\n",
      "Epoch 4559, Loss: 0.013706535275559872, Final Batch Loss: 0.00012364557187538594\n",
      "Epoch 4560, Loss: 0.020242351830347616, Final Batch Loss: 0.015388470143079758\n",
      "Epoch 4561, Loss: 0.0020685683502961183, Final Batch Loss: 1.621538285689894e-05\n",
      "Epoch 4562, Loss: 0.007487248082725273, Final Batch Loss: 4.358987644081935e-05\n",
      "Epoch 4563, Loss: 0.01388191534078942, Final Batch Loss: 0.000252187077421695\n",
      "Epoch 4564, Loss: 0.020102653419598937, Final Batch Loss: 0.0003938607987947762\n",
      "Epoch 4565, Loss: 0.01035796227370156, Final Batch Loss: 9.774700447451323e-05\n",
      "Epoch 4566, Loss: 0.017762319181201747, Final Batch Loss: 2.0003659301437438e-05\n",
      "Epoch 4567, Loss: 0.04841655880227336, Final Batch Loss: 2.9035258194198832e-05\n",
      "Epoch 4568, Loss: 0.014602791539800819, Final Batch Loss: 0.00048174112453125417\n",
      "Epoch 4569, Loss: 0.018509666828322224, Final Batch Loss: 0.00030250183772295713\n",
      "Epoch 4570, Loss: 0.0244664581532561, Final Batch Loss: 5.331423380994238e-05\n",
      "Epoch 4571, Loss: 0.019749314637010684, Final Batch Loss: 0.0006394250085577369\n",
      "Epoch 4572, Loss: 0.029493560026821797, Final Batch Loss: 0.0016510075656697154\n",
      "Epoch 4573, Loss: 0.010370969290306675, Final Batch Loss: 6.200057396199554e-05\n",
      "Epoch 4574, Loss: 0.008205423955587321, Final Batch Loss: 0.000231729747611098\n",
      "Epoch 4575, Loss: 0.009792009183001937, Final Batch Loss: 0.0009924651822075248\n",
      "Epoch 4576, Loss: 0.016546866352655343, Final Batch Loss: 5.0302533054491505e-05\n",
      "Epoch 4577, Loss: 0.0023879666568973335, Final Batch Loss: 1.8200254999101162e-05\n",
      "Epoch 4578, Loss: 0.005645723755151266, Final Batch Loss: 4.886540409643203e-05\n",
      "Epoch 4579, Loss: 0.026177705652116856, Final Batch Loss: 3.9736311009619385e-05\n",
      "Epoch 4580, Loss: 0.03240192255270813, Final Batch Loss: 0.0038748071528971195\n",
      "Epoch 4581, Loss: 0.008326681157996063, Final Batch Loss: 4.5916964154457673e-05\n",
      "Epoch 4582, Loss: 0.08185185373531567, Final Batch Loss: 0.00021725510305259377\n",
      "Epoch 4583, Loss: 0.060207541660929564, Final Batch Loss: 0.006051390431821346\n",
      "Epoch 4584, Loss: 0.022224226671824, Final Batch Loss: 0.00021675204334314913\n",
      "Epoch 4585, Loss: 0.010838284840019696, Final Batch Loss: 5.4242296755546704e-05\n",
      "Epoch 4586, Loss: 0.05400352209653647, Final Batch Loss: 0.004724576137959957\n",
      "Epoch 4587, Loss: 0.013594397707493044, Final Batch Loss: 0.0013951315777376294\n",
      "Epoch 4588, Loss: 0.038160207332111895, Final Batch Loss: 4.1948380385292694e-05\n",
      "Epoch 4589, Loss: 0.009149418098786555, Final Batch Loss: 8.564446761738509e-05\n",
      "Epoch 4590, Loss: 0.02495910869947693, Final Batch Loss: 9.271800809074193e-05\n",
      "Epoch 4591, Loss: 0.004362550722362357, Final Batch Loss: 0.0001972905738512054\n",
      "Epoch 4592, Loss: 0.0059239443216938525, Final Batch Loss: 0.0005039952229708433\n",
      "Epoch 4593, Loss: 0.005517187479199492, Final Batch Loss: 7.297875708900392e-05\n",
      "Epoch 4594, Loss: 0.016965769907983486, Final Batch Loss: 0.00019468917162157595\n",
      "Epoch 4595, Loss: 0.008485396646960908, Final Batch Loss: 1.6337741044480936e-06\n",
      "Epoch 4596, Loss: 0.02660988090610772, Final Batch Loss: 2.661915641510859e-05\n",
      "Epoch 4597, Loss: 0.04316429929349397, Final Batch Loss: 2.4958671929198317e-05\n",
      "Epoch 4598, Loss: 0.044405040145647945, Final Batch Loss: 0.00013194271014072\n",
      "Epoch 4599, Loss: 0.005308188399794744, Final Batch Loss: 0.0010694075608626008\n",
      "Epoch 4600, Loss: 0.011228225386730628, Final Batch Loss: 0.0006346543086692691\n",
      "Epoch 4601, Loss: 0.007587822969981062, Final Batch Loss: 0.001192681142129004\n",
      "Epoch 4602, Loss: 0.052625869126131875, Final Batch Loss: 0.0008191183442249894\n",
      "Epoch 4603, Loss: 0.015073549257976993, Final Batch Loss: 0.0005328875267878175\n",
      "Epoch 4604, Loss: 0.006547997805682826, Final Batch Loss: 7.590432505821809e-05\n",
      "Epoch 4605, Loss: 0.004309189098421484, Final Batch Loss: 0.001408637617714703\n",
      "Epoch 4606, Loss: 0.003994129605416674, Final Batch Loss: 0.000113551905087661\n",
      "Epoch 4607, Loss: 0.012137389288909617, Final Batch Loss: 0.00011679763701977208\n",
      "Epoch 4608, Loss: 0.009092960308407783, Final Batch Loss: 0.0007718478445895016\n",
      "Epoch 4609, Loss: 0.025271648602938512, Final Batch Loss: 1.784085907274857e-05\n",
      "Epoch 4610, Loss: 0.01891333116509486, Final Batch Loss: 2.6763467758428305e-05\n",
      "Epoch 4611, Loss: 0.04407506456482224, Final Batch Loss: 5.174755278858356e-05\n",
      "Epoch 4612, Loss: 0.03848300304161967, Final Batch Loss: 0.012447270564734936\n",
      "Epoch 4613, Loss: 0.02395978943150112, Final Batch Loss: 0.016937531530857086\n",
      "Epoch 4614, Loss: 0.0585658123027315, Final Batch Loss: 1.2387788956402801e-05\n",
      "Epoch 4615, Loss: 0.011861415958264843, Final Batch Loss: 0.00024044839665293694\n",
      "Epoch 4616, Loss: 0.012083738372894004, Final Batch Loss: 0.0005482467240653932\n",
      "Epoch 4617, Loss: 0.0064434959676873405, Final Batch Loss: 0.0001536605559522286\n",
      "Epoch 4618, Loss: 0.0036657227642535872, Final Batch Loss: 4.635465757019119e-06\n",
      "Epoch 4619, Loss: 0.04149398505614954, Final Batch Loss: 0.0005058914539404213\n",
      "Epoch 4620, Loss: 0.007746339346340392, Final Batch Loss: 4.987304055248387e-05\n",
      "Epoch 4621, Loss: 0.003730149630428059, Final Batch Loss: 0.00030133736436255276\n",
      "Epoch 4622, Loss: 0.05770964687280866, Final Batch Loss: 0.00019999399955850095\n",
      "Epoch 4623, Loss: 0.006882182030494732, Final Batch Loss: 7.544719119323418e-05\n",
      "Epoch 4624, Loss: 0.004251072645274689, Final Batch Loss: 0.0009334363858215511\n",
      "Epoch 4625, Loss: 0.0065427797635493334, Final Batch Loss: 4.9899368605110794e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4626, Loss: 0.0211636045423802, Final Batch Loss: 0.00010570917220320553\n",
      "Epoch 4627, Loss: 0.007667222909731208, Final Batch Loss: 0.00028317669057287276\n",
      "Epoch 4628, Loss: 0.026476425228793232, Final Batch Loss: 0.01686626672744751\n",
      "Epoch 4629, Loss: 0.0037979793351041735, Final Batch Loss: 0.00014130264753475785\n",
      "Epoch 4630, Loss: 0.00522186701073224, Final Batch Loss: 0.0007623344426974654\n",
      "Epoch 4631, Loss: 0.05491716916367295, Final Batch Loss: 0.00021713900787290186\n",
      "Epoch 4632, Loss: 0.08354795467312215, Final Batch Loss: 4.904347952106036e-05\n",
      "Epoch 4633, Loss: 0.007142836406274, Final Batch Loss: 0.0007822610205039382\n",
      "Epoch 4634, Loss: 0.096105776821787, Final Batch Loss: 0.0003573304566089064\n",
      "Epoch 4635, Loss: 0.08089837394072674, Final Batch Loss: 0.0009833906078711152\n",
      "Epoch 4636, Loss: 0.05887664163674344, Final Batch Loss: 0.0003690487938001752\n",
      "Epoch 4637, Loss: 0.009318444001110038, Final Batch Loss: 0.000371400557924062\n",
      "Epoch 4638, Loss: 0.02538222967632464, Final Batch Loss: 0.00028501832275651395\n",
      "Epoch 4639, Loss: 0.011783307410951238, Final Batch Loss: 0.00013505479728337377\n",
      "Epoch 4640, Loss: 0.00826096409218735, Final Batch Loss: 0.000566792965400964\n",
      "Epoch 4641, Loss: 0.017183234269396053, Final Batch Loss: 0.0016045370139181614\n",
      "Epoch 4642, Loss: 0.06687331671673746, Final Batch Loss: 0.0030452075880020857\n",
      "Epoch 4643, Loss: 0.05544477440707851, Final Batch Loss: 0.0022327471524477005\n",
      "Epoch 4644, Loss: 0.038928755100641865, Final Batch Loss: 0.0001206087545142509\n",
      "Epoch 4645, Loss: 0.08314433670238941, Final Batch Loss: 5.646271529258229e-05\n",
      "Epoch 4646, Loss: 0.02316605389933102, Final Batch Loss: 0.0008334360318258405\n",
      "Epoch 4647, Loss: 0.025575688363460358, Final Batch Loss: 0.00020840026263613254\n",
      "Epoch 4648, Loss: 0.014579493879864458, Final Batch Loss: 0.00018574719433672726\n",
      "Epoch 4649, Loss: 0.0069317668639996555, Final Batch Loss: 0.0005483008571900427\n",
      "Epoch 4650, Loss: 0.010065866434160853, Final Batch Loss: 0.0027187184896320105\n",
      "Epoch 4651, Loss: 0.011175951756740687, Final Batch Loss: 1.9693823560373858e-05\n",
      "Epoch 4652, Loss: 0.011440481674071634, Final Batch Loss: 9.722359391162172e-05\n",
      "Epoch 4653, Loss: 0.0270669517776696, Final Batch Loss: 0.000209655991056934\n",
      "Epoch 4654, Loss: 0.015604571066432982, Final Batch Loss: 1.0394285709480755e-05\n",
      "Epoch 4655, Loss: 0.011071426881244406, Final Batch Loss: 6.25155444140546e-05\n",
      "Epoch 4656, Loss: 0.01635966183312121, Final Batch Loss: 0.0019025957444682717\n",
      "Epoch 4657, Loss: 0.039473508342780406, Final Batch Loss: 7.049842679407448e-05\n",
      "Epoch 4658, Loss: 0.08582768027554266, Final Batch Loss: 0.013343780301511288\n",
      "Epoch 4659, Loss: 0.07161156800430035, Final Batch Loss: 0.00017674188711680472\n",
      "Epoch 4660, Loss: 0.007657446538360091, Final Batch Loss: 0.001236248528584838\n",
      "Epoch 4661, Loss: 0.017326786972262198, Final Batch Loss: 0.0001333776890533045\n",
      "Epoch 4662, Loss: 0.02573825203216984, Final Batch Loss: 0.0023909183219075203\n",
      "Epoch 4663, Loss: 0.054017575130274054, Final Batch Loss: 0.0002773187879938632\n",
      "Epoch 4664, Loss: 0.022277391191892093, Final Batch Loss: 0.0001681321009527892\n",
      "Epoch 4665, Loss: 0.02257588457723614, Final Batch Loss: 0.00034637723001651466\n",
      "Epoch 4666, Loss: 0.009229385552316671, Final Batch Loss: 0.0013792706886306405\n",
      "Epoch 4667, Loss: 0.014379155065398663, Final Batch Loss: 0.000631344853900373\n",
      "Epoch 4668, Loss: 0.011165809664817061, Final Batch Loss: 0.0022166059352457523\n",
      "Epoch 4669, Loss: 0.009552784824336413, Final Batch Loss: 0.0033237477764487267\n",
      "Epoch 4670, Loss: 0.0035723144428629894, Final Batch Loss: 5.317454269970767e-05\n",
      "Epoch 4671, Loss: 0.005516628347322694, Final Batch Loss: 0.00016850940301083028\n",
      "Epoch 4672, Loss: 0.009499819734628545, Final Batch Loss: 0.0006651919684372842\n",
      "Epoch 4673, Loss: 0.026117742496353458, Final Batch Loss: 0.0001055649554473348\n",
      "Epoch 4674, Loss: 0.01732941492446116, Final Batch Loss: 0.008276965469121933\n",
      "Epoch 4675, Loss: 0.002303027311427286, Final Batch Loss: 0.00024626273079775274\n",
      "Epoch 4676, Loss: 0.018123729476428707, Final Batch Loss: 0.0014711511321365833\n",
      "Epoch 4677, Loss: 0.014053916453121928, Final Batch Loss: 0.0017868529539555311\n",
      "Epoch 4678, Loss: 0.004490613262532861, Final Batch Loss: 0.00029858772177249193\n",
      "Epoch 4679, Loss: 0.025818915279160137, Final Batch Loss: 0.0029391103889793158\n",
      "Epoch 4680, Loss: 0.0407801806113639, Final Batch Loss: 0.0010559802176430821\n",
      "Epoch 4681, Loss: 0.013859945838703425, Final Batch Loss: 0.0011282312916591763\n",
      "Epoch 4682, Loss: 0.0033320827224088134, Final Batch Loss: 0.00023293956473935395\n",
      "Epoch 4683, Loss: 0.008047051778703462, Final Batch Loss: 0.003035938600078225\n",
      "Epoch 4684, Loss: 0.007266565712598094, Final Batch Loss: 0.005551880691200495\n",
      "Epoch 4685, Loss: 0.018329449761949945, Final Batch Loss: 0.0005213873228058219\n",
      "Epoch 4686, Loss: 0.04892458581525716, Final Batch Loss: 0.0010864140931516886\n",
      "Epoch 4687, Loss: 0.019784815014645574, Final Batch Loss: 0.0003778560785576701\n",
      "Epoch 4688, Loss: 0.011368357188985101, Final Batch Loss: 7.363309850916266e-05\n",
      "Epoch 4689, Loss: 0.004057326066686073, Final Batch Loss: 0.00028649126761592925\n",
      "Epoch 4690, Loss: 0.00595569277083996, Final Batch Loss: 4.953224924975075e-05\n",
      "Epoch 4691, Loss: 0.005395179863626254, Final Batch Loss: 3.7548546970356256e-05\n",
      "Epoch 4692, Loss: 0.01785611896957562, Final Batch Loss: 0.0012117365840822458\n",
      "Epoch 4693, Loss: 0.015226939372951165, Final Batch Loss: 9.965203935280442e-05\n",
      "Epoch 4694, Loss: 0.07419346559072437, Final Batch Loss: 0.0011315562296658754\n",
      "Epoch 4695, Loss: 0.024593425338025554, Final Batch Loss: 0.0004142997495364398\n",
      "Epoch 4696, Loss: 0.03417555745363643, Final Batch Loss: 0.0017732282867655158\n",
      "Epoch 4697, Loss: 0.049273939391241584, Final Batch Loss: 7.752833880658727e-06\n",
      "Epoch 4698, Loss: 0.010981428088598477, Final Batch Loss: 0.0006892282399348915\n",
      "Epoch 4699, Loss: 0.006912330909472075, Final Batch Loss: 0.0007989557925611734\n",
      "Epoch 4700, Loss: 0.0031933776172081707, Final Batch Loss: 0.0006430961075238883\n",
      "Epoch 4701, Loss: 0.0036158077723484894, Final Batch Loss: 4.084844022145262e-06\n",
      "Epoch 4702, Loss: 0.02557002504272532, Final Batch Loss: 2.1940688384347595e-05\n",
      "Epoch 4703, Loss: 0.026083658816105526, Final Batch Loss: 0.0001678946428000927\n",
      "Epoch 4704, Loss: 0.003713198142577312, Final Batch Loss: 3.0661427445011213e-05\n",
      "Epoch 4705, Loss: 0.03762912946513097, Final Batch Loss: 0.00030409463215619326\n",
      "Epoch 4706, Loss: 0.04183840461564614, Final Batch Loss: 0.00017630253569222987\n",
      "Epoch 4707, Loss: 0.008019340131795616, Final Batch Loss: 0.00026158872060477734\n",
      "Epoch 4708, Loss: 0.03458277957724931, Final Batch Loss: 1.678532498772256e-05\n",
      "Epoch 4709, Loss: 0.004842677060878486, Final Batch Loss: 9.935313573805615e-05\n",
      "Epoch 4710, Loss: 0.010503445794711297, Final Batch Loss: 0.00023528735619038343\n",
      "Epoch 4711, Loss: 0.004482604690565495, Final Batch Loss: 4.2828876757994294e-05\n",
      "Epoch 4712, Loss: 0.005787191398667346, Final Batch Loss: 1.3085345017316286e-05\n",
      "Epoch 4713, Loss: 0.010949724919555592, Final Batch Loss: 0.00010435208241688088\n",
      "Epoch 4714, Loss: 0.04835932859714376, Final Batch Loss: 0.0012072050012648106\n",
      "Epoch 4715, Loss: 0.05713191407085105, Final Batch Loss: 7.632895722053945e-05\n",
      "Epoch 4716, Loss: 0.06669278567824222, Final Batch Loss: 7.397063018288463e-05\n",
      "Epoch 4717, Loss: 0.010970380246362765, Final Batch Loss: 0.002223154529929161\n",
      "Epoch 4718, Loss: 0.011073930960264988, Final Batch Loss: 6.435209070332348e-05\n",
      "Epoch 4719, Loss: 0.00803097385323781, Final Batch Loss: 8.305841038236395e-05\n",
      "Epoch 4720, Loss: 0.02700033916698885, Final Batch Loss: 0.0015567837981507182\n",
      "Epoch 4721, Loss: 0.006900809807120822, Final Batch Loss: 0.000569498457480222\n",
      "Epoch 4722, Loss: 0.009413920726274227, Final Batch Loss: 2.8943184588570148e-05\n",
      "Epoch 4723, Loss: 0.014219125219824491, Final Batch Loss: 0.0009128047968260944\n",
      "Epoch 4724, Loss: 0.010278407457008143, Final Batch Loss: 0.00010248726903228089\n",
      "Epoch 4725, Loss: 0.023758663784974487, Final Batch Loss: 0.003336171852424741\n",
      "Epoch 4726, Loss: 0.022020214868462062, Final Batch Loss: 0.00037345942109823227\n",
      "Epoch 4727, Loss: 0.009226502104866086, Final Batch Loss: 7.451905548805371e-05\n",
      "Epoch 4728, Loss: 0.013660035889188293, Final Batch Loss: 0.009573712013661861\n",
      "Epoch 4729, Loss: 0.0013855942806912935, Final Batch Loss: 4.337628706707619e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4730, Loss: 0.0025172116156682023, Final Batch Loss: 0.00013307876361068338\n",
      "Epoch 4731, Loss: 0.02312753379192145, Final Batch Loss: 7.948110578581691e-05\n",
      "Epoch 4732, Loss: 0.0056660125082999, Final Batch Loss: 9.608981054043397e-05\n",
      "Epoch 4733, Loss: 0.0035872553398803575, Final Batch Loss: 0.0007807828369550407\n",
      "Epoch 4734, Loss: 0.00449016766378918, Final Batch Loss: 5.414424595073797e-05\n",
      "Epoch 4735, Loss: 0.0018113391797669465, Final Batch Loss: 0.0007724391180090606\n",
      "Epoch 4736, Loss: 0.004448315258741786, Final Batch Loss: 0.001221128972247243\n",
      "Epoch 4737, Loss: 0.012080035371582198, Final Batch Loss: 0.00011274230928393081\n",
      "Epoch 4738, Loss: 0.00596635055671868, Final Batch Loss: 4.028799230582081e-05\n",
      "Epoch 4739, Loss: 0.020362424596441997, Final Batch Loss: 0.00039619681774638593\n",
      "Epoch 4740, Loss: 0.007766578866721829, Final Batch Loss: 0.00015758986410219222\n",
      "Epoch 4741, Loss: 0.035334324686118634, Final Batch Loss: 0.017244137823581696\n",
      "Epoch 4742, Loss: 0.011194526705367025, Final Batch Loss: 0.00015083943435456604\n",
      "Epoch 4743, Loss: 0.13090638624544226, Final Batch Loss: 0.00043962165364064276\n",
      "Epoch 4744, Loss: 0.1532741118553531, Final Batch Loss: 0.003966081887483597\n",
      "Epoch 4745, Loss: 0.0876395344894263, Final Batch Loss: 0.00028998497873544693\n",
      "Epoch 4746, Loss: 0.031697034239186905, Final Batch Loss: 0.0012750260066241026\n",
      "Epoch 4747, Loss: 0.04074171520915115, Final Batch Loss: 0.001577325165271759\n",
      "Epoch 4748, Loss: 0.017237958374607842, Final Batch Loss: 0.0003245584375690669\n",
      "Epoch 4749, Loss: 0.03397349732404109, Final Batch Loss: 0.0013510347343981266\n",
      "Epoch 4750, Loss: 0.019456045331025962, Final Batch Loss: 0.0007107634446583688\n",
      "Epoch 4751, Loss: 0.01948778949008556, Final Batch Loss: 0.002384622348472476\n",
      "Epoch 4752, Loss: 0.03200763106360682, Final Batch Loss: 0.0002495465741958469\n",
      "Epoch 4753, Loss: 0.00666323708355776, Final Batch Loss: 0.000399121519876644\n",
      "Epoch 4754, Loss: 0.024570675088398275, Final Batch Loss: 0.0015665226383134723\n",
      "Epoch 4755, Loss: 0.04045171902544098, Final Batch Loss: 4.9022299208445475e-05\n",
      "Epoch 4756, Loss: 0.040860827972210245, Final Batch Loss: 0.0001012060311040841\n",
      "Epoch 4757, Loss: 0.010029630600911332, Final Batch Loss: 0.0024702560622245073\n",
      "Epoch 4758, Loss: 0.005364094220567495, Final Batch Loss: 0.00039630045648664236\n",
      "Epoch 4759, Loss: 0.009890701949188951, Final Batch Loss: 5.776577745564282e-05\n",
      "Epoch 4760, Loss: 0.020217432727804407, Final Batch Loss: 0.0006939313025213778\n",
      "Epoch 4761, Loss: 0.003759610299312044, Final Batch Loss: 0.0009147580713033676\n",
      "Epoch 4762, Loss: 0.005653653732679231, Final Batch Loss: 7.346857455559075e-05\n",
      "Epoch 4763, Loss: 0.003822925657914311, Final Batch Loss: 5.131495345267467e-05\n",
      "Epoch 4764, Loss: 0.02703004977411183, Final Batch Loss: 0.011871092021465302\n",
      "Epoch 4765, Loss: 0.03439264389817254, Final Batch Loss: 0.0006018023123033345\n",
      "Epoch 4766, Loss: 0.05247435487581242, Final Batch Loss: 7.88252946222201e-05\n",
      "Epoch 4767, Loss: 0.037085199064677, Final Batch Loss: 0.0004427660023793578\n",
      "Epoch 4768, Loss: 0.040489950730261626, Final Batch Loss: 0.0001770545495674014\n",
      "Epoch 4769, Loss: 0.016393025192883215, Final Batch Loss: 0.00014632262173108757\n",
      "Epoch 4770, Loss: 0.006404002341696469, Final Batch Loss: 0.0005839267978444695\n",
      "Epoch 4771, Loss: 0.020360360793347354, Final Batch Loss: 1.9120823708362877e-05\n",
      "Epoch 4772, Loss: 0.02290228055062471, Final Batch Loss: 0.00021756668866146356\n",
      "Epoch 4773, Loss: 0.026524959452217445, Final Batch Loss: 0.00031488577951677144\n",
      "Epoch 4774, Loss: 0.011427597622969188, Final Batch Loss: 2.3171289285528474e-05\n",
      "Epoch 4775, Loss: 0.004072907657246105, Final Batch Loss: 0.0004027136310469359\n",
      "Epoch 4776, Loss: 0.009157656386378221, Final Batch Loss: 0.0028303752187639475\n",
      "Epoch 4777, Loss: 0.028407648533175234, Final Batch Loss: 0.0007835983415134251\n",
      "Epoch 4778, Loss: 0.004725608048829599, Final Batch Loss: 0.00021828959870617837\n",
      "Epoch 4779, Loss: 0.04960942124671419, Final Batch Loss: 3.95822171412874e-05\n",
      "Epoch 4780, Loss: 0.014454831962211756, Final Batch Loss: 0.0002679263998288661\n",
      "Epoch 4781, Loss: 0.016185758002393413, Final Batch Loss: 0.00013394920097198337\n",
      "Epoch 4782, Loss: 0.00871766632553772, Final Batch Loss: 2.462766133248806e-05\n",
      "Epoch 4783, Loss: 0.005926017483943724, Final Batch Loss: 6.152850983198732e-05\n",
      "Epoch 4784, Loss: 0.0075928091109744855, Final Batch Loss: 5.2696344937430695e-05\n",
      "Epoch 4785, Loss: 0.004431318565366382, Final Batch Loss: 0.0004502583178691566\n",
      "Epoch 4786, Loss: 0.012102706527002738, Final Batch Loss: 0.00024851461057551205\n",
      "Epoch 4787, Loss: 0.018944976050079276, Final Batch Loss: 0.010679441504180431\n",
      "Epoch 4788, Loss: 0.023311871231271653, Final Batch Loss: 0.0002287002425873652\n",
      "Epoch 4789, Loss: 0.039811668277252465, Final Batch Loss: 0.0004202036652714014\n",
      "Epoch 4790, Loss: 0.03281340842931968, Final Batch Loss: 0.004442806821316481\n",
      "Epoch 4791, Loss: 0.005998429725877941, Final Batch Loss: 0.0002185846824431792\n",
      "Epoch 4792, Loss: 0.011258817152338452, Final Batch Loss: 0.00010612757614580914\n",
      "Epoch 4793, Loss: 0.010432101886181044, Final Batch Loss: 6.92518733558245e-05\n",
      "Epoch 4794, Loss: 0.015833699604627327, Final Batch Loss: 0.00444973586127162\n",
      "Epoch 4795, Loss: 0.006674506993476825, Final Batch Loss: 0.00034046117798425257\n",
      "Epoch 4796, Loss: 0.0020074012500117533, Final Batch Loss: 2.3581451387144625e-05\n",
      "Epoch 4797, Loss: 0.03671920487522584, Final Batch Loss: 0.022419307380914688\n",
      "Epoch 4798, Loss: 0.013651831508468604, Final Batch Loss: 0.0026372785214334726\n",
      "Epoch 4799, Loss: 0.004745857260786579, Final Batch Loss: 0.0013766672927886248\n",
      "Epoch 4800, Loss: 0.00629391712936922, Final Batch Loss: 0.00011215884296689183\n",
      "Epoch 4801, Loss: 0.005878169999959937, Final Batch Loss: 9.749994933372363e-05\n",
      "Epoch 4802, Loss: 0.014380527840330615, Final Batch Loss: 0.0002554399543441832\n",
      "Epoch 4803, Loss: 0.019248668480940978, Final Batch Loss: 0.010791437700390816\n",
      "Epoch 4804, Loss: 0.0035608906161996856, Final Batch Loss: 0.0002741481293924153\n",
      "Epoch 4805, Loss: 0.005726917788706487, Final Batch Loss: 0.0001063701492967084\n",
      "Epoch 4806, Loss: 0.003243326221308962, Final Batch Loss: 2.586623486422468e-05\n",
      "Epoch 4807, Loss: 0.01710042937884282, Final Batch Loss: 8.189697109628469e-05\n",
      "Epoch 4808, Loss: 0.009395236840646248, Final Batch Loss: 8.292948041344061e-05\n",
      "Epoch 4809, Loss: 0.003276260618804372, Final Batch Loss: 1.9905686713173054e-05\n",
      "Epoch 4810, Loss: 0.021605387943054666, Final Batch Loss: 0.0011096035595983267\n",
      "Epoch 4811, Loss: 0.015163157750976097, Final Batch Loss: 0.0002167783968616277\n",
      "Epoch 4812, Loss: 0.009221760234140675, Final Batch Loss: 0.00010847820522030815\n",
      "Epoch 4813, Loss: 0.005538366302062059, Final Batch Loss: 3.789189941016957e-05\n",
      "Epoch 4814, Loss: 0.0036671004854724742, Final Batch Loss: 0.00039661466144025326\n",
      "Epoch 4815, Loss: 0.009995271406296524, Final Batch Loss: 0.0020929917227476835\n",
      "Epoch 4816, Loss: 0.004742725952382898, Final Batch Loss: 2.7969876100542024e-05\n",
      "Epoch 4817, Loss: 0.013712732134081307, Final Batch Loss: 5.953020809101872e-05\n",
      "Epoch 4818, Loss: 0.007053172063933744, Final Batch Loss: 1.604722456249874e-05\n",
      "Epoch 4819, Loss: 0.039071834012247564, Final Batch Loss: 0.005625732708722353\n",
      "Epoch 4820, Loss: 0.046950006966653746, Final Batch Loss: 0.0014104845467954874\n",
      "Epoch 4821, Loss: 0.038650505608529784, Final Batch Loss: 0.00012359315587673336\n",
      "Epoch 4822, Loss: 0.009986857829971996, Final Batch Loss: 0.0004872564459219575\n",
      "Epoch 4823, Loss: 0.1639809529588092, Final Batch Loss: 0.07943281531333923\n",
      "Epoch 4824, Loss: 0.19423626273783157, Final Batch Loss: 0.007305337581783533\n",
      "Epoch 4825, Loss: 0.2636210830969503, Final Batch Loss: 0.00903889536857605\n",
      "Epoch 4826, Loss: 0.04743471625261009, Final Batch Loss: 0.0009948837105184793\n",
      "Epoch 4827, Loss: 0.10871393757406622, Final Batch Loss: 0.0007233062642626464\n",
      "Epoch 4828, Loss: 0.0520616525755031, Final Batch Loss: 0.008135860785841942\n",
      "Epoch 4829, Loss: 0.036107861655182205, Final Batch Loss: 0.00020279220188967884\n",
      "Epoch 4830, Loss: 0.055669519046205096, Final Batch Loss: 0.0018610007828101516\n",
      "Epoch 4831, Loss: 0.021145594524568878, Final Batch Loss: 0.00030858858372084796\n",
      "Epoch 4832, Loss: 0.011602082558965776, Final Batch Loss: 0.002329017035663128\n",
      "Epoch 4833, Loss: 0.011222220360650681, Final Batch Loss: 0.0005436939536593854\n",
      "Epoch 4834, Loss: 0.02507840039106668, Final Batch Loss: 0.0002508331381250173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4835, Loss: 0.022986927768215537, Final Batch Loss: 0.000561088789254427\n",
      "Epoch 4836, Loss: 0.073085291282041, Final Batch Loss: 0.0042634690180420876\n",
      "Epoch 4837, Loss: 0.008032591467781458, Final Batch Loss: 0.0002480717084836215\n",
      "Epoch 4838, Loss: 0.007222691201604903, Final Batch Loss: 0.0003145588852930814\n",
      "Epoch 4839, Loss: 0.0026900066095549846, Final Batch Loss: 2.630487688293215e-05\n",
      "Epoch 4840, Loss: 0.023080449431290617, Final Batch Loss: 0.000675861956551671\n",
      "Epoch 4841, Loss: 0.023874972230260028, Final Batch Loss: 0.000301266903989017\n",
      "Epoch 4842, Loss: 0.03772868808482599, Final Batch Loss: 0.00023925796267576516\n",
      "Epoch 4843, Loss: 0.00477704904278653, Final Batch Loss: 1.6122517990879714e-05\n",
      "Epoch 4844, Loss: 0.018654616877029184, Final Batch Loss: 0.00012636517931241542\n",
      "Epoch 4845, Loss: 0.01322617625555722, Final Batch Loss: 0.00035496734199114144\n",
      "Epoch 4846, Loss: 0.006982447412156034, Final Batch Loss: 0.0001853950961958617\n",
      "Epoch 4847, Loss: 0.022867203872010577, Final Batch Loss: 6.022275556460954e-05\n",
      "Epoch 4848, Loss: 0.0043644173492793925, Final Batch Loss: 9.942288306774572e-05\n",
      "Epoch 4849, Loss: 0.011041909558116458, Final Batch Loss: 0.0001780214224709198\n",
      "Epoch 4850, Loss: 0.014386993141670246, Final Batch Loss: 0.0007528085261583328\n",
      "Epoch 4851, Loss: 0.028732621500239475, Final Batch Loss: 0.0003521970647852868\n",
      "Epoch 4852, Loss: 0.012377782790281344, Final Batch Loss: 8.541081479052082e-05\n",
      "Epoch 4853, Loss: 0.056082030132529326, Final Batch Loss: 0.00047906310646794736\n",
      "Epoch 4854, Loss: 0.042068815244419966, Final Batch Loss: 0.00025709017063491046\n",
      "Epoch 4855, Loss: 0.021323034819943132, Final Batch Loss: 0.003848034655675292\n",
      "Epoch 4856, Loss: 0.017569701815773442, Final Batch Loss: 1.2008204976154957e-05\n",
      "Epoch 4857, Loss: 0.020113684643547458, Final Batch Loss: 0.010268551297485828\n",
      "Epoch 4858, Loss: 0.15419655496407358, Final Batch Loss: 0.0004023553628940135\n",
      "Epoch 4859, Loss: 0.0670835008713766, Final Batch Loss: 0.007667449302971363\n",
      "Epoch 4860, Loss: 0.026680081893573515, Final Batch Loss: 0.002385533880442381\n",
      "Epoch 4861, Loss: 0.01250780315240263, Final Batch Loss: 0.0009554328862577677\n",
      "Epoch 4862, Loss: 0.008622553845270886, Final Batch Loss: 0.000827817537356168\n",
      "Epoch 4863, Loss: 0.015256248916557524, Final Batch Loss: 0.000508324068505317\n",
      "Epoch 4864, Loss: 0.011499283195007592, Final Batch Loss: 0.00011338056356180459\n",
      "Epoch 4865, Loss: 0.0032131408552231733, Final Batch Loss: 0.0014767992543056607\n",
      "Epoch 4866, Loss: 0.003562742153008003, Final Batch Loss: 0.00025672593619674444\n",
      "Epoch 4867, Loss: 0.06493588125158567, Final Batch Loss: 0.028749126940965652\n",
      "Epoch 4868, Loss: 0.010001083710449166, Final Batch Loss: 0.00012195779709145427\n",
      "Epoch 4869, Loss: 0.035934743988036644, Final Batch Loss: 0.0004668670298997313\n",
      "Epoch 4870, Loss: 0.03823724169342313, Final Batch Loss: 0.00017151504289358854\n",
      "Epoch 4871, Loss: 0.02212852493448736, Final Batch Loss: 8.697295561432838e-05\n",
      "Epoch 4872, Loss: 0.027325485418259632, Final Batch Loss: 0.0006269247969612479\n",
      "Epoch 4873, Loss: 0.011151302645885153, Final Batch Loss: 5.235964636085555e-05\n",
      "Epoch 4874, Loss: 0.024013873606236302, Final Batch Loss: 8.318229811266065e-05\n",
      "Epoch 4875, Loss: 0.0371472064180125, Final Batch Loss: 0.00016042769129853696\n",
      "Epoch 4876, Loss: 0.009468043356719136, Final Batch Loss: 0.0037245203275233507\n",
      "Epoch 4877, Loss: 0.00656778603024577, Final Batch Loss: 8.124375017359853e-05\n",
      "Epoch 4878, Loss: 0.02059439791537443, Final Batch Loss: 0.00027232003048993647\n",
      "Epoch 4879, Loss: 0.02155418009715504, Final Batch Loss: 0.005880252458155155\n",
      "Epoch 4880, Loss: 0.029009345159920485, Final Batch Loss: 9.795211371965706e-05\n",
      "Epoch 4881, Loss: 0.016147371592523996, Final Batch Loss: 0.0002367279230384156\n",
      "Epoch 4882, Loss: 0.02593953509494895, Final Batch Loss: 0.003342564683407545\n",
      "Epoch 4883, Loss: 0.0715737567315955, Final Batch Loss: 0.0017719602910801768\n",
      "Epoch 4884, Loss: 0.027410270076870802, Final Batch Loss: 0.0003837761760223657\n",
      "Epoch 4885, Loss: 0.007346368420257932, Final Batch Loss: 0.00011093720968347043\n",
      "Epoch 4886, Loss: 0.017301573270742665, Final Batch Loss: 0.0001672929647611454\n",
      "Epoch 4887, Loss: 0.0023025528880680213, Final Batch Loss: 0.00015078783326316625\n",
      "Epoch 4888, Loss: 0.007443440959832515, Final Batch Loss: 3.7474768760148436e-05\n",
      "Epoch 4889, Loss: 0.011799231717304792, Final Batch Loss: 0.005929738283157349\n",
      "Epoch 4890, Loss: 0.0663458095441456, Final Batch Loss: 0.0002852593897841871\n",
      "Epoch 4891, Loss: 0.015622749660906265, Final Batch Loss: 0.00042863882845267653\n",
      "Epoch 4892, Loss: 0.015333775381805026, Final Batch Loss: 0.00031676661456003785\n",
      "Epoch 4893, Loss: 0.004272142628906295, Final Batch Loss: 7.173164340201765e-05\n",
      "Epoch 4894, Loss: 0.0037938297446089564, Final Batch Loss: 0.00019036616140510887\n",
      "Epoch 4895, Loss: 0.01999708600305894, Final Batch Loss: 0.0029543417040258646\n",
      "Epoch 4896, Loss: 0.007460621724021621, Final Batch Loss: 7.251506031025201e-05\n",
      "Epoch 4897, Loss: 0.009128120458626654, Final Batch Loss: 2.012954246310983e-05\n",
      "Epoch 4898, Loss: 0.019006700082627503, Final Batch Loss: 0.000440299219917506\n",
      "Epoch 4899, Loss: 0.004520387934462633, Final Batch Loss: 6.097032382967882e-05\n",
      "Epoch 4900, Loss: 0.003793296231833665, Final Batch Loss: 2.010143134612008e-06\n",
      "Epoch 4901, Loss: 0.014403715716980514, Final Batch Loss: 0.002506754593923688\n",
      "Epoch 4902, Loss: 0.004766030002429034, Final Batch Loss: 6.107777880970389e-05\n",
      "Epoch 4903, Loss: 0.013394844817412377, Final Batch Loss: 1.9263681679149158e-05\n",
      "Epoch 4904, Loss: 0.030115005026345898, Final Batch Loss: 0.00012767915904987603\n",
      "Epoch 4905, Loss: 0.025715703595324158, Final Batch Loss: 0.00042476446833461523\n",
      "Epoch 4906, Loss: 0.017070038014026068, Final Batch Loss: 9.182532267004717e-06\n",
      "Epoch 4907, Loss: 0.009987287434341852, Final Batch Loss: 2.9655240723513998e-05\n",
      "Epoch 4908, Loss: 0.06194991360644053, Final Batch Loss: 2.7418402169132605e-05\n",
      "Epoch 4909, Loss: 0.00920184835922555, Final Batch Loss: 1.6309606508002616e-05\n",
      "Epoch 4910, Loss: 0.02018382408641628, Final Batch Loss: 0.000883628090377897\n",
      "Epoch 4911, Loss: 0.036975740349589614, Final Batch Loss: 3.422323788981885e-05\n",
      "Epoch 4912, Loss: 0.02965299805327959, Final Batch Loss: 8.980547136161476e-05\n",
      "Epoch 4913, Loss: 0.06991414103049465, Final Batch Loss: 4.295050530345179e-05\n",
      "Epoch 4914, Loss: 0.005463834863576267, Final Batch Loss: 0.0005879905074834824\n",
      "Epoch 4915, Loss: 0.054691926281520864, Final Batch Loss: 6.177826435305178e-05\n",
      "Epoch 4916, Loss: 0.008775434782364755, Final Batch Loss: 0.00020162120927125216\n",
      "Epoch 4917, Loss: 0.0183208497655869, Final Batch Loss: 0.00037211162270978093\n",
      "Epoch 4918, Loss: 0.014050723544642096, Final Batch Loss: 0.00011323517537675798\n",
      "Epoch 4919, Loss: 0.010760154006675293, Final Batch Loss: 2.324806155229453e-05\n",
      "Epoch 4920, Loss: 0.026482186192879453, Final Batch Loss: 0.00022699889086652547\n",
      "Epoch 4921, Loss: 0.07808798396217753, Final Batch Loss: 0.0012637132313102484\n",
      "Epoch 4922, Loss: 0.014967026123485994, Final Batch Loss: 0.00020969919569324702\n",
      "Epoch 4923, Loss: 0.009701516828499734, Final Batch Loss: 5.31549085280858e-05\n",
      "Epoch 4924, Loss: 0.02365483604808105, Final Batch Loss: 0.001895467983558774\n",
      "Epoch 4925, Loss: 0.018170245997680468, Final Batch Loss: 0.0007252086652442813\n",
      "Epoch 4926, Loss: 0.00964850037735232, Final Batch Loss: 0.001468702219426632\n",
      "Epoch 4927, Loss: 0.010904117811151082, Final Batch Loss: 0.0001794528216123581\n",
      "Epoch 4928, Loss: 0.012378742150758626, Final Batch Loss: 0.00011798390914918855\n",
      "Epoch 4929, Loss: 0.004365226504887687, Final Batch Loss: 7.131786696845666e-05\n",
      "Epoch 4930, Loss: 0.03271727642277256, Final Batch Loss: 6.25990069238469e-05\n",
      "Epoch 4931, Loss: 0.006853581104223849, Final Batch Loss: 0.00018732980242930353\n",
      "Epoch 4932, Loss: 0.01112918878243363, Final Batch Loss: 0.00215220358222723\n",
      "Epoch 4933, Loss: 0.024872666002920596, Final Batch Loss: 0.0006076630670577288\n",
      "Epoch 4934, Loss: 0.06734089920428232, Final Batch Loss: 2.944060179288499e-05\n",
      "Epoch 4935, Loss: 0.00722860058885999, Final Batch Loss: 0.0005626294878311455\n",
      "Epoch 4936, Loss: 0.0546162100181391, Final Batch Loss: 0.00010160322563024238\n",
      "Epoch 4937, Loss: 0.04786195012547978, Final Batch Loss: 0.005605095066130161\n",
      "Epoch 4938, Loss: 0.0388433131884085, Final Batch Loss: 8.585736941313371e-05\n",
      "Epoch 4939, Loss: 0.028943273946424597, Final Batch Loss: 5.4751501011196524e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4940, Loss: 0.02500128341898744, Final Batch Loss: 0.0001802220067474991\n",
      "Epoch 4941, Loss: 0.027546203586098272, Final Batch Loss: 0.009488973766565323\n",
      "Epoch 4942, Loss: 0.03162911642175459, Final Batch Loss: 0.0001759874721756205\n",
      "Epoch 4943, Loss: 0.008605085731687723, Final Batch Loss: 0.00023390956630464643\n",
      "Epoch 4944, Loss: 0.01896996342975399, Final Batch Loss: 0.00016917489119805396\n",
      "Epoch 4945, Loss: 0.02140750397575175, Final Batch Loss: 0.00017068868328351527\n",
      "Epoch 4946, Loss: 0.05485647633395274, Final Batch Loss: 4.1066505218623206e-05\n",
      "Epoch 4947, Loss: 0.013366219936870039, Final Batch Loss: 0.0006130188703536987\n",
      "Epoch 4948, Loss: 0.015743155103336903, Final Batch Loss: 6.452495290432125e-05\n",
      "Epoch 4949, Loss: 0.012459401343221543, Final Batch Loss: 0.0018421161221340299\n",
      "Epoch 4950, Loss: 0.010935153878563142, Final Batch Loss: 3.0540137231582776e-05\n",
      "Epoch 4951, Loss: 0.04080652944412577, Final Batch Loss: 4.706954496214166e-05\n",
      "Epoch 4952, Loss: 0.0013181609829189256, Final Batch Loss: 0.00013230627519078553\n",
      "Epoch 4953, Loss: 0.0035192222057958134, Final Batch Loss: 7.960413495311514e-05\n",
      "Epoch 4954, Loss: 0.010846423811017303, Final Batch Loss: 8.045407594181597e-05\n",
      "Epoch 4955, Loss: 0.008367116173758404, Final Batch Loss: 0.0016604027478024364\n",
      "Epoch 4956, Loss: 0.013171450034860754, Final Batch Loss: 0.005012286826968193\n",
      "Epoch 4957, Loss: 0.02185162812929775, Final Batch Loss: 1.0451813068357296e-05\n",
      "Epoch 4958, Loss: 0.0240260960526939, Final Batch Loss: 0.0005680230678990483\n",
      "Epoch 4959, Loss: 0.06107409372452821, Final Batch Loss: 0.022872306406497955\n",
      "Epoch 4960, Loss: 0.01852787798270583, Final Batch Loss: 0.006400684360414743\n",
      "Epoch 4961, Loss: 0.020485598426603246, Final Batch Loss: 8.829923899611458e-05\n",
      "Epoch 4962, Loss: 0.014807116571319057, Final Batch Loss: 0.00035195404780097306\n",
      "Epoch 4963, Loss: 0.007223262180559686, Final Batch Loss: 0.00021238149201963097\n",
      "Epoch 4964, Loss: 0.007914328431070317, Final Batch Loss: 5.058801980339922e-05\n",
      "Epoch 4965, Loss: 0.0038771788522353745, Final Batch Loss: 9.537067671772093e-05\n",
      "Epoch 4966, Loss: 0.025957703814128763, Final Batch Loss: 0.000520602276083082\n",
      "Epoch 4967, Loss: 0.011621434427070199, Final Batch Loss: 0.0002549668133724481\n",
      "Epoch 4968, Loss: 0.019323928536323365, Final Batch Loss: 0.00015793395868968219\n",
      "Epoch 4969, Loss: 0.04458524337314884, Final Batch Loss: 0.00024978246074169874\n",
      "Epoch 4970, Loss: 0.035777209159277845, Final Batch Loss: 0.004166803322732449\n",
      "Epoch 4971, Loss: 0.01545242108113598, Final Batch Loss: 0.00042363599641248584\n",
      "Epoch 4972, Loss: 0.009215258468884713, Final Batch Loss: 0.0005040491232648492\n",
      "Epoch 4973, Loss: 0.012680841703058832, Final Batch Loss: 6.809319074818632e-06\n",
      "Epoch 4974, Loss: 0.008149467798375554, Final Batch Loss: 4.754251222038874e-06\n",
      "Epoch 4975, Loss: 0.0036281043212511577, Final Batch Loss: 7.908192492322996e-05\n",
      "Epoch 4976, Loss: 0.01016101691948279, Final Batch Loss: 0.0001945874246302992\n",
      "Epoch 4977, Loss: 0.009546844274609612, Final Batch Loss: 0.00032969852327369153\n",
      "Epoch 4978, Loss: 0.06648305354610784, Final Batch Loss: 0.0012416595127433538\n",
      "Epoch 4979, Loss: 0.026416010296088643, Final Batch Loss: 4.00637618440669e-05\n",
      "Epoch 4980, Loss: 0.006547628596308641, Final Batch Loss: 4.9368823965778574e-05\n",
      "Epoch 4981, Loss: 0.011918874271941604, Final Batch Loss: 0.00028463322087191045\n",
      "Epoch 4982, Loss: 0.002836449770256877, Final Batch Loss: 6.598291656700894e-05\n",
      "Epoch 4983, Loss: 0.0274425402203633, Final Batch Loss: 3.0729348509339616e-05\n",
      "Epoch 4984, Loss: 0.057780331262620166, Final Batch Loss: 0.00015361497935373336\n",
      "Epoch 4985, Loss: 0.036690624758193735, Final Batch Loss: 0.0011668911902233958\n",
      "Epoch 4986, Loss: 0.06713976566243218, Final Batch Loss: 0.00032731163082644343\n",
      "Epoch 4987, Loss: 0.012578260906593641, Final Batch Loss: 0.00033224685466848314\n",
      "Epoch 4988, Loss: 0.01783346413867548, Final Batch Loss: 0.0002998411073349416\n",
      "Epoch 4989, Loss: 0.01381959045829717, Final Batch Loss: 0.00016332832456100732\n",
      "Epoch 4990, Loss: 0.028156586267868988, Final Batch Loss: 0.0002574811514932662\n",
      "Epoch 4991, Loss: 0.015635853578714887, Final Batch Loss: 0.0036166661884635687\n",
      "Epoch 4992, Loss: 0.01832438304882089, Final Batch Loss: 0.0003261882229708135\n",
      "Epoch 4993, Loss: 0.011577653589483816, Final Batch Loss: 0.0002960245474241674\n",
      "Epoch 4994, Loss: 0.00953021407531196, Final Batch Loss: 3.5229753848398104e-05\n",
      "Epoch 4995, Loss: 0.015128607235965319, Final Batch Loss: 6.37342527625151e-05\n",
      "Epoch 4996, Loss: 0.04109195246019226, Final Batch Loss: 8.662307664053515e-05\n",
      "Epoch 4997, Loss: 0.030748130953725195, Final Batch Loss: 2.4503384338459e-05\n",
      "Epoch 4998, Loss: 0.03860482639174734, Final Batch Loss: 0.0008928869501687586\n",
      "Epoch 4999, Loss: 0.02988301905679691, Final Batch Loss: 0.0018012060318142176\n",
      "Epoch 5000, Loss: 0.05335398259740032, Final Batch Loss: 0.003665784141048789\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[465  31   0]\n",
      " [ 40 380   0]\n",
      " [  0   0 491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.921     0.938     0.929       496\n",
      "           1      0.925     0.905     0.915       420\n",
      "           2      1.000     1.000     1.000       491\n",
      "\n",
      "    accuracy                          0.950      1407\n",
      "   macro avg      0.948     0.947     0.948      1407\n",
      "weighted avg      0.950     0.950     0.949      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../saved_models/UCI 3 Label Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
