{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "\n",
    "    #X_train = data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z'] #Selecting only acceleration columns\n",
    "    \n",
    "    #X_train_1 = data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z']\n",
    "    #X_train_2 = data.loc[:,'81 tBodyAccJerk-mean()-X':'160 tBodyGyro-correlation()-Y,Z']\n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X = GAN_data.iloc[:,:-2].values\n",
    "    #X = GAN_data.loc[:,'1 tBodyAcc-mean()-X':'160 tBodyGyro-correlation()-Y,Z'].values\n",
    "    y = GAN_data[[y_label]].values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def get_act_matrix(batch_size, a_dim):\n",
    "    indexes = np.random.randint(a_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((len(indexes), indexes.max()+1))\n",
    "    one_hot[np.arange(len(indexes)),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "    \n",
    "def get_usr_matrix(batch_size, u_dim):\n",
    "    indexes = np.random.randint(u_dim, size = batch_size)\n",
    "    \n",
    "    one_hot = np.zeros((indexes.size, indexes.max()+1))\n",
    "    one_hot[np.arange(indexes.size),indexes] = 1\n",
    "    return torch.Tensor(indexes).long(), torch.Tensor(one_hot)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (gen): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=106, out_features=80, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=80, out_features=60, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=60, out_features=50, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "      (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Linear(in_features=50, out_features=46, bias=True)\n",
       "    (4): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator(z_dim = 106)\n",
    "load_model(gen, \"cGAN_UCI_30k_TEST_gen.param\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train On Real, Test On Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming activity labels (1, 3, 4 --> 0, 1, 2)\n",
    "for k in range(len(y)): \n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  0  0]\n",
      " [ 1 27  0]\n",
      " [ 0  0 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.978     1.000     0.989        44\n",
      "           1      1.000     0.964     0.982        28\n",
      "           2      1.000     1.000     1.000        28\n",
      "\n",
      "    accuracy                          0.990       100\n",
      "   macro avg      0.993     0.988     0.990       100\n",
      "weighted avg      0.990     0.990     0.990       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size = 0.2, shuffle = True)\n",
    "\n",
    "classifier_real_act = LogisticRegression(max_iter = 300)\n",
    "classifier_real_act.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier_real_act.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train On Fake, Test On Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43  1  0]\n",
      " [ 2 26  0]\n",
      " [ 0  0 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.956     0.977     0.966        44\n",
      "           1      0.963     0.929     0.945        28\n",
      "           2      1.000     1.000     1.000        28\n",
      "\n",
      "    accuracy                          0.970       100\n",
      "   macro avg      0.973     0.969     0.971       100\n",
      "weighted avg      0.970     0.970     0.970       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_vectors = get_noise(len(X_train), 100)\n",
    "act_vectors = get_act_matrix(len(X_train), 3)\n",
    "usr_vectors = get_usr_matrix(len(X_train), 3)\n",
    "\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach().numpy()\n",
    "\n",
    "classifier_fake = LogisticRegression(max_iter = 300)\n",
    "classifier_fake.fit(fake_features, act_vectors[0])\n",
    "\n",
    "y_pred = classifier_fake.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = start_data(activities, users, \"Subject\", sub_features, act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y)): \n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  1  0]\n",
      " [ 3 34  7]\n",
      " [ 0  1 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.897     0.963     0.929        27\n",
      "           1      0.944     0.773     0.850        44\n",
      "           2      0.800     0.966     0.875        29\n",
      "\n",
      "    accuracy                          0.880       100\n",
      "   macro avg      0.880     0.900     0.885       100\n",
      "weighted avg      0.890     0.880     0.878       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size = 0.2, shuffle = True)\n",
    "\n",
    "classifier_real_user = LogisticRegression(max_iter = 300)\n",
    "classifier_real_user.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier_real_user.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  0  2]\n",
      " [ 3 23  4]\n",
      " [ 5  0 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.818     0.947     0.878        38\n",
      "           1      1.000     0.767     0.868        30\n",
      "           2      0.818     0.844     0.831        32\n",
      "\n",
      "    accuracy                          0.860       100\n",
      "   macro avg      0.879     0.853     0.859       100\n",
      "weighted avg      0.873     0.860     0.860       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_vectors = get_noise(len(X_train), 100)\n",
    "act_vectors = get_act_matrix(len(X_train), 3)\n",
    "usr_vectors = get_usr_matrix(len(X_train), 3)\n",
    "\n",
    "to_gen = torch.cat((latent_vectors, act_vectors[1], usr_vectors[1]), 1)\n",
    "fake_features = gen(to_gen).detach().numpy()\n",
    "\n",
    "classifier_fake = LogisticRegression(max_iter = 300)\n",
    "classifier_fake.fit(fake_features, usr_vectors[0])\n",
    "\n",
    "y_pred = classifier_fake.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL ACTIVITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  0  0]\n",
      " [ 0 29  0]\n",
      " [ 0  0 30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000        41\n",
      "           1      1.000     1.000     1.000        29\n",
      "           2      1.000     1.000     1.000        30\n",
      "\n",
      "    accuracy                          1.000       100\n",
      "   macro avg      1.000     1.000     1.000       100\n",
      "weighted avg      1.000     1.000     1.000       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### ACTIVITY 0\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_1 = np.zeros(300)\n",
    "\n",
    "### ACTIVITY 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_2 = np.ones(300)\n",
    "\n",
    "### ACTIVITY 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_3 = np.ones(300) + 1\n",
    "\n",
    "X_train = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_train = np.concatenate((y_test_1, y_test_2, y_test_3))\n",
    "\n",
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "for k in range(len(y)): \n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X, y.flatten(), test_size = 0.2, shuffle = True)\n",
    "\n",
    "classifier = LogisticRegression(max_iter = 300)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  1  0]\n",
      " [ 4 30  4]\n",
      " [ 0  5 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.895     0.971     0.932        35\n",
      "           1      0.833     0.789     0.811        38\n",
      "           2      0.846     0.815     0.830        27\n",
      "\n",
      "    accuracy                          0.860       100\n",
      "   macro avg      0.858     0.859     0.858       100\n",
      "weighted avg      0.858     0.860     0.858       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SUBJECT 0\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_1 = np.zeros(300)\n",
    "\n",
    "### SUBJECT 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_2 = np.ones(300)\n",
    "\n",
    "### SUBJECT 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(len(X_test), 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_test_3 = np.ones(300) + 1\n",
    "\n",
    "\n",
    "X_train = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_train = np.concatenate((y_test_1, y_test_2, y_test_3))\n",
    "\n",
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X, y = start_data(activities, users, \"Subject\", sub_features, act_features)\n",
    "\n",
    "for k in range(len(y)): \n",
    "    if y[k] == 1:\n",
    "        y[k] = 0\n",
    "    elif y[k] == 3:\n",
    "        y[k] = 1\n",
    "    else:\n",
    "        y[k] = 2\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X, y.flatten(), test_size = 0.2, shuffle = True)\n",
    "\n",
    "classifier = LogisticRegression(max_iter = 300)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred, digits = 3)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
