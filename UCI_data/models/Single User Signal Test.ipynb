{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 tBodyAcc-mean()-X</th>\n",
       "      <th>2 tBodyAcc-mean()-Y</th>\n",
       "      <th>3 tBodyAcc-mean()-Z</th>\n",
       "      <th>4 tBodyAcc-std()-X</th>\n",
       "      <th>5 tBodyAcc-std()-Y</th>\n",
       "      <th>6 tBodyAcc-std()-Z</th>\n",
       "      <th>7 tBodyAcc-mad()-X</th>\n",
       "      <th>8 tBodyAcc-mad()-Y</th>\n",
       "      <th>9 tBodyAcc-mad()-Z</th>\n",
       "      <th>10 tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>33 tBodyAcc-arCoeff()-Y,4</th>\n",
       "      <th>34 tBodyAcc-arCoeff()-Z,1</th>\n",
       "      <th>35 tBodyAcc-arCoeff()-Z,2</th>\n",
       "      <th>36 tBodyAcc-arCoeff()-Z,3</th>\n",
       "      <th>37 tBodyAcc-arCoeff()-Z,4</th>\n",
       "      <th>38 tBodyAcc-correlation()-X,Y</th>\n",
       "      <th>39 tBodyAcc-correlation()-X,Z</th>\n",
       "      <th>40 tBodyAcc-correlation()-Y,Z</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095246</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>-0.465085</td>\n",
       "      <td>0.491936</td>\n",
       "      <td>-0.190884</td>\n",
       "      <td>0.376314</td>\n",
       "      <td>0.435129</td>\n",
       "      <td>0.660790</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281211</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>-0.022153</td>\n",
       "      <td>-0.016657</td>\n",
       "      <td>-0.220643</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>-0.072692</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332564</td>\n",
       "      <td>0.239281</td>\n",
       "      <td>-0.136204</td>\n",
       "      <td>0.173863</td>\n",
       "      <td>-0.299493</td>\n",
       "      <td>-0.124698</td>\n",
       "      <td>-0.181105</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170813</td>\n",
       "      <td>0.294938</td>\n",
       "      <td>-0.306081</td>\n",
       "      <td>0.482148</td>\n",
       "      <td>-0.470129</td>\n",
       "      <td>-0.305693</td>\n",
       "      <td>-0.362654</td>\n",
       "      <td>0.507459</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>-0.269069</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>-0.088952</td>\n",
       "      <td>-0.155804</td>\n",
       "      <td>-0.189763</td>\n",
       "      <td>0.599213</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
       "0             0.288585            -0.020294            -0.132905   \n",
       "1             0.278419            -0.016411            -0.123520   \n",
       "2             0.279653            -0.019467            -0.113462   \n",
       "3             0.279174            -0.026201            -0.123283   \n",
       "4             0.276629            -0.016570            -0.115362   \n",
       "\n",
       "   4 tBodyAcc-std()-X  5 tBodyAcc-std()-Y  6 tBodyAcc-std()-Z  \\\n",
       "0           -0.995279           -0.983111           -0.913526   \n",
       "1           -0.998245           -0.975300           -0.960322   \n",
       "2           -0.995380           -0.967187           -0.978944   \n",
       "3           -0.996091           -0.983403           -0.990675   \n",
       "4           -0.998139           -0.980817           -0.990482   \n",
       "\n",
       "   7 tBodyAcc-mad()-X  8 tBodyAcc-mad()-Y  9 tBodyAcc-mad()-Z  \\\n",
       "0           -0.995112           -0.983185           -0.923527   \n",
       "1           -0.998807           -0.974914           -0.957686   \n",
       "2           -0.996520           -0.963668           -0.977469   \n",
       "3           -0.997099           -0.982750           -0.989302   \n",
       "4           -0.998321           -0.979672           -0.990441   \n",
       "\n",
       "   10 tBodyAcc-max()-X  ...  33 tBodyAcc-arCoeff()-Y,4  \\\n",
       "0            -0.934724  ...                  -0.095246   \n",
       "1            -0.943068  ...                  -0.281211   \n",
       "2            -0.938692  ...                  -0.332564   \n",
       "3            -0.938692  ...                  -0.170813   \n",
       "4            -0.942469  ...                  -0.315375   \n",
       "\n",
       "   34 tBodyAcc-arCoeff()-Z,1  35 tBodyAcc-arCoeff()-Z,2  \\\n",
       "0                   0.278851                  -0.465085   \n",
       "1                   0.085988                  -0.022153   \n",
       "2                   0.239281                  -0.136204   \n",
       "3                   0.294938                  -0.306081   \n",
       "4                   0.439744                  -0.269069   \n",
       "\n",
       "   36 tBodyAcc-arCoeff()-Z,3  37 tBodyAcc-arCoeff()-Z,4  \\\n",
       "0                   0.491936                  -0.190884   \n",
       "1                  -0.016657                  -0.220643   \n",
       "2                   0.173863                  -0.299493   \n",
       "3                   0.482148                  -0.470129   \n",
       "4                   0.179414                  -0.088952   \n",
       "\n",
       "   38 tBodyAcc-correlation()-X,Y  39 tBodyAcc-correlation()-X,Z  \\\n",
       "0                       0.376314                       0.435129   \n",
       "1                      -0.013429                      -0.072692   \n",
       "2                      -0.124698                      -0.181105   \n",
       "3                      -0.305693                      -0.362654   \n",
       "4                      -0.155804                      -0.189763   \n",
       "\n",
       "   40 tBodyAcc-correlation()-Y,Z  Activity  Subject  \n",
       "0                       0.660790         5        1  \n",
       "1                       0.579382         5        1  \n",
       "2                       0.608900         5        1  \n",
       "3                       0.507459         5        1  \n",
       "4                       0.599213         5        1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "names = name_dataframe.values.tolist()\n",
    "names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in train dataframe\n",
    "data.columns = names #Setting column names\n",
    "\n",
    "X_train = data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z'] #Selecting only acceleration columns\n",
    "\n",
    "y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train_subject.columns = ['Subject']\n",
    "\n",
    "GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "GAN_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, current_user_tested):\n",
    "    \"\"\"\n",
    "    data: DataFrame\n",
    "    current_user_tested: int\n",
    "    \n",
    "    Returns: numpy arrays of X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    data = data.copy(deep = True)\n",
    "    \n",
    "    #Getting only acceleration columns\n",
    "    X_train = data[data['Subject'] == current_user_tested].loc[:, \"1 tBodyAcc-mean()-X\": \"40 tBodyAcc-correlation()-Y,Z\"].values\n",
    "    y_train = data[data['Subject'] == current_user_tested].loc[:, \"Activity\"].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)\n",
    "    \n",
    "    #Zero indexing all activity labels since they start at 1\n",
    "    y_train -= 1 \n",
    "    y_test -=1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 40):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 6)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(X_train, y_train, X_test, y_test):\n",
    "    lr = 0.001\n",
    "    batch_size = 100\n",
    "    \n",
    "    model = Classifier().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    train_features = torch.tensor(X_train).to(device)\n",
    "    train_labels = torch.tensor(y_train).to(device)\n",
    "    test_features = torch.tensor(X_test).to(device)\n",
    "    test_labels = torch.tensor(y_test).to(device)\n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)\n",
    "    \n",
    "    return model, train_loader, test_loader, optimizer, criterion\n",
    "\n",
    "def training_loop(model, train_loader, test_loader, optimizer, criterion, sub_num, n_epochs = 1000):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features.float())\n",
    "\n",
    "            loss = criterion(preds, labels) \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Subject {sub_num}, Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')\n",
    "    \n",
    "    return model, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, test_loader):\n",
    "    softmax = nn.Softmax(dim = 1)\n",
    "    for batch in test_loader: #Runs once since the batch is the entire testing data\n",
    "        features, labels = batch\n",
    "        _, preds = torch.max(softmax(model(features.float())), dim = 1) #Getting the model's predictions\n",
    "        report = metrics.classification_report(labels.cpu(), preds.cpu(), digits = 3, output_dict = True)\n",
    "        f1_score = pd.DataFrame(report).transpose().loc['weighted avg', :]['f1-score']\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_numbers = list(GAN_data['Subject'].unique()) #list of all unique subject numbers\n",
    "n_subjects = len(subject_numbers) #Number of unique subjects\n",
    "n_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 1, Loss: 5.456624269485474, Final Batch Loss: 1.819804072380066\n",
      "Subject 0, Epoch 2, Loss: 5.4452574253082275, Final Batch Loss: 1.828585147857666\n",
      "Subject 0, Epoch 3, Loss: 5.432973623275757, Final Batch Loss: 1.819122314453125\n",
      "Subject 0, Epoch 4, Loss: 5.412111163139343, Final Batch Loss: 1.8036606311798096\n",
      "Subject 0, Epoch 5, Loss: 5.405317544937134, Final Batch Loss: 1.8095568418502808\n",
      "Subject 0, Epoch 6, Loss: 5.400514006614685, Final Batch Loss: 1.8025137186050415\n",
      "Subject 0, Epoch 7, Loss: 5.380220293998718, Final Batch Loss: 1.762546181678772\n",
      "Subject 0, Epoch 8, Loss: 5.363085031509399, Final Batch Loss: 1.7890852689743042\n",
      "Subject 0, Epoch 9, Loss: 5.33820903301239, Final Batch Loss: 1.7749247550964355\n",
      "Subject 0, Epoch 10, Loss: 5.320154070854187, Final Batch Loss: 1.755284070968628\n",
      "Subject 0, Epoch 11, Loss: 5.289071321487427, Final Batch Loss: 1.7397757768630981\n",
      "Subject 0, Epoch 12, Loss: 5.2653738260269165, Final Batch Loss: 1.745194435119629\n",
      "Subject 0, Epoch 13, Loss: 5.226337432861328, Final Batch Loss: 1.7380402088165283\n",
      "Subject 0, Epoch 14, Loss: 5.185375690460205, Final Batch Loss: 1.7231754064559937\n",
      "Subject 0, Epoch 15, Loss: 5.129805564880371, Final Batch Loss: 1.718308925628662\n",
      "Subject 0, Epoch 16, Loss: 5.079731345176697, Final Batch Loss: 1.675093412399292\n",
      "Subject 0, Epoch 17, Loss: 5.033445954322815, Final Batch Loss: 1.6689116954803467\n",
      "Subject 0, Epoch 18, Loss: 4.954401135444641, Final Batch Loss: 1.6536506414413452\n",
      "Subject 0, Epoch 19, Loss: 4.864556550979614, Final Batch Loss: 1.6213099956512451\n",
      "Subject 0, Epoch 20, Loss: 4.794883728027344, Final Batch Loss: 1.6111408472061157\n",
      "Subject 0, Epoch 21, Loss: 4.687402129173279, Final Batch Loss: 1.5911096334457397\n",
      "Subject 0, Epoch 22, Loss: 4.5723665952682495, Final Batch Loss: 1.5150070190429688\n",
      "Subject 0, Epoch 23, Loss: 4.522810101509094, Final Batch Loss: 1.491844654083252\n",
      "Subject 0, Epoch 24, Loss: 4.410011053085327, Final Batch Loss: 1.4291719198226929\n",
      "Subject 0, Epoch 25, Loss: 4.268383979797363, Final Batch Loss: 1.3922231197357178\n",
      "Subject 0, Epoch 26, Loss: 4.281209349632263, Final Batch Loss: 1.3815680742263794\n",
      "Subject 0, Epoch 27, Loss: 4.180133819580078, Final Batch Loss: 1.4262747764587402\n",
      "Subject 0, Epoch 28, Loss: 4.12490439414978, Final Batch Loss: 1.3645248413085938\n",
      "Subject 0, Epoch 29, Loss: 3.996816873550415, Final Batch Loss: 1.3418784141540527\n",
      "Subject 0, Epoch 30, Loss: 3.9333945512771606, Final Batch Loss: 1.3412647247314453\n",
      "Subject 0, Epoch 31, Loss: 3.8319013118743896, Final Batch Loss: 1.333621859550476\n",
      "Subject 0, Epoch 32, Loss: 3.833685040473938, Final Batch Loss: 1.269042730331421\n",
      "Subject 0, Epoch 33, Loss: 3.7613537311553955, Final Batch Loss: 1.2505570650100708\n",
      "Subject 0, Epoch 34, Loss: 3.6595041751861572, Final Batch Loss: 1.219455599784851\n",
      "Subject 0, Epoch 35, Loss: 3.585542917251587, Final Batch Loss: 1.2580382823944092\n",
      "Subject 0, Epoch 36, Loss: 3.6565955877304077, Final Batch Loss: 1.2076747417449951\n",
      "Subject 0, Epoch 37, Loss: 3.6095365285873413, Final Batch Loss: 1.1661473512649536\n",
      "Subject 0, Epoch 38, Loss: 3.598601222038269, Final Batch Loss: 1.21668541431427\n",
      "Subject 0, Epoch 39, Loss: 3.5793981552124023, Final Batch Loss: 1.152578353881836\n",
      "Subject 0, Epoch 40, Loss: 3.5231821537017822, Final Batch Loss: 1.1876813173294067\n",
      "Subject 0, Epoch 41, Loss: 3.4642648696899414, Final Batch Loss: 1.1930290460586548\n",
      "Subject 0, Epoch 42, Loss: 3.440171718597412, Final Batch Loss: 1.144938349723816\n",
      "Subject 0, Epoch 43, Loss: 3.4109619855880737, Final Batch Loss: 1.1196273565292358\n",
      "Subject 0, Epoch 44, Loss: 3.3966550827026367, Final Batch Loss: 1.1204793453216553\n",
      "Subject 0, Epoch 45, Loss: 3.4611605405807495, Final Batch Loss: 1.2132173776626587\n",
      "Subject 0, Epoch 46, Loss: 3.305383324623108, Final Batch Loss: 1.0846163034439087\n",
      "Subject 0, Epoch 47, Loss: 3.327606201171875, Final Batch Loss: 1.1093506813049316\n",
      "Subject 0, Epoch 48, Loss: 3.2222379446029663, Final Batch Loss: 1.0561854839324951\n",
      "Subject 0, Epoch 49, Loss: 3.1650171875953674, Final Batch Loss: 0.9956453442573547\n",
      "Subject 0, Epoch 50, Loss: 3.216505289077759, Final Batch Loss: 1.093087077140808\n",
      "Subject 0, Epoch 51, Loss: 3.1276591420173645, Final Batch Loss: 0.9822880625724792\n",
      "Subject 0, Epoch 52, Loss: 3.026983380317688, Final Batch Loss: 0.9711102247238159\n",
      "Subject 0, Epoch 53, Loss: 3.01692795753479, Final Batch Loss: 1.0149779319763184\n",
      "Subject 0, Epoch 54, Loss: 3.0596998929977417, Final Batch Loss: 1.0237743854522705\n",
      "Subject 0, Epoch 55, Loss: 2.9832738637924194, Final Batch Loss: 0.9946562647819519\n",
      "Subject 0, Epoch 56, Loss: 2.9222843050956726, Final Batch Loss: 0.9763174653053284\n",
      "Subject 0, Epoch 57, Loss: 2.9248984456062317, Final Batch Loss: 0.9864436984062195\n",
      "Subject 0, Epoch 58, Loss: 2.8829808831214905, Final Batch Loss: 0.9463707208633423\n",
      "Subject 0, Epoch 59, Loss: 2.7806344628334045, Final Batch Loss: 0.9179352521896362\n",
      "Subject 0, Epoch 60, Loss: 2.801652669906616, Final Batch Loss: 0.9120576977729797\n",
      "Subject 0, Epoch 61, Loss: 2.6917057633399963, Final Batch Loss: 0.8866227269172668\n",
      "Subject 0, Epoch 62, Loss: 2.7259292006492615, Final Batch Loss: 0.8741248846054077\n",
      "Subject 0, Epoch 63, Loss: 2.6578919291496277, Final Batch Loss: 0.8700015544891357\n",
      "Subject 0, Epoch 64, Loss: 2.6564109325408936, Final Batch Loss: 0.9373787641525269\n",
      "Subject 0, Epoch 65, Loss: 2.6222100257873535, Final Batch Loss: 0.9078207612037659\n",
      "Subject 0, Epoch 66, Loss: 2.6818222999572754, Final Batch Loss: 0.8998094797134399\n",
      "Subject 0, Epoch 67, Loss: 2.5852404832839966, Final Batch Loss: 0.863519549369812\n",
      "Subject 0, Epoch 68, Loss: 2.5074195861816406, Final Batch Loss: 0.8077263236045837\n",
      "Subject 0, Epoch 69, Loss: 2.5634210109710693, Final Batch Loss: 0.8459113836288452\n",
      "Subject 0, Epoch 70, Loss: 2.5180962085723877, Final Batch Loss: 0.8332929611206055\n",
      "Subject 0, Epoch 71, Loss: 2.4816128611564636, Final Batch Loss: 0.9093936085700989\n",
      "Subject 0, Epoch 72, Loss: 2.4506048560142517, Final Batch Loss: 0.771402895450592\n",
      "Subject 0, Epoch 73, Loss: 2.3890862464904785, Final Batch Loss: 0.8178389668464661\n",
      "Subject 0, Epoch 74, Loss: 2.4596978425979614, Final Batch Loss: 0.7347939014434814\n",
      "Subject 0, Epoch 75, Loss: 2.425125241279602, Final Batch Loss: 0.7893896698951721\n",
      "Subject 0, Epoch 76, Loss: 2.503937005996704, Final Batch Loss: 0.8564728498458862\n",
      "Subject 0, Epoch 77, Loss: 2.432088017463684, Final Batch Loss: 0.833135724067688\n",
      "Subject 0, Epoch 78, Loss: 2.4344338178634644, Final Batch Loss: 0.7961138486862183\n",
      "Subject 0, Epoch 79, Loss: 2.4167691469192505, Final Batch Loss: 0.7916077375411987\n",
      "Subject 0, Epoch 80, Loss: 2.3165634870529175, Final Batch Loss: 0.7690436840057373\n",
      "Subject 0, Epoch 81, Loss: 2.3704161047935486, Final Batch Loss: 0.8266976475715637\n",
      "Subject 0, Epoch 82, Loss: 2.2427449822425842, Final Batch Loss: 0.7153076529502869\n",
      "Subject 0, Epoch 83, Loss: 2.2800172567367554, Final Batch Loss: 0.737583577632904\n",
      "Subject 0, Epoch 84, Loss: 2.224015951156616, Final Batch Loss: 0.779862642288208\n",
      "Subject 0, Epoch 85, Loss: 2.3052855134010315, Final Batch Loss: 0.7929946184158325\n",
      "Subject 0, Epoch 86, Loss: 2.289862871170044, Final Batch Loss: 0.7344303727149963\n",
      "Subject 0, Epoch 87, Loss: 2.2470842003822327, Final Batch Loss: 0.7332844138145447\n",
      "Subject 0, Epoch 88, Loss: 2.22274249792099, Final Batch Loss: 0.7537965774536133\n",
      "Subject 0, Epoch 89, Loss: 2.2552578449249268, Final Batch Loss: 0.7074933052062988\n",
      "Subject 0, Epoch 90, Loss: 2.1671003699302673, Final Batch Loss: 0.6448133587837219\n",
      "Subject 0, Epoch 91, Loss: 2.1615554690361023, Final Batch Loss: 0.7295229434967041\n",
      "Subject 0, Epoch 92, Loss: 2.1537826657295227, Final Batch Loss: 0.6464929580688477\n",
      "Subject 0, Epoch 93, Loss: 2.1649245023727417, Final Batch Loss: 0.7769351005554199\n",
      "Subject 0, Epoch 94, Loss: 2.2176197171211243, Final Batch Loss: 0.7014833092689514\n",
      "Subject 0, Epoch 95, Loss: 2.20360666513443, Final Batch Loss: 0.8186749219894409\n",
      "Subject 0, Epoch 96, Loss: 2.1957032084465027, Final Batch Loss: 0.7324572801589966\n",
      "Subject 0, Epoch 97, Loss: 2.091241955757141, Final Batch Loss: 0.7502753138542175\n",
      "Subject 0, Epoch 98, Loss: 2.1673324704170227, Final Batch Loss: 0.7636576890945435\n",
      "Subject 0, Epoch 99, Loss: 2.0172370076179504, Final Batch Loss: 0.6829351782798767\n",
      "Subject 0, Epoch 100, Loss: 2.021517276763916, Final Batch Loss: 0.6473020315170288\n",
      "Subject 0, Epoch 101, Loss: 1.9746403694152832, Final Batch Loss: 0.6798596382141113\n",
      "Subject 0, Epoch 102, Loss: 2.0155158638954163, Final Batch Loss: 0.6700292825698853\n",
      "Subject 0, Epoch 103, Loss: 2.123366415500641, Final Batch Loss: 0.6378211379051208\n",
      "Subject 0, Epoch 104, Loss: 1.9260376691818237, Final Batch Loss: 0.6022564768791199\n",
      "Subject 0, Epoch 105, Loss: 1.9039118885993958, Final Batch Loss: 0.627088725566864\n",
      "Subject 0, Epoch 106, Loss: 1.9628797769546509, Final Batch Loss: 0.6133891940116882\n",
      "Subject 0, Epoch 107, Loss: 1.9847797751426697, Final Batch Loss: 0.6730931997299194\n",
      "Subject 0, Epoch 108, Loss: 1.9051276445388794, Final Batch Loss: 0.6448507308959961\n",
      "Subject 0, Epoch 109, Loss: 1.9468398690223694, Final Batch Loss: 0.663761556148529\n",
      "Subject 0, Epoch 110, Loss: 1.8816041946411133, Final Batch Loss: 0.5877321362495422\n",
      "Subject 0, Epoch 111, Loss: 1.7432610988616943, Final Batch Loss: 0.5747765302658081\n",
      "Subject 0, Epoch 112, Loss: 1.847676694393158, Final Batch Loss: 0.5650879740715027\n",
      "Subject 0, Epoch 113, Loss: 1.832280695438385, Final Batch Loss: 0.6279375553131104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 114, Loss: 1.914885401725769, Final Batch Loss: 0.6398683786392212\n",
      "Subject 0, Epoch 115, Loss: 1.793815553188324, Final Batch Loss: 0.543848991394043\n",
      "Subject 0, Epoch 116, Loss: 1.8201097249984741, Final Batch Loss: 0.6440517902374268\n",
      "Subject 0, Epoch 117, Loss: 1.7998219728469849, Final Batch Loss: 0.6323148012161255\n",
      "Subject 0, Epoch 118, Loss: 1.8430272340774536, Final Batch Loss: 0.6041670441627502\n",
      "Subject 0, Epoch 119, Loss: 1.7915604710578918, Final Batch Loss: 0.6110338568687439\n",
      "Subject 0, Epoch 120, Loss: 1.736860990524292, Final Batch Loss: 0.6020115613937378\n",
      "Subject 0, Epoch 121, Loss: 1.637149155139923, Final Batch Loss: 0.5861057043075562\n",
      "Subject 0, Epoch 122, Loss: 1.7026094794273376, Final Batch Loss: 0.5115756392478943\n",
      "Subject 0, Epoch 123, Loss: 1.7233060598373413, Final Batch Loss: 0.5487342476844788\n",
      "Subject 0, Epoch 124, Loss: 1.7409935593605042, Final Batch Loss: 0.5283254384994507\n",
      "Subject 0, Epoch 125, Loss: 1.6954864859580994, Final Batch Loss: 0.5884921550750732\n",
      "Subject 0, Epoch 126, Loss: 1.7101614475250244, Final Batch Loss: 0.5177786946296692\n",
      "Subject 0, Epoch 127, Loss: 1.6746626496315002, Final Batch Loss: 0.594353437423706\n",
      "Subject 0, Epoch 128, Loss: 1.7323204278945923, Final Batch Loss: 0.6826108694076538\n",
      "Subject 0, Epoch 129, Loss: 1.6421024799346924, Final Batch Loss: 0.5155829787254333\n",
      "Subject 0, Epoch 130, Loss: 1.7297075390815735, Final Batch Loss: 0.4927293062210083\n",
      "Subject 0, Epoch 131, Loss: 1.6594493389129639, Final Batch Loss: 0.5523875951766968\n",
      "Subject 0, Epoch 132, Loss: 1.7324804067611694, Final Batch Loss: 0.5489387512207031\n",
      "Subject 0, Epoch 133, Loss: 1.6107974648475647, Final Batch Loss: 0.6056756973266602\n",
      "Subject 0, Epoch 134, Loss: 1.6166841387748718, Final Batch Loss: 0.5739015936851501\n",
      "Subject 0, Epoch 135, Loss: 1.6087508797645569, Final Batch Loss: 0.5108805298805237\n",
      "Subject 0, Epoch 136, Loss: 1.6667491793632507, Final Batch Loss: 0.5532719492912292\n",
      "Subject 0, Epoch 137, Loss: 1.6575018763542175, Final Batch Loss: 0.5642356276512146\n",
      "Subject 0, Epoch 138, Loss: 1.6336957514286041, Final Batch Loss: 0.491444855928421\n",
      "Subject 0, Epoch 139, Loss: 1.6288081109523773, Final Batch Loss: 0.5952802896499634\n",
      "Subject 0, Epoch 140, Loss: 1.6333935856819153, Final Batch Loss: 0.5152906775474548\n",
      "Subject 0, Epoch 141, Loss: 1.6444759964942932, Final Batch Loss: 0.521370530128479\n",
      "Subject 0, Epoch 142, Loss: 1.6199817061424255, Final Batch Loss: 0.5139126181602478\n",
      "Subject 0, Epoch 143, Loss: 1.6042430400848389, Final Batch Loss: 0.511313259601593\n",
      "Subject 0, Epoch 144, Loss: 1.5483767688274384, Final Batch Loss: 0.5133317708969116\n",
      "Subject 0, Epoch 145, Loss: 1.5973567962646484, Final Batch Loss: 0.5639041066169739\n",
      "Subject 0, Epoch 146, Loss: 1.5263909995555878, Final Batch Loss: 0.4865412712097168\n",
      "Subject 0, Epoch 147, Loss: 1.5253477990627289, Final Batch Loss: 0.4130707085132599\n",
      "Subject 0, Epoch 148, Loss: 1.6008698344230652, Final Batch Loss: 0.6079990267753601\n",
      "Subject 0, Epoch 149, Loss: 1.6562767624855042, Final Batch Loss: 0.6472578048706055\n",
      "Subject 0, Epoch 150, Loss: 1.584906280040741, Final Batch Loss: 0.5068817734718323\n",
      "Subject 0, Epoch 151, Loss: 1.5873088538646698, Final Batch Loss: 0.5713159441947937\n",
      "Subject 0, Epoch 152, Loss: 1.4853116571903229, Final Batch Loss: 0.44213631749153137\n",
      "Subject 0, Epoch 153, Loss: 1.6386820673942566, Final Batch Loss: 0.5426889657974243\n",
      "Subject 0, Epoch 154, Loss: 1.532446563243866, Final Batch Loss: 0.49620741605758667\n",
      "Subject 0, Epoch 155, Loss: 1.5554790496826172, Final Batch Loss: 0.4897170066833496\n",
      "Subject 0, Epoch 156, Loss: 1.5954330265522003, Final Batch Loss: 0.5898328423500061\n",
      "Subject 0, Epoch 157, Loss: 1.5036574304103851, Final Batch Loss: 0.5547312498092651\n",
      "Subject 0, Epoch 158, Loss: 1.5281585156917572, Final Batch Loss: 0.5428891181945801\n",
      "Subject 0, Epoch 159, Loss: 1.5279624462127686, Final Batch Loss: 0.550125241279602\n",
      "Subject 0, Epoch 160, Loss: 1.5067971050739288, Final Batch Loss: 0.5036540627479553\n",
      "Subject 0, Epoch 161, Loss: 1.5137321054935455, Final Batch Loss: 0.49301809072494507\n",
      "Subject 0, Epoch 162, Loss: 1.5396862626075745, Final Batch Loss: 0.5015782117843628\n",
      "Subject 0, Epoch 163, Loss: 1.5714492201805115, Final Batch Loss: 0.524213969707489\n",
      "Subject 0, Epoch 164, Loss: 1.4776002168655396, Final Batch Loss: 0.4680452048778534\n",
      "Subject 0, Epoch 165, Loss: 1.5656930208206177, Final Batch Loss: 0.5835499167442322\n",
      "Subject 0, Epoch 166, Loss: 1.4766061007976532, Final Batch Loss: 0.4359716773033142\n",
      "Subject 0, Epoch 167, Loss: 1.4961281418800354, Final Batch Loss: 0.522889256477356\n",
      "Subject 0, Epoch 168, Loss: 1.5070004761219025, Final Batch Loss: 0.5233875513076782\n",
      "Subject 0, Epoch 169, Loss: 1.4885093867778778, Final Batch Loss: 0.44343993067741394\n",
      "Subject 0, Epoch 170, Loss: 1.4901525974273682, Final Batch Loss: 0.4750604033470154\n",
      "Subject 0, Epoch 171, Loss: 1.4883221685886383, Final Batch Loss: 0.46660879254341125\n",
      "Subject 0, Epoch 172, Loss: 1.4169119894504547, Final Batch Loss: 0.42649582028388977\n",
      "Subject 0, Epoch 173, Loss: 1.4793410897254944, Final Batch Loss: 0.5184491872787476\n",
      "Subject 0, Epoch 174, Loss: 1.45699942111969, Final Batch Loss: 0.4588615298271179\n",
      "Subject 0, Epoch 175, Loss: 1.468990445137024, Final Batch Loss: 0.42003971338272095\n",
      "Subject 0, Epoch 176, Loss: 1.4592964947223663, Final Batch Loss: 0.5836564302444458\n",
      "Subject 0, Epoch 177, Loss: 1.4457831382751465, Final Batch Loss: 0.5595113039016724\n",
      "Subject 0, Epoch 178, Loss: 1.5333244800567627, Final Batch Loss: 0.4359128475189209\n",
      "Subject 0, Epoch 179, Loss: 1.471779704093933, Final Batch Loss: 0.5070250630378723\n",
      "Subject 0, Epoch 180, Loss: 1.4733096659183502, Final Batch Loss: 0.5736849904060364\n",
      "Subject 0, Epoch 181, Loss: 1.4939531087875366, Final Batch Loss: 0.5183138251304626\n",
      "Subject 0, Epoch 182, Loss: 1.574747622013092, Final Batch Loss: 0.5160855054855347\n",
      "Subject 0, Epoch 183, Loss: 1.4969403147697449, Final Batch Loss: 0.4724586606025696\n",
      "Subject 0, Epoch 184, Loss: 1.5093469321727753, Final Batch Loss: 0.4940646290779114\n",
      "Subject 0, Epoch 185, Loss: 1.4342985153198242, Final Batch Loss: 0.4754001796245575\n",
      "Subject 0, Epoch 186, Loss: 1.4713480770587921, Final Batch Loss: 0.5146286487579346\n",
      "Subject 0, Epoch 187, Loss: 1.4651876986026764, Final Batch Loss: 0.45998615026474\n",
      "Subject 0, Epoch 188, Loss: 1.474031001329422, Final Batch Loss: 0.43779832124710083\n",
      "Subject 0, Epoch 189, Loss: 1.4135650396347046, Final Batch Loss: 0.583980143070221\n",
      "Subject 0, Epoch 190, Loss: 1.4215807616710663, Final Batch Loss: 0.47301262617111206\n",
      "Subject 0, Epoch 191, Loss: 1.4482730031013489, Final Batch Loss: 0.4712054431438446\n",
      "Subject 0, Epoch 192, Loss: 1.41503044962883, Final Batch Loss: 0.45133504271507263\n",
      "Subject 0, Epoch 193, Loss: 1.4851150512695312, Final Batch Loss: 0.5210565328598022\n",
      "Subject 0, Epoch 194, Loss: 1.4432015419006348, Final Batch Loss: 0.5446697473526001\n",
      "Subject 0, Epoch 195, Loss: 1.4386610388755798, Final Batch Loss: 0.45331475138664246\n",
      "Subject 0, Epoch 196, Loss: 1.4346754848957062, Final Batch Loss: 0.4503639340400696\n",
      "Subject 0, Epoch 197, Loss: 1.4422589838504791, Final Batch Loss: 0.5285400152206421\n",
      "Subject 0, Epoch 198, Loss: 1.4846832156181335, Final Batch Loss: 0.4567084312438965\n",
      "Subject 0, Epoch 199, Loss: 1.4593829810619354, Final Batch Loss: 0.47118431329727173\n",
      "Subject 0, Epoch 200, Loss: 1.476602166891098, Final Batch Loss: 0.4594925343990326\n",
      "Subject 0, Epoch 201, Loss: 1.4622052013874054, Final Batch Loss: 0.48246991634368896\n",
      "Subject 0, Epoch 202, Loss: 1.3921187222003937, Final Batch Loss: 0.37562963366508484\n",
      "Subject 0, Epoch 203, Loss: 1.4200518429279327, Final Batch Loss: 0.5077612400054932\n",
      "Subject 0, Epoch 204, Loss: 1.3968801200389862, Final Batch Loss: 0.515095591545105\n",
      "Subject 0, Epoch 205, Loss: 1.4411157667636871, Final Batch Loss: 0.4962407350540161\n",
      "Subject 0, Epoch 206, Loss: 1.4673859179019928, Final Batch Loss: 0.5195339918136597\n",
      "Subject 0, Epoch 207, Loss: 1.4517264664173126, Final Batch Loss: 0.4873732328414917\n",
      "Subject 0, Epoch 208, Loss: 1.4562546014785767, Final Batch Loss: 0.5868741869926453\n",
      "Subject 0, Epoch 209, Loss: 1.4478490054607391, Final Batch Loss: 0.49390891194343567\n",
      "Subject 0, Epoch 210, Loss: 1.4725591838359833, Final Batch Loss: 0.5259175896644592\n",
      "Subject 0, Epoch 211, Loss: 1.4600935578346252, Final Batch Loss: 0.5415641069412231\n",
      "Subject 0, Epoch 212, Loss: 1.378450632095337, Final Batch Loss: 0.5343155264854431\n",
      "Subject 0, Epoch 213, Loss: 1.3924968838691711, Final Batch Loss: 0.43375495076179504\n",
      "Subject 0, Epoch 214, Loss: 1.4327262938022614, Final Batch Loss: 0.4906158447265625\n",
      "Subject 0, Epoch 215, Loss: 1.4320139288902283, Final Batch Loss: 0.4730636477470398\n",
      "Subject 0, Epoch 216, Loss: 1.436889410018921, Final Batch Loss: 0.5018452405929565\n",
      "Subject 0, Epoch 217, Loss: 1.4140167534351349, Final Batch Loss: 0.4380537271499634\n",
      "Subject 0, Epoch 218, Loss: 1.3886587917804718, Final Batch Loss: 0.42996373772621155\n",
      "Subject 0, Epoch 219, Loss: 1.3978671729564667, Final Batch Loss: 0.4890824556350708\n",
      "Subject 0, Epoch 220, Loss: 1.385114997625351, Final Batch Loss: 0.4844829738140106\n",
      "Subject 0, Epoch 221, Loss: 1.3950091302394867, Final Batch Loss: 0.46229833364486694\n",
      "Subject 0, Epoch 222, Loss: 1.3947010934352875, Final Batch Loss: 0.4768247902393341\n",
      "Subject 0, Epoch 223, Loss: 1.4258763790130615, Final Batch Loss: 0.5532293915748596\n",
      "Subject 0, Epoch 224, Loss: 1.4090100526809692, Final Batch Loss: 0.5104147791862488\n",
      "Subject 0, Epoch 225, Loss: 1.448529064655304, Final Batch Loss: 0.48514044284820557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 226, Loss: 1.4219816029071808, Final Batch Loss: 0.42409440875053406\n",
      "Subject 0, Epoch 227, Loss: 1.3759199976921082, Final Batch Loss: 0.4151027798652649\n",
      "Subject 0, Epoch 228, Loss: 1.365662306547165, Final Batch Loss: 0.4191722571849823\n",
      "Subject 0, Epoch 229, Loss: 1.4288081228733063, Final Batch Loss: 0.4228740334510803\n",
      "Subject 0, Epoch 230, Loss: 1.4014484286308289, Final Batch Loss: 0.4972996115684509\n",
      "Subject 0, Epoch 231, Loss: 1.3827761709690094, Final Batch Loss: 0.5994147658348083\n",
      "Subject 0, Epoch 232, Loss: 1.3715060651302338, Final Batch Loss: 0.39278897643089294\n",
      "Subject 0, Epoch 233, Loss: 1.3475214838981628, Final Batch Loss: 0.3742900490760803\n",
      "Subject 0, Epoch 234, Loss: 1.3844731450080872, Final Batch Loss: 0.46348294615745544\n",
      "Subject 0, Epoch 235, Loss: 1.47956183552742, Final Batch Loss: 0.5928741693496704\n",
      "Subject 0, Epoch 236, Loss: 1.3625699281692505, Final Batch Loss: 0.47842228412628174\n",
      "Subject 0, Epoch 237, Loss: 1.3900305330753326, Final Batch Loss: 0.4439442455768585\n",
      "Subject 0, Epoch 238, Loss: 1.3874552845954895, Final Batch Loss: 0.42786118388175964\n",
      "Subject 0, Epoch 239, Loss: 1.3794097900390625, Final Batch Loss: 0.46554791927337646\n",
      "Subject 0, Epoch 240, Loss: 1.3067623972892761, Final Batch Loss: 0.4628324806690216\n",
      "Subject 0, Epoch 241, Loss: 1.4325318336486816, Final Batch Loss: 0.4481397569179535\n",
      "Subject 0, Epoch 242, Loss: 1.3202113211154938, Final Batch Loss: 0.491435706615448\n",
      "Subject 0, Epoch 243, Loss: 1.3287507891654968, Final Batch Loss: 0.41569697856903076\n",
      "Subject 0, Epoch 244, Loss: 1.3250748813152313, Final Batch Loss: 0.38881316781044006\n",
      "Subject 0, Epoch 245, Loss: 1.3709207773208618, Final Batch Loss: 0.30821746587753296\n",
      "Subject 0, Epoch 246, Loss: 1.3449482023715973, Final Batch Loss: 0.5135681629180908\n",
      "Subject 0, Epoch 247, Loss: 1.3075458705425262, Final Batch Loss: 0.4722020626068115\n",
      "Subject 0, Epoch 248, Loss: 1.3503124713897705, Final Batch Loss: 0.4932670295238495\n",
      "Subject 0, Epoch 249, Loss: 1.3297994136810303, Final Batch Loss: 0.5514917373657227\n",
      "Subject 0, Epoch 250, Loss: 1.2986187934875488, Final Batch Loss: 0.3723052144050598\n",
      "Subject 0, Epoch 251, Loss: 1.3198725581169128, Final Batch Loss: 0.4693858027458191\n",
      "Subject 0, Epoch 252, Loss: 1.3684661388397217, Final Batch Loss: 0.4510525166988373\n",
      "Subject 0, Epoch 253, Loss: 1.3771394789218903, Final Batch Loss: 0.39208897948265076\n",
      "Subject 0, Epoch 254, Loss: 1.316094547510147, Final Batch Loss: 0.47051724791526794\n",
      "Subject 0, Epoch 255, Loss: 1.299787163734436, Final Batch Loss: 0.4709876477718353\n",
      "Subject 0, Epoch 256, Loss: 1.2896151542663574, Final Batch Loss: 0.436041921377182\n",
      "Subject 0, Epoch 257, Loss: 1.3230640292167664, Final Batch Loss: 0.45743054151535034\n",
      "Subject 0, Epoch 258, Loss: 1.3025768399238586, Final Batch Loss: 0.3381170630455017\n",
      "Subject 0, Epoch 259, Loss: 1.2539623975753784, Final Batch Loss: 0.39607471227645874\n",
      "Subject 0, Epoch 260, Loss: 1.2427424490451813, Final Batch Loss: 0.41047534346580505\n",
      "Subject 0, Epoch 261, Loss: 1.2896727919578552, Final Batch Loss: 0.5066883563995361\n",
      "Subject 0, Epoch 262, Loss: 1.1768434643745422, Final Batch Loss: 0.36645954847335815\n",
      "Subject 0, Epoch 263, Loss: 1.272005707025528, Final Batch Loss: 0.40385201573371887\n",
      "Subject 0, Epoch 264, Loss: 1.190332055091858, Final Batch Loss: 0.38561975955963135\n",
      "Subject 0, Epoch 265, Loss: 1.3121936917304993, Final Batch Loss: 0.4074237048625946\n",
      "Subject 0, Epoch 266, Loss: 1.2784742414951324, Final Batch Loss: 0.41076555848121643\n",
      "Subject 0, Epoch 267, Loss: 1.2024667859077454, Final Batch Loss: 0.3948328197002411\n",
      "Subject 0, Epoch 268, Loss: 1.1674333214759827, Final Batch Loss: 0.44443100690841675\n",
      "Subject 0, Epoch 269, Loss: 1.2859748601913452, Final Batch Loss: 0.46672800183296204\n",
      "Subject 0, Epoch 270, Loss: 1.226890653371811, Final Batch Loss: 0.4332848787307739\n",
      "Subject 0, Epoch 271, Loss: 1.2241205871105194, Final Batch Loss: 0.40035122632980347\n",
      "Subject 0, Epoch 272, Loss: 1.1427533328533173, Final Batch Loss: 0.3227391541004181\n",
      "Subject 0, Epoch 273, Loss: 1.125061810016632, Final Batch Loss: 0.3606977164745331\n",
      "Subject 0, Epoch 274, Loss: 1.2515275180339813, Final Batch Loss: 0.34714964032173157\n",
      "Subject 0, Epoch 275, Loss: 1.1761248409748077, Final Batch Loss: 0.3807470500469208\n",
      "Subject 0, Epoch 276, Loss: 1.1424067914485931, Final Batch Loss: 0.3543495535850525\n",
      "Subject 0, Epoch 277, Loss: 1.139267086982727, Final Batch Loss: 0.32702529430389404\n",
      "Subject 0, Epoch 278, Loss: 1.1912379264831543, Final Batch Loss: 0.3849642872810364\n",
      "Subject 0, Epoch 279, Loss: 1.1781985461711884, Final Batch Loss: 0.3980523645877838\n",
      "Subject 0, Epoch 280, Loss: 1.1086224019527435, Final Batch Loss: 0.2839217185974121\n",
      "Subject 0, Epoch 281, Loss: 1.0975728034973145, Final Batch Loss: 0.40851840376853943\n",
      "Subject 0, Epoch 282, Loss: 1.1350938975811005, Final Batch Loss: 0.43109235167503357\n",
      "Subject 0, Epoch 283, Loss: 1.1424820721149445, Final Batch Loss: 0.3517061471939087\n",
      "Subject 0, Epoch 284, Loss: 1.1558740139007568, Final Batch Loss: 0.34293290972709656\n",
      "Subject 0, Epoch 285, Loss: 1.137649804353714, Final Batch Loss: 0.3946034908294678\n",
      "Subject 0, Epoch 286, Loss: 1.1044748723506927, Final Batch Loss: 0.3134196102619171\n",
      "Subject 0, Epoch 287, Loss: 1.0835221409797668, Final Batch Loss: 0.3854152262210846\n",
      "Subject 0, Epoch 288, Loss: 1.0796842277050018, Final Batch Loss: 0.3584434688091278\n",
      "Subject 0, Epoch 289, Loss: 1.1183969378471375, Final Batch Loss: 0.3348068296909332\n",
      "Subject 0, Epoch 290, Loss: 1.1381522715091705, Final Batch Loss: 0.38097742199897766\n",
      "Subject 0, Epoch 291, Loss: 1.1381987035274506, Final Batch Loss: 0.4258785545825958\n",
      "Subject 0, Epoch 292, Loss: 1.0610632300376892, Final Batch Loss: 0.3611207604408264\n",
      "Subject 0, Epoch 293, Loss: 1.039772242307663, Final Batch Loss: 0.36046621203422546\n",
      "Subject 0, Epoch 294, Loss: 1.2191908955574036, Final Batch Loss: 0.40622013807296753\n",
      "Subject 0, Epoch 295, Loss: 1.1224065124988556, Final Batch Loss: 0.34305593371391296\n",
      "Subject 0, Epoch 296, Loss: 1.0597120821475983, Final Batch Loss: 0.31308645009994507\n",
      "Subject 0, Epoch 297, Loss: 1.1345077753067017, Final Batch Loss: 0.3814050257205963\n",
      "Subject 0, Epoch 298, Loss: 1.0449264347553253, Final Batch Loss: 0.41407448053359985\n",
      "Subject 0, Epoch 299, Loss: 1.0822235643863678, Final Batch Loss: 0.4174201786518097\n",
      "Subject 0, Epoch 300, Loss: 1.027444750070572, Final Batch Loss: 0.3219911456108093\n",
      "Subject 0, Epoch 301, Loss: 1.1036737859249115, Final Batch Loss: 0.38339996337890625\n",
      "Subject 0, Epoch 302, Loss: 1.0939165949821472, Final Batch Loss: 0.37991863489151\n",
      "Subject 0, Epoch 303, Loss: 0.9972369372844696, Final Batch Loss: 0.35065147280693054\n",
      "Subject 0, Epoch 304, Loss: 1.0485759973526, Final Batch Loss: 0.3721192479133606\n",
      "Subject 0, Epoch 305, Loss: 0.9782011806964874, Final Batch Loss: 0.3344977796077728\n",
      "Subject 0, Epoch 306, Loss: 1.1016357839107513, Final Batch Loss: 0.33972787857055664\n",
      "Subject 0, Epoch 307, Loss: 1.0797909200191498, Final Batch Loss: 0.34928110241889954\n",
      "Subject 0, Epoch 308, Loss: 1.0980537235736847, Final Batch Loss: 0.3250468671321869\n",
      "Subject 0, Epoch 309, Loss: 0.9474113583564758, Final Batch Loss: 0.34569257497787476\n",
      "Subject 0, Epoch 310, Loss: 0.9948905408382416, Final Batch Loss: 0.2923244535923004\n",
      "Subject 0, Epoch 311, Loss: 1.0604813992977142, Final Batch Loss: 0.3333956003189087\n",
      "Subject 0, Epoch 312, Loss: 0.9312456250190735, Final Batch Loss: 0.33032482862472534\n",
      "Subject 0, Epoch 313, Loss: 0.9550841748714447, Final Batch Loss: 0.29333677887916565\n",
      "Subject 0, Epoch 314, Loss: 0.948395311832428, Final Batch Loss: 0.33214375376701355\n",
      "Subject 0, Epoch 315, Loss: 0.9266471564769745, Final Batch Loss: 0.31989848613739014\n",
      "Subject 0, Epoch 316, Loss: 0.9417645931243896, Final Batch Loss: 0.26388120651245117\n",
      "Subject 0, Epoch 317, Loss: 0.9469966292381287, Final Batch Loss: 0.3325251042842865\n",
      "Subject 0, Epoch 318, Loss: 0.927311509847641, Final Batch Loss: 0.31676149368286133\n",
      "Subject 0, Epoch 319, Loss: 0.9814583957195282, Final Batch Loss: 0.30699002742767334\n",
      "Subject 0, Epoch 320, Loss: 0.9260387420654297, Final Batch Loss: 0.3078575134277344\n",
      "Subject 0, Epoch 321, Loss: 0.8555433601140976, Final Batch Loss: 0.3261701762676239\n",
      "Subject 0, Epoch 322, Loss: 1.0090687274932861, Final Batch Loss: 0.3794700503349304\n",
      "Subject 0, Epoch 323, Loss: 0.9441240727901459, Final Batch Loss: 0.32784396409988403\n",
      "Subject 0, Epoch 324, Loss: 0.9421963393688202, Final Batch Loss: 0.32642194628715515\n",
      "Subject 0, Epoch 325, Loss: 0.9476884007453918, Final Batch Loss: 0.34557899832725525\n",
      "Subject 0, Epoch 326, Loss: 0.8929010927677155, Final Batch Loss: 0.2959479093551636\n",
      "Subject 0, Epoch 327, Loss: 1.0719435811042786, Final Batch Loss: 0.38246992230415344\n",
      "Subject 0, Epoch 328, Loss: 1.044740378856659, Final Batch Loss: 0.35662317276000977\n",
      "Subject 0, Epoch 329, Loss: 0.918390691280365, Final Batch Loss: 0.32838544249534607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 330, Loss: 0.9304144382476807, Final Batch Loss: 0.3465802073478699\n",
      "Subject 0, Epoch 331, Loss: 0.9303751885890961, Final Batch Loss: 0.27310094237327576\n",
      "Subject 0, Epoch 332, Loss: 0.9200699627399445, Final Batch Loss: 0.3206087648868561\n",
      "Subject 0, Epoch 333, Loss: 0.8874774873256683, Final Batch Loss: 0.2764609754085541\n",
      "Subject 0, Epoch 334, Loss: 0.8998711407184601, Final Batch Loss: 0.2839709520339966\n",
      "Subject 0, Epoch 335, Loss: 0.9310825616121292, Final Batch Loss: 0.23601655662059784\n",
      "Subject 0, Epoch 336, Loss: 0.9483212828636169, Final Batch Loss: 0.3368874490261078\n",
      "Subject 0, Epoch 337, Loss: 0.9858790040016174, Final Batch Loss: 0.3219295144081116\n",
      "Subject 0, Epoch 338, Loss: 0.9305860549211502, Final Batch Loss: 0.24732203781604767\n",
      "Subject 0, Epoch 339, Loss: 0.9378388822078705, Final Batch Loss: 0.353707492351532\n",
      "Subject 0, Epoch 340, Loss: 0.9401772618293762, Final Batch Loss: 0.3330047130584717\n",
      "Subject 0, Epoch 341, Loss: 0.9060929417610168, Final Batch Loss: 0.23867079615592957\n",
      "Subject 0, Epoch 342, Loss: 0.9518070816993713, Final Batch Loss: 0.31842848658561707\n",
      "Subject 0, Epoch 343, Loss: 1.024531215429306, Final Batch Loss: 0.34126630425453186\n",
      "Subject 0, Epoch 344, Loss: 0.8939924836158752, Final Batch Loss: 0.3051992356777191\n",
      "Subject 0, Epoch 345, Loss: 0.8933846056461334, Final Batch Loss: 0.2677188813686371\n",
      "Subject 0, Epoch 346, Loss: 0.901166170835495, Final Batch Loss: 0.31192266941070557\n",
      "Subject 0, Epoch 347, Loss: 0.9045177102088928, Final Batch Loss: 0.33448266983032227\n",
      "Subject 0, Epoch 348, Loss: 0.9051192700862885, Final Batch Loss: 0.2530943751335144\n",
      "Subject 0, Epoch 349, Loss: 0.9254017770290375, Final Batch Loss: 0.2905522286891937\n",
      "Subject 0, Epoch 350, Loss: 1.0143739581108093, Final Batch Loss: 0.33279138803482056\n",
      "Subject 0, Epoch 351, Loss: 0.8990459591150284, Final Batch Loss: 0.30019912123680115\n",
      "Subject 0, Epoch 352, Loss: 0.9104601144790649, Final Batch Loss: 0.3175206482410431\n",
      "Subject 0, Epoch 353, Loss: 0.9279472231864929, Final Batch Loss: 0.3234986960887909\n",
      "Subject 0, Epoch 354, Loss: 0.9318133294582367, Final Batch Loss: 0.3112177848815918\n",
      "Subject 0, Epoch 355, Loss: 0.8955055922269821, Final Batch Loss: 0.3006933033466339\n",
      "Subject 0, Epoch 356, Loss: 0.85680291056633, Final Batch Loss: 0.3297252357006073\n",
      "Subject 0, Epoch 357, Loss: 0.9751860499382019, Final Batch Loss: 0.36593642830848694\n",
      "Subject 0, Epoch 358, Loss: 0.7939671725034714, Final Batch Loss: 0.25083455443382263\n",
      "Subject 0, Epoch 359, Loss: 0.8381564319133759, Final Batch Loss: 0.2171928584575653\n",
      "Subject 0, Epoch 360, Loss: 0.9346651136875153, Final Batch Loss: 0.26986607909202576\n",
      "Subject 0, Epoch 361, Loss: 0.9064761400222778, Final Batch Loss: 0.20576968789100647\n",
      "Subject 0, Epoch 362, Loss: 0.8438308537006378, Final Batch Loss: 0.264102578163147\n",
      "Subject 0, Epoch 363, Loss: 0.9276633858680725, Final Batch Loss: 0.2967371642589569\n",
      "Subject 0, Epoch 364, Loss: 0.7853626012802124, Final Batch Loss: 0.26304006576538086\n",
      "Subject 0, Epoch 365, Loss: 0.890740692615509, Final Batch Loss: 0.27459999918937683\n",
      "Subject 0, Epoch 366, Loss: 0.8278736174106598, Final Batch Loss: 0.2756313383579254\n",
      "Subject 0, Epoch 367, Loss: 0.8992026299238205, Final Batch Loss: 0.33898672461509705\n",
      "Subject 0, Epoch 368, Loss: 0.8106510192155838, Final Batch Loss: 0.2726435363292694\n",
      "Subject 0, Epoch 369, Loss: 0.8389585912227631, Final Batch Loss: 0.3279721140861511\n",
      "Subject 0, Epoch 370, Loss: 0.7920280694961548, Final Batch Loss: 0.2798636853694916\n",
      "Subject 0, Epoch 371, Loss: 0.8915799260139465, Final Batch Loss: 0.33893823623657227\n",
      "Subject 0, Epoch 372, Loss: 0.8050018101930618, Final Batch Loss: 0.2723348140716553\n",
      "Subject 0, Epoch 373, Loss: 0.9049784243106842, Final Batch Loss: 0.31537607312202454\n",
      "Subject 0, Epoch 374, Loss: 0.8125338852405548, Final Batch Loss: 0.2487838864326477\n",
      "Subject 0, Epoch 375, Loss: 0.8074924945831299, Final Batch Loss: 0.22193828225135803\n",
      "Subject 0, Epoch 376, Loss: 0.8689406216144562, Final Batch Loss: 0.29104578495025635\n",
      "Subject 0, Epoch 377, Loss: 0.8321565687656403, Final Batch Loss: 0.29497942328453064\n",
      "Subject 0, Epoch 378, Loss: 0.8541008234024048, Final Batch Loss: 0.3509701192378998\n",
      "Subject 0, Epoch 379, Loss: 0.8450233489274979, Final Batch Loss: 0.2659073770046234\n",
      "Subject 0, Epoch 380, Loss: 0.8475895524024963, Final Batch Loss: 0.3203147053718567\n",
      "Subject 0, Epoch 381, Loss: 0.9448341578245163, Final Batch Loss: 0.35677722096443176\n",
      "Subject 0, Epoch 382, Loss: 0.7971392422914505, Final Batch Loss: 0.30943363904953003\n",
      "Subject 0, Epoch 383, Loss: 0.8067139685153961, Final Batch Loss: 0.26966631412506104\n",
      "Subject 0, Epoch 384, Loss: 0.8115451335906982, Final Batch Loss: 0.2776623070240021\n",
      "Subject 0, Epoch 385, Loss: 0.7609753161668777, Final Batch Loss: 0.21393240988254547\n",
      "Subject 0, Epoch 386, Loss: 0.83966064453125, Final Batch Loss: 0.28346866369247437\n",
      "Subject 0, Epoch 387, Loss: 1.030821144580841, Final Batch Loss: 0.4122307002544403\n",
      "Subject 0, Epoch 388, Loss: 0.8258169740438461, Final Batch Loss: 0.2735973596572876\n",
      "Subject 0, Epoch 389, Loss: 0.7456344068050385, Final Batch Loss: 0.23954039812088013\n",
      "Subject 0, Epoch 390, Loss: 0.8885731399059296, Final Batch Loss: 0.279573917388916\n",
      "Subject 0, Epoch 391, Loss: 0.8286337703466415, Final Batch Loss: 0.28563666343688965\n",
      "Subject 0, Epoch 392, Loss: 0.8304203003644943, Final Batch Loss: 0.19430403411388397\n",
      "Subject 0, Epoch 393, Loss: 0.8202089667320251, Final Batch Loss: 0.22708499431610107\n",
      "Subject 0, Epoch 394, Loss: 0.8374761044979095, Final Batch Loss: 0.30044278502464294\n",
      "Subject 0, Epoch 395, Loss: 0.6888039261102676, Final Batch Loss: 0.21341633796691895\n",
      "Subject 0, Epoch 396, Loss: 0.7875740081071854, Final Batch Loss: 0.23494696617126465\n",
      "Subject 0, Epoch 397, Loss: 0.7522009164094925, Final Batch Loss: 0.2440408319234848\n",
      "Subject 0, Epoch 398, Loss: 0.811372846364975, Final Batch Loss: 0.2872285544872284\n",
      "Subject 0, Epoch 399, Loss: 0.7835459560155869, Final Batch Loss: 0.26119378209114075\n",
      "Subject 0, Epoch 400, Loss: 0.7375426739454269, Final Batch Loss: 0.20319606363773346\n",
      "Subject 0, Epoch 401, Loss: 0.7925363481044769, Final Batch Loss: 0.18392011523246765\n",
      "Subject 0, Epoch 402, Loss: 0.7813289612531662, Final Batch Loss: 0.2639799416065216\n",
      "Subject 0, Epoch 403, Loss: 0.8013365417718887, Final Batch Loss: 0.3129377067089081\n",
      "Subject 0, Epoch 404, Loss: 0.7408156841993332, Final Batch Loss: 0.24435992538928986\n",
      "Subject 0, Epoch 405, Loss: 0.7167621701955795, Final Batch Loss: 0.20388473570346832\n",
      "Subject 0, Epoch 406, Loss: 0.7918835431337357, Final Batch Loss: 0.3064366579055786\n",
      "Subject 0, Epoch 407, Loss: 0.8200448006391525, Final Batch Loss: 0.18591366708278656\n",
      "Subject 0, Epoch 408, Loss: 0.7024053037166595, Final Batch Loss: 0.2637336552143097\n",
      "Subject 0, Epoch 409, Loss: 0.8681958615779877, Final Batch Loss: 0.25008058547973633\n",
      "Subject 0, Epoch 410, Loss: 0.8298372626304626, Final Batch Loss: 0.2634527087211609\n",
      "Subject 0, Epoch 411, Loss: 0.8168036192655563, Final Batch Loss: 0.3153756856918335\n",
      "Subject 0, Epoch 412, Loss: 0.7384027540683746, Final Batch Loss: 0.19757214188575745\n",
      "Subject 0, Epoch 413, Loss: 0.7823223173618317, Final Batch Loss: 0.3042270541191101\n",
      "Subject 0, Epoch 414, Loss: 0.7615081816911697, Final Batch Loss: 0.2738153040409088\n",
      "Subject 0, Epoch 415, Loss: 0.7811093479394913, Final Batch Loss: 0.19648782908916473\n",
      "Subject 0, Epoch 416, Loss: 0.8095805943012238, Final Batch Loss: 0.2845706641674042\n",
      "Subject 0, Epoch 417, Loss: 0.7363406121730804, Final Batch Loss: 0.2456064671278\n",
      "Subject 0, Epoch 418, Loss: 0.7415618896484375, Final Batch Loss: 0.18936438858509064\n",
      "Subject 0, Epoch 419, Loss: 0.6785376220941544, Final Batch Loss: 0.2723616063594818\n",
      "Subject 0, Epoch 420, Loss: 0.7843902260065079, Final Batch Loss: 0.2929920256137848\n",
      "Subject 0, Epoch 421, Loss: 0.7252637594938278, Final Batch Loss: 0.22824791073799133\n",
      "Subject 0, Epoch 422, Loss: 0.7590152025222778, Final Batch Loss: 0.2140059769153595\n",
      "Subject 0, Epoch 423, Loss: 0.7821731269359589, Final Batch Loss: 0.28154727816581726\n",
      "Subject 0, Epoch 424, Loss: 0.7735669910907745, Final Batch Loss: 0.21585509181022644\n",
      "Subject 0, Epoch 425, Loss: 0.6840646862983704, Final Batch Loss: 0.20156905055046082\n",
      "Subject 0, Epoch 426, Loss: 0.7475180327892303, Final Batch Loss: 0.33760392665863037\n",
      "Subject 0, Epoch 427, Loss: 0.7228896617889404, Final Batch Loss: 0.15387746691703796\n",
      "Subject 0, Epoch 428, Loss: 0.7248038351535797, Final Batch Loss: 0.2655056416988373\n",
      "Subject 0, Epoch 429, Loss: 0.7264979183673859, Final Batch Loss: 0.20177368819713593\n",
      "Subject 0, Epoch 430, Loss: 0.7801135033369064, Final Batch Loss: 0.26666751503944397\n",
      "Subject 0, Epoch 431, Loss: 0.7862148582935333, Final Batch Loss: 0.32531803846359253\n",
      "Subject 0, Epoch 432, Loss: 0.7307100296020508, Final Batch Loss: 0.22603033483028412\n",
      "Subject 0, Epoch 433, Loss: 0.6705610752105713, Final Batch Loss: 0.20627586543560028\n",
      "Subject 0, Epoch 434, Loss: 0.7419577240943909, Final Batch Loss: 0.2160891890525818\n",
      "Subject 0, Epoch 435, Loss: 0.695100337266922, Final Batch Loss: 0.26803362369537354\n",
      "Subject 0, Epoch 436, Loss: 0.6862847357988358, Final Batch Loss: 0.23797954618930817\n",
      "Subject 0, Epoch 437, Loss: 0.8336286395788193, Final Batch Loss: 0.32962584495544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 438, Loss: 0.8226539343595505, Final Batch Loss: 0.24600768089294434\n",
      "Subject 0, Epoch 439, Loss: 0.7867465317249298, Final Batch Loss: 0.20179292559623718\n",
      "Subject 0, Epoch 440, Loss: 0.7206668257713318, Final Batch Loss: 0.2306816577911377\n",
      "Subject 0, Epoch 441, Loss: 0.802410751581192, Final Batch Loss: 0.24672242999076843\n",
      "Subject 0, Epoch 442, Loss: 0.7260285764932632, Final Batch Loss: 0.23454906046390533\n",
      "Subject 0, Epoch 443, Loss: 0.7178303003311157, Final Batch Loss: 0.20567665994167328\n",
      "Subject 0, Epoch 444, Loss: 0.661226436495781, Final Batch Loss: 0.20663510262966156\n",
      "Subject 0, Epoch 445, Loss: 0.7559047490358353, Final Batch Loss: 0.2727009654045105\n",
      "Subject 0, Epoch 446, Loss: 0.7511196285486221, Final Batch Loss: 0.20427893102169037\n",
      "Subject 0, Epoch 447, Loss: 0.7779697924852371, Final Batch Loss: 0.2024991661310196\n",
      "Subject 0, Epoch 448, Loss: 0.7604808658361435, Final Batch Loss: 0.2773992717266083\n",
      "Subject 0, Epoch 449, Loss: 0.6733974516391754, Final Batch Loss: 0.22979775071144104\n",
      "Subject 0, Epoch 450, Loss: 0.729508563876152, Final Batch Loss: 0.26752984523773193\n",
      "Subject 0, Epoch 451, Loss: 0.7206325829029083, Final Batch Loss: 0.239985391497612\n",
      "Subject 0, Epoch 452, Loss: 0.6283857673406601, Final Batch Loss: 0.18837304413318634\n",
      "Subject 0, Epoch 453, Loss: 0.6510581970214844, Final Batch Loss: 0.2187335193157196\n",
      "Subject 0, Epoch 454, Loss: 0.6941567063331604, Final Batch Loss: 0.2026442438364029\n",
      "Subject 0, Epoch 455, Loss: 0.7346096485853195, Final Batch Loss: 0.2712794840335846\n",
      "Subject 0, Epoch 456, Loss: 0.7247897535562515, Final Batch Loss: 0.2626735270023346\n",
      "Subject 0, Epoch 457, Loss: 0.7225679159164429, Final Batch Loss: 0.2023782581090927\n",
      "Subject 0, Epoch 458, Loss: 0.7344138920307159, Final Batch Loss: 0.19164378941059113\n",
      "Subject 0, Epoch 459, Loss: 0.8086099028587341, Final Batch Loss: 0.32475417852401733\n",
      "Subject 0, Epoch 460, Loss: 0.6650284379720688, Final Batch Loss: 0.18312330543994904\n",
      "Subject 0, Epoch 461, Loss: 0.7393151223659515, Final Batch Loss: 0.2799449861049652\n",
      "Subject 0, Epoch 462, Loss: 0.7937877327203751, Final Batch Loss: 0.20542503893375397\n",
      "Subject 0, Epoch 463, Loss: 0.7085585743188858, Final Batch Loss: 0.22965100407600403\n",
      "Subject 0, Epoch 464, Loss: 0.6748378574848175, Final Batch Loss: 0.2463521808385849\n",
      "Subject 0, Epoch 465, Loss: 0.6558089703321457, Final Batch Loss: 0.301340788602829\n",
      "Subject 0, Epoch 466, Loss: 0.7335403710603714, Final Batch Loss: 0.23332390189170837\n",
      "Subject 0, Epoch 467, Loss: 0.6339846849441528, Final Batch Loss: 0.17562755942344666\n",
      "Subject 0, Epoch 468, Loss: 0.6086816042661667, Final Batch Loss: 0.25344333052635193\n",
      "Subject 0, Epoch 469, Loss: 0.7079066634178162, Final Batch Loss: 0.21813181042671204\n",
      "Subject 0, Epoch 470, Loss: 0.6711672395467758, Final Batch Loss: 0.22127343714237213\n",
      "Subject 0, Epoch 471, Loss: 0.8714657127857208, Final Batch Loss: 0.2485712766647339\n",
      "Subject 0, Epoch 472, Loss: 0.7249498814344406, Final Batch Loss: 0.24281412363052368\n",
      "Subject 0, Epoch 473, Loss: 0.7226973176002502, Final Batch Loss: 0.18545317649841309\n",
      "Subject 0, Epoch 474, Loss: 0.8313594162464142, Final Batch Loss: 0.2569555640220642\n",
      "Subject 0, Epoch 475, Loss: 0.8120352774858475, Final Batch Loss: 0.2571711540222168\n",
      "Subject 0, Epoch 476, Loss: 0.6933549046516418, Final Batch Loss: 0.26786553859710693\n",
      "Subject 0, Epoch 477, Loss: 0.6928354948759079, Final Batch Loss: 0.30680546164512634\n",
      "Subject 0, Epoch 478, Loss: 0.6795105487108231, Final Batch Loss: 0.22308379411697388\n",
      "Subject 0, Epoch 479, Loss: 0.6204085797071457, Final Batch Loss: 0.2571207284927368\n",
      "Subject 0, Epoch 480, Loss: 0.6087418645620346, Final Batch Loss: 0.2287481129169464\n",
      "Subject 0, Epoch 481, Loss: 0.7412575632333755, Final Batch Loss: 0.24783790111541748\n",
      "Subject 0, Epoch 482, Loss: 0.6683612018823624, Final Batch Loss: 0.175947904586792\n",
      "Subject 0, Epoch 483, Loss: 0.6419782489538193, Final Batch Loss: 0.17892207205295563\n",
      "Subject 0, Epoch 484, Loss: 0.6937101036310196, Final Batch Loss: 0.23489680886268616\n",
      "Subject 0, Epoch 485, Loss: 0.7172306329011917, Final Batch Loss: 0.25644463300704956\n",
      "Subject 0, Epoch 486, Loss: 0.6769764423370361, Final Batch Loss: 0.22450274229049683\n",
      "Subject 0, Epoch 487, Loss: 0.6375690698623657, Final Batch Loss: 0.1271432340145111\n",
      "Subject 0, Epoch 488, Loss: 0.6253097653388977, Final Batch Loss: 0.17681485414505005\n",
      "Subject 0, Epoch 489, Loss: 0.646591991186142, Final Batch Loss: 0.2867061197757721\n",
      "Subject 0, Epoch 490, Loss: 0.6367315948009491, Final Batch Loss: 0.15248675644397736\n",
      "Subject 0, Epoch 491, Loss: 0.7340302765369415, Final Batch Loss: 0.2435656040906906\n",
      "Subject 0, Epoch 492, Loss: 0.6799383610486984, Final Batch Loss: 0.23748113214969635\n",
      "Subject 0, Epoch 493, Loss: 0.7543882578611374, Final Batch Loss: 0.28928035497665405\n",
      "Subject 0, Epoch 494, Loss: 0.6981055289506912, Final Batch Loss: 0.25325319170951843\n",
      "Subject 0, Epoch 495, Loss: 0.648546040058136, Final Batch Loss: 0.27821052074432373\n",
      "Subject 0, Epoch 496, Loss: 0.6451118141412735, Final Batch Loss: 0.29256144165992737\n",
      "Subject 0, Epoch 497, Loss: 0.6545802354812622, Final Batch Loss: 0.24959233403205872\n",
      "Subject 0, Epoch 498, Loss: 0.6785640120506287, Final Batch Loss: 0.18293872475624084\n",
      "Subject 0, Epoch 499, Loss: 0.6734891980886459, Final Batch Loss: 0.19212891161441803\n",
      "Subject 0, Epoch 500, Loss: 0.700599730014801, Final Batch Loss: 0.21314206719398499\n",
      "Subject 0, Epoch 501, Loss: 0.6862403303384781, Final Batch Loss: 0.1794309765100479\n",
      "Subject 0, Epoch 502, Loss: 0.640652671456337, Final Batch Loss: 0.19217900931835175\n",
      "Subject 0, Epoch 503, Loss: 0.6946390867233276, Final Batch Loss: 0.19721576571464539\n",
      "Subject 0, Epoch 504, Loss: 0.6655982285737991, Final Batch Loss: 0.21037234365940094\n",
      "Subject 0, Epoch 505, Loss: 0.6957619190216064, Final Batch Loss: 0.26986947655677795\n",
      "Subject 0, Epoch 506, Loss: 0.6676891893148422, Final Batch Loss: 0.21436139941215515\n",
      "Subject 0, Epoch 507, Loss: 0.6191675961017609, Final Batch Loss: 0.17554473876953125\n",
      "Subject 0, Epoch 508, Loss: 0.6811311542987823, Final Batch Loss: 0.2237122505903244\n",
      "Subject 0, Epoch 509, Loss: 0.7166335582733154, Final Batch Loss: 0.20453087985515594\n",
      "Subject 0, Epoch 510, Loss: 0.7200157940387726, Final Batch Loss: 0.21153798699378967\n",
      "Subject 0, Epoch 511, Loss: 0.6910224705934525, Final Batch Loss: 0.27565717697143555\n",
      "Subject 0, Epoch 512, Loss: 0.6531261056661606, Final Batch Loss: 0.21579550206661224\n",
      "Subject 0, Epoch 513, Loss: 0.6229665130376816, Final Batch Loss: 0.14125670492649078\n",
      "Subject 0, Epoch 514, Loss: 0.6270506083965302, Final Batch Loss: 0.223698228597641\n",
      "Subject 0, Epoch 515, Loss: 0.6455334201455116, Final Batch Loss: 0.2909640073776245\n",
      "Subject 0, Epoch 516, Loss: 0.6442264020442963, Final Batch Loss: 0.2245084047317505\n",
      "Subject 0, Epoch 517, Loss: 0.7007048279047012, Final Batch Loss: 0.19971121847629547\n",
      "Subject 0, Epoch 518, Loss: 0.6940345466136932, Final Batch Loss: 0.146800696849823\n",
      "Subject 0, Epoch 519, Loss: 0.7363628298044205, Final Batch Loss: 0.20388709008693695\n",
      "Subject 0, Epoch 520, Loss: 0.5977906882762909, Final Batch Loss: 0.1540841907262802\n",
      "Subject 0, Epoch 521, Loss: 0.6689053475856781, Final Batch Loss: 0.18562713265419006\n",
      "Subject 0, Epoch 522, Loss: 0.6577254384756088, Final Batch Loss: 0.26043131947517395\n",
      "Subject 0, Epoch 523, Loss: 0.6295828819274902, Final Batch Loss: 0.1821794956922531\n",
      "Subject 0, Epoch 524, Loss: 0.6738730669021606, Final Batch Loss: 0.24435307085514069\n",
      "Subject 0, Epoch 525, Loss: 0.6538761705160141, Final Batch Loss: 0.14543862640857697\n",
      "Subject 0, Epoch 526, Loss: 0.6523773521184921, Final Batch Loss: 0.21723803877830505\n",
      "Subject 0, Epoch 527, Loss: 0.7458631247282028, Final Batch Loss: 0.23325279355049133\n",
      "Subject 0, Epoch 528, Loss: 0.5988727509975433, Final Batch Loss: 0.21243628859519958\n",
      "Subject 0, Epoch 529, Loss: 0.6491266340017319, Final Batch Loss: 0.20722585916519165\n",
      "Subject 0, Epoch 530, Loss: 0.6429511606693268, Final Batch Loss: 0.15657539665699005\n",
      "Subject 0, Epoch 531, Loss: 0.6546887755393982, Final Batch Loss: 0.21723000705242157\n",
      "Subject 0, Epoch 532, Loss: 0.5541324466466904, Final Batch Loss: 0.21944136917591095\n",
      "Subject 0, Epoch 533, Loss: 0.6510558277368546, Final Batch Loss: 0.23890309035778046\n",
      "Subject 0, Epoch 534, Loss: 0.6829908937215805, Final Batch Loss: 0.2703213691711426\n",
      "Subject 0, Epoch 535, Loss: 0.664095401763916, Final Batch Loss: 0.2100132405757904\n",
      "Subject 0, Epoch 536, Loss: 0.6146283447742462, Final Batch Loss: 0.30152571201324463\n",
      "Subject 0, Epoch 537, Loss: 0.5839707404375076, Final Batch Loss: 0.20456427335739136\n",
      "Subject 0, Epoch 538, Loss: 0.705310508608818, Final Batch Loss: 0.2862951457500458\n",
      "Subject 0, Epoch 539, Loss: 0.6160581707954407, Final Batch Loss: 0.2279811054468155\n",
      "Subject 0, Epoch 540, Loss: 0.7014161646366119, Final Batch Loss: 0.33110710978507996\n",
      "Subject 0, Epoch 541, Loss: 0.6213703155517578, Final Batch Loss: 0.2381274402141571\n",
      "Subject 0, Epoch 542, Loss: 0.6752953231334686, Final Batch Loss: 0.17823246121406555\n",
      "Subject 0, Epoch 543, Loss: 0.6487555354833603, Final Batch Loss: 0.2615554928779602\n",
      "Subject 0, Epoch 544, Loss: 0.5639446377754211, Final Batch Loss: 0.18889375030994415\n",
      "Subject 0, Epoch 545, Loss: 0.6185041815042496, Final Batch Loss: 0.2004639059305191\n",
      "Subject 0, Epoch 546, Loss: 0.7770769894123077, Final Batch Loss: 0.26216375827789307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 547, Loss: 0.7144282758235931, Final Batch Loss: 0.23020996153354645\n",
      "Subject 0, Epoch 548, Loss: 0.5921042114496231, Final Batch Loss: 0.20882293581962585\n",
      "Subject 0, Epoch 549, Loss: 0.689664214849472, Final Batch Loss: 0.19919326901435852\n",
      "Subject 0, Epoch 550, Loss: 0.6285619288682938, Final Batch Loss: 0.22446110844612122\n",
      "Subject 0, Epoch 551, Loss: 0.5796474814414978, Final Batch Loss: 0.24384424090385437\n",
      "Subject 0, Epoch 552, Loss: 0.6636060327291489, Final Batch Loss: 0.21745091676712036\n",
      "Subject 0, Epoch 553, Loss: 0.6029377728700638, Final Batch Loss: 0.19769451022148132\n",
      "Subject 0, Epoch 554, Loss: 0.5737289190292358, Final Batch Loss: 0.18010038137435913\n",
      "Subject 0, Epoch 555, Loss: 0.5768224149942398, Final Batch Loss: 0.21544308960437775\n",
      "Subject 0, Epoch 556, Loss: 0.6114216297864914, Final Batch Loss: 0.28894585371017456\n",
      "Subject 0, Epoch 557, Loss: 0.6058240532875061, Final Batch Loss: 0.18903116881847382\n",
      "Subject 0, Epoch 558, Loss: 0.5330815017223358, Final Batch Loss: 0.1710415929555893\n",
      "Subject 0, Epoch 559, Loss: 0.6635650992393494, Final Batch Loss: 0.2178262621164322\n",
      "Subject 0, Epoch 560, Loss: 0.6977358460426331, Final Batch Loss: 0.28847721219062805\n",
      "Subject 0, Epoch 561, Loss: 0.5913704931735992, Final Batch Loss: 0.17595915496349335\n",
      "Subject 0, Epoch 562, Loss: 0.6371383517980576, Final Batch Loss: 0.17982520163059235\n",
      "Subject 0, Epoch 563, Loss: 0.6028378754854202, Final Batch Loss: 0.18918189406394958\n",
      "Subject 0, Epoch 564, Loss: 0.5832407623529434, Final Batch Loss: 0.22041955590248108\n",
      "Subject 0, Epoch 565, Loss: 0.5652261674404144, Final Batch Loss: 0.20858041942119598\n",
      "Subject 0, Epoch 566, Loss: 0.7045608013868332, Final Batch Loss: 0.25388476252555847\n",
      "Subject 0, Epoch 567, Loss: 0.6091694831848145, Final Batch Loss: 0.2206501066684723\n",
      "Subject 0, Epoch 568, Loss: 0.7256767153739929, Final Batch Loss: 0.24967573583126068\n",
      "Subject 0, Epoch 569, Loss: 0.6112791001796722, Final Batch Loss: 0.2325802892446518\n",
      "Subject 0, Epoch 570, Loss: 0.5781195610761642, Final Batch Loss: 0.21960431337356567\n",
      "Subject 0, Epoch 571, Loss: 0.5886178314685822, Final Batch Loss: 0.15481413900852203\n",
      "Subject 0, Epoch 572, Loss: 0.5636135637760162, Final Batch Loss: 0.18125586211681366\n",
      "Subject 0, Epoch 573, Loss: 0.599117636680603, Final Batch Loss: 0.28485679626464844\n",
      "Subject 0, Epoch 574, Loss: 0.6020742207765579, Final Batch Loss: 0.18293200433254242\n",
      "Subject 0, Epoch 575, Loss: 0.6097864657640457, Final Batch Loss: 0.1792278289794922\n",
      "Subject 0, Epoch 576, Loss: 0.6166562736034393, Final Batch Loss: 0.2109743058681488\n",
      "Subject 0, Epoch 577, Loss: 0.6416926234960556, Final Batch Loss: 0.22414791584014893\n",
      "Subject 0, Epoch 578, Loss: 0.557604469358921, Final Batch Loss: 0.21443277597427368\n",
      "Subject 0, Epoch 579, Loss: 0.6214627027511597, Final Batch Loss: 0.24956630170345306\n",
      "Subject 0, Epoch 580, Loss: 0.674928605556488, Final Batch Loss: 0.24108090996742249\n",
      "Subject 0, Epoch 581, Loss: 0.5527278482913971, Final Batch Loss: 0.2088364064693451\n",
      "Subject 0, Epoch 582, Loss: 0.6114562749862671, Final Batch Loss: 0.23201420903205872\n",
      "Subject 0, Epoch 583, Loss: 0.5063827335834503, Final Batch Loss: 0.167267844080925\n",
      "Subject 0, Epoch 584, Loss: 0.6196399480104446, Final Batch Loss: 0.20801599323749542\n",
      "Subject 0, Epoch 585, Loss: 0.5642673969268799, Final Batch Loss: 0.16802795231342316\n",
      "Subject 0, Epoch 586, Loss: 0.5272487550973892, Final Batch Loss: 0.1669158935546875\n",
      "Subject 0, Epoch 587, Loss: 0.6430256068706512, Final Batch Loss: 0.24606341123580933\n",
      "Subject 0, Epoch 588, Loss: 0.6781104058027267, Final Batch Loss: 0.22031284868717194\n",
      "Subject 0, Epoch 589, Loss: 0.5838239639997482, Final Batch Loss: 0.16687697172164917\n",
      "Subject 0, Epoch 590, Loss: 0.5591748803853989, Final Batch Loss: 0.1783757358789444\n",
      "Subject 0, Epoch 591, Loss: 0.6153782606124878, Final Batch Loss: 0.20035047829151154\n",
      "Subject 0, Epoch 592, Loss: 0.5580736398696899, Final Batch Loss: 0.15571795403957367\n",
      "Subject 0, Epoch 593, Loss: 0.6643451750278473, Final Batch Loss: 0.262363463640213\n",
      "Subject 0, Epoch 594, Loss: 0.5384670048952103, Final Batch Loss: 0.24630166590213776\n",
      "Subject 0, Epoch 595, Loss: 0.5797345787286758, Final Batch Loss: 0.14986948668956757\n",
      "Subject 0, Epoch 596, Loss: 0.4709813743829727, Final Batch Loss: 0.13078716397285461\n",
      "Subject 0, Epoch 597, Loss: 0.5272139012813568, Final Batch Loss: 0.18626795709133148\n",
      "Subject 0, Epoch 598, Loss: 0.6087252199649811, Final Batch Loss: 0.20803333818912506\n",
      "Subject 0, Epoch 599, Loss: 0.5157008171081543, Final Batch Loss: 0.1467026025056839\n",
      "Subject 0, Epoch 600, Loss: 0.5230605006217957, Final Batch Loss: 0.18876990675926208\n",
      "Subject 0, Epoch 601, Loss: 0.6081486195325851, Final Batch Loss: 0.21648865938186646\n",
      "Subject 0, Epoch 602, Loss: 0.6257851421833038, Final Batch Loss: 0.19726645946502686\n",
      "Subject 0, Epoch 603, Loss: 0.5615346729755402, Final Batch Loss: 0.16745585203170776\n",
      "Subject 0, Epoch 604, Loss: 0.5375502407550812, Final Batch Loss: 0.18095657229423523\n",
      "Subject 0, Epoch 605, Loss: 0.5764000862836838, Final Batch Loss: 0.14943544566631317\n",
      "Subject 0, Epoch 606, Loss: 0.6608523726463318, Final Batch Loss: 0.22697164118289948\n",
      "Subject 0, Epoch 607, Loss: 0.6213286221027374, Final Batch Loss: 0.2802465260028839\n",
      "Subject 0, Epoch 608, Loss: 0.6265529245138168, Final Batch Loss: 0.18752124905586243\n",
      "Subject 0, Epoch 609, Loss: 0.5395515859127045, Final Batch Loss: 0.18192432820796967\n",
      "Subject 0, Epoch 610, Loss: 0.5368482619524002, Final Batch Loss: 0.2095644176006317\n",
      "Subject 0, Epoch 611, Loss: 0.6446572542190552, Final Batch Loss: 0.16220273077487946\n",
      "Subject 0, Epoch 612, Loss: 0.584006279706955, Final Batch Loss: 0.16262781620025635\n",
      "Subject 0, Epoch 613, Loss: 0.5312770009040833, Final Batch Loss: 0.2082587480545044\n",
      "Subject 0, Epoch 614, Loss: 0.5467528104782104, Final Batch Loss: 0.21048010885715485\n",
      "Subject 0, Epoch 615, Loss: 0.6060594618320465, Final Batch Loss: 0.1656644344329834\n",
      "Subject 0, Epoch 616, Loss: 0.6319877952337265, Final Batch Loss: 0.2212221473455429\n",
      "Subject 0, Epoch 617, Loss: 0.5584683865308762, Final Batch Loss: 0.15108264982700348\n",
      "Subject 0, Epoch 618, Loss: 0.6076052635908127, Final Batch Loss: 0.238901749253273\n",
      "Subject 0, Epoch 619, Loss: 0.5173245668411255, Final Batch Loss: 0.21201249957084656\n",
      "Subject 0, Epoch 620, Loss: 0.7213029712438583, Final Batch Loss: 0.2226801961660385\n",
      "Subject 0, Epoch 621, Loss: 0.5343172699213028, Final Batch Loss: 0.18746566772460938\n",
      "Subject 0, Epoch 622, Loss: 0.531468003988266, Final Batch Loss: 0.21286940574645996\n",
      "Subject 0, Epoch 623, Loss: 0.5912818759679794, Final Batch Loss: 0.1410818099975586\n",
      "Subject 0, Epoch 624, Loss: 0.5452240705490112, Final Batch Loss: 0.1422491818666458\n",
      "Subject 0, Epoch 625, Loss: 0.6581577062606812, Final Batch Loss: 0.16059638559818268\n",
      "Subject 0, Epoch 626, Loss: 0.4703330993652344, Final Batch Loss: 0.14311572909355164\n",
      "Subject 0, Epoch 627, Loss: 0.5373717099428177, Final Batch Loss: 0.2208915501832962\n",
      "Subject 0, Epoch 628, Loss: 0.6000534147024155, Final Batch Loss: 0.1715845912694931\n",
      "Subject 0, Epoch 629, Loss: 0.5637051314115524, Final Batch Loss: 0.13170358538627625\n",
      "Subject 0, Epoch 630, Loss: 0.5295475274324417, Final Batch Loss: 0.1576896607875824\n",
      "Subject 0, Epoch 631, Loss: 0.6133691668510437, Final Batch Loss: 0.13426098227500916\n",
      "Subject 0, Epoch 632, Loss: 0.5704337507486343, Final Batch Loss: 0.2028028815984726\n",
      "Subject 0, Epoch 633, Loss: 0.5711237788200378, Final Batch Loss: 0.24062567949295044\n",
      "Subject 0, Epoch 634, Loss: 0.5510934144258499, Final Batch Loss: 0.191556915640831\n",
      "Subject 0, Epoch 635, Loss: 0.553058847784996, Final Batch Loss: 0.19932685792446136\n",
      "Subject 0, Epoch 636, Loss: 0.5183082669973373, Final Batch Loss: 0.17632198333740234\n",
      "Subject 0, Epoch 637, Loss: 0.5147451311349869, Final Batch Loss: 0.22819378972053528\n",
      "Subject 0, Epoch 638, Loss: 0.5415906757116318, Final Batch Loss: 0.14478084444999695\n",
      "Subject 0, Epoch 639, Loss: 0.5899031683802605, Final Batch Loss: 0.1246953085064888\n",
      "Subject 0, Epoch 640, Loss: 0.5025824308395386, Final Batch Loss: 0.17975535988807678\n",
      "Subject 0, Epoch 641, Loss: 0.4317072778940201, Final Batch Loss: 0.12297628074884415\n",
      "Subject 0, Epoch 642, Loss: 0.4720313400030136, Final Batch Loss: 0.12281434237957001\n",
      "Subject 0, Epoch 643, Loss: 0.5963358581066132, Final Batch Loss: 0.26541203260421753\n",
      "Subject 0, Epoch 644, Loss: 0.620297908782959, Final Batch Loss: 0.16035909950733185\n",
      "Subject 0, Epoch 645, Loss: 0.5355553179979324, Final Batch Loss: 0.13926929235458374\n",
      "Subject 0, Epoch 646, Loss: 0.5987434536218643, Final Batch Loss: 0.19431787729263306\n",
      "Subject 0, Epoch 647, Loss: 0.49311985075473785, Final Batch Loss: 0.14936178922653198\n",
      "Subject 0, Epoch 648, Loss: 0.47998349368572235, Final Batch Loss: 0.14071641862392426\n",
      "Subject 0, Epoch 649, Loss: 0.5891039669513702, Final Batch Loss: 0.20960338413715363\n",
      "Subject 0, Epoch 650, Loss: 0.5616432875394821, Final Batch Loss: 0.14861725270748138\n",
      "Subject 0, Epoch 651, Loss: 0.5871905982494354, Final Batch Loss: 0.21172045171260834\n",
      "Subject 0, Epoch 652, Loss: 0.5387997925281525, Final Batch Loss: 0.17568570375442505\n",
      "Subject 0, Epoch 653, Loss: 0.5510612577199936, Final Batch Loss: 0.19521063566207886\n",
      "Subject 0, Epoch 654, Loss: 0.5718156695365906, Final Batch Loss: 0.21399331092834473\n",
      "Subject 0, Epoch 655, Loss: 0.5283480584621429, Final Batch Loss: 0.1519855409860611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 656, Loss: 0.5507299900054932, Final Batch Loss: 0.16457629203796387\n",
      "Subject 0, Epoch 657, Loss: 0.5656938329339027, Final Batch Loss: 0.11150313168764114\n",
      "Subject 0, Epoch 658, Loss: 0.49767932295799255, Final Batch Loss: 0.09748728573322296\n",
      "Subject 0, Epoch 659, Loss: 0.5416554063558578, Final Batch Loss: 0.13856075704097748\n",
      "Subject 0, Epoch 660, Loss: 0.5397523045539856, Final Batch Loss: 0.16956517100334167\n",
      "Subject 0, Epoch 661, Loss: 0.5285134762525558, Final Batch Loss: 0.22074666619300842\n",
      "Subject 0, Epoch 662, Loss: 0.5334666222333908, Final Batch Loss: 0.24178723990917206\n",
      "Subject 0, Epoch 663, Loss: 0.5502060279250145, Final Batch Loss: 0.23856066167354584\n",
      "Subject 0, Epoch 664, Loss: 0.6053244471549988, Final Batch Loss: 0.2767027020454407\n",
      "Subject 0, Epoch 665, Loss: 0.4957849383354187, Final Batch Loss: 0.15920643508434296\n",
      "Subject 0, Epoch 666, Loss: 0.5324009656906128, Final Batch Loss: 0.16575652360916138\n",
      "Subject 0, Epoch 667, Loss: 0.5680431425571442, Final Batch Loss: 0.18303938210010529\n",
      "Subject 0, Epoch 668, Loss: 0.509551152586937, Final Batch Loss: 0.19450421631336212\n",
      "Subject 0, Epoch 669, Loss: 0.47335003316402435, Final Batch Loss: 0.13868099451065063\n",
      "Subject 0, Epoch 670, Loss: 0.4729780852794647, Final Batch Loss: 0.14656367897987366\n",
      "Subject 0, Epoch 671, Loss: 0.6117420047521591, Final Batch Loss: 0.17695997655391693\n",
      "Subject 0, Epoch 672, Loss: 0.4629230201244354, Final Batch Loss: 0.17202843725681305\n",
      "Subject 0, Epoch 673, Loss: 0.535152480006218, Final Batch Loss: 0.2185726761817932\n",
      "Subject 0, Epoch 674, Loss: 0.6021590679883957, Final Batch Loss: 0.1765192300081253\n",
      "Subject 0, Epoch 675, Loss: 0.5483663976192474, Final Batch Loss: 0.19038493931293488\n",
      "Subject 0, Epoch 676, Loss: 0.48086321353912354, Final Batch Loss: 0.12822113931179047\n",
      "Subject 0, Epoch 677, Loss: 0.503179706633091, Final Batch Loss: 0.2119569182395935\n",
      "Subject 0, Epoch 678, Loss: 0.46938861161470413, Final Batch Loss: 0.11436019092798233\n",
      "Subject 0, Epoch 679, Loss: 0.5008304119110107, Final Batch Loss: 0.15805372595787048\n",
      "Subject 0, Epoch 680, Loss: 0.4247125834226608, Final Batch Loss: 0.1122223287820816\n",
      "Subject 0, Epoch 681, Loss: 0.39309247583150864, Final Batch Loss: 0.11807914823293686\n",
      "Subject 0, Epoch 682, Loss: 0.42533431202173233, Final Batch Loss: 0.15869523584842682\n",
      "Subject 0, Epoch 683, Loss: 0.45839352905750275, Final Batch Loss: 0.1332281231880188\n",
      "Subject 0, Epoch 684, Loss: 0.4435231387615204, Final Batch Loss: 0.1333920806646347\n",
      "Subject 0, Epoch 685, Loss: 0.5264429897069931, Final Batch Loss: 0.18678529560565948\n",
      "Subject 0, Epoch 686, Loss: 0.4751460328698158, Final Batch Loss: 0.18077072501182556\n",
      "Subject 0, Epoch 687, Loss: 0.5378238409757614, Final Batch Loss: 0.23291240632534027\n",
      "Subject 0, Epoch 688, Loss: 0.44366197288036346, Final Batch Loss: 0.1947081983089447\n",
      "Subject 0, Epoch 689, Loss: 0.43289750814437866, Final Batch Loss: 0.12093490362167358\n",
      "Subject 0, Epoch 690, Loss: 0.4420149028301239, Final Batch Loss: 0.1561977118253708\n",
      "Subject 0, Epoch 691, Loss: 0.4566296562552452, Final Batch Loss: 0.16912999749183655\n",
      "Subject 0, Epoch 692, Loss: 0.4634987413883209, Final Batch Loss: 0.17065037786960602\n",
      "Subject 0, Epoch 693, Loss: 0.41459984332323074, Final Batch Loss: 0.12222539633512497\n",
      "Subject 0, Epoch 694, Loss: 0.532248854637146, Final Batch Loss: 0.1912277638912201\n",
      "Subject 0, Epoch 695, Loss: 0.46831901371479034, Final Batch Loss: 0.14201293885707855\n",
      "Subject 0, Epoch 696, Loss: 0.49918822944164276, Final Batch Loss: 0.17217160761356354\n",
      "Subject 0, Epoch 697, Loss: 0.3654297739267349, Final Batch Loss: 0.12178447097539902\n",
      "Subject 0, Epoch 698, Loss: 0.5056803077459335, Final Batch Loss: 0.1790919452905655\n",
      "Subject 0, Epoch 699, Loss: 0.5136327743530273, Final Batch Loss: 0.17404599487781525\n",
      "Subject 0, Epoch 700, Loss: 0.5005304366350174, Final Batch Loss: 0.18037311732769012\n",
      "Subject 0, Epoch 701, Loss: 0.4599667638540268, Final Batch Loss: 0.13953718543052673\n",
      "Subject 0, Epoch 702, Loss: 0.4906979352235794, Final Batch Loss: 0.11497589945793152\n",
      "Subject 0, Epoch 703, Loss: 0.5283030718564987, Final Batch Loss: 0.15498314797878265\n",
      "Subject 0, Epoch 704, Loss: 0.5462572574615479, Final Batch Loss: 0.16044020652770996\n",
      "Subject 0, Epoch 705, Loss: 0.568281427025795, Final Batch Loss: 0.18726173043251038\n",
      "Subject 0, Epoch 706, Loss: 0.6188733875751495, Final Batch Loss: 0.20961740612983704\n",
      "Subject 0, Epoch 707, Loss: 0.48834328353405, Final Batch Loss: 0.14872515201568604\n",
      "Subject 0, Epoch 708, Loss: 0.46560342609882355, Final Batch Loss: 0.11108936369419098\n",
      "Subject 0, Epoch 709, Loss: 0.405752032995224, Final Batch Loss: 0.12563017010688782\n",
      "Subject 0, Epoch 710, Loss: 0.44436024129390717, Final Batch Loss: 0.1374197006225586\n",
      "Subject 0, Epoch 711, Loss: 0.5565636157989502, Final Batch Loss: 0.19139137864112854\n",
      "Subject 0, Epoch 712, Loss: 0.4468650445342064, Final Batch Loss: 0.11518380790948868\n",
      "Subject 0, Epoch 713, Loss: 0.5251960456371307, Final Batch Loss: 0.15068812668323517\n",
      "Subject 0, Epoch 714, Loss: 0.47540877759456635, Final Batch Loss: 0.11929486691951752\n",
      "Subject 0, Epoch 715, Loss: 0.43571580946445465, Final Batch Loss: 0.15823577344417572\n",
      "Subject 0, Epoch 716, Loss: 0.5264774784445763, Final Batch Loss: 0.2453444004058838\n",
      "Subject 0, Epoch 717, Loss: 0.49420952051877975, Final Batch Loss: 0.20012125372886658\n",
      "Subject 0, Epoch 718, Loss: 0.3758317306637764, Final Batch Loss: 0.13089333474636078\n",
      "Subject 0, Epoch 719, Loss: 0.4336206614971161, Final Batch Loss: 0.1436324566602707\n",
      "Subject 0, Epoch 720, Loss: 0.46223311126232147, Final Batch Loss: 0.18599745631217957\n",
      "Subject 0, Epoch 721, Loss: 0.48782089352607727, Final Batch Loss: 0.15758344531059265\n",
      "Subject 0, Epoch 722, Loss: 0.4508989751338959, Final Batch Loss: 0.1548803746700287\n",
      "Subject 0, Epoch 723, Loss: 0.44325587153434753, Final Batch Loss: 0.14463266730308533\n",
      "Subject 0, Epoch 724, Loss: 0.4924967437982559, Final Batch Loss: 0.1621163785457611\n",
      "Subject 0, Epoch 725, Loss: 0.49119213223457336, Final Batch Loss: 0.23226287961006165\n",
      "Subject 0, Epoch 726, Loss: 0.4525497108697891, Final Batch Loss: 0.19315889477729797\n",
      "Subject 0, Epoch 727, Loss: 0.37053079158067703, Final Batch Loss: 0.09998252242803574\n",
      "Subject 0, Epoch 728, Loss: 0.48188790678977966, Final Batch Loss: 0.12689703702926636\n",
      "Subject 0, Epoch 729, Loss: 0.4743463769555092, Final Batch Loss: 0.1979314386844635\n",
      "Subject 0, Epoch 730, Loss: 0.3922407627105713, Final Batch Loss: 0.15404866635799408\n",
      "Subject 0, Epoch 731, Loss: 0.45072735100984573, Final Batch Loss: 0.15264004468917847\n",
      "Subject 0, Epoch 732, Loss: 0.5494338572025299, Final Batch Loss: 0.1795198917388916\n",
      "Subject 0, Epoch 733, Loss: 0.3908049762248993, Final Batch Loss: 0.11066200584173203\n",
      "Subject 0, Epoch 734, Loss: 0.4920441806316376, Final Batch Loss: 0.15887807309627533\n",
      "Subject 0, Epoch 735, Loss: 0.4432576224207878, Final Batch Loss: 0.16355444490909576\n",
      "Subject 0, Epoch 736, Loss: 0.44447221606969833, Final Batch Loss: 0.08276534825563431\n",
      "Subject 0, Epoch 737, Loss: 0.4307195544242859, Final Batch Loss: 0.14308376610279083\n",
      "Subject 0, Epoch 738, Loss: 0.4639556184411049, Final Batch Loss: 0.1745743453502655\n",
      "Subject 0, Epoch 739, Loss: 0.43848521262407303, Final Batch Loss: 0.08423451334238052\n",
      "Subject 0, Epoch 740, Loss: 0.3602168560028076, Final Batch Loss: 0.1098327562212944\n",
      "Subject 0, Epoch 741, Loss: 0.4200597107410431, Final Batch Loss: 0.11410801112651825\n",
      "Subject 0, Epoch 742, Loss: 0.4863791763782501, Final Batch Loss: 0.2031613439321518\n",
      "Subject 0, Epoch 743, Loss: 0.341880664229393, Final Batch Loss: 0.11326660960912704\n",
      "Subject 0, Epoch 744, Loss: 0.3583081439137459, Final Batch Loss: 0.10490509867668152\n",
      "Subject 0, Epoch 745, Loss: 0.42197728902101517, Final Batch Loss: 0.19496223330497742\n",
      "Subject 0, Epoch 746, Loss: 0.43914391845464706, Final Batch Loss: 0.11901962757110596\n",
      "Subject 0, Epoch 747, Loss: 0.4507353603839874, Final Batch Loss: 0.1785287857055664\n",
      "Subject 0, Epoch 748, Loss: 0.42699216306209564, Final Batch Loss: 0.1284436136484146\n",
      "Subject 0, Epoch 749, Loss: 0.5290823727846146, Final Batch Loss: 0.13933239877223969\n",
      "Subject 0, Epoch 750, Loss: 0.4367116689682007, Final Batch Loss: 0.13907849788665771\n",
      "Subject 0, Epoch 751, Loss: 0.41647154092788696, Final Batch Loss: 0.18864350020885468\n",
      "Subject 0, Epoch 752, Loss: 0.41123218834400177, Final Batch Loss: 0.09973360598087311\n",
      "Subject 0, Epoch 753, Loss: 0.4428170844912529, Final Batch Loss: 0.14280292391777039\n",
      "Subject 0, Epoch 754, Loss: 0.3707135319709778, Final Batch Loss: 0.13978558778762817\n",
      "Subject 0, Epoch 755, Loss: 0.33499690890312195, Final Batch Loss: 0.14893664419651031\n",
      "Subject 0, Epoch 756, Loss: 0.44382598996162415, Final Batch Loss: 0.15741677582263947\n",
      "Subject 0, Epoch 757, Loss: 0.4175039008259773, Final Batch Loss: 0.1461516171693802\n",
      "Subject 0, Epoch 758, Loss: 0.4196498990058899, Final Batch Loss: 0.177243173122406\n",
      "Subject 0, Epoch 759, Loss: 0.4040154442191124, Final Batch Loss: 0.11823118478059769\n",
      "Subject 0, Epoch 760, Loss: 0.35017044097185135, Final Batch Loss: 0.08657320588827133\n",
      "Subject 0, Epoch 761, Loss: 0.49883951246738434, Final Batch Loss: 0.1281864494085312\n",
      "Subject 0, Epoch 762, Loss: 0.34491628408432007, Final Batch Loss: 0.14902877807617188\n",
      "Subject 0, Epoch 763, Loss: 0.44600899517536163, Final Batch Loss: 0.06695960462093353\n",
      "Subject 0, Epoch 764, Loss: 0.39586979150772095, Final Batch Loss: 0.11330816894769669\n",
      "Subject 0, Epoch 765, Loss: 0.3782063275575638, Final Batch Loss: 0.10320870578289032\n",
      "Subject 0, Epoch 766, Loss: 0.429483562707901, Final Batch Loss: 0.13287200033664703\n",
      "Subject 0, Epoch 767, Loss: 0.32972000539302826, Final Batch Loss: 0.1321602165699005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 768, Loss: 0.43799253553152084, Final Batch Loss: 0.1806124597787857\n",
      "Subject 0, Epoch 769, Loss: 0.51456418633461, Final Batch Loss: 0.25305336713790894\n",
      "Subject 0, Epoch 770, Loss: 0.29825762659311295, Final Batch Loss: 0.10536685585975647\n",
      "Subject 0, Epoch 771, Loss: 0.3697572499513626, Final Batch Loss: 0.11425719410181046\n",
      "Subject 0, Epoch 772, Loss: 0.3715372234582901, Final Batch Loss: 0.22297345101833344\n",
      "Subject 0, Epoch 773, Loss: 0.41790901869535446, Final Batch Loss: 0.1616443246603012\n",
      "Subject 0, Epoch 774, Loss: 0.37238577753305435, Final Batch Loss: 0.12711891531944275\n",
      "Subject 0, Epoch 775, Loss: 0.3703703284263611, Final Batch Loss: 0.15143977105617523\n",
      "Subject 0, Epoch 776, Loss: 0.4187181517481804, Final Batch Loss: 0.21194297075271606\n",
      "Subject 0, Epoch 777, Loss: 0.327364519238472, Final Batch Loss: 0.10501516610383987\n",
      "Subject 0, Epoch 778, Loss: 0.3843827620148659, Final Batch Loss: 0.0727330669760704\n",
      "Subject 0, Epoch 779, Loss: 0.3189348429441452, Final Batch Loss: 0.12751269340515137\n",
      "Subject 0, Epoch 780, Loss: 0.41486093401908875, Final Batch Loss: 0.07529942691326141\n",
      "Subject 0, Epoch 781, Loss: 0.57911366969347, Final Batch Loss: 0.17863589525222778\n",
      "Subject 0, Epoch 782, Loss: 0.3969571962952614, Final Batch Loss: 0.10229606926441193\n",
      "Subject 0, Epoch 783, Loss: 0.3032558709383011, Final Batch Loss: 0.06145957112312317\n",
      "Subject 0, Epoch 784, Loss: 0.43435658514499664, Final Batch Loss: 0.13366925716400146\n",
      "Subject 0, Epoch 785, Loss: 0.4069991707801819, Final Batch Loss: 0.17103499174118042\n",
      "Subject 0, Epoch 786, Loss: 0.3547920286655426, Final Batch Loss: 0.10526915639638901\n",
      "Subject 0, Epoch 787, Loss: 0.37921182811260223, Final Batch Loss: 0.13235193490982056\n",
      "Subject 0, Epoch 788, Loss: 0.3985605686903, Final Batch Loss: 0.1510285884141922\n",
      "Subject 0, Epoch 789, Loss: 0.3967982307076454, Final Batch Loss: 0.1682056337594986\n",
      "Subject 0, Epoch 790, Loss: 0.3632005602121353, Final Batch Loss: 0.09062699973583221\n",
      "Subject 0, Epoch 791, Loss: 0.29508769512176514, Final Batch Loss: 0.07429958879947662\n",
      "Subject 0, Epoch 792, Loss: 0.41626372188329697, Final Batch Loss: 0.1276140660047531\n",
      "Subject 0, Epoch 793, Loss: 0.4253307655453682, Final Batch Loss: 0.15866228938102722\n",
      "Subject 0, Epoch 794, Loss: 0.33808063715696335, Final Batch Loss: 0.1591385006904602\n",
      "Subject 0, Epoch 795, Loss: 0.38450634479522705, Final Batch Loss: 0.13612382113933563\n",
      "Subject 0, Epoch 796, Loss: 0.33336759358644485, Final Batch Loss: 0.06829474121332169\n",
      "Subject 0, Epoch 797, Loss: 0.27699014544487, Final Batch Loss: 0.09451664239168167\n",
      "Subject 0, Epoch 798, Loss: 0.32664822041988373, Final Batch Loss: 0.08872893452644348\n",
      "Subject 0, Epoch 799, Loss: 0.42523280531167984, Final Batch Loss: 0.07367061823606491\n",
      "Subject 0, Epoch 800, Loss: 0.3107237294316292, Final Batch Loss: 0.14761808514595032\n",
      "Subject 0, Epoch 801, Loss: 0.32794953882694244, Final Batch Loss: 0.10607864707708359\n",
      "Subject 0, Epoch 802, Loss: 0.3069397136569023, Final Batch Loss: 0.09688276052474976\n",
      "Subject 0, Epoch 803, Loss: 0.38638824969530106, Final Batch Loss: 0.11521933227777481\n",
      "Subject 0, Epoch 804, Loss: 0.38314104825258255, Final Batch Loss: 0.14649195969104767\n",
      "Subject 0, Epoch 805, Loss: 0.360196128487587, Final Batch Loss: 0.0859353169798851\n",
      "Subject 0, Epoch 806, Loss: 0.35249846428632736, Final Batch Loss: 0.12865234911441803\n",
      "Subject 0, Epoch 807, Loss: 0.31234320253133774, Final Batch Loss: 0.11735472083091736\n",
      "Subject 0, Epoch 808, Loss: 0.3746657744050026, Final Batch Loss: 0.13936012983322144\n",
      "Subject 0, Epoch 809, Loss: 0.32711193710565567, Final Batch Loss: 0.07148268073797226\n",
      "Subject 0, Epoch 810, Loss: 0.2960543930530548, Final Batch Loss: 0.09298676997423172\n",
      "Subject 0, Epoch 811, Loss: 0.3475710600614548, Final Batch Loss: 0.16736595332622528\n",
      "Subject 0, Epoch 812, Loss: 0.30288638174533844, Final Batch Loss: 0.14458738267421722\n",
      "Subject 0, Epoch 813, Loss: 0.36153149604797363, Final Batch Loss: 0.08017170429229736\n",
      "Subject 0, Epoch 814, Loss: 0.3242422044277191, Final Batch Loss: 0.15150170028209686\n",
      "Subject 0, Epoch 815, Loss: 0.33968330919742584, Final Batch Loss: 0.16531595587730408\n",
      "Subject 0, Epoch 816, Loss: 0.3201252222061157, Final Batch Loss: 0.13766643404960632\n",
      "Subject 0, Epoch 817, Loss: 0.31795534491539, Final Batch Loss: 0.1189664676785469\n",
      "Subject 0, Epoch 818, Loss: 0.41583041846752167, Final Batch Loss: 0.08434682339429855\n",
      "Subject 0, Epoch 819, Loss: 0.3400856554508209, Final Batch Loss: 0.09407312422990799\n",
      "Subject 0, Epoch 820, Loss: 0.3372735008597374, Final Batch Loss: 0.11349456757307053\n",
      "Subject 0, Epoch 821, Loss: 0.3570583164691925, Final Batch Loss: 0.10634350031614304\n",
      "Subject 0, Epoch 822, Loss: 0.35366424173116684, Final Batch Loss: 0.22036434710025787\n",
      "Subject 0, Epoch 823, Loss: 0.2774479389190674, Final Batch Loss: 0.09702502936124802\n",
      "Subject 0, Epoch 824, Loss: 0.43836089223623276, Final Batch Loss: 0.16679802536964417\n",
      "Subject 0, Epoch 825, Loss: 0.3401865214109421, Final Batch Loss: 0.18517427146434784\n",
      "Subject 0, Epoch 826, Loss: 0.2699076309800148, Final Batch Loss: 0.08624844998121262\n",
      "Subject 0, Epoch 827, Loss: 0.38309990614652634, Final Batch Loss: 0.10942899435758591\n",
      "Subject 0, Epoch 828, Loss: 0.3532553091645241, Final Batch Loss: 0.1630118191242218\n",
      "Subject 0, Epoch 829, Loss: 0.32263920456171036, Final Batch Loss: 0.17252297699451447\n",
      "Subject 0, Epoch 830, Loss: 0.40016883611679077, Final Batch Loss: 0.10686877369880676\n",
      "Subject 0, Epoch 831, Loss: 0.3173121213912964, Final Batch Loss: 0.0998542383313179\n",
      "Subject 0, Epoch 832, Loss: 0.33366531878709793, Final Batch Loss: 0.10655590891838074\n",
      "Subject 0, Epoch 833, Loss: 0.3976789638400078, Final Batch Loss: 0.1683645099401474\n",
      "Subject 0, Epoch 834, Loss: 0.49394142627716064, Final Batch Loss: 0.20616737008094788\n",
      "Subject 0, Epoch 835, Loss: 0.3965817466378212, Final Batch Loss: 0.11245570331811905\n",
      "Subject 0, Epoch 836, Loss: 0.19823160022497177, Final Batch Loss: 0.07299487292766571\n",
      "Subject 0, Epoch 837, Loss: 0.28318630903959274, Final Batch Loss: 0.0940355658531189\n",
      "Subject 0, Epoch 838, Loss: 0.2537130229175091, Final Batch Loss: 0.07471053302288055\n",
      "Subject 0, Epoch 839, Loss: 0.34998195618391037, Final Batch Loss: 0.14479227364063263\n",
      "Subject 0, Epoch 840, Loss: 0.3629629909992218, Final Batch Loss: 0.11753221601247787\n",
      "Subject 0, Epoch 841, Loss: 0.32972322404384613, Final Batch Loss: 0.07540605962276459\n",
      "Subject 0, Epoch 842, Loss: 0.36879510432481766, Final Batch Loss: 0.15980185568332672\n",
      "Subject 0, Epoch 843, Loss: 0.28134772181510925, Final Batch Loss: 0.06515775620937347\n",
      "Subject 0, Epoch 844, Loss: 0.1983269415795803, Final Batch Loss: 0.08575043827295303\n",
      "Subject 0, Epoch 845, Loss: 0.3249371275305748, Final Batch Loss: 0.10376731306314468\n",
      "Subject 0, Epoch 846, Loss: 0.31336987018585205, Final Batch Loss: 0.1681995540857315\n",
      "Subject 0, Epoch 847, Loss: 0.3163423389196396, Final Batch Loss: 0.10451941937208176\n",
      "Subject 0, Epoch 848, Loss: 0.2815491631627083, Final Batch Loss: 0.08259712904691696\n",
      "Subject 0, Epoch 849, Loss: 0.2065500095486641, Final Batch Loss: 0.07077734917402267\n",
      "Subject 0, Epoch 850, Loss: 0.259240347892046, Final Batch Loss: 0.09609300643205643\n",
      "Subject 0, Epoch 851, Loss: 0.30708175152540207, Final Batch Loss: 0.07272565364837646\n",
      "Subject 0, Epoch 852, Loss: 0.35396865755319595, Final Batch Loss: 0.11869362741708755\n",
      "Subject 0, Epoch 853, Loss: 0.3198091685771942, Final Batch Loss: 0.11232328414916992\n",
      "Subject 0, Epoch 854, Loss: 0.35471148043870926, Final Batch Loss: 0.09045697003602982\n",
      "Subject 0, Epoch 855, Loss: 0.3763897456228733, Final Batch Loss: 0.21655339002609253\n",
      "Subject 0, Epoch 856, Loss: 0.3820137456059456, Final Batch Loss: 0.12380853295326233\n",
      "Subject 0, Epoch 857, Loss: 0.3645782545208931, Final Batch Loss: 0.16517163813114166\n",
      "Subject 0, Epoch 858, Loss: 0.2689155414700508, Final Batch Loss: 0.07462470978498459\n",
      "Subject 0, Epoch 859, Loss: 0.36544544249773026, Final Batch Loss: 0.12589727342128754\n",
      "Subject 0, Epoch 860, Loss: 0.3243279233574867, Final Batch Loss: 0.09294004738330841\n",
      "Subject 0, Epoch 861, Loss: 0.3344493955373764, Final Batch Loss: 0.10766687989234924\n",
      "Subject 0, Epoch 862, Loss: 0.30713582038879395, Final Batch Loss: 0.075986348092556\n",
      "Subject 0, Epoch 863, Loss: 0.34249814599752426, Final Batch Loss: 0.0659794732928276\n",
      "Subject 0, Epoch 864, Loss: 0.25137533247470856, Final Batch Loss: 0.0878521203994751\n",
      "Subject 0, Epoch 865, Loss: 0.35182105749845505, Final Batch Loss: 0.1917819380760193\n",
      "Subject 0, Epoch 866, Loss: 0.2589927241206169, Final Batch Loss: 0.12562991678714752\n",
      "Subject 0, Epoch 867, Loss: 0.21905335038900375, Final Batch Loss: 0.08255883306264877\n",
      "Subject 0, Epoch 868, Loss: 0.18428946658968925, Final Batch Loss: 0.041292209178209305\n",
      "Subject 0, Epoch 869, Loss: 0.34977832809090614, Final Batch Loss: 0.11255920678377151\n",
      "Subject 0, Epoch 870, Loss: 0.2959490343928337, Final Batch Loss: 0.09804327040910721\n",
      "Subject 0, Epoch 871, Loss: 0.44946252554655075, Final Batch Loss: 0.1218130961060524\n",
      "Subject 0, Epoch 872, Loss: 0.335632286965847, Final Batch Loss: 0.14590124785900116\n",
      "Subject 0, Epoch 873, Loss: 0.20023371279239655, Final Batch Loss: 0.0492854006588459\n",
      "Subject 0, Epoch 874, Loss: 0.26511601731181145, Final Batch Loss: 0.06765929609537125\n",
      "Subject 0, Epoch 875, Loss: 0.27714522555470467, Final Batch Loss: 0.12103293091058731\n",
      "Subject 0, Epoch 876, Loss: 0.3033655732870102, Final Batch Loss: 0.1290978044271469\n",
      "Subject 0, Epoch 877, Loss: 0.19783341139554977, Final Batch Loss: 0.07726617902517319\n",
      "Subject 0, Epoch 878, Loss: 0.3666485622525215, Final Batch Loss: 0.09386570751667023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 879, Loss: 0.22808270156383514, Final Batch Loss: 0.0899987444281578\n",
      "Subject 0, Epoch 880, Loss: 0.27012133225798607, Final Batch Loss: 0.10949687659740448\n",
      "Subject 0, Epoch 881, Loss: 0.32062845304608345, Final Batch Loss: 0.05466105416417122\n",
      "Subject 0, Epoch 882, Loss: 0.21555548906326294, Final Batch Loss: 0.04918166995048523\n",
      "Subject 0, Epoch 883, Loss: 0.361406110227108, Final Batch Loss: 0.15200793743133545\n",
      "Subject 0, Epoch 884, Loss: 0.24442439153790474, Final Batch Loss: 0.11398348957300186\n",
      "Subject 0, Epoch 885, Loss: 0.27657801657915115, Final Batch Loss: 0.12134993076324463\n",
      "Subject 0, Epoch 886, Loss: 0.24573635309934616, Final Batch Loss: 0.06558879464864731\n",
      "Subject 0, Epoch 887, Loss: 0.2780872844159603, Final Batch Loss: 0.0500481016933918\n",
      "Subject 0, Epoch 888, Loss: 0.3295084312558174, Final Batch Loss: 0.13131289184093475\n",
      "Subject 0, Epoch 889, Loss: 0.29503947496414185, Final Batch Loss: 0.0947866216301918\n",
      "Subject 0, Epoch 890, Loss: 0.19495359063148499, Final Batch Loss: 0.09003941714763641\n",
      "Subject 0, Epoch 891, Loss: 0.350699819624424, Final Batch Loss: 0.10355483740568161\n",
      "Subject 0, Epoch 892, Loss: 0.3669925034046173, Final Batch Loss: 0.083374984562397\n",
      "Subject 0, Epoch 893, Loss: 0.3062369003891945, Final Batch Loss: 0.13809074461460114\n",
      "Subject 0, Epoch 894, Loss: 0.23876401036977768, Final Batch Loss: 0.1414804309606552\n",
      "Subject 0, Epoch 895, Loss: 0.2832752391695976, Final Batch Loss: 0.13733388483524323\n",
      "Subject 0, Epoch 896, Loss: 0.3541499450802803, Final Batch Loss: 0.0839390903711319\n",
      "Subject 0, Epoch 897, Loss: 0.35632840543985367, Final Batch Loss: 0.108457051217556\n",
      "Subject 0, Epoch 898, Loss: 0.251468688249588, Final Batch Loss: 0.05724824219942093\n",
      "Subject 0, Epoch 899, Loss: 0.30699100717902184, Final Batch Loss: 0.16887731850147247\n",
      "Subject 0, Epoch 900, Loss: 0.2458893209695816, Final Batch Loss: 0.08323027938604355\n",
      "Subject 0, Epoch 901, Loss: 0.2730729468166828, Final Batch Loss: 0.1255415827035904\n",
      "Subject 0, Epoch 902, Loss: 0.3228374049067497, Final Batch Loss: 0.152591273188591\n",
      "Subject 0, Epoch 903, Loss: 0.2358342967927456, Final Batch Loss: 0.05360889807343483\n",
      "Subject 0, Epoch 904, Loss: 0.24824530631303787, Final Batch Loss: 0.09985975176095963\n",
      "Subject 0, Epoch 905, Loss: 0.30278611183166504, Final Batch Loss: 0.09630773216485977\n",
      "Subject 0, Epoch 906, Loss: 0.22374505922198296, Final Batch Loss: 0.06496656686067581\n",
      "Subject 0, Epoch 907, Loss: 0.38880079984664917, Final Batch Loss: 0.19847722351551056\n",
      "Subject 0, Epoch 908, Loss: 0.1959468349814415, Final Batch Loss: 0.038064517080783844\n",
      "Subject 0, Epoch 909, Loss: 0.3590972125530243, Final Batch Loss: 0.14602187275886536\n",
      "Subject 0, Epoch 910, Loss: 0.21322259679436684, Final Batch Loss: 0.046983636915683746\n",
      "Subject 0, Epoch 911, Loss: 0.22319817170500755, Final Batch Loss: 0.05629800260066986\n",
      "Subject 0, Epoch 912, Loss: 0.3369057960808277, Final Batch Loss: 0.20772530138492584\n",
      "Subject 0, Epoch 913, Loss: 0.2330385521054268, Final Batch Loss: 0.05555768311023712\n",
      "Subject 0, Epoch 914, Loss: 0.1466897837817669, Final Batch Loss: 0.05276532471179962\n",
      "Subject 0, Epoch 915, Loss: 0.3341091424226761, Final Batch Loss: 0.07229135185480118\n",
      "Subject 0, Epoch 916, Loss: 0.33801812678575516, Final Batch Loss: 0.0738866999745369\n",
      "Subject 0, Epoch 917, Loss: 0.3730953708291054, Final Batch Loss: 0.15119442343711853\n",
      "Subject 0, Epoch 918, Loss: 0.2072003185749054, Final Batch Loss: 0.08906199038028717\n",
      "Subject 0, Epoch 919, Loss: 0.28708644583821297, Final Batch Loss: 0.07703858613967896\n",
      "Subject 0, Epoch 920, Loss: 0.2429928034543991, Final Batch Loss: 0.11094872653484344\n",
      "Subject 0, Epoch 921, Loss: 0.2424713373184204, Final Batch Loss: 0.0771193727850914\n",
      "Subject 0, Epoch 922, Loss: 0.3324555903673172, Final Batch Loss: 0.04296942800283432\n",
      "Subject 0, Epoch 923, Loss: 0.33704905956983566, Final Batch Loss: 0.11920160800218582\n",
      "Subject 0, Epoch 924, Loss: 0.21366504579782486, Final Batch Loss: 0.037327565252780914\n",
      "Subject 0, Epoch 925, Loss: 0.24213312938809395, Final Batch Loss: 0.045887190848588943\n",
      "Subject 0, Epoch 926, Loss: 0.19964544847607613, Final Batch Loss: 0.11179511994123459\n",
      "Subject 0, Epoch 927, Loss: 0.2416793294250965, Final Batch Loss: 0.07214690744876862\n",
      "Subject 0, Epoch 928, Loss: 0.2351435236632824, Final Batch Loss: 0.13357004523277283\n",
      "Subject 0, Epoch 929, Loss: 0.22125911712646484, Final Batch Loss: 0.08818390965461731\n",
      "Subject 0, Epoch 930, Loss: 0.2298455312848091, Final Batch Loss: 0.10462404042482376\n",
      "Subject 0, Epoch 931, Loss: 0.2635195441544056, Final Batch Loss: 0.1435660868883133\n",
      "Subject 0, Epoch 932, Loss: 0.2463952824473381, Final Batch Loss: 0.05905667692422867\n",
      "Subject 0, Epoch 933, Loss: 0.2629152238368988, Final Batch Loss: 0.1105944886803627\n",
      "Subject 0, Epoch 934, Loss: 0.3146522343158722, Final Batch Loss: 0.07184195518493652\n",
      "Subject 0, Epoch 935, Loss: 0.2685498297214508, Final Batch Loss: 0.12718261778354645\n",
      "Subject 0, Epoch 936, Loss: 0.23718659579753876, Final Batch Loss: 0.11617293953895569\n",
      "Subject 0, Epoch 937, Loss: 0.3532751686871052, Final Batch Loss: 0.24815165996551514\n",
      "Subject 0, Epoch 938, Loss: 0.20099994912743568, Final Batch Loss: 0.05921495333313942\n",
      "Subject 0, Epoch 939, Loss: 0.21931054443120956, Final Batch Loss: 0.1398179531097412\n",
      "Subject 0, Epoch 940, Loss: 0.3028196543455124, Final Batch Loss: 0.10520831495523453\n",
      "Subject 0, Epoch 941, Loss: 0.2355031669139862, Final Batch Loss: 0.06088501960039139\n",
      "Subject 0, Epoch 942, Loss: 0.2925488278269768, Final Batch Loss: 0.11350325495004654\n",
      "Subject 0, Epoch 943, Loss: 0.21830007061362267, Final Batch Loss: 0.0796733871102333\n",
      "Subject 0, Epoch 944, Loss: 0.2971016615629196, Final Batch Loss: 0.09235953539609909\n",
      "Subject 0, Epoch 945, Loss: 0.2730772942304611, Final Batch Loss: 0.1562543660402298\n",
      "Subject 0, Epoch 946, Loss: 0.2452828846871853, Final Batch Loss: 0.036668527871370316\n",
      "Subject 0, Epoch 947, Loss: 0.2267731986939907, Final Batch Loss: 0.06034911051392555\n",
      "Subject 0, Epoch 948, Loss: 0.2761700414121151, Final Batch Loss: 0.08342581242322922\n",
      "Subject 0, Epoch 949, Loss: 0.22693029046058655, Final Batch Loss: 0.09867050498723984\n",
      "Subject 0, Epoch 950, Loss: 0.26209234446287155, Final Batch Loss: 0.06623347848653793\n",
      "Subject 0, Epoch 951, Loss: 0.2611103802919388, Final Batch Loss: 0.0968078225851059\n",
      "Subject 0, Epoch 952, Loss: 0.12963493540883064, Final Batch Loss: 0.051845792680978775\n",
      "Subject 0, Epoch 953, Loss: 0.3326655998826027, Final Batch Loss: 0.197523295879364\n",
      "Subject 0, Epoch 954, Loss: 0.30018530040979385, Final Batch Loss: 0.09395769983530045\n",
      "Subject 0, Epoch 955, Loss: 0.2000233791768551, Final Batch Loss: 0.06338563561439514\n",
      "Subject 0, Epoch 956, Loss: 0.28364284336566925, Final Batch Loss: 0.08409995585680008\n",
      "Subject 0, Epoch 957, Loss: 0.2530783675611019, Final Batch Loss: 0.11765216290950775\n",
      "Subject 0, Epoch 958, Loss: 0.1736702062189579, Final Batch Loss: 0.055619243532419205\n",
      "Subject 0, Epoch 959, Loss: 0.2906849607825279, Final Batch Loss: 0.06288513541221619\n",
      "Subject 0, Epoch 960, Loss: 0.34736109897494316, Final Batch Loss: 0.1509210616350174\n",
      "Subject 0, Epoch 961, Loss: 0.3334101438522339, Final Batch Loss: 0.09636592119932175\n",
      "Subject 0, Epoch 962, Loss: 0.21533355116844177, Final Batch Loss: 0.09856731444597244\n",
      "Subject 0, Epoch 963, Loss: 0.2647341191768646, Final Batch Loss: 0.12326916307210922\n",
      "Subject 0, Epoch 964, Loss: 0.2049831673502922, Final Batch Loss: 0.05225082486867905\n",
      "Subject 0, Epoch 965, Loss: 0.2028009332716465, Final Batch Loss: 0.09164439141750336\n",
      "Subject 0, Epoch 966, Loss: 0.2077139038592577, Final Batch Loss: 0.10795029252767563\n",
      "Subject 0, Epoch 967, Loss: 0.19346417114138603, Final Batch Loss: 0.03385292366147041\n",
      "Subject 0, Epoch 968, Loss: 0.223963912576437, Final Batch Loss: 0.05729460343718529\n",
      "Subject 0, Epoch 969, Loss: 0.27310118824243546, Final Batch Loss: 0.10727374255657196\n",
      "Subject 0, Epoch 970, Loss: 0.22727686911821365, Final Batch Loss: 0.028923973441123962\n",
      "Subject 0, Epoch 971, Loss: 0.23983925580978394, Final Batch Loss: 0.09025275707244873\n",
      "Subject 0, Epoch 972, Loss: 0.2191615030169487, Final Batch Loss: 0.06691835075616837\n",
      "Subject 0, Epoch 973, Loss: 0.22046487778425217, Final Batch Loss: 0.0693802461028099\n",
      "Subject 0, Epoch 974, Loss: 0.2087228950113058, Final Batch Loss: 0.02463926188647747\n",
      "Subject 0, Epoch 975, Loss: 0.22856532782316208, Final Batch Loss: 0.08313433080911636\n",
      "Subject 0, Epoch 976, Loss: 0.2643490731716156, Final Batch Loss: 0.07650414109230042\n",
      "Subject 0, Epoch 977, Loss: 0.15479985624551773, Final Batch Loss: 0.0641229972243309\n",
      "Subject 0, Epoch 978, Loss: 0.19466647133231163, Final Batch Loss: 0.06873451173305511\n",
      "Subject 0, Epoch 979, Loss: 0.1601773202419281, Final Batch Loss: 0.04103035479784012\n",
      "Subject 0, Epoch 980, Loss: 0.45026496052742004, Final Batch Loss: 0.16642193496227264\n",
      "Subject 0, Epoch 981, Loss: 0.2854946553707123, Final Batch Loss: 0.1376892626285553\n",
      "Subject 0, Epoch 982, Loss: 0.176980160176754, Final Batch Loss: 0.052466150373220444\n",
      "Subject 0, Epoch 983, Loss: 0.2546844072639942, Final Batch Loss: 0.06028665229678154\n",
      "Subject 0, Epoch 984, Loss: 0.23328487947583199, Final Batch Loss: 0.058641619980335236\n",
      "Subject 0, Epoch 985, Loss: 0.2808178886771202, Final Batch Loss: 0.19586166739463806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0, Epoch 986, Loss: 0.21534597128629684, Final Batch Loss: 0.08984378725290298\n",
      "Subject 0, Epoch 987, Loss: 0.23689495027065277, Final Batch Loss: 0.07769952714443207\n",
      "Subject 0, Epoch 988, Loss: 0.2143929973244667, Final Batch Loss: 0.06350668519735336\n",
      "Subject 0, Epoch 989, Loss: 0.3611656054854393, Final Batch Loss: 0.11548706144094467\n",
      "Subject 0, Epoch 990, Loss: 0.35272249579429626, Final Batch Loss: 0.16144336760044098\n",
      "Subject 0, Epoch 991, Loss: 0.18204113841056824, Final Batch Loss: 0.024456577375531197\n",
      "Subject 0, Epoch 992, Loss: 0.30162888765335083, Final Batch Loss: 0.11740079522132874\n",
      "Subject 0, Epoch 993, Loss: 0.17060443758964539, Final Batch Loss: 0.06355714052915573\n",
      "Subject 0, Epoch 994, Loss: 0.12577389553189278, Final Batch Loss: 0.03151565045118332\n",
      "Subject 0, Epoch 995, Loss: 0.16791042312979698, Final Batch Loss: 0.049151353538036346\n",
      "Subject 0, Epoch 996, Loss: 0.13069428130984306, Final Batch Loss: 0.0535687580704689\n",
      "Subject 0, Epoch 997, Loss: 0.2402807753533125, Final Batch Loss: 0.029064880684018135\n",
      "Subject 0, Epoch 998, Loss: 0.17228076606988907, Final Batch Loss: 0.04465712979435921\n",
      "Subject 0, Epoch 999, Loss: 0.24340306594967842, Final Batch Loss: 0.057321060448884964\n",
      "Subject 0, Epoch 1000, Loss: 0.2400105707347393, Final Batch Loss: 0.08043511211872101\n",
      "Subject 1, Epoch 1, Loss: 5.405092000961304, Final Batch Loss: 1.7990400791168213\n",
      "Subject 1, Epoch 2, Loss: 5.390669941902161, Final Batch Loss: 1.8012760877609253\n",
      "Subject 1, Epoch 3, Loss: 5.393904685974121, Final Batch Loss: 1.7994803190231323\n",
      "Subject 1, Epoch 4, Loss: 5.380959630012512, Final Batch Loss: 1.7851567268371582\n",
      "Subject 1, Epoch 5, Loss: 5.361932396888733, Final Batch Loss: 1.7900984287261963\n",
      "Subject 1, Epoch 6, Loss: 5.362804532051086, Final Batch Loss: 1.796744704246521\n",
      "Subject 1, Epoch 7, Loss: 5.34181272983551, Final Batch Loss: 1.7744219303131104\n",
      "Subject 1, Epoch 8, Loss: 5.334697961807251, Final Batch Loss: 1.7807819843292236\n",
      "Subject 1, Epoch 9, Loss: 5.318357586860657, Final Batch Loss: 1.7686372995376587\n",
      "Subject 1, Epoch 10, Loss: 5.280961513519287, Final Batch Loss: 1.7617411613464355\n",
      "Subject 1, Epoch 11, Loss: 5.289589285850525, Final Batch Loss: 1.7635434865951538\n",
      "Subject 1, Epoch 12, Loss: 5.258890628814697, Final Batch Loss: 1.7498750686645508\n",
      "Subject 1, Epoch 13, Loss: 5.194665312767029, Final Batch Loss: 1.7291301488876343\n",
      "Subject 1, Epoch 14, Loss: 5.171051263809204, Final Batch Loss: 1.714569091796875\n",
      "Subject 1, Epoch 15, Loss: 5.12557053565979, Final Batch Loss: 1.698716640472412\n",
      "Subject 1, Epoch 16, Loss: 5.052790522575378, Final Batch Loss: 1.6723147630691528\n",
      "Subject 1, Epoch 17, Loss: 4.982181072235107, Final Batch Loss: 1.645815372467041\n",
      "Subject 1, Epoch 18, Loss: 4.892272353172302, Final Batch Loss: 1.6434528827667236\n",
      "Subject 1, Epoch 19, Loss: 4.860348463058472, Final Batch Loss: 1.6063002347946167\n",
      "Subject 1, Epoch 20, Loss: 4.697764992713928, Final Batch Loss: 1.5530720949172974\n",
      "Subject 1, Epoch 21, Loss: 4.552730917930603, Final Batch Loss: 1.5227022171020508\n",
      "Subject 1, Epoch 22, Loss: 4.580841660499573, Final Batch Loss: 1.5597009658813477\n",
      "Subject 1, Epoch 23, Loss: 4.38298237323761, Final Batch Loss: 1.4139626026153564\n",
      "Subject 1, Epoch 24, Loss: 4.346471905708313, Final Batch Loss: 1.4672632217407227\n",
      "Subject 1, Epoch 25, Loss: 4.33009135723114, Final Batch Loss: 1.4148118495941162\n",
      "Subject 1, Epoch 26, Loss: 4.261553645133972, Final Batch Loss: 1.4295570850372314\n",
      "Subject 1, Epoch 27, Loss: 4.177027463912964, Final Batch Loss: 1.4290976524353027\n",
      "Subject 1, Epoch 28, Loss: 4.159851670265198, Final Batch Loss: 1.3799899816513062\n",
      "Subject 1, Epoch 29, Loss: 4.06654679775238, Final Batch Loss: 1.3046998977661133\n",
      "Subject 1, Epoch 30, Loss: 4.046271562576294, Final Batch Loss: 1.3189377784729004\n",
      "Subject 1, Epoch 31, Loss: 3.928887724876404, Final Batch Loss: 1.2929420471191406\n",
      "Subject 1, Epoch 32, Loss: 3.801827311515808, Final Batch Loss: 1.2692968845367432\n",
      "Subject 1, Epoch 33, Loss: 3.7175878286361694, Final Batch Loss: 1.3429558277130127\n",
      "Subject 1, Epoch 34, Loss: 3.743512988090515, Final Batch Loss: 1.2289804220199585\n",
      "Subject 1, Epoch 35, Loss: 3.642066717147827, Final Batch Loss: 1.1832859516143799\n",
      "Subject 1, Epoch 36, Loss: 3.6659367084503174, Final Batch Loss: 1.1868230104446411\n",
      "Subject 1, Epoch 37, Loss: 3.6225298643112183, Final Batch Loss: 1.1512874364852905\n",
      "Subject 1, Epoch 38, Loss: 3.5903526544570923, Final Batch Loss: 1.1320343017578125\n",
      "Subject 1, Epoch 39, Loss: 3.5921913385391235, Final Batch Loss: 1.191394329071045\n",
      "Subject 1, Epoch 40, Loss: 3.5190800428390503, Final Batch Loss: 1.186428427696228\n",
      "Subject 1, Epoch 41, Loss: 3.453348755836487, Final Batch Loss: 1.0989506244659424\n",
      "Subject 1, Epoch 42, Loss: 3.574275851249695, Final Batch Loss: 1.150870442390442\n",
      "Subject 1, Epoch 43, Loss: 3.5064998865127563, Final Batch Loss: 1.1957615613937378\n",
      "Subject 1, Epoch 44, Loss: 3.5140528678894043, Final Batch Loss: 1.1925286054611206\n",
      "Subject 1, Epoch 45, Loss: 3.4874629974365234, Final Batch Loss: 1.183484673500061\n",
      "Subject 1, Epoch 46, Loss: 3.4195152521133423, Final Batch Loss: 1.126935601234436\n",
      "Subject 1, Epoch 47, Loss: 3.4152954816818237, Final Batch Loss: 1.1441071033477783\n",
      "Subject 1, Epoch 48, Loss: 3.4522467851638794, Final Batch Loss: 1.1322004795074463\n",
      "Subject 1, Epoch 49, Loss: 3.41245436668396, Final Batch Loss: 1.1363608837127686\n",
      "Subject 1, Epoch 50, Loss: 3.4794148206710815, Final Batch Loss: 1.1809042692184448\n",
      "Subject 1, Epoch 51, Loss: 3.4924477338790894, Final Batch Loss: 1.1881682872772217\n",
      "Subject 1, Epoch 52, Loss: 3.4223990440368652, Final Batch Loss: 1.1604713201522827\n",
      "Subject 1, Epoch 53, Loss: 3.4285030364990234, Final Batch Loss: 1.1236294507980347\n",
      "Subject 1, Epoch 54, Loss: 3.433749794960022, Final Batch Loss: 1.1479512453079224\n",
      "Subject 1, Epoch 55, Loss: 3.466956853866577, Final Batch Loss: 1.1561909914016724\n",
      "Subject 1, Epoch 56, Loss: 3.4688923358917236, Final Batch Loss: 1.1528639793395996\n",
      "Subject 1, Epoch 57, Loss: 3.3964747190475464, Final Batch Loss: 1.1235899925231934\n",
      "Subject 1, Epoch 58, Loss: 3.482222080230713, Final Batch Loss: 1.1347980499267578\n",
      "Subject 1, Epoch 59, Loss: 3.4338427782058716, Final Batch Loss: 1.1585758924484253\n",
      "Subject 1, Epoch 60, Loss: 3.514189600944519, Final Batch Loss: 1.1809372901916504\n",
      "Subject 1, Epoch 61, Loss: 3.497405767440796, Final Batch Loss: 1.1519418954849243\n",
      "Subject 1, Epoch 62, Loss: 3.509201169013977, Final Batch Loss: 1.1600161790847778\n",
      "Subject 1, Epoch 63, Loss: 3.4213502407073975, Final Batch Loss: 1.1994493007659912\n",
      "Subject 1, Epoch 64, Loss: 3.3924609422683716, Final Batch Loss: 1.161234974861145\n",
      "Subject 1, Epoch 65, Loss: 3.4536566734313965, Final Batch Loss: 1.1413853168487549\n",
      "Subject 1, Epoch 66, Loss: 3.3835723400115967, Final Batch Loss: 1.167387843132019\n",
      "Subject 1, Epoch 67, Loss: 3.3713507652282715, Final Batch Loss: 1.1370863914489746\n",
      "Subject 1, Epoch 68, Loss: 3.376213550567627, Final Batch Loss: 1.1089030504226685\n",
      "Subject 1, Epoch 69, Loss: 3.386474132537842, Final Batch Loss: 1.1318978071212769\n",
      "Subject 1, Epoch 70, Loss: 3.2816919088363647, Final Batch Loss: 1.0461372137069702\n",
      "Subject 1, Epoch 71, Loss: 3.407836675643921, Final Batch Loss: 1.1314213275909424\n",
      "Subject 1, Epoch 72, Loss: 3.4095126390457153, Final Batch Loss: 1.1249558925628662\n",
      "Subject 1, Epoch 73, Loss: 3.336289644241333, Final Batch Loss: 1.1088770627975464\n",
      "Subject 1, Epoch 74, Loss: 3.3091315031051636, Final Batch Loss: 1.0826492309570312\n",
      "Subject 1, Epoch 75, Loss: 3.3276134729385376, Final Batch Loss: 1.071038007736206\n",
      "Subject 1, Epoch 76, Loss: 3.392507553100586, Final Batch Loss: 1.1543148756027222\n",
      "Subject 1, Epoch 77, Loss: 3.4665826559066772, Final Batch Loss: 1.1603947877883911\n",
      "Subject 1, Epoch 78, Loss: 3.30483615398407, Final Batch Loss: 1.0734385251998901\n",
      "Subject 1, Epoch 79, Loss: 3.327302932739258, Final Batch Loss: 1.0918043851852417\n",
      "Subject 1, Epoch 80, Loss: 3.3232972621917725, Final Batch Loss: 1.098919153213501\n",
      "Subject 1, Epoch 81, Loss: 3.3925914764404297, Final Batch Loss: 1.161611795425415\n",
      "Subject 1, Epoch 82, Loss: 3.2968162298202515, Final Batch Loss: 1.0830267667770386\n",
      "Subject 1, Epoch 83, Loss: 3.1839059591293335, Final Batch Loss: 1.048721432685852\n",
      "Subject 1, Epoch 84, Loss: 3.3175971508026123, Final Batch Loss: 1.101636290550232\n",
      "Subject 1, Epoch 85, Loss: 3.3552552461624146, Final Batch Loss: 1.1532971858978271\n",
      "Subject 1, Epoch 86, Loss: 3.329569697380066, Final Batch Loss: 1.1118159294128418\n",
      "Subject 1, Epoch 87, Loss: 3.3135745525360107, Final Batch Loss: 1.0823920965194702\n",
      "Subject 1, Epoch 88, Loss: 3.3254201412200928, Final Batch Loss: 1.1432315111160278\n",
      "Subject 1, Epoch 89, Loss: 3.2750229835510254, Final Batch Loss: 1.0990904569625854\n",
      "Subject 1, Epoch 90, Loss: 3.2513084411621094, Final Batch Loss: 1.0692329406738281\n",
      "Subject 1, Epoch 91, Loss: 3.247234344482422, Final Batch Loss: 1.0759682655334473\n",
      "Subject 1, Epoch 92, Loss: 3.281757116317749, Final Batch Loss: 1.1059913635253906\n",
      "Subject 1, Epoch 93, Loss: 3.2772237062454224, Final Batch Loss: 1.0768404006958008\n",
      "Subject 1, Epoch 94, Loss: 3.212378144264221, Final Batch Loss: 1.0595918893814087\n",
      "Subject 1, Epoch 95, Loss: 3.2298760414123535, Final Batch Loss: 1.0814673900604248\n",
      "Subject 1, Epoch 96, Loss: 3.1510509252548218, Final Batch Loss: 1.056546926498413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 97, Loss: 3.1773916482925415, Final Batch Loss: 1.078481674194336\n",
      "Subject 1, Epoch 98, Loss: 3.138564109802246, Final Batch Loss: 1.058218002319336\n",
      "Subject 1, Epoch 99, Loss: 3.192576289176941, Final Batch Loss: 1.0700712203979492\n",
      "Subject 1, Epoch 100, Loss: 3.183943033218384, Final Batch Loss: 1.0829579830169678\n",
      "Subject 1, Epoch 101, Loss: 3.1728345155715942, Final Batch Loss: 1.0426939725875854\n",
      "Subject 1, Epoch 102, Loss: 3.1397753953933716, Final Batch Loss: 1.0454109907150269\n",
      "Subject 1, Epoch 103, Loss: 3.1377121210098267, Final Batch Loss: 1.069794774055481\n",
      "Subject 1, Epoch 104, Loss: 3.1677956581115723, Final Batch Loss: 1.1058242321014404\n",
      "Subject 1, Epoch 105, Loss: 3.064247727394104, Final Batch Loss: 1.0286197662353516\n",
      "Subject 1, Epoch 106, Loss: 3.035823166370392, Final Batch Loss: 1.0226465463638306\n",
      "Subject 1, Epoch 107, Loss: 3.091127634048462, Final Batch Loss: 1.0199605226516724\n",
      "Subject 1, Epoch 108, Loss: 3.0514724254608154, Final Batch Loss: 1.0341706275939941\n",
      "Subject 1, Epoch 109, Loss: 2.957430124282837, Final Batch Loss: 0.9631701707839966\n",
      "Subject 1, Epoch 110, Loss: 2.9983675479888916, Final Batch Loss: 1.015642523765564\n",
      "Subject 1, Epoch 111, Loss: 2.9448506832122803, Final Batch Loss: 0.9981675744056702\n",
      "Subject 1, Epoch 112, Loss: 2.9930503964424133, Final Batch Loss: 0.9880862236022949\n",
      "Subject 1, Epoch 113, Loss: 2.990351438522339, Final Batch Loss: 1.0220216512680054\n",
      "Subject 1, Epoch 114, Loss: 2.845201015472412, Final Batch Loss: 0.9208971261978149\n",
      "Subject 1, Epoch 115, Loss: 2.9273850321769714, Final Batch Loss: 1.0210180282592773\n",
      "Subject 1, Epoch 116, Loss: 2.887157142162323, Final Batch Loss: 0.9372150301933289\n",
      "Subject 1, Epoch 117, Loss: 2.828627645969391, Final Batch Loss: 0.8843392133712769\n",
      "Subject 1, Epoch 118, Loss: 2.8480312824249268, Final Batch Loss: 0.9194852113723755\n",
      "Subject 1, Epoch 119, Loss: 2.774412214756012, Final Batch Loss: 0.9536334872245789\n",
      "Subject 1, Epoch 120, Loss: 2.8350751996040344, Final Batch Loss: 0.9515683650970459\n",
      "Subject 1, Epoch 121, Loss: 2.791382610797882, Final Batch Loss: 0.8696929216384888\n",
      "Subject 1, Epoch 122, Loss: 2.738710582256317, Final Batch Loss: 0.8372240662574768\n",
      "Subject 1, Epoch 123, Loss: 2.7322888374328613, Final Batch Loss: 0.972650408744812\n",
      "Subject 1, Epoch 124, Loss: 2.686070144176483, Final Batch Loss: 0.9008849859237671\n",
      "Subject 1, Epoch 125, Loss: 2.6528693437576294, Final Batch Loss: 0.8564777374267578\n",
      "Subject 1, Epoch 126, Loss: 2.563260078430176, Final Batch Loss: 0.829434871673584\n",
      "Subject 1, Epoch 127, Loss: 2.7377008199691772, Final Batch Loss: 0.9948002696037292\n",
      "Subject 1, Epoch 128, Loss: 2.663506031036377, Final Batch Loss: 0.9108786582946777\n",
      "Subject 1, Epoch 129, Loss: 2.6537428498268127, Final Batch Loss: 0.9229916334152222\n",
      "Subject 1, Epoch 130, Loss: 2.511174261569977, Final Batch Loss: 0.8696598410606384\n",
      "Subject 1, Epoch 131, Loss: 2.505425810813904, Final Batch Loss: 0.8525605201721191\n",
      "Subject 1, Epoch 132, Loss: 2.4633288979530334, Final Batch Loss: 0.7982370853424072\n",
      "Subject 1, Epoch 133, Loss: 2.5092230439186096, Final Batch Loss: 0.8212125301361084\n",
      "Subject 1, Epoch 134, Loss: 2.495838761329651, Final Batch Loss: 0.8247418403625488\n",
      "Subject 1, Epoch 135, Loss: 2.4751933217048645, Final Batch Loss: 0.8888584971427917\n",
      "Subject 1, Epoch 136, Loss: 2.395975649356842, Final Batch Loss: 0.79698646068573\n",
      "Subject 1, Epoch 137, Loss: 2.5384302139282227, Final Batch Loss: 0.932611882686615\n",
      "Subject 1, Epoch 138, Loss: 2.382716476917267, Final Batch Loss: 0.7856865525245667\n",
      "Subject 1, Epoch 139, Loss: 2.2737662196159363, Final Batch Loss: 0.7719773054122925\n",
      "Subject 1, Epoch 140, Loss: 2.3730770349502563, Final Batch Loss: 0.7764608263969421\n",
      "Subject 1, Epoch 141, Loss: 2.3177701830863953, Final Batch Loss: 0.8377962708473206\n",
      "Subject 1, Epoch 142, Loss: 2.2681437134742737, Final Batch Loss: 0.7482125163078308\n",
      "Subject 1, Epoch 143, Loss: 2.3160101175308228, Final Batch Loss: 0.7415262460708618\n",
      "Subject 1, Epoch 144, Loss: 2.2273700833320618, Final Batch Loss: 0.7396973967552185\n",
      "Subject 1, Epoch 145, Loss: 2.2788796424865723, Final Batch Loss: 0.8488949537277222\n",
      "Subject 1, Epoch 146, Loss: 2.2377981543540955, Final Batch Loss: 0.7363488674163818\n",
      "Subject 1, Epoch 147, Loss: 2.2698424458503723, Final Batch Loss: 0.7362132668495178\n",
      "Subject 1, Epoch 148, Loss: 2.1091296672821045, Final Batch Loss: 0.6675125956535339\n",
      "Subject 1, Epoch 149, Loss: 2.1229894757270813, Final Batch Loss: 0.6733458042144775\n",
      "Subject 1, Epoch 150, Loss: 2.1551101207733154, Final Batch Loss: 0.7753775715827942\n",
      "Subject 1, Epoch 151, Loss: 2.105747878551483, Final Batch Loss: 0.7394623756408691\n",
      "Subject 1, Epoch 152, Loss: 2.1214080452919006, Final Batch Loss: 0.7222791314125061\n",
      "Subject 1, Epoch 153, Loss: 2.1167759895324707, Final Batch Loss: 0.753334641456604\n",
      "Subject 1, Epoch 154, Loss: 2.053863048553467, Final Batch Loss: 0.7012870907783508\n",
      "Subject 1, Epoch 155, Loss: 1.939600944519043, Final Batch Loss: 0.7126169204711914\n",
      "Subject 1, Epoch 156, Loss: 2.0919241905212402, Final Batch Loss: 0.7338364124298096\n",
      "Subject 1, Epoch 157, Loss: 1.9781232476234436, Final Batch Loss: 0.7066498398780823\n",
      "Subject 1, Epoch 158, Loss: 1.9508172273635864, Final Batch Loss: 0.6131893396377563\n",
      "Subject 1, Epoch 159, Loss: 1.8586481213569641, Final Batch Loss: 0.6293671131134033\n",
      "Subject 1, Epoch 160, Loss: 1.8883118629455566, Final Batch Loss: 0.5740132927894592\n",
      "Subject 1, Epoch 161, Loss: 1.8888576030731201, Final Batch Loss: 0.6652138233184814\n",
      "Subject 1, Epoch 162, Loss: 1.823682188987732, Final Batch Loss: 0.6220501661300659\n",
      "Subject 1, Epoch 163, Loss: 1.8467791676521301, Final Batch Loss: 0.6071868538856506\n",
      "Subject 1, Epoch 164, Loss: 1.8928667902946472, Final Batch Loss: 0.542585551738739\n",
      "Subject 1, Epoch 165, Loss: 1.8232253193855286, Final Batch Loss: 0.5772088170051575\n",
      "Subject 1, Epoch 166, Loss: 1.8523577451705933, Final Batch Loss: 0.5341585278511047\n",
      "Subject 1, Epoch 167, Loss: 1.7335281670093536, Final Batch Loss: 0.4903436601161957\n",
      "Subject 1, Epoch 168, Loss: 1.6776635646820068, Final Batch Loss: 0.5402158498764038\n",
      "Subject 1, Epoch 169, Loss: 1.721569299697876, Final Batch Loss: 0.6164484620094299\n",
      "Subject 1, Epoch 170, Loss: 1.7256621718406677, Final Batch Loss: 0.565787136554718\n",
      "Subject 1, Epoch 171, Loss: 1.7603679299354553, Final Batch Loss: 0.6063368916511536\n",
      "Subject 1, Epoch 172, Loss: 1.596678078174591, Final Batch Loss: 0.5565679669380188\n",
      "Subject 1, Epoch 173, Loss: 1.7937569618225098, Final Batch Loss: 0.5806610584259033\n",
      "Subject 1, Epoch 174, Loss: 1.6784764528274536, Final Batch Loss: 0.532435953617096\n",
      "Subject 1, Epoch 175, Loss: 1.6074929237365723, Final Batch Loss: 0.5348424315452576\n",
      "Subject 1, Epoch 176, Loss: 1.5468981862068176, Final Batch Loss: 0.529076099395752\n",
      "Subject 1, Epoch 177, Loss: 1.617128849029541, Final Batch Loss: 0.6118777990341187\n",
      "Subject 1, Epoch 178, Loss: 1.6690062582492828, Final Batch Loss: 0.6522976160049438\n",
      "Subject 1, Epoch 179, Loss: 1.7262371182441711, Final Batch Loss: 0.5744028687477112\n",
      "Subject 1, Epoch 180, Loss: 1.5831919312477112, Final Batch Loss: 0.5193932056427002\n",
      "Subject 1, Epoch 181, Loss: 1.5968431234359741, Final Batch Loss: 0.5863788723945618\n",
      "Subject 1, Epoch 182, Loss: 1.5364323556423187, Final Batch Loss: 0.5110753774642944\n",
      "Subject 1, Epoch 183, Loss: 1.6698920726776123, Final Batch Loss: 0.5423702001571655\n",
      "Subject 1, Epoch 184, Loss: 1.6104629635810852, Final Batch Loss: 0.5462594032287598\n",
      "Subject 1, Epoch 185, Loss: 1.6031016111373901, Final Batch Loss: 0.5369930267333984\n",
      "Subject 1, Epoch 186, Loss: 1.7215802371501923, Final Batch Loss: 0.6023012399673462\n",
      "Subject 1, Epoch 187, Loss: 1.4662885963916779, Final Batch Loss: 0.4440787732601166\n",
      "Subject 1, Epoch 188, Loss: 1.5549111664295197, Final Batch Loss: 0.6113660335540771\n",
      "Subject 1, Epoch 189, Loss: 1.5487244129180908, Final Batch Loss: 0.5367177724838257\n",
      "Subject 1, Epoch 190, Loss: 1.4397222697734833, Final Batch Loss: 0.43868666887283325\n",
      "Subject 1, Epoch 191, Loss: 1.6426678001880646, Final Batch Loss: 0.5341529250144958\n",
      "Subject 1, Epoch 192, Loss: 1.6444473564624786, Final Batch Loss: 0.4816083014011383\n",
      "Subject 1, Epoch 193, Loss: 1.4141792356967926, Final Batch Loss: 0.4337610900402069\n",
      "Subject 1, Epoch 194, Loss: 1.4969819784164429, Final Batch Loss: 0.5133573412895203\n",
      "Subject 1, Epoch 195, Loss: 1.4805033504962921, Final Batch Loss: 0.5197101831436157\n",
      "Subject 1, Epoch 196, Loss: 1.4189271032810211, Final Batch Loss: 0.5477268695831299\n",
      "Subject 1, Epoch 197, Loss: 1.5404708087444305, Final Batch Loss: 0.4727620780467987\n",
      "Subject 1, Epoch 198, Loss: 1.5129523873329163, Final Batch Loss: 0.4533480405807495\n",
      "Subject 1, Epoch 199, Loss: 1.3405723869800568, Final Batch Loss: 0.33865734934806824\n",
      "Subject 1, Epoch 200, Loss: 1.3621287047863007, Final Batch Loss: 0.44325709342956543\n",
      "Subject 1, Epoch 201, Loss: 1.5018831491470337, Final Batch Loss: 0.4637756645679474\n",
      "Subject 1, Epoch 202, Loss: 1.4999536573886871, Final Batch Loss: 0.45908603072166443\n",
      "Subject 1, Epoch 203, Loss: 1.5897148847579956, Final Batch Loss: 0.6002746224403381\n",
      "Subject 1, Epoch 204, Loss: 1.3976399600505829, Final Batch Loss: 0.4403184652328491\n",
      "Subject 1, Epoch 205, Loss: 1.553420603275299, Final Batch Loss: 0.5257200598716736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 206, Loss: 1.4374755918979645, Final Batch Loss: 0.5479284524917603\n",
      "Subject 1, Epoch 207, Loss: 1.3860243260860443, Final Batch Loss: 0.4030326306819916\n",
      "Subject 1, Epoch 208, Loss: 1.349877417087555, Final Batch Loss: 0.4465664327144623\n",
      "Subject 1, Epoch 209, Loss: 1.3540362119674683, Final Batch Loss: 0.4600556492805481\n",
      "Subject 1, Epoch 210, Loss: 1.3031599819660187, Final Batch Loss: 0.4335074722766876\n",
      "Subject 1, Epoch 211, Loss: 1.276383399963379, Final Batch Loss: 0.3582216203212738\n",
      "Subject 1, Epoch 212, Loss: 1.3264791369438171, Final Batch Loss: 0.45028889179229736\n",
      "Subject 1, Epoch 213, Loss: 1.3150129914283752, Final Batch Loss: 0.4440796375274658\n",
      "Subject 1, Epoch 214, Loss: 1.312125951051712, Final Batch Loss: 0.40269050002098083\n",
      "Subject 1, Epoch 215, Loss: 1.4342950880527496, Final Batch Loss: 0.3914652168750763\n",
      "Subject 1, Epoch 216, Loss: 1.4300447702407837, Final Batch Loss: 0.43668845295906067\n",
      "Subject 1, Epoch 217, Loss: 1.3736965656280518, Final Batch Loss: 0.4401932954788208\n",
      "Subject 1, Epoch 218, Loss: 1.2818126678466797, Final Batch Loss: 0.4645656645298004\n",
      "Subject 1, Epoch 219, Loss: 1.2964074313640594, Final Batch Loss: 0.4596523642539978\n",
      "Subject 1, Epoch 220, Loss: 1.3081094026565552, Final Batch Loss: 0.4150913953781128\n",
      "Subject 1, Epoch 221, Loss: 1.4112028777599335, Final Batch Loss: 0.5029863119125366\n",
      "Subject 1, Epoch 222, Loss: 1.43523770570755, Final Batch Loss: 0.40099042654037476\n",
      "Subject 1, Epoch 223, Loss: 1.3306508660316467, Final Batch Loss: 0.4660641551017761\n",
      "Subject 1, Epoch 224, Loss: 1.2329925894737244, Final Batch Loss: 0.5077349543571472\n",
      "Subject 1, Epoch 225, Loss: 1.3045404553413391, Final Batch Loss: 0.46491721272468567\n",
      "Subject 1, Epoch 226, Loss: 1.270129233598709, Final Batch Loss: 0.4131455719470978\n",
      "Subject 1, Epoch 227, Loss: 1.2525280714035034, Final Batch Loss: 0.38946422934532166\n",
      "Subject 1, Epoch 228, Loss: 1.3122478425502777, Final Batch Loss: 0.4600522816181183\n",
      "Subject 1, Epoch 229, Loss: 1.2986362278461456, Final Batch Loss: 0.3984083831310272\n",
      "Subject 1, Epoch 230, Loss: 1.293228417634964, Final Batch Loss: 0.49224650859832764\n",
      "Subject 1, Epoch 231, Loss: 1.2642850875854492, Final Batch Loss: 0.415706992149353\n",
      "Subject 1, Epoch 232, Loss: 1.233531802892685, Final Batch Loss: 0.3403337299823761\n",
      "Subject 1, Epoch 233, Loss: 1.2710569202899933, Final Batch Loss: 0.4056101143360138\n",
      "Subject 1, Epoch 234, Loss: 1.3381804525852203, Final Batch Loss: 0.3979260325431824\n",
      "Subject 1, Epoch 235, Loss: 1.220373958349228, Final Batch Loss: 0.3231460452079773\n",
      "Subject 1, Epoch 236, Loss: 1.2691884338855743, Final Batch Loss: 0.48260417580604553\n",
      "Subject 1, Epoch 237, Loss: 1.2381134331226349, Final Batch Loss: 0.3530788719654083\n",
      "Subject 1, Epoch 238, Loss: 1.2913581728935242, Final Batch Loss: 0.46291810274124146\n",
      "Subject 1, Epoch 239, Loss: 1.3202447593212128, Final Batch Loss: 0.4379362463951111\n",
      "Subject 1, Epoch 240, Loss: 1.318219542503357, Final Batch Loss: 0.4351004362106323\n",
      "Subject 1, Epoch 241, Loss: 1.2747480869293213, Final Batch Loss: 0.3944470286369324\n",
      "Subject 1, Epoch 242, Loss: 1.279345154762268, Final Batch Loss: 0.4091629385948181\n",
      "Subject 1, Epoch 243, Loss: 1.2499797642230988, Final Batch Loss: 0.5402023196220398\n",
      "Subject 1, Epoch 244, Loss: 1.2035457789897919, Final Batch Loss: 0.4024682939052582\n",
      "Subject 1, Epoch 245, Loss: 1.2922120094299316, Final Batch Loss: 0.40088117122650146\n",
      "Subject 1, Epoch 246, Loss: 1.219211608171463, Final Batch Loss: 0.3526337742805481\n",
      "Subject 1, Epoch 247, Loss: 1.178654283285141, Final Batch Loss: 0.39732274413108826\n",
      "Subject 1, Epoch 248, Loss: 1.2129059135913849, Final Batch Loss: 0.3675592839717865\n",
      "Subject 1, Epoch 249, Loss: 1.2092465162277222, Final Batch Loss: 0.39592665433883667\n",
      "Subject 1, Epoch 250, Loss: 1.1313547194004059, Final Batch Loss: 0.36577534675598145\n",
      "Subject 1, Epoch 251, Loss: 1.2287270426750183, Final Batch Loss: 0.5052272081375122\n",
      "Subject 1, Epoch 252, Loss: 1.2456711530685425, Final Batch Loss: 0.40054723620414734\n",
      "Subject 1, Epoch 253, Loss: 1.1573400795459747, Final Batch Loss: 0.41444993019104004\n",
      "Subject 1, Epoch 254, Loss: 1.2550504505634308, Final Batch Loss: 0.42601069808006287\n",
      "Subject 1, Epoch 255, Loss: 1.2305787801742554, Final Batch Loss: 0.375069260597229\n",
      "Subject 1, Epoch 256, Loss: 1.1288743019104004, Final Batch Loss: 0.32848256826400757\n",
      "Subject 1, Epoch 257, Loss: 1.1368650496006012, Final Batch Loss: 0.35148531198501587\n",
      "Subject 1, Epoch 258, Loss: 1.2265777289867401, Final Batch Loss: 0.4516575336456299\n",
      "Subject 1, Epoch 259, Loss: 1.2180431187152863, Final Batch Loss: 0.37809693813323975\n",
      "Subject 1, Epoch 260, Loss: 1.1971534490585327, Final Batch Loss: 0.37508952617645264\n",
      "Subject 1, Epoch 261, Loss: 1.1946915686130524, Final Batch Loss: 0.3625176250934601\n",
      "Subject 1, Epoch 262, Loss: 1.1832038462162018, Final Batch Loss: 0.37217026948928833\n",
      "Subject 1, Epoch 263, Loss: 1.1985716819763184, Final Batch Loss: 0.3889021575450897\n",
      "Subject 1, Epoch 264, Loss: 1.1592536270618439, Final Batch Loss: 0.3688281774520874\n",
      "Subject 1, Epoch 265, Loss: 1.3203901946544647, Final Batch Loss: 0.3569061756134033\n",
      "Subject 1, Epoch 266, Loss: 1.1659087836742401, Final Batch Loss: 0.47698986530303955\n",
      "Subject 1, Epoch 267, Loss: 1.314279168844223, Final Batch Loss: 0.4107699990272522\n",
      "Subject 1, Epoch 268, Loss: 1.2611844837665558, Final Batch Loss: 0.38989514112472534\n",
      "Subject 1, Epoch 269, Loss: 1.2067977786064148, Final Batch Loss: 0.44646716117858887\n",
      "Subject 1, Epoch 270, Loss: 1.098433017730713, Final Batch Loss: 0.28592661023139954\n",
      "Subject 1, Epoch 271, Loss: 1.2361987233161926, Final Batch Loss: 0.4103746712207794\n",
      "Subject 1, Epoch 272, Loss: 1.1912059485912323, Final Batch Loss: 0.4769958257675171\n",
      "Subject 1, Epoch 273, Loss: 1.1044383645057678, Final Batch Loss: 0.4403455853462219\n",
      "Subject 1, Epoch 274, Loss: 1.1545633971691132, Final Batch Loss: 0.38234326243400574\n",
      "Subject 1, Epoch 275, Loss: 1.1115748286247253, Final Batch Loss: 0.2982456684112549\n",
      "Subject 1, Epoch 276, Loss: 1.0902892351150513, Final Batch Loss: 0.29878535866737366\n",
      "Subject 1, Epoch 277, Loss: 1.1867772340774536, Final Batch Loss: 0.37451454997062683\n",
      "Subject 1, Epoch 278, Loss: 1.107459932565689, Final Batch Loss: 0.38702791929244995\n",
      "Subject 1, Epoch 279, Loss: 1.1073467433452606, Final Batch Loss: 0.3582492470741272\n",
      "Subject 1, Epoch 280, Loss: 1.4295112192630768, Final Batch Loss: 0.6592801809310913\n",
      "Subject 1, Epoch 281, Loss: 1.221499264240265, Final Batch Loss: 0.38069063425064087\n",
      "Subject 1, Epoch 282, Loss: 1.2137162685394287, Final Batch Loss: 0.5011742115020752\n",
      "Subject 1, Epoch 283, Loss: 1.1805381774902344, Final Batch Loss: 0.386359304189682\n",
      "Subject 1, Epoch 284, Loss: 1.1467572450637817, Final Batch Loss: 0.4254761040210724\n",
      "Subject 1, Epoch 285, Loss: 1.1370112001895905, Final Batch Loss: 0.4815176725387573\n",
      "Subject 1, Epoch 286, Loss: 1.171615332365036, Final Batch Loss: 0.32398730516433716\n",
      "Subject 1, Epoch 287, Loss: 1.0953094363212585, Final Batch Loss: 0.383335143327713\n",
      "Subject 1, Epoch 288, Loss: 1.0685829520225525, Final Batch Loss: 0.3424070477485657\n",
      "Subject 1, Epoch 289, Loss: 1.1323558688163757, Final Batch Loss: 0.3040710389614105\n",
      "Subject 1, Epoch 290, Loss: 1.0595213174819946, Final Batch Loss: 0.26667097210884094\n",
      "Subject 1, Epoch 291, Loss: 1.3456175029277802, Final Batch Loss: 0.42698514461517334\n",
      "Subject 1, Epoch 292, Loss: 1.1199772357940674, Final Batch Loss: 0.41326963901519775\n",
      "Subject 1, Epoch 293, Loss: 1.0740622580051422, Final Batch Loss: 0.38517138361930847\n",
      "Subject 1, Epoch 294, Loss: 1.1149840950965881, Final Batch Loss: 0.3766101598739624\n",
      "Subject 1, Epoch 295, Loss: 1.053059995174408, Final Batch Loss: 0.40160441398620605\n",
      "Subject 1, Epoch 296, Loss: 1.0664866268634796, Final Batch Loss: 0.372382253408432\n",
      "Subject 1, Epoch 297, Loss: 1.0828337669372559, Final Batch Loss: 0.3392668664455414\n",
      "Subject 1, Epoch 298, Loss: 1.118551880121231, Final Batch Loss: 0.33701732754707336\n",
      "Subject 1, Epoch 299, Loss: 1.1001666188240051, Final Batch Loss: 0.3936834931373596\n",
      "Subject 1, Epoch 300, Loss: 1.2047948241233826, Final Batch Loss: 0.41419267654418945\n",
      "Subject 1, Epoch 301, Loss: 1.2005087435245514, Final Batch Loss: 0.4670129716396332\n",
      "Subject 1, Epoch 302, Loss: 1.142554759979248, Final Batch Loss: 0.3374652862548828\n",
      "Subject 1, Epoch 303, Loss: 1.1815183758735657, Final Batch Loss: 0.3388043940067291\n",
      "Subject 1, Epoch 304, Loss: 1.1375232934951782, Final Batch Loss: 0.5071865916252136\n",
      "Subject 1, Epoch 305, Loss: 1.060878187417984, Final Batch Loss: 0.3889097273349762\n",
      "Subject 1, Epoch 306, Loss: 1.0763816237449646, Final Batch Loss: 0.3684013783931732\n",
      "Subject 1, Epoch 307, Loss: 1.1220307350158691, Final Batch Loss: 0.2992989718914032\n",
      "Subject 1, Epoch 308, Loss: 1.265035629272461, Final Batch Loss: 0.5462454557418823\n",
      "Subject 1, Epoch 309, Loss: 1.1676930785179138, Final Batch Loss: 0.35450732707977295\n",
      "Subject 1, Epoch 310, Loss: 1.0773270726203918, Final Batch Loss: 0.327466756105423\n",
      "Subject 1, Epoch 311, Loss: 1.1352621614933014, Final Batch Loss: 0.3707442879676819\n",
      "Subject 1, Epoch 312, Loss: 1.0322603583335876, Final Batch Loss: 0.34835347533226013\n",
      "Subject 1, Epoch 313, Loss: 1.1138422787189484, Final Batch Loss: 0.4604487121105194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 314, Loss: 1.0993821024894714, Final Batch Loss: 0.3883924186229706\n",
      "Subject 1, Epoch 315, Loss: 1.1773135364055634, Final Batch Loss: 0.4273768961429596\n",
      "Subject 1, Epoch 316, Loss: 1.0861572623252869, Final Batch Loss: 0.3822653591632843\n",
      "Subject 1, Epoch 317, Loss: 1.128375768661499, Final Batch Loss: 0.32106563448905945\n",
      "Subject 1, Epoch 318, Loss: 1.1102220118045807, Final Batch Loss: 0.3594452142715454\n",
      "Subject 1, Epoch 319, Loss: 1.122550904750824, Final Batch Loss: 0.2811776399612427\n",
      "Subject 1, Epoch 320, Loss: 1.0385470688343048, Final Batch Loss: 0.34365585446357727\n",
      "Subject 1, Epoch 321, Loss: 1.089964121580124, Final Batch Loss: 0.3322128653526306\n",
      "Subject 1, Epoch 322, Loss: 1.0461851358413696, Final Batch Loss: 0.3011283576488495\n",
      "Subject 1, Epoch 323, Loss: 1.1028059124946594, Final Batch Loss: 0.3942109942436218\n",
      "Subject 1, Epoch 324, Loss: 0.9522426724433899, Final Batch Loss: 0.3481994867324829\n",
      "Subject 1, Epoch 325, Loss: 1.126319944858551, Final Batch Loss: 0.39505672454833984\n",
      "Subject 1, Epoch 326, Loss: 1.086065948009491, Final Batch Loss: 0.36027926206588745\n",
      "Subject 1, Epoch 327, Loss: 1.1216354668140411, Final Batch Loss: 0.4312107563018799\n",
      "Subject 1, Epoch 328, Loss: 1.1343986690044403, Final Batch Loss: 0.37139052152633667\n",
      "Subject 1, Epoch 329, Loss: 1.135063350200653, Final Batch Loss: 0.3465227484703064\n",
      "Subject 1, Epoch 330, Loss: 1.0987170040607452, Final Batch Loss: 0.3951285183429718\n",
      "Subject 1, Epoch 331, Loss: 0.9693411588668823, Final Batch Loss: 0.28149738907814026\n",
      "Subject 1, Epoch 332, Loss: 1.0818861424922943, Final Batch Loss: 0.3611849546432495\n",
      "Subject 1, Epoch 333, Loss: 1.1107765436172485, Final Batch Loss: 0.3392043709754944\n",
      "Subject 1, Epoch 334, Loss: 1.059485137462616, Final Batch Loss: 0.4018401801586151\n",
      "Subject 1, Epoch 335, Loss: 1.1521017849445343, Final Batch Loss: 0.36496609449386597\n",
      "Subject 1, Epoch 336, Loss: 1.0157640874385834, Final Batch Loss: 0.39163264632225037\n",
      "Subject 1, Epoch 337, Loss: 1.0619405210018158, Final Batch Loss: 0.33374011516571045\n",
      "Subject 1, Epoch 338, Loss: 1.1184485256671906, Final Batch Loss: 0.3760197162628174\n",
      "Subject 1, Epoch 339, Loss: 1.0936528146266937, Final Batch Loss: 0.34067854285240173\n",
      "Subject 1, Epoch 340, Loss: 1.1552562713623047, Final Batch Loss: 0.39876505732536316\n",
      "Subject 1, Epoch 341, Loss: 1.0731272101402283, Final Batch Loss: 0.29499655961990356\n",
      "Subject 1, Epoch 342, Loss: 1.106567233800888, Final Batch Loss: 0.4109577536582947\n",
      "Subject 1, Epoch 343, Loss: 1.0267727375030518, Final Batch Loss: 0.3802315294742584\n",
      "Subject 1, Epoch 344, Loss: 1.0948682129383087, Final Batch Loss: 0.32965993881225586\n",
      "Subject 1, Epoch 345, Loss: 1.0242384374141693, Final Batch Loss: 0.2613558769226074\n",
      "Subject 1, Epoch 346, Loss: 1.0919195413589478, Final Batch Loss: 0.32344916462898254\n",
      "Subject 1, Epoch 347, Loss: 1.0268466174602509, Final Batch Loss: 0.3624926805496216\n",
      "Subject 1, Epoch 348, Loss: 1.1419619917869568, Final Batch Loss: 0.40403711795806885\n",
      "Subject 1, Epoch 349, Loss: 1.0670302510261536, Final Batch Loss: 0.36920633912086487\n",
      "Subject 1, Epoch 350, Loss: 1.0566194653511047, Final Batch Loss: 0.3916054666042328\n",
      "Subject 1, Epoch 351, Loss: 1.051593840122223, Final Batch Loss: 0.3385242223739624\n",
      "Subject 1, Epoch 352, Loss: 0.9526688158512115, Final Batch Loss: 0.3117160499095917\n",
      "Subject 1, Epoch 353, Loss: 1.1954958736896515, Final Batch Loss: 0.373327374458313\n",
      "Subject 1, Epoch 354, Loss: 1.1035043001174927, Final Batch Loss: 0.39765673875808716\n",
      "Subject 1, Epoch 355, Loss: 1.0677204132080078, Final Batch Loss: 0.34628570079803467\n",
      "Subject 1, Epoch 356, Loss: 1.0213507413864136, Final Batch Loss: 0.3672719895839691\n",
      "Subject 1, Epoch 357, Loss: 1.03902205824852, Final Batch Loss: 0.3226010501384735\n",
      "Subject 1, Epoch 358, Loss: 1.0706243216991425, Final Batch Loss: 0.31854405999183655\n",
      "Subject 1, Epoch 359, Loss: 1.0195414125919342, Final Batch Loss: 0.32305485010147095\n",
      "Subject 1, Epoch 360, Loss: 1.0862435102462769, Final Batch Loss: 0.3633670508861542\n",
      "Subject 1, Epoch 361, Loss: 1.131668210029602, Final Batch Loss: 0.5033275485038757\n",
      "Subject 1, Epoch 362, Loss: 1.0874160826206207, Final Batch Loss: 0.4138287305831909\n",
      "Subject 1, Epoch 363, Loss: 1.1463180780410767, Final Batch Loss: 0.492791086435318\n",
      "Subject 1, Epoch 364, Loss: 1.1083759367465973, Final Batch Loss: 0.3367888629436493\n",
      "Subject 1, Epoch 365, Loss: 0.9669366180896759, Final Batch Loss: 0.36739352345466614\n",
      "Subject 1, Epoch 366, Loss: 1.0503482818603516, Final Batch Loss: 0.3207697868347168\n",
      "Subject 1, Epoch 367, Loss: 1.0309517681598663, Final Batch Loss: 0.3656800389289856\n",
      "Subject 1, Epoch 368, Loss: 1.0111918300390244, Final Batch Loss: 0.24356378614902496\n",
      "Subject 1, Epoch 369, Loss: 0.9515988528728485, Final Batch Loss: 0.3330903649330139\n",
      "Subject 1, Epoch 370, Loss: 1.0357380211353302, Final Batch Loss: 0.34134089946746826\n",
      "Subject 1, Epoch 371, Loss: 1.1662790775299072, Final Batch Loss: 0.34437093138694763\n",
      "Subject 1, Epoch 372, Loss: 1.2584024965763092, Final Batch Loss: 0.3735774755477905\n",
      "Subject 1, Epoch 373, Loss: 1.067265659570694, Final Batch Loss: 0.4053119421005249\n",
      "Subject 1, Epoch 374, Loss: 0.9947414696216583, Final Batch Loss: 0.362748384475708\n",
      "Subject 1, Epoch 375, Loss: 0.9789237976074219, Final Batch Loss: 0.31272992491722107\n",
      "Subject 1, Epoch 376, Loss: 1.0389876961708069, Final Batch Loss: 0.3684830069541931\n",
      "Subject 1, Epoch 377, Loss: 1.0357012450695038, Final Batch Loss: 0.3347480893135071\n",
      "Subject 1, Epoch 378, Loss: 0.9535561203956604, Final Batch Loss: 0.3199693262577057\n",
      "Subject 1, Epoch 379, Loss: 1.1005160212516785, Final Batch Loss: 0.35800448060035706\n",
      "Subject 1, Epoch 380, Loss: 1.0424967110157013, Final Batch Loss: 0.38480308651924133\n",
      "Subject 1, Epoch 381, Loss: 1.0668233036994934, Final Batch Loss: 0.3136230707168579\n",
      "Subject 1, Epoch 382, Loss: 1.0889962017536163, Final Batch Loss: 0.33432647585868835\n",
      "Subject 1, Epoch 383, Loss: 0.9085171818733215, Final Batch Loss: 0.274341344833374\n",
      "Subject 1, Epoch 384, Loss: 1.0398271679878235, Final Batch Loss: 0.33117398619651794\n",
      "Subject 1, Epoch 385, Loss: 1.0092940926551819, Final Batch Loss: 0.36688533425331116\n",
      "Subject 1, Epoch 386, Loss: 0.9561763107776642, Final Batch Loss: 0.2736911475658417\n",
      "Subject 1, Epoch 387, Loss: 1.084663212299347, Final Batch Loss: 0.4196375608444214\n",
      "Subject 1, Epoch 388, Loss: 1.008048802614212, Final Batch Loss: 0.38184428215026855\n",
      "Subject 1, Epoch 389, Loss: 0.9715994596481323, Final Batch Loss: 0.27017855644226074\n",
      "Subject 1, Epoch 390, Loss: 0.9653112292289734, Final Batch Loss: 0.32717448472976685\n",
      "Subject 1, Epoch 391, Loss: 0.9579805731773376, Final Batch Loss: 0.254670113325119\n",
      "Subject 1, Epoch 392, Loss: 0.9793035089969635, Final Batch Loss: 0.273211270570755\n",
      "Subject 1, Epoch 393, Loss: 0.9419801831245422, Final Batch Loss: 0.3397638201713562\n",
      "Subject 1, Epoch 394, Loss: 1.015840619802475, Final Batch Loss: 0.3864637315273285\n",
      "Subject 1, Epoch 395, Loss: 0.9593016505241394, Final Batch Loss: 0.30916914343833923\n",
      "Subject 1, Epoch 396, Loss: 1.0187494456768036, Final Batch Loss: 0.32193082571029663\n",
      "Subject 1, Epoch 397, Loss: 0.962003618478775, Final Batch Loss: 0.315150648355484\n",
      "Subject 1, Epoch 398, Loss: 0.9561301171779633, Final Batch Loss: 0.292300820350647\n",
      "Subject 1, Epoch 399, Loss: 0.9392280876636505, Final Batch Loss: 0.25690680742263794\n",
      "Subject 1, Epoch 400, Loss: 1.0498707294464111, Final Batch Loss: 0.40672409534454346\n",
      "Subject 1, Epoch 401, Loss: 0.9978479743003845, Final Batch Loss: 0.3612579107284546\n",
      "Subject 1, Epoch 402, Loss: 0.9151133298873901, Final Batch Loss: 0.3238546550273895\n",
      "Subject 1, Epoch 403, Loss: 0.9504867494106293, Final Batch Loss: 0.3248927891254425\n",
      "Subject 1, Epoch 404, Loss: 1.0477760136127472, Final Batch Loss: 0.38754168152809143\n",
      "Subject 1, Epoch 405, Loss: 1.0024727582931519, Final Batch Loss: 0.3418489098548889\n",
      "Subject 1, Epoch 406, Loss: 1.0301659107208252, Final Batch Loss: 0.3298407196998596\n",
      "Subject 1, Epoch 407, Loss: 0.9906184524297714, Final Batch Loss: 0.2473834604024887\n",
      "Subject 1, Epoch 408, Loss: 0.9344674795866013, Final Batch Loss: 0.30844876170158386\n",
      "Subject 1, Epoch 409, Loss: 0.9468703269958496, Final Batch Loss: 0.36037373542785645\n",
      "Subject 1, Epoch 410, Loss: 1.122618019580841, Final Batch Loss: 0.2993163764476776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 411, Loss: 0.9758158028125763, Final Batch Loss: 0.24410873651504517\n",
      "Subject 1, Epoch 412, Loss: 0.9714720547199249, Final Batch Loss: 0.26832273602485657\n",
      "Subject 1, Epoch 413, Loss: 0.9357072114944458, Final Batch Loss: 0.37544310092926025\n",
      "Subject 1, Epoch 414, Loss: 0.9947986006736755, Final Batch Loss: 0.34314385056495667\n",
      "Subject 1, Epoch 415, Loss: 0.903112918138504, Final Batch Loss: 0.28833407163619995\n",
      "Subject 1, Epoch 416, Loss: 1.079580307006836, Final Batch Loss: 0.4511798918247223\n",
      "Subject 1, Epoch 417, Loss: 0.9262370765209198, Final Batch Loss: 0.2556036412715912\n",
      "Subject 1, Epoch 418, Loss: 0.9061089605093002, Final Batch Loss: 0.2887895703315735\n",
      "Subject 1, Epoch 419, Loss: 0.8711024522781372, Final Batch Loss: 0.2848840057849884\n",
      "Subject 1, Epoch 420, Loss: 0.98353710770607, Final Batch Loss: 0.27983975410461426\n",
      "Subject 1, Epoch 421, Loss: 0.9959540665149689, Final Batch Loss: 0.3902650773525238\n",
      "Subject 1, Epoch 422, Loss: 1.028052568435669, Final Batch Loss: 0.28746965527534485\n",
      "Subject 1, Epoch 423, Loss: 0.8840382397174835, Final Batch Loss: 0.2560046315193176\n",
      "Subject 1, Epoch 424, Loss: 1.0908228754997253, Final Batch Loss: 0.3495733141899109\n",
      "Subject 1, Epoch 425, Loss: 0.8583659827709198, Final Batch Loss: 0.29645583033561707\n",
      "Subject 1, Epoch 426, Loss: 0.9410913586616516, Final Batch Loss: 0.337772935628891\n",
      "Subject 1, Epoch 427, Loss: 0.9008331298828125, Final Batch Loss: 0.2210269272327423\n",
      "Subject 1, Epoch 428, Loss: 0.9717283546924591, Final Batch Loss: 0.34000903367996216\n",
      "Subject 1, Epoch 429, Loss: 0.9164301156997681, Final Batch Loss: 0.30732396245002747\n",
      "Subject 1, Epoch 430, Loss: 0.9605695903301239, Final Batch Loss: 0.30189964175224304\n",
      "Subject 1, Epoch 431, Loss: 0.9989768862724304, Final Batch Loss: 0.26399752497673035\n",
      "Subject 1, Epoch 432, Loss: 1.0582878589630127, Final Batch Loss: 0.3518753945827484\n",
      "Subject 1, Epoch 433, Loss: 0.9358848333358765, Final Batch Loss: 0.27475863695144653\n",
      "Subject 1, Epoch 434, Loss: 0.8453192412853241, Final Batch Loss: 0.30107516050338745\n",
      "Subject 1, Epoch 435, Loss: 1.0572030544281006, Final Batch Loss: 0.2575186491012573\n",
      "Subject 1, Epoch 436, Loss: 0.9299617409706116, Final Batch Loss: 0.33606693148612976\n",
      "Subject 1, Epoch 437, Loss: 0.9862676858901978, Final Batch Loss: 0.35895276069641113\n",
      "Subject 1, Epoch 438, Loss: 0.9552477598190308, Final Batch Loss: 0.34865257143974304\n",
      "Subject 1, Epoch 439, Loss: 0.8772276192903519, Final Batch Loss: 0.23542501032352448\n",
      "Subject 1, Epoch 440, Loss: 0.9283903539180756, Final Batch Loss: 0.3091323673725128\n",
      "Subject 1, Epoch 441, Loss: 0.9895598590373993, Final Batch Loss: 0.3896871507167816\n",
      "Subject 1, Epoch 442, Loss: 0.9132795482873917, Final Batch Loss: 0.24478976428508759\n",
      "Subject 1, Epoch 443, Loss: 0.8453947305679321, Final Batch Loss: 0.2112889289855957\n",
      "Subject 1, Epoch 444, Loss: 0.9397478997707367, Final Batch Loss: 0.3198578655719757\n",
      "Subject 1, Epoch 445, Loss: 1.0144653618335724, Final Batch Loss: 0.25345802307128906\n",
      "Subject 1, Epoch 446, Loss: 0.9242240190505981, Final Batch Loss: 0.304332971572876\n",
      "Subject 1, Epoch 447, Loss: 0.8494845926761627, Final Batch Loss: 0.26448535919189453\n",
      "Subject 1, Epoch 448, Loss: 0.933557778596878, Final Batch Loss: 0.25042179226875305\n",
      "Subject 1, Epoch 449, Loss: 0.9016885757446289, Final Batch Loss: 0.34578239917755127\n",
      "Subject 1, Epoch 450, Loss: 0.969680592417717, Final Batch Loss: 0.2103232890367508\n",
      "Subject 1, Epoch 451, Loss: 0.8696431517601013, Final Batch Loss: 0.3089461624622345\n",
      "Subject 1, Epoch 452, Loss: 0.984018474817276, Final Batch Loss: 0.3589116632938385\n",
      "Subject 1, Epoch 453, Loss: 0.9059090912342072, Final Batch Loss: 0.31772810220718384\n",
      "Subject 1, Epoch 454, Loss: 0.8468768298625946, Final Batch Loss: 0.22249960899353027\n",
      "Subject 1, Epoch 455, Loss: 0.990588515996933, Final Batch Loss: 0.3132334351539612\n",
      "Subject 1, Epoch 456, Loss: 0.9670721590518951, Final Batch Loss: 0.3981281518936157\n",
      "Subject 1, Epoch 457, Loss: 0.9332244843244553, Final Batch Loss: 0.39753496646881104\n",
      "Subject 1, Epoch 458, Loss: 1.000204861164093, Final Batch Loss: 0.31023579835891724\n",
      "Subject 1, Epoch 459, Loss: 0.9186814427375793, Final Batch Loss: 0.28955644369125366\n",
      "Subject 1, Epoch 460, Loss: 0.9105885028839111, Final Batch Loss: 0.2805963456630707\n",
      "Subject 1, Epoch 461, Loss: 0.9064294397830963, Final Batch Loss: 0.3852979242801666\n",
      "Subject 1, Epoch 462, Loss: 0.9591725766658783, Final Batch Loss: 0.3374757766723633\n",
      "Subject 1, Epoch 463, Loss: 1.0012874603271484, Final Batch Loss: 0.3545510172843933\n",
      "Subject 1, Epoch 464, Loss: 0.9681151807308197, Final Batch Loss: 0.27212369441986084\n",
      "Subject 1, Epoch 465, Loss: 1.0302103757858276, Final Batch Loss: 0.29768282175064087\n",
      "Subject 1, Epoch 466, Loss: 1.039406806230545, Final Batch Loss: 0.2717677354812622\n",
      "Subject 1, Epoch 467, Loss: 0.8451743125915527, Final Batch Loss: 0.2598824203014374\n",
      "Subject 1, Epoch 468, Loss: 0.752352237701416, Final Batch Loss: 0.28139302134513855\n",
      "Subject 1, Epoch 469, Loss: 1.043902426958084, Final Batch Loss: 0.3045618236064911\n",
      "Subject 1, Epoch 470, Loss: 0.8629106879234314, Final Batch Loss: 0.3721170425415039\n",
      "Subject 1, Epoch 471, Loss: 0.9728401601314545, Final Batch Loss: 0.2758304178714752\n",
      "Subject 1, Epoch 472, Loss: 0.9124458134174347, Final Batch Loss: 0.21241194009780884\n",
      "Subject 1, Epoch 473, Loss: 0.9538398385047913, Final Batch Loss: 0.3411238491535187\n",
      "Subject 1, Epoch 474, Loss: 0.8978866934776306, Final Batch Loss: 0.305530846118927\n",
      "Subject 1, Epoch 475, Loss: 0.9423370361328125, Final Batch Loss: 0.29170340299606323\n",
      "Subject 1, Epoch 476, Loss: 0.9281714260578156, Final Batch Loss: 0.26525330543518066\n",
      "Subject 1, Epoch 477, Loss: 0.8346541821956635, Final Batch Loss: 0.3177526593208313\n",
      "Subject 1, Epoch 478, Loss: 0.9441214054822922, Final Batch Loss: 0.2095700353384018\n",
      "Subject 1, Epoch 479, Loss: 0.9065887331962585, Final Batch Loss: 0.24799776077270508\n",
      "Subject 1, Epoch 480, Loss: 0.8407689481973648, Final Batch Loss: 0.2658757269382477\n",
      "Subject 1, Epoch 481, Loss: 0.9656873643398285, Final Batch Loss: 0.3031035363674164\n",
      "Subject 1, Epoch 482, Loss: 0.926786869764328, Final Batch Loss: 0.30448004603385925\n",
      "Subject 1, Epoch 483, Loss: 0.8967355489730835, Final Batch Loss: 0.33747273683547974\n",
      "Subject 1, Epoch 484, Loss: 0.8944668769836426, Final Batch Loss: 0.3280857801437378\n",
      "Subject 1, Epoch 485, Loss: 1.0372395813465118, Final Batch Loss: 0.2731153070926666\n",
      "Subject 1, Epoch 486, Loss: 0.8070424199104309, Final Batch Loss: 0.2668459415435791\n",
      "Subject 1, Epoch 487, Loss: 0.8946735858917236, Final Batch Loss: 0.26172810792922974\n",
      "Subject 1, Epoch 488, Loss: 0.8480730354785919, Final Batch Loss: 0.27264535427093506\n",
      "Subject 1, Epoch 489, Loss: 0.8935694098472595, Final Batch Loss: 0.33374401926994324\n",
      "Subject 1, Epoch 490, Loss: 0.9014557003974915, Final Batch Loss: 0.3048824965953827\n",
      "Subject 1, Epoch 491, Loss: 0.9285275489091873, Final Batch Loss: 0.3351699411869049\n",
      "Subject 1, Epoch 492, Loss: 0.867125615477562, Final Batch Loss: 0.21371518075466156\n",
      "Subject 1, Epoch 493, Loss: 0.8633238524198532, Final Batch Loss: 0.29925429821014404\n",
      "Subject 1, Epoch 494, Loss: 0.8414665162563324, Final Batch Loss: 0.29833194613456726\n",
      "Subject 1, Epoch 495, Loss: 0.9375450015068054, Final Batch Loss: 0.35504451394081116\n",
      "Subject 1, Epoch 496, Loss: 1.00470831990242, Final Batch Loss: 0.3137505054473877\n",
      "Subject 1, Epoch 497, Loss: 0.8793157786130905, Final Batch Loss: 0.2936510145664215\n",
      "Subject 1, Epoch 498, Loss: 0.9934490919113159, Final Batch Loss: 0.3809276223182678\n",
      "Subject 1, Epoch 499, Loss: 0.9595131278038025, Final Batch Loss: 0.3314127027988434\n",
      "Subject 1, Epoch 500, Loss: 0.9381386339664459, Final Batch Loss: 0.2565930485725403\n",
      "Subject 1, Epoch 501, Loss: 0.8377321064472198, Final Batch Loss: 0.2994880676269531\n",
      "Subject 1, Epoch 502, Loss: 0.8694935739040375, Final Batch Loss: 0.30638331174850464\n",
      "Subject 1, Epoch 503, Loss: 0.8965942859649658, Final Batch Loss: 0.37651756405830383\n",
      "Subject 1, Epoch 504, Loss: 0.9270019680261612, Final Batch Loss: 0.24200214445590973\n",
      "Subject 1, Epoch 505, Loss: 0.8567809611558914, Final Batch Loss: 0.2583213746547699\n",
      "Subject 1, Epoch 506, Loss: 0.864336222410202, Final Batch Loss: 0.3461266756057739\n",
      "Subject 1, Epoch 507, Loss: 0.943429172039032, Final Batch Loss: 0.3249690532684326\n",
      "Subject 1, Epoch 508, Loss: 0.8721552789211273, Final Batch Loss: 0.3284780979156494\n",
      "Subject 1, Epoch 509, Loss: 1.014712542295456, Final Batch Loss: 0.3974590599536896\n",
      "Subject 1, Epoch 510, Loss: 0.8984058946371078, Final Batch Loss: 0.24933458864688873\n",
      "Subject 1, Epoch 511, Loss: 0.8683505356311798, Final Batch Loss: 0.3148443102836609\n",
      "Subject 1, Epoch 512, Loss: 0.8338329344987869, Final Batch Loss: 0.23837606608867645\n",
      "Subject 1, Epoch 513, Loss: 0.8657686859369278, Final Batch Loss: 0.30018848180770874\n",
      "Subject 1, Epoch 514, Loss: 0.8654908835887909, Final Batch Loss: 0.3549484610557556\n",
      "Subject 1, Epoch 515, Loss: 0.8176928013563156, Final Batch Loss: 0.2238149493932724\n",
      "Subject 1, Epoch 516, Loss: 0.8843662440776825, Final Batch Loss: 0.31525176763534546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 517, Loss: 0.8607899844646454, Final Batch Loss: 0.3002053201198578\n",
      "Subject 1, Epoch 518, Loss: 0.8372509479522705, Final Batch Loss: 0.28746965527534485\n",
      "Subject 1, Epoch 519, Loss: 0.9045746177434921, Final Batch Loss: 0.2173881083726883\n",
      "Subject 1, Epoch 520, Loss: 0.9103992581367493, Final Batch Loss: 0.36081501841545105\n",
      "Subject 1, Epoch 521, Loss: 0.9058856666088104, Final Batch Loss: 0.3071111738681793\n",
      "Subject 1, Epoch 522, Loss: 0.862388551235199, Final Batch Loss: 0.34553593397140503\n",
      "Subject 1, Epoch 523, Loss: 0.855292871594429, Final Batch Loss: 0.24177218973636627\n",
      "Subject 1, Epoch 524, Loss: 0.8406264185905457, Final Batch Loss: 0.3226848244667053\n",
      "Subject 1, Epoch 525, Loss: 0.9215940684080124, Final Batch Loss: 0.3663797378540039\n",
      "Subject 1, Epoch 526, Loss: 0.8266202509403229, Final Batch Loss: 0.2582968473434448\n",
      "Subject 1, Epoch 527, Loss: 0.9614047706127167, Final Batch Loss: 0.3433842658996582\n",
      "Subject 1, Epoch 528, Loss: 0.7589966505765915, Final Batch Loss: 0.2800644040107727\n",
      "Subject 1, Epoch 529, Loss: 0.7674276530742645, Final Batch Loss: 0.25281429290771484\n",
      "Subject 1, Epoch 530, Loss: 0.8947781026363373, Final Batch Loss: 0.23045837879180908\n",
      "Subject 1, Epoch 531, Loss: 0.8545070886611938, Final Batch Loss: 0.19931694865226746\n",
      "Subject 1, Epoch 532, Loss: 0.9069514721632004, Final Batch Loss: 0.2145257145166397\n",
      "Subject 1, Epoch 533, Loss: 0.9499140083789825, Final Batch Loss: 0.289543479681015\n",
      "Subject 1, Epoch 534, Loss: 0.8248632997274399, Final Batch Loss: 0.3345193564891815\n",
      "Subject 1, Epoch 535, Loss: 0.7578990459442139, Final Batch Loss: 0.2558661997318268\n",
      "Subject 1, Epoch 536, Loss: 0.9230460822582245, Final Batch Loss: 0.26314619183540344\n",
      "Subject 1, Epoch 537, Loss: 0.8222259283065796, Final Batch Loss: 0.24246302247047424\n",
      "Subject 1, Epoch 538, Loss: 0.8584210276603699, Final Batch Loss: 0.2851339876651764\n",
      "Subject 1, Epoch 539, Loss: 0.7472632378339767, Final Batch Loss: 0.22254423797130585\n",
      "Subject 1, Epoch 540, Loss: 0.7990260571241379, Final Batch Loss: 0.2944317162036896\n",
      "Subject 1, Epoch 541, Loss: 0.9466460347175598, Final Batch Loss: 0.45444703102111816\n",
      "Subject 1, Epoch 542, Loss: 0.7446511834859848, Final Batch Loss: 0.15492676198482513\n",
      "Subject 1, Epoch 543, Loss: 0.8258448243141174, Final Batch Loss: 0.21017271280288696\n",
      "Subject 1, Epoch 544, Loss: 0.8564432263374329, Final Batch Loss: 0.22803154587745667\n",
      "Subject 1, Epoch 545, Loss: 0.728715717792511, Final Batch Loss: 0.26443347334861755\n",
      "Subject 1, Epoch 546, Loss: 0.8509159982204437, Final Batch Loss: 0.332343190908432\n",
      "Subject 1, Epoch 547, Loss: 0.8117320835590363, Final Batch Loss: 0.3382948040962219\n",
      "Subject 1, Epoch 548, Loss: 0.8955428153276443, Final Batch Loss: 0.36872628331184387\n",
      "Subject 1, Epoch 549, Loss: 0.7586348354816437, Final Batch Loss: 0.2574232816696167\n",
      "Subject 1, Epoch 550, Loss: 0.8640209138393402, Final Batch Loss: 0.2758938670158386\n",
      "Subject 1, Epoch 551, Loss: 0.7111834585666656, Final Batch Loss: 0.2285274714231491\n",
      "Subject 1, Epoch 552, Loss: 0.7344165146350861, Final Batch Loss: 0.23226213455200195\n",
      "Subject 1, Epoch 553, Loss: 0.8027651757001877, Final Batch Loss: 0.17321114242076874\n",
      "Subject 1, Epoch 554, Loss: 0.7177295088768005, Final Batch Loss: 0.2432921975851059\n",
      "Subject 1, Epoch 555, Loss: 0.7676744163036346, Final Batch Loss: 0.2149101197719574\n",
      "Subject 1, Epoch 556, Loss: 0.764921098947525, Final Batch Loss: 0.27050232887268066\n",
      "Subject 1, Epoch 557, Loss: 0.8348158001899719, Final Batch Loss: 0.3373306393623352\n",
      "Subject 1, Epoch 558, Loss: 0.7748720943927765, Final Batch Loss: 0.2803584039211273\n",
      "Subject 1, Epoch 559, Loss: 0.7853788137435913, Final Batch Loss: 0.2601720690727234\n",
      "Subject 1, Epoch 560, Loss: 0.8146435022354126, Final Batch Loss: 0.2988509237766266\n",
      "Subject 1, Epoch 561, Loss: 0.7670898735523224, Final Batch Loss: 0.22732748091220856\n",
      "Subject 1, Epoch 562, Loss: 0.7243402749300003, Final Batch Loss: 0.20429503917694092\n",
      "Subject 1, Epoch 563, Loss: 0.7774974703788757, Final Batch Loss: 0.31518927216529846\n",
      "Subject 1, Epoch 564, Loss: 0.9358603805303574, Final Batch Loss: 0.19768591225147247\n",
      "Subject 1, Epoch 565, Loss: 0.7953366339206696, Final Batch Loss: 0.32805177569389343\n",
      "Subject 1, Epoch 566, Loss: 0.8328595757484436, Final Batch Loss: 0.3234328031539917\n",
      "Subject 1, Epoch 567, Loss: 0.856522411108017, Final Batch Loss: 0.28310656547546387\n",
      "Subject 1, Epoch 568, Loss: 0.8551703244447708, Final Batch Loss: 0.36407458782196045\n",
      "Subject 1, Epoch 569, Loss: 0.807023212313652, Final Batch Loss: 0.2292674034833908\n",
      "Subject 1, Epoch 570, Loss: 0.9469418525695801, Final Batch Loss: 0.34488025307655334\n",
      "Subject 1, Epoch 571, Loss: 0.8294191360473633, Final Batch Loss: 0.29306137561798096\n",
      "Subject 1, Epoch 572, Loss: 0.8410096168518066, Final Batch Loss: 0.3305301368236542\n",
      "Subject 1, Epoch 573, Loss: 0.7458045333623886, Final Batch Loss: 0.28263649344444275\n",
      "Subject 1, Epoch 574, Loss: 0.8720453977584839, Final Batch Loss: 0.2783406674861908\n",
      "Subject 1, Epoch 575, Loss: 0.7587847411632538, Final Batch Loss: 0.26976335048675537\n",
      "Subject 1, Epoch 576, Loss: 0.774572104215622, Final Batch Loss: 0.22413820028305054\n",
      "Subject 1, Epoch 577, Loss: 0.8642735332250595, Final Batch Loss: 0.31549346446990967\n",
      "Subject 1, Epoch 578, Loss: 0.8413283079862595, Final Batch Loss: 0.2231513410806656\n",
      "Subject 1, Epoch 579, Loss: 0.8550220131874084, Final Batch Loss: 0.24866119027137756\n",
      "Subject 1, Epoch 580, Loss: 0.8119490444660187, Final Batch Loss: 0.3326471149921417\n",
      "Subject 1, Epoch 581, Loss: 0.9331159591674805, Final Batch Loss: 0.34099528193473816\n",
      "Subject 1, Epoch 582, Loss: 0.7729854583740234, Final Batch Loss: 0.32382458448410034\n",
      "Subject 1, Epoch 583, Loss: 0.7582185119390488, Final Batch Loss: 0.2213943600654602\n",
      "Subject 1, Epoch 584, Loss: 0.8298625499010086, Final Batch Loss: 0.2883065938949585\n",
      "Subject 1, Epoch 585, Loss: 0.768550843000412, Final Batch Loss: 0.26117053627967834\n",
      "Subject 1, Epoch 586, Loss: 0.8279459029436111, Final Batch Loss: 0.3634597659111023\n",
      "Subject 1, Epoch 587, Loss: 0.7929941713809967, Final Batch Loss: 0.2501239478588104\n",
      "Subject 1, Epoch 588, Loss: 0.773445338010788, Final Batch Loss: 0.16982561349868774\n",
      "Subject 1, Epoch 589, Loss: 0.7968664169311523, Final Batch Loss: 0.23903366923332214\n",
      "Subject 1, Epoch 590, Loss: 0.685901939868927, Final Batch Loss: 0.23165945708751678\n",
      "Subject 1, Epoch 591, Loss: 0.8460717499256134, Final Batch Loss: 0.33861491084098816\n",
      "Subject 1, Epoch 592, Loss: 0.8291066437959671, Final Batch Loss: 0.2834108769893646\n",
      "Subject 1, Epoch 593, Loss: 0.8707272410392761, Final Batch Loss: 0.20403975248336792\n",
      "Subject 1, Epoch 594, Loss: 0.7695868015289307, Final Batch Loss: 0.22403477132320404\n",
      "Subject 1, Epoch 595, Loss: 0.7528620958328247, Final Batch Loss: 0.24519851803779602\n",
      "Subject 1, Epoch 596, Loss: 0.8588041365146637, Final Batch Loss: 0.22776520252227783\n",
      "Subject 1, Epoch 597, Loss: 0.7425448298454285, Final Batch Loss: 0.2921160161495209\n",
      "Subject 1, Epoch 598, Loss: 0.8434420078992844, Final Batch Loss: 0.3249475359916687\n",
      "Subject 1, Epoch 599, Loss: 0.7544519454240799, Final Batch Loss: 0.2530645430088043\n",
      "Subject 1, Epoch 600, Loss: 0.8150752931833267, Final Batch Loss: 0.25950494408607483\n",
      "Subject 1, Epoch 601, Loss: 0.8671147525310516, Final Batch Loss: 0.39109715819358826\n",
      "Subject 1, Epoch 602, Loss: 0.7678495645523071, Final Batch Loss: 0.29880306124687195\n",
      "Subject 1, Epoch 603, Loss: 0.7349919378757477, Final Batch Loss: 0.30812975764274597\n",
      "Subject 1, Epoch 604, Loss: 0.8681372702121735, Final Batch Loss: 0.37284550070762634\n",
      "Subject 1, Epoch 605, Loss: 0.7735835015773773, Final Batch Loss: 0.15627096593379974\n",
      "Subject 1, Epoch 606, Loss: 0.8267527222633362, Final Batch Loss: 0.2646874785423279\n",
      "Subject 1, Epoch 607, Loss: 0.8224333822727203, Final Batch Loss: 0.2669311463832855\n",
      "Subject 1, Epoch 608, Loss: 0.7557005882263184, Final Batch Loss: 0.296786367893219\n",
      "Subject 1, Epoch 609, Loss: 0.930042028427124, Final Batch Loss: 0.3739844858646393\n",
      "Subject 1, Epoch 610, Loss: 0.8093969076871872, Final Batch Loss: 0.2528263330459595\n",
      "Subject 1, Epoch 611, Loss: 0.8883966654539108, Final Batch Loss: 0.4848238527774811\n",
      "Subject 1, Epoch 612, Loss: 0.8340369760990143, Final Batch Loss: 0.27362293004989624\n",
      "Subject 1, Epoch 613, Loss: 0.9191767871379852, Final Batch Loss: 0.31756392121315\n",
      "Subject 1, Epoch 614, Loss: 0.8263812810182571, Final Batch Loss: 0.26903772354125977\n",
      "Subject 1, Epoch 615, Loss: 0.7105261832475662, Final Batch Loss: 0.24516022205352783\n",
      "Subject 1, Epoch 616, Loss: 0.7066510170698166, Final Batch Loss: 0.27480795979499817\n",
      "Subject 1, Epoch 617, Loss: 0.7904677093029022, Final Batch Loss: 0.2570103108882904\n",
      "Subject 1, Epoch 618, Loss: 0.7744203209877014, Final Batch Loss: 0.24041059613227844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 619, Loss: 0.7311164885759354, Final Batch Loss: 0.19921602308750153\n",
      "Subject 1, Epoch 620, Loss: 0.7733578830957413, Final Batch Loss: 0.3427627682685852\n",
      "Subject 1, Epoch 621, Loss: 0.7109912186861038, Final Batch Loss: 0.19199277460575104\n",
      "Subject 1, Epoch 622, Loss: 0.7820111513137817, Final Batch Loss: 0.2010221779346466\n",
      "Subject 1, Epoch 623, Loss: 0.8042416870594025, Final Batch Loss: 0.18205338716506958\n",
      "Subject 1, Epoch 624, Loss: 0.8202661424875259, Final Batch Loss: 0.3104991912841797\n",
      "Subject 1, Epoch 625, Loss: 0.7879376262426376, Final Batch Loss: 0.29974016547203064\n",
      "Subject 1, Epoch 626, Loss: 0.8169586062431335, Final Batch Loss: 0.1788390874862671\n",
      "Subject 1, Epoch 627, Loss: 0.8465453833341599, Final Batch Loss: 0.3808777630329132\n",
      "Subject 1, Epoch 628, Loss: 0.7291104942560196, Final Batch Loss: 0.1769777089357376\n",
      "Subject 1, Epoch 629, Loss: 0.7463525086641312, Final Batch Loss: 0.2857704758644104\n",
      "Subject 1, Epoch 630, Loss: 0.7569478005170822, Final Batch Loss: 0.22994403541088104\n",
      "Subject 1, Epoch 631, Loss: 0.7986140549182892, Final Batch Loss: 0.3272954523563385\n",
      "Subject 1, Epoch 632, Loss: 0.79283607006073, Final Batch Loss: 0.34122586250305176\n",
      "Subject 1, Epoch 633, Loss: 0.7170270830392838, Final Batch Loss: 0.240579754114151\n",
      "Subject 1, Epoch 634, Loss: 0.757652536034584, Final Batch Loss: 0.23483571410179138\n",
      "Subject 1, Epoch 635, Loss: 0.7153437584638596, Final Batch Loss: 0.2733041048049927\n",
      "Subject 1, Epoch 636, Loss: 0.6856047809123993, Final Batch Loss: 0.21730554103851318\n",
      "Subject 1, Epoch 637, Loss: 0.7562792897224426, Final Batch Loss: 0.3119982182979584\n",
      "Subject 1, Epoch 638, Loss: 0.8536507785320282, Final Batch Loss: 0.29255032539367676\n",
      "Subject 1, Epoch 639, Loss: 0.7716651111841202, Final Batch Loss: 0.25058382749557495\n",
      "Subject 1, Epoch 640, Loss: 0.8315489590167999, Final Batch Loss: 0.27878451347351074\n",
      "Subject 1, Epoch 641, Loss: 0.9026595652103424, Final Batch Loss: 0.3639838695526123\n",
      "Subject 1, Epoch 642, Loss: 0.7420815825462341, Final Batch Loss: 0.3156413733959198\n",
      "Subject 1, Epoch 643, Loss: 0.8373894691467285, Final Batch Loss: 0.30213698744773865\n",
      "Subject 1, Epoch 644, Loss: 0.7431716173887253, Final Batch Loss: 0.2654334604740143\n",
      "Subject 1, Epoch 645, Loss: 0.7055661380290985, Final Batch Loss: 0.1316388100385666\n",
      "Subject 1, Epoch 646, Loss: 0.8332424759864807, Final Batch Loss: 0.2766532599925995\n",
      "Subject 1, Epoch 647, Loss: 0.7722202837467194, Final Batch Loss: 0.19455459713935852\n",
      "Subject 1, Epoch 648, Loss: 0.7328514456748962, Final Batch Loss: 0.25526365637779236\n",
      "Subject 1, Epoch 649, Loss: 0.8260407894849777, Final Batch Loss: 0.21166928112506866\n",
      "Subject 1, Epoch 650, Loss: 0.8127536177635193, Final Batch Loss: 0.32859423756599426\n",
      "Subject 1, Epoch 651, Loss: 0.7243591248989105, Final Batch Loss: 0.26080724596977234\n",
      "Subject 1, Epoch 652, Loss: 0.8298336267471313, Final Batch Loss: 0.2693370580673218\n",
      "Subject 1, Epoch 653, Loss: 0.8182922303676605, Final Batch Loss: 0.25122442841529846\n",
      "Subject 1, Epoch 654, Loss: 0.8051818013191223, Final Batch Loss: 0.25310200452804565\n",
      "Subject 1, Epoch 655, Loss: 0.7812753915786743, Final Batch Loss: 0.2486179918050766\n",
      "Subject 1, Epoch 656, Loss: 0.8381282091140747, Final Batch Loss: 0.19582891464233398\n",
      "Subject 1, Epoch 657, Loss: 0.739143431186676, Final Batch Loss: 0.309427946805954\n",
      "Subject 1, Epoch 658, Loss: 0.7628286480903625, Final Batch Loss: 0.3301226794719696\n",
      "Subject 1, Epoch 659, Loss: 0.7246982753276825, Final Batch Loss: 0.1569712609052658\n",
      "Subject 1, Epoch 660, Loss: 0.6955394148826599, Final Batch Loss: 0.29342952370643616\n",
      "Subject 1, Epoch 661, Loss: 0.8909002095460892, Final Batch Loss: 0.23288120329380035\n",
      "Subject 1, Epoch 662, Loss: 0.7194607257843018, Final Batch Loss: 0.2196137011051178\n",
      "Subject 1, Epoch 663, Loss: 0.7138992995023727, Final Batch Loss: 0.23394864797592163\n",
      "Subject 1, Epoch 664, Loss: 0.6571561992168427, Final Batch Loss: 0.21866166591644287\n",
      "Subject 1, Epoch 665, Loss: 0.7127374410629272, Final Batch Loss: 0.23298713564872742\n",
      "Subject 1, Epoch 666, Loss: 0.7671490609645844, Final Batch Loss: 0.16435950994491577\n",
      "Subject 1, Epoch 667, Loss: 0.8773049712181091, Final Batch Loss: 0.3416222631931305\n",
      "Subject 1, Epoch 668, Loss: 0.6899171471595764, Final Batch Loss: 0.18748338520526886\n",
      "Subject 1, Epoch 669, Loss: 0.8419171869754791, Final Batch Loss: 0.2884834110736847\n",
      "Subject 1, Epoch 670, Loss: 0.6496377289295197, Final Batch Loss: 0.25135913491249084\n",
      "Subject 1, Epoch 671, Loss: 0.8130460977554321, Final Batch Loss: 0.3379012942314148\n",
      "Subject 1, Epoch 672, Loss: 0.7506707906723022, Final Batch Loss: 0.2145536243915558\n",
      "Subject 1, Epoch 673, Loss: 0.6560954749584198, Final Batch Loss: 0.2062402218580246\n",
      "Subject 1, Epoch 674, Loss: 0.6912342756986618, Final Batch Loss: 0.1882995218038559\n",
      "Subject 1, Epoch 675, Loss: 0.7260445356369019, Final Batch Loss: 0.17074701189994812\n",
      "Subject 1, Epoch 676, Loss: 0.7342557907104492, Final Batch Loss: 0.22679618000984192\n",
      "Subject 1, Epoch 677, Loss: 0.8041922450065613, Final Batch Loss: 0.2914329171180725\n",
      "Subject 1, Epoch 678, Loss: 0.7557680159807205, Final Batch Loss: 0.30947378277778625\n",
      "Subject 1, Epoch 679, Loss: 0.6293207854032516, Final Batch Loss: 0.2216905951499939\n",
      "Subject 1, Epoch 680, Loss: 0.7817012816667557, Final Batch Loss: 0.23841246962547302\n",
      "Subject 1, Epoch 681, Loss: 0.7674751579761505, Final Batch Loss: 0.29539012908935547\n",
      "Subject 1, Epoch 682, Loss: 0.8286716639995575, Final Batch Loss: 0.2613165080547333\n",
      "Subject 1, Epoch 683, Loss: 0.6634880602359772, Final Batch Loss: 0.2364167422056198\n",
      "Subject 1, Epoch 684, Loss: 0.7351678758859634, Final Batch Loss: 0.2383289784193039\n",
      "Subject 1, Epoch 685, Loss: 0.7077674567699432, Final Batch Loss: 0.21633124351501465\n",
      "Subject 1, Epoch 686, Loss: 0.7153677642345428, Final Batch Loss: 0.30771246552467346\n",
      "Subject 1, Epoch 687, Loss: 0.7253036201000214, Final Batch Loss: 0.2657449543476105\n",
      "Subject 1, Epoch 688, Loss: 0.7785615921020508, Final Batch Loss: 0.22456441819667816\n",
      "Subject 1, Epoch 689, Loss: 0.696237713098526, Final Batch Loss: 0.23849713802337646\n",
      "Subject 1, Epoch 690, Loss: 0.7677209973335266, Final Batch Loss: 0.34253156185150146\n",
      "Subject 1, Epoch 691, Loss: 0.6481563448905945, Final Batch Loss: 0.24608400464057922\n",
      "Subject 1, Epoch 692, Loss: 0.7443685084581375, Final Batch Loss: 0.30578166246414185\n",
      "Subject 1, Epoch 693, Loss: 0.7607143670320511, Final Batch Loss: 0.2832772433757782\n",
      "Subject 1, Epoch 694, Loss: 0.8050433993339539, Final Batch Loss: 0.2807499170303345\n",
      "Subject 1, Epoch 695, Loss: 0.7512773871421814, Final Batch Loss: 0.253833532333374\n",
      "Subject 1, Epoch 696, Loss: 0.734016478061676, Final Batch Loss: 0.2501940429210663\n",
      "Subject 1, Epoch 697, Loss: 0.6409875005483627, Final Batch Loss: 0.20374459028244019\n",
      "Subject 1, Epoch 698, Loss: 0.674486368894577, Final Batch Loss: 0.24710607528686523\n",
      "Subject 1, Epoch 699, Loss: 0.5986267626285553, Final Batch Loss: 0.2890438437461853\n",
      "Subject 1, Epoch 700, Loss: 0.7510484457015991, Final Batch Loss: 0.24746116995811462\n",
      "Subject 1, Epoch 701, Loss: 0.7307383418083191, Final Batch Loss: 0.24839812517166138\n",
      "Subject 1, Epoch 702, Loss: 0.7061584442853928, Final Batch Loss: 0.23157373070716858\n",
      "Subject 1, Epoch 703, Loss: 0.7489626854658127, Final Batch Loss: 0.26610058546066284\n",
      "Subject 1, Epoch 704, Loss: 0.6447238624095917, Final Batch Loss: 0.2666955888271332\n",
      "Subject 1, Epoch 705, Loss: 0.6490112990140915, Final Batch Loss: 0.17839832603931427\n",
      "Subject 1, Epoch 706, Loss: 0.7869149148464203, Final Batch Loss: 0.1790178120136261\n",
      "Subject 1, Epoch 707, Loss: 0.702221542596817, Final Batch Loss: 0.24042589962482452\n",
      "Subject 1, Epoch 708, Loss: 0.6374452859163284, Final Batch Loss: 0.27516570687294006\n",
      "Subject 1, Epoch 709, Loss: 0.7135945558547974, Final Batch Loss: 0.2281302809715271\n",
      "Subject 1, Epoch 710, Loss: 0.6066981703042984, Final Batch Loss: 0.2415214478969574\n",
      "Subject 1, Epoch 711, Loss: 0.7217775285243988, Final Batch Loss: 0.2756749391555786\n",
      "Subject 1, Epoch 712, Loss: 0.6857056468725204, Final Batch Loss: 0.21556638181209564\n",
      "Subject 1, Epoch 713, Loss: 0.5885849595069885, Final Batch Loss: 0.16377130150794983\n",
      "Subject 1, Epoch 714, Loss: 0.6350039094686508, Final Batch Loss: 0.18536874651908875\n",
      "Subject 1, Epoch 715, Loss: 0.7943429797887802, Final Batch Loss: 0.23499475419521332\n",
      "Subject 1, Epoch 716, Loss: 0.6649919003248215, Final Batch Loss: 0.24624790251255035\n",
      "Subject 1, Epoch 717, Loss: 0.7099739015102386, Final Batch Loss: 0.3214362859725952\n",
      "Subject 1, Epoch 718, Loss: 0.7101884186267853, Final Batch Loss: 0.19612933695316315\n",
      "Subject 1, Epoch 719, Loss: 0.6946039646863937, Final Batch Loss: 0.2695313096046448\n",
      "Subject 1, Epoch 720, Loss: 0.7252766042947769, Final Batch Loss: 0.2627321779727936\n",
      "Subject 1, Epoch 721, Loss: 0.7373331636190414, Final Batch Loss: 0.3148546516895294\n",
      "Subject 1, Epoch 722, Loss: 0.7037068158388138, Final Batch Loss: 0.22686883807182312\n",
      "Subject 1, Epoch 723, Loss: 0.6096957921981812, Final Batch Loss: 0.1847604215145111\n",
      "Subject 1, Epoch 724, Loss: 0.774263933300972, Final Batch Loss: 0.2952698767185211\n",
      "Subject 1, Epoch 725, Loss: 0.8018950372934341, Final Batch Loss: 0.24502171576023102\n",
      "Subject 1, Epoch 726, Loss: 0.6414548605680466, Final Batch Loss: 0.25950607657432556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 727, Loss: 0.7980595529079437, Final Batch Loss: 0.23455587029457092\n",
      "Subject 1, Epoch 728, Loss: 0.6927313804626465, Final Batch Loss: 0.18560154736042023\n",
      "Subject 1, Epoch 729, Loss: 0.7428957372903824, Final Batch Loss: 0.2610742151737213\n",
      "Subject 1, Epoch 730, Loss: 0.7072521895170212, Final Batch Loss: 0.23767460882663727\n",
      "Subject 1, Epoch 731, Loss: 0.6223434805870056, Final Batch Loss: 0.23956385254859924\n",
      "Subject 1, Epoch 732, Loss: 0.6950900852680206, Final Batch Loss: 0.23442013561725616\n",
      "Subject 1, Epoch 733, Loss: 0.69308802485466, Final Batch Loss: 0.22875180840492249\n",
      "Subject 1, Epoch 734, Loss: 0.7388261407613754, Final Batch Loss: 0.2290758639574051\n",
      "Subject 1, Epoch 735, Loss: 0.7964002192020416, Final Batch Loss: 0.2647706866264343\n",
      "Subject 1, Epoch 736, Loss: 0.5853035598993301, Final Batch Loss: 0.1931331306695938\n",
      "Subject 1, Epoch 737, Loss: 0.6540444493293762, Final Batch Loss: 0.19979192316532135\n",
      "Subject 1, Epoch 738, Loss: 0.6603999882936478, Final Batch Loss: 0.21902982890605927\n",
      "Subject 1, Epoch 739, Loss: 0.6415434032678604, Final Batch Loss: 0.1986118108034134\n",
      "Subject 1, Epoch 740, Loss: 0.8063802421092987, Final Batch Loss: 0.21343274414539337\n",
      "Subject 1, Epoch 741, Loss: 0.6324387267231941, Final Batch Loss: 0.1245202049612999\n",
      "Subject 1, Epoch 742, Loss: 0.8346060216426849, Final Batch Loss: 0.19220596551895142\n",
      "Subject 1, Epoch 743, Loss: 0.77505162358284, Final Batch Loss: 0.2532914876937866\n",
      "Subject 1, Epoch 744, Loss: 0.7247830182313919, Final Batch Loss: 0.22076375782489777\n",
      "Subject 1, Epoch 745, Loss: 0.7223280966281891, Final Batch Loss: 0.29183200001716614\n",
      "Subject 1, Epoch 746, Loss: 0.70600226521492, Final Batch Loss: 0.20051458477973938\n",
      "Subject 1, Epoch 747, Loss: 0.8238110542297363, Final Batch Loss: 0.19261686503887177\n",
      "Subject 1, Epoch 748, Loss: 0.7279170751571655, Final Batch Loss: 0.2546682059764862\n",
      "Subject 1, Epoch 749, Loss: 0.7382587641477585, Final Batch Loss: 0.17767350375652313\n",
      "Subject 1, Epoch 750, Loss: 0.7031699866056442, Final Batch Loss: 0.2089516669511795\n",
      "Subject 1, Epoch 751, Loss: 0.6309560984373093, Final Batch Loss: 0.16603729128837585\n",
      "Subject 1, Epoch 752, Loss: 0.716781422495842, Final Batch Loss: 0.23827610909938812\n",
      "Subject 1, Epoch 753, Loss: 0.6539640724658966, Final Batch Loss: 0.18632766604423523\n",
      "Subject 1, Epoch 754, Loss: 0.7112552970647812, Final Batch Loss: 0.1915738433599472\n",
      "Subject 1, Epoch 755, Loss: 0.6978916525840759, Final Batch Loss: 0.34354937076568604\n",
      "Subject 1, Epoch 756, Loss: 0.6213356703519821, Final Batch Loss: 0.1933424472808838\n",
      "Subject 1, Epoch 757, Loss: 0.7511585205793381, Final Batch Loss: 0.2939031720161438\n",
      "Subject 1, Epoch 758, Loss: 0.6580396145582199, Final Batch Loss: 0.23874257504940033\n",
      "Subject 1, Epoch 759, Loss: 0.6227402091026306, Final Batch Loss: 0.253205269575119\n",
      "Subject 1, Epoch 760, Loss: 0.7745027095079422, Final Batch Loss: 0.26062440872192383\n",
      "Subject 1, Epoch 761, Loss: 0.6702056974172592, Final Batch Loss: 0.24204625189304352\n",
      "Subject 1, Epoch 762, Loss: 0.706660658121109, Final Batch Loss: 0.18591681122779846\n",
      "Subject 1, Epoch 763, Loss: 0.5994710028171539, Final Batch Loss: 0.2685413062572479\n",
      "Subject 1, Epoch 764, Loss: 0.5819269716739655, Final Batch Loss: 0.16317927837371826\n",
      "Subject 1, Epoch 765, Loss: 0.6112426221370697, Final Batch Loss: 0.2221786379814148\n",
      "Subject 1, Epoch 766, Loss: 0.7380849272012711, Final Batch Loss: 0.27936163544654846\n",
      "Subject 1, Epoch 767, Loss: 0.698995441198349, Final Batch Loss: 0.19301390647888184\n",
      "Subject 1, Epoch 768, Loss: 0.6894608289003372, Final Batch Loss: 0.2602585256099701\n",
      "Subject 1, Epoch 769, Loss: 0.6689220368862152, Final Batch Loss: 0.2493942528963089\n",
      "Subject 1, Epoch 770, Loss: 0.6660411059856415, Final Batch Loss: 0.23456290364265442\n",
      "Subject 1, Epoch 771, Loss: 0.6977859064936638, Final Batch Loss: 0.3878803849220276\n",
      "Subject 1, Epoch 772, Loss: 0.7089674025774002, Final Batch Loss: 0.24344991147518158\n",
      "Subject 1, Epoch 773, Loss: 0.6398472189903259, Final Batch Loss: 0.20125091075897217\n",
      "Subject 1, Epoch 774, Loss: 0.6332566142082214, Final Batch Loss: 0.13883617520332336\n",
      "Subject 1, Epoch 775, Loss: 0.6920938193798065, Final Batch Loss: 0.21940481662750244\n",
      "Subject 1, Epoch 776, Loss: 0.6440839767456055, Final Batch Loss: 0.2319795787334442\n",
      "Subject 1, Epoch 777, Loss: 0.698972761631012, Final Batch Loss: 0.2640079855918884\n",
      "Subject 1, Epoch 778, Loss: 0.6757058501243591, Final Batch Loss: 0.2447795867919922\n",
      "Subject 1, Epoch 779, Loss: 0.6435637325048447, Final Batch Loss: 0.22067198157310486\n",
      "Subject 1, Epoch 780, Loss: 0.6480650901794434, Final Batch Loss: 0.2190573662519455\n",
      "Subject 1, Epoch 781, Loss: 0.6425976306200027, Final Batch Loss: 0.20712415874004364\n",
      "Subject 1, Epoch 782, Loss: 0.8230617344379425, Final Batch Loss: 0.3649310767650604\n",
      "Subject 1, Epoch 783, Loss: 0.5725115314126015, Final Batch Loss: 0.11592363566160202\n",
      "Subject 1, Epoch 784, Loss: 0.6433917582035065, Final Batch Loss: 0.2350459098815918\n",
      "Subject 1, Epoch 785, Loss: 0.7183145731687546, Final Batch Loss: 0.15038934350013733\n",
      "Subject 1, Epoch 786, Loss: 0.638580933213234, Final Batch Loss: 0.26519620418548584\n",
      "Subject 1, Epoch 787, Loss: 0.8004972040653229, Final Batch Loss: 0.23295539617538452\n",
      "Subject 1, Epoch 788, Loss: 0.7363695502281189, Final Batch Loss: 0.2475682646036148\n",
      "Subject 1, Epoch 789, Loss: 0.6417843401432037, Final Batch Loss: 0.23693014681339264\n",
      "Subject 1, Epoch 790, Loss: 0.6695112735033035, Final Batch Loss: 0.24932466447353363\n",
      "Subject 1, Epoch 791, Loss: 0.6629820764064789, Final Batch Loss: 0.25222980976104736\n",
      "Subject 1, Epoch 792, Loss: 0.6511183381080627, Final Batch Loss: 0.19994325935840607\n",
      "Subject 1, Epoch 793, Loss: 0.6464077085256577, Final Batch Loss: 0.15436311066150665\n",
      "Subject 1, Epoch 794, Loss: 0.8480203151702881, Final Batch Loss: 0.2542140781879425\n",
      "Subject 1, Epoch 795, Loss: 0.6658568233251572, Final Batch Loss: 0.2417677342891693\n",
      "Subject 1, Epoch 796, Loss: 0.6106529086828232, Final Batch Loss: 0.1356913298368454\n",
      "Subject 1, Epoch 797, Loss: 0.6815348118543625, Final Batch Loss: 0.22582872211933136\n",
      "Subject 1, Epoch 798, Loss: 0.6266312599182129, Final Batch Loss: 0.25523003935813904\n",
      "Subject 1, Epoch 799, Loss: 0.7030927240848541, Final Batch Loss: 0.17344743013381958\n",
      "Subject 1, Epoch 800, Loss: 0.5690756887197495, Final Batch Loss: 0.19737131893634796\n",
      "Subject 1, Epoch 801, Loss: 0.6764891147613525, Final Batch Loss: 0.1412079781293869\n",
      "Subject 1, Epoch 802, Loss: 0.6682728230953217, Final Batch Loss: 0.24873270094394684\n",
      "Subject 1, Epoch 803, Loss: 0.6496524810791016, Final Batch Loss: 0.2169119119644165\n",
      "Subject 1, Epoch 804, Loss: 0.7347354739904404, Final Batch Loss: 0.21077798306941986\n",
      "Subject 1, Epoch 805, Loss: 0.7413703501224518, Final Batch Loss: 0.3358188271522522\n",
      "Subject 1, Epoch 806, Loss: 0.5456575155258179, Final Batch Loss: 0.15046803653240204\n",
      "Subject 1, Epoch 807, Loss: 0.7048028409481049, Final Batch Loss: 0.23762692511081696\n",
      "Subject 1, Epoch 808, Loss: 0.5898818969726562, Final Batch Loss: 0.2516859471797943\n",
      "Subject 1, Epoch 809, Loss: 0.541018471121788, Final Batch Loss: 0.1752859205007553\n",
      "Subject 1, Epoch 810, Loss: 0.6076260060071945, Final Batch Loss: 0.18493124842643738\n",
      "Subject 1, Epoch 811, Loss: 0.6444086730480194, Final Batch Loss: 0.25501859188079834\n",
      "Subject 1, Epoch 812, Loss: 0.6495049148797989, Final Batch Loss: 0.2443125694990158\n",
      "Subject 1, Epoch 813, Loss: 0.6539765745401382, Final Batch Loss: 0.20367392897605896\n",
      "Subject 1, Epoch 814, Loss: 0.6077364236116409, Final Batch Loss: 0.2393755167722702\n",
      "Subject 1, Epoch 815, Loss: 0.5924535989761353, Final Batch Loss: 0.1703403890132904\n",
      "Subject 1, Epoch 816, Loss: 0.611946702003479, Final Batch Loss: 0.2240481674671173\n",
      "Subject 1, Epoch 817, Loss: 0.5770324170589447, Final Batch Loss: 0.29057106375694275\n",
      "Subject 1, Epoch 818, Loss: 0.6421530097723007, Final Batch Loss: 0.22411076724529266\n",
      "Subject 1, Epoch 819, Loss: 0.5646021217107773, Final Batch Loss: 0.13260024785995483\n",
      "Subject 1, Epoch 820, Loss: 0.5971921980381012, Final Batch Loss: 0.16705821454524994\n",
      "Subject 1, Epoch 821, Loss: 0.6499629765748978, Final Batch Loss: 0.1560896337032318\n",
      "Subject 1, Epoch 822, Loss: 0.6449280083179474, Final Batch Loss: 0.2040218859910965\n",
      "Subject 1, Epoch 823, Loss: 0.6126092672348022, Final Batch Loss: 0.1875041127204895\n",
      "Subject 1, Epoch 824, Loss: 0.6431119590997696, Final Batch Loss: 0.16654834151268005\n",
      "Subject 1, Epoch 825, Loss: 0.7663781493902206, Final Batch Loss: 0.2303134799003601\n",
      "Subject 1, Epoch 826, Loss: 0.6201070249080658, Final Batch Loss: 0.17265628278255463\n",
      "Subject 1, Epoch 827, Loss: 0.6556439250707626, Final Batch Loss: 0.2381693422794342\n",
      "Subject 1, Epoch 828, Loss: 0.6418229788541794, Final Batch Loss: 0.12601926922798157\n",
      "Subject 1, Epoch 829, Loss: 0.6314316093921661, Final Batch Loss: 0.26446837186813354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 830, Loss: 0.6206983625888824, Final Batch Loss: 0.19569098949432373\n",
      "Subject 1, Epoch 831, Loss: 0.5902343392372131, Final Batch Loss: 0.17238371074199677\n",
      "Subject 1, Epoch 832, Loss: 0.6913328170776367, Final Batch Loss: 0.22024370729923248\n",
      "Subject 1, Epoch 833, Loss: 0.641368955373764, Final Batch Loss: 0.18767830729484558\n",
      "Subject 1, Epoch 834, Loss: 0.5806914418935776, Final Batch Loss: 0.20210422575473785\n",
      "Subject 1, Epoch 835, Loss: 0.8261676728725433, Final Batch Loss: 0.33190837502479553\n",
      "Subject 1, Epoch 836, Loss: 0.6507381796836853, Final Batch Loss: 0.20083551108837128\n",
      "Subject 1, Epoch 837, Loss: 0.6135448813438416, Final Batch Loss: 0.1886596828699112\n",
      "Subject 1, Epoch 838, Loss: 0.6670415550470352, Final Batch Loss: 0.2095225304365158\n",
      "Subject 1, Epoch 839, Loss: 0.7392373383045197, Final Batch Loss: 0.341934472322464\n",
      "Subject 1, Epoch 840, Loss: 0.6278483718633652, Final Batch Loss: 0.17897707223892212\n",
      "Subject 1, Epoch 841, Loss: 0.546265721321106, Final Batch Loss: 0.18480683863162994\n",
      "Subject 1, Epoch 842, Loss: 0.6115530431270599, Final Batch Loss: 0.2409316748380661\n",
      "Subject 1, Epoch 843, Loss: 0.6502827480435371, Final Batch Loss: 0.17822785675525665\n",
      "Subject 1, Epoch 844, Loss: 0.5645155608654022, Final Batch Loss: 0.14019301533699036\n",
      "Subject 1, Epoch 845, Loss: 0.7285844087600708, Final Batch Loss: 0.18558183312416077\n",
      "Subject 1, Epoch 846, Loss: 0.6479366570711136, Final Batch Loss: 0.27441760897636414\n",
      "Subject 1, Epoch 847, Loss: 0.7071339637041092, Final Batch Loss: 0.21472497284412384\n",
      "Subject 1, Epoch 848, Loss: 0.6696549206972122, Final Batch Loss: 0.20696572959423065\n",
      "Subject 1, Epoch 849, Loss: 0.5885366201400757, Final Batch Loss: 0.21130409836769104\n",
      "Subject 1, Epoch 850, Loss: 0.6148770302534103, Final Batch Loss: 0.14452479779720306\n",
      "Subject 1, Epoch 851, Loss: 0.5571646764874458, Final Batch Loss: 0.12056451290845871\n",
      "Subject 1, Epoch 852, Loss: 0.6226281076669693, Final Batch Loss: 0.19553503394126892\n",
      "Subject 1, Epoch 853, Loss: 0.6243302375078201, Final Batch Loss: 0.13745403289794922\n",
      "Subject 1, Epoch 854, Loss: 0.5885808914899826, Final Batch Loss: 0.2034873515367508\n",
      "Subject 1, Epoch 855, Loss: 0.7821522355079651, Final Batch Loss: 0.38611114025115967\n",
      "Subject 1, Epoch 856, Loss: 0.7225347459316254, Final Batch Loss: 0.27316245436668396\n",
      "Subject 1, Epoch 857, Loss: 0.6201810538768768, Final Batch Loss: 0.18921472132205963\n",
      "Subject 1, Epoch 858, Loss: 0.5899209380149841, Final Batch Loss: 0.19663718342781067\n",
      "Subject 1, Epoch 859, Loss: 0.6016847491264343, Final Batch Loss: 0.16397283971309662\n",
      "Subject 1, Epoch 860, Loss: 0.577699825167656, Final Batch Loss: 0.20441366732120514\n",
      "Subject 1, Epoch 861, Loss: 0.5747779309749603, Final Batch Loss: 0.24762001633644104\n",
      "Subject 1, Epoch 862, Loss: 0.5496696829795837, Final Batch Loss: 0.16236737370491028\n",
      "Subject 1, Epoch 863, Loss: 0.7670372128486633, Final Batch Loss: 0.2476879060268402\n",
      "Subject 1, Epoch 864, Loss: 0.6599336266517639, Final Batch Loss: 0.1615786999464035\n",
      "Subject 1, Epoch 865, Loss: 0.6033208668231964, Final Batch Loss: 0.23114359378814697\n",
      "Subject 1, Epoch 866, Loss: 0.580890342593193, Final Batch Loss: 0.17576631903648376\n",
      "Subject 1, Epoch 867, Loss: 0.5994590371847153, Final Batch Loss: 0.2316543012857437\n",
      "Subject 1, Epoch 868, Loss: 0.5274496376514435, Final Batch Loss: 0.15415839850902557\n",
      "Subject 1, Epoch 869, Loss: 0.6301580220460892, Final Batch Loss: 0.19974611699581146\n",
      "Subject 1, Epoch 870, Loss: 0.5797286778688431, Final Batch Loss: 0.1415444016456604\n",
      "Subject 1, Epoch 871, Loss: 0.6532021313905716, Final Batch Loss: 0.24170362949371338\n",
      "Subject 1, Epoch 872, Loss: 0.7540162354707718, Final Batch Loss: 0.3080395758152008\n",
      "Subject 1, Epoch 873, Loss: 0.6540329158306122, Final Batch Loss: 0.2477162778377533\n",
      "Subject 1, Epoch 874, Loss: 0.5444768965244293, Final Batch Loss: 0.16581809520721436\n",
      "Subject 1, Epoch 875, Loss: 0.6378753036260605, Final Batch Loss: 0.16376522183418274\n",
      "Subject 1, Epoch 876, Loss: 0.6355868577957153, Final Batch Loss: 0.14588941633701324\n",
      "Subject 1, Epoch 877, Loss: 0.4998975545167923, Final Batch Loss: 0.1309102475643158\n",
      "Subject 1, Epoch 878, Loss: 0.6035944819450378, Final Batch Loss: 0.1681353598833084\n",
      "Subject 1, Epoch 879, Loss: 0.6333114504814148, Final Batch Loss: 0.2209887057542801\n",
      "Subject 1, Epoch 880, Loss: 0.6263997554779053, Final Batch Loss: 0.22481851279735565\n",
      "Subject 1, Epoch 881, Loss: 0.5421590805053711, Final Batch Loss: 0.17845483124256134\n",
      "Subject 1, Epoch 882, Loss: 0.550775408744812, Final Batch Loss: 0.16350972652435303\n",
      "Subject 1, Epoch 883, Loss: 0.5995855033397675, Final Batch Loss: 0.14265654981136322\n",
      "Subject 1, Epoch 884, Loss: 0.5551753118634224, Final Batch Loss: 0.20769603550434113\n",
      "Subject 1, Epoch 885, Loss: 0.5722011625766754, Final Batch Loss: 0.2665361166000366\n",
      "Subject 1, Epoch 886, Loss: 0.6282103657722473, Final Batch Loss: 0.28860774636268616\n",
      "Subject 1, Epoch 887, Loss: 0.6910175606608391, Final Batch Loss: 0.34710609912872314\n",
      "Subject 1, Epoch 888, Loss: 0.6221583038568497, Final Batch Loss: 0.13677415251731873\n",
      "Subject 1, Epoch 889, Loss: 0.6246065497398376, Final Batch Loss: 0.2380189746618271\n",
      "Subject 1, Epoch 890, Loss: 0.6404749155044556, Final Batch Loss: 0.23973949253559113\n",
      "Subject 1, Epoch 891, Loss: 0.6285499334335327, Final Batch Loss: 0.2650999426841736\n",
      "Subject 1, Epoch 892, Loss: 0.564423218369484, Final Batch Loss: 0.12816734611988068\n",
      "Subject 1, Epoch 893, Loss: 0.6123784184455872, Final Batch Loss: 0.2453770786523819\n",
      "Subject 1, Epoch 894, Loss: 0.5201645493507385, Final Batch Loss: 0.09636837244033813\n",
      "Subject 1, Epoch 895, Loss: 0.6226945966482162, Final Batch Loss: 0.22983190417289734\n",
      "Subject 1, Epoch 896, Loss: 0.5900191515684128, Final Batch Loss: 0.17617911100387573\n",
      "Subject 1, Epoch 897, Loss: 0.6732543110847473, Final Batch Loss: 0.17278431355953217\n",
      "Subject 1, Epoch 898, Loss: 0.5459243655204773, Final Batch Loss: 0.14552567899227142\n",
      "Subject 1, Epoch 899, Loss: 0.6701327711343765, Final Batch Loss: 0.1761375218629837\n",
      "Subject 1, Epoch 900, Loss: 0.5478278994560242, Final Batch Loss: 0.19087369740009308\n",
      "Subject 1, Epoch 901, Loss: 0.606816753745079, Final Batch Loss: 0.23072175681591034\n",
      "Subject 1, Epoch 902, Loss: 0.6359848603606224, Final Batch Loss: 0.09251195937395096\n",
      "Subject 1, Epoch 903, Loss: 0.59915491938591, Final Batch Loss: 0.16796882450580597\n",
      "Subject 1, Epoch 904, Loss: 0.5437207520008087, Final Batch Loss: 0.17362628877162933\n",
      "Subject 1, Epoch 905, Loss: 0.555967852473259, Final Batch Loss: 0.15024694800376892\n",
      "Subject 1, Epoch 906, Loss: 0.5287780612707138, Final Batch Loss: 0.1774095743894577\n",
      "Subject 1, Epoch 907, Loss: 0.5712782293558121, Final Batch Loss: 0.2234724909067154\n",
      "Subject 1, Epoch 908, Loss: 0.5346053838729858, Final Batch Loss: 0.1508503407239914\n",
      "Subject 1, Epoch 909, Loss: 0.543940469622612, Final Batch Loss: 0.17111234366893768\n",
      "Subject 1, Epoch 910, Loss: 0.585139125585556, Final Batch Loss: 0.13062115013599396\n",
      "Subject 1, Epoch 911, Loss: 0.6445032358169556, Final Batch Loss: 0.2453303337097168\n",
      "Subject 1, Epoch 912, Loss: 0.5645047724246979, Final Batch Loss: 0.1765274554491043\n",
      "Subject 1, Epoch 913, Loss: 0.4585787057876587, Final Batch Loss: 0.1489848643541336\n",
      "Subject 1, Epoch 914, Loss: 0.607706218957901, Final Batch Loss: 0.25840499997138977\n",
      "Subject 1, Epoch 915, Loss: 0.6019077450037003, Final Batch Loss: 0.20026642084121704\n",
      "Subject 1, Epoch 916, Loss: 0.6679393202066422, Final Batch Loss: 0.2587979733943939\n",
      "Subject 1, Epoch 917, Loss: 0.5524841398000717, Final Batch Loss: 0.18985512852668762\n",
      "Subject 1, Epoch 918, Loss: 0.6146261543035507, Final Batch Loss: 0.1871410757303238\n",
      "Subject 1, Epoch 919, Loss: 0.6032442897558212, Final Batch Loss: 0.19936418533325195\n",
      "Subject 1, Epoch 920, Loss: 0.5462195128202438, Final Batch Loss: 0.18651041388511658\n",
      "Subject 1, Epoch 921, Loss: 0.5243941098451614, Final Batch Loss: 0.08265292644500732\n",
      "Subject 1, Epoch 922, Loss: 0.5421375036239624, Final Batch Loss: 0.2654174864292145\n",
      "Subject 1, Epoch 923, Loss: 0.5628079622983932, Final Batch Loss: 0.17315535247325897\n",
      "Subject 1, Epoch 924, Loss: 0.598655104637146, Final Batch Loss: 0.2808959484100342\n",
      "Subject 1, Epoch 925, Loss: 0.4916316494345665, Final Batch Loss: 0.09373270720243454\n",
      "Subject 1, Epoch 926, Loss: 0.5804912745952606, Final Batch Loss: 0.12917259335517883\n",
      "Subject 1, Epoch 927, Loss: 0.5610020607709885, Final Batch Loss: 0.239407017827034\n",
      "Subject 1, Epoch 928, Loss: 0.6286669224500656, Final Batch Loss: 0.27191466093063354\n",
      "Subject 1, Epoch 929, Loss: 0.5327095091342926, Final Batch Loss: 0.1343691200017929\n",
      "Subject 1, Epoch 930, Loss: 0.5522856712341309, Final Batch Loss: 0.21912643313407898\n",
      "Subject 1, Epoch 931, Loss: 0.5027073472738266, Final Batch Loss: 0.21066652238368988\n",
      "Subject 1, Epoch 932, Loss: 0.5989162623882294, Final Batch Loss: 0.2530120313167572\n",
      "Subject 1, Epoch 933, Loss: 0.5655898600816727, Final Batch Loss: 0.15113307535648346\n",
      "Subject 1, Epoch 934, Loss: 0.5930803567171097, Final Batch Loss: 0.24050019681453705\n",
      "Subject 1, Epoch 935, Loss: 0.5332594141364098, Final Batch Loss: 0.18641085922718048\n",
      "Subject 1, Epoch 936, Loss: 0.6650201380252838, Final Batch Loss: 0.19921351969242096\n",
      "Subject 1, Epoch 937, Loss: 0.5179851055145264, Final Batch Loss: 0.13060270249843597\n",
      "Subject 1, Epoch 938, Loss: 0.6173668056726456, Final Batch Loss: 0.13877663016319275\n",
      "Subject 1, Epoch 939, Loss: 0.6441882103681564, Final Batch Loss: 0.16493351757526398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1, Epoch 940, Loss: 0.6046615988016129, Final Batch Loss: 0.21902011334896088\n",
      "Subject 1, Epoch 941, Loss: 0.5990165174007416, Final Batch Loss: 0.20205435156822205\n",
      "Subject 1, Epoch 942, Loss: 0.5666839927434921, Final Batch Loss: 0.1846526861190796\n",
      "Subject 1, Epoch 943, Loss: 0.5214153081178665, Final Batch Loss: 0.1410771906375885\n",
      "Subject 1, Epoch 944, Loss: 0.4961523711681366, Final Batch Loss: 0.141731396317482\n",
      "Subject 1, Epoch 945, Loss: 0.5826175063848495, Final Batch Loss: 0.13101916015148163\n",
      "Subject 1, Epoch 946, Loss: 0.67198346555233, Final Batch Loss: 0.1889948695898056\n",
      "Subject 1, Epoch 947, Loss: 0.6846605241298676, Final Batch Loss: 0.3789984881877899\n",
      "Subject 1, Epoch 948, Loss: 0.49665428698062897, Final Batch Loss: 0.1822780817747116\n",
      "Subject 1, Epoch 949, Loss: 0.5893213599920273, Final Batch Loss: 0.2772797644138336\n",
      "Subject 1, Epoch 950, Loss: 0.5819687992334366, Final Batch Loss: 0.17869524657726288\n",
      "Subject 1, Epoch 951, Loss: 0.48456841707229614, Final Batch Loss: 0.14322465658187866\n",
      "Subject 1, Epoch 952, Loss: 0.7048643529415131, Final Batch Loss: 0.17801092565059662\n",
      "Subject 1, Epoch 953, Loss: 0.5674312859773636, Final Batch Loss: 0.19479021430015564\n",
      "Subject 1, Epoch 954, Loss: 0.48699137568473816, Final Batch Loss: 0.15498816967010498\n",
      "Subject 1, Epoch 955, Loss: 0.5110264420509338, Final Batch Loss: 0.15362757444381714\n",
      "Subject 1, Epoch 956, Loss: 0.6063227355480194, Final Batch Loss: 0.15884850919246674\n",
      "Subject 1, Epoch 957, Loss: 0.5873513370752335, Final Batch Loss: 0.19157041609287262\n",
      "Subject 1, Epoch 958, Loss: 0.5619639903306961, Final Batch Loss: 0.24695274233818054\n",
      "Subject 1, Epoch 959, Loss: 0.5617674887180328, Final Batch Loss: 0.236498162150383\n",
      "Subject 1, Epoch 960, Loss: 0.4595669135451317, Final Batch Loss: 0.171395942568779\n",
      "Subject 1, Epoch 961, Loss: 0.5903548672795296, Final Batch Loss: 0.07142950594425201\n",
      "Subject 1, Epoch 962, Loss: 0.5048525333404541, Final Batch Loss: 0.14565598964691162\n",
      "Subject 1, Epoch 963, Loss: 0.5031639188528061, Final Batch Loss: 0.14187683165073395\n",
      "Subject 1, Epoch 964, Loss: 0.557170957326889, Final Batch Loss: 0.20523706078529358\n",
      "Subject 1, Epoch 965, Loss: 0.5923207849264145, Final Batch Loss: 0.23771165311336517\n",
      "Subject 1, Epoch 966, Loss: 0.4854144752025604, Final Batch Loss: 0.15456467866897583\n",
      "Subject 1, Epoch 967, Loss: 0.5421918034553528, Final Batch Loss: 0.24035139381885529\n",
      "Subject 1, Epoch 968, Loss: 0.5392036139965057, Final Batch Loss: 0.1818075031042099\n",
      "Subject 1, Epoch 969, Loss: 0.5406573414802551, Final Batch Loss: 0.22743546962738037\n",
      "Subject 1, Epoch 970, Loss: 0.6476243287324905, Final Batch Loss: 0.20215073227882385\n",
      "Subject 1, Epoch 971, Loss: 0.6101120859384537, Final Batch Loss: 0.19689182937145233\n",
      "Subject 1, Epoch 972, Loss: 0.5726241916418076, Final Batch Loss: 0.24941907823085785\n",
      "Subject 1, Epoch 973, Loss: 0.5941332578659058, Final Batch Loss: 0.16945333778858185\n",
      "Subject 1, Epoch 974, Loss: 0.5980497151613235, Final Batch Loss: 0.22363588213920593\n",
      "Subject 1, Epoch 975, Loss: 0.5804779827594757, Final Batch Loss: 0.17563042044639587\n",
      "Subject 1, Epoch 976, Loss: 0.6065519452095032, Final Batch Loss: 0.22927671670913696\n",
      "Subject 1, Epoch 977, Loss: 0.5177851170301437, Final Batch Loss: 0.16177015006542206\n",
      "Subject 1, Epoch 978, Loss: 0.6113445162773132, Final Batch Loss: 0.28947362303733826\n",
      "Subject 1, Epoch 979, Loss: 0.4591653048992157, Final Batch Loss: 0.19202277064323425\n",
      "Subject 1, Epoch 980, Loss: 0.4950987547636032, Final Batch Loss: 0.1199735701084137\n",
      "Subject 1, Epoch 981, Loss: 0.5587271451950073, Final Batch Loss: 0.13973289728164673\n",
      "Subject 1, Epoch 982, Loss: 0.5953086540102959, Final Batch Loss: 0.17978578805923462\n",
      "Subject 1, Epoch 983, Loss: 0.4291398897767067, Final Batch Loss: 0.15394602715969086\n",
      "Subject 1, Epoch 984, Loss: 0.5123187229037285, Final Batch Loss: 0.1117459312081337\n",
      "Subject 1, Epoch 985, Loss: 0.5512741059064865, Final Batch Loss: 0.19611354172229767\n",
      "Subject 1, Epoch 986, Loss: 0.5241450369358063, Final Batch Loss: 0.17034459114074707\n",
      "Subject 1, Epoch 987, Loss: 0.4549887180328369, Final Batch Loss: 0.13924641907215118\n",
      "Subject 1, Epoch 988, Loss: 0.6165626794099808, Final Batch Loss: 0.16745200753211975\n",
      "Subject 1, Epoch 989, Loss: 0.5811608582735062, Final Batch Loss: 0.1910465508699417\n",
      "Subject 1, Epoch 990, Loss: 0.4932959973812103, Final Batch Loss: 0.1490929275751114\n",
      "Subject 1, Epoch 991, Loss: 0.5416088551282883, Final Batch Loss: 0.20305755734443665\n",
      "Subject 1, Epoch 992, Loss: 0.6121773719787598, Final Batch Loss: 0.20234230160713196\n",
      "Subject 1, Epoch 993, Loss: 0.5561964809894562, Final Batch Loss: 0.1776057630777359\n",
      "Subject 1, Epoch 994, Loss: 0.4987967163324356, Final Batch Loss: 0.16001760959625244\n",
      "Subject 1, Epoch 995, Loss: 0.6065705120563507, Final Batch Loss: 0.17267556488513947\n",
      "Subject 1, Epoch 996, Loss: 0.4620611146092415, Final Batch Loss: 0.09703133255243301\n",
      "Subject 1, Epoch 997, Loss: 0.3827280253171921, Final Batch Loss: 0.1292584389448166\n",
      "Subject 1, Epoch 998, Loss: 0.5115989074110985, Final Batch Loss: 0.09536387771368027\n",
      "Subject 1, Epoch 999, Loss: 0.51409462839365, Final Batch Loss: 0.10310354083776474\n",
      "Subject 1, Epoch 1000, Loss: 0.5450121462345123, Final Batch Loss: 0.23791328072547913\n",
      "Subject 2, Epoch 1, Loss: 5.392983555793762, Final Batch Loss: 1.788469910621643\n",
      "Subject 2, Epoch 2, Loss: 5.406430721282959, Final Batch Loss: 1.8293955326080322\n",
      "Subject 2, Epoch 3, Loss: 5.399821996688843, Final Batch Loss: 1.8258072137832642\n",
      "Subject 2, Epoch 4, Loss: 5.37678062915802, Final Batch Loss: 1.7770321369171143\n",
      "Subject 2, Epoch 5, Loss: 5.381861448287964, Final Batch Loss: 1.7937688827514648\n",
      "Subject 2, Epoch 6, Loss: 5.3649208545684814, Final Batch Loss: 1.7770801782608032\n",
      "Subject 2, Epoch 7, Loss: 5.3656840324401855, Final Batch Loss: 1.786799430847168\n",
      "Subject 2, Epoch 8, Loss: 5.3607165813446045, Final Batch Loss: 1.7868541479110718\n",
      "Subject 2, Epoch 9, Loss: 5.360597491264343, Final Batch Loss: 1.7962545156478882\n",
      "Subject 2, Epoch 10, Loss: 5.345722079277039, Final Batch Loss: 1.776442289352417\n",
      "Subject 2, Epoch 11, Loss: 5.326221704483032, Final Batch Loss: 1.7727832794189453\n",
      "Subject 2, Epoch 12, Loss: 5.312944293022156, Final Batch Loss: 1.775065541267395\n",
      "Subject 2, Epoch 13, Loss: 5.279619336128235, Final Batch Loss: 1.7284080982208252\n",
      "Subject 2, Epoch 14, Loss: 5.285187840461731, Final Batch Loss: 1.7593765258789062\n",
      "Subject 2, Epoch 15, Loss: 5.256663918495178, Final Batch Loss: 1.751369833946228\n",
      "Subject 2, Epoch 16, Loss: 5.224843859672546, Final Batch Loss: 1.7402628660202026\n",
      "Subject 2, Epoch 17, Loss: 5.188958644866943, Final Batch Loss: 1.708581805229187\n",
      "Subject 2, Epoch 18, Loss: 5.148614883422852, Final Batch Loss: 1.7215839624404907\n",
      "Subject 2, Epoch 19, Loss: 5.06169581413269, Final Batch Loss: 1.636534333229065\n",
      "Subject 2, Epoch 20, Loss: 4.99215292930603, Final Batch Loss: 1.5824592113494873\n",
      "Subject 2, Epoch 21, Loss: 5.015043377876282, Final Batch Loss: 1.7210750579833984\n",
      "Subject 2, Epoch 22, Loss: 4.935904026031494, Final Batch Loss: 1.629743218421936\n",
      "Subject 2, Epoch 23, Loss: 4.889481067657471, Final Batch Loss: 1.6714372634887695\n",
      "Subject 2, Epoch 24, Loss: 4.892214417457581, Final Batch Loss: 1.7169495820999146\n",
      "Subject 2, Epoch 25, Loss: 4.798042058944702, Final Batch Loss: 1.6138887405395508\n",
      "Subject 2, Epoch 26, Loss: 4.6713645458221436, Final Batch Loss: 1.5739291906356812\n",
      "Subject 2, Epoch 27, Loss: 4.6100850105285645, Final Batch Loss: 1.6239575147628784\n",
      "Subject 2, Epoch 28, Loss: 4.499636650085449, Final Batch Loss: 1.5204986333847046\n",
      "Subject 2, Epoch 29, Loss: 4.382696747779846, Final Batch Loss: 1.4413210153579712\n",
      "Subject 2, Epoch 30, Loss: 4.277670621871948, Final Batch Loss: 1.378712773323059\n",
      "Subject 2, Epoch 31, Loss: 4.2391074895858765, Final Batch Loss: 1.3903719186782837\n",
      "Subject 2, Epoch 32, Loss: 4.146890759468079, Final Batch Loss: 1.3943184614181519\n",
      "Subject 2, Epoch 33, Loss: 4.065796136856079, Final Batch Loss: 1.4195505380630493\n",
      "Subject 2, Epoch 34, Loss: 3.9660321474075317, Final Batch Loss: 1.343891978263855\n",
      "Subject 2, Epoch 35, Loss: 3.749271035194397, Final Batch Loss: 1.2353678941726685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 36, Loss: 3.7169857025146484, Final Batch Loss: 1.1915847063064575\n",
      "Subject 2, Epoch 37, Loss: 3.6008663177490234, Final Batch Loss: 1.1654645204544067\n",
      "Subject 2, Epoch 38, Loss: 3.5954314470291138, Final Batch Loss: 1.195041298866272\n",
      "Subject 2, Epoch 39, Loss: 3.4381715059280396, Final Batch Loss: 1.122431993484497\n",
      "Subject 2, Epoch 40, Loss: 3.55504310131073, Final Batch Loss: 1.1818864345550537\n",
      "Subject 2, Epoch 41, Loss: 3.459978222846985, Final Batch Loss: 1.1709516048431396\n",
      "Subject 2, Epoch 42, Loss: 3.5351721048355103, Final Batch Loss: 1.1650445461273193\n",
      "Subject 2, Epoch 43, Loss: 3.419552803039551, Final Batch Loss: 1.165175199508667\n",
      "Subject 2, Epoch 44, Loss: 3.492815852165222, Final Batch Loss: 1.1579644680023193\n",
      "Subject 2, Epoch 45, Loss: 3.3428955078125, Final Batch Loss: 1.1364024877548218\n",
      "Subject 2, Epoch 46, Loss: 3.5032753944396973, Final Batch Loss: 1.1308581829071045\n",
      "Subject 2, Epoch 47, Loss: 3.4061083793640137, Final Batch Loss: 1.11594820022583\n",
      "Subject 2, Epoch 48, Loss: 3.489533305168152, Final Batch Loss: 1.1818153858184814\n",
      "Subject 2, Epoch 49, Loss: 3.379304885864258, Final Batch Loss: 1.1134815216064453\n",
      "Subject 2, Epoch 50, Loss: 3.3964505195617676, Final Batch Loss: 1.1145960092544556\n",
      "Subject 2, Epoch 51, Loss: 3.43593966960907, Final Batch Loss: 1.1977559328079224\n",
      "Subject 2, Epoch 52, Loss: 3.4181201457977295, Final Batch Loss: 1.166725754737854\n",
      "Subject 2, Epoch 53, Loss: 3.3743613958358765, Final Batch Loss: 1.1481082439422607\n",
      "Subject 2, Epoch 54, Loss: 3.3175705671310425, Final Batch Loss: 1.0579460859298706\n",
      "Subject 2, Epoch 55, Loss: 3.209553360939026, Final Batch Loss: 1.098473310470581\n",
      "Subject 2, Epoch 56, Loss: 3.300396680831909, Final Batch Loss: 1.1280529499053955\n",
      "Subject 2, Epoch 57, Loss: 3.278098464012146, Final Batch Loss: 1.07145357131958\n",
      "Subject 2, Epoch 58, Loss: 3.306272029876709, Final Batch Loss: 1.0833942890167236\n",
      "Subject 2, Epoch 59, Loss: 3.233376979827881, Final Batch Loss: 1.0165306329727173\n",
      "Subject 2, Epoch 60, Loss: 3.160510540008545, Final Batch Loss: 1.0231865644454956\n",
      "Subject 2, Epoch 61, Loss: 3.2411749362945557, Final Batch Loss: 1.034477710723877\n",
      "Subject 2, Epoch 62, Loss: 3.228084087371826, Final Batch Loss: 1.089148759841919\n",
      "Subject 2, Epoch 63, Loss: 3.094178080558777, Final Batch Loss: 1.019645094871521\n",
      "Subject 2, Epoch 64, Loss: 3.1237244606018066, Final Batch Loss: 1.0169039964675903\n",
      "Subject 2, Epoch 65, Loss: 3.1223445534706116, Final Batch Loss: 1.0718780755996704\n",
      "Subject 2, Epoch 66, Loss: 3.190835475921631, Final Batch Loss: 1.0938167572021484\n",
      "Subject 2, Epoch 67, Loss: 3.0883158445358276, Final Batch Loss: 0.9829068183898926\n",
      "Subject 2, Epoch 68, Loss: 3.155300498008728, Final Batch Loss: 1.0713326930999756\n",
      "Subject 2, Epoch 69, Loss: 3.0595008730888367, Final Batch Loss: 0.984000027179718\n",
      "Subject 2, Epoch 70, Loss: 3.0431123971939087, Final Batch Loss: 0.972923755645752\n",
      "Subject 2, Epoch 71, Loss: 2.9732717871665955, Final Batch Loss: 0.9718064665794373\n",
      "Subject 2, Epoch 72, Loss: 2.9962804317474365, Final Batch Loss: 1.0238598585128784\n",
      "Subject 2, Epoch 73, Loss: 2.939763128757477, Final Batch Loss: 0.9750168919563293\n",
      "Subject 2, Epoch 74, Loss: 2.955309212207794, Final Batch Loss: 0.9386927485466003\n",
      "Subject 2, Epoch 75, Loss: 2.9553498029708862, Final Batch Loss: 1.0003976821899414\n",
      "Subject 2, Epoch 76, Loss: 2.9273847341537476, Final Batch Loss: 1.0299063920974731\n",
      "Subject 2, Epoch 77, Loss: 2.783152997493744, Final Batch Loss: 0.9358673691749573\n",
      "Subject 2, Epoch 78, Loss: 2.76513135433197, Final Batch Loss: 0.9182185530662537\n",
      "Subject 2, Epoch 79, Loss: 2.8484307527542114, Final Batch Loss: 0.9736768007278442\n",
      "Subject 2, Epoch 80, Loss: 2.7188202142715454, Final Batch Loss: 0.8935677409172058\n",
      "Subject 2, Epoch 81, Loss: 2.7450908422470093, Final Batch Loss: 0.8654671907424927\n",
      "Subject 2, Epoch 82, Loss: 2.627630352973938, Final Batch Loss: 0.8657801747322083\n",
      "Subject 2, Epoch 83, Loss: 2.784695625305176, Final Batch Loss: 0.9120214581489563\n",
      "Subject 2, Epoch 84, Loss: 2.7060126066207886, Final Batch Loss: 0.9819527268409729\n",
      "Subject 2, Epoch 85, Loss: 2.6983930468559265, Final Batch Loss: 0.9065317511558533\n",
      "Subject 2, Epoch 86, Loss: 2.67334520816803, Final Batch Loss: 0.7989463806152344\n",
      "Subject 2, Epoch 87, Loss: 2.4959256649017334, Final Batch Loss: 0.7979251742362976\n",
      "Subject 2, Epoch 88, Loss: 2.5196797847747803, Final Batch Loss: 0.8239673972129822\n",
      "Subject 2, Epoch 89, Loss: 2.5772430896759033, Final Batch Loss: 0.8288136720657349\n",
      "Subject 2, Epoch 90, Loss: 2.698843240737915, Final Batch Loss: 0.9410825371742249\n",
      "Subject 2, Epoch 91, Loss: 2.59340238571167, Final Batch Loss: 0.7171466946601868\n",
      "Subject 2, Epoch 92, Loss: 2.6171277165412903, Final Batch Loss: 0.9050714373588562\n",
      "Subject 2, Epoch 93, Loss: 2.472173571586609, Final Batch Loss: 0.7847006916999817\n",
      "Subject 2, Epoch 94, Loss: 2.55287504196167, Final Batch Loss: 0.803779125213623\n",
      "Subject 2, Epoch 95, Loss: 2.5722151398658752, Final Batch Loss: 0.9045122861862183\n",
      "Subject 2, Epoch 96, Loss: 2.3744779229164124, Final Batch Loss: 0.8192086815834045\n",
      "Subject 2, Epoch 97, Loss: 2.5292890071868896, Final Batch Loss: 0.7920344471931458\n",
      "Subject 2, Epoch 98, Loss: 2.5389338731765747, Final Batch Loss: 0.916664183139801\n",
      "Subject 2, Epoch 99, Loss: 2.37053120136261, Final Batch Loss: 0.7847990393638611\n",
      "Subject 2, Epoch 100, Loss: 2.406490921974182, Final Batch Loss: 0.8616784811019897\n",
      "Subject 2, Epoch 101, Loss: 2.316375195980072, Final Batch Loss: 0.758360743522644\n",
      "Subject 2, Epoch 102, Loss: 2.3429384231567383, Final Batch Loss: 0.6898397207260132\n",
      "Subject 2, Epoch 103, Loss: 2.3989132046699524, Final Batch Loss: 0.7730609178543091\n",
      "Subject 2, Epoch 104, Loss: 2.3179088830947876, Final Batch Loss: 0.7578616142272949\n",
      "Subject 2, Epoch 105, Loss: 2.3354583978652954, Final Batch Loss: 0.7847062945365906\n",
      "Subject 2, Epoch 106, Loss: 2.3906708359718323, Final Batch Loss: 0.8988249897956848\n",
      "Subject 2, Epoch 107, Loss: 2.1612497568130493, Final Batch Loss: 0.779936671257019\n",
      "Subject 2, Epoch 108, Loss: 2.2712586522102356, Final Batch Loss: 0.7564141154289246\n",
      "Subject 2, Epoch 109, Loss: 2.287792980670929, Final Batch Loss: 0.690642237663269\n",
      "Subject 2, Epoch 110, Loss: 2.1946343183517456, Final Batch Loss: 0.6836093068122864\n",
      "Subject 2, Epoch 111, Loss: 2.244545638561249, Final Batch Loss: 0.7824236154556274\n",
      "Subject 2, Epoch 112, Loss: 2.357694447040558, Final Batch Loss: 0.8499001860618591\n",
      "Subject 2, Epoch 113, Loss: 2.334567368030548, Final Batch Loss: 0.7868021726608276\n",
      "Subject 2, Epoch 114, Loss: 2.3540409207344055, Final Batch Loss: 0.7484955191612244\n",
      "Subject 2, Epoch 115, Loss: 2.2272149324417114, Final Batch Loss: 0.8400701284408569\n",
      "Subject 2, Epoch 116, Loss: 2.2004058361053467, Final Batch Loss: 0.7862593531608582\n",
      "Subject 2, Epoch 117, Loss: 2.033454656600952, Final Batch Loss: 0.675179123878479\n",
      "Subject 2, Epoch 118, Loss: 2.2173982858657837, Final Batch Loss: 0.6353926658630371\n",
      "Subject 2, Epoch 119, Loss: 2.1808729767799377, Final Batch Loss: 0.7268814444541931\n",
      "Subject 2, Epoch 120, Loss: 2.027718186378479, Final Batch Loss: 0.6056825518608093\n",
      "Subject 2, Epoch 121, Loss: 2.0876176953315735, Final Batch Loss: 0.7298992872238159\n",
      "Subject 2, Epoch 122, Loss: 2.2462741136550903, Final Batch Loss: 0.8208169937133789\n",
      "Subject 2, Epoch 123, Loss: 2.0881690979003906, Final Batch Loss: 0.6354908347129822\n",
      "Subject 2, Epoch 124, Loss: 1.9877795577049255, Final Batch Loss: 0.6051667928695679\n",
      "Subject 2, Epoch 125, Loss: 2.1163793206214905, Final Batch Loss: 0.7006054520606995\n",
      "Subject 2, Epoch 126, Loss: 2.0865960717201233, Final Batch Loss: 0.6914849281311035\n",
      "Subject 2, Epoch 127, Loss: 2.0089266896247864, Final Batch Loss: 0.6321612000465393\n",
      "Subject 2, Epoch 128, Loss: 2.2477968335151672, Final Batch Loss: 0.8200752139091492\n",
      "Subject 2, Epoch 129, Loss: 1.9725859761238098, Final Batch Loss: 0.635651707649231\n",
      "Subject 2, Epoch 130, Loss: 1.975576639175415, Final Batch Loss: 0.6491912007331848\n",
      "Subject 2, Epoch 131, Loss: 2.0380000472068787, Final Batch Loss: 0.6881756782531738\n",
      "Subject 2, Epoch 132, Loss: 2.035768210887909, Final Batch Loss: 0.739571213722229\n",
      "Subject 2, Epoch 133, Loss: 2.0864592790603638, Final Batch Loss: 0.7458587288856506\n",
      "Subject 2, Epoch 134, Loss: 1.9756794571876526, Final Batch Loss: 0.6516256928443909\n",
      "Subject 2, Epoch 135, Loss: 1.851122796535492, Final Batch Loss: 0.6065405011177063\n",
      "Subject 2, Epoch 136, Loss: 1.9750458002090454, Final Batch Loss: 0.6917856335639954\n",
      "Subject 2, Epoch 137, Loss: 1.9224616885185242, Final Batch Loss: 0.6433773040771484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 138, Loss: 2.074581801891327, Final Batch Loss: 0.6747627854347229\n",
      "Subject 2, Epoch 139, Loss: 1.8790329694747925, Final Batch Loss: 0.6093389391899109\n",
      "Subject 2, Epoch 140, Loss: 1.9390790462493896, Final Batch Loss: 0.6837918162345886\n",
      "Subject 2, Epoch 141, Loss: 1.9558827877044678, Final Batch Loss: 0.6137177348136902\n",
      "Subject 2, Epoch 142, Loss: 1.7964918613433838, Final Batch Loss: 0.5743241310119629\n",
      "Subject 2, Epoch 143, Loss: 1.9256292581558228, Final Batch Loss: 0.6512365341186523\n",
      "Subject 2, Epoch 144, Loss: 1.824598640203476, Final Batch Loss: 0.645187258720398\n",
      "Subject 2, Epoch 145, Loss: 2.044796645641327, Final Batch Loss: 0.7398761510848999\n",
      "Subject 2, Epoch 146, Loss: 1.9003623127937317, Final Batch Loss: 0.5887568593025208\n",
      "Subject 2, Epoch 147, Loss: 1.731984794139862, Final Batch Loss: 0.5714403986930847\n",
      "Subject 2, Epoch 148, Loss: 1.8712618947029114, Final Batch Loss: 0.6301916837692261\n",
      "Subject 2, Epoch 149, Loss: 1.8304042220115662, Final Batch Loss: 0.5583184957504272\n",
      "Subject 2, Epoch 150, Loss: 1.879197359085083, Final Batch Loss: 0.6454187035560608\n",
      "Subject 2, Epoch 151, Loss: 1.736221730709076, Final Batch Loss: 0.5511860847473145\n",
      "Subject 2, Epoch 152, Loss: 1.7859852313995361, Final Batch Loss: 0.563500702381134\n",
      "Subject 2, Epoch 153, Loss: 1.8270245790481567, Final Batch Loss: 0.649860680103302\n",
      "Subject 2, Epoch 154, Loss: 1.8103051781654358, Final Batch Loss: 0.6690724492073059\n",
      "Subject 2, Epoch 155, Loss: 1.765152633190155, Final Batch Loss: 0.6542146801948547\n",
      "Subject 2, Epoch 156, Loss: 1.7092514038085938, Final Batch Loss: 0.5053475499153137\n",
      "Subject 2, Epoch 157, Loss: 1.6804033815860748, Final Batch Loss: 0.562358558177948\n",
      "Subject 2, Epoch 158, Loss: 1.738291084766388, Final Batch Loss: 0.5813551545143127\n",
      "Subject 2, Epoch 159, Loss: 1.8102350234985352, Final Batch Loss: 0.6813036799430847\n",
      "Subject 2, Epoch 160, Loss: 1.724522441625595, Final Batch Loss: 0.6515812873840332\n",
      "Subject 2, Epoch 161, Loss: 1.644555687904358, Final Batch Loss: 0.5746469497680664\n",
      "Subject 2, Epoch 162, Loss: 1.7898296117782593, Final Batch Loss: 0.6479392647743225\n",
      "Subject 2, Epoch 163, Loss: 1.72909414768219, Final Batch Loss: 0.5969957709312439\n",
      "Subject 2, Epoch 164, Loss: 1.75555020570755, Final Batch Loss: 0.49777930974960327\n",
      "Subject 2, Epoch 165, Loss: 1.6714885532855988, Final Batch Loss: 0.48287710547447205\n",
      "Subject 2, Epoch 166, Loss: 1.6677821576595306, Final Batch Loss: 0.5550769567489624\n",
      "Subject 2, Epoch 167, Loss: 1.603651523590088, Final Batch Loss: 0.5041700005531311\n",
      "Subject 2, Epoch 168, Loss: 1.5928909182548523, Final Batch Loss: 0.5272831320762634\n",
      "Subject 2, Epoch 169, Loss: 1.5856786668300629, Final Batch Loss: 0.4899134933948517\n",
      "Subject 2, Epoch 170, Loss: 1.5606203377246857, Final Batch Loss: 0.4365730583667755\n",
      "Subject 2, Epoch 171, Loss: 1.7013881206512451, Final Batch Loss: 0.777107298374176\n",
      "Subject 2, Epoch 172, Loss: 1.6320361196994781, Final Batch Loss: 0.4629000127315521\n",
      "Subject 2, Epoch 173, Loss: 1.7508018016815186, Final Batch Loss: 0.5846801400184631\n",
      "Subject 2, Epoch 174, Loss: 1.6639391779899597, Final Batch Loss: 0.5176592469215393\n",
      "Subject 2, Epoch 175, Loss: 1.5105614364147186, Final Batch Loss: 0.4423929750919342\n",
      "Subject 2, Epoch 176, Loss: 1.6726535558700562, Final Batch Loss: 0.5563810467720032\n",
      "Subject 2, Epoch 177, Loss: 1.54863041639328, Final Batch Loss: 0.47722333669662476\n",
      "Subject 2, Epoch 178, Loss: 1.660214513540268, Final Batch Loss: 0.47744813561439514\n",
      "Subject 2, Epoch 179, Loss: 1.650248408317566, Final Batch Loss: 0.49437904357910156\n",
      "Subject 2, Epoch 180, Loss: 1.4074255526065826, Final Batch Loss: 0.4138183295726776\n",
      "Subject 2, Epoch 181, Loss: 1.452596664428711, Final Batch Loss: 0.47768402099609375\n",
      "Subject 2, Epoch 182, Loss: 1.4851691126823425, Final Batch Loss: 0.3893703818321228\n",
      "Subject 2, Epoch 183, Loss: 1.4792286157608032, Final Batch Loss: 0.47277092933654785\n",
      "Subject 2, Epoch 184, Loss: 1.5564598739147186, Final Batch Loss: 0.48722121119499207\n",
      "Subject 2, Epoch 185, Loss: 1.392060399055481, Final Batch Loss: 0.3528873920440674\n",
      "Subject 2, Epoch 186, Loss: 1.5317546725273132, Final Batch Loss: 0.46785494685173035\n",
      "Subject 2, Epoch 187, Loss: 1.553238958120346, Final Batch Loss: 0.5530371069908142\n",
      "Subject 2, Epoch 188, Loss: 1.6385345458984375, Final Batch Loss: 0.5826287865638733\n",
      "Subject 2, Epoch 189, Loss: 1.5272749364376068, Final Batch Loss: 0.39785972237586975\n",
      "Subject 2, Epoch 190, Loss: 1.5713296830654144, Final Batch Loss: 0.45009851455688477\n",
      "Subject 2, Epoch 191, Loss: 1.4143275320529938, Final Batch Loss: 0.34599584341049194\n",
      "Subject 2, Epoch 192, Loss: 1.5392685532569885, Final Batch Loss: 0.6230179667472839\n",
      "Subject 2, Epoch 193, Loss: 1.3865077793598175, Final Batch Loss: 0.4680948853492737\n",
      "Subject 2, Epoch 194, Loss: 1.6255554556846619, Final Batch Loss: 0.5792562365531921\n",
      "Subject 2, Epoch 195, Loss: 1.5623177289962769, Final Batch Loss: 0.5257359743118286\n",
      "Subject 2, Epoch 196, Loss: 1.5169961154460907, Final Batch Loss: 0.5583564043045044\n",
      "Subject 2, Epoch 197, Loss: 1.5059645771980286, Final Batch Loss: 0.5645226240158081\n",
      "Subject 2, Epoch 198, Loss: 1.523563414812088, Final Batch Loss: 0.4455172121524811\n",
      "Subject 2, Epoch 199, Loss: 1.3850279748439789, Final Batch Loss: 0.4324323832988739\n",
      "Subject 2, Epoch 200, Loss: 1.3398919105529785, Final Batch Loss: 0.4492204785346985\n",
      "Subject 2, Epoch 201, Loss: 1.5753128826618195, Final Batch Loss: 0.5616214871406555\n",
      "Subject 2, Epoch 202, Loss: 1.3777190446853638, Final Batch Loss: 0.43502968549728394\n",
      "Subject 2, Epoch 203, Loss: 1.407241553068161, Final Batch Loss: 0.49126675724983215\n",
      "Subject 2, Epoch 204, Loss: 1.4475643336772919, Final Batch Loss: 0.49507445096969604\n",
      "Subject 2, Epoch 205, Loss: 1.2954781353473663, Final Batch Loss: 0.3143634498119354\n",
      "Subject 2, Epoch 206, Loss: 1.4424920082092285, Final Batch Loss: 0.4134342074394226\n",
      "Subject 2, Epoch 207, Loss: 1.4068565666675568, Final Batch Loss: 0.47767165303230286\n",
      "Subject 2, Epoch 208, Loss: 1.4934353828430176, Final Batch Loss: 0.5585850477218628\n",
      "Subject 2, Epoch 209, Loss: 1.3657892346382141, Final Batch Loss: 0.4838401973247528\n",
      "Subject 2, Epoch 210, Loss: 1.2758364081382751, Final Batch Loss: 0.4053014814853668\n",
      "Subject 2, Epoch 211, Loss: 1.349419504404068, Final Batch Loss: 0.3375908434391022\n",
      "Subject 2, Epoch 212, Loss: 1.532756894826889, Final Batch Loss: 0.48825350403785706\n",
      "Subject 2, Epoch 213, Loss: 1.4990180730819702, Final Batch Loss: 0.3982093334197998\n",
      "Subject 2, Epoch 214, Loss: 1.4186117351055145, Final Batch Loss: 0.48452451825141907\n",
      "Subject 2, Epoch 215, Loss: 1.5423787534236908, Final Batch Loss: 0.599916398525238\n",
      "Subject 2, Epoch 216, Loss: 1.4548842310905457, Final Batch Loss: 0.4534091353416443\n",
      "Subject 2, Epoch 217, Loss: 1.3724806904792786, Final Batch Loss: 0.43382543325424194\n",
      "Subject 2, Epoch 218, Loss: 1.287910670042038, Final Batch Loss: 0.46100637316703796\n",
      "Subject 2, Epoch 219, Loss: 1.236116498708725, Final Batch Loss: 0.3851769268512726\n",
      "Subject 2, Epoch 220, Loss: 1.4072812795639038, Final Batch Loss: 0.49332699179649353\n",
      "Subject 2, Epoch 221, Loss: 1.3801597654819489, Final Batch Loss: 0.5216555595397949\n",
      "Subject 2, Epoch 222, Loss: 1.478363960981369, Final Batch Loss: 0.45801329612731934\n",
      "Subject 2, Epoch 223, Loss: 1.3422276377677917, Final Batch Loss: 0.43334946036338806\n",
      "Subject 2, Epoch 224, Loss: 1.385967195034027, Final Batch Loss: 0.4389120936393738\n",
      "Subject 2, Epoch 225, Loss: 1.4163661301136017, Final Batch Loss: 0.5368475317955017\n",
      "Subject 2, Epoch 226, Loss: 1.3888276815414429, Final Batch Loss: 0.4236360192298889\n",
      "Subject 2, Epoch 227, Loss: 1.4892305135726929, Final Batch Loss: 0.5947931408882141\n",
      "Subject 2, Epoch 228, Loss: 1.3927838802337646, Final Batch Loss: 0.4021250307559967\n",
      "Subject 2, Epoch 229, Loss: 1.2836301028728485, Final Batch Loss: 0.32697606086730957\n",
      "Subject 2, Epoch 230, Loss: 1.3444243967533112, Final Batch Loss: 0.4865305721759796\n",
      "Subject 2, Epoch 231, Loss: 1.2615310847759247, Final Batch Loss: 0.3808092176914215\n",
      "Subject 2, Epoch 232, Loss: 1.3652319312095642, Final Batch Loss: 0.4335779547691345\n",
      "Subject 2, Epoch 233, Loss: 1.4040669798851013, Final Batch Loss: 0.5735591650009155\n",
      "Subject 2, Epoch 234, Loss: 1.1686882078647614, Final Batch Loss: 0.33823513984680176\n",
      "Subject 2, Epoch 235, Loss: 1.2763640582561493, Final Batch Loss: 0.5313938856124878\n",
      "Subject 2, Epoch 236, Loss: 1.354407638311386, Final Batch Loss: 0.46545371413230896\n",
      "Subject 2, Epoch 237, Loss: 1.1927208006381989, Final Batch Loss: 0.4040760099887848\n",
      "Subject 2, Epoch 238, Loss: 1.3199939727783203, Final Batch Loss: 0.3877297341823578\n",
      "Subject 2, Epoch 239, Loss: 1.3353614509105682, Final Batch Loss: 0.42469069361686707\n",
      "Subject 2, Epoch 240, Loss: 1.3321378231048584, Final Batch Loss: 0.4588134288787842\n",
      "Subject 2, Epoch 241, Loss: 1.4383613169193268, Final Batch Loss: 0.4338997006416321\n",
      "Subject 2, Epoch 242, Loss: 1.299458771944046, Final Batch Loss: 0.35285598039627075\n",
      "Subject 2, Epoch 243, Loss: 1.350016325712204, Final Batch Loss: 0.4314952492713928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 244, Loss: 1.338435798883438, Final Batch Loss: 0.5779737234115601\n",
      "Subject 2, Epoch 245, Loss: 1.3113699555397034, Final Batch Loss: 0.4597913324832916\n",
      "Subject 2, Epoch 246, Loss: 1.3856543004512787, Final Batch Loss: 0.5621255040168762\n",
      "Subject 2, Epoch 247, Loss: 1.4138449132442474, Final Batch Loss: 0.4504995048046112\n",
      "Subject 2, Epoch 248, Loss: 1.4366622865200043, Final Batch Loss: 0.5320540070533752\n",
      "Subject 2, Epoch 249, Loss: 1.3275402784347534, Final Batch Loss: 0.5257337093353271\n",
      "Subject 2, Epoch 250, Loss: 1.3393622934818268, Final Batch Loss: 0.41699737310409546\n",
      "Subject 2, Epoch 251, Loss: 1.3860108256340027, Final Batch Loss: 0.619540274143219\n",
      "Subject 2, Epoch 252, Loss: 1.2019188702106476, Final Batch Loss: 0.43985483050346375\n",
      "Subject 2, Epoch 253, Loss: 1.2395340502262115, Final Batch Loss: 0.3961951434612274\n",
      "Subject 2, Epoch 254, Loss: 1.2919680774211884, Final Batch Loss: 0.46011117100715637\n",
      "Subject 2, Epoch 255, Loss: 1.3979004919528961, Final Batch Loss: 0.5671670436859131\n",
      "Subject 2, Epoch 256, Loss: 1.2031365931034088, Final Batch Loss: 0.40834760665893555\n",
      "Subject 2, Epoch 257, Loss: 1.210512101650238, Final Batch Loss: 0.38413068652153015\n",
      "Subject 2, Epoch 258, Loss: 1.2671200037002563, Final Batch Loss: 0.41199836134910583\n",
      "Subject 2, Epoch 259, Loss: 1.261057198047638, Final Batch Loss: 0.3444964289665222\n",
      "Subject 2, Epoch 260, Loss: 1.220877230167389, Final Batch Loss: 0.2878568470478058\n",
      "Subject 2, Epoch 261, Loss: 1.286393791437149, Final Batch Loss: 0.43338528275489807\n",
      "Subject 2, Epoch 262, Loss: 1.1202150583267212, Final Batch Loss: 0.3585282862186432\n",
      "Subject 2, Epoch 263, Loss: 1.1521247625350952, Final Batch Loss: 0.3578793704509735\n",
      "Subject 2, Epoch 264, Loss: 1.2876489162445068, Final Batch Loss: 0.4501262605190277\n",
      "Subject 2, Epoch 265, Loss: 1.1599912345409393, Final Batch Loss: 0.3208411633968353\n",
      "Subject 2, Epoch 266, Loss: 1.2919494211673737, Final Batch Loss: 0.32567912340164185\n",
      "Subject 2, Epoch 267, Loss: 1.209973156452179, Final Batch Loss: 0.34998682141304016\n",
      "Subject 2, Epoch 268, Loss: 1.2944851219654083, Final Batch Loss: 0.44921237230300903\n",
      "Subject 2, Epoch 269, Loss: 1.4122099876403809, Final Batch Loss: 0.506130039691925\n",
      "Subject 2, Epoch 270, Loss: 1.3113502860069275, Final Batch Loss: 0.46173974871635437\n",
      "Subject 2, Epoch 271, Loss: 1.1474508941173553, Final Batch Loss: 0.3348705768585205\n",
      "Subject 2, Epoch 272, Loss: 1.228093147277832, Final Batch Loss: 0.4500877857208252\n",
      "Subject 2, Epoch 273, Loss: 1.398424357175827, Final Batch Loss: 0.39026397466659546\n",
      "Subject 2, Epoch 274, Loss: 1.3020351231098175, Final Batch Loss: 0.49722784757614136\n",
      "Subject 2, Epoch 275, Loss: 1.1034469604492188, Final Batch Loss: 0.2645834982395172\n",
      "Subject 2, Epoch 276, Loss: 1.2332529127597809, Final Batch Loss: 0.3816068172454834\n",
      "Subject 2, Epoch 277, Loss: 1.1135854125022888, Final Batch Loss: 0.28241053223609924\n",
      "Subject 2, Epoch 278, Loss: 1.2739664018154144, Final Batch Loss: 0.43510350584983826\n",
      "Subject 2, Epoch 279, Loss: 1.2501605153083801, Final Batch Loss: 0.37028709053993225\n",
      "Subject 2, Epoch 280, Loss: 1.2503679394721985, Final Batch Loss: 0.4077506959438324\n",
      "Subject 2, Epoch 281, Loss: 1.3283920884132385, Final Batch Loss: 0.5420550107955933\n",
      "Subject 2, Epoch 282, Loss: 1.3254052698612213, Final Batch Loss: 0.5607911348342896\n",
      "Subject 2, Epoch 283, Loss: 1.106879860162735, Final Batch Loss: 0.3396309018135071\n",
      "Subject 2, Epoch 284, Loss: 1.1935335993766785, Final Batch Loss: 0.4635184109210968\n",
      "Subject 2, Epoch 285, Loss: 1.182740181684494, Final Batch Loss: 0.3518006205558777\n",
      "Subject 2, Epoch 286, Loss: 1.2349857985973358, Final Batch Loss: 0.3736932873725891\n",
      "Subject 2, Epoch 287, Loss: 1.130966305732727, Final Batch Loss: 0.37937504053115845\n",
      "Subject 2, Epoch 288, Loss: 1.2035165131092072, Final Batch Loss: 0.37409907579421997\n",
      "Subject 2, Epoch 289, Loss: 1.1443856060504913, Final Batch Loss: 0.41503939032554626\n",
      "Subject 2, Epoch 290, Loss: 1.2458311021327972, Final Batch Loss: 0.418277770280838\n",
      "Subject 2, Epoch 291, Loss: 1.1050556898117065, Final Batch Loss: 0.4116840660572052\n",
      "Subject 2, Epoch 292, Loss: 1.2511992752552032, Final Batch Loss: 0.4526994526386261\n",
      "Subject 2, Epoch 293, Loss: 1.2106512486934662, Final Batch Loss: 0.3662910759449005\n",
      "Subject 2, Epoch 294, Loss: 1.3187161982059479, Final Batch Loss: 0.4513769745826721\n",
      "Subject 2, Epoch 295, Loss: 1.0443331003189087, Final Batch Loss: 0.30656224489212036\n",
      "Subject 2, Epoch 296, Loss: 1.235345184803009, Final Batch Loss: 0.3512527346611023\n",
      "Subject 2, Epoch 297, Loss: 1.113592118024826, Final Batch Loss: 0.3295350670814514\n",
      "Subject 2, Epoch 298, Loss: 1.180536150932312, Final Batch Loss: 0.36970970034599304\n",
      "Subject 2, Epoch 299, Loss: 1.164515197277069, Final Batch Loss: 0.37435659766197205\n",
      "Subject 2, Epoch 300, Loss: 1.2032048106193542, Final Batch Loss: 0.44241002202033997\n",
      "Subject 2, Epoch 301, Loss: 1.3950050473213196, Final Batch Loss: 0.5232414603233337\n",
      "Subject 2, Epoch 302, Loss: 1.16367569565773, Final Batch Loss: 0.3860827088356018\n",
      "Subject 2, Epoch 303, Loss: 1.270507663488388, Final Batch Loss: 0.500799298286438\n",
      "Subject 2, Epoch 304, Loss: 0.9990591108798981, Final Batch Loss: 0.297794908285141\n",
      "Subject 2, Epoch 305, Loss: 1.092736393213272, Final Batch Loss: 0.3230014443397522\n",
      "Subject 2, Epoch 306, Loss: 1.2575913965702057, Final Batch Loss: 0.4292246699333191\n",
      "Subject 2, Epoch 307, Loss: 1.1704826056957245, Final Batch Loss: 0.328258216381073\n",
      "Subject 2, Epoch 308, Loss: 1.1554292738437653, Final Batch Loss: 0.42592498660087585\n",
      "Subject 2, Epoch 309, Loss: 1.0505433678627014, Final Batch Loss: 0.3908948302268982\n",
      "Subject 2, Epoch 310, Loss: 1.1247640252113342, Final Batch Loss: 0.32968610525131226\n",
      "Subject 2, Epoch 311, Loss: 1.2699192464351654, Final Batch Loss: 0.5148081183433533\n",
      "Subject 2, Epoch 312, Loss: 1.3320093154907227, Final Batch Loss: 0.46953096985816956\n",
      "Subject 2, Epoch 313, Loss: 1.1172630190849304, Final Batch Loss: 0.37265142798423767\n",
      "Subject 2, Epoch 314, Loss: 1.1099388301372528, Final Batch Loss: 0.3637407124042511\n",
      "Subject 2, Epoch 315, Loss: 1.119554042816162, Final Batch Loss: 0.3972802460193634\n",
      "Subject 2, Epoch 316, Loss: 1.16962730884552, Final Batch Loss: 0.5124559998512268\n",
      "Subject 2, Epoch 317, Loss: 1.0480003356933594, Final Batch Loss: 0.28920978307724\n",
      "Subject 2, Epoch 318, Loss: 1.1005540788173676, Final Batch Loss: 0.3633311688899994\n",
      "Subject 2, Epoch 319, Loss: 1.204138159751892, Final Batch Loss: 0.43111780285835266\n",
      "Subject 2, Epoch 320, Loss: 1.0994505882263184, Final Batch Loss: 0.3813977539539337\n",
      "Subject 2, Epoch 321, Loss: 1.0618943870067596, Final Batch Loss: 0.3556045889854431\n",
      "Subject 2, Epoch 322, Loss: 1.097271740436554, Final Batch Loss: 0.4014298617839813\n",
      "Subject 2, Epoch 323, Loss: 1.25629061460495, Final Batch Loss: 0.43073299527168274\n",
      "Subject 2, Epoch 324, Loss: 1.1919932961463928, Final Batch Loss: 0.3979972004890442\n",
      "Subject 2, Epoch 325, Loss: 1.106046438217163, Final Batch Loss: 0.4315703511238098\n",
      "Subject 2, Epoch 326, Loss: 1.1275535225868225, Final Batch Loss: 0.3771539330482483\n",
      "Subject 2, Epoch 327, Loss: 1.0309084355831146, Final Batch Loss: 0.325214147567749\n",
      "Subject 2, Epoch 328, Loss: 1.1035353541374207, Final Batch Loss: 0.3742482364177704\n",
      "Subject 2, Epoch 329, Loss: 1.1467492878437042, Final Batch Loss: 0.41041502356529236\n",
      "Subject 2, Epoch 330, Loss: 1.1814264357089996, Final Batch Loss: 0.44041600823402405\n",
      "Subject 2, Epoch 331, Loss: 1.0352199077606201, Final Batch Loss: 0.33308663964271545\n",
      "Subject 2, Epoch 332, Loss: 0.9625077843666077, Final Batch Loss: 0.2950733006000519\n",
      "Subject 2, Epoch 333, Loss: 1.0927065014839172, Final Batch Loss: 0.4019033908843994\n",
      "Subject 2, Epoch 334, Loss: 0.9973694682121277, Final Batch Loss: 0.33689451217651367\n",
      "Subject 2, Epoch 335, Loss: 1.228822499513626, Final Batch Loss: 0.4298986792564392\n",
      "Subject 2, Epoch 336, Loss: 1.0828560888767242, Final Batch Loss: 0.37273532152175903\n",
      "Subject 2, Epoch 337, Loss: 0.9403061270713806, Final Batch Loss: 0.34748801589012146\n",
      "Subject 2, Epoch 338, Loss: 1.1605766117572784, Final Batch Loss: 0.4660210907459259\n",
      "Subject 2, Epoch 339, Loss: 1.077273041009903, Final Batch Loss: 0.35243239998817444\n",
      "Subject 2, Epoch 340, Loss: 1.0614731907844543, Final Batch Loss: 0.41004154086112976\n",
      "Subject 2, Epoch 341, Loss: 1.0357298851013184, Final Batch Loss: 0.3122267723083496\n",
      "Subject 2, Epoch 342, Loss: 1.2100869417190552, Final Batch Loss: 0.2960386872291565\n",
      "Subject 2, Epoch 343, Loss: 0.9768969416618347, Final Batch Loss: 0.26985782384872437\n",
      "Subject 2, Epoch 344, Loss: 1.2576690018177032, Final Batch Loss: 0.5492739677429199\n",
      "Subject 2, Epoch 345, Loss: 1.0909885168075562, Final Batch Loss: 0.4468647539615631\n",
      "Subject 2, Epoch 346, Loss: 1.1634178757667542, Final Batch Loss: 0.38619428873062134\n",
      "Subject 2, Epoch 347, Loss: 1.0227710902690887, Final Batch Loss: 0.3310072720050812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 348, Loss: 1.0106153786182404, Final Batch Loss: 0.2526012361049652\n",
      "Subject 2, Epoch 349, Loss: 1.073372632265091, Final Batch Loss: 0.2975444197654724\n",
      "Subject 2, Epoch 350, Loss: 1.223115861415863, Final Batch Loss: 0.47839805483818054\n",
      "Subject 2, Epoch 351, Loss: 0.8937316238880157, Final Batch Loss: 0.2758445143699646\n",
      "Subject 2, Epoch 352, Loss: 0.9410243034362793, Final Batch Loss: 0.27471643686294556\n",
      "Subject 2, Epoch 353, Loss: 1.2034127116203308, Final Batch Loss: 0.44739073514938354\n",
      "Subject 2, Epoch 354, Loss: 0.9590265899896622, Final Batch Loss: 0.3411681354045868\n",
      "Subject 2, Epoch 355, Loss: 1.0136355608701706, Final Batch Loss: 0.24544699490070343\n",
      "Subject 2, Epoch 356, Loss: 1.0087331235408783, Final Batch Loss: 0.3036370277404785\n",
      "Subject 2, Epoch 357, Loss: 1.1207198202610016, Final Batch Loss: 0.3401428759098053\n",
      "Subject 2, Epoch 358, Loss: 1.067801058292389, Final Batch Loss: 0.3955991566181183\n",
      "Subject 2, Epoch 359, Loss: 1.1257125735282898, Final Batch Loss: 0.45671015977859497\n",
      "Subject 2, Epoch 360, Loss: 1.2200253903865814, Final Batch Loss: 0.4187997281551361\n",
      "Subject 2, Epoch 361, Loss: 1.1932156682014465, Final Batch Loss: 0.43660768866539\n",
      "Subject 2, Epoch 362, Loss: 1.030873864889145, Final Batch Loss: 0.3136138617992401\n",
      "Subject 2, Epoch 363, Loss: 0.9574392735958099, Final Batch Loss: 0.2676282823085785\n",
      "Subject 2, Epoch 364, Loss: 1.0240309536457062, Final Batch Loss: 0.2900693714618683\n",
      "Subject 2, Epoch 365, Loss: 0.9962725639343262, Final Batch Loss: 0.3074275851249695\n",
      "Subject 2, Epoch 366, Loss: 1.007552593946457, Final Batch Loss: 0.2599044442176819\n",
      "Subject 2, Epoch 367, Loss: 1.1483906507492065, Final Batch Loss: 0.42515134811401367\n",
      "Subject 2, Epoch 368, Loss: 1.1480298936367035, Final Batch Loss: 0.4892970621585846\n",
      "Subject 2, Epoch 369, Loss: 0.9970743656158447, Final Batch Loss: 0.3419199585914612\n",
      "Subject 2, Epoch 370, Loss: 1.0050565302371979, Final Batch Loss: 0.3282339572906494\n",
      "Subject 2, Epoch 371, Loss: 0.9818074405193329, Final Batch Loss: 0.2986062169075012\n",
      "Subject 2, Epoch 372, Loss: 0.8926075398921967, Final Batch Loss: 0.3005581796169281\n",
      "Subject 2, Epoch 373, Loss: 1.012380599975586, Final Batch Loss: 0.34338217973709106\n",
      "Subject 2, Epoch 374, Loss: 1.0223776698112488, Final Batch Loss: 0.37537047266960144\n",
      "Subject 2, Epoch 375, Loss: 0.9968719482421875, Final Batch Loss: 0.33885934948921204\n",
      "Subject 2, Epoch 376, Loss: 0.9484020471572876, Final Batch Loss: 0.33148401975631714\n",
      "Subject 2, Epoch 377, Loss: 1.1243543922901154, Final Batch Loss: 0.48246636986732483\n",
      "Subject 2, Epoch 378, Loss: 0.997704952955246, Final Batch Loss: 0.3145192265510559\n",
      "Subject 2, Epoch 379, Loss: 0.9357415735721588, Final Batch Loss: 0.3260217607021332\n",
      "Subject 2, Epoch 380, Loss: 1.0162921845912933, Final Batch Loss: 0.3585927188396454\n",
      "Subject 2, Epoch 381, Loss: 1.0082788169384003, Final Batch Loss: 0.3137379288673401\n",
      "Subject 2, Epoch 382, Loss: 0.9700040221214294, Final Batch Loss: 0.29049113392829895\n",
      "Subject 2, Epoch 383, Loss: 0.9587863683700562, Final Batch Loss: 0.3054066300392151\n",
      "Subject 2, Epoch 384, Loss: 0.9143157005310059, Final Batch Loss: 0.3133540153503418\n",
      "Subject 2, Epoch 385, Loss: 0.9484087824821472, Final Batch Loss: 0.31666308641433716\n",
      "Subject 2, Epoch 386, Loss: 0.9554329514503479, Final Batch Loss: 0.37983304262161255\n",
      "Subject 2, Epoch 387, Loss: 0.957377016544342, Final Batch Loss: 0.2722285985946655\n",
      "Subject 2, Epoch 388, Loss: 1.0213563144207, Final Batch Loss: 0.33221063017845154\n",
      "Subject 2, Epoch 389, Loss: 0.9733195304870605, Final Batch Loss: 0.3166356086730957\n",
      "Subject 2, Epoch 390, Loss: 1.0170965194702148, Final Batch Loss: 0.34269851446151733\n",
      "Subject 2, Epoch 391, Loss: 1.0055842101573944, Final Batch Loss: 0.3120254874229431\n",
      "Subject 2, Epoch 392, Loss: 1.010406345129013, Final Batch Loss: 0.41241541504859924\n",
      "Subject 2, Epoch 393, Loss: 1.0754042267799377, Final Batch Loss: 0.32696768641471863\n",
      "Subject 2, Epoch 394, Loss: 1.048655092716217, Final Batch Loss: 0.42051368951797485\n",
      "Subject 2, Epoch 395, Loss: 0.9533721804618835, Final Batch Loss: 0.35793107748031616\n",
      "Subject 2, Epoch 396, Loss: 0.8630794882774353, Final Batch Loss: 0.3474399447441101\n",
      "Subject 2, Epoch 397, Loss: 0.8964678645133972, Final Batch Loss: 0.2831118404865265\n",
      "Subject 2, Epoch 398, Loss: 0.9685174524784088, Final Batch Loss: 0.2781199514865875\n",
      "Subject 2, Epoch 399, Loss: 0.9557002186775208, Final Batch Loss: 0.2967509329319\n",
      "Subject 2, Epoch 400, Loss: 0.8563568741083145, Final Batch Loss: 0.19036726653575897\n",
      "Subject 2, Epoch 401, Loss: 1.145099550485611, Final Batch Loss: 0.395203560590744\n",
      "Subject 2, Epoch 402, Loss: 0.9013425409793854, Final Batch Loss: 0.3049156665802002\n",
      "Subject 2, Epoch 403, Loss: 0.9898049235343933, Final Batch Loss: 0.28577786684036255\n",
      "Subject 2, Epoch 404, Loss: 1.026243507862091, Final Batch Loss: 0.3490210473537445\n",
      "Subject 2, Epoch 405, Loss: 0.9928411841392517, Final Batch Loss: 0.292045533657074\n",
      "Subject 2, Epoch 406, Loss: 0.8951188623905182, Final Batch Loss: 0.3327731192111969\n",
      "Subject 2, Epoch 407, Loss: 1.096649020910263, Final Batch Loss: 0.4925456643104553\n",
      "Subject 2, Epoch 408, Loss: 1.0860965549945831, Final Batch Loss: 0.46261629462242126\n",
      "Subject 2, Epoch 409, Loss: 1.0142786800861359, Final Batch Loss: 0.3565519154071808\n",
      "Subject 2, Epoch 410, Loss: 1.055991381406784, Final Batch Loss: 0.3804077208042145\n",
      "Subject 2, Epoch 411, Loss: 0.974480539560318, Final Batch Loss: 0.36262568831443787\n",
      "Subject 2, Epoch 412, Loss: 0.9257352352142334, Final Batch Loss: 0.2760147154331207\n",
      "Subject 2, Epoch 413, Loss: 0.9821940958499908, Final Batch Loss: 0.2789841294288635\n",
      "Subject 2, Epoch 414, Loss: 0.8973938822746277, Final Batch Loss: 0.2546089291572571\n",
      "Subject 2, Epoch 415, Loss: 1.0031688809394836, Final Batch Loss: 0.36536484956741333\n",
      "Subject 2, Epoch 416, Loss: 0.9996740520000458, Final Batch Loss: 0.377337247133255\n",
      "Subject 2, Epoch 417, Loss: 0.9637929201126099, Final Batch Loss: 0.2654436528682709\n",
      "Subject 2, Epoch 418, Loss: 1.065063625574112, Final Batch Loss: 0.4342624545097351\n",
      "Subject 2, Epoch 419, Loss: 0.8388054221868515, Final Batch Loss: 0.23028568923473358\n",
      "Subject 2, Epoch 420, Loss: 0.9792667627334595, Final Batch Loss: 0.28812864422798157\n",
      "Subject 2, Epoch 421, Loss: 0.8960687965154648, Final Batch Loss: 0.22947804629802704\n",
      "Subject 2, Epoch 422, Loss: 0.9280804395675659, Final Batch Loss: 0.2766241431236267\n",
      "Subject 2, Epoch 423, Loss: 0.8737048804759979, Final Batch Loss: 0.287803590297699\n",
      "Subject 2, Epoch 424, Loss: 1.0175541043281555, Final Batch Loss: 0.28356966376304626\n",
      "Subject 2, Epoch 425, Loss: 1.0205564498901367, Final Batch Loss: 0.33361005783081055\n",
      "Subject 2, Epoch 426, Loss: 0.9847115278244019, Final Batch Loss: 0.3942830562591553\n",
      "Subject 2, Epoch 427, Loss: 0.8673492223024368, Final Batch Loss: 0.24046443402767181\n",
      "Subject 2, Epoch 428, Loss: 0.9011274576187134, Final Batch Loss: 0.28078365325927734\n",
      "Subject 2, Epoch 429, Loss: 0.8942040801048279, Final Batch Loss: 0.30452489852905273\n",
      "Subject 2, Epoch 430, Loss: 0.9806360304355621, Final Batch Loss: 0.3847176730632782\n",
      "Subject 2, Epoch 431, Loss: 0.9384550750255585, Final Batch Loss: 0.30812621116638184\n",
      "Subject 2, Epoch 432, Loss: 1.0131190121173859, Final Batch Loss: 0.31801339983940125\n",
      "Subject 2, Epoch 433, Loss: 1.0082902610301971, Final Batch Loss: 0.33810028433799744\n",
      "Subject 2, Epoch 434, Loss: 1.0384730696678162, Final Batch Loss: 0.4553154408931732\n",
      "Subject 2, Epoch 435, Loss: 0.857393354177475, Final Batch Loss: 0.25913435220718384\n",
      "Subject 2, Epoch 436, Loss: 1.0359777510166168, Final Batch Loss: 0.37340542674064636\n",
      "Subject 2, Epoch 437, Loss: 0.8670168817043304, Final Batch Loss: 0.27774712443351746\n",
      "Subject 2, Epoch 438, Loss: 0.884910985827446, Final Batch Loss: 0.23907871544361115\n",
      "Subject 2, Epoch 439, Loss: 0.9148950278759003, Final Batch Loss: 0.28056782484054565\n",
      "Subject 2, Epoch 440, Loss: 0.8855130076408386, Final Batch Loss: 0.2731260061264038\n",
      "Subject 2, Epoch 441, Loss: 1.0172860622406006, Final Batch Loss: 0.354647159576416\n",
      "Subject 2, Epoch 442, Loss: 0.8559065610170364, Final Batch Loss: 0.3382302224636078\n",
      "Subject 2, Epoch 443, Loss: 0.8223039209842682, Final Batch Loss: 0.2954849898815155\n",
      "Subject 2, Epoch 444, Loss: 0.9374906420707703, Final Batch Loss: 0.26093214750289917\n",
      "Subject 2, Epoch 445, Loss: 1.044337123632431, Final Batch Loss: 0.314057856798172\n",
      "Subject 2, Epoch 446, Loss: 0.9062923789024353, Final Batch Loss: 0.33933594822883606\n",
      "Subject 2, Epoch 447, Loss: 0.8300045877695084, Final Batch Loss: 0.24448208510875702\n",
      "Subject 2, Epoch 448, Loss: 0.98057821393013, Final Batch Loss: 0.27988380193710327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 449, Loss: 0.9230916798114777, Final Batch Loss: 0.3784787058830261\n",
      "Subject 2, Epoch 450, Loss: 1.0256968140602112, Final Batch Loss: 0.38767513632774353\n",
      "Subject 2, Epoch 451, Loss: 0.9385358542203903, Final Batch Loss: 0.25356027483940125\n",
      "Subject 2, Epoch 452, Loss: 0.8964231461286545, Final Batch Loss: 0.23948706686496735\n",
      "Subject 2, Epoch 453, Loss: 0.7935964614152908, Final Batch Loss: 0.2349218875169754\n",
      "Subject 2, Epoch 454, Loss: 0.9060966372489929, Final Batch Loss: 0.3041512966156006\n",
      "Subject 2, Epoch 455, Loss: 0.8571846783161163, Final Batch Loss: 0.2836907207965851\n",
      "Subject 2, Epoch 456, Loss: 0.9039462208747864, Final Batch Loss: 0.34395211935043335\n",
      "Subject 2, Epoch 457, Loss: 0.9197153449058533, Final Batch Loss: 0.35263997316360474\n",
      "Subject 2, Epoch 458, Loss: 0.7921563386917114, Final Batch Loss: 0.24323828518390656\n",
      "Subject 2, Epoch 459, Loss: 0.9255800545215607, Final Batch Loss: 0.28583255410194397\n",
      "Subject 2, Epoch 460, Loss: 0.911569744348526, Final Batch Loss: 0.33271297812461853\n",
      "Subject 2, Epoch 461, Loss: 0.9239783883094788, Final Batch Loss: 0.3618571162223816\n",
      "Subject 2, Epoch 462, Loss: 0.805174857378006, Final Batch Loss: 0.22399523854255676\n",
      "Subject 2, Epoch 463, Loss: 0.9692251682281494, Final Batch Loss: 0.36954164505004883\n",
      "Subject 2, Epoch 464, Loss: 0.7765410244464874, Final Batch Loss: 0.23567688465118408\n",
      "Subject 2, Epoch 465, Loss: 0.7856884598731995, Final Batch Loss: 0.29146885871887207\n",
      "Subject 2, Epoch 466, Loss: 0.8739143908023834, Final Batch Loss: 0.271506130695343\n",
      "Subject 2, Epoch 467, Loss: 0.8999714851379395, Final Batch Loss: 0.3364050090312958\n",
      "Subject 2, Epoch 468, Loss: 0.9435339868068695, Final Batch Loss: 0.36306336522102356\n",
      "Subject 2, Epoch 469, Loss: 0.8356862515211105, Final Batch Loss: 0.2339124232530594\n",
      "Subject 2, Epoch 470, Loss: 0.8703592419624329, Final Batch Loss: 0.2529158890247345\n",
      "Subject 2, Epoch 471, Loss: 0.8411063849925995, Final Batch Loss: 0.31785255670547485\n",
      "Subject 2, Epoch 472, Loss: 0.9216093569993973, Final Batch Loss: 0.23554874956607819\n",
      "Subject 2, Epoch 473, Loss: 0.9187298715114594, Final Batch Loss: 0.36957961320877075\n",
      "Subject 2, Epoch 474, Loss: 0.9778345823287964, Final Batch Loss: 0.35224753618240356\n",
      "Subject 2, Epoch 475, Loss: 0.8677061200141907, Final Batch Loss: 0.28205665946006775\n",
      "Subject 2, Epoch 476, Loss: 0.9882474541664124, Final Batch Loss: 0.45316454768180847\n",
      "Subject 2, Epoch 477, Loss: 0.8129586428403854, Final Batch Loss: 0.22445662319660187\n",
      "Subject 2, Epoch 478, Loss: 0.8376620560884476, Final Batch Loss: 0.24171103537082672\n",
      "Subject 2, Epoch 479, Loss: 0.7709270864725113, Final Batch Loss: 0.19747157394886017\n",
      "Subject 2, Epoch 480, Loss: 0.992710679769516, Final Batch Loss: 0.3258015513420105\n",
      "Subject 2, Epoch 481, Loss: 0.910579651594162, Final Batch Loss: 0.3543759286403656\n",
      "Subject 2, Epoch 482, Loss: 0.9134616553783417, Final Batch Loss: 0.26982706785202026\n",
      "Subject 2, Epoch 483, Loss: 0.8419911563396454, Final Batch Loss: 0.264845073223114\n",
      "Subject 2, Epoch 484, Loss: 0.9761422872543335, Final Batch Loss: 0.39534443616867065\n",
      "Subject 2, Epoch 485, Loss: 0.8893248438835144, Final Batch Loss: 0.248521089553833\n",
      "Subject 2, Epoch 486, Loss: 0.8417667895555496, Final Batch Loss: 0.31807902455329895\n",
      "Subject 2, Epoch 487, Loss: 0.9716908037662506, Final Batch Loss: 0.32438716292381287\n",
      "Subject 2, Epoch 488, Loss: 0.8442756533622742, Final Batch Loss: 0.26360824704170227\n",
      "Subject 2, Epoch 489, Loss: 0.873092383146286, Final Batch Loss: 0.3414061665534973\n",
      "Subject 2, Epoch 490, Loss: 0.8771263957023621, Final Batch Loss: 0.3625662624835968\n",
      "Subject 2, Epoch 491, Loss: 1.072713315486908, Final Batch Loss: 0.45076799392700195\n",
      "Subject 2, Epoch 492, Loss: 0.8787320256233215, Final Batch Loss: 0.25763171911239624\n",
      "Subject 2, Epoch 493, Loss: 0.9017511159181595, Final Batch Loss: 0.34116271138191223\n",
      "Subject 2, Epoch 494, Loss: 0.9246107935905457, Final Batch Loss: 0.26754745841026306\n",
      "Subject 2, Epoch 495, Loss: 0.9552015662193298, Final Batch Loss: 0.3121553063392639\n",
      "Subject 2, Epoch 496, Loss: 0.8825221359729767, Final Batch Loss: 0.2721971571445465\n",
      "Subject 2, Epoch 497, Loss: 0.799703061580658, Final Batch Loss: 0.2827783524990082\n",
      "Subject 2, Epoch 498, Loss: 0.8893662989139557, Final Batch Loss: 0.29975754022598267\n",
      "Subject 2, Epoch 499, Loss: 0.7851334065198898, Final Batch Loss: 0.2837853729724884\n",
      "Subject 2, Epoch 500, Loss: 1.104341745376587, Final Batch Loss: 0.4691554009914398\n",
      "Subject 2, Epoch 501, Loss: 0.8489076197147369, Final Batch Loss: 0.2927111089229584\n",
      "Subject 2, Epoch 502, Loss: 0.8197522759437561, Final Batch Loss: 0.25897231698036194\n",
      "Subject 2, Epoch 503, Loss: 0.8731993436813354, Final Batch Loss: 0.3690152168273926\n",
      "Subject 2, Epoch 504, Loss: 0.938345730304718, Final Batch Loss: 0.35415175557136536\n",
      "Subject 2, Epoch 505, Loss: 0.8286150693893433, Final Batch Loss: 0.28442835807800293\n",
      "Subject 2, Epoch 506, Loss: 0.9642994105815887, Final Batch Loss: 0.3368263840675354\n",
      "Subject 2, Epoch 507, Loss: 0.9502357840538025, Final Batch Loss: 0.356008917093277\n",
      "Subject 2, Epoch 508, Loss: 0.9267862439155579, Final Batch Loss: 0.3411826193332672\n",
      "Subject 2, Epoch 509, Loss: 0.7529605627059937, Final Batch Loss: 0.2063150405883789\n",
      "Subject 2, Epoch 510, Loss: 0.7973873615264893, Final Batch Loss: 0.2560163736343384\n",
      "Subject 2, Epoch 511, Loss: 0.870297446846962, Final Batch Loss: 0.3390881419181824\n",
      "Subject 2, Epoch 512, Loss: 0.8442665040493011, Final Batch Loss: 0.24840089678764343\n",
      "Subject 2, Epoch 513, Loss: 0.8858294039964676, Final Batch Loss: 0.24616725742816925\n",
      "Subject 2, Epoch 514, Loss: 0.7863359302282333, Final Batch Loss: 0.20253552496433258\n",
      "Subject 2, Epoch 515, Loss: 0.8172088861465454, Final Batch Loss: 0.2684404253959656\n",
      "Subject 2, Epoch 516, Loss: 0.8055122047662735, Final Batch Loss: 0.25954392552375793\n",
      "Subject 2, Epoch 517, Loss: 0.7687723785638809, Final Batch Loss: 0.20967908203601837\n",
      "Subject 2, Epoch 518, Loss: 0.8936851322650909, Final Batch Loss: 0.3159026503562927\n",
      "Subject 2, Epoch 519, Loss: 0.8297791630029678, Final Batch Loss: 0.23578773438930511\n",
      "Subject 2, Epoch 520, Loss: 0.9431271404027939, Final Batch Loss: 0.3932175040245056\n",
      "Subject 2, Epoch 521, Loss: 0.8730151355266571, Final Batch Loss: 0.3795759081840515\n",
      "Subject 2, Epoch 522, Loss: 0.83243627846241, Final Batch Loss: 0.2868357002735138\n",
      "Subject 2, Epoch 523, Loss: 0.7591113150119781, Final Batch Loss: 0.17302033305168152\n",
      "Subject 2, Epoch 524, Loss: 0.8590913265943527, Final Batch Loss: 0.3560151159763336\n",
      "Subject 2, Epoch 525, Loss: 0.8463477492332458, Final Batch Loss: 0.28329727053642273\n",
      "Subject 2, Epoch 526, Loss: 0.8333432078361511, Final Batch Loss: 0.33402198553085327\n",
      "Subject 2, Epoch 527, Loss: 0.778323695063591, Final Batch Loss: 0.22455377876758575\n",
      "Subject 2, Epoch 528, Loss: 0.8794403970241547, Final Batch Loss: 0.2570458948612213\n",
      "Subject 2, Epoch 529, Loss: 0.8743985593318939, Final Batch Loss: 0.22164702415466309\n",
      "Subject 2, Epoch 530, Loss: 0.8257216215133667, Final Batch Loss: 0.2691647708415985\n",
      "Subject 2, Epoch 531, Loss: 0.7549316436052322, Final Batch Loss: 0.2801378071308136\n",
      "Subject 2, Epoch 532, Loss: 0.8089333772659302, Final Batch Loss: 0.2578912377357483\n",
      "Subject 2, Epoch 533, Loss: 0.7636477947235107, Final Batch Loss: 0.21393175423145294\n",
      "Subject 2, Epoch 534, Loss: 0.8387324512004852, Final Batch Loss: 0.2737097144126892\n",
      "Subject 2, Epoch 535, Loss: 0.8141755014657974, Final Batch Loss: 0.25522226095199585\n",
      "Subject 2, Epoch 536, Loss: 0.867127537727356, Final Batch Loss: 0.3021383285522461\n",
      "Subject 2, Epoch 537, Loss: 0.7800146341323853, Final Batch Loss: 0.24684536457061768\n",
      "Subject 2, Epoch 538, Loss: 0.8892726302146912, Final Batch Loss: 0.3588709831237793\n",
      "Subject 2, Epoch 539, Loss: 0.7942889481782913, Final Batch Loss: 0.24313059449195862\n",
      "Subject 2, Epoch 540, Loss: 0.738276332616806, Final Batch Loss: 0.2362140715122223\n",
      "Subject 2, Epoch 541, Loss: 0.8134208470582962, Final Batch Loss: 0.36086177825927734\n",
      "Subject 2, Epoch 542, Loss: 0.8756742179393768, Final Batch Loss: 0.25350937247276306\n",
      "Subject 2, Epoch 543, Loss: 0.9256055653095245, Final Batch Loss: 0.29786691069602966\n",
      "Subject 2, Epoch 544, Loss: 0.809645339846611, Final Batch Loss: 0.2021622657775879\n",
      "Subject 2, Epoch 545, Loss: 0.7918084859848022, Final Batch Loss: 0.19332298636436462\n",
      "Subject 2, Epoch 546, Loss: 0.8414452075958252, Final Batch Loss: 0.2087351381778717\n",
      "Subject 2, Epoch 547, Loss: 0.8208962380886078, Final Batch Loss: 0.3119617998600006\n",
      "Subject 2, Epoch 548, Loss: 0.8269349485635757, Final Batch Loss: 0.21070434153079987\n",
      "Subject 2, Epoch 549, Loss: 0.8299361914396286, Final Batch Loss: 0.2931598126888275\n",
      "Subject 2, Epoch 550, Loss: 0.8278965055942535, Final Batch Loss: 0.1880781352519989\n",
      "Subject 2, Epoch 551, Loss: 0.7125480473041534, Final Batch Loss: 0.2886817157268524\n",
      "Subject 2, Epoch 552, Loss: 0.8788716495037079, Final Batch Loss: 0.3502359390258789\n",
      "Subject 2, Epoch 553, Loss: 0.8425210118293762, Final Batch Loss: 0.29783400893211365\n",
      "Subject 2, Epoch 554, Loss: 0.9910188019275665, Final Batch Loss: 0.4605858027935028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 555, Loss: 0.9024450480937958, Final Batch Loss: 0.2577328085899353\n",
      "Subject 2, Epoch 556, Loss: 0.8634552359580994, Final Batch Loss: 0.23390117287635803\n",
      "Subject 2, Epoch 557, Loss: 0.8722525537014008, Final Batch Loss: 0.32453277707099915\n",
      "Subject 2, Epoch 558, Loss: 0.8038226962089539, Final Batch Loss: 0.23530498147010803\n",
      "Subject 2, Epoch 559, Loss: 0.7853777557611465, Final Batch Loss: 0.2271655648946762\n",
      "Subject 2, Epoch 560, Loss: 0.8402663469314575, Final Batch Loss: 0.2758815884590149\n",
      "Subject 2, Epoch 561, Loss: 0.8477759957313538, Final Batch Loss: 0.2418396770954132\n",
      "Subject 2, Epoch 562, Loss: 0.900051549077034, Final Batch Loss: 0.3916572034358978\n",
      "Subject 2, Epoch 563, Loss: 0.8494926393032074, Final Batch Loss: 0.3641623854637146\n",
      "Subject 2, Epoch 564, Loss: 0.9213912487030029, Final Batch Loss: 0.4051609933376312\n",
      "Subject 2, Epoch 565, Loss: 0.8446392714977264, Final Batch Loss: 0.3662547171115875\n",
      "Subject 2, Epoch 566, Loss: 0.7779565155506134, Final Batch Loss: 0.23722466826438904\n",
      "Subject 2, Epoch 567, Loss: 0.9550980180501938, Final Batch Loss: 0.3640609085559845\n",
      "Subject 2, Epoch 568, Loss: 0.8666580617427826, Final Batch Loss: 0.31981411576271057\n",
      "Subject 2, Epoch 569, Loss: 0.9302558898925781, Final Batch Loss: 0.3581051528453827\n",
      "Subject 2, Epoch 570, Loss: 0.8052692413330078, Final Batch Loss: 0.2631046772003174\n",
      "Subject 2, Epoch 571, Loss: 0.7975357472896576, Final Batch Loss: 0.2737845778465271\n",
      "Subject 2, Epoch 572, Loss: 0.8456092029809952, Final Batch Loss: 0.34121090173721313\n",
      "Subject 2, Epoch 573, Loss: 0.8112981766462326, Final Batch Loss: 0.19152967631816864\n",
      "Subject 2, Epoch 574, Loss: 0.7272360324859619, Final Batch Loss: 0.20019634068012238\n",
      "Subject 2, Epoch 575, Loss: 0.8355884402990341, Final Batch Loss: 0.28443819284439087\n",
      "Subject 2, Epoch 576, Loss: 0.7983327955007553, Final Batch Loss: 0.20773924887180328\n",
      "Subject 2, Epoch 577, Loss: 0.7989140897989273, Final Batch Loss: 0.22238130867481232\n",
      "Subject 2, Epoch 578, Loss: 0.6958982273936272, Final Batch Loss: 0.1055457815527916\n",
      "Subject 2, Epoch 579, Loss: 0.7833680361509323, Final Batch Loss: 0.22632943093776703\n",
      "Subject 2, Epoch 580, Loss: 0.7890726029872894, Final Batch Loss: 0.23631584644317627\n",
      "Subject 2, Epoch 581, Loss: 0.9596408754587173, Final Batch Loss: 0.5101305842399597\n",
      "Subject 2, Epoch 582, Loss: 0.7970971018075943, Final Batch Loss: 0.23198439180850983\n",
      "Subject 2, Epoch 583, Loss: 0.8940242230892181, Final Batch Loss: 0.3099150061607361\n",
      "Subject 2, Epoch 584, Loss: 0.7861796766519547, Final Batch Loss: 0.2163381427526474\n",
      "Subject 2, Epoch 585, Loss: 0.8102018386125565, Final Batch Loss: 0.30603405833244324\n",
      "Subject 2, Epoch 586, Loss: 0.8850886970758438, Final Batch Loss: 0.23777739703655243\n",
      "Subject 2, Epoch 587, Loss: 0.7691755294799805, Final Batch Loss: 0.24065876007080078\n",
      "Subject 2, Epoch 588, Loss: 0.9028142392635345, Final Batch Loss: 0.3784943222999573\n",
      "Subject 2, Epoch 589, Loss: 0.8793531358242035, Final Batch Loss: 0.29863840341567993\n",
      "Subject 2, Epoch 590, Loss: 0.8290112614631653, Final Batch Loss: 0.2802080512046814\n",
      "Subject 2, Epoch 591, Loss: 0.8343695402145386, Final Batch Loss: 0.3168570399284363\n",
      "Subject 2, Epoch 592, Loss: 0.8189748376607895, Final Batch Loss: 0.23306575417518616\n",
      "Subject 2, Epoch 593, Loss: 0.7018571496009827, Final Batch Loss: 0.2797607481479645\n",
      "Subject 2, Epoch 594, Loss: 0.8143191635608673, Final Batch Loss: 0.25888437032699585\n",
      "Subject 2, Epoch 595, Loss: 0.8411698341369629, Final Batch Loss: 0.324466735124588\n",
      "Subject 2, Epoch 596, Loss: 0.8007739633321762, Final Batch Loss: 0.29334327578544617\n",
      "Subject 2, Epoch 597, Loss: 0.7478208392858505, Final Batch Loss: 0.2693266272544861\n",
      "Subject 2, Epoch 598, Loss: 0.7449064403772354, Final Batch Loss: 0.2160257250070572\n",
      "Subject 2, Epoch 599, Loss: 0.8849917650222778, Final Batch Loss: 0.27581459283828735\n",
      "Subject 2, Epoch 600, Loss: 0.8103640377521515, Final Batch Loss: 0.2521050274372101\n",
      "Subject 2, Epoch 601, Loss: 0.7363124340772629, Final Batch Loss: 0.29079416394233704\n",
      "Subject 2, Epoch 602, Loss: 0.7703985124826431, Final Batch Loss: 0.26109275221824646\n",
      "Subject 2, Epoch 603, Loss: 0.8167853504419327, Final Batch Loss: 0.2309325784444809\n",
      "Subject 2, Epoch 604, Loss: 0.7294031530618668, Final Batch Loss: 0.16784049570560455\n",
      "Subject 2, Epoch 605, Loss: 0.6928923279047012, Final Batch Loss: 0.19896912574768066\n",
      "Subject 2, Epoch 606, Loss: 0.963787779211998, Final Batch Loss: 0.4503850042819977\n",
      "Subject 2, Epoch 607, Loss: 0.8007371723651886, Final Batch Loss: 0.302715003490448\n",
      "Subject 2, Epoch 608, Loss: 0.9055366963148117, Final Batch Loss: 0.37742599844932556\n",
      "Subject 2, Epoch 609, Loss: 0.7606437802314758, Final Batch Loss: 0.22902460396289825\n",
      "Subject 2, Epoch 610, Loss: 0.7553204894065857, Final Batch Loss: 0.17280706763267517\n",
      "Subject 2, Epoch 611, Loss: 1.0221469700336456, Final Batch Loss: 0.42815646529197693\n",
      "Subject 2, Epoch 612, Loss: 0.8320953249931335, Final Batch Loss: 0.22015821933746338\n",
      "Subject 2, Epoch 613, Loss: 0.7483957409858704, Final Batch Loss: 0.2916537821292877\n",
      "Subject 2, Epoch 614, Loss: 0.782007709145546, Final Batch Loss: 0.2591700255870819\n",
      "Subject 2, Epoch 615, Loss: 0.7541826516389847, Final Batch Loss: 0.326505571603775\n",
      "Subject 2, Epoch 616, Loss: 0.7572971880435944, Final Batch Loss: 0.2749210596084595\n",
      "Subject 2, Epoch 617, Loss: 0.7544015496969223, Final Batch Loss: 0.26068437099456787\n",
      "Subject 2, Epoch 618, Loss: 0.7995214313268661, Final Batch Loss: 0.3176890015602112\n",
      "Subject 2, Epoch 619, Loss: 0.7426830381155014, Final Batch Loss: 0.25158679485321045\n",
      "Subject 2, Epoch 620, Loss: 0.8385109156370163, Final Batch Loss: 0.2775544822216034\n",
      "Subject 2, Epoch 621, Loss: 0.7783576548099518, Final Batch Loss: 0.28472012281417847\n",
      "Subject 2, Epoch 622, Loss: 0.8622486889362335, Final Batch Loss: 0.3054109215736389\n",
      "Subject 2, Epoch 623, Loss: 0.8092497140169144, Final Batch Loss: 0.3648766577243805\n",
      "Subject 2, Epoch 624, Loss: 0.7541358023881912, Final Batch Loss: 0.29213258624076843\n",
      "Subject 2, Epoch 625, Loss: 0.8056842684745789, Final Batch Loss: 0.32345345616340637\n",
      "Subject 2, Epoch 626, Loss: 0.8198924660682678, Final Batch Loss: 0.30024734139442444\n",
      "Subject 2, Epoch 627, Loss: 0.8584952801465988, Final Batch Loss: 0.32216814160346985\n",
      "Subject 2, Epoch 628, Loss: 0.6880943477153778, Final Batch Loss: 0.19691844284534454\n",
      "Subject 2, Epoch 629, Loss: 0.7270456403493881, Final Batch Loss: 0.24234497547149658\n",
      "Subject 2, Epoch 630, Loss: 0.684029296040535, Final Batch Loss: 0.21729998290538788\n",
      "Subject 2, Epoch 631, Loss: 0.8242572396993637, Final Batch Loss: 0.21362437307834625\n",
      "Subject 2, Epoch 632, Loss: 0.8044243454933167, Final Batch Loss: 0.26947662234306335\n",
      "Subject 2, Epoch 633, Loss: 0.7199271470308304, Final Batch Loss: 0.21060483157634735\n",
      "Subject 2, Epoch 634, Loss: 0.8400407284498215, Final Batch Loss: 0.3721204102039337\n",
      "Subject 2, Epoch 635, Loss: 0.9639260768890381, Final Batch Loss: 0.3215331733226776\n",
      "Subject 2, Epoch 636, Loss: 0.8012297302484512, Final Batch Loss: 0.26290008425712585\n",
      "Subject 2, Epoch 637, Loss: 0.7552292495965958, Final Batch Loss: 0.22899921238422394\n",
      "Subject 2, Epoch 638, Loss: 0.8065226078033447, Final Batch Loss: 0.28881338238716125\n",
      "Subject 2, Epoch 639, Loss: 0.7633941322565079, Final Batch Loss: 0.22978268563747406\n",
      "Subject 2, Epoch 640, Loss: 0.769378736615181, Final Batch Loss: 0.24429747462272644\n",
      "Subject 2, Epoch 641, Loss: 0.7241010963916779, Final Batch Loss: 0.2313711941242218\n",
      "Subject 2, Epoch 642, Loss: 0.7116381675004959, Final Batch Loss: 0.18372850120067596\n",
      "Subject 2, Epoch 643, Loss: 0.736303523182869, Final Batch Loss: 0.1934422105550766\n",
      "Subject 2, Epoch 644, Loss: 0.6611559838056564, Final Batch Loss: 0.15836764872074127\n",
      "Subject 2, Epoch 645, Loss: 0.8564428836107254, Final Batch Loss: 0.37812018394470215\n",
      "Subject 2, Epoch 646, Loss: 0.8000490814447403, Final Batch Loss: 0.27380111813545227\n",
      "Subject 2, Epoch 647, Loss: 0.7143220603466034, Final Batch Loss: 0.150368794798851\n",
      "Subject 2, Epoch 648, Loss: 0.8177148848772049, Final Batch Loss: 0.24586285650730133\n",
      "Subject 2, Epoch 649, Loss: 0.7736658006906509, Final Batch Loss: 0.2271629422903061\n",
      "Subject 2, Epoch 650, Loss: 0.811968207359314, Final Batch Loss: 0.3181368410587311\n",
      "Subject 2, Epoch 651, Loss: 0.8225962817668915, Final Batch Loss: 0.2698516547679901\n",
      "Subject 2, Epoch 652, Loss: 0.7902936339378357, Final Batch Loss: 0.3368220329284668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 653, Loss: 0.7891417443752289, Final Batch Loss: 0.2654774785041809\n",
      "Subject 2, Epoch 654, Loss: 0.7404231429100037, Final Batch Loss: 0.17617511749267578\n",
      "Subject 2, Epoch 655, Loss: 0.7353040128946304, Final Batch Loss: 0.26776108145713806\n",
      "Subject 2, Epoch 656, Loss: 0.7821942418813705, Final Batch Loss: 0.3074246942996979\n",
      "Subject 2, Epoch 657, Loss: 0.7521575391292572, Final Batch Loss: 0.23621483147144318\n",
      "Subject 2, Epoch 658, Loss: 0.7416909039020538, Final Batch Loss: 0.20555351674556732\n",
      "Subject 2, Epoch 659, Loss: 0.7404803037643433, Final Batch Loss: 0.2388385534286499\n",
      "Subject 2, Epoch 660, Loss: 0.7941286861896515, Final Batch Loss: 0.32722407579421997\n",
      "Subject 2, Epoch 661, Loss: 0.7625858783721924, Final Batch Loss: 0.2566641569137573\n",
      "Subject 2, Epoch 662, Loss: 0.6780634671449661, Final Batch Loss: 0.18791361153125763\n",
      "Subject 2, Epoch 663, Loss: 0.7800689786672592, Final Batch Loss: 0.2674160897731781\n",
      "Subject 2, Epoch 664, Loss: 0.7311242967844009, Final Batch Loss: 0.19904327392578125\n",
      "Subject 2, Epoch 665, Loss: 0.7492490112781525, Final Batch Loss: 0.29615992307662964\n",
      "Subject 2, Epoch 666, Loss: 0.7513862252235413, Final Batch Loss: 0.31487104296684265\n",
      "Subject 2, Epoch 667, Loss: 0.7817726135253906, Final Batch Loss: 0.20411059260368347\n",
      "Subject 2, Epoch 668, Loss: 0.7193722724914551, Final Batch Loss: 0.2882545292377472\n",
      "Subject 2, Epoch 669, Loss: 0.7846311181783676, Final Batch Loss: 0.3461110591888428\n",
      "Subject 2, Epoch 670, Loss: 0.670258030295372, Final Batch Loss: 0.25697556138038635\n",
      "Subject 2, Epoch 671, Loss: 0.8043534904718399, Final Batch Loss: 0.37264615297317505\n",
      "Subject 2, Epoch 672, Loss: 0.8097256422042847, Final Batch Loss: 0.26953229308128357\n",
      "Subject 2, Epoch 673, Loss: 0.7093194276094437, Final Batch Loss: 0.17275692522525787\n",
      "Subject 2, Epoch 674, Loss: 0.735142707824707, Final Batch Loss: 0.2762787342071533\n",
      "Subject 2, Epoch 675, Loss: 0.6777126640081406, Final Batch Loss: 0.18598605692386627\n",
      "Subject 2, Epoch 676, Loss: 0.7433207482099533, Final Batch Loss: 0.27688783407211304\n",
      "Subject 2, Epoch 677, Loss: 0.8256580680608749, Final Batch Loss: 0.3395553529262543\n",
      "Subject 2, Epoch 678, Loss: 0.7251027375459671, Final Batch Loss: 0.23785240948200226\n",
      "Subject 2, Epoch 679, Loss: 0.8216913640499115, Final Batch Loss: 0.24793735146522522\n",
      "Subject 2, Epoch 680, Loss: 0.6956411600112915, Final Batch Loss: 0.23422816395759583\n",
      "Subject 2, Epoch 681, Loss: 0.6652735024690628, Final Batch Loss: 0.24375270307064056\n",
      "Subject 2, Epoch 682, Loss: 0.7091186493635178, Final Batch Loss: 0.23051384091377258\n",
      "Subject 2, Epoch 683, Loss: 0.681864008307457, Final Batch Loss: 0.21905095875263214\n",
      "Subject 2, Epoch 684, Loss: 0.7889014184474945, Final Batch Loss: 0.26971638202667236\n",
      "Subject 2, Epoch 685, Loss: 0.6799408048391342, Final Batch Loss: 0.14131729304790497\n",
      "Subject 2, Epoch 686, Loss: 0.8073593825101852, Final Batch Loss: 0.32422560453414917\n",
      "Subject 2, Epoch 687, Loss: 0.7643427550792694, Final Batch Loss: 0.27572447061538696\n",
      "Subject 2, Epoch 688, Loss: 0.8136697858572006, Final Batch Loss: 0.35651645064353943\n",
      "Subject 2, Epoch 689, Loss: 0.6922338008880615, Final Batch Loss: 0.21475298702716827\n",
      "Subject 2, Epoch 690, Loss: 0.7814988791942596, Final Batch Loss: 0.18864500522613525\n",
      "Subject 2, Epoch 691, Loss: 0.7929701805114746, Final Batch Loss: 0.22447331249713898\n",
      "Subject 2, Epoch 692, Loss: 0.6958346217870712, Final Batch Loss: 0.19243095815181732\n",
      "Subject 2, Epoch 693, Loss: 0.8462098985910416, Final Batch Loss: 0.33892449736595154\n",
      "Subject 2, Epoch 694, Loss: 0.6913433223962784, Final Batch Loss: 0.181759312748909\n",
      "Subject 2, Epoch 695, Loss: 0.7367357313632965, Final Batch Loss: 0.32178452610969543\n",
      "Subject 2, Epoch 696, Loss: 0.7390928268432617, Final Batch Loss: 0.2599261701107025\n",
      "Subject 2, Epoch 697, Loss: 0.6741666048765182, Final Batch Loss: 0.2898921072483063\n",
      "Subject 2, Epoch 698, Loss: 0.6795477569103241, Final Batch Loss: 0.23629991710186005\n",
      "Subject 2, Epoch 699, Loss: 0.6891108155250549, Final Batch Loss: 0.20476900041103363\n",
      "Subject 2, Epoch 700, Loss: 0.7119331955909729, Final Batch Loss: 0.1980849951505661\n",
      "Subject 2, Epoch 701, Loss: 0.6838246434926987, Final Batch Loss: 0.23145677149295807\n",
      "Subject 2, Epoch 702, Loss: 0.8133200705051422, Final Batch Loss: 0.24433067440986633\n",
      "Subject 2, Epoch 703, Loss: 0.7563363015651703, Final Batch Loss: 0.2742334008216858\n",
      "Subject 2, Epoch 704, Loss: 0.7877616137266159, Final Batch Loss: 0.22818626463413239\n",
      "Subject 2, Epoch 705, Loss: 0.7980652302503586, Final Batch Loss: 0.2951600253582001\n",
      "Subject 2, Epoch 706, Loss: 0.7405134588479996, Final Batch Loss: 0.2867361903190613\n",
      "Subject 2, Epoch 707, Loss: 0.7175363302230835, Final Batch Loss: 0.22795073688030243\n",
      "Subject 2, Epoch 708, Loss: 0.7868812531232834, Final Batch Loss: 0.32823583483695984\n",
      "Subject 2, Epoch 709, Loss: 0.6745807230472565, Final Batch Loss: 0.15941663086414337\n",
      "Subject 2, Epoch 710, Loss: 0.7267490178346634, Final Batch Loss: 0.20925560593605042\n",
      "Subject 2, Epoch 711, Loss: 0.6421851217746735, Final Batch Loss: 0.20395950973033905\n",
      "Subject 2, Epoch 712, Loss: 0.773565337061882, Final Batch Loss: 0.2268417924642563\n",
      "Subject 2, Epoch 713, Loss: 0.7386334985494614, Final Batch Loss: 0.28879156708717346\n",
      "Subject 2, Epoch 714, Loss: 0.7303686290979385, Final Batch Loss: 0.2685507833957672\n",
      "Subject 2, Epoch 715, Loss: 0.7002595812082291, Final Batch Loss: 0.22866377234458923\n",
      "Subject 2, Epoch 716, Loss: 0.7015492767095566, Final Batch Loss: 0.24753616750240326\n",
      "Subject 2, Epoch 717, Loss: 0.6794488877058029, Final Batch Loss: 0.22402270138263702\n",
      "Subject 2, Epoch 718, Loss: 0.8487696051597595, Final Batch Loss: 0.3862892985343933\n",
      "Subject 2, Epoch 719, Loss: 0.8162073194980621, Final Batch Loss: 0.27469441294670105\n",
      "Subject 2, Epoch 720, Loss: 0.7062069773674011, Final Batch Loss: 0.22364363074302673\n",
      "Subject 2, Epoch 721, Loss: 0.6480894386768341, Final Batch Loss: 0.22949504852294922\n",
      "Subject 2, Epoch 722, Loss: 0.6941200941801071, Final Batch Loss: 0.20812608301639557\n",
      "Subject 2, Epoch 723, Loss: 0.6915722042322159, Final Batch Loss: 0.21493330597877502\n",
      "Subject 2, Epoch 724, Loss: 0.6203987151384354, Final Batch Loss: 0.1659429967403412\n",
      "Subject 2, Epoch 725, Loss: 0.6922450810670853, Final Batch Loss: 0.23984622955322266\n",
      "Subject 2, Epoch 726, Loss: 0.6640827655792236, Final Batch Loss: 0.21814730763435364\n",
      "Subject 2, Epoch 727, Loss: 0.7291846573352814, Final Batch Loss: 0.2309112697839737\n",
      "Subject 2, Epoch 728, Loss: 0.6557688266038895, Final Batch Loss: 0.24137724936008453\n",
      "Subject 2, Epoch 729, Loss: 0.7163535058498383, Final Batch Loss: 0.22685767710208893\n",
      "Subject 2, Epoch 730, Loss: 0.7117459774017334, Final Batch Loss: 0.2119707316160202\n",
      "Subject 2, Epoch 731, Loss: 0.7678961008787155, Final Batch Loss: 0.33221954107284546\n",
      "Subject 2, Epoch 732, Loss: 0.7401099503040314, Final Batch Loss: 0.17053437232971191\n",
      "Subject 2, Epoch 733, Loss: 0.7052886188030243, Final Batch Loss: 0.25660765171051025\n",
      "Subject 2, Epoch 734, Loss: 0.6716406792402267, Final Batch Loss: 0.21378640830516815\n",
      "Subject 2, Epoch 735, Loss: 0.7117078900337219, Final Batch Loss: 0.18327085673809052\n",
      "Subject 2, Epoch 736, Loss: 0.7222615629434586, Final Batch Loss: 0.2869515120983124\n",
      "Subject 2, Epoch 737, Loss: 0.6329786479473114, Final Batch Loss: 0.21118605136871338\n",
      "Subject 2, Epoch 738, Loss: 0.6361965835094452, Final Batch Loss: 0.1437211036682129\n",
      "Subject 2, Epoch 739, Loss: 0.7055775076150894, Final Batch Loss: 0.2312787026166916\n",
      "Subject 2, Epoch 740, Loss: 0.670091301202774, Final Batch Loss: 0.2223658561706543\n",
      "Subject 2, Epoch 741, Loss: 0.8014061748981476, Final Batch Loss: 0.2147238552570343\n",
      "Subject 2, Epoch 742, Loss: 0.6668751686811447, Final Batch Loss: 0.2559472918510437\n",
      "Subject 2, Epoch 743, Loss: 0.8228716254234314, Final Batch Loss: 0.33044129610061646\n",
      "Subject 2, Epoch 744, Loss: 0.8637519180774689, Final Batch Loss: 0.3080470561981201\n",
      "Subject 2, Epoch 745, Loss: 0.715233713388443, Final Batch Loss: 0.24798858165740967\n",
      "Subject 2, Epoch 746, Loss: 0.7941428124904633, Final Batch Loss: 0.3150092661380768\n",
      "Subject 2, Epoch 747, Loss: 0.6713213920593262, Final Batch Loss: 0.23579475283622742\n",
      "Subject 2, Epoch 748, Loss: 0.7834502607584, Final Batch Loss: 0.19767339527606964\n",
      "Subject 2, Epoch 749, Loss: 0.6598666906356812, Final Batch Loss: 0.20380103588104248\n",
      "Subject 2, Epoch 750, Loss: 0.6901989281177521, Final Batch Loss: 0.21210141479969025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 751, Loss: 0.6714256405830383, Final Batch Loss: 0.16731078922748566\n",
      "Subject 2, Epoch 752, Loss: 0.6884311735630035, Final Batch Loss: 0.2266102284193039\n",
      "Subject 2, Epoch 753, Loss: 0.7524188309907913, Final Batch Loss: 0.3178851008415222\n",
      "Subject 2, Epoch 754, Loss: 0.6069348156452179, Final Batch Loss: 0.19171349704265594\n",
      "Subject 2, Epoch 755, Loss: 0.7201848179101944, Final Batch Loss: 0.239874005317688\n",
      "Subject 2, Epoch 756, Loss: 0.6587315201759338, Final Batch Loss: 0.1826009303331375\n",
      "Subject 2, Epoch 757, Loss: 0.7583236247301102, Final Batch Loss: 0.226974219083786\n",
      "Subject 2, Epoch 758, Loss: 0.774124801158905, Final Batch Loss: 0.2501761317253113\n",
      "Subject 2, Epoch 759, Loss: 0.6474872529506683, Final Batch Loss: 0.19908203184604645\n",
      "Subject 2, Epoch 760, Loss: 0.7036747485399246, Final Batch Loss: 0.182975634932518\n",
      "Subject 2, Epoch 761, Loss: 0.627741202712059, Final Batch Loss: 0.22508862614631653\n",
      "Subject 2, Epoch 762, Loss: 0.628292977809906, Final Batch Loss: 0.25006410479545593\n",
      "Subject 2, Epoch 763, Loss: 0.7256369441747665, Final Batch Loss: 0.2952893078327179\n",
      "Subject 2, Epoch 764, Loss: 0.6525353193283081, Final Batch Loss: 0.24254214763641357\n",
      "Subject 2, Epoch 765, Loss: 0.731918528676033, Final Batch Loss: 0.20036901533603668\n",
      "Subject 2, Epoch 766, Loss: 0.697047770023346, Final Batch Loss: 0.23985011875629425\n",
      "Subject 2, Epoch 767, Loss: 0.6288140565156937, Final Batch Loss: 0.18488046526908875\n",
      "Subject 2, Epoch 768, Loss: 0.6919443756341934, Final Batch Loss: 0.2444840371608734\n",
      "Subject 2, Epoch 769, Loss: 0.7439094632863998, Final Batch Loss: 0.27969080209732056\n",
      "Subject 2, Epoch 770, Loss: 0.6814489364624023, Final Batch Loss: 0.21498234570026398\n",
      "Subject 2, Epoch 771, Loss: 0.6941697001457214, Final Batch Loss: 0.24935130774974823\n",
      "Subject 2, Epoch 772, Loss: 0.7147164195775986, Final Batch Loss: 0.17262358963489532\n",
      "Subject 2, Epoch 773, Loss: 0.640271320939064, Final Batch Loss: 0.1457400619983673\n",
      "Subject 2, Epoch 774, Loss: 0.6994948834180832, Final Batch Loss: 0.20247209072113037\n",
      "Subject 2, Epoch 775, Loss: 0.7705464363098145, Final Batch Loss: 0.18631970882415771\n",
      "Subject 2, Epoch 776, Loss: 0.7341849952936172, Final Batch Loss: 0.3158567547798157\n",
      "Subject 2, Epoch 777, Loss: 0.7100913524627686, Final Batch Loss: 0.30632880330085754\n",
      "Subject 2, Epoch 778, Loss: 0.6579729318618774, Final Batch Loss: 0.2212592512369156\n",
      "Subject 2, Epoch 779, Loss: 0.7690158188343048, Final Batch Loss: 0.1891367882490158\n",
      "Subject 2, Epoch 780, Loss: 0.8857391774654388, Final Batch Loss: 0.3817368149757385\n",
      "Subject 2, Epoch 781, Loss: 0.7190205603837967, Final Batch Loss: 0.1921418011188507\n",
      "Subject 2, Epoch 782, Loss: 0.6519542634487152, Final Batch Loss: 0.22108955681324005\n",
      "Subject 2, Epoch 783, Loss: 0.7166425883769989, Final Batch Loss: 0.2381034940481186\n",
      "Subject 2, Epoch 784, Loss: 0.8115777224302292, Final Batch Loss: 0.2896428406238556\n",
      "Subject 2, Epoch 785, Loss: 0.7043233662843704, Final Batch Loss: 0.27285054326057434\n",
      "Subject 2, Epoch 786, Loss: 0.6718475818634033, Final Batch Loss: 0.1779344379901886\n",
      "Subject 2, Epoch 787, Loss: 0.7501614093780518, Final Batch Loss: 0.29094791412353516\n",
      "Subject 2, Epoch 788, Loss: 0.6542623937129974, Final Batch Loss: 0.14600367844104767\n",
      "Subject 2, Epoch 789, Loss: 0.5604027360677719, Final Batch Loss: 0.1278267204761505\n",
      "Subject 2, Epoch 790, Loss: 0.6678320467472076, Final Batch Loss: 0.23096060752868652\n",
      "Subject 2, Epoch 791, Loss: 0.6327772438526154, Final Batch Loss: 0.25331562757492065\n",
      "Subject 2, Epoch 792, Loss: 0.7077444046735764, Final Batch Loss: 0.2539813220500946\n",
      "Subject 2, Epoch 793, Loss: 0.6618438214063644, Final Batch Loss: 0.15985289216041565\n",
      "Subject 2, Epoch 794, Loss: 0.6808630377054214, Final Batch Loss: 0.2687837779521942\n",
      "Subject 2, Epoch 795, Loss: 0.771782174706459, Final Batch Loss: 0.20333366096019745\n",
      "Subject 2, Epoch 796, Loss: 0.7072558701038361, Final Batch Loss: 0.2559702694416046\n",
      "Subject 2, Epoch 797, Loss: 0.6816978007555008, Final Batch Loss: 0.18733619153499603\n",
      "Subject 2, Epoch 798, Loss: 0.7919166386127472, Final Batch Loss: 0.3186514377593994\n",
      "Subject 2, Epoch 799, Loss: 0.7145952880382538, Final Batch Loss: 0.32970085740089417\n",
      "Subject 2, Epoch 800, Loss: 0.7269526571035385, Final Batch Loss: 0.20392368733882904\n",
      "Subject 2, Epoch 801, Loss: 0.7156697660684586, Final Batch Loss: 0.19121341407299042\n",
      "Subject 2, Epoch 802, Loss: 0.7333369553089142, Final Batch Loss: 0.2364915907382965\n",
      "Subject 2, Epoch 803, Loss: 0.7698801606893539, Final Batch Loss: 0.37793779373168945\n",
      "Subject 2, Epoch 804, Loss: 0.6994792222976685, Final Batch Loss: 0.22590741515159607\n",
      "Subject 2, Epoch 805, Loss: 0.6443191468715668, Final Batch Loss: 0.21943731606006622\n",
      "Subject 2, Epoch 806, Loss: 0.617427796125412, Final Batch Loss: 0.23953887820243835\n",
      "Subject 2, Epoch 807, Loss: 0.7176646739244461, Final Batch Loss: 0.18294726312160492\n",
      "Subject 2, Epoch 808, Loss: 0.6623715162277222, Final Batch Loss: 0.24972914159297943\n",
      "Subject 2, Epoch 809, Loss: 0.732807919383049, Final Batch Loss: 0.265766441822052\n",
      "Subject 2, Epoch 810, Loss: 0.7600346803665161, Final Batch Loss: 0.24830222129821777\n",
      "Subject 2, Epoch 811, Loss: 0.8183024823665619, Final Batch Loss: 0.27964743971824646\n",
      "Subject 2, Epoch 812, Loss: 0.708074226975441, Final Batch Loss: 0.24194927513599396\n",
      "Subject 2, Epoch 813, Loss: 0.6949668824672699, Final Batch Loss: 0.17628586292266846\n",
      "Subject 2, Epoch 814, Loss: 0.6750149875879288, Final Batch Loss: 0.20492689311504364\n",
      "Subject 2, Epoch 815, Loss: 0.6506113260984421, Final Batch Loss: 0.1962333470582962\n",
      "Subject 2, Epoch 816, Loss: 0.7606573104858398, Final Batch Loss: 0.32715216279029846\n",
      "Subject 2, Epoch 817, Loss: 0.6038099378347397, Final Batch Loss: 0.20139218866825104\n",
      "Subject 2, Epoch 818, Loss: 0.6640398353338242, Final Batch Loss: 0.20593997836112976\n",
      "Subject 2, Epoch 819, Loss: 0.7511767596006393, Final Batch Loss: 0.3006385266780853\n",
      "Subject 2, Epoch 820, Loss: 0.6816331595182419, Final Batch Loss: 0.158042773604393\n",
      "Subject 2, Epoch 821, Loss: 0.7145471423864365, Final Batch Loss: 0.21810400485992432\n",
      "Subject 2, Epoch 822, Loss: 0.7132636606693268, Final Batch Loss: 0.27238929271698\n",
      "Subject 2, Epoch 823, Loss: 0.7378871440887451, Final Batch Loss: 0.2746404707431793\n",
      "Subject 2, Epoch 824, Loss: 0.6788449436426163, Final Batch Loss: 0.23410531878471375\n",
      "Subject 2, Epoch 825, Loss: 0.6883236169815063, Final Batch Loss: 0.2213052213191986\n",
      "Subject 2, Epoch 826, Loss: 0.6454285979270935, Final Batch Loss: 0.2140248715877533\n",
      "Subject 2, Epoch 827, Loss: 0.699446976184845, Final Batch Loss: 0.29407262802124023\n",
      "Subject 2, Epoch 828, Loss: 0.701850414276123, Final Batch Loss: 0.2480938583612442\n",
      "Subject 2, Epoch 829, Loss: 0.6400466412305832, Final Batch Loss: 0.23592467606067657\n",
      "Subject 2, Epoch 830, Loss: 0.6667060256004333, Final Batch Loss: 0.14015600085258484\n",
      "Subject 2, Epoch 831, Loss: 0.7073972076177597, Final Batch Loss: 0.2628595530986786\n",
      "Subject 2, Epoch 832, Loss: 0.6843400001525879, Final Batch Loss: 0.2740788459777832\n",
      "Subject 2, Epoch 833, Loss: 0.6109464466571808, Final Batch Loss: 0.14978155493736267\n",
      "Subject 2, Epoch 834, Loss: 0.6804194450378418, Final Batch Loss: 0.18532036244869232\n",
      "Subject 2, Epoch 835, Loss: 0.7078046947717667, Final Batch Loss: 0.27841916680336\n",
      "Subject 2, Epoch 836, Loss: 0.7033912688493729, Final Batch Loss: 0.27947792410850525\n",
      "Subject 2, Epoch 837, Loss: 0.7322027087211609, Final Batch Loss: 0.30157914757728577\n",
      "Subject 2, Epoch 838, Loss: 0.6841484010219574, Final Batch Loss: 0.1693240851163864\n",
      "Subject 2, Epoch 839, Loss: 0.657770648598671, Final Batch Loss: 0.2629297077655792\n",
      "Subject 2, Epoch 840, Loss: 0.6657020449638367, Final Batch Loss: 0.24390266835689545\n",
      "Subject 2, Epoch 841, Loss: 0.6639265567064285, Final Batch Loss: 0.23382222652435303\n",
      "Subject 2, Epoch 842, Loss: 0.6273771673440933, Final Batch Loss: 0.146127387881279\n",
      "Subject 2, Epoch 843, Loss: 0.6952021420001984, Final Batch Loss: 0.2729050815105438\n",
      "Subject 2, Epoch 844, Loss: 0.645622581243515, Final Batch Loss: 0.1860572248697281\n",
      "Subject 2, Epoch 845, Loss: 0.6962923407554626, Final Batch Loss: 0.23948533833026886\n",
      "Subject 2, Epoch 846, Loss: 0.7495595812797546, Final Batch Loss: 0.25349652767181396\n",
      "Subject 2, Epoch 847, Loss: 0.6150462925434113, Final Batch Loss: 0.20186468958854675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 848, Loss: 0.677023321390152, Final Batch Loss: 0.21911799907684326\n",
      "Subject 2, Epoch 849, Loss: 0.651352733373642, Final Batch Loss: 0.2001316249370575\n",
      "Subject 2, Epoch 850, Loss: 0.545967161655426, Final Batch Loss: 0.07837711274623871\n",
      "Subject 2, Epoch 851, Loss: 0.7062933444976807, Final Batch Loss: 0.23900344967842102\n",
      "Subject 2, Epoch 852, Loss: 0.6466870456933975, Final Batch Loss: 0.14885473251342773\n",
      "Subject 2, Epoch 853, Loss: 0.6178028434514999, Final Batch Loss: 0.16840137541294098\n",
      "Subject 2, Epoch 854, Loss: 0.6426786035299301, Final Batch Loss: 0.21231086552143097\n",
      "Subject 2, Epoch 855, Loss: 0.6623780280351639, Final Batch Loss: 0.2543078362941742\n",
      "Subject 2, Epoch 856, Loss: 0.6900977939367294, Final Batch Loss: 0.2484704554080963\n",
      "Subject 2, Epoch 857, Loss: 0.6459035724401474, Final Batch Loss: 0.21656575798988342\n",
      "Subject 2, Epoch 858, Loss: 0.6786821335554123, Final Batch Loss: 0.2558070719242096\n",
      "Subject 2, Epoch 859, Loss: 0.7197462618350983, Final Batch Loss: 0.338477224111557\n",
      "Subject 2, Epoch 860, Loss: 0.5873414725065231, Final Batch Loss: 0.22792311012744904\n",
      "Subject 2, Epoch 861, Loss: 0.5897662490606308, Final Batch Loss: 0.17981624603271484\n",
      "Subject 2, Epoch 862, Loss: 0.6529931873083115, Final Batch Loss: 0.20724661648273468\n",
      "Subject 2, Epoch 863, Loss: 0.6956059634685516, Final Batch Loss: 0.18257096409797668\n",
      "Subject 2, Epoch 864, Loss: 0.746700793504715, Final Batch Loss: 0.24584069848060608\n",
      "Subject 2, Epoch 865, Loss: 0.6707590371370316, Final Batch Loss: 0.2859363555908203\n",
      "Subject 2, Epoch 866, Loss: 0.6469049155712128, Final Batch Loss: 0.2332259714603424\n",
      "Subject 2, Epoch 867, Loss: 0.6931494772434235, Final Batch Loss: 0.22442278265953064\n",
      "Subject 2, Epoch 868, Loss: 0.6043758243322372, Final Batch Loss: 0.22928667068481445\n",
      "Subject 2, Epoch 869, Loss: 0.6622814685106277, Final Batch Loss: 0.2648700177669525\n",
      "Subject 2, Epoch 870, Loss: 0.6490025371313095, Final Batch Loss: 0.21381698548793793\n",
      "Subject 2, Epoch 871, Loss: 0.6221747696399689, Final Batch Loss: 0.2132038176059723\n",
      "Subject 2, Epoch 872, Loss: 0.6628020852804184, Final Batch Loss: 0.23909136652946472\n",
      "Subject 2, Epoch 873, Loss: 0.6423931121826172, Final Batch Loss: 0.21998606622219086\n",
      "Subject 2, Epoch 874, Loss: 0.6077628135681152, Final Batch Loss: 0.2038862556219101\n",
      "Subject 2, Epoch 875, Loss: 0.6551365554332733, Final Batch Loss: 0.2505663335323334\n",
      "Subject 2, Epoch 876, Loss: 0.6435284465551376, Final Batch Loss: 0.2083282470703125\n",
      "Subject 2, Epoch 877, Loss: 0.5902851521968842, Final Batch Loss: 0.17798872292041779\n",
      "Subject 2, Epoch 878, Loss: 0.6947495937347412, Final Batch Loss: 0.22115322947502136\n",
      "Subject 2, Epoch 879, Loss: 0.5981640517711639, Final Batch Loss: 0.20846867561340332\n",
      "Subject 2, Epoch 880, Loss: 0.5545653998851776, Final Batch Loss: 0.14511895179748535\n",
      "Subject 2, Epoch 881, Loss: 0.6161419898271561, Final Batch Loss: 0.1548490822315216\n",
      "Subject 2, Epoch 882, Loss: 0.626332014799118, Final Batch Loss: 0.17718271911144257\n",
      "Subject 2, Epoch 883, Loss: 0.6862094700336456, Final Batch Loss: 0.2776815593242645\n",
      "Subject 2, Epoch 884, Loss: 0.6058447062969208, Final Batch Loss: 0.23278412222862244\n",
      "Subject 2, Epoch 885, Loss: 0.6121588796377182, Final Batch Loss: 0.2697084844112396\n",
      "Subject 2, Epoch 886, Loss: 0.5594299137592316, Final Batch Loss: 0.1668289750814438\n",
      "Subject 2, Epoch 887, Loss: 0.6510946750640869, Final Batch Loss: 0.24872031807899475\n",
      "Subject 2, Epoch 888, Loss: 0.6745351403951645, Final Batch Loss: 0.29956573247909546\n",
      "Subject 2, Epoch 889, Loss: 0.727087214589119, Final Batch Loss: 0.19886089861392975\n",
      "Subject 2, Epoch 890, Loss: 0.7714010030031204, Final Batch Loss: 0.3079773485660553\n",
      "Subject 2, Epoch 891, Loss: 0.6347265988588333, Final Batch Loss: 0.19814364612102509\n",
      "Subject 2, Epoch 892, Loss: 0.6577147841453552, Final Batch Loss: 0.20784540474414825\n",
      "Subject 2, Epoch 893, Loss: 0.5807393789291382, Final Batch Loss: 0.21548782289028168\n",
      "Subject 2, Epoch 894, Loss: 0.5502251833677292, Final Batch Loss: 0.150144562125206\n",
      "Subject 2, Epoch 895, Loss: 0.6694044470787048, Final Batch Loss: 0.23871372640132904\n",
      "Subject 2, Epoch 896, Loss: 0.6877575367689133, Final Batch Loss: 0.20110030472278595\n",
      "Subject 2, Epoch 897, Loss: 0.691680058836937, Final Batch Loss: 0.25420209765434265\n",
      "Subject 2, Epoch 898, Loss: 0.7676470875740051, Final Batch Loss: 0.2524360716342926\n",
      "Subject 2, Epoch 899, Loss: 0.6973100900650024, Final Batch Loss: 0.2299501597881317\n",
      "Subject 2, Epoch 900, Loss: 0.6720945537090302, Final Batch Loss: 0.2692979872226715\n",
      "Subject 2, Epoch 901, Loss: 0.6926449537277222, Final Batch Loss: 0.2632122337818146\n",
      "Subject 2, Epoch 902, Loss: 0.5904603749513626, Final Batch Loss: 0.17388223111629486\n",
      "Subject 2, Epoch 903, Loss: 0.6085439771413803, Final Batch Loss: 0.1651291400194168\n",
      "Subject 2, Epoch 904, Loss: 0.6325866132974625, Final Batch Loss: 0.2194603830575943\n",
      "Subject 2, Epoch 905, Loss: 0.8387058973312378, Final Batch Loss: 0.380972295999527\n",
      "Subject 2, Epoch 906, Loss: 0.5864322632551193, Final Batch Loss: 0.18351562321186066\n",
      "Subject 2, Epoch 907, Loss: 0.6445497870445251, Final Batch Loss: 0.2642383575439453\n",
      "Subject 2, Epoch 908, Loss: 0.6197693347930908, Final Batch Loss: 0.22627072036266327\n",
      "Subject 2, Epoch 909, Loss: 0.6458862721920013, Final Batch Loss: 0.23607221245765686\n",
      "Subject 2, Epoch 910, Loss: 0.6456193774938583, Final Batch Loss: 0.1610638052225113\n",
      "Subject 2, Epoch 911, Loss: 0.6310854405164719, Final Batch Loss: 0.25738096237182617\n",
      "Subject 2, Epoch 912, Loss: 0.6988787949085236, Final Batch Loss: 0.32556629180908203\n",
      "Subject 2, Epoch 913, Loss: 0.6520978063344955, Final Batch Loss: 0.24560785293579102\n",
      "Subject 2, Epoch 914, Loss: 0.5999887585639954, Final Batch Loss: 0.1303754299879074\n",
      "Subject 2, Epoch 915, Loss: 0.7573988139629364, Final Batch Loss: 0.24968531727790833\n",
      "Subject 2, Epoch 916, Loss: 0.6307587772607803, Final Batch Loss: 0.19602753221988678\n",
      "Subject 2, Epoch 917, Loss: 0.674601137638092, Final Batch Loss: 0.2147977203130722\n",
      "Subject 2, Epoch 918, Loss: 0.6459915786981583, Final Batch Loss: 0.15896837413311005\n",
      "Subject 2, Epoch 919, Loss: 0.5767711997032166, Final Batch Loss: 0.19419634342193604\n",
      "Subject 2, Epoch 920, Loss: 0.5950646251440048, Final Batch Loss: 0.1664878875017166\n",
      "Subject 2, Epoch 921, Loss: 0.6797132641077042, Final Batch Loss: 0.2320852130651474\n",
      "Subject 2, Epoch 922, Loss: 0.7302456796169281, Final Batch Loss: 0.2451389878988266\n",
      "Subject 2, Epoch 923, Loss: 0.8974772095680237, Final Batch Loss: 0.4070914387702942\n",
      "Subject 2, Epoch 924, Loss: 0.7196148335933685, Final Batch Loss: 0.2415734976530075\n",
      "Subject 2, Epoch 925, Loss: 0.6176917105913162, Final Batch Loss: 0.22252610325813293\n",
      "Subject 2, Epoch 926, Loss: 0.7188799828290939, Final Batch Loss: 0.2556909918785095\n",
      "Subject 2, Epoch 927, Loss: 0.6081904619932175, Final Batch Loss: 0.19574134051799774\n",
      "Subject 2, Epoch 928, Loss: 0.7097151428461075, Final Batch Loss: 0.27740001678466797\n",
      "Subject 2, Epoch 929, Loss: 0.6240943372249603, Final Batch Loss: 0.15278710424900055\n",
      "Subject 2, Epoch 930, Loss: 0.6246398240327835, Final Batch Loss: 0.24946574866771698\n",
      "Subject 2, Epoch 931, Loss: 0.6140196472406387, Final Batch Loss: 0.2196114957332611\n",
      "Subject 2, Epoch 932, Loss: 0.6089673042297363, Final Batch Loss: 0.22869844734668732\n",
      "Subject 2, Epoch 933, Loss: 0.613429918885231, Final Batch Loss: 0.16737815737724304\n",
      "Subject 2, Epoch 934, Loss: 0.7129369378089905, Final Batch Loss: 0.22951097786426544\n",
      "Subject 2, Epoch 935, Loss: 0.5532380938529968, Final Batch Loss: 0.07481206953525543\n",
      "Subject 2, Epoch 936, Loss: 0.5805697292089462, Final Batch Loss: 0.20967388153076172\n",
      "Subject 2, Epoch 937, Loss: 0.5949588268995285, Final Batch Loss: 0.18886971473693848\n",
      "Subject 2, Epoch 938, Loss: 0.6035612225532532, Final Batch Loss: 0.17182527482509613\n",
      "Subject 2, Epoch 939, Loss: 0.6895478218793869, Final Batch Loss: 0.23544366657733917\n",
      "Subject 2, Epoch 940, Loss: 0.7216497957706451, Final Batch Loss: 0.3188190460205078\n",
      "Subject 2, Epoch 941, Loss: 0.6478292495012283, Final Batch Loss: 0.1822146624326706\n",
      "Subject 2, Epoch 942, Loss: 0.8157012909650803, Final Batch Loss: 0.23632276058197021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2, Epoch 943, Loss: 0.541994571685791, Final Batch Loss: 0.14536981284618378\n",
      "Subject 2, Epoch 944, Loss: 0.5682072192430496, Final Batch Loss: 0.14141175150871277\n",
      "Subject 2, Epoch 945, Loss: 0.6039087772369385, Final Batch Loss: 0.17755146324634552\n",
      "Subject 2, Epoch 946, Loss: 0.5710518956184387, Final Batch Loss: 0.16437098383903503\n",
      "Subject 2, Epoch 947, Loss: 0.6395388096570969, Final Batch Loss: 0.1840887814760208\n",
      "Subject 2, Epoch 948, Loss: 0.5943912118673325, Final Batch Loss: 0.22500404715538025\n",
      "Subject 2, Epoch 949, Loss: 0.6171929836273193, Final Batch Loss: 0.1503269523382187\n",
      "Subject 2, Epoch 950, Loss: 0.654141440987587, Final Batch Loss: 0.27323195338249207\n",
      "Subject 2, Epoch 951, Loss: 0.5463072210550308, Final Batch Loss: 0.12523140013217926\n",
      "Subject 2, Epoch 952, Loss: 0.7295457720756531, Final Batch Loss: 0.28342387080192566\n",
      "Subject 2, Epoch 953, Loss: 0.6357139945030212, Final Batch Loss: 0.22225305438041687\n",
      "Subject 2, Epoch 954, Loss: 0.6814349591732025, Final Batch Loss: 0.19219492375850677\n",
      "Subject 2, Epoch 955, Loss: 0.7005606144666672, Final Batch Loss: 0.2540872097015381\n",
      "Subject 2, Epoch 956, Loss: 0.6829255670309067, Final Batch Loss: 0.23282800614833832\n",
      "Subject 2, Epoch 957, Loss: 0.5425350069999695, Final Batch Loss: 0.1857144981622696\n",
      "Subject 2, Epoch 958, Loss: 0.6983257085084915, Final Batch Loss: 0.2893950045108795\n",
      "Subject 2, Epoch 959, Loss: 0.7050878703594208, Final Batch Loss: 0.2637302279472351\n",
      "Subject 2, Epoch 960, Loss: 0.5890615880489349, Final Batch Loss: 0.2145291417837143\n",
      "Subject 2, Epoch 961, Loss: 0.5776802599430084, Final Batch Loss: 0.15039856731891632\n",
      "Subject 2, Epoch 962, Loss: 0.7441093921661377, Final Batch Loss: 0.2333703190088272\n",
      "Subject 2, Epoch 963, Loss: 0.6949755400419235, Final Batch Loss: 0.2438647300004959\n",
      "Subject 2, Epoch 964, Loss: 0.5778158456087112, Final Batch Loss: 0.17297349870204926\n",
      "Subject 2, Epoch 965, Loss: 0.573689267039299, Final Batch Loss: 0.178008034825325\n",
      "Subject 2, Epoch 966, Loss: 0.6894665509462357, Final Batch Loss: 0.25588834285736084\n",
      "Subject 2, Epoch 967, Loss: 0.6672823131084442, Final Batch Loss: 0.17268429696559906\n",
      "Subject 2, Epoch 968, Loss: 0.6671768873929977, Final Batch Loss: 0.2817287743091583\n",
      "Subject 2, Epoch 969, Loss: 0.6490637511014938, Final Batch Loss: 0.22604061663150787\n",
      "Subject 2, Epoch 970, Loss: 0.5888794213533401, Final Batch Loss: 0.18317531049251556\n",
      "Subject 2, Epoch 971, Loss: 0.6108556240797043, Final Batch Loss: 0.21287842094898224\n",
      "Subject 2, Epoch 972, Loss: 0.7093738913536072, Final Batch Loss: 0.27578067779541016\n",
      "Subject 2, Epoch 973, Loss: 0.5832967311143875, Final Batch Loss: 0.17352333664894104\n",
      "Subject 2, Epoch 974, Loss: 0.6246868073940277, Final Batch Loss: 0.19431667029857635\n",
      "Subject 2, Epoch 975, Loss: 0.5907001197338104, Final Batch Loss: 0.14857295155525208\n",
      "Subject 2, Epoch 976, Loss: 0.5383796691894531, Final Batch Loss: 0.17372353374958038\n",
      "Subject 2, Epoch 977, Loss: 0.5947014689445496, Final Batch Loss: 0.11500321328639984\n",
      "Subject 2, Epoch 978, Loss: 0.6090506017208099, Final Batch Loss: 0.19576548039913177\n",
      "Subject 2, Epoch 979, Loss: 0.5732534974813461, Final Batch Loss: 0.183067187666893\n",
      "Subject 2, Epoch 980, Loss: 0.6325702369213104, Final Batch Loss: 0.15182800590991974\n",
      "Subject 2, Epoch 981, Loss: 0.6333108097314835, Final Batch Loss: 0.22397196292877197\n",
      "Subject 2, Epoch 982, Loss: 0.5895123779773712, Final Batch Loss: 0.1887563318014145\n",
      "Subject 2, Epoch 983, Loss: 0.6233842745423317, Final Batch Loss: 0.1206800565123558\n",
      "Subject 2, Epoch 984, Loss: 0.6573559194803238, Final Batch Loss: 0.26303908228874207\n",
      "Subject 2, Epoch 985, Loss: 0.6884058862924576, Final Batch Loss: 0.25830769538879395\n",
      "Subject 2, Epoch 986, Loss: 0.5866491794586182, Final Batch Loss: 0.20193199813365936\n",
      "Subject 2, Epoch 987, Loss: 0.5863400548696518, Final Batch Loss: 0.22063793241977692\n",
      "Subject 2, Epoch 988, Loss: 0.7572008073329926, Final Batch Loss: 0.3580656349658966\n",
      "Subject 2, Epoch 989, Loss: 0.5791405588388443, Final Batch Loss: 0.21590133011341095\n",
      "Subject 2, Epoch 990, Loss: 0.6473401784896851, Final Batch Loss: 0.20488567650318146\n",
      "Subject 2, Epoch 991, Loss: 0.6328233927488327, Final Batch Loss: 0.21132723987102509\n",
      "Subject 2, Epoch 992, Loss: 0.6347470581531525, Final Batch Loss: 0.2556472420692444\n",
      "Subject 2, Epoch 993, Loss: 0.6485689282417297, Final Batch Loss: 0.14208200573921204\n",
      "Subject 2, Epoch 994, Loss: 0.7824853509664536, Final Batch Loss: 0.2807685136795044\n",
      "Subject 2, Epoch 995, Loss: 0.6527848392724991, Final Batch Loss: 0.23611202836036682\n",
      "Subject 2, Epoch 996, Loss: 0.6828811764717102, Final Batch Loss: 0.2991001307964325\n",
      "Subject 2, Epoch 997, Loss: 0.5660132318735123, Final Batch Loss: 0.17785249650478363\n",
      "Subject 2, Epoch 998, Loss: 0.7742157429456711, Final Batch Loss: 0.21884359419345856\n",
      "Subject 2, Epoch 999, Loss: 0.6530525982379913, Final Batch Loss: 0.22227701544761658\n",
      "Subject 2, Epoch 1000, Loss: 0.6441077440977097, Final Batch Loss: 0.20159931480884552\n",
      "Subject 3, Epoch 1, Loss: 5.396777510643005, Final Batch Loss: 1.8097540140151978\n",
      "Subject 3, Epoch 2, Loss: 5.390499830245972, Final Batch Loss: 1.7943035364151\n",
      "Subject 3, Epoch 3, Loss: 5.387562394142151, Final Batch Loss: 1.7843384742736816\n",
      "Subject 3, Epoch 4, Loss: 5.388056755065918, Final Batch Loss: 1.7941348552703857\n",
      "Subject 3, Epoch 5, Loss: 5.389450788497925, Final Batch Loss: 1.8027689456939697\n",
      "Subject 3, Epoch 6, Loss: 5.387080550193787, Final Batch Loss: 1.7984496355056763\n",
      "Subject 3, Epoch 7, Loss: 5.386887907981873, Final Batch Loss: 1.8014631271362305\n",
      "Subject 3, Epoch 8, Loss: 5.380427241325378, Final Batch Loss: 1.7758407592773438\n",
      "Subject 3, Epoch 9, Loss: 5.381746172904968, Final Batch Loss: 1.7959266901016235\n",
      "Subject 3, Epoch 10, Loss: 5.3762511014938354, Final Batch Loss: 1.7945576906204224\n",
      "Subject 3, Epoch 11, Loss: 5.364095687866211, Final Batch Loss: 1.7940086126327515\n",
      "Subject 3, Epoch 12, Loss: 5.349837064743042, Final Batch Loss: 1.7887550592422485\n",
      "Subject 3, Epoch 13, Loss: 5.32621705532074, Final Batch Loss: 1.7875076532363892\n",
      "Subject 3, Epoch 14, Loss: 5.299072027206421, Final Batch Loss: 1.7612439393997192\n",
      "Subject 3, Epoch 15, Loss: 5.250363230705261, Final Batch Loss: 1.7441409826278687\n",
      "Subject 3, Epoch 16, Loss: 5.2301636934280396, Final Batch Loss: 1.7446799278259277\n",
      "Subject 3, Epoch 17, Loss: 5.166425704956055, Final Batch Loss: 1.7270551919937134\n",
      "Subject 3, Epoch 18, Loss: 5.138788819313049, Final Batch Loss: 1.6937884092330933\n",
      "Subject 3, Epoch 19, Loss: 5.090178847312927, Final Batch Loss: 1.7029240131378174\n",
      "Subject 3, Epoch 20, Loss: 5.021924734115601, Final Batch Loss: 1.6628150939941406\n",
      "Subject 3, Epoch 21, Loss: 4.950453400611877, Final Batch Loss: 1.619167685508728\n",
      "Subject 3, Epoch 22, Loss: 4.923503875732422, Final Batch Loss: 1.6002650260925293\n",
      "Subject 3, Epoch 23, Loss: 4.869652032852173, Final Batch Loss: 1.5901352167129517\n",
      "Subject 3, Epoch 24, Loss: 4.807814836502075, Final Batch Loss: 1.532507300376892\n",
      "Subject 3, Epoch 25, Loss: 4.775087237358093, Final Batch Loss: 1.5684845447540283\n",
      "Subject 3, Epoch 26, Loss: 4.62658166885376, Final Batch Loss: 1.4875752925872803\n",
      "Subject 3, Epoch 27, Loss: 4.620164632797241, Final Batch Loss: 1.5526888370513916\n",
      "Subject 3, Epoch 28, Loss: 4.5739336013793945, Final Batch Loss: 1.5113043785095215\n",
      "Subject 3, Epoch 29, Loss: 4.452702164649963, Final Batch Loss: 1.465462327003479\n",
      "Subject 3, Epoch 30, Loss: 4.360685110092163, Final Batch Loss: 1.4266554117202759\n",
      "Subject 3, Epoch 31, Loss: 4.296694874763489, Final Batch Loss: 1.4163488149642944\n",
      "Subject 3, Epoch 32, Loss: 4.200437307357788, Final Batch Loss: 1.3790180683135986\n",
      "Subject 3, Epoch 33, Loss: 4.051320314407349, Final Batch Loss: 1.3715988397598267\n",
      "Subject 3, Epoch 34, Loss: 4.087865352630615, Final Batch Loss: 1.3697261810302734\n",
      "Subject 3, Epoch 35, Loss: 3.9432934522628784, Final Batch Loss: 1.3461952209472656\n",
      "Subject 3, Epoch 36, Loss: 3.885188937187195, Final Batch Loss: 1.3003478050231934\n",
      "Subject 3, Epoch 37, Loss: 3.833825707435608, Final Batch Loss: 1.247145175933838\n",
      "Subject 3, Epoch 38, Loss: 3.732139825820923, Final Batch Loss: 1.2721370458602905\n",
      "Subject 3, Epoch 39, Loss: 3.7201722860336304, Final Batch Loss: 1.2218804359436035\n",
      "Subject 3, Epoch 40, Loss: 3.65436053276062, Final Batch Loss: 1.2123465538024902\n",
      "Subject 3, Epoch 41, Loss: 3.589945912361145, Final Batch Loss: 1.1660349369049072\n",
      "Subject 3, Epoch 42, Loss: 3.6433995962142944, Final Batch Loss: 1.1733710765838623\n",
      "Subject 3, Epoch 43, Loss: 3.601952075958252, Final Batch Loss: 1.215966820716858\n",
      "Subject 3, Epoch 44, Loss: 3.617557644844055, Final Batch Loss: 1.1736180782318115\n",
      "Subject 3, Epoch 45, Loss: 3.5854727029800415, Final Batch Loss: 1.2020572423934937\n",
      "Subject 3, Epoch 46, Loss: 3.628553032875061, Final Batch Loss: 1.1720279455184937\n",
      "Subject 3, Epoch 47, Loss: 3.5815521478652954, Final Batch Loss: 1.1864219903945923\n",
      "Subject 3, Epoch 48, Loss: 3.5817936658859253, Final Batch Loss: 1.196727991104126\n",
      "Subject 3, Epoch 49, Loss: 3.572219967842102, Final Batch Loss: 1.2116328477859497\n",
      "Subject 3, Epoch 50, Loss: 3.5147818326950073, Final Batch Loss: 1.1490280628204346\n",
      "Subject 3, Epoch 51, Loss: 3.6288485527038574, Final Batch Loss: 1.2187083959579468\n",
      "Subject 3, Epoch 52, Loss: 3.663583993911743, Final Batch Loss: 1.183702826499939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 53, Loss: 3.491973638534546, Final Batch Loss: 1.1192259788513184\n",
      "Subject 3, Epoch 54, Loss: 3.518079996109009, Final Batch Loss: 1.1713420152664185\n",
      "Subject 3, Epoch 55, Loss: 3.5792651176452637, Final Batch Loss: 1.1784290075302124\n",
      "Subject 3, Epoch 56, Loss: 3.530854821205139, Final Batch Loss: 1.1501708030700684\n",
      "Subject 3, Epoch 57, Loss: 3.4525903463363647, Final Batch Loss: 1.1041268110275269\n",
      "Subject 3, Epoch 58, Loss: 3.5519450902938843, Final Batch Loss: 1.1399633884429932\n",
      "Subject 3, Epoch 59, Loss: 3.474190592765808, Final Batch Loss: 1.1707137823104858\n",
      "Subject 3, Epoch 60, Loss: 3.39520800113678, Final Batch Loss: 1.1262184381484985\n",
      "Subject 3, Epoch 61, Loss: 3.457419514656067, Final Batch Loss: 1.1546008586883545\n",
      "Subject 3, Epoch 62, Loss: 3.463668704032898, Final Batch Loss: 1.1633628606796265\n",
      "Subject 3, Epoch 63, Loss: 3.46171498298645, Final Batch Loss: 1.13887619972229\n",
      "Subject 3, Epoch 64, Loss: 3.3650550842285156, Final Batch Loss: 1.1571966409683228\n",
      "Subject 3, Epoch 65, Loss: 3.412622570991516, Final Batch Loss: 1.1376971006393433\n",
      "Subject 3, Epoch 66, Loss: 3.5172359943389893, Final Batch Loss: 1.1939159631729126\n",
      "Subject 3, Epoch 67, Loss: 3.538139224052429, Final Batch Loss: 1.2326349020004272\n",
      "Subject 3, Epoch 68, Loss: 3.3910562992095947, Final Batch Loss: 1.1316465139389038\n",
      "Subject 3, Epoch 69, Loss: 3.3916019201278687, Final Batch Loss: 1.1837129592895508\n",
      "Subject 3, Epoch 70, Loss: 3.4353195428848267, Final Batch Loss: 1.1614185571670532\n",
      "Subject 3, Epoch 71, Loss: 3.4429014921188354, Final Batch Loss: 1.1359755992889404\n",
      "Subject 3, Epoch 72, Loss: 3.493933320045471, Final Batch Loss: 1.1770492792129517\n",
      "Subject 3, Epoch 73, Loss: 3.367785096168518, Final Batch Loss: 1.1456154584884644\n",
      "Subject 3, Epoch 74, Loss: 3.3836125135421753, Final Batch Loss: 1.1169112920761108\n",
      "Subject 3, Epoch 75, Loss: 3.3885338306427, Final Batch Loss: 1.1482672691345215\n",
      "Subject 3, Epoch 76, Loss: 3.4890068769454956, Final Batch Loss: 1.1325119733810425\n",
      "Subject 3, Epoch 77, Loss: 3.402437210083008, Final Batch Loss: 1.136839747428894\n",
      "Subject 3, Epoch 78, Loss: 3.3979039192199707, Final Batch Loss: 1.1457009315490723\n",
      "Subject 3, Epoch 79, Loss: 3.3214871883392334, Final Batch Loss: 1.0711950063705444\n",
      "Subject 3, Epoch 80, Loss: 3.241311550140381, Final Batch Loss: 1.0766210556030273\n",
      "Subject 3, Epoch 81, Loss: 3.364504098892212, Final Batch Loss: 1.1295905113220215\n",
      "Subject 3, Epoch 82, Loss: 3.334967613220215, Final Batch Loss: 1.0786480903625488\n",
      "Subject 3, Epoch 83, Loss: 3.4050110578536987, Final Batch Loss: 1.1198302507400513\n",
      "Subject 3, Epoch 84, Loss: 3.290258049964905, Final Batch Loss: 1.1361632347106934\n",
      "Subject 3, Epoch 85, Loss: 3.2870023250579834, Final Batch Loss: 1.093766212463379\n",
      "Subject 3, Epoch 86, Loss: 3.2347952127456665, Final Batch Loss: 1.0630611181259155\n",
      "Subject 3, Epoch 87, Loss: 3.311827301979065, Final Batch Loss: 1.072059988975525\n",
      "Subject 3, Epoch 88, Loss: 3.224037528038025, Final Batch Loss: 1.0672770738601685\n",
      "Subject 3, Epoch 89, Loss: 3.3338189125061035, Final Batch Loss: 1.0970484018325806\n",
      "Subject 3, Epoch 90, Loss: 3.2199547290802, Final Batch Loss: 1.0543274879455566\n",
      "Subject 3, Epoch 91, Loss: 3.215584635734558, Final Batch Loss: 1.1237834692001343\n",
      "Subject 3, Epoch 92, Loss: 3.2262805700302124, Final Batch Loss: 1.054539680480957\n",
      "Subject 3, Epoch 93, Loss: 3.291480541229248, Final Batch Loss: 1.0730923414230347\n",
      "Subject 3, Epoch 94, Loss: 3.2835073471069336, Final Batch Loss: 1.105587124824524\n",
      "Subject 3, Epoch 95, Loss: 3.2230100631713867, Final Batch Loss: 1.057016134262085\n",
      "Subject 3, Epoch 96, Loss: 3.1153711080551147, Final Batch Loss: 1.0413260459899902\n",
      "Subject 3, Epoch 97, Loss: 3.090479254722595, Final Batch Loss: 1.0335235595703125\n",
      "Subject 3, Epoch 98, Loss: 3.007018566131592, Final Batch Loss: 0.9941152334213257\n",
      "Subject 3, Epoch 99, Loss: 3.103424906730652, Final Batch Loss: 1.0252488851547241\n",
      "Subject 3, Epoch 100, Loss: 3.0935301780700684, Final Batch Loss: 1.039325475692749\n",
      "Subject 3, Epoch 101, Loss: 3.0355209708213806, Final Batch Loss: 1.0040194988250732\n",
      "Subject 3, Epoch 102, Loss: 2.9258471727371216, Final Batch Loss: 1.0061159133911133\n",
      "Subject 3, Epoch 103, Loss: 2.9135740995407104, Final Batch Loss: 0.9878032803535461\n",
      "Subject 3, Epoch 104, Loss: 2.91034072637558, Final Batch Loss: 0.9911601543426514\n",
      "Subject 3, Epoch 105, Loss: 2.8872960209846497, Final Batch Loss: 1.0032085180282593\n",
      "Subject 3, Epoch 106, Loss: 2.8523272275924683, Final Batch Loss: 0.9631209969520569\n",
      "Subject 3, Epoch 107, Loss: 2.812876582145691, Final Batch Loss: 0.9372310638427734\n",
      "Subject 3, Epoch 108, Loss: 2.762182116508484, Final Batch Loss: 0.8725326061248779\n",
      "Subject 3, Epoch 109, Loss: 2.5946081280708313, Final Batch Loss: 0.8800570964813232\n",
      "Subject 3, Epoch 110, Loss: 2.6711297631263733, Final Batch Loss: 0.8961138725280762\n",
      "Subject 3, Epoch 111, Loss: 2.588623344898224, Final Batch Loss: 0.8429182767868042\n",
      "Subject 3, Epoch 112, Loss: 2.6622783541679382, Final Batch Loss: 0.9057896137237549\n",
      "Subject 3, Epoch 113, Loss: 2.5976200103759766, Final Batch Loss: 0.8500863909721375\n",
      "Subject 3, Epoch 114, Loss: 2.3784347772598267, Final Batch Loss: 0.7968717217445374\n",
      "Subject 3, Epoch 115, Loss: 2.4279873967170715, Final Batch Loss: 0.8084190487861633\n",
      "Subject 3, Epoch 116, Loss: 2.415224850177765, Final Batch Loss: 0.8227086067199707\n",
      "Subject 3, Epoch 117, Loss: 2.3996723890304565, Final Batch Loss: 0.7824638485908508\n",
      "Subject 3, Epoch 118, Loss: 2.3162880539894104, Final Batch Loss: 0.8467094898223877\n",
      "Subject 3, Epoch 119, Loss: 2.348648190498352, Final Batch Loss: 0.763463020324707\n",
      "Subject 3, Epoch 120, Loss: 2.388402581214905, Final Batch Loss: 0.7936583757400513\n",
      "Subject 3, Epoch 121, Loss: 2.1469998359680176, Final Batch Loss: 0.7339162826538086\n",
      "Subject 3, Epoch 122, Loss: 2.2626461386680603, Final Batch Loss: 0.7984548211097717\n",
      "Subject 3, Epoch 123, Loss: 2.1704611778259277, Final Batch Loss: 0.6847879886627197\n",
      "Subject 3, Epoch 124, Loss: 2.154313087463379, Final Batch Loss: 0.6722320318222046\n",
      "Subject 3, Epoch 125, Loss: 2.1214094161987305, Final Batch Loss: 0.6917464733123779\n",
      "Subject 3, Epoch 126, Loss: 2.1290374994277954, Final Batch Loss: 0.8126511573791504\n",
      "Subject 3, Epoch 127, Loss: 2.0797005891799927, Final Batch Loss: 0.7563307285308838\n",
      "Subject 3, Epoch 128, Loss: 1.974917232990265, Final Batch Loss: 0.583772599697113\n",
      "Subject 3, Epoch 129, Loss: 2.0474416613578796, Final Batch Loss: 0.6589297652244568\n",
      "Subject 3, Epoch 130, Loss: 2.058788239955902, Final Batch Loss: 0.6866613030433655\n",
      "Subject 3, Epoch 131, Loss: 1.9706151485443115, Final Batch Loss: 0.6632125377655029\n",
      "Subject 3, Epoch 132, Loss: 1.9384315609931946, Final Batch Loss: 0.6929300427436829\n",
      "Subject 3, Epoch 133, Loss: 1.8972017168998718, Final Batch Loss: 0.6042158007621765\n",
      "Subject 3, Epoch 134, Loss: 1.7970025539398193, Final Batch Loss: 0.6394525766372681\n",
      "Subject 3, Epoch 135, Loss: 1.8988674879074097, Final Batch Loss: 0.6330626606941223\n",
      "Subject 3, Epoch 136, Loss: 1.8497469425201416, Final Batch Loss: 0.6174449324607849\n",
      "Subject 3, Epoch 137, Loss: 1.9171462059020996, Final Batch Loss: 0.7168208956718445\n",
      "Subject 3, Epoch 138, Loss: 1.7689538598060608, Final Batch Loss: 0.5737557411193848\n",
      "Subject 3, Epoch 139, Loss: 1.8987274169921875, Final Batch Loss: 0.6508992910385132\n",
      "Subject 3, Epoch 140, Loss: 1.910414218902588, Final Batch Loss: 0.6533697247505188\n",
      "Subject 3, Epoch 141, Loss: 1.8934933543205261, Final Batch Loss: 0.6387093663215637\n",
      "Subject 3, Epoch 142, Loss: 1.8812813758850098, Final Batch Loss: 0.573056697845459\n",
      "Subject 3, Epoch 143, Loss: 1.6078300476074219, Final Batch Loss: 0.5162742733955383\n",
      "Subject 3, Epoch 144, Loss: 1.7747568488121033, Final Batch Loss: 0.6637489795684814\n",
      "Subject 3, Epoch 145, Loss: 1.6829436421394348, Final Batch Loss: 0.5722986459732056\n",
      "Subject 3, Epoch 146, Loss: 1.8872107863426208, Final Batch Loss: 0.6855478286743164\n",
      "Subject 3, Epoch 147, Loss: 1.6363247632980347, Final Batch Loss: 0.5744329690933228\n",
      "Subject 3, Epoch 148, Loss: 1.786172330379486, Final Batch Loss: 0.5987722277641296\n",
      "Subject 3, Epoch 149, Loss: 1.700978398323059, Final Batch Loss: 0.5435600280761719\n",
      "Subject 3, Epoch 150, Loss: 1.7625526189804077, Final Batch Loss: 0.6053207516670227\n",
      "Subject 3, Epoch 151, Loss: 1.8583795428276062, Final Batch Loss: 0.7068281769752502\n",
      "Subject 3, Epoch 152, Loss: 1.8051756024360657, Final Batch Loss: 0.5607218146324158\n",
      "Subject 3, Epoch 153, Loss: 1.6461624503135681, Final Batch Loss: 0.5754774808883667\n",
      "Subject 3, Epoch 154, Loss: 1.7398781180381775, Final Batch Loss: 0.583747923374176\n",
      "Subject 3, Epoch 155, Loss: 1.656720757484436, Final Batch Loss: 0.5233264565467834\n",
      "Subject 3, Epoch 156, Loss: 1.7273156642913818, Final Batch Loss: 0.5914644002914429\n",
      "Subject 3, Epoch 157, Loss: 1.6716944575309753, Final Batch Loss: 0.5568290948867798\n",
      "Subject 3, Epoch 158, Loss: 1.679556429386139, Final Batch Loss: 0.5608461499214172\n",
      "Subject 3, Epoch 159, Loss: 1.5818189978599548, Final Batch Loss: 0.4642040729522705\n",
      "Subject 3, Epoch 160, Loss: 1.811324417591095, Final Batch Loss: 0.6257839202880859\n",
      "Subject 3, Epoch 161, Loss: 1.6710858941078186, Final Batch Loss: 0.6699896454811096\n",
      "Subject 3, Epoch 162, Loss: 1.6050905287265778, Final Batch Loss: 0.5235533118247986\n",
      "Subject 3, Epoch 163, Loss: 1.6651697158813477, Final Batch Loss: 0.572300374507904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 164, Loss: 1.5980414152145386, Final Batch Loss: 0.5732644200325012\n",
      "Subject 3, Epoch 165, Loss: 1.6589315533638, Final Batch Loss: 0.47059082984924316\n",
      "Subject 3, Epoch 166, Loss: 1.644020140171051, Final Batch Loss: 0.5243091583251953\n",
      "Subject 3, Epoch 167, Loss: 1.553817242383957, Final Batch Loss: 0.4944288432598114\n",
      "Subject 3, Epoch 168, Loss: 1.467583417892456, Final Batch Loss: 0.5292543172836304\n",
      "Subject 3, Epoch 169, Loss: 1.6492996215820312, Final Batch Loss: 0.5171147584915161\n",
      "Subject 3, Epoch 170, Loss: 1.6914797127246857, Final Batch Loss: 0.5766249895095825\n",
      "Subject 3, Epoch 171, Loss: 1.504301905632019, Final Batch Loss: 0.4465034008026123\n",
      "Subject 3, Epoch 172, Loss: 1.520310252904892, Final Batch Loss: 0.465730756521225\n",
      "Subject 3, Epoch 173, Loss: 1.5054413974285126, Final Batch Loss: 0.48603036999702454\n",
      "Subject 3, Epoch 174, Loss: 1.5801284611225128, Final Batch Loss: 0.6248759627342224\n",
      "Subject 3, Epoch 175, Loss: 1.5326718091964722, Final Batch Loss: 0.49148207902908325\n",
      "Subject 3, Epoch 176, Loss: 1.5938712358474731, Final Batch Loss: 0.5526678562164307\n",
      "Subject 3, Epoch 177, Loss: 1.5386628806591034, Final Batch Loss: 0.5040056109428406\n",
      "Subject 3, Epoch 178, Loss: 1.6116626262664795, Final Batch Loss: 0.649044930934906\n",
      "Subject 3, Epoch 179, Loss: 1.6058964133262634, Final Batch Loss: 0.5018346905708313\n",
      "Subject 3, Epoch 180, Loss: 1.3922256529331207, Final Batch Loss: 0.496578574180603\n",
      "Subject 3, Epoch 181, Loss: 1.5071640312671661, Final Batch Loss: 0.46730002760887146\n",
      "Subject 3, Epoch 182, Loss: 1.3985984027385712, Final Batch Loss: 0.4848591685295105\n",
      "Subject 3, Epoch 183, Loss: 1.547749638557434, Final Batch Loss: 0.5943876504898071\n",
      "Subject 3, Epoch 184, Loss: 1.5859791934490204, Final Batch Loss: 0.4906226098537445\n",
      "Subject 3, Epoch 185, Loss: 1.4514103829860687, Final Batch Loss: 0.5057318806648254\n",
      "Subject 3, Epoch 186, Loss: 1.4791743159294128, Final Batch Loss: 0.4744480550289154\n",
      "Subject 3, Epoch 187, Loss: 1.454399049282074, Final Batch Loss: 0.4225321412086487\n",
      "Subject 3, Epoch 188, Loss: 1.400452435016632, Final Batch Loss: 0.5025473833084106\n",
      "Subject 3, Epoch 189, Loss: 1.5170365571975708, Final Batch Loss: 0.53058260679245\n",
      "Subject 3, Epoch 190, Loss: 1.4865638613700867, Final Batch Loss: 0.5427939891815186\n",
      "Subject 3, Epoch 191, Loss: 1.5420834720134735, Final Batch Loss: 0.4395906627178192\n",
      "Subject 3, Epoch 192, Loss: 1.4381739795207977, Final Batch Loss: 0.45109814405441284\n",
      "Subject 3, Epoch 193, Loss: 1.4159833490848541, Final Batch Loss: 0.4558822214603424\n",
      "Subject 3, Epoch 194, Loss: 1.3752412796020508, Final Batch Loss: 0.42595964670181274\n",
      "Subject 3, Epoch 195, Loss: 1.3393381536006927, Final Batch Loss: 0.46100103855133057\n",
      "Subject 3, Epoch 196, Loss: 1.329547792673111, Final Batch Loss: 0.4974677860736847\n",
      "Subject 3, Epoch 197, Loss: 1.453097641468048, Final Batch Loss: 0.5530497431755066\n",
      "Subject 3, Epoch 198, Loss: 1.4401598274707794, Final Batch Loss: 0.46464022994041443\n",
      "Subject 3, Epoch 199, Loss: 1.5113409161567688, Final Batch Loss: 0.5326460599899292\n",
      "Subject 3, Epoch 200, Loss: 1.39529150724411, Final Batch Loss: 0.5267632007598877\n",
      "Subject 3, Epoch 201, Loss: 1.429218351840973, Final Batch Loss: 0.5267285704612732\n",
      "Subject 3, Epoch 202, Loss: 1.4660800099372864, Final Batch Loss: 0.5155694484710693\n",
      "Subject 3, Epoch 203, Loss: 1.3647270500659943, Final Batch Loss: 0.3660285770893097\n",
      "Subject 3, Epoch 204, Loss: 1.4777023196220398, Final Batch Loss: 0.42228198051452637\n",
      "Subject 3, Epoch 205, Loss: 1.4685936570167542, Final Batch Loss: 0.5197834372520447\n",
      "Subject 3, Epoch 206, Loss: 1.375295490026474, Final Batch Loss: 0.43137192726135254\n",
      "Subject 3, Epoch 207, Loss: 1.4318868517875671, Final Batch Loss: 0.526656448841095\n",
      "Subject 3, Epoch 208, Loss: 1.3835002183914185, Final Batch Loss: 0.4491470158100128\n",
      "Subject 3, Epoch 209, Loss: 1.412924885749817, Final Batch Loss: 0.5011014342308044\n",
      "Subject 3, Epoch 210, Loss: 1.4568608105182648, Final Batch Loss: 0.4838870167732239\n",
      "Subject 3, Epoch 211, Loss: 1.3939158916473389, Final Batch Loss: 0.521711528301239\n",
      "Subject 3, Epoch 212, Loss: 1.331365704536438, Final Batch Loss: 0.34368696808815\n",
      "Subject 3, Epoch 213, Loss: 1.4428669810295105, Final Batch Loss: 0.4869852662086487\n",
      "Subject 3, Epoch 214, Loss: 1.326438069343567, Final Batch Loss: 0.4499105215072632\n",
      "Subject 3, Epoch 215, Loss: 1.4228810966014862, Final Batch Loss: 0.4583786427974701\n",
      "Subject 3, Epoch 216, Loss: 1.3191228806972504, Final Batch Loss: 0.42357590794563293\n",
      "Subject 3, Epoch 217, Loss: 1.2775711119174957, Final Batch Loss: 0.4090310037136078\n",
      "Subject 3, Epoch 218, Loss: 1.3990267515182495, Final Batch Loss: 0.4575614929199219\n",
      "Subject 3, Epoch 219, Loss: 1.287946730852127, Final Batch Loss: 0.47464922070503235\n",
      "Subject 3, Epoch 220, Loss: 1.4177261590957642, Final Batch Loss: 0.4129023551940918\n",
      "Subject 3, Epoch 221, Loss: 1.307884156703949, Final Batch Loss: 0.5115880370140076\n",
      "Subject 3, Epoch 222, Loss: 1.3223659694194794, Final Batch Loss: 0.4356803894042969\n",
      "Subject 3, Epoch 223, Loss: 1.272755652666092, Final Batch Loss: 0.4032876789569855\n",
      "Subject 3, Epoch 224, Loss: 1.4243459701538086, Final Batch Loss: 0.4579324722290039\n",
      "Subject 3, Epoch 225, Loss: 1.3547878861427307, Final Batch Loss: 0.5062815546989441\n",
      "Subject 3, Epoch 226, Loss: 1.1687849760055542, Final Batch Loss: 0.43840914964675903\n",
      "Subject 3, Epoch 227, Loss: 1.3794580698013306, Final Batch Loss: 0.550322413444519\n",
      "Subject 3, Epoch 228, Loss: 1.3665452897548676, Final Batch Loss: 0.4870382249355316\n",
      "Subject 3, Epoch 229, Loss: 1.384716421365738, Final Batch Loss: 0.47439706325531006\n",
      "Subject 3, Epoch 230, Loss: 1.2860011756420135, Final Batch Loss: 0.4300198256969452\n",
      "Subject 3, Epoch 231, Loss: 1.2623401582241058, Final Batch Loss: 0.3908245265483856\n",
      "Subject 3, Epoch 232, Loss: 1.401106059551239, Final Batch Loss: 0.5045490860939026\n",
      "Subject 3, Epoch 233, Loss: 1.1846001744270325, Final Batch Loss: 0.3550330698490143\n",
      "Subject 3, Epoch 234, Loss: 1.3531759977340698, Final Batch Loss: 0.4789036810398102\n",
      "Subject 3, Epoch 235, Loss: 1.3611631393432617, Final Batch Loss: 0.6014294624328613\n",
      "Subject 3, Epoch 236, Loss: 1.2828565537929535, Final Batch Loss: 0.5365153551101685\n",
      "Subject 3, Epoch 237, Loss: 1.346833050251007, Final Batch Loss: 0.5341473817825317\n",
      "Subject 3, Epoch 238, Loss: 1.3109613955020905, Final Batch Loss: 0.4957082271575928\n",
      "Subject 3, Epoch 239, Loss: 1.438920259475708, Final Batch Loss: 0.4203660786151886\n",
      "Subject 3, Epoch 240, Loss: 1.4369355142116547, Final Batch Loss: 0.5437440872192383\n",
      "Subject 3, Epoch 241, Loss: 1.3229641914367676, Final Batch Loss: 0.5564112663269043\n",
      "Subject 3, Epoch 242, Loss: 1.3757427334785461, Final Batch Loss: 0.4457128942012787\n",
      "Subject 3, Epoch 243, Loss: 1.3516713380813599, Final Batch Loss: 0.44428518414497375\n",
      "Subject 3, Epoch 244, Loss: 1.2876352071762085, Final Batch Loss: 0.3391260802745819\n",
      "Subject 3, Epoch 245, Loss: 1.271896094083786, Final Batch Loss: 0.42091912031173706\n",
      "Subject 3, Epoch 246, Loss: 1.1790426671504974, Final Batch Loss: 0.4173958897590637\n",
      "Subject 3, Epoch 247, Loss: 1.4407942295074463, Final Batch Loss: 0.5306892395019531\n",
      "Subject 3, Epoch 248, Loss: 1.296918123960495, Final Batch Loss: 0.37338876724243164\n",
      "Subject 3, Epoch 249, Loss: 1.1670993566513062, Final Batch Loss: 0.3500160872936249\n",
      "Subject 3, Epoch 250, Loss: 1.157783567905426, Final Batch Loss: 0.3580286204814911\n",
      "Subject 3, Epoch 251, Loss: 1.2850828170776367, Final Batch Loss: 0.3921646177768707\n",
      "Subject 3, Epoch 252, Loss: 1.163485437631607, Final Batch Loss: 0.3848930597305298\n",
      "Subject 3, Epoch 253, Loss: 1.2347261905670166, Final Batch Loss: 0.38147568702697754\n",
      "Subject 3, Epoch 254, Loss: 1.2653612792491913, Final Batch Loss: 0.3967794179916382\n",
      "Subject 3, Epoch 255, Loss: 1.2353832423686981, Final Batch Loss: 0.3547506630420685\n",
      "Subject 3, Epoch 256, Loss: 1.314253568649292, Final Batch Loss: 0.45501241087913513\n",
      "Subject 3, Epoch 257, Loss: 1.1857887208461761, Final Batch Loss: 0.4384993612766266\n",
      "Subject 3, Epoch 258, Loss: 1.1482720077037811, Final Batch Loss: 0.34213709831237793\n",
      "Subject 3, Epoch 259, Loss: 1.1469481885433197, Final Batch Loss: 0.4595388174057007\n",
      "Subject 3, Epoch 260, Loss: 1.3162061274051666, Final Batch Loss: 0.4763364791870117\n",
      "Subject 3, Epoch 261, Loss: 1.2792324721813202, Final Batch Loss: 0.5015990734100342\n",
      "Subject 3, Epoch 262, Loss: 1.1044143438339233, Final Batch Loss: 0.2758880853652954\n",
      "Subject 3, Epoch 263, Loss: 1.1743386685848236, Final Batch Loss: 0.4044623374938965\n",
      "Subject 3, Epoch 264, Loss: 1.1449090242385864, Final Batch Loss: 0.39557862281799316\n",
      "Subject 3, Epoch 265, Loss: 1.1375911235809326, Final Batch Loss: 0.3579138517379761\n",
      "Subject 3, Epoch 266, Loss: 1.2787722945213318, Final Batch Loss: 0.5240457653999329\n",
      "Subject 3, Epoch 267, Loss: 1.239259034395218, Final Batch Loss: 0.4359706938266754\n",
      "Subject 3, Epoch 268, Loss: 1.0929162502288818, Final Batch Loss: 0.3788011372089386\n",
      "Subject 3, Epoch 269, Loss: 1.1629148125648499, Final Batch Loss: 0.38167765736579895\n",
      "Subject 3, Epoch 270, Loss: 1.1506844758987427, Final Batch Loss: 0.3195551037788391\n",
      "Subject 3, Epoch 271, Loss: 1.2127668261528015, Final Batch Loss: 0.4664711058139801\n",
      "Subject 3, Epoch 272, Loss: 1.2137932479381561, Final Batch Loss: 0.36229732632637024\n",
      "Subject 3, Epoch 273, Loss: 1.1703062057495117, Final Batch Loss: 0.3833942115306854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 274, Loss: 1.1997832655906677, Final Batch Loss: 0.3568613827228546\n",
      "Subject 3, Epoch 275, Loss: 1.147066742181778, Final Batch Loss: 0.338214248418808\n",
      "Subject 3, Epoch 276, Loss: 1.1739782392978668, Final Batch Loss: 0.4287448823451996\n",
      "Subject 3, Epoch 277, Loss: 1.1544966399669647, Final Batch Loss: 0.389423131942749\n",
      "Subject 3, Epoch 278, Loss: 1.165767252445221, Final Batch Loss: 0.4595804810523987\n",
      "Subject 3, Epoch 279, Loss: 1.2103889286518097, Final Batch Loss: 0.42528483271598816\n",
      "Subject 3, Epoch 280, Loss: 1.1571467816829681, Final Batch Loss: 0.39843234419822693\n",
      "Subject 3, Epoch 281, Loss: 1.2267980575561523, Final Batch Loss: 0.353665292263031\n",
      "Subject 3, Epoch 282, Loss: 1.1152627170085907, Final Batch Loss: 0.3507370352745056\n",
      "Subject 3, Epoch 283, Loss: 1.1571506559848785, Final Batch Loss: 0.45246872305870056\n",
      "Subject 3, Epoch 284, Loss: 1.1934611201286316, Final Batch Loss: 0.43891966342926025\n",
      "Subject 3, Epoch 285, Loss: 1.1424117982387543, Final Batch Loss: 0.4071454107761383\n",
      "Subject 3, Epoch 286, Loss: 1.0896650552749634, Final Batch Loss: 0.32364434003829956\n",
      "Subject 3, Epoch 287, Loss: 1.2279300391674042, Final Batch Loss: 0.3747641444206238\n",
      "Subject 3, Epoch 288, Loss: 1.181412786245346, Final Batch Loss: 0.33892539143562317\n",
      "Subject 3, Epoch 289, Loss: 1.3446285128593445, Final Batch Loss: 0.442573606967926\n",
      "Subject 3, Epoch 290, Loss: 1.1367639005184174, Final Batch Loss: 0.44009584188461304\n",
      "Subject 3, Epoch 291, Loss: 1.115837574005127, Final Batch Loss: 0.3591880202293396\n",
      "Subject 3, Epoch 292, Loss: 1.0752858519554138, Final Batch Loss: 0.3437182903289795\n",
      "Subject 3, Epoch 293, Loss: 1.2076416909694672, Final Batch Loss: 0.41232576966285706\n",
      "Subject 3, Epoch 294, Loss: 1.126792073249817, Final Batch Loss: 0.3937065005302429\n",
      "Subject 3, Epoch 295, Loss: 1.098852425813675, Final Batch Loss: 0.28978320956230164\n",
      "Subject 3, Epoch 296, Loss: 1.101853847503662, Final Batch Loss: 0.38503798842430115\n",
      "Subject 3, Epoch 297, Loss: 1.048670470714569, Final Batch Loss: 0.31652477383613586\n",
      "Subject 3, Epoch 298, Loss: 1.208310753107071, Final Batch Loss: 0.41581475734710693\n",
      "Subject 3, Epoch 299, Loss: 1.1345285773277283, Final Batch Loss: 0.3680681586265564\n",
      "Subject 3, Epoch 300, Loss: 1.1620711386203766, Final Batch Loss: 0.4486730396747589\n",
      "Subject 3, Epoch 301, Loss: 1.2198225855827332, Final Batch Loss: 0.4162735641002655\n",
      "Subject 3, Epoch 302, Loss: 1.0871139168739319, Final Batch Loss: 0.37760302424430847\n",
      "Subject 3, Epoch 303, Loss: 1.268295407295227, Final Batch Loss: 0.46745672821998596\n",
      "Subject 3, Epoch 304, Loss: 1.073516070842743, Final Batch Loss: 0.3758821487426758\n",
      "Subject 3, Epoch 305, Loss: 1.0982604622840881, Final Batch Loss: 0.4092937409877777\n",
      "Subject 3, Epoch 306, Loss: 1.0691551566123962, Final Batch Loss: 0.3842933177947998\n",
      "Subject 3, Epoch 307, Loss: 1.1201870441436768, Final Batch Loss: 0.3759593963623047\n",
      "Subject 3, Epoch 308, Loss: 1.1195879876613617, Final Batch Loss: 0.377101331949234\n",
      "Subject 3, Epoch 309, Loss: 1.1432925760746002, Final Batch Loss: 0.2950341999530792\n",
      "Subject 3, Epoch 310, Loss: 1.0963453650474548, Final Batch Loss: 0.30120834708213806\n",
      "Subject 3, Epoch 311, Loss: 1.2065284252166748, Final Batch Loss: 0.3475574553012848\n",
      "Subject 3, Epoch 312, Loss: 1.035468965768814, Final Batch Loss: 0.40265920758247375\n",
      "Subject 3, Epoch 313, Loss: 1.164005160331726, Final Batch Loss: 0.3967493772506714\n",
      "Subject 3, Epoch 314, Loss: 1.2298007309436798, Final Batch Loss: 0.4132324159145355\n",
      "Subject 3, Epoch 315, Loss: 1.0575312077999115, Final Batch Loss: 0.3074629604816437\n",
      "Subject 3, Epoch 316, Loss: 1.0632683336734772, Final Batch Loss: 0.3333202600479126\n",
      "Subject 3, Epoch 317, Loss: 1.047902226448059, Final Batch Loss: 0.395673543214798\n",
      "Subject 3, Epoch 318, Loss: 0.998632550239563, Final Batch Loss: 0.2551763653755188\n",
      "Subject 3, Epoch 319, Loss: 1.1413568258285522, Final Batch Loss: 0.30872878432273865\n",
      "Subject 3, Epoch 320, Loss: 1.159702718257904, Final Batch Loss: 0.38351690769195557\n",
      "Subject 3, Epoch 321, Loss: 1.0739913284778595, Final Batch Loss: 0.3040257692337036\n",
      "Subject 3, Epoch 322, Loss: 1.0921885967254639, Final Batch Loss: 0.3703669309616089\n",
      "Subject 3, Epoch 323, Loss: 1.0824377834796906, Final Batch Loss: 0.41244474053382874\n",
      "Subject 3, Epoch 324, Loss: 1.1588581502437592, Final Batch Loss: 0.41332364082336426\n",
      "Subject 3, Epoch 325, Loss: 1.0674763917922974, Final Batch Loss: 0.3351723253726959\n",
      "Subject 3, Epoch 326, Loss: 1.070385903120041, Final Batch Loss: 0.3226322829723358\n",
      "Subject 3, Epoch 327, Loss: 1.078755408525467, Final Batch Loss: 0.3655899465084076\n",
      "Subject 3, Epoch 328, Loss: 1.1419736742973328, Final Batch Loss: 0.36764267086982727\n",
      "Subject 3, Epoch 329, Loss: 1.09649258852005, Final Batch Loss: 0.37470513582229614\n",
      "Subject 3, Epoch 330, Loss: 1.2079653441905975, Final Batch Loss: 0.5004966259002686\n",
      "Subject 3, Epoch 331, Loss: 1.091467559337616, Final Batch Loss: 0.3560192286968231\n",
      "Subject 3, Epoch 332, Loss: 1.1953854262828827, Final Batch Loss: 0.29869934916496277\n",
      "Subject 3, Epoch 333, Loss: 1.0803305804729462, Final Batch Loss: 0.34550487995147705\n",
      "Subject 3, Epoch 334, Loss: 1.0643672347068787, Final Batch Loss: 0.38113072514533997\n",
      "Subject 3, Epoch 335, Loss: 1.08993262052536, Final Batch Loss: 0.3749406337738037\n",
      "Subject 3, Epoch 336, Loss: 1.0288704633712769, Final Batch Loss: 0.3827929198741913\n",
      "Subject 3, Epoch 337, Loss: 1.0442358255386353, Final Batch Loss: 0.2991877794265747\n",
      "Subject 3, Epoch 338, Loss: 1.019174963235855, Final Batch Loss: 0.3471124470233917\n",
      "Subject 3, Epoch 339, Loss: 1.0332500636577606, Final Batch Loss: 0.3546803593635559\n",
      "Subject 3, Epoch 340, Loss: 0.9363343417644501, Final Batch Loss: 0.2621796429157257\n",
      "Subject 3, Epoch 341, Loss: 1.052702248096466, Final Batch Loss: 0.3018428087234497\n",
      "Subject 3, Epoch 342, Loss: 1.059242695569992, Final Batch Loss: 0.4344298243522644\n",
      "Subject 3, Epoch 343, Loss: 1.0538086295127869, Final Batch Loss: 0.38637471199035645\n",
      "Subject 3, Epoch 344, Loss: 1.0054210722446442, Final Batch Loss: 0.4067522883415222\n",
      "Subject 3, Epoch 345, Loss: 1.1285737752914429, Final Batch Loss: 0.3280717730522156\n",
      "Subject 3, Epoch 346, Loss: 1.0468628406524658, Final Batch Loss: 0.4475630819797516\n",
      "Subject 3, Epoch 347, Loss: 0.9814655780792236, Final Batch Loss: 0.3328452408313751\n",
      "Subject 3, Epoch 348, Loss: 1.0790414810180664, Final Batch Loss: 0.3742867410182953\n",
      "Subject 3, Epoch 349, Loss: 1.1490621268749237, Final Batch Loss: 0.2494114637374878\n",
      "Subject 3, Epoch 350, Loss: 1.022524654865265, Final Batch Loss: 0.31542477011680603\n",
      "Subject 3, Epoch 351, Loss: 1.107925295829773, Final Batch Loss: 0.3314797878265381\n",
      "Subject 3, Epoch 352, Loss: 1.1609008312225342, Final Batch Loss: 0.4407562017440796\n",
      "Subject 3, Epoch 353, Loss: 1.0377992987632751, Final Batch Loss: 0.34161606431007385\n",
      "Subject 3, Epoch 354, Loss: 1.0670213103294373, Final Batch Loss: 0.3844124376773834\n",
      "Subject 3, Epoch 355, Loss: 1.0319068729877472, Final Batch Loss: 0.3368578851222992\n",
      "Subject 3, Epoch 356, Loss: 1.0779397785663605, Final Batch Loss: 0.40062397718429565\n",
      "Subject 3, Epoch 357, Loss: 0.9824426770210266, Final Batch Loss: 0.28225839138031006\n",
      "Subject 3, Epoch 358, Loss: 1.1196115016937256, Final Batch Loss: 0.3672432005405426\n",
      "Subject 3, Epoch 359, Loss: 1.0430546700954437, Final Batch Loss: 0.3580102324485779\n",
      "Subject 3, Epoch 360, Loss: 1.0354859828948975, Final Batch Loss: 0.3312598168849945\n",
      "Subject 3, Epoch 361, Loss: 1.0070308446884155, Final Batch Loss: 0.3129267990589142\n",
      "Subject 3, Epoch 362, Loss: 1.1127186715602875, Final Batch Loss: 0.40134280920028687\n",
      "Subject 3, Epoch 363, Loss: 1.1703337728977203, Final Batch Loss: 0.47817757725715637\n",
      "Subject 3, Epoch 364, Loss: 1.0095443427562714, Final Batch Loss: 0.41831961274147034\n",
      "Subject 3, Epoch 365, Loss: 1.0591528713703156, Final Batch Loss: 0.376351922750473\n",
      "Subject 3, Epoch 366, Loss: 0.9823590815067291, Final Batch Loss: 0.34508374333381653\n",
      "Subject 3, Epoch 367, Loss: 1.0083401501178741, Final Batch Loss: 0.4311879873275757\n",
      "Subject 3, Epoch 368, Loss: 1.0622886419296265, Final Batch Loss: 0.43073007464408875\n",
      "Subject 3, Epoch 369, Loss: 1.0867740511894226, Final Batch Loss: 0.419352650642395\n",
      "Subject 3, Epoch 370, Loss: 1.07401841878891, Final Batch Loss: 0.34328988194465637\n",
      "Subject 3, Epoch 371, Loss: 1.0560537278652191, Final Batch Loss: 0.26907920837402344\n",
      "Subject 3, Epoch 372, Loss: 0.9794904887676239, Final Batch Loss: 0.2653675973415375\n",
      "Subject 3, Epoch 373, Loss: 1.008107841014862, Final Batch Loss: 0.3601665198802948\n",
      "Subject 3, Epoch 374, Loss: 1.0085238218307495, Final Batch Loss: 0.30587640404701233\n",
      "Subject 3, Epoch 375, Loss: 0.9580216109752655, Final Batch Loss: 0.2450382113456726\n",
      "Subject 3, Epoch 376, Loss: 1.0057511925697327, Final Batch Loss: 0.3283058702945709\n",
      "Subject 3, Epoch 377, Loss: 0.9999537765979767, Final Batch Loss: 0.2694479823112488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 378, Loss: 1.086035579442978, Final Batch Loss: 0.3346823453903198\n",
      "Subject 3, Epoch 379, Loss: 1.0981661677360535, Final Batch Loss: 0.43767476081848145\n",
      "Subject 3, Epoch 380, Loss: 1.0464741587638855, Final Batch Loss: 0.3896629810333252\n",
      "Subject 3, Epoch 381, Loss: 1.163585901260376, Final Batch Loss: 0.4050087332725525\n",
      "Subject 3, Epoch 382, Loss: 1.0275104939937592, Final Batch Loss: 0.31635570526123047\n",
      "Subject 3, Epoch 383, Loss: 0.940024733543396, Final Batch Loss: 0.3190041482448578\n",
      "Subject 3, Epoch 384, Loss: 0.955640971660614, Final Batch Loss: 0.33789634704589844\n",
      "Subject 3, Epoch 385, Loss: 1.0272994339466095, Final Batch Loss: 0.39703285694122314\n",
      "Subject 3, Epoch 386, Loss: 1.0971291661262512, Final Batch Loss: 0.4281982183456421\n",
      "Subject 3, Epoch 387, Loss: 1.0826922953128815, Final Batch Loss: 0.4350332021713257\n",
      "Subject 3, Epoch 388, Loss: 1.0268145501613617, Final Batch Loss: 0.35727429389953613\n",
      "Subject 3, Epoch 389, Loss: 1.0021335780620575, Final Batch Loss: 0.3273816406726837\n",
      "Subject 3, Epoch 390, Loss: 0.9270219802856445, Final Batch Loss: 0.28492841124534607\n",
      "Subject 3, Epoch 391, Loss: 1.0321259498596191, Final Batch Loss: 0.4107675850391388\n",
      "Subject 3, Epoch 392, Loss: 0.9572935998439789, Final Batch Loss: 0.26587191224098206\n",
      "Subject 3, Epoch 393, Loss: 0.9447526633739471, Final Batch Loss: 0.26823854446411133\n",
      "Subject 3, Epoch 394, Loss: 0.9326155334711075, Final Batch Loss: 0.21022124588489532\n",
      "Subject 3, Epoch 395, Loss: 1.0201391875743866, Final Batch Loss: 0.38802364468574524\n",
      "Subject 3, Epoch 396, Loss: 0.9978683292865753, Final Batch Loss: 0.368877649307251\n",
      "Subject 3, Epoch 397, Loss: 0.9165318757295609, Final Batch Loss: 0.22945795953273773\n",
      "Subject 3, Epoch 398, Loss: 1.0772035419940948, Final Batch Loss: 0.33898839354515076\n",
      "Subject 3, Epoch 399, Loss: 0.9887227118015289, Final Batch Loss: 0.2938455045223236\n",
      "Subject 3, Epoch 400, Loss: 0.9344280064105988, Final Batch Loss: 0.2872806489467621\n",
      "Subject 3, Epoch 401, Loss: 0.980051726102829, Final Batch Loss: 0.37374138832092285\n",
      "Subject 3, Epoch 402, Loss: 1.0391338765621185, Final Batch Loss: 0.3710668981075287\n",
      "Subject 3, Epoch 403, Loss: 0.9573702812194824, Final Batch Loss: 0.3790239691734314\n",
      "Subject 3, Epoch 404, Loss: 0.9328929483890533, Final Batch Loss: 0.31969380378723145\n",
      "Subject 3, Epoch 405, Loss: 0.8865742087364197, Final Batch Loss: 0.3326549232006073\n",
      "Subject 3, Epoch 406, Loss: 1.0151322185993195, Final Batch Loss: 0.37849050760269165\n",
      "Subject 3, Epoch 407, Loss: 0.9634948074817657, Final Batch Loss: 0.34892892837524414\n",
      "Subject 3, Epoch 408, Loss: 0.9389335364103317, Final Batch Loss: 0.241712287068367\n",
      "Subject 3, Epoch 409, Loss: 0.8922662734985352, Final Batch Loss: 0.3424847722053528\n",
      "Subject 3, Epoch 410, Loss: 1.2620210349559784, Final Batch Loss: 0.38471344113349915\n",
      "Subject 3, Epoch 411, Loss: 0.9585487842559814, Final Batch Loss: 0.2959316074848175\n",
      "Subject 3, Epoch 412, Loss: 0.8906167894601822, Final Batch Loss: 0.3465259075164795\n",
      "Subject 3, Epoch 413, Loss: 0.9437227845191956, Final Batch Loss: 0.31063923239707947\n",
      "Subject 3, Epoch 414, Loss: 0.9717205762863159, Final Batch Loss: 0.27529019117355347\n",
      "Subject 3, Epoch 415, Loss: 1.0071823298931122, Final Batch Loss: 0.4238728880882263\n",
      "Subject 3, Epoch 416, Loss: 0.9717747569084167, Final Batch Loss: 0.31100401282310486\n",
      "Subject 3, Epoch 417, Loss: 0.9227186739444733, Final Batch Loss: 0.3063168227672577\n",
      "Subject 3, Epoch 418, Loss: 1.00178462266922, Final Batch Loss: 0.3499549627304077\n",
      "Subject 3, Epoch 419, Loss: 0.8930488228797913, Final Batch Loss: 0.30381307005882263\n",
      "Subject 3, Epoch 420, Loss: 1.0478785037994385, Final Batch Loss: 0.38757890462875366\n",
      "Subject 3, Epoch 421, Loss: 0.9401355981826782, Final Batch Loss: 0.28473517298698425\n",
      "Subject 3, Epoch 422, Loss: 1.0915039479732513, Final Batch Loss: 0.3198166489601135\n",
      "Subject 3, Epoch 423, Loss: 1.08039191365242, Final Batch Loss: 0.4065639078617096\n",
      "Subject 3, Epoch 424, Loss: 1.0306796729564667, Final Batch Loss: 0.38516175746917725\n",
      "Subject 3, Epoch 425, Loss: 0.9951164126396179, Final Batch Loss: 0.3123798668384552\n",
      "Subject 3, Epoch 426, Loss: 1.0573136508464813, Final Batch Loss: 0.2988675832748413\n",
      "Subject 3, Epoch 427, Loss: 0.9475424885749817, Final Batch Loss: 0.3088846802711487\n",
      "Subject 3, Epoch 428, Loss: 1.0107566714286804, Final Batch Loss: 0.3597278296947479\n",
      "Subject 3, Epoch 429, Loss: 0.9863240122795105, Final Batch Loss: 0.38246574997901917\n",
      "Subject 3, Epoch 430, Loss: 0.9322396516799927, Final Batch Loss: 0.33917737007141113\n",
      "Subject 3, Epoch 431, Loss: 1.0494036972522736, Final Batch Loss: 0.3542502522468567\n",
      "Subject 3, Epoch 432, Loss: 0.9897789657115936, Final Batch Loss: 0.2755573093891144\n",
      "Subject 3, Epoch 433, Loss: 1.0071938633918762, Final Batch Loss: 0.3492252826690674\n",
      "Subject 3, Epoch 434, Loss: 0.9999293684959412, Final Batch Loss: 0.3348686993122101\n",
      "Subject 3, Epoch 435, Loss: 0.9055330157279968, Final Batch Loss: 0.30941450595855713\n",
      "Subject 3, Epoch 436, Loss: 0.9548962712287903, Final Batch Loss: 0.29400163888931274\n",
      "Subject 3, Epoch 437, Loss: 0.862153634428978, Final Batch Loss: 0.22246937453746796\n",
      "Subject 3, Epoch 438, Loss: 0.9575378000736237, Final Batch Loss: 0.40157708525657654\n",
      "Subject 3, Epoch 439, Loss: 0.9534826576709747, Final Batch Loss: 0.3708878755569458\n",
      "Subject 3, Epoch 440, Loss: 1.006697177886963, Final Batch Loss: 0.3506108224391937\n",
      "Subject 3, Epoch 441, Loss: 0.9421577155590057, Final Batch Loss: 0.25924670696258545\n",
      "Subject 3, Epoch 442, Loss: 1.0125500559806824, Final Batch Loss: 0.3055630028247833\n",
      "Subject 3, Epoch 443, Loss: 0.9549322128295898, Final Batch Loss: 0.3483221232891083\n",
      "Subject 3, Epoch 444, Loss: 0.863216757774353, Final Batch Loss: 0.23896634578704834\n",
      "Subject 3, Epoch 445, Loss: 0.954609751701355, Final Batch Loss: 0.288628488779068\n",
      "Subject 3, Epoch 446, Loss: 1.018241822719574, Final Batch Loss: 0.26096901297569275\n",
      "Subject 3, Epoch 447, Loss: 0.9826983213424683, Final Batch Loss: 0.3665699064731598\n",
      "Subject 3, Epoch 448, Loss: 0.9111550450325012, Final Batch Loss: 0.3705812990665436\n",
      "Subject 3, Epoch 449, Loss: 0.8635039925575256, Final Batch Loss: 0.27906522154808044\n",
      "Subject 3, Epoch 450, Loss: 1.0134283304214478, Final Batch Loss: 0.3842621445655823\n",
      "Subject 3, Epoch 451, Loss: 0.9109477400779724, Final Batch Loss: 0.262037992477417\n",
      "Subject 3, Epoch 452, Loss: 1.1123242378234863, Final Batch Loss: 0.3461053967475891\n",
      "Subject 3, Epoch 453, Loss: 0.9179951548576355, Final Batch Loss: 0.28391778469085693\n",
      "Subject 3, Epoch 454, Loss: 0.973749190568924, Final Batch Loss: 0.32399091124534607\n",
      "Subject 3, Epoch 455, Loss: 1.0526649951934814, Final Batch Loss: 0.4051065146923065\n",
      "Subject 3, Epoch 456, Loss: 0.9842319935560226, Final Batch Loss: 0.4092104732990265\n",
      "Subject 3, Epoch 457, Loss: 0.9260197728872299, Final Batch Loss: 0.3232335150241852\n",
      "Subject 3, Epoch 458, Loss: 0.9415512084960938, Final Batch Loss: 0.32883137464523315\n",
      "Subject 3, Epoch 459, Loss: 0.9991892576217651, Final Batch Loss: 0.39509743452072144\n",
      "Subject 3, Epoch 460, Loss: 0.8929933905601501, Final Batch Loss: 0.2666330933570862\n",
      "Subject 3, Epoch 461, Loss: 0.933064877986908, Final Batch Loss: 0.2888033390045166\n",
      "Subject 3, Epoch 462, Loss: 0.9512423872947693, Final Batch Loss: 0.33115634322166443\n",
      "Subject 3, Epoch 463, Loss: 0.9950182735919952, Final Batch Loss: 0.28287991881370544\n",
      "Subject 3, Epoch 464, Loss: 0.9738890826702118, Final Batch Loss: 0.29029732942581177\n",
      "Subject 3, Epoch 465, Loss: 0.952680230140686, Final Batch Loss: 0.34971851110458374\n",
      "Subject 3, Epoch 466, Loss: 0.9111103117465973, Final Batch Loss: 0.3351033926010132\n",
      "Subject 3, Epoch 467, Loss: 0.8868569135665894, Final Batch Loss: 0.31140264868736267\n",
      "Subject 3, Epoch 468, Loss: 0.9328374862670898, Final Batch Loss: 0.31241968274116516\n",
      "Subject 3, Epoch 469, Loss: 0.9921154975891113, Final Batch Loss: 0.3745455741882324\n",
      "Subject 3, Epoch 470, Loss: 0.8351215720176697, Final Batch Loss: 0.30097079277038574\n",
      "Subject 3, Epoch 471, Loss: 0.8617442548274994, Final Batch Loss: 0.26742854714393616\n",
      "Subject 3, Epoch 472, Loss: 1.056412696838379, Final Batch Loss: 0.3664231598377228\n",
      "Subject 3, Epoch 473, Loss: 0.9459985196590424, Final Batch Loss: 0.25781601667404175\n",
      "Subject 3, Epoch 474, Loss: 0.9613862037658691, Final Batch Loss: 0.2594262957572937\n",
      "Subject 3, Epoch 475, Loss: 1.0651689171791077, Final Batch Loss: 0.3269352316856384\n",
      "Subject 3, Epoch 476, Loss: 0.9462316632270813, Final Batch Loss: 0.3434722423553467\n",
      "Subject 3, Epoch 477, Loss: 0.9128747433423996, Final Batch Loss: 0.32949844002723694\n",
      "Subject 3, Epoch 478, Loss: 0.8836666345596313, Final Batch Loss: 0.2654426693916321\n",
      "Subject 3, Epoch 479, Loss: 0.9662863314151764, Final Batch Loss: 0.2937089502811432\n",
      "Subject 3, Epoch 480, Loss: 0.9278407841920853, Final Batch Loss: 0.30186405777931213\n",
      "Subject 3, Epoch 481, Loss: 1.0533382892608643, Final Batch Loss: 0.34963202476501465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 482, Loss: 0.9095605611801147, Final Batch Loss: 0.29413992166519165\n",
      "Subject 3, Epoch 483, Loss: 0.9661943912506104, Final Batch Loss: 0.2619296610355377\n",
      "Subject 3, Epoch 484, Loss: 1.0169189870357513, Final Batch Loss: 0.32028329372406006\n",
      "Subject 3, Epoch 485, Loss: 0.9052046537399292, Final Batch Loss: 0.3270466923713684\n",
      "Subject 3, Epoch 486, Loss: 1.0130046010017395, Final Batch Loss: 0.3086850941181183\n",
      "Subject 3, Epoch 487, Loss: 0.8721658289432526, Final Batch Loss: 0.2261749804019928\n",
      "Subject 3, Epoch 488, Loss: 0.8701391071081161, Final Batch Loss: 0.2946552336215973\n",
      "Subject 3, Epoch 489, Loss: 0.897851437330246, Final Batch Loss: 0.2690086364746094\n",
      "Subject 3, Epoch 490, Loss: 0.8828973472118378, Final Batch Loss: 0.32147207856178284\n",
      "Subject 3, Epoch 491, Loss: 0.8490815460681915, Final Batch Loss: 0.23572489619255066\n",
      "Subject 3, Epoch 492, Loss: 0.9408767819404602, Final Batch Loss: 0.3585117757320404\n",
      "Subject 3, Epoch 493, Loss: 0.8886628448963165, Final Batch Loss: 0.27910032868385315\n",
      "Subject 3, Epoch 494, Loss: 0.8362541347742081, Final Batch Loss: 0.32322490215301514\n",
      "Subject 3, Epoch 495, Loss: 0.8922719359397888, Final Batch Loss: 0.3308731019496918\n",
      "Subject 3, Epoch 496, Loss: 0.8988201767206192, Final Batch Loss: 0.30445560812950134\n",
      "Subject 3, Epoch 497, Loss: 0.8885053992271423, Final Batch Loss: 0.2084904909133911\n",
      "Subject 3, Epoch 498, Loss: 0.8667320311069489, Final Batch Loss: 0.29257991909980774\n",
      "Subject 3, Epoch 499, Loss: 0.9026361405849457, Final Batch Loss: 0.26366114616394043\n",
      "Subject 3, Epoch 500, Loss: 0.893646627664566, Final Batch Loss: 0.2672652304172516\n",
      "Subject 3, Epoch 501, Loss: 0.9211967289447784, Final Batch Loss: 0.31017419695854187\n",
      "Subject 3, Epoch 502, Loss: 0.9833835065364838, Final Batch Loss: 0.3581937849521637\n",
      "Subject 3, Epoch 503, Loss: 0.9198577404022217, Final Batch Loss: 0.3183818757534027\n",
      "Subject 3, Epoch 504, Loss: 0.8207366168498993, Final Batch Loss: 0.27544042468070984\n",
      "Subject 3, Epoch 505, Loss: 0.8430527746677399, Final Batch Loss: 0.31812959909439087\n",
      "Subject 3, Epoch 506, Loss: 0.7538803815841675, Final Batch Loss: 0.2356952428817749\n",
      "Subject 3, Epoch 507, Loss: 0.9046170264482498, Final Batch Loss: 0.22022609412670135\n",
      "Subject 3, Epoch 508, Loss: 0.9661642909049988, Final Batch Loss: 0.2783518433570862\n",
      "Subject 3, Epoch 509, Loss: 0.8572769463062286, Final Batch Loss: 0.27464112639427185\n",
      "Subject 3, Epoch 510, Loss: 0.9079285860061646, Final Batch Loss: 0.3628722131252289\n",
      "Subject 3, Epoch 511, Loss: 0.9167012870311737, Final Batch Loss: 0.3789232671260834\n",
      "Subject 3, Epoch 512, Loss: 0.9898447394371033, Final Batch Loss: 0.3446636497974396\n",
      "Subject 3, Epoch 513, Loss: 0.9122382700443268, Final Batch Loss: 0.26123863458633423\n",
      "Subject 3, Epoch 514, Loss: 0.9510664939880371, Final Batch Loss: 0.2805737853050232\n",
      "Subject 3, Epoch 515, Loss: 0.9232717752456665, Final Batch Loss: 0.23258499801158905\n",
      "Subject 3, Epoch 516, Loss: 0.8661876618862152, Final Batch Loss: 0.3180684745311737\n",
      "Subject 3, Epoch 517, Loss: 0.8714842796325684, Final Batch Loss: 0.3381480872631073\n",
      "Subject 3, Epoch 518, Loss: 0.8845786452293396, Final Batch Loss: 0.20880573987960815\n",
      "Subject 3, Epoch 519, Loss: 0.8832466155290604, Final Batch Loss: 0.3039562702178955\n",
      "Subject 3, Epoch 520, Loss: 0.8448746055364609, Final Batch Loss: 0.22618909180164337\n",
      "Subject 3, Epoch 521, Loss: 0.9814132750034332, Final Batch Loss: 0.3706635534763336\n",
      "Subject 3, Epoch 522, Loss: 1.1091579496860504, Final Batch Loss: 0.49063777923583984\n",
      "Subject 3, Epoch 523, Loss: 0.8609568476676941, Final Batch Loss: 0.26107314229011536\n",
      "Subject 3, Epoch 524, Loss: 0.7971288859844208, Final Batch Loss: 0.19619296491146088\n",
      "Subject 3, Epoch 525, Loss: 0.8796170204877853, Final Batch Loss: 0.23560942709445953\n",
      "Subject 3, Epoch 526, Loss: 0.8727962672710419, Final Batch Loss: 0.27509185671806335\n",
      "Subject 3, Epoch 527, Loss: 0.9084819853305817, Final Batch Loss: 0.35964587330818176\n",
      "Subject 3, Epoch 528, Loss: 0.8192508518695831, Final Batch Loss: 0.2855723798274994\n",
      "Subject 3, Epoch 529, Loss: 0.8552303314208984, Final Batch Loss: 0.28690388798713684\n",
      "Subject 3, Epoch 530, Loss: 0.8896186351776123, Final Batch Loss: 0.3154158890247345\n",
      "Subject 3, Epoch 531, Loss: 0.7979490756988525, Final Batch Loss: 0.25900858640670776\n",
      "Subject 3, Epoch 532, Loss: 0.8797541260719299, Final Batch Loss: 0.3087141513824463\n",
      "Subject 3, Epoch 533, Loss: 0.8619945049285889, Final Batch Loss: 0.3249581754207611\n",
      "Subject 3, Epoch 534, Loss: 0.8724394738674164, Final Batch Loss: 0.2603003978729248\n",
      "Subject 3, Epoch 535, Loss: 0.9242754280567169, Final Batch Loss: 0.22764718532562256\n",
      "Subject 3, Epoch 536, Loss: 0.9453115016222, Final Batch Loss: 0.38573458790779114\n",
      "Subject 3, Epoch 537, Loss: 0.8527175486087799, Final Batch Loss: 0.22771340608596802\n",
      "Subject 3, Epoch 538, Loss: 0.8497684001922607, Final Batch Loss: 0.35083192586898804\n",
      "Subject 3, Epoch 539, Loss: 0.9621510803699493, Final Batch Loss: 0.3536771237850189\n",
      "Subject 3, Epoch 540, Loss: 0.8067210912704468, Final Batch Loss: 0.26795294880867004\n",
      "Subject 3, Epoch 541, Loss: 0.9195241928100586, Final Batch Loss: 0.27467578649520874\n",
      "Subject 3, Epoch 542, Loss: 0.9926525354385376, Final Batch Loss: 0.38567426800727844\n",
      "Subject 3, Epoch 543, Loss: 0.8606663048267365, Final Batch Loss: 0.3107226490974426\n",
      "Subject 3, Epoch 544, Loss: 0.7964303344488144, Final Batch Loss: 0.2917720675468445\n",
      "Subject 3, Epoch 545, Loss: 0.8744184970855713, Final Batch Loss: 0.4079906940460205\n",
      "Subject 3, Epoch 546, Loss: 0.9051856398582458, Final Batch Loss: 0.3225894868373871\n",
      "Subject 3, Epoch 547, Loss: 0.9159053862094879, Final Batch Loss: 0.36907362937927246\n",
      "Subject 3, Epoch 548, Loss: 0.8530499637126923, Final Batch Loss: 0.2867640256881714\n",
      "Subject 3, Epoch 549, Loss: 0.9127217829227448, Final Batch Loss: 0.2743922472000122\n",
      "Subject 3, Epoch 550, Loss: 0.9447119235992432, Final Batch Loss: 0.3234722912311554\n",
      "Subject 3, Epoch 551, Loss: 0.8637803196907043, Final Batch Loss: 0.3140996992588043\n",
      "Subject 3, Epoch 552, Loss: 0.8853267133235931, Final Batch Loss: 0.38756391406059265\n",
      "Subject 3, Epoch 553, Loss: 0.914659708738327, Final Batch Loss: 0.4136655330657959\n",
      "Subject 3, Epoch 554, Loss: 0.9141673445701599, Final Batch Loss: 0.3996449112892151\n",
      "Subject 3, Epoch 555, Loss: 0.9730594456195831, Final Batch Loss: 0.2839880585670471\n",
      "Subject 3, Epoch 556, Loss: 0.8777675926685333, Final Batch Loss: 0.2988573908805847\n",
      "Subject 3, Epoch 557, Loss: 0.7877232879400253, Final Batch Loss: 0.2388482540845871\n",
      "Subject 3, Epoch 558, Loss: 0.9216607213020325, Final Batch Loss: 0.2537480592727661\n",
      "Subject 3, Epoch 559, Loss: 0.8294978439807892, Final Batch Loss: 0.2919386029243469\n",
      "Subject 3, Epoch 560, Loss: 0.9161838591098785, Final Batch Loss: 0.3093993067741394\n",
      "Subject 3, Epoch 561, Loss: 0.885504350066185, Final Batch Loss: 0.23560740053653717\n",
      "Subject 3, Epoch 562, Loss: 0.8598909974098206, Final Batch Loss: 0.35456591844558716\n",
      "Subject 3, Epoch 563, Loss: 0.8904400765895844, Final Batch Loss: 0.28502753376960754\n",
      "Subject 3, Epoch 564, Loss: 0.9164992570877075, Final Batch Loss: 0.3486305773258209\n",
      "Subject 3, Epoch 565, Loss: 0.8549690246582031, Final Batch Loss: 0.2685012221336365\n",
      "Subject 3, Epoch 566, Loss: 0.8795383274555206, Final Batch Loss: 0.300678551197052\n",
      "Subject 3, Epoch 567, Loss: 0.8506200760602951, Final Batch Loss: 0.2955717444419861\n",
      "Subject 3, Epoch 568, Loss: 0.8724372386932373, Final Batch Loss: 0.2875760793685913\n",
      "Subject 3, Epoch 569, Loss: 0.8624584674835205, Final Batch Loss: 0.2947917580604553\n",
      "Subject 3, Epoch 570, Loss: 0.8328121900558472, Final Batch Loss: 0.2936585545539856\n",
      "Subject 3, Epoch 571, Loss: 0.8375402987003326, Final Batch Loss: 0.2537773549556732\n",
      "Subject 3, Epoch 572, Loss: 0.7978828400373459, Final Batch Loss: 0.18816827237606049\n",
      "Subject 3, Epoch 573, Loss: 0.823388010263443, Final Batch Loss: 0.2397383451461792\n",
      "Subject 3, Epoch 574, Loss: 0.8321606069803238, Final Batch Loss: 0.28028029203414917\n",
      "Subject 3, Epoch 575, Loss: 0.9026026427745819, Final Batch Loss: 0.26283320784568787\n",
      "Subject 3, Epoch 576, Loss: 0.7920278310775757, Final Batch Loss: 0.24647900462150574\n",
      "Subject 3, Epoch 577, Loss: 0.838245764374733, Final Batch Loss: 0.24120448529720306\n",
      "Subject 3, Epoch 578, Loss: 0.8014228492975235, Final Batch Loss: 0.20459477603435516\n",
      "Subject 3, Epoch 579, Loss: 0.9404346942901611, Final Batch Loss: 0.3458978533744812\n",
      "Subject 3, Epoch 580, Loss: 0.9092033505439758, Final Batch Loss: 0.38630640506744385\n",
      "Subject 3, Epoch 581, Loss: 0.8289696574211121, Final Batch Loss: 0.23802220821380615\n",
      "Subject 3, Epoch 582, Loss: 0.7778638154268265, Final Batch Loss: 0.2802237868309021\n",
      "Subject 3, Epoch 583, Loss: 0.8358822464942932, Final Batch Loss: 0.30093756318092346\n",
      "Subject 3, Epoch 584, Loss: 0.8094380050897598, Final Batch Loss: 0.31918302178382874\n",
      "Subject 3, Epoch 585, Loss: 0.8267047554254532, Final Batch Loss: 0.2725168764591217\n",
      "Subject 3, Epoch 586, Loss: 0.8324645012617111, Final Batch Loss: 0.23764793574810028\n",
      "Subject 3, Epoch 587, Loss: 0.8540557473897934, Final Batch Loss: 0.29018762707710266\n",
      "Subject 3, Epoch 588, Loss: 0.8241188228130341, Final Batch Loss: 0.21210673451423645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 589, Loss: 0.9667485952377319, Final Batch Loss: 0.4316685199737549\n",
      "Subject 3, Epoch 590, Loss: 0.9309553354978561, Final Batch Loss: 0.33757415413856506\n",
      "Subject 3, Epoch 591, Loss: 0.907090038061142, Final Batch Loss: 0.3153151571750641\n",
      "Subject 3, Epoch 592, Loss: 0.8722290098667145, Final Batch Loss: 0.2693101763725281\n",
      "Subject 3, Epoch 593, Loss: 0.9719720780849457, Final Batch Loss: 0.43081429600715637\n",
      "Subject 3, Epoch 594, Loss: 0.7384834587574005, Final Batch Loss: 0.20344409346580505\n",
      "Subject 3, Epoch 595, Loss: 0.8464298397302628, Final Batch Loss: 0.31045082211494446\n",
      "Subject 3, Epoch 596, Loss: 0.765065610408783, Final Batch Loss: 0.24091863632202148\n",
      "Subject 3, Epoch 597, Loss: 0.7815319150686264, Final Batch Loss: 0.21620501577854156\n",
      "Subject 3, Epoch 598, Loss: 0.7512827515602112, Final Batch Loss: 0.151644766330719\n",
      "Subject 3, Epoch 599, Loss: 0.863713413476944, Final Batch Loss: 0.2271113395690918\n",
      "Subject 3, Epoch 600, Loss: 0.7803703546524048, Final Batch Loss: 0.1808418333530426\n",
      "Subject 3, Epoch 601, Loss: 0.7840176224708557, Final Batch Loss: 0.26501309871673584\n",
      "Subject 3, Epoch 602, Loss: 0.7799826413393021, Final Batch Loss: 0.2156512588262558\n",
      "Subject 3, Epoch 603, Loss: 0.806269109249115, Final Batch Loss: 0.2663474380970001\n",
      "Subject 3, Epoch 604, Loss: 0.8048030734062195, Final Batch Loss: 0.2187006026506424\n",
      "Subject 3, Epoch 605, Loss: 0.8086500614881516, Final Batch Loss: 0.350153386592865\n",
      "Subject 3, Epoch 606, Loss: 0.8201403766870499, Final Batch Loss: 0.2093278020620346\n",
      "Subject 3, Epoch 607, Loss: 0.8310803771018982, Final Batch Loss: 0.33849620819091797\n",
      "Subject 3, Epoch 608, Loss: 0.804567277431488, Final Batch Loss: 0.17177993059158325\n",
      "Subject 3, Epoch 609, Loss: 0.8866084069013596, Final Batch Loss: 0.3117852509021759\n",
      "Subject 3, Epoch 610, Loss: 0.7860240936279297, Final Batch Loss: 0.20676502585411072\n",
      "Subject 3, Epoch 611, Loss: 0.7833282053470612, Final Batch Loss: 0.2158004641532898\n",
      "Subject 3, Epoch 612, Loss: 0.7359985858201981, Final Batch Loss: 0.22636909782886505\n",
      "Subject 3, Epoch 613, Loss: 0.8020806312561035, Final Batch Loss: 0.2930965721607208\n",
      "Subject 3, Epoch 614, Loss: 0.841855376958847, Final Batch Loss: 0.24675148725509644\n",
      "Subject 3, Epoch 615, Loss: 0.8811861872673035, Final Batch Loss: 0.283800333738327\n",
      "Subject 3, Epoch 616, Loss: 0.7461560368537903, Final Batch Loss: 0.18376761674880981\n",
      "Subject 3, Epoch 617, Loss: 0.8200419545173645, Final Batch Loss: 0.3088890612125397\n",
      "Subject 3, Epoch 618, Loss: 0.8541172742843628, Final Batch Loss: 0.27059292793273926\n",
      "Subject 3, Epoch 619, Loss: 0.9018089175224304, Final Batch Loss: 0.31068962812423706\n",
      "Subject 3, Epoch 620, Loss: 0.8951351046562195, Final Batch Loss: 0.40542104840278625\n",
      "Subject 3, Epoch 621, Loss: 0.7732124924659729, Final Batch Loss: 0.2854732871055603\n",
      "Subject 3, Epoch 622, Loss: 0.7807786166667938, Final Batch Loss: 0.27070656418800354\n",
      "Subject 3, Epoch 623, Loss: 0.8543109595775604, Final Batch Loss: 0.2683243453502655\n",
      "Subject 3, Epoch 624, Loss: 0.7564876824617386, Final Batch Loss: 0.22134755551815033\n",
      "Subject 3, Epoch 625, Loss: 0.8401410728693008, Final Batch Loss: 0.3176887333393097\n",
      "Subject 3, Epoch 626, Loss: 0.8162889331579208, Final Batch Loss: 0.29798364639282227\n",
      "Subject 3, Epoch 627, Loss: 0.7705918103456497, Final Batch Loss: 0.3093656301498413\n",
      "Subject 3, Epoch 628, Loss: 0.780805766582489, Final Batch Loss: 0.23138456046581268\n",
      "Subject 3, Epoch 629, Loss: 0.7709230780601501, Final Batch Loss: 0.2716875672340393\n",
      "Subject 3, Epoch 630, Loss: 0.849802240729332, Final Batch Loss: 0.33027100563049316\n",
      "Subject 3, Epoch 631, Loss: 0.771437793970108, Final Batch Loss: 0.33161988854408264\n",
      "Subject 3, Epoch 632, Loss: 0.8353194147348404, Final Batch Loss: 0.3099232614040375\n",
      "Subject 3, Epoch 633, Loss: 0.8274986445903778, Final Batch Loss: 0.2549833655357361\n",
      "Subject 3, Epoch 634, Loss: 0.7814392745494843, Final Batch Loss: 0.30398380756378174\n",
      "Subject 3, Epoch 635, Loss: 0.7764740437269211, Final Batch Loss: 0.2912117540836334\n",
      "Subject 3, Epoch 636, Loss: 0.77419613301754, Final Batch Loss: 0.30826717615127563\n",
      "Subject 3, Epoch 637, Loss: 0.7481668442487717, Final Batch Loss: 0.208159402012825\n",
      "Subject 3, Epoch 638, Loss: 0.7553237527608871, Final Batch Loss: 0.22738030552864075\n",
      "Subject 3, Epoch 639, Loss: 0.7748041152954102, Final Batch Loss: 0.23442551493644714\n",
      "Subject 3, Epoch 640, Loss: 0.8410073667764664, Final Batch Loss: 0.21985571086406708\n",
      "Subject 3, Epoch 641, Loss: 0.856857031583786, Final Batch Loss: 0.27322304248809814\n",
      "Subject 3, Epoch 642, Loss: 0.7327950447797775, Final Batch Loss: 0.27693501114845276\n",
      "Subject 3, Epoch 643, Loss: 0.7310405224561691, Final Batch Loss: 0.21613389253616333\n",
      "Subject 3, Epoch 644, Loss: 0.797850713133812, Final Batch Loss: 0.30012455582618713\n",
      "Subject 3, Epoch 645, Loss: 0.8665405958890915, Final Batch Loss: 0.2825580835342407\n",
      "Subject 3, Epoch 646, Loss: 0.9199328720569611, Final Batch Loss: 0.30606961250305176\n",
      "Subject 3, Epoch 647, Loss: 0.8042920529842377, Final Batch Loss: 0.22836270928382874\n",
      "Subject 3, Epoch 648, Loss: 0.7206691652536392, Final Batch Loss: 0.23131154477596283\n",
      "Subject 3, Epoch 649, Loss: 0.8384621441364288, Final Batch Loss: 0.31621092557907104\n",
      "Subject 3, Epoch 650, Loss: 0.8135447204113007, Final Batch Loss: 0.26345667243003845\n",
      "Subject 3, Epoch 651, Loss: 0.7901366800069809, Final Batch Loss: 0.255776971578598\n",
      "Subject 3, Epoch 652, Loss: 0.7341035604476929, Final Batch Loss: 0.20657050609588623\n",
      "Subject 3, Epoch 653, Loss: 0.6882553100585938, Final Batch Loss: 0.24090375006198883\n",
      "Subject 3, Epoch 654, Loss: 0.725787341594696, Final Batch Loss: 0.21789129078388214\n",
      "Subject 3, Epoch 655, Loss: 0.7554658204317093, Final Batch Loss: 0.2590368390083313\n",
      "Subject 3, Epoch 656, Loss: 0.8619662672281265, Final Batch Loss: 0.2717105448246002\n",
      "Subject 3, Epoch 657, Loss: 0.7374074310064316, Final Batch Loss: 0.2872479259967804\n",
      "Subject 3, Epoch 658, Loss: 0.7921278327703476, Final Batch Loss: 0.17373628914356232\n",
      "Subject 3, Epoch 659, Loss: 0.741064578294754, Final Batch Loss: 0.2037879228591919\n",
      "Subject 3, Epoch 660, Loss: 0.7124939262866974, Final Batch Loss: 0.2646957337856293\n",
      "Subject 3, Epoch 661, Loss: 0.7229211330413818, Final Batch Loss: 0.24062493443489075\n",
      "Subject 3, Epoch 662, Loss: 0.7581660747528076, Final Batch Loss: 0.22802317142486572\n",
      "Subject 3, Epoch 663, Loss: 0.8780872821807861, Final Batch Loss: 0.3990013599395752\n",
      "Subject 3, Epoch 664, Loss: 0.8118167072534561, Final Batch Loss: 0.3128354847431183\n",
      "Subject 3, Epoch 665, Loss: 0.9277085810899734, Final Batch Loss: 0.24104727804660797\n",
      "Subject 3, Epoch 666, Loss: 0.8042282462120056, Final Batch Loss: 0.3540448546409607\n",
      "Subject 3, Epoch 667, Loss: 0.8171007484197617, Final Batch Loss: 0.19391165673732758\n",
      "Subject 3, Epoch 668, Loss: 0.8342368751764297, Final Batch Loss: 0.3110993504524231\n",
      "Subject 3, Epoch 669, Loss: 0.8358838260173798, Final Batch Loss: 0.30361276865005493\n",
      "Subject 3, Epoch 670, Loss: 0.7915447354316711, Final Batch Loss: 0.2794182300567627\n",
      "Subject 3, Epoch 671, Loss: 0.8089723140001297, Final Batch Loss: 0.29320234060287476\n",
      "Subject 3, Epoch 672, Loss: 0.7620409280061722, Final Batch Loss: 0.19119377434253693\n",
      "Subject 3, Epoch 673, Loss: 0.9322270005941391, Final Batch Loss: 0.3915995657444\n",
      "Subject 3, Epoch 674, Loss: 0.8670321702957153, Final Batch Loss: 0.25649720430374146\n",
      "Subject 3, Epoch 675, Loss: 0.8243316411972046, Final Batch Loss: 0.32150986790657043\n",
      "Subject 3, Epoch 676, Loss: 0.6810175329446793, Final Batch Loss: 0.17027275264263153\n",
      "Subject 3, Epoch 677, Loss: 0.7161354422569275, Final Batch Loss: 0.1882915496826172\n",
      "Subject 3, Epoch 678, Loss: 0.7219278216362, Final Batch Loss: 0.1688624620437622\n",
      "Subject 3, Epoch 679, Loss: 0.8185792565345764, Final Batch Loss: 0.2551693916320801\n",
      "Subject 3, Epoch 680, Loss: 0.7876918911933899, Final Batch Loss: 0.3167453110218048\n",
      "Subject 3, Epoch 681, Loss: 0.7618556916713715, Final Batch Loss: 0.293142706155777\n",
      "Subject 3, Epoch 682, Loss: 0.7406254410743713, Final Batch Loss: 0.1866561770439148\n",
      "Subject 3, Epoch 683, Loss: 0.7995440065860748, Final Batch Loss: 0.26851534843444824\n",
      "Subject 3, Epoch 684, Loss: 0.8092387318611145, Final Batch Loss: 0.2754318416118622\n",
      "Subject 3, Epoch 685, Loss: 0.8383602797985077, Final Batch Loss: 0.279072105884552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 686, Loss: 0.6862950176000595, Final Batch Loss: 0.27421948313713074\n",
      "Subject 3, Epoch 687, Loss: 0.7839961498975754, Final Batch Loss: 0.3150433897972107\n",
      "Subject 3, Epoch 688, Loss: 0.7957420945167542, Final Batch Loss: 0.32028383016586304\n",
      "Subject 3, Epoch 689, Loss: 0.7299117743968964, Final Batch Loss: 0.2567331790924072\n",
      "Subject 3, Epoch 690, Loss: 0.8075694739818573, Final Batch Loss: 0.29525482654571533\n",
      "Subject 3, Epoch 691, Loss: 0.8296858966350555, Final Batch Loss: 0.369397908449173\n",
      "Subject 3, Epoch 692, Loss: 0.6639628410339355, Final Batch Loss: 0.15057949721813202\n",
      "Subject 3, Epoch 693, Loss: 0.7547554671764374, Final Batch Loss: 0.35243862867355347\n",
      "Subject 3, Epoch 694, Loss: 0.7556331604719162, Final Batch Loss: 0.21107926964759827\n",
      "Subject 3, Epoch 695, Loss: 0.8281889706850052, Final Batch Loss: 0.3017753064632416\n",
      "Subject 3, Epoch 696, Loss: 0.6911051869392395, Final Batch Loss: 0.258044570684433\n",
      "Subject 3, Epoch 697, Loss: 0.8269486874341965, Final Batch Loss: 0.26398414373397827\n",
      "Subject 3, Epoch 698, Loss: 0.7433667778968811, Final Batch Loss: 0.22155475616455078\n",
      "Subject 3, Epoch 699, Loss: 0.7619637250900269, Final Batch Loss: 0.22824475169181824\n",
      "Subject 3, Epoch 700, Loss: 0.7490836828947067, Final Batch Loss: 0.25182780623435974\n",
      "Subject 3, Epoch 701, Loss: 0.689066469669342, Final Batch Loss: 0.19553714990615845\n",
      "Subject 3, Epoch 702, Loss: 0.6991229504346848, Final Batch Loss: 0.20265892148017883\n",
      "Subject 3, Epoch 703, Loss: 0.7872554957866669, Final Batch Loss: 0.2649539113044739\n",
      "Subject 3, Epoch 704, Loss: 0.762433722615242, Final Batch Loss: 0.2772495746612549\n",
      "Subject 3, Epoch 705, Loss: 0.8488881886005402, Final Batch Loss: 0.2877231538295746\n",
      "Subject 3, Epoch 706, Loss: 0.649157926440239, Final Batch Loss: 0.165694922208786\n",
      "Subject 3, Epoch 707, Loss: 0.7381737530231476, Final Batch Loss: 0.22263288497924805\n",
      "Subject 3, Epoch 708, Loss: 0.7404088079929352, Final Batch Loss: 0.16743004322052002\n",
      "Subject 3, Epoch 709, Loss: 0.7396466434001923, Final Batch Loss: 0.23616141080856323\n",
      "Subject 3, Epoch 710, Loss: 0.7659784108400345, Final Batch Loss: 0.20267637073993683\n",
      "Subject 3, Epoch 711, Loss: 0.6496683359146118, Final Batch Loss: 0.15555439889431\n",
      "Subject 3, Epoch 712, Loss: 0.843289852142334, Final Batch Loss: 0.2287023961544037\n",
      "Subject 3, Epoch 713, Loss: 0.7573848068714142, Final Batch Loss: 0.2512625753879547\n",
      "Subject 3, Epoch 714, Loss: 0.7822559028863907, Final Batch Loss: 0.27386313676834106\n",
      "Subject 3, Epoch 715, Loss: 0.6785661578178406, Final Batch Loss: 0.25499337911605835\n",
      "Subject 3, Epoch 716, Loss: 0.7737915217876434, Final Batch Loss: 0.2737559378147125\n",
      "Subject 3, Epoch 717, Loss: 0.7296493202447891, Final Batch Loss: 0.23767885565757751\n",
      "Subject 3, Epoch 718, Loss: 0.7137046903371811, Final Batch Loss: 0.31567633152008057\n",
      "Subject 3, Epoch 719, Loss: 0.8579176664352417, Final Batch Loss: 0.28286170959472656\n",
      "Subject 3, Epoch 720, Loss: 0.7859931290149689, Final Batch Loss: 0.25793910026550293\n",
      "Subject 3, Epoch 721, Loss: 0.6927506476640701, Final Batch Loss: 0.17961730062961578\n",
      "Subject 3, Epoch 722, Loss: 0.7156440913677216, Final Batch Loss: 0.22348225116729736\n",
      "Subject 3, Epoch 723, Loss: 0.709450826048851, Final Batch Loss: 0.3131614625453949\n",
      "Subject 3, Epoch 724, Loss: 0.6701035350561142, Final Batch Loss: 0.22080785036087036\n",
      "Subject 3, Epoch 725, Loss: 0.7337173521518707, Final Batch Loss: 0.29109707474708557\n",
      "Subject 3, Epoch 726, Loss: 0.791963204741478, Final Batch Loss: 0.3028373718261719\n",
      "Subject 3, Epoch 727, Loss: 0.7712155729532242, Final Batch Loss: 0.24349209666252136\n",
      "Subject 3, Epoch 728, Loss: 0.7010803073644638, Final Batch Loss: 0.24763573706150055\n",
      "Subject 3, Epoch 729, Loss: 0.7668674737215042, Final Batch Loss: 0.2627328336238861\n",
      "Subject 3, Epoch 730, Loss: 0.722515881061554, Final Batch Loss: 0.29978859424591064\n",
      "Subject 3, Epoch 731, Loss: 0.7157418131828308, Final Batch Loss: 0.23432523012161255\n",
      "Subject 3, Epoch 732, Loss: 0.7562054544687271, Final Batch Loss: 0.24929647147655487\n",
      "Subject 3, Epoch 733, Loss: 0.7820002436637878, Final Batch Loss: 0.2591060996055603\n",
      "Subject 3, Epoch 734, Loss: 0.6990379244089127, Final Batch Loss: 0.24239899218082428\n",
      "Subject 3, Epoch 735, Loss: 0.7481605559587479, Final Batch Loss: 0.260299414396286\n",
      "Subject 3, Epoch 736, Loss: 0.6887239366769791, Final Batch Loss: 0.2402564287185669\n",
      "Subject 3, Epoch 737, Loss: 0.7402182072401047, Final Batch Loss: 0.17787404358386993\n",
      "Subject 3, Epoch 738, Loss: 0.704848974943161, Final Batch Loss: 0.34235265851020813\n",
      "Subject 3, Epoch 739, Loss: 0.7287147641181946, Final Batch Loss: 0.21263132989406586\n",
      "Subject 3, Epoch 740, Loss: 0.7638327181339264, Final Batch Loss: 0.22316014766693115\n",
      "Subject 3, Epoch 741, Loss: 0.663220003247261, Final Batch Loss: 0.29081544280052185\n",
      "Subject 3, Epoch 742, Loss: 0.7109076529741287, Final Batch Loss: 0.29341667890548706\n",
      "Subject 3, Epoch 743, Loss: 0.7032703459262848, Final Batch Loss: 0.26156285405158997\n",
      "Subject 3, Epoch 744, Loss: 0.7474519610404968, Final Batch Loss: 0.28234726190567017\n",
      "Subject 3, Epoch 745, Loss: 0.6400827318429947, Final Batch Loss: 0.20697610080242157\n",
      "Subject 3, Epoch 746, Loss: 0.6585167348384857, Final Batch Loss: 0.15283973515033722\n",
      "Subject 3, Epoch 747, Loss: 0.8580398857593536, Final Batch Loss: 0.37821537256240845\n",
      "Subject 3, Epoch 748, Loss: 0.683519184589386, Final Batch Loss: 0.18558910489082336\n",
      "Subject 3, Epoch 749, Loss: 0.7479513883590698, Final Batch Loss: 0.2532353103160858\n",
      "Subject 3, Epoch 750, Loss: 0.7140579968690872, Final Batch Loss: 0.23288342356681824\n",
      "Subject 3, Epoch 751, Loss: 0.7176316976547241, Final Batch Loss: 0.31940996646881104\n",
      "Subject 3, Epoch 752, Loss: 0.8122734129428864, Final Batch Loss: 0.17270559072494507\n",
      "Subject 3, Epoch 753, Loss: 0.7972860485315323, Final Batch Loss: 0.29920676350593567\n",
      "Subject 3, Epoch 754, Loss: 0.7186634540557861, Final Batch Loss: 0.17348477244377136\n",
      "Subject 3, Epoch 755, Loss: 0.6999263018369675, Final Batch Loss: 0.2463100105524063\n",
      "Subject 3, Epoch 756, Loss: 0.7153143584728241, Final Batch Loss: 0.21729348599910736\n",
      "Subject 3, Epoch 757, Loss: 0.6351217478513718, Final Batch Loss: 0.19046370685100555\n",
      "Subject 3, Epoch 758, Loss: 0.6486130505800247, Final Batch Loss: 0.2612394094467163\n",
      "Subject 3, Epoch 759, Loss: 0.7166544497013092, Final Batch Loss: 0.19387833774089813\n",
      "Subject 3, Epoch 760, Loss: 0.6766325384378433, Final Batch Loss: 0.25844597816467285\n",
      "Subject 3, Epoch 761, Loss: 0.6893985867500305, Final Batch Loss: 0.2671240270137787\n",
      "Subject 3, Epoch 762, Loss: 0.6514554023742676, Final Batch Loss: 0.18575933575630188\n",
      "Subject 3, Epoch 763, Loss: 0.7428209185600281, Final Batch Loss: 0.23372645676136017\n",
      "Subject 3, Epoch 764, Loss: 0.7367219924926758, Final Batch Loss: 0.2868041396141052\n",
      "Subject 3, Epoch 765, Loss: 0.624036118388176, Final Batch Loss: 0.2357538640499115\n",
      "Subject 3, Epoch 766, Loss: 0.6540666371583939, Final Batch Loss: 0.2524181306362152\n",
      "Subject 3, Epoch 767, Loss: 0.694768026471138, Final Batch Loss: 0.24560877680778503\n",
      "Subject 3, Epoch 768, Loss: 0.6753111779689789, Final Batch Loss: 0.2680548131465912\n",
      "Subject 3, Epoch 769, Loss: 0.6877176612615585, Final Batch Loss: 0.1846827119588852\n",
      "Subject 3, Epoch 770, Loss: 0.7221869677305222, Final Batch Loss: 0.22916199266910553\n",
      "Subject 3, Epoch 771, Loss: 0.6210029125213623, Final Batch Loss: 0.2520834505558014\n",
      "Subject 3, Epoch 772, Loss: 0.6544617116451263, Final Batch Loss: 0.2505146265029907\n",
      "Subject 3, Epoch 773, Loss: 0.7227406203746796, Final Batch Loss: 0.19058425724506378\n",
      "Subject 3, Epoch 774, Loss: 0.6928310543298721, Final Batch Loss: 0.2574724853038788\n",
      "Subject 3, Epoch 775, Loss: 0.6859862506389618, Final Batch Loss: 0.2845630943775177\n",
      "Subject 3, Epoch 776, Loss: 0.6605774462223053, Final Batch Loss: 0.2267802357673645\n",
      "Subject 3, Epoch 777, Loss: 0.5941362529993057, Final Batch Loss: 0.23797474801540375\n",
      "Subject 3, Epoch 778, Loss: 0.6946966797113419, Final Batch Loss: 0.18819670379161835\n",
      "Subject 3, Epoch 779, Loss: 0.6760973334312439, Final Batch Loss: 0.2292606234550476\n",
      "Subject 3, Epoch 780, Loss: 0.631680816411972, Final Batch Loss: 0.27180659770965576\n",
      "Subject 3, Epoch 781, Loss: 0.6461206823587418, Final Batch Loss: 0.23886288702487946\n",
      "Subject 3, Epoch 782, Loss: 0.5873506218194962, Final Batch Loss: 0.22472062706947327\n",
      "Subject 3, Epoch 783, Loss: 0.6675226241350174, Final Batch Loss: 0.2170274406671524\n",
      "Subject 3, Epoch 784, Loss: 0.683938667178154, Final Batch Loss: 0.2640661299228668\n",
      "Subject 3, Epoch 785, Loss: 0.6666201502084732, Final Batch Loss: 0.22980019450187683\n",
      "Subject 3, Epoch 786, Loss: 0.6381344944238663, Final Batch Loss: 0.17190690338611603\n",
      "Subject 3, Epoch 787, Loss: 0.7917715311050415, Final Batch Loss: 0.20846085250377655\n",
      "Subject 3, Epoch 788, Loss: 0.6737819164991379, Final Batch Loss: 0.23885345458984375\n",
      "Subject 3, Epoch 789, Loss: 0.6844973266124725, Final Batch Loss: 0.19209285080432892\n",
      "Subject 3, Epoch 790, Loss: 0.5840319246053696, Final Batch Loss: 0.1447121798992157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 791, Loss: 0.7026456892490387, Final Batch Loss: 0.26763907074928284\n",
      "Subject 3, Epoch 792, Loss: 0.6976500600576401, Final Batch Loss: 0.2697893977165222\n",
      "Subject 3, Epoch 793, Loss: 0.6718559414148331, Final Batch Loss: 0.2663808763027191\n",
      "Subject 3, Epoch 794, Loss: 0.6262964308261871, Final Batch Loss: 0.17058657109737396\n",
      "Subject 3, Epoch 795, Loss: 0.5813519209623337, Final Batch Loss: 0.1281581073999405\n",
      "Subject 3, Epoch 796, Loss: 0.7824922055006027, Final Batch Loss: 0.30114489793777466\n",
      "Subject 3, Epoch 797, Loss: 0.6554133296012878, Final Batch Loss: 0.2284814417362213\n",
      "Subject 3, Epoch 798, Loss: 0.6424376517534256, Final Batch Loss: 0.2589412033557892\n",
      "Subject 3, Epoch 799, Loss: 0.7395324558019638, Final Batch Loss: 0.3221648037433624\n",
      "Subject 3, Epoch 800, Loss: 0.8073218166828156, Final Batch Loss: 0.3110339939594269\n",
      "Subject 3, Epoch 801, Loss: 0.5976278781890869, Final Batch Loss: 0.16626687347888947\n",
      "Subject 3, Epoch 802, Loss: 0.6666092872619629, Final Batch Loss: 0.18425320088863373\n",
      "Subject 3, Epoch 803, Loss: 0.6123311519622803, Final Batch Loss: 0.13800765573978424\n",
      "Subject 3, Epoch 804, Loss: 0.6052679121494293, Final Batch Loss: 0.18140949308872223\n",
      "Subject 3, Epoch 805, Loss: 0.6377273350954056, Final Batch Loss: 0.19269324839115143\n",
      "Subject 3, Epoch 806, Loss: 0.6737351417541504, Final Batch Loss: 0.23062369227409363\n",
      "Subject 3, Epoch 807, Loss: 0.597074031829834, Final Batch Loss: 0.14669184386730194\n",
      "Subject 3, Epoch 808, Loss: 0.7492438703775406, Final Batch Loss: 0.2983543276786804\n",
      "Subject 3, Epoch 809, Loss: 0.5532421916723251, Final Batch Loss: 0.14351706206798553\n",
      "Subject 3, Epoch 810, Loss: 0.6291436702013016, Final Batch Loss: 0.20433247089385986\n",
      "Subject 3, Epoch 811, Loss: 0.7302891761064529, Final Batch Loss: 0.25603678822517395\n",
      "Subject 3, Epoch 812, Loss: 0.707897737622261, Final Batch Loss: 0.28698500990867615\n",
      "Subject 3, Epoch 813, Loss: 0.6757665723562241, Final Batch Loss: 0.1805463582277298\n",
      "Subject 3, Epoch 814, Loss: 0.701572373509407, Final Batch Loss: 0.2562619745731354\n",
      "Subject 3, Epoch 815, Loss: 0.5895307660102844, Final Batch Loss: 0.22884178161621094\n",
      "Subject 3, Epoch 816, Loss: 0.7579884082078934, Final Batch Loss: 0.21704696118831635\n",
      "Subject 3, Epoch 817, Loss: 0.6586932390928268, Final Batch Loss: 0.3019069731235504\n",
      "Subject 3, Epoch 818, Loss: 0.7331839203834534, Final Batch Loss: 0.2482743263244629\n",
      "Subject 3, Epoch 819, Loss: 0.6950966417789459, Final Batch Loss: 0.1836477518081665\n",
      "Subject 3, Epoch 820, Loss: 0.5723128318786621, Final Batch Loss: 0.27562031149864197\n",
      "Subject 3, Epoch 821, Loss: 0.641699030995369, Final Batch Loss: 0.260702520608902\n",
      "Subject 3, Epoch 822, Loss: 0.5701801478862762, Final Batch Loss: 0.1890348196029663\n",
      "Subject 3, Epoch 823, Loss: 0.5946002155542374, Final Batch Loss: 0.17294220626354218\n",
      "Subject 3, Epoch 824, Loss: 0.6149911135435104, Final Batch Loss: 0.1866767257452011\n",
      "Subject 3, Epoch 825, Loss: 0.5337985455989838, Final Batch Loss: 0.18131572008132935\n",
      "Subject 3, Epoch 826, Loss: 0.5925820618867874, Final Batch Loss: 0.24844345450401306\n",
      "Subject 3, Epoch 827, Loss: 0.5970265120267868, Final Batch Loss: 0.2628765106201172\n",
      "Subject 3, Epoch 828, Loss: 0.6519663780927658, Final Batch Loss: 0.18100273609161377\n",
      "Subject 3, Epoch 829, Loss: 0.6492558121681213, Final Batch Loss: 0.26994457840919495\n",
      "Subject 3, Epoch 830, Loss: 0.7013144642114639, Final Batch Loss: 0.2521814703941345\n",
      "Subject 3, Epoch 831, Loss: 0.6249194443225861, Final Batch Loss: 0.16833819448947906\n",
      "Subject 3, Epoch 832, Loss: 0.6045093461871147, Final Batch Loss: 0.12014680355787277\n",
      "Subject 3, Epoch 833, Loss: 0.6585159003734589, Final Batch Loss: 0.2745078206062317\n",
      "Subject 3, Epoch 834, Loss: 0.6552407294511795, Final Batch Loss: 0.2663172781467438\n",
      "Subject 3, Epoch 835, Loss: 0.6454970836639404, Final Batch Loss: 0.2578844726085663\n",
      "Subject 3, Epoch 836, Loss: 0.6099216938018799, Final Batch Loss: 0.2176874876022339\n",
      "Subject 3, Epoch 837, Loss: 0.5193357765674591, Final Batch Loss: 0.1498061865568161\n",
      "Subject 3, Epoch 838, Loss: 0.5812223255634308, Final Batch Loss: 0.15602295100688934\n",
      "Subject 3, Epoch 839, Loss: 0.5316622108221054, Final Batch Loss: 0.19117844104766846\n",
      "Subject 3, Epoch 840, Loss: 0.4966792166233063, Final Batch Loss: 0.1789214164018631\n",
      "Subject 3, Epoch 841, Loss: 0.6518971174955368, Final Batch Loss: 0.244584858417511\n",
      "Subject 3, Epoch 842, Loss: 0.6150825768709183, Final Batch Loss: 0.13170920312404633\n",
      "Subject 3, Epoch 843, Loss: 0.732839971780777, Final Batch Loss: 0.2554417550563812\n",
      "Subject 3, Epoch 844, Loss: 0.6665646880865097, Final Batch Loss: 0.18692544102668762\n",
      "Subject 3, Epoch 845, Loss: 0.6551230400800705, Final Batch Loss: 0.21511562168598175\n",
      "Subject 3, Epoch 846, Loss: 0.7317304313182831, Final Batch Loss: 0.31596189737319946\n",
      "Subject 3, Epoch 847, Loss: 0.6815626621246338, Final Batch Loss: 0.24304741621017456\n",
      "Subject 3, Epoch 848, Loss: 0.6778051108121872, Final Batch Loss: 0.20607952773571014\n",
      "Subject 3, Epoch 849, Loss: 0.6620890647172928, Final Batch Loss: 0.2534293234348297\n",
      "Subject 3, Epoch 850, Loss: 0.6014782786369324, Final Batch Loss: 0.1951397955417633\n",
      "Subject 3, Epoch 851, Loss: 0.5788560509681702, Final Batch Loss: 0.15449747443199158\n",
      "Subject 3, Epoch 852, Loss: 0.5706500262022018, Final Batch Loss: 0.15650399029254913\n",
      "Subject 3, Epoch 853, Loss: 0.7122887372970581, Final Batch Loss: 0.2591756582260132\n",
      "Subject 3, Epoch 854, Loss: 0.6342867612838745, Final Batch Loss: 0.22695794701576233\n",
      "Subject 3, Epoch 855, Loss: 0.6099866777658463, Final Batch Loss: 0.24095527827739716\n",
      "Subject 3, Epoch 856, Loss: 0.5640082508325577, Final Batch Loss: 0.2171761393547058\n",
      "Subject 3, Epoch 857, Loss: 0.6075756251811981, Final Batch Loss: 0.24859215319156647\n",
      "Subject 3, Epoch 858, Loss: 0.6467577815055847, Final Batch Loss: 0.16240061819553375\n",
      "Subject 3, Epoch 859, Loss: 0.5894138514995575, Final Batch Loss: 0.13424140214920044\n",
      "Subject 3, Epoch 860, Loss: 0.5851700454950333, Final Batch Loss: 0.13031478226184845\n",
      "Subject 3, Epoch 861, Loss: 0.6314049512147903, Final Batch Loss: 0.2517244815826416\n",
      "Subject 3, Epoch 862, Loss: 0.5828000903129578, Final Batch Loss: 0.219508096575737\n",
      "Subject 3, Epoch 863, Loss: 0.6009277552366257, Final Batch Loss: 0.22106002271175385\n",
      "Subject 3, Epoch 864, Loss: 0.6006092131137848, Final Batch Loss: 0.21302376687526703\n",
      "Subject 3, Epoch 865, Loss: 0.6572060585021973, Final Batch Loss: 0.2758985161781311\n",
      "Subject 3, Epoch 866, Loss: 0.5740453898906708, Final Batch Loss: 0.138255774974823\n",
      "Subject 3, Epoch 867, Loss: 0.5449823886156082, Final Batch Loss: 0.1595466434955597\n",
      "Subject 3, Epoch 868, Loss: 0.6299698352813721, Final Batch Loss: 0.2353224754333496\n",
      "Subject 3, Epoch 869, Loss: 0.7502792775630951, Final Batch Loss: 0.3311367630958557\n",
      "Subject 3, Epoch 870, Loss: 0.5664511770009995, Final Batch Loss: 0.25852781534194946\n",
      "Subject 3, Epoch 871, Loss: 0.5884271264076233, Final Batch Loss: 0.2367987185716629\n",
      "Subject 3, Epoch 872, Loss: 0.563204824924469, Final Batch Loss: 0.16832877695560455\n",
      "Subject 3, Epoch 873, Loss: 0.6698902249336243, Final Batch Loss: 0.2616608440876007\n",
      "Subject 3, Epoch 874, Loss: 0.6085461974143982, Final Batch Loss: 0.19861450791358948\n",
      "Subject 3, Epoch 875, Loss: 0.5552966818213463, Final Batch Loss: 0.10797566920518875\n",
      "Subject 3, Epoch 876, Loss: 0.6190565526485443, Final Batch Loss: 0.22496818006038666\n",
      "Subject 3, Epoch 877, Loss: 0.5991802364587784, Final Batch Loss: 0.26900073885917664\n",
      "Subject 3, Epoch 878, Loss: 0.5678359121084213, Final Batch Loss: 0.231777623295784\n",
      "Subject 3, Epoch 879, Loss: 0.5389228910207748, Final Batch Loss: 0.16548356413841248\n",
      "Subject 3, Epoch 880, Loss: 0.5580247193574905, Final Batch Loss: 0.13822674751281738\n",
      "Subject 3, Epoch 881, Loss: 0.540818840265274, Final Batch Loss: 0.11990554630756378\n",
      "Subject 3, Epoch 882, Loss: 0.5386411398649216, Final Batch Loss: 0.18386125564575195\n",
      "Subject 3, Epoch 883, Loss: 0.541658565402031, Final Batch Loss: 0.16907916963100433\n",
      "Subject 3, Epoch 884, Loss: 0.5584869384765625, Final Batch Loss: 0.1653367280960083\n",
      "Subject 3, Epoch 885, Loss: 0.6422029137611389, Final Batch Loss: 0.22188076376914978\n",
      "Subject 3, Epoch 886, Loss: 0.5856685489416122, Final Batch Loss: 0.18653710186481476\n",
      "Subject 3, Epoch 887, Loss: 0.5906210839748383, Final Batch Loss: 0.24811476469039917\n",
      "Subject 3, Epoch 888, Loss: 0.4641026556491852, Final Batch Loss: 0.18700693547725677\n",
      "Subject 3, Epoch 889, Loss: 0.6146911829710007, Final Batch Loss: 0.17539037764072418\n",
      "Subject 3, Epoch 890, Loss: 0.5306491255760193, Final Batch Loss: 0.1773277372121811\n",
      "Subject 3, Epoch 891, Loss: 0.5579877346754074, Final Batch Loss: 0.1700943410396576\n",
      "Subject 3, Epoch 892, Loss: 0.474292129278183, Final Batch Loss: 0.1365875005722046\n",
      "Subject 3, Epoch 893, Loss: 0.5903278440237045, Final Batch Loss: 0.2319551259279251\n",
      "Subject 3, Epoch 894, Loss: 0.5192213207483292, Final Batch Loss: 0.19855044782161713\n",
      "Subject 3, Epoch 895, Loss: 0.5527329444885254, Final Batch Loss: 0.18805643916130066\n",
      "Subject 3, Epoch 896, Loss: 0.49244801700115204, Final Batch Loss: 0.1729077845811844\n",
      "Subject 3, Epoch 897, Loss: 0.5562365651130676, Final Batch Loss: 0.16533786058425903\n",
      "Subject 3, Epoch 898, Loss: 0.45981651544570923, Final Batch Loss: 0.13793854415416718\n",
      "Subject 3, Epoch 899, Loss: 0.6223338544368744, Final Batch Loss: 0.23440293967723846\n",
      "Subject 3, Epoch 900, Loss: 0.5097994953393936, Final Batch Loss: 0.1249997615814209\n",
      "Subject 3, Epoch 901, Loss: 0.5870343446731567, Final Batch Loss: 0.1806146204471588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3, Epoch 902, Loss: 0.5026060566306114, Final Batch Loss: 0.22389572858810425\n",
      "Subject 3, Epoch 903, Loss: 0.5208663865923882, Final Batch Loss: 0.27681922912597656\n",
      "Subject 3, Epoch 904, Loss: 0.5160447582602501, Final Batch Loss: 0.12247767299413681\n",
      "Subject 3, Epoch 905, Loss: 0.5945306867361069, Final Batch Loss: 0.2772713601589203\n",
      "Subject 3, Epoch 906, Loss: 0.4664030820131302, Final Batch Loss: 0.1268523633480072\n",
      "Subject 3, Epoch 907, Loss: 0.4885690212249756, Final Batch Loss: 0.20002534985542297\n",
      "Subject 3, Epoch 908, Loss: 0.5460638701915741, Final Batch Loss: 0.17244994640350342\n",
      "Subject 3, Epoch 909, Loss: 0.5578815937042236, Final Batch Loss: 0.19218119978904724\n",
      "Subject 3, Epoch 910, Loss: 0.47649650275707245, Final Batch Loss: 0.15719476342201233\n",
      "Subject 3, Epoch 911, Loss: 0.6780437678098679, Final Batch Loss: 0.21798324584960938\n",
      "Subject 3, Epoch 912, Loss: 0.5582635849714279, Final Batch Loss: 0.2188824862241745\n",
      "Subject 3, Epoch 913, Loss: 0.5514422357082367, Final Batch Loss: 0.19760073721408844\n",
      "Subject 3, Epoch 914, Loss: 0.4768671244382858, Final Batch Loss: 0.18116146326065063\n",
      "Subject 3, Epoch 915, Loss: 0.49079659581184387, Final Batch Loss: 0.14259067177772522\n",
      "Subject 3, Epoch 916, Loss: 0.6154253035783768, Final Batch Loss: 0.1963587999343872\n",
      "Subject 3, Epoch 917, Loss: 0.4889712631702423, Final Batch Loss: 0.14942748844623566\n",
      "Subject 3, Epoch 918, Loss: 0.6828054189682007, Final Batch Loss: 0.24027515947818756\n",
      "Subject 3, Epoch 919, Loss: 0.5793973505496979, Final Batch Loss: 0.2203548550605774\n",
      "Subject 3, Epoch 920, Loss: 0.43822382390499115, Final Batch Loss: 0.13520802557468414\n",
      "Subject 3, Epoch 921, Loss: 0.6259972304105759, Final Batch Loss: 0.20277039706707\n",
      "Subject 3, Epoch 922, Loss: 0.4734763950109482, Final Batch Loss: 0.1944940686225891\n",
      "Subject 3, Epoch 923, Loss: 0.5397127121686935, Final Batch Loss: 0.22068798542022705\n",
      "Subject 3, Epoch 924, Loss: 0.6100136637687683, Final Batch Loss: 0.2795591354370117\n",
      "Subject 3, Epoch 925, Loss: 0.5372959077358246, Final Batch Loss: 0.19503629207611084\n",
      "Subject 3, Epoch 926, Loss: 0.5387902706861496, Final Batch Loss: 0.14452709257602692\n",
      "Subject 3, Epoch 927, Loss: 0.5120589137077332, Final Batch Loss: 0.14307662844657898\n",
      "Subject 3, Epoch 928, Loss: 0.5470553785562515, Final Batch Loss: 0.17820492386817932\n",
      "Subject 3, Epoch 929, Loss: 0.5070657581090927, Final Batch Loss: 0.1625300943851471\n",
      "Subject 3, Epoch 930, Loss: 0.4710446670651436, Final Batch Loss: 0.17083510756492615\n",
      "Subject 3, Epoch 931, Loss: 0.4442424774169922, Final Batch Loss: 0.11415275931358337\n",
      "Subject 3, Epoch 932, Loss: 0.45542535185813904, Final Batch Loss: 0.14293824136257172\n",
      "Subject 3, Epoch 933, Loss: 0.46175649762153625, Final Batch Loss: 0.1357823759317398\n",
      "Subject 3, Epoch 934, Loss: 0.4971272945404053, Final Batch Loss: 0.16540463268756866\n",
      "Subject 3, Epoch 935, Loss: 0.47566379606723785, Final Batch Loss: 0.16776984930038452\n",
      "Subject 3, Epoch 936, Loss: 0.4538116082549095, Final Batch Loss: 0.18285644054412842\n",
      "Subject 3, Epoch 937, Loss: 0.47870983928442, Final Batch Loss: 0.12371385842561722\n",
      "Subject 3, Epoch 938, Loss: 0.5889425426721573, Final Batch Loss: 0.1693248301744461\n",
      "Subject 3, Epoch 939, Loss: 0.49740323424339294, Final Batch Loss: 0.17226311564445496\n",
      "Subject 3, Epoch 940, Loss: 0.5253317207098007, Final Batch Loss: 0.13270287215709686\n",
      "Subject 3, Epoch 941, Loss: 0.5027110055088997, Final Batch Loss: 0.10795272141695023\n",
      "Subject 3, Epoch 942, Loss: 0.46980275958776474, Final Batch Loss: 0.12496762722730637\n",
      "Subject 3, Epoch 943, Loss: 0.5423469841480255, Final Batch Loss: 0.19508597254753113\n",
      "Subject 3, Epoch 944, Loss: 0.4435548484325409, Final Batch Loss: 0.1087956577539444\n",
      "Subject 3, Epoch 945, Loss: 0.6135299950838089, Final Batch Loss: 0.23366205394268036\n",
      "Subject 3, Epoch 946, Loss: 0.46397029608488083, Final Batch Loss: 0.11854200810194016\n",
      "Subject 3, Epoch 947, Loss: 0.4037143886089325, Final Batch Loss: 0.09554969519376755\n",
      "Subject 3, Epoch 948, Loss: 0.4645692706108093, Final Batch Loss: 0.11732913553714752\n",
      "Subject 3, Epoch 949, Loss: 0.4618075489997864, Final Batch Loss: 0.14566558599472046\n",
      "Subject 3, Epoch 950, Loss: 0.4751146733760834, Final Batch Loss: 0.1383240669965744\n",
      "Subject 3, Epoch 951, Loss: 0.4358244091272354, Final Batch Loss: 0.16256903111934662\n",
      "Subject 3, Epoch 952, Loss: 0.44954514503479004, Final Batch Loss: 0.1560215801000595\n",
      "Subject 3, Epoch 953, Loss: 0.5069934278726578, Final Batch Loss: 0.14632867276668549\n",
      "Subject 3, Epoch 954, Loss: 0.4288446307182312, Final Batch Loss: 0.13122673332691193\n",
      "Subject 3, Epoch 955, Loss: 0.5207372605800629, Final Batch Loss: 0.19264023005962372\n",
      "Subject 3, Epoch 956, Loss: 0.43065472692251205, Final Batch Loss: 0.18924036622047424\n",
      "Subject 3, Epoch 957, Loss: 0.5062246173620224, Final Batch Loss: 0.12616881728172302\n",
      "Subject 3, Epoch 958, Loss: 0.4979855567216873, Final Batch Loss: 0.1629980355501175\n",
      "Subject 3, Epoch 959, Loss: 0.4176051542162895, Final Batch Loss: 0.12119238078594208\n",
      "Subject 3, Epoch 960, Loss: 0.6242923885583878, Final Batch Loss: 0.1708100587129593\n",
      "Subject 3, Epoch 961, Loss: 0.5596450865268707, Final Batch Loss: 0.13486409187316895\n",
      "Subject 3, Epoch 962, Loss: 0.46846868097782135, Final Batch Loss: 0.14505821466445923\n",
      "Subject 3, Epoch 963, Loss: 0.5177450776100159, Final Batch Loss: 0.22397315502166748\n",
      "Subject 3, Epoch 964, Loss: 0.4255238100886345, Final Batch Loss: 0.12982603907585144\n",
      "Subject 3, Epoch 965, Loss: 0.4585626721382141, Final Batch Loss: 0.1242779940366745\n",
      "Subject 3, Epoch 966, Loss: 0.43665631115436554, Final Batch Loss: 0.1319555640220642\n",
      "Subject 3, Epoch 967, Loss: 0.432884581387043, Final Batch Loss: 0.1638270616531372\n",
      "Subject 3, Epoch 968, Loss: 0.5064872056245804, Final Batch Loss: 0.1854688674211502\n",
      "Subject 3, Epoch 969, Loss: 0.5826204046607018, Final Batch Loss: 0.22967851161956787\n",
      "Subject 3, Epoch 970, Loss: 0.4666111394762993, Final Batch Loss: 0.11523411422967911\n",
      "Subject 3, Epoch 971, Loss: 0.42712393403053284, Final Batch Loss: 0.1134878620505333\n",
      "Subject 3, Epoch 972, Loss: 0.5681893825531006, Final Batch Loss: 0.176448255777359\n",
      "Subject 3, Epoch 973, Loss: 0.44171303510665894, Final Batch Loss: 0.13726182281970978\n",
      "Subject 3, Epoch 974, Loss: 0.48203040659427643, Final Batch Loss: 0.1460045725107193\n",
      "Subject 3, Epoch 975, Loss: 0.4800334870815277, Final Batch Loss: 0.13182294368743896\n",
      "Subject 3, Epoch 976, Loss: 0.38563990592956543, Final Batch Loss: 0.16467803716659546\n",
      "Subject 3, Epoch 977, Loss: 0.433152973651886, Final Batch Loss: 0.11769071221351624\n",
      "Subject 3, Epoch 978, Loss: 0.4797946363687515, Final Batch Loss: 0.18557390570640564\n",
      "Subject 3, Epoch 979, Loss: 0.43143658339977264, Final Batch Loss: 0.14552351832389832\n",
      "Subject 3, Epoch 980, Loss: 0.5182565897703171, Final Batch Loss: 0.1907612830400467\n",
      "Subject 3, Epoch 981, Loss: 0.4193904250860214, Final Batch Loss: 0.10635018348693848\n",
      "Subject 3, Epoch 982, Loss: 0.46094512939453125, Final Batch Loss: 0.13209275901317596\n",
      "Subject 3, Epoch 983, Loss: 0.5114553943276405, Final Batch Loss: 0.21084190905094147\n",
      "Subject 3, Epoch 984, Loss: 0.3820560649037361, Final Batch Loss: 0.13220122456550598\n",
      "Subject 3, Epoch 985, Loss: 0.442079558968544, Final Batch Loss: 0.14953893423080444\n",
      "Subject 3, Epoch 986, Loss: 0.4210858419537544, Final Batch Loss: 0.10212220996618271\n",
      "Subject 3, Epoch 987, Loss: 0.5043286979198456, Final Batch Loss: 0.2278345227241516\n",
      "Subject 3, Epoch 988, Loss: 0.5341104567050934, Final Batch Loss: 0.20362886786460876\n",
      "Subject 3, Epoch 989, Loss: 0.4550909921526909, Final Batch Loss: 0.2328026443719864\n",
      "Subject 3, Epoch 990, Loss: 0.47283194959163666, Final Batch Loss: 0.1474316567182541\n",
      "Subject 3, Epoch 991, Loss: 0.39320108294487, Final Batch Loss: 0.1694091260433197\n",
      "Subject 3, Epoch 992, Loss: 0.5697211772203445, Final Batch Loss: 0.1426878720521927\n",
      "Subject 3, Epoch 993, Loss: 0.49752625823020935, Final Batch Loss: 0.21928523480892181\n",
      "Subject 3, Epoch 994, Loss: 0.6546134352684021, Final Batch Loss: 0.1644357144832611\n",
      "Subject 3, Epoch 995, Loss: 0.5798138380050659, Final Batch Loss: 0.18142645061016083\n",
      "Subject 3, Epoch 996, Loss: 0.47301241755485535, Final Batch Loss: 0.18420179188251495\n",
      "Subject 3, Epoch 997, Loss: 0.5356493145227432, Final Batch Loss: 0.09765300154685974\n",
      "Subject 3, Epoch 998, Loss: 0.484534814953804, Final Batch Loss: 0.13069646060466766\n",
      "Subject 3, Epoch 999, Loss: 0.4683077409863472, Final Batch Loss: 0.1939159631729126\n",
      "Subject 3, Epoch 1000, Loss: 0.5982704162597656, Final Batch Loss: 0.21636727452278137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 1, Loss: 5.402779459953308, Final Batch Loss: 1.7998000383377075\n",
      "Subject 4, Epoch 2, Loss: 5.396775841712952, Final Batch Loss: 1.8008112907409668\n",
      "Subject 4, Epoch 3, Loss: 5.388651013374329, Final Batch Loss: 1.7941898107528687\n",
      "Subject 4, Epoch 4, Loss: 5.382138729095459, Final Batch Loss: 1.8064037561416626\n",
      "Subject 4, Epoch 5, Loss: 5.382548451423645, Final Batch Loss: 1.8031686544418335\n",
      "Subject 4, Epoch 6, Loss: 5.371449112892151, Final Batch Loss: 1.7994598150253296\n",
      "Subject 4, Epoch 7, Loss: 5.363358855247498, Final Batch Loss: 1.8003243207931519\n",
      "Subject 4, Epoch 8, Loss: 5.350362062454224, Final Batch Loss: 1.7766413688659668\n",
      "Subject 4, Epoch 9, Loss: 5.333070755004883, Final Batch Loss: 1.7878531217575073\n",
      "Subject 4, Epoch 10, Loss: 5.3036134243011475, Final Batch Loss: 1.756002426147461\n",
      "Subject 4, Epoch 11, Loss: 5.298806428909302, Final Batch Loss: 1.7757470607757568\n",
      "Subject 4, Epoch 12, Loss: 5.262293219566345, Final Batch Loss: 1.7532020807266235\n",
      "Subject 4, Epoch 13, Loss: 5.22204327583313, Final Batch Loss: 1.7206288576126099\n",
      "Subject 4, Epoch 14, Loss: 5.204548358917236, Final Batch Loss: 1.731560230255127\n",
      "Subject 4, Epoch 15, Loss: 5.16427755355835, Final Batch Loss: 1.7216198444366455\n",
      "Subject 4, Epoch 16, Loss: 5.116237282752991, Final Batch Loss: 1.7099416255950928\n",
      "Subject 4, Epoch 17, Loss: 5.052987217903137, Final Batch Loss: 1.674425721168518\n",
      "Subject 4, Epoch 18, Loss: 4.976713418960571, Final Batch Loss: 1.6574177742004395\n",
      "Subject 4, Epoch 19, Loss: 4.930480122566223, Final Batch Loss: 1.623684048652649\n",
      "Subject 4, Epoch 20, Loss: 4.882781982421875, Final Batch Loss: 1.6400179862976074\n",
      "Subject 4, Epoch 21, Loss: 4.747469902038574, Final Batch Loss: 1.582155466079712\n",
      "Subject 4, Epoch 22, Loss: 4.636981129646301, Final Batch Loss: 1.5525344610214233\n",
      "Subject 4, Epoch 23, Loss: 4.520361661911011, Final Batch Loss: 1.4683678150177002\n",
      "Subject 4, Epoch 24, Loss: 4.355910897254944, Final Batch Loss: 1.4571537971496582\n",
      "Subject 4, Epoch 25, Loss: 4.259920835494995, Final Batch Loss: 1.4239665269851685\n",
      "Subject 4, Epoch 26, Loss: 4.150127291679382, Final Batch Loss: 1.3778451681137085\n",
      "Subject 4, Epoch 27, Loss: 4.046179533004761, Final Batch Loss: 1.3177450895309448\n",
      "Subject 4, Epoch 28, Loss: 3.9535340070724487, Final Batch Loss: 1.3048709630966187\n",
      "Subject 4, Epoch 29, Loss: 3.891712188720703, Final Batch Loss: 1.2370991706848145\n",
      "Subject 4, Epoch 30, Loss: 3.78181791305542, Final Batch Loss: 1.2407728433609009\n",
      "Subject 4, Epoch 31, Loss: 3.6817197799682617, Final Batch Loss: 1.24455988407135\n",
      "Subject 4, Epoch 32, Loss: 3.5502055883407593, Final Batch Loss: 1.139188528060913\n",
      "Subject 4, Epoch 33, Loss: 3.4266343116760254, Final Batch Loss: 1.1284905672073364\n",
      "Subject 4, Epoch 34, Loss: 3.4449328184127808, Final Batch Loss: 1.1030815839767456\n",
      "Subject 4, Epoch 35, Loss: 3.405954599380493, Final Batch Loss: 1.151068925857544\n",
      "Subject 4, Epoch 36, Loss: 3.2730404138565063, Final Batch Loss: 0.9968091249465942\n",
      "Subject 4, Epoch 37, Loss: 3.257991313934326, Final Batch Loss: 1.0569148063659668\n",
      "Subject 4, Epoch 38, Loss: 3.2001609802246094, Final Batch Loss: 1.0301369428634644\n",
      "Subject 4, Epoch 39, Loss: 3.093408524990082, Final Batch Loss: 1.0707465410232544\n",
      "Subject 4, Epoch 40, Loss: 3.1596726179122925, Final Batch Loss: 1.0908994674682617\n",
      "Subject 4, Epoch 41, Loss: 3.0297719836235046, Final Batch Loss: 1.0108810663223267\n",
      "Subject 4, Epoch 42, Loss: 3.1153379678726196, Final Batch Loss: 1.0164415836334229\n",
      "Subject 4, Epoch 43, Loss: 3.005528688430786, Final Batch Loss: 0.9765833020210266\n",
      "Subject 4, Epoch 44, Loss: 3.1368794441223145, Final Batch Loss: 1.0009469985961914\n",
      "Subject 4, Epoch 45, Loss: 2.885910391807556, Final Batch Loss: 0.8949153423309326\n",
      "Subject 4, Epoch 46, Loss: 2.8638163208961487, Final Batch Loss: 0.9115046262741089\n",
      "Subject 4, Epoch 47, Loss: 2.862093508243561, Final Batch Loss: 0.9775276184082031\n",
      "Subject 4, Epoch 48, Loss: 2.8358689546585083, Final Batch Loss: 0.9298734664916992\n",
      "Subject 4, Epoch 49, Loss: 2.6791523098945618, Final Batch Loss: 0.910735547542572\n",
      "Subject 4, Epoch 50, Loss: 2.6529284715652466, Final Batch Loss: 0.852120041847229\n",
      "Subject 4, Epoch 51, Loss: 2.6118292808532715, Final Batch Loss: 0.8553286790847778\n",
      "Subject 4, Epoch 52, Loss: 2.8094406723976135, Final Batch Loss: 1.0514214038848877\n",
      "Subject 4, Epoch 53, Loss: 2.739404082298279, Final Batch Loss: 0.8646584749221802\n",
      "Subject 4, Epoch 54, Loss: 2.7074291110038757, Final Batch Loss: 0.9389861822128296\n",
      "Subject 4, Epoch 55, Loss: 2.7076905369758606, Final Batch Loss: 0.9166713356971741\n",
      "Subject 4, Epoch 56, Loss: 2.615442395210266, Final Batch Loss: 0.8861544728279114\n",
      "Subject 4, Epoch 57, Loss: 2.616700053215027, Final Batch Loss: 0.8189142346382141\n",
      "Subject 4, Epoch 58, Loss: 2.7038084268569946, Final Batch Loss: 0.9824455380439758\n",
      "Subject 4, Epoch 59, Loss: 2.5810394883155823, Final Batch Loss: 0.8486737608909607\n",
      "Subject 4, Epoch 60, Loss: 2.434983968734741, Final Batch Loss: 0.7669126987457275\n",
      "Subject 4, Epoch 61, Loss: 2.529233992099762, Final Batch Loss: 0.9185677766799927\n",
      "Subject 4, Epoch 62, Loss: 2.366032361984253, Final Batch Loss: 0.7181404829025269\n",
      "Subject 4, Epoch 63, Loss: 2.476302146911621, Final Batch Loss: 0.8941162824630737\n",
      "Subject 4, Epoch 64, Loss: 2.269612431526184, Final Batch Loss: 0.7563959956169128\n",
      "Subject 4, Epoch 65, Loss: 2.416502892971039, Final Batch Loss: 0.8116446137428284\n",
      "Subject 4, Epoch 66, Loss: 2.1554397344589233, Final Batch Loss: 0.6145842671394348\n",
      "Subject 4, Epoch 67, Loss: 2.318350613117218, Final Batch Loss: 0.740704357624054\n",
      "Subject 4, Epoch 68, Loss: 2.363399624824524, Final Batch Loss: 0.7834991216659546\n",
      "Subject 4, Epoch 69, Loss: 2.25026273727417, Final Batch Loss: 0.6740508675575256\n",
      "Subject 4, Epoch 70, Loss: 2.2190489768981934, Final Batch Loss: 0.7357609272003174\n",
      "Subject 4, Epoch 71, Loss: 2.2887206077575684, Final Batch Loss: 0.8280224800109863\n",
      "Subject 4, Epoch 72, Loss: 2.2259480953216553, Final Batch Loss: 0.810255765914917\n",
      "Subject 4, Epoch 73, Loss: 2.3946529030799866, Final Batch Loss: 0.8754913210868835\n",
      "Subject 4, Epoch 74, Loss: 2.2265273332595825, Final Batch Loss: 0.6749627590179443\n",
      "Subject 4, Epoch 75, Loss: 2.225796639919281, Final Batch Loss: 0.7750195264816284\n",
      "Subject 4, Epoch 76, Loss: 2.166447937488556, Final Batch Loss: 0.6773952841758728\n",
      "Subject 4, Epoch 77, Loss: 1.9720631837844849, Final Batch Loss: 0.6771646738052368\n",
      "Subject 4, Epoch 78, Loss: 2.0686920285224915, Final Batch Loss: 0.759714663028717\n",
      "Subject 4, Epoch 79, Loss: 2.1377012729644775, Final Batch Loss: 0.7910933494567871\n",
      "Subject 4, Epoch 80, Loss: 2.0803416967391968, Final Batch Loss: 0.7952941060066223\n",
      "Subject 4, Epoch 81, Loss: 1.9681435823440552, Final Batch Loss: 0.7176980376243591\n",
      "Subject 4, Epoch 82, Loss: 1.92194265127182, Final Batch Loss: 0.6454739570617676\n",
      "Subject 4, Epoch 83, Loss: 2.002493143081665, Final Batch Loss: 0.7244778275489807\n",
      "Subject 4, Epoch 84, Loss: 1.9799436330795288, Final Batch Loss: 0.5616875886917114\n",
      "Subject 4, Epoch 85, Loss: 1.9191513061523438, Final Batch Loss: 0.6070467829704285\n",
      "Subject 4, Epoch 86, Loss: 1.8344423174858093, Final Batch Loss: 0.5321530103683472\n",
      "Subject 4, Epoch 87, Loss: 1.9007818698883057, Final Batch Loss: 0.6120373606681824\n",
      "Subject 4, Epoch 88, Loss: 1.8346871733665466, Final Batch Loss: 0.5684769153594971\n",
      "Subject 4, Epoch 89, Loss: 1.7795490026474, Final Batch Loss: 0.5460194945335388\n",
      "Subject 4, Epoch 90, Loss: 1.7964429259300232, Final Batch Loss: 0.6399621367454529\n",
      "Subject 4, Epoch 91, Loss: 1.6473042964935303, Final Batch Loss: 0.5724767446517944\n",
      "Subject 4, Epoch 92, Loss: 1.8580610752105713, Final Batch Loss: 0.7001901268959045\n",
      "Subject 4, Epoch 93, Loss: 1.972241461277008, Final Batch Loss: 0.7037068605422974\n",
      "Subject 4, Epoch 94, Loss: 1.8244745135307312, Final Batch Loss: 0.6003799438476562\n",
      "Subject 4, Epoch 95, Loss: 1.7895240783691406, Final Batch Loss: 0.561005711555481\n",
      "Subject 4, Epoch 96, Loss: 1.808201551437378, Final Batch Loss: 0.6176011562347412\n",
      "Subject 4, Epoch 97, Loss: 1.7147305607795715, Final Batch Loss: 0.525047779083252\n",
      "Subject 4, Epoch 98, Loss: 1.6598001718521118, Final Batch Loss: 0.6584605574607849\n",
      "Subject 4, Epoch 99, Loss: 1.5771930515766144, Final Batch Loss: 0.4295942485332489\n",
      "Subject 4, Epoch 100, Loss: 1.7209131717681885, Final Batch Loss: 0.610950767993927\n",
      "Subject 4, Epoch 101, Loss: 1.7632127106189728, Final Batch Loss: 0.6457239985466003\n",
      "Subject 4, Epoch 102, Loss: 1.6003636717796326, Final Batch Loss: 0.542899489402771\n",
      "Subject 4, Epoch 103, Loss: 1.6292639374732971, Final Batch Loss: 0.5431666970252991\n",
      "Subject 4, Epoch 104, Loss: 1.5571708381175995, Final Batch Loss: 0.47162961959838867\n",
      "Subject 4, Epoch 105, Loss: 1.6184166073799133, Final Batch Loss: 0.5032355189323425\n",
      "Subject 4, Epoch 106, Loss: 1.545078456401825, Final Batch Loss: 0.4648079574108124\n",
      "Subject 4, Epoch 107, Loss: 1.488244503736496, Final Batch Loss: 0.5151072144508362\n",
      "Subject 4, Epoch 108, Loss: 1.6149338483810425, Final Batch Loss: 0.5400186777114868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 109, Loss: 1.5162821114063263, Final Batch Loss: 0.4989743232727051\n",
      "Subject 4, Epoch 110, Loss: 1.470842570066452, Final Batch Loss: 0.5537567138671875\n",
      "Subject 4, Epoch 111, Loss: 1.4962305128574371, Final Batch Loss: 0.40234389901161194\n",
      "Subject 4, Epoch 112, Loss: 1.4451973140239716, Final Batch Loss: 0.417485773563385\n",
      "Subject 4, Epoch 113, Loss: 1.5011165142059326, Final Batch Loss: 0.5510680675506592\n",
      "Subject 4, Epoch 114, Loss: 1.4781765341758728, Final Batch Loss: 0.3593263030052185\n",
      "Subject 4, Epoch 115, Loss: 1.4814278781414032, Final Batch Loss: 0.43310320377349854\n",
      "Subject 4, Epoch 116, Loss: 1.4435620605945587, Final Batch Loss: 0.47509658336639404\n",
      "Subject 4, Epoch 117, Loss: 1.441216379404068, Final Batch Loss: 0.5560388565063477\n",
      "Subject 4, Epoch 118, Loss: 1.4804478585720062, Final Batch Loss: 0.4752160608768463\n",
      "Subject 4, Epoch 119, Loss: 1.4039246439933777, Final Batch Loss: 0.49275675415992737\n",
      "Subject 4, Epoch 120, Loss: 1.4658780694007874, Final Batch Loss: 0.49793681502342224\n",
      "Subject 4, Epoch 121, Loss: 1.5099246501922607, Final Batch Loss: 0.426482617855072\n",
      "Subject 4, Epoch 122, Loss: 1.4445250034332275, Final Batch Loss: 0.418223112821579\n",
      "Subject 4, Epoch 123, Loss: 1.4399689733982086, Final Batch Loss: 0.4810578227043152\n",
      "Subject 4, Epoch 124, Loss: 1.3612810373306274, Final Batch Loss: 0.4074397087097168\n",
      "Subject 4, Epoch 125, Loss: 1.2779925763607025, Final Batch Loss: 0.385954886674881\n",
      "Subject 4, Epoch 126, Loss: 1.3724392652511597, Final Batch Loss: 0.39054226875305176\n",
      "Subject 4, Epoch 127, Loss: 1.3714290261268616, Final Batch Loss: 0.5465131402015686\n",
      "Subject 4, Epoch 128, Loss: 1.400130808353424, Final Batch Loss: 0.5418294072151184\n",
      "Subject 4, Epoch 129, Loss: 1.3150021135807037, Final Batch Loss: 0.38680213689804077\n",
      "Subject 4, Epoch 130, Loss: 1.379073053598404, Final Batch Loss: 0.41352733969688416\n",
      "Subject 4, Epoch 131, Loss: 1.4392341673374176, Final Batch Loss: 0.4878487288951874\n",
      "Subject 4, Epoch 132, Loss: 1.3820599615573883, Final Batch Loss: 0.4379291534423828\n",
      "Subject 4, Epoch 133, Loss: 1.3355655372142792, Final Batch Loss: 0.46510422229766846\n",
      "Subject 4, Epoch 134, Loss: 1.289894938468933, Final Batch Loss: 0.594683825969696\n",
      "Subject 4, Epoch 135, Loss: 1.3367296159267426, Final Batch Loss: 0.38032329082489014\n",
      "Subject 4, Epoch 136, Loss: 1.6334241032600403, Final Batch Loss: 0.5273033976554871\n",
      "Subject 4, Epoch 137, Loss: 1.4389090240001678, Final Batch Loss: 0.5835682153701782\n",
      "Subject 4, Epoch 138, Loss: 1.397190511226654, Final Batch Loss: 0.4534868597984314\n",
      "Subject 4, Epoch 139, Loss: 1.4065799713134766, Final Batch Loss: 0.4864063560962677\n",
      "Subject 4, Epoch 140, Loss: 1.3265046775341034, Final Batch Loss: 0.3728391230106354\n",
      "Subject 4, Epoch 141, Loss: 1.3327082097530365, Final Batch Loss: 0.3811604976654053\n",
      "Subject 4, Epoch 142, Loss: 1.3032804131507874, Final Batch Loss: 0.4706265926361084\n",
      "Subject 4, Epoch 143, Loss: 1.1591301560401917, Final Batch Loss: 0.37018465995788574\n",
      "Subject 4, Epoch 144, Loss: 1.332947701215744, Final Batch Loss: 0.4594448506832123\n",
      "Subject 4, Epoch 145, Loss: 1.3139152824878693, Final Batch Loss: 0.470426470041275\n",
      "Subject 4, Epoch 146, Loss: 1.4080943167209625, Final Batch Loss: 0.5009405612945557\n",
      "Subject 4, Epoch 147, Loss: 1.3368823826313019, Final Batch Loss: 0.4794226586818695\n",
      "Subject 4, Epoch 148, Loss: 1.2779428660869598, Final Batch Loss: 0.43052154779434204\n",
      "Subject 4, Epoch 149, Loss: 1.3756976127624512, Final Batch Loss: 0.5143657922744751\n",
      "Subject 4, Epoch 150, Loss: 1.1990265846252441, Final Batch Loss: 0.3868982195854187\n",
      "Subject 4, Epoch 151, Loss: 1.1921972632408142, Final Batch Loss: 0.3526551127433777\n",
      "Subject 4, Epoch 152, Loss: 1.0850849449634552, Final Batch Loss: 0.2888585031032562\n",
      "Subject 4, Epoch 153, Loss: 1.2141903638839722, Final Batch Loss: 0.4443354308605194\n",
      "Subject 4, Epoch 154, Loss: 1.2431893348693848, Final Batch Loss: 0.3859134316444397\n",
      "Subject 4, Epoch 155, Loss: 1.3021831214427948, Final Batch Loss: 0.2997288405895233\n",
      "Subject 4, Epoch 156, Loss: 1.356532335281372, Final Batch Loss: 0.4898824393749237\n",
      "Subject 4, Epoch 157, Loss: 1.3402100205421448, Final Batch Loss: 0.4237196743488312\n",
      "Subject 4, Epoch 158, Loss: 1.281851202249527, Final Batch Loss: 0.35070928931236267\n",
      "Subject 4, Epoch 159, Loss: 1.2952729761600494, Final Batch Loss: 0.3852893114089966\n",
      "Subject 4, Epoch 160, Loss: 1.2369659543037415, Final Batch Loss: 0.3781522214412689\n",
      "Subject 4, Epoch 161, Loss: 1.183185636997223, Final Batch Loss: 0.3601023554801941\n",
      "Subject 4, Epoch 162, Loss: 1.14522585272789, Final Batch Loss: 0.322344034910202\n",
      "Subject 4, Epoch 163, Loss: 1.3069778978824615, Final Batch Loss: 0.4229907989501953\n",
      "Subject 4, Epoch 164, Loss: 1.3079909980297089, Final Batch Loss: 0.40511131286621094\n",
      "Subject 4, Epoch 165, Loss: 1.1688534915447235, Final Batch Loss: 0.3657151758670807\n",
      "Subject 4, Epoch 166, Loss: 1.1545996367931366, Final Batch Loss: 0.41199859976768494\n",
      "Subject 4, Epoch 167, Loss: 1.153548687696457, Final Batch Loss: 0.3372448980808258\n",
      "Subject 4, Epoch 168, Loss: 1.176237791776657, Final Batch Loss: 0.42869988083839417\n",
      "Subject 4, Epoch 169, Loss: 1.1077346205711365, Final Batch Loss: 0.4403349459171295\n",
      "Subject 4, Epoch 170, Loss: 1.232989490032196, Final Batch Loss: 0.5015242099761963\n",
      "Subject 4, Epoch 171, Loss: 1.2442757189273834, Final Batch Loss: 0.43456920981407166\n",
      "Subject 4, Epoch 172, Loss: 1.1804403364658356, Final Batch Loss: 0.3821766972541809\n",
      "Subject 4, Epoch 173, Loss: 1.290418952703476, Final Batch Loss: 0.48964792490005493\n",
      "Subject 4, Epoch 174, Loss: 1.1566319167613983, Final Batch Loss: 0.32831451296806335\n",
      "Subject 4, Epoch 175, Loss: 1.1257765144109726, Final Batch Loss: 0.24255375564098358\n",
      "Subject 4, Epoch 176, Loss: 1.1101739406585693, Final Batch Loss: 0.3061668872833252\n",
      "Subject 4, Epoch 177, Loss: 1.2467740774154663, Final Batch Loss: 0.4368072748184204\n",
      "Subject 4, Epoch 178, Loss: 1.2620897591114044, Final Batch Loss: 0.4063606560230255\n",
      "Subject 4, Epoch 179, Loss: 1.0644921660423279, Final Batch Loss: 0.3165716230869293\n",
      "Subject 4, Epoch 180, Loss: 1.118263602256775, Final Batch Loss: 0.4331003427505493\n",
      "Subject 4, Epoch 181, Loss: 1.2496748864650726, Final Batch Loss: 0.48626554012298584\n",
      "Subject 4, Epoch 182, Loss: 1.0820965468883514, Final Batch Loss: 0.30618998408317566\n",
      "Subject 4, Epoch 183, Loss: 1.2158575654029846, Final Batch Loss: 0.3430750370025635\n",
      "Subject 4, Epoch 184, Loss: 1.1747523546218872, Final Batch Loss: 0.32784828543663025\n",
      "Subject 4, Epoch 185, Loss: 1.1280328333377838, Final Batch Loss: 0.34561434388160706\n",
      "Subject 4, Epoch 186, Loss: 1.138258457183838, Final Batch Loss: 0.3913303017616272\n",
      "Subject 4, Epoch 187, Loss: 1.1341601610183716, Final Batch Loss: 0.3591885566711426\n",
      "Subject 4, Epoch 188, Loss: 1.1577297449111938, Final Batch Loss: 0.38806140422821045\n",
      "Subject 4, Epoch 189, Loss: 1.2633275091648102, Final Batch Loss: 0.43315553665161133\n",
      "Subject 4, Epoch 190, Loss: 1.202426254749298, Final Batch Loss: 0.4170331656932831\n",
      "Subject 4, Epoch 191, Loss: 0.9949520528316498, Final Batch Loss: 0.37619301676750183\n",
      "Subject 4, Epoch 192, Loss: 1.1791460812091827, Final Batch Loss: 0.39442482590675354\n",
      "Subject 4, Epoch 193, Loss: 1.0765078663825989, Final Batch Loss: 0.309188574552536\n",
      "Subject 4, Epoch 194, Loss: 1.2306113541126251, Final Batch Loss: 0.36728909611701965\n",
      "Subject 4, Epoch 195, Loss: 1.131993681192398, Final Batch Loss: 0.3775120973587036\n",
      "Subject 4, Epoch 196, Loss: 1.1665502190589905, Final Batch Loss: 0.3653203547000885\n",
      "Subject 4, Epoch 197, Loss: 1.2134418189525604, Final Batch Loss: 0.366559773683548\n",
      "Subject 4, Epoch 198, Loss: 1.1208233833312988, Final Batch Loss: 0.32329633831977844\n",
      "Subject 4, Epoch 199, Loss: 1.1786207854747772, Final Batch Loss: 0.40079039335250854\n",
      "Subject 4, Epoch 200, Loss: 1.0200017392635345, Final Batch Loss: 0.2975007891654968\n",
      "Subject 4, Epoch 201, Loss: 1.0914675295352936, Final Batch Loss: 0.3311653435230255\n",
      "Subject 4, Epoch 202, Loss: 1.11202934384346, Final Batch Loss: 0.3238285779953003\n",
      "Subject 4, Epoch 203, Loss: 1.0576454997062683, Final Batch Loss: 0.3850743770599365\n",
      "Subject 4, Epoch 204, Loss: 1.215229868888855, Final Batch Loss: 0.4757933020591736\n",
      "Subject 4, Epoch 205, Loss: 1.1123751401901245, Final Batch Loss: 0.3682982325553894\n",
      "Subject 4, Epoch 206, Loss: 1.1637073755264282, Final Batch Loss: 0.4200788140296936\n",
      "Subject 4, Epoch 207, Loss: 1.141671746969223, Final Batch Loss: 0.3787662088871002\n",
      "Subject 4, Epoch 208, Loss: 1.0479162335395813, Final Batch Loss: 0.27628472447395325\n",
      "Subject 4, Epoch 209, Loss: 1.056330382823944, Final Batch Loss: 0.3875832259654999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 210, Loss: 1.1930714547634125, Final Batch Loss: 0.39397189021110535\n",
      "Subject 4, Epoch 211, Loss: 1.0680430233478546, Final Batch Loss: 0.2971636950969696\n",
      "Subject 4, Epoch 212, Loss: 1.0567472279071808, Final Batch Loss: 0.42621681094169617\n",
      "Subject 4, Epoch 213, Loss: 1.131161391735077, Final Batch Loss: 0.2951925992965698\n",
      "Subject 4, Epoch 214, Loss: 1.1174827218055725, Final Batch Loss: 0.3409314751625061\n",
      "Subject 4, Epoch 215, Loss: 1.0736117660999298, Final Batch Loss: 0.3589628338813782\n",
      "Subject 4, Epoch 216, Loss: 1.0477297902107239, Final Batch Loss: 0.3602704405784607\n",
      "Subject 4, Epoch 217, Loss: 1.045487642288208, Final Batch Loss: 0.32162564992904663\n",
      "Subject 4, Epoch 218, Loss: 1.1231374442577362, Final Batch Loss: 0.43411558866500854\n",
      "Subject 4, Epoch 219, Loss: 1.0389113426208496, Final Batch Loss: 0.37315818667411804\n",
      "Subject 4, Epoch 220, Loss: 1.119038999080658, Final Batch Loss: 0.3453322649002075\n",
      "Subject 4, Epoch 221, Loss: 1.1207723319530487, Final Batch Loss: 0.42160898447036743\n",
      "Subject 4, Epoch 222, Loss: 1.011608511209488, Final Batch Loss: 0.35389244556427\n",
      "Subject 4, Epoch 223, Loss: 1.1235587894916534, Final Batch Loss: 0.4266255497932434\n",
      "Subject 4, Epoch 224, Loss: 1.0427465438842773, Final Batch Loss: 0.3595001995563507\n",
      "Subject 4, Epoch 225, Loss: 0.9801608622074127, Final Batch Loss: 0.25137031078338623\n",
      "Subject 4, Epoch 226, Loss: 1.127714455127716, Final Batch Loss: 0.45197924971580505\n",
      "Subject 4, Epoch 227, Loss: 1.1672702431678772, Final Batch Loss: 0.4768902659416199\n",
      "Subject 4, Epoch 228, Loss: 1.0334289073944092, Final Batch Loss: 0.33689215779304504\n",
      "Subject 4, Epoch 229, Loss: 1.0271156132221222, Final Batch Loss: 0.3119095265865326\n",
      "Subject 4, Epoch 230, Loss: 0.9806761741638184, Final Batch Loss: 0.27383631467819214\n",
      "Subject 4, Epoch 231, Loss: 1.0658230185508728, Final Batch Loss: 0.43254584074020386\n",
      "Subject 4, Epoch 232, Loss: 1.1390741765499115, Final Batch Loss: 0.43978336453437805\n",
      "Subject 4, Epoch 233, Loss: 1.0656567513942719, Final Batch Loss: 0.40012022852897644\n",
      "Subject 4, Epoch 234, Loss: 1.0447644889354706, Final Batch Loss: 0.38192322850227356\n",
      "Subject 4, Epoch 235, Loss: 0.9617037177085876, Final Batch Loss: 0.3001817464828491\n",
      "Subject 4, Epoch 236, Loss: 1.0121157467365265, Final Batch Loss: 0.33524414896965027\n",
      "Subject 4, Epoch 237, Loss: 1.0613368451595306, Final Batch Loss: 0.38684770464897156\n",
      "Subject 4, Epoch 238, Loss: 1.093228816986084, Final Batch Loss: 0.46733176708221436\n",
      "Subject 4, Epoch 239, Loss: 1.0474068224430084, Final Batch Loss: 0.3769601285457611\n",
      "Subject 4, Epoch 240, Loss: 1.0921504497528076, Final Batch Loss: 0.39386460185050964\n",
      "Subject 4, Epoch 241, Loss: 0.9530661702156067, Final Batch Loss: 0.226480633020401\n",
      "Subject 4, Epoch 242, Loss: 1.088362693786621, Final Batch Loss: 0.4976951479911804\n",
      "Subject 4, Epoch 243, Loss: 1.0884145200252533, Final Batch Loss: 0.4286523163318634\n",
      "Subject 4, Epoch 244, Loss: 1.011224091053009, Final Batch Loss: 0.2923296093940735\n",
      "Subject 4, Epoch 245, Loss: 1.0046406686306, Final Batch Loss: 0.25498679280281067\n",
      "Subject 4, Epoch 246, Loss: 1.0694543421268463, Final Batch Loss: 0.3802887797355652\n",
      "Subject 4, Epoch 247, Loss: 1.0060724318027496, Final Batch Loss: 0.3140449523925781\n",
      "Subject 4, Epoch 248, Loss: 1.0416416227817535, Final Batch Loss: 0.3261202275753021\n",
      "Subject 4, Epoch 249, Loss: 0.9829285740852356, Final Batch Loss: 0.3452349007129669\n",
      "Subject 4, Epoch 250, Loss: 1.0979833602905273, Final Batch Loss: 0.34399157762527466\n",
      "Subject 4, Epoch 251, Loss: 0.9507850110530853, Final Batch Loss: 0.2481556534767151\n",
      "Subject 4, Epoch 252, Loss: 0.9305171370506287, Final Batch Loss: 0.28894028067588806\n",
      "Subject 4, Epoch 253, Loss: 1.1195783615112305, Final Batch Loss: 0.3524940013885498\n",
      "Subject 4, Epoch 254, Loss: 0.8381936997175217, Final Batch Loss: 0.2275577336549759\n",
      "Subject 4, Epoch 255, Loss: 1.0468158423900604, Final Batch Loss: 0.338803231716156\n",
      "Subject 4, Epoch 256, Loss: 0.9686086177825928, Final Batch Loss: 0.3658524453639984\n",
      "Subject 4, Epoch 257, Loss: 1.1254197359085083, Final Batch Loss: 0.4465969502925873\n",
      "Subject 4, Epoch 258, Loss: 1.1025772094726562, Final Batch Loss: 0.3645407557487488\n",
      "Subject 4, Epoch 259, Loss: 0.9358294457197189, Final Batch Loss: 0.19314362108707428\n",
      "Subject 4, Epoch 260, Loss: 1.0441260039806366, Final Batch Loss: 0.3627052903175354\n",
      "Subject 4, Epoch 261, Loss: 1.09970360994339, Final Batch Loss: 0.3914358615875244\n",
      "Subject 4, Epoch 262, Loss: 1.0190207660198212, Final Batch Loss: 0.30134469270706177\n",
      "Subject 4, Epoch 263, Loss: 0.9610405564308167, Final Batch Loss: 0.24794957041740417\n",
      "Subject 4, Epoch 264, Loss: 1.094202697277069, Final Batch Loss: 0.36552131175994873\n",
      "Subject 4, Epoch 265, Loss: 1.0805512964725494, Final Batch Loss: 0.3973928987979889\n",
      "Subject 4, Epoch 266, Loss: 1.0328125655651093, Final Batch Loss: 0.3483162224292755\n",
      "Subject 4, Epoch 267, Loss: 0.9007279574871063, Final Batch Loss: 0.2698214650154114\n",
      "Subject 4, Epoch 268, Loss: 0.9418227672576904, Final Batch Loss: 0.32785069942474365\n",
      "Subject 4, Epoch 269, Loss: 0.963447630405426, Final Batch Loss: 0.30865782499313354\n",
      "Subject 4, Epoch 270, Loss: 1.00989630818367, Final Batch Loss: 0.3389851450920105\n",
      "Subject 4, Epoch 271, Loss: 1.0050621330738068, Final Batch Loss: 0.3209456205368042\n",
      "Subject 4, Epoch 272, Loss: 0.9841560423374176, Final Batch Loss: 0.2659587860107422\n",
      "Subject 4, Epoch 273, Loss: 0.9322638511657715, Final Batch Loss: 0.3664281666278839\n",
      "Subject 4, Epoch 274, Loss: 0.8648593425750732, Final Batch Loss: 0.2627373933792114\n",
      "Subject 4, Epoch 275, Loss: 0.975334495306015, Final Batch Loss: 0.27339616417884827\n",
      "Subject 4, Epoch 276, Loss: 0.9416114091873169, Final Batch Loss: 0.24523097276687622\n",
      "Subject 4, Epoch 277, Loss: 1.025310069322586, Final Batch Loss: 0.3920588791370392\n",
      "Subject 4, Epoch 278, Loss: 0.9637906849384308, Final Batch Loss: 0.24305075407028198\n",
      "Subject 4, Epoch 279, Loss: 0.9727081060409546, Final Batch Loss: 0.33279597759246826\n",
      "Subject 4, Epoch 280, Loss: 0.904029130935669, Final Batch Loss: 0.2567007541656494\n",
      "Subject 4, Epoch 281, Loss: 0.9779835194349289, Final Batch Loss: 0.4149209260940552\n",
      "Subject 4, Epoch 282, Loss: 0.8474003970623016, Final Batch Loss: 0.1943591833114624\n",
      "Subject 4, Epoch 283, Loss: 1.0795741081237793, Final Batch Loss: 0.4586716294288635\n",
      "Subject 4, Epoch 284, Loss: 0.9201757609844208, Final Batch Loss: 0.29279664158821106\n",
      "Subject 4, Epoch 285, Loss: 0.9023775160312653, Final Batch Loss: 0.32134348154067993\n",
      "Subject 4, Epoch 286, Loss: 0.9283857941627502, Final Batch Loss: 0.31151390075683594\n",
      "Subject 4, Epoch 287, Loss: 0.8929031789302826, Final Batch Loss: 0.2819614112377167\n",
      "Subject 4, Epoch 288, Loss: 1.033374935388565, Final Batch Loss: 0.4132331907749176\n",
      "Subject 4, Epoch 289, Loss: 0.9368259608745575, Final Batch Loss: 0.2954431176185608\n",
      "Subject 4, Epoch 290, Loss: 0.921288251876831, Final Batch Loss: 0.3162758946418762\n",
      "Subject 4, Epoch 291, Loss: 0.871954545378685, Final Batch Loss: 0.30296745896339417\n",
      "Subject 4, Epoch 292, Loss: 0.9286208152770996, Final Batch Loss: 0.3263969421386719\n",
      "Subject 4, Epoch 293, Loss: 0.9325033724308014, Final Batch Loss: 0.25923651456832886\n",
      "Subject 4, Epoch 294, Loss: 0.883465588092804, Final Batch Loss: 0.31484997272491455\n",
      "Subject 4, Epoch 295, Loss: 0.9418059289455414, Final Batch Loss: 0.2632158100605011\n",
      "Subject 4, Epoch 296, Loss: 0.8524243533611298, Final Batch Loss: 0.21944430470466614\n",
      "Subject 4, Epoch 297, Loss: 0.9894051849842072, Final Batch Loss: 0.29193440079689026\n",
      "Subject 4, Epoch 298, Loss: 0.938436359167099, Final Batch Loss: 0.2829003930091858\n",
      "Subject 4, Epoch 299, Loss: 0.9269566237926483, Final Batch Loss: 0.30896100401878357\n",
      "Subject 4, Epoch 300, Loss: 1.0222158432006836, Final Batch Loss: 0.4204932153224945\n",
      "Subject 4, Epoch 301, Loss: 0.9424072355031967, Final Batch Loss: 0.244045689702034\n",
      "Subject 4, Epoch 302, Loss: 0.960608720779419, Final Batch Loss: 0.3727755844593048\n",
      "Subject 4, Epoch 303, Loss: 0.9361052811145782, Final Batch Loss: 0.37707892060279846\n",
      "Subject 4, Epoch 304, Loss: 1.0002733767032623, Final Batch Loss: 0.30884236097335815\n",
      "Subject 4, Epoch 305, Loss: 0.9746321737766266, Final Batch Loss: 0.31776684522628784\n",
      "Subject 4, Epoch 306, Loss: 1.0558007955551147, Final Batch Loss: 0.3624601662158966\n",
      "Subject 4, Epoch 307, Loss: 0.9297078251838684, Final Batch Loss: 0.30795007944107056\n",
      "Subject 4, Epoch 308, Loss: 0.9172979891300201, Final Batch Loss: 0.3396345376968384\n",
      "Subject 4, Epoch 309, Loss: 0.8665425181388855, Final Batch Loss: 0.26314643025398254\n",
      "Subject 4, Epoch 310, Loss: 0.8799839913845062, Final Batch Loss: 0.2792660892009735\n",
      "Subject 4, Epoch 311, Loss: 0.93809375166893, Final Batch Loss: 0.36419954895973206\n",
      "Subject 4, Epoch 312, Loss: 0.9335350096225739, Final Batch Loss: 0.31069549918174744\n",
      "Subject 4, Epoch 313, Loss: 0.9185188412666321, Final Batch Loss: 0.36412322521209717\n",
      "Subject 4, Epoch 314, Loss: 0.9341106414794922, Final Batch Loss: 0.36013877391815186\n",
      "Subject 4, Epoch 315, Loss: 0.8135883808135986, Final Batch Loss: 0.27744460105895996\n",
      "Subject 4, Epoch 316, Loss: 0.9918757677078247, Final Batch Loss: 0.37567299604415894\n",
      "Subject 4, Epoch 317, Loss: 0.9292877018451691, Final Batch Loss: 0.39989206194877625\n",
      "Subject 4, Epoch 318, Loss: 0.870543897151947, Final Batch Loss: 0.2505171000957489\n",
      "Subject 4, Epoch 319, Loss: 0.9269227981567383, Final Batch Loss: 0.31349292397499084\n",
      "Subject 4, Epoch 320, Loss: 0.8856036067008972, Final Batch Loss: 0.27531009912490845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 321, Loss: 0.8270277082920074, Final Batch Loss: 0.27588146924972534\n",
      "Subject 4, Epoch 322, Loss: 1.0412187576293945, Final Batch Loss: 0.40353578329086304\n",
      "Subject 4, Epoch 323, Loss: 0.9428098499774933, Final Batch Loss: 0.2619909942150116\n",
      "Subject 4, Epoch 324, Loss: 0.88147833943367, Final Batch Loss: 0.2588825821876526\n",
      "Subject 4, Epoch 325, Loss: 0.9322119057178497, Final Batch Loss: 0.3086077570915222\n",
      "Subject 4, Epoch 326, Loss: 0.836638331413269, Final Batch Loss: 0.23704394698143005\n",
      "Subject 4, Epoch 327, Loss: 0.8738545775413513, Final Batch Loss: 0.3016403615474701\n",
      "Subject 4, Epoch 328, Loss: 0.8927843272686005, Final Batch Loss: 0.26313188672065735\n",
      "Subject 4, Epoch 329, Loss: 0.817940965294838, Final Batch Loss: 0.21788455545902252\n",
      "Subject 4, Epoch 330, Loss: 0.8670164048671722, Final Batch Loss: 0.29546821117401123\n",
      "Subject 4, Epoch 331, Loss: 0.9120708107948303, Final Batch Loss: 0.31132110953330994\n",
      "Subject 4, Epoch 332, Loss: 0.852847158908844, Final Batch Loss: 0.2607731819152832\n",
      "Subject 4, Epoch 333, Loss: 0.9057771265506744, Final Batch Loss: 0.2705019414424896\n",
      "Subject 4, Epoch 334, Loss: 0.8417118787765503, Final Batch Loss: 0.2657720744609833\n",
      "Subject 4, Epoch 335, Loss: 0.8092168718576431, Final Batch Loss: 0.2634085714817047\n",
      "Subject 4, Epoch 336, Loss: 0.8079754263162613, Final Batch Loss: 0.2350931614637375\n",
      "Subject 4, Epoch 337, Loss: 0.8317072987556458, Final Batch Loss: 0.2650107145309448\n",
      "Subject 4, Epoch 338, Loss: 0.8415830582380295, Final Batch Loss: 0.3113621175289154\n",
      "Subject 4, Epoch 339, Loss: 0.8992518186569214, Final Batch Loss: 0.2762526273727417\n",
      "Subject 4, Epoch 340, Loss: 0.8678456246852875, Final Batch Loss: 0.28238674998283386\n",
      "Subject 4, Epoch 341, Loss: 0.8405139297246933, Final Batch Loss: 0.2495715171098709\n",
      "Subject 4, Epoch 342, Loss: 0.9447507560253143, Final Batch Loss: 0.39051178097724915\n",
      "Subject 4, Epoch 343, Loss: 0.8669333159923553, Final Batch Loss: 0.27775660157203674\n",
      "Subject 4, Epoch 344, Loss: 0.9194632768630981, Final Batch Loss: 0.34585466980934143\n",
      "Subject 4, Epoch 345, Loss: 0.802922785282135, Final Batch Loss: 0.26525384187698364\n",
      "Subject 4, Epoch 346, Loss: 0.8557155430316925, Final Batch Loss: 0.2758122682571411\n",
      "Subject 4, Epoch 347, Loss: 1.0056209564208984, Final Batch Loss: 0.33445465564727783\n",
      "Subject 4, Epoch 348, Loss: 0.7761577367782593, Final Batch Loss: 0.1887783706188202\n",
      "Subject 4, Epoch 349, Loss: 0.8428802788257599, Final Batch Loss: 0.22863945364952087\n",
      "Subject 4, Epoch 350, Loss: 0.9063462018966675, Final Batch Loss: 0.3146477937698364\n",
      "Subject 4, Epoch 351, Loss: 0.8814282417297363, Final Batch Loss: 0.3057916760444641\n",
      "Subject 4, Epoch 352, Loss: 0.8771447837352753, Final Batch Loss: 0.3165103793144226\n",
      "Subject 4, Epoch 353, Loss: 0.7948575466871262, Final Batch Loss: 0.24372871220111847\n",
      "Subject 4, Epoch 354, Loss: 0.8313624262809753, Final Batch Loss: 0.25677958130836487\n",
      "Subject 4, Epoch 355, Loss: 0.8286649733781815, Final Batch Loss: 0.3260684609413147\n",
      "Subject 4, Epoch 356, Loss: 0.8475881665945053, Final Batch Loss: 0.3138810098171234\n",
      "Subject 4, Epoch 357, Loss: 0.8893531858921051, Final Batch Loss: 0.3443351089954376\n",
      "Subject 4, Epoch 358, Loss: 0.8270500004291534, Final Batch Loss: 0.27260497212409973\n",
      "Subject 4, Epoch 359, Loss: 0.89446821808815, Final Batch Loss: 0.32136449217796326\n",
      "Subject 4, Epoch 360, Loss: 0.8195605874061584, Final Batch Loss: 0.2810123860836029\n",
      "Subject 4, Epoch 361, Loss: 0.838348314166069, Final Batch Loss: 0.3243904411792755\n",
      "Subject 4, Epoch 362, Loss: 0.8537668585777283, Final Batch Loss: 0.32495665550231934\n",
      "Subject 4, Epoch 363, Loss: 0.8945969939231873, Final Batch Loss: 0.3266065716743469\n",
      "Subject 4, Epoch 364, Loss: 0.8610948175191879, Final Batch Loss: 0.23487426340579987\n",
      "Subject 4, Epoch 365, Loss: 0.8152354657649994, Final Batch Loss: 0.2801513373851776\n",
      "Subject 4, Epoch 366, Loss: 0.8706090152263641, Final Batch Loss: 0.3009154200553894\n",
      "Subject 4, Epoch 367, Loss: 0.8856378197669983, Final Batch Loss: 0.3216702342033386\n",
      "Subject 4, Epoch 368, Loss: 0.8536317497491837, Final Batch Loss: 0.24022315442562103\n",
      "Subject 4, Epoch 369, Loss: 0.7782484740018845, Final Batch Loss: 0.2145010083913803\n",
      "Subject 4, Epoch 370, Loss: 0.8860994130373001, Final Batch Loss: 0.37098124623298645\n",
      "Subject 4, Epoch 371, Loss: 0.7769396156072617, Final Batch Loss: 0.23460260033607483\n",
      "Subject 4, Epoch 372, Loss: 0.8547088205814362, Final Batch Loss: 0.34804201126098633\n",
      "Subject 4, Epoch 373, Loss: 0.8376479744911194, Final Batch Loss: 0.22642973065376282\n",
      "Subject 4, Epoch 374, Loss: 0.8360637128353119, Final Batch Loss: 0.3218684494495392\n",
      "Subject 4, Epoch 375, Loss: 0.9047167897224426, Final Batch Loss: 0.2907794117927551\n",
      "Subject 4, Epoch 376, Loss: 0.8668707609176636, Final Batch Loss: 0.2445889711380005\n",
      "Subject 4, Epoch 377, Loss: 0.8574448525905609, Final Batch Loss: 0.3221759498119354\n",
      "Subject 4, Epoch 378, Loss: 1.0169028341770172, Final Batch Loss: 0.39259636402130127\n",
      "Subject 4, Epoch 379, Loss: 0.8475742191076279, Final Batch Loss: 0.31340688467025757\n",
      "Subject 4, Epoch 380, Loss: 0.8675571978092194, Final Batch Loss: 0.31161946058273315\n",
      "Subject 4, Epoch 381, Loss: 0.8002599030733109, Final Batch Loss: 0.3175172507762909\n",
      "Subject 4, Epoch 382, Loss: 0.7655210047960281, Final Batch Loss: 0.2247397005558014\n",
      "Subject 4, Epoch 383, Loss: 0.8821865022182465, Final Batch Loss: 0.3211155831813812\n",
      "Subject 4, Epoch 384, Loss: 0.8831405639648438, Final Batch Loss: 0.3220488131046295\n",
      "Subject 4, Epoch 385, Loss: 0.8311782479286194, Final Batch Loss: 0.24762192368507385\n",
      "Subject 4, Epoch 386, Loss: 0.9964228272438049, Final Batch Loss: 0.41262951493263245\n",
      "Subject 4, Epoch 387, Loss: 0.9037501066923141, Final Batch Loss: 0.3898848593235016\n",
      "Subject 4, Epoch 388, Loss: 0.8354983627796173, Final Batch Loss: 0.3581112325191498\n",
      "Subject 4, Epoch 389, Loss: 0.8859066367149353, Final Batch Loss: 0.284633994102478\n",
      "Subject 4, Epoch 390, Loss: 0.7977049946784973, Final Batch Loss: 0.3011677861213684\n",
      "Subject 4, Epoch 391, Loss: 0.8054805397987366, Final Batch Loss: 0.2834290862083435\n",
      "Subject 4, Epoch 392, Loss: 0.8560899049043655, Final Batch Loss: 0.29573312401771545\n",
      "Subject 4, Epoch 393, Loss: 0.9476669579744339, Final Batch Loss: 0.41008761525154114\n",
      "Subject 4, Epoch 394, Loss: 0.8143769204616547, Final Batch Loss: 0.31331339478492737\n",
      "Subject 4, Epoch 395, Loss: 0.8266412913799286, Final Batch Loss: 0.28581568598747253\n",
      "Subject 4, Epoch 396, Loss: 0.9040805101394653, Final Batch Loss: 0.28543540835380554\n",
      "Subject 4, Epoch 397, Loss: 0.8219238966703415, Final Batch Loss: 0.22974462807178497\n",
      "Subject 4, Epoch 398, Loss: 0.8231614232063293, Final Batch Loss: 0.29974785447120667\n",
      "Subject 4, Epoch 399, Loss: 0.879893884062767, Final Batch Loss: 0.3390757143497467\n",
      "Subject 4, Epoch 400, Loss: 0.7635136395692825, Final Batch Loss: 0.21604974567890167\n",
      "Subject 4, Epoch 401, Loss: 0.8487178385257721, Final Batch Loss: 0.3043602705001831\n",
      "Subject 4, Epoch 402, Loss: 0.8608705848455429, Final Batch Loss: 0.2507857382297516\n",
      "Subject 4, Epoch 403, Loss: 0.9860549569129944, Final Batch Loss: 0.4082736074924469\n",
      "Subject 4, Epoch 404, Loss: 0.781057819724083, Final Batch Loss: 0.21219150722026825\n",
      "Subject 4, Epoch 405, Loss: 0.7375223785638809, Final Batch Loss: 0.2400026172399521\n",
      "Subject 4, Epoch 406, Loss: 0.8080240935087204, Final Batch Loss: 0.21529294550418854\n",
      "Subject 4, Epoch 407, Loss: 0.7875786423683167, Final Batch Loss: 0.23773561418056488\n",
      "Subject 4, Epoch 408, Loss: 0.8518882393836975, Final Batch Loss: 0.2772160768508911\n",
      "Subject 4, Epoch 409, Loss: 0.8000767827033997, Final Batch Loss: 0.2730439603328705\n",
      "Subject 4, Epoch 410, Loss: 0.745475634932518, Final Batch Loss: 0.23253025114536285\n",
      "Subject 4, Epoch 411, Loss: 0.7870526313781738, Final Batch Loss: 0.2859405279159546\n",
      "Subject 4, Epoch 412, Loss: 0.8974208235740662, Final Batch Loss: 0.2801298499107361\n",
      "Subject 4, Epoch 413, Loss: 0.8268652558326721, Final Batch Loss: 0.31209564208984375\n",
      "Subject 4, Epoch 414, Loss: 0.8447068482637405, Final Batch Loss: 0.30849796533584595\n",
      "Subject 4, Epoch 415, Loss: 0.8305651545524597, Final Batch Loss: 0.24590793251991272\n",
      "Subject 4, Epoch 416, Loss: 0.7876471430063248, Final Batch Loss: 0.2745268642902374\n",
      "Subject 4, Epoch 417, Loss: 0.9859471768140793, Final Batch Loss: 0.4789879322052002\n",
      "Subject 4, Epoch 418, Loss: 0.920789510011673, Final Batch Loss: 0.4093225598335266\n",
      "Subject 4, Epoch 419, Loss: 0.72468201816082, Final Batch Loss: 0.16630595922470093\n",
      "Subject 4, Epoch 420, Loss: 0.8292843997478485, Final Batch Loss: 0.23736503720283508\n",
      "Subject 4, Epoch 421, Loss: 0.7801514565944672, Final Batch Loss: 0.24840226769447327\n",
      "Subject 4, Epoch 422, Loss: 0.9249938726425171, Final Batch Loss: 0.32181867957115173\n",
      "Subject 4, Epoch 423, Loss: 0.8451204895973206, Final Batch Loss: 0.2776884138584137\n",
      "Subject 4, Epoch 424, Loss: 0.8577795922756195, Final Batch Loss: 0.29103606939315796\n",
      "Subject 4, Epoch 425, Loss: 0.821193054318428, Final Batch Loss: 0.2526699900627136\n",
      "Subject 4, Epoch 426, Loss: 0.7881501317024231, Final Batch Loss: 0.28370293974876404\n",
      "Subject 4, Epoch 427, Loss: 0.867335632443428, Final Batch Loss: 0.22269447147846222\n",
      "Subject 4, Epoch 428, Loss: 0.7324198484420776, Final Batch Loss: 0.2154550403356552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 429, Loss: 0.8218741416931152, Final Batch Loss: 0.3001876473426819\n",
      "Subject 4, Epoch 430, Loss: 0.7322515547275543, Final Batch Loss: 0.21715077757835388\n",
      "Subject 4, Epoch 431, Loss: 0.7801958620548248, Final Batch Loss: 0.2339029610157013\n",
      "Subject 4, Epoch 432, Loss: 0.7929746806621552, Final Batch Loss: 0.28153449296951294\n",
      "Subject 4, Epoch 433, Loss: 0.8233439773321152, Final Batch Loss: 0.3435673415660858\n",
      "Subject 4, Epoch 434, Loss: 0.818113386631012, Final Batch Loss: 0.29543808102607727\n",
      "Subject 4, Epoch 435, Loss: 0.7864002883434296, Final Batch Loss: 0.26296529173851013\n",
      "Subject 4, Epoch 436, Loss: 0.8845538944005966, Final Batch Loss: 0.3398216962814331\n",
      "Subject 4, Epoch 437, Loss: 0.8092141598463058, Final Batch Loss: 0.361905574798584\n",
      "Subject 4, Epoch 438, Loss: 0.7166139632463455, Final Batch Loss: 0.2140185534954071\n",
      "Subject 4, Epoch 439, Loss: 0.7531174868345261, Final Batch Loss: 0.22020776569843292\n",
      "Subject 4, Epoch 440, Loss: 0.821771040558815, Final Batch Loss: 0.3136148154735565\n",
      "Subject 4, Epoch 441, Loss: 0.8046557605266571, Final Batch Loss: 0.3319189250469208\n",
      "Subject 4, Epoch 442, Loss: 0.7130674719810486, Final Batch Loss: 0.244004487991333\n",
      "Subject 4, Epoch 443, Loss: 0.7719766944646835, Final Batch Loss: 0.27076223492622375\n",
      "Subject 4, Epoch 444, Loss: 0.7533828020095825, Final Batch Loss: 0.2446451485157013\n",
      "Subject 4, Epoch 445, Loss: 0.7898509949445724, Final Batch Loss: 0.24906998872756958\n",
      "Subject 4, Epoch 446, Loss: 0.8403349965810776, Final Batch Loss: 0.22919894754886627\n",
      "Subject 4, Epoch 447, Loss: 0.8716989159584045, Final Batch Loss: 0.341498464345932\n",
      "Subject 4, Epoch 448, Loss: 0.7870755791664124, Final Batch Loss: 0.23841074109077454\n",
      "Subject 4, Epoch 449, Loss: 0.950753316283226, Final Batch Loss: 0.4647451639175415\n",
      "Subject 4, Epoch 450, Loss: 0.7720828354358673, Final Batch Loss: 0.21226021647453308\n",
      "Subject 4, Epoch 451, Loss: 0.7944183051586151, Final Batch Loss: 0.28484994173049927\n",
      "Subject 4, Epoch 452, Loss: 0.7915586084127426, Final Batch Loss: 0.27033543586730957\n",
      "Subject 4, Epoch 453, Loss: 0.9078554511070251, Final Batch Loss: 0.26030078530311584\n",
      "Subject 4, Epoch 454, Loss: 0.7679415643215179, Final Batch Loss: 0.17735722661018372\n",
      "Subject 4, Epoch 455, Loss: 0.823491245508194, Final Batch Loss: 0.2430049479007721\n",
      "Subject 4, Epoch 456, Loss: 0.7801670432090759, Final Batch Loss: 0.24469828605651855\n",
      "Subject 4, Epoch 457, Loss: 0.7196947634220123, Final Batch Loss: 0.2546030282974243\n",
      "Subject 4, Epoch 458, Loss: 0.7472320944070816, Final Batch Loss: 0.2633195221424103\n",
      "Subject 4, Epoch 459, Loss: 0.9160223007202148, Final Batch Loss: 0.36150267720222473\n",
      "Subject 4, Epoch 460, Loss: 0.697819322347641, Final Batch Loss: 0.2442573457956314\n",
      "Subject 4, Epoch 461, Loss: 0.7533543258905411, Final Batch Loss: 0.28911086916923523\n",
      "Subject 4, Epoch 462, Loss: 0.7063532918691635, Final Batch Loss: 0.15658247470855713\n",
      "Subject 4, Epoch 463, Loss: 0.7987415194511414, Final Batch Loss: 0.2621982991695404\n",
      "Subject 4, Epoch 464, Loss: 0.7883763164281845, Final Batch Loss: 0.24227555096149445\n",
      "Subject 4, Epoch 465, Loss: 0.6668346971273422, Final Batch Loss: 0.22087952494621277\n",
      "Subject 4, Epoch 466, Loss: 0.7738304436206818, Final Batch Loss: 0.26246678829193115\n",
      "Subject 4, Epoch 467, Loss: 0.7032511979341507, Final Batch Loss: 0.11786310374736786\n",
      "Subject 4, Epoch 468, Loss: 0.734579399228096, Final Batch Loss: 0.20591537654399872\n",
      "Subject 4, Epoch 469, Loss: 0.8219747096300125, Final Batch Loss: 0.2554154396057129\n",
      "Subject 4, Epoch 470, Loss: 0.7467192113399506, Final Batch Loss: 0.29338759183883667\n",
      "Subject 4, Epoch 471, Loss: 0.8354390561580658, Final Batch Loss: 0.30132728815078735\n",
      "Subject 4, Epoch 472, Loss: 0.7640540450811386, Final Batch Loss: 0.2815781235694885\n",
      "Subject 4, Epoch 473, Loss: 0.7864125669002533, Final Batch Loss: 0.28455427289009094\n",
      "Subject 4, Epoch 474, Loss: 0.7676385939121246, Final Batch Loss: 0.23949836194515228\n",
      "Subject 4, Epoch 475, Loss: 0.7416234761476517, Final Batch Loss: 0.22562181949615479\n",
      "Subject 4, Epoch 476, Loss: 0.7509278655052185, Final Batch Loss: 0.2432226985692978\n",
      "Subject 4, Epoch 477, Loss: 0.7236811220645905, Final Batch Loss: 0.23691685497760773\n",
      "Subject 4, Epoch 478, Loss: 0.7854338884353638, Final Batch Loss: 0.18411734700202942\n",
      "Subject 4, Epoch 479, Loss: 0.7088364064693451, Final Batch Loss: 0.2157304733991623\n",
      "Subject 4, Epoch 480, Loss: 0.7478467375040054, Final Batch Loss: 0.225675567984581\n",
      "Subject 4, Epoch 481, Loss: 0.7076391577720642, Final Batch Loss: 0.25615522265434265\n",
      "Subject 4, Epoch 482, Loss: 0.7672709971666336, Final Batch Loss: 0.21789030730724335\n",
      "Subject 4, Epoch 483, Loss: 0.7181989103555679, Final Batch Loss: 0.24262820184230804\n",
      "Subject 4, Epoch 484, Loss: 0.711742103099823, Final Batch Loss: 0.23474770784378052\n",
      "Subject 4, Epoch 485, Loss: 0.7236104309558868, Final Batch Loss: 0.24093344807624817\n",
      "Subject 4, Epoch 486, Loss: 0.7714813202619553, Final Batch Loss: 0.3003806173801422\n",
      "Subject 4, Epoch 487, Loss: 0.7886759042739868, Final Batch Loss: 0.2856472432613373\n",
      "Subject 4, Epoch 488, Loss: 0.7594675868749619, Final Batch Loss: 0.2372003048658371\n",
      "Subject 4, Epoch 489, Loss: 0.7180888801813126, Final Batch Loss: 0.20136582851409912\n",
      "Subject 4, Epoch 490, Loss: 0.6914883255958557, Final Batch Loss: 0.22946952283382416\n",
      "Subject 4, Epoch 491, Loss: 0.6884602010250092, Final Batch Loss: 0.17034906148910522\n",
      "Subject 4, Epoch 492, Loss: 0.776753306388855, Final Batch Loss: 0.19108101725578308\n",
      "Subject 4, Epoch 493, Loss: 0.7307848185300827, Final Batch Loss: 0.2626389265060425\n",
      "Subject 4, Epoch 494, Loss: 0.8476576507091522, Final Batch Loss: 0.33097338676452637\n",
      "Subject 4, Epoch 495, Loss: 0.6296478360891342, Final Batch Loss: 0.16919907927513123\n",
      "Subject 4, Epoch 496, Loss: 0.6811412125825882, Final Batch Loss: 0.17179451882839203\n",
      "Subject 4, Epoch 497, Loss: 0.7460518926382065, Final Batch Loss: 0.2473883330821991\n",
      "Subject 4, Epoch 498, Loss: 0.7039800882339478, Final Batch Loss: 0.28935739398002625\n",
      "Subject 4, Epoch 499, Loss: 0.694007620215416, Final Batch Loss: 0.23732155561447144\n",
      "Subject 4, Epoch 500, Loss: 0.69877989590168, Final Batch Loss: 0.2626539468765259\n",
      "Subject 4, Epoch 501, Loss: 0.676978588104248, Final Batch Loss: 0.2401023507118225\n",
      "Subject 4, Epoch 502, Loss: 0.6929723471403122, Final Batch Loss: 0.2685602903366089\n",
      "Subject 4, Epoch 503, Loss: 0.6565933525562286, Final Batch Loss: 0.179977685213089\n",
      "Subject 4, Epoch 504, Loss: 0.7590833157300949, Final Batch Loss: 0.2214285135269165\n",
      "Subject 4, Epoch 505, Loss: 0.8093793094158173, Final Batch Loss: 0.23938705027103424\n",
      "Subject 4, Epoch 506, Loss: 0.6672004908323288, Final Batch Loss: 0.273162305355072\n",
      "Subject 4, Epoch 507, Loss: 0.7051919400691986, Final Batch Loss: 0.3179185688495636\n",
      "Subject 4, Epoch 508, Loss: 0.608394205570221, Final Batch Loss: 0.20630353689193726\n",
      "Subject 4, Epoch 509, Loss: 0.5843518674373627, Final Batch Loss: 0.19657433032989502\n",
      "Subject 4, Epoch 510, Loss: 0.6155944168567657, Final Batch Loss: 0.16008542478084564\n",
      "Subject 4, Epoch 511, Loss: 0.6270863264799118, Final Batch Loss: 0.16691137850284576\n",
      "Subject 4, Epoch 512, Loss: 0.6914327442646027, Final Batch Loss: 0.20459485054016113\n",
      "Subject 4, Epoch 513, Loss: 0.6290074735879898, Final Batch Loss: 0.17664727568626404\n",
      "Subject 4, Epoch 514, Loss: 0.6119294464588165, Final Batch Loss: 0.25336506962776184\n",
      "Subject 4, Epoch 515, Loss: 0.6621734201908112, Final Batch Loss: 0.17601227760314941\n",
      "Subject 4, Epoch 516, Loss: 0.6144856512546539, Final Batch Loss: 0.2213643193244934\n",
      "Subject 4, Epoch 517, Loss: 0.6943381428718567, Final Batch Loss: 0.24206149578094482\n",
      "Subject 4, Epoch 518, Loss: 0.660149335861206, Final Batch Loss: 0.17207854986190796\n",
      "Subject 4, Epoch 519, Loss: 0.6099483519792557, Final Batch Loss: 0.17530423402786255\n",
      "Subject 4, Epoch 520, Loss: 0.65524622797966, Final Batch Loss: 0.24065753817558289\n",
      "Subject 4, Epoch 521, Loss: 0.5897842794656754, Final Batch Loss: 0.186576709151268\n",
      "Subject 4, Epoch 522, Loss: 0.7470538467168808, Final Batch Loss: 0.2870756983757019\n",
      "Subject 4, Epoch 523, Loss: 0.6753976196050644, Final Batch Loss: 0.27588170766830444\n",
      "Subject 4, Epoch 524, Loss: 0.64511838555336, Final Batch Loss: 0.1452733278274536\n",
      "Subject 4, Epoch 525, Loss: 0.6359545141458511, Final Batch Loss: 0.30551984906196594\n",
      "Subject 4, Epoch 526, Loss: 0.5896419733762741, Final Batch Loss: 0.18793104588985443\n",
      "Subject 4, Epoch 527, Loss: 0.658970519900322, Final Batch Loss: 0.19727718830108643\n",
      "Subject 4, Epoch 528, Loss: 0.6879317611455917, Final Batch Loss: 0.23008671402931213\n",
      "Subject 4, Epoch 529, Loss: 0.5593025535345078, Final Batch Loss: 0.18686656653881073\n",
      "Subject 4, Epoch 530, Loss: 0.6370663493871689, Final Batch Loss: 0.2662171423435211\n",
      "Subject 4, Epoch 531, Loss: 0.5248643606901169, Final Batch Loss: 0.1777888536453247\n",
      "Subject 4, Epoch 532, Loss: 0.5540981590747833, Final Batch Loss: 0.16745389997959137\n",
      "Subject 4, Epoch 533, Loss: 0.5906319618225098, Final Batch Loss: 0.20364783704280853\n",
      "Subject 4, Epoch 534, Loss: 0.5776472687721252, Final Batch Loss: 0.14940926432609558\n",
      "Subject 4, Epoch 535, Loss: 0.602133259177208, Final Batch Loss: 0.2167578786611557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 536, Loss: 0.5879604369401932, Final Batch Loss: 0.19090579450130463\n",
      "Subject 4, Epoch 537, Loss: 0.5235386192798615, Final Batch Loss: 0.14844629168510437\n",
      "Subject 4, Epoch 538, Loss: 0.5886693596839905, Final Batch Loss: 0.16542398929595947\n",
      "Subject 4, Epoch 539, Loss: 0.5515864789485931, Final Batch Loss: 0.16694726049900055\n",
      "Subject 4, Epoch 540, Loss: 0.520051546394825, Final Batch Loss: 0.1474616676568985\n",
      "Subject 4, Epoch 541, Loss: 0.7003505676984787, Final Batch Loss: 0.2334434539079666\n",
      "Subject 4, Epoch 542, Loss: 0.6112045794725418, Final Batch Loss: 0.2184033989906311\n",
      "Subject 4, Epoch 543, Loss: 0.6575708538293839, Final Batch Loss: 0.18679431080818176\n",
      "Subject 4, Epoch 544, Loss: 0.6411882489919662, Final Batch Loss: 0.23196355998516083\n",
      "Subject 4, Epoch 545, Loss: 0.6320666372776031, Final Batch Loss: 0.24980983138084412\n",
      "Subject 4, Epoch 546, Loss: 0.5557175874710083, Final Batch Loss: 0.15523232519626617\n",
      "Subject 4, Epoch 547, Loss: 0.5511162728071213, Final Batch Loss: 0.20610357820987701\n",
      "Subject 4, Epoch 548, Loss: 0.5378313511610031, Final Batch Loss: 0.1703999936580658\n",
      "Subject 4, Epoch 549, Loss: 0.5816878080368042, Final Batch Loss: 0.10509583353996277\n",
      "Subject 4, Epoch 550, Loss: 0.6430283933877945, Final Batch Loss: 0.19640964269638062\n",
      "Subject 4, Epoch 551, Loss: 0.4938778132200241, Final Batch Loss: 0.15766175091266632\n",
      "Subject 4, Epoch 552, Loss: 0.5189662128686905, Final Batch Loss: 0.16186271607875824\n",
      "Subject 4, Epoch 553, Loss: 0.617918848991394, Final Batch Loss: 0.24701623618602753\n",
      "Subject 4, Epoch 554, Loss: 0.727720320224762, Final Batch Loss: 0.30475935339927673\n",
      "Subject 4, Epoch 555, Loss: 0.5029515847563744, Final Batch Loss: 0.11864209920167923\n",
      "Subject 4, Epoch 556, Loss: 0.5452563315629959, Final Batch Loss: 0.2538459002971649\n",
      "Subject 4, Epoch 557, Loss: 0.42036072909832, Final Batch Loss: 0.08254604041576385\n",
      "Subject 4, Epoch 558, Loss: 0.6275426596403122, Final Batch Loss: 0.24563579261302948\n",
      "Subject 4, Epoch 559, Loss: 0.47580402344465256, Final Batch Loss: 0.10998594015836716\n",
      "Subject 4, Epoch 560, Loss: 0.531930685043335, Final Batch Loss: 0.16257265210151672\n",
      "Subject 4, Epoch 561, Loss: 0.4258456900715828, Final Batch Loss: 0.1585029512643814\n",
      "Subject 4, Epoch 562, Loss: 0.5188842266798019, Final Batch Loss: 0.19367378950119019\n",
      "Subject 4, Epoch 563, Loss: 0.5786718279123306, Final Batch Loss: 0.19920577108860016\n",
      "Subject 4, Epoch 564, Loss: 0.5623533725738525, Final Batch Loss: 0.2050456553697586\n",
      "Subject 4, Epoch 565, Loss: 0.5453177839517593, Final Batch Loss: 0.15996451675891876\n",
      "Subject 4, Epoch 566, Loss: 0.5389488488435745, Final Batch Loss: 0.21437828242778778\n",
      "Subject 4, Epoch 567, Loss: 0.5787148475646973, Final Batch Loss: 0.18104252219200134\n",
      "Subject 4, Epoch 568, Loss: 0.5128609836101532, Final Batch Loss: 0.1590438187122345\n",
      "Subject 4, Epoch 569, Loss: 0.58608578145504, Final Batch Loss: 0.22972184419631958\n",
      "Subject 4, Epoch 570, Loss: 0.5161948353052139, Final Batch Loss: 0.1325797438621521\n",
      "Subject 4, Epoch 571, Loss: 0.47605057060718536, Final Batch Loss: 0.2125476896762848\n",
      "Subject 4, Epoch 572, Loss: 0.42470138892531395, Final Batch Loss: 0.060701485723257065\n",
      "Subject 4, Epoch 573, Loss: 0.6530794948339462, Final Batch Loss: 0.22888293862342834\n",
      "Subject 4, Epoch 574, Loss: 0.5284400433301926, Final Batch Loss: 0.22725015878677368\n",
      "Subject 4, Epoch 575, Loss: 0.5698978155851364, Final Batch Loss: 0.2724078893661499\n",
      "Subject 4, Epoch 576, Loss: 0.5118011385202408, Final Batch Loss: 0.11687377095222473\n",
      "Subject 4, Epoch 577, Loss: 0.4497286528348923, Final Batch Loss: 0.15363851189613342\n",
      "Subject 4, Epoch 578, Loss: 0.5642846077680588, Final Batch Loss: 0.19127699732780457\n",
      "Subject 4, Epoch 579, Loss: 0.5635695606470108, Final Batch Loss: 0.19866307079792023\n",
      "Subject 4, Epoch 580, Loss: 0.5827874317765236, Final Batch Loss: 0.20048560202121735\n",
      "Subject 4, Epoch 581, Loss: 0.4516989290714264, Final Batch Loss: 0.13117842376232147\n",
      "Subject 4, Epoch 582, Loss: 0.5750102400779724, Final Batch Loss: 0.2450537085533142\n",
      "Subject 4, Epoch 583, Loss: 0.4625447914004326, Final Batch Loss: 0.07977042347192764\n",
      "Subject 4, Epoch 584, Loss: 0.46146751940250397, Final Batch Loss: 0.14147165417671204\n",
      "Subject 4, Epoch 585, Loss: 0.5421864092350006, Final Batch Loss: 0.15437190234661102\n",
      "Subject 4, Epoch 586, Loss: 0.4771772027015686, Final Batch Loss: 0.1797221302986145\n",
      "Subject 4, Epoch 587, Loss: 0.4130457490682602, Final Batch Loss: 0.14206351339817047\n",
      "Subject 4, Epoch 588, Loss: 0.5026664435863495, Final Batch Loss: 0.1735551506280899\n",
      "Subject 4, Epoch 589, Loss: 0.5625083744525909, Final Batch Loss: 0.12050433456897736\n",
      "Subject 4, Epoch 590, Loss: 0.45289604365825653, Final Batch Loss: 0.1607368439435959\n",
      "Subject 4, Epoch 591, Loss: 0.4078531861305237, Final Batch Loss: 0.16607463359832764\n",
      "Subject 4, Epoch 592, Loss: 0.5625022649765015, Final Batch Loss: 0.20232300460338593\n",
      "Subject 4, Epoch 593, Loss: 0.3848850876092911, Final Batch Loss: 0.07664929330348969\n",
      "Subject 4, Epoch 594, Loss: 0.47151437401771545, Final Batch Loss: 0.11909335851669312\n",
      "Subject 4, Epoch 595, Loss: 0.46007880568504333, Final Batch Loss: 0.14032474160194397\n",
      "Subject 4, Epoch 596, Loss: 0.420296311378479, Final Batch Loss: 0.1300596445798874\n",
      "Subject 4, Epoch 597, Loss: 0.6108039766550064, Final Batch Loss: 0.24553295969963074\n",
      "Subject 4, Epoch 598, Loss: 0.534235268831253, Final Batch Loss: 0.1658613234758377\n",
      "Subject 4, Epoch 599, Loss: 0.4615343436598778, Final Batch Loss: 0.1722983866930008\n",
      "Subject 4, Epoch 600, Loss: 0.42244693636894226, Final Batch Loss: 0.14964529871940613\n",
      "Subject 4, Epoch 601, Loss: 0.6521958857774734, Final Batch Loss: 0.23964647948741913\n",
      "Subject 4, Epoch 602, Loss: 0.48592430353164673, Final Batch Loss: 0.15158410370349884\n",
      "Subject 4, Epoch 603, Loss: 0.5516297221183777, Final Batch Loss: 0.16544872522354126\n",
      "Subject 4, Epoch 604, Loss: 0.541825070977211, Final Batch Loss: 0.17766140401363373\n",
      "Subject 4, Epoch 605, Loss: 0.5906480997800827, Final Batch Loss: 0.27426472306251526\n",
      "Subject 4, Epoch 606, Loss: 0.4999590218067169, Final Batch Loss: 0.21185025572776794\n",
      "Subject 4, Epoch 607, Loss: 0.49789948761463165, Final Batch Loss: 0.1925540268421173\n",
      "Subject 4, Epoch 608, Loss: 0.4374736025929451, Final Batch Loss: 0.111774742603302\n",
      "Subject 4, Epoch 609, Loss: 0.5520852208137512, Final Batch Loss: 0.195503830909729\n",
      "Subject 4, Epoch 610, Loss: 0.46651221811771393, Final Batch Loss: 0.13446171581745148\n",
      "Subject 4, Epoch 611, Loss: 0.47874686121940613, Final Batch Loss: 0.17251929640769958\n",
      "Subject 4, Epoch 612, Loss: 0.42665985226631165, Final Batch Loss: 0.13069944083690643\n",
      "Subject 4, Epoch 613, Loss: 0.5172372385859489, Final Batch Loss: 0.25692737102508545\n",
      "Subject 4, Epoch 614, Loss: 0.452348954975605, Final Batch Loss: 0.1742001324892044\n",
      "Subject 4, Epoch 615, Loss: 0.39207959175109863, Final Batch Loss: 0.0877610296010971\n",
      "Subject 4, Epoch 616, Loss: 0.43852952122688293, Final Batch Loss: 0.1359109729528427\n",
      "Subject 4, Epoch 617, Loss: 0.485825315117836, Final Batch Loss: 0.16418494284152985\n",
      "Subject 4, Epoch 618, Loss: 0.4850296527147293, Final Batch Loss: 0.1767904907464981\n",
      "Subject 4, Epoch 619, Loss: 0.4112444072961807, Final Batch Loss: 0.1068776398897171\n",
      "Subject 4, Epoch 620, Loss: 0.38164322078227997, Final Batch Loss: 0.12659679353237152\n",
      "Subject 4, Epoch 621, Loss: 0.46937429904937744, Final Batch Loss: 0.1907399445772171\n",
      "Subject 4, Epoch 622, Loss: 0.4499392956495285, Final Batch Loss: 0.172621488571167\n",
      "Subject 4, Epoch 623, Loss: 0.39192483574151993, Final Batch Loss: 0.14205241203308105\n",
      "Subject 4, Epoch 624, Loss: 0.48535918444395065, Final Batch Loss: 0.12014935910701752\n",
      "Subject 4, Epoch 625, Loss: 0.5420402437448502, Final Batch Loss: 0.21602411568164825\n",
      "Subject 4, Epoch 626, Loss: 0.39108727127313614, Final Batch Loss: 0.09664911776781082\n",
      "Subject 4, Epoch 627, Loss: 0.4554443657398224, Final Batch Loss: 0.14144231379032135\n",
      "Subject 4, Epoch 628, Loss: 0.4792977124452591, Final Batch Loss: 0.17444437742233276\n",
      "Subject 4, Epoch 629, Loss: 0.3537270426750183, Final Batch Loss: 0.10884405672550201\n",
      "Subject 4, Epoch 630, Loss: 0.39662300795316696, Final Batch Loss: 0.2040412575006485\n",
      "Subject 4, Epoch 631, Loss: 0.3695971295237541, Final Batch Loss: 0.1401233822107315\n",
      "Subject 4, Epoch 632, Loss: 0.42576203495264053, Final Batch Loss: 0.16344085335731506\n",
      "Subject 4, Epoch 633, Loss: 0.44196975976228714, Final Batch Loss: 0.19988618791103363\n",
      "Subject 4, Epoch 634, Loss: 0.41777291148900986, Final Batch Loss: 0.24472226202487946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 635, Loss: 0.45860567688941956, Final Batch Loss: 0.15246108174324036\n",
      "Subject 4, Epoch 636, Loss: 0.40172387659549713, Final Batch Loss: 0.10829228162765503\n",
      "Subject 4, Epoch 637, Loss: 0.438617080450058, Final Batch Loss: 0.15533842146396637\n",
      "Subject 4, Epoch 638, Loss: 0.3452882841229439, Final Batch Loss: 0.0987541452050209\n",
      "Subject 4, Epoch 639, Loss: 0.47982701659202576, Final Batch Loss: 0.2483581304550171\n",
      "Subject 4, Epoch 640, Loss: 0.30690474063158035, Final Batch Loss: 0.1211422011256218\n",
      "Subject 4, Epoch 641, Loss: 0.40296947211027145, Final Batch Loss: 0.1181449368596077\n",
      "Subject 4, Epoch 642, Loss: 0.3208562433719635, Final Batch Loss: 0.07901737093925476\n",
      "Subject 4, Epoch 643, Loss: 0.4293070435523987, Final Batch Loss: 0.139547660946846\n",
      "Subject 4, Epoch 644, Loss: 0.4221942499279976, Final Batch Loss: 0.11110994219779968\n",
      "Subject 4, Epoch 645, Loss: 0.4215591698884964, Final Batch Loss: 0.13862958550453186\n",
      "Subject 4, Epoch 646, Loss: 0.524277925491333, Final Batch Loss: 0.24459509551525116\n",
      "Subject 4, Epoch 647, Loss: 0.5447266921401024, Final Batch Loss: 0.2517530024051666\n",
      "Subject 4, Epoch 648, Loss: 0.41737930476665497, Final Batch Loss: 0.18160288035869598\n",
      "Subject 4, Epoch 649, Loss: 0.3781866431236267, Final Batch Loss: 0.11616440117359161\n",
      "Subject 4, Epoch 650, Loss: 0.3653093948960304, Final Batch Loss: 0.04909536987543106\n",
      "Subject 4, Epoch 651, Loss: 0.4036514237523079, Final Batch Loss: 0.13932187855243683\n",
      "Subject 4, Epoch 652, Loss: 0.5601984411478043, Final Batch Loss: 0.29191914200782776\n",
      "Subject 4, Epoch 653, Loss: 0.39690887928009033, Final Batch Loss: 0.1464349776506424\n",
      "Subject 4, Epoch 654, Loss: 0.43824058771133423, Final Batch Loss: 0.2032659947872162\n",
      "Subject 4, Epoch 655, Loss: 0.46778586506843567, Final Batch Loss: 0.19248051941394806\n",
      "Subject 4, Epoch 656, Loss: 0.27129407599568367, Final Batch Loss: 0.05633918568491936\n",
      "Subject 4, Epoch 657, Loss: 0.5099843367934227, Final Batch Loss: 0.11881814152002335\n",
      "Subject 4, Epoch 658, Loss: 0.42496347427368164, Final Batch Loss: 0.13355128467082977\n",
      "Subject 4, Epoch 659, Loss: 0.40288690477609634, Final Batch Loss: 0.12057284265756607\n",
      "Subject 4, Epoch 660, Loss: 0.29848772287368774, Final Batch Loss: 0.09247773140668869\n",
      "Subject 4, Epoch 661, Loss: 0.4265270084142685, Final Batch Loss: 0.15778745710849762\n",
      "Subject 4, Epoch 662, Loss: 0.3137666955590248, Final Batch Loss: 0.07191550731658936\n",
      "Subject 4, Epoch 663, Loss: 0.4307277202606201, Final Batch Loss: 0.2106383740901947\n",
      "Subject 4, Epoch 664, Loss: 0.4648452028632164, Final Batch Loss: 0.23331202566623688\n",
      "Subject 4, Epoch 665, Loss: 0.4047907888889313, Final Batch Loss: 0.21287578344345093\n",
      "Subject 4, Epoch 666, Loss: 0.2902732789516449, Final Batch Loss: 0.10628791153430939\n",
      "Subject 4, Epoch 667, Loss: 0.3347518742084503, Final Batch Loss: 0.12270290404558182\n",
      "Subject 4, Epoch 668, Loss: 0.40495287626981735, Final Batch Loss: 0.17188797891139984\n",
      "Subject 4, Epoch 669, Loss: 0.44897742569446564, Final Batch Loss: 0.15402235090732574\n",
      "Subject 4, Epoch 670, Loss: 0.4225556403398514, Final Batch Loss: 0.13665837049484253\n",
      "Subject 4, Epoch 671, Loss: 0.4320335164666176, Final Batch Loss: 0.20410004258155823\n",
      "Subject 4, Epoch 672, Loss: 0.4246910661458969, Final Batch Loss: 0.12410914897918701\n",
      "Subject 4, Epoch 673, Loss: 0.4465625733137131, Final Batch Loss: 0.15547405183315277\n",
      "Subject 4, Epoch 674, Loss: 0.40572188049554825, Final Batch Loss: 0.1606988161802292\n",
      "Subject 4, Epoch 675, Loss: 0.4468710198998451, Final Batch Loss: 0.1420677900314331\n",
      "Subject 4, Epoch 676, Loss: 0.334563784301281, Final Batch Loss: 0.10027112811803818\n",
      "Subject 4, Epoch 677, Loss: 0.3433707132935524, Final Batch Loss: 0.09897211194038391\n",
      "Subject 4, Epoch 678, Loss: 0.3595605343580246, Final Batch Loss: 0.14441926777362823\n",
      "Subject 4, Epoch 679, Loss: 0.39900096505880356, Final Batch Loss: 0.11562582105398178\n",
      "Subject 4, Epoch 680, Loss: 0.3832322284579277, Final Batch Loss: 0.15665055811405182\n",
      "Subject 4, Epoch 681, Loss: 0.43680115044116974, Final Batch Loss: 0.15662994980812073\n",
      "Subject 4, Epoch 682, Loss: 0.339728944003582, Final Batch Loss: 0.09747548401355743\n",
      "Subject 4, Epoch 683, Loss: 0.3712308555841446, Final Batch Loss: 0.1079932302236557\n",
      "Subject 4, Epoch 684, Loss: 0.36589904874563217, Final Batch Loss: 0.07685966044664383\n",
      "Subject 4, Epoch 685, Loss: 0.3176656737923622, Final Batch Loss: 0.12455733120441437\n",
      "Subject 4, Epoch 686, Loss: 0.3615358993411064, Final Batch Loss: 0.11379658430814743\n",
      "Subject 4, Epoch 687, Loss: 0.3437066376209259, Final Batch Loss: 0.09581930935382843\n",
      "Subject 4, Epoch 688, Loss: 0.35561373084783554, Final Batch Loss: 0.15118549764156342\n",
      "Subject 4, Epoch 689, Loss: 0.2902217209339142, Final Batch Loss: 0.08761859685182571\n",
      "Subject 4, Epoch 690, Loss: 0.36336882412433624, Final Batch Loss: 0.14746366441249847\n",
      "Subject 4, Epoch 691, Loss: 0.3467862159013748, Final Batch Loss: 0.1574014574289322\n",
      "Subject 4, Epoch 692, Loss: 0.3379380702972412, Final Batch Loss: 0.13949082791805267\n",
      "Subject 4, Epoch 693, Loss: 0.3549012392759323, Final Batch Loss: 0.1214117631316185\n",
      "Subject 4, Epoch 694, Loss: 0.4679693579673767, Final Batch Loss: 0.10401827096939087\n",
      "Subject 4, Epoch 695, Loss: 0.4428575336933136, Final Batch Loss: 0.17724572122097015\n",
      "Subject 4, Epoch 696, Loss: 0.3282817527651787, Final Batch Loss: 0.08159254491329193\n",
      "Subject 4, Epoch 697, Loss: 0.42921266704797745, Final Batch Loss: 0.12417525053024292\n",
      "Subject 4, Epoch 698, Loss: 0.36085882782936096, Final Batch Loss: 0.11915506422519684\n",
      "Subject 4, Epoch 699, Loss: 0.31893064081668854, Final Batch Loss: 0.09203149378299713\n",
      "Subject 4, Epoch 700, Loss: 0.2445685677230358, Final Batch Loss: 0.04428432509303093\n",
      "Subject 4, Epoch 701, Loss: 0.302001528441906, Final Batch Loss: 0.06143692135810852\n",
      "Subject 4, Epoch 702, Loss: 0.30111850053071976, Final Batch Loss: 0.07661246508359909\n",
      "Subject 4, Epoch 703, Loss: 0.27499495446681976, Final Batch Loss: 0.08390818536281586\n",
      "Subject 4, Epoch 704, Loss: 0.325581394135952, Final Batch Loss: 0.13965262472629547\n",
      "Subject 4, Epoch 705, Loss: 0.4054601937532425, Final Batch Loss: 0.1719163954257965\n",
      "Subject 4, Epoch 706, Loss: 0.2865855321288109, Final Batch Loss: 0.06613955646753311\n",
      "Subject 4, Epoch 707, Loss: 0.25792064517736435, Final Batch Loss: 0.07352431118488312\n",
      "Subject 4, Epoch 708, Loss: 0.33913712948560715, Final Batch Loss: 0.1427324116230011\n",
      "Subject 4, Epoch 709, Loss: 0.3438499644398689, Final Batch Loss: 0.1691187471151352\n",
      "Subject 4, Epoch 710, Loss: 0.44949357211589813, Final Batch Loss: 0.13573284447193146\n",
      "Subject 4, Epoch 711, Loss: 0.34717857837677, Final Batch Loss: 0.16548974812030792\n",
      "Subject 4, Epoch 712, Loss: 0.3538176864385605, Final Batch Loss: 0.07617844641208649\n",
      "Subject 4, Epoch 713, Loss: 0.39372680336236954, Final Batch Loss: 0.1812051385641098\n",
      "Subject 4, Epoch 714, Loss: 0.4652135670185089, Final Batch Loss: 0.16360031068325043\n",
      "Subject 4, Epoch 715, Loss: 0.3457546681165695, Final Batch Loss: 0.12725749611854553\n",
      "Subject 4, Epoch 716, Loss: 0.2794182598590851, Final Batch Loss: 0.08848298341035843\n",
      "Subject 4, Epoch 717, Loss: 0.30345068126916885, Final Batch Loss: 0.0840081050992012\n",
      "Subject 4, Epoch 718, Loss: 0.2820972427725792, Final Batch Loss: 0.07452014088630676\n",
      "Subject 4, Epoch 719, Loss: 0.3037252277135849, Final Batch Loss: 0.10884957015514374\n",
      "Subject 4, Epoch 720, Loss: 0.2885149195790291, Final Batch Loss: 0.10563728213310242\n",
      "Subject 4, Epoch 721, Loss: 0.4163038209080696, Final Batch Loss: 0.15878911316394806\n",
      "Subject 4, Epoch 722, Loss: 0.30399832874536514, Final Batch Loss: 0.1010068729519844\n",
      "Subject 4, Epoch 723, Loss: 0.3914269804954529, Final Batch Loss: 0.16838377714157104\n",
      "Subject 4, Epoch 724, Loss: 0.3074536994099617, Final Batch Loss: 0.08557315170764923\n",
      "Subject 4, Epoch 725, Loss: 0.32159560173749924, Final Batch Loss: 0.15175923705101013\n",
      "Subject 4, Epoch 726, Loss: 0.31002332270145416, Final Batch Loss: 0.09128349274396896\n",
      "Subject 4, Epoch 727, Loss: 0.35184215381741524, Final Batch Loss: 0.1607564091682434\n",
      "Subject 4, Epoch 728, Loss: 0.3633454889059067, Final Batch Loss: 0.19622820615768433\n",
      "Subject 4, Epoch 729, Loss: 0.3746349960565567, Final Batch Loss: 0.16874189674854279\n",
      "Subject 4, Epoch 730, Loss: 0.29581523686647415, Final Batch Loss: 0.10811629891395569\n",
      "Subject 4, Epoch 731, Loss: 0.40173256397247314, Final Batch Loss: 0.2071266919374466\n",
      "Subject 4, Epoch 732, Loss: 0.31404804810881615, Final Batch Loss: 0.12772832810878754\n",
      "Subject 4, Epoch 733, Loss: 0.34446217119693756, Final Batch Loss: 0.10234975814819336\n",
      "Subject 4, Epoch 734, Loss: 0.354259729385376, Final Batch Loss: 0.1623922437429428\n",
      "Subject 4, Epoch 735, Loss: 0.33223865926265717, Final Batch Loss: 0.1286553591489792\n",
      "Subject 4, Epoch 736, Loss: 0.3247046247124672, Final Batch Loss: 0.11053697764873505\n",
      "Subject 4, Epoch 737, Loss: 0.31281502544879913, Final Batch Loss: 0.13399197161197662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 738, Loss: 0.3682396188378334, Final Batch Loss: 0.14765331149101257\n",
      "Subject 4, Epoch 739, Loss: 0.2842565216124058, Final Batch Loss: 0.10256727039813995\n",
      "Subject 4, Epoch 740, Loss: 0.30923323333263397, Final Batch Loss: 0.13325314223766327\n",
      "Subject 4, Epoch 741, Loss: 0.21943927556276321, Final Batch Loss: 0.04556959867477417\n",
      "Subject 4, Epoch 742, Loss: 0.36466578394174576, Final Batch Loss: 0.19087737798690796\n",
      "Subject 4, Epoch 743, Loss: 0.31971344351768494, Final Batch Loss: 0.1477908343076706\n",
      "Subject 4, Epoch 744, Loss: 0.26653386652469635, Final Batch Loss: 0.05617173761129379\n",
      "Subject 4, Epoch 745, Loss: 0.31212639808654785, Final Batch Loss: 0.04397483915090561\n",
      "Subject 4, Epoch 746, Loss: 0.28792696446180344, Final Batch Loss: 0.12519130110740662\n",
      "Subject 4, Epoch 747, Loss: 0.28842969238758087, Final Batch Loss: 0.09534958004951477\n",
      "Subject 4, Epoch 748, Loss: 0.2372783049941063, Final Batch Loss: 0.07837173342704773\n",
      "Subject 4, Epoch 749, Loss: 0.24487803131341934, Final Batch Loss: 0.044907353818416595\n",
      "Subject 4, Epoch 750, Loss: 0.36037830263376236, Final Batch Loss: 0.12583203613758087\n",
      "Subject 4, Epoch 751, Loss: 0.2727805972099304, Final Batch Loss: 0.05581677705049515\n",
      "Subject 4, Epoch 752, Loss: 0.26297787949442863, Final Batch Loss: 0.036935415118932724\n",
      "Subject 4, Epoch 753, Loss: 0.2564660646021366, Final Batch Loss: 0.0387808196246624\n",
      "Subject 4, Epoch 754, Loss: 0.20968162640929222, Final Batch Loss: 0.06412163376808167\n",
      "Subject 4, Epoch 755, Loss: 0.28232259303331375, Final Batch Loss: 0.07196789234876633\n",
      "Subject 4, Epoch 756, Loss: 0.28661662340164185, Final Batch Loss: 0.14404258131980896\n",
      "Subject 4, Epoch 757, Loss: 0.30715060234069824, Final Batch Loss: 0.07174888998270035\n",
      "Subject 4, Epoch 758, Loss: 0.2349005863070488, Final Batch Loss: 0.04370018094778061\n",
      "Subject 4, Epoch 759, Loss: 0.2697700262069702, Final Batch Loss: 0.05320161581039429\n",
      "Subject 4, Epoch 760, Loss: 0.21711518242955208, Final Batch Loss: 0.08992888033390045\n",
      "Subject 4, Epoch 761, Loss: 0.22234418988227844, Final Batch Loss: 0.08393877744674683\n",
      "Subject 4, Epoch 762, Loss: 0.2540144845843315, Final Batch Loss: 0.07881920784711838\n",
      "Subject 4, Epoch 763, Loss: 0.2555599845945835, Final Batch Loss: 0.047056447714567184\n",
      "Subject 4, Epoch 764, Loss: 0.22725914046168327, Final Batch Loss: 0.024955619126558304\n",
      "Subject 4, Epoch 765, Loss: 0.26865578815340996, Final Batch Loss: 0.034454282373189926\n",
      "Subject 4, Epoch 766, Loss: 0.25031426176428795, Final Batch Loss: 0.11092758178710938\n",
      "Subject 4, Epoch 767, Loss: 0.2694048061966896, Final Batch Loss: 0.07688870280981064\n",
      "Subject 4, Epoch 768, Loss: 0.2845807299017906, Final Batch Loss: 0.07626514136791229\n",
      "Subject 4, Epoch 769, Loss: 0.2547404281795025, Final Batch Loss: 0.12853193283081055\n",
      "Subject 4, Epoch 770, Loss: 0.23258356750011444, Final Batch Loss: 0.03479988873004913\n",
      "Subject 4, Epoch 771, Loss: 0.30795954540371895, Final Batch Loss: 0.12012030929327011\n",
      "Subject 4, Epoch 772, Loss: 0.2396182268857956, Final Batch Loss: 0.09112094342708588\n",
      "Subject 4, Epoch 773, Loss: 0.3925728052854538, Final Batch Loss: 0.23761117458343506\n",
      "Subject 4, Epoch 774, Loss: 0.29059121757745743, Final Batch Loss: 0.0977448895573616\n",
      "Subject 4, Epoch 775, Loss: 0.2650844156742096, Final Batch Loss: 0.023770667612552643\n",
      "Subject 4, Epoch 776, Loss: 0.27208443731069565, Final Batch Loss: 0.09719958901405334\n",
      "Subject 4, Epoch 777, Loss: 0.18786858022212982, Final Batch Loss: 0.0795406624674797\n",
      "Subject 4, Epoch 778, Loss: 0.2895459681749344, Final Batch Loss: 0.06361497938632965\n",
      "Subject 4, Epoch 779, Loss: 0.27361784502863884, Final Batch Loss: 0.03215958550572395\n",
      "Subject 4, Epoch 780, Loss: 0.20330294966697693, Final Batch Loss: 0.07729201763868332\n",
      "Subject 4, Epoch 781, Loss: 0.24088335782289505, Final Batch Loss: 0.07427287846803665\n",
      "Subject 4, Epoch 782, Loss: 0.3413110300898552, Final Batch Loss: 0.16848936676979065\n",
      "Subject 4, Epoch 783, Loss: 0.19071070104837418, Final Batch Loss: 0.06355751305818558\n",
      "Subject 4, Epoch 784, Loss: 0.24285757541656494, Final Batch Loss: 0.08328321576118469\n",
      "Subject 4, Epoch 785, Loss: 0.225444994866848, Final Batch Loss: 0.07068264484405518\n",
      "Subject 4, Epoch 786, Loss: 0.2474283054471016, Final Batch Loss: 0.07008548825979233\n",
      "Subject 4, Epoch 787, Loss: 0.25168346613645554, Final Batch Loss: 0.0726984515786171\n",
      "Subject 4, Epoch 788, Loss: 0.2074047140777111, Final Batch Loss: 0.09831401705741882\n",
      "Subject 4, Epoch 789, Loss: 0.23346815258264542, Final Batch Loss: 0.040958043187856674\n",
      "Subject 4, Epoch 790, Loss: 0.3170531317591667, Final Batch Loss: 0.15204454958438873\n",
      "Subject 4, Epoch 791, Loss: 0.20806358382105827, Final Batch Loss: 0.05365198478102684\n",
      "Subject 4, Epoch 792, Loss: 0.2704630568623543, Final Batch Loss: 0.11334796994924545\n",
      "Subject 4, Epoch 793, Loss: 0.24762344732880592, Final Batch Loss: 0.07327505201101303\n",
      "Subject 4, Epoch 794, Loss: 0.2521742098033428, Final Batch Loss: 0.08993466943502426\n",
      "Subject 4, Epoch 795, Loss: 0.3303482383489609, Final Batch Loss: 0.09399265050888062\n",
      "Subject 4, Epoch 796, Loss: 0.19604729115962982, Final Batch Loss: 0.03711674362421036\n",
      "Subject 4, Epoch 797, Loss: 0.25656957924366, Final Batch Loss: 0.08487198501825333\n",
      "Subject 4, Epoch 798, Loss: 0.3095829710364342, Final Batch Loss: 0.08842120319604874\n",
      "Subject 4, Epoch 799, Loss: 0.22537017986178398, Final Batch Loss: 0.12390545755624771\n",
      "Subject 4, Epoch 800, Loss: 0.19245867803692818, Final Batch Loss: 0.036018673330545425\n",
      "Subject 4, Epoch 801, Loss: 0.22909439355134964, Final Batch Loss: 0.03311694413423538\n",
      "Subject 4, Epoch 802, Loss: 0.19749765843153, Final Batch Loss: 0.07504001259803772\n",
      "Subject 4, Epoch 803, Loss: 0.24093182384967804, Final Batch Loss: 0.06922711431980133\n",
      "Subject 4, Epoch 804, Loss: 0.29237164556980133, Final Batch Loss: 0.18982601165771484\n",
      "Subject 4, Epoch 805, Loss: 0.2449857033789158, Final Batch Loss: 0.08305974304676056\n",
      "Subject 4, Epoch 806, Loss: 0.37761954218149185, Final Batch Loss: 0.08865424990653992\n",
      "Subject 4, Epoch 807, Loss: 0.2245463002473116, Final Batch Loss: 0.021203147247433662\n",
      "Subject 4, Epoch 808, Loss: 0.23241140320897102, Final Batch Loss: 0.046457309275865555\n",
      "Subject 4, Epoch 809, Loss: 0.32970942184329033, Final Batch Loss: 0.040274448692798615\n",
      "Subject 4, Epoch 810, Loss: 0.19168352335691452, Final Batch Loss: 0.07939588278532028\n",
      "Subject 4, Epoch 811, Loss: 0.20336049050092697, Final Batch Loss: 0.05731327086687088\n",
      "Subject 4, Epoch 812, Loss: 0.30957167223095894, Final Batch Loss: 0.16067945957183838\n",
      "Subject 4, Epoch 813, Loss: 0.168831218034029, Final Batch Loss: 0.026718538254499435\n",
      "Subject 4, Epoch 814, Loss: 0.17649603262543678, Final Batch Loss: 0.05136688053607941\n",
      "Subject 4, Epoch 815, Loss: 0.1696564257144928, Final Batch Loss: 0.038993481546640396\n",
      "Subject 4, Epoch 816, Loss: 0.21298512071371078, Final Batch Loss: 0.09318188577890396\n",
      "Subject 4, Epoch 817, Loss: 0.23274570889770985, Final Batch Loss: 0.1260310262441635\n",
      "Subject 4, Epoch 818, Loss: 0.19683175906538963, Final Batch Loss: 0.028797779232263565\n",
      "Subject 4, Epoch 819, Loss: 0.24896176531910896, Final Batch Loss: 0.050358135253190994\n",
      "Subject 4, Epoch 820, Loss: 0.16381578147411346, Final Batch Loss: 0.08709955960512161\n",
      "Subject 4, Epoch 821, Loss: 0.2108415924012661, Final Batch Loss: 0.09692652523517609\n",
      "Subject 4, Epoch 822, Loss: 0.3645539954304695, Final Batch Loss: 0.1623908430337906\n",
      "Subject 4, Epoch 823, Loss: 0.17451152205467224, Final Batch Loss: 0.06585761159658432\n",
      "Subject 4, Epoch 824, Loss: 0.2507311552762985, Final Batch Loss: 0.09833315759897232\n",
      "Subject 4, Epoch 825, Loss: 0.21940967440605164, Final Batch Loss: 0.05856678634881973\n",
      "Subject 4, Epoch 826, Loss: 0.24948705732822418, Final Batch Loss: 0.07923302799463272\n",
      "Subject 4, Epoch 827, Loss: 0.3119843676686287, Final Batch Loss: 0.0864521712064743\n",
      "Subject 4, Epoch 828, Loss: 0.22719085216522217, Final Batch Loss: 0.1127423569560051\n",
      "Subject 4, Epoch 829, Loss: 0.2544074170291424, Final Batch Loss: 0.04743995890021324\n",
      "Subject 4, Epoch 830, Loss: 0.1930614411830902, Final Batch Loss: 0.057510070502758026\n",
      "Subject 4, Epoch 831, Loss: 0.19874858856201172, Final Batch Loss: 0.03980429098010063\n",
      "Subject 4, Epoch 832, Loss: 0.21055977419018745, Final Batch Loss: 0.06802994012832642\n",
      "Subject 4, Epoch 833, Loss: 0.2088608406484127, Final Batch Loss: 0.07780326902866364\n",
      "Subject 4, Epoch 834, Loss: 0.286734014749527, Final Batch Loss: 0.07099052518606186\n",
      "Subject 4, Epoch 835, Loss: 0.21756064146757126, Final Batch Loss: 0.06737373024225235\n",
      "Subject 4, Epoch 836, Loss: 0.3059636428952217, Final Batch Loss: 0.11170036345720291\n",
      "Subject 4, Epoch 837, Loss: 0.32516496628522873, Final Batch Loss: 0.06996729224920273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 838, Loss: 0.17951253429055214, Final Batch Loss: 0.06425339728593826\n",
      "Subject 4, Epoch 839, Loss: 0.33896730467677116, Final Batch Loss: 0.0546049140393734\n",
      "Subject 4, Epoch 840, Loss: 0.2622452825307846, Final Batch Loss: 0.06642232090234756\n",
      "Subject 4, Epoch 841, Loss: 0.26681526005268097, Final Batch Loss: 0.10779949277639389\n",
      "Subject 4, Epoch 842, Loss: 0.19357115402817726, Final Batch Loss: 0.10293091088533401\n",
      "Subject 4, Epoch 843, Loss: 0.17600148916244507, Final Batch Loss: 0.06618254631757736\n",
      "Subject 4, Epoch 844, Loss: 0.24287733808159828, Final Batch Loss: 0.10129362344741821\n",
      "Subject 4, Epoch 845, Loss: 0.17099269479513168, Final Batch Loss: 0.03140239417552948\n",
      "Subject 4, Epoch 846, Loss: 0.14387556537985802, Final Batch Loss: 0.042530227452516556\n",
      "Subject 4, Epoch 847, Loss: 0.1737357545644045, Final Batch Loss: 0.1083146408200264\n",
      "Subject 4, Epoch 848, Loss: 0.21411550045013428, Final Batch Loss: 0.07919833809137344\n",
      "Subject 4, Epoch 849, Loss: 0.19749601557850838, Final Batch Loss: 0.1302666962146759\n",
      "Subject 4, Epoch 850, Loss: 0.1970913577824831, Final Batch Loss: 0.01913902349770069\n",
      "Subject 4, Epoch 851, Loss: 0.1821810081601143, Final Batch Loss: 0.019736073911190033\n",
      "Subject 4, Epoch 852, Loss: 0.2852986939251423, Final Batch Loss: 0.05500662699341774\n",
      "Subject 4, Epoch 853, Loss: 0.2467874437570572, Final Batch Loss: 0.10461656749248505\n",
      "Subject 4, Epoch 854, Loss: 0.24655437469482422, Final Batch Loss: 0.08922864496707916\n",
      "Subject 4, Epoch 855, Loss: 0.28559015691280365, Final Batch Loss: 0.07544167339801788\n",
      "Subject 4, Epoch 856, Loss: 0.14120597019791603, Final Batch Loss: 0.04234984517097473\n",
      "Subject 4, Epoch 857, Loss: 0.29728735983371735, Final Batch Loss: 0.1602097749710083\n",
      "Subject 4, Epoch 858, Loss: 0.19371105544269085, Final Batch Loss: 0.007725624367594719\n",
      "Subject 4, Epoch 859, Loss: 0.19682058319449425, Final Batch Loss: 0.05813680589199066\n",
      "Subject 4, Epoch 860, Loss: 0.15032295882701874, Final Batch Loss: 0.05008039250969887\n",
      "Subject 4, Epoch 861, Loss: 0.23487312719225883, Final Batch Loss: 0.09001492708921432\n",
      "Subject 4, Epoch 862, Loss: 0.213592529296875, Final Batch Loss: 0.0710829421877861\n",
      "Subject 4, Epoch 863, Loss: 0.18600228056311607, Final Batch Loss: 0.0845300704240799\n",
      "Subject 4, Epoch 864, Loss: 0.2932654097676277, Final Batch Loss: 0.07484419643878937\n",
      "Subject 4, Epoch 865, Loss: 0.22928578779101372, Final Batch Loss: 0.10350752621889114\n",
      "Subject 4, Epoch 866, Loss: 0.25555915012955666, Final Batch Loss: 0.08702655881643295\n",
      "Subject 4, Epoch 867, Loss: 0.19112835079431534, Final Batch Loss: 0.04491560161113739\n",
      "Subject 4, Epoch 868, Loss: 0.17812134511768818, Final Batch Loss: 0.021039800718426704\n",
      "Subject 4, Epoch 869, Loss: 0.30605047196149826, Final Batch Loss: 0.0751492902636528\n",
      "Subject 4, Epoch 870, Loss: 0.15871617197990417, Final Batch Loss: 0.04337841272354126\n",
      "Subject 4, Epoch 871, Loss: 0.1774723306298256, Final Batch Loss: 0.047672007232904434\n",
      "Subject 4, Epoch 872, Loss: 0.1658032462000847, Final Batch Loss: 0.04546947032213211\n",
      "Subject 4, Epoch 873, Loss: 0.18981192260980606, Final Batch Loss: 0.058024000376462936\n",
      "Subject 4, Epoch 874, Loss: 0.27317655086517334, Final Batch Loss: 0.09711004048585892\n",
      "Subject 4, Epoch 875, Loss: 0.1883373036980629, Final Batch Loss: 0.052926212549209595\n",
      "Subject 4, Epoch 876, Loss: 0.25330669432878494, Final Batch Loss: 0.11784582585096359\n",
      "Subject 4, Epoch 877, Loss: 0.27635299786925316, Final Batch Loss: 0.13382810354232788\n",
      "Subject 4, Epoch 878, Loss: 0.14238165505230427, Final Batch Loss: 0.03015911765396595\n",
      "Subject 4, Epoch 879, Loss: 0.165732242166996, Final Batch Loss: 0.043418869376182556\n",
      "Subject 4, Epoch 880, Loss: 0.2079879641532898, Final Batch Loss: 0.06538568437099457\n",
      "Subject 4, Epoch 881, Loss: 0.26851335167884827, Final Batch Loss: 0.05845755338668823\n",
      "Subject 4, Epoch 882, Loss: 0.14135700650513172, Final Batch Loss: 0.020052941516041756\n",
      "Subject 4, Epoch 883, Loss: 0.2072584480047226, Final Batch Loss: 0.04220319539308548\n",
      "Subject 4, Epoch 884, Loss: 0.18601952120661736, Final Batch Loss: 0.05986024811863899\n",
      "Subject 4, Epoch 885, Loss: 0.21199720352888107, Final Batch Loss: 0.03721056133508682\n",
      "Subject 4, Epoch 886, Loss: 0.21997284516692162, Final Batch Loss: 0.07269277423620224\n",
      "Subject 4, Epoch 887, Loss: 0.17174276150763035, Final Batch Loss: 0.03132657706737518\n",
      "Subject 4, Epoch 888, Loss: 0.20426718890666962, Final Batch Loss: 0.06426265835762024\n",
      "Subject 4, Epoch 889, Loss: 0.2583215646445751, Final Batch Loss: 0.04359544813632965\n",
      "Subject 4, Epoch 890, Loss: 0.23316547647118568, Final Batch Loss: 0.04688606038689613\n",
      "Subject 4, Epoch 891, Loss: 0.12022145092487335, Final Batch Loss: 0.01735736057162285\n",
      "Subject 4, Epoch 892, Loss: 0.19541503116488457, Final Batch Loss: 0.08550713956356049\n",
      "Subject 4, Epoch 893, Loss: 0.16775280237197876, Final Batch Loss: 0.04875914752483368\n",
      "Subject 4, Epoch 894, Loss: 0.19628356024622917, Final Batch Loss: 0.03572314605116844\n",
      "Subject 4, Epoch 895, Loss: 0.20429445058107376, Final Batch Loss: 0.05167797580361366\n",
      "Subject 4, Epoch 896, Loss: 0.1958540789783001, Final Batch Loss: 0.10322879254817963\n",
      "Subject 4, Epoch 897, Loss: 0.19769100099802017, Final Batch Loss: 0.09050484746694565\n",
      "Subject 4, Epoch 898, Loss: 0.10757391713559628, Final Batch Loss: 0.040491849184036255\n",
      "Subject 4, Epoch 899, Loss: 0.12551918253302574, Final Batch Loss: 0.04095827788114548\n",
      "Subject 4, Epoch 900, Loss: 0.12019834853708744, Final Batch Loss: 0.0491165965795517\n",
      "Subject 4, Epoch 901, Loss: 0.1802319958806038, Final Batch Loss: 0.056130219250917435\n",
      "Subject 4, Epoch 902, Loss: 0.26299043744802475, Final Batch Loss: 0.11193187534809113\n",
      "Subject 4, Epoch 903, Loss: 0.14666791260242462, Final Batch Loss: 0.05602766573429108\n",
      "Subject 4, Epoch 904, Loss: 0.18341818824410439, Final Batch Loss: 0.0649692565202713\n",
      "Subject 4, Epoch 905, Loss: 0.20910408161580563, Final Batch Loss: 0.06704428046941757\n",
      "Subject 4, Epoch 906, Loss: 0.184708159416914, Final Batch Loss: 0.027273517102003098\n",
      "Subject 4, Epoch 907, Loss: 0.20593084767460823, Final Batch Loss: 0.041126277297735214\n",
      "Subject 4, Epoch 908, Loss: 0.2839965298771858, Final Batch Loss: 0.16130024194717407\n",
      "Subject 4, Epoch 909, Loss: 0.15810760483145714, Final Batch Loss: 0.0433407761156559\n",
      "Subject 4, Epoch 910, Loss: 0.14475968293845654, Final Batch Loss: 0.032166987657547\n",
      "Subject 4, Epoch 911, Loss: 0.13777302578091621, Final Batch Loss: 0.02928254008293152\n",
      "Subject 4, Epoch 912, Loss: 0.32310783863067627, Final Batch Loss: 0.07223805785179138\n",
      "Subject 4, Epoch 913, Loss: 0.1880229227244854, Final Batch Loss: 0.10148891061544418\n",
      "Subject 4, Epoch 914, Loss: 0.13277218118309975, Final Batch Loss: 0.03761729970574379\n",
      "Subject 4, Epoch 915, Loss: 0.11805740930140018, Final Batch Loss: 0.028454391285777092\n",
      "Subject 4, Epoch 916, Loss: 0.1536852903664112, Final Batch Loss: 0.06365316361188889\n",
      "Subject 4, Epoch 917, Loss: 0.1026347279548645, Final Batch Loss: 0.04668646678328514\n",
      "Subject 4, Epoch 918, Loss: 0.16760797053575516, Final Batch Loss: 0.08104267716407776\n",
      "Subject 4, Epoch 919, Loss: 0.156350776553154, Final Batch Loss: 0.020048413425683975\n",
      "Subject 4, Epoch 920, Loss: 0.1561087816953659, Final Batch Loss: 0.05287942662835121\n",
      "Subject 4, Epoch 921, Loss: 0.09144446440041065, Final Batch Loss: 0.030812092125415802\n",
      "Subject 4, Epoch 922, Loss: 0.16196386888623238, Final Batch Loss: 0.021999910473823547\n",
      "Subject 4, Epoch 923, Loss: 0.17327836155891418, Final Batch Loss: 0.04155875742435455\n",
      "Subject 4, Epoch 924, Loss: 0.06662056129425764, Final Batch Loss: 0.01124993059784174\n",
      "Subject 4, Epoch 925, Loss: 0.2047225423157215, Final Batch Loss: 0.052929285913705826\n",
      "Subject 4, Epoch 926, Loss: 0.13512958586215973, Final Batch Loss: 0.042401596903800964\n",
      "Subject 4, Epoch 927, Loss: 0.21665681898593903, Final Batch Loss: 0.07441650331020355\n",
      "Subject 4, Epoch 928, Loss: 0.28766893595457077, Final Batch Loss: 0.143614262342453\n",
      "Subject 4, Epoch 929, Loss: 0.10678318608552217, Final Batch Loss: 0.051654987037181854\n",
      "Subject 4, Epoch 930, Loss: 0.18025314062833786, Final Batch Loss: 0.036170586943626404\n",
      "Subject 4, Epoch 931, Loss: 0.18670865893363953, Final Batch Loss: 0.09485673159360886\n",
      "Subject 4, Epoch 932, Loss: 0.16327249072492123, Final Batch Loss: 0.02829771675169468\n",
      "Subject 4, Epoch 933, Loss: 0.11192854307591915, Final Batch Loss: 0.0583772137761116\n",
      "Subject 4, Epoch 934, Loss: 0.118386659771204, Final Batch Loss: 0.031063728034496307\n",
      "Subject 4, Epoch 935, Loss: 0.16396371647715569, Final Batch Loss: 0.08237876743078232\n",
      "Subject 4, Epoch 936, Loss: 0.16173070669174194, Final Batch Loss: 0.044971875846385956\n",
      "Subject 4, Epoch 937, Loss: 0.11414401978254318, Final Batch Loss: 0.042956188321113586\n",
      "Subject 4, Epoch 938, Loss: 0.1649911254644394, Final Batch Loss: 0.04627929627895355\n",
      "Subject 4, Epoch 939, Loss: 0.11313889920711517, Final Batch Loss: 0.01365663856267929\n",
      "Subject 4, Epoch 940, Loss: 0.18042246997356415, Final Batch Loss: 0.05420535430312157\n",
      "Subject 4, Epoch 941, Loss: 0.14383022859692574, Final Batch Loss: 0.10119680315256119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 4, Epoch 942, Loss: 0.26423996686935425, Final Batch Loss: 0.1270957738161087\n",
      "Subject 4, Epoch 943, Loss: 0.14331021532416344, Final Batch Loss: 0.027180660516023636\n",
      "Subject 4, Epoch 944, Loss: 0.1901538111269474, Final Batch Loss: 0.03843646124005318\n",
      "Subject 4, Epoch 945, Loss: 0.14589353650808334, Final Batch Loss: 0.055947043001651764\n",
      "Subject 4, Epoch 946, Loss: 0.12638487853109837, Final Batch Loss: 0.05835165083408356\n",
      "Subject 4, Epoch 947, Loss: 0.08338466286659241, Final Batch Loss: 0.030527357012033463\n",
      "Subject 4, Epoch 948, Loss: 0.19465526193380356, Final Batch Loss: 0.07675527781248093\n",
      "Subject 4, Epoch 949, Loss: 0.10245850682258606, Final Batch Loss: 0.029708197340369225\n",
      "Subject 4, Epoch 950, Loss: 0.2146991267800331, Final Batch Loss: 0.09581593424081802\n",
      "Subject 4, Epoch 951, Loss: 0.16690154932439327, Final Batch Loss: 0.021134408190846443\n",
      "Subject 4, Epoch 952, Loss: 0.28133999556303024, Final Batch Loss: 0.12823514640331268\n",
      "Subject 4, Epoch 953, Loss: 0.13743171095848083, Final Batch Loss: 0.05581369251012802\n",
      "Subject 4, Epoch 954, Loss: 0.1398497261106968, Final Batch Loss: 0.02370753511786461\n",
      "Subject 4, Epoch 955, Loss: 0.14724649116396904, Final Batch Loss: 0.04387955740094185\n",
      "Subject 4, Epoch 956, Loss: 0.12936291471123695, Final Batch Loss: 0.042906392365694046\n",
      "Subject 4, Epoch 957, Loss: 0.1702023334801197, Final Batch Loss: 0.0410337969660759\n",
      "Subject 4, Epoch 958, Loss: 0.18987633660435677, Final Batch Loss: 0.0757237896323204\n",
      "Subject 4, Epoch 959, Loss: 0.10656996630132198, Final Batch Loss: 0.03368278592824936\n",
      "Subject 4, Epoch 960, Loss: 0.10203021764755249, Final Batch Loss: 0.044362809509038925\n",
      "Subject 4, Epoch 961, Loss: 0.11598511040210724, Final Batch Loss: 0.04622073471546173\n",
      "Subject 4, Epoch 962, Loss: 0.13886535167694092, Final Batch Loss: 0.022218257188796997\n",
      "Subject 4, Epoch 963, Loss: 0.09622604586184025, Final Batch Loss: 0.03225044161081314\n",
      "Subject 4, Epoch 964, Loss: 0.13607674837112427, Final Batch Loss: 0.02816501259803772\n",
      "Subject 4, Epoch 965, Loss: 0.1453937664628029, Final Batch Loss: 0.06731227040290833\n",
      "Subject 4, Epoch 966, Loss: 0.08584409207105637, Final Batch Loss: 0.03624366596341133\n",
      "Subject 4, Epoch 967, Loss: 0.2111511491239071, Final Batch Loss: 0.06936938315629959\n",
      "Subject 4, Epoch 968, Loss: 0.18569490499794483, Final Batch Loss: 0.11597073823213577\n",
      "Subject 4, Epoch 969, Loss: 0.17222955077886581, Final Batch Loss: 0.07291233539581299\n",
      "Subject 4, Epoch 970, Loss: 0.10992508754134178, Final Batch Loss: 0.019150182604789734\n",
      "Subject 4, Epoch 971, Loss: 0.1740692052990198, Final Batch Loss: 0.12190840393304825\n",
      "Subject 4, Epoch 972, Loss: 0.10689137224107981, Final Batch Loss: 0.01070216204971075\n",
      "Subject 4, Epoch 973, Loss: 0.12159868516027927, Final Batch Loss: 0.038198765367269516\n",
      "Subject 4, Epoch 974, Loss: 0.15196039341390133, Final Batch Loss: 0.0785069614648819\n",
      "Subject 4, Epoch 975, Loss: 0.17091583833098412, Final Batch Loss: 0.04501880705356598\n",
      "Subject 4, Epoch 976, Loss: 0.1103037130087614, Final Batch Loss: 0.026528162881731987\n",
      "Subject 4, Epoch 977, Loss: 0.12932617962360382, Final Batch Loss: 0.04509810358285904\n",
      "Subject 4, Epoch 978, Loss: 0.3583476059138775, Final Batch Loss: 0.2399877905845642\n",
      "Subject 4, Epoch 979, Loss: 0.20316720381379128, Final Batch Loss: 0.11372293531894684\n",
      "Subject 4, Epoch 980, Loss: 0.08001295290887356, Final Batch Loss: 0.024929706007242203\n",
      "Subject 4, Epoch 981, Loss: 0.13827691785991192, Final Batch Loss: 0.07683564722537994\n",
      "Subject 4, Epoch 982, Loss: 0.23623614199459553, Final Batch Loss: 0.028396526351571083\n",
      "Subject 4, Epoch 983, Loss: 0.1316167302429676, Final Batch Loss: 0.04797805845737457\n",
      "Subject 4, Epoch 984, Loss: 0.07455732207745314, Final Batch Loss: 0.010712389834225178\n",
      "Subject 4, Epoch 985, Loss: 0.15900796093046665, Final Batch Loss: 0.06857798248529434\n",
      "Subject 4, Epoch 986, Loss: 0.13846717961132526, Final Batch Loss: 0.029863590374588966\n",
      "Subject 4, Epoch 987, Loss: 0.18751388788223267, Final Batch Loss: 0.04771796241402626\n",
      "Subject 4, Epoch 988, Loss: 0.098192248493433, Final Batch Loss: 0.05860332399606705\n",
      "Subject 4, Epoch 989, Loss: 0.2682955339550972, Final Batch Loss: 0.18843521177768707\n",
      "Subject 4, Epoch 990, Loss: 0.12874007411301136, Final Batch Loss: 0.03315049782395363\n",
      "Subject 4, Epoch 991, Loss: 0.10220143012702465, Final Batch Loss: 0.04563593491911888\n",
      "Subject 4, Epoch 992, Loss: 0.2067248336970806, Final Batch Loss: 0.06055962294340134\n",
      "Subject 4, Epoch 993, Loss: 0.10139819327741861, Final Batch Loss: 0.0361749567091465\n",
      "Subject 4, Epoch 994, Loss: 0.09529090113937855, Final Batch Loss: 0.037480421364307404\n",
      "Subject 4, Epoch 995, Loss: 0.08081291057169437, Final Batch Loss: 0.022352205589413643\n",
      "Subject 4, Epoch 996, Loss: 0.07936286181211472, Final Batch Loss: 0.028070246800780296\n",
      "Subject 4, Epoch 997, Loss: 0.08980145864188671, Final Batch Loss: 0.049914781004190445\n",
      "Subject 4, Epoch 998, Loss: 0.13584084622561932, Final Batch Loss: 0.012238366529345512\n",
      "Subject 4, Epoch 999, Loss: 0.09864127822220325, Final Batch Loss: 0.02964438684284687\n",
      "Subject 4, Epoch 1000, Loss: 0.10927534103393555, Final Batch Loss: 0.038253895938396454\n",
      "Subject 5, Epoch 1, Loss: 5.465373158454895, Final Batch Loss: 1.8078691959381104\n",
      "Subject 5, Epoch 2, Loss: 5.43788754940033, Final Batch Loss: 1.7989736795425415\n",
      "Subject 5, Epoch 3, Loss: 5.428699374198914, Final Batch Loss: 1.8040845394134521\n",
      "Subject 5, Epoch 4, Loss: 5.438638806343079, Final Batch Loss: 1.8370630741119385\n",
      "Subject 5, Epoch 5, Loss: 5.413823962211609, Final Batch Loss: 1.8223847150802612\n",
      "Subject 5, Epoch 6, Loss: 5.349431037902832, Final Batch Loss: 1.7504796981811523\n",
      "Subject 5, Epoch 7, Loss: 5.390451908111572, Final Batch Loss: 1.8488070964813232\n",
      "Subject 5, Epoch 8, Loss: 5.3280497789382935, Final Batch Loss: 1.783216953277588\n",
      "Subject 5, Epoch 9, Loss: 5.291401624679565, Final Batch Loss: 1.7606611251831055\n",
      "Subject 5, Epoch 10, Loss: 5.203986048698425, Final Batch Loss: 1.6855140924453735\n",
      "Subject 5, Epoch 11, Loss: 5.149733185768127, Final Batch Loss: 1.6814121007919312\n",
      "Subject 5, Epoch 12, Loss: 5.143576145172119, Final Batch Loss: 1.723557949066162\n",
      "Subject 5, Epoch 13, Loss: 5.065000653266907, Final Batch Loss: 1.6863596439361572\n",
      "Subject 5, Epoch 14, Loss: 4.992061138153076, Final Batch Loss: 1.6680270433425903\n",
      "Subject 5, Epoch 15, Loss: 4.821072101593018, Final Batch Loss: 1.5845584869384766\n",
      "Subject 5, Epoch 16, Loss: 4.72581672668457, Final Batch Loss: 1.5858794450759888\n",
      "Subject 5, Epoch 17, Loss: 4.5980610847473145, Final Batch Loss: 1.4871342182159424\n",
      "Subject 5, Epoch 18, Loss: 4.553091883659363, Final Batch Loss: 1.539491891860962\n",
      "Subject 5, Epoch 19, Loss: 4.535735607147217, Final Batch Loss: 1.5067836046218872\n",
      "Subject 5, Epoch 20, Loss: 4.433903813362122, Final Batch Loss: 1.5506030321121216\n",
      "Subject 5, Epoch 21, Loss: 4.316463947296143, Final Batch Loss: 1.5040258169174194\n",
      "Subject 5, Epoch 22, Loss: 4.093562960624695, Final Batch Loss: 1.2755762338638306\n",
      "Subject 5, Epoch 23, Loss: 4.197834610939026, Final Batch Loss: 1.3686256408691406\n",
      "Subject 5, Epoch 24, Loss: 4.239876866340637, Final Batch Loss: 1.4125301837921143\n",
      "Subject 5, Epoch 25, Loss: 4.145580410957336, Final Batch Loss: 1.3258767127990723\n",
      "Subject 5, Epoch 26, Loss: 4.184362769126892, Final Batch Loss: 1.4504294395446777\n",
      "Subject 5, Epoch 27, Loss: 4.161868214607239, Final Batch Loss: 1.3759126663208008\n",
      "Subject 5, Epoch 28, Loss: 4.12093985080719, Final Batch Loss: 1.4477030038833618\n",
      "Subject 5, Epoch 29, Loss: 4.000101089477539, Final Batch Loss: 1.3443450927734375\n",
      "Subject 5, Epoch 30, Loss: 4.017330288887024, Final Batch Loss: 1.3631404638290405\n",
      "Subject 5, Epoch 31, Loss: 3.9861347675323486, Final Batch Loss: 1.348102331161499\n",
      "Subject 5, Epoch 32, Loss: 3.95844042301178, Final Batch Loss: 1.269963026046753\n",
      "Subject 5, Epoch 33, Loss: 3.9297295808792114, Final Batch Loss: 1.3289179801940918\n",
      "Subject 5, Epoch 34, Loss: 3.953251004219055, Final Batch Loss: 1.3880796432495117\n",
      "Subject 5, Epoch 35, Loss: 3.795681953430176, Final Batch Loss: 1.2393789291381836\n",
      "Subject 5, Epoch 36, Loss: 3.7428600788116455, Final Batch Loss: 1.2223973274230957\n",
      "Subject 5, Epoch 37, Loss: 3.80718731880188, Final Batch Loss: 1.2640788555145264\n",
      "Subject 5, Epoch 38, Loss: 3.7470306158065796, Final Batch Loss: 1.2340092658996582\n",
      "Subject 5, Epoch 39, Loss: 3.70532763004303, Final Batch Loss: 1.2234652042388916\n",
      "Subject 5, Epoch 40, Loss: 3.7283610105514526, Final Batch Loss: 1.2808122634887695\n",
      "Subject 5, Epoch 41, Loss: 3.67607843875885, Final Batch Loss: 1.2355471849441528\n",
      "Subject 5, Epoch 42, Loss: 3.577559232711792, Final Batch Loss: 1.1525062322616577\n",
      "Subject 5, Epoch 43, Loss: 3.6500390768051147, Final Batch Loss: 1.2208865880966187\n",
      "Subject 5, Epoch 44, Loss: 3.5900617837905884, Final Batch Loss: 1.2052725553512573\n",
      "Subject 5, Epoch 45, Loss: 3.4866175651550293, Final Batch Loss: 1.1321851015090942\n",
      "Subject 5, Epoch 46, Loss: 3.483409881591797, Final Batch Loss: 1.17119562625885\n",
      "Subject 5, Epoch 47, Loss: 3.6223849058151245, Final Batch Loss: 1.1997334957122803\n",
      "Subject 5, Epoch 48, Loss: 3.56334125995636, Final Batch Loss: 1.2460136413574219\n",
      "Subject 5, Epoch 49, Loss: 3.4382399320602417, Final Batch Loss: 1.1235647201538086\n",
      "Subject 5, Epoch 50, Loss: 3.4555447101593018, Final Batch Loss: 1.1196995973587036\n",
      "Subject 5, Epoch 51, Loss: 3.4959033727645874, Final Batch Loss: 1.1467989683151245\n",
      "Subject 5, Epoch 52, Loss: 3.4213006496429443, Final Batch Loss: 1.1216496229171753\n",
      "Subject 5, Epoch 53, Loss: 3.434695839881897, Final Batch Loss: 1.1913306713104248\n",
      "Subject 5, Epoch 54, Loss: 3.3984588384628296, Final Batch Loss: 1.1194472312927246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 55, Loss: 3.5131092071533203, Final Batch Loss: 1.166961908340454\n",
      "Subject 5, Epoch 56, Loss: 3.366551399230957, Final Batch Loss: 1.1006959676742554\n",
      "Subject 5, Epoch 57, Loss: 3.500925064086914, Final Batch Loss: 1.1546717882156372\n",
      "Subject 5, Epoch 58, Loss: 3.35578715801239, Final Batch Loss: 1.0744646787643433\n",
      "Subject 5, Epoch 59, Loss: 3.4354090690612793, Final Batch Loss: 1.1702181100845337\n",
      "Subject 5, Epoch 60, Loss: 3.5793254375457764, Final Batch Loss: 1.2522929906845093\n",
      "Subject 5, Epoch 61, Loss: 3.4396657943725586, Final Batch Loss: 1.15050208568573\n",
      "Subject 5, Epoch 62, Loss: 3.3721444606781006, Final Batch Loss: 1.1120818853378296\n",
      "Subject 5, Epoch 63, Loss: 3.3527077436447144, Final Batch Loss: 1.0823285579681396\n",
      "Subject 5, Epoch 64, Loss: 3.383143186569214, Final Batch Loss: 1.1107935905456543\n",
      "Subject 5, Epoch 65, Loss: 3.3818466663360596, Final Batch Loss: 1.1854661703109741\n",
      "Subject 5, Epoch 66, Loss: 3.328871488571167, Final Batch Loss: 1.085005521774292\n",
      "Subject 5, Epoch 67, Loss: 3.3584402799606323, Final Batch Loss: 1.1398745775222778\n",
      "Subject 5, Epoch 68, Loss: 3.374966025352478, Final Batch Loss: 1.1313714981079102\n",
      "Subject 5, Epoch 69, Loss: 3.3476771116256714, Final Batch Loss: 1.1087703704833984\n",
      "Subject 5, Epoch 70, Loss: 3.4343544244766235, Final Batch Loss: 1.1585397720336914\n",
      "Subject 5, Epoch 71, Loss: 3.370855927467346, Final Batch Loss: 1.1407746076583862\n",
      "Subject 5, Epoch 72, Loss: 3.374588131904602, Final Batch Loss: 1.1024980545043945\n",
      "Subject 5, Epoch 73, Loss: 3.2974050045013428, Final Batch Loss: 1.099182367324829\n",
      "Subject 5, Epoch 74, Loss: 3.3118348121643066, Final Batch Loss: 1.0735769271850586\n",
      "Subject 5, Epoch 75, Loss: 3.220309615135193, Final Batch Loss: 1.049357533454895\n",
      "Subject 5, Epoch 76, Loss: 3.3687814474105835, Final Batch Loss: 1.134481430053711\n",
      "Subject 5, Epoch 77, Loss: 3.2993704080581665, Final Batch Loss: 1.0888702869415283\n",
      "Subject 5, Epoch 78, Loss: 3.1966923475265503, Final Batch Loss: 1.0351800918579102\n",
      "Subject 5, Epoch 79, Loss: 3.2424209117889404, Final Batch Loss: 1.0810883045196533\n",
      "Subject 5, Epoch 80, Loss: 3.2443875074386597, Final Batch Loss: 1.0228289365768433\n",
      "Subject 5, Epoch 81, Loss: 3.2250359058380127, Final Batch Loss: 1.056065559387207\n",
      "Subject 5, Epoch 82, Loss: 3.1417489051818848, Final Batch Loss: 1.030784249305725\n",
      "Subject 5, Epoch 83, Loss: 3.145762324333191, Final Batch Loss: 1.0433169603347778\n",
      "Subject 5, Epoch 84, Loss: 3.208682656288147, Final Batch Loss: 1.0901899337768555\n",
      "Subject 5, Epoch 85, Loss: 3.074812412261963, Final Batch Loss: 1.0083743333816528\n",
      "Subject 5, Epoch 86, Loss: 3.1790287494659424, Final Batch Loss: 1.0866191387176514\n",
      "Subject 5, Epoch 87, Loss: 3.1683101654052734, Final Batch Loss: 1.0772523880004883\n",
      "Subject 5, Epoch 88, Loss: 3.0662792921066284, Final Batch Loss: 1.0276581048965454\n",
      "Subject 5, Epoch 89, Loss: 2.9631094932556152, Final Batch Loss: 0.9239026308059692\n",
      "Subject 5, Epoch 90, Loss: 3.0001092553138733, Final Batch Loss: 0.9911720156669617\n",
      "Subject 5, Epoch 91, Loss: 3.063399314880371, Final Batch Loss: 1.0464909076690674\n",
      "Subject 5, Epoch 92, Loss: 2.9718682765960693, Final Batch Loss: 1.0558767318725586\n",
      "Subject 5, Epoch 93, Loss: 2.9033302068710327, Final Batch Loss: 0.9216881990432739\n",
      "Subject 5, Epoch 94, Loss: 3.0458986163139343, Final Batch Loss: 1.08126699924469\n",
      "Subject 5, Epoch 95, Loss: 2.9112337827682495, Final Batch Loss: 0.960260272026062\n",
      "Subject 5, Epoch 96, Loss: 2.998715400695801, Final Batch Loss: 1.0490518808364868\n",
      "Subject 5, Epoch 97, Loss: 2.9802133440971375, Final Batch Loss: 1.05429208278656\n",
      "Subject 5, Epoch 98, Loss: 2.9421282410621643, Final Batch Loss: 0.9990951418876648\n",
      "Subject 5, Epoch 99, Loss: 2.866999387741089, Final Batch Loss: 0.9129784107208252\n",
      "Subject 5, Epoch 100, Loss: 2.75210702419281, Final Batch Loss: 0.8889729380607605\n",
      "Subject 5, Epoch 101, Loss: 2.909270942211151, Final Batch Loss: 1.0252845287322998\n",
      "Subject 5, Epoch 102, Loss: 2.7336941957473755, Final Batch Loss: 0.8043977618217468\n",
      "Subject 5, Epoch 103, Loss: 2.7293558716773987, Final Batch Loss: 0.8558558225631714\n",
      "Subject 5, Epoch 104, Loss: 2.8123409152030945, Final Batch Loss: 0.9959567189216614\n",
      "Subject 5, Epoch 105, Loss: 2.9128878116607666, Final Batch Loss: 1.043197512626648\n",
      "Subject 5, Epoch 106, Loss: 2.9904098510742188, Final Batch Loss: 1.0589611530303955\n",
      "Subject 5, Epoch 107, Loss: 2.7836341857910156, Final Batch Loss: 0.8714907765388489\n",
      "Subject 5, Epoch 108, Loss: 2.7923592925071716, Final Batch Loss: 0.926521897315979\n",
      "Subject 5, Epoch 109, Loss: 2.70430988073349, Final Batch Loss: 0.9278130531311035\n",
      "Subject 5, Epoch 110, Loss: 2.6967005729675293, Final Batch Loss: 0.8894113302230835\n",
      "Subject 5, Epoch 111, Loss: 2.6537081599235535, Final Batch Loss: 0.8585193753242493\n",
      "Subject 5, Epoch 112, Loss: 2.6733486652374268, Final Batch Loss: 0.8981944918632507\n",
      "Subject 5, Epoch 113, Loss: 2.643971860408783, Final Batch Loss: 0.8653203248977661\n",
      "Subject 5, Epoch 114, Loss: 2.7321178317070007, Final Batch Loss: 0.9937739372253418\n",
      "Subject 5, Epoch 115, Loss: 2.76718932390213, Final Batch Loss: 0.9657798409461975\n",
      "Subject 5, Epoch 116, Loss: 2.5713216066360474, Final Batch Loss: 0.8788551688194275\n",
      "Subject 5, Epoch 117, Loss: 2.671072542667389, Final Batch Loss: 0.8250876665115356\n",
      "Subject 5, Epoch 118, Loss: 2.5942060947418213, Final Batch Loss: 0.8430469036102295\n",
      "Subject 5, Epoch 119, Loss: 2.5891008973121643, Final Batch Loss: 0.8031716346740723\n",
      "Subject 5, Epoch 120, Loss: 2.6514798998832703, Final Batch Loss: 0.8993766903877258\n",
      "Subject 5, Epoch 121, Loss: 2.67403644323349, Final Batch Loss: 0.9615069031715393\n",
      "Subject 5, Epoch 122, Loss: 2.5232056379318237, Final Batch Loss: 0.8476923704147339\n",
      "Subject 5, Epoch 123, Loss: 2.585939109325409, Final Batch Loss: 0.8848277926445007\n",
      "Subject 5, Epoch 124, Loss: 2.551911473274231, Final Batch Loss: 0.8856568336486816\n",
      "Subject 5, Epoch 125, Loss: 2.4605661630630493, Final Batch Loss: 0.6946568489074707\n",
      "Subject 5, Epoch 126, Loss: 2.508687376976013, Final Batch Loss: 0.905663251876831\n",
      "Subject 5, Epoch 127, Loss: 2.4144200682640076, Final Batch Loss: 0.7041932344436646\n",
      "Subject 5, Epoch 128, Loss: 2.5842076539993286, Final Batch Loss: 0.8834556937217712\n",
      "Subject 5, Epoch 129, Loss: 2.43917053937912, Final Batch Loss: 0.7585274577140808\n",
      "Subject 5, Epoch 130, Loss: 2.4170175790786743, Final Batch Loss: 0.7392295598983765\n",
      "Subject 5, Epoch 131, Loss: 2.4701528549194336, Final Batch Loss: 0.8334596753120422\n",
      "Subject 5, Epoch 132, Loss: 2.5840169191360474, Final Batch Loss: 0.9363004565238953\n",
      "Subject 5, Epoch 133, Loss: 2.552164912223816, Final Batch Loss: 0.871425211429596\n",
      "Subject 5, Epoch 134, Loss: 2.5377228260040283, Final Batch Loss: 0.8784781098365784\n",
      "Subject 5, Epoch 135, Loss: 2.4237337708473206, Final Batch Loss: 0.8903167247772217\n",
      "Subject 5, Epoch 136, Loss: 2.4880277514457703, Final Batch Loss: 0.8200504183769226\n",
      "Subject 5, Epoch 137, Loss: 2.428195297718048, Final Batch Loss: 0.7866595387458801\n",
      "Subject 5, Epoch 138, Loss: 2.476157307624817, Final Batch Loss: 0.8587241768836975\n",
      "Subject 5, Epoch 139, Loss: 2.3788062930107117, Final Batch Loss: 0.7466261386871338\n",
      "Subject 5, Epoch 140, Loss: 2.3860605359077454, Final Batch Loss: 0.8450108766555786\n",
      "Subject 5, Epoch 141, Loss: 2.45278137922287, Final Batch Loss: 0.8627271056175232\n",
      "Subject 5, Epoch 142, Loss: 2.2623841762542725, Final Batch Loss: 0.7549716830253601\n",
      "Subject 5, Epoch 143, Loss: 2.3862724900245667, Final Batch Loss: 0.7191594243049622\n",
      "Subject 5, Epoch 144, Loss: 2.2099475264549255, Final Batch Loss: 0.6475026607513428\n",
      "Subject 5, Epoch 145, Loss: 2.31010639667511, Final Batch Loss: 0.758381724357605\n",
      "Subject 5, Epoch 146, Loss: 2.4445930123329163, Final Batch Loss: 0.9051591753959656\n",
      "Subject 5, Epoch 147, Loss: 2.2793519496917725, Final Batch Loss: 0.8015568256378174\n",
      "Subject 5, Epoch 148, Loss: 2.2753313779830933, Final Batch Loss: 0.6923956871032715\n",
      "Subject 5, Epoch 149, Loss: 2.3727265000343323, Final Batch Loss: 0.8138655424118042\n",
      "Subject 5, Epoch 150, Loss: 2.251661956310272, Final Batch Loss: 0.7549378871917725\n",
      "Subject 5, Epoch 151, Loss: 2.2178947925567627, Final Batch Loss: 0.6478216052055359\n",
      "Subject 5, Epoch 152, Loss: 2.366147816181183, Final Batch Loss: 0.8648284077644348\n",
      "Subject 5, Epoch 153, Loss: 2.146317183971405, Final Batch Loss: 0.665857195854187\n",
      "Subject 5, Epoch 154, Loss: 2.0315971970558167, Final Batch Loss: 0.6100437045097351\n",
      "Subject 5, Epoch 155, Loss: 2.0767117738723755, Final Batch Loss: 0.6074532270431519\n",
      "Subject 5, Epoch 156, Loss: 2.2062610387802124, Final Batch Loss: 0.6703793406486511\n",
      "Subject 5, Epoch 157, Loss: 2.147365391254425, Final Batch Loss: 0.7090335488319397\n",
      "Subject 5, Epoch 158, Loss: 2.3211095333099365, Final Batch Loss: 0.7502506971359253\n",
      "Subject 5, Epoch 159, Loss: 2.1601869463920593, Final Batch Loss: 0.8828810453414917\n",
      "Subject 5, Epoch 160, Loss: 2.127017319202423, Final Batch Loss: 0.6869681477546692\n",
      "Subject 5, Epoch 161, Loss: 2.158909261226654, Final Batch Loss: 0.7887886166572571\n",
      "Subject 5, Epoch 162, Loss: 2.0778165459632874, Final Batch Loss: 0.5699970126152039\n",
      "Subject 5, Epoch 163, Loss: 2.16839998960495, Final Batch Loss: 0.7273440361022949\n",
      "Subject 5, Epoch 164, Loss: 1.934040367603302, Final Batch Loss: 0.5450305342674255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 165, Loss: 1.9708083271980286, Final Batch Loss: 0.5539859533309937\n",
      "Subject 5, Epoch 166, Loss: 2.1346095204353333, Final Batch Loss: 0.6903640031814575\n",
      "Subject 5, Epoch 167, Loss: 1.98152095079422, Final Batch Loss: 0.5682222247123718\n",
      "Subject 5, Epoch 168, Loss: 2.1076993346214294, Final Batch Loss: 0.6491621136665344\n",
      "Subject 5, Epoch 169, Loss: 1.9084455370903015, Final Batch Loss: 0.5251083970069885\n",
      "Subject 5, Epoch 170, Loss: 1.9146499037742615, Final Batch Loss: 0.5653024911880493\n",
      "Subject 5, Epoch 171, Loss: 1.9109375476837158, Final Batch Loss: 0.7117687463760376\n",
      "Subject 5, Epoch 172, Loss: 2.032296657562256, Final Batch Loss: 0.667360782623291\n",
      "Subject 5, Epoch 173, Loss: 2.0734811425209045, Final Batch Loss: 0.7040127515792847\n",
      "Subject 5, Epoch 174, Loss: 1.9735512137413025, Final Batch Loss: 0.7358291149139404\n",
      "Subject 5, Epoch 175, Loss: 1.9261707663536072, Final Batch Loss: 0.6614843010902405\n",
      "Subject 5, Epoch 176, Loss: 1.8996642231941223, Final Batch Loss: 0.6630252003669739\n",
      "Subject 5, Epoch 177, Loss: 1.7211992144584656, Final Batch Loss: 0.5557834506034851\n",
      "Subject 5, Epoch 178, Loss: 2.0744028091430664, Final Batch Loss: 0.8944144248962402\n",
      "Subject 5, Epoch 179, Loss: 1.7599751949310303, Final Batch Loss: 0.6647076606750488\n",
      "Subject 5, Epoch 180, Loss: 1.8506592512130737, Final Batch Loss: 0.6473255157470703\n",
      "Subject 5, Epoch 181, Loss: 2.0300751328468323, Final Batch Loss: 0.7443674206733704\n",
      "Subject 5, Epoch 182, Loss: 1.890072226524353, Final Batch Loss: 0.6389897465705872\n",
      "Subject 5, Epoch 183, Loss: 1.8086850047111511, Final Batch Loss: 0.595500648021698\n",
      "Subject 5, Epoch 184, Loss: 1.7890167832374573, Final Batch Loss: 0.6207032799720764\n",
      "Subject 5, Epoch 185, Loss: 1.844442069530487, Final Batch Loss: 0.5806037187576294\n",
      "Subject 5, Epoch 186, Loss: 1.8363466262817383, Final Batch Loss: 0.5998464226722717\n",
      "Subject 5, Epoch 187, Loss: 1.8428797125816345, Final Batch Loss: 0.7207686901092529\n",
      "Subject 5, Epoch 188, Loss: 1.8067859411239624, Final Batch Loss: 0.632463812828064\n",
      "Subject 5, Epoch 189, Loss: 1.6947796940803528, Final Batch Loss: 0.537655234336853\n",
      "Subject 5, Epoch 190, Loss: 1.7814407348632812, Final Batch Loss: 0.6188616752624512\n",
      "Subject 5, Epoch 191, Loss: 1.7532735168933868, Final Batch Loss: 0.49700912833213806\n",
      "Subject 5, Epoch 192, Loss: 1.7266502380371094, Final Batch Loss: 0.6089008450508118\n",
      "Subject 5, Epoch 193, Loss: 1.8130557537078857, Final Batch Loss: 0.6214518547058105\n",
      "Subject 5, Epoch 194, Loss: 1.669442355632782, Final Batch Loss: 0.5055498480796814\n",
      "Subject 5, Epoch 195, Loss: 1.516950637102127, Final Batch Loss: 0.46479490399360657\n",
      "Subject 5, Epoch 196, Loss: 1.5515475571155548, Final Batch Loss: 0.39556965231895447\n",
      "Subject 5, Epoch 197, Loss: 1.8586450219154358, Final Batch Loss: 0.6896508932113647\n",
      "Subject 5, Epoch 198, Loss: 1.5368824303150177, Final Batch Loss: 0.44034042954444885\n",
      "Subject 5, Epoch 199, Loss: 1.5676167905330658, Final Batch Loss: 0.4508068263530731\n",
      "Subject 5, Epoch 200, Loss: 1.6878294348716736, Final Batch Loss: 0.6798638701438904\n",
      "Subject 5, Epoch 201, Loss: 1.7206270098686218, Final Batch Loss: 0.656159520149231\n",
      "Subject 5, Epoch 202, Loss: 1.5254852771759033, Final Batch Loss: 0.5046271681785583\n",
      "Subject 5, Epoch 203, Loss: 1.6379457712173462, Final Batch Loss: 0.5376147031784058\n",
      "Subject 5, Epoch 204, Loss: 1.8099250793457031, Final Batch Loss: 0.634900689125061\n",
      "Subject 5, Epoch 205, Loss: 1.6883001923561096, Final Batch Loss: 0.4902363419532776\n",
      "Subject 5, Epoch 206, Loss: 1.6188260912895203, Final Batch Loss: 0.5433472394943237\n",
      "Subject 5, Epoch 207, Loss: 1.569780558347702, Final Batch Loss: 0.5216197967529297\n",
      "Subject 5, Epoch 208, Loss: 1.5879886746406555, Final Batch Loss: 0.5527435541152954\n",
      "Subject 5, Epoch 209, Loss: 1.5326376259326935, Final Batch Loss: 0.4844905436038971\n",
      "Subject 5, Epoch 210, Loss: 1.440150886774063, Final Batch Loss: 0.3990389406681061\n",
      "Subject 5, Epoch 211, Loss: 1.624742031097412, Final Batch Loss: 0.5509487390518188\n",
      "Subject 5, Epoch 212, Loss: 1.5286035239696503, Final Batch Loss: 0.48865392804145813\n",
      "Subject 5, Epoch 213, Loss: 1.810398280620575, Final Batch Loss: 0.8228817582130432\n",
      "Subject 5, Epoch 214, Loss: 1.4822464883327484, Final Batch Loss: 0.3991047143936157\n",
      "Subject 5, Epoch 215, Loss: 1.477052241563797, Final Batch Loss: 0.4667864739894867\n",
      "Subject 5, Epoch 216, Loss: 1.8235706686973572, Final Batch Loss: 0.7467703819274902\n",
      "Subject 5, Epoch 217, Loss: 1.4594310224056244, Final Batch Loss: 0.39511850476264954\n",
      "Subject 5, Epoch 218, Loss: 1.7109705805778503, Final Batch Loss: 0.771929919719696\n",
      "Subject 5, Epoch 219, Loss: 1.6584206223487854, Final Batch Loss: 0.5698108673095703\n",
      "Subject 5, Epoch 220, Loss: 1.3954647183418274, Final Batch Loss: 0.4224400222301483\n",
      "Subject 5, Epoch 221, Loss: 1.5806346833705902, Final Batch Loss: 0.642382025718689\n",
      "Subject 5, Epoch 222, Loss: 1.4643571078777313, Final Batch Loss: 0.45344123244285583\n",
      "Subject 5, Epoch 223, Loss: 1.4970312714576721, Final Batch Loss: 0.4512675404548645\n",
      "Subject 5, Epoch 224, Loss: 1.4713408052921295, Final Batch Loss: 0.4409054219722748\n",
      "Subject 5, Epoch 225, Loss: 1.7133828997612, Final Batch Loss: 0.6124231815338135\n",
      "Subject 5, Epoch 226, Loss: 1.422063559293747, Final Batch Loss: 0.4696713089942932\n",
      "Subject 5, Epoch 227, Loss: 1.5593172013759613, Final Batch Loss: 0.6338098049163818\n",
      "Subject 5, Epoch 228, Loss: 1.6000811755657196, Final Batch Loss: 0.5964555144309998\n",
      "Subject 5, Epoch 229, Loss: 1.5527572333812714, Final Batch Loss: 0.5867540240287781\n",
      "Subject 5, Epoch 230, Loss: 1.4595355093479156, Final Batch Loss: 0.42529258131980896\n",
      "Subject 5, Epoch 231, Loss: 1.4682062268257141, Final Batch Loss: 0.5272951722145081\n",
      "Subject 5, Epoch 232, Loss: 1.5043669044971466, Final Batch Loss: 0.5003103613853455\n",
      "Subject 5, Epoch 233, Loss: 1.5766462981700897, Final Batch Loss: 0.5612285733222961\n",
      "Subject 5, Epoch 234, Loss: 1.3586813807487488, Final Batch Loss: 0.405199259519577\n",
      "Subject 5, Epoch 235, Loss: 1.3900845646858215, Final Batch Loss: 0.40682289004325867\n",
      "Subject 5, Epoch 236, Loss: 1.3519597053527832, Final Batch Loss: 0.46136289834976196\n",
      "Subject 5, Epoch 237, Loss: 1.3852091133594513, Final Batch Loss: 0.41428452730178833\n",
      "Subject 5, Epoch 238, Loss: 1.556060016155243, Final Batch Loss: 0.5710495114326477\n",
      "Subject 5, Epoch 239, Loss: 1.3653962314128876, Final Batch Loss: 0.43696916103363037\n",
      "Subject 5, Epoch 240, Loss: 1.4494804739952087, Final Batch Loss: 0.5255060791969299\n",
      "Subject 5, Epoch 241, Loss: 1.233563482761383, Final Batch Loss: 0.29822948575019836\n",
      "Subject 5, Epoch 242, Loss: 1.307460367679596, Final Batch Loss: 0.3510550558567047\n",
      "Subject 5, Epoch 243, Loss: 1.293634980916977, Final Batch Loss: 0.4093836545944214\n",
      "Subject 5, Epoch 244, Loss: 1.142970472574234, Final Batch Loss: 0.287126749753952\n",
      "Subject 5, Epoch 245, Loss: 1.5760228335857391, Final Batch Loss: 0.659177303314209\n",
      "Subject 5, Epoch 246, Loss: 1.4592842161655426, Final Batch Loss: 0.467802494764328\n",
      "Subject 5, Epoch 247, Loss: 1.2101945877075195, Final Batch Loss: 0.41922250390052795\n",
      "Subject 5, Epoch 248, Loss: 1.241336703300476, Final Batch Loss: 0.3466360569000244\n",
      "Subject 5, Epoch 249, Loss: 1.3201891481876373, Final Batch Loss: 0.43452879786491394\n",
      "Subject 5, Epoch 250, Loss: 1.2300401031970978, Final Batch Loss: 0.4024077355861664\n",
      "Subject 5, Epoch 251, Loss: 1.295322299003601, Final Batch Loss: 0.4805055856704712\n",
      "Subject 5, Epoch 252, Loss: 1.3770037293434143, Final Batch Loss: 0.45989251136779785\n",
      "Subject 5, Epoch 253, Loss: 1.2897785902023315, Final Batch Loss: 0.503870964050293\n",
      "Subject 5, Epoch 254, Loss: 1.4431393146514893, Final Batch Loss: 0.47615131735801697\n",
      "Subject 5, Epoch 255, Loss: 1.3324343860149384, Final Batch Loss: 0.4373239278793335\n",
      "Subject 5, Epoch 256, Loss: 1.4189150631427765, Final Batch Loss: 0.5250410437583923\n",
      "Subject 5, Epoch 257, Loss: 1.3409992158412933, Final Batch Loss: 0.5419303178787231\n",
      "Subject 5, Epoch 258, Loss: 1.2945700287818909, Final Batch Loss: 0.36868083477020264\n",
      "Subject 5, Epoch 259, Loss: 1.2312526404857635, Final Batch Loss: 0.4216248393058777\n",
      "Subject 5, Epoch 260, Loss: 1.4417808651924133, Final Batch Loss: 0.6203983426094055\n",
      "Subject 5, Epoch 261, Loss: 1.2054166495800018, Final Batch Loss: 0.3192671537399292\n",
      "Subject 5, Epoch 262, Loss: 1.3660555183887482, Final Batch Loss: 0.44017690420150757\n",
      "Subject 5, Epoch 263, Loss: 1.1601090729236603, Final Batch Loss: 0.339179128408432\n",
      "Subject 5, Epoch 264, Loss: 1.266895592212677, Final Batch Loss: 0.46184268593788147\n",
      "Subject 5, Epoch 265, Loss: 1.2376374006271362, Final Batch Loss: 0.4236840605735779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 266, Loss: 1.1919967830181122, Final Batch Loss: 0.43238645792007446\n",
      "Subject 5, Epoch 267, Loss: 1.2271841168403625, Final Batch Loss: 0.38698846101760864\n",
      "Subject 5, Epoch 268, Loss: 1.0448283404111862, Final Batch Loss: 0.21432916820049286\n",
      "Subject 5, Epoch 269, Loss: 1.194521814584732, Final Batch Loss: 0.3979763686656952\n",
      "Subject 5, Epoch 270, Loss: 1.127712607383728, Final Batch Loss: 0.3278382122516632\n",
      "Subject 5, Epoch 271, Loss: 1.0868556499481201, Final Batch Loss: 0.3051634728908539\n",
      "Subject 5, Epoch 272, Loss: 1.1937694549560547, Final Batch Loss: 0.37364712357521057\n",
      "Subject 5, Epoch 273, Loss: 1.3461561501026154, Final Batch Loss: 0.48916053771972656\n",
      "Subject 5, Epoch 274, Loss: 1.0155536830425262, Final Batch Loss: 0.23444843292236328\n",
      "Subject 5, Epoch 275, Loss: 1.204433798789978, Final Batch Loss: 0.35135185718536377\n",
      "Subject 5, Epoch 276, Loss: 1.23141747713089, Final Batch Loss: 0.4409221410751343\n",
      "Subject 5, Epoch 277, Loss: 1.2255369424819946, Final Batch Loss: 0.4704302251338959\n",
      "Subject 5, Epoch 278, Loss: 1.2321953773498535, Final Batch Loss: 0.4499473571777344\n",
      "Subject 5, Epoch 279, Loss: 1.0638588070869446, Final Batch Loss: 0.3361271917819977\n",
      "Subject 5, Epoch 280, Loss: 1.0811273455619812, Final Batch Loss: 0.35398322343826294\n",
      "Subject 5, Epoch 281, Loss: 1.0366136729717255, Final Batch Loss: 0.3493707478046417\n",
      "Subject 5, Epoch 282, Loss: 1.2461210489273071, Final Batch Loss: 0.4941408634185791\n",
      "Subject 5, Epoch 283, Loss: 1.320462852716446, Final Batch Loss: 0.5855428576469421\n",
      "Subject 5, Epoch 284, Loss: 1.1581109166145325, Final Batch Loss: 0.4951689839363098\n",
      "Subject 5, Epoch 285, Loss: 1.1376499235630035, Final Batch Loss: 0.30410778522491455\n",
      "Subject 5, Epoch 286, Loss: 1.00003582239151, Final Batch Loss: 0.4396142363548279\n",
      "Subject 5, Epoch 287, Loss: 1.0564798414707184, Final Batch Loss: 0.40137478709220886\n",
      "Subject 5, Epoch 288, Loss: 1.0498763918876648, Final Batch Loss: 0.3542875051498413\n",
      "Subject 5, Epoch 289, Loss: 0.9815177023410797, Final Batch Loss: 0.293385773897171\n",
      "Subject 5, Epoch 290, Loss: 1.1048941910266876, Final Batch Loss: 0.3235546052455902\n",
      "Subject 5, Epoch 291, Loss: 1.1720341742038727, Final Batch Loss: 0.3264942169189453\n",
      "Subject 5, Epoch 292, Loss: 1.1073870807886124, Final Batch Loss: 0.24611957371234894\n",
      "Subject 5, Epoch 293, Loss: 1.0548256933689117, Final Batch Loss: 0.32859936356544495\n",
      "Subject 5, Epoch 294, Loss: 1.1199145019054413, Final Batch Loss: 0.4567282199859619\n",
      "Subject 5, Epoch 295, Loss: 1.2079043984413147, Final Batch Loss: 0.5261331796646118\n",
      "Subject 5, Epoch 296, Loss: 1.0304476916790009, Final Batch Loss: 0.35646143555641174\n",
      "Subject 5, Epoch 297, Loss: 0.9877028465270996, Final Batch Loss: 0.31748166680336\n",
      "Subject 5, Epoch 298, Loss: 1.0739482939243317, Final Batch Loss: 0.3301618993282318\n",
      "Subject 5, Epoch 299, Loss: 1.0276488363742828, Final Batch Loss: 0.3609669506549835\n",
      "Subject 5, Epoch 300, Loss: 1.13950714468956, Final Batch Loss: 0.3731077015399933\n",
      "Subject 5, Epoch 301, Loss: 1.0154758393764496, Final Batch Loss: 0.3258129954338074\n",
      "Subject 5, Epoch 302, Loss: 1.1050056219100952, Final Batch Loss: 0.3604287803173065\n",
      "Subject 5, Epoch 303, Loss: 1.0514263212680817, Final Batch Loss: 0.39501461386680603\n",
      "Subject 5, Epoch 304, Loss: 1.042999267578125, Final Batch Loss: 0.408674418926239\n",
      "Subject 5, Epoch 305, Loss: 0.9046385884284973, Final Batch Loss: 0.2616129517555237\n",
      "Subject 5, Epoch 306, Loss: 0.8611356467008591, Final Batch Loss: 0.23113436996936798\n",
      "Subject 5, Epoch 307, Loss: 0.9267286956310272, Final Batch Loss: 0.3020523488521576\n",
      "Subject 5, Epoch 308, Loss: 1.1524188220500946, Final Batch Loss: 0.495425820350647\n",
      "Subject 5, Epoch 309, Loss: 1.1080071330070496, Final Batch Loss: 0.32020893692970276\n",
      "Subject 5, Epoch 310, Loss: 1.014070451259613, Final Batch Loss: 0.38266825675964355\n",
      "Subject 5, Epoch 311, Loss: 0.8391948044300079, Final Batch Loss: 0.1644618809223175\n",
      "Subject 5, Epoch 312, Loss: 0.854438453912735, Final Batch Loss: 0.27784672379493713\n",
      "Subject 5, Epoch 313, Loss: 0.9557638019323349, Final Batch Loss: 0.2155061513185501\n",
      "Subject 5, Epoch 314, Loss: 0.9810652285814285, Final Batch Loss: 0.5067902207374573\n",
      "Subject 5, Epoch 315, Loss: 0.8201042711734772, Final Batch Loss: 0.21825391054153442\n",
      "Subject 5, Epoch 316, Loss: 0.9797987043857574, Final Batch Loss: 0.32447394728660583\n",
      "Subject 5, Epoch 317, Loss: 1.068228006362915, Final Batch Loss: 0.45033395290374756\n",
      "Subject 5, Epoch 318, Loss: 1.0340900123119354, Final Batch Loss: 0.4950878620147705\n",
      "Subject 5, Epoch 319, Loss: 0.8394626975059509, Final Batch Loss: 0.2185308039188385\n",
      "Subject 5, Epoch 320, Loss: 1.1428182423114777, Final Batch Loss: 0.4278295040130615\n",
      "Subject 5, Epoch 321, Loss: 1.0505672991275787, Final Batch Loss: 0.3682570457458496\n",
      "Subject 5, Epoch 322, Loss: 1.0053139626979828, Final Batch Loss: 0.2930687963962555\n",
      "Subject 5, Epoch 323, Loss: 0.9702226370573044, Final Batch Loss: 0.33384522795677185\n",
      "Subject 5, Epoch 324, Loss: 0.9078069627285004, Final Batch Loss: 0.29286131262779236\n",
      "Subject 5, Epoch 325, Loss: 0.9742392599582672, Final Batch Loss: 0.38223621249198914\n",
      "Subject 5, Epoch 326, Loss: 0.8724952191114426, Final Batch Loss: 0.33991849422454834\n",
      "Subject 5, Epoch 327, Loss: 0.8756904304027557, Final Batch Loss: 0.2849813401699066\n",
      "Subject 5, Epoch 328, Loss: 0.9378919303417206, Final Batch Loss: 0.30510640144348145\n",
      "Subject 5, Epoch 329, Loss: 0.7922310531139374, Final Batch Loss: 0.27769362926483154\n",
      "Subject 5, Epoch 330, Loss: 0.6986186653375626, Final Batch Loss: 0.2543289065361023\n",
      "Subject 5, Epoch 331, Loss: 0.8019032925367355, Final Batch Loss: 0.2203083485364914\n",
      "Subject 5, Epoch 332, Loss: 0.8822112828493118, Final Batch Loss: 0.241611048579216\n",
      "Subject 5, Epoch 333, Loss: 0.9470738768577576, Final Batch Loss: 0.37379589676856995\n",
      "Subject 5, Epoch 334, Loss: 0.7904014140367508, Final Batch Loss: 0.24228033423423767\n",
      "Subject 5, Epoch 335, Loss: 0.9299890846014023, Final Batch Loss: 0.2858879566192627\n",
      "Subject 5, Epoch 336, Loss: 0.746914729475975, Final Batch Loss: 0.20691914856433868\n",
      "Subject 5, Epoch 337, Loss: 0.8424703478813171, Final Batch Loss: 0.1588190793991089\n",
      "Subject 5, Epoch 338, Loss: 0.9357794225215912, Final Batch Loss: 0.35863885283470154\n",
      "Subject 5, Epoch 339, Loss: 0.9402139484882355, Final Batch Loss: 0.43079671263694763\n",
      "Subject 5, Epoch 340, Loss: 0.7484147846698761, Final Batch Loss: 0.22168782353401184\n",
      "Subject 5, Epoch 341, Loss: 0.8213272392749786, Final Batch Loss: 0.2860397398471832\n",
      "Subject 5, Epoch 342, Loss: 0.8944096267223358, Final Batch Loss: 0.320396363735199\n",
      "Subject 5, Epoch 343, Loss: 0.81779845058918, Final Batch Loss: 0.3208252787590027\n",
      "Subject 5, Epoch 344, Loss: 0.6706360429525375, Final Batch Loss: 0.1656060814857483\n",
      "Subject 5, Epoch 345, Loss: 0.7973246723413467, Final Batch Loss: 0.30610939860343933\n",
      "Subject 5, Epoch 346, Loss: 0.795974537730217, Final Batch Loss: 0.21523122489452362\n",
      "Subject 5, Epoch 347, Loss: 0.8131393641233444, Final Batch Loss: 0.36936643719673157\n",
      "Subject 5, Epoch 348, Loss: 0.6352989822626114, Final Batch Loss: 0.18080325424671173\n",
      "Subject 5, Epoch 349, Loss: 0.8599991798400879, Final Batch Loss: 0.3956649601459503\n",
      "Subject 5, Epoch 350, Loss: 0.9089602828025818, Final Batch Loss: 0.4020555913448334\n",
      "Subject 5, Epoch 351, Loss: 0.9938628524541855, Final Batch Loss: 0.5896438360214233\n",
      "Subject 5, Epoch 352, Loss: 0.7137984931468964, Final Batch Loss: 0.22219312191009521\n",
      "Subject 5, Epoch 353, Loss: 0.8349902778863907, Final Batch Loss: 0.2972385883331299\n",
      "Subject 5, Epoch 354, Loss: 0.698676586151123, Final Batch Loss: 0.13299362361431122\n",
      "Subject 5, Epoch 355, Loss: 0.6781957298517227, Final Batch Loss: 0.1778450310230255\n",
      "Subject 5, Epoch 356, Loss: 0.5365939065814018, Final Batch Loss: 0.10714011639356613\n",
      "Subject 5, Epoch 357, Loss: 0.7402712404727936, Final Batch Loss: 0.21759366989135742\n",
      "Subject 5, Epoch 358, Loss: 0.6749340742826462, Final Batch Loss: 0.19913586974143982\n",
      "Subject 5, Epoch 359, Loss: 0.7036092877388, Final Batch Loss: 0.27351364493370056\n",
      "Subject 5, Epoch 360, Loss: 0.7732525765895844, Final Batch Loss: 0.3752199709415436\n",
      "Subject 5, Epoch 361, Loss: 0.6629870384931564, Final Batch Loss: 0.1840873807668686\n",
      "Subject 5, Epoch 362, Loss: 0.6655807793140411, Final Batch Loss: 0.20311370491981506\n",
      "Subject 5, Epoch 363, Loss: 0.7356840521097183, Final Batch Loss: 0.2691709101200104\n",
      "Subject 5, Epoch 364, Loss: 0.6306294649839401, Final Batch Loss: 0.18150489032268524\n",
      "Subject 5, Epoch 365, Loss: 0.6126074343919754, Final Batch Loss: 0.23504726588726044\n",
      "Subject 5, Epoch 366, Loss: 0.7491788864135742, Final Batch Loss: 0.2474004328250885\n",
      "Subject 5, Epoch 367, Loss: 0.859092503786087, Final Batch Loss: 0.45237070322036743\n",
      "Subject 5, Epoch 368, Loss: 0.5079582780599594, Final Batch Loss: 0.08043272793292999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 369, Loss: 0.6029875874519348, Final Batch Loss: 0.15926548838615417\n",
      "Subject 5, Epoch 370, Loss: 0.7361834049224854, Final Batch Loss: 0.13263845443725586\n",
      "Subject 5, Epoch 371, Loss: 0.6277700215578079, Final Batch Loss: 0.24429845809936523\n",
      "Subject 5, Epoch 372, Loss: 0.8126964718103409, Final Batch Loss: 0.35277363657951355\n",
      "Subject 5, Epoch 373, Loss: 0.557667002081871, Final Batch Loss: 0.16820207238197327\n",
      "Subject 5, Epoch 374, Loss: 0.7762296497821808, Final Batch Loss: 0.2145732343196869\n",
      "Subject 5, Epoch 375, Loss: 0.6290309578180313, Final Batch Loss: 0.19347406923770905\n",
      "Subject 5, Epoch 376, Loss: 0.6286941766738892, Final Batch Loss: 0.21067149937152863\n",
      "Subject 5, Epoch 377, Loss: 0.6742574125528336, Final Batch Loss: 0.17228460311889648\n",
      "Subject 5, Epoch 378, Loss: 0.6847278624773026, Final Batch Loss: 0.25783464312553406\n",
      "Subject 5, Epoch 379, Loss: 0.7322941869497299, Final Batch Loss: 0.27186834812164307\n",
      "Subject 5, Epoch 380, Loss: 0.6677766889333725, Final Batch Loss: 0.2926826775074005\n",
      "Subject 5, Epoch 381, Loss: 0.6549998223781586, Final Batch Loss: 0.13692288100719452\n",
      "Subject 5, Epoch 382, Loss: 0.6484436094760895, Final Batch Loss: 0.2091841846704483\n",
      "Subject 5, Epoch 383, Loss: 0.5908072888851166, Final Batch Loss: 0.1590365469455719\n",
      "Subject 5, Epoch 384, Loss: 0.6813545823097229, Final Batch Loss: 0.1810579001903534\n",
      "Subject 5, Epoch 385, Loss: 0.6198959648609161, Final Batch Loss: 0.20597876608371735\n",
      "Subject 5, Epoch 386, Loss: 0.8093830943107605, Final Batch Loss: 0.22399219870567322\n",
      "Subject 5, Epoch 387, Loss: 0.6056057810783386, Final Batch Loss: 0.1600359082221985\n",
      "Subject 5, Epoch 388, Loss: 0.6078083664178848, Final Batch Loss: 0.16060380637645721\n",
      "Subject 5, Epoch 389, Loss: 0.6520102024078369, Final Batch Loss: 0.14304427802562714\n",
      "Subject 5, Epoch 390, Loss: 0.5956597924232483, Final Batch Loss: 0.2034236639738083\n",
      "Subject 5, Epoch 391, Loss: 0.5312706381082535, Final Batch Loss: 0.09900699555873871\n",
      "Subject 5, Epoch 392, Loss: 0.5242895111441612, Final Batch Loss: 0.12206924706697464\n",
      "Subject 5, Epoch 393, Loss: 0.5321480110287666, Final Batch Loss: 0.1148797944188118\n",
      "Subject 5, Epoch 394, Loss: 0.7069220393896103, Final Batch Loss: 0.2586546838283539\n",
      "Subject 5, Epoch 395, Loss: 0.5093682706356049, Final Batch Loss: 0.16820688545703888\n",
      "Subject 5, Epoch 396, Loss: 0.6767760068178177, Final Batch Loss: 0.40411847829818726\n",
      "Subject 5, Epoch 397, Loss: 0.538126528263092, Final Batch Loss: 0.15338043868541718\n",
      "Subject 5, Epoch 398, Loss: 0.5314966589212418, Final Batch Loss: 0.1436127871274948\n",
      "Subject 5, Epoch 399, Loss: 0.5853516310453415, Final Batch Loss: 0.1843404322862625\n",
      "Subject 5, Epoch 400, Loss: 0.6678891330957413, Final Batch Loss: 0.2053891271352768\n",
      "Subject 5, Epoch 401, Loss: 0.70946404337883, Final Batch Loss: 0.3101896643638611\n",
      "Subject 5, Epoch 402, Loss: 0.5782547444105148, Final Batch Loss: 0.16300919651985168\n",
      "Subject 5, Epoch 403, Loss: 0.5208995193243027, Final Batch Loss: 0.18709474802017212\n",
      "Subject 5, Epoch 404, Loss: 0.5496747270226479, Final Batch Loss: 0.10678461939096451\n",
      "Subject 5, Epoch 405, Loss: 0.572397418320179, Final Batch Loss: 0.11745705455541611\n",
      "Subject 5, Epoch 406, Loss: 0.6716980189085007, Final Batch Loss: 0.18335016071796417\n",
      "Subject 5, Epoch 407, Loss: 0.7440444529056549, Final Batch Loss: 0.31474632024765015\n",
      "Subject 5, Epoch 408, Loss: 0.6715785264968872, Final Batch Loss: 0.26231691241264343\n",
      "Subject 5, Epoch 409, Loss: 0.6112077534198761, Final Batch Loss: 0.23545418679714203\n",
      "Subject 5, Epoch 410, Loss: 0.5799039453268051, Final Batch Loss: 0.18425506353378296\n",
      "Subject 5, Epoch 411, Loss: 0.6414679437875748, Final Batch Loss: 0.2578994333744049\n",
      "Subject 5, Epoch 412, Loss: 0.6131217181682587, Final Batch Loss: 0.1664096862077713\n",
      "Subject 5, Epoch 413, Loss: 0.5328441113233566, Final Batch Loss: 0.15477947890758514\n",
      "Subject 5, Epoch 414, Loss: 0.5355542972683907, Final Batch Loss: 0.0890219584107399\n",
      "Subject 5, Epoch 415, Loss: 0.5474022850394249, Final Batch Loss: 0.11192510277032852\n",
      "Subject 5, Epoch 416, Loss: 0.655698835849762, Final Batch Loss: 0.22796161472797394\n",
      "Subject 5, Epoch 417, Loss: 0.5027967095375061, Final Batch Loss: 0.16296982765197754\n",
      "Subject 5, Epoch 418, Loss: 0.3967726230621338, Final Batch Loss: 0.12521223723888397\n",
      "Subject 5, Epoch 419, Loss: 0.48979876935482025, Final Batch Loss: 0.13358119130134583\n",
      "Subject 5, Epoch 420, Loss: 0.5598944574594498, Final Batch Loss: 0.19923968613147736\n",
      "Subject 5, Epoch 421, Loss: 0.7300941795110703, Final Batch Loss: 0.35328590869903564\n",
      "Subject 5, Epoch 422, Loss: 0.5761351883411407, Final Batch Loss: 0.15864133834838867\n",
      "Subject 5, Epoch 423, Loss: 0.8040623068809509, Final Batch Loss: 0.4349224269390106\n",
      "Subject 5, Epoch 424, Loss: 0.44140195846557617, Final Batch Loss: 0.08968739211559296\n",
      "Subject 5, Epoch 425, Loss: 0.5166015475988388, Final Batch Loss: 0.1431436985731125\n",
      "Subject 5, Epoch 426, Loss: 0.5374704599380493, Final Batch Loss: 0.1607612818479538\n",
      "Subject 5, Epoch 427, Loss: 0.550222858786583, Final Batch Loss: 0.12266659736633301\n",
      "Subject 5, Epoch 428, Loss: 0.5087595880031586, Final Batch Loss: 0.07718326151371002\n",
      "Subject 5, Epoch 429, Loss: 0.6066295728087425, Final Batch Loss: 0.12282445281744003\n",
      "Subject 5, Epoch 430, Loss: 0.6276354342699051, Final Batch Loss: 0.21503488719463348\n",
      "Subject 5, Epoch 431, Loss: 0.594824030995369, Final Batch Loss: 0.18939308822155\n",
      "Subject 5, Epoch 432, Loss: 0.6544321179389954, Final Batch Loss: 0.15968793630599976\n",
      "Subject 5, Epoch 433, Loss: 0.5823990106582642, Final Batch Loss: 0.27198582887649536\n",
      "Subject 5, Epoch 434, Loss: 0.6207291334867477, Final Batch Loss: 0.3012869358062744\n",
      "Subject 5, Epoch 435, Loss: 0.47438766062259674, Final Batch Loss: 0.0703781396150589\n",
      "Subject 5, Epoch 436, Loss: 0.4742014929652214, Final Batch Loss: 0.08153168112039566\n",
      "Subject 5, Epoch 437, Loss: 0.4910971373319626, Final Batch Loss: 0.11751978099346161\n",
      "Subject 5, Epoch 438, Loss: 0.4985528588294983, Final Batch Loss: 0.08619211614131927\n",
      "Subject 5, Epoch 439, Loss: 0.512941911816597, Final Batch Loss: 0.06438377499580383\n",
      "Subject 5, Epoch 440, Loss: 0.67825086414814, Final Batch Loss: 0.2923242151737213\n",
      "Subject 5, Epoch 441, Loss: 0.5919263064861298, Final Batch Loss: 0.22093908488750458\n",
      "Subject 5, Epoch 442, Loss: 0.3975943252444267, Final Batch Loss: 0.12141042947769165\n",
      "Subject 5, Epoch 443, Loss: 0.5938080102205276, Final Batch Loss: 0.2510565519332886\n",
      "Subject 5, Epoch 444, Loss: 0.48873838037252426, Final Batch Loss: 0.11452623456716537\n",
      "Subject 5, Epoch 445, Loss: 0.5378159135580063, Final Batch Loss: 0.15027891099452972\n",
      "Subject 5, Epoch 446, Loss: 0.5395869165658951, Final Batch Loss: 0.22666813433170319\n",
      "Subject 5, Epoch 447, Loss: 0.6219405829906464, Final Batch Loss: 0.22041305899620056\n",
      "Subject 5, Epoch 448, Loss: 0.6596343368291855, Final Batch Loss: 0.2534865140914917\n",
      "Subject 5, Epoch 449, Loss: 0.48773740977048874, Final Batch Loss: 0.12311270087957382\n",
      "Subject 5, Epoch 450, Loss: 0.6273574233055115, Final Batch Loss: 0.18517203629016876\n",
      "Subject 5, Epoch 451, Loss: 0.45504480600357056, Final Batch Loss: 0.1521567851305008\n",
      "Subject 5, Epoch 452, Loss: 0.47385068237781525, Final Batch Loss: 0.16746893525123596\n",
      "Subject 5, Epoch 453, Loss: 0.40539737045764923, Final Batch Loss: 0.08980850875377655\n",
      "Subject 5, Epoch 454, Loss: 0.5106052160263062, Final Batch Loss: 0.15737418830394745\n",
      "Subject 5, Epoch 455, Loss: 0.5500610321760178, Final Batch Loss: 0.16990678012371063\n",
      "Subject 5, Epoch 456, Loss: 0.5999205559492111, Final Batch Loss: 0.2269313782453537\n",
      "Subject 5, Epoch 457, Loss: 0.5793497934937477, Final Batch Loss: 0.3184748589992523\n",
      "Subject 5, Epoch 458, Loss: 0.5857046544551849, Final Batch Loss: 0.2264932543039322\n",
      "Subject 5, Epoch 459, Loss: 0.6333809196949005, Final Batch Loss: 0.1662461906671524\n",
      "Subject 5, Epoch 460, Loss: 0.5733949840068817, Final Batch Loss: 0.15052831172943115\n",
      "Subject 5, Epoch 461, Loss: 0.4901973232626915, Final Batch Loss: 0.09301183372735977\n",
      "Subject 5, Epoch 462, Loss: 0.42617761343717575, Final Batch Loss: 0.09817903488874435\n",
      "Subject 5, Epoch 463, Loss: 0.6612402647733688, Final Batch Loss: 0.18604585528373718\n",
      "Subject 5, Epoch 464, Loss: 0.6191662102937698, Final Batch Loss: 0.2546823024749756\n",
      "Subject 5, Epoch 465, Loss: 0.8277666121721268, Final Batch Loss: 0.4441477656364441\n",
      "Subject 5, Epoch 466, Loss: 0.46183405071496964, Final Batch Loss: 0.10845140367746353\n",
      "Subject 5, Epoch 467, Loss: 0.4062008485198021, Final Batch Loss: 0.08124905824661255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 468, Loss: 0.3739522323012352, Final Batch Loss: 0.05141439288854599\n",
      "Subject 5, Epoch 469, Loss: 0.44744686037302017, Final Batch Loss: 0.09661904722452164\n",
      "Subject 5, Epoch 470, Loss: 0.6365859806537628, Final Batch Loss: 0.19677965342998505\n",
      "Subject 5, Epoch 471, Loss: 0.5743843913078308, Final Batch Loss: 0.181463360786438\n",
      "Subject 5, Epoch 472, Loss: 0.48060142993927, Final Batch Loss: 0.1078692376613617\n",
      "Subject 5, Epoch 473, Loss: 0.4990769177675247, Final Batch Loss: 0.15189678966999054\n",
      "Subject 5, Epoch 474, Loss: 0.6223042234778404, Final Batch Loss: 0.41162070631980896\n",
      "Subject 5, Epoch 475, Loss: 0.4202426075935364, Final Batch Loss: 0.08206227421760559\n",
      "Subject 5, Epoch 476, Loss: 0.5612084716558456, Final Batch Loss: 0.10704535245895386\n",
      "Subject 5, Epoch 477, Loss: 0.6138252317905426, Final Batch Loss: 0.14517956972122192\n",
      "Subject 5, Epoch 478, Loss: 0.4500325322151184, Final Batch Loss: 0.13457971811294556\n",
      "Subject 5, Epoch 479, Loss: 0.6093593537807465, Final Batch Loss: 0.23894202709197998\n",
      "Subject 5, Epoch 480, Loss: 0.516154907643795, Final Batch Loss: 0.2746764123439789\n",
      "Subject 5, Epoch 481, Loss: 0.6641797721385956, Final Batch Loss: 0.32876911759376526\n",
      "Subject 5, Epoch 482, Loss: 0.38997938483953476, Final Batch Loss: 0.10694808512926102\n",
      "Subject 5, Epoch 483, Loss: 0.48935508728027344, Final Batch Loss: 0.16123752295970917\n",
      "Subject 5, Epoch 484, Loss: 0.6355918496847153, Final Batch Loss: 0.32153815031051636\n",
      "Subject 5, Epoch 485, Loss: 0.4160769581794739, Final Batch Loss: 0.14976927638053894\n",
      "Subject 5, Epoch 486, Loss: 0.4139301925897598, Final Batch Loss: 0.12200146168470383\n",
      "Subject 5, Epoch 487, Loss: 0.4412112385034561, Final Batch Loss: 0.1655915230512619\n",
      "Subject 5, Epoch 488, Loss: 0.4278797134757042, Final Batch Loss: 0.10960566252470016\n",
      "Subject 5, Epoch 489, Loss: 0.5565274506807327, Final Batch Loss: 0.20552101731300354\n",
      "Subject 5, Epoch 490, Loss: 0.4721385985612869, Final Batch Loss: 0.22917473316192627\n",
      "Subject 5, Epoch 491, Loss: 0.4152570925652981, Final Batch Loss: 0.056378867477178574\n",
      "Subject 5, Epoch 492, Loss: 0.3918481543660164, Final Batch Loss: 0.0991537943482399\n",
      "Subject 5, Epoch 493, Loss: 0.45943963527679443, Final Batch Loss: 0.1922696828842163\n",
      "Subject 5, Epoch 494, Loss: 0.3984210714697838, Final Batch Loss: 0.05010741204023361\n",
      "Subject 5, Epoch 495, Loss: 0.4255867153406143, Final Batch Loss: 0.1487778276205063\n",
      "Subject 5, Epoch 496, Loss: 0.5457700788974762, Final Batch Loss: 0.21739764511585236\n",
      "Subject 5, Epoch 497, Loss: 0.4658786356449127, Final Batch Loss: 0.17425017058849335\n",
      "Subject 5, Epoch 498, Loss: 0.4330827370285988, Final Batch Loss: 0.11690995842218399\n",
      "Subject 5, Epoch 499, Loss: 0.457676038146019, Final Batch Loss: 0.23687633872032166\n",
      "Subject 5, Epoch 500, Loss: 0.48886946588754654, Final Batch Loss: 0.0918598398566246\n",
      "Subject 5, Epoch 501, Loss: 0.30830853432416916, Final Batch Loss: 0.1245608925819397\n",
      "Subject 5, Epoch 502, Loss: 0.3216710016131401, Final Batch Loss: 0.08263801783323288\n",
      "Subject 5, Epoch 503, Loss: 0.39167269691824913, Final Batch Loss: 0.0428781621158123\n",
      "Subject 5, Epoch 504, Loss: 0.4475417360663414, Final Batch Loss: 0.08874520659446716\n",
      "Subject 5, Epoch 505, Loss: 0.40955834090709686, Final Batch Loss: 0.08694642782211304\n",
      "Subject 5, Epoch 506, Loss: 0.5408781468868256, Final Batch Loss: 0.18116506934165955\n",
      "Subject 5, Epoch 507, Loss: 0.36705543100833893, Final Batch Loss: 0.10544468462467194\n",
      "Subject 5, Epoch 508, Loss: 0.44234801828861237, Final Batch Loss: 0.20932123064994812\n",
      "Subject 5, Epoch 509, Loss: 0.44606979191303253, Final Batch Loss: 0.17979666590690613\n",
      "Subject 5, Epoch 510, Loss: 0.5942276120185852, Final Batch Loss: 0.22725005447864532\n",
      "Subject 5, Epoch 511, Loss: 0.6306409612298012, Final Batch Loss: 0.4112273156642914\n",
      "Subject 5, Epoch 512, Loss: 0.4852418601512909, Final Batch Loss: 0.15765869617462158\n",
      "Subject 5, Epoch 513, Loss: 0.44560113549232483, Final Batch Loss: 0.10283002257347107\n",
      "Subject 5, Epoch 514, Loss: 0.41704539209604263, Final Batch Loss: 0.11077231913805008\n",
      "Subject 5, Epoch 515, Loss: 0.5213966965675354, Final Batch Loss: 0.15868541598320007\n",
      "Subject 5, Epoch 516, Loss: 0.4955817759037018, Final Batch Loss: 0.17223189771175385\n",
      "Subject 5, Epoch 517, Loss: 0.4806857481598854, Final Batch Loss: 0.22539183497428894\n",
      "Subject 5, Epoch 518, Loss: 0.39184947311878204, Final Batch Loss: 0.1055898442864418\n",
      "Subject 5, Epoch 519, Loss: 0.39104294776916504, Final Batch Loss: 0.12590497732162476\n",
      "Subject 5, Epoch 520, Loss: 0.3066963329911232, Final Batch Loss: 0.10060146450996399\n",
      "Subject 5, Epoch 521, Loss: 0.40193453431129456, Final Batch Loss: 0.12572510540485382\n",
      "Subject 5, Epoch 522, Loss: 0.4382133185863495, Final Batch Loss: 0.15498478710651398\n",
      "Subject 5, Epoch 523, Loss: 0.3971908539533615, Final Batch Loss: 0.16333824396133423\n",
      "Subject 5, Epoch 524, Loss: 0.300222285091877, Final Batch Loss: 0.08599772304296494\n",
      "Subject 5, Epoch 525, Loss: 0.4191443920135498, Final Batch Loss: 0.08730608224868774\n",
      "Subject 5, Epoch 526, Loss: 0.5140632688999176, Final Batch Loss: 0.22615136206150055\n",
      "Subject 5, Epoch 527, Loss: 0.3312244601547718, Final Batch Loss: 0.04116980358958244\n",
      "Subject 5, Epoch 528, Loss: 0.4980885088443756, Final Batch Loss: 0.215068057179451\n",
      "Subject 5, Epoch 529, Loss: 0.44297151267528534, Final Batch Loss: 0.15835456550121307\n",
      "Subject 5, Epoch 530, Loss: 0.45746612548828125, Final Batch Loss: 0.1151307001709938\n",
      "Subject 5, Epoch 531, Loss: 0.37786708772182465, Final Batch Loss: 0.07018597424030304\n",
      "Subject 5, Epoch 532, Loss: 0.3560384511947632, Final Batch Loss: 0.05004248023033142\n",
      "Subject 5, Epoch 533, Loss: 0.32704343646764755, Final Batch Loss: 0.08394438028335571\n",
      "Subject 5, Epoch 534, Loss: 0.48319706320762634, Final Batch Loss: 0.15679261088371277\n",
      "Subject 5, Epoch 535, Loss: 0.35143081098794937, Final Batch Loss: 0.09886743128299713\n",
      "Subject 5, Epoch 536, Loss: 0.43574902415275574, Final Batch Loss: 0.14466354250907898\n",
      "Subject 5, Epoch 537, Loss: 0.5203078240156174, Final Batch Loss: 0.13214868307113647\n",
      "Subject 5, Epoch 538, Loss: 0.4505840465426445, Final Batch Loss: 0.12119700759649277\n",
      "Subject 5, Epoch 539, Loss: 0.4175006076693535, Final Batch Loss: 0.10384228080511093\n",
      "Subject 5, Epoch 540, Loss: 0.49630463123321533, Final Batch Loss: 0.14910784363746643\n",
      "Subject 5, Epoch 541, Loss: 0.2710961364209652, Final Batch Loss: 0.024440836161375046\n",
      "Subject 5, Epoch 542, Loss: 0.4306088089942932, Final Batch Loss: 0.15010493993759155\n",
      "Subject 5, Epoch 543, Loss: 0.4679863750934601, Final Batch Loss: 0.15754131972789764\n",
      "Subject 5, Epoch 544, Loss: 0.48784977197647095, Final Batch Loss: 0.20505094528198242\n",
      "Subject 5, Epoch 545, Loss: 0.4703774005174637, Final Batch Loss: 0.12801380455493927\n",
      "Subject 5, Epoch 546, Loss: 0.4342796579003334, Final Batch Loss: 0.12039538472890854\n",
      "Subject 5, Epoch 547, Loss: 0.43655163049697876, Final Batch Loss: 0.16743767261505127\n",
      "Subject 5, Epoch 548, Loss: 0.34852227568626404, Final Batch Loss: 0.08113612979650497\n",
      "Subject 5, Epoch 549, Loss: 0.48512569069862366, Final Batch Loss: 0.14220985770225525\n",
      "Subject 5, Epoch 550, Loss: 0.3790797106921673, Final Batch Loss: 0.055656056851148605\n",
      "Subject 5, Epoch 551, Loss: 0.47136789560317993, Final Batch Loss: 0.21296744048595428\n",
      "Subject 5, Epoch 552, Loss: 0.39901360124349594, Final Batch Loss: 0.08669673651456833\n",
      "Subject 5, Epoch 553, Loss: 0.40737129002809525, Final Batch Loss: 0.16400329768657684\n",
      "Subject 5, Epoch 554, Loss: 0.3332069832831621, Final Batch Loss: 0.019829170778393745\n",
      "Subject 5, Epoch 555, Loss: 0.42437515407800674, Final Batch Loss: 0.17551501095294952\n",
      "Subject 5, Epoch 556, Loss: 0.4877539426088333, Final Batch Loss: 0.1855117380619049\n",
      "Subject 5, Epoch 557, Loss: 0.3312389738857746, Final Batch Loss: 0.062313247472047806\n",
      "Subject 5, Epoch 558, Loss: 0.37935493141412735, Final Batch Loss: 0.1200428232550621\n",
      "Subject 5, Epoch 559, Loss: 0.3639449253678322, Final Batch Loss: 0.09252629429101944\n",
      "Subject 5, Epoch 560, Loss: 0.3674602434039116, Final Batch Loss: 0.09416534751653671\n",
      "Subject 5, Epoch 561, Loss: 0.4889640659093857, Final Batch Loss: 0.15881572663784027\n",
      "Subject 5, Epoch 562, Loss: 0.27848832309246063, Final Batch Loss: 0.051806606352329254\n",
      "Subject 5, Epoch 563, Loss: 0.37527991458773613, Final Batch Loss: 0.061873387545347214\n",
      "Subject 5, Epoch 564, Loss: 0.47038745880126953, Final Batch Loss: 0.18290962278842926\n",
      "Subject 5, Epoch 565, Loss: 0.408392034471035, Final Batch Loss: 0.15789441764354706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 566, Loss: 0.4688571020960808, Final Batch Loss: 0.19367516040802002\n",
      "Subject 5, Epoch 567, Loss: 0.4349401891231537, Final Batch Loss: 0.12914475798606873\n",
      "Subject 5, Epoch 568, Loss: 0.2709471248090267, Final Batch Loss: 0.04077989235520363\n",
      "Subject 5, Epoch 569, Loss: 0.5447379797697067, Final Batch Loss: 0.252094030380249\n",
      "Subject 5, Epoch 570, Loss: 0.4929921627044678, Final Batch Loss: 0.2052406519651413\n",
      "Subject 5, Epoch 571, Loss: 0.43339822441339493, Final Batch Loss: 0.1638938933610916\n",
      "Subject 5, Epoch 572, Loss: 0.5820985361933708, Final Batch Loss: 0.29636287689208984\n",
      "Subject 5, Epoch 573, Loss: 0.3233194947242737, Final Batch Loss: 0.08647186309099197\n",
      "Subject 5, Epoch 574, Loss: 0.34169840812683105, Final Batch Loss: 0.06912654638290405\n",
      "Subject 5, Epoch 575, Loss: 0.3361050933599472, Final Batch Loss: 0.08615858852863312\n",
      "Subject 5, Epoch 576, Loss: 0.3687645383179188, Final Batch Loss: 0.1531321108341217\n",
      "Subject 5, Epoch 577, Loss: 0.4856320917606354, Final Batch Loss: 0.17824026942253113\n",
      "Subject 5, Epoch 578, Loss: 0.4041927605867386, Final Batch Loss: 0.13434627652168274\n",
      "Subject 5, Epoch 579, Loss: 0.47639116644859314, Final Batch Loss: 0.18855860829353333\n",
      "Subject 5, Epoch 580, Loss: 0.39460592716932297, Final Batch Loss: 0.14419417083263397\n",
      "Subject 5, Epoch 581, Loss: 0.2910628207027912, Final Batch Loss: 0.04063206538558006\n",
      "Subject 5, Epoch 582, Loss: 0.3876546248793602, Final Batch Loss: 0.10341227799654007\n",
      "Subject 5, Epoch 583, Loss: 0.3292297199368477, Final Batch Loss: 0.10952901840209961\n",
      "Subject 5, Epoch 584, Loss: 0.28790855407714844, Final Batch Loss: 0.09023872762918472\n",
      "Subject 5, Epoch 585, Loss: 0.42052022367715836, Final Batch Loss: 0.03926385939121246\n",
      "Subject 5, Epoch 586, Loss: 0.38465118408203125, Final Batch Loss: 0.13191193342208862\n",
      "Subject 5, Epoch 587, Loss: 0.3313104137778282, Final Batch Loss: 0.07734338194131851\n",
      "Subject 5, Epoch 588, Loss: 0.4561849609017372, Final Batch Loss: 0.21933622658252716\n",
      "Subject 5, Epoch 589, Loss: 0.3928823918104172, Final Batch Loss: 0.12009589374065399\n",
      "Subject 5, Epoch 590, Loss: 0.5991929322481155, Final Batch Loss: 0.27132007479667664\n",
      "Subject 5, Epoch 591, Loss: 0.47810395061969757, Final Batch Loss: 0.21935272216796875\n",
      "Subject 5, Epoch 592, Loss: 0.6044619753956795, Final Batch Loss: 0.3438774645328522\n",
      "Subject 5, Epoch 593, Loss: 0.2736438885331154, Final Batch Loss: 0.05456627160310745\n",
      "Subject 5, Epoch 594, Loss: 0.40741606801748276, Final Batch Loss: 0.07276493310928345\n",
      "Subject 5, Epoch 595, Loss: 0.3996172919869423, Final Batch Loss: 0.09499440342187881\n",
      "Subject 5, Epoch 596, Loss: 0.3701084330677986, Final Batch Loss: 0.1159539744257927\n",
      "Subject 5, Epoch 597, Loss: 0.3601427599787712, Final Batch Loss: 0.06348705291748047\n",
      "Subject 5, Epoch 598, Loss: 0.4273325204849243, Final Batch Loss: 0.10197599977254868\n",
      "Subject 5, Epoch 599, Loss: 0.5112972408533096, Final Batch Loss: 0.2678748071193695\n",
      "Subject 5, Epoch 600, Loss: 0.39466384798288345, Final Batch Loss: 0.08969978243112564\n",
      "Subject 5, Epoch 601, Loss: 0.34762679040431976, Final Batch Loss: 0.08282920718193054\n",
      "Subject 5, Epoch 602, Loss: 0.4614739492535591, Final Batch Loss: 0.16429921984672546\n",
      "Subject 5, Epoch 603, Loss: 0.31907089799642563, Final Batch Loss: 0.07609540224075317\n",
      "Subject 5, Epoch 604, Loss: 0.3643278628587723, Final Batch Loss: 0.09905604273080826\n",
      "Subject 5, Epoch 605, Loss: 0.365017119795084, Final Batch Loss: 0.1567573845386505\n",
      "Subject 5, Epoch 606, Loss: 0.3961962163448334, Final Batch Loss: 0.15222713351249695\n",
      "Subject 5, Epoch 607, Loss: 0.30297061055898666, Final Batch Loss: 0.09791741520166397\n",
      "Subject 5, Epoch 608, Loss: 0.5874988734722137, Final Batch Loss: 0.3948025405406952\n",
      "Subject 5, Epoch 609, Loss: 0.46732811629772186, Final Batch Loss: 0.15959638357162476\n",
      "Subject 5, Epoch 610, Loss: 0.3867287412285805, Final Batch Loss: 0.14979301393032074\n",
      "Subject 5, Epoch 611, Loss: 0.38460247218608856, Final Batch Loss: 0.13172665238380432\n",
      "Subject 5, Epoch 612, Loss: 0.35249098390340805, Final Batch Loss: 0.1007223129272461\n",
      "Subject 5, Epoch 613, Loss: 0.48639006167650223, Final Batch Loss: 0.19497138261795044\n",
      "Subject 5, Epoch 614, Loss: 0.328157015144825, Final Batch Loss: 0.04333669692277908\n",
      "Subject 5, Epoch 615, Loss: 0.35571109130978584, Final Batch Loss: 0.14262357354164124\n",
      "Subject 5, Epoch 616, Loss: 0.49245861172676086, Final Batch Loss: 0.14534641802310944\n",
      "Subject 5, Epoch 617, Loss: 0.48700591921806335, Final Batch Loss: 0.12955033779144287\n",
      "Subject 5, Epoch 618, Loss: 0.2721017450094223, Final Batch Loss: 0.041412509977817535\n",
      "Subject 5, Epoch 619, Loss: 0.3159012198448181, Final Batch Loss: 0.0879395604133606\n",
      "Subject 5, Epoch 620, Loss: 0.26870084553956985, Final Batch Loss: 0.0696859061717987\n",
      "Subject 5, Epoch 621, Loss: 0.2798992730677128, Final Batch Loss: 0.055190619081258774\n",
      "Subject 5, Epoch 622, Loss: 0.37105464190244675, Final Batch Loss: 0.08281712979078293\n",
      "Subject 5, Epoch 623, Loss: 0.3515514060854912, Final Batch Loss: 0.12245918810367584\n",
      "Subject 5, Epoch 624, Loss: 0.40113306790590286, Final Batch Loss: 0.1510007679462433\n",
      "Subject 5, Epoch 625, Loss: 0.36631860584020615, Final Batch Loss: 0.1457531750202179\n",
      "Subject 5, Epoch 626, Loss: 0.43492555618286133, Final Batch Loss: 0.1857137829065323\n",
      "Subject 5, Epoch 627, Loss: 0.37923839688301086, Final Batch Loss: 0.11657872796058655\n",
      "Subject 5, Epoch 628, Loss: 0.4880717247724533, Final Batch Loss: 0.2871082127094269\n",
      "Subject 5, Epoch 629, Loss: 0.3599495142698288, Final Batch Loss: 0.1030082106590271\n",
      "Subject 5, Epoch 630, Loss: 0.3586200028657913, Final Batch Loss: 0.10374194383621216\n",
      "Subject 5, Epoch 631, Loss: 0.3260262757539749, Final Batch Loss: 0.050989121198654175\n",
      "Subject 5, Epoch 632, Loss: 0.3413083031773567, Final Batch Loss: 0.06445642560720444\n",
      "Subject 5, Epoch 633, Loss: 0.37759365886449814, Final Batch Loss: 0.10083995014429092\n",
      "Subject 5, Epoch 634, Loss: 0.37493936717510223, Final Batch Loss: 0.08226004242897034\n",
      "Subject 5, Epoch 635, Loss: 0.4069531112909317, Final Batch Loss: 0.14522673189640045\n",
      "Subject 5, Epoch 636, Loss: 0.3492208570241928, Final Batch Loss: 0.14678150415420532\n",
      "Subject 5, Epoch 637, Loss: 0.3483751751482487, Final Batch Loss: 0.05779227986931801\n",
      "Subject 5, Epoch 638, Loss: 0.34619519859552383, Final Batch Loss: 0.10349112749099731\n",
      "Subject 5, Epoch 639, Loss: 0.4243658259510994, Final Batch Loss: 0.10757853835821152\n",
      "Subject 5, Epoch 640, Loss: 0.3398464098572731, Final Batch Loss: 0.11420116573572159\n",
      "Subject 5, Epoch 641, Loss: 0.49625760316848755, Final Batch Loss: 0.21592195332050323\n",
      "Subject 5, Epoch 642, Loss: 0.31433243304491043, Final Batch Loss: 0.08499985933303833\n",
      "Subject 5, Epoch 643, Loss: 0.3463953956961632, Final Batch Loss: 0.05681384354829788\n",
      "Subject 5, Epoch 644, Loss: 0.4427073299884796, Final Batch Loss: 0.13097161054611206\n",
      "Subject 5, Epoch 645, Loss: 0.3941270187497139, Final Batch Loss: 0.2124420404434204\n",
      "Subject 5, Epoch 646, Loss: 0.25089019536972046, Final Batch Loss: 0.09721501171588898\n",
      "Subject 5, Epoch 647, Loss: 0.27432238310575485, Final Batch Loss: 0.0810103639960289\n",
      "Subject 5, Epoch 648, Loss: 0.38172876089811325, Final Batch Loss: 0.17434346675872803\n",
      "Subject 5, Epoch 649, Loss: 0.2871897555887699, Final Batch Loss: 0.038518812507390976\n",
      "Subject 5, Epoch 650, Loss: 0.3746121898293495, Final Batch Loss: 0.09379806369543076\n",
      "Subject 5, Epoch 651, Loss: 0.3522646501660347, Final Batch Loss: 0.1656378209590912\n",
      "Subject 5, Epoch 652, Loss: 0.25883566215634346, Final Batch Loss: 0.06574319303035736\n",
      "Subject 5, Epoch 653, Loss: 0.27647823840379715, Final Batch Loss: 0.06000269204378128\n",
      "Subject 5, Epoch 654, Loss: 0.3880308046936989, Final Batch Loss: 0.15758299827575684\n",
      "Subject 5, Epoch 655, Loss: 0.37316301465034485, Final Batch Loss: 0.1332876831293106\n",
      "Subject 5, Epoch 656, Loss: 0.379217304289341, Final Batch Loss: 0.11749310046434402\n",
      "Subject 5, Epoch 657, Loss: 0.2869887799024582, Final Batch Loss: 0.08332456648349762\n",
      "Subject 5, Epoch 658, Loss: 0.389944925904274, Final Batch Loss: 0.06419907510280609\n",
      "Subject 5, Epoch 659, Loss: 0.2735605388879776, Final Batch Loss: 0.026910006999969482\n",
      "Subject 5, Epoch 660, Loss: 0.4577290117740631, Final Batch Loss: 0.14024317264556885\n",
      "Subject 5, Epoch 661, Loss: 0.2543400526046753, Final Batch Loss: 0.07719413936138153\n",
      "Subject 5, Epoch 662, Loss: 0.26871061883866787, Final Batch Loss: 0.011519936844706535\n",
      "Subject 5, Epoch 663, Loss: 0.36617933958768845, Final Batch Loss: 0.08316078037023544\n",
      "Subject 5, Epoch 664, Loss: 0.2935135141015053, Final Batch Loss: 0.06394655257463455\n",
      "Subject 5, Epoch 665, Loss: 0.30496497452259064, Final Batch Loss: 0.11151662468910217\n",
      "Subject 5, Epoch 666, Loss: 0.5453069470822811, Final Batch Loss: 0.3672078847885132\n",
      "Subject 5, Epoch 667, Loss: 0.40055620670318604, Final Batch Loss: 0.1052074059844017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 668, Loss: 0.2916957214474678, Final Batch Loss: 0.0497630313038826\n",
      "Subject 5, Epoch 669, Loss: 0.4170984476804733, Final Batch Loss: 0.10589421540498734\n",
      "Subject 5, Epoch 670, Loss: 0.2739727273583412, Final Batch Loss: 0.07435162365436554\n",
      "Subject 5, Epoch 671, Loss: 0.39858555793762207, Final Batch Loss: 0.10717325657606125\n",
      "Subject 5, Epoch 672, Loss: 0.25766440108418465, Final Batch Loss: 0.05821339786052704\n",
      "Subject 5, Epoch 673, Loss: 0.34722504019737244, Final Batch Loss: 0.14944705367088318\n",
      "Subject 5, Epoch 674, Loss: 0.31917092204093933, Final Batch Loss: 0.10200511664152145\n",
      "Subject 5, Epoch 675, Loss: 0.3421883136034012, Final Batch Loss: 0.06051468849182129\n",
      "Subject 5, Epoch 676, Loss: 0.4227389171719551, Final Batch Loss: 0.16930212080478668\n",
      "Subject 5, Epoch 677, Loss: 0.37691761553287506, Final Batch Loss: 0.0782020315527916\n",
      "Subject 5, Epoch 678, Loss: 0.1969062015414238, Final Batch Loss: 0.05159936845302582\n",
      "Subject 5, Epoch 679, Loss: 0.33944522961974144, Final Batch Loss: 0.062349896878004074\n",
      "Subject 5, Epoch 680, Loss: 0.3081928417086601, Final Batch Loss: 0.10950727015733719\n",
      "Subject 5, Epoch 681, Loss: 0.33022141084074974, Final Batch Loss: 0.060005199164152145\n",
      "Subject 5, Epoch 682, Loss: 0.422462597489357, Final Batch Loss: 0.1725674867630005\n",
      "Subject 5, Epoch 683, Loss: 0.36812129616737366, Final Batch Loss: 0.13031987845897675\n",
      "Subject 5, Epoch 684, Loss: 0.4068009927868843, Final Batch Loss: 0.15791738033294678\n",
      "Subject 5, Epoch 685, Loss: 0.3437417894601822, Final Batch Loss: 0.1272095888853073\n",
      "Subject 5, Epoch 686, Loss: 0.3897187411785126, Final Batch Loss: 0.19333858788013458\n",
      "Subject 5, Epoch 687, Loss: 0.29144085198640823, Final Batch Loss: 0.08429183810949326\n",
      "Subject 5, Epoch 688, Loss: 0.3708711415529251, Final Batch Loss: 0.17670972645282745\n",
      "Subject 5, Epoch 689, Loss: 0.2722010686993599, Final Batch Loss: 0.0681585893034935\n",
      "Subject 5, Epoch 690, Loss: 0.32339925318956375, Final Batch Loss: 0.15444669127464294\n",
      "Subject 5, Epoch 691, Loss: 0.27628475055098534, Final Batch Loss: 0.04568460211157799\n",
      "Subject 5, Epoch 692, Loss: 0.3641745336353779, Final Batch Loss: 0.25031155347824097\n",
      "Subject 5, Epoch 693, Loss: 0.38357651233673096, Final Batch Loss: 0.09114398062229156\n",
      "Subject 5, Epoch 694, Loss: 0.28176555410027504, Final Batch Loss: 0.04923396185040474\n",
      "Subject 5, Epoch 695, Loss: 0.2772440053522587, Final Batch Loss: 0.1571873426437378\n",
      "Subject 5, Epoch 696, Loss: 0.3013923838734627, Final Batch Loss: 0.17341899871826172\n",
      "Subject 5, Epoch 697, Loss: 0.38321996480226517, Final Batch Loss: 0.1643015444278717\n",
      "Subject 5, Epoch 698, Loss: 0.26443349197506905, Final Batch Loss: 0.050236206501722336\n",
      "Subject 5, Epoch 699, Loss: 0.3906131759285927, Final Batch Loss: 0.17040766775608063\n",
      "Subject 5, Epoch 700, Loss: 0.34814800322055817, Final Batch Loss: 0.15842926502227783\n",
      "Subject 5, Epoch 701, Loss: 0.32676880806684494, Final Batch Loss: 0.18114405870437622\n",
      "Subject 5, Epoch 702, Loss: 0.30221962183713913, Final Batch Loss: 0.04174048453569412\n",
      "Subject 5, Epoch 703, Loss: 0.3108944743871689, Final Batch Loss: 0.09549238532781601\n",
      "Subject 5, Epoch 704, Loss: 0.3341527059674263, Final Batch Loss: 0.1686917543411255\n",
      "Subject 5, Epoch 705, Loss: 0.1939495913684368, Final Batch Loss: 0.040412575006484985\n",
      "Subject 5, Epoch 706, Loss: 0.2927475646138191, Final Batch Loss: 0.060817234218120575\n",
      "Subject 5, Epoch 707, Loss: 0.32030266150832176, Final Batch Loss: 0.13422898948192596\n",
      "Subject 5, Epoch 708, Loss: 0.279277216643095, Final Batch Loss: 0.049226392060518265\n",
      "Subject 5, Epoch 709, Loss: 0.3648953437805176, Final Batch Loss: 0.08423326909542084\n",
      "Subject 5, Epoch 710, Loss: 0.2840671055018902, Final Batch Loss: 0.05874389782547951\n",
      "Subject 5, Epoch 711, Loss: 0.27756360173225403, Final Batch Loss: 0.05621858686208725\n",
      "Subject 5, Epoch 712, Loss: 0.27787866443395615, Final Batch Loss: 0.09303552657365799\n",
      "Subject 5, Epoch 713, Loss: 0.3815341666340828, Final Batch Loss: 0.15523968636989594\n",
      "Subject 5, Epoch 714, Loss: 0.18741494417190552, Final Batch Loss: 0.020144447684288025\n",
      "Subject 5, Epoch 715, Loss: 0.25716542452573776, Final Batch Loss: 0.10246723890304565\n",
      "Subject 5, Epoch 716, Loss: 0.30232977867126465, Final Batch Loss: 0.08535763621330261\n",
      "Subject 5, Epoch 717, Loss: 0.3651290535926819, Final Batch Loss: 0.06464050710201263\n",
      "Subject 5, Epoch 718, Loss: 0.40416889637708664, Final Batch Loss: 0.1007501482963562\n",
      "Subject 5, Epoch 719, Loss: 0.263716958463192, Final Batch Loss: 0.11055392771959305\n",
      "Subject 5, Epoch 720, Loss: 0.30851416289806366, Final Batch Loss: 0.11824164539575577\n",
      "Subject 5, Epoch 721, Loss: 0.29204951971769333, Final Batch Loss: 0.04171258956193924\n",
      "Subject 5, Epoch 722, Loss: 0.382467620074749, Final Batch Loss: 0.10301569104194641\n",
      "Subject 5, Epoch 723, Loss: 0.3710149750113487, Final Batch Loss: 0.069754458963871\n",
      "Subject 5, Epoch 724, Loss: 0.2876553889364004, Final Batch Loss: 0.019474243745207787\n",
      "Subject 5, Epoch 725, Loss: 0.3940062001347542, Final Batch Loss: 0.12713740766048431\n",
      "Subject 5, Epoch 726, Loss: 0.2999139875173569, Final Batch Loss: 0.06698993593454361\n",
      "Subject 5, Epoch 727, Loss: 0.3484909161925316, Final Batch Loss: 0.1598208099603653\n",
      "Subject 5, Epoch 728, Loss: 0.34057333320379257, Final Batch Loss: 0.08226265013217926\n",
      "Subject 5, Epoch 729, Loss: 0.33285433053970337, Final Batch Loss: 0.0846640095114708\n",
      "Subject 5, Epoch 730, Loss: 0.3776496723294258, Final Batch Loss: 0.20564091205596924\n",
      "Subject 5, Epoch 731, Loss: 0.29690950363874435, Final Batch Loss: 0.11829398572444916\n",
      "Subject 5, Epoch 732, Loss: 0.31011713296175003, Final Batch Loss: 0.12418818473815918\n",
      "Subject 5, Epoch 733, Loss: 0.3520943745970726, Final Batch Loss: 0.13706818222999573\n",
      "Subject 5, Epoch 734, Loss: 0.26507194712758064, Final Batch Loss: 0.11382462829351425\n",
      "Subject 5, Epoch 735, Loss: 0.3187233544886112, Final Batch Loss: 0.06735237687826157\n",
      "Subject 5, Epoch 736, Loss: 0.3108799308538437, Final Batch Loss: 0.11024664342403412\n",
      "Subject 5, Epoch 737, Loss: 0.5633950680494308, Final Batch Loss: 0.1620642989873886\n",
      "Subject 5, Epoch 738, Loss: 0.37891320139169693, Final Batch Loss: 0.11447207629680634\n",
      "Subject 5, Epoch 739, Loss: 0.35922425985336304, Final Batch Loss: 0.11482326686382294\n",
      "Subject 5, Epoch 740, Loss: 0.3751712143421173, Final Batch Loss: 0.17533811926841736\n",
      "Subject 5, Epoch 741, Loss: 0.35079871118068695, Final Batch Loss: 0.05659107118844986\n",
      "Subject 5, Epoch 742, Loss: 0.39314886927604675, Final Batch Loss: 0.19171908497810364\n",
      "Subject 5, Epoch 743, Loss: 0.3810807578265667, Final Batch Loss: 0.21610666811466217\n",
      "Subject 5, Epoch 744, Loss: 0.319293525069952, Final Batch Loss: 0.15059567987918854\n",
      "Subject 5, Epoch 745, Loss: 0.26929041743278503, Final Batch Loss: 0.04830598086118698\n",
      "Subject 5, Epoch 746, Loss: 0.34873299300670624, Final Batch Loss: 0.20138823986053467\n",
      "Subject 5, Epoch 747, Loss: 0.36518993973731995, Final Batch Loss: 0.06649453192949295\n",
      "Subject 5, Epoch 748, Loss: 0.32108446955680847, Final Batch Loss: 0.07272789627313614\n",
      "Subject 5, Epoch 749, Loss: 0.3423684798181057, Final Batch Loss: 0.06249326094985008\n",
      "Subject 5, Epoch 750, Loss: 0.3463384062051773, Final Batch Loss: 0.1622777283191681\n",
      "Subject 5, Epoch 751, Loss: 0.23552208580076694, Final Batch Loss: 0.028754686936736107\n",
      "Subject 5, Epoch 752, Loss: 0.3906606063246727, Final Batch Loss: 0.19222089648246765\n",
      "Subject 5, Epoch 753, Loss: 0.26481685787439346, Final Batch Loss: 0.06560951471328735\n",
      "Subject 5, Epoch 754, Loss: 0.3246885910630226, Final Batch Loss: 0.08359966427087784\n",
      "Subject 5, Epoch 755, Loss: 0.2907905802130699, Final Batch Loss: 0.07804279029369354\n",
      "Subject 5, Epoch 756, Loss: 0.29642683267593384, Final Batch Loss: 0.1122320294380188\n",
      "Subject 5, Epoch 757, Loss: 0.28670964390039444, Final Batch Loss: 0.06999891996383667\n",
      "Subject 5, Epoch 758, Loss: 0.28151355870068073, Final Batch Loss: 0.16319796442985535\n",
      "Subject 5, Epoch 759, Loss: 0.2439979948103428, Final Batch Loss: 0.12123583257198334\n",
      "Subject 5, Epoch 760, Loss: 0.3127601146697998, Final Batch Loss: 0.16735129058361053\n",
      "Subject 5, Epoch 761, Loss: 0.27325355261564255, Final Batch Loss: 0.09270769357681274\n",
      "Subject 5, Epoch 762, Loss: 0.299506813287735, Final Batch Loss: 0.08157370984554291\n",
      "Subject 5, Epoch 763, Loss: 0.3973519392311573, Final Batch Loss: 0.22316455841064453\n",
      "Subject 5, Epoch 764, Loss: 0.32624348998069763, Final Batch Loss: 0.14980483055114746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 765, Loss: 0.43652433156967163, Final Batch Loss: 0.30461153388023376\n",
      "Subject 5, Epoch 766, Loss: 0.4373391643166542, Final Batch Loss: 0.17834995687007904\n",
      "Subject 5, Epoch 767, Loss: 0.3076324798166752, Final Batch Loss: 0.05693945661187172\n",
      "Subject 5, Epoch 768, Loss: 0.3557669520378113, Final Batch Loss: 0.09266096353530884\n",
      "Subject 5, Epoch 769, Loss: 0.30370401032269, Final Batch Loss: 0.02870910055935383\n",
      "Subject 5, Epoch 770, Loss: 0.3432951048016548, Final Batch Loss: 0.10083700716495514\n",
      "Subject 5, Epoch 771, Loss: 0.38848187029361725, Final Batch Loss: 0.10878826677799225\n",
      "Subject 5, Epoch 772, Loss: 0.25494644045829773, Final Batch Loss: 0.0950111672282219\n",
      "Subject 5, Epoch 773, Loss: 0.4224112629890442, Final Batch Loss: 0.19270649552345276\n",
      "Subject 5, Epoch 774, Loss: 0.26952752843499184, Final Batch Loss: 0.04256731644272804\n",
      "Subject 5, Epoch 775, Loss: 0.426835298538208, Final Batch Loss: 0.1205383837223053\n",
      "Subject 5, Epoch 776, Loss: 0.23333408311009407, Final Batch Loss: 0.04355711117386818\n",
      "Subject 5, Epoch 777, Loss: 0.28496976010501385, Final Batch Loss: 0.022175369784235954\n",
      "Subject 5, Epoch 778, Loss: 0.29153358191251755, Final Batch Loss: 0.1173146665096283\n",
      "Subject 5, Epoch 779, Loss: 0.3526289686560631, Final Batch Loss: 0.20233578979969025\n",
      "Subject 5, Epoch 780, Loss: 0.4140525683760643, Final Batch Loss: 0.2061614841222763\n",
      "Subject 5, Epoch 781, Loss: 0.23290769010782242, Final Batch Loss: 0.0706510990858078\n",
      "Subject 5, Epoch 782, Loss: 0.23328259214758873, Final Batch Loss: 0.048116061836481094\n",
      "Subject 5, Epoch 783, Loss: 0.22707219421863556, Final Batch Loss: 0.0627298504114151\n",
      "Subject 5, Epoch 784, Loss: 0.28672138415277004, Final Batch Loss: 0.01670548878610134\n",
      "Subject 5, Epoch 785, Loss: 0.2816162630915642, Final Batch Loss: 0.07619567960500717\n",
      "Subject 5, Epoch 786, Loss: 0.2827602028846741, Final Batch Loss: 0.04651004076004028\n",
      "Subject 5, Epoch 787, Loss: 0.4733025133609772, Final Batch Loss: 0.21478110551834106\n",
      "Subject 5, Epoch 788, Loss: 0.3278256915509701, Final Batch Loss: 0.0824914202094078\n",
      "Subject 5, Epoch 789, Loss: 0.26166602969169617, Final Batch Loss: 0.05929076671600342\n",
      "Subject 5, Epoch 790, Loss: 0.2454759031534195, Final Batch Loss: 0.11451590806245804\n",
      "Subject 5, Epoch 791, Loss: 0.2126683872193098, Final Batch Loss: 0.022283753380179405\n",
      "Subject 5, Epoch 792, Loss: 0.22443341091275215, Final Batch Loss: 0.03518425300717354\n",
      "Subject 5, Epoch 793, Loss: 0.30516238138079643, Final Batch Loss: 0.046710219234228134\n",
      "Subject 5, Epoch 794, Loss: 0.25581325218081474, Final Batch Loss: 0.04037027433514595\n",
      "Subject 5, Epoch 795, Loss: 0.3157433122396469, Final Batch Loss: 0.1562211662530899\n",
      "Subject 5, Epoch 796, Loss: 0.27611613646149635, Final Batch Loss: 0.06073575094342232\n",
      "Subject 5, Epoch 797, Loss: 0.21774085611104965, Final Batch Loss: 0.06263098120689392\n",
      "Subject 5, Epoch 798, Loss: 0.22534522414207458, Final Batch Loss: 0.0450807586312294\n",
      "Subject 5, Epoch 799, Loss: 0.24887677282094955, Final Batch Loss: 0.11067339032888412\n",
      "Subject 5, Epoch 800, Loss: 0.3510085642337799, Final Batch Loss: 0.19774729013442993\n",
      "Subject 5, Epoch 801, Loss: 0.30925144627690315, Final Batch Loss: 0.037241917103528976\n",
      "Subject 5, Epoch 802, Loss: 0.3369702473282814, Final Batch Loss: 0.0553925484418869\n",
      "Subject 5, Epoch 803, Loss: 0.15757703594863415, Final Batch Loss: 0.01684165559709072\n",
      "Subject 5, Epoch 804, Loss: 0.26318372786045074, Final Batch Loss: 0.055320970714092255\n",
      "Subject 5, Epoch 805, Loss: 0.3650432154536247, Final Batch Loss: 0.15841761231422424\n",
      "Subject 5, Epoch 806, Loss: 0.2227340005338192, Final Batch Loss: 0.05053660646080971\n",
      "Subject 5, Epoch 807, Loss: 0.20685197785496712, Final Batch Loss: 0.02716093882918358\n",
      "Subject 5, Epoch 808, Loss: 0.19621071219444275, Final Batch Loss: 0.037433795630931854\n",
      "Subject 5, Epoch 809, Loss: 0.29743487387895584, Final Batch Loss: 0.10130183398723602\n",
      "Subject 5, Epoch 810, Loss: 0.28393589332699776, Final Batch Loss: 0.08639556914567947\n",
      "Subject 5, Epoch 811, Loss: 0.38637080788612366, Final Batch Loss: 0.06649485230445862\n",
      "Subject 5, Epoch 812, Loss: 0.3093440569937229, Final Batch Loss: 0.2066543996334076\n",
      "Subject 5, Epoch 813, Loss: 0.3255707770586014, Final Batch Loss: 0.1829250156879425\n",
      "Subject 5, Epoch 814, Loss: 0.3206256404519081, Final Batch Loss: 0.1570132076740265\n",
      "Subject 5, Epoch 815, Loss: 0.25946538150310516, Final Batch Loss: 0.055863190442323685\n",
      "Subject 5, Epoch 816, Loss: 0.23315851390361786, Final Batch Loss: 0.07823079824447632\n",
      "Subject 5, Epoch 817, Loss: 0.23250193521380424, Final Batch Loss: 0.05543694272637367\n",
      "Subject 5, Epoch 818, Loss: 0.2247212678194046, Final Batch Loss: 0.045495107769966125\n",
      "Subject 5, Epoch 819, Loss: 0.28846466168761253, Final Batch Loss: 0.046979788690805435\n",
      "Subject 5, Epoch 820, Loss: 0.28061749041080475, Final Batch Loss: 0.08418581634759903\n",
      "Subject 5, Epoch 821, Loss: 0.3539513200521469, Final Batch Loss: 0.18442490696907043\n",
      "Subject 5, Epoch 822, Loss: 0.4073401987552643, Final Batch Loss: 0.2154059112071991\n",
      "Subject 5, Epoch 823, Loss: 0.302962027490139, Final Batch Loss: 0.051302842795848846\n",
      "Subject 5, Epoch 824, Loss: 0.20157717168331146, Final Batch Loss: 0.031465500593185425\n",
      "Subject 5, Epoch 825, Loss: 0.2682446241378784, Final Batch Loss: 0.06783650815486908\n",
      "Subject 5, Epoch 826, Loss: 0.40033935755491257, Final Batch Loss: 0.25039759278297424\n",
      "Subject 5, Epoch 827, Loss: 0.19785333424806595, Final Batch Loss: 0.020211540162563324\n",
      "Subject 5, Epoch 828, Loss: 0.25553805753588676, Final Batch Loss: 0.03507016971707344\n",
      "Subject 5, Epoch 829, Loss: 0.26332031935453415, Final Batch Loss: 0.05053327977657318\n",
      "Subject 5, Epoch 830, Loss: 0.23677536472678185, Final Batch Loss: 0.060921091586351395\n",
      "Subject 5, Epoch 831, Loss: 0.24275778234004974, Final Batch Loss: 0.11528059095144272\n",
      "Subject 5, Epoch 832, Loss: 0.2911788560450077, Final Batch Loss: 0.034808602184057236\n",
      "Subject 5, Epoch 833, Loss: 0.24665283411741257, Final Batch Loss: 0.05994534492492676\n",
      "Subject 5, Epoch 834, Loss: 0.2443322129547596, Final Batch Loss: 0.03163729980587959\n",
      "Subject 5, Epoch 835, Loss: 0.3026846647262573, Final Batch Loss: 0.08581675589084625\n",
      "Subject 5, Epoch 836, Loss: 0.26942208409309387, Final Batch Loss: 0.11181161552667618\n",
      "Subject 5, Epoch 837, Loss: 0.34890934079885483, Final Batch Loss: 0.14681373536586761\n",
      "Subject 5, Epoch 838, Loss: 0.14997418131679296, Final Batch Loss: 0.015444441698491573\n",
      "Subject 5, Epoch 839, Loss: 0.1942465454339981, Final Batch Loss: 0.036667946726083755\n",
      "Subject 5, Epoch 840, Loss: 0.18808751925826073, Final Batch Loss: 0.03643961623311043\n",
      "Subject 5, Epoch 841, Loss: 0.25656913593411446, Final Batch Loss: 0.05645701661705971\n",
      "Subject 5, Epoch 842, Loss: 0.3125609755516052, Final Batch Loss: 0.12592089176177979\n",
      "Subject 5, Epoch 843, Loss: 0.30343354493379593, Final Batch Loss: 0.045615509152412415\n",
      "Subject 5, Epoch 844, Loss: 0.27922531589865685, Final Batch Loss: 0.1453854739665985\n",
      "Subject 5, Epoch 845, Loss: 0.1922924853861332, Final Batch Loss: 0.03607169911265373\n",
      "Subject 5, Epoch 846, Loss: 0.26077184453606606, Final Batch Loss: 0.08578832447528839\n",
      "Subject 5, Epoch 847, Loss: 0.35701918974518776, Final Batch Loss: 0.2375486046075821\n",
      "Subject 5, Epoch 848, Loss: 0.2449037805199623, Final Batch Loss: 0.08357088267803192\n",
      "Subject 5, Epoch 849, Loss: 0.25335859321057796, Final Batch Loss: 0.01832406036555767\n",
      "Subject 5, Epoch 850, Loss: 0.3365340307354927, Final Batch Loss: 0.14899659156799316\n",
      "Subject 5, Epoch 851, Loss: 0.2561986595392227, Final Batch Loss: 0.09347009658813477\n",
      "Subject 5, Epoch 852, Loss: 0.3181442953646183, Final Batch Loss: 0.1799713671207428\n",
      "Subject 5, Epoch 853, Loss: 0.3375055007636547, Final Batch Loss: 0.1797887235879898\n",
      "Subject 5, Epoch 854, Loss: 0.21796063520014286, Final Batch Loss: 0.0289728082716465\n",
      "Subject 5, Epoch 855, Loss: 0.2570726126432419, Final Batch Loss: 0.06102851778268814\n",
      "Subject 5, Epoch 856, Loss: 0.16875232011079788, Final Batch Loss: 0.018885686993598938\n",
      "Subject 5, Epoch 857, Loss: 0.23093386366963387, Final Batch Loss: 0.05996899679303169\n",
      "Subject 5, Epoch 858, Loss: 0.2697974964976311, Final Batch Loss: 0.03151770681142807\n",
      "Subject 5, Epoch 859, Loss: 0.2384570725262165, Final Batch Loss: 0.14540839195251465\n",
      "Subject 5, Epoch 860, Loss: 0.32996484637260437, Final Batch Loss: 0.1177859827876091\n",
      "Subject 5, Epoch 861, Loss: 0.3004508949816227, Final Batch Loss: 0.1222362369298935\n",
      "Subject 5, Epoch 862, Loss: 0.2803511843085289, Final Batch Loss: 0.08422324806451797\n",
      "Subject 5, Epoch 863, Loss: 0.3011636324226856, Final Batch Loss: 0.05520404502749443\n",
      "Subject 5, Epoch 864, Loss: 0.24891828000545502, Final Batch Loss: 0.09193142503499985\n",
      "Subject 5, Epoch 865, Loss: 0.25362037867307663, Final Batch Loss: 0.07305224239826202\n",
      "Subject 5, Epoch 866, Loss: 0.28514860570430756, Final Batch Loss: 0.13209277391433716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 867, Loss: 0.23241545632481575, Final Batch Loss: 0.033863555639982224\n",
      "Subject 5, Epoch 868, Loss: 0.186078492552042, Final Batch Loss: 0.03651900216937065\n",
      "Subject 5, Epoch 869, Loss: 0.357402928173542, Final Batch Loss: 0.15370431542396545\n",
      "Subject 5, Epoch 870, Loss: 0.2335994355380535, Final Batch Loss: 0.10384505987167358\n",
      "Subject 5, Epoch 871, Loss: 0.305181160569191, Final Batch Loss: 0.09720174968242645\n",
      "Subject 5, Epoch 872, Loss: 0.22069571167230606, Final Batch Loss: 0.07587136328220367\n",
      "Subject 5, Epoch 873, Loss: 0.19786697812378407, Final Batch Loss: 0.025468742474913597\n",
      "Subject 5, Epoch 874, Loss: 0.30737119168043137, Final Batch Loss: 0.13293448090553284\n",
      "Subject 5, Epoch 875, Loss: 0.27779002487659454, Final Batch Loss: 0.09695751219987869\n",
      "Subject 5, Epoch 876, Loss: 0.24158404394984245, Final Batch Loss: 0.043902281671762466\n",
      "Subject 5, Epoch 877, Loss: 0.334692794829607, Final Batch Loss: 0.1819341480731964\n",
      "Subject 5, Epoch 878, Loss: 0.2799872010946274, Final Batch Loss: 0.07833008468151093\n",
      "Subject 5, Epoch 879, Loss: 0.20631463080644608, Final Batch Loss: 0.06807135045528412\n",
      "Subject 5, Epoch 880, Loss: 0.21003227680921555, Final Batch Loss: 0.029845893383026123\n",
      "Subject 5, Epoch 881, Loss: 0.20422814786434174, Final Batch Loss: 0.06110210344195366\n",
      "Subject 5, Epoch 882, Loss: 0.3597652390599251, Final Batch Loss: 0.11662481725215912\n",
      "Subject 5, Epoch 883, Loss: 0.15166696161031723, Final Batch Loss: 0.022828705608844757\n",
      "Subject 5, Epoch 884, Loss: 0.21094054356217384, Final Batch Loss: 0.05447978153824806\n",
      "Subject 5, Epoch 885, Loss: 0.19246539194136858, Final Batch Loss: 0.013732467778027058\n",
      "Subject 5, Epoch 886, Loss: 0.28400664776563644, Final Batch Loss: 0.15398795902729034\n",
      "Subject 5, Epoch 887, Loss: 0.2975447326898575, Final Batch Loss: 0.12240435928106308\n",
      "Subject 5, Epoch 888, Loss: 0.22555368021130562, Final Batch Loss: 0.03269203379750252\n",
      "Subject 5, Epoch 889, Loss: 0.1553945355117321, Final Batch Loss: 0.03196927160024643\n",
      "Subject 5, Epoch 890, Loss: 0.2621511109173298, Final Batch Loss: 0.04554184153676033\n",
      "Subject 5, Epoch 891, Loss: 0.23098822683095932, Final Batch Loss: 0.12062962353229523\n",
      "Subject 5, Epoch 892, Loss: 0.26559368148446083, Final Batch Loss: 0.05602926388382912\n",
      "Subject 5, Epoch 893, Loss: 0.24944178760051727, Final Batch Loss: 0.061214134097099304\n",
      "Subject 5, Epoch 894, Loss: 0.2350379228591919, Final Batch Loss: 0.05785428360104561\n",
      "Subject 5, Epoch 895, Loss: 0.2830098643898964, Final Batch Loss: 0.09756775945425034\n",
      "Subject 5, Epoch 896, Loss: 0.1796193104237318, Final Batch Loss: 0.023419929668307304\n",
      "Subject 5, Epoch 897, Loss: 0.3004638869315386, Final Batch Loss: 0.02973007597029209\n",
      "Subject 5, Epoch 898, Loss: 0.31155383214354515, Final Batch Loss: 0.16812895238399506\n",
      "Subject 5, Epoch 899, Loss: 0.1809503175318241, Final Batch Loss: 0.08272091299295425\n",
      "Subject 5, Epoch 900, Loss: 0.22752699255943298, Final Batch Loss: 0.08237745612859726\n",
      "Subject 5, Epoch 901, Loss: 0.2724323906004429, Final Batch Loss: 0.058413054794073105\n",
      "Subject 5, Epoch 902, Loss: 0.2470664121210575, Final Batch Loss: 0.1509057581424713\n",
      "Subject 5, Epoch 903, Loss: 0.2190352901816368, Final Batch Loss: 0.13045720756053925\n",
      "Subject 5, Epoch 904, Loss: 0.3275848850607872, Final Batch Loss: 0.05034247785806656\n",
      "Subject 5, Epoch 905, Loss: 0.21331110782921314, Final Batch Loss: 0.02873026765882969\n",
      "Subject 5, Epoch 906, Loss: 0.3181380517780781, Final Batch Loss: 0.1756240725517273\n",
      "Subject 5, Epoch 907, Loss: 0.28937190026044846, Final Batch Loss: 0.08678402751684189\n",
      "Subject 5, Epoch 908, Loss: 0.3637666814029217, Final Batch Loss: 0.1888287216424942\n",
      "Subject 5, Epoch 909, Loss: 0.25162437558174133, Final Batch Loss: 0.09665603190660477\n",
      "Subject 5, Epoch 910, Loss: 0.24028106033802032, Final Batch Loss: 0.08958861976861954\n",
      "Subject 5, Epoch 911, Loss: 0.28717663884162903, Final Batch Loss: 0.08855187892913818\n",
      "Subject 5, Epoch 912, Loss: 0.16407543513923883, Final Batch Loss: 0.013552737422287464\n",
      "Subject 5, Epoch 913, Loss: 0.4552353546023369, Final Batch Loss: 0.23301678895950317\n",
      "Subject 5, Epoch 914, Loss: 0.26190100610256195, Final Batch Loss: 0.05114584416151047\n",
      "Subject 5, Epoch 915, Loss: 0.1244939137250185, Final Batch Loss: 0.01934804953634739\n",
      "Subject 5, Epoch 916, Loss: 0.2417609840631485, Final Batch Loss: 0.08116427809000015\n",
      "Subject 5, Epoch 917, Loss: 0.2216870430856943, Final Batch Loss: 0.028923993930220604\n",
      "Subject 5, Epoch 918, Loss: 0.25576840713620186, Final Batch Loss: 0.07857313007116318\n",
      "Subject 5, Epoch 919, Loss: 0.16128749586641788, Final Batch Loss: 0.03220083564519882\n",
      "Subject 5, Epoch 920, Loss: 0.26545798033475876, Final Batch Loss: 0.10072687268257141\n",
      "Subject 5, Epoch 921, Loss: 0.22077447175979614, Final Batch Loss: 0.057765960693359375\n",
      "Subject 5, Epoch 922, Loss: 0.1994069144129753, Final Batch Loss: 0.035420775413513184\n",
      "Subject 5, Epoch 923, Loss: 0.30503695271909237, Final Batch Loss: 0.023886820301413536\n",
      "Subject 5, Epoch 924, Loss: 0.2613028362393379, Final Batch Loss: 0.09807722270488739\n",
      "Subject 5, Epoch 925, Loss: 0.28634995967149734, Final Batch Loss: 0.06710860133171082\n",
      "Subject 5, Epoch 926, Loss: 0.23866988718509674, Final Batch Loss: 0.04130362719297409\n",
      "Subject 5, Epoch 927, Loss: 0.18879545107483864, Final Batch Loss: 0.02172410860657692\n",
      "Subject 5, Epoch 928, Loss: 0.29869481921195984, Final Batch Loss: 0.09311757981777191\n",
      "Subject 5, Epoch 929, Loss: 0.241080179810524, Final Batch Loss: 0.09818554669618607\n",
      "Subject 5, Epoch 930, Loss: 0.3417200669646263, Final Batch Loss: 0.17521238327026367\n",
      "Subject 5, Epoch 931, Loss: 0.34168989211320877, Final Batch Loss: 0.17140844464302063\n",
      "Subject 5, Epoch 932, Loss: 0.32025671005249023, Final Batch Loss: 0.0699392706155777\n",
      "Subject 5, Epoch 933, Loss: 0.23805741220712662, Final Batch Loss: 0.1172892302274704\n",
      "Subject 5, Epoch 934, Loss: 0.2756784185767174, Final Batch Loss: 0.1237700954079628\n",
      "Subject 5, Epoch 935, Loss: 0.2543295193463564, Final Batch Loss: 0.023368144407868385\n",
      "Subject 5, Epoch 936, Loss: 0.20800837315618992, Final Batch Loss: 0.026744956150650978\n",
      "Subject 5, Epoch 937, Loss: 0.24667851999402046, Final Batch Loss: 0.12767493724822998\n",
      "Subject 5, Epoch 938, Loss: 0.1716779861599207, Final Batch Loss: 0.027316207066178322\n",
      "Subject 5, Epoch 939, Loss: 0.17481735721230507, Final Batch Loss: 0.03582781180739403\n",
      "Subject 5, Epoch 940, Loss: 0.2460954636335373, Final Batch Loss: 0.05951402708888054\n",
      "Subject 5, Epoch 941, Loss: 0.2682429365813732, Final Batch Loss: 0.13037516176700592\n",
      "Subject 5, Epoch 942, Loss: 0.2655176669359207, Final Batch Loss: 0.06614416092634201\n",
      "Subject 5, Epoch 943, Loss: 0.19573272205889225, Final Batch Loss: 0.022855261340737343\n",
      "Subject 5, Epoch 944, Loss: 0.19175306148827076, Final Batch Loss: 0.01529047079384327\n",
      "Subject 5, Epoch 945, Loss: 0.18289890885353088, Final Batch Loss: 0.015441268682479858\n",
      "Subject 5, Epoch 946, Loss: 0.21454432606697083, Final Batch Loss: 0.02235499769449234\n",
      "Subject 5, Epoch 947, Loss: 0.3351047169417143, Final Batch Loss: 0.02486438862979412\n",
      "Subject 5, Epoch 948, Loss: 0.26040009036660194, Final Batch Loss: 0.11876209080219269\n",
      "Subject 5, Epoch 949, Loss: 0.19982701167464256, Final Batch Loss: 0.05691854655742645\n",
      "Subject 5, Epoch 950, Loss: 0.2167794182896614, Final Batch Loss: 0.0704217329621315\n",
      "Subject 5, Epoch 951, Loss: 0.16396553069353104, Final Batch Loss: 0.028492234647274017\n",
      "Subject 5, Epoch 952, Loss: 0.2465113326907158, Final Batch Loss: 0.12184267491102219\n",
      "Subject 5, Epoch 953, Loss: 0.19084259495139122, Final Batch Loss: 0.08867856115102768\n",
      "Subject 5, Epoch 954, Loss: 0.19565852917730808, Final Batch Loss: 0.04536222666501999\n",
      "Subject 5, Epoch 955, Loss: 0.31550053879618645, Final Batch Loss: 0.19063988327980042\n",
      "Subject 5, Epoch 956, Loss: 0.14718615263700485, Final Batch Loss: 0.02977135218679905\n",
      "Subject 5, Epoch 957, Loss: 0.21209252625703812, Final Batch Loss: 0.03423008322715759\n",
      "Subject 5, Epoch 958, Loss: 0.2227858155965805, Final Batch Loss: 0.06726830452680588\n",
      "Subject 5, Epoch 959, Loss: 0.1937037855386734, Final Batch Loss: 0.031112436205148697\n",
      "Subject 5, Epoch 960, Loss: 0.23338771611452103, Final Batch Loss: 0.07040586322546005\n",
      "Subject 5, Epoch 961, Loss: 0.3231877237558365, Final Batch Loss: 0.18487975001335144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 5, Epoch 962, Loss: 0.2221628949046135, Final Batch Loss: 0.0995091050863266\n",
      "Subject 5, Epoch 963, Loss: 0.2973114624619484, Final Batch Loss: 0.09568383544683456\n",
      "Subject 5, Epoch 964, Loss: 0.3305796906352043, Final Batch Loss: 0.2270534187555313\n",
      "Subject 5, Epoch 965, Loss: 0.2903764620423317, Final Batch Loss: 0.08052214235067368\n",
      "Subject 5, Epoch 966, Loss: 0.46162325888872147, Final Batch Loss: 0.2660445272922516\n",
      "Subject 5, Epoch 967, Loss: 0.22863061726093292, Final Batch Loss: 0.043891213834285736\n",
      "Subject 5, Epoch 968, Loss: 0.28163136541843414, Final Batch Loss: 0.13467088341712952\n",
      "Subject 5, Epoch 969, Loss: 0.26361748576164246, Final Batch Loss: 0.062272101640701294\n",
      "Subject 5, Epoch 970, Loss: 0.20578201115131378, Final Batch Loss: 0.06353756040334702\n",
      "Subject 5, Epoch 971, Loss: 0.2848040610551834, Final Batch Loss: 0.11265350878238678\n",
      "Subject 5, Epoch 972, Loss: 0.314955472946167, Final Batch Loss: 0.04450313746929169\n",
      "Subject 5, Epoch 973, Loss: 0.28355409763753414, Final Batch Loss: 0.0197500791400671\n",
      "Subject 5, Epoch 974, Loss: 0.27196385711431503, Final Batch Loss: 0.12986204028129578\n",
      "Subject 5, Epoch 975, Loss: 0.2181049808859825, Final Batch Loss: 0.06421709060668945\n",
      "Subject 5, Epoch 976, Loss: 0.2416469268500805, Final Batch Loss: 0.11618471145629883\n",
      "Subject 5, Epoch 977, Loss: 0.15312873758375645, Final Batch Loss: 0.020396823063492775\n",
      "Subject 5, Epoch 978, Loss: 0.20561949536204338, Final Batch Loss: 0.037775252014398575\n",
      "Subject 5, Epoch 979, Loss: 0.2364901565015316, Final Batch Loss: 0.12742571532726288\n",
      "Subject 5, Epoch 980, Loss: 0.4102705791592598, Final Batch Loss: 0.24226126074790955\n",
      "Subject 5, Epoch 981, Loss: 0.23353976756334305, Final Batch Loss: 0.08123678714036942\n",
      "Subject 5, Epoch 982, Loss: 0.20099477097392082, Final Batch Loss: 0.03453924134373665\n",
      "Subject 5, Epoch 983, Loss: 0.41472842544317245, Final Batch Loss: 0.27012747526168823\n",
      "Subject 5, Epoch 984, Loss: 0.15900219604372978, Final Batch Loss: 0.05373138561844826\n",
      "Subject 5, Epoch 985, Loss: 0.18820731714367867, Final Batch Loss: 0.031185489147901535\n",
      "Subject 5, Epoch 986, Loss: 0.19267859309911728, Final Batch Loss: 0.1221688911318779\n",
      "Subject 5, Epoch 987, Loss: 0.22824366763234138, Final Batch Loss: 0.1238836869597435\n",
      "Subject 5, Epoch 988, Loss: 0.25349491462111473, Final Batch Loss: 0.06915954500436783\n",
      "Subject 5, Epoch 989, Loss: 0.2950911819934845, Final Batch Loss: 0.06557141989469528\n",
      "Subject 5, Epoch 990, Loss: 0.25185613334178925, Final Batch Loss: 0.019956186413764954\n",
      "Subject 5, Epoch 991, Loss: 0.19919083453714848, Final Batch Loss: 0.028637418523430824\n",
      "Subject 5, Epoch 992, Loss: 0.3211235851049423, Final Batch Loss: 0.14953003823757172\n",
      "Subject 5, Epoch 993, Loss: 0.17628882639110088, Final Batch Loss: 0.024292780086398125\n",
      "Subject 5, Epoch 994, Loss: 0.3302431143820286, Final Batch Loss: 0.1637769192457199\n",
      "Subject 5, Epoch 995, Loss: 0.2678850404918194, Final Batch Loss: 0.17480848729610443\n",
      "Subject 5, Epoch 996, Loss: 0.2775754854083061, Final Batch Loss: 0.08480587601661682\n",
      "Subject 5, Epoch 997, Loss: 0.16367910616099834, Final Batch Loss: 0.02182929404079914\n",
      "Subject 5, Epoch 998, Loss: 0.1707184575498104, Final Batch Loss: 0.014191884547472\n",
      "Subject 5, Epoch 999, Loss: 0.33042390644550323, Final Batch Loss: 0.08911552280187607\n",
      "Subject 5, Epoch 1000, Loss: 0.20488915592432022, Final Batch Loss: 0.015400372445583344\n",
      "Subject 6, Epoch 1, Loss: 5.384754776954651, Final Batch Loss: 1.802501916885376\n",
      "Subject 6, Epoch 2, Loss: 5.375153064727783, Final Batch Loss: 1.780664324760437\n",
      "Subject 6, Epoch 3, Loss: 5.369676351547241, Final Batch Loss: 1.7762043476104736\n",
      "Subject 6, Epoch 4, Loss: 5.36503005027771, Final Batch Loss: 1.7999364137649536\n",
      "Subject 6, Epoch 5, Loss: 5.353692531585693, Final Batch Loss: 1.7749360799789429\n",
      "Subject 6, Epoch 6, Loss: 5.350760340690613, Final Batch Loss: 1.7824901342391968\n",
      "Subject 6, Epoch 7, Loss: 5.337461233139038, Final Batch Loss: 1.7723546028137207\n",
      "Subject 6, Epoch 8, Loss: 5.31993556022644, Final Batch Loss: 1.7751545906066895\n",
      "Subject 6, Epoch 9, Loss: 5.3093143701553345, Final Batch Loss: 1.7612396478652954\n",
      "Subject 6, Epoch 10, Loss: 5.287276029586792, Final Batch Loss: 1.7636425495147705\n",
      "Subject 6, Epoch 11, Loss: 5.2882760763168335, Final Batch Loss: 1.7696001529693604\n",
      "Subject 6, Epoch 12, Loss: 5.269154667854309, Final Batch Loss: 1.754793405532837\n",
      "Subject 6, Epoch 13, Loss: 5.237555980682373, Final Batch Loss: 1.749715805053711\n",
      "Subject 6, Epoch 14, Loss: 5.200402021408081, Final Batch Loss: 1.7132863998413086\n",
      "Subject 6, Epoch 15, Loss: 5.189786314964294, Final Batch Loss: 1.7294931411743164\n",
      "Subject 6, Epoch 16, Loss: 5.149389147758484, Final Batch Loss: 1.6955889463424683\n",
      "Subject 6, Epoch 17, Loss: 5.1015074253082275, Final Batch Loss: 1.6694306135177612\n",
      "Subject 6, Epoch 18, Loss: 5.093830704689026, Final Batch Loss: 1.689041018486023\n",
      "Subject 6, Epoch 19, Loss: 5.026747703552246, Final Batch Loss: 1.6673448085784912\n",
      "Subject 6, Epoch 20, Loss: 4.9682207107543945, Final Batch Loss: 1.656287431716919\n",
      "Subject 6, Epoch 21, Loss: 4.914742469787598, Final Batch Loss: 1.6167563199996948\n",
      "Subject 6, Epoch 22, Loss: 4.813247442245483, Final Batch Loss: 1.6199002265930176\n",
      "Subject 6, Epoch 23, Loss: 4.76267397403717, Final Batch Loss: 1.5897983312606812\n",
      "Subject 6, Epoch 24, Loss: 4.667507648468018, Final Batch Loss: 1.4912954568862915\n",
      "Subject 6, Epoch 25, Loss: 4.5463608503341675, Final Batch Loss: 1.4703727960586548\n",
      "Subject 6, Epoch 26, Loss: 4.38650369644165, Final Batch Loss: 1.4343186616897583\n",
      "Subject 6, Epoch 27, Loss: 4.296212673187256, Final Batch Loss: 1.4014389514923096\n",
      "Subject 6, Epoch 28, Loss: 4.208593964576721, Final Batch Loss: 1.389044165611267\n",
      "Subject 6, Epoch 29, Loss: 4.066174745559692, Final Batch Loss: 1.3817708492279053\n",
      "Subject 6, Epoch 30, Loss: 4.011852025985718, Final Batch Loss: 1.2856621742248535\n",
      "Subject 6, Epoch 31, Loss: 3.774491310119629, Final Batch Loss: 1.243385672569275\n",
      "Subject 6, Epoch 32, Loss: 3.851115107536316, Final Batch Loss: 1.2898579835891724\n",
      "Subject 6, Epoch 33, Loss: 3.676757574081421, Final Batch Loss: 1.2154232263565063\n",
      "Subject 6, Epoch 34, Loss: 3.60797381401062, Final Batch Loss: 1.1616169214248657\n",
      "Subject 6, Epoch 35, Loss: 3.6018593311309814, Final Batch Loss: 1.159147024154663\n",
      "Subject 6, Epoch 36, Loss: 3.559637427330017, Final Batch Loss: 1.2454941272735596\n",
      "Subject 6, Epoch 37, Loss: 3.4279967546463013, Final Batch Loss: 1.159217119216919\n",
      "Subject 6, Epoch 38, Loss: 3.3285887241363525, Final Batch Loss: 1.0926674604415894\n",
      "Subject 6, Epoch 39, Loss: 3.27663791179657, Final Batch Loss: 1.0406183004379272\n",
      "Subject 6, Epoch 40, Loss: 3.205446243286133, Final Batch Loss: 1.0185216665267944\n",
      "Subject 6, Epoch 41, Loss: 3.1001726388931274, Final Batch Loss: 0.9821666479110718\n",
      "Subject 6, Epoch 42, Loss: 3.0813610553741455, Final Batch Loss: 1.000450611114502\n",
      "Subject 6, Epoch 43, Loss: 3.0128133893013, Final Batch Loss: 0.9550068974494934\n",
      "Subject 6, Epoch 44, Loss: 2.9278960824012756, Final Batch Loss: 0.9356899857521057\n",
      "Subject 6, Epoch 45, Loss: 2.9995086193084717, Final Batch Loss: 1.030285120010376\n",
      "Subject 6, Epoch 46, Loss: 2.8427481651306152, Final Batch Loss: 0.9319367408752441\n",
      "Subject 6, Epoch 47, Loss: 2.8802936673164368, Final Batch Loss: 0.9751205444335938\n",
      "Subject 6, Epoch 48, Loss: 2.79092538356781, Final Batch Loss: 0.8799285888671875\n",
      "Subject 6, Epoch 49, Loss: 2.6786510944366455, Final Batch Loss: 0.926051139831543\n",
      "Subject 6, Epoch 50, Loss: 2.6886473298072815, Final Batch Loss: 0.9073114991188049\n",
      "Subject 6, Epoch 51, Loss: 2.693345844745636, Final Batch Loss: 0.9006543159484863\n",
      "Subject 6, Epoch 52, Loss: 2.547969698905945, Final Batch Loss: 0.8654578328132629\n",
      "Subject 6, Epoch 53, Loss: 2.6218056678771973, Final Batch Loss: 0.8554643392562866\n",
      "Subject 6, Epoch 54, Loss: 2.574488639831543, Final Batch Loss: 0.8734087944030762\n",
      "Subject 6, Epoch 55, Loss: 2.4526700377464294, Final Batch Loss: 0.701423168182373\n",
      "Subject 6, Epoch 56, Loss: 2.386122941970825, Final Batch Loss: 0.7262715697288513\n",
      "Subject 6, Epoch 57, Loss: 2.475046396255493, Final Batch Loss: 0.7471197843551636\n",
      "Subject 6, Epoch 58, Loss: 2.405560076236725, Final Batch Loss: 0.6903600692749023\n",
      "Subject 6, Epoch 59, Loss: 2.3539084792137146, Final Batch Loss: 0.7783348560333252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 60, Loss: 2.265691578388214, Final Batch Loss: 0.7329780459403992\n",
      "Subject 6, Epoch 61, Loss: 2.374021291732788, Final Batch Loss: 0.7344172596931458\n",
      "Subject 6, Epoch 62, Loss: 2.3240773677825928, Final Batch Loss: 0.8336989879608154\n",
      "Subject 6, Epoch 63, Loss: 2.243037521839142, Final Batch Loss: 0.6228991150856018\n",
      "Subject 6, Epoch 64, Loss: 2.294105887413025, Final Batch Loss: 0.6409685611724854\n",
      "Subject 6, Epoch 65, Loss: 2.3963958024978638, Final Batch Loss: 0.8983094692230225\n",
      "Subject 6, Epoch 66, Loss: 2.1686779856681824, Final Batch Loss: 0.7125044465065002\n",
      "Subject 6, Epoch 67, Loss: 2.2356629967689514, Final Batch Loss: 0.648125946521759\n",
      "Subject 6, Epoch 68, Loss: 2.140625238418579, Final Batch Loss: 0.678528368473053\n",
      "Subject 6, Epoch 69, Loss: 2.1499058604240417, Final Batch Loss: 0.8029232025146484\n",
      "Subject 6, Epoch 70, Loss: 2.140376389026642, Final Batch Loss: 0.5149140357971191\n",
      "Subject 6, Epoch 71, Loss: 2.149965286254883, Final Batch Loss: 0.7033661007881165\n",
      "Subject 6, Epoch 72, Loss: 2.153334677219391, Final Batch Loss: 0.7846182584762573\n",
      "Subject 6, Epoch 73, Loss: 2.0632749795913696, Final Batch Loss: 0.6895308494567871\n",
      "Subject 6, Epoch 74, Loss: 2.0617522597312927, Final Batch Loss: 0.6043132543563843\n",
      "Subject 6, Epoch 75, Loss: 2.0644344687461853, Final Batch Loss: 0.6913306713104248\n",
      "Subject 6, Epoch 76, Loss: 2.1111359000205994, Final Batch Loss: 0.6208096742630005\n",
      "Subject 6, Epoch 77, Loss: 1.9468187093734741, Final Batch Loss: 0.641478955745697\n",
      "Subject 6, Epoch 78, Loss: 2.039778172969818, Final Batch Loss: 0.7257341146469116\n",
      "Subject 6, Epoch 79, Loss: 2.0185433626174927, Final Batch Loss: 0.7235543131828308\n",
      "Subject 6, Epoch 80, Loss: 1.9731456637382507, Final Batch Loss: 0.6474102139472961\n",
      "Subject 6, Epoch 81, Loss: 2.030488908290863, Final Batch Loss: 0.7527012228965759\n",
      "Subject 6, Epoch 82, Loss: 2.011810839176178, Final Batch Loss: 0.6395828127861023\n",
      "Subject 6, Epoch 83, Loss: 1.9522691369056702, Final Batch Loss: 0.6389465928077698\n",
      "Subject 6, Epoch 84, Loss: 2.13028746843338, Final Batch Loss: 0.6445935368537903\n",
      "Subject 6, Epoch 85, Loss: 2.0177716612815857, Final Batch Loss: 0.6027156114578247\n",
      "Subject 6, Epoch 86, Loss: 1.943983256816864, Final Batch Loss: 0.7431285977363586\n",
      "Subject 6, Epoch 87, Loss: 1.9880983233451843, Final Batch Loss: 0.7019403576850891\n",
      "Subject 6, Epoch 88, Loss: 2.032006323337555, Final Batch Loss: 0.6044226288795471\n",
      "Subject 6, Epoch 89, Loss: 1.9394681453704834, Final Batch Loss: 0.8065632581710815\n",
      "Subject 6, Epoch 90, Loss: 2.0657660365104675, Final Batch Loss: 0.6376830339431763\n",
      "Subject 6, Epoch 91, Loss: 1.8959405422210693, Final Batch Loss: 0.6705324053764343\n",
      "Subject 6, Epoch 92, Loss: 1.8970463275909424, Final Batch Loss: 0.6968104243278503\n",
      "Subject 6, Epoch 93, Loss: 1.9225012063980103, Final Batch Loss: 0.63566654920578\n",
      "Subject 6, Epoch 94, Loss: 1.9957435131072998, Final Batch Loss: 0.7029957175254822\n",
      "Subject 6, Epoch 95, Loss: 1.8648415207862854, Final Batch Loss: 0.6273139119148254\n",
      "Subject 6, Epoch 96, Loss: 1.7032655477523804, Final Batch Loss: 0.5354593396186829\n",
      "Subject 6, Epoch 97, Loss: 1.9093269109725952, Final Batch Loss: 0.6749056577682495\n",
      "Subject 6, Epoch 98, Loss: 1.8031136393547058, Final Batch Loss: 0.5677440762519836\n",
      "Subject 6, Epoch 99, Loss: 1.8628947138786316, Final Batch Loss: 0.6402774453163147\n",
      "Subject 6, Epoch 100, Loss: 1.865996778011322, Final Batch Loss: 0.5847683548927307\n",
      "Subject 6, Epoch 101, Loss: 1.8234949111938477, Final Batch Loss: 0.5621550679206848\n",
      "Subject 6, Epoch 102, Loss: 1.8508595824241638, Final Batch Loss: 0.7388328909873962\n",
      "Subject 6, Epoch 103, Loss: 1.898567795753479, Final Batch Loss: 0.6397345662117004\n",
      "Subject 6, Epoch 104, Loss: 1.8829911947250366, Final Batch Loss: 0.656460165977478\n",
      "Subject 6, Epoch 105, Loss: 1.8618899583816528, Final Batch Loss: 0.5366789102554321\n",
      "Subject 6, Epoch 106, Loss: 1.7815864086151123, Final Batch Loss: 0.6231428980827332\n",
      "Subject 6, Epoch 107, Loss: 1.8269342184066772, Final Batch Loss: 0.5587623119354248\n",
      "Subject 6, Epoch 108, Loss: 1.821253478527069, Final Batch Loss: 0.6170345544815063\n",
      "Subject 6, Epoch 109, Loss: 1.7619727849960327, Final Batch Loss: 0.611508846282959\n",
      "Subject 6, Epoch 110, Loss: 1.8635923862457275, Final Batch Loss: 0.6737104654312134\n",
      "Subject 6, Epoch 111, Loss: 1.7176296710968018, Final Batch Loss: 0.5583663582801819\n",
      "Subject 6, Epoch 112, Loss: 1.7741591930389404, Final Batch Loss: 0.6460481882095337\n",
      "Subject 6, Epoch 113, Loss: 1.8105775713920593, Final Batch Loss: 0.5857834815979004\n",
      "Subject 6, Epoch 114, Loss: 1.7942595183849335, Final Batch Loss: 0.6053594946861267\n",
      "Subject 6, Epoch 115, Loss: 1.764319658279419, Final Batch Loss: 0.6057143211364746\n",
      "Subject 6, Epoch 116, Loss: 1.7012958228588104, Final Batch Loss: 0.4431632459163666\n",
      "Subject 6, Epoch 117, Loss: 1.7153212428092957, Final Batch Loss: 0.5304653644561768\n",
      "Subject 6, Epoch 118, Loss: 1.7313036918640137, Final Batch Loss: 0.5038511157035828\n",
      "Subject 6, Epoch 119, Loss: 1.7251463532447815, Final Batch Loss: 0.5875139832496643\n",
      "Subject 6, Epoch 120, Loss: 1.646422415971756, Final Batch Loss: 0.49826520681381226\n",
      "Subject 6, Epoch 121, Loss: 1.7711371779441833, Final Batch Loss: 0.6212663650512695\n",
      "Subject 6, Epoch 122, Loss: 1.7054545283317566, Final Batch Loss: 0.5356675386428833\n",
      "Subject 6, Epoch 123, Loss: 1.6621322929859161, Final Batch Loss: 0.49568411707878113\n",
      "Subject 6, Epoch 124, Loss: 1.6674168109893799, Final Batch Loss: 0.5235627293586731\n",
      "Subject 6, Epoch 125, Loss: 1.6603946685791016, Final Batch Loss: 0.5170028805732727\n",
      "Subject 6, Epoch 126, Loss: 1.7290148735046387, Final Batch Loss: 0.6794641017913818\n",
      "Subject 6, Epoch 127, Loss: 1.6254122257232666, Final Batch Loss: 0.5059959292411804\n",
      "Subject 6, Epoch 128, Loss: 1.6512857675552368, Final Batch Loss: 0.5586850047111511\n",
      "Subject 6, Epoch 129, Loss: 1.7537999153137207, Final Batch Loss: 0.5761774182319641\n",
      "Subject 6, Epoch 130, Loss: 1.6505016088485718, Final Batch Loss: 0.5186725854873657\n",
      "Subject 6, Epoch 131, Loss: 1.6839331984519958, Final Batch Loss: 0.5183801054954529\n",
      "Subject 6, Epoch 132, Loss: 1.710062325000763, Final Batch Loss: 0.6149633526802063\n",
      "Subject 6, Epoch 133, Loss: 1.6592450439929962, Final Batch Loss: 0.5853612422943115\n",
      "Subject 6, Epoch 134, Loss: 1.6777451038360596, Final Batch Loss: 0.5330047011375427\n",
      "Subject 6, Epoch 135, Loss: 1.6794236898422241, Final Batch Loss: 0.5547919869422913\n",
      "Subject 6, Epoch 136, Loss: 1.5712899267673492, Final Batch Loss: 0.5732709765434265\n",
      "Subject 6, Epoch 137, Loss: 1.567445993423462, Final Batch Loss: 0.5608406066894531\n",
      "Subject 6, Epoch 138, Loss: 1.6541181206703186, Final Batch Loss: 0.5854073762893677\n",
      "Subject 6, Epoch 139, Loss: 1.6181285679340363, Final Batch Loss: 0.6608629822731018\n",
      "Subject 6, Epoch 140, Loss: 1.5514505803585052, Final Batch Loss: 0.4554388225078583\n",
      "Subject 6, Epoch 141, Loss: 1.5907052755355835, Final Batch Loss: 0.5721735954284668\n",
      "Subject 6, Epoch 142, Loss: 1.6458596885204315, Final Batch Loss: 0.5437185168266296\n",
      "Subject 6, Epoch 143, Loss: 1.4317232966423035, Final Batch Loss: 0.4123867154121399\n",
      "Subject 6, Epoch 144, Loss: 1.549501746892929, Final Batch Loss: 0.5590695142745972\n",
      "Subject 6, Epoch 145, Loss: 1.5095354318618774, Final Batch Loss: 0.5003304481506348\n",
      "Subject 6, Epoch 146, Loss: 1.6126945614814758, Final Batch Loss: 0.5380158424377441\n",
      "Subject 6, Epoch 147, Loss: 1.4381811916828156, Final Batch Loss: 0.43642330169677734\n",
      "Subject 6, Epoch 148, Loss: 1.5278976559638977, Final Batch Loss: 0.5061311721801758\n",
      "Subject 6, Epoch 149, Loss: 1.5659191310405731, Final Batch Loss: 0.6928658485412598\n",
      "Subject 6, Epoch 150, Loss: 1.631687879562378, Final Batch Loss: 0.6038325428962708\n",
      "Subject 6, Epoch 151, Loss: 1.531906008720398, Final Batch Loss: 0.5489396452903748\n",
      "Subject 6, Epoch 152, Loss: 1.61102893948555, Final Batch Loss: 0.5939108729362488\n",
      "Subject 6, Epoch 153, Loss: 1.5269732177257538, Final Batch Loss: 0.44820791482925415\n",
      "Subject 6, Epoch 154, Loss: 1.536048799753189, Final Batch Loss: 0.5210392475128174\n",
      "Subject 6, Epoch 155, Loss: 1.4734476804733276, Final Batch Loss: 0.4339084029197693\n",
      "Subject 6, Epoch 156, Loss: 1.5041044056415558, Final Batch Loss: 0.47323843836784363\n",
      "Subject 6, Epoch 157, Loss: 1.4320567846298218, Final Batch Loss: 0.37855690717697144\n",
      "Subject 6, Epoch 158, Loss: 1.4028933644294739, Final Batch Loss: 0.40077757835388184\n",
      "Subject 6, Epoch 159, Loss: 1.4778233468532562, Final Batch Loss: 0.5273598432540894\n",
      "Subject 6, Epoch 160, Loss: 1.4515957236289978, Final Batch Loss: 0.458366334438324\n",
      "Subject 6, Epoch 161, Loss: 1.3602072298526764, Final Batch Loss: 0.496462881565094\n",
      "Subject 6, Epoch 162, Loss: 1.3524717688560486, Final Batch Loss: 0.38689664006233215\n",
      "Subject 6, Epoch 163, Loss: 1.4039219617843628, Final Batch Loss: 0.44329142570495605\n",
      "Subject 6, Epoch 164, Loss: 1.5170482695102692, Final Batch Loss: 0.565902829170227\n",
      "Subject 6, Epoch 165, Loss: 1.4282336235046387, Final Batch Loss: 0.5158246755599976\n",
      "Subject 6, Epoch 166, Loss: 1.3687471747398376, Final Batch Loss: 0.38431113958358765\n",
      "Subject 6, Epoch 167, Loss: 1.4609692990779877, Final Batch Loss: 0.5005773901939392\n",
      "Subject 6, Epoch 168, Loss: 1.3176060318946838, Final Batch Loss: 0.4364163279533386\n",
      "Subject 6, Epoch 169, Loss: 1.3829921185970306, Final Batch Loss: 0.39498305320739746\n",
      "Subject 6, Epoch 170, Loss: 1.4979530572891235, Final Batch Loss: 0.5921867489814758\n",
      "Subject 6, Epoch 171, Loss: 1.4413208663463593, Final Batch Loss: 0.39666077494621277\n",
      "Subject 6, Epoch 172, Loss: 1.334408462047577, Final Batch Loss: 0.4213261604309082\n",
      "Subject 6, Epoch 173, Loss: 1.3892937302589417, Final Batch Loss: 0.5080903172492981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 174, Loss: 1.3920701742172241, Final Batch Loss: 0.5535791516304016\n",
      "Subject 6, Epoch 175, Loss: 1.4583901762962341, Final Batch Loss: 0.4759151339530945\n",
      "Subject 6, Epoch 176, Loss: 1.4529722034931183, Final Batch Loss: 0.5397054553031921\n",
      "Subject 6, Epoch 177, Loss: 1.4271741807460785, Final Batch Loss: 0.5159595012664795\n",
      "Subject 6, Epoch 178, Loss: 1.3801371455192566, Final Batch Loss: 0.5085129737854004\n",
      "Subject 6, Epoch 179, Loss: 1.4181542992591858, Final Batch Loss: 0.4623644948005676\n",
      "Subject 6, Epoch 180, Loss: 1.3979353308677673, Final Batch Loss: 0.5188741087913513\n",
      "Subject 6, Epoch 181, Loss: 1.337151825428009, Final Batch Loss: 0.4686156213283539\n",
      "Subject 6, Epoch 182, Loss: 1.2472716271877289, Final Batch Loss: 0.4083670973777771\n",
      "Subject 6, Epoch 183, Loss: 1.3109722435474396, Final Batch Loss: 0.5167737007141113\n",
      "Subject 6, Epoch 184, Loss: 1.421210616827011, Final Batch Loss: 0.5846031904220581\n",
      "Subject 6, Epoch 185, Loss: 1.3295840620994568, Final Batch Loss: 0.4454321265220642\n",
      "Subject 6, Epoch 186, Loss: 1.3471346497535706, Final Batch Loss: 0.47385984659194946\n",
      "Subject 6, Epoch 187, Loss: 1.3288580477237701, Final Batch Loss: 0.546708881855011\n",
      "Subject 6, Epoch 188, Loss: 1.3535771667957306, Final Batch Loss: 0.5385306477546692\n",
      "Subject 6, Epoch 189, Loss: 1.298872172832489, Final Batch Loss: 0.47624868154525757\n",
      "Subject 6, Epoch 190, Loss: 1.2938469648361206, Final Batch Loss: 0.4047718048095703\n",
      "Subject 6, Epoch 191, Loss: 1.3291717767715454, Final Batch Loss: 0.429762065410614\n",
      "Subject 6, Epoch 192, Loss: 1.3185822367668152, Final Batch Loss: 0.34017008543014526\n",
      "Subject 6, Epoch 193, Loss: 1.298610121011734, Final Batch Loss: 0.439816415309906\n",
      "Subject 6, Epoch 194, Loss: 1.3985695838928223, Final Batch Loss: 0.4263560175895691\n",
      "Subject 6, Epoch 195, Loss: 1.3148274421691895, Final Batch Loss: 0.5670124888420105\n",
      "Subject 6, Epoch 196, Loss: 1.2921060621738434, Final Batch Loss: 0.3921799659729004\n",
      "Subject 6, Epoch 197, Loss: 1.3391835987567902, Final Batch Loss: 0.43515893816947937\n",
      "Subject 6, Epoch 198, Loss: 1.3596068918704987, Final Batch Loss: 0.5343576669692993\n",
      "Subject 6, Epoch 199, Loss: 1.4705961346626282, Final Batch Loss: 0.5698862671852112\n",
      "Subject 6, Epoch 200, Loss: 1.2921357154846191, Final Batch Loss: 0.3844391405582428\n",
      "Subject 6, Epoch 201, Loss: 1.3364686965942383, Final Batch Loss: 0.40350374579429626\n",
      "Subject 6, Epoch 202, Loss: 1.2799142897129059, Final Batch Loss: 0.4110454022884369\n",
      "Subject 6, Epoch 203, Loss: 1.253836840391159, Final Batch Loss: 0.40048307180404663\n",
      "Subject 6, Epoch 204, Loss: 1.2389204502105713, Final Batch Loss: 0.2981805205345154\n",
      "Subject 6, Epoch 205, Loss: 1.3025280237197876, Final Batch Loss: 0.5015427470207214\n",
      "Subject 6, Epoch 206, Loss: 1.3139037489891052, Final Batch Loss: 0.4856763184070587\n",
      "Subject 6, Epoch 207, Loss: 1.2629924714565277, Final Batch Loss: 0.3428182005882263\n",
      "Subject 6, Epoch 208, Loss: 1.341675341129303, Final Batch Loss: 0.5170039534568787\n",
      "Subject 6, Epoch 209, Loss: 1.3618776202201843, Final Batch Loss: 0.4512052536010742\n",
      "Subject 6, Epoch 210, Loss: 1.2871573567390442, Final Batch Loss: 0.450309693813324\n",
      "Subject 6, Epoch 211, Loss: 1.2482293546199799, Final Batch Loss: 0.4419252574443817\n",
      "Subject 6, Epoch 212, Loss: 1.2557460069656372, Final Batch Loss: 0.42619994282722473\n",
      "Subject 6, Epoch 213, Loss: 1.155817449092865, Final Batch Loss: 0.36740925908088684\n",
      "Subject 6, Epoch 214, Loss: 1.2263526022434235, Final Batch Loss: 0.40925833582878113\n",
      "Subject 6, Epoch 215, Loss: 1.247860610485077, Final Batch Loss: 0.40431204438209534\n",
      "Subject 6, Epoch 216, Loss: 1.394856870174408, Final Batch Loss: 0.5223979949951172\n",
      "Subject 6, Epoch 217, Loss: 1.2260954082012177, Final Batch Loss: 0.38078391551971436\n",
      "Subject 6, Epoch 218, Loss: 1.2643497288227081, Final Batch Loss: 0.3843128979206085\n",
      "Subject 6, Epoch 219, Loss: 1.2305217683315277, Final Batch Loss: 0.31536000967025757\n",
      "Subject 6, Epoch 220, Loss: 1.3343788385391235, Final Batch Loss: 0.4797627925872803\n",
      "Subject 6, Epoch 221, Loss: 1.2356602549552917, Final Batch Loss: 0.4426433742046356\n",
      "Subject 6, Epoch 222, Loss: 1.1711308658123016, Final Batch Loss: 0.4092658460140228\n",
      "Subject 6, Epoch 223, Loss: 1.17496258020401, Final Batch Loss: 0.41334307193756104\n",
      "Subject 6, Epoch 224, Loss: 1.3287199437618256, Final Batch Loss: 0.5294375419616699\n",
      "Subject 6, Epoch 225, Loss: 1.2422773241996765, Final Batch Loss: 0.46802273392677307\n",
      "Subject 6, Epoch 226, Loss: 1.2626432478427887, Final Batch Loss: 0.3906984329223633\n",
      "Subject 6, Epoch 227, Loss: 1.233093410730362, Final Batch Loss: 0.48424193263053894\n",
      "Subject 6, Epoch 228, Loss: 1.2175536751747131, Final Batch Loss: 0.44601547718048096\n",
      "Subject 6, Epoch 229, Loss: 1.3127601742744446, Final Batch Loss: 0.4563444256782532\n",
      "Subject 6, Epoch 230, Loss: 1.1070975363254547, Final Batch Loss: 0.28151601552963257\n",
      "Subject 6, Epoch 231, Loss: 1.2360528707504272, Final Batch Loss: 0.38945889472961426\n",
      "Subject 6, Epoch 232, Loss: 1.1650396883487701, Final Batch Loss: 0.4700462520122528\n",
      "Subject 6, Epoch 233, Loss: 1.1386046409606934, Final Batch Loss: 0.34567293524742126\n",
      "Subject 6, Epoch 234, Loss: 1.2840566039085388, Final Batch Loss: 0.4297139644622803\n",
      "Subject 6, Epoch 235, Loss: 1.277157485485077, Final Batch Loss: 0.3762132525444031\n",
      "Subject 6, Epoch 236, Loss: 1.223928302526474, Final Batch Loss: 0.44491031765937805\n",
      "Subject 6, Epoch 237, Loss: 1.137654423713684, Final Batch Loss: 0.3505266010761261\n",
      "Subject 6, Epoch 238, Loss: 1.2222716510295868, Final Batch Loss: 0.3411426246166229\n",
      "Subject 6, Epoch 239, Loss: 1.1094946563243866, Final Batch Loss: 0.36671969294548035\n",
      "Subject 6, Epoch 240, Loss: 1.162068098783493, Final Batch Loss: 0.3969060480594635\n",
      "Subject 6, Epoch 241, Loss: 1.176988422870636, Final Batch Loss: 0.42380765080451965\n",
      "Subject 6, Epoch 242, Loss: 1.1756005883216858, Final Batch Loss: 0.38728243112564087\n",
      "Subject 6, Epoch 243, Loss: 1.1442917883396149, Final Batch Loss: 0.41614970564842224\n",
      "Subject 6, Epoch 244, Loss: 1.200534164905548, Final Batch Loss: 0.4265028238296509\n",
      "Subject 6, Epoch 245, Loss: 1.1864685118198395, Final Batch Loss: 0.47736912965774536\n",
      "Subject 6, Epoch 246, Loss: 1.2451781332492828, Final Batch Loss: 0.3770531117916107\n",
      "Subject 6, Epoch 247, Loss: 1.372177004814148, Final Batch Loss: 0.5946440100669861\n",
      "Subject 6, Epoch 248, Loss: 1.1979438066482544, Final Batch Loss: 0.32632824778556824\n",
      "Subject 6, Epoch 249, Loss: 1.2284118831157684, Final Batch Loss: 0.4702177047729492\n",
      "Subject 6, Epoch 250, Loss: 1.140143722295761, Final Batch Loss: 0.2360243797302246\n",
      "Subject 6, Epoch 251, Loss: 1.0530739426612854, Final Batch Loss: 0.3007678687572479\n",
      "Subject 6, Epoch 252, Loss: 1.1632711291313171, Final Batch Loss: 0.39837557077407837\n",
      "Subject 6, Epoch 253, Loss: 1.0673702657222748, Final Batch Loss: 0.3305720388889313\n",
      "Subject 6, Epoch 254, Loss: 1.1112992763519287, Final Batch Loss: 0.3408340513706207\n",
      "Subject 6, Epoch 255, Loss: 1.2407974004745483, Final Batch Loss: 0.40880995988845825\n",
      "Subject 6, Epoch 256, Loss: 1.2041151523590088, Final Batch Loss: 0.4411279261112213\n",
      "Subject 6, Epoch 257, Loss: 1.1175535023212433, Final Batch Loss: 0.2600306272506714\n",
      "Subject 6, Epoch 258, Loss: 1.1691771149635315, Final Batch Loss: 0.4555230736732483\n",
      "Subject 6, Epoch 259, Loss: 1.2151073515415192, Final Batch Loss: 0.39606115221977234\n",
      "Subject 6, Epoch 260, Loss: 1.0769475996494293, Final Batch Loss: 0.3124102056026459\n",
      "Subject 6, Epoch 261, Loss: 1.285936862230301, Final Batch Loss: 0.5119495391845703\n",
      "Subject 6, Epoch 262, Loss: 1.1438210308551788, Final Batch Loss: 0.43317198753356934\n",
      "Subject 6, Epoch 263, Loss: 1.1397313177585602, Final Batch Loss: 0.4527626931667328\n",
      "Subject 6, Epoch 264, Loss: 1.1636711359024048, Final Batch Loss: 0.4225292503833771\n",
      "Subject 6, Epoch 265, Loss: 1.1971750259399414, Final Batch Loss: 0.43689608573913574\n",
      "Subject 6, Epoch 266, Loss: 1.143435150384903, Final Batch Loss: 0.44683837890625\n",
      "Subject 6, Epoch 267, Loss: 1.0649625062942505, Final Batch Loss: 0.3209776282310486\n",
      "Subject 6, Epoch 268, Loss: 1.131153643131256, Final Batch Loss: 0.35546526312828064\n",
      "Subject 6, Epoch 269, Loss: 1.1714267432689667, Final Batch Loss: 0.4206259250640869\n",
      "Subject 6, Epoch 270, Loss: 1.1044979393482208, Final Batch Loss: 0.36250534653663635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 271, Loss: 1.1094661056995392, Final Batch Loss: 0.4491560459136963\n",
      "Subject 6, Epoch 272, Loss: 1.1444013118743896, Final Batch Loss: 0.4226212799549103\n",
      "Subject 6, Epoch 273, Loss: 1.2433643639087677, Final Batch Loss: 0.45876383781433105\n",
      "Subject 6, Epoch 274, Loss: 1.1586226224899292, Final Batch Loss: 0.45880773663520813\n",
      "Subject 6, Epoch 275, Loss: 1.1006475687026978, Final Batch Loss: 0.34961962699890137\n",
      "Subject 6, Epoch 276, Loss: 1.0282852351665497, Final Batch Loss: 0.3779546916484833\n",
      "Subject 6, Epoch 277, Loss: 1.0794108211994171, Final Batch Loss: 0.2940710484981537\n",
      "Subject 6, Epoch 278, Loss: 1.0959783792495728, Final Batch Loss: 0.36103177070617676\n",
      "Subject 6, Epoch 279, Loss: 1.1137145459651947, Final Batch Loss: 0.3309057056903839\n",
      "Subject 6, Epoch 280, Loss: 1.1492324471473694, Final Batch Loss: 0.37584248185157776\n",
      "Subject 6, Epoch 281, Loss: 1.1050097942352295, Final Batch Loss: 0.3477989733219147\n",
      "Subject 6, Epoch 282, Loss: 1.1294349431991577, Final Batch Loss: 0.3744799494743347\n",
      "Subject 6, Epoch 283, Loss: 1.1722972095012665, Final Batch Loss: 0.4360837936401367\n",
      "Subject 6, Epoch 284, Loss: 1.1269524991512299, Final Batch Loss: 0.4107076823711395\n",
      "Subject 6, Epoch 285, Loss: 1.126518964767456, Final Batch Loss: 0.4028848707675934\n",
      "Subject 6, Epoch 286, Loss: 1.0959277153015137, Final Batch Loss: 0.3188307285308838\n",
      "Subject 6, Epoch 287, Loss: 1.041916161775589, Final Batch Loss: 0.28084322810173035\n",
      "Subject 6, Epoch 288, Loss: 1.1193516254425049, Final Batch Loss: 0.34379124641418457\n",
      "Subject 6, Epoch 289, Loss: 1.0331303179264069, Final Batch Loss: 0.3251625895500183\n",
      "Subject 6, Epoch 290, Loss: 1.0672408640384674, Final Batch Loss: 0.39681974053382874\n",
      "Subject 6, Epoch 291, Loss: 1.1607500314712524, Final Batch Loss: 0.3952423334121704\n",
      "Subject 6, Epoch 292, Loss: 1.118912547826767, Final Batch Loss: 0.3844607174396515\n",
      "Subject 6, Epoch 293, Loss: 1.109280377626419, Final Batch Loss: 0.42689836025238037\n",
      "Subject 6, Epoch 294, Loss: 0.9776573777198792, Final Batch Loss: 0.32741010189056396\n",
      "Subject 6, Epoch 295, Loss: 1.1288274824619293, Final Batch Loss: 0.3552323579788208\n",
      "Subject 6, Epoch 296, Loss: 1.2135092914104462, Final Batch Loss: 0.44493669271469116\n",
      "Subject 6, Epoch 297, Loss: 1.1046527028083801, Final Batch Loss: 0.41121575236320496\n",
      "Subject 6, Epoch 298, Loss: 1.035863995552063, Final Batch Loss: 0.3776155412197113\n",
      "Subject 6, Epoch 299, Loss: 1.0155592560768127, Final Batch Loss: 0.30239078402519226\n",
      "Subject 6, Epoch 300, Loss: 1.182620644569397, Final Batch Loss: 0.4083034098148346\n",
      "Subject 6, Epoch 301, Loss: 1.14715576171875, Final Batch Loss: 0.446408748626709\n",
      "Subject 6, Epoch 302, Loss: 1.0895011723041534, Final Batch Loss: 0.4059740900993347\n",
      "Subject 6, Epoch 303, Loss: 1.0700306594371796, Final Batch Loss: 0.3666756749153137\n",
      "Subject 6, Epoch 304, Loss: 1.0505395829677582, Final Batch Loss: 0.388824462890625\n",
      "Subject 6, Epoch 305, Loss: 0.9717025458812714, Final Batch Loss: 0.2928227186203003\n",
      "Subject 6, Epoch 306, Loss: 1.094261258840561, Final Batch Loss: 0.40161633491516113\n",
      "Subject 6, Epoch 307, Loss: 1.037826806306839, Final Batch Loss: 0.33873265981674194\n",
      "Subject 6, Epoch 308, Loss: 1.1004984378814697, Final Batch Loss: 0.4245944321155548\n",
      "Subject 6, Epoch 309, Loss: 0.9983159601688385, Final Batch Loss: 0.33936944603919983\n",
      "Subject 6, Epoch 310, Loss: 1.0391378998756409, Final Batch Loss: 0.2552112340927124\n",
      "Subject 6, Epoch 311, Loss: 1.0225764513015747, Final Batch Loss: 0.34669995307922363\n",
      "Subject 6, Epoch 312, Loss: 1.1174989342689514, Final Batch Loss: 0.3611741065979004\n",
      "Subject 6, Epoch 313, Loss: 1.0578125417232513, Final Batch Loss: 0.31854408979415894\n",
      "Subject 6, Epoch 314, Loss: 1.0591101497411728, Final Batch Loss: 0.24466349184513092\n",
      "Subject 6, Epoch 315, Loss: 1.1245747804641724, Final Batch Loss: 0.43937310576438904\n",
      "Subject 6, Epoch 316, Loss: 1.1478154063224792, Final Batch Loss: 0.4784976840019226\n",
      "Subject 6, Epoch 317, Loss: 1.005442500114441, Final Batch Loss: 0.2697460651397705\n",
      "Subject 6, Epoch 318, Loss: 1.03428116440773, Final Batch Loss: 0.3450717031955719\n",
      "Subject 6, Epoch 319, Loss: 1.0338586568832397, Final Batch Loss: 0.32795920968055725\n",
      "Subject 6, Epoch 320, Loss: 1.0841001868247986, Final Batch Loss: 0.46924734115600586\n",
      "Subject 6, Epoch 321, Loss: 0.9875961542129517, Final Batch Loss: 0.30009695887565613\n",
      "Subject 6, Epoch 322, Loss: 1.0063849985599518, Final Batch Loss: 0.2774788737297058\n",
      "Subject 6, Epoch 323, Loss: 1.063520908355713, Final Batch Loss: 0.25800299644470215\n",
      "Subject 6, Epoch 324, Loss: 0.9850042015314102, Final Batch Loss: 0.23944257199764252\n",
      "Subject 6, Epoch 325, Loss: 1.0850400030612946, Final Batch Loss: 0.3360365331172943\n",
      "Subject 6, Epoch 326, Loss: 1.1264550685882568, Final Batch Loss: 0.42280352115631104\n",
      "Subject 6, Epoch 327, Loss: 0.9749061167240143, Final Batch Loss: 0.3126277029514313\n",
      "Subject 6, Epoch 328, Loss: 1.0743152499198914, Final Batch Loss: 0.32598575949668884\n",
      "Subject 6, Epoch 329, Loss: 0.923933744430542, Final Batch Loss: 0.27844497561454773\n",
      "Subject 6, Epoch 330, Loss: 1.0181233882904053, Final Batch Loss: 0.2942734956741333\n",
      "Subject 6, Epoch 331, Loss: 1.1611380577087402, Final Batch Loss: 0.4322410821914673\n",
      "Subject 6, Epoch 332, Loss: 1.1068381369113922, Final Batch Loss: 0.42439016699790955\n",
      "Subject 6, Epoch 333, Loss: 1.0529950261116028, Final Batch Loss: 0.3452742099761963\n",
      "Subject 6, Epoch 334, Loss: 1.0077378153800964, Final Batch Loss: 0.39537379145622253\n",
      "Subject 6, Epoch 335, Loss: 1.0486580431461334, Final Batch Loss: 0.3404190242290497\n",
      "Subject 6, Epoch 336, Loss: 1.1185006499290466, Final Batch Loss: 0.41450396180152893\n",
      "Subject 6, Epoch 337, Loss: 1.0855381488800049, Final Batch Loss: 0.3030812442302704\n",
      "Subject 6, Epoch 338, Loss: 1.0433740019798279, Final Batch Loss: 0.3610314428806305\n",
      "Subject 6, Epoch 339, Loss: 1.0550172328948975, Final Batch Loss: 0.3303525149822235\n",
      "Subject 6, Epoch 340, Loss: 1.1015247106552124, Final Batch Loss: 0.3153059184551239\n",
      "Subject 6, Epoch 341, Loss: 1.075425699353218, Final Batch Loss: 0.4064739942550659\n",
      "Subject 6, Epoch 342, Loss: 0.9669974148273468, Final Batch Loss: 0.3255837857723236\n",
      "Subject 6, Epoch 343, Loss: 1.0635009407997131, Final Batch Loss: 0.3168540596961975\n",
      "Subject 6, Epoch 344, Loss: 0.9969851076602936, Final Batch Loss: 0.2791515588760376\n",
      "Subject 6, Epoch 345, Loss: 1.0902506411075592, Final Batch Loss: 0.3558693528175354\n",
      "Subject 6, Epoch 346, Loss: 1.0847239792346954, Final Batch Loss: 0.4235338568687439\n",
      "Subject 6, Epoch 347, Loss: 1.0388185381889343, Final Batch Loss: 0.33308854699134827\n",
      "Subject 6, Epoch 348, Loss: 0.9677795469760895, Final Batch Loss: 0.27633732557296753\n",
      "Subject 6, Epoch 349, Loss: 0.9960203468799591, Final Batch Loss: 0.34429049491882324\n",
      "Subject 6, Epoch 350, Loss: 1.1445008218288422, Final Batch Loss: 0.3829765021800995\n",
      "Subject 6, Epoch 351, Loss: 1.0645083785057068, Final Batch Loss: 0.3790099620819092\n",
      "Subject 6, Epoch 352, Loss: 1.0042960047721863, Final Batch Loss: 0.4228188097476959\n",
      "Subject 6, Epoch 353, Loss: 0.9815552234649658, Final Batch Loss: 0.286954790353775\n",
      "Subject 6, Epoch 354, Loss: 0.9678498804569244, Final Batch Loss: 0.2985672354698181\n",
      "Subject 6, Epoch 355, Loss: 1.065719485282898, Final Batch Loss: 0.36822444200515747\n",
      "Subject 6, Epoch 356, Loss: 1.0017908215522766, Final Batch Loss: 0.29921120405197144\n",
      "Subject 6, Epoch 357, Loss: 1.0583317279815674, Final Batch Loss: 0.35240811109542847\n",
      "Subject 6, Epoch 358, Loss: 0.9404905438423157, Final Batch Loss: 0.2808813154697418\n",
      "Subject 6, Epoch 359, Loss: 1.0018582046031952, Final Batch Loss: 0.37684622406959534\n",
      "Subject 6, Epoch 360, Loss: 0.9721449613571167, Final Batch Loss: 0.30423134565353394\n",
      "Subject 6, Epoch 361, Loss: 0.9896619617938995, Final Batch Loss: 0.34379225969314575\n",
      "Subject 6, Epoch 362, Loss: 0.9551004767417908, Final Batch Loss: 0.2901264727115631\n",
      "Subject 6, Epoch 363, Loss: 1.0259219408035278, Final Batch Loss: 0.4162341356277466\n",
      "Subject 6, Epoch 364, Loss: 1.0359072089195251, Final Batch Loss: 0.24779975414276123\n",
      "Subject 6, Epoch 365, Loss: 1.0140984058380127, Final Batch Loss: 0.3330945074558258\n",
      "Subject 6, Epoch 366, Loss: 0.92482790350914, Final Batch Loss: 0.2923988997936249\n",
      "Subject 6, Epoch 367, Loss: 1.0029760152101517, Final Batch Loss: 0.3969435393810272\n",
      "Subject 6, Epoch 368, Loss: 0.9973675012588501, Final Batch Loss: 0.31601518392562866\n",
      "Subject 6, Epoch 369, Loss: 0.9107421338558197, Final Batch Loss: 0.2680394649505615\n",
      "Subject 6, Epoch 370, Loss: 0.9558745622634888, Final Batch Loss: 0.3181617856025696\n",
      "Subject 6, Epoch 371, Loss: 0.8669895231723785, Final Batch Loss: 0.261730819940567\n",
      "Subject 6, Epoch 372, Loss: 0.8880666196346283, Final Batch Loss: 0.32767584919929504\n",
      "Subject 6, Epoch 373, Loss: 0.9372584521770477, Final Batch Loss: 0.3048461973667145\n",
      "Subject 6, Epoch 374, Loss: 1.0522427260875702, Final Batch Loss: 0.26822882890701294\n",
      "Subject 6, Epoch 375, Loss: 1.0079534351825714, Final Batch Loss: 0.32696348428726196\n",
      "Subject 6, Epoch 376, Loss: 1.0077425390481949, Final Batch Loss: 0.24642394483089447\n",
      "Subject 6, Epoch 377, Loss: 0.9833015203475952, Final Batch Loss: 0.34742268919944763\n",
      "Subject 6, Epoch 378, Loss: 0.9511055648326874, Final Batch Loss: 0.2738383114337921\n",
      "Subject 6, Epoch 379, Loss: 0.9150742739439011, Final Batch Loss: 0.23408325016498566\n",
      "Subject 6, Epoch 380, Loss: 0.9699338525533676, Final Batch Loss: 0.36054176092147827\n",
      "Subject 6, Epoch 381, Loss: 1.0215904414653778, Final Batch Loss: 0.3995974063873291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 382, Loss: 0.976500540971756, Final Batch Loss: 0.43206024169921875\n",
      "Subject 6, Epoch 383, Loss: 0.9213634431362152, Final Batch Loss: 0.32301533222198486\n",
      "Subject 6, Epoch 384, Loss: 0.8982704877853394, Final Batch Loss: 0.22992193698883057\n",
      "Subject 6, Epoch 385, Loss: 0.9647098481655121, Final Batch Loss: 0.33410173654556274\n",
      "Subject 6, Epoch 386, Loss: 0.9391032159328461, Final Batch Loss: 0.34561997652053833\n",
      "Subject 6, Epoch 387, Loss: 0.9261820316314697, Final Batch Loss: 0.3256324529647827\n",
      "Subject 6, Epoch 388, Loss: 0.9878789782524109, Final Batch Loss: 0.3129551410675049\n",
      "Subject 6, Epoch 389, Loss: 0.9886497557163239, Final Batch Loss: 0.3117552697658539\n",
      "Subject 6, Epoch 390, Loss: 0.9880161881446838, Final Batch Loss: 0.26139649748802185\n",
      "Subject 6, Epoch 391, Loss: 0.8317941725254059, Final Batch Loss: 0.13940081000328064\n",
      "Subject 6, Epoch 392, Loss: 0.8723516911268234, Final Batch Loss: 0.2746438980102539\n",
      "Subject 6, Epoch 393, Loss: 0.9418266713619232, Final Batch Loss: 0.3609527051448822\n",
      "Subject 6, Epoch 394, Loss: 0.9191652238368988, Final Batch Loss: 0.27615422010421753\n",
      "Subject 6, Epoch 395, Loss: 0.8806598037481308, Final Batch Loss: 0.24844186007976532\n",
      "Subject 6, Epoch 396, Loss: 0.957953691482544, Final Batch Loss: 0.35157570242881775\n",
      "Subject 6, Epoch 397, Loss: 1.0223488360643387, Final Batch Loss: 0.4546137750148773\n",
      "Subject 6, Epoch 398, Loss: 0.9597697854042053, Final Batch Loss: 0.3623203635215759\n",
      "Subject 6, Epoch 399, Loss: 0.9980662763118744, Final Batch Loss: 0.4051699936389923\n",
      "Subject 6, Epoch 400, Loss: 1.0181739032268524, Final Batch Loss: 0.3789788782596588\n",
      "Subject 6, Epoch 401, Loss: 0.9378160238265991, Final Batch Loss: 0.25692445039749146\n",
      "Subject 6, Epoch 402, Loss: 0.920328289270401, Final Batch Loss: 0.34357428550720215\n",
      "Subject 6, Epoch 403, Loss: 0.9045751988887787, Final Batch Loss: 0.33072638511657715\n",
      "Subject 6, Epoch 404, Loss: 0.9296150505542755, Final Batch Loss: 0.3365747928619385\n",
      "Subject 6, Epoch 405, Loss: 0.9460283517837524, Final Batch Loss: 0.33424341678619385\n",
      "Subject 6, Epoch 406, Loss: 0.8463034927845001, Final Batch Loss: 0.24064916372299194\n",
      "Subject 6, Epoch 407, Loss: 0.918799489736557, Final Batch Loss: 0.31234508752822876\n",
      "Subject 6, Epoch 408, Loss: 0.8722428530454636, Final Batch Loss: 0.17897586524486542\n",
      "Subject 6, Epoch 409, Loss: 0.992960587143898, Final Batch Loss: 0.24230702221393585\n",
      "Subject 6, Epoch 410, Loss: 0.9299020171165466, Final Batch Loss: 0.3727751672267914\n",
      "Subject 6, Epoch 411, Loss: 0.9536638259887695, Final Batch Loss: 0.3378909230232239\n",
      "Subject 6, Epoch 412, Loss: 0.8653350472450256, Final Batch Loss: 0.21772614121437073\n",
      "Subject 6, Epoch 413, Loss: 0.8458241522312164, Final Batch Loss: 0.2707570493221283\n",
      "Subject 6, Epoch 414, Loss: 0.86163629591465, Final Batch Loss: 0.2627348005771637\n",
      "Subject 6, Epoch 415, Loss: 1.0239365100860596, Final Batch Loss: 0.4171125888824463\n",
      "Subject 6, Epoch 416, Loss: 0.8895102441310883, Final Batch Loss: 0.2455788254737854\n",
      "Subject 6, Epoch 417, Loss: 1.0171949863433838, Final Batch Loss: 0.415520042181015\n",
      "Subject 6, Epoch 418, Loss: 0.8747563660144806, Final Batch Loss: 0.28153112530708313\n",
      "Subject 6, Epoch 419, Loss: 0.9504469037055969, Final Batch Loss: 0.26209330558776855\n",
      "Subject 6, Epoch 420, Loss: 1.0154621005058289, Final Batch Loss: 0.3858950138092041\n",
      "Subject 6, Epoch 421, Loss: 1.0045808553695679, Final Batch Loss: 0.3535287380218506\n",
      "Subject 6, Epoch 422, Loss: 0.9562021195888519, Final Batch Loss: 0.3295655846595764\n",
      "Subject 6, Epoch 423, Loss: 0.9660204648971558, Final Batch Loss: 0.3229506313800812\n",
      "Subject 6, Epoch 424, Loss: 0.9077820181846619, Final Batch Loss: 0.36213061213493347\n",
      "Subject 6, Epoch 425, Loss: 0.9231716096401215, Final Batch Loss: 0.2098095715045929\n",
      "Subject 6, Epoch 426, Loss: 0.8743387460708618, Final Batch Loss: 0.3026468753814697\n",
      "Subject 6, Epoch 427, Loss: 0.8686180114746094, Final Batch Loss: 0.2525470554828644\n",
      "Subject 6, Epoch 428, Loss: 1.0140132308006287, Final Batch Loss: 0.32096585631370544\n",
      "Subject 6, Epoch 429, Loss: 0.8877114653587341, Final Batch Loss: 0.2954843044281006\n",
      "Subject 6, Epoch 430, Loss: 0.9366447627544403, Final Batch Loss: 0.33068424463272095\n",
      "Subject 6, Epoch 431, Loss: 0.8670803904533386, Final Batch Loss: 0.3331846296787262\n",
      "Subject 6, Epoch 432, Loss: 0.9567948877811432, Final Batch Loss: 0.22878944873809814\n",
      "Subject 6, Epoch 433, Loss: 0.8383338898420334, Final Batch Loss: 0.2478259652853012\n",
      "Subject 6, Epoch 434, Loss: 0.8490049988031387, Final Batch Loss: 0.21151916682720184\n",
      "Subject 6, Epoch 435, Loss: 1.0238369703292847, Final Batch Loss: 0.3569558262825012\n",
      "Subject 6, Epoch 436, Loss: 0.8087204694747925, Final Batch Loss: 0.21476957201957703\n",
      "Subject 6, Epoch 437, Loss: 0.8983699977397919, Final Batch Loss: 0.2966800332069397\n",
      "Subject 6, Epoch 438, Loss: 0.8828801661729813, Final Batch Loss: 0.2114229053258896\n",
      "Subject 6, Epoch 439, Loss: 0.8001660853624344, Final Batch Loss: 0.2832716405391693\n",
      "Subject 6, Epoch 440, Loss: 0.9307873100042343, Final Batch Loss: 0.3451390862464905\n",
      "Subject 6, Epoch 441, Loss: 0.9550968110561371, Final Batch Loss: 0.31776687502861023\n",
      "Subject 6, Epoch 442, Loss: 0.8599178493022919, Final Batch Loss: 0.31166842579841614\n",
      "Subject 6, Epoch 443, Loss: 0.9444269835948944, Final Batch Loss: 0.358514666557312\n",
      "Subject 6, Epoch 444, Loss: 0.8074455708265305, Final Batch Loss: 0.1883770376443863\n",
      "Subject 6, Epoch 445, Loss: 0.901326984167099, Final Batch Loss: 0.23192048072814941\n",
      "Subject 6, Epoch 446, Loss: 0.9848803281784058, Final Batch Loss: 0.32360148429870605\n",
      "Subject 6, Epoch 447, Loss: 0.9411633610725403, Final Batch Loss: 0.28418344259262085\n",
      "Subject 6, Epoch 448, Loss: 0.7623545378446579, Final Batch Loss: 0.27644848823547363\n",
      "Subject 6, Epoch 449, Loss: 1.0293385088443756, Final Batch Loss: 0.4989957809448242\n",
      "Subject 6, Epoch 450, Loss: 0.9550146758556366, Final Batch Loss: 0.4117676317691803\n",
      "Subject 6, Epoch 451, Loss: 0.8890827894210815, Final Batch Loss: 0.34726446866989136\n",
      "Subject 6, Epoch 452, Loss: 0.8583004474639893, Final Batch Loss: 0.3460234999656677\n",
      "Subject 6, Epoch 453, Loss: 0.9026222825050354, Final Batch Loss: 0.2960609197616577\n",
      "Subject 6, Epoch 454, Loss: 0.986673042178154, Final Batch Loss: 0.4810977876186371\n",
      "Subject 6, Epoch 455, Loss: 0.8263728469610214, Final Batch Loss: 0.22778825461864471\n",
      "Subject 6, Epoch 456, Loss: 0.8568064570426941, Final Batch Loss: 0.1805592179298401\n",
      "Subject 6, Epoch 457, Loss: 0.8652697056531906, Final Batch Loss: 0.3558104634284973\n",
      "Subject 6, Epoch 458, Loss: 0.9179987907409668, Final Batch Loss: 0.2758176326751709\n",
      "Subject 6, Epoch 459, Loss: 0.7840200960636139, Final Batch Loss: 0.23048387467861176\n",
      "Subject 6, Epoch 460, Loss: 0.9501726627349854, Final Batch Loss: 0.35471031069755554\n",
      "Subject 6, Epoch 461, Loss: 0.8511138707399368, Final Batch Loss: 0.21429233253002167\n",
      "Subject 6, Epoch 462, Loss: 0.9053023457527161, Final Batch Loss: 0.2931880056858063\n",
      "Subject 6, Epoch 463, Loss: 0.8497282415628433, Final Batch Loss: 0.2968345880508423\n",
      "Subject 6, Epoch 464, Loss: 0.6887896656990051, Final Batch Loss: 0.18420210480690002\n",
      "Subject 6, Epoch 465, Loss: 0.8738705068826675, Final Batch Loss: 0.3549458682537079\n",
      "Subject 6, Epoch 466, Loss: 0.8092570602893829, Final Batch Loss: 0.23023836314678192\n",
      "Subject 6, Epoch 467, Loss: 1.0092289447784424, Final Batch Loss: 0.3723662495613098\n",
      "Subject 6, Epoch 468, Loss: 0.8661340475082397, Final Batch Loss: 0.262841135263443\n",
      "Subject 6, Epoch 469, Loss: 0.900011420249939, Final Batch Loss: 0.3161875307559967\n",
      "Subject 6, Epoch 470, Loss: 0.981447845697403, Final Batch Loss: 0.3960566520690918\n",
      "Subject 6, Epoch 471, Loss: 0.8887106776237488, Final Batch Loss: 0.26678475737571716\n",
      "Subject 6, Epoch 472, Loss: 0.9166235625743866, Final Batch Loss: 0.20013275742530823\n",
      "Subject 6, Epoch 473, Loss: 1.0164723694324493, Final Batch Loss: 0.3756248950958252\n",
      "Subject 6, Epoch 474, Loss: 0.8115307539701462, Final Batch Loss: 0.21909916400909424\n",
      "Subject 6, Epoch 475, Loss: 0.9186531901359558, Final Batch Loss: 0.35097065567970276\n",
      "Subject 6, Epoch 476, Loss: 0.9025668799877167, Final Batch Loss: 0.43550875782966614\n",
      "Subject 6, Epoch 477, Loss: 0.8552103340625763, Final Batch Loss: 0.29957130551338196\n",
      "Subject 6, Epoch 478, Loss: 0.9180791676044464, Final Batch Loss: 0.29417872428894043\n",
      "Subject 6, Epoch 479, Loss: 0.9477911591529846, Final Batch Loss: 0.2584594488143921\n",
      "Subject 6, Epoch 480, Loss: 0.8007526695728302, Final Batch Loss: 0.15525871515274048\n",
      "Subject 6, Epoch 481, Loss: 0.7964889407157898, Final Batch Loss: 0.3320520222187042\n",
      "Subject 6, Epoch 482, Loss: 0.8537996411323547, Final Batch Loss: 0.29348254203796387\n",
      "Subject 6, Epoch 483, Loss: 0.8430787920951843, Final Batch Loss: 0.26039794087409973\n",
      "Subject 6, Epoch 484, Loss: 0.7903973460197449, Final Batch Loss: 0.33350223302841187\n",
      "Subject 6, Epoch 485, Loss: 0.9660572409629822, Final Batch Loss: 0.4082931876182556\n",
      "Subject 6, Epoch 486, Loss: 0.9371272027492523, Final Batch Loss: 0.29524916410446167\n",
      "Subject 6, Epoch 487, Loss: 0.8026610463857651, Final Batch Loss: 0.20579542219638824\n",
      "Subject 6, Epoch 488, Loss: 0.8661722838878632, Final Batch Loss: 0.22647732496261597\n",
      "Subject 6, Epoch 489, Loss: 0.8379674553871155, Final Batch Loss: 0.27432939410209656\n",
      "Subject 6, Epoch 490, Loss: 0.8074028193950653, Final Batch Loss: 0.2148953080177307\n",
      "Subject 6, Epoch 491, Loss: 0.8036493808031082, Final Batch Loss: 0.28332701325416565\n",
      "Subject 6, Epoch 492, Loss: 0.8636350035667419, Final Batch Loss: 0.27360352873802185\n",
      "Subject 6, Epoch 493, Loss: 0.8500424325466156, Final Batch Loss: 0.266781210899353\n",
      "Subject 6, Epoch 494, Loss: 0.8164469748735428, Final Batch Loss: 0.30437472462654114\n",
      "Subject 6, Epoch 495, Loss: 0.9274950921535492, Final Batch Loss: 0.3885061740875244\n",
      "Subject 6, Epoch 496, Loss: 0.8229111880064011, Final Batch Loss: 0.3526456356048584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 497, Loss: 0.8140561282634735, Final Batch Loss: 0.28808239102363586\n",
      "Subject 6, Epoch 498, Loss: 0.8442840278148651, Final Batch Loss: 0.22161027789115906\n",
      "Subject 6, Epoch 499, Loss: 0.9166843295097351, Final Batch Loss: 0.2931321859359741\n",
      "Subject 6, Epoch 500, Loss: 0.795007050037384, Final Batch Loss: 0.2563301622867584\n",
      "Subject 6, Epoch 501, Loss: 0.9065984785556793, Final Batch Loss: 0.2746303081512451\n",
      "Subject 6, Epoch 502, Loss: 0.8623607754707336, Final Batch Loss: 0.33184242248535156\n",
      "Subject 6, Epoch 503, Loss: 0.8592314273118973, Final Batch Loss: 0.38321080803871155\n",
      "Subject 6, Epoch 504, Loss: 0.871530681848526, Final Batch Loss: 0.30996599793434143\n",
      "Subject 6, Epoch 505, Loss: 0.8751171827316284, Final Batch Loss: 0.25499558448791504\n",
      "Subject 6, Epoch 506, Loss: 0.8942362815141678, Final Batch Loss: 0.21158213913440704\n",
      "Subject 6, Epoch 507, Loss: 0.8074427396059036, Final Batch Loss: 0.27148139476776123\n",
      "Subject 6, Epoch 508, Loss: 0.7619156241416931, Final Batch Loss: 0.28915300965309143\n",
      "Subject 6, Epoch 509, Loss: 0.8560860455036163, Final Batch Loss: 0.2793284058570862\n",
      "Subject 6, Epoch 510, Loss: 0.8191139400005341, Final Batch Loss: 0.22597923874855042\n",
      "Subject 6, Epoch 511, Loss: 0.8109287321567535, Final Batch Loss: 0.2613385021686554\n",
      "Subject 6, Epoch 512, Loss: 0.7824467569589615, Final Batch Loss: 0.2934134006500244\n",
      "Subject 6, Epoch 513, Loss: 0.8628328740596771, Final Batch Loss: 0.19758844375610352\n",
      "Subject 6, Epoch 514, Loss: 0.7681392580270767, Final Batch Loss: 0.21794189512729645\n",
      "Subject 6, Epoch 515, Loss: 0.8420928567647934, Final Batch Loss: 0.2468995749950409\n",
      "Subject 6, Epoch 516, Loss: 0.9377765953540802, Final Batch Loss: 0.3374019265174866\n",
      "Subject 6, Epoch 517, Loss: 0.8604480922222137, Final Batch Loss: 0.3654051721096039\n",
      "Subject 6, Epoch 518, Loss: 0.796403169631958, Final Batch Loss: 0.28709420561790466\n",
      "Subject 6, Epoch 519, Loss: 0.9467378556728363, Final Batch Loss: 0.33398908376693726\n",
      "Subject 6, Epoch 520, Loss: 0.8872308135032654, Final Batch Loss: 0.2994353771209717\n",
      "Subject 6, Epoch 521, Loss: 0.8212872743606567, Final Batch Loss: 0.28589972853660583\n",
      "Subject 6, Epoch 522, Loss: 0.8823533356189728, Final Batch Loss: 0.19684937596321106\n",
      "Subject 6, Epoch 523, Loss: 0.8575194627046585, Final Batch Loss: 0.28572311997413635\n",
      "Subject 6, Epoch 524, Loss: 0.8076709508895874, Final Batch Loss: 0.2370351403951645\n",
      "Subject 6, Epoch 525, Loss: 0.9243194460868835, Final Batch Loss: 0.2908756732940674\n",
      "Subject 6, Epoch 526, Loss: 0.7980632781982422, Final Batch Loss: 0.22478830814361572\n",
      "Subject 6, Epoch 527, Loss: 0.8981269299983978, Final Batch Loss: 0.25612521171569824\n",
      "Subject 6, Epoch 528, Loss: 0.7740109115839005, Final Batch Loss: 0.23004527390003204\n",
      "Subject 6, Epoch 529, Loss: 0.7496908158063889, Final Batch Loss: 0.2842940092086792\n",
      "Subject 6, Epoch 530, Loss: 0.7425855398178101, Final Batch Loss: 0.13571205735206604\n",
      "Subject 6, Epoch 531, Loss: 0.8204582333564758, Final Batch Loss: 0.23038652539253235\n",
      "Subject 6, Epoch 532, Loss: 0.8043771982192993, Final Batch Loss: 0.2861691117286682\n",
      "Subject 6, Epoch 533, Loss: 0.9099929630756378, Final Batch Loss: 0.35151228308677673\n",
      "Subject 6, Epoch 534, Loss: 0.8253792226314545, Final Batch Loss: 0.2741266191005707\n",
      "Subject 6, Epoch 535, Loss: 0.85323666036129, Final Batch Loss: 0.21516211330890656\n",
      "Subject 6, Epoch 536, Loss: 0.7976377606391907, Final Batch Loss: 0.25091803073883057\n",
      "Subject 6, Epoch 537, Loss: 0.8294006884098053, Final Batch Loss: 0.36225616931915283\n",
      "Subject 6, Epoch 538, Loss: 0.8081470131874084, Final Batch Loss: 0.21637359261512756\n",
      "Subject 6, Epoch 539, Loss: 0.7604974955320358, Final Batch Loss: 0.2114478498697281\n",
      "Subject 6, Epoch 540, Loss: 0.8913435488939285, Final Batch Loss: 0.3824215531349182\n",
      "Subject 6, Epoch 541, Loss: 0.7357329875230789, Final Batch Loss: 0.1696753352880478\n",
      "Subject 6, Epoch 542, Loss: 0.7993802428245544, Final Batch Loss: 0.260476291179657\n",
      "Subject 6, Epoch 543, Loss: 0.8886105418205261, Final Batch Loss: 0.23762714862823486\n",
      "Subject 6, Epoch 544, Loss: 0.8300676643848419, Final Batch Loss: 0.2846483886241913\n",
      "Subject 6, Epoch 545, Loss: 0.9416385591030121, Final Batch Loss: 0.33865660429000854\n",
      "Subject 6, Epoch 546, Loss: 0.728851705789566, Final Batch Loss: 0.24920915067195892\n",
      "Subject 6, Epoch 547, Loss: 0.8662027418613434, Final Batch Loss: 0.29723313450813293\n",
      "Subject 6, Epoch 548, Loss: 0.8646465539932251, Final Batch Loss: 0.28491100668907166\n",
      "Subject 6, Epoch 549, Loss: 0.7990387827157974, Final Batch Loss: 0.15883146226406097\n",
      "Subject 6, Epoch 550, Loss: 0.7895298302173615, Final Batch Loss: 0.25788384675979614\n",
      "Subject 6, Epoch 551, Loss: 0.8119344264268875, Final Batch Loss: 0.31160587072372437\n",
      "Subject 6, Epoch 552, Loss: 0.7497797608375549, Final Batch Loss: 0.22640976309776306\n",
      "Subject 6, Epoch 553, Loss: 0.7321148812770844, Final Batch Loss: 0.2579624652862549\n",
      "Subject 6, Epoch 554, Loss: 0.8048107028007507, Final Batch Loss: 0.22217535972595215\n",
      "Subject 6, Epoch 555, Loss: 0.8000786900520325, Final Batch Loss: 0.277561753988266\n",
      "Subject 6, Epoch 556, Loss: 0.7476180791854858, Final Batch Loss: 0.24998816847801208\n",
      "Subject 6, Epoch 557, Loss: 0.6887214481830597, Final Batch Loss: 0.2306709736585617\n",
      "Subject 6, Epoch 558, Loss: 0.8688936233520508, Final Batch Loss: 0.3255769908428192\n",
      "Subject 6, Epoch 559, Loss: 0.7402223348617554, Final Batch Loss: 0.2523365318775177\n",
      "Subject 6, Epoch 560, Loss: 0.6796921342611313, Final Batch Loss: 0.18657992780208588\n",
      "Subject 6, Epoch 561, Loss: 0.8508995175361633, Final Batch Loss: 0.2894400656223297\n",
      "Subject 6, Epoch 562, Loss: 0.7321268618106842, Final Batch Loss: 0.22072497010231018\n",
      "Subject 6, Epoch 563, Loss: 0.7707435786724091, Final Batch Loss: 0.2900678515434265\n",
      "Subject 6, Epoch 564, Loss: 0.7653710395097733, Final Batch Loss: 0.2633211314678192\n",
      "Subject 6, Epoch 565, Loss: 0.7390858680009842, Final Batch Loss: 0.245874285697937\n",
      "Subject 6, Epoch 566, Loss: 0.8274901211261749, Final Batch Loss: 0.3080344796180725\n",
      "Subject 6, Epoch 567, Loss: 0.776711642742157, Final Batch Loss: 0.22405777871608734\n",
      "Subject 6, Epoch 568, Loss: 0.7478115856647491, Final Batch Loss: 0.1886214017868042\n",
      "Subject 6, Epoch 569, Loss: 0.8585910201072693, Final Batch Loss: 0.2859051525592804\n",
      "Subject 6, Epoch 570, Loss: 0.8683046847581863, Final Batch Loss: 0.35422244668006897\n",
      "Subject 6, Epoch 571, Loss: 0.7357159852981567, Final Batch Loss: 0.24268299341201782\n",
      "Subject 6, Epoch 572, Loss: 0.8011925220489502, Final Batch Loss: 0.24920736253261566\n",
      "Subject 6, Epoch 573, Loss: 0.8844453394412994, Final Batch Loss: 0.27563220262527466\n",
      "Subject 6, Epoch 574, Loss: 0.7043985277414322, Final Batch Loss: 0.16377685964107513\n",
      "Subject 6, Epoch 575, Loss: 0.8001730889081955, Final Batch Loss: 0.27619606256484985\n",
      "Subject 6, Epoch 576, Loss: 0.7142003327608109, Final Batch Loss: 0.28365132212638855\n",
      "Subject 6, Epoch 577, Loss: 0.7366830259561539, Final Batch Loss: 0.3101496398448944\n",
      "Subject 6, Epoch 578, Loss: 0.7314707785844803, Final Batch Loss: 0.2262801080942154\n",
      "Subject 6, Epoch 579, Loss: 0.80189149081707, Final Batch Loss: 0.15756066143512726\n",
      "Subject 6, Epoch 580, Loss: 0.7591163665056229, Final Batch Loss: 0.2077869027853012\n",
      "Subject 6, Epoch 581, Loss: 0.8266702443361282, Final Batch Loss: 0.22514857351779938\n",
      "Subject 6, Epoch 582, Loss: 0.8390383720397949, Final Batch Loss: 0.2965182662010193\n",
      "Subject 6, Epoch 583, Loss: 0.8415798246860504, Final Batch Loss: 0.2719922661781311\n",
      "Subject 6, Epoch 584, Loss: 0.8260606229305267, Final Batch Loss: 0.2906503975391388\n",
      "Subject 6, Epoch 585, Loss: 0.8089938163757324, Final Batch Loss: 0.2662714123725891\n",
      "Subject 6, Epoch 586, Loss: 0.7273919582366943, Final Batch Loss: 0.18325591087341309\n",
      "Subject 6, Epoch 587, Loss: 0.7629062533378601, Final Batch Loss: 0.28831106424331665\n",
      "Subject 6, Epoch 588, Loss: 0.7258119881153107, Final Batch Loss: 0.15201866626739502\n",
      "Subject 6, Epoch 589, Loss: 0.8862878829240799, Final Batch Loss: 0.41089802980422974\n",
      "Subject 6, Epoch 590, Loss: 0.6514609307050705, Final Batch Loss: 0.22973866760730743\n",
      "Subject 6, Epoch 591, Loss: 0.6642294526100159, Final Batch Loss: 0.20972728729248047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 592, Loss: 0.7663074284791946, Final Batch Loss: 0.23099708557128906\n",
      "Subject 6, Epoch 593, Loss: 0.772870659828186, Final Batch Loss: 0.27035069465637207\n",
      "Subject 6, Epoch 594, Loss: 0.8395088315010071, Final Batch Loss: 0.3403327167034149\n",
      "Subject 6, Epoch 595, Loss: 0.8676473647356033, Final Batch Loss: 0.38405686616897583\n",
      "Subject 6, Epoch 596, Loss: 0.7521810978651047, Final Batch Loss: 0.23557738959789276\n",
      "Subject 6, Epoch 597, Loss: 0.7907065749168396, Final Batch Loss: 0.3229386508464813\n",
      "Subject 6, Epoch 598, Loss: 0.7673402279615402, Final Batch Loss: 0.26158496737480164\n",
      "Subject 6, Epoch 599, Loss: 0.7851274609565735, Final Batch Loss: 0.307142436504364\n",
      "Subject 6, Epoch 600, Loss: 0.845915213227272, Final Batch Loss: 0.3659910261631012\n",
      "Subject 6, Epoch 601, Loss: 0.7439595609903336, Final Batch Loss: 0.32248589396476746\n",
      "Subject 6, Epoch 602, Loss: 0.7133552730083466, Final Batch Loss: 0.1387677788734436\n",
      "Subject 6, Epoch 603, Loss: 0.6493450105190277, Final Batch Loss: 0.19900886714458466\n",
      "Subject 6, Epoch 604, Loss: 0.7241373807191849, Final Batch Loss: 0.1727355718612671\n",
      "Subject 6, Epoch 605, Loss: 0.6559763997793198, Final Batch Loss: 0.1645117998123169\n",
      "Subject 6, Epoch 606, Loss: 0.6784995794296265, Final Batch Loss: 0.2066686451435089\n",
      "Subject 6, Epoch 607, Loss: 0.8215614259243011, Final Batch Loss: 0.3095795214176178\n",
      "Subject 6, Epoch 608, Loss: 0.8552656024694443, Final Batch Loss: 0.2155957967042923\n",
      "Subject 6, Epoch 609, Loss: 0.6096459329128265, Final Batch Loss: 0.1440991461277008\n",
      "Subject 6, Epoch 610, Loss: 0.6956570670008659, Final Batch Loss: 0.12395232170820236\n",
      "Subject 6, Epoch 611, Loss: 0.8524062782526016, Final Batch Loss: 0.36984241008758545\n",
      "Subject 6, Epoch 612, Loss: 0.8400518149137497, Final Batch Loss: 0.3110608756542206\n",
      "Subject 6, Epoch 613, Loss: 0.7522255033254623, Final Batch Loss: 0.196615532040596\n",
      "Subject 6, Epoch 614, Loss: 0.7791695147752762, Final Batch Loss: 0.23868019878864288\n",
      "Subject 6, Epoch 615, Loss: 0.6822816282510757, Final Batch Loss: 0.2688978910446167\n",
      "Subject 6, Epoch 616, Loss: 0.7035292685031891, Final Batch Loss: 0.2001742124557495\n",
      "Subject 6, Epoch 617, Loss: 0.6965656876564026, Final Batch Loss: 0.19956377148628235\n",
      "Subject 6, Epoch 618, Loss: 0.7197567224502563, Final Batch Loss: 0.2993139922618866\n",
      "Subject 6, Epoch 619, Loss: 0.7854473292827606, Final Batch Loss: 0.19009485840797424\n",
      "Subject 6, Epoch 620, Loss: 0.7940922975540161, Final Batch Loss: 0.2886155843734741\n",
      "Subject 6, Epoch 621, Loss: 0.6352440267801285, Final Batch Loss: 0.12998653948307037\n",
      "Subject 6, Epoch 622, Loss: 0.8049129098653793, Final Batch Loss: 0.2786814272403717\n",
      "Subject 6, Epoch 623, Loss: 0.7768829315900803, Final Batch Loss: 0.23575249314308167\n",
      "Subject 6, Epoch 624, Loss: 0.6856469064950943, Final Batch Loss: 0.2455771416425705\n",
      "Subject 6, Epoch 625, Loss: 0.7519985139369965, Final Batch Loss: 0.23398339748382568\n",
      "Subject 6, Epoch 626, Loss: 0.7406854182481766, Final Batch Loss: 0.29937031865119934\n",
      "Subject 6, Epoch 627, Loss: 0.7755541503429413, Final Batch Loss: 0.2058868706226349\n",
      "Subject 6, Epoch 628, Loss: 0.6654946506023407, Final Batch Loss: 0.14939464628696442\n",
      "Subject 6, Epoch 629, Loss: 0.7012438774108887, Final Batch Loss: 0.2867608666419983\n",
      "Subject 6, Epoch 630, Loss: 0.6621450930833817, Final Batch Loss: 0.19470083713531494\n",
      "Subject 6, Epoch 631, Loss: 0.7103452682495117, Final Batch Loss: 0.15039610862731934\n",
      "Subject 6, Epoch 632, Loss: 0.703184574842453, Final Batch Loss: 0.14899961650371552\n",
      "Subject 6, Epoch 633, Loss: 0.7008171826601028, Final Batch Loss: 0.2434329390525818\n",
      "Subject 6, Epoch 634, Loss: 0.7172981351613998, Final Batch Loss: 0.2765849828720093\n",
      "Subject 6, Epoch 635, Loss: 0.6397919207811356, Final Batch Loss: 0.1474970579147339\n",
      "Subject 6, Epoch 636, Loss: 0.7465738952159882, Final Batch Loss: 0.2907947897911072\n",
      "Subject 6, Epoch 637, Loss: 0.6117767244577408, Final Batch Loss: 0.19885337352752686\n",
      "Subject 6, Epoch 638, Loss: 0.7381198704242706, Final Batch Loss: 0.27086853981018066\n",
      "Subject 6, Epoch 639, Loss: 0.7890908867120743, Final Batch Loss: 0.1928558349609375\n",
      "Subject 6, Epoch 640, Loss: 0.7952499538660049, Final Batch Loss: 0.29787272214889526\n",
      "Subject 6, Epoch 641, Loss: 0.7053887993097305, Final Batch Loss: 0.22884884476661682\n",
      "Subject 6, Epoch 642, Loss: 0.6382732391357422, Final Batch Loss: 0.19959212839603424\n",
      "Subject 6, Epoch 643, Loss: 0.7338848412036896, Final Batch Loss: 0.1951906979084015\n",
      "Subject 6, Epoch 644, Loss: 0.7248578518629074, Final Batch Loss: 0.22106687724590302\n",
      "Subject 6, Epoch 645, Loss: 0.7027228474617004, Final Batch Loss: 0.2680050730705261\n",
      "Subject 6, Epoch 646, Loss: 0.822352722287178, Final Batch Loss: 0.30980241298675537\n",
      "Subject 6, Epoch 647, Loss: 0.7436149716377258, Final Batch Loss: 0.24676984548568726\n",
      "Subject 6, Epoch 648, Loss: 0.714817687869072, Final Batch Loss: 0.2359280288219452\n",
      "Subject 6, Epoch 649, Loss: 0.6916280835866928, Final Batch Loss: 0.1439845710992813\n",
      "Subject 6, Epoch 650, Loss: 0.780076801776886, Final Batch Loss: 0.30565333366394043\n",
      "Subject 6, Epoch 651, Loss: 0.6776237934827805, Final Batch Loss: 0.15894673764705658\n",
      "Subject 6, Epoch 652, Loss: 0.7774834930896759, Final Batch Loss: 0.27481383085250854\n",
      "Subject 6, Epoch 653, Loss: 0.6863607168197632, Final Batch Loss: 0.18294131755828857\n",
      "Subject 6, Epoch 654, Loss: 0.6584182381629944, Final Batch Loss: 0.1461656093597412\n",
      "Subject 6, Epoch 655, Loss: 0.8057770431041718, Final Batch Loss: 0.35693541169166565\n",
      "Subject 6, Epoch 656, Loss: 0.8100564777851105, Final Batch Loss: 0.2760488986968994\n",
      "Subject 6, Epoch 657, Loss: 0.6661904752254486, Final Batch Loss: 0.15972745418548584\n",
      "Subject 6, Epoch 658, Loss: 0.7581335753202438, Final Batch Loss: 0.3651200532913208\n",
      "Subject 6, Epoch 659, Loss: 0.7023066133260727, Final Batch Loss: 0.18645520508289337\n",
      "Subject 6, Epoch 660, Loss: 0.775265097618103, Final Batch Loss: 0.3477593958377838\n",
      "Subject 6, Epoch 661, Loss: 0.6433158069849014, Final Batch Loss: 0.2695833742618561\n",
      "Subject 6, Epoch 662, Loss: 0.6899550259113312, Final Batch Loss: 0.2385457307100296\n",
      "Subject 6, Epoch 663, Loss: 0.7492941915988922, Final Batch Loss: 0.264295369386673\n",
      "Subject 6, Epoch 664, Loss: 0.5824770629405975, Final Batch Loss: 0.16616976261138916\n",
      "Subject 6, Epoch 665, Loss: 0.7970580160617828, Final Batch Loss: 0.3058187663555145\n",
      "Subject 6, Epoch 666, Loss: 0.7604977786540985, Final Batch Loss: 0.2621117830276489\n",
      "Subject 6, Epoch 667, Loss: 0.7781718224287033, Final Batch Loss: 0.24584423005580902\n",
      "Subject 6, Epoch 668, Loss: 0.6959882080554962, Final Batch Loss: 0.18365132808685303\n",
      "Subject 6, Epoch 669, Loss: 0.6753679364919662, Final Batch Loss: 0.22219425439834595\n",
      "Subject 6, Epoch 670, Loss: 0.6428411453962326, Final Batch Loss: 0.16922858357429504\n",
      "Subject 6, Epoch 671, Loss: 0.6715717762708664, Final Batch Loss: 0.33673879504203796\n",
      "Subject 6, Epoch 672, Loss: 0.8296920210123062, Final Batch Loss: 0.3950289785861969\n",
      "Subject 6, Epoch 673, Loss: 0.7987193763256073, Final Batch Loss: 0.2794520854949951\n",
      "Subject 6, Epoch 674, Loss: 0.6978304833173752, Final Batch Loss: 0.23932689428329468\n",
      "Subject 6, Epoch 675, Loss: 0.7834892272949219, Final Batch Loss: 0.2728162705898285\n",
      "Subject 6, Epoch 676, Loss: 0.7307132482528687, Final Batch Loss: 0.14304813742637634\n",
      "Subject 6, Epoch 677, Loss: 0.6638976782560349, Final Batch Loss: 0.1964622139930725\n",
      "Subject 6, Epoch 678, Loss: 0.6849122941493988, Final Batch Loss: 0.13745111227035522\n",
      "Subject 6, Epoch 679, Loss: 0.6924472749233246, Final Batch Loss: 0.30156660079956055\n",
      "Subject 6, Epoch 680, Loss: 0.6637017726898193, Final Batch Loss: 0.1844346523284912\n",
      "Subject 6, Epoch 681, Loss: 0.665835902094841, Final Batch Loss: 0.15664206445217133\n",
      "Subject 6, Epoch 682, Loss: 0.6849759072065353, Final Batch Loss: 0.23193514347076416\n",
      "Subject 6, Epoch 683, Loss: 0.6575501561164856, Final Batch Loss: 0.24589093029499054\n",
      "Subject 6, Epoch 684, Loss: 0.6403725445270538, Final Batch Loss: 0.21191978454589844\n",
      "Subject 6, Epoch 685, Loss: 0.6613263785839081, Final Batch Loss: 0.2082744836807251\n",
      "Subject 6, Epoch 686, Loss: 0.6530287563800812, Final Batch Loss: 0.2531387209892273\n",
      "Subject 6, Epoch 687, Loss: 0.7101746797561646, Final Batch Loss: 0.2458266317844391\n",
      "Subject 6, Epoch 688, Loss: 0.6696943193674088, Final Batch Loss: 0.14294233918190002\n",
      "Subject 6, Epoch 689, Loss: 0.7197973281145096, Final Batch Loss: 0.2649226486682892\n",
      "Subject 6, Epoch 690, Loss: 0.6400623470544815, Final Batch Loss: 0.23729509115219116\n",
      "Subject 6, Epoch 691, Loss: 0.6279125511646271, Final Batch Loss: 0.12809719145298004\n",
      "Subject 6, Epoch 692, Loss: 0.7138553410768509, Final Batch Loss: 0.27653107047080994\n",
      "Subject 6, Epoch 693, Loss: 0.6803381145000458, Final Batch Loss: 0.2818503975868225\n",
      "Subject 6, Epoch 694, Loss: 0.5926592648029327, Final Batch Loss: 0.19255751371383667\n",
      "Subject 6, Epoch 695, Loss: 0.6910096257925034, Final Batch Loss: 0.16352836787700653\n",
      "Subject 6, Epoch 696, Loss: 0.6600249856710434, Final Batch Loss: 0.25696390867233276\n",
      "Subject 6, Epoch 697, Loss: 0.7185327857732773, Final Batch Loss: 0.28656482696533203\n",
      "Subject 6, Epoch 698, Loss: 0.6546410471200943, Final Batch Loss: 0.22954022884368896\n",
      "Subject 6, Epoch 699, Loss: 0.751025453209877, Final Batch Loss: 0.2700473368167877\n",
      "Subject 6, Epoch 700, Loss: 0.7473587393760681, Final Batch Loss: 0.2876465320587158\n",
      "Subject 6, Epoch 701, Loss: 0.6597380191087723, Final Batch Loss: 0.2890318036079407\n",
      "Subject 6, Epoch 702, Loss: 0.749566912651062, Final Batch Loss: 0.3330160677433014\n",
      "Subject 6, Epoch 703, Loss: 0.6915062516927719, Final Batch Loss: 0.2500147223472595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 704, Loss: 0.639899879693985, Final Batch Loss: 0.19433730840682983\n",
      "Subject 6, Epoch 705, Loss: 0.7149257659912109, Final Batch Loss: 0.25301551818847656\n",
      "Subject 6, Epoch 706, Loss: 0.6767287701368332, Final Batch Loss: 0.2938077449798584\n",
      "Subject 6, Epoch 707, Loss: 0.5956779569387436, Final Batch Loss: 0.14048431813716888\n",
      "Subject 6, Epoch 708, Loss: 0.6217788457870483, Final Batch Loss: 0.21807728707790375\n",
      "Subject 6, Epoch 709, Loss: 0.6913117170333862, Final Batch Loss: 0.17762768268585205\n",
      "Subject 6, Epoch 710, Loss: 0.6812181919813156, Final Batch Loss: 0.22415708005428314\n",
      "Subject 6, Epoch 711, Loss: 0.6001909226179123, Final Batch Loss: 0.13600066304206848\n",
      "Subject 6, Epoch 712, Loss: 0.6797318607568741, Final Batch Loss: 0.2710185945034027\n",
      "Subject 6, Epoch 713, Loss: 0.5819124579429626, Final Batch Loss: 0.17999593913555145\n",
      "Subject 6, Epoch 714, Loss: 0.8732655793428421, Final Batch Loss: 0.44772934913635254\n",
      "Subject 6, Epoch 715, Loss: 0.582435205578804, Final Batch Loss: 0.12652574479579926\n",
      "Subject 6, Epoch 716, Loss: 0.6376286745071411, Final Batch Loss: 0.2543520927429199\n",
      "Subject 6, Epoch 717, Loss: 0.8433986902236938, Final Batch Loss: 0.3195185363292694\n",
      "Subject 6, Epoch 718, Loss: 0.746575191617012, Final Batch Loss: 0.37492305040359497\n",
      "Subject 6, Epoch 719, Loss: 0.7390922755002975, Final Batch Loss: 0.12666621804237366\n",
      "Subject 6, Epoch 720, Loss: 0.6411941349506378, Final Batch Loss: 0.22177433967590332\n",
      "Subject 6, Epoch 721, Loss: 0.6880142688751221, Final Batch Loss: 0.2151585966348648\n",
      "Subject 6, Epoch 722, Loss: 0.5624189376831055, Final Batch Loss: 0.1887642741203308\n",
      "Subject 6, Epoch 723, Loss: 0.6059053242206573, Final Batch Loss: 0.15400588512420654\n",
      "Subject 6, Epoch 724, Loss: 0.6064406633377075, Final Batch Loss: 0.18867062032222748\n",
      "Subject 6, Epoch 725, Loss: 0.7229429334402084, Final Batch Loss: 0.20774812996387482\n",
      "Subject 6, Epoch 726, Loss: 0.5955312699079514, Final Batch Loss: 0.23080959916114807\n",
      "Subject 6, Epoch 727, Loss: 0.7318149954080582, Final Batch Loss: 0.30339041352272034\n",
      "Subject 6, Epoch 728, Loss: 0.5437067598104477, Final Batch Loss: 0.20316046476364136\n",
      "Subject 6, Epoch 729, Loss: 0.5898527950048447, Final Batch Loss: 0.14124572277069092\n",
      "Subject 6, Epoch 730, Loss: 0.6394902467727661, Final Batch Loss: 0.2158377766609192\n",
      "Subject 6, Epoch 731, Loss: 0.7160413265228271, Final Batch Loss: 0.2108096182346344\n",
      "Subject 6, Epoch 732, Loss: 0.6622655689716339, Final Batch Loss: 0.19336798787117004\n",
      "Subject 6, Epoch 733, Loss: 0.6448621600866318, Final Batch Loss: 0.20431789755821228\n",
      "Subject 6, Epoch 734, Loss: 0.747869610786438, Final Batch Loss: 0.317165344953537\n",
      "Subject 6, Epoch 735, Loss: 0.558758556842804, Final Batch Loss: 0.13773921132087708\n",
      "Subject 6, Epoch 736, Loss: 0.6044717878103256, Final Batch Loss: 0.12602952122688293\n",
      "Subject 6, Epoch 737, Loss: 0.6999759674072266, Final Batch Loss: 0.24373328685760498\n",
      "Subject 6, Epoch 738, Loss: 0.611675500869751, Final Batch Loss: 0.1897035539150238\n",
      "Subject 6, Epoch 739, Loss: 0.6856314986944199, Final Batch Loss: 0.2534545063972473\n",
      "Subject 6, Epoch 740, Loss: 0.651794359087944, Final Batch Loss: 0.23319920897483826\n",
      "Subject 6, Epoch 741, Loss: 0.6587465927004814, Final Batch Loss: 0.3239365816116333\n",
      "Subject 6, Epoch 742, Loss: 0.6212676167488098, Final Batch Loss: 0.16127440333366394\n",
      "Subject 6, Epoch 743, Loss: 0.6863520592451096, Final Batch Loss: 0.3000078499317169\n",
      "Subject 6, Epoch 744, Loss: 0.6943247616291046, Final Batch Loss: 0.2767164707183838\n",
      "Subject 6, Epoch 745, Loss: 0.685439258813858, Final Batch Loss: 0.26447945833206177\n",
      "Subject 6, Epoch 746, Loss: 0.6072671115398407, Final Batch Loss: 0.1784592866897583\n",
      "Subject 6, Epoch 747, Loss: 0.5913477092981339, Final Batch Loss: 0.16017024219036102\n",
      "Subject 6, Epoch 748, Loss: 0.6735855042934418, Final Batch Loss: 0.2009575217962265\n",
      "Subject 6, Epoch 749, Loss: 0.6176244914531708, Final Batch Loss: 0.22624264657497406\n",
      "Subject 6, Epoch 750, Loss: 0.7086419314146042, Final Batch Loss: 0.18190115690231323\n",
      "Subject 6, Epoch 751, Loss: 0.6651036292314529, Final Batch Loss: 0.2759745717048645\n",
      "Subject 6, Epoch 752, Loss: 0.7962904423475266, Final Batch Loss: 0.2716294527053833\n",
      "Subject 6, Epoch 753, Loss: 0.5618368089199066, Final Batch Loss: 0.1552688479423523\n",
      "Subject 6, Epoch 754, Loss: 0.6420911699533463, Final Batch Loss: 0.263293981552124\n",
      "Subject 6, Epoch 755, Loss: 0.6176193803548813, Final Batch Loss: 0.12127922475337982\n",
      "Subject 6, Epoch 756, Loss: 0.738118439912796, Final Batch Loss: 0.2539272904396057\n",
      "Subject 6, Epoch 757, Loss: 0.6394853293895721, Final Batch Loss: 0.19572235643863678\n",
      "Subject 6, Epoch 758, Loss: 0.6376001983880997, Final Batch Loss: 0.19212020933628082\n",
      "Subject 6, Epoch 759, Loss: 0.609110489487648, Final Batch Loss: 0.17886994779109955\n",
      "Subject 6, Epoch 760, Loss: 0.5645561069250107, Final Batch Loss: 0.1635444164276123\n",
      "Subject 6, Epoch 761, Loss: 0.584087148308754, Final Batch Loss: 0.24560560286045074\n",
      "Subject 6, Epoch 762, Loss: 0.6255744248628616, Final Batch Loss: 0.2137886881828308\n",
      "Subject 6, Epoch 763, Loss: 0.7174307107925415, Final Batch Loss: 0.16781765222549438\n",
      "Subject 6, Epoch 764, Loss: 0.5195159018039703, Final Batch Loss: 0.16406607627868652\n",
      "Subject 6, Epoch 765, Loss: 0.6549651771783829, Final Batch Loss: 0.21593959629535675\n",
      "Subject 6, Epoch 766, Loss: 0.5450673401355743, Final Batch Loss: 0.17339810729026794\n",
      "Subject 6, Epoch 767, Loss: 0.6681611388921738, Final Batch Loss: 0.19609606266021729\n",
      "Subject 6, Epoch 768, Loss: 0.565427765250206, Final Batch Loss: 0.18250811100006104\n",
      "Subject 6, Epoch 769, Loss: 0.5897160768508911, Final Batch Loss: 0.19288894534111023\n",
      "Subject 6, Epoch 770, Loss: 0.5963950008153915, Final Batch Loss: 0.1595735102891922\n",
      "Subject 6, Epoch 771, Loss: 0.5911778807640076, Final Batch Loss: 0.23829466104507446\n",
      "Subject 6, Epoch 772, Loss: 0.6295731663703918, Final Batch Loss: 0.22891175746917725\n",
      "Subject 6, Epoch 773, Loss: 0.6940152049064636, Final Batch Loss: 0.22987604141235352\n",
      "Subject 6, Epoch 774, Loss: 0.7069219350814819, Final Batch Loss: 0.26570895314216614\n",
      "Subject 6, Epoch 775, Loss: 0.6233434677124023, Final Batch Loss: 0.22398990392684937\n",
      "Subject 6, Epoch 776, Loss: 0.5902409851551056, Final Batch Loss: 0.14977924525737762\n",
      "Subject 6, Epoch 777, Loss: 0.6533133536577225, Final Batch Loss: 0.17147941887378693\n",
      "Subject 6, Epoch 778, Loss: 0.5979741513729095, Final Batch Loss: 0.22676196694374084\n",
      "Subject 6, Epoch 779, Loss: 0.6602443605661392, Final Batch Loss: 0.2364940047264099\n",
      "Subject 6, Epoch 780, Loss: 0.6735695600509644, Final Batch Loss: 0.158779114484787\n",
      "Subject 6, Epoch 781, Loss: 0.5904505103826523, Final Batch Loss: 0.1959073543548584\n",
      "Subject 6, Epoch 782, Loss: 0.5607364997267723, Final Batch Loss: 0.24485164880752563\n",
      "Subject 6, Epoch 783, Loss: 0.5962813049554825, Final Batch Loss: 0.20403052866458893\n",
      "Subject 6, Epoch 784, Loss: 0.5998789072036743, Final Batch Loss: 0.15737546980381012\n",
      "Subject 6, Epoch 785, Loss: 0.5889294147491455, Final Batch Loss: 0.1462668776512146\n",
      "Subject 6, Epoch 786, Loss: 0.5811359286308289, Final Batch Loss: 0.163249209523201\n",
      "Subject 6, Epoch 787, Loss: 0.6393776386976242, Final Batch Loss: 0.24842321872711182\n",
      "Subject 6, Epoch 788, Loss: 0.5366551131010056, Final Batch Loss: 0.17155995965003967\n",
      "Subject 6, Epoch 789, Loss: 0.6889718323945999, Final Batch Loss: 0.27475419640541077\n",
      "Subject 6, Epoch 790, Loss: 0.6263978183269501, Final Batch Loss: 0.21030494570732117\n",
      "Subject 6, Epoch 791, Loss: 0.6146768629550934, Final Batch Loss: 0.15243424475193024\n",
      "Subject 6, Epoch 792, Loss: 0.6282313913106918, Final Batch Loss: 0.27104613184928894\n",
      "Subject 6, Epoch 793, Loss: 0.5425366908311844, Final Batch Loss: 0.18452422320842743\n",
      "Subject 6, Epoch 794, Loss: 0.6195242702960968, Final Batch Loss: 0.23397813737392426\n",
      "Subject 6, Epoch 795, Loss: 0.6574971377849579, Final Batch Loss: 0.1873384565114975\n",
      "Subject 6, Epoch 796, Loss: 0.526653528213501, Final Batch Loss: 0.1423204392194748\n",
      "Subject 6, Epoch 797, Loss: 0.5090350359678268, Final Batch Loss: 0.1121593713760376\n",
      "Subject 6, Epoch 798, Loss: 0.7072391510009766, Final Batch Loss: 0.17688079178333282\n",
      "Subject 6, Epoch 799, Loss: 0.5196752846240997, Final Batch Loss: 0.20359773933887482\n",
      "Subject 6, Epoch 800, Loss: 0.6716609299182892, Final Batch Loss: 0.16182006895542145\n",
      "Subject 6, Epoch 801, Loss: 0.6422438621520996, Final Batch Loss: 0.2167636603116989\n",
      "Subject 6, Epoch 802, Loss: 0.6583891808986664, Final Batch Loss: 0.1647399663925171\n",
      "Subject 6, Epoch 803, Loss: 0.5603032335639, Final Batch Loss: 0.1059858426451683\n",
      "Subject 6, Epoch 804, Loss: 0.6627819985151291, Final Batch Loss: 0.1971694380044937\n",
      "Subject 6, Epoch 805, Loss: 0.5981403440237045, Final Batch Loss: 0.27520549297332764\n",
      "Subject 6, Epoch 806, Loss: 0.6095295995473862, Final Batch Loss: 0.20799285173416138\n",
      "Subject 6, Epoch 807, Loss: 0.48178666085004807, Final Batch Loss: 0.07519381493330002\n",
      "Subject 6, Epoch 808, Loss: 0.6740961819887161, Final Batch Loss: 0.14100730419158936\n",
      "Subject 6, Epoch 809, Loss: 0.5345853567123413, Final Batch Loss: 0.11182336509227753\n",
      "Subject 6, Epoch 810, Loss: 0.5443481355905533, Final Batch Loss: 0.2253679782152176\n",
      "Subject 6, Epoch 811, Loss: 0.6109108775854111, Final Batch Loss: 0.24428421258926392\n",
      "Subject 6, Epoch 812, Loss: 0.6612119749188423, Final Batch Loss: 0.1173018291592598\n",
      "Subject 6, Epoch 813, Loss: 0.7125914543867111, Final Batch Loss: 0.29256120324134827\n",
      "Subject 6, Epoch 814, Loss: 0.5238447487354279, Final Batch Loss: 0.16856259107589722\n",
      "Subject 6, Epoch 815, Loss: 0.5346992015838623, Final Batch Loss: 0.1484433114528656\n",
      "Subject 6, Epoch 816, Loss: 0.5389057397842407, Final Batch Loss: 0.21910731494426727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 817, Loss: 0.6968100219964981, Final Batch Loss: 0.17495395243167877\n",
      "Subject 6, Epoch 818, Loss: 0.5518440306186676, Final Batch Loss: 0.18296489119529724\n",
      "Subject 6, Epoch 819, Loss: 0.6492621302604675, Final Batch Loss: 0.2083832025527954\n",
      "Subject 6, Epoch 820, Loss: 0.4792403131723404, Final Batch Loss: 0.1672462522983551\n",
      "Subject 6, Epoch 821, Loss: 0.5277397930622101, Final Batch Loss: 0.18203526735305786\n",
      "Subject 6, Epoch 822, Loss: 0.5383814126253128, Final Batch Loss: 0.15956977009773254\n",
      "Subject 6, Epoch 823, Loss: 0.5420025885105133, Final Batch Loss: 0.15770669281482697\n",
      "Subject 6, Epoch 824, Loss: 0.5071771144866943, Final Batch Loss: 0.17530660331249237\n",
      "Subject 6, Epoch 825, Loss: 0.5909279882907867, Final Batch Loss: 0.17363609373569489\n",
      "Subject 6, Epoch 826, Loss: 0.59329654276371, Final Batch Loss: 0.16755333542823792\n",
      "Subject 6, Epoch 827, Loss: 0.5436332821846008, Final Batch Loss: 0.1631692498922348\n",
      "Subject 6, Epoch 828, Loss: 0.4964136332273483, Final Batch Loss: 0.11394418776035309\n",
      "Subject 6, Epoch 829, Loss: 0.6458002924919128, Final Batch Loss: 0.17555956542491913\n",
      "Subject 6, Epoch 830, Loss: 0.5550624430179596, Final Batch Loss: 0.1795433908700943\n",
      "Subject 6, Epoch 831, Loss: 0.5997915863990784, Final Batch Loss: 0.3036780059337616\n",
      "Subject 6, Epoch 832, Loss: 0.6495845466852188, Final Batch Loss: 0.26537880301475525\n",
      "Subject 6, Epoch 833, Loss: 0.6079772859811783, Final Batch Loss: 0.18504193425178528\n",
      "Subject 6, Epoch 834, Loss: 0.7121509313583374, Final Batch Loss: 0.22757352888584137\n",
      "Subject 6, Epoch 835, Loss: 0.5765489116311073, Final Batch Loss: 0.07310942560434341\n",
      "Subject 6, Epoch 836, Loss: 0.5951103121042252, Final Batch Loss: 0.2126106172800064\n",
      "Subject 6, Epoch 837, Loss: 0.5142520666122437, Final Batch Loss: 0.16484224796295166\n",
      "Subject 6, Epoch 838, Loss: 0.5784662663936615, Final Batch Loss: 0.235623300075531\n",
      "Subject 6, Epoch 839, Loss: 0.5814711153507233, Final Batch Loss: 0.15040726959705353\n",
      "Subject 6, Epoch 840, Loss: 0.6492653042078018, Final Batch Loss: 0.20775575935840607\n",
      "Subject 6, Epoch 841, Loss: 0.49631255120038986, Final Batch Loss: 0.17124857008457184\n",
      "Subject 6, Epoch 842, Loss: 0.527904212474823, Final Batch Loss: 0.17945940792560577\n",
      "Subject 6, Epoch 843, Loss: 0.5126389786601067, Final Batch Loss: 0.25350692868232727\n",
      "Subject 6, Epoch 844, Loss: 0.6779999434947968, Final Batch Loss: 0.19768643379211426\n",
      "Subject 6, Epoch 845, Loss: 0.5880392044782639, Final Batch Loss: 0.2005859613418579\n",
      "Subject 6, Epoch 846, Loss: 0.5700796842575073, Final Batch Loss: 0.23114952445030212\n",
      "Subject 6, Epoch 847, Loss: 0.6648005992174149, Final Batch Loss: 0.34683850407600403\n",
      "Subject 6, Epoch 848, Loss: 0.5329323410987854, Final Batch Loss: 0.173447847366333\n",
      "Subject 6, Epoch 849, Loss: 0.5707863122224808, Final Batch Loss: 0.2092435657978058\n",
      "Subject 6, Epoch 850, Loss: 0.6418517380952835, Final Batch Loss: 0.21801966428756714\n",
      "Subject 6, Epoch 851, Loss: 0.6663833856582642, Final Batch Loss: 0.3313537836074829\n",
      "Subject 6, Epoch 852, Loss: 0.5507505536079407, Final Batch Loss: 0.20349441468715668\n",
      "Subject 6, Epoch 853, Loss: 0.5233989804983139, Final Batch Loss: 0.26279574632644653\n",
      "Subject 6, Epoch 854, Loss: 0.6229363977909088, Final Batch Loss: 0.20884844660758972\n",
      "Subject 6, Epoch 855, Loss: 0.5225751101970673, Final Batch Loss: 0.17286360263824463\n",
      "Subject 6, Epoch 856, Loss: 0.5066372007131577, Final Batch Loss: 0.2122238427400589\n",
      "Subject 6, Epoch 857, Loss: 0.645361602306366, Final Batch Loss: 0.1954885721206665\n",
      "Subject 6, Epoch 858, Loss: 0.583792194724083, Final Batch Loss: 0.19030442833900452\n",
      "Subject 6, Epoch 859, Loss: 0.47791870683431625, Final Batch Loss: 0.21086975932121277\n",
      "Subject 6, Epoch 860, Loss: 0.5405227839946747, Final Batch Loss: 0.22643263638019562\n",
      "Subject 6, Epoch 861, Loss: 0.6388365253806114, Final Batch Loss: 0.2732976973056793\n",
      "Subject 6, Epoch 862, Loss: 0.6196879521012306, Final Batch Loss: 0.27455052733421326\n",
      "Subject 6, Epoch 863, Loss: 0.6466637477278709, Final Batch Loss: 0.1162494346499443\n",
      "Subject 6, Epoch 864, Loss: 0.5794344395399094, Final Batch Loss: 0.19872939586639404\n",
      "Subject 6, Epoch 865, Loss: 0.4811325743794441, Final Batch Loss: 0.10362749546766281\n",
      "Subject 6, Epoch 866, Loss: 0.5846046507358551, Final Batch Loss: 0.23242047429084778\n",
      "Subject 6, Epoch 867, Loss: 0.5391810089349747, Final Batch Loss: 0.21670353412628174\n",
      "Subject 6, Epoch 868, Loss: 0.6536187678575516, Final Batch Loss: 0.19784852862358093\n",
      "Subject 6, Epoch 869, Loss: 0.5413355082273483, Final Batch Loss: 0.18067412078380585\n",
      "Subject 6, Epoch 870, Loss: 0.6273708045482635, Final Batch Loss: 0.19433234632015228\n",
      "Subject 6, Epoch 871, Loss: 0.5200951546430588, Final Batch Loss: 0.2019355297088623\n",
      "Subject 6, Epoch 872, Loss: 0.583792507648468, Final Batch Loss: 0.2457606941461563\n",
      "Subject 6, Epoch 873, Loss: 0.4730831906199455, Final Batch Loss: 0.16479642689228058\n",
      "Subject 6, Epoch 874, Loss: 0.554770901799202, Final Batch Loss: 0.2343359738588333\n",
      "Subject 6, Epoch 875, Loss: 0.5410895645618439, Final Batch Loss: 0.22447572648525238\n",
      "Subject 6, Epoch 876, Loss: 0.5059312582015991, Final Batch Loss: 0.14275994896888733\n",
      "Subject 6, Epoch 877, Loss: 0.49936434626579285, Final Batch Loss: 0.21606671810150146\n",
      "Subject 6, Epoch 878, Loss: 0.5805197805166245, Final Batch Loss: 0.1981675922870636\n",
      "Subject 6, Epoch 879, Loss: 0.4986860156059265, Final Batch Loss: 0.1761917918920517\n",
      "Subject 6, Epoch 880, Loss: 0.43775802105665207, Final Batch Loss: 0.1371873915195465\n",
      "Subject 6, Epoch 881, Loss: 0.5159069150686264, Final Batch Loss: 0.1779720038175583\n",
      "Subject 6, Epoch 882, Loss: 0.4943416565656662, Final Batch Loss: 0.1518007069826126\n",
      "Subject 6, Epoch 883, Loss: 0.44239845126867294, Final Batch Loss: 0.1984669715166092\n",
      "Subject 6, Epoch 884, Loss: 0.5433380603790283, Final Batch Loss: 0.19535696506500244\n",
      "Subject 6, Epoch 885, Loss: 0.49929043650627136, Final Batch Loss: 0.16956627368927002\n",
      "Subject 6, Epoch 886, Loss: 0.5925001055002213, Final Batch Loss: 0.24625276029109955\n",
      "Subject 6, Epoch 887, Loss: 0.5144316256046295, Final Batch Loss: 0.19909337162971497\n",
      "Subject 6, Epoch 888, Loss: 0.5601134300231934, Final Batch Loss: 0.22376370429992676\n",
      "Subject 6, Epoch 889, Loss: 0.550106942653656, Final Batch Loss: 0.20507566630840302\n",
      "Subject 6, Epoch 890, Loss: 0.5228137224912643, Final Batch Loss: 0.19057103991508484\n",
      "Subject 6, Epoch 891, Loss: 0.5567132234573364, Final Batch Loss: 0.1751212477684021\n",
      "Subject 6, Epoch 892, Loss: 0.5668327957391739, Final Batch Loss: 0.19169022142887115\n",
      "Subject 6, Epoch 893, Loss: 0.5368928536772728, Final Batch Loss: 0.11364885419607162\n",
      "Subject 6, Epoch 894, Loss: 0.49757568538188934, Final Batch Loss: 0.10171382874250412\n",
      "Subject 6, Epoch 895, Loss: 0.5095247626304626, Final Batch Loss: 0.21526850759983063\n",
      "Subject 6, Epoch 896, Loss: 0.5590699166059494, Final Batch Loss: 0.17882812023162842\n",
      "Subject 6, Epoch 897, Loss: 0.5963975191116333, Final Batch Loss: 0.1734812706708908\n",
      "Subject 6, Epoch 898, Loss: 0.4509325996041298, Final Batch Loss: 0.10466345399618149\n",
      "Subject 6, Epoch 899, Loss: 0.42338118702173233, Final Batch Loss: 0.10622857511043549\n",
      "Subject 6, Epoch 900, Loss: 0.5449101030826569, Final Batch Loss: 0.19515663385391235\n",
      "Subject 6, Epoch 901, Loss: 0.4798087924718857, Final Batch Loss: 0.15110790729522705\n",
      "Subject 6, Epoch 902, Loss: 0.4694415479898453, Final Batch Loss: 0.15357951819896698\n",
      "Subject 6, Epoch 903, Loss: 0.5754444301128387, Final Batch Loss: 0.20955334603786469\n",
      "Subject 6, Epoch 904, Loss: 0.45575548708438873, Final Batch Loss: 0.16958649456501007\n",
      "Subject 6, Epoch 905, Loss: 0.49314433336257935, Final Batch Loss: 0.15721331536769867\n",
      "Subject 6, Epoch 906, Loss: 0.5353030115365982, Final Batch Loss: 0.16500984132289886\n",
      "Subject 6, Epoch 907, Loss: 0.4340423047542572, Final Batch Loss: 0.19636619091033936\n",
      "Subject 6, Epoch 908, Loss: 0.5214321166276932, Final Batch Loss: 0.1549249291419983\n",
      "Subject 6, Epoch 909, Loss: 0.5400190651416779, Final Batch Loss: 0.1807798147201538\n",
      "Subject 6, Epoch 910, Loss: 0.5142134800553322, Final Batch Loss: 0.10990612953901291\n",
      "Subject 6, Epoch 911, Loss: 0.5915720760822296, Final Batch Loss: 0.18571197986602783\n",
      "Subject 6, Epoch 912, Loss: 0.5406609773635864, Final Batch Loss: 0.15383648872375488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 6, Epoch 913, Loss: 0.5899322330951691, Final Batch Loss: 0.2720337212085724\n",
      "Subject 6, Epoch 914, Loss: 0.5968198180198669, Final Batch Loss: 0.16319501399993896\n",
      "Subject 6, Epoch 915, Loss: 0.4301013723015785, Final Batch Loss: 0.13320229947566986\n",
      "Subject 6, Epoch 916, Loss: 0.6096662133932114, Final Batch Loss: 0.20259037613868713\n",
      "Subject 6, Epoch 917, Loss: 0.6178153604269028, Final Batch Loss: 0.283368319272995\n",
      "Subject 6, Epoch 918, Loss: 0.4843333885073662, Final Batch Loss: 0.11248091608285904\n",
      "Subject 6, Epoch 919, Loss: 0.6859069466590881, Final Batch Loss: 0.27875980734825134\n",
      "Subject 6, Epoch 920, Loss: 0.5113845095038414, Final Batch Loss: 0.15152043104171753\n",
      "Subject 6, Epoch 921, Loss: 0.5454232394695282, Final Batch Loss: 0.16515116393566132\n",
      "Subject 6, Epoch 922, Loss: 0.4804840609431267, Final Batch Loss: 0.12424292415380478\n",
      "Subject 6, Epoch 923, Loss: 0.4819350838661194, Final Batch Loss: 0.20652177929878235\n",
      "Subject 6, Epoch 924, Loss: 0.5355617478489876, Final Batch Loss: 0.13681338727474213\n",
      "Subject 6, Epoch 925, Loss: 0.5954511314630508, Final Batch Loss: 0.2768106758594513\n",
      "Subject 6, Epoch 926, Loss: 0.5901960283517838, Final Batch Loss: 0.14909344911575317\n",
      "Subject 6, Epoch 927, Loss: 0.49464041739702225, Final Batch Loss: 0.2130313515663147\n",
      "Subject 6, Epoch 928, Loss: 0.4258740320801735, Final Batch Loss: 0.10469261556863785\n",
      "Subject 6, Epoch 929, Loss: 0.5314528495073318, Final Batch Loss: 0.1534907966852188\n",
      "Subject 6, Epoch 930, Loss: 0.4259175807237625, Final Batch Loss: 0.1308685690164566\n",
      "Subject 6, Epoch 931, Loss: 0.686823695898056, Final Batch Loss: 0.31663399934768677\n",
      "Subject 6, Epoch 932, Loss: 0.4277709871530533, Final Batch Loss: 0.1840328872203827\n",
      "Subject 6, Epoch 933, Loss: 0.5527402609586716, Final Batch Loss: 0.23651224374771118\n",
      "Subject 6, Epoch 934, Loss: 0.6776320487260818, Final Batch Loss: 0.2689584791660309\n",
      "Subject 6, Epoch 935, Loss: 0.5119359791278839, Final Batch Loss: 0.24985361099243164\n",
      "Subject 6, Epoch 936, Loss: 0.49232155084609985, Final Batch Loss: 0.18599781394004822\n",
      "Subject 6, Epoch 937, Loss: 0.4873017519712448, Final Batch Loss: 0.2038448452949524\n",
      "Subject 6, Epoch 938, Loss: 0.6084902137517929, Final Batch Loss: 0.26424267888069153\n",
      "Subject 6, Epoch 939, Loss: 0.5407139509916306, Final Batch Loss: 0.16471581161022186\n",
      "Subject 6, Epoch 940, Loss: 0.42804093658924103, Final Batch Loss: 0.13211168348789215\n",
      "Subject 6, Epoch 941, Loss: 0.5564865916967392, Final Batch Loss: 0.2515751123428345\n",
      "Subject 6, Epoch 942, Loss: 0.5860312283039093, Final Batch Loss: 0.1777949333190918\n",
      "Subject 6, Epoch 943, Loss: 0.5043614655733109, Final Batch Loss: 0.15832507610321045\n",
      "Subject 6, Epoch 944, Loss: 0.46143988519907, Final Batch Loss: 0.08532457798719406\n",
      "Subject 6, Epoch 945, Loss: 0.5192733854055405, Final Batch Loss: 0.17717860639095306\n",
      "Subject 6, Epoch 946, Loss: 0.5244713127613068, Final Batch Loss: 0.150746151804924\n",
      "Subject 6, Epoch 947, Loss: 0.38814840465784073, Final Batch Loss: 0.08403750509023666\n",
      "Subject 6, Epoch 948, Loss: 0.507977232336998, Final Batch Loss: 0.20419342815876007\n",
      "Subject 6, Epoch 949, Loss: 0.5717544853687286, Final Batch Loss: 0.13817818462848663\n",
      "Subject 6, Epoch 950, Loss: 0.5063371881842613, Final Batch Loss: 0.11337622255086899\n",
      "Subject 6, Epoch 951, Loss: 0.44291720539331436, Final Batch Loss: 0.11251241713762283\n",
      "Subject 6, Epoch 952, Loss: 0.4491797387599945, Final Batch Loss: 0.17266501486301422\n",
      "Subject 6, Epoch 953, Loss: 0.49743814766407013, Final Batch Loss: 0.17235659062862396\n",
      "Subject 6, Epoch 954, Loss: 0.5724273920059204, Final Batch Loss: 0.16530174016952515\n",
      "Subject 6, Epoch 955, Loss: 0.432142935693264, Final Batch Loss: 0.12389812618494034\n",
      "Subject 6, Epoch 956, Loss: 0.5153661593794823, Final Batch Loss: 0.07303941994905472\n",
      "Subject 6, Epoch 957, Loss: 0.47700925171375275, Final Batch Loss: 0.1332402378320694\n",
      "Subject 6, Epoch 958, Loss: 0.5515129715204239, Final Batch Loss: 0.2453821897506714\n",
      "Subject 6, Epoch 959, Loss: 0.5700656175613403, Final Batch Loss: 0.17125332355499268\n",
      "Subject 6, Epoch 960, Loss: 0.4549713060259819, Final Batch Loss: 0.19315260648727417\n",
      "Subject 6, Epoch 961, Loss: 0.4602617472410202, Final Batch Loss: 0.14779342710971832\n",
      "Subject 6, Epoch 962, Loss: 0.4644935354590416, Final Batch Loss: 0.20466122031211853\n",
      "Subject 6, Epoch 963, Loss: 0.4327887296676636, Final Batch Loss: 0.0974680483341217\n",
      "Subject 6, Epoch 964, Loss: 0.3915392905473709, Final Batch Loss: 0.07670629024505615\n",
      "Subject 6, Epoch 965, Loss: 0.4819951355457306, Final Batch Loss: 0.16758780181407928\n",
      "Subject 6, Epoch 966, Loss: 0.44275903701782227, Final Batch Loss: 0.13763993978500366\n",
      "Subject 6, Epoch 967, Loss: 0.3517827168107033, Final Batch Loss: 0.13997653126716614\n",
      "Subject 6, Epoch 968, Loss: 0.5225255638360977, Final Batch Loss: 0.195829838514328\n",
      "Subject 6, Epoch 969, Loss: 0.48989666998386383, Final Batch Loss: 0.15699000656604767\n",
      "Subject 6, Epoch 970, Loss: 0.39562853425741196, Final Batch Loss: 0.10668578743934631\n",
      "Subject 6, Epoch 971, Loss: 0.4307997077703476, Final Batch Loss: 0.14728204905986786\n",
      "Subject 6, Epoch 972, Loss: 0.44755883514881134, Final Batch Loss: 0.13210684061050415\n",
      "Subject 6, Epoch 973, Loss: 0.41377104073762894, Final Batch Loss: 0.08541234582662582\n",
      "Subject 6, Epoch 974, Loss: 0.4303606301546097, Final Batch Loss: 0.09441691637039185\n",
      "Subject 6, Epoch 975, Loss: 0.46788954734802246, Final Batch Loss: 0.2083778977394104\n",
      "Subject 6, Epoch 976, Loss: 0.4980742633342743, Final Batch Loss: 0.1273937076330185\n",
      "Subject 6, Epoch 977, Loss: 0.44359011948108673, Final Batch Loss: 0.1510361135005951\n",
      "Subject 6, Epoch 978, Loss: 0.4544316679239273, Final Batch Loss: 0.17442643642425537\n",
      "Subject 6, Epoch 979, Loss: 0.5761298388242722, Final Batch Loss: 0.19741572439670563\n",
      "Subject 6, Epoch 980, Loss: 0.4006718397140503, Final Batch Loss: 0.13859029114246368\n",
      "Subject 6, Epoch 981, Loss: 0.5410510450601578, Final Batch Loss: 0.17411023378372192\n",
      "Subject 6, Epoch 982, Loss: 0.4182368516921997, Final Batch Loss: 0.2118697315454483\n",
      "Subject 6, Epoch 983, Loss: 0.46525898575782776, Final Batch Loss: 0.14557546377182007\n",
      "Subject 6, Epoch 984, Loss: 0.3867829442024231, Final Batch Loss: 0.08733604103326797\n",
      "Subject 6, Epoch 985, Loss: 0.5682982355356216, Final Batch Loss: 0.14833125472068787\n",
      "Subject 6, Epoch 986, Loss: 0.38854388892650604, Final Batch Loss: 0.14569520950317383\n",
      "Subject 6, Epoch 987, Loss: 0.46444071829319, Final Batch Loss: 0.16076886653900146\n",
      "Subject 6, Epoch 988, Loss: 0.5999906808137894, Final Batch Loss: 0.1554262340068817\n",
      "Subject 6, Epoch 989, Loss: 0.5284207463264465, Final Batch Loss: 0.21422621607780457\n",
      "Subject 6, Epoch 990, Loss: 0.47018593549728394, Final Batch Loss: 0.13435618579387665\n",
      "Subject 6, Epoch 991, Loss: 0.48620204627513885, Final Batch Loss: 0.10164235532283783\n",
      "Subject 6, Epoch 992, Loss: 0.4954715743660927, Final Batch Loss: 0.2618139386177063\n",
      "Subject 6, Epoch 993, Loss: 0.44014137983322144, Final Batch Loss: 0.11031950265169144\n",
      "Subject 6, Epoch 994, Loss: 0.4290109723806381, Final Batch Loss: 0.13598386943340302\n",
      "Subject 6, Epoch 995, Loss: 0.504669725894928, Final Batch Loss: 0.2094147950410843\n",
      "Subject 6, Epoch 996, Loss: 0.4186505228281021, Final Batch Loss: 0.18781879544258118\n",
      "Subject 6, Epoch 997, Loss: 0.3975077420473099, Final Batch Loss: 0.06556367129087448\n",
      "Subject 6, Epoch 998, Loss: 0.3910740241408348, Final Batch Loss: 0.1441119909286499\n",
      "Subject 6, Epoch 999, Loss: 0.4166012927889824, Final Batch Loss: 0.08979063481092453\n",
      "Subject 6, Epoch 1000, Loss: 0.3558865487575531, Final Batch Loss: 0.09917406737804413\n",
      "Subject 7, Epoch 1, Loss: 5.397295594215393, Final Batch Loss: 1.807316541671753\n",
      "Subject 7, Epoch 2, Loss: 5.394942760467529, Final Batch Loss: 1.7869279384613037\n",
      "Subject 7, Epoch 3, Loss: 5.389480710029602, Final Batch Loss: 1.7863925695419312\n",
      "Subject 7, Epoch 4, Loss: 5.37797474861145, Final Batch Loss: 1.7840265035629272\n",
      "Subject 7, Epoch 5, Loss: 5.3891366720199585, Final Batch Loss: 1.822792410850525\n",
      "Subject 7, Epoch 6, Loss: 5.37804102897644, Final Batch Loss: 1.7982969284057617\n",
      "Subject 7, Epoch 7, Loss: 5.363732099533081, Final Batch Loss: 1.7810349464416504\n",
      "Subject 7, Epoch 8, Loss: 5.3672367334365845, Final Batch Loss: 1.7946076393127441\n",
      "Subject 7, Epoch 9, Loss: 5.346552133560181, Final Batch Loss: 1.7759591341018677\n",
      "Subject 7, Epoch 10, Loss: 5.3441736698150635, Final Batch Loss: 1.7757694721221924\n",
      "Subject 7, Epoch 11, Loss: 5.323314189910889, Final Batch Loss: 1.776045560836792\n",
      "Subject 7, Epoch 12, Loss: 5.318180561065674, Final Batch Loss: 1.7749569416046143\n",
      "Subject 7, Epoch 13, Loss: 5.304881930351257, Final Batch Loss: 1.7666174173355103\n",
      "Subject 7, Epoch 14, Loss: 5.288713693618774, Final Batch Loss: 1.769752860069275\n",
      "Subject 7, Epoch 15, Loss: 5.264893889427185, Final Batch Loss: 1.7497694492340088\n",
      "Subject 7, Epoch 16, Loss: 5.2356276512146, Final Batch Loss: 1.7417453527450562\n",
      "Subject 7, Epoch 17, Loss: 5.209299087524414, Final Batch Loss: 1.7349181175231934\n",
      "Subject 7, Epoch 18, Loss: 5.167380094528198, Final Batch Loss: 1.7209848165512085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 19, Loss: 5.129702925682068, Final Batch Loss: 1.7079435586929321\n",
      "Subject 7, Epoch 20, Loss: 5.064125418663025, Final Batch Loss: 1.682725191116333\n",
      "Subject 7, Epoch 21, Loss: 4.998878836631775, Final Batch Loss: 1.650854229927063\n",
      "Subject 7, Epoch 22, Loss: 4.913115620613098, Final Batch Loss: 1.635414481163025\n",
      "Subject 7, Epoch 23, Loss: 4.793745398521423, Final Batch Loss: 1.5902609825134277\n",
      "Subject 7, Epoch 24, Loss: 4.731497168540955, Final Batch Loss: 1.5695232152938843\n",
      "Subject 7, Epoch 25, Loss: 4.597543954849243, Final Batch Loss: 1.5137207508087158\n",
      "Subject 7, Epoch 26, Loss: 4.455339550971985, Final Batch Loss: 1.4819256067276\n",
      "Subject 7, Epoch 27, Loss: 4.308735966682434, Final Batch Loss: 1.4278552532196045\n",
      "Subject 7, Epoch 28, Loss: 4.194767355918884, Final Batch Loss: 1.373329520225525\n",
      "Subject 7, Epoch 29, Loss: 4.073545217514038, Final Batch Loss: 1.339524507522583\n",
      "Subject 7, Epoch 30, Loss: 3.9856337308883667, Final Batch Loss: 1.3195089101791382\n",
      "Subject 7, Epoch 31, Loss: 3.849239230155945, Final Batch Loss: 1.2637715339660645\n",
      "Subject 7, Epoch 32, Loss: 3.821590304374695, Final Batch Loss: 1.281518578529358\n",
      "Subject 7, Epoch 33, Loss: 3.6340748071670532, Final Batch Loss: 1.2055381536483765\n",
      "Subject 7, Epoch 34, Loss: 3.614526867866516, Final Batch Loss: 1.2327901124954224\n",
      "Subject 7, Epoch 35, Loss: 3.612539291381836, Final Batch Loss: 1.1313890218734741\n",
      "Subject 7, Epoch 36, Loss: 3.4532564878463745, Final Batch Loss: 1.1510354280471802\n",
      "Subject 7, Epoch 37, Loss: 3.422847032546997, Final Batch Loss: 1.12724769115448\n",
      "Subject 7, Epoch 38, Loss: 3.436708092689514, Final Batch Loss: 1.139880895614624\n",
      "Subject 7, Epoch 39, Loss: 3.408759355545044, Final Batch Loss: 1.1178585290908813\n",
      "Subject 7, Epoch 40, Loss: 3.291173577308655, Final Batch Loss: 1.0419939756393433\n",
      "Subject 7, Epoch 41, Loss: 3.2595211267471313, Final Batch Loss: 1.058849811553955\n",
      "Subject 7, Epoch 42, Loss: 3.3235960006713867, Final Batch Loss: 1.1146721839904785\n",
      "Subject 7, Epoch 43, Loss: 3.2782537937164307, Final Batch Loss: 1.0720789432525635\n",
      "Subject 7, Epoch 44, Loss: 3.3494248390197754, Final Batch Loss: 1.135343074798584\n",
      "Subject 7, Epoch 45, Loss: 3.2233954668045044, Final Batch Loss: 1.0760610103607178\n",
      "Subject 7, Epoch 46, Loss: 3.159104287624359, Final Batch Loss: 1.1330196857452393\n",
      "Subject 7, Epoch 47, Loss: 3.239079713821411, Final Batch Loss: 1.0226171016693115\n",
      "Subject 7, Epoch 48, Loss: 2.9950583577156067, Final Batch Loss: 0.9774965047836304\n",
      "Subject 7, Epoch 49, Loss: 3.002493679523468, Final Batch Loss: 0.9710416197776794\n",
      "Subject 7, Epoch 50, Loss: 3.0805978775024414, Final Batch Loss: 1.0314183235168457\n",
      "Subject 7, Epoch 51, Loss: 2.98240864276886, Final Batch Loss: 0.9775854349136353\n",
      "Subject 7, Epoch 52, Loss: 3.04023540019989, Final Batch Loss: 0.9461662769317627\n",
      "Subject 7, Epoch 53, Loss: 2.856678545475006, Final Batch Loss: 0.9521247148513794\n",
      "Subject 7, Epoch 54, Loss: 2.824638843536377, Final Batch Loss: 0.9870824217796326\n",
      "Subject 7, Epoch 55, Loss: 2.774042069911957, Final Batch Loss: 0.9718624949455261\n",
      "Subject 7, Epoch 56, Loss: 2.7199965715408325, Final Batch Loss: 0.9178987741470337\n",
      "Subject 7, Epoch 57, Loss: 2.717590034008026, Final Batch Loss: 0.9294520020484924\n",
      "Subject 7, Epoch 58, Loss: 2.6140429377555847, Final Batch Loss: 0.8185909986495972\n",
      "Subject 7, Epoch 59, Loss: 2.674059569835663, Final Batch Loss: 0.8461375832557678\n",
      "Subject 7, Epoch 60, Loss: 2.4850083589553833, Final Batch Loss: 0.7766698598861694\n",
      "Subject 7, Epoch 61, Loss: 2.5526371598243713, Final Batch Loss: 0.7753356099128723\n",
      "Subject 7, Epoch 62, Loss: 2.641929805278778, Final Batch Loss: 0.8712830543518066\n",
      "Subject 7, Epoch 63, Loss: 2.6228721737861633, Final Batch Loss: 0.9157755970954895\n",
      "Subject 7, Epoch 64, Loss: 2.4039875268936157, Final Batch Loss: 0.7122398614883423\n",
      "Subject 7, Epoch 65, Loss: 2.3897138237953186, Final Batch Loss: 0.8870471119880676\n",
      "Subject 7, Epoch 66, Loss: 2.3182421922683716, Final Batch Loss: 0.8034765720367432\n",
      "Subject 7, Epoch 67, Loss: 2.4195427894592285, Final Batch Loss: 0.8191545605659485\n",
      "Subject 7, Epoch 68, Loss: 2.325508773326874, Final Batch Loss: 0.717201828956604\n",
      "Subject 7, Epoch 69, Loss: 2.2342501282691956, Final Batch Loss: 0.7826264500617981\n",
      "Subject 7, Epoch 70, Loss: 2.3399763703346252, Final Batch Loss: 0.7496901154518127\n",
      "Subject 7, Epoch 71, Loss: 2.2808614373207092, Final Batch Loss: 0.940606415271759\n",
      "Subject 7, Epoch 72, Loss: 2.2150455713272095, Final Batch Loss: 0.8161194324493408\n",
      "Subject 7, Epoch 73, Loss: 2.0705354809761047, Final Batch Loss: 0.6829617023468018\n",
      "Subject 7, Epoch 74, Loss: 2.056061565876007, Final Batch Loss: 0.7417008280754089\n",
      "Subject 7, Epoch 75, Loss: 2.081063747406006, Final Batch Loss: 0.6728226542472839\n",
      "Subject 7, Epoch 76, Loss: 2.096829056739807, Final Batch Loss: 0.6893572807312012\n",
      "Subject 7, Epoch 77, Loss: 1.9889296889305115, Final Batch Loss: 0.6531513333320618\n",
      "Subject 7, Epoch 78, Loss: 2.1014103293418884, Final Batch Loss: 0.7935144901275635\n",
      "Subject 7, Epoch 79, Loss: 1.890182077884674, Final Batch Loss: 0.5451880097389221\n",
      "Subject 7, Epoch 80, Loss: 1.998129665851593, Final Batch Loss: 0.7481958270072937\n",
      "Subject 7, Epoch 81, Loss: 1.8603834509849548, Final Batch Loss: 0.6182019710540771\n",
      "Subject 7, Epoch 82, Loss: 1.9350883960723877, Final Batch Loss: 0.6308931112289429\n",
      "Subject 7, Epoch 83, Loss: 1.7604138255119324, Final Batch Loss: 0.5882964134216309\n",
      "Subject 7, Epoch 84, Loss: 1.8447383046150208, Final Batch Loss: 0.610835611820221\n",
      "Subject 7, Epoch 85, Loss: 1.7192692756652832, Final Batch Loss: 0.6009073257446289\n",
      "Subject 7, Epoch 86, Loss: 1.762206792831421, Final Batch Loss: 0.6129857897758484\n",
      "Subject 7, Epoch 87, Loss: 1.7002920508384705, Final Batch Loss: 0.5726134181022644\n",
      "Subject 7, Epoch 88, Loss: 1.7403855323791504, Final Batch Loss: 0.5721902847290039\n",
      "Subject 7, Epoch 89, Loss: 1.568979263305664, Final Batch Loss: 0.5599184036254883\n",
      "Subject 7, Epoch 90, Loss: 1.6187707781791687, Final Batch Loss: 0.5038070678710938\n",
      "Subject 7, Epoch 91, Loss: 1.6420392394065857, Final Batch Loss: 0.5401729941368103\n",
      "Subject 7, Epoch 92, Loss: 1.6273653209209442, Final Batch Loss: 0.4889298975467682\n",
      "Subject 7, Epoch 93, Loss: 1.4834366142749786, Final Batch Loss: 0.4835449457168579\n",
      "Subject 7, Epoch 94, Loss: 1.6329997181892395, Final Batch Loss: 0.5221350789070129\n",
      "Subject 7, Epoch 95, Loss: 1.5530582666397095, Final Batch Loss: 0.6165698170661926\n",
      "Subject 7, Epoch 96, Loss: 1.5020601153373718, Final Batch Loss: 0.547770619392395\n",
      "Subject 7, Epoch 97, Loss: 1.5279150009155273, Final Batch Loss: 0.5237497091293335\n",
      "Subject 7, Epoch 98, Loss: 1.5121920704841614, Final Batch Loss: 0.46999090909957886\n",
      "Subject 7, Epoch 99, Loss: 1.4970621168613434, Final Batch Loss: 0.44457173347473145\n",
      "Subject 7, Epoch 100, Loss: 1.5428967773914337, Final Batch Loss: 0.5250097513198853\n",
      "Subject 7, Epoch 101, Loss: 1.4151234328746796, Final Batch Loss: 0.5495620965957642\n",
      "Subject 7, Epoch 102, Loss: 1.3681881129741669, Final Batch Loss: 0.4705005884170532\n",
      "Subject 7, Epoch 103, Loss: 1.5755197703838348, Final Batch Loss: 0.5950191020965576\n",
      "Subject 7, Epoch 104, Loss: 1.4650827050209045, Final Batch Loss: 0.5203489065170288\n",
      "Subject 7, Epoch 105, Loss: 1.414579838514328, Final Batch Loss: 0.4112508296966553\n",
      "Subject 7, Epoch 106, Loss: 1.355734646320343, Final Batch Loss: 0.4753176271915436\n",
      "Subject 7, Epoch 107, Loss: 1.242853432893753, Final Batch Loss: 0.44928431510925293\n",
      "Subject 7, Epoch 108, Loss: 1.3900867998600006, Final Batch Loss: 0.5092881321907043\n",
      "Subject 7, Epoch 109, Loss: 1.3510138094425201, Final Batch Loss: 0.4542717933654785\n",
      "Subject 7, Epoch 110, Loss: 1.4389536082744598, Final Batch Loss: 0.5285899639129639\n",
      "Subject 7, Epoch 111, Loss: 1.2709476351737976, Final Batch Loss: 0.43882858753204346\n",
      "Subject 7, Epoch 112, Loss: 1.283475011587143, Final Batch Loss: 0.3906792104244232\n",
      "Subject 7, Epoch 113, Loss: 1.2944203913211823, Final Batch Loss: 0.3894284665584564\n",
      "Subject 7, Epoch 114, Loss: 1.3425311148166656, Final Batch Loss: 0.42122209072113037\n",
      "Subject 7, Epoch 115, Loss: 1.330557942390442, Final Batch Loss: 0.4409520924091339\n",
      "Subject 7, Epoch 116, Loss: 1.28884819149971, Final Batch Loss: 0.46770039200782776\n",
      "Subject 7, Epoch 117, Loss: 1.2113477885723114, Final Batch Loss: 0.4268629848957062\n",
      "Subject 7, Epoch 118, Loss: 1.168979525566101, Final Batch Loss: 0.34589266777038574\n",
      "Subject 7, Epoch 119, Loss: 1.2640331387519836, Final Batch Loss: 0.4405636787414551\n",
      "Subject 7, Epoch 120, Loss: 1.1828672289848328, Final Batch Loss: 0.36432141065597534\n",
      "Subject 7, Epoch 121, Loss: 1.0810973346233368, Final Batch Loss: 0.2807086706161499\n",
      "Subject 7, Epoch 122, Loss: 1.381496399641037, Final Batch Loss: 0.5177793502807617\n",
      "Subject 7, Epoch 123, Loss: 1.1728890538215637, Final Batch Loss: 0.4004723131656647\n",
      "Subject 7, Epoch 124, Loss: 1.219980150461197, Final Batch Loss: 0.4516177177429199\n",
      "Subject 7, Epoch 125, Loss: 1.2334104180335999, Final Batch Loss: 0.38698533177375793\n",
      "Subject 7, Epoch 126, Loss: 1.2140198647975922, Final Batch Loss: 0.34049198031425476\n",
      "Subject 7, Epoch 127, Loss: 1.1243746876716614, Final Batch Loss: 0.2711045444011688\n",
      "Subject 7, Epoch 128, Loss: 1.159094661474228, Final Batch Loss: 0.3391589820384979\n",
      "Subject 7, Epoch 129, Loss: 1.2665870487689972, Final Batch Loss: 0.564487636089325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 130, Loss: 1.1947879791259766, Final Batch Loss: 0.3195701837539673\n",
      "Subject 7, Epoch 131, Loss: 1.1619876027107239, Final Batch Loss: 0.45973819494247437\n",
      "Subject 7, Epoch 132, Loss: 1.2518032491207123, Final Batch Loss: 0.36102283000946045\n",
      "Subject 7, Epoch 133, Loss: 1.1321774125099182, Final Batch Loss: 0.4538935720920563\n",
      "Subject 7, Epoch 134, Loss: 1.2590281963348389, Final Batch Loss: 0.4310046136379242\n",
      "Subject 7, Epoch 135, Loss: 1.1808050572872162, Final Batch Loss: 0.38241660594940186\n",
      "Subject 7, Epoch 136, Loss: 1.1743009686470032, Final Batch Loss: 0.46024179458618164\n",
      "Subject 7, Epoch 137, Loss: 1.1697877049446106, Final Batch Loss: 0.3700079917907715\n",
      "Subject 7, Epoch 138, Loss: 1.0635517537593842, Final Batch Loss: 0.3892086446285248\n",
      "Subject 7, Epoch 139, Loss: 1.0804728865623474, Final Batch Loss: 0.37894994020462036\n",
      "Subject 7, Epoch 140, Loss: 1.0319185703992844, Final Batch Loss: 0.23387230932712555\n",
      "Subject 7, Epoch 141, Loss: 1.1486425399780273, Final Batch Loss: 0.34755387902259827\n",
      "Subject 7, Epoch 142, Loss: 1.031221091747284, Final Batch Loss: 0.24357056617736816\n",
      "Subject 7, Epoch 143, Loss: 1.199336290359497, Final Batch Loss: 0.463654488325119\n",
      "Subject 7, Epoch 144, Loss: 1.1721863448619843, Final Batch Loss: 0.3520280420780182\n",
      "Subject 7, Epoch 145, Loss: 1.0189723074436188, Final Batch Loss: 0.3193347752094269\n",
      "Subject 7, Epoch 146, Loss: 1.1379799842834473, Final Batch Loss: 0.43426457047462463\n",
      "Subject 7, Epoch 147, Loss: 1.0751493573188782, Final Batch Loss: 0.31094422936439514\n",
      "Subject 7, Epoch 148, Loss: 1.0429729223251343, Final Batch Loss: 0.3028424382209778\n",
      "Subject 7, Epoch 149, Loss: 0.983085960149765, Final Batch Loss: 0.33523133397102356\n",
      "Subject 7, Epoch 150, Loss: 1.1099006235599518, Final Batch Loss: 0.3704875409603119\n",
      "Subject 7, Epoch 151, Loss: 1.0807927548885345, Final Batch Loss: 0.35864365100860596\n",
      "Subject 7, Epoch 152, Loss: 1.0582968592643738, Final Batch Loss: 0.3163036108016968\n",
      "Subject 7, Epoch 153, Loss: 1.0459968745708466, Final Batch Loss: 0.3517448902130127\n",
      "Subject 7, Epoch 154, Loss: 1.042463093996048, Final Batch Loss: 0.3095391094684601\n",
      "Subject 7, Epoch 155, Loss: 1.0445751547813416, Final Batch Loss: 0.34191620349884033\n",
      "Subject 7, Epoch 156, Loss: 1.1763241589069366, Final Batch Loss: 0.3913174569606781\n",
      "Subject 7, Epoch 157, Loss: 1.0377517342567444, Final Batch Loss: 0.35873866081237793\n",
      "Subject 7, Epoch 158, Loss: 1.0595887005329132, Final Batch Loss: 0.37063324451446533\n",
      "Subject 7, Epoch 159, Loss: 1.033906489610672, Final Batch Loss: 0.3516663610935211\n",
      "Subject 7, Epoch 160, Loss: 1.0184929966926575, Final Batch Loss: 0.3264334201812744\n",
      "Subject 7, Epoch 161, Loss: 1.0204239785671234, Final Batch Loss: 0.2697127163410187\n",
      "Subject 7, Epoch 162, Loss: 1.1223818063735962, Final Batch Loss: 0.41091734170913696\n",
      "Subject 7, Epoch 163, Loss: 1.0608106553554535, Final Batch Loss: 0.3593180179595947\n",
      "Subject 7, Epoch 164, Loss: 0.9885772168636322, Final Batch Loss: 0.3141339421272278\n",
      "Subject 7, Epoch 165, Loss: 1.0092251747846603, Final Batch Loss: 0.2430620938539505\n",
      "Subject 7, Epoch 166, Loss: 1.0014942586421967, Final Batch Loss: 0.3780812621116638\n",
      "Subject 7, Epoch 167, Loss: 0.979546844959259, Final Batch Loss: 0.27487942576408386\n",
      "Subject 7, Epoch 168, Loss: 1.0386511385440826, Final Batch Loss: 0.2898598909378052\n",
      "Subject 7, Epoch 169, Loss: 0.9433482885360718, Final Batch Loss: 0.3137231469154358\n",
      "Subject 7, Epoch 170, Loss: 1.0221438109874725, Final Batch Loss: 0.3025721311569214\n",
      "Subject 7, Epoch 171, Loss: 1.021336168050766, Final Batch Loss: 0.36200255155563354\n",
      "Subject 7, Epoch 172, Loss: 0.9785290062427521, Final Batch Loss: 0.318881630897522\n",
      "Subject 7, Epoch 173, Loss: 1.0017591416835785, Final Batch Loss: 0.3178340494632721\n",
      "Subject 7, Epoch 174, Loss: 0.953711211681366, Final Batch Loss: 0.2802675664424896\n",
      "Subject 7, Epoch 175, Loss: 1.0733656287193298, Final Batch Loss: 0.46958428621292114\n",
      "Subject 7, Epoch 176, Loss: 0.9140996634960175, Final Batch Loss: 0.258099228143692\n",
      "Subject 7, Epoch 177, Loss: 0.9638863205909729, Final Batch Loss: 0.35008108615875244\n",
      "Subject 7, Epoch 178, Loss: 1.0787105858325958, Final Batch Loss: 0.33784815669059753\n",
      "Subject 7, Epoch 179, Loss: 0.9065601527690887, Final Batch Loss: 0.28740882873535156\n",
      "Subject 7, Epoch 180, Loss: 0.9321430325508118, Final Batch Loss: 0.3085933327674866\n",
      "Subject 7, Epoch 181, Loss: 1.0165202021598816, Final Batch Loss: 0.2877197563648224\n",
      "Subject 7, Epoch 182, Loss: 1.0000457465648651, Final Batch Loss: 0.3703400492668152\n",
      "Subject 7, Epoch 183, Loss: 1.0155722200870514, Final Batch Loss: 0.34731781482696533\n",
      "Subject 7, Epoch 184, Loss: 0.9651552140712738, Final Batch Loss: 0.3191811442375183\n",
      "Subject 7, Epoch 185, Loss: 1.0512817203998566, Final Batch Loss: 0.3415270447731018\n",
      "Subject 7, Epoch 186, Loss: 0.969926118850708, Final Batch Loss: 0.30432090163230896\n",
      "Subject 7, Epoch 187, Loss: 1.0373804867267609, Final Batch Loss: 0.3903512954711914\n",
      "Subject 7, Epoch 188, Loss: 0.8735158890485764, Final Batch Loss: 0.22373555600643158\n",
      "Subject 7, Epoch 189, Loss: 1.1728413999080658, Final Batch Loss: 0.5455800294876099\n",
      "Subject 7, Epoch 190, Loss: 0.8997847735881805, Final Batch Loss: 0.3026198446750641\n",
      "Subject 7, Epoch 191, Loss: 0.917155921459198, Final Batch Loss: 0.310019850730896\n",
      "Subject 7, Epoch 192, Loss: 0.964529812335968, Final Batch Loss: 0.31901541352272034\n",
      "Subject 7, Epoch 193, Loss: 1.0192366242408752, Final Batch Loss: 0.28909558057785034\n",
      "Subject 7, Epoch 194, Loss: 0.8685111403465271, Final Batch Loss: 0.2527206540107727\n",
      "Subject 7, Epoch 195, Loss: 0.9817178845405579, Final Batch Loss: 0.3278898596763611\n",
      "Subject 7, Epoch 196, Loss: 0.9183808863162994, Final Batch Loss: 0.2894271910190582\n",
      "Subject 7, Epoch 197, Loss: 0.9472974240779877, Final Batch Loss: 0.4200149476528168\n",
      "Subject 7, Epoch 198, Loss: 1.0045133531093597, Final Batch Loss: 0.41068825125694275\n",
      "Subject 7, Epoch 199, Loss: 1.0071114897727966, Final Batch Loss: 0.3265126347541809\n",
      "Subject 7, Epoch 200, Loss: 0.9590044021606445, Final Batch Loss: 0.28394055366516113\n",
      "Subject 7, Epoch 201, Loss: 1.0731030106544495, Final Batch Loss: 0.41940441727638245\n",
      "Subject 7, Epoch 202, Loss: 1.0132699608802795, Final Batch Loss: 0.3410467803478241\n",
      "Subject 7, Epoch 203, Loss: 0.9382261037826538, Final Batch Loss: 0.3479844331741333\n",
      "Subject 7, Epoch 204, Loss: 0.9619613438844681, Final Batch Loss: 0.23861362040042877\n",
      "Subject 7, Epoch 205, Loss: 0.9528285562992096, Final Batch Loss: 0.29862546920776367\n",
      "Subject 7, Epoch 206, Loss: 0.9588429629802704, Final Batch Loss: 0.37528562545776367\n",
      "Subject 7, Epoch 207, Loss: 1.054030954837799, Final Batch Loss: 0.34635496139526367\n",
      "Subject 7, Epoch 208, Loss: 0.92801234126091, Final Batch Loss: 0.2465238869190216\n",
      "Subject 7, Epoch 209, Loss: 0.9697673916816711, Final Batch Loss: 0.2859199643135071\n",
      "Subject 7, Epoch 210, Loss: 1.011091709136963, Final Batch Loss: 0.38779571652412415\n",
      "Subject 7, Epoch 211, Loss: 0.951095849275589, Final Batch Loss: 0.3279186189174652\n",
      "Subject 7, Epoch 212, Loss: 0.9464509785175323, Final Batch Loss: 0.3597523868083954\n",
      "Subject 7, Epoch 213, Loss: 0.9478984922170639, Final Batch Loss: 0.40779924392700195\n",
      "Subject 7, Epoch 214, Loss: 0.9997412264347076, Final Batch Loss: 0.22509631514549255\n",
      "Subject 7, Epoch 215, Loss: 0.9608193337917328, Final Batch Loss: 0.29789111018180847\n",
      "Subject 7, Epoch 216, Loss: 0.9549890756607056, Final Batch Loss: 0.3326823115348816\n",
      "Subject 7, Epoch 217, Loss: 1.0544521808624268, Final Batch Loss: 0.37644463777542114\n",
      "Subject 7, Epoch 218, Loss: 0.9907577037811279, Final Batch Loss: 0.3949658274650574\n",
      "Subject 7, Epoch 219, Loss: 0.9088359177112579, Final Batch Loss: 0.32870975136756897\n",
      "Subject 7, Epoch 220, Loss: 0.8897847831249237, Final Batch Loss: 0.31077465415000916\n",
      "Subject 7, Epoch 221, Loss: 0.9467165172100067, Final Batch Loss: 0.2602388560771942\n",
      "Subject 7, Epoch 222, Loss: 0.8949570506811142, Final Batch Loss: 0.24274183809757233\n",
      "Subject 7, Epoch 223, Loss: 0.9747134447097778, Final Batch Loss: 0.312361478805542\n",
      "Subject 7, Epoch 224, Loss: 0.9596202075481415, Final Batch Loss: 0.3185904324054718\n",
      "Subject 7, Epoch 225, Loss: 0.9863013327121735, Final Batch Loss: 0.28732946515083313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 226, Loss: 0.9489979445934296, Final Batch Loss: 0.33871912956237793\n",
      "Subject 7, Epoch 227, Loss: 0.9139746427536011, Final Batch Loss: 0.27338457107543945\n",
      "Subject 7, Epoch 228, Loss: 0.9670346081256866, Final Batch Loss: 0.3476443290710449\n",
      "Subject 7, Epoch 229, Loss: 0.9137836992740631, Final Batch Loss: 0.3189634382724762\n",
      "Subject 7, Epoch 230, Loss: 0.931229829788208, Final Batch Loss: 0.28602996468544006\n",
      "Subject 7, Epoch 231, Loss: 0.8717356324195862, Final Batch Loss: 0.3119390308856964\n",
      "Subject 7, Epoch 232, Loss: 0.9478013813495636, Final Batch Loss: 0.2925679087638855\n",
      "Subject 7, Epoch 233, Loss: 0.9525547027587891, Final Batch Loss: 0.34865421056747437\n",
      "Subject 7, Epoch 234, Loss: 0.9660631418228149, Final Batch Loss: 0.37574508786201477\n",
      "Subject 7, Epoch 235, Loss: 0.9195625633001328, Final Batch Loss: 0.35830700397491455\n",
      "Subject 7, Epoch 236, Loss: 0.9081668704748154, Final Batch Loss: 0.3047049641609192\n",
      "Subject 7, Epoch 237, Loss: 0.9279497861862183, Final Batch Loss: 0.3659195005893707\n",
      "Subject 7, Epoch 238, Loss: 1.08864027261734, Final Batch Loss: 0.40532469749450684\n",
      "Subject 7, Epoch 239, Loss: 0.8799801468849182, Final Batch Loss: 0.26678723096847534\n",
      "Subject 7, Epoch 240, Loss: 0.9922709912061691, Final Batch Loss: 0.35785987973213196\n",
      "Subject 7, Epoch 241, Loss: 0.911882221698761, Final Batch Loss: 0.29078373312950134\n",
      "Subject 7, Epoch 242, Loss: 0.8543832153081894, Final Batch Loss: 0.20997749269008636\n",
      "Subject 7, Epoch 243, Loss: 0.9356959909200668, Final Batch Loss: 0.24597172439098358\n",
      "Subject 7, Epoch 244, Loss: 0.9024123549461365, Final Batch Loss: 0.29388052225112915\n",
      "Subject 7, Epoch 245, Loss: 0.8671835511922836, Final Batch Loss: 0.21035318076610565\n",
      "Subject 7, Epoch 246, Loss: 0.9599912762641907, Final Batch Loss: 0.3425469398498535\n",
      "Subject 7, Epoch 247, Loss: 0.7695543766021729, Final Batch Loss: 0.2162153720855713\n",
      "Subject 7, Epoch 248, Loss: 0.8668486773967743, Final Batch Loss: 0.23466524481773376\n",
      "Subject 7, Epoch 249, Loss: 0.9932880997657776, Final Batch Loss: 0.35565945506095886\n",
      "Subject 7, Epoch 250, Loss: 0.9747752994298935, Final Batch Loss: 0.24961881339550018\n",
      "Subject 7, Epoch 251, Loss: 0.9221043288707733, Final Batch Loss: 0.29805582761764526\n",
      "Subject 7, Epoch 252, Loss: 0.9376630485057831, Final Batch Loss: 0.3862684369087219\n",
      "Subject 7, Epoch 253, Loss: 0.9500262141227722, Final Batch Loss: 0.40760597586631775\n",
      "Subject 7, Epoch 254, Loss: 0.8653700351715088, Final Batch Loss: 0.26138049364089966\n",
      "Subject 7, Epoch 255, Loss: 0.8969428539276123, Final Batch Loss: 0.26307299733161926\n",
      "Subject 7, Epoch 256, Loss: 0.8516284823417664, Final Batch Loss: 0.22472770512104034\n",
      "Subject 7, Epoch 257, Loss: 0.8751933872699738, Final Batch Loss: 0.27436143159866333\n",
      "Subject 7, Epoch 258, Loss: 0.9197743237018585, Final Batch Loss: 0.35415616631507874\n",
      "Subject 7, Epoch 259, Loss: 0.8798625767230988, Final Batch Loss: 0.2804291844367981\n",
      "Subject 7, Epoch 260, Loss: 0.9045791327953339, Final Batch Loss: 0.2537413537502289\n",
      "Subject 7, Epoch 261, Loss: 0.9764758944511414, Final Batch Loss: 0.3721037805080414\n",
      "Subject 7, Epoch 262, Loss: 0.8410993367433548, Final Batch Loss: 0.2429407685995102\n",
      "Subject 7, Epoch 263, Loss: 0.879478245973587, Final Batch Loss: 0.2863529324531555\n",
      "Subject 7, Epoch 264, Loss: 1.055332899093628, Final Batch Loss: 0.43437179923057556\n",
      "Subject 7, Epoch 265, Loss: 0.9238387942314148, Final Batch Loss: 0.33645758032798767\n",
      "Subject 7, Epoch 266, Loss: 0.930236428976059, Final Batch Loss: 0.29783573746681213\n",
      "Subject 7, Epoch 267, Loss: 0.944890946149826, Final Batch Loss: 0.3414154052734375\n",
      "Subject 7, Epoch 268, Loss: 0.8981375396251678, Final Batch Loss: 0.30869001150131226\n",
      "Subject 7, Epoch 269, Loss: 0.8549792468547821, Final Batch Loss: 0.2700886130332947\n",
      "Subject 7, Epoch 270, Loss: 0.8265142291784286, Final Batch Loss: 0.41722366213798523\n",
      "Subject 7, Epoch 271, Loss: 0.9258999973535538, Final Batch Loss: 0.3217107355594635\n",
      "Subject 7, Epoch 272, Loss: 0.9062074720859528, Final Batch Loss: 0.3008585274219513\n",
      "Subject 7, Epoch 273, Loss: 0.8620197623968124, Final Batch Loss: 0.1968042403459549\n",
      "Subject 7, Epoch 274, Loss: 0.8693485260009766, Final Batch Loss: 0.3127847909927368\n",
      "Subject 7, Epoch 275, Loss: 0.8965738415718079, Final Batch Loss: 0.2959480583667755\n",
      "Subject 7, Epoch 276, Loss: 0.894698441028595, Final Batch Loss: 0.31256499886512756\n",
      "Subject 7, Epoch 277, Loss: 0.8637978434562683, Final Batch Loss: 0.2983463406562805\n",
      "Subject 7, Epoch 278, Loss: 0.8970068991184235, Final Batch Loss: 0.31686732172966003\n",
      "Subject 7, Epoch 279, Loss: 0.8136623203754425, Final Batch Loss: 0.29118019342422485\n",
      "Subject 7, Epoch 280, Loss: 0.8708318769931793, Final Batch Loss: 0.30526837706565857\n",
      "Subject 7, Epoch 281, Loss: 0.8866383135318756, Final Batch Loss: 0.33326035737991333\n",
      "Subject 7, Epoch 282, Loss: 0.8673054575920105, Final Batch Loss: 0.2961356043815613\n",
      "Subject 7, Epoch 283, Loss: 0.9364550709724426, Final Batch Loss: 0.33706897497177124\n",
      "Subject 7, Epoch 284, Loss: 0.8404484987258911, Final Batch Loss: 0.29224786162376404\n",
      "Subject 7, Epoch 285, Loss: 0.9144407510757446, Final Batch Loss: 0.2427278459072113\n",
      "Subject 7, Epoch 286, Loss: 0.9129248261451721, Final Batch Loss: 0.2818678617477417\n",
      "Subject 7, Epoch 287, Loss: 0.7745805382728577, Final Batch Loss: 0.17119264602661133\n",
      "Subject 7, Epoch 288, Loss: 0.7714926600456238, Final Batch Loss: 0.23169159889221191\n",
      "Subject 7, Epoch 289, Loss: 0.8286243081092834, Final Batch Loss: 0.3014596402645111\n",
      "Subject 7, Epoch 290, Loss: 0.9766975939273834, Final Batch Loss: 0.34542569518089294\n",
      "Subject 7, Epoch 291, Loss: 0.8851718306541443, Final Batch Loss: 0.3212718963623047\n",
      "Subject 7, Epoch 292, Loss: 0.8588543385267258, Final Batch Loss: 0.354483038187027\n",
      "Subject 7, Epoch 293, Loss: 0.8299385756254196, Final Batch Loss: 0.19790033996105194\n",
      "Subject 7, Epoch 294, Loss: 0.8542216867208481, Final Batch Loss: 0.28940528631210327\n",
      "Subject 7, Epoch 295, Loss: 0.9184027910232544, Final Batch Loss: 0.2511509656906128\n",
      "Subject 7, Epoch 296, Loss: 0.8615761697292328, Final Batch Loss: 0.25422990322113037\n",
      "Subject 7, Epoch 297, Loss: 0.7989054769277573, Final Batch Loss: 0.3485592007637024\n",
      "Subject 7, Epoch 298, Loss: 0.8455467224121094, Final Batch Loss: 0.2711034417152405\n",
      "Subject 7, Epoch 299, Loss: 0.8878634870052338, Final Batch Loss: 0.3060799539089203\n",
      "Subject 7, Epoch 300, Loss: 0.8411057591438293, Final Batch Loss: 0.3125248849391937\n",
      "Subject 7, Epoch 301, Loss: 0.9819230735301971, Final Batch Loss: 0.3659598231315613\n",
      "Subject 7, Epoch 302, Loss: 0.8368640244007111, Final Batch Loss: 0.2697487771511078\n",
      "Subject 7, Epoch 303, Loss: 0.8933905214071274, Final Batch Loss: 0.3577709496021271\n",
      "Subject 7, Epoch 304, Loss: 0.8573081642389297, Final Batch Loss: 0.3478662371635437\n",
      "Subject 7, Epoch 305, Loss: 0.8741262406110764, Final Batch Loss: 0.3364449143409729\n",
      "Subject 7, Epoch 306, Loss: 0.8428041636943817, Final Batch Loss: 0.2893981635570526\n",
      "Subject 7, Epoch 307, Loss: 0.7852032780647278, Final Batch Loss: 0.2177041471004486\n",
      "Subject 7, Epoch 308, Loss: 0.8068042695522308, Final Batch Loss: 0.2601585388183594\n",
      "Subject 7, Epoch 309, Loss: 0.8770191073417664, Final Batch Loss: 0.2609734833240509\n",
      "Subject 7, Epoch 310, Loss: 0.8800210505723953, Final Batch Loss: 0.3701574206352234\n",
      "Subject 7, Epoch 311, Loss: 0.8178042471408844, Final Batch Loss: 0.3159278929233551\n",
      "Subject 7, Epoch 312, Loss: 0.8255023658275604, Final Batch Loss: 0.25122585892677307\n",
      "Subject 7, Epoch 313, Loss: 0.7707525044679642, Final Batch Loss: 0.21097932755947113\n",
      "Subject 7, Epoch 314, Loss: 0.855245977640152, Final Batch Loss: 0.30944979190826416\n",
      "Subject 7, Epoch 315, Loss: 0.7997098416090012, Final Batch Loss: 0.24944256246089935\n",
      "Subject 7, Epoch 316, Loss: 0.8115050792694092, Final Batch Loss: 0.24010750651359558\n",
      "Subject 7, Epoch 317, Loss: 0.8642933219671249, Final Batch Loss: 0.24949641525745392\n",
      "Subject 7, Epoch 318, Loss: 0.9067334830760956, Final Batch Loss: 0.3243491053581238\n",
      "Subject 7, Epoch 319, Loss: 0.7470664381980896, Final Batch Loss: 0.2352105677127838\n",
      "Subject 7, Epoch 320, Loss: 0.8295837342739105, Final Batch Loss: 0.25610896944999695\n",
      "Subject 7, Epoch 321, Loss: 0.8739473819732666, Final Batch Loss: 0.2836700975894928\n",
      "Subject 7, Epoch 322, Loss: 0.937932014465332, Final Batch Loss: 0.3398677408695221\n",
      "Subject 7, Epoch 323, Loss: 0.7999346554279327, Final Batch Loss: 0.2899649441242218\n",
      "Subject 7, Epoch 324, Loss: 0.8540024310350418, Final Batch Loss: 0.2888769209384918\n",
      "Subject 7, Epoch 325, Loss: 0.8755272328853607, Final Batch Loss: 0.23173505067825317\n",
      "Subject 7, Epoch 326, Loss: 0.77959044277668, Final Batch Loss: 0.23833058774471283\n",
      "Subject 7, Epoch 327, Loss: 0.8022427707910538, Final Batch Loss: 0.29739075899124146\n",
      "Subject 7, Epoch 328, Loss: 0.908063992857933, Final Batch Loss: 0.3619166910648346\n",
      "Subject 7, Epoch 329, Loss: 0.7370282858610153, Final Batch Loss: 0.2669834792613983\n",
      "Subject 7, Epoch 330, Loss: 0.8892907202243805, Final Batch Loss: 0.2571755349636078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 331, Loss: 0.8042125552892685, Final Batch Loss: 0.23699502646923065\n",
      "Subject 7, Epoch 332, Loss: 0.7125391811132431, Final Batch Loss: 0.2381288707256317\n",
      "Subject 7, Epoch 333, Loss: 0.8480605185031891, Final Batch Loss: 0.2725891172885895\n",
      "Subject 7, Epoch 334, Loss: 0.7325892150402069, Final Batch Loss: 0.2511010766029358\n",
      "Subject 7, Epoch 335, Loss: 0.7719392627477646, Final Batch Loss: 0.24658267199993134\n",
      "Subject 7, Epoch 336, Loss: 0.8325662910938263, Final Batch Loss: 0.22414222359657288\n",
      "Subject 7, Epoch 337, Loss: 0.7845913618803024, Final Batch Loss: 0.28560522198677063\n",
      "Subject 7, Epoch 338, Loss: 0.8467928469181061, Final Batch Loss: 0.2544991970062256\n",
      "Subject 7, Epoch 339, Loss: 0.7915638983249664, Final Batch Loss: 0.31392043828964233\n",
      "Subject 7, Epoch 340, Loss: 0.8141278028488159, Final Batch Loss: 0.2734625041484833\n",
      "Subject 7, Epoch 341, Loss: 0.8174085468053818, Final Batch Loss: 0.22665910422801971\n",
      "Subject 7, Epoch 342, Loss: 0.800674244761467, Final Batch Loss: 0.17445941269397736\n",
      "Subject 7, Epoch 343, Loss: 1.028859704732895, Final Batch Loss: 0.3536266088485718\n",
      "Subject 7, Epoch 344, Loss: 0.8027239888906479, Final Batch Loss: 0.25499385595321655\n",
      "Subject 7, Epoch 345, Loss: 0.8195400089025497, Final Batch Loss: 0.33408311009407043\n",
      "Subject 7, Epoch 346, Loss: 0.8189233094453812, Final Batch Loss: 0.24097202718257904\n",
      "Subject 7, Epoch 347, Loss: 0.8494136780500412, Final Batch Loss: 0.2686861753463745\n",
      "Subject 7, Epoch 348, Loss: 0.7766521275043488, Final Batch Loss: 0.2958473563194275\n",
      "Subject 7, Epoch 349, Loss: 0.8191182464361191, Final Batch Loss: 0.35317736864089966\n",
      "Subject 7, Epoch 350, Loss: 0.7620604783296585, Final Batch Loss: 0.27566394209861755\n",
      "Subject 7, Epoch 351, Loss: 0.7995805144309998, Final Batch Loss: 0.25152021646499634\n",
      "Subject 7, Epoch 352, Loss: 0.8631746470928192, Final Batch Loss: 0.33333656191825867\n",
      "Subject 7, Epoch 353, Loss: 0.8054520487785339, Final Batch Loss: 0.2973730266094208\n",
      "Subject 7, Epoch 354, Loss: 0.773363396525383, Final Batch Loss: 0.2816459834575653\n",
      "Subject 7, Epoch 355, Loss: 0.8052990585565567, Final Batch Loss: 0.2773270905017853\n",
      "Subject 7, Epoch 356, Loss: 0.8031743168830872, Final Batch Loss: 0.3049614131450653\n",
      "Subject 7, Epoch 357, Loss: 0.7997512519359589, Final Batch Loss: 0.24924206733703613\n",
      "Subject 7, Epoch 358, Loss: 0.8083655834197998, Final Batch Loss: 0.3085055947303772\n",
      "Subject 7, Epoch 359, Loss: 0.7636537849903107, Final Batch Loss: 0.31790852546691895\n",
      "Subject 7, Epoch 360, Loss: 0.7960746139287949, Final Batch Loss: 0.2569582760334015\n",
      "Subject 7, Epoch 361, Loss: 0.8083697259426117, Final Batch Loss: 0.2734639346599579\n",
      "Subject 7, Epoch 362, Loss: 0.810367614030838, Final Batch Loss: 0.2859662175178528\n",
      "Subject 7, Epoch 363, Loss: 0.8551464378833771, Final Batch Loss: 0.3095317482948303\n",
      "Subject 7, Epoch 364, Loss: 0.7981017678976059, Final Batch Loss: 0.22707076370716095\n",
      "Subject 7, Epoch 365, Loss: 0.8340024352073669, Final Batch Loss: 0.29093512892723083\n",
      "Subject 7, Epoch 366, Loss: 0.7934387028217316, Final Batch Loss: 0.3142125904560089\n",
      "Subject 7, Epoch 367, Loss: 0.7311384379863739, Final Batch Loss: 0.22588419914245605\n",
      "Subject 7, Epoch 368, Loss: 0.8549254238605499, Final Batch Loss: 0.36369046568870544\n",
      "Subject 7, Epoch 369, Loss: 0.7696355134248734, Final Batch Loss: 0.2013489454984665\n",
      "Subject 7, Epoch 370, Loss: 0.7283743768930435, Final Batch Loss: 0.26929962635040283\n",
      "Subject 7, Epoch 371, Loss: 0.8055584728717804, Final Batch Loss: 0.22456049919128418\n",
      "Subject 7, Epoch 372, Loss: 0.8053871840238571, Final Batch Loss: 0.2573254108428955\n",
      "Subject 7, Epoch 373, Loss: 0.8397025018930435, Final Batch Loss: 0.32228922843933105\n",
      "Subject 7, Epoch 374, Loss: 0.8292325586080551, Final Batch Loss: 0.29278987646102905\n",
      "Subject 7, Epoch 375, Loss: 0.7803754359483719, Final Batch Loss: 0.21750785410404205\n",
      "Subject 7, Epoch 376, Loss: 0.8298252820968628, Final Batch Loss: 0.23453715443611145\n",
      "Subject 7, Epoch 377, Loss: 0.7648117542266846, Final Batch Loss: 0.25900202989578247\n",
      "Subject 7, Epoch 378, Loss: 0.788498044013977, Final Batch Loss: 0.2857815623283386\n",
      "Subject 7, Epoch 379, Loss: 0.7469245344400406, Final Batch Loss: 0.27858850359916687\n",
      "Subject 7, Epoch 380, Loss: 0.8243645578622818, Final Batch Loss: 0.34728625416755676\n",
      "Subject 7, Epoch 381, Loss: 0.7716701179742813, Final Batch Loss: 0.2077593356370926\n",
      "Subject 7, Epoch 382, Loss: 0.7944640666246414, Final Batch Loss: 0.27858978509902954\n",
      "Subject 7, Epoch 383, Loss: 0.7380118072032928, Final Batch Loss: 0.253561794757843\n",
      "Subject 7, Epoch 384, Loss: 0.8024852275848389, Final Batch Loss: 0.30020296573638916\n",
      "Subject 7, Epoch 385, Loss: 0.8719405084848404, Final Batch Loss: 0.36535966396331787\n",
      "Subject 7, Epoch 386, Loss: 0.7149174362421036, Final Batch Loss: 0.21809619665145874\n",
      "Subject 7, Epoch 387, Loss: 0.8077026754617691, Final Batch Loss: 0.300808310508728\n",
      "Subject 7, Epoch 388, Loss: 0.7690374553203583, Final Batch Loss: 0.28677207231521606\n",
      "Subject 7, Epoch 389, Loss: 0.7656543701887131, Final Batch Loss: 0.2307984083890915\n",
      "Subject 7, Epoch 390, Loss: 0.714955061674118, Final Batch Loss: 0.25364333391189575\n",
      "Subject 7, Epoch 391, Loss: 0.7508079707622528, Final Batch Loss: 0.23054122924804688\n",
      "Subject 7, Epoch 392, Loss: 0.8053238540887833, Final Batch Loss: 0.3290424048900604\n",
      "Subject 7, Epoch 393, Loss: 0.8085600435733795, Final Batch Loss: 0.28853315114974976\n",
      "Subject 7, Epoch 394, Loss: 0.7180419415235519, Final Batch Loss: 0.2488500028848648\n",
      "Subject 7, Epoch 395, Loss: 0.7405767142772675, Final Batch Loss: 0.16404291987419128\n",
      "Subject 7, Epoch 396, Loss: 0.8286839425563812, Final Batch Loss: 0.3348892629146576\n",
      "Subject 7, Epoch 397, Loss: 0.7763172686100006, Final Batch Loss: 0.21008935570716858\n",
      "Subject 7, Epoch 398, Loss: 0.6875358074903488, Final Batch Loss: 0.1999455988407135\n",
      "Subject 7, Epoch 399, Loss: 0.6478096693754196, Final Batch Loss: 0.1466270238161087\n",
      "Subject 7, Epoch 400, Loss: 0.7935221344232559, Final Batch Loss: 0.18136565387248993\n",
      "Subject 7, Epoch 401, Loss: 0.7426552027463913, Final Batch Loss: 0.28297606110572815\n",
      "Subject 7, Epoch 402, Loss: 0.7841609418392181, Final Batch Loss: 0.28354838490486145\n",
      "Subject 7, Epoch 403, Loss: 0.7386774122714996, Final Batch Loss: 0.2188914269208908\n",
      "Subject 7, Epoch 404, Loss: 0.743449941277504, Final Batch Loss: 0.23783475160598755\n",
      "Subject 7, Epoch 405, Loss: 0.8517130315303802, Final Batch Loss: 0.3305397927761078\n",
      "Subject 7, Epoch 406, Loss: 0.6763917207717896, Final Batch Loss: 0.21472254395484924\n",
      "Subject 7, Epoch 407, Loss: 0.7344110310077667, Final Batch Loss: 0.26765522360801697\n",
      "Subject 7, Epoch 408, Loss: 0.9062225520610809, Final Batch Loss: 0.3270513713359833\n",
      "Subject 7, Epoch 409, Loss: 0.8586180210113525, Final Batch Loss: 0.2888352870941162\n",
      "Subject 7, Epoch 410, Loss: 0.7300374805927277, Final Batch Loss: 0.2393442690372467\n",
      "Subject 7, Epoch 411, Loss: 0.7170744240283966, Final Batch Loss: 0.23771491646766663\n",
      "Subject 7, Epoch 412, Loss: 0.7094938308000565, Final Batch Loss: 0.2583845853805542\n",
      "Subject 7, Epoch 413, Loss: 0.6966312378644943, Final Batch Loss: 0.2686103284358978\n",
      "Subject 7, Epoch 414, Loss: 0.6712680757045746, Final Batch Loss: 0.20242944359779358\n",
      "Subject 7, Epoch 415, Loss: 0.7931092232465744, Final Batch Loss: 0.27216336131095886\n",
      "Subject 7, Epoch 416, Loss: 0.7386050820350647, Final Batch Loss: 0.2883949875831604\n",
      "Subject 7, Epoch 417, Loss: 0.7877963930368423, Final Batch Loss: 0.33819353580474854\n",
      "Subject 7, Epoch 418, Loss: 0.806204304099083, Final Batch Loss: 0.22501806914806366\n",
      "Subject 7, Epoch 419, Loss: 0.7726554572582245, Final Batch Loss: 0.25603875517845154\n",
      "Subject 7, Epoch 420, Loss: 0.7616824209690094, Final Batch Loss: 0.316030353307724\n",
      "Subject 7, Epoch 421, Loss: 0.7637906521558762, Final Batch Loss: 0.29191043972969055\n",
      "Subject 7, Epoch 422, Loss: 0.6958997547626495, Final Batch Loss: 0.26145806908607483\n",
      "Subject 7, Epoch 423, Loss: 0.7064099609851837, Final Batch Loss: 0.15837307274341583\n",
      "Subject 7, Epoch 424, Loss: 0.6635672599077225, Final Batch Loss: 0.2184513509273529\n",
      "Subject 7, Epoch 425, Loss: 0.7753175795078278, Final Batch Loss: 0.3109529912471771\n",
      "Subject 7, Epoch 426, Loss: 0.7498751580715179, Final Batch Loss: 0.2606666684150696\n",
      "Subject 7, Epoch 427, Loss: 0.7224813103675842, Final Batch Loss: 0.23957541584968567\n",
      "Subject 7, Epoch 428, Loss: 0.747139498591423, Final Batch Loss: 0.2531322240829468\n",
      "Subject 7, Epoch 429, Loss: 0.7716623544692993, Final Batch Loss: 0.3122449815273285\n",
      "Subject 7, Epoch 430, Loss: 0.7437354326248169, Final Batch Loss: 0.20694109797477722\n",
      "Subject 7, Epoch 431, Loss: 0.7125568091869354, Final Batch Loss: 0.25737273693084717\n",
      "Subject 7, Epoch 432, Loss: 0.7121706008911133, Final Batch Loss: 0.2013937383890152\n",
      "Subject 7, Epoch 433, Loss: 0.7630833834409714, Final Batch Loss: 0.1465121954679489\n",
      "Subject 7, Epoch 434, Loss: 0.6393872946500778, Final Batch Loss: 0.20503763854503632\n",
      "Subject 7, Epoch 435, Loss: 0.8182677030563354, Final Batch Loss: 0.30086126923561096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 436, Loss: 0.6887294948101044, Final Batch Loss: 0.18516068160533905\n",
      "Subject 7, Epoch 437, Loss: 0.7078132480382919, Final Batch Loss: 0.15977632999420166\n",
      "Subject 7, Epoch 438, Loss: 0.8249337822198868, Final Batch Loss: 0.2395857721567154\n",
      "Subject 7, Epoch 439, Loss: 0.8476980924606323, Final Batch Loss: 0.3481891453266144\n",
      "Subject 7, Epoch 440, Loss: 0.7052217572927475, Final Batch Loss: 0.21821357309818268\n",
      "Subject 7, Epoch 441, Loss: 0.7946800291538239, Final Batch Loss: 0.2519432008266449\n",
      "Subject 7, Epoch 442, Loss: 0.7135275602340698, Final Batch Loss: 0.21752992272377014\n",
      "Subject 7, Epoch 443, Loss: 0.7109517157077789, Final Batch Loss: 0.23403087258338928\n",
      "Subject 7, Epoch 444, Loss: 0.7390792220830917, Final Batch Loss: 0.27984386682510376\n",
      "Subject 7, Epoch 445, Loss: 0.7263548672199249, Final Batch Loss: 0.23990730941295624\n",
      "Subject 7, Epoch 446, Loss: 0.7368360310792923, Final Batch Loss: 0.2472403198480606\n",
      "Subject 7, Epoch 447, Loss: 0.6623093038797379, Final Batch Loss: 0.22835801541805267\n",
      "Subject 7, Epoch 448, Loss: 0.7111949324607849, Final Batch Loss: 0.23692695796489716\n",
      "Subject 7, Epoch 449, Loss: 0.7412330657243729, Final Batch Loss: 0.27413856983184814\n",
      "Subject 7, Epoch 450, Loss: 0.628063440322876, Final Batch Loss: 0.20740723609924316\n",
      "Subject 7, Epoch 451, Loss: 0.7070711851119995, Final Batch Loss: 0.2807022035121918\n",
      "Subject 7, Epoch 452, Loss: 0.6585393249988556, Final Batch Loss: 0.23888523876667023\n",
      "Subject 7, Epoch 453, Loss: 0.6610109508037567, Final Batch Loss: 0.2356230765581131\n",
      "Subject 7, Epoch 454, Loss: 0.719586580991745, Final Batch Loss: 0.2682056725025177\n",
      "Subject 7, Epoch 455, Loss: 0.6725033819675446, Final Batch Loss: 0.2076287865638733\n",
      "Subject 7, Epoch 456, Loss: 0.6723164469003677, Final Batch Loss: 0.304017573595047\n",
      "Subject 7, Epoch 457, Loss: 0.6883718967437744, Final Batch Loss: 0.1731480211019516\n",
      "Subject 7, Epoch 458, Loss: 0.6926263570785522, Final Batch Loss: 0.28346478939056396\n",
      "Subject 7, Epoch 459, Loss: 0.7227838486433029, Final Batch Loss: 0.2618294954299927\n",
      "Subject 7, Epoch 460, Loss: 0.6803379356861115, Final Batch Loss: 0.24908705055713654\n",
      "Subject 7, Epoch 461, Loss: 0.6927885264158249, Final Batch Loss: 0.20316846668720245\n",
      "Subject 7, Epoch 462, Loss: 0.7112548351287842, Final Batch Loss: 0.24957507848739624\n",
      "Subject 7, Epoch 463, Loss: 0.6547621637582779, Final Batch Loss: 0.17653995752334595\n",
      "Subject 7, Epoch 464, Loss: 0.7621182948350906, Final Batch Loss: 0.3051358163356781\n",
      "Subject 7, Epoch 465, Loss: 0.7594538182020187, Final Batch Loss: 0.2453988790512085\n",
      "Subject 7, Epoch 466, Loss: 0.6983728855848312, Final Batch Loss: 0.14779551327228546\n",
      "Subject 7, Epoch 467, Loss: 0.7029535323381424, Final Batch Loss: 0.3058643043041229\n",
      "Subject 7, Epoch 468, Loss: 0.621975764632225, Final Batch Loss: 0.20312181115150452\n",
      "Subject 7, Epoch 469, Loss: 0.6887570172548294, Final Batch Loss: 0.2601044774055481\n",
      "Subject 7, Epoch 470, Loss: 0.7124911546707153, Final Batch Loss: 0.2544385492801666\n",
      "Subject 7, Epoch 471, Loss: 0.7088582366704941, Final Batch Loss: 0.2625988721847534\n",
      "Subject 7, Epoch 472, Loss: 0.6908231675624847, Final Batch Loss: 0.2092837691307068\n",
      "Subject 7, Epoch 473, Loss: 0.6567771434783936, Final Batch Loss: 0.26547250151634216\n",
      "Subject 7, Epoch 474, Loss: 0.751150444149971, Final Batch Loss: 0.3497699201107025\n",
      "Subject 7, Epoch 475, Loss: 0.6096271276473999, Final Batch Loss: 0.2008814513683319\n",
      "Subject 7, Epoch 476, Loss: 0.612638995051384, Final Batch Loss: 0.17148533463478088\n",
      "Subject 7, Epoch 477, Loss: 0.6088657975196838, Final Batch Loss: 0.19195787608623505\n",
      "Subject 7, Epoch 478, Loss: 0.5880655944347382, Final Batch Loss: 0.14716579020023346\n",
      "Subject 7, Epoch 479, Loss: 0.6990974396467209, Final Batch Loss: 0.24551942944526672\n",
      "Subject 7, Epoch 480, Loss: 0.6459892243146896, Final Batch Loss: 0.1505730003118515\n",
      "Subject 7, Epoch 481, Loss: 0.7031437009572983, Final Batch Loss: 0.3042978346347809\n",
      "Subject 7, Epoch 482, Loss: 0.6607963442802429, Final Batch Loss: 0.17337282001972198\n",
      "Subject 7, Epoch 483, Loss: 0.6227454245090485, Final Batch Loss: 0.2694668769836426\n",
      "Subject 7, Epoch 484, Loss: 0.7657105624675751, Final Batch Loss: 0.30335304141044617\n",
      "Subject 7, Epoch 485, Loss: 0.6922007203102112, Final Batch Loss: 0.278065025806427\n",
      "Subject 7, Epoch 486, Loss: 0.6736943572759628, Final Batch Loss: 0.2247406542301178\n",
      "Subject 7, Epoch 487, Loss: 0.6410008817911148, Final Batch Loss: 0.14895157516002655\n",
      "Subject 7, Epoch 488, Loss: 0.6878156214952469, Final Batch Loss: 0.21663817763328552\n",
      "Subject 7, Epoch 489, Loss: 0.6483040153980255, Final Batch Loss: 0.18167418241500854\n",
      "Subject 7, Epoch 490, Loss: 0.6361707597970963, Final Batch Loss: 0.16860166192054749\n",
      "Subject 7, Epoch 491, Loss: 0.6258193105459213, Final Batch Loss: 0.20725153386592865\n",
      "Subject 7, Epoch 492, Loss: 0.772948682308197, Final Batch Loss: 0.28305256366729736\n",
      "Subject 7, Epoch 493, Loss: 0.6104535311460495, Final Batch Loss: 0.1971130520105362\n",
      "Subject 7, Epoch 494, Loss: 0.7292788475751877, Final Batch Loss: 0.1932312697172165\n",
      "Subject 7, Epoch 495, Loss: 0.6671451926231384, Final Batch Loss: 0.1906130462884903\n",
      "Subject 7, Epoch 496, Loss: 0.6436627805233002, Final Batch Loss: 0.21474216878414154\n",
      "Subject 7, Epoch 497, Loss: 0.6136389076709747, Final Batch Loss: 0.2669788599014282\n",
      "Subject 7, Epoch 498, Loss: 0.6525009721517563, Final Batch Loss: 0.23959407210350037\n",
      "Subject 7, Epoch 499, Loss: 0.621354803442955, Final Batch Loss: 0.16772793233394623\n",
      "Subject 7, Epoch 500, Loss: 0.6007020473480225, Final Batch Loss: 0.1291026622056961\n",
      "Subject 7, Epoch 501, Loss: 0.7049668431282043, Final Batch Loss: 0.17019827663898468\n",
      "Subject 7, Epoch 502, Loss: 0.5384437441825867, Final Batch Loss: 0.20135965943336487\n",
      "Subject 7, Epoch 503, Loss: 0.641826294362545, Final Batch Loss: 0.2102855145931244\n",
      "Subject 7, Epoch 504, Loss: 0.6497337520122528, Final Batch Loss: 0.24647356569766998\n",
      "Subject 7, Epoch 505, Loss: 0.6633856445550919, Final Batch Loss: 0.2020476758480072\n",
      "Subject 7, Epoch 506, Loss: 0.6832821518182755, Final Batch Loss: 0.34999972581863403\n",
      "Subject 7, Epoch 507, Loss: 0.5820335000753403, Final Batch Loss: 0.16603098809719086\n",
      "Subject 7, Epoch 508, Loss: 0.6085313260555267, Final Batch Loss: 0.17687812447547913\n",
      "Subject 7, Epoch 509, Loss: 0.5462986528873444, Final Batch Loss: 0.1831706464290619\n",
      "Subject 7, Epoch 510, Loss: 0.6741367429494858, Final Batch Loss: 0.23483385145664215\n",
      "Subject 7, Epoch 511, Loss: 0.697998121380806, Final Batch Loss: 0.2896595001220703\n",
      "Subject 7, Epoch 512, Loss: 0.7453011274337769, Final Batch Loss: 0.38002637028694153\n",
      "Subject 7, Epoch 513, Loss: 0.7304211258888245, Final Batch Loss: 0.3295535445213318\n",
      "Subject 7, Epoch 514, Loss: 0.7217311710119247, Final Batch Loss: 0.2877644896507263\n",
      "Subject 7, Epoch 515, Loss: 0.7549938857555389, Final Batch Loss: 0.2835072875022888\n",
      "Subject 7, Epoch 516, Loss: 0.6059430316090584, Final Batch Loss: 0.10295598953962326\n",
      "Subject 7, Epoch 517, Loss: 0.7182512730360031, Final Batch Loss: 0.3526363670825958\n",
      "Subject 7, Epoch 518, Loss: 0.6361277550458908, Final Batch Loss: 0.2106197029352188\n",
      "Subject 7, Epoch 519, Loss: 0.5898668766021729, Final Batch Loss: 0.17770875990390778\n",
      "Subject 7, Epoch 520, Loss: 0.6156578809022903, Final Batch Loss: 0.11739619076251984\n",
      "Subject 7, Epoch 521, Loss: 0.5914352089166641, Final Batch Loss: 0.21786142885684967\n",
      "Subject 7, Epoch 522, Loss: 0.6452544778585434, Final Batch Loss: 0.21197354793548584\n",
      "Subject 7, Epoch 523, Loss: 0.6384653300046921, Final Batch Loss: 0.1771489530801773\n",
      "Subject 7, Epoch 524, Loss: 0.6939722895622253, Final Batch Loss: 0.30458393692970276\n",
      "Subject 7, Epoch 525, Loss: 0.5368741154670715, Final Batch Loss: 0.1607358455657959\n",
      "Subject 7, Epoch 526, Loss: 0.5832898169755936, Final Batch Loss: 0.14870886504650116\n",
      "Subject 7, Epoch 527, Loss: 0.5725463628768921, Final Batch Loss: 0.22862084209918976\n",
      "Subject 7, Epoch 528, Loss: 0.6342223137617111, Final Batch Loss: 0.18357066810131073\n",
      "Subject 7, Epoch 529, Loss: 0.6738455593585968, Final Batch Loss: 0.27090075612068176\n",
      "Subject 7, Epoch 530, Loss: 0.5985748469829559, Final Batch Loss: 0.141044020652771\n",
      "Subject 7, Epoch 531, Loss: 0.6404690444469452, Final Batch Loss: 0.234693244099617\n",
      "Subject 7, Epoch 532, Loss: 0.6565499305725098, Final Batch Loss: 0.23434187471866608\n",
      "Subject 7, Epoch 533, Loss: 0.6059056520462036, Final Batch Loss: 0.21854673326015472\n",
      "Subject 7, Epoch 534, Loss: 0.6020639687776566, Final Batch Loss: 0.20413537323474884\n",
      "Subject 7, Epoch 535, Loss: 0.522436112165451, Final Batch Loss: 0.11343632638454437\n",
      "Subject 7, Epoch 536, Loss: 0.5641713291406631, Final Batch Loss: 0.14400270581245422\n",
      "Subject 7, Epoch 537, Loss: 0.6057313531637192, Final Batch Loss: 0.14834657311439514\n",
      "Subject 7, Epoch 538, Loss: 0.7644530683755875, Final Batch Loss: 0.3368380665779114\n",
      "Subject 7, Epoch 539, Loss: 0.6045378297567368, Final Batch Loss: 0.2177492082118988\n",
      "Subject 7, Epoch 540, Loss: 0.6732376217842102, Final Batch Loss: 0.21024559438228607\n",
      "Subject 7, Epoch 541, Loss: 0.6123048514127731, Final Batch Loss: 0.22609449923038483\n",
      "Subject 7, Epoch 542, Loss: 0.5801214426755905, Final Batch Loss: 0.21111662685871124\n",
      "Subject 7, Epoch 543, Loss: 0.6337093412876129, Final Batch Loss: 0.19449515640735626\n",
      "Subject 7, Epoch 544, Loss: 0.5990832448005676, Final Batch Loss: 0.13733164966106415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 545, Loss: 0.5764645338058472, Final Batch Loss: 0.15758363902568817\n",
      "Subject 7, Epoch 546, Loss: 0.6905898749828339, Final Batch Loss: 0.2459051012992859\n",
      "Subject 7, Epoch 547, Loss: 0.5889046341180801, Final Batch Loss: 0.19485101103782654\n",
      "Subject 7, Epoch 548, Loss: 0.6046577990055084, Final Batch Loss: 0.18624848127365112\n",
      "Subject 7, Epoch 549, Loss: 0.5647441744804382, Final Batch Loss: 0.16378307342529297\n",
      "Subject 7, Epoch 550, Loss: 0.6437705159187317, Final Batch Loss: 0.1973600536584854\n",
      "Subject 7, Epoch 551, Loss: 0.5647182166576385, Final Batch Loss: 0.15664324164390564\n",
      "Subject 7, Epoch 552, Loss: 0.6964246928691864, Final Batch Loss: 0.2724596858024597\n",
      "Subject 7, Epoch 553, Loss: 0.6604316830635071, Final Batch Loss: 0.20095676183700562\n",
      "Subject 7, Epoch 554, Loss: 0.6271350681781769, Final Batch Loss: 0.24064958095550537\n",
      "Subject 7, Epoch 555, Loss: 0.5387562066316605, Final Batch Loss: 0.25889840722084045\n",
      "Subject 7, Epoch 556, Loss: 0.5104237794876099, Final Batch Loss: 0.17634491622447968\n",
      "Subject 7, Epoch 557, Loss: 0.5405589938163757, Final Batch Loss: 0.15043658018112183\n",
      "Subject 7, Epoch 558, Loss: 0.6310627311468124, Final Batch Loss: 0.18620841205120087\n",
      "Subject 7, Epoch 559, Loss: 0.6431747078895569, Final Batch Loss: 0.22797775268554688\n",
      "Subject 7, Epoch 560, Loss: 0.6300041228532791, Final Batch Loss: 0.18733377754688263\n",
      "Subject 7, Epoch 561, Loss: 0.5980728566646576, Final Batch Loss: 0.17957419157028198\n",
      "Subject 7, Epoch 562, Loss: 0.5352663695812225, Final Batch Loss: 0.17925171554088593\n",
      "Subject 7, Epoch 563, Loss: 0.5279980599880219, Final Batch Loss: 0.21163801848888397\n",
      "Subject 7, Epoch 564, Loss: 0.6374440044164658, Final Batch Loss: 0.21022380888462067\n",
      "Subject 7, Epoch 565, Loss: 0.5513304024934769, Final Batch Loss: 0.15703655779361725\n",
      "Subject 7, Epoch 566, Loss: 0.5435646176338196, Final Batch Loss: 0.17987775802612305\n",
      "Subject 7, Epoch 567, Loss: 0.5585374236106873, Final Batch Loss: 0.20066118240356445\n",
      "Subject 7, Epoch 568, Loss: 0.5595366805791855, Final Batch Loss: 0.1616167277097702\n",
      "Subject 7, Epoch 569, Loss: 0.5369163900613785, Final Batch Loss: 0.13527753949165344\n",
      "Subject 7, Epoch 570, Loss: 0.5976358503103256, Final Batch Loss: 0.218135267496109\n",
      "Subject 7, Epoch 571, Loss: 0.542970135807991, Final Batch Loss: 0.17477528750896454\n",
      "Subject 7, Epoch 572, Loss: 0.5582011789083481, Final Batch Loss: 0.20562128722667694\n",
      "Subject 7, Epoch 573, Loss: 0.6690177172422409, Final Batch Loss: 0.24214935302734375\n",
      "Subject 7, Epoch 574, Loss: 0.5105531811714172, Final Batch Loss: 0.1626611053943634\n",
      "Subject 7, Epoch 575, Loss: 0.5431686788797379, Final Batch Loss: 0.1404252052307129\n",
      "Subject 7, Epoch 576, Loss: 0.5254563540220261, Final Batch Loss: 0.1568080484867096\n",
      "Subject 7, Epoch 577, Loss: 0.5439277440309525, Final Batch Loss: 0.20518332719802856\n",
      "Subject 7, Epoch 578, Loss: 0.6187105178833008, Final Batch Loss: 0.17170646786689758\n",
      "Subject 7, Epoch 579, Loss: 0.5233265087008476, Final Batch Loss: 0.19952702522277832\n",
      "Subject 7, Epoch 580, Loss: 0.6718981862068176, Final Batch Loss: 0.2175627499818802\n",
      "Subject 7, Epoch 581, Loss: 0.6345870792865753, Final Batch Loss: 0.27994686365127563\n",
      "Subject 7, Epoch 582, Loss: 0.5137170255184174, Final Batch Loss: 0.15136666595935822\n",
      "Subject 7, Epoch 583, Loss: 0.48380032926797867, Final Batch Loss: 0.1079699769616127\n",
      "Subject 7, Epoch 584, Loss: 0.5821805000305176, Final Batch Loss: 0.18419739603996277\n",
      "Subject 7, Epoch 585, Loss: 0.6196414083242416, Final Batch Loss: 0.22582662105560303\n",
      "Subject 7, Epoch 586, Loss: 0.539362221956253, Final Batch Loss: 0.16486287117004395\n",
      "Subject 7, Epoch 587, Loss: 0.5426840335130692, Final Batch Loss: 0.15755045413970947\n",
      "Subject 7, Epoch 588, Loss: 0.6113560348749161, Final Batch Loss: 0.16635316610336304\n",
      "Subject 7, Epoch 589, Loss: 0.526508241891861, Final Batch Loss: 0.17116105556488037\n",
      "Subject 7, Epoch 590, Loss: 0.530949130654335, Final Batch Loss: 0.2386813759803772\n",
      "Subject 7, Epoch 591, Loss: 0.5010580867528915, Final Batch Loss: 0.17019912600517273\n",
      "Subject 7, Epoch 592, Loss: 0.5458928644657135, Final Batch Loss: 0.1653508096933365\n",
      "Subject 7, Epoch 593, Loss: 0.611961841583252, Final Batch Loss: 0.18194009363651276\n",
      "Subject 7, Epoch 594, Loss: 0.5840955525636673, Final Batch Loss: 0.30226433277130127\n",
      "Subject 7, Epoch 595, Loss: 0.5622362643480301, Final Batch Loss: 0.18918925523757935\n",
      "Subject 7, Epoch 596, Loss: 0.5353159382939339, Final Batch Loss: 0.12433420866727829\n",
      "Subject 7, Epoch 597, Loss: 0.5947469174861908, Final Batch Loss: 0.19863219559192657\n",
      "Subject 7, Epoch 598, Loss: 0.661293163895607, Final Batch Loss: 0.29413825273513794\n",
      "Subject 7, Epoch 599, Loss: 0.628031775355339, Final Batch Loss: 0.22160130739212036\n",
      "Subject 7, Epoch 600, Loss: 0.5532432943582535, Final Batch Loss: 0.2164287269115448\n",
      "Subject 7, Epoch 601, Loss: 0.5287137627601624, Final Batch Loss: 0.10456590354442596\n",
      "Subject 7, Epoch 602, Loss: 0.5975136905908585, Final Batch Loss: 0.27821874618530273\n",
      "Subject 7, Epoch 603, Loss: 0.5074420496821404, Final Batch Loss: 0.24312058091163635\n",
      "Subject 7, Epoch 604, Loss: 0.5440093129873276, Final Batch Loss: 0.18136295676231384\n",
      "Subject 7, Epoch 605, Loss: 0.554721012711525, Final Batch Loss: 0.14703470468521118\n",
      "Subject 7, Epoch 606, Loss: 0.6533398330211639, Final Batch Loss: 0.24140587449073792\n",
      "Subject 7, Epoch 607, Loss: 0.508023127913475, Final Batch Loss: 0.17899726331233978\n",
      "Subject 7, Epoch 608, Loss: 0.5642421320080757, Final Batch Loss: 0.09086938947439194\n",
      "Subject 7, Epoch 609, Loss: 0.5365540534257889, Final Batch Loss: 0.15483298897743225\n",
      "Subject 7, Epoch 610, Loss: 0.6117893606424332, Final Batch Loss: 0.2962581217288971\n",
      "Subject 7, Epoch 611, Loss: 0.5630439817905426, Final Batch Loss: 0.14092428982257843\n",
      "Subject 7, Epoch 612, Loss: 0.5956487655639648, Final Batch Loss: 0.20902980864048004\n",
      "Subject 7, Epoch 613, Loss: 0.5610872581601143, Final Batch Loss: 0.28439947962760925\n",
      "Subject 7, Epoch 614, Loss: 0.6289084553718567, Final Batch Loss: 0.21231384575366974\n",
      "Subject 7, Epoch 615, Loss: 0.5592971071600914, Final Batch Loss: 0.11983158439397812\n",
      "Subject 7, Epoch 616, Loss: 0.49420325458049774, Final Batch Loss: 0.15719683468341827\n",
      "Subject 7, Epoch 617, Loss: 0.554079532623291, Final Batch Loss: 0.22800713777542114\n",
      "Subject 7, Epoch 618, Loss: 0.6066465526819229, Final Batch Loss: 0.14572550356388092\n",
      "Subject 7, Epoch 619, Loss: 0.5731952339410782, Final Batch Loss: 0.20920982956886292\n",
      "Subject 7, Epoch 620, Loss: 0.521151065826416, Final Batch Loss: 0.17032261192798615\n",
      "Subject 7, Epoch 621, Loss: 0.5946085751056671, Final Batch Loss: 0.24482020735740662\n",
      "Subject 7, Epoch 622, Loss: 0.5931505411863327, Final Batch Loss: 0.18578962981700897\n",
      "Subject 7, Epoch 623, Loss: 0.5579377561807632, Final Batch Loss: 0.07707850635051727\n",
      "Subject 7, Epoch 624, Loss: 0.5636061131954193, Final Batch Loss: 0.158047154545784\n",
      "Subject 7, Epoch 625, Loss: 0.5363991409540176, Final Batch Loss: 0.20366759598255157\n",
      "Subject 7, Epoch 626, Loss: 0.49775631725788116, Final Batch Loss: 0.19978579878807068\n",
      "Subject 7, Epoch 627, Loss: 0.6303265392780304, Final Batch Loss: 0.19007782638072968\n",
      "Subject 7, Epoch 628, Loss: 0.5464168041944504, Final Batch Loss: 0.21779003739356995\n",
      "Subject 7, Epoch 629, Loss: 0.5421985536813736, Final Batch Loss: 0.14959026873111725\n",
      "Subject 7, Epoch 630, Loss: 0.5908161550760269, Final Batch Loss: 0.14081721007823944\n",
      "Subject 7, Epoch 631, Loss: 0.5746141970157623, Final Batch Loss: 0.18017084896564484\n",
      "Subject 7, Epoch 632, Loss: 0.552126407623291, Final Batch Loss: 0.16621212661266327\n",
      "Subject 7, Epoch 633, Loss: 0.5592775791883469, Final Batch Loss: 0.22004202008247375\n",
      "Subject 7, Epoch 634, Loss: 0.5268600583076477, Final Batch Loss: 0.1623283177614212\n",
      "Subject 7, Epoch 635, Loss: 0.549118235707283, Final Batch Loss: 0.15738919377326965\n",
      "Subject 7, Epoch 636, Loss: 0.5003257542848587, Final Batch Loss: 0.1690225452184677\n",
      "Subject 7, Epoch 637, Loss: 0.486799955368042, Final Batch Loss: 0.13593892753124237\n",
      "Subject 7, Epoch 638, Loss: 0.558983825147152, Final Batch Loss: 0.11167962104082108\n",
      "Subject 7, Epoch 639, Loss: 0.501091405749321, Final Batch Loss: 0.1409783959388733\n",
      "Subject 7, Epoch 640, Loss: 0.5823137760162354, Final Batch Loss: 0.1634419709444046\n",
      "Subject 7, Epoch 641, Loss: 0.4725862741470337, Final Batch Loss: 0.16086916625499725\n",
      "Subject 7, Epoch 642, Loss: 0.5732323974370956, Final Batch Loss: 0.2011866569519043\n",
      "Subject 7, Epoch 643, Loss: 0.44732317328453064, Final Batch Loss: 0.14221444725990295\n",
      "Subject 7, Epoch 644, Loss: 0.6637073457241058, Final Batch Loss: 0.28063857555389404\n",
      "Subject 7, Epoch 645, Loss: 0.49134229123592377, Final Batch Loss: 0.1997784674167633\n",
      "Subject 7, Epoch 646, Loss: 0.5176781564950943, Final Batch Loss: 0.1604537069797516\n",
      "Subject 7, Epoch 647, Loss: 0.5665123611688614, Final Batch Loss: 0.15755176544189453\n",
      "Subject 7, Epoch 648, Loss: 0.5272456854581833, Final Batch Loss: 0.17949700355529785\n",
      "Subject 7, Epoch 649, Loss: 0.49181103706359863, Final Batch Loss: 0.16553208231925964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 650, Loss: 0.5536661297082901, Final Batch Loss: 0.18400071561336517\n",
      "Subject 7, Epoch 651, Loss: 0.520461268723011, Final Batch Loss: 0.2611231803894043\n",
      "Subject 7, Epoch 652, Loss: 0.4962310940027237, Final Batch Loss: 0.1991274654865265\n",
      "Subject 7, Epoch 653, Loss: 0.584112823009491, Final Batch Loss: 0.17505380511283875\n",
      "Subject 7, Epoch 654, Loss: 0.5350253134965897, Final Batch Loss: 0.1920330673456192\n",
      "Subject 7, Epoch 655, Loss: 0.5200653299689293, Final Batch Loss: 0.21743473410606384\n",
      "Subject 7, Epoch 656, Loss: 0.49336713552474976, Final Batch Loss: 0.15215405821800232\n",
      "Subject 7, Epoch 657, Loss: 0.5560272037982941, Final Batch Loss: 0.23883561789989471\n",
      "Subject 7, Epoch 658, Loss: 0.5648673623800278, Final Batch Loss: 0.23523469269275665\n",
      "Subject 7, Epoch 659, Loss: 0.5500340163707733, Final Batch Loss: 0.13651148974895477\n",
      "Subject 7, Epoch 660, Loss: 0.5104202777147293, Final Batch Loss: 0.15913428366184235\n",
      "Subject 7, Epoch 661, Loss: 0.4843848943710327, Final Batch Loss: 0.19216904044151306\n",
      "Subject 7, Epoch 662, Loss: 0.4604048877954483, Final Batch Loss: 0.1534966379404068\n",
      "Subject 7, Epoch 663, Loss: 0.46814532577991486, Final Batch Loss: 0.14532019197940826\n",
      "Subject 7, Epoch 664, Loss: 0.45571406185626984, Final Batch Loss: 0.15810877084732056\n",
      "Subject 7, Epoch 665, Loss: 0.48243872821331024, Final Batch Loss: 0.10608184337615967\n",
      "Subject 7, Epoch 666, Loss: 0.5217739120125771, Final Batch Loss: 0.1998790204524994\n",
      "Subject 7, Epoch 667, Loss: 0.48734524846076965, Final Batch Loss: 0.14791038632392883\n",
      "Subject 7, Epoch 668, Loss: 0.5806794613599777, Final Batch Loss: 0.16527777910232544\n",
      "Subject 7, Epoch 669, Loss: 0.4407655820250511, Final Batch Loss: 0.13226507604122162\n",
      "Subject 7, Epoch 670, Loss: 0.5091727077960968, Final Batch Loss: 0.13492394983768463\n",
      "Subject 7, Epoch 671, Loss: 0.4962523430585861, Final Batch Loss: 0.15036281943321228\n",
      "Subject 7, Epoch 672, Loss: 0.48504452407360077, Final Batch Loss: 0.19524483382701874\n",
      "Subject 7, Epoch 673, Loss: 0.5250968337059021, Final Batch Loss: 0.15803837776184082\n",
      "Subject 7, Epoch 674, Loss: 0.5565976500511169, Final Batch Loss: 0.148098424077034\n",
      "Subject 7, Epoch 675, Loss: 0.7547239661216736, Final Batch Loss: 0.21919187903404236\n",
      "Subject 7, Epoch 676, Loss: 0.4820757955312729, Final Batch Loss: 0.18771082162857056\n",
      "Subject 7, Epoch 677, Loss: 0.5012933164834976, Final Batch Loss: 0.1703563779592514\n",
      "Subject 7, Epoch 678, Loss: 0.48654135316610336, Final Batch Loss: 0.10757394880056381\n",
      "Subject 7, Epoch 679, Loss: 0.5156689286231995, Final Batch Loss: 0.1718478500843048\n",
      "Subject 7, Epoch 680, Loss: 0.5451759397983551, Final Batch Loss: 0.22355984151363373\n",
      "Subject 7, Epoch 681, Loss: 0.5454476624727249, Final Batch Loss: 0.25149786472320557\n",
      "Subject 7, Epoch 682, Loss: 0.528950572013855, Final Batch Loss: 0.23692050576210022\n",
      "Subject 7, Epoch 683, Loss: 0.5644804984331131, Final Batch Loss: 0.22876612842082977\n",
      "Subject 7, Epoch 684, Loss: 0.5193217247724533, Final Batch Loss: 0.20146122574806213\n",
      "Subject 7, Epoch 685, Loss: 0.4825323671102524, Final Batch Loss: 0.15099014341831207\n",
      "Subject 7, Epoch 686, Loss: 0.5098804384469986, Final Batch Loss: 0.12624092400074005\n",
      "Subject 7, Epoch 687, Loss: 0.4462007135152817, Final Batch Loss: 0.1723230928182602\n",
      "Subject 7, Epoch 688, Loss: 0.5162383019924164, Final Batch Loss: 0.12240065634250641\n",
      "Subject 7, Epoch 689, Loss: 0.5606353431940079, Final Batch Loss: 0.23401951789855957\n",
      "Subject 7, Epoch 690, Loss: 0.5261225253343582, Final Batch Loss: 0.15059037506580353\n",
      "Subject 7, Epoch 691, Loss: 0.4754646420478821, Final Batch Loss: 0.13732290267944336\n",
      "Subject 7, Epoch 692, Loss: 0.5499616116285324, Final Batch Loss: 0.20140042901039124\n",
      "Subject 7, Epoch 693, Loss: 0.44907196611166, Final Batch Loss: 0.15855464339256287\n",
      "Subject 7, Epoch 694, Loss: 0.5371641516685486, Final Batch Loss: 0.1765487790107727\n",
      "Subject 7, Epoch 695, Loss: 0.46519115567207336, Final Batch Loss: 0.12088373303413391\n",
      "Subject 7, Epoch 696, Loss: 0.45925428718328476, Final Batch Loss: 0.2102978527545929\n",
      "Subject 7, Epoch 697, Loss: 0.5385812222957611, Final Batch Loss: 0.1742377132177353\n",
      "Subject 7, Epoch 698, Loss: 0.46530604362487793, Final Batch Loss: 0.11396856606006622\n",
      "Subject 7, Epoch 699, Loss: 0.49159132689237595, Final Batch Loss: 0.20243310928344727\n",
      "Subject 7, Epoch 700, Loss: 0.45793338119983673, Final Batch Loss: 0.15986883640289307\n",
      "Subject 7, Epoch 701, Loss: 0.413643516600132, Final Batch Loss: 0.09595755487680435\n",
      "Subject 7, Epoch 702, Loss: 0.5473170876502991, Final Batch Loss: 0.24453936517238617\n",
      "Subject 7, Epoch 703, Loss: 0.552422821521759, Final Batch Loss: 0.2531304359436035\n",
      "Subject 7, Epoch 704, Loss: 0.5907790809869766, Final Batch Loss: 0.18083563446998596\n",
      "Subject 7, Epoch 705, Loss: 0.48451487720012665, Final Batch Loss: 0.16145609319210052\n",
      "Subject 7, Epoch 706, Loss: 0.5558779686689377, Final Batch Loss: 0.18113373219966888\n",
      "Subject 7, Epoch 707, Loss: 0.4372170940041542, Final Batch Loss: 0.10674764961004257\n",
      "Subject 7, Epoch 708, Loss: 0.47122523188591003, Final Batch Loss: 0.18149334192276\n",
      "Subject 7, Epoch 709, Loss: 0.46548061072826385, Final Batch Loss: 0.1750660538673401\n",
      "Subject 7, Epoch 710, Loss: 0.4467134177684784, Final Batch Loss: 0.14668874442577362\n",
      "Subject 7, Epoch 711, Loss: 0.47351010143756866, Final Batch Loss: 0.15853798389434814\n",
      "Subject 7, Epoch 712, Loss: 0.4854852631688118, Final Batch Loss: 0.17357222735881805\n",
      "Subject 7, Epoch 713, Loss: 0.5413849726319313, Final Batch Loss: 0.08013270050287247\n",
      "Subject 7, Epoch 714, Loss: 0.5544669032096863, Final Batch Loss: 0.14872683584690094\n",
      "Subject 7, Epoch 715, Loss: 0.48708967864513397, Final Batch Loss: 0.1852640062570572\n",
      "Subject 7, Epoch 716, Loss: 0.41901808232069016, Final Batch Loss: 0.14413148164749146\n",
      "Subject 7, Epoch 717, Loss: 0.40236037969589233, Final Batch Loss: 0.07550327479839325\n",
      "Subject 7, Epoch 718, Loss: 0.5051629096269608, Final Batch Loss: 0.16794975101947784\n",
      "Subject 7, Epoch 719, Loss: 0.40800929069519043, Final Batch Loss: 0.11656095832586288\n",
      "Subject 7, Epoch 720, Loss: 0.5782143175601959, Final Batch Loss: 0.11393670737743378\n",
      "Subject 7, Epoch 721, Loss: 0.46345728635787964, Final Batch Loss: 0.15360160171985626\n",
      "Subject 7, Epoch 722, Loss: 0.46006911247968674, Final Batch Loss: 0.15079909563064575\n",
      "Subject 7, Epoch 723, Loss: 0.4270673841238022, Final Batch Loss: 0.12283171713352203\n",
      "Subject 7, Epoch 724, Loss: 0.48109574615955353, Final Batch Loss: 0.1735900640487671\n",
      "Subject 7, Epoch 725, Loss: 0.553033210337162, Final Batch Loss: 0.2377198487520218\n",
      "Subject 7, Epoch 726, Loss: 0.4449084848165512, Final Batch Loss: 0.15763375163078308\n",
      "Subject 7, Epoch 727, Loss: 0.442123606801033, Final Batch Loss: 0.10827246308326721\n",
      "Subject 7, Epoch 728, Loss: 0.48720867931842804, Final Batch Loss: 0.1064121425151825\n",
      "Subject 7, Epoch 729, Loss: 0.4719543606042862, Final Batch Loss: 0.17370548844337463\n",
      "Subject 7, Epoch 730, Loss: 0.4465433210134506, Final Batch Loss: 0.09866997599601746\n",
      "Subject 7, Epoch 731, Loss: 0.5647277384996414, Final Batch Loss: 0.20722533762454987\n",
      "Subject 7, Epoch 732, Loss: 0.5287824869155884, Final Batch Loss: 0.21962638199329376\n",
      "Subject 7, Epoch 733, Loss: 0.4370178282260895, Final Batch Loss: 0.1312732994556427\n",
      "Subject 7, Epoch 734, Loss: 0.5474789291620255, Final Batch Loss: 0.26564088463783264\n",
      "Subject 7, Epoch 735, Loss: 0.48225824534893036, Final Batch Loss: 0.17392145097255707\n",
      "Subject 7, Epoch 736, Loss: 0.4980006515979767, Final Batch Loss: 0.09324109554290771\n",
      "Subject 7, Epoch 737, Loss: 0.4360319823026657, Final Batch Loss: 0.14229831099510193\n",
      "Subject 7, Epoch 738, Loss: 0.4603768587112427, Final Batch Loss: 0.13602840900421143\n",
      "Subject 7, Epoch 739, Loss: 0.4814091697335243, Final Batch Loss: 0.11429240554571152\n",
      "Subject 7, Epoch 740, Loss: 0.569844976067543, Final Batch Loss: 0.21899022161960602\n",
      "Subject 7, Epoch 741, Loss: 0.4785149097442627, Final Batch Loss: 0.12405310571193695\n",
      "Subject 7, Epoch 742, Loss: 0.48042232543230057, Final Batch Loss: 0.16378560662269592\n",
      "Subject 7, Epoch 743, Loss: 0.47506700456142426, Final Batch Loss: 0.08922088146209717\n",
      "Subject 7, Epoch 744, Loss: 0.49392513930797577, Final Batch Loss: 0.20485569536685944\n",
      "Subject 7, Epoch 745, Loss: 0.4027899503707886, Final Batch Loss: 0.13283084332942963\n",
      "Subject 7, Epoch 746, Loss: 0.5763920843601227, Final Batch Loss: 0.1201440691947937\n",
      "Subject 7, Epoch 747, Loss: 0.594581350684166, Final Batch Loss: 0.2473643273115158\n",
      "Subject 7, Epoch 748, Loss: 0.5106073468923569, Final Batch Loss: 0.1903628408908844\n",
      "Subject 7, Epoch 749, Loss: 0.3628191128373146, Final Batch Loss: 0.10500936955213547\n",
      "Subject 7, Epoch 750, Loss: 0.48963092267513275, Final Batch Loss: 0.1595771312713623\n",
      "Subject 7, Epoch 751, Loss: 0.5209729820489883, Final Batch Loss: 0.13443520665168762\n",
      "Subject 7, Epoch 752, Loss: 0.49753274768590927, Final Batch Loss: 0.21547338366508484\n",
      "Subject 7, Epoch 753, Loss: 0.5261723101139069, Final Batch Loss: 0.11985796689987183\n",
      "Subject 7, Epoch 754, Loss: 0.4756520614027977, Final Batch Loss: 0.10308032482862473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 755, Loss: 0.4396737590432167, Final Batch Loss: 0.08640911430120468\n",
      "Subject 7, Epoch 756, Loss: 0.5025355294346809, Final Batch Loss: 0.21514764428138733\n",
      "Subject 7, Epoch 757, Loss: 0.4049447625875473, Final Batch Loss: 0.1580173224210739\n",
      "Subject 7, Epoch 758, Loss: 0.5212596505880356, Final Batch Loss: 0.24548693001270294\n",
      "Subject 7, Epoch 759, Loss: 0.3924209102988243, Final Batch Loss: 0.07916805893182755\n",
      "Subject 7, Epoch 760, Loss: 0.5516482442617416, Final Batch Loss: 0.2692084014415741\n",
      "Subject 7, Epoch 761, Loss: 0.5727206021547318, Final Batch Loss: 0.19533269107341766\n",
      "Subject 7, Epoch 762, Loss: 0.43864624947309494, Final Batch Loss: 0.16749058663845062\n",
      "Subject 7, Epoch 763, Loss: 0.4956650733947754, Final Batch Loss: 0.11160142719745636\n",
      "Subject 7, Epoch 764, Loss: 0.5287483483552933, Final Batch Loss: 0.16935788094997406\n",
      "Subject 7, Epoch 765, Loss: 0.569633886218071, Final Batch Loss: 0.25551339983940125\n",
      "Subject 7, Epoch 766, Loss: 0.4825821593403816, Final Batch Loss: 0.15458336472511292\n",
      "Subject 7, Epoch 767, Loss: 0.4619298577308655, Final Batch Loss: 0.16572760045528412\n",
      "Subject 7, Epoch 768, Loss: 0.4592058062553406, Final Batch Loss: 0.1270250380039215\n",
      "Subject 7, Epoch 769, Loss: 0.4203043356537819, Final Batch Loss: 0.18325230479240417\n",
      "Subject 7, Epoch 770, Loss: 0.5437287464737892, Final Batch Loss: 0.26603394746780396\n",
      "Subject 7, Epoch 771, Loss: 0.5048508793115616, Final Batch Loss: 0.21312326192855835\n",
      "Subject 7, Epoch 772, Loss: 0.5996319651603699, Final Batch Loss: 0.13636548817157745\n",
      "Subject 7, Epoch 773, Loss: 0.44071516394615173, Final Batch Loss: 0.1566108763217926\n",
      "Subject 7, Epoch 774, Loss: 0.37993352860212326, Final Batch Loss: 0.12395911663770676\n",
      "Subject 7, Epoch 775, Loss: 0.43347786366939545, Final Batch Loss: 0.18462194502353668\n",
      "Subject 7, Epoch 776, Loss: 0.45332758873701096, Final Batch Loss: 0.19667752087116241\n",
      "Subject 7, Epoch 777, Loss: 0.4784286767244339, Final Batch Loss: 0.11528313159942627\n",
      "Subject 7, Epoch 778, Loss: 0.496137335896492, Final Batch Loss: 0.1589694768190384\n",
      "Subject 7, Epoch 779, Loss: 0.371257483959198, Final Batch Loss: 0.1075800210237503\n",
      "Subject 7, Epoch 780, Loss: 0.4214118793606758, Final Batch Loss: 0.11668441444635391\n",
      "Subject 7, Epoch 781, Loss: 0.43362319469451904, Final Batch Loss: 0.16628298163414001\n",
      "Subject 7, Epoch 782, Loss: 0.4809093475341797, Final Batch Loss: 0.16171804070472717\n",
      "Subject 7, Epoch 783, Loss: 0.4088849574327469, Final Batch Loss: 0.10432370752096176\n",
      "Subject 7, Epoch 784, Loss: 0.5256693959236145, Final Batch Loss: 0.2454710453748703\n",
      "Subject 7, Epoch 785, Loss: 0.49030880630016327, Final Batch Loss: 0.16827501356601715\n",
      "Subject 7, Epoch 786, Loss: 0.4673498719930649, Final Batch Loss: 0.17231421172618866\n",
      "Subject 7, Epoch 787, Loss: 0.4245305061340332, Final Batch Loss: 0.0936780497431755\n",
      "Subject 7, Epoch 788, Loss: 0.4753863885998726, Final Batch Loss: 0.1942375898361206\n",
      "Subject 7, Epoch 789, Loss: 0.4839321970939636, Final Batch Loss: 0.1285310834646225\n",
      "Subject 7, Epoch 790, Loss: 0.5127060115337372, Final Batch Loss: 0.11475792527198792\n",
      "Subject 7, Epoch 791, Loss: 0.48431694507598877, Final Batch Loss: 0.171676367521286\n",
      "Subject 7, Epoch 792, Loss: 0.47648952156305313, Final Batch Loss: 0.11352694779634476\n",
      "Subject 7, Epoch 793, Loss: 0.4218398556113243, Final Batch Loss: 0.178117036819458\n",
      "Subject 7, Epoch 794, Loss: 0.4199628233909607, Final Batch Loss: 0.1382584124803543\n",
      "Subject 7, Epoch 795, Loss: 0.4103787690401077, Final Batch Loss: 0.10686732828617096\n",
      "Subject 7, Epoch 796, Loss: 0.49767299741506577, Final Batch Loss: 0.16499461233615875\n",
      "Subject 7, Epoch 797, Loss: 0.49600064754486084, Final Batch Loss: 0.16597974300384521\n",
      "Subject 7, Epoch 798, Loss: 0.394657164812088, Final Batch Loss: 0.12544308602809906\n",
      "Subject 7, Epoch 799, Loss: 0.5048098638653755, Final Batch Loss: 0.22552956640720367\n",
      "Subject 7, Epoch 800, Loss: 0.4648209735751152, Final Batch Loss: 0.18240481615066528\n",
      "Subject 7, Epoch 801, Loss: 0.5776688158512115, Final Batch Loss: 0.25479552149772644\n",
      "Subject 7, Epoch 802, Loss: 0.5179504901170731, Final Batch Loss: 0.19868004322052002\n",
      "Subject 7, Epoch 803, Loss: 0.4467473775148392, Final Batch Loss: 0.15699449181556702\n",
      "Subject 7, Epoch 804, Loss: 0.4284067079424858, Final Batch Loss: 0.15166234970092773\n",
      "Subject 7, Epoch 805, Loss: 0.44834092259407043, Final Batch Loss: 0.1630045473575592\n",
      "Subject 7, Epoch 806, Loss: 0.4760018140077591, Final Batch Loss: 0.09866127371788025\n",
      "Subject 7, Epoch 807, Loss: 0.5967819541692734, Final Batch Loss: 0.20920370519161224\n",
      "Subject 7, Epoch 808, Loss: 0.5004731118679047, Final Batch Loss: 0.11193038523197174\n",
      "Subject 7, Epoch 809, Loss: 0.5245828032493591, Final Batch Loss: 0.2244197279214859\n",
      "Subject 7, Epoch 810, Loss: 0.4772489219903946, Final Batch Loss: 0.13416437804698944\n",
      "Subject 7, Epoch 811, Loss: 0.28521641343832016, Final Batch Loss: 0.07978890836238861\n",
      "Subject 7, Epoch 812, Loss: 0.43943294137716293, Final Batch Loss: 0.17499467730522156\n",
      "Subject 7, Epoch 813, Loss: 0.46744494140148163, Final Batch Loss: 0.1406022161245346\n",
      "Subject 7, Epoch 814, Loss: 0.42187848687171936, Final Batch Loss: 0.15945427119731903\n",
      "Subject 7, Epoch 815, Loss: 0.6617389023303986, Final Batch Loss: 0.3709973990917206\n",
      "Subject 7, Epoch 816, Loss: 0.4878065213561058, Final Batch Loss: 0.20860983431339264\n",
      "Subject 7, Epoch 817, Loss: 0.46982621401548386, Final Batch Loss: 0.20471303164958954\n",
      "Subject 7, Epoch 818, Loss: 0.41177669167518616, Final Batch Loss: 0.16481566429138184\n",
      "Subject 7, Epoch 819, Loss: 0.4811610281467438, Final Batch Loss: 0.24835769832134247\n",
      "Subject 7, Epoch 820, Loss: 0.3573859930038452, Final Batch Loss: 0.10066786408424377\n",
      "Subject 7, Epoch 821, Loss: 0.3876829966902733, Final Batch Loss: 0.1000625416636467\n",
      "Subject 7, Epoch 822, Loss: 0.5103434920310974, Final Batch Loss: 0.1860005259513855\n",
      "Subject 7, Epoch 823, Loss: 0.4442035108804703, Final Batch Loss: 0.1194710060954094\n",
      "Subject 7, Epoch 824, Loss: 0.4086849242448807, Final Batch Loss: 0.1500532478094101\n",
      "Subject 7, Epoch 825, Loss: 0.48737093806266785, Final Batch Loss: 0.16842390596866608\n",
      "Subject 7, Epoch 826, Loss: 0.49184902012348175, Final Batch Loss: 0.1641010344028473\n",
      "Subject 7, Epoch 827, Loss: 0.40960874408483505, Final Batch Loss: 0.1796298772096634\n",
      "Subject 7, Epoch 828, Loss: 0.4117559418082237, Final Batch Loss: 0.17312799394130707\n",
      "Subject 7, Epoch 829, Loss: 0.43066179007291794, Final Batch Loss: 0.18111862242221832\n",
      "Subject 7, Epoch 830, Loss: 0.42922934144735336, Final Batch Loss: 0.15102267265319824\n",
      "Subject 7, Epoch 831, Loss: 0.43233950436115265, Final Batch Loss: 0.19679301977157593\n",
      "Subject 7, Epoch 832, Loss: 0.3431454673409462, Final Batch Loss: 0.08387793600559235\n",
      "Subject 7, Epoch 833, Loss: 0.4115500897169113, Final Batch Loss: 0.1577017605304718\n",
      "Subject 7, Epoch 834, Loss: 0.4347798377275467, Final Batch Loss: 0.15397025644779205\n",
      "Subject 7, Epoch 835, Loss: 0.40585921704769135, Final Batch Loss: 0.1483435481786728\n",
      "Subject 7, Epoch 836, Loss: 0.4697565883398056, Final Batch Loss: 0.17167586088180542\n",
      "Subject 7, Epoch 837, Loss: 0.44398439675569534, Final Batch Loss: 0.20000632107257843\n",
      "Subject 7, Epoch 838, Loss: 0.4669986963272095, Final Batch Loss: 0.18669261038303375\n",
      "Subject 7, Epoch 839, Loss: 0.3809592202305794, Final Batch Loss: 0.1017831489443779\n",
      "Subject 7, Epoch 840, Loss: 0.3863772600889206, Final Batch Loss: 0.11025702953338623\n",
      "Subject 7, Epoch 841, Loss: 0.3938389867544174, Final Batch Loss: 0.1353711634874344\n",
      "Subject 7, Epoch 842, Loss: 0.42989372462034225, Final Batch Loss: 0.13947787880897522\n",
      "Subject 7, Epoch 843, Loss: 0.4637908488512039, Final Batch Loss: 0.13228823244571686\n",
      "Subject 7, Epoch 844, Loss: 0.44600294530391693, Final Batch Loss: 0.08896804600954056\n",
      "Subject 7, Epoch 845, Loss: 0.4690600782632828, Final Batch Loss: 0.1008538156747818\n",
      "Subject 7, Epoch 846, Loss: 0.3460563123226166, Final Batch Loss: 0.12051399052143097\n",
      "Subject 7, Epoch 847, Loss: 0.41728660464286804, Final Batch Loss: 0.18606261909008026\n",
      "Subject 7, Epoch 848, Loss: 0.4111945629119873, Final Batch Loss: 0.13807016611099243\n",
      "Subject 7, Epoch 849, Loss: 0.4047651141881943, Final Batch Loss: 0.1521994173526764\n",
      "Subject 7, Epoch 850, Loss: 0.45813028514385223, Final Batch Loss: 0.1721511334180832\n",
      "Subject 7, Epoch 851, Loss: 0.49239426106214523, Final Batch Loss: 0.19631677865982056\n",
      "Subject 7, Epoch 852, Loss: 0.4772026687860489, Final Batch Loss: 0.20608390867710114\n",
      "Subject 7, Epoch 853, Loss: 0.46268972009420395, Final Batch Loss: 0.2300129234790802\n",
      "Subject 7, Epoch 854, Loss: 0.3669421076774597, Final Batch Loss: 0.08295942842960358\n",
      "Subject 7, Epoch 855, Loss: 0.3410944268107414, Final Batch Loss: 0.05688319355249405\n",
      "Subject 7, Epoch 856, Loss: 0.39836958795785904, Final Batch Loss: 0.15845726430416107\n",
      "Subject 7, Epoch 857, Loss: 0.34045586735010147, Final Batch Loss: 0.1706027239561081\n",
      "Subject 7, Epoch 858, Loss: 0.4193530008196831, Final Batch Loss: 0.1704353541135788\n",
      "Subject 7, Epoch 859, Loss: 0.5212844908237457, Final Batch Loss: 0.10865239799022675\n",
      "Subject 7, Epoch 860, Loss: 0.4808649569749832, Final Batch Loss: 0.13374164700508118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 861, Loss: 0.429848775267601, Final Batch Loss: 0.12367647886276245\n",
      "Subject 7, Epoch 862, Loss: 0.4319763481616974, Final Batch Loss: 0.16086941957473755\n",
      "Subject 7, Epoch 863, Loss: 0.464057058095932, Final Batch Loss: 0.2042679488658905\n",
      "Subject 7, Epoch 864, Loss: 0.38376878947019577, Final Batch Loss: 0.1347821205854416\n",
      "Subject 7, Epoch 865, Loss: 0.42128583788871765, Final Batch Loss: 0.1354592740535736\n",
      "Subject 7, Epoch 866, Loss: 0.47538039088249207, Final Batch Loss: 0.215305894613266\n",
      "Subject 7, Epoch 867, Loss: 0.424348346889019, Final Batch Loss: 0.09453123062849045\n",
      "Subject 7, Epoch 868, Loss: 0.4876454994082451, Final Batch Loss: 0.18087437748908997\n",
      "Subject 7, Epoch 869, Loss: 0.4440900981426239, Final Batch Loss: 0.16147853434085846\n",
      "Subject 7, Epoch 870, Loss: 0.41570326685905457, Final Batch Loss: 0.06681567430496216\n",
      "Subject 7, Epoch 871, Loss: 0.39406708627939224, Final Batch Loss: 0.14395643770694733\n",
      "Subject 7, Epoch 872, Loss: 0.4336906224489212, Final Batch Loss: 0.14755460619926453\n",
      "Subject 7, Epoch 873, Loss: 0.45562484860420227, Final Batch Loss: 0.2002325803041458\n",
      "Subject 7, Epoch 874, Loss: 0.541870579123497, Final Batch Loss: 0.11305762827396393\n",
      "Subject 7, Epoch 875, Loss: 0.38382238149642944, Final Batch Loss: 0.1399264633655548\n",
      "Subject 7, Epoch 876, Loss: 0.4295244663953781, Final Batch Loss: 0.1757001429796219\n",
      "Subject 7, Epoch 877, Loss: 0.4745532497763634, Final Batch Loss: 0.20168772339820862\n",
      "Subject 7, Epoch 878, Loss: 0.45244620740413666, Final Batch Loss: 0.1560029685497284\n",
      "Subject 7, Epoch 879, Loss: 0.5385694652795792, Final Batch Loss: 0.20960982143878937\n",
      "Subject 7, Epoch 880, Loss: 0.52006034553051, Final Batch Loss: 0.17042842507362366\n",
      "Subject 7, Epoch 881, Loss: 0.3593924045562744, Final Batch Loss: 0.07617321610450745\n",
      "Subject 7, Epoch 882, Loss: 0.552041657269001, Final Batch Loss: 0.08377891033887863\n",
      "Subject 7, Epoch 883, Loss: 0.35150204598903656, Final Batch Loss: 0.06624621152877808\n",
      "Subject 7, Epoch 884, Loss: 0.34554174542427063, Final Batch Loss: 0.11672710627317429\n",
      "Subject 7, Epoch 885, Loss: 0.34568629413843155, Final Batch Loss: 0.12449921667575836\n",
      "Subject 7, Epoch 886, Loss: 0.4668016731739044, Final Batch Loss: 0.17564579844474792\n",
      "Subject 7, Epoch 887, Loss: 0.331538088619709, Final Batch Loss: 0.11416979134082794\n",
      "Subject 7, Epoch 888, Loss: 0.4825274422764778, Final Batch Loss: 0.14503642916679382\n",
      "Subject 7, Epoch 889, Loss: 0.415695920586586, Final Batch Loss: 0.10376441478729248\n",
      "Subject 7, Epoch 890, Loss: 0.49684829264879227, Final Batch Loss: 0.2938224673271179\n",
      "Subject 7, Epoch 891, Loss: 0.3594011664390564, Final Batch Loss: 0.1657811850309372\n",
      "Subject 7, Epoch 892, Loss: 0.45435144752264023, Final Batch Loss: 0.11458905786275864\n",
      "Subject 7, Epoch 893, Loss: 0.45131463557481766, Final Batch Loss: 0.14983859658241272\n",
      "Subject 7, Epoch 894, Loss: 0.40523964911699295, Final Batch Loss: 0.1344936490058899\n",
      "Subject 7, Epoch 895, Loss: 0.4049855023622513, Final Batch Loss: 0.06892052292823792\n",
      "Subject 7, Epoch 896, Loss: 0.3887077197432518, Final Batch Loss: 0.10608809441328049\n",
      "Subject 7, Epoch 897, Loss: 0.4444687068462372, Final Batch Loss: 0.11653174459934235\n",
      "Subject 7, Epoch 898, Loss: 0.4388144761323929, Final Batch Loss: 0.21046093106269836\n",
      "Subject 7, Epoch 899, Loss: 0.3566761687397957, Final Batch Loss: 0.11770511418581009\n",
      "Subject 7, Epoch 900, Loss: 0.4155551865696907, Final Batch Loss: 0.1776476353406906\n",
      "Subject 7, Epoch 901, Loss: 0.4543630927801132, Final Batch Loss: 0.12865157425403595\n",
      "Subject 7, Epoch 902, Loss: 0.4828111231327057, Final Batch Loss: 0.20859283208847046\n",
      "Subject 7, Epoch 903, Loss: 0.43383001536130905, Final Batch Loss: 0.1566772311925888\n",
      "Subject 7, Epoch 904, Loss: 0.479840524494648, Final Batch Loss: 0.22681497037410736\n",
      "Subject 7, Epoch 905, Loss: 0.32776614278554916, Final Batch Loss: 0.10605157911777496\n",
      "Subject 7, Epoch 906, Loss: 0.3401053324341774, Final Batch Loss: 0.10568990558385849\n",
      "Subject 7, Epoch 907, Loss: 0.3630448430776596, Final Batch Loss: 0.14296825230121613\n",
      "Subject 7, Epoch 908, Loss: 0.3578854128718376, Final Batch Loss: 0.0995061919093132\n",
      "Subject 7, Epoch 909, Loss: 0.42302314937114716, Final Batch Loss: 0.20122697949409485\n",
      "Subject 7, Epoch 910, Loss: 0.38975897431373596, Final Batch Loss: 0.19670051336288452\n",
      "Subject 7, Epoch 911, Loss: 0.5533108189702034, Final Batch Loss: 0.320279985666275\n",
      "Subject 7, Epoch 912, Loss: 0.31887439638376236, Final Batch Loss: 0.10200122743844986\n",
      "Subject 7, Epoch 913, Loss: 0.493640698492527, Final Batch Loss: 0.11519024521112442\n",
      "Subject 7, Epoch 914, Loss: 0.4901008829474449, Final Batch Loss: 0.11328063160181046\n",
      "Subject 7, Epoch 915, Loss: 0.34107907116413116, Final Batch Loss: 0.1029469445347786\n",
      "Subject 7, Epoch 916, Loss: 0.3462938144803047, Final Batch Loss: 0.10266007483005524\n",
      "Subject 7, Epoch 917, Loss: 0.36516624689102173, Final Batch Loss: 0.04776931554079056\n",
      "Subject 7, Epoch 918, Loss: 0.4021270200610161, Final Batch Loss: 0.08112452179193497\n",
      "Subject 7, Epoch 919, Loss: 0.3425515741109848, Final Batch Loss: 0.09905054420232773\n",
      "Subject 7, Epoch 920, Loss: 0.44259749352931976, Final Batch Loss: 0.20097708702087402\n",
      "Subject 7, Epoch 921, Loss: 0.37786777317523956, Final Batch Loss: 0.13286133110523224\n",
      "Subject 7, Epoch 922, Loss: 0.38956886529922485, Final Batch Loss: 0.1578502357006073\n",
      "Subject 7, Epoch 923, Loss: 0.37028469517827034, Final Batch Loss: 0.19022436439990997\n",
      "Subject 7, Epoch 924, Loss: 0.5386547297239304, Final Batch Loss: 0.1864234358072281\n",
      "Subject 7, Epoch 925, Loss: 0.4043491929769516, Final Batch Loss: 0.07820461690425873\n",
      "Subject 7, Epoch 926, Loss: 0.3978956267237663, Final Batch Loss: 0.11237768083810806\n",
      "Subject 7, Epoch 927, Loss: 0.36839842796325684, Final Batch Loss: 0.10712460428476334\n",
      "Subject 7, Epoch 928, Loss: 0.42967309057712555, Final Batch Loss: 0.16987954080104828\n",
      "Subject 7, Epoch 929, Loss: 0.4721135124564171, Final Batch Loss: 0.15686659514904022\n",
      "Subject 7, Epoch 930, Loss: 0.5173422172665596, Final Batch Loss: 0.26931455731391907\n",
      "Subject 7, Epoch 931, Loss: 0.4047236517071724, Final Batch Loss: 0.06448627263307571\n",
      "Subject 7, Epoch 932, Loss: 0.38830364495515823, Final Batch Loss: 0.11551936715841293\n",
      "Subject 7, Epoch 933, Loss: 0.42647840082645416, Final Batch Loss: 0.16350440680980682\n",
      "Subject 7, Epoch 934, Loss: 0.39626891165971756, Final Batch Loss: 0.1928122490644455\n",
      "Subject 7, Epoch 935, Loss: 0.3774576932191849, Final Batch Loss: 0.10017488151788712\n",
      "Subject 7, Epoch 936, Loss: 0.3880755454301834, Final Batch Loss: 0.08950980007648468\n",
      "Subject 7, Epoch 937, Loss: 0.2782471403479576, Final Batch Loss: 0.060871414840221405\n",
      "Subject 7, Epoch 938, Loss: 0.438861221075058, Final Batch Loss: 0.16644923388957977\n",
      "Subject 7, Epoch 939, Loss: 0.33719291910529137, Final Batch Loss: 0.12216682732105255\n",
      "Subject 7, Epoch 940, Loss: 0.41638343781232834, Final Batch Loss: 0.19708773493766785\n",
      "Subject 7, Epoch 941, Loss: 0.38491933792829514, Final Batch Loss: 0.17284642159938812\n",
      "Subject 7, Epoch 942, Loss: 0.47466813772916794, Final Batch Loss: 0.19116149842739105\n",
      "Subject 7, Epoch 943, Loss: 0.5446889027953148, Final Batch Loss: 0.34530624747276306\n",
      "Subject 7, Epoch 944, Loss: 0.3624039590358734, Final Batch Loss: 0.11893842369318008\n",
      "Subject 7, Epoch 945, Loss: 0.4537753313779831, Final Batch Loss: 0.1327304095029831\n",
      "Subject 7, Epoch 946, Loss: 0.43814248591661453, Final Batch Loss: 0.11652962118387222\n",
      "Subject 7, Epoch 947, Loss: 0.4730013757944107, Final Batch Loss: 0.180477112531662\n",
      "Subject 7, Epoch 948, Loss: 0.3927670791745186, Final Batch Loss: 0.11266940087080002\n",
      "Subject 7, Epoch 949, Loss: 0.35146939754486084, Final Batch Loss: 0.10241856426000595\n",
      "Subject 7, Epoch 950, Loss: 0.3408980369567871, Final Batch Loss: 0.154994934797287\n",
      "Subject 7, Epoch 951, Loss: 0.32832808047533035, Final Batch Loss: 0.11795400828123093\n",
      "Subject 7, Epoch 952, Loss: 0.4295031279325485, Final Batch Loss: 0.26476842164993286\n",
      "Subject 7, Epoch 953, Loss: 0.4472787007689476, Final Batch Loss: 0.2253066450357437\n",
      "Subject 7, Epoch 954, Loss: 0.4414355158805847, Final Batch Loss: 0.24312424659729004\n",
      "Subject 7, Epoch 955, Loss: 0.36956383287906647, Final Batch Loss: 0.16058707237243652\n",
      "Subject 7, Epoch 956, Loss: 0.3952813148498535, Final Batch Loss: 0.1506599634885788\n",
      "Subject 7, Epoch 957, Loss: 0.3177131935954094, Final Batch Loss: 0.09228871017694473\n",
      "Subject 7, Epoch 958, Loss: 0.44109802693128586, Final Batch Loss: 0.12003243714570999\n",
      "Subject 7, Epoch 959, Loss: 0.3227534741163254, Final Batch Loss: 0.1215723305940628\n",
      "Subject 7, Epoch 960, Loss: 0.4432593658566475, Final Batch Loss: 0.13859327137470245\n",
      "Subject 7, Epoch 961, Loss: 0.330062098801136, Final Batch Loss: 0.08845965564250946\n",
      "Subject 7, Epoch 962, Loss: 0.47047238051891327, Final Batch Loss: 0.24796679615974426\n",
      "Subject 7, Epoch 963, Loss: 0.41282128542661667, Final Batch Loss: 0.15880313515663147\n",
      "Subject 7, Epoch 964, Loss: 0.44504493474960327, Final Batch Loss: 0.19470146298408508\n",
      "Subject 7, Epoch 965, Loss: 0.3552631661295891, Final Batch Loss: 0.1373707801103592\n",
      "Subject 7, Epoch 966, Loss: 0.42307692766189575, Final Batch Loss: 0.15809956192970276\n",
      "Subject 7, Epoch 967, Loss: 0.317458376288414, Final Batch Loss: 0.08382347226142883\n",
      "Subject 7, Epoch 968, Loss: 0.5238198190927505, Final Batch Loss: 0.20111536979675293\n",
      "Subject 7, Epoch 969, Loss: 0.3396066278219223, Final Batch Loss: 0.06480947881937027\n",
      "Subject 7, Epoch 970, Loss: 0.46506935358047485, Final Batch Loss: 0.11847418546676636\n",
      "Subject 7, Epoch 971, Loss: 0.3881315439939499, Final Batch Loss: 0.1525510847568512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 7, Epoch 972, Loss: 0.4161811023950577, Final Batch Loss: 0.13759483397006989\n",
      "Subject 7, Epoch 973, Loss: 0.3887675553560257, Final Batch Loss: 0.04426012933254242\n",
      "Subject 7, Epoch 974, Loss: 0.4369242265820503, Final Batch Loss: 0.10414416342973709\n",
      "Subject 7, Epoch 975, Loss: 0.43633081018924713, Final Batch Loss: 0.1198509931564331\n",
      "Subject 7, Epoch 976, Loss: 0.346342571079731, Final Batch Loss: 0.1362331509590149\n",
      "Subject 7, Epoch 977, Loss: 0.40238596498966217, Final Batch Loss: 0.18146848678588867\n",
      "Subject 7, Epoch 978, Loss: 0.4316655993461609, Final Batch Loss: 0.26406392455101013\n",
      "Subject 7, Epoch 979, Loss: 0.3119453340768814, Final Batch Loss: 0.07555721700191498\n",
      "Subject 7, Epoch 980, Loss: 0.3949529454112053, Final Batch Loss: 0.1232168897986412\n",
      "Subject 7, Epoch 981, Loss: 0.3629821166396141, Final Batch Loss: 0.08449678122997284\n",
      "Subject 7, Epoch 982, Loss: 0.3608184680342674, Final Batch Loss: 0.1750861555337906\n",
      "Subject 7, Epoch 983, Loss: 0.37397150695323944, Final Batch Loss: 0.12151162326335907\n",
      "Subject 7, Epoch 984, Loss: 0.38503579422831535, Final Batch Loss: 0.055001962929964066\n",
      "Subject 7, Epoch 985, Loss: 0.3484528511762619, Final Batch Loss: 0.10028336197137833\n",
      "Subject 7, Epoch 986, Loss: 0.4850233644247055, Final Batch Loss: 0.1675591617822647\n",
      "Subject 7, Epoch 987, Loss: 0.6633271351456642, Final Batch Loss: 0.4110405445098877\n",
      "Subject 7, Epoch 988, Loss: 0.36444829404354095, Final Batch Loss: 0.08446001261472702\n",
      "Subject 7, Epoch 989, Loss: 0.46294940263032913, Final Batch Loss: 0.10488234460353851\n",
      "Subject 7, Epoch 990, Loss: 0.29793085902929306, Final Batch Loss: 0.08077132701873779\n",
      "Subject 7, Epoch 991, Loss: 0.4668712168931961, Final Batch Loss: 0.13263972103595734\n",
      "Subject 7, Epoch 992, Loss: 0.3571379780769348, Final Batch Loss: 0.05652356147766113\n",
      "Subject 7, Epoch 993, Loss: 0.3899823799729347, Final Batch Loss: 0.12130903452634811\n",
      "Subject 7, Epoch 994, Loss: 0.3736952543258667, Final Batch Loss: 0.13006554543972015\n",
      "Subject 7, Epoch 995, Loss: 0.3848545104265213, Final Batch Loss: 0.16522809863090515\n",
      "Subject 7, Epoch 996, Loss: 0.4496275410056114, Final Batch Loss: 0.16717147827148438\n",
      "Subject 7, Epoch 997, Loss: 0.40223296731710434, Final Batch Loss: 0.16551747918128967\n",
      "Subject 7, Epoch 998, Loss: 0.3460336923599243, Final Batch Loss: 0.10908263176679611\n",
      "Subject 7, Epoch 999, Loss: 0.42397191375494003, Final Batch Loss: 0.18247225880622864\n",
      "Subject 7, Epoch 1000, Loss: 0.31315645575523376, Final Batch Loss: 0.0873495489358902\n",
      "Subject 8, Epoch 1, Loss: 5.3925710916519165, Final Batch Loss: 1.8243905305862427\n",
      "Subject 8, Epoch 2, Loss: 5.381103515625, Final Batch Loss: 1.787327527999878\n",
      "Subject 8, Epoch 3, Loss: 5.37239933013916, Final Batch Loss: 1.7632315158843994\n",
      "Subject 8, Epoch 4, Loss: 5.365461945533752, Final Batch Loss: 1.7676702737808228\n",
      "Subject 8, Epoch 5, Loss: 5.366907119750977, Final Batch Loss: 1.7937761545181274\n",
      "Subject 8, Epoch 6, Loss: 5.362082481384277, Final Batch Loss: 1.792679786682129\n",
      "Subject 8, Epoch 7, Loss: 5.348039150238037, Final Batch Loss: 1.76205313205719\n",
      "Subject 8, Epoch 8, Loss: 5.339893102645874, Final Batch Loss: 1.7686370611190796\n",
      "Subject 8, Epoch 9, Loss: 5.334527969360352, Final Batch Loss: 1.7822965383529663\n",
      "Subject 8, Epoch 10, Loss: 5.307548761367798, Final Batch Loss: 1.7449774742126465\n",
      "Subject 8, Epoch 11, Loss: 5.291010499000549, Final Batch Loss: 1.7460678815841675\n",
      "Subject 8, Epoch 12, Loss: 5.271458387374878, Final Batch Loss: 1.7573809623718262\n",
      "Subject 8, Epoch 13, Loss: 5.221435189247131, Final Batch Loss: 1.698535442352295\n",
      "Subject 8, Epoch 14, Loss: 5.178820490837097, Final Batch Loss: 1.6988294124603271\n",
      "Subject 8, Epoch 15, Loss: 5.137480139732361, Final Batch Loss: 1.7015094757080078\n",
      "Subject 8, Epoch 16, Loss: 5.064268708229065, Final Batch Loss: 1.6869078874588013\n",
      "Subject 8, Epoch 17, Loss: 4.957334995269775, Final Batch Loss: 1.6027663946151733\n",
      "Subject 8, Epoch 18, Loss: 4.855453729629517, Final Batch Loss: 1.5882341861724854\n",
      "Subject 8, Epoch 19, Loss: 4.792100548744202, Final Batch Loss: 1.5223989486694336\n",
      "Subject 8, Epoch 20, Loss: 4.707820653915405, Final Batch Loss: 1.5674620866775513\n",
      "Subject 8, Epoch 21, Loss: 4.682852864265442, Final Batch Loss: 1.6093381643295288\n",
      "Subject 8, Epoch 22, Loss: 4.606676340103149, Final Batch Loss: 1.5477681159973145\n",
      "Subject 8, Epoch 23, Loss: 4.599090337753296, Final Batch Loss: 1.5530261993408203\n",
      "Subject 8, Epoch 24, Loss: 4.4305795431137085, Final Batch Loss: 1.4918955564498901\n",
      "Subject 8, Epoch 25, Loss: 4.417090177536011, Final Batch Loss: 1.4907197952270508\n",
      "Subject 8, Epoch 26, Loss: 4.358787417411804, Final Batch Loss: 1.476407527923584\n",
      "Subject 8, Epoch 27, Loss: 4.2647987604141235, Final Batch Loss: 1.4117281436920166\n",
      "Subject 8, Epoch 28, Loss: 4.308359146118164, Final Batch Loss: 1.4208369255065918\n",
      "Subject 8, Epoch 29, Loss: 4.302605986595154, Final Batch Loss: 1.4302204847335815\n",
      "Subject 8, Epoch 30, Loss: 4.242953181266785, Final Batch Loss: 1.402984619140625\n",
      "Subject 8, Epoch 31, Loss: 4.2185986042022705, Final Batch Loss: 1.371994137763977\n",
      "Subject 8, Epoch 32, Loss: 4.087488770484924, Final Batch Loss: 1.3241034746170044\n",
      "Subject 8, Epoch 33, Loss: 4.133245825767517, Final Batch Loss: 1.389762043952942\n",
      "Subject 8, Epoch 34, Loss: 4.035810947418213, Final Batch Loss: 1.3625638484954834\n",
      "Subject 8, Epoch 35, Loss: 4.021251916885376, Final Batch Loss: 1.348215103149414\n",
      "Subject 8, Epoch 36, Loss: 4.0190171003341675, Final Batch Loss: 1.3551863431930542\n",
      "Subject 8, Epoch 37, Loss: 3.9717252254486084, Final Batch Loss: 1.2522838115692139\n",
      "Subject 8, Epoch 38, Loss: 3.8009088039398193, Final Batch Loss: 1.2470954656600952\n",
      "Subject 8, Epoch 39, Loss: 3.7615692615509033, Final Batch Loss: 1.2647647857666016\n",
      "Subject 8, Epoch 40, Loss: 3.8024874925613403, Final Batch Loss: 1.28573739528656\n",
      "Subject 8, Epoch 41, Loss: 3.713639736175537, Final Batch Loss: 1.1972955465316772\n",
      "Subject 8, Epoch 42, Loss: 3.630019187927246, Final Batch Loss: 1.1609821319580078\n",
      "Subject 8, Epoch 43, Loss: 3.6066612005233765, Final Batch Loss: 1.2147148847579956\n",
      "Subject 8, Epoch 44, Loss: 3.526789426803589, Final Batch Loss: 1.1481748819351196\n",
      "Subject 8, Epoch 45, Loss: 3.6378233432769775, Final Batch Loss: 1.2413817644119263\n",
      "Subject 8, Epoch 46, Loss: 3.594715118408203, Final Batch Loss: 1.2503236532211304\n",
      "Subject 8, Epoch 47, Loss: 3.5711814165115356, Final Batch Loss: 1.20009446144104\n",
      "Subject 8, Epoch 48, Loss: 3.4457364082336426, Final Batch Loss: 1.1731313467025757\n",
      "Subject 8, Epoch 49, Loss: 3.4804582595825195, Final Batch Loss: 1.173675298690796\n",
      "Subject 8, Epoch 50, Loss: 3.4651883840560913, Final Batch Loss: 1.1983988285064697\n",
      "Subject 8, Epoch 51, Loss: 3.4319217205047607, Final Batch Loss: 1.158431053161621\n",
      "Subject 8, Epoch 52, Loss: 3.4343032836914062, Final Batch Loss: 1.1439846754074097\n",
      "Subject 8, Epoch 53, Loss: 3.5524353981018066, Final Batch Loss: 1.165960431098938\n",
      "Subject 8, Epoch 54, Loss: 3.378048062324524, Final Batch Loss: 1.1156870126724243\n",
      "Subject 8, Epoch 55, Loss: 3.358761191368103, Final Batch Loss: 1.0931999683380127\n",
      "Subject 8, Epoch 56, Loss: 3.3593387603759766, Final Batch Loss: 1.144946813583374\n",
      "Subject 8, Epoch 57, Loss: 3.4321868419647217, Final Batch Loss: 1.130379557609558\n",
      "Subject 8, Epoch 58, Loss: 3.384131669998169, Final Batch Loss: 1.1432538032531738\n",
      "Subject 8, Epoch 59, Loss: 3.38584303855896, Final Batch Loss: 1.141947865486145\n",
      "Subject 8, Epoch 60, Loss: 3.467416286468506, Final Batch Loss: 1.1673223972320557\n",
      "Subject 8, Epoch 61, Loss: 3.3328531980514526, Final Batch Loss: 1.1121258735656738\n",
      "Subject 8, Epoch 62, Loss: 3.3712499141693115, Final Batch Loss: 1.1153463125228882\n",
      "Subject 8, Epoch 63, Loss: 3.389445424079895, Final Batch Loss: 1.1579670906066895\n",
      "Subject 8, Epoch 64, Loss: 3.355244517326355, Final Batch Loss: 1.1258981227874756\n",
      "Subject 8, Epoch 65, Loss: 3.364869475364685, Final Batch Loss: 1.0810108184814453\n",
      "Subject 8, Epoch 66, Loss: 3.342728614807129, Final Batch Loss: 1.0819178819656372\n",
      "Subject 8, Epoch 67, Loss: 3.348122239112854, Final Batch Loss: 1.1311044692993164\n",
      "Subject 8, Epoch 68, Loss: 3.318588137626648, Final Batch Loss: 1.1005222797393799\n",
      "Subject 8, Epoch 69, Loss: 3.3218804597854614, Final Batch Loss: 1.0738790035247803\n",
      "Subject 8, Epoch 70, Loss: 3.3229111433029175, Final Batch Loss: 1.1631594896316528\n",
      "Subject 8, Epoch 71, Loss: 3.2822623252868652, Final Batch Loss: 1.0637563467025757\n",
      "Subject 8, Epoch 72, Loss: 3.3230807781219482, Final Batch Loss: 1.0864540338516235\n",
      "Subject 8, Epoch 73, Loss: 3.307458996772766, Final Batch Loss: 1.137508749961853\n",
      "Subject 8, Epoch 74, Loss: 3.2620242834091187, Final Batch Loss: 1.0829604864120483\n",
      "Subject 8, Epoch 75, Loss: 3.2571487426757812, Final Batch Loss: 1.0788495540618896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 76, Loss: 3.3058077096939087, Final Batch Loss: 1.0796914100646973\n",
      "Subject 8, Epoch 77, Loss: 3.3054364919662476, Final Batch Loss: 1.1004236936569214\n",
      "Subject 8, Epoch 78, Loss: 3.2909475564956665, Final Batch Loss: 1.0653964281082153\n",
      "Subject 8, Epoch 79, Loss: 3.317835569381714, Final Batch Loss: 1.1509073972702026\n",
      "Subject 8, Epoch 80, Loss: 3.2356455326080322, Final Batch Loss: 1.064666986465454\n",
      "Subject 8, Epoch 81, Loss: 3.2641055583953857, Final Batch Loss: 1.0478229522705078\n",
      "Subject 8, Epoch 82, Loss: 3.2093207836151123, Final Batch Loss: 1.075938105583191\n",
      "Subject 8, Epoch 83, Loss: 3.267855167388916, Final Batch Loss: 1.1624096632003784\n",
      "Subject 8, Epoch 84, Loss: 3.1694588661193848, Final Batch Loss: 1.0580800771713257\n",
      "Subject 8, Epoch 85, Loss: 3.2112854719161987, Final Batch Loss: 1.1033927202224731\n",
      "Subject 8, Epoch 86, Loss: 3.2124677896499634, Final Batch Loss: 1.0498173236846924\n",
      "Subject 8, Epoch 87, Loss: 3.167231798171997, Final Batch Loss: 1.049765706062317\n",
      "Subject 8, Epoch 88, Loss: 3.2240957021713257, Final Batch Loss: 1.0163291692733765\n",
      "Subject 8, Epoch 89, Loss: 3.2601715326309204, Final Batch Loss: 1.1085199117660522\n",
      "Subject 8, Epoch 90, Loss: 3.1806811094284058, Final Batch Loss: 1.045267939567566\n",
      "Subject 8, Epoch 91, Loss: 3.0953439474105835, Final Batch Loss: 1.0576138496398926\n",
      "Subject 8, Epoch 92, Loss: 3.050906181335449, Final Batch Loss: 1.0632330179214478\n",
      "Subject 8, Epoch 93, Loss: 3.0511422157287598, Final Batch Loss: 0.9805684089660645\n",
      "Subject 8, Epoch 94, Loss: 3.0215064883232117, Final Batch Loss: 0.9622413516044617\n",
      "Subject 8, Epoch 95, Loss: 3.028299331665039, Final Batch Loss: 1.0034306049346924\n",
      "Subject 8, Epoch 96, Loss: 2.9912145137786865, Final Batch Loss: 1.0046013593673706\n",
      "Subject 8, Epoch 97, Loss: 2.9881322979927063, Final Batch Loss: 0.9891369342803955\n",
      "Subject 8, Epoch 98, Loss: 2.9625170826911926, Final Batch Loss: 0.990976095199585\n",
      "Subject 8, Epoch 99, Loss: 2.914767265319824, Final Batch Loss: 0.990638256072998\n",
      "Subject 8, Epoch 100, Loss: 2.865092396736145, Final Batch Loss: 0.9595199227333069\n",
      "Subject 8, Epoch 101, Loss: 2.820013403892517, Final Batch Loss: 0.925969123840332\n",
      "Subject 8, Epoch 102, Loss: 2.8418047428131104, Final Batch Loss: 0.9568135142326355\n",
      "Subject 8, Epoch 103, Loss: 2.8437161445617676, Final Batch Loss: 0.9844032526016235\n",
      "Subject 8, Epoch 104, Loss: 2.6465219259262085, Final Batch Loss: 0.8634828329086304\n",
      "Subject 8, Epoch 105, Loss: 2.7560530304908752, Final Batch Loss: 0.8325371146202087\n",
      "Subject 8, Epoch 106, Loss: 2.6841248869895935, Final Batch Loss: 0.8682547211647034\n",
      "Subject 8, Epoch 107, Loss: 2.5870659351348877, Final Batch Loss: 0.8691405057907104\n",
      "Subject 8, Epoch 108, Loss: 2.664340317249298, Final Batch Loss: 0.9009398221969604\n",
      "Subject 8, Epoch 109, Loss: 2.697392225265503, Final Batch Loss: 0.8470698595046997\n",
      "Subject 8, Epoch 110, Loss: 2.572835683822632, Final Batch Loss: 0.809840202331543\n",
      "Subject 8, Epoch 111, Loss: 2.5825394988059998, Final Batch Loss: 0.8211024403572083\n",
      "Subject 8, Epoch 112, Loss: 2.648001492023468, Final Batch Loss: 0.931189239025116\n",
      "Subject 8, Epoch 113, Loss: 2.627187669277191, Final Batch Loss: 0.8893809914588928\n",
      "Subject 8, Epoch 114, Loss: 2.5307363271713257, Final Batch Loss: 0.8393368124961853\n",
      "Subject 8, Epoch 115, Loss: 2.4724578261375427, Final Batch Loss: 0.8139126300811768\n",
      "Subject 8, Epoch 116, Loss: 2.659960925579071, Final Batch Loss: 0.855026125907898\n",
      "Subject 8, Epoch 117, Loss: 2.478421628475189, Final Batch Loss: 0.85013747215271\n",
      "Subject 8, Epoch 118, Loss: 2.4515246748924255, Final Batch Loss: 0.8481956720352173\n",
      "Subject 8, Epoch 119, Loss: 2.5882570147514343, Final Batch Loss: 0.8238471746444702\n",
      "Subject 8, Epoch 120, Loss: 2.5182782411575317, Final Batch Loss: 0.7899013757705688\n",
      "Subject 8, Epoch 121, Loss: 2.4778472781181335, Final Batch Loss: 0.8653477430343628\n",
      "Subject 8, Epoch 122, Loss: 2.4796769618988037, Final Batch Loss: 0.8107393980026245\n",
      "Subject 8, Epoch 123, Loss: 2.527709424495697, Final Batch Loss: 0.8679513335227966\n",
      "Subject 8, Epoch 124, Loss: 2.400146007537842, Final Batch Loss: 0.7789624333381653\n",
      "Subject 8, Epoch 125, Loss: 2.4422231912612915, Final Batch Loss: 0.7423818707466125\n",
      "Subject 8, Epoch 126, Loss: 2.4340405464172363, Final Batch Loss: 0.8437341451644897\n",
      "Subject 8, Epoch 127, Loss: 2.3906639218330383, Final Batch Loss: 0.766828715801239\n",
      "Subject 8, Epoch 128, Loss: 2.4694941639900208, Final Batch Loss: 0.7646331191062927\n",
      "Subject 8, Epoch 129, Loss: 2.430320918560028, Final Batch Loss: 0.8504710793495178\n",
      "Subject 8, Epoch 130, Loss: 2.471878707408905, Final Batch Loss: 0.8171807527542114\n",
      "Subject 8, Epoch 131, Loss: 2.358427882194519, Final Batch Loss: 0.8011659383773804\n",
      "Subject 8, Epoch 132, Loss: 2.4890509247779846, Final Batch Loss: 0.7674100995063782\n",
      "Subject 8, Epoch 133, Loss: 2.377973973751068, Final Batch Loss: 0.8193156123161316\n",
      "Subject 8, Epoch 134, Loss: 2.462726056575775, Final Batch Loss: 0.7939521670341492\n",
      "Subject 8, Epoch 135, Loss: 2.420967221260071, Final Batch Loss: 0.8038502335548401\n",
      "Subject 8, Epoch 136, Loss: 2.369179427623749, Final Batch Loss: 0.8176681995391846\n",
      "Subject 8, Epoch 137, Loss: 2.3579826951026917, Final Batch Loss: 0.7467665076255798\n",
      "Subject 8, Epoch 138, Loss: 2.4192458391189575, Final Batch Loss: 0.7970951199531555\n",
      "Subject 8, Epoch 139, Loss: 2.342601478099823, Final Batch Loss: 0.7235337495803833\n",
      "Subject 8, Epoch 140, Loss: 2.535212278366089, Final Batch Loss: 0.8526420593261719\n",
      "Subject 8, Epoch 141, Loss: 2.345396339893341, Final Batch Loss: 0.7084716558456421\n",
      "Subject 8, Epoch 142, Loss: 2.3900853395462036, Final Batch Loss: 0.7723256349563599\n",
      "Subject 8, Epoch 143, Loss: 2.3642513155937195, Final Batch Loss: 0.8494296073913574\n",
      "Subject 8, Epoch 144, Loss: 2.5136650800704956, Final Batch Loss: 0.7987607717514038\n",
      "Subject 8, Epoch 145, Loss: 2.342431128025055, Final Batch Loss: 0.8184947967529297\n",
      "Subject 8, Epoch 146, Loss: 2.2965219616889954, Final Batch Loss: 0.7133310437202454\n",
      "Subject 8, Epoch 147, Loss: 2.339909791946411, Final Batch Loss: 0.7645969986915588\n",
      "Subject 8, Epoch 148, Loss: 2.3725759387016296, Final Batch Loss: 0.8607217669487\n",
      "Subject 8, Epoch 149, Loss: 2.383087456226349, Final Batch Loss: 0.8695157170295715\n",
      "Subject 8, Epoch 150, Loss: 2.340109944343567, Final Batch Loss: 0.8577688336372375\n",
      "Subject 8, Epoch 151, Loss: 2.339621901512146, Final Batch Loss: 0.8491491675376892\n",
      "Subject 8, Epoch 152, Loss: 2.250046133995056, Final Batch Loss: 0.7327019572257996\n",
      "Subject 8, Epoch 153, Loss: 2.341634213924408, Final Batch Loss: 0.7153527736663818\n",
      "Subject 8, Epoch 154, Loss: 2.3370178937911987, Final Batch Loss: 0.7907065153121948\n",
      "Subject 8, Epoch 155, Loss: 2.352282702922821, Final Batch Loss: 0.8219681978225708\n",
      "Subject 8, Epoch 156, Loss: 2.322079598903656, Final Batch Loss: 0.7574927806854248\n",
      "Subject 8, Epoch 157, Loss: 2.2316765785217285, Final Batch Loss: 0.8038690686225891\n",
      "Subject 8, Epoch 158, Loss: 2.4075059294700623, Final Batch Loss: 0.852045476436615\n",
      "Subject 8, Epoch 159, Loss: 2.2997226119041443, Final Batch Loss: 0.7215120792388916\n",
      "Subject 8, Epoch 160, Loss: 2.256557524204254, Final Batch Loss: 0.7229087948799133\n",
      "Subject 8, Epoch 161, Loss: 2.173910915851593, Final Batch Loss: 0.6882003545761108\n",
      "Subject 8, Epoch 162, Loss: 2.243409752845764, Final Batch Loss: 0.6743187308311462\n",
      "Subject 8, Epoch 163, Loss: 2.184248685836792, Final Batch Loss: 0.6626978516578674\n",
      "Subject 8, Epoch 164, Loss: 2.3171563148498535, Final Batch Loss: 0.7501041293144226\n",
      "Subject 8, Epoch 165, Loss: 2.2252930402755737, Final Batch Loss: 0.7599319815635681\n",
      "Subject 8, Epoch 166, Loss: 2.2758461833000183, Final Batch Loss: 0.7223318815231323\n",
      "Subject 8, Epoch 167, Loss: 2.273342192173004, Final Batch Loss: 0.7936727404594421\n",
      "Subject 8, Epoch 168, Loss: 2.2857567071914673, Final Batch Loss: 0.7967106699943542\n",
      "Subject 8, Epoch 169, Loss: 2.1967285871505737, Final Batch Loss: 0.7476514577865601\n",
      "Subject 8, Epoch 170, Loss: 2.3764429092407227, Final Batch Loss: 0.834902822971344\n",
      "Subject 8, Epoch 171, Loss: 2.2190732955932617, Final Batch Loss: 0.7351450324058533\n",
      "Subject 8, Epoch 172, Loss: 2.158785879611969, Final Batch Loss: 0.6656230092048645\n",
      "Subject 8, Epoch 173, Loss: 2.174595594406128, Final Batch Loss: 0.834771990776062\n",
      "Subject 8, Epoch 174, Loss: 2.1581339240074158, Final Batch Loss: 0.7308542728424072\n",
      "Subject 8, Epoch 175, Loss: 2.1632145643234253, Final Batch Loss: 0.6875768899917603\n",
      "Subject 8, Epoch 176, Loss: 2.1510809659957886, Final Batch Loss: 0.6597662568092346\n",
      "Subject 8, Epoch 177, Loss: 2.2674363255500793, Final Batch Loss: 0.792456328868866\n",
      "Subject 8, Epoch 178, Loss: 2.1439217925071716, Final Batch Loss: 0.6728048324584961\n",
      "Subject 8, Epoch 179, Loss: 2.1308470964431763, Final Batch Loss: 0.6512925028800964\n",
      "Subject 8, Epoch 180, Loss: 2.137122690677643, Final Batch Loss: 0.6756627559661865\n",
      "Subject 8, Epoch 181, Loss: 2.212577223777771, Final Batch Loss: 0.7267217636108398\n",
      "Subject 8, Epoch 182, Loss: 2.171391189098358, Final Batch Loss: 0.7361438274383545\n",
      "Subject 8, Epoch 183, Loss: 2.1378018856048584, Final Batch Loss: 0.7312154769897461\n",
      "Subject 8, Epoch 184, Loss: 2.1694063544273376, Final Batch Loss: 0.6832566261291504\n",
      "Subject 8, Epoch 185, Loss: 2.218480348587036, Final Batch Loss: 0.7834389805793762\n",
      "Subject 8, Epoch 186, Loss: 2.2841357588768005, Final Batch Loss: 0.7769786715507507\n",
      "Subject 8, Epoch 187, Loss: 2.0780966877937317, Final Batch Loss: 0.6985903382301331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 188, Loss: 2.064019024372101, Final Batch Loss: 0.7009942531585693\n",
      "Subject 8, Epoch 189, Loss: 2.1589658856391907, Final Batch Loss: 0.7897789478302002\n",
      "Subject 8, Epoch 190, Loss: 2.0670775175094604, Final Batch Loss: 0.6895166635513306\n",
      "Subject 8, Epoch 191, Loss: 2.210347294807434, Final Batch Loss: 0.7000432014465332\n",
      "Subject 8, Epoch 192, Loss: 2.14570015668869, Final Batch Loss: 0.7780798673629761\n",
      "Subject 8, Epoch 193, Loss: 2.057993173599243, Final Batch Loss: 0.6724697947502136\n",
      "Subject 8, Epoch 194, Loss: 2.0780882835388184, Final Batch Loss: 0.6691156625747681\n",
      "Subject 8, Epoch 195, Loss: 2.0522290468215942, Final Batch Loss: 0.6617101430892944\n",
      "Subject 8, Epoch 196, Loss: 1.9975114464759827, Final Batch Loss: 0.6450487375259399\n",
      "Subject 8, Epoch 197, Loss: 1.9706279039382935, Final Batch Loss: 0.6537790298461914\n",
      "Subject 8, Epoch 198, Loss: 2.059817850589752, Final Batch Loss: 0.7393577098846436\n",
      "Subject 8, Epoch 199, Loss: 2.034229278564453, Final Batch Loss: 0.6733340620994568\n",
      "Subject 8, Epoch 200, Loss: 2.013074040412903, Final Batch Loss: 0.6810659170150757\n",
      "Subject 8, Epoch 201, Loss: 1.930755376815796, Final Batch Loss: 0.6555313467979431\n",
      "Subject 8, Epoch 202, Loss: 2.1179758310317993, Final Batch Loss: 0.6741322875022888\n",
      "Subject 8, Epoch 203, Loss: 2.044945776462555, Final Batch Loss: 0.6975295543670654\n",
      "Subject 8, Epoch 204, Loss: 1.9904588460922241, Final Batch Loss: 0.701417088508606\n",
      "Subject 8, Epoch 205, Loss: 2.006739020347595, Final Batch Loss: 0.6503725051879883\n",
      "Subject 8, Epoch 206, Loss: 2.0329331755638123, Final Batch Loss: 0.7963754534721375\n",
      "Subject 8, Epoch 207, Loss: 1.9187774658203125, Final Batch Loss: 0.6196317076683044\n",
      "Subject 8, Epoch 208, Loss: 1.9553847312927246, Final Batch Loss: 0.6489760875701904\n",
      "Subject 8, Epoch 209, Loss: 2.1115024089813232, Final Batch Loss: 0.6623912453651428\n",
      "Subject 8, Epoch 210, Loss: 1.9093119502067566, Final Batch Loss: 0.6583040952682495\n",
      "Subject 8, Epoch 211, Loss: 1.956409215927124, Final Batch Loss: 0.5824776291847229\n",
      "Subject 8, Epoch 212, Loss: 2.001384973526001, Final Batch Loss: 0.6340920925140381\n",
      "Subject 8, Epoch 213, Loss: 1.9460269808769226, Final Batch Loss: 0.6750749945640564\n",
      "Subject 8, Epoch 214, Loss: 1.9780882596969604, Final Batch Loss: 0.6900070905685425\n",
      "Subject 8, Epoch 215, Loss: 2.0044144988059998, Final Batch Loss: 0.6319252848625183\n",
      "Subject 8, Epoch 216, Loss: 1.9523429870605469, Final Batch Loss: 0.6510120630264282\n",
      "Subject 8, Epoch 217, Loss: 1.9153398871421814, Final Batch Loss: 0.5502960681915283\n",
      "Subject 8, Epoch 218, Loss: 2.016545593738556, Final Batch Loss: 0.6795824766159058\n",
      "Subject 8, Epoch 219, Loss: 1.9745216369628906, Final Batch Loss: 0.7484450936317444\n",
      "Subject 8, Epoch 220, Loss: 1.8657643795013428, Final Batch Loss: 0.6514499187469482\n",
      "Subject 8, Epoch 221, Loss: 1.9913108348846436, Final Batch Loss: 0.6981266140937805\n",
      "Subject 8, Epoch 222, Loss: 1.8135799169540405, Final Batch Loss: 0.6364713907241821\n",
      "Subject 8, Epoch 223, Loss: 1.9303337931632996, Final Batch Loss: 0.6647965312004089\n",
      "Subject 8, Epoch 224, Loss: 1.9250839948654175, Final Batch Loss: 0.6266043186187744\n",
      "Subject 8, Epoch 225, Loss: 1.8545249700546265, Final Batch Loss: 0.6214091181755066\n",
      "Subject 8, Epoch 226, Loss: 1.9316173791885376, Final Batch Loss: 0.6247118711471558\n",
      "Subject 8, Epoch 227, Loss: 1.8675848841667175, Final Batch Loss: 0.6343201994895935\n",
      "Subject 8, Epoch 228, Loss: 2.026540696620941, Final Batch Loss: 0.6566005945205688\n",
      "Subject 8, Epoch 229, Loss: 1.8961201906204224, Final Batch Loss: 0.7269035577774048\n",
      "Subject 8, Epoch 230, Loss: 1.9095535278320312, Final Batch Loss: 0.5548813343048096\n",
      "Subject 8, Epoch 231, Loss: 1.9099258780479431, Final Batch Loss: 0.6057940125465393\n",
      "Subject 8, Epoch 232, Loss: 1.9568765461444855, Final Batch Loss: 0.4991171061992645\n",
      "Subject 8, Epoch 233, Loss: 1.8897212147712708, Final Batch Loss: 0.6188603639602661\n",
      "Subject 8, Epoch 234, Loss: 1.9028828740119934, Final Batch Loss: 0.6266574263572693\n",
      "Subject 8, Epoch 235, Loss: 1.9065883159637451, Final Batch Loss: 0.6672027111053467\n",
      "Subject 8, Epoch 236, Loss: 1.7979521751403809, Final Batch Loss: 0.5980773568153381\n",
      "Subject 8, Epoch 237, Loss: 1.8625407814979553, Final Batch Loss: 0.5944700241088867\n",
      "Subject 8, Epoch 238, Loss: 1.8156181573867798, Final Batch Loss: 0.5943263173103333\n",
      "Subject 8, Epoch 239, Loss: 1.8343526721000671, Final Batch Loss: 0.5537135601043701\n",
      "Subject 8, Epoch 240, Loss: 1.8156754970550537, Final Batch Loss: 0.6039522886276245\n",
      "Subject 8, Epoch 241, Loss: 1.8699618577957153, Final Batch Loss: 0.624919593334198\n",
      "Subject 8, Epoch 242, Loss: 1.757088541984558, Final Batch Loss: 0.638310432434082\n",
      "Subject 8, Epoch 243, Loss: 1.7215223908424377, Final Batch Loss: 0.5448309779167175\n",
      "Subject 8, Epoch 244, Loss: 1.7171205580234528, Final Batch Loss: 0.5750923752784729\n",
      "Subject 8, Epoch 245, Loss: 1.8116351962089539, Final Batch Loss: 0.6172620058059692\n",
      "Subject 8, Epoch 246, Loss: 1.8838700652122498, Final Batch Loss: 0.6712331771850586\n",
      "Subject 8, Epoch 247, Loss: 1.746498703956604, Final Batch Loss: 0.5847354531288147\n",
      "Subject 8, Epoch 248, Loss: 1.827984094619751, Final Batch Loss: 0.7289207577705383\n",
      "Subject 8, Epoch 249, Loss: 1.7079982161521912, Final Batch Loss: 0.5521087050437927\n",
      "Subject 8, Epoch 250, Loss: 1.7604859173297882, Final Batch Loss: 0.4924897849559784\n",
      "Subject 8, Epoch 251, Loss: 1.8042194843292236, Final Batch Loss: 0.6296661496162415\n",
      "Subject 8, Epoch 252, Loss: 1.7777098417282104, Final Batch Loss: 0.7333314418792725\n",
      "Subject 8, Epoch 253, Loss: 1.9075344800949097, Final Batch Loss: 0.6919216513633728\n",
      "Subject 8, Epoch 254, Loss: 1.8535929322242737, Final Batch Loss: 0.7029725909233093\n",
      "Subject 8, Epoch 255, Loss: 1.726923406124115, Final Batch Loss: 0.6153658032417297\n",
      "Subject 8, Epoch 256, Loss: 1.7527735829353333, Final Batch Loss: 0.6601002812385559\n",
      "Subject 8, Epoch 257, Loss: 1.7866328358650208, Final Batch Loss: 0.5668354034423828\n",
      "Subject 8, Epoch 258, Loss: 1.8022754192352295, Final Batch Loss: 0.6811677813529968\n",
      "Subject 8, Epoch 259, Loss: 1.7945724129676819, Final Batch Loss: 0.6115704774856567\n",
      "Subject 8, Epoch 260, Loss: 1.6459232866764069, Final Batch Loss: 0.6377643942832947\n",
      "Subject 8, Epoch 261, Loss: 1.766785740852356, Final Batch Loss: 0.5166260600090027\n",
      "Subject 8, Epoch 262, Loss: 1.6957994103431702, Final Batch Loss: 0.5841785073280334\n",
      "Subject 8, Epoch 263, Loss: 1.7083868980407715, Final Batch Loss: 0.6148268580436707\n",
      "Subject 8, Epoch 264, Loss: 1.780741810798645, Final Batch Loss: 0.5712040066719055\n",
      "Subject 8, Epoch 265, Loss: 1.8619071245193481, Final Batch Loss: 0.6743581891059875\n",
      "Subject 8, Epoch 266, Loss: 1.726396083831787, Final Batch Loss: 0.6401897668838501\n",
      "Subject 8, Epoch 267, Loss: 1.8200844526290894, Final Batch Loss: 0.6043028235435486\n",
      "Subject 8, Epoch 268, Loss: 1.8241498470306396, Final Batch Loss: 0.5309550166130066\n",
      "Subject 8, Epoch 269, Loss: 1.7215279340744019, Final Batch Loss: 0.562493085861206\n",
      "Subject 8, Epoch 270, Loss: 1.5824098587036133, Final Batch Loss: 0.5166835188865662\n",
      "Subject 8, Epoch 271, Loss: 1.6237506866455078, Final Batch Loss: 0.5420315265655518\n",
      "Subject 8, Epoch 272, Loss: 1.7464300990104675, Final Batch Loss: 0.5329896211624146\n",
      "Subject 8, Epoch 273, Loss: 1.7574961185455322, Final Batch Loss: 0.5021494030952454\n",
      "Subject 8, Epoch 274, Loss: 1.6558744311332703, Final Batch Loss: 0.5992993116378784\n",
      "Subject 8, Epoch 275, Loss: 1.6169914305210114, Final Batch Loss: 0.4896446764469147\n",
      "Subject 8, Epoch 276, Loss: 1.6186384558677673, Final Batch Loss: 0.5315804481506348\n",
      "Subject 8, Epoch 277, Loss: 1.5711378455162048, Final Batch Loss: 0.5386160016059875\n",
      "Subject 8, Epoch 278, Loss: 1.6614174246788025, Final Batch Loss: 0.5251443386077881\n",
      "Subject 8, Epoch 279, Loss: 1.6919569969177246, Final Batch Loss: 0.5649064183235168\n",
      "Subject 8, Epoch 280, Loss: 1.584372878074646, Final Batch Loss: 0.5018309354782104\n",
      "Subject 8, Epoch 281, Loss: 1.7453565001487732, Final Batch Loss: 0.545872151851654\n",
      "Subject 8, Epoch 282, Loss: 1.6227152347564697, Final Batch Loss: 0.590688943862915\n",
      "Subject 8, Epoch 283, Loss: 1.6455811262130737, Final Batch Loss: 0.5088745355606079\n",
      "Subject 8, Epoch 284, Loss: 1.7137346267700195, Final Batch Loss: 0.625025749206543\n",
      "Subject 8, Epoch 285, Loss: 1.5699117481708527, Final Batch Loss: 0.47885164618492126\n",
      "Subject 8, Epoch 286, Loss: 1.5856136977672577, Final Batch Loss: 0.4950987994670868\n",
      "Subject 8, Epoch 287, Loss: 1.6681183576583862, Final Batch Loss: 0.5863470435142517\n",
      "Subject 8, Epoch 288, Loss: 1.5779592990875244, Final Batch Loss: 0.5395097732543945\n",
      "Subject 8, Epoch 289, Loss: 1.5711049735546112, Final Batch Loss: 0.5716450810432434\n",
      "Subject 8, Epoch 290, Loss: 1.552063524723053, Final Batch Loss: 0.5230672359466553\n",
      "Subject 8, Epoch 291, Loss: 1.547319620847702, Final Batch Loss: 0.4796275198459625\n",
      "Subject 8, Epoch 292, Loss: 1.60966095328331, Final Batch Loss: 0.5795885920524597\n",
      "Subject 8, Epoch 293, Loss: 1.604222446680069, Final Batch Loss: 0.4138014018535614\n",
      "Subject 8, Epoch 294, Loss: 1.6612650752067566, Final Batch Loss: 0.5235612988471985\n",
      "Subject 8, Epoch 295, Loss: 1.6258449852466583, Final Batch Loss: 0.48828545212745667\n",
      "Subject 8, Epoch 296, Loss: 1.6192227005958557, Final Batch Loss: 0.5156782865524292\n",
      "Subject 8, Epoch 297, Loss: 1.7585211992263794, Final Batch Loss: 0.5501447319984436\n",
      "Subject 8, Epoch 298, Loss: 1.6368061304092407, Final Batch Loss: 0.5858567953109741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 299, Loss: 1.6685319244861603, Final Batch Loss: 0.6003975868225098\n",
      "Subject 8, Epoch 300, Loss: 1.6862550973892212, Final Batch Loss: 0.5682242512702942\n",
      "Subject 8, Epoch 301, Loss: 1.5971136093139648, Final Batch Loss: 0.49494630098342896\n",
      "Subject 8, Epoch 302, Loss: 1.517689287662506, Final Batch Loss: 0.4025111198425293\n",
      "Subject 8, Epoch 303, Loss: 1.6985962390899658, Final Batch Loss: 0.6152611970901489\n",
      "Subject 8, Epoch 304, Loss: 1.5151366293430328, Final Batch Loss: 0.5174079537391663\n",
      "Subject 8, Epoch 305, Loss: 1.6055437326431274, Final Batch Loss: 0.4627622961997986\n",
      "Subject 8, Epoch 306, Loss: 1.664939284324646, Final Batch Loss: 0.6005077362060547\n",
      "Subject 8, Epoch 307, Loss: 1.6874749660491943, Final Batch Loss: 0.5982131958007812\n",
      "Subject 8, Epoch 308, Loss: 1.6670821905136108, Final Batch Loss: 0.47097229957580566\n",
      "Subject 8, Epoch 309, Loss: 1.5146088302135468, Final Batch Loss: 0.4923289716243744\n",
      "Subject 8, Epoch 310, Loss: 1.56455659866333, Final Batch Loss: 0.4721355438232422\n",
      "Subject 8, Epoch 311, Loss: 1.5506082475185394, Final Batch Loss: 0.41585609316825867\n",
      "Subject 8, Epoch 312, Loss: 1.6948029398918152, Final Batch Loss: 0.6395490169525146\n",
      "Subject 8, Epoch 313, Loss: 1.5086646378040314, Final Batch Loss: 0.48801368474960327\n",
      "Subject 8, Epoch 314, Loss: 1.5652039349079132, Final Batch Loss: 0.6090459823608398\n",
      "Subject 8, Epoch 315, Loss: 1.4947618544101715, Final Batch Loss: 0.498542845249176\n",
      "Subject 8, Epoch 316, Loss: 1.5555808246135712, Final Batch Loss: 0.5601312518119812\n",
      "Subject 8, Epoch 317, Loss: 1.5428166091442108, Final Batch Loss: 0.5970490574836731\n",
      "Subject 8, Epoch 318, Loss: 1.4674139022827148, Final Batch Loss: 0.4745662808418274\n",
      "Subject 8, Epoch 319, Loss: 1.5195602774620056, Final Batch Loss: 0.5068840980529785\n",
      "Subject 8, Epoch 320, Loss: 1.5390592217445374, Final Batch Loss: 0.4656156301498413\n",
      "Subject 8, Epoch 321, Loss: 1.6024925708770752, Final Batch Loss: 0.5750837922096252\n",
      "Subject 8, Epoch 322, Loss: 1.471911370754242, Final Batch Loss: 0.5355321168899536\n",
      "Subject 8, Epoch 323, Loss: 1.5092296600341797, Final Batch Loss: 0.5265305638313293\n",
      "Subject 8, Epoch 324, Loss: 1.5224208235740662, Final Batch Loss: 0.5269120335578918\n",
      "Subject 8, Epoch 325, Loss: 1.4596182703971863, Final Batch Loss: 0.5380730032920837\n",
      "Subject 8, Epoch 326, Loss: 1.4603218138217926, Final Batch Loss: 0.45286908745765686\n",
      "Subject 8, Epoch 327, Loss: 1.553411602973938, Final Batch Loss: 0.5007283091545105\n",
      "Subject 8, Epoch 328, Loss: 1.5268955826759338, Final Batch Loss: 0.5549689531326294\n",
      "Subject 8, Epoch 329, Loss: 1.4790819585323334, Final Batch Loss: 0.5523768663406372\n",
      "Subject 8, Epoch 330, Loss: 1.4210281670093536, Final Batch Loss: 0.4210464358329773\n",
      "Subject 8, Epoch 331, Loss: 1.5935998558998108, Final Batch Loss: 0.5205104351043701\n",
      "Subject 8, Epoch 332, Loss: 1.5224155187606812, Final Batch Loss: 0.5113658905029297\n",
      "Subject 8, Epoch 333, Loss: 1.557565987110138, Final Batch Loss: 0.5325100421905518\n",
      "Subject 8, Epoch 334, Loss: 1.5460757613182068, Final Batch Loss: 0.4920095205307007\n",
      "Subject 8, Epoch 335, Loss: 1.514804095029831, Final Batch Loss: 0.40089794993400574\n",
      "Subject 8, Epoch 336, Loss: 1.5120162665843964, Final Batch Loss: 0.4654683768749237\n",
      "Subject 8, Epoch 337, Loss: 1.3824960887432098, Final Batch Loss: 0.4878033399581909\n",
      "Subject 8, Epoch 338, Loss: 1.4785184860229492, Final Batch Loss: 0.5281497240066528\n",
      "Subject 8, Epoch 339, Loss: 1.5812180042266846, Final Batch Loss: 0.46204155683517456\n",
      "Subject 8, Epoch 340, Loss: 1.4617669880390167, Final Batch Loss: 0.5060652494430542\n",
      "Subject 8, Epoch 341, Loss: 1.5625976920127869, Final Batch Loss: 0.5601595044136047\n",
      "Subject 8, Epoch 342, Loss: 1.5787397027015686, Final Batch Loss: 0.5034712553024292\n",
      "Subject 8, Epoch 343, Loss: 1.6020402908325195, Final Batch Loss: 0.5016608834266663\n",
      "Subject 8, Epoch 344, Loss: 1.582051545381546, Final Batch Loss: 0.4942384362220764\n",
      "Subject 8, Epoch 345, Loss: 1.5072073936462402, Final Batch Loss: 0.5266278386116028\n",
      "Subject 8, Epoch 346, Loss: 1.511864572763443, Final Batch Loss: 0.5042833089828491\n",
      "Subject 8, Epoch 347, Loss: 1.4862589836120605, Final Batch Loss: 0.45447492599487305\n",
      "Subject 8, Epoch 348, Loss: 1.4553866982460022, Final Batch Loss: 0.44010019302368164\n",
      "Subject 8, Epoch 349, Loss: 1.4626711905002594, Final Batch Loss: 0.4138890206813812\n",
      "Subject 8, Epoch 350, Loss: 1.4186568856239319, Final Batch Loss: 0.43300172686576843\n",
      "Subject 8, Epoch 351, Loss: 1.4732542634010315, Final Batch Loss: 0.5267052054405212\n",
      "Subject 8, Epoch 352, Loss: 1.5094057619571686, Final Batch Loss: 0.5141721963882446\n",
      "Subject 8, Epoch 353, Loss: 1.4492965340614319, Final Batch Loss: 0.4979225993156433\n",
      "Subject 8, Epoch 354, Loss: 1.488156259059906, Final Batch Loss: 0.4912424683570862\n",
      "Subject 8, Epoch 355, Loss: 1.40399169921875, Final Batch Loss: 0.43438878655433655\n",
      "Subject 8, Epoch 356, Loss: 1.5227672755718231, Final Batch Loss: 0.48423609137535095\n",
      "Subject 8, Epoch 357, Loss: 1.4630125164985657, Final Batch Loss: 0.4304395318031311\n",
      "Subject 8, Epoch 358, Loss: 1.4677300155162811, Final Batch Loss: 0.47704726457595825\n",
      "Subject 8, Epoch 359, Loss: 1.4043780267238617, Final Batch Loss: 0.3959236741065979\n",
      "Subject 8, Epoch 360, Loss: 1.4335270524024963, Final Batch Loss: 0.4864279329776764\n",
      "Subject 8, Epoch 361, Loss: 1.4457799792289734, Final Batch Loss: 0.4007073938846588\n",
      "Subject 8, Epoch 362, Loss: 1.5339093208312988, Final Batch Loss: 0.5370078682899475\n",
      "Subject 8, Epoch 363, Loss: 1.3808663189411163, Final Batch Loss: 0.4520711302757263\n",
      "Subject 8, Epoch 364, Loss: 1.4129087924957275, Final Batch Loss: 0.3768157362937927\n",
      "Subject 8, Epoch 365, Loss: 1.4636985063552856, Final Batch Loss: 0.47369474172592163\n",
      "Subject 8, Epoch 366, Loss: 1.4920207858085632, Final Batch Loss: 0.5759549736976624\n",
      "Subject 8, Epoch 367, Loss: 1.4824979305267334, Final Batch Loss: 0.4533468782901764\n",
      "Subject 8, Epoch 368, Loss: 1.3743315041065216, Final Batch Loss: 0.44046491384506226\n",
      "Subject 8, Epoch 369, Loss: 1.4644447267055511, Final Batch Loss: 0.5729392170906067\n",
      "Subject 8, Epoch 370, Loss: 1.4482987523078918, Final Batch Loss: 0.5469285249710083\n",
      "Subject 8, Epoch 371, Loss: 1.5287868678569794, Final Batch Loss: 0.4097703993320465\n",
      "Subject 8, Epoch 372, Loss: 1.4470380246639252, Final Batch Loss: 0.48960813879966736\n",
      "Subject 8, Epoch 373, Loss: 1.3660201728343964, Final Batch Loss: 0.47524866461753845\n",
      "Subject 8, Epoch 374, Loss: 1.4678560197353363, Final Batch Loss: 0.5010229349136353\n",
      "Subject 8, Epoch 375, Loss: 1.5074340999126434, Final Batch Loss: 0.5084862112998962\n",
      "Subject 8, Epoch 376, Loss: 1.493535578250885, Final Batch Loss: 0.4763825833797455\n",
      "Subject 8, Epoch 377, Loss: 1.5012340545654297, Final Batch Loss: 0.5376517176628113\n",
      "Subject 8, Epoch 378, Loss: 1.3926692605018616, Final Batch Loss: 0.42552146315574646\n",
      "Subject 8, Epoch 379, Loss: 1.4129840433597565, Final Batch Loss: 0.44688981771469116\n",
      "Subject 8, Epoch 380, Loss: 1.490814059972763, Final Batch Loss: 0.4956716299057007\n",
      "Subject 8, Epoch 381, Loss: 1.3972405195236206, Final Batch Loss: 0.5052862763404846\n",
      "Subject 8, Epoch 382, Loss: 1.5209114849567413, Final Batch Loss: 0.5986084342002869\n",
      "Subject 8, Epoch 383, Loss: 1.5599806904792786, Final Batch Loss: 0.58857262134552\n",
      "Subject 8, Epoch 384, Loss: 1.4140124022960663, Final Batch Loss: 0.4049220085144043\n",
      "Subject 8, Epoch 385, Loss: 1.4036484360694885, Final Batch Loss: 0.4899144768714905\n",
      "Subject 8, Epoch 386, Loss: 1.3900919258594513, Final Batch Loss: 0.4749220311641693\n",
      "Subject 8, Epoch 387, Loss: 1.3358795940876007, Final Batch Loss: 0.47102218866348267\n",
      "Subject 8, Epoch 388, Loss: 1.3933237195014954, Final Batch Loss: 0.5047526955604553\n",
      "Subject 8, Epoch 389, Loss: 1.4181053042411804, Final Batch Loss: 0.5459257364273071\n",
      "Subject 8, Epoch 390, Loss: 1.377247005701065, Final Batch Loss: 0.41323480010032654\n",
      "Subject 8, Epoch 391, Loss: 1.388939917087555, Final Batch Loss: 0.43029603362083435\n",
      "Subject 8, Epoch 392, Loss: 1.3978395760059357, Final Batch Loss: 0.4714263081550598\n",
      "Subject 8, Epoch 393, Loss: 1.4694501161575317, Final Batch Loss: 0.518926739692688\n",
      "Subject 8, Epoch 394, Loss: 1.3700042068958282, Final Batch Loss: 0.44327279925346375\n",
      "Subject 8, Epoch 395, Loss: 1.3889849185943604, Final Batch Loss: 0.42442718148231506\n",
      "Subject 8, Epoch 396, Loss: 1.3261337876319885, Final Batch Loss: 0.3287908732891083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 397, Loss: 1.3453626930713654, Final Batch Loss: 0.46212637424468994\n",
      "Subject 8, Epoch 398, Loss: 1.4671335220336914, Final Batch Loss: 0.5784558057785034\n",
      "Subject 8, Epoch 399, Loss: 1.3130062818527222, Final Batch Loss: 0.42239248752593994\n",
      "Subject 8, Epoch 400, Loss: 1.3452729284763336, Final Batch Loss: 0.3869388699531555\n",
      "Subject 8, Epoch 401, Loss: 1.4110436737537384, Final Batch Loss: 0.472749263048172\n",
      "Subject 8, Epoch 402, Loss: 1.3621504306793213, Final Batch Loss: 0.45942825078964233\n",
      "Subject 8, Epoch 403, Loss: 1.3340516686439514, Final Batch Loss: 0.5389708876609802\n",
      "Subject 8, Epoch 404, Loss: 1.3950179517269135, Final Batch Loss: 0.40982335805892944\n",
      "Subject 8, Epoch 405, Loss: 1.3426650166511536, Final Batch Loss: 0.3419829308986664\n",
      "Subject 8, Epoch 406, Loss: 1.3290394842624664, Final Batch Loss: 0.5315460562705994\n",
      "Subject 8, Epoch 407, Loss: 1.2899368405342102, Final Batch Loss: 0.43435636162757874\n",
      "Subject 8, Epoch 408, Loss: 1.3277786374092102, Final Batch Loss: 0.5117707252502441\n",
      "Subject 8, Epoch 409, Loss: 1.463018536567688, Final Batch Loss: 0.5907124280929565\n",
      "Subject 8, Epoch 410, Loss: 1.3124451339244843, Final Batch Loss: 0.3602394461631775\n",
      "Subject 8, Epoch 411, Loss: 1.2445628345012665, Final Batch Loss: 0.44221681356430054\n",
      "Subject 8, Epoch 412, Loss: 1.3807736039161682, Final Batch Loss: 0.43769943714141846\n",
      "Subject 8, Epoch 413, Loss: 1.2967343628406525, Final Batch Loss: 0.4395909309387207\n",
      "Subject 8, Epoch 414, Loss: 1.3045302033424377, Final Batch Loss: 0.4619806110858917\n",
      "Subject 8, Epoch 415, Loss: 1.4542681574821472, Final Batch Loss: 0.4099956452846527\n",
      "Subject 8, Epoch 416, Loss: 1.2809898257255554, Final Batch Loss: 0.4888693392276764\n",
      "Subject 8, Epoch 417, Loss: 1.366661787033081, Final Batch Loss: 0.49007439613342285\n",
      "Subject 8, Epoch 418, Loss: 1.4148551523685455, Final Batch Loss: 0.4384811520576477\n",
      "Subject 8, Epoch 419, Loss: 1.3437257409095764, Final Batch Loss: 0.4566570818424225\n",
      "Subject 8, Epoch 420, Loss: 1.4068396985530853, Final Batch Loss: 0.5660296082496643\n",
      "Subject 8, Epoch 421, Loss: 1.2460472881793976, Final Batch Loss: 0.4043208658695221\n",
      "Subject 8, Epoch 422, Loss: 1.499889612197876, Final Batch Loss: 0.5046412348747253\n",
      "Subject 8, Epoch 423, Loss: 1.3358573615550995, Final Batch Loss: 0.45231470465660095\n",
      "Subject 8, Epoch 424, Loss: 1.3765540421009064, Final Batch Loss: 0.4546527862548828\n",
      "Subject 8, Epoch 425, Loss: 1.245828241109848, Final Batch Loss: 0.35786935687065125\n",
      "Subject 8, Epoch 426, Loss: 1.5205397009849548, Final Batch Loss: 0.6298631429672241\n",
      "Subject 8, Epoch 427, Loss: 1.3641657531261444, Final Batch Loss: 0.5222194194793701\n",
      "Subject 8, Epoch 428, Loss: 1.2274830639362335, Final Batch Loss: 0.40151235461235046\n",
      "Subject 8, Epoch 429, Loss: 1.211466670036316, Final Batch Loss: 0.4538528621196747\n",
      "Subject 8, Epoch 430, Loss: 1.2576803863048553, Final Batch Loss: 0.4700889587402344\n",
      "Subject 8, Epoch 431, Loss: 1.3227839469909668, Final Batch Loss: 0.43218857049942017\n",
      "Subject 8, Epoch 432, Loss: 1.4477961957454681, Final Batch Loss: 0.43998849391937256\n",
      "Subject 8, Epoch 433, Loss: 1.391526073217392, Final Batch Loss: 0.4376944601535797\n",
      "Subject 8, Epoch 434, Loss: 1.3417998552322388, Final Batch Loss: 0.5061864852905273\n",
      "Subject 8, Epoch 435, Loss: 1.3096542358398438, Final Batch Loss: 0.4760957956314087\n",
      "Subject 8, Epoch 436, Loss: 1.2657462656497955, Final Batch Loss: 0.4183605909347534\n",
      "Subject 8, Epoch 437, Loss: 1.268006443977356, Final Batch Loss: 0.4356481730937958\n",
      "Subject 8, Epoch 438, Loss: 1.2330073118209839, Final Batch Loss: 0.3912810683250427\n",
      "Subject 8, Epoch 439, Loss: 1.3083289861679077, Final Batch Loss: 0.40006399154663086\n",
      "Subject 8, Epoch 440, Loss: 1.4863958060741425, Final Batch Loss: 0.5797030329704285\n",
      "Subject 8, Epoch 441, Loss: 1.3057904243469238, Final Batch Loss: 0.38431835174560547\n",
      "Subject 8, Epoch 442, Loss: 1.2899458706378937, Final Batch Loss: 0.40938520431518555\n",
      "Subject 8, Epoch 443, Loss: 1.1913531124591827, Final Batch Loss: 0.3878646492958069\n",
      "Subject 8, Epoch 444, Loss: 1.2957499623298645, Final Batch Loss: 0.416948527097702\n",
      "Subject 8, Epoch 445, Loss: 1.3080407679080963, Final Batch Loss: 0.3774954080581665\n",
      "Subject 8, Epoch 446, Loss: 1.4365503191947937, Final Batch Loss: 0.5022908449172974\n",
      "Subject 8, Epoch 447, Loss: 1.2664447128772736, Final Batch Loss: 0.4074992537498474\n",
      "Subject 8, Epoch 448, Loss: 1.3582004606723785, Final Batch Loss: 0.4406365156173706\n",
      "Subject 8, Epoch 449, Loss: 1.2450226247310638, Final Batch Loss: 0.3895076811313629\n",
      "Subject 8, Epoch 450, Loss: 1.236616462469101, Final Batch Loss: 0.34525826573371887\n",
      "Subject 8, Epoch 451, Loss: 1.2862136662006378, Final Batch Loss: 0.40001609921455383\n",
      "Subject 8, Epoch 452, Loss: 1.254805862903595, Final Batch Loss: 0.4240954518318176\n",
      "Subject 8, Epoch 453, Loss: 1.3152733743190765, Final Batch Loss: 0.36220166087150574\n",
      "Subject 8, Epoch 454, Loss: 1.3088032007217407, Final Batch Loss: 0.4663649797439575\n",
      "Subject 8, Epoch 455, Loss: 1.2148984670639038, Final Batch Loss: 0.35062700510025024\n",
      "Subject 8, Epoch 456, Loss: 1.1655231714248657, Final Batch Loss: 0.3789328932762146\n",
      "Subject 8, Epoch 457, Loss: 1.3134750127792358, Final Batch Loss: 0.4063146114349365\n",
      "Subject 8, Epoch 458, Loss: 1.3337498605251312, Final Batch Loss: 0.5147613286972046\n",
      "Subject 8, Epoch 459, Loss: 1.3621774315834045, Final Batch Loss: 0.48111167550086975\n",
      "Subject 8, Epoch 460, Loss: 1.3195667266845703, Final Batch Loss: 0.3376610279083252\n",
      "Subject 8, Epoch 461, Loss: 1.2702337205410004, Final Batch Loss: 0.4634227752685547\n",
      "Subject 8, Epoch 462, Loss: 1.3133600950241089, Final Batch Loss: 0.44122618436813354\n",
      "Subject 8, Epoch 463, Loss: 1.2534388899803162, Final Batch Loss: 0.4288690388202667\n",
      "Subject 8, Epoch 464, Loss: 1.4029009342193604, Final Batch Loss: 0.5432724356651306\n",
      "Subject 8, Epoch 465, Loss: 1.2653295397758484, Final Batch Loss: 0.41986390948295593\n",
      "Subject 8, Epoch 466, Loss: 1.2816317677497864, Final Batch Loss: 0.40616631507873535\n",
      "Subject 8, Epoch 467, Loss: 1.2361633479595184, Final Batch Loss: 0.4547797441482544\n",
      "Subject 8, Epoch 468, Loss: 1.264392465353012, Final Batch Loss: 0.4179658591747284\n",
      "Subject 8, Epoch 469, Loss: 1.2595103085041046, Final Batch Loss: 0.4146910309791565\n",
      "Subject 8, Epoch 470, Loss: 1.2645011842250824, Final Batch Loss: 0.3322157859802246\n",
      "Subject 8, Epoch 471, Loss: 1.1354306042194366, Final Batch Loss: 0.3482501208782196\n",
      "Subject 8, Epoch 472, Loss: 1.269291877746582, Final Batch Loss: 0.5332407355308533\n",
      "Subject 8, Epoch 473, Loss: 1.2641647160053253, Final Batch Loss: 0.534724771976471\n",
      "Subject 8, Epoch 474, Loss: 1.3074052035808563, Final Batch Loss: 0.4951627552509308\n",
      "Subject 8, Epoch 475, Loss: 1.256406307220459, Final Batch Loss: 0.38038694858551025\n",
      "Subject 8, Epoch 476, Loss: 1.2219896018505096, Final Batch Loss: 0.3848390579223633\n",
      "Subject 8, Epoch 477, Loss: 1.2124979794025421, Final Batch Loss: 0.39893633127212524\n",
      "Subject 8, Epoch 478, Loss: 1.2038612365722656, Final Batch Loss: 0.4412679374217987\n",
      "Subject 8, Epoch 479, Loss: 1.250877559185028, Final Batch Loss: 0.43541619181632996\n",
      "Subject 8, Epoch 480, Loss: 1.195723295211792, Final Batch Loss: 0.38319632411003113\n",
      "Subject 8, Epoch 481, Loss: 1.1762405037879944, Final Batch Loss: 0.37834838032722473\n",
      "Subject 8, Epoch 482, Loss: 1.2936103343963623, Final Batch Loss: 0.47064605355262756\n",
      "Subject 8, Epoch 483, Loss: 1.2343967854976654, Final Batch Loss: 0.3961867094039917\n",
      "Subject 8, Epoch 484, Loss: 1.3116047978401184, Final Batch Loss: 0.3712964951992035\n",
      "Subject 8, Epoch 485, Loss: 1.360508680343628, Final Batch Loss: 0.38283368945121765\n",
      "Subject 8, Epoch 486, Loss: 1.253872573375702, Final Batch Loss: 0.411007821559906\n",
      "Subject 8, Epoch 487, Loss: 1.2299088537693024, Final Batch Loss: 0.4018358290195465\n",
      "Subject 8, Epoch 488, Loss: 1.1600249409675598, Final Batch Loss: 0.4024825394153595\n",
      "Subject 8, Epoch 489, Loss: 1.1398780345916748, Final Batch Loss: 0.43266159296035767\n",
      "Subject 8, Epoch 490, Loss: 1.166143923997879, Final Batch Loss: 0.33001184463500977\n",
      "Subject 8, Epoch 491, Loss: 1.3046355247497559, Final Batch Loss: 0.5132308602333069\n",
      "Subject 8, Epoch 492, Loss: 1.1812822222709656, Final Batch Loss: 0.3782704770565033\n",
      "Subject 8, Epoch 493, Loss: 1.160834789276123, Final Batch Loss: 0.4269798994064331\n",
      "Subject 8, Epoch 494, Loss: 1.278959572315216, Final Batch Loss: 0.42360901832580566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 495, Loss: 1.213690608739853, Final Batch Loss: 0.34302183985710144\n",
      "Subject 8, Epoch 496, Loss: 1.2505116760730743, Final Batch Loss: 0.4739115834236145\n",
      "Subject 8, Epoch 497, Loss: 1.2471121549606323, Final Batch Loss: 0.35007423162460327\n",
      "Subject 8, Epoch 498, Loss: 1.2635509073734283, Final Batch Loss: 0.46278929710388184\n",
      "Subject 8, Epoch 499, Loss: 1.1078818440437317, Final Batch Loss: 0.37885162234306335\n",
      "Subject 8, Epoch 500, Loss: 1.2400105595588684, Final Batch Loss: 0.4116586446762085\n",
      "Subject 8, Epoch 501, Loss: 1.313536912202835, Final Batch Loss: 0.45746514201164246\n",
      "Subject 8, Epoch 502, Loss: 1.140537291765213, Final Batch Loss: 0.38986772298812866\n",
      "Subject 8, Epoch 503, Loss: 1.1521460711956024, Final Batch Loss: 0.36425864696502686\n",
      "Subject 8, Epoch 504, Loss: 1.2082571983337402, Final Batch Loss: 0.36501002311706543\n",
      "Subject 8, Epoch 505, Loss: 1.2271826267242432, Final Batch Loss: 0.4056159257888794\n",
      "Subject 8, Epoch 506, Loss: 1.2911099195480347, Final Batch Loss: 0.42274338006973267\n",
      "Subject 8, Epoch 507, Loss: 1.3221966326236725, Final Batch Loss: 0.4616623520851135\n",
      "Subject 8, Epoch 508, Loss: 1.0754429697990417, Final Batch Loss: 0.35972413420677185\n",
      "Subject 8, Epoch 509, Loss: 1.1224052011966705, Final Batch Loss: 0.3036498427391052\n",
      "Subject 8, Epoch 510, Loss: 1.1542129516601562, Final Batch Loss: 0.3311174809932709\n",
      "Subject 8, Epoch 511, Loss: 1.143663763999939, Final Batch Loss: 0.48760244250297546\n",
      "Subject 8, Epoch 512, Loss: 1.1740534007549286, Final Batch Loss: 0.41706880927085876\n",
      "Subject 8, Epoch 513, Loss: 1.1308252811431885, Final Batch Loss: 0.34951117634773254\n",
      "Subject 8, Epoch 514, Loss: 1.1558476388454437, Final Batch Loss: 0.33163201808929443\n",
      "Subject 8, Epoch 515, Loss: 1.2891134917736053, Final Batch Loss: 0.4808368682861328\n",
      "Subject 8, Epoch 516, Loss: 1.0483485758304596, Final Batch Loss: 0.30509626865386963\n",
      "Subject 8, Epoch 517, Loss: 1.1521926820278168, Final Batch Loss: 0.4402313232421875\n",
      "Subject 8, Epoch 518, Loss: 1.234724223613739, Final Batch Loss: 0.39322397112846375\n",
      "Subject 8, Epoch 519, Loss: 1.081979662179947, Final Batch Loss: 0.34296083450317383\n",
      "Subject 8, Epoch 520, Loss: 1.088237464427948, Final Batch Loss: 0.2939470112323761\n",
      "Subject 8, Epoch 521, Loss: 1.2669345438480377, Final Batch Loss: 0.3617514967918396\n",
      "Subject 8, Epoch 522, Loss: 1.0638495683670044, Final Batch Loss: 0.35544466972351074\n",
      "Subject 8, Epoch 523, Loss: 1.1147803962230682, Final Batch Loss: 0.32214364409446716\n",
      "Subject 8, Epoch 524, Loss: 1.1896567046642303, Final Batch Loss: 0.4497781991958618\n",
      "Subject 8, Epoch 525, Loss: 1.0670049488544464, Final Batch Loss: 0.34868696331977844\n",
      "Subject 8, Epoch 526, Loss: 1.093698799610138, Final Batch Loss: 0.3883492350578308\n",
      "Subject 8, Epoch 527, Loss: 1.1435667276382446, Final Batch Loss: 0.42247244715690613\n",
      "Subject 8, Epoch 528, Loss: 1.124603420495987, Final Batch Loss: 0.3637903332710266\n",
      "Subject 8, Epoch 529, Loss: 1.161980390548706, Final Batch Loss: 0.3725578486919403\n",
      "Subject 8, Epoch 530, Loss: 1.063373476266861, Final Batch Loss: 0.36385300755500793\n",
      "Subject 8, Epoch 531, Loss: 1.1308950185775757, Final Batch Loss: 0.37347283959388733\n",
      "Subject 8, Epoch 532, Loss: 1.172637403011322, Final Batch Loss: 0.3555391728878021\n",
      "Subject 8, Epoch 533, Loss: 1.2554493248462677, Final Batch Loss: 0.5459797978401184\n",
      "Subject 8, Epoch 534, Loss: 1.1157659888267517, Final Batch Loss: 0.4272480010986328\n",
      "Subject 8, Epoch 535, Loss: 1.210379809141159, Final Batch Loss: 0.45422857999801636\n",
      "Subject 8, Epoch 536, Loss: 1.228807121515274, Final Batch Loss: 0.41444024443626404\n",
      "Subject 8, Epoch 537, Loss: 1.0930963456630707, Final Batch Loss: 0.35517382621765137\n",
      "Subject 8, Epoch 538, Loss: 1.0569889545440674, Final Batch Loss: 0.3044261634349823\n",
      "Subject 8, Epoch 539, Loss: 1.1718864440917969, Final Batch Loss: 0.42667481303215027\n",
      "Subject 8, Epoch 540, Loss: 1.104196548461914, Final Batch Loss: 0.4449167549610138\n",
      "Subject 8, Epoch 541, Loss: 1.092131108045578, Final Batch Loss: 0.3282982409000397\n",
      "Subject 8, Epoch 542, Loss: 1.108339786529541, Final Batch Loss: 0.2874998450279236\n",
      "Subject 8, Epoch 543, Loss: 1.1258218884468079, Final Batch Loss: 0.3063373267650604\n",
      "Subject 8, Epoch 544, Loss: 1.0380226075649261, Final Batch Loss: 0.35164839029312134\n",
      "Subject 8, Epoch 545, Loss: 1.1605899333953857, Final Batch Loss: 0.3481648862361908\n",
      "Subject 8, Epoch 546, Loss: 1.1466318666934967, Final Batch Loss: 0.3829871118068695\n",
      "Subject 8, Epoch 547, Loss: 1.174659103155136, Final Batch Loss: 0.3856463134288788\n",
      "Subject 8, Epoch 548, Loss: 1.180840253829956, Final Batch Loss: 0.4480924904346466\n",
      "Subject 8, Epoch 549, Loss: 0.9576394408941269, Final Batch Loss: 0.34778299927711487\n",
      "Subject 8, Epoch 550, Loss: 1.348712980747223, Final Batch Loss: 0.4099901616573334\n",
      "Subject 8, Epoch 551, Loss: 1.113236129283905, Final Batch Loss: 0.41584914922714233\n",
      "Subject 8, Epoch 552, Loss: 1.1936299353837967, Final Batch Loss: 0.2182827740907669\n",
      "Subject 8, Epoch 553, Loss: 1.0245194733142853, Final Batch Loss: 0.34637609124183655\n",
      "Subject 8, Epoch 554, Loss: 1.2301277220249176, Final Batch Loss: 0.4662017822265625\n",
      "Subject 8, Epoch 555, Loss: 1.0607667863368988, Final Batch Loss: 0.3215585947036743\n",
      "Subject 8, Epoch 556, Loss: 1.00447878241539, Final Batch Loss: 0.3072337508201599\n",
      "Subject 8, Epoch 557, Loss: 1.1142233610153198, Final Batch Loss: 0.3621275722980499\n",
      "Subject 8, Epoch 558, Loss: 1.1283644139766693, Final Batch Loss: 0.3524293303489685\n",
      "Subject 8, Epoch 559, Loss: 1.012579470872879, Final Batch Loss: 0.3354603946208954\n",
      "Subject 8, Epoch 560, Loss: 1.180559128522873, Final Batch Loss: 0.41675710678100586\n",
      "Subject 8, Epoch 561, Loss: 1.1004469692707062, Final Batch Loss: 0.3620161712169647\n",
      "Subject 8, Epoch 562, Loss: 1.1448399722576141, Final Batch Loss: 0.46220284700393677\n",
      "Subject 8, Epoch 563, Loss: 1.1123640835285187, Final Batch Loss: 0.32128608226776123\n",
      "Subject 8, Epoch 564, Loss: 1.0786443054676056, Final Batch Loss: 0.3194868266582489\n",
      "Subject 8, Epoch 565, Loss: 1.0505655705928802, Final Batch Loss: 0.32336077094078064\n",
      "Subject 8, Epoch 566, Loss: 1.040551334619522, Final Batch Loss: 0.37326788902282715\n",
      "Subject 8, Epoch 567, Loss: 0.983896791934967, Final Batch Loss: 0.2932193875312805\n",
      "Subject 8, Epoch 568, Loss: 1.0526829361915588, Final Batch Loss: 0.4066225588321686\n",
      "Subject 8, Epoch 569, Loss: 1.038842797279358, Final Batch Loss: 0.34464770555496216\n",
      "Subject 8, Epoch 570, Loss: 1.1473569869995117, Final Batch Loss: 0.3849690854549408\n",
      "Subject 8, Epoch 571, Loss: 1.0099143981933594, Final Batch Loss: 0.2962173819541931\n",
      "Subject 8, Epoch 572, Loss: 1.0250445306301117, Final Batch Loss: 0.3403998017311096\n",
      "Subject 8, Epoch 573, Loss: 0.9745259881019592, Final Batch Loss: 0.3579394221305847\n",
      "Subject 8, Epoch 574, Loss: 1.070078194141388, Final Batch Loss: 0.3180930018424988\n",
      "Subject 8, Epoch 575, Loss: 0.9866200089454651, Final Batch Loss: 0.32994988560676575\n",
      "Subject 8, Epoch 576, Loss: 1.0305316150188446, Final Batch Loss: 0.263975590467453\n",
      "Subject 8, Epoch 577, Loss: 1.0378879606723785, Final Batch Loss: 0.3560417592525482\n",
      "Subject 8, Epoch 578, Loss: 1.120993047952652, Final Batch Loss: 0.40182235836982727\n",
      "Subject 8, Epoch 579, Loss: 1.0851176977157593, Final Batch Loss: 0.4345252811908722\n",
      "Subject 8, Epoch 580, Loss: 1.0205723643302917, Final Batch Loss: 0.4023502767086029\n",
      "Subject 8, Epoch 581, Loss: 0.9179805517196655, Final Batch Loss: 0.32680150866508484\n",
      "Subject 8, Epoch 582, Loss: 1.0479285717010498, Final Batch Loss: 0.3299133777618408\n",
      "Subject 8, Epoch 583, Loss: 1.0685308277606964, Final Batch Loss: 0.3094138205051422\n",
      "Subject 8, Epoch 584, Loss: 0.9641551077365875, Final Batch Loss: 0.28578028082847595\n",
      "Subject 8, Epoch 585, Loss: 1.1755228340625763, Final Batch Loss: 0.45657050609588623\n",
      "Subject 8, Epoch 586, Loss: 0.9990566074848175, Final Batch Loss: 0.37344756722450256\n",
      "Subject 8, Epoch 587, Loss: 1.1211477220058441, Final Batch Loss: 0.4560932219028473\n",
      "Subject 8, Epoch 588, Loss: 0.9833065271377563, Final Batch Loss: 0.2851119637489319\n",
      "Subject 8, Epoch 589, Loss: 1.0458757877349854, Final Batch Loss: 0.3411678969860077\n",
      "Subject 8, Epoch 590, Loss: 0.8600417673587799, Final Batch Loss: 0.2556309401988983\n",
      "Subject 8, Epoch 591, Loss: 1.10225048661232, Final Batch Loss: 0.40053170919418335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 592, Loss: 1.0776489675045013, Final Batch Loss: 0.375814288854599\n",
      "Subject 8, Epoch 593, Loss: 0.8728656768798828, Final Batch Loss: 0.275363951921463\n",
      "Subject 8, Epoch 594, Loss: 0.8904260694980621, Final Batch Loss: 0.36349549889564514\n",
      "Subject 8, Epoch 595, Loss: 1.0890148878097534, Final Batch Loss: 0.322441428899765\n",
      "Subject 8, Epoch 596, Loss: 1.0404478311538696, Final Batch Loss: 0.30548208951950073\n",
      "Subject 8, Epoch 597, Loss: 0.9755590260028839, Final Batch Loss: 0.2614823877811432\n",
      "Subject 8, Epoch 598, Loss: 1.1249908804893494, Final Batch Loss: 0.40593352913856506\n",
      "Subject 8, Epoch 599, Loss: 0.8647659569978714, Final Batch Loss: 0.34968191385269165\n",
      "Subject 8, Epoch 600, Loss: 0.9655212461948395, Final Batch Loss: 0.3065739870071411\n",
      "Subject 8, Epoch 601, Loss: 0.8937774896621704, Final Batch Loss: 0.25214076042175293\n",
      "Subject 8, Epoch 602, Loss: 1.0173282325267792, Final Batch Loss: 0.29788678884506226\n",
      "Subject 8, Epoch 603, Loss: 0.928722083568573, Final Batch Loss: 0.35801127552986145\n",
      "Subject 8, Epoch 604, Loss: 1.0621851086616516, Final Batch Loss: 0.32272109389305115\n",
      "Subject 8, Epoch 605, Loss: 1.0362101197242737, Final Batch Loss: 0.38741499185562134\n",
      "Subject 8, Epoch 606, Loss: 1.0117788016796112, Final Batch Loss: 0.3424215018749237\n",
      "Subject 8, Epoch 607, Loss: 1.028669685125351, Final Batch Loss: 0.349752277135849\n",
      "Subject 8, Epoch 608, Loss: 1.0051356554031372, Final Batch Loss: 0.29116541147232056\n",
      "Subject 8, Epoch 609, Loss: 1.2185640633106232, Final Batch Loss: 0.43566352128982544\n",
      "Subject 8, Epoch 610, Loss: 0.8789140582084656, Final Batch Loss: 0.2849668860435486\n",
      "Subject 8, Epoch 611, Loss: 1.0538670420646667, Final Batch Loss: 0.4433451294898987\n",
      "Subject 8, Epoch 612, Loss: 1.025121420621872, Final Batch Loss: 0.34331876039505005\n",
      "Subject 8, Epoch 613, Loss: 0.987457662820816, Final Batch Loss: 0.3404158651828766\n",
      "Subject 8, Epoch 614, Loss: 0.9303439855575562, Final Batch Loss: 0.29380908608436584\n",
      "Subject 8, Epoch 615, Loss: 0.9829374849796295, Final Batch Loss: 0.3074726164340973\n",
      "Subject 8, Epoch 616, Loss: 1.0382501482963562, Final Batch Loss: 0.3574625253677368\n",
      "Subject 8, Epoch 617, Loss: 0.842032790184021, Final Batch Loss: 0.2532871961593628\n",
      "Subject 8, Epoch 618, Loss: 0.9258629083633423, Final Batch Loss: 0.2695786654949188\n",
      "Subject 8, Epoch 619, Loss: 0.9965663850307465, Final Batch Loss: 0.34702324867248535\n",
      "Subject 8, Epoch 620, Loss: 1.0688585340976715, Final Batch Loss: 0.43971189856529236\n",
      "Subject 8, Epoch 621, Loss: 0.9586718380451202, Final Batch Loss: 0.34963053464889526\n",
      "Subject 8, Epoch 622, Loss: 0.8250750303268433, Final Batch Loss: 0.2894027531147003\n",
      "Subject 8, Epoch 623, Loss: 0.9432933628559113, Final Batch Loss: 0.29707303643226624\n",
      "Subject 8, Epoch 624, Loss: 0.9160885810852051, Final Batch Loss: 0.305364727973938\n",
      "Subject 8, Epoch 625, Loss: 0.9944856464862823, Final Batch Loss: 0.3839607834815979\n",
      "Subject 8, Epoch 626, Loss: 1.0810869634151459, Final Batch Loss: 0.38442033529281616\n",
      "Subject 8, Epoch 627, Loss: 0.8852108120918274, Final Batch Loss: 0.2822187840938568\n",
      "Subject 8, Epoch 628, Loss: 1.0421082973480225, Final Batch Loss: 0.30471071600914\n",
      "Subject 8, Epoch 629, Loss: 1.007040560245514, Final Batch Loss: 0.3716742992401123\n",
      "Subject 8, Epoch 630, Loss: 0.8583243936300278, Final Batch Loss: 0.24824421107769012\n",
      "Subject 8, Epoch 631, Loss: 0.8271278291940689, Final Batch Loss: 0.21911010146141052\n",
      "Subject 8, Epoch 632, Loss: 1.0740799307823181, Final Batch Loss: 0.4188688099384308\n",
      "Subject 8, Epoch 633, Loss: 0.8685564994812012, Final Batch Loss: 0.3270614445209503\n",
      "Subject 8, Epoch 634, Loss: 1.0065182447433472, Final Batch Loss: 0.3916705846786499\n",
      "Subject 8, Epoch 635, Loss: 0.9318434298038483, Final Batch Loss: 0.23373520374298096\n",
      "Subject 8, Epoch 636, Loss: 0.8945339620113373, Final Batch Loss: 0.27249598503112793\n",
      "Subject 8, Epoch 637, Loss: 0.984417200088501, Final Batch Loss: 0.18391838669776917\n",
      "Subject 8, Epoch 638, Loss: 0.9664321541786194, Final Batch Loss: 0.336829274892807\n",
      "Subject 8, Epoch 639, Loss: 0.922583818435669, Final Batch Loss: 0.30787235498428345\n",
      "Subject 8, Epoch 640, Loss: 0.8931543678045273, Final Batch Loss: 0.3016514480113983\n",
      "Subject 8, Epoch 641, Loss: 0.8939937651157379, Final Batch Loss: 0.32119640707969666\n",
      "Subject 8, Epoch 642, Loss: 1.0288226306438446, Final Batch Loss: 0.3251188099384308\n",
      "Subject 8, Epoch 643, Loss: 0.8909614533185959, Final Batch Loss: 0.29639795422554016\n",
      "Subject 8, Epoch 644, Loss: 0.9559038877487183, Final Batch Loss: 0.3435584008693695\n",
      "Subject 8, Epoch 645, Loss: 0.9726510643959045, Final Batch Loss: 0.3361104726791382\n",
      "Subject 8, Epoch 646, Loss: 0.8599577248096466, Final Batch Loss: 0.2818243205547333\n",
      "Subject 8, Epoch 647, Loss: 0.9566417038440704, Final Batch Loss: 0.32333308458328247\n",
      "Subject 8, Epoch 648, Loss: 0.9676042199134827, Final Batch Loss: 0.3229091763496399\n",
      "Subject 8, Epoch 649, Loss: 1.0422658920288086, Final Batch Loss: 0.34678784012794495\n",
      "Subject 8, Epoch 650, Loss: 1.019728496670723, Final Batch Loss: 0.4715525209903717\n",
      "Subject 8, Epoch 651, Loss: 1.0197252035140991, Final Batch Loss: 0.38049179315567017\n",
      "Subject 8, Epoch 652, Loss: 1.014578253030777, Final Batch Loss: 0.3529013991355896\n",
      "Subject 8, Epoch 653, Loss: 0.8545909523963928, Final Batch Loss: 0.28606632351875305\n",
      "Subject 8, Epoch 654, Loss: 0.7960689663887024, Final Batch Loss: 0.3129217326641083\n",
      "Subject 8, Epoch 655, Loss: 1.056262493133545, Final Batch Loss: 0.2291196882724762\n",
      "Subject 8, Epoch 656, Loss: 0.9920593500137329, Final Batch Loss: 0.3683241307735443\n",
      "Subject 8, Epoch 657, Loss: 1.0993281602859497, Final Batch Loss: 0.330511212348938\n",
      "Subject 8, Epoch 658, Loss: 0.9442958235740662, Final Batch Loss: 0.31932857632637024\n",
      "Subject 8, Epoch 659, Loss: 0.9703597128391266, Final Batch Loss: 0.3391622006893158\n",
      "Subject 8, Epoch 660, Loss: 0.8727510273456573, Final Batch Loss: 0.2621987760066986\n",
      "Subject 8, Epoch 661, Loss: 0.856667086482048, Final Batch Loss: 0.2272188514471054\n",
      "Subject 8, Epoch 662, Loss: 0.8369234800338745, Final Batch Loss: 0.3124692738056183\n",
      "Subject 8, Epoch 663, Loss: 0.9377663731575012, Final Batch Loss: 0.3423020541667938\n",
      "Subject 8, Epoch 664, Loss: 0.8987865597009659, Final Batch Loss: 0.2191736251115799\n",
      "Subject 8, Epoch 665, Loss: 0.8894847482442856, Final Batch Loss: 0.3477242887020111\n",
      "Subject 8, Epoch 666, Loss: 0.8551975637674332, Final Batch Loss: 0.19810594618320465\n",
      "Subject 8, Epoch 667, Loss: 0.8545747399330139, Final Batch Loss: 0.28046736121177673\n",
      "Subject 8, Epoch 668, Loss: 0.8489798903465271, Final Batch Loss: 0.2999263405799866\n",
      "Subject 8, Epoch 669, Loss: 1.0582160651683807, Final Batch Loss: 0.35865122079849243\n",
      "Subject 8, Epoch 670, Loss: 1.0165561735630035, Final Batch Loss: 0.39028632640838623\n",
      "Subject 8, Epoch 671, Loss: 1.0726786255836487, Final Batch Loss: 0.4036920368671417\n",
      "Subject 8, Epoch 672, Loss: 0.8748224079608917, Final Batch Loss: 0.31008222699165344\n",
      "Subject 8, Epoch 673, Loss: 0.9211101233959198, Final Batch Loss: 0.27209028601646423\n",
      "Subject 8, Epoch 674, Loss: 0.7485032826662064, Final Batch Loss: 0.23957718908786774\n",
      "Subject 8, Epoch 675, Loss: 0.7783153355121613, Final Batch Loss: 0.2532329261302948\n",
      "Subject 8, Epoch 676, Loss: 0.8751040697097778, Final Batch Loss: 0.2542208433151245\n",
      "Subject 8, Epoch 677, Loss: 0.8673244416713715, Final Batch Loss: 0.3399752378463745\n",
      "Subject 8, Epoch 678, Loss: 0.8917527794837952, Final Batch Loss: 0.2628205120563507\n",
      "Subject 8, Epoch 679, Loss: 0.7520769536495209, Final Batch Loss: 0.257254421710968\n",
      "Subject 8, Epoch 680, Loss: 0.8459743112325668, Final Batch Loss: 0.24943603575229645\n",
      "Subject 8, Epoch 681, Loss: 0.7897823452949524, Final Batch Loss: 0.23233765363693237\n",
      "Subject 8, Epoch 682, Loss: 0.9819022715091705, Final Batch Loss: 0.37905463576316833\n",
      "Subject 8, Epoch 683, Loss: 0.9386280924081802, Final Batch Loss: 0.23782433569431305\n",
      "Subject 8, Epoch 684, Loss: 0.9381605982780457, Final Batch Loss: 0.3122539222240448\n",
      "Subject 8, Epoch 685, Loss: 0.9345988631248474, Final Batch Loss: 0.3410768210887909\n",
      "Subject 8, Epoch 686, Loss: 0.7980956137180328, Final Batch Loss: 0.18980523943901062\n",
      "Subject 8, Epoch 687, Loss: 0.7419116348028183, Final Batch Loss: 0.2670210599899292\n",
      "Subject 8, Epoch 688, Loss: 0.9933442771434784, Final Batch Loss: 0.4119209349155426\n",
      "Subject 8, Epoch 689, Loss: 0.751781091094017, Final Batch Loss: 0.20515595376491547\n",
      "Subject 8, Epoch 690, Loss: 0.8532805740833282, Final Batch Loss: 0.20645546913146973\n",
      "Subject 8, Epoch 691, Loss: 0.8891739994287491, Final Batch Loss: 0.3502473831176758\n",
      "Subject 8, Epoch 692, Loss: 0.9294856041669846, Final Batch Loss: 0.4319036304950714\n",
      "Subject 8, Epoch 693, Loss: 0.9313709586858749, Final Batch Loss: 0.23057971894741058\n",
      "Subject 8, Epoch 694, Loss: 0.7237771302461624, Final Batch Loss: 0.25699254870414734\n",
      "Subject 8, Epoch 695, Loss: 1.0292212814092636, Final Batch Loss: 0.3657703697681427\n",
      "Subject 8, Epoch 696, Loss: 0.9054058492183685, Final Batch Loss: 0.36157241463661194\n",
      "Subject 8, Epoch 697, Loss: 0.9305061101913452, Final Batch Loss: 0.27099373936653137\n",
      "Subject 8, Epoch 698, Loss: 0.8926464319229126, Final Batch Loss: 0.29041242599487305\n",
      "Subject 8, Epoch 699, Loss: 0.7713897079229355, Final Batch Loss: 0.26490524411201477\n",
      "Subject 8, Epoch 700, Loss: 0.7006876915693283, Final Batch Loss: 0.22614498436450958\n",
      "Subject 8, Epoch 701, Loss: 0.9690678715705872, Final Batch Loss: 0.30627715587615967\n",
      "Subject 8, Epoch 702, Loss: 0.9061530530452728, Final Batch Loss: 0.32673993706703186\n",
      "Subject 8, Epoch 703, Loss: 0.7930428683757782, Final Batch Loss: 0.31664204597473145\n",
      "Subject 8, Epoch 704, Loss: 0.7664534896612167, Final Batch Loss: 0.23562966287136078\n",
      "Subject 8, Epoch 705, Loss: 0.7853015661239624, Final Batch Loss: 0.25708091259002686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 706, Loss: 0.8258738815784454, Final Batch Loss: 0.1723688840866089\n",
      "Subject 8, Epoch 707, Loss: 0.9273775815963745, Final Batch Loss: 0.3339127004146576\n",
      "Subject 8, Epoch 708, Loss: 0.7865122854709625, Final Batch Loss: 0.24353481829166412\n",
      "Subject 8, Epoch 709, Loss: 0.8893844485282898, Final Batch Loss: 0.2948780655860901\n",
      "Subject 8, Epoch 710, Loss: 0.847255140542984, Final Batch Loss: 0.2518974840641022\n",
      "Subject 8, Epoch 711, Loss: 0.9023280441761017, Final Batch Loss: 0.33828309178352356\n",
      "Subject 8, Epoch 712, Loss: 0.9324433207511902, Final Batch Loss: 0.3319622278213501\n",
      "Subject 8, Epoch 713, Loss: 0.8219944685697556, Final Batch Loss: 0.3701540529727936\n",
      "Subject 8, Epoch 714, Loss: 0.9534914195537567, Final Batch Loss: 0.3324930965900421\n",
      "Subject 8, Epoch 715, Loss: 0.8890901058912277, Final Batch Loss: 0.3022446930408478\n",
      "Subject 8, Epoch 716, Loss: 0.8464752733707428, Final Batch Loss: 0.29033327102661133\n",
      "Subject 8, Epoch 717, Loss: 0.8200446665287018, Final Batch Loss: 0.2594502866268158\n",
      "Subject 8, Epoch 718, Loss: 0.8384032845497131, Final Batch Loss: 0.15811756253242493\n",
      "Subject 8, Epoch 719, Loss: 0.8759700655937195, Final Batch Loss: 0.26620450615882874\n",
      "Subject 8, Epoch 720, Loss: 0.8051112145185471, Final Batch Loss: 0.20656977593898773\n",
      "Subject 8, Epoch 721, Loss: 0.7809097319841385, Final Batch Loss: 0.26765647530555725\n",
      "Subject 8, Epoch 722, Loss: 0.984631210565567, Final Batch Loss: 0.3113410770893097\n",
      "Subject 8, Epoch 723, Loss: 0.84567591547966, Final Batch Loss: 0.20202818512916565\n",
      "Subject 8, Epoch 724, Loss: 0.7461712956428528, Final Batch Loss: 0.23020663857460022\n",
      "Subject 8, Epoch 725, Loss: 0.8084552064538002, Final Batch Loss: 0.12419862300157547\n",
      "Subject 8, Epoch 726, Loss: 0.7316593080759048, Final Batch Loss: 0.22348162531852722\n",
      "Subject 8, Epoch 727, Loss: 0.9333054721355438, Final Batch Loss: 0.31895238161087036\n",
      "Subject 8, Epoch 728, Loss: 0.7612508684396744, Final Batch Loss: 0.19669659435749054\n",
      "Subject 8, Epoch 729, Loss: 0.9335651397705078, Final Batch Loss: 0.30586618185043335\n",
      "Subject 8, Epoch 730, Loss: 0.8769840747117996, Final Batch Loss: 0.37914296984672546\n",
      "Subject 8, Epoch 731, Loss: 0.7607418149709702, Final Batch Loss: 0.1971781998872757\n",
      "Subject 8, Epoch 732, Loss: 0.775981530547142, Final Batch Loss: 0.24938510358333588\n",
      "Subject 8, Epoch 733, Loss: 0.8820630013942719, Final Batch Loss: 0.34813031554222107\n",
      "Subject 8, Epoch 734, Loss: 0.7575132101774216, Final Batch Loss: 0.19769127666950226\n",
      "Subject 8, Epoch 735, Loss: 0.8374916315078735, Final Batch Loss: 0.2392762303352356\n",
      "Subject 8, Epoch 736, Loss: 0.7574748396873474, Final Batch Loss: 0.3414170444011688\n",
      "Subject 8, Epoch 737, Loss: 1.0898782908916473, Final Batch Loss: 0.5282942056655884\n",
      "Subject 8, Epoch 738, Loss: 0.7705480009317398, Final Batch Loss: 0.2891862690448761\n",
      "Subject 8, Epoch 739, Loss: 0.9091675877571106, Final Batch Loss: 0.31123945116996765\n",
      "Subject 8, Epoch 740, Loss: 0.7357181757688522, Final Batch Loss: 0.27132952213287354\n",
      "Subject 8, Epoch 741, Loss: 0.7187743932008743, Final Batch Loss: 0.21445401012897491\n",
      "Subject 8, Epoch 742, Loss: 0.697669118642807, Final Batch Loss: 0.16191630065441132\n",
      "Subject 8, Epoch 743, Loss: 0.8303093910217285, Final Batch Loss: 0.2951148450374603\n",
      "Subject 8, Epoch 744, Loss: 0.7917735874652863, Final Batch Loss: 0.27606824040412903\n",
      "Subject 8, Epoch 745, Loss: 0.7850425839424133, Final Batch Loss: 0.2053607553243637\n",
      "Subject 8, Epoch 746, Loss: 0.8466919213533401, Final Batch Loss: 0.3463198244571686\n",
      "Subject 8, Epoch 747, Loss: 0.7082129865884781, Final Batch Loss: 0.1756940633058548\n",
      "Subject 8, Epoch 748, Loss: 0.6722843945026398, Final Batch Loss: 0.17446014285087585\n",
      "Subject 8, Epoch 749, Loss: 0.6633329689502716, Final Batch Loss: 0.17998528480529785\n",
      "Subject 8, Epoch 750, Loss: 0.7076594084501266, Final Batch Loss: 0.21565508842468262\n",
      "Subject 8, Epoch 751, Loss: 0.8217074275016785, Final Batch Loss: 0.22164618968963623\n",
      "Subject 8, Epoch 752, Loss: 0.7876735180616379, Final Batch Loss: 0.318178653717041\n",
      "Subject 8, Epoch 753, Loss: 0.7936932742595673, Final Batch Loss: 0.15221595764160156\n",
      "Subject 8, Epoch 754, Loss: 0.696227103471756, Final Batch Loss: 0.1872052103281021\n",
      "Subject 8, Epoch 755, Loss: 0.7386055290699005, Final Batch Loss: 0.20852316915988922\n",
      "Subject 8, Epoch 756, Loss: 0.6421836018562317, Final Batch Loss: 0.20639096200466156\n",
      "Subject 8, Epoch 757, Loss: 0.7648518234491348, Final Batch Loss: 0.2842382490634918\n",
      "Subject 8, Epoch 758, Loss: 0.7966093271970749, Final Batch Loss: 0.21567286550998688\n",
      "Subject 8, Epoch 759, Loss: 0.7282992154359818, Final Batch Loss: 0.24511730670928955\n",
      "Subject 8, Epoch 760, Loss: 0.7616610527038574, Final Batch Loss: 0.23886321485042572\n",
      "Subject 8, Epoch 761, Loss: 0.8695916384458542, Final Batch Loss: 0.24590297043323517\n",
      "Subject 8, Epoch 762, Loss: 0.6631685048341751, Final Batch Loss: 0.199782133102417\n",
      "Subject 8, Epoch 763, Loss: 0.8370509743690491, Final Batch Loss: 0.29914844036102295\n",
      "Subject 8, Epoch 764, Loss: 0.9669426679611206, Final Batch Loss: 0.4155622720718384\n",
      "Subject 8, Epoch 765, Loss: 0.888578325510025, Final Batch Loss: 0.2719694674015045\n",
      "Subject 8, Epoch 766, Loss: 0.7765412032604218, Final Batch Loss: 0.2684190571308136\n",
      "Subject 8, Epoch 767, Loss: 0.7973828464746475, Final Batch Loss: 0.3268580138683319\n",
      "Subject 8, Epoch 768, Loss: 0.7126937210559845, Final Batch Loss: 0.17638608813285828\n",
      "Subject 8, Epoch 769, Loss: 0.68098483979702, Final Batch Loss: 0.293526291847229\n",
      "Subject 8, Epoch 770, Loss: 0.6218753010034561, Final Batch Loss: 0.20937882363796234\n",
      "Subject 8, Epoch 771, Loss: 0.5925281494855881, Final Batch Loss: 0.17475616931915283\n",
      "Subject 8, Epoch 772, Loss: 0.8308192193508148, Final Batch Loss: 0.2259947955608368\n",
      "Subject 8, Epoch 773, Loss: 0.7495741248130798, Final Batch Loss: 0.24518753588199615\n",
      "Subject 8, Epoch 774, Loss: 0.6786224544048309, Final Batch Loss: 0.19370011985301971\n",
      "Subject 8, Epoch 775, Loss: 0.7851491570472717, Final Batch Loss: 0.2928657829761505\n",
      "Subject 8, Epoch 776, Loss: 0.6465774327516556, Final Batch Loss: 0.26391881704330444\n",
      "Subject 8, Epoch 777, Loss: 0.7496083974838257, Final Batch Loss: 0.2737398147583008\n",
      "Subject 8, Epoch 778, Loss: 0.9027301669120789, Final Batch Loss: 0.28183263540267944\n",
      "Subject 8, Epoch 779, Loss: 0.6319259852170944, Final Batch Loss: 0.2636314928531647\n",
      "Subject 8, Epoch 780, Loss: 0.707932636141777, Final Batch Loss: 0.23826457560062408\n",
      "Subject 8, Epoch 781, Loss: 0.8121428787708282, Final Batch Loss: 0.255635529756546\n",
      "Subject 8, Epoch 782, Loss: 0.7538185119628906, Final Batch Loss: 0.22365723550319672\n",
      "Subject 8, Epoch 783, Loss: 0.7518460154533386, Final Batch Loss: 0.20276384055614471\n",
      "Subject 8, Epoch 784, Loss: 0.7114416509866714, Final Batch Loss: 0.2717399597167969\n",
      "Subject 8, Epoch 785, Loss: 0.8175799995660782, Final Batch Loss: 0.2934565246105194\n",
      "Subject 8, Epoch 786, Loss: 0.7483564615249634, Final Batch Loss: 0.236338809132576\n",
      "Subject 8, Epoch 787, Loss: 0.7253261059522629, Final Batch Loss: 0.2276667058467865\n",
      "Subject 8, Epoch 788, Loss: 0.7563119232654572, Final Batch Loss: 0.21700230240821838\n",
      "Subject 8, Epoch 789, Loss: 0.7860245257616043, Final Batch Loss: 0.2342028021812439\n",
      "Subject 8, Epoch 790, Loss: 0.6095144897699356, Final Batch Loss: 0.19898828864097595\n",
      "Subject 8, Epoch 791, Loss: 0.8416028618812561, Final Batch Loss: 0.22822123765945435\n",
      "Subject 8, Epoch 792, Loss: 0.6648826450109482, Final Batch Loss: 0.26521044969558716\n",
      "Subject 8, Epoch 793, Loss: 0.6293857991695404, Final Batch Loss: 0.22201058268547058\n",
      "Subject 8, Epoch 794, Loss: 0.6698670834302902, Final Batch Loss: 0.2501472234725952\n",
      "Subject 8, Epoch 795, Loss: 0.6515639126300812, Final Batch Loss: 0.22776177525520325\n",
      "Subject 8, Epoch 796, Loss: 0.7534247636795044, Final Batch Loss: 0.22137700021266937\n",
      "Subject 8, Epoch 797, Loss: 0.7449741661548615, Final Batch Loss: 0.32087087631225586\n",
      "Subject 8, Epoch 798, Loss: 0.6923568546772003, Final Batch Loss: 0.2522536516189575\n",
      "Subject 8, Epoch 799, Loss: 0.7848295420408249, Final Batch Loss: 0.21027709543704987\n",
      "Subject 8, Epoch 800, Loss: 0.6973410248756409, Final Batch Loss: 0.3019581735134125\n",
      "Subject 8, Epoch 801, Loss: 0.7562170475721359, Final Batch Loss: 0.26357224583625793\n",
      "Subject 8, Epoch 802, Loss: 0.8330264687538147, Final Batch Loss: 0.3574790954589844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 803, Loss: 0.5803971961140633, Final Batch Loss: 0.12179236859083176\n",
      "Subject 8, Epoch 804, Loss: 0.6503608971834183, Final Batch Loss: 0.1670512855052948\n",
      "Subject 8, Epoch 805, Loss: 0.7037558555603027, Final Batch Loss: 0.22917701303958893\n",
      "Subject 8, Epoch 806, Loss: 0.7223429828882217, Final Batch Loss: 0.223456010222435\n",
      "Subject 8, Epoch 807, Loss: 0.784713551402092, Final Batch Loss: 0.17897722125053406\n",
      "Subject 8, Epoch 808, Loss: 0.8767875581979752, Final Batch Loss: 0.23947174847126007\n",
      "Subject 8, Epoch 809, Loss: 0.7492874562740326, Final Batch Loss: 0.26570844650268555\n",
      "Subject 8, Epoch 810, Loss: 0.681459903717041, Final Batch Loss: 0.15389478206634521\n",
      "Subject 8, Epoch 811, Loss: 0.6453482657670975, Final Batch Loss: 0.1636730283498764\n",
      "Subject 8, Epoch 812, Loss: 0.7405093312263489, Final Batch Loss: 0.23345749080181122\n",
      "Subject 8, Epoch 813, Loss: 0.587664358317852, Final Batch Loss: 0.1221284493803978\n",
      "Subject 8, Epoch 814, Loss: 0.5948490053415298, Final Batch Loss: 0.13069571554660797\n",
      "Subject 8, Epoch 815, Loss: 0.5662980228662491, Final Batch Loss: 0.2034570872783661\n",
      "Subject 8, Epoch 816, Loss: 0.9130125045776367, Final Batch Loss: 0.389987975358963\n",
      "Subject 8, Epoch 817, Loss: 0.6293692886829376, Final Batch Loss: 0.213975191116333\n",
      "Subject 8, Epoch 818, Loss: 0.7852987796068192, Final Batch Loss: 0.21183882653713226\n",
      "Subject 8, Epoch 819, Loss: 0.6134761720895767, Final Batch Loss: 0.2672486901283264\n",
      "Subject 8, Epoch 820, Loss: 0.5817174315452576, Final Batch Loss: 0.14350247383117676\n",
      "Subject 8, Epoch 821, Loss: 0.6473040878772736, Final Batch Loss: 0.2361963540315628\n",
      "Subject 8, Epoch 822, Loss: 0.5948613584041595, Final Batch Loss: 0.18380558490753174\n",
      "Subject 8, Epoch 823, Loss: 0.9128730893135071, Final Batch Loss: 0.30335336923599243\n",
      "Subject 8, Epoch 824, Loss: 0.7567855268716812, Final Batch Loss: 0.24037478864192963\n",
      "Subject 8, Epoch 825, Loss: 0.5968601852655411, Final Batch Loss: 0.2143038958311081\n",
      "Subject 8, Epoch 826, Loss: 0.7165398895740509, Final Batch Loss: 0.234934002161026\n",
      "Subject 8, Epoch 827, Loss: 0.6548501402139664, Final Batch Loss: 0.18860745429992676\n",
      "Subject 8, Epoch 828, Loss: 0.8017187118530273, Final Batch Loss: 0.2729295492172241\n",
      "Subject 8, Epoch 829, Loss: 0.7589937746524811, Final Batch Loss: 0.32575440406799316\n",
      "Subject 8, Epoch 830, Loss: 0.7112554758787155, Final Batch Loss: 0.17587877810001373\n",
      "Subject 8, Epoch 831, Loss: 0.6275958567857742, Final Batch Loss: 0.19122329354286194\n",
      "Subject 8, Epoch 832, Loss: 0.7136201709508896, Final Batch Loss: 0.23613429069519043\n",
      "Subject 8, Epoch 833, Loss: 0.5965294688940048, Final Batch Loss: 0.2370268553495407\n",
      "Subject 8, Epoch 834, Loss: 0.5809283256530762, Final Batch Loss: 0.15930074453353882\n",
      "Subject 8, Epoch 835, Loss: 0.5298925191164017, Final Batch Loss: 0.19632694125175476\n",
      "Subject 8, Epoch 836, Loss: 0.5383805334568024, Final Batch Loss: 0.1683236062526703\n",
      "Subject 8, Epoch 837, Loss: 0.778452605009079, Final Batch Loss: 0.21497662365436554\n",
      "Subject 8, Epoch 838, Loss: 0.6703723073005676, Final Batch Loss: 0.26024216413497925\n",
      "Subject 8, Epoch 839, Loss: 0.6532205641269684, Final Batch Loss: 0.21315883100032806\n",
      "Subject 8, Epoch 840, Loss: 0.5743983536958694, Final Batch Loss: 0.2675420939922333\n",
      "Subject 8, Epoch 841, Loss: 0.5989466607570648, Final Batch Loss: 0.14679236710071564\n",
      "Subject 8, Epoch 842, Loss: 0.6651190668344498, Final Batch Loss: 0.26545748114585876\n",
      "Subject 8, Epoch 843, Loss: 0.6711569130420685, Final Batch Loss: 0.20431451499462128\n",
      "Subject 8, Epoch 844, Loss: 0.573027104139328, Final Batch Loss: 0.22041447460651398\n",
      "Subject 8, Epoch 845, Loss: 0.6242313385009766, Final Batch Loss: 0.16468097269535065\n",
      "Subject 8, Epoch 846, Loss: 0.5179905295372009, Final Batch Loss: 0.18947525322437286\n",
      "Subject 8, Epoch 847, Loss: 0.7411853075027466, Final Batch Loss: 0.13450363278388977\n",
      "Subject 8, Epoch 848, Loss: 0.5900192558765411, Final Batch Loss: 0.24633437395095825\n",
      "Subject 8, Epoch 849, Loss: 0.5610336810350418, Final Batch Loss: 0.20591391623020172\n",
      "Subject 8, Epoch 850, Loss: 0.4701959118247032, Final Batch Loss: 0.1865290403366089\n",
      "Subject 8, Epoch 851, Loss: 0.6732331812381744, Final Batch Loss: 0.20938105881214142\n",
      "Subject 8, Epoch 852, Loss: 0.6205205321311951, Final Batch Loss: 0.148471400141716\n",
      "Subject 8, Epoch 853, Loss: 0.5865694656968117, Final Batch Loss: 0.156992107629776\n",
      "Subject 8, Epoch 854, Loss: 0.6529815495014191, Final Batch Loss: 0.2597919702529907\n",
      "Subject 8, Epoch 855, Loss: 0.5481358915567398, Final Batch Loss: 0.21609582006931305\n",
      "Subject 8, Epoch 856, Loss: 0.5625750124454498, Final Batch Loss: 0.1869281530380249\n",
      "Subject 8, Epoch 857, Loss: 0.5461313277482986, Final Batch Loss: 0.1187315583229065\n",
      "Subject 8, Epoch 858, Loss: 0.6257336139678955, Final Batch Loss: 0.27015453577041626\n",
      "Subject 8, Epoch 859, Loss: 0.6165096759796143, Final Batch Loss: 0.23781341314315796\n",
      "Subject 8, Epoch 860, Loss: 0.7510157823562622, Final Batch Loss: 0.2547813057899475\n",
      "Subject 8, Epoch 861, Loss: 0.6358984708786011, Final Batch Loss: 0.17500531673431396\n",
      "Subject 8, Epoch 862, Loss: 0.6912784725427628, Final Batch Loss: 0.13048897683620453\n",
      "Subject 8, Epoch 863, Loss: 0.6861272752285004, Final Batch Loss: 0.23024997115135193\n",
      "Subject 8, Epoch 864, Loss: 0.616779088973999, Final Batch Loss: 0.18189842998981476\n",
      "Subject 8, Epoch 865, Loss: 0.6580632477998734, Final Batch Loss: 0.18332520127296448\n",
      "Subject 8, Epoch 866, Loss: 0.7411728650331497, Final Batch Loss: 0.38341769576072693\n",
      "Subject 8, Epoch 867, Loss: 0.6129116266965866, Final Batch Loss: 0.20637111365795135\n",
      "Subject 8, Epoch 868, Loss: 0.6203714311122894, Final Batch Loss: 0.2786632478237152\n",
      "Subject 8, Epoch 869, Loss: 0.682173952460289, Final Batch Loss: 0.24358493089675903\n",
      "Subject 8, Epoch 870, Loss: 0.5923622995615005, Final Batch Loss: 0.19147957861423492\n",
      "Subject 8, Epoch 871, Loss: 0.7431778162717819, Final Batch Loss: 0.2621805965900421\n",
      "Subject 8, Epoch 872, Loss: 0.7724307477474213, Final Batch Loss: 0.37804678082466125\n",
      "Subject 8, Epoch 873, Loss: 0.6757689565420151, Final Batch Loss: 0.31317397952079773\n",
      "Subject 8, Epoch 874, Loss: 0.5254863053560257, Final Batch Loss: 0.18374310433864594\n",
      "Subject 8, Epoch 875, Loss: 0.5690198242664337, Final Batch Loss: 0.18168550729751587\n",
      "Subject 8, Epoch 876, Loss: 0.4778171181678772, Final Batch Loss: 0.15812428295612335\n",
      "Subject 8, Epoch 877, Loss: 0.5548947304487228, Final Batch Loss: 0.19371499121189117\n",
      "Subject 8, Epoch 878, Loss: 0.6908207535743713, Final Batch Loss: 0.17409613728523254\n",
      "Subject 8, Epoch 879, Loss: 0.6783057153224945, Final Batch Loss: 0.20033559203147888\n",
      "Subject 8, Epoch 880, Loss: 0.46218714118003845, Final Batch Loss: 0.13049988448619843\n",
      "Subject 8, Epoch 881, Loss: 0.5917298197746277, Final Batch Loss: 0.24451583623886108\n",
      "Subject 8, Epoch 882, Loss: 0.6738851815462112, Final Batch Loss: 0.2079283446073532\n",
      "Subject 8, Epoch 883, Loss: 0.49357131123542786, Final Batch Loss: 0.1876593679189682\n",
      "Subject 8, Epoch 884, Loss: 0.6510187983512878, Final Batch Loss: 0.2507222890853882\n",
      "Subject 8, Epoch 885, Loss: 0.5892307162284851, Final Batch Loss: 0.24376681447029114\n",
      "Subject 8, Epoch 886, Loss: 0.5855770260095596, Final Batch Loss: 0.20940810441970825\n",
      "Subject 8, Epoch 887, Loss: 0.6838023066520691, Final Batch Loss: 0.14658793807029724\n",
      "Subject 8, Epoch 888, Loss: 0.6070005148649216, Final Batch Loss: 0.19088390469551086\n",
      "Subject 8, Epoch 889, Loss: 0.6981728821992874, Final Batch Loss: 0.389026015996933\n",
      "Subject 8, Epoch 890, Loss: 0.5556023865938187, Final Batch Loss: 0.1544569432735443\n",
      "Subject 8, Epoch 891, Loss: 0.7107198089361191, Final Batch Loss: 0.19921688735485077\n",
      "Subject 8, Epoch 892, Loss: 0.5586143434047699, Final Batch Loss: 0.15296950936317444\n",
      "Subject 8, Epoch 893, Loss: 0.48625408113002777, Final Batch Loss: 0.16055721044540405\n",
      "Subject 8, Epoch 894, Loss: 0.46777820587158203, Final Batch Loss: 0.14881013333797455\n",
      "Subject 8, Epoch 895, Loss: 0.7248694002628326, Final Batch Loss: 0.25318941473960876\n",
      "Subject 8, Epoch 896, Loss: 0.5003723055124283, Final Batch Loss: 0.1689106822013855\n",
      "Subject 8, Epoch 897, Loss: 0.581009030342102, Final Batch Loss: 0.1908557116985321\n",
      "Subject 8, Epoch 898, Loss: 0.5398835986852646, Final Batch Loss: 0.21521756052970886\n",
      "Subject 8, Epoch 899, Loss: 0.5486045777797699, Final Batch Loss: 0.1158507764339447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 8, Epoch 900, Loss: 0.5014457777142525, Final Batch Loss: 0.21562744677066803\n",
      "Subject 8, Epoch 901, Loss: 0.4448414444923401, Final Batch Loss: 0.1479317992925644\n",
      "Subject 8, Epoch 902, Loss: 0.5338214784860611, Final Batch Loss: 0.17928868532180786\n",
      "Subject 8, Epoch 903, Loss: 0.6664931327104568, Final Batch Loss: 0.2557041347026825\n",
      "Subject 8, Epoch 904, Loss: 0.532983623445034, Final Batch Loss: 0.20516414940357208\n",
      "Subject 8, Epoch 905, Loss: 0.5010389685630798, Final Batch Loss: 0.1578538864850998\n",
      "Subject 8, Epoch 906, Loss: 0.6351988762617111, Final Batch Loss: 0.12082917988300323\n",
      "Subject 8, Epoch 907, Loss: 0.5551815927028656, Final Batch Loss: 0.3099677264690399\n",
      "Subject 8, Epoch 908, Loss: 0.668864294886589, Final Batch Loss: 0.23387733101844788\n",
      "Subject 8, Epoch 909, Loss: 0.47496581077575684, Final Batch Loss: 0.17258498072624207\n",
      "Subject 8, Epoch 910, Loss: 0.5324194431304932, Final Batch Loss: 0.13852143287658691\n",
      "Subject 8, Epoch 911, Loss: 0.4926956593990326, Final Batch Loss: 0.21840956807136536\n",
      "Subject 8, Epoch 912, Loss: 0.45829831063747406, Final Batch Loss: 0.13039344549179077\n",
      "Subject 8, Epoch 913, Loss: 0.48694639652967453, Final Batch Loss: 0.16168899834156036\n",
      "Subject 8, Epoch 914, Loss: 0.3634035065770149, Final Batch Loss: 0.10809636861085892\n",
      "Subject 8, Epoch 915, Loss: 0.4291427731513977, Final Batch Loss: 0.1246376633644104\n",
      "Subject 8, Epoch 916, Loss: 0.49975550174713135, Final Batch Loss: 0.11588943004608154\n",
      "Subject 8, Epoch 917, Loss: 0.5856195092201233, Final Batch Loss: 0.17830206453800201\n",
      "Subject 8, Epoch 918, Loss: 0.576506718993187, Final Batch Loss: 0.12342438101768494\n",
      "Subject 8, Epoch 919, Loss: 0.510845422744751, Final Batch Loss: 0.1428024023771286\n",
      "Subject 8, Epoch 920, Loss: 0.6256929785013199, Final Batch Loss: 0.1814744621515274\n",
      "Subject 8, Epoch 921, Loss: 0.5826733857393265, Final Batch Loss: 0.2004922777414322\n",
      "Subject 8, Epoch 922, Loss: 0.6360687464475632, Final Batch Loss: 0.19240696728229523\n",
      "Subject 8, Epoch 923, Loss: 0.49956969916820526, Final Batch Loss: 0.16013720631599426\n",
      "Subject 8, Epoch 924, Loss: 0.5481273680925369, Final Batch Loss: 0.19293425977230072\n",
      "Subject 8, Epoch 925, Loss: 0.601419985294342, Final Batch Loss: 0.21466225385665894\n",
      "Subject 8, Epoch 926, Loss: 0.5210333615541458, Final Batch Loss: 0.1515171378850937\n",
      "Subject 8, Epoch 927, Loss: 0.5612150132656097, Final Batch Loss: 0.19347570836544037\n",
      "Subject 8, Epoch 928, Loss: 0.4342994913458824, Final Batch Loss: 0.10365503281354904\n",
      "Subject 8, Epoch 929, Loss: 0.6653004810214043, Final Batch Loss: 0.4435674250125885\n",
      "Subject 8, Epoch 930, Loss: 0.5985390245914459, Final Batch Loss: 0.18130375444889069\n",
      "Subject 8, Epoch 931, Loss: 0.3819311335682869, Final Batch Loss: 0.15738354623317719\n",
      "Subject 8, Epoch 932, Loss: 0.6068485826253891, Final Batch Loss: 0.22268161177635193\n",
      "Subject 8, Epoch 933, Loss: 0.5760635584592819, Final Batch Loss: 0.1993895024061203\n",
      "Subject 8, Epoch 934, Loss: 0.48028209805488586, Final Batch Loss: 0.16092607378959656\n",
      "Subject 8, Epoch 935, Loss: 0.5276543349027634, Final Batch Loss: 0.15994596481323242\n",
      "Subject 8, Epoch 936, Loss: 0.5467991977930069, Final Batch Loss: 0.14313971996307373\n",
      "Subject 8, Epoch 937, Loss: 0.5837808549404144, Final Batch Loss: 0.16186930239200592\n",
      "Subject 8, Epoch 938, Loss: 0.5652223601937294, Final Batch Loss: 0.23083540797233582\n",
      "Subject 8, Epoch 939, Loss: 0.6773272454738617, Final Batch Loss: 0.22569575905799866\n",
      "Subject 8, Epoch 940, Loss: 0.5943817049264908, Final Batch Loss: 0.14006467163562775\n",
      "Subject 8, Epoch 941, Loss: 0.5511674731969833, Final Batch Loss: 0.20800067484378815\n",
      "Subject 8, Epoch 942, Loss: 0.5867103189229965, Final Batch Loss: 0.19269675016403198\n",
      "Subject 8, Epoch 943, Loss: 0.44780559092760086, Final Batch Loss: 0.16821280121803284\n",
      "Subject 8, Epoch 944, Loss: 0.5652712285518646, Final Batch Loss: 0.19223910570144653\n",
      "Subject 8, Epoch 945, Loss: 0.4665975123643875, Final Batch Loss: 0.14644204080104828\n",
      "Subject 8, Epoch 946, Loss: 0.4844024032354355, Final Batch Loss: 0.13629929721355438\n",
      "Subject 8, Epoch 947, Loss: 0.46793851256370544, Final Batch Loss: 0.13110333681106567\n",
      "Subject 8, Epoch 948, Loss: 0.5221936628222466, Final Batch Loss: 0.10828893631696701\n",
      "Subject 8, Epoch 949, Loss: 0.5236168727278709, Final Batch Loss: 0.12490025907754898\n",
      "Subject 8, Epoch 950, Loss: 0.5872625857591629, Final Batch Loss: 0.1617153137922287\n",
      "Subject 8, Epoch 951, Loss: 0.534492090344429, Final Batch Loss: 0.16583234071731567\n",
      "Subject 8, Epoch 952, Loss: 0.45027025043964386, Final Batch Loss: 0.18468281626701355\n",
      "Subject 8, Epoch 953, Loss: 0.4778529033064842, Final Batch Loss: 0.12098369747400284\n",
      "Subject 8, Epoch 954, Loss: 0.427265465259552, Final Batch Loss: 0.11003699898719788\n",
      "Subject 8, Epoch 955, Loss: 0.6291343867778778, Final Batch Loss: 0.23850621283054352\n",
      "Subject 8, Epoch 956, Loss: 0.4508625194430351, Final Batch Loss: 0.11332448571920395\n",
      "Subject 8, Epoch 957, Loss: 0.37762079387903214, Final Batch Loss: 0.1543421447277069\n",
      "Subject 8, Epoch 958, Loss: 0.7508727610111237, Final Batch Loss: 0.36838585138320923\n",
      "Subject 8, Epoch 959, Loss: 0.4427414536476135, Final Batch Loss: 0.14121292531490326\n",
      "Subject 8, Epoch 960, Loss: 0.5492735207080841, Final Batch Loss: 0.19303753972053528\n",
      "Subject 8, Epoch 961, Loss: 0.6270559132099152, Final Batch Loss: 0.2613081634044647\n",
      "Subject 8, Epoch 962, Loss: 0.3669619709253311, Final Batch Loss: 0.09785563498735428\n",
      "Subject 8, Epoch 963, Loss: 0.5422881543636322, Final Batch Loss: 0.24190109968185425\n",
      "Subject 8, Epoch 964, Loss: 0.5240339487791061, Final Batch Loss: 0.11298689246177673\n",
      "Subject 8, Epoch 965, Loss: 0.43449877947568893, Final Batch Loss: 0.1551164984703064\n",
      "Subject 8, Epoch 966, Loss: 0.6176809072494507, Final Batch Loss: 0.21913142502307892\n",
      "Subject 8, Epoch 967, Loss: 0.42572787404060364, Final Batch Loss: 0.15609708428382874\n",
      "Subject 8, Epoch 968, Loss: 0.487746462225914, Final Batch Loss: 0.15292446315288544\n",
      "Subject 8, Epoch 969, Loss: 0.657118484377861, Final Batch Loss: 0.23632624745368958\n",
      "Subject 8, Epoch 970, Loss: 0.48086127638816833, Final Batch Loss: 0.2133592963218689\n",
      "Subject 8, Epoch 971, Loss: 0.3728942573070526, Final Batch Loss: 0.11013485491275787\n",
      "Subject 8, Epoch 972, Loss: 0.5418311655521393, Final Batch Loss: 0.12685589492321014\n",
      "Subject 8, Epoch 973, Loss: 0.5663483589887619, Final Batch Loss: 0.17702588438987732\n",
      "Subject 8, Epoch 974, Loss: 0.49612465500831604, Final Batch Loss: 0.1935216635465622\n",
      "Subject 8, Epoch 975, Loss: 0.44795820116996765, Final Batch Loss: 0.14678168296813965\n",
      "Subject 8, Epoch 976, Loss: 0.3752262070775032, Final Batch Loss: 0.10472974926233292\n",
      "Subject 8, Epoch 977, Loss: 0.39517439901828766, Final Batch Loss: 0.12307035177946091\n",
      "Subject 8, Epoch 978, Loss: 0.5865371972322464, Final Batch Loss: 0.2029953896999359\n",
      "Subject 8, Epoch 979, Loss: 0.4255013018846512, Final Batch Loss: 0.14514340460300446\n",
      "Subject 8, Epoch 980, Loss: 0.5419358462095261, Final Batch Loss: 0.3060818910598755\n",
      "Subject 8, Epoch 981, Loss: 0.5541585981845856, Final Batch Loss: 0.15662993490695953\n",
      "Subject 8, Epoch 982, Loss: 0.34262558817863464, Final Batch Loss: 0.1287621408700943\n",
      "Subject 8, Epoch 983, Loss: 0.5496847555041313, Final Batch Loss: 0.21494640409946442\n",
      "Subject 8, Epoch 984, Loss: 0.5183761417865753, Final Batch Loss: 0.2783617377281189\n",
      "Subject 8, Epoch 985, Loss: 0.5888039842247963, Final Batch Loss: 0.27248600125312805\n",
      "Subject 8, Epoch 986, Loss: 0.505243793129921, Final Batch Loss: 0.17430506646633148\n",
      "Subject 8, Epoch 987, Loss: 0.41677142679691315, Final Batch Loss: 0.1747894138097763\n",
      "Subject 8, Epoch 988, Loss: 0.5079564452171326, Final Batch Loss: 0.10366173088550568\n",
      "Subject 8, Epoch 989, Loss: 0.44412052631378174, Final Batch Loss: 0.14178946614265442\n",
      "Subject 8, Epoch 990, Loss: 0.6728546917438507, Final Batch Loss: 0.21250438690185547\n",
      "Subject 8, Epoch 991, Loss: 0.45458176732063293, Final Batch Loss: 0.13924407958984375\n",
      "Subject 8, Epoch 992, Loss: 0.6082242131233215, Final Batch Loss: 0.18591448664665222\n",
      "Subject 8, Epoch 993, Loss: 0.29639463871717453, Final Batch Loss: 0.09886233508586884\n",
      "Subject 8, Epoch 994, Loss: 0.4205126017332077, Final Batch Loss: 0.11113671958446503\n",
      "Subject 8, Epoch 995, Loss: 0.43678339570760727, Final Batch Loss: 0.11093083769083023\n",
      "Subject 8, Epoch 996, Loss: 0.577620193362236, Final Batch Loss: 0.13410584628582\n",
      "Subject 8, Epoch 997, Loss: 0.4238724671304226, Final Batch Loss: 0.05942229554057121\n",
      "Subject 8, Epoch 998, Loss: 0.53032585978508, Final Batch Loss: 0.13311132788658142\n",
      "Subject 8, Epoch 999, Loss: 0.5349288135766983, Final Batch Loss: 0.1859811544418335\n",
      "Subject 8, Epoch 1000, Loss: 0.6886086463928223, Final Batch Loss: 0.30528566241264343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 1, Loss: 7.306611657142639, Final Batch Loss: 1.8866897821426392\n",
      "Subject 9, Epoch 2, Loss: 7.242794632911682, Final Batch Loss: 1.8326771259307861\n",
      "Subject 9, Epoch 3, Loss: 7.2208709716796875, Final Batch Loss: 1.8113051652908325\n",
      "Subject 9, Epoch 4, Loss: 7.206335425376892, Final Batch Loss: 1.8003363609313965\n",
      "Subject 9, Epoch 5, Loss: 7.230687499046326, Final Batch Loss: 1.8333141803741455\n",
      "Subject 9, Epoch 6, Loss: 7.172323942184448, Final Batch Loss: 1.7782807350158691\n",
      "Subject 9, Epoch 7, Loss: 7.177950024604797, Final Batch Loss: 1.800374984741211\n",
      "Subject 9, Epoch 8, Loss: 7.1366190910339355, Final Batch Loss: 1.7722656726837158\n",
      "Subject 9, Epoch 9, Loss: 7.156048893928528, Final Batch Loss: 1.8117221593856812\n",
      "Subject 9, Epoch 10, Loss: 7.134728312492371, Final Batch Loss: 1.806718349456787\n",
      "Subject 9, Epoch 11, Loss: 7.095849871635437, Final Batch Loss: 1.7929905652999878\n",
      "Subject 9, Epoch 12, Loss: 7.053700804710388, Final Batch Loss: 1.7779850959777832\n",
      "Subject 9, Epoch 13, Loss: 6.995422124862671, Final Batch Loss: 1.7534748315811157\n",
      "Subject 9, Epoch 14, Loss: 6.967429876327515, Final Batch Loss: 1.7581385374069214\n",
      "Subject 9, Epoch 15, Loss: 6.842849016189575, Final Batch Loss: 1.6861907243728638\n",
      "Subject 9, Epoch 16, Loss: 6.722057819366455, Final Batch Loss: 1.6211868524551392\n",
      "Subject 9, Epoch 17, Loss: 6.5596699714660645, Final Batch Loss: 1.5297000408172607\n",
      "Subject 9, Epoch 18, Loss: 6.507641911506653, Final Batch Loss: 1.629982352256775\n",
      "Subject 9, Epoch 19, Loss: 6.326215505599976, Final Batch Loss: 1.530849814414978\n",
      "Subject 9, Epoch 20, Loss: 6.312802195549011, Final Batch Loss: 1.5900607109069824\n",
      "Subject 9, Epoch 21, Loss: 6.259127378463745, Final Batch Loss: 1.6100269556045532\n",
      "Subject 9, Epoch 22, Loss: 5.94864821434021, Final Batch Loss: 1.4469337463378906\n",
      "Subject 9, Epoch 23, Loss: 5.906461715698242, Final Batch Loss: 1.4708095788955688\n",
      "Subject 9, Epoch 24, Loss: 5.8107287883758545, Final Batch Loss: 1.438302755355835\n",
      "Subject 9, Epoch 25, Loss: 5.607232213020325, Final Batch Loss: 1.3440139293670654\n",
      "Subject 9, Epoch 26, Loss: 5.577162146568298, Final Batch Loss: 1.3066420555114746\n",
      "Subject 9, Epoch 27, Loss: 5.4462974071502686, Final Batch Loss: 1.2753233909606934\n",
      "Subject 9, Epoch 28, Loss: 5.5315282344818115, Final Batch Loss: 1.4283385276794434\n",
      "Subject 9, Epoch 29, Loss: 5.168819308280945, Final Batch Loss: 1.138941764831543\n",
      "Subject 9, Epoch 30, Loss: 5.226099729537964, Final Batch Loss: 1.1569809913635254\n",
      "Subject 9, Epoch 31, Loss: 5.162814140319824, Final Batch Loss: 1.2023916244506836\n",
      "Subject 9, Epoch 32, Loss: 5.156451344490051, Final Batch Loss: 1.2453932762145996\n",
      "Subject 9, Epoch 33, Loss: 4.97573709487915, Final Batch Loss: 1.159403920173645\n",
      "Subject 9, Epoch 34, Loss: 5.035181283950806, Final Batch Loss: 1.2822946310043335\n",
      "Subject 9, Epoch 35, Loss: 5.137554168701172, Final Batch Loss: 1.3483562469482422\n",
      "Subject 9, Epoch 36, Loss: 4.8401981592178345, Final Batch Loss: 1.1076511144638062\n",
      "Subject 9, Epoch 37, Loss: 4.891981720924377, Final Batch Loss: 1.1140931844711304\n",
      "Subject 9, Epoch 38, Loss: 4.987502455711365, Final Batch Loss: 1.3665176630020142\n",
      "Subject 9, Epoch 39, Loss: 4.838214755058289, Final Batch Loss: 1.2088690996170044\n",
      "Subject 9, Epoch 40, Loss: 4.738813877105713, Final Batch Loss: 1.2270607948303223\n",
      "Subject 9, Epoch 41, Loss: 4.626156568527222, Final Batch Loss: 1.139642357826233\n",
      "Subject 9, Epoch 42, Loss: 4.653034329414368, Final Batch Loss: 1.2032703161239624\n",
      "Subject 9, Epoch 43, Loss: 4.736846446990967, Final Batch Loss: 1.350106120109558\n",
      "Subject 9, Epoch 44, Loss: 4.652641654014587, Final Batch Loss: 1.2436301708221436\n",
      "Subject 9, Epoch 45, Loss: 4.344053268432617, Final Batch Loss: 1.0395170450210571\n",
      "Subject 9, Epoch 46, Loss: 4.50534188747406, Final Batch Loss: 1.173891305923462\n",
      "Subject 9, Epoch 47, Loss: 4.501846194267273, Final Batch Loss: 1.040417194366455\n",
      "Subject 9, Epoch 48, Loss: 4.224391579627991, Final Batch Loss: 1.0023201704025269\n",
      "Subject 9, Epoch 49, Loss: 4.397545099258423, Final Batch Loss: 1.131776213645935\n",
      "Subject 9, Epoch 50, Loss: 4.261086940765381, Final Batch Loss: 1.0465534925460815\n",
      "Subject 9, Epoch 51, Loss: 4.158755660057068, Final Batch Loss: 1.0062671899795532\n",
      "Subject 9, Epoch 52, Loss: 3.948248028755188, Final Batch Loss: 0.8346942663192749\n",
      "Subject 9, Epoch 53, Loss: 4.287127375602722, Final Batch Loss: 1.1431829929351807\n",
      "Subject 9, Epoch 54, Loss: 3.953977346420288, Final Batch Loss: 0.9859834909439087\n",
      "Subject 9, Epoch 55, Loss: 3.805719554424286, Final Batch Loss: 0.9190704822540283\n",
      "Subject 9, Epoch 56, Loss: 3.8723440170288086, Final Batch Loss: 1.0659832954406738\n",
      "Subject 9, Epoch 57, Loss: 4.088522732257843, Final Batch Loss: 1.1901696920394897\n",
      "Subject 9, Epoch 58, Loss: 3.7509671449661255, Final Batch Loss: 0.8873714804649353\n",
      "Subject 9, Epoch 59, Loss: 3.932075798511505, Final Batch Loss: 1.0736583471298218\n",
      "Subject 9, Epoch 60, Loss: 3.5290136337280273, Final Batch Loss: 0.7069903612136841\n",
      "Subject 9, Epoch 61, Loss: 3.6586790680885315, Final Batch Loss: 0.909554660320282\n",
      "Subject 9, Epoch 62, Loss: 3.7247869968414307, Final Batch Loss: 1.0051850080490112\n",
      "Subject 9, Epoch 63, Loss: 3.6738280653953552, Final Batch Loss: 1.000331163406372\n",
      "Subject 9, Epoch 64, Loss: 3.4748151898384094, Final Batch Loss: 0.6881080269813538\n",
      "Subject 9, Epoch 65, Loss: 3.4509313106536865, Final Batch Loss: 0.8149786591529846\n",
      "Subject 9, Epoch 66, Loss: 3.822133779525757, Final Batch Loss: 1.1406474113464355\n",
      "Subject 9, Epoch 67, Loss: 3.5252784490585327, Final Batch Loss: 0.8698516488075256\n",
      "Subject 9, Epoch 68, Loss: 3.5840026140213013, Final Batch Loss: 0.8466255068778992\n",
      "Subject 9, Epoch 69, Loss: 3.5076680183410645, Final Batch Loss: 0.8208461999893188\n",
      "Subject 9, Epoch 70, Loss: 3.5597649216651917, Final Batch Loss: 0.916049599647522\n",
      "Subject 9, Epoch 71, Loss: 3.4106669425964355, Final Batch Loss: 0.841129720211029\n",
      "Subject 9, Epoch 72, Loss: 3.485020101070404, Final Batch Loss: 0.9261813759803772\n",
      "Subject 9, Epoch 73, Loss: 3.7102609872817993, Final Batch Loss: 1.0187907218933105\n",
      "Subject 9, Epoch 74, Loss: 3.607934057712555, Final Batch Loss: 1.1060478687286377\n",
      "Subject 9, Epoch 75, Loss: 3.4156678915023804, Final Batch Loss: 0.8638307452201843\n",
      "Subject 9, Epoch 76, Loss: 3.7258289456367493, Final Batch Loss: 1.2725863456726074\n",
      "Subject 9, Epoch 77, Loss: 3.2183554768562317, Final Batch Loss: 0.6869253516197205\n",
      "Subject 9, Epoch 78, Loss: 3.4195908904075623, Final Batch Loss: 1.0066050291061401\n",
      "Subject 9, Epoch 79, Loss: 3.184392035007477, Final Batch Loss: 0.617260754108429\n",
      "Subject 9, Epoch 80, Loss: 3.3326549530029297, Final Batch Loss: 0.7571095824241638\n",
      "Subject 9, Epoch 81, Loss: 3.2152256965637207, Final Batch Loss: 0.6710360050201416\n",
      "Subject 9, Epoch 82, Loss: 3.0991413593292236, Final Batch Loss: 0.6737701296806335\n",
      "Subject 9, Epoch 83, Loss: 3.1655858159065247, Final Batch Loss: 0.7133457064628601\n",
      "Subject 9, Epoch 84, Loss: 3.42006778717041, Final Batch Loss: 0.9934772253036499\n",
      "Subject 9, Epoch 85, Loss: 3.456898510456085, Final Batch Loss: 1.0348132848739624\n",
      "Subject 9, Epoch 86, Loss: 3.4045116305351257, Final Batch Loss: 1.0102019309997559\n",
      "Subject 9, Epoch 87, Loss: 3.210412621498108, Final Batch Loss: 0.7526166439056396\n",
      "Subject 9, Epoch 88, Loss: 3.409965693950653, Final Batch Loss: 0.998893141746521\n",
      "Subject 9, Epoch 89, Loss: 3.1999427676200867, Final Batch Loss: 0.8158784508705139\n",
      "Subject 9, Epoch 90, Loss: 3.219286024570465, Final Batch Loss: 0.7680951952934265\n",
      "Subject 9, Epoch 91, Loss: 3.3537721037864685, Final Batch Loss: 0.9934138655662537\n",
      "Subject 9, Epoch 92, Loss: 3.1971485018730164, Final Batch Loss: 0.7929368615150452\n",
      "Subject 9, Epoch 93, Loss: 3.4096354246139526, Final Batch Loss: 1.0950337648391724\n",
      "Subject 9, Epoch 94, Loss: 2.8175947070121765, Final Batch Loss: 0.5616146326065063\n",
      "Subject 9, Epoch 95, Loss: 3.095237970352173, Final Batch Loss: 0.7679040431976318\n",
      "Subject 9, Epoch 96, Loss: 3.309068441390991, Final Batch Loss: 0.9619934558868408\n",
      "Subject 9, Epoch 97, Loss: 3.0467856526374817, Final Batch Loss: 0.686978280544281\n",
      "Subject 9, Epoch 98, Loss: 2.9141205549240112, Final Batch Loss: 0.6613429188728333\n",
      "Subject 9, Epoch 99, Loss: 2.868509531021118, Final Batch Loss: 0.6728563904762268\n",
      "Subject 9, Epoch 100, Loss: 3.130653142929077, Final Batch Loss: 0.8534961938858032\n",
      "Subject 9, Epoch 101, Loss: 2.9693159461021423, Final Batch Loss: 0.7134649753570557\n",
      "Subject 9, Epoch 102, Loss: 2.8078925013542175, Final Batch Loss: 0.5869066715240479\n",
      "Subject 9, Epoch 103, Loss: 2.7990604639053345, Final Batch Loss: 0.5056478977203369\n",
      "Subject 9, Epoch 104, Loss: 3.2839421033859253, Final Batch Loss: 1.0466097593307495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 105, Loss: 2.9387624263763428, Final Batch Loss: 0.6664685606956482\n",
      "Subject 9, Epoch 106, Loss: 2.875955581665039, Final Batch Loss: 0.7272549271583557\n",
      "Subject 9, Epoch 107, Loss: 2.808295488357544, Final Batch Loss: 0.6662606000900269\n",
      "Subject 9, Epoch 108, Loss: 2.826058030128479, Final Batch Loss: 0.585269570350647\n",
      "Subject 9, Epoch 109, Loss: 2.652769982814789, Final Batch Loss: 0.5487010478973389\n",
      "Subject 9, Epoch 110, Loss: 2.6677364706993103, Final Batch Loss: 0.5697706341743469\n",
      "Subject 9, Epoch 111, Loss: 2.9561387300491333, Final Batch Loss: 0.8337016701698303\n",
      "Subject 9, Epoch 112, Loss: 2.5890680849552155, Final Batch Loss: 0.4585879147052765\n",
      "Subject 9, Epoch 113, Loss: 2.922001302242279, Final Batch Loss: 0.8253276348114014\n",
      "Subject 9, Epoch 114, Loss: 2.526124060153961, Final Batch Loss: 0.526701807975769\n",
      "Subject 9, Epoch 115, Loss: 2.9560763835906982, Final Batch Loss: 0.8888272643089294\n",
      "Subject 9, Epoch 116, Loss: 2.748916447162628, Final Batch Loss: 0.5520893931388855\n",
      "Subject 9, Epoch 117, Loss: 2.939961075782776, Final Batch Loss: 0.8734738826751709\n",
      "Subject 9, Epoch 118, Loss: 2.798963248729706, Final Batch Loss: 0.7176384925842285\n",
      "Subject 9, Epoch 119, Loss: 2.513499528169632, Final Batch Loss: 0.46648266911506653\n",
      "Subject 9, Epoch 120, Loss: 2.837296426296234, Final Batch Loss: 0.8425857424736023\n",
      "Subject 9, Epoch 121, Loss: 2.695703089237213, Final Batch Loss: 0.6307510137557983\n",
      "Subject 9, Epoch 122, Loss: 2.7664555311203003, Final Batch Loss: 0.6657284498214722\n",
      "Subject 9, Epoch 123, Loss: 2.424542188644409, Final Batch Loss: 0.573666512966156\n",
      "Subject 9, Epoch 124, Loss: 2.750811278820038, Final Batch Loss: 0.6362729668617249\n",
      "Subject 9, Epoch 125, Loss: 2.6808807849884033, Final Batch Loss: 0.8079630136489868\n",
      "Subject 9, Epoch 126, Loss: 2.634735584259033, Final Batch Loss: 0.6598848700523376\n",
      "Subject 9, Epoch 127, Loss: 2.579103112220764, Final Batch Loss: 0.5819240212440491\n",
      "Subject 9, Epoch 128, Loss: 2.600712537765503, Final Batch Loss: 0.7187207341194153\n",
      "Subject 9, Epoch 129, Loss: 3.1686059832572937, Final Batch Loss: 1.2803691625595093\n",
      "Subject 9, Epoch 130, Loss: 2.5240257382392883, Final Batch Loss: 0.624896764755249\n",
      "Subject 9, Epoch 131, Loss: 2.539625883102417, Final Batch Loss: 0.6395846009254456\n",
      "Subject 9, Epoch 132, Loss: 2.7231284976005554, Final Batch Loss: 0.8746053576469421\n",
      "Subject 9, Epoch 133, Loss: 2.290778398513794, Final Batch Loss: 0.43716907501220703\n",
      "Subject 9, Epoch 134, Loss: 2.307299464941025, Final Batch Loss: 0.4502846896648407\n",
      "Subject 9, Epoch 135, Loss: 2.8798083066940308, Final Batch Loss: 0.9133459329605103\n",
      "Subject 9, Epoch 136, Loss: 2.3853705525398254, Final Batch Loss: 0.45715177059173584\n",
      "Subject 9, Epoch 137, Loss: 2.4358819127082825, Final Batch Loss: 0.7019741535186768\n",
      "Subject 9, Epoch 138, Loss: 2.598465919494629, Final Batch Loss: 0.7410056591033936\n",
      "Subject 9, Epoch 139, Loss: 2.1505808234214783, Final Batch Loss: 0.34203827381134033\n",
      "Subject 9, Epoch 140, Loss: 2.379917085170746, Final Batch Loss: 0.5837231278419495\n",
      "Subject 9, Epoch 141, Loss: 2.5977091789245605, Final Batch Loss: 0.7650451064109802\n",
      "Subject 9, Epoch 142, Loss: 2.5478721261024475, Final Batch Loss: 0.7617918252944946\n",
      "Subject 9, Epoch 143, Loss: 2.2045534551143646, Final Batch Loss: 0.31500938534736633\n",
      "Subject 9, Epoch 144, Loss: 2.3724292516708374, Final Batch Loss: 0.6293759942054749\n",
      "Subject 9, Epoch 145, Loss: 2.563970983028412, Final Batch Loss: 0.8047513365745544\n",
      "Subject 9, Epoch 146, Loss: 2.4563152194023132, Final Batch Loss: 0.5987196564674377\n",
      "Subject 9, Epoch 147, Loss: 2.1487937271595, Final Batch Loss: 0.39962008595466614\n",
      "Subject 9, Epoch 148, Loss: 2.3253214359283447, Final Batch Loss: 0.5788824558258057\n",
      "Subject 9, Epoch 149, Loss: 2.130286693572998, Final Batch Loss: 0.41703319549560547\n",
      "Subject 9, Epoch 150, Loss: 2.2442376613616943, Final Batch Loss: 0.46773719787597656\n",
      "Subject 9, Epoch 151, Loss: 2.4369921684265137, Final Batch Loss: 0.6306777596473694\n",
      "Subject 9, Epoch 152, Loss: 2.2421253621578217, Final Batch Loss: 0.558727502822876\n",
      "Subject 9, Epoch 153, Loss: 2.4306355118751526, Final Batch Loss: 0.6135125160217285\n",
      "Subject 9, Epoch 154, Loss: 2.46364963054657, Final Batch Loss: 0.7301412224769592\n",
      "Subject 9, Epoch 155, Loss: 2.4398717284202576, Final Batch Loss: 0.7328164577484131\n",
      "Subject 9, Epoch 156, Loss: 2.068174809217453, Final Batch Loss: 0.38082048296928406\n",
      "Subject 9, Epoch 157, Loss: 2.283926486968994, Final Batch Loss: 0.573194682598114\n",
      "Subject 9, Epoch 158, Loss: 2.6441689133644104, Final Batch Loss: 0.9230926036834717\n",
      "Subject 9, Epoch 159, Loss: 2.5838932394981384, Final Batch Loss: 0.9112933874130249\n",
      "Subject 9, Epoch 160, Loss: 2.0220757126808167, Final Batch Loss: 0.3977765142917633\n",
      "Subject 9, Epoch 161, Loss: 2.2989326119422913, Final Batch Loss: 0.471208393573761\n",
      "Subject 9, Epoch 162, Loss: 2.3283148407936096, Final Batch Loss: 0.764299213886261\n",
      "Subject 9, Epoch 163, Loss: 2.2945576310157776, Final Batch Loss: 0.642970621585846\n",
      "Subject 9, Epoch 164, Loss: 1.9584542214870453, Final Batch Loss: 0.3047686219215393\n",
      "Subject 9, Epoch 165, Loss: 2.0773697197437286, Final Batch Loss: 0.4262603521347046\n",
      "Subject 9, Epoch 166, Loss: 2.113264501094818, Final Batch Loss: 0.5261327624320984\n",
      "Subject 9, Epoch 167, Loss: 2.1203474402427673, Final Batch Loss: 0.578454852104187\n",
      "Subject 9, Epoch 168, Loss: 2.5944029092788696, Final Batch Loss: 0.9322622418403625\n",
      "Subject 9, Epoch 169, Loss: 2.5111937820911407, Final Batch Loss: 0.9573982357978821\n",
      "Subject 9, Epoch 170, Loss: 2.212449789047241, Final Batch Loss: 0.7256578803062439\n",
      "Subject 9, Epoch 171, Loss: 2.249645233154297, Final Batch Loss: 0.6912285685539246\n",
      "Subject 9, Epoch 172, Loss: 2.435826539993286, Final Batch Loss: 0.8875810503959656\n",
      "Subject 9, Epoch 173, Loss: 1.9558867514133453, Final Batch Loss: 0.4130461812019348\n",
      "Subject 9, Epoch 174, Loss: 2.119686096906662, Final Batch Loss: 0.3739944398403168\n",
      "Subject 9, Epoch 175, Loss: 2.5792199969291687, Final Batch Loss: 0.8331995010375977\n",
      "Subject 9, Epoch 176, Loss: 2.466010272502899, Final Batch Loss: 0.8517932891845703\n",
      "Subject 9, Epoch 177, Loss: 2.148090124130249, Final Batch Loss: 0.5711798667907715\n",
      "Subject 9, Epoch 178, Loss: 2.170868992805481, Final Batch Loss: 0.5228321552276611\n",
      "Subject 9, Epoch 179, Loss: 2.1245745420455933, Final Batch Loss: 0.6490991115570068\n",
      "Subject 9, Epoch 180, Loss: 2.339782804250717, Final Batch Loss: 0.8294373154640198\n",
      "Subject 9, Epoch 181, Loss: 2.109951972961426, Final Batch Loss: 0.5204856395721436\n",
      "Subject 9, Epoch 182, Loss: 2.347511351108551, Final Batch Loss: 0.8133777379989624\n",
      "Subject 9, Epoch 183, Loss: 2.052656978368759, Final Batch Loss: 0.3951091468334198\n",
      "Subject 9, Epoch 184, Loss: 2.1240431368350983, Final Batch Loss: 0.49144333600997925\n",
      "Subject 9, Epoch 185, Loss: 2.45675790309906, Final Batch Loss: 0.896094560623169\n",
      "Subject 9, Epoch 186, Loss: 2.1399568915367126, Final Batch Loss: 0.652628481388092\n",
      "Subject 9, Epoch 187, Loss: 2.2197061479091644, Final Batch Loss: 0.643868625164032\n",
      "Subject 9, Epoch 188, Loss: 1.9041946530342102, Final Batch Loss: 0.33903566002845764\n",
      "Subject 9, Epoch 189, Loss: 2.016910344362259, Final Batch Loss: 0.5270401835441589\n",
      "Subject 9, Epoch 190, Loss: 1.8535669147968292, Final Batch Loss: 0.29759785532951355\n",
      "Subject 9, Epoch 191, Loss: 1.7891857028007507, Final Batch Loss: 0.3204832077026367\n",
      "Subject 9, Epoch 192, Loss: 2.126814544200897, Final Batch Loss: 0.7089318633079529\n",
      "Subject 9, Epoch 193, Loss: 2.3155056834220886, Final Batch Loss: 0.8126882314682007\n",
      "Subject 9, Epoch 194, Loss: 1.93264839053154, Final Batch Loss: 0.4272230863571167\n",
      "Subject 9, Epoch 195, Loss: 1.9204769730567932, Final Batch Loss: 0.43172627687454224\n",
      "Subject 9, Epoch 196, Loss: 1.8164288103580475, Final Batch Loss: 0.33278071880340576\n",
      "Subject 9, Epoch 197, Loss: 1.6781180426478386, Final Batch Loss: 0.09963827580213547\n",
      "Subject 9, Epoch 198, Loss: 1.786841094493866, Final Batch Loss: 0.3674735724925995\n",
      "Subject 9, Epoch 199, Loss: 2.075378119945526, Final Batch Loss: 0.5740661025047302\n",
      "Subject 9, Epoch 200, Loss: 1.9443557262420654, Final Batch Loss: 0.4185616672039032\n",
      "Subject 9, Epoch 201, Loss: 2.1303825080394745, Final Batch Loss: 0.6807540059089661\n",
      "Subject 9, Epoch 202, Loss: 1.9055549204349518, Final Batch Loss: 0.5088860988616943\n",
      "Subject 9, Epoch 203, Loss: 2.045087903738022, Final Batch Loss: 0.623212993144989\n",
      "Subject 9, Epoch 204, Loss: 1.6485076546669006, Final Batch Loss: 0.1306937336921692\n",
      "Subject 9, Epoch 205, Loss: 1.930087149143219, Final Batch Loss: 0.5481634140014648\n",
      "Subject 9, Epoch 206, Loss: 1.719605177640915, Final Batch Loss: 0.27818137407302856\n",
      "Subject 9, Epoch 207, Loss: 2.125034511089325, Final Batch Loss: 0.7096983790397644\n",
      "Subject 9, Epoch 208, Loss: 1.6577555239200592, Final Batch Loss: 0.29497450590133667\n",
      "Subject 9, Epoch 209, Loss: 2.0308931171894073, Final Batch Loss: 0.5920129418373108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 210, Loss: 1.7875992357730865, Final Batch Loss: 0.3896443545818329\n",
      "Subject 9, Epoch 211, Loss: 2.0429230630397797, Final Batch Loss: 0.7010886073112488\n",
      "Subject 9, Epoch 212, Loss: 1.7221761643886566, Final Batch Loss: 0.2970549762248993\n",
      "Subject 9, Epoch 213, Loss: 1.6867009997367859, Final Batch Loss: 0.22809481620788574\n",
      "Subject 9, Epoch 214, Loss: 1.7616649568080902, Final Batch Loss: 0.4084421992301941\n",
      "Subject 9, Epoch 215, Loss: 1.9633051455020905, Final Batch Loss: 0.5600306391716003\n",
      "Subject 9, Epoch 216, Loss: 2.0549950003623962, Final Batch Loss: 0.667207658290863\n",
      "Subject 9, Epoch 217, Loss: 1.6286267936229706, Final Batch Loss: 0.27931907773017883\n",
      "Subject 9, Epoch 218, Loss: 2.065702944993973, Final Batch Loss: 0.5912710428237915\n",
      "Subject 9, Epoch 219, Loss: 1.938535064458847, Final Batch Loss: 0.570194661617279\n",
      "Subject 9, Epoch 220, Loss: 2.01211017370224, Final Batch Loss: 0.6727392673492432\n",
      "Subject 9, Epoch 221, Loss: 1.9069149196147919, Final Batch Loss: 0.5194528698921204\n",
      "Subject 9, Epoch 222, Loss: 1.9666591584682465, Final Batch Loss: 0.6094130873680115\n",
      "Subject 9, Epoch 223, Loss: 1.7164207696914673, Final Batch Loss: 0.3777608573436737\n",
      "Subject 9, Epoch 224, Loss: 2.014988362789154, Final Batch Loss: 0.6217557787895203\n",
      "Subject 9, Epoch 225, Loss: 1.7120542526245117, Final Batch Loss: 0.4055062234401703\n",
      "Subject 9, Epoch 226, Loss: 1.965807467699051, Final Batch Loss: 0.5397077202796936\n",
      "Subject 9, Epoch 227, Loss: 1.773889183998108, Final Batch Loss: 0.44681569933891296\n",
      "Subject 9, Epoch 228, Loss: 1.5115591436624527, Final Batch Loss: 0.23411260545253754\n",
      "Subject 9, Epoch 229, Loss: 1.7002345025539398, Final Batch Loss: 0.3435855209827423\n",
      "Subject 9, Epoch 230, Loss: 1.9296420216560364, Final Batch Loss: 0.6604577898979187\n",
      "Subject 9, Epoch 231, Loss: 1.7170658707618713, Final Batch Loss: 0.3699260950088501\n",
      "Subject 9, Epoch 232, Loss: 1.8795047998428345, Final Batch Loss: 0.46473920345306396\n",
      "Subject 9, Epoch 233, Loss: 2.0136031210422516, Final Batch Loss: 0.6107909679412842\n",
      "Subject 9, Epoch 234, Loss: 2.092811644077301, Final Batch Loss: 0.6689611077308655\n",
      "Subject 9, Epoch 235, Loss: 1.7330540120601654, Final Batch Loss: 0.4227891266345978\n",
      "Subject 9, Epoch 236, Loss: 1.8365009725093842, Final Batch Loss: 0.4606817066669464\n",
      "Subject 9, Epoch 237, Loss: 1.7334806025028229, Final Batch Loss: 0.35640594363212585\n",
      "Subject 9, Epoch 238, Loss: 1.620789259672165, Final Batch Loss: 0.3310334384441376\n",
      "Subject 9, Epoch 239, Loss: 1.807875543832779, Final Batch Loss: 0.5198431611061096\n",
      "Subject 9, Epoch 240, Loss: 1.575374260544777, Final Batch Loss: 0.20422868430614471\n",
      "Subject 9, Epoch 241, Loss: 1.7515682876110077, Final Batch Loss: 0.3862913250923157\n",
      "Subject 9, Epoch 242, Loss: 1.5767772495746613, Final Batch Loss: 0.2776622772216797\n",
      "Subject 9, Epoch 243, Loss: 2.193985015153885, Final Batch Loss: 0.8233991861343384\n",
      "Subject 9, Epoch 244, Loss: 1.5327582657337189, Final Batch Loss: 0.21000394225120544\n",
      "Subject 9, Epoch 245, Loss: 1.5682735443115234, Final Batch Loss: 0.36170071363449097\n",
      "Subject 9, Epoch 246, Loss: 1.6759409010410309, Final Batch Loss: 0.406227707862854\n",
      "Subject 9, Epoch 247, Loss: 1.8200888335704803, Final Batch Loss: 0.5117888450622559\n",
      "Subject 9, Epoch 248, Loss: 1.6721793711185455, Final Batch Loss: 0.3259318172931671\n",
      "Subject 9, Epoch 249, Loss: 1.5371077358722687, Final Batch Loss: 0.25636473298072815\n",
      "Subject 9, Epoch 250, Loss: 1.7849440574645996, Final Batch Loss: 0.44744786620140076\n",
      "Subject 9, Epoch 251, Loss: 1.780267357826233, Final Batch Loss: 0.44652172923088074\n",
      "Subject 9, Epoch 252, Loss: 1.9843288362026215, Final Batch Loss: 0.7450627684593201\n",
      "Subject 9, Epoch 253, Loss: 1.6777659952640533, Final Batch Loss: 0.3312196135520935\n",
      "Subject 9, Epoch 254, Loss: 1.8453983962535858, Final Batch Loss: 0.48204705119132996\n",
      "Subject 9, Epoch 255, Loss: 1.94512540102005, Final Batch Loss: 0.5447099208831787\n",
      "Subject 9, Epoch 256, Loss: 1.710587739944458, Final Batch Loss: 0.4333253800868988\n",
      "Subject 9, Epoch 257, Loss: 1.8500114381313324, Final Batch Loss: 0.5885169506072998\n",
      "Subject 9, Epoch 258, Loss: 1.6165370643138885, Final Batch Loss: 0.37216365337371826\n",
      "Subject 9, Epoch 259, Loss: 1.7602011561393738, Final Batch Loss: 0.5307477116584778\n",
      "Subject 9, Epoch 260, Loss: 1.684696227312088, Final Batch Loss: 0.3447410464286804\n",
      "Subject 9, Epoch 261, Loss: 2.126104772090912, Final Batch Loss: 0.8245506286621094\n",
      "Subject 9, Epoch 262, Loss: 1.6436318755149841, Final Batch Loss: 0.33857986330986023\n",
      "Subject 9, Epoch 263, Loss: 1.6660144329071045, Final Batch Loss: 0.4347701370716095\n",
      "Subject 9, Epoch 264, Loss: 1.899114578962326, Final Batch Loss: 0.6578742861747742\n",
      "Subject 9, Epoch 265, Loss: 1.5817236006259918, Final Batch Loss: 0.2720530331134796\n",
      "Subject 9, Epoch 266, Loss: 1.7604767382144928, Final Batch Loss: 0.43088826537132263\n",
      "Subject 9, Epoch 267, Loss: 1.955981582403183, Final Batch Loss: 0.6041516661643982\n",
      "Subject 9, Epoch 268, Loss: 1.426221802830696, Final Batch Loss: 0.13878442347049713\n",
      "Subject 9, Epoch 269, Loss: 1.660464733839035, Final Batch Loss: 0.40581977367401123\n",
      "Subject 9, Epoch 270, Loss: 1.6057765483856201, Final Batch Loss: 0.33410879969596863\n",
      "Subject 9, Epoch 271, Loss: 1.3340300023555756, Final Batch Loss: 0.1463787853717804\n",
      "Subject 9, Epoch 272, Loss: 1.7537561058998108, Final Batch Loss: 0.4621328115463257\n",
      "Subject 9, Epoch 273, Loss: 1.6923547685146332, Final Batch Loss: 0.3077292740345001\n",
      "Subject 9, Epoch 274, Loss: 1.5272834300994873, Final Batch Loss: 0.21082991361618042\n",
      "Subject 9, Epoch 275, Loss: 1.5872012674808502, Final Batch Loss: 0.32587289810180664\n",
      "Subject 9, Epoch 276, Loss: 2.0922544598579407, Final Batch Loss: 0.9105595946311951\n",
      "Subject 9, Epoch 277, Loss: 1.7721737325191498, Final Batch Loss: 0.5386229157447815\n",
      "Subject 9, Epoch 278, Loss: 1.7738324701786041, Final Batch Loss: 0.5476836562156677\n",
      "Subject 9, Epoch 279, Loss: 1.595677673816681, Final Batch Loss: 0.2795903980731964\n",
      "Subject 9, Epoch 280, Loss: 1.7747702300548553, Final Batch Loss: 0.49241378903388977\n",
      "Subject 9, Epoch 281, Loss: 1.708222895860672, Final Batch Loss: 0.49007147550582886\n",
      "Subject 9, Epoch 282, Loss: 1.7851063907146454, Final Batch Loss: 0.475324809551239\n",
      "Subject 9, Epoch 283, Loss: 1.9542716443538666, Final Batch Loss: 0.6137499213218689\n",
      "Subject 9, Epoch 284, Loss: 1.6434314996004105, Final Batch Loss: 0.24173273146152496\n",
      "Subject 9, Epoch 285, Loss: 1.4994956851005554, Final Batch Loss: 0.30130311846733093\n",
      "Subject 9, Epoch 286, Loss: 1.7397411465644836, Final Batch Loss: 0.5401491522789001\n",
      "Subject 9, Epoch 287, Loss: 1.6584560871124268, Final Batch Loss: 0.47442930936813354\n",
      "Subject 9, Epoch 288, Loss: 1.5649656355381012, Final Batch Loss: 0.36386868357658386\n",
      "Subject 9, Epoch 289, Loss: 1.4929991662502289, Final Batch Loss: 0.29905012249946594\n",
      "Subject 9, Epoch 290, Loss: 1.7300030887126923, Final Batch Loss: 0.5198915004730225\n",
      "Subject 9, Epoch 291, Loss: 1.666895866394043, Final Batch Loss: 0.3745073080062866\n",
      "Subject 9, Epoch 292, Loss: 1.5207646489143372, Final Batch Loss: 0.29793915152549744\n",
      "Subject 9, Epoch 293, Loss: 1.665928214788437, Final Batch Loss: 0.4304673373699188\n",
      "Subject 9, Epoch 294, Loss: 1.6660585403442383, Final Batch Loss: 0.401314377784729\n",
      "Subject 9, Epoch 295, Loss: 1.87716943025589, Final Batch Loss: 0.5729647278785706\n",
      "Subject 9, Epoch 296, Loss: 1.6430180072784424, Final Batch Loss: 0.3904876410961151\n",
      "Subject 9, Epoch 297, Loss: 1.57852903008461, Final Batch Loss: 0.2684796452522278\n",
      "Subject 9, Epoch 298, Loss: 1.5848405361175537, Final Batch Loss: 0.3948068618774414\n",
      "Subject 9, Epoch 299, Loss: 1.4662818610668182, Final Batch Loss: 0.310041606426239\n",
      "Subject 9, Epoch 300, Loss: 1.5543046295642853, Final Batch Loss: 0.2894158363342285\n",
      "Subject 9, Epoch 301, Loss: 1.444751352071762, Final Batch Loss: 0.22614803910255432\n",
      "Subject 9, Epoch 302, Loss: 1.5222647786140442, Final Batch Loss: 0.3082233965396881\n",
      "Subject 9, Epoch 303, Loss: 1.888871043920517, Final Batch Loss: 0.5992214679718018\n",
      "Subject 9, Epoch 304, Loss: 1.783892184495926, Final Batch Loss: 0.552471399307251\n",
      "Subject 9, Epoch 305, Loss: 1.7751236259937286, Final Batch Loss: 0.5568420886993408\n",
      "Subject 9, Epoch 306, Loss: 1.5385060906410217, Final Batch Loss: 0.3360222578048706\n",
      "Subject 9, Epoch 307, Loss: 1.6212858855724335, Final Batch Loss: 0.4086858928203583\n",
      "Subject 9, Epoch 308, Loss: 1.6782201826572418, Final Batch Loss: 0.5952224731445312\n",
      "Subject 9, Epoch 309, Loss: 1.7326412796974182, Final Batch Loss: 0.5714123845100403\n",
      "Subject 9, Epoch 310, Loss: 1.7451343536376953, Final Batch Loss: 0.5664008855819702\n",
      "Subject 9, Epoch 311, Loss: 1.5211433172225952, Final Batch Loss: 0.39054134488105774\n",
      "Subject 9, Epoch 312, Loss: 1.41802978515625, Final Batch Loss: 0.2983972430229187\n",
      "Subject 9, Epoch 313, Loss: 1.430555745959282, Final Batch Loss: 0.2111925631761551\n",
      "Subject 9, Epoch 314, Loss: 1.721644788980484, Final Batch Loss: 0.5561573505401611\n",
      "Subject 9, Epoch 315, Loss: 1.3547729700803757, Final Batch Loss: 0.20083819329738617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 316, Loss: 1.5126965045928955, Final Batch Loss: 0.3980202376842499\n",
      "Subject 9, Epoch 317, Loss: 1.386757344007492, Final Batch Loss: 0.17225879430770874\n",
      "Subject 9, Epoch 318, Loss: 1.3377332091331482, Final Batch Loss: 0.2174113690853119\n",
      "Subject 9, Epoch 319, Loss: 1.3574233502149582, Final Batch Loss: 0.21267078816890717\n",
      "Subject 9, Epoch 320, Loss: 1.6468870639801025, Final Batch Loss: 0.48231780529022217\n",
      "Subject 9, Epoch 321, Loss: 1.414608657360077, Final Batch Loss: 0.25685006380081177\n",
      "Subject 9, Epoch 322, Loss: 1.6690095365047455, Final Batch Loss: 0.4086749851703644\n",
      "Subject 9, Epoch 323, Loss: 1.4597221463918686, Final Batch Loss: 0.20374493300914764\n",
      "Subject 9, Epoch 324, Loss: 1.5889583230018616, Final Batch Loss: 0.34532755613327026\n",
      "Subject 9, Epoch 325, Loss: 1.3971683084964752, Final Batch Loss: 0.29831013083457947\n",
      "Subject 9, Epoch 326, Loss: 1.4646899998188019, Final Batch Loss: 0.3771047294139862\n",
      "Subject 9, Epoch 327, Loss: 1.480775624513626, Final Batch Loss: 0.28630873560905457\n",
      "Subject 9, Epoch 328, Loss: 1.492414951324463, Final Batch Loss: 0.376118540763855\n",
      "Subject 9, Epoch 329, Loss: 1.2472051531076431, Final Batch Loss: 0.18723104894161224\n",
      "Subject 9, Epoch 330, Loss: 1.6832619607448578, Final Batch Loss: 0.4624066650867462\n",
      "Subject 9, Epoch 331, Loss: 1.5068060159683228, Final Batch Loss: 0.3611340820789337\n",
      "Subject 9, Epoch 332, Loss: 1.6166190803050995, Final Batch Loss: 0.49917924404144287\n",
      "Subject 9, Epoch 333, Loss: 1.5536330044269562, Final Batch Loss: 0.380242258310318\n",
      "Subject 9, Epoch 334, Loss: 1.2968871891498566, Final Batch Loss: 0.16028276085853577\n",
      "Subject 9, Epoch 335, Loss: 1.530291497707367, Final Batch Loss: 0.42215585708618164\n",
      "Subject 9, Epoch 336, Loss: 1.2730956599116325, Final Batch Loss: 0.08787744492292404\n",
      "Subject 9, Epoch 337, Loss: 1.3932079672813416, Final Batch Loss: 0.3150755763053894\n",
      "Subject 9, Epoch 338, Loss: 1.5862566232681274, Final Batch Loss: 0.4844355285167694\n",
      "Subject 9, Epoch 339, Loss: 1.4751505851745605, Final Batch Loss: 0.2690730094909668\n",
      "Subject 9, Epoch 340, Loss: 2.0000859797000885, Final Batch Loss: 0.9450232982635498\n",
      "Subject 9, Epoch 341, Loss: 1.5776236057281494, Final Batch Loss: 0.37247487902641296\n",
      "Subject 9, Epoch 342, Loss: 1.6664614379405975, Final Batch Loss: 0.47359371185302734\n",
      "Subject 9, Epoch 343, Loss: 1.4821272194385529, Final Batch Loss: 0.3566405475139618\n",
      "Subject 9, Epoch 344, Loss: 1.5652352571487427, Final Batch Loss: 0.3936275541782379\n",
      "Subject 9, Epoch 345, Loss: 1.6904526948928833, Final Batch Loss: 0.5704773664474487\n",
      "Subject 9, Epoch 346, Loss: 1.336728110909462, Final Batch Loss: 0.13115467131137848\n",
      "Subject 9, Epoch 347, Loss: 1.407885581254959, Final Batch Loss: 0.29190996289253235\n",
      "Subject 9, Epoch 348, Loss: 1.4129252135753632, Final Batch Loss: 0.30677330493927\n",
      "Subject 9, Epoch 349, Loss: 1.5170264840126038, Final Batch Loss: 0.4026607275009155\n",
      "Subject 9, Epoch 350, Loss: 1.5301280617713928, Final Batch Loss: 0.4451461136341095\n",
      "Subject 9, Epoch 351, Loss: 1.4882116913795471, Final Batch Loss: 0.3685178756713867\n",
      "Subject 9, Epoch 352, Loss: 1.5405341386795044, Final Batch Loss: 0.3878050744533539\n",
      "Subject 9, Epoch 353, Loss: 1.4727634489536285, Final Batch Loss: 0.39138534665107727\n",
      "Subject 9, Epoch 354, Loss: 1.2969671040773392, Final Batch Loss: 0.1498389095067978\n",
      "Subject 9, Epoch 355, Loss: 1.3740282505750656, Final Batch Loss: 0.20771776139736176\n",
      "Subject 9, Epoch 356, Loss: 1.4716169834136963, Final Batch Loss: 0.3426305055618286\n",
      "Subject 9, Epoch 357, Loss: 1.3783356547355652, Final Batch Loss: 0.310003399848938\n",
      "Subject 9, Epoch 358, Loss: 1.499391257762909, Final Batch Loss: 0.33531269431114197\n",
      "Subject 9, Epoch 359, Loss: 1.418813407421112, Final Batch Loss: 0.29755666851997375\n",
      "Subject 9, Epoch 360, Loss: 1.8932483196258545, Final Batch Loss: 0.7964230179786682\n",
      "Subject 9, Epoch 361, Loss: 1.4860965609550476, Final Batch Loss: 0.4346791207790375\n",
      "Subject 9, Epoch 362, Loss: 1.2587752491235733, Final Batch Loss: 0.17296023666858673\n",
      "Subject 9, Epoch 363, Loss: 1.7397653460502625, Final Batch Loss: 0.7019849419593811\n",
      "Subject 9, Epoch 364, Loss: 1.2649374157190323, Final Batch Loss: 0.19417168200016022\n",
      "Subject 9, Epoch 365, Loss: 1.571705162525177, Final Batch Loss: 0.46232545375823975\n",
      "Subject 9, Epoch 366, Loss: 1.5190999805927277, Final Batch Loss: 0.4316723644733429\n",
      "Subject 9, Epoch 367, Loss: 1.4302136301994324, Final Batch Loss: 0.2852254807949066\n",
      "Subject 9, Epoch 368, Loss: 1.4913973808288574, Final Batch Loss: 0.18774914741516113\n",
      "Subject 9, Epoch 369, Loss: 1.454385757446289, Final Batch Loss: 0.34436100721359253\n",
      "Subject 9, Epoch 370, Loss: 1.4019454717636108, Final Batch Loss: 0.32308483123779297\n",
      "Subject 9, Epoch 371, Loss: 1.4584537148475647, Final Batch Loss: 0.1872599720954895\n",
      "Subject 9, Epoch 372, Loss: 1.6882064640522003, Final Batch Loss: 0.5690765976905823\n",
      "Subject 9, Epoch 373, Loss: 1.8590731918811798, Final Batch Loss: 0.7928544878959656\n",
      "Subject 9, Epoch 374, Loss: 1.7339680790901184, Final Batch Loss: 0.5970170497894287\n",
      "Subject 9, Epoch 375, Loss: 1.258669689297676, Final Batch Loss: 0.14813460409641266\n",
      "Subject 9, Epoch 376, Loss: 1.2373159676790237, Final Batch Loss: 0.19778622686862946\n",
      "Subject 9, Epoch 377, Loss: 1.450237363576889, Final Batch Loss: 0.35017839074134827\n",
      "Subject 9, Epoch 378, Loss: 1.5058798491954803, Final Batch Loss: 0.3937341868877411\n",
      "Subject 9, Epoch 379, Loss: 1.4502545893192291, Final Batch Loss: 0.27985039353370667\n",
      "Subject 9, Epoch 380, Loss: 1.3985559344291687, Final Batch Loss: 0.26389408111572266\n",
      "Subject 9, Epoch 381, Loss: 1.777130901813507, Final Batch Loss: 0.6116332411766052\n",
      "Subject 9, Epoch 382, Loss: 1.473356455564499, Final Batch Loss: 0.34265196323394775\n",
      "Subject 9, Epoch 383, Loss: 1.303002804517746, Final Batch Loss: 0.18318238854408264\n",
      "Subject 9, Epoch 384, Loss: 1.4023303389549255, Final Batch Loss: 0.2023886740207672\n",
      "Subject 9, Epoch 385, Loss: 1.18397656083107, Final Batch Loss: 0.15180057287216187\n",
      "Subject 9, Epoch 386, Loss: 1.4804955422878265, Final Batch Loss: 0.39363062381744385\n",
      "Subject 9, Epoch 387, Loss: 1.3922969996929169, Final Batch Loss: 0.27394944429397583\n",
      "Subject 9, Epoch 388, Loss: 1.6219957768917084, Final Batch Loss: 0.5477161407470703\n",
      "Subject 9, Epoch 389, Loss: 1.2404457181692123, Final Batch Loss: 0.09429018199443817\n",
      "Subject 9, Epoch 390, Loss: 1.3481247127056122, Final Batch Loss: 0.2543114125728607\n",
      "Subject 9, Epoch 391, Loss: 1.2607608437538147, Final Batch Loss: 0.24352741241455078\n",
      "Subject 9, Epoch 392, Loss: 1.4085853695869446, Final Batch Loss: 0.39902859926223755\n",
      "Subject 9, Epoch 393, Loss: 1.5582618713378906, Final Batch Loss: 0.4662013053894043\n",
      "Subject 9, Epoch 394, Loss: 1.434926688671112, Final Batch Loss: 0.2743661105632782\n",
      "Subject 9, Epoch 395, Loss: 1.5293030142784119, Final Batch Loss: 0.384654700756073\n",
      "Subject 9, Epoch 396, Loss: 1.36530402302742, Final Batch Loss: 0.30656126141548157\n",
      "Subject 9, Epoch 397, Loss: 1.6229428946971893, Final Batch Loss: 0.588225781917572\n",
      "Subject 9, Epoch 398, Loss: 1.5989004969596863, Final Batch Loss: 0.4536947011947632\n",
      "Subject 9, Epoch 399, Loss: 1.7544342577457428, Final Batch Loss: 0.7404873371124268\n",
      "Subject 9, Epoch 400, Loss: 1.622018963098526, Final Batch Loss: 0.4659287631511688\n",
      "Subject 9, Epoch 401, Loss: 1.3781313002109528, Final Batch Loss: 0.36275967955589294\n",
      "Subject 9, Epoch 402, Loss: 1.510012686252594, Final Batch Loss: 0.4855666160583496\n",
      "Subject 9, Epoch 403, Loss: 1.4584022909402847, Final Batch Loss: 0.42863771319389343\n",
      "Subject 9, Epoch 404, Loss: 1.2349931448698044, Final Batch Loss: 0.15639005601406097\n",
      "Subject 9, Epoch 405, Loss: 1.518220067024231, Final Batch Loss: 0.45690104365348816\n",
      "Subject 9, Epoch 406, Loss: 1.837902694940567, Final Batch Loss: 0.7597898244857788\n",
      "Subject 9, Epoch 407, Loss: 1.387284129858017, Final Batch Loss: 0.28375351428985596\n",
      "Subject 9, Epoch 408, Loss: 1.1438793912529945, Final Batch Loss: 0.11223805695772171\n",
      "Subject 9, Epoch 409, Loss: 1.561120331287384, Final Batch Loss: 0.49213406443595886\n",
      "Subject 9, Epoch 410, Loss: 1.4562609195709229, Final Batch Loss: 0.41819724440574646\n",
      "Subject 9, Epoch 411, Loss: 1.442942202091217, Final Batch Loss: 0.3352348506450653\n",
      "Subject 9, Epoch 412, Loss: 1.2758416384458542, Final Batch Loss: 0.18936313688755035\n",
      "Subject 9, Epoch 413, Loss: 1.2755135595798492, Final Batch Loss: 0.1495409607887268\n",
      "Subject 9, Epoch 414, Loss: 1.4166963398456573, Final Batch Loss: 0.32432520389556885\n",
      "Subject 9, Epoch 415, Loss: 1.414046734571457, Final Batch Loss: 0.37030094861984253\n",
      "Subject 9, Epoch 416, Loss: 1.3574306964874268, Final Batch Loss: 0.2877498269081116\n",
      "Subject 9, Epoch 417, Loss: 1.3955776691436768, Final Batch Loss: 0.375095397233963\n",
      "Subject 9, Epoch 418, Loss: 1.4284669160842896, Final Batch Loss: 0.4101867377758026\n",
      "Subject 9, Epoch 419, Loss: 1.2861268073320389, Final Batch Loss: 0.2436976581811905\n",
      "Subject 9, Epoch 420, Loss: 1.2276324927806854, Final Batch Loss: 0.19991779327392578\n",
      "Subject 9, Epoch 421, Loss: 1.24653659760952, Final Batch Loss: 0.23974238336086273\n",
      "Subject 9, Epoch 422, Loss: 1.4754548370838165, Final Batch Loss: 0.3808310329914093\n",
      "Subject 9, Epoch 423, Loss: 1.309317409992218, Final Batch Loss: 0.2810934782028198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 424, Loss: 1.287191390991211, Final Batch Loss: 0.2375296652317047\n",
      "Subject 9, Epoch 425, Loss: 1.2382141053676605, Final Batch Loss: 0.21346163749694824\n",
      "Subject 9, Epoch 426, Loss: 1.412230759859085, Final Batch Loss: 0.36837175488471985\n",
      "Subject 9, Epoch 427, Loss: 1.2156985700130463, Final Batch Loss: 0.18447622656822205\n",
      "Subject 9, Epoch 428, Loss: 1.4353085160255432, Final Batch Loss: 0.3495681881904602\n",
      "Subject 9, Epoch 429, Loss: 1.3901979625225067, Final Batch Loss: 0.3567350208759308\n",
      "Subject 9, Epoch 430, Loss: 1.4541349112987518, Final Batch Loss: 0.4437694549560547\n",
      "Subject 9, Epoch 431, Loss: 1.4131976664066315, Final Batch Loss: 0.4279060661792755\n",
      "Subject 9, Epoch 432, Loss: 1.6216183006763458, Final Batch Loss: 0.49460700154304504\n",
      "Subject 9, Epoch 433, Loss: 1.2483539283275604, Final Batch Loss: 0.20362433791160583\n",
      "Subject 9, Epoch 434, Loss: 1.44130939245224, Final Batch Loss: 0.35163286328315735\n",
      "Subject 9, Epoch 435, Loss: 1.6274517178535461, Final Batch Loss: 0.6504514813423157\n",
      "Subject 9, Epoch 436, Loss: 1.2811959385871887, Final Batch Loss: 0.25464728474617004\n",
      "Subject 9, Epoch 437, Loss: 1.5359209775924683, Final Batch Loss: 0.45560896396636963\n",
      "Subject 9, Epoch 438, Loss: 1.448593020439148, Final Batch Loss: 0.3630882501602173\n",
      "Subject 9, Epoch 439, Loss: 1.5454896986484528, Final Batch Loss: 0.39297235012054443\n",
      "Subject 9, Epoch 440, Loss: 1.62913978099823, Final Batch Loss: 0.3961102366447449\n",
      "Subject 9, Epoch 441, Loss: 1.5160312056541443, Final Batch Loss: 0.43561869859695435\n",
      "Subject 9, Epoch 442, Loss: 1.553708165884018, Final Batch Loss: 0.4754224121570587\n",
      "Subject 9, Epoch 443, Loss: 1.5530393421649933, Final Batch Loss: 0.4324439764022827\n",
      "Subject 9, Epoch 444, Loss: 1.286354199051857, Final Batch Loss: 0.1939765065908432\n",
      "Subject 9, Epoch 445, Loss: 1.214144617319107, Final Batch Loss: 0.1397455632686615\n",
      "Subject 9, Epoch 446, Loss: 1.3340494930744171, Final Batch Loss: 0.26008880138397217\n",
      "Subject 9, Epoch 447, Loss: 1.3619160950183868, Final Batch Loss: 0.32845914363861084\n",
      "Subject 9, Epoch 448, Loss: 1.3704315423965454, Final Batch Loss: 0.3349226117134094\n",
      "Subject 9, Epoch 449, Loss: 1.2852418720722198, Final Batch Loss: 0.27333471179008484\n",
      "Subject 9, Epoch 450, Loss: 1.1901157051324844, Final Batch Loss: 0.14522291719913483\n",
      "Subject 9, Epoch 451, Loss: 1.368132472038269, Final Batch Loss: 0.3343406915664673\n",
      "Subject 9, Epoch 452, Loss: 1.2941086292266846, Final Batch Loss: 0.2286328673362732\n",
      "Subject 9, Epoch 453, Loss: 1.3053756952285767, Final Batch Loss: 0.2624399960041046\n",
      "Subject 9, Epoch 454, Loss: 1.4159058928489685, Final Batch Loss: 0.35167762637138367\n",
      "Subject 9, Epoch 455, Loss: 1.3180452585220337, Final Batch Loss: 0.30254051089286804\n",
      "Subject 9, Epoch 456, Loss: 1.7974390983581543, Final Batch Loss: 0.7357057929039001\n",
      "Subject 9, Epoch 457, Loss: 1.296972244977951, Final Batch Loss: 0.2731096148490906\n",
      "Subject 9, Epoch 458, Loss: 1.469640702009201, Final Batch Loss: 0.3866804242134094\n",
      "Subject 9, Epoch 459, Loss: 1.4940419495105743, Final Batch Loss: 0.41424810886383057\n",
      "Subject 9, Epoch 460, Loss: 1.4253840148448944, Final Batch Loss: 0.4215643107891083\n",
      "Subject 9, Epoch 461, Loss: 1.5956586599349976, Final Batch Loss: 0.5705352425575256\n",
      "Subject 9, Epoch 462, Loss: 1.2815568298101425, Final Batch Loss: 0.2268529087305069\n",
      "Subject 9, Epoch 463, Loss: 1.2937084436416626, Final Batch Loss: 0.2993090748786926\n",
      "Subject 9, Epoch 464, Loss: 1.407077968120575, Final Batch Loss: 0.43309086561203003\n",
      "Subject 9, Epoch 465, Loss: 1.3005461692810059, Final Batch Loss: 0.34862059354782104\n",
      "Subject 9, Epoch 466, Loss: 1.4500388503074646, Final Batch Loss: 0.4113594889640808\n",
      "Subject 9, Epoch 467, Loss: 1.3919402360916138, Final Batch Loss: 0.3540537357330322\n",
      "Subject 9, Epoch 468, Loss: 1.2126888781785965, Final Batch Loss: 0.15705285966396332\n",
      "Subject 9, Epoch 469, Loss: 1.254788875579834, Final Batch Loss: 0.21368908882141113\n",
      "Subject 9, Epoch 470, Loss: 1.3358674347400665, Final Batch Loss: 0.3465152978897095\n",
      "Subject 9, Epoch 471, Loss: 1.1637785285711288, Final Batch Loss: 0.1756443828344345\n",
      "Subject 9, Epoch 472, Loss: 1.2771304845809937, Final Batch Loss: 0.31100085377693176\n",
      "Subject 9, Epoch 473, Loss: 1.4661279618740082, Final Batch Loss: 0.4423164427280426\n",
      "Subject 9, Epoch 474, Loss: 1.3778688609600067, Final Batch Loss: 0.3906562626361847\n",
      "Subject 9, Epoch 475, Loss: 1.3972629308700562, Final Batch Loss: 0.4011020362377167\n",
      "Subject 9, Epoch 476, Loss: 1.215278521180153, Final Batch Loss: 0.23071514070034027\n",
      "Subject 9, Epoch 477, Loss: 1.2438841462135315, Final Batch Loss: 0.29442211985588074\n",
      "Subject 9, Epoch 478, Loss: 1.61958447098732, Final Batch Loss: 0.5282576084136963\n",
      "Subject 9, Epoch 479, Loss: 1.4596645534038544, Final Batch Loss: 0.49441978335380554\n",
      "Subject 9, Epoch 480, Loss: 1.4525272250175476, Final Batch Loss: 0.3937658965587616\n",
      "Subject 9, Epoch 481, Loss: 1.4288988709449768, Final Batch Loss: 0.4195612967014313\n",
      "Subject 9, Epoch 482, Loss: 1.2711847424507141, Final Batch Loss: 0.32052546739578247\n",
      "Subject 9, Epoch 483, Loss: 1.2545546889305115, Final Batch Loss: 0.2275918424129486\n",
      "Subject 9, Epoch 484, Loss: 1.3163515031337738, Final Batch Loss: 0.36548110842704773\n",
      "Subject 9, Epoch 485, Loss: 1.4728379547595978, Final Batch Loss: 0.4390260875225067\n",
      "Subject 9, Epoch 486, Loss: 1.2132611870765686, Final Batch Loss: 0.258381724357605\n",
      "Subject 9, Epoch 487, Loss: 1.1068433821201324, Final Batch Loss: 0.13995784521102905\n",
      "Subject 9, Epoch 488, Loss: 1.438563972711563, Final Batch Loss: 0.47377365827560425\n",
      "Subject 9, Epoch 489, Loss: 1.3079330623149872, Final Batch Loss: 0.3296564519405365\n",
      "Subject 9, Epoch 490, Loss: 1.2607972621917725, Final Batch Loss: 0.24565964937210083\n",
      "Subject 9, Epoch 491, Loss: 1.3731482326984406, Final Batch Loss: 0.35227009654045105\n",
      "Subject 9, Epoch 492, Loss: 1.4531258344650269, Final Batch Loss: 0.49843788146972656\n",
      "Subject 9, Epoch 493, Loss: 1.36286860704422, Final Batch Loss: 0.45045045018196106\n",
      "Subject 9, Epoch 494, Loss: 1.366000384092331, Final Batch Loss: 0.34202271699905396\n",
      "Subject 9, Epoch 495, Loss: 1.2485867142677307, Final Batch Loss: 0.31048905849456787\n",
      "Subject 9, Epoch 496, Loss: 1.4138611555099487, Final Batch Loss: 0.41017866134643555\n",
      "Subject 9, Epoch 497, Loss: 1.218457654118538, Final Batch Loss: 0.18828000128269196\n",
      "Subject 9, Epoch 498, Loss: 1.2306282073259354, Final Batch Loss: 0.2320815473794937\n",
      "Subject 9, Epoch 499, Loss: 1.3480184972286224, Final Batch Loss: 0.3693535029888153\n",
      "Subject 9, Epoch 500, Loss: 1.460924744606018, Final Batch Loss: 0.4801432490348816\n",
      "Subject 9, Epoch 501, Loss: 1.2352812141180038, Final Batch Loss: 0.24109898507595062\n",
      "Subject 9, Epoch 502, Loss: 1.3102636933326721, Final Batch Loss: 0.3080633878707886\n",
      "Subject 9, Epoch 503, Loss: 1.6452129483222961, Final Batch Loss: 0.6152834296226501\n",
      "Subject 9, Epoch 504, Loss: 1.487660676240921, Final Batch Loss: 0.5372591614723206\n",
      "Subject 9, Epoch 505, Loss: 1.196192279458046, Final Batch Loss: 0.21490608155727386\n",
      "Subject 9, Epoch 506, Loss: 1.1723321825265884, Final Batch Loss: 0.1889977902173996\n",
      "Subject 9, Epoch 507, Loss: 1.3326159119606018, Final Batch Loss: 0.3170960545539856\n",
      "Subject 9, Epoch 508, Loss: 1.2136229574680328, Final Batch Loss: 0.2308749258518219\n",
      "Subject 9, Epoch 509, Loss: 1.373917669057846, Final Batch Loss: 0.42492806911468506\n",
      "Subject 9, Epoch 510, Loss: 1.5394813120365143, Final Batch Loss: 0.42689990997314453\n",
      "Subject 9, Epoch 511, Loss: 1.1499804705381393, Final Batch Loss: 0.19247876107692719\n",
      "Subject 9, Epoch 512, Loss: 1.4614960849285126, Final Batch Loss: 0.4723634719848633\n",
      "Subject 9, Epoch 513, Loss: 1.3362066447734833, Final Batch Loss: 0.38440200686454773\n",
      "Subject 9, Epoch 514, Loss: 1.775657206773758, Final Batch Loss: 0.707773745059967\n",
      "Subject 9, Epoch 515, Loss: 1.3338401019573212, Final Batch Loss: 0.28597986698150635\n",
      "Subject 9, Epoch 516, Loss: 1.3159981071949005, Final Batch Loss: 0.32052335143089294\n",
      "Subject 9, Epoch 517, Loss: 1.3924094140529633, Final Batch Loss: 0.4230853319168091\n",
      "Subject 9, Epoch 518, Loss: 1.3105432987213135, Final Batch Loss: 0.2584967315196991\n",
      "Subject 9, Epoch 519, Loss: 1.6009916365146637, Final Batch Loss: 0.5655715465545654\n",
      "Subject 9, Epoch 520, Loss: 1.161399558186531, Final Batch Loss: 0.20720814168453217\n",
      "Subject 9, Epoch 521, Loss: 1.1862807422876358, Final Batch Loss: 0.18172584474086761\n",
      "Subject 9, Epoch 522, Loss: 1.1916081458330154, Final Batch Loss: 0.12674404680728912\n",
      "Subject 9, Epoch 523, Loss: 1.2016530558466911, Final Batch Loss: 0.12165062874555588\n",
      "Subject 9, Epoch 524, Loss: 1.367303341627121, Final Batch Loss: 0.2812157869338989\n",
      "Subject 9, Epoch 525, Loss: 1.524575799703598, Final Batch Loss: 0.5990860462188721\n",
      "Subject 9, Epoch 526, Loss: 1.2041035443544388, Final Batch Loss: 0.22406266629695892\n",
      "Subject 9, Epoch 527, Loss: 1.068443700671196, Final Batch Loss: 0.13929618895053864\n",
      "Subject 9, Epoch 528, Loss: 1.4151232540607452, Final Batch Loss: 0.3892512321472168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 529, Loss: 1.221183568239212, Final Batch Loss: 0.3015765845775604\n",
      "Subject 9, Epoch 530, Loss: 1.207399532198906, Final Batch Loss: 0.2299589067697525\n",
      "Subject 9, Epoch 531, Loss: 1.4116909205913544, Final Batch Loss: 0.2724120318889618\n",
      "Subject 9, Epoch 532, Loss: 1.3049189150333405, Final Batch Loss: 0.3363928198814392\n",
      "Subject 9, Epoch 533, Loss: 1.2619604468345642, Final Batch Loss: 0.30071720480918884\n",
      "Subject 9, Epoch 534, Loss: 1.1868286281824112, Final Batch Loss: 0.1772441416978836\n",
      "Subject 9, Epoch 535, Loss: 1.3419798910617828, Final Batch Loss: 0.36398905515670776\n",
      "Subject 9, Epoch 536, Loss: 1.4174640774726868, Final Batch Loss: 0.450151652097702\n",
      "Subject 9, Epoch 537, Loss: 1.2386916279792786, Final Batch Loss: 0.294765830039978\n",
      "Subject 9, Epoch 538, Loss: 1.2401681542396545, Final Batch Loss: 0.35662394762039185\n",
      "Subject 9, Epoch 539, Loss: 1.3452868461608887, Final Batch Loss: 0.3402513265609741\n",
      "Subject 9, Epoch 540, Loss: 1.269962728023529, Final Batch Loss: 0.3285616636276245\n",
      "Subject 9, Epoch 541, Loss: 1.236672729253769, Final Batch Loss: 0.3216512203216553\n",
      "Subject 9, Epoch 542, Loss: 1.1463219225406647, Final Batch Loss: 0.1737963855266571\n",
      "Subject 9, Epoch 543, Loss: 1.190007507801056, Final Batch Loss: 0.15106409788131714\n",
      "Subject 9, Epoch 544, Loss: 1.2847476303577423, Final Batch Loss: 0.2613598108291626\n",
      "Subject 9, Epoch 545, Loss: 1.1655974984169006, Final Batch Loss: 0.19915810227394104\n",
      "Subject 9, Epoch 546, Loss: 1.3587974309921265, Final Batch Loss: 0.4281627833843231\n",
      "Subject 9, Epoch 547, Loss: 1.3586628139019012, Final Batch Loss: 0.4317052960395813\n",
      "Subject 9, Epoch 548, Loss: 1.1628307551145554, Final Batch Loss: 0.11445958912372589\n",
      "Subject 9, Epoch 549, Loss: 1.4002081155776978, Final Batch Loss: 0.3846053183078766\n",
      "Subject 9, Epoch 550, Loss: 1.3324271142482758, Final Batch Loss: 0.3077719211578369\n",
      "Subject 9, Epoch 551, Loss: 1.2820964753627777, Final Batch Loss: 0.3216858208179474\n",
      "Subject 9, Epoch 552, Loss: 1.4839609563350677, Final Batch Loss: 0.49089813232421875\n",
      "Subject 9, Epoch 553, Loss: 1.2722868025302887, Final Batch Loss: 0.3308403789997101\n",
      "Subject 9, Epoch 554, Loss: 1.4526487290859222, Final Batch Loss: 0.447672575712204\n",
      "Subject 9, Epoch 555, Loss: 1.316295102238655, Final Batch Loss: 0.28487589955329895\n",
      "Subject 9, Epoch 556, Loss: 1.4399785995483398, Final Batch Loss: 0.47467949986457825\n",
      "Subject 9, Epoch 557, Loss: 1.2974957823753357, Final Batch Loss: 0.3159796893596649\n",
      "Subject 9, Epoch 558, Loss: 1.3243496716022491, Final Batch Loss: 0.3707077205181122\n",
      "Subject 9, Epoch 559, Loss: 1.3512167036533356, Final Batch Loss: 0.30023184418678284\n",
      "Subject 9, Epoch 560, Loss: 1.4504401981830597, Final Batch Loss: 0.4724558889865875\n",
      "Subject 9, Epoch 561, Loss: 1.155233472585678, Final Batch Loss: 0.18802207708358765\n",
      "Subject 9, Epoch 562, Loss: 1.1595178991556168, Final Batch Loss: 0.22471405565738678\n",
      "Subject 9, Epoch 563, Loss: 1.196087285876274, Final Batch Loss: 0.1968381255865097\n",
      "Subject 9, Epoch 564, Loss: 1.4176779985427856, Final Batch Loss: 0.4648401141166687\n",
      "Subject 9, Epoch 565, Loss: 1.2720695734024048, Final Batch Loss: 0.3133646249771118\n",
      "Subject 9, Epoch 566, Loss: 1.0871244370937347, Final Batch Loss: 0.15418750047683716\n",
      "Subject 9, Epoch 567, Loss: 1.3607814311981201, Final Batch Loss: 0.39809590578079224\n",
      "Subject 9, Epoch 568, Loss: 1.2089532762765884, Final Batch Loss: 0.22430811822414398\n",
      "Subject 9, Epoch 569, Loss: 1.091854527592659, Final Batch Loss: 0.17541809380054474\n",
      "Subject 9, Epoch 570, Loss: 1.1063932925462723, Final Batch Loss: 0.1375429481267929\n",
      "Subject 9, Epoch 571, Loss: 0.9591504707932472, Final Batch Loss: 0.09042099863290787\n",
      "Subject 9, Epoch 572, Loss: 1.1287976503372192, Final Batch Loss: 0.23407450318336487\n",
      "Subject 9, Epoch 573, Loss: 1.1056411266326904, Final Batch Loss: 0.15830808877944946\n",
      "Subject 9, Epoch 574, Loss: 1.0404715463519096, Final Batch Loss: 0.08174892514944077\n",
      "Subject 9, Epoch 575, Loss: 1.4906135201454163, Final Batch Loss: 0.5003535747528076\n",
      "Subject 9, Epoch 576, Loss: 1.4024932086467743, Final Batch Loss: 0.48157262802124023\n",
      "Subject 9, Epoch 577, Loss: 1.3473914861679077, Final Batch Loss: 0.34661194682121277\n",
      "Subject 9, Epoch 578, Loss: 1.2327015697956085, Final Batch Loss: 0.29222819209098816\n",
      "Subject 9, Epoch 579, Loss: 1.188545510172844, Final Batch Loss: 0.1726963073015213\n",
      "Subject 9, Epoch 580, Loss: 1.177198275923729, Final Batch Loss: 0.20874367654323578\n",
      "Subject 9, Epoch 581, Loss: 1.3047288656234741, Final Batch Loss: 0.35913190245628357\n",
      "Subject 9, Epoch 582, Loss: 1.2609170079231262, Final Batch Loss: 0.3346903920173645\n",
      "Subject 9, Epoch 583, Loss: 1.1861089766025543, Final Batch Loss: 0.2384231686592102\n",
      "Subject 9, Epoch 584, Loss: 1.246925175189972, Final Batch Loss: 0.2508482336997986\n",
      "Subject 9, Epoch 585, Loss: 1.1250143945217133, Final Batch Loss: 0.19522124528884888\n",
      "Subject 9, Epoch 586, Loss: 1.1533637940883636, Final Batch Loss: 0.24483656883239746\n",
      "Subject 9, Epoch 587, Loss: 1.3004895448684692, Final Batch Loss: 0.3228435516357422\n",
      "Subject 9, Epoch 588, Loss: 1.144559234380722, Final Batch Loss: 0.22823581099510193\n",
      "Subject 9, Epoch 589, Loss: 1.2120659202337265, Final Batch Loss: 0.20295526087284088\n",
      "Subject 9, Epoch 590, Loss: 1.243835836648941, Final Batch Loss: 0.3734314739704132\n",
      "Subject 9, Epoch 591, Loss: 1.1410892009735107, Final Batch Loss: 0.23793286085128784\n",
      "Subject 9, Epoch 592, Loss: 1.551492989063263, Final Batch Loss: 0.6113218069076538\n",
      "Subject 9, Epoch 593, Loss: 1.5794652998447418, Final Batch Loss: 0.5458816289901733\n",
      "Subject 9, Epoch 594, Loss: 1.4862704873085022, Final Batch Loss: 0.38851234316825867\n",
      "Subject 9, Epoch 595, Loss: 1.4099924564361572, Final Batch Loss: 0.3529638648033142\n",
      "Subject 9, Epoch 596, Loss: 1.2284599244594574, Final Batch Loss: 0.2509464919567108\n",
      "Subject 9, Epoch 597, Loss: 1.3350453972816467, Final Batch Loss: 0.3151104152202606\n",
      "Subject 9, Epoch 598, Loss: 1.268254429101944, Final Batch Loss: 0.28334516286849976\n",
      "Subject 9, Epoch 599, Loss: 1.4298431277275085, Final Batch Loss: 0.37009504437446594\n",
      "Subject 9, Epoch 600, Loss: 1.2726302742958069, Final Batch Loss: 0.24138030409812927\n",
      "Subject 9, Epoch 601, Loss: 1.506611317396164, Final Batch Loss: 0.6461558938026428\n",
      "Subject 9, Epoch 602, Loss: 1.2317927777767181, Final Batch Loss: 0.2992934286594391\n",
      "Subject 9, Epoch 603, Loss: 1.1081455051898956, Final Batch Loss: 0.15478375554084778\n",
      "Subject 9, Epoch 604, Loss: 1.4983462691307068, Final Batch Loss: 0.4516195058822632\n",
      "Subject 9, Epoch 605, Loss: 1.2663147747516632, Final Batch Loss: 0.37034910917282104\n",
      "Subject 9, Epoch 606, Loss: 1.3209709227085114, Final Batch Loss: 0.31466272473335266\n",
      "Subject 9, Epoch 607, Loss: 1.435307800769806, Final Batch Loss: 0.4640321731567383\n",
      "Subject 9, Epoch 608, Loss: 1.3275956511497498, Final Batch Loss: 0.2875280976295471\n",
      "Subject 9, Epoch 609, Loss: 1.3599708378314972, Final Batch Loss: 0.30669447779655457\n",
      "Subject 9, Epoch 610, Loss: 1.5397409498691559, Final Batch Loss: 0.5969915986061096\n",
      "Subject 9, Epoch 611, Loss: 1.4536449909210205, Final Batch Loss: 0.49641987681388855\n",
      "Subject 9, Epoch 612, Loss: 1.4006959199905396, Final Batch Loss: 0.45251771807670593\n",
      "Subject 9, Epoch 613, Loss: 1.4412135779857635, Final Batch Loss: 0.5275813937187195\n",
      "Subject 9, Epoch 614, Loss: 1.2184209823608398, Final Batch Loss: 0.25928279757499695\n",
      "Subject 9, Epoch 615, Loss: 1.2700065970420837, Final Batch Loss: 0.28219687938690186\n",
      "Subject 9, Epoch 616, Loss: 1.0642680823802948, Final Batch Loss: 0.09766766428947449\n",
      "Subject 9, Epoch 617, Loss: 1.4689191579818726, Final Batch Loss: 0.5483378767967224\n",
      "Subject 9, Epoch 618, Loss: 1.284265175461769, Final Batch Loss: 0.3371441960334778\n",
      "Subject 9, Epoch 619, Loss: 1.2501194775104523, Final Batch Loss: 0.35569092631340027\n",
      "Subject 9, Epoch 620, Loss: 1.1396393328905106, Final Batch Loss: 0.23268388211727142\n",
      "Subject 9, Epoch 621, Loss: 1.2830077707767487, Final Batch Loss: 0.3587202727794647\n",
      "Subject 9, Epoch 622, Loss: 1.3994100689888, Final Batch Loss: 0.2554272413253784\n",
      "Subject 9, Epoch 623, Loss: 1.2620374262332916, Final Batch Loss: 0.32244059443473816\n",
      "Subject 9, Epoch 624, Loss: 1.3306604474782944, Final Batch Loss: 0.35223057866096497\n",
      "Subject 9, Epoch 625, Loss: 1.2320020198822021, Final Batch Loss: 0.34588953852653503\n",
      "Subject 9, Epoch 626, Loss: 1.2634604573249817, Final Batch Loss: 0.32728826999664307\n",
      "Subject 9, Epoch 627, Loss: 1.2948524355888367, Final Batch Loss: 0.3072282075881958\n",
      "Subject 9, Epoch 628, Loss: 1.1847716569900513, Final Batch Loss: 0.233967125415802\n",
      "Subject 9, Epoch 629, Loss: 1.2414100766181946, Final Batch Loss: 0.30931365489959717\n",
      "Subject 9, Epoch 630, Loss: 1.4549895077943802, Final Batch Loss: 0.5251527428627014\n",
      "Subject 9, Epoch 631, Loss: 1.2297260463237762, Final Batch Loss: 0.25830820202827454\n",
      "Subject 9, Epoch 632, Loss: 1.1041076928377151, Final Batch Loss: 0.15060915052890778\n",
      "Subject 9, Epoch 633, Loss: 1.2563158869743347, Final Batch Loss: 0.32689550518989563\n",
      "Subject 9, Epoch 634, Loss: 1.3144223988056183, Final Batch Loss: 0.3746317923069\n",
      "Subject 9, Epoch 635, Loss: 1.348512351512909, Final Batch Loss: 0.42547398805618286\n",
      "Subject 9, Epoch 636, Loss: 1.3996144831180573, Final Batch Loss: 0.35062557458877563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 637, Loss: 1.3047027289867401, Final Batch Loss: 0.2580205202102661\n",
      "Subject 9, Epoch 638, Loss: 1.3752310574054718, Final Batch Loss: 0.46484020352363586\n",
      "Subject 9, Epoch 639, Loss: 1.3657204806804657, Final Batch Loss: 0.36751553416252136\n",
      "Subject 9, Epoch 640, Loss: 1.1528636813163757, Final Batch Loss: 0.2736460268497467\n",
      "Subject 9, Epoch 641, Loss: 1.0194158554077148, Final Batch Loss: 0.10377740859985352\n",
      "Subject 9, Epoch 642, Loss: 1.3423836529254913, Final Batch Loss: 0.4054495096206665\n",
      "Subject 9, Epoch 643, Loss: 1.2782799303531647, Final Batch Loss: 0.3829634487628937\n",
      "Subject 9, Epoch 644, Loss: 1.455354481935501, Final Batch Loss: 0.37976664304733276\n",
      "Subject 9, Epoch 645, Loss: 1.0439165830612183, Final Batch Loss: 0.1324373483657837\n",
      "Subject 9, Epoch 646, Loss: 1.1918992698192596, Final Batch Loss: 0.31537550687789917\n",
      "Subject 9, Epoch 647, Loss: 1.0743509978055954, Final Batch Loss: 0.1787462681531906\n",
      "Subject 9, Epoch 648, Loss: 1.2383174300193787, Final Batch Loss: 0.2718650996685028\n",
      "Subject 9, Epoch 649, Loss: 1.0756135284900665, Final Batch Loss: 0.13607263565063477\n",
      "Subject 9, Epoch 650, Loss: 1.3614642322063446, Final Batch Loss: 0.47154396772384644\n",
      "Subject 9, Epoch 651, Loss: 1.2167614698410034, Final Batch Loss: 0.2973562180995941\n",
      "Subject 9, Epoch 652, Loss: 1.3417088091373444, Final Batch Loss: 0.38937854766845703\n",
      "Subject 9, Epoch 653, Loss: 1.1634688675403595, Final Batch Loss: 0.19212746620178223\n",
      "Subject 9, Epoch 654, Loss: 1.1495052725076675, Final Batch Loss: 0.23202644288539886\n",
      "Subject 9, Epoch 655, Loss: 1.1690157651901245, Final Batch Loss: 0.26889050006866455\n",
      "Subject 9, Epoch 656, Loss: 1.0490735173225403, Final Batch Loss: 0.10366463661193848\n",
      "Subject 9, Epoch 657, Loss: 1.1309032142162323, Final Batch Loss: 0.2376944124698639\n",
      "Subject 9, Epoch 658, Loss: 1.0640128999948502, Final Batch Loss: 0.16751347482204437\n",
      "Subject 9, Epoch 659, Loss: 1.4525865316390991, Final Batch Loss: 0.5151925683021545\n",
      "Subject 9, Epoch 660, Loss: 1.1721474826335907, Final Batch Loss: 0.2514457702636719\n",
      "Subject 9, Epoch 661, Loss: 1.1171382665634155, Final Batch Loss: 0.19090038537979126\n",
      "Subject 9, Epoch 662, Loss: 1.2021697908639908, Final Batch Loss: 0.14365170896053314\n",
      "Subject 9, Epoch 663, Loss: 1.181472271680832, Final Batch Loss: 0.24559125304222107\n",
      "Subject 9, Epoch 664, Loss: 1.2519586086273193, Final Batch Loss: 0.33670899271965027\n",
      "Subject 9, Epoch 665, Loss: 1.1128346621990204, Final Batch Loss: 0.21158021688461304\n",
      "Subject 9, Epoch 666, Loss: 1.241243064403534, Final Batch Loss: 0.39685705304145813\n",
      "Subject 9, Epoch 667, Loss: 1.1371000558137894, Final Batch Loss: 0.19359497725963593\n",
      "Subject 9, Epoch 668, Loss: 1.241529941558838, Final Batch Loss: 0.3742615580558777\n",
      "Subject 9, Epoch 669, Loss: 1.340013176202774, Final Batch Loss: 0.4045257270336151\n",
      "Subject 9, Epoch 670, Loss: 1.1944378912448883, Final Batch Loss: 0.3239027261734009\n",
      "Subject 9, Epoch 671, Loss: 1.5389801114797592, Final Batch Loss: 0.6714286804199219\n",
      "Subject 9, Epoch 672, Loss: 1.167160987854004, Final Batch Loss: 0.3144148886203766\n",
      "Subject 9, Epoch 673, Loss: 1.275342434644699, Final Batch Loss: 0.32801395654678345\n",
      "Subject 9, Epoch 674, Loss: 1.0938386768102646, Final Batch Loss: 0.17249353229999542\n",
      "Subject 9, Epoch 675, Loss: 1.0916536301374435, Final Batch Loss: 0.15308378636837006\n",
      "Subject 9, Epoch 676, Loss: 1.2170100063085556, Final Batch Loss: 0.2285008281469345\n",
      "Subject 9, Epoch 677, Loss: 1.1240016967058182, Final Batch Loss: 0.26099681854248047\n",
      "Subject 9, Epoch 678, Loss: 1.4251722991466522, Final Batch Loss: 0.41407835483551025\n",
      "Subject 9, Epoch 679, Loss: 1.3429851531982422, Final Batch Loss: 0.4766016900539398\n",
      "Subject 9, Epoch 680, Loss: 1.2175050377845764, Final Batch Loss: 0.3036078214645386\n",
      "Subject 9, Epoch 681, Loss: 1.215674251317978, Final Batch Loss: 0.3555164933204651\n",
      "Subject 9, Epoch 682, Loss: 1.2048340439796448, Final Batch Loss: 0.313369482755661\n",
      "Subject 9, Epoch 683, Loss: 1.0288984850049019, Final Batch Loss: 0.10233718901872635\n",
      "Subject 9, Epoch 684, Loss: 1.0734416879713535, Final Batch Loss: 0.04941857233643532\n",
      "Subject 9, Epoch 685, Loss: 0.9645729959011078, Final Batch Loss: 0.07257792353630066\n",
      "Subject 9, Epoch 686, Loss: 1.0065957382321358, Final Batch Loss: 0.09585130959749222\n",
      "Subject 9, Epoch 687, Loss: 1.1041648387908936, Final Batch Loss: 0.1167115867137909\n",
      "Subject 9, Epoch 688, Loss: 1.4769473671913147, Final Batch Loss: 0.5510823726654053\n",
      "Subject 9, Epoch 689, Loss: 1.3314004242420197, Final Batch Loss: 0.3825041651725769\n",
      "Subject 9, Epoch 690, Loss: 1.187143862247467, Final Batch Loss: 0.29221537709236145\n",
      "Subject 9, Epoch 691, Loss: 1.1715291142463684, Final Batch Loss: 0.27702292799949646\n",
      "Subject 9, Epoch 692, Loss: 1.4689987152814865, Final Batch Loss: 0.6007813811302185\n",
      "Subject 9, Epoch 693, Loss: 1.179773598909378, Final Batch Loss: 0.22564488649368286\n",
      "Subject 9, Epoch 694, Loss: 1.1917306780815125, Final Batch Loss: 0.2754904329776764\n",
      "Subject 9, Epoch 695, Loss: 1.356311410665512, Final Batch Loss: 0.3981572985649109\n",
      "Subject 9, Epoch 696, Loss: 1.165298879146576, Final Batch Loss: 0.21627673506736755\n",
      "Subject 9, Epoch 697, Loss: 1.1251229494810104, Final Batch Loss: 0.20741774141788483\n",
      "Subject 9, Epoch 698, Loss: 1.1755288988351822, Final Batch Loss: 0.24045167863368988\n",
      "Subject 9, Epoch 699, Loss: 1.1268237829208374, Final Batch Loss: 0.29696303606033325\n",
      "Subject 9, Epoch 700, Loss: 1.192575842142105, Final Batch Loss: 0.3075006306171417\n",
      "Subject 9, Epoch 701, Loss: 0.9313262421637774, Final Batch Loss: 0.01824432797729969\n",
      "Subject 9, Epoch 702, Loss: 1.3042482435703278, Final Batch Loss: 0.41279542446136475\n",
      "Subject 9, Epoch 703, Loss: 1.1619363725185394, Final Batch Loss: 0.1983124315738678\n",
      "Subject 9, Epoch 704, Loss: 1.0560928881168365, Final Batch Loss: 0.17082849144935608\n",
      "Subject 9, Epoch 705, Loss: 1.1932422518730164, Final Batch Loss: 0.36246949434280396\n",
      "Subject 9, Epoch 706, Loss: 1.1070437729358673, Final Batch Loss: 0.26288050413131714\n",
      "Subject 9, Epoch 707, Loss: 1.232672706246376, Final Batch Loss: 0.34060612320899963\n",
      "Subject 9, Epoch 708, Loss: 1.049323931336403, Final Batch Loss: 0.17469826340675354\n",
      "Subject 9, Epoch 709, Loss: 1.476059839129448, Final Batch Loss: 0.6252367496490479\n",
      "Subject 9, Epoch 710, Loss: 1.156881421804428, Final Batch Loss: 0.3102790117263794\n",
      "Subject 9, Epoch 711, Loss: 1.3181779980659485, Final Batch Loss: 0.3047623038291931\n",
      "Subject 9, Epoch 712, Loss: 1.587685078382492, Final Batch Loss: 0.6960129737854004\n",
      "Subject 9, Epoch 713, Loss: 1.3145948350429535, Final Batch Loss: 0.45445868372917175\n",
      "Subject 9, Epoch 714, Loss: 1.2194602638483047, Final Batch Loss: 0.23331449925899506\n",
      "Subject 9, Epoch 715, Loss: 1.492293804883957, Final Batch Loss: 0.4260235130786896\n",
      "Subject 9, Epoch 716, Loss: 1.1823525726795197, Final Batch Loss: 0.3115535080432892\n",
      "Subject 9, Epoch 717, Loss: 1.1329038590192795, Final Batch Loss: 0.19309230148792267\n",
      "Subject 9, Epoch 718, Loss: 1.2638984322547913, Final Batch Loss: 0.32448670268058777\n",
      "Subject 9, Epoch 719, Loss: 1.250521183013916, Final Batch Loss: 0.33560431003570557\n",
      "Subject 9, Epoch 720, Loss: 1.0216703414916992, Final Batch Loss: 0.10655471682548523\n",
      "Subject 9, Epoch 721, Loss: 1.2820002734661102, Final Batch Loss: 0.25697460770606995\n",
      "Subject 9, Epoch 722, Loss: 1.359753280878067, Final Batch Loss: 0.4110134243965149\n",
      "Subject 9, Epoch 723, Loss: 1.1109633594751358, Final Batch Loss: 0.234080970287323\n",
      "Subject 9, Epoch 724, Loss: 1.1012260764837265, Final Batch Loss: 0.16861630976200104\n",
      "Subject 9, Epoch 725, Loss: 1.1760511547327042, Final Batch Loss: 0.255121111869812\n",
      "Subject 9, Epoch 726, Loss: 1.1745246350765228, Final Batch Loss: 0.28780701756477356\n",
      "Subject 9, Epoch 727, Loss: 1.1618353575468063, Final Batch Loss: 0.20931483805179596\n",
      "Subject 9, Epoch 728, Loss: 1.2471443712711334, Final Batch Loss: 0.3295464813709259\n",
      "Subject 9, Epoch 729, Loss: 1.5350020229816437, Final Batch Loss: 0.5104685425758362\n",
      "Subject 9, Epoch 730, Loss: 1.1575077325105667, Final Batch Loss: 0.16905896365642548\n",
      "Subject 9, Epoch 731, Loss: 1.1465495377779007, Final Batch Loss: 0.1633404940366745\n",
      "Subject 9, Epoch 732, Loss: 1.5272832214832306, Final Batch Loss: 0.578174889087677\n",
      "Subject 9, Epoch 733, Loss: 1.2234850227832794, Final Batch Loss: 0.16644948720932007\n",
      "Subject 9, Epoch 734, Loss: 1.4128854274749756, Final Batch Loss: 0.49102556705474854\n",
      "Subject 9, Epoch 735, Loss: 1.3217212557792664, Final Batch Loss: 0.41391274333000183\n",
      "Subject 9, Epoch 736, Loss: 1.1135419458150864, Final Batch Loss: 0.18241015076637268\n",
      "Subject 9, Epoch 737, Loss: 1.135437861084938, Final Batch Loss: 0.1887965351343155\n",
      "Subject 9, Epoch 738, Loss: 1.3176529258489609, Final Batch Loss: 0.3306129276752472\n",
      "Subject 9, Epoch 739, Loss: 1.2408707588911057, Final Batch Loss: 0.22671552002429962\n",
      "Subject 9, Epoch 740, Loss: 1.1870777010917664, Final Batch Loss: 0.28365811705589294\n",
      "Subject 9, Epoch 741, Loss: 1.384096920490265, Final Batch Loss: 0.5108923316001892\n",
      "Subject 9, Epoch 742, Loss: 1.2770977467298508, Final Batch Loss: 0.368218332529068\n",
      "Subject 9, Epoch 743, Loss: 0.998942032456398, Final Batch Loss: 0.12828250229358673\n",
      "Subject 9, Epoch 744, Loss: 1.2259675115346909, Final Batch Loss: 0.3453729450702667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 745, Loss: 1.3225413858890533, Final Batch Loss: 0.4212414026260376\n",
      "Subject 9, Epoch 746, Loss: 1.0957937687635422, Final Batch Loss: 0.2339492291212082\n",
      "Subject 9, Epoch 747, Loss: 1.0405429750680923, Final Batch Loss: 0.1489272564649582\n",
      "Subject 9, Epoch 748, Loss: 1.0565920621156693, Final Batch Loss: 0.16947375237941742\n",
      "Subject 9, Epoch 749, Loss: 1.190694510936737, Final Batch Loss: 0.3026016354560852\n",
      "Subject 9, Epoch 750, Loss: 1.23763769865036, Final Batch Loss: 0.397396057844162\n",
      "Subject 9, Epoch 751, Loss: 1.0960723012685776, Final Batch Loss: 0.17234285175800323\n",
      "Subject 9, Epoch 752, Loss: 1.183463379740715, Final Batch Loss: 0.22152991592884064\n",
      "Subject 9, Epoch 753, Loss: 1.0905752927064896, Final Batch Loss: 0.18664947152137756\n",
      "Subject 9, Epoch 754, Loss: 1.3907279819250107, Final Batch Loss: 0.5232225656509399\n",
      "Subject 9, Epoch 755, Loss: 1.1723205298185349, Final Batch Loss: 0.339976042509079\n",
      "Subject 9, Epoch 756, Loss: 1.461315631866455, Final Batch Loss: 0.523964524269104\n",
      "Subject 9, Epoch 757, Loss: 1.0052489191293716, Final Batch Loss: 0.15661786496639252\n",
      "Subject 9, Epoch 758, Loss: 1.3874025344848633, Final Batch Loss: 0.4092215299606323\n",
      "Subject 9, Epoch 759, Loss: 1.1218818128108978, Final Batch Loss: 0.2327277511358261\n",
      "Subject 9, Epoch 760, Loss: 1.2901800572872162, Final Batch Loss: 0.38771960139274597\n",
      "Subject 9, Epoch 761, Loss: 1.3374904841184616, Final Batch Loss: 0.48035457730293274\n",
      "Subject 9, Epoch 762, Loss: 1.3036526143550873, Final Batch Loss: 0.3719501793384552\n",
      "Subject 9, Epoch 763, Loss: 1.2753391563892365, Final Batch Loss: 0.37953808903694153\n",
      "Subject 9, Epoch 764, Loss: 1.2443988025188446, Final Batch Loss: 0.34524938464164734\n",
      "Subject 9, Epoch 765, Loss: 1.264722317457199, Final Batch Loss: 0.3543042540550232\n",
      "Subject 9, Epoch 766, Loss: 1.119914174079895, Final Batch Loss: 0.1916704773902893\n",
      "Subject 9, Epoch 767, Loss: 1.1289522349834442, Final Batch Loss: 0.25323739647865295\n",
      "Subject 9, Epoch 768, Loss: 1.1681791543960571, Final Batch Loss: 0.30919697880744934\n",
      "Subject 9, Epoch 769, Loss: 1.1167493909597397, Final Batch Loss: 0.19909025728702545\n",
      "Subject 9, Epoch 770, Loss: 1.2901949882507324, Final Batch Loss: 0.44460228085517883\n",
      "Subject 9, Epoch 771, Loss: 1.1634770780801773, Final Batch Loss: 0.26692524552345276\n",
      "Subject 9, Epoch 772, Loss: 1.2799423933029175, Final Batch Loss: 0.3788335621356964\n",
      "Subject 9, Epoch 773, Loss: 1.061041809618473, Final Batch Loss: 0.10510633140802383\n",
      "Subject 9, Epoch 774, Loss: 1.2022883296012878, Final Batch Loss: 0.3171047270298004\n",
      "Subject 9, Epoch 775, Loss: 1.2672405540943146, Final Batch Loss: 0.37824639678001404\n",
      "Subject 9, Epoch 776, Loss: 1.1191623955965042, Final Batch Loss: 0.21372617781162262\n",
      "Subject 9, Epoch 777, Loss: 1.2598200738430023, Final Batch Loss: 0.3952983617782593\n",
      "Subject 9, Epoch 778, Loss: 1.2034924626350403, Final Batch Loss: 0.3466259837150574\n",
      "Subject 9, Epoch 779, Loss: 1.0709370076656342, Final Batch Loss: 0.24227944016456604\n",
      "Subject 9, Epoch 780, Loss: 1.2712381780147552, Final Batch Loss: 0.3160581588745117\n",
      "Subject 9, Epoch 781, Loss: 1.0570333898067474, Final Batch Loss: 0.21454033255577087\n",
      "Subject 9, Epoch 782, Loss: 1.2561184912919998, Final Batch Loss: 0.41297462582588196\n",
      "Subject 9, Epoch 783, Loss: 1.1045056879520416, Final Batch Loss: 0.24329859018325806\n",
      "Subject 9, Epoch 784, Loss: 1.0061962381005287, Final Batch Loss: 0.08583498746156693\n",
      "Subject 9, Epoch 785, Loss: 1.1835967004299164, Final Batch Loss: 0.3342643082141876\n",
      "Subject 9, Epoch 786, Loss: 1.2751788198947906, Final Batch Loss: 0.4378175437450409\n",
      "Subject 9, Epoch 787, Loss: 1.2056459784507751, Final Batch Loss: 0.3177211582660675\n",
      "Subject 9, Epoch 788, Loss: 1.1707076132297516, Final Batch Loss: 0.30331742763519287\n",
      "Subject 9, Epoch 789, Loss: 1.186601996421814, Final Batch Loss: 0.36010387539863586\n",
      "Subject 9, Epoch 790, Loss: 1.287272721529007, Final Batch Loss: 0.3930072486400604\n",
      "Subject 9, Epoch 791, Loss: 0.9644094482064247, Final Batch Loss: 0.06692347675561905\n",
      "Subject 9, Epoch 792, Loss: 0.9875615239143372, Final Batch Loss: 0.1171579360961914\n",
      "Subject 9, Epoch 793, Loss: 1.148295134305954, Final Batch Loss: 0.30646267533302307\n",
      "Subject 9, Epoch 794, Loss: 1.1710104644298553, Final Batch Loss: 0.3006855249404907\n",
      "Subject 9, Epoch 795, Loss: 1.1578528583049774, Final Batch Loss: 0.272309273481369\n",
      "Subject 9, Epoch 796, Loss: 1.2273903489112854, Final Batch Loss: 0.36852237582206726\n",
      "Subject 9, Epoch 797, Loss: 0.941080667078495, Final Batch Loss: 0.12184836715459824\n",
      "Subject 9, Epoch 798, Loss: 1.464533507823944, Final Batch Loss: 0.6197128891944885\n",
      "Subject 9, Epoch 799, Loss: 1.1158442795276642, Final Batch Loss: 0.2714424729347229\n",
      "Subject 9, Epoch 800, Loss: 1.2397594451904297, Final Batch Loss: 0.2947368323802948\n",
      "Subject 9, Epoch 801, Loss: 1.271119773387909, Final Batch Loss: 0.36442244052886963\n",
      "Subject 9, Epoch 802, Loss: 1.1431177109479904, Final Batch Loss: 0.21648524701595306\n",
      "Subject 9, Epoch 803, Loss: 1.0944139659404755, Final Batch Loss: 0.1804489940404892\n",
      "Subject 9, Epoch 804, Loss: 1.2153474688529968, Final Batch Loss: 0.3150580823421478\n",
      "Subject 9, Epoch 805, Loss: 1.0530024096369743, Final Batch Loss: 0.11782460659742355\n",
      "Subject 9, Epoch 806, Loss: 1.097558133304119, Final Batch Loss: 0.11752574890851974\n",
      "Subject 9, Epoch 807, Loss: 1.0683405697345734, Final Batch Loss: 0.180105060338974\n",
      "Subject 9, Epoch 808, Loss: 1.3204722106456757, Final Batch Loss: 0.4662434458732605\n",
      "Subject 9, Epoch 809, Loss: 1.0692030191421509, Final Batch Loss: 0.19055892527103424\n",
      "Subject 9, Epoch 810, Loss: 1.163400262594223, Final Batch Loss: 0.3121212124824524\n",
      "Subject 9, Epoch 811, Loss: 1.2047052085399628, Final Batch Loss: 0.3804769814014435\n",
      "Subject 9, Epoch 812, Loss: 1.303991287946701, Final Batch Loss: 0.3574550151824951\n",
      "Subject 9, Epoch 813, Loss: 1.3942051231861115, Final Batch Loss: 0.4146600067615509\n",
      "Subject 9, Epoch 814, Loss: 1.1054346859455109, Final Batch Loss: 0.17655768990516663\n",
      "Subject 9, Epoch 815, Loss: 1.0177299082279205, Final Batch Loss: 0.16998571157455444\n",
      "Subject 9, Epoch 816, Loss: 1.2379363477230072, Final Batch Loss: 0.3939397931098938\n",
      "Subject 9, Epoch 817, Loss: 1.2040634751319885, Final Batch Loss: 0.33499830961227417\n",
      "Subject 9, Epoch 818, Loss: 1.274299144744873, Final Batch Loss: 0.3456461727619171\n",
      "Subject 9, Epoch 819, Loss: 1.308243453502655, Final Batch Loss: 0.44045162200927734\n",
      "Subject 9, Epoch 820, Loss: 0.9419952258467674, Final Batch Loss: 0.04155879467725754\n",
      "Subject 9, Epoch 821, Loss: 1.1442705690860748, Final Batch Loss: 0.3214893043041229\n",
      "Subject 9, Epoch 822, Loss: 1.1735188364982605, Final Batch Loss: 0.2773321866989136\n",
      "Subject 9, Epoch 823, Loss: 1.0965889543294907, Final Batch Loss: 0.21257568895816803\n",
      "Subject 9, Epoch 824, Loss: 1.2660786509513855, Final Batch Loss: 0.4753517210483551\n",
      "Subject 9, Epoch 825, Loss: 1.0816644877195358, Final Batch Loss: 0.2151319831609726\n",
      "Subject 9, Epoch 826, Loss: 1.1038193106651306, Final Batch Loss: 0.22483810782432556\n",
      "Subject 9, Epoch 827, Loss: 1.0688259601593018, Final Batch Loss: 0.17575256526470184\n",
      "Subject 9, Epoch 828, Loss: 1.5512790381908417, Final Batch Loss: 0.6942899227142334\n",
      "Subject 9, Epoch 829, Loss: 1.2756845951080322, Final Batch Loss: 0.45308229327201843\n",
      "Subject 9, Epoch 830, Loss: 1.6008424162864685, Final Batch Loss: 0.5513794422149658\n",
      "Subject 9, Epoch 831, Loss: 1.066115640103817, Final Batch Loss: 0.11609666794538498\n",
      "Subject 9, Epoch 832, Loss: 1.0723905861377716, Final Batch Loss: 0.22227786481380463\n",
      "Subject 9, Epoch 833, Loss: 1.2935121059417725, Final Batch Loss: 0.40846797823905945\n",
      "Subject 9, Epoch 834, Loss: 1.286876231431961, Final Batch Loss: 0.4168103039264679\n",
      "Subject 9, Epoch 835, Loss: 1.2336511015892029, Final Batch Loss: 0.375922292470932\n",
      "Subject 9, Epoch 836, Loss: 1.1896724104881287, Final Batch Loss: 0.3349308371543884\n",
      "Subject 9, Epoch 837, Loss: 1.2173107862472534, Final Batch Loss: 0.3585827648639679\n",
      "Subject 9, Epoch 838, Loss: 1.0987155884504318, Final Batch Loss: 0.21536065638065338\n",
      "Subject 9, Epoch 839, Loss: 1.0052093267440796, Final Batch Loss: 0.1501958966255188\n",
      "Subject 9, Epoch 840, Loss: 1.2312671095132828, Final Batch Loss: 0.39975297451019287\n",
      "Subject 9, Epoch 841, Loss: 1.2078475654125214, Final Batch Loss: 0.32054150104522705\n",
      "Subject 9, Epoch 842, Loss: 1.2784345895051956, Final Batch Loss: 0.43689486384391785\n",
      "Subject 9, Epoch 843, Loss: 1.057147279381752, Final Batch Loss: 0.14525021612644196\n",
      "Subject 9, Epoch 844, Loss: 1.096600353717804, Final Batch Loss: 0.2531844675540924\n",
      "Subject 9, Epoch 845, Loss: 1.0533349961042404, Final Batch Loss: 0.19888748228549957\n",
      "Subject 9, Epoch 846, Loss: 1.0362930595874786, Final Batch Loss: 0.16960924863815308\n",
      "Subject 9, Epoch 847, Loss: 1.0264144241809845, Final Batch Loss: 0.20864783227443695\n",
      "Subject 9, Epoch 848, Loss: 0.976945161819458, Final Batch Loss: 0.11865966022014618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 849, Loss: 1.1842783987522125, Final Batch Loss: 0.3599267303943634\n",
      "Subject 9, Epoch 850, Loss: 0.9777835458517075, Final Batch Loss: 0.08063815534114838\n",
      "Subject 9, Epoch 851, Loss: 0.9193064570426941, Final Batch Loss: 0.057053446769714355\n",
      "Subject 9, Epoch 852, Loss: 1.1592945009469986, Final Batch Loss: 0.2835627794265747\n",
      "Subject 9, Epoch 853, Loss: 1.1095995604991913, Final Batch Loss: 0.19181323051452637\n",
      "Subject 9, Epoch 854, Loss: 1.067685380578041, Final Batch Loss: 0.1615721434354782\n",
      "Subject 9, Epoch 855, Loss: 1.0849191099405289, Final Batch Loss: 0.24770516157150269\n",
      "Subject 9, Epoch 856, Loss: 1.2161463499069214, Final Batch Loss: 0.3771861791610718\n",
      "Subject 9, Epoch 857, Loss: 1.4102678298950195, Final Batch Loss: 0.5779209733009338\n",
      "Subject 9, Epoch 858, Loss: 1.275168925523758, Final Batch Loss: 0.3153418302536011\n",
      "Subject 9, Epoch 859, Loss: 1.1018410325050354, Final Batch Loss: 0.16807836294174194\n",
      "Subject 9, Epoch 860, Loss: 1.319721907377243, Final Batch Loss: 0.4065760374069214\n",
      "Subject 9, Epoch 861, Loss: 1.0557103753089905, Final Batch Loss: 0.13287153840065002\n",
      "Subject 9, Epoch 862, Loss: 0.968338742852211, Final Batch Loss: 0.16000200808048248\n",
      "Subject 9, Epoch 863, Loss: 1.1039647161960602, Final Batch Loss: 0.2528804838657379\n",
      "Subject 9, Epoch 864, Loss: 1.2176597118377686, Final Batch Loss: 0.3228735327720642\n",
      "Subject 9, Epoch 865, Loss: 1.1041057109832764, Final Batch Loss: 0.2640244662761688\n",
      "Subject 9, Epoch 866, Loss: 1.0217799693346024, Final Batch Loss: 0.1749502718448639\n",
      "Subject 9, Epoch 867, Loss: 1.1756548583507538, Final Batch Loss: 0.29016759991645813\n",
      "Subject 9, Epoch 868, Loss: 1.0070658475160599, Final Batch Loss: 0.19476614892482758\n",
      "Subject 9, Epoch 869, Loss: 1.3835574984550476, Final Batch Loss: 0.362441748380661\n",
      "Subject 9, Epoch 870, Loss: 1.1470032781362534, Final Batch Loss: 0.29175734519958496\n",
      "Subject 9, Epoch 871, Loss: 1.1471831500530243, Final Batch Loss: 0.26329323649406433\n",
      "Subject 9, Epoch 872, Loss: 1.125013530254364, Final Batch Loss: 0.2576451897621155\n",
      "Subject 9, Epoch 873, Loss: 1.1607904732227325, Final Batch Loss: 0.314419150352478\n",
      "Subject 9, Epoch 874, Loss: 1.1872105598449707, Final Batch Loss: 0.3331775665283203\n",
      "Subject 9, Epoch 875, Loss: 1.1393128633499146, Final Batch Loss: 0.3215162456035614\n",
      "Subject 9, Epoch 876, Loss: 1.1622238159179688, Final Batch Loss: 0.34378913044929504\n",
      "Subject 9, Epoch 877, Loss: 1.2196913361549377, Final Batch Loss: 0.37887534499168396\n",
      "Subject 9, Epoch 878, Loss: 0.8424012139439583, Final Batch Loss: 0.03393805772066116\n",
      "Subject 9, Epoch 879, Loss: 1.27676060795784, Final Batch Loss: 0.3432832360267639\n",
      "Subject 9, Epoch 880, Loss: 1.0196902304887772, Final Batch Loss: 0.20465384423732758\n",
      "Subject 9, Epoch 881, Loss: 1.167353093624115, Final Batch Loss: 0.34187716245651245\n",
      "Subject 9, Epoch 882, Loss: 1.0433079823851585, Final Batch Loss: 0.06535273045301437\n",
      "Subject 9, Epoch 883, Loss: 1.177554652094841, Final Batch Loss: 0.3388676941394806\n",
      "Subject 9, Epoch 884, Loss: 1.1469383537769318, Final Batch Loss: 0.36568719148635864\n",
      "Subject 9, Epoch 885, Loss: 0.9590872675180435, Final Batch Loss: 0.1388576477766037\n",
      "Subject 9, Epoch 886, Loss: 1.05794557929039, Final Batch Loss: 0.28910669684410095\n",
      "Subject 9, Epoch 887, Loss: 1.1083735078573227, Final Batch Loss: 0.31767281889915466\n",
      "Subject 9, Epoch 888, Loss: 0.9507435709238052, Final Batch Loss: 0.18685971200466156\n",
      "Subject 9, Epoch 889, Loss: 0.9773304462432861, Final Batch Loss: 0.16765625774860382\n",
      "Subject 9, Epoch 890, Loss: 1.2608663439750671, Final Batch Loss: 0.4328521490097046\n",
      "Subject 9, Epoch 891, Loss: 1.349226474761963, Final Batch Loss: 0.49616315960884094\n",
      "Subject 9, Epoch 892, Loss: 1.0120094418525696, Final Batch Loss: 0.16426189243793488\n",
      "Subject 9, Epoch 893, Loss: 0.9025450348854065, Final Batch Loss: 0.08442336320877075\n",
      "Subject 9, Epoch 894, Loss: 1.4693409502506256, Final Batch Loss: 0.5778341293334961\n",
      "Subject 9, Epoch 895, Loss: 1.1864564418792725, Final Batch Loss: 0.2819371819496155\n",
      "Subject 9, Epoch 896, Loss: 1.2461068332195282, Final Batch Loss: 0.3030093014240265\n",
      "Subject 9, Epoch 897, Loss: 1.113553762435913, Final Batch Loss: 0.2176278978586197\n",
      "Subject 9, Epoch 898, Loss: 1.1911587566137314, Final Batch Loss: 0.4140828847885132\n",
      "Subject 9, Epoch 899, Loss: 1.238370105624199, Final Batch Loss: 0.43527376651763916\n",
      "Subject 9, Epoch 900, Loss: 1.1443895995616913, Final Batch Loss: 0.3567536771297455\n",
      "Subject 9, Epoch 901, Loss: 1.1655816733837128, Final Batch Loss: 0.39126256108283997\n",
      "Subject 9, Epoch 902, Loss: 1.0650851428508759, Final Batch Loss: 0.299972802400589\n",
      "Subject 9, Epoch 903, Loss: 1.0357155352830887, Final Batch Loss: 0.16747616231441498\n",
      "Subject 9, Epoch 904, Loss: 0.9721430242061615, Final Batch Loss: 0.15229040384292603\n",
      "Subject 9, Epoch 905, Loss: 1.0787147283554077, Final Batch Loss: 0.29934242367744446\n",
      "Subject 9, Epoch 906, Loss: 1.135695904493332, Final Batch Loss: 0.35903409123420715\n",
      "Subject 9, Epoch 907, Loss: 1.03712859749794, Final Batch Loss: 0.2750416100025177\n",
      "Subject 9, Epoch 908, Loss: 1.0398686528205872, Final Batch Loss: 0.2417038083076477\n",
      "Subject 9, Epoch 909, Loss: 0.8651495724916458, Final Batch Loss: 0.07118724286556244\n",
      "Subject 9, Epoch 910, Loss: 1.0705374777317047, Final Batch Loss: 0.27240678668022156\n",
      "Subject 9, Epoch 911, Loss: 0.9047635942697525, Final Batch Loss: 0.13225869834423065\n",
      "Subject 9, Epoch 912, Loss: 1.1279274225234985, Final Batch Loss: 0.31644439697265625\n",
      "Subject 9, Epoch 913, Loss: 1.2342457473278046, Final Batch Loss: 0.3962707817554474\n",
      "Subject 9, Epoch 914, Loss: 1.1205372363328934, Final Batch Loss: 0.27433446049690247\n",
      "Subject 9, Epoch 915, Loss: 1.0963041633367538, Final Batch Loss: 0.2838142216205597\n",
      "Subject 9, Epoch 916, Loss: 1.197057157754898, Final Batch Loss: 0.44374319911003113\n",
      "Subject 9, Epoch 917, Loss: 1.0267958343029022, Final Batch Loss: 0.2969329059123993\n",
      "Subject 9, Epoch 918, Loss: 1.0126735419034958, Final Batch Loss: 0.24121952056884766\n",
      "Subject 9, Epoch 919, Loss: 1.2299228608608246, Final Batch Loss: 0.3814644515514374\n",
      "Subject 9, Epoch 920, Loss: 1.2505589127540588, Final Batch Loss: 0.40972834825515747\n",
      "Subject 9, Epoch 921, Loss: 0.8922236636281013, Final Batch Loss: 0.10376321524381638\n",
      "Subject 9, Epoch 922, Loss: 1.0537360906600952, Final Batch Loss: 0.207451730966568\n",
      "Subject 9, Epoch 923, Loss: 1.1647815257310867, Final Batch Loss: 0.3911319077014923\n",
      "Subject 9, Epoch 924, Loss: 1.161893218755722, Final Batch Loss: 0.3889259099960327\n",
      "Subject 9, Epoch 925, Loss: 0.9687488973140717, Final Batch Loss: 0.1708391308784485\n",
      "Subject 9, Epoch 926, Loss: 0.9758437275886536, Final Batch Loss: 0.19794420897960663\n",
      "Subject 9, Epoch 927, Loss: 1.1134894713759422, Final Batch Loss: 0.10305597633123398\n",
      "Subject 9, Epoch 928, Loss: 1.1574178338050842, Final Batch Loss: 0.3375099003314972\n",
      "Subject 9, Epoch 929, Loss: 1.3847045749425888, Final Batch Loss: 0.5792273879051208\n",
      "Subject 9, Epoch 930, Loss: 1.2074957638978958, Final Batch Loss: 0.43690547347068787\n",
      "Subject 9, Epoch 931, Loss: 1.2657215148210526, Final Batch Loss: 0.45706164836883545\n",
      "Subject 9, Epoch 932, Loss: 1.02725188434124, Final Batch Loss: 0.25215867161750793\n",
      "Subject 9, Epoch 933, Loss: 1.2644811570644379, Final Batch Loss: 0.5144481658935547\n",
      "Subject 9, Epoch 934, Loss: 1.2498759627342224, Final Batch Loss: 0.475261390209198\n",
      "Subject 9, Epoch 935, Loss: 0.9829462468624115, Final Batch Loss: 0.19961372017860413\n",
      "Subject 9, Epoch 936, Loss: 0.9493093267083168, Final Batch Loss: 0.10092876106500626\n",
      "Subject 9, Epoch 937, Loss: 1.1351846605539322, Final Batch Loss: 0.38602641224861145\n",
      "Subject 9, Epoch 938, Loss: 1.1248738318681717, Final Batch Loss: 0.39736852049827576\n",
      "Subject 9, Epoch 939, Loss: 1.3798400461673737, Final Batch Loss: 0.6648457050323486\n",
      "Subject 9, Epoch 940, Loss: 1.0481035709381104, Final Batch Loss: 0.2667830288410187\n",
      "Subject 9, Epoch 941, Loss: 1.0076459795236588, Final Batch Loss: 0.21625223755836487\n",
      "Subject 9, Epoch 942, Loss: 1.016799047589302, Final Batch Loss: 0.19717368483543396\n",
      "Subject 9, Epoch 943, Loss: 1.2219109237194061, Final Batch Loss: 0.4521081745624542\n",
      "Subject 9, Epoch 944, Loss: 0.9708816856145859, Final Batch Loss: 0.20137129724025726\n",
      "Subject 9, Epoch 945, Loss: 1.068909227848053, Final Batch Loss: 0.29800334572792053\n",
      "Subject 9, Epoch 946, Loss: 1.0415960177779198, Final Batch Loss: 0.10692810267210007\n",
      "Subject 9, Epoch 947, Loss: 1.2480940222740173, Final Batch Loss: 0.4081460237503052\n",
      "Subject 9, Epoch 948, Loss: 1.2476036846637726, Final Batch Loss: 0.3691161274909973\n",
      "Subject 9, Epoch 949, Loss: 1.1443372070789337, Final Batch Loss: 0.2613678574562073\n",
      "Subject 9, Epoch 950, Loss: 1.1028093546628952, Final Batch Loss: 0.3298567533493042\n",
      "Subject 9, Epoch 951, Loss: 1.0059968382120132, Final Batch Loss: 0.06431473791599274\n",
      "Subject 9, Epoch 952, Loss: 1.2489215284585953, Final Batch Loss: 0.468336284160614\n",
      "Subject 9, Epoch 953, Loss: 1.1742073595523834, Final Batch Loss: 0.25634562969207764\n",
      "Subject 9, Epoch 954, Loss: 1.3430475741624832, Final Batch Loss: 0.5659568309783936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 9, Epoch 955, Loss: 1.175104781985283, Final Batch Loss: 0.3734579384326935\n",
      "Subject 9, Epoch 956, Loss: 1.14089934527874, Final Batch Loss: 0.34535905718803406\n",
      "Subject 9, Epoch 957, Loss: 1.2454937249422073, Final Batch Loss: 0.3477288484573364\n",
      "Subject 9, Epoch 958, Loss: 0.9689276218414307, Final Batch Loss: 0.16227947175502777\n",
      "Subject 9, Epoch 959, Loss: 1.0974183082580566, Final Batch Loss: 0.28356075286865234\n",
      "Subject 9, Epoch 960, Loss: 1.1340306997299194, Final Batch Loss: 0.2643411457538605\n",
      "Subject 9, Epoch 961, Loss: 1.0824171900749207, Final Batch Loss: 0.273539274930954\n",
      "Subject 9, Epoch 962, Loss: 1.0868219137191772, Final Batch Loss: 0.3172575533390045\n",
      "Subject 9, Epoch 963, Loss: 0.9754811823368073, Final Batch Loss: 0.18507127463817596\n",
      "Subject 9, Epoch 964, Loss: 1.160346657037735, Final Batch Loss: 0.3405137360095978\n",
      "Subject 9, Epoch 965, Loss: 1.1267606019973755, Final Batch Loss: 0.37515562772750854\n",
      "Subject 9, Epoch 966, Loss: 1.0454458743333817, Final Batch Loss: 0.26586195826530457\n",
      "Subject 9, Epoch 967, Loss: 1.0333650410175323, Final Batch Loss: 0.24329300224781036\n",
      "Subject 9, Epoch 968, Loss: 0.8808223083615303, Final Batch Loss: 0.06939629465341568\n",
      "Subject 9, Epoch 969, Loss: 0.9525900781154633, Final Batch Loss: 0.15407328307628632\n",
      "Subject 9, Epoch 970, Loss: 1.0009936541318893, Final Batch Loss: 0.29323384165763855\n",
      "Subject 9, Epoch 971, Loss: 0.8470438867807388, Final Batch Loss: 0.05863182246685028\n",
      "Subject 9, Epoch 972, Loss: 1.6817372739315033, Final Batch Loss: 0.8351218700408936\n",
      "Subject 9, Epoch 973, Loss: 0.9810502827167511, Final Batch Loss: 0.1274162381887436\n",
      "Subject 9, Epoch 974, Loss: 1.1384416818618774, Final Batch Loss: 0.3466637432575226\n",
      "Subject 9, Epoch 975, Loss: 1.2137585431337357, Final Batch Loss: 0.4768451750278473\n",
      "Subject 9, Epoch 976, Loss: 1.1994399428367615, Final Batch Loss: 0.3179076313972473\n",
      "Subject 9, Epoch 977, Loss: 0.8666601479053497, Final Batch Loss: 0.14118099212646484\n",
      "Subject 9, Epoch 978, Loss: 1.2328664660453796, Final Batch Loss: 0.5242782831192017\n",
      "Subject 9, Epoch 979, Loss: 1.1055758446455002, Final Batch Loss: 0.2687857449054718\n",
      "Subject 9, Epoch 980, Loss: 0.8909851163625717, Final Batch Loss: 0.05998899042606354\n",
      "Subject 9, Epoch 981, Loss: 0.9137069880962372, Final Batch Loss: 0.1365944743156433\n",
      "Subject 9, Epoch 982, Loss: 1.3098281174898148, Final Batch Loss: 0.5478588938713074\n",
      "Subject 9, Epoch 983, Loss: 1.1049789935350418, Final Batch Loss: 0.2899283170700073\n",
      "Subject 9, Epoch 984, Loss: 1.0322982221841812, Final Batch Loss: 0.230394646525383\n",
      "Subject 9, Epoch 985, Loss: 0.9246519953012466, Final Batch Loss: 0.17693424224853516\n",
      "Subject 9, Epoch 986, Loss: 1.0576215535402298, Final Batch Loss: 0.15397566556930542\n",
      "Subject 9, Epoch 987, Loss: 1.2263353168964386, Final Batch Loss: 0.4410872161388397\n",
      "Subject 9, Epoch 988, Loss: 0.8627702295780182, Final Batch Loss: 0.11884264647960663\n",
      "Subject 9, Epoch 989, Loss: 0.9286095723509789, Final Batch Loss: 0.10211861878633499\n",
      "Subject 9, Epoch 990, Loss: 1.1535298377275467, Final Batch Loss: 0.3554932177066803\n",
      "Subject 9, Epoch 991, Loss: 1.0495765954256058, Final Batch Loss: 0.22649140655994415\n",
      "Subject 9, Epoch 992, Loss: 1.3178728967905045, Final Batch Loss: 0.5218392014503479\n",
      "Subject 9, Epoch 993, Loss: 1.1801705211400986, Final Batch Loss: 0.31919366121292114\n",
      "Subject 9, Epoch 994, Loss: 1.0567785054445267, Final Batch Loss: 0.2809624969959259\n",
      "Subject 9, Epoch 995, Loss: 0.9302186742424965, Final Batch Loss: 0.11627209931612015\n",
      "Subject 9, Epoch 996, Loss: 0.93144291639328, Final Batch Loss: 0.18723860383033752\n",
      "Subject 9, Epoch 997, Loss: 0.9332700446248055, Final Batch Loss: 0.11998698860406876\n",
      "Subject 9, Epoch 998, Loss: 0.9668716937303543, Final Batch Loss: 0.16722743213176727\n",
      "Subject 9, Epoch 999, Loss: 1.2864772975444794, Final Batch Loss: 0.30808666348457336\n",
      "Subject 9, Epoch 1000, Loss: 1.1664842367172241, Final Batch Loss: 0.3244704604148865\n",
      "Subject 10, Epoch 1, Loss: 7.099870443344116, Final Batch Loss: 1.7258368730545044\n",
      "Subject 10, Epoch 2, Loss: 7.13109278678894, Final Batch Loss: 1.7867902517318726\n",
      "Subject 10, Epoch 3, Loss: 7.165573835372925, Final Batch Loss: 1.8323558568954468\n",
      "Subject 10, Epoch 4, Loss: 7.02378237247467, Final Batch Loss: 1.6973447799682617\n",
      "Subject 10, Epoch 5, Loss: 6.987558126449585, Final Batch Loss: 1.6750096082687378\n",
      "Subject 10, Epoch 6, Loss: 7.021866083145142, Final Batch Loss: 1.7389469146728516\n",
      "Subject 10, Epoch 7, Loss: 7.003079891204834, Final Batch Loss: 1.7700153589248657\n",
      "Subject 10, Epoch 8, Loss: 6.980478286743164, Final Batch Loss: 1.7724701166152954\n",
      "Subject 10, Epoch 9, Loss: 6.829577684402466, Final Batch Loss: 1.666261076927185\n",
      "Subject 10, Epoch 10, Loss: 6.7301013469696045, Final Batch Loss: 1.6363776922225952\n",
      "Subject 10, Epoch 11, Loss: 6.7009851932525635, Final Batch Loss: 1.6771835088729858\n",
      "Subject 10, Epoch 12, Loss: 6.6128456592559814, Final Batch Loss: 1.6719250679016113\n",
      "Subject 10, Epoch 13, Loss: 6.597710013389587, Final Batch Loss: 1.785923957824707\n",
      "Subject 10, Epoch 14, Loss: 6.106911301612854, Final Batch Loss: 1.364261269569397\n",
      "Subject 10, Epoch 15, Loss: 6.313903570175171, Final Batch Loss: 1.6608606576919556\n",
      "Subject 10, Epoch 16, Loss: 6.050183176994324, Final Batch Loss: 1.434098720550537\n",
      "Subject 10, Epoch 17, Loss: 5.8039573431015015, Final Batch Loss: 1.2721349000930786\n",
      "Subject 10, Epoch 18, Loss: 6.171717166900635, Final Batch Loss: 1.6722450256347656\n",
      "Subject 10, Epoch 19, Loss: 5.867661952972412, Final Batch Loss: 1.4899147748947144\n",
      "Subject 10, Epoch 20, Loss: 5.884996175765991, Final Batch Loss: 1.5122756958007812\n",
      "Subject 10, Epoch 21, Loss: 5.558062791824341, Final Batch Loss: 1.2305567264556885\n",
      "Subject 10, Epoch 22, Loss: 5.650139689445496, Final Batch Loss: 1.323862075805664\n",
      "Subject 10, Epoch 23, Loss: 5.642627477645874, Final Batch Loss: 1.4407998323440552\n",
      "Subject 10, Epoch 24, Loss: 5.5499587059021, Final Batch Loss: 1.3789548873901367\n",
      "Subject 10, Epoch 25, Loss: 5.387280464172363, Final Batch Loss: 1.3544007539749146\n",
      "Subject 10, Epoch 26, Loss: 5.1613394021987915, Final Batch Loss: 1.2306461334228516\n",
      "Subject 10, Epoch 27, Loss: 5.027299165725708, Final Batch Loss: 1.1897636651992798\n",
      "Subject 10, Epoch 28, Loss: 5.043732762336731, Final Batch Loss: 1.2510920763015747\n",
      "Subject 10, Epoch 29, Loss: 5.00259256362915, Final Batch Loss: 1.2513096332550049\n",
      "Subject 10, Epoch 30, Loss: 4.959599614143372, Final Batch Loss: 1.3259937763214111\n",
      "Subject 10, Epoch 31, Loss: 4.864951491355896, Final Batch Loss: 1.1636229753494263\n",
      "Subject 10, Epoch 32, Loss: 4.809166193008423, Final Batch Loss: 1.1760116815567017\n",
      "Subject 10, Epoch 33, Loss: 4.576622128486633, Final Batch Loss: 1.143717646598816\n",
      "Subject 10, Epoch 34, Loss: 4.5228025913238525, Final Batch Loss: 1.0180920362472534\n",
      "Subject 10, Epoch 35, Loss: 4.694359660148621, Final Batch Loss: 1.126716136932373\n",
      "Subject 10, Epoch 36, Loss: 4.596999526023865, Final Batch Loss: 1.1398667097091675\n",
      "Subject 10, Epoch 37, Loss: 4.536254048347473, Final Batch Loss: 1.1032475233078003\n",
      "Subject 10, Epoch 38, Loss: 4.606862187385559, Final Batch Loss: 1.156371831893921\n",
      "Subject 10, Epoch 39, Loss: 4.600149989128113, Final Batch Loss: 1.1535853147506714\n",
      "Subject 10, Epoch 40, Loss: 4.520524501800537, Final Batch Loss: 1.0280176401138306\n",
      "Subject 10, Epoch 41, Loss: 4.757474899291992, Final Batch Loss: 1.2960723638534546\n",
      "Subject 10, Epoch 42, Loss: 4.419649243354797, Final Batch Loss: 1.1433168649673462\n",
      "Subject 10, Epoch 43, Loss: 4.702921032905579, Final Batch Loss: 1.3057503700256348\n",
      "Subject 10, Epoch 44, Loss: 4.467970609664917, Final Batch Loss: 1.1327005624771118\n",
      "Subject 10, Epoch 45, Loss: 4.452239274978638, Final Batch Loss: 1.059322714805603\n",
      "Subject 10, Epoch 46, Loss: 4.42016065120697, Final Batch Loss: 1.1745492219924927\n",
      "Subject 10, Epoch 47, Loss: 4.474857211112976, Final Batch Loss: 1.1149187088012695\n",
      "Subject 10, Epoch 48, Loss: 4.549821615219116, Final Batch Loss: 1.192802906036377\n",
      "Subject 10, Epoch 49, Loss: 4.404470443725586, Final Batch Loss: 1.0664292573928833\n",
      "Subject 10, Epoch 50, Loss: 4.376505136489868, Final Batch Loss: 1.1143871545791626\n",
      "Subject 10, Epoch 51, Loss: 4.515368819236755, Final Batch Loss: 1.2102853059768677\n",
      "Subject 10, Epoch 52, Loss: 4.411473512649536, Final Batch Loss: 1.1592302322387695\n",
      "Subject 10, Epoch 53, Loss: 4.384637951850891, Final Batch Loss: 1.0901037454605103\n",
      "Subject 10, Epoch 54, Loss: 4.145533621311188, Final Batch Loss: 0.9516907334327698\n",
      "Subject 10, Epoch 55, Loss: 4.109702825546265, Final Batch Loss: 0.965294599533081\n",
      "Subject 10, Epoch 56, Loss: 4.192816615104675, Final Batch Loss: 1.0042051076889038\n",
      "Subject 10, Epoch 57, Loss: 4.0943663120269775, Final Batch Loss: 1.040245771408081\n",
      "Subject 10, Epoch 58, Loss: 4.051474153995514, Final Batch Loss: 1.0542408227920532\n",
      "Subject 10, Epoch 59, Loss: 3.9739531874656677, Final Batch Loss: 0.8546896576881409\n",
      "Subject 10, Epoch 60, Loss: 3.9232032895088196, Final Batch Loss: 0.8922638297080994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 61, Loss: 3.910481870174408, Final Batch Loss: 0.9491177201271057\n",
      "Subject 10, Epoch 62, Loss: 3.8724148869514465, Final Batch Loss: 0.941234290599823\n",
      "Subject 10, Epoch 63, Loss: 4.124695122241974, Final Batch Loss: 1.0598156452178955\n",
      "Subject 10, Epoch 64, Loss: 3.900629997253418, Final Batch Loss: 1.0142425298690796\n",
      "Subject 10, Epoch 65, Loss: 3.72709196805954, Final Batch Loss: 0.8536794781684875\n",
      "Subject 10, Epoch 66, Loss: 3.6388573050498962, Final Batch Loss: 0.9577605724334717\n",
      "Subject 10, Epoch 67, Loss: 3.843228757381439, Final Batch Loss: 1.0159398317337036\n",
      "Subject 10, Epoch 68, Loss: 3.497736632823944, Final Batch Loss: 0.8599801659584045\n",
      "Subject 10, Epoch 69, Loss: 3.3884912729263306, Final Batch Loss: 0.8019881248474121\n",
      "Subject 10, Epoch 70, Loss: 3.5921967029571533, Final Batch Loss: 0.9539967179298401\n",
      "Subject 10, Epoch 71, Loss: 3.4092766642570496, Final Batch Loss: 0.8171911835670471\n",
      "Subject 10, Epoch 72, Loss: 3.595536708831787, Final Batch Loss: 1.0684889554977417\n",
      "Subject 10, Epoch 73, Loss: 3.2557913064956665, Final Batch Loss: 0.6865842938423157\n",
      "Subject 10, Epoch 74, Loss: 3.108629047870636, Final Batch Loss: 0.6672453880310059\n",
      "Subject 10, Epoch 75, Loss: 3.1999486684799194, Final Batch Loss: 0.7730576992034912\n",
      "Subject 10, Epoch 76, Loss: 3.2517714500427246, Final Batch Loss: 0.821962833404541\n",
      "Subject 10, Epoch 77, Loss: 3.1323089599609375, Final Batch Loss: 0.6909134984016418\n",
      "Subject 10, Epoch 78, Loss: 3.168946146965027, Final Batch Loss: 0.7646434307098389\n",
      "Subject 10, Epoch 79, Loss: 3.1495023369789124, Final Batch Loss: 0.7657473087310791\n",
      "Subject 10, Epoch 80, Loss: 3.153491735458374, Final Batch Loss: 0.6846136450767517\n",
      "Subject 10, Epoch 81, Loss: 2.9250444769859314, Final Batch Loss: 0.587308406829834\n",
      "Subject 10, Epoch 82, Loss: 3.206334948539734, Final Batch Loss: 0.8766023516654968\n",
      "Subject 10, Epoch 83, Loss: 2.7653712034225464, Final Batch Loss: 0.5334450602531433\n",
      "Subject 10, Epoch 84, Loss: 2.9346556663513184, Final Batch Loss: 0.6628418564796448\n",
      "Subject 10, Epoch 85, Loss: 2.7906627655029297, Final Batch Loss: 0.533443033695221\n",
      "Subject 10, Epoch 86, Loss: 2.8504300117492676, Final Batch Loss: 0.6511155962944031\n",
      "Subject 10, Epoch 87, Loss: 2.799745202064514, Final Batch Loss: 0.6695043444633484\n",
      "Subject 10, Epoch 88, Loss: 2.998851954936981, Final Batch Loss: 0.8741114139556885\n",
      "Subject 10, Epoch 89, Loss: 2.7244452238082886, Final Batch Loss: 0.5809494853019714\n",
      "Subject 10, Epoch 90, Loss: 2.8600468039512634, Final Batch Loss: 0.7569141387939453\n",
      "Subject 10, Epoch 91, Loss: 2.6296192407608032, Final Batch Loss: 0.5504855513572693\n",
      "Subject 10, Epoch 92, Loss: 2.927100121974945, Final Batch Loss: 0.8854029178619385\n",
      "Subject 10, Epoch 93, Loss: 2.9113783836364746, Final Batch Loss: 0.894565761089325\n",
      "Subject 10, Epoch 94, Loss: 2.718654215335846, Final Batch Loss: 0.7561843395233154\n",
      "Subject 10, Epoch 95, Loss: 2.841099441051483, Final Batch Loss: 0.7351945042610168\n",
      "Subject 10, Epoch 96, Loss: 2.6012587547302246, Final Batch Loss: 0.595185399055481\n",
      "Subject 10, Epoch 97, Loss: 2.8161298036575317, Final Batch Loss: 0.8191075921058655\n",
      "Subject 10, Epoch 98, Loss: 2.704993963241577, Final Batch Loss: 0.7364904880523682\n",
      "Subject 10, Epoch 99, Loss: 2.7026293873786926, Final Batch Loss: 0.7069573402404785\n",
      "Subject 10, Epoch 100, Loss: 2.7087764739990234, Final Batch Loss: 0.6918873190879822\n",
      "Subject 10, Epoch 101, Loss: 2.589297592639923, Final Batch Loss: 0.6063506603240967\n",
      "Subject 10, Epoch 102, Loss: 2.729030728340149, Final Batch Loss: 0.7334091663360596\n",
      "Subject 10, Epoch 103, Loss: 2.4927082657814026, Final Batch Loss: 0.5300739407539368\n",
      "Subject 10, Epoch 104, Loss: 2.623574197292328, Final Batch Loss: 0.6670842170715332\n",
      "Subject 10, Epoch 105, Loss: 2.433434635400772, Final Batch Loss: 0.48520269989967346\n",
      "Subject 10, Epoch 106, Loss: 2.3220041692256927, Final Batch Loss: 0.3648814260959625\n",
      "Subject 10, Epoch 107, Loss: 2.4799033999443054, Final Batch Loss: 0.5363665819168091\n",
      "Subject 10, Epoch 108, Loss: 2.494166135787964, Final Batch Loss: 0.5714326500892639\n",
      "Subject 10, Epoch 109, Loss: 2.2551880180835724, Final Batch Loss: 0.4262560307979584\n",
      "Subject 10, Epoch 110, Loss: 2.3249312043190002, Final Batch Loss: 0.5129702091217041\n",
      "Subject 10, Epoch 111, Loss: 2.4697704315185547, Final Batch Loss: 0.5911015868186951\n",
      "Subject 10, Epoch 112, Loss: 2.2479056417942047, Final Batch Loss: 0.4896232783794403\n",
      "Subject 10, Epoch 113, Loss: 2.3815155029296875, Final Batch Loss: 0.5596380829811096\n",
      "Subject 10, Epoch 114, Loss: 2.4458712339401245, Final Batch Loss: 0.5417672991752625\n",
      "Subject 10, Epoch 115, Loss: 2.6135658025741577, Final Batch Loss: 0.7097653746604919\n",
      "Subject 10, Epoch 116, Loss: 2.3461828231811523, Final Batch Loss: 0.5612619519233704\n",
      "Subject 10, Epoch 117, Loss: 2.6003734469413757, Final Batch Loss: 0.7197659611701965\n",
      "Subject 10, Epoch 118, Loss: 2.5052968859672546, Final Batch Loss: 0.6049417853355408\n",
      "Subject 10, Epoch 119, Loss: 2.6357914805412292, Final Batch Loss: 0.8234636187553406\n",
      "Subject 10, Epoch 120, Loss: 2.7180293798446655, Final Batch Loss: 0.862497866153717\n",
      "Subject 10, Epoch 121, Loss: 2.593284845352173, Final Batch Loss: 0.7180571556091309\n",
      "Subject 10, Epoch 122, Loss: 2.3326236605644226, Final Batch Loss: 0.5654101967811584\n",
      "Subject 10, Epoch 123, Loss: 2.418272018432617, Final Batch Loss: 0.5668463110923767\n",
      "Subject 10, Epoch 124, Loss: 2.3618780970573425, Final Batch Loss: 0.5138664245605469\n",
      "Subject 10, Epoch 125, Loss: 2.584981322288513, Final Batch Loss: 0.8389590382575989\n",
      "Subject 10, Epoch 126, Loss: 2.396921932697296, Final Batch Loss: 0.6793069839477539\n",
      "Subject 10, Epoch 127, Loss: 2.2481278777122498, Final Batch Loss: 0.5165353417396545\n",
      "Subject 10, Epoch 128, Loss: 2.6811277270317078, Final Batch Loss: 0.8925731182098389\n",
      "Subject 10, Epoch 129, Loss: 2.3492390513420105, Final Batch Loss: 0.5888438820838928\n",
      "Subject 10, Epoch 130, Loss: 2.316999614238739, Final Batch Loss: 0.5212224125862122\n",
      "Subject 10, Epoch 131, Loss: 2.340343713760376, Final Batch Loss: 0.7096615433692932\n",
      "Subject 10, Epoch 132, Loss: 2.2139292657375336, Final Batch Loss: 0.49171051383018494\n",
      "Subject 10, Epoch 133, Loss: 2.462933599948883, Final Batch Loss: 0.7300739884376526\n",
      "Subject 10, Epoch 134, Loss: 2.183546632528305, Final Batch Loss: 0.46036258339881897\n",
      "Subject 10, Epoch 135, Loss: 2.3961598873138428, Final Batch Loss: 0.6672858595848083\n",
      "Subject 10, Epoch 136, Loss: 2.420639932155609, Final Batch Loss: 0.5747261047363281\n",
      "Subject 10, Epoch 137, Loss: 2.1973314583301544, Final Batch Loss: 0.4552824795246124\n",
      "Subject 10, Epoch 138, Loss: 2.2076222896575928, Final Batch Loss: 0.5358191728591919\n",
      "Subject 10, Epoch 139, Loss: 2.1944815516471863, Final Batch Loss: 0.5958952903747559\n",
      "Subject 10, Epoch 140, Loss: 2.288955181837082, Final Batch Loss: 0.4671075642108917\n",
      "Subject 10, Epoch 141, Loss: 1.9769857823848724, Final Batch Loss: 0.31868013739585876\n",
      "Subject 10, Epoch 142, Loss: 2.599140465259552, Final Batch Loss: 0.940603494644165\n",
      "Subject 10, Epoch 143, Loss: 2.1420189440250397, Final Batch Loss: 0.41174033284187317\n",
      "Subject 10, Epoch 144, Loss: 2.2499276995658875, Final Batch Loss: 0.5671570897102356\n",
      "Subject 10, Epoch 145, Loss: 2.47203928232193, Final Batch Loss: 0.7784795165061951\n",
      "Subject 10, Epoch 146, Loss: 2.3883040249347687, Final Batch Loss: 0.7200812697410583\n",
      "Subject 10, Epoch 147, Loss: 2.0684307515621185, Final Batch Loss: 0.4392670691013336\n",
      "Subject 10, Epoch 148, Loss: 1.9965052902698517, Final Batch Loss: 0.37371039390563965\n",
      "Subject 10, Epoch 149, Loss: 2.234375834465027, Final Batch Loss: 0.43587183952331543\n",
      "Subject 10, Epoch 150, Loss: 2.353770434856415, Final Batch Loss: 0.6059882044792175\n",
      "Subject 10, Epoch 151, Loss: 2.31451553106308, Final Batch Loss: 0.5814399123191833\n",
      "Subject 10, Epoch 152, Loss: 1.980974018573761, Final Batch Loss: 0.2764502763748169\n",
      "Subject 10, Epoch 153, Loss: 2.1508138477802277, Final Batch Loss: 0.47890135645866394\n",
      "Subject 10, Epoch 154, Loss: 2.2379565238952637, Final Batch Loss: 0.5416585206985474\n",
      "Subject 10, Epoch 155, Loss: 2.354671597480774, Final Batch Loss: 0.6125500798225403\n",
      "Subject 10, Epoch 156, Loss: 2.172133296728134, Final Batch Loss: 0.5573194026947021\n",
      "Subject 10, Epoch 157, Loss: 1.9199002981185913, Final Batch Loss: 0.37222418189048767\n",
      "Subject 10, Epoch 158, Loss: 2.32319512963295, Final Batch Loss: 0.7453864216804504\n",
      "Subject 10, Epoch 159, Loss: 2.1909872591495514, Final Batch Loss: 0.6989295482635498\n",
      "Subject 10, Epoch 160, Loss: 2.197136253118515, Final Batch Loss: 0.46953871846199036\n",
      "Subject 10, Epoch 161, Loss: 2.0999523997306824, Final Batch Loss: 0.4360637962818146\n",
      "Subject 10, Epoch 162, Loss: 2.0950921773910522, Final Batch Loss: 0.45154237747192383\n",
      "Subject 10, Epoch 163, Loss: 2.1281533539295197, Final Batch Loss: 0.48750242590904236\n",
      "Subject 10, Epoch 164, Loss: 2.4026583433151245, Final Batch Loss: 0.8156132102012634\n",
      "Subject 10, Epoch 165, Loss: 2.392350971698761, Final Batch Loss: 0.8124889731407166\n",
      "Subject 10, Epoch 166, Loss: 2.294646680355072, Final Batch Loss: 0.6226344108581543\n",
      "Subject 10, Epoch 167, Loss: 2.153012365102768, Final Batch Loss: 0.5186442732810974\n",
      "Subject 10, Epoch 168, Loss: 2.119605839252472, Final Batch Loss: 0.5336862206459045\n",
      "Subject 10, Epoch 169, Loss: 2.132416069507599, Final Batch Loss: 0.52129727602005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 170, Loss: 1.9231332540512085, Final Batch Loss: 0.38332030177116394\n",
      "Subject 10, Epoch 171, Loss: 2.4649040699005127, Final Batch Loss: 0.9595219492912292\n",
      "Subject 10, Epoch 172, Loss: 2.2711671888828278, Final Batch Loss: 0.701255738735199\n",
      "Subject 10, Epoch 173, Loss: 1.9999661147594452, Final Batch Loss: 0.41182398796081543\n",
      "Subject 10, Epoch 174, Loss: 2.0470414757728577, Final Batch Loss: 0.5321522355079651\n",
      "Subject 10, Epoch 175, Loss: 1.9674854278564453, Final Batch Loss: 0.40428221225738525\n",
      "Subject 10, Epoch 176, Loss: 2.018377274274826, Final Batch Loss: 0.4883810579776764\n",
      "Subject 10, Epoch 177, Loss: 2.0100341141223907, Final Batch Loss: 0.49266764521598816\n",
      "Subject 10, Epoch 178, Loss: 1.9807976484298706, Final Batch Loss: 0.4548196792602539\n",
      "Subject 10, Epoch 179, Loss: 2.338336408138275, Final Batch Loss: 0.7348014712333679\n",
      "Subject 10, Epoch 180, Loss: 1.9924113154411316, Final Batch Loss: 0.48977625370025635\n",
      "Subject 10, Epoch 181, Loss: 1.964404821395874, Final Batch Loss: 0.33904770016670227\n",
      "Subject 10, Epoch 182, Loss: 1.9891337156295776, Final Batch Loss: 0.5028975605964661\n",
      "Subject 10, Epoch 183, Loss: 2.1918246746063232, Final Batch Loss: 0.6346117258071899\n",
      "Subject 10, Epoch 184, Loss: 2.2360655665397644, Final Batch Loss: 0.5790266394615173\n",
      "Subject 10, Epoch 185, Loss: 1.9842884242534637, Final Batch Loss: 0.4015715420246124\n",
      "Subject 10, Epoch 186, Loss: 1.8442731201648712, Final Batch Loss: 0.30226707458496094\n",
      "Subject 10, Epoch 187, Loss: 1.8956943154335022, Final Batch Loss: 0.32777902483940125\n",
      "Subject 10, Epoch 188, Loss: 1.9465236067771912, Final Batch Loss: 0.532227098941803\n",
      "Subject 10, Epoch 189, Loss: 1.8487454950809479, Final Batch Loss: 0.286824494600296\n",
      "Subject 10, Epoch 190, Loss: 1.7579770684242249, Final Batch Loss: 0.29641667008399963\n",
      "Subject 10, Epoch 191, Loss: 1.998833179473877, Final Batch Loss: 0.4681081771850586\n",
      "Subject 10, Epoch 192, Loss: 2.1469858586788177, Final Batch Loss: 0.47953304648399353\n",
      "Subject 10, Epoch 193, Loss: 2.0343776047229767, Final Batch Loss: 0.5351472496986389\n",
      "Subject 10, Epoch 194, Loss: 1.9249545633792877, Final Batch Loss: 0.3964887857437134\n",
      "Subject 10, Epoch 195, Loss: 1.910731092095375, Final Batch Loss: 0.23267097771167755\n",
      "Subject 10, Epoch 196, Loss: 1.8233636617660522, Final Batch Loss: 0.29246267676353455\n",
      "Subject 10, Epoch 197, Loss: 1.883283942937851, Final Batch Loss: 0.3995644152164459\n",
      "Subject 10, Epoch 198, Loss: 1.9763148427009583, Final Batch Loss: 0.3718053102493286\n",
      "Subject 10, Epoch 199, Loss: 1.7437985837459564, Final Batch Loss: 0.3159198462963104\n",
      "Subject 10, Epoch 200, Loss: 2.058517426252365, Final Batch Loss: 0.6521205902099609\n",
      "Subject 10, Epoch 201, Loss: 1.7794390320777893, Final Batch Loss: 0.3377240002155304\n",
      "Subject 10, Epoch 202, Loss: 2.1375449895858765, Final Batch Loss: 0.6262853145599365\n",
      "Subject 10, Epoch 203, Loss: 2.2816946506500244, Final Batch Loss: 0.8700693249702454\n",
      "Subject 10, Epoch 204, Loss: 1.6297376304864883, Final Batch Loss: 0.1908232718706131\n",
      "Subject 10, Epoch 205, Loss: 1.919528216123581, Final Batch Loss: 0.5002073049545288\n",
      "Subject 10, Epoch 206, Loss: 1.8108137845993042, Final Batch Loss: 0.4653349220752716\n",
      "Subject 10, Epoch 207, Loss: 1.7980915009975433, Final Batch Loss: 0.2932822108268738\n",
      "Subject 10, Epoch 208, Loss: 1.9915778636932373, Final Batch Loss: 0.5418354868888855\n",
      "Subject 10, Epoch 209, Loss: 1.8715343475341797, Final Batch Loss: 0.5753157734870911\n",
      "Subject 10, Epoch 210, Loss: 1.7360544800758362, Final Batch Loss: 0.36795520782470703\n",
      "Subject 10, Epoch 211, Loss: 1.7416810393333435, Final Batch Loss: 0.31041646003723145\n",
      "Subject 10, Epoch 212, Loss: 1.6209335625171661, Final Batch Loss: 0.3040499985218048\n",
      "Subject 10, Epoch 213, Loss: 2.0215345919132233, Final Batch Loss: 0.6704723238945007\n",
      "Subject 10, Epoch 214, Loss: 2.1262705624103546, Final Batch Loss: 0.7847905158996582\n",
      "Subject 10, Epoch 215, Loss: 1.7553029656410217, Final Batch Loss: 0.3037045896053314\n",
      "Subject 10, Epoch 216, Loss: 1.7898480892181396, Final Batch Loss: 0.41462960839271545\n",
      "Subject 10, Epoch 217, Loss: 1.672379493713379, Final Batch Loss: 0.3218994438648224\n",
      "Subject 10, Epoch 218, Loss: 1.9029866456985474, Final Batch Loss: 0.5242606997489929\n",
      "Subject 10, Epoch 219, Loss: 1.706216275691986, Final Batch Loss: 0.3579070568084717\n",
      "Subject 10, Epoch 220, Loss: 2.0342496037483215, Final Batch Loss: 0.48930343985557556\n",
      "Subject 10, Epoch 221, Loss: 1.7426290810108185, Final Batch Loss: 0.37270238995552063\n",
      "Subject 10, Epoch 222, Loss: 1.8193714916706085, Final Batch Loss: 0.4417611062526703\n",
      "Subject 10, Epoch 223, Loss: 1.8071739673614502, Final Batch Loss: 0.3867436349391937\n",
      "Subject 10, Epoch 224, Loss: 1.9275972247123718, Final Batch Loss: 0.558259129524231\n",
      "Subject 10, Epoch 225, Loss: 1.7851578891277313, Final Batch Loss: 0.41619858145713806\n",
      "Subject 10, Epoch 226, Loss: 1.85550057888031, Final Batch Loss: 0.4668671190738678\n",
      "Subject 10, Epoch 227, Loss: 1.9364045560359955, Final Batch Loss: 0.6823534965515137\n",
      "Subject 10, Epoch 228, Loss: 1.7948099374771118, Final Batch Loss: 0.36938488483428955\n",
      "Subject 10, Epoch 229, Loss: 2.210572451353073, Final Batch Loss: 0.9702993035316467\n",
      "Subject 10, Epoch 230, Loss: 1.9219976365566254, Final Batch Loss: 0.47559258341789246\n",
      "Subject 10, Epoch 231, Loss: 1.5505711734294891, Final Batch Loss: 0.26620206236839294\n",
      "Subject 10, Epoch 232, Loss: 1.5259643495082855, Final Batch Loss: 0.16180995106697083\n",
      "Subject 10, Epoch 233, Loss: 1.5515425503253937, Final Batch Loss: 0.31204310059547424\n",
      "Subject 10, Epoch 234, Loss: 1.8920739889144897, Final Batch Loss: 0.44106975197792053\n",
      "Subject 10, Epoch 235, Loss: 1.908400148153305, Final Batch Loss: 0.542349636554718\n",
      "Subject 10, Epoch 236, Loss: 2.006518006324768, Final Batch Loss: 0.644310712814331\n",
      "Subject 10, Epoch 237, Loss: 1.7723673582077026, Final Batch Loss: 0.5050450563430786\n",
      "Subject 10, Epoch 238, Loss: 1.5820837020874023, Final Batch Loss: 0.3039446771144867\n",
      "Subject 10, Epoch 239, Loss: 1.90302374958992, Final Batch Loss: 0.5186650156974792\n",
      "Subject 10, Epoch 240, Loss: 1.5354783236980438, Final Batch Loss: 0.2714186906814575\n",
      "Subject 10, Epoch 241, Loss: 1.9300951659679413, Final Batch Loss: 0.6852914690971375\n",
      "Subject 10, Epoch 242, Loss: 1.5563495755195618, Final Batch Loss: 0.34697413444519043\n",
      "Subject 10, Epoch 243, Loss: 1.6305307745933533, Final Batch Loss: 0.3690296411514282\n",
      "Subject 10, Epoch 244, Loss: 1.6097059547901154, Final Batch Loss: 0.31276413798332214\n",
      "Subject 10, Epoch 245, Loss: 1.5878776609897614, Final Batch Loss: 0.30862340331077576\n",
      "Subject 10, Epoch 246, Loss: 1.5103814005851746, Final Batch Loss: 0.25438034534454346\n",
      "Subject 10, Epoch 247, Loss: 1.6852357685565948, Final Batch Loss: 0.4176250398159027\n",
      "Subject 10, Epoch 248, Loss: 1.706212341785431, Final Batch Loss: 0.32609495520591736\n",
      "Subject 10, Epoch 249, Loss: 1.4964257776737213, Final Batch Loss: 0.2141740918159485\n",
      "Subject 10, Epoch 250, Loss: 1.654327005147934, Final Batch Loss: 0.3882557153701782\n",
      "Subject 10, Epoch 251, Loss: 1.7753362655639648, Final Batch Loss: 0.5428797602653503\n",
      "Subject 10, Epoch 252, Loss: 1.5075218826532364, Final Batch Loss: 0.2257491797208786\n",
      "Subject 10, Epoch 253, Loss: 1.7662388980388641, Final Batch Loss: 0.4122290313243866\n",
      "Subject 10, Epoch 254, Loss: 1.6584123969078064, Final Batch Loss: 0.38301515579223633\n",
      "Subject 10, Epoch 255, Loss: 1.7272096872329712, Final Batch Loss: 0.39949682354927063\n",
      "Subject 10, Epoch 256, Loss: 1.5354972332715988, Final Batch Loss: 0.22579549252986908\n",
      "Subject 10, Epoch 257, Loss: 1.7142063081264496, Final Batch Loss: 0.4943261444568634\n",
      "Subject 10, Epoch 258, Loss: 1.7349583208560944, Final Batch Loss: 0.3394300639629364\n",
      "Subject 10, Epoch 259, Loss: 1.7148090600967407, Final Batch Loss: 0.4086012542247772\n",
      "Subject 10, Epoch 260, Loss: 1.51694855093956, Final Batch Loss: 0.2629866898059845\n",
      "Subject 10, Epoch 261, Loss: 1.5350607335567474, Final Batch Loss: 0.2474413514137268\n",
      "Subject 10, Epoch 262, Loss: 1.8041493892669678, Final Batch Loss: 0.49926623702049255\n",
      "Subject 10, Epoch 263, Loss: 1.394458457827568, Final Batch Loss: 0.17967842519283295\n",
      "Subject 10, Epoch 264, Loss: 1.540571689605713, Final Batch Loss: 0.25842976570129395\n",
      "Subject 10, Epoch 265, Loss: 1.6125044226646423, Final Batch Loss: 0.37723615765571594\n",
      "Subject 10, Epoch 266, Loss: 1.7264043390750885, Final Batch Loss: 0.31495213508605957\n",
      "Subject 10, Epoch 267, Loss: 1.719073861837387, Final Batch Loss: 0.3723026514053345\n",
      "Subject 10, Epoch 268, Loss: 1.6992380321025848, Final Batch Loss: 0.3893274962902069\n",
      "Subject 10, Epoch 269, Loss: 1.81234210729599, Final Batch Loss: 0.47865453362464905\n",
      "Subject 10, Epoch 270, Loss: 2.029877632856369, Final Batch Loss: 0.7495803833007812\n",
      "Subject 10, Epoch 271, Loss: 1.6949657797813416, Final Batch Loss: 0.40567341446876526\n",
      "Subject 10, Epoch 272, Loss: 1.6312402784824371, Final Batch Loss: 0.4605681896209717\n",
      "Subject 10, Epoch 273, Loss: 1.8041113913059235, Final Batch Loss: 0.5545682311058044\n",
      "Subject 10, Epoch 274, Loss: 1.7264070510864258, Final Batch Loss: 0.3580557107925415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 275, Loss: 1.5841870605945587, Final Batch Loss: 0.42305147647857666\n",
      "Subject 10, Epoch 276, Loss: 1.7763102054595947, Final Batch Loss: 0.547671377658844\n",
      "Subject 10, Epoch 277, Loss: 1.6783884167671204, Final Batch Loss: 0.41467949748039246\n",
      "Subject 10, Epoch 278, Loss: 1.6281873285770416, Final Batch Loss: 0.3768305480480194\n",
      "Subject 10, Epoch 279, Loss: 1.5943377315998077, Final Batch Loss: 0.29828035831451416\n",
      "Subject 10, Epoch 280, Loss: 1.5776360630989075, Final Batch Loss: 0.3664863109588623\n",
      "Subject 10, Epoch 281, Loss: 1.6403610408306122, Final Batch Loss: 0.4315929114818573\n",
      "Subject 10, Epoch 282, Loss: 1.683952420949936, Final Batch Loss: 0.34524261951446533\n",
      "Subject 10, Epoch 283, Loss: 1.523084968328476, Final Batch Loss: 0.3470627963542938\n",
      "Subject 10, Epoch 284, Loss: 1.526201069355011, Final Batch Loss: 0.3604106903076172\n",
      "Subject 10, Epoch 285, Loss: 1.66676065325737, Final Batch Loss: 0.33528468012809753\n",
      "Subject 10, Epoch 286, Loss: 1.6040013432502747, Final Batch Loss: 0.35744431614875793\n",
      "Subject 10, Epoch 287, Loss: 1.357251763343811, Final Batch Loss: 0.2556709945201874\n",
      "Subject 10, Epoch 288, Loss: 1.666812926530838, Final Batch Loss: 0.35554036498069763\n",
      "Subject 10, Epoch 289, Loss: 1.579541027545929, Final Batch Loss: 0.30475282669067383\n",
      "Subject 10, Epoch 290, Loss: 1.5423242449760437, Final Batch Loss: 0.2836688458919525\n",
      "Subject 10, Epoch 291, Loss: 1.608495146036148, Final Batch Loss: 0.37067532539367676\n",
      "Subject 10, Epoch 292, Loss: 1.4824250638484955, Final Batch Loss: 0.25733211636543274\n",
      "Subject 10, Epoch 293, Loss: 1.6426705121994019, Final Batch Loss: 0.3765977919101715\n",
      "Subject 10, Epoch 294, Loss: 1.5864733159542084, Final Batch Loss: 0.34554019570350647\n",
      "Subject 10, Epoch 295, Loss: 1.483577162027359, Final Batch Loss: 0.3194901645183563\n",
      "Subject 10, Epoch 296, Loss: 1.8608248233795166, Final Batch Loss: 0.6924683451652527\n",
      "Subject 10, Epoch 297, Loss: 1.5108480155467987, Final Batch Loss: 0.3057231903076172\n",
      "Subject 10, Epoch 298, Loss: 1.7198935151100159, Final Batch Loss: 0.5042590498924255\n",
      "Subject 10, Epoch 299, Loss: 1.399264708161354, Final Batch Loss: 0.17237453162670135\n",
      "Subject 10, Epoch 300, Loss: 1.5143649131059647, Final Batch Loss: 0.24971799552440643\n",
      "Subject 10, Epoch 301, Loss: 1.5855443477630615, Final Batch Loss: 0.4089965522289276\n",
      "Subject 10, Epoch 302, Loss: 1.6821972727775574, Final Batch Loss: 0.3657531440258026\n",
      "Subject 10, Epoch 303, Loss: 1.372336819767952, Final Batch Loss: 0.2442108541727066\n",
      "Subject 10, Epoch 304, Loss: 1.4206889271736145, Final Batch Loss: 0.2886373698711395\n",
      "Subject 10, Epoch 305, Loss: 1.392829030752182, Final Batch Loss: 0.3221741020679474\n",
      "Subject 10, Epoch 306, Loss: 1.6399368345737457, Final Batch Loss: 0.4313656985759735\n",
      "Subject 10, Epoch 307, Loss: 1.4319453239440918, Final Batch Loss: 0.26470518112182617\n",
      "Subject 10, Epoch 308, Loss: 1.6876122057437897, Final Batch Loss: 0.4649578630924225\n",
      "Subject 10, Epoch 309, Loss: 1.6036807894706726, Final Batch Loss: 0.3566538393497467\n",
      "Subject 10, Epoch 310, Loss: 1.4231770932674408, Final Batch Loss: 0.2912601828575134\n",
      "Subject 10, Epoch 311, Loss: 2.000444144010544, Final Batch Loss: 0.9096223711967468\n",
      "Subject 10, Epoch 312, Loss: 1.5169892609119415, Final Batch Loss: 0.4772312641143799\n",
      "Subject 10, Epoch 313, Loss: 1.702536165714264, Final Batch Loss: 0.34059953689575195\n",
      "Subject 10, Epoch 314, Loss: 1.3828255534172058, Final Batch Loss: 0.1970527172088623\n",
      "Subject 10, Epoch 315, Loss: 1.4369325041770935, Final Batch Loss: 0.2955707013607025\n",
      "Subject 10, Epoch 316, Loss: 1.5570522844791412, Final Batch Loss: 0.3979330062866211\n",
      "Subject 10, Epoch 317, Loss: 1.4724694341421127, Final Batch Loss: 0.2433501034975052\n",
      "Subject 10, Epoch 318, Loss: 1.4086112082004547, Final Batch Loss: 0.2875989079475403\n",
      "Subject 10, Epoch 319, Loss: 1.469824880361557, Final Batch Loss: 0.308600515127182\n",
      "Subject 10, Epoch 320, Loss: 1.6715788543224335, Final Batch Loss: 0.5142865777015686\n",
      "Subject 10, Epoch 321, Loss: 1.6627197861671448, Final Batch Loss: 0.5891848206520081\n",
      "Subject 10, Epoch 322, Loss: 1.5799281001091003, Final Batch Loss: 0.5347181558609009\n",
      "Subject 10, Epoch 323, Loss: 1.6006827354431152, Final Batch Loss: 0.5261533856391907\n",
      "Subject 10, Epoch 324, Loss: 1.6894441545009613, Final Batch Loss: 0.5984726548194885\n",
      "Subject 10, Epoch 325, Loss: 1.4908865690231323, Final Batch Loss: 0.45103248953819275\n",
      "Subject 10, Epoch 326, Loss: 1.5871549844741821, Final Batch Loss: 0.40044498443603516\n",
      "Subject 10, Epoch 327, Loss: 1.9260231256484985, Final Batch Loss: 0.9096078276634216\n",
      "Subject 10, Epoch 328, Loss: 1.8692828714847565, Final Batch Loss: 0.7150333523750305\n",
      "Subject 10, Epoch 329, Loss: 1.683360606431961, Final Batch Loss: 0.6020905375480652\n",
      "Subject 10, Epoch 330, Loss: 1.3164499551057816, Final Batch Loss: 0.13027940690517426\n",
      "Subject 10, Epoch 331, Loss: 1.5271507799625397, Final Batch Loss: 0.4317077100276947\n",
      "Subject 10, Epoch 332, Loss: 1.6907176077365875, Final Batch Loss: 0.6078656911849976\n",
      "Subject 10, Epoch 333, Loss: 1.3053739070892334, Final Batch Loss: 0.1593380570411682\n",
      "Subject 10, Epoch 334, Loss: 1.46199631690979, Final Batch Loss: 0.3765307366847992\n",
      "Subject 10, Epoch 335, Loss: 1.4184610396623611, Final Batch Loss: 0.23232536017894745\n",
      "Subject 10, Epoch 336, Loss: 1.3169382959604263, Final Batch Loss: 0.15312017500400543\n",
      "Subject 10, Epoch 337, Loss: 1.4945121705532074, Final Batch Loss: 0.39715906977653503\n",
      "Subject 10, Epoch 338, Loss: 1.6431101262569427, Final Batch Loss: 0.5645526647567749\n",
      "Subject 10, Epoch 339, Loss: 1.5537128448486328, Final Batch Loss: 0.4883629083633423\n",
      "Subject 10, Epoch 340, Loss: 1.41965714097023, Final Batch Loss: 0.26957452297210693\n",
      "Subject 10, Epoch 341, Loss: 1.4324631094932556, Final Batch Loss: 0.36927852034568787\n",
      "Subject 10, Epoch 342, Loss: 1.4345089793205261, Final Batch Loss: 0.3825105130672455\n",
      "Subject 10, Epoch 343, Loss: 1.3865021169185638, Final Batch Loss: 0.22672444581985474\n",
      "Subject 10, Epoch 344, Loss: 1.4321093559265137, Final Batch Loss: 0.33891651034355164\n",
      "Subject 10, Epoch 345, Loss: 1.4281430840492249, Final Batch Loss: 0.44729897379875183\n",
      "Subject 10, Epoch 346, Loss: 1.3185129910707474, Final Batch Loss: 0.16228504478931427\n",
      "Subject 10, Epoch 347, Loss: 1.5291486382484436, Final Batch Loss: 0.3446718454360962\n",
      "Subject 10, Epoch 348, Loss: 1.4561956524848938, Final Batch Loss: 0.3730904161930084\n",
      "Subject 10, Epoch 349, Loss: 1.4338998794555664, Final Batch Loss: 0.3854977786540985\n",
      "Subject 10, Epoch 350, Loss: 1.2742706835269928, Final Batch Loss: 0.2811288833618164\n",
      "Subject 10, Epoch 351, Loss: 1.3183627501130104, Final Batch Loss: 0.09594405442476273\n",
      "Subject 10, Epoch 352, Loss: 1.5340191721916199, Final Batch Loss: 0.35953715443611145\n",
      "Subject 10, Epoch 353, Loss: 1.3563168346881866, Final Batch Loss: 0.2507116198539734\n",
      "Subject 10, Epoch 354, Loss: 1.4867607653141022, Final Batch Loss: 0.4024416208267212\n",
      "Subject 10, Epoch 355, Loss: 1.3689274489879608, Final Batch Loss: 0.2173033356666565\n",
      "Subject 10, Epoch 356, Loss: 1.4018913209438324, Final Batch Loss: 0.46424391865730286\n",
      "Subject 10, Epoch 357, Loss: 1.3602930009365082, Final Batch Loss: 0.36200419068336487\n",
      "Subject 10, Epoch 358, Loss: 1.225187361240387, Final Batch Loss: 0.23546069860458374\n",
      "Subject 10, Epoch 359, Loss: 1.6352517902851105, Final Batch Loss: 0.4729817807674408\n",
      "Subject 10, Epoch 360, Loss: 1.2599140107631683, Final Batch Loss: 0.25380513072013855\n",
      "Subject 10, Epoch 361, Loss: 1.4255324006080627, Final Batch Loss: 0.3771456778049469\n",
      "Subject 10, Epoch 362, Loss: 1.2237483114004135, Final Batch Loss: 0.13203854858875275\n",
      "Subject 10, Epoch 363, Loss: 1.6094515025615692, Final Batch Loss: 0.6322537064552307\n",
      "Subject 10, Epoch 364, Loss: 1.3354094922542572, Final Batch Loss: 0.36594974994659424\n",
      "Subject 10, Epoch 365, Loss: 1.2595138996839523, Final Batch Loss: 0.150808647274971\n",
      "Subject 10, Epoch 366, Loss: 1.3435395061969757, Final Batch Loss: 0.2821299135684967\n",
      "Subject 10, Epoch 367, Loss: 1.3093286752700806, Final Batch Loss: 0.2750166356563568\n",
      "Subject 10, Epoch 368, Loss: 1.3449590504169464, Final Batch Loss: 0.29888585209846497\n",
      "Subject 10, Epoch 369, Loss: 1.257897935807705, Final Batch Loss: 0.11112957447767258\n",
      "Subject 10, Epoch 370, Loss: 1.5589291751384735, Final Batch Loss: 0.413048654794693\n",
      "Subject 10, Epoch 371, Loss: 1.1672697067260742, Final Batch Loss: 0.2043517827987671\n",
      "Subject 10, Epoch 372, Loss: 1.2021519839763641, Final Batch Loss: 0.24259686470031738\n",
      "Subject 10, Epoch 373, Loss: 1.4421559274196625, Final Batch Loss: 0.48488613963127136\n",
      "Subject 10, Epoch 374, Loss: 1.2183333337306976, Final Batch Loss: 0.19015294313430786\n",
      "Subject 10, Epoch 375, Loss: 1.040912315249443, Final Batch Loss: 0.06379248201847076\n",
      "Subject 10, Epoch 376, Loss: 1.2735278755426407, Final Batch Loss: 0.21770836412906647\n",
      "Subject 10, Epoch 377, Loss: 1.4367896914482117, Final Batch Loss: 0.37232717871665955\n",
      "Subject 10, Epoch 378, Loss: 1.2557934820652008, Final Batch Loss: 0.26851019263267517\n",
      "Subject 10, Epoch 379, Loss: 1.486160010099411, Final Batch Loss: 0.5302984118461609\n",
      "Subject 10, Epoch 380, Loss: 1.659781664609909, Final Batch Loss: 0.5742886662483215\n",
      "Subject 10, Epoch 381, Loss: 1.0956886410713196, Final Batch Loss: 0.10845214128494263\n",
      "Subject 10, Epoch 382, Loss: 1.7002581655979156, Final Batch Loss: 0.6716079115867615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 383, Loss: 1.3825459480285645, Final Batch Loss: 0.282094269990921\n",
      "Subject 10, Epoch 384, Loss: 1.174918681383133, Final Batch Loss: 0.19126421213150024\n",
      "Subject 10, Epoch 385, Loss: 1.2936964333057404, Final Batch Loss: 0.3136393129825592\n",
      "Subject 10, Epoch 386, Loss: 1.3474031388759613, Final Batch Loss: 0.38006627559661865\n",
      "Subject 10, Epoch 387, Loss: 1.5300475358963013, Final Batch Loss: 0.5629191398620605\n",
      "Subject 10, Epoch 388, Loss: 1.3151944130659103, Final Batch Loss: 0.24940206110477448\n",
      "Subject 10, Epoch 389, Loss: 1.2595572173595428, Final Batch Loss: 0.26415273547172546\n",
      "Subject 10, Epoch 390, Loss: 1.3698937892913818, Final Batch Loss: 0.3900347650051117\n",
      "Subject 10, Epoch 391, Loss: 1.3293037414550781, Final Batch Loss: 0.3766416311264038\n",
      "Subject 10, Epoch 392, Loss: 1.4329721927642822, Final Batch Loss: 0.4360117018222809\n",
      "Subject 10, Epoch 393, Loss: 1.5008351802825928, Final Batch Loss: 0.3731386959552765\n",
      "Subject 10, Epoch 394, Loss: 1.4914386570453644, Final Batch Loss: 0.45880651473999023\n",
      "Subject 10, Epoch 395, Loss: 1.345999151468277, Final Batch Loss: 0.3555750846862793\n",
      "Subject 10, Epoch 396, Loss: 1.745641827583313, Final Batch Loss: 0.7017225623130798\n",
      "Subject 10, Epoch 397, Loss: 1.2171732485294342, Final Batch Loss: 0.1839684247970581\n",
      "Subject 10, Epoch 398, Loss: 1.1944803893566132, Final Batch Loss: 0.2731093764305115\n",
      "Subject 10, Epoch 399, Loss: 1.3820506036281586, Final Batch Loss: 0.4329265058040619\n",
      "Subject 10, Epoch 400, Loss: 1.3498068749904633, Final Batch Loss: 0.32413849234580994\n",
      "Subject 10, Epoch 401, Loss: 1.227301761507988, Final Batch Loss: 0.3296421468257904\n",
      "Subject 10, Epoch 402, Loss: 1.28164941072464, Final Batch Loss: 0.26768627762794495\n",
      "Subject 10, Epoch 403, Loss: 1.5069771111011505, Final Batch Loss: 0.508764386177063\n",
      "Subject 10, Epoch 404, Loss: 1.3835566639900208, Final Batch Loss: 0.31476929783821106\n",
      "Subject 10, Epoch 405, Loss: 1.428095668554306, Final Batch Loss: 0.4097250998020172\n",
      "Subject 10, Epoch 406, Loss: 1.2226178050041199, Final Batch Loss: 0.29137420654296875\n",
      "Subject 10, Epoch 407, Loss: 1.3129723817110062, Final Batch Loss: 0.2241237610578537\n",
      "Subject 10, Epoch 408, Loss: 1.6617459058761597, Final Batch Loss: 0.6751047968864441\n",
      "Subject 10, Epoch 409, Loss: 1.301539108157158, Final Batch Loss: 0.24693970382213593\n",
      "Subject 10, Epoch 410, Loss: 1.3834291696548462, Final Batch Loss: 0.3919518291950226\n",
      "Subject 10, Epoch 411, Loss: 1.26566644012928, Final Batch Loss: 0.20386536419391632\n",
      "Subject 10, Epoch 412, Loss: 1.6136692464351654, Final Batch Loss: 0.6059477925300598\n",
      "Subject 10, Epoch 413, Loss: 1.5042206645011902, Final Batch Loss: 0.34246936440467834\n",
      "Subject 10, Epoch 414, Loss: 1.2877369225025177, Final Batch Loss: 0.2676651179790497\n",
      "Subject 10, Epoch 415, Loss: 1.2825565040111542, Final Batch Loss: 0.2905823290348053\n",
      "Subject 10, Epoch 416, Loss: 1.3823152780532837, Final Batch Loss: 0.3829660713672638\n",
      "Subject 10, Epoch 417, Loss: 1.6326380968093872, Final Batch Loss: 0.6635898351669312\n",
      "Subject 10, Epoch 418, Loss: 1.2881579846143723, Final Batch Loss: 0.27809321880340576\n",
      "Subject 10, Epoch 419, Loss: 1.348429560661316, Final Batch Loss: 0.3387904167175293\n",
      "Subject 10, Epoch 420, Loss: 1.210714891552925, Final Batch Loss: 0.24997122585773468\n",
      "Subject 10, Epoch 421, Loss: 1.2884986698627472, Final Batch Loss: 0.3025064170360565\n",
      "Subject 10, Epoch 422, Loss: 1.2295660972595215, Final Batch Loss: 0.2903100550174713\n",
      "Subject 10, Epoch 423, Loss: 1.1906220763921738, Final Batch Loss: 0.21997256577014923\n",
      "Subject 10, Epoch 424, Loss: 1.113632470369339, Final Batch Loss: 0.1585148274898529\n",
      "Subject 10, Epoch 425, Loss: 1.366681694984436, Final Batch Loss: 0.36889004707336426\n",
      "Subject 10, Epoch 426, Loss: 1.3859281688928604, Final Batch Loss: 0.40937289595603943\n",
      "Subject 10, Epoch 427, Loss: 1.1835027039051056, Final Batch Loss: 0.20692354440689087\n",
      "Subject 10, Epoch 428, Loss: 1.1099447757005692, Final Batch Loss: 0.16177667677402496\n",
      "Subject 10, Epoch 429, Loss: 1.4160353243350983, Final Batch Loss: 0.37477993965148926\n",
      "Subject 10, Epoch 430, Loss: 1.1834235340356827, Final Batch Loss: 0.2400926798582077\n",
      "Subject 10, Epoch 431, Loss: 1.3700984120368958, Final Batch Loss: 0.346421480178833\n",
      "Subject 10, Epoch 432, Loss: 1.6392283141613007, Final Batch Loss: 0.6604486107826233\n",
      "Subject 10, Epoch 433, Loss: 1.303622767329216, Final Batch Loss: 0.39110997319221497\n",
      "Subject 10, Epoch 434, Loss: 1.2972233593463898, Final Batch Loss: 0.31832006573677063\n",
      "Subject 10, Epoch 435, Loss: 1.1995050609111786, Final Batch Loss: 0.1963261216878891\n",
      "Subject 10, Epoch 436, Loss: 1.3238321244716644, Final Batch Loss: 0.3474917709827423\n",
      "Subject 10, Epoch 437, Loss: 1.3055192232131958, Final Batch Loss: 0.40335166454315186\n",
      "Subject 10, Epoch 438, Loss: 1.4141592681407928, Final Batch Loss: 0.5761323571205139\n",
      "Subject 10, Epoch 439, Loss: 1.3375525772571564, Final Batch Loss: 0.42310068011283875\n",
      "Subject 10, Epoch 440, Loss: 1.335272490978241, Final Batch Loss: 0.36719825863838196\n",
      "Subject 10, Epoch 441, Loss: 1.1985569596290588, Final Batch Loss: 0.27228328585624695\n",
      "Subject 10, Epoch 442, Loss: 1.073437675833702, Final Batch Loss: 0.14016501605510712\n",
      "Subject 10, Epoch 443, Loss: 1.2249653786420822, Final Batch Loss: 0.1736062616109848\n",
      "Subject 10, Epoch 444, Loss: 1.3081274032592773, Final Batch Loss: 0.4126974046230316\n",
      "Subject 10, Epoch 445, Loss: 1.2311094850301743, Final Batch Loss: 0.3274690508842468\n",
      "Subject 10, Epoch 446, Loss: 1.0420462489128113, Final Batch Loss: 0.19193744659423828\n",
      "Subject 10, Epoch 447, Loss: 1.3417968600988388, Final Batch Loss: 0.4761369228363037\n",
      "Subject 10, Epoch 448, Loss: 1.4164592325687408, Final Batch Loss: 0.44565293192863464\n",
      "Subject 10, Epoch 449, Loss: 1.1873788237571716, Final Batch Loss: 0.20065468549728394\n",
      "Subject 10, Epoch 450, Loss: 1.3598790764808655, Final Batch Loss: 0.2831377685070038\n",
      "Subject 10, Epoch 451, Loss: 1.5057044923305511, Final Batch Loss: 0.5405346751213074\n",
      "Subject 10, Epoch 452, Loss: 1.0197522900998592, Final Batch Loss: 0.0414753220975399\n",
      "Subject 10, Epoch 453, Loss: 1.470500499010086, Final Batch Loss: 0.3378683626651764\n",
      "Subject 10, Epoch 454, Loss: 1.0870572626590729, Final Batch Loss: 0.12280741333961487\n",
      "Subject 10, Epoch 455, Loss: 1.2329688370227814, Final Batch Loss: 0.340352863073349\n",
      "Subject 10, Epoch 456, Loss: 1.268833488225937, Final Batch Loss: 0.3325611650943756\n",
      "Subject 10, Epoch 457, Loss: 1.1650462448596954, Final Batch Loss: 0.1944485306739807\n",
      "Subject 10, Epoch 458, Loss: 0.9766054078936577, Final Batch Loss: 0.09358090907335281\n",
      "Subject 10, Epoch 459, Loss: 1.162647306919098, Final Batch Loss: 0.14531472325325012\n",
      "Subject 10, Epoch 460, Loss: 1.7178385108709335, Final Batch Loss: 0.7529489398002625\n",
      "Subject 10, Epoch 461, Loss: 1.1444916874170303, Final Batch Loss: 0.1761583685874939\n",
      "Subject 10, Epoch 462, Loss: 1.1950806379318237, Final Batch Loss: 0.35668060183525085\n",
      "Subject 10, Epoch 463, Loss: 1.1455489248037338, Final Batch Loss: 0.18397708237171173\n",
      "Subject 10, Epoch 464, Loss: 1.169886752963066, Final Batch Loss: 0.2409159392118454\n",
      "Subject 10, Epoch 465, Loss: 1.246612936258316, Final Batch Loss: 0.3508020341396332\n",
      "Subject 10, Epoch 466, Loss: 1.1609001457691193, Final Batch Loss: 0.23661310970783234\n",
      "Subject 10, Epoch 467, Loss: 0.9861603155732155, Final Batch Loss: 0.0676121786236763\n",
      "Subject 10, Epoch 468, Loss: 1.1554512679576874, Final Batch Loss: 0.22851639986038208\n",
      "Subject 10, Epoch 469, Loss: 1.094320923089981, Final Batch Loss: 0.20083875954151154\n",
      "Subject 10, Epoch 470, Loss: 1.1820489317178726, Final Batch Loss: 0.22210346162319183\n",
      "Subject 10, Epoch 471, Loss: 0.9941088929772377, Final Batch Loss: 0.09504855424165726\n",
      "Subject 10, Epoch 472, Loss: 1.1142616420984268, Final Batch Loss: 0.19995518028736115\n",
      "Subject 10, Epoch 473, Loss: 1.4073711037635803, Final Batch Loss: 0.511600136756897\n",
      "Subject 10, Epoch 474, Loss: 1.176200956106186, Final Batch Loss: 0.3062910735607147\n",
      "Subject 10, Epoch 475, Loss: 1.2791642099618912, Final Batch Loss: 0.4704413115978241\n",
      "Subject 10, Epoch 476, Loss: 1.8109556883573532, Final Batch Loss: 0.8570826053619385\n",
      "Subject 10, Epoch 477, Loss: 1.1380757093429565, Final Batch Loss: 0.20960241556167603\n",
      "Subject 10, Epoch 478, Loss: 1.2805141508579254, Final Batch Loss: 0.426422119140625\n",
      "Subject 10, Epoch 479, Loss: 0.9650705214589834, Final Batch Loss: 0.02921757660806179\n",
      "Subject 10, Epoch 480, Loss: 1.2929766923189163, Final Batch Loss: 0.21947450935840607\n",
      "Subject 10, Epoch 481, Loss: 1.1938092559576035, Final Batch Loss: 0.2034677118062973\n",
      "Subject 10, Epoch 482, Loss: 1.296359121799469, Final Batch Loss: 0.4384433329105377\n",
      "Subject 10, Epoch 483, Loss: 1.083590105175972, Final Batch Loss: 0.1558353155851364\n",
      "Subject 10, Epoch 484, Loss: 1.187445506453514, Final Batch Loss: 0.3083920180797577\n",
      "Subject 10, Epoch 485, Loss: 1.2337478250265121, Final Batch Loss: 0.21473459899425507\n",
      "Subject 10, Epoch 486, Loss: 1.207987904548645, Final Batch Loss: 0.2461566925048828\n",
      "Subject 10, Epoch 487, Loss: 1.2841582894325256, Final Batch Loss: 0.33762335777282715\n",
      "Subject 10, Epoch 488, Loss: 1.4073574542999268, Final Batch Loss: 0.45416465401649475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 489, Loss: 1.1498701870441437, Final Batch Loss: 0.251993864774704\n",
      "Subject 10, Epoch 490, Loss: 1.2744762301445007, Final Batch Loss: 0.4104512929916382\n",
      "Subject 10, Epoch 491, Loss: 1.1722330451011658, Final Batch Loss: 0.2643478214740753\n",
      "Subject 10, Epoch 492, Loss: 1.1793102622032166, Final Batch Loss: 0.21127033233642578\n",
      "Subject 10, Epoch 493, Loss: 1.0820722430944443, Final Batch Loss: 0.12561242282390594\n",
      "Subject 10, Epoch 494, Loss: 1.3861569464206696, Final Batch Loss: 0.45135298371315\n",
      "Subject 10, Epoch 495, Loss: 1.1464919224381447, Final Batch Loss: 0.10932139307260513\n",
      "Subject 10, Epoch 496, Loss: 1.1676742434501648, Final Batch Loss: 0.32964256405830383\n",
      "Subject 10, Epoch 497, Loss: 1.506883293390274, Final Batch Loss: 0.5869154930114746\n",
      "Subject 10, Epoch 498, Loss: 1.139034166932106, Final Batch Loss: 0.23654313385486603\n",
      "Subject 10, Epoch 499, Loss: 1.1824032068252563, Final Batch Loss: 0.2685588300228119\n",
      "Subject 10, Epoch 500, Loss: 1.1829905062913895, Final Batch Loss: 0.2561384439468384\n",
      "Subject 10, Epoch 501, Loss: 1.272401511669159, Final Batch Loss: 0.36357739567756653\n",
      "Subject 10, Epoch 502, Loss: 1.020322822034359, Final Batch Loss: 0.11261700838804245\n",
      "Subject 10, Epoch 503, Loss: 1.2939327955245972, Final Batch Loss: 0.32577207684516907\n",
      "Subject 10, Epoch 504, Loss: 1.2796835005283356, Final Batch Loss: 0.3293551206588745\n",
      "Subject 10, Epoch 505, Loss: 1.2410988211631775, Final Batch Loss: 0.35373878479003906\n",
      "Subject 10, Epoch 506, Loss: 0.9689532890915871, Final Batch Loss: 0.06854934245347977\n",
      "Subject 10, Epoch 507, Loss: 1.5661530494689941, Final Batch Loss: 0.5699331164360046\n",
      "Subject 10, Epoch 508, Loss: 1.3503187894821167, Final Batch Loss: 0.4115583002567291\n",
      "Subject 10, Epoch 509, Loss: 1.331568866968155, Final Batch Loss: 0.4575459957122803\n",
      "Subject 10, Epoch 510, Loss: 1.3237631618976593, Final Batch Loss: 0.45393869280815125\n",
      "Subject 10, Epoch 511, Loss: 1.1156802475452423, Final Batch Loss: 0.19599223136901855\n",
      "Subject 10, Epoch 512, Loss: 1.1753214746713638, Final Batch Loss: 0.2173827737569809\n",
      "Subject 10, Epoch 513, Loss: 1.3083069026470184, Final Batch Loss: 0.4589112102985382\n",
      "Subject 10, Epoch 514, Loss: 1.0886027216911316, Final Batch Loss: 0.20360493659973145\n",
      "Subject 10, Epoch 515, Loss: 1.2070456594228745, Final Batch Loss: 0.1777670830488205\n",
      "Subject 10, Epoch 516, Loss: 1.2383335530757904, Final Batch Loss: 0.30267617106437683\n",
      "Subject 10, Epoch 517, Loss: 1.3098631501197815, Final Batch Loss: 0.3894329369068146\n",
      "Subject 10, Epoch 518, Loss: 1.1980954110622406, Final Batch Loss: 0.26558157801628113\n",
      "Subject 10, Epoch 519, Loss: 1.1803439855575562, Final Batch Loss: 0.2888200581073761\n",
      "Subject 10, Epoch 520, Loss: 1.3277560025453568, Final Batch Loss: 0.3442772924900055\n",
      "Subject 10, Epoch 521, Loss: 1.3723658323287964, Final Batch Loss: 0.3981681764125824\n",
      "Subject 10, Epoch 522, Loss: 1.1238690465688705, Final Batch Loss: 0.24853599071502686\n",
      "Subject 10, Epoch 523, Loss: 1.1311723589897156, Final Batch Loss: 0.3171226382255554\n",
      "Subject 10, Epoch 524, Loss: 1.2106484919786453, Final Batch Loss: 0.16005901992321014\n",
      "Subject 10, Epoch 525, Loss: 1.1405383720993996, Final Batch Loss: 0.12028831988573074\n",
      "Subject 10, Epoch 526, Loss: 1.1396186798810959, Final Batch Loss: 0.17627881467342377\n",
      "Subject 10, Epoch 527, Loss: 0.991243951022625, Final Batch Loss: 0.10480108112096786\n",
      "Subject 10, Epoch 528, Loss: 1.002086341381073, Final Batch Loss: 0.07854428887367249\n",
      "Subject 10, Epoch 529, Loss: 1.144868642091751, Final Batch Loss: 0.39628005027770996\n",
      "Subject 10, Epoch 530, Loss: 1.0924330353736877, Final Batch Loss: 0.19286507368087769\n",
      "Subject 10, Epoch 531, Loss: 0.9283511489629745, Final Batch Loss: 0.10469987988471985\n",
      "Subject 10, Epoch 532, Loss: 1.136079102754593, Final Batch Loss: 0.31262847781181335\n",
      "Subject 10, Epoch 533, Loss: 1.2438619136810303, Final Batch Loss: 0.29077139496803284\n",
      "Subject 10, Epoch 534, Loss: 1.1723043322563171, Final Batch Loss: 0.28631171584129333\n",
      "Subject 10, Epoch 535, Loss: 1.3222718238830566, Final Batch Loss: 0.4898470342159271\n",
      "Subject 10, Epoch 536, Loss: 1.1514693349599838, Final Batch Loss: 0.39077258110046387\n",
      "Subject 10, Epoch 537, Loss: 1.1207270175218582, Final Batch Loss: 0.3663913905620575\n",
      "Subject 10, Epoch 538, Loss: 1.1404927521944046, Final Batch Loss: 0.24785839021205902\n",
      "Subject 10, Epoch 539, Loss: 1.0096705853939056, Final Batch Loss: 0.17746295034885406\n",
      "Subject 10, Epoch 540, Loss: 1.0923950225114822, Final Batch Loss: 0.2702951431274414\n",
      "Subject 10, Epoch 541, Loss: 1.3150788247585297, Final Batch Loss: 0.3925112187862396\n",
      "Subject 10, Epoch 542, Loss: 1.2096701264381409, Final Batch Loss: 0.3265836238861084\n",
      "Subject 10, Epoch 543, Loss: 1.0509733259677887, Final Batch Loss: 0.23977452516555786\n",
      "Subject 10, Epoch 544, Loss: 1.295056089758873, Final Batch Loss: 0.4886993169784546\n",
      "Subject 10, Epoch 545, Loss: 1.5153099596500397, Final Batch Loss: 0.6722526550292969\n",
      "Subject 10, Epoch 546, Loss: 1.0845683068037033, Final Batch Loss: 0.1653677374124527\n",
      "Subject 10, Epoch 547, Loss: 0.8391725760884583, Final Batch Loss: 0.00765753211453557\n",
      "Subject 10, Epoch 548, Loss: 1.2583630084991455, Final Batch Loss: 0.3424142897129059\n",
      "Subject 10, Epoch 549, Loss: 1.2136241048574448, Final Batch Loss: 0.2837085723876953\n",
      "Subject 10, Epoch 550, Loss: 1.142015054821968, Final Batch Loss: 0.24620316922664642\n",
      "Subject 10, Epoch 551, Loss: 0.8881816901266575, Final Batch Loss: 0.060793135315179825\n",
      "Subject 10, Epoch 552, Loss: 1.356293112039566, Final Batch Loss: 0.3119864761829376\n",
      "Subject 10, Epoch 553, Loss: 1.4067370891571045, Final Batch Loss: 0.4993385970592499\n",
      "Subject 10, Epoch 554, Loss: 0.9742656946182251, Final Batch Loss: 0.05853301286697388\n",
      "Subject 10, Epoch 555, Loss: 0.9876461178064346, Final Batch Loss: 0.12890200316905975\n",
      "Subject 10, Epoch 556, Loss: 1.1372582018375397, Final Batch Loss: 0.23868876695632935\n",
      "Subject 10, Epoch 557, Loss: 1.0900114476680756, Final Batch Loss: 0.20479118824005127\n",
      "Subject 10, Epoch 558, Loss: 1.275254100561142, Final Batch Loss: 0.4382360875606537\n",
      "Subject 10, Epoch 559, Loss: 0.9018497094511986, Final Batch Loss: 0.10363071411848068\n",
      "Subject 10, Epoch 560, Loss: 0.9940536171197891, Final Batch Loss: 0.14197330176830292\n",
      "Subject 10, Epoch 561, Loss: 1.240489810705185, Final Batch Loss: 0.40511420369148254\n",
      "Subject 10, Epoch 562, Loss: 1.0741368681192398, Final Batch Loss: 0.23655565083026886\n",
      "Subject 10, Epoch 563, Loss: 1.0323018431663513, Final Batch Loss: 0.22679860889911652\n",
      "Subject 10, Epoch 564, Loss: 1.1565918773412704, Final Batch Loss: 0.3066559135913849\n",
      "Subject 10, Epoch 565, Loss: 1.0148395150899887, Final Batch Loss: 0.13223259150981903\n",
      "Subject 10, Epoch 566, Loss: 1.0396276861429214, Final Batch Loss: 0.23988842964172363\n",
      "Subject 10, Epoch 567, Loss: 1.2195205986499786, Final Batch Loss: 0.340996116399765\n",
      "Subject 10, Epoch 568, Loss: 0.9244706332683563, Final Batch Loss: 0.15083767473697662\n",
      "Subject 10, Epoch 569, Loss: 1.130927935242653, Final Batch Loss: 0.3241310119628906\n",
      "Subject 10, Epoch 570, Loss: 1.2095774710178375, Final Batch Loss: 0.4044060707092285\n",
      "Subject 10, Epoch 571, Loss: 1.2076108753681183, Final Batch Loss: 0.3895290791988373\n",
      "Subject 10, Epoch 572, Loss: 1.0369821339845657, Final Batch Loss: 0.2330239862203598\n",
      "Subject 10, Epoch 573, Loss: 1.14686980843544, Final Batch Loss: 0.2745882272720337\n",
      "Subject 10, Epoch 574, Loss: 1.4222370386123657, Final Batch Loss: 0.4751298725605011\n",
      "Subject 10, Epoch 575, Loss: 1.023861899971962, Final Batch Loss: 0.20654036104679108\n",
      "Subject 10, Epoch 576, Loss: 1.2928701341152191, Final Batch Loss: 0.3724054992198944\n",
      "Subject 10, Epoch 577, Loss: 1.355791836977005, Final Batch Loss: 0.5446125864982605\n",
      "Subject 10, Epoch 578, Loss: 1.0344443917274475, Final Batch Loss: 0.16486050188541412\n",
      "Subject 10, Epoch 579, Loss: 1.5267194211483002, Final Batch Loss: 0.7133482098579407\n",
      "Subject 10, Epoch 580, Loss: 0.8887871727347374, Final Batch Loss: 0.08602943271398544\n",
      "Subject 10, Epoch 581, Loss: 1.28790944814682, Final Batch Loss: 0.39093711972236633\n",
      "Subject 10, Epoch 582, Loss: 0.9730094969272614, Final Batch Loss: 0.21084678173065186\n",
      "Subject 10, Epoch 583, Loss: 1.093044251203537, Final Batch Loss: 0.29093918204307556\n",
      "Subject 10, Epoch 584, Loss: 1.4501129984855652, Final Batch Loss: 0.6502427458763123\n",
      "Subject 10, Epoch 585, Loss: 0.981901153922081, Final Batch Loss: 0.1448754221200943\n",
      "Subject 10, Epoch 586, Loss: 1.0587204992771149, Final Batch Loss: 0.24049657583236694\n",
      "Subject 10, Epoch 587, Loss: 1.0287033170461655, Final Batch Loss: 0.1669296771287918\n",
      "Subject 10, Epoch 588, Loss: 1.3173318654298782, Final Batch Loss: 0.49401184916496277\n",
      "Subject 10, Epoch 589, Loss: 0.9631964266300201, Final Batch Loss: 0.20845896005630493\n",
      "Subject 10, Epoch 590, Loss: 1.009196326136589, Final Batch Loss: 0.16473247110843658\n",
      "Subject 10, Epoch 591, Loss: 1.1118119657039642, Final Batch Loss: 0.3384905755519867\n",
      "Subject 10, Epoch 592, Loss: 1.0547968596220016, Final Batch Loss: 0.282576322555542\n",
      "Subject 10, Epoch 593, Loss: 0.8814365267753601, Final Batch Loss: 0.13193096220493317\n",
      "Subject 10, Epoch 594, Loss: 1.1500772535800934, Final Batch Loss: 0.15332669019699097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 595, Loss: 0.9797855615615845, Final Batch Loss: 0.2084445357322693\n",
      "Subject 10, Epoch 596, Loss: 1.04386505484581, Final Batch Loss: 0.26837512850761414\n",
      "Subject 10, Epoch 597, Loss: 1.0456341952085495, Final Batch Loss: 0.2615082859992981\n",
      "Subject 10, Epoch 598, Loss: 0.9738542288541794, Final Batch Loss: 0.1468849927186966\n",
      "Subject 10, Epoch 599, Loss: 1.0715766102075577, Final Batch Loss: 0.20317168533802032\n",
      "Subject 10, Epoch 600, Loss: 0.9434940218925476, Final Batch Loss: 0.12245708703994751\n",
      "Subject 10, Epoch 601, Loss: 0.8655705526471138, Final Batch Loss: 0.07701664417982101\n",
      "Subject 10, Epoch 602, Loss: 1.3536827564239502, Final Batch Loss: 0.5861849188804626\n",
      "Subject 10, Epoch 603, Loss: 0.9499179199337959, Final Batch Loss: 0.09117189794778824\n",
      "Subject 10, Epoch 604, Loss: 1.070883348584175, Final Batch Loss: 0.20905567705631256\n",
      "Subject 10, Epoch 605, Loss: 1.203204795718193, Final Batch Loss: 0.3985002934932709\n",
      "Subject 10, Epoch 606, Loss: 1.0741565078496933, Final Batch Loss: 0.31816545128822327\n",
      "Subject 10, Epoch 607, Loss: 1.3631041944026947, Final Batch Loss: 0.4659152925014496\n",
      "Subject 10, Epoch 608, Loss: 1.0495903864502907, Final Batch Loss: 0.1176140084862709\n",
      "Subject 10, Epoch 609, Loss: 1.3060247600078583, Final Batch Loss: 0.47708117961883545\n",
      "Subject 10, Epoch 610, Loss: 1.2082154154777527, Final Batch Loss: 0.3607666492462158\n",
      "Subject 10, Epoch 611, Loss: 1.22505983710289, Final Batch Loss: 0.33781519532203674\n",
      "Subject 10, Epoch 612, Loss: 1.157249316573143, Final Batch Loss: 0.36513471603393555\n",
      "Subject 10, Epoch 613, Loss: 1.3088502883911133, Final Batch Loss: 0.42864230275154114\n",
      "Subject 10, Epoch 614, Loss: 1.3546014428138733, Final Batch Loss: 0.5597359538078308\n",
      "Subject 10, Epoch 615, Loss: 1.1127033531665802, Final Batch Loss: 0.3088590204715729\n",
      "Subject 10, Epoch 616, Loss: 0.8587636426091194, Final Batch Loss: 0.11242296546697617\n",
      "Subject 10, Epoch 617, Loss: 1.0551446825265884, Final Batch Loss: 0.2025788277387619\n",
      "Subject 10, Epoch 618, Loss: 1.0440020710229874, Final Batch Loss: 0.2706511914730072\n",
      "Subject 10, Epoch 619, Loss: 1.019526943564415, Final Batch Loss: 0.20096950232982635\n",
      "Subject 10, Epoch 620, Loss: 1.274160385131836, Final Batch Loss: 0.40698370337486267\n",
      "Subject 10, Epoch 621, Loss: 1.3997828960418701, Final Batch Loss: 0.5161343216896057\n",
      "Subject 10, Epoch 622, Loss: 1.0790408849716187, Final Batch Loss: 0.15992596745491028\n",
      "Subject 10, Epoch 623, Loss: 0.9919824451208115, Final Batch Loss: 0.16316242516040802\n",
      "Subject 10, Epoch 624, Loss: 0.928385391831398, Final Batch Loss: 0.16833649575710297\n",
      "Subject 10, Epoch 625, Loss: 0.9704292342066765, Final Batch Loss: 0.10938360542058945\n",
      "Subject 10, Epoch 626, Loss: 1.0516453832387924, Final Batch Loss: 0.3249718248844147\n",
      "Subject 10, Epoch 627, Loss: 1.1484223008155823, Final Batch Loss: 0.43252333998680115\n",
      "Subject 10, Epoch 628, Loss: 1.006483793258667, Final Batch Loss: 0.19595056772232056\n",
      "Subject 10, Epoch 629, Loss: 0.9895375519990921, Final Batch Loss: 0.19882595539093018\n",
      "Subject 10, Epoch 630, Loss: 1.2163738161325455, Final Batch Loss: 0.33738982677459717\n",
      "Subject 10, Epoch 631, Loss: 0.9867028445005417, Final Batch Loss: 0.16456754505634308\n",
      "Subject 10, Epoch 632, Loss: 1.1301285475492477, Final Batch Loss: 0.35491299629211426\n",
      "Subject 10, Epoch 633, Loss: 1.0548092126846313, Final Batch Loss: 0.28336426615715027\n",
      "Subject 10, Epoch 634, Loss: 1.0822855681180954, Final Batch Loss: 0.25958290696144104\n",
      "Subject 10, Epoch 635, Loss: 1.0196487307548523, Final Batch Loss: 0.16234099864959717\n",
      "Subject 10, Epoch 636, Loss: 1.043372318148613, Final Batch Loss: 0.20791961252689362\n",
      "Subject 10, Epoch 637, Loss: 0.9826615452766418, Final Batch Loss: 0.20001424849033356\n",
      "Subject 10, Epoch 638, Loss: 1.1401593089103699, Final Batch Loss: 0.36386409401893616\n",
      "Subject 10, Epoch 639, Loss: 1.0234894752502441, Final Batch Loss: 0.149658203125\n",
      "Subject 10, Epoch 640, Loss: 1.201501652598381, Final Batch Loss: 0.41917145252227783\n",
      "Subject 10, Epoch 641, Loss: 1.0823645144701004, Final Batch Loss: 0.3489147424697876\n",
      "Subject 10, Epoch 642, Loss: 1.0027030855417252, Final Batch Loss: 0.13093875348567963\n",
      "Subject 10, Epoch 643, Loss: 0.9923789799213409, Final Batch Loss: 0.22798384726047516\n",
      "Subject 10, Epoch 644, Loss: 1.1859413832426071, Final Batch Loss: 0.395353227853775\n",
      "Subject 10, Epoch 645, Loss: 1.0663646757602692, Final Batch Loss: 0.28623130917549133\n",
      "Subject 10, Epoch 646, Loss: 1.047486424446106, Final Batch Loss: 0.22200936079025269\n",
      "Subject 10, Epoch 647, Loss: 1.2002763003110886, Final Batch Loss: 0.3858081102371216\n",
      "Subject 10, Epoch 648, Loss: 1.2374507784843445, Final Batch Loss: 0.3388150930404663\n",
      "Subject 10, Epoch 649, Loss: 1.0839429944753647, Final Batch Loss: 0.2337346076965332\n",
      "Subject 10, Epoch 650, Loss: 1.1444725841283798, Final Batch Loss: 0.31708744168281555\n",
      "Subject 10, Epoch 651, Loss: 0.8014824017882347, Final Batch Loss: 0.06904742866754532\n",
      "Subject 10, Epoch 652, Loss: 0.948710098862648, Final Batch Loss: 0.2554709315299988\n",
      "Subject 10, Epoch 653, Loss: 1.30476675927639, Final Batch Loss: 0.4956965446472168\n",
      "Subject 10, Epoch 654, Loss: 1.1229307800531387, Final Batch Loss: 0.39105114340782166\n",
      "Subject 10, Epoch 655, Loss: 0.9070640802383423, Final Batch Loss: 0.2189733386039734\n",
      "Subject 10, Epoch 656, Loss: 1.123851716518402, Final Batch Loss: 0.32776662707328796\n",
      "Subject 10, Epoch 657, Loss: 1.0362931340932846, Final Batch Loss: 0.19712932407855988\n",
      "Subject 10, Epoch 658, Loss: 0.9401506632566452, Final Batch Loss: 0.18363367021083832\n",
      "Subject 10, Epoch 659, Loss: 0.8804237768054008, Final Batch Loss: 0.11777069419622421\n",
      "Subject 10, Epoch 660, Loss: 0.966112956404686, Final Batch Loss: 0.12655772268772125\n",
      "Subject 10, Epoch 661, Loss: 0.9671078473329544, Final Batch Loss: 0.12348374724388123\n",
      "Subject 10, Epoch 662, Loss: 1.0166411697864532, Final Batch Loss: 0.25131988525390625\n",
      "Subject 10, Epoch 663, Loss: 1.178440436720848, Final Batch Loss: 0.1784379482269287\n",
      "Subject 10, Epoch 664, Loss: 1.2552521526813507, Final Batch Loss: 0.3877454996109009\n",
      "Subject 10, Epoch 665, Loss: 0.9892611056566238, Final Batch Loss: 0.20286647975444794\n",
      "Subject 10, Epoch 666, Loss: 1.0020997524261475, Final Batch Loss: 0.12159126996994019\n",
      "Subject 10, Epoch 667, Loss: 1.0436496436595917, Final Batch Loss: 0.2694799304008484\n",
      "Subject 10, Epoch 668, Loss: 1.251505196094513, Final Batch Loss: 0.4496457576751709\n",
      "Subject 10, Epoch 669, Loss: 1.0511374920606613, Final Batch Loss: 0.28945162892341614\n",
      "Subject 10, Epoch 670, Loss: 1.2689728438854218, Final Batch Loss: 0.4314135015010834\n",
      "Subject 10, Epoch 671, Loss: 1.1110248118638992, Final Batch Loss: 0.38020649552345276\n",
      "Subject 10, Epoch 672, Loss: 1.301680475473404, Final Batch Loss: 0.5727939605712891\n",
      "Subject 10, Epoch 673, Loss: 1.3351643979549408, Final Batch Loss: 0.43624797463417053\n",
      "Subject 10, Epoch 674, Loss: 0.8915876001119614, Final Batch Loss: 0.06401079893112183\n",
      "Subject 10, Epoch 675, Loss: 1.3392349779605865, Final Batch Loss: 0.5549554228782654\n",
      "Subject 10, Epoch 676, Loss: 0.9576749950647354, Final Batch Loss: 0.128090038895607\n",
      "Subject 10, Epoch 677, Loss: 0.9383837878704071, Final Batch Loss: 0.2369924634695053\n",
      "Subject 10, Epoch 678, Loss: 0.9439381211996078, Final Batch Loss: 0.15559633076190948\n",
      "Subject 10, Epoch 679, Loss: 1.1772237718105316, Final Batch Loss: 0.4678899049758911\n",
      "Subject 10, Epoch 680, Loss: 1.0267636179924011, Final Batch Loss: 0.25471535325050354\n",
      "Subject 10, Epoch 681, Loss: 1.1837708055973053, Final Batch Loss: 0.32628992199897766\n",
      "Subject 10, Epoch 682, Loss: 1.3251658380031586, Final Batch Loss: 0.3644370138645172\n",
      "Subject 10, Epoch 683, Loss: 0.9365535601973534, Final Batch Loss: 0.11754288524389267\n",
      "Subject 10, Epoch 684, Loss: 1.173120141029358, Final Batch Loss: 0.39561727643013\n",
      "Subject 10, Epoch 685, Loss: 0.9464795291423798, Final Batch Loss: 0.15930502116680145\n",
      "Subject 10, Epoch 686, Loss: 1.0409139394760132, Final Batch Loss: 0.2875211536884308\n",
      "Subject 10, Epoch 687, Loss: 0.9212128967046738, Final Batch Loss: 0.20409749448299408\n",
      "Subject 10, Epoch 688, Loss: 1.0881747752428055, Final Batch Loss: 0.1767711192369461\n",
      "Subject 10, Epoch 689, Loss: 1.1742896139621735, Final Batch Loss: 0.3651251494884491\n",
      "Subject 10, Epoch 690, Loss: 1.039761334657669, Final Batch Loss: 0.22168566286563873\n",
      "Subject 10, Epoch 691, Loss: 1.0523759573698044, Final Batch Loss: 0.28544601798057556\n",
      "Subject 10, Epoch 692, Loss: 1.008654236793518, Final Batch Loss: 0.2437167912721634\n",
      "Subject 10, Epoch 693, Loss: 0.9680995494127274, Final Batch Loss: 0.24027907848358154\n",
      "Subject 10, Epoch 694, Loss: 1.141139268875122, Final Batch Loss: 0.3241722285747528\n",
      "Subject 10, Epoch 695, Loss: 1.1004129499197006, Final Batch Loss: 0.23605258762836456\n",
      "Subject 10, Epoch 696, Loss: 1.2030006349086761, Final Batch Loss: 0.3210170567035675\n",
      "Subject 10, Epoch 697, Loss: 0.9826139658689499, Final Batch Loss: 0.15986789762973785\n",
      "Subject 10, Epoch 698, Loss: 1.2213973253965378, Final Batch Loss: 0.40357163548469543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 699, Loss: 0.9600773453712463, Final Batch Loss: 0.17231090366840363\n",
      "Subject 10, Epoch 700, Loss: 1.2339760810136795, Final Batch Loss: 0.3681935966014862\n",
      "Subject 10, Epoch 701, Loss: 1.0113727152347565, Final Batch Loss: 0.20778094232082367\n",
      "Subject 10, Epoch 702, Loss: 1.0766407549381256, Final Batch Loss: 0.22739684581756592\n",
      "Subject 10, Epoch 703, Loss: 1.0881147533655167, Final Batch Loss: 0.2129218429327011\n",
      "Subject 10, Epoch 704, Loss: 1.0927822440862656, Final Batch Loss: 0.29750558733940125\n",
      "Subject 10, Epoch 705, Loss: 0.9463833123445511, Final Batch Loss: 0.14635558426380157\n",
      "Subject 10, Epoch 706, Loss: 0.8769844770431519, Final Batch Loss: 0.11224791407585144\n",
      "Subject 10, Epoch 707, Loss: 0.9372438117861748, Final Batch Loss: 0.1077738031744957\n",
      "Subject 10, Epoch 708, Loss: 0.9797685444355011, Final Batch Loss: 0.13942290842533112\n",
      "Subject 10, Epoch 709, Loss: 1.0415831357240677, Final Batch Loss: 0.31658533215522766\n",
      "Subject 10, Epoch 710, Loss: 1.0155156254768372, Final Batch Loss: 0.27044588327407837\n",
      "Subject 10, Epoch 711, Loss: 0.9827754497528076, Final Batch Loss: 0.18619823455810547\n",
      "Subject 10, Epoch 712, Loss: 1.126112923026085, Final Batch Loss: 0.35791635513305664\n",
      "Subject 10, Epoch 713, Loss: 0.9611939936876297, Final Batch Loss: 0.20376092195510864\n",
      "Subject 10, Epoch 714, Loss: 0.9560517966747284, Final Batch Loss: 0.16255001723766327\n",
      "Subject 10, Epoch 715, Loss: 1.134907841682434, Final Batch Loss: 0.3967672288417816\n",
      "Subject 10, Epoch 716, Loss: 1.1386247873306274, Final Batch Loss: 0.2713705599308014\n",
      "Subject 10, Epoch 717, Loss: 1.229096233844757, Final Batch Loss: 0.4059775173664093\n",
      "Subject 10, Epoch 718, Loss: 0.9560788720846176, Final Batch Loss: 0.1434764713048935\n",
      "Subject 10, Epoch 719, Loss: 1.0408921390771866, Final Batch Loss: 0.250336617231369\n",
      "Subject 10, Epoch 720, Loss: 0.9953683763742447, Final Batch Loss: 0.2286563664674759\n",
      "Subject 10, Epoch 721, Loss: 0.9742715060710907, Final Batch Loss: 0.2806237041950226\n",
      "Subject 10, Epoch 722, Loss: 0.9265925288200378, Final Batch Loss: 0.1600542813539505\n",
      "Subject 10, Epoch 723, Loss: 0.9534387439489365, Final Batch Loss: 0.15695656836032867\n",
      "Subject 10, Epoch 724, Loss: 1.193833902478218, Final Batch Loss: 0.3839142322540283\n",
      "Subject 10, Epoch 725, Loss: 1.1699801236391068, Final Batch Loss: 0.4015047550201416\n",
      "Subject 10, Epoch 726, Loss: 0.9197013527154922, Final Batch Loss: 0.1396135538816452\n",
      "Subject 10, Epoch 727, Loss: 1.0464663356542587, Final Batch Loss: 0.1488548070192337\n",
      "Subject 10, Epoch 728, Loss: 1.3213052451610565, Final Batch Loss: 0.5729552507400513\n",
      "Subject 10, Epoch 729, Loss: 0.842192105948925, Final Batch Loss: 0.08796098083257675\n",
      "Subject 10, Epoch 730, Loss: 1.0586802661418915, Final Batch Loss: 0.28810229897499084\n",
      "Subject 10, Epoch 731, Loss: 1.0053935497999191, Final Batch Loss: 0.2048364281654358\n",
      "Subject 10, Epoch 732, Loss: 0.8920612186193466, Final Batch Loss: 0.20696669816970825\n",
      "Subject 10, Epoch 733, Loss: 0.7944569960236549, Final Batch Loss: 0.10290848463773727\n",
      "Subject 10, Epoch 734, Loss: 1.0089776366949081, Final Batch Loss: 0.24308522045612335\n",
      "Subject 10, Epoch 735, Loss: 0.9085624366998672, Final Batch Loss: 0.2115776538848877\n",
      "Subject 10, Epoch 736, Loss: 1.0629390329122543, Final Batch Loss: 0.34791508316993713\n",
      "Subject 10, Epoch 737, Loss: 1.0114194005727768, Final Batch Loss: 0.1954193264245987\n",
      "Subject 10, Epoch 738, Loss: 1.1103148758411407, Final Batch Loss: 0.4314194917678833\n",
      "Subject 10, Epoch 739, Loss: 1.1080875098705292, Final Batch Loss: 0.3708345890045166\n",
      "Subject 10, Epoch 740, Loss: 0.9564250856637955, Final Batch Loss: 0.19660453498363495\n",
      "Subject 10, Epoch 741, Loss: 0.9637449830770493, Final Batch Loss: 0.22074301540851593\n",
      "Subject 10, Epoch 742, Loss: 1.1168627440929413, Final Batch Loss: 0.37873485684394836\n",
      "Subject 10, Epoch 743, Loss: 1.1498170793056488, Final Batch Loss: 0.4776287078857422\n",
      "Subject 10, Epoch 744, Loss: 1.0850136429071426, Final Batch Loss: 0.37222686409950256\n",
      "Subject 10, Epoch 745, Loss: 1.0339460372924805, Final Batch Loss: 0.3421737253665924\n",
      "Subject 10, Epoch 746, Loss: 0.9277740716934204, Final Batch Loss: 0.19704067707061768\n",
      "Subject 10, Epoch 747, Loss: 1.2490214109420776, Final Batch Loss: 0.25949475169181824\n",
      "Subject 10, Epoch 748, Loss: 0.9037536382675171, Final Batch Loss: 0.1314736008644104\n",
      "Subject 10, Epoch 749, Loss: 1.3492328077554703, Final Batch Loss: 0.5671889781951904\n",
      "Subject 10, Epoch 750, Loss: 0.8883009254932404, Final Batch Loss: 0.19342488050460815\n",
      "Subject 10, Epoch 751, Loss: 1.161491021513939, Final Batch Loss: 0.35491225123405457\n",
      "Subject 10, Epoch 752, Loss: 1.0511012226343155, Final Batch Loss: 0.28978249430656433\n",
      "Subject 10, Epoch 753, Loss: 1.0628062188625336, Final Batch Loss: 0.20447564125061035\n",
      "Subject 10, Epoch 754, Loss: 1.0943780094385147, Final Batch Loss: 0.2925880253314972\n",
      "Subject 10, Epoch 755, Loss: 0.945756584405899, Final Batch Loss: 0.2608734965324402\n",
      "Subject 10, Epoch 756, Loss: 0.9291746616363525, Final Batch Loss: 0.16692374646663666\n",
      "Subject 10, Epoch 757, Loss: 1.1550107300281525, Final Batch Loss: 0.30484604835510254\n",
      "Subject 10, Epoch 758, Loss: 1.011122927069664, Final Batch Loss: 0.29786327481269836\n",
      "Subject 10, Epoch 759, Loss: 1.0228010416030884, Final Batch Loss: 0.2948252558708191\n",
      "Subject 10, Epoch 760, Loss: 0.8397375345230103, Final Batch Loss: 0.14350874722003937\n",
      "Subject 10, Epoch 761, Loss: 1.1354825794696808, Final Batch Loss: 0.4105360507965088\n",
      "Subject 10, Epoch 762, Loss: 1.0989548414945602, Final Batch Loss: 0.3497326672077179\n",
      "Subject 10, Epoch 763, Loss: 1.2284483164548874, Final Batch Loss: 0.4185275733470917\n",
      "Subject 10, Epoch 764, Loss: 1.1696266829967499, Final Batch Loss: 0.39461758732795715\n",
      "Subject 10, Epoch 765, Loss: 1.0324887037277222, Final Batch Loss: 0.2769296169281006\n",
      "Subject 10, Epoch 766, Loss: 0.8416508585214615, Final Batch Loss: 0.1640586405992508\n",
      "Subject 10, Epoch 767, Loss: 1.9478295892477036, Final Batch Loss: 1.0033460855484009\n",
      "Subject 10, Epoch 768, Loss: 1.1049534678459167, Final Batch Loss: 0.3639223277568817\n",
      "Subject 10, Epoch 769, Loss: 0.969138115644455, Final Batch Loss: 0.24025221168994904\n",
      "Subject 10, Epoch 770, Loss: 0.9536318629980087, Final Batch Loss: 0.23511187732219696\n",
      "Subject 10, Epoch 771, Loss: 0.9728884100914001, Final Batch Loss: 0.27133744955062866\n",
      "Subject 10, Epoch 772, Loss: 1.338149294257164, Final Batch Loss: 0.42796218395233154\n",
      "Subject 10, Epoch 773, Loss: 0.9888746589422226, Final Batch Loss: 0.18830783665180206\n",
      "Subject 10, Epoch 774, Loss: 1.107433795928955, Final Batch Loss: 0.354339599609375\n",
      "Subject 10, Epoch 775, Loss: 0.7758150026202202, Final Batch Loss: 0.07817327231168747\n",
      "Subject 10, Epoch 776, Loss: 1.0766633450984955, Final Batch Loss: 0.224286749958992\n",
      "Subject 10, Epoch 777, Loss: 0.8191144093871117, Final Batch Loss: 0.08989915996789932\n",
      "Subject 10, Epoch 778, Loss: 1.1038955003023148, Final Batch Loss: 0.2987460494041443\n",
      "Subject 10, Epoch 779, Loss: 0.8364428654313087, Final Batch Loss: 0.09012239426374435\n",
      "Subject 10, Epoch 780, Loss: 0.8992080390453339, Final Batch Loss: 0.08723434805870056\n",
      "Subject 10, Epoch 781, Loss: 0.9391582757234573, Final Batch Loss: 0.1517203152179718\n",
      "Subject 10, Epoch 782, Loss: 1.2226575762033463, Final Batch Loss: 0.4530913531780243\n",
      "Subject 10, Epoch 783, Loss: 0.987810805439949, Final Batch Loss: 0.2861813008785248\n",
      "Subject 10, Epoch 784, Loss: 1.2260960787534714, Final Batch Loss: 0.5260984301567078\n",
      "Subject 10, Epoch 785, Loss: 0.9603908956050873, Final Batch Loss: 0.2384481281042099\n",
      "Subject 10, Epoch 786, Loss: 1.2131397873163223, Final Batch Loss: 0.4923681914806366\n",
      "Subject 10, Epoch 787, Loss: 0.8565535321831703, Final Batch Loss: 0.10750315338373184\n",
      "Subject 10, Epoch 788, Loss: 0.9785958081483841, Final Batch Loss: 0.2815028727054596\n",
      "Subject 10, Epoch 789, Loss: 0.8726662248373032, Final Batch Loss: 0.19441340863704681\n",
      "Subject 10, Epoch 790, Loss: 1.5331212133169174, Final Batch Loss: 0.8002102971076965\n",
      "Subject 10, Epoch 791, Loss: 0.8345399051904678, Final Batch Loss: 0.09492337703704834\n",
      "Subject 10, Epoch 792, Loss: 0.9687924683094025, Final Batch Loss: 0.13591745495796204\n",
      "Subject 10, Epoch 793, Loss: 1.0654300898313522, Final Batch Loss: 0.31199589371681213\n",
      "Subject 10, Epoch 794, Loss: 1.0651658177375793, Final Batch Loss: 0.3464820683002472\n",
      "Subject 10, Epoch 795, Loss: 0.9378772228956223, Final Batch Loss: 0.237305149435997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 796, Loss: 1.21883425116539, Final Batch Loss: 0.40746545791625977\n",
      "Subject 10, Epoch 797, Loss: 1.0182413756847382, Final Batch Loss: 0.2184058427810669\n",
      "Subject 10, Epoch 798, Loss: 1.2046010047197342, Final Batch Loss: 0.41148707270622253\n",
      "Subject 10, Epoch 799, Loss: 0.8211130201816559, Final Batch Loss: 0.08000405132770538\n",
      "Subject 10, Epoch 800, Loss: 1.0664251893758774, Final Batch Loss: 0.39480164647102356\n",
      "Subject 10, Epoch 801, Loss: 0.849664032459259, Final Batch Loss: 0.1682300567626953\n",
      "Subject 10, Epoch 802, Loss: 1.1026875674724579, Final Batch Loss: 0.34403762221336365\n",
      "Subject 10, Epoch 803, Loss: 0.8240044042468071, Final Batch Loss: 0.09508230537176132\n",
      "Subject 10, Epoch 804, Loss: 1.1394067108631134, Final Batch Loss: 0.4554286003112793\n",
      "Subject 10, Epoch 805, Loss: 0.9508332759141922, Final Batch Loss: 0.27333691716194153\n",
      "Subject 10, Epoch 806, Loss: 0.983897402882576, Final Batch Loss: 0.24213744699954987\n",
      "Subject 10, Epoch 807, Loss: 0.9319603145122528, Final Batch Loss: 0.2066219300031662\n",
      "Subject 10, Epoch 808, Loss: 1.036517396569252, Final Batch Loss: 0.30480167269706726\n",
      "Subject 10, Epoch 809, Loss: 0.8339735567569733, Final Batch Loss: 0.14574527740478516\n",
      "Subject 10, Epoch 810, Loss: 1.0428914576768875, Final Batch Loss: 0.28381210565567017\n",
      "Subject 10, Epoch 811, Loss: 1.0707128942012787, Final Batch Loss: 0.2724423110485077\n",
      "Subject 10, Epoch 812, Loss: 0.9382391571998596, Final Batch Loss: 0.19630135595798492\n",
      "Subject 10, Epoch 813, Loss: 1.0330616384744644, Final Batch Loss: 0.3382364809513092\n",
      "Subject 10, Epoch 814, Loss: 0.9229318648576736, Final Batch Loss: 0.15530022978782654\n",
      "Subject 10, Epoch 815, Loss: 0.9176190458238125, Final Batch Loss: 0.06199264153838158\n",
      "Subject 10, Epoch 816, Loss: 0.9247074276208878, Final Batch Loss: 0.21026335656642914\n",
      "Subject 10, Epoch 817, Loss: 0.9760701805353165, Final Batch Loss: 0.22159264981746674\n",
      "Subject 10, Epoch 818, Loss: 0.9219537228345871, Final Batch Loss: 0.27270299196243286\n",
      "Subject 10, Epoch 819, Loss: 1.0217921137809753, Final Batch Loss: 0.22547143697738647\n",
      "Subject 10, Epoch 820, Loss: 0.7759378030896187, Final Batch Loss: 0.10211855918169022\n",
      "Subject 10, Epoch 821, Loss: 0.9174193888902664, Final Batch Loss: 0.22675947844982147\n",
      "Subject 10, Epoch 822, Loss: 0.8905407339334488, Final Batch Loss: 0.1326262503862381\n",
      "Subject 10, Epoch 823, Loss: 1.0167811810970306, Final Batch Loss: 0.2515869736671448\n",
      "Subject 10, Epoch 824, Loss: 0.924668699502945, Final Batch Loss: 0.1306248903274536\n",
      "Subject 10, Epoch 825, Loss: 0.8873825371265411, Final Batch Loss: 0.1699313521385193\n",
      "Subject 10, Epoch 826, Loss: 1.0808918625116348, Final Batch Loss: 0.43813180923461914\n",
      "Subject 10, Epoch 827, Loss: 0.9231554567813873, Final Batch Loss: 0.21681441366672516\n",
      "Subject 10, Epoch 828, Loss: 0.9212298095226288, Final Batch Loss: 0.17401354014873505\n",
      "Subject 10, Epoch 829, Loss: 0.9413272440433502, Final Batch Loss: 0.25713562965393066\n",
      "Subject 10, Epoch 830, Loss: 1.1319074183702469, Final Batch Loss: 0.38786375522613525\n",
      "Subject 10, Epoch 831, Loss: 1.1236412674188614, Final Batch Loss: 0.33134475350379944\n",
      "Subject 10, Epoch 832, Loss: 1.1121190190315247, Final Batch Loss: 0.3349215090274811\n",
      "Subject 10, Epoch 833, Loss: 0.7998916581273079, Final Batch Loss: 0.08933605998754501\n",
      "Subject 10, Epoch 834, Loss: 0.8331389054656029, Final Batch Loss: 0.07980161160230637\n",
      "Subject 10, Epoch 835, Loss: 1.0212012380361557, Final Batch Loss: 0.27457335591316223\n",
      "Subject 10, Epoch 836, Loss: 0.8238096386194229, Final Batch Loss: 0.22503064572811127\n",
      "Subject 10, Epoch 837, Loss: 0.9915293753147125, Final Batch Loss: 0.2717305123806\n",
      "Subject 10, Epoch 838, Loss: 0.9424728155136108, Final Batch Loss: 0.23254649341106415\n",
      "Subject 10, Epoch 839, Loss: 1.1143213361501694, Final Batch Loss: 0.40330687165260315\n",
      "Subject 10, Epoch 840, Loss: 0.8495577871799469, Final Batch Loss: 0.20098739862442017\n",
      "Subject 10, Epoch 841, Loss: 1.424642637372017, Final Batch Loss: 0.7798497676849365\n",
      "Subject 10, Epoch 842, Loss: 1.2277405858039856, Final Batch Loss: 0.5417292714118958\n",
      "Subject 10, Epoch 843, Loss: 0.929802343249321, Final Batch Loss: 0.19647850096225739\n",
      "Subject 10, Epoch 844, Loss: 0.9803651422262192, Final Batch Loss: 0.18687482178211212\n",
      "Subject 10, Epoch 845, Loss: 0.9779943227767944, Final Batch Loss: 0.31229349970817566\n",
      "Subject 10, Epoch 846, Loss: 1.113422378897667, Final Batch Loss: 0.36789584159851074\n",
      "Subject 10, Epoch 847, Loss: 0.9181388467550278, Final Batch Loss: 0.23977349698543549\n",
      "Subject 10, Epoch 848, Loss: 0.9547850489616394, Final Batch Loss: 0.2499031275510788\n",
      "Subject 10, Epoch 849, Loss: 0.9634599089622498, Final Batch Loss: 0.1976187825202942\n",
      "Subject 10, Epoch 850, Loss: 0.8210603967308998, Final Batch Loss: 0.07833588868379593\n",
      "Subject 10, Epoch 851, Loss: 1.0407838821411133, Final Batch Loss: 0.22158245742321014\n",
      "Subject 10, Epoch 852, Loss: 0.8999046161770821, Final Batch Loss: 0.07860750705003738\n",
      "Subject 10, Epoch 853, Loss: 0.8948445469141006, Final Batch Loss: 0.09576612710952759\n",
      "Subject 10, Epoch 854, Loss: 0.9719494134187698, Final Batch Loss: 0.1733829826116562\n",
      "Subject 10, Epoch 855, Loss: 0.8936428129673004, Final Batch Loss: 0.21848009526729584\n",
      "Subject 10, Epoch 856, Loss: 0.8028805777430534, Final Batch Loss: 0.09291467815637589\n",
      "Subject 10, Epoch 857, Loss: 0.9160047322511673, Final Batch Loss: 0.21575583517551422\n",
      "Subject 10, Epoch 858, Loss: 1.0227679312229156, Final Batch Loss: 0.3134940564632416\n",
      "Subject 10, Epoch 859, Loss: 0.8048664033412933, Final Batch Loss: 0.20730452239513397\n",
      "Subject 10, Epoch 860, Loss: 1.0343578904867172, Final Batch Loss: 0.2088995724916458\n",
      "Subject 10, Epoch 861, Loss: 1.1743672341108322, Final Batch Loss: 0.431563138961792\n",
      "Subject 10, Epoch 862, Loss: 0.8308382853865623, Final Batch Loss: 0.07624957710504532\n",
      "Subject 10, Epoch 863, Loss: 0.9774062186479568, Final Batch Loss: 0.24057848751544952\n",
      "Subject 10, Epoch 864, Loss: 1.1413139551877975, Final Batch Loss: 0.39975953102111816\n",
      "Subject 10, Epoch 865, Loss: 1.1605682671070099, Final Batch Loss: 0.4391426146030426\n",
      "Subject 10, Epoch 866, Loss: 0.9723466634750366, Final Batch Loss: 0.26131606101989746\n",
      "Subject 10, Epoch 867, Loss: 0.8713153228163719, Final Batch Loss: 0.0838131234049797\n",
      "Subject 10, Epoch 868, Loss: 0.8583511710166931, Final Batch Loss: 0.13323211669921875\n",
      "Subject 10, Epoch 869, Loss: 0.8062948882579803, Final Batch Loss: 0.13613902032375336\n",
      "Subject 10, Epoch 870, Loss: 0.9044315814971924, Final Batch Loss: 0.12534695863723755\n",
      "Subject 10, Epoch 871, Loss: 0.8131576329469681, Final Batch Loss: 0.11213523149490356\n",
      "Subject 10, Epoch 872, Loss: 0.9732489138841629, Final Batch Loss: 0.2025919407606125\n",
      "Subject 10, Epoch 873, Loss: 1.185298964381218, Final Batch Loss: 0.4858306646347046\n",
      "Subject 10, Epoch 874, Loss: 0.9247475564479828, Final Batch Loss: 0.16812457144260406\n",
      "Subject 10, Epoch 875, Loss: 1.2171140313148499, Final Batch Loss: 0.552391529083252\n",
      "Subject 10, Epoch 876, Loss: 1.089841291308403, Final Batch Loss: 0.20135115087032318\n",
      "Subject 10, Epoch 877, Loss: 1.0680815130472183, Final Batch Loss: 0.3327680230140686\n",
      "Subject 10, Epoch 878, Loss: 0.8260241821408272, Final Batch Loss: 0.0771210715174675\n",
      "Subject 10, Epoch 879, Loss: 0.9329536333680153, Final Batch Loss: 0.12268397957086563\n",
      "Subject 10, Epoch 880, Loss: 0.9913519471883774, Final Batch Loss: 0.27099958062171936\n",
      "Subject 10, Epoch 881, Loss: 0.98856221139431, Final Batch Loss: 0.3021753132343292\n",
      "Subject 10, Epoch 882, Loss: 0.9819227904081345, Final Batch Loss: 0.28459131717681885\n",
      "Subject 10, Epoch 883, Loss: 0.8084951713681221, Final Batch Loss: 0.11504863947629929\n",
      "Subject 10, Epoch 884, Loss: 0.7996092513203621, Final Batch Loss: 0.09755057841539383\n",
      "Subject 10, Epoch 885, Loss: 0.8601153641939163, Final Batch Loss: 0.14284886419773102\n",
      "Subject 10, Epoch 886, Loss: 0.9579055458307266, Final Batch Loss: 0.29108208417892456\n",
      "Subject 10, Epoch 887, Loss: 0.911389634013176, Final Batch Loss: 0.25205495953559875\n",
      "Subject 10, Epoch 888, Loss: 0.922580748796463, Final Batch Loss: 0.18539763987064362\n",
      "Subject 10, Epoch 889, Loss: 0.9218645393848419, Final Batch Loss: 0.2661041021347046\n",
      "Subject 10, Epoch 890, Loss: 1.578786015510559, Final Batch Loss: 0.8494575023651123\n",
      "Subject 10, Epoch 891, Loss: 0.990841418504715, Final Batch Loss: 0.2973448634147644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 892, Loss: 0.9046944379806519, Final Batch Loss: 0.17360858619213104\n",
      "Subject 10, Epoch 893, Loss: 1.0206047594547272, Final Batch Loss: 0.2859673798084259\n",
      "Subject 10, Epoch 894, Loss: 0.8400265946984291, Final Batch Loss: 0.08952712267637253\n",
      "Subject 10, Epoch 895, Loss: 0.8654165640473366, Final Batch Loss: 0.12274966388940811\n",
      "Subject 10, Epoch 896, Loss: 0.9642914235591888, Final Batch Loss: 0.3112066388130188\n",
      "Subject 10, Epoch 897, Loss: 0.9807217568159103, Final Batch Loss: 0.24389569461345673\n",
      "Subject 10, Epoch 898, Loss: 0.8398384600877762, Final Batch Loss: 0.19180220365524292\n",
      "Subject 10, Epoch 899, Loss: 0.9674853533506393, Final Batch Loss: 0.20442579686641693\n",
      "Subject 10, Epoch 900, Loss: 1.2248539477586746, Final Batch Loss: 0.530701220035553\n",
      "Subject 10, Epoch 901, Loss: 0.9634120017290115, Final Batch Loss: 0.2517874240875244\n",
      "Subject 10, Epoch 902, Loss: 1.6227437108755112, Final Batch Loss: 0.8777216076850891\n",
      "Subject 10, Epoch 903, Loss: 0.9309771209955215, Final Batch Loss: 0.21729974448680878\n",
      "Subject 10, Epoch 904, Loss: 1.0352527797222137, Final Batch Loss: 0.31940409541130066\n",
      "Subject 10, Epoch 905, Loss: 0.9392591267824173, Final Batch Loss: 0.16645242273807526\n",
      "Subject 10, Epoch 906, Loss: 1.041147768497467, Final Batch Loss: 0.37757372856140137\n",
      "Subject 10, Epoch 907, Loss: 0.8839748501777649, Final Batch Loss: 0.05916282534599304\n",
      "Subject 10, Epoch 908, Loss: 1.0247592777013779, Final Batch Loss: 0.27012136578559875\n",
      "Subject 10, Epoch 909, Loss: 1.0010702311992645, Final Batch Loss: 0.3596927225589752\n",
      "Subject 10, Epoch 910, Loss: 0.9877054393291473, Final Batch Loss: 0.28139063715934753\n",
      "Subject 10, Epoch 911, Loss: 0.877041645348072, Final Batch Loss: 0.1192106381058693\n",
      "Subject 10, Epoch 912, Loss: 0.9699780344963074, Final Batch Loss: 0.3293052017688751\n",
      "Subject 10, Epoch 913, Loss: 0.8353041857481003, Final Batch Loss: 0.1395799219608307\n",
      "Subject 10, Epoch 914, Loss: 0.7675174251198769, Final Batch Loss: 0.0972365215420723\n",
      "Subject 10, Epoch 915, Loss: 1.067768320441246, Final Batch Loss: 0.3706742525100708\n",
      "Subject 10, Epoch 916, Loss: 0.9761480242013931, Final Batch Loss: 0.3173387050628662\n",
      "Subject 10, Epoch 917, Loss: 0.843060314655304, Final Batch Loss: 0.1375918835401535\n",
      "Subject 10, Epoch 918, Loss: 0.9681992530822754, Final Batch Loss: 0.26785847544670105\n",
      "Subject 10, Epoch 919, Loss: 0.8759377300739288, Final Batch Loss: 0.17065781354904175\n",
      "Subject 10, Epoch 920, Loss: 0.8003034591674805, Final Batch Loss: 0.15855805575847626\n",
      "Subject 10, Epoch 921, Loss: 0.9867343306541443, Final Batch Loss: 0.29448163509368896\n",
      "Subject 10, Epoch 922, Loss: 1.1013498157262802, Final Batch Loss: 0.37501704692840576\n",
      "Subject 10, Epoch 923, Loss: 0.8468058183789253, Final Batch Loss: 0.10734089463949203\n",
      "Subject 10, Epoch 924, Loss: 0.7752641774713993, Final Batch Loss: 0.032137636095285416\n",
      "Subject 10, Epoch 925, Loss: 0.8669024407863617, Final Batch Loss: 0.2042606621980667\n",
      "Subject 10, Epoch 926, Loss: 0.971635676920414, Final Batch Loss: 0.10291051119565964\n",
      "Subject 10, Epoch 927, Loss: 0.8902329802513123, Final Batch Loss: 0.18025732040405273\n",
      "Subject 10, Epoch 928, Loss: 0.9314477741718292, Final Batch Loss: 0.1046600341796875\n",
      "Subject 10, Epoch 929, Loss: 0.9061172604560852, Final Batch Loss: 0.1802724003791809\n",
      "Subject 10, Epoch 930, Loss: 0.9882384091615677, Final Batch Loss: 0.22424368560314178\n",
      "Subject 10, Epoch 931, Loss: 0.8157297819852829, Final Batch Loss: 0.1453743726015091\n",
      "Subject 10, Epoch 932, Loss: 1.0947035104036331, Final Batch Loss: 0.34409189224243164\n",
      "Subject 10, Epoch 933, Loss: 0.8396528512239456, Final Batch Loss: 0.20270709693431854\n",
      "Subject 10, Epoch 934, Loss: 0.8807229027152061, Final Batch Loss: 0.1138501837849617\n",
      "Subject 10, Epoch 935, Loss: 1.180440530180931, Final Batch Loss: 0.6319380402565002\n",
      "Subject 10, Epoch 936, Loss: 0.8120428398251534, Final Batch Loss: 0.11138389259576797\n",
      "Subject 10, Epoch 937, Loss: 1.0784022957086563, Final Batch Loss: 0.37147918343544006\n",
      "Subject 10, Epoch 938, Loss: 1.0026930421590805, Final Batch Loss: 0.2722837030887604\n",
      "Subject 10, Epoch 939, Loss: 0.9454402327537537, Final Batch Loss: 0.2894843518733978\n",
      "Subject 10, Epoch 940, Loss: 1.1348451673984528, Final Batch Loss: 0.4683297574520111\n",
      "Subject 10, Epoch 941, Loss: 0.8843087032437325, Final Batch Loss: 0.10597681254148483\n",
      "Subject 10, Epoch 942, Loss: 1.086340606212616, Final Batch Loss: 0.3502981662750244\n",
      "Subject 10, Epoch 943, Loss: 0.8690036088228226, Final Batch Loss: 0.2287759780883789\n",
      "Subject 10, Epoch 944, Loss: 0.9332537353038788, Final Batch Loss: 0.24944312870502472\n",
      "Subject 10, Epoch 945, Loss: 0.8633207976818085, Final Batch Loss: 0.17779022455215454\n",
      "Subject 10, Epoch 946, Loss: 0.953681230545044, Final Batch Loss: 0.2520536482334137\n",
      "Subject 10, Epoch 947, Loss: 0.8352494984865189, Final Batch Loss: 0.15427915751934052\n",
      "Subject 10, Epoch 948, Loss: 0.7416505496948957, Final Batch Loss: 0.030027074739336967\n",
      "Subject 10, Epoch 949, Loss: 0.9166423231363297, Final Batch Loss: 0.23725992441177368\n",
      "Subject 10, Epoch 950, Loss: 0.8539387732744217, Final Batch Loss: 0.1706651896238327\n",
      "Subject 10, Epoch 951, Loss: 0.6841898430138826, Final Batch Loss: 0.021549420431256294\n",
      "Subject 10, Epoch 952, Loss: 0.9492397904396057, Final Batch Loss: 0.24251939356327057\n",
      "Subject 10, Epoch 953, Loss: 0.8470765054225922, Final Batch Loss: 0.17548024654388428\n",
      "Subject 10, Epoch 954, Loss: 0.7423007786273956, Final Batch Loss: 0.1350824236869812\n",
      "Subject 10, Epoch 955, Loss: 0.9089223593473434, Final Batch Loss: 0.20761726796627045\n",
      "Subject 10, Epoch 956, Loss: 0.7512616962194443, Final Batch Loss: 0.059205666184425354\n",
      "Subject 10, Epoch 957, Loss: 0.7223979458212852, Final Batch Loss: 0.06935121864080429\n",
      "Subject 10, Epoch 958, Loss: 0.8780601918697357, Final Batch Loss: 0.22543931007385254\n",
      "Subject 10, Epoch 959, Loss: 1.034776508808136, Final Batch Loss: 0.26334649324417114\n",
      "Subject 10, Epoch 960, Loss: 0.9670368283987045, Final Batch Loss: 0.37172552943229675\n",
      "Subject 10, Epoch 961, Loss: 0.84075927734375, Final Batch Loss: 0.17025332152843475\n",
      "Subject 10, Epoch 962, Loss: 0.8160579204559326, Final Batch Loss: 0.20936548709869385\n",
      "Subject 10, Epoch 963, Loss: 0.8113638833165169, Final Batch Loss: 0.07768768817186356\n",
      "Subject 10, Epoch 964, Loss: 1.1597488671541214, Final Batch Loss: 0.5877500176429749\n",
      "Subject 10, Epoch 965, Loss: 0.7775992713868618, Final Batch Loss: 0.058403100818395615\n",
      "Subject 10, Epoch 966, Loss: 0.7953954041004181, Final Batch Loss: 0.16880832612514496\n",
      "Subject 10, Epoch 967, Loss: 0.9688438028097153, Final Batch Loss: 0.3300522267818451\n",
      "Subject 10, Epoch 968, Loss: 1.1795514076948166, Final Batch Loss: 0.5433812737464905\n",
      "Subject 10, Epoch 969, Loss: 1.2225207835435867, Final Batch Loss: 0.5244861841201782\n",
      "Subject 10, Epoch 970, Loss: 0.9482883363962173, Final Batch Loss: 0.1954454928636551\n",
      "Subject 10, Epoch 971, Loss: 0.8457129299640656, Final Batch Loss: 0.11707359552383423\n",
      "Subject 10, Epoch 972, Loss: 0.9476751387119293, Final Batch Loss: 0.2420751005411148\n",
      "Subject 10, Epoch 973, Loss: 0.925063744187355, Final Batch Loss: 0.1876888871192932\n",
      "Subject 10, Epoch 974, Loss: 0.851667508482933, Final Batch Loss: 0.1608608365058899\n",
      "Subject 10, Epoch 975, Loss: 1.111293002963066, Final Batch Loss: 0.29414597153663635\n",
      "Subject 10, Epoch 976, Loss: 0.8812292367219925, Final Batch Loss: 0.1769670844078064\n",
      "Subject 10, Epoch 977, Loss: 0.7931520789861679, Final Batch Loss: 0.20542049407958984\n",
      "Subject 10, Epoch 978, Loss: 0.9941432774066925, Final Batch Loss: 0.30695971846580505\n",
      "Subject 10, Epoch 979, Loss: 1.0731722861528397, Final Batch Loss: 0.3456908166408539\n",
      "Subject 10, Epoch 980, Loss: 0.8707062155008316, Final Batch Loss: 0.17005270719528198\n",
      "Subject 10, Epoch 981, Loss: 1.1767302006483078, Final Batch Loss: 0.4985334575176239\n",
      "Subject 10, Epoch 982, Loss: 0.9053491353988647, Final Batch Loss: 0.15535405278205872\n",
      "Subject 10, Epoch 983, Loss: 0.8547531813383102, Final Batch Loss: 0.17212188243865967\n",
      "Subject 10, Epoch 984, Loss: 1.059685006737709, Final Batch Loss: 0.4085393249988556\n",
      "Subject 10, Epoch 985, Loss: 0.7982697859406471, Final Batch Loss: 0.07927785068750381\n",
      "Subject 10, Epoch 986, Loss: 0.8122245967388153, Final Batch Loss: 0.13540314137935638\n",
      "Subject 10, Epoch 987, Loss: 0.6777292843908072, Final Batch Loss: 0.01999177597463131\n",
      "Subject 10, Epoch 988, Loss: 0.8658100217580795, Final Batch Loss: 0.2297518104314804\n",
      "Subject 10, Epoch 989, Loss: 0.9252138882875443, Final Batch Loss: 0.25237002968788147\n",
      "Subject 10, Epoch 990, Loss: 0.854945458471775, Final Batch Loss: 0.06727581471204758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10, Epoch 991, Loss: 0.8190134614706039, Final Batch Loss: 0.17437030375003815\n",
      "Subject 10, Epoch 992, Loss: 0.8067004829645157, Final Batch Loss: 0.07319650053977966\n",
      "Subject 10, Epoch 993, Loss: 0.7726998552680016, Final Batch Loss: 0.10300736874341965\n",
      "Subject 10, Epoch 994, Loss: 0.7122465390712023, Final Batch Loss: 0.021539585664868355\n",
      "Subject 10, Epoch 995, Loss: 0.9860833585262299, Final Batch Loss: 0.3035571575164795\n",
      "Subject 10, Epoch 996, Loss: 0.9978966861963272, Final Batch Loss: 0.3364551365375519\n",
      "Subject 10, Epoch 997, Loss: 0.8202225416898727, Final Batch Loss: 0.21586988866329193\n",
      "Subject 10, Epoch 998, Loss: 1.1540946513414383, Final Batch Loss: 0.4848744869232178\n",
      "Subject 10, Epoch 999, Loss: 0.9234053492546082, Final Batch Loss: 0.144598588347435\n",
      "Subject 10, Epoch 1000, Loss: 0.7534875646233559, Final Batch Loss: 0.10732877999544144\n",
      "Subject 11, Epoch 1, Loss: 7.388501048088074, Final Batch Loss: 1.8357402086257935\n",
      "Subject 11, Epoch 2, Loss: 7.403011679649353, Final Batch Loss: 1.8810548782348633\n",
      "Subject 11, Epoch 3, Loss: 7.205072283744812, Final Batch Loss: 1.6986041069030762\n",
      "Subject 11, Epoch 4, Loss: 7.349621415138245, Final Batch Loss: 1.8689804077148438\n",
      "Subject 11, Epoch 5, Loss: 7.263275384902954, Final Batch Loss: 1.8108043670654297\n",
      "Subject 11, Epoch 6, Loss: 7.266895771026611, Final Batch Loss: 1.8546067476272583\n",
      "Subject 11, Epoch 7, Loss: 7.08281934261322, Final Batch Loss: 1.7044553756713867\n",
      "Subject 11, Epoch 8, Loss: 7.044741749763489, Final Batch Loss: 1.7220395803451538\n",
      "Subject 11, Epoch 9, Loss: 6.92626416683197, Final Batch Loss: 1.6563223600387573\n",
      "Subject 11, Epoch 10, Loss: 6.916755795478821, Final Batch Loss: 1.7372370958328247\n",
      "Subject 11, Epoch 11, Loss: 6.793990731239319, Final Batch Loss: 1.7020395994186401\n",
      "Subject 11, Epoch 12, Loss: 6.610450744628906, Final Batch Loss: 1.6089609861373901\n",
      "Subject 11, Epoch 13, Loss: 6.520509958267212, Final Batch Loss: 1.5920013189315796\n",
      "Subject 11, Epoch 14, Loss: 6.379531383514404, Final Batch Loss: 1.5874322652816772\n",
      "Subject 11, Epoch 15, Loss: 6.244348526000977, Final Batch Loss: 1.5972343683242798\n",
      "Subject 11, Epoch 16, Loss: 5.939689040184021, Final Batch Loss: 1.4750765562057495\n",
      "Subject 11, Epoch 17, Loss: 5.870445370674133, Final Batch Loss: 1.5328487157821655\n",
      "Subject 11, Epoch 18, Loss: 5.61703085899353, Final Batch Loss: 1.2949661016464233\n",
      "Subject 11, Epoch 19, Loss: 5.591839671134949, Final Batch Loss: 1.414332389831543\n",
      "Subject 11, Epoch 20, Loss: 5.567993760108948, Final Batch Loss: 1.391536831855774\n",
      "Subject 11, Epoch 21, Loss: 5.3743884563446045, Final Batch Loss: 1.3893269300460815\n",
      "Subject 11, Epoch 22, Loss: 5.392861008644104, Final Batch Loss: 1.4137059450149536\n",
      "Subject 11, Epoch 23, Loss: 4.937513709068298, Final Batch Loss: 1.0459694862365723\n",
      "Subject 11, Epoch 24, Loss: 5.078544855117798, Final Batch Loss: 1.2572803497314453\n",
      "Subject 11, Epoch 25, Loss: 4.992344975471497, Final Batch Loss: 1.268936038017273\n",
      "Subject 11, Epoch 26, Loss: 5.078126907348633, Final Batch Loss: 1.250583529472351\n",
      "Subject 11, Epoch 27, Loss: 4.863643646240234, Final Batch Loss: 1.2500723600387573\n",
      "Subject 11, Epoch 28, Loss: 4.799324989318848, Final Batch Loss: 1.1466256380081177\n",
      "Subject 11, Epoch 29, Loss: 4.804298162460327, Final Batch Loss: 1.1457687616348267\n",
      "Subject 11, Epoch 30, Loss: 4.844725251197815, Final Batch Loss: 1.2897299528121948\n",
      "Subject 11, Epoch 31, Loss: 4.72583544254303, Final Batch Loss: 1.1639800071716309\n",
      "Subject 11, Epoch 32, Loss: 4.643685698509216, Final Batch Loss: 1.156481146812439\n",
      "Subject 11, Epoch 33, Loss: 4.470245838165283, Final Batch Loss: 1.1280244588851929\n",
      "Subject 11, Epoch 34, Loss: 4.385716259479523, Final Batch Loss: 0.9907209277153015\n",
      "Subject 11, Epoch 35, Loss: 4.955054402351379, Final Batch Loss: 1.5465513467788696\n",
      "Subject 11, Epoch 36, Loss: 4.321511209011078, Final Batch Loss: 0.9485310912132263\n",
      "Subject 11, Epoch 37, Loss: 4.5850266218185425, Final Batch Loss: 1.0793079137802124\n",
      "Subject 11, Epoch 38, Loss: 4.564665198326111, Final Batch Loss: 1.1882883310317993\n",
      "Subject 11, Epoch 39, Loss: 4.432729959487915, Final Batch Loss: 1.0373111963272095\n",
      "Subject 11, Epoch 40, Loss: 4.418982982635498, Final Batch Loss: 1.0561150312423706\n",
      "Subject 11, Epoch 41, Loss: 4.302925765514374, Final Batch Loss: 0.9674345850944519\n",
      "Subject 11, Epoch 42, Loss: 4.868100166320801, Final Batch Loss: 1.4783164262771606\n",
      "Subject 11, Epoch 43, Loss: 4.677397012710571, Final Batch Loss: 1.2235859632492065\n",
      "Subject 11, Epoch 44, Loss: 4.311010897159576, Final Batch Loss: 0.9947512745857239\n",
      "Subject 11, Epoch 45, Loss: 4.6165406703948975, Final Batch Loss: 1.2322818040847778\n",
      "Subject 11, Epoch 46, Loss: 4.471060872077942, Final Batch Loss: 1.1900819540023804\n",
      "Subject 11, Epoch 47, Loss: 4.468036770820618, Final Batch Loss: 1.1383620500564575\n",
      "Subject 11, Epoch 48, Loss: 4.419247388839722, Final Batch Loss: 1.127395749092102\n",
      "Subject 11, Epoch 49, Loss: 4.202678382396698, Final Batch Loss: 0.8923752903938293\n",
      "Subject 11, Epoch 50, Loss: 4.380905270576477, Final Batch Loss: 1.1193727254867554\n",
      "Subject 11, Epoch 51, Loss: 4.380605101585388, Final Batch Loss: 1.1736871004104614\n",
      "Subject 11, Epoch 52, Loss: 4.3038023710250854, Final Batch Loss: 1.0549490451812744\n",
      "Subject 11, Epoch 53, Loss: 4.305326461791992, Final Batch Loss: 1.032525658607483\n",
      "Subject 11, Epoch 54, Loss: 4.394902944564819, Final Batch Loss: 1.1492507457733154\n",
      "Subject 11, Epoch 55, Loss: 4.208398401737213, Final Batch Loss: 0.8848589062690735\n",
      "Subject 11, Epoch 56, Loss: 4.251632809638977, Final Batch Loss: 1.0500715970993042\n",
      "Subject 11, Epoch 57, Loss: 4.144084811210632, Final Batch Loss: 0.9333786964416504\n",
      "Subject 11, Epoch 58, Loss: 4.473465919494629, Final Batch Loss: 1.1632153987884521\n",
      "Subject 11, Epoch 59, Loss: 4.084860026836395, Final Batch Loss: 0.9152219891548157\n",
      "Subject 11, Epoch 60, Loss: 4.120492994785309, Final Batch Loss: 0.8916060328483582\n",
      "Subject 11, Epoch 61, Loss: 4.379913330078125, Final Batch Loss: 1.1961525678634644\n",
      "Subject 11, Epoch 62, Loss: 4.263819456100464, Final Batch Loss: 1.1500853300094604\n",
      "Subject 11, Epoch 63, Loss: 4.072337746620178, Final Batch Loss: 1.015480637550354\n",
      "Subject 11, Epoch 64, Loss: 4.261633396148682, Final Batch Loss: 1.0636892318725586\n",
      "Subject 11, Epoch 65, Loss: 4.087381601333618, Final Batch Loss: 1.021708607673645\n",
      "Subject 11, Epoch 66, Loss: 4.131968200206757, Final Batch Loss: 0.9908077120780945\n",
      "Subject 11, Epoch 67, Loss: 4.313363194465637, Final Batch Loss: 1.165183424949646\n",
      "Subject 11, Epoch 68, Loss: 4.166505336761475, Final Batch Loss: 1.0865392684936523\n",
      "Subject 11, Epoch 69, Loss: 4.023729383945465, Final Batch Loss: 0.9509860873222351\n",
      "Subject 11, Epoch 70, Loss: 3.989128589630127, Final Batch Loss: 0.9481699466705322\n",
      "Subject 11, Epoch 71, Loss: 4.189648747444153, Final Batch Loss: 1.1610976457595825\n",
      "Subject 11, Epoch 72, Loss: 3.7875083684921265, Final Batch Loss: 0.7446701526641846\n",
      "Subject 11, Epoch 73, Loss: 3.705054223537445, Final Batch Loss: 0.6442379355430603\n",
      "Subject 11, Epoch 74, Loss: 3.9463650584220886, Final Batch Loss: 0.8316759467124939\n",
      "Subject 11, Epoch 75, Loss: 4.149575769901276, Final Batch Loss: 1.142566204071045\n",
      "Subject 11, Epoch 76, Loss: 4.220640003681183, Final Batch Loss: 1.191483974456787\n",
      "Subject 11, Epoch 77, Loss: 3.8436816334724426, Final Batch Loss: 0.8346317410469055\n",
      "Subject 11, Epoch 78, Loss: 4.148333013057709, Final Batch Loss: 1.1071048974990845\n",
      "Subject 11, Epoch 79, Loss: 4.197513580322266, Final Batch Loss: 1.320658564567566\n",
      "Subject 11, Epoch 80, Loss: 3.8462732434272766, Final Batch Loss: 0.9778614640235901\n",
      "Subject 11, Epoch 81, Loss: 4.023155748844147, Final Batch Loss: 1.1346930265426636\n",
      "Subject 11, Epoch 82, Loss: 3.740440249443054, Final Batch Loss: 0.8474181294441223\n",
      "Subject 11, Epoch 83, Loss: 3.65292227268219, Final Batch Loss: 0.7112586498260498\n",
      "Subject 11, Epoch 84, Loss: 4.063138127326965, Final Batch Loss: 1.168305516242981\n",
      "Subject 11, Epoch 85, Loss: 3.8486257195472717, Final Batch Loss: 0.9357404708862305\n",
      "Subject 11, Epoch 86, Loss: 3.7685016989707947, Final Batch Loss: 0.8421468734741211\n",
      "Subject 11, Epoch 87, Loss: 3.827729821205139, Final Batch Loss: 1.0078762769699097\n",
      "Subject 11, Epoch 88, Loss: 3.9621164798736572, Final Batch Loss: 1.1593724489212036\n",
      "Subject 11, Epoch 89, Loss: 3.638746440410614, Final Batch Loss: 0.808650016784668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 90, Loss: 3.5632640719413757, Final Batch Loss: 0.7795157432556152\n",
      "Subject 11, Epoch 91, Loss: 3.3998520970344543, Final Batch Loss: 0.6424916386604309\n",
      "Subject 11, Epoch 92, Loss: 3.6332631707191467, Final Batch Loss: 0.9401757121086121\n",
      "Subject 11, Epoch 93, Loss: 4.148004412651062, Final Batch Loss: 1.3739582300186157\n",
      "Subject 11, Epoch 94, Loss: 3.61088228225708, Final Batch Loss: 0.8879544138908386\n",
      "Subject 11, Epoch 95, Loss: 3.6486268639564514, Final Batch Loss: 0.9295551180839539\n",
      "Subject 11, Epoch 96, Loss: 3.603864371776581, Final Batch Loss: 0.8852961659431458\n",
      "Subject 11, Epoch 97, Loss: 3.8455101251602173, Final Batch Loss: 1.1132209300994873\n",
      "Subject 11, Epoch 98, Loss: 4.089975535869598, Final Batch Loss: 1.3948599100112915\n",
      "Subject 11, Epoch 99, Loss: 3.6177308559417725, Final Batch Loss: 0.9130239486694336\n",
      "Subject 11, Epoch 100, Loss: 3.5227718353271484, Final Batch Loss: 0.8636568188667297\n",
      "Subject 11, Epoch 101, Loss: 3.358918607234955, Final Batch Loss: 0.7889435291290283\n",
      "Subject 11, Epoch 102, Loss: 3.484660565853119, Final Batch Loss: 0.7761552333831787\n",
      "Subject 11, Epoch 103, Loss: 3.7033194303512573, Final Batch Loss: 1.0086191892623901\n",
      "Subject 11, Epoch 104, Loss: 3.7390044927597046, Final Batch Loss: 1.101502776145935\n",
      "Subject 11, Epoch 105, Loss: 3.5934624671936035, Final Batch Loss: 1.0123237371444702\n",
      "Subject 11, Epoch 106, Loss: 3.402852475643158, Final Batch Loss: 0.831960141658783\n",
      "Subject 11, Epoch 107, Loss: 3.3429566621780396, Final Batch Loss: 0.7843791842460632\n",
      "Subject 11, Epoch 108, Loss: 3.4340131282806396, Final Batch Loss: 0.9232808947563171\n",
      "Subject 11, Epoch 109, Loss: 3.1748173236846924, Final Batch Loss: 0.641570508480072\n",
      "Subject 11, Epoch 110, Loss: 3.507569432258606, Final Batch Loss: 1.0046576261520386\n",
      "Subject 11, Epoch 111, Loss: 3.364971339702606, Final Batch Loss: 0.8287087082862854\n",
      "Subject 11, Epoch 112, Loss: 2.9294018149375916, Final Batch Loss: 0.5034189820289612\n",
      "Subject 11, Epoch 113, Loss: 3.433621048927307, Final Batch Loss: 0.9789122939109802\n",
      "Subject 11, Epoch 114, Loss: 3.1972082257270813, Final Batch Loss: 0.7665989995002747\n",
      "Subject 11, Epoch 115, Loss: 3.398671567440033, Final Batch Loss: 1.0080405473709106\n",
      "Subject 11, Epoch 116, Loss: 3.0010348558425903, Final Batch Loss: 0.6249656081199646\n",
      "Subject 11, Epoch 117, Loss: 3.334074020385742, Final Batch Loss: 0.9299097061157227\n",
      "Subject 11, Epoch 118, Loss: 2.9839635491371155, Final Batch Loss: 0.5624294877052307\n",
      "Subject 11, Epoch 119, Loss: 3.189022660255432, Final Batch Loss: 0.8428750038146973\n",
      "Subject 11, Epoch 120, Loss: 3.3010897040367126, Final Batch Loss: 0.8984324336051941\n",
      "Subject 11, Epoch 121, Loss: 3.423568367958069, Final Batch Loss: 1.074742078781128\n",
      "Subject 11, Epoch 122, Loss: 3.260747253894806, Final Batch Loss: 0.9147788882255554\n",
      "Subject 11, Epoch 123, Loss: 3.074761152267456, Final Batch Loss: 0.7658664584159851\n",
      "Subject 11, Epoch 124, Loss: 2.8135780096054077, Final Batch Loss: 0.6659899353981018\n",
      "Subject 11, Epoch 125, Loss: 2.8886133432388306, Final Batch Loss: 0.6823729872703552\n",
      "Subject 11, Epoch 126, Loss: 3.0406763553619385, Final Batch Loss: 0.8720107674598694\n",
      "Subject 11, Epoch 127, Loss: 3.104203164577484, Final Batch Loss: 0.9913037419319153\n",
      "Subject 11, Epoch 128, Loss: 2.7548888325691223, Final Batch Loss: 0.6021220088005066\n",
      "Subject 11, Epoch 129, Loss: 3.0083304047584534, Final Batch Loss: 0.7581891417503357\n",
      "Subject 11, Epoch 130, Loss: 3.062851369380951, Final Batch Loss: 0.8695734143257141\n",
      "Subject 11, Epoch 131, Loss: 2.5220439732074738, Final Batch Loss: 0.363471120595932\n",
      "Subject 11, Epoch 132, Loss: 2.509436309337616, Final Batch Loss: 0.42222511768341064\n",
      "Subject 11, Epoch 133, Loss: 2.6800303757190704, Final Batch Loss: 0.4092114269733429\n",
      "Subject 11, Epoch 134, Loss: 2.5223100185394287, Final Batch Loss: 0.42246317863464355\n",
      "Subject 11, Epoch 135, Loss: 3.1550703644752502, Final Batch Loss: 1.1147018671035767\n",
      "Subject 11, Epoch 136, Loss: 2.7436509132385254, Final Batch Loss: 0.6806374192237854\n",
      "Subject 11, Epoch 137, Loss: 2.663178026676178, Final Batch Loss: 0.5213170647621155\n",
      "Subject 11, Epoch 138, Loss: 2.841548800468445, Final Batch Loss: 0.8223363757133484\n",
      "Subject 11, Epoch 139, Loss: 2.5355304479599, Final Batch Loss: 0.532426118850708\n",
      "Subject 11, Epoch 140, Loss: 2.7597708106040955, Final Batch Loss: 0.5922691226005554\n",
      "Subject 11, Epoch 141, Loss: 2.6637625098228455, Final Batch Loss: 0.582429826259613\n",
      "Subject 11, Epoch 142, Loss: 2.420225888490677, Final Batch Loss: 0.42313042283058167\n",
      "Subject 11, Epoch 143, Loss: 2.7392491698265076, Final Batch Loss: 0.7340598106384277\n",
      "Subject 11, Epoch 144, Loss: 2.6930081248283386, Final Batch Loss: 0.6142006516456604\n",
      "Subject 11, Epoch 145, Loss: 2.821943759918213, Final Batch Loss: 0.7446794509887695\n",
      "Subject 11, Epoch 146, Loss: 2.9960986375808716, Final Batch Loss: 1.0060499906539917\n",
      "Subject 11, Epoch 147, Loss: 2.352613180875778, Final Batch Loss: 0.36067572236061096\n",
      "Subject 11, Epoch 148, Loss: 2.41256046295166, Final Batch Loss: 0.46202921867370605\n",
      "Subject 11, Epoch 149, Loss: 2.7158437371253967, Final Batch Loss: 0.6883871555328369\n",
      "Subject 11, Epoch 150, Loss: 2.7630881667137146, Final Batch Loss: 0.8659191131591797\n",
      "Subject 11, Epoch 151, Loss: 2.855359375476837, Final Batch Loss: 0.8378796577453613\n",
      "Subject 11, Epoch 152, Loss: 2.450946867465973, Final Batch Loss: 0.5462567210197449\n",
      "Subject 11, Epoch 153, Loss: 2.678959846496582, Final Batch Loss: 0.7878561019897461\n",
      "Subject 11, Epoch 154, Loss: 2.4402304887771606, Final Batch Loss: 0.5396520495414734\n",
      "Subject 11, Epoch 155, Loss: 2.4638403356075287, Final Batch Loss: 0.4825901687145233\n",
      "Subject 11, Epoch 156, Loss: 2.725594937801361, Final Batch Loss: 0.8972721099853516\n",
      "Subject 11, Epoch 157, Loss: 2.6071627140045166, Final Batch Loss: 0.7690331339836121\n",
      "Subject 11, Epoch 158, Loss: 2.5979520082473755, Final Batch Loss: 0.7778703570365906\n",
      "Subject 11, Epoch 159, Loss: 2.684666633605957, Final Batch Loss: 0.6814701557159424\n",
      "Subject 11, Epoch 160, Loss: 2.535045623779297, Final Batch Loss: 0.596872866153717\n",
      "Subject 11, Epoch 161, Loss: 2.510368674993515, Final Batch Loss: 0.6612760424613953\n",
      "Subject 11, Epoch 162, Loss: 2.3404879570007324, Final Batch Loss: 0.4061610698699951\n",
      "Subject 11, Epoch 163, Loss: 2.458134710788727, Final Batch Loss: 0.6161438822746277\n",
      "Subject 11, Epoch 164, Loss: 2.284348428249359, Final Batch Loss: 0.5819756388664246\n",
      "Subject 11, Epoch 165, Loss: 2.7408387064933777, Final Batch Loss: 0.9574573636054993\n",
      "Subject 11, Epoch 166, Loss: 1.9851650595664978, Final Batch Loss: 0.29976290464401245\n",
      "Subject 11, Epoch 167, Loss: 2.153152883052826, Final Batch Loss: 0.24835902452468872\n",
      "Subject 11, Epoch 168, Loss: 2.240572899580002, Final Batch Loss: 0.38340917229652405\n",
      "Subject 11, Epoch 169, Loss: 2.2853127121925354, Final Batch Loss: 0.49039018154144287\n",
      "Subject 11, Epoch 170, Loss: 2.4371123909950256, Final Batch Loss: 0.7802470326423645\n",
      "Subject 11, Epoch 171, Loss: 2.283928632736206, Final Batch Loss: 0.6113556027412415\n",
      "Subject 11, Epoch 172, Loss: 1.9124650061130524, Final Batch Loss: 0.11521682143211365\n",
      "Subject 11, Epoch 173, Loss: 2.462960183620453, Final Batch Loss: 0.577416718006134\n",
      "Subject 11, Epoch 174, Loss: 2.3579558730125427, Final Batch Loss: 0.6372467875480652\n",
      "Subject 11, Epoch 175, Loss: 2.37337926030159, Final Batch Loss: 0.6440717577934265\n",
      "Subject 11, Epoch 176, Loss: 2.150051385164261, Final Batch Loss: 0.38920602202415466\n",
      "Subject 11, Epoch 177, Loss: 2.130109190940857, Final Batch Loss: 0.45208343863487244\n",
      "Subject 11, Epoch 178, Loss: 2.1479903161525726, Final Batch Loss: 0.4184190332889557\n",
      "Subject 11, Epoch 179, Loss: 2.28913950920105, Final Batch Loss: 0.6455140709877014\n",
      "Subject 11, Epoch 180, Loss: 2.120432287454605, Final Batch Loss: 0.38892611861228943\n",
      "Subject 11, Epoch 181, Loss: 2.05004346370697, Final Batch Loss: 0.4587486684322357\n",
      "Subject 11, Epoch 182, Loss: 2.114984929561615, Final Batch Loss: 0.5436926484107971\n",
      "Subject 11, Epoch 183, Loss: 2.44376403093338, Final Batch Loss: 0.7326529622077942\n",
      "Subject 11, Epoch 184, Loss: 2.4732664227485657, Final Batch Loss: 0.7539450526237488\n",
      "Subject 11, Epoch 185, Loss: 2.136882930994034, Final Batch Loss: 0.4646616280078888\n",
      "Subject 11, Epoch 186, Loss: 2.326545387506485, Final Batch Loss: 0.6744822859764099\n",
      "Subject 11, Epoch 187, Loss: 2.2295570075511932, Final Batch Loss: 0.40292611718177795\n",
      "Subject 11, Epoch 188, Loss: 2.005174607038498, Final Batch Loss: 0.3318813145160675\n",
      "Subject 11, Epoch 189, Loss: 2.2845608592033386, Final Batch Loss: 0.594821035861969\n",
      "Subject 11, Epoch 190, Loss: 2.0151821970939636, Final Batch Loss: 0.4152480363845825\n",
      "Subject 11, Epoch 191, Loss: 2.1352871656417847, Final Batch Loss: 0.5537012219429016\n",
      "Subject 11, Epoch 192, Loss: 1.9543600976467133, Final Batch Loss: 0.3687743842601776\n",
      "Subject 11, Epoch 193, Loss: 2.3169448375701904, Final Batch Loss: 0.7126843929290771\n",
      "Subject 11, Epoch 194, Loss: 2.553010940551758, Final Batch Loss: 1.0110970735549927\n",
      "Subject 11, Epoch 195, Loss: 1.8925430476665497, Final Batch Loss: 0.38302716612815857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 196, Loss: 2.317738652229309, Final Batch Loss: 0.6966314315795898\n",
      "Subject 11, Epoch 197, Loss: 2.158714473247528, Final Batch Loss: 0.5941693186759949\n",
      "Subject 11, Epoch 198, Loss: 1.792205810546875, Final Batch Loss: 0.21410846710205078\n",
      "Subject 11, Epoch 199, Loss: 2.2362390756607056, Final Batch Loss: 0.7298476099967957\n",
      "Subject 11, Epoch 200, Loss: 1.9512512385845184, Final Batch Loss: 0.40545257925987244\n",
      "Subject 11, Epoch 201, Loss: 1.8445679247379303, Final Batch Loss: 0.30857551097869873\n",
      "Subject 11, Epoch 202, Loss: 1.9422695636749268, Final Batch Loss: 0.4284079968929291\n",
      "Subject 11, Epoch 203, Loss: 2.1448894441127777, Final Batch Loss: 0.6428448557853699\n",
      "Subject 11, Epoch 204, Loss: 2.266698569059372, Final Batch Loss: 0.6446309089660645\n",
      "Subject 11, Epoch 205, Loss: 2.1999122202396393, Final Batch Loss: 0.6155041456222534\n",
      "Subject 11, Epoch 206, Loss: 2.1443023681640625, Final Batch Loss: 0.7503682971000671\n",
      "Subject 11, Epoch 207, Loss: 2.160923093557358, Final Batch Loss: 0.5912702083587646\n",
      "Subject 11, Epoch 208, Loss: 2.557539165019989, Final Batch Loss: 0.7677214741706848\n",
      "Subject 11, Epoch 209, Loss: 2.237907975912094, Final Batch Loss: 0.6822829246520996\n",
      "Subject 11, Epoch 210, Loss: 1.5819858238101006, Final Batch Loss: 0.11298771947622299\n",
      "Subject 11, Epoch 211, Loss: 1.749328464269638, Final Batch Loss: 0.1796351671218872\n",
      "Subject 11, Epoch 212, Loss: 2.10706827044487, Final Batch Loss: 0.6958042979240417\n",
      "Subject 11, Epoch 213, Loss: 2.1594749689102173, Final Batch Loss: 0.7045629024505615\n",
      "Subject 11, Epoch 214, Loss: 1.941216617822647, Final Batch Loss: 0.3882327079772949\n",
      "Subject 11, Epoch 215, Loss: 1.6701543033123016, Final Batch Loss: 0.22994637489318848\n",
      "Subject 11, Epoch 216, Loss: 2.0862198173999786, Final Batch Loss: 0.6973243355751038\n",
      "Subject 11, Epoch 217, Loss: 2.02060666680336, Final Batch Loss: 0.569939911365509\n",
      "Subject 11, Epoch 218, Loss: 1.657629832625389, Final Batch Loss: 0.14322642982006073\n",
      "Subject 11, Epoch 219, Loss: 2.071150481700897, Final Batch Loss: 0.6008278727531433\n",
      "Subject 11, Epoch 220, Loss: 2.1509048342704773, Final Batch Loss: 0.699385941028595\n",
      "Subject 11, Epoch 221, Loss: 1.8271308243274689, Final Batch Loss: 0.28631481528282166\n",
      "Subject 11, Epoch 222, Loss: 2.156642109155655, Final Batch Loss: 0.8067960739135742\n",
      "Subject 11, Epoch 223, Loss: 1.7180759012699127, Final Batch Loss: 0.35728999972343445\n",
      "Subject 11, Epoch 224, Loss: 1.983525812625885, Final Batch Loss: 0.49999499320983887\n",
      "Subject 11, Epoch 225, Loss: 1.9591480791568756, Final Batch Loss: 0.43396806716918945\n",
      "Subject 11, Epoch 226, Loss: 2.039017140865326, Final Batch Loss: 0.5625617504119873\n",
      "Subject 11, Epoch 227, Loss: 1.6807709038257599, Final Batch Loss: 0.3697464168071747\n",
      "Subject 11, Epoch 228, Loss: 1.616566687822342, Final Batch Loss: 0.25338926911354065\n",
      "Subject 11, Epoch 229, Loss: 1.887922078371048, Final Batch Loss: 0.4553718566894531\n",
      "Subject 11, Epoch 230, Loss: 1.6978460550308228, Final Batch Loss: 0.4002901017665863\n",
      "Subject 11, Epoch 231, Loss: 2.0413845479488373, Final Batch Loss: 0.5688051581382751\n",
      "Subject 11, Epoch 232, Loss: 1.8669342398643494, Final Batch Loss: 0.4287996292114258\n",
      "Subject 11, Epoch 233, Loss: 1.8958370387554169, Final Batch Loss: 0.5129736065864563\n",
      "Subject 11, Epoch 234, Loss: 1.8464211523532867, Final Batch Loss: 0.475974440574646\n",
      "Subject 11, Epoch 235, Loss: 2.0094578564167023, Final Batch Loss: 0.5962993502616882\n",
      "Subject 11, Epoch 236, Loss: 1.772535890340805, Final Batch Loss: 0.35376954078674316\n",
      "Subject 11, Epoch 237, Loss: 1.6384902596473694, Final Batch Loss: 0.34226205945014954\n",
      "Subject 11, Epoch 238, Loss: 1.860494464635849, Final Batch Loss: 0.5639632344245911\n",
      "Subject 11, Epoch 239, Loss: 1.695219337940216, Final Batch Loss: 0.32627150416374207\n",
      "Subject 11, Epoch 240, Loss: 2.124216139316559, Final Batch Loss: 0.7903377413749695\n",
      "Subject 11, Epoch 241, Loss: 1.7986774444580078, Final Batch Loss: 0.30673450231552124\n",
      "Subject 11, Epoch 242, Loss: 1.8969310224056244, Final Batch Loss: 0.44507166743278503\n",
      "Subject 11, Epoch 243, Loss: 1.639712542295456, Final Batch Loss: 0.28693830966949463\n",
      "Subject 11, Epoch 244, Loss: 1.7410460412502289, Final Batch Loss: 0.317553848028183\n",
      "Subject 11, Epoch 245, Loss: 1.952134519815445, Final Batch Loss: 0.6173315048217773\n",
      "Subject 11, Epoch 246, Loss: 1.9180500507354736, Final Batch Loss: 0.6063210964202881\n",
      "Subject 11, Epoch 247, Loss: 1.7030719220638275, Final Batch Loss: 0.2147415280342102\n",
      "Subject 11, Epoch 248, Loss: 1.7482710778713226, Final Batch Loss: 0.5093721747398376\n",
      "Subject 11, Epoch 249, Loss: 1.6022108495235443, Final Batch Loss: 0.3314049243927002\n",
      "Subject 11, Epoch 250, Loss: 1.8920728862285614, Final Batch Loss: 0.44672954082489014\n",
      "Subject 11, Epoch 251, Loss: 1.6466132998466492, Final Batch Loss: 0.2602844834327698\n",
      "Subject 11, Epoch 252, Loss: 1.7920385897159576, Final Batch Loss: 0.46905186772346497\n",
      "Subject 11, Epoch 253, Loss: 1.7810383439064026, Final Batch Loss: 0.5188701748847961\n",
      "Subject 11, Epoch 254, Loss: 1.751266598701477, Final Batch Loss: 0.35569754242897034\n",
      "Subject 11, Epoch 255, Loss: 1.6203222274780273, Final Batch Loss: 0.24561750888824463\n",
      "Subject 11, Epoch 256, Loss: 1.597859650850296, Final Batch Loss: 0.2874746322631836\n",
      "Subject 11, Epoch 257, Loss: 1.648014098405838, Final Batch Loss: 0.3622252643108368\n",
      "Subject 11, Epoch 258, Loss: 1.554691344499588, Final Batch Loss: 0.2794012725353241\n",
      "Subject 11, Epoch 259, Loss: 1.762836754322052, Final Batch Loss: 0.5539281964302063\n",
      "Subject 11, Epoch 260, Loss: 1.5264568328857422, Final Batch Loss: 0.2603418231010437\n",
      "Subject 11, Epoch 261, Loss: 1.5413133054971695, Final Batch Loss: 0.17369414865970612\n",
      "Subject 11, Epoch 262, Loss: 1.7802644968032837, Final Batch Loss: 0.5063315033912659\n",
      "Subject 11, Epoch 263, Loss: 1.4836765974760056, Final Batch Loss: 0.22764934599399567\n",
      "Subject 11, Epoch 264, Loss: 1.3494454398751259, Final Batch Loss: 0.10833198577165604\n",
      "Subject 11, Epoch 265, Loss: 2.0465960800647736, Final Batch Loss: 0.9260342121124268\n",
      "Subject 11, Epoch 266, Loss: 1.4061716198921204, Final Batch Loss: 0.24161046743392944\n",
      "Subject 11, Epoch 267, Loss: 1.5911746621131897, Final Batch Loss: 0.2614997327327728\n",
      "Subject 11, Epoch 268, Loss: 1.706585854291916, Final Batch Loss: 0.40133431553840637\n",
      "Subject 11, Epoch 269, Loss: 1.256368264555931, Final Batch Loss: 0.1427718549966812\n",
      "Subject 11, Epoch 270, Loss: 1.7256006598472595, Final Batch Loss: 0.5829964280128479\n",
      "Subject 11, Epoch 271, Loss: 1.4396676570177078, Final Batch Loss: 0.2271927446126938\n",
      "Subject 11, Epoch 272, Loss: 1.7270326018333435, Final Batch Loss: 0.4225884675979614\n",
      "Subject 11, Epoch 273, Loss: 1.4012620598077774, Final Batch Loss: 0.2169037014245987\n",
      "Subject 11, Epoch 274, Loss: 1.444736123085022, Final Batch Loss: 0.30486565828323364\n",
      "Subject 11, Epoch 275, Loss: 1.5019769817590714, Final Batch Loss: 0.16932614147663116\n",
      "Subject 11, Epoch 276, Loss: 1.505038857460022, Final Batch Loss: 0.33971452713012695\n",
      "Subject 11, Epoch 277, Loss: 1.7450082302093506, Final Batch Loss: 0.5116257071495056\n",
      "Subject 11, Epoch 278, Loss: 1.5324272215366364, Final Batch Loss: 0.26259666681289673\n",
      "Subject 11, Epoch 279, Loss: 1.7249568700790405, Final Batch Loss: 0.597973108291626\n",
      "Subject 11, Epoch 280, Loss: 1.4295878410339355, Final Batch Loss: 0.21702063083648682\n",
      "Subject 11, Epoch 281, Loss: 1.932306170463562, Final Batch Loss: 0.7065129280090332\n",
      "Subject 11, Epoch 282, Loss: 1.3692284524440765, Final Batch Loss: 0.15693315863609314\n",
      "Subject 11, Epoch 283, Loss: 1.6537602245807648, Final Batch Loss: 0.44306063652038574\n",
      "Subject 11, Epoch 284, Loss: 1.9511765241622925, Final Batch Loss: 0.7528107166290283\n",
      "Subject 11, Epoch 285, Loss: 1.3563887178897858, Final Batch Loss: 0.1666972041130066\n",
      "Subject 11, Epoch 286, Loss: 1.5508970022201538, Final Batch Loss: 0.2512491047382355\n",
      "Subject 11, Epoch 287, Loss: 1.3537149876356125, Final Batch Loss: 0.13247735798358917\n",
      "Subject 11, Epoch 288, Loss: 1.3746523186564445, Final Batch Loss: 0.1233569011092186\n",
      "Subject 11, Epoch 289, Loss: 1.5464464128017426, Final Batch Loss: 0.3132808208465576\n",
      "Subject 11, Epoch 290, Loss: 1.3245570361614227, Final Batch Loss: 0.16093692183494568\n",
      "Subject 11, Epoch 291, Loss: 1.4777464270591736, Final Batch Loss: 0.3328186571598053\n",
      "Subject 11, Epoch 292, Loss: 1.4103037118911743, Final Batch Loss: 0.19850879907608032\n",
      "Subject 11, Epoch 293, Loss: 1.8151978850364685, Final Batch Loss: 0.6476828455924988\n",
      "Subject 11, Epoch 294, Loss: 1.5632719993591309, Final Batch Loss: 0.3432011604309082\n",
      "Subject 11, Epoch 295, Loss: 1.3252058923244476, Final Batch Loss: 0.22978293895721436\n",
      "Subject 11, Epoch 296, Loss: 1.2431015707552433, Final Batch Loss: 0.04785344377160072\n",
      "Subject 11, Epoch 297, Loss: 1.5563623011112213, Final Batch Loss: 0.34349456429481506\n",
      "Subject 11, Epoch 298, Loss: 1.4909019768238068, Final Batch Loss: 0.3107142150402069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 299, Loss: 1.363452784717083, Final Batch Loss: 0.09531199187040329\n",
      "Subject 11, Epoch 300, Loss: 1.4390391632914543, Final Batch Loss: 0.1156981810927391\n",
      "Subject 11, Epoch 301, Loss: 1.7941789031028748, Final Batch Loss: 0.6495173573493958\n",
      "Subject 11, Epoch 302, Loss: 1.5133218318223953, Final Batch Loss: 0.24603046476840973\n",
      "Subject 11, Epoch 303, Loss: 1.365896612405777, Final Batch Loss: 0.1681559681892395\n",
      "Subject 11, Epoch 304, Loss: 1.420069932937622, Final Batch Loss: 0.32822561264038086\n",
      "Subject 11, Epoch 305, Loss: 1.335367500782013, Final Batch Loss: 0.24045372009277344\n",
      "Subject 11, Epoch 306, Loss: 1.3468761891126633, Final Batch Loss: 0.17380912601947784\n",
      "Subject 11, Epoch 307, Loss: 1.6100082695484161, Final Batch Loss: 0.3598686158657074\n",
      "Subject 11, Epoch 308, Loss: 1.7673889100551605, Final Batch Loss: 0.5667966604232788\n",
      "Subject 11, Epoch 309, Loss: 1.4158037304878235, Final Batch Loss: 0.31838545203208923\n",
      "Subject 11, Epoch 310, Loss: 1.1692010313272476, Final Batch Loss: 0.06394480168819427\n",
      "Subject 11, Epoch 311, Loss: 1.327809453010559, Final Batch Loss: 0.2271135449409485\n",
      "Subject 11, Epoch 312, Loss: 1.2812805101275444, Final Batch Loss: 0.10585080832242966\n",
      "Subject 11, Epoch 313, Loss: 1.6270228624343872, Final Batch Loss: 0.4639599323272705\n",
      "Subject 11, Epoch 314, Loss: 1.5653045177459717, Final Batch Loss: 0.49637162685394287\n",
      "Subject 11, Epoch 315, Loss: 1.634240210056305, Final Batch Loss: 0.36745965480804443\n",
      "Subject 11, Epoch 316, Loss: 1.5869477093219757, Final Batch Loss: 0.43185022473335266\n",
      "Subject 11, Epoch 317, Loss: 1.53480863571167, Final Batch Loss: 0.4729517996311188\n",
      "Subject 11, Epoch 318, Loss: 1.355496346950531, Final Batch Loss: 0.3684033453464508\n",
      "Subject 11, Epoch 319, Loss: 1.3447303622961044, Final Batch Loss: 0.17987723648548126\n",
      "Subject 11, Epoch 320, Loss: 1.4750047326087952, Final Batch Loss: 0.38590726256370544\n",
      "Subject 11, Epoch 321, Loss: 1.2913701087236404, Final Batch Loss: 0.20121125876903534\n",
      "Subject 11, Epoch 322, Loss: 1.3520025610923767, Final Batch Loss: 0.26122090220451355\n",
      "Subject 11, Epoch 323, Loss: 1.6039775609970093, Final Batch Loss: 0.5081334114074707\n",
      "Subject 11, Epoch 324, Loss: 1.757181316614151, Final Batch Loss: 0.7228301167488098\n",
      "Subject 11, Epoch 325, Loss: 1.5422804951667786, Final Batch Loss: 0.4124045670032501\n",
      "Subject 11, Epoch 326, Loss: 2.2275681495666504, Final Batch Loss: 1.2100316286087036\n",
      "Subject 11, Epoch 327, Loss: 1.0692904312163591, Final Batch Loss: 0.02904948778450489\n",
      "Subject 11, Epoch 328, Loss: 1.2546401470899582, Final Batch Loss: 0.24445568025112152\n",
      "Subject 11, Epoch 329, Loss: 1.292842298746109, Final Batch Loss: 0.3231196403503418\n",
      "Subject 11, Epoch 330, Loss: 1.2121686786413193, Final Batch Loss: 0.17794914543628693\n",
      "Subject 11, Epoch 331, Loss: 1.1687098443508148, Final Batch Loss: 0.1635514795780182\n",
      "Subject 11, Epoch 332, Loss: 1.275787889957428, Final Batch Loss: 0.2516386806964874\n",
      "Subject 11, Epoch 333, Loss: 1.3471980094909668, Final Batch Loss: 0.32247552275657654\n",
      "Subject 11, Epoch 334, Loss: 1.5280772149562836, Final Batch Loss: 0.5006305575370789\n",
      "Subject 11, Epoch 335, Loss: 1.455705463886261, Final Batch Loss: 0.3553570806980133\n",
      "Subject 11, Epoch 336, Loss: 1.5509321093559265, Final Batch Loss: 0.4560210704803467\n",
      "Subject 11, Epoch 337, Loss: 1.3914113938808441, Final Batch Loss: 0.36132845282554626\n",
      "Subject 11, Epoch 338, Loss: 1.8631444573402405, Final Batch Loss: 0.9100649952888489\n",
      "Subject 11, Epoch 339, Loss: 1.2189834713935852, Final Batch Loss: 0.19111579656600952\n",
      "Subject 11, Epoch 340, Loss: 1.535262405872345, Final Batch Loss: 0.4686889350414276\n",
      "Subject 11, Epoch 341, Loss: 1.4491626620292664, Final Batch Loss: 0.334871381521225\n",
      "Subject 11, Epoch 342, Loss: 1.4934625923633575, Final Batch Loss: 0.33322873711586\n",
      "Subject 11, Epoch 343, Loss: 1.1557676941156387, Final Batch Loss: 0.1478600651025772\n",
      "Subject 11, Epoch 344, Loss: 1.5576411187648773, Final Batch Loss: 0.5381782054901123\n",
      "Subject 11, Epoch 345, Loss: 1.3377502858638763, Final Batch Loss: 0.2515528202056885\n",
      "Subject 11, Epoch 346, Loss: 0.9658849313855171, Final Batch Loss: 0.07063017040491104\n",
      "Subject 11, Epoch 347, Loss: 1.3466744720935822, Final Batch Loss: 0.2639414370059967\n",
      "Subject 11, Epoch 348, Loss: 1.2233171314001083, Final Batch Loss: 0.21141786873340607\n",
      "Subject 11, Epoch 349, Loss: 1.0007222779095173, Final Batch Loss: 0.03299720957875252\n",
      "Subject 11, Epoch 350, Loss: 1.28678098320961, Final Batch Loss: 0.34166011214256287\n",
      "Subject 11, Epoch 351, Loss: 1.1760976910591125, Final Batch Loss: 0.21488840878009796\n",
      "Subject 11, Epoch 352, Loss: 1.116713434457779, Final Batch Loss: 0.12638550996780396\n",
      "Subject 11, Epoch 353, Loss: 1.4150209724903107, Final Batch Loss: 0.5086832642555237\n",
      "Subject 11, Epoch 354, Loss: 1.222156971693039, Final Batch Loss: 0.16766977310180664\n",
      "Subject 11, Epoch 355, Loss: 1.2950157523155212, Final Batch Loss: 0.36269113421440125\n",
      "Subject 11, Epoch 356, Loss: 1.499371498823166, Final Batch Loss: 0.4515664875507355\n",
      "Subject 11, Epoch 357, Loss: 1.0768304765224457, Final Batch Loss: 0.09670931100845337\n",
      "Subject 11, Epoch 358, Loss: 1.1440972089767456, Final Batch Loss: 0.2467065006494522\n",
      "Subject 11, Epoch 359, Loss: 1.228684514760971, Final Batch Loss: 0.2725728452205658\n",
      "Subject 11, Epoch 360, Loss: 1.1270952075719833, Final Batch Loss: 0.1643000990152359\n",
      "Subject 11, Epoch 361, Loss: 1.5078009366989136, Final Batch Loss: 0.5831066966056824\n",
      "Subject 11, Epoch 362, Loss: 0.9854624420404434, Final Batch Loss: 0.15530236065387726\n",
      "Subject 11, Epoch 363, Loss: 1.1254390478134155, Final Batch Loss: 0.29823434352874756\n",
      "Subject 11, Epoch 364, Loss: 1.1381563544273376, Final Batch Loss: 0.2523958384990692\n",
      "Subject 11, Epoch 365, Loss: 0.9088390693068504, Final Batch Loss: 0.09419264644384384\n",
      "Subject 11, Epoch 366, Loss: 1.0061479806900024, Final Batch Loss: 0.13608036935329437\n",
      "Subject 11, Epoch 367, Loss: 0.9674580171704292, Final Batch Loss: 0.08734915405511856\n",
      "Subject 11, Epoch 368, Loss: 0.8932202868163586, Final Batch Loss: 0.04797742888331413\n",
      "Subject 11, Epoch 369, Loss: 0.894872859120369, Final Batch Loss: 0.13893304765224457\n",
      "Subject 11, Epoch 370, Loss: 1.5424519777297974, Final Batch Loss: 0.5716149806976318\n",
      "Subject 11, Epoch 371, Loss: 1.1409044116735458, Final Batch Loss: 0.34281012415885925\n",
      "Subject 11, Epoch 372, Loss: 1.0797956883907318, Final Batch Loss: 0.24479793012142181\n",
      "Subject 11, Epoch 373, Loss: 1.3394858241081238, Final Batch Loss: 0.3889385759830475\n",
      "Subject 11, Epoch 374, Loss: 1.5103119015693665, Final Batch Loss: 0.5239036679267883\n",
      "Subject 11, Epoch 375, Loss: 1.1442252546548843, Final Batch Loss: 0.18530939519405365\n",
      "Subject 11, Epoch 376, Loss: 0.9663299471139908, Final Batch Loss: 0.06462214887142181\n",
      "Subject 11, Epoch 377, Loss: 1.2140465080738068, Final Batch Loss: 0.32214635610580444\n",
      "Subject 11, Epoch 378, Loss: 0.9612024277448654, Final Batch Loss: 0.140388622879982\n",
      "Subject 11, Epoch 379, Loss: 1.2937377989292145, Final Batch Loss: 0.38968583941459656\n",
      "Subject 11, Epoch 380, Loss: 1.1296887695789337, Final Batch Loss: 0.2959239184856415\n",
      "Subject 11, Epoch 381, Loss: 0.8631179388612509, Final Batch Loss: 0.0216656681150198\n",
      "Subject 11, Epoch 382, Loss: 1.569880649447441, Final Batch Loss: 0.618433952331543\n",
      "Subject 11, Epoch 383, Loss: 0.9287165477871895, Final Batch Loss: 0.0661730244755745\n",
      "Subject 11, Epoch 384, Loss: 0.8738471791148186, Final Batch Loss: 0.01272457093000412\n",
      "Subject 11, Epoch 385, Loss: 1.1396829336881638, Final Batch Loss: 0.39226970076560974\n",
      "Subject 11, Epoch 386, Loss: 1.2020845711231232, Final Batch Loss: 0.3210895359516144\n",
      "Subject 11, Epoch 387, Loss: 1.0567169040441513, Final Batch Loss: 0.19284696877002716\n",
      "Subject 11, Epoch 388, Loss: 1.5386281609535217, Final Batch Loss: 0.6407748460769653\n",
      "Subject 11, Epoch 389, Loss: 0.8639174178242683, Final Batch Loss: 0.0926755890250206\n",
      "Subject 11, Epoch 390, Loss: 1.2037841826677322, Final Batch Loss: 0.3010805547237396\n",
      "Subject 11, Epoch 391, Loss: 1.1308779567480087, Final Batch Loss: 0.41320160031318665\n",
      "Subject 11, Epoch 392, Loss: 0.9442622810602188, Final Batch Loss: 0.02995862066745758\n",
      "Subject 11, Epoch 393, Loss: 0.9694164842367172, Final Batch Loss: 0.1360982209444046\n",
      "Subject 11, Epoch 394, Loss: 0.9934337474405766, Final Batch Loss: 0.04764828458428383\n",
      "Subject 11, Epoch 395, Loss: 0.8271244466304779, Final Batch Loss: 0.13240472972393036\n",
      "Subject 11, Epoch 396, Loss: 0.8488845080137253, Final Batch Loss: 0.07489137351512909\n",
      "Subject 11, Epoch 397, Loss: 0.8136694133281708, Final Batch Loss: 0.10991767048835754\n",
      "Subject 11, Epoch 398, Loss: 0.8372135628014803, Final Batch Loss: 0.017996596172451973\n",
      "Subject 11, Epoch 399, Loss: 1.0059875398874283, Final Batch Loss: 0.20601117610931396\n",
      "Subject 11, Epoch 400, Loss: 1.0148306638002396, Final Batch Loss: 0.3327919840812683\n",
      "Subject 11, Epoch 401, Loss: 0.8189940452575684, Final Batch Loss: 0.08576846122741699\n",
      "Subject 11, Epoch 402, Loss: 1.1363317370414734, Final Batch Loss: 0.3364435136318207\n",
      "Subject 11, Epoch 403, Loss: 0.8564538955688477, Final Batch Loss: 0.11261415481567383\n",
      "Subject 11, Epoch 404, Loss: 0.8705172911286354, Final Batch Loss: 0.06888792663812637\n",
      "Subject 11, Epoch 405, Loss: 1.064125582575798, Final Batch Loss: 0.35778236389160156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 406, Loss: 1.1782850623130798, Final Batch Loss: 0.4134104549884796\n",
      "Subject 11, Epoch 407, Loss: 0.8793364465236664, Final Batch Loss: 0.21390140056610107\n",
      "Subject 11, Epoch 408, Loss: 0.7098794355988503, Final Batch Loss: 0.025481529533863068\n",
      "Subject 11, Epoch 409, Loss: 1.0541462451219559, Final Batch Loss: 0.3379265069961548\n",
      "Subject 11, Epoch 410, Loss: 1.0780740976333618, Final Batch Loss: 0.2907525300979614\n",
      "Subject 11, Epoch 411, Loss: 0.8194772563874722, Final Batch Loss: 0.04503263160586357\n",
      "Subject 11, Epoch 412, Loss: 1.209277629852295, Final Batch Loss: 0.44520285725593567\n",
      "Subject 11, Epoch 413, Loss: 0.8863179013133049, Final Batch Loss: 0.09868577867746353\n",
      "Subject 11, Epoch 414, Loss: 0.9061890542507172, Final Batch Loss: 0.14432881772518158\n",
      "Subject 11, Epoch 415, Loss: 0.7282170057296753, Final Batch Loss: 0.08177405595779419\n",
      "Subject 11, Epoch 416, Loss: 0.8190611340105534, Final Batch Loss: 0.05898507311940193\n",
      "Subject 11, Epoch 417, Loss: 0.8400215953588486, Final Batch Loss: 0.13870400190353394\n",
      "Subject 11, Epoch 418, Loss: 0.7273073643445969, Final Batch Loss: 0.09344902634620667\n",
      "Subject 11, Epoch 419, Loss: 0.9129533171653748, Final Batch Loss: 0.14130011200904846\n",
      "Subject 11, Epoch 420, Loss: 1.1614255011081696, Final Batch Loss: 0.4672769010066986\n",
      "Subject 11, Epoch 421, Loss: 0.9981775730848312, Final Batch Loss: 0.29516664147377014\n",
      "Subject 11, Epoch 422, Loss: 1.0384348928928375, Final Batch Loss: 0.36891427636146545\n",
      "Subject 11, Epoch 423, Loss: 0.8882615137845278, Final Batch Loss: 0.02970053441822529\n",
      "Subject 11, Epoch 424, Loss: 0.8109117411077023, Final Batch Loss: 0.041086237877607346\n",
      "Subject 11, Epoch 425, Loss: 1.1071528494358063, Final Batch Loss: 0.5059804320335388\n",
      "Subject 11, Epoch 426, Loss: 0.9595875889062881, Final Batch Loss: 0.3137013614177704\n",
      "Subject 11, Epoch 427, Loss: 2.097410574555397, Final Batch Loss: 1.348626971244812\n",
      "Subject 11, Epoch 428, Loss: 0.6408997774124146, Final Batch Loss: 0.0967119038105011\n",
      "Subject 11, Epoch 429, Loss: 0.6468596551567316, Final Batch Loss: 0.016424188390374184\n",
      "Subject 11, Epoch 430, Loss: 0.7356816083192825, Final Batch Loss: 0.06582455337047577\n",
      "Subject 11, Epoch 431, Loss: 1.1297242492437363, Final Batch Loss: 0.33154332637786865\n",
      "Subject 11, Epoch 432, Loss: 0.8057205406948924, Final Batch Loss: 0.008578467182815075\n",
      "Subject 11, Epoch 433, Loss: 0.6691069230437279, Final Batch Loss: 0.07820207625627518\n",
      "Subject 11, Epoch 434, Loss: 0.6424745991826057, Final Batch Loss: 0.07028185576200485\n",
      "Subject 11, Epoch 435, Loss: 0.5867933928966522, Final Batch Loss: 0.04117362201213837\n",
      "Subject 11, Epoch 436, Loss: 0.5589409060776234, Final Batch Loss: 0.04062618687748909\n",
      "Subject 11, Epoch 437, Loss: 0.7360464185476303, Final Batch Loss: 0.14678986370563507\n",
      "Subject 11, Epoch 438, Loss: 1.087274357676506, Final Batch Loss: 0.4392346441745758\n",
      "Subject 11, Epoch 439, Loss: 0.9064626842737198, Final Batch Loss: 0.32453539967536926\n",
      "Subject 11, Epoch 440, Loss: 0.8846385776996613, Final Batch Loss: 0.3099513351917267\n",
      "Subject 11, Epoch 441, Loss: 0.8021165132522583, Final Batch Loss: 0.14006634056568146\n",
      "Subject 11, Epoch 442, Loss: 0.7940631210803986, Final Batch Loss: 0.2606506049633026\n",
      "Subject 11, Epoch 443, Loss: 0.7197351902723312, Final Batch Loss: 0.0921354591846466\n",
      "Subject 11, Epoch 444, Loss: 0.5094804801046848, Final Batch Loss: 0.047502171248197556\n",
      "Subject 11, Epoch 445, Loss: 0.76117392629385, Final Batch Loss: 0.18006499111652374\n",
      "Subject 11, Epoch 446, Loss: 0.626031719148159, Final Batch Loss: 0.11335548013448715\n",
      "Subject 11, Epoch 447, Loss: 0.7970136851072311, Final Batch Loss: 0.23757316172122955\n",
      "Subject 11, Epoch 448, Loss: 0.7133995369076729, Final Batch Loss: 0.10844919830560684\n",
      "Subject 11, Epoch 449, Loss: 1.0669205635786057, Final Batch Loss: 0.4531363248825073\n",
      "Subject 11, Epoch 450, Loss: 0.9891455173492432, Final Batch Loss: 0.3123021125793457\n",
      "Subject 11, Epoch 451, Loss: 0.8942324668169022, Final Batch Loss: 0.15147122740745544\n",
      "Subject 11, Epoch 452, Loss: 0.7310141064226627, Final Batch Loss: 0.048182595521211624\n",
      "Subject 11, Epoch 453, Loss: 0.7092510308139026, Final Batch Loss: 0.005447521340101957\n",
      "Subject 11, Epoch 454, Loss: 1.1482836157083511, Final Batch Loss: 0.5794922709465027\n",
      "Subject 11, Epoch 455, Loss: 0.5025455290451646, Final Batch Loss: 0.012041234411299229\n",
      "Subject 11, Epoch 456, Loss: 0.7308348640799522, Final Batch Loss: 0.11252910643815994\n",
      "Subject 11, Epoch 457, Loss: 0.7456940859556198, Final Batch Loss: 0.17209230363368988\n",
      "Subject 11, Epoch 458, Loss: 0.5604594983160496, Final Batch Loss: 0.037256840616464615\n",
      "Subject 11, Epoch 459, Loss: 0.6262466013431549, Final Batch Loss: 0.1653788536787033\n",
      "Subject 11, Epoch 460, Loss: 0.7996453046798706, Final Batch Loss: 0.33587023615837097\n",
      "Subject 11, Epoch 461, Loss: 0.6246312484145164, Final Batch Loss: 0.10387123376131058\n",
      "Subject 11, Epoch 462, Loss: 0.6136622987687588, Final Batch Loss: 0.0361897312104702\n",
      "Subject 11, Epoch 463, Loss: 0.6633790954947472, Final Batch Loss: 0.09137364476919174\n",
      "Subject 11, Epoch 464, Loss: 0.6790725365281105, Final Batch Loss: 0.07342614978551865\n",
      "Subject 11, Epoch 465, Loss: 0.6657785996794701, Final Batch Loss: 0.06415761262178421\n",
      "Subject 11, Epoch 466, Loss: 1.0177491307258606, Final Batch Loss: 0.5517255663871765\n",
      "Subject 11, Epoch 467, Loss: 1.1467140018939972, Final Batch Loss: 0.6041498780250549\n",
      "Subject 11, Epoch 468, Loss: 0.6166246496140957, Final Batch Loss: 0.061864566057920456\n",
      "Subject 11, Epoch 469, Loss: 0.8089917451143265, Final Batch Loss: 0.1755491942167282\n",
      "Subject 11, Epoch 470, Loss: 0.6748683685436845, Final Batch Loss: 0.010297068394720554\n",
      "Subject 11, Epoch 471, Loss: 1.1563272327184677, Final Batch Loss: 0.7391052842140198\n",
      "Subject 11, Epoch 472, Loss: 0.6481893733143806, Final Batch Loss: 0.0805630013346672\n",
      "Subject 11, Epoch 473, Loss: 0.6479056850075722, Final Batch Loss: 0.0965162143111229\n",
      "Subject 11, Epoch 474, Loss: 0.6410051584243774, Final Batch Loss: 0.125056192278862\n",
      "Subject 11, Epoch 475, Loss: 0.9071457535028458, Final Batch Loss: 0.33871033787727356\n",
      "Subject 11, Epoch 476, Loss: 0.7522697448730469, Final Batch Loss: 0.27275195717811584\n",
      "Subject 11, Epoch 477, Loss: 0.7709838002920151, Final Batch Loss: 0.19922132790088654\n",
      "Subject 11, Epoch 478, Loss: 0.8251775503158569, Final Batch Loss: 0.3659302890300751\n",
      "Subject 11, Epoch 479, Loss: 0.5890917778015137, Final Batch Loss: 0.05319985747337341\n",
      "Subject 11, Epoch 480, Loss: 0.696980893611908, Final Batch Loss: 0.14463908970355988\n",
      "Subject 11, Epoch 481, Loss: 0.5772246606647968, Final Batch Loss: 0.051690515130758286\n",
      "Subject 11, Epoch 482, Loss: 0.5639595873653889, Final Batch Loss: 0.05042681470513344\n",
      "Subject 11, Epoch 483, Loss: 0.7736346423625946, Final Batch Loss: 0.15088902413845062\n",
      "Subject 11, Epoch 484, Loss: 0.46302794432267547, Final Batch Loss: 0.005600777920335531\n",
      "Subject 11, Epoch 485, Loss: 0.5944155659526587, Final Batch Loss: 0.02417934499680996\n",
      "Subject 11, Epoch 486, Loss: 0.6067957542836666, Final Batch Loss: 0.060273509472608566\n",
      "Subject 11, Epoch 487, Loss: 1.0637554228305817, Final Batch Loss: 0.5060190558433533\n",
      "Subject 11, Epoch 488, Loss: 0.8562828823924065, Final Batch Loss: 0.4581669867038727\n",
      "Subject 11, Epoch 489, Loss: 0.5759482607245445, Final Batch Loss: 0.0943576917052269\n",
      "Subject 11, Epoch 490, Loss: 0.9156216681003571, Final Batch Loss: 0.3179861605167389\n",
      "Subject 11, Epoch 491, Loss: 0.6487198323011398, Final Batch Loss: 0.11041375249624252\n",
      "Subject 11, Epoch 492, Loss: 0.720420353114605, Final Batch Loss: 0.2144896239042282\n",
      "Subject 11, Epoch 493, Loss: 0.7834556177258492, Final Batch Loss: 0.27525582909584045\n",
      "Subject 11, Epoch 494, Loss: 0.8165299594402313, Final Batch Loss: 0.3769199848175049\n",
      "Subject 11, Epoch 495, Loss: 0.4928072318434715, Final Batch Loss: 0.09036412090063095\n",
      "Subject 11, Epoch 496, Loss: 0.46806060522794724, Final Batch Loss: 0.036793142557144165\n",
      "Subject 11, Epoch 497, Loss: 0.5807224847376347, Final Batch Loss: 0.05511580780148506\n",
      "Subject 11, Epoch 498, Loss: 0.6059155091643333, Final Batch Loss: 0.070856474339962\n",
      "Subject 11, Epoch 499, Loss: 0.5371346734464169, Final Batch Loss: 0.03798255696892738\n",
      "Subject 11, Epoch 500, Loss: 0.5248638559132814, Final Batch Loss: 0.02873203344643116\n",
      "Subject 11, Epoch 501, Loss: 0.6911113858222961, Final Batch Loss: 0.27633100748062134\n",
      "Subject 11, Epoch 502, Loss: 0.4069115184247494, Final Batch Loss: 0.04404296353459358\n",
      "Subject 11, Epoch 503, Loss: 0.5560310408473015, Final Batch Loss: 0.17655207216739655\n",
      "Subject 11, Epoch 504, Loss: 0.7187315449118614, Final Batch Loss: 0.2907133400440216\n",
      "Subject 11, Epoch 505, Loss: 0.6572036743164062, Final Batch Loss: 0.056780800223350525\n",
      "Subject 11, Epoch 506, Loss: 0.6009369604289532, Final Batch Loss: 0.049700554460287094\n",
      "Subject 11, Epoch 507, Loss: 0.4887137170881033, Final Batch Loss: 0.017737945541739464\n",
      "Subject 11, Epoch 508, Loss: 0.61919966340065, Final Batch Loss: 0.13289082050323486\n",
      "Subject 11, Epoch 509, Loss: 1.0365238338708878, Final Batch Loss: 0.6425544619560242\n",
      "Subject 11, Epoch 510, Loss: 0.6493718549609184, Final Batch Loss: 0.10017029196023941\n",
      "Subject 11, Epoch 511, Loss: 0.7238396927714348, Final Batch Loss: 0.22904567420482635\n",
      "Subject 11, Epoch 512, Loss: 0.44077853113412857, Final Batch Loss: 0.02613641321659088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 513, Loss: 0.8479750752449036, Final Batch Loss: 0.3401374816894531\n",
      "Subject 11, Epoch 514, Loss: 0.8756628409028053, Final Batch Loss: 0.34137603640556335\n",
      "Subject 11, Epoch 515, Loss: 0.5442102774977684, Final Batch Loss: 0.1539839655160904\n",
      "Subject 11, Epoch 516, Loss: 0.6716857329010963, Final Batch Loss: 0.11293260008096695\n",
      "Subject 11, Epoch 517, Loss: 0.5604836493730545, Final Batch Loss: 0.11217409372329712\n",
      "Subject 11, Epoch 518, Loss: 0.4765651635825634, Final Batch Loss: 0.03742456063628197\n",
      "Subject 11, Epoch 519, Loss: 0.49330899119377136, Final Batch Loss: 0.08240659534931183\n",
      "Subject 11, Epoch 520, Loss: 0.530937984585762, Final Batch Loss: 0.10423865914344788\n",
      "Subject 11, Epoch 521, Loss: 0.5453141257166862, Final Batch Loss: 0.11832494288682938\n",
      "Subject 11, Epoch 522, Loss: 0.7831432968378067, Final Batch Loss: 0.301069051027298\n",
      "Subject 11, Epoch 523, Loss: 0.5848499312996864, Final Batch Loss: 0.0989784374833107\n",
      "Subject 11, Epoch 524, Loss: 0.6100560054183006, Final Batch Loss: 0.0767364576458931\n",
      "Subject 11, Epoch 525, Loss: 0.4897716734558344, Final Batch Loss: 0.02092115767300129\n",
      "Subject 11, Epoch 526, Loss: 0.5311648696660995, Final Batch Loss: 0.09419631958007812\n",
      "Subject 11, Epoch 527, Loss: 0.5963580012321472, Final Batch Loss: 0.06958403438329697\n",
      "Subject 11, Epoch 528, Loss: 0.803717628121376, Final Batch Loss: 0.22159510850906372\n",
      "Subject 11, Epoch 529, Loss: 0.38498532958328724, Final Batch Loss: 0.030399614945054054\n",
      "Subject 11, Epoch 530, Loss: 0.38289825804531574, Final Batch Loss: 0.02535516954958439\n",
      "Subject 11, Epoch 531, Loss: 0.5044327974319458, Final Batch Loss: 0.02421935647726059\n",
      "Subject 11, Epoch 532, Loss: 0.4925267919898033, Final Batch Loss: 0.14886288344860077\n",
      "Subject 11, Epoch 533, Loss: 0.516285415738821, Final Batch Loss: 0.11167911440134048\n",
      "Subject 11, Epoch 534, Loss: 0.47214245051145554, Final Batch Loss: 0.011389896273612976\n",
      "Subject 11, Epoch 535, Loss: 0.5577824637293816, Final Batch Loss: 0.17747782170772552\n",
      "Subject 11, Epoch 536, Loss: 0.554840438067913, Final Batch Loss: 0.03443387895822525\n",
      "Subject 11, Epoch 537, Loss: 0.387429503723979, Final Batch Loss: 0.0308592077344656\n",
      "Subject 11, Epoch 538, Loss: 0.6989767700433731, Final Batch Loss: 0.22825568914413452\n",
      "Subject 11, Epoch 539, Loss: 0.5157913938164711, Final Batch Loss: 0.05584007501602173\n",
      "Subject 11, Epoch 540, Loss: 0.4304036684334278, Final Batch Loss: 0.10047163814306259\n",
      "Subject 11, Epoch 541, Loss: 0.4952525794506073, Final Batch Loss: 0.0886240303516388\n",
      "Subject 11, Epoch 542, Loss: 0.36054414231330156, Final Batch Loss: 0.009842229075729847\n",
      "Subject 11, Epoch 543, Loss: 0.41807419806718826, Final Batch Loss: 0.03492973744869232\n",
      "Subject 11, Epoch 544, Loss: 0.37320910207927227, Final Batch Loss: 0.0015714894980192184\n",
      "Subject 11, Epoch 545, Loss: 0.4774399846792221, Final Batch Loss: 0.04178041219711304\n",
      "Subject 11, Epoch 546, Loss: 0.4682573126628995, Final Batch Loss: 0.00855634082108736\n",
      "Subject 11, Epoch 547, Loss: 0.46535084024071693, Final Batch Loss: 0.0591236911714077\n",
      "Subject 11, Epoch 548, Loss: 0.7998414114117622, Final Batch Loss: 0.4751492738723755\n",
      "Subject 11, Epoch 549, Loss: 0.5478406511247158, Final Batch Loss: 0.05684323236346245\n",
      "Subject 11, Epoch 550, Loss: 0.4096509665250778, Final Batch Loss: 0.026651836931705475\n",
      "Subject 11, Epoch 551, Loss: 0.5456241555511951, Final Batch Loss: 0.06050778552889824\n",
      "Subject 11, Epoch 552, Loss: 0.45313380286097527, Final Batch Loss: 0.05073988065123558\n",
      "Subject 11, Epoch 553, Loss: 0.39461383037269115, Final Batch Loss: 0.023294968530535698\n",
      "Subject 11, Epoch 554, Loss: 0.6001311019062996, Final Batch Loss: 0.1766740083694458\n",
      "Subject 11, Epoch 555, Loss: 0.38403771072626114, Final Batch Loss: 0.037651631981134415\n",
      "Subject 11, Epoch 556, Loss: 0.6013297289609909, Final Batch Loss: 0.23261357843875885\n",
      "Subject 11, Epoch 557, Loss: 0.45525892823934555, Final Batch Loss: 0.1134163960814476\n",
      "Subject 11, Epoch 558, Loss: 0.40046337991952896, Final Batch Loss: 0.07269067317247391\n",
      "Subject 11, Epoch 559, Loss: 0.7728047892451286, Final Batch Loss: 0.43672633171081543\n",
      "Subject 11, Epoch 560, Loss: 0.44180057430639863, Final Batch Loss: 0.00768223637714982\n",
      "Subject 11, Epoch 561, Loss: 0.3821060545742512, Final Batch Loss: 0.011744026094675064\n",
      "Subject 11, Epoch 562, Loss: 0.5230363458395004, Final Batch Loss: 0.11191940307617188\n",
      "Subject 11, Epoch 563, Loss: 0.666827917098999, Final Batch Loss: 0.20188529789447784\n",
      "Subject 11, Epoch 564, Loss: 0.4442799841053784, Final Batch Loss: 0.0019121323712170124\n",
      "Subject 11, Epoch 565, Loss: 0.652713730931282, Final Batch Loss: 0.2447051852941513\n",
      "Subject 11, Epoch 566, Loss: 0.6137072294950485, Final Batch Loss: 0.13220350444316864\n",
      "Subject 11, Epoch 567, Loss: 0.579647958278656, Final Batch Loss: 0.18755067884922028\n",
      "Subject 11, Epoch 568, Loss: 0.4063497520983219, Final Batch Loss: 0.08608024567365646\n",
      "Subject 11, Epoch 569, Loss: 0.39856059290468693, Final Batch Loss: 0.02696131356060505\n",
      "Subject 11, Epoch 570, Loss: 0.6923385038971901, Final Batch Loss: 0.24045367538928986\n",
      "Subject 11, Epoch 571, Loss: 0.3934192140586674, Final Batch Loss: 0.004535221029073\n",
      "Subject 11, Epoch 572, Loss: 0.4007497578859329, Final Batch Loss: 0.027998916804790497\n",
      "Subject 11, Epoch 573, Loss: 0.44723841128870845, Final Batch Loss: 0.006037499289959669\n",
      "Subject 11, Epoch 574, Loss: 0.5211846306920052, Final Batch Loss: 0.151825949549675\n",
      "Subject 11, Epoch 575, Loss: 0.5190771445631981, Final Batch Loss: 0.10904344916343689\n",
      "Subject 11, Epoch 576, Loss: 0.31538395676761866, Final Batch Loss: 0.00848857406526804\n",
      "Subject 11, Epoch 577, Loss: 1.052233725786209, Final Batch Loss: 0.7526875138282776\n",
      "Subject 11, Epoch 578, Loss: 0.2930823201313615, Final Batch Loss: 0.014734559692442417\n",
      "Subject 11, Epoch 579, Loss: 0.602024495601654, Final Batch Loss: 0.25400665402412415\n",
      "Subject 11, Epoch 580, Loss: 0.33832846907898784, Final Batch Loss: 0.0008024680428206921\n",
      "Subject 11, Epoch 581, Loss: 0.4197959937155247, Final Batch Loss: 0.0596330352127552\n",
      "Subject 11, Epoch 582, Loss: 0.40718384832143784, Final Batch Loss: 0.06036192178726196\n",
      "Subject 11, Epoch 583, Loss: 1.2718637585639954, Final Batch Loss: 0.9238616824150085\n",
      "Subject 11, Epoch 584, Loss: 0.4707041624933481, Final Batch Loss: 0.02031322754919529\n",
      "Subject 11, Epoch 585, Loss: 0.541633503511548, Final Batch Loss: 0.02405943162739277\n",
      "Subject 11, Epoch 586, Loss: 0.561073325574398, Final Batch Loss: 0.11685097217559814\n",
      "Subject 11, Epoch 587, Loss: 0.4800040463451296, Final Batch Loss: 0.002107214415445924\n",
      "Subject 11, Epoch 588, Loss: 0.4283969420939684, Final Batch Loss: 0.016336435452103615\n",
      "Subject 11, Epoch 589, Loss: 0.5259848888963461, Final Batch Loss: 0.03113619051873684\n",
      "Subject 11, Epoch 590, Loss: 0.3789570741355419, Final Batch Loss: 0.03665583208203316\n",
      "Subject 11, Epoch 591, Loss: 0.4004611661657691, Final Batch Loss: 0.010318971239030361\n",
      "Subject 11, Epoch 592, Loss: 0.6587594076991081, Final Batch Loss: 0.355170339345932\n",
      "Subject 11, Epoch 593, Loss: 0.5099915489554405, Final Batch Loss: 0.1717098206281662\n",
      "Subject 11, Epoch 594, Loss: 0.461589802056551, Final Batch Loss: 0.052299994975328445\n",
      "Subject 11, Epoch 595, Loss: 0.5246551930904388, Final Batch Loss: 0.1321713775396347\n",
      "Subject 11, Epoch 596, Loss: 0.38285903353244066, Final Batch Loss: 0.01393874455243349\n",
      "Subject 11, Epoch 597, Loss: 0.5656324550509453, Final Batch Loss: 0.17918629944324493\n",
      "Subject 11, Epoch 598, Loss: 0.5724936574697495, Final Batch Loss: 0.24402473866939545\n",
      "Subject 11, Epoch 599, Loss: 0.5747910216450691, Final Batch Loss: 0.16692109405994415\n",
      "Subject 11, Epoch 600, Loss: 0.4049649201333523, Final Batch Loss: 0.03412961587309837\n",
      "Subject 11, Epoch 601, Loss: 0.4324714858084917, Final Batch Loss: 0.01864686794579029\n",
      "Subject 11, Epoch 602, Loss: 0.43680224753916264, Final Batch Loss: 0.02882547117769718\n",
      "Subject 11, Epoch 603, Loss: 0.428344601765275, Final Batch Loss: 0.02603469230234623\n",
      "Subject 11, Epoch 604, Loss: 0.3997555822134018, Final Batch Loss: 0.051325008273124695\n",
      "Subject 11, Epoch 605, Loss: 0.333163533359766, Final Batch Loss: 0.03138696774840355\n",
      "Subject 11, Epoch 606, Loss: 0.524169072508812, Final Batch Loss: 0.07889506220817566\n",
      "Subject 11, Epoch 607, Loss: 0.44031018018722534, Final Batch Loss: 0.06549099087715149\n",
      "Subject 11, Epoch 608, Loss: 0.3740208698436618, Final Batch Loss: 0.013927395455539227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 609, Loss: 0.512324258685112, Final Batch Loss: 0.10663189738988876\n",
      "Subject 11, Epoch 610, Loss: 0.403778993524611, Final Batch Loss: 0.00838854257017374\n",
      "Subject 11, Epoch 611, Loss: 0.31135809049010277, Final Batch Loss: 0.017572611570358276\n",
      "Subject 11, Epoch 612, Loss: 0.6195903345942497, Final Batch Loss: 0.21404129266738892\n",
      "Subject 11, Epoch 613, Loss: 0.40555737540125847, Final Batch Loss: 0.03873572126030922\n",
      "Subject 11, Epoch 614, Loss: 0.3232879023998976, Final Batch Loss: 0.0248331930488348\n",
      "Subject 11, Epoch 615, Loss: 0.4240834228694439, Final Batch Loss: 0.05628039315342903\n",
      "Subject 11, Epoch 616, Loss: 0.3785487413406372, Final Batch Loss: 0.052788496017456055\n",
      "Subject 11, Epoch 617, Loss: 0.39372412860393524, Final Batch Loss: 0.05998915433883667\n",
      "Subject 11, Epoch 618, Loss: 0.3359867772087455, Final Batch Loss: 0.01128778513520956\n",
      "Subject 11, Epoch 619, Loss: 0.9978966824710369, Final Batch Loss: 0.7285810112953186\n",
      "Subject 11, Epoch 620, Loss: 0.3017437818925828, Final Batch Loss: 0.0021680358331650496\n",
      "Subject 11, Epoch 621, Loss: 0.27683759294450283, Final Batch Loss: 0.01753639243543148\n",
      "Subject 11, Epoch 622, Loss: 0.43030519038438797, Final Batch Loss: 0.11713109165430069\n",
      "Subject 11, Epoch 623, Loss: 0.375214159488678, Final Batch Loss: 0.09499766677618027\n",
      "Subject 11, Epoch 624, Loss: 0.4063478021416813, Final Batch Loss: 0.0035794584546238184\n",
      "Subject 11, Epoch 625, Loss: 1.3863258957862854, Final Batch Loss: 1.0639575719833374\n",
      "Subject 11, Epoch 626, Loss: 0.5153993852436543, Final Batch Loss: 0.05449938401579857\n",
      "Subject 11, Epoch 627, Loss: 0.5712335631251335, Final Batch Loss: 0.012282125651836395\n",
      "Subject 11, Epoch 628, Loss: 0.6069474630057812, Final Batch Loss: 0.015677880495786667\n",
      "Subject 11, Epoch 629, Loss: 0.5037364102900028, Final Batch Loss: 0.05470782890915871\n",
      "Subject 11, Epoch 630, Loss: 0.5445039942860603, Final Batch Loss: 0.11712246388196945\n",
      "Subject 11, Epoch 631, Loss: 0.7898036167025566, Final Batch Loss: 0.4461153447628021\n",
      "Subject 11, Epoch 632, Loss: 0.4073023581877351, Final Batch Loss: 0.009393737651407719\n",
      "Subject 11, Epoch 633, Loss: 0.5169350802898407, Final Batch Loss: 0.2590439021587372\n",
      "Subject 11, Epoch 634, Loss: 0.6237359791994095, Final Batch Loss: 0.12812550365924835\n",
      "Subject 11, Epoch 635, Loss: 0.5317534245550632, Final Batch Loss: 0.03342461958527565\n",
      "Subject 11, Epoch 636, Loss: 0.3838945347815752, Final Batch Loss: 0.02528536505997181\n",
      "Subject 11, Epoch 637, Loss: 0.5195737555623055, Final Batch Loss: 0.20002292096614838\n",
      "Subject 11, Epoch 638, Loss: 0.4792490750551224, Final Batch Loss: 0.11646140366792679\n",
      "Subject 11, Epoch 639, Loss: 0.4319685846567154, Final Batch Loss: 0.03148810565471649\n",
      "Subject 11, Epoch 640, Loss: 0.3859924894059077, Final Batch Loss: 0.0005192753160372376\n",
      "Subject 11, Epoch 641, Loss: 0.28355018654838204, Final Batch Loss: 0.005254148039966822\n",
      "Subject 11, Epoch 642, Loss: 0.5693454295396805, Final Batch Loss: 0.1891525834798813\n",
      "Subject 11, Epoch 643, Loss: 0.3595103432890028, Final Batch Loss: 0.0025202033575624228\n",
      "Subject 11, Epoch 644, Loss: 0.2966780886054039, Final Batch Loss: 0.05169425904750824\n",
      "Subject 11, Epoch 645, Loss: 0.3534424032550305, Final Batch Loss: 0.002498547313734889\n",
      "Subject 11, Epoch 646, Loss: 0.40515488665550947, Final Batch Loss: 0.0017259372398257256\n",
      "Subject 11, Epoch 647, Loss: 0.3369737546890974, Final Batch Loss: 0.02259143255650997\n",
      "Subject 11, Epoch 648, Loss: 0.3958406187593937, Final Batch Loss: 0.03414839133620262\n",
      "Subject 11, Epoch 649, Loss: 0.28702449053525925, Final Batch Loss: 0.029221177101135254\n",
      "Subject 11, Epoch 650, Loss: 0.347515307366848, Final Batch Loss: 0.03661938011646271\n",
      "Subject 11, Epoch 651, Loss: 0.6553458496928215, Final Batch Loss: 0.3287806510925293\n",
      "Subject 11, Epoch 652, Loss: 0.3291122689843178, Final Batch Loss: 0.03227144479751587\n",
      "Subject 11, Epoch 653, Loss: 0.4982320237904787, Final Batch Loss: 0.025748031213879585\n",
      "Subject 11, Epoch 654, Loss: 0.4543637242168188, Final Batch Loss: 0.00658424012362957\n",
      "Subject 11, Epoch 655, Loss: 0.3609262928366661, Final Batch Loss: 0.0639786496758461\n",
      "Subject 11, Epoch 656, Loss: 0.6587793976068497, Final Batch Loss: 0.32538148760795593\n",
      "Subject 11, Epoch 657, Loss: 0.4089214466512203, Final Batch Loss: 0.07339420169591904\n",
      "Subject 11, Epoch 658, Loss: 0.22278105653822422, Final Batch Loss: 0.006601141765713692\n",
      "Subject 11, Epoch 659, Loss: 0.45894977869465947, Final Batch Loss: 0.007714901585131884\n",
      "Subject 11, Epoch 660, Loss: 0.36519379541277885, Final Batch Loss: 0.012454565614461899\n",
      "Subject 11, Epoch 661, Loss: 0.5460847765207291, Final Batch Loss: 0.236916184425354\n",
      "Subject 11, Epoch 662, Loss: 0.44662804901599884, Final Batch Loss: 0.10616487264633179\n",
      "Subject 11, Epoch 663, Loss: 0.30662090750411153, Final Batch Loss: 0.007162888068705797\n",
      "Subject 11, Epoch 664, Loss: 0.5977576822042465, Final Batch Loss: 0.15884114801883698\n",
      "Subject 11, Epoch 665, Loss: 0.3507528221234679, Final Batch Loss: 0.011029004119336605\n",
      "Subject 11, Epoch 666, Loss: 0.30434674583375454, Final Batch Loss: 0.02250792644917965\n",
      "Subject 11, Epoch 667, Loss: 0.2765906401909888, Final Batch Loss: 0.006611334625631571\n",
      "Subject 11, Epoch 668, Loss: 0.31090155243873596, Final Batch Loss: 0.012891814112663269\n",
      "Subject 11, Epoch 669, Loss: 0.6444889530539513, Final Batch Loss: 0.27805566787719727\n",
      "Subject 11, Epoch 670, Loss: 0.4350939840078354, Final Batch Loss: 0.1094968244433403\n",
      "Subject 11, Epoch 671, Loss: 0.3423618469387293, Final Batch Loss: 0.00405491329729557\n",
      "Subject 11, Epoch 672, Loss: 0.44269970804452896, Final Batch Loss: 0.13063322007656097\n",
      "Subject 11, Epoch 673, Loss: 0.3432235084474087, Final Batch Loss: 0.04397214949131012\n",
      "Subject 11, Epoch 674, Loss: 0.4353262558579445, Final Batch Loss: 0.07294697314500809\n",
      "Subject 11, Epoch 675, Loss: 0.31885455921292305, Final Batch Loss: 0.01696931943297386\n",
      "Subject 11, Epoch 676, Loss: 0.35286203771829605, Final Batch Loss: 0.05130811035633087\n",
      "Subject 11, Epoch 677, Loss: 0.28345177974551916, Final Batch Loss: 0.012979785911738873\n",
      "Subject 11, Epoch 678, Loss: 0.328758105635643, Final Batch Loss: 0.07775453478097916\n",
      "Subject 11, Epoch 679, Loss: 0.32456057518720627, Final Batch Loss: 0.02878940850496292\n",
      "Subject 11, Epoch 680, Loss: 0.33202457055449486, Final Batch Loss: 0.08416569977998734\n",
      "Subject 11, Epoch 681, Loss: 0.37391067668795586, Final Batch Loss: 0.0398750975728035\n",
      "Subject 11, Epoch 682, Loss: 0.31099019944667816, Final Batch Loss: 0.07084503024816513\n",
      "Subject 11, Epoch 683, Loss: 0.29165057837963104, Final Batch Loss: 0.06813449412584305\n",
      "Subject 11, Epoch 684, Loss: 0.3014309916179627, Final Batch Loss: 0.002900614170357585\n",
      "Subject 11, Epoch 685, Loss: 0.7673253007233143, Final Batch Loss: 0.565707266330719\n",
      "Subject 11, Epoch 686, Loss: 0.33549607172608376, Final Batch Loss: 0.04688484966754913\n",
      "Subject 11, Epoch 687, Loss: 0.26531419390812516, Final Batch Loss: 0.0014722482301294804\n",
      "Subject 11, Epoch 688, Loss: 0.37500835582613945, Final Batch Loss: 0.02618926763534546\n",
      "Subject 11, Epoch 689, Loss: 0.22966675832867622, Final Batch Loss: 0.01658739522099495\n",
      "Subject 11, Epoch 690, Loss: 0.30713873729109764, Final Batch Loss: 0.08597683906555176\n",
      "Subject 11, Epoch 691, Loss: 0.3512516529299319, Final Batch Loss: 0.00491136172786355\n",
      "Subject 11, Epoch 692, Loss: 0.5781291276216507, Final Batch Loss: 0.2700295150279999\n",
      "Subject 11, Epoch 693, Loss: 0.44323479384183884, Final Batch Loss: 0.183545783162117\n",
      "Subject 11, Epoch 694, Loss: 0.2836589999496937, Final Batch Loss: 0.03063960373401642\n",
      "Subject 11, Epoch 695, Loss: 0.22531518526375294, Final Batch Loss: 0.016776947304606438\n",
      "Subject 11, Epoch 696, Loss: 0.31578133115544915, Final Batch Loss: 0.005088763777166605\n",
      "Subject 11, Epoch 697, Loss: 0.8507158234715462, Final Batch Loss: 0.5146558880805969\n",
      "Subject 11, Epoch 698, Loss: 0.28918249905109406, Final Batch Loss: 0.02415306121110916\n",
      "Subject 11, Epoch 699, Loss: 0.36051627714186907, Final Batch Loss: 0.01260520238429308\n",
      "Subject 11, Epoch 700, Loss: 0.5756334866164252, Final Batch Loss: 0.0005781705258414149\n",
      "Subject 11, Epoch 701, Loss: 0.5020028501749039, Final Batch Loss: 0.05342218279838562\n",
      "Subject 11, Epoch 702, Loss: 0.8351058289408684, Final Batch Loss: 0.5073694586753845\n",
      "Subject 11, Epoch 703, Loss: 0.27212250884622335, Final Batch Loss: 0.010936341248452663\n",
      "Subject 11, Epoch 704, Loss: 0.4919857494533062, Final Batch Loss: 0.20038019120693207\n",
      "Subject 11, Epoch 705, Loss: 0.36037818482145667, Final Batch Loss: 0.006770235951989889\n",
      "Subject 11, Epoch 706, Loss: 0.3998979441821575, Final Batch Loss: 0.04415183141827583\n",
      "Subject 11, Epoch 707, Loss: 0.4232275076210499, Final Batch Loss: 0.18574486672878265\n",
      "Subject 11, Epoch 708, Loss: 1.3041925467550755, Final Batch Loss: 0.9292187094688416\n",
      "Subject 11, Epoch 709, Loss: 0.35788440331816673, Final Batch Loss: 0.031671155244112015\n",
      "Subject 11, Epoch 710, Loss: 0.3559066504240036, Final Batch Loss: 0.11253581196069717\n",
      "Subject 11, Epoch 711, Loss: 0.34858505707234144, Final Batch Loss: 0.011767338030040264\n",
      "Subject 11, Epoch 712, Loss: 0.3759772162884474, Final Batch Loss: 0.029948903247714043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 713, Loss: 0.35751352459192276, Final Batch Loss: 0.04299028217792511\n",
      "Subject 11, Epoch 714, Loss: 0.28459877893328667, Final Batch Loss: 0.017336439341306686\n",
      "Subject 11, Epoch 715, Loss: 0.299092351924628, Final Batch Loss: 0.006543430965393782\n",
      "Subject 11, Epoch 716, Loss: 0.3532867244211957, Final Batch Loss: 0.0012956446735188365\n",
      "Subject 11, Epoch 717, Loss: 0.9314884096384048, Final Batch Loss: 0.6193938255310059\n",
      "Subject 11, Epoch 718, Loss: 0.3048267720732838, Final Batch Loss: 0.003757892409339547\n",
      "Subject 11, Epoch 719, Loss: 0.35668162629008293, Final Batch Loss: 0.07317057996988297\n",
      "Subject 11, Epoch 720, Loss: 0.30735177360475063, Final Batch Loss: 0.015905769541859627\n",
      "Subject 11, Epoch 721, Loss: 0.295371126383543, Final Batch Loss: 0.04363758862018585\n",
      "Subject 11, Epoch 722, Loss: 0.28626400232315063, Final Batch Loss: 0.018174991011619568\n",
      "Subject 11, Epoch 723, Loss: 0.41828468441963196, Final Batch Loss: 0.14759832620620728\n",
      "Subject 11, Epoch 724, Loss: 0.34045227244496346, Final Batch Loss: 0.03078804910182953\n",
      "Subject 11, Epoch 725, Loss: 0.3639499694108963, Final Batch Loss: 0.1297057420015335\n",
      "Subject 11, Epoch 726, Loss: 0.30653298273682594, Final Batch Loss: 0.05389917269349098\n",
      "Subject 11, Epoch 727, Loss: 0.4277471750974655, Final Batch Loss: 0.0894676148891449\n",
      "Subject 11, Epoch 728, Loss: 0.2338208705186844, Final Batch Loss: 0.015445657074451447\n",
      "Subject 11, Epoch 729, Loss: 0.296517051756382, Final Batch Loss: 0.013910003006458282\n",
      "Subject 11, Epoch 730, Loss: 0.2611542325466871, Final Batch Loss: 0.023595383390784264\n",
      "Subject 11, Epoch 731, Loss: 0.21641784999519587, Final Batch Loss: 0.0013505266979336739\n",
      "Subject 11, Epoch 732, Loss: 0.7192435972392559, Final Batch Loss: 0.3788825571537018\n",
      "Subject 11, Epoch 733, Loss: 0.4041399471461773, Final Batch Loss: 0.04970283806324005\n",
      "Subject 11, Epoch 734, Loss: 0.26871119253337383, Final Batch Loss: 0.022289777174592018\n",
      "Subject 11, Epoch 735, Loss: 0.2596228513866663, Final Batch Loss: 0.009054543450474739\n",
      "Subject 11, Epoch 736, Loss: 0.3087860234081745, Final Batch Loss: 0.07295597344636917\n",
      "Subject 11, Epoch 737, Loss: 0.2639177297241986, Final Batch Loss: 0.006775007117539644\n",
      "Subject 11, Epoch 738, Loss: 0.27578714303672314, Final Batch Loss: 0.006577102467417717\n",
      "Subject 11, Epoch 739, Loss: 0.28579625114798546, Final Batch Loss: 0.05795443058013916\n",
      "Subject 11, Epoch 740, Loss: 0.48585178703069687, Final Batch Loss: 0.23316925764083862\n",
      "Subject 11, Epoch 741, Loss: 0.6831149570643902, Final Batch Loss: 0.4374519884586334\n",
      "Subject 11, Epoch 742, Loss: 0.2737966161221266, Final Batch Loss: 0.019911019131541252\n",
      "Subject 11, Epoch 743, Loss: 0.31573448795825243, Final Batch Loss: 0.01375302392989397\n",
      "Subject 11, Epoch 744, Loss: 0.26162824407219887, Final Batch Loss: 0.047882940620183945\n",
      "Subject 11, Epoch 745, Loss: 0.3138076215982437, Final Batch Loss: 0.055093780159950256\n",
      "Subject 11, Epoch 746, Loss: 0.3364919573068619, Final Batch Loss: 0.06932055950164795\n",
      "Subject 11, Epoch 747, Loss: 0.5318258032202721, Final Batch Loss: 0.20464403927326202\n",
      "Subject 11, Epoch 748, Loss: 0.2686142139136791, Final Batch Loss: 0.02658683806657791\n",
      "Subject 11, Epoch 749, Loss: 0.191776966676116, Final Batch Loss: 0.017336582764983177\n",
      "Subject 11, Epoch 750, Loss: 0.3643752932548523, Final Batch Loss: 0.10410654544830322\n",
      "Subject 11, Epoch 751, Loss: 0.23486769339069724, Final Batch Loss: 0.005488043185323477\n",
      "Subject 11, Epoch 752, Loss: 0.345231506973505, Final Batch Loss: 0.11851867288351059\n",
      "Subject 11, Epoch 753, Loss: 0.3402024284005165, Final Batch Loss: 0.091572605073452\n",
      "Subject 11, Epoch 754, Loss: 0.42271605134010315, Final Batch Loss: 0.14831912517547607\n",
      "Subject 11, Epoch 755, Loss: 0.19541891035623848, Final Batch Loss: 0.0025519554037600756\n",
      "Subject 11, Epoch 756, Loss: 0.25574716250412166, Final Batch Loss: 0.0029180317651480436\n",
      "Subject 11, Epoch 757, Loss: 0.2589592421427369, Final Batch Loss: 0.008543905802071095\n",
      "Subject 11, Epoch 758, Loss: 0.3640706427395344, Final Batch Loss: 0.1144285574555397\n",
      "Subject 11, Epoch 759, Loss: 0.23407103028148413, Final Batch Loss: 0.010828564874827862\n",
      "Subject 11, Epoch 760, Loss: 0.28859823383390903, Final Batch Loss: 0.020061681047081947\n",
      "Subject 11, Epoch 761, Loss: 0.21117210388183594, Final Batch Loss: 0.010229267179965973\n",
      "Subject 11, Epoch 762, Loss: 0.3114598346874118, Final Batch Loss: 0.014683905057609081\n",
      "Subject 11, Epoch 763, Loss: 0.24750810279510915, Final Batch Loss: 0.001191416522487998\n",
      "Subject 11, Epoch 764, Loss: 0.2269767684629187, Final Batch Loss: 0.0017078755190595984\n",
      "Subject 11, Epoch 765, Loss: 0.19781660777516663, Final Batch Loss: 0.0029211838264018297\n",
      "Subject 11, Epoch 766, Loss: 0.3074150048196316, Final Batch Loss: 0.07781540602445602\n",
      "Subject 11, Epoch 767, Loss: 0.2014138549566269, Final Batch Loss: 0.013846397399902344\n",
      "Subject 11, Epoch 768, Loss: 0.25658233277499676, Final Batch Loss: 0.02616996131837368\n",
      "Subject 11, Epoch 769, Loss: 0.21156367752701044, Final Batch Loss: 0.012883019633591175\n",
      "Subject 11, Epoch 770, Loss: 0.28564998880028725, Final Batch Loss: 0.0342814140021801\n",
      "Subject 11, Epoch 771, Loss: 0.22682193998480216, Final Batch Loss: 0.0005086339660920203\n",
      "Subject 11, Epoch 772, Loss: 0.20983427204191685, Final Batch Loss: 0.029499785974621773\n",
      "Subject 11, Epoch 773, Loss: 0.3906297981739044, Final Batch Loss: 0.14872226119041443\n",
      "Subject 11, Epoch 774, Loss: 0.7058968842029572, Final Batch Loss: 0.4753408432006836\n",
      "Subject 11, Epoch 775, Loss: 0.36074023076798767, Final Batch Loss: 0.00152418517973274\n",
      "Subject 11, Epoch 776, Loss: 0.6326759234070778, Final Batch Loss: 0.19199274480342865\n",
      "Subject 11, Epoch 777, Loss: 1.0801637321710587, Final Batch Loss: 0.6954159736633301\n",
      "Subject 11, Epoch 778, Loss: 0.24194224923849106, Final Batch Loss: 0.016217123717069626\n",
      "Subject 11, Epoch 779, Loss: 0.33515230054035783, Final Batch Loss: 0.004042746964842081\n",
      "Subject 11, Epoch 780, Loss: 0.257163601112552, Final Batch Loss: 0.0018399005057290196\n",
      "Subject 11, Epoch 781, Loss: 0.37373531237244606, Final Batch Loss: 0.03282250091433525\n",
      "Subject 11, Epoch 782, Loss: 0.49721553176641464, Final Batch Loss: 0.14914782345294952\n",
      "Subject 11, Epoch 783, Loss: 0.2933094762265682, Final Batch Loss: 0.05986950919032097\n",
      "Subject 11, Epoch 784, Loss: 0.2871211961610243, Final Batch Loss: 0.0016628586454316974\n",
      "Subject 11, Epoch 785, Loss: 0.20234668953344226, Final Batch Loss: 0.00235276622697711\n",
      "Subject 11, Epoch 786, Loss: 0.46593133732676506, Final Batch Loss: 0.25071224570274353\n",
      "Subject 11, Epoch 787, Loss: 0.29827623534947634, Final Batch Loss: 0.01271106768399477\n",
      "Subject 11, Epoch 788, Loss: 0.27619629725813866, Final Batch Loss: 0.01024232804775238\n",
      "Subject 11, Epoch 789, Loss: 0.3902016803622246, Final Batch Loss: 0.01041635125875473\n",
      "Subject 11, Epoch 790, Loss: 0.38554342836141586, Final Batch Loss: 0.04084308072924614\n",
      "Subject 11, Epoch 791, Loss: 0.3203124366700649, Final Batch Loss: 0.04891981557011604\n",
      "Subject 11, Epoch 792, Loss: 0.3370371535420418, Final Batch Loss: 0.03614812344312668\n",
      "Subject 11, Epoch 793, Loss: 0.24497944675385952, Final Batch Loss: 0.03122865967452526\n",
      "Subject 11, Epoch 794, Loss: 0.3276410698890686, Final Batch Loss: 0.0798366516828537\n",
      "Subject 11, Epoch 795, Loss: 0.26033419370651245, Final Batch Loss: 0.02899007499217987\n",
      "Subject 11, Epoch 796, Loss: 0.3586175888776779, Final Batch Loss: 0.08237949013710022\n",
      "Subject 11, Epoch 797, Loss: 0.1944125237641856, Final Batch Loss: 0.0013685430167242885\n",
      "Subject 11, Epoch 798, Loss: 0.2688942812383175, Final Batch Loss: 0.03799472376704216\n",
      "Subject 11, Epoch 799, Loss: 0.2945626489818096, Final Batch Loss: 0.04593677446246147\n",
      "Subject 11, Epoch 800, Loss: 0.2834685370326042, Final Batch Loss: 0.040144987404346466\n",
      "Subject 11, Epoch 801, Loss: 0.2126122685149312, Final Batch Loss: 0.014501121826469898\n",
      "Subject 11, Epoch 802, Loss: 0.24868320021778345, Final Batch Loss: 0.0027280161157250404\n",
      "Subject 11, Epoch 803, Loss: 0.24644730240106583, Final Batch Loss: 0.04331989586353302\n",
      "Subject 11, Epoch 804, Loss: 0.20407245168462396, Final Batch Loss: 0.0075027369894087315\n",
      "Subject 11, Epoch 805, Loss: 0.2849476831033826, Final Batch Loss: 0.0024561556056141853\n",
      "Subject 11, Epoch 806, Loss: 0.2360198237001896, Final Batch Loss: 0.002715114504098892\n",
      "Subject 11, Epoch 807, Loss: 0.2289960104972124, Final Batch Loss: 0.02635340206325054\n",
      "Subject 11, Epoch 808, Loss: 0.24107564985752106, Final Batch Loss: 0.03372173011302948\n",
      "Subject 11, Epoch 809, Loss: 0.1795141166076064, Final Batch Loss: 0.008408247493207455\n",
      "Subject 11, Epoch 810, Loss: 0.32674213871359825, Final Batch Loss: 0.03482028469443321\n",
      "Subject 11, Epoch 811, Loss: 0.21263281628489494, Final Batch Loss: 0.012386266142129898\n",
      "Subject 11, Epoch 812, Loss: 0.20596261275932193, Final Batch Loss: 0.007433771621435881\n",
      "Subject 11, Epoch 813, Loss: 0.30307798460125923, Final Batch Loss: 0.13927143812179565\n",
      "Subject 11, Epoch 814, Loss: 0.2440732754766941, Final Batch Loss: 0.048416778445243835\n",
      "Subject 11, Epoch 815, Loss: 0.38388656452298164, Final Batch Loss: 0.13880623877048492\n",
      "Subject 11, Epoch 816, Loss: 0.2534495033323765, Final Batch Loss: 0.03230802342295647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 817, Loss: 0.2181584034115076, Final Batch Loss: 0.024515116587281227\n",
      "Subject 11, Epoch 818, Loss: 0.1708559850230813, Final Batch Loss: 0.014575588516891003\n",
      "Subject 11, Epoch 819, Loss: 0.2601994890719652, Final Batch Loss: 0.022915171459317207\n",
      "Subject 11, Epoch 820, Loss: 0.19794110395014286, Final Batch Loss: 0.010845128446817398\n",
      "Subject 11, Epoch 821, Loss: 0.2083374238354736, Final Batch Loss: 9.301256068283692e-05\n",
      "Subject 11, Epoch 822, Loss: 0.33278634026646614, Final Batch Loss: 0.157920241355896\n",
      "Subject 11, Epoch 823, Loss: 0.26117947325110435, Final Batch Loss: 0.03284356743097305\n",
      "Subject 11, Epoch 824, Loss: 0.2269534757360816, Final Batch Loss: 0.008532523177564144\n",
      "Subject 11, Epoch 825, Loss: 0.2951009087264538, Final Batch Loss: 0.009596385061740875\n",
      "Subject 11, Epoch 826, Loss: 0.22792189905885607, Final Batch Loss: 0.0016304549062624574\n",
      "Subject 11, Epoch 827, Loss: 0.21147907245904207, Final Batch Loss: 0.010452617891132832\n",
      "Subject 11, Epoch 828, Loss: 0.2871679626405239, Final Batch Loss: 0.03204194828867912\n",
      "Subject 11, Epoch 829, Loss: 0.2745360853150487, Final Batch Loss: 0.014191982336342335\n",
      "Subject 11, Epoch 830, Loss: 0.24980171976494603, Final Batch Loss: 0.000286633352516219\n",
      "Subject 11, Epoch 831, Loss: 0.31175008055288345, Final Batch Loss: 0.0014375486643984914\n",
      "Subject 11, Epoch 832, Loss: 0.18664300069212914, Final Batch Loss: 0.040363699197769165\n",
      "Subject 11, Epoch 833, Loss: 0.23562015290372074, Final Batch Loss: 0.002236318541690707\n",
      "Subject 11, Epoch 834, Loss: 0.2971666045486927, Final Batch Loss: 0.10683536529541016\n",
      "Subject 11, Epoch 835, Loss: 0.2502838373184204, Final Batch Loss: 0.012299943715333939\n",
      "Subject 11, Epoch 836, Loss: 0.22919106669723988, Final Batch Loss: 0.07554007321596146\n",
      "Subject 11, Epoch 837, Loss: 0.3047022148966789, Final Batch Loss: 0.10586079210042953\n",
      "Subject 11, Epoch 838, Loss: 0.22738857194781303, Final Batch Loss: 0.011626370251178741\n",
      "Subject 11, Epoch 839, Loss: 0.19283558055758476, Final Batch Loss: 0.04527226462960243\n",
      "Subject 11, Epoch 840, Loss: 0.2272501327097416, Final Batch Loss: 0.04628976061940193\n",
      "Subject 11, Epoch 841, Loss: 0.3449295163154602, Final Batch Loss: 0.10252687335014343\n",
      "Subject 11, Epoch 842, Loss: 0.22833834122866392, Final Batch Loss: 0.011198303662240505\n",
      "Subject 11, Epoch 843, Loss: 0.22528058663010597, Final Batch Loss: 0.019905682653188705\n",
      "Subject 11, Epoch 844, Loss: 0.2236916907131672, Final Batch Loss: 0.00073951855301857\n",
      "Subject 11, Epoch 845, Loss: 0.21791214030236006, Final Batch Loss: 0.0146187050268054\n",
      "Subject 11, Epoch 846, Loss: 0.662768267095089, Final Batch Loss: 0.47991135716438293\n",
      "Subject 11, Epoch 847, Loss: 0.5962998643517494, Final Batch Loss: 0.3858211040496826\n",
      "Subject 11, Epoch 848, Loss: 0.1871075015515089, Final Batch Loss: 0.006505301222205162\n",
      "Subject 11, Epoch 849, Loss: 0.21593021671287715, Final Batch Loss: 0.003760427935048938\n",
      "Subject 11, Epoch 850, Loss: 0.32495929673314095, Final Batch Loss: 0.00786130502820015\n",
      "Subject 11, Epoch 851, Loss: 0.28968654200434685, Final Batch Loss: 0.05331050232052803\n",
      "Subject 11, Epoch 852, Loss: 0.29423169419169426, Final Batch Loss: 0.018066279590129852\n",
      "Subject 11, Epoch 853, Loss: 0.1683892970904708, Final Batch Loss: 0.001958589069545269\n",
      "Subject 11, Epoch 854, Loss: 0.17434883129317313, Final Batch Loss: 0.0017958098324015737\n",
      "Subject 11, Epoch 855, Loss: 0.26075368747115135, Final Batch Loss: 0.04219986870884895\n",
      "Subject 11, Epoch 856, Loss: 0.3485332317650318, Final Batch Loss: 0.10333370417356491\n",
      "Subject 11, Epoch 857, Loss: 0.3338359408080578, Final Batch Loss: 0.07528168708086014\n",
      "Subject 11, Epoch 858, Loss: 0.32783398032188416, Final Batch Loss: 0.10828766226768494\n",
      "Subject 11, Epoch 859, Loss: 0.20411840546876192, Final Batch Loss: 0.004681362770497799\n",
      "Subject 11, Epoch 860, Loss: 0.2783531975001097, Final Batch Loss: 0.0053506772965192795\n",
      "Subject 11, Epoch 861, Loss: 0.22037978656589985, Final Batch Loss: 0.01623980514705181\n",
      "Subject 11, Epoch 862, Loss: 0.24328010366298258, Final Batch Loss: 0.003244376042857766\n",
      "Subject 11, Epoch 863, Loss: 0.2653488260693848, Final Batch Loss: 0.001007255632430315\n",
      "Subject 11, Epoch 864, Loss: 0.19539924629498273, Final Batch Loss: 0.0012191060231998563\n",
      "Subject 11, Epoch 865, Loss: 0.3003822285681963, Final Batch Loss: 0.028589798137545586\n",
      "Subject 11, Epoch 866, Loss: 0.2123170078266412, Final Batch Loss: 0.0030483261216431856\n",
      "Subject 11, Epoch 867, Loss: 0.20087425224483013, Final Batch Loss: 0.0014567691832780838\n",
      "Subject 11, Epoch 868, Loss: 0.2113458402454853, Final Batch Loss: 0.033406954258680344\n",
      "Subject 11, Epoch 869, Loss: 0.2519565839320421, Final Batch Loss: 0.03342413529753685\n",
      "Subject 11, Epoch 870, Loss: 0.24231443600729108, Final Batch Loss: 0.006148379761725664\n",
      "Subject 11, Epoch 871, Loss: 1.1253136284649372, Final Batch Loss: 0.9402592182159424\n",
      "Subject 11, Epoch 872, Loss: 0.2746494619641453, Final Batch Loss: 0.0031895360443741083\n",
      "Subject 11, Epoch 873, Loss: 0.5574428737163544, Final Batch Loss: 0.14635200798511505\n",
      "Subject 11, Epoch 874, Loss: 0.4135278968606144, Final Batch Loss: 0.0032279847655445337\n",
      "Subject 11, Epoch 875, Loss: 0.37353271059691906, Final Batch Loss: 0.03054020367562771\n",
      "Subject 11, Epoch 876, Loss: 0.2571455780416727, Final Batch Loss: 0.018543658778071404\n",
      "Subject 11, Epoch 877, Loss: 0.33142825588583946, Final Batch Loss: 0.03387480974197388\n",
      "Subject 11, Epoch 878, Loss: 0.2530418559908867, Final Batch Loss: 0.013428688049316406\n",
      "Subject 11, Epoch 879, Loss: 0.255459257401526, Final Batch Loss: 0.0025657126680016518\n",
      "Subject 11, Epoch 880, Loss: 0.6439527571201324, Final Batch Loss: 0.33767297863960266\n",
      "Subject 11, Epoch 881, Loss: 0.18356626015156507, Final Batch Loss: 0.03366539627313614\n",
      "Subject 11, Epoch 882, Loss: 0.4363442175090313, Final Batch Loss: 0.1685313731431961\n",
      "Subject 11, Epoch 883, Loss: 0.27596355974674225, Final Batch Loss: 0.0812401995062828\n",
      "Subject 11, Epoch 884, Loss: 0.28859690576791763, Final Batch Loss: 0.04886702075600624\n",
      "Subject 11, Epoch 885, Loss: 0.38225967809557915, Final Batch Loss: 0.22391285002231598\n",
      "Subject 11, Epoch 886, Loss: 0.19508214824600145, Final Batch Loss: 0.0008554064552299678\n",
      "Subject 11, Epoch 887, Loss: 0.31778113543987274, Final Batch Loss: 0.15869773924350739\n",
      "Subject 11, Epoch 888, Loss: 0.2338755689561367, Final Batch Loss: 0.06488358229398727\n",
      "Subject 11, Epoch 889, Loss: 0.19352055603667395, Final Batch Loss: 0.00011628011270659044\n",
      "Subject 11, Epoch 890, Loss: 0.23707970418035984, Final Batch Loss: 0.009978020563721657\n",
      "Subject 11, Epoch 891, Loss: 0.29982326296158135, Final Batch Loss: 0.002978117438033223\n",
      "Subject 11, Epoch 892, Loss: 0.3481209743767977, Final Batch Loss: 0.12829549610614777\n",
      "Subject 11, Epoch 893, Loss: 0.22175259934738278, Final Batch Loss: 0.00254327105358243\n",
      "Subject 11, Epoch 894, Loss: 0.19616783340461552, Final Batch Loss: 0.003629162209108472\n",
      "Subject 11, Epoch 895, Loss: 0.2814799528568983, Final Batch Loss: 0.022416064515709877\n",
      "Subject 11, Epoch 896, Loss: 0.2811601497232914, Final Batch Loss: 0.044767607003450394\n",
      "Subject 11, Epoch 897, Loss: 0.21489130146801472, Final Batch Loss: 0.024220967665314674\n",
      "Subject 11, Epoch 898, Loss: 0.16413786634802818, Final Batch Loss: 0.027004459872841835\n",
      "Subject 11, Epoch 899, Loss: 0.23747767880558968, Final Batch Loss: 0.03887834772467613\n",
      "Subject 11, Epoch 900, Loss: 0.18476050719618797, Final Batch Loss: 0.04616008326411247\n",
      "Subject 11, Epoch 901, Loss: 0.47377319261431694, Final Batch Loss: 0.2650810778141022\n",
      "Subject 11, Epoch 902, Loss: 0.26969276973977685, Final Batch Loss: 0.005654355976730585\n",
      "Subject 11, Epoch 903, Loss: 0.15135374246165156, Final Batch Loss: 0.0014728154055774212\n",
      "Subject 11, Epoch 904, Loss: 0.22921313252300024, Final Batch Loss: 0.009126449935138226\n",
      "Subject 11, Epoch 905, Loss: 0.27630574628710747, Final Batch Loss: 0.07262732088565826\n",
      "Subject 11, Epoch 906, Loss: 0.22071082377806306, Final Batch Loss: 0.005365359131246805\n",
      "Subject 11, Epoch 907, Loss: 0.12199795339256525, Final Batch Loss: 0.013172863982617855\n",
      "Subject 11, Epoch 908, Loss: 0.19027454941533506, Final Batch Loss: 0.0030950529035180807\n",
      "Subject 11, Epoch 909, Loss: 0.20006171124987304, Final Batch Loss: 0.001070140628144145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11, Epoch 910, Loss: 0.14936024323105812, Final Batch Loss: 0.008813221007585526\n",
      "Subject 11, Epoch 911, Loss: 0.2531935634324327, Final Batch Loss: 0.0016518669435754418\n",
      "Subject 11, Epoch 912, Loss: 0.22464735433459282, Final Batch Loss: 0.01772170327603817\n",
      "Subject 11, Epoch 913, Loss: 0.1868044501170516, Final Batch Loss: 0.005013062618672848\n",
      "Subject 11, Epoch 914, Loss: 0.3151322416961193, Final Batch Loss: 0.1476687341928482\n",
      "Subject 11, Epoch 915, Loss: 0.5401976518332958, Final Batch Loss: 0.4235396683216095\n",
      "Subject 11, Epoch 916, Loss: 0.1885053738951683, Final Batch Loss: 0.026263363659381866\n",
      "Subject 11, Epoch 917, Loss: 0.2536422163248062, Final Batch Loss: 0.06789173930883408\n",
      "Subject 11, Epoch 918, Loss: 0.2347887372598052, Final Batch Loss: 0.012648220174014568\n",
      "Subject 11, Epoch 919, Loss: 0.2043660522904247, Final Batch Loss: 0.0024825239088386297\n",
      "Subject 11, Epoch 920, Loss: 0.21705833077430725, Final Batch Loss: 0.005304330959916115\n",
      "Subject 11, Epoch 921, Loss: 0.12965189924580045, Final Batch Loss: 0.0003169791598338634\n",
      "Subject 11, Epoch 922, Loss: 0.3309576027095318, Final Batch Loss: 0.07650070637464523\n",
      "Subject 11, Epoch 923, Loss: 0.15673136804252863, Final Batch Loss: 0.011254807002842426\n",
      "Subject 11, Epoch 924, Loss: 0.1663469523191452, Final Batch Loss: 0.011298634111881256\n",
      "Subject 11, Epoch 925, Loss: 0.20370314922183752, Final Batch Loss: 0.002668219618499279\n",
      "Subject 11, Epoch 926, Loss: 0.20446636993438005, Final Batch Loss: 0.011585150845348835\n",
      "Subject 11, Epoch 927, Loss: 0.22593771293759346, Final Batch Loss: 0.039739180356264114\n",
      "Subject 11, Epoch 928, Loss: 0.1506763813085854, Final Batch Loss: 0.007761400658637285\n",
      "Subject 11, Epoch 929, Loss: 0.19328881800174713, Final Batch Loss: 0.012164805084466934\n",
      "Subject 11, Epoch 930, Loss: 0.15585656027542427, Final Batch Loss: 0.0006758508388884366\n",
      "Subject 11, Epoch 931, Loss: 0.14057946298271418, Final Batch Loss: 0.010299007408320904\n",
      "Subject 11, Epoch 932, Loss: 0.1826015659607947, Final Batch Loss: 0.005044654477387667\n",
      "Subject 11, Epoch 933, Loss: 0.2185124158859253, Final Batch Loss: 0.005936797708272934\n",
      "Subject 11, Epoch 934, Loss: 0.1234804957639426, Final Batch Loss: 0.0029421381186693907\n",
      "Subject 11, Epoch 935, Loss: 0.11427263059886172, Final Batch Loss: 0.0009651955333538353\n",
      "Subject 11, Epoch 936, Loss: 0.24822471663355827, Final Batch Loss: 0.054170068353414536\n",
      "Subject 11, Epoch 937, Loss: 0.16779730841517448, Final Batch Loss: 0.03357170149683952\n",
      "Subject 11, Epoch 938, Loss: 0.13838032679632306, Final Batch Loss: 0.0069397748447954655\n",
      "Subject 11, Epoch 939, Loss: 0.23415804654359818, Final Batch Loss: 0.04043451324105263\n",
      "Subject 11, Epoch 940, Loss: 0.20348329795524478, Final Batch Loss: 0.006926048081368208\n",
      "Subject 11, Epoch 941, Loss: 0.16376632126048207, Final Batch Loss: 0.004426528234034777\n",
      "Subject 11, Epoch 942, Loss: 0.17375215096399188, Final Batch Loss: 0.00634279428049922\n",
      "Subject 11, Epoch 943, Loss: 0.1731602195650339, Final Batch Loss: 0.028461208567023277\n",
      "Subject 11, Epoch 944, Loss: 0.25834545865654945, Final Batch Loss: 0.037164054811000824\n",
      "Subject 11, Epoch 945, Loss: 0.19313253462314606, Final Batch Loss: 0.019865024834871292\n",
      "Subject 11, Epoch 946, Loss: 0.19507578574120998, Final Batch Loss: 0.022198019549250603\n",
      "Subject 11, Epoch 947, Loss: 0.15631280234083533, Final Batch Loss: 0.004746706690639257\n",
      "Subject 11, Epoch 948, Loss: 0.19694461254402995, Final Batch Loss: 0.004660464357584715\n",
      "Subject 11, Epoch 949, Loss: 0.39943329244852066, Final Batch Loss: 0.17008954286575317\n",
      "Subject 11, Epoch 950, Loss: 0.28915077075362206, Final Batch Loss: 0.07825211435556412\n",
      "Subject 11, Epoch 951, Loss: 0.3279259502887726, Final Batch Loss: 0.20066577196121216\n",
      "Subject 11, Epoch 952, Loss: 0.2314440682530403, Final Batch Loss: 0.03853803128004074\n",
      "Subject 11, Epoch 953, Loss: 0.19701313157565892, Final Batch Loss: 0.002575072692707181\n",
      "Subject 11, Epoch 954, Loss: 0.24305125512182713, Final Batch Loss: 0.04256391152739525\n",
      "Subject 11, Epoch 955, Loss: 0.20593493152409792, Final Batch Loss: 0.009948384948074818\n",
      "Subject 11, Epoch 956, Loss: 0.160875751869753, Final Batch Loss: 0.003901425050571561\n",
      "Subject 11, Epoch 957, Loss: 0.23913615383207798, Final Batch Loss: 0.018059933558106422\n",
      "Subject 11, Epoch 958, Loss: 0.7382011860609055, Final Batch Loss: 0.5375404357910156\n",
      "Subject 11, Epoch 959, Loss: 0.22109436988830566, Final Batch Loss: 0.07780570536851883\n",
      "Subject 11, Epoch 960, Loss: 0.25349562987685204, Final Batch Loss: 0.08434643596410751\n",
      "Subject 11, Epoch 961, Loss: 0.21714120358228683, Final Batch Loss: 0.03530212864279747\n",
      "Subject 11, Epoch 962, Loss: 0.2231044229120016, Final Batch Loss: 0.10680139064788818\n",
      "Subject 11, Epoch 963, Loss: 0.19551759818568826, Final Batch Loss: 0.0042986744083464146\n",
      "Subject 11, Epoch 964, Loss: 0.17643607780337334, Final Batch Loss: 0.032377939671278\n",
      "Subject 11, Epoch 965, Loss: 0.14181416854262352, Final Batch Loss: 0.024955710396170616\n",
      "Subject 11, Epoch 966, Loss: 0.28471630439162254, Final Batch Loss: 0.036740027368068695\n",
      "Subject 11, Epoch 967, Loss: 0.20113146305084229, Final Batch Loss: 0.00886552408337593\n",
      "Subject 11, Epoch 968, Loss: 0.18419996462762356, Final Batch Loss: 0.0037950370460748672\n",
      "Subject 11, Epoch 969, Loss: 0.11691119242459536, Final Batch Loss: 0.0030940137803554535\n",
      "Subject 11, Epoch 970, Loss: 0.09013431426137686, Final Batch Loss: 0.00918476190418005\n",
      "Subject 11, Epoch 971, Loss: 0.20027399063110352, Final Batch Loss: 0.03405975177884102\n",
      "Subject 11, Epoch 972, Loss: 0.2432310003787279, Final Batch Loss: 0.055647220462560654\n",
      "Subject 11, Epoch 973, Loss: 0.1188839366659522, Final Batch Loss: 0.010522625409066677\n",
      "Subject 11, Epoch 974, Loss: 0.19436934031546116, Final Batch Loss: 0.02082769386470318\n",
      "Subject 11, Epoch 975, Loss: 0.32930829748511314, Final Batch Loss: 0.05231380835175514\n",
      "Subject 11, Epoch 976, Loss: 0.1433581132441759, Final Batch Loss: 0.03156772255897522\n",
      "Subject 11, Epoch 977, Loss: 0.12476164463441819, Final Batch Loss: 0.0017398929921910167\n",
      "Subject 11, Epoch 978, Loss: 0.18139090575277805, Final Batch Loss: 0.06820160150527954\n",
      "Subject 11, Epoch 979, Loss: 0.16569307493045926, Final Batch Loss: 0.0009922892786562443\n",
      "Subject 11, Epoch 980, Loss: 0.1250050775706768, Final Batch Loss: 0.0032964125275611877\n",
      "Subject 11, Epoch 981, Loss: 0.15648797433823347, Final Batch Loss: 0.013432138599455357\n",
      "Subject 11, Epoch 982, Loss: 0.18217056104913354, Final Batch Loss: 0.0005306624807417393\n",
      "Subject 11, Epoch 983, Loss: 0.1597519339993596, Final Batch Loss: 0.003188100643455982\n",
      "Subject 11, Epoch 984, Loss: 0.1717127077281475, Final Batch Loss: 0.01196124404668808\n",
      "Subject 11, Epoch 985, Loss: 0.16249246150255203, Final Batch Loss: 0.043056029826402664\n",
      "Subject 11, Epoch 986, Loss: 0.22334149666130543, Final Batch Loss: 0.019835757091641426\n",
      "Subject 11, Epoch 987, Loss: 0.24255742045352235, Final Batch Loss: 0.000819551816675812\n",
      "Subject 11, Epoch 988, Loss: 0.25748120807111263, Final Batch Loss: 0.10624939203262329\n",
      "Subject 11, Epoch 989, Loss: 0.23385708779096603, Final Batch Loss: 0.04243412986397743\n",
      "Subject 11, Epoch 990, Loss: 0.7180629279464483, Final Batch Loss: 0.48590362071990967\n",
      "Subject 11, Epoch 991, Loss: 0.3555479720234871, Final Batch Loss: 0.13955090939998627\n",
      "Subject 11, Epoch 992, Loss: 0.12282550872623688, Final Batch Loss: 2.2489366529043764e-05\n",
      "Subject 11, Epoch 993, Loss: 0.18867281809798442, Final Batch Loss: 0.00042241052142344415\n",
      "Subject 11, Epoch 994, Loss: 0.25525521486997604, Final Batch Loss: 0.07686427235603333\n",
      "Subject 11, Epoch 995, Loss: 0.21473906096071005, Final Batch Loss: 0.003611764870584011\n",
      "Subject 11, Epoch 996, Loss: 0.15580530249280855, Final Batch Loss: 0.000904611952137202\n",
      "Subject 11, Epoch 997, Loss: 0.19729067012667656, Final Batch Loss: 0.026800768449902534\n",
      "Subject 11, Epoch 998, Loss: 0.16696655715350062, Final Batch Loss: 0.0012086067581549287\n",
      "Subject 11, Epoch 999, Loss: 0.1426007505506277, Final Batch Loss: 0.005796592682600021\n",
      "Subject 11, Epoch 1000, Loss: 0.12813281919807196, Final Batch Loss: 0.008764985017478466\n",
      "Subject 12, Epoch 1, Loss: 7.310832262039185, Final Batch Loss: 1.8306196928024292\n",
      "Subject 12, Epoch 2, Loss: 7.283363103866577, Final Batch Loss: 1.8402413129806519\n",
      "Subject 12, Epoch 3, Loss: 7.2505388259887695, Final Batch Loss: 1.8286930322647095\n",
      "Subject 12, Epoch 4, Loss: 7.189148664474487, Final Batch Loss: 1.7635390758514404\n",
      "Subject 12, Epoch 5, Loss: 7.176666855812073, Final Batch Loss: 1.7915356159210205\n",
      "Subject 12, Epoch 6, Loss: 7.122065544128418, Final Batch Loss: 1.7275065183639526\n",
      "Subject 12, Epoch 7, Loss: 7.098244547843933, Final Batch Loss: 1.7650645971298218\n",
      "Subject 12, Epoch 8, Loss: 7.0404157638549805, Final Batch Loss: 1.7497146129608154\n",
      "Subject 12, Epoch 9, Loss: 6.96703314781189, Final Batch Loss: 1.7453210353851318\n",
      "Subject 12, Epoch 10, Loss: 6.929883599281311, Final Batch Loss: 1.7159674167633057\n",
      "Subject 12, Epoch 11, Loss: 6.844926834106445, Final Batch Loss: 1.7080109119415283\n",
      "Subject 12, Epoch 12, Loss: 6.705835700035095, Final Batch Loss: 1.6440902948379517\n",
      "Subject 12, Epoch 13, Loss: 6.585759997367859, Final Batch Loss: 1.6975815296173096\n",
      "Subject 12, Epoch 14, Loss: 6.445985317230225, Final Batch Loss: 1.5994670391082764\n",
      "Subject 12, Epoch 15, Loss: 6.342121481895447, Final Batch Loss: 1.5562645196914673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 16, Loss: 6.269872426986694, Final Batch Loss: 1.5812180042266846\n",
      "Subject 12, Epoch 17, Loss: 6.12306535243988, Final Batch Loss: 1.559250831604004\n",
      "Subject 12, Epoch 18, Loss: 5.965001463890076, Final Batch Loss: 1.4401153326034546\n",
      "Subject 12, Epoch 19, Loss: 5.911787271499634, Final Batch Loss: 1.5162750482559204\n",
      "Subject 12, Epoch 20, Loss: 5.7409069538116455, Final Batch Loss: 1.3074707984924316\n",
      "Subject 12, Epoch 21, Loss: 5.638504981994629, Final Batch Loss: 1.4104397296905518\n",
      "Subject 12, Epoch 22, Loss: 5.54539692401886, Final Batch Loss: 1.2913833856582642\n",
      "Subject 12, Epoch 23, Loss: 5.487424612045288, Final Batch Loss: 1.4370641708374023\n",
      "Subject 12, Epoch 24, Loss: 5.377653360366821, Final Batch Loss: 1.2978020906448364\n",
      "Subject 12, Epoch 25, Loss: 5.309705495834351, Final Batch Loss: 1.4154261350631714\n",
      "Subject 12, Epoch 26, Loss: 5.062268257141113, Final Batch Loss: 1.2105904817581177\n",
      "Subject 12, Epoch 27, Loss: 5.0261757373809814, Final Batch Loss: 1.2964539527893066\n",
      "Subject 12, Epoch 28, Loss: 4.72601318359375, Final Batch Loss: 1.0087440013885498\n",
      "Subject 12, Epoch 29, Loss: 5.025558829307556, Final Batch Loss: 1.3076883554458618\n",
      "Subject 12, Epoch 30, Loss: 4.923252582550049, Final Batch Loss: 1.2885597944259644\n",
      "Subject 12, Epoch 31, Loss: 4.687275052070618, Final Batch Loss: 1.1675918102264404\n",
      "Subject 12, Epoch 32, Loss: 4.6308170557022095, Final Batch Loss: 1.0583600997924805\n",
      "Subject 12, Epoch 33, Loss: 4.757652521133423, Final Batch Loss: 1.1684350967407227\n",
      "Subject 12, Epoch 34, Loss: 4.687866449356079, Final Batch Loss: 1.240880012512207\n",
      "Subject 12, Epoch 35, Loss: 4.511656641960144, Final Batch Loss: 1.088240385055542\n",
      "Subject 12, Epoch 36, Loss: 4.516741991043091, Final Batch Loss: 1.1795752048492432\n",
      "Subject 12, Epoch 37, Loss: 4.5130215883255005, Final Batch Loss: 1.130489468574524\n",
      "Subject 12, Epoch 38, Loss: 4.208006024360657, Final Batch Loss: 0.9955329895019531\n",
      "Subject 12, Epoch 39, Loss: 4.453205347061157, Final Batch Loss: 1.1202738285064697\n",
      "Subject 12, Epoch 40, Loss: 4.513823747634888, Final Batch Loss: 1.0601428747177124\n",
      "Subject 12, Epoch 41, Loss: 4.232300400733948, Final Batch Loss: 1.0320476293563843\n",
      "Subject 12, Epoch 42, Loss: 4.28061306476593, Final Batch Loss: 1.0827001333236694\n",
      "Subject 12, Epoch 43, Loss: 4.229207873344421, Final Batch Loss: 1.0100246667861938\n",
      "Subject 12, Epoch 44, Loss: 4.192501127719879, Final Batch Loss: 0.9955493807792664\n",
      "Subject 12, Epoch 45, Loss: 4.166048765182495, Final Batch Loss: 1.0319114923477173\n",
      "Subject 12, Epoch 46, Loss: 4.09473717212677, Final Batch Loss: 1.0425482988357544\n",
      "Subject 12, Epoch 47, Loss: 3.998575747013092, Final Batch Loss: 0.9864633083343506\n",
      "Subject 12, Epoch 48, Loss: 3.958935022354126, Final Batch Loss: 0.9609390497207642\n",
      "Subject 12, Epoch 49, Loss: 3.772181272506714, Final Batch Loss: 0.9017266631126404\n",
      "Subject 12, Epoch 50, Loss: 3.892531931400299, Final Batch Loss: 1.0062251091003418\n",
      "Subject 12, Epoch 51, Loss: 3.7226043939590454, Final Batch Loss: 0.8633176684379578\n",
      "Subject 12, Epoch 52, Loss: 3.5217984914779663, Final Batch Loss: 0.7976871728897095\n",
      "Subject 12, Epoch 53, Loss: 3.590536594390869, Final Batch Loss: 0.8243055939674377\n",
      "Subject 12, Epoch 54, Loss: 3.748072028160095, Final Batch Loss: 1.0216989517211914\n",
      "Subject 12, Epoch 55, Loss: 3.581480622291565, Final Batch Loss: 0.9088497161865234\n",
      "Subject 12, Epoch 56, Loss: 3.497922956943512, Final Batch Loss: 0.8671898245811462\n",
      "Subject 12, Epoch 57, Loss: 3.2908604741096497, Final Batch Loss: 0.7637089490890503\n",
      "Subject 12, Epoch 58, Loss: 3.2875528931617737, Final Batch Loss: 0.8230606317520142\n",
      "Subject 12, Epoch 59, Loss: 3.4036388993263245, Final Batch Loss: 0.9811633825302124\n",
      "Subject 12, Epoch 60, Loss: 3.1076186299324036, Final Batch Loss: 0.752257227897644\n",
      "Subject 12, Epoch 61, Loss: 3.196147859096527, Final Batch Loss: 0.7173217535018921\n",
      "Subject 12, Epoch 62, Loss: 3.0683014392852783, Final Batch Loss: 0.8246839046478271\n",
      "Subject 12, Epoch 63, Loss: 3.0532407760620117, Final Batch Loss: 0.8671435713768005\n",
      "Subject 12, Epoch 64, Loss: 2.9738017320632935, Final Batch Loss: 0.7842413187026978\n",
      "Subject 12, Epoch 65, Loss: 2.903145670890808, Final Batch Loss: 0.7084184885025024\n",
      "Subject 12, Epoch 66, Loss: 2.772451877593994, Final Batch Loss: 0.6476446390151978\n",
      "Subject 12, Epoch 67, Loss: 2.6513407826423645, Final Batch Loss: 0.613155722618103\n",
      "Subject 12, Epoch 68, Loss: 2.7803874015808105, Final Batch Loss: 0.6773780584335327\n",
      "Subject 12, Epoch 69, Loss: 2.7555291652679443, Final Batch Loss: 0.7105656266212463\n",
      "Subject 12, Epoch 70, Loss: 2.890277922153473, Final Batch Loss: 0.7164414525032043\n",
      "Subject 12, Epoch 71, Loss: 2.5236637592315674, Final Batch Loss: 0.6088331341743469\n",
      "Subject 12, Epoch 72, Loss: 2.6924524903297424, Final Batch Loss: 0.7358343005180359\n",
      "Subject 12, Epoch 73, Loss: 2.882743775844574, Final Batch Loss: 0.7807117700576782\n",
      "Subject 12, Epoch 74, Loss: 2.593488037586212, Final Batch Loss: 0.5984158515930176\n",
      "Subject 12, Epoch 75, Loss: 2.6008734703063965, Final Batch Loss: 0.6328849792480469\n",
      "Subject 12, Epoch 76, Loss: 2.585781455039978, Final Batch Loss: 0.5841274857521057\n",
      "Subject 12, Epoch 77, Loss: 2.64688640832901, Final Batch Loss: 0.6385590434074402\n",
      "Subject 12, Epoch 78, Loss: 2.6199938654899597, Final Batch Loss: 0.7552469372749329\n",
      "Subject 12, Epoch 79, Loss: 2.6475398540496826, Final Batch Loss: 0.703090250492096\n",
      "Subject 12, Epoch 80, Loss: 2.434796154499054, Final Batch Loss: 0.5453265905380249\n",
      "Subject 12, Epoch 81, Loss: 2.541642904281616, Final Batch Loss: 0.654153048992157\n",
      "Subject 12, Epoch 82, Loss: 2.45769339799881, Final Batch Loss: 0.6769266724586487\n",
      "Subject 12, Epoch 83, Loss: 2.442924678325653, Final Batch Loss: 0.5975379347801208\n",
      "Subject 12, Epoch 84, Loss: 2.399297535419464, Final Batch Loss: 0.531000554561615\n",
      "Subject 12, Epoch 85, Loss: 2.5987799167633057, Final Batch Loss: 0.6762021780014038\n",
      "Subject 12, Epoch 86, Loss: 2.3541048765182495, Final Batch Loss: 0.5546653866767883\n",
      "Subject 12, Epoch 87, Loss: 2.359725594520569, Final Batch Loss: 0.5662384033203125\n",
      "Subject 12, Epoch 88, Loss: 2.2965367436408997, Final Batch Loss: 0.5419543981552124\n",
      "Subject 12, Epoch 89, Loss: 2.284581243991852, Final Batch Loss: 0.6009271740913391\n",
      "Subject 12, Epoch 90, Loss: 2.3900049924850464, Final Batch Loss: 0.642154335975647\n",
      "Subject 12, Epoch 91, Loss: 2.347191631793976, Final Batch Loss: 0.5597138404846191\n",
      "Subject 12, Epoch 92, Loss: 2.2631903290748596, Final Batch Loss: 0.49514275789260864\n",
      "Subject 12, Epoch 93, Loss: 2.3407501578330994, Final Batch Loss: 0.6354009509086609\n",
      "Subject 12, Epoch 94, Loss: 2.2932448983192444, Final Batch Loss: 0.6419336199760437\n",
      "Subject 12, Epoch 95, Loss: 2.2560351490974426, Final Batch Loss: 0.5060002207756042\n",
      "Subject 12, Epoch 96, Loss: 2.385297954082489, Final Batch Loss: 0.691393256187439\n",
      "Subject 12, Epoch 97, Loss: 2.3062275052070618, Final Batch Loss: 0.5257730484008789\n",
      "Subject 12, Epoch 98, Loss: 2.450173020362854, Final Batch Loss: 0.7195988297462463\n",
      "Subject 12, Epoch 99, Loss: 2.27510803937912, Final Batch Loss: 0.5254890322685242\n",
      "Subject 12, Epoch 100, Loss: 2.154482424259186, Final Batch Loss: 0.5759831070899963\n",
      "Subject 12, Epoch 101, Loss: 2.234869599342346, Final Batch Loss: 0.6320081949234009\n",
      "Subject 12, Epoch 102, Loss: 2.3230499029159546, Final Batch Loss: 0.5545306205749512\n",
      "Subject 12, Epoch 103, Loss: 2.2040265798568726, Final Batch Loss: 0.6305580735206604\n",
      "Subject 12, Epoch 104, Loss: 2.1731672883033752, Final Batch Loss: 0.5340457558631897\n",
      "Subject 12, Epoch 105, Loss: 2.4089515209198, Final Batch Loss: 0.6748480200767517\n",
      "Subject 12, Epoch 106, Loss: 2.303262710571289, Final Batch Loss: 0.5666202902793884\n",
      "Subject 12, Epoch 107, Loss: 2.14952552318573, Final Batch Loss: 0.5047411918640137\n",
      "Subject 12, Epoch 108, Loss: 2.233592599630356, Final Batch Loss: 0.5702781081199646\n",
      "Subject 12, Epoch 109, Loss: 2.2704155445098877, Final Batch Loss: 0.6603618264198303\n",
      "Subject 12, Epoch 110, Loss: 2.059422254562378, Final Batch Loss: 0.5224512815475464\n",
      "Subject 12, Epoch 111, Loss: 2.1198167204856873, Final Batch Loss: 0.532010555267334\n",
      "Subject 12, Epoch 112, Loss: 2.1324779391288757, Final Batch Loss: 0.529026210308075\n",
      "Subject 12, Epoch 113, Loss: 2.067126452922821, Final Batch Loss: 0.37936127185821533\n",
      "Subject 12, Epoch 114, Loss: 2.0795778632164, Final Batch Loss: 0.5172023773193359\n",
      "Subject 12, Epoch 115, Loss: 2.034038931131363, Final Batch Loss: 0.5768745541572571\n",
      "Subject 12, Epoch 116, Loss: 2.0882548093795776, Final Batch Loss: 0.5172831416130066\n",
      "Subject 12, Epoch 117, Loss: 2.045399248600006, Final Batch Loss: 0.5002981424331665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 118, Loss: 1.9709240794181824, Final Batch Loss: 0.48300787806510925\n",
      "Subject 12, Epoch 119, Loss: 2.1145960092544556, Final Batch Loss: 0.5611278414726257\n",
      "Subject 12, Epoch 120, Loss: 1.9740335643291473, Final Batch Loss: 0.4898899793624878\n",
      "Subject 12, Epoch 121, Loss: 1.8897812366485596, Final Batch Loss: 0.5141037702560425\n",
      "Subject 12, Epoch 122, Loss: 2.1138587296009064, Final Batch Loss: 0.6083234548568726\n",
      "Subject 12, Epoch 123, Loss: 2.0522509813308716, Final Batch Loss: 0.5563722848892212\n",
      "Subject 12, Epoch 124, Loss: 1.82854425907135, Final Batch Loss: 0.41063404083251953\n",
      "Subject 12, Epoch 125, Loss: 1.9214598834514618, Final Batch Loss: 0.444818913936615\n",
      "Subject 12, Epoch 126, Loss: 1.9944525957107544, Final Batch Loss: 0.5721760392189026\n",
      "Subject 12, Epoch 127, Loss: 1.7144096493721008, Final Batch Loss: 0.453204870223999\n",
      "Subject 12, Epoch 128, Loss: 1.8638137578964233, Final Batch Loss: 0.36930087208747864\n",
      "Subject 12, Epoch 129, Loss: 1.870234727859497, Final Batch Loss: 0.5317237377166748\n",
      "Subject 12, Epoch 130, Loss: 1.8680075407028198, Final Batch Loss: 0.4452654719352722\n",
      "Subject 12, Epoch 131, Loss: 1.7357350885868073, Final Batch Loss: 0.38757142424583435\n",
      "Subject 12, Epoch 132, Loss: 1.9488860368728638, Final Batch Loss: 0.400312215089798\n",
      "Subject 12, Epoch 133, Loss: 1.814805954694748, Final Batch Loss: 0.49949848651885986\n",
      "Subject 12, Epoch 134, Loss: 1.6276150345802307, Final Batch Loss: 0.3448103964328766\n",
      "Subject 12, Epoch 135, Loss: 1.6175549030303955, Final Batch Loss: 0.3343846797943115\n",
      "Subject 12, Epoch 136, Loss: 1.704759657382965, Final Batch Loss: 0.46668827533721924\n",
      "Subject 12, Epoch 137, Loss: 1.8447511792182922, Final Batch Loss: 0.4564158320426941\n",
      "Subject 12, Epoch 138, Loss: 1.7072263658046722, Final Batch Loss: 0.41328978538513184\n",
      "Subject 12, Epoch 139, Loss: 1.7923725545406342, Final Batch Loss: 0.5286179184913635\n",
      "Subject 12, Epoch 140, Loss: 1.5806221663951874, Final Batch Loss: 0.420512318611145\n",
      "Subject 12, Epoch 141, Loss: 1.6411361396312714, Final Batch Loss: 0.5519589185714722\n",
      "Subject 12, Epoch 142, Loss: 1.7397663295269012, Final Batch Loss: 0.3818814158439636\n",
      "Subject 12, Epoch 143, Loss: 1.496525138616562, Final Batch Loss: 0.3157488703727722\n",
      "Subject 12, Epoch 144, Loss: 1.7223534286022186, Final Batch Loss: 0.3748231828212738\n",
      "Subject 12, Epoch 145, Loss: 1.7872477173805237, Final Batch Loss: 0.5730533003807068\n",
      "Subject 12, Epoch 146, Loss: 1.6371845304965973, Final Batch Loss: 0.3768644630908966\n",
      "Subject 12, Epoch 147, Loss: 1.6194325387477875, Final Batch Loss: 0.5465479493141174\n",
      "Subject 12, Epoch 148, Loss: 1.6073727011680603, Final Batch Loss: 0.41832268238067627\n",
      "Subject 12, Epoch 149, Loss: 1.6543286442756653, Final Batch Loss: 0.41611990332603455\n",
      "Subject 12, Epoch 150, Loss: 1.6310025453567505, Final Batch Loss: 0.396808922290802\n",
      "Subject 12, Epoch 151, Loss: 1.6184774041175842, Final Batch Loss: 0.4490225911140442\n",
      "Subject 12, Epoch 152, Loss: 1.7475265562534332, Final Batch Loss: 0.49358588457107544\n",
      "Subject 12, Epoch 153, Loss: 1.7278558909893036, Final Batch Loss: 0.5286055207252502\n",
      "Subject 12, Epoch 154, Loss: 1.5797349512577057, Final Batch Loss: 0.43136635422706604\n",
      "Subject 12, Epoch 155, Loss: 1.5376244187355042, Final Batch Loss: 0.3832642436027527\n",
      "Subject 12, Epoch 156, Loss: 1.6486826837062836, Final Batch Loss: 0.382817804813385\n",
      "Subject 12, Epoch 157, Loss: 1.5495222508907318, Final Batch Loss: 0.45124807953834534\n",
      "Subject 12, Epoch 158, Loss: 1.5549148917198181, Final Batch Loss: 0.38572096824645996\n",
      "Subject 12, Epoch 159, Loss: 1.694187045097351, Final Batch Loss: 0.3922153115272522\n",
      "Subject 12, Epoch 160, Loss: 1.559887856245041, Final Batch Loss: 0.43249544501304626\n",
      "Subject 12, Epoch 161, Loss: 1.3780473172664642, Final Batch Loss: 0.3480812609195709\n",
      "Subject 12, Epoch 162, Loss: 1.4210695624351501, Final Batch Loss: 0.38648661971092224\n",
      "Subject 12, Epoch 163, Loss: 1.5873956680297852, Final Batch Loss: 0.44010043144226074\n",
      "Subject 12, Epoch 164, Loss: 1.4601219296455383, Final Batch Loss: 0.36872220039367676\n",
      "Subject 12, Epoch 165, Loss: 1.4452238976955414, Final Batch Loss: 0.37883779406547546\n",
      "Subject 12, Epoch 166, Loss: 1.4834136068820953, Final Batch Loss: 0.3931598663330078\n",
      "Subject 12, Epoch 167, Loss: 1.5114139914512634, Final Batch Loss: 0.36773037910461426\n",
      "Subject 12, Epoch 168, Loss: 1.5495359897613525, Final Batch Loss: 0.3208366930484772\n",
      "Subject 12, Epoch 169, Loss: 1.4421530961990356, Final Batch Loss: 0.27979832887649536\n",
      "Subject 12, Epoch 170, Loss: 1.4171594083309174, Final Batch Loss: 0.3329658806324005\n",
      "Subject 12, Epoch 171, Loss: 1.4087956845760345, Final Batch Loss: 0.33967456221580505\n",
      "Subject 12, Epoch 172, Loss: 1.500870168209076, Final Batch Loss: 0.33967486023902893\n",
      "Subject 12, Epoch 173, Loss: 1.4456510245800018, Final Batch Loss: 0.3859449326992035\n",
      "Subject 12, Epoch 174, Loss: 1.4607597142457962, Final Batch Loss: 0.20220370590686798\n",
      "Subject 12, Epoch 175, Loss: 1.3969244956970215, Final Batch Loss: 0.3601377606391907\n",
      "Subject 12, Epoch 176, Loss: 1.535616248846054, Final Batch Loss: 0.41997644305229187\n",
      "Subject 12, Epoch 177, Loss: 1.5240396559238434, Final Batch Loss: 0.4577619731426239\n",
      "Subject 12, Epoch 178, Loss: 1.5184026658535004, Final Batch Loss: 0.4015255868434906\n",
      "Subject 12, Epoch 179, Loss: 1.4308623969554901, Final Batch Loss: 0.3888406753540039\n",
      "Subject 12, Epoch 180, Loss: 1.4358028173446655, Final Batch Loss: 0.3991459906101227\n",
      "Subject 12, Epoch 181, Loss: 1.3405681401491165, Final Batch Loss: 0.18478886783123016\n",
      "Subject 12, Epoch 182, Loss: 1.4308787882328033, Final Batch Loss: 0.37441083788871765\n",
      "Subject 12, Epoch 183, Loss: 1.4425675570964813, Final Batch Loss: 0.3607121706008911\n",
      "Subject 12, Epoch 184, Loss: 1.3279859721660614, Final Batch Loss: 0.3108082711696625\n",
      "Subject 12, Epoch 185, Loss: 1.3250439167022705, Final Batch Loss: 0.26964035630226135\n",
      "Subject 12, Epoch 186, Loss: 1.4683698415756226, Final Batch Loss: 0.37068453431129456\n",
      "Subject 12, Epoch 187, Loss: 1.398423671722412, Final Batch Loss: 0.33151862025260925\n",
      "Subject 12, Epoch 188, Loss: 1.3104391992092133, Final Batch Loss: 0.3490944802761078\n",
      "Subject 12, Epoch 189, Loss: 1.285191923379898, Final Batch Loss: 0.22928094863891602\n",
      "Subject 12, Epoch 190, Loss: 1.353879064321518, Final Batch Loss: 0.26376551389694214\n",
      "Subject 12, Epoch 191, Loss: 1.3965737521648407, Final Batch Loss: 0.3702349066734314\n",
      "Subject 12, Epoch 192, Loss: 1.3207499384880066, Final Batch Loss: 0.2733912765979767\n",
      "Subject 12, Epoch 193, Loss: 1.3816399574279785, Final Batch Loss: 0.38605329394340515\n",
      "Subject 12, Epoch 194, Loss: 1.278905838727951, Final Batch Loss: 0.3560236692428589\n",
      "Subject 12, Epoch 195, Loss: 1.335634022951126, Final Batch Loss: 0.2984524071216583\n",
      "Subject 12, Epoch 196, Loss: 1.3088403642177582, Final Batch Loss: 0.36039677262306213\n",
      "Subject 12, Epoch 197, Loss: 1.3153774440288544, Final Batch Loss: 0.2688875198364258\n",
      "Subject 12, Epoch 198, Loss: 1.5295672118663788, Final Batch Loss: 0.3743751049041748\n",
      "Subject 12, Epoch 199, Loss: 1.5731641352176666, Final Batch Loss: 0.3816949725151062\n",
      "Subject 12, Epoch 200, Loss: 1.3166817128658295, Final Batch Loss: 0.30571234226226807\n",
      "Subject 12, Epoch 201, Loss: 1.371535301208496, Final Batch Loss: 0.3095518946647644\n",
      "Subject 12, Epoch 202, Loss: 1.3624404072761536, Final Batch Loss: 0.3853875398635864\n",
      "Subject 12, Epoch 203, Loss: 1.3376611173152924, Final Batch Loss: 0.2995101809501648\n",
      "Subject 12, Epoch 204, Loss: 1.3138000071048737, Final Batch Loss: 0.26400938630104065\n",
      "Subject 12, Epoch 205, Loss: 1.3419487476348877, Final Batch Loss: 0.37832555174827576\n",
      "Subject 12, Epoch 206, Loss: 1.2703980803489685, Final Batch Loss: 0.3085692524909973\n",
      "Subject 12, Epoch 207, Loss: 1.3572387993335724, Final Batch Loss: 0.39460182189941406\n",
      "Subject 12, Epoch 208, Loss: 1.372932642698288, Final Batch Loss: 0.44349202513694763\n",
      "Subject 12, Epoch 209, Loss: 1.3655391335487366, Final Batch Loss: 0.5044068098068237\n",
      "Subject 12, Epoch 210, Loss: 1.467782586812973, Final Batch Loss: 0.354761004447937\n",
      "Subject 12, Epoch 211, Loss: 1.2829457819461823, Final Batch Loss: 0.26659825444221497\n",
      "Subject 12, Epoch 212, Loss: 1.2907927930355072, Final Batch Loss: 0.3204805254936218\n",
      "Subject 12, Epoch 213, Loss: 1.4446462392807007, Final Batch Loss: 0.37525424361228943\n",
      "Subject 12, Epoch 214, Loss: 1.2522439360618591, Final Batch Loss: 0.2945643961429596\n",
      "Subject 12, Epoch 215, Loss: 1.355423927307129, Final Batch Loss: 0.34495022892951965\n",
      "Subject 12, Epoch 216, Loss: 1.240447849035263, Final Batch Loss: 0.30441799759864807\n",
      "Subject 12, Epoch 217, Loss: 1.2113039195537567, Final Batch Loss: 0.2792169749736786\n",
      "Subject 12, Epoch 218, Loss: 1.2481269240379333, Final Batch Loss: 0.28083473443984985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 219, Loss: 1.4029817283153534, Final Batch Loss: 0.2908221185207367\n",
      "Subject 12, Epoch 220, Loss: 1.23076730966568, Final Batch Loss: 0.30419936776161194\n",
      "Subject 12, Epoch 221, Loss: 1.1640109419822693, Final Batch Loss: 0.1616857647895813\n",
      "Subject 12, Epoch 222, Loss: 1.2602964341640472, Final Batch Loss: 0.33598342537879944\n",
      "Subject 12, Epoch 223, Loss: 1.319761723279953, Final Batch Loss: 0.31660196185112\n",
      "Subject 12, Epoch 224, Loss: 1.1983784139156342, Final Batch Loss: 0.2674853205680847\n",
      "Subject 12, Epoch 225, Loss: 1.1861045360565186, Final Batch Loss: 0.36463290452957153\n",
      "Subject 12, Epoch 226, Loss: 1.2955958545207977, Final Batch Loss: 0.40066689252853394\n",
      "Subject 12, Epoch 227, Loss: 1.2539631128311157, Final Batch Loss: 0.3974798917770386\n",
      "Subject 12, Epoch 228, Loss: 1.2285618782043457, Final Batch Loss: 0.3420813977718353\n",
      "Subject 12, Epoch 229, Loss: 1.3474125862121582, Final Batch Loss: 0.39753666520118713\n",
      "Subject 12, Epoch 230, Loss: 1.3408855497837067, Final Batch Loss: 0.415965735912323\n",
      "Subject 12, Epoch 231, Loss: 1.3941972255706787, Final Batch Loss: 0.2828541100025177\n",
      "Subject 12, Epoch 232, Loss: 1.1732707470655441, Final Batch Loss: 0.2963936924934387\n",
      "Subject 12, Epoch 233, Loss: 1.236213743686676, Final Batch Loss: 0.33508896827697754\n",
      "Subject 12, Epoch 234, Loss: 1.2381934225559235, Final Batch Loss: 0.2584976255893707\n",
      "Subject 12, Epoch 235, Loss: 1.2145518362522125, Final Batch Loss: 0.2547498643398285\n",
      "Subject 12, Epoch 236, Loss: 1.2839624881744385, Final Batch Loss: 0.3759090304374695\n",
      "Subject 12, Epoch 237, Loss: 1.1721915900707245, Final Batch Loss: 0.2971882224082947\n",
      "Subject 12, Epoch 238, Loss: 1.1440790891647339, Final Batch Loss: 0.22626760601997375\n",
      "Subject 12, Epoch 239, Loss: 1.1775159537792206, Final Batch Loss: 0.28217813372612\n",
      "Subject 12, Epoch 240, Loss: 1.084286779165268, Final Batch Loss: 0.16825270652770996\n",
      "Subject 12, Epoch 241, Loss: 1.1559242010116577, Final Batch Loss: 0.2455209195613861\n",
      "Subject 12, Epoch 242, Loss: 1.1580225229263306, Final Batch Loss: 0.25970932841300964\n",
      "Subject 12, Epoch 243, Loss: 1.113954246044159, Final Batch Loss: 0.24018526077270508\n",
      "Subject 12, Epoch 244, Loss: 1.3583932518959045, Final Batch Loss: 0.31907519698143005\n",
      "Subject 12, Epoch 245, Loss: 1.2627717852592468, Final Batch Loss: 0.33813968300819397\n",
      "Subject 12, Epoch 246, Loss: 1.233319640159607, Final Batch Loss: 0.3357391059398651\n",
      "Subject 12, Epoch 247, Loss: 1.2484776973724365, Final Batch Loss: 0.37722060084342957\n",
      "Subject 12, Epoch 248, Loss: 1.3385600745677948, Final Batch Loss: 0.4250372052192688\n",
      "Subject 12, Epoch 249, Loss: 1.1689890772104263, Final Batch Loss: 0.22739435732364655\n",
      "Subject 12, Epoch 250, Loss: 1.1733793318271637, Final Batch Loss: 0.3502514958381653\n",
      "Subject 12, Epoch 251, Loss: 1.223619431257248, Final Batch Loss: 0.382933646440506\n",
      "Subject 12, Epoch 252, Loss: 1.3025690615177155, Final Batch Loss: 0.44197073578834534\n",
      "Subject 12, Epoch 253, Loss: 1.169817954301834, Final Batch Loss: 0.2389202117919922\n",
      "Subject 12, Epoch 254, Loss: 1.1337355971336365, Final Batch Loss: 0.26114848256111145\n",
      "Subject 12, Epoch 255, Loss: 1.1681534945964813, Final Batch Loss: 0.3191896975040436\n",
      "Subject 12, Epoch 256, Loss: 1.292865812778473, Final Batch Loss: 0.3236117362976074\n",
      "Subject 12, Epoch 257, Loss: 1.2417201101779938, Final Batch Loss: 0.39699500799179077\n",
      "Subject 12, Epoch 258, Loss: 1.1306146383285522, Final Batch Loss: 0.3541082441806793\n",
      "Subject 12, Epoch 259, Loss: 1.1369597017765045, Final Batch Loss: 0.258294016122818\n",
      "Subject 12, Epoch 260, Loss: 1.184031367301941, Final Batch Loss: 0.3133743405342102\n",
      "Subject 12, Epoch 261, Loss: 1.279681771993637, Final Batch Loss: 0.37265026569366455\n",
      "Subject 12, Epoch 262, Loss: 1.1927086412906647, Final Batch Loss: 0.2689342498779297\n",
      "Subject 12, Epoch 263, Loss: 1.2644086182117462, Final Batch Loss: 0.36845046281814575\n",
      "Subject 12, Epoch 264, Loss: 1.2031791806221008, Final Batch Loss: 0.30108529329299927\n",
      "Subject 12, Epoch 265, Loss: 0.9795328974723816, Final Batch Loss: 0.18671396374702454\n",
      "Subject 12, Epoch 266, Loss: 1.2254953980445862, Final Batch Loss: 0.34381479024887085\n",
      "Subject 12, Epoch 267, Loss: 1.1138199865818024, Final Batch Loss: 0.2323097586631775\n",
      "Subject 12, Epoch 268, Loss: 1.2225816249847412, Final Batch Loss: 0.311066210269928\n",
      "Subject 12, Epoch 269, Loss: 1.2026054561138153, Final Batch Loss: 0.337186723947525\n",
      "Subject 12, Epoch 270, Loss: 1.3010609149932861, Final Batch Loss: 0.4790651798248291\n",
      "Subject 12, Epoch 271, Loss: 1.2171021103858948, Final Batch Loss: 0.43594953417778015\n",
      "Subject 12, Epoch 272, Loss: 1.2251556068658829, Final Batch Loss: 0.2253255695104599\n",
      "Subject 12, Epoch 273, Loss: 1.1457150131464005, Final Batch Loss: 0.31270065903663635\n",
      "Subject 12, Epoch 274, Loss: 1.2057963609695435, Final Batch Loss: 0.26831528544425964\n",
      "Subject 12, Epoch 275, Loss: 1.1281911134719849, Final Batch Loss: 0.2990412414073944\n",
      "Subject 12, Epoch 276, Loss: 1.0969175100326538, Final Batch Loss: 0.28418445587158203\n",
      "Subject 12, Epoch 277, Loss: 1.1630759984254837, Final Batch Loss: 0.2210579812526703\n",
      "Subject 12, Epoch 278, Loss: 1.192106544971466, Final Batch Loss: 0.3359554409980774\n",
      "Subject 12, Epoch 279, Loss: 1.0850267857313156, Final Batch Loss: 0.28941288590431213\n",
      "Subject 12, Epoch 280, Loss: 1.1202130317687988, Final Batch Loss: 0.2862805724143982\n",
      "Subject 12, Epoch 281, Loss: 1.1097366213798523, Final Batch Loss: 0.3126657009124756\n",
      "Subject 12, Epoch 282, Loss: 1.0968358367681503, Final Batch Loss: 0.24853241443634033\n",
      "Subject 12, Epoch 283, Loss: 1.018600419163704, Final Batch Loss: 0.23624010384082794\n",
      "Subject 12, Epoch 284, Loss: 1.2126757353544235, Final Batch Loss: 0.23114623129367828\n",
      "Subject 12, Epoch 285, Loss: 1.287000760436058, Final Batch Loss: 0.5100589990615845\n",
      "Subject 12, Epoch 286, Loss: 1.2244974374771118, Final Batch Loss: 0.35687607526779175\n",
      "Subject 12, Epoch 287, Loss: 1.1511252671480179, Final Batch Loss: 0.2714649438858032\n",
      "Subject 12, Epoch 288, Loss: 1.2351278960704803, Final Batch Loss: 0.3071932792663574\n",
      "Subject 12, Epoch 289, Loss: 0.9895952641963959, Final Batch Loss: 0.16547641158103943\n",
      "Subject 12, Epoch 290, Loss: 1.077472135424614, Final Batch Loss: 0.28417107462882996\n",
      "Subject 12, Epoch 291, Loss: 1.1300488412380219, Final Batch Loss: 0.2960561215877533\n",
      "Subject 12, Epoch 292, Loss: 1.0955316871404648, Final Batch Loss: 0.28648513555526733\n",
      "Subject 12, Epoch 293, Loss: 1.1820325553417206, Final Batch Loss: 0.37148424983024597\n",
      "Subject 12, Epoch 294, Loss: 1.2600826025009155, Final Batch Loss: 0.42523160576820374\n",
      "Subject 12, Epoch 295, Loss: 1.1680676937103271, Final Batch Loss: 0.19468680024147034\n",
      "Subject 12, Epoch 296, Loss: 1.1825848519802094, Final Batch Loss: 0.2338644564151764\n",
      "Subject 12, Epoch 297, Loss: 1.054268553853035, Final Batch Loss: 0.20539341866970062\n",
      "Subject 12, Epoch 298, Loss: 1.0170718431472778, Final Batch Loss: 0.26138705015182495\n",
      "Subject 12, Epoch 299, Loss: 1.172984093427658, Final Batch Loss: 0.31401118636131287\n",
      "Subject 12, Epoch 300, Loss: 1.1550443470478058, Final Batch Loss: 0.2715188264846802\n",
      "Subject 12, Epoch 301, Loss: 1.1021891683340073, Final Batch Loss: 0.2818191349506378\n",
      "Subject 12, Epoch 302, Loss: 1.01105535030365, Final Batch Loss: 0.28055182099342346\n",
      "Subject 12, Epoch 303, Loss: 1.0450173020362854, Final Batch Loss: 0.20960718393325806\n",
      "Subject 12, Epoch 304, Loss: 1.1081665307283401, Final Batch Loss: 0.3508671522140503\n",
      "Subject 12, Epoch 305, Loss: 1.0712109804153442, Final Batch Loss: 0.34696364402770996\n",
      "Subject 12, Epoch 306, Loss: 1.1518874764442444, Final Batch Loss: 0.3051251471042633\n",
      "Subject 12, Epoch 307, Loss: 1.147718757390976, Final Batch Loss: 0.3767533600330353\n",
      "Subject 12, Epoch 308, Loss: 1.2239946126937866, Final Batch Loss: 0.39030614495277405\n",
      "Subject 12, Epoch 309, Loss: 1.201122522354126, Final Batch Loss: 0.3413125276565552\n",
      "Subject 12, Epoch 310, Loss: 1.1935693621635437, Final Batch Loss: 0.2654847502708435\n",
      "Subject 12, Epoch 311, Loss: 1.15896937251091, Final Batch Loss: 0.35677918791770935\n",
      "Subject 12, Epoch 312, Loss: 1.1723555028438568, Final Batch Loss: 0.3925049901008606\n",
      "Subject 12, Epoch 313, Loss: 1.0441699028015137, Final Batch Loss: 0.22210820019245148\n",
      "Subject 12, Epoch 314, Loss: 1.0673116147518158, Final Batch Loss: 0.2523624300956726\n",
      "Subject 12, Epoch 315, Loss: 1.0883219540119171, Final Batch Loss: 0.2793176770210266\n",
      "Subject 12, Epoch 316, Loss: 1.0922562181949615, Final Batch Loss: 0.23365885019302368\n",
      "Subject 12, Epoch 317, Loss: 1.001987800002098, Final Batch Loss: 0.2864989936351776\n",
      "Subject 12, Epoch 318, Loss: 1.0227677375078201, Final Batch Loss: 0.18502087891101837\n",
      "Subject 12, Epoch 319, Loss: 1.065101906657219, Final Batch Loss: 0.18136851489543915\n",
      "Subject 12, Epoch 320, Loss: 1.1647007763385773, Final Batch Loss: 0.35025110840797424\n",
      "Subject 12, Epoch 321, Loss: 1.1994844377040863, Final Batch Loss: 0.2976701259613037\n",
      "Subject 12, Epoch 322, Loss: 1.2697272300720215, Final Batch Loss: 0.48635491728782654\n",
      "Subject 12, Epoch 323, Loss: 0.9562085866928101, Final Batch Loss: 0.2180803269147873\n",
      "Subject 12, Epoch 324, Loss: 1.1322410553693771, Final Batch Loss: 0.3700123727321625\n",
      "Subject 12, Epoch 325, Loss: 1.12163645029068, Final Batch Loss: 0.34696316719055176\n",
      "Subject 12, Epoch 326, Loss: 1.0517795830965042, Final Batch Loss: 0.23115676641464233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 327, Loss: 1.0306448191404343, Final Batch Loss: 0.18615536391735077\n",
      "Subject 12, Epoch 328, Loss: 1.2344470918178558, Final Batch Loss: 0.39426401257514954\n",
      "Subject 12, Epoch 329, Loss: 1.1194116026163101, Final Batch Loss: 0.22170241177082062\n",
      "Subject 12, Epoch 330, Loss: 1.0518924593925476, Final Batch Loss: 0.2927356958389282\n",
      "Subject 12, Epoch 331, Loss: 1.0682886242866516, Final Batch Loss: 0.2668066918849945\n",
      "Subject 12, Epoch 332, Loss: 1.0674609988927841, Final Batch Loss: 0.29983091354370117\n",
      "Subject 12, Epoch 333, Loss: 0.9573124200105667, Final Batch Loss: 0.1940590888261795\n",
      "Subject 12, Epoch 334, Loss: 0.9688648283481598, Final Batch Loss: 0.19214877486228943\n",
      "Subject 12, Epoch 335, Loss: 1.005626454949379, Final Batch Loss: 0.20436504483222961\n",
      "Subject 12, Epoch 336, Loss: 1.1219393014907837, Final Batch Loss: 0.2999829649925232\n",
      "Subject 12, Epoch 337, Loss: 1.150521993637085, Final Batch Loss: 0.37117451429367065\n",
      "Subject 12, Epoch 338, Loss: 1.1392738372087479, Final Batch Loss: 0.37638339400291443\n",
      "Subject 12, Epoch 339, Loss: 1.102311059832573, Final Batch Loss: 0.3173558712005615\n",
      "Subject 12, Epoch 340, Loss: 1.1515547037124634, Final Batch Loss: 0.2569088041782379\n",
      "Subject 12, Epoch 341, Loss: 1.0786585360765457, Final Batch Loss: 0.3205908238887787\n",
      "Subject 12, Epoch 342, Loss: 1.0283010751008987, Final Batch Loss: 0.21862730383872986\n",
      "Subject 12, Epoch 343, Loss: 1.0340675115585327, Final Batch Loss: 0.22725750505924225\n",
      "Subject 12, Epoch 344, Loss: 1.1632935404777527, Final Batch Loss: 0.2831592261791229\n",
      "Subject 12, Epoch 345, Loss: 0.9856569916009903, Final Batch Loss: 0.27616286277770996\n",
      "Subject 12, Epoch 346, Loss: 0.9860264509916306, Final Batch Loss: 0.2589166760444641\n",
      "Subject 12, Epoch 347, Loss: 1.0374941676855087, Final Batch Loss: 0.20781809091567993\n",
      "Subject 12, Epoch 348, Loss: 1.0195107012987137, Final Batch Loss: 0.22297222912311554\n",
      "Subject 12, Epoch 349, Loss: 1.012460172176361, Final Batch Loss: 0.21078138053417206\n",
      "Subject 12, Epoch 350, Loss: 0.9480292052030563, Final Batch Loss: 0.1833464801311493\n",
      "Subject 12, Epoch 351, Loss: 1.0057383626699448, Final Batch Loss: 0.17918503284454346\n",
      "Subject 12, Epoch 352, Loss: 0.9716256707906723, Final Batch Loss: 0.19725443422794342\n",
      "Subject 12, Epoch 353, Loss: 0.9970812797546387, Final Batch Loss: 0.23729701340198517\n",
      "Subject 12, Epoch 354, Loss: 1.0861538350582123, Final Batch Loss: 0.34083041548728943\n",
      "Subject 12, Epoch 355, Loss: 1.0071741044521332, Final Batch Loss: 0.24422813951969147\n",
      "Subject 12, Epoch 356, Loss: 1.078050658106804, Final Batch Loss: 0.2503402531147003\n",
      "Subject 12, Epoch 357, Loss: 1.0896954089403152, Final Batch Loss: 0.30086949467658997\n",
      "Subject 12, Epoch 358, Loss: 1.0235897898674011, Final Batch Loss: 0.3101537227630615\n",
      "Subject 12, Epoch 359, Loss: 1.0389072448015213, Final Batch Loss: 0.3188594579696655\n",
      "Subject 12, Epoch 360, Loss: 1.0794305950403214, Final Batch Loss: 0.26076292991638184\n",
      "Subject 12, Epoch 361, Loss: 1.0086321532726288, Final Batch Loss: 0.26359832286834717\n",
      "Subject 12, Epoch 362, Loss: 0.9361836165189743, Final Batch Loss: 0.19803477823734283\n",
      "Subject 12, Epoch 363, Loss: 0.981911689043045, Final Batch Loss: 0.23090755939483643\n",
      "Subject 12, Epoch 364, Loss: 0.9597930461168289, Final Batch Loss: 0.2176600992679596\n",
      "Subject 12, Epoch 365, Loss: 1.1644681990146637, Final Batch Loss: 0.3139612078666687\n",
      "Subject 12, Epoch 366, Loss: 0.9843067824840546, Final Batch Loss: 0.21881115436553955\n",
      "Subject 12, Epoch 367, Loss: 1.1193214058876038, Final Batch Loss: 0.21700799465179443\n",
      "Subject 12, Epoch 368, Loss: 1.0253196060657501, Final Batch Loss: 0.19375939667224884\n",
      "Subject 12, Epoch 369, Loss: 1.0239636451005936, Final Batch Loss: 0.2589569687843323\n",
      "Subject 12, Epoch 370, Loss: 0.9633505642414093, Final Batch Loss: 0.229991152882576\n",
      "Subject 12, Epoch 371, Loss: 1.0627186745405197, Final Batch Loss: 0.3837272524833679\n",
      "Subject 12, Epoch 372, Loss: 1.094833955168724, Final Batch Loss: 0.3233548402786255\n",
      "Subject 12, Epoch 373, Loss: 1.0705767422914505, Final Batch Loss: 0.32525530457496643\n",
      "Subject 12, Epoch 374, Loss: 1.1534671187400818, Final Batch Loss: 0.42678675055503845\n",
      "Subject 12, Epoch 375, Loss: 1.015620306134224, Final Batch Loss: 0.3278743028640747\n",
      "Subject 12, Epoch 376, Loss: 1.0128352791070938, Final Batch Loss: 0.3135364353656769\n",
      "Subject 12, Epoch 377, Loss: 1.13664111495018, Final Batch Loss: 0.2941252589225769\n",
      "Subject 12, Epoch 378, Loss: 0.932402640581131, Final Batch Loss: 0.16555655002593994\n",
      "Subject 12, Epoch 379, Loss: 0.9230508357286453, Final Batch Loss: 0.19322076439857483\n",
      "Subject 12, Epoch 380, Loss: 1.0014719069004059, Final Batch Loss: 0.34958648681640625\n",
      "Subject 12, Epoch 381, Loss: 1.0511388033628464, Final Batch Loss: 0.2600884735584259\n",
      "Subject 12, Epoch 382, Loss: 0.9257623106241226, Final Batch Loss: 0.21325920522212982\n",
      "Subject 12, Epoch 383, Loss: 0.9255003184080124, Final Batch Loss: 0.13654352724552155\n",
      "Subject 12, Epoch 384, Loss: 0.9429849833250046, Final Batch Loss: 0.3130699694156647\n",
      "Subject 12, Epoch 385, Loss: 1.0429483652114868, Final Batch Loss: 0.2931956946849823\n",
      "Subject 12, Epoch 386, Loss: 0.9722101390361786, Final Batch Loss: 0.22154435515403748\n",
      "Subject 12, Epoch 387, Loss: 0.8636353760957718, Final Batch Loss: 0.14973831176757812\n",
      "Subject 12, Epoch 388, Loss: 0.9694802910089493, Final Batch Loss: 0.1824771910905838\n",
      "Subject 12, Epoch 389, Loss: 1.0298647433519363, Final Batch Loss: 0.334529310464859\n",
      "Subject 12, Epoch 390, Loss: 1.0792605131864548, Final Batch Loss: 0.24739672243595123\n",
      "Subject 12, Epoch 391, Loss: 0.9581288844347, Final Batch Loss: 0.2427440732717514\n",
      "Subject 12, Epoch 392, Loss: 1.0501099675893784, Final Batch Loss: 0.40245670080184937\n",
      "Subject 12, Epoch 393, Loss: 1.0079015046358109, Final Batch Loss: 0.31978127360343933\n",
      "Subject 12, Epoch 394, Loss: 0.9933334589004517, Final Batch Loss: 0.2040141522884369\n",
      "Subject 12, Epoch 395, Loss: 1.1168991923332214, Final Batch Loss: 0.3542516827583313\n",
      "Subject 12, Epoch 396, Loss: 0.9539013355970383, Final Batch Loss: 0.23167672753334045\n",
      "Subject 12, Epoch 397, Loss: 1.0128800421953201, Final Batch Loss: 0.35891202092170715\n",
      "Subject 12, Epoch 398, Loss: 1.0741707384586334, Final Batch Loss: 0.2173139899969101\n",
      "Subject 12, Epoch 399, Loss: 0.9095407724380493, Final Batch Loss: 0.19393129646778107\n",
      "Subject 12, Epoch 400, Loss: 1.011294275522232, Final Batch Loss: 0.2525448501110077\n",
      "Subject 12, Epoch 401, Loss: 1.0284783989191055, Final Batch Loss: 0.28195247054100037\n",
      "Subject 12, Epoch 402, Loss: 0.9132236391305923, Final Batch Loss: 0.21051177382469177\n",
      "Subject 12, Epoch 403, Loss: 1.036243036389351, Final Batch Loss: 0.3151509761810303\n",
      "Subject 12, Epoch 404, Loss: 0.98949034512043, Final Batch Loss: 0.21853171288967133\n",
      "Subject 12, Epoch 405, Loss: 0.9612811505794525, Final Batch Loss: 0.22009781002998352\n",
      "Subject 12, Epoch 406, Loss: 0.913597360253334, Final Batch Loss: 0.24336861073970795\n",
      "Subject 12, Epoch 407, Loss: 1.1876893639564514, Final Batch Loss: 0.45251211524009705\n",
      "Subject 12, Epoch 408, Loss: 1.0217005163431168, Final Batch Loss: 0.16476280987262726\n",
      "Subject 12, Epoch 409, Loss: 0.7640816867351532, Final Batch Loss: 0.12578558921813965\n",
      "Subject 12, Epoch 410, Loss: 1.0358690023422241, Final Batch Loss: 0.1633647233247757\n",
      "Subject 12, Epoch 411, Loss: 1.0069911479949951, Final Batch Loss: 0.24636676907539368\n",
      "Subject 12, Epoch 412, Loss: 0.9312766790390015, Final Batch Loss: 0.18823041021823883\n",
      "Subject 12, Epoch 413, Loss: 0.9897957593202591, Final Batch Loss: 0.2480286955833435\n",
      "Subject 12, Epoch 414, Loss: 1.003638043999672, Final Batch Loss: 0.3767752945423126\n",
      "Subject 12, Epoch 415, Loss: 0.8658220916986465, Final Batch Loss: 0.1526680290699005\n",
      "Subject 12, Epoch 416, Loss: 0.9944112598896027, Final Batch Loss: 0.22056053578853607\n",
      "Subject 12, Epoch 417, Loss: 1.042428731918335, Final Batch Loss: 0.3379633128643036\n",
      "Subject 12, Epoch 418, Loss: 0.9182341694831848, Final Batch Loss: 0.24547269940376282\n",
      "Subject 12, Epoch 419, Loss: 0.9543571025133133, Final Batch Loss: 0.33395394682884216\n",
      "Subject 12, Epoch 420, Loss: 0.9573316872119904, Final Batch Loss: 0.2317478209733963\n",
      "Subject 12, Epoch 421, Loss: 0.9448120296001434, Final Batch Loss: 0.18304353952407837\n",
      "Subject 12, Epoch 422, Loss: 0.9596970826387405, Final Batch Loss: 0.2597377598285675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 423, Loss: 1.0581276416778564, Final Batch Loss: 0.423585444688797\n",
      "Subject 12, Epoch 424, Loss: 1.0168323665857315, Final Batch Loss: 0.18538197875022888\n",
      "Subject 12, Epoch 425, Loss: 0.9719214141368866, Final Batch Loss: 0.2604632079601288\n",
      "Subject 12, Epoch 426, Loss: 0.9456754922866821, Final Batch Loss: 0.21765078604221344\n",
      "Subject 12, Epoch 427, Loss: 1.0939016789197922, Final Batch Loss: 0.3759070634841919\n",
      "Subject 12, Epoch 428, Loss: 1.1118281036615372, Final Batch Loss: 0.2969338893890381\n",
      "Subject 12, Epoch 429, Loss: 0.9265887439250946, Final Batch Loss: 0.24146722257137299\n",
      "Subject 12, Epoch 430, Loss: 0.9037632942199707, Final Batch Loss: 0.1996859461069107\n",
      "Subject 12, Epoch 431, Loss: 0.9020169079303741, Final Batch Loss: 0.14851807057857513\n",
      "Subject 12, Epoch 432, Loss: 0.9070560485124588, Final Batch Loss: 0.20166745781898499\n",
      "Subject 12, Epoch 433, Loss: 0.9237463772296906, Final Batch Loss: 0.19532307982444763\n",
      "Subject 12, Epoch 434, Loss: 0.7863273471593857, Final Batch Loss: 0.12048943340778351\n",
      "Subject 12, Epoch 435, Loss: 0.8618255257606506, Final Batch Loss: 0.18491289019584656\n",
      "Subject 12, Epoch 436, Loss: 0.9655590355396271, Final Batch Loss: 0.22657474875450134\n",
      "Subject 12, Epoch 437, Loss: 0.9263476580381393, Final Batch Loss: 0.23696517944335938\n",
      "Subject 12, Epoch 438, Loss: 0.9735873341560364, Final Batch Loss: 0.25999924540519714\n",
      "Subject 12, Epoch 439, Loss: 0.8644031584262848, Final Batch Loss: 0.2106148898601532\n",
      "Subject 12, Epoch 440, Loss: 0.8854140490293503, Final Batch Loss: 0.1574927270412445\n",
      "Subject 12, Epoch 441, Loss: 0.9140219837427139, Final Batch Loss: 0.1913304477930069\n",
      "Subject 12, Epoch 442, Loss: 0.8913464248180389, Final Batch Loss: 0.1885363608598709\n",
      "Subject 12, Epoch 443, Loss: 1.0245997309684753, Final Batch Loss: 0.37924110889434814\n",
      "Subject 12, Epoch 444, Loss: 0.8262951374053955, Final Batch Loss: 0.16962873935699463\n",
      "Subject 12, Epoch 445, Loss: 0.9343878626823425, Final Batch Loss: 0.1900225728750229\n",
      "Subject 12, Epoch 446, Loss: 0.8069687336683273, Final Batch Loss: 0.18509778380393982\n",
      "Subject 12, Epoch 447, Loss: 0.9701028764247894, Final Batch Loss: 0.31093063950538635\n",
      "Subject 12, Epoch 448, Loss: 0.945155143737793, Final Batch Loss: 0.19758255779743195\n",
      "Subject 12, Epoch 449, Loss: 0.9680447727441788, Final Batch Loss: 0.2571395933628082\n",
      "Subject 12, Epoch 450, Loss: 0.9262330383062363, Final Batch Loss: 0.24811463057994843\n",
      "Subject 12, Epoch 451, Loss: 0.8422233611345291, Final Batch Loss: 0.19498877227306366\n",
      "Subject 12, Epoch 452, Loss: 0.8945332020521164, Final Batch Loss: 0.13099174201488495\n",
      "Subject 12, Epoch 453, Loss: 0.9546649605035782, Final Batch Loss: 0.24608992040157318\n",
      "Subject 12, Epoch 454, Loss: 1.004556804895401, Final Batch Loss: 0.37261542677879333\n",
      "Subject 12, Epoch 455, Loss: 0.8374188095331192, Final Batch Loss: 0.14592114090919495\n",
      "Subject 12, Epoch 456, Loss: 0.9448064416646957, Final Batch Loss: 0.22385115921497345\n",
      "Subject 12, Epoch 457, Loss: 0.9312684834003448, Final Batch Loss: 0.17119698226451874\n",
      "Subject 12, Epoch 458, Loss: 0.9794778674840927, Final Batch Loss: 0.2793440818786621\n",
      "Subject 12, Epoch 459, Loss: 0.8466541171073914, Final Batch Loss: 0.1340142786502838\n",
      "Subject 12, Epoch 460, Loss: 0.9100897759199142, Final Batch Loss: 0.22034159302711487\n",
      "Subject 12, Epoch 461, Loss: 0.8783234506845474, Final Batch Loss: 0.1976071000099182\n",
      "Subject 12, Epoch 462, Loss: 0.8806443363428116, Final Batch Loss: 0.1428542286157608\n",
      "Subject 12, Epoch 463, Loss: 0.9781991392374039, Final Batch Loss: 0.21982507407665253\n",
      "Subject 12, Epoch 464, Loss: 0.8841767609119415, Final Batch Loss: 0.18060410022735596\n",
      "Subject 12, Epoch 465, Loss: 0.9627072066068649, Final Batch Loss: 0.30091893672943115\n",
      "Subject 12, Epoch 466, Loss: 0.914066344499588, Final Batch Loss: 0.17910747230052948\n",
      "Subject 12, Epoch 467, Loss: 0.9384569525718689, Final Batch Loss: 0.1691068410873413\n",
      "Subject 12, Epoch 468, Loss: 1.113663837313652, Final Batch Loss: 0.3578488230705261\n",
      "Subject 12, Epoch 469, Loss: 1.0422549098730087, Final Batch Loss: 0.28788208961486816\n",
      "Subject 12, Epoch 470, Loss: 0.857128843665123, Final Batch Loss: 0.18434806168079376\n",
      "Subject 12, Epoch 471, Loss: 0.8299532830715179, Final Batch Loss: 0.1816151887178421\n",
      "Subject 12, Epoch 472, Loss: 0.9529630988836288, Final Batch Loss: 0.28083640336990356\n",
      "Subject 12, Epoch 473, Loss: 0.8463780134916306, Final Batch Loss: 0.17085976898670197\n",
      "Subject 12, Epoch 474, Loss: 0.799333930015564, Final Batch Loss: 0.13936133682727814\n",
      "Subject 12, Epoch 475, Loss: 0.9546773731708527, Final Batch Loss: 0.2855317294597626\n",
      "Subject 12, Epoch 476, Loss: 0.8100319057703018, Final Batch Loss: 0.11990337073802948\n",
      "Subject 12, Epoch 477, Loss: 0.9857956320047379, Final Batch Loss: 0.2512415945529938\n",
      "Subject 12, Epoch 478, Loss: 0.904878094792366, Final Batch Loss: 0.21003341674804688\n",
      "Subject 12, Epoch 479, Loss: 0.7934326976537704, Final Batch Loss: 0.13790132105350494\n",
      "Subject 12, Epoch 480, Loss: 0.8473353236913681, Final Batch Loss: 0.30976754426956177\n",
      "Subject 12, Epoch 481, Loss: 0.8612902164459229, Final Batch Loss: 0.13048382103443146\n",
      "Subject 12, Epoch 482, Loss: 0.9517210870981216, Final Batch Loss: 0.33750230073928833\n",
      "Subject 12, Epoch 483, Loss: 0.9425055682659149, Final Batch Loss: 0.24206717312335968\n",
      "Subject 12, Epoch 484, Loss: 0.8984442055225372, Final Batch Loss: 0.23988792300224304\n",
      "Subject 12, Epoch 485, Loss: 0.9671765863895416, Final Batch Loss: 0.2062409669160843\n",
      "Subject 12, Epoch 486, Loss: 0.9182585179805756, Final Batch Loss: 0.21801604330539703\n",
      "Subject 12, Epoch 487, Loss: 0.9904333353042603, Final Batch Loss: 0.3202441930770874\n",
      "Subject 12, Epoch 488, Loss: 0.81676085293293, Final Batch Loss: 0.19187574088573456\n",
      "Subject 12, Epoch 489, Loss: 0.933892011642456, Final Batch Loss: 0.2616210877895355\n",
      "Subject 12, Epoch 490, Loss: 0.9806079864501953, Final Batch Loss: 0.303579717874527\n",
      "Subject 12, Epoch 491, Loss: 0.882748544216156, Final Batch Loss: 0.11516366899013519\n",
      "Subject 12, Epoch 492, Loss: 0.8405501842498779, Final Batch Loss: 0.18129980564117432\n",
      "Subject 12, Epoch 493, Loss: 1.184203028678894, Final Batch Loss: 0.14828616380691528\n",
      "Subject 12, Epoch 494, Loss: 0.9193118810653687, Final Batch Loss: 0.27972543239593506\n",
      "Subject 12, Epoch 495, Loss: 0.9201194643974304, Final Batch Loss: 0.3347545266151428\n",
      "Subject 12, Epoch 496, Loss: 0.8204985111951828, Final Batch Loss: 0.26922333240509033\n",
      "Subject 12, Epoch 497, Loss: 0.8040491789579391, Final Batch Loss: 0.16037921607494354\n",
      "Subject 12, Epoch 498, Loss: 0.9254190325737, Final Batch Loss: 0.2297443449497223\n",
      "Subject 12, Epoch 499, Loss: 0.9461236894130707, Final Batch Loss: 0.14188531041145325\n",
      "Subject 12, Epoch 500, Loss: 0.842139944434166, Final Batch Loss: 0.1512995958328247\n",
      "Subject 12, Epoch 501, Loss: 0.8556479662656784, Final Batch Loss: 0.17420223355293274\n",
      "Subject 12, Epoch 502, Loss: 0.835199162364006, Final Batch Loss: 0.14794683456420898\n",
      "Subject 12, Epoch 503, Loss: 0.9146454483270645, Final Batch Loss: 0.2599477469921112\n",
      "Subject 12, Epoch 504, Loss: 1.1109649240970612, Final Batch Loss: 0.39767616987228394\n",
      "Subject 12, Epoch 505, Loss: 0.777371421456337, Final Batch Loss: 0.1392543613910675\n",
      "Subject 12, Epoch 506, Loss: 0.8195911645889282, Final Batch Loss: 0.1642349660396576\n",
      "Subject 12, Epoch 507, Loss: 0.8790615499019623, Final Batch Loss: 0.19160258769989014\n",
      "Subject 12, Epoch 508, Loss: 0.9050905257463455, Final Batch Loss: 0.16597889363765717\n",
      "Subject 12, Epoch 509, Loss: 0.8966261744499207, Final Batch Loss: 0.21580633521080017\n",
      "Subject 12, Epoch 510, Loss: 0.8590880781412125, Final Batch Loss: 0.25768572092056274\n",
      "Subject 12, Epoch 511, Loss: 0.8661595284938812, Final Batch Loss: 0.21998056769371033\n",
      "Subject 12, Epoch 512, Loss: 0.937643975019455, Final Batch Loss: 0.27529412508010864\n",
      "Subject 12, Epoch 513, Loss: 0.776597149670124, Final Batch Loss: 0.15764285624027252\n",
      "Subject 12, Epoch 514, Loss: 0.9218091368675232, Final Batch Loss: 0.2792932987213135\n",
      "Subject 12, Epoch 515, Loss: 0.9120428115129471, Final Batch Loss: 0.2535054087638855\n",
      "Subject 12, Epoch 516, Loss: 1.0172888934612274, Final Batch Loss: 0.46364626288414\n",
      "Subject 12, Epoch 517, Loss: 0.8433214649558067, Final Batch Loss: 0.11628887802362442\n",
      "Subject 12, Epoch 518, Loss: 0.9543614387512207, Final Batch Loss: 0.27802616357803345\n",
      "Subject 12, Epoch 519, Loss: 0.9345564097166061, Final Batch Loss: 0.25141942501068115\n",
      "Subject 12, Epoch 520, Loss: 0.8808058947324753, Final Batch Loss: 0.21077093482017517\n",
      "Subject 12, Epoch 521, Loss: 0.8795353472232819, Final Batch Loss: 0.21801888942718506\n",
      "Subject 12, Epoch 522, Loss: 0.9872689545154572, Final Batch Loss: 0.23744697868824005\n",
      "Subject 12, Epoch 523, Loss: 0.8507348150014877, Final Batch Loss: 0.19110393524169922\n",
      "Subject 12, Epoch 524, Loss: 0.8852904289960861, Final Batch Loss: 0.18132935464382172\n",
      "Subject 12, Epoch 525, Loss: 0.8345093131065369, Final Batch Loss: 0.16057948768138885\n",
      "Subject 12, Epoch 526, Loss: 0.9036172032356262, Final Batch Loss: 0.22670231759548187\n",
      "Subject 12, Epoch 527, Loss: 0.7517029494047165, Final Batch Loss: 0.11680248379707336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 528, Loss: 0.818900465965271, Final Batch Loss: 0.13949082791805267\n",
      "Subject 12, Epoch 529, Loss: 0.8377082198858261, Final Batch Loss: 0.13821996748447418\n",
      "Subject 12, Epoch 530, Loss: 0.8492553681135178, Final Batch Loss: 0.22447168827056885\n",
      "Subject 12, Epoch 531, Loss: 1.0110361576080322, Final Batch Loss: 0.3595713675022125\n",
      "Subject 12, Epoch 532, Loss: 0.8417733609676361, Final Batch Loss: 0.16021177172660828\n",
      "Subject 12, Epoch 533, Loss: 0.8428408205509186, Final Batch Loss: 0.2650052011013031\n",
      "Subject 12, Epoch 534, Loss: 0.8116387575864792, Final Batch Loss: 0.17625761032104492\n",
      "Subject 12, Epoch 535, Loss: 0.8003408461809158, Final Batch Loss: 0.12221579253673553\n",
      "Subject 12, Epoch 536, Loss: 0.8112825304269791, Final Batch Loss: 0.17673780024051666\n",
      "Subject 12, Epoch 537, Loss: 0.7849755585193634, Final Batch Loss: 0.21320046484470367\n",
      "Subject 12, Epoch 538, Loss: 0.8588939905166626, Final Batch Loss: 0.24725227057933807\n",
      "Subject 12, Epoch 539, Loss: 0.8341371193528175, Final Batch Loss: 0.12212922424077988\n",
      "Subject 12, Epoch 540, Loss: 0.8193237036466599, Final Batch Loss: 0.1929512619972229\n",
      "Subject 12, Epoch 541, Loss: 0.8919015377759933, Final Batch Loss: 0.2090165764093399\n",
      "Subject 12, Epoch 542, Loss: 0.8169725835323334, Final Batch Loss: 0.18768520653247833\n",
      "Subject 12, Epoch 543, Loss: 0.706989549100399, Final Batch Loss: 0.10135900229215622\n",
      "Subject 12, Epoch 544, Loss: 1.0466940701007843, Final Batch Loss: 0.2433347851037979\n",
      "Subject 12, Epoch 545, Loss: 0.7576320245862007, Final Batch Loss: 0.10843143612146378\n",
      "Subject 12, Epoch 546, Loss: 0.8325373977422714, Final Batch Loss: 0.23744067549705505\n",
      "Subject 12, Epoch 547, Loss: 0.8675218373537064, Final Batch Loss: 0.18378522992134094\n",
      "Subject 12, Epoch 548, Loss: 0.9227640479803085, Final Batch Loss: 0.2977885603904724\n",
      "Subject 12, Epoch 549, Loss: 0.8573689013719559, Final Batch Loss: 0.24560929834842682\n",
      "Subject 12, Epoch 550, Loss: 0.7887445986270905, Final Batch Loss: 0.15499044954776764\n",
      "Subject 12, Epoch 551, Loss: 0.9206870347261429, Final Batch Loss: 0.19118288159370422\n",
      "Subject 12, Epoch 552, Loss: 0.7890345454216003, Final Batch Loss: 0.2202446609735489\n",
      "Subject 12, Epoch 553, Loss: 0.818353921175003, Final Batch Loss: 0.16511084139347076\n",
      "Subject 12, Epoch 554, Loss: 0.8299235701560974, Final Batch Loss: 0.14589953422546387\n",
      "Subject 12, Epoch 555, Loss: 0.9080572575330734, Final Batch Loss: 0.2437908798456192\n",
      "Subject 12, Epoch 556, Loss: 0.8955569714307785, Final Batch Loss: 0.28489360213279724\n",
      "Subject 12, Epoch 557, Loss: 0.8066961020231247, Final Batch Loss: 0.16631342470645905\n",
      "Subject 12, Epoch 558, Loss: 0.867437556385994, Final Batch Loss: 0.23303385078907013\n",
      "Subject 12, Epoch 559, Loss: 0.8481764495372772, Final Batch Loss: 0.27920082211494446\n",
      "Subject 12, Epoch 560, Loss: 0.8979806452989578, Final Batch Loss: 0.3321234881877899\n",
      "Subject 12, Epoch 561, Loss: 0.7262577712535858, Final Batch Loss: 0.06540067493915558\n",
      "Subject 12, Epoch 562, Loss: 0.8138371556997299, Final Batch Loss: 0.10436592996120453\n",
      "Subject 12, Epoch 563, Loss: 0.828899472951889, Final Batch Loss: 0.23810112476348877\n",
      "Subject 12, Epoch 564, Loss: 0.9613348245620728, Final Batch Loss: 0.20997989177703857\n",
      "Subject 12, Epoch 565, Loss: 0.9301649332046509, Final Batch Loss: 0.2930678725242615\n",
      "Subject 12, Epoch 566, Loss: 0.7725775986909866, Final Batch Loss: 0.1277935951948166\n",
      "Subject 12, Epoch 567, Loss: 0.7003170102834702, Final Batch Loss: 0.1279103010892868\n",
      "Subject 12, Epoch 568, Loss: 0.9391043335199356, Final Batch Loss: 0.40916791558265686\n",
      "Subject 12, Epoch 569, Loss: 0.7345719635486603, Final Batch Loss: 0.14307120442390442\n",
      "Subject 12, Epoch 570, Loss: 0.8328711837530136, Final Batch Loss: 0.16032841801643372\n",
      "Subject 12, Epoch 571, Loss: 0.8169527649879456, Final Batch Loss: 0.2109440118074417\n",
      "Subject 12, Epoch 572, Loss: 0.7754346281290054, Final Batch Loss: 0.17815013229846954\n",
      "Subject 12, Epoch 573, Loss: 0.9031683653593063, Final Batch Loss: 0.2175699770450592\n",
      "Subject 12, Epoch 574, Loss: 0.8481881320476532, Final Batch Loss: 0.13792775571346283\n",
      "Subject 12, Epoch 575, Loss: 0.9229037016630173, Final Batch Loss: 0.3061997592449188\n",
      "Subject 12, Epoch 576, Loss: 0.7426002323627472, Final Batch Loss: 0.15835998952388763\n",
      "Subject 12, Epoch 577, Loss: 0.7849360406398773, Final Batch Loss: 0.2217901200056076\n",
      "Subject 12, Epoch 578, Loss: 0.7368793860077858, Final Batch Loss: 0.09778473526239395\n",
      "Subject 12, Epoch 579, Loss: 0.7758137658238411, Final Batch Loss: 0.11829905956983566\n",
      "Subject 12, Epoch 580, Loss: 0.764736607670784, Final Batch Loss: 0.13232937455177307\n",
      "Subject 12, Epoch 581, Loss: 0.7310857772827148, Final Batch Loss: 0.13337942957878113\n",
      "Subject 12, Epoch 582, Loss: 0.8558786660432816, Final Batch Loss: 0.18805281817913055\n",
      "Subject 12, Epoch 583, Loss: 0.8427512347698212, Final Batch Loss: 0.2185685932636261\n",
      "Subject 12, Epoch 584, Loss: 0.7631369978189468, Final Batch Loss: 0.21758081018924713\n",
      "Subject 12, Epoch 585, Loss: 0.9020572304725647, Final Batch Loss: 0.18981429934501648\n",
      "Subject 12, Epoch 586, Loss: 0.9175847470760345, Final Batch Loss: 0.3034152686595917\n",
      "Subject 12, Epoch 587, Loss: 0.8739066123962402, Final Batch Loss: 0.16514445841312408\n",
      "Subject 12, Epoch 588, Loss: 0.8448548465967178, Final Batch Loss: 0.2365567833185196\n",
      "Subject 12, Epoch 589, Loss: 0.8827735930681229, Final Batch Loss: 0.16914992034435272\n",
      "Subject 12, Epoch 590, Loss: 0.7434247955679893, Final Batch Loss: 0.10873817652463913\n",
      "Subject 12, Epoch 591, Loss: 0.7378276586532593, Final Batch Loss: 0.10765273869037628\n",
      "Subject 12, Epoch 592, Loss: 0.9118025749921799, Final Batch Loss: 0.20240989327430725\n",
      "Subject 12, Epoch 593, Loss: 0.6932682991027832, Final Batch Loss: 0.12755264341831207\n",
      "Subject 12, Epoch 594, Loss: 0.7300165817141533, Final Batch Loss: 0.0634634867310524\n",
      "Subject 12, Epoch 595, Loss: 0.7044638246297836, Final Batch Loss: 0.11956827342510223\n",
      "Subject 12, Epoch 596, Loss: 0.7425416857004166, Final Batch Loss: 0.16224978864192963\n",
      "Subject 12, Epoch 597, Loss: 0.7982871830463409, Final Batch Loss: 0.18105702102184296\n",
      "Subject 12, Epoch 598, Loss: 0.7592410296201706, Final Batch Loss: 0.19081327319145203\n",
      "Subject 12, Epoch 599, Loss: 0.749679759144783, Final Batch Loss: 0.20139028131961823\n",
      "Subject 12, Epoch 600, Loss: 0.693540170788765, Final Batch Loss: 0.11823967099189758\n",
      "Subject 12, Epoch 601, Loss: 0.7991486191749573, Final Batch Loss: 0.1261201798915863\n",
      "Subject 12, Epoch 602, Loss: 0.8034069687128067, Final Batch Loss: 0.24285955727100372\n",
      "Subject 12, Epoch 603, Loss: 0.772514671087265, Final Batch Loss: 0.16802732646465302\n",
      "Subject 12, Epoch 604, Loss: 0.8065238893032074, Final Batch Loss: 0.23206891119480133\n",
      "Subject 12, Epoch 605, Loss: 0.6828066632151604, Final Batch Loss: 0.06883084028959274\n",
      "Subject 12, Epoch 606, Loss: 0.9527131468057632, Final Batch Loss: 0.30623888969421387\n",
      "Subject 12, Epoch 607, Loss: 0.8291012048721313, Final Batch Loss: 0.1566580981016159\n",
      "Subject 12, Epoch 608, Loss: 0.8915640413761139, Final Batch Loss: 0.2749002277851105\n",
      "Subject 12, Epoch 609, Loss: 0.8869631737470627, Final Batch Loss: 0.19497008621692657\n",
      "Subject 12, Epoch 610, Loss: 0.9141219854354858, Final Batch Loss: 0.4011119306087494\n",
      "Subject 12, Epoch 611, Loss: 0.874399408698082, Final Batch Loss: 0.17234596610069275\n",
      "Subject 12, Epoch 612, Loss: 0.711011253297329, Final Batch Loss: 0.10969720035791397\n",
      "Subject 12, Epoch 613, Loss: 0.7205567210912704, Final Batch Loss: 0.16952434182167053\n",
      "Subject 12, Epoch 614, Loss: 0.7412184774875641, Final Batch Loss: 0.14089620113372803\n",
      "Subject 12, Epoch 615, Loss: 0.8670603036880493, Final Batch Loss: 0.2433689385652542\n",
      "Subject 12, Epoch 616, Loss: 0.7214209660887718, Final Batch Loss: 0.10383760184049606\n",
      "Subject 12, Epoch 617, Loss: 0.7448740303516388, Final Batch Loss: 0.14174582064151764\n",
      "Subject 12, Epoch 618, Loss: 0.8990576416254044, Final Batch Loss: 0.29978305101394653\n",
      "Subject 12, Epoch 619, Loss: 0.8397603183984756, Final Batch Loss: 0.31090936064720154\n",
      "Subject 12, Epoch 620, Loss: 0.8358946293592453, Final Batch Loss: 0.13581062853336334\n",
      "Subject 12, Epoch 621, Loss: 0.824426993727684, Final Batch Loss: 0.29972508549690247\n",
      "Subject 12, Epoch 622, Loss: 0.7480693906545639, Final Batch Loss: 0.21160581707954407\n",
      "Subject 12, Epoch 623, Loss: 0.6710030734539032, Final Batch Loss: 0.1539437174797058\n",
      "Subject 12, Epoch 624, Loss: 0.7433958873152733, Final Batch Loss: 0.2285737842321396\n",
      "Subject 12, Epoch 625, Loss: 0.7229570597410202, Final Batch Loss: 0.17978164553642273\n",
      "Subject 12, Epoch 626, Loss: 0.7927582412958145, Final Batch Loss: 0.1756420135498047\n",
      "Subject 12, Epoch 627, Loss: 0.8461200967431068, Final Batch Loss: 0.21636821329593658\n",
      "Subject 12, Epoch 628, Loss: 0.7227374613285065, Final Batch Loss: 0.17021813988685608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 629, Loss: 0.6659386903047562, Final Batch Loss: 0.12687444686889648\n",
      "Subject 12, Epoch 630, Loss: 0.734135240316391, Final Batch Loss: 0.17221511900424957\n",
      "Subject 12, Epoch 631, Loss: 0.6883626580238342, Final Batch Loss: 0.11675924062728882\n",
      "Subject 12, Epoch 632, Loss: 0.8026831448078156, Final Batch Loss: 0.1581980139017105\n",
      "Subject 12, Epoch 633, Loss: 0.9102945327758789, Final Batch Loss: 0.31283068656921387\n",
      "Subject 12, Epoch 634, Loss: 0.6849393472075462, Final Batch Loss: 0.08481629937887192\n",
      "Subject 12, Epoch 635, Loss: 0.8243028372526169, Final Batch Loss: 0.21210941672325134\n",
      "Subject 12, Epoch 636, Loss: 0.717794731259346, Final Batch Loss: 0.1525755226612091\n",
      "Subject 12, Epoch 637, Loss: 0.8278592526912689, Final Batch Loss: 0.2183464765548706\n",
      "Subject 12, Epoch 638, Loss: 0.8210210651159286, Final Batch Loss: 0.21228106319904327\n",
      "Subject 12, Epoch 639, Loss: 0.830229714512825, Final Batch Loss: 0.26860061287879944\n",
      "Subject 12, Epoch 640, Loss: 0.8234205842018127, Final Batch Loss: 0.24044401943683624\n",
      "Subject 12, Epoch 641, Loss: 0.6683111637830734, Final Batch Loss: 0.19461943209171295\n",
      "Subject 12, Epoch 642, Loss: 0.8362023532390594, Final Batch Loss: 0.15642209351062775\n",
      "Subject 12, Epoch 643, Loss: 0.6642882227897644, Final Batch Loss: 0.19433364272117615\n",
      "Subject 12, Epoch 644, Loss: 0.7258479595184326, Final Batch Loss: 0.19318899512290955\n",
      "Subject 12, Epoch 645, Loss: 0.787486270070076, Final Batch Loss: 0.20024904608726501\n",
      "Subject 12, Epoch 646, Loss: 0.7087508291006088, Final Batch Loss: 0.14021779596805573\n",
      "Subject 12, Epoch 647, Loss: 0.7986178994178772, Final Batch Loss: 0.19523033499717712\n",
      "Subject 12, Epoch 648, Loss: 0.7445862144231796, Final Batch Loss: 0.14998546242713928\n",
      "Subject 12, Epoch 649, Loss: 0.7206644788384438, Final Batch Loss: 0.09570804983377457\n",
      "Subject 12, Epoch 650, Loss: 0.8107815906405449, Final Batch Loss: 0.31203150749206543\n",
      "Subject 12, Epoch 651, Loss: 0.7554179131984711, Final Batch Loss: 0.27415192127227783\n",
      "Subject 12, Epoch 652, Loss: 0.7451355010271072, Final Batch Loss: 0.13795353472232819\n",
      "Subject 12, Epoch 653, Loss: 0.7375444173812866, Final Batch Loss: 0.1981896311044693\n",
      "Subject 12, Epoch 654, Loss: 0.8738675117492676, Final Batch Loss: 0.310343474149704\n",
      "Subject 12, Epoch 655, Loss: 0.7423709034919739, Final Batch Loss: 0.18158863484859467\n",
      "Subject 12, Epoch 656, Loss: 0.6932422071695328, Final Batch Loss: 0.21918602287769318\n",
      "Subject 12, Epoch 657, Loss: 0.7223050445318222, Final Batch Loss: 0.15959089994430542\n",
      "Subject 12, Epoch 658, Loss: 0.7448008954524994, Final Batch Loss: 0.21746937930583954\n",
      "Subject 12, Epoch 659, Loss: 0.7804233580827713, Final Batch Loss: 0.21011754870414734\n",
      "Subject 12, Epoch 660, Loss: 0.7536976039409637, Final Batch Loss: 0.1666804403066635\n",
      "Subject 12, Epoch 661, Loss: 0.6740334928035736, Final Batch Loss: 0.13397684693336487\n",
      "Subject 12, Epoch 662, Loss: 0.7602953314781189, Final Batch Loss: 0.20035229623317719\n",
      "Subject 12, Epoch 663, Loss: 0.75431127846241, Final Batch Loss: 0.14040617644786835\n",
      "Subject 12, Epoch 664, Loss: 0.6514328718185425, Final Batch Loss: 0.1416189968585968\n",
      "Subject 12, Epoch 665, Loss: 0.80230413377285, Final Batch Loss: 0.27787911891937256\n",
      "Subject 12, Epoch 666, Loss: 0.8525659888982773, Final Batch Loss: 0.18440255522727966\n",
      "Subject 12, Epoch 667, Loss: 0.7353980392217636, Final Batch Loss: 0.250537633895874\n",
      "Subject 12, Epoch 668, Loss: 0.6730362325906754, Final Batch Loss: 0.16554953157901764\n",
      "Subject 12, Epoch 669, Loss: 0.7050016969442368, Final Batch Loss: 0.17389145493507385\n",
      "Subject 12, Epoch 670, Loss: 0.7471601516008377, Final Batch Loss: 0.14199496805667877\n",
      "Subject 12, Epoch 671, Loss: 0.8452817648649216, Final Batch Loss: 0.2212107628583908\n",
      "Subject 12, Epoch 672, Loss: 0.7576389387249947, Final Batch Loss: 0.2495952844619751\n",
      "Subject 12, Epoch 673, Loss: 0.7905740588903427, Final Batch Loss: 0.17324763536453247\n",
      "Subject 12, Epoch 674, Loss: 0.7794847935438156, Final Batch Loss: 0.18174098432064056\n",
      "Subject 12, Epoch 675, Loss: 0.7202758491039276, Final Batch Loss: 0.07398635149002075\n",
      "Subject 12, Epoch 676, Loss: 0.8322290629148483, Final Batch Loss: 0.1877768337726593\n",
      "Subject 12, Epoch 677, Loss: 0.7378173172473907, Final Batch Loss: 0.18817569315433502\n",
      "Subject 12, Epoch 678, Loss: 0.857820600271225, Final Batch Loss: 0.1999186873435974\n",
      "Subject 12, Epoch 679, Loss: 0.6830434501171112, Final Batch Loss: 0.1613629311323166\n",
      "Subject 12, Epoch 680, Loss: 0.6659667044878006, Final Batch Loss: 0.18543711304664612\n",
      "Subject 12, Epoch 681, Loss: 0.7823616564273834, Final Batch Loss: 0.20563767850399017\n",
      "Subject 12, Epoch 682, Loss: 0.7377365529537201, Final Batch Loss: 0.13989907503128052\n",
      "Subject 12, Epoch 683, Loss: 0.7009758502244949, Final Batch Loss: 0.16000716388225555\n",
      "Subject 12, Epoch 684, Loss: 0.6954047009348869, Final Batch Loss: 0.11648639291524887\n",
      "Subject 12, Epoch 685, Loss: 0.7692699655890465, Final Batch Loss: 0.2018633931875229\n",
      "Subject 12, Epoch 686, Loss: 0.7645674645900726, Final Batch Loss: 0.1260831505060196\n",
      "Subject 12, Epoch 687, Loss: 0.7470113337039948, Final Batch Loss: 0.17156219482421875\n",
      "Subject 12, Epoch 688, Loss: 0.808793880045414, Final Batch Loss: 0.12312258034944534\n",
      "Subject 12, Epoch 689, Loss: 0.7184319868683815, Final Batch Loss: 0.14172030985355377\n",
      "Subject 12, Epoch 690, Loss: 0.6885800659656525, Final Batch Loss: 0.1445675492286682\n",
      "Subject 12, Epoch 691, Loss: 0.6830410733819008, Final Batch Loss: 0.1915082484483719\n",
      "Subject 12, Epoch 692, Loss: 0.770753026008606, Final Batch Loss: 0.27256715297698975\n",
      "Subject 12, Epoch 693, Loss: 0.8129638284444809, Final Batch Loss: 0.17664651572704315\n",
      "Subject 12, Epoch 694, Loss: 0.6317491233348846, Final Batch Loss: 0.2281244993209839\n",
      "Subject 12, Epoch 695, Loss: 0.702291801571846, Final Batch Loss: 0.1511390507221222\n",
      "Subject 12, Epoch 696, Loss: 0.8397530019283295, Final Batch Loss: 0.2846376597881317\n",
      "Subject 12, Epoch 697, Loss: 0.6808224022388458, Final Batch Loss: 0.15493173897266388\n",
      "Subject 12, Epoch 698, Loss: 0.7323244214057922, Final Batch Loss: 0.1742566078901291\n",
      "Subject 12, Epoch 699, Loss: 0.711698517203331, Final Batch Loss: 0.18429134786128998\n",
      "Subject 12, Epoch 700, Loss: 0.746583878993988, Final Batch Loss: 0.24299389123916626\n",
      "Subject 12, Epoch 701, Loss: 0.7937122955918312, Final Batch Loss: 0.12419811636209488\n",
      "Subject 12, Epoch 702, Loss: 0.7826316207647324, Final Batch Loss: 0.13494077324867249\n",
      "Subject 12, Epoch 703, Loss: 0.9580399543046951, Final Batch Loss: 0.22958341240882874\n",
      "Subject 12, Epoch 704, Loss: 0.7288970500230789, Final Batch Loss: 0.13261447846889496\n",
      "Subject 12, Epoch 705, Loss: 0.8480182513594627, Final Batch Loss: 0.4433199465274811\n",
      "Subject 12, Epoch 706, Loss: 0.7027008682489395, Final Batch Loss: 0.1750158667564392\n",
      "Subject 12, Epoch 707, Loss: 0.6114648431539536, Final Batch Loss: 0.10483264178037643\n",
      "Subject 12, Epoch 708, Loss: 0.7723465859889984, Final Batch Loss: 0.2073267698287964\n",
      "Subject 12, Epoch 709, Loss: 0.8105632811784744, Final Batch Loss: 0.17844019830226898\n",
      "Subject 12, Epoch 710, Loss: 0.609975054860115, Final Batch Loss: 0.05722048878669739\n",
      "Subject 12, Epoch 711, Loss: 0.6947617381811142, Final Batch Loss: 0.13802951574325562\n",
      "Subject 12, Epoch 712, Loss: 0.7230546027421951, Final Batch Loss: 0.13510717451572418\n",
      "Subject 12, Epoch 713, Loss: 0.7851433157920837, Final Batch Loss: 0.212643101811409\n",
      "Subject 12, Epoch 714, Loss: 0.6585157662630081, Final Batch Loss: 0.1791037619113922\n",
      "Subject 12, Epoch 715, Loss: 0.7702090442180634, Final Batch Loss: 0.25169146060943604\n",
      "Subject 12, Epoch 716, Loss: 0.6583317369222641, Final Batch Loss: 0.16306163370609283\n",
      "Subject 12, Epoch 717, Loss: 0.7635351270437241, Final Batch Loss: 0.2662058174610138\n",
      "Subject 12, Epoch 718, Loss: 0.7273852005600929, Final Batch Loss: 0.19433049857616425\n",
      "Subject 12, Epoch 719, Loss: 0.6855475455522537, Final Batch Loss: 0.1683437079191208\n",
      "Subject 12, Epoch 720, Loss: 0.638630174100399, Final Batch Loss: 0.09516609460115433\n",
      "Subject 12, Epoch 721, Loss: 0.608812727034092, Final Batch Loss: 0.09459235519170761\n",
      "Subject 12, Epoch 722, Loss: 0.6064245328307152, Final Batch Loss: 0.08133552223443985\n",
      "Subject 12, Epoch 723, Loss: 0.6548391729593277, Final Batch Loss: 0.21584917604923248\n",
      "Subject 12, Epoch 724, Loss: 0.5786373615264893, Final Batch Loss: 0.11006467044353485\n",
      "Subject 12, Epoch 725, Loss: 0.7197127491235733, Final Batch Loss: 0.19865703582763672\n",
      "Subject 12, Epoch 726, Loss: 0.6735389232635498, Final Batch Loss: 0.23428955674171448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 727, Loss: 0.6416147351264954, Final Batch Loss: 0.09487123787403107\n",
      "Subject 12, Epoch 728, Loss: 0.647503986954689, Final Batch Loss: 0.19807979464530945\n",
      "Subject 12, Epoch 729, Loss: 0.5679107829928398, Final Batch Loss: 0.06485819071531296\n",
      "Subject 12, Epoch 730, Loss: 0.669027216732502, Final Batch Loss: 0.12269104272127151\n",
      "Subject 12, Epoch 731, Loss: 0.8226457387208939, Final Batch Loss: 0.28738999366760254\n",
      "Subject 12, Epoch 732, Loss: 0.7357586175203323, Final Batch Loss: 0.25875648856163025\n",
      "Subject 12, Epoch 733, Loss: 0.718022808432579, Final Batch Loss: 0.21306277811527252\n",
      "Subject 12, Epoch 734, Loss: 0.55610541254282, Final Batch Loss: 0.09646741300821304\n",
      "Subject 12, Epoch 735, Loss: 0.7829945534467697, Final Batch Loss: 0.27950963377952576\n",
      "Subject 12, Epoch 736, Loss: 0.6861518621444702, Final Batch Loss: 0.2065296620130539\n",
      "Subject 12, Epoch 737, Loss: 0.6616470068693161, Final Batch Loss: 0.27089303731918335\n",
      "Subject 12, Epoch 738, Loss: 0.7379148155450821, Final Batch Loss: 0.20243625342845917\n",
      "Subject 12, Epoch 739, Loss: 0.6317967176437378, Final Batch Loss: 0.09446988999843597\n",
      "Subject 12, Epoch 740, Loss: 0.7116418331861496, Final Batch Loss: 0.14482207596302032\n",
      "Subject 12, Epoch 741, Loss: 1.092099629342556, Final Batch Loss: 0.49108147621154785\n",
      "Subject 12, Epoch 742, Loss: 0.6050055772066116, Final Batch Loss: 0.16026724874973297\n",
      "Subject 12, Epoch 743, Loss: 0.5917741730809212, Final Batch Loss: 0.10562071949243546\n",
      "Subject 12, Epoch 744, Loss: 0.7305833101272583, Final Batch Loss: 0.18286754190921783\n",
      "Subject 12, Epoch 745, Loss: 0.5957359969615936, Final Batch Loss: 0.11117977648973465\n",
      "Subject 12, Epoch 746, Loss: 0.5810807272791862, Final Batch Loss: 0.11365880817174911\n",
      "Subject 12, Epoch 747, Loss: 0.7548680007457733, Final Batch Loss: 0.31256338953971863\n",
      "Subject 12, Epoch 748, Loss: 0.672264888882637, Final Batch Loss: 0.1359930783510208\n",
      "Subject 12, Epoch 749, Loss: 0.6918139904737473, Final Batch Loss: 0.10974903404712677\n",
      "Subject 12, Epoch 750, Loss: 0.670517310500145, Final Batch Loss: 0.19168177247047424\n",
      "Subject 12, Epoch 751, Loss: 0.6753255352377892, Final Batch Loss: 0.18048210442066193\n",
      "Subject 12, Epoch 752, Loss: 0.7002669125795364, Final Batch Loss: 0.12069837749004364\n",
      "Subject 12, Epoch 753, Loss: 0.6509909853339195, Final Batch Loss: 0.17252585291862488\n",
      "Subject 12, Epoch 754, Loss: 0.6836635321378708, Final Batch Loss: 0.2135343998670578\n",
      "Subject 12, Epoch 755, Loss: 0.6917469203472137, Final Batch Loss: 0.32518285512924194\n",
      "Subject 12, Epoch 756, Loss: 0.6859503388404846, Final Batch Loss: 0.1256016343832016\n",
      "Subject 12, Epoch 757, Loss: 0.6736492812633514, Final Batch Loss: 0.17494557797908783\n",
      "Subject 12, Epoch 758, Loss: 0.552777536213398, Final Batch Loss: 0.04468271881341934\n",
      "Subject 12, Epoch 759, Loss: 0.6215326339006424, Final Batch Loss: 0.2518421709537506\n",
      "Subject 12, Epoch 760, Loss: 0.7571441978216171, Final Batch Loss: 0.24133369326591492\n",
      "Subject 12, Epoch 761, Loss: 0.72338005900383, Final Batch Loss: 0.16182103753089905\n",
      "Subject 12, Epoch 762, Loss: 0.6716197431087494, Final Batch Loss: 0.21111944317817688\n",
      "Subject 12, Epoch 763, Loss: 0.707317978143692, Final Batch Loss: 0.22131434082984924\n",
      "Subject 12, Epoch 764, Loss: 0.6509101539850235, Final Batch Loss: 0.12080028653144836\n",
      "Subject 12, Epoch 765, Loss: 0.5946918204426765, Final Batch Loss: 0.1248999610543251\n",
      "Subject 12, Epoch 766, Loss: 0.6787598133087158, Final Batch Loss: 0.1210489273071289\n",
      "Subject 12, Epoch 767, Loss: 0.6900901794433594, Final Batch Loss: 0.2190716564655304\n",
      "Subject 12, Epoch 768, Loss: 0.7651216685771942, Final Batch Loss: 0.11407600343227386\n",
      "Subject 12, Epoch 769, Loss: 0.5714757442474365, Final Batch Loss: 0.13157223165035248\n",
      "Subject 12, Epoch 770, Loss: 0.6473506838083267, Final Batch Loss: 0.16537010669708252\n",
      "Subject 12, Epoch 771, Loss: 0.6008208841085434, Final Batch Loss: 0.06668485701084137\n",
      "Subject 12, Epoch 772, Loss: 0.6098441109061241, Final Batch Loss: 0.11028780788183212\n",
      "Subject 12, Epoch 773, Loss: 0.8842434883117676, Final Batch Loss: 0.3459552228450775\n",
      "Subject 12, Epoch 774, Loss: 0.6177537590265274, Final Batch Loss: 0.1365480273962021\n",
      "Subject 12, Epoch 775, Loss: 0.6753371059894562, Final Batch Loss: 0.12180383503437042\n",
      "Subject 12, Epoch 776, Loss: 0.7023766189813614, Final Batch Loss: 0.2268005907535553\n",
      "Subject 12, Epoch 777, Loss: 0.6249997615814209, Final Batch Loss: 0.23355239629745483\n",
      "Subject 12, Epoch 778, Loss: 0.6692050620913506, Final Batch Loss: 0.2773408889770508\n",
      "Subject 12, Epoch 779, Loss: 0.6327281296253204, Final Batch Loss: 0.17924270033836365\n",
      "Subject 12, Epoch 780, Loss: 0.6533056646585464, Final Batch Loss: 0.17302514612674713\n",
      "Subject 12, Epoch 781, Loss: 0.587766207754612, Final Batch Loss: 0.14951486885547638\n",
      "Subject 12, Epoch 782, Loss: 0.6701313257217407, Final Batch Loss: 0.19123221933841705\n",
      "Subject 12, Epoch 783, Loss: 0.6302373334765434, Final Batch Loss: 0.11273648589849472\n",
      "Subject 12, Epoch 784, Loss: 0.5708016753196716, Final Batch Loss: 0.13168510794639587\n",
      "Subject 12, Epoch 785, Loss: 0.5533625110983849, Final Batch Loss: 0.10976860672235489\n",
      "Subject 12, Epoch 786, Loss: 0.5936048477888107, Final Batch Loss: 0.17221902310848236\n",
      "Subject 12, Epoch 787, Loss: 0.521382212638855, Final Batch Loss: 0.10585514456033707\n",
      "Subject 12, Epoch 788, Loss: 0.6287759691476822, Final Batch Loss: 0.16262434422969818\n",
      "Subject 12, Epoch 789, Loss: 0.74199178814888, Final Batch Loss: 0.35091981291770935\n",
      "Subject 12, Epoch 790, Loss: 0.5653900280594826, Final Batch Loss: 0.15200568735599518\n",
      "Subject 12, Epoch 791, Loss: 0.7868014797568321, Final Batch Loss: 0.15953607857227325\n",
      "Subject 12, Epoch 792, Loss: 0.7184633314609528, Final Batch Loss: 0.2637762725353241\n",
      "Subject 12, Epoch 793, Loss: 0.7052183523774147, Final Batch Loss: 0.24580088257789612\n",
      "Subject 12, Epoch 794, Loss: 0.5278458222746849, Final Batch Loss: 0.13079069554805756\n",
      "Subject 12, Epoch 795, Loss: 0.5895336791872978, Final Batch Loss: 0.14967411756515503\n",
      "Subject 12, Epoch 796, Loss: 0.587904080748558, Final Batch Loss: 0.06275548785924911\n",
      "Subject 12, Epoch 797, Loss: 0.5989901050925255, Final Batch Loss: 0.16782741248607635\n",
      "Subject 12, Epoch 798, Loss: 0.5810941457748413, Final Batch Loss: 0.13797278702259064\n",
      "Subject 12, Epoch 799, Loss: 0.589162603020668, Final Batch Loss: 0.11490048468112946\n",
      "Subject 12, Epoch 800, Loss: 0.6509025767445564, Final Batch Loss: 0.16079100966453552\n",
      "Subject 12, Epoch 801, Loss: 0.5979070961475372, Final Batch Loss: 0.1284128874540329\n",
      "Subject 12, Epoch 802, Loss: 0.6209687814116478, Final Batch Loss: 0.1421717405319214\n",
      "Subject 12, Epoch 803, Loss: 0.5580712035298347, Final Batch Loss: 0.08086086809635162\n",
      "Subject 12, Epoch 804, Loss: 0.6905760690569878, Final Batch Loss: 0.1819160133600235\n",
      "Subject 12, Epoch 805, Loss: 0.5896067172288895, Final Batch Loss: 0.14710763096809387\n",
      "Subject 12, Epoch 806, Loss: 0.5943465307354927, Final Batch Loss: 0.18762150406837463\n",
      "Subject 12, Epoch 807, Loss: 0.6479293331503868, Final Batch Loss: 0.08572142571210861\n",
      "Subject 12, Epoch 808, Loss: 0.6512230634689331, Final Batch Loss: 0.2103138417005539\n",
      "Subject 12, Epoch 809, Loss: 0.5565452724695206, Final Batch Loss: 0.07321570813655853\n",
      "Subject 12, Epoch 810, Loss: 0.5814237967133522, Final Batch Loss: 0.1479351669549942\n",
      "Subject 12, Epoch 811, Loss: 0.6008751839399338, Final Batch Loss: 0.2185291200876236\n",
      "Subject 12, Epoch 812, Loss: 0.5176215022802353, Final Batch Loss: 0.1362788826227188\n",
      "Subject 12, Epoch 813, Loss: 0.5127706453204155, Final Batch Loss: 0.08943163603544235\n",
      "Subject 12, Epoch 814, Loss: 0.5960885211825371, Final Batch Loss: 0.074561208486557\n",
      "Subject 12, Epoch 815, Loss: 0.5870539471507072, Final Batch Loss: 0.11494079977273941\n",
      "Subject 12, Epoch 816, Loss: 0.6288050711154938, Final Batch Loss: 0.1701345145702362\n",
      "Subject 12, Epoch 817, Loss: 0.6011189445853233, Final Batch Loss: 0.16877290606498718\n",
      "Subject 12, Epoch 818, Loss: 0.6402967497706413, Final Batch Loss: 0.08640668541193008\n",
      "Subject 12, Epoch 819, Loss: 0.5703832060098648, Final Batch Loss: 0.13154232501983643\n",
      "Subject 12, Epoch 820, Loss: 0.6642133742570877, Final Batch Loss: 0.266549676656723\n",
      "Subject 12, Epoch 821, Loss: 0.6151858866214752, Final Batch Loss: 0.12468317151069641\n",
      "Subject 12, Epoch 822, Loss: 0.5043350532650948, Final Batch Loss: 0.1526203155517578\n",
      "Subject 12, Epoch 823, Loss: 0.5731423310935497, Final Batch Loss: 0.05996572598814964\n",
      "Subject 12, Epoch 824, Loss: 0.6248769089579582, Final Batch Loss: 0.20686738193035126\n",
      "Subject 12, Epoch 825, Loss: 0.5640566572546959, Final Batch Loss: 0.11837633699178696\n",
      "Subject 12, Epoch 826, Loss: 0.6783628091216087, Final Batch Loss: 0.10187865048646927\n",
      "Subject 12, Epoch 827, Loss: 0.5394452512264252, Final Batch Loss: 0.1301414966583252\n",
      "Subject 12, Epoch 828, Loss: 0.5539444386959076, Final Batch Loss: 0.15072742104530334\n",
      "Subject 12, Epoch 829, Loss: 0.6640058532357216, Final Batch Loss: 0.1805151402950287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 830, Loss: 0.5741919502615929, Final Batch Loss: 0.15680822730064392\n",
      "Subject 12, Epoch 831, Loss: 0.5047389268875122, Final Batch Loss: 0.06362704932689667\n",
      "Subject 12, Epoch 832, Loss: 0.6404127180576324, Final Batch Loss: 0.1625656634569168\n",
      "Subject 12, Epoch 833, Loss: 0.6456512659788132, Final Batch Loss: 0.14892669022083282\n",
      "Subject 12, Epoch 834, Loss: 0.6821770071983337, Final Batch Loss: 0.303246408700943\n",
      "Subject 12, Epoch 835, Loss: 0.6696597263216972, Final Batch Loss: 0.2234145849943161\n",
      "Subject 12, Epoch 836, Loss: 0.5311554372310638, Final Batch Loss: 0.09977186471223831\n",
      "Subject 12, Epoch 837, Loss: 0.6180287972092628, Final Batch Loss: 0.18565553426742554\n",
      "Subject 12, Epoch 838, Loss: 0.6218137890100479, Final Batch Loss: 0.15419983863830566\n",
      "Subject 12, Epoch 839, Loss: 0.7129473686218262, Final Batch Loss: 0.20622101426124573\n",
      "Subject 12, Epoch 840, Loss: 0.715164452791214, Final Batch Loss: 0.18255260586738586\n",
      "Subject 12, Epoch 841, Loss: 0.639152504503727, Final Batch Loss: 0.11938018351793289\n",
      "Subject 12, Epoch 842, Loss: 0.7073503136634827, Final Batch Loss: 0.3331506848335266\n",
      "Subject 12, Epoch 843, Loss: 0.6590259149670601, Final Batch Loss: 0.15087907016277313\n",
      "Subject 12, Epoch 844, Loss: 0.6433932334184647, Final Batch Loss: 0.20437505841255188\n",
      "Subject 12, Epoch 845, Loss: 0.5974038764834404, Final Batch Loss: 0.08695965260267258\n",
      "Subject 12, Epoch 846, Loss: 0.5810404419898987, Final Batch Loss: 0.13476590812206268\n",
      "Subject 12, Epoch 847, Loss: 0.5345750972628593, Final Batch Loss: 0.08273550868034363\n",
      "Subject 12, Epoch 848, Loss: 0.5892785117030144, Final Batch Loss: 0.11964132636785507\n",
      "Subject 12, Epoch 849, Loss: 0.6966642439365387, Final Batch Loss: 0.2152712494134903\n",
      "Subject 12, Epoch 850, Loss: 0.5687267184257507, Final Batch Loss: 0.12386003881692886\n",
      "Subject 12, Epoch 851, Loss: 0.7927985042333603, Final Batch Loss: 0.36232203245162964\n",
      "Subject 12, Epoch 852, Loss: 0.6748265624046326, Final Batch Loss: 0.22907033562660217\n",
      "Subject 12, Epoch 853, Loss: 0.6356341615319252, Final Batch Loss: 0.12837834656238556\n",
      "Subject 12, Epoch 854, Loss: 0.5363568216562271, Final Batch Loss: 0.11222773045301437\n",
      "Subject 12, Epoch 855, Loss: 0.45144936442375183, Final Batch Loss: 0.0756131187081337\n",
      "Subject 12, Epoch 856, Loss: 0.5199533179402351, Final Batch Loss: 0.09696301072835922\n",
      "Subject 12, Epoch 857, Loss: 0.5754659622907639, Final Batch Loss: 0.12920798361301422\n",
      "Subject 12, Epoch 858, Loss: 0.5498310774564743, Final Batch Loss: 0.1404256969690323\n",
      "Subject 12, Epoch 859, Loss: 0.5660284459590912, Final Batch Loss: 0.17940665781497955\n",
      "Subject 12, Epoch 860, Loss: 0.6009647771716118, Final Batch Loss: 0.21191459894180298\n",
      "Subject 12, Epoch 861, Loss: 0.4417930915951729, Final Batch Loss: 0.06496226042509079\n",
      "Subject 12, Epoch 862, Loss: 0.5454517304897308, Final Batch Loss: 0.14306780695915222\n",
      "Subject 12, Epoch 863, Loss: 0.5604769587516785, Final Batch Loss: 0.13098888099193573\n",
      "Subject 12, Epoch 864, Loss: 0.7229371070861816, Final Batch Loss: 0.26159125566482544\n",
      "Subject 12, Epoch 865, Loss: 0.7184450775384903, Final Batch Loss: 0.2007608264684677\n",
      "Subject 12, Epoch 866, Loss: 0.6416709572076797, Final Batch Loss: 0.13345564901828766\n",
      "Subject 12, Epoch 867, Loss: 0.5483112931251526, Final Batch Loss: 0.09231717884540558\n",
      "Subject 12, Epoch 868, Loss: 0.571556955575943, Final Batch Loss: 0.1328405886888504\n",
      "Subject 12, Epoch 869, Loss: 0.6238196566700935, Final Batch Loss: 0.158847376704216\n",
      "Subject 12, Epoch 870, Loss: 0.5207445472478867, Final Batch Loss: 0.10936398059129715\n",
      "Subject 12, Epoch 871, Loss: 0.5680544376373291, Final Batch Loss: 0.12028411030769348\n",
      "Subject 12, Epoch 872, Loss: 0.6497423276305199, Final Batch Loss: 0.24418283998966217\n",
      "Subject 12, Epoch 873, Loss: 0.6550093814730644, Final Batch Loss: 0.14812928438186646\n",
      "Subject 12, Epoch 874, Loss: 0.4969922825694084, Final Batch Loss: 0.1476360559463501\n",
      "Subject 12, Epoch 875, Loss: 0.5964049249887466, Final Batch Loss: 0.183844193816185\n",
      "Subject 12, Epoch 876, Loss: 0.6197196841239929, Final Batch Loss: 0.11612838506698608\n",
      "Subject 12, Epoch 877, Loss: 0.5190921798348427, Final Batch Loss: 0.09071305394172668\n",
      "Subject 12, Epoch 878, Loss: 0.569136768579483, Final Batch Loss: 0.1733323186635971\n",
      "Subject 12, Epoch 879, Loss: 0.5272525623440742, Final Batch Loss: 0.0462893545627594\n",
      "Subject 12, Epoch 880, Loss: 0.6925419196486473, Final Batch Loss: 0.18867148458957672\n",
      "Subject 12, Epoch 881, Loss: 0.4171392433345318, Final Batch Loss: 0.04949759319424629\n",
      "Subject 12, Epoch 882, Loss: 0.5470273569226265, Final Batch Loss: 0.15813860297203064\n",
      "Subject 12, Epoch 883, Loss: 0.5721177607774734, Final Batch Loss: 0.12710148096084595\n",
      "Subject 12, Epoch 884, Loss: 0.5253297314047813, Final Batch Loss: 0.09101051092147827\n",
      "Subject 12, Epoch 885, Loss: 0.44459113851189613, Final Batch Loss: 0.03969337418675423\n",
      "Subject 12, Epoch 886, Loss: 0.5532736405730247, Final Batch Loss: 0.1541071981191635\n",
      "Subject 12, Epoch 887, Loss: 0.6895793676376343, Final Batch Loss: 0.17000925540924072\n",
      "Subject 12, Epoch 888, Loss: 0.5225707143545151, Final Batch Loss: 0.13217301666736603\n",
      "Subject 12, Epoch 889, Loss: 0.7276100516319275, Final Batch Loss: 0.20624256134033203\n",
      "Subject 12, Epoch 890, Loss: 0.7094851061701775, Final Batch Loss: 0.3814002275466919\n",
      "Subject 12, Epoch 891, Loss: 0.5253066048026085, Final Batch Loss: 0.09798742085695267\n",
      "Subject 12, Epoch 892, Loss: 0.5006190165877342, Final Batch Loss: 0.14604103565216064\n",
      "Subject 12, Epoch 893, Loss: 0.45702068507671356, Final Batch Loss: 0.05310676246881485\n",
      "Subject 12, Epoch 894, Loss: 0.5881227478384972, Final Batch Loss: 0.2325739860534668\n",
      "Subject 12, Epoch 895, Loss: 0.45216961577534676, Final Batch Loss: 0.055328723043203354\n",
      "Subject 12, Epoch 896, Loss: 0.5919102877378464, Final Batch Loss: 0.19710107147693634\n",
      "Subject 12, Epoch 897, Loss: 0.5200624391436577, Final Batch Loss: 0.07544507831335068\n",
      "Subject 12, Epoch 898, Loss: 0.5024863481521606, Final Batch Loss: 0.11508329957723618\n",
      "Subject 12, Epoch 899, Loss: 0.5148709937930107, Final Batch Loss: 0.162629172205925\n",
      "Subject 12, Epoch 900, Loss: 0.49329373985528946, Final Batch Loss: 0.12956106662750244\n",
      "Subject 12, Epoch 901, Loss: 0.631840169429779, Final Batch Loss: 0.22743546962738037\n",
      "Subject 12, Epoch 902, Loss: 0.4890909492969513, Final Batch Loss: 0.06538327038288116\n",
      "Subject 12, Epoch 903, Loss: 0.6921684145927429, Final Batch Loss: 0.3863367736339569\n",
      "Subject 12, Epoch 904, Loss: 0.5713480114936829, Final Batch Loss: 0.14720489084720612\n",
      "Subject 12, Epoch 905, Loss: 0.5569009408354759, Final Batch Loss: 0.1315878927707672\n",
      "Subject 12, Epoch 906, Loss: 0.6043754816055298, Final Batch Loss: 0.1531178504228592\n",
      "Subject 12, Epoch 907, Loss: 0.649660587310791, Final Batch Loss: 0.1380150467157364\n",
      "Subject 12, Epoch 908, Loss: 0.5008063763380051, Final Batch Loss: 0.11319387704133987\n",
      "Subject 12, Epoch 909, Loss: 0.5566060543060303, Final Batch Loss: 0.07518084347248077\n",
      "Subject 12, Epoch 910, Loss: 0.5872339904308319, Final Batch Loss: 0.23063409328460693\n",
      "Subject 12, Epoch 911, Loss: 0.4479449763894081, Final Batch Loss: 0.12107586860656738\n",
      "Subject 12, Epoch 912, Loss: 0.6953300684690475, Final Batch Loss: 0.2045769840478897\n",
      "Subject 12, Epoch 913, Loss: 0.6170600950717926, Final Batch Loss: 0.2427608072757721\n",
      "Subject 12, Epoch 914, Loss: 0.6468528658151627, Final Batch Loss: 0.27470970153808594\n",
      "Subject 12, Epoch 915, Loss: 0.47256697714328766, Final Batch Loss: 0.11435212194919586\n",
      "Subject 12, Epoch 916, Loss: 0.5941624157130718, Final Batch Loss: 0.05083031579852104\n",
      "Subject 12, Epoch 917, Loss: 0.4686654359102249, Final Batch Loss: 0.07725711166858673\n",
      "Subject 12, Epoch 918, Loss: 0.5551260858774185, Final Batch Loss: 0.09737669676542282\n",
      "Subject 12, Epoch 919, Loss: 0.4553854465484619, Final Batch Loss: 0.0839574933052063\n",
      "Subject 12, Epoch 920, Loss: 0.5057463422417641, Final Batch Loss: 0.13176444172859192\n",
      "Subject 12, Epoch 921, Loss: 0.50398238748312, Final Batch Loss: 0.03579721599817276\n",
      "Subject 12, Epoch 922, Loss: 0.43357184529304504, Final Batch Loss: 0.08451762795448303\n",
      "Subject 12, Epoch 923, Loss: 0.6022488176822662, Final Batch Loss: 0.21968188881874084\n",
      "Subject 12, Epoch 924, Loss: 0.47786932438611984, Final Batch Loss: 0.12889596819877625\n",
      "Subject 12, Epoch 925, Loss: 0.6089960038661957, Final Batch Loss: 0.149754598736763\n",
      "Subject 12, Epoch 926, Loss: 0.4679403007030487, Final Batch Loss: 0.10836009681224823\n",
      "Subject 12, Epoch 927, Loss: 0.48561927676200867, Final Batch Loss: 0.09298016875982285\n",
      "Subject 12, Epoch 928, Loss: 0.5777549669146538, Final Batch Loss: 0.21824991703033447\n",
      "Subject 12, Epoch 929, Loss: 0.4517337195575237, Final Batch Loss: 0.056983474642038345\n",
      "Subject 12, Epoch 930, Loss: 0.5247812718153, Final Batch Loss: 0.22893117368221283\n",
      "Subject 12, Epoch 931, Loss: 0.5138024389743805, Final Batch Loss: 0.06860889494419098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12, Epoch 932, Loss: 0.6002144664525986, Final Batch Loss: 0.14168061316013336\n",
      "Subject 12, Epoch 933, Loss: 0.4440518692135811, Final Batch Loss: 0.10406991839408875\n",
      "Subject 12, Epoch 934, Loss: 0.532392144203186, Final Batch Loss: 0.15311215817928314\n",
      "Subject 12, Epoch 935, Loss: 0.5830016806721687, Final Batch Loss: 0.14582635462284088\n",
      "Subject 12, Epoch 936, Loss: 0.6018859520554543, Final Batch Loss: 0.24386140704154968\n",
      "Subject 12, Epoch 937, Loss: 0.5039992928504944, Final Batch Loss: 0.10229271650314331\n",
      "Subject 12, Epoch 938, Loss: 0.544634111225605, Final Batch Loss: 0.07592695951461792\n",
      "Subject 12, Epoch 939, Loss: 0.44536225497722626, Final Batch Loss: 0.04036697745323181\n",
      "Subject 12, Epoch 940, Loss: 0.5758715569972992, Final Batch Loss: 0.20187893509864807\n",
      "Subject 12, Epoch 941, Loss: 0.6390176638960838, Final Batch Loss: 0.35387226939201355\n",
      "Subject 12, Epoch 942, Loss: 0.5081108808517456, Final Batch Loss: 0.12891264259815216\n",
      "Subject 12, Epoch 943, Loss: 0.5760553181171417, Final Batch Loss: 0.20115602016448975\n",
      "Subject 12, Epoch 944, Loss: 0.530786007642746, Final Batch Loss: 0.1997171938419342\n",
      "Subject 12, Epoch 945, Loss: 0.48271802067756653, Final Batch Loss: 0.07581575214862823\n",
      "Subject 12, Epoch 946, Loss: 0.5097400546073914, Final Batch Loss: 0.11064400523900986\n",
      "Subject 12, Epoch 947, Loss: 0.6654499173164368, Final Batch Loss: 0.16176900267601013\n",
      "Subject 12, Epoch 948, Loss: 0.6089247092604637, Final Batch Loss: 0.28512606024742126\n",
      "Subject 12, Epoch 949, Loss: 0.564229354262352, Final Batch Loss: 0.2150193154811859\n",
      "Subject 12, Epoch 950, Loss: 0.49927646666765213, Final Batch Loss: 0.1098715215921402\n",
      "Subject 12, Epoch 951, Loss: 0.5055915117263794, Final Batch Loss: 0.12370342761278152\n",
      "Subject 12, Epoch 952, Loss: 0.565078467130661, Final Batch Loss: 0.1580924093723297\n",
      "Subject 12, Epoch 953, Loss: 0.38648796454072, Final Batch Loss: 0.05725341662764549\n",
      "Subject 12, Epoch 954, Loss: 0.4440196081995964, Final Batch Loss: 0.10946840047836304\n",
      "Subject 12, Epoch 955, Loss: 0.5006452277302742, Final Batch Loss: 0.0866352915763855\n",
      "Subject 12, Epoch 956, Loss: 0.5910314619541168, Final Batch Loss: 0.18823686242103577\n",
      "Subject 12, Epoch 957, Loss: 0.5243726149201393, Final Batch Loss: 0.12117405235767365\n",
      "Subject 12, Epoch 958, Loss: 0.4444719925522804, Final Batch Loss: 0.04130461812019348\n",
      "Subject 12, Epoch 959, Loss: 0.46040499210357666, Final Batch Loss: 0.07879726588726044\n",
      "Subject 12, Epoch 960, Loss: 0.5923376604914665, Final Batch Loss: 0.20350418984889984\n",
      "Subject 12, Epoch 961, Loss: 0.41866378486156464, Final Batch Loss: 0.05980738252401352\n",
      "Subject 12, Epoch 962, Loss: 0.4359903708100319, Final Batch Loss: 0.07952576875686646\n",
      "Subject 12, Epoch 963, Loss: 0.6330988705158234, Final Batch Loss: 0.10097640752792358\n",
      "Subject 12, Epoch 964, Loss: 0.46775349974632263, Final Batch Loss: 0.1459849774837494\n",
      "Subject 12, Epoch 965, Loss: 0.46608807891607285, Final Batch Loss: 0.12858225405216217\n",
      "Subject 12, Epoch 966, Loss: 0.4433176815509796, Final Batch Loss: 0.15746162831783295\n",
      "Subject 12, Epoch 967, Loss: 0.48295488953590393, Final Batch Loss: 0.08721720427274704\n",
      "Subject 12, Epoch 968, Loss: 0.4452713802456856, Final Batch Loss: 0.08309375494718552\n",
      "Subject 12, Epoch 969, Loss: 0.5311122462153435, Final Batch Loss: 0.21312002837657928\n",
      "Subject 12, Epoch 970, Loss: 0.47000255435705185, Final Batch Loss: 0.12603890895843506\n",
      "Subject 12, Epoch 971, Loss: 0.4593408741056919, Final Batch Loss: 0.03919011726975441\n",
      "Subject 12, Epoch 972, Loss: 0.4789824113249779, Final Batch Loss: 0.0977654904127121\n",
      "Subject 12, Epoch 973, Loss: 0.42709634453058243, Final Batch Loss: 0.09396533668041229\n",
      "Subject 12, Epoch 974, Loss: 0.5732829049229622, Final Batch Loss: 0.08294811099767685\n",
      "Subject 12, Epoch 975, Loss: 0.5798689648509026, Final Batch Loss: 0.09056944400072098\n",
      "Subject 12, Epoch 976, Loss: 0.4157913587987423, Final Batch Loss: 0.033923011273145676\n",
      "Subject 12, Epoch 977, Loss: 0.5806697905063629, Final Batch Loss: 0.10031364113092422\n",
      "Subject 12, Epoch 978, Loss: 0.3993557095527649, Final Batch Loss: 0.12673324346542358\n",
      "Subject 12, Epoch 979, Loss: 0.5928257182240486, Final Batch Loss: 0.16971421241760254\n",
      "Subject 12, Epoch 980, Loss: 0.44963623583316803, Final Batch Loss: 0.1291845738887787\n",
      "Subject 12, Epoch 981, Loss: 0.44048332795500755, Final Batch Loss: 0.04156309738755226\n",
      "Subject 12, Epoch 982, Loss: 0.4295606091618538, Final Batch Loss: 0.05092161148786545\n",
      "Subject 12, Epoch 983, Loss: 0.460196889936924, Final Batch Loss: 0.17583024501800537\n",
      "Subject 12, Epoch 984, Loss: 0.5117901191115379, Final Batch Loss: 0.18783104419708252\n",
      "Subject 12, Epoch 985, Loss: 0.42226456105709076, Final Batch Loss: 0.054305098950862885\n",
      "Subject 12, Epoch 986, Loss: 0.4018131196498871, Final Batch Loss: 0.09657000750303268\n",
      "Subject 12, Epoch 987, Loss: 0.46216483414173126, Final Batch Loss: 0.14207914471626282\n",
      "Subject 12, Epoch 988, Loss: 0.502208836376667, Final Batch Loss: 0.0995384007692337\n",
      "Subject 12, Epoch 989, Loss: 0.5144961103796959, Final Batch Loss: 0.14894743263721466\n",
      "Subject 12, Epoch 990, Loss: 0.4723568335175514, Final Batch Loss: 0.14505036175251007\n",
      "Subject 12, Epoch 991, Loss: 0.5889415070414543, Final Batch Loss: 0.19928063452243805\n",
      "Subject 12, Epoch 992, Loss: 0.5346275791525841, Final Batch Loss: 0.18983739614486694\n",
      "Subject 12, Epoch 993, Loss: 0.46737970784306526, Final Batch Loss: 0.05027296394109726\n",
      "Subject 12, Epoch 994, Loss: 0.3827381953597069, Final Batch Loss: 0.04176075756549835\n",
      "Subject 12, Epoch 995, Loss: 0.4337168484926224, Final Batch Loss: 0.13872987031936646\n",
      "Subject 12, Epoch 996, Loss: 0.47370695322752, Final Batch Loss: 0.15545141696929932\n",
      "Subject 12, Epoch 997, Loss: 0.41381968557834625, Final Batch Loss: 0.11536486446857452\n",
      "Subject 12, Epoch 998, Loss: 0.5502483770251274, Final Batch Loss: 0.14917102456092834\n",
      "Subject 12, Epoch 999, Loss: 0.46643706411123276, Final Batch Loss: 0.07039987295866013\n",
      "Subject 12, Epoch 1000, Loss: 0.49813980609178543, Final Batch Loss: 0.19207479059696198\n",
      "Subject 13, Epoch 1, Loss: 5.333171725273132, Final Batch Loss: 1.7618310451507568\n",
      "Subject 13, Epoch 2, Loss: 5.328488230705261, Final Batch Loss: 1.7667351961135864\n",
      "Subject 13, Epoch 3, Loss: 5.322322964668274, Final Batch Loss: 1.7600796222686768\n",
      "Subject 13, Epoch 4, Loss: 5.327946424484253, Final Batch Loss: 1.777416706085205\n",
      "Subject 13, Epoch 5, Loss: 5.324104309082031, Final Batch Loss: 1.7788481712341309\n",
      "Subject 13, Epoch 6, Loss: 5.327571630477905, Final Batch Loss: 1.8020981550216675\n",
      "Subject 13, Epoch 7, Loss: 5.314175963401794, Final Batch Loss: 1.78007173538208\n",
      "Subject 13, Epoch 8, Loss: 5.311218619346619, Final Batch Loss: 1.7517051696777344\n",
      "Subject 13, Epoch 9, Loss: 5.308600306510925, Final Batch Loss: 1.7639886140823364\n",
      "Subject 13, Epoch 10, Loss: 5.301488518714905, Final Batch Loss: 1.7647764682769775\n",
      "Subject 13, Epoch 11, Loss: 5.299388527870178, Final Batch Loss: 1.7732911109924316\n",
      "Subject 13, Epoch 12, Loss: 5.291759252548218, Final Batch Loss: 1.7730859518051147\n",
      "Subject 13, Epoch 13, Loss: 5.285745143890381, Final Batch Loss: 1.7716782093048096\n",
      "Subject 13, Epoch 14, Loss: 5.2740689516067505, Final Batch Loss: 1.7603087425231934\n",
      "Subject 13, Epoch 15, Loss: 5.249418616294861, Final Batch Loss: 1.728527545928955\n",
      "Subject 13, Epoch 16, Loss: 5.2355979681015015, Final Batch Loss: 1.7424933910369873\n",
      "Subject 13, Epoch 17, Loss: 5.220781207084656, Final Batch Loss: 1.7310959100723267\n",
      "Subject 13, Epoch 18, Loss: 5.195150256156921, Final Batch Loss: 1.71748948097229\n",
      "Subject 13, Epoch 19, Loss: 5.1674652099609375, Final Batch Loss: 1.6946792602539062\n",
      "Subject 13, Epoch 20, Loss: 5.148457407951355, Final Batch Loss: 1.725753903388977\n",
      "Subject 13, Epoch 21, Loss: 5.089696764945984, Final Batch Loss: 1.6797101497650146\n",
      "Subject 13, Epoch 22, Loss: 5.0777809619903564, Final Batch Loss: 1.652721643447876\n",
      "Subject 13, Epoch 23, Loss: 5.0497807264328, Final Batch Loss: 1.7052836418151855\n",
      "Subject 13, Epoch 24, Loss: 4.972550392150879, Final Batch Loss: 1.688504695892334\n",
      "Subject 13, Epoch 25, Loss: 4.8956931829452515, Final Batch Loss: 1.6198639869689941\n",
      "Subject 13, Epoch 26, Loss: 4.82312798500061, Final Batch Loss: 1.5524559020996094\n",
      "Subject 13, Epoch 27, Loss: 4.781068444252014, Final Batch Loss: 1.6758832931518555\n",
      "Subject 13, Epoch 28, Loss: 4.653579831123352, Final Batch Loss: 1.5275263786315918\n",
      "Subject 13, Epoch 29, Loss: 4.533657431602478, Final Batch Loss: 1.4315189123153687\n",
      "Subject 13, Epoch 30, Loss: 4.483753442764282, Final Batch Loss: 1.4830713272094727\n",
      "Subject 13, Epoch 31, Loss: 4.42498505115509, Final Batch Loss: 1.5278394222259521\n",
      "Subject 13, Epoch 32, Loss: 4.288352012634277, Final Batch Loss: 1.4632039070129395\n",
      "Subject 13, Epoch 33, Loss: 4.17982804775238, Final Batch Loss: 1.3082350492477417\n",
      "Subject 13, Epoch 34, Loss: 4.091789245605469, Final Batch Loss: 1.3142048120498657\n",
      "Subject 13, Epoch 35, Loss: 4.021774411201477, Final Batch Loss: 1.238360047340393\n",
      "Subject 13, Epoch 36, Loss: 3.9310011863708496, Final Batch Loss: 1.286286473274231\n",
      "Subject 13, Epoch 37, Loss: 3.9338197708129883, Final Batch Loss: 1.3438639640808105\n",
      "Subject 13, Epoch 38, Loss: 3.872334599494934, Final Batch Loss: 1.250193476676941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 39, Loss: 3.7017600536346436, Final Batch Loss: 1.247063398361206\n",
      "Subject 13, Epoch 40, Loss: 3.7920316457748413, Final Batch Loss: 1.2768751382827759\n",
      "Subject 13, Epoch 41, Loss: 3.595876693725586, Final Batch Loss: 1.1899529695510864\n",
      "Subject 13, Epoch 42, Loss: 3.6118721961975098, Final Batch Loss: 1.2597767114639282\n",
      "Subject 13, Epoch 43, Loss: 3.493359327316284, Final Batch Loss: 1.1682592630386353\n",
      "Subject 13, Epoch 44, Loss: 3.3475176095962524, Final Batch Loss: 1.1126374006271362\n",
      "Subject 13, Epoch 45, Loss: 3.3413307666778564, Final Batch Loss: 1.1378490924835205\n",
      "Subject 13, Epoch 46, Loss: 3.295398235321045, Final Batch Loss: 1.1189548969268799\n",
      "Subject 13, Epoch 47, Loss: 3.2368624210357666, Final Batch Loss: 1.118242621421814\n",
      "Subject 13, Epoch 48, Loss: 3.1740715503692627, Final Batch Loss: 1.0552116632461548\n",
      "Subject 13, Epoch 49, Loss: 3.0595078468322754, Final Batch Loss: 1.0369668006896973\n",
      "Subject 13, Epoch 50, Loss: 2.9891427159309387, Final Batch Loss: 1.0333592891693115\n",
      "Subject 13, Epoch 51, Loss: 2.9817053079605103, Final Batch Loss: 0.9880588054656982\n",
      "Subject 13, Epoch 52, Loss: 2.8187283873558044, Final Batch Loss: 1.0030369758605957\n",
      "Subject 13, Epoch 53, Loss: 2.911954939365387, Final Batch Loss: 0.9795563220977783\n",
      "Subject 13, Epoch 54, Loss: 2.8251659274101257, Final Batch Loss: 0.9327167868614197\n",
      "Subject 13, Epoch 55, Loss: 2.8646198511123657, Final Batch Loss: 0.9379901885986328\n",
      "Subject 13, Epoch 56, Loss: 2.620376706123352, Final Batch Loss: 0.9105328321456909\n",
      "Subject 13, Epoch 57, Loss: 2.694276452064514, Final Batch Loss: 0.9285776615142822\n",
      "Subject 13, Epoch 58, Loss: 2.6312294602394104, Final Batch Loss: 0.8994464874267578\n",
      "Subject 13, Epoch 59, Loss: 2.631648302078247, Final Batch Loss: 0.7773832678794861\n",
      "Subject 13, Epoch 60, Loss: 2.7239862084388733, Final Batch Loss: 0.861339807510376\n",
      "Subject 13, Epoch 61, Loss: 2.4261444211006165, Final Batch Loss: 0.715130090713501\n",
      "Subject 13, Epoch 62, Loss: 2.531348764896393, Final Batch Loss: 0.9205172061920166\n",
      "Subject 13, Epoch 63, Loss: 2.4729448556900024, Final Batch Loss: 0.8799204230308533\n",
      "Subject 13, Epoch 64, Loss: 2.519147038459778, Final Batch Loss: 0.8668980002403259\n",
      "Subject 13, Epoch 65, Loss: 2.3352328538894653, Final Batch Loss: 0.8772347569465637\n",
      "Subject 13, Epoch 66, Loss: 2.415722906589508, Final Batch Loss: 0.7915279865264893\n",
      "Subject 13, Epoch 67, Loss: 2.393014669418335, Final Batch Loss: 0.7830618619918823\n",
      "Subject 13, Epoch 68, Loss: 2.2702934741973877, Final Batch Loss: 0.6934041976928711\n",
      "Subject 13, Epoch 69, Loss: 2.319425880908966, Final Batch Loss: 0.7731668949127197\n",
      "Subject 13, Epoch 70, Loss: 2.2811458110809326, Final Batch Loss: 0.8047694563865662\n",
      "Subject 13, Epoch 71, Loss: 2.318175792694092, Final Batch Loss: 0.8260769248008728\n",
      "Subject 13, Epoch 72, Loss: 2.2252352833747864, Final Batch Loss: 0.6542197465896606\n",
      "Subject 13, Epoch 73, Loss: 2.2121769785881042, Final Batch Loss: 0.7458102703094482\n",
      "Subject 13, Epoch 74, Loss: 2.1784729957580566, Final Batch Loss: 0.7446109056472778\n",
      "Subject 13, Epoch 75, Loss: 2.1588892340660095, Final Batch Loss: 0.7885915040969849\n",
      "Subject 13, Epoch 76, Loss: 2.2370095252990723, Final Batch Loss: 0.7157386541366577\n",
      "Subject 13, Epoch 77, Loss: 1.985723614692688, Final Batch Loss: 0.6885398030281067\n",
      "Subject 13, Epoch 78, Loss: 2.169403314590454, Final Batch Loss: 0.7315959930419922\n",
      "Subject 13, Epoch 79, Loss: 2.1973745822906494, Final Batch Loss: 0.7256796360015869\n",
      "Subject 13, Epoch 80, Loss: 2.03629207611084, Final Batch Loss: 0.6556898951530457\n",
      "Subject 13, Epoch 81, Loss: 1.9210010766983032, Final Batch Loss: 0.6479224562644958\n",
      "Subject 13, Epoch 82, Loss: 1.9911803007125854, Final Batch Loss: 0.6289132237434387\n",
      "Subject 13, Epoch 83, Loss: 2.0030888319015503, Final Batch Loss: 0.6552996039390564\n",
      "Subject 13, Epoch 84, Loss: 1.9520145654678345, Final Batch Loss: 0.6334257125854492\n",
      "Subject 13, Epoch 85, Loss: 2.16615629196167, Final Batch Loss: 0.748551070690155\n",
      "Subject 13, Epoch 86, Loss: 1.8493738770484924, Final Batch Loss: 0.6015951037406921\n",
      "Subject 13, Epoch 87, Loss: 1.9505752325057983, Final Batch Loss: 0.6027913093566895\n",
      "Subject 13, Epoch 88, Loss: 1.9682735204696655, Final Batch Loss: 0.7067832350730896\n",
      "Subject 13, Epoch 89, Loss: 1.9410008192062378, Final Batch Loss: 0.6268672347068787\n",
      "Subject 13, Epoch 90, Loss: 1.8699709177017212, Final Batch Loss: 0.622256875038147\n",
      "Subject 13, Epoch 91, Loss: 1.9542412161827087, Final Batch Loss: 0.7024450302124023\n",
      "Subject 13, Epoch 92, Loss: 1.8749821186065674, Final Batch Loss: 0.5947867631912231\n",
      "Subject 13, Epoch 93, Loss: 1.9159066081047058, Final Batch Loss: 0.6514685153961182\n",
      "Subject 13, Epoch 94, Loss: 1.748196393251419, Final Batch Loss: 0.6756818294525146\n",
      "Subject 13, Epoch 95, Loss: 1.8245850801467896, Final Batch Loss: 0.6478009819984436\n",
      "Subject 13, Epoch 96, Loss: 1.8375727534294128, Final Batch Loss: 0.6014890670776367\n",
      "Subject 13, Epoch 97, Loss: 1.8435434699058533, Final Batch Loss: 0.6846621036529541\n",
      "Subject 13, Epoch 98, Loss: 1.942366600036621, Final Batch Loss: 0.677463173866272\n",
      "Subject 13, Epoch 99, Loss: 1.7380118370056152, Final Batch Loss: 0.6378599405288696\n",
      "Subject 13, Epoch 100, Loss: 1.7183705568313599, Final Batch Loss: 0.5725882649421692\n",
      "Subject 13, Epoch 101, Loss: 1.7031633853912354, Final Batch Loss: 0.5466286540031433\n",
      "Subject 13, Epoch 102, Loss: 1.749252200126648, Final Batch Loss: 0.5676481127738953\n",
      "Subject 13, Epoch 103, Loss: 1.7664445638656616, Final Batch Loss: 0.6134898066520691\n",
      "Subject 13, Epoch 104, Loss: 1.8914142847061157, Final Batch Loss: 0.5590901374816895\n",
      "Subject 13, Epoch 105, Loss: 1.768070101737976, Final Batch Loss: 0.6666837930679321\n",
      "Subject 13, Epoch 106, Loss: 1.7024759650230408, Final Batch Loss: 0.6167385578155518\n",
      "Subject 13, Epoch 107, Loss: 1.6050539016723633, Final Batch Loss: 0.48310840129852295\n",
      "Subject 13, Epoch 108, Loss: 1.5675556659698486, Final Batch Loss: 0.4846225380897522\n",
      "Subject 13, Epoch 109, Loss: 1.5892816185951233, Final Batch Loss: 0.5233895182609558\n",
      "Subject 13, Epoch 110, Loss: 1.8206778764724731, Final Batch Loss: 0.5859765410423279\n",
      "Subject 13, Epoch 111, Loss: 1.6716580092906952, Final Batch Loss: 0.5864302515983582\n",
      "Subject 13, Epoch 112, Loss: 1.555229902267456, Final Batch Loss: 0.514074444770813\n",
      "Subject 13, Epoch 113, Loss: 1.620479702949524, Final Batch Loss: 0.5079345107078552\n",
      "Subject 13, Epoch 114, Loss: 1.6680544018745422, Final Batch Loss: 0.4538941979408264\n",
      "Subject 13, Epoch 115, Loss: 1.5539368391036987, Final Batch Loss: 0.46493059396743774\n",
      "Subject 13, Epoch 116, Loss: 1.499605804681778, Final Batch Loss: 0.4378126561641693\n",
      "Subject 13, Epoch 117, Loss: 1.6454870402812958, Final Batch Loss: 0.6351616382598877\n",
      "Subject 13, Epoch 118, Loss: 1.6307846307754517, Final Batch Loss: 0.546532392501831\n",
      "Subject 13, Epoch 119, Loss: 1.516894817352295, Final Batch Loss: 0.498306542634964\n",
      "Subject 13, Epoch 120, Loss: 1.5602787137031555, Final Batch Loss: 0.4536939859390259\n",
      "Subject 13, Epoch 121, Loss: 1.5697415471076965, Final Batch Loss: 0.5594967603683472\n",
      "Subject 13, Epoch 122, Loss: 1.4507477581501007, Final Batch Loss: 0.4957248866558075\n",
      "Subject 13, Epoch 123, Loss: 1.465712308883667, Final Batch Loss: 0.48130765557289124\n",
      "Subject 13, Epoch 124, Loss: 1.4962857961654663, Final Batch Loss: 0.391060471534729\n",
      "Subject 13, Epoch 125, Loss: 1.592849612236023, Final Batch Loss: 0.5511627197265625\n",
      "Subject 13, Epoch 126, Loss: 1.5409317016601562, Final Batch Loss: 0.5059735178947449\n",
      "Subject 13, Epoch 127, Loss: 1.4945496916770935, Final Batch Loss: 0.48768603801727295\n",
      "Subject 13, Epoch 128, Loss: 1.5965475142002106, Final Batch Loss: 0.6049370765686035\n",
      "Subject 13, Epoch 129, Loss: 1.4973173439502716, Final Batch Loss: 0.4917949438095093\n",
      "Subject 13, Epoch 130, Loss: 1.4618891477584839, Final Batch Loss: 0.486513614654541\n",
      "Subject 13, Epoch 131, Loss: 1.4104575216770172, Final Batch Loss: 0.47782719135284424\n",
      "Subject 13, Epoch 132, Loss: 1.3767131865024567, Final Batch Loss: 0.42032039165496826\n",
      "Subject 13, Epoch 133, Loss: 1.4621400237083435, Final Batch Loss: 0.4306103587150574\n",
      "Subject 13, Epoch 134, Loss: 1.4572263658046722, Final Batch Loss: 0.5117396116256714\n",
      "Subject 13, Epoch 135, Loss: 1.4729781746864319, Final Batch Loss: 0.5274376273155212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 136, Loss: 1.3806171119213104, Final Batch Loss: 0.5087906718254089\n",
      "Subject 13, Epoch 137, Loss: 1.3761215209960938, Final Batch Loss: 0.4295763373374939\n",
      "Subject 13, Epoch 138, Loss: 1.4446446895599365, Final Batch Loss: 0.47028371691703796\n",
      "Subject 13, Epoch 139, Loss: 1.4367694854736328, Final Batch Loss: 0.4417620003223419\n",
      "Subject 13, Epoch 140, Loss: 1.360675722360611, Final Batch Loss: 0.4743030369281769\n",
      "Subject 13, Epoch 141, Loss: 1.444498360157013, Final Batch Loss: 0.46944183111190796\n",
      "Subject 13, Epoch 142, Loss: 1.4878459870815277, Final Batch Loss: 0.5390884876251221\n",
      "Subject 13, Epoch 143, Loss: 1.4571938812732697, Final Batch Loss: 0.4991433322429657\n",
      "Subject 13, Epoch 144, Loss: 1.3003805577754974, Final Batch Loss: 0.4894505739212036\n",
      "Subject 13, Epoch 145, Loss: 1.4432533085346222, Final Batch Loss: 0.5002655982971191\n",
      "Subject 13, Epoch 146, Loss: 1.4303556978702545, Final Batch Loss: 0.533632755279541\n",
      "Subject 13, Epoch 147, Loss: 1.4105199575424194, Final Batch Loss: 0.5177491903305054\n",
      "Subject 13, Epoch 148, Loss: 1.2923665344715118, Final Batch Loss: 0.34979531168937683\n",
      "Subject 13, Epoch 149, Loss: 1.3637672066688538, Final Batch Loss: 0.47365182638168335\n",
      "Subject 13, Epoch 150, Loss: 1.4516780078411102, Final Batch Loss: 0.5133376121520996\n",
      "Subject 13, Epoch 151, Loss: 1.3663350939750671, Final Batch Loss: 0.46400636434555054\n",
      "Subject 13, Epoch 152, Loss: 1.2301763594150543, Final Batch Loss: 0.3572486639022827\n",
      "Subject 13, Epoch 153, Loss: 1.2527468502521515, Final Batch Loss: 0.4397934675216675\n",
      "Subject 13, Epoch 154, Loss: 1.3333658874034882, Final Batch Loss: 0.45183563232421875\n",
      "Subject 13, Epoch 155, Loss: 1.3102517127990723, Final Batch Loss: 0.3896316587924957\n",
      "Subject 13, Epoch 156, Loss: 1.345392107963562, Final Batch Loss: 0.5176922082901001\n",
      "Subject 13, Epoch 157, Loss: 1.4110930562019348, Final Batch Loss: 0.50478595495224\n",
      "Subject 13, Epoch 158, Loss: 1.4302071332931519, Final Batch Loss: 0.5559296011924744\n",
      "Subject 13, Epoch 159, Loss: 1.3938598334789276, Final Batch Loss: 0.41682949662208557\n",
      "Subject 13, Epoch 160, Loss: 1.2953561842441559, Final Batch Loss: 0.40676766633987427\n",
      "Subject 13, Epoch 161, Loss: 1.3313372135162354, Final Batch Loss: 0.4327618479728699\n",
      "Subject 13, Epoch 162, Loss: 1.334914743900299, Final Batch Loss: 0.4802957773208618\n",
      "Subject 13, Epoch 163, Loss: 1.3126115798950195, Final Batch Loss: 0.46671390533447266\n",
      "Subject 13, Epoch 164, Loss: 1.216607689857483, Final Batch Loss: 0.4115886688232422\n",
      "Subject 13, Epoch 165, Loss: 1.3459879457950592, Final Batch Loss: 0.4236614406108856\n",
      "Subject 13, Epoch 166, Loss: 1.2922261655330658, Final Batch Loss: 0.4860497713088989\n",
      "Subject 13, Epoch 167, Loss: 1.2851388156414032, Final Batch Loss: 0.5287922024726868\n",
      "Subject 13, Epoch 168, Loss: 1.259149193763733, Final Batch Loss: 0.38371485471725464\n",
      "Subject 13, Epoch 169, Loss: 1.2422213554382324, Final Batch Loss: 0.45330512523651123\n",
      "Subject 13, Epoch 170, Loss: 1.1803924143314362, Final Batch Loss: 0.41828393936157227\n",
      "Subject 13, Epoch 171, Loss: 1.2256149649620056, Final Batch Loss: 0.4289509952068329\n",
      "Subject 13, Epoch 172, Loss: 1.2482307851314545, Final Batch Loss: 0.4363936185836792\n",
      "Subject 13, Epoch 173, Loss: 1.1438723802566528, Final Batch Loss: 0.32844558358192444\n",
      "Subject 13, Epoch 174, Loss: 1.1588377356529236, Final Batch Loss: 0.3739246129989624\n",
      "Subject 13, Epoch 175, Loss: 1.2604824900627136, Final Batch Loss: 0.5579326748847961\n",
      "Subject 13, Epoch 176, Loss: 1.3088271617889404, Final Batch Loss: 0.40686529874801636\n",
      "Subject 13, Epoch 177, Loss: 1.219995766878128, Final Batch Loss: 0.4019750654697418\n",
      "Subject 13, Epoch 178, Loss: 1.1845650374889374, Final Batch Loss: 0.37945130467414856\n",
      "Subject 13, Epoch 179, Loss: 1.1688100397586823, Final Batch Loss: 0.45500823855400085\n",
      "Subject 13, Epoch 180, Loss: 1.1885572373867035, Final Batch Loss: 0.3599182963371277\n",
      "Subject 13, Epoch 181, Loss: 1.2974829077720642, Final Batch Loss: 0.4310835301876068\n",
      "Subject 13, Epoch 182, Loss: 1.1968920826911926, Final Batch Loss: 0.37676548957824707\n",
      "Subject 13, Epoch 183, Loss: 1.1883519887924194, Final Batch Loss: 0.4167097210884094\n",
      "Subject 13, Epoch 184, Loss: 1.2629347145557404, Final Batch Loss: 0.42533746361732483\n",
      "Subject 13, Epoch 185, Loss: 1.328274518251419, Final Batch Loss: 0.4395849406719208\n",
      "Subject 13, Epoch 186, Loss: 1.1908542215824127, Final Batch Loss: 0.3555229902267456\n",
      "Subject 13, Epoch 187, Loss: 1.268698513507843, Final Batch Loss: 0.4044005572795868\n",
      "Subject 13, Epoch 188, Loss: 1.1392622888088226, Final Batch Loss: 0.3487282395362854\n",
      "Subject 13, Epoch 189, Loss: 1.1867766082286835, Final Batch Loss: 0.394447386264801\n",
      "Subject 13, Epoch 190, Loss: 1.1209111213684082, Final Batch Loss: 0.342673659324646\n",
      "Subject 13, Epoch 191, Loss: 1.1796690225601196, Final Batch Loss: 0.40433746576309204\n",
      "Subject 13, Epoch 192, Loss: 1.2515572607517242, Final Batch Loss: 0.4650402069091797\n",
      "Subject 13, Epoch 193, Loss: 1.2040138840675354, Final Batch Loss: 0.4421430230140686\n",
      "Subject 13, Epoch 194, Loss: 1.132431447505951, Final Batch Loss: 0.3371107876300812\n",
      "Subject 13, Epoch 195, Loss: 1.304555594921112, Final Batch Loss: 0.34912437200546265\n",
      "Subject 13, Epoch 196, Loss: 1.1829187273979187, Final Batch Loss: 0.43922048807144165\n",
      "Subject 13, Epoch 197, Loss: 1.1462350189685822, Final Batch Loss: 0.35120320320129395\n",
      "Subject 13, Epoch 198, Loss: 1.1217488050460815, Final Batch Loss: 0.3116215467453003\n",
      "Subject 13, Epoch 199, Loss: 1.0549632608890533, Final Batch Loss: 0.2902536690235138\n",
      "Subject 13, Epoch 200, Loss: 1.2623475193977356, Final Batch Loss: 0.42793217301368713\n",
      "Subject 13, Epoch 201, Loss: 1.1440815925598145, Final Batch Loss: 0.3350595533847809\n",
      "Subject 13, Epoch 202, Loss: 1.0747419893741608, Final Batch Loss: 0.3334313631057739\n",
      "Subject 13, Epoch 203, Loss: 1.180231660604477, Final Batch Loss: 0.43668413162231445\n",
      "Subject 13, Epoch 204, Loss: 1.1743907630443573, Final Batch Loss: 0.45362961292266846\n",
      "Subject 13, Epoch 205, Loss: 1.1013279855251312, Final Batch Loss: 0.36767324805259705\n",
      "Subject 13, Epoch 206, Loss: 1.1399032771587372, Final Batch Loss: 0.32559552788734436\n",
      "Subject 13, Epoch 207, Loss: 1.2115902602672577, Final Batch Loss: 0.39078375697135925\n",
      "Subject 13, Epoch 208, Loss: 1.104732722043991, Final Batch Loss: 0.3630180060863495\n",
      "Subject 13, Epoch 209, Loss: 1.0510473549365997, Final Batch Loss: 0.42658159136772156\n",
      "Subject 13, Epoch 210, Loss: 1.049888014793396, Final Batch Loss: 0.3192451000213623\n",
      "Subject 13, Epoch 211, Loss: 1.1739842295646667, Final Batch Loss: 0.44067108631134033\n",
      "Subject 13, Epoch 212, Loss: 1.050489753484726, Final Batch Loss: 0.3370685279369354\n",
      "Subject 13, Epoch 213, Loss: 1.0563728511333466, Final Batch Loss: 0.31402385234832764\n",
      "Subject 13, Epoch 214, Loss: 1.232231318950653, Final Batch Loss: 0.3508910536766052\n",
      "Subject 13, Epoch 215, Loss: 1.0659074187278748, Final Batch Loss: 0.2668795585632324\n",
      "Subject 13, Epoch 216, Loss: 1.0297781229019165, Final Batch Loss: 0.32753002643585205\n",
      "Subject 13, Epoch 217, Loss: 0.9668318033218384, Final Batch Loss: 0.2926192283630371\n",
      "Subject 13, Epoch 218, Loss: 1.082986205816269, Final Batch Loss: 0.3885406255722046\n",
      "Subject 13, Epoch 219, Loss: 1.0843161642551422, Final Batch Loss: 0.37480947375297546\n",
      "Subject 13, Epoch 220, Loss: 1.0729673504829407, Final Batch Loss: 0.3383859395980835\n",
      "Subject 13, Epoch 221, Loss: 1.1393061876296997, Final Batch Loss: 0.4141985774040222\n",
      "Subject 13, Epoch 222, Loss: 1.1661710441112518, Final Batch Loss: 0.43009865283966064\n",
      "Subject 13, Epoch 223, Loss: 0.9558596014976501, Final Batch Loss: 0.2908722162246704\n",
      "Subject 13, Epoch 224, Loss: 0.9976441860198975, Final Batch Loss: 0.341238409280777\n",
      "Subject 13, Epoch 225, Loss: 1.1012557744979858, Final Batch Loss: 0.4522115886211395\n",
      "Subject 13, Epoch 226, Loss: 1.1757950484752655, Final Batch Loss: 0.44982782006263733\n",
      "Subject 13, Epoch 227, Loss: 1.025128722190857, Final Batch Loss: 0.372267484664917\n",
      "Subject 13, Epoch 228, Loss: 0.9509720802307129, Final Batch Loss: 0.2897275686264038\n",
      "Subject 13, Epoch 229, Loss: 1.202589064836502, Final Batch Loss: 0.5161610841751099\n",
      "Subject 13, Epoch 230, Loss: 1.1002685725688934, Final Batch Loss: 0.39704737067222595\n",
      "Subject 13, Epoch 231, Loss: 1.0382308959960938, Final Batch Loss: 0.42315900325775146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 232, Loss: 1.0815395414829254, Final Batch Loss: 0.35200241208076477\n",
      "Subject 13, Epoch 233, Loss: 0.9692125916481018, Final Batch Loss: 0.22179466485977173\n",
      "Subject 13, Epoch 234, Loss: 1.151324212551117, Final Batch Loss: 0.3452293574810028\n",
      "Subject 13, Epoch 235, Loss: 0.9702371656894684, Final Batch Loss: 0.29703769087791443\n",
      "Subject 13, Epoch 236, Loss: 0.9974170327186584, Final Batch Loss: 0.37246862053871155\n",
      "Subject 13, Epoch 237, Loss: 0.9017395973205566, Final Batch Loss: 0.31136399507522583\n",
      "Subject 13, Epoch 238, Loss: 0.9365635812282562, Final Batch Loss: 0.3335570693016052\n",
      "Subject 13, Epoch 239, Loss: 1.0462845265865326, Final Batch Loss: 0.2865733802318573\n",
      "Subject 13, Epoch 240, Loss: 1.0390407145023346, Final Batch Loss: 0.4291486144065857\n",
      "Subject 13, Epoch 241, Loss: 0.9832824468612671, Final Batch Loss: 0.3161454498767853\n",
      "Subject 13, Epoch 242, Loss: 1.0394869148731232, Final Batch Loss: 0.30977532267570496\n",
      "Subject 13, Epoch 243, Loss: 0.8862977027893066, Final Batch Loss: 0.3242764472961426\n",
      "Subject 13, Epoch 244, Loss: 0.8727271109819412, Final Batch Loss: 0.2492591291666031\n",
      "Subject 13, Epoch 245, Loss: 1.1170372664928436, Final Batch Loss: 0.3163505792617798\n",
      "Subject 13, Epoch 246, Loss: 0.9044221490621567, Final Batch Loss: 0.37126827239990234\n",
      "Subject 13, Epoch 247, Loss: 1.001459002494812, Final Batch Loss: 0.36725175380706787\n",
      "Subject 13, Epoch 248, Loss: 0.9265171587467194, Final Batch Loss: 0.3201538324356079\n",
      "Subject 13, Epoch 249, Loss: 1.0017679035663605, Final Batch Loss: 0.3802396059036255\n",
      "Subject 13, Epoch 250, Loss: 0.9048181772232056, Final Batch Loss: 0.2807045578956604\n",
      "Subject 13, Epoch 251, Loss: 1.0535706579685211, Final Batch Loss: 0.43570220470428467\n",
      "Subject 13, Epoch 252, Loss: 0.9502067863941193, Final Batch Loss: 0.31362882256507874\n",
      "Subject 13, Epoch 253, Loss: 1.0175312161445618, Final Batch Loss: 0.32972970604896545\n",
      "Subject 13, Epoch 254, Loss: 0.9891879558563232, Final Batch Loss: 0.3187725245952606\n",
      "Subject 13, Epoch 255, Loss: 1.0349199175834656, Final Batch Loss: 0.38270220160484314\n",
      "Subject 13, Epoch 256, Loss: 0.9530084431171417, Final Batch Loss: 0.3251953721046448\n",
      "Subject 13, Epoch 257, Loss: 0.9636269509792328, Final Batch Loss: 0.34044182300567627\n",
      "Subject 13, Epoch 258, Loss: 0.9305169582366943, Final Batch Loss: 0.285320907831192\n",
      "Subject 13, Epoch 259, Loss: 0.8079931437969208, Final Batch Loss: 0.23870083689689636\n",
      "Subject 13, Epoch 260, Loss: 0.9515184760093689, Final Batch Loss: 0.30012214183807373\n",
      "Subject 13, Epoch 261, Loss: 0.880805253982544, Final Batch Loss: 0.2533535361289978\n",
      "Subject 13, Epoch 262, Loss: 0.9091360569000244, Final Batch Loss: 0.31088462471961975\n",
      "Subject 13, Epoch 263, Loss: 1.042914241552353, Final Batch Loss: 0.3318248689174652\n",
      "Subject 13, Epoch 264, Loss: 0.877997636795044, Final Batch Loss: 0.32031333446502686\n",
      "Subject 13, Epoch 265, Loss: 0.7518660128116608, Final Batch Loss: 0.23906071484088898\n",
      "Subject 13, Epoch 266, Loss: 0.965336799621582, Final Batch Loss: 0.25348222255706787\n",
      "Subject 13, Epoch 267, Loss: 0.980071097612381, Final Batch Loss: 0.27858486771583557\n",
      "Subject 13, Epoch 268, Loss: 0.9372177720069885, Final Batch Loss: 0.33032602071762085\n",
      "Subject 13, Epoch 269, Loss: 0.8658154010772705, Final Batch Loss: 0.2774416506290436\n",
      "Subject 13, Epoch 270, Loss: 0.8824401199817657, Final Batch Loss: 0.25989294052124023\n",
      "Subject 13, Epoch 271, Loss: 0.8489963412284851, Final Batch Loss: 0.24481964111328125\n",
      "Subject 13, Epoch 272, Loss: 0.8566355854272842, Final Batch Loss: 0.24492301046848297\n",
      "Subject 13, Epoch 273, Loss: 0.8138700425624847, Final Batch Loss: 0.31522226333618164\n",
      "Subject 13, Epoch 274, Loss: 0.9563553780317307, Final Batch Loss: 0.3015105128288269\n",
      "Subject 13, Epoch 275, Loss: 0.8840292245149612, Final Batch Loss: 0.3682297468185425\n",
      "Subject 13, Epoch 276, Loss: 0.9172383546829224, Final Batch Loss: 0.36017906665802\n",
      "Subject 13, Epoch 277, Loss: 0.876559317111969, Final Batch Loss: 0.28751757740974426\n",
      "Subject 13, Epoch 278, Loss: 0.8931907415390015, Final Batch Loss: 0.2882603108882904\n",
      "Subject 13, Epoch 279, Loss: 0.793656587600708, Final Batch Loss: 0.22442859411239624\n",
      "Subject 13, Epoch 280, Loss: 0.76608045399189, Final Batch Loss: 0.2816086411476135\n",
      "Subject 13, Epoch 281, Loss: 0.855049729347229, Final Batch Loss: 0.2125401496887207\n",
      "Subject 13, Epoch 282, Loss: 0.9652117788791656, Final Batch Loss: 0.33014512062072754\n",
      "Subject 13, Epoch 283, Loss: 0.7443598657846451, Final Batch Loss: 0.2390916347503662\n",
      "Subject 13, Epoch 284, Loss: 0.8936493694782257, Final Batch Loss: 0.2915881872177124\n",
      "Subject 13, Epoch 285, Loss: 0.985862672328949, Final Batch Loss: 0.38917773962020874\n",
      "Subject 13, Epoch 286, Loss: 0.9226027727127075, Final Batch Loss: 0.2355463206768036\n",
      "Subject 13, Epoch 287, Loss: 0.7766474187374115, Final Batch Loss: 0.24579085409641266\n",
      "Subject 13, Epoch 288, Loss: 0.855027973651886, Final Batch Loss: 0.3708316385746002\n",
      "Subject 13, Epoch 289, Loss: 0.7596042305231094, Final Batch Loss: 0.22632519900798798\n",
      "Subject 13, Epoch 290, Loss: 0.9901457130908966, Final Batch Loss: 0.3041364252567291\n",
      "Subject 13, Epoch 291, Loss: 0.7515726238489151, Final Batch Loss: 0.22808824479579926\n",
      "Subject 13, Epoch 292, Loss: 0.8027885407209396, Final Batch Loss: 0.28274786472320557\n",
      "Subject 13, Epoch 293, Loss: 0.7483966499567032, Final Batch Loss: 0.23481371998786926\n",
      "Subject 13, Epoch 294, Loss: 1.0046469867229462, Final Batch Loss: 0.37690481543540955\n",
      "Subject 13, Epoch 295, Loss: 0.8761591613292694, Final Batch Loss: 0.3657802939414978\n",
      "Subject 13, Epoch 296, Loss: 0.7831825762987137, Final Batch Loss: 0.30596816539764404\n",
      "Subject 13, Epoch 297, Loss: 0.8187581300735474, Final Batch Loss: 0.3091553747653961\n",
      "Subject 13, Epoch 298, Loss: 0.7981536686420441, Final Batch Loss: 0.2825893759727478\n",
      "Subject 13, Epoch 299, Loss: 0.7316677272319794, Final Batch Loss: 0.2134302258491516\n",
      "Subject 13, Epoch 300, Loss: 0.8803804814815521, Final Batch Loss: 0.26292598247528076\n",
      "Subject 13, Epoch 301, Loss: 0.8152906596660614, Final Batch Loss: 0.25549665093421936\n",
      "Subject 13, Epoch 302, Loss: 0.7609778642654419, Final Batch Loss: 0.22521840035915375\n",
      "Subject 13, Epoch 303, Loss: 0.8150902837514877, Final Batch Loss: 0.29248592257499695\n",
      "Subject 13, Epoch 304, Loss: 0.9331795871257782, Final Batch Loss: 0.28627562522888184\n",
      "Subject 13, Epoch 305, Loss: 0.8929077088832855, Final Batch Loss: 0.2651231288909912\n",
      "Subject 13, Epoch 306, Loss: 0.8341145813465118, Final Batch Loss: 0.2737768888473511\n",
      "Subject 13, Epoch 307, Loss: 0.8136661946773529, Final Batch Loss: 0.25947827100753784\n",
      "Subject 13, Epoch 308, Loss: 0.7852765023708344, Final Batch Loss: 0.311735600233078\n",
      "Subject 13, Epoch 309, Loss: 0.8116606622934341, Final Batch Loss: 0.27175822854042053\n",
      "Subject 13, Epoch 310, Loss: 0.6433301270008087, Final Batch Loss: 0.2072744518518448\n",
      "Subject 13, Epoch 311, Loss: 0.8940142095088959, Final Batch Loss: 0.3690626323223114\n",
      "Subject 13, Epoch 312, Loss: 0.7183020859956741, Final Batch Loss: 0.17367032170295715\n",
      "Subject 13, Epoch 313, Loss: 0.7485834658145905, Final Batch Loss: 0.21977218985557556\n",
      "Subject 13, Epoch 314, Loss: 0.8504657447338104, Final Batch Loss: 0.2114538848400116\n",
      "Subject 13, Epoch 315, Loss: 0.6903163939714432, Final Batch Loss: 0.2667434811592102\n",
      "Subject 13, Epoch 316, Loss: 0.8565645217895508, Final Batch Loss: 0.25475502014160156\n",
      "Subject 13, Epoch 317, Loss: 0.6406957060098648, Final Batch Loss: 0.215487539768219\n",
      "Subject 13, Epoch 318, Loss: 0.7797248214483261, Final Batch Loss: 0.33642685413360596\n",
      "Subject 13, Epoch 319, Loss: 0.6478406935930252, Final Batch Loss: 0.2090824991464615\n",
      "Subject 13, Epoch 320, Loss: 0.7270375490188599, Final Batch Loss: 0.26954567432403564\n",
      "Subject 13, Epoch 321, Loss: 0.74175626039505, Final Batch Loss: 0.27012234926223755\n",
      "Subject 13, Epoch 322, Loss: 0.7937724739313126, Final Batch Loss: 0.30047109723091125\n",
      "Subject 13, Epoch 323, Loss: 0.6827547997236252, Final Batch Loss: 0.19528158009052277\n",
      "Subject 13, Epoch 324, Loss: 0.736406221985817, Final Batch Loss: 0.3050519526004791\n",
      "Subject 13, Epoch 325, Loss: 0.7019141763448715, Final Batch Loss: 0.26382461190223694\n",
      "Subject 13, Epoch 326, Loss: 0.7195994555950165, Final Batch Loss: 0.17573726177215576\n",
      "Subject 13, Epoch 327, Loss: 0.6275862604379654, Final Batch Loss: 0.2009701430797577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 328, Loss: 0.8631983697414398, Final Batch Loss: 0.3408900499343872\n",
      "Subject 13, Epoch 329, Loss: 0.7123245596885681, Final Batch Loss: 0.2615772783756256\n",
      "Subject 13, Epoch 330, Loss: 0.5677188113331795, Final Batch Loss: 0.2630678415298462\n",
      "Subject 13, Epoch 331, Loss: 0.6172584742307663, Final Batch Loss: 0.18025626242160797\n",
      "Subject 13, Epoch 332, Loss: 0.6376654654741287, Final Batch Loss: 0.20852620899677277\n",
      "Subject 13, Epoch 333, Loss: 0.5858901292085648, Final Batch Loss: 0.19913779199123383\n",
      "Subject 13, Epoch 334, Loss: 0.7181970328092575, Final Batch Loss: 0.28904062509536743\n",
      "Subject 13, Epoch 335, Loss: 0.6223786175251007, Final Batch Loss: 0.20061969757080078\n",
      "Subject 13, Epoch 336, Loss: 0.6947971135377884, Final Batch Loss: 0.25765755772590637\n",
      "Subject 13, Epoch 337, Loss: 0.7276112735271454, Final Batch Loss: 0.19534380733966827\n",
      "Subject 13, Epoch 338, Loss: 0.7733921855688095, Final Batch Loss: 0.39765048027038574\n",
      "Subject 13, Epoch 339, Loss: 0.6638345569372177, Final Batch Loss: 0.16699329018592834\n",
      "Subject 13, Epoch 340, Loss: 0.6563424170017242, Final Batch Loss: 0.2538149952888489\n",
      "Subject 13, Epoch 341, Loss: 0.6114810854196548, Final Batch Loss: 0.21445904672145844\n",
      "Subject 13, Epoch 342, Loss: 0.5779745429754257, Final Batch Loss: 0.18541821837425232\n",
      "Subject 13, Epoch 343, Loss: 0.6841902434825897, Final Batch Loss: 0.2323218435049057\n",
      "Subject 13, Epoch 344, Loss: 0.5744171887636185, Final Batch Loss: 0.23114095628261566\n",
      "Subject 13, Epoch 345, Loss: 0.7243516147136688, Final Batch Loss: 0.27734652161598206\n",
      "Subject 13, Epoch 346, Loss: 0.6037616431713104, Final Batch Loss: 0.24630731344223022\n",
      "Subject 13, Epoch 347, Loss: 0.564502164721489, Final Batch Loss: 0.23162105679512024\n",
      "Subject 13, Epoch 348, Loss: 0.5332812517881393, Final Batch Loss: 0.19565893709659576\n",
      "Subject 13, Epoch 349, Loss: 0.6262990087270737, Final Batch Loss: 0.17016956210136414\n",
      "Subject 13, Epoch 350, Loss: 0.6414363235235214, Final Batch Loss: 0.3105587363243103\n",
      "Subject 13, Epoch 351, Loss: 0.48518645763397217, Final Batch Loss: 0.10529470443725586\n",
      "Subject 13, Epoch 352, Loss: 0.579200953245163, Final Batch Loss: 0.17234204709529877\n",
      "Subject 13, Epoch 353, Loss: 0.6316741332411766, Final Batch Loss: 0.33509668707847595\n",
      "Subject 13, Epoch 354, Loss: 0.669393315911293, Final Batch Loss: 0.20832861959934235\n",
      "Subject 13, Epoch 355, Loss: 0.496710367500782, Final Batch Loss: 0.12129288166761398\n",
      "Subject 13, Epoch 356, Loss: 0.7936009615659714, Final Batch Loss: 0.28436580300331116\n",
      "Subject 13, Epoch 357, Loss: 0.5944916754961014, Final Batch Loss: 0.22355253994464874\n",
      "Subject 13, Epoch 358, Loss: 0.6595902442932129, Final Batch Loss: 0.2601902484893799\n",
      "Subject 13, Epoch 359, Loss: 0.6779901385307312, Final Batch Loss: 0.240809366106987\n",
      "Subject 13, Epoch 360, Loss: 0.5384168177843094, Final Batch Loss: 0.20981727540493011\n",
      "Subject 13, Epoch 361, Loss: 0.47406741976737976, Final Batch Loss: 0.13692061603069305\n",
      "Subject 13, Epoch 362, Loss: 0.5539505481719971, Final Batch Loss: 0.1381548047065735\n",
      "Subject 13, Epoch 363, Loss: 0.6412287503480911, Final Batch Loss: 0.18137869238853455\n",
      "Subject 13, Epoch 364, Loss: 0.6183917224407196, Final Batch Loss: 0.273316353559494\n",
      "Subject 13, Epoch 365, Loss: 0.6161268353462219, Final Batch Loss: 0.197149395942688\n",
      "Subject 13, Epoch 366, Loss: 0.46988482773303986, Final Batch Loss: 0.16706429421901703\n",
      "Subject 13, Epoch 367, Loss: 0.5544034689664841, Final Batch Loss: 0.18688884377479553\n",
      "Subject 13, Epoch 368, Loss: 0.5020971298217773, Final Batch Loss: 0.16546806693077087\n",
      "Subject 13, Epoch 369, Loss: 0.5019991472363472, Final Batch Loss: 0.21033114194869995\n",
      "Subject 13, Epoch 370, Loss: 0.5718843787908554, Final Batch Loss: 0.17365720868110657\n",
      "Subject 13, Epoch 371, Loss: 0.6317043602466583, Final Batch Loss: 0.30944159626960754\n",
      "Subject 13, Epoch 372, Loss: 0.49926355481147766, Final Batch Loss: 0.14579683542251587\n",
      "Subject 13, Epoch 373, Loss: 0.5761892944574356, Final Batch Loss: 0.22207792103290558\n",
      "Subject 13, Epoch 374, Loss: 0.5595398545265198, Final Batch Loss: 0.1664402186870575\n",
      "Subject 13, Epoch 375, Loss: 0.6023073494434357, Final Batch Loss: 0.20510996878147125\n",
      "Subject 13, Epoch 376, Loss: 0.5298696905374527, Final Batch Loss: 0.16947591304779053\n",
      "Subject 13, Epoch 377, Loss: 0.48136964440345764, Final Batch Loss: 0.12853595614433289\n",
      "Subject 13, Epoch 378, Loss: 0.5804349333047867, Final Batch Loss: 0.1546160727739334\n",
      "Subject 13, Epoch 379, Loss: 0.6039622724056244, Final Batch Loss: 0.25523656606674194\n",
      "Subject 13, Epoch 380, Loss: 0.742293119430542, Final Batch Loss: 0.27380886673927307\n",
      "Subject 13, Epoch 381, Loss: 0.46329018473625183, Final Batch Loss: 0.11664354801177979\n",
      "Subject 13, Epoch 382, Loss: 0.5469371676445007, Final Batch Loss: 0.16094277799129486\n",
      "Subject 13, Epoch 383, Loss: 0.5101665407419205, Final Batch Loss: 0.19558528065681458\n",
      "Subject 13, Epoch 384, Loss: 0.49610036611557007, Final Batch Loss: 0.14135393500328064\n",
      "Subject 13, Epoch 385, Loss: 0.5499774813652039, Final Batch Loss: 0.20803385972976685\n",
      "Subject 13, Epoch 386, Loss: 0.45795927196741104, Final Batch Loss: 0.09328553825616837\n",
      "Subject 13, Epoch 387, Loss: 0.4630659967660904, Final Batch Loss: 0.10447177290916443\n",
      "Subject 13, Epoch 388, Loss: 0.5953656136989594, Final Batch Loss: 0.21925866603851318\n",
      "Subject 13, Epoch 389, Loss: 0.5006876587867737, Final Batch Loss: 0.13403494656085968\n",
      "Subject 13, Epoch 390, Loss: 0.4605560749769211, Final Batch Loss: 0.10046300292015076\n",
      "Subject 13, Epoch 391, Loss: 0.43707655370235443, Final Batch Loss: 0.15116357803344727\n",
      "Subject 13, Epoch 392, Loss: 0.6258958429098129, Final Batch Loss: 0.2616628110408783\n",
      "Subject 13, Epoch 393, Loss: 0.5394671559333801, Final Batch Loss: 0.15198582410812378\n",
      "Subject 13, Epoch 394, Loss: 0.5335776209831238, Final Batch Loss: 0.12587901949882507\n",
      "Subject 13, Epoch 395, Loss: 0.5186333209276199, Final Batch Loss: 0.18841250240802765\n",
      "Subject 13, Epoch 396, Loss: 0.3927020728588104, Final Batch Loss: 0.09678904712200165\n",
      "Subject 13, Epoch 397, Loss: 0.4942583590745926, Final Batch Loss: 0.19952750205993652\n",
      "Subject 13, Epoch 398, Loss: 0.5515599548816681, Final Batch Loss: 0.15215089917182922\n",
      "Subject 13, Epoch 399, Loss: 0.3884229138493538, Final Batch Loss: 0.16155952215194702\n",
      "Subject 13, Epoch 400, Loss: 0.5614096373319626, Final Batch Loss: 0.15369173884391785\n",
      "Subject 13, Epoch 401, Loss: 0.42566752433776855, Final Batch Loss: 0.1371023803949356\n",
      "Subject 13, Epoch 402, Loss: 0.4908788874745369, Final Batch Loss: 0.12129836529493332\n",
      "Subject 13, Epoch 403, Loss: 0.412976510822773, Final Batch Loss: 0.13843804597854614\n",
      "Subject 13, Epoch 404, Loss: 0.4981409013271332, Final Batch Loss: 0.20900124311447144\n",
      "Subject 13, Epoch 405, Loss: 0.47311975806951523, Final Batch Loss: 0.11387389153242111\n",
      "Subject 13, Epoch 406, Loss: 0.6091866195201874, Final Batch Loss: 0.3100275993347168\n",
      "Subject 13, Epoch 407, Loss: 0.5521441251039505, Final Batch Loss: 0.23630958795547485\n",
      "Subject 13, Epoch 408, Loss: 0.5142322778701782, Final Batch Loss: 0.1735578030347824\n",
      "Subject 13, Epoch 409, Loss: 0.49685150384902954, Final Batch Loss: 0.19468480348587036\n",
      "Subject 13, Epoch 410, Loss: 0.5468835383653641, Final Batch Loss: 0.11843469738960266\n",
      "Subject 13, Epoch 411, Loss: 0.34156376495957375, Final Batch Loss: 0.0472787581384182\n",
      "Subject 13, Epoch 412, Loss: 0.5008269995450974, Final Batch Loss: 0.18079450726509094\n",
      "Subject 13, Epoch 413, Loss: 0.505209892988205, Final Batch Loss: 0.12035414576530457\n",
      "Subject 13, Epoch 414, Loss: 0.5247509926557541, Final Batch Loss: 0.24012213945388794\n",
      "Subject 13, Epoch 415, Loss: 0.5329038947820663, Final Batch Loss: 0.12605880200862885\n",
      "Subject 13, Epoch 416, Loss: 0.36906901001930237, Final Batch Loss: 0.10517588257789612\n",
      "Subject 13, Epoch 417, Loss: 0.40499918162822723, Final Batch Loss: 0.14462225139141083\n",
      "Subject 13, Epoch 418, Loss: 0.465492881834507, Final Batch Loss: 0.10429148375988007\n",
      "Subject 13, Epoch 419, Loss: 0.5400129407644272, Final Batch Loss: 0.22201743721961975\n",
      "Subject 13, Epoch 420, Loss: 0.49728017300367355, Final Batch Loss: 0.22110752761363983\n",
      "Subject 13, Epoch 421, Loss: 0.5395928025245667, Final Batch Loss: 0.20665059983730316\n",
      "Subject 13, Epoch 422, Loss: 0.42417851090431213, Final Batch Loss: 0.1373300403356552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 423, Loss: 0.3582485020160675, Final Batch Loss: 0.054721370339393616\n",
      "Subject 13, Epoch 424, Loss: 0.5337430089712143, Final Batch Loss: 0.17237652838230133\n",
      "Subject 13, Epoch 425, Loss: 0.4602597802877426, Final Batch Loss: 0.21746964752674103\n",
      "Subject 13, Epoch 426, Loss: 0.36035458743572235, Final Batch Loss: 0.09077963978052139\n",
      "Subject 13, Epoch 427, Loss: 0.37319672107696533, Final Batch Loss: 0.1041983887553215\n",
      "Subject 13, Epoch 428, Loss: 0.4034590348601341, Final Batch Loss: 0.11497639119625092\n",
      "Subject 13, Epoch 429, Loss: 0.4275532513856888, Final Batch Loss: 0.1816018968820572\n",
      "Subject 13, Epoch 430, Loss: 0.4341622218489647, Final Batch Loss: 0.20207513868808746\n",
      "Subject 13, Epoch 431, Loss: 0.4583725333213806, Final Batch Loss: 0.12569352984428406\n",
      "Subject 13, Epoch 432, Loss: 0.4518205225467682, Final Batch Loss: 0.1523556262254715\n",
      "Subject 13, Epoch 433, Loss: 0.41913557797670364, Final Batch Loss: 0.09113423526287079\n",
      "Subject 13, Epoch 434, Loss: 0.5311103910207748, Final Batch Loss: 0.21731702983379364\n",
      "Subject 13, Epoch 435, Loss: 0.4985867738723755, Final Batch Loss: 0.20949827134609222\n",
      "Subject 13, Epoch 436, Loss: 0.3889583647251129, Final Batch Loss: 0.1595831662416458\n",
      "Subject 13, Epoch 437, Loss: 0.5113027915358543, Final Batch Loss: 0.2998124957084656\n",
      "Subject 13, Epoch 438, Loss: 0.4211641773581505, Final Batch Loss: 0.1696745753288269\n",
      "Subject 13, Epoch 439, Loss: 0.5374087542295456, Final Batch Loss: 0.09230424463748932\n",
      "Subject 13, Epoch 440, Loss: 0.39338938891887665, Final Batch Loss: 0.10894659161567688\n",
      "Subject 13, Epoch 441, Loss: 0.5328936874866486, Final Batch Loss: 0.16118349134922028\n",
      "Subject 13, Epoch 442, Loss: 0.47177083790302277, Final Batch Loss: 0.1126575618982315\n",
      "Subject 13, Epoch 443, Loss: 0.509940505027771, Final Batch Loss: 0.1124514639377594\n",
      "Subject 13, Epoch 444, Loss: 0.4887688010931015, Final Batch Loss: 0.23058956861495972\n",
      "Subject 13, Epoch 445, Loss: 0.38112910836935043, Final Batch Loss: 0.1461072713136673\n",
      "Subject 13, Epoch 446, Loss: 0.37501369416713715, Final Batch Loss: 0.09230834990739822\n",
      "Subject 13, Epoch 447, Loss: 0.49445879459381104, Final Batch Loss: 0.21727949380874634\n",
      "Subject 13, Epoch 448, Loss: 0.4693717584013939, Final Batch Loss: 0.24665938317775726\n",
      "Subject 13, Epoch 449, Loss: 0.3859614357352257, Final Batch Loss: 0.09703808277845383\n",
      "Subject 13, Epoch 450, Loss: 0.45782407373189926, Final Batch Loss: 0.1748867630958557\n",
      "Subject 13, Epoch 451, Loss: 0.4739289656281471, Final Batch Loss: 0.1614655703306198\n",
      "Subject 13, Epoch 452, Loss: 0.4750051572918892, Final Batch Loss: 0.09398465603590012\n",
      "Subject 13, Epoch 453, Loss: 0.4023021310567856, Final Batch Loss: 0.08749599754810333\n",
      "Subject 13, Epoch 454, Loss: 0.3709305450320244, Final Batch Loss: 0.09410355240106583\n",
      "Subject 13, Epoch 455, Loss: 0.3989327475428581, Final Batch Loss: 0.1574590802192688\n",
      "Subject 13, Epoch 456, Loss: 0.35470499098300934, Final Batch Loss: 0.13001522421836853\n",
      "Subject 13, Epoch 457, Loss: 0.4158731922507286, Final Batch Loss: 0.14844577014446259\n",
      "Subject 13, Epoch 458, Loss: 0.38534028083086014, Final Batch Loss: 0.13699871301651\n",
      "Subject 13, Epoch 459, Loss: 0.3633115887641907, Final Batch Loss: 0.16755934059619904\n",
      "Subject 13, Epoch 460, Loss: 0.3218243569135666, Final Batch Loss: 0.060391589999198914\n",
      "Subject 13, Epoch 461, Loss: 0.46850964426994324, Final Batch Loss: 0.133377343416214\n",
      "Subject 13, Epoch 462, Loss: 0.3948092982172966, Final Batch Loss: 0.1039624884724617\n",
      "Subject 13, Epoch 463, Loss: 0.35588595271110535, Final Batch Loss: 0.09463296085596085\n",
      "Subject 13, Epoch 464, Loss: 0.43600085377693176, Final Batch Loss: 0.07853741943836212\n",
      "Subject 13, Epoch 465, Loss: 0.2590259537100792, Final Batch Loss: 0.07568404078483582\n",
      "Subject 13, Epoch 466, Loss: 0.42282523959875107, Final Batch Loss: 0.09198495000600815\n",
      "Subject 13, Epoch 467, Loss: 0.5352116376161575, Final Batch Loss: 0.2682349979877472\n",
      "Subject 13, Epoch 468, Loss: 0.3751693367958069, Final Batch Loss: 0.10045512765645981\n",
      "Subject 13, Epoch 469, Loss: 0.4125894606113434, Final Batch Loss: 0.18406221270561218\n",
      "Subject 13, Epoch 470, Loss: 0.3830753117799759, Final Batch Loss: 0.12832164764404297\n",
      "Subject 13, Epoch 471, Loss: 0.3015831634402275, Final Batch Loss: 0.10691094398498535\n",
      "Subject 13, Epoch 472, Loss: 0.33990081399679184, Final Batch Loss: 0.09582578390836716\n",
      "Subject 13, Epoch 473, Loss: 0.34704866260290146, Final Batch Loss: 0.09936860203742981\n",
      "Subject 13, Epoch 474, Loss: 0.42795711755752563, Final Batch Loss: 0.1630834937095642\n",
      "Subject 13, Epoch 475, Loss: 0.42651844769716263, Final Batch Loss: 0.13322032988071442\n",
      "Subject 13, Epoch 476, Loss: 0.3697860687971115, Final Batch Loss: 0.1965738832950592\n",
      "Subject 13, Epoch 477, Loss: 0.39030151069164276, Final Batch Loss: 0.1027902141213417\n",
      "Subject 13, Epoch 478, Loss: 0.45951391756534576, Final Batch Loss: 0.17708703875541687\n",
      "Subject 13, Epoch 479, Loss: 0.38556864112615585, Final Batch Loss: 0.12153000384569168\n",
      "Subject 13, Epoch 480, Loss: 0.4762937128543854, Final Batch Loss: 0.1958533376455307\n",
      "Subject 13, Epoch 481, Loss: 0.44069721549749374, Final Batch Loss: 0.20389075577259064\n",
      "Subject 13, Epoch 482, Loss: 0.2919826880097389, Final Batch Loss: 0.07118671387434006\n",
      "Subject 13, Epoch 483, Loss: 0.40911272913217545, Final Batch Loss: 0.183421328663826\n",
      "Subject 13, Epoch 484, Loss: 0.35528121143579483, Final Batch Loss: 0.18458086252212524\n",
      "Subject 13, Epoch 485, Loss: 0.3921132907271385, Final Batch Loss: 0.15679298341274261\n",
      "Subject 13, Epoch 486, Loss: 0.4813462942838669, Final Batch Loss: 0.2144947648048401\n",
      "Subject 13, Epoch 487, Loss: 0.3554207980632782, Final Batch Loss: 0.05813683569431305\n",
      "Subject 13, Epoch 488, Loss: 0.4345998018980026, Final Batch Loss: 0.15391622483730316\n",
      "Subject 13, Epoch 489, Loss: 0.2629137858748436, Final Batch Loss: 0.06531623005867004\n",
      "Subject 13, Epoch 490, Loss: 0.5035307928919792, Final Batch Loss: 0.20259667932987213\n",
      "Subject 13, Epoch 491, Loss: 0.30687473714351654, Final Batch Loss: 0.12080319970846176\n",
      "Subject 13, Epoch 492, Loss: 0.37545081228017807, Final Batch Loss: 0.1370316594839096\n",
      "Subject 13, Epoch 493, Loss: 0.5740615949034691, Final Batch Loss: 0.3414469361305237\n",
      "Subject 13, Epoch 494, Loss: 0.3640065938234329, Final Batch Loss: 0.10889577120542526\n",
      "Subject 13, Epoch 495, Loss: 0.35712265968322754, Final Batch Loss: 0.1585494875907898\n",
      "Subject 13, Epoch 496, Loss: 0.34381095319986343, Final Batch Loss: 0.11395937949419022\n",
      "Subject 13, Epoch 497, Loss: 0.39724961668252945, Final Batch Loss: 0.16400936245918274\n",
      "Subject 13, Epoch 498, Loss: 0.2487499825656414, Final Batch Loss: 0.10600945353507996\n",
      "Subject 13, Epoch 499, Loss: 0.37898702174425125, Final Batch Loss: 0.08803794533014297\n",
      "Subject 13, Epoch 500, Loss: 0.30739036202430725, Final Batch Loss: 0.10262494534254074\n",
      "Subject 13, Epoch 501, Loss: 0.3169042393565178, Final Batch Loss: 0.11157214641571045\n",
      "Subject 13, Epoch 502, Loss: 0.3043275959789753, Final Batch Loss: 0.04481375589966774\n",
      "Subject 13, Epoch 503, Loss: 0.2669692412018776, Final Batch Loss: 0.11452469229698181\n",
      "Subject 13, Epoch 504, Loss: 0.3399925231933594, Final Batch Loss: 0.1165386289358139\n",
      "Subject 13, Epoch 505, Loss: 0.4361874908208847, Final Batch Loss: 0.2236754149198532\n",
      "Subject 13, Epoch 506, Loss: 0.45548752695322037, Final Batch Loss: 0.2349652498960495\n",
      "Subject 13, Epoch 507, Loss: 0.4594581425189972, Final Batch Loss: 0.19556637108325958\n",
      "Subject 13, Epoch 508, Loss: 0.21553902328014374, Final Batch Loss: 0.024960331618785858\n",
      "Subject 13, Epoch 509, Loss: 0.2717990018427372, Final Batch Loss: 0.05921425297856331\n",
      "Subject 13, Epoch 510, Loss: 0.3205670043826103, Final Batch Loss: 0.13754071295261383\n",
      "Subject 13, Epoch 511, Loss: 0.26213744282722473, Final Batch Loss: 0.046837061643600464\n",
      "Subject 13, Epoch 512, Loss: 0.24958059936761856, Final Batch Loss: 0.07760164141654968\n",
      "Subject 13, Epoch 513, Loss: 0.320149190723896, Final Batch Loss: 0.08774814754724503\n",
      "Subject 13, Epoch 514, Loss: 0.3324989974498749, Final Batch Loss: 0.05156254768371582\n",
      "Subject 13, Epoch 515, Loss: 0.397347629070282, Final Batch Loss: 0.17193476855754852\n",
      "Subject 13, Epoch 516, Loss: 0.3574952781200409, Final Batch Loss: 0.06902237236499786\n",
      "Subject 13, Epoch 517, Loss: 0.3108367696404457, Final Batch Loss: 0.09684649109840393\n",
      "Subject 13, Epoch 518, Loss: 0.34431109577417374, Final Batch Loss: 0.08967084437608719\n",
      "Subject 13, Epoch 519, Loss: 0.31055794656276703, Final Batch Loss: 0.06714444607496262\n",
      "Subject 13, Epoch 520, Loss: 0.42851780354976654, Final Batch Loss: 0.08824416995048523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 521, Loss: 0.3394024111330509, Final Batch Loss: 0.1412128359079361\n",
      "Subject 13, Epoch 522, Loss: 0.25637489184737206, Final Batch Loss: 0.048520419746637344\n",
      "Subject 13, Epoch 523, Loss: 0.3186241537332535, Final Batch Loss: 0.09189976751804352\n",
      "Subject 13, Epoch 524, Loss: 0.3720633015036583, Final Batch Loss: 0.17932838201522827\n",
      "Subject 13, Epoch 525, Loss: 0.3268699645996094, Final Batch Loss: 0.1157827228307724\n",
      "Subject 13, Epoch 526, Loss: 0.4289385452866554, Final Batch Loss: 0.14964644610881805\n",
      "Subject 13, Epoch 527, Loss: 0.42649978399276733, Final Batch Loss: 0.14886362850666046\n",
      "Subject 13, Epoch 528, Loss: 0.3849514126777649, Final Batch Loss: 0.16745048761367798\n",
      "Subject 13, Epoch 529, Loss: 0.3552107512950897, Final Batch Loss: 0.1925090253353119\n",
      "Subject 13, Epoch 530, Loss: 0.26322032883763313, Final Batch Loss: 0.09850781410932541\n",
      "Subject 13, Epoch 531, Loss: 0.3017456531524658, Final Batch Loss: 0.08534689247608185\n",
      "Subject 13, Epoch 532, Loss: 0.31381670385599136, Final Batch Loss: 0.09639526158571243\n",
      "Subject 13, Epoch 533, Loss: 0.3469849303364754, Final Batch Loss: 0.10966499149799347\n",
      "Subject 13, Epoch 534, Loss: 0.29581624269485474, Final Batch Loss: 0.08219458162784576\n",
      "Subject 13, Epoch 535, Loss: 0.28517116606235504, Final Batch Loss: 0.07451384514570236\n",
      "Subject 13, Epoch 536, Loss: 0.42245233803987503, Final Batch Loss: 0.11479128897190094\n",
      "Subject 13, Epoch 537, Loss: 0.452777162194252, Final Batch Loss: 0.10805974900722504\n",
      "Subject 13, Epoch 538, Loss: 0.26533204317092896, Final Batch Loss: 0.10314669460058212\n",
      "Subject 13, Epoch 539, Loss: 0.2801545262336731, Final Batch Loss: 0.07940518110990524\n",
      "Subject 13, Epoch 540, Loss: 0.2864192947745323, Final Batch Loss: 0.102060966193676\n",
      "Subject 13, Epoch 541, Loss: 0.20164267718791962, Final Batch Loss: 0.0630718320608139\n",
      "Subject 13, Epoch 542, Loss: 0.311065211892128, Final Batch Loss: 0.09333052486181259\n",
      "Subject 13, Epoch 543, Loss: 0.3369701988995075, Final Batch Loss: 0.15111635625362396\n",
      "Subject 13, Epoch 544, Loss: 0.3029012568295002, Final Batch Loss: 0.136592835187912\n",
      "Subject 13, Epoch 545, Loss: 0.1913400050252676, Final Batch Loss: 0.019645726308226585\n",
      "Subject 13, Epoch 546, Loss: 0.2025627326220274, Final Batch Loss: 0.023085301741957664\n",
      "Subject 13, Epoch 547, Loss: 0.3094432093203068, Final Batch Loss: 0.09233797341585159\n",
      "Subject 13, Epoch 548, Loss: 0.30930740386247635, Final Batch Loss: 0.09732750058174133\n",
      "Subject 13, Epoch 549, Loss: 0.21353958547115326, Final Batch Loss: 0.04715193808078766\n",
      "Subject 13, Epoch 550, Loss: 0.3574156239628792, Final Batch Loss: 0.1695268154144287\n",
      "Subject 13, Epoch 551, Loss: 0.4119100868701935, Final Batch Loss: 0.15348993241786957\n",
      "Subject 13, Epoch 552, Loss: 0.27951616048812866, Final Batch Loss: 0.0828092098236084\n",
      "Subject 13, Epoch 553, Loss: 0.2048463337123394, Final Batch Loss: 0.05927041545510292\n",
      "Subject 13, Epoch 554, Loss: 0.2654014825820923, Final Batch Loss: 0.10271936655044556\n",
      "Subject 13, Epoch 555, Loss: 0.2866435721516609, Final Batch Loss: 0.12305280566215515\n",
      "Subject 13, Epoch 556, Loss: 0.28336986526846886, Final Batch Loss: 0.1303398311138153\n",
      "Subject 13, Epoch 557, Loss: 0.24994619190692902, Final Batch Loss: 0.09651292860507965\n",
      "Subject 13, Epoch 558, Loss: 0.23851299285888672, Final Batch Loss: 0.05820871889591217\n",
      "Subject 13, Epoch 559, Loss: 0.3001798242330551, Final Batch Loss: 0.057980045676231384\n",
      "Subject 13, Epoch 560, Loss: 0.3537697419524193, Final Batch Loss: 0.19639045000076294\n",
      "Subject 13, Epoch 561, Loss: 0.2556833103299141, Final Batch Loss: 0.07102539390325546\n",
      "Subject 13, Epoch 562, Loss: 0.3328660652041435, Final Batch Loss: 0.15936145186424255\n",
      "Subject 13, Epoch 563, Loss: 0.25929536670446396, Final Batch Loss: 0.047997839748859406\n",
      "Subject 13, Epoch 564, Loss: 0.25377174466848373, Final Batch Loss: 0.06449535489082336\n",
      "Subject 13, Epoch 565, Loss: 0.3048861473798752, Final Batch Loss: 0.13466912508010864\n",
      "Subject 13, Epoch 566, Loss: 0.3508743941783905, Final Batch Loss: 0.11086925864219666\n",
      "Subject 13, Epoch 567, Loss: 0.22385907918214798, Final Batch Loss: 0.06741217523813248\n",
      "Subject 13, Epoch 568, Loss: 0.31251130998134613, Final Batch Loss: 0.11305294930934906\n",
      "Subject 13, Epoch 569, Loss: 0.2783035561442375, Final Batch Loss: 0.09645576030015945\n",
      "Subject 13, Epoch 570, Loss: 0.27621469646692276, Final Batch Loss: 0.0454174280166626\n",
      "Subject 13, Epoch 571, Loss: 0.3131348118185997, Final Batch Loss: 0.10101268440485\n",
      "Subject 13, Epoch 572, Loss: 0.23594221472740173, Final Batch Loss: 0.10202157497406006\n",
      "Subject 13, Epoch 573, Loss: 0.24187429621815681, Final Batch Loss: 0.10983283817768097\n",
      "Subject 13, Epoch 574, Loss: 0.21546093001961708, Final Batch Loss: 0.045677218586206436\n",
      "Subject 13, Epoch 575, Loss: 0.3588375709950924, Final Batch Loss: 0.07241972535848618\n",
      "Subject 13, Epoch 576, Loss: 0.2559705525636673, Final Batch Loss: 0.10343518853187561\n",
      "Subject 13, Epoch 577, Loss: 0.3357308357954025, Final Batch Loss: 0.1375579535961151\n",
      "Subject 13, Epoch 578, Loss: 0.3066212013363838, Final Batch Loss: 0.1345262974500656\n",
      "Subject 13, Epoch 579, Loss: 0.2591918632388115, Final Batch Loss: 0.05834394693374634\n",
      "Subject 13, Epoch 580, Loss: 0.2992296889424324, Final Batch Loss: 0.11953304708003998\n",
      "Subject 13, Epoch 581, Loss: 0.18851889669895172, Final Batch Loss: 0.04763279855251312\n",
      "Subject 13, Epoch 582, Loss: 0.31896254420280457, Final Batch Loss: 0.09596246480941772\n",
      "Subject 13, Epoch 583, Loss: 0.31063178181648254, Final Batch Loss: 0.08590598404407501\n",
      "Subject 13, Epoch 584, Loss: 0.24284375458955765, Final Batch Loss: 0.07647345960140228\n",
      "Subject 13, Epoch 585, Loss: 0.33847011625766754, Final Batch Loss: 0.1842973828315735\n",
      "Subject 13, Epoch 586, Loss: 0.3073483482003212, Final Batch Loss: 0.10844764858484268\n",
      "Subject 13, Epoch 587, Loss: 0.3486904799938202, Final Batch Loss: 0.10495512932538986\n",
      "Subject 13, Epoch 588, Loss: 0.2720239944756031, Final Batch Loss: 0.11797063797712326\n",
      "Subject 13, Epoch 589, Loss: 0.36871255189180374, Final Batch Loss: 0.19357489049434662\n",
      "Subject 13, Epoch 590, Loss: 0.27613340690732, Final Batch Loss: 0.12194689363241196\n",
      "Subject 13, Epoch 591, Loss: 0.3222835138440132, Final Batch Loss: 0.0824439749121666\n",
      "Subject 13, Epoch 592, Loss: 0.25695936754345894, Final Batch Loss: 0.05101775750517845\n",
      "Subject 13, Epoch 593, Loss: 0.26177262142300606, Final Batch Loss: 0.0939084067940712\n",
      "Subject 13, Epoch 594, Loss: 0.18379128351807594, Final Batch Loss: 0.044917698949575424\n",
      "Subject 13, Epoch 595, Loss: 0.19825701415538788, Final Batch Loss: 0.038481637835502625\n",
      "Subject 13, Epoch 596, Loss: 0.2505806013941765, Final Batch Loss: 0.07753334194421768\n",
      "Subject 13, Epoch 597, Loss: 0.23086642287671566, Final Batch Loss: 0.02665231190621853\n",
      "Subject 13, Epoch 598, Loss: 0.28880422562360764, Final Batch Loss: 0.13318614661693573\n",
      "Subject 13, Epoch 599, Loss: 0.3143256828188896, Final Batch Loss: 0.08937389403581619\n",
      "Subject 13, Epoch 600, Loss: 0.39623646438121796, Final Batch Loss: 0.21259739995002747\n",
      "Subject 13, Epoch 601, Loss: 0.3754876032471657, Final Batch Loss: 0.06250252574682236\n",
      "Subject 13, Epoch 602, Loss: 0.461015909910202, Final Batch Loss: 0.14902116358280182\n",
      "Subject 13, Epoch 603, Loss: 0.24436764419078827, Final Batch Loss: 0.05064786970615387\n",
      "Subject 13, Epoch 604, Loss: 0.2125728353857994, Final Batch Loss: 0.04703567922115326\n",
      "Subject 13, Epoch 605, Loss: 0.33094316720962524, Final Batch Loss: 0.18159440159797668\n",
      "Subject 13, Epoch 606, Loss: 0.2495492845773697, Final Batch Loss: 0.06447520852088928\n",
      "Subject 13, Epoch 607, Loss: 0.2444206103682518, Final Batch Loss: 0.059161823242902756\n",
      "Subject 13, Epoch 608, Loss: 0.21686379611492157, Final Batch Loss: 0.050749316811561584\n",
      "Subject 13, Epoch 609, Loss: 0.3098698928952217, Final Batch Loss: 0.13195684552192688\n",
      "Subject 13, Epoch 610, Loss: 0.30252377688884735, Final Batch Loss: 0.09707936644554138\n",
      "Subject 13, Epoch 611, Loss: 0.24047716706991196, Final Batch Loss: 0.11378877609968185\n",
      "Subject 13, Epoch 612, Loss: 0.24630919471383095, Final Batch Loss: 0.038561608642339706\n",
      "Subject 13, Epoch 613, Loss: 0.23558050766587257, Final Batch Loss: 0.05598195269703865\n",
      "Subject 13, Epoch 614, Loss: 0.22678516060113907, Final Batch Loss: 0.0858255922794342\n",
      "Subject 13, Epoch 615, Loss: 0.25186755508184433, Final Batch Loss: 0.06427503377199173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 616, Loss: 0.1999242976307869, Final Batch Loss: 0.07363227009773254\n",
      "Subject 13, Epoch 617, Loss: 0.27338995784521103, Final Batch Loss: 0.10274216532707214\n",
      "Subject 13, Epoch 618, Loss: 0.25668053328990936, Final Batch Loss: 0.1076376736164093\n",
      "Subject 13, Epoch 619, Loss: 0.2629813477396965, Final Batch Loss: 0.07047397643327713\n",
      "Subject 13, Epoch 620, Loss: 0.21493392810225487, Final Batch Loss: 0.05548602715134621\n",
      "Subject 13, Epoch 621, Loss: 0.18637766689062119, Final Batch Loss: 0.07222238928079605\n",
      "Subject 13, Epoch 622, Loss: 0.16875508800148964, Final Batch Loss: 0.04678206518292427\n",
      "Subject 13, Epoch 623, Loss: 0.22090081498026848, Final Batch Loss: 0.057858143001794815\n",
      "Subject 13, Epoch 624, Loss: 0.1636706441640854, Final Batch Loss: 0.03255055844783783\n",
      "Subject 13, Epoch 625, Loss: 0.19768454506993294, Final Batch Loss: 0.04544413089752197\n",
      "Subject 13, Epoch 626, Loss: 0.17614370584487915, Final Batch Loss: 0.03968336433172226\n",
      "Subject 13, Epoch 627, Loss: 0.2243822067975998, Final Batch Loss: 0.07589298486709595\n",
      "Subject 13, Epoch 628, Loss: 0.23986972495913506, Final Batch Loss: 0.04003593698143959\n",
      "Subject 13, Epoch 629, Loss: 0.1783118173480034, Final Batch Loss: 0.0457826592028141\n",
      "Subject 13, Epoch 630, Loss: 0.285883329808712, Final Batch Loss: 0.13668571412563324\n",
      "Subject 13, Epoch 631, Loss: 0.44865182042121887, Final Batch Loss: 0.23438303172588348\n",
      "Subject 13, Epoch 632, Loss: 0.2311195321381092, Final Batch Loss: 0.04585001617670059\n",
      "Subject 13, Epoch 633, Loss: 0.2704446092247963, Final Batch Loss: 0.13442407548427582\n",
      "Subject 13, Epoch 634, Loss: 0.23564260080456734, Final Batch Loss: 0.06221863254904747\n",
      "Subject 13, Epoch 635, Loss: 0.17682039737701416, Final Batch Loss: 0.0561244934797287\n",
      "Subject 13, Epoch 636, Loss: 0.21186604350805283, Final Batch Loss: 0.08221478015184402\n",
      "Subject 13, Epoch 637, Loss: 0.16064487397670746, Final Batch Loss: 0.04876807704567909\n",
      "Subject 13, Epoch 638, Loss: 0.3212057501077652, Final Batch Loss: 0.06679114699363708\n",
      "Subject 13, Epoch 639, Loss: 0.1874401718378067, Final Batch Loss: 0.06189412623643875\n",
      "Subject 13, Epoch 640, Loss: 0.2666032686829567, Final Batch Loss: 0.052939847111701965\n",
      "Subject 13, Epoch 641, Loss: 0.3425358310341835, Final Batch Loss: 0.12143705785274506\n",
      "Subject 13, Epoch 642, Loss: 0.23488113284111023, Final Batch Loss: 0.08202369511127472\n",
      "Subject 13, Epoch 643, Loss: 0.21673517301678658, Final Batch Loss: 0.07617693394422531\n",
      "Subject 13, Epoch 644, Loss: 0.2082057110965252, Final Batch Loss: 0.04079833999276161\n",
      "Subject 13, Epoch 645, Loss: 0.36523985862731934, Final Batch Loss: 0.1405297964811325\n",
      "Subject 13, Epoch 646, Loss: 0.16347915679216385, Final Batch Loss: 0.07348122447729111\n",
      "Subject 13, Epoch 647, Loss: 0.217724971473217, Final Batch Loss: 0.08627953380346298\n",
      "Subject 13, Epoch 648, Loss: 0.3185102194547653, Final Batch Loss: 0.07128150016069412\n",
      "Subject 13, Epoch 649, Loss: 0.23793864995241165, Final Batch Loss: 0.08663690090179443\n",
      "Subject 13, Epoch 650, Loss: 0.22220293432474136, Final Batch Loss: 0.05147189646959305\n",
      "Subject 13, Epoch 651, Loss: 0.27489103376865387, Final Batch Loss: 0.11368346214294434\n",
      "Subject 13, Epoch 652, Loss: 0.20492258295416832, Final Batch Loss: 0.0604010634124279\n",
      "Subject 13, Epoch 653, Loss: 0.29622894525527954, Final Batch Loss: 0.10553872585296631\n",
      "Subject 13, Epoch 654, Loss: 0.14129555597901344, Final Batch Loss: 0.04711369052529335\n",
      "Subject 13, Epoch 655, Loss: 0.22751003503799438, Final Batch Loss: 0.10566582530736923\n",
      "Subject 13, Epoch 656, Loss: 0.24465277418494225, Final Batch Loss: 0.11455239355564117\n",
      "Subject 13, Epoch 657, Loss: 0.14430296793580055, Final Batch Loss: 0.08419588208198547\n",
      "Subject 13, Epoch 658, Loss: 0.2548639811575413, Final Batch Loss: 0.10549035668373108\n",
      "Subject 13, Epoch 659, Loss: 0.19338654726743698, Final Batch Loss: 0.09074234217405319\n",
      "Subject 13, Epoch 660, Loss: 0.17112817987799644, Final Batch Loss: 0.04871315881609917\n",
      "Subject 13, Epoch 661, Loss: 0.20479808375239372, Final Batch Loss: 0.0954764187335968\n",
      "Subject 13, Epoch 662, Loss: 0.19903723895549774, Final Batch Loss: 0.05839076265692711\n",
      "Subject 13, Epoch 663, Loss: 0.15943573229014874, Final Batch Loss: 0.05994393303990364\n",
      "Subject 13, Epoch 664, Loss: 0.20149816572666168, Final Batch Loss: 0.07738053798675537\n",
      "Subject 13, Epoch 665, Loss: 0.16193718649446964, Final Batch Loss: 0.02063600905239582\n",
      "Subject 13, Epoch 666, Loss: 0.16457396373152733, Final Batch Loss: 0.039780206978321075\n",
      "Subject 13, Epoch 667, Loss: 0.21453853696584702, Final Batch Loss: 0.07053625583648682\n",
      "Subject 13, Epoch 668, Loss: 0.26750998944044113, Final Batch Loss: 0.12582430243492126\n",
      "Subject 13, Epoch 669, Loss: 0.189682524651289, Final Batch Loss: 0.11248400807380676\n",
      "Subject 13, Epoch 670, Loss: 0.22659122571349144, Final Batch Loss: 0.08262773603200912\n",
      "Subject 13, Epoch 671, Loss: 0.2797871418297291, Final Batch Loss: 0.057608697563409805\n",
      "Subject 13, Epoch 672, Loss: 0.24772315099835396, Final Batch Loss: 0.07548674941062927\n",
      "Subject 13, Epoch 673, Loss: 0.2989659085869789, Final Batch Loss: 0.10071788728237152\n",
      "Subject 13, Epoch 674, Loss: 0.1716435793787241, Final Batch Loss: 0.02469089813530445\n",
      "Subject 13, Epoch 675, Loss: 0.3094446361064911, Final Batch Loss: 0.11158096790313721\n",
      "Subject 13, Epoch 676, Loss: 0.22615548595786095, Final Batch Loss: 0.0880633294582367\n",
      "Subject 13, Epoch 677, Loss: 0.22343146800994873, Final Batch Loss: 0.04438307136297226\n",
      "Subject 13, Epoch 678, Loss: 0.17120231315493584, Final Batch Loss: 0.05137152597308159\n",
      "Subject 13, Epoch 679, Loss: 0.12958376668393612, Final Batch Loss: 0.07241553068161011\n",
      "Subject 13, Epoch 680, Loss: 0.16549978405237198, Final Batch Loss: 0.04653958976268768\n",
      "Subject 13, Epoch 681, Loss: 0.3229352906346321, Final Batch Loss: 0.054419323801994324\n",
      "Subject 13, Epoch 682, Loss: 0.2736315205693245, Final Batch Loss: 0.11694017052650452\n",
      "Subject 13, Epoch 683, Loss: 0.13543074019253254, Final Batch Loss: 0.020217863842844963\n",
      "Subject 13, Epoch 684, Loss: 0.32098549976944923, Final Batch Loss: 0.03560945764183998\n",
      "Subject 13, Epoch 685, Loss: 0.22139722853899002, Final Batch Loss: 0.1342552900314331\n",
      "Subject 13, Epoch 686, Loss: 0.23793629929423332, Final Batch Loss: 0.04202555492520332\n",
      "Subject 13, Epoch 687, Loss: 0.17737970873713493, Final Batch Loss: 0.03666554391384125\n",
      "Subject 13, Epoch 688, Loss: 0.22262921556830406, Final Batch Loss: 0.06328585743904114\n",
      "Subject 13, Epoch 689, Loss: 0.2263980619609356, Final Batch Loss: 0.056932482868433\n",
      "Subject 13, Epoch 690, Loss: 0.2976350076496601, Final Batch Loss: 0.05123721435666084\n",
      "Subject 13, Epoch 691, Loss: 0.18720488995313644, Final Batch Loss: 0.03440959379076958\n",
      "Subject 13, Epoch 692, Loss: 0.1583811528980732, Final Batch Loss: 0.03731776773929596\n",
      "Subject 13, Epoch 693, Loss: 0.25325535237789154, Final Batch Loss: 0.10538152605295181\n",
      "Subject 13, Epoch 694, Loss: 0.25818575359880924, Final Batch Loss: 0.030671188607811928\n",
      "Subject 13, Epoch 695, Loss: 0.33827248960733414, Final Batch Loss: 0.07903430610895157\n",
      "Subject 13, Epoch 696, Loss: 0.19443628937005997, Final Batch Loss: 0.053251463919878006\n",
      "Subject 13, Epoch 697, Loss: 0.23441088199615479, Final Batch Loss: 0.06178995594382286\n",
      "Subject 13, Epoch 698, Loss: 0.2518814876675606, Final Batch Loss: 0.07046796381473541\n",
      "Subject 13, Epoch 699, Loss: 0.18293602764606476, Final Batch Loss: 0.07159126549959183\n",
      "Subject 13, Epoch 700, Loss: 0.15636397898197174, Final Batch Loss: 0.038730550557374954\n",
      "Subject 13, Epoch 701, Loss: 0.14690077304840088, Final Batch Loss: 0.04149320721626282\n",
      "Subject 13, Epoch 702, Loss: 0.1476559415459633, Final Batch Loss: 0.053476009517908096\n",
      "Subject 13, Epoch 703, Loss: 0.20132039859890938, Final Batch Loss: 0.029595796018838882\n",
      "Subject 13, Epoch 704, Loss: 0.17389224097132683, Final Batch Loss: 0.0926048755645752\n",
      "Subject 13, Epoch 705, Loss: 0.20806606858968735, Final Batch Loss: 0.11006975173950195\n",
      "Subject 13, Epoch 706, Loss: 0.20826388895511627, Final Batch Loss: 0.08678683638572693\n",
      "Subject 13, Epoch 707, Loss: 0.184383112937212, Final Batch Loss: 0.060632772743701935\n",
      "Subject 13, Epoch 708, Loss: 0.25449004396796227, Final Batch Loss: 0.043835874646902084\n",
      "Subject 13, Epoch 709, Loss: 0.22038284689188004, Final Batch Loss: 0.1056225523352623\n",
      "Subject 13, Epoch 710, Loss: 0.20796658098697662, Final Batch Loss: 0.05844493210315704\n",
      "Subject 13, Epoch 711, Loss: 0.14039161801338196, Final Batch Loss: 0.046787530183792114\n",
      "Subject 13, Epoch 712, Loss: 0.1338280625641346, Final Batch Loss: 0.0323774553835392\n",
      "Subject 13, Epoch 713, Loss: 0.26310451328754425, Final Batch Loss: 0.10749038308858871\n",
      "Subject 13, Epoch 714, Loss: 0.22255833074450493, Final Batch Loss: 0.05228014290332794\n",
      "Subject 13, Epoch 715, Loss: 0.24066822230815887, Final Batch Loss: 0.067807637155056\n",
      "Subject 13, Epoch 716, Loss: 0.2168494537472725, Final Batch Loss: 0.0765000730752945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 717, Loss: 0.22259558737277985, Final Batch Loss: 0.06838933378458023\n",
      "Subject 13, Epoch 718, Loss: 0.11421310901641846, Final Batch Loss: 0.031873974949121475\n",
      "Subject 13, Epoch 719, Loss: 0.29581011831760406, Final Batch Loss: 0.10302893072366714\n",
      "Subject 13, Epoch 720, Loss: 0.1722230687737465, Final Batch Loss: 0.03845043107867241\n",
      "Subject 13, Epoch 721, Loss: 0.27263888344168663, Final Batch Loss: 0.15426397323608398\n",
      "Subject 13, Epoch 722, Loss: 0.2659444659948349, Final Batch Loss: 0.0708533227443695\n",
      "Subject 13, Epoch 723, Loss: 0.21537966281175613, Final Batch Loss: 0.06259018927812576\n",
      "Subject 13, Epoch 724, Loss: 0.1522439867258072, Final Batch Loss: 0.028686877340078354\n",
      "Subject 13, Epoch 725, Loss: 0.18384148180484772, Final Batch Loss: 0.05419789254665375\n",
      "Subject 13, Epoch 726, Loss: 0.1878114491701126, Final Batch Loss: 0.026774227619171143\n",
      "Subject 13, Epoch 727, Loss: 0.19264531135559082, Final Batch Loss: 0.06356071680784225\n",
      "Subject 13, Epoch 728, Loss: 0.2762109152972698, Final Batch Loss: 0.01787124201655388\n",
      "Subject 13, Epoch 729, Loss: 0.17137502692639828, Final Batch Loss: 0.029459919780492783\n",
      "Subject 13, Epoch 730, Loss: 0.12988810613751411, Final Batch Loss: 0.05583642050623894\n",
      "Subject 13, Epoch 731, Loss: 0.14760140981525183, Final Batch Loss: 0.06276454776525497\n",
      "Subject 13, Epoch 732, Loss: 0.21340617910027504, Final Batch Loss: 0.057700734585523605\n",
      "Subject 13, Epoch 733, Loss: 0.20822124555706978, Final Batch Loss: 0.09344854205846786\n",
      "Subject 13, Epoch 734, Loss: 0.13003331795334816, Final Batch Loss: 0.03229133412241936\n",
      "Subject 13, Epoch 735, Loss: 0.189423693343997, Final Batch Loss: 0.02215980552136898\n",
      "Subject 13, Epoch 736, Loss: 0.20391418784856796, Final Batch Loss: 0.050895676016807556\n",
      "Subject 13, Epoch 737, Loss: 0.2184130772948265, Final Batch Loss: 0.09537345170974731\n",
      "Subject 13, Epoch 738, Loss: 0.15781161561608315, Final Batch Loss: 0.053094565868377686\n",
      "Subject 13, Epoch 739, Loss: 0.16785695776343346, Final Batch Loss: 0.05026011914014816\n",
      "Subject 13, Epoch 740, Loss: 0.11464370228350163, Final Batch Loss: 0.018117545172572136\n",
      "Subject 13, Epoch 741, Loss: 0.13423801213502884, Final Batch Loss: 0.011500116437673569\n",
      "Subject 13, Epoch 742, Loss: 0.14963089115917683, Final Batch Loss: 0.014731647446751595\n",
      "Subject 13, Epoch 743, Loss: 0.17366503924131393, Final Batch Loss: 0.048426494002342224\n",
      "Subject 13, Epoch 744, Loss: 0.21577810496091843, Final Batch Loss: 0.07853221148252487\n",
      "Subject 13, Epoch 745, Loss: 0.22751618921756744, Final Batch Loss: 0.08489744365215302\n",
      "Subject 13, Epoch 746, Loss: 0.11292434856295586, Final Batch Loss: 0.0204454492777586\n",
      "Subject 13, Epoch 747, Loss: 0.2604006491601467, Final Batch Loss: 0.13573569059371948\n",
      "Subject 13, Epoch 748, Loss: 0.2090197429060936, Final Batch Loss: 0.04226774722337723\n",
      "Subject 13, Epoch 749, Loss: 0.21580123528838158, Final Batch Loss: 0.0846419483423233\n",
      "Subject 13, Epoch 750, Loss: 0.13302762806415558, Final Batch Loss: 0.040557779371738434\n",
      "Subject 13, Epoch 751, Loss: 0.23778042942285538, Final Batch Loss: 0.1263689547777176\n",
      "Subject 13, Epoch 752, Loss: 0.16295713931322098, Final Batch Loss: 0.05377649888396263\n",
      "Subject 13, Epoch 753, Loss: 0.15373294986784458, Final Batch Loss: 0.020371997728943825\n",
      "Subject 13, Epoch 754, Loss: 0.14752290397882462, Final Batch Loss: 0.05391981452703476\n",
      "Subject 13, Epoch 755, Loss: 0.12645107507705688, Final Batch Loss: 0.03106880933046341\n",
      "Subject 13, Epoch 756, Loss: 0.09905158169567585, Final Batch Loss: 0.023722561076283455\n",
      "Subject 13, Epoch 757, Loss: 0.19166932255029678, Final Batch Loss: 0.0654970109462738\n",
      "Subject 13, Epoch 758, Loss: 0.2089260034263134, Final Batch Loss: 0.047509048134088516\n",
      "Subject 13, Epoch 759, Loss: 0.14352016150951385, Final Batch Loss: 0.05515417456626892\n",
      "Subject 13, Epoch 760, Loss: 0.23968922346830368, Final Batch Loss: 0.07213586568832397\n",
      "Subject 13, Epoch 761, Loss: 0.18317235261201859, Final Batch Loss: 0.107731893658638\n",
      "Subject 13, Epoch 762, Loss: 0.3214104473590851, Final Batch Loss: 0.11303193122148514\n",
      "Subject 13, Epoch 763, Loss: 0.2813453674316406, Final Batch Loss: 0.07712237536907196\n",
      "Subject 13, Epoch 764, Loss: 0.23278381675481796, Final Batch Loss: 0.04609910398721695\n",
      "Subject 13, Epoch 765, Loss: 0.21651656925678253, Final Batch Loss: 0.11214110255241394\n",
      "Subject 13, Epoch 766, Loss: 0.2592054381966591, Final Batch Loss: 0.11791709810495377\n",
      "Subject 13, Epoch 767, Loss: 0.1917450688779354, Final Batch Loss: 0.048312682658433914\n",
      "Subject 13, Epoch 768, Loss: 0.23947212100028992, Final Batch Loss: 0.07044006139039993\n",
      "Subject 13, Epoch 769, Loss: 0.22025369480252266, Final Batch Loss: 0.04946945980191231\n",
      "Subject 13, Epoch 770, Loss: 0.15837736427783966, Final Batch Loss: 0.027118459343910217\n",
      "Subject 13, Epoch 771, Loss: 0.28133711963891983, Final Batch Loss: 0.132978618144989\n",
      "Subject 13, Epoch 772, Loss: 0.08953803405165672, Final Batch Loss: 0.01751268096268177\n",
      "Subject 13, Epoch 773, Loss: 0.22159511223435402, Final Batch Loss: 0.08567893505096436\n",
      "Subject 13, Epoch 774, Loss: 0.14125588163733482, Final Batch Loss: 0.04124020040035248\n",
      "Subject 13, Epoch 775, Loss: 0.31368011981248856, Final Batch Loss: 0.13881295919418335\n",
      "Subject 13, Epoch 776, Loss: 0.20626289397478104, Final Batch Loss: 0.10045310109853745\n",
      "Subject 13, Epoch 777, Loss: 0.14875228330492973, Final Batch Loss: 0.05077904462814331\n",
      "Subject 13, Epoch 778, Loss: 0.12758899107575417, Final Batch Loss: 0.05844810977578163\n",
      "Subject 13, Epoch 779, Loss: 0.23633534461259842, Final Batch Loss: 0.08393544703722\n",
      "Subject 13, Epoch 780, Loss: 0.18803396821022034, Final Batch Loss: 0.03633623942732811\n",
      "Subject 13, Epoch 781, Loss: 0.17450406029820442, Final Batch Loss: 0.09084060788154602\n",
      "Subject 13, Epoch 782, Loss: 0.17649754509329796, Final Batch Loss: 0.050641875714063644\n",
      "Subject 13, Epoch 783, Loss: 0.10063441842794418, Final Batch Loss: 0.02495610900223255\n",
      "Subject 13, Epoch 784, Loss: 0.16686154529452324, Final Batch Loss: 0.10108030587434769\n",
      "Subject 13, Epoch 785, Loss: 0.12757951952517033, Final Batch Loss: 0.016537221148610115\n",
      "Subject 13, Epoch 786, Loss: 0.16850536316633224, Final Batch Loss: 0.06894717365503311\n",
      "Subject 13, Epoch 787, Loss: 0.19022443145513535, Final Batch Loss: 0.02621990442276001\n",
      "Subject 13, Epoch 788, Loss: 0.13445370644330978, Final Batch Loss: 0.03370368853211403\n",
      "Subject 13, Epoch 789, Loss: 0.1452863309532404, Final Batch Loss: 0.03124828450381756\n",
      "Subject 13, Epoch 790, Loss: 0.17442712932825089, Final Batch Loss: 0.0744437575340271\n",
      "Subject 13, Epoch 791, Loss: 0.09596249461174011, Final Batch Loss: 0.03131864219903946\n",
      "Subject 13, Epoch 792, Loss: 0.1294407993555069, Final Batch Loss: 0.03076571226119995\n",
      "Subject 13, Epoch 793, Loss: 0.13687524944543839, Final Batch Loss: 0.053612690418958664\n",
      "Subject 13, Epoch 794, Loss: 0.18061885237693787, Final Batch Loss: 0.09830190986394882\n",
      "Subject 13, Epoch 795, Loss: 0.09157477878034115, Final Batch Loss: 0.032693833112716675\n",
      "Subject 13, Epoch 796, Loss: 0.16951517388224602, Final Batch Loss: 0.09744451195001602\n",
      "Subject 13, Epoch 797, Loss: 0.16928505711257458, Final Batch Loss: 0.04488328844308853\n",
      "Subject 13, Epoch 798, Loss: 0.13101374730467796, Final Batch Loss: 0.01863231509923935\n",
      "Subject 13, Epoch 799, Loss: 0.13242403231561184, Final Batch Loss: 0.0466509610414505\n",
      "Subject 13, Epoch 800, Loss: 0.26726769283413887, Final Batch Loss: 0.03348219767212868\n",
      "Subject 13, Epoch 801, Loss: 0.15197940915822983, Final Batch Loss: 0.03506359085440636\n",
      "Subject 13, Epoch 802, Loss: 0.1094424370676279, Final Batch Loss: 0.040185797959566116\n",
      "Subject 13, Epoch 803, Loss: 0.2384045347571373, Final Batch Loss: 0.07647385448217392\n",
      "Subject 13, Epoch 804, Loss: 0.18091170862317085, Final Batch Loss: 0.04725147783756256\n",
      "Subject 13, Epoch 805, Loss: 0.17412698455154896, Final Batch Loss: 0.11210771650075912\n",
      "Subject 13, Epoch 806, Loss: 0.17094668745994568, Final Batch Loss: 0.05924178659915924\n",
      "Subject 13, Epoch 807, Loss: 0.23199430108070374, Final Batch Loss: 0.04693347215652466\n",
      "Subject 13, Epoch 808, Loss: 0.13192472979426384, Final Batch Loss: 0.011434298008680344\n",
      "Subject 13, Epoch 809, Loss: 0.15426919981837273, Final Batch Loss: 0.051402777433395386\n",
      "Subject 13, Epoch 810, Loss: 0.17009274289011955, Final Batch Loss: 0.03928668424487114\n",
      "Subject 13, Epoch 811, Loss: 0.12009237706661224, Final Batch Loss: 0.033810894936323166\n",
      "Subject 13, Epoch 812, Loss: 0.1405072584748268, Final Batch Loss: 0.06870971620082855\n",
      "Subject 13, Epoch 813, Loss: 0.15475384518504143, Final Batch Loss: 0.04892835393548012\n",
      "Subject 13, Epoch 814, Loss: 0.12354438379406929, Final Batch Loss: 0.03445785865187645\n",
      "Subject 13, Epoch 815, Loss: 0.16614973172545433, Final Batch Loss: 0.06206009164452553\n",
      "Subject 13, Epoch 816, Loss: 0.17786093801259995, Final Batch Loss: 0.03280765935778618\n",
      "Subject 13, Epoch 817, Loss: 0.12197860702872276, Final Batch Loss: 0.019039951264858246\n",
      "Subject 13, Epoch 818, Loss: 0.26643963903188705, Final Batch Loss: 0.1190999448299408\n",
      "Subject 13, Epoch 819, Loss: 0.13004839047789574, Final Batch Loss: 0.018630556762218475\n",
      "Subject 13, Epoch 820, Loss: 0.2626619264483452, Final Batch Loss: 0.06700118631124496\n",
      "Subject 13, Epoch 821, Loss: 0.1697986051440239, Final Batch Loss: 0.09808853268623352\n",
      "Subject 13, Epoch 822, Loss: 0.0766876321285963, Final Batch Loss: 0.040201831609010696\n",
      "Subject 13, Epoch 823, Loss: 0.06982625462114811, Final Batch Loss: 0.031089140102267265\n",
      "Subject 13, Epoch 824, Loss: 0.18359193950891495, Final Batch Loss: 0.06432810425758362\n",
      "Subject 13, Epoch 825, Loss: 0.10212928615510464, Final Batch Loss: 0.0406290628015995\n",
      "Subject 13, Epoch 826, Loss: 0.10159235075116158, Final Batch Loss: 0.04636068642139435\n",
      "Subject 13, Epoch 827, Loss: 0.10858980566263199, Final Batch Loss: 0.030930208042263985\n",
      "Subject 13, Epoch 828, Loss: 0.13756699115037918, Final Batch Loss: 0.05462847277522087\n",
      "Subject 13, Epoch 829, Loss: 0.15954159572720528, Final Batch Loss: 0.04975544661283493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 830, Loss: 0.21167970821261406, Final Batch Loss: 0.12701474130153656\n",
      "Subject 13, Epoch 831, Loss: 0.10717716068029404, Final Batch Loss: 0.027598541229963303\n",
      "Subject 13, Epoch 832, Loss: 0.2290380820631981, Final Batch Loss: 0.12125114351511002\n",
      "Subject 13, Epoch 833, Loss: 0.13863950222730637, Final Batch Loss: 0.044796139001846313\n",
      "Subject 13, Epoch 834, Loss: 0.11106995306909084, Final Batch Loss: 0.06409915536642075\n",
      "Subject 13, Epoch 835, Loss: 0.1732189804315567, Final Batch Loss: 0.04796336591243744\n",
      "Subject 13, Epoch 836, Loss: 0.13098573312163353, Final Batch Loss: 0.0431298166513443\n",
      "Subject 13, Epoch 837, Loss: 0.15167216211557388, Final Batch Loss: 0.08609960973262787\n",
      "Subject 13, Epoch 838, Loss: 0.10328666027635336, Final Batch Loss: 0.008029548451304436\n",
      "Subject 13, Epoch 839, Loss: 0.12488404847681522, Final Batch Loss: 0.016936948522925377\n",
      "Subject 13, Epoch 840, Loss: 0.12088610418140888, Final Batch Loss: 0.030141005292534828\n",
      "Subject 13, Epoch 841, Loss: 0.08940501138567924, Final Batch Loss: 0.03434247523546219\n",
      "Subject 13, Epoch 842, Loss: 0.10320338048040867, Final Batch Loss: 0.03412174433469772\n",
      "Subject 13, Epoch 843, Loss: 0.12416922673583031, Final Batch Loss: 0.02480360120534897\n",
      "Subject 13, Epoch 844, Loss: 0.09748815558850765, Final Batch Loss: 0.03678075224161148\n",
      "Subject 13, Epoch 845, Loss: 0.24862521141767502, Final Batch Loss: 0.08660266548395157\n",
      "Subject 13, Epoch 846, Loss: 0.27127720415592194, Final Batch Loss: 0.06923765689134598\n",
      "Subject 13, Epoch 847, Loss: 0.26556555181741714, Final Batch Loss: 0.1288868933916092\n",
      "Subject 13, Epoch 848, Loss: 0.16340767964720726, Final Batch Loss: 0.03735199570655823\n",
      "Subject 13, Epoch 849, Loss: 0.16190761141479015, Final Batch Loss: 0.02332920767366886\n",
      "Subject 13, Epoch 850, Loss: 0.11226211115717888, Final Batch Loss: 0.046471331268548965\n",
      "Subject 13, Epoch 851, Loss: 0.26352230459451675, Final Batch Loss: 0.15381841361522675\n",
      "Subject 13, Epoch 852, Loss: 0.22921215556561947, Final Batch Loss: 0.0977383628487587\n",
      "Subject 13, Epoch 853, Loss: 0.28424130380153656, Final Batch Loss: 0.07049208134412766\n",
      "Subject 13, Epoch 854, Loss: 0.20183319877833128, Final Batch Loss: 0.012373088859021664\n",
      "Subject 13, Epoch 855, Loss: 0.2923937775194645, Final Batch Loss: 0.13908910751342773\n",
      "Subject 13, Epoch 856, Loss: 0.18847263976931572, Final Batch Loss: 0.07692670077085495\n",
      "Subject 13, Epoch 857, Loss: 0.14025293290615082, Final Batch Loss: 0.05600275471806526\n",
      "Subject 13, Epoch 858, Loss: 0.12463466264307499, Final Batch Loss: 0.06796734035015106\n",
      "Subject 13, Epoch 859, Loss: 0.13586144894361496, Final Batch Loss: 0.04571100324392319\n",
      "Subject 13, Epoch 860, Loss: 0.10517137497663498, Final Batch Loss: 0.028621233999729156\n",
      "Subject 13, Epoch 861, Loss: 0.15247023850679398, Final Batch Loss: 0.08545222878456116\n",
      "Subject 13, Epoch 862, Loss: 0.24288060516119003, Final Batch Loss: 0.16823850572109222\n",
      "Subject 13, Epoch 863, Loss: 0.18224244564771652, Final Batch Loss: 0.02966412901878357\n",
      "Subject 13, Epoch 864, Loss: 0.24222354777157307, Final Batch Loss: 0.02971022017300129\n",
      "Subject 13, Epoch 865, Loss: 0.2821779251098633, Final Batch Loss: 0.07864336669445038\n",
      "Subject 13, Epoch 866, Loss: 0.13773947581648827, Final Batch Loss: 0.02904462441802025\n",
      "Subject 13, Epoch 867, Loss: 0.17535915970802307, Final Batch Loss: 0.04883377626538277\n",
      "Subject 13, Epoch 868, Loss: 0.1952952817082405, Final Batch Loss: 0.11934348195791245\n",
      "Subject 13, Epoch 869, Loss: 0.10717103257775307, Final Batch Loss: 0.03431994467973709\n",
      "Subject 13, Epoch 870, Loss: 0.135409377515316, Final Batch Loss: 0.04070093855261803\n",
      "Subject 13, Epoch 871, Loss: 0.10034581273794174, Final Batch Loss: 0.02758137695491314\n",
      "Subject 13, Epoch 872, Loss: 0.09469999093562365, Final Batch Loss: 0.042373765259981155\n",
      "Subject 13, Epoch 873, Loss: 0.14401094429194927, Final Batch Loss: 0.017881030216813087\n",
      "Subject 13, Epoch 874, Loss: 0.17846880480647087, Final Batch Loss: 0.06385873258113861\n",
      "Subject 13, Epoch 875, Loss: 0.1265317238867283, Final Batch Loss: 0.03332814574241638\n",
      "Subject 13, Epoch 876, Loss: 0.15987620502710342, Final Batch Loss: 0.047565098851919174\n",
      "Subject 13, Epoch 877, Loss: 0.21870138496160507, Final Batch Loss: 0.06330084800720215\n",
      "Subject 13, Epoch 878, Loss: 0.10920645482838154, Final Batch Loss: 0.04297521337866783\n",
      "Subject 13, Epoch 879, Loss: 0.1395389661192894, Final Batch Loss: 0.030083712190389633\n",
      "Subject 13, Epoch 880, Loss: 0.13279923051595688, Final Batch Loss: 0.05105127394199371\n",
      "Subject 13, Epoch 881, Loss: 0.15847830846905708, Final Batch Loss: 0.03233031556010246\n",
      "Subject 13, Epoch 882, Loss: 0.17585471831262112, Final Batch Loss: 0.06272318959236145\n",
      "Subject 13, Epoch 883, Loss: 0.2564733996987343, Final Batch Loss: 0.08037737011909485\n",
      "Subject 13, Epoch 884, Loss: 0.171501524746418, Final Batch Loss: 0.06492701917886734\n",
      "Subject 13, Epoch 885, Loss: 0.23329134285449982, Final Batch Loss: 0.05562632530927658\n",
      "Subject 13, Epoch 886, Loss: 0.08190116658806801, Final Batch Loss: 0.02695411443710327\n",
      "Subject 13, Epoch 887, Loss: 0.08470499329268932, Final Batch Loss: 0.01706945151090622\n",
      "Subject 13, Epoch 888, Loss: 0.15274779498577118, Final Batch Loss: 0.0603361576795578\n",
      "Subject 13, Epoch 889, Loss: 0.06542634405195713, Final Batch Loss: 0.021152283996343613\n",
      "Subject 13, Epoch 890, Loss: 0.16444618627429008, Final Batch Loss: 0.02522249147295952\n",
      "Subject 13, Epoch 891, Loss: 0.09022054076194763, Final Batch Loss: 0.029682733118534088\n",
      "Subject 13, Epoch 892, Loss: 0.14504248648881912, Final Batch Loss: 0.040067680180072784\n",
      "Subject 13, Epoch 893, Loss: 0.09657767228782177, Final Batch Loss: 0.041982024908065796\n",
      "Subject 13, Epoch 894, Loss: 0.12242481485009193, Final Batch Loss: 0.03707332909107208\n",
      "Subject 13, Epoch 895, Loss: 0.14030341058969498, Final Batch Loss: 0.10185179859399796\n",
      "Subject 13, Epoch 896, Loss: 0.13539256528019905, Final Batch Loss: 0.036990318447351456\n",
      "Subject 13, Epoch 897, Loss: 0.2008600663393736, Final Batch Loss: 0.049100179225206375\n",
      "Subject 13, Epoch 898, Loss: 0.17297957092523575, Final Batch Loss: 0.09124376624822617\n",
      "Subject 13, Epoch 899, Loss: 0.1895950883626938, Final Batch Loss: 0.08127494901418686\n",
      "Subject 13, Epoch 900, Loss: 0.16557428427040577, Final Batch Loss: 0.08754264563322067\n",
      "Subject 13, Epoch 901, Loss: 0.1131401052698493, Final Batch Loss: 0.06301882863044739\n",
      "Subject 13, Epoch 902, Loss: 0.12808589823544025, Final Batch Loss: 0.02595708705484867\n",
      "Subject 13, Epoch 903, Loss: 0.2893669940531254, Final Batch Loss: 0.07355838268995285\n",
      "Subject 13, Epoch 904, Loss: 0.20497091859579086, Final Batch Loss: 0.06729260087013245\n",
      "Subject 13, Epoch 905, Loss: 0.23329033702611923, Final Batch Loss: 0.10015241801738739\n",
      "Subject 13, Epoch 906, Loss: 0.12011540494859219, Final Batch Loss: 0.06314465403556824\n",
      "Subject 13, Epoch 907, Loss: 0.15405351668596268, Final Batch Loss: 0.05931122601032257\n",
      "Subject 13, Epoch 908, Loss: 0.1364342514425516, Final Batch Loss: 0.05028309300541878\n",
      "Subject 13, Epoch 909, Loss: 0.07472782768309116, Final Batch Loss: 0.022785410284996033\n",
      "Subject 13, Epoch 910, Loss: 0.1053054928779602, Final Batch Loss: 0.03726224973797798\n",
      "Subject 13, Epoch 911, Loss: 0.15654894150793552, Final Batch Loss: 0.09759226441383362\n",
      "Subject 13, Epoch 912, Loss: 0.05886066146194935, Final Batch Loss: 0.006885100156068802\n",
      "Subject 13, Epoch 913, Loss: 0.11432546842843294, Final Batch Loss: 0.0373784564435482\n",
      "Subject 13, Epoch 914, Loss: 0.17864469066262245, Final Batch Loss: 0.076749287545681\n",
      "Subject 13, Epoch 915, Loss: 0.05803854390978813, Final Batch Loss: 0.008596044033765793\n",
      "Subject 13, Epoch 916, Loss: 0.11240926943719387, Final Batch Loss: 0.03961123898625374\n",
      "Subject 13, Epoch 917, Loss: 0.07870878931134939, Final Batch Loss: 0.042297620326280594\n",
      "Subject 13, Epoch 918, Loss: 0.1878855563700199, Final Batch Loss: 0.07455989718437195\n",
      "Subject 13, Epoch 919, Loss: 0.11958488449454308, Final Batch Loss: 0.06846076250076294\n",
      "Subject 13, Epoch 920, Loss: 0.12503736279904842, Final Batch Loss: 0.03046773187816143\n",
      "Subject 13, Epoch 921, Loss: 0.26187463104724884, Final Batch Loss: 0.13886699080467224\n",
      "Subject 13, Epoch 922, Loss: 0.06766755972057581, Final Batch Loss: 0.023010171949863434\n",
      "Subject 13, Epoch 923, Loss: 0.10039241146296263, Final Batch Loss: 0.05735911428928375\n",
      "Subject 13, Epoch 924, Loss: 0.06487768888473511, Final Batch Loss: 0.015147361904382706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13, Epoch 925, Loss: 0.1991555169224739, Final Batch Loss: 0.0648193433880806\n",
      "Subject 13, Epoch 926, Loss: 0.12055815570056438, Final Batch Loss: 0.019238976761698723\n",
      "Subject 13, Epoch 927, Loss: 0.18931680731475353, Final Batch Loss: 0.025681359693408012\n",
      "Subject 13, Epoch 928, Loss: 0.15216824412345886, Final Batch Loss: 0.038979947566986084\n",
      "Subject 13, Epoch 929, Loss: 0.08378458209335804, Final Batch Loss: 0.04561808705329895\n",
      "Subject 13, Epoch 930, Loss: 0.15420985035598278, Final Batch Loss: 0.05944579839706421\n",
      "Subject 13, Epoch 931, Loss: 0.1617927337065339, Final Batch Loss: 0.06123634800314903\n",
      "Subject 13, Epoch 932, Loss: 0.2543248124420643, Final Batch Loss: 0.09105463325977325\n",
      "Subject 13, Epoch 933, Loss: 0.08407485485076904, Final Batch Loss: 0.05179760977625847\n",
      "Subject 13, Epoch 934, Loss: 0.15370766445994377, Final Batch Loss: 0.06810794025659561\n",
      "Subject 13, Epoch 935, Loss: 0.12326416745781898, Final Batch Loss: 0.04229094088077545\n",
      "Subject 13, Epoch 936, Loss: 0.10361785441637039, Final Batch Loss: 0.04173753410577774\n",
      "Subject 13, Epoch 937, Loss: 0.15150215849280357, Final Batch Loss: 0.08379949629306793\n",
      "Subject 13, Epoch 938, Loss: 0.07914777472615242, Final Batch Loss: 0.026229005306959152\n",
      "Subject 13, Epoch 939, Loss: 0.09942205157130957, Final Batch Loss: 0.013962469063699245\n",
      "Subject 13, Epoch 940, Loss: 0.08911627158522606, Final Batch Loss: 0.01720321550965309\n",
      "Subject 13, Epoch 941, Loss: 0.13942496851086617, Final Batch Loss: 0.03549148142337799\n",
      "Subject 13, Epoch 942, Loss: 0.07609819434583187, Final Batch Loss: 0.020298782736063004\n",
      "Subject 13, Epoch 943, Loss: 0.14363479195162654, Final Batch Loss: 0.00679103983566165\n",
      "Subject 13, Epoch 944, Loss: 0.16944498755037785, Final Batch Loss: 0.061226554214954376\n",
      "Subject 13, Epoch 945, Loss: 0.14282883889973164, Final Batch Loss: 0.0709393098950386\n",
      "Subject 13, Epoch 946, Loss: 0.08928802609443665, Final Batch Loss: 0.03583751618862152\n",
      "Subject 13, Epoch 947, Loss: 0.08459132350981236, Final Batch Loss: 0.024943940341472626\n",
      "Subject 13, Epoch 948, Loss: 0.15344098769128323, Final Batch Loss: 0.022522149607539177\n",
      "Subject 13, Epoch 949, Loss: 0.10774158127605915, Final Batch Loss: 0.08910490572452545\n",
      "Subject 13, Epoch 950, Loss: 0.08736623451113701, Final Batch Loss: 0.02027430199086666\n",
      "Subject 13, Epoch 951, Loss: 0.12317031994462013, Final Batch Loss: 0.022080112248659134\n",
      "Subject 13, Epoch 952, Loss: 0.14706730283796787, Final Batch Loss: 0.08981220424175262\n",
      "Subject 13, Epoch 953, Loss: 0.0734589733183384, Final Batch Loss: 0.021861635148525238\n",
      "Subject 13, Epoch 954, Loss: 0.14404314570128918, Final Batch Loss: 0.07574515044689178\n",
      "Subject 13, Epoch 955, Loss: 0.1868436262011528, Final Batch Loss: 0.03164220601320267\n",
      "Subject 13, Epoch 956, Loss: 0.15792123600840569, Final Batch Loss: 0.06839506328105927\n",
      "Subject 13, Epoch 957, Loss: 0.08525859378278255, Final Batch Loss: 0.019960694015026093\n",
      "Subject 13, Epoch 958, Loss: 0.10396620817482471, Final Batch Loss: 0.02655082195997238\n",
      "Subject 13, Epoch 959, Loss: 0.13689272478222847, Final Batch Loss: 0.06184320151805878\n",
      "Subject 13, Epoch 960, Loss: 0.18938567116856575, Final Batch Loss: 0.08264260739088058\n",
      "Subject 13, Epoch 961, Loss: 0.13477462530136108, Final Batch Loss: 0.05418993905186653\n",
      "Subject 13, Epoch 962, Loss: 0.18464945256710052, Final Batch Loss: 0.01646970957517624\n",
      "Subject 13, Epoch 963, Loss: 0.11874706391245127, Final Batch Loss: 0.0762968510389328\n",
      "Subject 13, Epoch 964, Loss: 0.14684584736824036, Final Batch Loss: 0.0787961333990097\n",
      "Subject 13, Epoch 965, Loss: 0.15241897758096457, Final Batch Loss: 0.015134881250560284\n",
      "Subject 13, Epoch 966, Loss: 0.09152669087052345, Final Batch Loss: 0.025588465854525566\n",
      "Subject 13, Epoch 967, Loss: 0.1312062069773674, Final Batch Loss: 0.04565545171499252\n",
      "Subject 13, Epoch 968, Loss: 0.1313230600208044, Final Batch Loss: 0.07050714641809464\n",
      "Subject 13, Epoch 969, Loss: 0.1128578670322895, Final Batch Loss: 0.013322941958904266\n",
      "Subject 13, Epoch 970, Loss: 0.12587595637887716, Final Batch Loss: 0.060408081859350204\n",
      "Subject 13, Epoch 971, Loss: 0.15275445207953453, Final Batch Loss: 0.08824016153812408\n",
      "Subject 13, Epoch 972, Loss: 0.13099553436040878, Final Batch Loss: 0.016885872930288315\n",
      "Subject 13, Epoch 973, Loss: 0.1671074703335762, Final Batch Loss: 0.029214628040790558\n",
      "Subject 13, Epoch 974, Loss: 0.10117438994348049, Final Batch Loss: 0.02183903567492962\n",
      "Subject 13, Epoch 975, Loss: 0.10734965652227402, Final Batch Loss: 0.03674715384840965\n",
      "Subject 13, Epoch 976, Loss: 0.16080312989652157, Final Batch Loss: 0.030477067455649376\n",
      "Subject 13, Epoch 977, Loss: 0.07185290567576885, Final Batch Loss: 0.016965171322226524\n",
      "Subject 13, Epoch 978, Loss: 0.21047873049974442, Final Batch Loss: 0.08459652960300446\n",
      "Subject 13, Epoch 979, Loss: 0.15254491940140724, Final Batch Loss: 0.04152452200651169\n",
      "Subject 13, Epoch 980, Loss: 0.12176595255732536, Final Batch Loss: 0.0678783655166626\n",
      "Subject 13, Epoch 981, Loss: 0.14848633855581284, Final Batch Loss: 0.09483714401721954\n",
      "Subject 13, Epoch 982, Loss: 0.0478384168818593, Final Batch Loss: 0.0175804253667593\n",
      "Subject 13, Epoch 983, Loss: 0.1230640560388565, Final Batch Loss: 0.035185761749744415\n",
      "Subject 13, Epoch 984, Loss: 0.1317204274237156, Final Batch Loss: 0.06604041904211044\n",
      "Subject 13, Epoch 985, Loss: 0.15571827441453934, Final Batch Loss: 0.030743079259991646\n",
      "Subject 13, Epoch 986, Loss: 0.13320170622318983, Final Batch Loss: 0.04876777529716492\n",
      "Subject 13, Epoch 987, Loss: 0.12111637368798256, Final Batch Loss: 0.018027670681476593\n",
      "Subject 13, Epoch 988, Loss: 0.10512010008096695, Final Batch Loss: 0.03601481765508652\n",
      "Subject 13, Epoch 989, Loss: 0.11649129539728165, Final Batch Loss: 0.04247508943080902\n",
      "Subject 13, Epoch 990, Loss: 0.09019830916076899, Final Batch Loss: 0.00713286641985178\n",
      "Subject 13, Epoch 991, Loss: 0.14412094466388226, Final Batch Loss: 0.016338909044861794\n",
      "Subject 13, Epoch 992, Loss: 0.10893498361110687, Final Batch Loss: 0.026141028851270676\n",
      "Subject 13, Epoch 993, Loss: 0.11957401037216187, Final Batch Loss: 0.028729651123285294\n",
      "Subject 13, Epoch 994, Loss: 0.2390261758118868, Final Batch Loss: 0.15488280355930328\n",
      "Subject 13, Epoch 995, Loss: 0.0774959521368146, Final Batch Loss: 0.009849918074905872\n",
      "Subject 13, Epoch 996, Loss: 0.17816322948783636, Final Batch Loss: 0.04198111221194267\n",
      "Subject 13, Epoch 997, Loss: 0.07041585817933083, Final Batch Loss: 0.02570905163884163\n",
      "Subject 13, Epoch 998, Loss: 0.09002726525068283, Final Batch Loss: 0.03201225399971008\n",
      "Subject 13, Epoch 999, Loss: 0.04103030730038881, Final Batch Loss: 0.0159304141998291\n",
      "Subject 13, Epoch 1000, Loss: 0.14339202642440796, Final Batch Loss: 0.06558077782392502\n",
      "Subject 14, Epoch 1, Loss: 7.240321159362793, Final Batch Loss: 1.8616375923156738\n",
      "Subject 14, Epoch 2, Loss: 7.103680372238159, Final Batch Loss: 1.7277185916900635\n",
      "Subject 14, Epoch 3, Loss: 7.128581523895264, Final Batch Loss: 1.7751532793045044\n",
      "Subject 14, Epoch 4, Loss: 7.113962411880493, Final Batch Loss: 1.7846877574920654\n",
      "Subject 14, Epoch 5, Loss: 7.080163955688477, Final Batch Loss: 1.7581212520599365\n",
      "Subject 14, Epoch 6, Loss: 7.076064586639404, Final Batch Loss: 1.7916607856750488\n",
      "Subject 14, Epoch 7, Loss: 7.014255523681641, Final Batch Loss: 1.778789758682251\n",
      "Subject 14, Epoch 8, Loss: 6.888458609580994, Final Batch Loss: 1.6739721298217773\n",
      "Subject 14, Epoch 9, Loss: 6.769260406494141, Final Batch Loss: 1.5904207229614258\n",
      "Subject 14, Epoch 10, Loss: 6.7438249588012695, Final Batch Loss: 1.631993055343628\n",
      "Subject 14, Epoch 11, Loss: 6.791350603103638, Final Batch Loss: 1.7352721691131592\n",
      "Subject 14, Epoch 12, Loss: 6.488529443740845, Final Batch Loss: 1.48778235912323\n",
      "Subject 14, Epoch 13, Loss: 6.577966213226318, Final Batch Loss: 1.6601117849349976\n",
      "Subject 14, Epoch 14, Loss: 6.33509373664856, Final Batch Loss: 1.519801378250122\n",
      "Subject 14, Epoch 15, Loss: 6.341741919517517, Final Batch Loss: 1.5454397201538086\n",
      "Subject 14, Epoch 16, Loss: 6.1942843198776245, Final Batch Loss: 1.4835621118545532\n",
      "Subject 14, Epoch 17, Loss: 5.902773380279541, Final Batch Loss: 1.2781769037246704\n",
      "Subject 14, Epoch 18, Loss: 6.132521033287048, Final Batch Loss: 1.5623557567596436\n",
      "Subject 14, Epoch 19, Loss: 5.900211930274963, Final Batch Loss: 1.4157910346984863\n",
      "Subject 14, Epoch 20, Loss: 5.960786461830139, Final Batch Loss: 1.5757063627243042\n",
      "Subject 14, Epoch 21, Loss: 5.80013108253479, Final Batch Loss: 1.3720347881317139\n",
      "Subject 14, Epoch 22, Loss: 5.893281579017639, Final Batch Loss: 1.4843143224716187\n",
      "Subject 14, Epoch 23, Loss: 5.602052688598633, Final Batch Loss: 1.3462122678756714\n",
      "Subject 14, Epoch 24, Loss: 5.775397419929504, Final Batch Loss: 1.4937729835510254\n",
      "Subject 14, Epoch 25, Loss: 5.7245951890945435, Final Batch Loss: 1.4491970539093018\n",
      "Subject 14, Epoch 26, Loss: 5.670196294784546, Final Batch Loss: 1.416574239730835\n",
      "Subject 14, Epoch 27, Loss: 5.291090726852417, Final Batch Loss: 1.1724423170089722\n",
      "Subject 14, Epoch 28, Loss: 5.329408407211304, Final Batch Loss: 1.3043655157089233\n",
      "Subject 14, Epoch 29, Loss: 5.255200028419495, Final Batch Loss: 1.2975796461105347\n",
      "Subject 14, Epoch 30, Loss: 5.356752276420593, Final Batch Loss: 1.3778619766235352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 31, Loss: 5.13164210319519, Final Batch Loss: 1.2894655466079712\n",
      "Subject 14, Epoch 32, Loss: 4.7568066120147705, Final Batch Loss: 0.9889004230499268\n",
      "Subject 14, Epoch 33, Loss: 4.905429363250732, Final Batch Loss: 1.1547189950942993\n",
      "Subject 14, Epoch 34, Loss: 4.8244723081588745, Final Batch Loss: 1.232528567314148\n",
      "Subject 14, Epoch 35, Loss: 4.90232241153717, Final Batch Loss: 1.3262642621994019\n",
      "Subject 14, Epoch 36, Loss: 4.351182818412781, Final Batch Loss: 0.977002739906311\n",
      "Subject 14, Epoch 37, Loss: 4.353362739086151, Final Batch Loss: 0.9585360884666443\n",
      "Subject 14, Epoch 38, Loss: 4.283237099647522, Final Batch Loss: 0.9807876348495483\n",
      "Subject 14, Epoch 39, Loss: 4.305668294429779, Final Batch Loss: 1.1660737991333008\n",
      "Subject 14, Epoch 40, Loss: 3.951687514781952, Final Batch Loss: 0.8529233336448669\n",
      "Subject 14, Epoch 41, Loss: 4.038576126098633, Final Batch Loss: 1.0792784690856934\n",
      "Subject 14, Epoch 42, Loss: 4.079136490821838, Final Batch Loss: 1.0829432010650635\n",
      "Subject 14, Epoch 43, Loss: 3.9355528354644775, Final Batch Loss: 0.9489907026290894\n",
      "Subject 14, Epoch 44, Loss: 3.7023062705993652, Final Batch Loss: 0.890397310256958\n",
      "Subject 14, Epoch 45, Loss: 3.784795641899109, Final Batch Loss: 0.9033476710319519\n",
      "Subject 14, Epoch 46, Loss: 3.5051058530807495, Final Batch Loss: 0.8069770932197571\n",
      "Subject 14, Epoch 47, Loss: 3.515363872051239, Final Batch Loss: 0.804480791091919\n",
      "Subject 14, Epoch 48, Loss: 3.8100301027297974, Final Batch Loss: 1.0283193588256836\n",
      "Subject 14, Epoch 49, Loss: 3.6598458886146545, Final Batch Loss: 0.9615237712860107\n",
      "Subject 14, Epoch 50, Loss: 3.4743379950523376, Final Batch Loss: 0.8828624486923218\n",
      "Subject 14, Epoch 51, Loss: 3.4009079337120056, Final Batch Loss: 0.8265661001205444\n",
      "Subject 14, Epoch 52, Loss: 3.722838580608368, Final Batch Loss: 1.0784169435501099\n",
      "Subject 14, Epoch 53, Loss: 3.1241673827171326, Final Batch Loss: 0.7166626453399658\n",
      "Subject 14, Epoch 54, Loss: 3.1883946657180786, Final Batch Loss: 0.6659523248672485\n",
      "Subject 14, Epoch 55, Loss: 3.5127195715904236, Final Batch Loss: 0.8943610787391663\n",
      "Subject 14, Epoch 56, Loss: 3.1466715335845947, Final Batch Loss: 0.8280972838401794\n",
      "Subject 14, Epoch 57, Loss: 3.362036406993866, Final Batch Loss: 1.0548146963119507\n",
      "Subject 14, Epoch 58, Loss: 3.4050859808921814, Final Batch Loss: 1.024420142173767\n",
      "Subject 14, Epoch 59, Loss: 3.227059543132782, Final Batch Loss: 0.7798423171043396\n",
      "Subject 14, Epoch 60, Loss: 3.458808958530426, Final Batch Loss: 1.13672935962677\n",
      "Subject 14, Epoch 61, Loss: 2.9744604229927063, Final Batch Loss: 0.7232477068901062\n",
      "Subject 14, Epoch 62, Loss: 2.816358208656311, Final Batch Loss: 0.5720354914665222\n",
      "Subject 14, Epoch 63, Loss: 2.979044258594513, Final Batch Loss: 0.7246567606925964\n",
      "Subject 14, Epoch 64, Loss: 2.9124727845191956, Final Batch Loss: 0.6702784895896912\n",
      "Subject 14, Epoch 65, Loss: 3.0112937688827515, Final Batch Loss: 0.7208382487297058\n",
      "Subject 14, Epoch 66, Loss: 3.000765800476074, Final Batch Loss: 0.7277479767799377\n",
      "Subject 14, Epoch 67, Loss: 2.820637822151184, Final Batch Loss: 0.7528201341629028\n",
      "Subject 14, Epoch 68, Loss: 2.5874909162521362, Final Batch Loss: 0.40291666984558105\n",
      "Subject 14, Epoch 69, Loss: 2.86960369348526, Final Batch Loss: 0.719226062297821\n",
      "Subject 14, Epoch 70, Loss: 2.774091064929962, Final Batch Loss: 0.7398512363433838\n",
      "Subject 14, Epoch 71, Loss: 2.6767704486846924, Final Batch Loss: 0.620432436466217\n",
      "Subject 14, Epoch 72, Loss: 2.7961052656173706, Final Batch Loss: 0.7547624111175537\n",
      "Subject 14, Epoch 73, Loss: 2.588107168674469, Final Batch Loss: 0.5639277696609497\n",
      "Subject 14, Epoch 74, Loss: 2.5189828276634216, Final Batch Loss: 0.5482465624809265\n",
      "Subject 14, Epoch 75, Loss: 2.6311030983924866, Final Batch Loss: 0.7077966332435608\n",
      "Subject 14, Epoch 76, Loss: 2.4635614156723022, Final Batch Loss: 0.5134205222129822\n",
      "Subject 14, Epoch 77, Loss: 2.6168623566627502, Final Batch Loss: 0.6806914806365967\n",
      "Subject 14, Epoch 78, Loss: 2.420211911201477, Final Batch Loss: 0.548733651638031\n",
      "Subject 14, Epoch 79, Loss: 2.6070517897605896, Final Batch Loss: 0.8095526099205017\n",
      "Subject 14, Epoch 80, Loss: 2.2629190385341644, Final Batch Loss: 0.35469475388526917\n",
      "Subject 14, Epoch 81, Loss: 2.455112099647522, Final Batch Loss: 0.670096218585968\n",
      "Subject 14, Epoch 82, Loss: 2.4788621068000793, Final Batch Loss: 0.6592413187026978\n",
      "Subject 14, Epoch 83, Loss: 2.5155640244483948, Final Batch Loss: 0.5534535646438599\n",
      "Subject 14, Epoch 84, Loss: 2.35024756193161, Final Batch Loss: 0.577185869216919\n",
      "Subject 14, Epoch 85, Loss: 2.3588966131210327, Final Batch Loss: 0.6746830940246582\n",
      "Subject 14, Epoch 86, Loss: 2.3346293568611145, Final Batch Loss: 0.5923563241958618\n",
      "Subject 14, Epoch 87, Loss: 2.507804751396179, Final Batch Loss: 0.7219453454017639\n",
      "Subject 14, Epoch 88, Loss: 2.2476212978363037, Final Batch Loss: 0.5528551936149597\n",
      "Subject 14, Epoch 89, Loss: 2.2602142095565796, Final Batch Loss: 0.5532890558242798\n",
      "Subject 14, Epoch 90, Loss: 2.4376137256622314, Final Batch Loss: 0.7325541377067566\n",
      "Subject 14, Epoch 91, Loss: 2.1572960019111633, Final Batch Loss: 0.5208234786987305\n",
      "Subject 14, Epoch 92, Loss: 1.9657783508300781, Final Batch Loss: 0.32672369480133057\n",
      "Subject 14, Epoch 93, Loss: 2.157303363084793, Final Batch Loss: 0.4302232563495636\n",
      "Subject 14, Epoch 94, Loss: 2.266971707344055, Final Batch Loss: 0.5279991626739502\n",
      "Subject 14, Epoch 95, Loss: 2.096031993627548, Final Batch Loss: 0.47568193078041077\n",
      "Subject 14, Epoch 96, Loss: 2.139659285545349, Final Batch Loss: 0.5610390901565552\n",
      "Subject 14, Epoch 97, Loss: 2.2126165628433228, Final Batch Loss: 0.5574145317077637\n",
      "Subject 14, Epoch 98, Loss: 2.520998954772949, Final Batch Loss: 0.9736716747283936\n",
      "Subject 14, Epoch 99, Loss: 2.202719211578369, Final Batch Loss: 0.6158634424209595\n",
      "Subject 14, Epoch 100, Loss: 2.154313772916794, Final Batch Loss: 0.6261588335037231\n",
      "Subject 14, Epoch 101, Loss: 1.9692765474319458, Final Batch Loss: 0.3745229244232178\n",
      "Subject 14, Epoch 102, Loss: 2.0768553018569946, Final Batch Loss: 0.4808257818222046\n",
      "Subject 14, Epoch 103, Loss: 2.0051109194755554, Final Batch Loss: 0.4740331172943115\n",
      "Subject 14, Epoch 104, Loss: 2.199751079082489, Final Batch Loss: 0.5797460675239563\n",
      "Subject 14, Epoch 105, Loss: 2.110135495662689, Final Batch Loss: 0.6737029552459717\n",
      "Subject 14, Epoch 106, Loss: 1.9358041882514954, Final Batch Loss: 0.3372519612312317\n",
      "Subject 14, Epoch 107, Loss: 1.8700856566429138, Final Batch Loss: 0.32520022988319397\n",
      "Subject 14, Epoch 108, Loss: 2.0651198625564575, Final Batch Loss: 0.6109815239906311\n",
      "Subject 14, Epoch 109, Loss: 1.8764375448226929, Final Batch Loss: 0.4734245836734772\n",
      "Subject 14, Epoch 110, Loss: 1.85232612490654, Final Batch Loss: 0.4547923505306244\n",
      "Subject 14, Epoch 111, Loss: 1.8554234206676483, Final Batch Loss: 0.4946518540382385\n",
      "Subject 14, Epoch 112, Loss: 1.8652900159358978, Final Batch Loss: 0.4382506310939789\n",
      "Subject 14, Epoch 113, Loss: 1.8564592003822327, Final Batch Loss: 0.3718336224555969\n",
      "Subject 14, Epoch 114, Loss: 1.8913746774196625, Final Batch Loss: 0.3963322043418884\n",
      "Subject 14, Epoch 115, Loss: 1.9437929391860962, Final Batch Loss: 0.4022297263145447\n",
      "Subject 14, Epoch 116, Loss: 1.798511654138565, Final Batch Loss: 0.37143823504447937\n",
      "Subject 14, Epoch 117, Loss: 1.855194628238678, Final Batch Loss: 0.33528515696525574\n",
      "Subject 14, Epoch 118, Loss: 1.8009902834892273, Final Batch Loss: 0.3094293773174286\n",
      "Subject 14, Epoch 119, Loss: 1.7413770854473114, Final Batch Loss: 0.397258460521698\n",
      "Subject 14, Epoch 120, Loss: 1.784664899110794, Final Batch Loss: 0.35285085439682007\n",
      "Subject 14, Epoch 121, Loss: 1.805951088666916, Final Batch Loss: 0.33740997314453125\n",
      "Subject 14, Epoch 122, Loss: 1.9829934537410736, Final Batch Loss: 0.6303656101226807\n",
      "Subject 14, Epoch 123, Loss: 1.8286263346672058, Final Batch Loss: 0.45265862345695496\n",
      "Subject 14, Epoch 124, Loss: 1.6877574622631073, Final Batch Loss: 0.46230611205101013\n",
      "Subject 14, Epoch 125, Loss: 1.6495051383972168, Final Batch Loss: 0.3562462627887726\n",
      "Subject 14, Epoch 126, Loss: 1.6315941512584686, Final Batch Loss: 0.23331981897354126\n",
      "Subject 14, Epoch 127, Loss: 1.6678088307380676, Final Batch Loss: 0.33442774415016174\n",
      "Subject 14, Epoch 128, Loss: 1.5342842042446136, Final Batch Loss: 0.14274337887763977\n",
      "Subject 14, Epoch 129, Loss: 1.877603441476822, Final Batch Loss: 0.5164392590522766\n",
      "Subject 14, Epoch 130, Loss: 2.0235047340393066, Final Batch Loss: 0.6123590469360352\n",
      "Subject 14, Epoch 131, Loss: 1.6355521082878113, Final Batch Loss: 0.2544044554233551\n",
      "Subject 14, Epoch 132, Loss: 1.7317498624324799, Final Batch Loss: 0.36129796504974365\n",
      "Subject 14, Epoch 133, Loss: 1.572004109621048, Final Batch Loss: 0.38106396794319153\n",
      "Subject 14, Epoch 134, Loss: 1.8984158337116241, Final Batch Loss: 0.4769613742828369\n",
      "Subject 14, Epoch 135, Loss: 1.7608757019042969, Final Batch Loss: 0.34481990337371826\n",
      "Subject 14, Epoch 136, Loss: 1.6034574210643768, Final Batch Loss: 0.3955378532409668\n",
      "Subject 14, Epoch 137, Loss: 1.7016389071941376, Final Batch Loss: 0.4138718247413635\n",
      "Subject 14, Epoch 138, Loss: 1.6550996601581573, Final Batch Loss: 0.3810376822948456\n",
      "Subject 14, Epoch 139, Loss: 1.6389722526073456, Final Batch Loss: 0.30889615416526794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 140, Loss: 1.5983484983444214, Final Batch Loss: 0.3792650103569031\n",
      "Subject 14, Epoch 141, Loss: 1.7376999855041504, Final Batch Loss: 0.42283007502555847\n",
      "Subject 14, Epoch 142, Loss: 1.7883915901184082, Final Batch Loss: 0.46573227643966675\n",
      "Subject 14, Epoch 143, Loss: 1.7447502613067627, Final Batch Loss: 0.5191410779953003\n",
      "Subject 14, Epoch 144, Loss: 1.8559129238128662, Final Batch Loss: 0.4792082905769348\n",
      "Subject 14, Epoch 145, Loss: 1.7843576073646545, Final Batch Loss: 0.5888867974281311\n",
      "Subject 14, Epoch 146, Loss: 1.7728257477283478, Final Batch Loss: 0.6135618090629578\n",
      "Subject 14, Epoch 147, Loss: 1.6596195697784424, Final Batch Loss: 0.3819805085659027\n",
      "Subject 14, Epoch 148, Loss: 1.4105862975120544, Final Batch Loss: 0.21354946494102478\n",
      "Subject 14, Epoch 149, Loss: 1.3657122254371643, Final Batch Loss: 0.15494367480278015\n",
      "Subject 14, Epoch 150, Loss: 1.5898888409137726, Final Batch Loss: 0.3963896334171295\n",
      "Subject 14, Epoch 151, Loss: 1.7531163096427917, Final Batch Loss: 0.5354256629943848\n",
      "Subject 14, Epoch 152, Loss: 1.738719403743744, Final Batch Loss: 0.4221467673778534\n",
      "Subject 14, Epoch 153, Loss: 1.627955138683319, Final Batch Loss: 0.4759957790374756\n",
      "Subject 14, Epoch 154, Loss: 1.4570660591125488, Final Batch Loss: 0.3154217600822449\n",
      "Subject 14, Epoch 155, Loss: 1.714734822511673, Final Batch Loss: 0.3964788019657135\n",
      "Subject 14, Epoch 156, Loss: 1.4540636837482452, Final Batch Loss: 0.2082696557044983\n",
      "Subject 14, Epoch 157, Loss: 1.8075683116912842, Final Batch Loss: 0.6744186282157898\n",
      "Subject 14, Epoch 158, Loss: 1.6293182969093323, Final Batch Loss: 0.4703194200992584\n",
      "Subject 14, Epoch 159, Loss: 1.371473252773285, Final Batch Loss: 0.3197404146194458\n",
      "Subject 14, Epoch 160, Loss: 1.5245716273784637, Final Batch Loss: 0.31442660093307495\n",
      "Subject 14, Epoch 161, Loss: 1.3485514521598816, Final Batch Loss: 0.28894737362861633\n",
      "Subject 14, Epoch 162, Loss: 1.472207248210907, Final Batch Loss: 0.41437292098999023\n",
      "Subject 14, Epoch 163, Loss: 1.5705691277980804, Final Batch Loss: 0.41750794649124146\n",
      "Subject 14, Epoch 164, Loss: 1.43503937125206, Final Batch Loss: 0.2762662470340729\n",
      "Subject 14, Epoch 165, Loss: 1.586863934993744, Final Batch Loss: 0.37800437211990356\n",
      "Subject 14, Epoch 166, Loss: 1.5570069551467896, Final Batch Loss: 0.4267258048057556\n",
      "Subject 14, Epoch 167, Loss: 1.297211617231369, Final Batch Loss: 0.27534785866737366\n",
      "Subject 14, Epoch 168, Loss: 1.4518989324569702, Final Batch Loss: 0.36922940611839294\n",
      "Subject 14, Epoch 169, Loss: 1.4898231029510498, Final Batch Loss: 0.30683228373527527\n",
      "Subject 14, Epoch 170, Loss: 1.5449284315109253, Final Batch Loss: 0.45006558299064636\n",
      "Subject 14, Epoch 171, Loss: 1.5615794658660889, Final Batch Loss: 0.38591524958610535\n",
      "Subject 14, Epoch 172, Loss: 1.6020939648151398, Final Batch Loss: 0.4626276195049286\n",
      "Subject 14, Epoch 173, Loss: 1.4354352951049805, Final Batch Loss: 0.3621259331703186\n",
      "Subject 14, Epoch 174, Loss: 1.4099296927452087, Final Batch Loss: 0.26800626516342163\n",
      "Subject 14, Epoch 175, Loss: 1.4213916063308716, Final Batch Loss: 0.3343546390533447\n",
      "Subject 14, Epoch 176, Loss: 1.4699948132038116, Final Batch Loss: 0.5026844143867493\n",
      "Subject 14, Epoch 177, Loss: 1.3727155029773712, Final Batch Loss: 0.3233460783958435\n",
      "Subject 14, Epoch 178, Loss: 1.5816156268119812, Final Batch Loss: 0.5050996541976929\n",
      "Subject 14, Epoch 179, Loss: 1.5856177508831024, Final Batch Loss: 0.5437415838241577\n",
      "Subject 14, Epoch 180, Loss: 1.3978264331817627, Final Batch Loss: 0.296834796667099\n",
      "Subject 14, Epoch 181, Loss: 1.2984191924333572, Final Batch Loss: 0.17574982345104218\n",
      "Subject 14, Epoch 182, Loss: 1.4515741765499115, Final Batch Loss: 0.38018888235092163\n",
      "Subject 14, Epoch 183, Loss: 1.4101715385913849, Final Batch Loss: 0.2769337594509125\n",
      "Subject 14, Epoch 184, Loss: 1.2210147678852081, Final Batch Loss: 0.22802391648292542\n",
      "Subject 14, Epoch 185, Loss: 1.2885091602802277, Final Batch Loss: 0.2127414345741272\n",
      "Subject 14, Epoch 186, Loss: 1.336329236626625, Final Batch Loss: 0.2480737715959549\n",
      "Subject 14, Epoch 187, Loss: 1.25094373524189, Final Batch Loss: 0.1791117936372757\n",
      "Subject 14, Epoch 188, Loss: 1.2855325639247894, Final Batch Loss: 0.1852359175682068\n",
      "Subject 14, Epoch 189, Loss: 1.4003994762897491, Final Batch Loss: 0.2958006262779236\n",
      "Subject 14, Epoch 190, Loss: 1.2347054034471512, Final Batch Loss: 0.23134668171405792\n",
      "Subject 14, Epoch 191, Loss: 1.2775799036026, Final Batch Loss: 0.3232080340385437\n",
      "Subject 14, Epoch 192, Loss: 1.4070221185684204, Final Batch Loss: 0.3484882414340973\n",
      "Subject 14, Epoch 193, Loss: 1.4427188634872437, Final Batch Loss: 0.4922025203704834\n",
      "Subject 14, Epoch 194, Loss: 1.4399142861366272, Final Batch Loss: 0.2933262288570404\n",
      "Subject 14, Epoch 195, Loss: 1.277994990348816, Final Batch Loss: 0.2834300696849823\n",
      "Subject 14, Epoch 196, Loss: 1.376414954662323, Final Batch Loss: 0.3919217884540558\n",
      "Subject 14, Epoch 197, Loss: 1.2828506231307983, Final Batch Loss: 0.3228084444999695\n",
      "Subject 14, Epoch 198, Loss: 1.306877225637436, Final Batch Loss: 0.34311479330062866\n",
      "Subject 14, Epoch 199, Loss: 1.4748162925243378, Final Batch Loss: 0.4083893597126007\n",
      "Subject 14, Epoch 200, Loss: 1.5841598212718964, Final Batch Loss: 0.5769858360290527\n",
      "Subject 14, Epoch 201, Loss: 1.6677986085414886, Final Batch Loss: 0.5426075458526611\n",
      "Subject 14, Epoch 202, Loss: 1.2191210091114044, Final Batch Loss: 0.19646704196929932\n",
      "Subject 14, Epoch 203, Loss: 1.4389719069004059, Final Batch Loss: 0.5146467685699463\n",
      "Subject 14, Epoch 204, Loss: 1.085292488336563, Final Batch Loss: 0.21308881044387817\n",
      "Subject 14, Epoch 205, Loss: 1.341213583946228, Final Batch Loss: 0.2936283349990845\n",
      "Subject 14, Epoch 206, Loss: 1.4249388575553894, Final Batch Loss: 0.4810839891433716\n",
      "Subject 14, Epoch 207, Loss: 1.2818526327610016, Final Batch Loss: 0.2897911071777344\n",
      "Subject 14, Epoch 208, Loss: 1.343694418668747, Final Batch Loss: 0.2863895893096924\n",
      "Subject 14, Epoch 209, Loss: 1.2804651260375977, Final Batch Loss: 0.21083128452301025\n",
      "Subject 14, Epoch 210, Loss: 1.4037795066833496, Final Batch Loss: 0.3680250346660614\n",
      "Subject 14, Epoch 211, Loss: 1.2591761946678162, Final Batch Loss: 0.4141569137573242\n",
      "Subject 14, Epoch 212, Loss: 1.1700192540884018, Final Batch Loss: 0.1413918286561966\n",
      "Subject 14, Epoch 213, Loss: 1.198406159877777, Final Batch Loss: 0.24069741368293762\n",
      "Subject 14, Epoch 214, Loss: 1.0873254239559174, Final Batch Loss: 0.23900404572486877\n",
      "Subject 14, Epoch 215, Loss: 1.4467428922653198, Final Batch Loss: 0.4448336362838745\n",
      "Subject 14, Epoch 216, Loss: 1.2147662192583084, Final Batch Loss: 0.34793999791145325\n",
      "Subject 14, Epoch 217, Loss: 1.031472735106945, Final Batch Loss: 0.12033391743898392\n",
      "Subject 14, Epoch 218, Loss: 1.1606017500162125, Final Batch Loss: 0.2439550906419754\n",
      "Subject 14, Epoch 219, Loss: 1.3405279964208603, Final Batch Loss: 0.3787291347980499\n",
      "Subject 14, Epoch 220, Loss: 1.1780312955379486, Final Batch Loss: 0.31661224365234375\n",
      "Subject 14, Epoch 221, Loss: 1.260907769203186, Final Batch Loss: 0.3253609240055084\n",
      "Subject 14, Epoch 222, Loss: 1.1793172806501389, Final Batch Loss: 0.24334625899791718\n",
      "Subject 14, Epoch 223, Loss: 1.014216959476471, Final Batch Loss: 0.13604629039764404\n",
      "Subject 14, Epoch 224, Loss: 1.3691404461860657, Final Batch Loss: 0.4681795835494995\n",
      "Subject 14, Epoch 225, Loss: 1.0810536742210388, Final Batch Loss: 0.2138320505619049\n",
      "Subject 14, Epoch 226, Loss: 1.4767214953899384, Final Batch Loss: 0.6483863592147827\n",
      "Subject 14, Epoch 227, Loss: 1.1057943105697632, Final Batch Loss: 0.20640970766544342\n",
      "Subject 14, Epoch 228, Loss: 1.2751785814762115, Final Batch Loss: 0.3572165369987488\n",
      "Subject 14, Epoch 229, Loss: 1.1870602667331696, Final Batch Loss: 0.25342223048210144\n",
      "Subject 14, Epoch 230, Loss: 1.1018925905227661, Final Batch Loss: 0.21325309574604034\n",
      "Subject 14, Epoch 231, Loss: 1.2866000533103943, Final Batch Loss: 0.4613151252269745\n",
      "Subject 14, Epoch 232, Loss: 1.1555317640304565, Final Batch Loss: 0.27033743262290955\n",
      "Subject 14, Epoch 233, Loss: 1.2792297303676605, Final Batch Loss: 0.34702274203300476\n",
      "Subject 14, Epoch 234, Loss: 1.2797768115997314, Final Batch Loss: 0.3975546360015869\n",
      "Subject 14, Epoch 235, Loss: 1.0971207171678543, Final Batch Loss: 0.2037188559770584\n",
      "Subject 14, Epoch 236, Loss: 2.0209231674671173, Final Batch Loss: 0.9902569055557251\n",
      "Subject 14, Epoch 237, Loss: 1.3482900559902191, Final Batch Loss: 0.454134464263916\n",
      "Subject 14, Epoch 238, Loss: 1.2913851141929626, Final Batch Loss: 0.41529732942581177\n",
      "Subject 14, Epoch 239, Loss: 1.2983006834983826, Final Batch Loss: 0.38963305950164795\n",
      "Subject 14, Epoch 240, Loss: 1.0875191688537598, Final Batch Loss: 0.28518950939178467\n",
      "Subject 14, Epoch 241, Loss: 1.4147783815860748, Final Batch Loss: 0.475905179977417\n",
      "Subject 14, Epoch 242, Loss: 1.068087637424469, Final Batch Loss: 0.16135984659194946\n",
      "Subject 14, Epoch 243, Loss: 1.126975178718567, Final Batch Loss: 0.263096421957016\n",
      "Subject 14, Epoch 244, Loss: 1.1264209896326065, Final Batch Loss: 0.17864127457141876\n",
      "Subject 14, Epoch 245, Loss: 1.3425011038780212, Final Batch Loss: 0.4292553663253784\n",
      "Subject 14, Epoch 246, Loss: 1.451454684138298, Final Batch Loss: 0.6161171793937683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 247, Loss: 1.2042420208454132, Final Batch Loss: 0.298880010843277\n",
      "Subject 14, Epoch 248, Loss: 1.050439491868019, Final Batch Loss: 0.23086698353290558\n",
      "Subject 14, Epoch 249, Loss: 1.0236892700195312, Final Batch Loss: 0.1807188093662262\n",
      "Subject 14, Epoch 250, Loss: 1.2554252445697784, Final Batch Loss: 0.35245513916015625\n",
      "Subject 14, Epoch 251, Loss: 0.9723863303661346, Final Batch Loss: 0.18965816497802734\n",
      "Subject 14, Epoch 252, Loss: 1.1589959263801575, Final Batch Loss: 0.2881861925125122\n",
      "Subject 14, Epoch 253, Loss: 1.1051024198532104, Final Batch Loss: 0.2932589054107666\n",
      "Subject 14, Epoch 254, Loss: 1.0942753702402115, Final Batch Loss: 0.2089339941740036\n",
      "Subject 14, Epoch 255, Loss: 1.026669666171074, Final Batch Loss: 0.25026780366897583\n",
      "Subject 14, Epoch 256, Loss: 1.046205535531044, Final Batch Loss: 0.22585973143577576\n",
      "Subject 14, Epoch 257, Loss: 1.3426012992858887, Final Batch Loss: 0.4929881989955902\n",
      "Subject 14, Epoch 258, Loss: 1.187327727675438, Final Batch Loss: 0.25246337056159973\n",
      "Subject 14, Epoch 259, Loss: 1.011701911687851, Final Batch Loss: 0.1429242491722107\n",
      "Subject 14, Epoch 260, Loss: 1.1430016160011292, Final Batch Loss: 0.3521778881549835\n",
      "Subject 14, Epoch 261, Loss: 1.023318886756897, Final Batch Loss: 0.19470322132110596\n",
      "Subject 14, Epoch 262, Loss: 1.0353999435901642, Final Batch Loss: 0.1329362988471985\n",
      "Subject 14, Epoch 263, Loss: 1.3813708126544952, Final Batch Loss: 0.3642684817314148\n",
      "Subject 14, Epoch 264, Loss: 1.190266728401184, Final Batch Loss: 0.31136253476142883\n",
      "Subject 14, Epoch 265, Loss: 1.0670595169067383, Final Batch Loss: 0.1910405158996582\n",
      "Subject 14, Epoch 266, Loss: 0.9759228676557541, Final Batch Loss: 0.15778355300426483\n",
      "Subject 14, Epoch 267, Loss: 1.034177765250206, Final Batch Loss: 0.22065414488315582\n",
      "Subject 14, Epoch 268, Loss: 0.9857039302587509, Final Batch Loss: 0.17964394390583038\n",
      "Subject 14, Epoch 269, Loss: 1.0743209719657898, Final Batch Loss: 0.18507790565490723\n",
      "Subject 14, Epoch 270, Loss: 1.1555855423212051, Final Batch Loss: 0.29809391498565674\n",
      "Subject 14, Epoch 271, Loss: 1.0695782452821732, Final Batch Loss: 0.24723504483699799\n",
      "Subject 14, Epoch 272, Loss: 1.0111429691314697, Final Batch Loss: 0.20684176683425903\n",
      "Subject 14, Epoch 273, Loss: 1.270301342010498, Final Batch Loss: 0.4249098598957062\n",
      "Subject 14, Epoch 274, Loss: 1.2993399798870087, Final Batch Loss: 0.40514466166496277\n",
      "Subject 14, Epoch 275, Loss: 1.055815428495407, Final Batch Loss: 0.31013932824134827\n",
      "Subject 14, Epoch 276, Loss: 0.9035668820142746, Final Batch Loss: 0.10433697700500488\n",
      "Subject 14, Epoch 277, Loss: 1.090226948261261, Final Batch Loss: 0.22504779696464539\n",
      "Subject 14, Epoch 278, Loss: 1.3716094940900803, Final Batch Loss: 0.6274921298027039\n",
      "Subject 14, Epoch 279, Loss: 1.24616739153862, Final Batch Loss: 0.4922061860561371\n",
      "Subject 14, Epoch 280, Loss: 0.9720752835273743, Final Batch Loss: 0.22390535473823547\n",
      "Subject 14, Epoch 281, Loss: 1.021230548620224, Final Batch Loss: 0.18311870098114014\n",
      "Subject 14, Epoch 282, Loss: 0.9883520007133484, Final Batch Loss: 0.21164561808109283\n",
      "Subject 14, Epoch 283, Loss: 1.3049439191818237, Final Batch Loss: 0.3590751886367798\n",
      "Subject 14, Epoch 284, Loss: 1.2371144741773605, Final Batch Loss: 0.4558764100074768\n",
      "Subject 14, Epoch 285, Loss: 1.294632077217102, Final Batch Loss: 0.452640175819397\n",
      "Subject 14, Epoch 286, Loss: 0.9884049296379089, Final Batch Loss: 0.21285715699195862\n",
      "Subject 14, Epoch 287, Loss: 1.066064715385437, Final Batch Loss: 0.15053290128707886\n",
      "Subject 14, Epoch 288, Loss: 0.9925554096698761, Final Batch Loss: 0.1773919314146042\n",
      "Subject 14, Epoch 289, Loss: 1.0179446339607239, Final Batch Loss: 0.16740486025810242\n",
      "Subject 14, Epoch 290, Loss: 1.063907414674759, Final Batch Loss: 0.2651115655899048\n",
      "Subject 14, Epoch 291, Loss: 1.0088081359863281, Final Batch Loss: 0.24640773236751556\n",
      "Subject 14, Epoch 292, Loss: 1.0870414823293686, Final Batch Loss: 0.33331313729286194\n",
      "Subject 14, Epoch 293, Loss: 1.2699528932571411, Final Batch Loss: 0.46128755807876587\n",
      "Subject 14, Epoch 294, Loss: 1.0359686017036438, Final Batch Loss: 0.26344582438468933\n",
      "Subject 14, Epoch 295, Loss: 0.9117380976676941, Final Batch Loss: 0.14289537072181702\n",
      "Subject 14, Epoch 296, Loss: 0.8512069657444954, Final Batch Loss: 0.07497843354940414\n",
      "Subject 14, Epoch 297, Loss: 1.1093027889728546, Final Batch Loss: 0.2867027819156647\n",
      "Subject 14, Epoch 298, Loss: 1.1848281174898148, Final Batch Loss: 0.36448952555656433\n",
      "Subject 14, Epoch 299, Loss: 1.079489067196846, Final Batch Loss: 0.29888415336608887\n",
      "Subject 14, Epoch 300, Loss: 1.0950651168823242, Final Batch Loss: 0.33954113721847534\n",
      "Subject 14, Epoch 301, Loss: 0.94438736140728, Final Batch Loss: 0.13821756839752197\n",
      "Subject 14, Epoch 302, Loss: 1.1614432632923126, Final Batch Loss: 0.36263489723205566\n",
      "Subject 14, Epoch 303, Loss: 1.1910289376974106, Final Batch Loss: 0.41950201988220215\n",
      "Subject 14, Epoch 304, Loss: 0.96758633852005, Final Batch Loss: 0.1751718968153\n",
      "Subject 14, Epoch 305, Loss: 1.0524384379386902, Final Batch Loss: 0.26054495573043823\n",
      "Subject 14, Epoch 306, Loss: 1.044034093618393, Final Batch Loss: 0.25092756748199463\n",
      "Subject 14, Epoch 307, Loss: 1.099577471613884, Final Batch Loss: 0.3804388642311096\n",
      "Subject 14, Epoch 308, Loss: 1.1140882819890976, Final Batch Loss: 0.40606462955474854\n",
      "Subject 14, Epoch 309, Loss: 1.2439707070589066, Final Batch Loss: 0.46992558240890503\n",
      "Subject 14, Epoch 310, Loss: 0.9538712799549103, Final Batch Loss: 0.22880679368972778\n",
      "Subject 14, Epoch 311, Loss: 1.1889382898807526, Final Batch Loss: 0.4825897812843323\n",
      "Subject 14, Epoch 312, Loss: 1.0185329169034958, Final Batch Loss: 0.2453419417142868\n",
      "Subject 14, Epoch 313, Loss: 1.0217799693346024, Final Batch Loss: 0.2816277742385864\n",
      "Subject 14, Epoch 314, Loss: 1.1705327481031418, Final Batch Loss: 0.43339991569519043\n",
      "Subject 14, Epoch 315, Loss: 0.9921117275953293, Final Batch Loss: 0.23495830595493317\n",
      "Subject 14, Epoch 316, Loss: 1.0233347564935684, Final Batch Loss: 0.3241593539714813\n",
      "Subject 14, Epoch 317, Loss: 0.8822033107280731, Final Batch Loss: 0.1365337371826172\n",
      "Subject 14, Epoch 318, Loss: 0.9951215237379074, Final Batch Loss: 0.3048938810825348\n",
      "Subject 14, Epoch 319, Loss: 1.0338923931121826, Final Batch Loss: 0.13224300742149353\n",
      "Subject 14, Epoch 320, Loss: 1.1545043140649796, Final Batch Loss: 0.2759237289428711\n",
      "Subject 14, Epoch 321, Loss: 0.9332747161388397, Final Batch Loss: 0.2627837657928467\n",
      "Subject 14, Epoch 322, Loss: 0.9831607788801193, Final Batch Loss: 0.23420418798923492\n",
      "Subject 14, Epoch 323, Loss: 0.906191885471344, Final Batch Loss: 0.19641752541065216\n",
      "Subject 14, Epoch 324, Loss: 0.9346907287836075, Final Batch Loss: 0.21269792318344116\n",
      "Subject 14, Epoch 325, Loss: 0.9236956387758255, Final Batch Loss: 0.19849823415279388\n",
      "Subject 14, Epoch 326, Loss: 0.8831518292427063, Final Batch Loss: 0.1447213590145111\n",
      "Subject 14, Epoch 327, Loss: 1.0292534083127975, Final Batch Loss: 0.20759941637516022\n",
      "Subject 14, Epoch 328, Loss: 0.9103821814060211, Final Batch Loss: 0.12760242819786072\n",
      "Subject 14, Epoch 329, Loss: 0.8978854566812515, Final Batch Loss: 0.1977851390838623\n",
      "Subject 14, Epoch 330, Loss: 1.0421224385499954, Final Batch Loss: 0.2573927640914917\n",
      "Subject 14, Epoch 331, Loss: 1.1873022615909576, Final Batch Loss: 0.3483279347419739\n",
      "Subject 14, Epoch 332, Loss: 1.0536321997642517, Final Batch Loss: 0.222499281167984\n",
      "Subject 14, Epoch 333, Loss: 0.8136181458830833, Final Batch Loss: 0.07926192134618759\n",
      "Subject 14, Epoch 334, Loss: 1.049925059080124, Final Batch Loss: 0.2461688369512558\n",
      "Subject 14, Epoch 335, Loss: 1.0958811342716217, Final Batch Loss: 0.4237898588180542\n",
      "Subject 14, Epoch 336, Loss: 0.991600513458252, Final Batch Loss: 0.2796905040740967\n",
      "Subject 14, Epoch 337, Loss: 1.004357472062111, Final Batch Loss: 0.24230289459228516\n",
      "Subject 14, Epoch 338, Loss: 0.8899144679307938, Final Batch Loss: 0.21403886377811432\n",
      "Subject 14, Epoch 339, Loss: 0.8735644519329071, Final Batch Loss: 0.10987193882465363\n",
      "Subject 14, Epoch 340, Loss: 1.0339227318763733, Final Batch Loss: 0.3449699282646179\n",
      "Subject 14, Epoch 341, Loss: 0.9867949336767197, Final Batch Loss: 0.2932724058628082\n",
      "Subject 14, Epoch 342, Loss: 0.8594600111246109, Final Batch Loss: 0.14861226081848145\n",
      "Subject 14, Epoch 343, Loss: 0.9676843732595444, Final Batch Loss: 0.16820786893367767\n",
      "Subject 14, Epoch 344, Loss: 0.8920353055000305, Final Batch Loss: 0.16133487224578857\n",
      "Subject 14, Epoch 345, Loss: 1.0364810079336166, Final Batch Loss: 0.2449752539396286\n",
      "Subject 14, Epoch 346, Loss: 0.9900936782360077, Final Batch Loss: 0.23834878206253052\n",
      "Subject 14, Epoch 347, Loss: 0.939434140920639, Final Batch Loss: 0.15276116132736206\n",
      "Subject 14, Epoch 348, Loss: 0.893897607922554, Final Batch Loss: 0.23094125092029572\n",
      "Subject 14, Epoch 349, Loss: 0.8186521679162979, Final Batch Loss: 0.2322726994752884\n",
      "Subject 14, Epoch 350, Loss: 0.839118167757988, Final Batch Loss: 0.14560502767562866\n",
      "Subject 14, Epoch 351, Loss: 1.18166121840477, Final Batch Loss: 0.4990639090538025\n",
      "Subject 14, Epoch 352, Loss: 0.9297947585582733, Final Batch Loss: 0.3064330518245697\n",
      "Subject 14, Epoch 353, Loss: 1.0010732561349869, Final Batch Loss: 0.24153605103492737\n",
      "Subject 14, Epoch 354, Loss: 0.9563160389661789, Final Batch Loss: 0.28463077545166016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 355, Loss: 0.8877787441015244, Final Batch Loss: 0.17821261286735535\n",
      "Subject 14, Epoch 356, Loss: 0.9766241163015366, Final Batch Loss: 0.29615727066993713\n",
      "Subject 14, Epoch 357, Loss: 0.9105859249830246, Final Batch Loss: 0.20903925597667694\n",
      "Subject 14, Epoch 358, Loss: 0.9261654168367386, Final Batch Loss: 0.15203939378261566\n",
      "Subject 14, Epoch 359, Loss: 0.8556945025920868, Final Batch Loss: 0.14190393686294556\n",
      "Subject 14, Epoch 360, Loss: 0.9334595054388046, Final Batch Loss: 0.3332809507846832\n",
      "Subject 14, Epoch 361, Loss: 0.9134928733110428, Final Batch Loss: 0.17094193398952484\n",
      "Subject 14, Epoch 362, Loss: 0.792715460062027, Final Batch Loss: 0.15994974970817566\n",
      "Subject 14, Epoch 363, Loss: 1.0205630958080292, Final Batch Loss: 0.30293309688568115\n",
      "Subject 14, Epoch 364, Loss: 0.8661286681890488, Final Batch Loss: 0.13767960667610168\n",
      "Subject 14, Epoch 365, Loss: 0.8628660440444946, Final Batch Loss: 0.14430540800094604\n",
      "Subject 14, Epoch 366, Loss: 0.8428187966346741, Final Batch Loss: 0.193147212266922\n",
      "Subject 14, Epoch 367, Loss: 0.7991763055324554, Final Batch Loss: 0.1117788702249527\n",
      "Subject 14, Epoch 368, Loss: 1.0734443813562393, Final Batch Loss: 0.40870389342308044\n",
      "Subject 14, Epoch 369, Loss: 0.9362671971321106, Final Batch Loss: 0.16720008850097656\n",
      "Subject 14, Epoch 370, Loss: 1.0344180017709732, Final Batch Loss: 0.37871912121772766\n",
      "Subject 14, Epoch 371, Loss: 0.8031551241874695, Final Batch Loss: 0.15393799543380737\n",
      "Subject 14, Epoch 372, Loss: 1.1176499426364899, Final Batch Loss: 0.31145724654197693\n",
      "Subject 14, Epoch 373, Loss: 0.8194222450256348, Final Batch Loss: 0.23211143910884857\n",
      "Subject 14, Epoch 374, Loss: 1.002284973859787, Final Batch Loss: 0.3427291512489319\n",
      "Subject 14, Epoch 375, Loss: 1.0504122376441956, Final Batch Loss: 0.367410272359848\n",
      "Subject 14, Epoch 376, Loss: 1.0670049488544464, Final Batch Loss: 0.37241971492767334\n",
      "Subject 14, Epoch 377, Loss: 0.9602817595005035, Final Batch Loss: 0.1835298240184784\n",
      "Subject 14, Epoch 378, Loss: 0.9259300529956818, Final Batch Loss: 0.27285313606262207\n",
      "Subject 14, Epoch 379, Loss: 0.8441712409257889, Final Batch Loss: 0.17315010726451874\n",
      "Subject 14, Epoch 380, Loss: 1.0194082707166672, Final Batch Loss: 0.40148189663887024\n",
      "Subject 14, Epoch 381, Loss: 1.2422231435775757, Final Batch Loss: 0.5914772748947144\n",
      "Subject 14, Epoch 382, Loss: 0.8471704870462418, Final Batch Loss: 0.14997972548007965\n",
      "Subject 14, Epoch 383, Loss: 0.799678236246109, Final Batch Loss: 0.18679633736610413\n",
      "Subject 14, Epoch 384, Loss: 1.0801598727703094, Final Batch Loss: 0.22479018568992615\n",
      "Subject 14, Epoch 385, Loss: 0.9378127604722977, Final Batch Loss: 0.2752481997013092\n",
      "Subject 14, Epoch 386, Loss: 0.8469500094652176, Final Batch Loss: 0.20627915859222412\n",
      "Subject 14, Epoch 387, Loss: 0.7582785487174988, Final Batch Loss: 0.12686163187026978\n",
      "Subject 14, Epoch 388, Loss: 0.9808118790388107, Final Batch Loss: 0.32038870453834534\n",
      "Subject 14, Epoch 389, Loss: 0.9449542909860611, Final Batch Loss: 0.24472615122795105\n",
      "Subject 14, Epoch 390, Loss: 0.8187754899263382, Final Batch Loss: 0.14046792685985565\n",
      "Subject 14, Epoch 391, Loss: 0.8403749465942383, Final Batch Loss: 0.21483953297138214\n",
      "Subject 14, Epoch 392, Loss: 0.7957980036735535, Final Batch Loss: 0.1504567414522171\n",
      "Subject 14, Epoch 393, Loss: 0.7912309616804123, Final Batch Loss: 0.11496677994728088\n",
      "Subject 14, Epoch 394, Loss: 0.87411168217659, Final Batch Loss: 0.24532769620418549\n",
      "Subject 14, Epoch 395, Loss: 0.7899502962827682, Final Batch Loss: 0.14750592410564423\n",
      "Subject 14, Epoch 396, Loss: 0.8318770825862885, Final Batch Loss: 0.23338474333286285\n",
      "Subject 14, Epoch 397, Loss: 0.8068039119243622, Final Batch Loss: 0.1694132685661316\n",
      "Subject 14, Epoch 398, Loss: 0.7580979615449905, Final Batch Loss: 0.07431663572788239\n",
      "Subject 14, Epoch 399, Loss: 0.8639576286077499, Final Batch Loss: 0.2880139946937561\n",
      "Subject 14, Epoch 400, Loss: 0.7126511931419373, Final Batch Loss: 0.13638941943645477\n",
      "Subject 14, Epoch 401, Loss: 0.7357029095292091, Final Batch Loss: 0.10072264820337296\n",
      "Subject 14, Epoch 402, Loss: 0.7788064479827881, Final Batch Loss: 0.09098227322101593\n",
      "Subject 14, Epoch 403, Loss: 0.9240504801273346, Final Batch Loss: 0.3517966866493225\n",
      "Subject 14, Epoch 404, Loss: 0.7910284176468849, Final Batch Loss: 0.11466870456933975\n",
      "Subject 14, Epoch 405, Loss: 0.7458742558956146, Final Batch Loss: 0.10126981139183044\n",
      "Subject 14, Epoch 406, Loss: 0.9677670300006866, Final Batch Loss: 0.31098997592926025\n",
      "Subject 14, Epoch 407, Loss: 0.9701398611068726, Final Batch Loss: 0.3997780978679657\n",
      "Subject 14, Epoch 408, Loss: 0.7896870672702789, Final Batch Loss: 0.20324935019016266\n",
      "Subject 14, Epoch 409, Loss: 0.7402093335986137, Final Batch Loss: 0.11152719706296921\n",
      "Subject 14, Epoch 410, Loss: 0.8558102697134018, Final Batch Loss: 0.16593113541603088\n",
      "Subject 14, Epoch 411, Loss: 0.669722206890583, Final Batch Loss: 0.1155082955956459\n",
      "Subject 14, Epoch 412, Loss: 0.6606016457080841, Final Batch Loss: 0.08431282639503479\n",
      "Subject 14, Epoch 413, Loss: 0.9394396394491196, Final Batch Loss: 0.25865447521209717\n",
      "Subject 14, Epoch 414, Loss: 0.8622060567140579, Final Batch Loss: 0.2009803056716919\n",
      "Subject 14, Epoch 415, Loss: 0.7577471137046814, Final Batch Loss: 0.20672349631786346\n",
      "Subject 14, Epoch 416, Loss: 0.6735289841890335, Final Batch Loss: 0.12720490992069244\n",
      "Subject 14, Epoch 417, Loss: 0.8054689168930054, Final Batch Loss: 0.18877290189266205\n",
      "Subject 14, Epoch 418, Loss: 0.9059304296970367, Final Batch Loss: 0.31096649169921875\n",
      "Subject 14, Epoch 419, Loss: 0.6552590876817703, Final Batch Loss: 0.12026867270469666\n",
      "Subject 14, Epoch 420, Loss: 0.7951379120349884, Final Batch Loss: 0.25056591629981995\n",
      "Subject 14, Epoch 421, Loss: 0.7960797399282455, Final Batch Loss: 0.2329113483428955\n",
      "Subject 14, Epoch 422, Loss: 0.9398235231637955, Final Batch Loss: 0.28933143615722656\n",
      "Subject 14, Epoch 423, Loss: 0.8508910536766052, Final Batch Loss: 0.16149166226387024\n",
      "Subject 14, Epoch 424, Loss: 0.8109068274497986, Final Batch Loss: 0.15234874188899994\n",
      "Subject 14, Epoch 425, Loss: 0.7698291540145874, Final Batch Loss: 0.20108327269554138\n",
      "Subject 14, Epoch 426, Loss: 0.6747501268982887, Final Batch Loss: 0.0919797345995903\n",
      "Subject 14, Epoch 427, Loss: 0.732998751103878, Final Batch Loss: 0.0682176873087883\n",
      "Subject 14, Epoch 428, Loss: 0.7942705750465393, Final Batch Loss: 0.1775507777929306\n",
      "Subject 14, Epoch 429, Loss: 0.8430290669202805, Final Batch Loss: 0.3073117733001709\n",
      "Subject 14, Epoch 430, Loss: 0.8920304626226425, Final Batch Loss: 0.38420024514198303\n",
      "Subject 14, Epoch 431, Loss: 0.6524419412016869, Final Batch Loss: 0.10078311711549759\n",
      "Subject 14, Epoch 432, Loss: 0.8951896578073502, Final Batch Loss: 0.31528499722480774\n",
      "Subject 14, Epoch 433, Loss: 0.6425929144024849, Final Batch Loss: 0.055898942053318024\n",
      "Subject 14, Epoch 434, Loss: 0.8269479721784592, Final Batch Loss: 0.32724064588546753\n",
      "Subject 14, Epoch 435, Loss: 0.8215662688016891, Final Batch Loss: 0.2647891938686371\n",
      "Subject 14, Epoch 436, Loss: 0.841490313410759, Final Batch Loss: 0.28274667263031006\n",
      "Subject 14, Epoch 437, Loss: 0.8072972074151039, Final Batch Loss: 0.17610961198806763\n",
      "Subject 14, Epoch 438, Loss: 0.6374649703502655, Final Batch Loss: 0.06404990702867508\n",
      "Subject 14, Epoch 439, Loss: 0.8029668033123016, Final Batch Loss: 0.19974368810653687\n",
      "Subject 14, Epoch 440, Loss: 0.6757638305425644, Final Batch Loss: 0.0797397792339325\n",
      "Subject 14, Epoch 441, Loss: 0.7750702947378159, Final Batch Loss: 0.15997037291526794\n",
      "Subject 14, Epoch 442, Loss: 0.7978942543268204, Final Batch Loss: 0.13848376274108887\n",
      "Subject 14, Epoch 443, Loss: 0.768930122256279, Final Batch Loss: 0.22973869740962982\n",
      "Subject 14, Epoch 444, Loss: 0.726214274764061, Final Batch Loss: 0.15320034325122833\n",
      "Subject 14, Epoch 445, Loss: 0.7380962818861008, Final Batch Loss: 0.14623354375362396\n",
      "Subject 14, Epoch 446, Loss: 0.8032948672771454, Final Batch Loss: 0.17361164093017578\n",
      "Subject 14, Epoch 447, Loss: 0.6447175517678261, Final Batch Loss: 0.11504658311605453\n",
      "Subject 14, Epoch 448, Loss: 0.8015123307704926, Final Batch Loss: 0.2686024010181427\n",
      "Subject 14, Epoch 449, Loss: 0.7846624553203583, Final Batch Loss: 0.17473413050174713\n",
      "Subject 14, Epoch 450, Loss: 0.6167627945542336, Final Batch Loss: 0.11800210922956467\n",
      "Subject 14, Epoch 451, Loss: 0.9723309725522995, Final Batch Loss: 0.3155941069126129\n",
      "Subject 14, Epoch 452, Loss: 0.7783353328704834, Final Batch Loss: 0.18081369996070862\n",
      "Subject 14, Epoch 453, Loss: 0.8203864991664886, Final Batch Loss: 0.2531925439834595\n",
      "Subject 14, Epoch 454, Loss: 0.797526553273201, Final Batch Loss: 0.2988594174385071\n",
      "Subject 14, Epoch 455, Loss: 0.6870996356010437, Final Batch Loss: 0.12167549133300781\n",
      "Subject 14, Epoch 456, Loss: 0.6251226514577866, Final Batch Loss: 0.0790022760629654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 457, Loss: 0.7010405883193016, Final Batch Loss: 0.10559766739606857\n",
      "Subject 14, Epoch 458, Loss: 0.6972838118672371, Final Batch Loss: 0.11317161470651627\n",
      "Subject 14, Epoch 459, Loss: 0.8404122889041901, Final Batch Loss: 0.24969331920146942\n",
      "Subject 14, Epoch 460, Loss: 0.7766512483358383, Final Batch Loss: 0.2114093005657196\n",
      "Subject 14, Epoch 461, Loss: 0.7578925043344498, Final Batch Loss: 0.1632997989654541\n",
      "Subject 14, Epoch 462, Loss: 0.6404502391815186, Final Batch Loss: 0.14056254923343658\n",
      "Subject 14, Epoch 463, Loss: 1.1190651804208755, Final Batch Loss: 0.5143510103225708\n",
      "Subject 14, Epoch 464, Loss: 0.7331685721874237, Final Batch Loss: 0.13873152434825897\n",
      "Subject 14, Epoch 465, Loss: 0.6163098961114883, Final Batch Loss: 0.11027325689792633\n",
      "Subject 14, Epoch 466, Loss: 0.5819828184321523, Final Batch Loss: 0.012367081828415394\n",
      "Subject 14, Epoch 467, Loss: 0.9226628914475441, Final Batch Loss: 0.4479559361934662\n",
      "Subject 14, Epoch 468, Loss: 0.7749137133359909, Final Batch Loss: 0.13436484336853027\n",
      "Subject 14, Epoch 469, Loss: 0.8425373435020447, Final Batch Loss: 0.24175462126731873\n",
      "Subject 14, Epoch 470, Loss: 0.7977356165647507, Final Batch Loss: 0.26829469203948975\n",
      "Subject 14, Epoch 471, Loss: 1.0027441829442978, Final Batch Loss: 0.17946338653564453\n",
      "Subject 14, Epoch 472, Loss: 0.6705068051815033, Final Batch Loss: 0.095461905002594\n",
      "Subject 14, Epoch 473, Loss: 0.655824676156044, Final Batch Loss: 0.14245709776878357\n",
      "Subject 14, Epoch 474, Loss: 0.6243064850568771, Final Batch Loss: 0.1672249436378479\n",
      "Subject 14, Epoch 475, Loss: 0.8775881230831146, Final Batch Loss: 0.2262517809867859\n",
      "Subject 14, Epoch 476, Loss: 0.7934125065803528, Final Batch Loss: 0.15684032440185547\n",
      "Subject 14, Epoch 477, Loss: 0.760616347193718, Final Batch Loss: 0.16050349175930023\n",
      "Subject 14, Epoch 478, Loss: 0.6391848921775818, Final Batch Loss: 0.06751403212547302\n",
      "Subject 14, Epoch 479, Loss: 0.8080891221761703, Final Batch Loss: 0.22521957755088806\n",
      "Subject 14, Epoch 480, Loss: 0.6560798734426498, Final Batch Loss: 0.20694437623023987\n",
      "Subject 14, Epoch 481, Loss: 0.6149249821901321, Final Batch Loss: 0.08024322986602783\n",
      "Subject 14, Epoch 482, Loss: 0.6971137523651123, Final Batch Loss: 0.12517648935317993\n",
      "Subject 14, Epoch 483, Loss: 0.6829884648323059, Final Batch Loss: 0.22205157577991486\n",
      "Subject 14, Epoch 484, Loss: 0.7343578785657883, Final Batch Loss: 0.26190537214279175\n",
      "Subject 14, Epoch 485, Loss: 0.8204059898853302, Final Batch Loss: 0.12267504632472992\n",
      "Subject 14, Epoch 486, Loss: 0.8065028637647629, Final Batch Loss: 0.18263064324855804\n",
      "Subject 14, Epoch 487, Loss: 0.7604436352849007, Final Batch Loss: 0.24525180459022522\n",
      "Subject 14, Epoch 488, Loss: 0.7351030707359314, Final Batch Loss: 0.19726917147636414\n",
      "Subject 14, Epoch 489, Loss: 0.7104042321443558, Final Batch Loss: 0.1928984522819519\n",
      "Subject 14, Epoch 490, Loss: 0.8681454807519913, Final Batch Loss: 0.24381674826145172\n",
      "Subject 14, Epoch 491, Loss: 0.8658287674188614, Final Batch Loss: 0.31910568475723267\n",
      "Subject 14, Epoch 492, Loss: 0.7282149940729141, Final Batch Loss: 0.1684754192829132\n",
      "Subject 14, Epoch 493, Loss: 0.7561877965927124, Final Batch Loss: 0.2553289234638214\n",
      "Subject 14, Epoch 494, Loss: 0.7710889577865601, Final Batch Loss: 0.1977986991405487\n",
      "Subject 14, Epoch 495, Loss: 0.5812982469797134, Final Batch Loss: 0.11183933168649673\n",
      "Subject 14, Epoch 496, Loss: 0.770814448595047, Final Batch Loss: 0.14382793009281158\n",
      "Subject 14, Epoch 497, Loss: 0.6582327634096146, Final Batch Loss: 0.08883191645145416\n",
      "Subject 14, Epoch 498, Loss: 0.6543572396039963, Final Batch Loss: 0.1630130261182785\n",
      "Subject 14, Epoch 499, Loss: 0.6541643887758255, Final Batch Loss: 0.19663497805595398\n",
      "Subject 14, Epoch 500, Loss: 0.6304948925971985, Final Batch Loss: 0.15320011973381042\n",
      "Subject 14, Epoch 501, Loss: 0.626221552491188, Final Batch Loss: 0.1396787315607071\n",
      "Subject 14, Epoch 502, Loss: 0.6883334964513779, Final Batch Loss: 0.16812430322170258\n",
      "Subject 14, Epoch 503, Loss: 0.541035033762455, Final Batch Loss: 0.09629622846841812\n",
      "Subject 14, Epoch 504, Loss: 0.6233839318156242, Final Batch Loss: 0.11877256631851196\n",
      "Subject 14, Epoch 505, Loss: 0.5776442214846611, Final Batch Loss: 0.07597080618143082\n",
      "Subject 14, Epoch 506, Loss: 0.7287352532148361, Final Batch Loss: 0.1690235137939453\n",
      "Subject 14, Epoch 507, Loss: 0.6026715412735939, Final Batch Loss: 0.11491034179925919\n",
      "Subject 14, Epoch 508, Loss: 0.8353504985570908, Final Batch Loss: 0.2749776542186737\n",
      "Subject 14, Epoch 509, Loss: 0.6848946362733841, Final Batch Loss: 0.1825624406337738\n",
      "Subject 14, Epoch 510, Loss: 0.555806890130043, Final Batch Loss: 0.10809315741062164\n",
      "Subject 14, Epoch 511, Loss: 0.9079939723014832, Final Batch Loss: 0.3739096224308014\n",
      "Subject 14, Epoch 512, Loss: 0.8481821417808533, Final Batch Loss: 0.28247955441474915\n",
      "Subject 14, Epoch 513, Loss: 0.7546323239803314, Final Batch Loss: 0.1715431809425354\n",
      "Subject 14, Epoch 514, Loss: 1.0760187655687332, Final Batch Loss: 0.5019794702529907\n",
      "Subject 14, Epoch 515, Loss: 0.7391104102134705, Final Batch Loss: 0.16154401004314423\n",
      "Subject 14, Epoch 516, Loss: 0.847818911075592, Final Batch Loss: 0.25001659989356995\n",
      "Subject 14, Epoch 517, Loss: 0.6976006552577019, Final Batch Loss: 0.06677883118391037\n",
      "Subject 14, Epoch 518, Loss: 0.7306995242834091, Final Batch Loss: 0.20246712863445282\n",
      "Subject 14, Epoch 519, Loss: 0.7084817662835121, Final Batch Loss: 0.12417630106210709\n",
      "Subject 14, Epoch 520, Loss: 0.5631640404462814, Final Batch Loss: 0.04388001561164856\n",
      "Subject 14, Epoch 521, Loss: 0.6528868079185486, Final Batch Loss: 0.12646475434303284\n",
      "Subject 14, Epoch 522, Loss: 0.64206762611866, Final Batch Loss: 0.14976370334625244\n",
      "Subject 14, Epoch 523, Loss: 0.7600524351000786, Final Batch Loss: 0.24052323400974274\n",
      "Subject 14, Epoch 524, Loss: 0.5700709372758865, Final Batch Loss: 0.09797290712594986\n",
      "Subject 14, Epoch 525, Loss: 0.5485427305102348, Final Batch Loss: 0.022479258477687836\n",
      "Subject 14, Epoch 526, Loss: 0.7035625725984573, Final Batch Loss: 0.27718961238861084\n",
      "Subject 14, Epoch 527, Loss: 0.751052051782608, Final Batch Loss: 0.2824633717536926\n",
      "Subject 14, Epoch 528, Loss: 0.6916137784719467, Final Batch Loss: 0.18701045215129852\n",
      "Subject 14, Epoch 529, Loss: 0.6577874049544334, Final Batch Loss: 0.09878184646368027\n",
      "Subject 14, Epoch 530, Loss: 0.5350134335458279, Final Batch Loss: 0.057729002088308334\n",
      "Subject 14, Epoch 531, Loss: 0.6878287196159363, Final Batch Loss: 0.1374538242816925\n",
      "Subject 14, Epoch 532, Loss: 0.589261069893837, Final Batch Loss: 0.129784494638443\n",
      "Subject 14, Epoch 533, Loss: 0.6775428503751755, Final Batch Loss: 0.21216972172260284\n",
      "Subject 14, Epoch 534, Loss: 0.630144476890564, Final Batch Loss: 0.14613084495067596\n",
      "Subject 14, Epoch 535, Loss: 0.6435692235827446, Final Batch Loss: 0.16179490089416504\n",
      "Subject 14, Epoch 536, Loss: 0.7007793635129929, Final Batch Loss: 0.23493976891040802\n",
      "Subject 14, Epoch 537, Loss: 0.5955437272787094, Final Batch Loss: 0.058376386761665344\n",
      "Subject 14, Epoch 538, Loss: 0.6531308740377426, Final Batch Loss: 0.1486336588859558\n",
      "Subject 14, Epoch 539, Loss: 0.5481690466403961, Final Batch Loss: 0.11849544197320938\n",
      "Subject 14, Epoch 540, Loss: 0.5041568428277969, Final Batch Loss: 0.1103864535689354\n",
      "Subject 14, Epoch 541, Loss: 0.6494541764259338, Final Batch Loss: 0.1538437008857727\n",
      "Subject 14, Epoch 542, Loss: 0.6603706479072571, Final Batch Loss: 0.1064411848783493\n",
      "Subject 14, Epoch 543, Loss: 0.7617756277322769, Final Batch Loss: 0.19958366453647614\n",
      "Subject 14, Epoch 544, Loss: 0.6160463839769363, Final Batch Loss: 0.07496345043182373\n",
      "Subject 14, Epoch 545, Loss: 0.6623412370681763, Final Batch Loss: 0.09623381495475769\n",
      "Subject 14, Epoch 546, Loss: 0.655241534113884, Final Batch Loss: 0.17499575018882751\n",
      "Subject 14, Epoch 547, Loss: 0.6570282801985741, Final Batch Loss: 0.18174034357070923\n",
      "Subject 14, Epoch 548, Loss: 0.5883860364556313, Final Batch Loss: 0.0710979476571083\n",
      "Subject 14, Epoch 549, Loss: 0.7242975160479546, Final Batch Loss: 0.1449735015630722\n",
      "Subject 14, Epoch 550, Loss: 0.610185019671917, Final Batch Loss: 0.1993526816368103\n",
      "Subject 14, Epoch 551, Loss: 0.7381097450852394, Final Batch Loss: 0.324575811624527\n",
      "Subject 14, Epoch 552, Loss: 0.6219647228717804, Final Batch Loss: 0.14796368777751923\n",
      "Subject 14, Epoch 553, Loss: 0.6214390099048615, Final Batch Loss: 0.20822836458683014\n",
      "Subject 14, Epoch 554, Loss: 0.591089554131031, Final Batch Loss: 0.12291353195905685\n",
      "Subject 14, Epoch 555, Loss: 0.6489111632108688, Final Batch Loss: 0.17505665123462677\n",
      "Subject 14, Epoch 556, Loss: 0.5853303372859955, Final Batch Loss: 0.16196385025978088\n",
      "Subject 14, Epoch 557, Loss: 0.8775895833969116, Final Batch Loss: 0.37242454290390015\n",
      "Subject 14, Epoch 558, Loss: 0.6396545469760895, Final Batch Loss: 0.16549713909626007\n",
      "Subject 14, Epoch 559, Loss: 0.8807567358016968, Final Batch Loss: 0.4168972074985504\n",
      "Subject 14, Epoch 560, Loss: 0.625149667263031, Final Batch Loss: 0.20011159777641296\n",
      "Subject 14, Epoch 561, Loss: 0.625573143362999, Final Batch Loss: 0.03244364261627197\n",
      "Subject 14, Epoch 562, Loss: 0.6692355126142502, Final Batch Loss: 0.19435928761959076\n",
      "Subject 14, Epoch 563, Loss: 0.8971251845359802, Final Batch Loss: 0.4170822501182556\n",
      "Subject 14, Epoch 564, Loss: 0.6353287547826767, Final Batch Loss: 0.11844374239444733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 565, Loss: 0.6459774076938629, Final Batch Loss: 0.1264200657606125\n",
      "Subject 14, Epoch 566, Loss: 0.5919987931847572, Final Batch Loss: 0.11191995441913605\n",
      "Subject 14, Epoch 567, Loss: 0.5920964032411575, Final Batch Loss: 0.17410245537757874\n",
      "Subject 14, Epoch 568, Loss: 0.49670446291565895, Final Batch Loss: 0.05262274667620659\n",
      "Subject 14, Epoch 569, Loss: 0.5290298387408257, Final Batch Loss: 0.14366361498832703\n",
      "Subject 14, Epoch 570, Loss: 0.5763860642910004, Final Batch Loss: 0.07774196565151215\n",
      "Subject 14, Epoch 571, Loss: 0.641961857676506, Final Batch Loss: 0.1572594940662384\n",
      "Subject 14, Epoch 572, Loss: 0.585383877158165, Final Batch Loss: 0.1581820398569107\n",
      "Subject 14, Epoch 573, Loss: 0.5279360190033913, Final Batch Loss: 0.1585949808359146\n",
      "Subject 14, Epoch 574, Loss: 0.7666964381933212, Final Batch Loss: 0.37373819947242737\n",
      "Subject 14, Epoch 575, Loss: 0.705469936132431, Final Batch Loss: 0.2154788225889206\n",
      "Subject 14, Epoch 576, Loss: 0.6842567324638367, Final Batch Loss: 0.15545520186424255\n",
      "Subject 14, Epoch 577, Loss: 0.6168772280216217, Final Batch Loss: 0.182803213596344\n",
      "Subject 14, Epoch 578, Loss: 0.5775199607014656, Final Batch Loss: 0.10507851094007492\n",
      "Subject 14, Epoch 579, Loss: 0.690842017531395, Final Batch Loss: 0.13402634859085083\n",
      "Subject 14, Epoch 580, Loss: 0.5910714566707611, Final Batch Loss: 0.14406807720661163\n",
      "Subject 14, Epoch 581, Loss: 0.5974922031164169, Final Batch Loss: 0.08351454138755798\n",
      "Subject 14, Epoch 582, Loss: 0.5916234850883484, Final Batch Loss: 0.16021671891212463\n",
      "Subject 14, Epoch 583, Loss: 0.8624559491872787, Final Batch Loss: 0.3701820373535156\n",
      "Subject 14, Epoch 584, Loss: 0.5626329183578491, Final Batch Loss: 0.13733673095703125\n",
      "Subject 14, Epoch 585, Loss: 0.6292693838477135, Final Batch Loss: 0.16152165830135345\n",
      "Subject 14, Epoch 586, Loss: 0.6686217784881592, Final Batch Loss: 0.20568528771400452\n",
      "Subject 14, Epoch 587, Loss: 0.5712573230266571, Final Batch Loss: 0.13808417320251465\n",
      "Subject 14, Epoch 588, Loss: 0.5076370686292648, Final Batch Loss: 0.06130041927099228\n",
      "Subject 14, Epoch 589, Loss: 0.513410672545433, Final Batch Loss: 0.07640965282917023\n",
      "Subject 14, Epoch 590, Loss: 0.812595508992672, Final Batch Loss: 0.3425481915473938\n",
      "Subject 14, Epoch 591, Loss: 0.5637249425053596, Final Batch Loss: 0.08531411737203598\n",
      "Subject 14, Epoch 592, Loss: 0.6273966506123543, Final Batch Loss: 0.1329575628042221\n",
      "Subject 14, Epoch 593, Loss: 0.560729119926691, Final Batch Loss: 0.057143520563840866\n",
      "Subject 14, Epoch 594, Loss: 0.6403018608689308, Final Batch Loss: 0.16016264259815216\n",
      "Subject 14, Epoch 595, Loss: 0.5770606771111488, Final Batch Loss: 0.13539966940879822\n",
      "Subject 14, Epoch 596, Loss: 0.6435849517583847, Final Batch Loss: 0.20648019015789032\n",
      "Subject 14, Epoch 597, Loss: 0.7787910550832748, Final Batch Loss: 0.31807461380958557\n",
      "Subject 14, Epoch 598, Loss: 0.6407944560050964, Final Batch Loss: 0.13154523074626923\n",
      "Subject 14, Epoch 599, Loss: 0.497705802321434, Final Batch Loss: 0.07112834602594376\n",
      "Subject 14, Epoch 600, Loss: 0.6939316093921661, Final Batch Loss: 0.2149863988161087\n",
      "Subject 14, Epoch 601, Loss: 0.649311363697052, Final Batch Loss: 0.2278209924697876\n",
      "Subject 14, Epoch 602, Loss: 0.7549844831228256, Final Batch Loss: 0.3102116286754608\n",
      "Subject 14, Epoch 603, Loss: 0.570049874484539, Final Batch Loss: 0.13901223242282867\n",
      "Subject 14, Epoch 604, Loss: 0.7670655325055122, Final Batch Loss: 0.22206035256385803\n",
      "Subject 14, Epoch 605, Loss: 0.7777777761220932, Final Batch Loss: 0.33817341923713684\n",
      "Subject 14, Epoch 606, Loss: 0.5085267499089241, Final Batch Loss: 0.05690183490514755\n",
      "Subject 14, Epoch 607, Loss: 0.8596475124359131, Final Batch Loss: 0.31556594371795654\n",
      "Subject 14, Epoch 608, Loss: 0.7159248888492584, Final Batch Loss: 0.2879772186279297\n",
      "Subject 14, Epoch 609, Loss: 0.8526889234781265, Final Batch Loss: 0.4118674695491791\n",
      "Subject 14, Epoch 610, Loss: 0.8049406707286835, Final Batch Loss: 0.34395983815193176\n",
      "Subject 14, Epoch 611, Loss: 0.8063308447599411, Final Batch Loss: 0.3044300973415375\n",
      "Subject 14, Epoch 612, Loss: 0.5682404562830925, Final Batch Loss: 0.20501643419265747\n",
      "Subject 14, Epoch 613, Loss: 0.5415480062365532, Final Batch Loss: 0.15734238922595978\n",
      "Subject 14, Epoch 614, Loss: 0.5430013611912727, Final Batch Loss: 0.08962540328502655\n",
      "Subject 14, Epoch 615, Loss: 0.6066288724541664, Final Batch Loss: 0.1739823967218399\n",
      "Subject 14, Epoch 616, Loss: 0.5347627550363541, Final Batch Loss: 0.07936356961727142\n",
      "Subject 14, Epoch 617, Loss: 0.6581610962748528, Final Batch Loss: 0.22276316583156586\n",
      "Subject 14, Epoch 618, Loss: 0.5105650722980499, Final Batch Loss: 0.06431270390748978\n",
      "Subject 14, Epoch 619, Loss: 0.5669020041823387, Final Batch Loss: 0.17359736561775208\n",
      "Subject 14, Epoch 620, Loss: 0.5356058776378632, Final Batch Loss: 0.04862953722476959\n",
      "Subject 14, Epoch 621, Loss: 0.6408572793006897, Final Batch Loss: 0.21976785361766815\n",
      "Subject 14, Epoch 622, Loss: 0.8120282739400864, Final Batch Loss: 0.4182286560535431\n",
      "Subject 14, Epoch 623, Loss: 0.5870937183499336, Final Batch Loss: 0.16326913237571716\n",
      "Subject 14, Epoch 624, Loss: 0.5714997947216034, Final Batch Loss: 0.10249859094619751\n",
      "Subject 14, Epoch 625, Loss: 0.6058659106492996, Final Batch Loss: 0.09487569332122803\n",
      "Subject 14, Epoch 626, Loss: 0.871666744351387, Final Batch Loss: 0.3675660789012909\n",
      "Subject 14, Epoch 627, Loss: 0.5262732207775116, Final Batch Loss: 0.03897067904472351\n",
      "Subject 14, Epoch 628, Loss: 0.5468117222189903, Final Batch Loss: 0.0905727669596672\n",
      "Subject 14, Epoch 629, Loss: 0.666338711977005, Final Batch Loss: 0.19413648545742035\n",
      "Subject 14, Epoch 630, Loss: 0.6945172548294067, Final Batch Loss: 0.25251564383506775\n",
      "Subject 14, Epoch 631, Loss: 0.4096740260720253, Final Batch Loss: 0.05049072951078415\n",
      "Subject 14, Epoch 632, Loss: 0.6784588694572449, Final Batch Loss: 0.12114813923835754\n",
      "Subject 14, Epoch 633, Loss: 0.4762304499745369, Final Batch Loss: 0.06237218528985977\n",
      "Subject 14, Epoch 634, Loss: 0.5697584748268127, Final Batch Loss: 0.21304112672805786\n",
      "Subject 14, Epoch 635, Loss: 0.45245300978422165, Final Batch Loss: 0.09871820360422134\n",
      "Subject 14, Epoch 636, Loss: 0.6177876070141792, Final Batch Loss: 0.2284061163663864\n",
      "Subject 14, Epoch 637, Loss: 0.5000416189432144, Final Batch Loss: 0.140374094247818\n",
      "Subject 14, Epoch 638, Loss: 0.5062393099069595, Final Batch Loss: 0.05360525846481323\n",
      "Subject 14, Epoch 639, Loss: 0.44263940304517746, Final Batch Loss: 0.05268578976392746\n",
      "Subject 14, Epoch 640, Loss: 0.6196132823824883, Final Batch Loss: 0.1807805746793747\n",
      "Subject 14, Epoch 641, Loss: 0.5963704735040665, Final Batch Loss: 0.1534648835659027\n",
      "Subject 14, Epoch 642, Loss: 0.7654729709029198, Final Batch Loss: 0.3998211622238159\n",
      "Subject 14, Epoch 643, Loss: 0.5417853370308876, Final Batch Loss: 0.12104848027229309\n",
      "Subject 14, Epoch 644, Loss: 0.4507712423801422, Final Batch Loss: 0.07143586874008179\n",
      "Subject 14, Epoch 645, Loss: 0.5644180625677109, Final Batch Loss: 0.103254534304142\n",
      "Subject 14, Epoch 646, Loss: 0.46957217156887054, Final Batch Loss: 0.07717905938625336\n",
      "Subject 14, Epoch 647, Loss: 0.5255542695522308, Final Batch Loss: 0.10734941065311432\n",
      "Subject 14, Epoch 648, Loss: 0.4881123900413513, Final Batch Loss: 0.05281198024749756\n",
      "Subject 14, Epoch 649, Loss: 0.6726185902953148, Final Batch Loss: 0.2574324905872345\n",
      "Subject 14, Epoch 650, Loss: 0.4217543490231037, Final Batch Loss: 0.014551680535078049\n",
      "Subject 14, Epoch 651, Loss: 0.5882880464196205, Final Batch Loss: 0.0776657983660698\n",
      "Subject 14, Epoch 652, Loss: 0.6227832138538361, Final Batch Loss: 0.14349952340126038\n",
      "Subject 14, Epoch 653, Loss: 0.4507701247930527, Final Batch Loss: 0.059886932373046875\n",
      "Subject 14, Epoch 654, Loss: 0.4106219783425331, Final Batch Loss: 0.050676487386226654\n",
      "Subject 14, Epoch 655, Loss: 0.7359425574541092, Final Batch Loss: 0.29613006114959717\n",
      "Subject 14, Epoch 656, Loss: 0.57565538585186, Final Batch Loss: 0.14718466997146606\n",
      "Subject 14, Epoch 657, Loss: 0.6952841803431511, Final Batch Loss: 0.33517658710479736\n",
      "Subject 14, Epoch 658, Loss: 0.5958136767148972, Final Batch Loss: 0.15012231469154358\n",
      "Subject 14, Epoch 659, Loss: 0.6093281954526901, Final Batch Loss: 0.27269163727760315\n",
      "Subject 14, Epoch 660, Loss: 0.5112348645925522, Final Batch Loss: 0.08489776402711868\n",
      "Subject 14, Epoch 661, Loss: 0.5556535422801971, Final Batch Loss: 0.1410813331604004\n",
      "Subject 14, Epoch 662, Loss: 0.43461968563497066, Final Batch Loss: 0.00945589505136013\n",
      "Subject 14, Epoch 663, Loss: 0.5857708007097244, Final Batch Loss: 0.17378434538841248\n",
      "Subject 14, Epoch 664, Loss: 0.371667692437768, Final Batch Loss: 0.017010750249028206\n",
      "Subject 14, Epoch 665, Loss: 0.8049810826778412, Final Batch Loss: 0.32014000415802\n",
      "Subject 14, Epoch 666, Loss: 0.48819904029369354, Final Batch Loss: 0.10225296765565872\n",
      "Subject 14, Epoch 667, Loss: 0.46212854236364365, Final Batch Loss: 0.05689486861228943\n",
      "Subject 14, Epoch 668, Loss: 0.45263634622097015, Final Batch Loss: 0.11364032328128815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 669, Loss: 0.5948766991496086, Final Batch Loss: 0.24291573464870453\n",
      "Subject 14, Epoch 670, Loss: 0.4361570440232754, Final Batch Loss: 0.06514834612607956\n",
      "Subject 14, Epoch 671, Loss: 0.5812630504369736, Final Batch Loss: 0.1773100197315216\n",
      "Subject 14, Epoch 672, Loss: 0.4903639703989029, Final Batch Loss: 0.04968966543674469\n",
      "Subject 14, Epoch 673, Loss: 0.5962535664439201, Final Batch Loss: 0.2060663104057312\n",
      "Subject 14, Epoch 674, Loss: 0.599151000380516, Final Batch Loss: 0.1665477156639099\n",
      "Subject 14, Epoch 675, Loss: 0.5432732552289963, Final Batch Loss: 0.0690443143248558\n",
      "Subject 14, Epoch 676, Loss: 0.6471464335918427, Final Batch Loss: 0.24104249477386475\n",
      "Subject 14, Epoch 677, Loss: 0.46931812912225723, Final Batch Loss: 0.10641541332006454\n",
      "Subject 14, Epoch 678, Loss: 0.5552661493420601, Final Batch Loss: 0.09952745586633682\n",
      "Subject 14, Epoch 679, Loss: 0.5848314985632896, Final Batch Loss: 0.12373881042003632\n",
      "Subject 14, Epoch 680, Loss: 0.6558625474572182, Final Batch Loss: 0.21848900616168976\n",
      "Subject 14, Epoch 681, Loss: 0.5341516956686974, Final Batch Loss: 0.20959877967834473\n",
      "Subject 14, Epoch 682, Loss: 0.6556216031312943, Final Batch Loss: 0.2618173062801361\n",
      "Subject 14, Epoch 683, Loss: 0.5351642146706581, Final Batch Loss: 0.14987556636333466\n",
      "Subject 14, Epoch 684, Loss: 0.6756627708673477, Final Batch Loss: 0.20350590348243713\n",
      "Subject 14, Epoch 685, Loss: 0.7077785730361938, Final Batch Loss: 0.1128794252872467\n",
      "Subject 14, Epoch 686, Loss: 0.6603395566344261, Final Batch Loss: 0.2633548080921173\n",
      "Subject 14, Epoch 687, Loss: 0.5061013214290142, Final Batch Loss: 0.03729813173413277\n",
      "Subject 14, Epoch 688, Loss: 0.5533361658453941, Final Batch Loss: 0.09097769111394882\n",
      "Subject 14, Epoch 689, Loss: 0.4193013571202755, Final Batch Loss: 0.0417923703789711\n",
      "Subject 14, Epoch 690, Loss: 0.6051019579172134, Final Batch Loss: 0.22644317150115967\n",
      "Subject 14, Epoch 691, Loss: 0.4842442125082016, Final Batch Loss: 0.1182083934545517\n",
      "Subject 14, Epoch 692, Loss: 0.6326322108507156, Final Batch Loss: 0.2696593701839447\n",
      "Subject 14, Epoch 693, Loss: 0.5389931425452232, Final Batch Loss: 0.15331661701202393\n",
      "Subject 14, Epoch 694, Loss: 0.5606853365898132, Final Batch Loss: 0.09160976856946945\n",
      "Subject 14, Epoch 695, Loss: 0.43961185216903687, Final Batch Loss: 0.06378478556871414\n",
      "Subject 14, Epoch 696, Loss: 0.5235741212964058, Final Batch Loss: 0.20128630101680756\n",
      "Subject 14, Epoch 697, Loss: 0.3784150220453739, Final Batch Loss: 0.05116884782910347\n",
      "Subject 14, Epoch 698, Loss: 0.5683592334389687, Final Batch Loss: 0.1680135726928711\n",
      "Subject 14, Epoch 699, Loss: 0.5973137691617012, Final Batch Loss: 0.17691656947135925\n",
      "Subject 14, Epoch 700, Loss: 0.38991598412394524, Final Batch Loss: 0.058034058660268784\n",
      "Subject 14, Epoch 701, Loss: 0.45980967581272125, Final Batch Loss: 0.11341756582260132\n",
      "Subject 14, Epoch 702, Loss: 0.8495835587382317, Final Batch Loss: 0.36843281984329224\n",
      "Subject 14, Epoch 703, Loss: 0.47449731081724167, Final Batch Loss: 0.14076940715312958\n",
      "Subject 14, Epoch 704, Loss: 0.41833760775625706, Final Batch Loss: 0.019784869626164436\n",
      "Subject 14, Epoch 705, Loss: 0.36947355326265097, Final Batch Loss: 0.009186944924294949\n",
      "Subject 14, Epoch 706, Loss: 0.44922028481960297, Final Batch Loss: 0.10297947376966476\n",
      "Subject 14, Epoch 707, Loss: 0.3962910808622837, Final Batch Loss: 0.06621511280536652\n",
      "Subject 14, Epoch 708, Loss: 0.4464075639843941, Final Batch Loss: 0.13947974145412445\n",
      "Subject 14, Epoch 709, Loss: 0.44445398449897766, Final Batch Loss: 0.06913938373327255\n",
      "Subject 14, Epoch 710, Loss: 0.8264565840363503, Final Batch Loss: 0.4673067033290863\n",
      "Subject 14, Epoch 711, Loss: 0.45342979580163956, Final Batch Loss: 0.12754571437835693\n",
      "Subject 14, Epoch 712, Loss: 0.4660431519150734, Final Batch Loss: 0.03732839971780777\n",
      "Subject 14, Epoch 713, Loss: 0.5981757417321205, Final Batch Loss: 0.13028989732265472\n",
      "Subject 14, Epoch 714, Loss: 0.49261967837810516, Final Batch Loss: 0.0959085002541542\n",
      "Subject 14, Epoch 715, Loss: 0.5823553949594498, Final Batch Loss: 0.26224780082702637\n",
      "Subject 14, Epoch 716, Loss: 0.4399694502353668, Final Batch Loss: 0.13880950212478638\n",
      "Subject 14, Epoch 717, Loss: 0.585057944059372, Final Batch Loss: 0.15710747241973877\n",
      "Subject 14, Epoch 718, Loss: 0.43100010231137276, Final Batch Loss: 0.02257085219025612\n",
      "Subject 14, Epoch 719, Loss: 0.583497054874897, Final Batch Loss: 0.21451115608215332\n",
      "Subject 14, Epoch 720, Loss: 0.48173124343156815, Final Batch Loss: 0.114194855093956\n",
      "Subject 14, Epoch 721, Loss: 0.37677307799458504, Final Batch Loss: 0.04138629510998726\n",
      "Subject 14, Epoch 722, Loss: 0.5158220380544662, Final Batch Loss: 0.08972617238759995\n",
      "Subject 14, Epoch 723, Loss: 0.5272451564669609, Final Batch Loss: 0.17143693566322327\n",
      "Subject 14, Epoch 724, Loss: 0.44961603730916977, Final Batch Loss: 0.035753294825553894\n",
      "Subject 14, Epoch 725, Loss: 0.48615792393684387, Final Batch Loss: 0.0831768736243248\n",
      "Subject 14, Epoch 726, Loss: 0.5005947165191174, Final Batch Loss: 0.059785086661577225\n",
      "Subject 14, Epoch 727, Loss: 0.5710264891386032, Final Batch Loss: 0.18389293551445007\n",
      "Subject 14, Epoch 728, Loss: 0.5072757676243782, Final Batch Loss: 0.0829167366027832\n",
      "Subject 14, Epoch 729, Loss: 0.6598285958170891, Final Batch Loss: 0.28453224897384644\n",
      "Subject 14, Epoch 730, Loss: 0.5326704233884811, Final Batch Loss: 0.061991795897483826\n",
      "Subject 14, Epoch 731, Loss: 0.33346837013959885, Final Batch Loss: 0.05679168552160263\n",
      "Subject 14, Epoch 732, Loss: 0.6310534738004208, Final Batch Loss: 0.3725029230117798\n",
      "Subject 14, Epoch 733, Loss: 0.35450683906674385, Final Batch Loss: 0.042587678879499435\n",
      "Subject 14, Epoch 734, Loss: 0.47929176688194275, Final Batch Loss: 0.05077223479747772\n",
      "Subject 14, Epoch 735, Loss: 0.352664977312088, Final Batch Loss: 0.018262222409248352\n",
      "Subject 14, Epoch 736, Loss: 0.5783327594399452, Final Batch Loss: 0.24040678143501282\n",
      "Subject 14, Epoch 737, Loss: 0.5720174834132195, Final Batch Loss: 0.1483922302722931\n",
      "Subject 14, Epoch 738, Loss: 0.5159961618483067, Final Batch Loss: 0.3033483624458313\n",
      "Subject 14, Epoch 739, Loss: 0.6263702884316444, Final Batch Loss: 0.16690148413181305\n",
      "Subject 14, Epoch 740, Loss: 0.3924825247377157, Final Batch Loss: 0.012954263016581535\n",
      "Subject 14, Epoch 741, Loss: 0.45961857959628105, Final Batch Loss: 0.04737867787480354\n",
      "Subject 14, Epoch 742, Loss: 0.36772121489048004, Final Batch Loss: 0.04036732763051987\n",
      "Subject 14, Epoch 743, Loss: 0.4067203849554062, Final Batch Loss: 0.05452785640954971\n",
      "Subject 14, Epoch 744, Loss: 0.45667754113674164, Final Batch Loss: 0.18262052536010742\n",
      "Subject 14, Epoch 745, Loss: 0.42426564544439316, Final Batch Loss: 0.12767183780670166\n",
      "Subject 14, Epoch 746, Loss: 0.4017948508262634, Final Batch Loss: 0.09468330442905426\n",
      "Subject 14, Epoch 747, Loss: 0.6456261575222015, Final Batch Loss: 0.22497206926345825\n",
      "Subject 14, Epoch 748, Loss: 0.29665083810687065, Final Batch Loss: 0.03925386816263199\n",
      "Subject 14, Epoch 749, Loss: 0.3662421964108944, Final Batch Loss: 0.012465659528970718\n",
      "Subject 14, Epoch 750, Loss: 0.690930150449276, Final Batch Loss: 0.19559049606323242\n",
      "Subject 14, Epoch 751, Loss: 0.57870864123106, Final Batch Loss: 0.21880628168582916\n",
      "Subject 14, Epoch 752, Loss: 0.5189245119690895, Final Batch Loss: 0.08378610014915466\n",
      "Subject 14, Epoch 753, Loss: 0.4524654597043991, Final Batch Loss: 0.027303777635097504\n",
      "Subject 14, Epoch 754, Loss: 0.5327342785894871, Final Batch Loss: 0.21213433146476746\n",
      "Subject 14, Epoch 755, Loss: 0.4599047601222992, Final Batch Loss: 0.07480687648057938\n",
      "Subject 14, Epoch 756, Loss: 0.5444158390164375, Final Batch Loss: 0.08224311470985413\n",
      "Subject 14, Epoch 757, Loss: 0.40072743594646454, Final Batch Loss: 0.08211298286914825\n",
      "Subject 14, Epoch 758, Loss: 0.39873638935387135, Final Batch Loss: 0.006705122068524361\n",
      "Subject 14, Epoch 759, Loss: 0.41827914863824844, Final Batch Loss: 0.08005886524915695\n",
      "Subject 14, Epoch 760, Loss: 0.3979874402284622, Final Batch Loss: 0.07707678526639938\n",
      "Subject 14, Epoch 761, Loss: 0.35796525329351425, Final Batch Loss: 0.0850800946354866\n",
      "Subject 14, Epoch 762, Loss: 0.3788922429084778, Final Batch Loss: 0.0641980692744255\n",
      "Subject 14, Epoch 763, Loss: 0.4167774096131325, Final Batch Loss: 0.16388311982154846\n",
      "Subject 14, Epoch 764, Loss: 0.40635719522833824, Final Batch Loss: 0.0327640064060688\n",
      "Subject 14, Epoch 765, Loss: 0.45115647464990616, Final Batch Loss: 0.08772211521863937\n",
      "Subject 14, Epoch 766, Loss: 0.6253503188490868, Final Batch Loss: 0.2909771800041199\n",
      "Subject 14, Epoch 767, Loss: 0.5200089365243912, Final Batch Loss: 0.10243847966194153\n",
      "Subject 14, Epoch 768, Loss: 0.5330919250845909, Final Batch Loss: 0.17800389230251312\n",
      "Subject 14, Epoch 769, Loss: 0.440485455095768, Final Batch Loss: 0.03674867004156113\n",
      "Subject 14, Epoch 770, Loss: 0.38163815066218376, Final Batch Loss: 0.06202511861920357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 771, Loss: 0.4056285321712494, Final Batch Loss: 0.09311547130346298\n",
      "Subject 14, Epoch 772, Loss: 0.5739319548010826, Final Batch Loss: 0.20755545794963837\n",
      "Subject 14, Epoch 773, Loss: 0.38315301947295666, Final Batch Loss: 0.01455741934478283\n",
      "Subject 14, Epoch 774, Loss: 0.42824890464544296, Final Batch Loss: 0.08770061284303665\n",
      "Subject 14, Epoch 775, Loss: 0.4598116874694824, Final Batch Loss: 0.13697542250156403\n",
      "Subject 14, Epoch 776, Loss: 0.6873622909188271, Final Batch Loss: 0.3423710763454437\n",
      "Subject 14, Epoch 777, Loss: 0.3980887830257416, Final Batch Loss: 0.07228502631187439\n",
      "Subject 14, Epoch 778, Loss: 0.556767150759697, Final Batch Loss: 0.18343256413936615\n",
      "Subject 14, Epoch 779, Loss: 0.4620843380689621, Final Batch Loss: 0.12075456976890564\n",
      "Subject 14, Epoch 780, Loss: 0.3262457102537155, Final Batch Loss: 0.03993814438581467\n",
      "Subject 14, Epoch 781, Loss: 0.4371362254023552, Final Batch Loss: 0.12202279269695282\n",
      "Subject 14, Epoch 782, Loss: 0.4973929673433304, Final Batch Loss: 0.10608489066362381\n",
      "Subject 14, Epoch 783, Loss: 0.3853590860962868, Final Batch Loss: 0.08298107981681824\n",
      "Subject 14, Epoch 784, Loss: 0.37927320972085, Final Batch Loss: 0.039135534316301346\n",
      "Subject 14, Epoch 785, Loss: 0.3670569583773613, Final Batch Loss: 0.04099196940660477\n",
      "Subject 14, Epoch 786, Loss: 0.45723728090524673, Final Batch Loss: 0.11354043334722519\n",
      "Subject 14, Epoch 787, Loss: 0.39119021221995354, Final Batch Loss: 0.04095270112156868\n",
      "Subject 14, Epoch 788, Loss: 0.43742500245571136, Final Batch Loss: 0.10801951587200165\n",
      "Subject 14, Epoch 789, Loss: 0.34404096007347107, Final Batch Loss: 0.06679694354534149\n",
      "Subject 14, Epoch 790, Loss: 0.3782382160425186, Final Batch Loss: 0.07493679970502853\n",
      "Subject 14, Epoch 791, Loss: 0.4132366180419922, Final Batch Loss: 0.1081865206360817\n",
      "Subject 14, Epoch 792, Loss: 0.38751826994121075, Final Batch Loss: 0.030670790001749992\n",
      "Subject 14, Epoch 793, Loss: 0.5270072892308235, Final Batch Loss: 0.17149809002876282\n",
      "Subject 14, Epoch 794, Loss: 0.40611768513917923, Final Batch Loss: 0.03905872255563736\n",
      "Subject 14, Epoch 795, Loss: 0.4238133653998375, Final Batch Loss: 0.019653651863336563\n",
      "Subject 14, Epoch 796, Loss: 0.3106674961745739, Final Batch Loss: 0.054480791091918945\n",
      "Subject 14, Epoch 797, Loss: 0.3534918576478958, Final Batch Loss: 0.03241245448589325\n",
      "Subject 14, Epoch 798, Loss: 0.37243907526135445, Final Batch Loss: 0.10742956399917603\n",
      "Subject 14, Epoch 799, Loss: 0.3946378566324711, Final Batch Loss: 0.0386439748108387\n",
      "Subject 14, Epoch 800, Loss: 0.41995398327708244, Final Batch Loss: 0.05764919891953468\n",
      "Subject 14, Epoch 801, Loss: 0.3003423735499382, Final Batch Loss: 0.02300180494785309\n",
      "Subject 14, Epoch 802, Loss: 0.41954411938786507, Final Batch Loss: 0.15107403695583344\n",
      "Subject 14, Epoch 803, Loss: 0.34815390780568123, Final Batch Loss: 0.010086793452501297\n",
      "Subject 14, Epoch 804, Loss: 0.33554334938526154, Final Batch Loss: 0.07248897105455399\n",
      "Subject 14, Epoch 805, Loss: 0.4018280804157257, Final Batch Loss: 0.0185181125998497\n",
      "Subject 14, Epoch 806, Loss: 0.6157086864113808, Final Batch Loss: 0.14045405387878418\n",
      "Subject 14, Epoch 807, Loss: 0.29116398096084595, Final Batch Loss: 0.02934284508228302\n",
      "Subject 14, Epoch 808, Loss: 0.4727242738008499, Final Batch Loss: 0.16701817512512207\n",
      "Subject 14, Epoch 809, Loss: 0.45186222344636917, Final Batch Loss: 0.03427640348672867\n",
      "Subject 14, Epoch 810, Loss: 0.30041928216814995, Final Batch Loss: 0.04710641875863075\n",
      "Subject 14, Epoch 811, Loss: 0.3093263581395149, Final Batch Loss: 0.029050052165985107\n",
      "Subject 14, Epoch 812, Loss: 0.27485622465610504, Final Batch Loss: 0.040352724492549896\n",
      "Subject 14, Epoch 813, Loss: 0.5292536243796349, Final Batch Loss: 0.12757425010204315\n",
      "Subject 14, Epoch 814, Loss: 0.441387303173542, Final Batch Loss: 0.14321428537368774\n",
      "Subject 14, Epoch 815, Loss: 0.5530950948596001, Final Batch Loss: 0.22629714012145996\n",
      "Subject 14, Epoch 816, Loss: 0.29765670746564865, Final Batch Loss: 0.04350724071264267\n",
      "Subject 14, Epoch 817, Loss: 0.5924081355333328, Final Batch Loss: 0.2522699534893036\n",
      "Subject 14, Epoch 818, Loss: 0.3313673548400402, Final Batch Loss: 0.04285232350230217\n",
      "Subject 14, Epoch 819, Loss: 0.4190659560263157, Final Batch Loss: 0.01916215941309929\n",
      "Subject 14, Epoch 820, Loss: 0.5778575092554092, Final Batch Loss: 0.25425222516059875\n",
      "Subject 14, Epoch 821, Loss: 0.3642483651638031, Final Batch Loss: 0.04174304008483887\n",
      "Subject 14, Epoch 822, Loss: 0.46893052756786346, Final Batch Loss: 0.05935531109571457\n",
      "Subject 14, Epoch 823, Loss: 0.9219934195280075, Final Batch Loss: 0.36138907074928284\n",
      "Subject 14, Epoch 824, Loss: 0.42933087795972824, Final Batch Loss: 0.11695259809494019\n",
      "Subject 14, Epoch 825, Loss: 0.5435365736484528, Final Batch Loss: 0.17569102346897125\n",
      "Subject 14, Epoch 826, Loss: 0.3884444572031498, Final Batch Loss: 0.04591452702879906\n",
      "Subject 14, Epoch 827, Loss: 0.34370047599077225, Final Batch Loss: 0.02152494341135025\n",
      "Subject 14, Epoch 828, Loss: 0.350012943148613, Final Batch Loss: 0.022483855485916138\n",
      "Subject 14, Epoch 829, Loss: 0.6223903894424438, Final Batch Loss: 0.20286442339420319\n",
      "Subject 14, Epoch 830, Loss: 0.3305533118546009, Final Batch Loss: 0.054784145206213\n",
      "Subject 14, Epoch 831, Loss: 0.3296856451779604, Final Batch Loss: 0.017447778955101967\n",
      "Subject 14, Epoch 832, Loss: 0.32557566836476326, Final Batch Loss: 0.05725704878568649\n",
      "Subject 14, Epoch 833, Loss: 0.5695613101124763, Final Batch Loss: 0.253138929605484\n",
      "Subject 14, Epoch 834, Loss: 0.2822282500565052, Final Batch Loss: 0.03656122088432312\n",
      "Subject 14, Epoch 835, Loss: 0.2970188148319721, Final Batch Loss: 0.030341360718011856\n",
      "Subject 14, Epoch 836, Loss: 0.7469408735632896, Final Batch Loss: 0.4200054109096527\n",
      "Subject 14, Epoch 837, Loss: 0.3367623705416918, Final Batch Loss: 0.023342566564679146\n",
      "Subject 14, Epoch 838, Loss: 0.36934794485569, Final Batch Loss: 0.03725368529558182\n",
      "Subject 14, Epoch 839, Loss: 0.3671105057001114, Final Batch Loss: 0.07735185325145721\n",
      "Subject 14, Epoch 840, Loss: 0.34457237645983696, Final Batch Loss: 0.033343929797410965\n",
      "Subject 14, Epoch 841, Loss: 0.3247583247721195, Final Batch Loss: 0.04939782992005348\n",
      "Subject 14, Epoch 842, Loss: 0.4612542390823364, Final Batch Loss: 0.17212532460689545\n",
      "Subject 14, Epoch 843, Loss: 0.27318523824214935, Final Batch Loss: 0.040614403784275055\n",
      "Subject 14, Epoch 844, Loss: 0.3669204115867615, Final Batch Loss: 0.06530936062335968\n",
      "Subject 14, Epoch 845, Loss: 0.3050880879163742, Final Batch Loss: 0.0944901630282402\n",
      "Subject 14, Epoch 846, Loss: 0.7389518618583679, Final Batch Loss: 0.431572824716568\n",
      "Subject 14, Epoch 847, Loss: 0.430924866348505, Final Batch Loss: 0.1062215194106102\n",
      "Subject 14, Epoch 848, Loss: 0.3099406585097313, Final Batch Loss: 0.07307198643684387\n",
      "Subject 14, Epoch 849, Loss: 0.4300754591822624, Final Batch Loss: 0.1516718864440918\n",
      "Subject 14, Epoch 850, Loss: 0.29627692326903343, Final Batch Loss: 0.06059751287102699\n",
      "Subject 14, Epoch 851, Loss: 0.5140387192368507, Final Batch Loss: 0.21078720688819885\n",
      "Subject 14, Epoch 852, Loss: 0.3940501883625984, Final Batch Loss: 0.07969734072685242\n",
      "Subject 14, Epoch 853, Loss: 0.31659840792417526, Final Batch Loss: 0.05473779886960983\n",
      "Subject 14, Epoch 854, Loss: 0.4019225537776947, Final Batch Loss: 0.04643378406763077\n",
      "Subject 14, Epoch 855, Loss: 0.3153605554252863, Final Batch Loss: 0.003668801859021187\n",
      "Subject 14, Epoch 856, Loss: 0.3135827202349901, Final Batch Loss: 0.023345230147242546\n",
      "Subject 14, Epoch 857, Loss: 0.34381940215826035, Final Batch Loss: 0.02196020632982254\n",
      "Subject 14, Epoch 858, Loss: 0.4680720567703247, Final Batch Loss: 0.17530661821365356\n",
      "Subject 14, Epoch 859, Loss: 0.29829953238368034, Final Batch Loss: 0.039291705936193466\n",
      "Subject 14, Epoch 860, Loss: 0.39549142867326736, Final Batch Loss: 0.13883495330810547\n",
      "Subject 14, Epoch 861, Loss: 0.24781625717878342, Final Batch Loss: 0.052122198045253754\n",
      "Subject 14, Epoch 862, Loss: 0.41533488035202026, Final Batch Loss: 0.1415826678276062\n",
      "Subject 14, Epoch 863, Loss: 0.25649834983050823, Final Batch Loss: 0.02334841527044773\n",
      "Subject 14, Epoch 864, Loss: 0.2835116032510996, Final Batch Loss: 0.02286284975707531\n",
      "Subject 14, Epoch 865, Loss: 0.3028374947607517, Final Batch Loss: 0.03893651068210602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 866, Loss: 0.2533043809235096, Final Batch Loss: 0.020509418100118637\n",
      "Subject 14, Epoch 867, Loss: 0.2981192823499441, Final Batch Loss: 0.008042240515351295\n",
      "Subject 14, Epoch 868, Loss: 0.38786594942212105, Final Batch Loss: 0.055327240377664566\n",
      "Subject 14, Epoch 869, Loss: 0.41069935634732246, Final Batch Loss: 0.01122434064745903\n",
      "Subject 14, Epoch 870, Loss: 0.20104809245094657, Final Batch Loss: 0.0032212859950959682\n",
      "Subject 14, Epoch 871, Loss: 0.28031136095523834, Final Batch Loss: 0.0258968323469162\n",
      "Subject 14, Epoch 872, Loss: 0.33321772888302803, Final Batch Loss: 0.10574934631586075\n",
      "Subject 14, Epoch 873, Loss: 0.22765331249684095, Final Batch Loss: 0.01158048864454031\n",
      "Subject 14, Epoch 874, Loss: 0.3371234368532896, Final Batch Loss: 0.030465496703982353\n",
      "Subject 14, Epoch 875, Loss: 0.268792062997818, Final Batch Loss: 0.05549216270446777\n",
      "Subject 14, Epoch 876, Loss: 0.4124846085906029, Final Batch Loss: 0.11354191601276398\n",
      "Subject 14, Epoch 877, Loss: 0.41235483810305595, Final Batch Loss: 0.06099129840731621\n",
      "Subject 14, Epoch 878, Loss: 0.4454214572906494, Final Batch Loss: 0.0854969173669815\n",
      "Subject 14, Epoch 879, Loss: 0.38611403852701187, Final Batch Loss: 0.15600694715976715\n",
      "Subject 14, Epoch 880, Loss: 0.28283216804265976, Final Batch Loss: 0.07077313959598541\n",
      "Subject 14, Epoch 881, Loss: 0.391304649412632, Final Batch Loss: 0.1317998766899109\n",
      "Subject 14, Epoch 882, Loss: 0.48885527998209, Final Batch Loss: 0.23770947754383087\n",
      "Subject 14, Epoch 883, Loss: 0.24476108700037003, Final Batch Loss: 0.012350775301456451\n",
      "Subject 14, Epoch 884, Loss: 0.28322453796863556, Final Batch Loss: 0.07343264669179916\n",
      "Subject 14, Epoch 885, Loss: 0.4349036291241646, Final Batch Loss: 0.14272341132164001\n",
      "Subject 14, Epoch 886, Loss: 0.49047887325286865, Final Batch Loss: 0.21663881838321686\n",
      "Subject 14, Epoch 887, Loss: 0.46395858377218246, Final Batch Loss: 0.15088887512683868\n",
      "Subject 14, Epoch 888, Loss: 0.28978633880615234, Final Batch Loss: 0.07007933408021927\n",
      "Subject 14, Epoch 889, Loss: 0.39371446892619133, Final Batch Loss: 0.15295200049877167\n",
      "Subject 14, Epoch 890, Loss: 0.3609768636524677, Final Batch Loss: 0.05820193514227867\n",
      "Subject 14, Epoch 891, Loss: 0.29262400791049004, Final Batch Loss: 0.033492375165224075\n",
      "Subject 14, Epoch 892, Loss: 0.42020512372255325, Final Batch Loss: 0.1772807389497757\n",
      "Subject 14, Epoch 893, Loss: 0.2689384510740638, Final Batch Loss: 0.011881093494594097\n",
      "Subject 14, Epoch 894, Loss: 0.28542155399918556, Final Batch Loss: 0.07955256849527359\n",
      "Subject 14, Epoch 895, Loss: 0.2885618219152093, Final Batch Loss: 0.009374220855534077\n",
      "Subject 14, Epoch 896, Loss: 0.4171550124883652, Final Batch Loss: 0.05977821350097656\n",
      "Subject 14, Epoch 897, Loss: 0.3092966787517071, Final Batch Loss: 0.02864106371998787\n",
      "Subject 14, Epoch 898, Loss: 0.3415834791958332, Final Batch Loss: 0.03647070750594139\n",
      "Subject 14, Epoch 899, Loss: 0.31444716081023216, Final Batch Loss: 0.02809283509850502\n",
      "Subject 14, Epoch 900, Loss: 0.3233475312590599, Final Batch Loss: 0.05894060432910919\n",
      "Subject 14, Epoch 901, Loss: 0.24709445610642433, Final Batch Loss: 0.04080525040626526\n",
      "Subject 14, Epoch 902, Loss: 0.404670387506485, Final Batch Loss: 0.18135382235050201\n",
      "Subject 14, Epoch 903, Loss: 0.417033351957798, Final Batch Loss: 0.10509122163057327\n",
      "Subject 14, Epoch 904, Loss: 0.1994355171918869, Final Batch Loss: 0.02767670527100563\n",
      "Subject 14, Epoch 905, Loss: 0.19356521358713508, Final Batch Loss: 0.00485358526930213\n",
      "Subject 14, Epoch 906, Loss: 0.2653002645820379, Final Batch Loss: 0.012290207669138908\n",
      "Subject 14, Epoch 907, Loss: 0.27640433609485626, Final Batch Loss: 0.043208856135606766\n",
      "Subject 14, Epoch 908, Loss: 0.2864317446947098, Final Batch Loss: 0.04258027672767639\n",
      "Subject 14, Epoch 909, Loss: 0.3002452850341797, Final Batch Loss: 0.028950367122888565\n",
      "Subject 14, Epoch 910, Loss: 0.19198917597532272, Final Batch Loss: 0.03238074481487274\n",
      "Subject 14, Epoch 911, Loss: 0.3026115093380213, Final Batch Loss: 0.12341532111167908\n",
      "Subject 14, Epoch 912, Loss: 0.2791219614446163, Final Batch Loss: 0.041239574551582336\n",
      "Subject 14, Epoch 913, Loss: 0.2906006723642349, Final Batch Loss: 0.009599484503269196\n",
      "Subject 14, Epoch 914, Loss: 0.22187934815883636, Final Batch Loss: 0.06041194871068001\n",
      "Subject 14, Epoch 915, Loss: 0.3938805013895035, Final Batch Loss: 0.036491088569164276\n",
      "Subject 14, Epoch 916, Loss: 0.3443921469151974, Final Batch Loss: 0.08731894195079803\n",
      "Subject 14, Epoch 917, Loss: 0.3486393317580223, Final Batch Loss: 0.10168902575969696\n",
      "Subject 14, Epoch 918, Loss: 0.3253241032361984, Final Batch Loss: 0.04804115742444992\n",
      "Subject 14, Epoch 919, Loss: 0.29448902420699596, Final Batch Loss: 0.026333985850214958\n",
      "Subject 14, Epoch 920, Loss: 0.29342174157500267, Final Batch Loss: 0.04225208982825279\n",
      "Subject 14, Epoch 921, Loss: 0.26213281974196434, Final Batch Loss: 0.038852669298648834\n",
      "Subject 14, Epoch 922, Loss: 0.3168047368526459, Final Batch Loss: 0.06186530366539955\n",
      "Subject 14, Epoch 923, Loss: 0.30433780141174793, Final Batch Loss: 0.01813616417348385\n",
      "Subject 14, Epoch 924, Loss: 0.2159125618636608, Final Batch Loss: 0.030638013035058975\n",
      "Subject 14, Epoch 925, Loss: 0.162772455252707, Final Batch Loss: 0.01251413207501173\n",
      "Subject 14, Epoch 926, Loss: 0.2734345830976963, Final Batch Loss: 0.05516217648983002\n",
      "Subject 14, Epoch 927, Loss: 0.25461872667074203, Final Batch Loss: 0.048929303884506226\n",
      "Subject 14, Epoch 928, Loss: 0.21741510555148125, Final Batch Loss: 0.03413393348455429\n",
      "Subject 14, Epoch 929, Loss: 0.29920826852321625, Final Batch Loss: 0.07612846791744232\n",
      "Subject 14, Epoch 930, Loss: 0.2399847749620676, Final Batch Loss: 0.03484944999217987\n",
      "Subject 14, Epoch 931, Loss: 0.3531356453895569, Final Batch Loss: 0.12403689324855804\n",
      "Subject 14, Epoch 932, Loss: 0.26543276757001877, Final Batch Loss: 0.08676492422819138\n",
      "Subject 14, Epoch 933, Loss: 0.29093873128294945, Final Batch Loss: 0.1050461009144783\n",
      "Subject 14, Epoch 934, Loss: 0.32855588756501675, Final Batch Loss: 0.02839961089193821\n",
      "Subject 14, Epoch 935, Loss: 0.18525983951985836, Final Batch Loss: 0.02442937158048153\n",
      "Subject 14, Epoch 936, Loss: 0.2228615190833807, Final Batch Loss: 0.05246350169181824\n",
      "Subject 14, Epoch 937, Loss: 0.2738219238817692, Final Batch Loss: 0.03777176886796951\n",
      "Subject 14, Epoch 938, Loss: 0.2570594474673271, Final Batch Loss: 0.06634481251239777\n",
      "Subject 14, Epoch 939, Loss: 0.2541417432948947, Final Batch Loss: 0.008006843738257885\n",
      "Subject 14, Epoch 940, Loss: 0.24592254683375359, Final Batch Loss: 0.05043676495552063\n",
      "Subject 14, Epoch 941, Loss: 0.2289009541273117, Final Batch Loss: 0.017665920779109\n",
      "Subject 14, Epoch 942, Loss: 0.21466037444770336, Final Batch Loss: 0.006451232358813286\n",
      "Subject 14, Epoch 943, Loss: 0.23096507554873824, Final Batch Loss: 0.004137279000133276\n",
      "Subject 14, Epoch 944, Loss: 0.16202673129737377, Final Batch Loss: 0.00811758078634739\n",
      "Subject 14, Epoch 945, Loss: 0.4351028949022293, Final Batch Loss: 0.045137517154216766\n",
      "Subject 14, Epoch 946, Loss: 0.3019617013633251, Final Batch Loss: 0.03992066904902458\n",
      "Subject 14, Epoch 947, Loss: 0.3695680517703295, Final Batch Loss: 0.09154653549194336\n",
      "Subject 14, Epoch 948, Loss: 0.19215370930032805, Final Batch Loss: 0.0007526262779720128\n",
      "Subject 14, Epoch 949, Loss: 0.38370685279369354, Final Batch Loss: 0.10929510742425919\n",
      "Subject 14, Epoch 950, Loss: 0.42792000621557236, Final Batch Loss: 0.11682439595460892\n",
      "Subject 14, Epoch 951, Loss: 0.2535768672823906, Final Batch Loss: 0.007620945572853088\n",
      "Subject 14, Epoch 952, Loss: 0.257254246622324, Final Batch Loss: 0.038912780582904816\n",
      "Subject 14, Epoch 953, Loss: 0.40962231904268265, Final Batch Loss: 0.1243160292506218\n",
      "Subject 14, Epoch 954, Loss: 0.29883284494280815, Final Batch Loss: 0.049584753811359406\n",
      "Subject 14, Epoch 955, Loss: 0.22451086295768619, Final Batch Loss: 0.0032465639524161816\n",
      "Subject 14, Epoch 956, Loss: 0.35582894273102283, Final Batch Loss: 0.025632621720433235\n",
      "Subject 14, Epoch 957, Loss: 0.25137848779559135, Final Batch Loss: 0.05042281001806259\n",
      "Subject 14, Epoch 958, Loss: 0.31526316329836845, Final Batch Loss: 0.026761386543512344\n",
      "Subject 14, Epoch 959, Loss: 0.4903170317411423, Final Batch Loss: 0.22730672359466553\n",
      "Subject 14, Epoch 960, Loss: 0.1904500713571906, Final Batch Loss: 0.005389719270169735\n",
      "Subject 14, Epoch 961, Loss: 0.3502451553940773, Final Batch Loss: 0.07618246227502823\n",
      "Subject 14, Epoch 962, Loss: 0.34384721238166094, Final Batch Loss: 0.004633781500160694\n",
      "Subject 14, Epoch 963, Loss: 0.26403510943055153, Final Batch Loss: 0.0799834355711937\n",
      "Subject 14, Epoch 964, Loss: 0.3316558499936946, Final Batch Loss: 0.0009762247209437191\n",
      "Subject 14, Epoch 965, Loss: 0.44505735114216805, Final Batch Loss: 0.19271403551101685\n",
      "Subject 14, Epoch 966, Loss: 0.3938720300793648, Final Batch Loss: 0.04816313087940216\n",
      "Subject 14, Epoch 967, Loss: 0.22926210332661867, Final Batch Loss: 0.010913641192018986\n",
      "Subject 14, Epoch 968, Loss: 0.4454885683953762, Final Batch Loss: 0.2417566180229187\n",
      "Subject 14, Epoch 969, Loss: 0.36961333081126213, Final Batch Loss: 0.03188236430287361\n",
      "Subject 14, Epoch 970, Loss: 0.2319370899349451, Final Batch Loss: 0.030827684327960014\n",
      "Subject 14, Epoch 971, Loss: 0.24267839081585407, Final Batch Loss: 0.07316689938306808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14, Epoch 972, Loss: 0.38583992421627045, Final Batch Loss: 0.18356016278266907\n",
      "Subject 14, Epoch 973, Loss: 0.21157513465732336, Final Batch Loss: 0.0022581620141863823\n",
      "Subject 14, Epoch 974, Loss: 0.7866920568048954, Final Batch Loss: 0.5456249713897705\n",
      "Subject 14, Epoch 975, Loss: 0.22274816036224365, Final Batch Loss: 0.03659817576408386\n",
      "Subject 14, Epoch 976, Loss: 0.2775191441178322, Final Batch Loss: 0.07832702249288559\n",
      "Subject 14, Epoch 977, Loss: 0.22274857759475708, Final Batch Loss: 0.00532696396112442\n",
      "Subject 14, Epoch 978, Loss: 0.2785353846848011, Final Batch Loss: 0.08800318837165833\n",
      "Subject 14, Epoch 979, Loss: 0.26233985647559166, Final Batch Loss: 0.08274652063846588\n",
      "Subject 14, Epoch 980, Loss: 0.21060172840952873, Final Batch Loss: 0.05965451896190643\n",
      "Subject 14, Epoch 981, Loss: 0.26423024013638496, Final Batch Loss: 0.05854777619242668\n",
      "Subject 14, Epoch 982, Loss: 0.2751930095255375, Final Batch Loss: 0.04679986089468002\n",
      "Subject 14, Epoch 983, Loss: 0.19176622293889523, Final Batch Loss: 0.018532097339630127\n",
      "Subject 14, Epoch 984, Loss: 0.2737892363220453, Final Batch Loss: 0.15682151913642883\n",
      "Subject 14, Epoch 985, Loss: 0.19431894761510193, Final Batch Loss: 0.0030321788508445024\n",
      "Subject 14, Epoch 986, Loss: 0.1775617990642786, Final Batch Loss: 0.027125054970383644\n",
      "Subject 14, Epoch 987, Loss: 0.29904452711343765, Final Batch Loss: 0.09504984319210052\n",
      "Subject 14, Epoch 988, Loss: 0.11981011368334293, Final Batch Loss: 0.011417515575885773\n",
      "Subject 14, Epoch 989, Loss: 0.2999192625284195, Final Batch Loss: 0.14273037016391754\n",
      "Subject 14, Epoch 990, Loss: 0.16515610367059708, Final Batch Loss: 0.031113745644688606\n",
      "Subject 14, Epoch 991, Loss: 0.23633578047156334, Final Batch Loss: 0.04438314959406853\n",
      "Subject 14, Epoch 992, Loss: 0.24430963769555092, Final Batch Loss: 0.036702364683151245\n",
      "Subject 14, Epoch 993, Loss: 0.21767375897616148, Final Batch Loss: 0.008676792494952679\n",
      "Subject 14, Epoch 994, Loss: 0.160671460442245, Final Batch Loss: 0.0033874986693263054\n",
      "Subject 14, Epoch 995, Loss: 0.21598736010491848, Final Batch Loss: 0.01851526089012623\n",
      "Subject 14, Epoch 996, Loss: 0.353187195956707, Final Batch Loss: 0.12356700748205185\n",
      "Subject 14, Epoch 997, Loss: 0.24588866159319878, Final Batch Loss: 0.054154690355062485\n",
      "Subject 14, Epoch 998, Loss: 0.17994601093232632, Final Batch Loss: 0.027271097525954247\n",
      "Subject 14, Epoch 999, Loss: 0.20091067627072334, Final Batch Loss: 0.05618324130773544\n",
      "Subject 14, Epoch 1000, Loss: 0.20684261620044708, Final Batch Loss: 0.015204299241304398\n",
      "Subject 15, Epoch 1, Loss: 7.239910244941711, Final Batch Loss: 1.7904552221298218\n",
      "Subject 15, Epoch 2, Loss: 7.231620788574219, Final Batch Loss: 1.8012516498565674\n",
      "Subject 15, Epoch 3, Loss: 7.2169718742370605, Final Batch Loss: 1.8157012462615967\n",
      "Subject 15, Epoch 4, Loss: 7.192321181297302, Final Batch Loss: 1.7770668268203735\n",
      "Subject 15, Epoch 5, Loss: 7.186802268028259, Final Batch Loss: 1.783799409866333\n",
      "Subject 15, Epoch 6, Loss: 7.174385190010071, Final Batch Loss: 1.7867069244384766\n",
      "Subject 15, Epoch 7, Loss: 7.189121127128601, Final Batch Loss: 1.810642123222351\n",
      "Subject 15, Epoch 8, Loss: 7.154913902282715, Final Batch Loss: 1.777003526687622\n",
      "Subject 15, Epoch 9, Loss: 7.151021242141724, Final Batch Loss: 1.7952040433883667\n",
      "Subject 15, Epoch 10, Loss: 7.15401029586792, Final Batch Loss: 1.8109737634658813\n",
      "Subject 15, Epoch 11, Loss: 7.127551913261414, Final Batch Loss: 1.7852448225021362\n",
      "Subject 15, Epoch 12, Loss: 7.088390946388245, Final Batch Loss: 1.7695587873458862\n",
      "Subject 15, Epoch 13, Loss: 7.066652417182922, Final Batch Loss: 1.7728602886199951\n",
      "Subject 15, Epoch 14, Loss: 7.004681706428528, Final Batch Loss: 1.7405176162719727\n",
      "Subject 15, Epoch 15, Loss: 6.946762681007385, Final Batch Loss: 1.7058602571487427\n",
      "Subject 15, Epoch 16, Loss: 6.867107152938843, Final Batch Loss: 1.6938064098358154\n",
      "Subject 15, Epoch 17, Loss: 6.778504133224487, Final Batch Loss: 1.6708861589431763\n",
      "Subject 15, Epoch 18, Loss: 6.645580887794495, Final Batch Loss: 1.648698091506958\n",
      "Subject 15, Epoch 19, Loss: 6.547024965286255, Final Batch Loss: 1.624776840209961\n",
      "Subject 15, Epoch 20, Loss: 6.330570220947266, Final Batch Loss: 1.5402145385742188\n",
      "Subject 15, Epoch 21, Loss: 6.297039985656738, Final Batch Loss: 1.5876030921936035\n",
      "Subject 15, Epoch 22, Loss: 6.079044342041016, Final Batch Loss: 1.5010085105895996\n",
      "Subject 15, Epoch 23, Loss: 5.832814931869507, Final Batch Loss: 1.4149411916732788\n",
      "Subject 15, Epoch 24, Loss: 5.768591284751892, Final Batch Loss: 1.493780255317688\n",
      "Subject 15, Epoch 25, Loss: 5.64155113697052, Final Batch Loss: 1.418238878250122\n",
      "Subject 15, Epoch 26, Loss: 5.401162266731262, Final Batch Loss: 1.338767409324646\n",
      "Subject 15, Epoch 27, Loss: 5.471228241920471, Final Batch Loss: 1.3225425481796265\n",
      "Subject 15, Epoch 28, Loss: 5.2622586488723755, Final Batch Loss: 1.2941125631332397\n",
      "Subject 15, Epoch 29, Loss: 5.1929237842559814, Final Batch Loss: 1.297188401222229\n",
      "Subject 15, Epoch 30, Loss: 5.16122841835022, Final Batch Loss: 1.2638814449310303\n",
      "Subject 15, Epoch 31, Loss: 5.06506609916687, Final Batch Loss: 1.2190747261047363\n",
      "Subject 15, Epoch 32, Loss: 4.7919957637786865, Final Batch Loss: 1.0794460773468018\n",
      "Subject 15, Epoch 33, Loss: 4.874406337738037, Final Batch Loss: 1.3097158670425415\n",
      "Subject 15, Epoch 34, Loss: 4.864926815032959, Final Batch Loss: 1.1840792894363403\n",
      "Subject 15, Epoch 35, Loss: 4.649709582328796, Final Batch Loss: 1.2341405153274536\n",
      "Subject 15, Epoch 36, Loss: 4.695479512214661, Final Batch Loss: 1.2183616161346436\n",
      "Subject 15, Epoch 37, Loss: 4.52003812789917, Final Batch Loss: 1.1464056968688965\n",
      "Subject 15, Epoch 38, Loss: 4.534041523933411, Final Batch Loss: 1.107475996017456\n",
      "Subject 15, Epoch 39, Loss: 4.437456369400024, Final Batch Loss: 1.0482842922210693\n",
      "Subject 15, Epoch 40, Loss: 4.332346141338348, Final Batch Loss: 0.9998460412025452\n",
      "Subject 15, Epoch 41, Loss: 4.244767069816589, Final Batch Loss: 1.0457851886749268\n",
      "Subject 15, Epoch 42, Loss: 4.172101020812988, Final Batch Loss: 0.9760414361953735\n",
      "Subject 15, Epoch 43, Loss: 4.19424033164978, Final Batch Loss: 1.0608787536621094\n",
      "Subject 15, Epoch 44, Loss: 4.028414309024811, Final Batch Loss: 1.0464417934417725\n",
      "Subject 15, Epoch 45, Loss: 4.234184503555298, Final Batch Loss: 1.0682728290557861\n",
      "Subject 15, Epoch 46, Loss: 3.970982015132904, Final Batch Loss: 0.9792859554290771\n",
      "Subject 15, Epoch 47, Loss: 3.997572958469391, Final Batch Loss: 0.9207779169082642\n",
      "Subject 15, Epoch 48, Loss: 3.9138664603233337, Final Batch Loss: 0.9668871164321899\n",
      "Subject 15, Epoch 49, Loss: 3.81983345746994, Final Batch Loss: 0.8989499807357788\n",
      "Subject 15, Epoch 50, Loss: 4.066862940788269, Final Batch Loss: 0.9894692301750183\n",
      "Subject 15, Epoch 51, Loss: 3.896929681301117, Final Batch Loss: 1.0349793434143066\n",
      "Subject 15, Epoch 52, Loss: 3.8609328269958496, Final Batch Loss: 1.0132571458816528\n",
      "Subject 15, Epoch 53, Loss: 3.684384822845459, Final Batch Loss: 0.8639724254608154\n",
      "Subject 15, Epoch 54, Loss: 3.6365326046943665, Final Batch Loss: 0.8512703776359558\n",
      "Subject 15, Epoch 55, Loss: 3.5774827003479004, Final Batch Loss: 0.901392936706543\n",
      "Subject 15, Epoch 56, Loss: 3.586991608142853, Final Batch Loss: 1.0312130451202393\n",
      "Subject 15, Epoch 57, Loss: 3.7022083401679993, Final Batch Loss: 0.9781541228294373\n",
      "Subject 15, Epoch 58, Loss: 3.5841100811958313, Final Batch Loss: 0.9016898274421692\n",
      "Subject 15, Epoch 59, Loss: 3.50361168384552, Final Batch Loss: 0.8510777354240417\n",
      "Subject 15, Epoch 60, Loss: 3.5364416241645813, Final Batch Loss: 0.8661052584648132\n",
      "Subject 15, Epoch 61, Loss: 3.652070939540863, Final Batch Loss: 0.8802118301391602\n",
      "Subject 15, Epoch 62, Loss: 3.476723790168762, Final Batch Loss: 0.7613579630851746\n",
      "Subject 15, Epoch 63, Loss: 3.281577229499817, Final Batch Loss: 0.7452518939971924\n",
      "Subject 15, Epoch 64, Loss: 3.522169828414917, Final Batch Loss: 0.8985617756843567\n",
      "Subject 15, Epoch 65, Loss: 3.4330455660820007, Final Batch Loss: 0.8680580854415894\n",
      "Subject 15, Epoch 66, Loss: 3.613103449344635, Final Batch Loss: 0.9708666801452637\n",
      "Subject 15, Epoch 67, Loss: 3.4573694467544556, Final Batch Loss: 0.841938316822052\n",
      "Subject 15, Epoch 68, Loss: 3.3053842186927795, Final Batch Loss: 0.8487476706504822\n",
      "Subject 15, Epoch 69, Loss: 3.6138510704040527, Final Batch Loss: 1.0422122478485107\n",
      "Subject 15, Epoch 70, Loss: 3.3007932901382446, Final Batch Loss: 0.8078675866127014\n",
      "Subject 15, Epoch 71, Loss: 3.2414706349372864, Final Batch Loss: 0.7437492609024048\n",
      "Subject 15, Epoch 72, Loss: 3.340888798236847, Final Batch Loss: 0.8129873871803284\n",
      "Subject 15, Epoch 73, Loss: 3.1363635063171387, Final Batch Loss: 0.7665642499923706\n",
      "Subject 15, Epoch 74, Loss: 3.351207435131073, Final Batch Loss: 0.9117007255554199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 75, Loss: 3.3126785159111023, Final Batch Loss: 0.8231782913208008\n",
      "Subject 15, Epoch 76, Loss: 3.1825981736183167, Final Batch Loss: 0.7634649872779846\n",
      "Subject 15, Epoch 77, Loss: 3.1465498208999634, Final Batch Loss: 0.7478067874908447\n",
      "Subject 15, Epoch 78, Loss: 3.1915932297706604, Final Batch Loss: 0.7295271754264832\n",
      "Subject 15, Epoch 79, Loss: 3.120531439781189, Final Batch Loss: 0.9418695569038391\n",
      "Subject 15, Epoch 80, Loss: 3.0743481516838074, Final Batch Loss: 0.770924985408783\n",
      "Subject 15, Epoch 81, Loss: 3.1410337686538696, Final Batch Loss: 0.7929003238677979\n",
      "Subject 15, Epoch 82, Loss: 2.7700294852256775, Final Batch Loss: 0.6394765377044678\n",
      "Subject 15, Epoch 83, Loss: 3.16979318857193, Final Batch Loss: 0.8018432259559631\n",
      "Subject 15, Epoch 84, Loss: 2.982741415500641, Final Batch Loss: 0.797148585319519\n",
      "Subject 15, Epoch 85, Loss: 2.888335406780243, Final Batch Loss: 0.7151307463645935\n",
      "Subject 15, Epoch 86, Loss: 2.9724382162094116, Final Batch Loss: 0.7522681355476379\n",
      "Subject 15, Epoch 87, Loss: 2.726390540599823, Final Batch Loss: 0.6518528461456299\n",
      "Subject 15, Epoch 88, Loss: 3.050865352153778, Final Batch Loss: 0.7924748659133911\n",
      "Subject 15, Epoch 89, Loss: 2.8477061986923218, Final Batch Loss: 0.689999520778656\n",
      "Subject 15, Epoch 90, Loss: 2.979689598083496, Final Batch Loss: 0.7231066226959229\n",
      "Subject 15, Epoch 91, Loss: 2.851826310157776, Final Batch Loss: 0.8082104325294495\n",
      "Subject 15, Epoch 92, Loss: 2.8183762431144714, Final Batch Loss: 0.7665451765060425\n",
      "Subject 15, Epoch 93, Loss: 2.7585331797599792, Final Batch Loss: 0.6735951900482178\n",
      "Subject 15, Epoch 94, Loss: 2.9141162633895874, Final Batch Loss: 0.7280933856964111\n",
      "Subject 15, Epoch 95, Loss: 2.6336020827293396, Final Batch Loss: 0.700623631477356\n",
      "Subject 15, Epoch 96, Loss: 2.6731297373771667, Final Batch Loss: 0.6609643697738647\n",
      "Subject 15, Epoch 97, Loss: 2.745141565799713, Final Batch Loss: 0.7061244249343872\n",
      "Subject 15, Epoch 98, Loss: 2.7951424717903137, Final Batch Loss: 0.749873161315918\n",
      "Subject 15, Epoch 99, Loss: 2.7527597546577454, Final Batch Loss: 0.7028334736824036\n",
      "Subject 15, Epoch 100, Loss: 2.6155897974967957, Final Batch Loss: 0.666761040687561\n",
      "Subject 15, Epoch 101, Loss: 2.6294655799865723, Final Batch Loss: 0.5740875005722046\n",
      "Subject 15, Epoch 102, Loss: 2.5507779717445374, Final Batch Loss: 0.5313215255737305\n",
      "Subject 15, Epoch 103, Loss: 2.646835148334503, Final Batch Loss: 0.6335499286651611\n",
      "Subject 15, Epoch 104, Loss: 2.7754507660865784, Final Batch Loss: 0.7342675924301147\n",
      "Subject 15, Epoch 105, Loss: 2.6867631673812866, Final Batch Loss: 0.7420639395713806\n",
      "Subject 15, Epoch 106, Loss: 2.53448623418808, Final Batch Loss: 0.6161640882492065\n",
      "Subject 15, Epoch 107, Loss: 2.446306973695755, Final Batch Loss: 0.4884522259235382\n",
      "Subject 15, Epoch 108, Loss: 2.867610514163971, Final Batch Loss: 0.8326694369316101\n",
      "Subject 15, Epoch 109, Loss: 2.534969687461853, Final Batch Loss: 0.6695787310600281\n",
      "Subject 15, Epoch 110, Loss: 2.50028795003891, Final Batch Loss: 0.5825056433677673\n",
      "Subject 15, Epoch 111, Loss: 2.4523074626922607, Final Batch Loss: 0.5445297956466675\n",
      "Subject 15, Epoch 112, Loss: 2.616620182991028, Final Batch Loss: 0.6211657524108887\n",
      "Subject 15, Epoch 113, Loss: 2.527893364429474, Final Batch Loss: 0.6245702505111694\n",
      "Subject 15, Epoch 114, Loss: 2.321999579668045, Final Batch Loss: 0.47727295756340027\n",
      "Subject 15, Epoch 115, Loss: 2.316528230905533, Final Batch Loss: 0.4709216058254242\n",
      "Subject 15, Epoch 116, Loss: 2.358047306537628, Final Batch Loss: 0.5909205675125122\n",
      "Subject 15, Epoch 117, Loss: 2.4859312176704407, Final Batch Loss: 0.5632249116897583\n",
      "Subject 15, Epoch 118, Loss: 2.530121088027954, Final Batch Loss: 0.6829678416252136\n",
      "Subject 15, Epoch 119, Loss: 2.371278315782547, Final Batch Loss: 0.48372939229011536\n",
      "Subject 15, Epoch 120, Loss: 2.373807668685913, Final Batch Loss: 0.6471223831176758\n",
      "Subject 15, Epoch 121, Loss: 2.2747950553894043, Final Batch Loss: 0.5349972248077393\n",
      "Subject 15, Epoch 122, Loss: 2.3382202982902527, Final Batch Loss: 0.5876991748809814\n",
      "Subject 15, Epoch 123, Loss: 2.2835481762886047, Final Batch Loss: 0.561272382736206\n",
      "Subject 15, Epoch 124, Loss: 2.5395593643188477, Final Batch Loss: 0.5792094469070435\n",
      "Subject 15, Epoch 125, Loss: 2.2170664072036743, Final Batch Loss: 0.537481963634491\n",
      "Subject 15, Epoch 126, Loss: 2.366402417421341, Final Batch Loss: 0.5496048331260681\n",
      "Subject 15, Epoch 127, Loss: 2.2838322520256042, Final Batch Loss: 0.5328155755996704\n",
      "Subject 15, Epoch 128, Loss: 2.5118671655654907, Final Batch Loss: 0.5871484875679016\n",
      "Subject 15, Epoch 129, Loss: 2.463189482688904, Final Batch Loss: 0.6929552555084229\n",
      "Subject 15, Epoch 130, Loss: 2.421945035457611, Final Batch Loss: 0.6686922907829285\n",
      "Subject 15, Epoch 131, Loss: 2.273021161556244, Final Batch Loss: 0.4598589539527893\n",
      "Subject 15, Epoch 132, Loss: 2.2537331581115723, Final Batch Loss: 0.6648284196853638\n",
      "Subject 15, Epoch 133, Loss: 2.2519582509994507, Final Batch Loss: 0.5025495290756226\n",
      "Subject 15, Epoch 134, Loss: 2.4456499218940735, Final Batch Loss: 0.6519511342048645\n",
      "Subject 15, Epoch 135, Loss: 2.185611307621002, Final Batch Loss: 0.44941556453704834\n",
      "Subject 15, Epoch 136, Loss: 2.647957742214203, Final Batch Loss: 0.792120099067688\n",
      "Subject 15, Epoch 137, Loss: 2.2887527346611023, Final Batch Loss: 0.5782526731491089\n",
      "Subject 15, Epoch 138, Loss: 2.2836470901966095, Final Batch Loss: 0.7445368766784668\n",
      "Subject 15, Epoch 139, Loss: 2.1935911774635315, Final Batch Loss: 0.45977461338043213\n",
      "Subject 15, Epoch 140, Loss: 2.405004382133484, Final Batch Loss: 0.6028546094894409\n",
      "Subject 15, Epoch 141, Loss: 2.1460262537002563, Final Batch Loss: 0.542719841003418\n",
      "Subject 15, Epoch 142, Loss: 2.372943103313446, Final Batch Loss: 0.5453706979751587\n",
      "Subject 15, Epoch 143, Loss: 2.139466345310211, Final Batch Loss: 0.5048049092292786\n",
      "Subject 15, Epoch 144, Loss: 2.3204022645950317, Final Batch Loss: 0.653515100479126\n",
      "Subject 15, Epoch 145, Loss: 2.187063753604889, Final Batch Loss: 0.5411739349365234\n",
      "Subject 15, Epoch 146, Loss: 2.2384576201438904, Final Batch Loss: 0.5264888405799866\n",
      "Subject 15, Epoch 147, Loss: 2.0506666004657745, Final Batch Loss: 0.49147841334342957\n",
      "Subject 15, Epoch 148, Loss: 2.335128664970398, Final Batch Loss: 0.6292455792427063\n",
      "Subject 15, Epoch 149, Loss: 2.2070321440696716, Final Batch Loss: 0.5469697117805481\n",
      "Subject 15, Epoch 150, Loss: 2.0267415940761566, Final Batch Loss: 0.5191302299499512\n",
      "Subject 15, Epoch 151, Loss: 2.2417882084846497, Final Batch Loss: 0.6343740224838257\n",
      "Subject 15, Epoch 152, Loss: 2.1210691332817078, Final Batch Loss: 0.5013196468353271\n",
      "Subject 15, Epoch 153, Loss: 2.1779264509677887, Final Batch Loss: 0.48028305172920227\n",
      "Subject 15, Epoch 154, Loss: 2.152906596660614, Final Batch Loss: 0.6525294780731201\n",
      "Subject 15, Epoch 155, Loss: 2.1134833991527557, Final Batch Loss: 0.43923792243003845\n",
      "Subject 15, Epoch 156, Loss: 2.1898468136787415, Final Batch Loss: 0.5347234606742859\n",
      "Subject 15, Epoch 157, Loss: 2.418837010860443, Final Batch Loss: 0.7914444804191589\n",
      "Subject 15, Epoch 158, Loss: 2.085650384426117, Final Batch Loss: 0.4443932771682739\n",
      "Subject 15, Epoch 159, Loss: 2.054865390062332, Final Batch Loss: 0.5373558402061462\n",
      "Subject 15, Epoch 160, Loss: 2.063597619533539, Final Batch Loss: 0.5048083662986755\n",
      "Subject 15, Epoch 161, Loss: 2.1442809104919434, Final Batch Loss: 0.6543283462524414\n",
      "Subject 15, Epoch 162, Loss: 2.1123021841049194, Final Batch Loss: 0.5252406001091003\n",
      "Subject 15, Epoch 163, Loss: 2.0263537764549255, Final Batch Loss: 0.43914592266082764\n",
      "Subject 15, Epoch 164, Loss: 2.242137610912323, Final Batch Loss: 0.6677515506744385\n",
      "Subject 15, Epoch 165, Loss: 2.068821281194687, Final Batch Loss: 0.5764305591583252\n",
      "Subject 15, Epoch 166, Loss: 2.2333124577999115, Final Batch Loss: 0.7156108021736145\n",
      "Subject 15, Epoch 167, Loss: 2.0461263358592987, Final Batch Loss: 0.48367950320243835\n",
      "Subject 15, Epoch 168, Loss: 2.0423302352428436, Final Batch Loss: 0.50814288854599\n",
      "Subject 15, Epoch 169, Loss: 1.9180963635444641, Final Batch Loss: 0.40559524297714233\n",
      "Subject 15, Epoch 170, Loss: 2.0795293748378754, Final Batch Loss: 0.41970929503440857\n",
      "Subject 15, Epoch 171, Loss: 2.000241309404373, Final Batch Loss: 0.4061782658100128\n",
      "Subject 15, Epoch 172, Loss: 2.2309616804122925, Final Batch Loss: 0.5033388137817383\n",
      "Subject 15, Epoch 173, Loss: 2.0592895448207855, Final Batch Loss: 0.6107848882675171\n",
      "Subject 15, Epoch 174, Loss: 2.0937503576278687, Final Batch Loss: 0.5488665699958801\n",
      "Subject 15, Epoch 175, Loss: 2.114335358142853, Final Batch Loss: 0.5528519153594971\n",
      "Subject 15, Epoch 176, Loss: 2.0195215940475464, Final Batch Loss: 0.437866747379303\n",
      "Subject 15, Epoch 177, Loss: 2.0931892096996307, Final Batch Loss: 0.5329657793045044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 178, Loss: 2.10764679312706, Final Batch Loss: 0.460529625415802\n",
      "Subject 15, Epoch 179, Loss: 1.984766811132431, Final Batch Loss: 0.4854113459587097\n",
      "Subject 15, Epoch 180, Loss: 2.1209788024425507, Final Batch Loss: 0.626664936542511\n",
      "Subject 15, Epoch 181, Loss: 1.8791379630565643, Final Batch Loss: 0.42663416266441345\n",
      "Subject 15, Epoch 182, Loss: 2.118951141834259, Final Batch Loss: 0.5322625041007996\n",
      "Subject 15, Epoch 183, Loss: 2.1452926993370056, Final Batch Loss: 0.42606788873672485\n",
      "Subject 15, Epoch 184, Loss: 2.178892195224762, Final Batch Loss: 0.5974562764167786\n",
      "Subject 15, Epoch 185, Loss: 2.031413286924362, Final Batch Loss: 0.5079365968704224\n",
      "Subject 15, Epoch 186, Loss: 2.079137772321701, Final Batch Loss: 0.49596038460731506\n",
      "Subject 15, Epoch 187, Loss: 1.9670251905918121, Final Batch Loss: 0.4956185817718506\n",
      "Subject 15, Epoch 188, Loss: 1.9877865016460419, Final Batch Loss: 0.4622724950313568\n",
      "Subject 15, Epoch 189, Loss: 2.0039028227329254, Final Batch Loss: 0.41706982254981995\n",
      "Subject 15, Epoch 190, Loss: 2.010919004678726, Final Batch Loss: 0.5608882904052734\n",
      "Subject 15, Epoch 191, Loss: 2.0082486867904663, Final Batch Loss: 0.43532902002334595\n",
      "Subject 15, Epoch 192, Loss: 1.9866371154785156, Final Batch Loss: 0.4740448296070099\n",
      "Subject 15, Epoch 193, Loss: 1.8949582874774933, Final Batch Loss: 0.44392129778862\n",
      "Subject 15, Epoch 194, Loss: 1.960644394159317, Final Batch Loss: 0.5211132764816284\n",
      "Subject 15, Epoch 195, Loss: 2.1338821053504944, Final Batch Loss: 0.6932907104492188\n",
      "Subject 15, Epoch 196, Loss: 1.974943459033966, Final Batch Loss: 0.5214404463768005\n",
      "Subject 15, Epoch 197, Loss: 1.9991440176963806, Final Batch Loss: 0.5228720307350159\n",
      "Subject 15, Epoch 198, Loss: 2.119996190071106, Final Batch Loss: 0.6148204207420349\n",
      "Subject 15, Epoch 199, Loss: 2.0187485218048096, Final Batch Loss: 0.5662370324134827\n",
      "Subject 15, Epoch 200, Loss: 1.86143097281456, Final Batch Loss: 0.5101832747459412\n",
      "Subject 15, Epoch 201, Loss: 2.0564270317554474, Final Batch Loss: 0.526158332824707\n",
      "Subject 15, Epoch 202, Loss: 1.8927125930786133, Final Batch Loss: 0.4634060561656952\n",
      "Subject 15, Epoch 203, Loss: 1.7268051207065582, Final Batch Loss: 0.3369925618171692\n",
      "Subject 15, Epoch 204, Loss: 2.011277735233307, Final Batch Loss: 0.5170618295669556\n",
      "Subject 15, Epoch 205, Loss: 1.950068473815918, Final Batch Loss: 0.499508261680603\n",
      "Subject 15, Epoch 206, Loss: 2.1201132237911224, Final Batch Loss: 0.5352745056152344\n",
      "Subject 15, Epoch 207, Loss: 2.0674712657928467, Final Batch Loss: 0.6001750230789185\n",
      "Subject 15, Epoch 208, Loss: 1.8366220593452454, Final Batch Loss: 0.35286086797714233\n",
      "Subject 15, Epoch 209, Loss: 1.851690262556076, Final Batch Loss: 0.4586820900440216\n",
      "Subject 15, Epoch 210, Loss: 1.9286760091781616, Final Batch Loss: 0.538507878780365\n",
      "Subject 15, Epoch 211, Loss: 1.8558020889759064, Final Batch Loss: 0.4229179918766022\n",
      "Subject 15, Epoch 212, Loss: 1.9332168996334076, Final Batch Loss: 0.43799251317977905\n",
      "Subject 15, Epoch 213, Loss: 1.9900452494621277, Final Batch Loss: 0.4823361337184906\n",
      "Subject 15, Epoch 214, Loss: 1.9744137525558472, Final Batch Loss: 0.4965478479862213\n",
      "Subject 15, Epoch 215, Loss: 1.7539767026901245, Final Batch Loss: 0.347714900970459\n",
      "Subject 15, Epoch 216, Loss: 1.9210433661937714, Final Batch Loss: 0.4268779754638672\n",
      "Subject 15, Epoch 217, Loss: 1.830100268125534, Final Batch Loss: 0.33267462253570557\n",
      "Subject 15, Epoch 218, Loss: 1.909912794828415, Final Batch Loss: 0.4058166742324829\n",
      "Subject 15, Epoch 219, Loss: 1.9519018232822418, Final Batch Loss: 0.5267584919929504\n",
      "Subject 15, Epoch 220, Loss: 1.8877044320106506, Final Batch Loss: 0.470714271068573\n",
      "Subject 15, Epoch 221, Loss: 1.8889698386192322, Final Batch Loss: 0.45120158791542053\n",
      "Subject 15, Epoch 222, Loss: 1.9775818288326263, Final Batch Loss: 0.6133175492286682\n",
      "Subject 15, Epoch 223, Loss: 2.0030975937843323, Final Batch Loss: 0.6544831991195679\n",
      "Subject 15, Epoch 224, Loss: 1.8319335877895355, Final Batch Loss: 0.4480803608894348\n",
      "Subject 15, Epoch 225, Loss: 1.8849031329154968, Final Batch Loss: 0.3733513057231903\n",
      "Subject 15, Epoch 226, Loss: 1.9239493608474731, Final Batch Loss: 0.5527865886688232\n",
      "Subject 15, Epoch 227, Loss: 1.740225613117218, Final Batch Loss: 0.32445064187049866\n",
      "Subject 15, Epoch 228, Loss: 1.9058749675750732, Final Batch Loss: 0.4893912971019745\n",
      "Subject 15, Epoch 229, Loss: 1.861789494752884, Final Batch Loss: 0.4428073763847351\n",
      "Subject 15, Epoch 230, Loss: 1.8838652968406677, Final Batch Loss: 0.43178436160087585\n",
      "Subject 15, Epoch 231, Loss: 1.6704919934272766, Final Batch Loss: 0.36183738708496094\n",
      "Subject 15, Epoch 232, Loss: 1.8750554621219635, Final Batch Loss: 0.3727989196777344\n",
      "Subject 15, Epoch 233, Loss: 1.9405772686004639, Final Batch Loss: 0.6403389573097229\n",
      "Subject 15, Epoch 234, Loss: 1.809384435415268, Final Batch Loss: 0.5643818378448486\n",
      "Subject 15, Epoch 235, Loss: 1.73931223154068, Final Batch Loss: 0.49303582310676575\n",
      "Subject 15, Epoch 236, Loss: 2.0119668841362, Final Batch Loss: 0.6540807485580444\n",
      "Subject 15, Epoch 237, Loss: 1.8931752145290375, Final Batch Loss: 0.4754233658313751\n",
      "Subject 15, Epoch 238, Loss: 1.7767401337623596, Final Batch Loss: 0.3924944996833801\n",
      "Subject 15, Epoch 239, Loss: 1.8572389781475067, Final Batch Loss: 0.4414921700954437\n",
      "Subject 15, Epoch 240, Loss: 1.8870465755462646, Final Batch Loss: 0.48475366830825806\n",
      "Subject 15, Epoch 241, Loss: 1.8189107477664948, Final Batch Loss: 0.48408830165863037\n",
      "Subject 15, Epoch 242, Loss: 1.9168558418750763, Final Batch Loss: 0.544853925704956\n",
      "Subject 15, Epoch 243, Loss: 1.7791112661361694, Final Batch Loss: 0.2660537362098694\n",
      "Subject 15, Epoch 244, Loss: 1.8238787353038788, Final Batch Loss: 0.4746992588043213\n",
      "Subject 15, Epoch 245, Loss: 1.8654009401798248, Final Batch Loss: 0.5037968754768372\n",
      "Subject 15, Epoch 246, Loss: 1.7124611735343933, Final Batch Loss: 0.4234587550163269\n",
      "Subject 15, Epoch 247, Loss: 1.8148852586746216, Final Batch Loss: 0.42197319865226746\n",
      "Subject 15, Epoch 248, Loss: 1.7293887436389923, Final Batch Loss: 0.4118930697441101\n",
      "Subject 15, Epoch 249, Loss: 1.8113382756710052, Final Batch Loss: 0.5027058124542236\n",
      "Subject 15, Epoch 250, Loss: 1.8676926493644714, Final Batch Loss: 0.5078514814376831\n",
      "Subject 15, Epoch 251, Loss: 1.9291477799415588, Final Batch Loss: 0.5954692959785461\n",
      "Subject 15, Epoch 252, Loss: 1.8646422028541565, Final Batch Loss: 0.47409090399742126\n",
      "Subject 15, Epoch 253, Loss: 1.8994752764701843, Final Batch Loss: 0.3801063001155853\n",
      "Subject 15, Epoch 254, Loss: 2.056050330400467, Final Batch Loss: 0.45935913920402527\n",
      "Subject 15, Epoch 255, Loss: 1.788988083600998, Final Batch Loss: 0.48771634697914124\n",
      "Subject 15, Epoch 256, Loss: 1.8947709500789642, Final Batch Loss: 0.4295583665370941\n",
      "Subject 15, Epoch 257, Loss: 1.9677524864673615, Final Batch Loss: 0.5351889729499817\n",
      "Subject 15, Epoch 258, Loss: 1.776313066482544, Final Batch Loss: 0.42145732045173645\n",
      "Subject 15, Epoch 259, Loss: 1.738920122385025, Final Batch Loss: 0.48919475078582764\n",
      "Subject 15, Epoch 260, Loss: 1.7058788537979126, Final Batch Loss: 0.39884498715400696\n",
      "Subject 15, Epoch 261, Loss: 1.9028414189815521, Final Batch Loss: 0.47707927227020264\n",
      "Subject 15, Epoch 262, Loss: 1.7322907745838165, Final Batch Loss: 0.389679491519928\n",
      "Subject 15, Epoch 263, Loss: 1.9834032356739044, Final Batch Loss: 0.4509272575378418\n",
      "Subject 15, Epoch 264, Loss: 1.8376012742519379, Final Batch Loss: 0.4351440966129303\n",
      "Subject 15, Epoch 265, Loss: 1.8650405704975128, Final Batch Loss: 0.4511675238609314\n",
      "Subject 15, Epoch 266, Loss: 2.02560031414032, Final Batch Loss: 0.5206576585769653\n",
      "Subject 15, Epoch 267, Loss: 1.7004447281360626, Final Batch Loss: 0.32817959785461426\n",
      "Subject 15, Epoch 268, Loss: 1.7346189618110657, Final Batch Loss: 0.40658825635910034\n",
      "Subject 15, Epoch 269, Loss: 1.7942678034305573, Final Batch Loss: 0.4031858742237091\n",
      "Subject 15, Epoch 270, Loss: 1.7862274050712585, Final Batch Loss: 0.43652036786079407\n",
      "Subject 15, Epoch 271, Loss: 1.7275061905384064, Final Batch Loss: 0.3881937265396118\n",
      "Subject 15, Epoch 272, Loss: 1.7187028527259827, Final Batch Loss: 0.44129064679145813\n",
      "Subject 15, Epoch 273, Loss: 1.7463936805725098, Final Batch Loss: 0.40372782945632935\n",
      "Subject 15, Epoch 274, Loss: 1.7313158810138702, Final Batch Loss: 0.44144174456596375\n",
      "Subject 15, Epoch 275, Loss: 1.6905787289142609, Final Batch Loss: 0.3798946142196655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 276, Loss: 1.7419982254505157, Final Batch Loss: 0.49452656507492065\n",
      "Subject 15, Epoch 277, Loss: 1.6879159212112427, Final Batch Loss: 0.383954793214798\n",
      "Subject 15, Epoch 278, Loss: 1.5921260714530945, Final Batch Loss: 0.4300457835197449\n",
      "Subject 15, Epoch 279, Loss: 1.8980595171451569, Final Batch Loss: 0.4635975658893585\n",
      "Subject 15, Epoch 280, Loss: 1.8340531289577484, Final Batch Loss: 0.5445247888565063\n",
      "Subject 15, Epoch 281, Loss: 1.6487602293491364, Final Batch Loss: 0.3972565531730652\n",
      "Subject 15, Epoch 282, Loss: 1.7590103149414062, Final Batch Loss: 0.39713233709335327\n",
      "Subject 15, Epoch 283, Loss: 1.8113019466400146, Final Batch Loss: 0.43446701765060425\n",
      "Subject 15, Epoch 284, Loss: 1.610268771648407, Final Batch Loss: 0.3519384562969208\n",
      "Subject 15, Epoch 285, Loss: 1.807747334241867, Final Batch Loss: 0.4995554983615875\n",
      "Subject 15, Epoch 286, Loss: 1.6534284055233002, Final Batch Loss: 0.4758414924144745\n",
      "Subject 15, Epoch 287, Loss: 1.7719259560108185, Final Batch Loss: 0.39805689454078674\n",
      "Subject 15, Epoch 288, Loss: 1.6083273589611053, Final Batch Loss: 0.39777079224586487\n",
      "Subject 15, Epoch 289, Loss: 1.8023830950260162, Final Batch Loss: 0.5126246213912964\n",
      "Subject 15, Epoch 290, Loss: 1.707106590270996, Final Batch Loss: 0.3013078272342682\n",
      "Subject 15, Epoch 291, Loss: 1.8081677854061127, Final Batch Loss: 0.46511760354042053\n",
      "Subject 15, Epoch 292, Loss: 1.830592542886734, Final Batch Loss: 0.3867051601409912\n",
      "Subject 15, Epoch 293, Loss: 1.6365944743156433, Final Batch Loss: 0.33896371722221375\n",
      "Subject 15, Epoch 294, Loss: 1.858116865158081, Final Batch Loss: 0.5283796787261963\n",
      "Subject 15, Epoch 295, Loss: 1.803056925535202, Final Batch Loss: 0.4323933720588684\n",
      "Subject 15, Epoch 296, Loss: 1.5651826560497284, Final Batch Loss: 0.3717491924762726\n",
      "Subject 15, Epoch 297, Loss: 1.6885356903076172, Final Batch Loss: 0.37560707330703735\n",
      "Subject 15, Epoch 298, Loss: 1.6005937159061432, Final Batch Loss: 0.35876867175102234\n",
      "Subject 15, Epoch 299, Loss: 1.7370171248912811, Final Batch Loss: 0.3708365559577942\n",
      "Subject 15, Epoch 300, Loss: 1.8458155393600464, Final Batch Loss: 0.411891907453537\n",
      "Subject 15, Epoch 301, Loss: 1.8287294209003448, Final Batch Loss: 0.6251224279403687\n",
      "Subject 15, Epoch 302, Loss: 1.656974971294403, Final Batch Loss: 0.3381905257701874\n",
      "Subject 15, Epoch 303, Loss: 1.6898978054523468, Final Batch Loss: 0.3681224584579468\n",
      "Subject 15, Epoch 304, Loss: 1.7409136891365051, Final Batch Loss: 0.40266796946525574\n",
      "Subject 15, Epoch 305, Loss: 1.5928006768226624, Final Batch Loss: 0.37246546149253845\n",
      "Subject 15, Epoch 306, Loss: 1.886017769575119, Final Batch Loss: 0.5025197267532349\n",
      "Subject 15, Epoch 307, Loss: 1.7395541667938232, Final Batch Loss: 0.34603777527809143\n",
      "Subject 15, Epoch 308, Loss: 1.803900808095932, Final Batch Loss: 0.5299075245857239\n",
      "Subject 15, Epoch 309, Loss: 1.7089928090572357, Final Batch Loss: 0.4337449073791504\n",
      "Subject 15, Epoch 310, Loss: 1.6467870473861694, Final Batch Loss: 0.4170339107513428\n",
      "Subject 15, Epoch 311, Loss: 1.6436647772789001, Final Batch Loss: 0.4379962384700775\n",
      "Subject 15, Epoch 312, Loss: 1.569708377122879, Final Batch Loss: 0.4184108376502991\n",
      "Subject 15, Epoch 313, Loss: 1.6116403937339783, Final Batch Loss: 0.3844454288482666\n",
      "Subject 15, Epoch 314, Loss: 1.6606733798980713, Final Batch Loss: 0.4731079339981079\n",
      "Subject 15, Epoch 315, Loss: 1.79340398311615, Final Batch Loss: 0.37028518319129944\n",
      "Subject 15, Epoch 316, Loss: 1.7464476227760315, Final Batch Loss: 0.5169645547866821\n",
      "Subject 15, Epoch 317, Loss: 1.6867434680461884, Final Batch Loss: 0.47317928075790405\n",
      "Subject 15, Epoch 318, Loss: 1.689101368188858, Final Batch Loss: 0.5235909223556519\n",
      "Subject 15, Epoch 319, Loss: 1.7366927564144135, Final Batch Loss: 0.43041691184043884\n",
      "Subject 15, Epoch 320, Loss: 1.6878874599933624, Final Batch Loss: 0.427888959646225\n",
      "Subject 15, Epoch 321, Loss: 1.7171322107315063, Final Batch Loss: 0.40753230452537537\n",
      "Subject 15, Epoch 322, Loss: 1.6107893288135529, Final Batch Loss: 0.4169735908508301\n",
      "Subject 15, Epoch 323, Loss: 1.7803724110126495, Final Batch Loss: 0.512712836265564\n",
      "Subject 15, Epoch 324, Loss: 1.6015524566173553, Final Batch Loss: 0.3856790065765381\n",
      "Subject 15, Epoch 325, Loss: 1.701581358909607, Final Batch Loss: 0.3791082799434662\n",
      "Subject 15, Epoch 326, Loss: 1.6631874442100525, Final Batch Loss: 0.4900982081890106\n",
      "Subject 15, Epoch 327, Loss: 1.637186735868454, Final Batch Loss: 0.4428948760032654\n",
      "Subject 15, Epoch 328, Loss: 1.7321354150772095, Final Batch Loss: 0.41641807556152344\n",
      "Subject 15, Epoch 329, Loss: 1.6735819876194, Final Batch Loss: 0.4629083573818207\n",
      "Subject 15, Epoch 330, Loss: 1.6763587296009064, Final Batch Loss: 0.39731964468955994\n",
      "Subject 15, Epoch 331, Loss: 1.7353200018405914, Final Batch Loss: 0.435606986284256\n",
      "Subject 15, Epoch 332, Loss: 1.6288827061653137, Final Batch Loss: 0.3807632625102997\n",
      "Subject 15, Epoch 333, Loss: 1.5594594478607178, Final Batch Loss: 0.2996388375759125\n",
      "Subject 15, Epoch 334, Loss: 1.7397478520870209, Final Batch Loss: 0.41957494616508484\n",
      "Subject 15, Epoch 335, Loss: 1.6521168947219849, Final Batch Loss: 0.43128034472465515\n",
      "Subject 15, Epoch 336, Loss: 1.5445505380630493, Final Batch Loss: 0.3195178210735321\n",
      "Subject 15, Epoch 337, Loss: 1.6197154521942139, Final Batch Loss: 0.34080570936203003\n",
      "Subject 15, Epoch 338, Loss: 1.8132568895816803, Final Batch Loss: 0.5102338194847107\n",
      "Subject 15, Epoch 339, Loss: 1.7205068171024323, Final Batch Loss: 0.4014620780944824\n",
      "Subject 15, Epoch 340, Loss: 1.6873608827590942, Final Batch Loss: 0.49585601687431335\n",
      "Subject 15, Epoch 341, Loss: 1.6457239091396332, Final Batch Loss: 0.321814626455307\n",
      "Subject 15, Epoch 342, Loss: 1.687957763671875, Final Batch Loss: 0.4862159490585327\n",
      "Subject 15, Epoch 343, Loss: 1.7065871059894562, Final Batch Loss: 0.47212520241737366\n",
      "Subject 15, Epoch 344, Loss: 1.5594419836997986, Final Batch Loss: 0.33865824341773987\n",
      "Subject 15, Epoch 345, Loss: 1.6481663882732391, Final Batch Loss: 0.27588868141174316\n",
      "Subject 15, Epoch 346, Loss: 1.6257586479187012, Final Batch Loss: 0.4158923029899597\n",
      "Subject 15, Epoch 347, Loss: 1.801307588815689, Final Batch Loss: 0.48958271741867065\n",
      "Subject 15, Epoch 348, Loss: 1.5608382821083069, Final Batch Loss: 0.33345937728881836\n",
      "Subject 15, Epoch 349, Loss: 1.8672591745853424, Final Batch Loss: 0.485740065574646\n",
      "Subject 15, Epoch 350, Loss: 1.679459810256958, Final Batch Loss: 0.38036224246025085\n",
      "Subject 15, Epoch 351, Loss: 1.6660809814929962, Final Batch Loss: 0.41082561016082764\n",
      "Subject 15, Epoch 352, Loss: 1.6227263808250427, Final Batch Loss: 0.2792423367500305\n",
      "Subject 15, Epoch 353, Loss: 1.6275252401828766, Final Batch Loss: 0.49088072776794434\n",
      "Subject 15, Epoch 354, Loss: 1.6780676245689392, Final Batch Loss: 0.35034874081611633\n",
      "Subject 15, Epoch 355, Loss: 1.6016728579998016, Final Batch Loss: 0.41589072346687317\n",
      "Subject 15, Epoch 356, Loss: 1.675361543893814, Final Batch Loss: 0.508259117603302\n",
      "Subject 15, Epoch 357, Loss: 1.5775817036628723, Final Batch Loss: 0.43998247385025024\n",
      "Subject 15, Epoch 358, Loss: 1.598621904850006, Final Batch Loss: 0.379881888628006\n",
      "Subject 15, Epoch 359, Loss: 1.5603550374507904, Final Batch Loss: 0.2826235592365265\n",
      "Subject 15, Epoch 360, Loss: 1.5976526141166687, Final Batch Loss: 0.3925028145313263\n",
      "Subject 15, Epoch 361, Loss: 1.6490620076656342, Final Batch Loss: 0.2799430787563324\n",
      "Subject 15, Epoch 362, Loss: 1.439050018787384, Final Batch Loss: 0.29262807965278625\n",
      "Subject 15, Epoch 363, Loss: 1.623780608177185, Final Batch Loss: 0.3514034152030945\n",
      "Subject 15, Epoch 364, Loss: 1.7044494152069092, Final Batch Loss: 0.47717317938804626\n",
      "Subject 15, Epoch 365, Loss: 1.6908009052276611, Final Batch Loss: 0.48480236530303955\n",
      "Subject 15, Epoch 366, Loss: 1.7980417907238007, Final Batch Loss: 0.3837029039859772\n",
      "Subject 15, Epoch 367, Loss: 1.6183077692985535, Final Batch Loss: 0.3309251070022583\n",
      "Subject 15, Epoch 368, Loss: 1.643485575914383, Final Batch Loss: 0.3908923864364624\n",
      "Subject 15, Epoch 369, Loss: 1.514000654220581, Final Batch Loss: 0.26104018092155457\n",
      "Subject 15, Epoch 370, Loss: 1.5618903040885925, Final Batch Loss: 0.41416874527931213\n",
      "Subject 15, Epoch 371, Loss: 1.4346566796302795, Final Batch Loss: 0.34382370114326477\n",
      "Subject 15, Epoch 372, Loss: 1.4849400520324707, Final Batch Loss: 0.24930110573768616\n",
      "Subject 15, Epoch 373, Loss: 1.8337047696113586, Final Batch Loss: 0.3912144899368286\n",
      "Subject 15, Epoch 374, Loss: 1.6025681793689728, Final Batch Loss: 0.4104906916618347\n",
      "Subject 15, Epoch 375, Loss: 1.6171025335788727, Final Batch Loss: 0.48740866780281067\n",
      "Subject 15, Epoch 376, Loss: 1.6808502674102783, Final Batch Loss: 0.6180809736251831\n",
      "Subject 15, Epoch 377, Loss: 1.6289040446281433, Final Batch Loss: 0.4082009792327881\n",
      "Subject 15, Epoch 378, Loss: 1.63791024684906, Final Batch Loss: 0.39619746804237366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 379, Loss: 1.4722851812839508, Final Batch Loss: 0.2654792070388794\n",
      "Subject 15, Epoch 380, Loss: 1.5114840269088745, Final Batch Loss: 0.3651948869228363\n",
      "Subject 15, Epoch 381, Loss: 1.5804888606071472, Final Batch Loss: 0.4494020938873291\n",
      "Subject 15, Epoch 382, Loss: 1.533846229314804, Final Batch Loss: 0.30052390694618225\n",
      "Subject 15, Epoch 383, Loss: 1.6028265953063965, Final Batch Loss: 0.27756941318511963\n",
      "Subject 15, Epoch 384, Loss: 1.5846170485019684, Final Batch Loss: 0.319376140832901\n",
      "Subject 15, Epoch 385, Loss: 1.6857253015041351, Final Batch Loss: 0.5688864588737488\n",
      "Subject 15, Epoch 386, Loss: 1.6152248978614807, Final Batch Loss: 0.4454111158847809\n",
      "Subject 15, Epoch 387, Loss: 1.5341465473175049, Final Batch Loss: 0.31437844038009644\n",
      "Subject 15, Epoch 388, Loss: 1.5141454339027405, Final Batch Loss: 0.38009029626846313\n",
      "Subject 15, Epoch 389, Loss: 1.6445229053497314, Final Batch Loss: 0.5166804790496826\n",
      "Subject 15, Epoch 390, Loss: 1.7650699019432068, Final Batch Loss: 0.5465478897094727\n",
      "Subject 15, Epoch 391, Loss: 1.622964322566986, Final Batch Loss: 0.3868067264556885\n",
      "Subject 15, Epoch 392, Loss: 1.5664857923984528, Final Batch Loss: 0.39430516958236694\n",
      "Subject 15, Epoch 393, Loss: 1.794865995645523, Final Batch Loss: 0.5955367684364319\n",
      "Subject 15, Epoch 394, Loss: 1.5532045364379883, Final Batch Loss: 0.37573131918907166\n",
      "Subject 15, Epoch 395, Loss: 1.574259877204895, Final Batch Loss: 0.3760873079299927\n",
      "Subject 15, Epoch 396, Loss: 1.7525681257247925, Final Batch Loss: 0.46104225516319275\n",
      "Subject 15, Epoch 397, Loss: 1.6223961412906647, Final Batch Loss: 0.39517706632614136\n",
      "Subject 15, Epoch 398, Loss: 1.5996654629707336, Final Batch Loss: 0.4119724631309509\n",
      "Subject 15, Epoch 399, Loss: 1.6668077409267426, Final Batch Loss: 0.3185538053512573\n",
      "Subject 15, Epoch 400, Loss: 1.644066482782364, Final Batch Loss: 0.42988327145576477\n",
      "Subject 15, Epoch 401, Loss: 1.4391525834798813, Final Batch Loss: 0.24979041516780853\n",
      "Subject 15, Epoch 402, Loss: 1.5730908513069153, Final Batch Loss: 0.4682449996471405\n",
      "Subject 15, Epoch 403, Loss: 1.6914893686771393, Final Batch Loss: 0.4630312919616699\n",
      "Subject 15, Epoch 404, Loss: 1.5819932520389557, Final Batch Loss: 0.3802137076854706\n",
      "Subject 15, Epoch 405, Loss: 1.4740029275417328, Final Batch Loss: 0.2772665321826935\n",
      "Subject 15, Epoch 406, Loss: 1.45965875685215, Final Batch Loss: 0.3237074613571167\n",
      "Subject 15, Epoch 407, Loss: 1.741319477558136, Final Batch Loss: 0.47095128893852234\n",
      "Subject 15, Epoch 408, Loss: 1.6346189081668854, Final Batch Loss: 0.6564821004867554\n",
      "Subject 15, Epoch 409, Loss: 1.4659150540828705, Final Batch Loss: 0.23501238226890564\n",
      "Subject 15, Epoch 410, Loss: 1.5157130360603333, Final Batch Loss: 0.41277551651000977\n",
      "Subject 15, Epoch 411, Loss: 1.3942312002182007, Final Batch Loss: 0.2626136839389801\n",
      "Subject 15, Epoch 412, Loss: 1.57777738571167, Final Batch Loss: 0.3694692850112915\n",
      "Subject 15, Epoch 413, Loss: 1.6076841950416565, Final Batch Loss: 0.4411783516407013\n",
      "Subject 15, Epoch 414, Loss: 1.5313324928283691, Final Batch Loss: 0.3945172131061554\n",
      "Subject 15, Epoch 415, Loss: 1.5874645411968231, Final Batch Loss: 0.4033135771751404\n",
      "Subject 15, Epoch 416, Loss: 1.6482296288013458, Final Batch Loss: 0.4646734297275543\n",
      "Subject 15, Epoch 417, Loss: 1.6567919254302979, Final Batch Loss: 0.5067115426063538\n",
      "Subject 15, Epoch 418, Loss: 1.571289986371994, Final Batch Loss: 0.4593493342399597\n",
      "Subject 15, Epoch 419, Loss: 1.5583398342132568, Final Batch Loss: 0.41449466347694397\n",
      "Subject 15, Epoch 420, Loss: 1.4666317105293274, Final Batch Loss: 0.4261953830718994\n",
      "Subject 15, Epoch 421, Loss: 1.6672095656394958, Final Batch Loss: 0.49492165446281433\n",
      "Subject 15, Epoch 422, Loss: 1.6364575028419495, Final Batch Loss: 0.31568047404289246\n",
      "Subject 15, Epoch 423, Loss: 1.611698716878891, Final Batch Loss: 0.3972844183444977\n",
      "Subject 15, Epoch 424, Loss: 1.4027306735515594, Final Batch Loss: 0.3356112539768219\n",
      "Subject 15, Epoch 425, Loss: 1.5934289395809174, Final Batch Loss: 0.3251340389251709\n",
      "Subject 15, Epoch 426, Loss: 1.6246434450149536, Final Batch Loss: 0.5372921824455261\n",
      "Subject 15, Epoch 427, Loss: 1.4991371929645538, Final Batch Loss: 0.38978955149650574\n",
      "Subject 15, Epoch 428, Loss: 1.47011598944664, Final Batch Loss: 0.3441929519176483\n",
      "Subject 15, Epoch 429, Loss: 1.4355541467666626, Final Batch Loss: 0.3364245891571045\n",
      "Subject 15, Epoch 430, Loss: 1.6654104888439178, Final Batch Loss: 0.45238587260246277\n",
      "Subject 15, Epoch 431, Loss: 1.5190061032772064, Final Batch Loss: 0.3100503981113434\n",
      "Subject 15, Epoch 432, Loss: 1.5608010590076447, Final Batch Loss: 0.4033970534801483\n",
      "Subject 15, Epoch 433, Loss: 1.5065077543258667, Final Batch Loss: 0.3757120370864868\n",
      "Subject 15, Epoch 434, Loss: 1.442406326532364, Final Batch Loss: 0.29340410232543945\n",
      "Subject 15, Epoch 435, Loss: 1.5781474113464355, Final Batch Loss: 0.350093811750412\n",
      "Subject 15, Epoch 436, Loss: 1.4015830755233765, Final Batch Loss: 0.4631994068622589\n",
      "Subject 15, Epoch 437, Loss: 1.5220604538917542, Final Batch Loss: 0.30675387382507324\n",
      "Subject 15, Epoch 438, Loss: 1.4764913618564606, Final Batch Loss: 0.3814335763454437\n",
      "Subject 15, Epoch 439, Loss: 1.356695830821991, Final Batch Loss: 0.2943538725376129\n",
      "Subject 15, Epoch 440, Loss: 1.5232675075531006, Final Batch Loss: 0.43130502104759216\n",
      "Subject 15, Epoch 441, Loss: 1.7502604126930237, Final Batch Loss: 0.5031331777572632\n",
      "Subject 15, Epoch 442, Loss: 1.4793117344379425, Final Batch Loss: 0.3265489935874939\n",
      "Subject 15, Epoch 443, Loss: 1.5249494910240173, Final Batch Loss: 0.30018293857574463\n",
      "Subject 15, Epoch 444, Loss: 1.5387475490570068, Final Batch Loss: 0.35264575481414795\n",
      "Subject 15, Epoch 445, Loss: 1.6535578072071075, Final Batch Loss: 0.49516361951828003\n",
      "Subject 15, Epoch 446, Loss: 1.6386415362358093, Final Batch Loss: 0.31779804825782776\n",
      "Subject 15, Epoch 447, Loss: 1.6210402250289917, Final Batch Loss: 0.4055916368961334\n",
      "Subject 15, Epoch 448, Loss: 1.6186621189117432, Final Batch Loss: 0.39107298851013184\n",
      "Subject 15, Epoch 449, Loss: 1.3830977976322174, Final Batch Loss: 0.3416871726512909\n",
      "Subject 15, Epoch 450, Loss: 1.5467109680175781, Final Batch Loss: 0.3186832368373871\n",
      "Subject 15, Epoch 451, Loss: 1.5917719006538391, Final Batch Loss: 0.46274030208587646\n",
      "Subject 15, Epoch 452, Loss: 1.6327984631061554, Final Batch Loss: 0.3851793110370636\n",
      "Subject 15, Epoch 453, Loss: 1.527749091386795, Final Batch Loss: 0.3473730683326721\n",
      "Subject 15, Epoch 454, Loss: 1.36712047457695, Final Batch Loss: 0.3372476398944855\n",
      "Subject 15, Epoch 455, Loss: 1.5841946303844452, Final Batch Loss: 0.416828453540802\n",
      "Subject 15, Epoch 456, Loss: 1.4811505675315857, Final Batch Loss: 0.4069090783596039\n",
      "Subject 15, Epoch 457, Loss: 1.5383362770080566, Final Batch Loss: 0.36746853590011597\n",
      "Subject 15, Epoch 458, Loss: 1.4425275325775146, Final Batch Loss: 0.2882632911205292\n",
      "Subject 15, Epoch 459, Loss: 1.4134699702262878, Final Batch Loss: 0.27776920795440674\n",
      "Subject 15, Epoch 460, Loss: 1.4671511352062225, Final Batch Loss: 0.43619009852409363\n",
      "Subject 15, Epoch 461, Loss: 1.5159421265125275, Final Batch Loss: 0.305077463388443\n",
      "Subject 15, Epoch 462, Loss: 1.3761329352855682, Final Batch Loss: 0.33420291543006897\n",
      "Subject 15, Epoch 463, Loss: 1.5233017206192017, Final Batch Loss: 0.3969891667366028\n",
      "Subject 15, Epoch 464, Loss: 1.3845574855804443, Final Batch Loss: 0.29603585600852966\n",
      "Subject 15, Epoch 465, Loss: 1.4219436943531036, Final Batch Loss: 0.3289939761161804\n",
      "Subject 15, Epoch 466, Loss: 1.3513852208852768, Final Batch Loss: 0.2240409106016159\n",
      "Subject 15, Epoch 467, Loss: 1.634860247373581, Final Batch Loss: 0.46048542857170105\n",
      "Subject 15, Epoch 468, Loss: 1.4452009797096252, Final Batch Loss: 0.31879234313964844\n",
      "Subject 15, Epoch 469, Loss: 1.5190174877643585, Final Batch Loss: 0.4712230861186981\n",
      "Subject 15, Epoch 470, Loss: 1.6147245466709137, Final Batch Loss: 0.36738675832748413\n",
      "Subject 15, Epoch 471, Loss: 1.7066426873207092, Final Batch Loss: 0.5122362375259399\n",
      "Subject 15, Epoch 472, Loss: 1.5329077541828156, Final Batch Loss: 0.30385512113571167\n",
      "Subject 15, Epoch 473, Loss: 1.394432783126831, Final Batch Loss: 0.2541244328022003\n",
      "Subject 15, Epoch 474, Loss: 1.5243085324764252, Final Batch Loss: 0.4732253849506378\n",
      "Subject 15, Epoch 475, Loss: 1.6417539417743683, Final Batch Loss: 0.3778861165046692\n",
      "Subject 15, Epoch 476, Loss: 1.4399130642414093, Final Batch Loss: 0.37742072343826294\n",
      "Subject 15, Epoch 477, Loss: 1.3828135430812836, Final Batch Loss: 0.3158772885799408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 478, Loss: 1.384299486875534, Final Batch Loss: 0.3408328890800476\n",
      "Subject 15, Epoch 479, Loss: 1.501802235841751, Final Batch Loss: 0.3596634864807129\n",
      "Subject 15, Epoch 480, Loss: 1.4636104702949524, Final Batch Loss: 0.40101173520088196\n",
      "Subject 15, Epoch 481, Loss: 1.3645491898059845, Final Batch Loss: 0.36770328879356384\n",
      "Subject 15, Epoch 482, Loss: 1.4018281400203705, Final Batch Loss: 0.22173669934272766\n",
      "Subject 15, Epoch 483, Loss: 1.6200509667396545, Final Batch Loss: 0.49093392491340637\n",
      "Subject 15, Epoch 484, Loss: 1.3859989941120148, Final Batch Loss: 0.31336578726768494\n",
      "Subject 15, Epoch 485, Loss: 1.3861114978790283, Final Batch Loss: 0.32705527544021606\n",
      "Subject 15, Epoch 486, Loss: 1.382333368062973, Final Batch Loss: 0.35394227504730225\n",
      "Subject 15, Epoch 487, Loss: 1.6811642944812775, Final Batch Loss: 0.3748058080673218\n",
      "Subject 15, Epoch 488, Loss: 1.4086435735225677, Final Batch Loss: 0.37535062432289124\n",
      "Subject 15, Epoch 489, Loss: 1.51447993516922, Final Batch Loss: 0.34761297702789307\n",
      "Subject 15, Epoch 490, Loss: 1.4034684896469116, Final Batch Loss: 0.3827489912509918\n",
      "Subject 15, Epoch 491, Loss: 1.5791911482810974, Final Batch Loss: 0.602787435054779\n",
      "Subject 15, Epoch 492, Loss: 1.5014272034168243, Final Batch Loss: 0.3862122893333435\n",
      "Subject 15, Epoch 493, Loss: 1.573357492685318, Final Batch Loss: 0.4120398461818695\n",
      "Subject 15, Epoch 494, Loss: 1.4436173141002655, Final Batch Loss: 0.35040929913520813\n",
      "Subject 15, Epoch 495, Loss: 1.4135627150535583, Final Batch Loss: 0.34072551131248474\n",
      "Subject 15, Epoch 496, Loss: 1.4754396677017212, Final Batch Loss: 0.3285096287727356\n",
      "Subject 15, Epoch 497, Loss: 1.3586689233779907, Final Batch Loss: 0.3435971736907959\n",
      "Subject 15, Epoch 498, Loss: 1.4421934187412262, Final Batch Loss: 0.336673766374588\n",
      "Subject 15, Epoch 499, Loss: 1.3384166359901428, Final Batch Loss: 0.3254794180393219\n",
      "Subject 15, Epoch 500, Loss: 1.3962448239326477, Final Batch Loss: 0.35094258189201355\n",
      "Subject 15, Epoch 501, Loss: 1.4158612489700317, Final Batch Loss: 0.3828775882720947\n",
      "Subject 15, Epoch 502, Loss: 1.5080033838748932, Final Batch Loss: 0.49899908900260925\n",
      "Subject 15, Epoch 503, Loss: 1.4998307824134827, Final Batch Loss: 0.33509373664855957\n",
      "Subject 15, Epoch 504, Loss: 1.4817602634429932, Final Batch Loss: 0.3622841238975525\n",
      "Subject 15, Epoch 505, Loss: 1.3415214717388153, Final Batch Loss: 0.3070339262485504\n",
      "Subject 15, Epoch 506, Loss: 1.3714869022369385, Final Batch Loss: 0.29522594809532166\n",
      "Subject 15, Epoch 507, Loss: 1.4342671930789948, Final Batch Loss: 0.35614800453186035\n",
      "Subject 15, Epoch 508, Loss: 1.3488961607217789, Final Batch Loss: 0.29406532645225525\n",
      "Subject 15, Epoch 509, Loss: 1.3400417268276215, Final Batch Loss: 0.33070048689842224\n",
      "Subject 15, Epoch 510, Loss: 1.5253023207187653, Final Batch Loss: 0.4416850209236145\n",
      "Subject 15, Epoch 511, Loss: 1.428002029657364, Final Batch Loss: 0.2690275013446808\n",
      "Subject 15, Epoch 512, Loss: 1.3676854968070984, Final Batch Loss: 0.4061618149280548\n",
      "Subject 15, Epoch 513, Loss: 1.391606867313385, Final Batch Loss: 0.4128265082836151\n",
      "Subject 15, Epoch 514, Loss: 1.348877489566803, Final Batch Loss: 0.2987254559993744\n",
      "Subject 15, Epoch 515, Loss: 1.3623252511024475, Final Batch Loss: 0.2760448157787323\n",
      "Subject 15, Epoch 516, Loss: 1.4509282112121582, Final Batch Loss: 0.4489278197288513\n",
      "Subject 15, Epoch 517, Loss: 1.4134242534637451, Final Batch Loss: 0.33448129892349243\n",
      "Subject 15, Epoch 518, Loss: 1.3185050785541534, Final Batch Loss: 0.2975172698497772\n",
      "Subject 15, Epoch 519, Loss: 1.3907546401023865, Final Batch Loss: 0.2725522816181183\n",
      "Subject 15, Epoch 520, Loss: 1.4523502588272095, Final Batch Loss: 0.40477079153060913\n",
      "Subject 15, Epoch 521, Loss: 1.400071233510971, Final Batch Loss: 0.37802934646606445\n",
      "Subject 15, Epoch 522, Loss: 1.473452776670456, Final Batch Loss: 0.39867448806762695\n",
      "Subject 15, Epoch 523, Loss: 1.3453477323055267, Final Batch Loss: 0.3881787955760956\n",
      "Subject 15, Epoch 524, Loss: 1.3894733786582947, Final Batch Loss: 0.29790565371513367\n",
      "Subject 15, Epoch 525, Loss: 1.5727384686470032, Final Batch Loss: 0.405131071805954\n",
      "Subject 15, Epoch 526, Loss: 1.3822091221809387, Final Batch Loss: 0.2880096137523651\n",
      "Subject 15, Epoch 527, Loss: 1.381165236234665, Final Batch Loss: 0.3748542368412018\n",
      "Subject 15, Epoch 528, Loss: 1.3494971096515656, Final Batch Loss: 0.34617286920547485\n",
      "Subject 15, Epoch 529, Loss: 1.3016075491905212, Final Batch Loss: 0.2948239743709564\n",
      "Subject 15, Epoch 530, Loss: 1.2899827361106873, Final Batch Loss: 0.28243836760520935\n",
      "Subject 15, Epoch 531, Loss: 1.3320000767707825, Final Batch Loss: 0.34426769614219666\n",
      "Subject 15, Epoch 532, Loss: 1.3406781554222107, Final Batch Loss: 0.33942410349845886\n",
      "Subject 15, Epoch 533, Loss: 1.3376626074314117, Final Batch Loss: 0.3401547372341156\n",
      "Subject 15, Epoch 534, Loss: 1.4313737154006958, Final Batch Loss: 0.3636516034603119\n",
      "Subject 15, Epoch 535, Loss: 1.3629953861236572, Final Batch Loss: 0.3302316963672638\n",
      "Subject 15, Epoch 536, Loss: 1.260327398777008, Final Batch Loss: 0.3039701282978058\n",
      "Subject 15, Epoch 537, Loss: 1.4780218303203583, Final Batch Loss: 0.43609926104545593\n",
      "Subject 15, Epoch 538, Loss: 1.510088950395584, Final Batch Loss: 0.4050101935863495\n",
      "Subject 15, Epoch 539, Loss: 1.3032415509223938, Final Batch Loss: 0.3276076912879944\n",
      "Subject 15, Epoch 540, Loss: 1.3059428930282593, Final Batch Loss: 0.2913857102394104\n",
      "Subject 15, Epoch 541, Loss: 1.5529201328754425, Final Batch Loss: 0.5761798620223999\n",
      "Subject 15, Epoch 542, Loss: 1.4632624387741089, Final Batch Loss: 0.4309307932853699\n",
      "Subject 15, Epoch 543, Loss: 1.4505994021892548, Final Batch Loss: 0.40273210406303406\n",
      "Subject 15, Epoch 544, Loss: 1.448129028081894, Final Batch Loss: 0.33699357509613037\n",
      "Subject 15, Epoch 545, Loss: 1.4592841267585754, Final Batch Loss: 0.36145856976509094\n",
      "Subject 15, Epoch 546, Loss: 1.2574301064014435, Final Batch Loss: 0.2835356593132019\n",
      "Subject 15, Epoch 547, Loss: 1.317043274641037, Final Batch Loss: 0.3244864344596863\n",
      "Subject 15, Epoch 548, Loss: 1.3205560147762299, Final Batch Loss: 0.3277025520801544\n",
      "Subject 15, Epoch 549, Loss: 1.411128431558609, Final Batch Loss: 0.33002039790153503\n",
      "Subject 15, Epoch 550, Loss: 1.3588006794452667, Final Batch Loss: 0.36292752623558044\n",
      "Subject 15, Epoch 551, Loss: 1.4338130950927734, Final Batch Loss: 0.3480111062526703\n",
      "Subject 15, Epoch 552, Loss: 1.3371806889772415, Final Batch Loss: 0.3593086898326874\n",
      "Subject 15, Epoch 553, Loss: 1.3529868423938751, Final Batch Loss: 0.31227007508277893\n",
      "Subject 15, Epoch 554, Loss: 1.342524915933609, Final Batch Loss: 0.3431757986545563\n",
      "Subject 15, Epoch 555, Loss: 1.333076149225235, Final Batch Loss: 0.381598562002182\n",
      "Subject 15, Epoch 556, Loss: 1.3397555947303772, Final Batch Loss: 0.31199872493743896\n",
      "Subject 15, Epoch 557, Loss: 1.2654480040073395, Final Batch Loss: 0.273003488779068\n",
      "Subject 15, Epoch 558, Loss: 1.2743660807609558, Final Batch Loss: 0.28114280104637146\n",
      "Subject 15, Epoch 559, Loss: 1.1548264622688293, Final Batch Loss: 0.2338135540485382\n",
      "Subject 15, Epoch 560, Loss: 1.4695497155189514, Final Batch Loss: 0.28680911660194397\n",
      "Subject 15, Epoch 561, Loss: 1.273706316947937, Final Batch Loss: 0.2701011896133423\n",
      "Subject 15, Epoch 562, Loss: 1.4106769561767578, Final Batch Loss: 0.37768104672431946\n",
      "Subject 15, Epoch 563, Loss: 1.2914283573627472, Final Batch Loss: 0.34792250394821167\n",
      "Subject 15, Epoch 564, Loss: 1.4177584052085876, Final Batch Loss: 0.5287920236587524\n",
      "Subject 15, Epoch 565, Loss: 1.2596242427825928, Final Batch Loss: 0.2751207649707794\n",
      "Subject 15, Epoch 566, Loss: 1.2699692249298096, Final Batch Loss: 0.25234150886535645\n",
      "Subject 15, Epoch 567, Loss: 1.342520534992218, Final Batch Loss: 0.37096136808395386\n",
      "Subject 15, Epoch 568, Loss: 1.2434993386268616, Final Batch Loss: 0.31173834204673767\n",
      "Subject 15, Epoch 569, Loss: 1.3633074462413788, Final Batch Loss: 0.27540910243988037\n",
      "Subject 15, Epoch 570, Loss: 1.2519206702709198, Final Batch Loss: 0.32336515188217163\n",
      "Subject 15, Epoch 571, Loss: 1.2915670275688171, Final Batch Loss: 0.30405953526496887\n",
      "Subject 15, Epoch 572, Loss: 1.2433053702116013, Final Batch Loss: 0.21969778835773468\n",
      "Subject 15, Epoch 573, Loss: 1.352082073688507, Final Batch Loss: 0.29910850524902344\n",
      "Subject 15, Epoch 574, Loss: 1.34034925699234, Final Batch Loss: 0.41771575808525085\n",
      "Subject 15, Epoch 575, Loss: 1.2406437993049622, Final Batch Loss: 0.25719642639160156\n",
      "Subject 15, Epoch 576, Loss: 1.250227838754654, Final Batch Loss: 0.24872562289237976\n",
      "Subject 15, Epoch 577, Loss: 1.3548333644866943, Final Batch Loss: 0.30394914746284485\n",
      "Subject 15, Epoch 578, Loss: 1.3156685531139374, Final Batch Loss: 0.2715432941913605\n",
      "Subject 15, Epoch 579, Loss: 1.5208054184913635, Final Batch Loss: 0.4914032816886902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 580, Loss: 1.2937345802783966, Final Batch Loss: 0.20252221822738647\n",
      "Subject 15, Epoch 581, Loss: 1.1003965884447098, Final Batch Loss: 0.2106103152036667\n",
      "Subject 15, Epoch 582, Loss: 1.2745748460292816, Final Batch Loss: 0.2982111871242523\n",
      "Subject 15, Epoch 583, Loss: 1.301719218492508, Final Batch Loss: 0.38268402218818665\n",
      "Subject 15, Epoch 584, Loss: 1.2895745933055878, Final Batch Loss: 0.29386696219444275\n",
      "Subject 15, Epoch 585, Loss: 1.400008574128151, Final Batch Loss: 0.44747352600097656\n",
      "Subject 15, Epoch 586, Loss: 1.2929844558238983, Final Batch Loss: 0.2587936520576477\n",
      "Subject 15, Epoch 587, Loss: 1.339611291885376, Final Batch Loss: 0.24734383821487427\n",
      "Subject 15, Epoch 588, Loss: 1.33765909075737, Final Batch Loss: 0.32870805263519287\n",
      "Subject 15, Epoch 589, Loss: 1.3216115534305573, Final Batch Loss: 0.29654163122177124\n",
      "Subject 15, Epoch 590, Loss: 1.249163031578064, Final Batch Loss: 0.31623971462249756\n",
      "Subject 15, Epoch 591, Loss: 1.2671311795711517, Final Batch Loss: 0.29645445942878723\n",
      "Subject 15, Epoch 592, Loss: 1.3533369302749634, Final Batch Loss: 0.33694741129875183\n",
      "Subject 15, Epoch 593, Loss: 1.2584847211837769, Final Batch Loss: 0.3452666401863098\n",
      "Subject 15, Epoch 594, Loss: 1.2728162705898285, Final Batch Loss: 0.3432411849498749\n",
      "Subject 15, Epoch 595, Loss: 1.2235395014286041, Final Batch Loss: 0.286480188369751\n",
      "Subject 15, Epoch 596, Loss: 1.2475887089967728, Final Batch Loss: 0.2430763691663742\n",
      "Subject 15, Epoch 597, Loss: 1.3698766827583313, Final Batch Loss: 0.40160122513771057\n",
      "Subject 15, Epoch 598, Loss: 1.3001128137111664, Final Batch Loss: 0.3083178400993347\n",
      "Subject 15, Epoch 599, Loss: 1.2997659146785736, Final Batch Loss: 0.28342264890670776\n",
      "Subject 15, Epoch 600, Loss: 1.2212018072605133, Final Batch Loss: 0.3357654809951782\n",
      "Subject 15, Epoch 601, Loss: 1.2646189332008362, Final Batch Loss: 0.294944167137146\n",
      "Subject 15, Epoch 602, Loss: 1.407917320728302, Final Batch Loss: 0.4126873314380646\n",
      "Subject 15, Epoch 603, Loss: 1.2198776304721832, Final Batch Loss: 0.25274598598480225\n",
      "Subject 15, Epoch 604, Loss: 1.5686299204826355, Final Batch Loss: 0.36382684111595154\n",
      "Subject 15, Epoch 605, Loss: 1.3143281638622284, Final Batch Loss: 0.3739824891090393\n",
      "Subject 15, Epoch 606, Loss: 1.373869389295578, Final Batch Loss: 0.3878454267978668\n",
      "Subject 15, Epoch 607, Loss: 1.3536527156829834, Final Batch Loss: 0.3286118507385254\n",
      "Subject 15, Epoch 608, Loss: 1.2833089530467987, Final Batch Loss: 0.3187168538570404\n",
      "Subject 15, Epoch 609, Loss: 1.279835820198059, Final Batch Loss: 0.3563823699951172\n",
      "Subject 15, Epoch 610, Loss: 1.213501751422882, Final Batch Loss: 0.31498536467552185\n",
      "Subject 15, Epoch 611, Loss: 1.3995790779590607, Final Batch Loss: 0.4998146891593933\n",
      "Subject 15, Epoch 612, Loss: 1.3071926534175873, Final Batch Loss: 0.32129520177841187\n",
      "Subject 15, Epoch 613, Loss: 1.3029722571372986, Final Batch Loss: 0.307328462600708\n",
      "Subject 15, Epoch 614, Loss: 1.3593253791332245, Final Batch Loss: 0.4129624366760254\n",
      "Subject 15, Epoch 615, Loss: 1.3158682584762573, Final Batch Loss: 0.3153334856033325\n",
      "Subject 15, Epoch 616, Loss: 1.341922640800476, Final Batch Loss: 0.36177167296409607\n",
      "Subject 15, Epoch 617, Loss: 1.2319121807813644, Final Batch Loss: 0.31608209013938904\n",
      "Subject 15, Epoch 618, Loss: 1.3717544376850128, Final Batch Loss: 0.447507381439209\n",
      "Subject 15, Epoch 619, Loss: 1.2182479500770569, Final Batch Loss: 0.26910465955734253\n",
      "Subject 15, Epoch 620, Loss: 1.1772849708795547, Final Batch Loss: 0.3387244939804077\n",
      "Subject 15, Epoch 621, Loss: 1.2592935860157013, Final Batch Loss: 0.3940012454986572\n",
      "Subject 15, Epoch 622, Loss: 1.185461401939392, Final Batch Loss: 0.2851647734642029\n",
      "Subject 15, Epoch 623, Loss: 1.2436431348323822, Final Batch Loss: 0.312910258769989\n",
      "Subject 15, Epoch 624, Loss: 1.126609429717064, Final Batch Loss: 0.2803836762905121\n",
      "Subject 15, Epoch 625, Loss: 1.2352161407470703, Final Batch Loss: 0.2916828691959381\n",
      "Subject 15, Epoch 626, Loss: 1.0838395357131958, Final Batch Loss: 0.1774732917547226\n",
      "Subject 15, Epoch 627, Loss: 1.2770746350288391, Final Batch Loss: 0.3239099085330963\n",
      "Subject 15, Epoch 628, Loss: 1.3001077473163605, Final Batch Loss: 0.3005125820636749\n",
      "Subject 15, Epoch 629, Loss: 1.232868731021881, Final Batch Loss: 0.2881123423576355\n",
      "Subject 15, Epoch 630, Loss: 1.3453996777534485, Final Batch Loss: 0.3587168753147125\n",
      "Subject 15, Epoch 631, Loss: 1.0840198546648026, Final Batch Loss: 0.1886751502752304\n",
      "Subject 15, Epoch 632, Loss: 1.2129893600940704, Final Batch Loss: 0.2991727590560913\n",
      "Subject 15, Epoch 633, Loss: 1.0950563549995422, Final Batch Loss: 0.26806962490081787\n",
      "Subject 15, Epoch 634, Loss: 1.2328305393457413, Final Batch Loss: 0.21922560036182404\n",
      "Subject 15, Epoch 635, Loss: 1.2089285552501678, Final Batch Loss: 0.28998681902885437\n",
      "Subject 15, Epoch 636, Loss: 1.242845430970192, Final Batch Loss: 0.24085856974124908\n",
      "Subject 15, Epoch 637, Loss: 1.2620375752449036, Final Batch Loss: 0.34020334482192993\n",
      "Subject 15, Epoch 638, Loss: 1.156635358929634, Final Batch Loss: 0.21046774089336395\n",
      "Subject 15, Epoch 639, Loss: 1.2086575329303741, Final Batch Loss: 0.2888939380645752\n",
      "Subject 15, Epoch 640, Loss: 1.2681593745946884, Final Batch Loss: 0.37195366621017456\n",
      "Subject 15, Epoch 641, Loss: 1.218346655368805, Final Batch Loss: 0.30318182706832886\n",
      "Subject 15, Epoch 642, Loss: 1.3331907540559769, Final Batch Loss: 0.230656698346138\n",
      "Subject 15, Epoch 643, Loss: 1.1786009073257446, Final Batch Loss: 0.27325278520584106\n",
      "Subject 15, Epoch 644, Loss: 1.1328794956207275, Final Batch Loss: 0.25855308771133423\n",
      "Subject 15, Epoch 645, Loss: 1.1331413835287094, Final Batch Loss: 0.28419142961502075\n",
      "Subject 15, Epoch 646, Loss: 1.2461221814155579, Final Batch Loss: 0.36184611916542053\n",
      "Subject 15, Epoch 647, Loss: 1.1441238969564438, Final Batch Loss: 0.1857830137014389\n",
      "Subject 15, Epoch 648, Loss: 1.182031974196434, Final Batch Loss: 0.2242717295885086\n",
      "Subject 15, Epoch 649, Loss: 1.0679304897785187, Final Batch Loss: 0.2186436951160431\n",
      "Subject 15, Epoch 650, Loss: 1.2748063206672668, Final Batch Loss: 0.2800004780292511\n",
      "Subject 15, Epoch 651, Loss: 1.2476836889982224, Final Batch Loss: 0.3983747959136963\n",
      "Subject 15, Epoch 652, Loss: 1.2634029388427734, Final Batch Loss: 0.24246686697006226\n",
      "Subject 15, Epoch 653, Loss: 1.1159602403640747, Final Batch Loss: 0.22860269248485565\n",
      "Subject 15, Epoch 654, Loss: 1.2958986163139343, Final Batch Loss: 0.41254040598869324\n",
      "Subject 15, Epoch 655, Loss: 1.2812480330467224, Final Batch Loss: 0.29409754276275635\n",
      "Subject 15, Epoch 656, Loss: 1.141427218914032, Final Batch Loss: 0.2657371759414673\n",
      "Subject 15, Epoch 657, Loss: 1.1761287450790405, Final Batch Loss: 0.2440032958984375\n",
      "Subject 15, Epoch 658, Loss: 1.1821852922439575, Final Batch Loss: 0.32360783219337463\n",
      "Subject 15, Epoch 659, Loss: 1.1537754237651825, Final Batch Loss: 0.274259090423584\n",
      "Subject 15, Epoch 660, Loss: 1.0037107020616531, Final Batch Loss: 0.18584884703159332\n",
      "Subject 15, Epoch 661, Loss: 1.2256946861743927, Final Batch Loss: 0.2925477921962738\n",
      "Subject 15, Epoch 662, Loss: 1.237507849931717, Final Batch Loss: 0.3102423846721649\n",
      "Subject 15, Epoch 663, Loss: 1.240388184785843, Final Batch Loss: 0.4062270224094391\n",
      "Subject 15, Epoch 664, Loss: 1.1749103367328644, Final Batch Loss: 0.26852044463157654\n",
      "Subject 15, Epoch 665, Loss: 1.3179042339324951, Final Batch Loss: 0.3697304427623749\n",
      "Subject 15, Epoch 666, Loss: 1.1640136390924454, Final Batch Loss: 0.3768238425254822\n",
      "Subject 15, Epoch 667, Loss: 1.1506078839302063, Final Batch Loss: 0.28580692410469055\n",
      "Subject 15, Epoch 668, Loss: 1.2839578092098236, Final Batch Loss: 0.42194700241088867\n",
      "Subject 15, Epoch 669, Loss: 1.359881728887558, Final Batch Loss: 0.4824737310409546\n",
      "Subject 15, Epoch 670, Loss: 1.2426795363426208, Final Batch Loss: 0.2645629644393921\n",
      "Subject 15, Epoch 671, Loss: 1.1784381717443466, Final Batch Loss: 0.19485624134540558\n",
      "Subject 15, Epoch 672, Loss: 1.1511041671037674, Final Batch Loss: 0.19297565519809723\n",
      "Subject 15, Epoch 673, Loss: 1.2520538568496704, Final Batch Loss: 0.309352308511734\n",
      "Subject 15, Epoch 674, Loss: 1.1995946764945984, Final Batch Loss: 0.34506210684776306\n",
      "Subject 15, Epoch 675, Loss: 1.1746787279844284, Final Batch Loss: 0.26571404933929443\n",
      "Subject 15, Epoch 676, Loss: 1.216840922832489, Final Batch Loss: 0.29976192116737366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 677, Loss: 1.0873881876468658, Final Batch Loss: 0.241046741604805\n",
      "Subject 15, Epoch 678, Loss: 1.4176397919654846, Final Batch Loss: 0.38680019974708557\n",
      "Subject 15, Epoch 679, Loss: 1.1368099600076675, Final Batch Loss: 0.3509971499443054\n",
      "Subject 15, Epoch 680, Loss: 1.1392236351966858, Final Batch Loss: 0.2876542806625366\n",
      "Subject 15, Epoch 681, Loss: 1.2195145636796951, Final Batch Loss: 0.3438553810119629\n",
      "Subject 15, Epoch 682, Loss: 1.133653998374939, Final Batch Loss: 0.22975437343120575\n",
      "Subject 15, Epoch 683, Loss: 1.1503615528345108, Final Batch Loss: 0.21595780551433563\n",
      "Subject 15, Epoch 684, Loss: 1.1528232544660568, Final Batch Loss: 0.3275655508041382\n",
      "Subject 15, Epoch 685, Loss: 1.1757984310388565, Final Batch Loss: 0.2790991961956024\n",
      "Subject 15, Epoch 686, Loss: 1.2337662875652313, Final Batch Loss: 0.25913703441619873\n",
      "Subject 15, Epoch 687, Loss: 1.1658922135829926, Final Batch Loss: 0.27759847044944763\n",
      "Subject 15, Epoch 688, Loss: 1.2128857374191284, Final Batch Loss: 0.4009815454483032\n",
      "Subject 15, Epoch 689, Loss: 1.1145025044679642, Final Batch Loss: 0.19369880855083466\n",
      "Subject 15, Epoch 690, Loss: 1.0605694502592087, Final Batch Loss: 0.20158489048480988\n",
      "Subject 15, Epoch 691, Loss: 1.0559260547161102, Final Batch Loss: 0.16906103491783142\n",
      "Subject 15, Epoch 692, Loss: 1.1424401700496674, Final Batch Loss: 0.2877734303474426\n",
      "Subject 15, Epoch 693, Loss: 1.0361708998680115, Final Batch Loss: 0.20256748795509338\n",
      "Subject 15, Epoch 694, Loss: 1.1811810433864594, Final Batch Loss: 0.29591530561447144\n",
      "Subject 15, Epoch 695, Loss: 1.2033547163009644, Final Batch Loss: 0.3095836341381073\n",
      "Subject 15, Epoch 696, Loss: 1.1984603255987167, Final Batch Loss: 0.2790485620498657\n",
      "Subject 15, Epoch 697, Loss: 1.2572917640209198, Final Batch Loss: 0.333805114030838\n",
      "Subject 15, Epoch 698, Loss: 1.2833216190338135, Final Batch Loss: 0.24970465898513794\n",
      "Subject 15, Epoch 699, Loss: 1.1466695219278336, Final Batch Loss: 0.2345999926328659\n",
      "Subject 15, Epoch 700, Loss: 1.0627425611019135, Final Batch Loss: 0.30091536045074463\n",
      "Subject 15, Epoch 701, Loss: 1.099014088511467, Final Batch Loss: 0.27483686804771423\n",
      "Subject 15, Epoch 702, Loss: 1.2341115772724152, Final Batch Loss: 0.39524126052856445\n",
      "Subject 15, Epoch 703, Loss: 1.1761474013328552, Final Batch Loss: 0.2693832218647003\n",
      "Subject 15, Epoch 704, Loss: 1.2329963147640228, Final Batch Loss: 0.2871539890766144\n",
      "Subject 15, Epoch 705, Loss: 1.1403108090162277, Final Batch Loss: 0.24125991761684418\n",
      "Subject 15, Epoch 706, Loss: 1.488095611333847, Final Batch Loss: 0.5101925134658813\n",
      "Subject 15, Epoch 707, Loss: 1.208723559975624, Final Batch Loss: 0.3293032646179199\n",
      "Subject 15, Epoch 708, Loss: 1.1568090170621872, Final Batch Loss: 0.3600102663040161\n",
      "Subject 15, Epoch 709, Loss: 1.2247865498065948, Final Batch Loss: 0.3431498408317566\n",
      "Subject 15, Epoch 710, Loss: 1.1376637369394302, Final Batch Loss: 0.24109862744808197\n",
      "Subject 15, Epoch 711, Loss: 1.070093259215355, Final Batch Loss: 0.20653276145458221\n",
      "Subject 15, Epoch 712, Loss: 1.2121160626411438, Final Batch Loss: 0.33616071939468384\n",
      "Subject 15, Epoch 713, Loss: 1.0902864784002304, Final Batch Loss: 0.3045540750026703\n",
      "Subject 15, Epoch 714, Loss: 1.0857650637626648, Final Batch Loss: 0.20527246594429016\n",
      "Subject 15, Epoch 715, Loss: 1.231993392109871, Final Batch Loss: 0.41360676288604736\n",
      "Subject 15, Epoch 716, Loss: 1.1052789390087128, Final Batch Loss: 0.30173638463020325\n",
      "Subject 15, Epoch 717, Loss: 1.091170608997345, Final Batch Loss: 0.27193892002105713\n",
      "Subject 15, Epoch 718, Loss: 1.120875135064125, Final Batch Loss: 0.26651519536972046\n",
      "Subject 15, Epoch 719, Loss: 1.2120145857334137, Final Batch Loss: 0.2933046519756317\n",
      "Subject 15, Epoch 720, Loss: 1.1039704531431198, Final Batch Loss: 0.2633115351200104\n",
      "Subject 15, Epoch 721, Loss: 1.076240986585617, Final Batch Loss: 0.25705990195274353\n",
      "Subject 15, Epoch 722, Loss: 1.1059822887182236, Final Batch Loss: 0.2743091285228729\n",
      "Subject 15, Epoch 723, Loss: 1.1421648859977722, Final Batch Loss: 0.30099549889564514\n",
      "Subject 15, Epoch 724, Loss: 1.1160401403903961, Final Batch Loss: 0.20268326997756958\n",
      "Subject 15, Epoch 725, Loss: 1.02961765229702, Final Batch Loss: 0.18089456856250763\n",
      "Subject 15, Epoch 726, Loss: 1.0950156450271606, Final Batch Loss: 0.22960886359214783\n",
      "Subject 15, Epoch 727, Loss: 1.1335219740867615, Final Batch Loss: 0.2340107560157776\n",
      "Subject 15, Epoch 728, Loss: 1.3904299139976501, Final Batch Loss: 0.4192279577255249\n",
      "Subject 15, Epoch 729, Loss: 1.0771323889493942, Final Batch Loss: 0.3494383990764618\n",
      "Subject 15, Epoch 730, Loss: 1.2882995009422302, Final Batch Loss: 0.31574851274490356\n",
      "Subject 15, Epoch 731, Loss: 1.0938621759414673, Final Batch Loss: 0.309224396944046\n",
      "Subject 15, Epoch 732, Loss: 1.145626276731491, Final Batch Loss: 0.34185102581977844\n",
      "Subject 15, Epoch 733, Loss: 1.1928500533103943, Final Batch Loss: 0.30833277106285095\n",
      "Subject 15, Epoch 734, Loss: 1.279736965894699, Final Batch Loss: 0.25958970189094543\n",
      "Subject 15, Epoch 735, Loss: 1.2237712889909744, Final Batch Loss: 0.2294818013906479\n",
      "Subject 15, Epoch 736, Loss: 1.1187067478895187, Final Batch Loss: 0.21174290776252747\n",
      "Subject 15, Epoch 737, Loss: 1.133712813258171, Final Batch Loss: 0.23059649765491486\n",
      "Subject 15, Epoch 738, Loss: 1.047176405787468, Final Batch Loss: 0.1976621150970459\n",
      "Subject 15, Epoch 739, Loss: 1.1791567206382751, Final Batch Loss: 0.25306951999664307\n",
      "Subject 15, Epoch 740, Loss: 1.2043628841638565, Final Batch Loss: 0.1961478739976883\n",
      "Subject 15, Epoch 741, Loss: 1.1788060665130615, Final Batch Loss: 0.2876655161380768\n",
      "Subject 15, Epoch 742, Loss: 1.154752790927887, Final Batch Loss: 0.3167174458503723\n",
      "Subject 15, Epoch 743, Loss: 1.2473176717758179, Final Batch Loss: 0.28869467973709106\n",
      "Subject 15, Epoch 744, Loss: 1.0328541547060013, Final Batch Loss: 0.1930263787508011\n",
      "Subject 15, Epoch 745, Loss: 1.3133175671100616, Final Batch Loss: 0.35899901390075684\n",
      "Subject 15, Epoch 746, Loss: 1.053049311041832, Final Batch Loss: 0.26473376154899597\n",
      "Subject 15, Epoch 747, Loss: 0.9970280528068542, Final Batch Loss: 0.1909005343914032\n",
      "Subject 15, Epoch 748, Loss: 1.024258479475975, Final Batch Loss: 0.22398899495601654\n",
      "Subject 15, Epoch 749, Loss: 0.9771522879600525, Final Batch Loss: 0.2038206309080124\n",
      "Subject 15, Epoch 750, Loss: 1.16044682264328, Final Batch Loss: 0.2996646761894226\n",
      "Subject 15, Epoch 751, Loss: 1.1077541410923004, Final Batch Loss: 0.3181244432926178\n",
      "Subject 15, Epoch 752, Loss: 1.2270303070545197, Final Batch Loss: 0.4321517050266266\n",
      "Subject 15, Epoch 753, Loss: 0.986883282661438, Final Batch Loss: 0.22961117327213287\n",
      "Subject 15, Epoch 754, Loss: 1.107407659292221, Final Batch Loss: 0.34230390191078186\n",
      "Subject 15, Epoch 755, Loss: 1.1660728007555008, Final Batch Loss: 0.38878005743026733\n",
      "Subject 15, Epoch 756, Loss: 1.0356579273939133, Final Batch Loss: 0.26446524262428284\n",
      "Subject 15, Epoch 757, Loss: 1.0289378464221954, Final Batch Loss: 0.21994276344776154\n",
      "Subject 15, Epoch 758, Loss: 1.167105257511139, Final Batch Loss: 0.31282299757003784\n",
      "Subject 15, Epoch 759, Loss: 1.0393612384796143, Final Batch Loss: 0.1481092870235443\n",
      "Subject 15, Epoch 760, Loss: 1.130563735961914, Final Batch Loss: 0.3310297429561615\n",
      "Subject 15, Epoch 761, Loss: 1.0590525716543198, Final Batch Loss: 0.21756131947040558\n",
      "Subject 15, Epoch 762, Loss: 1.2394806146621704, Final Batch Loss: 0.30776315927505493\n",
      "Subject 15, Epoch 763, Loss: 1.0790314078330994, Final Batch Loss: 0.28832265734672546\n",
      "Subject 15, Epoch 764, Loss: 1.2055112272500992, Final Batch Loss: 0.38142597675323486\n",
      "Subject 15, Epoch 765, Loss: 1.0866985470056534, Final Batch Loss: 0.2917677164077759\n",
      "Subject 15, Epoch 766, Loss: 1.0660288035869598, Final Batch Loss: 0.24011683464050293\n",
      "Subject 15, Epoch 767, Loss: 1.1141821593046188, Final Batch Loss: 0.30886563658714294\n",
      "Subject 15, Epoch 768, Loss: 1.033753678202629, Final Batch Loss: 0.269481360912323\n",
      "Subject 15, Epoch 769, Loss: 1.0861615985631943, Final Batch Loss: 0.3017895817756653\n",
      "Subject 15, Epoch 770, Loss: 1.1275171786546707, Final Batch Loss: 0.353331059217453\n",
      "Subject 15, Epoch 771, Loss: 1.046561062335968, Final Batch Loss: 0.2888658940792084\n",
      "Subject 15, Epoch 772, Loss: 1.060171514749527, Final Batch Loss: 0.2712809443473816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 773, Loss: 1.1439860463142395, Final Batch Loss: 0.3342033624649048\n",
      "Subject 15, Epoch 774, Loss: 0.9969964325428009, Final Batch Loss: 0.28963133692741394\n",
      "Subject 15, Epoch 775, Loss: 1.059333711862564, Final Batch Loss: 0.2653070092201233\n",
      "Subject 15, Epoch 776, Loss: 1.0702053010463715, Final Batch Loss: 0.20065000653266907\n",
      "Subject 15, Epoch 777, Loss: 0.9603000432252884, Final Batch Loss: 0.1913716197013855\n",
      "Subject 15, Epoch 778, Loss: 0.9955349117517471, Final Batch Loss: 0.18758688867092133\n",
      "Subject 15, Epoch 779, Loss: 1.028974175453186, Final Batch Loss: 0.27277645468711853\n",
      "Subject 15, Epoch 780, Loss: 0.8950231522321701, Final Batch Loss: 0.1513095647096634\n",
      "Subject 15, Epoch 781, Loss: 1.0585914701223373, Final Batch Loss: 0.2662675082683563\n",
      "Subject 15, Epoch 782, Loss: 1.021251156926155, Final Batch Loss: 0.16252847015857697\n",
      "Subject 15, Epoch 783, Loss: 1.0743160992860794, Final Batch Loss: 0.3378607928752899\n",
      "Subject 15, Epoch 784, Loss: 1.0053704977035522, Final Batch Loss: 0.2743099629878998\n",
      "Subject 15, Epoch 785, Loss: 1.1481656581163406, Final Batch Loss: 0.22326244413852692\n",
      "Subject 15, Epoch 786, Loss: 1.098758190870285, Final Batch Loss: 0.36937788128852844\n",
      "Subject 15, Epoch 787, Loss: 1.0076511353254318, Final Batch Loss: 0.22933191061019897\n",
      "Subject 15, Epoch 788, Loss: 1.049128383398056, Final Batch Loss: 0.22225508093833923\n",
      "Subject 15, Epoch 789, Loss: 1.1291543543338776, Final Batch Loss: 0.28318968415260315\n",
      "Subject 15, Epoch 790, Loss: 1.017709419131279, Final Batch Loss: 0.28734704852104187\n",
      "Subject 15, Epoch 791, Loss: 1.1728734970092773, Final Batch Loss: 0.25302204489707947\n",
      "Subject 15, Epoch 792, Loss: 1.3306919038295746, Final Batch Loss: 0.4855140447616577\n",
      "Subject 15, Epoch 793, Loss: 0.9867234528064728, Final Batch Loss: 0.22770443558692932\n",
      "Subject 15, Epoch 794, Loss: 0.993291512131691, Final Batch Loss: 0.15142612159252167\n",
      "Subject 15, Epoch 795, Loss: 1.0917811840772629, Final Batch Loss: 0.3262614607810974\n",
      "Subject 15, Epoch 796, Loss: 1.1294782012701035, Final Batch Loss: 0.22704078257083893\n",
      "Subject 15, Epoch 797, Loss: 0.9694322198629379, Final Batch Loss: 0.21257221698760986\n",
      "Subject 15, Epoch 798, Loss: 1.1676735281944275, Final Batch Loss: 0.2536293864250183\n",
      "Subject 15, Epoch 799, Loss: 1.0673778504133224, Final Batch Loss: 0.22666817903518677\n",
      "Subject 15, Epoch 800, Loss: 1.0549619495868683, Final Batch Loss: 0.17976190149784088\n",
      "Subject 15, Epoch 801, Loss: 1.0161996334791183, Final Batch Loss: 0.2269611805677414\n",
      "Subject 15, Epoch 802, Loss: 1.189481571316719, Final Batch Loss: 0.21881110966205597\n",
      "Subject 15, Epoch 803, Loss: 1.0698009133338928, Final Batch Loss: 0.22831127047538757\n",
      "Subject 15, Epoch 804, Loss: 1.0910584777593613, Final Batch Loss: 0.31184542179107666\n",
      "Subject 15, Epoch 805, Loss: 0.9663439840078354, Final Batch Loss: 0.20549499988555908\n",
      "Subject 15, Epoch 806, Loss: 1.2190026938915253, Final Batch Loss: 0.4263128638267517\n",
      "Subject 15, Epoch 807, Loss: 0.9978436380624771, Final Batch Loss: 0.21408168971538544\n",
      "Subject 15, Epoch 808, Loss: 0.9141390174627304, Final Batch Loss: 0.17970143258571625\n",
      "Subject 15, Epoch 809, Loss: 1.0481746792793274, Final Batch Loss: 0.28316810727119446\n",
      "Subject 15, Epoch 810, Loss: 1.07065749168396, Final Batch Loss: 0.24267533421516418\n",
      "Subject 15, Epoch 811, Loss: 1.1413084268569946, Final Batch Loss: 0.28256955742836\n",
      "Subject 15, Epoch 812, Loss: 1.0925879627466202, Final Batch Loss: 0.37521860003471375\n",
      "Subject 15, Epoch 813, Loss: 1.1290592849254608, Final Batch Loss: 0.21203294396400452\n",
      "Subject 15, Epoch 814, Loss: 0.970704197883606, Final Batch Loss: 0.20877057313919067\n",
      "Subject 15, Epoch 815, Loss: 1.111171305179596, Final Batch Loss: 0.3502616882324219\n",
      "Subject 15, Epoch 816, Loss: 1.1496785879135132, Final Batch Loss: 0.2944498062133789\n",
      "Subject 15, Epoch 817, Loss: 1.1270544081926346, Final Batch Loss: 0.30579596757888794\n",
      "Subject 15, Epoch 818, Loss: 1.2452597320079803, Final Batch Loss: 0.3893721103668213\n",
      "Subject 15, Epoch 819, Loss: 0.9900925308465958, Final Batch Loss: 0.1602255254983902\n",
      "Subject 15, Epoch 820, Loss: 1.2170651406049728, Final Batch Loss: 0.32250040769577026\n",
      "Subject 15, Epoch 821, Loss: 1.0828082859516144, Final Batch Loss: 0.27199462056159973\n",
      "Subject 15, Epoch 822, Loss: 0.9693160653114319, Final Batch Loss: 0.23154689371585846\n",
      "Subject 15, Epoch 823, Loss: 1.0634393990039825, Final Batch Loss: 0.2973751425743103\n",
      "Subject 15, Epoch 824, Loss: 1.1522263288497925, Final Batch Loss: 0.306596577167511\n",
      "Subject 15, Epoch 825, Loss: 1.1535832583904266, Final Batch Loss: 0.3507586121559143\n",
      "Subject 15, Epoch 826, Loss: 1.019333764910698, Final Batch Loss: 0.2289973795413971\n",
      "Subject 15, Epoch 827, Loss: 1.0379498898983002, Final Batch Loss: 0.26407918334007263\n",
      "Subject 15, Epoch 828, Loss: 0.9543393701314926, Final Batch Loss: 0.22603948414325714\n",
      "Subject 15, Epoch 829, Loss: 1.0822169035673141, Final Batch Loss: 0.24866585433483124\n",
      "Subject 15, Epoch 830, Loss: 1.1412281692028046, Final Batch Loss: 0.25692638754844666\n",
      "Subject 15, Epoch 831, Loss: 0.9270916730165482, Final Batch Loss: 0.21387162804603577\n",
      "Subject 15, Epoch 832, Loss: 1.040290281176567, Final Batch Loss: 0.21473099291324615\n",
      "Subject 15, Epoch 833, Loss: 0.8562193438410759, Final Batch Loss: 0.11961669474840164\n",
      "Subject 15, Epoch 834, Loss: 1.1477416902780533, Final Batch Loss: 0.3429005742073059\n",
      "Subject 15, Epoch 835, Loss: 0.9895974844694138, Final Batch Loss: 0.1733424812555313\n",
      "Subject 15, Epoch 836, Loss: 1.1381827294826508, Final Batch Loss: 0.22495421767234802\n",
      "Subject 15, Epoch 837, Loss: 1.087347373366356, Final Batch Loss: 0.24973982572555542\n",
      "Subject 15, Epoch 838, Loss: 1.0023246854543686, Final Batch Loss: 0.23274588584899902\n",
      "Subject 15, Epoch 839, Loss: 1.057109773159027, Final Batch Loss: 0.1966167390346527\n",
      "Subject 15, Epoch 840, Loss: 1.111290067434311, Final Batch Loss: 0.24752134084701538\n",
      "Subject 15, Epoch 841, Loss: 0.9635935127735138, Final Batch Loss: 0.2109825611114502\n",
      "Subject 15, Epoch 842, Loss: 1.0646255761384964, Final Batch Loss: 0.21470282971858978\n",
      "Subject 15, Epoch 843, Loss: 1.0282351523637772, Final Batch Loss: 0.19402636587619781\n",
      "Subject 15, Epoch 844, Loss: 0.9910924434661865, Final Batch Loss: 0.2419891208410263\n",
      "Subject 15, Epoch 845, Loss: 1.007115587592125, Final Batch Loss: 0.2767356336116791\n",
      "Subject 15, Epoch 846, Loss: 1.2361287772655487, Final Batch Loss: 0.2860734462738037\n",
      "Subject 15, Epoch 847, Loss: 1.1511427760124207, Final Batch Loss: 0.2507634162902832\n",
      "Subject 15, Epoch 848, Loss: 1.1197360306978226, Final Batch Loss: 0.3515934348106384\n",
      "Subject 15, Epoch 849, Loss: 0.9053655415773392, Final Batch Loss: 0.2177019864320755\n",
      "Subject 15, Epoch 850, Loss: 1.0146970599889755, Final Batch Loss: 0.16903594136238098\n",
      "Subject 15, Epoch 851, Loss: 1.061359390616417, Final Batch Loss: 0.3124493956565857\n",
      "Subject 15, Epoch 852, Loss: 1.0382212698459625, Final Batch Loss: 0.2758537828922272\n",
      "Subject 15, Epoch 853, Loss: 0.9502655416727066, Final Batch Loss: 0.13846856355667114\n",
      "Subject 15, Epoch 854, Loss: 0.9749899208545685, Final Batch Loss: 0.1677360087633133\n",
      "Subject 15, Epoch 855, Loss: 1.0827065855264664, Final Batch Loss: 0.17256690561771393\n",
      "Subject 15, Epoch 856, Loss: 0.9549922943115234, Final Batch Loss: 0.20958928763866425\n",
      "Subject 15, Epoch 857, Loss: 0.9778546988964081, Final Batch Loss: 0.29175251722335815\n",
      "Subject 15, Epoch 858, Loss: 1.0376245081424713, Final Batch Loss: 0.2616157829761505\n",
      "Subject 15, Epoch 859, Loss: 1.0221883952617645, Final Batch Loss: 0.26187849044799805\n",
      "Subject 15, Epoch 860, Loss: 0.983794167637825, Final Batch Loss: 0.2199174165725708\n",
      "Subject 15, Epoch 861, Loss: 1.1029356718063354, Final Batch Loss: 0.40389037132263184\n",
      "Subject 15, Epoch 862, Loss: 0.9640632718801498, Final Batch Loss: 0.19279217720031738\n",
      "Subject 15, Epoch 863, Loss: 1.1563935428857803, Final Batch Loss: 0.3394777476787567\n",
      "Subject 15, Epoch 864, Loss: 0.9971030801534653, Final Batch Loss: 0.18291932344436646\n",
      "Subject 15, Epoch 865, Loss: 1.0493607223033905, Final Batch Loss: 0.3392931818962097\n",
      "Subject 15, Epoch 866, Loss: 1.2597284615039825, Final Batch Loss: 0.28987598419189453\n",
      "Subject 15, Epoch 867, Loss: 1.1892841160297394, Final Batch Loss: 0.4347749352455139\n",
      "Subject 15, Epoch 868, Loss: 0.9961130768060684, Final Batch Loss: 0.26452749967575073\n",
      "Subject 15, Epoch 869, Loss: 1.0443979352712631, Final Batch Loss: 0.2908707559108734\n",
      "Subject 15, Epoch 870, Loss: 1.2795265018939972, Final Batch Loss: 0.3286159932613373\n",
      "Subject 15, Epoch 871, Loss: 0.9446843862533569, Final Batch Loss: 0.14697639644145966\n",
      "Subject 15, Epoch 872, Loss: 0.9804191887378693, Final Batch Loss: 0.18522940576076508\n",
      "Subject 15, Epoch 873, Loss: 1.0734412223100662, Final Batch Loss: 0.2719162702560425\n",
      "Subject 15, Epoch 874, Loss: 1.134617105126381, Final Batch Loss: 0.3026788532733917\n",
      "Subject 15, Epoch 875, Loss: 0.9088568091392517, Final Batch Loss: 0.2252979725599289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 876, Loss: 0.9243573993444443, Final Batch Loss: 0.22060410678386688\n",
      "Subject 15, Epoch 877, Loss: 1.048586905002594, Final Batch Loss: 0.3034825921058655\n",
      "Subject 15, Epoch 878, Loss: 0.9968410581350327, Final Batch Loss: 0.3202977776527405\n",
      "Subject 15, Epoch 879, Loss: 0.9835532456636429, Final Batch Loss: 0.3158899247646332\n",
      "Subject 15, Epoch 880, Loss: 1.1550646126270294, Final Batch Loss: 0.3550948202610016\n",
      "Subject 15, Epoch 881, Loss: 0.9439506381750107, Final Batch Loss: 0.18327374756336212\n",
      "Subject 15, Epoch 882, Loss: 1.0758795589208603, Final Batch Loss: 0.3358030617237091\n",
      "Subject 15, Epoch 883, Loss: 1.0841828882694244, Final Batch Loss: 0.3149069845676422\n",
      "Subject 15, Epoch 884, Loss: 1.128437951207161, Final Batch Loss: 0.3367486596107483\n",
      "Subject 15, Epoch 885, Loss: 1.033912718296051, Final Batch Loss: 0.3075854778289795\n",
      "Subject 15, Epoch 886, Loss: 1.022199109196663, Final Batch Loss: 0.273619681596756\n",
      "Subject 15, Epoch 887, Loss: 0.8888309150934219, Final Batch Loss: 0.20340654253959656\n",
      "Subject 15, Epoch 888, Loss: 1.2306453585624695, Final Batch Loss: 0.35822075605392456\n",
      "Subject 15, Epoch 889, Loss: 0.9612783938646317, Final Batch Loss: 0.2337358146905899\n",
      "Subject 15, Epoch 890, Loss: 1.014642372727394, Final Batch Loss: 0.2560725808143616\n",
      "Subject 15, Epoch 891, Loss: 0.9410948902368546, Final Batch Loss: 0.17604826390743256\n",
      "Subject 15, Epoch 892, Loss: 1.1528867185115814, Final Batch Loss: 0.26760393381118774\n",
      "Subject 15, Epoch 893, Loss: 0.9846137464046478, Final Batch Loss: 0.24742507934570312\n",
      "Subject 15, Epoch 894, Loss: 0.9934362173080444, Final Batch Loss: 0.22512945532798767\n",
      "Subject 15, Epoch 895, Loss: 0.952368825674057, Final Batch Loss: 0.24762682616710663\n",
      "Subject 15, Epoch 896, Loss: 0.9555326551198959, Final Batch Loss: 0.17396153509616852\n",
      "Subject 15, Epoch 897, Loss: 0.9157940000295639, Final Batch Loss: 0.1270042210817337\n",
      "Subject 15, Epoch 898, Loss: 1.070884793996811, Final Batch Loss: 0.28594133257865906\n",
      "Subject 15, Epoch 899, Loss: 0.9702596813440323, Final Batch Loss: 0.18649399280548096\n",
      "Subject 15, Epoch 900, Loss: 1.060543268918991, Final Batch Loss: 0.1275174617767334\n",
      "Subject 15, Epoch 901, Loss: 1.0796044617891312, Final Batch Loss: 0.19307361543178558\n",
      "Subject 15, Epoch 902, Loss: 1.1363715082406998, Final Batch Loss: 0.3986556828022003\n",
      "Subject 15, Epoch 903, Loss: 1.044426441192627, Final Batch Loss: 0.253639817237854\n",
      "Subject 15, Epoch 904, Loss: 1.1142893731594086, Final Batch Loss: 0.22284603118896484\n",
      "Subject 15, Epoch 905, Loss: 1.112451210618019, Final Batch Loss: 0.33053189516067505\n",
      "Subject 15, Epoch 906, Loss: 0.9307045191526413, Final Batch Loss: 0.1395825892686844\n",
      "Subject 15, Epoch 907, Loss: 1.0872002691030502, Final Batch Loss: 0.27366164326667786\n",
      "Subject 15, Epoch 908, Loss: 1.0093298703432083, Final Batch Loss: 0.28913813829421997\n",
      "Subject 15, Epoch 909, Loss: 0.8776979297399521, Final Batch Loss: 0.10547792911529541\n",
      "Subject 15, Epoch 910, Loss: 1.1298730075359344, Final Batch Loss: 0.21265894174575806\n",
      "Subject 15, Epoch 911, Loss: 0.8885429352521896, Final Batch Loss: 0.1803496778011322\n",
      "Subject 15, Epoch 912, Loss: 0.983334869146347, Final Batch Loss: 0.2571239173412323\n",
      "Subject 15, Epoch 913, Loss: 1.0959003269672394, Final Batch Loss: 0.2679883539676666\n",
      "Subject 15, Epoch 914, Loss: 1.0379681587219238, Final Batch Loss: 0.20287767052650452\n",
      "Subject 15, Epoch 915, Loss: 0.9365389049053192, Final Batch Loss: 0.1726756989955902\n",
      "Subject 15, Epoch 916, Loss: 0.8872284293174744, Final Batch Loss: 0.1930008977651596\n",
      "Subject 15, Epoch 917, Loss: 1.0056313127279282, Final Batch Loss: 0.25444427132606506\n",
      "Subject 15, Epoch 918, Loss: 0.9571375548839569, Final Batch Loss: 0.2262997180223465\n",
      "Subject 15, Epoch 919, Loss: 0.9834719747304916, Final Batch Loss: 0.21554772555828094\n",
      "Subject 15, Epoch 920, Loss: 0.8524491488933563, Final Batch Loss: 0.1568077951669693\n",
      "Subject 15, Epoch 921, Loss: 0.9871442019939423, Final Batch Loss: 0.27997133135795593\n",
      "Subject 15, Epoch 922, Loss: 1.0178916603326797, Final Batch Loss: 0.18820758163928986\n",
      "Subject 15, Epoch 923, Loss: 1.079211875796318, Final Batch Loss: 0.23324643075466156\n",
      "Subject 15, Epoch 924, Loss: 1.1740278601646423, Final Batch Loss: 0.31711217761039734\n",
      "Subject 15, Epoch 925, Loss: 0.8716483563184738, Final Batch Loss: 0.17628031969070435\n",
      "Subject 15, Epoch 926, Loss: 0.852249026298523, Final Batch Loss: 0.24315254390239716\n",
      "Subject 15, Epoch 927, Loss: 0.9779717773199081, Final Batch Loss: 0.28933387994766235\n",
      "Subject 15, Epoch 928, Loss: 0.9698813259601593, Final Batch Loss: 0.28527185320854187\n",
      "Subject 15, Epoch 929, Loss: 0.9697452932596207, Final Batch Loss: 0.18345490097999573\n",
      "Subject 15, Epoch 930, Loss: 0.9573958963155746, Final Batch Loss: 0.2591764032840729\n",
      "Subject 15, Epoch 931, Loss: 1.0061988979578018, Final Batch Loss: 0.3669517934322357\n",
      "Subject 15, Epoch 932, Loss: 0.9238013029098511, Final Batch Loss: 0.24884475767612457\n",
      "Subject 15, Epoch 933, Loss: 1.0146050602197647, Final Batch Loss: 0.25293606519699097\n",
      "Subject 15, Epoch 934, Loss: 1.0491642951965332, Final Batch Loss: 0.334437757730484\n",
      "Subject 15, Epoch 935, Loss: 0.9255982339382172, Final Batch Loss: 0.2583938539028168\n",
      "Subject 15, Epoch 936, Loss: 1.004970759153366, Final Batch Loss: 0.23991210758686066\n",
      "Subject 15, Epoch 937, Loss: 0.9437927752733231, Final Batch Loss: 0.2082466185092926\n",
      "Subject 15, Epoch 938, Loss: 0.921411395072937, Final Batch Loss: 0.14861014485359192\n",
      "Subject 15, Epoch 939, Loss: 1.286728948354721, Final Batch Loss: 0.4501732289791107\n",
      "Subject 15, Epoch 940, Loss: 1.00127412378788, Final Batch Loss: 0.3052031397819519\n",
      "Subject 15, Epoch 941, Loss: 1.1012665033340454, Final Batch Loss: 0.35191231966018677\n",
      "Subject 15, Epoch 942, Loss: 0.9661348760128021, Final Batch Loss: 0.23323093354701996\n",
      "Subject 15, Epoch 943, Loss: 0.8526139706373215, Final Batch Loss: 0.20288227498531342\n",
      "Subject 15, Epoch 944, Loss: 1.1492060124874115, Final Batch Loss: 0.21835957467556\n",
      "Subject 15, Epoch 945, Loss: 1.056564286351204, Final Batch Loss: 0.23616275191307068\n",
      "Subject 15, Epoch 946, Loss: 0.9708156287670135, Final Batch Loss: 0.2648357152938843\n",
      "Subject 15, Epoch 947, Loss: 0.8621879070997238, Final Batch Loss: 0.1970057338476181\n",
      "Subject 15, Epoch 948, Loss: 0.9242735952138901, Final Batch Loss: 0.27589336037635803\n",
      "Subject 15, Epoch 949, Loss: 0.9277893751859665, Final Batch Loss: 0.20150767266750336\n",
      "Subject 15, Epoch 950, Loss: 0.9813038557767868, Final Batch Loss: 0.26300567388534546\n",
      "Subject 15, Epoch 951, Loss: 1.0052801072597504, Final Batch Loss: 0.2672038972377777\n",
      "Subject 15, Epoch 952, Loss: 0.821392834186554, Final Batch Loss: 0.15422073006629944\n",
      "Subject 15, Epoch 953, Loss: 1.0367074012756348, Final Batch Loss: 0.307614803314209\n",
      "Subject 15, Epoch 954, Loss: 0.921335831284523, Final Batch Loss: 0.23715586960315704\n",
      "Subject 15, Epoch 955, Loss: 0.9869491457939148, Final Batch Loss: 0.24251164495944977\n",
      "Subject 15, Epoch 956, Loss: 0.8850279748439789, Final Batch Loss: 0.25083303451538086\n",
      "Subject 15, Epoch 957, Loss: 0.7758276760578156, Final Batch Loss: 0.11650283634662628\n",
      "Subject 15, Epoch 958, Loss: 0.9787811040878296, Final Batch Loss: 0.2012958526611328\n",
      "Subject 15, Epoch 959, Loss: 1.0109051316976547, Final Batch Loss: 0.3383291959762573\n",
      "Subject 15, Epoch 960, Loss: 0.8710006028413773, Final Batch Loss: 0.20725227892398834\n",
      "Subject 15, Epoch 961, Loss: 0.9764230847358704, Final Batch Loss: 0.2149881273508072\n",
      "Subject 15, Epoch 962, Loss: 0.937449648976326, Final Batch Loss: 0.15398693084716797\n",
      "Subject 15, Epoch 963, Loss: 0.969108834862709, Final Batch Loss: 0.2637002766132355\n",
      "Subject 15, Epoch 964, Loss: 1.0289510786533356, Final Batch Loss: 0.30333998799324036\n",
      "Subject 15, Epoch 965, Loss: 0.8828301429748535, Final Batch Loss: 0.12277813255786896\n",
      "Subject 15, Epoch 966, Loss: 0.8509916663169861, Final Batch Loss: 0.12744075059890747\n",
      "Subject 15, Epoch 967, Loss: 0.9072143882513046, Final Batch Loss: 0.2148941308259964\n",
      "Subject 15, Epoch 968, Loss: 0.8232434391975403, Final Batch Loss: 0.1601448953151703\n",
      "Subject 15, Epoch 969, Loss: 0.9673303812742233, Final Batch Loss: 0.2101554125547409\n",
      "Subject 15, Epoch 970, Loss: 0.9412627220153809, Final Batch Loss: 0.2441331148147583\n",
      "Subject 15, Epoch 971, Loss: 0.9330444633960724, Final Batch Loss: 0.2517322301864624\n",
      "Subject 15, Epoch 972, Loss: 0.96616031229496, Final Batch Loss: 0.2834559381008148\n",
      "Subject 15, Epoch 973, Loss: 1.1297989338636398, Final Batch Loss: 0.2738070487976074\n",
      "Subject 15, Epoch 974, Loss: 1.1741034537553787, Final Batch Loss: 0.46795937418937683\n",
      "Subject 15, Epoch 975, Loss: 0.9041556566953659, Final Batch Loss: 0.2418559491634369\n",
      "Subject 15, Epoch 976, Loss: 0.9134580194950104, Final Batch Loss: 0.2087249904870987\n",
      "Subject 15, Epoch 977, Loss: 0.8547559678554535, Final Batch Loss: 0.1702994555234909\n",
      "Subject 15, Epoch 978, Loss: 0.9043120741844177, Final Batch Loss: 0.18144993484020233\n",
      "Subject 15, Epoch 979, Loss: 0.9860710799694061, Final Batch Loss: 0.31574079394340515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15, Epoch 980, Loss: 1.0511134415864944, Final Batch Loss: 0.24354441463947296\n",
      "Subject 15, Epoch 981, Loss: 0.9641537368297577, Final Batch Loss: 0.2352921962738037\n",
      "Subject 15, Epoch 982, Loss: 1.1899226605892181, Final Batch Loss: 0.31963178515434265\n",
      "Subject 15, Epoch 983, Loss: 0.9080388396978378, Final Batch Loss: 0.18534961342811584\n",
      "Subject 15, Epoch 984, Loss: 0.8415659070014954, Final Batch Loss: 0.12641164660453796\n",
      "Subject 15, Epoch 985, Loss: 0.9411471337080002, Final Batch Loss: 0.2780073583126068\n",
      "Subject 15, Epoch 986, Loss: 0.8767105937004089, Final Batch Loss: 0.15256793797016144\n",
      "Subject 15, Epoch 987, Loss: 0.8616982996463776, Final Batch Loss: 0.19746091961860657\n",
      "Subject 15, Epoch 988, Loss: 1.0265117287635803, Final Batch Loss: 0.3199269473552704\n",
      "Subject 15, Epoch 989, Loss: 0.7783755958080292, Final Batch Loss: 0.16615734994411469\n",
      "Subject 15, Epoch 990, Loss: 0.9125049263238907, Final Batch Loss: 0.20747099816799164\n",
      "Subject 15, Epoch 991, Loss: 0.9961145520210266, Final Batch Loss: 0.2570137083530426\n",
      "Subject 15, Epoch 992, Loss: 0.8674553632736206, Final Batch Loss: 0.15688464045524597\n",
      "Subject 15, Epoch 993, Loss: 0.9348073154687881, Final Batch Loss: 0.26135337352752686\n",
      "Subject 15, Epoch 994, Loss: 0.8666902035474777, Final Batch Loss: 0.14486470818519592\n",
      "Subject 15, Epoch 995, Loss: 0.9252900630235672, Final Batch Loss: 0.27717503905296326\n",
      "Subject 15, Epoch 996, Loss: 1.0081261098384857, Final Batch Loss: 0.28725284337997437\n",
      "Subject 15, Epoch 997, Loss: 0.9704032689332962, Final Batch Loss: 0.2147088497877121\n",
      "Subject 15, Epoch 998, Loss: 0.9741549491882324, Final Batch Loss: 0.22295379638671875\n",
      "Subject 15, Epoch 999, Loss: 1.009479969739914, Final Batch Loss: 0.22275997698307037\n",
      "Subject 15, Epoch 1000, Loss: 0.88376684486866, Final Batch Loss: 0.25582948327064514\n",
      "Subject 16, Epoch 1, Loss: 7.22247052192688, Final Batch Loss: 1.785908579826355\n",
      "Subject 16, Epoch 2, Loss: 7.2394901514053345, Final Batch Loss: 1.8410804271697998\n",
      "Subject 16, Epoch 3, Loss: 7.207439303398132, Final Batch Loss: 1.8036140203475952\n",
      "Subject 16, Epoch 4, Loss: 7.179279804229736, Final Batch Loss: 1.7854195833206177\n",
      "Subject 16, Epoch 5, Loss: 7.166107773780823, Final Batch Loss: 1.7805919647216797\n",
      "Subject 16, Epoch 6, Loss: 7.1630213260650635, Final Batch Loss: 1.7897183895111084\n",
      "Subject 16, Epoch 7, Loss: 7.1406471729278564, Final Batch Loss: 1.7741718292236328\n",
      "Subject 16, Epoch 8, Loss: 7.142256259918213, Final Batch Loss: 1.7912486791610718\n",
      "Subject 16, Epoch 9, Loss: 7.091385006904602, Final Batch Loss: 1.7605732679367065\n",
      "Subject 16, Epoch 10, Loss: 7.086497068405151, Final Batch Loss: 1.7681254148483276\n",
      "Subject 16, Epoch 11, Loss: 7.059067130088806, Final Batch Loss: 1.7679001092910767\n",
      "Subject 16, Epoch 12, Loss: 6.987742781639099, Final Batch Loss: 1.719838261604309\n",
      "Subject 16, Epoch 13, Loss: 6.970531225204468, Final Batch Loss: 1.7429677248001099\n",
      "Subject 16, Epoch 14, Loss: 6.90375030040741, Final Batch Loss: 1.7111072540283203\n",
      "Subject 16, Epoch 15, Loss: 6.837719082832336, Final Batch Loss: 1.7220767736434937\n",
      "Subject 16, Epoch 16, Loss: 6.754370450973511, Final Batch Loss: 1.6792999505996704\n",
      "Subject 16, Epoch 17, Loss: 6.67002546787262, Final Batch Loss: 1.6576670408248901\n",
      "Subject 16, Epoch 18, Loss: 6.53115451335907, Final Batch Loss: 1.5969074964523315\n",
      "Subject 16, Epoch 19, Loss: 6.409236311912537, Final Batch Loss: 1.6519246101379395\n",
      "Subject 16, Epoch 20, Loss: 6.203237056732178, Final Batch Loss: 1.5510704517364502\n",
      "Subject 16, Epoch 21, Loss: 6.025121331214905, Final Batch Loss: 1.5004370212554932\n",
      "Subject 16, Epoch 22, Loss: 5.803060531616211, Final Batch Loss: 1.4505664110183716\n",
      "Subject 16, Epoch 23, Loss: 5.616403818130493, Final Batch Loss: 1.4003486633300781\n",
      "Subject 16, Epoch 24, Loss: 5.478066563606262, Final Batch Loss: 1.3551125526428223\n",
      "Subject 16, Epoch 25, Loss: 5.289044260978699, Final Batch Loss: 1.2774834632873535\n",
      "Subject 16, Epoch 26, Loss: 5.153208017349243, Final Batch Loss: 1.237141489982605\n",
      "Subject 16, Epoch 27, Loss: 4.869006633758545, Final Batch Loss: 1.2060863971710205\n",
      "Subject 16, Epoch 28, Loss: 4.960590720176697, Final Batch Loss: 1.1949068307876587\n",
      "Subject 16, Epoch 29, Loss: 4.754188060760498, Final Batch Loss: 1.0721849203109741\n",
      "Subject 16, Epoch 30, Loss: 4.8420844078063965, Final Batch Loss: 1.2783610820770264\n",
      "Subject 16, Epoch 31, Loss: 4.777953624725342, Final Batch Loss: 1.2317097187042236\n",
      "Subject 16, Epoch 32, Loss: 4.551682472229004, Final Batch Loss: 1.0950796604156494\n",
      "Subject 16, Epoch 33, Loss: 4.591835141181946, Final Batch Loss: 1.170304298400879\n",
      "Subject 16, Epoch 34, Loss: 4.455280661582947, Final Batch Loss: 1.0200624465942383\n",
      "Subject 16, Epoch 35, Loss: 4.466683626174927, Final Batch Loss: 1.1286084651947021\n",
      "Subject 16, Epoch 36, Loss: 4.506916165351868, Final Batch Loss: 1.170778512954712\n",
      "Subject 16, Epoch 37, Loss: 4.282248497009277, Final Batch Loss: 1.0346461534500122\n",
      "Subject 16, Epoch 38, Loss: 4.245182991027832, Final Batch Loss: 1.0887562036514282\n",
      "Subject 16, Epoch 39, Loss: 4.23697829246521, Final Batch Loss: 1.0446946620941162\n",
      "Subject 16, Epoch 40, Loss: 4.227538228034973, Final Batch Loss: 1.0154838562011719\n",
      "Subject 16, Epoch 41, Loss: 4.09376847743988, Final Batch Loss: 1.0842238664627075\n",
      "Subject 16, Epoch 42, Loss: 4.0841986536979675, Final Batch Loss: 1.0807890892028809\n",
      "Subject 16, Epoch 43, Loss: 4.090802013874054, Final Batch Loss: 1.0027285814285278\n",
      "Subject 16, Epoch 44, Loss: 4.1109296679496765, Final Batch Loss: 0.9924178719520569\n",
      "Subject 16, Epoch 45, Loss: 3.8360390663146973, Final Batch Loss: 0.8596028685569763\n",
      "Subject 16, Epoch 46, Loss: 3.9046773314476013, Final Batch Loss: 0.9092705249786377\n",
      "Subject 16, Epoch 47, Loss: 3.865551769733429, Final Batch Loss: 0.9352904558181763\n",
      "Subject 16, Epoch 48, Loss: 3.951357364654541, Final Batch Loss: 0.9762229919433594\n",
      "Subject 16, Epoch 49, Loss: 3.605120301246643, Final Batch Loss: 0.893477201461792\n",
      "Subject 16, Epoch 50, Loss: 3.638031840324402, Final Batch Loss: 0.8253552913665771\n",
      "Subject 16, Epoch 51, Loss: 3.7935503721237183, Final Batch Loss: 1.0075852870941162\n",
      "Subject 16, Epoch 52, Loss: 3.5508525371551514, Final Batch Loss: 0.9078210592269897\n",
      "Subject 16, Epoch 53, Loss: 3.686020612716675, Final Batch Loss: 0.8611798882484436\n",
      "Subject 16, Epoch 54, Loss: 3.3896331191062927, Final Batch Loss: 0.755092442035675\n",
      "Subject 16, Epoch 55, Loss: 3.618936836719513, Final Batch Loss: 0.9356034398078918\n",
      "Subject 16, Epoch 56, Loss: 3.4666666984558105, Final Batch Loss: 0.9962342977523804\n",
      "Subject 16, Epoch 57, Loss: 3.654936909675598, Final Batch Loss: 1.1388936042785645\n",
      "Subject 16, Epoch 58, Loss: 3.3677417039871216, Final Batch Loss: 0.8966277241706848\n",
      "Subject 16, Epoch 59, Loss: 3.311833441257477, Final Batch Loss: 0.8632332682609558\n",
      "Subject 16, Epoch 60, Loss: 3.308822751045227, Final Batch Loss: 0.7606018781661987\n",
      "Subject 16, Epoch 61, Loss: 3.135354220867157, Final Batch Loss: 0.7243837118148804\n",
      "Subject 16, Epoch 62, Loss: 3.119615375995636, Final Batch Loss: 0.7402880787849426\n",
      "Subject 16, Epoch 63, Loss: 3.0033676624298096, Final Batch Loss: 0.8124603033065796\n",
      "Subject 16, Epoch 64, Loss: 3.25113046169281, Final Batch Loss: 0.8441066741943359\n",
      "Subject 16, Epoch 65, Loss: 3.0338587760925293, Final Batch Loss: 0.7736286520957947\n",
      "Subject 16, Epoch 66, Loss: 3.0702022314071655, Final Batch Loss: 0.7570329904556274\n",
      "Subject 16, Epoch 67, Loss: 2.889953911304474, Final Batch Loss: 0.7683408856391907\n",
      "Subject 16, Epoch 68, Loss: 3.0640963912010193, Final Batch Loss: 0.7932136058807373\n",
      "Subject 16, Epoch 69, Loss: 3.018338203430176, Final Batch Loss: 0.8950176239013672\n",
      "Subject 16, Epoch 70, Loss: 2.9392720460891724, Final Batch Loss: 0.6287060976028442\n",
      "Subject 16, Epoch 71, Loss: 2.9751635789871216, Final Batch Loss: 0.8084850907325745\n",
      "Subject 16, Epoch 72, Loss: 2.8286327123641968, Final Batch Loss: 0.7487316131591797\n",
      "Subject 16, Epoch 73, Loss: 2.8890048265457153, Final Batch Loss: 0.7152042388916016\n",
      "Subject 16, Epoch 74, Loss: 2.7677430510520935, Final Batch Loss: 0.823387622833252\n",
      "Subject 16, Epoch 75, Loss: 2.612188696861267, Final Batch Loss: 0.6074225306510925\n",
      "Subject 16, Epoch 76, Loss: 2.7821788787841797, Final Batch Loss: 0.5498332381248474\n",
      "Subject 16, Epoch 77, Loss: 2.623681604862213, Final Batch Loss: 0.5644846558570862\n",
      "Subject 16, Epoch 78, Loss: 2.660247504711151, Final Batch Loss: 0.7021331191062927\n",
      "Subject 16, Epoch 79, Loss: 2.888271927833557, Final Batch Loss: 0.6772623658180237\n",
      "Subject 16, Epoch 80, Loss: 2.518511176109314, Final Batch Loss: 0.6487109661102295\n",
      "Subject 16, Epoch 81, Loss: 2.63640695810318, Final Batch Loss: 0.6007124781608582\n",
      "Subject 16, Epoch 82, Loss: 2.9050907492637634, Final Batch Loss: 0.8870297074317932\n",
      "Subject 16, Epoch 83, Loss: 2.602227032184601, Final Batch Loss: 0.6518141627311707\n",
      "Subject 16, Epoch 84, Loss: 2.4992262721061707, Final Batch Loss: 0.5932936072349548\n",
      "Subject 16, Epoch 85, Loss: 2.7638098001480103, Final Batch Loss: 0.7403968572616577\n",
      "Subject 16, Epoch 86, Loss: 2.5634900331497192, Final Batch Loss: 0.5878166556358337\n",
      "Subject 16, Epoch 87, Loss: 2.819981098175049, Final Batch Loss: 0.7525818943977356\n",
      "Subject 16, Epoch 88, Loss: 2.555543303489685, Final Batch Loss: 0.6625874638557434\n",
      "Subject 16, Epoch 89, Loss: 2.514409363269806, Final Batch Loss: 0.5992398262023926\n",
      "Subject 16, Epoch 90, Loss: 2.536859452724457, Final Batch Loss: 0.7394900321960449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 91, Loss: 2.4624523520469666, Final Batch Loss: 0.6122388243675232\n",
      "Subject 16, Epoch 92, Loss: 2.5231295824050903, Final Batch Loss: 0.6694188714027405\n",
      "Subject 16, Epoch 93, Loss: 2.521942377090454, Final Batch Loss: 0.5612158179283142\n",
      "Subject 16, Epoch 94, Loss: 2.3516452312469482, Final Batch Loss: 0.5503214001655579\n",
      "Subject 16, Epoch 95, Loss: 2.4750271439552307, Final Batch Loss: 0.5482321381568909\n",
      "Subject 16, Epoch 96, Loss: 2.24202162027359, Final Batch Loss: 0.5179259181022644\n",
      "Subject 16, Epoch 97, Loss: 2.510248303413391, Final Batch Loss: 0.5972421169281006\n",
      "Subject 16, Epoch 98, Loss: 2.346258282661438, Final Batch Loss: 0.5887435674667358\n",
      "Subject 16, Epoch 99, Loss: 2.418323040008545, Final Batch Loss: 0.6212003827095032\n",
      "Subject 16, Epoch 100, Loss: 2.4796428084373474, Final Batch Loss: 0.7009695768356323\n",
      "Subject 16, Epoch 101, Loss: 2.297481060028076, Final Batch Loss: 0.540859580039978\n",
      "Subject 16, Epoch 102, Loss: 2.3510690331459045, Final Batch Loss: 0.5261812806129456\n",
      "Subject 16, Epoch 103, Loss: 2.321509063243866, Final Batch Loss: 0.5539877414703369\n",
      "Subject 16, Epoch 104, Loss: 2.1293486654758453, Final Batch Loss: 0.6349747180938721\n",
      "Subject 16, Epoch 105, Loss: 2.382342219352722, Final Batch Loss: 0.6972240209579468\n",
      "Subject 16, Epoch 106, Loss: 2.102510988712311, Final Batch Loss: 0.4554276466369629\n",
      "Subject 16, Epoch 107, Loss: 2.2510489523410797, Final Batch Loss: 0.6313951015472412\n",
      "Subject 16, Epoch 108, Loss: 2.3510529696941376, Final Batch Loss: 0.7207313776016235\n",
      "Subject 16, Epoch 109, Loss: 2.2111353278160095, Final Batch Loss: 0.5896835327148438\n",
      "Subject 16, Epoch 110, Loss: 2.128407597541809, Final Batch Loss: 0.5538468956947327\n",
      "Subject 16, Epoch 111, Loss: 2.038501739501953, Final Batch Loss: 0.3919621706008911\n",
      "Subject 16, Epoch 112, Loss: 2.538853406906128, Final Batch Loss: 0.9015678763389587\n",
      "Subject 16, Epoch 113, Loss: 2.114892601966858, Final Batch Loss: 0.4927644431591034\n",
      "Subject 16, Epoch 114, Loss: 2.1895812153816223, Final Batch Loss: 0.5084435343742371\n",
      "Subject 16, Epoch 115, Loss: 2.003386527299881, Final Batch Loss: 0.38156673312187195\n",
      "Subject 16, Epoch 116, Loss: 2.1130325198173523, Final Batch Loss: 0.42590951919555664\n",
      "Subject 16, Epoch 117, Loss: 2.0605963468551636, Final Batch Loss: 0.5098592042922974\n",
      "Subject 16, Epoch 118, Loss: 2.255737006664276, Final Batch Loss: 0.6706291437149048\n",
      "Subject 16, Epoch 119, Loss: 1.9913679659366608, Final Batch Loss: 0.5023336410522461\n",
      "Subject 16, Epoch 120, Loss: 1.9452727138996124, Final Batch Loss: 0.4790911376476288\n",
      "Subject 16, Epoch 121, Loss: 1.888606607913971, Final Batch Loss: 0.45568811893463135\n",
      "Subject 16, Epoch 122, Loss: 2.296942323446274, Final Batch Loss: 0.813670814037323\n",
      "Subject 16, Epoch 123, Loss: 1.9823964536190033, Final Batch Loss: 0.4850822687149048\n",
      "Subject 16, Epoch 124, Loss: 2.059373766183853, Final Batch Loss: 0.4850687086582184\n",
      "Subject 16, Epoch 125, Loss: 1.9494550228118896, Final Batch Loss: 0.3306255638599396\n",
      "Subject 16, Epoch 126, Loss: 1.8431891202926636, Final Batch Loss: 0.39965951442718506\n",
      "Subject 16, Epoch 127, Loss: 2.032165080308914, Final Batch Loss: 0.4598442316055298\n",
      "Subject 16, Epoch 128, Loss: 2.001987487077713, Final Batch Loss: 0.5531917810440063\n",
      "Subject 16, Epoch 129, Loss: 2.0731253027915955, Final Batch Loss: 0.5634102821350098\n",
      "Subject 16, Epoch 130, Loss: 1.7776615023612976, Final Batch Loss: 0.44523879885673523\n",
      "Subject 16, Epoch 131, Loss: 1.8207001686096191, Final Batch Loss: 0.42538848519325256\n",
      "Subject 16, Epoch 132, Loss: 1.8297456502914429, Final Batch Loss: 0.3496380150318146\n",
      "Subject 16, Epoch 133, Loss: 2.0479661226272583, Final Batch Loss: 0.6183525919914246\n",
      "Subject 16, Epoch 134, Loss: 2.002488285303116, Final Batch Loss: 0.5543121695518494\n",
      "Subject 16, Epoch 135, Loss: 1.9448781907558441, Final Batch Loss: 0.5474404096603394\n",
      "Subject 16, Epoch 136, Loss: 1.8780509531497955, Final Batch Loss: 0.4566486179828644\n",
      "Subject 16, Epoch 137, Loss: 1.9429329633712769, Final Batch Loss: 0.34422755241394043\n",
      "Subject 16, Epoch 138, Loss: 1.7061394453048706, Final Batch Loss: 0.3643701374530792\n",
      "Subject 16, Epoch 139, Loss: 1.9592553079128265, Final Batch Loss: 0.48664382100105286\n",
      "Subject 16, Epoch 140, Loss: 1.8483754396438599, Final Batch Loss: 0.4590597450733185\n",
      "Subject 16, Epoch 141, Loss: 1.8469600677490234, Final Batch Loss: 0.4937182366847992\n",
      "Subject 16, Epoch 142, Loss: 2.1345276534557343, Final Batch Loss: 0.7105758786201477\n",
      "Subject 16, Epoch 143, Loss: 1.8570621609687805, Final Batch Loss: 0.38656604290008545\n",
      "Subject 16, Epoch 144, Loss: 1.690717101097107, Final Batch Loss: 0.30906593799591064\n",
      "Subject 16, Epoch 145, Loss: 2.0349228382110596, Final Batch Loss: 0.5885002017021179\n",
      "Subject 16, Epoch 146, Loss: 1.8745775818824768, Final Batch Loss: 0.48255714774131775\n",
      "Subject 16, Epoch 147, Loss: 1.7892060279846191, Final Batch Loss: 0.4829855263233185\n",
      "Subject 16, Epoch 148, Loss: 2.0357520282268524, Final Batch Loss: 0.6784390211105347\n",
      "Subject 16, Epoch 149, Loss: 1.9222615957260132, Final Batch Loss: 0.5601282119750977\n",
      "Subject 16, Epoch 150, Loss: 1.8900464475154877, Final Batch Loss: 0.588178813457489\n",
      "Subject 16, Epoch 151, Loss: 1.7822753190994263, Final Batch Loss: 0.3732597827911377\n",
      "Subject 16, Epoch 152, Loss: 1.784491091966629, Final Batch Loss: 0.38558894395828247\n",
      "Subject 16, Epoch 153, Loss: 1.7125849723815918, Final Batch Loss: 0.38817793130874634\n",
      "Subject 16, Epoch 154, Loss: 1.9486443102359772, Final Batch Loss: 0.5909162759780884\n",
      "Subject 16, Epoch 155, Loss: 1.5752239525318146, Final Batch Loss: 0.2623704671859741\n",
      "Subject 16, Epoch 156, Loss: 1.8104152083396912, Final Batch Loss: 0.5270031094551086\n",
      "Subject 16, Epoch 157, Loss: 1.6689750254154205, Final Batch Loss: 0.4524261951446533\n",
      "Subject 16, Epoch 158, Loss: 1.7376162707805634, Final Batch Loss: 0.48841169476509094\n",
      "Subject 16, Epoch 159, Loss: 1.865704357624054, Final Batch Loss: 0.4791056513786316\n",
      "Subject 16, Epoch 160, Loss: 1.7136427164077759, Final Batch Loss: 0.4511050283908844\n",
      "Subject 16, Epoch 161, Loss: 1.6531121730804443, Final Batch Loss: 0.383152574300766\n",
      "Subject 16, Epoch 162, Loss: 1.6363232135772705, Final Batch Loss: 0.34924420714378357\n",
      "Subject 16, Epoch 163, Loss: 1.6838161647319794, Final Batch Loss: 0.4627060890197754\n",
      "Subject 16, Epoch 164, Loss: 1.6611300110816956, Final Batch Loss: 0.3412826955318451\n",
      "Subject 16, Epoch 165, Loss: 1.726277619600296, Final Batch Loss: 0.4612305760383606\n",
      "Subject 16, Epoch 166, Loss: 1.6685404777526855, Final Batch Loss: 0.41577020287513733\n",
      "Subject 16, Epoch 167, Loss: 1.8236985206604004, Final Batch Loss: 0.6568512916564941\n",
      "Subject 16, Epoch 168, Loss: 1.5802057385444641, Final Batch Loss: 0.3315153121948242\n",
      "Subject 16, Epoch 169, Loss: 1.8666592240333557, Final Batch Loss: 0.5208239555358887\n",
      "Subject 16, Epoch 170, Loss: 1.527691274881363, Final Batch Loss: 0.3547464907169342\n",
      "Subject 16, Epoch 171, Loss: 1.712662398815155, Final Batch Loss: 0.5131415724754333\n",
      "Subject 16, Epoch 172, Loss: 1.6855060458183289, Final Batch Loss: 0.46759897470474243\n",
      "Subject 16, Epoch 173, Loss: 1.7006780207157135, Final Batch Loss: 0.3973289132118225\n",
      "Subject 16, Epoch 174, Loss: 1.877558708190918, Final Batch Loss: 0.38375985622406006\n",
      "Subject 16, Epoch 175, Loss: 1.6092797219753265, Final Batch Loss: 0.39852455258369446\n",
      "Subject 16, Epoch 176, Loss: 1.5784133076667786, Final Batch Loss: 0.34546998143196106\n",
      "Subject 16, Epoch 177, Loss: 1.7329787611961365, Final Batch Loss: 0.46549418568611145\n",
      "Subject 16, Epoch 178, Loss: 1.7433695495128632, Final Batch Loss: 0.6060296297073364\n",
      "Subject 16, Epoch 179, Loss: 1.5200648307800293, Final Batch Loss: 0.30352649092674255\n",
      "Subject 16, Epoch 180, Loss: 1.7581186592578888, Final Batch Loss: 0.4527648091316223\n",
      "Subject 16, Epoch 181, Loss: 1.6095722913742065, Final Batch Loss: 0.3436601459980011\n",
      "Subject 16, Epoch 182, Loss: 1.619462102651596, Final Batch Loss: 0.3866473138332367\n",
      "Subject 16, Epoch 183, Loss: 1.6802433729171753, Final Batch Loss: 0.4323064088821411\n",
      "Subject 16, Epoch 184, Loss: 1.4965196251869202, Final Batch Loss: 0.3448181450366974\n",
      "Subject 16, Epoch 185, Loss: 1.5335918366909027, Final Batch Loss: 0.32447749376296997\n",
      "Subject 16, Epoch 186, Loss: 1.7809494733810425, Final Batch Loss: 0.6814507842063904\n",
      "Subject 16, Epoch 187, Loss: 1.706668198108673, Final Batch Loss: 0.45125919580459595\n",
      "Subject 16, Epoch 188, Loss: 1.5193069875240326, Final Batch Loss: 0.30563464760780334\n",
      "Subject 16, Epoch 189, Loss: 1.549081027507782, Final Batch Loss: 0.31790298223495483\n",
      "Subject 16, Epoch 190, Loss: 1.5417303293943405, Final Batch Loss: 0.2408800572156906\n",
      "Subject 16, Epoch 191, Loss: 1.6408949494361877, Final Batch Loss: 0.526405394077301\n",
      "Subject 16, Epoch 192, Loss: 1.606374740600586, Final Batch Loss: 0.4306674003601074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 193, Loss: 1.5471421778202057, Final Batch Loss: 0.3020787537097931\n",
      "Subject 16, Epoch 194, Loss: 1.4663166403770447, Final Batch Loss: 0.3421264588832855\n",
      "Subject 16, Epoch 195, Loss: 1.5590652525424957, Final Batch Loss: 0.3744662404060364\n",
      "Subject 16, Epoch 196, Loss: 1.684196650981903, Final Batch Loss: 0.4976629316806793\n",
      "Subject 16, Epoch 197, Loss: 1.5876387059688568, Final Batch Loss: 0.49402064085006714\n",
      "Subject 16, Epoch 198, Loss: 1.6021471321582794, Final Batch Loss: 0.32721155881881714\n",
      "Subject 16, Epoch 199, Loss: 1.6240570545196533, Final Batch Loss: 0.3939198851585388\n",
      "Subject 16, Epoch 200, Loss: 1.4761923849582672, Final Batch Loss: 0.3375191390514374\n",
      "Subject 16, Epoch 201, Loss: 1.4570160955190659, Final Batch Loss: 0.2281057983636856\n",
      "Subject 16, Epoch 202, Loss: 1.5154402256011963, Final Batch Loss: 0.3843630850315094\n",
      "Subject 16, Epoch 203, Loss: 1.5790219902992249, Final Batch Loss: 0.29008394479751587\n",
      "Subject 16, Epoch 204, Loss: 1.3918486535549164, Final Batch Loss: 0.19275471568107605\n",
      "Subject 16, Epoch 205, Loss: 1.3957675397396088, Final Batch Loss: 0.32274043560028076\n",
      "Subject 16, Epoch 206, Loss: 1.501509964466095, Final Batch Loss: 0.35441040992736816\n",
      "Subject 16, Epoch 207, Loss: 1.388933002948761, Final Batch Loss: 0.2794397473335266\n",
      "Subject 16, Epoch 208, Loss: 1.5624639391899109, Final Batch Loss: 0.48006874322891235\n",
      "Subject 16, Epoch 209, Loss: 1.4692583680152893, Final Batch Loss: 0.29051461815834045\n",
      "Subject 16, Epoch 210, Loss: 1.5610763132572174, Final Batch Loss: 0.38368651270866394\n",
      "Subject 16, Epoch 211, Loss: 1.5490449666976929, Final Batch Loss: 0.35271570086479187\n",
      "Subject 16, Epoch 212, Loss: 1.6187645196914673, Final Batch Loss: 0.41716527938842773\n",
      "Subject 16, Epoch 213, Loss: 1.618079513311386, Final Batch Loss: 0.3311205804347992\n",
      "Subject 16, Epoch 214, Loss: 1.4455948323011398, Final Batch Loss: 0.23051001131534576\n",
      "Subject 16, Epoch 215, Loss: 1.5474155247211456, Final Batch Loss: 0.4521932601928711\n",
      "Subject 16, Epoch 216, Loss: 1.7041235268115997, Final Batch Loss: 0.50197833776474\n",
      "Subject 16, Epoch 217, Loss: 1.4677444100379944, Final Batch Loss: 0.3773597478866577\n",
      "Subject 16, Epoch 218, Loss: 1.4857513904571533, Final Batch Loss: 0.38307619094848633\n",
      "Subject 16, Epoch 219, Loss: 1.3436789214611053, Final Batch Loss: 0.22130253911018372\n",
      "Subject 16, Epoch 220, Loss: 1.4832971394062042, Final Batch Loss: 0.3453552722930908\n",
      "Subject 16, Epoch 221, Loss: 1.383491337299347, Final Batch Loss: 0.2996463477611542\n",
      "Subject 16, Epoch 222, Loss: 1.339120864868164, Final Batch Loss: 0.2748423218727112\n",
      "Subject 16, Epoch 223, Loss: 1.4324525892734528, Final Batch Loss: 0.35716700553894043\n",
      "Subject 16, Epoch 224, Loss: 1.5250490009784698, Final Batch Loss: 0.4442170262336731\n",
      "Subject 16, Epoch 225, Loss: 1.3910367488861084, Final Batch Loss: 0.3040829002857208\n",
      "Subject 16, Epoch 226, Loss: 1.5538860261440277, Final Batch Loss: 0.3309875428676605\n",
      "Subject 16, Epoch 227, Loss: 1.3343174308538437, Final Batch Loss: 0.23844771087169647\n",
      "Subject 16, Epoch 228, Loss: 1.5371458232402802, Final Batch Loss: 0.3924230933189392\n",
      "Subject 16, Epoch 229, Loss: 1.4460347890853882, Final Batch Loss: 0.27454257011413574\n",
      "Subject 16, Epoch 230, Loss: 1.5227222740650177, Final Batch Loss: 0.35806089639663696\n",
      "Subject 16, Epoch 231, Loss: 1.414812982082367, Final Batch Loss: 0.2628784775733948\n",
      "Subject 16, Epoch 232, Loss: 1.4766746759414673, Final Batch Loss: 0.462690144777298\n",
      "Subject 16, Epoch 233, Loss: 1.4535889625549316, Final Batch Loss: 0.4219841957092285\n",
      "Subject 16, Epoch 234, Loss: 1.4485538005828857, Final Batch Loss: 0.3731107711791992\n",
      "Subject 16, Epoch 235, Loss: 1.3479384183883667, Final Batch Loss: 0.29596924781799316\n",
      "Subject 16, Epoch 236, Loss: 1.541408747434616, Final Batch Loss: 0.4609817862510681\n",
      "Subject 16, Epoch 237, Loss: 1.3527302742004395, Final Batch Loss: 0.3275257349014282\n",
      "Subject 16, Epoch 238, Loss: 1.393378108739853, Final Batch Loss: 0.35718202590942383\n",
      "Subject 16, Epoch 239, Loss: 1.3432436883449554, Final Batch Loss: 0.26156413555145264\n",
      "Subject 16, Epoch 240, Loss: 1.570986121892929, Final Batch Loss: 0.5991306304931641\n",
      "Subject 16, Epoch 241, Loss: 1.527095079421997, Final Batch Loss: 0.3751051127910614\n",
      "Subject 16, Epoch 242, Loss: 1.386659324169159, Final Batch Loss: 0.2985623776912689\n",
      "Subject 16, Epoch 243, Loss: 1.3998479545116425, Final Batch Loss: 0.40292906761169434\n",
      "Subject 16, Epoch 244, Loss: 1.3711348474025726, Final Batch Loss: 0.23799178004264832\n",
      "Subject 16, Epoch 245, Loss: 1.3518807291984558, Final Batch Loss: 0.2848927974700928\n",
      "Subject 16, Epoch 246, Loss: 1.4133583903312683, Final Batch Loss: 0.33929792046546936\n",
      "Subject 16, Epoch 247, Loss: 1.495284378528595, Final Batch Loss: 0.3669016659259796\n",
      "Subject 16, Epoch 248, Loss: 1.356472909450531, Final Batch Loss: 0.2466840147972107\n",
      "Subject 16, Epoch 249, Loss: 1.2863593250513077, Final Batch Loss: 0.18995393812656403\n",
      "Subject 16, Epoch 250, Loss: 1.3532655239105225, Final Batch Loss: 0.25786492228507996\n",
      "Subject 16, Epoch 251, Loss: 1.2531037032604218, Final Batch Loss: 0.24946379661560059\n",
      "Subject 16, Epoch 252, Loss: 1.4111640751361847, Final Batch Loss: 0.34675702452659607\n",
      "Subject 16, Epoch 253, Loss: 1.3670175671577454, Final Batch Loss: 0.32840657234191895\n",
      "Subject 16, Epoch 254, Loss: 1.3032889068126678, Final Batch Loss: 0.34113141894340515\n",
      "Subject 16, Epoch 255, Loss: 1.396501898765564, Final Batch Loss: 0.32642850279808044\n",
      "Subject 16, Epoch 256, Loss: 1.3244784474372864, Final Batch Loss: 0.3536660671234131\n",
      "Subject 16, Epoch 257, Loss: 1.4379962086677551, Final Batch Loss: 0.40276607871055603\n",
      "Subject 16, Epoch 258, Loss: 1.5081187188625336, Final Batch Loss: 0.43331634998321533\n",
      "Subject 16, Epoch 259, Loss: 1.4057975709438324, Final Batch Loss: 0.38338005542755127\n",
      "Subject 16, Epoch 260, Loss: 1.3433696329593658, Final Batch Loss: 0.29610589146614075\n",
      "Subject 16, Epoch 261, Loss: 1.3232919871807098, Final Batch Loss: 0.2429216504096985\n",
      "Subject 16, Epoch 262, Loss: 1.4441353976726532, Final Batch Loss: 0.3480542302131653\n",
      "Subject 16, Epoch 263, Loss: 1.3084410429000854, Final Batch Loss: 0.2538522779941559\n",
      "Subject 16, Epoch 264, Loss: 1.2967043220996857, Final Batch Loss: 0.3366471827030182\n",
      "Subject 16, Epoch 265, Loss: 1.481473445892334, Final Batch Loss: 0.3769594132900238\n",
      "Subject 16, Epoch 266, Loss: 1.300027221441269, Final Batch Loss: 0.23963361978530884\n",
      "Subject 16, Epoch 267, Loss: 1.2227012813091278, Final Batch Loss: 0.2635071277618408\n",
      "Subject 16, Epoch 268, Loss: 1.3227523565292358, Final Batch Loss: 0.34463393688201904\n",
      "Subject 16, Epoch 269, Loss: 1.4874520897865295, Final Batch Loss: 0.4435214102268219\n",
      "Subject 16, Epoch 270, Loss: 1.2840000689029694, Final Batch Loss: 0.3154749870300293\n",
      "Subject 16, Epoch 271, Loss: 1.2630055844783783, Final Batch Loss: 0.20988240838050842\n",
      "Subject 16, Epoch 272, Loss: 1.3623026311397552, Final Batch Loss: 0.3938272297382355\n",
      "Subject 16, Epoch 273, Loss: 1.3718335330486298, Final Batch Loss: 0.32618293166160583\n",
      "Subject 16, Epoch 274, Loss: 1.338022768497467, Final Batch Loss: 0.4101487696170807\n",
      "Subject 16, Epoch 275, Loss: 1.3932337760925293, Final Batch Loss: 0.32645636796951294\n",
      "Subject 16, Epoch 276, Loss: 1.454101800918579, Final Batch Loss: 0.47822055220603943\n",
      "Subject 16, Epoch 277, Loss: 1.2610273957252502, Final Batch Loss: 0.23144274950027466\n",
      "Subject 16, Epoch 278, Loss: 1.2617087662220001, Final Batch Loss: 0.28439226746559143\n",
      "Subject 16, Epoch 279, Loss: 1.3078550398349762, Final Batch Loss: 0.33014512062072754\n",
      "Subject 16, Epoch 280, Loss: 1.4211909472942352, Final Batch Loss: 0.30612531304359436\n",
      "Subject 16, Epoch 281, Loss: 1.3407730460166931, Final Batch Loss: 0.3163304328918457\n",
      "Subject 16, Epoch 282, Loss: 1.3164647668600082, Final Batch Loss: 0.3209622800350189\n",
      "Subject 16, Epoch 283, Loss: 1.4693523943424225, Final Batch Loss: 0.4327750504016876\n",
      "Subject 16, Epoch 284, Loss: 1.2837900221347809, Final Batch Loss: 0.25493696331977844\n",
      "Subject 16, Epoch 285, Loss: 1.4267792701721191, Final Batch Loss: 0.3094628155231476\n",
      "Subject 16, Epoch 286, Loss: 1.3064168095588684, Final Batch Loss: 0.27619805932044983\n",
      "Subject 16, Epoch 287, Loss: 1.2711630165576935, Final Batch Loss: 0.28485822677612305\n",
      "Subject 16, Epoch 288, Loss: 1.3312219232320786, Final Batch Loss: 0.37787115573883057\n",
      "Subject 16, Epoch 289, Loss: 1.3478968739509583, Final Batch Loss: 0.36904335021972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 290, Loss: 1.3007950186729431, Final Batch Loss: 0.2768392562866211\n",
      "Subject 16, Epoch 291, Loss: 1.25699782371521, Final Batch Loss: 0.2602342963218689\n",
      "Subject 16, Epoch 292, Loss: 1.3705143928527832, Final Batch Loss: 0.4063732326030731\n",
      "Subject 16, Epoch 293, Loss: 1.3621345460414886, Final Batch Loss: 0.295161634683609\n",
      "Subject 16, Epoch 294, Loss: 1.4933096766471863, Final Batch Loss: 0.41950762271881104\n",
      "Subject 16, Epoch 295, Loss: 1.318104088306427, Final Batch Loss: 0.33336135745048523\n",
      "Subject 16, Epoch 296, Loss: 1.3593600690364838, Final Batch Loss: 0.3858589231967926\n",
      "Subject 16, Epoch 297, Loss: 1.3869795203208923, Final Batch Loss: 0.3300492465496063\n",
      "Subject 16, Epoch 298, Loss: 1.2839372754096985, Final Batch Loss: 0.27596938610076904\n",
      "Subject 16, Epoch 299, Loss: 1.3332255184650421, Final Batch Loss: 0.3176308274269104\n",
      "Subject 16, Epoch 300, Loss: 1.3564716279506683, Final Batch Loss: 0.37916573882102966\n",
      "Subject 16, Epoch 301, Loss: 1.2527703642845154, Final Batch Loss: 0.2629905045032501\n",
      "Subject 16, Epoch 302, Loss: 1.3200611770153046, Final Batch Loss: 0.37447431683540344\n",
      "Subject 16, Epoch 303, Loss: 1.230774387717247, Final Batch Loss: 0.25657013058662415\n",
      "Subject 16, Epoch 304, Loss: 1.2733348906040192, Final Batch Loss: 0.32243603467941284\n",
      "Subject 16, Epoch 305, Loss: 1.2738767564296722, Final Batch Loss: 0.2990652918815613\n",
      "Subject 16, Epoch 306, Loss: 1.4022392332553864, Final Batch Loss: 0.44007545709609985\n",
      "Subject 16, Epoch 307, Loss: 1.2457718551158905, Final Batch Loss: 0.3003014624118805\n",
      "Subject 16, Epoch 308, Loss: 1.2413328289985657, Final Batch Loss: 0.2999056279659271\n",
      "Subject 16, Epoch 309, Loss: 1.376583606004715, Final Batch Loss: 0.41034165024757385\n",
      "Subject 16, Epoch 310, Loss: 1.166458398103714, Final Batch Loss: 0.2596774399280548\n",
      "Subject 16, Epoch 311, Loss: 1.126553237438202, Final Batch Loss: 0.1678808033466339\n",
      "Subject 16, Epoch 312, Loss: 1.2818431109189987, Final Batch Loss: 0.22101272642612457\n",
      "Subject 16, Epoch 313, Loss: 1.2348522543907166, Final Batch Loss: 0.3077974021434784\n",
      "Subject 16, Epoch 314, Loss: 1.2036237120628357, Final Batch Loss: 0.23749276995658875\n",
      "Subject 16, Epoch 315, Loss: 1.3663625717163086, Final Batch Loss: 0.3764634430408478\n",
      "Subject 16, Epoch 316, Loss: 1.2316674292087555, Final Batch Loss: 0.27577492594718933\n",
      "Subject 16, Epoch 317, Loss: 1.1612536907196045, Final Batch Loss: 0.25357308983802795\n",
      "Subject 16, Epoch 318, Loss: 1.226089671254158, Final Batch Loss: 0.3653505742549896\n",
      "Subject 16, Epoch 319, Loss: 1.237228125333786, Final Batch Loss: 0.3016461730003357\n",
      "Subject 16, Epoch 320, Loss: 1.3127931654453278, Final Batch Loss: 0.3065680265426636\n",
      "Subject 16, Epoch 321, Loss: 1.4193756580352783, Final Batch Loss: 0.34198200702667236\n",
      "Subject 16, Epoch 322, Loss: 1.2438182830810547, Final Batch Loss: 0.3411905765533447\n",
      "Subject 16, Epoch 323, Loss: 1.267350196838379, Final Batch Loss: 0.270711749792099\n",
      "Subject 16, Epoch 324, Loss: 1.2027008384466171, Final Batch Loss: 0.23494376242160797\n",
      "Subject 16, Epoch 325, Loss: 1.238991767168045, Final Batch Loss: 0.2612793445587158\n",
      "Subject 16, Epoch 326, Loss: 1.3636492788791656, Final Batch Loss: 0.36326953768730164\n",
      "Subject 16, Epoch 327, Loss: 1.348926305770874, Final Batch Loss: 0.38092952966690063\n",
      "Subject 16, Epoch 328, Loss: 1.1965781152248383, Final Batch Loss: 0.37432411313056946\n",
      "Subject 16, Epoch 329, Loss: 1.1944482326507568, Final Batch Loss: 0.3349984586238861\n",
      "Subject 16, Epoch 330, Loss: 1.3855133950710297, Final Batch Loss: 0.3460533022880554\n",
      "Subject 16, Epoch 331, Loss: 1.1786707788705826, Final Batch Loss: 0.2183159738779068\n",
      "Subject 16, Epoch 332, Loss: 1.2303424924612045, Final Batch Loss: 0.21199817955493927\n",
      "Subject 16, Epoch 333, Loss: 1.2859588265419006, Final Batch Loss: 0.2300025224685669\n",
      "Subject 16, Epoch 334, Loss: 1.1708095073699951, Final Batch Loss: 0.22709479928016663\n",
      "Subject 16, Epoch 335, Loss: 1.1729212999343872, Final Batch Loss: 0.26586392521858215\n",
      "Subject 16, Epoch 336, Loss: 1.29474475979805, Final Batch Loss: 0.4436337351799011\n",
      "Subject 16, Epoch 337, Loss: 1.1275736838579178, Final Batch Loss: 0.2581911087036133\n",
      "Subject 16, Epoch 338, Loss: 1.2044973969459534, Final Batch Loss: 0.3047991991043091\n",
      "Subject 16, Epoch 339, Loss: 1.2064266800880432, Final Batch Loss: 0.24080529808998108\n",
      "Subject 16, Epoch 340, Loss: 1.1681183278560638, Final Batch Loss: 0.28289762139320374\n",
      "Subject 16, Epoch 341, Loss: 1.2234471142292023, Final Batch Loss: 0.3649473786354065\n",
      "Subject 16, Epoch 342, Loss: 1.204421043395996, Final Batch Loss: 0.3333339989185333\n",
      "Subject 16, Epoch 343, Loss: 1.1609544903039932, Final Batch Loss: 0.16195620596408844\n",
      "Subject 16, Epoch 344, Loss: 1.127699464559555, Final Batch Loss: 0.2957109212875366\n",
      "Subject 16, Epoch 345, Loss: 1.2901352494955063, Final Batch Loss: 0.4302999973297119\n",
      "Subject 16, Epoch 346, Loss: 1.1989744305610657, Final Batch Loss: 0.29890599846839905\n",
      "Subject 16, Epoch 347, Loss: 1.130904033780098, Final Batch Loss: 0.22680355608463287\n",
      "Subject 16, Epoch 348, Loss: 1.2444657683372498, Final Batch Loss: 0.31310996413230896\n",
      "Subject 16, Epoch 349, Loss: 1.0573257058858871, Final Batch Loss: 0.2386089712381363\n",
      "Subject 16, Epoch 350, Loss: 1.168944612145424, Final Batch Loss: 0.29873910546302795\n",
      "Subject 16, Epoch 351, Loss: 1.139468640089035, Final Batch Loss: 0.35995766520500183\n",
      "Subject 16, Epoch 352, Loss: 1.1174737960100174, Final Batch Loss: 0.18969814479351044\n",
      "Subject 16, Epoch 353, Loss: 1.1780767142772675, Final Batch Loss: 0.2559889853000641\n",
      "Subject 16, Epoch 354, Loss: 1.3014358878135681, Final Batch Loss: 0.38492995500564575\n",
      "Subject 16, Epoch 355, Loss: 1.1042524874210358, Final Batch Loss: 0.2748314440250397\n",
      "Subject 16, Epoch 356, Loss: 1.2813071608543396, Final Batch Loss: 0.37710368633270264\n",
      "Subject 16, Epoch 357, Loss: 1.0716774016618729, Final Batch Loss: 0.2724417746067047\n",
      "Subject 16, Epoch 358, Loss: 1.3502736687660217, Final Batch Loss: 0.38889020681381226\n",
      "Subject 16, Epoch 359, Loss: 1.0208188593387604, Final Batch Loss: 0.2190931737422943\n",
      "Subject 16, Epoch 360, Loss: 1.2772344052791595, Final Batch Loss: 0.43442502617836\n",
      "Subject 16, Epoch 361, Loss: 1.2053741216659546, Final Batch Loss: 0.31925809383392334\n",
      "Subject 16, Epoch 362, Loss: 1.1154016852378845, Final Batch Loss: 0.2916458547115326\n",
      "Subject 16, Epoch 363, Loss: 1.1957865208387375, Final Batch Loss: 0.31601855158805847\n",
      "Subject 16, Epoch 364, Loss: 1.2018319368362427, Final Batch Loss: 0.4072319269180298\n",
      "Subject 16, Epoch 365, Loss: 1.267397090792656, Final Batch Loss: 0.39487093687057495\n",
      "Subject 16, Epoch 366, Loss: 1.1732261776924133, Final Batch Loss: 0.3280584216117859\n",
      "Subject 16, Epoch 367, Loss: 1.1138152182102203, Final Batch Loss: 0.3584854006767273\n",
      "Subject 16, Epoch 368, Loss: 1.1585712432861328, Final Batch Loss: 0.32744264602661133\n",
      "Subject 16, Epoch 369, Loss: 1.2048499137163162, Final Batch Loss: 0.3754333257675171\n",
      "Subject 16, Epoch 370, Loss: 1.0496626198291779, Final Batch Loss: 0.16836538910865784\n",
      "Subject 16, Epoch 371, Loss: 1.0531184375286102, Final Batch Loss: 0.2558022141456604\n",
      "Subject 16, Epoch 372, Loss: 1.101997509598732, Final Batch Loss: 0.2814468443393707\n",
      "Subject 16, Epoch 373, Loss: 1.0951713025569916, Final Batch Loss: 0.3451773524284363\n",
      "Subject 16, Epoch 374, Loss: 1.0656071901321411, Final Batch Loss: 0.2081264853477478\n",
      "Subject 16, Epoch 375, Loss: 1.1505223363637924, Final Batch Loss: 0.23605434596538544\n",
      "Subject 16, Epoch 376, Loss: 1.0390547811985016, Final Batch Loss: 0.18305112421512604\n",
      "Subject 16, Epoch 377, Loss: 0.9813485741615295, Final Batch Loss: 0.26898449659347534\n",
      "Subject 16, Epoch 378, Loss: 1.1970139890909195, Final Batch Loss: 0.39063167572021484\n",
      "Subject 16, Epoch 379, Loss: 1.051124781370163, Final Batch Loss: 0.22862772643566132\n",
      "Subject 16, Epoch 380, Loss: 1.0778109580278397, Final Batch Loss: 0.21731595695018768\n",
      "Subject 16, Epoch 381, Loss: 1.0571686327457428, Final Batch Loss: 0.2998034358024597\n",
      "Subject 16, Epoch 382, Loss: 1.0879638493061066, Final Batch Loss: 0.2514673173427582\n",
      "Subject 16, Epoch 383, Loss: 0.9412660896778107, Final Batch Loss: 0.23648002743721008\n",
      "Subject 16, Epoch 384, Loss: 1.1105000674724579, Final Batch Loss: 0.34409263730049133\n",
      "Subject 16, Epoch 385, Loss: 0.9042744636535645, Final Batch Loss: 0.18501590192317963\n",
      "Subject 16, Epoch 386, Loss: 0.9711016416549683, Final Batch Loss: 0.2053312510251999\n",
      "Subject 16, Epoch 387, Loss: 0.9785311222076416, Final Batch Loss: 0.1607893407344818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 388, Loss: 1.0265269428491592, Final Batch Loss: 0.3045595586299896\n",
      "Subject 16, Epoch 389, Loss: 1.0214514881372452, Final Batch Loss: 0.32242217659950256\n",
      "Subject 16, Epoch 390, Loss: 0.971958264708519, Final Batch Loss: 0.2521307170391083\n",
      "Subject 16, Epoch 391, Loss: 1.0201570391654968, Final Batch Loss: 0.23867575824260712\n",
      "Subject 16, Epoch 392, Loss: 1.1055088341236115, Final Batch Loss: 0.3683505654335022\n",
      "Subject 16, Epoch 393, Loss: 1.0179163664579391, Final Batch Loss: 0.3206738233566284\n",
      "Subject 16, Epoch 394, Loss: 0.9382949024438858, Final Batch Loss: 0.2147044837474823\n",
      "Subject 16, Epoch 395, Loss: 1.008434697985649, Final Batch Loss: 0.28764718770980835\n",
      "Subject 16, Epoch 396, Loss: 1.0735322684049606, Final Batch Loss: 0.19217894971370697\n",
      "Subject 16, Epoch 397, Loss: 1.011520653963089, Final Batch Loss: 0.21715238690376282\n",
      "Subject 16, Epoch 398, Loss: 0.865964949131012, Final Batch Loss: 0.12791131436824799\n",
      "Subject 16, Epoch 399, Loss: 0.9632117599248886, Final Batch Loss: 0.22580242156982422\n",
      "Subject 16, Epoch 400, Loss: 0.9901864379644394, Final Batch Loss: 0.2478330433368683\n",
      "Subject 16, Epoch 401, Loss: 0.891135036945343, Final Batch Loss: 0.16897360980510712\n",
      "Subject 16, Epoch 402, Loss: 1.014535441994667, Final Batch Loss: 0.23135113716125488\n",
      "Subject 16, Epoch 403, Loss: 0.905171737074852, Final Batch Loss: 0.16019399464130402\n",
      "Subject 16, Epoch 404, Loss: 0.9883118569850922, Final Batch Loss: 0.23709437251091003\n",
      "Subject 16, Epoch 405, Loss: 1.080014854669571, Final Batch Loss: 0.33039718866348267\n",
      "Subject 16, Epoch 406, Loss: 0.9399632066488266, Final Batch Loss: 0.14204749464988708\n",
      "Subject 16, Epoch 407, Loss: 0.9943149536848068, Final Batch Loss: 0.30197983980178833\n",
      "Subject 16, Epoch 408, Loss: 0.9335574507713318, Final Batch Loss: 0.24123483896255493\n",
      "Subject 16, Epoch 409, Loss: 0.8978578671813011, Final Batch Loss: 0.11053770035505295\n",
      "Subject 16, Epoch 410, Loss: 0.9949732422828674, Final Batch Loss: 0.3129255175590515\n",
      "Subject 16, Epoch 411, Loss: 1.005228877067566, Final Batch Loss: 0.3126547634601593\n",
      "Subject 16, Epoch 412, Loss: 0.9709964096546173, Final Batch Loss: 0.2777225971221924\n",
      "Subject 16, Epoch 413, Loss: 1.066535472869873, Final Batch Loss: 0.36835765838623047\n",
      "Subject 16, Epoch 414, Loss: 0.8538887351751328, Final Batch Loss: 0.1526467502117157\n",
      "Subject 16, Epoch 415, Loss: 1.014105573296547, Final Batch Loss: 0.31264448165893555\n",
      "Subject 16, Epoch 416, Loss: 0.9107004702091217, Final Batch Loss: 0.21162784099578857\n",
      "Subject 16, Epoch 417, Loss: 0.9953232705593109, Final Batch Loss: 0.25439023971557617\n",
      "Subject 16, Epoch 418, Loss: 0.9232117682695389, Final Batch Loss: 0.26458585262298584\n",
      "Subject 16, Epoch 419, Loss: 1.0750197619199753, Final Batch Loss: 0.2740827798843384\n",
      "Subject 16, Epoch 420, Loss: 0.9480195641517639, Final Batch Loss: 0.27622732520103455\n",
      "Subject 16, Epoch 421, Loss: 1.0207743793725967, Final Batch Loss: 0.3248257339000702\n",
      "Subject 16, Epoch 422, Loss: 0.8552517592906952, Final Batch Loss: 0.18310299515724182\n",
      "Subject 16, Epoch 423, Loss: 0.8890726640820503, Final Batch Loss: 0.1134546771645546\n",
      "Subject 16, Epoch 424, Loss: 0.9402046054601669, Final Batch Loss: 0.2669413983821869\n",
      "Subject 16, Epoch 425, Loss: 0.9097976833581924, Final Batch Loss: 0.16017308831214905\n",
      "Subject 16, Epoch 426, Loss: 1.0295489728450775, Final Batch Loss: 0.3142033517360687\n",
      "Subject 16, Epoch 427, Loss: 0.7877630442380905, Final Batch Loss: 0.14362020790576935\n",
      "Subject 16, Epoch 428, Loss: 0.945911094546318, Final Batch Loss: 0.27649053931236267\n",
      "Subject 16, Epoch 429, Loss: 0.7876479178667068, Final Batch Loss: 0.14555716514587402\n",
      "Subject 16, Epoch 430, Loss: 0.9334472268819809, Final Batch Loss: 0.2733280658721924\n",
      "Subject 16, Epoch 431, Loss: 1.0122980177402496, Final Batch Loss: 0.23968251049518585\n",
      "Subject 16, Epoch 432, Loss: 0.8998341262340546, Final Batch Loss: 0.17128592729568481\n",
      "Subject 16, Epoch 433, Loss: 0.9862731248140335, Final Batch Loss: 0.29365456104278564\n",
      "Subject 16, Epoch 434, Loss: 0.9403282403945923, Final Batch Loss: 0.09798440337181091\n",
      "Subject 16, Epoch 435, Loss: 0.9190240800380707, Final Batch Loss: 0.21217170357704163\n",
      "Subject 16, Epoch 436, Loss: 0.9038948714733124, Final Batch Loss: 0.2837514877319336\n",
      "Subject 16, Epoch 437, Loss: 0.7956673353910446, Final Batch Loss: 0.12298870086669922\n",
      "Subject 16, Epoch 438, Loss: 0.8671861588954926, Final Batch Loss: 0.23607876896858215\n",
      "Subject 16, Epoch 439, Loss: 0.867110475897789, Final Batch Loss: 0.19622893631458282\n",
      "Subject 16, Epoch 440, Loss: 0.8868334740400314, Final Batch Loss: 0.2985855042934418\n",
      "Subject 16, Epoch 441, Loss: 0.8797638267278671, Final Batch Loss: 0.22073349356651306\n",
      "Subject 16, Epoch 442, Loss: 0.8919739723205566, Final Batch Loss: 0.18061569333076477\n",
      "Subject 16, Epoch 443, Loss: 0.79783795773983, Final Batch Loss: 0.17706416547298431\n",
      "Subject 16, Epoch 444, Loss: 0.965399831533432, Final Batch Loss: 0.2092663198709488\n",
      "Subject 16, Epoch 445, Loss: 0.8397698849439621, Final Batch Loss: 0.17576339840888977\n",
      "Subject 16, Epoch 446, Loss: 0.7881554961204529, Final Batch Loss: 0.15054164826869965\n",
      "Subject 16, Epoch 447, Loss: 0.8907411992549896, Final Batch Loss: 0.2326095849275589\n",
      "Subject 16, Epoch 448, Loss: 0.8263650983572006, Final Batch Loss: 0.13849076628684998\n",
      "Subject 16, Epoch 449, Loss: 0.6952939480543137, Final Batch Loss: 0.14279399812221527\n",
      "Subject 16, Epoch 450, Loss: 0.7726546227931976, Final Batch Loss: 0.23337769508361816\n",
      "Subject 16, Epoch 451, Loss: 0.8835573568940163, Final Batch Loss: 0.1175490990281105\n",
      "Subject 16, Epoch 452, Loss: 0.7146585434675217, Final Batch Loss: 0.17127062380313873\n",
      "Subject 16, Epoch 453, Loss: 1.2031894773244858, Final Batch Loss: 0.3486957848072052\n",
      "Subject 16, Epoch 454, Loss: 0.7597412168979645, Final Batch Loss: 0.20656803250312805\n",
      "Subject 16, Epoch 455, Loss: 0.8685170114040375, Final Batch Loss: 0.22187767922878265\n",
      "Subject 16, Epoch 456, Loss: 0.881226658821106, Final Batch Loss: 0.16783079504966736\n",
      "Subject 16, Epoch 457, Loss: 0.8509395271539688, Final Batch Loss: 0.2691529095172882\n",
      "Subject 16, Epoch 458, Loss: 0.8505418598651886, Final Batch Loss: 0.16096633672714233\n",
      "Subject 16, Epoch 459, Loss: 0.7661328613758087, Final Batch Loss: 0.19663622975349426\n",
      "Subject 16, Epoch 460, Loss: 0.7670784741640091, Final Batch Loss: 0.210152730345726\n",
      "Subject 16, Epoch 461, Loss: 0.9092694222927094, Final Batch Loss: 0.24915720522403717\n",
      "Subject 16, Epoch 462, Loss: 0.9484554529190063, Final Batch Loss: 0.284367173910141\n",
      "Subject 16, Epoch 463, Loss: 0.7140538990497589, Final Batch Loss: 0.07708427309989929\n",
      "Subject 16, Epoch 464, Loss: 0.7908705472946167, Final Batch Loss: 0.2618987560272217\n",
      "Subject 16, Epoch 465, Loss: 0.8864481449127197, Final Batch Loss: 0.3084965646266937\n",
      "Subject 16, Epoch 466, Loss: 0.8079016655683517, Final Batch Loss: 0.32473212480545044\n",
      "Subject 16, Epoch 467, Loss: 0.930477648973465, Final Batch Loss: 0.41600772738456726\n",
      "Subject 16, Epoch 468, Loss: 0.8918980956077576, Final Batch Loss: 0.26251593232154846\n",
      "Subject 16, Epoch 469, Loss: 0.7409253865480423, Final Batch Loss: 0.20068606734275818\n",
      "Subject 16, Epoch 470, Loss: 0.7698229402303696, Final Batch Loss: 0.13779330253601074\n",
      "Subject 16, Epoch 471, Loss: 0.6898211315274239, Final Batch Loss: 0.12072651833295822\n",
      "Subject 16, Epoch 472, Loss: 0.7907304167747498, Final Batch Loss: 0.2243352234363556\n",
      "Subject 16, Epoch 473, Loss: 0.8920139968395233, Final Batch Loss: 0.26408079266548157\n",
      "Subject 16, Epoch 474, Loss: 0.7389465421438217, Final Batch Loss: 0.24169482290744781\n",
      "Subject 16, Epoch 475, Loss: 0.8082267194986343, Final Batch Loss: 0.24313253164291382\n",
      "Subject 16, Epoch 476, Loss: 0.6494687050580978, Final Batch Loss: 0.12279267609119415\n",
      "Subject 16, Epoch 477, Loss: 0.6777287013828754, Final Batch Loss: 0.05668641999363899\n",
      "Subject 16, Epoch 478, Loss: 0.8248155117034912, Final Batch Loss: 0.18368184566497803\n",
      "Subject 16, Epoch 479, Loss: 0.8404414802789688, Final Batch Loss: 0.24395158886909485\n",
      "Subject 16, Epoch 480, Loss: 0.8109563589096069, Final Batch Loss: 0.2456093430519104\n",
      "Subject 16, Epoch 481, Loss: 0.7551785409450531, Final Batch Loss: 0.19297362864017487\n",
      "Subject 16, Epoch 482, Loss: 0.8406536728143692, Final Batch Loss: 0.2652955949306488\n",
      "Subject 16, Epoch 483, Loss: 0.8845061361789703, Final Batch Loss: 0.31069380044937134\n",
      "Subject 16, Epoch 484, Loss: 0.7670158892869949, Final Batch Loss: 0.18078050017356873\n",
      "Subject 16, Epoch 485, Loss: 0.6875637769699097, Final Batch Loss: 0.11260218918323517\n",
      "Subject 16, Epoch 486, Loss: 0.8084267824888229, Final Batch Loss: 0.21302711963653564\n",
      "Subject 16, Epoch 487, Loss: 0.8021706938743591, Final Batch Loss: 0.18308322131633759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 488, Loss: 0.7858042269945145, Final Batch Loss: 0.22987453639507294\n",
      "Subject 16, Epoch 489, Loss: 0.7931453734636307, Final Batch Loss: 0.23099343478679657\n",
      "Subject 16, Epoch 490, Loss: 0.6970968469977379, Final Batch Loss: 0.24674157798290253\n",
      "Subject 16, Epoch 491, Loss: 0.7224805653095245, Final Batch Loss: 0.16401700675487518\n",
      "Subject 16, Epoch 492, Loss: 0.6688013523817062, Final Batch Loss: 0.0656019002199173\n",
      "Subject 16, Epoch 493, Loss: 0.717751756310463, Final Batch Loss: 0.2103213667869568\n",
      "Subject 16, Epoch 494, Loss: 0.7716055065393448, Final Batch Loss: 0.15861624479293823\n",
      "Subject 16, Epoch 495, Loss: 0.8737587928771973, Final Batch Loss: 0.22261500358581543\n",
      "Subject 16, Epoch 496, Loss: 0.7608264535665512, Final Batch Loss: 0.200971782207489\n",
      "Subject 16, Epoch 497, Loss: 0.7950620800256729, Final Batch Loss: 0.2890075445175171\n",
      "Subject 16, Epoch 498, Loss: 0.6324919387698174, Final Batch Loss: 0.17165397107601166\n",
      "Subject 16, Epoch 499, Loss: 1.2472939491271973, Final Batch Loss: 0.7050701379776001\n",
      "Subject 16, Epoch 500, Loss: 0.7743490487337112, Final Batch Loss: 0.16818054020404816\n",
      "Subject 16, Epoch 501, Loss: 0.6557555496692657, Final Batch Loss: 0.11118555068969727\n",
      "Subject 16, Epoch 502, Loss: 0.7721539884805679, Final Batch Loss: 0.33396366238594055\n",
      "Subject 16, Epoch 503, Loss: 0.6930322051048279, Final Batch Loss: 0.1745145320892334\n",
      "Subject 16, Epoch 504, Loss: 0.7349534332752228, Final Batch Loss: 0.17869266867637634\n",
      "Subject 16, Epoch 505, Loss: 0.7837565094232559, Final Batch Loss: 0.22550716996192932\n",
      "Subject 16, Epoch 506, Loss: 0.9456257373094559, Final Batch Loss: 0.4381810128688812\n",
      "Subject 16, Epoch 507, Loss: 0.5594998672604561, Final Batch Loss: 0.11177607625722885\n",
      "Subject 16, Epoch 508, Loss: 0.7894902527332306, Final Batch Loss: 0.17556464672088623\n",
      "Subject 16, Epoch 509, Loss: 0.7705584168434143, Final Batch Loss: 0.15340997278690338\n",
      "Subject 16, Epoch 510, Loss: 0.7606672942638397, Final Batch Loss: 0.1406279355287552\n",
      "Subject 16, Epoch 511, Loss: 0.6760176569223404, Final Batch Loss: 0.09536884725093842\n",
      "Subject 16, Epoch 512, Loss: 0.8033903390169144, Final Batch Loss: 0.2661430239677429\n",
      "Subject 16, Epoch 513, Loss: 0.7928304672241211, Final Batch Loss: 0.12485599517822266\n",
      "Subject 16, Epoch 514, Loss: 0.8030100613832474, Final Batch Loss: 0.16015848517417908\n",
      "Subject 16, Epoch 515, Loss: 0.7137918770313263, Final Batch Loss: 0.17955590784549713\n",
      "Subject 16, Epoch 516, Loss: 0.6918346434831619, Final Batch Loss: 0.1270553171634674\n",
      "Subject 16, Epoch 517, Loss: 0.6310098394751549, Final Batch Loss: 0.09170914441347122\n",
      "Subject 16, Epoch 518, Loss: 0.8016745299100876, Final Batch Loss: 0.1704278141260147\n",
      "Subject 16, Epoch 519, Loss: 0.6447711735963821, Final Batch Loss: 0.09888358414173126\n",
      "Subject 16, Epoch 520, Loss: 0.7712377458810806, Final Batch Loss: 0.17212237417697906\n",
      "Subject 16, Epoch 521, Loss: 0.7409059554338455, Final Batch Loss: 0.16974122822284698\n",
      "Subject 16, Epoch 522, Loss: 0.6389860957860947, Final Batch Loss: 0.11416468024253845\n",
      "Subject 16, Epoch 523, Loss: 0.8458518981933594, Final Batch Loss: 0.15428279340267181\n",
      "Subject 16, Epoch 524, Loss: 0.6842578276991844, Final Batch Loss: 0.22513674199581146\n",
      "Subject 16, Epoch 525, Loss: 0.71107979118824, Final Batch Loss: 0.1676006019115448\n",
      "Subject 16, Epoch 526, Loss: 0.7742467224597931, Final Batch Loss: 0.15012872219085693\n",
      "Subject 16, Epoch 527, Loss: 0.7710419297218323, Final Batch Loss: 0.25727856159210205\n",
      "Subject 16, Epoch 528, Loss: 0.8220395296812057, Final Batch Loss: 0.2669385075569153\n",
      "Subject 16, Epoch 529, Loss: 0.630441889166832, Final Batch Loss: 0.1381809562444687\n",
      "Subject 16, Epoch 530, Loss: 0.8181030750274658, Final Batch Loss: 0.21640333533287048\n",
      "Subject 16, Epoch 531, Loss: 0.8064846023917198, Final Batch Loss: 0.3350679278373718\n",
      "Subject 16, Epoch 532, Loss: 0.6400348246097565, Final Batch Loss: 0.11707954108715057\n",
      "Subject 16, Epoch 533, Loss: 0.8435395881533623, Final Batch Loss: 0.29152143001556396\n",
      "Subject 16, Epoch 534, Loss: 0.6716787815093994, Final Batch Loss: 0.10059855878353119\n",
      "Subject 16, Epoch 535, Loss: 0.6240395754575729, Final Batch Loss: 0.188805490732193\n",
      "Subject 16, Epoch 536, Loss: 0.6003167480230331, Final Batch Loss: 0.07123956084251404\n",
      "Subject 16, Epoch 537, Loss: 0.7550764381885529, Final Batch Loss: 0.17869725823402405\n",
      "Subject 16, Epoch 538, Loss: 0.6870908513665199, Final Batch Loss: 0.11928393691778183\n",
      "Subject 16, Epoch 539, Loss: 0.7661854177713394, Final Batch Loss: 0.15721414983272552\n",
      "Subject 16, Epoch 540, Loss: 0.7750087231397629, Final Batch Loss: 0.18797722458839417\n",
      "Subject 16, Epoch 541, Loss: 0.6842802241444588, Final Batch Loss: 0.06715873628854752\n",
      "Subject 16, Epoch 542, Loss: 0.5426474809646606, Final Batch Loss: 0.15487515926361084\n",
      "Subject 16, Epoch 543, Loss: 0.8098526448011398, Final Batch Loss: 0.3045424818992615\n",
      "Subject 16, Epoch 544, Loss: 0.880272388458252, Final Batch Loss: 0.4339245557785034\n",
      "Subject 16, Epoch 545, Loss: 0.7143452614545822, Final Batch Loss: 0.12915760278701782\n",
      "Subject 16, Epoch 546, Loss: 0.5967415273189545, Final Batch Loss: 0.14919504523277283\n",
      "Subject 16, Epoch 547, Loss: 0.673710435628891, Final Batch Loss: 0.14561328291893005\n",
      "Subject 16, Epoch 548, Loss: 0.7106432542204857, Final Batch Loss: 0.11527203768491745\n",
      "Subject 16, Epoch 549, Loss: 0.7683418244123459, Final Batch Loss: 0.2501906752586365\n",
      "Subject 16, Epoch 550, Loss: 0.9117480218410492, Final Batch Loss: 0.24208232760429382\n",
      "Subject 16, Epoch 551, Loss: 0.734198123216629, Final Batch Loss: 0.14739325642585754\n",
      "Subject 16, Epoch 552, Loss: 0.8163880109786987, Final Batch Loss: 0.24496997892856598\n",
      "Subject 16, Epoch 553, Loss: 0.6407782360911369, Final Batch Loss: 0.10755978524684906\n",
      "Subject 16, Epoch 554, Loss: 0.8162676244974136, Final Batch Loss: 0.25615811347961426\n",
      "Subject 16, Epoch 555, Loss: 0.6930719763040543, Final Batch Loss: 0.17392966151237488\n",
      "Subject 16, Epoch 556, Loss: 0.6709698289632797, Final Batch Loss: 0.12575167417526245\n",
      "Subject 16, Epoch 557, Loss: 0.6136124655604362, Final Batch Loss: 0.055970244109630585\n",
      "Subject 16, Epoch 558, Loss: 0.6222884580492973, Final Batch Loss: 0.07523094862699509\n",
      "Subject 16, Epoch 559, Loss: 0.7679108530282974, Final Batch Loss: 0.2163638174533844\n",
      "Subject 16, Epoch 560, Loss: 0.7603742927312851, Final Batch Loss: 0.21509161591529846\n",
      "Subject 16, Epoch 561, Loss: 0.7158508151769638, Final Batch Loss: 0.13198450207710266\n",
      "Subject 16, Epoch 562, Loss: 0.5758558288216591, Final Batch Loss: 0.12161282449960709\n",
      "Subject 16, Epoch 563, Loss: 0.850997269153595, Final Batch Loss: 0.2688615918159485\n",
      "Subject 16, Epoch 564, Loss: 0.6213221698999405, Final Batch Loss: 0.07138463854789734\n",
      "Subject 16, Epoch 565, Loss: 0.6123786829411983, Final Batch Loss: 0.040623556822538376\n",
      "Subject 16, Epoch 566, Loss: 0.8565221130847931, Final Batch Loss: 0.35875117778778076\n",
      "Subject 16, Epoch 567, Loss: 0.6524071395397186, Final Batch Loss: 0.09651350975036621\n",
      "Subject 16, Epoch 568, Loss: 0.7084997147321701, Final Batch Loss: 0.21992497146129608\n",
      "Subject 16, Epoch 569, Loss: 0.6015798151493073, Final Batch Loss: 0.09772217273712158\n",
      "Subject 16, Epoch 570, Loss: 0.5493354350328445, Final Batch Loss: 0.07502475380897522\n",
      "Subject 16, Epoch 571, Loss: 0.6723799481987953, Final Batch Loss: 0.11419058591127396\n",
      "Subject 16, Epoch 572, Loss: 0.621340848505497, Final Batch Loss: 0.07815299183130264\n",
      "Subject 16, Epoch 573, Loss: 0.5251495838165283, Final Batch Loss: 0.08927815407514572\n",
      "Subject 16, Epoch 574, Loss: 0.856543704867363, Final Batch Loss: 0.21646390855312347\n",
      "Subject 16, Epoch 575, Loss: 0.8590681403875351, Final Batch Loss: 0.2668442726135254\n",
      "Subject 16, Epoch 576, Loss: 0.6061312705278397, Final Batch Loss: 0.14342817664146423\n",
      "Subject 16, Epoch 577, Loss: 0.8447763174772263, Final Batch Loss: 0.3164893090724945\n",
      "Subject 16, Epoch 578, Loss: 0.5959524437785149, Final Batch Loss: 0.11793573945760727\n",
      "Subject 16, Epoch 579, Loss: 0.7002193182706833, Final Batch Loss: 0.1987360119819641\n",
      "Subject 16, Epoch 580, Loss: 0.7200205028057098, Final Batch Loss: 0.16440033912658691\n",
      "Subject 16, Epoch 581, Loss: 0.6181500926613808, Final Batch Loss: 0.08325740694999695\n",
      "Subject 16, Epoch 582, Loss: 0.6791000887751579, Final Batch Loss: 0.08830384165048599\n",
      "Subject 16, Epoch 583, Loss: 0.5895792990922928, Final Batch Loss: 0.13130736351013184\n",
      "Subject 16, Epoch 584, Loss: 0.6857268661260605, Final Batch Loss: 0.2324330359697342\n",
      "Subject 16, Epoch 585, Loss: 0.8460694551467896, Final Batch Loss: 0.3020824193954468\n",
      "Subject 16, Epoch 586, Loss: 0.5831777304410934, Final Batch Loss: 0.13993723690509796\n",
      "Subject 16, Epoch 587, Loss: 0.9935525804758072, Final Batch Loss: 0.47606438398361206\n",
      "Subject 16, Epoch 588, Loss: 0.6026440486311913, Final Batch Loss: 0.12290395051240921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 589, Loss: 0.6725846081972122, Final Batch Loss: 0.176410973072052\n",
      "Subject 16, Epoch 590, Loss: 0.6543725878000259, Final Batch Loss: 0.20159851014614105\n",
      "Subject 16, Epoch 591, Loss: 0.6894969195127487, Final Batch Loss: 0.16663767397403717\n",
      "Subject 16, Epoch 592, Loss: 0.7455878332257271, Final Batch Loss: 0.09817112237215042\n",
      "Subject 16, Epoch 593, Loss: 0.6031039357185364, Final Batch Loss: 0.15726040303707123\n",
      "Subject 16, Epoch 594, Loss: 0.608728751540184, Final Batch Loss: 0.14338284730911255\n",
      "Subject 16, Epoch 595, Loss: 0.6079120561480522, Final Batch Loss: 0.09680335968732834\n",
      "Subject 16, Epoch 596, Loss: 0.6546544879674911, Final Batch Loss: 0.14599069952964783\n",
      "Subject 16, Epoch 597, Loss: 0.6119389459490776, Final Batch Loss: 0.2001320868730545\n",
      "Subject 16, Epoch 598, Loss: 0.6599039435386658, Final Batch Loss: 0.1560489982366562\n",
      "Subject 16, Epoch 599, Loss: 0.671986311674118, Final Batch Loss: 0.1319030523300171\n",
      "Subject 16, Epoch 600, Loss: 0.7301319092512131, Final Batch Loss: 0.32192638516426086\n",
      "Subject 16, Epoch 601, Loss: 0.7882070615887642, Final Batch Loss: 0.3362058103084564\n",
      "Subject 16, Epoch 602, Loss: 0.6224803179502487, Final Batch Loss: 0.16802741587162018\n",
      "Subject 16, Epoch 603, Loss: 0.5833751112222672, Final Batch Loss: 0.14584797620773315\n",
      "Subject 16, Epoch 604, Loss: 0.7412624508142471, Final Batch Loss: 0.24555113911628723\n",
      "Subject 16, Epoch 605, Loss: 0.6578743159770966, Final Batch Loss: 0.150746151804924\n",
      "Subject 16, Epoch 606, Loss: 0.5952582806348801, Final Batch Loss: 0.11395099759101868\n",
      "Subject 16, Epoch 607, Loss: 0.7678691297769547, Final Batch Loss: 0.20617996156215668\n",
      "Subject 16, Epoch 608, Loss: 0.8171308934688568, Final Batch Loss: 0.2557659149169922\n",
      "Subject 16, Epoch 609, Loss: 0.6276890784502029, Final Batch Loss: 0.11346502602100372\n",
      "Subject 16, Epoch 610, Loss: 0.6157477870583534, Final Batch Loss: 0.22121562063694\n",
      "Subject 16, Epoch 611, Loss: 0.6214286237955093, Final Batch Loss: 0.1522386223077774\n",
      "Subject 16, Epoch 612, Loss: 0.5611587464809418, Final Batch Loss: 0.16210629045963287\n",
      "Subject 16, Epoch 613, Loss: 0.7400089129805565, Final Batch Loss: 0.3053317070007324\n",
      "Subject 16, Epoch 614, Loss: 0.627773817628622, Final Batch Loss: 0.05925574526190758\n",
      "Subject 16, Epoch 615, Loss: 0.5660953596234322, Final Batch Loss: 0.13064900040626526\n",
      "Subject 16, Epoch 616, Loss: 0.6837108731269836, Final Batch Loss: 0.20092390477657318\n",
      "Subject 16, Epoch 617, Loss: 0.5370199829339981, Final Batch Loss: 0.06446077674627304\n",
      "Subject 16, Epoch 618, Loss: 0.7303272634744644, Final Batch Loss: 0.14622856676578522\n",
      "Subject 16, Epoch 619, Loss: 0.6621658802032471, Final Batch Loss: 0.15408819913864136\n",
      "Subject 16, Epoch 620, Loss: 0.6548783630132675, Final Batch Loss: 0.1699945628643036\n",
      "Subject 16, Epoch 621, Loss: 0.8373344540596008, Final Batch Loss: 0.33215731382369995\n",
      "Subject 16, Epoch 622, Loss: 0.7217054888606071, Final Batch Loss: 0.2693960666656494\n",
      "Subject 16, Epoch 623, Loss: 0.6215850487351418, Final Batch Loss: 0.13510359823703766\n",
      "Subject 16, Epoch 624, Loss: 0.5657954588532448, Final Batch Loss: 0.18816599249839783\n",
      "Subject 16, Epoch 625, Loss: 0.6105220019817352, Final Batch Loss: 0.14940966665744781\n",
      "Subject 16, Epoch 626, Loss: 0.6936332136392593, Final Batch Loss: 0.23290663957595825\n",
      "Subject 16, Epoch 627, Loss: 0.7475750669836998, Final Batch Loss: 0.33377838134765625\n",
      "Subject 16, Epoch 628, Loss: 0.6592925041913986, Final Batch Loss: 0.20797701179981232\n",
      "Subject 16, Epoch 629, Loss: 0.5385118424892426, Final Batch Loss: 0.06280165165662766\n",
      "Subject 16, Epoch 630, Loss: 0.6427296027541161, Final Batch Loss: 0.1692270189523697\n",
      "Subject 16, Epoch 631, Loss: 0.6447156667709351, Final Batch Loss: 0.13062655925750732\n",
      "Subject 16, Epoch 632, Loss: 0.6354116722941399, Final Batch Loss: 0.1173526719212532\n",
      "Subject 16, Epoch 633, Loss: 0.5720376893877983, Final Batch Loss: 0.1397324502468109\n",
      "Subject 16, Epoch 634, Loss: 0.6402629241347313, Final Batch Loss: 0.16066695749759674\n",
      "Subject 16, Epoch 635, Loss: 0.684060700237751, Final Batch Loss: 0.10267465561628342\n",
      "Subject 16, Epoch 636, Loss: 0.46405669301748276, Final Batch Loss: 0.05880054086446762\n",
      "Subject 16, Epoch 637, Loss: 0.6337772607803345, Final Batch Loss: 0.18594393134117126\n",
      "Subject 16, Epoch 638, Loss: 0.5549562871456146, Final Batch Loss: 0.1342102289199829\n",
      "Subject 16, Epoch 639, Loss: 0.5847869217395782, Final Batch Loss: 0.10455980896949768\n",
      "Subject 16, Epoch 640, Loss: 0.540462888777256, Final Batch Loss: 0.11460115760564804\n",
      "Subject 16, Epoch 641, Loss: 0.5822287350893021, Final Batch Loss: 0.15803170204162598\n",
      "Subject 16, Epoch 642, Loss: 0.6513323709368706, Final Batch Loss: 0.12546204030513763\n",
      "Subject 16, Epoch 643, Loss: 0.7064206004142761, Final Batch Loss: 0.31325215101242065\n",
      "Subject 16, Epoch 644, Loss: 0.7250820100307465, Final Batch Loss: 0.1594545841217041\n",
      "Subject 16, Epoch 645, Loss: 0.48885220289230347, Final Batch Loss: 0.03221156448125839\n",
      "Subject 16, Epoch 646, Loss: 0.5742152556777, Final Batch Loss: 0.16363494098186493\n",
      "Subject 16, Epoch 647, Loss: 0.7093126252293587, Final Batch Loss: 0.08211832493543625\n",
      "Subject 16, Epoch 648, Loss: 0.59657783806324, Final Batch Loss: 0.14386704564094543\n",
      "Subject 16, Epoch 649, Loss: 0.6163256391882896, Final Batch Loss: 0.26374608278274536\n",
      "Subject 16, Epoch 650, Loss: 0.5223450660705566, Final Batch Loss: 0.10062554478645325\n",
      "Subject 16, Epoch 651, Loss: 0.7190396934747696, Final Batch Loss: 0.17930138111114502\n",
      "Subject 16, Epoch 652, Loss: 0.5194989889860153, Final Batch Loss: 0.1557949036359787\n",
      "Subject 16, Epoch 653, Loss: 0.6896913945674896, Final Batch Loss: 0.16611213982105255\n",
      "Subject 16, Epoch 654, Loss: 0.5521120429039001, Final Batch Loss: 0.12995298206806183\n",
      "Subject 16, Epoch 655, Loss: 0.5758517012000084, Final Batch Loss: 0.0641629621386528\n",
      "Subject 16, Epoch 656, Loss: 0.6611721441149712, Final Batch Loss: 0.18912048637866974\n",
      "Subject 16, Epoch 657, Loss: 0.6741129830479622, Final Batch Loss: 0.0802808627486229\n",
      "Subject 16, Epoch 658, Loss: 0.6093377321958542, Final Batch Loss: 0.16463887691497803\n",
      "Subject 16, Epoch 659, Loss: 0.5873652696609497, Final Batch Loss: 0.10600080341100693\n",
      "Subject 16, Epoch 660, Loss: 0.6742812693119049, Final Batch Loss: 0.3232245445251465\n",
      "Subject 16, Epoch 661, Loss: 0.5849745497107506, Final Batch Loss: 0.1783384084701538\n",
      "Subject 16, Epoch 662, Loss: 0.7079888582229614, Final Batch Loss: 0.20534569025039673\n",
      "Subject 16, Epoch 663, Loss: 0.6206726059317589, Final Batch Loss: 0.06688448041677475\n",
      "Subject 16, Epoch 664, Loss: 0.506822407245636, Final Batch Loss: 0.12625698745250702\n",
      "Subject 16, Epoch 665, Loss: 0.5392986685037613, Final Batch Loss: 0.10675299167633057\n",
      "Subject 16, Epoch 666, Loss: 0.632650576531887, Final Batch Loss: 0.1119828149676323\n",
      "Subject 16, Epoch 667, Loss: 0.5991808548569679, Final Batch Loss: 0.17207598686218262\n",
      "Subject 16, Epoch 668, Loss: 0.5887275189161301, Final Batch Loss: 0.09865140914916992\n",
      "Subject 16, Epoch 669, Loss: 0.6256751902401447, Final Batch Loss: 0.057565707713365555\n",
      "Subject 16, Epoch 670, Loss: 0.5063595026731491, Final Batch Loss: 0.08174003660678864\n",
      "Subject 16, Epoch 671, Loss: 0.5300665646791458, Final Batch Loss: 0.16288527846336365\n",
      "Subject 16, Epoch 672, Loss: 0.6206016093492508, Final Batch Loss: 0.1592923104763031\n",
      "Subject 16, Epoch 673, Loss: 0.49738864600658417, Final Batch Loss: 0.07516276091337204\n",
      "Subject 16, Epoch 674, Loss: 0.6205201521515846, Final Batch Loss: 0.11197984218597412\n",
      "Subject 16, Epoch 675, Loss: 0.5804347544908524, Final Batch Loss: 0.11702264845371246\n",
      "Subject 16, Epoch 676, Loss: 0.5363659262657166, Final Batch Loss: 0.12445305287837982\n",
      "Subject 16, Epoch 677, Loss: 0.6680470332503319, Final Batch Loss: 0.22263062000274658\n",
      "Subject 16, Epoch 678, Loss: 0.5073591992259026, Final Batch Loss: 0.09388389438390732\n",
      "Subject 16, Epoch 679, Loss: 0.445170134305954, Final Batch Loss: 0.06549850851297379\n",
      "Subject 16, Epoch 680, Loss: 0.6711153462529182, Final Batch Loss: 0.2392682135105133\n",
      "Subject 16, Epoch 681, Loss: 0.5315931215882301, Final Batch Loss: 0.16234083473682404\n",
      "Subject 16, Epoch 682, Loss: 0.5788937956094742, Final Batch Loss: 0.21539779007434845\n",
      "Subject 16, Epoch 683, Loss: 0.7175118029117584, Final Batch Loss: 0.3389228880405426\n",
      "Subject 16, Epoch 684, Loss: 0.6493851542472839, Final Batch Loss: 0.12701186537742615\n",
      "Subject 16, Epoch 685, Loss: 0.5492944940924644, Final Batch Loss: 0.10679938644170761\n",
      "Subject 16, Epoch 686, Loss: 0.5905039831995964, Final Batch Loss: 0.16662359237670898\n",
      "Subject 16, Epoch 687, Loss: 0.5340009704232216, Final Batch Loss: 0.07476596534252167\n",
      "Subject 16, Epoch 688, Loss: 0.5566650032997131, Final Batch Loss: 0.16990725696086884\n",
      "Subject 16, Epoch 689, Loss: 0.5178375318646431, Final Batch Loss: 0.19720815122127533\n",
      "Subject 16, Epoch 690, Loss: 0.6303515881299973, Final Batch Loss: 0.22662262618541718\n",
      "Subject 16, Epoch 691, Loss: 0.6666538715362549, Final Batch Loss: 0.13968802988529205\n",
      "Subject 16, Epoch 692, Loss: 0.5901645869016647, Final Batch Loss: 0.09461428225040436\n",
      "Subject 16, Epoch 693, Loss: 0.6715086624026299, Final Batch Loss: 0.3089471161365509\n",
      "Subject 16, Epoch 694, Loss: 0.6847815737128258, Final Batch Loss: 0.22020304203033447\n",
      "Subject 16, Epoch 695, Loss: 0.4468122310936451, Final Batch Loss: 0.04993930831551552\n",
      "Subject 16, Epoch 696, Loss: 0.6155900359153748, Final Batch Loss: 0.05726894736289978\n",
      "Subject 16, Epoch 697, Loss: 0.7650714591145515, Final Batch Loss: 0.26725491881370544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 698, Loss: 0.6138684898614883, Final Batch Loss: 0.15749546885490417\n",
      "Subject 16, Epoch 699, Loss: 0.6793103069067001, Final Batch Loss: 0.2750382125377655\n",
      "Subject 16, Epoch 700, Loss: 0.6401541009545326, Final Batch Loss: 0.047862790524959564\n",
      "Subject 16, Epoch 701, Loss: 0.5060014724731445, Final Batch Loss: 0.09796945750713348\n",
      "Subject 16, Epoch 702, Loss: 0.5657256171107292, Final Batch Loss: 0.07067086547613144\n",
      "Subject 16, Epoch 703, Loss: 0.5371533930301666, Final Batch Loss: 0.12591974437236786\n",
      "Subject 16, Epoch 704, Loss: 0.55994663387537, Final Batch Loss: 0.2234712392091751\n",
      "Subject 16, Epoch 705, Loss: 0.6221011281013489, Final Batch Loss: 0.18251189589500427\n",
      "Subject 16, Epoch 706, Loss: 0.6355680227279663, Final Batch Loss: 0.16981203854084015\n",
      "Subject 16, Epoch 707, Loss: 0.587226964533329, Final Batch Loss: 0.10474390536546707\n",
      "Subject 16, Epoch 708, Loss: 0.7370246350765228, Final Batch Loss: 0.26099443435668945\n",
      "Subject 16, Epoch 709, Loss: 0.5009302794933319, Final Batch Loss: 0.07919435948133469\n",
      "Subject 16, Epoch 710, Loss: 0.5095381289720535, Final Batch Loss: 0.1577620804309845\n",
      "Subject 16, Epoch 711, Loss: 0.5741029530763626, Final Batch Loss: 0.12874862551689148\n",
      "Subject 16, Epoch 712, Loss: 0.7441000267863274, Final Batch Loss: 0.18430152535438538\n",
      "Subject 16, Epoch 713, Loss: 0.554029181599617, Final Batch Loss: 0.082674540579319\n",
      "Subject 16, Epoch 714, Loss: 0.4606761559844017, Final Batch Loss: 0.08296997845172882\n",
      "Subject 16, Epoch 715, Loss: 0.5232326090335846, Final Batch Loss: 0.1287938356399536\n",
      "Subject 16, Epoch 716, Loss: 0.5908277630805969, Final Batch Loss: 0.08833204954862595\n",
      "Subject 16, Epoch 717, Loss: 0.7329673618078232, Final Batch Loss: 0.22428615391254425\n",
      "Subject 16, Epoch 718, Loss: 0.5065259002149105, Final Batch Loss: 0.052608978003263474\n",
      "Subject 16, Epoch 719, Loss: 0.49178875982761383, Final Batch Loss: 0.16067129373550415\n",
      "Subject 16, Epoch 720, Loss: 0.6673320978879929, Final Batch Loss: 0.23849423229694366\n",
      "Subject 16, Epoch 721, Loss: 0.6051258072257042, Final Batch Loss: 0.18314127624034882\n",
      "Subject 16, Epoch 722, Loss: 0.475515715777874, Final Batch Loss: 0.11055701225996017\n",
      "Subject 16, Epoch 723, Loss: 0.6363067477941513, Final Batch Loss: 0.1858462691307068\n",
      "Subject 16, Epoch 724, Loss: 0.5672957748174667, Final Batch Loss: 0.23100332915782928\n",
      "Subject 16, Epoch 725, Loss: 0.6190565228462219, Final Batch Loss: 0.13305726647377014\n",
      "Subject 16, Epoch 726, Loss: 0.4885246604681015, Final Batch Loss: 0.11093887686729431\n",
      "Subject 16, Epoch 727, Loss: 0.47585805505514145, Final Batch Loss: 0.056576937437057495\n",
      "Subject 16, Epoch 728, Loss: 0.41313450783491135, Final Batch Loss: 0.034000374376773834\n",
      "Subject 16, Epoch 729, Loss: 0.5252720899879932, Final Batch Loss: 0.06008129194378853\n",
      "Subject 16, Epoch 730, Loss: 0.6947146058082581, Final Batch Loss: 0.26395830512046814\n",
      "Subject 16, Epoch 731, Loss: 0.5899503380060196, Final Batch Loss: 0.1554379165172577\n",
      "Subject 16, Epoch 732, Loss: 0.6474626362323761, Final Batch Loss: 0.22898712754249573\n",
      "Subject 16, Epoch 733, Loss: 0.5841057933866978, Final Batch Loss: 0.21993151307106018\n",
      "Subject 16, Epoch 734, Loss: 0.5768564715981483, Final Batch Loss: 0.18864195048809052\n",
      "Subject 16, Epoch 735, Loss: 0.5083868876099586, Final Batch Loss: 0.07112263888120651\n",
      "Subject 16, Epoch 736, Loss: 0.467678502202034, Final Batch Loss: 0.06724774837493896\n",
      "Subject 16, Epoch 737, Loss: 0.6782442703843117, Final Batch Loss: 0.3339892625808716\n",
      "Subject 16, Epoch 738, Loss: 0.6349957138299942, Final Batch Loss: 0.18685881793498993\n",
      "Subject 16, Epoch 739, Loss: 0.8064184039831161, Final Batch Loss: 0.2790883183479309\n",
      "Subject 16, Epoch 740, Loss: 0.7786083817481995, Final Batch Loss: 0.247741237282753\n",
      "Subject 16, Epoch 741, Loss: 0.629501149058342, Final Batch Loss: 0.10908372700214386\n",
      "Subject 16, Epoch 742, Loss: 0.5419845581054688, Final Batch Loss: 0.09152784943580627\n",
      "Subject 16, Epoch 743, Loss: 0.49931375682353973, Final Batch Loss: 0.10772877931594849\n",
      "Subject 16, Epoch 744, Loss: 0.5178967341780663, Final Batch Loss: 0.1440136581659317\n",
      "Subject 16, Epoch 745, Loss: 0.5229964777827263, Final Batch Loss: 0.13170209527015686\n",
      "Subject 16, Epoch 746, Loss: 0.5281317234039307, Final Batch Loss: 0.10605047643184662\n",
      "Subject 16, Epoch 747, Loss: 0.43560105562210083, Final Batch Loss: 0.12184523046016693\n",
      "Subject 16, Epoch 748, Loss: 0.5388134345412254, Final Batch Loss: 0.0932178646326065\n",
      "Subject 16, Epoch 749, Loss: 0.5537570789456367, Final Batch Loss: 0.14887921512126923\n",
      "Subject 16, Epoch 750, Loss: 0.5432375445961952, Final Batch Loss: 0.18259358406066895\n",
      "Subject 16, Epoch 751, Loss: 0.47477416694164276, Final Batch Loss: 0.12658239901065826\n",
      "Subject 16, Epoch 752, Loss: 0.47520675510168076, Final Batch Loss: 0.07536205649375916\n",
      "Subject 16, Epoch 753, Loss: 0.5608347058296204, Final Batch Loss: 0.12835593521595\n",
      "Subject 16, Epoch 754, Loss: 0.6577262058854103, Final Batch Loss: 0.280071496963501\n",
      "Subject 16, Epoch 755, Loss: 0.6782746464014053, Final Batch Loss: 0.16213756799697876\n",
      "Subject 16, Epoch 756, Loss: 0.6596211716532707, Final Batch Loss: 0.14369985461235046\n",
      "Subject 16, Epoch 757, Loss: 0.579540342092514, Final Batch Loss: 0.23299822211265564\n",
      "Subject 16, Epoch 758, Loss: 0.48993518576025963, Final Batch Loss: 0.07407291233539581\n",
      "Subject 16, Epoch 759, Loss: 0.5937582701444626, Final Batch Loss: 0.16232134401798248\n",
      "Subject 16, Epoch 760, Loss: 0.5531159788370132, Final Batch Loss: 0.0882866308093071\n",
      "Subject 16, Epoch 761, Loss: 0.600659679621458, Final Batch Loss: 0.24351203441619873\n",
      "Subject 16, Epoch 762, Loss: 0.4925314113497734, Final Batch Loss: 0.08050024509429932\n",
      "Subject 16, Epoch 763, Loss: 0.4450033903121948, Final Batch Loss: 0.07066803425550461\n",
      "Subject 16, Epoch 764, Loss: 0.4939424395561218, Final Batch Loss: 0.130648672580719\n",
      "Subject 16, Epoch 765, Loss: 0.5632164478302002, Final Batch Loss: 0.10495314002037048\n",
      "Subject 16, Epoch 766, Loss: 0.46746019646525383, Final Batch Loss: 0.057588446885347366\n",
      "Subject 16, Epoch 767, Loss: 0.4535346105694771, Final Batch Loss: 0.08009371906518936\n",
      "Subject 16, Epoch 768, Loss: 0.42077165096998215, Final Batch Loss: 0.10520083457231522\n",
      "Subject 16, Epoch 769, Loss: 0.494969479739666, Final Batch Loss: 0.15536487102508545\n",
      "Subject 16, Epoch 770, Loss: 0.38223429024219513, Final Batch Loss: 0.05891745537519455\n",
      "Subject 16, Epoch 771, Loss: 0.5368061512708664, Final Batch Loss: 0.21090656518936157\n",
      "Subject 16, Epoch 772, Loss: 0.5280965194106102, Final Batch Loss: 0.07026198506355286\n",
      "Subject 16, Epoch 773, Loss: 0.5467938780784607, Final Batch Loss: 0.20095287263393402\n",
      "Subject 16, Epoch 774, Loss: 0.5914329066872597, Final Batch Loss: 0.13550063967704773\n",
      "Subject 16, Epoch 775, Loss: 0.44897814095020294, Final Batch Loss: 0.04974701255559921\n",
      "Subject 16, Epoch 776, Loss: 0.4260018691420555, Final Batch Loss: 0.08077822625637054\n",
      "Subject 16, Epoch 777, Loss: 0.40409745275974274, Final Batch Loss: 0.04290693625807762\n",
      "Subject 16, Epoch 778, Loss: 0.651229303330183, Final Batch Loss: 0.3644426465034485\n",
      "Subject 16, Epoch 779, Loss: 0.5053965747356415, Final Batch Loss: 0.06931110471487045\n",
      "Subject 16, Epoch 780, Loss: 0.5218088962137699, Final Batch Loss: 0.053607333451509476\n",
      "Subject 16, Epoch 781, Loss: 0.6339068189263344, Final Batch Loss: 0.3229183554649353\n",
      "Subject 16, Epoch 782, Loss: 0.5554814413189888, Final Batch Loss: 0.13221096992492676\n",
      "Subject 16, Epoch 783, Loss: 0.39793410152196884, Final Batch Loss: 0.0495985709130764\n",
      "Subject 16, Epoch 784, Loss: 0.42063186317682266, Final Batch Loss: 0.11168825626373291\n",
      "Subject 16, Epoch 785, Loss: 0.5908984616398811, Final Batch Loss: 0.17747078835964203\n",
      "Subject 16, Epoch 786, Loss: 0.4619102329015732, Final Batch Loss: 0.1278548240661621\n",
      "Subject 16, Epoch 787, Loss: 0.6314035654067993, Final Batch Loss: 0.2550452947616577\n",
      "Subject 16, Epoch 788, Loss: 0.5262971334159374, Final Batch Loss: 0.12729020416736603\n",
      "Subject 16, Epoch 789, Loss: 0.390304870903492, Final Batch Loss: 0.03247833251953125\n",
      "Subject 16, Epoch 790, Loss: 0.5068313181400299, Final Batch Loss: 0.13393376767635345\n",
      "Subject 16, Epoch 791, Loss: 0.38419071584939957, Final Batch Loss: 0.03368475288152695\n",
      "Subject 16, Epoch 792, Loss: 0.45379652082920074, Final Batch Loss: 0.06296662241220474\n",
      "Subject 16, Epoch 793, Loss: 0.65940011292696, Final Batch Loss: 0.15410788357257843\n",
      "Subject 16, Epoch 794, Loss: 0.750274907797575, Final Batch Loss: 0.39230072498321533\n",
      "Subject 16, Epoch 795, Loss: 0.5773217901587486, Final Batch Loss: 0.05643966794013977\n",
      "Subject 16, Epoch 796, Loss: 0.6173568814992905, Final Batch Loss: 0.19296333193778992\n",
      "Subject 16, Epoch 797, Loss: 0.4979374632239342, Final Batch Loss: 0.06102153658866882\n",
      "Subject 16, Epoch 798, Loss: 0.49527648091316223, Final Batch Loss: 0.2102244645357132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 799, Loss: 0.4519990086555481, Final Batch Loss: 0.13431325554847717\n",
      "Subject 16, Epoch 800, Loss: 0.35772884264588356, Final Batch Loss: 0.06451959908008575\n",
      "Subject 16, Epoch 801, Loss: 0.4491603523492813, Final Batch Loss: 0.11774003505706787\n",
      "Subject 16, Epoch 802, Loss: 0.39914440736174583, Final Batch Loss: 0.05810258910059929\n",
      "Subject 16, Epoch 803, Loss: 0.5847378820180893, Final Batch Loss: 0.1652231514453888\n",
      "Subject 16, Epoch 804, Loss: 0.42545272782444954, Final Batch Loss: 0.06038514897227287\n",
      "Subject 16, Epoch 805, Loss: 0.4003812335431576, Final Batch Loss: 0.03939182683825493\n",
      "Subject 16, Epoch 806, Loss: 0.47259581834077835, Final Batch Loss: 0.10771355777978897\n",
      "Subject 16, Epoch 807, Loss: 0.6007219552993774, Final Batch Loss: 0.17295262217521667\n",
      "Subject 16, Epoch 808, Loss: 0.5101164877414703, Final Batch Loss: 0.11611199378967285\n",
      "Subject 16, Epoch 809, Loss: 0.5352449864149094, Final Batch Loss: 0.0968865156173706\n",
      "Subject 16, Epoch 810, Loss: 0.48740115761756897, Final Batch Loss: 0.0817561224102974\n",
      "Subject 16, Epoch 811, Loss: 0.368692334741354, Final Batch Loss: 0.10536502301692963\n",
      "Subject 16, Epoch 812, Loss: 0.48258958756923676, Final Batch Loss: 0.11941882967948914\n",
      "Subject 16, Epoch 813, Loss: 0.4549543932080269, Final Batch Loss: 0.13378633558750153\n",
      "Subject 16, Epoch 814, Loss: 0.4228856638073921, Final Batch Loss: 0.08418886363506317\n",
      "Subject 16, Epoch 815, Loss: 0.549502532929182, Final Batch Loss: 0.04112520441412926\n",
      "Subject 16, Epoch 816, Loss: 0.5052598491311073, Final Batch Loss: 0.11807404458522797\n",
      "Subject 16, Epoch 817, Loss: 0.44686415791511536, Final Batch Loss: 0.09605738520622253\n",
      "Subject 16, Epoch 818, Loss: 0.4682197719812393, Final Batch Loss: 0.06720494478940964\n",
      "Subject 16, Epoch 819, Loss: 0.4372768774628639, Final Batch Loss: 0.0924101322889328\n",
      "Subject 16, Epoch 820, Loss: 0.4230482503771782, Final Batch Loss: 0.07162324339151382\n",
      "Subject 16, Epoch 821, Loss: 0.45856069028377533, Final Batch Loss: 0.14618335664272308\n",
      "Subject 16, Epoch 822, Loss: 0.3665529415011406, Final Batch Loss: 0.0916891098022461\n",
      "Subject 16, Epoch 823, Loss: 0.4819563552737236, Final Batch Loss: 0.0857897698879242\n",
      "Subject 16, Epoch 824, Loss: 0.6550119370222092, Final Batch Loss: 0.24662144482135773\n",
      "Subject 16, Epoch 825, Loss: 0.5260185599327087, Final Batch Loss: 0.22764231264591217\n",
      "Subject 16, Epoch 826, Loss: 0.41056110709905624, Final Batch Loss: 0.1007847785949707\n",
      "Subject 16, Epoch 827, Loss: 0.48316752910614014, Final Batch Loss: 0.1653212308883667\n",
      "Subject 16, Epoch 828, Loss: 0.45218291878700256, Final Batch Loss: 0.03397302329540253\n",
      "Subject 16, Epoch 829, Loss: 0.35802748426795006, Final Batch Loss: 0.11326881498098373\n",
      "Subject 16, Epoch 830, Loss: 0.49566978961229324, Final Batch Loss: 0.13983213901519775\n",
      "Subject 16, Epoch 831, Loss: 0.43183163553476334, Final Batch Loss: 0.03602156043052673\n",
      "Subject 16, Epoch 832, Loss: 0.6556781530380249, Final Batch Loss: 0.1397007554769516\n",
      "Subject 16, Epoch 833, Loss: 0.3511747885495424, Final Batch Loss: 0.022023463621735573\n",
      "Subject 16, Epoch 834, Loss: 0.4494699090719223, Final Batch Loss: 0.08617912977933884\n",
      "Subject 16, Epoch 835, Loss: 0.5212429761886597, Final Batch Loss: 0.06369505077600479\n",
      "Subject 16, Epoch 836, Loss: 0.4412432163953781, Final Batch Loss: 0.09288091957569122\n",
      "Subject 16, Epoch 837, Loss: 0.5593776181340218, Final Batch Loss: 0.23100388050079346\n",
      "Subject 16, Epoch 838, Loss: 0.4850846529006958, Final Batch Loss: 0.09101314842700958\n",
      "Subject 16, Epoch 839, Loss: 0.4945359192788601, Final Batch Loss: 0.046472471207380295\n",
      "Subject 16, Epoch 840, Loss: 0.36106936633586884, Final Batch Loss: 0.06127646565437317\n",
      "Subject 16, Epoch 841, Loss: 0.3487936705350876, Final Batch Loss: 0.11023327708244324\n",
      "Subject 16, Epoch 842, Loss: 0.3817201070487499, Final Batch Loss: 0.06698711216449738\n",
      "Subject 16, Epoch 843, Loss: 0.5386122912168503, Final Batch Loss: 0.1433214396238327\n",
      "Subject 16, Epoch 844, Loss: 0.4947410300374031, Final Batch Loss: 0.14171752333641052\n",
      "Subject 16, Epoch 845, Loss: 0.6247126162052155, Final Batch Loss: 0.12788449227809906\n",
      "Subject 16, Epoch 846, Loss: 0.5253315269947052, Final Batch Loss: 0.17709587514400482\n",
      "Subject 16, Epoch 847, Loss: 0.6838498562574387, Final Batch Loss: 0.2125074714422226\n",
      "Subject 16, Epoch 848, Loss: 0.3843485377728939, Final Batch Loss: 0.05917666479945183\n",
      "Subject 16, Epoch 849, Loss: 0.44694211333990097, Final Batch Loss: 0.14514011144638062\n",
      "Subject 16, Epoch 850, Loss: 0.5792822688817978, Final Batch Loss: 0.19264589250087738\n",
      "Subject 16, Epoch 851, Loss: 0.6410795748233795, Final Batch Loss: 0.12360046803951263\n",
      "Subject 16, Epoch 852, Loss: 0.49803607165813446, Final Batch Loss: 0.15277144312858582\n",
      "Subject 16, Epoch 853, Loss: 0.6163840815424919, Final Batch Loss: 0.2780488431453705\n",
      "Subject 16, Epoch 854, Loss: 0.44329186901450157, Final Batch Loss: 0.14579404890537262\n",
      "Subject 16, Epoch 855, Loss: 0.5959805399179459, Final Batch Loss: 0.09299248456954956\n",
      "Subject 16, Epoch 856, Loss: 0.42162855342030525, Final Batch Loss: 0.10287842154502869\n",
      "Subject 16, Epoch 857, Loss: 0.36146481335163116, Final Batch Loss: 0.07546045631170273\n",
      "Subject 16, Epoch 858, Loss: 0.654673159122467, Final Batch Loss: 0.1848084032535553\n",
      "Subject 16, Epoch 859, Loss: 0.5652518570423126, Final Batch Loss: 0.13828499615192413\n",
      "Subject 16, Epoch 860, Loss: 0.37662605196237564, Final Batch Loss: 0.10368174314498901\n",
      "Subject 16, Epoch 861, Loss: 0.477089487016201, Final Batch Loss: 0.09668711572885513\n",
      "Subject 16, Epoch 862, Loss: 0.38683556765317917, Final Batch Loss: 0.07105561345815659\n",
      "Subject 16, Epoch 863, Loss: 0.6310745924711227, Final Batch Loss: 0.3044068217277527\n",
      "Subject 16, Epoch 864, Loss: 0.47146376222372055, Final Batch Loss: 0.07436730712652206\n",
      "Subject 16, Epoch 865, Loss: 0.39880581945180893, Final Batch Loss: 0.11007431894540787\n",
      "Subject 16, Epoch 866, Loss: 0.41958051174879074, Final Batch Loss: 0.13434217870235443\n",
      "Subject 16, Epoch 867, Loss: 0.40755532309412956, Final Batch Loss: 0.057690080255270004\n",
      "Subject 16, Epoch 868, Loss: 0.44156936183571815, Final Batch Loss: 0.09747766703367233\n",
      "Subject 16, Epoch 869, Loss: 0.4852288141846657, Final Batch Loss: 0.1089848205447197\n",
      "Subject 16, Epoch 870, Loss: 0.4982062205672264, Final Batch Loss: 0.10893160849809647\n",
      "Subject 16, Epoch 871, Loss: 0.4516952484846115, Final Batch Loss: 0.07434584200382233\n",
      "Subject 16, Epoch 872, Loss: 0.5080833174288273, Final Batch Loss: 0.13663554191589355\n",
      "Subject 16, Epoch 873, Loss: 0.48206647858023643, Final Batch Loss: 0.18290868401527405\n",
      "Subject 16, Epoch 874, Loss: 0.3817169591784477, Final Batch Loss: 0.13019125163555145\n",
      "Subject 16, Epoch 875, Loss: 0.38332464918494225, Final Batch Loss: 0.055412303656339645\n",
      "Subject 16, Epoch 876, Loss: 0.3721011206507683, Final Batch Loss: 0.087290920317173\n",
      "Subject 16, Epoch 877, Loss: 0.4327986091375351, Final Batch Loss: 0.11839037388563156\n",
      "Subject 16, Epoch 878, Loss: 0.606427326798439, Final Batch Loss: 0.08576670289039612\n",
      "Subject 16, Epoch 879, Loss: 0.731989573687315, Final Batch Loss: 0.28986501693725586\n",
      "Subject 16, Epoch 880, Loss: 0.4341789036989212, Final Batch Loss: 0.13574746251106262\n",
      "Subject 16, Epoch 881, Loss: 0.5057994350790977, Final Batch Loss: 0.16124221682548523\n",
      "Subject 16, Epoch 882, Loss: 0.5507637113332748, Final Batch Loss: 0.2281569540500641\n",
      "Subject 16, Epoch 883, Loss: 0.49050432443618774, Final Batch Loss: 0.08575927466154099\n",
      "Subject 16, Epoch 884, Loss: 0.38468996435403824, Final Batch Loss: 0.06995254009962082\n",
      "Subject 16, Epoch 885, Loss: 0.43812382966279984, Final Batch Loss: 0.13990049064159393\n",
      "Subject 16, Epoch 886, Loss: 0.5742200613021851, Final Batch Loss: 0.12352597713470459\n",
      "Subject 16, Epoch 887, Loss: 0.3627285994589329, Final Batch Loss: 0.04050007089972496\n",
      "Subject 16, Epoch 888, Loss: 0.3610396981239319, Final Batch Loss: 0.08325975388288498\n",
      "Subject 16, Epoch 889, Loss: 0.4489217847585678, Final Batch Loss: 0.17074789106845856\n",
      "Subject 16, Epoch 890, Loss: 0.4869220592081547, Final Batch Loss: 0.2319411337375641\n",
      "Subject 16, Epoch 891, Loss: 0.4714101701974869, Final Batch Loss: 0.15533021092414856\n",
      "Subject 16, Epoch 892, Loss: 0.3693016842007637, Final Batch Loss: 0.10241182893514633\n",
      "Subject 16, Epoch 893, Loss: 0.3965926095843315, Final Batch Loss: 0.07664072513580322\n",
      "Subject 16, Epoch 894, Loss: 0.4022284597158432, Final Batch Loss: 0.12395422160625458\n",
      "Subject 16, Epoch 895, Loss: 0.36389804258942604, Final Batch Loss: 0.05232289806008339\n",
      "Subject 16, Epoch 896, Loss: 0.4771176055073738, Final Batch Loss: 0.08439618349075317\n",
      "Subject 16, Epoch 897, Loss: 0.41940754652023315, Final Batch Loss: 0.10756631195545197\n",
      "Subject 16, Epoch 898, Loss: 0.4072333797812462, Final Batch Loss: 0.10211905092000961\n",
      "Subject 16, Epoch 899, Loss: 0.3310425542294979, Final Batch Loss: 0.038550686091184616\n",
      "Subject 16, Epoch 900, Loss: 0.3931842967867851, Final Batch Loss: 0.12153151631355286\n",
      "Subject 16, Epoch 901, Loss: 0.6506194546818733, Final Batch Loss: 0.27393102645874023\n",
      "Subject 16, Epoch 902, Loss: 0.49097269773483276, Final Batch Loss: 0.17667174339294434\n",
      "Subject 16, Epoch 903, Loss: 0.3322402760386467, Final Batch Loss: 0.08817794919013977\n",
      "Subject 16, Epoch 904, Loss: 0.39931944385170937, Final Batch Loss: 0.06381978839635849\n",
      "Subject 16, Epoch 905, Loss: 0.4108673706650734, Final Batch Loss: 0.09336753934621811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16, Epoch 906, Loss: 0.45346198230981827, Final Batch Loss: 0.04080721735954285\n",
      "Subject 16, Epoch 907, Loss: 0.48001811653375626, Final Batch Loss: 0.1220179870724678\n",
      "Subject 16, Epoch 908, Loss: 0.4623786322772503, Final Batch Loss: 0.2122941017150879\n",
      "Subject 16, Epoch 909, Loss: 0.5595677942037582, Final Batch Loss: 0.1507505476474762\n",
      "Subject 16, Epoch 910, Loss: 0.5319635346531868, Final Batch Loss: 0.1783144772052765\n",
      "Subject 16, Epoch 911, Loss: 0.45346759632229805, Final Batch Loss: 0.13393855094909668\n",
      "Subject 16, Epoch 912, Loss: 0.267334146425128, Final Batch Loss: 0.030969178304076195\n",
      "Subject 16, Epoch 913, Loss: 0.3559342846274376, Final Batch Loss: 0.12288627028465271\n",
      "Subject 16, Epoch 914, Loss: 0.2807318679988384, Final Batch Loss: 0.06160153076052666\n",
      "Subject 16, Epoch 915, Loss: 0.34282976388931274, Final Batch Loss: 0.06475124508142471\n",
      "Subject 16, Epoch 916, Loss: 0.4131532497704029, Final Batch Loss: 0.059066858142614365\n",
      "Subject 16, Epoch 917, Loss: 0.39980121701955795, Final Batch Loss: 0.08275391906499863\n",
      "Subject 16, Epoch 918, Loss: 0.3549579903483391, Final Batch Loss: 0.07155727595090866\n",
      "Subject 16, Epoch 919, Loss: 0.4547298960387707, Final Batch Loss: 0.18162597715854645\n",
      "Subject 16, Epoch 920, Loss: 0.35473242588341236, Final Batch Loss: 0.02834092266857624\n",
      "Subject 16, Epoch 921, Loss: 0.3978733532130718, Final Batch Loss: 0.0503440760076046\n",
      "Subject 16, Epoch 922, Loss: 0.42380235344171524, Final Batch Loss: 0.138709157705307\n",
      "Subject 16, Epoch 923, Loss: 0.43008681014180183, Final Batch Loss: 0.04094117507338524\n",
      "Subject 16, Epoch 924, Loss: 0.2847463246434927, Final Batch Loss: 0.028133219107985497\n",
      "Subject 16, Epoch 925, Loss: 0.5909644141793251, Final Batch Loss: 0.17815645039081573\n",
      "Subject 16, Epoch 926, Loss: 0.32956234738230705, Final Batch Loss: 0.07502228766679764\n",
      "Subject 16, Epoch 927, Loss: 0.4859364554286003, Final Batch Loss: 0.19339728355407715\n",
      "Subject 16, Epoch 928, Loss: 0.4575970433652401, Final Batch Loss: 0.1880277544260025\n",
      "Subject 16, Epoch 929, Loss: 0.4882403537631035, Final Batch Loss: 0.2351243942975998\n",
      "Subject 16, Epoch 930, Loss: 0.4125489592552185, Final Batch Loss: 0.04877900704741478\n",
      "Subject 16, Epoch 931, Loss: 0.35636014863848686, Final Batch Loss: 0.08357278257608414\n",
      "Subject 16, Epoch 932, Loss: 0.6619366966187954, Final Batch Loss: 0.4313833713531494\n",
      "Subject 16, Epoch 933, Loss: 0.3674900755286217, Final Batch Loss: 0.06255518645048141\n",
      "Subject 16, Epoch 934, Loss: 0.29909511283040047, Final Batch Loss: 0.05943693965673447\n",
      "Subject 16, Epoch 935, Loss: 0.3144315481185913, Final Batch Loss: 0.07257884740829468\n",
      "Subject 16, Epoch 936, Loss: 0.34651369601488113, Final Batch Loss: 0.03390619531273842\n",
      "Subject 16, Epoch 937, Loss: 0.6434240117669106, Final Batch Loss: 0.374688059091568\n",
      "Subject 16, Epoch 938, Loss: 0.3485753946006298, Final Batch Loss: 0.1369246542453766\n",
      "Subject 16, Epoch 939, Loss: 0.41360506787896156, Final Batch Loss: 0.04524660483002663\n",
      "Subject 16, Epoch 940, Loss: 0.39189182594418526, Final Batch Loss: 0.051144544035196304\n",
      "Subject 16, Epoch 941, Loss: 0.42789729684591293, Final Batch Loss: 0.10204540938138962\n",
      "Subject 16, Epoch 942, Loss: 0.3180700484663248, Final Batch Loss: 0.020765231922268867\n",
      "Subject 16, Epoch 943, Loss: 0.3507786840200424, Final Batch Loss: 0.08433838933706284\n",
      "Subject 16, Epoch 944, Loss: 0.4661247879266739, Final Batch Loss: 0.07808727025985718\n",
      "Subject 16, Epoch 945, Loss: 0.3460689187049866, Final Batch Loss: 0.09231026470661163\n",
      "Subject 16, Epoch 946, Loss: 0.2984676882624626, Final Batch Loss: 0.0534265972673893\n",
      "Subject 16, Epoch 947, Loss: 0.4783087596297264, Final Batch Loss: 0.12930791079998016\n",
      "Subject 16, Epoch 948, Loss: 0.2895803079009056, Final Batch Loss: 0.06826909631490707\n",
      "Subject 16, Epoch 949, Loss: 0.5751690194010735, Final Batch Loss: 0.15599973499774933\n",
      "Subject 16, Epoch 950, Loss: 0.40905580297112465, Final Batch Loss: 0.055299725383520126\n",
      "Subject 16, Epoch 951, Loss: 0.3408439550548792, Final Batch Loss: 0.018662946298718452\n",
      "Subject 16, Epoch 952, Loss: 0.3443206436932087, Final Batch Loss: 0.05310298502445221\n",
      "Subject 16, Epoch 953, Loss: 0.40548698231577873, Final Batch Loss: 0.2132321298122406\n",
      "Subject 16, Epoch 954, Loss: 0.41842762380838394, Final Batch Loss: 0.057588644325733185\n",
      "Subject 16, Epoch 955, Loss: 0.40424948930740356, Final Batch Loss: 0.1316782832145691\n",
      "Subject 16, Epoch 956, Loss: 0.45480237901210785, Final Batch Loss: 0.14650951325893402\n",
      "Subject 16, Epoch 957, Loss: 0.3213922642171383, Final Batch Loss: 0.04676232114434242\n",
      "Subject 16, Epoch 958, Loss: 0.362919382750988, Final Batch Loss: 0.07336044311523438\n",
      "Subject 16, Epoch 959, Loss: 0.4477657154202461, Final Batch Loss: 0.11568838357925415\n",
      "Subject 16, Epoch 960, Loss: 0.3891196697950363, Final Batch Loss: 0.11770772933959961\n",
      "Subject 16, Epoch 961, Loss: 0.412923127412796, Final Batch Loss: 0.09342078119516373\n",
      "Subject 16, Epoch 962, Loss: 0.36449138447642326, Final Batch Loss: 0.1043078601360321\n",
      "Subject 16, Epoch 963, Loss: 0.36304618418216705, Final Batch Loss: 0.059623196721076965\n",
      "Subject 16, Epoch 964, Loss: 0.3600767217576504, Final Batch Loss: 0.029447104781866074\n",
      "Subject 16, Epoch 965, Loss: 0.3371061198413372, Final Batch Loss: 0.03272950276732445\n",
      "Subject 16, Epoch 966, Loss: 0.2889811582863331, Final Batch Loss: 0.040532320737838745\n",
      "Subject 16, Epoch 967, Loss: 0.5023596957325935, Final Batch Loss: 0.19076836109161377\n",
      "Subject 16, Epoch 968, Loss: 0.31799820624291897, Final Batch Loss: 0.01716233603656292\n",
      "Subject 16, Epoch 969, Loss: 0.4350207895040512, Final Batch Loss: 0.10033653676509857\n",
      "Subject 16, Epoch 970, Loss: 0.28572389110922813, Final Batch Loss: 0.05274762585759163\n",
      "Subject 16, Epoch 971, Loss: 0.7557526901364326, Final Batch Loss: 0.4041065573692322\n",
      "Subject 16, Epoch 972, Loss: 0.3283124603331089, Final Batch Loss: 0.053615521639585495\n",
      "Subject 16, Epoch 973, Loss: 0.33592017367482185, Final Batch Loss: 0.050641607493162155\n",
      "Subject 16, Epoch 974, Loss: 0.40944501385092735, Final Batch Loss: 0.06414283812046051\n",
      "Subject 16, Epoch 975, Loss: 0.43161970004439354, Final Batch Loss: 0.05246822163462639\n",
      "Subject 16, Epoch 976, Loss: 0.45865348912775517, Final Batch Loss: 0.022207507863640785\n",
      "Subject 16, Epoch 977, Loss: 0.3383779674768448, Final Batch Loss: 0.0417584627866745\n",
      "Subject 16, Epoch 978, Loss: 0.33406511321663857, Final Batch Loss: 0.016582157462835312\n",
      "Subject 16, Epoch 979, Loss: 0.30636412277817726, Final Batch Loss: 0.032015327364206314\n",
      "Subject 16, Epoch 980, Loss: 0.32434260472655296, Final Batch Loss: 0.10008608549833298\n",
      "Subject 16, Epoch 981, Loss: 0.35615346021950245, Final Batch Loss: 0.01735021360218525\n",
      "Subject 16, Epoch 982, Loss: 0.4774675890803337, Final Batch Loss: 0.18933017551898956\n",
      "Subject 16, Epoch 983, Loss: 0.5620633885264397, Final Batch Loss: 0.03302403539419174\n",
      "Subject 16, Epoch 984, Loss: 0.31812408566474915, Final Batch Loss: 0.045362040400505066\n",
      "Subject 16, Epoch 985, Loss: 0.43668777495622635, Final Batch Loss: 0.08516769856214523\n",
      "Subject 16, Epoch 986, Loss: 0.37298300862312317, Final Batch Loss: 0.09351114183664322\n",
      "Subject 16, Epoch 987, Loss: 0.38494009897112846, Final Batch Loss: 0.04519742727279663\n",
      "Subject 16, Epoch 988, Loss: 0.2995840981602669, Final Batch Loss: 0.03232534974813461\n",
      "Subject 16, Epoch 989, Loss: 0.34857745096087456, Final Batch Loss: 0.060240183025598526\n",
      "Subject 16, Epoch 990, Loss: 0.3358716331422329, Final Batch Loss: 0.08183937519788742\n",
      "Subject 16, Epoch 991, Loss: 0.3474891930818558, Final Batch Loss: 0.050606824457645416\n",
      "Subject 16, Epoch 992, Loss: 0.41898487135767937, Final Batch Loss: 0.14781691133975983\n",
      "Subject 16, Epoch 993, Loss: 0.49020441621541977, Final Batch Loss: 0.2563241720199585\n",
      "Subject 16, Epoch 994, Loss: 0.3770540654659271, Final Batch Loss: 0.10456740856170654\n",
      "Subject 16, Epoch 995, Loss: 0.3141619600355625, Final Batch Loss: 0.07450339943170547\n",
      "Subject 16, Epoch 996, Loss: 0.33487165719270706, Final Batch Loss: 0.04491999000310898\n",
      "Subject 16, Epoch 997, Loss: 0.46505482494831085, Final Batch Loss: 0.19416382908821106\n",
      "Subject 16, Epoch 998, Loss: 0.4807698428630829, Final Batch Loss: 0.052562326192855835\n",
      "Subject 16, Epoch 999, Loss: 0.4246842600405216, Final Batch Loss: 0.19001218676567078\n",
      "Subject 16, Epoch 1000, Loss: 0.2574690207839012, Final Batch Loss: 0.021674714982509613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 1, Loss: 7.241556882858276, Final Batch Loss: 1.7748292684555054\n",
      "Subject 17, Epoch 2, Loss: 7.213839530944824, Final Batch Loss: 1.7664804458618164\n",
      "Subject 17, Epoch 3, Loss: 7.201183199882507, Final Batch Loss: 1.7804145812988281\n",
      "Subject 17, Epoch 4, Loss: 7.240662574768066, Final Batch Loss: 1.844093918800354\n",
      "Subject 17, Epoch 5, Loss: 7.1591808795928955, Final Batch Loss: 1.7779573202133179\n",
      "Subject 17, Epoch 6, Loss: 7.063669323921204, Final Batch Loss: 1.723419427871704\n",
      "Subject 17, Epoch 7, Loss: 7.0478585958480835, Final Batch Loss: 1.7611805200576782\n",
      "Subject 17, Epoch 8, Loss: 6.95012903213501, Final Batch Loss: 1.6937220096588135\n",
      "Subject 17, Epoch 9, Loss: 6.8704657554626465, Final Batch Loss: 1.7016828060150146\n",
      "Subject 17, Epoch 10, Loss: 6.759349465370178, Final Batch Loss: 1.6281208992004395\n",
      "Subject 17, Epoch 11, Loss: 6.621298313140869, Final Batch Loss: 1.6822675466537476\n",
      "Subject 17, Epoch 12, Loss: 6.44411039352417, Final Batch Loss: 1.5705944299697876\n",
      "Subject 17, Epoch 13, Loss: 6.281435251235962, Final Batch Loss: 1.5124629735946655\n",
      "Subject 17, Epoch 14, Loss: 6.047961354255676, Final Batch Loss: 1.4549137353897095\n",
      "Subject 17, Epoch 15, Loss: 6.0772950649261475, Final Batch Loss: 1.611706018447876\n",
      "Subject 17, Epoch 16, Loss: 5.795010209083557, Final Batch Loss: 1.4753082990646362\n",
      "Subject 17, Epoch 17, Loss: 5.882614374160767, Final Batch Loss: 1.5333465337753296\n",
      "Subject 17, Epoch 18, Loss: 5.726134181022644, Final Batch Loss: 1.457254409790039\n",
      "Subject 17, Epoch 19, Loss: 5.7239075899124146, Final Batch Loss: 1.5899157524108887\n",
      "Subject 17, Epoch 20, Loss: 5.52591073513031, Final Batch Loss: 1.3299261331558228\n",
      "Subject 17, Epoch 21, Loss: 5.5042126178741455, Final Batch Loss: 1.2997666597366333\n",
      "Subject 17, Epoch 22, Loss: 5.683116436004639, Final Batch Loss: 1.5147985219955444\n",
      "Subject 17, Epoch 23, Loss: 5.436103701591492, Final Batch Loss: 1.2863340377807617\n",
      "Subject 17, Epoch 24, Loss: 5.4301674365997314, Final Batch Loss: 1.3602596521377563\n",
      "Subject 17, Epoch 25, Loss: 5.435956716537476, Final Batch Loss: 1.332033395767212\n",
      "Subject 17, Epoch 26, Loss: 5.5337241888046265, Final Batch Loss: 1.4305983781814575\n",
      "Subject 17, Epoch 27, Loss: 5.289209961891174, Final Batch Loss: 1.3154289722442627\n",
      "Subject 17, Epoch 28, Loss: 5.169488310813904, Final Batch Loss: 1.1903045177459717\n",
      "Subject 17, Epoch 29, Loss: 5.300525903701782, Final Batch Loss: 1.3143459558486938\n",
      "Subject 17, Epoch 30, Loss: 5.322753667831421, Final Batch Loss: 1.4070460796356201\n",
      "Subject 17, Epoch 31, Loss: 5.064645171165466, Final Batch Loss: 1.2210851907730103\n",
      "Subject 17, Epoch 32, Loss: 4.949860095977783, Final Batch Loss: 1.170502781867981\n",
      "Subject 17, Epoch 33, Loss: 4.922615051269531, Final Batch Loss: 1.21719491481781\n",
      "Subject 17, Epoch 34, Loss: 4.7587562799453735, Final Batch Loss: 1.123644232749939\n",
      "Subject 17, Epoch 35, Loss: 5.0276957750320435, Final Batch Loss: 1.3638039827346802\n",
      "Subject 17, Epoch 36, Loss: 4.782874703407288, Final Batch Loss: 1.1651118993759155\n",
      "Subject 17, Epoch 37, Loss: 4.776047229766846, Final Batch Loss: 1.1875243186950684\n",
      "Subject 17, Epoch 38, Loss: 4.710724949836731, Final Batch Loss: 1.1441025733947754\n",
      "Subject 17, Epoch 39, Loss: 4.717890739440918, Final Batch Loss: 1.170954704284668\n",
      "Subject 17, Epoch 40, Loss: 4.689026951789856, Final Batch Loss: 1.1556875705718994\n",
      "Subject 17, Epoch 41, Loss: 4.673386931419373, Final Batch Loss: 1.1641181707382202\n",
      "Subject 17, Epoch 42, Loss: 4.763895630836487, Final Batch Loss: 1.217901349067688\n",
      "Subject 17, Epoch 43, Loss: 4.623274922370911, Final Batch Loss: 1.116884708404541\n",
      "Subject 17, Epoch 44, Loss: 4.951869368553162, Final Batch Loss: 1.3669836521148682\n",
      "Subject 17, Epoch 45, Loss: 4.558072805404663, Final Batch Loss: 1.0986824035644531\n",
      "Subject 17, Epoch 46, Loss: 4.465134620666504, Final Batch Loss: 1.0933204889297485\n",
      "Subject 17, Epoch 47, Loss: 4.612133026123047, Final Batch Loss: 1.177536129951477\n",
      "Subject 17, Epoch 48, Loss: 4.524818778038025, Final Batch Loss: 1.1256639957427979\n",
      "Subject 17, Epoch 49, Loss: 4.604521632194519, Final Batch Loss: 1.1768604516983032\n",
      "Subject 17, Epoch 50, Loss: 4.6903908252716064, Final Batch Loss: 1.343273401260376\n",
      "Subject 17, Epoch 51, Loss: 4.5929542779922485, Final Batch Loss: 1.1647861003875732\n",
      "Subject 17, Epoch 52, Loss: 4.512876272201538, Final Batch Loss: 1.135904312133789\n",
      "Subject 17, Epoch 53, Loss: 4.5389792919158936, Final Batch Loss: 1.1217767000198364\n",
      "Subject 17, Epoch 54, Loss: 4.5744160413742065, Final Batch Loss: 1.1220442056655884\n",
      "Subject 17, Epoch 55, Loss: 4.583457827568054, Final Batch Loss: 1.1863036155700684\n",
      "Subject 17, Epoch 56, Loss: 4.514566659927368, Final Batch Loss: 1.1099112033843994\n",
      "Subject 17, Epoch 57, Loss: 4.5744524002075195, Final Batch Loss: 1.1820719242095947\n",
      "Subject 17, Epoch 58, Loss: 4.518862962722778, Final Batch Loss: 1.0927631855010986\n",
      "Subject 17, Epoch 59, Loss: 4.441653251647949, Final Batch Loss: 1.0409901142120361\n",
      "Subject 17, Epoch 60, Loss: 4.5235360860824585, Final Batch Loss: 1.1209012269973755\n",
      "Subject 17, Epoch 61, Loss: 4.657072186470032, Final Batch Loss: 1.2477184534072876\n",
      "Subject 17, Epoch 62, Loss: 4.4586650133132935, Final Batch Loss: 1.141669750213623\n",
      "Subject 17, Epoch 63, Loss: 4.482198238372803, Final Batch Loss: 1.158799171447754\n",
      "Subject 17, Epoch 64, Loss: 4.577900052070618, Final Batch Loss: 1.1484168767929077\n",
      "Subject 17, Epoch 65, Loss: 4.45185387134552, Final Batch Loss: 1.0964292287826538\n",
      "Subject 17, Epoch 66, Loss: 4.5711363554000854, Final Batch Loss: 1.1368770599365234\n",
      "Subject 17, Epoch 67, Loss: 4.510507822036743, Final Batch Loss: 1.1216763257980347\n",
      "Subject 17, Epoch 68, Loss: 4.515286922454834, Final Batch Loss: 1.1061996221542358\n",
      "Subject 17, Epoch 69, Loss: 4.515519738197327, Final Batch Loss: 1.2340943813323975\n",
      "Subject 17, Epoch 70, Loss: 4.481018424034119, Final Batch Loss: 1.1205151081085205\n",
      "Subject 17, Epoch 71, Loss: 4.456000924110413, Final Batch Loss: 1.1017239093780518\n",
      "Subject 17, Epoch 72, Loss: 4.358152866363525, Final Batch Loss: 1.0620521306991577\n",
      "Subject 17, Epoch 73, Loss: 4.42375123500824, Final Batch Loss: 1.1074107885360718\n",
      "Subject 17, Epoch 74, Loss: 4.389228940010071, Final Batch Loss: 1.104917049407959\n",
      "Subject 17, Epoch 75, Loss: 4.4036760330200195, Final Batch Loss: 1.0753669738769531\n",
      "Subject 17, Epoch 76, Loss: 4.476121544837952, Final Batch Loss: 1.125460147857666\n",
      "Subject 17, Epoch 77, Loss: 4.446055769920349, Final Batch Loss: 1.1187160015106201\n",
      "Subject 17, Epoch 78, Loss: 4.37240731716156, Final Batch Loss: 1.0490798950195312\n",
      "Subject 17, Epoch 79, Loss: 4.429645419120789, Final Batch Loss: 1.0816951990127563\n",
      "Subject 17, Epoch 80, Loss: 4.382793188095093, Final Batch Loss: 1.0800319910049438\n",
      "Subject 17, Epoch 81, Loss: 4.4869972467422485, Final Batch Loss: 1.1444463729858398\n",
      "Subject 17, Epoch 82, Loss: 4.479134678840637, Final Batch Loss: 1.1726025342941284\n",
      "Subject 17, Epoch 83, Loss: 4.26050877571106, Final Batch Loss: 1.0425682067871094\n",
      "Subject 17, Epoch 84, Loss: 4.279753923416138, Final Batch Loss: 1.0344901084899902\n",
      "Subject 17, Epoch 85, Loss: 4.386658072471619, Final Batch Loss: 1.1325241327285767\n",
      "Subject 17, Epoch 86, Loss: 4.366636753082275, Final Batch Loss: 1.135087251663208\n",
      "Subject 17, Epoch 87, Loss: 4.259148478507996, Final Batch Loss: 1.098739504814148\n",
      "Subject 17, Epoch 88, Loss: 4.213689804077148, Final Batch Loss: 1.0613120794296265\n",
      "Subject 17, Epoch 89, Loss: 4.224164009094238, Final Batch Loss: 1.0815247297286987\n",
      "Subject 17, Epoch 90, Loss: 4.224689245223999, Final Batch Loss: 1.0729260444641113\n",
      "Subject 17, Epoch 91, Loss: 4.123232126235962, Final Batch Loss: 1.0179975032806396\n",
      "Subject 17, Epoch 92, Loss: 4.213641881942749, Final Batch Loss: 1.0491530895233154\n",
      "Subject 17, Epoch 93, Loss: 4.17235803604126, Final Batch Loss: 1.0572258234024048\n",
      "Subject 17, Epoch 94, Loss: 4.03612756729126, Final Batch Loss: 0.9137980937957764\n",
      "Subject 17, Epoch 95, Loss: 4.092681050300598, Final Batch Loss: 1.0760600566864014\n",
      "Subject 17, Epoch 96, Loss: 4.089091956615448, Final Batch Loss: 1.0748807191848755\n",
      "Subject 17, Epoch 97, Loss: 4.112890839576721, Final Batch Loss: 1.0422143936157227\n",
      "Subject 17, Epoch 98, Loss: 4.140158772468567, Final Batch Loss: 1.0422368049621582\n",
      "Subject 17, Epoch 99, Loss: 3.9884634017944336, Final Batch Loss: 0.9575532674789429\n",
      "Subject 17, Epoch 100, Loss: 3.9726478457450867, Final Batch Loss: 0.9428288340568542\n",
      "Subject 17, Epoch 101, Loss: 3.8892546892166138, Final Batch Loss: 0.9174667000770569\n",
      "Subject 17, Epoch 102, Loss: 3.8751017451286316, Final Batch Loss: 0.9366726279258728\n",
      "Subject 17, Epoch 103, Loss: 3.8728426098823547, Final Batch Loss: 0.9428291320800781\n",
      "Subject 17, Epoch 104, Loss: 3.7492032647132874, Final Batch Loss: 0.8707634806632996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 105, Loss: 3.8014522194862366, Final Batch Loss: 0.9775370359420776\n",
      "Subject 17, Epoch 106, Loss: 3.841583788394928, Final Batch Loss: 0.9664676785469055\n",
      "Subject 17, Epoch 107, Loss: 3.898584544658661, Final Batch Loss: 1.0206817388534546\n",
      "Subject 17, Epoch 108, Loss: 3.943232536315918, Final Batch Loss: 0.9752410054206848\n",
      "Subject 17, Epoch 109, Loss: 3.7813757061958313, Final Batch Loss: 0.9497968554496765\n",
      "Subject 17, Epoch 110, Loss: 3.6934603452682495, Final Batch Loss: 0.8434903025627136\n",
      "Subject 17, Epoch 111, Loss: 3.6807896494865417, Final Batch Loss: 0.9050242900848389\n",
      "Subject 17, Epoch 112, Loss: 3.657360076904297, Final Batch Loss: 0.9320077300071716\n",
      "Subject 17, Epoch 113, Loss: 3.6977341771125793, Final Batch Loss: 0.8956721425056458\n",
      "Subject 17, Epoch 114, Loss: 3.6553645730018616, Final Batch Loss: 0.9726383686065674\n",
      "Subject 17, Epoch 115, Loss: 3.670560359954834, Final Batch Loss: 1.0381172895431519\n",
      "Subject 17, Epoch 116, Loss: 3.4099926948547363, Final Batch Loss: 0.8723295331001282\n",
      "Subject 17, Epoch 117, Loss: 3.5903968811035156, Final Batch Loss: 0.9555653929710388\n",
      "Subject 17, Epoch 118, Loss: 3.4289706349372864, Final Batch Loss: 0.8078760504722595\n",
      "Subject 17, Epoch 119, Loss: 3.3503547310829163, Final Batch Loss: 0.8480965495109558\n",
      "Subject 17, Epoch 120, Loss: 3.3715417981147766, Final Batch Loss: 0.7934209108352661\n",
      "Subject 17, Epoch 121, Loss: 3.2984087467193604, Final Batch Loss: 0.7832635045051575\n",
      "Subject 17, Epoch 122, Loss: 3.3102681040763855, Final Batch Loss: 0.7748383283615112\n",
      "Subject 17, Epoch 123, Loss: 3.2884451746940613, Final Batch Loss: 0.8526543974876404\n",
      "Subject 17, Epoch 124, Loss: 3.2689441442489624, Final Batch Loss: 0.8261765241622925\n",
      "Subject 17, Epoch 125, Loss: 3.2356584668159485, Final Batch Loss: 0.9096408486366272\n",
      "Subject 17, Epoch 126, Loss: 3.1368345618247986, Final Batch Loss: 0.7993741631507874\n",
      "Subject 17, Epoch 127, Loss: 2.970299243927002, Final Batch Loss: 0.6969189643859863\n",
      "Subject 17, Epoch 128, Loss: 3.198670268058777, Final Batch Loss: 0.8584588766098022\n",
      "Subject 17, Epoch 129, Loss: 2.937408149242401, Final Batch Loss: 0.728580117225647\n",
      "Subject 17, Epoch 130, Loss: 3.0479893684387207, Final Batch Loss: 0.6969195008277893\n",
      "Subject 17, Epoch 131, Loss: 3.003267765045166, Final Batch Loss: 0.7101715207099915\n",
      "Subject 17, Epoch 132, Loss: 2.827672004699707, Final Batch Loss: 0.592667818069458\n",
      "Subject 17, Epoch 133, Loss: 2.9095888137817383, Final Batch Loss: 0.6410146951675415\n",
      "Subject 17, Epoch 134, Loss: 2.656995177268982, Final Batch Loss: 0.5444225072860718\n",
      "Subject 17, Epoch 135, Loss: 2.8781689405441284, Final Batch Loss: 0.6435141563415527\n",
      "Subject 17, Epoch 136, Loss: 2.96029269695282, Final Batch Loss: 0.7917677164077759\n",
      "Subject 17, Epoch 137, Loss: 2.649561643600464, Final Batch Loss: 0.4504225254058838\n",
      "Subject 17, Epoch 138, Loss: 2.8610686659812927, Final Batch Loss: 0.6152393817901611\n",
      "Subject 17, Epoch 139, Loss: 2.8358492851257324, Final Batch Loss: 0.6031873822212219\n",
      "Subject 17, Epoch 140, Loss: 2.7553441524505615, Final Batch Loss: 0.6111044883728027\n",
      "Subject 17, Epoch 141, Loss: 2.851197838783264, Final Batch Loss: 0.8211286664009094\n",
      "Subject 17, Epoch 142, Loss: 2.7589593529701233, Final Batch Loss: 0.5650377869606018\n",
      "Subject 17, Epoch 143, Loss: 2.8325865268707275, Final Batch Loss: 0.7595604062080383\n",
      "Subject 17, Epoch 144, Loss: 3.0283056497573853, Final Batch Loss: 0.8652808666229248\n",
      "Subject 17, Epoch 145, Loss: 2.8848846554756165, Final Batch Loss: 0.7371090650558472\n",
      "Subject 17, Epoch 146, Loss: 2.796285033226013, Final Batch Loss: 0.6476346850395203\n",
      "Subject 17, Epoch 147, Loss: 2.6860584020614624, Final Batch Loss: 0.5493190884590149\n",
      "Subject 17, Epoch 148, Loss: 2.5554832220077515, Final Batch Loss: 0.555331289768219\n",
      "Subject 17, Epoch 149, Loss: 2.7683552503585815, Final Batch Loss: 0.7047879695892334\n",
      "Subject 17, Epoch 150, Loss: 2.686530828475952, Final Batch Loss: 0.6104423403739929\n",
      "Subject 17, Epoch 151, Loss: 2.855813205242157, Final Batch Loss: 0.8018528819084167\n",
      "Subject 17, Epoch 152, Loss: 2.700161039829254, Final Batch Loss: 0.630365252494812\n",
      "Subject 17, Epoch 153, Loss: 2.622697949409485, Final Batch Loss: 0.614127516746521\n",
      "Subject 17, Epoch 154, Loss: 2.8184642791748047, Final Batch Loss: 0.8085818290710449\n",
      "Subject 17, Epoch 155, Loss: 2.8922303915023804, Final Batch Loss: 0.9687897562980652\n",
      "Subject 17, Epoch 156, Loss: 2.5970503091812134, Final Batch Loss: 0.5607001781463623\n",
      "Subject 17, Epoch 157, Loss: 2.6553629636764526, Final Batch Loss: 0.628500759601593\n",
      "Subject 17, Epoch 158, Loss: 2.7071778178215027, Final Batch Loss: 0.6797011494636536\n",
      "Subject 17, Epoch 159, Loss: 2.7079648971557617, Final Batch Loss: 0.7553000450134277\n",
      "Subject 17, Epoch 160, Loss: 2.6060335636138916, Final Batch Loss: 0.6623196601867676\n",
      "Subject 17, Epoch 161, Loss: 2.6036216616630554, Final Batch Loss: 0.5866500735282898\n",
      "Subject 17, Epoch 162, Loss: 2.6157135367393494, Final Batch Loss: 0.7276241183280945\n",
      "Subject 17, Epoch 163, Loss: 2.6987725496292114, Final Batch Loss: 0.7815274000167847\n",
      "Subject 17, Epoch 164, Loss: 2.468746066093445, Final Batch Loss: 0.5741056799888611\n",
      "Subject 17, Epoch 165, Loss: 2.783637762069702, Final Batch Loss: 0.7282906174659729\n",
      "Subject 17, Epoch 166, Loss: 2.40380996465683, Final Batch Loss: 0.5157128572463989\n",
      "Subject 17, Epoch 167, Loss: 2.7435877919197083, Final Batch Loss: 0.7362236976623535\n",
      "Subject 17, Epoch 168, Loss: 2.4892961978912354, Final Batch Loss: 0.6249944567680359\n",
      "Subject 17, Epoch 169, Loss: 2.541965425014496, Final Batch Loss: 0.6221879720687866\n",
      "Subject 17, Epoch 170, Loss: 2.5118584036827087, Final Batch Loss: 0.5844419598579407\n",
      "Subject 17, Epoch 171, Loss: 2.749825179576874, Final Batch Loss: 0.844886302947998\n",
      "Subject 17, Epoch 172, Loss: 2.6818395256996155, Final Batch Loss: 0.6805877685546875\n",
      "Subject 17, Epoch 173, Loss: 2.637355089187622, Final Batch Loss: 0.718128502368927\n",
      "Subject 17, Epoch 174, Loss: 2.386012613773346, Final Batch Loss: 0.43206942081451416\n",
      "Subject 17, Epoch 175, Loss: 2.7479665875434875, Final Batch Loss: 0.9717350006103516\n",
      "Subject 17, Epoch 176, Loss: 2.380373477935791, Final Batch Loss: 0.5428388118743896\n",
      "Subject 17, Epoch 177, Loss: 2.630697011947632, Final Batch Loss: 0.7115814685821533\n",
      "Subject 17, Epoch 178, Loss: 2.3879055976867676, Final Batch Loss: 0.5221723318099976\n",
      "Subject 17, Epoch 179, Loss: 2.421074390411377, Final Batch Loss: 0.5544577240943909\n",
      "Subject 17, Epoch 180, Loss: 2.6019453406333923, Final Batch Loss: 0.8156427145004272\n",
      "Subject 17, Epoch 181, Loss: 2.3335736095905304, Final Batch Loss: 0.44096192717552185\n",
      "Subject 17, Epoch 182, Loss: 2.3072873055934906, Final Batch Loss: 0.47027787566185\n",
      "Subject 17, Epoch 183, Loss: 2.4022374749183655, Final Batch Loss: 0.5884652137756348\n",
      "Subject 17, Epoch 184, Loss: 2.4298658967018127, Final Batch Loss: 0.6005355715751648\n",
      "Subject 17, Epoch 185, Loss: 2.1882438957691193, Final Batch Loss: 0.42633333802223206\n",
      "Subject 17, Epoch 186, Loss: 2.3689451217651367, Final Batch Loss: 0.626958429813385\n",
      "Subject 17, Epoch 187, Loss: 2.2870131134986877, Final Batch Loss: 0.5159494280815125\n",
      "Subject 17, Epoch 188, Loss: 2.552543044090271, Final Batch Loss: 0.7861323356628418\n",
      "Subject 17, Epoch 189, Loss: 2.5493345260620117, Final Batch Loss: 0.8071615695953369\n",
      "Subject 17, Epoch 190, Loss: 2.3669105768203735, Final Batch Loss: 0.6228532791137695\n",
      "Subject 17, Epoch 191, Loss: 2.3789206743240356, Final Batch Loss: 0.5707655549049377\n",
      "Subject 17, Epoch 192, Loss: 2.3580724000930786, Final Batch Loss: 0.6521562337875366\n",
      "Subject 17, Epoch 193, Loss: 2.277927666902542, Final Batch Loss: 0.48432931303977966\n",
      "Subject 17, Epoch 194, Loss: 2.3539745211601257, Final Batch Loss: 0.5880385041236877\n",
      "Subject 17, Epoch 195, Loss: 2.3014104664325714, Final Batch Loss: 0.6385107636451721\n",
      "Subject 17, Epoch 196, Loss: 2.1253355741500854, Final Batch Loss: 0.4164673686027527\n",
      "Subject 17, Epoch 197, Loss: 2.2089953124523163, Final Batch Loss: 0.4464211165904999\n",
      "Subject 17, Epoch 198, Loss: 2.3253111839294434, Final Batch Loss: 0.6669027209281921\n",
      "Subject 17, Epoch 199, Loss: 2.4066396355628967, Final Batch Loss: 0.6547077894210815\n",
      "Subject 17, Epoch 200, Loss: 2.2620824575424194, Final Batch Loss: 0.4608166217803955\n",
      "Subject 17, Epoch 201, Loss: 2.346106678247452, Final Batch Loss: 0.6783561706542969\n",
      "Subject 17, Epoch 202, Loss: 2.3474209308624268, Final Batch Loss: 0.5387230515480042\n",
      "Subject 17, Epoch 203, Loss: 2.322372257709503, Final Batch Loss: 0.6573058366775513\n",
      "Subject 17, Epoch 204, Loss: 2.429457366466522, Final Batch Loss: 0.6612846255302429\n",
      "Subject 17, Epoch 205, Loss: 2.2401573061943054, Final Batch Loss: 0.5533429980278015\n",
      "Subject 17, Epoch 206, Loss: 2.1720317900180817, Final Batch Loss: 0.5118377208709717\n",
      "Subject 17, Epoch 207, Loss: 2.253131866455078, Final Batch Loss: 0.5712077021598816\n",
      "Subject 17, Epoch 208, Loss: 2.2996662259101868, Final Batch Loss: 0.5342755317687988\n",
      "Subject 17, Epoch 209, Loss: 2.1347240805625916, Final Batch Loss: 0.5008712410926819\n",
      "Subject 17, Epoch 210, Loss: 2.090754419565201, Final Batch Loss: 0.3864544928073883\n",
      "Subject 17, Epoch 211, Loss: 2.502709239721298, Final Batch Loss: 0.9646624326705933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 212, Loss: 2.181573987007141, Final Batch Loss: 0.5781033635139465\n",
      "Subject 17, Epoch 213, Loss: 2.115003526210785, Final Batch Loss: 0.5085335373878479\n",
      "Subject 17, Epoch 214, Loss: 2.0417996048927307, Final Batch Loss: 0.43059393763542175\n",
      "Subject 17, Epoch 215, Loss: 2.1573526859283447, Final Batch Loss: 0.46255213022232056\n",
      "Subject 17, Epoch 216, Loss: 2.2152024507522583, Final Batch Loss: 0.5736629366874695\n",
      "Subject 17, Epoch 217, Loss: 2.0649452209472656, Final Batch Loss: 0.40018028020858765\n",
      "Subject 17, Epoch 218, Loss: 1.9599748849868774, Final Batch Loss: 0.36120471358299255\n",
      "Subject 17, Epoch 219, Loss: 2.1559061408042908, Final Batch Loss: 0.5240418910980225\n",
      "Subject 17, Epoch 220, Loss: 2.080111473798752, Final Batch Loss: 0.40191397070884705\n",
      "Subject 17, Epoch 221, Loss: 2.1069046556949615, Final Batch Loss: 0.5703562498092651\n",
      "Subject 17, Epoch 222, Loss: 2.191116690635681, Final Batch Loss: 0.5914636254310608\n",
      "Subject 17, Epoch 223, Loss: 2.074697345495224, Final Batch Loss: 0.5275728106498718\n",
      "Subject 17, Epoch 224, Loss: 2.17395943403244, Final Batch Loss: 0.5523231625556946\n",
      "Subject 17, Epoch 225, Loss: 2.1548977494239807, Final Batch Loss: 0.574204683303833\n",
      "Subject 17, Epoch 226, Loss: 2.159437119960785, Final Batch Loss: 0.5614597201347351\n",
      "Subject 17, Epoch 227, Loss: 2.0425407588481903, Final Batch Loss: 0.45306262373924255\n",
      "Subject 17, Epoch 228, Loss: 2.1008165776729584, Final Batch Loss: 0.5396401286125183\n",
      "Subject 17, Epoch 229, Loss: 2.0432718992233276, Final Batch Loss: 0.48761212825775146\n",
      "Subject 17, Epoch 230, Loss: 2.147164076566696, Final Batch Loss: 0.5970902442932129\n",
      "Subject 17, Epoch 231, Loss: 2.0550135374069214, Final Batch Loss: 0.5008603930473328\n",
      "Subject 17, Epoch 232, Loss: 2.0047809779644012, Final Batch Loss: 0.3824048936367035\n",
      "Subject 17, Epoch 233, Loss: 2.0765708088874817, Final Batch Loss: 0.5163553357124329\n",
      "Subject 17, Epoch 234, Loss: 2.0838460326194763, Final Batch Loss: 0.5236155986785889\n",
      "Subject 17, Epoch 235, Loss: 1.8663668930530548, Final Batch Loss: 0.30674365162849426\n",
      "Subject 17, Epoch 236, Loss: 2.0352813005447388, Final Batch Loss: 0.42522314190864563\n",
      "Subject 17, Epoch 237, Loss: 1.9771047532558441, Final Batch Loss: 0.46473026275634766\n",
      "Subject 17, Epoch 238, Loss: 2.1749497056007385, Final Batch Loss: 0.5561401844024658\n",
      "Subject 17, Epoch 239, Loss: 1.9934998452663422, Final Batch Loss: 0.396923303604126\n",
      "Subject 17, Epoch 240, Loss: 2.0706026554107666, Final Batch Loss: 0.5925123691558838\n",
      "Subject 17, Epoch 241, Loss: 1.9050225615501404, Final Batch Loss: 0.3431891202926636\n",
      "Subject 17, Epoch 242, Loss: 1.9478975236415863, Final Batch Loss: 0.3851403594017029\n",
      "Subject 17, Epoch 243, Loss: 2.0234313309192657, Final Batch Loss: 0.5767346620559692\n",
      "Subject 17, Epoch 244, Loss: 1.8471225500106812, Final Batch Loss: 0.3255688548088074\n",
      "Subject 17, Epoch 245, Loss: 2.055477350950241, Final Batch Loss: 0.5514021515846252\n",
      "Subject 17, Epoch 246, Loss: 1.9629308581352234, Final Batch Loss: 0.4116727113723755\n",
      "Subject 17, Epoch 247, Loss: 1.917664259672165, Final Batch Loss: 0.45174944400787354\n",
      "Subject 17, Epoch 248, Loss: 2.0106640458106995, Final Batch Loss: 0.4914247989654541\n",
      "Subject 17, Epoch 249, Loss: 1.9904993772506714, Final Batch Loss: 0.5524213910102844\n",
      "Subject 17, Epoch 250, Loss: 1.9615090489387512, Final Batch Loss: 0.45547279715538025\n",
      "Subject 17, Epoch 251, Loss: 2.0052395462989807, Final Batch Loss: 0.5298317670822144\n",
      "Subject 17, Epoch 252, Loss: 1.9135313034057617, Final Batch Loss: 0.471489816904068\n",
      "Subject 17, Epoch 253, Loss: 1.8316746652126312, Final Batch Loss: 0.35512715578079224\n",
      "Subject 17, Epoch 254, Loss: 1.7663139700889587, Final Batch Loss: 0.33027204871177673\n",
      "Subject 17, Epoch 255, Loss: 1.969464659690857, Final Batch Loss: 0.5323731899261475\n",
      "Subject 17, Epoch 256, Loss: 1.9019686579704285, Final Batch Loss: 0.4210488200187683\n",
      "Subject 17, Epoch 257, Loss: 1.967483252286911, Final Batch Loss: 0.5113210082054138\n",
      "Subject 17, Epoch 258, Loss: 1.7363280057907104, Final Batch Loss: 0.3323246240615845\n",
      "Subject 17, Epoch 259, Loss: 1.86953005194664, Final Batch Loss: 0.4116765260696411\n",
      "Subject 17, Epoch 260, Loss: 1.9916656911373138, Final Batch Loss: 0.6061643958091736\n",
      "Subject 17, Epoch 261, Loss: 1.8920685648918152, Final Batch Loss: 0.5513387322425842\n",
      "Subject 17, Epoch 262, Loss: 1.8994260430335999, Final Batch Loss: 0.48845773935317993\n",
      "Subject 17, Epoch 263, Loss: 1.853557139635086, Final Batch Loss: 0.38073065876960754\n",
      "Subject 17, Epoch 264, Loss: 2.0837560892105103, Final Batch Loss: 0.5623517632484436\n",
      "Subject 17, Epoch 265, Loss: 1.8605119287967682, Final Batch Loss: 0.5095461010932922\n",
      "Subject 17, Epoch 266, Loss: 2.007103681564331, Final Batch Loss: 0.5926496386528015\n",
      "Subject 17, Epoch 267, Loss: 1.8634827733039856, Final Batch Loss: 0.3991888761520386\n",
      "Subject 17, Epoch 268, Loss: 1.779373437166214, Final Batch Loss: 0.3790226876735687\n",
      "Subject 17, Epoch 269, Loss: 1.8072968423366547, Final Batch Loss: 0.35918688774108887\n",
      "Subject 17, Epoch 270, Loss: 1.672841191291809, Final Batch Loss: 0.18504586815834045\n",
      "Subject 17, Epoch 271, Loss: 1.9605842530727386, Final Batch Loss: 0.5427832007408142\n",
      "Subject 17, Epoch 272, Loss: 1.8177448213100433, Final Batch Loss: 0.3819102346897125\n",
      "Subject 17, Epoch 273, Loss: 1.9004348516464233, Final Batch Loss: 0.5012473464012146\n",
      "Subject 17, Epoch 274, Loss: 1.8853197395801544, Final Batch Loss: 0.4469059407711029\n",
      "Subject 17, Epoch 275, Loss: 1.8629304766654968, Final Batch Loss: 0.4997827708721161\n",
      "Subject 17, Epoch 276, Loss: 2.0845813155174255, Final Batch Loss: 0.7434548139572144\n",
      "Subject 17, Epoch 277, Loss: 2.1559109687805176, Final Batch Loss: 0.8888875842094421\n",
      "Subject 17, Epoch 278, Loss: 1.9311230778694153, Final Batch Loss: 0.5614102482795715\n",
      "Subject 17, Epoch 279, Loss: 1.937963992357254, Final Batch Loss: 0.6071090698242188\n",
      "Subject 17, Epoch 280, Loss: 1.7730132341384888, Final Batch Loss: 0.4832614064216614\n",
      "Subject 17, Epoch 281, Loss: 1.6805374026298523, Final Batch Loss: 0.37779369950294495\n",
      "Subject 17, Epoch 282, Loss: 1.8211432993412018, Final Batch Loss: 0.4406179189682007\n",
      "Subject 17, Epoch 283, Loss: 1.840258538722992, Final Batch Loss: 0.4909483790397644\n",
      "Subject 17, Epoch 284, Loss: 1.767987698316574, Final Batch Loss: 0.3968654274940491\n",
      "Subject 17, Epoch 285, Loss: 1.883516550064087, Final Batch Loss: 0.544689953327179\n",
      "Subject 17, Epoch 286, Loss: 1.9750093817710876, Final Batch Loss: 0.6496357917785645\n",
      "Subject 17, Epoch 287, Loss: 1.677992194890976, Final Batch Loss: 0.3211720585823059\n",
      "Subject 17, Epoch 288, Loss: 1.8554783463478088, Final Batch Loss: 0.4437781870365143\n",
      "Subject 17, Epoch 289, Loss: 1.8370835781097412, Final Batch Loss: 0.4569847583770752\n",
      "Subject 17, Epoch 290, Loss: 1.9744810461997986, Final Batch Loss: 0.49791938066482544\n",
      "Subject 17, Epoch 291, Loss: 1.830306351184845, Final Batch Loss: 0.4461986720561981\n",
      "Subject 17, Epoch 292, Loss: 1.7309355735778809, Final Batch Loss: 0.36679527163505554\n",
      "Subject 17, Epoch 293, Loss: 1.7382907569408417, Final Batch Loss: 0.3384465277194977\n",
      "Subject 17, Epoch 294, Loss: 1.8328976035118103, Final Batch Loss: 0.5217112898826599\n",
      "Subject 17, Epoch 295, Loss: 1.8298296630382538, Final Batch Loss: 0.5342500805854797\n",
      "Subject 17, Epoch 296, Loss: 1.663591593503952, Final Batch Loss: 0.3708658218383789\n",
      "Subject 17, Epoch 297, Loss: 1.8242578208446503, Final Batch Loss: 0.5534664988517761\n",
      "Subject 17, Epoch 298, Loss: 1.681097388267517, Final Batch Loss: 0.4147454798221588\n",
      "Subject 17, Epoch 299, Loss: 2.0047620832920074, Final Batch Loss: 0.7596061825752258\n",
      "Subject 17, Epoch 300, Loss: 1.84365576505661, Final Batch Loss: 0.479542076587677\n",
      "Subject 17, Epoch 301, Loss: 2.0661795139312744, Final Batch Loss: 0.7699220776557922\n",
      "Subject 17, Epoch 302, Loss: 1.9289275705814362, Final Batch Loss: 0.6076067686080933\n",
      "Subject 17, Epoch 303, Loss: 1.8098743259906769, Final Batch Loss: 0.4828144907951355\n",
      "Subject 17, Epoch 304, Loss: 1.7459596991539001, Final Batch Loss: 0.45155978202819824\n",
      "Subject 17, Epoch 305, Loss: 1.6697440147399902, Final Batch Loss: 0.37195172905921936\n",
      "Subject 17, Epoch 306, Loss: 1.8475486040115356, Final Batch Loss: 0.5072758197784424\n",
      "Subject 17, Epoch 307, Loss: 1.884195476770401, Final Batch Loss: 0.6370493173599243\n",
      "Subject 17, Epoch 308, Loss: 1.708316296339035, Final Batch Loss: 0.4389459192752838\n",
      "Subject 17, Epoch 309, Loss: 1.6399931609630585, Final Batch Loss: 0.2998567819595337\n",
      "Subject 17, Epoch 310, Loss: 1.841464251279831, Final Batch Loss: 0.5163559317588806\n",
      "Subject 17, Epoch 311, Loss: 1.6033943891525269, Final Batch Loss: 0.3419373631477356\n",
      "Subject 17, Epoch 312, Loss: 1.7245360612869263, Final Batch Loss: 0.3816029131412506\n",
      "Subject 17, Epoch 313, Loss: 1.6446662247180939, Final Batch Loss: 0.3689318597316742\n",
      "Subject 17, Epoch 314, Loss: 1.7920061945915222, Final Batch Loss: 0.5127780437469482\n",
      "Subject 17, Epoch 315, Loss: 1.7138870656490326, Final Batch Loss: 0.4742055833339691\n",
      "Subject 17, Epoch 316, Loss: 1.6764393150806427, Final Batch Loss: 0.343721479177475\n",
      "Subject 17, Epoch 317, Loss: 1.8038136661052704, Final Batch Loss: 0.5458218455314636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 318, Loss: 1.7419173121452332, Final Batch Loss: 0.5342323184013367\n",
      "Subject 17, Epoch 319, Loss: 1.5491541028022766, Final Batch Loss: 0.30512863397598267\n",
      "Subject 17, Epoch 320, Loss: 1.7302979230880737, Final Batch Loss: 0.5053917169570923\n",
      "Subject 17, Epoch 321, Loss: 1.7359574139118195, Final Batch Loss: 0.5258887410163879\n",
      "Subject 17, Epoch 322, Loss: 1.7078299224376678, Final Batch Loss: 0.36226949095726013\n",
      "Subject 17, Epoch 323, Loss: 1.6880371570587158, Final Batch Loss: 0.3684464991092682\n",
      "Subject 17, Epoch 324, Loss: 1.7491689026355743, Final Batch Loss: 0.4387074112892151\n",
      "Subject 17, Epoch 325, Loss: 1.7992190718650818, Final Batch Loss: 0.6229237914085388\n",
      "Subject 17, Epoch 326, Loss: 1.6033406555652618, Final Batch Loss: 0.35794156789779663\n",
      "Subject 17, Epoch 327, Loss: 1.5881127715110779, Final Batch Loss: 0.2756924331188202\n",
      "Subject 17, Epoch 328, Loss: 1.4943545162677765, Final Batch Loss: 0.2234765887260437\n",
      "Subject 17, Epoch 329, Loss: 1.5917151272296906, Final Batch Loss: 0.30232134461402893\n",
      "Subject 17, Epoch 330, Loss: 1.5142747610807419, Final Batch Loss: 0.24781997501850128\n",
      "Subject 17, Epoch 331, Loss: 1.57279634475708, Final Batch Loss: 0.2563326358795166\n",
      "Subject 17, Epoch 332, Loss: 1.5808050036430359, Final Batch Loss: 0.39638224244117737\n",
      "Subject 17, Epoch 333, Loss: 1.6682104766368866, Final Batch Loss: 0.33063453435897827\n",
      "Subject 17, Epoch 334, Loss: 1.661320835351944, Final Batch Loss: 0.45428723096847534\n",
      "Subject 17, Epoch 335, Loss: 1.7250701189041138, Final Batch Loss: 0.4688974618911743\n",
      "Subject 17, Epoch 336, Loss: 1.5622051060199738, Final Batch Loss: 0.33934423327445984\n",
      "Subject 17, Epoch 337, Loss: 1.6036005318164825, Final Batch Loss: 0.4475036561489105\n",
      "Subject 17, Epoch 338, Loss: 1.528958410024643, Final Batch Loss: 0.3421885669231415\n",
      "Subject 17, Epoch 339, Loss: 1.5317699015140533, Final Batch Loss: 0.3448307514190674\n",
      "Subject 17, Epoch 340, Loss: 1.5382173359394073, Final Batch Loss: 0.3823482394218445\n",
      "Subject 17, Epoch 341, Loss: 1.5738699436187744, Final Batch Loss: 0.3844369947910309\n",
      "Subject 17, Epoch 342, Loss: 1.4659987688064575, Final Batch Loss: 0.2871101200580597\n",
      "Subject 17, Epoch 343, Loss: 1.8014284372329712, Final Batch Loss: 0.6258894801139832\n",
      "Subject 17, Epoch 344, Loss: 1.4897737801074982, Final Batch Loss: 0.3814816176891327\n",
      "Subject 17, Epoch 345, Loss: 1.417321652173996, Final Batch Loss: 0.1429215967655182\n",
      "Subject 17, Epoch 346, Loss: 1.5501286387443542, Final Batch Loss: 0.33652839064598083\n",
      "Subject 17, Epoch 347, Loss: 1.4135837256908417, Final Batch Loss: 0.2514609396457672\n",
      "Subject 17, Epoch 348, Loss: 1.4964815378189087, Final Batch Loss: 0.3365492820739746\n",
      "Subject 17, Epoch 349, Loss: 1.5312964022159576, Final Batch Loss: 0.36085399985313416\n",
      "Subject 17, Epoch 350, Loss: 1.3901934921741486, Final Batch Loss: 0.29534807801246643\n",
      "Subject 17, Epoch 351, Loss: 1.4052104353904724, Final Batch Loss: 0.2649637460708618\n",
      "Subject 17, Epoch 352, Loss: 1.4354247152805328, Final Batch Loss: 0.35648471117019653\n",
      "Subject 17, Epoch 353, Loss: 1.4372589886188507, Final Batch Loss: 0.2700619101524353\n",
      "Subject 17, Epoch 354, Loss: 1.6934499442577362, Final Batch Loss: 0.6275025606155396\n",
      "Subject 17, Epoch 355, Loss: 1.4225088953971863, Final Batch Loss: 0.3288336396217346\n",
      "Subject 17, Epoch 356, Loss: 1.5541735291481018, Final Batch Loss: 0.43948739767074585\n",
      "Subject 17, Epoch 357, Loss: 1.4252192974090576, Final Batch Loss: 0.34899044036865234\n",
      "Subject 17, Epoch 358, Loss: 1.4097014963626862, Final Batch Loss: 0.3270600736141205\n",
      "Subject 17, Epoch 359, Loss: 1.6069710850715637, Final Batch Loss: 0.5489782691001892\n",
      "Subject 17, Epoch 360, Loss: 1.393282264471054, Final Batch Loss: 0.25699540972709656\n",
      "Subject 17, Epoch 361, Loss: 1.4046281278133392, Final Batch Loss: 0.30801236629486084\n",
      "Subject 17, Epoch 362, Loss: 1.5025196373462677, Final Batch Loss: 0.3770892918109894\n",
      "Subject 17, Epoch 363, Loss: 1.5451729595661163, Final Batch Loss: 0.4589821696281433\n",
      "Subject 17, Epoch 364, Loss: 1.3372294455766678, Final Batch Loss: 0.2342555969953537\n",
      "Subject 17, Epoch 365, Loss: 1.382486492395401, Final Batch Loss: 0.33136120438575745\n",
      "Subject 17, Epoch 366, Loss: 1.5319873094558716, Final Batch Loss: 0.4482308030128479\n",
      "Subject 17, Epoch 367, Loss: 1.5367031395435333, Final Batch Loss: 0.5392534732818604\n",
      "Subject 17, Epoch 368, Loss: 1.586340069770813, Final Batch Loss: 0.5274742245674133\n",
      "Subject 17, Epoch 369, Loss: 1.5497399866580963, Final Batch Loss: 0.36573466658592224\n",
      "Subject 17, Epoch 370, Loss: 1.50637885928154, Final Batch Loss: 0.42726606130599976\n",
      "Subject 17, Epoch 371, Loss: 1.3388962745666504, Final Batch Loss: 0.18187835812568665\n",
      "Subject 17, Epoch 372, Loss: 1.3004281669855118, Final Batch Loss: 0.2056303471326828\n",
      "Subject 17, Epoch 373, Loss: 1.3238950669765472, Final Batch Loss: 0.2460673749446869\n",
      "Subject 17, Epoch 374, Loss: 1.463558852672577, Final Batch Loss: 0.3669567406177521\n",
      "Subject 17, Epoch 375, Loss: 1.6998957395553589, Final Batch Loss: 0.6578909158706665\n",
      "Subject 17, Epoch 376, Loss: 1.4360812604427338, Final Batch Loss: 0.40942272543907166\n",
      "Subject 17, Epoch 377, Loss: 1.399720013141632, Final Batch Loss: 0.3186819553375244\n",
      "Subject 17, Epoch 378, Loss: 1.4726166129112244, Final Batch Loss: 0.37633010745048523\n",
      "Subject 17, Epoch 379, Loss: 1.4158889651298523, Final Batch Loss: 0.3748662769794464\n",
      "Subject 17, Epoch 380, Loss: 1.2993204444646835, Final Batch Loss: 0.18186037242412567\n",
      "Subject 17, Epoch 381, Loss: 1.3968222439289093, Final Batch Loss: 0.34960487484931946\n",
      "Subject 17, Epoch 382, Loss: 1.3958758413791656, Final Batch Loss: 0.4258550703525543\n",
      "Subject 17, Epoch 383, Loss: 1.369348555803299, Final Batch Loss: 0.31777632236480713\n",
      "Subject 17, Epoch 384, Loss: 1.515560805797577, Final Batch Loss: 0.29588213562965393\n",
      "Subject 17, Epoch 385, Loss: 1.8457779288291931, Final Batch Loss: 0.7640060782432556\n",
      "Subject 17, Epoch 386, Loss: 1.388429969549179, Final Batch Loss: 0.31322211027145386\n",
      "Subject 17, Epoch 387, Loss: 1.4019899368286133, Final Batch Loss: 0.30115094780921936\n",
      "Subject 17, Epoch 388, Loss: 1.4957623481750488, Final Batch Loss: 0.31666624546051025\n",
      "Subject 17, Epoch 389, Loss: 1.5976525843143463, Final Batch Loss: 0.5629618167877197\n",
      "Subject 17, Epoch 390, Loss: 1.4412867426872253, Final Batch Loss: 0.4363669753074646\n",
      "Subject 17, Epoch 391, Loss: 1.6623149812221527, Final Batch Loss: 0.6150974631309509\n",
      "Subject 17, Epoch 392, Loss: 1.3891578316688538, Final Batch Loss: 0.3271036148071289\n",
      "Subject 17, Epoch 393, Loss: 1.3513204157352448, Final Batch Loss: 0.3141050934791565\n",
      "Subject 17, Epoch 394, Loss: 1.6178142726421356, Final Batch Loss: 0.4793007969856262\n",
      "Subject 17, Epoch 395, Loss: 1.3872219026088715, Final Batch Loss: 0.333330899477005\n",
      "Subject 17, Epoch 396, Loss: 1.3416417837142944, Final Batch Loss: 0.3468274176120758\n",
      "Subject 17, Epoch 397, Loss: 1.5992710888385773, Final Batch Loss: 0.6012959480285645\n",
      "Subject 17, Epoch 398, Loss: 1.3796509206295013, Final Batch Loss: 0.314077228307724\n",
      "Subject 17, Epoch 399, Loss: 1.336273968219757, Final Batch Loss: 0.2892097234725952\n",
      "Subject 17, Epoch 400, Loss: 1.259098842740059, Final Batch Loss: 0.21884049475193024\n",
      "Subject 17, Epoch 401, Loss: 1.3412566483020782, Final Batch Loss: 0.31268900632858276\n",
      "Subject 17, Epoch 402, Loss: 1.3868377804756165, Final Batch Loss: 0.29577329754829407\n",
      "Subject 17, Epoch 403, Loss: 1.400710105895996, Final Batch Loss: 0.3350678086280823\n",
      "Subject 17, Epoch 404, Loss: 1.2871122360229492, Final Batch Loss: 0.2953779995441437\n",
      "Subject 17, Epoch 405, Loss: 1.22673662006855, Final Batch Loss: 0.19301749765872955\n",
      "Subject 17, Epoch 406, Loss: 1.3680437505245209, Final Batch Loss: 0.34430044889450073\n",
      "Subject 17, Epoch 407, Loss: 1.3747521042823792, Final Batch Loss: 0.36253252625465393\n",
      "Subject 17, Epoch 408, Loss: 1.359251081943512, Final Batch Loss: 0.3093157410621643\n",
      "Subject 17, Epoch 409, Loss: 1.3024971634149551, Final Batch Loss: 0.23881126940250397\n",
      "Subject 17, Epoch 410, Loss: 1.3249027729034424, Final Batch Loss: 0.33760687708854675\n",
      "Subject 17, Epoch 411, Loss: 1.3706539571285248, Final Batch Loss: 0.25765323638916016\n",
      "Subject 17, Epoch 412, Loss: 1.2300344705581665, Final Batch Loss: 0.2306203842163086\n",
      "Subject 17, Epoch 413, Loss: 1.3038869202136993, Final Batch Loss: 0.2666223645210266\n",
      "Subject 17, Epoch 414, Loss: 1.3313314020633698, Final Batch Loss: 0.3306746184825897\n",
      "Subject 17, Epoch 415, Loss: 1.3611851334571838, Final Batch Loss: 0.288499116897583\n",
      "Subject 17, Epoch 416, Loss: 1.4689556956291199, Final Batch Loss: 0.5541322231292725\n",
      "Subject 17, Epoch 417, Loss: 1.6372672319412231, Final Batch Loss: 0.668548583984375\n",
      "Subject 17, Epoch 418, Loss: 1.2808807492256165, Final Batch Loss: 0.27812597155570984\n",
      "Subject 17, Epoch 419, Loss: 1.3910321593284607, Final Batch Loss: 0.4405075013637543\n",
      "Subject 17, Epoch 420, Loss: 1.355835348367691, Final Batch Loss: 0.36168432235717773\n",
      "Subject 17, Epoch 421, Loss: 1.1618431210517883, Final Batch Loss: 0.14488612115383148\n",
      "Subject 17, Epoch 422, Loss: 1.4840939939022064, Final Batch Loss: 0.49484026432037354\n",
      "Subject 17, Epoch 423, Loss: 1.4201222360134125, Final Batch Loss: 0.376812219619751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 424, Loss: 1.3887497782707214, Final Batch Loss: 0.40938153862953186\n",
      "Subject 17, Epoch 425, Loss: 1.6338458061218262, Final Batch Loss: 0.7519819736480713\n",
      "Subject 17, Epoch 426, Loss: 1.2519289553165436, Final Batch Loss: 0.28419774770736694\n",
      "Subject 17, Epoch 427, Loss: 1.3175512552261353, Final Batch Loss: 0.32576969265937805\n",
      "Subject 17, Epoch 428, Loss: 1.3047707676887512, Final Batch Loss: 0.2960851192474365\n",
      "Subject 17, Epoch 429, Loss: 1.4152199029922485, Final Batch Loss: 0.47928133606910706\n",
      "Subject 17, Epoch 430, Loss: 1.2158223539590836, Final Batch Loss: 0.24942584335803986\n",
      "Subject 17, Epoch 431, Loss: 1.2691317945718765, Final Batch Loss: 0.26484033465385437\n",
      "Subject 17, Epoch 432, Loss: 1.3588939607143402, Final Batch Loss: 0.3370317220687866\n",
      "Subject 17, Epoch 433, Loss: 1.1936420798301697, Final Batch Loss: 0.2378530502319336\n",
      "Subject 17, Epoch 434, Loss: 1.2797576487064362, Final Batch Loss: 0.2875826358795166\n",
      "Subject 17, Epoch 435, Loss: 1.2934035658836365, Final Batch Loss: 0.2544630765914917\n",
      "Subject 17, Epoch 436, Loss: 1.200937807559967, Final Batch Loss: 0.21653223037719727\n",
      "Subject 17, Epoch 437, Loss: 1.3768969774246216, Final Batch Loss: 0.3657557964324951\n",
      "Subject 17, Epoch 438, Loss: 1.231457680463791, Final Batch Loss: 0.24109399318695068\n",
      "Subject 17, Epoch 439, Loss: 1.50862255692482, Final Batch Loss: 0.3936147689819336\n",
      "Subject 17, Epoch 440, Loss: 1.3233186304569244, Final Batch Loss: 0.3166891038417816\n",
      "Subject 17, Epoch 441, Loss: 1.2382538467645645, Final Batch Loss: 0.2519209384918213\n",
      "Subject 17, Epoch 442, Loss: 1.270386666059494, Final Batch Loss: 0.2839345633983612\n",
      "Subject 17, Epoch 443, Loss: 1.233013778924942, Final Batch Loss: 0.25152790546417236\n",
      "Subject 17, Epoch 444, Loss: 1.5611878037452698, Final Batch Loss: 0.6526589393615723\n",
      "Subject 17, Epoch 445, Loss: 1.2423080801963806, Final Batch Loss: 0.2785003185272217\n",
      "Subject 17, Epoch 446, Loss: 1.3102891743183136, Final Batch Loss: 0.32937976717948914\n",
      "Subject 17, Epoch 447, Loss: 1.3210801780223846, Final Batch Loss: 0.3065725564956665\n",
      "Subject 17, Epoch 448, Loss: 1.1810514777898788, Final Batch Loss: 0.13612352311611176\n",
      "Subject 17, Epoch 449, Loss: 1.1919897198677063, Final Batch Loss: 0.2251882255077362\n",
      "Subject 17, Epoch 450, Loss: 1.1585349142551422, Final Batch Loss: 0.2799850106239319\n",
      "Subject 17, Epoch 451, Loss: 1.2974512875080109, Final Batch Loss: 0.31734150648117065\n",
      "Subject 17, Epoch 452, Loss: 1.2152641415596008, Final Batch Loss: 0.26617828011512756\n",
      "Subject 17, Epoch 453, Loss: 1.2413984686136246, Final Batch Loss: 0.2275373488664627\n",
      "Subject 17, Epoch 454, Loss: 1.523871123790741, Final Batch Loss: 0.6273894309997559\n",
      "Subject 17, Epoch 455, Loss: 1.2525865733623505, Final Batch Loss: 0.2608583867549896\n",
      "Subject 17, Epoch 456, Loss: 1.6778286695480347, Final Batch Loss: 0.8056036829948425\n",
      "Subject 17, Epoch 457, Loss: 1.3238143026828766, Final Batch Loss: 0.35768675804138184\n",
      "Subject 17, Epoch 458, Loss: 1.2514684200286865, Final Batch Loss: 0.29168930649757385\n",
      "Subject 17, Epoch 459, Loss: 1.234893947839737, Final Batch Loss: 0.2735827565193176\n",
      "Subject 17, Epoch 460, Loss: 1.333249032497406, Final Batch Loss: 0.42653313279151917\n",
      "Subject 17, Epoch 461, Loss: 1.0843529403209686, Final Batch Loss: 0.16039568185806274\n",
      "Subject 17, Epoch 462, Loss: 1.1845638900995255, Final Batch Loss: 0.20206455886363983\n",
      "Subject 17, Epoch 463, Loss: 1.5347625315189362, Final Batch Loss: 0.6596646308898926\n",
      "Subject 17, Epoch 464, Loss: 1.25277978181839, Final Batch Loss: 0.2635018527507782\n",
      "Subject 17, Epoch 465, Loss: 1.2122597694396973, Final Batch Loss: 0.2828920781612396\n",
      "Subject 17, Epoch 466, Loss: 1.168196976184845, Final Batch Loss: 0.20928257703781128\n",
      "Subject 17, Epoch 467, Loss: 1.2785485088825226, Final Batch Loss: 0.3680398464202881\n",
      "Subject 17, Epoch 468, Loss: 1.4362899661064148, Final Batch Loss: 0.5339283347129822\n",
      "Subject 17, Epoch 469, Loss: 1.1712636500597, Final Batch Loss: 0.22758571803569794\n",
      "Subject 17, Epoch 470, Loss: 1.1184771358966827, Final Batch Loss: 0.1944539099931717\n",
      "Subject 17, Epoch 471, Loss: 1.2116925716400146, Final Batch Loss: 0.2934856116771698\n",
      "Subject 17, Epoch 472, Loss: 1.1843895465135574, Final Batch Loss: 0.24587048590183258\n",
      "Subject 17, Epoch 473, Loss: 1.291644811630249, Final Batch Loss: 0.36735138297080994\n",
      "Subject 17, Epoch 474, Loss: 1.2367474734783173, Final Batch Loss: 0.32483983039855957\n",
      "Subject 17, Epoch 475, Loss: 1.3473649471998215, Final Batch Loss: 0.19795945286750793\n",
      "Subject 17, Epoch 476, Loss: 1.217649519443512, Final Batch Loss: 0.2888565957546234\n",
      "Subject 17, Epoch 477, Loss: 1.301550179719925, Final Batch Loss: 0.36949676275253296\n",
      "Subject 17, Epoch 478, Loss: 1.0805616080760956, Final Batch Loss: 0.13982951641082764\n",
      "Subject 17, Epoch 479, Loss: 1.1797569692134857, Final Batch Loss: 0.3240474760532379\n",
      "Subject 17, Epoch 480, Loss: 1.2633393704891205, Final Batch Loss: 0.3260076642036438\n",
      "Subject 17, Epoch 481, Loss: 1.3893406689167023, Final Batch Loss: 0.522437572479248\n",
      "Subject 17, Epoch 482, Loss: 1.1979392319917679, Final Batch Loss: 0.2430199533700943\n",
      "Subject 17, Epoch 483, Loss: 1.0655791461467743, Final Batch Loss: 0.1747453808784485\n",
      "Subject 17, Epoch 484, Loss: 1.1623914688825607, Final Batch Loss: 0.23313838243484497\n",
      "Subject 17, Epoch 485, Loss: 1.3254497051239014, Final Batch Loss: 0.38417813181877136\n",
      "Subject 17, Epoch 486, Loss: 0.9998216181993484, Final Batch Loss: 0.1801050454378128\n",
      "Subject 17, Epoch 487, Loss: 1.3903536796569824, Final Batch Loss: 0.2981109619140625\n",
      "Subject 17, Epoch 488, Loss: 1.1520631462335587, Final Batch Loss: 0.20376987755298615\n",
      "Subject 17, Epoch 489, Loss: 1.1533477008342743, Final Batch Loss: 0.22558501362800598\n",
      "Subject 17, Epoch 490, Loss: 0.9986365884542465, Final Batch Loss: 0.13619185984134674\n",
      "Subject 17, Epoch 491, Loss: 1.0787337720394135, Final Batch Loss: 0.17837491631507874\n",
      "Subject 17, Epoch 492, Loss: 1.3532353937625885, Final Batch Loss: 0.4426071345806122\n",
      "Subject 17, Epoch 493, Loss: 1.239092692732811, Final Batch Loss: 0.3541441857814789\n",
      "Subject 17, Epoch 494, Loss: 1.1857779026031494, Final Batch Loss: 0.2757178246974945\n",
      "Subject 17, Epoch 495, Loss: 1.178115040063858, Final Batch Loss: 0.2365853190422058\n",
      "Subject 17, Epoch 496, Loss: 1.1309431046247482, Final Batch Loss: 0.19846707582473755\n",
      "Subject 17, Epoch 497, Loss: 1.1749169826507568, Final Batch Loss: 0.254892498254776\n",
      "Subject 17, Epoch 498, Loss: 1.0844462662935257, Final Batch Loss: 0.2160947471857071\n",
      "Subject 17, Epoch 499, Loss: 1.260455459356308, Final Batch Loss: 0.3860812485218048\n",
      "Subject 17, Epoch 500, Loss: 1.0291520804166794, Final Batch Loss: 0.13705603778362274\n",
      "Subject 17, Epoch 501, Loss: 1.2535262554883957, Final Batch Loss: 0.49135899543762207\n",
      "Subject 17, Epoch 502, Loss: 1.2040534615516663, Final Batch Loss: 0.3797920346260071\n",
      "Subject 17, Epoch 503, Loss: 1.1993996351957321, Final Batch Loss: 0.35854214429855347\n",
      "Subject 17, Epoch 504, Loss: 1.0952068865299225, Final Batch Loss: 0.27492356300354004\n",
      "Subject 17, Epoch 505, Loss: 0.971604872494936, Final Batch Loss: 0.05576397851109505\n",
      "Subject 17, Epoch 506, Loss: 1.0949656069278717, Final Batch Loss: 0.22292867302894592\n",
      "Subject 17, Epoch 507, Loss: 1.0612692683935165, Final Batch Loss: 0.23453080654144287\n",
      "Subject 17, Epoch 508, Loss: 1.0121822506189346, Final Batch Loss: 0.15134285390377045\n",
      "Subject 17, Epoch 509, Loss: 1.0743797421455383, Final Batch Loss: 0.25363656878471375\n",
      "Subject 17, Epoch 510, Loss: 1.0465789139270782, Final Batch Loss: 0.1655735969543457\n",
      "Subject 17, Epoch 511, Loss: 1.140154853463173, Final Batch Loss: 0.2600471079349518\n",
      "Subject 17, Epoch 512, Loss: 1.171508252620697, Final Batch Loss: 0.2782408595085144\n",
      "Subject 17, Epoch 513, Loss: 1.3424806594848633, Final Batch Loss: 0.5445852875709534\n",
      "Subject 17, Epoch 514, Loss: 1.0505269914865494, Final Batch Loss: 0.21673913300037384\n",
      "Subject 17, Epoch 515, Loss: 1.117065817117691, Final Batch Loss: 0.24417302012443542\n",
      "Subject 17, Epoch 516, Loss: 1.1655237972736359, Final Batch Loss: 0.28044915199279785\n",
      "Subject 17, Epoch 517, Loss: 1.3434390276670456, Final Batch Loss: 0.19446535408496857\n",
      "Subject 17, Epoch 518, Loss: 0.9595282524824142, Final Batch Loss: 0.17821410298347473\n",
      "Subject 17, Epoch 519, Loss: 1.3266995549201965, Final Batch Loss: 0.4322970509529114\n",
      "Subject 17, Epoch 520, Loss: 1.1199995875358582, Final Batch Loss: 0.28836068511009216\n",
      "Subject 17, Epoch 521, Loss: 1.1167231500148773, Final Batch Loss: 0.2540118992328644\n",
      "Subject 17, Epoch 522, Loss: 1.2017537355422974, Final Batch Loss: 0.33866631984710693\n",
      "Subject 17, Epoch 523, Loss: 1.1056758910417557, Final Batch Loss: 0.1500413864850998\n",
      "Subject 17, Epoch 524, Loss: 1.1821570992469788, Final Batch Loss: 0.3315769135951996\n",
      "Subject 17, Epoch 525, Loss: 1.1538812965154648, Final Batch Loss: 0.3469207286834717\n",
      "Subject 17, Epoch 526, Loss: 1.1697636097669601, Final Batch Loss: 0.37956953048706055\n",
      "Subject 17, Epoch 527, Loss: 1.127196580171585, Final Batch Loss: 0.24152851104736328\n",
      "Subject 17, Epoch 528, Loss: 1.0591461211442947, Final Batch Loss: 0.2659119665622711\n",
      "Subject 17, Epoch 529, Loss: 1.0636104941368103, Final Batch Loss: 0.32940348982810974\n",
      "Subject 17, Epoch 530, Loss: 1.0965954512357712, Final Batch Loss: 0.24533648788928986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 531, Loss: 1.18269082903862, Final Batch Loss: 0.3110530376434326\n",
      "Subject 17, Epoch 532, Loss: 1.3852342367172241, Final Batch Loss: 0.6021940112113953\n",
      "Subject 17, Epoch 533, Loss: 1.1038784384727478, Final Batch Loss: 0.28264960646629333\n",
      "Subject 17, Epoch 534, Loss: 1.092028632760048, Final Batch Loss: 0.24850134551525116\n",
      "Subject 17, Epoch 535, Loss: 1.0543707311153412, Final Batch Loss: 0.3359828293323517\n",
      "Subject 17, Epoch 536, Loss: 0.9915476739406586, Final Batch Loss: 0.17526468634605408\n",
      "Subject 17, Epoch 537, Loss: 1.186886191368103, Final Batch Loss: 0.3911025822162628\n",
      "Subject 17, Epoch 538, Loss: 1.0135572254657745, Final Batch Loss: 0.21673478186130524\n",
      "Subject 17, Epoch 539, Loss: 1.1401642262935638, Final Batch Loss: 0.3312012553215027\n",
      "Subject 17, Epoch 540, Loss: 1.0498261749744415, Final Batch Loss: 0.31239086389541626\n",
      "Subject 17, Epoch 541, Loss: 1.0321316421031952, Final Batch Loss: 0.23791305720806122\n",
      "Subject 17, Epoch 542, Loss: 1.1295257210731506, Final Batch Loss: 0.32609519362449646\n",
      "Subject 17, Epoch 543, Loss: 1.277409866452217, Final Batch Loss: 0.566761314868927\n",
      "Subject 17, Epoch 544, Loss: 1.7341632395982742, Final Batch Loss: 0.8283267617225647\n",
      "Subject 17, Epoch 545, Loss: 1.0287481248378754, Final Batch Loss: 0.2058488130569458\n",
      "Subject 17, Epoch 546, Loss: 0.9410229474306107, Final Batch Loss: 0.15157465636730194\n",
      "Subject 17, Epoch 547, Loss: 0.9827924370765686, Final Batch Loss: 0.25801974534988403\n",
      "Subject 17, Epoch 548, Loss: 0.9586275815963745, Final Batch Loss: 0.20557011663913727\n",
      "Subject 17, Epoch 549, Loss: 1.0479677468538284, Final Batch Loss: 0.21130606532096863\n",
      "Subject 17, Epoch 550, Loss: 0.9123384803533554, Final Batch Loss: 0.12881605327129364\n",
      "Subject 17, Epoch 551, Loss: 0.9497631937265396, Final Batch Loss: 0.18823103606700897\n",
      "Subject 17, Epoch 552, Loss: 1.0798479914665222, Final Batch Loss: 0.21117547154426575\n",
      "Subject 17, Epoch 553, Loss: 0.8577858433127403, Final Batch Loss: 0.09943496435880661\n",
      "Subject 17, Epoch 554, Loss: 1.204804390668869, Final Batch Loss: 0.17952406406402588\n",
      "Subject 17, Epoch 555, Loss: 1.0085690915584564, Final Batch Loss: 0.20361900329589844\n",
      "Subject 17, Epoch 556, Loss: 1.3034638315439224, Final Batch Loss: 0.5039389729499817\n",
      "Subject 17, Epoch 557, Loss: 1.1815707832574844, Final Batch Loss: 0.39773598313331604\n",
      "Subject 17, Epoch 558, Loss: 1.1496649086475372, Final Batch Loss: 0.3440554440021515\n",
      "Subject 17, Epoch 559, Loss: 1.059889480471611, Final Batch Loss: 0.24806272983551025\n",
      "Subject 17, Epoch 560, Loss: 0.9521980285644531, Final Batch Loss: 0.15462668240070343\n",
      "Subject 17, Epoch 561, Loss: 0.9443973153829575, Final Batch Loss: 0.15719588100910187\n",
      "Subject 17, Epoch 562, Loss: 1.0467652678489685, Final Batch Loss: 0.3082813024520874\n",
      "Subject 17, Epoch 563, Loss: 1.0799137502908707, Final Batch Loss: 0.2756044864654541\n",
      "Subject 17, Epoch 564, Loss: 1.040374755859375, Final Batch Loss: 0.21805576980113983\n",
      "Subject 17, Epoch 565, Loss: 1.0032758265733719, Final Batch Loss: 0.24777045845985413\n",
      "Subject 17, Epoch 566, Loss: 0.922838494181633, Final Batch Loss: 0.18280766904354095\n",
      "Subject 17, Epoch 567, Loss: 0.9541594013571739, Final Batch Loss: 0.12369439750909805\n",
      "Subject 17, Epoch 568, Loss: 1.0457125753164291, Final Batch Loss: 0.3570932149887085\n",
      "Subject 17, Epoch 569, Loss: 0.9501393288373947, Final Batch Loss: 0.19255386292934418\n",
      "Subject 17, Epoch 570, Loss: 1.0841704607009888, Final Batch Loss: 0.3736611008644104\n",
      "Subject 17, Epoch 571, Loss: 0.9467243552207947, Final Batch Loss: 0.1299179196357727\n",
      "Subject 17, Epoch 572, Loss: 1.1023043394088745, Final Batch Loss: 0.3571465313434601\n",
      "Subject 17, Epoch 573, Loss: 1.3801968395709991, Final Batch Loss: 0.6428787112236023\n",
      "Subject 17, Epoch 574, Loss: 1.1752062439918518, Final Batch Loss: 0.4184931814670563\n",
      "Subject 17, Epoch 575, Loss: 1.1055816411972046, Final Batch Loss: 0.30370083451271057\n",
      "Subject 17, Epoch 576, Loss: 0.9752503633499146, Final Batch Loss: 0.2746695578098297\n",
      "Subject 17, Epoch 577, Loss: 0.9989581108093262, Final Batch Loss: 0.1755923479795456\n",
      "Subject 17, Epoch 578, Loss: 1.0437489300966263, Final Batch Loss: 0.22443224489688873\n",
      "Subject 17, Epoch 579, Loss: 1.047565445303917, Final Batch Loss: 0.2742660343647003\n",
      "Subject 17, Epoch 580, Loss: 0.8924809545278549, Final Batch Loss: 0.13085578382015228\n",
      "Subject 17, Epoch 581, Loss: 1.2574802935123444, Final Batch Loss: 0.4493463933467865\n",
      "Subject 17, Epoch 582, Loss: 0.9861920475959778, Final Batch Loss: 0.23486754298210144\n",
      "Subject 17, Epoch 583, Loss: 1.5540202111005783, Final Batch Loss: 0.7774311900138855\n",
      "Subject 17, Epoch 584, Loss: 0.9493257701396942, Final Batch Loss: 0.26848483085632324\n",
      "Subject 17, Epoch 585, Loss: 1.146663248538971, Final Batch Loss: 0.2513267993927002\n",
      "Subject 17, Epoch 586, Loss: 0.9712403863668442, Final Batch Loss: 0.21008582413196564\n",
      "Subject 17, Epoch 587, Loss: 0.9468015879392624, Final Batch Loss: 0.18572135269641876\n",
      "Subject 17, Epoch 588, Loss: 0.8972690552473068, Final Batch Loss: 0.11192749440670013\n",
      "Subject 17, Epoch 589, Loss: 1.0735737085342407, Final Batch Loss: 0.3720052242279053\n",
      "Subject 17, Epoch 590, Loss: 0.868551179766655, Final Batch Loss: 0.17887143790721893\n",
      "Subject 17, Epoch 591, Loss: 0.8447487056255341, Final Batch Loss: 0.07126064598560333\n",
      "Subject 17, Epoch 592, Loss: 0.8438893035054207, Final Batch Loss: 0.06652136892080307\n",
      "Subject 17, Epoch 593, Loss: 1.288340002298355, Final Batch Loss: 0.6116212606430054\n",
      "Subject 17, Epoch 594, Loss: 0.9853249937295914, Final Batch Loss: 0.24956341087818146\n",
      "Subject 17, Epoch 595, Loss: 0.9550008326768875, Final Batch Loss: 0.2175469845533371\n",
      "Subject 17, Epoch 596, Loss: 0.84665647149086, Final Batch Loss: 0.14586730301380157\n",
      "Subject 17, Epoch 597, Loss: 1.0400744080543518, Final Batch Loss: 0.35049426555633545\n",
      "Subject 17, Epoch 598, Loss: 0.8536389693617821, Final Batch Loss: 0.0983949527144432\n",
      "Subject 17, Epoch 599, Loss: 0.9125276207923889, Final Batch Loss: 0.24522575736045837\n",
      "Subject 17, Epoch 600, Loss: 0.8213620856404305, Final Batch Loss: 0.09777716547250748\n",
      "Subject 17, Epoch 601, Loss: 0.8605799674987793, Final Batch Loss: 0.15338854491710663\n",
      "Subject 17, Epoch 602, Loss: 1.0024099797010422, Final Batch Loss: 0.20308513939380646\n",
      "Subject 17, Epoch 603, Loss: 1.2494007647037506, Final Batch Loss: 0.6576144695281982\n",
      "Subject 17, Epoch 604, Loss: 1.0155393183231354, Final Batch Loss: 0.2327394038438797\n",
      "Subject 17, Epoch 605, Loss: 1.0148651748895645, Final Batch Loss: 0.32633084058761597\n",
      "Subject 17, Epoch 606, Loss: 0.8881017863750458, Final Batch Loss: 0.0990462452173233\n",
      "Subject 17, Epoch 607, Loss: 0.8776021897792816, Final Batch Loss: 0.12735000252723694\n",
      "Subject 17, Epoch 608, Loss: 1.0278791189193726, Final Batch Loss: 0.25766080617904663\n",
      "Subject 17, Epoch 609, Loss: 0.8476085215806961, Final Batch Loss: 0.10986557602882385\n",
      "Subject 17, Epoch 610, Loss: 1.1208195388317108, Final Batch Loss: 0.3026992976665497\n",
      "Subject 17, Epoch 611, Loss: 0.8269246816635132, Final Batch Loss: 0.10142718255519867\n",
      "Subject 17, Epoch 612, Loss: 1.2086459398269653, Final Batch Loss: 0.5918782353401184\n",
      "Subject 17, Epoch 613, Loss: 0.98533995449543, Final Batch Loss: 0.177993044257164\n",
      "Subject 17, Epoch 614, Loss: 0.9532308578491211, Final Batch Loss: 0.26703158020973206\n",
      "Subject 17, Epoch 615, Loss: 0.8942117989063263, Final Batch Loss: 0.15745170414447784\n",
      "Subject 17, Epoch 616, Loss: 0.7961398065090179, Final Batch Loss: 0.13407090306282043\n",
      "Subject 17, Epoch 617, Loss: 0.8348834067583084, Final Batch Loss: 0.145816370844841\n",
      "Subject 17, Epoch 618, Loss: 0.9485240131616592, Final Batch Loss: 0.1748969554901123\n",
      "Subject 17, Epoch 619, Loss: 0.8247290700674057, Final Batch Loss: 0.19127584993839264\n",
      "Subject 17, Epoch 620, Loss: 0.9128285795450211, Final Batch Loss: 0.13671179115772247\n",
      "Subject 17, Epoch 621, Loss: 1.0083012729883194, Final Batch Loss: 0.19218036532402039\n",
      "Subject 17, Epoch 622, Loss: 0.8677778095006943, Final Batch Loss: 0.1763899028301239\n",
      "Subject 17, Epoch 623, Loss: 0.8576761335134506, Final Batch Loss: 0.2224452644586563\n",
      "Subject 17, Epoch 624, Loss: 0.8806423991918564, Final Batch Loss: 0.21539387106895447\n",
      "Subject 17, Epoch 625, Loss: 0.9024644941091537, Final Batch Loss: 0.14388242363929749\n",
      "Subject 17, Epoch 626, Loss: 0.8906749933958054, Final Batch Loss: 0.14340029656887054\n",
      "Subject 17, Epoch 627, Loss: 0.7784955278038979, Final Batch Loss: 0.09969320148229599\n",
      "Subject 17, Epoch 628, Loss: 0.9807364642620087, Final Batch Loss: 0.23847268521785736\n",
      "Subject 17, Epoch 629, Loss: 0.8790425807237625, Final Batch Loss: 0.2445850819349289\n",
      "Subject 17, Epoch 630, Loss: 0.8748024553060532, Final Batch Loss: 0.2115776389837265\n",
      "Subject 17, Epoch 631, Loss: 1.1696696430444717, Final Batch Loss: 0.5007706880569458\n",
      "Subject 17, Epoch 632, Loss: 1.0087498724460602, Final Batch Loss: 0.37264037132263184\n",
      "Subject 17, Epoch 633, Loss: 1.25861656665802, Final Batch Loss: 0.6148001551628113\n",
      "Subject 17, Epoch 634, Loss: 1.0060256123542786, Final Batch Loss: 0.37589511275291443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 635, Loss: 1.0443771481513977, Final Batch Loss: 0.3641743063926697\n",
      "Subject 17, Epoch 636, Loss: 0.8575756847858429, Final Batch Loss: 0.1982109397649765\n",
      "Subject 17, Epoch 637, Loss: 0.8508042544126511, Final Batch Loss: 0.1813981980085373\n",
      "Subject 17, Epoch 638, Loss: 0.962252214550972, Final Batch Loss: 0.303231805562973\n",
      "Subject 17, Epoch 639, Loss: 0.700842909514904, Final Batch Loss: 0.09911870211362839\n",
      "Subject 17, Epoch 640, Loss: 0.945278137922287, Final Batch Loss: 0.27233126759529114\n",
      "Subject 17, Epoch 641, Loss: 0.7202744483947754, Final Batch Loss: 0.05334599316120148\n",
      "Subject 17, Epoch 642, Loss: 0.8075205981731415, Final Batch Loss: 0.0979737788438797\n",
      "Subject 17, Epoch 643, Loss: 0.8259608745574951, Final Batch Loss: 0.2501855790615082\n",
      "Subject 17, Epoch 644, Loss: 0.9661672711372375, Final Batch Loss: 0.27062514424324036\n",
      "Subject 17, Epoch 645, Loss: 0.7456222176551819, Final Batch Loss: 0.15220294892787933\n",
      "Subject 17, Epoch 646, Loss: 1.1869197636842728, Final Batch Loss: 0.4001808762550354\n",
      "Subject 17, Epoch 647, Loss: 0.9296067506074905, Final Batch Loss: 0.24608007073402405\n",
      "Subject 17, Epoch 648, Loss: 0.9221870750188828, Final Batch Loss: 0.25608834624290466\n",
      "Subject 17, Epoch 649, Loss: 0.9516360908746719, Final Batch Loss: 0.23952913284301758\n",
      "Subject 17, Epoch 650, Loss: 0.8413194119930267, Final Batch Loss: 0.21452169120311737\n",
      "Subject 17, Epoch 651, Loss: 0.9022272825241089, Final Batch Loss: 0.13063326478004456\n",
      "Subject 17, Epoch 652, Loss: 0.8752850741147995, Final Batch Loss: 0.2228040248155594\n",
      "Subject 17, Epoch 653, Loss: 0.8103827685117722, Final Batch Loss: 0.11225144565105438\n",
      "Subject 17, Epoch 654, Loss: 0.7892294377088547, Final Batch Loss: 0.21927614510059357\n",
      "Subject 17, Epoch 655, Loss: 0.6913246214389801, Final Batch Loss: 0.13923688232898712\n",
      "Subject 17, Epoch 656, Loss: 0.7705634534358978, Final Batch Loss: 0.15917910635471344\n",
      "Subject 17, Epoch 657, Loss: 0.7458833754062653, Final Batch Loss: 0.1400824785232544\n",
      "Subject 17, Epoch 658, Loss: 0.7791638225317001, Final Batch Loss: 0.19066791236400604\n",
      "Subject 17, Epoch 659, Loss: 0.7632876858115196, Final Batch Loss: 0.06440622359514236\n",
      "Subject 17, Epoch 660, Loss: 0.741709977388382, Final Batch Loss: 0.15966616570949554\n",
      "Subject 17, Epoch 661, Loss: 0.7491752356290817, Final Batch Loss: 0.13997945189476013\n",
      "Subject 17, Epoch 662, Loss: 0.8502938151359558, Final Batch Loss: 0.18113741278648376\n",
      "Subject 17, Epoch 663, Loss: 0.821154847741127, Final Batch Loss: 0.2270871251821518\n",
      "Subject 17, Epoch 664, Loss: 0.6510939709842205, Final Batch Loss: 0.059911590069532394\n",
      "Subject 17, Epoch 665, Loss: 0.7729455679655075, Final Batch Loss: 0.14948098361492157\n",
      "Subject 17, Epoch 666, Loss: 0.6733664125204086, Final Batch Loss: 0.07889112830162048\n",
      "Subject 17, Epoch 667, Loss: 0.8008781224489212, Final Batch Loss: 0.24377503991127014\n",
      "Subject 17, Epoch 668, Loss: 0.648134395480156, Final Batch Loss: 0.12428170442581177\n",
      "Subject 17, Epoch 669, Loss: 0.6348503232002258, Final Batch Loss: 0.10847710072994232\n",
      "Subject 17, Epoch 670, Loss: 0.8520299047231674, Final Batch Loss: 0.27916282415390015\n",
      "Subject 17, Epoch 671, Loss: 0.7531799823045731, Final Batch Loss: 0.16392847895622253\n",
      "Subject 17, Epoch 672, Loss: 0.7439347356557846, Final Batch Loss: 0.15266236662864685\n",
      "Subject 17, Epoch 673, Loss: 0.8279383480548859, Final Batch Loss: 0.22922295331954956\n",
      "Subject 17, Epoch 674, Loss: 0.7455115020275116, Final Batch Loss: 0.15258283913135529\n",
      "Subject 17, Epoch 675, Loss: 0.7426616549491882, Final Batch Loss: 0.17701777815818787\n",
      "Subject 17, Epoch 676, Loss: 0.817104235291481, Final Batch Loss: 0.2587738335132599\n",
      "Subject 17, Epoch 677, Loss: 0.6505698189139366, Final Batch Loss: 0.10018856078386307\n",
      "Subject 17, Epoch 678, Loss: 0.5949316322803497, Final Batch Loss: 0.060395509004592896\n",
      "Subject 17, Epoch 679, Loss: 0.7785713747143745, Final Batch Loss: 0.09921789914369583\n",
      "Subject 17, Epoch 680, Loss: 0.730340301990509, Final Batch Loss: 0.22454141080379486\n",
      "Subject 17, Epoch 681, Loss: 1.0461644977331161, Final Batch Loss: 0.40879127383232117\n",
      "Subject 17, Epoch 682, Loss: 0.9127586930990219, Final Batch Loss: 0.2901824712753296\n",
      "Subject 17, Epoch 683, Loss: 1.006755456328392, Final Batch Loss: 0.4493691921234131\n",
      "Subject 17, Epoch 684, Loss: 1.2371802628040314, Final Batch Loss: 0.5957586169242859\n",
      "Subject 17, Epoch 685, Loss: 0.6702453345060349, Final Batch Loss: 0.12514561414718628\n",
      "Subject 17, Epoch 686, Loss: 0.7464569807052612, Final Batch Loss: 0.21094872057437897\n",
      "Subject 17, Epoch 687, Loss: 0.7674039006233215, Final Batch Loss: 0.1616136133670807\n",
      "Subject 17, Epoch 688, Loss: 0.7180804759263992, Final Batch Loss: 0.16070975363254547\n",
      "Subject 17, Epoch 689, Loss: 0.7498718351125717, Final Batch Loss: 0.1689615100622177\n",
      "Subject 17, Epoch 690, Loss: 0.673390582203865, Final Batch Loss: 0.1217900812625885\n",
      "Subject 17, Epoch 691, Loss: 0.93906369805336, Final Batch Loss: 0.4449695348739624\n",
      "Subject 17, Epoch 692, Loss: 0.7510009706020355, Final Batch Loss: 0.21884074807167053\n",
      "Subject 17, Epoch 693, Loss: 0.7679931223392487, Final Batch Loss: 0.21443286538124084\n",
      "Subject 17, Epoch 694, Loss: 0.7229524701833725, Final Batch Loss: 0.22288377583026886\n",
      "Subject 17, Epoch 695, Loss: 0.9298796355724335, Final Batch Loss: 0.42166829109191895\n",
      "Subject 17, Epoch 696, Loss: 0.7242753580212593, Final Batch Loss: 0.1978350579738617\n",
      "Subject 17, Epoch 697, Loss: 0.6429086402058601, Final Batch Loss: 0.09438169747591019\n",
      "Subject 17, Epoch 698, Loss: 0.6183390133082867, Final Batch Loss: 0.04136541113257408\n",
      "Subject 17, Epoch 699, Loss: 0.7138598784804344, Final Batch Loss: 0.08166820555925369\n",
      "Subject 17, Epoch 700, Loss: 0.6708894520998001, Final Batch Loss: 0.1978791356086731\n",
      "Subject 17, Epoch 701, Loss: 0.6043951213359833, Final Batch Loss: 0.07685884833335876\n",
      "Subject 17, Epoch 702, Loss: 0.7475760951638222, Final Batch Loss: 0.22316496074199677\n",
      "Subject 17, Epoch 703, Loss: 0.7916225492954254, Final Batch Loss: 0.26894229650497437\n",
      "Subject 17, Epoch 704, Loss: 0.5507451444864273, Final Batch Loss: 0.08208075165748596\n",
      "Subject 17, Epoch 705, Loss: 0.5267299041152, Final Batch Loss: 0.0496826246380806\n",
      "Subject 17, Epoch 706, Loss: 1.0785446986556053, Final Batch Loss: 0.5967023968696594\n",
      "Subject 17, Epoch 707, Loss: 0.6477450579404831, Final Batch Loss: 0.1653832346200943\n",
      "Subject 17, Epoch 708, Loss: 0.6240826100111008, Final Batch Loss: 0.08466164767742157\n",
      "Subject 17, Epoch 709, Loss: 0.7352401912212372, Final Batch Loss: 0.275847852230072\n",
      "Subject 17, Epoch 710, Loss: 0.587220124900341, Final Batch Loss: 0.08165644854307175\n",
      "Subject 17, Epoch 711, Loss: 0.6250223517417908, Final Batch Loss: 0.16985517740249634\n",
      "Subject 17, Epoch 712, Loss: 0.5961495116353035, Final Batch Loss: 0.09708438068628311\n",
      "Subject 17, Epoch 713, Loss: 0.5904591232538223, Final Batch Loss: 0.11643251776695251\n",
      "Subject 17, Epoch 714, Loss: 0.7461661398410797, Final Batch Loss: 0.2786470651626587\n",
      "Subject 17, Epoch 715, Loss: 0.7426390945911407, Final Batch Loss: 0.19748523831367493\n",
      "Subject 17, Epoch 716, Loss: 0.6464224085211754, Final Batch Loss: 0.19553950428962708\n",
      "Subject 17, Epoch 717, Loss: 0.7234824150800705, Final Batch Loss: 0.313883900642395\n",
      "Subject 17, Epoch 718, Loss: 0.6204866617918015, Final Batch Loss: 0.15090219676494598\n",
      "Subject 17, Epoch 719, Loss: 0.589165486395359, Final Batch Loss: 0.09180301427841187\n",
      "Subject 17, Epoch 720, Loss: 0.7172193825244904, Final Batch Loss: 0.14000488817691803\n",
      "Subject 17, Epoch 721, Loss: 0.6809248998761177, Final Batch Loss: 0.1943284571170807\n",
      "Subject 17, Epoch 722, Loss: 0.8580339401960373, Final Batch Loss: 0.13236939907073975\n",
      "Subject 17, Epoch 723, Loss: 0.8034224212169647, Final Batch Loss: 0.30866381525993347\n",
      "Subject 17, Epoch 724, Loss: 0.8174639865756035, Final Batch Loss: 0.1266520619392395\n",
      "Subject 17, Epoch 725, Loss: 0.9828173890709877, Final Batch Loss: 0.4848790466785431\n",
      "Subject 17, Epoch 726, Loss: 0.5986719578504562, Final Batch Loss: 0.16191703081130981\n",
      "Subject 17, Epoch 727, Loss: 0.8794906735420227, Final Batch Loss: 0.3537284731864929\n",
      "Subject 17, Epoch 728, Loss: 0.605569563806057, Final Batch Loss: 0.0835929587483406\n",
      "Subject 17, Epoch 729, Loss: 0.6544276922941208, Final Batch Loss: 0.0914282500743866\n",
      "Subject 17, Epoch 730, Loss: 0.7221097499132156, Final Batch Loss: 0.3406325876712799\n",
      "Subject 17, Epoch 731, Loss: 0.688084289431572, Final Batch Loss: 0.20885558426380157\n",
      "Subject 17, Epoch 732, Loss: 0.5550228953361511, Final Batch Loss: 0.06571266055107117\n",
      "Subject 17, Epoch 733, Loss: 0.574117872864008, Final Batch Loss: 0.04308268800377846\n",
      "Subject 17, Epoch 734, Loss: 0.6955568790435791, Final Batch Loss: 0.1639961153268814\n",
      "Subject 17, Epoch 735, Loss: 0.5882253050804138, Final Batch Loss: 0.14386536180973053\n",
      "Subject 17, Epoch 736, Loss: 0.685317225754261, Final Batch Loss: 0.2629105746746063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 737, Loss: 0.8664725795388222, Final Batch Loss: 0.26800277829170227\n",
      "Subject 17, Epoch 738, Loss: 0.6415314227342606, Final Batch Loss: 0.0867496132850647\n",
      "Subject 17, Epoch 739, Loss: 0.9358583390712738, Final Batch Loss: 0.2861866056919098\n",
      "Subject 17, Epoch 740, Loss: 0.750893846154213, Final Batch Loss: 0.24061745405197144\n",
      "Subject 17, Epoch 741, Loss: 0.8142244890332222, Final Batch Loss: 0.33939772844314575\n",
      "Subject 17, Epoch 742, Loss: 0.7904663681983948, Final Batch Loss: 0.26110535860061646\n",
      "Subject 17, Epoch 743, Loss: 0.5874693766236305, Final Batch Loss: 0.179756760597229\n",
      "Subject 17, Epoch 744, Loss: 0.6012862324714661, Final Batch Loss: 0.21200454235076904\n",
      "Subject 17, Epoch 745, Loss: 0.6322181969881058, Final Batch Loss: 0.17645063996315002\n",
      "Subject 17, Epoch 746, Loss: 0.6017849966883659, Final Batch Loss: 0.09701742976903915\n",
      "Subject 17, Epoch 747, Loss: 0.6354806944727898, Final Batch Loss: 0.12371043115854263\n",
      "Subject 17, Epoch 748, Loss: 0.5175698101520538, Final Batch Loss: 0.10596807301044464\n",
      "Subject 17, Epoch 749, Loss: 0.5847040042281151, Final Batch Loss: 0.14952999353408813\n",
      "Subject 17, Epoch 750, Loss: 0.6220951527357101, Final Batch Loss: 0.16603422164916992\n",
      "Subject 17, Epoch 751, Loss: 0.6331244483590126, Final Batch Loss: 0.21145839989185333\n",
      "Subject 17, Epoch 752, Loss: 0.7258709818124771, Final Batch Loss: 0.3202625513076782\n",
      "Subject 17, Epoch 753, Loss: 0.68037548661232, Final Batch Loss: 0.2111031413078308\n",
      "Subject 17, Epoch 754, Loss: 0.436796635389328, Final Batch Loss: 0.057836972177028656\n",
      "Subject 17, Epoch 755, Loss: 0.53816107660532, Final Batch Loss: 0.06054484099149704\n",
      "Subject 17, Epoch 756, Loss: 0.6677880510687828, Final Batch Loss: 0.2693594992160797\n",
      "Subject 17, Epoch 757, Loss: 0.5630791783332825, Final Batch Loss: 0.06083281338214874\n",
      "Subject 17, Epoch 758, Loss: 0.46020256355404854, Final Batch Loss: 0.046551693230867386\n",
      "Subject 17, Epoch 759, Loss: 0.5033803060650826, Final Batch Loss: 0.14088715612888336\n",
      "Subject 17, Epoch 760, Loss: 0.43589854054152966, Final Batch Loss: 0.02019619010388851\n",
      "Subject 17, Epoch 761, Loss: 0.4956803359091282, Final Batch Loss: 0.033093322068452835\n",
      "Subject 17, Epoch 762, Loss: 0.6792630478739738, Final Batch Loss: 0.30060505867004395\n",
      "Subject 17, Epoch 763, Loss: 0.481121763586998, Final Batch Loss: 0.09545260667800903\n",
      "Subject 17, Epoch 764, Loss: 0.48121978156268597, Final Batch Loss: 0.026747239753603935\n",
      "Subject 17, Epoch 765, Loss: 0.5318651348352432, Final Batch Loss: 0.15725886821746826\n",
      "Subject 17, Epoch 766, Loss: 0.6104088574647903, Final Batch Loss: 0.23020829260349274\n",
      "Subject 17, Epoch 767, Loss: 0.8521658331155777, Final Batch Loss: 0.44528117775917053\n",
      "Subject 17, Epoch 768, Loss: 0.5475766658782959, Final Batch Loss: 0.10469246655702591\n",
      "Subject 17, Epoch 769, Loss: 0.44680195301771164, Final Batch Loss: 0.040955811738967896\n",
      "Subject 17, Epoch 770, Loss: 0.6035935580730438, Final Batch Loss: 0.17392151057720184\n",
      "Subject 17, Epoch 771, Loss: 0.89657311886549, Final Batch Loss: 0.5480830669403076\n",
      "Subject 17, Epoch 772, Loss: 0.4587503559887409, Final Batch Loss: 0.05679469183087349\n",
      "Subject 17, Epoch 773, Loss: 0.5360220521688461, Final Batch Loss: 0.14123345911502838\n",
      "Subject 17, Epoch 774, Loss: 0.43996728025376797, Final Batch Loss: 0.028003579005599022\n",
      "Subject 17, Epoch 775, Loss: 0.5078234747052193, Final Batch Loss: 0.13520537316799164\n",
      "Subject 17, Epoch 776, Loss: 0.45637674257159233, Final Batch Loss: 0.04882750287652016\n",
      "Subject 17, Epoch 777, Loss: 0.4733441695570946, Final Batch Loss: 0.09798512607812881\n",
      "Subject 17, Epoch 778, Loss: 0.4905066713690758, Final Batch Loss: 0.06593620032072067\n",
      "Subject 17, Epoch 779, Loss: 0.5277959629893303, Final Batch Loss: 0.11882009357213974\n",
      "Subject 17, Epoch 780, Loss: 0.48659998178482056, Final Batch Loss: 0.053166911005973816\n",
      "Subject 17, Epoch 781, Loss: 0.5644744113087654, Final Batch Loss: 0.11231610924005508\n",
      "Subject 17, Epoch 782, Loss: 0.4379999302327633, Final Batch Loss: 0.052732523530721664\n",
      "Subject 17, Epoch 783, Loss: 0.42140065878629684, Final Batch Loss: 0.02095858007669449\n",
      "Subject 17, Epoch 784, Loss: 0.6645924150943756, Final Batch Loss: 0.3556126654148102\n",
      "Subject 17, Epoch 785, Loss: 0.4202449843287468, Final Batch Loss: 0.044601649045944214\n",
      "Subject 17, Epoch 786, Loss: 0.4307752624154091, Final Batch Loss: 0.12667721509933472\n",
      "Subject 17, Epoch 787, Loss: 0.6194859817624092, Final Batch Loss: 0.25176137685775757\n",
      "Subject 17, Epoch 788, Loss: 0.4513300731778145, Final Batch Loss: 0.03873881697654724\n",
      "Subject 17, Epoch 789, Loss: 0.4710896834731102, Final Batch Loss: 0.099080890417099\n",
      "Subject 17, Epoch 790, Loss: 0.6039166823029518, Final Batch Loss: 0.16892988979816437\n",
      "Subject 17, Epoch 791, Loss: 0.411092109978199, Final Batch Loss: 0.1124681681394577\n",
      "Subject 17, Epoch 792, Loss: 0.4777168110013008, Final Batch Loss: 0.13291975855827332\n",
      "Subject 17, Epoch 793, Loss: 0.4055241718888283, Final Batch Loss: 0.018036343157291412\n",
      "Subject 17, Epoch 794, Loss: 0.44416755996644497, Final Batch Loss: 0.023766284808516502\n",
      "Subject 17, Epoch 795, Loss: 0.4358171159401536, Final Batch Loss: 0.015599296428263187\n",
      "Subject 17, Epoch 796, Loss: 0.5104869157075882, Final Batch Loss: 0.09030281752347946\n",
      "Subject 17, Epoch 797, Loss: 1.0161275267601013, Final Batch Loss: 0.6688060164451599\n",
      "Subject 17, Epoch 798, Loss: 0.4357548337429762, Final Batch Loss: 0.026615334674715996\n",
      "Subject 17, Epoch 799, Loss: 0.5535338073968887, Final Batch Loss: 0.20630808174610138\n",
      "Subject 17, Epoch 800, Loss: 0.6047392264008522, Final Batch Loss: 0.22119612991809845\n",
      "Subject 17, Epoch 801, Loss: 0.47730716317892075, Final Batch Loss: 0.054947249591350555\n",
      "Subject 17, Epoch 802, Loss: 0.5334095731377602, Final Batch Loss: 0.10811077803373337\n",
      "Subject 17, Epoch 803, Loss: 0.35425301641225815, Final Batch Loss: 0.029764734208583832\n",
      "Subject 17, Epoch 804, Loss: 0.4974764883518219, Final Batch Loss: 0.12871068716049194\n",
      "Subject 17, Epoch 805, Loss: 0.5067732110619545, Final Batch Loss: 0.09472928196191788\n",
      "Subject 17, Epoch 806, Loss: 0.39030805230140686, Final Batch Loss: 0.07654479891061783\n",
      "Subject 17, Epoch 807, Loss: 0.4164924919605255, Final Batch Loss: 0.03188219666481018\n",
      "Subject 17, Epoch 808, Loss: 0.43307123333215714, Final Batch Loss: 0.09874715656042099\n",
      "Subject 17, Epoch 809, Loss: 0.4091189056634903, Final Batch Loss: 0.04158800095319748\n",
      "Subject 17, Epoch 810, Loss: 0.4694293923676014, Final Batch Loss: 0.027301210910081863\n",
      "Subject 17, Epoch 811, Loss: 0.46020107716321945, Final Batch Loss: 0.051879771053791046\n",
      "Subject 17, Epoch 812, Loss: 0.7589889466762543, Final Batch Loss: 0.46549010276794434\n",
      "Subject 17, Epoch 813, Loss: 0.408076748251915, Final Batch Loss: 0.03684345632791519\n",
      "Subject 17, Epoch 814, Loss: 0.6972944661974907, Final Batch Loss: 0.31520071625709534\n",
      "Subject 17, Epoch 815, Loss: 0.49721747636795044, Final Batch Loss: 0.15318609774112701\n",
      "Subject 17, Epoch 816, Loss: 0.4558241628110409, Final Batch Loss: 0.05259796604514122\n",
      "Subject 17, Epoch 817, Loss: 0.4790383465588093, Final Batch Loss: 0.06168368086218834\n",
      "Subject 17, Epoch 818, Loss: 0.4361133985221386, Final Batch Loss: 0.0567769892513752\n",
      "Subject 17, Epoch 819, Loss: 0.5680018216371536, Final Batch Loss: 0.310881108045578\n",
      "Subject 17, Epoch 820, Loss: 0.4518710821866989, Final Batch Loss: 0.1202382817864418\n",
      "Subject 17, Epoch 821, Loss: 0.8241094276309013, Final Batch Loss: 0.42983266711235046\n",
      "Subject 17, Epoch 822, Loss: 0.4604121297597885, Final Batch Loss: 0.10501625388860703\n",
      "Subject 17, Epoch 823, Loss: 0.41854702681303024, Final Batch Loss: 0.06509928405284882\n",
      "Subject 17, Epoch 824, Loss: 0.5248665362596512, Final Batch Loss: 0.12995631992816925\n",
      "Subject 17, Epoch 825, Loss: 0.6004985496401787, Final Batch Loss: 0.2574441730976105\n",
      "Subject 17, Epoch 826, Loss: 0.4287918508052826, Final Batch Loss: 0.031152449548244476\n",
      "Subject 17, Epoch 827, Loss: 0.3802896998822689, Final Batch Loss: 0.0425574965775013\n",
      "Subject 17, Epoch 828, Loss: 0.5710183940827847, Final Batch Loss: 0.036970432847738266\n",
      "Subject 17, Epoch 829, Loss: 0.40549230948090553, Final Batch Loss: 0.047462280839681625\n",
      "Subject 17, Epoch 830, Loss: 0.4543372318148613, Final Batch Loss: 0.15822602808475494\n",
      "Subject 17, Epoch 831, Loss: 0.514294259250164, Final Batch Loss: 0.13657593727111816\n",
      "Subject 17, Epoch 832, Loss: 0.4356957748532295, Final Batch Loss: 0.1300143599510193\n",
      "Subject 17, Epoch 833, Loss: 0.33440243266522884, Final Batch Loss: 0.018466724082827568\n",
      "Subject 17, Epoch 834, Loss: 0.5546287074685097, Final Batch Loss: 0.16391006112098694\n",
      "Subject 17, Epoch 835, Loss: 0.5345907807350159, Final Batch Loss: 0.09457037597894669\n",
      "Subject 17, Epoch 836, Loss: 0.53758554905653, Final Batch Loss: 0.163004070520401\n",
      "Subject 17, Epoch 837, Loss: 0.542836993932724, Final Batch Loss: 0.1869061142206192\n",
      "Subject 17, Epoch 838, Loss: 0.47760748863220215, Final Batch Loss: 0.15355996787548065\n",
      "Subject 17, Epoch 839, Loss: 0.7133887708187103, Final Batch Loss: 0.20029670000076294\n",
      "Subject 17, Epoch 840, Loss: 0.41831629537045956, Final Batch Loss: 0.024601733312010765\n",
      "Subject 17, Epoch 841, Loss: 0.4347521588206291, Final Batch Loss: 0.03134744614362717\n",
      "Subject 17, Epoch 842, Loss: 0.709189347922802, Final Batch Loss: 0.11314498633146286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 843, Loss: 0.5416376814246178, Final Batch Loss: 0.14019201695919037\n",
      "Subject 17, Epoch 844, Loss: 0.4346790052950382, Final Batch Loss: 0.04346661642193794\n",
      "Subject 17, Epoch 845, Loss: 0.6523014456033707, Final Batch Loss: 0.17752039432525635\n",
      "Subject 17, Epoch 846, Loss: 0.572815515100956, Final Batch Loss: 0.16619066894054413\n",
      "Subject 17, Epoch 847, Loss: 0.38857290893793106, Final Batch Loss: 0.06651977449655533\n",
      "Subject 17, Epoch 848, Loss: 0.4463230222463608, Final Batch Loss: 0.04571627825498581\n",
      "Subject 17, Epoch 849, Loss: 0.44498705863952637, Final Batch Loss: 0.06930097192525864\n",
      "Subject 17, Epoch 850, Loss: 0.3833981081843376, Final Batch Loss: 0.021629653871059418\n",
      "Subject 17, Epoch 851, Loss: 0.38702579587697983, Final Batch Loss: 0.13484546542167664\n",
      "Subject 17, Epoch 852, Loss: 0.5016699582338333, Final Batch Loss: 0.24121233820915222\n",
      "Subject 17, Epoch 853, Loss: 0.3725694492459297, Final Batch Loss: 0.0439172089099884\n",
      "Subject 17, Epoch 854, Loss: 0.3607345689088106, Final Batch Loss: 0.023881109431385994\n",
      "Subject 17, Epoch 855, Loss: 0.43154843896627426, Final Batch Loss: 0.08944277465343475\n",
      "Subject 17, Epoch 856, Loss: 0.46069665625691414, Final Batch Loss: 0.0519154854118824\n",
      "Subject 17, Epoch 857, Loss: 0.4142901189625263, Final Batch Loss: 0.05222469940781593\n",
      "Subject 17, Epoch 858, Loss: 0.5048623979091644, Final Batch Loss: 0.08566071093082428\n",
      "Subject 17, Epoch 859, Loss: 0.5444476678967476, Final Batch Loss: 0.17723041772842407\n",
      "Subject 17, Epoch 860, Loss: 0.4110807850956917, Final Batch Loss: 0.049814373254776\n",
      "Subject 17, Epoch 861, Loss: 0.35180047526955605, Final Batch Loss: 0.06710569560527802\n",
      "Subject 17, Epoch 862, Loss: 0.3326888158917427, Final Batch Loss: 0.07762551307678223\n",
      "Subject 17, Epoch 863, Loss: 0.7745691016316414, Final Batch Loss: 0.5056074857711792\n",
      "Subject 17, Epoch 864, Loss: 0.4691309481859207, Final Batch Loss: 0.016755104064941406\n",
      "Subject 17, Epoch 865, Loss: 0.5193637832999229, Final Batch Loss: 0.11135906726121902\n",
      "Subject 17, Epoch 866, Loss: 0.38637883216142654, Final Batch Loss: 0.040834590792655945\n",
      "Subject 17, Epoch 867, Loss: 0.42927567288279533, Final Batch Loss: 0.04197246953845024\n",
      "Subject 17, Epoch 868, Loss: 0.6062486469745636, Final Batch Loss: 0.2501842677593231\n",
      "Subject 17, Epoch 869, Loss: 0.39405278861522675, Final Batch Loss: 0.06838608533143997\n",
      "Subject 17, Epoch 870, Loss: 0.4827803149819374, Final Batch Loss: 0.11251992732286453\n",
      "Subject 17, Epoch 871, Loss: 0.49234260991215706, Final Batch Loss: 0.015318233519792557\n",
      "Subject 17, Epoch 872, Loss: 0.4819495975971222, Final Batch Loss: 0.19101840257644653\n",
      "Subject 17, Epoch 873, Loss: 0.43244124948978424, Final Batch Loss: 0.09865061938762665\n",
      "Subject 17, Epoch 874, Loss: 0.4590553231537342, Final Batch Loss: 0.12667721509933472\n",
      "Subject 17, Epoch 875, Loss: 0.31791758351027966, Final Batch Loss: 0.024445073679089546\n",
      "Subject 17, Epoch 876, Loss: 0.4015791043639183, Final Batch Loss: 0.1145389974117279\n",
      "Subject 17, Epoch 877, Loss: 0.38745077326893806, Final Batch Loss: 0.0381128154695034\n",
      "Subject 17, Epoch 878, Loss: 0.39591517113149166, Final Batch Loss: 0.02729983441531658\n",
      "Subject 17, Epoch 879, Loss: 0.42039255052804947, Final Batch Loss: 0.11403355002403259\n",
      "Subject 17, Epoch 880, Loss: 0.32058316469192505, Final Batch Loss: 0.04424154758453369\n",
      "Subject 17, Epoch 881, Loss: 0.4624246172606945, Final Batch Loss: 0.17130829393863678\n",
      "Subject 17, Epoch 882, Loss: 0.3541357386857271, Final Batch Loss: 0.028575172647833824\n",
      "Subject 17, Epoch 883, Loss: 0.5644218549132347, Final Batch Loss: 0.27034586668014526\n",
      "Subject 17, Epoch 884, Loss: 0.5437284409999847, Final Batch Loss: 0.10474728792905807\n",
      "Subject 17, Epoch 885, Loss: 0.3940578065812588, Final Batch Loss: 0.08009598404169083\n",
      "Subject 17, Epoch 886, Loss: 0.27423397451639175, Final Batch Loss: 0.03357665613293648\n",
      "Subject 17, Epoch 887, Loss: 0.4085690379142761, Final Batch Loss: 0.11144240200519562\n",
      "Subject 17, Epoch 888, Loss: 0.40106235072016716, Final Batch Loss: 0.02671026811003685\n",
      "Subject 17, Epoch 889, Loss: 0.3987314701080322, Final Batch Loss: 0.1403653472661972\n",
      "Subject 17, Epoch 890, Loss: 0.5564832426607609, Final Batch Loss: 0.2862021028995514\n",
      "Subject 17, Epoch 891, Loss: 0.6810647211968899, Final Batch Loss: 0.42307958006858826\n",
      "Subject 17, Epoch 892, Loss: 0.5584178268909454, Final Batch Loss: 0.2749628722667694\n",
      "Subject 17, Epoch 893, Loss: 0.3955710828304291, Final Batch Loss: 0.018074549734592438\n",
      "Subject 17, Epoch 894, Loss: 0.426902137696743, Final Batch Loss: 0.08767522871494293\n",
      "Subject 17, Epoch 895, Loss: 0.4145691469311714, Final Batch Loss: 0.12149406969547272\n",
      "Subject 17, Epoch 896, Loss: 0.31186118721961975, Final Batch Loss: 0.03570636361837387\n",
      "Subject 17, Epoch 897, Loss: 0.3739156723022461, Final Batch Loss: 0.049635954201221466\n",
      "Subject 17, Epoch 898, Loss: 0.299177972599864, Final Batch Loss: 0.0227926354855299\n",
      "Subject 17, Epoch 899, Loss: 0.45136595144867897, Final Batch Loss: 0.21003715693950653\n",
      "Subject 17, Epoch 900, Loss: 0.5520216375589371, Final Batch Loss: 0.20678400993347168\n",
      "Subject 17, Epoch 901, Loss: 0.49521879106760025, Final Batch Loss: 0.133448526263237\n",
      "Subject 17, Epoch 902, Loss: 0.3455567732453346, Final Batch Loss: 0.06982830166816711\n",
      "Subject 17, Epoch 903, Loss: 0.3910275176167488, Final Batch Loss: 0.06946498900651932\n",
      "Subject 17, Epoch 904, Loss: 0.36162729375064373, Final Batch Loss: 0.02578071318566799\n",
      "Subject 17, Epoch 905, Loss: 0.4382433071732521, Final Batch Loss: 0.15644705295562744\n",
      "Subject 17, Epoch 906, Loss: 0.4782940149307251, Final Batch Loss: 0.15777722001075745\n",
      "Subject 17, Epoch 907, Loss: 0.38491522520780563, Final Batch Loss: 0.11521108448505402\n",
      "Subject 17, Epoch 908, Loss: 0.3739214316010475, Final Batch Loss: 0.032799288630485535\n",
      "Subject 17, Epoch 909, Loss: 0.3804036844521761, Final Batch Loss: 0.016734065487980843\n",
      "Subject 17, Epoch 910, Loss: 0.46556447446346283, Final Batch Loss: 0.22213315963745117\n",
      "Subject 17, Epoch 911, Loss: 0.4102199897170067, Final Batch Loss: 0.11713018268346786\n",
      "Subject 17, Epoch 912, Loss: 0.3995051942765713, Final Batch Loss: 0.009776037186384201\n",
      "Subject 17, Epoch 913, Loss: 0.4526861868798733, Final Batch Loss: 0.034821126610040665\n",
      "Subject 17, Epoch 914, Loss: 0.3553079329431057, Final Batch Loss: 0.046963904052972794\n",
      "Subject 17, Epoch 915, Loss: 0.3907996714115143, Final Batch Loss: 0.11622732877731323\n",
      "Subject 17, Epoch 916, Loss: 0.44020769000053406, Final Batch Loss: 0.1361844837665558\n",
      "Subject 17, Epoch 917, Loss: 0.35393456742167473, Final Batch Loss: 0.07292913645505905\n",
      "Subject 17, Epoch 918, Loss: 0.48668573424220085, Final Batch Loss: 0.22868171334266663\n",
      "Subject 17, Epoch 919, Loss: 0.3619975820183754, Final Batch Loss: 0.08089806884527206\n",
      "Subject 17, Epoch 920, Loss: 0.42135409638285637, Final Batch Loss: 0.04690460115671158\n",
      "Subject 17, Epoch 921, Loss: 0.4440556839108467, Final Batch Loss: 0.14138415455818176\n",
      "Subject 17, Epoch 922, Loss: 0.42742791026830673, Final Batch Loss: 0.18536362051963806\n",
      "Subject 17, Epoch 923, Loss: 0.464499793946743, Final Batch Loss: 0.1147800013422966\n",
      "Subject 17, Epoch 924, Loss: 0.33843354880809784, Final Batch Loss: 0.07133476436138153\n",
      "Subject 17, Epoch 925, Loss: 0.3504810221493244, Final Batch Loss: 0.04270792379975319\n",
      "Subject 17, Epoch 926, Loss: 0.4561829548329115, Final Batch Loss: 0.0237420741468668\n",
      "Subject 17, Epoch 927, Loss: 0.4109761845320463, Final Batch Loss: 0.029956651851534843\n",
      "Subject 17, Epoch 928, Loss: 0.3300383538007736, Final Batch Loss: 0.11871945112943649\n",
      "Subject 17, Epoch 929, Loss: 0.40072182193398476, Final Batch Loss: 0.057106245309114456\n",
      "Subject 17, Epoch 930, Loss: 0.33280010893940926, Final Batch Loss: 0.04071417078375816\n",
      "Subject 17, Epoch 931, Loss: 0.38478489220142365, Final Batch Loss: 0.14254130423069\n",
      "Subject 17, Epoch 932, Loss: 0.37822335585951805, Final Batch Loss: 0.08645284920930862\n",
      "Subject 17, Epoch 933, Loss: 0.3253353014588356, Final Batch Loss: 0.018252268433570862\n",
      "Subject 17, Epoch 934, Loss: 0.5502838268876076, Final Batch Loss: 0.29607969522476196\n",
      "Subject 17, Epoch 935, Loss: 0.471965491771698, Final Batch Loss: 0.10971689224243164\n",
      "Subject 17, Epoch 936, Loss: 0.35070663318037987, Final Batch Loss: 0.06123528257012367\n",
      "Subject 17, Epoch 937, Loss: 0.3350960649549961, Final Batch Loss: 0.04763409122824669\n",
      "Subject 17, Epoch 938, Loss: 0.31977322325110435, Final Batch Loss: 0.022911887615919113\n",
      "Subject 17, Epoch 939, Loss: 0.31126566231250763, Final Batch Loss: 0.08889041095972061\n",
      "Subject 17, Epoch 940, Loss: 0.46897732466459274, Final Batch Loss: 0.23941117525100708\n",
      "Subject 17, Epoch 941, Loss: 0.44958239048719406, Final Batch Loss: 0.11027967184782028\n",
      "Subject 17, Epoch 942, Loss: 0.3407691456377506, Final Batch Loss: 0.020373500883579254\n",
      "Subject 17, Epoch 943, Loss: 0.47084444761276245, Final Batch Loss: 0.18973584473133087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17, Epoch 944, Loss: 0.5208976715803146, Final Batch Loss: 0.25956928730010986\n",
      "Subject 17, Epoch 945, Loss: 0.3085000589489937, Final Batch Loss: 0.07228338718414307\n",
      "Subject 17, Epoch 946, Loss: 0.4073239080607891, Final Batch Loss: 0.03345334902405739\n",
      "Subject 17, Epoch 947, Loss: 0.4651191830635071, Final Batch Loss: 0.1841794103384018\n",
      "Subject 17, Epoch 948, Loss: 0.33260555006563663, Final Batch Loss: 0.025334926322102547\n",
      "Subject 17, Epoch 949, Loss: 0.4643746353685856, Final Batch Loss: 0.13058166205883026\n",
      "Subject 17, Epoch 950, Loss: 0.43512221798300743, Final Batch Loss: 0.013457637280225754\n",
      "Subject 17, Epoch 951, Loss: 0.458345890045166, Final Batch Loss: 0.10691235959529877\n",
      "Subject 17, Epoch 952, Loss: 0.3853660486638546, Final Batch Loss: 0.10089249163866043\n",
      "Subject 17, Epoch 953, Loss: 0.5054319426417351, Final Batch Loss: 0.14091645181179047\n",
      "Subject 17, Epoch 954, Loss: 0.42113518714904785, Final Batch Loss: 0.1225392073392868\n",
      "Subject 17, Epoch 955, Loss: 0.4234333299100399, Final Batch Loss: 0.16486608982086182\n",
      "Subject 17, Epoch 956, Loss: 0.5881678909063339, Final Batch Loss: 0.3900412619113922\n",
      "Subject 17, Epoch 957, Loss: 0.3827475979924202, Final Batch Loss: 0.04602385312318802\n",
      "Subject 17, Epoch 958, Loss: 0.3434372767806053, Final Batch Loss: 0.06659170985221863\n",
      "Subject 17, Epoch 959, Loss: 0.39539987221360207, Final Batch Loss: 0.05003955587744713\n",
      "Subject 17, Epoch 960, Loss: 0.4841068387031555, Final Batch Loss: 0.16893650591373444\n",
      "Subject 17, Epoch 961, Loss: 0.3495984338223934, Final Batch Loss: 0.05355105176568031\n",
      "Subject 17, Epoch 962, Loss: 0.5235364735126495, Final Batch Loss: 0.28056323528289795\n",
      "Subject 17, Epoch 963, Loss: 0.40039547346532345, Final Batch Loss: 0.030538057908415794\n",
      "Subject 17, Epoch 964, Loss: 0.7590151131153107, Final Batch Loss: 0.2850317656993866\n",
      "Subject 17, Epoch 965, Loss: 0.5754876434803009, Final Batch Loss: 0.1932697743177414\n",
      "Subject 17, Epoch 966, Loss: 0.46646712720394135, Final Batch Loss: 0.0853303000330925\n",
      "Subject 17, Epoch 967, Loss: 0.4809007756412029, Final Batch Loss: 0.03561122342944145\n",
      "Subject 17, Epoch 968, Loss: 0.5199370346963406, Final Batch Loss: 0.0610695444047451\n",
      "Subject 17, Epoch 969, Loss: 0.4425009489059448, Final Batch Loss: 0.08595737814903259\n",
      "Subject 17, Epoch 970, Loss: 0.4960091635584831, Final Batch Loss: 0.176766037940979\n",
      "Subject 17, Epoch 971, Loss: 0.3044165298342705, Final Batch Loss: 0.03420237451791763\n",
      "Subject 17, Epoch 972, Loss: 0.32325873523950577, Final Batch Loss: 0.06336700171232224\n",
      "Subject 17, Epoch 973, Loss: 0.3731084819883108, Final Batch Loss: 0.014481483027338982\n",
      "Subject 17, Epoch 974, Loss: 0.3736310936510563, Final Batch Loss: 0.05462289974093437\n",
      "Subject 17, Epoch 975, Loss: 0.4502740725874901, Final Batch Loss: 0.17871522903442383\n",
      "Subject 17, Epoch 976, Loss: 0.5076955929398537, Final Batch Loss: 0.1396029144525528\n",
      "Subject 17, Epoch 977, Loss: 0.25889506936073303, Final Batch Loss: 0.03314037248492241\n",
      "Subject 17, Epoch 978, Loss: 0.2605317812412977, Final Batch Loss: 0.020568685606122017\n",
      "Subject 17, Epoch 979, Loss: 0.44080837070941925, Final Batch Loss: 0.27617359161376953\n",
      "Subject 17, Epoch 980, Loss: 0.281468041241169, Final Batch Loss: 0.04880449175834656\n",
      "Subject 17, Epoch 981, Loss: 0.6002257764339447, Final Batch Loss: 0.21725629270076752\n",
      "Subject 17, Epoch 982, Loss: 0.7144093904644251, Final Batch Loss: 0.46796664595603943\n",
      "Subject 17, Epoch 983, Loss: 0.4344486743211746, Final Batch Loss: 0.1529701203107834\n",
      "Subject 17, Epoch 984, Loss: 0.3321681171655655, Final Batch Loss: 0.05006787180900574\n",
      "Subject 17, Epoch 985, Loss: 0.38461438566446304, Final Batch Loss: 0.06846033781766891\n",
      "Subject 17, Epoch 986, Loss: 0.31898853182792664, Final Batch Loss: 0.10568195581436157\n",
      "Subject 17, Epoch 987, Loss: 0.4421380013227463, Final Batch Loss: 0.08760584890842438\n",
      "Subject 17, Epoch 988, Loss: 0.386464461684227, Final Batch Loss: 0.09351462870836258\n",
      "Subject 17, Epoch 989, Loss: 0.3708321452140808, Final Batch Loss: 0.12236171215772629\n",
      "Subject 17, Epoch 990, Loss: 0.3306995891034603, Final Batch Loss: 0.0392269529402256\n",
      "Subject 17, Epoch 991, Loss: 0.32019313983619213, Final Batch Loss: 0.01689049042761326\n",
      "Subject 17, Epoch 992, Loss: 0.31029197573661804, Final Batch Loss: 0.07497027516365051\n",
      "Subject 17, Epoch 993, Loss: 0.3075568899512291, Final Batch Loss: 0.04126402735710144\n",
      "Subject 17, Epoch 994, Loss: 0.31117761693894863, Final Batch Loss: 0.028889821842312813\n",
      "Subject 17, Epoch 995, Loss: 0.35385870933532715, Final Batch Loss: 0.09101702272891998\n",
      "Subject 17, Epoch 996, Loss: 0.32833746541291475, Final Batch Loss: 0.012823318131268024\n",
      "Subject 17, Epoch 997, Loss: 0.24327557999640703, Final Batch Loss: 0.006468978710472584\n",
      "Subject 17, Epoch 998, Loss: 0.3302469253540039, Final Batch Loss: 0.0555226095020771\n",
      "Subject 17, Epoch 999, Loss: 0.345006987452507, Final Batch Loss: 0.08058793842792511\n",
      "Subject 17, Epoch 1000, Loss: 0.4169368129223585, Final Batch Loss: 0.028724735602736473\n",
      "Subject 18, Epoch 1, Loss: 7.190523862838745, Final Batch Loss: 1.8013747930526733\n",
      "Subject 18, Epoch 2, Loss: 7.183884024620056, Final Batch Loss: 1.8034979104995728\n",
      "Subject 18, Epoch 3, Loss: 7.184791684150696, Final Batch Loss: 1.8147811889648438\n",
      "Subject 18, Epoch 4, Loss: 7.125938534736633, Final Batch Loss: 1.7461785078048706\n",
      "Subject 18, Epoch 5, Loss: 7.147806882858276, Final Batch Loss: 1.7976223230361938\n",
      "Subject 18, Epoch 6, Loss: 7.148581266403198, Final Batch Loss: 1.8142379522323608\n",
      "Subject 18, Epoch 7, Loss: 7.091537714004517, Final Batch Loss: 1.7664366960525513\n",
      "Subject 18, Epoch 8, Loss: 7.099482417106628, Final Batch Loss: 1.8117181062698364\n",
      "Subject 18, Epoch 9, Loss: 7.068362474441528, Final Batch Loss: 1.7957967519760132\n",
      "Subject 18, Epoch 10, Loss: 6.985339403152466, Final Batch Loss: 1.7617472410202026\n",
      "Subject 18, Epoch 11, Loss: 6.847336769104004, Final Batch Loss: 1.627791404724121\n",
      "Subject 18, Epoch 12, Loss: 6.807220935821533, Final Batch Loss: 1.6631401777267456\n",
      "Subject 18, Epoch 13, Loss: 6.677579879760742, Final Batch Loss: 1.639760136604309\n",
      "Subject 18, Epoch 14, Loss: 6.63589346408844, Final Batch Loss: 1.6298246383666992\n",
      "Subject 18, Epoch 15, Loss: 6.585991144180298, Final Batch Loss: 1.7071291208267212\n",
      "Subject 18, Epoch 16, Loss: 6.384875416755676, Final Batch Loss: 1.5843491554260254\n",
      "Subject 18, Epoch 17, Loss: 6.24127733707428, Final Batch Loss: 1.521049976348877\n",
      "Subject 18, Epoch 18, Loss: 6.287140727043152, Final Batch Loss: 1.644883632659912\n",
      "Subject 18, Epoch 19, Loss: 6.04339861869812, Final Batch Loss: 1.4480422735214233\n",
      "Subject 18, Epoch 20, Loss: 6.022339105606079, Final Batch Loss: 1.4854998588562012\n",
      "Subject 18, Epoch 21, Loss: 5.899475336074829, Final Batch Loss: 1.507792353630066\n",
      "Subject 18, Epoch 22, Loss: 5.750733017921448, Final Batch Loss: 1.4295645952224731\n",
      "Subject 18, Epoch 23, Loss: 5.688528060913086, Final Batch Loss: 1.4337362051010132\n",
      "Subject 18, Epoch 24, Loss: 5.436233997344971, Final Batch Loss: 1.2920407056808472\n",
      "Subject 18, Epoch 25, Loss: 5.3129870891571045, Final Batch Loss: 1.2947145700454712\n",
      "Subject 18, Epoch 26, Loss: 5.278439521789551, Final Batch Loss: 1.3233097791671753\n",
      "Subject 18, Epoch 27, Loss: 5.013169050216675, Final Batch Loss: 1.2447012662887573\n",
      "Subject 18, Epoch 28, Loss: 5.1358267068862915, Final Batch Loss: 1.4093698263168335\n",
      "Subject 18, Epoch 29, Loss: 4.934829115867615, Final Batch Loss: 1.2310081720352173\n",
      "Subject 18, Epoch 30, Loss: 4.7175304889678955, Final Batch Loss: 1.1126525402069092\n",
      "Subject 18, Epoch 31, Loss: 4.718049764633179, Final Batch Loss: 1.133348822593689\n",
      "Subject 18, Epoch 32, Loss: 4.583770513534546, Final Batch Loss: 1.091391682624817\n",
      "Subject 18, Epoch 33, Loss: 4.71463406085968, Final Batch Loss: 1.150044560432434\n",
      "Subject 18, Epoch 34, Loss: 4.509795665740967, Final Batch Loss: 1.0710362195968628\n",
      "Subject 18, Epoch 35, Loss: 4.552226781845093, Final Batch Loss: 1.1217478513717651\n",
      "Subject 18, Epoch 36, Loss: 4.559773921966553, Final Batch Loss: 1.131744384765625\n",
      "Subject 18, Epoch 37, Loss: 4.436955571174622, Final Batch Loss: 1.0677777528762817\n",
      "Subject 18, Epoch 38, Loss: 4.501041769981384, Final Batch Loss: 1.0854662656784058\n",
      "Subject 18, Epoch 39, Loss: 4.522027373313904, Final Batch Loss: 1.1753865480422974\n",
      "Subject 18, Epoch 40, Loss: 4.4891533851623535, Final Batch Loss: 1.082391381263733\n",
      "Subject 18, Epoch 41, Loss: 4.499578595161438, Final Batch Loss: 1.0721296072006226\n",
      "Subject 18, Epoch 42, Loss: 4.47845733165741, Final Batch Loss: 1.135636806488037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 43, Loss: 4.511377811431885, Final Batch Loss: 1.1464987993240356\n",
      "Subject 18, Epoch 44, Loss: 4.473828315734863, Final Batch Loss: 1.0831307172775269\n",
      "Subject 18, Epoch 45, Loss: 4.495174527168274, Final Batch Loss: 1.1730657815933228\n",
      "Subject 18, Epoch 46, Loss: 4.500425815582275, Final Batch Loss: 1.110957145690918\n",
      "Subject 18, Epoch 47, Loss: 4.443629503250122, Final Batch Loss: 1.144176721572876\n",
      "Subject 18, Epoch 48, Loss: 4.402215361595154, Final Batch Loss: 1.0947682857513428\n",
      "Subject 18, Epoch 49, Loss: 4.375535607337952, Final Batch Loss: 1.1400731801986694\n",
      "Subject 18, Epoch 50, Loss: 4.427764058113098, Final Batch Loss: 1.1398035287857056\n",
      "Subject 18, Epoch 51, Loss: 4.385983109474182, Final Batch Loss: 1.1253565549850464\n",
      "Subject 18, Epoch 52, Loss: 4.364747762680054, Final Batch Loss: 1.1205713748931885\n",
      "Subject 18, Epoch 53, Loss: 4.271949768066406, Final Batch Loss: 1.0398812294006348\n",
      "Subject 18, Epoch 54, Loss: 4.360281348228455, Final Batch Loss: 1.1519522666931152\n",
      "Subject 18, Epoch 55, Loss: 4.253527045249939, Final Batch Loss: 1.0923892259597778\n",
      "Subject 18, Epoch 56, Loss: 4.137523829936981, Final Batch Loss: 0.9764518141746521\n",
      "Subject 18, Epoch 57, Loss: 4.161700248718262, Final Batch Loss: 1.0111864805221558\n",
      "Subject 18, Epoch 58, Loss: 4.152081727981567, Final Batch Loss: 1.0578473806381226\n",
      "Subject 18, Epoch 59, Loss: 4.206642031669617, Final Batch Loss: 1.0945991277694702\n",
      "Subject 18, Epoch 60, Loss: 4.116720616817474, Final Batch Loss: 0.9995899200439453\n",
      "Subject 18, Epoch 61, Loss: 3.962666928768158, Final Batch Loss: 0.9752710461616516\n",
      "Subject 18, Epoch 62, Loss: 3.9070785641670227, Final Batch Loss: 0.9309864044189453\n",
      "Subject 18, Epoch 63, Loss: 3.9438648223876953, Final Batch Loss: 0.9473015666007996\n",
      "Subject 18, Epoch 64, Loss: 3.7906076908111572, Final Batch Loss: 0.8146293759346008\n",
      "Subject 18, Epoch 65, Loss: 3.93884938955307, Final Batch Loss: 1.0502463579177856\n",
      "Subject 18, Epoch 66, Loss: 3.859097719192505, Final Batch Loss: 1.0346405506134033\n",
      "Subject 18, Epoch 67, Loss: 3.814876675605774, Final Batch Loss: 0.9543635249137878\n",
      "Subject 18, Epoch 68, Loss: 3.694897711277008, Final Batch Loss: 0.9397203922271729\n",
      "Subject 18, Epoch 69, Loss: 3.4539020657539368, Final Batch Loss: 0.7120866775512695\n",
      "Subject 18, Epoch 70, Loss: 3.5691295862197876, Final Batch Loss: 0.9137029647827148\n",
      "Subject 18, Epoch 71, Loss: 3.4204150438308716, Final Batch Loss: 0.6868577003479004\n",
      "Subject 18, Epoch 72, Loss: 3.498054623603821, Final Batch Loss: 0.8178496360778809\n",
      "Subject 18, Epoch 73, Loss: 3.3485831022262573, Final Batch Loss: 0.743018627166748\n",
      "Subject 18, Epoch 74, Loss: 3.345880150794983, Final Batch Loss: 0.8096397519111633\n",
      "Subject 18, Epoch 75, Loss: 3.311406135559082, Final Batch Loss: 0.7417902946472168\n",
      "Subject 18, Epoch 76, Loss: 3.41876882314682, Final Batch Loss: 0.8871625065803528\n",
      "Subject 18, Epoch 77, Loss: 3.416822910308838, Final Batch Loss: 0.9946756958961487\n",
      "Subject 18, Epoch 78, Loss: 3.374095141887665, Final Batch Loss: 0.7968571782112122\n",
      "Subject 18, Epoch 79, Loss: 3.3408265709877014, Final Batch Loss: 0.9332295060157776\n",
      "Subject 18, Epoch 80, Loss: 3.3955376744270325, Final Batch Loss: 1.0170962810516357\n",
      "Subject 18, Epoch 81, Loss: 3.1085744500160217, Final Batch Loss: 0.7070769667625427\n",
      "Subject 18, Epoch 82, Loss: 3.1794488430023193, Final Batch Loss: 0.8656851649284363\n",
      "Subject 18, Epoch 83, Loss: 3.1037548780441284, Final Batch Loss: 0.6791461110115051\n",
      "Subject 18, Epoch 84, Loss: 3.087004840373993, Final Batch Loss: 0.7158681750297546\n",
      "Subject 18, Epoch 85, Loss: 3.0402894020080566, Final Batch Loss: 0.6173783540725708\n",
      "Subject 18, Epoch 86, Loss: 3.1303564310073853, Final Batch Loss: 0.7586870789527893\n",
      "Subject 18, Epoch 87, Loss: 3.0705341696739197, Final Batch Loss: 0.792012631893158\n",
      "Subject 18, Epoch 88, Loss: 3.0952564477920532, Final Batch Loss: 0.7868916392326355\n",
      "Subject 18, Epoch 89, Loss: 3.076354742050171, Final Batch Loss: 0.8193981051445007\n",
      "Subject 18, Epoch 90, Loss: 3.0950719118118286, Final Batch Loss: 0.815639317035675\n",
      "Subject 18, Epoch 91, Loss: 2.9520621299743652, Final Batch Loss: 0.6361168622970581\n",
      "Subject 18, Epoch 92, Loss: 2.871245563030243, Final Batch Loss: 0.6526997089385986\n",
      "Subject 18, Epoch 93, Loss: 2.896215498447418, Final Batch Loss: 0.6623354554176331\n",
      "Subject 18, Epoch 94, Loss: 3.091616451740265, Final Batch Loss: 0.9523284435272217\n",
      "Subject 18, Epoch 95, Loss: 3.004458010196686, Final Batch Loss: 0.8003851771354675\n",
      "Subject 18, Epoch 96, Loss: 2.8583988547325134, Final Batch Loss: 0.6987249255180359\n",
      "Subject 18, Epoch 97, Loss: 2.9230189323425293, Final Batch Loss: 0.727510929107666\n",
      "Subject 18, Epoch 98, Loss: 2.85480797290802, Final Batch Loss: 0.6593931317329407\n",
      "Subject 18, Epoch 99, Loss: 2.7786630988121033, Final Batch Loss: 0.6715643405914307\n",
      "Subject 18, Epoch 100, Loss: 2.6669060587882996, Final Batch Loss: 0.5230373740196228\n",
      "Subject 18, Epoch 101, Loss: 2.965444028377533, Final Batch Loss: 0.8826231956481934\n",
      "Subject 18, Epoch 102, Loss: 2.80597323179245, Final Batch Loss: 0.7302641868591309\n",
      "Subject 18, Epoch 103, Loss: 2.77121764421463, Final Batch Loss: 0.718644917011261\n",
      "Subject 18, Epoch 104, Loss: 2.889897406101227, Final Batch Loss: 0.7412230372428894\n",
      "Subject 18, Epoch 105, Loss: 2.658474087715149, Final Batch Loss: 0.6408061981201172\n",
      "Subject 18, Epoch 106, Loss: 2.824442744255066, Final Batch Loss: 0.7542307376861572\n",
      "Subject 18, Epoch 107, Loss: 2.661802887916565, Final Batch Loss: 0.759179413318634\n",
      "Subject 18, Epoch 108, Loss: 2.6446579098701477, Final Batch Loss: 0.6931071281433105\n",
      "Subject 18, Epoch 109, Loss: 2.771664798259735, Final Batch Loss: 0.6880515217781067\n",
      "Subject 18, Epoch 110, Loss: 2.65389084815979, Final Batch Loss: 0.6099529266357422\n",
      "Subject 18, Epoch 111, Loss: 2.8481377959251404, Final Batch Loss: 0.8367449641227722\n",
      "Subject 18, Epoch 112, Loss: 2.5253995060920715, Final Batch Loss: 0.5768576860427856\n",
      "Subject 18, Epoch 113, Loss: 2.874913811683655, Final Batch Loss: 0.965630829334259\n",
      "Subject 18, Epoch 114, Loss: 2.6319188475608826, Final Batch Loss: 0.785465657711029\n",
      "Subject 18, Epoch 115, Loss: 2.42437481880188, Final Batch Loss: 0.5072473287582397\n",
      "Subject 18, Epoch 116, Loss: 2.648136019706726, Final Batch Loss: 0.6360984444618225\n",
      "Subject 18, Epoch 117, Loss: 2.7649784088134766, Final Batch Loss: 0.8438930511474609\n",
      "Subject 18, Epoch 118, Loss: 2.6017138957977295, Final Batch Loss: 0.5331159234046936\n",
      "Subject 18, Epoch 119, Loss: 2.182587295770645, Final Batch Loss: 0.433498352766037\n",
      "Subject 18, Epoch 120, Loss: 2.461682617664337, Final Batch Loss: 0.5987808108329773\n",
      "Subject 18, Epoch 121, Loss: 2.453769564628601, Final Batch Loss: 0.5778445601463318\n",
      "Subject 18, Epoch 122, Loss: 2.4807605743408203, Final Batch Loss: 0.5833559036254883\n",
      "Subject 18, Epoch 123, Loss: 2.519903063774109, Final Batch Loss: 0.5635665059089661\n",
      "Subject 18, Epoch 124, Loss: 2.5140766501426697, Final Batch Loss: 0.7223019599914551\n",
      "Subject 18, Epoch 125, Loss: 2.5247005224227905, Final Batch Loss: 0.6682806611061096\n",
      "Subject 18, Epoch 126, Loss: 2.598382294178009, Final Batch Loss: 0.7015686631202698\n",
      "Subject 18, Epoch 127, Loss: 2.3681562542915344, Final Batch Loss: 0.5790608525276184\n",
      "Subject 18, Epoch 128, Loss: 2.284092366695404, Final Batch Loss: 0.3996915817260742\n",
      "Subject 18, Epoch 129, Loss: 2.584247350692749, Final Batch Loss: 0.6749700903892517\n",
      "Subject 18, Epoch 130, Loss: 2.6732455492019653, Final Batch Loss: 0.9037746787071228\n",
      "Subject 18, Epoch 131, Loss: 2.2503715455532074, Final Batch Loss: 0.47728005051612854\n",
      "Subject 18, Epoch 132, Loss: 2.260854095220566, Final Batch Loss: 0.45229604840278625\n",
      "Subject 18, Epoch 133, Loss: 2.48690003156662, Final Batch Loss: 0.7237157225608826\n",
      "Subject 18, Epoch 134, Loss: 2.3403050303459167, Final Batch Loss: 0.5880394577980042\n",
      "Subject 18, Epoch 135, Loss: 2.377845525741577, Final Batch Loss: 0.6636187434196472\n",
      "Subject 18, Epoch 136, Loss: 2.2508169412612915, Final Batch Loss: 0.5199773907661438\n",
      "Subject 18, Epoch 137, Loss: 2.3048394322395325, Final Batch Loss: 0.4769502878189087\n",
      "Subject 18, Epoch 138, Loss: 2.5724355578422546, Final Batch Loss: 0.842457115650177\n",
      "Subject 18, Epoch 139, Loss: 2.2898962199687958, Final Batch Loss: 0.4831521809101105\n",
      "Subject 18, Epoch 140, Loss: 2.248108357191086, Final Batch Loss: 0.46111008524894714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 141, Loss: 2.450082778930664, Final Batch Loss: 0.8089323043823242\n",
      "Subject 18, Epoch 142, Loss: 2.3683903217315674, Final Batch Loss: 0.6538448333740234\n",
      "Subject 18, Epoch 143, Loss: 2.1847851872444153, Final Batch Loss: 0.5120375752449036\n",
      "Subject 18, Epoch 144, Loss: 2.222024232149124, Final Batch Loss: 0.4584689438343048\n",
      "Subject 18, Epoch 145, Loss: 2.3398270308971405, Final Batch Loss: 0.6944265961647034\n",
      "Subject 18, Epoch 146, Loss: 2.1361473500728607, Final Batch Loss: 0.4465883672237396\n",
      "Subject 18, Epoch 147, Loss: 2.266183376312256, Final Batch Loss: 0.5273022055625916\n",
      "Subject 18, Epoch 148, Loss: 2.1175256967544556, Final Batch Loss: 0.5221681594848633\n",
      "Subject 18, Epoch 149, Loss: 2.2772430777549744, Final Batch Loss: 0.6596598029136658\n",
      "Subject 18, Epoch 150, Loss: 2.3043397068977356, Final Batch Loss: 0.6527692675590515\n",
      "Subject 18, Epoch 151, Loss: 2.15126633644104, Final Batch Loss: 0.5292973518371582\n",
      "Subject 18, Epoch 152, Loss: 2.1849669814109802, Final Batch Loss: 0.5041680932044983\n",
      "Subject 18, Epoch 153, Loss: 2.167117655277252, Final Batch Loss: 0.44226717948913574\n",
      "Subject 18, Epoch 154, Loss: 1.963356763124466, Final Batch Loss: 0.3845880925655365\n",
      "Subject 18, Epoch 155, Loss: 1.9563594162464142, Final Batch Loss: 0.302977055311203\n",
      "Subject 18, Epoch 156, Loss: 2.0019980669021606, Final Batch Loss: 0.5039362907409668\n",
      "Subject 18, Epoch 157, Loss: 2.0693213045597076, Final Batch Loss: 0.4599502980709076\n",
      "Subject 18, Epoch 158, Loss: 2.129038095474243, Final Batch Loss: 0.5304570198059082\n",
      "Subject 18, Epoch 159, Loss: 2.190775513648987, Final Batch Loss: 0.5069715976715088\n",
      "Subject 18, Epoch 160, Loss: 2.32046115398407, Final Batch Loss: 0.8098861575126648\n",
      "Subject 18, Epoch 161, Loss: 2.1001140475273132, Final Batch Loss: 0.41812121868133545\n",
      "Subject 18, Epoch 162, Loss: 2.2053549587726593, Final Batch Loss: 0.4620867669582367\n",
      "Subject 18, Epoch 163, Loss: 2.0771941244602203, Final Batch Loss: 0.5757436752319336\n",
      "Subject 18, Epoch 164, Loss: 2.248609036207199, Final Batch Loss: 0.7010760307312012\n",
      "Subject 18, Epoch 165, Loss: 2.1465328335762024, Final Batch Loss: 0.5881164073944092\n",
      "Subject 18, Epoch 166, Loss: 2.082645297050476, Final Batch Loss: 0.5001583099365234\n",
      "Subject 18, Epoch 167, Loss: 2.157499521970749, Final Batch Loss: 0.5920236706733704\n",
      "Subject 18, Epoch 168, Loss: 1.8668979108333588, Final Batch Loss: 0.36203089356422424\n",
      "Subject 18, Epoch 169, Loss: 2.195159614086151, Final Batch Loss: 0.7283050417900085\n",
      "Subject 18, Epoch 170, Loss: 2.029271364212036, Final Batch Loss: 0.4861927926540375\n",
      "Subject 18, Epoch 171, Loss: 2.001371443271637, Final Batch Loss: 0.4996452331542969\n",
      "Subject 18, Epoch 172, Loss: 2.2258930802345276, Final Batch Loss: 0.7614907622337341\n",
      "Subject 18, Epoch 173, Loss: 1.9621634483337402, Final Batch Loss: 0.4358784258365631\n",
      "Subject 18, Epoch 174, Loss: 1.8729094862937927, Final Batch Loss: 0.39626750349998474\n",
      "Subject 18, Epoch 175, Loss: 1.999603807926178, Final Batch Loss: 0.5054976344108582\n",
      "Subject 18, Epoch 176, Loss: 2.1550082564353943, Final Batch Loss: 0.6202121376991272\n",
      "Subject 18, Epoch 177, Loss: 1.9470651745796204, Final Batch Loss: 0.36333906650543213\n",
      "Subject 18, Epoch 178, Loss: 2.0788266956806183, Final Batch Loss: 0.5201107859611511\n",
      "Subject 18, Epoch 179, Loss: 2.138640373945236, Final Batch Loss: 0.6542536616325378\n",
      "Subject 18, Epoch 180, Loss: 1.8682092726230621, Final Batch Loss: 0.4187811315059662\n",
      "Subject 18, Epoch 181, Loss: 1.9131987988948822, Final Batch Loss: 0.4857957065105438\n",
      "Subject 18, Epoch 182, Loss: 1.974405586719513, Final Batch Loss: 0.39290738105773926\n",
      "Subject 18, Epoch 183, Loss: 1.8721921145915985, Final Batch Loss: 0.5842651128768921\n",
      "Subject 18, Epoch 184, Loss: 2.0811928510665894, Final Batch Loss: 0.621706485748291\n",
      "Subject 18, Epoch 185, Loss: 1.9391708076000214, Final Batch Loss: 0.47224339842796326\n",
      "Subject 18, Epoch 186, Loss: 1.9090581834316254, Final Batch Loss: 0.5064334273338318\n",
      "Subject 18, Epoch 187, Loss: 2.023699313402176, Final Batch Loss: 0.4694633185863495\n",
      "Subject 18, Epoch 188, Loss: 1.8235473334789276, Final Batch Loss: 0.323442280292511\n",
      "Subject 18, Epoch 189, Loss: 1.960078626871109, Final Batch Loss: 0.4329078495502472\n",
      "Subject 18, Epoch 190, Loss: 2.047720193862915, Final Batch Loss: 0.5579782128334045\n",
      "Subject 18, Epoch 191, Loss: 1.9313247203826904, Final Batch Loss: 0.4167659282684326\n",
      "Subject 18, Epoch 192, Loss: 2.0091772973537445, Final Batch Loss: 0.4215156137943268\n",
      "Subject 18, Epoch 193, Loss: 1.7696698606014252, Final Batch Loss: 0.4010578393936157\n",
      "Subject 18, Epoch 194, Loss: 1.8282327353954315, Final Batch Loss: 0.4402700364589691\n",
      "Subject 18, Epoch 195, Loss: 1.974341481924057, Final Batch Loss: 0.5109714865684509\n",
      "Subject 18, Epoch 196, Loss: 1.984117478132248, Final Batch Loss: 0.5842557549476624\n",
      "Subject 18, Epoch 197, Loss: 1.8526692688465118, Final Batch Loss: 0.4566044509410858\n",
      "Subject 18, Epoch 198, Loss: 1.6751150041818619, Final Batch Loss: 0.24984128773212433\n",
      "Subject 18, Epoch 199, Loss: 1.9481032192707062, Final Batch Loss: 0.40340670943260193\n",
      "Subject 18, Epoch 200, Loss: 1.7725935876369476, Final Batch Loss: 0.36080288887023926\n",
      "Subject 18, Epoch 201, Loss: 1.9866430163383484, Final Batch Loss: 0.517555296421051\n",
      "Subject 18, Epoch 202, Loss: 1.757577657699585, Final Batch Loss: 0.3387577533721924\n",
      "Subject 18, Epoch 203, Loss: 1.9160441160202026, Final Batch Loss: 0.4814845621585846\n",
      "Subject 18, Epoch 204, Loss: 1.8711026012897491, Final Batch Loss: 0.48479267954826355\n",
      "Subject 18, Epoch 205, Loss: 1.8645064234733582, Final Batch Loss: 0.37571337819099426\n",
      "Subject 18, Epoch 206, Loss: 1.8292372226715088, Final Batch Loss: 0.5192161798477173\n",
      "Subject 18, Epoch 207, Loss: 1.7669288516044617, Final Batch Loss: 0.37840577960014343\n",
      "Subject 18, Epoch 208, Loss: 1.9921948313713074, Final Batch Loss: 0.5329276323318481\n",
      "Subject 18, Epoch 209, Loss: 1.7576232552528381, Final Batch Loss: 0.4238424301147461\n",
      "Subject 18, Epoch 210, Loss: 1.7813695669174194, Final Batch Loss: 0.4522295296192169\n",
      "Subject 18, Epoch 211, Loss: 1.7343790531158447, Final Batch Loss: 0.36323702335357666\n",
      "Subject 18, Epoch 212, Loss: 1.758414477109909, Final Batch Loss: 0.3350290060043335\n",
      "Subject 18, Epoch 213, Loss: 1.8011318743228912, Final Batch Loss: 0.3780975639820099\n",
      "Subject 18, Epoch 214, Loss: 1.8458607196807861, Final Batch Loss: 0.5756253004074097\n",
      "Subject 18, Epoch 215, Loss: 1.7820445001125336, Final Batch Loss: 0.5192186236381531\n",
      "Subject 18, Epoch 216, Loss: 1.709272027015686, Final Batch Loss: 0.3934585750102997\n",
      "Subject 18, Epoch 217, Loss: 1.706690788269043, Final Batch Loss: 0.4152021110057831\n",
      "Subject 18, Epoch 218, Loss: 1.7706304490566254, Final Batch Loss: 0.3833913505077362\n",
      "Subject 18, Epoch 219, Loss: 1.8779004514217377, Final Batch Loss: 0.5500710606575012\n",
      "Subject 18, Epoch 220, Loss: 1.8429621160030365, Final Batch Loss: 0.5708946585655212\n",
      "Subject 18, Epoch 221, Loss: 1.8202831745147705, Final Batch Loss: 0.42375364899635315\n",
      "Subject 18, Epoch 222, Loss: 1.7757681608200073, Final Batch Loss: 0.4498711824417114\n",
      "Subject 18, Epoch 223, Loss: 1.7563640773296356, Final Batch Loss: 0.38190579414367676\n",
      "Subject 18, Epoch 224, Loss: 1.8388986587524414, Final Batch Loss: 0.35710275173187256\n",
      "Subject 18, Epoch 225, Loss: 1.8082561492919922, Final Batch Loss: 0.4758969843387604\n",
      "Subject 18, Epoch 226, Loss: 1.7818993926048279, Final Batch Loss: 0.3882496654987335\n",
      "Subject 18, Epoch 227, Loss: 1.7753733098506927, Final Batch Loss: 0.39131948351860046\n",
      "Subject 18, Epoch 228, Loss: 1.7090420424938202, Final Batch Loss: 0.45979538559913635\n",
      "Subject 18, Epoch 229, Loss: 1.7995496988296509, Final Batch Loss: 0.5271590352058411\n",
      "Subject 18, Epoch 230, Loss: 1.7211083471775055, Final Batch Loss: 0.46776440739631653\n",
      "Subject 18, Epoch 231, Loss: 1.6929263174533844, Final Batch Loss: 0.41969916224479675\n",
      "Subject 18, Epoch 232, Loss: 1.7031112313270569, Final Batch Loss: 0.4370311498641968\n",
      "Subject 18, Epoch 233, Loss: 1.9153115153312683, Final Batch Loss: 0.37641534209251404\n",
      "Subject 18, Epoch 234, Loss: 1.7313104569911957, Final Batch Loss: 0.5046454668045044\n",
      "Subject 18, Epoch 235, Loss: 2.0311571657657623, Final Batch Loss: 0.6796774864196777\n",
      "Subject 18, Epoch 236, Loss: 1.79028981924057, Final Batch Loss: 0.5060358643531799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 237, Loss: 1.7681893706321716, Final Batch Loss: 0.3275253176689148\n",
      "Subject 18, Epoch 238, Loss: 1.7341754138469696, Final Batch Loss: 0.4568912982940674\n",
      "Subject 18, Epoch 239, Loss: 1.8087138235569, Final Batch Loss: 0.5162010192871094\n",
      "Subject 18, Epoch 240, Loss: 1.6997048258781433, Final Batch Loss: 0.35378769040107727\n",
      "Subject 18, Epoch 241, Loss: 1.8296636044979095, Final Batch Loss: 0.517636775970459\n",
      "Subject 18, Epoch 242, Loss: 1.618046224117279, Final Batch Loss: 0.29947629570961\n",
      "Subject 18, Epoch 243, Loss: 1.6738429963588715, Final Batch Loss: 0.4441198408603668\n",
      "Subject 18, Epoch 244, Loss: 1.8472319841384888, Final Batch Loss: 0.49585044384002686\n",
      "Subject 18, Epoch 245, Loss: 1.784316897392273, Final Batch Loss: 0.5320667624473572\n",
      "Subject 18, Epoch 246, Loss: 1.6604059636592865, Final Batch Loss: 0.3109176456928253\n",
      "Subject 18, Epoch 247, Loss: 1.7419815361499786, Final Batch Loss: 0.3909062147140503\n",
      "Subject 18, Epoch 248, Loss: 1.725732147693634, Final Batch Loss: 0.2748894989490509\n",
      "Subject 18, Epoch 249, Loss: 1.5519066452980042, Final Batch Loss: 0.2527748942375183\n",
      "Subject 18, Epoch 250, Loss: 1.715764433145523, Final Batch Loss: 0.50454181432724\n",
      "Subject 18, Epoch 251, Loss: 1.7000551223754883, Final Batch Loss: 0.35023602843284607\n",
      "Subject 18, Epoch 252, Loss: 1.6668154001235962, Final Batch Loss: 0.40276455879211426\n",
      "Subject 18, Epoch 253, Loss: 1.7241868078708649, Final Batch Loss: 0.43730220198631287\n",
      "Subject 18, Epoch 254, Loss: 1.7051947712898254, Final Batch Loss: 0.44496479630470276\n",
      "Subject 18, Epoch 255, Loss: 1.7688559889793396, Final Batch Loss: 0.46681126952171326\n",
      "Subject 18, Epoch 256, Loss: 1.6227341294288635, Final Batch Loss: 0.3898559510707855\n",
      "Subject 18, Epoch 257, Loss: 1.7522177398204803, Final Batch Loss: 0.4516189992427826\n",
      "Subject 18, Epoch 258, Loss: 1.564726322889328, Final Batch Loss: 0.36657384037971497\n",
      "Subject 18, Epoch 259, Loss: 1.6430875957012177, Final Batch Loss: 0.3460404872894287\n",
      "Subject 18, Epoch 260, Loss: 1.8771786093711853, Final Batch Loss: 0.5603094696998596\n",
      "Subject 18, Epoch 261, Loss: 1.742711216211319, Final Batch Loss: 0.554130494594574\n",
      "Subject 18, Epoch 262, Loss: 1.7571598887443542, Final Batch Loss: 0.5466130971908569\n",
      "Subject 18, Epoch 263, Loss: 1.4282523840665817, Final Batch Loss: 0.24479763209819794\n",
      "Subject 18, Epoch 264, Loss: 1.6829447150230408, Final Batch Loss: 0.3987385034561157\n",
      "Subject 18, Epoch 265, Loss: 1.6932750940322876, Final Batch Loss: 0.4341529905796051\n",
      "Subject 18, Epoch 266, Loss: 1.8025078475475311, Final Batch Loss: 0.6004302501678467\n",
      "Subject 18, Epoch 267, Loss: 1.8419901132583618, Final Batch Loss: 0.5217933058738708\n",
      "Subject 18, Epoch 268, Loss: 1.570195347070694, Final Batch Loss: 0.3278241753578186\n",
      "Subject 18, Epoch 269, Loss: 1.892275184392929, Final Batch Loss: 0.603705644607544\n",
      "Subject 18, Epoch 270, Loss: 1.6599692702293396, Final Batch Loss: 0.29906895756721497\n",
      "Subject 18, Epoch 271, Loss: 1.7018331289291382, Final Batch Loss: 0.35955917835235596\n",
      "Subject 18, Epoch 272, Loss: 1.3803940117359161, Final Batch Loss: 0.186149001121521\n",
      "Subject 18, Epoch 273, Loss: 1.9013338088989258, Final Batch Loss: 0.7607211470603943\n",
      "Subject 18, Epoch 274, Loss: 1.5943359434604645, Final Batch Loss: 0.19973695278167725\n",
      "Subject 18, Epoch 275, Loss: 1.7444706559181213, Final Batch Loss: 0.40884876251220703\n",
      "Subject 18, Epoch 276, Loss: 1.607744961977005, Final Batch Loss: 0.33400118350982666\n",
      "Subject 18, Epoch 277, Loss: 1.7276062667369843, Final Batch Loss: 0.4640083312988281\n",
      "Subject 18, Epoch 278, Loss: 1.6544841527938843, Final Batch Loss: 0.4414212703704834\n",
      "Subject 18, Epoch 279, Loss: 1.613140046596527, Final Batch Loss: 0.39641430974006653\n",
      "Subject 18, Epoch 280, Loss: 1.7193373441696167, Final Batch Loss: 0.524409294128418\n",
      "Subject 18, Epoch 281, Loss: 1.8111632466316223, Final Batch Loss: 0.5398855209350586\n",
      "Subject 18, Epoch 282, Loss: 1.629806786775589, Final Batch Loss: 0.35520291328430176\n",
      "Subject 18, Epoch 283, Loss: 1.7306585311889648, Final Batch Loss: 0.4688422381877899\n",
      "Subject 18, Epoch 284, Loss: 1.661368578672409, Final Batch Loss: 0.49951449036598206\n",
      "Subject 18, Epoch 285, Loss: 1.5759929418563843, Final Batch Loss: 0.28833508491516113\n",
      "Subject 18, Epoch 286, Loss: 1.5760659873485565, Final Batch Loss: 0.342938631772995\n",
      "Subject 18, Epoch 287, Loss: 1.5244856476783752, Final Batch Loss: 0.34439828991889954\n",
      "Subject 18, Epoch 288, Loss: 1.5446840822696686, Final Batch Loss: 0.2693486511707306\n",
      "Subject 18, Epoch 289, Loss: 1.4900387525558472, Final Batch Loss: 0.26179468631744385\n",
      "Subject 18, Epoch 290, Loss: 1.6600125432014465, Final Batch Loss: 0.5023990869522095\n",
      "Subject 18, Epoch 291, Loss: 1.6246845424175262, Final Batch Loss: 0.4765123128890991\n",
      "Subject 18, Epoch 292, Loss: 1.6078303456306458, Final Batch Loss: 0.3410186469554901\n",
      "Subject 18, Epoch 293, Loss: 1.5571916997432709, Final Batch Loss: 0.37733039259910583\n",
      "Subject 18, Epoch 294, Loss: 1.7165807485580444, Final Batch Loss: 0.5273355841636658\n",
      "Subject 18, Epoch 295, Loss: 1.6421698778867722, Final Batch Loss: 0.23992566764354706\n",
      "Subject 18, Epoch 296, Loss: 1.5747984051704407, Final Batch Loss: 0.3987707197666168\n",
      "Subject 18, Epoch 297, Loss: 1.5338043570518494, Final Batch Loss: 0.3076065182685852\n",
      "Subject 18, Epoch 298, Loss: 1.7944852113723755, Final Batch Loss: 0.5625936985015869\n",
      "Subject 18, Epoch 299, Loss: 1.6823384761810303, Final Batch Loss: 0.47760891914367676\n",
      "Subject 18, Epoch 300, Loss: 1.5157623589038849, Final Batch Loss: 0.3407132625579834\n",
      "Subject 18, Epoch 301, Loss: 1.5050404369831085, Final Batch Loss: 0.274381160736084\n",
      "Subject 18, Epoch 302, Loss: 1.5825451612472534, Final Batch Loss: 0.4229961335659027\n",
      "Subject 18, Epoch 303, Loss: 1.455314040184021, Final Batch Loss: 0.2764289379119873\n",
      "Subject 18, Epoch 304, Loss: 1.4664751291275024, Final Batch Loss: 0.301347941160202\n",
      "Subject 18, Epoch 305, Loss: 1.5111453533172607, Final Batch Loss: 0.4133288860321045\n",
      "Subject 18, Epoch 306, Loss: 1.617701530456543, Final Batch Loss: 0.4532370865345001\n",
      "Subject 18, Epoch 307, Loss: 1.5135284662246704, Final Batch Loss: 0.28656908869743347\n",
      "Subject 18, Epoch 308, Loss: 1.4936712384223938, Final Batch Loss: 0.2616281509399414\n",
      "Subject 18, Epoch 309, Loss: 1.462888091802597, Final Batch Loss: 0.3553151786327362\n",
      "Subject 18, Epoch 310, Loss: 1.6211538910865784, Final Batch Loss: 0.4581887722015381\n",
      "Subject 18, Epoch 311, Loss: 1.56106698513031, Final Batch Loss: 0.30121591687202454\n",
      "Subject 18, Epoch 312, Loss: 1.7695632874965668, Final Batch Loss: 0.5850806832313538\n",
      "Subject 18, Epoch 313, Loss: 1.5731995105743408, Final Batch Loss: 0.34912800788879395\n",
      "Subject 18, Epoch 314, Loss: 1.5543560981750488, Final Batch Loss: 0.28818878531455994\n",
      "Subject 18, Epoch 315, Loss: 1.6343446671962738, Final Batch Loss: 0.4713168144226074\n",
      "Subject 18, Epoch 316, Loss: 1.786519169807434, Final Batch Loss: 0.674674928188324\n",
      "Subject 18, Epoch 317, Loss: 1.6903029084205627, Final Batch Loss: 0.515031635761261\n",
      "Subject 18, Epoch 318, Loss: 1.77143132686615, Final Batch Loss: 0.6439259648323059\n",
      "Subject 18, Epoch 319, Loss: 1.6341189444065094, Final Batch Loss: 0.561287522315979\n",
      "Subject 18, Epoch 320, Loss: 1.5808760523796082, Final Batch Loss: 0.4236994683742523\n",
      "Subject 18, Epoch 321, Loss: 1.59412083029747, Final Batch Loss: 0.46750426292419434\n",
      "Subject 18, Epoch 322, Loss: 1.3924809247255325, Final Batch Loss: 0.24983523786067963\n",
      "Subject 18, Epoch 323, Loss: 1.5606275498867035, Final Batch Loss: 0.49131640791893005\n",
      "Subject 18, Epoch 324, Loss: 1.3681380450725555, Final Batch Loss: 0.29787251353263855\n",
      "Subject 18, Epoch 325, Loss: 1.649663507938385, Final Batch Loss: 0.35500088334083557\n",
      "Subject 18, Epoch 326, Loss: 1.4376273453235626, Final Batch Loss: 0.2780444025993347\n",
      "Subject 18, Epoch 327, Loss: 1.4802296161651611, Final Batch Loss: 0.44748255610466003\n",
      "Subject 18, Epoch 328, Loss: 1.6088688373565674, Final Batch Loss: 0.3551781177520752\n",
      "Subject 18, Epoch 329, Loss: 1.6204152405261993, Final Batch Loss: 0.4135081470012665\n",
      "Subject 18, Epoch 330, Loss: 1.5832397043704987, Final Batch Loss: 0.4703349173069\n",
      "Subject 18, Epoch 331, Loss: 1.6948957741260529, Final Batch Loss: 0.42477771639823914\n",
      "Subject 18, Epoch 332, Loss: 1.5196846425533295, Final Batch Loss: 0.31213775277137756\n",
      "Subject 18, Epoch 333, Loss: 1.5747921764850616, Final Batch Loss: 0.37366291880607605\n",
      "Subject 18, Epoch 334, Loss: 1.621450662612915, Final Batch Loss: 0.5219323039054871\n",
      "Subject 18, Epoch 335, Loss: 1.3939370810985565, Final Batch Loss: 0.2324291467666626\n",
      "Subject 18, Epoch 336, Loss: 1.4356628358364105, Final Batch Loss: 0.29210028052330017\n",
      "Subject 18, Epoch 337, Loss: 1.4303686320781708, Final Batch Loss: 0.2856583297252655\n",
      "Subject 18, Epoch 338, Loss: 1.4834432303905487, Final Batch Loss: 0.4140230119228363\n",
      "Subject 18, Epoch 339, Loss: 1.528542399406433, Final Batch Loss: 0.4139072895050049\n",
      "Subject 18, Epoch 340, Loss: 1.3084020614624023, Final Batch Loss: 0.2366378903388977\n",
      "Subject 18, Epoch 341, Loss: 1.3859034180641174, Final Batch Loss: 0.2531684935092926\n",
      "Subject 18, Epoch 342, Loss: 1.4298467338085175, Final Batch Loss: 0.33631566166877747\n",
      "Subject 18, Epoch 343, Loss: 1.723666250705719, Final Batch Loss: 0.5394841432571411\n",
      "Subject 18, Epoch 344, Loss: 1.4108071327209473, Final Batch Loss: 0.25190332531929016\n",
      "Subject 18, Epoch 345, Loss: 1.616159975528717, Final Batch Loss: 0.4885568618774414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 346, Loss: 1.3995206356048584, Final Batch Loss: 0.268435537815094\n",
      "Subject 18, Epoch 347, Loss: 1.8350840508937836, Final Batch Loss: 0.7193613052368164\n",
      "Subject 18, Epoch 348, Loss: 1.4580950140953064, Final Batch Loss: 0.29135486483573914\n",
      "Subject 18, Epoch 349, Loss: 1.5362577736377716, Final Batch Loss: 0.45757198333740234\n",
      "Subject 18, Epoch 350, Loss: 1.6555198729038239, Final Batch Loss: 0.5798112154006958\n",
      "Subject 18, Epoch 351, Loss: 1.4058517217636108, Final Batch Loss: 0.33111992478370667\n",
      "Subject 18, Epoch 352, Loss: 1.7585533559322357, Final Batch Loss: 0.5026209354400635\n",
      "Subject 18, Epoch 353, Loss: 1.5736618638038635, Final Batch Loss: 0.44958099722862244\n",
      "Subject 18, Epoch 354, Loss: 1.3532607555389404, Final Batch Loss: 0.3551374673843384\n",
      "Subject 18, Epoch 355, Loss: 1.5219034552574158, Final Batch Loss: 0.4446631371974945\n",
      "Subject 18, Epoch 356, Loss: 1.5257875323295593, Final Batch Loss: 0.4524526298046112\n",
      "Subject 18, Epoch 357, Loss: 1.4763561189174652, Final Batch Loss: 0.3553377091884613\n",
      "Subject 18, Epoch 358, Loss: 1.3459048867225647, Final Batch Loss: 0.33023881912231445\n",
      "Subject 18, Epoch 359, Loss: 1.574526846408844, Final Batch Loss: 0.48991528153419495\n",
      "Subject 18, Epoch 360, Loss: 1.4259426593780518, Final Batch Loss: 0.36491599678993225\n",
      "Subject 18, Epoch 361, Loss: 1.4580576121807098, Final Batch Loss: 0.34066784381866455\n",
      "Subject 18, Epoch 362, Loss: 1.434736043214798, Final Batch Loss: 0.33981606364250183\n",
      "Subject 18, Epoch 363, Loss: 1.4706712663173676, Final Batch Loss: 0.4026442766189575\n",
      "Subject 18, Epoch 364, Loss: 1.3092308640480042, Final Batch Loss: 0.31361329555511475\n",
      "Subject 18, Epoch 365, Loss: 1.3661334365606308, Final Batch Loss: 0.21656186878681183\n",
      "Subject 18, Epoch 366, Loss: 1.4365728795528412, Final Batch Loss: 0.4816242754459381\n",
      "Subject 18, Epoch 367, Loss: 1.4252959489822388, Final Batch Loss: 0.3367372453212738\n",
      "Subject 18, Epoch 368, Loss: 1.529705673456192, Final Batch Loss: 0.42371833324432373\n",
      "Subject 18, Epoch 369, Loss: 1.279532939195633, Final Batch Loss: 0.2547012269496918\n",
      "Subject 18, Epoch 370, Loss: 1.4278169572353363, Final Batch Loss: 0.4004344046115875\n",
      "Subject 18, Epoch 371, Loss: 1.2821622639894485, Final Batch Loss: 0.1431359201669693\n",
      "Subject 18, Epoch 372, Loss: 1.4540913105010986, Final Batch Loss: 0.36463531851768494\n",
      "Subject 18, Epoch 373, Loss: 1.38201242685318, Final Batch Loss: 0.2900073230266571\n",
      "Subject 18, Epoch 374, Loss: 1.4004536867141724, Final Batch Loss: 0.3575007915496826\n",
      "Subject 18, Epoch 375, Loss: 1.6272660493850708, Final Batch Loss: 0.5248538851737976\n",
      "Subject 18, Epoch 376, Loss: 1.4805099964141846, Final Batch Loss: 0.46161356568336487\n",
      "Subject 18, Epoch 377, Loss: 1.3690822422504425, Final Batch Loss: 0.32878950238227844\n",
      "Subject 18, Epoch 378, Loss: 1.3472740948200226, Final Batch Loss: 0.23750782012939453\n",
      "Subject 18, Epoch 379, Loss: 1.509280651807785, Final Batch Loss: 0.5015988945960999\n",
      "Subject 18, Epoch 380, Loss: 1.3017424046993256, Final Batch Loss: 0.35239431262016296\n",
      "Subject 18, Epoch 381, Loss: 1.2611803263425827, Final Batch Loss: 0.1926429718732834\n",
      "Subject 18, Epoch 382, Loss: 1.2835482209920883, Final Batch Loss: 0.19702564179897308\n",
      "Subject 18, Epoch 383, Loss: 1.4295005202293396, Final Batch Loss: 0.44424310326576233\n",
      "Subject 18, Epoch 384, Loss: 1.3529022783041, Final Batch Loss: 0.24781174957752228\n",
      "Subject 18, Epoch 385, Loss: 1.2248650044202805, Final Batch Loss: 0.22585751116275787\n",
      "Subject 18, Epoch 386, Loss: 1.486418902873993, Final Batch Loss: 0.4601489305496216\n",
      "Subject 18, Epoch 387, Loss: 1.2747536599636078, Final Batch Loss: 0.2743321359157562\n",
      "Subject 18, Epoch 388, Loss: 1.46432563662529, Final Batch Loss: 0.3955604135990143\n",
      "Subject 18, Epoch 389, Loss: 1.2939614355564117, Final Batch Loss: 0.2629318833351135\n",
      "Subject 18, Epoch 390, Loss: 1.718730092048645, Final Batch Loss: 0.656916081905365\n",
      "Subject 18, Epoch 391, Loss: 1.4744893908500671, Final Batch Loss: 0.335486501455307\n",
      "Subject 18, Epoch 392, Loss: 1.2264790832996368, Final Batch Loss: 0.26100242137908936\n",
      "Subject 18, Epoch 393, Loss: 1.531562179327011, Final Batch Loss: 0.39194419980049133\n",
      "Subject 18, Epoch 394, Loss: 1.512233018875122, Final Batch Loss: 0.4895584285259247\n",
      "Subject 18, Epoch 395, Loss: 1.5773099660873413, Final Batch Loss: 0.5869739055633545\n",
      "Subject 18, Epoch 396, Loss: 1.45725217461586, Final Batch Loss: 0.3775928318500519\n",
      "Subject 18, Epoch 397, Loss: 1.3300648033618927, Final Batch Loss: 0.20319998264312744\n",
      "Subject 18, Epoch 398, Loss: 1.355763852596283, Final Batch Loss: 0.35475456714630127\n",
      "Subject 18, Epoch 399, Loss: 1.363158494234085, Final Batch Loss: 0.3307385742664337\n",
      "Subject 18, Epoch 400, Loss: 1.3821907341480255, Final Batch Loss: 0.35753798484802246\n",
      "Subject 18, Epoch 401, Loss: 1.476877212524414, Final Batch Loss: 0.3535301983356476\n",
      "Subject 18, Epoch 402, Loss: 1.3989112377166748, Final Batch Loss: 0.3425099849700928\n",
      "Subject 18, Epoch 403, Loss: 1.5178135931491852, Final Batch Loss: 0.444439172744751\n",
      "Subject 18, Epoch 404, Loss: 1.5312673151493073, Final Batch Loss: 0.44808855652809143\n",
      "Subject 18, Epoch 405, Loss: 1.3154676258563995, Final Batch Loss: 0.311886727809906\n",
      "Subject 18, Epoch 406, Loss: 1.4563010931015015, Final Batch Loss: 0.4339364767074585\n",
      "Subject 18, Epoch 407, Loss: 1.3540992736816406, Final Batch Loss: 0.3032185137271881\n",
      "Subject 18, Epoch 408, Loss: 1.5230181962251663, Final Batch Loss: 0.5150801539421082\n",
      "Subject 18, Epoch 409, Loss: 1.3543117940425873, Final Batch Loss: 0.3781487047672272\n",
      "Subject 18, Epoch 410, Loss: 1.2940292060375214, Final Batch Loss: 0.2656891345977783\n",
      "Subject 18, Epoch 411, Loss: 1.2821459621191025, Final Batch Loss: 0.20159728825092316\n",
      "Subject 18, Epoch 412, Loss: 1.422315627336502, Final Batch Loss: 0.30735281109809875\n",
      "Subject 18, Epoch 413, Loss: 1.3478037416934967, Final Batch Loss: 0.2867245376110077\n",
      "Subject 18, Epoch 414, Loss: 1.3290177583694458, Final Batch Loss: 0.28168144822120667\n",
      "Subject 18, Epoch 415, Loss: 1.3568962812423706, Final Batch Loss: 0.3527810275554657\n",
      "Subject 18, Epoch 416, Loss: 1.5746352970600128, Final Batch Loss: 0.5767015814781189\n",
      "Subject 18, Epoch 417, Loss: 1.2785703539848328, Final Batch Loss: 0.3147646486759186\n",
      "Subject 18, Epoch 418, Loss: 1.4607519209384918, Final Batch Loss: 0.37269386649131775\n",
      "Subject 18, Epoch 419, Loss: 1.3211230039596558, Final Batch Loss: 0.2592718005180359\n",
      "Subject 18, Epoch 420, Loss: 1.3662764132022858, Final Batch Loss: 0.33503425121307373\n",
      "Subject 18, Epoch 421, Loss: 1.2654706537723541, Final Batch Loss: 0.2192150354385376\n",
      "Subject 18, Epoch 422, Loss: 1.3207582831382751, Final Batch Loss: 0.27755650877952576\n",
      "Subject 18, Epoch 423, Loss: 1.389743447303772, Final Batch Loss: 0.2888762354850769\n",
      "Subject 18, Epoch 424, Loss: 1.260879024863243, Final Batch Loss: 0.24367158114910126\n",
      "Subject 18, Epoch 425, Loss: 1.528875470161438, Final Batch Loss: 0.5052129626274109\n",
      "Subject 18, Epoch 426, Loss: 1.368619292974472, Final Batch Loss: 0.28554147481918335\n",
      "Subject 18, Epoch 427, Loss: 1.3051940500736237, Final Batch Loss: 0.2363913655281067\n",
      "Subject 18, Epoch 428, Loss: 1.2772221267223358, Final Batch Loss: 0.29378071427345276\n",
      "Subject 18, Epoch 429, Loss: 1.4570490717887878, Final Batch Loss: 0.42852962017059326\n",
      "Subject 18, Epoch 430, Loss: 1.3996625542640686, Final Batch Loss: 0.3979499340057373\n",
      "Subject 18, Epoch 431, Loss: 1.3440375328063965, Final Batch Loss: 0.24035966396331787\n",
      "Subject 18, Epoch 432, Loss: 1.2219936549663544, Final Batch Loss: 0.1994531750679016\n",
      "Subject 18, Epoch 433, Loss: 1.3969221413135529, Final Batch Loss: 0.3921259641647339\n",
      "Subject 18, Epoch 434, Loss: 1.3491178452968597, Final Batch Loss: 0.2955016791820526\n",
      "Subject 18, Epoch 435, Loss: 1.2363959848880768, Final Batch Loss: 0.30905577540397644\n",
      "Subject 18, Epoch 436, Loss: 1.3980680108070374, Final Batch Loss: 0.4016093313694\n",
      "Subject 18, Epoch 437, Loss: 1.4226557612419128, Final Batch Loss: 0.42423591017723083\n",
      "Subject 18, Epoch 438, Loss: 1.2733089327812195, Final Batch Loss: 0.3215777575969696\n",
      "Subject 18, Epoch 439, Loss: 1.3105144798755646, Final Batch Loss: 0.31347522139549255\n",
      "Subject 18, Epoch 440, Loss: 1.3149809539318085, Final Batch Loss: 0.3740837574005127\n",
      "Subject 18, Epoch 441, Loss: 1.2756258845329285, Final Batch Loss: 0.28606370091438293\n",
      "Subject 18, Epoch 442, Loss: 1.317592740058899, Final Batch Loss: 0.2750698924064636\n",
      "Subject 18, Epoch 443, Loss: 1.527684673666954, Final Batch Loss: 0.5437008142471313\n",
      "Subject 18, Epoch 444, Loss: 1.2043579071760178, Final Batch Loss: 0.2689065635204315\n",
      "Subject 18, Epoch 445, Loss: 1.4448478817939758, Final Batch Loss: 0.32384979724884033\n",
      "Subject 18, Epoch 446, Loss: 1.2047496140003204, Final Batch Loss: 0.21457457542419434\n",
      "Subject 18, Epoch 447, Loss: 1.4623290300369263, Final Batch Loss: 0.4774427115917206\n",
      "Subject 18, Epoch 448, Loss: 1.28522390127182, Final Batch Loss: 0.26949959993362427\n",
      "Subject 18, Epoch 449, Loss: 1.329197645187378, Final Batch Loss: 0.30000200867652893\n",
      "Subject 18, Epoch 450, Loss: 1.3101973235607147, Final Batch Loss: 0.27104851603507996\n",
      "Subject 18, Epoch 451, Loss: 1.4156634509563446, Final Batch Loss: 0.3683045208454132\n",
      "Subject 18, Epoch 452, Loss: 1.2749658823013306, Final Batch Loss: 0.3160502016544342\n",
      "Subject 18, Epoch 453, Loss: 1.2663516402244568, Final Batch Loss: 0.2462579607963562\n",
      "Subject 18, Epoch 454, Loss: 1.4261519759893417, Final Batch Loss: 0.40997251868247986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 455, Loss: 1.3324277102947235, Final Batch Loss: 0.3904384672641754\n",
      "Subject 18, Epoch 456, Loss: 1.341292679309845, Final Batch Loss: 0.3777691125869751\n",
      "Subject 18, Epoch 457, Loss: 1.3167190551757812, Final Batch Loss: 0.366690069437027\n",
      "Subject 18, Epoch 458, Loss: 1.2830466032028198, Final Batch Loss: 0.3313843011856079\n",
      "Subject 18, Epoch 459, Loss: 1.3971708416938782, Final Batch Loss: 0.33875417709350586\n",
      "Subject 18, Epoch 460, Loss: 1.2328713983297348, Final Batch Loss: 0.22406543791294098\n",
      "Subject 18, Epoch 461, Loss: 1.4113186597824097, Final Batch Loss: 0.46060648560523987\n",
      "Subject 18, Epoch 462, Loss: 1.4452002942562103, Final Batch Loss: 0.5027335286140442\n",
      "Subject 18, Epoch 463, Loss: 1.2705605328083038, Final Batch Loss: 0.3342039883136749\n",
      "Subject 18, Epoch 464, Loss: 1.2748880982398987, Final Batch Loss: 0.3136914074420929\n",
      "Subject 18, Epoch 465, Loss: 1.4196403324604034, Final Batch Loss: 0.31635311245918274\n",
      "Subject 18, Epoch 466, Loss: 1.346357524394989, Final Batch Loss: 0.3083551824092865\n",
      "Subject 18, Epoch 467, Loss: 1.2524166405200958, Final Batch Loss: 0.3093762695789337\n",
      "Subject 18, Epoch 468, Loss: 1.2508586794137955, Final Batch Loss: 0.24030478298664093\n",
      "Subject 18, Epoch 469, Loss: 1.1623107120394707, Final Batch Loss: 0.11477630585432053\n",
      "Subject 18, Epoch 470, Loss: 1.2342554330825806, Final Batch Loss: 0.2876201272010803\n",
      "Subject 18, Epoch 471, Loss: 1.2549168765544891, Final Batch Loss: 0.20796918869018555\n",
      "Subject 18, Epoch 472, Loss: 1.3231843411922455, Final Batch Loss: 0.3243050277233124\n",
      "Subject 18, Epoch 473, Loss: 1.1755964905023575, Final Batch Loss: 0.2053651064634323\n",
      "Subject 18, Epoch 474, Loss: 1.3203878998756409, Final Batch Loss: 0.25353479385375977\n",
      "Subject 18, Epoch 475, Loss: 1.1781822741031647, Final Batch Loss: 0.25432470440864563\n",
      "Subject 18, Epoch 476, Loss: 1.209953397512436, Final Batch Loss: 0.27097561955451965\n",
      "Subject 18, Epoch 477, Loss: 1.3153068125247955, Final Batch Loss: 0.33019766211509705\n",
      "Subject 18, Epoch 478, Loss: 1.5333459079265594, Final Batch Loss: 0.45750299096107483\n",
      "Subject 18, Epoch 479, Loss: 1.3357390314340591, Final Batch Loss: 0.46881532669067383\n",
      "Subject 18, Epoch 480, Loss: 1.206722766160965, Final Batch Loss: 0.23931121826171875\n",
      "Subject 18, Epoch 481, Loss: 1.3344679772853851, Final Batch Loss: 0.37323951721191406\n",
      "Subject 18, Epoch 482, Loss: 1.1931897103786469, Final Batch Loss: 0.32096946239471436\n",
      "Subject 18, Epoch 483, Loss: 1.2296965718269348, Final Batch Loss: 0.2484254240989685\n",
      "Subject 18, Epoch 484, Loss: 1.3242918252944946, Final Batch Loss: 0.3218059837818146\n",
      "Subject 18, Epoch 485, Loss: 1.1304182559251785, Final Batch Loss: 0.18462522327899933\n",
      "Subject 18, Epoch 486, Loss: 1.4924641847610474, Final Batch Loss: 0.4294159412384033\n",
      "Subject 18, Epoch 487, Loss: 1.3297989964485168, Final Batch Loss: 0.33776381611824036\n",
      "Subject 18, Epoch 488, Loss: 1.2021287381649017, Final Batch Loss: 0.29371383786201477\n",
      "Subject 18, Epoch 489, Loss: 1.419540524482727, Final Batch Loss: 0.4602753818035126\n",
      "Subject 18, Epoch 490, Loss: 1.214860737323761, Final Batch Loss: 0.26568603515625\n",
      "Subject 18, Epoch 491, Loss: 1.275892049074173, Final Batch Loss: 0.3320756256580353\n",
      "Subject 18, Epoch 492, Loss: 1.2264426350593567, Final Batch Loss: 0.26534590125083923\n",
      "Subject 18, Epoch 493, Loss: 1.3245694935321808, Final Batch Loss: 0.3565826416015625\n",
      "Subject 18, Epoch 494, Loss: 1.2294605374336243, Final Batch Loss: 0.2545579969882965\n",
      "Subject 18, Epoch 495, Loss: 1.1299136728048325, Final Batch Loss: 0.17349286377429962\n",
      "Subject 18, Epoch 496, Loss: 1.29811030626297, Final Batch Loss: 0.3345378637313843\n",
      "Subject 18, Epoch 497, Loss: 1.0723953545093536, Final Batch Loss: 0.1706475019454956\n",
      "Subject 18, Epoch 498, Loss: 1.2909595370292664, Final Batch Loss: 0.34428274631500244\n",
      "Subject 18, Epoch 499, Loss: 1.256192147731781, Final Batch Loss: 0.2710725963115692\n",
      "Subject 18, Epoch 500, Loss: 1.347854807972908, Final Batch Loss: 0.22614224255084991\n",
      "Subject 18, Epoch 501, Loss: 1.4609203487634659, Final Batch Loss: 0.4552587568759918\n",
      "Subject 18, Epoch 502, Loss: 1.2025745809078217, Final Batch Loss: 0.22592973709106445\n",
      "Subject 18, Epoch 503, Loss: 1.2627556025981903, Final Batch Loss: 0.28854838013648987\n",
      "Subject 18, Epoch 504, Loss: 1.4125729203224182, Final Batch Loss: 0.4511472284793854\n",
      "Subject 18, Epoch 505, Loss: 1.2050218880176544, Final Batch Loss: 0.2250334620475769\n",
      "Subject 18, Epoch 506, Loss: 1.2149890065193176, Final Batch Loss: 0.2605413496494293\n",
      "Subject 18, Epoch 507, Loss: 1.275784581899643, Final Batch Loss: 0.25289416313171387\n",
      "Subject 18, Epoch 508, Loss: 1.2490283250808716, Final Batch Loss: 0.2621507942676544\n",
      "Subject 18, Epoch 509, Loss: 1.2275294065475464, Final Batch Loss: 0.33695390820503235\n",
      "Subject 18, Epoch 510, Loss: 1.1647924184799194, Final Batch Loss: 0.19141137599945068\n",
      "Subject 18, Epoch 511, Loss: 1.2628571391105652, Final Batch Loss: 0.25634655356407166\n",
      "Subject 18, Epoch 512, Loss: 1.2025184035301208, Final Batch Loss: 0.1995595097541809\n",
      "Subject 18, Epoch 513, Loss: 1.1196298450231552, Final Batch Loss: 0.21065329015254974\n",
      "Subject 18, Epoch 514, Loss: 1.1389449387788773, Final Batch Loss: 0.2133001834154129\n",
      "Subject 18, Epoch 515, Loss: 1.2981911897659302, Final Batch Loss: 0.34247922897338867\n",
      "Subject 18, Epoch 516, Loss: 1.139377310872078, Final Batch Loss: 0.16734369099140167\n",
      "Subject 18, Epoch 517, Loss: 1.197578027844429, Final Batch Loss: 0.22003252804279327\n",
      "Subject 18, Epoch 518, Loss: 1.2565492391586304, Final Batch Loss: 0.26690730452537537\n",
      "Subject 18, Epoch 519, Loss: 1.151711344718933, Final Batch Loss: 0.296903520822525\n",
      "Subject 18, Epoch 520, Loss: 1.20119309425354, Final Batch Loss: 0.3024052679538727\n",
      "Subject 18, Epoch 521, Loss: 1.2263374626636505, Final Batch Loss: 0.3076467216014862\n",
      "Subject 18, Epoch 522, Loss: 1.198911651968956, Final Batch Loss: 0.23094458878040314\n",
      "Subject 18, Epoch 523, Loss: 1.0520498678088188, Final Batch Loss: 0.11234530061483383\n",
      "Subject 18, Epoch 524, Loss: 1.2742418050765991, Final Batch Loss: 0.2760140895843506\n",
      "Subject 18, Epoch 525, Loss: 1.25895494222641, Final Batch Loss: 0.3523004949092865\n",
      "Subject 18, Epoch 526, Loss: 1.2077937126159668, Final Batch Loss: 0.29073959589004517\n",
      "Subject 18, Epoch 527, Loss: 1.1562144756317139, Final Batch Loss: 0.2833363115787506\n",
      "Subject 18, Epoch 528, Loss: 1.2391647547483444, Final Batch Loss: 0.33620476722717285\n",
      "Subject 18, Epoch 529, Loss: 1.3366270959377289, Final Batch Loss: 0.45444679260253906\n",
      "Subject 18, Epoch 530, Loss: 1.3926953375339508, Final Batch Loss: 0.46066057682037354\n",
      "Subject 18, Epoch 531, Loss: 1.249901443719864, Final Batch Loss: 0.32270678877830505\n",
      "Subject 18, Epoch 532, Loss: 1.1278800517320633, Final Batch Loss: 0.23749245703220367\n",
      "Subject 18, Epoch 533, Loss: 1.2023769915103912, Final Batch Loss: 0.413423091173172\n",
      "Subject 18, Epoch 534, Loss: 1.1254030764102936, Final Batch Loss: 0.20754146575927734\n",
      "Subject 18, Epoch 535, Loss: 1.211355745792389, Final Batch Loss: 0.22369647026062012\n",
      "Subject 18, Epoch 536, Loss: 1.3177820295095444, Final Batch Loss: 0.45640191435813904\n",
      "Subject 18, Epoch 537, Loss: 1.1273289620876312, Final Batch Loss: 0.179093137383461\n",
      "Subject 18, Epoch 538, Loss: 1.2935566008090973, Final Batch Loss: 0.2792626917362213\n",
      "Subject 18, Epoch 539, Loss: 1.2576287686824799, Final Batch Loss: 0.3843037784099579\n",
      "Subject 18, Epoch 540, Loss: 1.2693083584308624, Final Batch Loss: 0.3740294873714447\n",
      "Subject 18, Epoch 541, Loss: 1.4865073561668396, Final Batch Loss: 0.5350376963615417\n",
      "Subject 18, Epoch 542, Loss: 1.3023298680782318, Final Batch Loss: 0.3464280366897583\n",
      "Subject 18, Epoch 543, Loss: 1.3878560066223145, Final Batch Loss: 0.5038319826126099\n",
      "Subject 18, Epoch 544, Loss: 1.1508983969688416, Final Batch Loss: 0.2612045109272003\n",
      "Subject 18, Epoch 545, Loss: 1.2097113579511642, Final Batch Loss: 0.28222018480300903\n",
      "Subject 18, Epoch 546, Loss: 1.2265643179416656, Final Batch Loss: 0.2950398027896881\n",
      "Subject 18, Epoch 547, Loss: 1.1237997859716415, Final Batch Loss: 0.19107471406459808\n",
      "Subject 18, Epoch 548, Loss: 1.275777518749237, Final Batch Loss: 0.33291295170783997\n",
      "Subject 18, Epoch 549, Loss: 1.0921332985162735, Final Batch Loss: 0.13839741051197052\n",
      "Subject 18, Epoch 550, Loss: 1.1435605138540268, Final Batch Loss: 0.21214605867862701\n",
      "Subject 18, Epoch 551, Loss: 1.3269708901643753, Final Batch Loss: 0.431453138589859\n",
      "Subject 18, Epoch 552, Loss: 1.2175317704677582, Final Batch Loss: 0.32792729139328003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 553, Loss: 1.2248846143484116, Final Batch Loss: 0.2085217982530594\n",
      "Subject 18, Epoch 554, Loss: 1.1289036273956299, Final Batch Loss: 0.18411928415298462\n",
      "Subject 18, Epoch 555, Loss: 1.4088582396507263, Final Batch Loss: 0.45979949831962585\n",
      "Subject 18, Epoch 556, Loss: 1.3082095980644226, Final Batch Loss: 0.2635158598423004\n",
      "Subject 18, Epoch 557, Loss: 1.209123820066452, Final Batch Loss: 0.3105989098548889\n",
      "Subject 18, Epoch 558, Loss: 1.6350756883621216, Final Batch Loss: 0.7333406805992126\n",
      "Subject 18, Epoch 559, Loss: 1.1805122792720795, Final Batch Loss: 0.32002928853034973\n",
      "Subject 18, Epoch 560, Loss: 1.258219450712204, Final Batch Loss: 0.4228729009628296\n",
      "Subject 18, Epoch 561, Loss: 1.270550012588501, Final Batch Loss: 0.4164826571941376\n",
      "Subject 18, Epoch 562, Loss: 1.2866833806037903, Final Batch Loss: 0.37415602803230286\n",
      "Subject 18, Epoch 563, Loss: 1.2515927255153656, Final Batch Loss: 0.3204621374607086\n",
      "Subject 18, Epoch 564, Loss: 1.224022388458252, Final Batch Loss: 0.30753034353256226\n",
      "Subject 18, Epoch 565, Loss: 1.1867535263299942, Final Batch Loss: 0.2846795618534088\n",
      "Subject 18, Epoch 566, Loss: 1.246636688709259, Final Batch Loss: 0.29291823506355286\n",
      "Subject 18, Epoch 567, Loss: 1.237024188041687, Final Batch Loss: 0.34951820969581604\n",
      "Subject 18, Epoch 568, Loss: 1.309230089187622, Final Batch Loss: 0.41048166155815125\n",
      "Subject 18, Epoch 569, Loss: 1.4375838339328766, Final Batch Loss: 0.5318012237548828\n",
      "Subject 18, Epoch 570, Loss: 1.1901908665895462, Final Batch Loss: 0.2645195424556732\n",
      "Subject 18, Epoch 571, Loss: 1.4048476815223694, Final Batch Loss: 0.4873791038990021\n",
      "Subject 18, Epoch 572, Loss: 1.4254205822944641, Final Batch Loss: 0.4934045076370239\n",
      "Subject 18, Epoch 573, Loss: 1.1256952285766602, Final Batch Loss: 0.2726079523563385\n",
      "Subject 18, Epoch 574, Loss: 1.3541335761547089, Final Batch Loss: 0.2947203516960144\n",
      "Subject 18, Epoch 575, Loss: 1.1319453716278076, Final Batch Loss: 0.24432377517223358\n",
      "Subject 18, Epoch 576, Loss: 1.1368501484394073, Final Batch Loss: 0.16824275255203247\n",
      "Subject 18, Epoch 577, Loss: 1.2055906057357788, Final Batch Loss: 0.33438315987586975\n",
      "Subject 18, Epoch 578, Loss: 1.0598196238279343, Final Batch Loss: 0.20712709426879883\n",
      "Subject 18, Epoch 579, Loss: 1.2264752686023712, Final Batch Loss: 0.28683850169181824\n",
      "Subject 18, Epoch 580, Loss: 1.350112408399582, Final Batch Loss: 0.49218490719795227\n",
      "Subject 18, Epoch 581, Loss: 1.2079988718032837, Final Batch Loss: 0.23505163192749023\n",
      "Subject 18, Epoch 582, Loss: 1.1354492604732513, Final Batch Loss: 0.2799881100654602\n",
      "Subject 18, Epoch 583, Loss: 1.119535580277443, Final Batch Loss: 0.19145344197750092\n",
      "Subject 18, Epoch 584, Loss: 1.1901367753744125, Final Batch Loss: 0.33124375343322754\n",
      "Subject 18, Epoch 585, Loss: 1.153458595275879, Final Batch Loss: 0.19392848014831543\n",
      "Subject 18, Epoch 586, Loss: 1.2598211467266083, Final Batch Loss: 0.3526516258716583\n",
      "Subject 18, Epoch 587, Loss: 1.2111948728561401, Final Batch Loss: 0.2600347697734833\n",
      "Subject 18, Epoch 588, Loss: 1.024728685617447, Final Batch Loss: 0.19635379314422607\n",
      "Subject 18, Epoch 589, Loss: 1.1256496459245682, Final Batch Loss: 0.2647010087966919\n",
      "Subject 18, Epoch 590, Loss: 0.9927146136760712, Final Batch Loss: 0.0696883499622345\n",
      "Subject 18, Epoch 591, Loss: 1.244407594203949, Final Batch Loss: 0.39832648634910583\n",
      "Subject 18, Epoch 592, Loss: 1.3046129047870636, Final Batch Loss: 0.3994005024433136\n",
      "Subject 18, Epoch 593, Loss: 1.235505223274231, Final Batch Loss: 0.32331767678260803\n",
      "Subject 18, Epoch 594, Loss: 1.2612466514110565, Final Batch Loss: 0.33807191252708435\n",
      "Subject 18, Epoch 595, Loss: 1.3072073757648468, Final Batch Loss: 0.42839276790618896\n",
      "Subject 18, Epoch 596, Loss: 1.242123693227768, Final Batch Loss: 0.36274316906929016\n",
      "Subject 18, Epoch 597, Loss: 1.2172846496105194, Final Batch Loss: 0.3597208559513092\n",
      "Subject 18, Epoch 598, Loss: 1.0898202657699585, Final Batch Loss: 0.23673880100250244\n",
      "Subject 18, Epoch 599, Loss: 1.0994489043951035, Final Batch Loss: 0.2659669518470764\n",
      "Subject 18, Epoch 600, Loss: 1.166907548904419, Final Batch Loss: 0.33379849791526794\n",
      "Subject 18, Epoch 601, Loss: 1.1396308243274689, Final Batch Loss: 0.21793198585510254\n",
      "Subject 18, Epoch 602, Loss: 1.0943550169467926, Final Batch Loss: 0.23103410005569458\n",
      "Subject 18, Epoch 603, Loss: 1.1383903175592422, Final Batch Loss: 0.23079706728458405\n",
      "Subject 18, Epoch 604, Loss: 1.2938784807920456, Final Batch Loss: 0.5450641512870789\n",
      "Subject 18, Epoch 605, Loss: 1.0738248378038406, Final Batch Loss: 0.24310527741909027\n",
      "Subject 18, Epoch 606, Loss: 0.9694010615348816, Final Batch Loss: 0.11203396320343018\n",
      "Subject 18, Epoch 607, Loss: 1.2155105769634247, Final Batch Loss: 0.3789292275905609\n",
      "Subject 18, Epoch 608, Loss: 1.4887002557516098, Final Batch Loss: 0.6462977528572083\n",
      "Subject 18, Epoch 609, Loss: 1.191519170999527, Final Batch Loss: 0.39274439215660095\n",
      "Subject 18, Epoch 610, Loss: 1.1851463913917542, Final Batch Loss: 0.31948593258857727\n",
      "Subject 18, Epoch 611, Loss: 1.2077758610248566, Final Batch Loss: 0.3150058686733246\n",
      "Subject 18, Epoch 612, Loss: 1.1172815263271332, Final Batch Loss: 0.2781192660331726\n",
      "Subject 18, Epoch 613, Loss: 1.093144178390503, Final Batch Loss: 0.2869679629802704\n",
      "Subject 18, Epoch 614, Loss: 1.0737297534942627, Final Batch Loss: 0.17579114437103271\n",
      "Subject 18, Epoch 615, Loss: 1.2245697677135468, Final Batch Loss: 0.36510738730430603\n",
      "Subject 18, Epoch 616, Loss: 0.9899064749479294, Final Batch Loss: 0.18737876415252686\n",
      "Subject 18, Epoch 617, Loss: 1.1197929382324219, Final Batch Loss: 0.27975913882255554\n",
      "Subject 18, Epoch 618, Loss: 1.1903586983680725, Final Batch Loss: 0.25805553793907166\n",
      "Subject 18, Epoch 619, Loss: 1.060003787279129, Final Batch Loss: 0.25350645184516907\n",
      "Subject 18, Epoch 620, Loss: 1.2058042585849762, Final Batch Loss: 0.29513677954673767\n",
      "Subject 18, Epoch 621, Loss: 1.2962029874324799, Final Batch Loss: 0.46342524886131287\n",
      "Subject 18, Epoch 622, Loss: 0.9966719001531601, Final Batch Loss: 0.17797137796878815\n",
      "Subject 18, Epoch 623, Loss: 1.1510997712612152, Final Batch Loss: 0.25766870379447937\n",
      "Subject 18, Epoch 624, Loss: 1.479403167963028, Final Batch Loss: 0.5044288635253906\n",
      "Subject 18, Epoch 625, Loss: 1.2083472907543182, Final Batch Loss: 0.3379817306995392\n",
      "Subject 18, Epoch 626, Loss: 1.0493555516004562, Final Batch Loss: 0.1709655076265335\n",
      "Subject 18, Epoch 627, Loss: 1.3746465146541595, Final Batch Loss: 0.4719630479812622\n",
      "Subject 18, Epoch 628, Loss: 1.1272564828395844, Final Batch Loss: 0.31787487864494324\n",
      "Subject 18, Epoch 629, Loss: 1.1237232983112335, Final Batch Loss: 0.27459555864334106\n",
      "Subject 18, Epoch 630, Loss: 1.2090868055820465, Final Batch Loss: 0.36331412196159363\n",
      "Subject 18, Epoch 631, Loss: 1.2154761403799057, Final Batch Loss: 0.23854553699493408\n",
      "Subject 18, Epoch 632, Loss: 1.1067219227552414, Final Batch Loss: 0.17272911965847015\n",
      "Subject 18, Epoch 633, Loss: 1.0521094501018524, Final Batch Loss: 0.18273483216762543\n",
      "Subject 18, Epoch 634, Loss: 1.024198517203331, Final Batch Loss: 0.15736393630504608\n",
      "Subject 18, Epoch 635, Loss: 1.1775217950344086, Final Batch Loss: 0.3130260407924652\n",
      "Subject 18, Epoch 636, Loss: 1.0354164838790894, Final Batch Loss: 0.26403990387916565\n",
      "Subject 18, Epoch 637, Loss: 1.2186307311058044, Final Batch Loss: 0.3448401391506195\n",
      "Subject 18, Epoch 638, Loss: 1.439703419804573, Final Batch Loss: 0.4827097952365875\n",
      "Subject 18, Epoch 639, Loss: 1.2870320677757263, Final Batch Loss: 0.3563099801540375\n",
      "Subject 18, Epoch 640, Loss: 0.9660274088382721, Final Batch Loss: 0.12423285841941833\n",
      "Subject 18, Epoch 641, Loss: 1.0264544785022736, Final Batch Loss: 0.24802184104919434\n",
      "Subject 18, Epoch 642, Loss: 1.1713304966688156, Final Batch Loss: 0.31422850489616394\n",
      "Subject 18, Epoch 643, Loss: 0.9636376947164536, Final Batch Loss: 0.1320285052061081\n",
      "Subject 18, Epoch 644, Loss: 1.2970679253339767, Final Batch Loss: 0.4652853012084961\n",
      "Subject 18, Epoch 645, Loss: 1.147584855556488, Final Batch Loss: 0.2619735896587372\n",
      "Subject 18, Epoch 646, Loss: 1.1526389420032501, Final Batch Loss: 0.31244850158691406\n",
      "Subject 18, Epoch 647, Loss: 1.1672514975070953, Final Batch Loss: 0.31796348094940186\n",
      "Subject 18, Epoch 648, Loss: 1.2047596275806427, Final Batch Loss: 0.4020613729953766\n",
      "Subject 18, Epoch 649, Loss: 1.0682224333286285, Final Batch Loss: 0.31113681197166443\n",
      "Subject 18, Epoch 650, Loss: 1.277143508195877, Final Batch Loss: 0.45490726828575134\n",
      "Subject 18, Epoch 651, Loss: 1.021043673157692, Final Batch Loss: 0.17894409596920013\n",
      "Subject 18, Epoch 652, Loss: 1.1315748393535614, Final Batch Loss: 0.22329062223434448\n",
      "Subject 18, Epoch 653, Loss: 1.1484922617673874, Final Batch Loss: 0.2717801332473755\n",
      "Subject 18, Epoch 654, Loss: 1.0555786788463593, Final Batch Loss: 0.20056211948394775\n",
      "Subject 18, Epoch 655, Loss: 1.0183227956295013, Final Batch Loss: 0.23607927560806274\n",
      "Subject 18, Epoch 656, Loss: 1.2878956198692322, Final Batch Loss: 0.4374014437198639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 657, Loss: 1.2025572657585144, Final Batch Loss: 0.4005413353443146\n",
      "Subject 18, Epoch 658, Loss: 1.151569277048111, Final Batch Loss: 0.358250230550766\n",
      "Subject 18, Epoch 659, Loss: 1.1524564921855927, Final Batch Loss: 0.27477148175239563\n",
      "Subject 18, Epoch 660, Loss: 1.122898817062378, Final Batch Loss: 0.2127273678779602\n",
      "Subject 18, Epoch 661, Loss: 1.3341196179389954, Final Batch Loss: 0.5201950669288635\n",
      "Subject 18, Epoch 662, Loss: 1.0860540717840195, Final Batch Loss: 0.1454787701368332\n",
      "Subject 18, Epoch 663, Loss: 0.9745669811964035, Final Batch Loss: 0.15646283328533173\n",
      "Subject 18, Epoch 664, Loss: 1.2901369035243988, Final Batch Loss: 0.4516665041446686\n",
      "Subject 18, Epoch 665, Loss: 1.040820524096489, Final Batch Loss: 0.22219674289226532\n",
      "Subject 18, Epoch 666, Loss: 1.048253059387207, Final Batch Loss: 0.19751572608947754\n",
      "Subject 18, Epoch 667, Loss: 1.2343700379133224, Final Batch Loss: 0.3581123650074005\n",
      "Subject 18, Epoch 668, Loss: 1.2193463891744614, Final Batch Loss: 0.42915916442871094\n",
      "Subject 18, Epoch 669, Loss: 1.163239911198616, Final Batch Loss: 0.38784727454185486\n",
      "Subject 18, Epoch 670, Loss: 1.2395494282245636, Final Batch Loss: 0.3833404779434204\n",
      "Subject 18, Epoch 671, Loss: 1.136115401983261, Final Batch Loss: 0.3608863055706024\n",
      "Subject 18, Epoch 672, Loss: 1.0366136133670807, Final Batch Loss: 0.25644180178642273\n",
      "Subject 18, Epoch 673, Loss: 1.3105819821357727, Final Batch Loss: 0.46679189801216125\n",
      "Subject 18, Epoch 674, Loss: 1.056188553571701, Final Batch Loss: 0.2037113904953003\n",
      "Subject 18, Epoch 675, Loss: 1.0639860928058624, Final Batch Loss: 0.26254239678382874\n",
      "Subject 18, Epoch 676, Loss: 1.0371418446302414, Final Batch Loss: 0.14016281068325043\n",
      "Subject 18, Epoch 677, Loss: 1.0994466692209244, Final Batch Loss: 0.2808785140514374\n",
      "Subject 18, Epoch 678, Loss: 1.0259985327720642, Final Batch Loss: 0.18044213950634003\n",
      "Subject 18, Epoch 679, Loss: 0.9264762103557587, Final Batch Loss: 0.14035075902938843\n",
      "Subject 18, Epoch 680, Loss: 1.107580542564392, Final Batch Loss: 0.23775844275951385\n",
      "Subject 18, Epoch 681, Loss: 1.0873471945524216, Final Batch Loss: 0.26502636075019836\n",
      "Subject 18, Epoch 682, Loss: 1.1464449912309647, Final Batch Loss: 0.1972605437040329\n",
      "Subject 18, Epoch 683, Loss: 0.9972062259912491, Final Batch Loss: 0.21773429214954376\n",
      "Subject 18, Epoch 684, Loss: 1.136501669883728, Final Batch Loss: 0.28230777382850647\n",
      "Subject 18, Epoch 685, Loss: 1.1226100325584412, Final Batch Loss: 0.23169535398483276\n",
      "Subject 18, Epoch 686, Loss: 1.0862013101577759, Final Batch Loss: 0.23334145545959473\n",
      "Subject 18, Epoch 687, Loss: 1.0127120912075043, Final Batch Loss: 0.16408708691596985\n",
      "Subject 18, Epoch 688, Loss: 1.1639747619628906, Final Batch Loss: 0.24396109580993652\n",
      "Subject 18, Epoch 689, Loss: 1.0751933157444, Final Batch Loss: 0.2784484624862671\n",
      "Subject 18, Epoch 690, Loss: 1.1286396384239197, Final Batch Loss: 0.34404465556144714\n",
      "Subject 18, Epoch 691, Loss: 1.3080693483352661, Final Batch Loss: 0.38652899861335754\n",
      "Subject 18, Epoch 692, Loss: 0.9709562808275223, Final Batch Loss: 0.1639121025800705\n",
      "Subject 18, Epoch 693, Loss: 1.0570722222328186, Final Batch Loss: 0.189020574092865\n",
      "Subject 18, Epoch 694, Loss: 1.0127932131290436, Final Batch Loss: 0.22546613216400146\n",
      "Subject 18, Epoch 695, Loss: 1.0297060757875443, Final Batch Loss: 0.196105495095253\n",
      "Subject 18, Epoch 696, Loss: 0.9951562583446503, Final Batch Loss: 0.20368826389312744\n",
      "Subject 18, Epoch 697, Loss: 1.1765175312757492, Final Batch Loss: 0.37505042552948\n",
      "Subject 18, Epoch 698, Loss: 1.101612851023674, Final Batch Loss: 0.32140693068504333\n",
      "Subject 18, Epoch 699, Loss: 1.0545768737792969, Final Batch Loss: 0.22666583955287933\n",
      "Subject 18, Epoch 700, Loss: 1.1526481956243515, Final Batch Loss: 0.30984821915626526\n",
      "Subject 18, Epoch 701, Loss: 1.0860653221607208, Final Batch Loss: 0.29024019837379456\n",
      "Subject 18, Epoch 702, Loss: 1.1177941858768463, Final Batch Loss: 0.3175087869167328\n",
      "Subject 18, Epoch 703, Loss: 1.130641758441925, Final Batch Loss: 0.3214205801486969\n",
      "Subject 18, Epoch 704, Loss: 1.1868408024311066, Final Batch Loss: 0.21703416109085083\n",
      "Subject 18, Epoch 705, Loss: 1.1207440495491028, Final Batch Loss: 0.24186581373214722\n",
      "Subject 18, Epoch 706, Loss: 1.1681198328733444, Final Batch Loss: 0.29812654852867126\n",
      "Subject 18, Epoch 707, Loss: 1.0531366169452667, Final Batch Loss: 0.1776580959558487\n",
      "Subject 18, Epoch 708, Loss: 1.0526311248540878, Final Batch Loss: 0.27493852376937866\n",
      "Subject 18, Epoch 709, Loss: 1.095229834318161, Final Batch Loss: 0.2635258138179779\n",
      "Subject 18, Epoch 710, Loss: 1.1023240983486176, Final Batch Loss: 0.2444724440574646\n",
      "Subject 18, Epoch 711, Loss: 1.3315741121768951, Final Batch Loss: 0.39857614040374756\n",
      "Subject 18, Epoch 712, Loss: 1.0087349861860275, Final Batch Loss: 0.21722601354122162\n",
      "Subject 18, Epoch 713, Loss: 0.9458542466163635, Final Batch Loss: 0.19839130342006683\n",
      "Subject 18, Epoch 714, Loss: 1.1881754398345947, Final Batch Loss: 0.35329076647758484\n",
      "Subject 18, Epoch 715, Loss: 1.0700554996728897, Final Batch Loss: 0.3117668032646179\n",
      "Subject 18, Epoch 716, Loss: 1.0964765101671219, Final Batch Loss: 0.28304314613342285\n",
      "Subject 18, Epoch 717, Loss: 1.0617625415325165, Final Batch Loss: 0.2718748152256012\n",
      "Subject 18, Epoch 718, Loss: 1.2046317011117935, Final Batch Loss: 0.43446359038352966\n",
      "Subject 18, Epoch 719, Loss: 1.0105793923139572, Final Batch Loss: 0.21128928661346436\n",
      "Subject 18, Epoch 720, Loss: 1.0469761937856674, Final Batch Loss: 0.2721915543079376\n",
      "Subject 18, Epoch 721, Loss: 0.960602268576622, Final Batch Loss: 0.13821102678775787\n",
      "Subject 18, Epoch 722, Loss: 1.3195594251155853, Final Batch Loss: 0.4884050190448761\n",
      "Subject 18, Epoch 723, Loss: 0.9410264194011688, Final Batch Loss: 0.2321026474237442\n",
      "Subject 18, Epoch 724, Loss: 1.1829716861248016, Final Batch Loss: 0.4416452646255493\n",
      "Subject 18, Epoch 725, Loss: 1.0825892984867096, Final Batch Loss: 0.3526087999343872\n",
      "Subject 18, Epoch 726, Loss: 1.016002669930458, Final Batch Loss: 0.1857387274503708\n",
      "Subject 18, Epoch 727, Loss: 1.0723233819007874, Final Batch Loss: 0.2862936556339264\n",
      "Subject 18, Epoch 728, Loss: 1.110845372080803, Final Batch Loss: 0.2856065332889557\n",
      "Subject 18, Epoch 729, Loss: 1.1369040459394455, Final Batch Loss: 0.285289466381073\n",
      "Subject 18, Epoch 730, Loss: 0.982556626200676, Final Batch Loss: 0.22461259365081787\n",
      "Subject 18, Epoch 731, Loss: 0.9558156430721283, Final Batch Loss: 0.1953510046005249\n",
      "Subject 18, Epoch 732, Loss: 0.9826701134443283, Final Batch Loss: 0.19889114797115326\n",
      "Subject 18, Epoch 733, Loss: 0.9525561332702637, Final Batch Loss: 0.24096249043941498\n",
      "Subject 18, Epoch 734, Loss: 1.1640676856040955, Final Batch Loss: 0.39879098534584045\n",
      "Subject 18, Epoch 735, Loss: 1.0327999740839005, Final Batch Loss: 0.2820148766040802\n",
      "Subject 18, Epoch 736, Loss: 1.097652018070221, Final Batch Loss: 0.26752251386642456\n",
      "Subject 18, Epoch 737, Loss: 1.076905831694603, Final Batch Loss: 0.23306284844875336\n",
      "Subject 18, Epoch 738, Loss: 1.01740263402462, Final Batch Loss: 0.23115307092666626\n",
      "Subject 18, Epoch 739, Loss: 1.09142604470253, Final Batch Loss: 0.31624189019203186\n",
      "Subject 18, Epoch 740, Loss: 1.0243892818689346, Final Batch Loss: 0.21173201501369476\n",
      "Subject 18, Epoch 741, Loss: 1.2182215750217438, Final Batch Loss: 0.44900307059288025\n",
      "Subject 18, Epoch 742, Loss: 0.9506774097681046, Final Batch Loss: 0.24662815034389496\n",
      "Subject 18, Epoch 743, Loss: 1.2013887912034988, Final Batch Loss: 0.357904314994812\n",
      "Subject 18, Epoch 744, Loss: 1.0403952300548553, Final Batch Loss: 0.2535509765148163\n",
      "Subject 18, Epoch 745, Loss: 1.1623010486364365, Final Batch Loss: 0.3649432957172394\n",
      "Subject 18, Epoch 746, Loss: 1.0181547850370407, Final Batch Loss: 0.25360575318336487\n",
      "Subject 18, Epoch 747, Loss: 1.1216075271368027, Final Batch Loss: 0.36176905035972595\n",
      "Subject 18, Epoch 748, Loss: 1.1205705851316452, Final Batch Loss: 0.3306269645690918\n",
      "Subject 18, Epoch 749, Loss: 0.8571778014302254, Final Batch Loss: 0.10485676676034927\n",
      "Subject 18, Epoch 750, Loss: 1.0668163299560547, Final Batch Loss: 0.27778494358062744\n",
      "Subject 18, Epoch 751, Loss: 1.0771098583936691, Final Batch Loss: 0.2710786759853363\n",
      "Subject 18, Epoch 752, Loss: 0.9661411494016647, Final Batch Loss: 0.27593994140625\n",
      "Subject 18, Epoch 753, Loss: 0.9649104177951813, Final Batch Loss: 0.2495063990354538\n",
      "Subject 18, Epoch 754, Loss: 1.1160868406295776, Final Batch Loss: 0.32509496808052063\n",
      "Subject 18, Epoch 755, Loss: 0.9249750673770905, Final Batch Loss: 0.21618640422821045\n",
      "Subject 18, Epoch 756, Loss: 1.0172317326068878, Final Batch Loss: 0.2109161764383316\n",
      "Subject 18, Epoch 757, Loss: 0.9477890059351921, Final Batch Loss: 0.09898283332586288\n",
      "Subject 18, Epoch 758, Loss: 1.0862866938114166, Final Batch Loss: 0.2922818064689636\n",
      "Subject 18, Epoch 759, Loss: 1.0325579196214676, Final Batch Loss: 0.3293401300907135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 760, Loss: 1.0584405213594437, Final Batch Loss: 0.3495248556137085\n",
      "Subject 18, Epoch 761, Loss: 1.160729080438614, Final Batch Loss: 0.3343266546726227\n",
      "Subject 18, Epoch 762, Loss: 1.0380275398492813, Final Batch Loss: 0.3014790415763855\n",
      "Subject 18, Epoch 763, Loss: 0.9828935712575912, Final Batch Loss: 0.12603627145290375\n",
      "Subject 18, Epoch 764, Loss: 0.9720788449048996, Final Batch Loss: 0.214997336268425\n",
      "Subject 18, Epoch 765, Loss: 0.9547673463821411, Final Batch Loss: 0.23603199422359467\n",
      "Subject 18, Epoch 766, Loss: 1.0121382474899292, Final Batch Loss: 0.2269420623779297\n",
      "Subject 18, Epoch 767, Loss: 1.0950356721878052, Final Batch Loss: 0.2522395849227905\n",
      "Subject 18, Epoch 768, Loss: 1.256426528096199, Final Batch Loss: 0.4965149462223053\n",
      "Subject 18, Epoch 769, Loss: 1.0620318353176117, Final Batch Loss: 0.3246799409389496\n",
      "Subject 18, Epoch 770, Loss: 1.06999471783638, Final Batch Loss: 0.31875213980674744\n",
      "Subject 18, Epoch 771, Loss: 1.0355327725410461, Final Batch Loss: 0.3036592900753021\n",
      "Subject 18, Epoch 772, Loss: 0.9652862548828125, Final Batch Loss: 0.17669737339019775\n",
      "Subject 18, Epoch 773, Loss: 1.0990801900625229, Final Batch Loss: 0.3515797555446625\n",
      "Subject 18, Epoch 774, Loss: 1.099690780043602, Final Batch Loss: 0.38882604241371155\n",
      "Subject 18, Epoch 775, Loss: 0.8997094482183456, Final Batch Loss: 0.15192095935344696\n",
      "Subject 18, Epoch 776, Loss: 1.0631950050592422, Final Batch Loss: 0.3065919578075409\n",
      "Subject 18, Epoch 777, Loss: 1.0976212471723557, Final Batch Loss: 0.23058544099330902\n",
      "Subject 18, Epoch 778, Loss: 0.8300955295562744, Final Batch Loss: 0.18857479095458984\n",
      "Subject 18, Epoch 779, Loss: 1.080411672592163, Final Batch Loss: 0.287593275308609\n",
      "Subject 18, Epoch 780, Loss: 0.949742004275322, Final Batch Loss: 0.2700233459472656\n",
      "Subject 18, Epoch 781, Loss: 1.0158920139074326, Final Batch Loss: 0.19673417508602142\n",
      "Subject 18, Epoch 782, Loss: 1.1964644491672516, Final Batch Loss: 0.2703603506088257\n",
      "Subject 18, Epoch 783, Loss: 0.972597137093544, Final Batch Loss: 0.2232344001531601\n",
      "Subject 18, Epoch 784, Loss: 0.9386211335659027, Final Batch Loss: 0.22315527498722076\n",
      "Subject 18, Epoch 785, Loss: 1.0762091279029846, Final Batch Loss: 0.27371758222579956\n",
      "Subject 18, Epoch 786, Loss: 1.0820227414369583, Final Batch Loss: 0.2205754518508911\n",
      "Subject 18, Epoch 787, Loss: 0.9577847272157669, Final Batch Loss: 0.19860535860061646\n",
      "Subject 18, Epoch 788, Loss: 0.868240475654602, Final Batch Loss: 0.17940515279769897\n",
      "Subject 18, Epoch 789, Loss: 0.9442430883646011, Final Batch Loss: 0.17799504101276398\n",
      "Subject 18, Epoch 790, Loss: 0.9958696365356445, Final Batch Loss: 0.17272859811782837\n",
      "Subject 18, Epoch 791, Loss: 1.024390548467636, Final Batch Loss: 0.29747042059898376\n",
      "Subject 18, Epoch 792, Loss: 1.027980387210846, Final Batch Loss: 0.23748373985290527\n",
      "Subject 18, Epoch 793, Loss: 1.000928372144699, Final Batch Loss: 0.24289585649967194\n",
      "Subject 18, Epoch 794, Loss: 1.0254619270563126, Final Batch Loss: 0.23694656789302826\n",
      "Subject 18, Epoch 795, Loss: 0.9986472427845001, Final Batch Loss: 0.265080064535141\n",
      "Subject 18, Epoch 796, Loss: 1.1341715455055237, Final Batch Loss: 0.33962562680244446\n",
      "Subject 18, Epoch 797, Loss: 1.3972965478897095, Final Batch Loss: 0.6080577969551086\n",
      "Subject 18, Epoch 798, Loss: 1.2939824610948563, Final Batch Loss: 0.48467525839805603\n",
      "Subject 18, Epoch 799, Loss: 1.0353467017412186, Final Batch Loss: 0.2831410765647888\n",
      "Subject 18, Epoch 800, Loss: 1.018126830458641, Final Batch Loss: 0.23308221995830536\n",
      "Subject 18, Epoch 801, Loss: 0.9932149797677994, Final Batch Loss: 0.30432140827178955\n",
      "Subject 18, Epoch 802, Loss: 0.9927651435136795, Final Batch Loss: 0.2349301427602768\n",
      "Subject 18, Epoch 803, Loss: 0.9960634112358093, Final Batch Loss: 0.21155859529972076\n",
      "Subject 18, Epoch 804, Loss: 0.9874397069215775, Final Batch Loss: 0.15411873161792755\n",
      "Subject 18, Epoch 805, Loss: 1.092993512749672, Final Batch Loss: 0.3482332229614258\n",
      "Subject 18, Epoch 806, Loss: 1.1962208598852158, Final Batch Loss: 0.4535590708255768\n",
      "Subject 18, Epoch 807, Loss: 0.9563338905572891, Final Batch Loss: 0.18269900977611542\n",
      "Subject 18, Epoch 808, Loss: 1.1808804869651794, Final Batch Loss: 0.4264583885669708\n",
      "Subject 18, Epoch 809, Loss: 0.9954042285680771, Final Batch Loss: 0.17756648361682892\n",
      "Subject 18, Epoch 810, Loss: 1.022352322936058, Final Batch Loss: 0.20234894752502441\n",
      "Subject 18, Epoch 811, Loss: 1.0038599222898483, Final Batch Loss: 0.25432050228118896\n",
      "Subject 18, Epoch 812, Loss: 0.9569283276796341, Final Batch Loss: 0.2320733219385147\n",
      "Subject 18, Epoch 813, Loss: 1.1239292621612549, Final Batch Loss: 0.34175124764442444\n",
      "Subject 18, Epoch 814, Loss: 0.9588696509599686, Final Batch Loss: 0.26036685705184937\n",
      "Subject 18, Epoch 815, Loss: 0.9949516952037811, Final Batch Loss: 0.2813251316547394\n",
      "Subject 18, Epoch 816, Loss: 1.0688000917434692, Final Batch Loss: 0.28763076663017273\n",
      "Subject 18, Epoch 817, Loss: 0.8467656224966049, Final Batch Loss: 0.1412273347377777\n",
      "Subject 18, Epoch 818, Loss: 1.0160193592309952, Final Batch Loss: 0.2930130660533905\n",
      "Subject 18, Epoch 819, Loss: 1.0581229329109192, Final Batch Loss: 0.3999847173690796\n",
      "Subject 18, Epoch 820, Loss: 0.9263724982738495, Final Batch Loss: 0.16176719963550568\n",
      "Subject 18, Epoch 821, Loss: 0.9099078327417374, Final Batch Loss: 0.14318718016147614\n",
      "Subject 18, Epoch 822, Loss: 1.043377473950386, Final Batch Loss: 0.2487376183271408\n",
      "Subject 18, Epoch 823, Loss: 0.9520323723554611, Final Batch Loss: 0.2563560903072357\n",
      "Subject 18, Epoch 824, Loss: 0.8898273259401321, Final Batch Loss: 0.14734496176242828\n",
      "Subject 18, Epoch 825, Loss: 0.9944728314876556, Final Batch Loss: 0.24679780006408691\n",
      "Subject 18, Epoch 826, Loss: 0.9335502535104752, Final Batch Loss: 0.2827170789241791\n",
      "Subject 18, Epoch 827, Loss: 1.024370551109314, Final Batch Loss: 0.28757646679878235\n",
      "Subject 18, Epoch 828, Loss: 0.8402141034603119, Final Batch Loss: 0.1636601984500885\n",
      "Subject 18, Epoch 829, Loss: 0.9661175012588501, Final Batch Loss: 0.18003208935260773\n",
      "Subject 18, Epoch 830, Loss: 0.949026346206665, Final Batch Loss: 0.24652542173862457\n",
      "Subject 18, Epoch 831, Loss: 0.8621440827846527, Final Batch Loss: 0.16993348300457\n",
      "Subject 18, Epoch 832, Loss: 1.0108294636011124, Final Batch Loss: 0.2810921370983124\n",
      "Subject 18, Epoch 833, Loss: 1.1686468720436096, Final Batch Loss: 0.31122806668281555\n",
      "Subject 18, Epoch 834, Loss: 0.9301847219467163, Final Batch Loss: 0.25798454880714417\n",
      "Subject 18, Epoch 835, Loss: 1.0097649842500687, Final Batch Loss: 0.2730658948421478\n",
      "Subject 18, Epoch 836, Loss: 0.9105898290872574, Final Batch Loss: 0.26726409792900085\n",
      "Subject 18, Epoch 837, Loss: 0.9537126868963242, Final Batch Loss: 0.20228905975818634\n",
      "Subject 18, Epoch 838, Loss: 0.8971191942691803, Final Batch Loss: 0.2572381794452667\n",
      "Subject 18, Epoch 839, Loss: 1.0634519904851913, Final Batch Loss: 0.3056686818599701\n",
      "Subject 18, Epoch 840, Loss: 0.9589827060699463, Final Batch Loss: 0.2870757579803467\n",
      "Subject 18, Epoch 841, Loss: 1.2292101234197617, Final Batch Loss: 0.5380133390426636\n",
      "Subject 18, Epoch 842, Loss: 0.8239468485116959, Final Batch Loss: 0.15979360044002533\n",
      "Subject 18, Epoch 843, Loss: 0.8673031106591225, Final Batch Loss: 0.11798989027738571\n",
      "Subject 18, Epoch 844, Loss: 1.0474238395690918, Final Batch Loss: 0.36393818259239197\n",
      "Subject 18, Epoch 845, Loss: 0.9406704306602478, Final Batch Loss: 0.2967863976955414\n",
      "Subject 18, Epoch 846, Loss: 0.8158559054136276, Final Batch Loss: 0.14243076741695404\n",
      "Subject 18, Epoch 847, Loss: 0.8385038375854492, Final Batch Loss: 0.22732873260974884\n",
      "Subject 18, Epoch 848, Loss: 0.9097399562597275, Final Batch Loss: 0.1867091804742813\n",
      "Subject 18, Epoch 849, Loss: 0.9244956374168396, Final Batch Loss: 0.14087577164173126\n",
      "Subject 18, Epoch 850, Loss: 0.991106390953064, Final Batch Loss: 0.2142784744501114\n",
      "Subject 18, Epoch 851, Loss: 0.8201845586299896, Final Batch Loss: 0.10378074645996094\n",
      "Subject 18, Epoch 852, Loss: 1.0614746361970901, Final Batch Loss: 0.2182559221982956\n",
      "Subject 18, Epoch 853, Loss: 1.1202483624219894, Final Batch Loss: 0.3406837284564972\n",
      "Subject 18, Epoch 854, Loss: 0.8044392317533493, Final Batch Loss: 0.19618932902812958\n",
      "Subject 18, Epoch 855, Loss: 0.98660908639431, Final Batch Loss: 0.24877071380615234\n",
      "Subject 18, Epoch 856, Loss: 0.9246622622013092, Final Batch Loss: 0.2685869634151459\n",
      "Subject 18, Epoch 857, Loss: 0.9903919547796249, Final Batch Loss: 0.24914789199829102\n",
      "Subject 18, Epoch 858, Loss: 0.9360688328742981, Final Batch Loss: 0.1657339632511139\n",
      "Subject 18, Epoch 859, Loss: 0.9853717684745789, Final Batch Loss: 0.329314261674881\n",
      "Subject 18, Epoch 860, Loss: 0.9684313237667084, Final Batch Loss: 0.176863893866539\n",
      "Subject 18, Epoch 861, Loss: 1.0333858132362366, Final Batch Loss: 0.31206196546554565\n",
      "Subject 18, Epoch 862, Loss: 0.8197557777166367, Final Batch Loss: 0.25717806816101074\n",
      "Subject 18, Epoch 863, Loss: 0.7719833478331566, Final Batch Loss: 0.12178084999322891\n",
      "Subject 18, Epoch 864, Loss: 1.0177664309740067, Final Batch Loss: 0.2956351935863495\n",
      "Subject 18, Epoch 865, Loss: 0.8737599104642868, Final Batch Loss: 0.21051912009716034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 866, Loss: 0.97615647315979, Final Batch Loss: 0.3035213053226471\n",
      "Subject 18, Epoch 867, Loss: 1.0774786919355392, Final Batch Loss: 0.28571459650993347\n",
      "Subject 18, Epoch 868, Loss: 0.975305363535881, Final Batch Loss: 0.2102782279253006\n",
      "Subject 18, Epoch 869, Loss: 1.0321521759033203, Final Batch Loss: 0.26907774806022644\n",
      "Subject 18, Epoch 870, Loss: 0.8847889304161072, Final Batch Loss: 0.15146157145500183\n",
      "Subject 18, Epoch 871, Loss: 0.9574085921049118, Final Batch Loss: 0.2019089311361313\n",
      "Subject 18, Epoch 872, Loss: 0.9682737588882446, Final Batch Loss: 0.27679792046546936\n",
      "Subject 18, Epoch 873, Loss: 0.9175342917442322, Final Batch Loss: 0.21205943822860718\n",
      "Subject 18, Epoch 874, Loss: 0.9306459277868271, Final Batch Loss: 0.2308582067489624\n",
      "Subject 18, Epoch 875, Loss: 0.8878505825996399, Final Batch Loss: 0.17497336864471436\n",
      "Subject 18, Epoch 876, Loss: 1.0192745178937912, Final Batch Loss: 0.39208006858825684\n",
      "Subject 18, Epoch 877, Loss: 0.9548563212156296, Final Batch Loss: 0.2897084057331085\n",
      "Subject 18, Epoch 878, Loss: 0.9995650053024292, Final Batch Loss: 0.18149727582931519\n",
      "Subject 18, Epoch 879, Loss: 0.8131257444620132, Final Batch Loss: 0.24612368643283844\n",
      "Subject 18, Epoch 880, Loss: 0.9344581663608551, Final Batch Loss: 0.23123173415660858\n",
      "Subject 18, Epoch 881, Loss: 0.9802875220775604, Final Batch Loss: 0.22271965444087982\n",
      "Subject 18, Epoch 882, Loss: 0.9206278026103973, Final Batch Loss: 0.2728241980075836\n",
      "Subject 18, Epoch 883, Loss: 0.8892110139131546, Final Batch Loss: 0.1298745721578598\n",
      "Subject 18, Epoch 884, Loss: 0.9387691766023636, Final Batch Loss: 0.31596967577934265\n",
      "Subject 18, Epoch 885, Loss: 0.9772316217422485, Final Batch Loss: 0.317353755235672\n",
      "Subject 18, Epoch 886, Loss: 0.8513572514057159, Final Batch Loss: 0.23777557909488678\n",
      "Subject 18, Epoch 887, Loss: 0.928709477186203, Final Batch Loss: 0.2030118703842163\n",
      "Subject 18, Epoch 888, Loss: 0.8611352294683456, Final Batch Loss: 0.17751914262771606\n",
      "Subject 18, Epoch 889, Loss: 0.9321449548006058, Final Batch Loss: 0.16060689091682434\n",
      "Subject 18, Epoch 890, Loss: 0.9558422416448593, Final Batch Loss: 0.21685485541820526\n",
      "Subject 18, Epoch 891, Loss: 0.8479805141687393, Final Batch Loss: 0.13742849230766296\n",
      "Subject 18, Epoch 892, Loss: 0.8243669122457504, Final Batch Loss: 0.1557038128376007\n",
      "Subject 18, Epoch 893, Loss: 0.9383638203144073, Final Batch Loss: 0.24118421971797943\n",
      "Subject 18, Epoch 894, Loss: 0.9922727346420288, Final Batch Loss: 0.24724678695201874\n",
      "Subject 18, Epoch 895, Loss: 0.978338822722435, Final Batch Loss: 0.2678034007549286\n",
      "Subject 18, Epoch 896, Loss: 0.7631771229207516, Final Batch Loss: 0.045814838260412216\n",
      "Subject 18, Epoch 897, Loss: 0.8871906697750092, Final Batch Loss: 0.14663170278072357\n",
      "Subject 18, Epoch 898, Loss: 1.0403254926204681, Final Batch Loss: 0.25796017050743103\n",
      "Subject 18, Epoch 899, Loss: 0.9330793619155884, Final Batch Loss: 0.2120916247367859\n",
      "Subject 18, Epoch 900, Loss: 0.8537937551736832, Final Batch Loss: 0.1289910525083542\n",
      "Subject 18, Epoch 901, Loss: 0.899199515581131, Final Batch Loss: 0.22518949210643768\n",
      "Subject 18, Epoch 902, Loss: 0.8581946194171906, Final Batch Loss: 0.20066748559474945\n",
      "Subject 18, Epoch 903, Loss: 0.8529522866010666, Final Batch Loss: 0.14990676939487457\n",
      "Subject 18, Epoch 904, Loss: 0.8431028872728348, Final Batch Loss: 0.1866709142923355\n",
      "Subject 18, Epoch 905, Loss: 1.1076250970363617, Final Batch Loss: 0.19635672867298126\n",
      "Subject 18, Epoch 906, Loss: 1.0620785504579544, Final Batch Loss: 0.4506656229496002\n",
      "Subject 18, Epoch 907, Loss: 0.8537687510251999, Final Batch Loss: 0.15551134943962097\n",
      "Subject 18, Epoch 908, Loss: 0.9209174662828445, Final Batch Loss: 0.20555591583251953\n",
      "Subject 18, Epoch 909, Loss: 1.062220260500908, Final Batch Loss: 0.24490852653980255\n",
      "Subject 18, Epoch 910, Loss: 0.7946775406599045, Final Batch Loss: 0.23714029788970947\n",
      "Subject 18, Epoch 911, Loss: 0.7869972959160805, Final Batch Loss: 0.07449924200773239\n",
      "Subject 18, Epoch 912, Loss: 0.851592093706131, Final Batch Loss: 0.1799602061510086\n",
      "Subject 18, Epoch 913, Loss: 0.8471270352602005, Final Batch Loss: 0.18814808130264282\n",
      "Subject 18, Epoch 914, Loss: 0.9954601526260376, Final Batch Loss: 0.3485470116138458\n",
      "Subject 18, Epoch 915, Loss: 0.8271192908287048, Final Batch Loss: 0.15713387727737427\n",
      "Subject 18, Epoch 916, Loss: 0.9532696306705475, Final Batch Loss: 0.228487029671669\n",
      "Subject 18, Epoch 917, Loss: 1.0386481881141663, Final Batch Loss: 0.38574472069740295\n",
      "Subject 18, Epoch 918, Loss: 0.9297950565814972, Final Batch Loss: 0.29771754145622253\n",
      "Subject 18, Epoch 919, Loss: 0.8284769058227539, Final Batch Loss: 0.1661446988582611\n",
      "Subject 18, Epoch 920, Loss: 0.8140579164028168, Final Batch Loss: 0.21527545154094696\n",
      "Subject 18, Epoch 921, Loss: 0.8869724273681641, Final Batch Loss: 0.18570782244205475\n",
      "Subject 18, Epoch 922, Loss: 0.8718535304069519, Final Batch Loss: 0.15510469675064087\n",
      "Subject 18, Epoch 923, Loss: 0.8611591458320618, Final Batch Loss: 0.2586784362792969\n",
      "Subject 18, Epoch 924, Loss: 0.9732905775308609, Final Batch Loss: 0.32150009274482727\n",
      "Subject 18, Epoch 925, Loss: 0.8644586205482483, Final Batch Loss: 0.1815851777791977\n",
      "Subject 18, Epoch 926, Loss: 0.921750858426094, Final Batch Loss: 0.23066295683383942\n",
      "Subject 18, Epoch 927, Loss: 0.9207422286272049, Final Batch Loss: 0.24878071248531342\n",
      "Subject 18, Epoch 928, Loss: 0.8751239329576492, Final Batch Loss: 0.21541447937488556\n",
      "Subject 18, Epoch 929, Loss: 0.8471514135599136, Final Batch Loss: 0.15906184911727905\n",
      "Subject 18, Epoch 930, Loss: 0.9933546483516693, Final Batch Loss: 0.3251037299633026\n",
      "Subject 18, Epoch 931, Loss: 0.801935501396656, Final Batch Loss: 0.11092431098222733\n",
      "Subject 18, Epoch 932, Loss: 0.9176436960697174, Final Batch Loss: 0.22520571947097778\n",
      "Subject 18, Epoch 933, Loss: 1.0099026411771774, Final Batch Loss: 0.36095190048217773\n",
      "Subject 18, Epoch 934, Loss: 0.8687188923358917, Final Batch Loss: 0.08289109170436859\n",
      "Subject 18, Epoch 935, Loss: 0.8821189105510712, Final Batch Loss: 0.17073990404605865\n",
      "Subject 18, Epoch 936, Loss: 0.8054895251989365, Final Batch Loss: 0.17794089019298553\n",
      "Subject 18, Epoch 937, Loss: 0.8206848502159119, Final Batch Loss: 0.16270001232624054\n",
      "Subject 18, Epoch 938, Loss: 0.7975839599967003, Final Batch Loss: 0.1094977930188179\n",
      "Subject 18, Epoch 939, Loss: 0.8544430509209633, Final Batch Loss: 0.0925581231713295\n",
      "Subject 18, Epoch 940, Loss: 0.818313866853714, Final Batch Loss: 0.17982757091522217\n",
      "Subject 18, Epoch 941, Loss: 0.8987414687871933, Final Batch Loss: 0.246073916554451\n",
      "Subject 18, Epoch 942, Loss: 0.7875908315181732, Final Batch Loss: 0.16495540738105774\n",
      "Subject 18, Epoch 943, Loss: 1.02757066488266, Final Batch Loss: 0.3180095851421356\n",
      "Subject 18, Epoch 944, Loss: 0.9393051564693451, Final Batch Loss: 0.33087286353111267\n",
      "Subject 18, Epoch 945, Loss: 1.1218712478876114, Final Batch Loss: 0.309683620929718\n",
      "Subject 18, Epoch 946, Loss: 0.9628854244947433, Final Batch Loss: 0.23646485805511475\n",
      "Subject 18, Epoch 947, Loss: 0.8132605850696564, Final Batch Loss: 0.1571558564901352\n",
      "Subject 18, Epoch 948, Loss: 0.8378655165433884, Final Batch Loss: 0.15198612213134766\n",
      "Subject 18, Epoch 949, Loss: 0.8335666581988335, Final Batch Loss: 0.09535741060972214\n",
      "Subject 18, Epoch 950, Loss: 0.8579728007316589, Final Batch Loss: 0.20002847909927368\n",
      "Subject 18, Epoch 951, Loss: 0.9225938022136688, Final Batch Loss: 0.3809352219104767\n",
      "Subject 18, Epoch 952, Loss: 0.8870015442371368, Final Batch Loss: 0.20540110766887665\n",
      "Subject 18, Epoch 953, Loss: 0.8690776824951172, Final Batch Loss: 0.2082526683807373\n",
      "Subject 18, Epoch 954, Loss: 0.9356797784566879, Final Batch Loss: 0.20537938177585602\n",
      "Subject 18, Epoch 955, Loss: 0.8926457464694977, Final Batch Loss: 0.23801691830158234\n",
      "Subject 18, Epoch 956, Loss: 0.9096030443906784, Final Batch Loss: 0.2567570209503174\n",
      "Subject 18, Epoch 957, Loss: 0.7205967158079147, Final Batch Loss: 0.179579496383667\n",
      "Subject 18, Epoch 958, Loss: 0.7422819957137108, Final Batch Loss: 0.1054404154419899\n",
      "Subject 18, Epoch 959, Loss: 0.9581308364868164, Final Batch Loss: 0.3476116359233856\n",
      "Subject 18, Epoch 960, Loss: 0.8311999440193176, Final Batch Loss: 0.2241223305463791\n",
      "Subject 18, Epoch 961, Loss: 0.924868032336235, Final Batch Loss: 0.30553629994392395\n",
      "Subject 18, Epoch 962, Loss: 0.878152385354042, Final Batch Loss: 0.2834402024745941\n",
      "Subject 18, Epoch 963, Loss: 0.8608897179365158, Final Batch Loss: 0.132029190659523\n",
      "Subject 18, Epoch 964, Loss: 0.7705817967653275, Final Batch Loss: 0.1272651106119156\n",
      "Subject 18, Epoch 965, Loss: 0.7274771630764008, Final Batch Loss: 0.1226111352443695\n",
      "Subject 18, Epoch 966, Loss: 0.9454818218946457, Final Batch Loss: 0.2491730898618698\n",
      "Subject 18, Epoch 967, Loss: 0.9488217681646347, Final Batch Loss: 0.20624065399169922\n",
      "Subject 18, Epoch 968, Loss: 0.7621157467365265, Final Batch Loss: 0.1266184151172638\n",
      "Subject 18, Epoch 969, Loss: 0.7892835289239883, Final Batch Loss: 0.12579725682735443\n",
      "Subject 18, Epoch 970, Loss: 1.052697330713272, Final Batch Loss: 0.4034685790538788\n",
      "Subject 18, Epoch 971, Loss: 0.9141697883605957, Final Batch Loss: 0.3008521497249603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18, Epoch 972, Loss: 0.8335200250148773, Final Batch Loss: 0.13594837486743927\n",
      "Subject 18, Epoch 973, Loss: 1.1022485941648483, Final Batch Loss: 0.4387832581996918\n",
      "Subject 18, Epoch 974, Loss: 0.9162413775920868, Final Batch Loss: 0.2749767601490021\n",
      "Subject 18, Epoch 975, Loss: 0.7509145140647888, Final Batch Loss: 0.1375398337841034\n",
      "Subject 18, Epoch 976, Loss: 0.7141327261924744, Final Batch Loss: 0.14117294549942017\n",
      "Subject 18, Epoch 977, Loss: 0.7683541625738144, Final Batch Loss: 0.13408015668392181\n",
      "Subject 18, Epoch 978, Loss: 0.8776749148964882, Final Batch Loss: 0.31254464387893677\n",
      "Subject 18, Epoch 979, Loss: 0.7464792877435684, Final Batch Loss: 0.19045865535736084\n",
      "Subject 18, Epoch 980, Loss: 0.8810672163963318, Final Batch Loss: 0.20663638412952423\n",
      "Subject 18, Epoch 981, Loss: 0.9983795434236526, Final Batch Loss: 0.16644705832004547\n",
      "Subject 18, Epoch 982, Loss: 0.9860918670892715, Final Batch Loss: 0.2926643192768097\n",
      "Subject 18, Epoch 983, Loss: 0.8551542311906815, Final Batch Loss: 0.18440401554107666\n",
      "Subject 18, Epoch 984, Loss: 1.0904462337493896, Final Batch Loss: 0.3270176351070404\n",
      "Subject 18, Epoch 985, Loss: 0.9100659042596817, Final Batch Loss: 0.34697067737579346\n",
      "Subject 18, Epoch 986, Loss: 0.9281040281057358, Final Batch Loss: 0.28018561005592346\n",
      "Subject 18, Epoch 987, Loss: 0.9449509084224701, Final Batch Loss: 0.3308209776878357\n",
      "Subject 18, Epoch 988, Loss: 0.8366475999355316, Final Batch Loss: 0.24543273448944092\n",
      "Subject 18, Epoch 989, Loss: 0.7620446979999542, Final Batch Loss: 0.2056557685136795\n",
      "Subject 18, Epoch 990, Loss: 0.9307675808668137, Final Batch Loss: 0.3231309652328491\n",
      "Subject 18, Epoch 991, Loss: 0.8114454224705696, Final Batch Loss: 0.10409928113222122\n",
      "Subject 18, Epoch 992, Loss: 0.8815851211547852, Final Batch Loss: 0.19884788990020752\n",
      "Subject 18, Epoch 993, Loss: 0.8565355837345123, Final Batch Loss: 0.24844717979431152\n",
      "Subject 18, Epoch 994, Loss: 0.9405392408370972, Final Batch Loss: 0.2730322480201721\n",
      "Subject 18, Epoch 995, Loss: 0.7161488309502602, Final Batch Loss: 0.09297915548086166\n",
      "Subject 18, Epoch 996, Loss: 0.8678274303674698, Final Batch Loss: 0.21770818531513214\n",
      "Subject 18, Epoch 997, Loss: 0.9555325508117676, Final Batch Loss: 0.32992497086524963\n",
      "Subject 18, Epoch 998, Loss: 0.8412490040063858, Final Batch Loss: 0.2947879135608673\n",
      "Subject 18, Epoch 999, Loss: 1.0425793379545212, Final Batch Loss: 0.3717621862888336\n",
      "Subject 18, Epoch 1000, Loss: 0.7100418694317341, Final Batch Loss: 0.05058731511235237\n",
      "Subject 19, Epoch 1, Loss: 5.424984931945801, Final Batch Loss: 1.794095754623413\n",
      "Subject 19, Epoch 2, Loss: 5.419429540634155, Final Batch Loss: 1.819982886314392\n",
      "Subject 19, Epoch 3, Loss: 5.406931757926941, Final Batch Loss: 1.781261920928955\n",
      "Subject 19, Epoch 4, Loss: 5.4133031368255615, Final Batch Loss: 1.8434464931488037\n",
      "Subject 19, Epoch 5, Loss: 5.399875998497009, Final Batch Loss: 1.7919789552688599\n",
      "Subject 19, Epoch 6, Loss: 5.384775876998901, Final Batch Loss: 1.7810609340667725\n",
      "Subject 19, Epoch 7, Loss: 5.362298011779785, Final Batch Loss: 1.7692307233810425\n",
      "Subject 19, Epoch 8, Loss: 5.354711055755615, Final Batch Loss: 1.7776917219161987\n",
      "Subject 19, Epoch 9, Loss: 5.344076156616211, Final Batch Loss: 1.7794002294540405\n",
      "Subject 19, Epoch 10, Loss: 5.321892142295837, Final Batch Loss: 1.758313536643982\n",
      "Subject 19, Epoch 11, Loss: 5.297504186630249, Final Batch Loss: 1.7936071157455444\n",
      "Subject 19, Epoch 12, Loss: 5.270564913749695, Final Batch Loss: 1.740269660949707\n",
      "Subject 19, Epoch 13, Loss: 5.241210579872131, Final Batch Loss: 1.7422367334365845\n",
      "Subject 19, Epoch 14, Loss: 5.194311022758484, Final Batch Loss: 1.7298939228057861\n",
      "Subject 19, Epoch 15, Loss: 5.169739246368408, Final Batch Loss: 1.7204616069793701\n",
      "Subject 19, Epoch 16, Loss: 5.089555501937866, Final Batch Loss: 1.667483925819397\n",
      "Subject 19, Epoch 17, Loss: 5.018993496894836, Final Batch Loss: 1.6402727365493774\n",
      "Subject 19, Epoch 18, Loss: 4.959679365158081, Final Batch Loss: 1.6191902160644531\n",
      "Subject 19, Epoch 19, Loss: 4.961481809616089, Final Batch Loss: 1.6559720039367676\n",
      "Subject 19, Epoch 20, Loss: 4.836961507797241, Final Batch Loss: 1.6675865650177002\n",
      "Subject 19, Epoch 21, Loss: 4.797845125198364, Final Batch Loss: 1.6311182975769043\n",
      "Subject 19, Epoch 22, Loss: 4.6913594007492065, Final Batch Loss: 1.5815942287445068\n",
      "Subject 19, Epoch 23, Loss: 4.662278413772583, Final Batch Loss: 1.5221028327941895\n",
      "Subject 19, Epoch 24, Loss: 4.572561144828796, Final Batch Loss: 1.5723254680633545\n",
      "Subject 19, Epoch 25, Loss: 4.562450647354126, Final Batch Loss: 1.5268007516860962\n",
      "Subject 19, Epoch 26, Loss: 4.508922576904297, Final Batch Loss: 1.4910393953323364\n",
      "Subject 19, Epoch 27, Loss: 4.407012939453125, Final Batch Loss: 1.4095923900604248\n",
      "Subject 19, Epoch 28, Loss: 4.336743593215942, Final Batch Loss: 1.392889380455017\n",
      "Subject 19, Epoch 29, Loss: 4.207913041114807, Final Batch Loss: 1.3959028720855713\n",
      "Subject 19, Epoch 30, Loss: 4.227597713470459, Final Batch Loss: 1.3722918033599854\n",
      "Subject 19, Epoch 31, Loss: 4.108854413032532, Final Batch Loss: 1.401861310005188\n",
      "Subject 19, Epoch 32, Loss: 4.053610444068909, Final Batch Loss: 1.4455957412719727\n",
      "Subject 19, Epoch 33, Loss: 4.040220022201538, Final Batch Loss: 1.341526985168457\n",
      "Subject 19, Epoch 34, Loss: 3.8812849521636963, Final Batch Loss: 1.3253328800201416\n",
      "Subject 19, Epoch 35, Loss: 3.8296271562576294, Final Batch Loss: 1.2609487771987915\n",
      "Subject 19, Epoch 36, Loss: 3.828439950942993, Final Batch Loss: 1.2281244993209839\n",
      "Subject 19, Epoch 37, Loss: 3.800610899925232, Final Batch Loss: 1.3400782346725464\n",
      "Subject 19, Epoch 38, Loss: 3.667561888694763, Final Batch Loss: 1.2353715896606445\n",
      "Subject 19, Epoch 39, Loss: 3.670261859893799, Final Batch Loss: 1.1856857538223267\n",
      "Subject 19, Epoch 40, Loss: 3.6210174560546875, Final Batch Loss: 1.1631419658660889\n",
      "Subject 19, Epoch 41, Loss: 3.5532747507095337, Final Batch Loss: 1.1920952796936035\n",
      "Subject 19, Epoch 42, Loss: 3.568011522293091, Final Batch Loss: 1.170209527015686\n",
      "Subject 19, Epoch 43, Loss: 3.4768364429473877, Final Batch Loss: 1.097762107849121\n",
      "Subject 19, Epoch 44, Loss: 3.4585068225860596, Final Batch Loss: 1.1263363361358643\n",
      "Subject 19, Epoch 45, Loss: 3.4575438499450684, Final Batch Loss: 1.1645333766937256\n",
      "Subject 19, Epoch 46, Loss: 3.438730478286743, Final Batch Loss: 1.1571369171142578\n",
      "Subject 19, Epoch 47, Loss: 3.3420170545578003, Final Batch Loss: 1.0230220556259155\n",
      "Subject 19, Epoch 48, Loss: 3.2794952392578125, Final Batch Loss: 1.0744432210922241\n",
      "Subject 19, Epoch 49, Loss: 3.2801761627197266, Final Batch Loss: 1.0909438133239746\n",
      "Subject 19, Epoch 50, Loss: 3.2933586835861206, Final Batch Loss: 1.0642898082733154\n",
      "Subject 19, Epoch 51, Loss: 3.2788796424865723, Final Batch Loss: 1.105513095855713\n",
      "Subject 19, Epoch 52, Loss: 3.15878164768219, Final Batch Loss: 1.0699199438095093\n",
      "Subject 19, Epoch 53, Loss: 3.1420488357543945, Final Batch Loss: 1.024001121520996\n",
      "Subject 19, Epoch 54, Loss: 3.1835063695907593, Final Batch Loss: 1.0228252410888672\n",
      "Subject 19, Epoch 55, Loss: 3.282681107521057, Final Batch Loss: 1.1686569452285767\n",
      "Subject 19, Epoch 56, Loss: 3.15149462223053, Final Batch Loss: 1.0947126150131226\n",
      "Subject 19, Epoch 57, Loss: 3.181085467338562, Final Batch Loss: 1.0031609535217285\n",
      "Subject 19, Epoch 58, Loss: 3.075637638568878, Final Batch Loss: 1.06010901927948\n",
      "Subject 19, Epoch 59, Loss: 3.030647873878479, Final Batch Loss: 1.007981538772583\n",
      "Subject 19, Epoch 60, Loss: 2.956880748271942, Final Batch Loss: 1.0071007013320923\n",
      "Subject 19, Epoch 61, Loss: 2.964203178882599, Final Batch Loss: 1.0073162317276\n",
      "Subject 19, Epoch 62, Loss: 2.9171698093414307, Final Batch Loss: 0.9551813006401062\n",
      "Subject 19, Epoch 63, Loss: 2.8555542826652527, Final Batch Loss: 0.9264093637466431\n",
      "Subject 19, Epoch 64, Loss: 2.8638463616371155, Final Batch Loss: 0.9108127355575562\n",
      "Subject 19, Epoch 65, Loss: 2.7803433537483215, Final Batch Loss: 0.9139671921730042\n",
      "Subject 19, Epoch 66, Loss: 2.769422471523285, Final Batch Loss: 0.8683280944824219\n",
      "Subject 19, Epoch 67, Loss: 2.864531695842743, Final Batch Loss: 0.9768473505973816\n",
      "Subject 19, Epoch 68, Loss: 2.772552013397217, Final Batch Loss: 0.9239656925201416\n",
      "Subject 19, Epoch 69, Loss: 2.836757183074951, Final Batch Loss: 0.9564716219902039\n",
      "Subject 19, Epoch 70, Loss: 2.599728763103485, Final Batch Loss: 0.8817452192306519\n",
      "Subject 19, Epoch 71, Loss: 2.602754533290863, Final Batch Loss: 0.8601125478744507\n",
      "Subject 19, Epoch 72, Loss: 2.654481828212738, Final Batch Loss: 0.910688579082489\n",
      "Subject 19, Epoch 73, Loss: 2.662199079990387, Final Batch Loss: 0.8798309564590454\n",
      "Subject 19, Epoch 74, Loss: 2.6868881583213806, Final Batch Loss: 0.8515008091926575\n",
      "Subject 19, Epoch 75, Loss: 2.568776309490204, Final Batch Loss: 0.8172117471694946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 76, Loss: 2.543252468109131, Final Batch Loss: 0.8484060168266296\n",
      "Subject 19, Epoch 77, Loss: 2.441224992275238, Final Batch Loss: 0.8308475613594055\n",
      "Subject 19, Epoch 78, Loss: 2.5508565306663513, Final Batch Loss: 0.889396607875824\n",
      "Subject 19, Epoch 79, Loss: 2.4737660884857178, Final Batch Loss: 0.8176500201225281\n",
      "Subject 19, Epoch 80, Loss: 2.470659554004669, Final Batch Loss: 0.8305214643478394\n",
      "Subject 19, Epoch 81, Loss: 2.424779772758484, Final Batch Loss: 0.8458481431007385\n",
      "Subject 19, Epoch 82, Loss: 2.411615550518036, Final Batch Loss: 0.7773793339729309\n",
      "Subject 19, Epoch 83, Loss: 2.392176389694214, Final Batch Loss: 0.8234519362449646\n",
      "Subject 19, Epoch 84, Loss: 2.3870421648025513, Final Batch Loss: 0.7825251221656799\n",
      "Subject 19, Epoch 85, Loss: 2.4546656608581543, Final Batch Loss: 0.8598935604095459\n",
      "Subject 19, Epoch 86, Loss: 2.3526397943496704, Final Batch Loss: 0.7180182337760925\n",
      "Subject 19, Epoch 87, Loss: 2.433474123477936, Final Batch Loss: 0.8220831751823425\n",
      "Subject 19, Epoch 88, Loss: 2.3202285766601562, Final Batch Loss: 0.8397437334060669\n",
      "Subject 19, Epoch 89, Loss: 2.354869067668915, Final Batch Loss: 0.7887872457504272\n",
      "Subject 19, Epoch 90, Loss: 2.326729714870453, Final Batch Loss: 0.7579770088195801\n",
      "Subject 19, Epoch 91, Loss: 2.3083059191703796, Final Batch Loss: 0.7586341500282288\n",
      "Subject 19, Epoch 92, Loss: 2.322725534439087, Final Batch Loss: 0.7731548547744751\n",
      "Subject 19, Epoch 93, Loss: 2.242005467414856, Final Batch Loss: 0.713752806186676\n",
      "Subject 19, Epoch 94, Loss: 2.2536279559135437, Final Batch Loss: 0.7357555627822876\n",
      "Subject 19, Epoch 95, Loss: 2.2173608541488647, Final Batch Loss: 0.7970350384712219\n",
      "Subject 19, Epoch 96, Loss: 2.286853075027466, Final Batch Loss: 0.872230589389801\n",
      "Subject 19, Epoch 97, Loss: 2.1914783716201782, Final Batch Loss: 0.6952061653137207\n",
      "Subject 19, Epoch 98, Loss: 2.1874828934669495, Final Batch Loss: 0.6878729462623596\n",
      "Subject 19, Epoch 99, Loss: 2.1679022908210754, Final Batch Loss: 0.7367478013038635\n",
      "Subject 19, Epoch 100, Loss: 2.1167280077934265, Final Batch Loss: 0.7507640719413757\n",
      "Subject 19, Epoch 101, Loss: 2.1029393672943115, Final Batch Loss: 0.6149047613143921\n",
      "Subject 19, Epoch 102, Loss: 2.067255437374115, Final Batch Loss: 0.7220624089241028\n",
      "Subject 19, Epoch 103, Loss: 2.1234254837036133, Final Batch Loss: 0.6784912943840027\n",
      "Subject 19, Epoch 104, Loss: 2.155086934566498, Final Batch Loss: 0.6387903094291687\n",
      "Subject 19, Epoch 105, Loss: 2.182988464832306, Final Batch Loss: 0.7802649736404419\n",
      "Subject 19, Epoch 106, Loss: 2.067684292793274, Final Batch Loss: 0.664527177810669\n",
      "Subject 19, Epoch 107, Loss: 2.0109118223190308, Final Batch Loss: 0.6743369698524475\n",
      "Subject 19, Epoch 108, Loss: 2.1552746891975403, Final Batch Loss: 0.679320752620697\n",
      "Subject 19, Epoch 109, Loss: 2.0614078044891357, Final Batch Loss: 0.6891781091690063\n",
      "Subject 19, Epoch 110, Loss: 2.0421478152275085, Final Batch Loss: 0.6958875060081482\n",
      "Subject 19, Epoch 111, Loss: 1.931144893169403, Final Batch Loss: 0.5990206003189087\n",
      "Subject 19, Epoch 112, Loss: 1.9734548926353455, Final Batch Loss: 0.5861323475837708\n",
      "Subject 19, Epoch 113, Loss: 1.9018163084983826, Final Batch Loss: 0.6799382567405701\n",
      "Subject 19, Epoch 114, Loss: 1.8981961011886597, Final Batch Loss: 0.6277952790260315\n",
      "Subject 19, Epoch 115, Loss: 2.0918306708335876, Final Batch Loss: 0.7676078677177429\n",
      "Subject 19, Epoch 116, Loss: 1.9502760171890259, Final Batch Loss: 0.7102927565574646\n",
      "Subject 19, Epoch 117, Loss: 2.011324107646942, Final Batch Loss: 0.5887506604194641\n",
      "Subject 19, Epoch 118, Loss: 1.8943873047828674, Final Batch Loss: 0.6316489577293396\n",
      "Subject 19, Epoch 119, Loss: 1.9439568519592285, Final Batch Loss: 0.5995652675628662\n",
      "Subject 19, Epoch 120, Loss: 1.7697854042053223, Final Batch Loss: 0.5771118402481079\n",
      "Subject 19, Epoch 121, Loss: 1.8908280730247498, Final Batch Loss: 0.6339548230171204\n",
      "Subject 19, Epoch 122, Loss: 1.838063359260559, Final Batch Loss: 0.6223616003990173\n",
      "Subject 19, Epoch 123, Loss: 1.952143371105194, Final Batch Loss: 0.7113367319107056\n",
      "Subject 19, Epoch 124, Loss: 1.9219208359718323, Final Batch Loss: 0.6300513744354248\n",
      "Subject 19, Epoch 125, Loss: 1.8486100435256958, Final Batch Loss: 0.5226280093193054\n",
      "Subject 19, Epoch 126, Loss: 1.7556723952293396, Final Batch Loss: 0.6174564957618713\n",
      "Subject 19, Epoch 127, Loss: 1.7483478784561157, Final Batch Loss: 0.5824066400527954\n",
      "Subject 19, Epoch 128, Loss: 1.77639502286911, Final Batch Loss: 0.6343995928764343\n",
      "Subject 19, Epoch 129, Loss: 1.7052633166313171, Final Batch Loss: 0.5330819487571716\n",
      "Subject 19, Epoch 130, Loss: 1.716217190027237, Final Batch Loss: 0.5494527220726013\n",
      "Subject 19, Epoch 131, Loss: 1.8019424080848694, Final Batch Loss: 0.6396885514259338\n",
      "Subject 19, Epoch 132, Loss: 1.6964613497257233, Final Batch Loss: 0.6039878726005554\n",
      "Subject 19, Epoch 133, Loss: 1.7468854784965515, Final Batch Loss: 0.6422069668769836\n",
      "Subject 19, Epoch 134, Loss: 1.67996484041214, Final Batch Loss: 0.512896716594696\n",
      "Subject 19, Epoch 135, Loss: 1.6404410004615784, Final Batch Loss: 0.5702657103538513\n",
      "Subject 19, Epoch 136, Loss: 1.6832927465438843, Final Batch Loss: 0.5144988894462585\n",
      "Subject 19, Epoch 137, Loss: 1.5704327523708344, Final Batch Loss: 0.49977627396583557\n",
      "Subject 19, Epoch 138, Loss: 1.6068114042282104, Final Batch Loss: 0.6038684248924255\n",
      "Subject 19, Epoch 139, Loss: 1.5797887444496155, Final Batch Loss: 0.5122594833374023\n",
      "Subject 19, Epoch 140, Loss: 1.5960862636566162, Final Batch Loss: 0.5276037454605103\n",
      "Subject 19, Epoch 141, Loss: 1.6005502343177795, Final Batch Loss: 0.5691695213317871\n",
      "Subject 19, Epoch 142, Loss: 1.5032908618450165, Final Batch Loss: 0.43065083026885986\n",
      "Subject 19, Epoch 143, Loss: 1.7432613372802734, Final Batch Loss: 0.5459414720535278\n",
      "Subject 19, Epoch 144, Loss: 1.5598906576633453, Final Batch Loss: 0.5128531455993652\n",
      "Subject 19, Epoch 145, Loss: 1.6309695839881897, Final Batch Loss: 0.5105512738227844\n",
      "Subject 19, Epoch 146, Loss: 1.5740443766117096, Final Batch Loss: 0.5900338888168335\n",
      "Subject 19, Epoch 147, Loss: 1.557510107755661, Final Batch Loss: 0.44960543513298035\n",
      "Subject 19, Epoch 148, Loss: 1.5854218006134033, Final Batch Loss: 0.5085614919662476\n",
      "Subject 19, Epoch 149, Loss: 1.6100832223892212, Final Batch Loss: 0.5491032600402832\n",
      "Subject 19, Epoch 150, Loss: 1.4931471943855286, Final Batch Loss: 0.478374719619751\n",
      "Subject 19, Epoch 151, Loss: 1.586412489414215, Final Batch Loss: 0.5600387454032898\n",
      "Subject 19, Epoch 152, Loss: 1.4620460867881775, Final Batch Loss: 0.4123581647872925\n",
      "Subject 19, Epoch 153, Loss: 1.4569153189659119, Final Batch Loss: 0.448922336101532\n",
      "Subject 19, Epoch 154, Loss: 1.5002292096614838, Final Batch Loss: 0.5127526521682739\n",
      "Subject 19, Epoch 155, Loss: 1.517746478319168, Final Batch Loss: 0.4946976900100708\n",
      "Subject 19, Epoch 156, Loss: 1.5571246147155762, Final Batch Loss: 0.5544446110725403\n",
      "Subject 19, Epoch 157, Loss: 1.5897815525531769, Final Batch Loss: 0.4886470139026642\n",
      "Subject 19, Epoch 158, Loss: 1.45591139793396, Final Batch Loss: 0.4095506966114044\n",
      "Subject 19, Epoch 159, Loss: 1.3663196563720703, Final Batch Loss: 0.3637789785861969\n",
      "Subject 19, Epoch 160, Loss: 1.4470328986644745, Final Batch Loss: 0.4631001949310303\n",
      "Subject 19, Epoch 161, Loss: 1.2516591250896454, Final Batch Loss: 0.4178540110588074\n",
      "Subject 19, Epoch 162, Loss: 1.3799362182617188, Final Batch Loss: 0.4462753236293793\n",
      "Subject 19, Epoch 163, Loss: 1.486139178276062, Final Batch Loss: 0.544157087802887\n",
      "Subject 19, Epoch 164, Loss: 1.5203179717063904, Final Batch Loss: 0.5704876780509949\n",
      "Subject 19, Epoch 165, Loss: 1.5247937142848969, Final Batch Loss: 0.44794324040412903\n",
      "Subject 19, Epoch 166, Loss: 1.3690457046031952, Final Batch Loss: 0.39599737524986267\n",
      "Subject 19, Epoch 167, Loss: 1.297054499387741, Final Batch Loss: 0.36946040391921997\n",
      "Subject 19, Epoch 168, Loss: 1.4362188875675201, Final Batch Loss: 0.42397305369377136\n",
      "Subject 19, Epoch 169, Loss: 1.326389342546463, Final Batch Loss: 0.45634007453918457\n",
      "Subject 19, Epoch 170, Loss: 1.5049816966056824, Final Batch Loss: 0.4893997311592102\n",
      "Subject 19, Epoch 171, Loss: 1.302282601594925, Final Batch Loss: 0.3754807412624359\n",
      "Subject 19, Epoch 172, Loss: 1.417745977640152, Final Batch Loss: 0.4933595657348633\n",
      "Subject 19, Epoch 173, Loss: 1.395495980978012, Final Batch Loss: 0.557083785533905\n",
      "Subject 19, Epoch 174, Loss: 1.4950484335422516, Final Batch Loss: 0.5569764971733093\n",
      "Subject 19, Epoch 175, Loss: 1.3611038625240326, Final Batch Loss: 0.4507525861263275\n",
      "Subject 19, Epoch 176, Loss: 1.3333662450313568, Final Batch Loss: 0.4878973662853241\n",
      "Subject 19, Epoch 177, Loss: 1.2811284363269806, Final Batch Loss: 0.486329585313797\n",
      "Subject 19, Epoch 178, Loss: 1.3787211775779724, Final Batch Loss: 0.5466323494911194\n",
      "Subject 19, Epoch 179, Loss: 1.3541739583015442, Final Batch Loss: 0.4273189604282379\n",
      "Subject 19, Epoch 180, Loss: 1.2328109741210938, Final Batch Loss: 0.3890141546726227\n",
      "Subject 19, Epoch 181, Loss: 1.2586109042167664, Final Batch Loss: 0.4196605384349823\n",
      "Subject 19, Epoch 182, Loss: 1.2214605510234833, Final Batch Loss: 0.36199018359184265\n",
      "Subject 19, Epoch 183, Loss: 1.2627818584442139, Final Batch Loss: 0.4178376793861389\n",
      "Subject 19, Epoch 184, Loss: 1.2264767587184906, Final Batch Loss: 0.39384356141090393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 185, Loss: 1.235370248556137, Final Batch Loss: 0.35387346148490906\n",
      "Subject 19, Epoch 186, Loss: 1.186993658542633, Final Batch Loss: 0.40484553575515747\n",
      "Subject 19, Epoch 187, Loss: 1.2968507707118988, Final Batch Loss: 0.44125694036483765\n",
      "Subject 19, Epoch 188, Loss: 1.2250690758228302, Final Batch Loss: 0.42189016938209534\n",
      "Subject 19, Epoch 189, Loss: 1.332738846540451, Final Batch Loss: 0.33004090189933777\n",
      "Subject 19, Epoch 190, Loss: 1.2379199862480164, Final Batch Loss: 0.43370258808135986\n",
      "Subject 19, Epoch 191, Loss: 1.2434166967868805, Final Batch Loss: 0.4076000154018402\n",
      "Subject 19, Epoch 192, Loss: 1.1956194639205933, Final Batch Loss: 0.38214758038520813\n",
      "Subject 19, Epoch 193, Loss: 1.1237711608409882, Final Batch Loss: 0.2565843462944031\n",
      "Subject 19, Epoch 194, Loss: 1.2432465553283691, Final Batch Loss: 0.41119393706321716\n",
      "Subject 19, Epoch 195, Loss: 1.3143504559993744, Final Batch Loss: 0.38423624634742737\n",
      "Subject 19, Epoch 196, Loss: 1.1573944091796875, Final Batch Loss: 0.46147432923316956\n",
      "Subject 19, Epoch 197, Loss: 1.2383220493793488, Final Batch Loss: 0.3579002320766449\n",
      "Subject 19, Epoch 198, Loss: 1.233184039592743, Final Batch Loss: 0.5337505340576172\n",
      "Subject 19, Epoch 199, Loss: 1.2032533884048462, Final Batch Loss: 0.4216744899749756\n",
      "Subject 19, Epoch 200, Loss: 1.2128120064735413, Final Batch Loss: 0.40923571586608887\n",
      "Subject 19, Epoch 201, Loss: 1.2140867412090302, Final Batch Loss: 0.39156609773635864\n",
      "Subject 19, Epoch 202, Loss: 1.2344338297843933, Final Batch Loss: 0.34900963306427\n",
      "Subject 19, Epoch 203, Loss: 1.2536480128765106, Final Batch Loss: 0.3418556749820709\n",
      "Subject 19, Epoch 204, Loss: 1.168287456035614, Final Batch Loss: 0.38748690485954285\n",
      "Subject 19, Epoch 205, Loss: 1.2170179188251495, Final Batch Loss: 0.4087865948677063\n",
      "Subject 19, Epoch 206, Loss: 1.0651007890701294, Final Batch Loss: 0.388335257768631\n",
      "Subject 19, Epoch 207, Loss: 1.1874375641345978, Final Batch Loss: 0.4561688005924225\n",
      "Subject 19, Epoch 208, Loss: 1.1754870116710663, Final Batch Loss: 0.3866433799266815\n",
      "Subject 19, Epoch 209, Loss: 1.0821820497512817, Final Batch Loss: 0.3532434403896332\n",
      "Subject 19, Epoch 210, Loss: 1.0351064801216125, Final Batch Loss: 0.36445894837379456\n",
      "Subject 19, Epoch 211, Loss: 1.1413545310497284, Final Batch Loss: 0.38498568534851074\n",
      "Subject 19, Epoch 212, Loss: 1.1635823547840118, Final Batch Loss: 0.3912162482738495\n",
      "Subject 19, Epoch 213, Loss: 1.1806808710098267, Final Batch Loss: 0.328528493642807\n",
      "Subject 19, Epoch 214, Loss: 1.1222597062587738, Final Batch Loss: 0.3560759425163269\n",
      "Subject 19, Epoch 215, Loss: 1.1395424008369446, Final Batch Loss: 0.3064761757850647\n",
      "Subject 19, Epoch 216, Loss: 1.161100298166275, Final Batch Loss: 0.33917224407196045\n",
      "Subject 19, Epoch 217, Loss: 1.1145745813846588, Final Batch Loss: 0.30669498443603516\n",
      "Subject 19, Epoch 218, Loss: 1.1646562218666077, Final Batch Loss: 0.3427794277667999\n",
      "Subject 19, Epoch 219, Loss: 1.1404120028018951, Final Batch Loss: 0.3563065826892853\n",
      "Subject 19, Epoch 220, Loss: 1.194388061761856, Final Batch Loss: 0.3925609886646271\n",
      "Subject 19, Epoch 221, Loss: 1.3161992728710175, Final Batch Loss: 0.488618403673172\n",
      "Subject 19, Epoch 222, Loss: 1.1131112277507782, Final Batch Loss: 0.34081321954727173\n",
      "Subject 19, Epoch 223, Loss: 1.0976069271564484, Final Batch Loss: 0.33593231439590454\n",
      "Subject 19, Epoch 224, Loss: 1.223062425851822, Final Batch Loss: 0.3773680031299591\n",
      "Subject 19, Epoch 225, Loss: 1.1165401339530945, Final Batch Loss: 0.3384028375148773\n",
      "Subject 19, Epoch 226, Loss: 1.2015715539455414, Final Batch Loss: 0.3679109215736389\n",
      "Subject 19, Epoch 227, Loss: 1.0553227961063385, Final Batch Loss: 0.37850114703178406\n",
      "Subject 19, Epoch 228, Loss: 1.066883623600006, Final Batch Loss: 0.38912150263786316\n",
      "Subject 19, Epoch 229, Loss: 1.076528400182724, Final Batch Loss: 0.3972521722316742\n",
      "Subject 19, Epoch 230, Loss: 1.059125393629074, Final Batch Loss: 0.3916833698749542\n",
      "Subject 19, Epoch 231, Loss: 1.181949257850647, Final Batch Loss: 0.3797285556793213\n",
      "Subject 19, Epoch 232, Loss: 1.1365134716033936, Final Batch Loss: 0.3328738808631897\n",
      "Subject 19, Epoch 233, Loss: 1.049602746963501, Final Batch Loss: 0.3694992959499359\n",
      "Subject 19, Epoch 234, Loss: 1.0340275764465332, Final Batch Loss: 0.3822771906852722\n",
      "Subject 19, Epoch 235, Loss: 1.107984185218811, Final Batch Loss: 0.3283010721206665\n",
      "Subject 19, Epoch 236, Loss: 1.1630845367908478, Final Batch Loss: 0.38933730125427246\n",
      "Subject 19, Epoch 237, Loss: 1.0995918214321136, Final Batch Loss: 0.37081700563430786\n",
      "Subject 19, Epoch 238, Loss: 1.1660808622837067, Final Batch Loss: 0.40627023577690125\n",
      "Subject 19, Epoch 239, Loss: 1.1997544169425964, Final Batch Loss: 0.4621112644672394\n",
      "Subject 19, Epoch 240, Loss: 1.0774666965007782, Final Batch Loss: 0.3238152861595154\n",
      "Subject 19, Epoch 241, Loss: 1.0183474123477936, Final Batch Loss: 0.3443453013896942\n",
      "Subject 19, Epoch 242, Loss: 1.0947567820549011, Final Batch Loss: 0.3907523453235626\n",
      "Subject 19, Epoch 243, Loss: 1.0721542835235596, Final Batch Loss: 0.4139089286327362\n",
      "Subject 19, Epoch 244, Loss: 1.1376763880252838, Final Batch Loss: 0.34762319922447205\n",
      "Subject 19, Epoch 245, Loss: 1.0139318704605103, Final Batch Loss: 0.34108784794807434\n",
      "Subject 19, Epoch 246, Loss: 1.0376939475536346, Final Batch Loss: 0.2996140718460083\n",
      "Subject 19, Epoch 247, Loss: 1.0441153049468994, Final Batch Loss: 0.3299511969089508\n",
      "Subject 19, Epoch 248, Loss: 1.0247920453548431, Final Batch Loss: 0.36575719714164734\n",
      "Subject 19, Epoch 249, Loss: 1.04828679561615, Final Batch Loss: 0.395092248916626\n",
      "Subject 19, Epoch 250, Loss: 1.1553800106048584, Final Batch Loss: 0.42209210991859436\n",
      "Subject 19, Epoch 251, Loss: 1.0344134867191315, Final Batch Loss: 0.3420901596546173\n",
      "Subject 19, Epoch 252, Loss: 1.109921008348465, Final Batch Loss: 0.3734227418899536\n",
      "Subject 19, Epoch 253, Loss: 1.0363537967205048, Final Batch Loss: 0.3887832462787628\n",
      "Subject 19, Epoch 254, Loss: 1.0306933224201202, Final Batch Loss: 0.353967547416687\n",
      "Subject 19, Epoch 255, Loss: 0.9502962529659271, Final Batch Loss: 0.339642733335495\n",
      "Subject 19, Epoch 256, Loss: 1.0710894763469696, Final Batch Loss: 0.40540122985839844\n",
      "Subject 19, Epoch 257, Loss: 0.8958795666694641, Final Batch Loss: 0.2967882752418518\n",
      "Subject 19, Epoch 258, Loss: 1.2188386023044586, Final Batch Loss: 0.465285062789917\n",
      "Subject 19, Epoch 259, Loss: 1.003150463104248, Final Batch Loss: 0.35046643018722534\n",
      "Subject 19, Epoch 260, Loss: 1.018358439207077, Final Batch Loss: 0.29330411553382874\n",
      "Subject 19, Epoch 261, Loss: 1.0196417272090912, Final Batch Loss: 0.309622198343277\n",
      "Subject 19, Epoch 262, Loss: 1.0808134078979492, Final Batch Loss: 0.39399096369743347\n",
      "Subject 19, Epoch 263, Loss: 0.9467411190271378, Final Batch Loss: 0.23916678130626678\n",
      "Subject 19, Epoch 264, Loss: 1.1105716228485107, Final Batch Loss: 0.38625916838645935\n",
      "Subject 19, Epoch 265, Loss: 1.1054927110671997, Final Batch Loss: 0.3812350332736969\n",
      "Subject 19, Epoch 266, Loss: 0.9684593379497528, Final Batch Loss: 0.3192831873893738\n",
      "Subject 19, Epoch 267, Loss: 1.0064978897571564, Final Batch Loss: 0.41277313232421875\n",
      "Subject 19, Epoch 268, Loss: 1.0595174729824066, Final Batch Loss: 0.3516826331615448\n",
      "Subject 19, Epoch 269, Loss: 1.010945439338684, Final Batch Loss: 0.3243691623210907\n",
      "Subject 19, Epoch 270, Loss: 1.0779943764209747, Final Batch Loss: 0.3078621029853821\n",
      "Subject 19, Epoch 271, Loss: 1.0326151251792908, Final Batch Loss: 0.3446062207221985\n",
      "Subject 19, Epoch 272, Loss: 1.0813438296318054, Final Batch Loss: 0.34041720628738403\n",
      "Subject 19, Epoch 273, Loss: 1.0562950670719147, Final Batch Loss: 0.3860743045806885\n",
      "Subject 19, Epoch 274, Loss: 0.9822244644165039, Final Batch Loss: 0.29207709431648254\n",
      "Subject 19, Epoch 275, Loss: 1.0741645395755768, Final Batch Loss: 0.34238308668136597\n",
      "Subject 19, Epoch 276, Loss: 1.0102270245552063, Final Batch Loss: 0.36501166224479675\n",
      "Subject 19, Epoch 277, Loss: 1.0211555361747742, Final Batch Loss: 0.32967033982276917\n",
      "Subject 19, Epoch 278, Loss: 1.1273675858974457, Final Batch Loss: 0.4260067343711853\n",
      "Subject 19, Epoch 279, Loss: 1.0821370780467987, Final Batch Loss: 0.42808839678764343\n",
      "Subject 19, Epoch 280, Loss: 1.0437257885932922, Final Batch Loss: 0.36469215154647827\n",
      "Subject 19, Epoch 281, Loss: 0.9684979021549225, Final Batch Loss: 0.368730753660202\n",
      "Subject 19, Epoch 282, Loss: 1.0141076445579529, Final Batch Loss: 0.2603665292263031\n",
      "Subject 19, Epoch 283, Loss: 0.9789063036441803, Final Batch Loss: 0.3926582634449005\n",
      "Subject 19, Epoch 284, Loss: 0.9834935963153839, Final Batch Loss: 0.27911052107810974\n",
      "Subject 19, Epoch 285, Loss: 1.0345717668533325, Final Batch Loss: 0.39691174030303955\n",
      "Subject 19, Epoch 286, Loss: 0.9235111474990845, Final Batch Loss: 0.2980881631374359\n",
      "Subject 19, Epoch 287, Loss: 0.9622116386890411, Final Batch Loss: 0.3762078881263733\n",
      "Subject 19, Epoch 288, Loss: 0.9339206516742706, Final Batch Loss: 0.3045711815357208\n",
      "Subject 19, Epoch 289, Loss: 0.9202389717102051, Final Batch Loss: 0.29364874958992004\n",
      "Subject 19, Epoch 290, Loss: 1.0008623898029327, Final Batch Loss: 0.28089678287506104\n",
      "Subject 19, Epoch 291, Loss: 0.9627154171466827, Final Batch Loss: 0.3280889391899109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 292, Loss: 0.9607968628406525, Final Batch Loss: 0.3229588568210602\n",
      "Subject 19, Epoch 293, Loss: 0.8929843008518219, Final Batch Loss: 0.27600258588790894\n",
      "Subject 19, Epoch 294, Loss: 1.0291506350040436, Final Batch Loss: 0.3303625285625458\n",
      "Subject 19, Epoch 295, Loss: 0.9497356116771698, Final Batch Loss: 0.3110249638557434\n",
      "Subject 19, Epoch 296, Loss: 1.0359635651111603, Final Batch Loss: 0.36403754353523254\n",
      "Subject 19, Epoch 297, Loss: 0.9441570341587067, Final Batch Loss: 0.2965245842933655\n",
      "Subject 19, Epoch 298, Loss: 0.947152704000473, Final Batch Loss: 0.32988440990448\n",
      "Subject 19, Epoch 299, Loss: 0.9137925505638123, Final Batch Loss: 0.2782423496246338\n",
      "Subject 19, Epoch 300, Loss: 0.986667275428772, Final Batch Loss: 0.3648384213447571\n",
      "Subject 19, Epoch 301, Loss: 1.0222810208797455, Final Batch Loss: 0.24215564131736755\n",
      "Subject 19, Epoch 302, Loss: 0.8838073015213013, Final Batch Loss: 0.3140459656715393\n",
      "Subject 19, Epoch 303, Loss: 0.9005557894706726, Final Batch Loss: 0.31959420442581177\n",
      "Subject 19, Epoch 304, Loss: 0.9559109210968018, Final Batch Loss: 0.27105042338371277\n",
      "Subject 19, Epoch 305, Loss: 1.0514973998069763, Final Batch Loss: 0.32557329535484314\n",
      "Subject 19, Epoch 306, Loss: 0.9164115786552429, Final Batch Loss: 0.33184632658958435\n",
      "Subject 19, Epoch 307, Loss: 0.9661740362644196, Final Batch Loss: 0.3558520972728729\n",
      "Subject 19, Epoch 308, Loss: 1.0084688365459442, Final Batch Loss: 0.3436259627342224\n",
      "Subject 19, Epoch 309, Loss: 0.8876356184482574, Final Batch Loss: 0.2334512174129486\n",
      "Subject 19, Epoch 310, Loss: 0.9656573235988617, Final Batch Loss: 0.28228959441185\n",
      "Subject 19, Epoch 311, Loss: 0.9717186987400055, Final Batch Loss: 0.26115959882736206\n",
      "Subject 19, Epoch 312, Loss: 0.9561682343482971, Final Batch Loss: 0.2965952754020691\n",
      "Subject 19, Epoch 313, Loss: 0.9593692421913147, Final Batch Loss: 0.3813712000846863\n",
      "Subject 19, Epoch 314, Loss: 0.9706820547580719, Final Batch Loss: 0.33085739612579346\n",
      "Subject 19, Epoch 315, Loss: 0.8872715830802917, Final Batch Loss: 0.27238729596138\n",
      "Subject 19, Epoch 316, Loss: 0.81472247838974, Final Batch Loss: 0.23555347323417664\n",
      "Subject 19, Epoch 317, Loss: 0.8878054618835449, Final Batch Loss: 0.2484082579612732\n",
      "Subject 19, Epoch 318, Loss: 0.897119790315628, Final Batch Loss: 0.28817808628082275\n",
      "Subject 19, Epoch 319, Loss: 0.9233063161373138, Final Batch Loss: 0.3209892511367798\n",
      "Subject 19, Epoch 320, Loss: 0.9300039112567902, Final Batch Loss: 0.3407137989997864\n",
      "Subject 19, Epoch 321, Loss: 0.9353130012750626, Final Batch Loss: 0.32965585589408875\n",
      "Subject 19, Epoch 322, Loss: 0.8468682169914246, Final Batch Loss: 0.2924468219280243\n",
      "Subject 19, Epoch 323, Loss: 1.0030741393566132, Final Batch Loss: 0.3219795227050781\n",
      "Subject 19, Epoch 324, Loss: 0.9206198155879974, Final Batch Loss: 0.3106946647167206\n",
      "Subject 19, Epoch 325, Loss: 0.893688514828682, Final Batch Loss: 0.3256765305995941\n",
      "Subject 19, Epoch 326, Loss: 0.8743560612201691, Final Batch Loss: 0.2704218924045563\n",
      "Subject 19, Epoch 327, Loss: 0.8149964809417725, Final Batch Loss: 0.3332802653312683\n",
      "Subject 19, Epoch 328, Loss: 0.8680408000946045, Final Batch Loss: 0.3001810312271118\n",
      "Subject 19, Epoch 329, Loss: 1.009245604276657, Final Batch Loss: 0.3325441777706146\n",
      "Subject 19, Epoch 330, Loss: 0.9368191212415695, Final Batch Loss: 0.37375718355178833\n",
      "Subject 19, Epoch 331, Loss: 1.0137719511985779, Final Batch Loss: 0.352967768907547\n",
      "Subject 19, Epoch 332, Loss: 1.0227504521608353, Final Batch Loss: 0.370984822511673\n",
      "Subject 19, Epoch 333, Loss: 0.9120067656040192, Final Batch Loss: 0.32007235288619995\n",
      "Subject 19, Epoch 334, Loss: 0.9288114309310913, Final Batch Loss: 0.31905871629714966\n",
      "Subject 19, Epoch 335, Loss: 0.9628778398036957, Final Batch Loss: 0.29185011982917786\n",
      "Subject 19, Epoch 336, Loss: 0.8906867206096649, Final Batch Loss: 0.22936800122261047\n",
      "Subject 19, Epoch 337, Loss: 0.8974439799785614, Final Batch Loss: 0.2500072121620178\n",
      "Subject 19, Epoch 338, Loss: 0.9452407360076904, Final Batch Loss: 0.3249136805534363\n",
      "Subject 19, Epoch 339, Loss: 0.957773745059967, Final Batch Loss: 0.3127663731575012\n",
      "Subject 19, Epoch 340, Loss: 0.8228222280740738, Final Batch Loss: 0.26526373624801636\n",
      "Subject 19, Epoch 341, Loss: 0.8936875760555267, Final Batch Loss: 0.26045432686805725\n",
      "Subject 19, Epoch 342, Loss: 0.8993237018585205, Final Batch Loss: 0.3532945513725281\n",
      "Subject 19, Epoch 343, Loss: 0.8870023488998413, Final Batch Loss: 0.2737404704093933\n",
      "Subject 19, Epoch 344, Loss: 0.9537225067615509, Final Batch Loss: 0.31382399797439575\n",
      "Subject 19, Epoch 345, Loss: 0.9437003284692764, Final Batch Loss: 0.36436358094215393\n",
      "Subject 19, Epoch 346, Loss: 0.775614470243454, Final Batch Loss: 0.24118056893348694\n",
      "Subject 19, Epoch 347, Loss: 0.9132796227931976, Final Batch Loss: 0.27675706148147583\n",
      "Subject 19, Epoch 348, Loss: 0.9840397536754608, Final Batch Loss: 0.385688453912735\n",
      "Subject 19, Epoch 349, Loss: 0.9391633868217468, Final Batch Loss: 0.35653477907180786\n",
      "Subject 19, Epoch 350, Loss: 0.8584373891353607, Final Batch Loss: 0.25638315081596375\n",
      "Subject 19, Epoch 351, Loss: 0.9509577751159668, Final Batch Loss: 0.291733980178833\n",
      "Subject 19, Epoch 352, Loss: 0.8846040517091751, Final Batch Loss: 0.22600729763507843\n",
      "Subject 19, Epoch 353, Loss: 0.8639585077762604, Final Batch Loss: 0.32272806763648987\n",
      "Subject 19, Epoch 354, Loss: 0.8709864169359207, Final Batch Loss: 0.30210253596305847\n",
      "Subject 19, Epoch 355, Loss: 0.868270754814148, Final Batch Loss: 0.260439395904541\n",
      "Subject 19, Epoch 356, Loss: 0.9256340265274048, Final Batch Loss: 0.3269708752632141\n",
      "Subject 19, Epoch 357, Loss: 0.883189469575882, Final Batch Loss: 0.2963680326938629\n",
      "Subject 19, Epoch 358, Loss: 0.8511596471071243, Final Batch Loss: 0.3265259265899658\n",
      "Subject 19, Epoch 359, Loss: 0.8932807445526123, Final Batch Loss: 0.3183569014072418\n",
      "Subject 19, Epoch 360, Loss: 0.8309773802757263, Final Batch Loss: 0.2130887806415558\n",
      "Subject 19, Epoch 361, Loss: 0.8527391105890274, Final Batch Loss: 0.2922237515449524\n",
      "Subject 19, Epoch 362, Loss: 0.8917108774185181, Final Batch Loss: 0.336580753326416\n",
      "Subject 19, Epoch 363, Loss: 0.8093176782131195, Final Batch Loss: 0.2633834183216095\n",
      "Subject 19, Epoch 364, Loss: 0.905776709318161, Final Batch Loss: 0.31444215774536133\n",
      "Subject 19, Epoch 365, Loss: 0.8546774536371231, Final Batch Loss: 0.31986531615257263\n",
      "Subject 19, Epoch 366, Loss: 0.8315946161746979, Final Batch Loss: 0.28919753432273865\n",
      "Subject 19, Epoch 367, Loss: 0.8756840229034424, Final Batch Loss: 0.1654277741909027\n",
      "Subject 19, Epoch 368, Loss: 0.9346164762973785, Final Batch Loss: 0.2907411456108093\n",
      "Subject 19, Epoch 369, Loss: 0.8691083341836929, Final Batch Loss: 0.21831341087818146\n",
      "Subject 19, Epoch 370, Loss: 0.9021340906620026, Final Batch Loss: 0.27711668610572815\n",
      "Subject 19, Epoch 371, Loss: 0.9152361005544662, Final Batch Loss: 0.19594143331050873\n",
      "Subject 19, Epoch 372, Loss: 0.8605082631111145, Final Batch Loss: 0.3373754322528839\n",
      "Subject 19, Epoch 373, Loss: 0.8742535412311554, Final Batch Loss: 0.3121100664138794\n",
      "Subject 19, Epoch 374, Loss: 0.8341193199157715, Final Batch Loss: 0.30778029561042786\n",
      "Subject 19, Epoch 375, Loss: 0.8131734877824783, Final Batch Loss: 0.22739647328853607\n",
      "Subject 19, Epoch 376, Loss: 0.7945130169391632, Final Batch Loss: 0.2428467720746994\n",
      "Subject 19, Epoch 377, Loss: 0.8735345304012299, Final Batch Loss: 0.27809950709342957\n",
      "Subject 19, Epoch 378, Loss: 0.7846761494874954, Final Batch Loss: 0.23766927421092987\n",
      "Subject 19, Epoch 379, Loss: 0.780741348862648, Final Batch Loss: 0.2514287829399109\n",
      "Subject 19, Epoch 380, Loss: 0.8667247295379639, Final Batch Loss: 0.2456398606300354\n",
      "Subject 19, Epoch 381, Loss: 0.8158541619777679, Final Batch Loss: 0.3178495168685913\n",
      "Subject 19, Epoch 382, Loss: 0.8033330738544464, Final Batch Loss: 0.23559044301509857\n",
      "Subject 19, Epoch 383, Loss: 0.7772555500268936, Final Batch Loss: 0.2432965785264969\n",
      "Subject 19, Epoch 384, Loss: 0.7671239674091339, Final Batch Loss: 0.22645272314548492\n",
      "Subject 19, Epoch 385, Loss: 0.8198830634355545, Final Batch Loss: 0.3188529908657074\n",
      "Subject 19, Epoch 386, Loss: 0.9355209767818451, Final Batch Loss: 0.324616402387619\n",
      "Subject 19, Epoch 387, Loss: 0.8165668994188309, Final Batch Loss: 0.31985941529273987\n",
      "Subject 19, Epoch 388, Loss: 0.8880108892917633, Final Batch Loss: 0.26361626386642456\n",
      "Subject 19, Epoch 389, Loss: 0.7643310725688934, Final Batch Loss: 0.22674217820167542\n",
      "Subject 19, Epoch 390, Loss: 0.8619230538606644, Final Batch Loss: 0.320026695728302\n",
      "Subject 19, Epoch 391, Loss: 0.8291970491409302, Final Batch Loss: 0.2164231538772583\n",
      "Subject 19, Epoch 392, Loss: 0.912470132112503, Final Batch Loss: 0.28110000491142273\n",
      "Subject 19, Epoch 393, Loss: 0.7996337860822678, Final Batch Loss: 0.1987471729516983\n",
      "Subject 19, Epoch 394, Loss: 0.8573248982429504, Final Batch Loss: 0.24823564291000366\n",
      "Subject 19, Epoch 395, Loss: 0.8465735912322998, Final Batch Loss: 0.3312802016735077\n",
      "Subject 19, Epoch 396, Loss: 0.8721605688333511, Final Batch Loss: 0.389232873916626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 397, Loss: 0.8269490897655487, Final Batch Loss: 0.3068406879901886\n",
      "Subject 19, Epoch 398, Loss: 0.7193426340818405, Final Batch Loss: 0.2742902338504791\n",
      "Subject 19, Epoch 399, Loss: 0.8242324888706207, Final Batch Loss: 0.2522721588611603\n",
      "Subject 19, Epoch 400, Loss: 0.8017578125, Final Batch Loss: 0.23974058032035828\n",
      "Subject 19, Epoch 401, Loss: 0.7730270028114319, Final Batch Loss: 0.3419530689716339\n",
      "Subject 19, Epoch 402, Loss: 0.7983035147190094, Final Batch Loss: 0.25324422121047974\n",
      "Subject 19, Epoch 403, Loss: 0.9466188848018646, Final Batch Loss: 0.33551353216171265\n",
      "Subject 19, Epoch 404, Loss: 1.0190162062644958, Final Batch Loss: 0.42807960510253906\n",
      "Subject 19, Epoch 405, Loss: 0.8390454053878784, Final Batch Loss: 0.26403242349624634\n",
      "Subject 19, Epoch 406, Loss: 0.9184859991073608, Final Batch Loss: 0.28649789094924927\n",
      "Subject 19, Epoch 407, Loss: 0.7075192332267761, Final Batch Loss: 0.24404074251651764\n",
      "Subject 19, Epoch 408, Loss: 0.7626479268074036, Final Batch Loss: 0.3215373456478119\n",
      "Subject 19, Epoch 409, Loss: 0.8422816395759583, Final Batch Loss: 0.3151988685131073\n",
      "Subject 19, Epoch 410, Loss: 0.8792086094617844, Final Batch Loss: 0.3981500566005707\n",
      "Subject 19, Epoch 411, Loss: 0.8081532418727875, Final Batch Loss: 0.25340601801872253\n",
      "Subject 19, Epoch 412, Loss: 0.8063599914312363, Final Batch Loss: 0.22110895812511444\n",
      "Subject 19, Epoch 413, Loss: 0.8011899590492249, Final Batch Loss: 0.26530444622039795\n",
      "Subject 19, Epoch 414, Loss: 0.9119153618812561, Final Batch Loss: 0.23831462860107422\n",
      "Subject 19, Epoch 415, Loss: 0.8524655103683472, Final Batch Loss: 0.2412634789943695\n",
      "Subject 19, Epoch 416, Loss: 0.813346341252327, Final Batch Loss: 0.19689206779003143\n",
      "Subject 19, Epoch 417, Loss: 0.9083101451396942, Final Batch Loss: 0.3510127663612366\n",
      "Subject 19, Epoch 418, Loss: 0.9362763464450836, Final Batch Loss: 0.25075313448905945\n",
      "Subject 19, Epoch 419, Loss: 0.8640165030956268, Final Batch Loss: 0.2902023494243622\n",
      "Subject 19, Epoch 420, Loss: 0.9237312376499176, Final Batch Loss: 0.31337523460388184\n",
      "Subject 19, Epoch 421, Loss: 0.9004519134759903, Final Batch Loss: 0.24388916790485382\n",
      "Subject 19, Epoch 422, Loss: 0.8832377791404724, Final Batch Loss: 0.24811410903930664\n",
      "Subject 19, Epoch 423, Loss: 0.7702952176332474, Final Batch Loss: 0.1625479906797409\n",
      "Subject 19, Epoch 424, Loss: 0.7153210192918777, Final Batch Loss: 0.19137737154960632\n",
      "Subject 19, Epoch 425, Loss: 0.7815608531236649, Final Batch Loss: 0.24048663675785065\n",
      "Subject 19, Epoch 426, Loss: 0.8307123631238937, Final Batch Loss: 0.27577894926071167\n",
      "Subject 19, Epoch 427, Loss: 0.7668729722499847, Final Batch Loss: 0.24721793830394745\n",
      "Subject 19, Epoch 428, Loss: 0.7457841485738754, Final Batch Loss: 0.22340287268161774\n",
      "Subject 19, Epoch 429, Loss: 0.7925033867359161, Final Batch Loss: 0.306896448135376\n",
      "Subject 19, Epoch 430, Loss: 0.8724220842123032, Final Batch Loss: 0.27375373244285583\n",
      "Subject 19, Epoch 431, Loss: 0.8039075136184692, Final Batch Loss: 0.28336092829704285\n",
      "Subject 19, Epoch 432, Loss: 0.9265487194061279, Final Batch Loss: 0.279914528131485\n",
      "Subject 19, Epoch 433, Loss: 0.7366849184036255, Final Batch Loss: 0.2792433202266693\n",
      "Subject 19, Epoch 434, Loss: 0.7458437234163284, Final Batch Loss: 0.2653370201587677\n",
      "Subject 19, Epoch 435, Loss: 0.742879718542099, Final Batch Loss: 0.2433973252773285\n",
      "Subject 19, Epoch 436, Loss: 0.7852805554866791, Final Batch Loss: 0.3095237910747528\n",
      "Subject 19, Epoch 437, Loss: 0.8488831073045731, Final Batch Loss: 0.36788883805274963\n",
      "Subject 19, Epoch 438, Loss: 0.9007765352725983, Final Batch Loss: 0.3094243109226227\n",
      "Subject 19, Epoch 439, Loss: 0.8383224010467529, Final Batch Loss: 0.2353295385837555\n",
      "Subject 19, Epoch 440, Loss: 0.7499744743108749, Final Batch Loss: 0.23867067694664001\n",
      "Subject 19, Epoch 441, Loss: 0.7730763256549835, Final Batch Loss: 0.2597409188747406\n",
      "Subject 19, Epoch 442, Loss: 0.8123592585325241, Final Batch Loss: 0.3150632381439209\n",
      "Subject 19, Epoch 443, Loss: 0.9013335108757019, Final Batch Loss: 0.3463286757469177\n",
      "Subject 19, Epoch 444, Loss: 0.8459253013134003, Final Batch Loss: 0.26301687955856323\n",
      "Subject 19, Epoch 445, Loss: 0.7513620406389236, Final Batch Loss: 0.1696373075246811\n",
      "Subject 19, Epoch 446, Loss: 0.803848996758461, Final Batch Loss: 0.32569175958633423\n",
      "Subject 19, Epoch 447, Loss: 0.7148230671882629, Final Batch Loss: 0.2127300500869751\n",
      "Subject 19, Epoch 448, Loss: 0.6934472173452377, Final Batch Loss: 0.2765468657016754\n",
      "Subject 19, Epoch 449, Loss: 0.7963517010211945, Final Batch Loss: 0.16327843070030212\n",
      "Subject 19, Epoch 450, Loss: 0.7355189025402069, Final Batch Loss: 0.3274509012699127\n",
      "Subject 19, Epoch 451, Loss: 0.747994914650917, Final Batch Loss: 0.2890144884586334\n",
      "Subject 19, Epoch 452, Loss: 0.8902701586484909, Final Batch Loss: 0.32530367374420166\n",
      "Subject 19, Epoch 453, Loss: 0.7955332547426224, Final Batch Loss: 0.20702342689037323\n",
      "Subject 19, Epoch 454, Loss: 0.7398530095815659, Final Batch Loss: 0.20392782986164093\n",
      "Subject 19, Epoch 455, Loss: 0.7493398934602737, Final Batch Loss: 0.25639304518699646\n",
      "Subject 19, Epoch 456, Loss: 0.8059911876916885, Final Batch Loss: 0.31924742460250854\n",
      "Subject 19, Epoch 457, Loss: 0.8420581221580505, Final Batch Loss: 0.276468425989151\n",
      "Subject 19, Epoch 458, Loss: 0.7497034668922424, Final Batch Loss: 0.19693608582019806\n",
      "Subject 19, Epoch 459, Loss: 0.7608966380357742, Final Batch Loss: 0.24774180352687836\n",
      "Subject 19, Epoch 460, Loss: 0.7577788531780243, Final Batch Loss: 0.29145559668540955\n",
      "Subject 19, Epoch 461, Loss: 0.6948542445898056, Final Batch Loss: 0.2489963173866272\n",
      "Subject 19, Epoch 462, Loss: 0.7813945412635803, Final Batch Loss: 0.2354753315448761\n",
      "Subject 19, Epoch 463, Loss: 0.7250268310308456, Final Batch Loss: 0.2185886651277542\n",
      "Subject 19, Epoch 464, Loss: 0.766334056854248, Final Batch Loss: 0.2503630518913269\n",
      "Subject 19, Epoch 465, Loss: 0.8011257648468018, Final Batch Loss: 0.30191391706466675\n",
      "Subject 19, Epoch 466, Loss: 0.758490264415741, Final Batch Loss: 0.16501985490322113\n",
      "Subject 19, Epoch 467, Loss: 0.8109697997570038, Final Batch Loss: 0.26321232318878174\n",
      "Subject 19, Epoch 468, Loss: 0.8628441542387009, Final Batch Loss: 0.38444116711616516\n",
      "Subject 19, Epoch 469, Loss: 0.8282454907894135, Final Batch Loss: 0.24640601873397827\n",
      "Subject 19, Epoch 470, Loss: 0.801594153046608, Final Batch Loss: 0.28527578711509705\n",
      "Subject 19, Epoch 471, Loss: 0.7232709378004074, Final Batch Loss: 0.18454910814762115\n",
      "Subject 19, Epoch 472, Loss: 0.7720548957586288, Final Batch Loss: 0.28517666459083557\n",
      "Subject 19, Epoch 473, Loss: 0.7397604882717133, Final Batch Loss: 0.26731687784194946\n",
      "Subject 19, Epoch 474, Loss: 0.86906498670578, Final Batch Loss: 0.36115962266921997\n",
      "Subject 19, Epoch 475, Loss: 0.7413498908281326, Final Batch Loss: 0.20071576535701752\n",
      "Subject 19, Epoch 476, Loss: 0.7086703926324844, Final Batch Loss: 0.2274346500635147\n",
      "Subject 19, Epoch 477, Loss: 0.7291555404663086, Final Batch Loss: 0.2737225592136383\n",
      "Subject 19, Epoch 478, Loss: 0.6051406115293503, Final Batch Loss: 0.2142617553472519\n",
      "Subject 19, Epoch 479, Loss: 0.7861958742141724, Final Batch Loss: 0.2588030695915222\n",
      "Subject 19, Epoch 480, Loss: 0.7352099716663361, Final Batch Loss: 0.26867446303367615\n",
      "Subject 19, Epoch 481, Loss: 0.783038929104805, Final Batch Loss: 0.2821606695652008\n",
      "Subject 19, Epoch 482, Loss: 0.7549731135368347, Final Batch Loss: 0.2778700888156891\n",
      "Subject 19, Epoch 483, Loss: 0.667184442281723, Final Batch Loss: 0.23661501705646515\n",
      "Subject 19, Epoch 484, Loss: 0.7141426801681519, Final Batch Loss: 0.25002521276474\n",
      "Subject 19, Epoch 485, Loss: 0.7632530480623245, Final Batch Loss: 0.20612414181232452\n",
      "Subject 19, Epoch 486, Loss: 0.6933956295251846, Final Batch Loss: 0.2224983274936676\n",
      "Subject 19, Epoch 487, Loss: 0.7389624267816544, Final Batch Loss: 0.33110034465789795\n",
      "Subject 19, Epoch 488, Loss: 0.686370313167572, Final Batch Loss: 0.2623595893383026\n",
      "Subject 19, Epoch 489, Loss: 0.7685775309801102, Final Batch Loss: 0.2671038806438446\n",
      "Subject 19, Epoch 490, Loss: 0.6946051865816116, Final Batch Loss: 0.24396276473999023\n",
      "Subject 19, Epoch 491, Loss: 0.6819888949394226, Final Batch Loss: 0.17435650527477264\n",
      "Subject 19, Epoch 492, Loss: 0.7401056289672852, Final Batch Loss: 0.3250470459461212\n",
      "Subject 19, Epoch 493, Loss: 0.7299568802118301, Final Batch Loss: 0.20406198501586914\n",
      "Subject 19, Epoch 494, Loss: 0.8060141801834106, Final Batch Loss: 0.24065712094306946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 495, Loss: 0.8069446384906769, Final Batch Loss: 0.25764837861061096\n",
      "Subject 19, Epoch 496, Loss: 0.8287400156259537, Final Batch Loss: 0.22227784991264343\n",
      "Subject 19, Epoch 497, Loss: 0.8280358016490936, Final Batch Loss: 0.20346786081790924\n",
      "Subject 19, Epoch 498, Loss: 0.7026716470718384, Final Batch Loss: 0.20891782641410828\n",
      "Subject 19, Epoch 499, Loss: 0.7997090667486191, Final Batch Loss: 0.25338536500930786\n",
      "Subject 19, Epoch 500, Loss: 0.7324941009283066, Final Batch Loss: 0.2567650377750397\n",
      "Subject 19, Epoch 501, Loss: 0.7483341842889786, Final Batch Loss: 0.20946921408176422\n",
      "Subject 19, Epoch 502, Loss: 0.7315805852413177, Final Batch Loss: 0.23878851532936096\n",
      "Subject 19, Epoch 503, Loss: 0.7284188866615295, Final Batch Loss: 0.2217225432395935\n",
      "Subject 19, Epoch 504, Loss: 0.7891435623168945, Final Batch Loss: 0.2701006233692169\n",
      "Subject 19, Epoch 505, Loss: 0.7404054552316666, Final Batch Loss: 0.2675812840461731\n",
      "Subject 19, Epoch 506, Loss: 0.7347234040498734, Final Batch Loss: 0.19344085454940796\n",
      "Subject 19, Epoch 507, Loss: 0.7578008472919464, Final Batch Loss: 0.34623053669929504\n",
      "Subject 19, Epoch 508, Loss: 0.7514310479164124, Final Batch Loss: 0.2744319438934326\n",
      "Subject 19, Epoch 509, Loss: 0.7192460000514984, Final Batch Loss: 0.22239595651626587\n",
      "Subject 19, Epoch 510, Loss: 0.6902471631765366, Final Batch Loss: 0.2177184820175171\n",
      "Subject 19, Epoch 511, Loss: 0.6631133407354355, Final Batch Loss: 0.2211756706237793\n",
      "Subject 19, Epoch 512, Loss: 0.7981986105442047, Final Batch Loss: 0.2667466104030609\n",
      "Subject 19, Epoch 513, Loss: 0.7357094883918762, Final Batch Loss: 0.26328566670417786\n",
      "Subject 19, Epoch 514, Loss: 0.7182141691446304, Final Batch Loss: 0.2126874327659607\n",
      "Subject 19, Epoch 515, Loss: 0.7873449325561523, Final Batch Loss: 0.214412122964859\n",
      "Subject 19, Epoch 516, Loss: 0.7546488791704178, Final Batch Loss: 0.3185049295425415\n",
      "Subject 19, Epoch 517, Loss: 0.6968200951814651, Final Batch Loss: 0.17877039313316345\n",
      "Subject 19, Epoch 518, Loss: 0.8111048340797424, Final Batch Loss: 0.2438720464706421\n",
      "Subject 19, Epoch 519, Loss: 0.7351206541061401, Final Batch Loss: 0.2809736430644989\n",
      "Subject 19, Epoch 520, Loss: 0.7332449853420258, Final Batch Loss: 0.3232945501804352\n",
      "Subject 19, Epoch 521, Loss: 0.7185799777507782, Final Batch Loss: 0.22545818984508514\n",
      "Subject 19, Epoch 522, Loss: 0.7859307080507278, Final Batch Loss: 0.24651722609996796\n",
      "Subject 19, Epoch 523, Loss: 0.6940712481737137, Final Batch Loss: 0.20038828253746033\n",
      "Subject 19, Epoch 524, Loss: 0.7978547960519791, Final Batch Loss: 0.2191600650548935\n",
      "Subject 19, Epoch 525, Loss: 0.777063176035881, Final Batch Loss: 0.24142049252986908\n",
      "Subject 19, Epoch 526, Loss: 0.7007838487625122, Final Batch Loss: 0.2066948115825653\n",
      "Subject 19, Epoch 527, Loss: 0.5969117134809494, Final Batch Loss: 0.14692845940589905\n",
      "Subject 19, Epoch 528, Loss: 0.746828481554985, Final Batch Loss: 0.25217393040657043\n",
      "Subject 19, Epoch 529, Loss: 0.6944642663002014, Final Batch Loss: 0.20185545086860657\n",
      "Subject 19, Epoch 530, Loss: 0.6535127609968185, Final Batch Loss: 0.20639458298683167\n",
      "Subject 19, Epoch 531, Loss: 0.8899049609899521, Final Batch Loss: 0.26612046360969543\n",
      "Subject 19, Epoch 532, Loss: 0.6935060024261475, Final Batch Loss: 0.2453576773405075\n",
      "Subject 19, Epoch 533, Loss: 0.7464982271194458, Final Batch Loss: 0.23340517282485962\n",
      "Subject 19, Epoch 534, Loss: 0.6969985961914062, Final Batch Loss: 0.20543141663074493\n",
      "Subject 19, Epoch 535, Loss: 0.7264575809240341, Final Batch Loss: 0.1718941628932953\n",
      "Subject 19, Epoch 536, Loss: 0.8263729214668274, Final Batch Loss: 0.21793746948242188\n",
      "Subject 19, Epoch 537, Loss: 0.6992311626672745, Final Batch Loss: 0.21394090354442596\n",
      "Subject 19, Epoch 538, Loss: 0.6764727085828781, Final Batch Loss: 0.231580451130867\n",
      "Subject 19, Epoch 539, Loss: 0.7222785204648972, Final Batch Loss: 0.19224655628204346\n",
      "Subject 19, Epoch 540, Loss: 0.7129999250173569, Final Batch Loss: 0.26660606265068054\n",
      "Subject 19, Epoch 541, Loss: 0.7384282648563385, Final Batch Loss: 0.26454341411590576\n",
      "Subject 19, Epoch 542, Loss: 0.7099476903676987, Final Batch Loss: 0.32630863785743713\n",
      "Subject 19, Epoch 543, Loss: 0.6276433914899826, Final Batch Loss: 0.22694139182567596\n",
      "Subject 19, Epoch 544, Loss: 0.745578870177269, Final Batch Loss: 0.18860672414302826\n",
      "Subject 19, Epoch 545, Loss: 0.706297293305397, Final Batch Loss: 0.20696455240249634\n",
      "Subject 19, Epoch 546, Loss: 0.6764524132013321, Final Batch Loss: 0.23206521570682526\n",
      "Subject 19, Epoch 547, Loss: 0.692447230219841, Final Batch Loss: 0.2191917598247528\n",
      "Subject 19, Epoch 548, Loss: 0.6417168974876404, Final Batch Loss: 0.20221774280071259\n",
      "Subject 19, Epoch 549, Loss: 0.6628260910511017, Final Batch Loss: 0.259987473487854\n",
      "Subject 19, Epoch 550, Loss: 0.7708416283130646, Final Batch Loss: 0.23982830345630646\n",
      "Subject 19, Epoch 551, Loss: 0.7179094552993774, Final Batch Loss: 0.2865605354309082\n",
      "Subject 19, Epoch 552, Loss: 0.6187163144350052, Final Batch Loss: 0.11401915550231934\n",
      "Subject 19, Epoch 553, Loss: 0.738253191113472, Final Batch Loss: 0.3033316731452942\n",
      "Subject 19, Epoch 554, Loss: 0.6505349278450012, Final Batch Loss: 0.1879207342863083\n",
      "Subject 19, Epoch 555, Loss: 0.6699244827032089, Final Batch Loss: 0.2241566926240921\n",
      "Subject 19, Epoch 556, Loss: 0.6990479826927185, Final Batch Loss: 0.2208617627620697\n",
      "Subject 19, Epoch 557, Loss: 0.7089984714984894, Final Batch Loss: 0.17247934639453888\n",
      "Subject 19, Epoch 558, Loss: 0.6809591203927994, Final Batch Loss: 0.21771802008152008\n",
      "Subject 19, Epoch 559, Loss: 0.7191352844238281, Final Batch Loss: 0.2128523588180542\n",
      "Subject 19, Epoch 560, Loss: 0.6929347664117813, Final Batch Loss: 0.21496562659740448\n",
      "Subject 19, Epoch 561, Loss: 0.6858476549386978, Final Batch Loss: 0.2588217258453369\n",
      "Subject 19, Epoch 562, Loss: 0.8157075941562653, Final Batch Loss: 0.22348752617835999\n",
      "Subject 19, Epoch 563, Loss: 0.6666211634874344, Final Batch Loss: 0.236063152551651\n",
      "Subject 19, Epoch 564, Loss: 0.7553394138813019, Final Batch Loss: 0.24553854763507843\n",
      "Subject 19, Epoch 565, Loss: 0.7213807553052902, Final Batch Loss: 0.20217646658420563\n",
      "Subject 19, Epoch 566, Loss: 0.7279392331838608, Final Batch Loss: 0.21014843881130219\n",
      "Subject 19, Epoch 567, Loss: 0.6716816127300262, Final Batch Loss: 0.18985478579998016\n",
      "Subject 19, Epoch 568, Loss: 0.6498693525791168, Final Batch Loss: 0.1826903373003006\n",
      "Subject 19, Epoch 569, Loss: 0.7082404047250748, Final Batch Loss: 0.2594504952430725\n",
      "Subject 19, Epoch 570, Loss: 0.7529097646474838, Final Batch Loss: 0.2292158305644989\n",
      "Subject 19, Epoch 571, Loss: 0.6749983876943588, Final Batch Loss: 0.2356387823820114\n",
      "Subject 19, Epoch 572, Loss: 0.6488734930753708, Final Batch Loss: 0.22942915558815002\n",
      "Subject 19, Epoch 573, Loss: 0.7680712193250656, Final Batch Loss: 0.1870536357164383\n",
      "Subject 19, Epoch 574, Loss: 0.7231908589601517, Final Batch Loss: 0.22210079431533813\n",
      "Subject 19, Epoch 575, Loss: 0.6716207563877106, Final Batch Loss: 0.23027172684669495\n",
      "Subject 19, Epoch 576, Loss: 0.6972374022006989, Final Batch Loss: 0.2860304117202759\n",
      "Subject 19, Epoch 577, Loss: 0.6568185538053513, Final Batch Loss: 0.22858914732933044\n",
      "Subject 19, Epoch 578, Loss: 0.6303114593029022, Final Batch Loss: 0.21864090859889984\n",
      "Subject 19, Epoch 579, Loss: 0.7805713266134262, Final Batch Loss: 0.29005324840545654\n",
      "Subject 19, Epoch 580, Loss: 0.7382442951202393, Final Batch Loss: 0.3514341413974762\n",
      "Subject 19, Epoch 581, Loss: 0.6355285495519638, Final Batch Loss: 0.2274743765592575\n",
      "Subject 19, Epoch 582, Loss: 0.6510973274707794, Final Batch Loss: 0.21366570889949799\n",
      "Subject 19, Epoch 583, Loss: 0.6806865483522415, Final Batch Loss: 0.21867989003658295\n",
      "Subject 19, Epoch 584, Loss: 0.6418697983026505, Final Batch Loss: 0.19996079802513123\n",
      "Subject 19, Epoch 585, Loss: 0.6660131514072418, Final Batch Loss: 0.32022711634635925\n",
      "Subject 19, Epoch 586, Loss: 0.655524805188179, Final Batch Loss: 0.16347458958625793\n",
      "Subject 19, Epoch 587, Loss: 0.6766992062330246, Final Batch Loss: 0.14186152815818787\n",
      "Subject 19, Epoch 588, Loss: 0.7402181923389435, Final Batch Loss: 0.26575618982315063\n",
      "Subject 19, Epoch 589, Loss: 0.6292769312858582, Final Batch Loss: 0.25555717945098877\n",
      "Subject 19, Epoch 590, Loss: 0.6205524206161499, Final Batch Loss: 0.17450405657291412\n",
      "Subject 19, Epoch 591, Loss: 0.6503267288208008, Final Batch Loss: 0.3006316125392914\n",
      "Subject 19, Epoch 592, Loss: 0.6981953531503677, Final Batch Loss: 0.23341570794582367\n",
      "Subject 19, Epoch 593, Loss: 0.6817586421966553, Final Batch Loss: 0.21519747376441956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 594, Loss: 0.6692995429039001, Final Batch Loss: 0.19200961291790009\n",
      "Subject 19, Epoch 595, Loss: 0.6646671295166016, Final Batch Loss: 0.27704131603240967\n",
      "Subject 19, Epoch 596, Loss: 0.6193284541368484, Final Batch Loss: 0.15846094489097595\n",
      "Subject 19, Epoch 597, Loss: 0.6945514976978302, Final Batch Loss: 0.23437516391277313\n",
      "Subject 19, Epoch 598, Loss: 0.680494099855423, Final Batch Loss: 0.20123222470283508\n",
      "Subject 19, Epoch 599, Loss: 0.7106267809867859, Final Batch Loss: 0.17900170385837555\n",
      "Subject 19, Epoch 600, Loss: 0.6860378831624985, Final Batch Loss: 0.20668040215969086\n",
      "Subject 19, Epoch 601, Loss: 0.670494481921196, Final Batch Loss: 0.2441386580467224\n",
      "Subject 19, Epoch 602, Loss: 0.7007743865251541, Final Batch Loss: 0.3186059594154358\n",
      "Subject 19, Epoch 603, Loss: 0.6647768318653107, Final Batch Loss: 0.19037504494190216\n",
      "Subject 19, Epoch 604, Loss: 0.6263134181499481, Final Batch Loss: 0.24570466578006744\n",
      "Subject 19, Epoch 605, Loss: 0.6336710900068283, Final Batch Loss: 0.2128848433494568\n",
      "Subject 19, Epoch 606, Loss: 0.6242361813783646, Final Batch Loss: 0.20512162148952484\n",
      "Subject 19, Epoch 607, Loss: 0.6865634173154831, Final Batch Loss: 0.21581898629665375\n",
      "Subject 19, Epoch 608, Loss: 0.678138941526413, Final Batch Loss: 0.17761918902397156\n",
      "Subject 19, Epoch 609, Loss: 0.6433996707201004, Final Batch Loss: 0.23040883243083954\n",
      "Subject 19, Epoch 610, Loss: 0.6681347638368607, Final Batch Loss: 0.16464753448963165\n",
      "Subject 19, Epoch 611, Loss: 0.6396402418613434, Final Batch Loss: 0.31383612751960754\n",
      "Subject 19, Epoch 612, Loss: 0.6978795230388641, Final Batch Loss: 0.18699884414672852\n",
      "Subject 19, Epoch 613, Loss: 0.6569765955209732, Final Batch Loss: 0.2291291505098343\n",
      "Subject 19, Epoch 614, Loss: 0.6817421168088913, Final Batch Loss: 0.2604220509529114\n",
      "Subject 19, Epoch 615, Loss: 0.6192827075719833, Final Batch Loss: 0.23547086119651794\n",
      "Subject 19, Epoch 616, Loss: 0.6177181154489517, Final Batch Loss: 0.2260851263999939\n",
      "Subject 19, Epoch 617, Loss: 0.5935407131910324, Final Batch Loss: 0.2690238654613495\n",
      "Subject 19, Epoch 618, Loss: 0.6880691945552826, Final Batch Loss: 0.23591315746307373\n",
      "Subject 19, Epoch 619, Loss: 0.7515908032655716, Final Batch Loss: 0.19651682674884796\n",
      "Subject 19, Epoch 620, Loss: 0.6944203972816467, Final Batch Loss: 0.24252140522003174\n",
      "Subject 19, Epoch 621, Loss: 0.6568411886692047, Final Batch Loss: 0.15283694863319397\n",
      "Subject 19, Epoch 622, Loss: 0.6550962179899216, Final Batch Loss: 0.1619640290737152\n",
      "Subject 19, Epoch 623, Loss: 0.5691790580749512, Final Batch Loss: 0.20118285715579987\n",
      "Subject 19, Epoch 624, Loss: 0.6450034826993942, Final Batch Loss: 0.1720387041568756\n",
      "Subject 19, Epoch 625, Loss: 0.6216624677181244, Final Batch Loss: 0.18943601846694946\n",
      "Subject 19, Epoch 626, Loss: 0.6297752857208252, Final Batch Loss: 0.13328363001346588\n",
      "Subject 19, Epoch 627, Loss: 0.6338170915842056, Final Batch Loss: 0.1638481765985489\n",
      "Subject 19, Epoch 628, Loss: 0.6035828590393066, Final Batch Loss: 0.1669708788394928\n",
      "Subject 19, Epoch 629, Loss: 0.6729658544063568, Final Batch Loss: 0.2631995677947998\n",
      "Subject 19, Epoch 630, Loss: 0.7153375446796417, Final Batch Loss: 0.17180441319942474\n",
      "Subject 19, Epoch 631, Loss: 0.5944356173276901, Final Batch Loss: 0.20104476809501648\n",
      "Subject 19, Epoch 632, Loss: 0.6086060106754303, Final Batch Loss: 0.21538902819156647\n",
      "Subject 19, Epoch 633, Loss: 0.5915495008230209, Final Batch Loss: 0.180636465549469\n",
      "Subject 19, Epoch 634, Loss: 0.6488870084285736, Final Batch Loss: 0.19464664161205292\n",
      "Subject 19, Epoch 635, Loss: 0.6268825978040695, Final Batch Loss: 0.21147885918617249\n",
      "Subject 19, Epoch 636, Loss: 0.5663683414459229, Final Batch Loss: 0.12024126946926117\n",
      "Subject 19, Epoch 637, Loss: 0.6585816890001297, Final Batch Loss: 0.20775963366031647\n",
      "Subject 19, Epoch 638, Loss: 0.7192690074443817, Final Batch Loss: 0.23448266088962555\n",
      "Subject 19, Epoch 639, Loss: 0.6209625154733658, Final Batch Loss: 0.2794398367404938\n",
      "Subject 19, Epoch 640, Loss: 0.591901570558548, Final Batch Loss: 0.245441272854805\n",
      "Subject 19, Epoch 641, Loss: 0.6830359697341919, Final Batch Loss: 0.21018598973751068\n",
      "Subject 19, Epoch 642, Loss: 0.6147670298814774, Final Batch Loss: 0.1587045043706894\n",
      "Subject 19, Epoch 643, Loss: 0.690994992852211, Final Batch Loss: 0.16262231767177582\n",
      "Subject 19, Epoch 644, Loss: 0.5541282147169113, Final Batch Loss: 0.1707862913608551\n",
      "Subject 19, Epoch 645, Loss: 0.5981582254171371, Final Batch Loss: 0.1935703456401825\n",
      "Subject 19, Epoch 646, Loss: 0.7035884857177734, Final Batch Loss: 0.24180132150650024\n",
      "Subject 19, Epoch 647, Loss: 0.6323315352201462, Final Batch Loss: 0.259196400642395\n",
      "Subject 19, Epoch 648, Loss: 0.6492026597261429, Final Batch Loss: 0.2049732208251953\n",
      "Subject 19, Epoch 649, Loss: 0.5495173707604408, Final Batch Loss: 0.11945753544569016\n",
      "Subject 19, Epoch 650, Loss: 0.5718531906604767, Final Batch Loss: 0.19979311525821686\n",
      "Subject 19, Epoch 651, Loss: 0.5892619639635086, Final Batch Loss: 0.22999568283557892\n",
      "Subject 19, Epoch 652, Loss: 0.6262031346559525, Final Batch Loss: 0.20277118682861328\n",
      "Subject 19, Epoch 653, Loss: 0.5908622443675995, Final Batch Loss: 0.19498690962791443\n",
      "Subject 19, Epoch 654, Loss: 0.6424174010753632, Final Batch Loss: 0.2546389102935791\n",
      "Subject 19, Epoch 655, Loss: 0.5854109823703766, Final Batch Loss: 0.2109804004430771\n",
      "Subject 19, Epoch 656, Loss: 0.6232725828886032, Final Batch Loss: 0.21941690146923065\n",
      "Subject 19, Epoch 657, Loss: 0.5688490122556686, Final Batch Loss: 0.1496230512857437\n",
      "Subject 19, Epoch 658, Loss: 0.613206297159195, Final Batch Loss: 0.2792436480522156\n",
      "Subject 19, Epoch 659, Loss: 0.6062833219766617, Final Batch Loss: 0.2017332762479782\n",
      "Subject 19, Epoch 660, Loss: 0.7446058392524719, Final Batch Loss: 0.23817932605743408\n",
      "Subject 19, Epoch 661, Loss: 0.5777452290058136, Final Batch Loss: 0.15487001836299896\n",
      "Subject 19, Epoch 662, Loss: 0.5711421966552734, Final Batch Loss: 0.19877572357654572\n",
      "Subject 19, Epoch 663, Loss: 0.596595898270607, Final Batch Loss: 0.18370597064495087\n",
      "Subject 19, Epoch 664, Loss: 0.6220172047615051, Final Batch Loss: 0.2197035849094391\n",
      "Subject 19, Epoch 665, Loss: 0.5858685970306396, Final Batch Loss: 0.17290198802947998\n",
      "Subject 19, Epoch 666, Loss: 0.6399926245212555, Final Batch Loss: 0.2239857167005539\n",
      "Subject 19, Epoch 667, Loss: 0.6428491473197937, Final Batch Loss: 0.15817084908485413\n",
      "Subject 19, Epoch 668, Loss: 0.6391461044549942, Final Batch Loss: 0.19162976741790771\n",
      "Subject 19, Epoch 669, Loss: 0.6225959956645966, Final Batch Loss: 0.18643225729465485\n",
      "Subject 19, Epoch 670, Loss: 0.654409646987915, Final Batch Loss: 0.23600620031356812\n",
      "Subject 19, Epoch 671, Loss: 0.6311976313591003, Final Batch Loss: 0.23825381696224213\n",
      "Subject 19, Epoch 672, Loss: 0.6439923942089081, Final Batch Loss: 0.20348316431045532\n",
      "Subject 19, Epoch 673, Loss: 0.5518798530101776, Final Batch Loss: 0.22818465530872345\n",
      "Subject 19, Epoch 674, Loss: 0.6587134450674057, Final Batch Loss: 0.24719886481761932\n",
      "Subject 19, Epoch 675, Loss: 0.5767406970262527, Final Batch Loss: 0.20228709280490875\n",
      "Subject 19, Epoch 676, Loss: 0.5909801423549652, Final Batch Loss: 0.19889949262142181\n",
      "Subject 19, Epoch 677, Loss: 0.6372489482164383, Final Batch Loss: 0.24203850328922272\n",
      "Subject 19, Epoch 678, Loss: 0.6314580738544464, Final Batch Loss: 0.22279642522335052\n",
      "Subject 19, Epoch 679, Loss: 0.6474646776914597, Final Batch Loss: 0.22741268575191498\n",
      "Subject 19, Epoch 680, Loss: 0.644620418548584, Final Batch Loss: 0.15318521857261658\n",
      "Subject 19, Epoch 681, Loss: 0.543038859963417, Final Batch Loss: 0.18523122370243073\n",
      "Subject 19, Epoch 682, Loss: 0.673388883471489, Final Batch Loss: 0.17993132770061493\n",
      "Subject 19, Epoch 683, Loss: 0.6271041482686996, Final Batch Loss: 0.19717641174793243\n",
      "Subject 19, Epoch 684, Loss: 0.586891233921051, Final Batch Loss: 0.22834523022174835\n",
      "Subject 19, Epoch 685, Loss: 0.6045284271240234, Final Batch Loss: 0.19818730652332306\n",
      "Subject 19, Epoch 686, Loss: 0.5680407881736755, Final Batch Loss: 0.24672847986221313\n",
      "Subject 19, Epoch 687, Loss: 0.6183368265628815, Final Batch Loss: 0.21163807809352875\n",
      "Subject 19, Epoch 688, Loss: 0.5755839198827744, Final Batch Loss: 0.13322272896766663\n",
      "Subject 19, Epoch 689, Loss: 0.637601688504219, Final Batch Loss: 0.22269116342067719\n",
      "Subject 19, Epoch 690, Loss: 0.5185179114341736, Final Batch Loss: 0.16487593948841095\n",
      "Subject 19, Epoch 691, Loss: 0.5638344138860703, Final Batch Loss: 0.21123158931732178\n",
      "Subject 19, Epoch 692, Loss: 0.5690588504076004, Final Batch Loss: 0.23078051209449768\n",
      "Subject 19, Epoch 693, Loss: 0.587108239531517, Final Batch Loss: 0.14958547055721283\n",
      "Subject 19, Epoch 694, Loss: 0.5658125132322311, Final Batch Loss: 0.22118858993053436\n",
      "Subject 19, Epoch 695, Loss: 0.628640741109848, Final Batch Loss: 0.20225068926811218\n",
      "Subject 19, Epoch 696, Loss: 0.5553543716669083, Final Batch Loss: 0.18139517307281494\n",
      "Subject 19, Epoch 697, Loss: 0.563503310084343, Final Batch Loss: 0.1797538846731186\n",
      "Subject 19, Epoch 698, Loss: 0.5954674035310745, Final Batch Loss: 0.19716839492321014\n",
      "Subject 19, Epoch 699, Loss: 0.6102094203233719, Final Batch Loss: 0.2404361367225647\n",
      "Subject 19, Epoch 700, Loss: 0.5699852406978607, Final Batch Loss: 0.1729961633682251\n",
      "Subject 19, Epoch 701, Loss: 0.521617516875267, Final Batch Loss: 0.1605895757675171\n",
      "Subject 19, Epoch 702, Loss: 0.5599091351032257, Final Batch Loss: 0.1662193238735199\n",
      "Subject 19, Epoch 703, Loss: 0.5828330218791962, Final Batch Loss: 0.21926970779895782\n",
      "Subject 19, Epoch 704, Loss: 0.527171403169632, Final Batch Loss: 0.2117045372724533\n",
      "Subject 19, Epoch 705, Loss: 0.5788127481937408, Final Batch Loss: 0.18848849833011627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 706, Loss: 0.5977665185928345, Final Batch Loss: 0.2085622251033783\n",
      "Subject 19, Epoch 707, Loss: 0.5858303010463715, Final Batch Loss: 0.1877775639295578\n",
      "Subject 19, Epoch 708, Loss: 0.581884577870369, Final Batch Loss: 0.14623378217220306\n",
      "Subject 19, Epoch 709, Loss: 0.5936229526996613, Final Batch Loss: 0.21196530759334564\n",
      "Subject 19, Epoch 710, Loss: 0.6768931895494461, Final Batch Loss: 0.2126879096031189\n",
      "Subject 19, Epoch 711, Loss: 0.6086997389793396, Final Batch Loss: 0.21370410919189453\n",
      "Subject 19, Epoch 712, Loss: 0.6481200307607651, Final Batch Loss: 0.24257878959178925\n",
      "Subject 19, Epoch 713, Loss: 0.5244740694761276, Final Batch Loss: 0.1483454555273056\n",
      "Subject 19, Epoch 714, Loss: 0.5832385569810867, Final Batch Loss: 0.16122472286224365\n",
      "Subject 19, Epoch 715, Loss: 0.5523168444633484, Final Batch Loss: 0.1963328719139099\n",
      "Subject 19, Epoch 716, Loss: 0.5691535025835037, Final Batch Loss: 0.15648113191127777\n",
      "Subject 19, Epoch 717, Loss: 0.656053438782692, Final Batch Loss: 0.19381195306777954\n",
      "Subject 19, Epoch 718, Loss: 0.570182204246521, Final Batch Loss: 0.13458146154880524\n",
      "Subject 19, Epoch 719, Loss: 0.611147366464138, Final Batch Loss: 0.09071587771177292\n",
      "Subject 19, Epoch 720, Loss: 0.6014348268508911, Final Batch Loss: 0.150354266166687\n",
      "Subject 19, Epoch 721, Loss: 0.5975981503725052, Final Batch Loss: 0.15196126699447632\n",
      "Subject 19, Epoch 722, Loss: 0.5473842620849609, Final Batch Loss: 0.17559322714805603\n",
      "Subject 19, Epoch 723, Loss: 0.49640560150146484, Final Batch Loss: 0.13108539581298828\n",
      "Subject 19, Epoch 724, Loss: 0.4864015430212021, Final Batch Loss: 0.14538279175758362\n",
      "Subject 19, Epoch 725, Loss: 0.5686556249856949, Final Batch Loss: 0.15678973495960236\n",
      "Subject 19, Epoch 726, Loss: 0.5150688886642456, Final Batch Loss: 0.17180944979190826\n",
      "Subject 19, Epoch 727, Loss: 0.7152025699615479, Final Batch Loss: 0.27988940477371216\n",
      "Subject 19, Epoch 728, Loss: 0.5250751227140427, Final Batch Loss: 0.15393134951591492\n",
      "Subject 19, Epoch 729, Loss: 0.573043942451477, Final Batch Loss: 0.19824859499931335\n",
      "Subject 19, Epoch 730, Loss: 0.5477338284254074, Final Batch Loss: 0.1905204802751541\n",
      "Subject 19, Epoch 731, Loss: 0.5452375411987305, Final Batch Loss: 0.16984809935092926\n",
      "Subject 19, Epoch 732, Loss: 0.6143544912338257, Final Batch Loss: 0.20602644979953766\n",
      "Subject 19, Epoch 733, Loss: 0.5362997874617577, Final Batch Loss: 0.2539290487766266\n",
      "Subject 19, Epoch 734, Loss: 0.5948053747415543, Final Batch Loss: 0.1267562210559845\n",
      "Subject 19, Epoch 735, Loss: 0.5230366736650467, Final Batch Loss: 0.18769894540309906\n",
      "Subject 19, Epoch 736, Loss: 0.588437870144844, Final Batch Loss: 0.18494641780853271\n",
      "Subject 19, Epoch 737, Loss: 0.5362173020839691, Final Batch Loss: 0.19338898360729218\n",
      "Subject 19, Epoch 738, Loss: 0.592893585562706, Final Batch Loss: 0.1524161845445633\n",
      "Subject 19, Epoch 739, Loss: 0.5922755748033524, Final Batch Loss: 0.2211788147687912\n",
      "Subject 19, Epoch 740, Loss: 0.5009837001562119, Final Batch Loss: 0.15430611371994019\n",
      "Subject 19, Epoch 741, Loss: 0.5921550691127777, Final Batch Loss: 0.22786636650562286\n",
      "Subject 19, Epoch 742, Loss: 0.621615469455719, Final Batch Loss: 0.18013693392276764\n",
      "Subject 19, Epoch 743, Loss: 0.520988866686821, Final Batch Loss: 0.19259090721607208\n",
      "Subject 19, Epoch 744, Loss: 0.549376368522644, Final Batch Loss: 0.18893533945083618\n",
      "Subject 19, Epoch 745, Loss: 0.5164424926042557, Final Batch Loss: 0.14244018495082855\n",
      "Subject 19, Epoch 746, Loss: 0.5362291038036346, Final Batch Loss: 0.18092548847198486\n",
      "Subject 19, Epoch 747, Loss: 0.5125199407339096, Final Batch Loss: 0.11503686010837555\n",
      "Subject 19, Epoch 748, Loss: 0.6050892025232315, Final Batch Loss: 0.24039433896541595\n",
      "Subject 19, Epoch 749, Loss: 0.5655794888734818, Final Batch Loss: 0.12498293817043304\n",
      "Subject 19, Epoch 750, Loss: 0.567657083272934, Final Batch Loss: 0.18785756826400757\n",
      "Subject 19, Epoch 751, Loss: 0.5848756432533264, Final Batch Loss: 0.19737796485424042\n",
      "Subject 19, Epoch 752, Loss: 0.5220006257295609, Final Batch Loss: 0.19236786663532257\n",
      "Subject 19, Epoch 753, Loss: 0.48464547097682953, Final Batch Loss: 0.17168596386909485\n",
      "Subject 19, Epoch 754, Loss: 0.6001148372888565, Final Batch Loss: 0.20095320045948029\n",
      "Subject 19, Epoch 755, Loss: 0.579730287194252, Final Batch Loss: 0.15654636919498444\n",
      "Subject 19, Epoch 756, Loss: 0.7094317078590393, Final Batch Loss: 0.26362377405166626\n",
      "Subject 19, Epoch 757, Loss: 0.5669207125902176, Final Batch Loss: 0.2463727444410324\n",
      "Subject 19, Epoch 758, Loss: 0.5147122889757156, Final Batch Loss: 0.15621323883533478\n",
      "Subject 19, Epoch 759, Loss: 0.4698780030012131, Final Batch Loss: 0.15412619709968567\n",
      "Subject 19, Epoch 760, Loss: 0.5082997977733612, Final Batch Loss: 0.16276784241199493\n",
      "Subject 19, Epoch 761, Loss: 0.5131266266107559, Final Batch Loss: 0.2327873855829239\n",
      "Subject 19, Epoch 762, Loss: 0.5974125415086746, Final Batch Loss: 0.18625439703464508\n",
      "Subject 19, Epoch 763, Loss: 0.5153823494911194, Final Batch Loss: 0.11369967460632324\n",
      "Subject 19, Epoch 764, Loss: 0.5537078827619553, Final Batch Loss: 0.20276959240436554\n",
      "Subject 19, Epoch 765, Loss: 0.4813598543405533, Final Batch Loss: 0.11399130523204803\n",
      "Subject 19, Epoch 766, Loss: 0.5325692817568779, Final Batch Loss: 0.11791060119867325\n",
      "Subject 19, Epoch 767, Loss: 0.6157204657793045, Final Batch Loss: 0.22468438744544983\n",
      "Subject 19, Epoch 768, Loss: 0.5165031850337982, Final Batch Loss: 0.23024150729179382\n",
      "Subject 19, Epoch 769, Loss: 0.6063977926969528, Final Batch Loss: 0.18110133707523346\n",
      "Subject 19, Epoch 770, Loss: 0.5470479130744934, Final Batch Loss: 0.18703994154930115\n",
      "Subject 19, Epoch 771, Loss: 0.5834067612886429, Final Batch Loss: 0.1773502677679062\n",
      "Subject 19, Epoch 772, Loss: 0.6265338510274887, Final Batch Loss: 0.27371689677238464\n",
      "Subject 19, Epoch 773, Loss: 0.43474820256233215, Final Batch Loss: 0.16116547584533691\n",
      "Subject 19, Epoch 774, Loss: 0.5804296880960464, Final Batch Loss: 0.19760917127132416\n",
      "Subject 19, Epoch 775, Loss: 0.740921288728714, Final Batch Loss: 0.3506702482700348\n",
      "Subject 19, Epoch 776, Loss: 0.610081136226654, Final Batch Loss: 0.19710420072078705\n",
      "Subject 19, Epoch 777, Loss: 0.4912771135568619, Final Batch Loss: 0.13974151015281677\n",
      "Subject 19, Epoch 778, Loss: 0.5349741280078888, Final Batch Loss: 0.09168529510498047\n",
      "Subject 19, Epoch 779, Loss: 0.48061978816986084, Final Batch Loss: 0.1012326329946518\n",
      "Subject 19, Epoch 780, Loss: 0.5891970470547676, Final Batch Loss: 0.26993101835250854\n",
      "Subject 19, Epoch 781, Loss: 0.5077897161245346, Final Batch Loss: 0.14871440827846527\n",
      "Subject 19, Epoch 782, Loss: 0.5726500153541565, Final Batch Loss: 0.2288624495267868\n",
      "Subject 19, Epoch 783, Loss: 0.5554944276809692, Final Batch Loss: 0.15918339788913727\n",
      "Subject 19, Epoch 784, Loss: 0.5524022877216339, Final Batch Loss: 0.20164692401885986\n",
      "Subject 19, Epoch 785, Loss: 0.5849367529153824, Final Batch Loss: 0.15714053809642792\n",
      "Subject 19, Epoch 786, Loss: 0.504729188978672, Final Batch Loss: 0.15875963866710663\n",
      "Subject 19, Epoch 787, Loss: 0.6060852557420731, Final Batch Loss: 0.18310841917991638\n",
      "Subject 19, Epoch 788, Loss: 0.5269217640161514, Final Batch Loss: 0.18565820157527924\n",
      "Subject 19, Epoch 789, Loss: 0.5851288735866547, Final Batch Loss: 0.16105106472969055\n",
      "Subject 19, Epoch 790, Loss: 0.5098768472671509, Final Batch Loss: 0.14109109342098236\n",
      "Subject 19, Epoch 791, Loss: 0.5161895602941513, Final Batch Loss: 0.16465573012828827\n",
      "Subject 19, Epoch 792, Loss: 0.4694390892982483, Final Batch Loss: 0.15914572775363922\n",
      "Subject 19, Epoch 793, Loss: 0.4979403167963028, Final Batch Loss: 0.15407781302928925\n",
      "Subject 19, Epoch 794, Loss: 0.5382599979639053, Final Batch Loss: 0.15405912697315216\n",
      "Subject 19, Epoch 795, Loss: 0.4892589747905731, Final Batch Loss: 0.17199242115020752\n",
      "Subject 19, Epoch 796, Loss: 0.5193959027528763, Final Batch Loss: 0.1357589215040207\n",
      "Subject 19, Epoch 797, Loss: 0.4710998833179474, Final Batch Loss: 0.16525410115718842\n",
      "Subject 19, Epoch 798, Loss: 0.4885499179363251, Final Batch Loss: 0.20182038843631744\n",
      "Subject 19, Epoch 799, Loss: 0.5267919152975082, Final Batch Loss: 0.16531728208065033\n",
      "Subject 19, Epoch 800, Loss: 0.4970211312174797, Final Batch Loss: 0.1964118331670761\n",
      "Subject 19, Epoch 801, Loss: 0.46558865159749985, Final Batch Loss: 0.11620741337537766\n",
      "Subject 19, Epoch 802, Loss: 0.4976353198289871, Final Batch Loss: 0.1906396448612213\n",
      "Subject 19, Epoch 803, Loss: 0.4626714140176773, Final Batch Loss: 0.15486347675323486\n",
      "Subject 19, Epoch 804, Loss: 0.458450123667717, Final Batch Loss: 0.18681633472442627\n",
      "Subject 19, Epoch 805, Loss: 0.4615047127008438, Final Batch Loss: 0.13489370048046112\n",
      "Subject 19, Epoch 806, Loss: 0.490818552672863, Final Batch Loss: 0.17641256749629974\n",
      "Subject 19, Epoch 807, Loss: 0.48129063099622726, Final Batch Loss: 0.19465585052967072\n",
      "Subject 19, Epoch 808, Loss: 0.41311196237802505, Final Batch Loss: 0.09458225965499878\n",
      "Subject 19, Epoch 809, Loss: 0.46967629343271255, Final Batch Loss: 0.1473265290260315\n",
      "Subject 19, Epoch 810, Loss: 0.4625549390912056, Final Batch Loss: 0.16896221041679382\n",
      "Subject 19, Epoch 811, Loss: 0.4856738969683647, Final Batch Loss: 0.20443686842918396\n",
      "Subject 19, Epoch 812, Loss: 0.5049088597297668, Final Batch Loss: 0.0981951504945755\n",
      "Subject 19, Epoch 813, Loss: 0.49186670780181885, Final Batch Loss: 0.15902161598205566\n",
      "Subject 19, Epoch 814, Loss: 0.4405512884259224, Final Batch Loss: 0.14542217552661896\n",
      "Subject 19, Epoch 815, Loss: 0.5877073854207993, Final Batch Loss: 0.232204869389534\n",
      "Subject 19, Epoch 816, Loss: 0.40774644911289215, Final Batch Loss: 0.12279767543077469\n",
      "Subject 19, Epoch 817, Loss: 0.465706929564476, Final Batch Loss: 0.17748208343982697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 818, Loss: 0.49377401918172836, Final Batch Loss: 0.14168614149093628\n",
      "Subject 19, Epoch 819, Loss: 0.5768755078315735, Final Batch Loss: 0.27707669138908386\n",
      "Subject 19, Epoch 820, Loss: 0.5300015807151794, Final Batch Loss: 0.1513558179140091\n",
      "Subject 19, Epoch 821, Loss: 0.5205594748258591, Final Batch Loss: 0.18052594363689423\n",
      "Subject 19, Epoch 822, Loss: 0.616874948143959, Final Batch Loss: 0.21669593453407288\n",
      "Subject 19, Epoch 823, Loss: 0.5474847853183746, Final Batch Loss: 0.14961838722229004\n",
      "Subject 19, Epoch 824, Loss: 0.6235232204198837, Final Batch Loss: 0.2853800058364868\n",
      "Subject 19, Epoch 825, Loss: 0.5271666646003723, Final Batch Loss: 0.1325029879808426\n",
      "Subject 19, Epoch 826, Loss: 0.5579580664634705, Final Batch Loss: 0.2076391577720642\n",
      "Subject 19, Epoch 827, Loss: 0.42386066913604736, Final Batch Loss: 0.1328032910823822\n",
      "Subject 19, Epoch 828, Loss: 0.47357597947120667, Final Batch Loss: 0.20347358286380768\n",
      "Subject 19, Epoch 829, Loss: 0.479459673166275, Final Batch Loss: 0.16926981508731842\n",
      "Subject 19, Epoch 830, Loss: 0.4482614994049072, Final Batch Loss: 0.12669110298156738\n",
      "Subject 19, Epoch 831, Loss: 0.4559157192707062, Final Batch Loss: 0.12871716916561127\n",
      "Subject 19, Epoch 832, Loss: 0.4515550509095192, Final Batch Loss: 0.1411980390548706\n",
      "Subject 19, Epoch 833, Loss: 0.4907119572162628, Final Batch Loss: 0.13861697912216187\n",
      "Subject 19, Epoch 834, Loss: 0.4844849407672882, Final Batch Loss: 0.13531965017318726\n",
      "Subject 19, Epoch 835, Loss: 0.5120100751519203, Final Batch Loss: 0.3014484643936157\n",
      "Subject 19, Epoch 836, Loss: 0.3802763670682907, Final Batch Loss: 0.10431966930627823\n",
      "Subject 19, Epoch 837, Loss: 0.4598129540681839, Final Batch Loss: 0.14072753489017487\n",
      "Subject 19, Epoch 838, Loss: 0.4933284521102905, Final Batch Loss: 0.1501203179359436\n",
      "Subject 19, Epoch 839, Loss: 0.5514572486281395, Final Batch Loss: 0.15126945078372955\n",
      "Subject 19, Epoch 840, Loss: 0.43675878643989563, Final Batch Loss: 0.20250152051448822\n",
      "Subject 19, Epoch 841, Loss: 0.4593432992696762, Final Batch Loss: 0.17287462949752808\n",
      "Subject 19, Epoch 842, Loss: 0.43429166078567505, Final Batch Loss: 0.13903918862342834\n",
      "Subject 19, Epoch 843, Loss: 0.5029465407133102, Final Batch Loss: 0.17861530184745789\n",
      "Subject 19, Epoch 844, Loss: 0.5045676678419113, Final Batch Loss: 0.13783742487430573\n",
      "Subject 19, Epoch 845, Loss: 0.45277661085128784, Final Batch Loss: 0.13493682444095612\n",
      "Subject 19, Epoch 846, Loss: 0.49011150002479553, Final Batch Loss: 0.15771138668060303\n",
      "Subject 19, Epoch 847, Loss: 0.5003912895917892, Final Batch Loss: 0.1288730800151825\n",
      "Subject 19, Epoch 848, Loss: 0.5886004418134689, Final Batch Loss: 0.19020450115203857\n",
      "Subject 19, Epoch 849, Loss: 0.38457072526216507, Final Batch Loss: 0.0766570046544075\n",
      "Subject 19, Epoch 850, Loss: 0.5164127498865128, Final Batch Loss: 0.16363833844661713\n",
      "Subject 19, Epoch 851, Loss: 0.45580071210861206, Final Batch Loss: 0.1470167636871338\n",
      "Subject 19, Epoch 852, Loss: 0.5159038305282593, Final Batch Loss: 0.2566341161727905\n",
      "Subject 19, Epoch 853, Loss: 0.4759896546602249, Final Batch Loss: 0.11657267808914185\n",
      "Subject 19, Epoch 854, Loss: 0.46729836612939835, Final Batch Loss: 0.14048434793949127\n",
      "Subject 19, Epoch 855, Loss: 0.4556982219219208, Final Batch Loss: 0.15795886516571045\n",
      "Subject 19, Epoch 856, Loss: 0.46122588962316513, Final Batch Loss: 0.10676548629999161\n",
      "Subject 19, Epoch 857, Loss: 0.4164225608110428, Final Batch Loss: 0.14705690741539001\n",
      "Subject 19, Epoch 858, Loss: 0.49530354142189026, Final Batch Loss: 0.178094282746315\n",
      "Subject 19, Epoch 859, Loss: 0.411161869764328, Final Batch Loss: 0.10905483365058899\n",
      "Subject 19, Epoch 860, Loss: 0.46194134652614594, Final Batch Loss: 0.20261473953723907\n",
      "Subject 19, Epoch 861, Loss: 0.4756460338830948, Final Batch Loss: 0.1611572802066803\n",
      "Subject 19, Epoch 862, Loss: 0.5100448429584503, Final Batch Loss: 0.21987174451351166\n",
      "Subject 19, Epoch 863, Loss: 0.48554594069719315, Final Batch Loss: 0.27072250843048096\n",
      "Subject 19, Epoch 864, Loss: 0.47503969073295593, Final Batch Loss: 0.14283260703086853\n",
      "Subject 19, Epoch 865, Loss: 0.4359940364956856, Final Batch Loss: 0.10272226482629776\n",
      "Subject 19, Epoch 866, Loss: 0.3547695279121399, Final Batch Loss: 0.11419765651226044\n",
      "Subject 19, Epoch 867, Loss: 0.46593551337718964, Final Batch Loss: 0.14433418214321136\n",
      "Subject 19, Epoch 868, Loss: 0.43786416202783585, Final Batch Loss: 0.11395563930273056\n",
      "Subject 19, Epoch 869, Loss: 0.4636053144931793, Final Batch Loss: 0.1991567760705948\n",
      "Subject 19, Epoch 870, Loss: 0.4700610339641571, Final Batch Loss: 0.1679118126630783\n",
      "Subject 19, Epoch 871, Loss: 0.42344360053539276, Final Batch Loss: 0.15601536631584167\n",
      "Subject 19, Epoch 872, Loss: 0.49034346640110016, Final Batch Loss: 0.22080089151859283\n",
      "Subject 19, Epoch 873, Loss: 0.4353250116109848, Final Batch Loss: 0.19501733779907227\n",
      "Subject 19, Epoch 874, Loss: 0.34887397289276123, Final Batch Loss: 0.07082679122686386\n",
      "Subject 19, Epoch 875, Loss: 0.40340862423181534, Final Batch Loss: 0.14508728682994843\n",
      "Subject 19, Epoch 876, Loss: 0.4931366518139839, Final Batch Loss: 0.11705110222101212\n",
      "Subject 19, Epoch 877, Loss: 0.4576500356197357, Final Batch Loss: 0.1201772689819336\n",
      "Subject 19, Epoch 878, Loss: 0.6123644858598709, Final Batch Loss: 0.297075092792511\n",
      "Subject 19, Epoch 879, Loss: 0.5100487023591995, Final Batch Loss: 0.1748739778995514\n",
      "Subject 19, Epoch 880, Loss: 0.48502781242132187, Final Batch Loss: 0.11333473771810532\n",
      "Subject 19, Epoch 881, Loss: 0.49744604527950287, Final Batch Loss: 0.1338086724281311\n",
      "Subject 19, Epoch 882, Loss: 0.4678866118192673, Final Batch Loss: 0.13363878428936005\n",
      "Subject 19, Epoch 883, Loss: 0.4717394560575485, Final Batch Loss: 0.15509483218193054\n",
      "Subject 19, Epoch 884, Loss: 0.4306359142065048, Final Batch Loss: 0.14694733917713165\n",
      "Subject 19, Epoch 885, Loss: 0.4718199372291565, Final Batch Loss: 0.1455298811197281\n",
      "Subject 19, Epoch 886, Loss: 0.4360364228487015, Final Batch Loss: 0.14487792551517487\n",
      "Subject 19, Epoch 887, Loss: 0.5434388965368271, Final Batch Loss: 0.10877908021211624\n",
      "Subject 19, Epoch 888, Loss: 0.4463713839650154, Final Batch Loss: 0.19429658353328705\n",
      "Subject 19, Epoch 889, Loss: 0.5156823098659515, Final Batch Loss: 0.22714899480342865\n",
      "Subject 19, Epoch 890, Loss: 0.4982764422893524, Final Batch Loss: 0.15878483653068542\n",
      "Subject 19, Epoch 891, Loss: 0.4000776335597038, Final Batch Loss: 0.13538601994514465\n",
      "Subject 19, Epoch 892, Loss: 0.4172357842326164, Final Batch Loss: 0.16481861472129822\n",
      "Subject 19, Epoch 893, Loss: 0.4711460769176483, Final Batch Loss: 0.1810871809720993\n",
      "Subject 19, Epoch 894, Loss: 0.474270723760128, Final Batch Loss: 0.1652812510728836\n",
      "Subject 19, Epoch 895, Loss: 0.45229028165340424, Final Batch Loss: 0.16941893100738525\n",
      "Subject 19, Epoch 896, Loss: 0.3817809820175171, Final Batch Loss: 0.13300324976444244\n",
      "Subject 19, Epoch 897, Loss: 0.40964425355196, Final Batch Loss: 0.08570992201566696\n",
      "Subject 19, Epoch 898, Loss: 0.49061721563339233, Final Batch Loss: 0.1348646879196167\n",
      "Subject 19, Epoch 899, Loss: 0.40255967527627945, Final Batch Loss: 0.15786384046077728\n",
      "Subject 19, Epoch 900, Loss: 0.5085413530468941, Final Batch Loss: 0.13548842072486877\n",
      "Subject 19, Epoch 901, Loss: 0.42947524040937424, Final Batch Loss: 0.1826062798500061\n",
      "Subject 19, Epoch 902, Loss: 0.3931335061788559, Final Batch Loss: 0.1395818591117859\n",
      "Subject 19, Epoch 903, Loss: 0.3853696137666702, Final Batch Loss: 0.08962199091911316\n",
      "Subject 19, Epoch 904, Loss: 0.5014845579862595, Final Batch Loss: 0.13836294412612915\n",
      "Subject 19, Epoch 905, Loss: 0.5618580132722855, Final Batch Loss: 0.22828911244869232\n",
      "Subject 19, Epoch 906, Loss: 0.4764536917209625, Final Batch Loss: 0.17257331311702728\n",
      "Subject 19, Epoch 907, Loss: 0.47434114664793015, Final Batch Loss: 0.1077883169054985\n",
      "Subject 19, Epoch 908, Loss: 0.478605255484581, Final Batch Loss: 0.13600341975688934\n",
      "Subject 19, Epoch 909, Loss: 0.5332618206739426, Final Batch Loss: 0.15826700627803802\n",
      "Subject 19, Epoch 910, Loss: 0.4715869054198265, Final Batch Loss: 0.11833224445581436\n",
      "Subject 19, Epoch 911, Loss: 0.4269850179553032, Final Batch Loss: 0.19768953323364258\n",
      "Subject 19, Epoch 912, Loss: 0.3924091085791588, Final Batch Loss: 0.13776017725467682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19, Epoch 913, Loss: 0.43630827963352203, Final Batch Loss: 0.16352157294750214\n",
      "Subject 19, Epoch 914, Loss: 0.4617810994386673, Final Batch Loss: 0.13842609524726868\n",
      "Subject 19, Epoch 915, Loss: 0.43488599359989166, Final Batch Loss: 0.13748255372047424\n",
      "Subject 19, Epoch 916, Loss: 0.4124970883131027, Final Batch Loss: 0.1306125968694687\n",
      "Subject 19, Epoch 917, Loss: 0.4222304970026016, Final Batch Loss: 0.1298341304063797\n",
      "Subject 19, Epoch 918, Loss: 0.435317724943161, Final Batch Loss: 0.0875435620546341\n",
      "Subject 19, Epoch 919, Loss: 0.4923155680298805, Final Batch Loss: 0.10956016927957535\n",
      "Subject 19, Epoch 920, Loss: 0.36527783423662186, Final Batch Loss: 0.09827550500631332\n",
      "Subject 19, Epoch 921, Loss: 0.42101215571165085, Final Batch Loss: 0.19374503195285797\n",
      "Subject 19, Epoch 922, Loss: 0.41054144501686096, Final Batch Loss: 0.17528212070465088\n",
      "Subject 19, Epoch 923, Loss: 0.4103461131453514, Final Batch Loss: 0.11771753430366516\n",
      "Subject 19, Epoch 924, Loss: 0.5576970875263214, Final Batch Loss: 0.20250645279884338\n",
      "Subject 19, Epoch 925, Loss: 0.36825621128082275, Final Batch Loss: 0.12927387654781342\n",
      "Subject 19, Epoch 926, Loss: 0.43815814703702927, Final Batch Loss: 0.17547999322414398\n",
      "Subject 19, Epoch 927, Loss: 0.5130902379751205, Final Batch Loss: 0.11738494038581848\n",
      "Subject 19, Epoch 928, Loss: 0.3597262352705002, Final Batch Loss: 0.17112591862678528\n",
      "Subject 19, Epoch 929, Loss: 0.38816532492637634, Final Batch Loss: 0.15932048857212067\n",
      "Subject 19, Epoch 930, Loss: 0.4771484509110451, Final Batch Loss: 0.22428493201732635\n",
      "Subject 19, Epoch 931, Loss: 0.47342950105667114, Final Batch Loss: 0.12828095257282257\n",
      "Subject 19, Epoch 932, Loss: 0.43871136009693146, Final Batch Loss: 0.1973484754562378\n",
      "Subject 19, Epoch 933, Loss: 0.3590715825557709, Final Batch Loss: 0.08122548460960388\n",
      "Subject 19, Epoch 934, Loss: 0.42445479333400726, Final Batch Loss: 0.0859868973493576\n",
      "Subject 19, Epoch 935, Loss: 0.4173797070980072, Final Batch Loss: 0.09610965847969055\n",
      "Subject 19, Epoch 936, Loss: 0.37605614960193634, Final Batch Loss: 0.15000386536121368\n",
      "Subject 19, Epoch 937, Loss: 0.37637536972761154, Final Batch Loss: 0.0926729366183281\n",
      "Subject 19, Epoch 938, Loss: 0.27909085154533386, Final Batch Loss: 0.08710972964763641\n",
      "Subject 19, Epoch 939, Loss: 0.3681474104523659, Final Batch Loss: 0.11436880379915237\n",
      "Subject 19, Epoch 940, Loss: 0.3928757384419441, Final Batch Loss: 0.15901519358158112\n",
      "Subject 19, Epoch 941, Loss: 0.4359225034713745, Final Batch Loss: 0.1604601889848709\n",
      "Subject 19, Epoch 942, Loss: 0.4084182530641556, Final Batch Loss: 0.1168154627084732\n",
      "Subject 19, Epoch 943, Loss: 0.37196530401706696, Final Batch Loss: 0.1374267339706421\n",
      "Subject 19, Epoch 944, Loss: 0.3172426074743271, Final Batch Loss: 0.10860040038824081\n",
      "Subject 19, Epoch 945, Loss: 0.5970309227705002, Final Batch Loss: 0.15990591049194336\n",
      "Subject 19, Epoch 946, Loss: 0.3679429069161415, Final Batch Loss: 0.1824439913034439\n",
      "Subject 19, Epoch 947, Loss: 0.3525794520974159, Final Batch Loss: 0.13139627873897552\n",
      "Subject 19, Epoch 948, Loss: 0.3718194514513016, Final Batch Loss: 0.06445103883743286\n",
      "Subject 19, Epoch 949, Loss: 0.49765104055404663, Final Batch Loss: 0.1810048371553421\n",
      "Subject 19, Epoch 950, Loss: 0.37932197749614716, Final Batch Loss: 0.09688016772270203\n",
      "Subject 19, Epoch 951, Loss: 0.4280496835708618, Final Batch Loss: 0.1515548676252365\n",
      "Subject 19, Epoch 952, Loss: 0.3614974766969681, Final Batch Loss: 0.08291225135326385\n",
      "Subject 19, Epoch 953, Loss: 0.4489656761288643, Final Batch Loss: 0.07715261727571487\n",
      "Subject 19, Epoch 954, Loss: 0.3917436823248863, Final Batch Loss: 0.14312516152858734\n",
      "Subject 19, Epoch 955, Loss: 0.48112286627292633, Final Batch Loss: 0.09634701907634735\n",
      "Subject 19, Epoch 956, Loss: 0.37089163064956665, Final Batch Loss: 0.09861765801906586\n",
      "Subject 19, Epoch 957, Loss: 0.40107353031635284, Final Batch Loss: 0.13517701625823975\n",
      "Subject 19, Epoch 958, Loss: 0.39375392347574234, Final Batch Loss: 0.1045149490237236\n",
      "Subject 19, Epoch 959, Loss: 0.35719510167837143, Final Batch Loss: 0.11224327981472015\n",
      "Subject 19, Epoch 960, Loss: 0.3832702860236168, Final Batch Loss: 0.13881322741508484\n",
      "Subject 19, Epoch 961, Loss: 0.35853663086891174, Final Batch Loss: 0.11783324182033539\n",
      "Subject 19, Epoch 962, Loss: 0.3454236760735512, Final Batch Loss: 0.09260708093643188\n",
      "Subject 19, Epoch 963, Loss: 0.38047878444194794, Final Batch Loss: 0.15838810801506042\n",
      "Subject 19, Epoch 964, Loss: 0.36895714700222015, Final Batch Loss: 0.12240082025527954\n",
      "Subject 19, Epoch 965, Loss: 0.36845074594020844, Final Batch Loss: 0.08712427318096161\n",
      "Subject 19, Epoch 966, Loss: 0.3904222622513771, Final Batch Loss: 0.1422155350446701\n",
      "Subject 19, Epoch 967, Loss: 0.354319728910923, Final Batch Loss: 0.11478191614151001\n",
      "Subject 19, Epoch 968, Loss: 0.3295292630791664, Final Batch Loss: 0.1302337348461151\n",
      "Subject 19, Epoch 969, Loss: 0.33355095237493515, Final Batch Loss: 0.07442561537027359\n",
      "Subject 19, Epoch 970, Loss: 0.4396027997136116, Final Batch Loss: 0.12369947135448456\n",
      "Subject 19, Epoch 971, Loss: 0.3619471341371536, Final Batch Loss: 0.08346432447433472\n",
      "Subject 19, Epoch 972, Loss: 0.4128755107522011, Final Batch Loss: 0.13106243312358856\n",
      "Subject 19, Epoch 973, Loss: 0.4043671563267708, Final Batch Loss: 0.1064598485827446\n",
      "Subject 19, Epoch 974, Loss: 0.3861887753009796, Final Batch Loss: 0.13590888679027557\n",
      "Subject 19, Epoch 975, Loss: 0.45835042744874954, Final Batch Loss: 0.19400301575660706\n",
      "Subject 19, Epoch 976, Loss: 0.36395465582609177, Final Batch Loss: 0.07290055602788925\n",
      "Subject 19, Epoch 977, Loss: 0.404449000954628, Final Batch Loss: 0.08745959401130676\n",
      "Subject 19, Epoch 978, Loss: 0.3911813944578171, Final Batch Loss: 0.15518070757389069\n",
      "Subject 19, Epoch 979, Loss: 0.5165746733546257, Final Batch Loss: 0.11656630784273148\n",
      "Subject 19, Epoch 980, Loss: 0.37656039744615555, Final Batch Loss: 0.13810496032238007\n",
      "Subject 19, Epoch 981, Loss: 0.4512154906988144, Final Batch Loss: 0.16574887931346893\n",
      "Subject 19, Epoch 982, Loss: 0.3865882456302643, Final Batch Loss: 0.08661752939224243\n",
      "Subject 19, Epoch 983, Loss: 0.34970708936452866, Final Batch Loss: 0.11801367253065109\n",
      "Subject 19, Epoch 984, Loss: 0.48231394588947296, Final Batch Loss: 0.1221381276845932\n",
      "Subject 19, Epoch 985, Loss: 0.33667828142642975, Final Batch Loss: 0.15156573057174683\n",
      "Subject 19, Epoch 986, Loss: 0.4389374628663063, Final Batch Loss: 0.11096008867025375\n",
      "Subject 19, Epoch 987, Loss: 0.3684089332818985, Final Batch Loss: 0.11164387315511703\n",
      "Subject 19, Epoch 988, Loss: 0.3736971840262413, Final Batch Loss: 0.15084347128868103\n",
      "Subject 19, Epoch 989, Loss: 0.36756277084350586, Final Batch Loss: 0.14269794523715973\n",
      "Subject 19, Epoch 990, Loss: 0.4305766224861145, Final Batch Loss: 0.10290087759494781\n",
      "Subject 19, Epoch 991, Loss: 0.36860542744398117, Final Batch Loss: 0.06682441383600235\n",
      "Subject 19, Epoch 992, Loss: 0.3697533458471298, Final Batch Loss: 0.15170590579509735\n",
      "Subject 19, Epoch 993, Loss: 0.4105381444096565, Final Batch Loss: 0.1055377796292305\n",
      "Subject 19, Epoch 994, Loss: 0.4014555439352989, Final Batch Loss: 0.15802589058876038\n",
      "Subject 19, Epoch 995, Loss: 0.4528384506702423, Final Batch Loss: 0.20771312713623047\n",
      "Subject 19, Epoch 996, Loss: 0.4519181624054909, Final Batch Loss: 0.18749713897705078\n",
      "Subject 19, Epoch 997, Loss: 0.43736178427934647, Final Batch Loss: 0.12789595127105713\n",
      "Subject 19, Epoch 998, Loss: 0.5046812742948532, Final Batch Loss: 0.16705265641212463\n",
      "Subject 19, Epoch 999, Loss: 0.3764156103134155, Final Batch Loss: 0.17490509152412415\n",
      "Subject 19, Epoch 1000, Loss: 0.4396917223930359, Final Batch Loss: 0.11592081189155579\n",
      "Subject 20, Epoch 1, Loss: 7.224955677986145, Final Batch Loss: 1.8107038736343384\n",
      "Subject 20, Epoch 2, Loss: 7.204412341117859, Final Batch Loss: 1.7968121767044067\n",
      "Subject 20, Epoch 3, Loss: 7.2161489725112915, Final Batch Loss: 1.8312325477600098\n",
      "Subject 20, Epoch 4, Loss: 7.178724050521851, Final Batch Loss: 1.7951979637145996\n",
      "Subject 20, Epoch 5, Loss: 7.151089191436768, Final Batch Loss: 1.7878895998001099\n",
      "Subject 20, Epoch 6, Loss: 7.138037800788879, Final Batch Loss: 1.7745568752288818\n",
      "Subject 20, Epoch 7, Loss: 7.136072874069214, Final Batch Loss: 1.7911229133605957\n",
      "Subject 20, Epoch 8, Loss: 7.079040169715881, Final Batch Loss: 1.7377076148986816\n",
      "Subject 20, Epoch 9, Loss: 7.095304012298584, Final Batch Loss: 1.7810282707214355\n",
      "Subject 20, Epoch 10, Loss: 7.0538904666900635, Final Batch Loss: 1.7631068229675293\n",
      "Subject 20, Epoch 11, Loss: 7.007236480712891, Final Batch Loss: 1.7572287321090698\n",
      "Subject 20, Epoch 12, Loss: 6.927754759788513, Final Batch Loss: 1.7256720066070557\n",
      "Subject 20, Epoch 13, Loss: 6.863691329956055, Final Batch Loss: 1.704802393913269\n",
      "Subject 20, Epoch 14, Loss: 6.7731674909591675, Final Batch Loss: 1.6768851280212402\n",
      "Subject 20, Epoch 15, Loss: 6.5903565883636475, Final Batch Loss: 1.5985873937606812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 16, Loss: 6.42387318611145, Final Batch Loss: 1.5398451089859009\n",
      "Subject 20, Epoch 17, Loss: 6.423132538795471, Final Batch Loss: 1.6403802633285522\n",
      "Subject 20, Epoch 18, Loss: 6.1711729764938354, Final Batch Loss: 1.4662315845489502\n",
      "Subject 20, Epoch 19, Loss: 6.0555548667907715, Final Batch Loss: 1.5382319688796997\n",
      "Subject 20, Epoch 20, Loss: 5.9340773820877075, Final Batch Loss: 1.5039657354354858\n",
      "Subject 20, Epoch 21, Loss: 5.749865412712097, Final Batch Loss: 1.3864288330078125\n",
      "Subject 20, Epoch 22, Loss: 5.583054065704346, Final Batch Loss: 1.2574684619903564\n",
      "Subject 20, Epoch 23, Loss: 5.3929442167282104, Final Batch Loss: 1.2856636047363281\n",
      "Subject 20, Epoch 24, Loss: 5.356025338172913, Final Batch Loss: 1.3663623332977295\n",
      "Subject 20, Epoch 25, Loss: 5.241158723831177, Final Batch Loss: 1.2870434522628784\n",
      "Subject 20, Epoch 26, Loss: 5.275053143501282, Final Batch Loss: 1.3477096557617188\n",
      "Subject 20, Epoch 27, Loss: 5.174532055854797, Final Batch Loss: 1.3202850818634033\n",
      "Subject 20, Epoch 28, Loss: 5.043140769004822, Final Batch Loss: 1.3072468042373657\n",
      "Subject 20, Epoch 29, Loss: 4.899636745452881, Final Batch Loss: 1.2314046621322632\n",
      "Subject 20, Epoch 30, Loss: 4.949521541595459, Final Batch Loss: 1.2199379205703735\n",
      "Subject 20, Epoch 31, Loss: 4.920535087585449, Final Batch Loss: 1.2696014642715454\n",
      "Subject 20, Epoch 32, Loss: 4.903066873550415, Final Batch Loss: 1.2832417488098145\n",
      "Subject 20, Epoch 33, Loss: 4.610840201377869, Final Batch Loss: 1.0736829042434692\n",
      "Subject 20, Epoch 34, Loss: 4.594179630279541, Final Batch Loss: 1.1011193990707397\n",
      "Subject 20, Epoch 35, Loss: 4.600720047950745, Final Batch Loss: 1.080854058265686\n",
      "Subject 20, Epoch 36, Loss: 4.610769987106323, Final Batch Loss: 1.1211971044540405\n",
      "Subject 20, Epoch 37, Loss: 4.594414114952087, Final Batch Loss: 1.1425237655639648\n",
      "Subject 20, Epoch 38, Loss: 4.672059178352356, Final Batch Loss: 1.2205811738967896\n",
      "Subject 20, Epoch 39, Loss: 4.692659139633179, Final Batch Loss: 1.1783726215362549\n",
      "Subject 20, Epoch 40, Loss: 4.648293375968933, Final Batch Loss: 1.1825451850891113\n",
      "Subject 20, Epoch 41, Loss: 4.581078767776489, Final Batch Loss: 1.1701083183288574\n",
      "Subject 20, Epoch 42, Loss: 4.451110482215881, Final Batch Loss: 1.0499262809753418\n",
      "Subject 20, Epoch 43, Loss: 4.585089206695557, Final Batch Loss: 1.1725250482559204\n",
      "Subject 20, Epoch 44, Loss: 4.625848174095154, Final Batch Loss: 1.1970221996307373\n",
      "Subject 20, Epoch 45, Loss: 4.4847869873046875, Final Batch Loss: 1.0904587507247925\n",
      "Subject 20, Epoch 46, Loss: 4.630579352378845, Final Batch Loss: 1.2177430391311646\n",
      "Subject 20, Epoch 47, Loss: 4.500310897827148, Final Batch Loss: 1.115017294883728\n",
      "Subject 20, Epoch 48, Loss: 4.546709895133972, Final Batch Loss: 1.155186653137207\n",
      "Subject 20, Epoch 49, Loss: 4.543924450874329, Final Batch Loss: 1.1285209655761719\n",
      "Subject 20, Epoch 50, Loss: 4.481017708778381, Final Batch Loss: 1.1512436866760254\n",
      "Subject 20, Epoch 51, Loss: 4.5497965812683105, Final Batch Loss: 1.1558207273483276\n",
      "Subject 20, Epoch 52, Loss: 4.443631529808044, Final Batch Loss: 1.1210628747940063\n",
      "Subject 20, Epoch 53, Loss: 4.541875958442688, Final Batch Loss: 1.1492630243301392\n",
      "Subject 20, Epoch 54, Loss: 4.468306183815002, Final Batch Loss: 1.1337794065475464\n",
      "Subject 20, Epoch 55, Loss: 4.428130626678467, Final Batch Loss: 1.1045337915420532\n",
      "Subject 20, Epoch 56, Loss: 4.341103553771973, Final Batch Loss: 1.0831489562988281\n",
      "Subject 20, Epoch 57, Loss: 4.353380799293518, Final Batch Loss: 1.0757157802581787\n",
      "Subject 20, Epoch 58, Loss: 4.373186945915222, Final Batch Loss: 1.006590485572815\n",
      "Subject 20, Epoch 59, Loss: 4.297259330749512, Final Batch Loss: 1.0518418550491333\n",
      "Subject 20, Epoch 60, Loss: 4.326191544532776, Final Batch Loss: 1.060227870941162\n",
      "Subject 20, Epoch 61, Loss: 4.354064583778381, Final Batch Loss: 1.134061336517334\n",
      "Subject 20, Epoch 62, Loss: 4.293522357940674, Final Batch Loss: 1.0498301982879639\n",
      "Subject 20, Epoch 63, Loss: 4.2364490032196045, Final Batch Loss: 1.0940767526626587\n",
      "Subject 20, Epoch 64, Loss: 4.178932547569275, Final Batch Loss: 1.0381089448928833\n",
      "Subject 20, Epoch 65, Loss: 4.183928608894348, Final Batch Loss: 1.0424342155456543\n",
      "Subject 20, Epoch 66, Loss: 4.163178086280823, Final Batch Loss: 1.0358086824417114\n",
      "Subject 20, Epoch 67, Loss: 4.231879234313965, Final Batch Loss: 1.0691750049591064\n",
      "Subject 20, Epoch 68, Loss: 4.098861932754517, Final Batch Loss: 1.018546462059021\n",
      "Subject 20, Epoch 69, Loss: 4.068589448928833, Final Batch Loss: 0.9619781970977783\n",
      "Subject 20, Epoch 70, Loss: 4.151981234550476, Final Batch Loss: 1.1227729320526123\n",
      "Subject 20, Epoch 71, Loss: 4.012322187423706, Final Batch Loss: 0.9551448822021484\n",
      "Subject 20, Epoch 72, Loss: 3.9717191457748413, Final Batch Loss: 0.9856498837471008\n",
      "Subject 20, Epoch 73, Loss: 3.9239598512649536, Final Batch Loss: 0.9752516746520996\n",
      "Subject 20, Epoch 74, Loss: 3.9534718990325928, Final Batch Loss: 1.0049995183944702\n",
      "Subject 20, Epoch 75, Loss: 3.946598768234253, Final Batch Loss: 0.980338454246521\n",
      "Subject 20, Epoch 76, Loss: 3.713925302028656, Final Batch Loss: 0.8227776885032654\n",
      "Subject 20, Epoch 77, Loss: 3.752570867538452, Final Batch Loss: 0.9245312213897705\n",
      "Subject 20, Epoch 78, Loss: 3.7219016551971436, Final Batch Loss: 0.8877090215682983\n",
      "Subject 20, Epoch 79, Loss: 3.653577148914337, Final Batch Loss: 0.8292698860168457\n",
      "Subject 20, Epoch 80, Loss: 3.55012845993042, Final Batch Loss: 0.8629562854766846\n",
      "Subject 20, Epoch 81, Loss: 3.7391965985298157, Final Batch Loss: 1.026047945022583\n",
      "Subject 20, Epoch 82, Loss: 3.603808641433716, Final Batch Loss: 0.78302401304245\n",
      "Subject 20, Epoch 83, Loss: 3.4869295358657837, Final Batch Loss: 0.8028348684310913\n",
      "Subject 20, Epoch 84, Loss: 3.5660811066627502, Final Batch Loss: 0.9820128083229065\n",
      "Subject 20, Epoch 85, Loss: 3.43201744556427, Final Batch Loss: 0.8356619477272034\n",
      "Subject 20, Epoch 86, Loss: 3.4553021788597107, Final Batch Loss: 0.9280545115470886\n",
      "Subject 20, Epoch 87, Loss: 3.5112804174423218, Final Batch Loss: 0.8887822031974792\n",
      "Subject 20, Epoch 88, Loss: 3.3795183897018433, Final Batch Loss: 0.839255690574646\n",
      "Subject 20, Epoch 89, Loss: 3.414979875087738, Final Batch Loss: 0.8324494361877441\n",
      "Subject 20, Epoch 90, Loss: 3.234127163887024, Final Batch Loss: 0.6879188418388367\n",
      "Subject 20, Epoch 91, Loss: 3.3682445883750916, Final Batch Loss: 0.8378389477729797\n",
      "Subject 20, Epoch 92, Loss: 3.3010054230690002, Final Batch Loss: 0.846526563167572\n",
      "Subject 20, Epoch 93, Loss: 3.260086238384247, Final Batch Loss: 0.945586621761322\n",
      "Subject 20, Epoch 94, Loss: 3.1018102169036865, Final Batch Loss: 0.7623298168182373\n",
      "Subject 20, Epoch 95, Loss: 3.290438175201416, Final Batch Loss: 0.9407610893249512\n",
      "Subject 20, Epoch 96, Loss: 3.361295521259308, Final Batch Loss: 0.8814952969551086\n",
      "Subject 20, Epoch 97, Loss: 3.046819269657135, Final Batch Loss: 0.7086631655693054\n",
      "Subject 20, Epoch 98, Loss: 3.023551285266876, Final Batch Loss: 0.706265926361084\n",
      "Subject 20, Epoch 99, Loss: 3.029254972934723, Final Batch Loss: 0.6219441294670105\n",
      "Subject 20, Epoch 100, Loss: 3.179210662841797, Final Batch Loss: 0.836283266544342\n",
      "Subject 20, Epoch 101, Loss: 3.03398060798645, Final Batch Loss: 0.7955461740493774\n",
      "Subject 20, Epoch 102, Loss: 3.056721568107605, Final Batch Loss: 0.8101141452789307\n",
      "Subject 20, Epoch 103, Loss: 2.9218615889549255, Final Batch Loss: 0.6698747277259827\n",
      "Subject 20, Epoch 104, Loss: 2.9266804456710815, Final Batch Loss: 0.737597644329071\n",
      "Subject 20, Epoch 105, Loss: 3.1811277866363525, Final Batch Loss: 0.9214703440666199\n",
      "Subject 20, Epoch 106, Loss: 2.960351347923279, Final Batch Loss: 0.8011643886566162\n",
      "Subject 20, Epoch 107, Loss: 2.7999890446662903, Final Batch Loss: 0.581421434879303\n",
      "Subject 20, Epoch 108, Loss: 3.0907915830612183, Final Batch Loss: 0.8947685360908508\n",
      "Subject 20, Epoch 109, Loss: 2.911240577697754, Final Batch Loss: 0.8068735003471375\n",
      "Subject 20, Epoch 110, Loss: 3.0406768321990967, Final Batch Loss: 0.8993898034095764\n",
      "Subject 20, Epoch 111, Loss: 2.903527855873108, Final Batch Loss: 0.7889342308044434\n",
      "Subject 20, Epoch 112, Loss: 2.8077112436294556, Final Batch Loss: 0.6618033647537231\n",
      "Subject 20, Epoch 113, Loss: 2.8140562176704407, Final Batch Loss: 0.7597422003746033\n",
      "Subject 20, Epoch 114, Loss: 2.6483753323554993, Final Batch Loss: 0.5338847637176514\n",
      "Subject 20, Epoch 115, Loss: 2.636422038078308, Final Batch Loss: 0.6215019822120667\n",
      "Subject 20, Epoch 116, Loss: 2.8770740628242493, Final Batch Loss: 0.8018632531166077\n",
      "Subject 20, Epoch 117, Loss: 2.6944496035575867, Final Batch Loss: 0.6326366066932678\n",
      "Subject 20, Epoch 118, Loss: 2.6654054522514343, Final Batch Loss: 0.7147659063339233\n",
      "Subject 20, Epoch 119, Loss: 2.646304488182068, Final Batch Loss: 0.6487312912940979\n",
      "Subject 20, Epoch 120, Loss: 2.914100468158722, Final Batch Loss: 0.7955727577209473\n",
      "Subject 20, Epoch 121, Loss: 2.5461424589157104, Final Batch Loss: 0.5231645107269287\n",
      "Subject 20, Epoch 122, Loss: 2.6629468202590942, Final Batch Loss: 0.6255934834480286\n",
      "Subject 20, Epoch 123, Loss: 2.624044954776764, Final Batch Loss: 0.7210800051689148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 124, Loss: 2.726100265979767, Final Batch Loss: 0.696093738079071\n",
      "Subject 20, Epoch 125, Loss: 2.3192940056324005, Final Batch Loss: 0.4987378418445587\n",
      "Subject 20, Epoch 126, Loss: 2.8070679306983948, Final Batch Loss: 0.8457366228103638\n",
      "Subject 20, Epoch 127, Loss: 2.5066254138946533, Final Batch Loss: 0.6100171804428101\n",
      "Subject 20, Epoch 128, Loss: 2.448277473449707, Final Batch Loss: 0.5654422640800476\n",
      "Subject 20, Epoch 129, Loss: 2.56331729888916, Final Batch Loss: 0.6913955807685852\n",
      "Subject 20, Epoch 130, Loss: 2.4665504097938538, Final Batch Loss: 0.6106468439102173\n",
      "Subject 20, Epoch 131, Loss: 2.436589479446411, Final Batch Loss: 0.44352126121520996\n",
      "Subject 20, Epoch 132, Loss: 2.2765667140483856, Final Batch Loss: 0.4429261386394501\n",
      "Subject 20, Epoch 133, Loss: 2.3735716342926025, Final Batch Loss: 0.564358115196228\n",
      "Subject 20, Epoch 134, Loss: 2.4021588563919067, Final Batch Loss: 0.6465159058570862\n",
      "Subject 20, Epoch 135, Loss: 2.26624995470047, Final Batch Loss: 0.4591161012649536\n",
      "Subject 20, Epoch 136, Loss: 2.3153539896011353, Final Batch Loss: 0.5292543172836304\n",
      "Subject 20, Epoch 137, Loss: 2.2448405623435974, Final Batch Loss: 0.5280572772026062\n",
      "Subject 20, Epoch 138, Loss: 2.5446918606758118, Final Batch Loss: 0.7354761362075806\n",
      "Subject 20, Epoch 139, Loss: 2.351835757493973, Final Batch Loss: 0.7143902778625488\n",
      "Subject 20, Epoch 140, Loss: 2.6105828285217285, Final Batch Loss: 0.7327024936676025\n",
      "Subject 20, Epoch 141, Loss: 2.2635443806648254, Final Batch Loss: 0.57522052526474\n",
      "Subject 20, Epoch 142, Loss: 2.2508024871349335, Final Batch Loss: 0.4808711111545563\n",
      "Subject 20, Epoch 143, Loss: 2.376651108264923, Final Batch Loss: 0.717661440372467\n",
      "Subject 20, Epoch 144, Loss: 2.4059550762176514, Final Batch Loss: 0.687059760093689\n",
      "Subject 20, Epoch 145, Loss: 2.0780719220638275, Final Batch Loss: 0.3042304217815399\n",
      "Subject 20, Epoch 146, Loss: 2.1609610319137573, Final Batch Loss: 0.5440808534622192\n",
      "Subject 20, Epoch 147, Loss: 2.2982747554779053, Final Batch Loss: 0.5646977424621582\n",
      "Subject 20, Epoch 148, Loss: 2.199624687433243, Final Batch Loss: 0.6154626607894897\n",
      "Subject 20, Epoch 149, Loss: 2.3227126002311707, Final Batch Loss: 0.48418110609054565\n",
      "Subject 20, Epoch 150, Loss: 2.2431678771972656, Final Batch Loss: 0.6063395738601685\n",
      "Subject 20, Epoch 151, Loss: 2.139557361602783, Final Batch Loss: 0.5637226104736328\n",
      "Subject 20, Epoch 152, Loss: 2.161181628704071, Final Batch Loss: 0.43443888425827026\n",
      "Subject 20, Epoch 153, Loss: 2.3287856578826904, Final Batch Loss: 0.7240278720855713\n",
      "Subject 20, Epoch 154, Loss: 1.9907526075839996, Final Batch Loss: 0.4548477232456207\n",
      "Subject 20, Epoch 155, Loss: 2.237941265106201, Final Batch Loss: 0.6612247228622437\n",
      "Subject 20, Epoch 156, Loss: 2.0732619762420654, Final Batch Loss: 0.5475980043411255\n",
      "Subject 20, Epoch 157, Loss: 2.2724063396453857, Final Batch Loss: 0.597252368927002\n",
      "Subject 20, Epoch 158, Loss: 2.313575893640518, Final Batch Loss: 0.621831476688385\n",
      "Subject 20, Epoch 159, Loss: 2.2720643281936646, Final Batch Loss: 0.6135531067848206\n",
      "Subject 20, Epoch 160, Loss: 2.0298972129821777, Final Batch Loss: 0.44578003883361816\n",
      "Subject 20, Epoch 161, Loss: 2.0572435557842255, Final Batch Loss: 0.4987487494945526\n",
      "Subject 20, Epoch 162, Loss: 2.271833747625351, Final Batch Loss: 0.5682497620582581\n",
      "Subject 20, Epoch 163, Loss: 2.2263879776000977, Final Batch Loss: 0.6511905789375305\n",
      "Subject 20, Epoch 164, Loss: 2.1671248376369476, Final Batch Loss: 0.7071007490158081\n",
      "Subject 20, Epoch 165, Loss: 2.0105133056640625, Final Batch Loss: 0.4489908218383789\n",
      "Subject 20, Epoch 166, Loss: 2.0653316974639893, Final Batch Loss: 0.5201893448829651\n",
      "Subject 20, Epoch 167, Loss: 2.099953383207321, Final Batch Loss: 0.436247318983078\n",
      "Subject 20, Epoch 168, Loss: 2.1028436422348022, Final Batch Loss: 0.5119318962097168\n",
      "Subject 20, Epoch 169, Loss: 2.1069333851337433, Final Batch Loss: 0.6740714311599731\n",
      "Subject 20, Epoch 170, Loss: 2.08650141954422, Final Batch Loss: 0.40861305594444275\n",
      "Subject 20, Epoch 171, Loss: 2.1329556107521057, Final Batch Loss: 0.6528502702713013\n",
      "Subject 20, Epoch 172, Loss: 1.99803227186203, Final Batch Loss: 0.48921072483062744\n",
      "Subject 20, Epoch 173, Loss: 2.0231071412563324, Final Batch Loss: 0.46633145213127136\n",
      "Subject 20, Epoch 174, Loss: 2.114664375782013, Final Batch Loss: 0.5393922924995422\n",
      "Subject 20, Epoch 175, Loss: 1.884442389011383, Final Batch Loss: 0.4692332446575165\n",
      "Subject 20, Epoch 176, Loss: 1.8628047704696655, Final Batch Loss: 0.4075504243373871\n",
      "Subject 20, Epoch 177, Loss: 2.089406907558441, Final Batch Loss: 0.6542463898658752\n",
      "Subject 20, Epoch 178, Loss: 1.9621202647686005, Final Batch Loss: 0.6254312992095947\n",
      "Subject 20, Epoch 179, Loss: 1.862761229276657, Final Batch Loss: 0.46084538102149963\n",
      "Subject 20, Epoch 180, Loss: 2.2142675817012787, Final Batch Loss: 0.641659140586853\n",
      "Subject 20, Epoch 181, Loss: 1.9736023843288422, Final Batch Loss: 0.4804217219352722\n",
      "Subject 20, Epoch 182, Loss: 2.1436944007873535, Final Batch Loss: 0.5823104977607727\n",
      "Subject 20, Epoch 183, Loss: 2.165058881044388, Final Batch Loss: 0.7159671187400818\n",
      "Subject 20, Epoch 184, Loss: 2.2792885303497314, Final Batch Loss: 0.7377526760101318\n",
      "Subject 20, Epoch 185, Loss: 1.9199433326721191, Final Batch Loss: 0.5511233806610107\n",
      "Subject 20, Epoch 186, Loss: 1.8142155408859253, Final Batch Loss: 0.3966439962387085\n",
      "Subject 20, Epoch 187, Loss: 1.9566794335842133, Final Batch Loss: 0.42940399050712585\n",
      "Subject 20, Epoch 188, Loss: 1.9103321433067322, Final Batch Loss: 0.4162253141403198\n",
      "Subject 20, Epoch 189, Loss: 2.1038548052310944, Final Batch Loss: 0.5792335867881775\n",
      "Subject 20, Epoch 190, Loss: 2.056913912296295, Final Batch Loss: 0.48336657881736755\n",
      "Subject 20, Epoch 191, Loss: 1.9549220204353333, Final Batch Loss: 0.46026620268821716\n",
      "Subject 20, Epoch 192, Loss: 1.9206234216690063, Final Batch Loss: 0.5024135708808899\n",
      "Subject 20, Epoch 193, Loss: 1.956660658121109, Final Batch Loss: 0.5716872215270996\n",
      "Subject 20, Epoch 194, Loss: 1.924899935722351, Final Batch Loss: 0.48722130060195923\n",
      "Subject 20, Epoch 195, Loss: 2.179107815027237, Final Batch Loss: 0.7533043026924133\n",
      "Subject 20, Epoch 196, Loss: 1.9591703116893768, Final Batch Loss: 0.4533993899822235\n",
      "Subject 20, Epoch 197, Loss: 1.840038150548935, Final Batch Loss: 0.4221709370613098\n",
      "Subject 20, Epoch 198, Loss: 1.7077154517173767, Final Batch Loss: 0.4062490165233612\n",
      "Subject 20, Epoch 199, Loss: 1.8702355027198792, Final Batch Loss: 0.49834591150283813\n",
      "Subject 20, Epoch 200, Loss: 1.8942480385303497, Final Batch Loss: 0.5327371954917908\n",
      "Subject 20, Epoch 201, Loss: 1.7885269820690155, Final Batch Loss: 0.4578504264354706\n",
      "Subject 20, Epoch 202, Loss: 1.7779754996299744, Final Batch Loss: 0.37886127829551697\n",
      "Subject 20, Epoch 203, Loss: 1.7436546683311462, Final Batch Loss: 0.3486238121986389\n",
      "Subject 20, Epoch 204, Loss: 1.8018572330474854, Final Batch Loss: 0.4247060716152191\n",
      "Subject 20, Epoch 205, Loss: 1.8555708229541779, Final Batch Loss: 0.44738543033599854\n",
      "Subject 20, Epoch 206, Loss: 1.864696741104126, Final Batch Loss: 0.5106921195983887\n",
      "Subject 20, Epoch 207, Loss: 1.803903728723526, Final Batch Loss: 0.48011311888694763\n",
      "Subject 20, Epoch 208, Loss: 1.8909066319465637, Final Batch Loss: 0.5505097508430481\n",
      "Subject 20, Epoch 209, Loss: 1.7081353664398193, Final Batch Loss: 0.4430283010005951\n",
      "Subject 20, Epoch 210, Loss: 1.909660816192627, Final Batch Loss: 0.558488667011261\n",
      "Subject 20, Epoch 211, Loss: 1.8116476237773895, Final Batch Loss: 0.38221049308776855\n",
      "Subject 20, Epoch 212, Loss: 1.8224916756153107, Final Batch Loss: 0.48385223746299744\n",
      "Subject 20, Epoch 213, Loss: 1.6756038963794708, Final Batch Loss: 0.38700535893440247\n",
      "Subject 20, Epoch 214, Loss: 2.062797784805298, Final Batch Loss: 0.5960992574691772\n",
      "Subject 20, Epoch 215, Loss: 1.848430871963501, Final Batch Loss: 0.4664582312107086\n",
      "Subject 20, Epoch 216, Loss: 1.632103592157364, Final Batch Loss: 0.3088349401950836\n",
      "Subject 20, Epoch 217, Loss: 1.9930670261383057, Final Batch Loss: 0.6678616404533386\n",
      "Subject 20, Epoch 218, Loss: 1.9987905323505402, Final Batch Loss: 0.6895087957382202\n",
      "Subject 20, Epoch 219, Loss: 1.8334453105926514, Final Batch Loss: 0.42248889803886414\n",
      "Subject 20, Epoch 220, Loss: 1.7611959874629974, Final Batch Loss: 0.362522691488266\n",
      "Subject 20, Epoch 221, Loss: 1.7651510834693909, Final Batch Loss: 0.3924124240875244\n",
      "Subject 20, Epoch 222, Loss: 1.6935526430606842, Final Batch Loss: 0.41456249356269836\n",
      "Subject 20, Epoch 223, Loss: 1.6347197443246841, Final Batch Loss: 0.24181102216243744\n",
      "Subject 20, Epoch 224, Loss: 1.701570749282837, Final Batch Loss: 0.42198893427848816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 225, Loss: 1.7105069756507874, Final Batch Loss: 0.387989342212677\n",
      "Subject 20, Epoch 226, Loss: 1.6219123005867004, Final Batch Loss: 0.31018292903900146\n",
      "Subject 20, Epoch 227, Loss: 1.8376766443252563, Final Batch Loss: 0.46832412481307983\n",
      "Subject 20, Epoch 228, Loss: 1.714556097984314, Final Batch Loss: 0.3331722319126129\n",
      "Subject 20, Epoch 229, Loss: 1.7114487886428833, Final Batch Loss: 0.3621147572994232\n",
      "Subject 20, Epoch 230, Loss: 1.8007846772670746, Final Batch Loss: 0.44313088059425354\n",
      "Subject 20, Epoch 231, Loss: 1.8076664805412292, Final Batch Loss: 0.4697522819042206\n",
      "Subject 20, Epoch 232, Loss: 1.7199118733406067, Final Batch Loss: 0.4325433373451233\n",
      "Subject 20, Epoch 233, Loss: 1.6942890584468842, Final Batch Loss: 0.3476305902004242\n",
      "Subject 20, Epoch 234, Loss: 1.7760489881038666, Final Batch Loss: 0.399669349193573\n",
      "Subject 20, Epoch 235, Loss: 1.7924529314041138, Final Batch Loss: 0.5676912665367126\n",
      "Subject 20, Epoch 236, Loss: 1.6558693945407867, Final Batch Loss: 0.43526703119277954\n",
      "Subject 20, Epoch 237, Loss: 1.6727874875068665, Final Batch Loss: 0.4859546422958374\n",
      "Subject 20, Epoch 238, Loss: 1.8173230588436127, Final Batch Loss: 0.42604687809944153\n",
      "Subject 20, Epoch 239, Loss: 1.4152446687221527, Final Batch Loss: 0.15079045295715332\n",
      "Subject 20, Epoch 240, Loss: 1.6338199079036713, Final Batch Loss: 0.33611905574798584\n",
      "Subject 20, Epoch 241, Loss: 1.8037033379077911, Final Batch Loss: 0.7169535160064697\n",
      "Subject 20, Epoch 242, Loss: 1.658509612083435, Final Batch Loss: 0.4825357496738434\n",
      "Subject 20, Epoch 243, Loss: 1.607473999261856, Final Batch Loss: 0.2981126010417938\n",
      "Subject 20, Epoch 244, Loss: 1.8232406675815582, Final Batch Loss: 0.44158899784088135\n",
      "Subject 20, Epoch 245, Loss: 1.622179388999939, Final Batch Loss: 0.3994978666305542\n",
      "Subject 20, Epoch 246, Loss: 1.4993430078029633, Final Batch Loss: 0.34070488810539246\n",
      "Subject 20, Epoch 247, Loss: 1.543305367231369, Final Batch Loss: 0.3060602843761444\n",
      "Subject 20, Epoch 248, Loss: 1.940941959619522, Final Batch Loss: 0.6379094123840332\n",
      "Subject 20, Epoch 249, Loss: 1.833906888961792, Final Batch Loss: 0.7047479152679443\n",
      "Subject 20, Epoch 250, Loss: 1.606415867805481, Final Batch Loss: 0.34780362248420715\n",
      "Subject 20, Epoch 251, Loss: 1.6160732209682465, Final Batch Loss: 0.36529549956321716\n",
      "Subject 20, Epoch 252, Loss: 1.704881638288498, Final Batch Loss: 0.505190908908844\n",
      "Subject 20, Epoch 253, Loss: 1.6050118207931519, Final Batch Loss: 0.3417423367500305\n",
      "Subject 20, Epoch 254, Loss: 1.6833376586437225, Final Batch Loss: 0.441987007856369\n",
      "Subject 20, Epoch 255, Loss: 1.6157236993312836, Final Batch Loss: 0.3174044191837311\n",
      "Subject 20, Epoch 256, Loss: 1.5296131372451782, Final Batch Loss: 0.31758740544319153\n",
      "Subject 20, Epoch 257, Loss: 1.7545301020145416, Final Batch Loss: 0.5991137027740479\n",
      "Subject 20, Epoch 258, Loss: 1.571633517742157, Final Batch Loss: 0.35296109318733215\n",
      "Subject 20, Epoch 259, Loss: 1.6991235613822937, Final Batch Loss: 0.37713703513145447\n",
      "Subject 20, Epoch 260, Loss: 1.5737962126731873, Final Batch Loss: 0.36388543248176575\n",
      "Subject 20, Epoch 261, Loss: 1.7486777305603027, Final Batch Loss: 0.6069624423980713\n",
      "Subject 20, Epoch 262, Loss: 1.7909229695796967, Final Batch Loss: 0.44146308302879333\n",
      "Subject 20, Epoch 263, Loss: 1.6594412922859192, Final Batch Loss: 0.4720899164676666\n",
      "Subject 20, Epoch 264, Loss: 1.7110041081905365, Final Batch Loss: 0.42848098278045654\n",
      "Subject 20, Epoch 265, Loss: 1.6871711313724518, Final Batch Loss: 0.5144236087799072\n",
      "Subject 20, Epoch 266, Loss: 1.5770228803157806, Final Batch Loss: 0.3484938442707062\n",
      "Subject 20, Epoch 267, Loss: 1.4781141579151154, Final Batch Loss: 0.2641904056072235\n",
      "Subject 20, Epoch 268, Loss: 1.8556221425533295, Final Batch Loss: 0.6350371837615967\n",
      "Subject 20, Epoch 269, Loss: 1.7775092720985413, Final Batch Loss: 0.5835702419281006\n",
      "Subject 20, Epoch 270, Loss: 1.5214499235153198, Final Batch Loss: 0.3527000844478607\n",
      "Subject 20, Epoch 271, Loss: 1.7123954892158508, Final Batch Loss: 0.567124605178833\n",
      "Subject 20, Epoch 272, Loss: 1.6377111971378326, Final Batch Loss: 0.4217723608016968\n",
      "Subject 20, Epoch 273, Loss: 1.557157576084137, Final Batch Loss: 0.38514944911003113\n",
      "Subject 20, Epoch 274, Loss: 1.6655880212783813, Final Batch Loss: 0.43510663509368896\n",
      "Subject 20, Epoch 275, Loss: 1.5177737772464752, Final Batch Loss: 0.3760724663734436\n",
      "Subject 20, Epoch 276, Loss: 1.4830468893051147, Final Batch Loss: 0.21316233277320862\n",
      "Subject 20, Epoch 277, Loss: 1.6659352779388428, Final Batch Loss: 0.4922139346599579\n",
      "Subject 20, Epoch 278, Loss: 1.4784732460975647, Final Batch Loss: 0.28378844261169434\n",
      "Subject 20, Epoch 279, Loss: 1.50167778134346, Final Batch Loss: 0.3660575747489929\n",
      "Subject 20, Epoch 280, Loss: 1.6510440111160278, Final Batch Loss: 0.3701822757720947\n",
      "Subject 20, Epoch 281, Loss: 1.6475467383861542, Final Batch Loss: 0.40517833828926086\n",
      "Subject 20, Epoch 282, Loss: 1.4340380877256393, Final Batch Loss: 0.21950526535511017\n",
      "Subject 20, Epoch 283, Loss: 1.755785197019577, Final Batch Loss: 0.5487066507339478\n",
      "Subject 20, Epoch 284, Loss: 1.621972143650055, Final Batch Loss: 0.42621731758117676\n",
      "Subject 20, Epoch 285, Loss: 1.9339465498924255, Final Batch Loss: 0.7552997469902039\n",
      "Subject 20, Epoch 286, Loss: 1.5789329409599304, Final Batch Loss: 0.3780456781387329\n",
      "Subject 20, Epoch 287, Loss: 1.8599322140216827, Final Batch Loss: 0.6889306902885437\n",
      "Subject 20, Epoch 288, Loss: 1.4979905188083649, Final Batch Loss: 0.3088812530040741\n",
      "Subject 20, Epoch 289, Loss: 1.45081228017807, Final Batch Loss: 0.22039440274238586\n",
      "Subject 20, Epoch 290, Loss: 1.6899450421333313, Final Batch Loss: 0.5479259490966797\n",
      "Subject 20, Epoch 291, Loss: 1.411585807800293, Final Batch Loss: 0.3037518560886383\n",
      "Subject 20, Epoch 292, Loss: 1.4395507276058197, Final Batch Loss: 0.3075488209724426\n",
      "Subject 20, Epoch 293, Loss: 1.357937440276146, Final Batch Loss: 0.24728848040103912\n",
      "Subject 20, Epoch 294, Loss: 1.5225316286087036, Final Batch Loss: 0.4293491840362549\n",
      "Subject 20, Epoch 295, Loss: 1.488725185394287, Final Batch Loss: 0.29255378246307373\n",
      "Subject 20, Epoch 296, Loss: 1.4633145034313202, Final Batch Loss: 0.286553293466568\n",
      "Subject 20, Epoch 297, Loss: 1.5196027159690857, Final Batch Loss: 0.5210686326026917\n",
      "Subject 20, Epoch 298, Loss: 1.5405612885951996, Final Batch Loss: 0.3877015709877014\n",
      "Subject 20, Epoch 299, Loss: 1.513096958398819, Final Batch Loss: 0.4549846649169922\n",
      "Subject 20, Epoch 300, Loss: 1.631609320640564, Final Batch Loss: 0.46303990483283997\n",
      "Subject 20, Epoch 301, Loss: 1.529419630765915, Final Batch Loss: 0.4229097068309784\n",
      "Subject 20, Epoch 302, Loss: 1.6296256184577942, Final Batch Loss: 0.3227337896823883\n",
      "Subject 20, Epoch 303, Loss: 1.5861481726169586, Final Batch Loss: 0.5323569774627686\n",
      "Subject 20, Epoch 304, Loss: 1.4068227410316467, Final Batch Loss: 0.3353006839752197\n",
      "Subject 20, Epoch 305, Loss: 1.742852658033371, Final Batch Loss: 0.5103338360786438\n",
      "Subject 20, Epoch 306, Loss: 1.3888946175575256, Final Batch Loss: 0.32263854146003723\n",
      "Subject 20, Epoch 307, Loss: 1.3876731395721436, Final Batch Loss: 0.3385125398635864\n",
      "Subject 20, Epoch 308, Loss: 1.735406517982483, Final Batch Loss: 0.7076511383056641\n",
      "Subject 20, Epoch 309, Loss: 1.4835695624351501, Final Batch Loss: 0.33123111724853516\n",
      "Subject 20, Epoch 310, Loss: 1.62248957157135, Final Batch Loss: 0.4730074405670166\n",
      "Subject 20, Epoch 311, Loss: 1.6405831277370453, Final Batch Loss: 0.5520667433738708\n",
      "Subject 20, Epoch 312, Loss: 1.4275379925966263, Final Batch Loss: 0.23983438313007355\n",
      "Subject 20, Epoch 313, Loss: 1.6375204026699066, Final Batch Loss: 0.5561251044273376\n",
      "Subject 20, Epoch 314, Loss: 1.6537910401821136, Final Batch Loss: 0.4535924792289734\n",
      "Subject 20, Epoch 315, Loss: 1.5196903944015503, Final Batch Loss: 0.3798573613166809\n",
      "Subject 20, Epoch 316, Loss: 1.6004916429519653, Final Batch Loss: 0.45112019777297974\n",
      "Subject 20, Epoch 317, Loss: 1.4798810482025146, Final Batch Loss: 0.3605138659477234\n",
      "Subject 20, Epoch 318, Loss: 1.6728601455688477, Final Batch Loss: 0.6042722463607788\n",
      "Subject 20, Epoch 319, Loss: 1.5293480455875397, Final Batch Loss: 0.39198073744773865\n",
      "Subject 20, Epoch 320, Loss: 1.5000481009483337, Final Batch Loss: 0.41874486207962036\n",
      "Subject 20, Epoch 321, Loss: 1.4849072992801666, Final Batch Loss: 0.3893883228302002\n",
      "Subject 20, Epoch 322, Loss: 1.6588440239429474, Final Batch Loss: 0.5361406803131104\n",
      "Subject 20, Epoch 323, Loss: 1.3037098497152328, Final Batch Loss: 0.2399018257856369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 324, Loss: 1.5230273306369781, Final Batch Loss: 0.46090975403785706\n",
      "Subject 20, Epoch 325, Loss: 1.2869400382041931, Final Batch Loss: 0.1597372591495514\n",
      "Subject 20, Epoch 326, Loss: 1.4575411081314087, Final Batch Loss: 0.4102189540863037\n",
      "Subject 20, Epoch 327, Loss: 1.4157141745090485, Final Batch Loss: 0.256511926651001\n",
      "Subject 20, Epoch 328, Loss: 1.3557864427566528, Final Batch Loss: 0.3175836503505707\n",
      "Subject 20, Epoch 329, Loss: 1.2276021838188171, Final Batch Loss: 0.21130529046058655\n",
      "Subject 20, Epoch 330, Loss: 1.4221819341182709, Final Batch Loss: 0.3085501790046692\n",
      "Subject 20, Epoch 331, Loss: 1.3415320366621017, Final Batch Loss: 0.20114542543888092\n",
      "Subject 20, Epoch 332, Loss: 1.3236354440450668, Final Batch Loss: 0.24623943865299225\n",
      "Subject 20, Epoch 333, Loss: 1.5690860450267792, Final Batch Loss: 0.502981960773468\n",
      "Subject 20, Epoch 334, Loss: 1.3698053061962128, Final Batch Loss: 0.28878822922706604\n",
      "Subject 20, Epoch 335, Loss: 1.2624838650226593, Final Batch Loss: 0.24713170528411865\n",
      "Subject 20, Epoch 336, Loss: 1.598843663930893, Final Batch Loss: 0.4696403443813324\n",
      "Subject 20, Epoch 337, Loss: 1.4538017809391022, Final Batch Loss: 0.39633455872535706\n",
      "Subject 20, Epoch 338, Loss: 1.373768538236618, Final Batch Loss: 0.4055791199207306\n",
      "Subject 20, Epoch 339, Loss: 1.2801673859357834, Final Batch Loss: 0.18938638269901276\n",
      "Subject 20, Epoch 340, Loss: 1.3499867618083954, Final Batch Loss: 0.27022644877433777\n",
      "Subject 20, Epoch 341, Loss: 1.3694818317890167, Final Batch Loss: 0.2709437906742096\n",
      "Subject 20, Epoch 342, Loss: 1.5268236994743347, Final Batch Loss: 0.414908230304718\n",
      "Subject 20, Epoch 343, Loss: 1.454522728919983, Final Batch Loss: 0.34171128273010254\n",
      "Subject 20, Epoch 344, Loss: 1.4510294198989868, Final Batch Loss: 0.3629509210586548\n",
      "Subject 20, Epoch 345, Loss: 1.3795344531536102, Final Batch Loss: 0.32184311747550964\n",
      "Subject 20, Epoch 346, Loss: 1.574759155511856, Final Batch Loss: 0.5695896744728088\n",
      "Subject 20, Epoch 347, Loss: 1.506565272808075, Final Batch Loss: 0.3284977972507477\n",
      "Subject 20, Epoch 348, Loss: 1.4064504504203796, Final Batch Loss: 0.299864798784256\n",
      "Subject 20, Epoch 349, Loss: 1.4192716479301453, Final Batch Loss: 0.33030951023101807\n",
      "Subject 20, Epoch 350, Loss: 1.5454222857952118, Final Batch Loss: 0.40974846482276917\n",
      "Subject 20, Epoch 351, Loss: 1.421829342842102, Final Batch Loss: 0.44861558079719543\n",
      "Subject 20, Epoch 352, Loss: 1.446634829044342, Final Batch Loss: 0.3098386824131012\n",
      "Subject 20, Epoch 353, Loss: 1.588920533657074, Final Batch Loss: 0.5022211670875549\n",
      "Subject 20, Epoch 354, Loss: 1.5963495373725891, Final Batch Loss: 0.562670886516571\n",
      "Subject 20, Epoch 355, Loss: 1.4865432381629944, Final Batch Loss: 0.4038970172405243\n",
      "Subject 20, Epoch 356, Loss: 1.4319897592067719, Final Batch Loss: 0.2736707627773285\n",
      "Subject 20, Epoch 357, Loss: 1.4099415242671967, Final Batch Loss: 0.41254979372024536\n",
      "Subject 20, Epoch 358, Loss: 1.306056171655655, Final Batch Loss: 0.3375611901283264\n",
      "Subject 20, Epoch 359, Loss: 1.3151542693376541, Final Batch Loss: 0.19534097611904144\n",
      "Subject 20, Epoch 360, Loss: 1.2542722523212433, Final Batch Loss: 0.2831496298313141\n",
      "Subject 20, Epoch 361, Loss: 1.2879443764686584, Final Batch Loss: 0.28613412380218506\n",
      "Subject 20, Epoch 362, Loss: 1.3474102318286896, Final Batch Loss: 0.3946419954299927\n",
      "Subject 20, Epoch 363, Loss: 1.2667355835437775, Final Batch Loss: 0.26769405603408813\n",
      "Subject 20, Epoch 364, Loss: 1.3562301099300385, Final Batch Loss: 0.26806432008743286\n",
      "Subject 20, Epoch 365, Loss: 1.4261497557163239, Final Batch Loss: 0.4198310971260071\n",
      "Subject 20, Epoch 366, Loss: 1.3495770394802094, Final Batch Loss: 0.25957396626472473\n",
      "Subject 20, Epoch 367, Loss: 1.4101580679416656, Final Batch Loss: 0.3636190891265869\n",
      "Subject 20, Epoch 368, Loss: 1.1708524376153946, Final Batch Loss: 0.2133999615907669\n",
      "Subject 20, Epoch 369, Loss: 1.3557883501052856, Final Batch Loss: 0.327958881855011\n",
      "Subject 20, Epoch 370, Loss: 1.3423254191875458, Final Batch Loss: 0.327012836933136\n",
      "Subject 20, Epoch 371, Loss: 1.5134483575820923, Final Batch Loss: 0.4603712558746338\n",
      "Subject 20, Epoch 372, Loss: 1.334459662437439, Final Batch Loss: 0.26322031021118164\n",
      "Subject 20, Epoch 373, Loss: 1.4445089399814606, Final Batch Loss: 0.47250160574913025\n",
      "Subject 20, Epoch 374, Loss: 1.3462038040161133, Final Batch Loss: 0.3289022445678711\n",
      "Subject 20, Epoch 375, Loss: 1.3364389836788177, Final Batch Loss: 0.3521708548069\n",
      "Subject 20, Epoch 376, Loss: 1.2613731622695923, Final Batch Loss: 0.2524338662624359\n",
      "Subject 20, Epoch 377, Loss: 1.3541630655527115, Final Batch Loss: 0.42020851373672485\n",
      "Subject 20, Epoch 378, Loss: 1.306612253189087, Final Batch Loss: 0.31169551610946655\n",
      "Subject 20, Epoch 379, Loss: 1.3038890659809113, Final Batch Loss: 0.29604583978652954\n",
      "Subject 20, Epoch 380, Loss: 1.2038682103157043, Final Batch Loss: 0.23505353927612305\n",
      "Subject 20, Epoch 381, Loss: 1.3487391471862793, Final Batch Loss: 0.3171330988407135\n",
      "Subject 20, Epoch 382, Loss: 1.4633927047252655, Final Batch Loss: 0.4673555791378021\n",
      "Subject 20, Epoch 383, Loss: 1.301989495754242, Final Batch Loss: 0.33636474609375\n",
      "Subject 20, Epoch 384, Loss: 1.1908533722162247, Final Batch Loss: 0.22095851600170135\n",
      "Subject 20, Epoch 385, Loss: 1.1969190537929535, Final Batch Loss: 0.27329131960868835\n",
      "Subject 20, Epoch 386, Loss: 1.2967684268951416, Final Batch Loss: 0.37529224157333374\n",
      "Subject 20, Epoch 387, Loss: 1.2186975181102753, Final Batch Loss: 0.3435412347316742\n",
      "Subject 20, Epoch 388, Loss: 1.3140555322170258, Final Batch Loss: 0.29903852939605713\n",
      "Subject 20, Epoch 389, Loss: 1.4065671265125275, Final Batch Loss: 0.48266932368278503\n",
      "Subject 20, Epoch 390, Loss: 1.2047910690307617, Final Batch Loss: 0.20274987816810608\n",
      "Subject 20, Epoch 391, Loss: 1.136308416724205, Final Batch Loss: 0.18906505405902863\n",
      "Subject 20, Epoch 392, Loss: 1.2006770074367523, Final Batch Loss: 0.22840231657028198\n",
      "Subject 20, Epoch 393, Loss: 1.3906014561653137, Final Batch Loss: 0.410005122423172\n",
      "Subject 20, Epoch 394, Loss: 1.2150867283344269, Final Batch Loss: 0.26569685339927673\n",
      "Subject 20, Epoch 395, Loss: 1.429935783147812, Final Batch Loss: 0.3936489224433899\n",
      "Subject 20, Epoch 396, Loss: 1.328527808189392, Final Batch Loss: 0.2983865737915039\n",
      "Subject 20, Epoch 397, Loss: 1.4360714852809906, Final Batch Loss: 0.5860476493835449\n",
      "Subject 20, Epoch 398, Loss: 1.4997298419475555, Final Batch Loss: 0.432280570268631\n",
      "Subject 20, Epoch 399, Loss: 1.2805737257003784, Final Batch Loss: 0.2270299792289734\n",
      "Subject 20, Epoch 400, Loss: 1.2702428698539734, Final Batch Loss: 0.31069421768188477\n",
      "Subject 20, Epoch 401, Loss: 1.2037433683872223, Final Batch Loss: 0.2549143433570862\n",
      "Subject 20, Epoch 402, Loss: 1.4987103939056396, Final Batch Loss: 0.47347187995910645\n",
      "Subject 20, Epoch 403, Loss: 1.2366006970405579, Final Batch Loss: 0.38513481616973877\n",
      "Subject 20, Epoch 404, Loss: 1.2700106799602509, Final Batch Loss: 0.3346594572067261\n",
      "Subject 20, Epoch 405, Loss: 1.496014416217804, Final Batch Loss: 0.607852041721344\n",
      "Subject 20, Epoch 406, Loss: 1.220066487789154, Final Batch Loss: 0.32938358187675476\n",
      "Subject 20, Epoch 407, Loss: 1.1713030338287354, Final Batch Loss: 0.23740613460540771\n",
      "Subject 20, Epoch 408, Loss: 1.212606966495514, Final Batch Loss: 0.2529284954071045\n",
      "Subject 20, Epoch 409, Loss: 1.2414022386074066, Final Batch Loss: 0.3364054560661316\n",
      "Subject 20, Epoch 410, Loss: 1.3999358415603638, Final Batch Loss: 0.3483227491378784\n",
      "Subject 20, Epoch 411, Loss: 1.603192299604416, Final Batch Loss: 0.6475822329521179\n",
      "Subject 20, Epoch 412, Loss: 1.2115200757980347, Final Batch Loss: 0.2984808087348938\n",
      "Subject 20, Epoch 413, Loss: 1.2363336086273193, Final Batch Loss: 0.3100162446498871\n",
      "Subject 20, Epoch 414, Loss: 1.1480199694633484, Final Batch Loss: 0.24317306280136108\n",
      "Subject 20, Epoch 415, Loss: 1.1757581532001495, Final Batch Loss: 0.18604722619056702\n",
      "Subject 20, Epoch 416, Loss: 1.149782121181488, Final Batch Loss: 0.23765918612480164\n",
      "Subject 20, Epoch 417, Loss: 1.1845387518405914, Final Batch Loss: 0.32283341884613037\n",
      "Subject 20, Epoch 418, Loss: 1.1605376601219177, Final Batch Loss: 0.3129044771194458\n",
      "Subject 20, Epoch 419, Loss: 1.303653508424759, Final Batch Loss: 0.3147101104259491\n",
      "Subject 20, Epoch 420, Loss: 1.2688632905483246, Final Batch Loss: 0.41228199005126953\n",
      "Subject 20, Epoch 421, Loss: 1.1009882092475891, Final Batch Loss: 0.2080773413181305\n",
      "Subject 20, Epoch 422, Loss: 1.1921103596687317, Final Batch Loss: 0.3629198372364044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 423, Loss: 1.141846314072609, Final Batch Loss: 0.19500364363193512\n",
      "Subject 20, Epoch 424, Loss: 1.3423295617103577, Final Batch Loss: 0.43413078784942627\n",
      "Subject 20, Epoch 425, Loss: 1.2342451959848404, Final Batch Loss: 0.4299984276294708\n",
      "Subject 20, Epoch 426, Loss: 1.3749604523181915, Final Batch Loss: 0.3438131809234619\n",
      "Subject 20, Epoch 427, Loss: 1.1833352595567703, Final Batch Loss: 0.3694052994251251\n",
      "Subject 20, Epoch 428, Loss: 1.1057157814502716, Final Batch Loss: 0.2587587833404541\n",
      "Subject 20, Epoch 429, Loss: 1.0902614742517471, Final Batch Loss: 0.1910882443189621\n",
      "Subject 20, Epoch 430, Loss: 1.165584310889244, Final Batch Loss: 0.2978055477142334\n",
      "Subject 20, Epoch 431, Loss: 1.2890047132968903, Final Batch Loss: 0.44383716583251953\n",
      "Subject 20, Epoch 432, Loss: 1.3165745735168457, Final Batch Loss: 0.3501158058643341\n",
      "Subject 20, Epoch 433, Loss: 1.332971215248108, Final Batch Loss: 0.39755967259407043\n",
      "Subject 20, Epoch 434, Loss: 1.5329659581184387, Final Batch Loss: 0.6056333184242249\n",
      "Subject 20, Epoch 435, Loss: 1.3343462944030762, Final Batch Loss: 0.40040189027786255\n",
      "Subject 20, Epoch 436, Loss: 1.1753571331501007, Final Batch Loss: 0.33213096857070923\n",
      "Subject 20, Epoch 437, Loss: 1.2182631343603134, Final Batch Loss: 0.4015817940235138\n",
      "Subject 20, Epoch 438, Loss: 1.1378616988658905, Final Batch Loss: 0.278804212808609\n",
      "Subject 20, Epoch 439, Loss: 1.3813098073005676, Final Batch Loss: 0.5164356231689453\n",
      "Subject 20, Epoch 440, Loss: 1.1797198802232742, Final Batch Loss: 0.38163578510284424\n",
      "Subject 20, Epoch 441, Loss: 1.1764227896928787, Final Batch Loss: 0.3514198362827301\n",
      "Subject 20, Epoch 442, Loss: 1.1865183413028717, Final Batch Loss: 0.2971328794956207\n",
      "Subject 20, Epoch 443, Loss: 1.2744609862565994, Final Batch Loss: 0.5048273205757141\n",
      "Subject 20, Epoch 444, Loss: 1.0678746551275253, Final Batch Loss: 0.22396422922611237\n",
      "Subject 20, Epoch 445, Loss: 1.374981939792633, Final Batch Loss: 0.540197491645813\n",
      "Subject 20, Epoch 446, Loss: 1.0182670652866364, Final Batch Loss: 0.18351010978221893\n",
      "Subject 20, Epoch 447, Loss: 1.1346998810768127, Final Batch Loss: 0.2771611511707306\n",
      "Subject 20, Epoch 448, Loss: 1.0364239364862442, Final Batch Loss: 0.23992381989955902\n",
      "Subject 20, Epoch 449, Loss: 1.10113924741745, Final Batch Loss: 0.250723272562027\n",
      "Subject 20, Epoch 450, Loss: 1.2357188314199448, Final Batch Loss: 0.3369876742362976\n",
      "Subject 20, Epoch 451, Loss: 1.1222629696130753, Final Batch Loss: 0.2231086939573288\n",
      "Subject 20, Epoch 452, Loss: 1.1334860175848007, Final Batch Loss: 0.31111112236976624\n",
      "Subject 20, Epoch 453, Loss: 1.265727549791336, Final Batch Loss: 0.40770530700683594\n",
      "Subject 20, Epoch 454, Loss: 1.092283308506012, Final Batch Loss: 0.2518438994884491\n",
      "Subject 20, Epoch 455, Loss: 1.1256103962659836, Final Batch Loss: 0.24517740309238434\n",
      "Subject 20, Epoch 456, Loss: 1.2955254912376404, Final Batch Loss: 0.2665092349052429\n",
      "Subject 20, Epoch 457, Loss: 1.07757967710495, Final Batch Loss: 0.20921877026557922\n",
      "Subject 20, Epoch 458, Loss: 1.118283748626709, Final Batch Loss: 0.2874094843864441\n",
      "Subject 20, Epoch 459, Loss: 1.0538136959075928, Final Batch Loss: 0.2781027555465698\n",
      "Subject 20, Epoch 460, Loss: 1.1385947316884995, Final Batch Loss: 0.32894378900527954\n",
      "Subject 20, Epoch 461, Loss: 1.1756159365177155, Final Batch Loss: 0.2581969201564789\n",
      "Subject 20, Epoch 462, Loss: 1.0244366228580475, Final Batch Loss: 0.2062516063451767\n",
      "Subject 20, Epoch 463, Loss: 1.1978595554828644, Final Batch Loss: 0.2782982289791107\n",
      "Subject 20, Epoch 464, Loss: 1.1849682927131653, Final Batch Loss: 0.4104759097099304\n",
      "Subject 20, Epoch 465, Loss: 1.1407885551452637, Final Batch Loss: 0.3033257722854614\n",
      "Subject 20, Epoch 466, Loss: 0.9311427772045135, Final Batch Loss: 0.17152486741542816\n",
      "Subject 20, Epoch 467, Loss: 0.9240420311689377, Final Batch Loss: 0.0626402199268341\n",
      "Subject 20, Epoch 468, Loss: 1.2914568781852722, Final Batch Loss: 0.4006705582141876\n",
      "Subject 20, Epoch 469, Loss: 1.0298840999603271, Final Batch Loss: 0.2500699758529663\n",
      "Subject 20, Epoch 470, Loss: 1.205552950501442, Final Batch Loss: 0.2124081403017044\n",
      "Subject 20, Epoch 471, Loss: 1.1667651534080505, Final Batch Loss: 0.31164902448654175\n",
      "Subject 20, Epoch 472, Loss: 1.2051464021205902, Final Batch Loss: 0.34786468744277954\n",
      "Subject 20, Epoch 473, Loss: 1.316064640879631, Final Batch Loss: 0.5197064280509949\n",
      "Subject 20, Epoch 474, Loss: 1.1337221264839172, Final Batch Loss: 0.23180049657821655\n",
      "Subject 20, Epoch 475, Loss: 1.1037265062332153, Final Batch Loss: 0.2620832920074463\n",
      "Subject 20, Epoch 476, Loss: 1.0720189064741135, Final Batch Loss: 0.24367351830005646\n",
      "Subject 20, Epoch 477, Loss: 1.3914932012557983, Final Batch Loss: 0.46868255734443665\n",
      "Subject 20, Epoch 478, Loss: 1.0388123542070389, Final Batch Loss: 0.245381161570549\n",
      "Subject 20, Epoch 479, Loss: 1.007380172610283, Final Batch Loss: 0.1765661984682083\n",
      "Subject 20, Epoch 480, Loss: 1.091439038515091, Final Batch Loss: 0.26783353090286255\n",
      "Subject 20, Epoch 481, Loss: 1.025294616818428, Final Batch Loss: 0.15558691322803497\n",
      "Subject 20, Epoch 482, Loss: 1.06920625269413, Final Batch Loss: 0.17087821662425995\n",
      "Subject 20, Epoch 483, Loss: 0.9187166541814804, Final Batch Loss: 0.17494887113571167\n",
      "Subject 20, Epoch 484, Loss: 1.0722001194953918, Final Batch Loss: 0.23245158791542053\n",
      "Subject 20, Epoch 485, Loss: 0.8146823346614838, Final Batch Loss: 0.19233977794647217\n",
      "Subject 20, Epoch 486, Loss: 1.0754208117723465, Final Batch Loss: 0.23724278807640076\n",
      "Subject 20, Epoch 487, Loss: 0.8734477236866951, Final Batch Loss: 0.10663460940122604\n",
      "Subject 20, Epoch 488, Loss: 0.9308765381574631, Final Batch Loss: 0.15612655878067017\n",
      "Subject 20, Epoch 489, Loss: 1.0751170963048935, Final Batch Loss: 0.2674367427825928\n",
      "Subject 20, Epoch 490, Loss: 1.034887582063675, Final Batch Loss: 0.23131650686264038\n",
      "Subject 20, Epoch 491, Loss: 1.0353102385997772, Final Batch Loss: 0.3294438421726227\n",
      "Subject 20, Epoch 492, Loss: 1.0456392914056778, Final Batch Loss: 0.364388108253479\n",
      "Subject 20, Epoch 493, Loss: 1.062088593840599, Final Batch Loss: 0.1997777372598648\n",
      "Subject 20, Epoch 494, Loss: 0.9079437553882599, Final Batch Loss: 0.11991754174232483\n",
      "Subject 20, Epoch 495, Loss: 0.9244085103273392, Final Batch Loss: 0.16475339233875275\n",
      "Subject 20, Epoch 496, Loss: 0.9280077219009399, Final Batch Loss: 0.1733863651752472\n",
      "Subject 20, Epoch 497, Loss: 1.0879616141319275, Final Batch Loss: 0.38222643733024597\n",
      "Subject 20, Epoch 498, Loss: 0.9475585967302322, Final Batch Loss: 0.17176149785518646\n",
      "Subject 20, Epoch 499, Loss: 1.0543297678232193, Final Batch Loss: 0.28183457255363464\n",
      "Subject 20, Epoch 500, Loss: 1.1719017177820206, Final Batch Loss: 0.44449537992477417\n",
      "Subject 20, Epoch 501, Loss: 1.1424425691366196, Final Batch Loss: 0.37068653106689453\n",
      "Subject 20, Epoch 502, Loss: 1.059151440858841, Final Batch Loss: 0.1914750337600708\n",
      "Subject 20, Epoch 503, Loss: 1.0820266008377075, Final Batch Loss: 0.2317400574684143\n",
      "Subject 20, Epoch 504, Loss: 1.0329994857311249, Final Batch Loss: 0.2555070221424103\n",
      "Subject 20, Epoch 505, Loss: 0.977850154042244, Final Batch Loss: 0.22286775708198547\n",
      "Subject 20, Epoch 506, Loss: 0.8947055637836456, Final Batch Loss: 0.2409825474023819\n",
      "Subject 20, Epoch 507, Loss: 0.8867589682340622, Final Batch Loss: 0.08825969696044922\n",
      "Subject 20, Epoch 508, Loss: 0.9943279772996902, Final Batch Loss: 0.24080222845077515\n",
      "Subject 20, Epoch 509, Loss: 0.9285196214914322, Final Batch Loss: 0.16372837126255035\n",
      "Subject 20, Epoch 510, Loss: 1.043409451842308, Final Batch Loss: 0.3549440801143646\n",
      "Subject 20, Epoch 511, Loss: 0.9782570004463196, Final Batch Loss: 0.2498093843460083\n",
      "Subject 20, Epoch 512, Loss: 0.910569816827774, Final Batch Loss: 0.15691964328289032\n",
      "Subject 20, Epoch 513, Loss: 0.9756536930799484, Final Batch Loss: 0.16888350248336792\n",
      "Subject 20, Epoch 514, Loss: 1.030943050980568, Final Batch Loss: 0.28992733359336853\n",
      "Subject 20, Epoch 515, Loss: 1.0973244607448578, Final Batch Loss: 0.3544086515903473\n",
      "Subject 20, Epoch 516, Loss: 1.118581548333168, Final Batch Loss: 0.38633275032043457\n",
      "Subject 20, Epoch 517, Loss: 0.9404905736446381, Final Batch Loss: 0.2423766851425171\n",
      "Subject 20, Epoch 518, Loss: 0.8620381653308868, Final Batch Loss: 0.17020994424819946\n",
      "Subject 20, Epoch 519, Loss: 0.9736104905605316, Final Batch Loss: 0.22688885033130646\n",
      "Subject 20, Epoch 520, Loss: 0.9006498456001282, Final Batch Loss: 0.16579598188400269\n",
      "Subject 20, Epoch 521, Loss: 0.9639047384262085, Final Batch Loss: 0.1910528540611267\n",
      "Subject 20, Epoch 522, Loss: 0.9300866276025772, Final Batch Loss: 0.17592033743858337\n",
      "Subject 20, Epoch 523, Loss: 0.9335030913352966, Final Batch Loss: 0.17609617114067078\n",
      "Subject 20, Epoch 524, Loss: 0.9195527583360672, Final Batch Loss: 0.24753932654857635\n",
      "Subject 20, Epoch 525, Loss: 1.061364397406578, Final Batch Loss: 0.37143296003341675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 526, Loss: 1.1049941331148148, Final Batch Loss: 0.3243744671344757\n",
      "Subject 20, Epoch 527, Loss: 0.9261207282543182, Final Batch Loss: 0.12366281449794769\n",
      "Subject 20, Epoch 528, Loss: 1.0084002315998077, Final Batch Loss: 0.16942965984344482\n",
      "Subject 20, Epoch 529, Loss: 1.0386436879634857, Final Batch Loss: 0.19961410760879517\n",
      "Subject 20, Epoch 530, Loss: 1.0528944283723831, Final Batch Loss: 0.3736205995082855\n",
      "Subject 20, Epoch 531, Loss: 0.9115379303693771, Final Batch Loss: 0.24900442361831665\n",
      "Subject 20, Epoch 532, Loss: 0.845007061958313, Final Batch Loss: 0.09042307734489441\n",
      "Subject 20, Epoch 533, Loss: 0.9326612651348114, Final Batch Loss: 0.21237753331661224\n",
      "Subject 20, Epoch 534, Loss: 1.0219798684120178, Final Batch Loss: 0.2875082492828369\n",
      "Subject 20, Epoch 535, Loss: 1.1057410836219788, Final Batch Loss: 0.3856608271598816\n",
      "Subject 20, Epoch 536, Loss: 0.9311588108539581, Final Batch Loss: 0.1932561844587326\n",
      "Subject 20, Epoch 537, Loss: 0.931653693318367, Final Batch Loss: 0.2097112536430359\n",
      "Subject 20, Epoch 538, Loss: 1.0405292063951492, Final Batch Loss: 0.4260703921318054\n",
      "Subject 20, Epoch 539, Loss: 0.9959837347269058, Final Batch Loss: 0.365377813577652\n",
      "Subject 20, Epoch 540, Loss: 0.866368755698204, Final Batch Loss: 0.25633347034454346\n",
      "Subject 20, Epoch 541, Loss: 0.8817102611064911, Final Batch Loss: 0.18983407318592072\n",
      "Subject 20, Epoch 542, Loss: 1.0327474772930145, Final Batch Loss: 0.34432724118232727\n",
      "Subject 20, Epoch 543, Loss: 1.1624568402767181, Final Batch Loss: 0.46130749583244324\n",
      "Subject 20, Epoch 544, Loss: 1.006123661994934, Final Batch Loss: 0.2854717969894409\n",
      "Subject 20, Epoch 545, Loss: 0.8992571979761124, Final Batch Loss: 0.2736873924732208\n",
      "Subject 20, Epoch 546, Loss: 0.8986227661371231, Final Batch Loss: 0.16314129531383514\n",
      "Subject 20, Epoch 547, Loss: 1.0868174582719803, Final Batch Loss: 0.42956656217575073\n",
      "Subject 20, Epoch 548, Loss: 0.9670167714357376, Final Batch Loss: 0.2332392930984497\n",
      "Subject 20, Epoch 549, Loss: 1.0885574519634247, Final Batch Loss: 0.25116419792175293\n",
      "Subject 20, Epoch 550, Loss: 0.8528369814157486, Final Batch Loss: 0.2027590423822403\n",
      "Subject 20, Epoch 551, Loss: 0.9634857475757599, Final Batch Loss: 0.2570399045944214\n",
      "Subject 20, Epoch 552, Loss: 0.8644159436225891, Final Batch Loss: 0.19092820584774017\n",
      "Subject 20, Epoch 553, Loss: 0.9137310534715652, Final Batch Loss: 0.26256224513053894\n",
      "Subject 20, Epoch 554, Loss: 0.9758644998073578, Final Batch Loss: 0.3420390784740448\n",
      "Subject 20, Epoch 555, Loss: 0.8366657346487045, Final Batch Loss: 0.1509414166212082\n",
      "Subject 20, Epoch 556, Loss: 0.8050783723592758, Final Batch Loss: 0.1809210330247879\n",
      "Subject 20, Epoch 557, Loss: 0.8560827523469925, Final Batch Loss: 0.18805934488773346\n",
      "Subject 20, Epoch 558, Loss: 0.8042325153946877, Final Batch Loss: 0.1003609374165535\n",
      "Subject 20, Epoch 559, Loss: 1.0264544934034348, Final Batch Loss: 0.349232941865921\n",
      "Subject 20, Epoch 560, Loss: 0.8861468881368637, Final Batch Loss: 0.19893576204776764\n",
      "Subject 20, Epoch 561, Loss: 0.9473539441823959, Final Batch Loss: 0.264263778924942\n",
      "Subject 20, Epoch 562, Loss: 0.839819148182869, Final Batch Loss: 0.20621365308761597\n",
      "Subject 20, Epoch 563, Loss: 0.753556951880455, Final Batch Loss: 0.07892829179763794\n",
      "Subject 20, Epoch 564, Loss: 0.9075758904218674, Final Batch Loss: 0.2533992528915405\n",
      "Subject 20, Epoch 565, Loss: 0.8009169846773148, Final Batch Loss: 0.14049766957759857\n",
      "Subject 20, Epoch 566, Loss: 0.7423566579818726, Final Batch Loss: 0.07700632512569427\n",
      "Subject 20, Epoch 567, Loss: 0.8112705051898956, Final Batch Loss: 0.1273086965084076\n",
      "Subject 20, Epoch 568, Loss: 0.7988722920417786, Final Batch Loss: 0.13924187421798706\n",
      "Subject 20, Epoch 569, Loss: 0.9291424602270126, Final Batch Loss: 0.3285415768623352\n",
      "Subject 20, Epoch 570, Loss: 0.7861671894788742, Final Batch Loss: 0.15059936046600342\n",
      "Subject 20, Epoch 571, Loss: 0.7942421287298203, Final Batch Loss: 0.1670190691947937\n",
      "Subject 20, Epoch 572, Loss: 0.894171804189682, Final Batch Loss: 0.2223794311285019\n",
      "Subject 20, Epoch 573, Loss: 0.7739185839891434, Final Batch Loss: 0.13519838452339172\n",
      "Subject 20, Epoch 574, Loss: 0.8852053880691528, Final Batch Loss: 0.14436675608158112\n",
      "Subject 20, Epoch 575, Loss: 0.8957391530275345, Final Batch Loss: 0.1602412611246109\n",
      "Subject 20, Epoch 576, Loss: 0.9796833097934723, Final Batch Loss: 0.30471479892730713\n",
      "Subject 20, Epoch 577, Loss: 0.9640392512083054, Final Batch Loss: 0.29555341601371765\n",
      "Subject 20, Epoch 578, Loss: 0.8365427404642105, Final Batch Loss: 0.2251589596271515\n",
      "Subject 20, Epoch 579, Loss: 0.9782650619745255, Final Batch Loss: 0.3488236963748932\n",
      "Subject 20, Epoch 580, Loss: 0.9667142331600189, Final Batch Loss: 0.2480686902999878\n",
      "Subject 20, Epoch 581, Loss: 0.8155399858951569, Final Batch Loss: 0.1269085705280304\n",
      "Subject 20, Epoch 582, Loss: 0.8037432879209518, Final Batch Loss: 0.1254427582025528\n",
      "Subject 20, Epoch 583, Loss: 1.0127103477716446, Final Batch Loss: 0.4065488874912262\n",
      "Subject 20, Epoch 584, Loss: 0.7363017946481705, Final Batch Loss: 0.18659210205078125\n",
      "Subject 20, Epoch 585, Loss: 0.8575464934110641, Final Batch Loss: 0.26981818675994873\n",
      "Subject 20, Epoch 586, Loss: 0.8232322037220001, Final Batch Loss: 0.20456403493881226\n",
      "Subject 20, Epoch 587, Loss: 0.8064761459827423, Final Batch Loss: 0.20142537355422974\n",
      "Subject 20, Epoch 588, Loss: 0.9020485281944275, Final Batch Loss: 0.2565852403640747\n",
      "Subject 20, Epoch 589, Loss: 0.9545499235391617, Final Batch Loss: 0.403488427400589\n",
      "Subject 20, Epoch 590, Loss: 0.8291410356760025, Final Batch Loss: 0.20048440992832184\n",
      "Subject 20, Epoch 591, Loss: 0.7829820066690445, Final Batch Loss: 0.16867507994174957\n",
      "Subject 20, Epoch 592, Loss: 0.9341760873794556, Final Batch Loss: 0.2581954300403595\n",
      "Subject 20, Epoch 593, Loss: 0.7905281037092209, Final Batch Loss: 0.12876169383525848\n",
      "Subject 20, Epoch 594, Loss: 0.810062825679779, Final Batch Loss: 0.19182462990283966\n",
      "Subject 20, Epoch 595, Loss: 0.8135557323694229, Final Batch Loss: 0.18432830274105072\n",
      "Subject 20, Epoch 596, Loss: 0.797209694981575, Final Batch Loss: 0.1812339425086975\n",
      "Subject 20, Epoch 597, Loss: 0.7557814568281174, Final Batch Loss: 0.18330970406532288\n",
      "Subject 20, Epoch 598, Loss: 0.6872344762086868, Final Batch Loss: 0.19147945940494537\n",
      "Subject 20, Epoch 599, Loss: 0.927641361951828, Final Batch Loss: 0.3536943793296814\n",
      "Subject 20, Epoch 600, Loss: 0.7916052788496017, Final Batch Loss: 0.11313681304454803\n",
      "Subject 20, Epoch 601, Loss: 0.7575066909193993, Final Batch Loss: 0.14312130212783813\n",
      "Subject 20, Epoch 602, Loss: 0.7314559519290924, Final Batch Loss: 0.10403281450271606\n",
      "Subject 20, Epoch 603, Loss: 0.8787218481302261, Final Batch Loss: 0.1758461445569992\n",
      "Subject 20, Epoch 604, Loss: 0.814614549279213, Final Batch Loss: 0.19311736524105072\n",
      "Subject 20, Epoch 605, Loss: 0.7545223236083984, Final Batch Loss: 0.08897240459918976\n",
      "Subject 20, Epoch 606, Loss: 0.7658414393663406, Final Batch Loss: 0.14346164464950562\n",
      "Subject 20, Epoch 607, Loss: 0.735320121049881, Final Batch Loss: 0.1248413622379303\n",
      "Subject 20, Epoch 608, Loss: 0.7748022153973579, Final Batch Loss: 0.11550743132829666\n",
      "Subject 20, Epoch 609, Loss: 0.8812756389379501, Final Batch Loss: 0.256814181804657\n",
      "Subject 20, Epoch 610, Loss: 0.7332032769918442, Final Batch Loss: 0.14923635125160217\n",
      "Subject 20, Epoch 611, Loss: 0.6974001750349998, Final Batch Loss: 0.10853596776723862\n",
      "Subject 20, Epoch 612, Loss: 0.7788679897785187, Final Batch Loss: 0.18084317445755005\n",
      "Subject 20, Epoch 613, Loss: 0.8864776641130447, Final Batch Loss: 0.2533153295516968\n",
      "Subject 20, Epoch 614, Loss: 0.9257274642586708, Final Batch Loss: 0.41591739654541016\n",
      "Subject 20, Epoch 615, Loss: 0.7133421301841736, Final Batch Loss: 0.1620640903711319\n",
      "Subject 20, Epoch 616, Loss: 0.9386749416589737, Final Batch Loss: 0.39217427372932434\n",
      "Subject 20, Epoch 617, Loss: 0.952503427863121, Final Batch Loss: 0.32170531153678894\n",
      "Subject 20, Epoch 618, Loss: 1.0791319012641907, Final Batch Loss: 0.41424015164375305\n",
      "Subject 20, Epoch 619, Loss: 0.7531418353319168, Final Batch Loss: 0.17007604241371155\n",
      "Subject 20, Epoch 620, Loss: 0.7318045496940613, Final Batch Loss: 0.14873434603214264\n",
      "Subject 20, Epoch 621, Loss: 0.8278712183237076, Final Batch Loss: 0.2880667448043823\n",
      "Subject 20, Epoch 622, Loss: 0.7793881818652153, Final Batch Loss: 0.2310190200805664\n",
      "Subject 20, Epoch 623, Loss: 0.8855265229940414, Final Batch Loss: 0.31408828496932983\n",
      "Subject 20, Epoch 624, Loss: 0.8342434614896774, Final Batch Loss: 0.32980018854141235\n",
      "Subject 20, Epoch 625, Loss: 0.7495146691799164, Final Batch Loss: 0.15472304821014404\n",
      "Subject 20, Epoch 626, Loss: 0.763576939702034, Final Batch Loss: 0.16947393119335175\n",
      "Subject 20, Epoch 627, Loss: 0.6461265385150909, Final Batch Loss: 0.09870743751525879\n",
      "Subject 20, Epoch 628, Loss: 0.6882193610072136, Final Batch Loss: 0.1229230985045433\n",
      "Subject 20, Epoch 629, Loss: 0.6897140592336655, Final Batch Loss: 0.14872726798057556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 630, Loss: 1.0406809449195862, Final Batch Loss: 0.4526137411594391\n",
      "Subject 20, Epoch 631, Loss: 0.7918632254004478, Final Batch Loss: 0.20736055076122284\n",
      "Subject 20, Epoch 632, Loss: 0.6849991083145142, Final Batch Loss: 0.13895359635353088\n",
      "Subject 20, Epoch 633, Loss: 0.7944235950708389, Final Batch Loss: 0.23483635485172272\n",
      "Subject 20, Epoch 634, Loss: 0.8590097874403, Final Batch Loss: 0.351603239774704\n",
      "Subject 20, Epoch 635, Loss: 0.7725168615579605, Final Batch Loss: 0.16772638261318207\n",
      "Subject 20, Epoch 636, Loss: 0.8498936742544174, Final Batch Loss: 0.25394076108932495\n",
      "Subject 20, Epoch 637, Loss: 0.6990882456302643, Final Batch Loss: 0.15605343878269196\n",
      "Subject 20, Epoch 638, Loss: 0.796584889292717, Final Batch Loss: 0.27543434500694275\n",
      "Subject 20, Epoch 639, Loss: 0.726659283041954, Final Batch Loss: 0.1576818823814392\n",
      "Subject 20, Epoch 640, Loss: 0.8148255795240402, Final Batch Loss: 0.13004665076732635\n",
      "Subject 20, Epoch 641, Loss: 0.6384571343660355, Final Batch Loss: 0.13038644194602966\n",
      "Subject 20, Epoch 642, Loss: 0.7205202430486679, Final Batch Loss: 0.1372659057378769\n",
      "Subject 20, Epoch 643, Loss: 0.7868693470954895, Final Batch Loss: 0.19635534286499023\n",
      "Subject 20, Epoch 644, Loss: 0.7942668795585632, Final Batch Loss: 0.26370999217033386\n",
      "Subject 20, Epoch 645, Loss: 0.7653699517250061, Final Batch Loss: 0.1995764672756195\n",
      "Subject 20, Epoch 646, Loss: 0.7835629060864449, Final Batch Loss: 0.38157039880752563\n",
      "Subject 20, Epoch 647, Loss: 0.6624521762132645, Final Batch Loss: 0.08519630134105682\n",
      "Subject 20, Epoch 648, Loss: 0.8295097649097443, Final Batch Loss: 0.30831217765808105\n",
      "Subject 20, Epoch 649, Loss: 0.6461801081895828, Final Batch Loss: 0.13435561954975128\n",
      "Subject 20, Epoch 650, Loss: 0.7204123884439468, Final Batch Loss: 0.1901359111070633\n",
      "Subject 20, Epoch 651, Loss: 0.6812690794467926, Final Batch Loss: 0.14469203352928162\n",
      "Subject 20, Epoch 652, Loss: 0.5954954475164413, Final Batch Loss: 0.1606493890285492\n",
      "Subject 20, Epoch 653, Loss: 0.6401552706956863, Final Batch Loss: 0.11054345965385437\n",
      "Subject 20, Epoch 654, Loss: 0.6184616982936859, Final Batch Loss: 0.07880127429962158\n",
      "Subject 20, Epoch 655, Loss: 0.7035464197397232, Final Batch Loss: 0.18313482403755188\n",
      "Subject 20, Epoch 656, Loss: 0.9348915815353394, Final Batch Loss: 0.3283287286758423\n",
      "Subject 20, Epoch 657, Loss: 0.6433544158935547, Final Batch Loss: 0.10930733382701874\n",
      "Subject 20, Epoch 658, Loss: 0.674884669482708, Final Batch Loss: 0.07534574717283249\n",
      "Subject 20, Epoch 659, Loss: 1.1303582340478897, Final Batch Loss: 0.5711424350738525\n",
      "Subject 20, Epoch 660, Loss: 0.6527160555124283, Final Batch Loss: 0.14742408692836761\n",
      "Subject 20, Epoch 661, Loss: 0.7802916467189789, Final Batch Loss: 0.24143724143505096\n",
      "Subject 20, Epoch 662, Loss: 0.6323672831058502, Final Batch Loss: 0.10787077248096466\n",
      "Subject 20, Epoch 663, Loss: 0.8530056476593018, Final Batch Loss: 0.3638079762458801\n",
      "Subject 20, Epoch 664, Loss: 0.7754207700490952, Final Batch Loss: 0.22895362973213196\n",
      "Subject 20, Epoch 665, Loss: 0.7726547867059708, Final Batch Loss: 0.15906471014022827\n",
      "Subject 20, Epoch 666, Loss: 0.6263633742928505, Final Batch Loss: 0.08135897666215897\n",
      "Subject 20, Epoch 667, Loss: 0.5761837661266327, Final Batch Loss: 0.04728226363658905\n",
      "Subject 20, Epoch 668, Loss: 0.5980450175702572, Final Batch Loss: 0.04598122462630272\n",
      "Subject 20, Epoch 669, Loss: 0.8073383122682571, Final Batch Loss: 0.09733638167381287\n",
      "Subject 20, Epoch 670, Loss: 0.6903653889894485, Final Batch Loss: 0.1354997307062149\n",
      "Subject 20, Epoch 671, Loss: 0.6752611547708511, Final Batch Loss: 0.06966185569763184\n",
      "Subject 20, Epoch 672, Loss: 0.7062076181173325, Final Batch Loss: 0.14755575358867645\n",
      "Subject 20, Epoch 673, Loss: 0.9019771292805672, Final Batch Loss: 0.343718945980072\n",
      "Subject 20, Epoch 674, Loss: 0.5474656000733376, Final Batch Loss: 0.14114923775196075\n",
      "Subject 20, Epoch 675, Loss: 0.6259548142552376, Final Batch Loss: 0.20263193547725677\n",
      "Subject 20, Epoch 676, Loss: 0.749781146645546, Final Batch Loss: 0.23675277829170227\n",
      "Subject 20, Epoch 677, Loss: 0.5939504876732826, Final Batch Loss: 0.10670115798711777\n",
      "Subject 20, Epoch 678, Loss: 0.6953757256269455, Final Batch Loss: 0.06925544142723083\n",
      "Subject 20, Epoch 679, Loss: 0.9891886860132217, Final Batch Loss: 0.46991047263145447\n",
      "Subject 20, Epoch 680, Loss: 1.0188651233911514, Final Batch Loss: 0.46573516726493835\n",
      "Subject 20, Epoch 681, Loss: 0.7517572492361069, Final Batch Loss: 0.1760285198688507\n",
      "Subject 20, Epoch 682, Loss: 0.6718253195285797, Final Batch Loss: 0.13206514716148376\n",
      "Subject 20, Epoch 683, Loss: 0.9292350262403488, Final Batch Loss: 0.2084747552871704\n",
      "Subject 20, Epoch 684, Loss: 0.7115990370512009, Final Batch Loss: 0.15681956708431244\n",
      "Subject 20, Epoch 685, Loss: 0.9308916479349136, Final Batch Loss: 0.3686555027961731\n",
      "Subject 20, Epoch 686, Loss: 0.6927227228879929, Final Batch Loss: 0.1954474300146103\n",
      "Subject 20, Epoch 687, Loss: 0.6877743378281593, Final Batch Loss: 0.11778773367404938\n",
      "Subject 20, Epoch 688, Loss: 0.6366400122642517, Final Batch Loss: 0.14587360620498657\n",
      "Subject 20, Epoch 689, Loss: 0.6718592196702957, Final Batch Loss: 0.20412559807300568\n",
      "Subject 20, Epoch 690, Loss: 0.6290711015462875, Final Batch Loss: 0.1683756709098816\n",
      "Subject 20, Epoch 691, Loss: 0.6810908988118172, Final Batch Loss: 0.15845467150211334\n",
      "Subject 20, Epoch 692, Loss: 0.6275003552436829, Final Batch Loss: 0.08003440499305725\n",
      "Subject 20, Epoch 693, Loss: 0.6464304104447365, Final Batch Loss: 0.06756872683763504\n",
      "Subject 20, Epoch 694, Loss: 0.7194276228547096, Final Batch Loss: 0.2046356052160263\n",
      "Subject 20, Epoch 695, Loss: 0.7085041999816895, Final Batch Loss: 0.2125343531370163\n",
      "Subject 20, Epoch 696, Loss: 0.6559052094817162, Final Batch Loss: 0.05705926567316055\n",
      "Subject 20, Epoch 697, Loss: 0.5868047997355461, Final Batch Loss: 0.07992337644100189\n",
      "Subject 20, Epoch 698, Loss: 0.6996232122182846, Final Batch Loss: 0.08705459535121918\n",
      "Subject 20, Epoch 699, Loss: 0.5640214905142784, Final Batch Loss: 0.08540546149015427\n",
      "Subject 20, Epoch 700, Loss: 0.6946466416120529, Final Batch Loss: 0.24810628592967987\n",
      "Subject 20, Epoch 701, Loss: 0.47288820520043373, Final Batch Loss: 0.03676054999232292\n",
      "Subject 20, Epoch 702, Loss: 0.7604202926158905, Final Batch Loss: 0.25769293308258057\n",
      "Subject 20, Epoch 703, Loss: 0.7327226400375366, Final Batch Loss: 0.19317558407783508\n",
      "Subject 20, Epoch 704, Loss: 0.7395506501197815, Final Batch Loss: 0.21598045527935028\n",
      "Subject 20, Epoch 705, Loss: 0.6050537452101707, Final Batch Loss: 0.0928666964173317\n",
      "Subject 20, Epoch 706, Loss: 0.7811962142586708, Final Batch Loss: 0.3009830415248871\n",
      "Subject 20, Epoch 707, Loss: 0.7179979309439659, Final Batch Loss: 0.10466840118169785\n",
      "Subject 20, Epoch 708, Loss: 0.6935926526784897, Final Batch Loss: 0.06543032824993134\n",
      "Subject 20, Epoch 709, Loss: 0.5977310538291931, Final Batch Loss: 0.1391233205795288\n",
      "Subject 20, Epoch 710, Loss: 0.6681706011295319, Final Batch Loss: 0.12075966596603394\n",
      "Subject 20, Epoch 711, Loss: 0.7150094509124756, Final Batch Loss: 0.13828204572200775\n",
      "Subject 20, Epoch 712, Loss: 0.6212515905499458, Final Batch Loss: 0.059889622032642365\n",
      "Subject 20, Epoch 713, Loss: 0.9772516116499901, Final Batch Loss: 0.5542100667953491\n",
      "Subject 20, Epoch 714, Loss: 0.74299356341362, Final Batch Loss: 0.2287987321615219\n",
      "Subject 20, Epoch 715, Loss: 1.0211614221334457, Final Batch Loss: 0.5191869735717773\n",
      "Subject 20, Epoch 716, Loss: 0.5691249817609787, Final Batch Loss: 0.09289656579494476\n",
      "Subject 20, Epoch 717, Loss: 0.6399050652980804, Final Batch Loss: 0.15962043404579163\n",
      "Subject 20, Epoch 718, Loss: 0.6414579302072525, Final Batch Loss: 0.08517149090766907\n",
      "Subject 20, Epoch 719, Loss: 0.6192890405654907, Final Batch Loss: 0.13151897490024567\n",
      "Subject 20, Epoch 720, Loss: 0.6370963156223297, Final Batch Loss: 0.0984605997800827\n",
      "Subject 20, Epoch 721, Loss: 0.6401447281241417, Final Batch Loss: 0.12867820262908936\n",
      "Subject 20, Epoch 722, Loss: 0.5382025763392448, Final Batch Loss: 0.0810248926281929\n",
      "Subject 20, Epoch 723, Loss: 0.5992976054549217, Final Batch Loss: 0.15640513598918915\n",
      "Subject 20, Epoch 724, Loss: 0.7579157501459122, Final Batch Loss: 0.23046554625034332\n",
      "Subject 20, Epoch 725, Loss: 0.7431281358003616, Final Batch Loss: 0.1467437595129013\n",
      "Subject 20, Epoch 726, Loss: 0.6572469174861908, Final Batch Loss: 0.1360279619693756\n",
      "Subject 20, Epoch 727, Loss: 0.8329634070396423, Final Batch Loss: 0.1975499391555786\n",
      "Subject 20, Epoch 728, Loss: 0.5582556948065758, Final Batch Loss: 0.08094152808189392\n",
      "Subject 20, Epoch 729, Loss: 0.7113398611545563, Final Batch Loss: 0.22970335185527802\n",
      "Subject 20, Epoch 730, Loss: 0.5506177842617035, Final Batch Loss: 0.1645476520061493\n",
      "Subject 20, Epoch 731, Loss: 0.563329391181469, Final Batch Loss: 0.15916182100772858\n",
      "Subject 20, Epoch 732, Loss: 0.5201748497784138, Final Batch Loss: 0.06223570182919502\n",
      "Subject 20, Epoch 733, Loss: 0.6375207602977753, Final Batch Loss: 0.10802547633647919\n",
      "Subject 20, Epoch 734, Loss: 0.7614665403962135, Final Batch Loss: 0.23702268302440643\n",
      "Subject 20, Epoch 735, Loss: 0.5728245228528976, Final Batch Loss: 0.12363915145397186\n",
      "Subject 20, Epoch 736, Loss: 0.5004783868789673, Final Batch Loss: 0.1296735405921936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 737, Loss: 0.6646216586232185, Final Batch Loss: 0.24236875772476196\n",
      "Subject 20, Epoch 738, Loss: 0.662579633295536, Final Batch Loss: 0.262666255235672\n",
      "Subject 20, Epoch 739, Loss: 0.6841345727443695, Final Batch Loss: 0.2657565176486969\n",
      "Subject 20, Epoch 740, Loss: 0.6707337945699692, Final Batch Loss: 0.17888620495796204\n",
      "Subject 20, Epoch 741, Loss: 0.5898948982357979, Final Batch Loss: 0.04236336797475815\n",
      "Subject 20, Epoch 742, Loss: 0.6830967739224434, Final Batch Loss: 0.11434803158044815\n",
      "Subject 20, Epoch 743, Loss: 0.6698699817061424, Final Batch Loss: 0.11090756207704544\n",
      "Subject 20, Epoch 744, Loss: 0.6300237961113453, Final Batch Loss: 0.057591501623392105\n",
      "Subject 20, Epoch 745, Loss: 0.6332842409610748, Final Batch Loss: 0.1325320452451706\n",
      "Subject 20, Epoch 746, Loss: 0.6483563184738159, Final Batch Loss: 0.1184190958738327\n",
      "Subject 20, Epoch 747, Loss: 0.6147522628307343, Final Batch Loss: 0.14376245439052582\n",
      "Subject 20, Epoch 748, Loss: 0.6811227053403854, Final Batch Loss: 0.24121461808681488\n",
      "Subject 20, Epoch 749, Loss: 0.8473963737487793, Final Batch Loss: 0.4490409791469574\n",
      "Subject 20, Epoch 750, Loss: 0.6393036060035229, Final Batch Loss: 0.033970799297094345\n",
      "Subject 20, Epoch 751, Loss: 0.7121608778834343, Final Batch Loss: 0.24171018600463867\n",
      "Subject 20, Epoch 752, Loss: 0.4934035502374172, Final Batch Loss: 0.03575427457690239\n",
      "Subject 20, Epoch 753, Loss: 0.719757154583931, Final Batch Loss: 0.24867424368858337\n",
      "Subject 20, Epoch 754, Loss: 0.5119004249572754, Final Batch Loss: 0.0832585021853447\n",
      "Subject 20, Epoch 755, Loss: 0.6406878530979156, Final Batch Loss: 0.11245602369308472\n",
      "Subject 20, Epoch 756, Loss: 0.638725183904171, Final Batch Loss: 0.1821531057357788\n",
      "Subject 20, Epoch 757, Loss: 0.5557601898908615, Final Batch Loss: 0.12983781099319458\n",
      "Subject 20, Epoch 758, Loss: 0.5149211287498474, Final Batch Loss: 0.1023453027009964\n",
      "Subject 20, Epoch 759, Loss: 0.860548660159111, Final Batch Loss: 0.1394614726305008\n",
      "Subject 20, Epoch 760, Loss: 0.618212379515171, Final Batch Loss: 0.18210452795028687\n",
      "Subject 20, Epoch 761, Loss: 0.5150100812315941, Final Batch Loss: 0.08494842797517776\n",
      "Subject 20, Epoch 762, Loss: 0.4752810299396515, Final Batch Loss: 0.12249311804771423\n",
      "Subject 20, Epoch 763, Loss: 0.5359629988670349, Final Batch Loss: 0.05061952769756317\n",
      "Subject 20, Epoch 764, Loss: 0.7292376458644867, Final Batch Loss: 0.20912028849124908\n",
      "Subject 20, Epoch 765, Loss: 0.7016502544283867, Final Batch Loss: 0.18695640563964844\n",
      "Subject 20, Epoch 766, Loss: 0.6322854533791542, Final Batch Loss: 0.1018550843000412\n",
      "Subject 20, Epoch 767, Loss: 0.6353544071316719, Final Batch Loss: 0.10718914121389389\n",
      "Subject 20, Epoch 768, Loss: 0.5056931525468826, Final Batch Loss: 0.06758157908916473\n",
      "Subject 20, Epoch 769, Loss: 0.5064820982515812, Final Batch Loss: 0.03218306228518486\n",
      "Subject 20, Epoch 770, Loss: 0.5035139694809914, Final Batch Loss: 0.06832712143659592\n",
      "Subject 20, Epoch 771, Loss: 0.6542653515934944, Final Batch Loss: 0.17155689001083374\n",
      "Subject 20, Epoch 772, Loss: 0.5497230030596256, Final Batch Loss: 0.044671330600976944\n",
      "Subject 20, Epoch 773, Loss: 0.5269568413496017, Final Batch Loss: 0.06714877486228943\n",
      "Subject 20, Epoch 774, Loss: 0.6333283185958862, Final Batch Loss: 0.17240101099014282\n",
      "Subject 20, Epoch 775, Loss: 0.7866100743412971, Final Batch Loss: 0.2931344509124756\n",
      "Subject 20, Epoch 776, Loss: 0.557232953608036, Final Batch Loss: 0.06548166275024414\n",
      "Subject 20, Epoch 777, Loss: 0.6640638634562492, Final Batch Loss: 0.2531518340110779\n",
      "Subject 20, Epoch 778, Loss: 0.5102511867880821, Final Batch Loss: 0.05492255836725235\n",
      "Subject 20, Epoch 779, Loss: 0.680419921875, Final Batch Loss: 0.2630149722099304\n",
      "Subject 20, Epoch 780, Loss: 0.6983358412981033, Final Batch Loss: 0.29917553067207336\n",
      "Subject 20, Epoch 781, Loss: 0.7082692757248878, Final Batch Loss: 0.20245233178138733\n",
      "Subject 20, Epoch 782, Loss: 0.5987228229641914, Final Batch Loss: 0.13811521232128143\n",
      "Subject 20, Epoch 783, Loss: 0.8041743338108063, Final Batch Loss: 0.3020564019680023\n",
      "Subject 20, Epoch 784, Loss: 0.6166534349322319, Final Batch Loss: 0.17371965944766998\n",
      "Subject 20, Epoch 785, Loss: 0.5806309953331947, Final Batch Loss: 0.0689760372042656\n",
      "Subject 20, Epoch 786, Loss: 0.7214064970612526, Final Batch Loss: 0.15908244252204895\n",
      "Subject 20, Epoch 787, Loss: 0.47381697222590446, Final Batch Loss: 0.02733756974339485\n",
      "Subject 20, Epoch 788, Loss: 0.723254919052124, Final Batch Loss: 0.1677914559841156\n",
      "Subject 20, Epoch 789, Loss: 0.6585366800427437, Final Batch Loss: 0.10146819800138474\n",
      "Subject 20, Epoch 790, Loss: 0.6732949316501617, Final Batch Loss: 0.2349376678466797\n",
      "Subject 20, Epoch 791, Loss: 0.5264380723237991, Final Batch Loss: 0.06918257474899292\n",
      "Subject 20, Epoch 792, Loss: 0.5787227749824524, Final Batch Loss: 0.08080626279115677\n",
      "Subject 20, Epoch 793, Loss: 0.5289168134331703, Final Batch Loss: 0.09406748414039612\n",
      "Subject 20, Epoch 794, Loss: 0.6026507169008255, Final Batch Loss: 0.10357671976089478\n",
      "Subject 20, Epoch 795, Loss: 0.4587259814143181, Final Batch Loss: 0.03561802953481674\n",
      "Subject 20, Epoch 796, Loss: 0.5324370004236698, Final Batch Loss: 0.03676671162247658\n",
      "Subject 20, Epoch 797, Loss: 0.6234532743692398, Final Batch Loss: 0.1980646848678589\n",
      "Subject 20, Epoch 798, Loss: 0.5386630222201347, Final Batch Loss: 0.07831548899412155\n",
      "Subject 20, Epoch 799, Loss: 0.5849878266453743, Final Batch Loss: 0.10122739523649216\n",
      "Subject 20, Epoch 800, Loss: 0.6409801989793777, Final Batch Loss: 0.14734257757663727\n",
      "Subject 20, Epoch 801, Loss: 0.6863890588283539, Final Batch Loss: 0.14446832239627838\n",
      "Subject 20, Epoch 802, Loss: 0.5907373577356339, Final Batch Loss: 0.18435083329677582\n",
      "Subject 20, Epoch 803, Loss: 0.4518596902489662, Final Batch Loss: 0.014127485454082489\n",
      "Subject 20, Epoch 804, Loss: 0.5937177091836929, Final Batch Loss: 0.09231335669755936\n",
      "Subject 20, Epoch 805, Loss: 0.46934446692466736, Final Batch Loss: 0.08742441236972809\n",
      "Subject 20, Epoch 806, Loss: 0.7143313363194466, Final Batch Loss: 0.2426932454109192\n",
      "Subject 20, Epoch 807, Loss: 0.7328955382108688, Final Batch Loss: 0.24258418381214142\n",
      "Subject 20, Epoch 808, Loss: 0.5898396447300911, Final Batch Loss: 0.22601629793643951\n",
      "Subject 20, Epoch 809, Loss: 0.6551975235342979, Final Batch Loss: 0.17261014878749847\n",
      "Subject 20, Epoch 810, Loss: 0.5060307886451483, Final Batch Loss: 0.024244485422968864\n",
      "Subject 20, Epoch 811, Loss: 0.5379042029380798, Final Batch Loss: 0.1607954055070877\n",
      "Subject 20, Epoch 812, Loss: 0.5356835201382637, Final Batch Loss: 0.13801704347133636\n",
      "Subject 20, Epoch 813, Loss: 0.5737361162900925, Final Batch Loss: 0.11407367885112762\n",
      "Subject 20, Epoch 814, Loss: 0.7093316093087196, Final Batch Loss: 0.32584601640701294\n",
      "Subject 20, Epoch 815, Loss: 0.545261561870575, Final Batch Loss: 0.181810200214386\n",
      "Subject 20, Epoch 816, Loss: 0.6065428778529167, Final Batch Loss: 0.20981650054454803\n",
      "Subject 20, Epoch 817, Loss: 0.5076235216110945, Final Batch Loss: 0.03014269284904003\n",
      "Subject 20, Epoch 818, Loss: 0.5142078623175621, Final Batch Loss: 0.14662009477615356\n",
      "Subject 20, Epoch 819, Loss: 0.5380571410059929, Final Batch Loss: 0.1912745237350464\n",
      "Subject 20, Epoch 820, Loss: 0.7532740607857704, Final Batch Loss: 0.29104045033454895\n",
      "Subject 20, Epoch 821, Loss: 0.5290519595146179, Final Batch Loss: 0.10842423141002655\n",
      "Subject 20, Epoch 822, Loss: 0.5788705572485924, Final Batch Loss: 0.0689820721745491\n",
      "Subject 20, Epoch 823, Loss: 0.6497095376253128, Final Batch Loss: 0.302563339471817\n",
      "Subject 20, Epoch 824, Loss: 0.5149390175938606, Final Batch Loss: 0.17764562368392944\n",
      "Subject 20, Epoch 825, Loss: 0.5178399756550789, Final Batch Loss: 0.12826532125473022\n",
      "Subject 20, Epoch 826, Loss: 0.7578780055046082, Final Batch Loss: 0.3151005804538727\n",
      "Subject 20, Epoch 827, Loss: 0.519717626273632, Final Batch Loss: 0.09054997563362122\n",
      "Subject 20, Epoch 828, Loss: 0.543142169713974, Final Batch Loss: 0.0971599593758583\n",
      "Subject 20, Epoch 829, Loss: 0.45541661977767944, Final Batch Loss: 0.06765495985746384\n",
      "Subject 20, Epoch 830, Loss: 0.5998706072568893, Final Batch Loss: 0.15181738138198853\n",
      "Subject 20, Epoch 831, Loss: 0.48283597081899643, Final Batch Loss: 0.09463674575090408\n",
      "Subject 20, Epoch 832, Loss: 0.47185737639665604, Final Batch Loss: 0.09120191633701324\n",
      "Subject 20, Epoch 833, Loss: 0.4050019606947899, Final Batch Loss: 0.04872344434261322\n",
      "Subject 20, Epoch 834, Loss: 0.559974879026413, Final Batch Loss: 0.20891256630420685\n",
      "Subject 20, Epoch 835, Loss: 0.4401194080710411, Final Batch Loss: 0.07874548435211182\n",
      "Subject 20, Epoch 836, Loss: 0.5784590467810631, Final Batch Loss: 0.1460309475660324\n",
      "Subject 20, Epoch 837, Loss: 0.5919205099344254, Final Batch Loss: 0.21995864808559418\n",
      "Subject 20, Epoch 838, Loss: 0.5178152993321419, Final Batch Loss: 0.12300403416156769\n",
      "Subject 20, Epoch 839, Loss: 0.48814237490296364, Final Batch Loss: 0.043372321873903275\n",
      "Subject 20, Epoch 840, Loss: 0.3847005106508732, Final Batch Loss: 0.03791535273194313\n",
      "Subject 20, Epoch 841, Loss: 0.6497202664613724, Final Batch Loss: 0.17595551908016205\n",
      "Subject 20, Epoch 842, Loss: 0.4679114408791065, Final Batch Loss: 0.05599066615104675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 843, Loss: 0.3565152511000633, Final Batch Loss: 0.06646816432476044\n",
      "Subject 20, Epoch 844, Loss: 0.47312355041503906, Final Batch Loss: 0.04323451966047287\n",
      "Subject 20, Epoch 845, Loss: 0.49170249700546265, Final Batch Loss: 0.17876030504703522\n",
      "Subject 20, Epoch 846, Loss: 0.5373286753892899, Final Batch Loss: 0.08994501829147339\n",
      "Subject 20, Epoch 847, Loss: 0.7506758868694305, Final Batch Loss: 0.21058301627635956\n",
      "Subject 20, Epoch 848, Loss: 0.556461937725544, Final Batch Loss: 0.12074095755815506\n",
      "Subject 20, Epoch 849, Loss: 0.3953806720674038, Final Batch Loss: 0.0530773289501667\n",
      "Subject 20, Epoch 850, Loss: 0.4925357922911644, Final Batch Loss: 0.10142043977975845\n",
      "Subject 20, Epoch 851, Loss: 0.4680699482560158, Final Batch Loss: 0.14042872190475464\n",
      "Subject 20, Epoch 852, Loss: 0.7580598145723343, Final Batch Loss: 0.15015798807144165\n",
      "Subject 20, Epoch 853, Loss: 0.5136668235063553, Final Batch Loss: 0.05874045193195343\n",
      "Subject 20, Epoch 854, Loss: 0.4735068380832672, Final Batch Loss: 0.04629293829202652\n",
      "Subject 20, Epoch 855, Loss: 0.4911498352885246, Final Batch Loss: 0.10679244250059128\n",
      "Subject 20, Epoch 856, Loss: 0.6616531386971474, Final Batch Loss: 0.2592739164829254\n",
      "Subject 20, Epoch 857, Loss: 0.5079765133559704, Final Batch Loss: 0.05731530115008354\n",
      "Subject 20, Epoch 858, Loss: 0.5129373967647552, Final Batch Loss: 0.09606248140335083\n",
      "Subject 20, Epoch 859, Loss: 0.4863133952021599, Final Batch Loss: 0.07633674889802933\n",
      "Subject 20, Epoch 860, Loss: 0.6437270753085613, Final Batch Loss: 0.3970228433609009\n",
      "Subject 20, Epoch 861, Loss: 0.5412793755531311, Final Batch Loss: 0.12686672806739807\n",
      "Subject 20, Epoch 862, Loss: 0.39731764793395996, Final Batch Loss: 0.06258910894393921\n",
      "Subject 20, Epoch 863, Loss: 0.5052170678973198, Final Batch Loss: 0.1764833778142929\n",
      "Subject 20, Epoch 864, Loss: 0.5922407582402229, Final Batch Loss: 0.22460263967514038\n",
      "Subject 20, Epoch 865, Loss: 0.41277460753917694, Final Batch Loss: 0.07469062507152557\n",
      "Subject 20, Epoch 866, Loss: 0.5172451436519623, Final Batch Loss: 0.10227806866168976\n",
      "Subject 20, Epoch 867, Loss: 0.4770556911826134, Final Batch Loss: 0.13827942311763763\n",
      "Subject 20, Epoch 868, Loss: 0.46269064396619797, Final Batch Loss: 0.07302939146757126\n",
      "Subject 20, Epoch 869, Loss: 0.44515739381313324, Final Batch Loss: 0.08713651448488235\n",
      "Subject 20, Epoch 870, Loss: 0.6713925749063492, Final Batch Loss: 0.3138527572154999\n",
      "Subject 20, Epoch 871, Loss: 0.43261438608169556, Final Batch Loss: 0.1260313093662262\n",
      "Subject 20, Epoch 872, Loss: 0.426938034594059, Final Batch Loss: 0.06634119153022766\n",
      "Subject 20, Epoch 873, Loss: 0.6160078309476376, Final Batch Loss: 0.163402259349823\n",
      "Subject 20, Epoch 874, Loss: 0.5224188566207886, Final Batch Loss: 0.13303397595882416\n",
      "Subject 20, Epoch 875, Loss: 0.6133914813399315, Final Batch Loss: 0.09701976180076599\n",
      "Subject 20, Epoch 876, Loss: 0.5530041232705116, Final Batch Loss: 0.18896499276161194\n",
      "Subject 20, Epoch 877, Loss: 0.4093834385275841, Final Batch Loss: 0.1387156993150711\n",
      "Subject 20, Epoch 878, Loss: 0.6205385588109493, Final Batch Loss: 0.034083325415849686\n",
      "Subject 20, Epoch 879, Loss: 0.460434976965189, Final Batch Loss: 0.041013311594724655\n",
      "Subject 20, Epoch 880, Loss: 0.4597764015197754, Final Batch Loss: 0.06399184465408325\n",
      "Subject 20, Epoch 881, Loss: 0.46402666345238686, Final Batch Loss: 0.04134785011410713\n",
      "Subject 20, Epoch 882, Loss: 0.5305367633700371, Final Batch Loss: 0.06016991287469864\n",
      "Subject 20, Epoch 883, Loss: 0.6836868487298489, Final Batch Loss: 0.3281381130218506\n",
      "Subject 20, Epoch 884, Loss: 0.4457673132419586, Final Batch Loss: 0.09910058230161667\n",
      "Subject 20, Epoch 885, Loss: 0.6313894763588905, Final Batch Loss: 0.1963823437690735\n",
      "Subject 20, Epoch 886, Loss: 0.637459434568882, Final Batch Loss: 0.32051223516464233\n",
      "Subject 20, Epoch 887, Loss: 0.4337068349123001, Final Batch Loss: 0.0850372314453125\n",
      "Subject 20, Epoch 888, Loss: 0.46554454416036606, Final Batch Loss: 0.08702804893255234\n",
      "Subject 20, Epoch 889, Loss: 0.4593255389481783, Final Batch Loss: 0.026624629274010658\n",
      "Subject 20, Epoch 890, Loss: 0.48578043282032013, Final Batch Loss: 0.11917382478713989\n",
      "Subject 20, Epoch 891, Loss: 0.47642453014850616, Final Batch Loss: 0.09711822867393494\n",
      "Subject 20, Epoch 892, Loss: 0.5514244213700294, Final Batch Loss: 0.2807939052581787\n",
      "Subject 20, Epoch 893, Loss: 0.4337105229496956, Final Batch Loss: 0.12440422922372818\n",
      "Subject 20, Epoch 894, Loss: 0.42580288648605347, Final Batch Loss: 0.03164122998714447\n",
      "Subject 20, Epoch 895, Loss: 0.4968712851405144, Final Batch Loss: 0.09325624257326126\n",
      "Subject 20, Epoch 896, Loss: 0.46370134875178337, Final Batch Loss: 0.039931055158376694\n",
      "Subject 20, Epoch 897, Loss: 0.5550047606229782, Final Batch Loss: 0.22253961861133575\n",
      "Subject 20, Epoch 898, Loss: 0.49686428904533386, Final Batch Loss: 0.11556114256381989\n",
      "Subject 20, Epoch 899, Loss: 0.6151830330491066, Final Batch Loss: 0.12344136834144592\n",
      "Subject 20, Epoch 900, Loss: 0.4727958142757416, Final Batch Loss: 0.10009599477052689\n",
      "Subject 20, Epoch 901, Loss: 0.4651135541498661, Final Batch Loss: 0.05057711526751518\n",
      "Subject 20, Epoch 902, Loss: 0.41875604540109634, Final Batch Loss: 0.021729066967964172\n",
      "Subject 20, Epoch 903, Loss: 0.656886488199234, Final Batch Loss: 0.07979193329811096\n",
      "Subject 20, Epoch 904, Loss: 0.5783494412899017, Final Batch Loss: 0.24089929461479187\n",
      "Subject 20, Epoch 905, Loss: 0.40927208960056305, Final Batch Loss: 0.03739573061466217\n",
      "Subject 20, Epoch 906, Loss: 0.4544673226773739, Final Batch Loss: 0.17828373610973358\n",
      "Subject 20, Epoch 907, Loss: 0.4785514585673809, Final Batch Loss: 0.22854314744472504\n",
      "Subject 20, Epoch 908, Loss: 0.4517718367278576, Final Batch Loss: 0.05007723346352577\n",
      "Subject 20, Epoch 909, Loss: 0.4972448796033859, Final Batch Loss: 0.16273996233940125\n",
      "Subject 20, Epoch 910, Loss: 0.5181940123438835, Final Batch Loss: 0.17564529180526733\n",
      "Subject 20, Epoch 911, Loss: 0.37226148322224617, Final Batch Loss: 0.05768948420882225\n",
      "Subject 20, Epoch 912, Loss: 0.4761466756463051, Final Batch Loss: 0.06188884377479553\n",
      "Subject 20, Epoch 913, Loss: 0.46229643374681473, Final Batch Loss: 0.09486551582813263\n",
      "Subject 20, Epoch 914, Loss: 0.5145151689648628, Final Batch Loss: 0.11843838542699814\n",
      "Subject 20, Epoch 915, Loss: 0.4664434716105461, Final Batch Loss: 0.07616832852363586\n",
      "Subject 20, Epoch 916, Loss: 0.3563991189002991, Final Batch Loss: 0.0682220533490181\n",
      "Subject 20, Epoch 917, Loss: 0.4993603751063347, Final Batch Loss: 0.06871693581342697\n",
      "Subject 20, Epoch 918, Loss: 0.43944311141967773, Final Batch Loss: 0.03684832155704498\n",
      "Subject 20, Epoch 919, Loss: 0.3383956551551819, Final Batch Loss: 0.03215797245502472\n",
      "Subject 20, Epoch 920, Loss: 0.6200946643948555, Final Batch Loss: 0.34558963775634766\n",
      "Subject 20, Epoch 921, Loss: 0.4295053854584694, Final Batch Loss: 0.1516418606042862\n",
      "Subject 20, Epoch 922, Loss: 0.3460613489151001, Final Batch Loss: 0.07802747189998627\n",
      "Subject 20, Epoch 923, Loss: 0.5228674486279488, Final Batch Loss: 0.17844010889530182\n",
      "Subject 20, Epoch 924, Loss: 0.3269011117517948, Final Batch Loss: 0.04760486260056496\n",
      "Subject 20, Epoch 925, Loss: 0.4020641501992941, Final Batch Loss: 0.019689036533236504\n",
      "Subject 20, Epoch 926, Loss: 0.3625781312584877, Final Batch Loss: 0.03486859053373337\n",
      "Subject 20, Epoch 927, Loss: 0.4106396287679672, Final Batch Loss: 0.12241203337907791\n",
      "Subject 20, Epoch 928, Loss: 0.4211426414549351, Final Batch Loss: 0.140624538064003\n",
      "Subject 20, Epoch 929, Loss: 0.29113880172371864, Final Batch Loss: 0.05527998134493828\n",
      "Subject 20, Epoch 930, Loss: 0.5035004988312721, Final Batch Loss: 0.08824215829372406\n",
      "Subject 20, Epoch 931, Loss: 0.36378346383571625, Final Batch Loss: 0.07549171149730682\n",
      "Subject 20, Epoch 932, Loss: 0.35455814749002457, Final Batch Loss: 0.07676956057548523\n",
      "Subject 20, Epoch 933, Loss: 0.5536886751651764, Final Batch Loss: 0.08445068448781967\n",
      "Subject 20, Epoch 934, Loss: 0.46483464539051056, Final Batch Loss: 0.0664290115237236\n",
      "Subject 20, Epoch 935, Loss: 0.32264209538698196, Final Batch Loss: 0.027006328105926514\n",
      "Subject 20, Epoch 936, Loss: 0.44523491710424423, Final Batch Loss: 0.03066248446702957\n",
      "Subject 20, Epoch 937, Loss: 0.4092015027999878, Final Batch Loss: 0.08183615654706955\n",
      "Subject 20, Epoch 938, Loss: 0.43105299025774, Final Batch Loss: 0.03228612244129181\n",
      "Subject 20, Epoch 939, Loss: 0.38237500190734863, Final Batch Loss: 0.14196152985095978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20, Epoch 940, Loss: 0.5094058178365231, Final Batch Loss: 0.06061476841568947\n",
      "Subject 20, Epoch 941, Loss: 0.43026573956012726, Final Batch Loss: 0.04943590611219406\n",
      "Subject 20, Epoch 942, Loss: 0.3632921986281872, Final Batch Loss: 0.04257887229323387\n",
      "Subject 20, Epoch 943, Loss: 0.3194118104875088, Final Batch Loss: 0.07990914583206177\n",
      "Subject 20, Epoch 944, Loss: 0.34300709515810013, Final Batch Loss: 0.03640823811292648\n",
      "Subject 20, Epoch 945, Loss: 0.31671446189284325, Final Batch Loss: 0.023316223174333572\n",
      "Subject 20, Epoch 946, Loss: 0.5011211708188057, Final Batch Loss: 0.20150279998779297\n",
      "Subject 20, Epoch 947, Loss: 0.4958079755306244, Final Batch Loss: 0.2258155792951584\n",
      "Subject 20, Epoch 948, Loss: 0.3934917449951172, Final Batch Loss: 0.11064191162586212\n",
      "Subject 20, Epoch 949, Loss: 0.32511530444025993, Final Batch Loss: 0.05247778445482254\n",
      "Subject 20, Epoch 950, Loss: 0.4845297448337078, Final Batch Loss: 0.18361860513687134\n",
      "Subject 20, Epoch 951, Loss: 0.4386191889643669, Final Batch Loss: 0.07004671543836594\n",
      "Subject 20, Epoch 952, Loss: 0.3925953097641468, Final Batch Loss: 0.09634284675121307\n",
      "Subject 20, Epoch 953, Loss: 0.4411422163248062, Final Batch Loss: 0.14148582518100739\n",
      "Subject 20, Epoch 954, Loss: 0.4396625366061926, Final Batch Loss: 0.01998939923942089\n",
      "Subject 20, Epoch 955, Loss: 0.414797306060791, Final Batch Loss: 0.14024430513381958\n",
      "Subject 20, Epoch 956, Loss: 0.3595067523419857, Final Batch Loss: 0.04660959169268608\n",
      "Subject 20, Epoch 957, Loss: 0.4656435623764992, Final Batch Loss: 0.09249244630336761\n",
      "Subject 20, Epoch 958, Loss: 0.39798544347286224, Final Batch Loss: 0.05472441017627716\n",
      "Subject 20, Epoch 959, Loss: 0.39133019745349884, Final Batch Loss: 0.024457238614559174\n",
      "Subject 20, Epoch 960, Loss: 0.44526509940624237, Final Batch Loss: 0.1484784185886383\n",
      "Subject 20, Epoch 961, Loss: 0.3811862990260124, Final Batch Loss: 0.05021149665117264\n",
      "Subject 20, Epoch 962, Loss: 0.4458988532423973, Final Batch Loss: 0.08648993819952011\n",
      "Subject 20, Epoch 963, Loss: 0.563367310911417, Final Batch Loss: 0.3045860230922699\n",
      "Subject 20, Epoch 964, Loss: 0.39151908457279205, Final Batch Loss: 0.12463107705116272\n",
      "Subject 20, Epoch 965, Loss: 0.4008457027375698, Final Batch Loss: 0.045659828931093216\n",
      "Subject 20, Epoch 966, Loss: 0.5178334787487984, Final Batch Loss: 0.13118891417980194\n",
      "Subject 20, Epoch 967, Loss: 0.41911477595567703, Final Batch Loss: 0.1310836672782898\n",
      "Subject 20, Epoch 968, Loss: 0.40058955550193787, Final Batch Loss: 0.12478795647621155\n",
      "Subject 20, Epoch 969, Loss: 0.5343413278460503, Final Batch Loss: 0.17558620870113373\n",
      "Subject 20, Epoch 970, Loss: 0.32365700230002403, Final Batch Loss: 0.14689753949642181\n",
      "Subject 20, Epoch 971, Loss: 0.3184264041483402, Final Batch Loss: 0.050556402653455734\n",
      "Subject 20, Epoch 972, Loss: 0.4241602197289467, Final Batch Loss: 0.09152960032224655\n",
      "Subject 20, Epoch 973, Loss: 0.5849470868706703, Final Batch Loss: 0.09991668909788132\n",
      "Subject 20, Epoch 974, Loss: 0.33488695323467255, Final Batch Loss: 0.05961807072162628\n",
      "Subject 20, Epoch 975, Loss: 0.37562232464551926, Final Batch Loss: 0.08917031437158585\n",
      "Subject 20, Epoch 976, Loss: 0.3638647496700287, Final Batch Loss: 0.04029585421085358\n",
      "Subject 20, Epoch 977, Loss: 0.41391919925808907, Final Batch Loss: 0.04881281778216362\n",
      "Subject 20, Epoch 978, Loss: 0.453701414167881, Final Batch Loss: 0.03683841973543167\n",
      "Subject 20, Epoch 979, Loss: 0.40812980756163597, Final Batch Loss: 0.03950093314051628\n",
      "Subject 20, Epoch 980, Loss: 0.540711298584938, Final Batch Loss: 0.2634662389755249\n",
      "Subject 20, Epoch 981, Loss: 0.3742641881108284, Final Batch Loss: 0.09357009083032608\n",
      "Subject 20, Epoch 982, Loss: 0.39691613614559174, Final Batch Loss: 0.04788216948509216\n",
      "Subject 20, Epoch 983, Loss: 0.4003239870071411, Final Batch Loss: 0.07423418760299683\n",
      "Subject 20, Epoch 984, Loss: 0.2928832322359085, Final Batch Loss: 0.022755257785320282\n",
      "Subject 20, Epoch 985, Loss: 0.3197201956063509, Final Batch Loss: 0.022699328139424324\n",
      "Subject 20, Epoch 986, Loss: 0.4983034282922745, Final Batch Loss: 0.2991368770599365\n",
      "Subject 20, Epoch 987, Loss: 0.3343309387564659, Final Batch Loss: 0.03189270943403244\n",
      "Subject 20, Epoch 988, Loss: 0.4135204739868641, Final Batch Loss: 0.0985199436545372\n",
      "Subject 20, Epoch 989, Loss: 0.45278026163578033, Final Batch Loss: 0.18396879732608795\n",
      "Subject 20, Epoch 990, Loss: 0.3109407536685467, Final Batch Loss: 0.008542228490114212\n",
      "Subject 20, Epoch 991, Loss: 0.48803170025348663, Final Batch Loss: 0.17228271067142487\n",
      "Subject 20, Epoch 992, Loss: 0.34285016637295485, Final Batch Loss: 0.009290087036788464\n",
      "Subject 20, Epoch 993, Loss: 0.2941951169632375, Final Batch Loss: 0.005153098609298468\n",
      "Subject 20, Epoch 994, Loss: 0.4930514916777611, Final Batch Loss: 0.15541833639144897\n",
      "Subject 20, Epoch 995, Loss: 0.44581810757517815, Final Batch Loss: 0.13414235413074493\n",
      "Subject 20, Epoch 996, Loss: 0.3889625258743763, Final Batch Loss: 0.05775212123990059\n",
      "Subject 20, Epoch 997, Loss: 0.35796472430229187, Final Batch Loss: 0.08276589214801788\n",
      "Subject 20, Epoch 998, Loss: 0.3976898118853569, Final Batch Loss: 0.11295977234840393\n",
      "Subject 20, Epoch 999, Loss: 0.4994370937347412, Final Batch Loss: 0.2520483136177063\n",
      "Subject 20, Epoch 1000, Loss: 0.3381071612238884, Final Batch Loss: 0.03946763649582863\n"
     ]
    }
   ],
   "source": [
    "all_scores = np.zeros((n_subjects, n_iters + 1))\n",
    "\n",
    "for k in range(n_subjects):\n",
    "    #Get train/test data of the current user\n",
    "    X_train, y_train, X_test, y_test = get_train_test_split(GAN_data, subject_numbers[k])\n",
    "    #Get untrained model, data loaders of current user\n",
    "    model, train_loader, test_loader, optimizer, criterion = initialize_params(X_train, y_train, X_test, y_test)\n",
    "    #Get trained model on the current user\n",
    "    model, test_loader = training_loop(model, train_loader, test_loader, optimizer, criterion, k)\n",
    "    for j in range(n_iters):\n",
    "        #Evaluate model on current user\n",
    "        f1_score = evaluation(model, test_loader)\n",
    "        all_scores[k, j] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.84331756,  0.86647924,  0.84280248, ...,  0.8266438 ,\n",
       "         0.88374882,  1.        ],\n",
       "       [ 0.94103282,  0.90357936,  0.92115174, ...,  0.92005495,\n",
       "         0.96120973,  3.        ],\n",
       "       [ 0.78065481,  0.86352657,  0.80165631, ...,  0.81401602,\n",
       "         0.83441438,  5.        ],\n",
       "       ...,\n",
       "       [ 0.89581819,  0.86434783,  0.82963518, ...,  0.88013136,\n",
       "         0.83378132, 28.        ],\n",
       "       [ 0.88596692,  0.88570804,  0.92451923, ...,  0.82289544,\n",
       "         0.86727624, 29.        ],\n",
       "       [ 0.89675698,  0.9307266 ,  0.89865739, ...,  0.89657909,\n",
       "         0.89663572, 30.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(n_subjects):\n",
    "    all_scores[k, -1] = subject_numbers[k]\n",
    "    \n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 'Subject Number']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = list(np.arange(0, n_iters, 1)) + ['Subject Number']\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results saved to Single User F-1 Scores.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"../model_outputs/Single User F-1 Scores.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    csvwriter.writerow(heading)\n",
    "    csvwriter.writerows(all_scores)\n",
    "\n",
    "print(\"Model results saved to Single User F-1 Scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
