{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '125 tBodyGyro-std()-Y',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '138 tBodyGyro-energy()-Y',\n",
    " '165 tBodyGyroJerk-std()-Y',\n",
    " '168 tBodyGyroJerk-mad()-Y',\n",
    " '178 tBodyGyroJerk-energy()-Y',\n",
    " '181 tBodyGyroJerk-iqr()-Y',\n",
    " '425 fBodyGyro-mean()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '431 fBodyGyro-mad()-Y',\n",
    " '441 fBodyGyro-energy()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '478 fBodyGyro-bandsEnergy()-25,32',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '204 tBodyAccMag-max()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '217 tGravityAccMag-max()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '275 fBodyAcc-max()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '506 fBodyAccMag-max()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 9)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "\n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(35)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(35)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(35) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(35) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(35) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(35) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(35) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(35) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN_gen.param\")\n",
    "latent_vectors = get_noise(35, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(35) + 7\n",
    "\n",
    "X_train = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9))\n",
    "y_train = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5]\n",
    "X_test, y_test = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "_, X_test, _, y_test = train_test_split(X_test, y_test, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.4255921840667725, Final Batch Loss: 2.2105813026428223\n",
      "Epoch 2, Loss: 4.429545640945435, Final Batch Loss: 2.2171683311462402\n",
      "Epoch 3, Loss: 4.417402267456055, Final Batch Loss: 2.2021896839141846\n",
      "Epoch 4, Loss: 4.417039394378662, Final Batch Loss: 2.2066190242767334\n",
      "Epoch 5, Loss: 4.378518104553223, Final Batch Loss: 2.1561367511749268\n",
      "Epoch 6, Loss: 4.3835296630859375, Final Batch Loss: 2.164398431777954\n",
      "Epoch 7, Loss: 4.4138922691345215, Final Batch Loss: 2.205383062362671\n",
      "Epoch 8, Loss: 4.407275676727295, Final Batch Loss: 2.1974239349365234\n",
      "Epoch 9, Loss: 4.426706790924072, Final Batch Loss: 2.2281370162963867\n",
      "Epoch 10, Loss: 4.435963869094849, Final Batch Loss: 2.242360830307007\n",
      "Epoch 11, Loss: 4.393233060836792, Final Batch Loss: 2.185807943344116\n",
      "Epoch 12, Loss: 4.405672073364258, Final Batch Loss: 2.2059266567230225\n",
      "Epoch 13, Loss: 4.412482738494873, Final Batch Loss: 2.2161779403686523\n",
      "Epoch 14, Loss: 4.378667116165161, Final Batch Loss: 2.17116379737854\n",
      "Epoch 15, Loss: 4.4111247062683105, Final Batch Loss: 2.218095302581787\n",
      "Epoch 16, Loss: 4.382704973220825, Final Batch Loss: 2.1841354370117188\n",
      "Epoch 17, Loss: 4.3753345012664795, Final Batch Loss: 2.178638458251953\n",
      "Epoch 18, Loss: 4.375577688217163, Final Batch Loss: 2.1845788955688477\n",
      "Epoch 19, Loss: 4.35038423538208, Final Batch Loss: 2.1526570320129395\n",
      "Epoch 20, Loss: 4.360867023468018, Final Batch Loss: 2.176058053970337\n",
      "Epoch 21, Loss: 4.367330074310303, Final Batch Loss: 2.1903750896453857\n",
      "Epoch 22, Loss: 4.367509126663208, Final Batch Loss: 2.201425790786743\n",
      "Epoch 23, Loss: 4.33701753616333, Final Batch Loss: 2.171811819076538\n",
      "Epoch 24, Loss: 4.295751094818115, Final Batch Loss: 2.1295645236968994\n",
      "Epoch 25, Loss: 4.286592483520508, Final Batch Loss: 2.1273350715637207\n",
      "Epoch 26, Loss: 4.28366494178772, Final Batch Loss: 2.15102481842041\n",
      "Epoch 27, Loss: 4.261948585510254, Final Batch Loss: 2.133496046066284\n",
      "Epoch 28, Loss: 4.253088474273682, Final Batch Loss: 2.142808675765991\n",
      "Epoch 29, Loss: 4.189023494720459, Final Batch Loss: 2.0835773944854736\n",
      "Epoch 30, Loss: 4.191275596618652, Final Batch Loss: 2.101323127746582\n",
      "Epoch 31, Loss: 4.125903129577637, Final Batch Loss: 2.03739333152771\n",
      "Epoch 32, Loss: 4.106525659561157, Final Batch Loss: 2.0471086502075195\n",
      "Epoch 33, Loss: 4.051759719848633, Final Batch Loss: 2.0263173580169678\n",
      "Epoch 34, Loss: 4.091519832611084, Final Batch Loss: 2.0762765407562256\n",
      "Epoch 35, Loss: 3.9438291788101196, Final Batch Loss: 1.9386955499649048\n",
      "Epoch 36, Loss: 3.970973253250122, Final Batch Loss: 2.002955436706543\n",
      "Epoch 37, Loss: 3.882236123085022, Final Batch Loss: 1.9235769510269165\n",
      "Epoch 38, Loss: 3.8235827684402466, Final Batch Loss: 1.899808645248413\n",
      "Epoch 39, Loss: 3.7277138233184814, Final Batch Loss: 1.7949931621551514\n",
      "Epoch 40, Loss: 3.7364317178726196, Final Batch Loss: 1.835222601890564\n",
      "Epoch 41, Loss: 3.751388192176819, Final Batch Loss: 1.8986661434173584\n",
      "Epoch 42, Loss: 3.7085845470428467, Final Batch Loss: 1.8654046058654785\n",
      "Epoch 43, Loss: 3.699106812477112, Final Batch Loss: 1.8997102975845337\n",
      "Epoch 44, Loss: 3.5729135274887085, Final Batch Loss: 1.725292444229126\n",
      "Epoch 45, Loss: 3.5832096338272095, Final Batch Loss: 1.7715809345245361\n",
      "Epoch 46, Loss: 3.601957678794861, Final Batch Loss: 1.8414915800094604\n",
      "Epoch 47, Loss: 3.44306218624115, Final Batch Loss: 1.6819275617599487\n",
      "Epoch 48, Loss: 3.544684648513794, Final Batch Loss: 1.841323733329773\n",
      "Epoch 49, Loss: 3.4840054512023926, Final Batch Loss: 1.7667522430419922\n",
      "Epoch 50, Loss: 3.389228343963623, Final Batch Loss: 1.695181131362915\n",
      "Epoch 51, Loss: 3.3364367485046387, Final Batch Loss: 1.6348837614059448\n",
      "Epoch 52, Loss: 3.278943181037903, Final Batch Loss: 1.6332883834838867\n",
      "Epoch 53, Loss: 3.3311017751693726, Final Batch Loss: 1.7285593748092651\n",
      "Epoch 54, Loss: 3.3036890029907227, Final Batch Loss: 1.7381645441055298\n",
      "Epoch 55, Loss: 3.1683502197265625, Final Batch Loss: 1.5571773052215576\n",
      "Epoch 56, Loss: 3.101484775543213, Final Batch Loss: 1.5365896224975586\n",
      "Epoch 57, Loss: 3.074455738067627, Final Batch Loss: 1.5246813297271729\n",
      "Epoch 58, Loss: 3.0120363235473633, Final Batch Loss: 1.446491003036499\n",
      "Epoch 59, Loss: 2.9721858501434326, Final Batch Loss: 1.4790196418762207\n",
      "Epoch 60, Loss: 2.983527183532715, Final Batch Loss: 1.494460940361023\n",
      "Epoch 61, Loss: 2.8699162006378174, Final Batch Loss: 1.3955554962158203\n",
      "Epoch 62, Loss: 2.9622656106948853, Final Batch Loss: 1.5028069019317627\n",
      "Epoch 63, Loss: 2.8911107778549194, Final Batch Loss: 1.4399728775024414\n",
      "Epoch 64, Loss: 2.938167929649353, Final Batch Loss: 1.545883059501648\n",
      "Epoch 65, Loss: 2.7697808742523193, Final Batch Loss: 1.406401515007019\n",
      "Epoch 66, Loss: 2.715092897415161, Final Batch Loss: 1.337558388710022\n",
      "Epoch 67, Loss: 2.7557133436203003, Final Batch Loss: 1.3981419801712036\n",
      "Epoch 68, Loss: 2.7359533309936523, Final Batch Loss: 1.3759219646453857\n",
      "Epoch 69, Loss: 2.6280696392059326, Final Batch Loss: 1.2970002889633179\n",
      "Epoch 70, Loss: 2.5896246433258057, Final Batch Loss: 1.2517215013504028\n",
      "Epoch 71, Loss: 2.658631443977356, Final Batch Loss: 1.3621068000793457\n",
      "Epoch 72, Loss: 2.6460639238357544, Final Batch Loss: 1.3066670894622803\n",
      "Epoch 73, Loss: 2.718924403190613, Final Batch Loss: 1.4047424793243408\n",
      "Epoch 74, Loss: 2.5513588190078735, Final Batch Loss: 1.2286388874053955\n",
      "Epoch 75, Loss: 2.5135809183120728, Final Batch Loss: 1.2907178401947021\n",
      "Epoch 76, Loss: 2.5963752269744873, Final Batch Loss: 1.2942456007003784\n",
      "Epoch 77, Loss: 2.5534892082214355, Final Batch Loss: 1.2799333333969116\n",
      "Epoch 78, Loss: 2.6071397066116333, Final Batch Loss: 1.3167732954025269\n",
      "Epoch 79, Loss: 2.5795209407806396, Final Batch Loss: 1.294105052947998\n",
      "Epoch 80, Loss: 2.501762270927429, Final Batch Loss: 1.2253834009170532\n",
      "Epoch 81, Loss: 2.4651352167129517, Final Batch Loss: 1.2048696279525757\n",
      "Epoch 82, Loss: 2.4901838302612305, Final Batch Loss: 1.2837202548980713\n",
      "Epoch 83, Loss: 2.4233261346817017, Final Batch Loss: 1.232947587966919\n",
      "Epoch 84, Loss: 2.4512418508529663, Final Batch Loss: 1.2481110095977783\n",
      "Epoch 85, Loss: 2.4696731567382812, Final Batch Loss: 1.2362364530563354\n",
      "Epoch 86, Loss: 2.4210904836654663, Final Batch Loss: 1.2242454290390015\n",
      "Epoch 87, Loss: 2.3725098371505737, Final Batch Loss: 1.1518385410308838\n",
      "Epoch 88, Loss: 2.4289324283599854, Final Batch Loss: 1.2067888975143433\n",
      "Epoch 89, Loss: 2.3887999057769775, Final Batch Loss: 1.1900051832199097\n",
      "Epoch 90, Loss: 2.372398018836975, Final Batch Loss: 1.154586672782898\n",
      "Epoch 91, Loss: 2.4353058338165283, Final Batch Loss: 1.2491470575332642\n",
      "Epoch 92, Loss: 2.3305269479751587, Final Batch Loss: 1.1017181873321533\n",
      "Epoch 93, Loss: 2.422014594078064, Final Batch Loss: 1.2279430627822876\n",
      "Epoch 94, Loss: 2.428004741668701, Final Batch Loss: 1.2227129936218262\n",
      "Epoch 95, Loss: 2.4494770765304565, Final Batch Loss: 1.2676608562469482\n",
      "Epoch 96, Loss: 2.378490924835205, Final Batch Loss: 1.1342113018035889\n",
      "Epoch 97, Loss: 2.311946153640747, Final Batch Loss: 1.1702649593353271\n",
      "Epoch 98, Loss: 2.2122305631637573, Final Batch Loss: 1.1084506511688232\n",
      "Epoch 99, Loss: 2.3440388441085815, Final Batch Loss: 1.146179437637329\n",
      "Epoch 100, Loss: 2.4759249687194824, Final Batch Loss: 1.2532833814620972\n",
      "Epoch 101, Loss: 2.3429548740386963, Final Batch Loss: 1.1717383861541748\n",
      "Epoch 102, Loss: 2.285931944847107, Final Batch Loss: 1.1470214128494263\n",
      "Epoch 103, Loss: 2.2197009325027466, Final Batch Loss: 1.0591468811035156\n",
      "Epoch 104, Loss: 2.262123227119446, Final Batch Loss: 1.1165920495986938\n",
      "Epoch 105, Loss: 2.294135332107544, Final Batch Loss: 1.110994815826416\n",
      "Epoch 106, Loss: 2.306430697441101, Final Batch Loss: 1.1331496238708496\n",
      "Epoch 107, Loss: 2.3053888082504272, Final Batch Loss: 1.1203508377075195\n",
      "Epoch 108, Loss: 2.319968342781067, Final Batch Loss: 1.189943790435791\n",
      "Epoch 109, Loss: 2.3700467348098755, Final Batch Loss: 1.2644121646881104\n",
      "Epoch 110, Loss: 2.3177582025527954, Final Batch Loss: 1.1907190084457397\n",
      "Epoch 111, Loss: 2.257576107978821, Final Batch Loss: 1.114654779434204\n",
      "Epoch 112, Loss: 2.3310614824295044, Final Batch Loss: 1.2067928314208984\n",
      "Epoch 113, Loss: 2.252368450164795, Final Batch Loss: 1.1227670907974243\n",
      "Epoch 114, Loss: 2.209386467933655, Final Batch Loss: 1.0872857570648193\n",
      "Epoch 115, Loss: 2.129445195198059, Final Batch Loss: 1.0109081268310547\n",
      "Epoch 116, Loss: 2.3081905841827393, Final Batch Loss: 1.1892378330230713\n",
      "Epoch 117, Loss: 2.1019805669784546, Final Batch Loss: 1.0168708562850952\n",
      "Epoch 118, Loss: 2.0949578881263733, Final Batch Loss: 0.9916895031929016\n",
      "Epoch 119, Loss: 2.30170738697052, Final Batch Loss: 1.2120739221572876\n",
      "Epoch 120, Loss: 2.262346863746643, Final Batch Loss: 1.194005012512207\n",
      "Epoch 121, Loss: 2.1102805137634277, Final Batch Loss: 1.0519956350326538\n",
      "Epoch 122, Loss: 2.199002504348755, Final Batch Loss: 1.0955694913864136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123, Loss: 2.207185387611389, Final Batch Loss: 1.1145744323730469\n",
      "Epoch 124, Loss: 2.1200754642486572, Final Batch Loss: 1.0508846044540405\n",
      "Epoch 125, Loss: 2.224388837814331, Final Batch Loss: 1.1398764848709106\n",
      "Epoch 126, Loss: 2.1321444511413574, Final Batch Loss: 1.027369499206543\n",
      "Epoch 127, Loss: 2.199636697769165, Final Batch Loss: 1.071033000946045\n",
      "Epoch 128, Loss: 2.259600281715393, Final Batch Loss: 1.183103322982788\n",
      "Epoch 129, Loss: 2.1562068462371826, Final Batch Loss: 1.0377159118652344\n",
      "Epoch 130, Loss: 2.101579785346985, Final Batch Loss: 1.065350890159607\n",
      "Epoch 131, Loss: 2.298153519630432, Final Batch Loss: 1.2246284484863281\n",
      "Epoch 132, Loss: 2.0777549743652344, Final Batch Loss: 1.0589368343353271\n",
      "Epoch 133, Loss: 2.308066964149475, Final Batch Loss: 1.2149765491485596\n",
      "Epoch 134, Loss: 2.009783446788788, Final Batch Loss: 0.9373393654823303\n",
      "Epoch 135, Loss: 2.21931791305542, Final Batch Loss: 1.1587544679641724\n",
      "Epoch 136, Loss: 2.075596332550049, Final Batch Loss: 1.0054987668991089\n",
      "Epoch 137, Loss: 2.0450063943862915, Final Batch Loss: 0.9823554754257202\n",
      "Epoch 138, Loss: 2.1140488386154175, Final Batch Loss: 1.043332576751709\n",
      "Epoch 139, Loss: 2.188580274581909, Final Batch Loss: 1.1637799739837646\n",
      "Epoch 140, Loss: 2.0104504227638245, Final Batch Loss: 0.9512383341789246\n",
      "Epoch 141, Loss: 2.1074737310409546, Final Batch Loss: 1.1069543361663818\n",
      "Epoch 142, Loss: 2.060468912124634, Final Batch Loss: 1.028841257095337\n",
      "Epoch 143, Loss: 1.9971361756324768, Final Batch Loss: 0.9674645066261292\n",
      "Epoch 144, Loss: 1.9722567796707153, Final Batch Loss: 0.9758702516555786\n",
      "Epoch 145, Loss: 2.04812091588974, Final Batch Loss: 1.076712965965271\n",
      "Epoch 146, Loss: 2.2676842212677, Final Batch Loss: 1.1784576177597046\n",
      "Epoch 147, Loss: 2.0945767164230347, Final Batch Loss: 1.022297739982605\n",
      "Epoch 148, Loss: 2.0009217858314514, Final Batch Loss: 0.9682970643043518\n",
      "Epoch 149, Loss: 1.978054404258728, Final Batch Loss: 0.9462392330169678\n",
      "Epoch 150, Loss: 2.062849760055542, Final Batch Loss: 1.0508754253387451\n",
      "Epoch 151, Loss: 2.0857402086257935, Final Batch Loss: 1.0164085626602173\n",
      "Epoch 152, Loss: 2.0823029279708862, Final Batch Loss: 1.0600711107254028\n",
      "Epoch 153, Loss: 1.948019802570343, Final Batch Loss: 0.9534443616867065\n",
      "Epoch 154, Loss: 1.9717335104942322, Final Batch Loss: 0.9272183775901794\n",
      "Epoch 155, Loss: 1.9595282673835754, Final Batch Loss: 0.9464215636253357\n",
      "Epoch 156, Loss: 2.0859848260879517, Final Batch Loss: 1.0521104335784912\n",
      "Epoch 157, Loss: 2.0851356983184814, Final Batch Loss: 1.0642319917678833\n",
      "Epoch 158, Loss: 1.933771550655365, Final Batch Loss: 0.9409966468811035\n",
      "Epoch 159, Loss: 2.0411277413368225, Final Batch Loss: 0.9883443713188171\n",
      "Epoch 160, Loss: 2.088199019432068, Final Batch Loss: 1.0573692321777344\n",
      "Epoch 161, Loss: 2.13519823551178, Final Batch Loss: 1.0742096900939941\n",
      "Epoch 162, Loss: 1.9576488137245178, Final Batch Loss: 0.9117804169654846\n",
      "Epoch 163, Loss: 1.9125239253044128, Final Batch Loss: 0.9318117499351501\n",
      "Epoch 164, Loss: 1.8512459993362427, Final Batch Loss: 0.8230692148208618\n",
      "Epoch 165, Loss: 1.9225728511810303, Final Batch Loss: 0.9519444108009338\n",
      "Epoch 166, Loss: 1.9279034733772278, Final Batch Loss: 0.9641611576080322\n",
      "Epoch 167, Loss: 2.1432644724845886, Final Batch Loss: 1.171721339225769\n",
      "Epoch 168, Loss: 1.8745813965797424, Final Batch Loss: 0.9482854604721069\n",
      "Epoch 169, Loss: 1.9619690775871277, Final Batch Loss: 0.9943400621414185\n",
      "Epoch 170, Loss: 2.032916784286499, Final Batch Loss: 1.0305824279785156\n",
      "Epoch 171, Loss: 2.0078845024108887, Final Batch Loss: 0.979912281036377\n",
      "Epoch 172, Loss: 1.82107812166214, Final Batch Loss: 0.8769538402557373\n",
      "Epoch 173, Loss: 1.9084596037864685, Final Batch Loss: 0.9429425597190857\n",
      "Epoch 174, Loss: 2.1014974117279053, Final Batch Loss: 1.0891437530517578\n",
      "Epoch 175, Loss: 1.9206328988075256, Final Batch Loss: 0.9690796732902527\n",
      "Epoch 176, Loss: 1.9156088829040527, Final Batch Loss: 0.9619569182395935\n",
      "Epoch 177, Loss: 1.921860933303833, Final Batch Loss: 1.007243037223816\n",
      "Epoch 178, Loss: 2.0591588020324707, Final Batch Loss: 1.0340176820755005\n",
      "Epoch 179, Loss: 2.013341009616852, Final Batch Loss: 1.029927134513855\n",
      "Epoch 180, Loss: 1.9371646046638489, Final Batch Loss: 0.9555810689926147\n",
      "Epoch 181, Loss: 1.9623230695724487, Final Batch Loss: 1.007637619972229\n",
      "Epoch 182, Loss: 1.9871439933776855, Final Batch Loss: 0.994369387626648\n",
      "Epoch 183, Loss: 1.9024633765220642, Final Batch Loss: 0.8929083943367004\n",
      "Epoch 184, Loss: 1.8851744532585144, Final Batch Loss: 0.8863074779510498\n",
      "Epoch 185, Loss: 1.9252288341522217, Final Batch Loss: 0.9771487712860107\n",
      "Epoch 186, Loss: 1.933611273765564, Final Batch Loss: 0.9820341467857361\n",
      "Epoch 187, Loss: 1.9076343774795532, Final Batch Loss: 0.9532353281974792\n",
      "Epoch 188, Loss: 1.9909308552742004, Final Batch Loss: 1.0078567266464233\n",
      "Epoch 189, Loss: 1.9067359566688538, Final Batch Loss: 0.9468088150024414\n",
      "Epoch 190, Loss: 1.8852444291114807, Final Batch Loss: 0.9363406896591187\n",
      "Epoch 191, Loss: 1.8499462604522705, Final Batch Loss: 0.9076237082481384\n",
      "Epoch 192, Loss: 1.9624925255775452, Final Batch Loss: 1.0491036176681519\n",
      "Epoch 193, Loss: 1.9240537285804749, Final Batch Loss: 0.9661952257156372\n",
      "Epoch 194, Loss: 1.8969810009002686, Final Batch Loss: 0.9329832196235657\n",
      "Epoch 195, Loss: 2.01462185382843, Final Batch Loss: 1.033681869506836\n",
      "Epoch 196, Loss: 1.8998318314552307, Final Batch Loss: 0.8898867964744568\n",
      "Epoch 197, Loss: 2.0610592365264893, Final Batch Loss: 1.1303284168243408\n",
      "Epoch 198, Loss: 1.8695017099380493, Final Batch Loss: 0.9391636848449707\n",
      "Epoch 199, Loss: 1.9285831451416016, Final Batch Loss: 1.0114420652389526\n",
      "Epoch 200, Loss: 2.0955825448036194, Final Batch Loss: 1.1438485383987427\n",
      "Epoch 201, Loss: 1.9414101243019104, Final Batch Loss: 0.9815645217895508\n",
      "Epoch 202, Loss: 1.9928542971611023, Final Batch Loss: 1.0557055473327637\n",
      "Epoch 203, Loss: 1.8555691242218018, Final Batch Loss: 0.8922541737556458\n",
      "Epoch 204, Loss: 1.9437013864517212, Final Batch Loss: 0.9705143570899963\n",
      "Epoch 205, Loss: 1.827712059020996, Final Batch Loss: 0.8685410618782043\n",
      "Epoch 206, Loss: 1.8645713329315186, Final Batch Loss: 0.964274525642395\n",
      "Epoch 207, Loss: 2.0710191130638123, Final Batch Loss: 1.1219983100891113\n",
      "Epoch 208, Loss: 1.9290863871574402, Final Batch Loss: 0.9888532161712646\n",
      "Epoch 209, Loss: 1.8117130994796753, Final Batch Loss: 0.8585673570632935\n",
      "Epoch 210, Loss: 1.972040832042694, Final Batch Loss: 1.0212525129318237\n",
      "Epoch 211, Loss: 1.7472876906394958, Final Batch Loss: 0.7993324398994446\n",
      "Epoch 212, Loss: 1.872498869895935, Final Batch Loss: 0.9298542141914368\n",
      "Epoch 213, Loss: 1.8391985297203064, Final Batch Loss: 0.9173187613487244\n",
      "Epoch 214, Loss: 1.9123564958572388, Final Batch Loss: 0.9902530312538147\n",
      "Epoch 215, Loss: 1.9536197185516357, Final Batch Loss: 1.003444790840149\n",
      "Epoch 216, Loss: 1.874595582485199, Final Batch Loss: 0.9445847272872925\n",
      "Epoch 217, Loss: 1.913870096206665, Final Batch Loss: 1.0060938596725464\n",
      "Epoch 218, Loss: 1.9771575927734375, Final Batch Loss: 1.0418589115142822\n",
      "Epoch 219, Loss: 1.8065369129180908, Final Batch Loss: 0.8834063410758972\n",
      "Epoch 220, Loss: 1.821371853351593, Final Batch Loss: 0.9009281396865845\n",
      "Epoch 221, Loss: 1.9288058280944824, Final Batch Loss: 1.0203731060028076\n",
      "Epoch 222, Loss: 1.8241044282913208, Final Batch Loss: 0.8930009007453918\n",
      "Epoch 223, Loss: 1.97362619638443, Final Batch Loss: 1.0031548738479614\n",
      "Epoch 224, Loss: 1.9882872700691223, Final Batch Loss: 1.1049084663391113\n",
      "Epoch 225, Loss: 1.7626498937606812, Final Batch Loss: 0.83175128698349\n",
      "Epoch 226, Loss: 1.8741592764854431, Final Batch Loss: 0.9727889895439148\n",
      "Epoch 227, Loss: 1.978665828704834, Final Batch Loss: 1.0431442260742188\n",
      "Epoch 228, Loss: 1.7691736817359924, Final Batch Loss: 0.8822203874588013\n",
      "Epoch 229, Loss: 1.94237619638443, Final Batch Loss: 1.0648657083511353\n",
      "Epoch 230, Loss: 1.706855833530426, Final Batch Loss: 0.8317615985870361\n",
      "Epoch 231, Loss: 1.7888854146003723, Final Batch Loss: 0.8547906875610352\n",
      "Epoch 232, Loss: 1.8400510549545288, Final Batch Loss: 0.9593420624732971\n",
      "Epoch 233, Loss: 1.9087427258491516, Final Batch Loss: 1.0324326753616333\n",
      "Epoch 234, Loss: 1.9555931687355042, Final Batch Loss: 1.07033371925354\n",
      "Epoch 235, Loss: 1.73594069480896, Final Batch Loss: 0.8385477066040039\n",
      "Epoch 236, Loss: 1.9474827647209167, Final Batch Loss: 1.004518747329712\n",
      "Epoch 237, Loss: 1.8173826336860657, Final Batch Loss: 0.9214158654212952\n",
      "Epoch 238, Loss: 1.8393032550811768, Final Batch Loss: 0.8390190601348877\n",
      "Epoch 239, Loss: 1.8545798659324646, Final Batch Loss: 0.9734451770782471\n",
      "Epoch 240, Loss: 1.8178097605705261, Final Batch Loss: 0.8921631574630737\n",
      "Epoch 241, Loss: 1.8376182913780212, Final Batch Loss: 0.9608144164085388\n",
      "Epoch 242, Loss: 1.7740347981452942, Final Batch Loss: 0.8790373802185059\n",
      "Epoch 243, Loss: 1.702703833580017, Final Batch Loss: 0.8064236640930176\n",
      "Epoch 244, Loss: 1.8385931253433228, Final Batch Loss: 0.9737565517425537\n",
      "Epoch 245, Loss: 1.8121772408485413, Final Batch Loss: 0.9184477925300598\n",
      "Epoch 246, Loss: 1.74910968542099, Final Batch Loss: 0.8126459121704102\n",
      "Epoch 247, Loss: 1.6014873385429382, Final Batch Loss: 0.7616780400276184\n",
      "Epoch 248, Loss: 1.8100177645683289, Final Batch Loss: 0.9045504927635193\n",
      "Epoch 249, Loss: 1.912663996219635, Final Batch Loss: 1.0387241840362549\n",
      "Epoch 250, Loss: 1.6917969584465027, Final Batch Loss: 0.8078165054321289\n",
      "Epoch 251, Loss: 2.0018247961997986, Final Batch Loss: 1.1079870462417603\n",
      "Epoch 252, Loss: 1.916877269744873, Final Batch Loss: 0.9718889594078064\n",
      "Epoch 253, Loss: 1.7356042265892029, Final Batch Loss: 0.8362666368484497\n",
      "Epoch 254, Loss: 1.7375845313072205, Final Batch Loss: 0.8899206519126892\n",
      "Epoch 255, Loss: 1.8395419120788574, Final Batch Loss: 0.986590564250946\n",
      "Epoch 256, Loss: 1.7169755697250366, Final Batch Loss: 0.821434736251831\n",
      "Epoch 257, Loss: 1.8162389397621155, Final Batch Loss: 0.9500524997711182\n",
      "Epoch 258, Loss: 1.7094328999519348, Final Batch Loss: 0.7639618515968323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259, Loss: 1.7952112555503845, Final Batch Loss: 0.92955082654953\n",
      "Epoch 260, Loss: 1.8047552108764648, Final Batch Loss: 0.9139426946640015\n",
      "Epoch 261, Loss: 1.7471272945404053, Final Batch Loss: 0.8749401569366455\n",
      "Epoch 262, Loss: 1.7031832337379456, Final Batch Loss: 0.7729291915893555\n",
      "Epoch 263, Loss: 1.7932570576667786, Final Batch Loss: 0.9264013171195984\n",
      "Epoch 264, Loss: 1.730264961719513, Final Batch Loss: 0.8632463812828064\n",
      "Epoch 265, Loss: 1.8900806903839111, Final Batch Loss: 0.979037344455719\n",
      "Epoch 266, Loss: 1.81007319688797, Final Batch Loss: 0.9451936483383179\n",
      "Epoch 267, Loss: 1.7333170771598816, Final Batch Loss: 0.8756592869758606\n",
      "Epoch 268, Loss: 1.872378647327423, Final Batch Loss: 0.9894999861717224\n",
      "Epoch 269, Loss: 1.6645959615707397, Final Batch Loss: 0.755240261554718\n",
      "Epoch 270, Loss: 1.6754107475280762, Final Batch Loss: 0.864798903465271\n",
      "Epoch 271, Loss: 1.775054395198822, Final Batch Loss: 0.9081486463546753\n",
      "Epoch 272, Loss: 1.774889349937439, Final Batch Loss: 0.9639930129051208\n",
      "Epoch 273, Loss: 1.6635521054267883, Final Batch Loss: 0.7785989046096802\n",
      "Epoch 274, Loss: 1.8628958463668823, Final Batch Loss: 0.9561343789100647\n",
      "Epoch 275, Loss: 1.7666776180267334, Final Batch Loss: 0.8960415720939636\n",
      "Epoch 276, Loss: 1.7319691181182861, Final Batch Loss: 0.8177053928375244\n",
      "Epoch 277, Loss: 1.7522574663162231, Final Batch Loss: 0.8847129344940186\n",
      "Epoch 278, Loss: 1.7808945775032043, Final Batch Loss: 0.9324446320533752\n",
      "Epoch 279, Loss: 1.756256878376007, Final Batch Loss: 0.9151334166526794\n",
      "Epoch 280, Loss: 1.7322429418563843, Final Batch Loss: 0.873534619808197\n",
      "Epoch 281, Loss: 1.9021116495132446, Final Batch Loss: 1.0234906673431396\n",
      "Epoch 282, Loss: 1.74147367477417, Final Batch Loss: 0.8856805562973022\n",
      "Epoch 283, Loss: 1.713822364807129, Final Batch Loss: 0.8604088425636292\n",
      "Epoch 284, Loss: 1.6787989735603333, Final Batch Loss: 0.7994060516357422\n",
      "Epoch 285, Loss: 1.692542850971222, Final Batch Loss: 0.8030111789703369\n",
      "Epoch 286, Loss: 1.7309722304344177, Final Batch Loss: 0.8489261865615845\n",
      "Epoch 287, Loss: 1.8108434677124023, Final Batch Loss: 0.91130530834198\n",
      "Epoch 288, Loss: 1.5556671619415283, Final Batch Loss: 0.7192102670669556\n",
      "Epoch 289, Loss: 1.6583882570266724, Final Batch Loss: 0.8442875742912292\n",
      "Epoch 290, Loss: 1.5692315101623535, Final Batch Loss: 0.7731454372406006\n",
      "Epoch 291, Loss: 1.771081030368805, Final Batch Loss: 0.8970241546630859\n",
      "Epoch 292, Loss: 1.7704904675483704, Final Batch Loss: 0.8587096333503723\n",
      "Epoch 293, Loss: 1.6716601252555847, Final Batch Loss: 0.8312720656394958\n",
      "Epoch 294, Loss: 1.6440507173538208, Final Batch Loss: 0.7711753249168396\n",
      "Epoch 295, Loss: 1.7955889105796814, Final Batch Loss: 0.9597781300544739\n",
      "Epoch 296, Loss: 1.693250298500061, Final Batch Loss: 0.8459959626197815\n",
      "Epoch 297, Loss: 1.7464909553527832, Final Batch Loss: 0.8578895926475525\n",
      "Epoch 298, Loss: 1.6167148351669312, Final Batch Loss: 0.7693476676940918\n",
      "Epoch 299, Loss: 1.6570561528205872, Final Batch Loss: 0.8314539790153503\n",
      "Epoch 300, Loss: 1.66721773147583, Final Batch Loss: 0.8291715979576111\n",
      "Epoch 301, Loss: 1.5681124925613403, Final Batch Loss: 0.7293615341186523\n",
      "Epoch 302, Loss: 1.603188931941986, Final Batch Loss: 0.7727802991867065\n",
      "Epoch 303, Loss: 1.6328760385513306, Final Batch Loss: 0.7832280993461609\n",
      "Epoch 304, Loss: 1.607973873615265, Final Batch Loss: 0.7393474578857422\n",
      "Epoch 305, Loss: 1.6676323413848877, Final Batch Loss: 0.8214516043663025\n",
      "Epoch 306, Loss: 1.630664885044098, Final Batch Loss: 0.7634552717208862\n",
      "Epoch 307, Loss: 1.7044116258621216, Final Batch Loss: 0.9146278500556946\n",
      "Epoch 308, Loss: 1.6606262922286987, Final Batch Loss: 0.8297107219696045\n",
      "Epoch 309, Loss: 1.6896678805351257, Final Batch Loss: 0.9000696539878845\n",
      "Epoch 310, Loss: 1.6585227251052856, Final Batch Loss: 0.8299489617347717\n",
      "Epoch 311, Loss: 1.7105961441993713, Final Batch Loss: 0.8974127769470215\n",
      "Epoch 312, Loss: 1.5957707166671753, Final Batch Loss: 0.7747905254364014\n",
      "Epoch 313, Loss: 1.6767485737800598, Final Batch Loss: 0.8318972587585449\n",
      "Epoch 314, Loss: 1.5670559406280518, Final Batch Loss: 0.702267587184906\n",
      "Epoch 315, Loss: 1.614729881286621, Final Batch Loss: 0.7898184061050415\n",
      "Epoch 316, Loss: 1.6583169102668762, Final Batch Loss: 0.8409587144851685\n",
      "Epoch 317, Loss: 1.5140305757522583, Final Batch Loss: 0.7363268733024597\n",
      "Epoch 318, Loss: 1.5978025197982788, Final Batch Loss: 0.7904381155967712\n",
      "Epoch 319, Loss: 1.6600930094718933, Final Batch Loss: 0.747836172580719\n",
      "Epoch 320, Loss: 1.5450827479362488, Final Batch Loss: 0.7107023596763611\n",
      "Epoch 321, Loss: 1.5696310997009277, Final Batch Loss: 0.7177622318267822\n",
      "Epoch 322, Loss: 1.7269023656845093, Final Batch Loss: 0.9374690055847168\n",
      "Epoch 323, Loss: 1.5941005945205688, Final Batch Loss: 0.7945355772972107\n",
      "Epoch 324, Loss: 1.7990195751190186, Final Batch Loss: 0.9051318764686584\n",
      "Epoch 325, Loss: 1.6547695398330688, Final Batch Loss: 0.807374119758606\n",
      "Epoch 326, Loss: 1.752674400806427, Final Batch Loss: 0.934374988079071\n",
      "Epoch 327, Loss: 1.660239040851593, Final Batch Loss: 0.7994275093078613\n",
      "Epoch 328, Loss: 1.543639898300171, Final Batch Loss: 0.7078877091407776\n",
      "Epoch 329, Loss: 1.698538362979889, Final Batch Loss: 0.8671138286590576\n",
      "Epoch 330, Loss: 1.600595235824585, Final Batch Loss: 0.8305624127388\n",
      "Epoch 331, Loss: 1.6099199652671814, Final Batch Loss: 0.7835984230041504\n",
      "Epoch 332, Loss: 1.640869677066803, Final Batch Loss: 0.8698626160621643\n",
      "Epoch 333, Loss: 1.508241593837738, Final Batch Loss: 0.7292389869689941\n",
      "Epoch 334, Loss: 1.6485590934753418, Final Batch Loss: 0.849030077457428\n",
      "Epoch 335, Loss: 1.5669466257095337, Final Batch Loss: 0.7632067203521729\n",
      "Epoch 336, Loss: 1.5321865677833557, Final Batch Loss: 0.8018835783004761\n",
      "Epoch 337, Loss: 1.6736642122268677, Final Batch Loss: 0.9388017058372498\n",
      "Epoch 338, Loss: 1.567157506942749, Final Batch Loss: 0.7699408531188965\n",
      "Epoch 339, Loss: 1.6337900161743164, Final Batch Loss: 0.8158453106880188\n",
      "Epoch 340, Loss: 1.467045247554779, Final Batch Loss: 0.6749140620231628\n",
      "Epoch 341, Loss: 1.535157859325409, Final Batch Loss: 0.7322225570678711\n",
      "Epoch 342, Loss: 1.569445013999939, Final Batch Loss: 0.7843901515007019\n",
      "Epoch 343, Loss: 1.583447515964508, Final Batch Loss: 0.7803861498832703\n",
      "Epoch 344, Loss: 1.6629372835159302, Final Batch Loss: 0.9045851230621338\n",
      "Epoch 345, Loss: 1.4988357424736023, Final Batch Loss: 0.725990891456604\n",
      "Epoch 346, Loss: 1.627704679965973, Final Batch Loss: 0.8598499894142151\n",
      "Epoch 347, Loss: 1.609895408153534, Final Batch Loss: 0.8540299534797668\n",
      "Epoch 348, Loss: 1.4627156853675842, Final Batch Loss: 0.6681888103485107\n",
      "Epoch 349, Loss: 1.5787555575370789, Final Batch Loss: 0.8538818955421448\n",
      "Epoch 350, Loss: 1.5087888836860657, Final Batch Loss: 0.7492978572845459\n",
      "Epoch 351, Loss: 1.5502323508262634, Final Batch Loss: 0.7694640159606934\n",
      "Epoch 352, Loss: 1.5941464304924011, Final Batch Loss: 0.8427372574806213\n",
      "Epoch 353, Loss: 1.4438128471374512, Final Batch Loss: 0.7195620536804199\n",
      "Epoch 354, Loss: 1.485429286956787, Final Batch Loss: 0.7057129144668579\n",
      "Epoch 355, Loss: 1.6246477365493774, Final Batch Loss: 0.9185909032821655\n",
      "Epoch 356, Loss: 1.5244725346565247, Final Batch Loss: 0.7540507316589355\n",
      "Epoch 357, Loss: 1.592207908630371, Final Batch Loss: 0.8293930292129517\n",
      "Epoch 358, Loss: 1.443939208984375, Final Batch Loss: 0.7095109224319458\n",
      "Epoch 359, Loss: 1.386994183063507, Final Batch Loss: 0.6363333463668823\n",
      "Epoch 360, Loss: 1.4777994751930237, Final Batch Loss: 0.7472003102302551\n",
      "Epoch 361, Loss: 1.503415584564209, Final Batch Loss: 0.7901924848556519\n",
      "Epoch 362, Loss: 1.5236751437187195, Final Batch Loss: 0.8052403926849365\n",
      "Epoch 363, Loss: 1.422148585319519, Final Batch Loss: 0.6752141118049622\n",
      "Epoch 364, Loss: 1.4381445050239563, Final Batch Loss: 0.6490404009819031\n",
      "Epoch 365, Loss: 1.4219184517860413, Final Batch Loss: 0.7394648790359497\n",
      "Epoch 366, Loss: 1.3027369976043701, Final Batch Loss: 0.5522269010543823\n",
      "Epoch 367, Loss: 1.3625195622444153, Final Batch Loss: 0.7234148979187012\n",
      "Epoch 368, Loss: 1.6335943937301636, Final Batch Loss: 0.9150552153587341\n",
      "Epoch 369, Loss: 1.2933948040008545, Final Batch Loss: 0.5563411116600037\n",
      "Epoch 370, Loss: 1.3921347856521606, Final Batch Loss: 0.7302532196044922\n",
      "Epoch 371, Loss: 1.433313250541687, Final Batch Loss: 0.7296023368835449\n",
      "Epoch 372, Loss: 1.2891448140144348, Final Batch Loss: 0.5856286287307739\n",
      "Epoch 373, Loss: 1.4403844475746155, Final Batch Loss: 0.7238218784332275\n",
      "Epoch 374, Loss: 1.4957948327064514, Final Batch Loss: 0.7752043604850769\n",
      "Epoch 375, Loss: 1.6730482578277588, Final Batch Loss: 0.9744753241539001\n",
      "Epoch 376, Loss: 1.3024753332138062, Final Batch Loss: 0.6157841086387634\n",
      "Epoch 377, Loss: 1.5011975765228271, Final Batch Loss: 0.6909305453300476\n",
      "Epoch 378, Loss: 1.3810479044914246, Final Batch Loss: 0.6715237498283386\n",
      "Epoch 379, Loss: 1.4233809113502502, Final Batch Loss: 0.7055577039718628\n",
      "Epoch 380, Loss: 1.46084862947464, Final Batch Loss: 0.7801057696342468\n",
      "Epoch 381, Loss: 1.5714621543884277, Final Batch Loss: 0.8486539125442505\n",
      "Epoch 382, Loss: 1.4394565224647522, Final Batch Loss: 0.6912813186645508\n",
      "Epoch 383, Loss: 1.3734835386276245, Final Batch Loss: 0.6990410685539246\n",
      "Epoch 384, Loss: 1.4411444664001465, Final Batch Loss: 0.720825731754303\n",
      "Epoch 385, Loss: 1.3426693677902222, Final Batch Loss: 0.6535654664039612\n",
      "Epoch 386, Loss: 1.2961017489433289, Final Batch Loss: 0.5831858515739441\n",
      "Epoch 387, Loss: 1.3650525212287903, Final Batch Loss: 0.6726742386817932\n",
      "Epoch 388, Loss: 1.202003836631775, Final Batch Loss: 0.5189236998558044\n",
      "Epoch 389, Loss: 1.6031394004821777, Final Batch Loss: 0.934589147567749\n",
      "Epoch 390, Loss: 1.2901595830917358, Final Batch Loss: 0.6761119961738586\n",
      "Epoch 391, Loss: 1.2873650789260864, Final Batch Loss: 0.5983762741088867\n",
      "Epoch 392, Loss: 1.3623602390289307, Final Batch Loss: 0.6532191038131714\n",
      "Epoch 393, Loss: 1.4302145838737488, Final Batch Loss: 0.7235615849494934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394, Loss: 1.401848554611206, Final Batch Loss: 0.7440906763076782\n",
      "Epoch 395, Loss: 1.4374247789382935, Final Batch Loss: 0.7680925726890564\n",
      "Epoch 396, Loss: 1.4570482969284058, Final Batch Loss: 0.7705191373825073\n",
      "Epoch 397, Loss: 1.4322638511657715, Final Batch Loss: 0.7582924962043762\n",
      "Epoch 398, Loss: 1.3671597242355347, Final Batch Loss: 0.6597623825073242\n",
      "Epoch 399, Loss: 1.3407734036445618, Final Batch Loss: 0.6622685194015503\n",
      "Epoch 400, Loss: 1.2895036935806274, Final Batch Loss: 0.590908408164978\n",
      "Epoch 401, Loss: 1.2803862690925598, Final Batch Loss: 0.6158269643783569\n",
      "Epoch 402, Loss: 1.3187863230705261, Final Batch Loss: 0.6608487963676453\n",
      "Epoch 403, Loss: 1.2820913195610046, Final Batch Loss: 0.5788602828979492\n",
      "Epoch 404, Loss: 1.3974415063858032, Final Batch Loss: 0.7759535312652588\n",
      "Epoch 405, Loss: 1.3116986155509949, Final Batch Loss: 0.6659000515937805\n",
      "Epoch 406, Loss: 1.3567039966583252, Final Batch Loss: 0.7509855031967163\n",
      "Epoch 407, Loss: 1.3893995881080627, Final Batch Loss: 0.6938765645027161\n",
      "Epoch 408, Loss: 1.2793599367141724, Final Batch Loss: 0.6084455847740173\n",
      "Epoch 409, Loss: 1.3659760355949402, Final Batch Loss: 0.7019065618515015\n",
      "Epoch 410, Loss: 1.3408395648002625, Final Batch Loss: 0.6405437588691711\n",
      "Epoch 411, Loss: 1.2778873443603516, Final Batch Loss: 0.6620824933052063\n",
      "Epoch 412, Loss: 1.2807535529136658, Final Batch Loss: 0.6612991690635681\n",
      "Epoch 413, Loss: 1.232558250427246, Final Batch Loss: 0.552481472492218\n",
      "Epoch 414, Loss: 1.3503533601760864, Final Batch Loss: 0.6881027221679688\n",
      "Epoch 415, Loss: 1.3341909050941467, Final Batch Loss: 0.6502114534378052\n",
      "Epoch 416, Loss: 1.317301094532013, Final Batch Loss: 0.674595832824707\n",
      "Epoch 417, Loss: 1.2435173988342285, Final Batch Loss: 0.5714741349220276\n",
      "Epoch 418, Loss: 1.3510530591011047, Final Batch Loss: 0.7345133423805237\n",
      "Epoch 419, Loss: 1.2823290824890137, Final Batch Loss: 0.6075680255889893\n",
      "Epoch 420, Loss: 1.1716862916946411, Final Batch Loss: 0.5001446604728699\n",
      "Epoch 421, Loss: 1.160689890384674, Final Batch Loss: 0.5417649149894714\n",
      "Epoch 422, Loss: 1.3127376437187195, Final Batch Loss: 0.7064839601516724\n",
      "Epoch 423, Loss: 1.3638207912445068, Final Batch Loss: 0.7298797369003296\n",
      "Epoch 424, Loss: 1.2558423280715942, Final Batch Loss: 0.6494097113609314\n",
      "Epoch 425, Loss: 1.2181859016418457, Final Batch Loss: 0.5906385779380798\n",
      "Epoch 426, Loss: 1.2454959750175476, Final Batch Loss: 0.6284046769142151\n",
      "Epoch 427, Loss: 1.2806825637817383, Final Batch Loss: 0.6202784180641174\n",
      "Epoch 428, Loss: 1.2934629321098328, Final Batch Loss: 0.7183494567871094\n",
      "Epoch 429, Loss: 1.2673579454421997, Final Batch Loss: 0.6575214266777039\n",
      "Epoch 430, Loss: 1.259774088859558, Final Batch Loss: 0.6565699577331543\n",
      "Epoch 431, Loss: 1.4046958684921265, Final Batch Loss: 0.7244362235069275\n",
      "Epoch 432, Loss: 1.3144280910491943, Final Batch Loss: 0.6744434833526611\n",
      "Epoch 433, Loss: 1.2840180397033691, Final Batch Loss: 0.6175759434700012\n",
      "Epoch 434, Loss: 1.3551909923553467, Final Batch Loss: 0.6989055275917053\n",
      "Epoch 435, Loss: 1.3132265210151672, Final Batch Loss: 0.6799526810646057\n",
      "Epoch 436, Loss: 1.2033804059028625, Final Batch Loss: 0.6062054634094238\n",
      "Epoch 437, Loss: 1.190512478351593, Final Batch Loss: 0.5657407641410828\n",
      "Epoch 438, Loss: 1.2339961528778076, Final Batch Loss: 0.6255776286125183\n",
      "Epoch 439, Loss: 1.2340276837348938, Final Batch Loss: 0.6189842224121094\n",
      "Epoch 440, Loss: 1.275890827178955, Final Batch Loss: 0.676635205745697\n",
      "Epoch 441, Loss: 1.2546456456184387, Final Batch Loss: 0.6336720585823059\n",
      "Epoch 442, Loss: 1.3761557936668396, Final Batch Loss: 0.7858235239982605\n",
      "Epoch 443, Loss: 1.3410319685935974, Final Batch Loss: 0.6750914454460144\n",
      "Epoch 444, Loss: 1.2624598145484924, Final Batch Loss: 0.6280084252357483\n",
      "Epoch 445, Loss: 1.2734994888305664, Final Batch Loss: 0.6858934760093689\n",
      "Epoch 446, Loss: 1.2557992339134216, Final Batch Loss: 0.6488350629806519\n",
      "Epoch 447, Loss: 1.261318862438202, Final Batch Loss: 0.6185251474380493\n",
      "Epoch 448, Loss: 1.2122435569763184, Final Batch Loss: 0.6231845617294312\n",
      "Epoch 449, Loss: 1.2496156096458435, Final Batch Loss: 0.6594240665435791\n",
      "Epoch 450, Loss: 1.1811909079551697, Final Batch Loss: 0.622313916683197\n",
      "Epoch 451, Loss: 1.2860915064811707, Final Batch Loss: 0.6487108469009399\n",
      "Epoch 452, Loss: 1.2364473342895508, Final Batch Loss: 0.6851115226745605\n",
      "Epoch 453, Loss: 1.252285122871399, Final Batch Loss: 0.6690866947174072\n",
      "Epoch 454, Loss: 1.3133651614189148, Final Batch Loss: 0.6614754796028137\n",
      "Epoch 455, Loss: 1.2022567987442017, Final Batch Loss: 0.5683965682983398\n",
      "Epoch 456, Loss: 1.1035995483398438, Final Batch Loss: 0.5208059549331665\n",
      "Epoch 457, Loss: 1.237282633781433, Final Batch Loss: 0.6521755456924438\n",
      "Epoch 458, Loss: 1.210540533065796, Final Batch Loss: 0.6430923938751221\n",
      "Epoch 459, Loss: 1.1489874720573425, Final Batch Loss: 0.5962824821472168\n",
      "Epoch 460, Loss: 1.3426164388656616, Final Batch Loss: 0.7444676160812378\n",
      "Epoch 461, Loss: 1.1204187273979187, Final Batch Loss: 0.5235322117805481\n",
      "Epoch 462, Loss: 1.3584529757499695, Final Batch Loss: 0.7306197881698608\n",
      "Epoch 463, Loss: 1.1481348872184753, Final Batch Loss: 0.5540227293968201\n",
      "Epoch 464, Loss: 1.1422756910324097, Final Batch Loss: 0.562865674495697\n",
      "Epoch 465, Loss: 1.2456218004226685, Final Batch Loss: 0.63385009765625\n",
      "Epoch 466, Loss: 1.2059064507484436, Final Batch Loss: 0.575262725353241\n",
      "Epoch 467, Loss: 1.2225804924964905, Final Batch Loss: 0.5732908844947815\n",
      "Epoch 468, Loss: 1.2348219156265259, Final Batch Loss: 0.5719833374023438\n",
      "Epoch 469, Loss: 1.214280605316162, Final Batch Loss: 0.6210755705833435\n",
      "Epoch 470, Loss: 1.3216415047645569, Final Batch Loss: 0.7415431141853333\n",
      "Epoch 471, Loss: 1.3278884887695312, Final Batch Loss: 0.7374016642570496\n",
      "Epoch 472, Loss: 1.3306934237480164, Final Batch Loss: 0.6748772263526917\n",
      "Epoch 473, Loss: 1.1696128845214844, Final Batch Loss: 0.574443519115448\n",
      "Epoch 474, Loss: 1.1965354084968567, Final Batch Loss: 0.5707506537437439\n",
      "Epoch 475, Loss: 1.052141934633255, Final Batch Loss: 0.40600672364234924\n",
      "Epoch 476, Loss: 1.1896716952323914, Final Batch Loss: 0.637191891670227\n",
      "Epoch 477, Loss: 1.2563242316246033, Final Batch Loss: 0.6896907091140747\n",
      "Epoch 478, Loss: 1.1094948053359985, Final Batch Loss: 0.5170751214027405\n",
      "Epoch 479, Loss: 1.1194252967834473, Final Batch Loss: 0.5548629760742188\n",
      "Epoch 480, Loss: 1.1592454314231873, Final Batch Loss: 0.5084571242332458\n",
      "Epoch 481, Loss: 1.217785894870758, Final Batch Loss: 0.6263133883476257\n",
      "Epoch 482, Loss: 1.1804497241973877, Final Batch Loss: 0.5738040208816528\n",
      "Epoch 483, Loss: 1.1324188709259033, Final Batch Loss: 0.5336757898330688\n",
      "Epoch 484, Loss: 1.0872200727462769, Final Batch Loss: 0.538286030292511\n",
      "Epoch 485, Loss: 1.0567490756511688, Final Batch Loss: 0.4979463517665863\n",
      "Epoch 486, Loss: 1.2596871852874756, Final Batch Loss: 0.6781908869743347\n",
      "Epoch 487, Loss: 1.1050592064857483, Final Batch Loss: 0.494442880153656\n",
      "Epoch 488, Loss: 1.0168085098266602, Final Batch Loss: 0.41477566957473755\n",
      "Epoch 489, Loss: 1.1823571920394897, Final Batch Loss: 0.5876979827880859\n",
      "Epoch 490, Loss: 1.1060097813606262, Final Batch Loss: 0.528751790523529\n",
      "Epoch 491, Loss: 1.1317842602729797, Final Batch Loss: 0.605802059173584\n",
      "Epoch 492, Loss: 1.2344560623168945, Final Batch Loss: 0.6439273953437805\n",
      "Epoch 493, Loss: 1.2327343821525574, Final Batch Loss: 0.6841874122619629\n",
      "Epoch 494, Loss: 1.0508237481117249, Final Batch Loss: 0.5104973316192627\n",
      "Epoch 495, Loss: 1.0605974793434143, Final Batch Loss: 0.5049933195114136\n",
      "Epoch 496, Loss: 1.075721114873886, Final Batch Loss: 0.4658036530017853\n",
      "Epoch 497, Loss: 1.1263392567634583, Final Batch Loss: 0.5394876599311829\n",
      "Epoch 498, Loss: 1.1223556399345398, Final Batch Loss: 0.5353542566299438\n",
      "Epoch 499, Loss: 1.0147856175899506, Final Batch Loss: 0.4373951256275177\n",
      "Epoch 500, Loss: 1.0522097051143646, Final Batch Loss: 0.49731937050819397\n",
      "Epoch 501, Loss: 1.2521116733551025, Final Batch Loss: 0.6267659664154053\n",
      "Epoch 502, Loss: 1.1507447361946106, Final Batch Loss: 0.5715031623840332\n",
      "Epoch 503, Loss: 1.0926373600959778, Final Batch Loss: 0.5272485017776489\n",
      "Epoch 504, Loss: 1.064123421907425, Final Batch Loss: 0.45893803238868713\n",
      "Epoch 505, Loss: 1.0310468673706055, Final Batch Loss: 0.49024438858032227\n",
      "Epoch 506, Loss: 1.0765568614006042, Final Batch Loss: 0.5548735857009888\n",
      "Epoch 507, Loss: 1.062523365020752, Final Batch Loss: 0.4893134832382202\n",
      "Epoch 508, Loss: 1.0589764416217804, Final Batch Loss: 0.4734804332256317\n",
      "Epoch 509, Loss: 1.1331427693367004, Final Batch Loss: 0.6230356097221375\n",
      "Epoch 510, Loss: 1.0233303904533386, Final Batch Loss: 0.46417176723480225\n",
      "Epoch 511, Loss: 1.1552828550338745, Final Batch Loss: 0.6378657221794128\n",
      "Epoch 512, Loss: 1.0591099262237549, Final Batch Loss: 0.505416989326477\n",
      "Epoch 513, Loss: 1.1197876334190369, Final Batch Loss: 0.5426875352859497\n",
      "Epoch 514, Loss: 1.0299971401691437, Final Batch Loss: 0.4995712339878082\n",
      "Epoch 515, Loss: 1.0718196034431458, Final Batch Loss: 0.5299582481384277\n",
      "Epoch 516, Loss: 1.126892864704132, Final Batch Loss: 0.537470281124115\n",
      "Epoch 517, Loss: 1.2528557181358337, Final Batch Loss: 0.6417989730834961\n",
      "Epoch 518, Loss: 1.0342780947685242, Final Batch Loss: 0.4873046278953552\n",
      "Epoch 519, Loss: 1.0491545498371124, Final Batch Loss: 0.48697611689567566\n",
      "Epoch 520, Loss: 0.9492352604866028, Final Batch Loss: 0.44623512029647827\n",
      "Epoch 521, Loss: 1.1329151391983032, Final Batch Loss: 0.5887518525123596\n",
      "Epoch 522, Loss: 1.1394514441490173, Final Batch Loss: 0.5392246246337891\n",
      "Epoch 523, Loss: 1.1065165400505066, Final Batch Loss: 0.5842204689979553\n",
      "Epoch 524, Loss: 1.347603440284729, Final Batch Loss: 0.7622717022895813\n",
      "Epoch 525, Loss: 1.1093974113464355, Final Batch Loss: 0.5522497892379761\n",
      "Epoch 526, Loss: 1.1696364283561707, Final Batch Loss: 0.5830706357955933\n",
      "Epoch 527, Loss: 1.220827579498291, Final Batch Loss: 0.5937951803207397\n",
      "Epoch 528, Loss: 1.117433786392212, Final Batch Loss: 0.554563581943512\n",
      "Epoch 529, Loss: 1.011557698249817, Final Batch Loss: 0.49515074491500854\n",
      "Epoch 530, Loss: 1.0511019825935364, Final Batch Loss: 0.5393025279045105\n",
      "Epoch 531, Loss: 1.0748478174209595, Final Batch Loss: 0.5235148668289185\n",
      "Epoch 532, Loss: 1.0037949085235596, Final Batch Loss: 0.4598866105079651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 533, Loss: 1.1340494751930237, Final Batch Loss: 0.5706457495689392\n",
      "Epoch 534, Loss: 1.0958423614501953, Final Batch Loss: 0.5231528282165527\n",
      "Epoch 535, Loss: 0.9740162789821625, Final Batch Loss: 0.453858882188797\n",
      "Epoch 536, Loss: 1.1685205698013306, Final Batch Loss: 0.6289130449295044\n",
      "Epoch 537, Loss: 1.150349497795105, Final Batch Loss: 0.5815105438232422\n",
      "Epoch 538, Loss: 1.1717252731323242, Final Batch Loss: 0.6271188855171204\n",
      "Epoch 539, Loss: 1.1246063113212585, Final Batch Loss: 0.6000583171844482\n",
      "Epoch 540, Loss: 1.1099607348442078, Final Batch Loss: 0.529817521572113\n",
      "Epoch 541, Loss: 1.084691047668457, Final Batch Loss: 0.5215722322463989\n",
      "Epoch 542, Loss: 1.1573519706726074, Final Batch Loss: 0.6361480355262756\n",
      "Epoch 543, Loss: 0.9627979695796967, Final Batch Loss: 0.42465832829475403\n",
      "Epoch 544, Loss: 1.2685922384262085, Final Batch Loss: 0.7114410400390625\n",
      "Epoch 545, Loss: 0.9891577363014221, Final Batch Loss: 0.4841845631599426\n",
      "Epoch 546, Loss: 1.0050953328609467, Final Batch Loss: 0.4637126624584198\n",
      "Epoch 547, Loss: 1.03961843252182, Final Batch Loss: 0.49598264694213867\n",
      "Epoch 548, Loss: 1.042108952999115, Final Batch Loss: 0.4931303858757019\n",
      "Epoch 549, Loss: 1.0739235877990723, Final Batch Loss: 0.5350240468978882\n",
      "Epoch 550, Loss: 1.0767664313316345, Final Batch Loss: 0.5436919927597046\n",
      "Epoch 551, Loss: 1.049622118473053, Final Batch Loss: 0.5094689726829529\n",
      "Epoch 552, Loss: 1.1583603620529175, Final Batch Loss: 0.6108851432800293\n",
      "Epoch 553, Loss: 1.096436083316803, Final Batch Loss: 0.5554730892181396\n",
      "Epoch 554, Loss: 1.0218870639801025, Final Batch Loss: 0.496338427066803\n",
      "Epoch 555, Loss: 1.0588391721248627, Final Batch Loss: 0.5643922686576843\n",
      "Epoch 556, Loss: 0.9487153887748718, Final Batch Loss: 0.4220113754272461\n",
      "Epoch 557, Loss: 0.9282050132751465, Final Batch Loss: 0.41041314601898193\n",
      "Epoch 558, Loss: 1.0088023841381073, Final Batch Loss: 0.47589829564094543\n",
      "Epoch 559, Loss: 1.0597257614135742, Final Batch Loss: 0.4946250319480896\n",
      "Epoch 560, Loss: 0.9234027862548828, Final Batch Loss: 0.4135585427284241\n",
      "Epoch 561, Loss: 1.1690962612628937, Final Batch Loss: 0.6957431435585022\n",
      "Epoch 562, Loss: 1.0457761883735657, Final Batch Loss: 0.5345988273620605\n",
      "Epoch 563, Loss: 0.8801074028015137, Final Batch Loss: 0.41667380928993225\n",
      "Epoch 564, Loss: 0.9937999248504639, Final Batch Loss: 0.49135708808898926\n",
      "Epoch 565, Loss: 0.9379894137382507, Final Batch Loss: 0.4758545756340027\n",
      "Epoch 566, Loss: 1.0274078845977783, Final Batch Loss: 0.5244119763374329\n",
      "Epoch 567, Loss: 1.0228818953037262, Final Batch Loss: 0.5260448455810547\n",
      "Epoch 568, Loss: 1.0192193686962128, Final Batch Loss: 0.48418691754341125\n",
      "Epoch 569, Loss: 1.1380535960197449, Final Batch Loss: 0.6284976601600647\n",
      "Epoch 570, Loss: 0.9581837058067322, Final Batch Loss: 0.42166101932525635\n",
      "Epoch 571, Loss: 1.1438834965229034, Final Batch Loss: 0.6484193801879883\n",
      "Epoch 572, Loss: 0.9643926620483398, Final Batch Loss: 0.46291130781173706\n",
      "Epoch 573, Loss: 0.981569916009903, Final Batch Loss: 0.4479736387729645\n",
      "Epoch 574, Loss: 0.8681575655937195, Final Batch Loss: 0.3569289445877075\n",
      "Epoch 575, Loss: 1.0852259993553162, Final Batch Loss: 0.5819875597953796\n",
      "Epoch 576, Loss: 1.045718789100647, Final Batch Loss: 0.5336591601371765\n",
      "Epoch 577, Loss: 1.0005443096160889, Final Batch Loss: 0.49984049797058105\n",
      "Epoch 578, Loss: 1.153226524591446, Final Batch Loss: 0.6694070100784302\n",
      "Epoch 579, Loss: 1.0522130131721497, Final Batch Loss: 0.54444819688797\n",
      "Epoch 580, Loss: 1.0605398416519165, Final Batch Loss: 0.5278690457344055\n",
      "Epoch 581, Loss: 0.9431003034114838, Final Batch Loss: 0.4777812957763672\n",
      "Epoch 582, Loss: 0.9686987698078156, Final Batch Loss: 0.47194820642471313\n",
      "Epoch 583, Loss: 1.1894401907920837, Final Batch Loss: 0.6531354188919067\n",
      "Epoch 584, Loss: 0.8746328353881836, Final Batch Loss: 0.37963688373565674\n",
      "Epoch 585, Loss: 0.8793495893478394, Final Batch Loss: 0.4184359908103943\n",
      "Epoch 586, Loss: 1.0337452292442322, Final Batch Loss: 0.5463454127311707\n",
      "Epoch 587, Loss: 0.9036502540111542, Final Batch Loss: 0.39392563700675964\n",
      "Epoch 588, Loss: 1.1685765981674194, Final Batch Loss: 0.6353383660316467\n",
      "Epoch 589, Loss: 1.0883873105049133, Final Batch Loss: 0.5902332663536072\n",
      "Epoch 590, Loss: 1.0279991030693054, Final Batch Loss: 0.4579436182975769\n",
      "Epoch 591, Loss: 1.0265478193759918, Final Batch Loss: 0.48361411690711975\n",
      "Epoch 592, Loss: 0.9890509843826294, Final Batch Loss: 0.48076993227005005\n",
      "Epoch 593, Loss: 0.9514387547969818, Final Batch Loss: 0.479921817779541\n",
      "Epoch 594, Loss: 1.0229198634624481, Final Batch Loss: 0.4979231655597687\n",
      "Epoch 595, Loss: 0.9935320317745209, Final Batch Loss: 0.5259513854980469\n",
      "Epoch 596, Loss: 1.0492777824401855, Final Batch Loss: 0.5785483121871948\n",
      "Epoch 597, Loss: 0.9385960102081299, Final Batch Loss: 0.42017799615859985\n",
      "Epoch 598, Loss: 0.9836823642253876, Final Batch Loss: 0.4989403486251831\n",
      "Epoch 599, Loss: 1.0068411827087402, Final Batch Loss: 0.5427237153053284\n",
      "Epoch 600, Loss: 0.931507021188736, Final Batch Loss: 0.43246379494667053\n",
      "Epoch 601, Loss: 1.0205563306808472, Final Batch Loss: 0.48454952239990234\n",
      "Epoch 602, Loss: 1.016928255558014, Final Batch Loss: 0.5093754529953003\n",
      "Epoch 603, Loss: 1.0783753991127014, Final Batch Loss: 0.6228570342063904\n",
      "Epoch 604, Loss: 1.0395203828811646, Final Batch Loss: 0.5554907917976379\n",
      "Epoch 605, Loss: 0.9281706213951111, Final Batch Loss: 0.43729180097579956\n",
      "Epoch 606, Loss: 0.9756145179271698, Final Batch Loss: 0.4714101254940033\n",
      "Epoch 607, Loss: 1.0643665194511414, Final Batch Loss: 0.5087871551513672\n",
      "Epoch 608, Loss: 0.8634085953235626, Final Batch Loss: 0.3754858374595642\n",
      "Epoch 609, Loss: 0.9889893531799316, Final Batch Loss: 0.5089159607887268\n",
      "Epoch 610, Loss: 0.9021575748920441, Final Batch Loss: 0.45230361819267273\n",
      "Epoch 611, Loss: 0.9563639461994171, Final Batch Loss: 0.45702630281448364\n",
      "Epoch 612, Loss: 0.9360131025314331, Final Batch Loss: 0.46803122758865356\n",
      "Epoch 613, Loss: 0.9749482572078705, Final Batch Loss: 0.5267972350120544\n",
      "Epoch 614, Loss: 0.8912635743618011, Final Batch Loss: 0.37038692831993103\n",
      "Epoch 615, Loss: 0.9808716773986816, Final Batch Loss: 0.5141481757164001\n",
      "Epoch 616, Loss: 1.0047254860401154, Final Batch Loss: 0.5248703360557556\n",
      "Epoch 617, Loss: 1.0530281364917755, Final Batch Loss: 0.5798987150192261\n",
      "Epoch 618, Loss: 0.9383844435214996, Final Batch Loss: 0.45648661255836487\n",
      "Epoch 619, Loss: 0.9200125634670258, Final Batch Loss: 0.4813852310180664\n",
      "Epoch 620, Loss: 0.9685679972171783, Final Batch Loss: 0.4747902452945709\n",
      "Epoch 621, Loss: 0.8643945157527924, Final Batch Loss: 0.40663546323776245\n",
      "Epoch 622, Loss: 0.9527531564235687, Final Batch Loss: 0.4520592987537384\n",
      "Epoch 623, Loss: 0.9135894179344177, Final Batch Loss: 0.47648611664772034\n",
      "Epoch 624, Loss: 0.9814116060733795, Final Batch Loss: 0.5334525108337402\n",
      "Epoch 625, Loss: 1.002178966999054, Final Batch Loss: 0.552635669708252\n",
      "Epoch 626, Loss: 1.0007040202617645, Final Batch Loss: 0.5463101863861084\n",
      "Epoch 627, Loss: 1.0096275210380554, Final Batch Loss: 0.6064414381980896\n",
      "Epoch 628, Loss: 0.983230859041214, Final Batch Loss: 0.49401238560676575\n",
      "Epoch 629, Loss: 0.9700033664703369, Final Batch Loss: 0.4744603931903839\n",
      "Epoch 630, Loss: 1.0566624701023102, Final Batch Loss: 0.4960469901561737\n",
      "Epoch 631, Loss: 0.9874314963817596, Final Batch Loss: 0.46035948395729065\n",
      "Epoch 632, Loss: 0.986347109079361, Final Batch Loss: 0.4965358376502991\n",
      "Epoch 633, Loss: 0.9139301478862762, Final Batch Loss: 0.4470292031764984\n",
      "Epoch 634, Loss: 0.9293261468410492, Final Batch Loss: 0.42498210072517395\n",
      "Epoch 635, Loss: 0.8902175724506378, Final Batch Loss: 0.4487660825252533\n",
      "Epoch 636, Loss: 0.88485187292099, Final Batch Loss: 0.42809394001960754\n",
      "Epoch 637, Loss: 0.9082063436508179, Final Batch Loss: 0.4607764482498169\n",
      "Epoch 638, Loss: 0.8617040514945984, Final Batch Loss: 0.44111964106559753\n",
      "Epoch 639, Loss: 1.0266617238521576, Final Batch Loss: 0.5978173613548279\n",
      "Epoch 640, Loss: 1.071487694978714, Final Batch Loss: 0.6045311689376831\n",
      "Epoch 641, Loss: 0.8580648005008698, Final Batch Loss: 0.4127919375896454\n",
      "Epoch 642, Loss: 0.9564833045005798, Final Batch Loss: 0.4863128364086151\n",
      "Epoch 643, Loss: 0.8048696517944336, Final Batch Loss: 0.3652925193309784\n",
      "Epoch 644, Loss: 0.9624858498573303, Final Batch Loss: 0.4837191104888916\n",
      "Epoch 645, Loss: 0.9493840038776398, Final Batch Loss: 0.44057098031044006\n",
      "Epoch 646, Loss: 0.8792569935321808, Final Batch Loss: 0.44614723324775696\n",
      "Epoch 647, Loss: 0.9066323935985565, Final Batch Loss: 0.4211507737636566\n",
      "Epoch 648, Loss: 0.9351722300052643, Final Batch Loss: 0.4845925569534302\n",
      "Epoch 649, Loss: 0.8871138691902161, Final Batch Loss: 0.4445367455482483\n",
      "Epoch 650, Loss: 0.9654226303100586, Final Batch Loss: 0.47094783186912537\n",
      "Epoch 651, Loss: 1.0084684789180756, Final Batch Loss: 0.5222939252853394\n",
      "Epoch 652, Loss: 0.9020922482013702, Final Batch Loss: 0.47139376401901245\n",
      "Epoch 653, Loss: 0.8750534653663635, Final Batch Loss: 0.3937267065048218\n",
      "Epoch 654, Loss: 0.8352303802967072, Final Batch Loss: 0.4368032217025757\n",
      "Epoch 655, Loss: 0.7906337380409241, Final Batch Loss: 0.3320518732070923\n",
      "Epoch 656, Loss: 0.8042538166046143, Final Batch Loss: 0.34339645504951477\n",
      "Epoch 657, Loss: 1.041898638010025, Final Batch Loss: 0.49191418290138245\n",
      "Epoch 658, Loss: 0.9821172058582306, Final Batch Loss: 0.5558539032936096\n",
      "Epoch 659, Loss: 0.9072835743427277, Final Batch Loss: 0.46022990345954895\n",
      "Epoch 660, Loss: 0.9030555486679077, Final Batch Loss: 0.46436432003974915\n",
      "Epoch 661, Loss: 0.9993421733379364, Final Batch Loss: 0.5261498093605042\n",
      "Epoch 662, Loss: 0.8333836495876312, Final Batch Loss: 0.3336483836174011\n",
      "Epoch 663, Loss: 1.0114582777023315, Final Batch Loss: 0.5569935441017151\n",
      "Epoch 664, Loss: 1.0761158168315887, Final Batch Loss: 0.5915300846099854\n",
      "Epoch 665, Loss: 0.8690817058086395, Final Batch Loss: 0.48769769072532654\n",
      "Epoch 666, Loss: 0.8373146057128906, Final Batch Loss: 0.40727272629737854\n",
      "Epoch 667, Loss: 0.7500262260437012, Final Batch Loss: 0.3479282557964325\n",
      "Epoch 668, Loss: 0.812778890132904, Final Batch Loss: 0.36509230732917786\n",
      "Epoch 669, Loss: 0.8246348798274994, Final Batch Loss: 0.36963972449302673\n",
      "Epoch 670, Loss: 0.954584389925003, Final Batch Loss: 0.5260939002037048\n",
      "Epoch 671, Loss: 0.7531956732273102, Final Batch Loss: 0.3628842234611511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 672, Loss: 0.8130506575107574, Final Batch Loss: 0.3652189373970032\n",
      "Epoch 673, Loss: 0.9395175874233246, Final Batch Loss: 0.5077999830245972\n",
      "Epoch 674, Loss: 0.9225142598152161, Final Batch Loss: 0.4442424774169922\n",
      "Epoch 675, Loss: 0.8165604472160339, Final Batch Loss: 0.3979955315589905\n",
      "Epoch 676, Loss: 0.9711689352989197, Final Batch Loss: 0.530526876449585\n",
      "Epoch 677, Loss: 0.9085776507854462, Final Batch Loss: 0.5282914042472839\n",
      "Epoch 678, Loss: 0.8901261985301971, Final Batch Loss: 0.40442290902137756\n",
      "Epoch 679, Loss: 0.7921691834926605, Final Batch Loss: 0.33431917428970337\n",
      "Epoch 680, Loss: 0.9941673576831818, Final Batch Loss: 0.6027175188064575\n",
      "Epoch 681, Loss: 0.8789304494857788, Final Batch Loss: 0.4879153072834015\n",
      "Epoch 682, Loss: 0.8967277109622955, Final Batch Loss: 0.44381487369537354\n",
      "Epoch 683, Loss: 0.8236236274242401, Final Batch Loss: 0.4208150804042816\n",
      "Epoch 684, Loss: 0.9492027461528778, Final Batch Loss: 0.44922858476638794\n",
      "Epoch 685, Loss: 0.9213606417179108, Final Batch Loss: 0.5381566882133484\n",
      "Epoch 686, Loss: 0.7675151228904724, Final Batch Loss: 0.4030579924583435\n",
      "Epoch 687, Loss: 0.8235548734664917, Final Batch Loss: 0.3822867274284363\n",
      "Epoch 688, Loss: 0.9268715083599091, Final Batch Loss: 0.5119761824607849\n",
      "Epoch 689, Loss: 0.9276789128780365, Final Batch Loss: 0.5084974765777588\n",
      "Epoch 690, Loss: 0.8549612164497375, Final Batch Loss: 0.4123977720737457\n",
      "Epoch 691, Loss: 0.87581005692482, Final Batch Loss: 0.47600555419921875\n",
      "Epoch 692, Loss: 0.8822617828845978, Final Batch Loss: 0.4860836863517761\n",
      "Epoch 693, Loss: 0.7275715470314026, Final Batch Loss: 0.269848108291626\n",
      "Epoch 694, Loss: 0.8237904608249664, Final Batch Loss: 0.3634926676750183\n",
      "Epoch 695, Loss: 0.8431695699691772, Final Batch Loss: 0.4002823829650879\n",
      "Epoch 696, Loss: 0.9795788526535034, Final Batch Loss: 0.5431287288665771\n",
      "Epoch 697, Loss: 0.8508449792861938, Final Batch Loss: 0.37638145685195923\n",
      "Epoch 698, Loss: 0.9115470051765442, Final Batch Loss: 0.4376285672187805\n",
      "Epoch 699, Loss: 0.9366834759712219, Final Batch Loss: 0.4565131366252899\n",
      "Epoch 700, Loss: 0.8046839833259583, Final Batch Loss: 0.40685129165649414\n",
      "Epoch 701, Loss: 0.7595194280147552, Final Batch Loss: 0.3998163640499115\n",
      "Epoch 702, Loss: 0.8294818103313446, Final Batch Loss: 0.4080178737640381\n",
      "Epoch 703, Loss: 0.9564757645130157, Final Batch Loss: 0.5405946969985962\n",
      "Epoch 704, Loss: 0.859727680683136, Final Batch Loss: 0.4440007209777832\n",
      "Epoch 705, Loss: 0.80594801902771, Final Batch Loss: 0.36508482694625854\n",
      "Epoch 706, Loss: 0.8467452824115753, Final Batch Loss: 0.45152053236961365\n",
      "Epoch 707, Loss: 0.9127418100833893, Final Batch Loss: 0.484479159116745\n",
      "Epoch 708, Loss: 0.8462443351745605, Final Batch Loss: 0.432756245136261\n",
      "Epoch 709, Loss: 0.875987708568573, Final Batch Loss: 0.42825666069984436\n",
      "Epoch 710, Loss: 0.80382239818573, Final Batch Loss: 0.35607826709747314\n",
      "Epoch 711, Loss: 0.7348456084728241, Final Batch Loss: 0.31979045271873474\n",
      "Epoch 712, Loss: 0.7909365296363831, Final Batch Loss: 0.4055598974227905\n",
      "Epoch 713, Loss: 0.855023592710495, Final Batch Loss: 0.3778114914894104\n",
      "Epoch 714, Loss: 0.9763456284999847, Final Batch Loss: 0.47592559456825256\n",
      "Epoch 715, Loss: 0.7623947560787201, Final Batch Loss: 0.38205617666244507\n",
      "Epoch 716, Loss: 0.854801744222641, Final Batch Loss: 0.4555211365222931\n",
      "Epoch 717, Loss: 0.8237216770648956, Final Batch Loss: 0.3747420310974121\n",
      "Epoch 718, Loss: 0.820682942867279, Final Batch Loss: 0.40003320574760437\n",
      "Epoch 719, Loss: 0.8277356028556824, Final Batch Loss: 0.42472630739212036\n",
      "Epoch 720, Loss: 0.8536790609359741, Final Batch Loss: 0.39526015520095825\n",
      "Epoch 721, Loss: 1.0142577290534973, Final Batch Loss: 0.5729960799217224\n",
      "Epoch 722, Loss: 0.7428553700447083, Final Batch Loss: 0.3516446053981781\n",
      "Epoch 723, Loss: 0.8357064425945282, Final Batch Loss: 0.4509902596473694\n",
      "Epoch 724, Loss: 0.736573189496994, Final Batch Loss: 0.34224599599838257\n",
      "Epoch 725, Loss: 0.8061360716819763, Final Batch Loss: 0.4160008132457733\n",
      "Epoch 726, Loss: 0.8071119487285614, Final Batch Loss: 0.4095537066459656\n",
      "Epoch 727, Loss: 0.8328017592430115, Final Batch Loss: 0.379544198513031\n",
      "Epoch 728, Loss: 0.9044880270957947, Final Batch Loss: 0.5463160872459412\n",
      "Epoch 729, Loss: 0.7618740499019623, Final Batch Loss: 0.36060476303100586\n",
      "Epoch 730, Loss: 0.736440122127533, Final Batch Loss: 0.36822909116744995\n",
      "Epoch 731, Loss: 0.8481309115886688, Final Batch Loss: 0.4726918041706085\n",
      "Epoch 732, Loss: 0.8568710088729858, Final Batch Loss: 0.40587136149406433\n",
      "Epoch 733, Loss: 0.8722570240497589, Final Batch Loss: 0.4934552013874054\n",
      "Epoch 734, Loss: 0.7991549670696259, Final Batch Loss: 0.4217505156993866\n",
      "Epoch 735, Loss: 0.8591610491275787, Final Batch Loss: 0.4033495783805847\n",
      "Epoch 736, Loss: 0.9398961365222931, Final Batch Loss: 0.5132650136947632\n",
      "Epoch 737, Loss: 0.8098513782024384, Final Batch Loss: 0.403014212846756\n",
      "Epoch 738, Loss: 0.7696792483329773, Final Batch Loss: 0.3346364498138428\n",
      "Epoch 739, Loss: 0.8637283742427826, Final Batch Loss: 0.43818438053131104\n",
      "Epoch 740, Loss: 0.8412672281265259, Final Batch Loss: 0.4635744094848633\n",
      "Epoch 741, Loss: 0.7671307623386383, Final Batch Loss: 0.3912067115306854\n",
      "Epoch 742, Loss: 0.8937792181968689, Final Batch Loss: 0.4513823091983795\n",
      "Epoch 743, Loss: 0.8281592130661011, Final Batch Loss: 0.4166312515735626\n",
      "Epoch 744, Loss: 0.8529082238674164, Final Batch Loss: 0.3775884509086609\n",
      "Epoch 745, Loss: 0.8711424171924591, Final Batch Loss: 0.44615423679351807\n",
      "Epoch 746, Loss: 0.7418461740016937, Final Batch Loss: 0.3633311688899994\n",
      "Epoch 747, Loss: 0.6908406615257263, Final Batch Loss: 0.2857988476753235\n",
      "Epoch 748, Loss: 0.8301244378089905, Final Batch Loss: 0.44514694809913635\n",
      "Epoch 749, Loss: 0.7946769595146179, Final Batch Loss: 0.34354469180107117\n",
      "Epoch 750, Loss: 0.7812100052833557, Final Batch Loss: 0.33237314224243164\n",
      "Epoch 751, Loss: 0.778626412153244, Final Batch Loss: 0.3985132873058319\n",
      "Epoch 752, Loss: 0.6950653791427612, Final Batch Loss: 0.3085547089576721\n",
      "Epoch 753, Loss: 0.7081948518753052, Final Batch Loss: 0.3152208626270294\n",
      "Epoch 754, Loss: 0.6423295140266418, Final Batch Loss: 0.27257272601127625\n",
      "Epoch 755, Loss: 0.7378748059272766, Final Batch Loss: 0.29742833971977234\n",
      "Epoch 756, Loss: 0.8503621816635132, Final Batch Loss: 0.5185545682907104\n",
      "Epoch 757, Loss: 0.7297161519527435, Final Batch Loss: 0.3489776849746704\n",
      "Epoch 758, Loss: 0.7581847012042999, Final Batch Loss: 0.35391682386398315\n",
      "Epoch 759, Loss: 0.6457821130752563, Final Batch Loss: 0.2839481234550476\n",
      "Epoch 760, Loss: 0.6746608912944794, Final Batch Loss: 0.28135377168655396\n",
      "Epoch 761, Loss: 0.6358681619167328, Final Batch Loss: 0.2747204601764679\n",
      "Epoch 762, Loss: 0.6766137778759003, Final Batch Loss: 0.3149498999118805\n",
      "Epoch 763, Loss: 0.7494138479232788, Final Batch Loss: 0.4170305132865906\n",
      "Epoch 764, Loss: 0.77940633893013, Final Batch Loss: 0.3783356845378876\n",
      "Epoch 765, Loss: 0.7787506878376007, Final Batch Loss: 0.40819990634918213\n",
      "Epoch 766, Loss: 0.7982691824436188, Final Batch Loss: 0.3876645565032959\n",
      "Epoch 767, Loss: 0.8107410669326782, Final Batch Loss: 0.4261057376861572\n",
      "Epoch 768, Loss: 0.7475483119487762, Final Batch Loss: 0.3477770984172821\n",
      "Epoch 769, Loss: 0.8089436888694763, Final Batch Loss: 0.4165843725204468\n",
      "Epoch 770, Loss: 0.6787292659282684, Final Batch Loss: 0.31313443183898926\n",
      "Epoch 771, Loss: 0.7719516754150391, Final Batch Loss: 0.3656562864780426\n",
      "Epoch 772, Loss: 0.7522267997264862, Final Batch Loss: 0.31917324662208557\n",
      "Epoch 773, Loss: 0.8269378244876862, Final Batch Loss: 0.39449259638786316\n",
      "Epoch 774, Loss: 0.9778428077697754, Final Batch Loss: 0.5451655387878418\n",
      "Epoch 775, Loss: 0.7049959301948547, Final Batch Loss: 0.30556029081344604\n",
      "Epoch 776, Loss: 0.7496088743209839, Final Batch Loss: 0.33049476146698\n",
      "Epoch 777, Loss: 0.8036693930625916, Final Batch Loss: 0.3788577914237976\n",
      "Epoch 778, Loss: 0.7419412732124329, Final Batch Loss: 0.36480775475502014\n",
      "Epoch 779, Loss: 0.7400232553482056, Final Batch Loss: 0.35375046730041504\n",
      "Epoch 780, Loss: 0.7536056935787201, Final Batch Loss: 0.3827144503593445\n",
      "Epoch 781, Loss: 0.9020103514194489, Final Batch Loss: 0.4890124499797821\n",
      "Epoch 782, Loss: 0.6637203693389893, Final Batch Loss: 0.36108988523483276\n",
      "Epoch 783, Loss: 0.7202729880809784, Final Batch Loss: 0.3481672406196594\n",
      "Epoch 784, Loss: 0.7205930948257446, Final Batch Loss: 0.27389058470726013\n",
      "Epoch 785, Loss: 0.6700466871261597, Final Batch Loss: 0.31559813022613525\n",
      "Epoch 786, Loss: 0.6248048841953278, Final Batch Loss: 0.27917715907096863\n",
      "Epoch 787, Loss: 0.701410323381424, Final Batch Loss: 0.3291400372982025\n",
      "Epoch 788, Loss: 0.7257064282894135, Final Batch Loss: 0.31952738761901855\n",
      "Epoch 789, Loss: 0.6748485565185547, Final Batch Loss: 0.2964807450771332\n",
      "Epoch 790, Loss: 0.873613715171814, Final Batch Loss: 0.4860807955265045\n",
      "Epoch 791, Loss: 0.8150452077388763, Final Batch Loss: 0.42644426226615906\n",
      "Epoch 792, Loss: 0.6863804459571838, Final Batch Loss: 0.3319183588027954\n",
      "Epoch 793, Loss: 0.7840532064437866, Final Batch Loss: 0.392423540353775\n",
      "Epoch 794, Loss: 0.7018244862556458, Final Batch Loss: 0.3020114302635193\n",
      "Epoch 795, Loss: 0.7203871011734009, Final Batch Loss: 0.3586592972278595\n",
      "Epoch 796, Loss: 0.7577815651893616, Final Batch Loss: 0.3942209482192993\n",
      "Epoch 797, Loss: 0.6457303464412689, Final Batch Loss: 0.3136862814426422\n",
      "Epoch 798, Loss: 0.8111215829849243, Final Batch Loss: 0.43391793966293335\n",
      "Epoch 799, Loss: 0.6833626627922058, Final Batch Loss: 0.3013492226600647\n",
      "Epoch 800, Loss: 0.631498783826828, Final Batch Loss: 0.2760498821735382\n",
      "Epoch 801, Loss: 0.7597715258598328, Final Batch Loss: 0.361861914396286\n",
      "Epoch 802, Loss: 0.7599985599517822, Final Batch Loss: 0.36158835887908936\n",
      "Epoch 803, Loss: 0.764738142490387, Final Batch Loss: 0.3503582775592804\n",
      "Epoch 804, Loss: 0.8330776393413544, Final Batch Loss: 0.4556271433830261\n",
      "Epoch 805, Loss: 0.8147547841072083, Final Batch Loss: 0.4136001765727997\n",
      "Epoch 806, Loss: 0.7977427244186401, Final Batch Loss: 0.44283077120780945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807, Loss: 0.772411048412323, Final Batch Loss: 0.42599257826805115\n",
      "Epoch 808, Loss: 0.7089802920818329, Final Batch Loss: 0.428761750459671\n",
      "Epoch 809, Loss: 0.7589907646179199, Final Batch Loss: 0.40716585516929626\n",
      "Epoch 810, Loss: 0.8304724097251892, Final Batch Loss: 0.4548329710960388\n",
      "Epoch 811, Loss: 0.6164547801017761, Final Batch Loss: 0.25322359800338745\n",
      "Epoch 812, Loss: 0.6848934292793274, Final Batch Loss: 0.3031279742717743\n",
      "Epoch 813, Loss: 0.8275701999664307, Final Batch Loss: 0.4626738131046295\n",
      "Epoch 814, Loss: 0.7940008342266083, Final Batch Loss: 0.4513685405254364\n",
      "Epoch 815, Loss: 0.7047123610973358, Final Batch Loss: 0.36756882071495056\n",
      "Epoch 816, Loss: 0.7291024625301361, Final Batch Loss: 0.40334534645080566\n",
      "Epoch 817, Loss: 0.6780075430870056, Final Batch Loss: 0.2923703193664551\n",
      "Epoch 818, Loss: 0.6367287039756775, Final Batch Loss: 0.324149489402771\n",
      "Epoch 819, Loss: 0.81592857837677, Final Batch Loss: 0.4256768822669983\n",
      "Epoch 820, Loss: 0.7656275629997253, Final Batch Loss: 0.3887178599834442\n",
      "Epoch 821, Loss: 0.7344703078269958, Final Batch Loss: 0.3911679983139038\n",
      "Epoch 822, Loss: 0.7188242971897125, Final Batch Loss: 0.36493274569511414\n",
      "Epoch 823, Loss: 0.7952300608158112, Final Batch Loss: 0.37176692485809326\n",
      "Epoch 824, Loss: 0.7349372208118439, Final Batch Loss: 0.3777490258216858\n",
      "Epoch 825, Loss: 0.7740010917186737, Final Batch Loss: 0.4131939113140106\n",
      "Epoch 826, Loss: 0.5789991915225983, Final Batch Loss: 0.23660916090011597\n",
      "Epoch 827, Loss: 0.7069283127784729, Final Batch Loss: 0.34705108404159546\n",
      "Epoch 828, Loss: 0.8114922940731049, Final Batch Loss: 0.39540305733680725\n",
      "Epoch 829, Loss: 0.7581879794597626, Final Batch Loss: 0.38913464546203613\n",
      "Epoch 830, Loss: 0.5954549908638, Final Batch Loss: 0.2626170217990875\n",
      "Epoch 831, Loss: 0.6955274045467377, Final Batch Loss: 0.31581738591194153\n",
      "Epoch 832, Loss: 0.600453108549118, Final Batch Loss: 0.26075196266174316\n",
      "Epoch 833, Loss: 0.6787138879299164, Final Batch Loss: 0.28951212763786316\n",
      "Epoch 834, Loss: 0.6063934862613678, Final Batch Loss: 0.25014638900756836\n",
      "Epoch 835, Loss: 0.6733075976371765, Final Batch Loss: 0.29243820905685425\n",
      "Epoch 836, Loss: 0.9092280864715576, Final Batch Loss: 0.5424692630767822\n",
      "Epoch 837, Loss: 0.7162826955318451, Final Batch Loss: 0.37700456380844116\n",
      "Epoch 838, Loss: 0.6568111181259155, Final Batch Loss: 0.2605363726615906\n",
      "Epoch 839, Loss: 0.7784050107002258, Final Batch Loss: 0.46233615279197693\n",
      "Epoch 840, Loss: 0.9351979792118073, Final Batch Loss: 0.549939751625061\n",
      "Epoch 841, Loss: 0.7181748449802399, Final Batch Loss: 0.29099905490875244\n",
      "Epoch 842, Loss: 0.7360018789768219, Final Batch Loss: 0.42395395040512085\n",
      "Epoch 843, Loss: 0.7104332149028778, Final Batch Loss: 0.40089473128318787\n",
      "Epoch 844, Loss: 0.7282618284225464, Final Batch Loss: 0.3806404769420624\n",
      "Epoch 845, Loss: 0.7200558185577393, Final Batch Loss: 0.34107327461242676\n",
      "Epoch 846, Loss: 0.6435004472732544, Final Batch Loss: 0.3027239739894867\n",
      "Epoch 847, Loss: 0.6556298136711121, Final Batch Loss: 0.3104588985443115\n",
      "Epoch 848, Loss: 0.7439244091510773, Final Batch Loss: 0.3170523941516876\n",
      "Epoch 849, Loss: 0.7163176536560059, Final Batch Loss: 0.3517534136772156\n",
      "Epoch 850, Loss: 0.77199786901474, Final Batch Loss: 0.42048904299736023\n",
      "Epoch 851, Loss: 0.7034668326377869, Final Batch Loss: 0.31993183493614197\n",
      "Epoch 852, Loss: 0.721897155046463, Final Batch Loss: 0.3935492932796478\n",
      "Epoch 853, Loss: 0.6439683437347412, Final Batch Loss: 0.26069679856300354\n",
      "Epoch 854, Loss: 0.7275910973548889, Final Batch Loss: 0.32155293226242065\n",
      "Epoch 855, Loss: 0.7299506664276123, Final Batch Loss: 0.3463951349258423\n",
      "Epoch 856, Loss: 0.6652840673923492, Final Batch Loss: 0.35266584157943726\n",
      "Epoch 857, Loss: 0.6577351093292236, Final Batch Loss: 0.290615975856781\n",
      "Epoch 858, Loss: 0.68189537525177, Final Batch Loss: 0.3622380793094635\n",
      "Epoch 859, Loss: 0.704823225736618, Final Batch Loss: 0.35445359349250793\n",
      "Epoch 860, Loss: 0.7145949304103851, Final Batch Loss: 0.3807498514652252\n",
      "Epoch 861, Loss: 0.6475965678691864, Final Batch Loss: 0.31936734914779663\n",
      "Epoch 862, Loss: 0.7541108131408691, Final Batch Loss: 0.41920560598373413\n",
      "Epoch 863, Loss: 0.761656790971756, Final Batch Loss: 0.41161707043647766\n",
      "Epoch 864, Loss: 0.7247460782527924, Final Batch Loss: 0.38798987865448\n",
      "Epoch 865, Loss: 0.6433351933956146, Final Batch Loss: 0.29003238677978516\n",
      "Epoch 866, Loss: 0.7784594595432281, Final Batch Loss: 0.4376433193683624\n",
      "Epoch 867, Loss: 0.6373526751995087, Final Batch Loss: 0.28945204615592957\n",
      "Epoch 868, Loss: 0.7918542623519897, Final Batch Loss: 0.42795607447624207\n",
      "Epoch 869, Loss: 0.7064014971256256, Final Batch Loss: 0.364152193069458\n",
      "Epoch 870, Loss: 0.6586084365844727, Final Batch Loss: 0.2881200909614563\n",
      "Epoch 871, Loss: 0.6180491149425507, Final Batch Loss: 0.27364611625671387\n",
      "Epoch 872, Loss: 0.6305191218852997, Final Batch Loss: 0.2753685414791107\n",
      "Epoch 873, Loss: 0.6835210025310516, Final Batch Loss: 0.2848164737224579\n",
      "Epoch 874, Loss: 0.6174294352531433, Final Batch Loss: 0.26486995816230774\n",
      "Epoch 875, Loss: 0.8060948550701141, Final Batch Loss: 0.47313955426216125\n",
      "Epoch 876, Loss: 0.7297266125679016, Final Batch Loss: 0.4203980565071106\n",
      "Epoch 877, Loss: 0.6414093971252441, Final Batch Loss: 0.2882787883281708\n",
      "Epoch 878, Loss: 0.6849627494812012, Final Batch Loss: 0.3726179003715515\n",
      "Epoch 879, Loss: 0.6763911545276642, Final Batch Loss: 0.3566526174545288\n",
      "Epoch 880, Loss: 0.7025668025016785, Final Batch Loss: 0.33598172664642334\n",
      "Epoch 881, Loss: 0.6876743733882904, Final Batch Loss: 0.3857336938381195\n",
      "Epoch 882, Loss: 0.6926568150520325, Final Batch Loss: 0.353619784116745\n",
      "Epoch 883, Loss: 0.6305961906909943, Final Batch Loss: 0.2896263599395752\n",
      "Epoch 884, Loss: 0.6593707501888275, Final Batch Loss: 0.32230451703071594\n",
      "Epoch 885, Loss: 0.6729982495307922, Final Batch Loss: 0.29881519079208374\n",
      "Epoch 886, Loss: 0.8065063953399658, Final Batch Loss: 0.4703907072544098\n",
      "Epoch 887, Loss: 0.6512037515640259, Final Batch Loss: 0.2757008969783783\n",
      "Epoch 888, Loss: 0.8472957611083984, Final Batch Loss: 0.523188591003418\n",
      "Epoch 889, Loss: 0.658795028924942, Final Batch Loss: 0.30805763602256775\n",
      "Epoch 890, Loss: 0.6807781159877777, Final Batch Loss: 0.3899734914302826\n",
      "Epoch 891, Loss: 0.7066946923732758, Final Batch Loss: 0.36397531628608704\n",
      "Epoch 892, Loss: 0.7191547453403473, Final Batch Loss: 0.4033157527446747\n",
      "Epoch 893, Loss: 0.6433515846729279, Final Batch Loss: 0.3151870667934418\n",
      "Epoch 894, Loss: 0.6714018881320953, Final Batch Loss: 0.31152456998825073\n",
      "Epoch 895, Loss: 0.8002903759479523, Final Batch Loss: 0.40319037437438965\n",
      "Epoch 896, Loss: 0.7037540674209595, Final Batch Loss: 0.4230632185935974\n",
      "Epoch 897, Loss: 0.7372075915336609, Final Batch Loss: 0.3816920518875122\n",
      "Epoch 898, Loss: 0.6193104684352875, Final Batch Loss: 0.3622410297393799\n",
      "Epoch 899, Loss: 0.7445017695426941, Final Batch Loss: 0.3979261815547943\n",
      "Epoch 900, Loss: 0.6813848614692688, Final Batch Loss: 0.3735893964767456\n",
      "Epoch 901, Loss: 0.5955479443073273, Final Batch Loss: 0.26197656989097595\n",
      "Epoch 902, Loss: 0.7420709133148193, Final Batch Loss: 0.3722081184387207\n",
      "Epoch 903, Loss: 0.7060939371585846, Final Batch Loss: 0.3911057114601135\n",
      "Epoch 904, Loss: 0.6876765191555023, Final Batch Loss: 0.41126611828804016\n",
      "Epoch 905, Loss: 0.729501485824585, Final Batch Loss: 0.36646100878715515\n",
      "Epoch 906, Loss: 0.65734001994133, Final Batch Loss: 0.2942422032356262\n",
      "Epoch 907, Loss: 0.8230204284191132, Final Batch Loss: 0.5160935521125793\n",
      "Epoch 908, Loss: 0.5431081801652908, Final Batch Loss: 0.24698878824710846\n",
      "Epoch 909, Loss: 0.7546496093273163, Final Batch Loss: 0.44094598293304443\n",
      "Epoch 910, Loss: 0.6505201458930969, Final Batch Loss: 0.2863876521587372\n",
      "Epoch 911, Loss: 0.6375553607940674, Final Batch Loss: 0.3322634994983673\n",
      "Epoch 912, Loss: 0.6126790940761566, Final Batch Loss: 0.3185274600982666\n",
      "Epoch 913, Loss: 0.5985076129436493, Final Batch Loss: 0.29270902276039124\n",
      "Epoch 914, Loss: 0.5937339663505554, Final Batch Loss: 0.2625829577445984\n",
      "Epoch 915, Loss: 0.6387292444705963, Final Batch Loss: 0.2927379012107849\n",
      "Epoch 916, Loss: 0.6928386390209198, Final Batch Loss: 0.39472970366477966\n",
      "Epoch 917, Loss: 0.5798638164997101, Final Batch Loss: 0.27641910314559937\n",
      "Epoch 918, Loss: 0.5940157473087311, Final Batch Loss: 0.27684277296066284\n",
      "Epoch 919, Loss: 0.7017680704593658, Final Batch Loss: 0.36266693472862244\n",
      "Epoch 920, Loss: 0.6389258801937103, Final Batch Loss: 0.31283140182495117\n",
      "Epoch 921, Loss: 0.6170562505722046, Final Batch Loss: 0.2955544590950012\n",
      "Epoch 922, Loss: 0.7069139182567596, Final Batch Loss: 0.37068819999694824\n",
      "Epoch 923, Loss: 0.6324509084224701, Final Batch Loss: 0.2835799753665924\n",
      "Epoch 924, Loss: 0.6496411263942719, Final Batch Loss: 0.36208221316337585\n",
      "Epoch 925, Loss: 0.6689987778663635, Final Batch Loss: 0.3488751947879791\n",
      "Epoch 926, Loss: 0.6734412610530853, Final Batch Loss: 0.30683350563049316\n",
      "Epoch 927, Loss: 0.6595251858234406, Final Batch Loss: 0.35008683800697327\n",
      "Epoch 928, Loss: 0.5989750623703003, Final Batch Loss: 0.263897567987442\n",
      "Epoch 929, Loss: 0.5787210762500763, Final Batch Loss: 0.26037824153900146\n",
      "Epoch 930, Loss: 0.7651247084140778, Final Batch Loss: 0.3767973780632019\n",
      "Epoch 931, Loss: 0.7358009815216064, Final Batch Loss: 0.3587265610694885\n",
      "Epoch 932, Loss: 0.6063894927501678, Final Batch Loss: 0.29316747188568115\n",
      "Epoch 933, Loss: 0.703652024269104, Final Batch Loss: 0.3788358271121979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 934, Loss: 0.7849903702735901, Final Batch Loss: 0.43819427490234375\n",
      "Epoch 935, Loss: 0.5856075137853622, Final Batch Loss: 0.24045582115650177\n",
      "Epoch 936, Loss: 0.6710776686668396, Final Batch Loss: 0.3346376419067383\n",
      "Epoch 937, Loss: 0.7254503965377808, Final Batch Loss: 0.36425742506980896\n",
      "Epoch 938, Loss: 0.7293844223022461, Final Batch Loss: 0.3946298658847809\n",
      "Epoch 939, Loss: 0.6363787353038788, Final Batch Loss: 0.3358936905860901\n",
      "Epoch 940, Loss: 0.6397497951984406, Final Batch Loss: 0.26076391339302063\n",
      "Epoch 941, Loss: 0.6163944602012634, Final Batch Loss: 0.33307507634162903\n",
      "Epoch 942, Loss: 0.696717232465744, Final Batch Loss: 0.3939422369003296\n",
      "Epoch 943, Loss: 0.5190802812576294, Final Batch Loss: 0.20868340134620667\n",
      "Epoch 944, Loss: 0.6291146278381348, Final Batch Loss: 0.31835412979125977\n",
      "Epoch 945, Loss: 0.5810599029064178, Final Batch Loss: 0.24668318033218384\n",
      "Epoch 946, Loss: 0.6551522314548492, Final Batch Loss: 0.34716716408729553\n",
      "Epoch 947, Loss: 0.5805150270462036, Final Batch Loss: 0.26979464292526245\n",
      "Epoch 948, Loss: 0.6092286109924316, Final Batch Loss: 0.2607830762863159\n",
      "Epoch 949, Loss: 0.6439992487430573, Final Batch Loss: 0.27040696144104004\n",
      "Epoch 950, Loss: 0.5492734313011169, Final Batch Loss: 0.21779882907867432\n",
      "Epoch 951, Loss: 0.6581094861030579, Final Batch Loss: 0.3443915843963623\n",
      "Epoch 952, Loss: 0.5722442269325256, Final Batch Loss: 0.2763814330101013\n",
      "Epoch 953, Loss: 0.5856361389160156, Final Batch Loss: 0.2918176054954529\n",
      "Epoch 954, Loss: 0.6841558516025543, Final Batch Loss: 0.370612770318985\n",
      "Epoch 955, Loss: 0.7900882065296173, Final Batch Loss: 0.46122926473617554\n",
      "Epoch 956, Loss: 0.5647060871124268, Final Batch Loss: 0.24612566828727722\n",
      "Epoch 957, Loss: 0.6197706758975983, Final Batch Loss: 0.29696720838546753\n",
      "Epoch 958, Loss: 0.6805002391338348, Final Batch Loss: 0.2940952479839325\n",
      "Epoch 959, Loss: 0.7231584191322327, Final Batch Loss: 0.3945256471633911\n",
      "Epoch 960, Loss: 0.6340590417385101, Final Batch Loss: 0.32142454385757446\n",
      "Epoch 961, Loss: 0.597457230091095, Final Batch Loss: 0.2640686631202698\n",
      "Epoch 962, Loss: 0.6273523569107056, Final Batch Loss: 0.2679238021373749\n",
      "Epoch 963, Loss: 0.5808078050613403, Final Batch Loss: 0.30417385697364807\n",
      "Epoch 964, Loss: 0.5832592844963074, Final Batch Loss: 0.31506970524787903\n",
      "Epoch 965, Loss: 0.5837145447731018, Final Batch Loss: 0.25088179111480713\n",
      "Epoch 966, Loss: 0.6242610514163971, Final Batch Loss: 0.2697546184062958\n",
      "Epoch 967, Loss: 0.5532925724983215, Final Batch Loss: 0.23733025789260864\n",
      "Epoch 968, Loss: 0.7276685237884521, Final Batch Loss: 0.4851471185684204\n",
      "Epoch 969, Loss: 0.7087929248809814, Final Batch Loss: 0.3548576831817627\n",
      "Epoch 970, Loss: 0.6518775522708893, Final Batch Loss: 0.2746899724006653\n",
      "Epoch 971, Loss: 0.6395325362682343, Final Batch Loss: 0.33380192518234253\n",
      "Epoch 972, Loss: 0.6442252695560455, Final Batch Loss: 0.3491813540458679\n",
      "Epoch 973, Loss: 0.6239006817340851, Final Batch Loss: 0.3321283757686615\n",
      "Epoch 974, Loss: 0.6631959676742554, Final Batch Loss: 0.3335992991924286\n",
      "Epoch 975, Loss: 0.6213833689689636, Final Batch Loss: 0.27334102988243103\n",
      "Epoch 976, Loss: 0.6054888665676117, Final Batch Loss: 0.26068586111068726\n",
      "Epoch 977, Loss: 0.6515161097049713, Final Batch Loss: 0.2962806522846222\n",
      "Epoch 978, Loss: 0.5245231837034225, Final Batch Loss: 0.22207550704479218\n",
      "Epoch 979, Loss: 0.558281198143959, Final Batch Loss: 0.24058841168880463\n",
      "Epoch 980, Loss: 0.5505727380514145, Final Batch Loss: 0.2323562353849411\n",
      "Epoch 981, Loss: 0.6547872424125671, Final Batch Loss: 0.34047359228134155\n",
      "Epoch 982, Loss: 0.5432716012001038, Final Batch Loss: 0.25869423151016235\n",
      "Epoch 983, Loss: 0.7189668118953705, Final Batch Loss: 0.43974336981773376\n",
      "Epoch 984, Loss: 0.525331974029541, Final Batch Loss: 0.22801539301872253\n",
      "Epoch 985, Loss: 0.6088282018899918, Final Batch Loss: 0.24559222161769867\n",
      "Epoch 986, Loss: 0.5776946246623993, Final Batch Loss: 0.23102280497550964\n",
      "Epoch 987, Loss: 0.5797402262687683, Final Batch Loss: 0.3067220449447632\n",
      "Epoch 988, Loss: 0.639649361371994, Final Batch Loss: 0.3467997610569\n",
      "Epoch 989, Loss: 0.6267419755458832, Final Batch Loss: 0.32614657282829285\n",
      "Epoch 990, Loss: 0.6436350047588348, Final Batch Loss: 0.32311147451400757\n",
      "Epoch 991, Loss: 0.5654766857624054, Final Batch Loss: 0.2622973918914795\n",
      "Epoch 992, Loss: 0.522066742181778, Final Batch Loss: 0.2578631043434143\n",
      "Epoch 993, Loss: 0.6639821827411652, Final Batch Loss: 0.3766348361968994\n",
      "Epoch 994, Loss: 0.5969801843166351, Final Batch Loss: 0.3052557110786438\n",
      "Epoch 995, Loss: 0.716873288154602, Final Batch Loss: 0.4113921821117401\n",
      "Epoch 996, Loss: 0.6579332649707794, Final Batch Loss: 0.3347078561782837\n",
      "Epoch 997, Loss: 0.5916599333286285, Final Batch Loss: 0.2926841676235199\n",
      "Epoch 998, Loss: 0.5018429160118103, Final Batch Loss: 0.18868115544319153\n",
      "Epoch 999, Loss: 0.609835684299469, Final Batch Loss: 0.30608782172203064\n",
      "Epoch 1000, Loss: 0.7776862382888794, Final Batch Loss: 0.4752466082572937\n",
      "Epoch 1001, Loss: 0.5305851399898529, Final Batch Loss: 0.2635180354118347\n",
      "Epoch 1002, Loss: 0.687200129032135, Final Batch Loss: 0.41212570667266846\n",
      "Epoch 1003, Loss: 0.5500601828098297, Final Batch Loss: 0.2883981764316559\n",
      "Epoch 1004, Loss: 0.5319137573242188, Final Batch Loss: 0.24182087182998657\n",
      "Epoch 1005, Loss: 0.5917122066020966, Final Batch Loss: 0.25486990809440613\n",
      "Epoch 1006, Loss: 0.4953016936779022, Final Batch Loss: 0.19721972942352295\n",
      "Epoch 1007, Loss: 0.6448560953140259, Final Batch Loss: 0.3363184630870819\n",
      "Epoch 1008, Loss: 0.5973154604434967, Final Batch Loss: 0.3204030990600586\n",
      "Epoch 1009, Loss: 0.6673432886600494, Final Batch Loss: 0.3629540205001831\n",
      "Epoch 1010, Loss: 0.5124725997447968, Final Batch Loss: 0.2617144286632538\n",
      "Epoch 1011, Loss: 0.6904305219650269, Final Batch Loss: 0.40127864480018616\n",
      "Epoch 1012, Loss: 0.6193101704120636, Final Batch Loss: 0.3043147921562195\n",
      "Epoch 1013, Loss: 0.6170751452445984, Final Batch Loss: 0.32483071088790894\n",
      "Epoch 1014, Loss: 0.6009770929813385, Final Batch Loss: 0.27640169858932495\n",
      "Epoch 1015, Loss: 0.5783108472824097, Final Batch Loss: 0.31200936436653137\n",
      "Epoch 1016, Loss: 0.6911883056163788, Final Batch Loss: 0.34005236625671387\n",
      "Epoch 1017, Loss: 0.6256916224956512, Final Batch Loss: 0.31244349479675293\n",
      "Epoch 1018, Loss: 0.5313393473625183, Final Batch Loss: 0.2769306004047394\n",
      "Epoch 1019, Loss: 0.6892307996749878, Final Batch Loss: 0.35942164063453674\n",
      "Epoch 1020, Loss: 0.5915713310241699, Final Batch Loss: 0.2900031805038452\n",
      "Epoch 1021, Loss: 0.579259842634201, Final Batch Loss: 0.2632248103618622\n",
      "Epoch 1022, Loss: 0.5451536923646927, Final Batch Loss: 0.23870714008808136\n",
      "Epoch 1023, Loss: 0.49054351449012756, Final Batch Loss: 0.1818431317806244\n",
      "Epoch 1024, Loss: 0.7784568667411804, Final Batch Loss: 0.3320617079734802\n",
      "Epoch 1025, Loss: 0.6393761336803436, Final Batch Loss: 0.3013153076171875\n",
      "Epoch 1026, Loss: 0.6776566207408905, Final Batch Loss: 0.3754771649837494\n",
      "Epoch 1027, Loss: 0.555875837802887, Final Batch Loss: 0.29653894901275635\n",
      "Epoch 1028, Loss: 0.7042500674724579, Final Batch Loss: 0.32357218861579895\n",
      "Epoch 1029, Loss: 0.639521062374115, Final Batch Loss: 0.35253214836120605\n",
      "Epoch 1030, Loss: 0.5824696123600006, Final Batch Loss: 0.28161847591400146\n",
      "Epoch 1031, Loss: 0.5503504276275635, Final Batch Loss: 0.25519829988479614\n",
      "Epoch 1032, Loss: 0.5355698019266129, Final Batch Loss: 0.22607381641864777\n",
      "Epoch 1033, Loss: 0.500578910112381, Final Batch Loss: 0.22198829054832458\n",
      "Epoch 1034, Loss: 0.5819312632083893, Final Batch Loss: 0.29010674357414246\n",
      "Epoch 1035, Loss: 0.6502447426319122, Final Batch Loss: 0.35636577010154724\n",
      "Epoch 1036, Loss: 0.7036339342594147, Final Batch Loss: 0.3767877221107483\n",
      "Epoch 1037, Loss: 0.5637791752815247, Final Batch Loss: 0.24808838963508606\n",
      "Epoch 1038, Loss: 0.6726229786872864, Final Batch Loss: 0.3867993652820587\n",
      "Epoch 1039, Loss: 0.48857270181179047, Final Batch Loss: 0.2305448204278946\n",
      "Epoch 1040, Loss: 0.5860162675380707, Final Batch Loss: 0.2951571047306061\n",
      "Epoch 1041, Loss: 0.5242917537689209, Final Batch Loss: 0.25103244185447693\n",
      "Epoch 1042, Loss: 0.6338589787483215, Final Batch Loss: 0.34990862011909485\n",
      "Epoch 1043, Loss: 0.5432029664516449, Final Batch Loss: 0.259667307138443\n",
      "Epoch 1044, Loss: 0.5358117520809174, Final Batch Loss: 0.2503281831741333\n",
      "Epoch 1045, Loss: 0.6872167885303497, Final Batch Loss: 0.4087271988391876\n",
      "Epoch 1046, Loss: 0.5678304433822632, Final Batch Loss: 0.24794402718544006\n",
      "Epoch 1047, Loss: 0.6119437217712402, Final Batch Loss: 0.3211175799369812\n",
      "Epoch 1048, Loss: 0.720319539308548, Final Batch Loss: 0.34487536549568176\n",
      "Epoch 1049, Loss: 0.5506110489368439, Final Batch Loss: 0.2789420783519745\n",
      "Epoch 1050, Loss: 0.499069482088089, Final Batch Loss: 0.2085111141204834\n",
      "Epoch 1051, Loss: 0.5680990666151047, Final Batch Loss: 0.2376595288515091\n",
      "Epoch 1052, Loss: 0.5590321719646454, Final Batch Loss: 0.2688400149345398\n",
      "Epoch 1053, Loss: 0.5674807727336884, Final Batch Loss: 0.3044436573982239\n",
      "Epoch 1054, Loss: 0.6668624877929688, Final Batch Loss: 0.32329973578453064\n",
      "Epoch 1055, Loss: 0.49638889729976654, Final Batch Loss: 0.23677416145801544\n",
      "Epoch 1056, Loss: 0.6461742073297501, Final Batch Loss: 0.40032702684402466\n",
      "Epoch 1057, Loss: 0.5460761487483978, Final Batch Loss: 0.25973793864250183\n",
      "Epoch 1058, Loss: 0.8068659901618958, Final Batch Loss: 0.5395442247390747\n",
      "Epoch 1059, Loss: 0.7012353837490082, Final Batch Loss: 0.37370046973228455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1060, Loss: 0.542625367641449, Final Batch Loss: 0.2910861670970917\n",
      "Epoch 1061, Loss: 0.5775213688611984, Final Batch Loss: 0.24481604993343353\n",
      "Epoch 1062, Loss: 0.6372721791267395, Final Batch Loss: 0.3353279232978821\n",
      "Epoch 1063, Loss: 0.6291782855987549, Final Batch Loss: 0.33111414313316345\n",
      "Epoch 1064, Loss: 0.5708066523075104, Final Batch Loss: 0.28984788060188293\n",
      "Epoch 1065, Loss: 0.5793061554431915, Final Batch Loss: 0.306458443403244\n",
      "Epoch 1066, Loss: 0.4699651151895523, Final Batch Loss: 0.15077967941761017\n",
      "Epoch 1067, Loss: 0.6253163516521454, Final Batch Loss: 0.3554241359233856\n",
      "Epoch 1068, Loss: 0.5728164315223694, Final Batch Loss: 0.3174850642681122\n",
      "Epoch 1069, Loss: 0.6005687117576599, Final Batch Loss: 0.27815404534339905\n",
      "Epoch 1070, Loss: 0.4768024981021881, Final Batch Loss: 0.19112327694892883\n",
      "Epoch 1071, Loss: 0.6141839921474457, Final Batch Loss: 0.35082316398620605\n",
      "Epoch 1072, Loss: 0.5352602303028107, Final Batch Loss: 0.23882773518562317\n",
      "Epoch 1073, Loss: 0.4501255005598068, Final Batch Loss: 0.19154153764247894\n",
      "Epoch 1074, Loss: 0.5318106859922409, Final Batch Loss: 0.22571708261966705\n",
      "Epoch 1075, Loss: 0.48483605682849884, Final Batch Loss: 0.2002667933702469\n",
      "Epoch 1076, Loss: 0.5533885657787323, Final Batch Loss: 0.2820570766925812\n",
      "Epoch 1077, Loss: 0.5983374714851379, Final Batch Loss: 0.3394150137901306\n",
      "Epoch 1078, Loss: 0.629033237695694, Final Batch Loss: 0.35500723123550415\n",
      "Epoch 1079, Loss: 0.6029346287250519, Final Batch Loss: 0.3365217447280884\n",
      "Epoch 1080, Loss: 0.6688559055328369, Final Batch Loss: 0.43287986516952515\n",
      "Epoch 1081, Loss: 0.5509756505489349, Final Batch Loss: 0.263285368680954\n",
      "Epoch 1082, Loss: 0.6828380823135376, Final Batch Loss: 0.4000887870788574\n",
      "Epoch 1083, Loss: 0.49488912522792816, Final Batch Loss: 0.2354399412870407\n",
      "Epoch 1084, Loss: 0.6165612637996674, Final Batch Loss: 0.3322739899158478\n",
      "Epoch 1085, Loss: 0.5716909319162369, Final Batch Loss: 0.24016772210597992\n",
      "Epoch 1086, Loss: 0.565635085105896, Final Batch Loss: 0.25515279173851013\n",
      "Epoch 1087, Loss: 0.6418758630752563, Final Batch Loss: 0.3657762408256531\n",
      "Epoch 1088, Loss: 0.5025542080402374, Final Batch Loss: 0.20231619477272034\n",
      "Epoch 1089, Loss: 0.5638789534568787, Final Batch Loss: 0.2711659371852875\n",
      "Epoch 1090, Loss: 0.5918236076831818, Final Batch Loss: 0.30188480019569397\n",
      "Epoch 1091, Loss: 0.6634498238563538, Final Batch Loss: 0.3717709183692932\n",
      "Epoch 1092, Loss: 0.5788019895553589, Final Batch Loss: 0.2740083634853363\n",
      "Epoch 1093, Loss: 0.6912158727645874, Final Batch Loss: 0.40730783343315125\n",
      "Epoch 1094, Loss: 0.6326457858085632, Final Batch Loss: 0.32769402861595154\n",
      "Epoch 1095, Loss: 0.5066207498311996, Final Batch Loss: 0.21682222187519073\n",
      "Epoch 1096, Loss: 0.7112019062042236, Final Batch Loss: 0.35816001892089844\n",
      "Epoch 1097, Loss: 0.5808887183666229, Final Batch Loss: 0.25224193930625916\n",
      "Epoch 1098, Loss: 0.6192574203014374, Final Batch Loss: 0.35613301396369934\n",
      "Epoch 1099, Loss: 0.6686208248138428, Final Batch Loss: 0.33181145787239075\n",
      "Epoch 1100, Loss: 0.6516854465007782, Final Batch Loss: 0.38628676533699036\n",
      "Epoch 1101, Loss: 0.7495015859603882, Final Batch Loss: 0.4818781912326813\n",
      "Epoch 1102, Loss: 0.5019042044878006, Final Batch Loss: 0.20085616409778595\n",
      "Epoch 1103, Loss: 0.5837684273719788, Final Batch Loss: 0.3019416034221649\n",
      "Epoch 1104, Loss: 0.6362357139587402, Final Batch Loss: 0.3238227963447571\n",
      "Epoch 1105, Loss: 0.5726355314254761, Final Batch Loss: 0.25967252254486084\n",
      "Epoch 1106, Loss: 0.5368999540805817, Final Batch Loss: 0.25554484128952026\n",
      "Epoch 1107, Loss: 0.6463943123817444, Final Batch Loss: 0.36866483092308044\n",
      "Epoch 1108, Loss: 0.6010277271270752, Final Batch Loss: 0.33031851053237915\n",
      "Epoch 1109, Loss: 0.530233234167099, Final Batch Loss: 0.22802501916885376\n",
      "Epoch 1110, Loss: 0.5872257649898529, Final Batch Loss: 0.3110100328922272\n",
      "Epoch 1111, Loss: 0.5640437006950378, Final Batch Loss: 0.24877727031707764\n",
      "Epoch 1112, Loss: 0.5127535462379456, Final Batch Loss: 0.21214720606803894\n",
      "Epoch 1113, Loss: 0.6920973062515259, Final Batch Loss: 0.4396345019340515\n",
      "Epoch 1114, Loss: 0.5371025204658508, Final Batch Loss: 0.25151312351226807\n",
      "Epoch 1115, Loss: 0.48046740889549255, Final Batch Loss: 0.2015637755393982\n",
      "Epoch 1116, Loss: 0.6361375749111176, Final Batch Loss: 0.3105947971343994\n",
      "Epoch 1117, Loss: 0.5314678251743317, Final Batch Loss: 0.2834101617336273\n",
      "Epoch 1118, Loss: 0.5314721465110779, Final Batch Loss: 0.2350827157497406\n",
      "Epoch 1119, Loss: 0.49061740934848785, Final Batch Loss: 0.21474377810955048\n",
      "Epoch 1120, Loss: 0.49215462803840637, Final Batch Loss: 0.18207114934921265\n",
      "Epoch 1121, Loss: 0.5471897721290588, Final Batch Loss: 0.263416588306427\n",
      "Epoch 1122, Loss: 0.4982988238334656, Final Batch Loss: 0.24450206756591797\n",
      "Epoch 1123, Loss: 0.543109729886055, Final Batch Loss: 0.21803997457027435\n",
      "Epoch 1124, Loss: 0.5845344364643097, Final Batch Loss: 0.3399920165538788\n",
      "Epoch 1125, Loss: 0.5649527013301849, Final Batch Loss: 0.3047177195549011\n",
      "Epoch 1126, Loss: 0.5436223447322845, Final Batch Loss: 0.22767627239227295\n",
      "Epoch 1127, Loss: 0.602797120809555, Final Batch Loss: 0.29778867959976196\n",
      "Epoch 1128, Loss: 0.48640669882297516, Final Batch Loss: 0.20134352147579193\n",
      "Epoch 1129, Loss: 0.5366053879261017, Final Batch Loss: 0.2933768928050995\n",
      "Epoch 1130, Loss: 0.570937991142273, Final Batch Loss: 0.3046758472919464\n",
      "Epoch 1131, Loss: 0.5325292944908142, Final Batch Loss: 0.251319020986557\n",
      "Epoch 1132, Loss: 0.639362633228302, Final Batch Loss: 0.34325525164604187\n",
      "Epoch 1133, Loss: 0.6153244972229004, Final Batch Loss: 0.3041762411594391\n",
      "Epoch 1134, Loss: 0.4852469116449356, Final Batch Loss: 0.21977432072162628\n",
      "Epoch 1135, Loss: 0.49621883034706116, Final Batch Loss: 0.21141216158866882\n",
      "Epoch 1136, Loss: 0.4965823292732239, Final Batch Loss: 0.22972974181175232\n",
      "Epoch 1137, Loss: 0.6524752974510193, Final Batch Loss: 0.42513856291770935\n",
      "Epoch 1138, Loss: 0.569178581237793, Final Batch Loss: 0.28593385219573975\n",
      "Epoch 1139, Loss: 0.4784640371799469, Final Batch Loss: 0.2549469769001007\n",
      "Epoch 1140, Loss: 0.5384765416383743, Final Batch Loss: 0.29358476400375366\n",
      "Epoch 1141, Loss: 0.5096599459648132, Final Batch Loss: 0.2517455518245697\n",
      "Epoch 1142, Loss: 0.5607208609580994, Final Batch Loss: 0.2885269820690155\n",
      "Epoch 1143, Loss: 0.47700342535972595, Final Batch Loss: 0.19489461183547974\n",
      "Epoch 1144, Loss: 0.6161328852176666, Final Batch Loss: 0.3139617145061493\n",
      "Epoch 1145, Loss: 0.4594888985157013, Final Batch Loss: 0.1506938636302948\n",
      "Epoch 1146, Loss: 0.6128525137901306, Final Batch Loss: 0.35759878158569336\n",
      "Epoch 1147, Loss: 0.5371198356151581, Final Batch Loss: 0.276572048664093\n",
      "Epoch 1148, Loss: 0.4730810225009918, Final Batch Loss: 0.21752363443374634\n",
      "Epoch 1149, Loss: 0.47999057173728943, Final Batch Loss: 0.19949615001678467\n",
      "Epoch 1150, Loss: 0.5411640405654907, Final Batch Loss: 0.28092750906944275\n",
      "Epoch 1151, Loss: 0.45440855622291565, Final Batch Loss: 0.16101858019828796\n",
      "Epoch 1152, Loss: 0.4782177060842514, Final Batch Loss: 0.23075327277183533\n",
      "Epoch 1153, Loss: 0.45505450665950775, Final Batch Loss: 0.22265072166919708\n",
      "Epoch 1154, Loss: 0.5731970369815826, Final Batch Loss: 0.292233407497406\n",
      "Epoch 1155, Loss: 0.4904485195875168, Final Batch Loss: 0.22678963840007782\n",
      "Epoch 1156, Loss: 0.6006659865379333, Final Batch Loss: 0.32309332489967346\n",
      "Epoch 1157, Loss: 0.5898719877004623, Final Batch Loss: 0.3428434133529663\n",
      "Epoch 1158, Loss: 0.5966311395168304, Final Batch Loss: 0.3414798080921173\n",
      "Epoch 1159, Loss: 0.44757577776908875, Final Batch Loss: 0.21632273495197296\n",
      "Epoch 1160, Loss: 0.5529097616672516, Final Batch Loss: 0.28894728422164917\n",
      "Epoch 1161, Loss: 0.5393460690975189, Final Batch Loss: 0.23257139325141907\n",
      "Epoch 1162, Loss: 0.5963389873504639, Final Batch Loss: 0.3071875274181366\n",
      "Epoch 1163, Loss: 0.5020315498113632, Final Batch Loss: 0.22146989405155182\n",
      "Epoch 1164, Loss: 0.6757243275642395, Final Batch Loss: 0.4246029257774353\n",
      "Epoch 1165, Loss: 0.468406617641449, Final Batch Loss: 0.20493260025978088\n",
      "Epoch 1166, Loss: 0.5204878151416779, Final Batch Loss: 0.25316542387008667\n",
      "Epoch 1167, Loss: 0.5155766606330872, Final Batch Loss: 0.25160831212997437\n",
      "Epoch 1168, Loss: 0.5220728516578674, Final Batch Loss: 0.2510702908039093\n",
      "Epoch 1169, Loss: 0.7135331928730011, Final Batch Loss: 0.4372883439064026\n",
      "Epoch 1170, Loss: 0.6079148650169373, Final Batch Loss: 0.3644025921821594\n",
      "Epoch 1171, Loss: 0.6205203831195831, Final Batch Loss: 0.3298848271369934\n",
      "Epoch 1172, Loss: 0.5395255088806152, Final Batch Loss: 0.2788071632385254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1173, Loss: 0.5968198180198669, Final Batch Loss: 0.3157169818878174\n",
      "Epoch 1174, Loss: 0.5051396042108536, Final Batch Loss: 0.27275124192237854\n",
      "Epoch 1175, Loss: 0.7525443136692047, Final Batch Loss: 0.49725082516670227\n",
      "Epoch 1176, Loss: 0.46402959525585175, Final Batch Loss: 0.2271878868341446\n",
      "Epoch 1177, Loss: 0.5304336845874786, Final Batch Loss: 0.2709602117538452\n",
      "Epoch 1178, Loss: 0.4525996148586273, Final Batch Loss: 0.2064007818698883\n",
      "Epoch 1179, Loss: 0.5143804848194122, Final Batch Loss: 0.2298576533794403\n",
      "Epoch 1180, Loss: 0.4692511558532715, Final Batch Loss: 0.21744221448898315\n",
      "Epoch 1181, Loss: 0.4905849099159241, Final Batch Loss: 0.22682422399520874\n",
      "Epoch 1182, Loss: 0.579164445400238, Final Batch Loss: 0.30755919218063354\n",
      "Epoch 1183, Loss: 0.5808131992816925, Final Batch Loss: 0.32997387647628784\n",
      "Epoch 1184, Loss: 0.46846388280391693, Final Batch Loss: 0.2020406872034073\n",
      "Epoch 1185, Loss: 0.4831634610891342, Final Batch Loss: 0.22249479591846466\n",
      "Epoch 1186, Loss: 0.4761423319578171, Final Batch Loss: 0.22095225751399994\n",
      "Epoch 1187, Loss: 0.5111512988805771, Final Batch Loss: 0.1789674609899521\n",
      "Epoch 1188, Loss: 0.5542691797018051, Final Batch Loss: 0.188922718167305\n",
      "Epoch 1189, Loss: 0.45266465842723846, Final Batch Loss: 0.16670875251293182\n",
      "Epoch 1190, Loss: 0.513222262263298, Final Batch Loss: 0.2684391438961029\n",
      "Epoch 1191, Loss: 0.5381504893302917, Final Batch Loss: 0.24047648906707764\n",
      "Epoch 1192, Loss: 0.6078649759292603, Final Batch Loss: 0.33160772919654846\n",
      "Epoch 1193, Loss: 0.41312873363494873, Final Batch Loss: 0.1415625512599945\n",
      "Epoch 1194, Loss: 0.42350561916828156, Final Batch Loss: 0.16350631415843964\n",
      "Epoch 1195, Loss: 0.6105579435825348, Final Batch Loss: 0.37655138969421387\n",
      "Epoch 1196, Loss: 0.5088219046592712, Final Batch Loss: 0.26225745677948\n",
      "Epoch 1197, Loss: 0.46813294291496277, Final Batch Loss: 0.22653381526470184\n",
      "Epoch 1198, Loss: 0.6756796538829803, Final Batch Loss: 0.45059773325920105\n",
      "Epoch 1199, Loss: 0.4764993041753769, Final Batch Loss: 0.23746392130851746\n",
      "Epoch 1200, Loss: 0.5386488139629364, Final Batch Loss: 0.26342955231666565\n",
      "Epoch 1201, Loss: 0.386850044131279, Final Batch Loss: 0.15697869658470154\n",
      "Epoch 1202, Loss: 0.470865860581398, Final Batch Loss: 0.2125117927789688\n",
      "Epoch 1203, Loss: 0.4736461341381073, Final Batch Loss: 0.1866511106491089\n",
      "Epoch 1204, Loss: 0.48088450729846954, Final Batch Loss: 0.23546430468559265\n",
      "Epoch 1205, Loss: 0.507679671049118, Final Batch Loss: 0.2824762463569641\n",
      "Epoch 1206, Loss: 0.4701170474290848, Final Batch Loss: 0.18540899455547333\n",
      "Epoch 1207, Loss: 0.662351667881012, Final Batch Loss: 0.2621561884880066\n",
      "Epoch 1208, Loss: 0.6085289716720581, Final Batch Loss: 0.3419220447540283\n",
      "Epoch 1209, Loss: 0.5176685452461243, Final Batch Loss: 0.2640199661254883\n",
      "Epoch 1210, Loss: 0.4158719480037689, Final Batch Loss: 0.159184992313385\n",
      "Epoch 1211, Loss: 0.5782454907894135, Final Batch Loss: 0.28845155239105225\n",
      "Epoch 1212, Loss: 0.560567319393158, Final Batch Loss: 0.29148679971694946\n",
      "Epoch 1213, Loss: 0.5314839482307434, Final Batch Loss: 0.25672250986099243\n",
      "Epoch 1214, Loss: 0.44518667459487915, Final Batch Loss: 0.1911875605583191\n",
      "Epoch 1215, Loss: 0.5840103179216385, Final Batch Loss: 0.33772918581962585\n",
      "Epoch 1216, Loss: 0.5226528197526932, Final Batch Loss: 0.31168264150619507\n",
      "Epoch 1217, Loss: 0.4967088848352432, Final Batch Loss: 0.2874175012111664\n",
      "Epoch 1218, Loss: 0.4624412804841995, Final Batch Loss: 0.19788996875286102\n",
      "Epoch 1219, Loss: 0.5065961331129074, Final Batch Loss: 0.23713193833827972\n",
      "Epoch 1220, Loss: 0.5209293961524963, Final Batch Loss: 0.292436420917511\n",
      "Epoch 1221, Loss: 0.48743169009685516, Final Batch Loss: 0.19364313781261444\n",
      "Epoch 1222, Loss: 0.5040575116872787, Final Batch Loss: 0.26093432307243347\n",
      "Epoch 1223, Loss: 0.49950915575027466, Final Batch Loss: 0.24218535423278809\n",
      "Epoch 1224, Loss: 0.5319135189056396, Final Batch Loss: 0.26540908217430115\n",
      "Epoch 1225, Loss: 0.4469445049762726, Final Batch Loss: 0.18634265661239624\n",
      "Epoch 1226, Loss: 0.5348258167505264, Final Batch Loss: 0.3074367046356201\n",
      "Epoch 1227, Loss: 0.5112378597259521, Final Batch Loss: 0.28001952171325684\n",
      "Epoch 1228, Loss: 0.6133613735437393, Final Batch Loss: 0.36839497089385986\n",
      "Epoch 1229, Loss: 0.7121720314025879, Final Batch Loss: 0.438262015581131\n",
      "Epoch 1230, Loss: 0.5210407972335815, Final Batch Loss: 0.26376667618751526\n",
      "Epoch 1231, Loss: 0.6270236521959305, Final Batch Loss: 0.4440035820007324\n",
      "Epoch 1232, Loss: 0.4910043627023697, Final Batch Loss: 0.23942629992961884\n",
      "Epoch 1233, Loss: 0.5591904073953629, Final Batch Loss: 0.36936450004577637\n",
      "Epoch 1234, Loss: 0.4869513511657715, Final Batch Loss: 0.24803058803081512\n",
      "Epoch 1235, Loss: 0.6064517796039581, Final Batch Loss: 0.3549520969390869\n",
      "Epoch 1236, Loss: 0.4101528823375702, Final Batch Loss: 0.21003206074237823\n",
      "Epoch 1237, Loss: 0.4532734602689743, Final Batch Loss: 0.20778246223926544\n",
      "Epoch 1238, Loss: 0.44460873305797577, Final Batch Loss: 0.19373248517513275\n",
      "Epoch 1239, Loss: 0.5293849855661392, Final Batch Loss: 0.238350048661232\n",
      "Epoch 1240, Loss: 0.5337317287921906, Final Batch Loss: 0.2759868800640106\n",
      "Epoch 1241, Loss: 0.548226609826088, Final Batch Loss: 0.31458282470703125\n",
      "Epoch 1242, Loss: 0.6353913396596909, Final Batch Loss: 0.3956981301307678\n",
      "Epoch 1243, Loss: 0.4608485996723175, Final Batch Loss: 0.20286831259727478\n",
      "Epoch 1244, Loss: 0.4833502024412155, Final Batch Loss: 0.26743000745773315\n",
      "Epoch 1245, Loss: 0.47124001383781433, Final Batch Loss: 0.25572067499160767\n",
      "Epoch 1246, Loss: 0.5814189910888672, Final Batch Loss: 0.27603453397750854\n",
      "Epoch 1247, Loss: 0.47750385105609894, Final Batch Loss: 0.22546415030956268\n",
      "Epoch 1248, Loss: 0.5425697267055511, Final Batch Loss: 0.2863307297229767\n",
      "Epoch 1249, Loss: 0.6910421252250671, Final Batch Loss: 0.46404406428337097\n",
      "Epoch 1250, Loss: 0.4339601993560791, Final Batch Loss: 0.21535001695156097\n",
      "Epoch 1251, Loss: 0.5512285828590393, Final Batch Loss: 0.29143819212913513\n",
      "Epoch 1252, Loss: 0.5791100561618805, Final Batch Loss: 0.3137142062187195\n",
      "Epoch 1253, Loss: 0.40000152587890625, Final Batch Loss: 0.16581441462039948\n",
      "Epoch 1254, Loss: 0.4195486903190613, Final Batch Loss: 0.16209393739700317\n",
      "Epoch 1255, Loss: 0.450903058052063, Final Batch Loss: 0.2128518670797348\n",
      "Epoch 1256, Loss: 0.5797476023435593, Final Batch Loss: 0.34538960456848145\n",
      "Epoch 1257, Loss: 0.562121570110321, Final Batch Loss: 0.3104245960712433\n",
      "Epoch 1258, Loss: 0.5625014454126358, Final Batch Loss: 0.35353487730026245\n",
      "Epoch 1259, Loss: 0.4949338138103485, Final Batch Loss: 0.22108864784240723\n",
      "Epoch 1260, Loss: 0.5568195581436157, Final Batch Loss: 0.2935972809791565\n",
      "Epoch 1261, Loss: 0.5700971931219101, Final Batch Loss: 0.33151310682296753\n",
      "Epoch 1262, Loss: 0.4026022404432297, Final Batch Loss: 0.15023423731327057\n",
      "Epoch 1263, Loss: 0.41502639651298523, Final Batch Loss: 0.16980324685573578\n",
      "Epoch 1264, Loss: 0.47826434671878815, Final Batch Loss: 0.2676416337490082\n",
      "Epoch 1265, Loss: 0.6297273486852646, Final Batch Loss: 0.4006844460964203\n",
      "Epoch 1266, Loss: 0.44516848027706146, Final Batch Loss: 0.23411300778388977\n",
      "Epoch 1267, Loss: 0.4538111686706543, Final Batch Loss: 0.20546583831310272\n",
      "Epoch 1268, Loss: 0.4693911522626877, Final Batch Loss: 0.2148555964231491\n",
      "Epoch 1269, Loss: 0.5487768352031708, Final Batch Loss: 0.2632453739643097\n",
      "Epoch 1270, Loss: 0.5571398735046387, Final Batch Loss: 0.2934505045413971\n",
      "Epoch 1271, Loss: 0.47548937797546387, Final Batch Loss: 0.24932628870010376\n",
      "Epoch 1272, Loss: 0.4190225899219513, Final Batch Loss: 0.19075554609298706\n",
      "Epoch 1273, Loss: 0.4187176525592804, Final Batch Loss: 0.17102599143981934\n",
      "Epoch 1274, Loss: 0.43346117436885834, Final Batch Loss: 0.1813247948884964\n",
      "Epoch 1275, Loss: 0.6164134442806244, Final Batch Loss: 0.3284289240837097\n",
      "Epoch 1276, Loss: 0.5109727382659912, Final Batch Loss: 0.2867082357406616\n",
      "Epoch 1277, Loss: 0.4595824182033539, Final Batch Loss: 0.23773406445980072\n",
      "Epoch 1278, Loss: 0.5024750530719757, Final Batch Loss: 0.21596184372901917\n",
      "Epoch 1279, Loss: 0.5663032233715057, Final Batch Loss: 0.30838578939437866\n",
      "Epoch 1280, Loss: 0.5991125702857971, Final Batch Loss: 0.3378239870071411\n",
      "Epoch 1281, Loss: 0.4222314804792404, Final Batch Loss: 0.17695733904838562\n",
      "Epoch 1282, Loss: 0.44495175778865814, Final Batch Loss: 0.269298791885376\n",
      "Epoch 1283, Loss: 0.48958079516887665, Final Batch Loss: 0.20963092148303986\n",
      "Epoch 1284, Loss: 0.41879265010356903, Final Batch Loss: 0.1745329201221466\n",
      "Epoch 1285, Loss: 0.4757796972990036, Final Batch Loss: 0.2363901436328888\n",
      "Epoch 1286, Loss: 0.4317403882741928, Final Batch Loss: 0.15094994008541107\n",
      "Epoch 1287, Loss: 0.4621364325284958, Final Batch Loss: 0.2020667940378189\n",
      "Epoch 1288, Loss: 0.5296993553638458, Final Batch Loss: 0.27469906210899353\n",
      "Epoch 1289, Loss: 0.5545074641704559, Final Batch Loss: 0.31650084257125854\n",
      "Epoch 1290, Loss: 0.5713915228843689, Final Batch Loss: 0.25227075815200806\n",
      "Epoch 1291, Loss: 0.34790392220020294, Final Batch Loss: 0.14425060153007507\n",
      "Epoch 1292, Loss: 0.7207128703594208, Final Batch Loss: 0.46348369121551514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1293, Loss: 0.5142138004302979, Final Batch Loss: 0.26365795731544495\n",
      "Epoch 1294, Loss: 0.39555831253528595, Final Batch Loss: 0.22896547615528107\n",
      "Epoch 1295, Loss: 0.6224778592586517, Final Batch Loss: 0.3800611197948456\n",
      "Epoch 1296, Loss: 0.5556412637233734, Final Batch Loss: 0.36709269881248474\n",
      "Epoch 1297, Loss: 0.5108286887407303, Final Batch Loss: 0.2805991470813751\n",
      "Epoch 1298, Loss: 0.5125318616628647, Final Batch Loss: 0.2456987053155899\n",
      "Epoch 1299, Loss: 0.4201042950153351, Final Batch Loss: 0.18553851544857025\n",
      "Epoch 1300, Loss: 0.5048450380563736, Final Batch Loss: 0.30844593048095703\n",
      "Epoch 1301, Loss: 0.47439026832580566, Final Batch Loss: 0.24335359036922455\n",
      "Epoch 1302, Loss: 0.68628790974617, Final Batch Loss: 0.47934266924858093\n",
      "Epoch 1303, Loss: 0.3795110434293747, Final Batch Loss: 0.1738658845424652\n",
      "Epoch 1304, Loss: 0.5308387875556946, Final Batch Loss: 0.26376721262931824\n",
      "Epoch 1305, Loss: 0.7318771779537201, Final Batch Loss: 0.4746319651603699\n",
      "Epoch 1306, Loss: 0.49317094683647156, Final Batch Loss: 0.2520909309387207\n",
      "Epoch 1307, Loss: 0.45010681450366974, Final Batch Loss: 0.2316293865442276\n",
      "Epoch 1308, Loss: 0.38110093772411346, Final Batch Loss: 0.1793816238641739\n",
      "Epoch 1309, Loss: 0.5242473632097244, Final Batch Loss: 0.28702422976493835\n",
      "Epoch 1310, Loss: 0.4720258414745331, Final Batch Loss: 0.2414921075105667\n",
      "Epoch 1311, Loss: 0.3956473171710968, Final Batch Loss: 0.153291255235672\n",
      "Epoch 1312, Loss: 0.4526735842227936, Final Batch Loss: 0.20982052385807037\n",
      "Epoch 1313, Loss: 0.4719308167695999, Final Batch Loss: 0.2625207304954529\n",
      "Epoch 1314, Loss: 0.4445950388908386, Final Batch Loss: 0.2146483063697815\n",
      "Epoch 1315, Loss: 0.48004572093486786, Final Batch Loss: 0.232618510723114\n",
      "Epoch 1316, Loss: 0.6022324115037918, Final Batch Loss: 0.4256499707698822\n",
      "Epoch 1317, Loss: 0.4171793609857559, Final Batch Loss: 0.1943635195493698\n",
      "Epoch 1318, Loss: 0.3897102177143097, Final Batch Loss: 0.14657604694366455\n",
      "Epoch 1319, Loss: 0.42290887236595154, Final Batch Loss: 0.17908410727977753\n",
      "Epoch 1320, Loss: 0.45036283135414124, Final Batch Loss: 0.17656037211418152\n",
      "Epoch 1321, Loss: 0.39857177436351776, Final Batch Loss: 0.1576833724975586\n",
      "Epoch 1322, Loss: 0.6084394454956055, Final Batch Loss: 0.30542412400245667\n",
      "Epoch 1323, Loss: 0.49391597509384155, Final Batch Loss: 0.2590543329715729\n",
      "Epoch 1324, Loss: 0.42214253544807434, Final Batch Loss: 0.22311021387577057\n",
      "Epoch 1325, Loss: 0.5074574202299118, Final Batch Loss: 0.27870339155197144\n",
      "Epoch 1326, Loss: 0.4337332546710968, Final Batch Loss: 0.19483700394630432\n",
      "Epoch 1327, Loss: 0.565142959356308, Final Batch Loss: 0.3046630918979645\n",
      "Epoch 1328, Loss: 0.4034620374441147, Final Batch Loss: 0.15921348333358765\n",
      "Epoch 1329, Loss: 0.42344580590724945, Final Batch Loss: 0.20190738141536713\n",
      "Epoch 1330, Loss: 0.41187770664691925, Final Batch Loss: 0.1935035139322281\n",
      "Epoch 1331, Loss: 0.4431464225053787, Final Batch Loss: 0.2113712877035141\n",
      "Epoch 1332, Loss: 0.42590467631816864, Final Batch Loss: 0.145504429936409\n",
      "Epoch 1333, Loss: 0.3840509355068207, Final Batch Loss: 0.1351742446422577\n",
      "Epoch 1334, Loss: 0.5093368887901306, Final Batch Loss: 0.31764036417007446\n",
      "Epoch 1335, Loss: 0.5009576976299286, Final Batch Loss: 0.2940610945224762\n",
      "Epoch 1336, Loss: 0.39152373373508453, Final Batch Loss: 0.18350015580654144\n",
      "Epoch 1337, Loss: 0.44851237535476685, Final Batch Loss: 0.20494785904884338\n",
      "Epoch 1338, Loss: 0.4030846655368805, Final Batch Loss: 0.1763090193271637\n",
      "Epoch 1339, Loss: 0.4143173098564148, Final Batch Loss: 0.2198440134525299\n",
      "Epoch 1340, Loss: 0.493457555770874, Final Batch Loss: 0.25382381677627563\n",
      "Epoch 1341, Loss: 0.43586476147174835, Final Batch Loss: 0.19377370178699493\n",
      "Epoch 1342, Loss: 0.4821859151124954, Final Batch Loss: 0.21674244105815887\n",
      "Epoch 1343, Loss: 0.44806067645549774, Final Batch Loss: 0.16143958270549774\n",
      "Epoch 1344, Loss: 0.4148329049348831, Final Batch Loss: 0.17433588206768036\n",
      "Epoch 1345, Loss: 0.5648847222328186, Final Batch Loss: 0.3702918589115143\n",
      "Epoch 1346, Loss: 0.42079752683639526, Final Batch Loss: 0.20210884511470795\n",
      "Epoch 1347, Loss: 0.5564341843128204, Final Batch Loss: 0.3478544056415558\n",
      "Epoch 1348, Loss: 0.4696720987558365, Final Batch Loss: 0.2370842546224594\n",
      "Epoch 1349, Loss: 0.4169895648956299, Final Batch Loss: 0.20042775571346283\n",
      "Epoch 1350, Loss: 0.5354901552200317, Final Batch Loss: 0.31302210688591003\n",
      "Epoch 1351, Loss: 0.5390511304140091, Final Batch Loss: 0.2963375747203827\n",
      "Epoch 1352, Loss: 0.3933676481246948, Final Batch Loss: 0.19340087473392487\n",
      "Epoch 1353, Loss: 0.47741730511188507, Final Batch Loss: 0.2403494119644165\n",
      "Epoch 1354, Loss: 0.42035768926143646, Final Batch Loss: 0.21323299407958984\n",
      "Epoch 1355, Loss: 0.49885721504688263, Final Batch Loss: 0.28171032667160034\n",
      "Epoch 1356, Loss: 0.5358282923698425, Final Batch Loss: 0.27896541357040405\n",
      "Epoch 1357, Loss: 0.3773422986268997, Final Batch Loss: 0.1581256091594696\n",
      "Epoch 1358, Loss: 0.36144882440567017, Final Batch Loss: 0.18552759289741516\n",
      "Epoch 1359, Loss: 0.46381236612796783, Final Batch Loss: 0.1881118267774582\n",
      "Epoch 1360, Loss: 0.42854510247707367, Final Batch Loss: 0.18272125720977783\n",
      "Epoch 1361, Loss: 0.42990484833717346, Final Batch Loss: 0.18924778699874878\n",
      "Epoch 1362, Loss: 0.43416234850883484, Final Batch Loss: 0.16294795274734497\n",
      "Epoch 1363, Loss: 0.4407918155193329, Final Batch Loss: 0.22531835734844208\n",
      "Epoch 1364, Loss: 0.47660742700099945, Final Batch Loss: 0.15539763867855072\n",
      "Epoch 1365, Loss: 0.6900211423635483, Final Batch Loss: 0.4789279103279114\n",
      "Epoch 1366, Loss: 0.5527521669864655, Final Batch Loss: 0.3241892457008362\n",
      "Epoch 1367, Loss: 0.38228388130664825, Final Batch Loss: 0.1758846789598465\n",
      "Epoch 1368, Loss: 0.47272956371307373, Final Batch Loss: 0.25950562953948975\n",
      "Epoch 1369, Loss: 0.4346301555633545, Final Batch Loss: 0.19159913063049316\n",
      "Epoch 1370, Loss: 0.45059943199157715, Final Batch Loss: 0.2117864489555359\n",
      "Epoch 1371, Loss: 0.4945669025182724, Final Batch Loss: 0.29314789175987244\n",
      "Epoch 1372, Loss: 0.41103900969028473, Final Batch Loss: 0.1994655877351761\n",
      "Epoch 1373, Loss: 0.36793728172779083, Final Batch Loss: 0.14423047006130219\n",
      "Epoch 1374, Loss: 0.33478623628616333, Final Batch Loss: 0.13106247782707214\n",
      "Epoch 1375, Loss: 0.42004938423633575, Final Batch Loss: 0.2421458661556244\n",
      "Epoch 1376, Loss: 0.3949556052684784, Final Batch Loss: 0.17616720497608185\n",
      "Epoch 1377, Loss: 0.4560292959213257, Final Batch Loss: 0.257313072681427\n",
      "Epoch 1378, Loss: 0.42495767772197723, Final Batch Loss: 0.18974165618419647\n",
      "Epoch 1379, Loss: 0.46480531990528107, Final Batch Loss: 0.19861118495464325\n",
      "Epoch 1380, Loss: 0.575858935713768, Final Batch Loss: 0.40581950545310974\n",
      "Epoch 1381, Loss: 0.48435305058956146, Final Batch Loss: 0.28876155614852905\n",
      "Epoch 1382, Loss: 0.49068495631217957, Final Batch Loss: 0.28833627700805664\n",
      "Epoch 1383, Loss: 0.3932272791862488, Final Batch Loss: 0.18275535106658936\n",
      "Epoch 1384, Loss: 0.5165859311819077, Final Batch Loss: 0.26769891381263733\n",
      "Epoch 1385, Loss: 0.4620426893234253, Final Batch Loss: 0.2669570744037628\n",
      "Epoch 1386, Loss: 0.4057711064815521, Final Batch Loss: 0.1997646987438202\n",
      "Epoch 1387, Loss: 0.3940618932247162, Final Batch Loss: 0.14533185958862305\n",
      "Epoch 1388, Loss: 0.4910401850938797, Final Batch Loss: 0.28103792667388916\n",
      "Epoch 1389, Loss: 0.40957440435886383, Final Batch Loss: 0.23239314556121826\n",
      "Epoch 1390, Loss: 0.36899447441101074, Final Batch Loss: 0.17924177646636963\n",
      "Epoch 1391, Loss: 0.5381749719381332, Final Batch Loss: 0.3065108358860016\n",
      "Epoch 1392, Loss: 0.33149877190589905, Final Batch Loss: 0.1348688304424286\n",
      "Epoch 1393, Loss: 0.5529040098190308, Final Batch Loss: 0.34184470772743225\n",
      "Epoch 1394, Loss: 0.32350459694862366, Final Batch Loss: 0.14741677045822144\n",
      "Epoch 1395, Loss: 0.3446085453033447, Final Batch Loss: 0.13796623051166534\n",
      "Epoch 1396, Loss: 0.45895422995090485, Final Batch Loss: 0.2104993760585785\n",
      "Epoch 1397, Loss: 0.4130025804042816, Final Batch Loss: 0.23503850400447845\n",
      "Epoch 1398, Loss: 0.482797309756279, Final Batch Loss: 0.2751539647579193\n",
      "Epoch 1399, Loss: 0.5613955706357956, Final Batch Loss: 0.3116127550601959\n",
      "Epoch 1400, Loss: 0.39674779772758484, Final Batch Loss: 0.15489724278450012\n",
      "Epoch 1401, Loss: 0.3541165292263031, Final Batch Loss: 0.16700159013271332\n",
      "Epoch 1402, Loss: 0.40474337339401245, Final Batch Loss: 0.1635257452726364\n",
      "Epoch 1403, Loss: 0.3051060885190964, Final Batch Loss: 0.09153278172016144\n",
      "Epoch 1404, Loss: 0.3812413662672043, Final Batch Loss: 0.20353318750858307\n",
      "Epoch 1405, Loss: 0.4266250878572464, Final Batch Loss: 0.232822448015213\n",
      "Epoch 1406, Loss: 0.3477061539888382, Final Batch Loss: 0.17482997477054596\n",
      "Epoch 1407, Loss: 0.5214558243751526, Final Batch Loss: 0.24609753489494324\n",
      "Epoch 1408, Loss: 0.4189571291208267, Final Batch Loss: 0.16647092998027802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1409, Loss: 0.3788393437862396, Final Batch Loss: 0.15229609608650208\n",
      "Epoch 1410, Loss: 0.405598521232605, Final Batch Loss: 0.17943981289863586\n",
      "Epoch 1411, Loss: 0.46467357873916626, Final Batch Loss: 0.23723042011260986\n",
      "Epoch 1412, Loss: 0.41984619200229645, Final Batch Loss: 0.2238539159297943\n",
      "Epoch 1413, Loss: 0.4747154712677002, Final Batch Loss: 0.24837279319763184\n",
      "Epoch 1414, Loss: 0.5614451169967651, Final Batch Loss: 0.34764769673347473\n",
      "Epoch 1415, Loss: 0.9689005315303802, Final Batch Loss: 0.7608566880226135\n",
      "Epoch 1416, Loss: 0.28994861245155334, Final Batch Loss: 0.08685119450092316\n",
      "Epoch 1417, Loss: 0.5103339701890945, Final Batch Loss: 0.27699947357177734\n",
      "Epoch 1418, Loss: 0.4248402416706085, Final Batch Loss: 0.17905953526496887\n",
      "Epoch 1419, Loss: 0.37564265727996826, Final Batch Loss: 0.15265151858329773\n",
      "Epoch 1420, Loss: 0.29578205198049545, Final Batch Loss: 0.10044623166322708\n",
      "Epoch 1421, Loss: 0.4560799300670624, Final Batch Loss: 0.2679058909416199\n",
      "Epoch 1422, Loss: 0.4577767997980118, Final Batch Loss: 0.2626490592956543\n",
      "Epoch 1423, Loss: 0.4134071618318558, Final Batch Loss: 0.15039066970348358\n",
      "Epoch 1424, Loss: 0.4795799106359482, Final Batch Loss: 0.23815125226974487\n",
      "Epoch 1425, Loss: 0.44507768750190735, Final Batch Loss: 0.20521272718906403\n",
      "Epoch 1426, Loss: 0.49985502660274506, Final Batch Loss: 0.2619808316230774\n",
      "Epoch 1427, Loss: 0.45077620446681976, Final Batch Loss: 0.20340879261493683\n",
      "Epoch 1428, Loss: 0.5198203474283218, Final Batch Loss: 0.31982263922691345\n",
      "Epoch 1429, Loss: 0.5155925750732422, Final Batch Loss: 0.3077348470687866\n",
      "Epoch 1430, Loss: 0.42977121472358704, Final Batch Loss: 0.1855933517217636\n",
      "Epoch 1431, Loss: 0.4901202619075775, Final Batch Loss: 0.3100985586643219\n",
      "Epoch 1432, Loss: 0.34059612452983856, Final Batch Loss: 0.1767003834247589\n",
      "Epoch 1433, Loss: 0.39547766745090485, Final Batch Loss: 0.20364125072956085\n",
      "Epoch 1434, Loss: 0.41003789007663727, Final Batch Loss: 0.21995554864406586\n",
      "Epoch 1435, Loss: 0.40109190344810486, Final Batch Loss: 0.1710100620985031\n",
      "Epoch 1436, Loss: 0.37754836678504944, Final Batch Loss: 0.2297479808330536\n",
      "Epoch 1437, Loss: 0.40352821350097656, Final Batch Loss: 0.17957627773284912\n",
      "Epoch 1438, Loss: 0.42020443081855774, Final Batch Loss: 0.17152738571166992\n",
      "Epoch 1439, Loss: 0.3947511613368988, Final Batch Loss: 0.19250401854515076\n",
      "Epoch 1440, Loss: 0.3802422434091568, Final Batch Loss: 0.139607772231102\n",
      "Epoch 1441, Loss: 0.3966747224330902, Final Batch Loss: 0.17515909671783447\n",
      "Epoch 1442, Loss: 0.427265465259552, Final Batch Loss: 0.22425435483455658\n",
      "Epoch 1443, Loss: 0.431232750415802, Final Batch Loss: 0.2395278811454773\n",
      "Epoch 1444, Loss: 0.3671271651983261, Final Batch Loss: 0.18276342749595642\n",
      "Epoch 1445, Loss: 0.46216797828674316, Final Batch Loss: 0.268735408782959\n",
      "Epoch 1446, Loss: 0.37930095195770264, Final Batch Loss: 0.19449540972709656\n",
      "Epoch 1447, Loss: 0.4769936800003052, Final Batch Loss: 0.1233760416507721\n",
      "Epoch 1448, Loss: 0.3785717636346817, Final Batch Loss: 0.21690556406974792\n",
      "Epoch 1449, Loss: 0.43445204198360443, Final Batch Loss: 0.24109458923339844\n",
      "Epoch 1450, Loss: 0.41325265169143677, Final Batch Loss: 0.24870890378952026\n",
      "Epoch 1451, Loss: 0.4130416065454483, Final Batch Loss: 0.2021258920431137\n",
      "Epoch 1452, Loss: 0.42029179632663727, Final Batch Loss: 0.2159469723701477\n",
      "Epoch 1453, Loss: 0.41175420582294464, Final Batch Loss: 0.21084710955619812\n",
      "Epoch 1454, Loss: 0.3949992060661316, Final Batch Loss: 0.1993401199579239\n",
      "Epoch 1455, Loss: 0.3693517595529556, Final Batch Loss: 0.17613868415355682\n",
      "Epoch 1456, Loss: 0.4850539267063141, Final Batch Loss: 0.2209046185016632\n",
      "Epoch 1457, Loss: 0.43461260199546814, Final Batch Loss: 0.23012486100196838\n",
      "Epoch 1458, Loss: 0.4802201986312866, Final Batch Loss: 0.3120214343070984\n",
      "Epoch 1459, Loss: 0.3877650946378708, Final Batch Loss: 0.1978975385427475\n",
      "Epoch 1460, Loss: 0.4306277334690094, Final Batch Loss: 0.23716150224208832\n",
      "Epoch 1461, Loss: 0.43614204227924347, Final Batch Loss: 0.18099786341190338\n",
      "Epoch 1462, Loss: 0.434117391705513, Final Batch Loss: 0.23855486512184143\n",
      "Epoch 1463, Loss: 0.4389496147632599, Final Batch Loss: 0.16158980131149292\n",
      "Epoch 1464, Loss: 0.40302327275276184, Final Batch Loss: 0.17698794603347778\n",
      "Epoch 1465, Loss: 0.3105938136577606, Final Batch Loss: 0.1333926022052765\n",
      "Epoch 1466, Loss: 0.4402482807636261, Final Batch Loss: 0.20111891627311707\n",
      "Epoch 1467, Loss: 0.45217999815940857, Final Batch Loss: 0.23051981627941132\n",
      "Epoch 1468, Loss: 0.46694719791412354, Final Batch Loss: 0.2658329904079437\n",
      "Epoch 1469, Loss: 0.41142113506793976, Final Batch Loss: 0.19075466692447662\n",
      "Epoch 1470, Loss: 0.2946179360151291, Final Batch Loss: 0.12693630158901215\n",
      "Epoch 1471, Loss: 0.39494773745536804, Final Batch Loss: 0.18172967433929443\n",
      "Epoch 1472, Loss: 0.32207141071558, Final Batch Loss: 0.11547818034887314\n",
      "Epoch 1473, Loss: 0.39258986711502075, Final Batch Loss: 0.22075730562210083\n",
      "Epoch 1474, Loss: 0.47159427404403687, Final Batch Loss: 0.24144382774829865\n",
      "Epoch 1475, Loss: 0.3389514237642288, Final Batch Loss: 0.1415676474571228\n",
      "Epoch 1476, Loss: 0.34756359457969666, Final Batch Loss: 0.19418367743492126\n",
      "Epoch 1477, Loss: 0.38732972741127014, Final Batch Loss: 0.17752952873706818\n",
      "Epoch 1478, Loss: 0.4405075013637543, Final Batch Loss: 0.21574436128139496\n",
      "Epoch 1479, Loss: 0.30110345780849457, Final Batch Loss: 0.1495129019021988\n",
      "Epoch 1480, Loss: 0.3933521658182144, Final Batch Loss: 0.1838534027338028\n",
      "Epoch 1481, Loss: 0.312338262796402, Final Batch Loss: 0.1026756763458252\n",
      "Epoch 1482, Loss: 0.349551260471344, Final Batch Loss: 0.1620955914258957\n",
      "Epoch 1483, Loss: 0.44017478823661804, Final Batch Loss: 0.25019359588623047\n",
      "Epoch 1484, Loss: 0.4193696975708008, Final Batch Loss: 0.19831164181232452\n",
      "Epoch 1485, Loss: 0.41492079198360443, Final Batch Loss: 0.26908162236213684\n",
      "Epoch 1486, Loss: 0.3435930907726288, Final Batch Loss: 0.13530385494232178\n",
      "Epoch 1487, Loss: 0.4705814868211746, Final Batch Loss: 0.314241498708725\n",
      "Epoch 1488, Loss: 0.40705353021621704, Final Batch Loss: 0.23528340458869934\n",
      "Epoch 1489, Loss: 0.3830101191997528, Final Batch Loss: 0.200499027967453\n",
      "Epoch 1490, Loss: 0.3427421897649765, Final Batch Loss: 0.12613269686698914\n",
      "Epoch 1491, Loss: 0.3950895071029663, Final Batch Loss: 0.20840060710906982\n",
      "Epoch 1492, Loss: 0.3514467477798462, Final Batch Loss: 0.14436055719852448\n",
      "Epoch 1493, Loss: 0.36836616694927216, Final Batch Loss: 0.21335618197917938\n",
      "Epoch 1494, Loss: 0.42257295548915863, Final Batch Loss: 0.22399833798408508\n",
      "Epoch 1495, Loss: 0.5179572254419327, Final Batch Loss: 0.3321298062801361\n",
      "Epoch 1496, Loss: 0.4470597505569458, Final Batch Loss: 0.29658326506614685\n",
      "Epoch 1497, Loss: 0.39454637467861176, Final Batch Loss: 0.2389761060476303\n",
      "Epoch 1498, Loss: 0.3490292578935623, Final Batch Loss: 0.13376764953136444\n",
      "Epoch 1499, Loss: 0.449678897857666, Final Batch Loss: 0.25494712591171265\n",
      "Epoch 1500, Loss: 0.41659989953041077, Final Batch Loss: 0.22435425221920013\n",
      "Epoch 1501, Loss: 0.40464605391025543, Final Batch Loss: 0.1817312240600586\n",
      "Epoch 1502, Loss: 0.41820985078811646, Final Batch Loss: 0.198720782995224\n",
      "Epoch 1503, Loss: 0.3836434483528137, Final Batch Loss: 0.18071483075618744\n",
      "Epoch 1504, Loss: 0.3381788730621338, Final Batch Loss: 0.16311024129390717\n",
      "Epoch 1505, Loss: 0.3779168128967285, Final Batch Loss: 0.20818983018398285\n",
      "Epoch 1506, Loss: 0.3740732818841934, Final Batch Loss: 0.2247305065393448\n",
      "Epoch 1507, Loss: 0.3395543694496155, Final Batch Loss: 0.1695958375930786\n",
      "Epoch 1508, Loss: 0.40659159421920776, Final Batch Loss: 0.23458585143089294\n",
      "Epoch 1509, Loss: 0.362557515501976, Final Batch Loss: 0.19601939618587494\n",
      "Epoch 1510, Loss: 0.373578280210495, Final Batch Loss: 0.20909513533115387\n",
      "Epoch 1511, Loss: 0.35649098455905914, Final Batch Loss: 0.1311836838722229\n",
      "Epoch 1512, Loss: 0.3885089308023453, Final Batch Loss: 0.19238713383674622\n",
      "Epoch 1513, Loss: 0.3473298102617264, Final Batch Loss: 0.1398380547761917\n",
      "Epoch 1514, Loss: 0.40945960581302643, Final Batch Loss: 0.2280273139476776\n",
      "Epoch 1515, Loss: 0.3241191953420639, Final Batch Loss: 0.15608735382556915\n",
      "Epoch 1516, Loss: 0.43164893984794617, Final Batch Loss: 0.22925697267055511\n",
      "Epoch 1517, Loss: 0.4329475611448288, Final Batch Loss: 0.2680598795413971\n",
      "Epoch 1518, Loss: 0.3199313282966614, Final Batch Loss: 0.17745406925678253\n",
      "Epoch 1519, Loss: 0.42481204867362976, Final Batch Loss: 0.21895243227481842\n",
      "Epoch 1520, Loss: 0.4952598810195923, Final Batch Loss: 0.2899145781993866\n",
      "Epoch 1521, Loss: 0.3297591656446457, Final Batch Loss: 0.127531960606575\n",
      "Epoch 1522, Loss: 0.3403096944093704, Final Batch Loss: 0.17200380563735962\n",
      "Epoch 1523, Loss: 0.47806569933891296, Final Batch Loss: 0.28400975465774536\n",
      "Epoch 1524, Loss: 0.44985368847846985, Final Batch Loss: 0.2574974000453949\n",
      "Epoch 1525, Loss: 0.41592735052108765, Final Batch Loss: 0.2405899316072464\n",
      "Epoch 1526, Loss: 0.35437244176864624, Final Batch Loss: 0.17034272849559784\n",
      "Epoch 1527, Loss: 0.31284099817276, Final Batch Loss: 0.1405857354402542\n",
      "Epoch 1528, Loss: 0.5311714261770248, Final Batch Loss: 0.3646853268146515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1529, Loss: 0.3513966351747513, Final Batch Loss: 0.2148621380329132\n",
      "Epoch 1530, Loss: 0.32849404215812683, Final Batch Loss: 0.13774172961711884\n",
      "Epoch 1531, Loss: 0.32063740491867065, Final Batch Loss: 0.1359880119562149\n",
      "Epoch 1532, Loss: 0.44347940385341644, Final Batch Loss: 0.285110205411911\n",
      "Epoch 1533, Loss: 0.31419914960861206, Final Batch Loss: 0.15306727588176727\n",
      "Epoch 1534, Loss: 0.349617674946785, Final Batch Loss: 0.16769662499427795\n",
      "Epoch 1535, Loss: 0.3237939178943634, Final Batch Loss: 0.13447259366512299\n",
      "Epoch 1536, Loss: 0.31869326531887054, Final Batch Loss: 0.14587879180908203\n",
      "Epoch 1537, Loss: 0.3964253067970276, Final Batch Loss: 0.21578028798103333\n",
      "Epoch 1538, Loss: 0.35484224557876587, Final Batch Loss: 0.18048129975795746\n",
      "Epoch 1539, Loss: 0.40777236223220825, Final Batch Loss: 0.2066185176372528\n",
      "Epoch 1540, Loss: 0.27309733629226685, Final Batch Loss: 0.10383695363998413\n",
      "Epoch 1541, Loss: 0.3470258116722107, Final Batch Loss: 0.13691852986812592\n",
      "Epoch 1542, Loss: 0.4427925646305084, Final Batch Loss: 0.2504405975341797\n",
      "Epoch 1543, Loss: 0.38688068091869354, Final Batch Loss: 0.14576470851898193\n",
      "Epoch 1544, Loss: 0.30212758481502533, Final Batch Loss: 0.12707743048667908\n",
      "Epoch 1545, Loss: 0.39938321709632874, Final Batch Loss: 0.1737833321094513\n",
      "Epoch 1546, Loss: 0.3797464668750763, Final Batch Loss: 0.1752099096775055\n",
      "Epoch 1547, Loss: 0.388371080160141, Final Batch Loss: 0.2020287811756134\n",
      "Epoch 1548, Loss: 0.32659292221069336, Final Batch Loss: 0.13654416799545288\n",
      "Epoch 1549, Loss: 0.41425441205501556, Final Batch Loss: 0.22987385094165802\n",
      "Epoch 1550, Loss: 0.358584389090538, Final Batch Loss: 0.14324192702770233\n",
      "Epoch 1551, Loss: 0.49018335342407227, Final Batch Loss: 0.30693134665489197\n",
      "Epoch 1552, Loss: 0.3469085842370987, Final Batch Loss: 0.1382543295621872\n",
      "Epoch 1553, Loss: 0.32717248797416687, Final Batch Loss: 0.17124493420124054\n",
      "Epoch 1554, Loss: 0.31272484362125397, Final Batch Loss: 0.15326076745986938\n",
      "Epoch 1555, Loss: 0.4409790188074112, Final Batch Loss: 0.2621319890022278\n",
      "Epoch 1556, Loss: 0.31465837359428406, Final Batch Loss: 0.12258563935756683\n",
      "Epoch 1557, Loss: 0.3262062221765518, Final Batch Loss: 0.1597742736339569\n",
      "Epoch 1558, Loss: 0.29522842168807983, Final Batch Loss: 0.10004228353500366\n",
      "Epoch 1559, Loss: 0.3854534327983856, Final Batch Loss: 0.18909285962581635\n",
      "Epoch 1560, Loss: 0.26957015693187714, Final Batch Loss: 0.13587690889835358\n",
      "Epoch 1561, Loss: 0.3378233015537262, Final Batch Loss: 0.18557770550251007\n",
      "Epoch 1562, Loss: 0.3711579293012619, Final Batch Loss: 0.18121466040611267\n",
      "Epoch 1563, Loss: 0.3414536118507385, Final Batch Loss: 0.16309507191181183\n",
      "Epoch 1564, Loss: 0.36481915414333344, Final Batch Loss: 0.18544889986515045\n",
      "Epoch 1565, Loss: 0.5250789076089859, Final Batch Loss: 0.34168916940689087\n",
      "Epoch 1566, Loss: 0.3684484511613846, Final Batch Loss: 0.148229718208313\n",
      "Epoch 1567, Loss: 0.42494073510169983, Final Batch Loss: 0.25806039571762085\n",
      "Epoch 1568, Loss: 0.30268068611621857, Final Batch Loss: 0.16557030379772186\n",
      "Epoch 1569, Loss: 0.3116312623023987, Final Batch Loss: 0.14722302556037903\n",
      "Epoch 1570, Loss: 0.4210222363471985, Final Batch Loss: 0.2596054971218109\n",
      "Epoch 1571, Loss: 0.3381623700261116, Final Batch Loss: 0.08934428542852402\n",
      "Epoch 1572, Loss: 0.4366564303636551, Final Batch Loss: 0.2677753269672394\n",
      "Epoch 1573, Loss: 0.28345488011837006, Final Batch Loss: 0.12863050401210785\n",
      "Epoch 1574, Loss: 0.33650726079940796, Final Batch Loss: 0.1458595097064972\n",
      "Epoch 1575, Loss: 0.2890869528055191, Final Batch Loss: 0.1212390810251236\n",
      "Epoch 1576, Loss: 0.2862021401524544, Final Batch Loss: 0.11862374097108841\n",
      "Epoch 1577, Loss: 0.30103667080402374, Final Batch Loss: 0.14186528325080872\n",
      "Epoch 1578, Loss: 0.4124006927013397, Final Batch Loss: 0.29282742738723755\n",
      "Epoch 1579, Loss: 0.4774305373430252, Final Batch Loss: 0.28059786558151245\n",
      "Epoch 1580, Loss: 0.2993175983428955, Final Batch Loss: 0.13354165852069855\n",
      "Epoch 1581, Loss: 0.31272125989198685, Final Batch Loss: 0.11379092186689377\n",
      "Epoch 1582, Loss: 0.3908722847700119, Final Batch Loss: 0.24597156047821045\n",
      "Epoch 1583, Loss: 0.37652818113565445, Final Batch Loss: 0.12022874504327774\n",
      "Epoch 1584, Loss: 0.347857341170311, Final Batch Loss: 0.19149430096149445\n",
      "Epoch 1585, Loss: 0.3790622502565384, Final Batch Loss: 0.19682317972183228\n",
      "Epoch 1586, Loss: 0.44840967655181885, Final Batch Loss: 0.24333032965660095\n",
      "Epoch 1587, Loss: 0.31076304614543915, Final Batch Loss: 0.1653958112001419\n",
      "Epoch 1588, Loss: 0.3581407219171524, Final Batch Loss: 0.16895316541194916\n",
      "Epoch 1589, Loss: 0.3253537565469742, Final Batch Loss: 0.15912923216819763\n",
      "Epoch 1590, Loss: 0.3443942219018936, Final Batch Loss: 0.16040001809597015\n",
      "Epoch 1591, Loss: 0.3225305527448654, Final Batch Loss: 0.15361565351486206\n",
      "Epoch 1592, Loss: 0.3220764398574829, Final Batch Loss: 0.1460685431957245\n",
      "Epoch 1593, Loss: 0.3556312918663025, Final Batch Loss: 0.1645592749118805\n",
      "Epoch 1594, Loss: 0.34041863679885864, Final Batch Loss: 0.17087112367153168\n",
      "Epoch 1595, Loss: 0.3944445848464966, Final Batch Loss: 0.24006326496601105\n",
      "Epoch 1596, Loss: 0.40902481973171234, Final Batch Loss: 0.22776943445205688\n",
      "Epoch 1597, Loss: 0.27407291531562805, Final Batch Loss: 0.1095968633890152\n",
      "Epoch 1598, Loss: 0.42134232819080353, Final Batch Loss: 0.22166863083839417\n",
      "Epoch 1599, Loss: 0.3826054632663727, Final Batch Loss: 0.20153164863586426\n",
      "Epoch 1600, Loss: 0.4059562236070633, Final Batch Loss: 0.25947609543800354\n",
      "Epoch 1601, Loss: 0.34512968361377716, Final Batch Loss: 0.13745421171188354\n",
      "Epoch 1602, Loss: 0.2811400592327118, Final Batch Loss: 0.11793014407157898\n",
      "Epoch 1603, Loss: 0.5024082958698273, Final Batch Loss: 0.31559696793556213\n",
      "Epoch 1604, Loss: 0.47069787979125977, Final Batch Loss: 0.31554678082466125\n",
      "Epoch 1605, Loss: 0.3061898648738861, Final Batch Loss: 0.14027263224124908\n",
      "Epoch 1606, Loss: 0.3399600386619568, Final Batch Loss: 0.17780524492263794\n",
      "Epoch 1607, Loss: 0.3512689024209976, Final Batch Loss: 0.11487281322479248\n",
      "Epoch 1608, Loss: 0.4122008979320526, Final Batch Loss: 0.22584137320518494\n",
      "Epoch 1609, Loss: 0.3209673911333084, Final Batch Loss: 0.1855168491601944\n",
      "Epoch 1610, Loss: 0.33769215643405914, Final Batch Loss: 0.19052518904209137\n",
      "Epoch 1611, Loss: 0.34682390093803406, Final Batch Loss: 0.15786391496658325\n",
      "Epoch 1612, Loss: 0.3007471114397049, Final Batch Loss: 0.14782877266407013\n",
      "Epoch 1613, Loss: 0.2697906494140625, Final Batch Loss: 0.09563060104846954\n",
      "Epoch 1614, Loss: 0.33421242237091064, Final Batch Loss: 0.1484096646308899\n",
      "Epoch 1615, Loss: 0.41327084600925446, Final Batch Loss: 0.21150794625282288\n",
      "Epoch 1616, Loss: 0.32471364736557007, Final Batch Loss: 0.1424071192741394\n",
      "Epoch 1617, Loss: 0.3684711903333664, Final Batch Loss: 0.1330210566520691\n",
      "Epoch 1618, Loss: 0.4730915427207947, Final Batch Loss: 0.2727663815021515\n",
      "Epoch 1619, Loss: 0.35859329998493195, Final Batch Loss: 0.17942282557487488\n",
      "Epoch 1620, Loss: 0.31000860035419464, Final Batch Loss: 0.1507417857646942\n",
      "Epoch 1621, Loss: 0.30783236026763916, Final Batch Loss: 0.17793501913547516\n",
      "Epoch 1622, Loss: 0.44834432005882263, Final Batch Loss: 0.2892022728919983\n",
      "Epoch 1623, Loss: 0.283625565469265, Final Batch Loss: 0.11533812433481216\n",
      "Epoch 1624, Loss: 0.4290695935487747, Final Batch Loss: 0.22343845665454865\n",
      "Epoch 1625, Loss: 0.25238964706659317, Final Batch Loss: 0.07628070563077927\n",
      "Epoch 1626, Loss: 0.39771659672260284, Final Batch Loss: 0.2230990082025528\n",
      "Epoch 1627, Loss: 0.259384348988533, Final Batch Loss: 0.14793306589126587\n",
      "Epoch 1628, Loss: 0.32482577860355377, Final Batch Loss: 0.186623215675354\n",
      "Epoch 1629, Loss: 0.3405987173318863, Final Batch Loss: 0.21041956543922424\n",
      "Epoch 1630, Loss: 0.4497840404510498, Final Batch Loss: 0.22878199815750122\n",
      "Epoch 1631, Loss: 0.2935412973165512, Final Batch Loss: 0.12739410996437073\n",
      "Epoch 1632, Loss: 0.34761716425418854, Final Batch Loss: 0.20404542982578278\n",
      "Epoch 1633, Loss: 0.25408661365509033, Final Batch Loss: 0.12547819316387177\n",
      "Epoch 1634, Loss: 0.46456843614578247, Final Batch Loss: 0.26550254225730896\n",
      "Epoch 1635, Loss: 0.36462125182151794, Final Batch Loss: 0.21446238458156586\n",
      "Epoch 1636, Loss: 0.2973916530609131, Final Batch Loss: 0.13091638684272766\n",
      "Epoch 1637, Loss: 0.2512592077255249, Final Batch Loss: 0.11254706978797913\n",
      "Epoch 1638, Loss: 0.2823815867304802, Final Batch Loss: 0.09824007004499435\n",
      "Epoch 1639, Loss: 0.22791648656129837, Final Batch Loss: 0.07032788544893265\n",
      "Epoch 1640, Loss: 0.30319108814001083, Final Batch Loss: 0.1017250046133995\n",
      "Epoch 1641, Loss: 0.37873703241348267, Final Batch Loss: 0.17173726856708527\n",
      "Epoch 1642, Loss: 0.2557600513100624, Final Batch Loss: 0.11183031648397446\n",
      "Epoch 1643, Loss: 0.2868705913424492, Final Batch Loss: 0.162329763174057\n",
      "Epoch 1644, Loss: 0.29721397161483765, Final Batch Loss: 0.16738638281822205\n",
      "Epoch 1645, Loss: 0.3468248248100281, Final Batch Loss: 0.17694547772407532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1646, Loss: 0.25438715517520905, Final Batch Loss: 0.11764255166053772\n",
      "Epoch 1647, Loss: 0.2774599716067314, Final Batch Loss: 0.11891668289899826\n",
      "Epoch 1648, Loss: 0.3238717317581177, Final Batch Loss: 0.19380947947502136\n",
      "Epoch 1649, Loss: 0.2911115139722824, Final Batch Loss: 0.1296916902065277\n",
      "Epoch 1650, Loss: 0.26155348867177963, Final Batch Loss: 0.10423333197832108\n",
      "Epoch 1651, Loss: 0.22631105780601501, Final Batch Loss: 0.08808717131614685\n",
      "Epoch 1652, Loss: 0.30922630429267883, Final Batch Loss: 0.15523353219032288\n",
      "Epoch 1653, Loss: 0.26621051877737045, Final Batch Loss: 0.11587091535329819\n",
      "Epoch 1654, Loss: 0.29185162484645844, Final Batch Loss: 0.15889757871627808\n",
      "Epoch 1655, Loss: 0.2605270594358444, Final Batch Loss: 0.10149230062961578\n",
      "Epoch 1656, Loss: 0.4056703597307205, Final Batch Loss: 0.22298558056354523\n",
      "Epoch 1657, Loss: 0.4927818328142166, Final Batch Loss: 0.3435579240322113\n",
      "Epoch 1658, Loss: 0.332510307431221, Final Batch Loss: 0.19245940446853638\n",
      "Epoch 1659, Loss: 0.29720404744148254, Final Batch Loss: 0.1403283029794693\n",
      "Epoch 1660, Loss: 0.25455719232559204, Final Batch Loss: 0.10868088901042938\n",
      "Epoch 1661, Loss: 0.24886560440063477, Final Batch Loss: 0.10116903483867645\n",
      "Epoch 1662, Loss: 0.2528451085090637, Final Batch Loss: 0.1148548275232315\n",
      "Epoch 1663, Loss: 0.30682046711444855, Final Batch Loss: 0.15180937945842743\n",
      "Epoch 1664, Loss: 0.2829302102327347, Final Batch Loss: 0.14973624050617218\n",
      "Epoch 1665, Loss: 0.20317485928535461, Final Batch Loss: 0.08189932256937027\n",
      "Epoch 1666, Loss: 0.25153468549251556, Final Batch Loss: 0.11827407777309418\n",
      "Epoch 1667, Loss: 0.40821367502212524, Final Batch Loss: 0.21749649941921234\n",
      "Epoch 1668, Loss: 0.30927976965904236, Final Batch Loss: 0.09615223109722137\n",
      "Epoch 1669, Loss: 0.2665490210056305, Final Batch Loss: 0.12845227122306824\n",
      "Epoch 1670, Loss: 0.21224600821733475, Final Batch Loss: 0.06088750809431076\n",
      "Epoch 1671, Loss: 0.3439292013645172, Final Batch Loss: 0.17067605257034302\n",
      "Epoch 1672, Loss: 0.4393021762371063, Final Batch Loss: 0.2640226483345032\n",
      "Epoch 1673, Loss: 0.3021990954875946, Final Batch Loss: 0.14689680933952332\n",
      "Epoch 1674, Loss: 0.30818741023540497, Final Batch Loss: 0.16789574921131134\n",
      "Epoch 1675, Loss: 0.3035825341939926, Final Batch Loss: 0.15989500284194946\n",
      "Epoch 1676, Loss: 0.3628488779067993, Final Batch Loss: 0.23506127297878265\n",
      "Epoch 1677, Loss: 0.3516056016087532, Final Batch Loss: 0.23785528540611267\n",
      "Epoch 1678, Loss: 0.32229000329971313, Final Batch Loss: 0.18948324024677277\n",
      "Epoch 1679, Loss: 0.27977460622787476, Final Batch Loss: 0.12914933264255524\n",
      "Epoch 1680, Loss: 0.309666708111763, Final Batch Loss: 0.16468939185142517\n",
      "Epoch 1681, Loss: 0.29308274388313293, Final Batch Loss: 0.18342609703540802\n",
      "Epoch 1682, Loss: 0.3904883414506912, Final Batch Loss: 0.1629665642976761\n",
      "Epoch 1683, Loss: 0.38813601434230804, Final Batch Loss: 0.23352807760238647\n",
      "Epoch 1684, Loss: 0.2269231155514717, Final Batch Loss: 0.0845407024025917\n",
      "Epoch 1685, Loss: 0.27201133221387863, Final Batch Loss: 0.10121338814496994\n",
      "Epoch 1686, Loss: 0.3423893004655838, Final Batch Loss: 0.12814556062221527\n",
      "Epoch 1687, Loss: 0.3540504425764084, Final Batch Loss: 0.14256072044372559\n",
      "Epoch 1688, Loss: 0.5493947267532349, Final Batch Loss: 0.3891461491584778\n",
      "Epoch 1689, Loss: 0.3067009821534157, Final Batch Loss: 0.10639580339193344\n",
      "Epoch 1690, Loss: 0.40882229804992676, Final Batch Loss: 0.23994728922843933\n",
      "Epoch 1691, Loss: 0.30293209850788116, Final Batch Loss: 0.15802165865898132\n",
      "Epoch 1692, Loss: 0.33632831275463104, Final Batch Loss: 0.1711823046207428\n",
      "Epoch 1693, Loss: 0.273518830537796, Final Batch Loss: 0.15649189054965973\n",
      "Epoch 1694, Loss: 0.2947515547275543, Final Batch Loss: 0.1288536936044693\n",
      "Epoch 1695, Loss: 0.28291863948106766, Final Batch Loss: 0.12358523160219193\n",
      "Epoch 1696, Loss: 0.3339656889438629, Final Batch Loss: 0.17858701944351196\n",
      "Epoch 1697, Loss: 0.2926821634173393, Final Batch Loss: 0.10792436450719833\n",
      "Epoch 1698, Loss: 0.3591989576816559, Final Batch Loss: 0.23353485763072968\n",
      "Epoch 1699, Loss: 0.25764042139053345, Final Batch Loss: 0.07182170450687408\n",
      "Epoch 1700, Loss: 0.35705216228961945, Final Batch Loss: 0.23110733926296234\n",
      "Epoch 1701, Loss: 0.22061824798583984, Final Batch Loss: 0.10669896751642227\n",
      "Epoch 1702, Loss: 0.2983861342072487, Final Batch Loss: 0.18860548734664917\n",
      "Epoch 1703, Loss: 0.24533577263355255, Final Batch Loss: 0.10084940493106842\n",
      "Epoch 1704, Loss: 0.31678755581378937, Final Batch Loss: 0.1260189712047577\n",
      "Epoch 1705, Loss: 0.23694194853305817, Final Batch Loss: 0.10333755612373352\n",
      "Epoch 1706, Loss: 0.2881429046392441, Final Batch Loss: 0.15044115483760834\n",
      "Epoch 1707, Loss: 0.2876651957631111, Final Batch Loss: 0.11762235313653946\n",
      "Epoch 1708, Loss: 0.3241909444332123, Final Batch Loss: 0.15996474027633667\n",
      "Epoch 1709, Loss: 0.39969463646411896, Final Batch Loss: 0.24504363536834717\n",
      "Epoch 1710, Loss: 0.39700354635715485, Final Batch Loss: 0.19907112419605255\n",
      "Epoch 1711, Loss: 0.35892346501350403, Final Batch Loss: 0.16881908476352692\n",
      "Epoch 1712, Loss: 0.29209502041339874, Final Batch Loss: 0.1297140121459961\n",
      "Epoch 1713, Loss: 0.3361120671033859, Final Batch Loss: 0.19216929376125336\n",
      "Epoch 1714, Loss: 0.31617899239063263, Final Batch Loss: 0.14907684922218323\n",
      "Epoch 1715, Loss: 0.4349720925092697, Final Batch Loss: 0.2444058209657669\n",
      "Epoch 1716, Loss: 0.2635660767555237, Final Batch Loss: 0.09590423107147217\n",
      "Epoch 1717, Loss: 0.2730153799057007, Final Batch Loss: 0.15315599739551544\n",
      "Epoch 1718, Loss: 0.33734868466854095, Final Batch Loss: 0.2038329392671585\n",
      "Epoch 1719, Loss: 0.38168832659721375, Final Batch Loss: 0.22502465546131134\n",
      "Epoch 1720, Loss: 0.32440729439258575, Final Batch Loss: 0.1708146184682846\n",
      "Epoch 1721, Loss: 0.3026614487171173, Final Batch Loss: 0.12716862559318542\n",
      "Epoch 1722, Loss: 0.20985934883356094, Final Batch Loss: 0.09493114799261093\n",
      "Epoch 1723, Loss: 0.29503610730171204, Final Batch Loss: 0.07834658026695251\n",
      "Epoch 1724, Loss: 0.30814480781555176, Final Batch Loss: 0.1739172339439392\n",
      "Epoch 1725, Loss: 0.2569759637117386, Final Batch Loss: 0.10959963500499725\n",
      "Epoch 1726, Loss: 0.41095465421676636, Final Batch Loss: 0.1968390941619873\n",
      "Epoch 1727, Loss: 0.1774866282939911, Final Batch Loss: 0.05772092938423157\n",
      "Epoch 1728, Loss: 0.31984156370162964, Final Batch Loss: 0.14237070083618164\n",
      "Epoch 1729, Loss: 0.3462270349264145, Final Batch Loss: 0.18071091175079346\n",
      "Epoch 1730, Loss: 0.286767840385437, Final Batch Loss: 0.13398030400276184\n",
      "Epoch 1731, Loss: 0.28888554126024246, Final Batch Loss: 0.1652541309595108\n",
      "Epoch 1732, Loss: 0.3063989281654358, Final Batch Loss: 0.10403484106063843\n",
      "Epoch 1733, Loss: 0.3004968911409378, Final Batch Loss: 0.12659470736980438\n",
      "Epoch 1734, Loss: 0.3256196826696396, Final Batch Loss: 0.1672424077987671\n",
      "Epoch 1735, Loss: 0.25390495359897614, Final Batch Loss: 0.08408117294311523\n",
      "Epoch 1736, Loss: 0.26313480734825134, Final Batch Loss: 0.13807158172130585\n",
      "Epoch 1737, Loss: 0.19173480942845345, Final Batch Loss: 0.060259658843278885\n",
      "Epoch 1738, Loss: 0.25065121054649353, Final Batch Loss: 0.1251889318227768\n",
      "Epoch 1739, Loss: 0.2206272967159748, Final Batch Loss: 0.059152889996767044\n",
      "Epoch 1740, Loss: 0.2705191969871521, Final Batch Loss: 0.12684142589569092\n",
      "Epoch 1741, Loss: 0.3009755462408066, Final Batch Loss: 0.17602458596229553\n",
      "Epoch 1742, Loss: 0.27474741637706757, Final Batch Loss: 0.15955451130867004\n",
      "Epoch 1743, Loss: 0.2701515108346939, Final Batch Loss: 0.1633586287498474\n",
      "Epoch 1744, Loss: 0.2740859389305115, Final Batch Loss: 0.1397847980260849\n",
      "Epoch 1745, Loss: 0.2813011705875397, Final Batch Loss: 0.15358512103557587\n",
      "Epoch 1746, Loss: 0.29927001148462296, Final Batch Loss: 0.17644351720809937\n",
      "Epoch 1747, Loss: 0.24881219863891602, Final Batch Loss: 0.10038876533508301\n",
      "Epoch 1748, Loss: 0.2842618450522423, Final Batch Loss: 0.1129985973238945\n",
      "Epoch 1749, Loss: 0.23711708188056946, Final Batch Loss: 0.11428864300251007\n",
      "Epoch 1750, Loss: 0.24834460765123367, Final Batch Loss: 0.12920744717121124\n",
      "Epoch 1751, Loss: 0.340627945959568, Final Batch Loss: 0.22760184109210968\n",
      "Epoch 1752, Loss: 0.420801118016243, Final Batch Loss: 0.27873602509498596\n",
      "Epoch 1753, Loss: 0.2038353532552719, Final Batch Loss: 0.09650023281574249\n",
      "Epoch 1754, Loss: 0.3741646856069565, Final Batch Loss: 0.24562320113182068\n",
      "Epoch 1755, Loss: 0.360529862344265, Final Batch Loss: 0.23704764246940613\n",
      "Epoch 1756, Loss: 0.32912401109933853, Final Batch Loss: 0.21876457333564758\n",
      "Epoch 1757, Loss: 0.3434080481529236, Final Batch Loss: 0.20708106458187103\n",
      "Epoch 1758, Loss: 0.2209484502673149, Final Batch Loss: 0.10445134341716766\n",
      "Epoch 1759, Loss: 0.2121891900897026, Final Batch Loss: 0.08726329356431961\n",
      "Epoch 1760, Loss: 0.27229970693588257, Final Batch Loss: 0.14319810271263123\n",
      "Epoch 1761, Loss: 0.22570374608039856, Final Batch Loss: 0.08052542805671692\n",
      "Epoch 1762, Loss: 0.27720730006694794, Final Batch Loss: 0.152632936835289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1763, Loss: 0.2131389081478119, Final Batch Loss: 0.10149534046649933\n",
      "Epoch 1764, Loss: 0.25040245056152344, Final Batch Loss: 0.12667159736156464\n",
      "Epoch 1765, Loss: 0.41093654930591583, Final Batch Loss: 0.2979122996330261\n",
      "Epoch 1766, Loss: 0.2774968668818474, Final Batch Loss: 0.17782588303089142\n",
      "Epoch 1767, Loss: 0.3576652854681015, Final Batch Loss: 0.20706705749034882\n",
      "Epoch 1768, Loss: 0.3112013190984726, Final Batch Loss: 0.1510584056377411\n",
      "Epoch 1769, Loss: 0.18264050781726837, Final Batch Loss: 0.06836515665054321\n",
      "Epoch 1770, Loss: 0.2472522333264351, Final Batch Loss: 0.12803007662296295\n",
      "Epoch 1771, Loss: 0.2555965557694435, Final Batch Loss: 0.11951930075883865\n",
      "Epoch 1772, Loss: 0.2450891137123108, Final Batch Loss: 0.10008427500724792\n",
      "Epoch 1773, Loss: 0.2591594085097313, Final Batch Loss: 0.13544785976409912\n",
      "Epoch 1774, Loss: 0.29543620347976685, Final Batch Loss: 0.16723713278770447\n",
      "Epoch 1775, Loss: 0.2685316354036331, Final Batch Loss: 0.1251697838306427\n",
      "Epoch 1776, Loss: 0.40920136868953705, Final Batch Loss: 0.2788090109825134\n",
      "Epoch 1777, Loss: 0.27231642603874207, Final Batch Loss: 0.14078594744205475\n",
      "Epoch 1778, Loss: 0.29199308156967163, Final Batch Loss: 0.12713801860809326\n",
      "Epoch 1779, Loss: 0.2254122570157051, Final Batch Loss: 0.09171991795301437\n",
      "Epoch 1780, Loss: 0.31021901965141296, Final Batch Loss: 0.15729235112667084\n",
      "Epoch 1781, Loss: 0.22522702813148499, Final Batch Loss: 0.10668948292732239\n",
      "Epoch 1782, Loss: 0.33603427559137344, Final Batch Loss: 0.23524940013885498\n",
      "Epoch 1783, Loss: 0.2211240902543068, Final Batch Loss: 0.12689557671546936\n",
      "Epoch 1784, Loss: 0.20553237944841385, Final Batch Loss: 0.11121325194835663\n",
      "Epoch 1785, Loss: 0.23673785477876663, Final Batch Loss: 0.12871934473514557\n",
      "Epoch 1786, Loss: 0.2672867104411125, Final Batch Loss: 0.1514078974723816\n",
      "Epoch 1787, Loss: 0.26062166690826416, Final Batch Loss: 0.13185927271842957\n",
      "Epoch 1788, Loss: 0.27608828991651535, Final Batch Loss: 0.15648625791072845\n",
      "Epoch 1789, Loss: 0.23198405653238297, Final Batch Loss: 0.09603866189718246\n",
      "Epoch 1790, Loss: 0.28995027393102646, Final Batch Loss: 0.11374121159315109\n",
      "Epoch 1791, Loss: 0.2911463677883148, Final Batch Loss: 0.14253728091716766\n",
      "Epoch 1792, Loss: 0.22591331601142883, Final Batch Loss: 0.06284050643444061\n",
      "Epoch 1793, Loss: 0.21808447688817978, Final Batch Loss: 0.11929453164339066\n",
      "Epoch 1794, Loss: 0.23785964399576187, Final Batch Loss: 0.10269100219011307\n",
      "Epoch 1795, Loss: 0.2332717925310135, Final Batch Loss: 0.1093827411532402\n",
      "Epoch 1796, Loss: 0.23381076008081436, Final Batch Loss: 0.12714746594429016\n",
      "Epoch 1797, Loss: 0.2829238995909691, Final Batch Loss: 0.12171442061662674\n",
      "Epoch 1798, Loss: 0.2926274761557579, Final Batch Loss: 0.10748333483934402\n",
      "Epoch 1799, Loss: 0.23923895508050919, Final Batch Loss: 0.09359138458967209\n",
      "Epoch 1800, Loss: 0.2248808592557907, Final Batch Loss: 0.11808300018310547\n",
      "Epoch 1801, Loss: 0.2540350407361984, Final Batch Loss: 0.12192504107952118\n",
      "Epoch 1802, Loss: 0.22095240652561188, Final Batch Loss: 0.11412376910448074\n",
      "Epoch 1803, Loss: 0.2885281592607498, Final Batch Loss: 0.13554149866104126\n",
      "Epoch 1804, Loss: 0.32612432539463043, Final Batch Loss: 0.19200028479099274\n",
      "Epoch 1805, Loss: 0.2436063587665558, Final Batch Loss: 0.09775307774543762\n",
      "Epoch 1806, Loss: 0.2486165389418602, Final Batch Loss: 0.06562452763319016\n",
      "Epoch 1807, Loss: 0.22611773759126663, Final Batch Loss: 0.11066722869873047\n",
      "Epoch 1808, Loss: 0.2700188383460045, Final Batch Loss: 0.16271883249282837\n",
      "Epoch 1809, Loss: 0.16358831524848938, Final Batch Loss: 0.03307458758354187\n",
      "Epoch 1810, Loss: 0.32913658022880554, Final Batch Loss: 0.16043202579021454\n",
      "Epoch 1811, Loss: 0.30321959406137466, Final Batch Loss: 0.22574490308761597\n",
      "Epoch 1812, Loss: 0.294264480471611, Final Batch Loss: 0.16148212552070618\n",
      "Epoch 1813, Loss: 0.19889090210199356, Final Batch Loss: 0.0738043263554573\n",
      "Epoch 1814, Loss: 0.285146564245224, Final Batch Loss: 0.13880781829357147\n",
      "Epoch 1815, Loss: 0.21727989614009857, Final Batch Loss: 0.122667595744133\n",
      "Epoch 1816, Loss: 0.22945870459079742, Final Batch Loss: 0.1078210398554802\n",
      "Epoch 1817, Loss: 0.2890324369072914, Final Batch Loss: 0.17690132558345795\n",
      "Epoch 1818, Loss: 0.2185877561569214, Final Batch Loss: 0.12289901822805405\n",
      "Epoch 1819, Loss: 0.21295540034770966, Final Batch Loss: 0.10493185371160507\n",
      "Epoch 1820, Loss: 0.2731497958302498, Final Batch Loss: 0.15447552502155304\n",
      "Epoch 1821, Loss: 0.256709560751915, Final Batch Loss: 0.13841567933559418\n",
      "Epoch 1822, Loss: 0.2342306300997734, Final Batch Loss: 0.11768421530723572\n",
      "Epoch 1823, Loss: 0.3300739601254463, Final Batch Loss: 0.20877857506275177\n",
      "Epoch 1824, Loss: 0.2275371327996254, Final Batch Loss: 0.07704519480466843\n",
      "Epoch 1825, Loss: 0.2599068880081177, Final Batch Loss: 0.0970502495765686\n",
      "Epoch 1826, Loss: 0.2680821046233177, Final Batch Loss: 0.15579867362976074\n",
      "Epoch 1827, Loss: 0.23227330297231674, Final Batch Loss: 0.11666712909936905\n",
      "Epoch 1828, Loss: 0.28745198249816895, Final Batch Loss: 0.18676838278770447\n",
      "Epoch 1829, Loss: 0.2369692623615265, Final Batch Loss: 0.12531469762325287\n",
      "Epoch 1830, Loss: 0.2666008174419403, Final Batch Loss: 0.06285114586353302\n",
      "Epoch 1831, Loss: 0.2742735594511032, Final Batch Loss: 0.150414377450943\n",
      "Epoch 1832, Loss: 0.28018054366111755, Final Batch Loss: 0.12607137858867645\n",
      "Epoch 1833, Loss: 0.317229762673378, Final Batch Loss: 0.19816870987415314\n",
      "Epoch 1834, Loss: 0.20752708613872528, Final Batch Loss: 0.10275881737470627\n",
      "Epoch 1835, Loss: 0.28194460272789, Final Batch Loss: 0.14757634699344635\n",
      "Epoch 1836, Loss: 0.2741142064332962, Final Batch Loss: 0.18926531076431274\n",
      "Epoch 1837, Loss: 0.20771454274654388, Final Batch Loss: 0.09476007521152496\n",
      "Epoch 1838, Loss: 0.22686346620321274, Final Batch Loss: 0.14257729053497314\n",
      "Epoch 1839, Loss: 0.26206250488758087, Final Batch Loss: 0.12473875284194946\n",
      "Epoch 1840, Loss: 0.2083580270409584, Final Batch Loss: 0.09217062592506409\n",
      "Epoch 1841, Loss: 0.1815737783908844, Final Batch Loss: 0.07621020078659058\n",
      "Epoch 1842, Loss: 0.17656926438212395, Final Batch Loss: 0.05752474442124367\n",
      "Epoch 1843, Loss: 0.18020925670862198, Final Batch Loss: 0.0539122000336647\n",
      "Epoch 1844, Loss: 0.278388187289238, Final Batch Loss: 0.13876578211784363\n",
      "Epoch 1845, Loss: 0.2452177256345749, Final Batch Loss: 0.1252073496580124\n",
      "Epoch 1846, Loss: 0.25457917898893356, Final Batch Loss: 0.10535154491662979\n",
      "Epoch 1847, Loss: 0.18949587643146515, Final Batch Loss: 0.07926727086305618\n",
      "Epoch 1848, Loss: 0.2215588614344597, Final Batch Loss: 0.09566936641931534\n",
      "Epoch 1849, Loss: 0.26834551990032196, Final Batch Loss: 0.10129272937774658\n",
      "Epoch 1850, Loss: 0.32231666147708893, Final Batch Loss: 0.218361034989357\n",
      "Epoch 1851, Loss: 0.2949295789003372, Final Batch Loss: 0.1560354232788086\n",
      "Epoch 1852, Loss: 0.31554143130779266, Final Batch Loss: 0.23140688240528107\n",
      "Epoch 1853, Loss: 0.21441830694675446, Final Batch Loss: 0.0905367061495781\n",
      "Epoch 1854, Loss: 0.2690265104174614, Final Batch Loss: 0.12265079468488693\n",
      "Epoch 1855, Loss: 0.2580288127064705, Final Batch Loss: 0.1519387811422348\n",
      "Epoch 1856, Loss: 0.2785412222146988, Final Batch Loss: 0.13058535754680634\n",
      "Epoch 1857, Loss: 0.25103282928466797, Final Batch Loss: 0.15918146073818207\n",
      "Epoch 1858, Loss: 0.21795451641082764, Final Batch Loss: 0.13441012799739838\n",
      "Epoch 1859, Loss: 0.20350898057222366, Final Batch Loss: 0.07723631709814072\n",
      "Epoch 1860, Loss: 0.2769051492214203, Final Batch Loss: 0.15024511516094208\n",
      "Epoch 1861, Loss: 0.3096151649951935, Final Batch Loss: 0.14669033885002136\n",
      "Epoch 1862, Loss: 0.22099557518959045, Final Batch Loss: 0.07119408249855042\n",
      "Epoch 1863, Loss: 0.21602105349302292, Final Batch Loss: 0.06952882558107376\n",
      "Epoch 1864, Loss: 0.1865057423710823, Final Batch Loss: 0.07478351145982742\n",
      "Epoch 1865, Loss: 0.2170463278889656, Final Batch Loss: 0.08043573051691055\n",
      "Epoch 1866, Loss: 0.20171622931957245, Final Batch Loss: 0.1308726668357849\n",
      "Epoch 1867, Loss: 0.2422223463654518, Final Batch Loss: 0.13346625864505768\n",
      "Epoch 1868, Loss: 0.24778663367033005, Final Batch Loss: 0.09601015597581863\n",
      "Epoch 1869, Loss: 0.20961741358041763, Final Batch Loss: 0.08932101726531982\n",
      "Epoch 1870, Loss: 0.26823295652866364, Final Batch Loss: 0.16795650124549866\n",
      "Epoch 1871, Loss: 0.22234801948070526, Final Batch Loss: 0.13382257521152496\n",
      "Epoch 1872, Loss: 0.20360678434371948, Final Batch Loss: 0.06358650326728821\n",
      "Epoch 1873, Loss: 0.21861661970615387, Final Batch Loss: 0.10214976221323013\n",
      "Epoch 1874, Loss: 0.1790240965783596, Final Batch Loss: 0.04481485113501549\n",
      "Epoch 1875, Loss: 0.28487686812877655, Final Batch Loss: 0.18518833816051483\n",
      "Epoch 1876, Loss: 0.24836914241313934, Final Batch Loss: 0.11464758217334747\n",
      "Epoch 1877, Loss: 0.2475123181939125, Final Batch Loss: 0.11069492250680923\n",
      "Epoch 1878, Loss: 0.156230166554451, Final Batch Loss: 0.04596257954835892\n",
      "Epoch 1879, Loss: 0.15863288193941116, Final Batch Loss: 0.04472728073596954\n",
      "Epoch 1880, Loss: 0.153642687946558, Final Batch Loss: 0.02623598650097847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1881, Loss: 0.3288079798221588, Final Batch Loss: 0.15765602886676788\n",
      "Epoch 1882, Loss: 0.17932891100645065, Final Batch Loss: 0.07240425795316696\n",
      "Epoch 1883, Loss: 0.23201192170381546, Final Batch Loss: 0.13716164231300354\n",
      "Epoch 1884, Loss: 0.25035813450813293, Final Batch Loss: 0.13492615520954132\n",
      "Epoch 1885, Loss: 0.19154200702905655, Final Batch Loss: 0.08304223418235779\n",
      "Epoch 1886, Loss: 0.2863772511482239, Final Batch Loss: 0.164690300822258\n",
      "Epoch 1887, Loss: 0.4546741396188736, Final Batch Loss: 0.2903439998626709\n",
      "Epoch 1888, Loss: 0.169229194521904, Final Batch Loss: 0.06136303395032883\n",
      "Epoch 1889, Loss: 0.1929009035229683, Final Batch Loss: 0.07941502332687378\n",
      "Epoch 1890, Loss: 0.1774558275938034, Final Batch Loss: 0.07009467482566833\n",
      "Epoch 1891, Loss: 0.24521420896053314, Final Batch Loss: 0.14722207188606262\n",
      "Epoch 1892, Loss: 0.1415998935699463, Final Batch Loss: 0.07550352066755295\n",
      "Epoch 1893, Loss: 0.3795987218618393, Final Batch Loss: 0.2668514847755432\n",
      "Epoch 1894, Loss: 0.2842150181531906, Final Batch Loss: 0.13018673658370972\n",
      "Epoch 1895, Loss: 0.2961060181260109, Final Batch Loss: 0.18022958934307098\n",
      "Epoch 1896, Loss: 0.1959836333990097, Final Batch Loss: 0.06796549260616302\n",
      "Epoch 1897, Loss: 0.21622639149427414, Final Batch Loss: 0.10531201213598251\n",
      "Epoch 1898, Loss: 0.1984519436955452, Final Batch Loss: 0.08620501309633255\n",
      "Epoch 1899, Loss: 0.29215067625045776, Final Batch Loss: 0.12535694241523743\n",
      "Epoch 1900, Loss: 0.19386284053325653, Final Batch Loss: 0.08161701261997223\n",
      "Epoch 1901, Loss: 0.1909085363149643, Final Batch Loss: 0.09377913922071457\n",
      "Epoch 1902, Loss: 0.19383779168128967, Final Batch Loss: 0.09466198831796646\n",
      "Epoch 1903, Loss: 0.26581598818302155, Final Batch Loss: 0.13919931650161743\n",
      "Epoch 1904, Loss: 0.27593351900577545, Final Batch Loss: 0.13176588714122772\n",
      "Epoch 1905, Loss: 0.2164989709854126, Final Batch Loss: 0.10008330643177032\n",
      "Epoch 1906, Loss: 0.2496698945760727, Final Batch Loss: 0.11508694291114807\n",
      "Epoch 1907, Loss: 0.22836436331272125, Final Batch Loss: 0.1167173832654953\n",
      "Epoch 1908, Loss: 0.1926582232117653, Final Batch Loss: 0.10952319204807281\n",
      "Epoch 1909, Loss: 0.17100487649440765, Final Batch Loss: 0.05052991956472397\n",
      "Epoch 1910, Loss: 0.21969563513994217, Final Batch Loss: 0.12156593799591064\n",
      "Epoch 1911, Loss: 0.2852552980184555, Final Batch Loss: 0.11236359179019928\n",
      "Epoch 1912, Loss: 0.20001638680696487, Final Batch Loss: 0.08967569470405579\n",
      "Epoch 1913, Loss: 0.13607650250196457, Final Batch Loss: 0.0666414126753807\n",
      "Epoch 1914, Loss: 0.4080536365509033, Final Batch Loss: 0.27620235085487366\n",
      "Epoch 1915, Loss: 0.19877541437745094, Final Batch Loss: 0.04838213697075844\n",
      "Epoch 1916, Loss: 0.20128615200519562, Final Batch Loss: 0.1046702191233635\n",
      "Epoch 1917, Loss: 0.2462545484304428, Final Batch Loss: 0.14496611058712006\n",
      "Epoch 1918, Loss: 0.16479020193219185, Final Batch Loss: 0.04355333372950554\n",
      "Epoch 1919, Loss: 0.28213824331760406, Final Batch Loss: 0.21478959918022156\n",
      "Epoch 1920, Loss: 0.1890806034207344, Final Batch Loss: 0.09278292953968048\n",
      "Epoch 1921, Loss: 0.19226964190602303, Final Batch Loss: 0.0570325143635273\n",
      "Epoch 1922, Loss: 0.251564584672451, Final Batch Loss: 0.13265925645828247\n",
      "Epoch 1923, Loss: 0.23562244325876236, Final Batch Loss: 0.09762009233236313\n",
      "Epoch 1924, Loss: 0.21945611387491226, Final Batch Loss: 0.11756313592195511\n",
      "Epoch 1925, Loss: 0.15772385150194168, Final Batch Loss: 0.05652085691690445\n",
      "Epoch 1926, Loss: 0.15242234617471695, Final Batch Loss: 0.0556444451212883\n",
      "Epoch 1927, Loss: 0.14089826121926308, Final Batch Loss: 0.04955366626381874\n",
      "Epoch 1928, Loss: 0.14662126079201698, Final Batch Loss: 0.04363337531685829\n",
      "Epoch 1929, Loss: 0.20751063525676727, Final Batch Loss: 0.09950365871191025\n",
      "Epoch 1930, Loss: 0.27648650109767914, Final Batch Loss: 0.17779478430747986\n",
      "Epoch 1931, Loss: 0.2232733890414238, Final Batch Loss: 0.09733819216489792\n",
      "Epoch 1932, Loss: 0.17333335429430008, Final Batch Loss: 0.07665419578552246\n",
      "Epoch 1933, Loss: 0.22661173716187477, Final Batch Loss: 0.06001632288098335\n",
      "Epoch 1934, Loss: 0.3474649041891098, Final Batch Loss: 0.24556949734687805\n",
      "Epoch 1935, Loss: 0.1997048705816269, Final Batch Loss: 0.10852862894535065\n",
      "Epoch 1936, Loss: 0.15193583071231842, Final Batch Loss: 0.06951940804719925\n",
      "Epoch 1937, Loss: 0.1827496811747551, Final Batch Loss: 0.09969666600227356\n",
      "Epoch 1938, Loss: 0.23114798218011856, Final Batch Loss: 0.09489559382200241\n",
      "Epoch 1939, Loss: 0.2520325928926468, Final Batch Loss: 0.12670741975307465\n",
      "Epoch 1940, Loss: 0.24034452438354492, Final Batch Loss: 0.13984985649585724\n",
      "Epoch 1941, Loss: 0.19550082087516785, Final Batch Loss: 0.07692131400108337\n",
      "Epoch 1942, Loss: 0.19574332237243652, Final Batch Loss: 0.10639895498752594\n",
      "Epoch 1943, Loss: 0.2125132530927658, Final Batch Loss: 0.06801539659500122\n",
      "Epoch 1944, Loss: 0.15629365295171738, Final Batch Loss: 0.06862886995077133\n",
      "Epoch 1945, Loss: 0.2651079222559929, Final Batch Loss: 0.14659203588962555\n",
      "Epoch 1946, Loss: 0.21808595955371857, Final Batch Loss: 0.07428619265556335\n",
      "Epoch 1947, Loss: 0.2358691766858101, Final Batch Loss: 0.15786898136138916\n",
      "Epoch 1948, Loss: 0.26682138442993164, Final Batch Loss: 0.16233734786510468\n",
      "Epoch 1949, Loss: 0.24830786138772964, Final Batch Loss: 0.11796801537275314\n",
      "Epoch 1950, Loss: 0.27949636429548264, Final Batch Loss: 0.15537595748901367\n",
      "Epoch 1951, Loss: 0.2128242775797844, Final Batch Loss: 0.10747280716896057\n",
      "Epoch 1952, Loss: 0.2915751785039902, Final Batch Loss: 0.17871403694152832\n",
      "Epoch 1953, Loss: 0.14423219114542007, Final Batch Loss: 0.06070701777935028\n",
      "Epoch 1954, Loss: 0.1671450063586235, Final Batch Loss: 0.07290897518396378\n",
      "Epoch 1955, Loss: 0.27863605320453644, Final Batch Loss: 0.1431417167186737\n",
      "Epoch 1956, Loss: 0.22594190388917923, Final Batch Loss: 0.1051044762134552\n",
      "Epoch 1957, Loss: 0.26795850694179535, Final Batch Loss: 0.12935857474803925\n",
      "Epoch 1958, Loss: 0.17351032048463821, Final Batch Loss: 0.09005150198936462\n",
      "Epoch 1959, Loss: 0.1450943537056446, Final Batch Loss: 0.05911489203572273\n",
      "Epoch 1960, Loss: 0.10080838948488235, Final Batch Loss: 0.031964100897312164\n",
      "Epoch 1961, Loss: 0.15126106142997742, Final Batch Loss: 0.08014523237943649\n",
      "Epoch 1962, Loss: 0.20532993227243423, Final Batch Loss: 0.08621598035097122\n",
      "Epoch 1963, Loss: 0.29997409880161285, Final Batch Loss: 0.20029570162296295\n",
      "Epoch 1964, Loss: 0.16351810097694397, Final Batch Loss: 0.07699338346719742\n",
      "Epoch 1965, Loss: 0.27526094764471054, Final Batch Loss: 0.18533696234226227\n",
      "Epoch 1966, Loss: 0.18334557861089706, Final Batch Loss: 0.07640312612056732\n",
      "Epoch 1967, Loss: 0.226782888174057, Final Batch Loss: 0.11501097679138184\n",
      "Epoch 1968, Loss: 0.20671016722917557, Final Batch Loss: 0.10408546775579453\n",
      "Epoch 1969, Loss: 0.19142302870750427, Final Batch Loss: 0.07229194045066833\n",
      "Epoch 1970, Loss: 0.1669631376862526, Final Batch Loss: 0.07450282573699951\n",
      "Epoch 1971, Loss: 0.2005840688943863, Final Batch Loss: 0.12118088454008102\n",
      "Epoch 1972, Loss: 0.1562567502260208, Final Batch Loss: 0.05992131680250168\n",
      "Epoch 1973, Loss: 0.1953960508108139, Final Batch Loss: 0.09093478322029114\n",
      "Epoch 1974, Loss: 0.30926378071308136, Final Batch Loss: 0.14746785163879395\n",
      "Epoch 1975, Loss: 0.17233343422412872, Final Batch Loss: 0.07453083246946335\n",
      "Epoch 1976, Loss: 0.2143312692642212, Final Batch Loss: 0.12765170633792877\n",
      "Epoch 1977, Loss: 0.23177580535411835, Final Batch Loss: 0.07585754990577698\n",
      "Epoch 1978, Loss: 0.18727181106805801, Final Batch Loss: 0.10657194256782532\n",
      "Epoch 1979, Loss: 0.1425805762410164, Final Batch Loss: 0.05683805048465729\n",
      "Epoch 1980, Loss: 0.14899390190839767, Final Batch Loss: 0.06853678077459335\n",
      "Epoch 1981, Loss: 0.19510813802480698, Final Batch Loss: 0.0854073166847229\n",
      "Epoch 1982, Loss: 0.21385108679533005, Final Batch Loss: 0.10414951294660568\n",
      "Epoch 1983, Loss: 0.20009447634220123, Final Batch Loss: 0.1101396456360817\n",
      "Epoch 1984, Loss: 0.21800890564918518, Final Batch Loss: 0.12739574909210205\n",
      "Epoch 1985, Loss: 0.17761163413524628, Final Batch Loss: 0.1123012974858284\n",
      "Epoch 1986, Loss: 0.19790635257959366, Final Batch Loss: 0.11790934205055237\n",
      "Epoch 1987, Loss: 0.2823377922177315, Final Batch Loss: 0.1889858841896057\n",
      "Epoch 1988, Loss: 0.2643957883119583, Final Batch Loss: 0.09188851714134216\n",
      "Epoch 1989, Loss: 0.16692491620779037, Final Batch Loss: 0.05659294128417969\n",
      "Epoch 1990, Loss: 0.22810310125350952, Final Batch Loss: 0.12887413799762726\n",
      "Epoch 1991, Loss: 0.22860299795866013, Final Batch Loss: 0.10125616937875748\n",
      "Epoch 1992, Loss: 0.2424999102950096, Final Batch Loss: 0.12805598974227905\n",
      "Epoch 1993, Loss: 0.190809927880764, Final Batch Loss: 0.07080855220556259\n",
      "Epoch 1994, Loss: 0.22499462962150574, Final Batch Loss: 0.10007123649120331\n",
      "Epoch 1995, Loss: 0.16831132024526596, Final Batch Loss: 0.06419157981872559\n",
      "Epoch 1996, Loss: 0.17077619582414627, Final Batch Loss: 0.0759846419095993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1997, Loss: 0.17985881119966507, Final Batch Loss: 0.0820995420217514\n",
      "Epoch 1998, Loss: 0.1414797306060791, Final Batch Loss: 0.046590857207775116\n",
      "Epoch 1999, Loss: 0.15418583899736404, Final Batch Loss: 0.07516912370920181\n",
      "Epoch 2000, Loss: 0.12423091009259224, Final Batch Loss: 0.040380802005529404\n",
      "Epoch 2001, Loss: 0.10601865127682686, Final Batch Loss: 0.038405466824769974\n",
      "Epoch 2002, Loss: 0.13703720644116402, Final Batch Loss: 0.05773066356778145\n",
      "Epoch 2003, Loss: 0.17206524312496185, Final Batch Loss: 0.06571699678897858\n",
      "Epoch 2004, Loss: 0.1540515273809433, Final Batch Loss: 0.057428330183029175\n",
      "Epoch 2005, Loss: 0.17888199537992477, Final Batch Loss: 0.11283339560031891\n",
      "Epoch 2006, Loss: 0.17432286217808723, Final Batch Loss: 0.040204692631959915\n",
      "Epoch 2007, Loss: 0.2199498564004898, Final Batch Loss: 0.1079801470041275\n",
      "Epoch 2008, Loss: 0.21115496009588242, Final Batch Loss: 0.0553995743393898\n",
      "Epoch 2009, Loss: 0.177597276866436, Final Batch Loss: 0.09450773894786835\n",
      "Epoch 2010, Loss: 0.12866390869021416, Final Batch Loss: 0.04385588690638542\n",
      "Epoch 2011, Loss: 0.14390278607606888, Final Batch Loss: 0.06623014062643051\n",
      "Epoch 2012, Loss: 0.2365681156516075, Final Batch Loss: 0.12147551029920578\n",
      "Epoch 2013, Loss: 0.12536410614848137, Final Batch Loss: 0.029217500239610672\n",
      "Epoch 2014, Loss: 0.16925746202468872, Final Batch Loss: 0.08470723032951355\n",
      "Epoch 2015, Loss: 0.16692699491977692, Final Batch Loss: 0.0791187658905983\n",
      "Epoch 2016, Loss: 0.20405706018209457, Final Batch Loss: 0.1335950642824173\n",
      "Epoch 2017, Loss: 0.11979977786540985, Final Batch Loss: 0.04415348917245865\n",
      "Epoch 2018, Loss: 0.262448288500309, Final Batch Loss: 0.16777385771274567\n",
      "Epoch 2019, Loss: 0.25485699251294136, Final Batch Loss: 0.2117944359779358\n",
      "Epoch 2020, Loss: 0.11164252832531929, Final Batch Loss: 0.03944123163819313\n",
      "Epoch 2021, Loss: 0.14542093873023987, Final Batch Loss: 0.06985534727573395\n",
      "Epoch 2022, Loss: 0.1936550810933113, Final Batch Loss: 0.10054192692041397\n",
      "Epoch 2023, Loss: 0.1799279898405075, Final Batch Loss: 0.06627251207828522\n",
      "Epoch 2024, Loss: 0.16223261505365372, Final Batch Loss: 0.04633422940969467\n",
      "Epoch 2025, Loss: 0.18640457093715668, Final Batch Loss: 0.09547968953847885\n",
      "Epoch 2026, Loss: 0.1384812518954277, Final Batch Loss: 0.032514385879039764\n",
      "Epoch 2027, Loss: 0.1998380348086357, Final Batch Loss: 0.09198670089244843\n",
      "Epoch 2028, Loss: 0.17975902557373047, Final Batch Loss: 0.07498708367347717\n",
      "Epoch 2029, Loss: 0.19486849755048752, Final Batch Loss: 0.10005024820566177\n",
      "Epoch 2030, Loss: 0.18927884101867676, Final Batch Loss: 0.09357363730669022\n",
      "Epoch 2031, Loss: 0.2006017565727234, Final Batch Loss: 0.11371918767690659\n",
      "Epoch 2032, Loss: 0.14432824775576591, Final Batch Loss: 0.05089942738413811\n",
      "Epoch 2033, Loss: 0.1950569599866867, Final Batch Loss: 0.09857405722141266\n",
      "Epoch 2034, Loss: 0.1448228657245636, Final Batch Loss: 0.06474541127681732\n",
      "Epoch 2035, Loss: 0.23390089720487595, Final Batch Loss: 0.13253162801265717\n",
      "Epoch 2036, Loss: 0.1458299420773983, Final Batch Loss: 0.04899701103568077\n",
      "Epoch 2037, Loss: 0.3242918998003006, Final Batch Loss: 0.18937774002552032\n",
      "Epoch 2038, Loss: 0.15599339082837105, Final Batch Loss: 0.04792239889502525\n",
      "Epoch 2039, Loss: 0.16932927817106247, Final Batch Loss: 0.082236148416996\n",
      "Epoch 2040, Loss: 0.16378982365131378, Final Batch Loss: 0.07586974650621414\n",
      "Epoch 2041, Loss: 0.24229419231414795, Final Batch Loss: 0.11764714121818542\n",
      "Epoch 2042, Loss: 0.15157289803028107, Final Batch Loss: 0.09772106260061264\n",
      "Epoch 2043, Loss: 0.22971902787685394, Final Batch Loss: 0.09335146844387054\n",
      "Epoch 2044, Loss: 0.3172754794359207, Final Batch Loss: 0.16060994565486908\n",
      "Epoch 2045, Loss: 0.25055306404829025, Final Batch Loss: 0.07713659852743149\n",
      "Epoch 2046, Loss: 0.26791052520275116, Final Batch Loss: 0.1361428052186966\n",
      "Epoch 2047, Loss: 0.15804259106516838, Final Batch Loss: 0.036653440445661545\n",
      "Epoch 2048, Loss: 0.30160149186849594, Final Batch Loss: 0.21000133454799652\n",
      "Epoch 2049, Loss: 0.1471336930990219, Final Batch Loss: 0.06909475475549698\n",
      "Epoch 2050, Loss: 0.17513488233089447, Final Batch Loss: 0.09700307250022888\n",
      "Epoch 2051, Loss: 0.21914896368980408, Final Batch Loss: 0.14242614805698395\n",
      "Epoch 2052, Loss: 0.19546512514352798, Final Batch Loss: 0.11379577964544296\n",
      "Epoch 2053, Loss: 0.1297001615166664, Final Batch Loss: 0.045347101986408234\n",
      "Epoch 2054, Loss: 0.2609521374106407, Final Batch Loss: 0.15610596537590027\n",
      "Epoch 2055, Loss: 0.19974175840616226, Final Batch Loss: 0.11052041500806808\n",
      "Epoch 2056, Loss: 0.2649631202220917, Final Batch Loss: 0.15941433608531952\n",
      "Epoch 2057, Loss: 0.22979681938886642, Final Batch Loss: 0.11483480781316757\n",
      "Epoch 2058, Loss: 0.1929042711853981, Final Batch Loss: 0.05810212343931198\n",
      "Epoch 2059, Loss: 0.18212349712848663, Final Batch Loss: 0.07992248237133026\n",
      "Epoch 2060, Loss: 0.1456579491496086, Final Batch Loss: 0.04710227996110916\n",
      "Epoch 2061, Loss: 0.16003958135843277, Final Batch Loss: 0.07524261623620987\n",
      "Epoch 2062, Loss: 0.17156266421079636, Final Batch Loss: 0.04012180119752884\n",
      "Epoch 2063, Loss: 0.26868886500597, Final Batch Loss: 0.12481855601072311\n",
      "Epoch 2064, Loss: 0.1945709064602852, Final Batch Loss: 0.07728661596775055\n",
      "Epoch 2065, Loss: 0.16232770681381226, Final Batch Loss: 0.09398970007896423\n",
      "Epoch 2066, Loss: 0.16493503004312515, Final Batch Loss: 0.058096155524253845\n",
      "Epoch 2067, Loss: 0.21457819640636444, Final Batch Loss: 0.1408240646123886\n",
      "Epoch 2068, Loss: 0.1440427601337433, Final Batch Loss: 0.10196983814239502\n",
      "Epoch 2069, Loss: 0.2059919834136963, Final Batch Loss: 0.11033105850219727\n",
      "Epoch 2070, Loss: 0.16729812696576118, Final Batch Loss: 0.06085594370961189\n",
      "Epoch 2071, Loss: 0.1684078872203827, Final Batch Loss: 0.03657776117324829\n",
      "Epoch 2072, Loss: 0.18434100598096848, Final Batch Loss: 0.0994190201163292\n",
      "Epoch 2073, Loss: 0.1889432743191719, Final Batch Loss: 0.10614802688360214\n",
      "Epoch 2074, Loss: 0.12069029733538628, Final Batch Loss: 0.0360117070376873\n",
      "Epoch 2075, Loss: 0.16195962578058243, Final Batch Loss: 0.029831446707248688\n",
      "Epoch 2076, Loss: 0.14826970174908638, Final Batch Loss: 0.049562033265829086\n",
      "Epoch 2077, Loss: 0.1355314590036869, Final Batch Loss: 0.05158831551671028\n",
      "Epoch 2078, Loss: 0.18946078419685364, Final Batch Loss: 0.12462168186903\n",
      "Epoch 2079, Loss: 0.20689883828163147, Final Batch Loss: 0.10616076737642288\n",
      "Epoch 2080, Loss: 0.15992401540279388, Final Batch Loss: 0.09833042323589325\n",
      "Epoch 2081, Loss: 0.11333834752440453, Final Batch Loss: 0.04729476943612099\n",
      "Epoch 2082, Loss: 0.14327586442232132, Final Batch Loss: 0.078588105738163\n",
      "Epoch 2083, Loss: 0.18733620643615723, Final Batch Loss: 0.10804463177919388\n",
      "Epoch 2084, Loss: 0.11664238199591637, Final Batch Loss: 0.052746403962373734\n",
      "Epoch 2085, Loss: 0.24403978884220123, Final Batch Loss: 0.1409004181623459\n",
      "Epoch 2086, Loss: 0.15801876783370972, Final Batch Loss: 0.07968957722187042\n",
      "Epoch 2087, Loss: 0.0968320406973362, Final Batch Loss: 0.030819464474916458\n",
      "Epoch 2088, Loss: 0.21684837341308594, Final Batch Loss: 0.10921492427587509\n",
      "Epoch 2089, Loss: 0.19218793883919716, Final Batch Loss: 0.05964154377579689\n",
      "Epoch 2090, Loss: 0.12138636782765388, Final Batch Loss: 0.04380282387137413\n",
      "Epoch 2091, Loss: 0.16112462803721428, Final Batch Loss: 0.10069708526134491\n",
      "Epoch 2092, Loss: 0.2501060664653778, Final Batch Loss: 0.17166058719158173\n",
      "Epoch 2093, Loss: 0.2005830556154251, Final Batch Loss: 0.1141563355922699\n",
      "Epoch 2094, Loss: 0.20919263362884521, Final Batch Loss: 0.06274601817131042\n",
      "Epoch 2095, Loss: 0.16594306379556656, Final Batch Loss: 0.07589010894298553\n",
      "Epoch 2096, Loss: 0.17541763186454773, Final Batch Loss: 0.08294538408517838\n",
      "Epoch 2097, Loss: 0.2485821768641472, Final Batch Loss: 0.15904556214809418\n",
      "Epoch 2098, Loss: 0.19692086428403854, Final Batch Loss: 0.10913661867380142\n",
      "Epoch 2099, Loss: 0.12019076943397522, Final Batch Loss: 0.05641260743141174\n",
      "Epoch 2100, Loss: 0.1823149248957634, Final Batch Loss: 0.1079544872045517\n",
      "Epoch 2101, Loss: 0.18612564355134964, Final Batch Loss: 0.10232597589492798\n",
      "Epoch 2102, Loss: 0.19049228727817535, Final Batch Loss: 0.06510508060455322\n",
      "Epoch 2103, Loss: 0.27501388639211655, Final Batch Loss: 0.20686830580234528\n",
      "Epoch 2104, Loss: 0.21145595237612724, Final Batch Loss: 0.15379653871059418\n",
      "Epoch 2105, Loss: 0.3020094484090805, Final Batch Loss: 0.14495128393173218\n",
      "Epoch 2106, Loss: 0.15246467292308807, Final Batch Loss: 0.0629434585571289\n",
      "Epoch 2107, Loss: 0.1423390954732895, Final Batch Loss: 0.07783818989992142\n",
      "Epoch 2108, Loss: 0.18320336937904358, Final Batch Loss: 0.0690603256225586\n",
      "Epoch 2109, Loss: 0.11355866864323616, Final Batch Loss: 0.051075126975774765\n",
      "Epoch 2110, Loss: 0.1669970005750656, Final Batch Loss: 0.0920066088438034\n",
      "Epoch 2111, Loss: 0.12151415273547173, Final Batch Loss: 0.0555148683488369\n",
      "Epoch 2112, Loss: 0.26286787539720535, Final Batch Loss: 0.15473107993602753\n",
      "Epoch 2113, Loss: 0.24182886630296707, Final Batch Loss: 0.14214125275611877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2114, Loss: 0.14351516216993332, Final Batch Loss: 0.06716452538967133\n",
      "Epoch 2115, Loss: 0.08840046264231205, Final Batch Loss: 0.021450625732541084\n",
      "Epoch 2116, Loss: 0.12620148062705994, Final Batch Loss: 0.06406334042549133\n",
      "Epoch 2117, Loss: 0.15158728137612343, Final Batch Loss: 0.06171729788184166\n",
      "Epoch 2118, Loss: 0.10203750059008598, Final Batch Loss: 0.03975982591509819\n",
      "Epoch 2119, Loss: 0.1038436722010374, Final Batch Loss: 0.029464175924658775\n",
      "Epoch 2120, Loss: 0.1296341009438038, Final Batch Loss: 0.07250698655843735\n",
      "Epoch 2121, Loss: 0.115147415548563, Final Batch Loss: 0.051915910094976425\n",
      "Epoch 2122, Loss: 0.27313680201768875, Final Batch Loss: 0.15498323738574982\n",
      "Epoch 2123, Loss: 0.28580158203840256, Final Batch Loss: 0.20329757034778595\n",
      "Epoch 2124, Loss: 0.215461865067482, Final Batch Loss: 0.14587286114692688\n",
      "Epoch 2125, Loss: 0.12089885398745537, Final Batch Loss: 0.06761816143989563\n",
      "Epoch 2126, Loss: 0.18859919160604477, Final Batch Loss: 0.10723799467086792\n",
      "Epoch 2127, Loss: 0.13654429465532303, Final Batch Loss: 0.07278243452310562\n",
      "Epoch 2128, Loss: 0.16535719111561775, Final Batch Loss: 0.041955579072237015\n",
      "Epoch 2129, Loss: 0.10452252347022295, Final Batch Loss: 0.015586529858410358\n",
      "Epoch 2130, Loss: 0.3109922409057617, Final Batch Loss: 0.1840505301952362\n",
      "Epoch 2131, Loss: 0.22710120677947998, Final Batch Loss: 0.12628798186779022\n",
      "Epoch 2132, Loss: 0.17518357932567596, Final Batch Loss: 0.08031053841114044\n",
      "Epoch 2133, Loss: 0.18408092483878136, Final Batch Loss: 0.12571971118450165\n",
      "Epoch 2134, Loss: 0.20458844304084778, Final Batch Loss: 0.11076559871435165\n",
      "Epoch 2135, Loss: 0.2674666792154312, Final Batch Loss: 0.08948297798633575\n",
      "Epoch 2136, Loss: 0.1377601958811283, Final Batch Loss: 0.04794628545641899\n",
      "Epoch 2137, Loss: 0.1086837463080883, Final Batch Loss: 0.043843019753694534\n",
      "Epoch 2138, Loss: 0.21242129802703857, Final Batch Loss: 0.11561328917741776\n",
      "Epoch 2139, Loss: 0.13595331832766533, Final Batch Loss: 0.053141091018915176\n",
      "Epoch 2140, Loss: 0.16872787475585938, Final Batch Loss: 0.08794353157281876\n",
      "Epoch 2141, Loss: 0.1614900380373001, Final Batch Loss: 0.05705403536558151\n",
      "Epoch 2142, Loss: 0.17563153058290482, Final Batch Loss: 0.09202610701322556\n",
      "Epoch 2143, Loss: 0.13063902035355568, Final Batch Loss: 0.04830412194132805\n",
      "Epoch 2144, Loss: 0.38209138810634613, Final Batch Loss: 0.2942466735839844\n",
      "Epoch 2145, Loss: 0.15291306376457214, Final Batch Loss: 0.06571783870458603\n",
      "Epoch 2146, Loss: 0.1378620993345976, Final Batch Loss: 0.02540905587375164\n",
      "Epoch 2147, Loss: 0.17807000130414963, Final Batch Loss: 0.10266615450382233\n",
      "Epoch 2148, Loss: 0.17001984268426895, Final Batch Loss: 0.051726073026657104\n",
      "Epoch 2149, Loss: 0.1954297125339508, Final Batch Loss: 0.09277040511369705\n",
      "Epoch 2150, Loss: 0.179793082177639, Final Batch Loss: 0.10634033381938934\n",
      "Epoch 2151, Loss: 0.13960598036646843, Final Batch Loss: 0.0544053353369236\n",
      "Epoch 2152, Loss: 0.12025516852736473, Final Batch Loss: 0.0611722506582737\n",
      "Epoch 2153, Loss: 0.17566321045160294, Final Batch Loss: 0.0889861062169075\n",
      "Epoch 2154, Loss: 0.1665123701095581, Final Batch Loss: 0.07726815342903137\n",
      "Epoch 2155, Loss: 0.16648965328931808, Final Batch Loss: 0.08348263055086136\n",
      "Epoch 2156, Loss: 0.17014425247907639, Final Batch Loss: 0.07220690697431564\n",
      "Epoch 2157, Loss: 0.11323757842183113, Final Batch Loss: 0.03505730256438255\n",
      "Epoch 2158, Loss: 0.3431443050503731, Final Batch Loss: 0.2646259367465973\n",
      "Epoch 2159, Loss: 0.14764925837516785, Final Batch Loss: 0.041939251124858856\n",
      "Epoch 2160, Loss: 0.21143431216478348, Final Batch Loss: 0.0970144271850586\n",
      "Epoch 2161, Loss: 0.20107509940862656, Final Batch Loss: 0.1286199986934662\n",
      "Epoch 2162, Loss: 0.09488729573786259, Final Batch Loss: 0.03030906803905964\n",
      "Epoch 2163, Loss: 0.15679389238357544, Final Batch Loss: 0.07119552046060562\n",
      "Epoch 2164, Loss: 0.1199154444038868, Final Batch Loss: 0.034051161259412766\n",
      "Epoch 2165, Loss: 0.19182004034519196, Final Batch Loss: 0.12362940609455109\n",
      "Epoch 2166, Loss: 0.19335085153579712, Final Batch Loss: 0.11011628061532974\n",
      "Epoch 2167, Loss: 0.10692224651575089, Final Batch Loss: 0.041979044675827026\n",
      "Epoch 2168, Loss: 0.13067638874053955, Final Batch Loss: 0.0630253404378891\n",
      "Epoch 2169, Loss: 0.16506635025143623, Final Batch Loss: 0.11119337379932404\n",
      "Epoch 2170, Loss: 0.17050453275442123, Final Batch Loss: 0.09322240948677063\n",
      "Epoch 2171, Loss: 0.11664822697639465, Final Batch Loss: 0.05913994461297989\n",
      "Epoch 2172, Loss: 0.23537562042474747, Final Batch Loss: 0.15860192477703094\n",
      "Epoch 2173, Loss: 0.20676971226930618, Final Batch Loss: 0.14146177470684052\n",
      "Epoch 2174, Loss: 0.14745446294546127, Final Batch Loss: 0.03311268240213394\n",
      "Epoch 2175, Loss: 0.233182854950428, Final Batch Loss: 0.15704284608364105\n",
      "Epoch 2176, Loss: 0.10811179131269455, Final Batch Loss: 0.03487274795770645\n",
      "Epoch 2177, Loss: 0.14010000228881836, Final Batch Loss: 0.06313632428646088\n",
      "Epoch 2178, Loss: 0.17992175742983818, Final Batch Loss: 0.13580425083637238\n",
      "Epoch 2179, Loss: 0.11050079390406609, Final Batch Loss: 0.05812210217118263\n",
      "Epoch 2180, Loss: 0.1754361242055893, Final Batch Loss: 0.0961713120341301\n",
      "Epoch 2181, Loss: 0.13696766644716263, Final Batch Loss: 0.08191417157649994\n",
      "Epoch 2182, Loss: 0.18168392032384872, Final Batch Loss: 0.0891578420996666\n",
      "Epoch 2183, Loss: 0.14410146325826645, Final Batch Loss: 0.07481130957603455\n",
      "Epoch 2184, Loss: 0.11858737096190453, Final Batch Loss: 0.03385375067591667\n",
      "Epoch 2185, Loss: 0.26950979232788086, Final Batch Loss: 0.18512974679470062\n",
      "Epoch 2186, Loss: 0.2901974320411682, Final Batch Loss: 0.15722763538360596\n",
      "Epoch 2187, Loss: 0.17923519015312195, Final Batch Loss: 0.06802043318748474\n",
      "Epoch 2188, Loss: 0.08821718208491802, Final Batch Loss: 0.028524933382868767\n",
      "Epoch 2189, Loss: 0.18013597652316093, Final Batch Loss: 0.12133955955505371\n",
      "Epoch 2190, Loss: 0.1951727643609047, Final Batch Loss: 0.11696507036685944\n",
      "Epoch 2191, Loss: 0.20070475339889526, Final Batch Loss: 0.10140983015298843\n",
      "Epoch 2192, Loss: 0.23895179480314255, Final Batch Loss: 0.15905047953128815\n",
      "Epoch 2193, Loss: 0.2387712001800537, Final Batch Loss: 0.11877501755952835\n",
      "Epoch 2194, Loss: 0.1623716615140438, Final Batch Loss: 0.05680050328373909\n",
      "Epoch 2195, Loss: 0.09969196654856205, Final Batch Loss: 0.029276015236973763\n",
      "Epoch 2196, Loss: 0.13838111236691475, Final Batch Loss: 0.05825144425034523\n",
      "Epoch 2197, Loss: 0.08953849226236343, Final Batch Loss: 0.03339063748717308\n",
      "Epoch 2198, Loss: 0.16955669596791267, Final Batch Loss: 0.05977455899119377\n",
      "Epoch 2199, Loss: 0.16371002793312073, Final Batch Loss: 0.07701797038316727\n",
      "Epoch 2200, Loss: 0.12418396025896072, Final Batch Loss: 0.0608048215508461\n",
      "Epoch 2201, Loss: 0.0762886106967926, Final Batch Loss: 0.022755000740289688\n",
      "Epoch 2202, Loss: 0.09341649897396564, Final Batch Loss: 0.030564354732632637\n",
      "Epoch 2203, Loss: 0.2323913350701332, Final Batch Loss: 0.16041870415210724\n",
      "Epoch 2204, Loss: 0.16984383016824722, Final Batch Loss: 0.06532645225524902\n",
      "Epoch 2205, Loss: 0.27775902301073074, Final Batch Loss: 0.16426672041416168\n",
      "Epoch 2206, Loss: 0.1793675646185875, Final Batch Loss: 0.1055632159113884\n",
      "Epoch 2207, Loss: 0.24392281472682953, Final Batch Loss: 0.1751033216714859\n",
      "Epoch 2208, Loss: 0.16725491732358932, Final Batch Loss: 0.06199605017900467\n",
      "Epoch 2209, Loss: 0.09568720310926437, Final Batch Loss: 0.039252869784832\n",
      "Epoch 2210, Loss: 0.2156207635998726, Final Batch Loss: 0.14356572926044464\n",
      "Epoch 2211, Loss: 0.16160757839679718, Final Batch Loss: 0.06290573626756668\n",
      "Epoch 2212, Loss: 0.15391072630882263, Final Batch Loss: 0.06869179010391235\n",
      "Epoch 2213, Loss: 0.07922335714101791, Final Batch Loss: 0.03848974034190178\n",
      "Epoch 2214, Loss: 0.09191512688994408, Final Batch Loss: 0.04393138363957405\n",
      "Epoch 2215, Loss: 0.17406325787305832, Final Batch Loss: 0.10077931731939316\n",
      "Epoch 2216, Loss: 0.13107101991772652, Final Batch Loss: 0.042152535170316696\n",
      "Epoch 2217, Loss: 0.09899619966745377, Final Batch Loss: 0.04103348031640053\n",
      "Epoch 2218, Loss: 0.12377746775746346, Final Batch Loss: 0.058495160192251205\n",
      "Epoch 2219, Loss: 0.12882954813539982, Final Batch Loss: 0.024570131674408913\n",
      "Epoch 2220, Loss: 0.14435254409909248, Final Batch Loss: 0.08282413333654404\n",
      "Epoch 2221, Loss: 0.1807379424571991, Final Batch Loss: 0.12123697251081467\n",
      "Epoch 2222, Loss: 0.09952257201075554, Final Batch Loss: 0.034826185554265976\n",
      "Epoch 2223, Loss: 0.1914246529340744, Final Batch Loss: 0.09836387634277344\n",
      "Epoch 2224, Loss: 0.13472682610154152, Final Batch Loss: 0.0800163671374321\n",
      "Epoch 2225, Loss: 0.14082076027989388, Final Batch Loss: 0.08782816678285599\n",
      "Epoch 2226, Loss: 0.09884802624583244, Final Batch Loss: 0.039400286972522736\n",
      "Epoch 2227, Loss: 0.234025776386261, Final Batch Loss: 0.11878903210163116\n",
      "Epoch 2228, Loss: 0.17904063686728477, Final Batch Loss: 0.05956949666142464\n",
      "Epoch 2229, Loss: 0.1582811027765274, Final Batch Loss: 0.05714840441942215\n",
      "Epoch 2230, Loss: 0.22954950481653214, Final Batch Loss: 0.14642539620399475\n",
      "Epoch 2231, Loss: 0.15408986806869507, Final Batch Loss: 0.061472855508327484\n",
      "Epoch 2232, Loss: 0.1746172532439232, Final Batch Loss: 0.07730292528867722\n",
      "Epoch 2233, Loss: 0.17277763783931732, Final Batch Loss: 0.053107745945453644\n",
      "Epoch 2234, Loss: 0.09742171689867973, Final Batch Loss: 0.041689515113830566\n",
      "Epoch 2235, Loss: 0.17309708520770073, Final Batch Loss: 0.04321786388754845\n",
      "Epoch 2236, Loss: 0.11986646056175232, Final Batch Loss: 0.04962965101003647\n",
      "Epoch 2237, Loss: 0.10124415531754494, Final Batch Loss: 0.035965923219919205\n",
      "Epoch 2238, Loss: 0.11706743016839027, Final Batch Loss: 0.05765482410788536\n",
      "Epoch 2239, Loss: 0.19783912599086761, Final Batch Loss: 0.12070221453905106\n",
      "Epoch 2240, Loss: 0.20958711206912994, Final Batch Loss: 0.09413211047649384\n",
      "Epoch 2241, Loss: 0.24977827072143555, Final Batch Loss: 0.15948578715324402\n",
      "Epoch 2242, Loss: 0.17535505443811417, Final Batch Loss: 0.09633190929889679\n",
      "Epoch 2243, Loss: 0.14838037639856339, Final Batch Loss: 0.0714329183101654\n",
      "Epoch 2244, Loss: 0.1748553067445755, Final Batch Loss: 0.06289511173963547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2245, Loss: 0.15605247020721436, Final Batch Loss: 0.06281965225934982\n",
      "Epoch 2246, Loss: 0.1447114646434784, Final Batch Loss: 0.05591234564781189\n",
      "Epoch 2247, Loss: 0.08916567265987396, Final Batch Loss: 0.036904532462358475\n",
      "Epoch 2248, Loss: 0.19139254093170166, Final Batch Loss: 0.08703484386205673\n",
      "Epoch 2249, Loss: 0.1468840204179287, Final Batch Loss: 0.09518328309059143\n",
      "Epoch 2250, Loss: 0.25079914182424545, Final Batch Loss: 0.18370835483074188\n",
      "Epoch 2251, Loss: 0.10083112865686417, Final Batch Loss: 0.027737222611904144\n",
      "Epoch 2252, Loss: 0.10706044360995293, Final Batch Loss: 0.044336963444948196\n",
      "Epoch 2253, Loss: 0.3088027313351631, Final Batch Loss: 0.22760026156902313\n",
      "Epoch 2254, Loss: 0.17514973878860474, Final Batch Loss: 0.08972939848899841\n",
      "Epoch 2255, Loss: 0.11603517830371857, Final Batch Loss: 0.04779431223869324\n",
      "Epoch 2256, Loss: 0.1995410919189453, Final Batch Loss: 0.12686364352703094\n",
      "Epoch 2257, Loss: 0.1477072834968567, Final Batch Loss: 0.02784907817840576\n",
      "Epoch 2258, Loss: 0.09287497401237488, Final Batch Loss: 0.05249965190887451\n",
      "Epoch 2259, Loss: 0.15378577262163162, Final Batch Loss: 0.08538109064102173\n",
      "Epoch 2260, Loss: 0.12365657836198807, Final Batch Loss: 0.07048839330673218\n",
      "Epoch 2261, Loss: 0.11494062468409538, Final Batch Loss: 0.0497966893017292\n",
      "Epoch 2262, Loss: 0.1717657335102558, Final Batch Loss: 0.11628030240535736\n",
      "Epoch 2263, Loss: 0.16322457045316696, Final Batch Loss: 0.0730288177728653\n",
      "Epoch 2264, Loss: 0.12081016227602959, Final Batch Loss: 0.07249590009450912\n",
      "Epoch 2265, Loss: 0.09632062911987305, Final Batch Loss: 0.05155572667717934\n",
      "Epoch 2266, Loss: 0.08869780600070953, Final Batch Loss: 0.015107974410057068\n",
      "Epoch 2267, Loss: 0.21425369381904602, Final Batch Loss: 0.12490513175725937\n",
      "Epoch 2268, Loss: 0.12028804793953896, Final Batch Loss: 0.04771162196993828\n",
      "Epoch 2269, Loss: 0.13235608488321304, Final Batch Loss: 0.07901682704687119\n",
      "Epoch 2270, Loss: 0.12091170251369476, Final Batch Loss: 0.08057229220867157\n",
      "Epoch 2271, Loss: 0.14232011139392853, Final Batch Loss: 0.07383295893669128\n",
      "Epoch 2272, Loss: 0.1964530423283577, Final Batch Loss: 0.13888658583164215\n",
      "Epoch 2273, Loss: 0.11698514595627785, Final Batch Loss: 0.06933199614286423\n",
      "Epoch 2274, Loss: 0.10391981899738312, Final Batch Loss: 0.04027929902076721\n",
      "Epoch 2275, Loss: 0.07969833165407181, Final Batch Loss: 0.04473401606082916\n",
      "Epoch 2276, Loss: 0.18876879662275314, Final Batch Loss: 0.09905987232923508\n",
      "Epoch 2277, Loss: 0.07863561809062958, Final Batch Loss: 0.03972082585096359\n",
      "Epoch 2278, Loss: 0.1499493643641472, Final Batch Loss: 0.06073414534330368\n",
      "Epoch 2279, Loss: 0.14824122190475464, Final Batch Loss: 0.05987528711557388\n",
      "Epoch 2280, Loss: 0.17915331572294235, Final Batch Loss: 0.1130019798874855\n",
      "Epoch 2281, Loss: 0.23121823370456696, Final Batch Loss: 0.15231530368328094\n",
      "Epoch 2282, Loss: 0.14194663241505623, Final Batch Loss: 0.03261252120137215\n",
      "Epoch 2283, Loss: 0.1572212353348732, Final Batch Loss: 0.056669726967811584\n",
      "Epoch 2284, Loss: 0.09061920642852783, Final Batch Loss: 0.040706366300582886\n",
      "Epoch 2285, Loss: 0.19140153378248215, Final Batch Loss: 0.09102308750152588\n",
      "Epoch 2286, Loss: 0.11411654949188232, Final Batch Loss: 0.05678682401776314\n",
      "Epoch 2287, Loss: 0.14941655844449997, Final Batch Loss: 0.07617267221212387\n",
      "Epoch 2288, Loss: 0.09676612727344036, Final Batch Loss: 0.02490781806409359\n",
      "Epoch 2289, Loss: 0.11004076898097992, Final Batch Loss: 0.039443835616111755\n",
      "Epoch 2290, Loss: 0.14485995471477509, Final Batch Loss: 0.07583141326904297\n",
      "Epoch 2291, Loss: 0.10255172476172447, Final Batch Loss: 0.04788626357913017\n",
      "Epoch 2292, Loss: 0.10159497708082199, Final Batch Loss: 0.05060708522796631\n",
      "Epoch 2293, Loss: 0.09188975766301155, Final Batch Loss: 0.038121145218610764\n",
      "Epoch 2294, Loss: 0.14706136658787727, Final Batch Loss: 0.08614114671945572\n",
      "Epoch 2295, Loss: 0.12244690209627151, Final Batch Loss: 0.05061497539281845\n",
      "Epoch 2296, Loss: 0.07891211472451687, Final Batch Loss: 0.02398812212049961\n",
      "Epoch 2297, Loss: 0.07637311890721321, Final Batch Loss: 0.032854244112968445\n",
      "Epoch 2298, Loss: 0.14002874493598938, Final Batch Loss: 0.07701785117387772\n",
      "Epoch 2299, Loss: 0.21748265624046326, Final Batch Loss: 0.12243233621120453\n",
      "Epoch 2300, Loss: 0.1671820804476738, Final Batch Loss: 0.11910443007946014\n",
      "Epoch 2301, Loss: 0.12168718874454498, Final Batch Loss: 0.04247131198644638\n",
      "Epoch 2302, Loss: 0.15260694921016693, Final Batch Loss: 0.09251721948385239\n",
      "Epoch 2303, Loss: 0.15290860831737518, Final Batch Loss: 0.07957975566387177\n",
      "Epoch 2304, Loss: 0.08172400947660208, Final Batch Loss: 0.010345413349568844\n",
      "Epoch 2305, Loss: 0.15893185138702393, Final Batch Loss: 0.0909462496638298\n",
      "Epoch 2306, Loss: 0.12140058726072311, Final Batch Loss: 0.04975235462188721\n",
      "Epoch 2307, Loss: 0.13912764191627502, Final Batch Loss: 0.06811138987541199\n",
      "Epoch 2308, Loss: 0.2106231451034546, Final Batch Loss: 0.1391414850950241\n",
      "Epoch 2309, Loss: 0.12687353044748306, Final Batch Loss: 0.06059611588716507\n",
      "Epoch 2310, Loss: 0.10556029342114925, Final Batch Loss: 0.03053220920264721\n",
      "Epoch 2311, Loss: 0.15301113575696945, Final Batch Loss: 0.0394144207239151\n",
      "Epoch 2312, Loss: 0.1537105143070221, Final Batch Loss: 0.07488704472780228\n",
      "Epoch 2313, Loss: 0.07405804842710495, Final Batch Loss: 0.021899912506341934\n",
      "Epoch 2314, Loss: 0.13695529475808144, Final Batch Loss: 0.07662548869848251\n",
      "Epoch 2315, Loss: 0.15121104568243027, Final Batch Loss: 0.06351839005947113\n",
      "Epoch 2316, Loss: 0.10443571954965591, Final Batch Loss: 0.05768381059169769\n",
      "Epoch 2317, Loss: 0.1146077923476696, Final Batch Loss: 0.05026252940297127\n",
      "Epoch 2318, Loss: 0.12649809941649437, Final Batch Loss: 0.04963419958949089\n",
      "Epoch 2319, Loss: 0.10515626519918442, Final Batch Loss: 0.02265840768814087\n",
      "Epoch 2320, Loss: 0.12279648706316948, Final Batch Loss: 0.07742585241794586\n",
      "Epoch 2321, Loss: 0.1637975387275219, Final Batch Loss: 0.11956106126308441\n",
      "Epoch 2322, Loss: 0.11234361119568348, Final Batch Loss: 0.016330713406205177\n",
      "Epoch 2323, Loss: 0.1252223439514637, Final Batch Loss: 0.035868968814611435\n",
      "Epoch 2324, Loss: 0.09457413479685783, Final Batch Loss: 0.049777865409851074\n",
      "Epoch 2325, Loss: 0.1313595287501812, Final Batch Loss: 0.05353834852576256\n",
      "Epoch 2326, Loss: 0.1892121359705925, Final Batch Loss: 0.07480263710021973\n",
      "Epoch 2327, Loss: 0.14096979051828384, Final Batch Loss: 0.08216818422079086\n",
      "Epoch 2328, Loss: 0.19113260135054588, Final Batch Loss: 0.14048504829406738\n",
      "Epoch 2329, Loss: 0.12317107245326042, Final Batch Loss: 0.07143691927194595\n",
      "Epoch 2330, Loss: 0.14962994307279587, Final Batch Loss: 0.051157645881175995\n",
      "Epoch 2331, Loss: 0.23561207205057144, Final Batch Loss: 0.0955754742026329\n",
      "Epoch 2332, Loss: 0.15186461061239243, Final Batch Loss: 0.06040468066930771\n",
      "Epoch 2333, Loss: 0.1471908949315548, Final Batch Loss: 0.036196742206811905\n",
      "Epoch 2334, Loss: 0.1226121224462986, Final Batch Loss: 0.05099717900156975\n",
      "Epoch 2335, Loss: 0.17213038355112076, Final Batch Loss: 0.13075433671474457\n",
      "Epoch 2336, Loss: 0.13078981637954712, Final Batch Loss: 0.05559054762125015\n",
      "Epoch 2337, Loss: 0.13075565919280052, Final Batch Loss: 0.050270769745111465\n",
      "Epoch 2338, Loss: 0.08238986507058144, Final Batch Loss: 0.03283083438873291\n",
      "Epoch 2339, Loss: 0.11401073262095451, Final Batch Loss: 0.018038544803857803\n",
      "Epoch 2340, Loss: 0.1585358902812004, Final Batch Loss: 0.07169454544782639\n",
      "Epoch 2341, Loss: 0.1800273135304451, Final Batch Loss: 0.10934700071811676\n",
      "Epoch 2342, Loss: 0.14598767459392548, Final Batch Loss: 0.05655869096517563\n",
      "Epoch 2343, Loss: 0.14168858528137207, Final Batch Loss: 0.06780991703271866\n",
      "Epoch 2344, Loss: 0.12171502783894539, Final Batch Loss: 0.05852247402071953\n",
      "Epoch 2345, Loss: 0.09608194977045059, Final Batch Loss: 0.03466607630252838\n",
      "Epoch 2346, Loss: 0.1906084418296814, Final Batch Loss: 0.1020832434296608\n",
      "Epoch 2347, Loss: 0.11151469126343727, Final Batch Loss: 0.054909393191337585\n",
      "Epoch 2348, Loss: 0.2723951190710068, Final Batch Loss: 0.1262325793504715\n",
      "Epoch 2349, Loss: 0.10029006376862526, Final Batch Loss: 0.053241267800331116\n",
      "Epoch 2350, Loss: 0.09430171176791191, Final Batch Loss: 0.04845626279711723\n",
      "Epoch 2351, Loss: 0.1286560334265232, Final Batch Loss: 0.08136224746704102\n",
      "Epoch 2352, Loss: 0.09492138400673866, Final Batch Loss: 0.03850357607007027\n",
      "Epoch 2353, Loss: 0.13843051344156265, Final Batch Loss: 0.034925900399684906\n",
      "Epoch 2354, Loss: 0.15823707729578018, Final Batch Loss: 0.055918626487255096\n",
      "Epoch 2355, Loss: 0.12808744981884956, Final Batch Loss: 0.03944265469908714\n",
      "Epoch 2356, Loss: 0.16994790732860565, Final Batch Loss: 0.11291750520467758\n",
      "Epoch 2357, Loss: 0.11675070412456989, Final Batch Loss: 0.023867184296250343\n",
      "Epoch 2358, Loss: 0.11647291108965874, Final Batch Loss: 0.04271173104643822\n",
      "Epoch 2359, Loss: 0.14335590600967407, Final Batch Loss: 0.02782873809337616\n",
      "Epoch 2360, Loss: 0.09457559883594513, Final Batch Loss: 0.028643019497394562\n",
      "Epoch 2361, Loss: 0.12267990410327911, Final Batch Loss: 0.04569949209690094\n",
      "Epoch 2362, Loss: 0.13485189154744148, Final Batch Loss: 0.074971042573452\n",
      "Epoch 2363, Loss: 0.12097214069217443, Final Batch Loss: 0.014260566793382168\n",
      "Epoch 2364, Loss: 0.19751770049333572, Final Batch Loss: 0.11437088251113892\n",
      "Epoch 2365, Loss: 0.11789972335100174, Final Batch Loss: 0.048174016177654266\n",
      "Epoch 2366, Loss: 0.06907880678772926, Final Batch Loss: 0.014402784407138824\n",
      "Epoch 2367, Loss: 0.07321693561971188, Final Batch Loss: 0.029497841373085976\n",
      "Epoch 2368, Loss: 0.1488398090004921, Final Batch Loss: 0.08487208187580109\n",
      "Epoch 2369, Loss: 0.07972213625907898, Final Batch Loss: 0.029203996062278748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2370, Loss: 0.10869702696800232, Final Batch Loss: 0.019998252391815186\n",
      "Epoch 2371, Loss: 0.13119702599942684, Final Batch Loss: 0.028447164222598076\n",
      "Epoch 2372, Loss: 0.7376131005585194, Final Batch Loss: 0.7007796764373779\n",
      "Epoch 2373, Loss: 0.18008700758218765, Final Batch Loss: 0.10500133037567139\n",
      "Epoch 2374, Loss: 0.14326239749789238, Final Batch Loss: 0.08622241020202637\n",
      "Epoch 2375, Loss: 0.16430320218205452, Final Batch Loss: 0.11044144630432129\n",
      "Epoch 2376, Loss: 0.1737973839044571, Final Batch Loss: 0.08385461568832397\n",
      "Epoch 2377, Loss: 0.16504230350255966, Final Batch Loss: 0.04791709780693054\n",
      "Epoch 2378, Loss: 0.20970256626605988, Final Batch Loss: 0.12343455106019974\n",
      "Epoch 2379, Loss: 0.11009283736348152, Final Batch Loss: 0.0285772867500782\n",
      "Epoch 2380, Loss: 0.11687194555997849, Final Batch Loss: 0.0253123939037323\n",
      "Epoch 2381, Loss: 0.1167132556438446, Final Batch Loss: 0.0357862263917923\n",
      "Epoch 2382, Loss: 0.33243463933467865, Final Batch Loss: 0.2250334918498993\n",
      "Epoch 2383, Loss: 0.2346712052822113, Final Batch Loss: 0.16019178926944733\n",
      "Epoch 2384, Loss: 0.09743541851639748, Final Batch Loss: 0.035638757050037384\n",
      "Epoch 2385, Loss: 0.1180516928434372, Final Batch Loss: 0.07858770340681076\n",
      "Epoch 2386, Loss: 0.09490302205085754, Final Batch Loss: 0.041000936180353165\n",
      "Epoch 2387, Loss: 0.11526104994118214, Final Batch Loss: 0.023170212283730507\n",
      "Epoch 2388, Loss: 0.24490907788276672, Final Batch Loss: 0.143835186958313\n",
      "Epoch 2389, Loss: 0.21909864991903305, Final Batch Loss: 0.12240184843540192\n",
      "Epoch 2390, Loss: 0.10720054432749748, Final Batch Loss: 0.043599989265203476\n",
      "Epoch 2391, Loss: 0.13941242918372154, Final Batch Loss: 0.046239499002695084\n",
      "Epoch 2392, Loss: 0.11098593845963478, Final Batch Loss: 0.06872041523456573\n",
      "Epoch 2393, Loss: 0.17368334159255028, Final Batch Loss: 0.055083442479372025\n",
      "Epoch 2394, Loss: 0.10823090374469757, Final Batch Loss: 0.022155672311782837\n",
      "Epoch 2395, Loss: 0.09390386193990707, Final Batch Loss: 0.0360894538462162\n",
      "Epoch 2396, Loss: 0.09445593878626823, Final Batch Loss: 0.03905269131064415\n",
      "Epoch 2397, Loss: 0.11606784164905548, Final Batch Loss: 0.06966174393892288\n",
      "Epoch 2398, Loss: 0.11123980581760406, Final Batch Loss: 0.048531562089920044\n",
      "Epoch 2399, Loss: 0.1694198176264763, Final Batch Loss: 0.07089099287986755\n",
      "Epoch 2400, Loss: 0.10196075960993767, Final Batch Loss: 0.05419785901904106\n",
      "Epoch 2401, Loss: 0.14314446970820427, Final Batch Loss: 0.08403393626213074\n",
      "Epoch 2402, Loss: 0.15888752043247223, Final Batch Loss: 0.10670921206474304\n",
      "Epoch 2403, Loss: 0.08921068906784058, Final Batch Loss: 0.03391532599925995\n",
      "Epoch 2404, Loss: 0.10820970870554447, Final Batch Loss: 0.024140501394867897\n",
      "Epoch 2405, Loss: 0.20981507748365402, Final Batch Loss: 0.07408442348241806\n",
      "Epoch 2406, Loss: 0.12133805826306343, Final Batch Loss: 0.054146651178598404\n",
      "Epoch 2407, Loss: 0.21330885589122772, Final Batch Loss: 0.11498041450977325\n",
      "Epoch 2408, Loss: 0.1376931183040142, Final Batch Loss: 0.04591965302824974\n",
      "Epoch 2409, Loss: 0.10318110138177872, Final Batch Loss: 0.02942076325416565\n",
      "Epoch 2410, Loss: 0.09739266149699688, Final Batch Loss: 0.029494157060980797\n",
      "Epoch 2411, Loss: 0.10881215892732143, Final Batch Loss: 0.02353995107114315\n",
      "Epoch 2412, Loss: 0.19924616068601608, Final Batch Loss: 0.12104161083698273\n",
      "Epoch 2413, Loss: 0.20003585889935493, Final Batch Loss: 0.15240950882434845\n",
      "Epoch 2414, Loss: 0.10865852981805801, Final Batch Loss: 0.03826194256544113\n",
      "Epoch 2415, Loss: 0.12370944395661354, Final Batch Loss: 0.06271535903215408\n",
      "Epoch 2416, Loss: 0.1499723568558693, Final Batch Loss: 0.08485744893550873\n",
      "Epoch 2417, Loss: 0.09113961458206177, Final Batch Loss: 0.04711192101240158\n",
      "Epoch 2418, Loss: 0.09834334999322891, Final Batch Loss: 0.058429259806871414\n",
      "Epoch 2419, Loss: 0.08474300429224968, Final Batch Loss: 0.03379499539732933\n",
      "Epoch 2420, Loss: 0.13899996131658554, Final Batch Loss: 0.028169766068458557\n",
      "Epoch 2421, Loss: 0.2699979916214943, Final Batch Loss: 0.20377206802368164\n",
      "Epoch 2422, Loss: 0.24667467176914215, Final Batch Loss: 0.14821355044841766\n",
      "Epoch 2423, Loss: 0.15910086035728455, Final Batch Loss: 0.10553498566150665\n",
      "Epoch 2424, Loss: 0.09221364371478558, Final Batch Loss: 0.02575601451098919\n",
      "Epoch 2425, Loss: 0.12572341412305832, Final Batch Loss: 0.04351477324962616\n",
      "Epoch 2426, Loss: 0.1267668940126896, Final Batch Loss: 0.03225458785891533\n",
      "Epoch 2427, Loss: 0.20353106409311295, Final Batch Loss: 0.1464388072490692\n",
      "Epoch 2428, Loss: 0.1252937614917755, Final Batch Loss: 0.059291064739227295\n",
      "Epoch 2429, Loss: 0.12696855515241623, Final Batch Loss: 0.0481344610452652\n",
      "Epoch 2430, Loss: 0.1795097216963768, Final Batch Loss: 0.1381572037935257\n",
      "Epoch 2431, Loss: 0.07676855847239494, Final Batch Loss: 0.027543053030967712\n",
      "Epoch 2432, Loss: 0.1943381428718567, Final Batch Loss: 0.11453355848789215\n",
      "Epoch 2433, Loss: 0.15030142292380333, Final Batch Loss: 0.09466757625341415\n",
      "Epoch 2434, Loss: 0.17284877598285675, Final Batch Loss: 0.09601282328367233\n",
      "Epoch 2435, Loss: 0.12755268439650536, Final Batch Loss: 0.057537201792001724\n",
      "Epoch 2436, Loss: 0.19361386448144913, Final Batch Loss: 0.14170990884304047\n",
      "Epoch 2437, Loss: 0.15253130719065666, Final Batch Loss: 0.03895967826247215\n",
      "Epoch 2438, Loss: 0.1354016251862049, Final Batch Loss: 0.0496223084628582\n",
      "Epoch 2439, Loss: 0.09647370874881744, Final Batch Loss: 0.03647720813751221\n",
      "Epoch 2440, Loss: 0.20922153443098068, Final Batch Loss: 0.10967209935188293\n",
      "Epoch 2441, Loss: 0.09646666049957275, Final Batch Loss: 0.027276605367660522\n",
      "Epoch 2442, Loss: 0.09986904636025429, Final Batch Loss: 0.05909518152475357\n",
      "Epoch 2443, Loss: 0.18460013717412949, Final Batch Loss: 0.10921421647071838\n",
      "Epoch 2444, Loss: 0.1011138204485178, Final Batch Loss: 0.019226523116230965\n",
      "Epoch 2445, Loss: 0.2584635056555271, Final Batch Loss: 0.20863282680511475\n",
      "Epoch 2446, Loss: 0.10230892710387707, Final Batch Loss: 0.019107921048998833\n",
      "Epoch 2447, Loss: 0.11776390671730042, Final Batch Loss: 0.0667935162782669\n",
      "Epoch 2448, Loss: 0.1064070425927639, Final Batch Loss: 0.04511169344186783\n",
      "Epoch 2449, Loss: 0.14665375277400017, Final Batch Loss: 0.04377606883645058\n",
      "Epoch 2450, Loss: 0.2066279798746109, Final Batch Loss: 0.10679125785827637\n",
      "Epoch 2451, Loss: 0.1319817751646042, Final Batch Loss: 0.07554551959037781\n",
      "Epoch 2452, Loss: 0.10784999094903469, Final Batch Loss: 0.0200969185680151\n",
      "Epoch 2453, Loss: 0.0893772728741169, Final Batch Loss: 0.03190627321600914\n",
      "Epoch 2454, Loss: 0.11407918483018875, Final Batch Loss: 0.06963105499744415\n",
      "Epoch 2455, Loss: 0.1334393098950386, Final Batch Loss: 0.03604299575090408\n",
      "Epoch 2456, Loss: 0.13840405642986298, Final Batch Loss: 0.061916790902614594\n",
      "Epoch 2457, Loss: 0.09998047724366188, Final Batch Loss: 0.05527776852250099\n",
      "Epoch 2458, Loss: 0.09483739733695984, Final Batch Loss: 0.03469254449009895\n",
      "Epoch 2459, Loss: 0.13474562764167786, Final Batch Loss: 0.04680582135915756\n",
      "Epoch 2460, Loss: 0.11012762039899826, Final Batch Loss: 0.04746676981449127\n",
      "Epoch 2461, Loss: 0.18025675415992737, Final Batch Loss: 0.10819481313228607\n",
      "Epoch 2462, Loss: 0.1306203007698059, Final Batch Loss: 0.07780593633651733\n",
      "Epoch 2463, Loss: 0.06771606765687466, Final Batch Loss: 0.026050614193081856\n",
      "Epoch 2464, Loss: 0.17655547708272934, Final Batch Loss: 0.07309083640575409\n",
      "Epoch 2465, Loss: 0.09521924331784248, Final Batch Loss: 0.03916312754154205\n",
      "Epoch 2466, Loss: 0.13162072002887726, Final Batch Loss: 0.044687166810035706\n",
      "Epoch 2467, Loss: 0.17927641421556473, Final Batch Loss: 0.1075764149427414\n",
      "Epoch 2468, Loss: 0.09208372049033642, Final Batch Loss: 0.018926424905657768\n",
      "Epoch 2469, Loss: 0.08339041098952293, Final Batch Loss: 0.04087023437023163\n",
      "Epoch 2470, Loss: 0.1851278841495514, Final Batch Loss: 0.0845080018043518\n",
      "Epoch 2471, Loss: 0.10460915416479111, Final Batch Loss: 0.045725706964731216\n",
      "Epoch 2472, Loss: 0.09698531962931156, Final Batch Loss: 0.02959275431931019\n",
      "Epoch 2473, Loss: 0.12653504312038422, Final Batch Loss: 0.07111621648073196\n",
      "Epoch 2474, Loss: 0.10257069393992424, Final Batch Loss: 0.04459276422858238\n",
      "Epoch 2475, Loss: 0.11050215363502502, Final Batch Loss: 0.030681081116199493\n",
      "Epoch 2476, Loss: 0.12594640254974365, Final Batch Loss: 0.06237222999334335\n",
      "Epoch 2477, Loss: 0.10013352334499359, Final Batch Loss: 0.04016304388642311\n",
      "Epoch 2478, Loss: 0.06664559058845043, Final Batch Loss: 0.027077237144112587\n",
      "Epoch 2479, Loss: 0.1576496995985508, Final Batch Loss: 0.10137571394443512\n",
      "Epoch 2480, Loss: 0.07474016770720482, Final Batch Loss: 0.0369151346385479\n",
      "Epoch 2481, Loss: 0.0865486990660429, Final Batch Loss: 0.024544699117541313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2482, Loss: 0.1270357221364975, Final Batch Loss: 0.06553802639245987\n",
      "Epoch 2483, Loss: 0.10133550502359867, Final Batch Loss: 0.07061989605426788\n",
      "Epoch 2484, Loss: 0.09748362563550472, Final Batch Loss: 0.06654917448759079\n",
      "Epoch 2485, Loss: 0.12751341983675957, Final Batch Loss: 0.04857533797621727\n",
      "Epoch 2486, Loss: 0.14660483226180077, Final Batch Loss: 0.11350877583026886\n",
      "Epoch 2487, Loss: 0.25941964983940125, Final Batch Loss: 0.20950816571712494\n",
      "Epoch 2488, Loss: 0.1509794294834137, Final Batch Loss: 0.08740594238042831\n",
      "Epoch 2489, Loss: 0.11693421564996243, Final Batch Loss: 0.029207227751612663\n",
      "Epoch 2490, Loss: 0.1293637380003929, Final Batch Loss: 0.08560357987880707\n",
      "Epoch 2491, Loss: 0.21568739786744118, Final Batch Loss: 0.15635527670383453\n",
      "Epoch 2492, Loss: 0.07989208400249481, Final Batch Loss: 0.020408831536769867\n",
      "Epoch 2493, Loss: 0.19437403231859207, Final Batch Loss: 0.06643027812242508\n",
      "Epoch 2494, Loss: 0.09825868159532547, Final Batch Loss: 0.029216162860393524\n",
      "Epoch 2495, Loss: 0.0682948175817728, Final Batch Loss: 0.027087537571787834\n",
      "Epoch 2496, Loss: 0.11923617497086525, Final Batch Loss: 0.07832956314086914\n",
      "Epoch 2497, Loss: 0.1497792713344097, Final Batch Loss: 0.05294325575232506\n",
      "Epoch 2498, Loss: 0.14428528398275375, Final Batch Loss: 0.1115306094288826\n",
      "Epoch 2499, Loss: 0.09367784857749939, Final Batch Loss: 0.03978251665830612\n",
      "Epoch 2500, Loss: 0.17500894516706467, Final Batch Loss: 0.07805313915014267\n",
      "Epoch 2501, Loss: 0.09494909271597862, Final Batch Loss: 0.03616587817668915\n",
      "Epoch 2502, Loss: 0.05123960878700018, Final Batch Loss: 0.013199652545154095\n",
      "Epoch 2503, Loss: 0.1707613691687584, Final Batch Loss: 0.08730025589466095\n",
      "Epoch 2504, Loss: 0.14872898161411285, Final Batch Loss: 0.07878940552473068\n",
      "Epoch 2505, Loss: 0.06458031013607979, Final Batch Loss: 0.03279506415128708\n",
      "Epoch 2506, Loss: 0.164046011865139, Final Batch Loss: 0.08025316148996353\n",
      "Epoch 2507, Loss: 0.10032926872372627, Final Batch Loss: 0.041127175092697144\n",
      "Epoch 2508, Loss: 0.12098876386880875, Final Batch Loss: 0.048003487288951874\n",
      "Epoch 2509, Loss: 0.05867704190313816, Final Batch Loss: 0.02791915088891983\n",
      "Epoch 2510, Loss: 0.1688712239265442, Final Batch Loss: 0.06447146087884903\n",
      "Epoch 2511, Loss: 0.11014760285615921, Final Batch Loss: 0.035110607743263245\n",
      "Epoch 2512, Loss: 0.20545945689082146, Final Batch Loss: 0.14859019219875336\n",
      "Epoch 2513, Loss: 0.09216887503862381, Final Batch Loss: 0.04894932731986046\n",
      "Epoch 2514, Loss: 0.1588483825325966, Final Batch Loss: 0.12634621560573578\n",
      "Epoch 2515, Loss: 0.1391625590622425, Final Batch Loss: 0.08378800749778748\n",
      "Epoch 2516, Loss: 0.11445524916052818, Final Batch Loss: 0.07596759498119354\n",
      "Epoch 2517, Loss: 0.10279052332043648, Final Batch Loss: 0.030415315181016922\n",
      "Epoch 2518, Loss: 0.0843270979821682, Final Batch Loss: 0.046268414705991745\n",
      "Epoch 2519, Loss: 0.10030145198106766, Final Batch Loss: 0.03714926540851593\n",
      "Epoch 2520, Loss: 0.1541570909321308, Final Batch Loss: 0.09291903674602509\n",
      "Epoch 2521, Loss: 0.060159413143992424, Final Batch Loss: 0.024678831920027733\n",
      "Epoch 2522, Loss: 0.06775595620274544, Final Batch Loss: 0.025654476135969162\n",
      "Epoch 2523, Loss: 0.09556752070784569, Final Batch Loss: 0.03275184705853462\n",
      "Epoch 2524, Loss: 0.11697246506810188, Final Batch Loss: 0.049992967396974564\n",
      "Epoch 2525, Loss: 0.11643758043646812, Final Batch Loss: 0.06185871362686157\n",
      "Epoch 2526, Loss: 0.09328059107065201, Final Batch Loss: 0.041245561093091965\n",
      "Epoch 2527, Loss: 0.06716848723590374, Final Batch Loss: 0.016195887699723244\n",
      "Epoch 2528, Loss: 0.08088875655084848, Final Batch Loss: 0.014156282879412174\n",
      "Epoch 2529, Loss: 0.05506002902984619, Final Batch Loss: 0.023845741525292397\n",
      "Epoch 2530, Loss: 0.17304617911577225, Final Batch Loss: 0.09360940754413605\n",
      "Epoch 2531, Loss: 0.13581354171037674, Final Batch Loss: 0.07546496391296387\n",
      "Epoch 2532, Loss: 0.12945665046572685, Final Batch Loss: 0.05096578225493431\n",
      "Epoch 2533, Loss: 0.08963502570986748, Final Batch Loss: 0.0317186564207077\n",
      "Epoch 2534, Loss: 0.06552394293248653, Final Batch Loss: 0.018911289051175117\n",
      "Epoch 2535, Loss: 0.08774715662002563, Final Batch Loss: 0.05153406038880348\n",
      "Epoch 2536, Loss: 0.09434775821864605, Final Batch Loss: 0.0239116083830595\n",
      "Epoch 2537, Loss: 0.07643324509263039, Final Batch Loss: 0.029793325811624527\n",
      "Epoch 2538, Loss: 0.14832207560539246, Final Batch Loss: 0.0653611272573471\n",
      "Epoch 2539, Loss: 0.12173300981521606, Final Batch Loss: 0.05747423321008682\n",
      "Epoch 2540, Loss: 0.16932694986462593, Final Batch Loss: 0.10868056118488312\n",
      "Epoch 2541, Loss: 0.10785376653075218, Final Batch Loss: 0.06362485885620117\n",
      "Epoch 2542, Loss: 0.11603998020291328, Final Batch Loss: 0.04379552975296974\n",
      "Epoch 2543, Loss: 0.07704888842999935, Final Batch Loss: 0.0293963011354208\n",
      "Epoch 2544, Loss: 0.06909383833408356, Final Batch Loss: 0.032128091901540756\n",
      "Epoch 2545, Loss: 0.05246695689857006, Final Batch Loss: 0.017979519441723824\n",
      "Epoch 2546, Loss: 0.14760341495275497, Final Batch Loss: 0.08551677316427231\n",
      "Epoch 2547, Loss: 0.09184512495994568, Final Batch Loss: 0.042821962386369705\n",
      "Epoch 2548, Loss: 0.11756046488881111, Final Batch Loss: 0.02529066428542137\n",
      "Epoch 2549, Loss: 0.09702072106301785, Final Batch Loss: 0.07102455943822861\n",
      "Epoch 2550, Loss: 0.0916278213262558, Final Batch Loss: 0.03895366191864014\n",
      "Epoch 2551, Loss: 0.08775908499956131, Final Batch Loss: 0.05594228208065033\n",
      "Epoch 2552, Loss: 0.11693307012319565, Final Batch Loss: 0.045922525227069855\n",
      "Epoch 2553, Loss: 0.10501721873879433, Final Batch Loss: 0.05148940160870552\n",
      "Epoch 2554, Loss: 0.10240117460489273, Final Batch Loss: 0.05255274102091789\n",
      "Epoch 2555, Loss: 0.10888499021530151, Final Batch Loss: 0.05312265083193779\n",
      "Epoch 2556, Loss: 0.12104886770248413, Final Batch Loss: 0.05549492686986923\n",
      "Epoch 2557, Loss: 0.09138781949877739, Final Batch Loss: 0.019406039267778397\n",
      "Epoch 2558, Loss: 0.12556206062436104, Final Batch Loss: 0.09066008031368256\n",
      "Epoch 2559, Loss: 0.14306620508432388, Final Batch Loss: 0.09749844670295715\n",
      "Epoch 2560, Loss: 0.09700823575258255, Final Batch Loss: 0.0487876832485199\n",
      "Epoch 2561, Loss: 0.12559090182185173, Final Batch Loss: 0.08713886141777039\n",
      "Epoch 2562, Loss: 0.12883437052369118, Final Batch Loss: 0.06676686555147171\n",
      "Epoch 2563, Loss: 0.06334824115037918, Final Batch Loss: 0.010591242462396622\n",
      "Epoch 2564, Loss: 0.06863614730536938, Final Batch Loss: 0.0231414046138525\n",
      "Epoch 2565, Loss: 0.08212306350469589, Final Batch Loss: 0.03284083679318428\n",
      "Epoch 2566, Loss: 0.11717823892831802, Final Batch Loss: 0.07704108208417892\n",
      "Epoch 2567, Loss: 0.14463920146226883, Final Batch Loss: 0.09998291730880737\n",
      "Epoch 2568, Loss: 0.1286678910255432, Final Batch Loss: 0.08234068006277084\n",
      "Epoch 2569, Loss: 0.12970860674977303, Final Batch Loss: 0.07289445400238037\n",
      "Epoch 2570, Loss: 0.09278733283281326, Final Batch Loss: 0.03909442946314812\n",
      "Epoch 2571, Loss: 0.10550705716013908, Final Batch Loss: 0.039762016385793686\n",
      "Epoch 2572, Loss: 0.09340566396713257, Final Batch Loss: 0.025450214743614197\n",
      "Epoch 2573, Loss: 0.0897486824542284, Final Batch Loss: 0.03084024228155613\n",
      "Epoch 2574, Loss: 0.15994832664728165, Final Batch Loss: 0.07601825892925262\n",
      "Epoch 2575, Loss: 0.08890227973461151, Final Batch Loss: 0.018671847879886627\n",
      "Epoch 2576, Loss: 0.10189248993992805, Final Batch Loss: 0.0515516996383667\n",
      "Epoch 2577, Loss: 0.13756642118096352, Final Batch Loss: 0.09206169098615646\n",
      "Epoch 2578, Loss: 0.10434272512793541, Final Batch Loss: 0.04757862910628319\n",
      "Epoch 2579, Loss: 0.09719954431056976, Final Batch Loss: 0.02890666574239731\n",
      "Epoch 2580, Loss: 0.11285079643130302, Final Batch Loss: 0.03972594067454338\n",
      "Epoch 2581, Loss: 0.18543386459350586, Final Batch Loss: 0.09806227684020996\n",
      "Epoch 2582, Loss: 0.1659279502928257, Final Batch Loss: 0.10397928953170776\n",
      "Epoch 2583, Loss: 0.2426534742116928, Final Batch Loss: 0.158505380153656\n",
      "Epoch 2584, Loss: 0.09597325325012207, Final Batch Loss: 0.035603515803813934\n",
      "Epoch 2585, Loss: 0.10909844934940338, Final Batch Loss: 0.05082642287015915\n",
      "Epoch 2586, Loss: 0.23669859021902084, Final Batch Loss: 0.14655521512031555\n",
      "Epoch 2587, Loss: 0.16232385486364365, Final Batch Loss: 0.06579967588186264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2588, Loss: 0.11013123393058777, Final Batch Loss: 0.031862154603004456\n",
      "Epoch 2589, Loss: 0.12138300389051437, Final Batch Loss: 0.05815618485212326\n",
      "Epoch 2590, Loss: 0.13602736219763756, Final Batch Loss: 0.06174108758568764\n",
      "Epoch 2591, Loss: 0.28491898626089096, Final Batch Loss: 0.2161511331796646\n",
      "Epoch 2592, Loss: 0.16051924973726273, Final Batch Loss: 0.06532229483127594\n",
      "Epoch 2593, Loss: 0.1496702991425991, Final Batch Loss: 0.049318987876176834\n",
      "Epoch 2594, Loss: 0.12470941618084908, Final Batch Loss: 0.04047618433833122\n",
      "Epoch 2595, Loss: 0.13448775187134743, Final Batch Loss: 0.04719151183962822\n",
      "Epoch 2596, Loss: 0.07214292325079441, Final Batch Loss: 0.023423941805958748\n",
      "Epoch 2597, Loss: 0.16687099635601044, Final Batch Loss: 0.1132638230919838\n",
      "Epoch 2598, Loss: 0.14438565075397491, Final Batch Loss: 0.07227768748998642\n",
      "Epoch 2599, Loss: 0.22400401532649994, Final Batch Loss: 0.11575966328382492\n",
      "Epoch 2600, Loss: 0.12624534592032433, Final Batch Loss: 0.06839901208877563\n",
      "Epoch 2601, Loss: 0.16671831905841827, Final Batch Loss: 0.0752079039812088\n",
      "Epoch 2602, Loss: 0.10470205172896385, Final Batch Loss: 0.036240559071302414\n",
      "Epoch 2603, Loss: 0.10138611868023872, Final Batch Loss: 0.0370217002928257\n",
      "Epoch 2604, Loss: 0.09362467005848885, Final Batch Loss: 0.04274687170982361\n",
      "Epoch 2605, Loss: 0.09811543114483356, Final Batch Loss: 0.021740416064858437\n",
      "Epoch 2606, Loss: 0.07282228581607342, Final Batch Loss: 0.029606139287352562\n",
      "Epoch 2607, Loss: 0.07693155109882355, Final Batch Loss: 0.021935638040304184\n",
      "Epoch 2608, Loss: 0.16289472579956055, Final Batch Loss: 0.09459509700536728\n",
      "Epoch 2609, Loss: 0.21141311526298523, Final Batch Loss: 0.13074181973934174\n",
      "Epoch 2610, Loss: 0.13192739337682724, Final Batch Loss: 0.07114020735025406\n",
      "Epoch 2611, Loss: 0.10945942252874374, Final Batch Loss: 0.0418197438120842\n",
      "Epoch 2612, Loss: 0.09403754398226738, Final Batch Loss: 0.04998985677957535\n",
      "Epoch 2613, Loss: 0.14518408849835396, Final Batch Loss: 0.08729269355535507\n",
      "Epoch 2614, Loss: 0.12333161011338234, Final Batch Loss: 0.04285163804888725\n",
      "Epoch 2615, Loss: 0.0980321578681469, Final Batch Loss: 0.04571276158094406\n",
      "Epoch 2616, Loss: 0.1050287876278162, Final Batch Loss: 0.07489944249391556\n",
      "Epoch 2617, Loss: 0.12023380026221275, Final Batch Loss: 0.03310355916619301\n",
      "Epoch 2618, Loss: 0.10372013226151466, Final Batch Loss: 0.03477538749575615\n",
      "Epoch 2619, Loss: 0.08565516956150532, Final Batch Loss: 0.02105705626308918\n",
      "Epoch 2620, Loss: 0.06896529346704483, Final Batch Loss: 0.03630952537059784\n",
      "Epoch 2621, Loss: 0.09010574221611023, Final Batch Loss: 0.057600319385528564\n",
      "Epoch 2622, Loss: 0.1107599250972271, Final Batch Loss: 0.042983170598745346\n",
      "Epoch 2623, Loss: 0.12130715698003769, Final Batch Loss: 0.07348137348890305\n",
      "Epoch 2624, Loss: 0.10984385758638382, Final Batch Loss: 0.05772339180111885\n",
      "Epoch 2625, Loss: 0.11598034203052521, Final Batch Loss: 0.031908780336380005\n",
      "Epoch 2626, Loss: 0.21727174520492554, Final Batch Loss: 0.12803731858730316\n",
      "Epoch 2627, Loss: 0.3810838423669338, Final Batch Loss: 0.3499498665332794\n",
      "Epoch 2628, Loss: 0.1143735833466053, Final Batch Loss: 0.04394273832440376\n",
      "Epoch 2629, Loss: 0.12335975468158722, Final Batch Loss: 0.047466009855270386\n",
      "Epoch 2630, Loss: 0.11312506720423698, Final Batch Loss: 0.054487068206071854\n",
      "Epoch 2631, Loss: 0.09233718737959862, Final Batch Loss: 0.03154124319553375\n",
      "Epoch 2632, Loss: 0.0755129512399435, Final Batch Loss: 0.024287009611725807\n",
      "Epoch 2633, Loss: 0.10533788613975048, Final Batch Loss: 0.028421951457858086\n",
      "Epoch 2634, Loss: 0.11039284244179726, Final Batch Loss: 0.060115307569503784\n",
      "Epoch 2635, Loss: 0.1631273813545704, Final Batch Loss: 0.10439805686473846\n",
      "Epoch 2636, Loss: 0.08995872363448143, Final Batch Loss: 0.026021581143140793\n",
      "Epoch 2637, Loss: 0.08561618626117706, Final Batch Loss: 0.04659895598888397\n",
      "Epoch 2638, Loss: 0.08777840435504913, Final Batch Loss: 0.053722627460956573\n",
      "Epoch 2639, Loss: 0.13613127171993256, Final Batch Loss: 0.05753599852323532\n",
      "Epoch 2640, Loss: 0.07879219576716423, Final Batch Loss: 0.032602328807115555\n",
      "Epoch 2641, Loss: 0.07496457174420357, Final Batch Loss: 0.036180637776851654\n",
      "Epoch 2642, Loss: 0.07437152788043022, Final Batch Loss: 0.027817733585834503\n",
      "Epoch 2643, Loss: 0.11855104938149452, Final Batch Loss: 0.05543292686343193\n",
      "Epoch 2644, Loss: 0.07878337055444717, Final Batch Loss: 0.03202856332063675\n",
      "Epoch 2645, Loss: 0.044293954968452454, Final Batch Loss: 0.013191845268011093\n",
      "Epoch 2646, Loss: 0.08315666392445564, Final Batch Loss: 0.03853964805603027\n",
      "Epoch 2647, Loss: 0.14698275923728943, Final Batch Loss: 0.0725349634885788\n",
      "Epoch 2648, Loss: 0.125786654651165, Final Batch Loss: 0.06932543963193893\n",
      "Epoch 2649, Loss: 0.08944505825638771, Final Batch Loss: 0.04794842377305031\n",
      "Epoch 2650, Loss: 0.09171263128519058, Final Batch Loss: 0.026181720197200775\n",
      "Epoch 2651, Loss: 0.11585492268204689, Final Batch Loss: 0.07413668185472488\n",
      "Epoch 2652, Loss: 0.04803035594522953, Final Batch Loss: 0.017000921070575714\n",
      "Epoch 2653, Loss: 0.13774267956614494, Final Batch Loss: 0.0572139210999012\n",
      "Epoch 2654, Loss: 0.09329532459378242, Final Batch Loss: 0.04921490699052811\n",
      "Epoch 2655, Loss: 0.1696593277156353, Final Batch Loss: 0.10961432009935379\n",
      "Epoch 2656, Loss: 0.09834156930446625, Final Batch Loss: 0.03704032674431801\n",
      "Epoch 2657, Loss: 0.07645002566277981, Final Batch Loss: 0.02791658602654934\n",
      "Epoch 2658, Loss: 0.19088321924209595, Final Batch Loss: 0.11733889579772949\n",
      "Epoch 2659, Loss: 0.06362010538578033, Final Batch Loss: 0.02442379668354988\n",
      "Epoch 2660, Loss: 0.12770451791584492, Final Batch Loss: 0.03067423589527607\n",
      "Epoch 2661, Loss: 0.11639945581555367, Final Batch Loss: 0.04136187210679054\n",
      "Epoch 2662, Loss: 0.10222825407981873, Final Batch Loss: 0.04123705253005028\n",
      "Epoch 2663, Loss: 0.10039376094937325, Final Batch Loss: 0.029195930808782578\n",
      "Epoch 2664, Loss: 0.23703718930482864, Final Batch Loss: 0.1812460720539093\n",
      "Epoch 2665, Loss: 0.10075313225388527, Final Batch Loss: 0.06203154847025871\n",
      "Epoch 2666, Loss: 0.1638421192765236, Final Batch Loss: 0.1023782417178154\n",
      "Epoch 2667, Loss: 0.16179925203323364, Final Batch Loss: 0.0882096216082573\n",
      "Epoch 2668, Loss: 0.16751305758953094, Final Batch Loss: 0.10878784209489822\n",
      "Epoch 2669, Loss: 0.08455714955925941, Final Batch Loss: 0.03469215705990791\n",
      "Epoch 2670, Loss: 0.12155387178063393, Final Batch Loss: 0.03727765753865242\n",
      "Epoch 2671, Loss: 0.5454417392611504, Final Batch Loss: 0.4677139222621918\n",
      "Epoch 2672, Loss: 0.1246974729001522, Final Batch Loss: 0.03879622742533684\n",
      "Epoch 2673, Loss: 0.11218305490911007, Final Batch Loss: 0.026288015767931938\n",
      "Epoch 2674, Loss: 0.09775368496775627, Final Batch Loss: 0.07045428454875946\n",
      "Epoch 2675, Loss: 0.1573008242994547, Final Batch Loss: 0.12608566880226135\n",
      "Epoch 2676, Loss: 0.0689084567129612, Final Batch Loss: 0.034471288323402405\n",
      "Epoch 2677, Loss: 0.07533627189695835, Final Batch Loss: 0.022390345111489296\n",
      "Epoch 2678, Loss: 0.07227823883295059, Final Batch Loss: 0.026637032628059387\n",
      "Epoch 2679, Loss: 0.33598846569657326, Final Batch Loss: 0.04532349482178688\n",
      "Epoch 2680, Loss: 0.19902391731739044, Final Batch Loss: 0.11641249060630798\n",
      "Epoch 2681, Loss: 0.16096721589565277, Final Batch Loss: 0.1078186184167862\n",
      "Epoch 2682, Loss: 0.15527080744504929, Final Batch Loss: 0.07359213382005692\n",
      "Epoch 2683, Loss: 0.14574293047189713, Final Batch Loss: 0.08743326365947723\n",
      "Epoch 2684, Loss: 0.17380303516983986, Final Batch Loss: 0.13042853772640228\n",
      "Epoch 2685, Loss: 0.15968742221593857, Final Batch Loss: 0.08082392811775208\n",
      "Epoch 2686, Loss: 0.07983304373919964, Final Batch Loss: 0.018836049363017082\n",
      "Epoch 2687, Loss: 0.07426193729043007, Final Batch Loss: 0.03855153173208237\n",
      "Epoch 2688, Loss: 0.09770531207323074, Final Batch Loss: 0.022953063249588013\n",
      "Epoch 2689, Loss: 0.12183769419789314, Final Batch Loss: 0.056040357798337936\n",
      "Epoch 2690, Loss: 0.13264408707618713, Final Batch Loss: 0.10181694477796555\n",
      "Epoch 2691, Loss: 0.08610035479068756, Final Batch Loss: 0.03810160979628563\n",
      "Epoch 2692, Loss: 0.06935822777450085, Final Batch Loss: 0.020551493391394615\n",
      "Epoch 2693, Loss: 0.137923464179039, Final Batch Loss: 0.08200760930776596\n",
      "Epoch 2694, Loss: 0.09988470003008842, Final Batch Loss: 0.05062803626060486\n",
      "Epoch 2695, Loss: 0.12536946311593056, Final Batch Loss: 0.047270696610212326\n",
      "Epoch 2696, Loss: 0.19465326517820358, Final Batch Loss: 0.12166700512170792\n",
      "Epoch 2697, Loss: 0.1024150401353836, Final Batch Loss: 0.0570542998611927\n",
      "Epoch 2698, Loss: 0.16002409532666206, Final Batch Loss: 0.10749027132987976\n",
      "Epoch 2699, Loss: 0.07133919559419155, Final Batch Loss: 0.044744059443473816\n",
      "Epoch 2700, Loss: 0.09982797876000404, Final Batch Loss: 0.0507713258266449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2701, Loss: 0.05879770312458277, Final Batch Loss: 0.014365450479090214\n",
      "Epoch 2702, Loss: 0.07176072709262371, Final Batch Loss: 0.027833977714180946\n",
      "Epoch 2703, Loss: 0.09292639046907425, Final Batch Loss: 0.03148970380425453\n",
      "Epoch 2704, Loss: 0.11206416040658951, Final Batch Loss: 0.029408156871795654\n",
      "Epoch 2705, Loss: 0.06466561742126942, Final Batch Loss: 0.028013864532113075\n",
      "Epoch 2706, Loss: 0.08352157846093178, Final Batch Loss: 0.043552789837121964\n",
      "Epoch 2707, Loss: 0.09042155742645264, Final Batch Loss: 0.05528837442398071\n",
      "Epoch 2708, Loss: 0.06828219257295132, Final Batch Loss: 0.02422899566590786\n",
      "Epoch 2709, Loss: 0.04484158009290695, Final Batch Loss: 0.014207826927304268\n",
      "Epoch 2710, Loss: 0.15774215385317802, Final Batch Loss: 0.04637545719742775\n",
      "Epoch 2711, Loss: 0.09969416633248329, Final Batch Loss: 0.040715292096138\n",
      "Epoch 2712, Loss: 0.10065600275993347, Final Batch Loss: 0.068222276866436\n",
      "Epoch 2713, Loss: 0.1387650966644287, Final Batch Loss: 0.08742213994264603\n",
      "Epoch 2714, Loss: 0.09316901676356792, Final Batch Loss: 0.027192896232008934\n",
      "Epoch 2715, Loss: 0.07588588818907738, Final Batch Loss: 0.031354162842035294\n",
      "Epoch 2716, Loss: 0.1589556634426117, Final Batch Loss: 0.12160409986972809\n",
      "Epoch 2717, Loss: 0.16966240853071213, Final Batch Loss: 0.10332606732845306\n",
      "Epoch 2718, Loss: 0.10087385028600693, Final Batch Loss: 0.0575052946805954\n",
      "Epoch 2719, Loss: 0.21313080191612244, Final Batch Loss: 0.14696568250656128\n",
      "Epoch 2720, Loss: 0.17380552738904953, Final Batch Loss: 0.12155630439519882\n",
      "Epoch 2721, Loss: 0.1862480416893959, Final Batch Loss: 0.04849798232316971\n",
      "Epoch 2722, Loss: 0.1595245748758316, Final Batch Loss: 0.06546797603368759\n",
      "Epoch 2723, Loss: 0.17642126977443695, Final Batch Loss: 0.071424201130867\n",
      "Epoch 2724, Loss: 0.18450909852981567, Final Batch Loss: 0.08890482038259506\n",
      "Epoch 2725, Loss: 0.11823775619268417, Final Batch Loss: 0.025342032313346863\n",
      "Epoch 2726, Loss: 0.12031789496541023, Final Batch Loss: 0.031016085296869278\n",
      "Epoch 2727, Loss: 0.08449152112007141, Final Batch Loss: 0.03712994232773781\n",
      "Epoch 2728, Loss: 0.07739916071295738, Final Batch Loss: 0.03780758008360863\n",
      "Epoch 2729, Loss: 0.20275868847966194, Final Batch Loss: 0.15064644813537598\n",
      "Epoch 2730, Loss: 0.06078440137207508, Final Batch Loss: 0.02985909953713417\n",
      "Epoch 2731, Loss: 0.12254355847835541, Final Batch Loss: 0.026309683918952942\n",
      "Epoch 2732, Loss: 0.13267124071717262, Final Batch Loss: 0.05420694872736931\n",
      "Epoch 2733, Loss: 0.07034831214696169, Final Batch Loss: 0.012647085823118687\n",
      "Epoch 2734, Loss: 0.15675495564937592, Final Batch Loss: 0.12244582176208496\n",
      "Epoch 2735, Loss: 0.08063288405537605, Final Batch Loss: 0.03235558047890663\n",
      "Epoch 2736, Loss: 0.1072387546300888, Final Batch Loss: 0.05251707509160042\n",
      "Epoch 2737, Loss: 0.1259712278842926, Final Batch Loss: 0.08710100501775742\n",
      "Epoch 2738, Loss: 0.13695626333355904, Final Batch Loss: 0.09108787775039673\n",
      "Epoch 2739, Loss: 0.14300157129764557, Final Batch Loss: 0.04906085133552551\n",
      "Epoch 2740, Loss: 0.09338178485631943, Final Batch Loss: 0.03267700597643852\n",
      "Epoch 2741, Loss: 0.06711471639573574, Final Batch Loss: 0.029531488195061684\n",
      "Epoch 2742, Loss: 0.12529919296503067, Final Batch Loss: 0.08758509159088135\n",
      "Epoch 2743, Loss: 0.07680334709584713, Final Batch Loss: 0.024117102846503258\n",
      "Epoch 2744, Loss: 0.11581204831600189, Final Batch Loss: 0.02960365265607834\n",
      "Epoch 2745, Loss: 0.13240793719887733, Final Batch Loss: 0.10082411766052246\n",
      "Epoch 2746, Loss: 0.07795247249305248, Final Batch Loss: 0.04944171756505966\n",
      "Epoch 2747, Loss: 0.060906143859028816, Final Batch Loss: 0.03284318372607231\n",
      "Epoch 2748, Loss: 0.10158158093690872, Final Batch Loss: 0.05616816133260727\n",
      "Epoch 2749, Loss: 0.08511849120259285, Final Batch Loss: 0.03911355510354042\n",
      "Epoch 2750, Loss: 0.07774641178548336, Final Batch Loss: 0.03062550537288189\n",
      "Epoch 2751, Loss: 0.07592561841011047, Final Batch Loss: 0.0323699414730072\n",
      "Epoch 2752, Loss: 0.0969107635319233, Final Batch Loss: 0.03603591397404671\n",
      "Epoch 2753, Loss: 0.08210163190960884, Final Batch Loss: 0.023607656359672546\n",
      "Epoch 2754, Loss: 0.14647278934717178, Final Batch Loss: 0.10104896128177643\n",
      "Epoch 2755, Loss: 0.15185808390378952, Final Batch Loss: 0.0874653309583664\n",
      "Epoch 2756, Loss: 0.06767995841801167, Final Batch Loss: 0.025462640449404716\n",
      "Epoch 2757, Loss: 0.09888487122952938, Final Batch Loss: 0.022792762145400047\n",
      "Epoch 2758, Loss: 0.06289078667759895, Final Batch Loss: 0.042062755674123764\n",
      "Epoch 2759, Loss: 0.11077534779906273, Final Batch Loss: 0.043980713933706284\n",
      "Epoch 2760, Loss: 0.1094471774995327, Final Batch Loss: 0.04038744792342186\n",
      "Epoch 2761, Loss: 0.10726041346788406, Final Batch Loss: 0.06712018698453903\n",
      "Epoch 2762, Loss: 0.07077789679169655, Final Batch Loss: 0.03281421959400177\n",
      "Epoch 2763, Loss: 0.11139146238565445, Final Batch Loss: 0.045871131122112274\n",
      "Epoch 2764, Loss: 0.07652141898870468, Final Batch Loss: 0.01998186856508255\n",
      "Epoch 2765, Loss: 0.1433512456715107, Final Batch Loss: 0.08586444705724716\n",
      "Epoch 2766, Loss: 0.05713607929646969, Final Batch Loss: 0.02485978789627552\n",
      "Epoch 2767, Loss: 0.12109450995922089, Final Batch Loss: 0.0558471754193306\n",
      "Epoch 2768, Loss: 0.16940277069807053, Final Batch Loss: 0.07778237015008926\n",
      "Epoch 2769, Loss: 0.10101351514458656, Final Batch Loss: 0.05857893452048302\n",
      "Epoch 2770, Loss: 0.10411187261343002, Final Batch Loss: 0.048812419176101685\n",
      "Epoch 2771, Loss: 0.07538328319787979, Final Batch Loss: 0.03717302531003952\n",
      "Epoch 2772, Loss: 0.07554225996136665, Final Batch Loss: 0.03594256937503815\n",
      "Epoch 2773, Loss: 0.09540751576423645, Final Batch Loss: 0.038576941937208176\n",
      "Epoch 2774, Loss: 0.07469311356544495, Final Batch Loss: 0.024734046310186386\n",
      "Epoch 2775, Loss: 0.0845482386648655, Final Batch Loss: 0.033316515386104584\n",
      "Epoch 2776, Loss: 0.1349206194281578, Final Batch Loss: 0.06599409878253937\n",
      "Epoch 2777, Loss: 0.1127719022333622, Final Batch Loss: 0.03438420966267586\n",
      "Epoch 2778, Loss: 0.11561751365661621, Final Batch Loss: 0.03982102870941162\n",
      "Epoch 2779, Loss: 0.05900421924889088, Final Batch Loss: 0.015418073162436485\n",
      "Epoch 2780, Loss: 0.11148940399289131, Final Batch Loss: 0.054202862083911896\n",
      "Epoch 2781, Loss: 0.07818539440631866, Final Batch Loss: 0.04184040427207947\n",
      "Epoch 2782, Loss: 0.07231473550200462, Final Batch Loss: 0.036035895347595215\n",
      "Epoch 2783, Loss: 0.08946546539664268, Final Batch Loss: 0.04457249492406845\n",
      "Epoch 2784, Loss: 0.08368239365518093, Final Batch Loss: 0.028181245550513268\n",
      "Epoch 2785, Loss: 0.18809223547577858, Final Batch Loss: 0.1472601443529129\n",
      "Epoch 2786, Loss: 0.08725712075829506, Final Batch Loss: 0.0366700105369091\n",
      "Epoch 2787, Loss: 0.07962568290531635, Final Batch Loss: 0.013356724753975868\n",
      "Epoch 2788, Loss: 0.10364267602562904, Final Batch Loss: 0.047843340784311295\n",
      "Epoch 2789, Loss: 0.059972554445266724, Final Batch Loss: 0.02423054724931717\n",
      "Epoch 2790, Loss: 0.07512442767620087, Final Batch Loss: 0.0366080142557621\n",
      "Epoch 2791, Loss: 0.161879513412714, Final Batch Loss: 0.04216570034623146\n",
      "Epoch 2792, Loss: 0.13285624235868454, Final Batch Loss: 0.05608775466680527\n",
      "Epoch 2793, Loss: 0.08365261182188988, Final Batch Loss: 0.023070525377988815\n",
      "Epoch 2794, Loss: 0.19028469175100327, Final Batch Loss: 0.09912765771150589\n",
      "Epoch 2795, Loss: 0.10863212123513222, Final Batch Loss: 0.06965111941099167\n",
      "Epoch 2796, Loss: 0.12931155785918236, Final Batch Loss: 0.08152883499860764\n",
      "Epoch 2797, Loss: 0.05697675608098507, Final Batch Loss: 0.0246994998306036\n",
      "Epoch 2798, Loss: 0.06474813632667065, Final Batch Loss: 0.03461098298430443\n",
      "Epoch 2799, Loss: 0.12417458556592464, Final Batch Loss: 0.02460390143096447\n",
      "Epoch 2800, Loss: 0.1290850043296814, Final Batch Loss: 0.05951884388923645\n",
      "Epoch 2801, Loss: 0.07866248488426208, Final Batch Loss: 0.031625986099243164\n",
      "Epoch 2802, Loss: 0.05241137929260731, Final Batch Loss: 0.029869966208934784\n",
      "Epoch 2803, Loss: 0.13371405005455017, Final Batch Loss: 0.04937126487493515\n",
      "Epoch 2804, Loss: 0.07073380425572395, Final Batch Loss: 0.028536513447761536\n",
      "Epoch 2805, Loss: 0.16097765415906906, Final Batch Loss: 0.07613669335842133\n",
      "Epoch 2806, Loss: 0.0998338907957077, Final Batch Loss: 0.0640643835067749\n",
      "Epoch 2807, Loss: 0.12850401178002357, Final Batch Loss: 0.08145029842853546\n",
      "Epoch 2808, Loss: 0.09546077996492386, Final Batch Loss: 0.03205588459968567\n",
      "Epoch 2809, Loss: 0.07776725571602583, Final Batch Loss: 0.015016417019069195\n",
      "Epoch 2810, Loss: 0.12376393005251884, Final Batch Loss: 0.041384268552064896\n",
      "Epoch 2811, Loss: 0.0711359791457653, Final Batch Loss: 0.03371880203485489\n",
      "Epoch 2812, Loss: 0.0631936565041542, Final Batch Loss: 0.017590437084436417\n",
      "Epoch 2813, Loss: 0.14931552857160568, Final Batch Loss: 0.10560359060764313\n",
      "Epoch 2814, Loss: 0.11173106357455254, Final Batch Loss: 0.05851871892809868\n",
      "Epoch 2815, Loss: 0.0886838361620903, Final Batch Loss: 0.02838454768061638\n",
      "Epoch 2816, Loss: 0.10286248475313187, Final Batch Loss: 0.0385899618268013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2817, Loss: 0.086881622672081, Final Batch Loss: 0.03307262808084488\n",
      "Epoch 2818, Loss: 0.0832757018506527, Final Batch Loss: 0.035157401114702225\n",
      "Epoch 2819, Loss: 0.11064337566494942, Final Batch Loss: 0.06905853748321533\n",
      "Epoch 2820, Loss: 0.06873504258692265, Final Batch Loss: 0.037918951362371445\n",
      "Epoch 2821, Loss: 0.08406035602092743, Final Batch Loss: 0.04811936989426613\n",
      "Epoch 2822, Loss: 0.04643734358251095, Final Batch Loss: 0.016369299963116646\n",
      "Epoch 2823, Loss: 0.18171048164367676, Final Batch Loss: 0.09864037483930588\n",
      "Epoch 2824, Loss: 0.09537743777036667, Final Batch Loss: 0.0447949580848217\n",
      "Epoch 2825, Loss: 0.13772041350603104, Final Batch Loss: 0.07565603405237198\n",
      "Epoch 2826, Loss: 0.07823043689131737, Final Batch Loss: 0.03843357786536217\n",
      "Epoch 2827, Loss: 0.11826450005173683, Final Batch Loss: 0.037316109985113144\n",
      "Epoch 2828, Loss: 0.09999709948897362, Final Batch Loss: 0.023513678461313248\n",
      "Epoch 2829, Loss: 0.07751436531543732, Final Batch Loss: 0.021733030676841736\n",
      "Epoch 2830, Loss: 0.1382531002163887, Final Batch Loss: 0.0697699710726738\n",
      "Epoch 2831, Loss: 0.2315252721309662, Final Batch Loss: 0.15412066876888275\n",
      "Epoch 2832, Loss: 0.06520011462271214, Final Batch Loss: 0.025060823187232018\n",
      "Epoch 2833, Loss: 0.10135095939040184, Final Batch Loss: 0.05759245529770851\n",
      "Epoch 2834, Loss: 0.15161864832043648, Final Batch Loss: 0.11382096260786057\n",
      "Epoch 2835, Loss: 0.07282314822077751, Final Batch Loss: 0.02067328244447708\n",
      "Epoch 2836, Loss: 0.08111970499157906, Final Batch Loss: 0.023866143077611923\n",
      "Epoch 2837, Loss: 0.321140818297863, Final Batch Loss: 0.23116855323314667\n",
      "Epoch 2838, Loss: 0.11042552813887596, Final Batch Loss: 0.07742463052272797\n",
      "Epoch 2839, Loss: 0.07844979129731655, Final Batch Loss: 0.049865514039993286\n",
      "Epoch 2840, Loss: 0.21486994996666908, Final Batch Loss: 0.16557398438453674\n",
      "Epoch 2841, Loss: 0.057289063930511475, Final Batch Loss: 0.021093785762786865\n",
      "Epoch 2842, Loss: 0.07471595332026482, Final Batch Loss: 0.04303566366434097\n",
      "Epoch 2843, Loss: 0.16990100592374802, Final Batch Loss: 0.10335593670606613\n",
      "Epoch 2844, Loss: 0.14301858842372894, Final Batch Loss: 0.06470107287168503\n",
      "Epoch 2845, Loss: 0.08670957572758198, Final Batch Loss: 0.06638488918542862\n",
      "Epoch 2846, Loss: 0.0768485888838768, Final Batch Loss: 0.02137186750769615\n",
      "Epoch 2847, Loss: 0.07748530060052872, Final Batch Loss: 0.045565493404865265\n",
      "Epoch 2848, Loss: 0.0969395637512207, Final Batch Loss: 0.045563723891973495\n",
      "Epoch 2849, Loss: 0.08429818600416183, Final Batch Loss: 0.016091905534267426\n",
      "Epoch 2850, Loss: 0.15544453635811806, Final Batch Loss: 0.06057354435324669\n",
      "Epoch 2851, Loss: 0.15961656719446182, Final Batch Loss: 0.08262339234352112\n",
      "Epoch 2852, Loss: 0.09174023196101189, Final Batch Loss: 0.01795734092593193\n",
      "Epoch 2853, Loss: 0.10984204895794392, Final Batch Loss: 0.08036583662033081\n",
      "Epoch 2854, Loss: 0.13246532529592514, Final Batch Loss: 0.04992050677537918\n",
      "Epoch 2855, Loss: 0.15779157727956772, Final Batch Loss: 0.07990185171365738\n",
      "Epoch 2856, Loss: 0.09173126891255379, Final Batch Loss: 0.03688277304172516\n",
      "Epoch 2857, Loss: 0.06903859041631222, Final Batch Loss: 0.01945415697991848\n",
      "Epoch 2858, Loss: 0.0979440025985241, Final Batch Loss: 0.047211308032274246\n",
      "Epoch 2859, Loss: 0.0661950446665287, Final Batch Loss: 0.025106020271778107\n",
      "Epoch 2860, Loss: 0.0800472404807806, Final Batch Loss: 0.029091836884617805\n",
      "Epoch 2861, Loss: 0.11363506689667702, Final Batch Loss: 0.06311533600091934\n",
      "Epoch 2862, Loss: 0.08977751806378365, Final Batch Loss: 0.045713555067777634\n",
      "Epoch 2863, Loss: 0.06473226100206375, Final Batch Loss: 0.025972507894039154\n",
      "Epoch 2864, Loss: 0.16364402323961258, Final Batch Loss: 0.10177216678857803\n",
      "Epoch 2865, Loss: 0.2349122017621994, Final Batch Loss: 0.16057293117046356\n",
      "Epoch 2866, Loss: 0.09981206431984901, Final Batch Loss: 0.06033025681972504\n",
      "Epoch 2867, Loss: 0.11495665460824966, Final Batch Loss: 0.07643072307109833\n",
      "Epoch 2868, Loss: 0.06782360188663006, Final Batch Loss: 0.026796745136380196\n",
      "Epoch 2869, Loss: 0.18041878938674927, Final Batch Loss: 0.1390731781721115\n",
      "Epoch 2870, Loss: 0.05463552800938487, Final Batch Loss: 0.007640823256224394\n",
      "Epoch 2871, Loss: 0.06061186036095023, Final Batch Loss: 0.006006357725709677\n",
      "Epoch 2872, Loss: 0.2141148168593645, Final Batch Loss: 0.18443232774734497\n",
      "Epoch 2873, Loss: 0.0585181750357151, Final Batch Loss: 0.029744254425168037\n",
      "Epoch 2874, Loss: 0.06692018359899521, Final Batch Loss: 0.03226922079920769\n",
      "Epoch 2875, Loss: 0.05738074705004692, Final Batch Loss: 0.01885119453072548\n",
      "Epoch 2876, Loss: 0.10927347838878632, Final Batch Loss: 0.05087566375732422\n",
      "Epoch 2877, Loss: 0.07248115912079811, Final Batch Loss: 0.02233731746673584\n",
      "Epoch 2878, Loss: 0.07334467768669128, Final Batch Loss: 0.035935480147600174\n",
      "Epoch 2879, Loss: 0.1407218724489212, Final Batch Loss: 0.08353226631879807\n",
      "Epoch 2880, Loss: 0.08134283311665058, Final Batch Loss: 0.028807049617171288\n",
      "Epoch 2881, Loss: 0.08632994443178177, Final Batch Loss: 0.02830919623374939\n",
      "Epoch 2882, Loss: 0.14442078769207, Final Batch Loss: 0.10838620364665985\n",
      "Epoch 2883, Loss: 0.07878490723669529, Final Batch Loss: 0.023509105667471886\n",
      "Epoch 2884, Loss: 0.09374057129025459, Final Batch Loss: 0.052333928644657135\n",
      "Epoch 2885, Loss: 0.21599652990698814, Final Batch Loss: 0.1588628590106964\n",
      "Epoch 2886, Loss: 0.12903527170419693, Final Batch Loss: 0.089949831366539\n",
      "Epoch 2887, Loss: 0.1368178427219391, Final Batch Loss: 0.06866224855184555\n",
      "Epoch 2888, Loss: 0.1104925163090229, Final Batch Loss: 0.06746421754360199\n",
      "Epoch 2889, Loss: 0.11090751737356186, Final Batch Loss: 0.06704697757959366\n",
      "Epoch 2890, Loss: 0.08756653219461441, Final Batch Loss: 0.03390856087207794\n",
      "Epoch 2891, Loss: 0.11594166606664658, Final Batch Loss: 0.07657934725284576\n",
      "Epoch 2892, Loss: 0.08233224600553513, Final Batch Loss: 0.04852067679166794\n",
      "Epoch 2893, Loss: 0.0731815006583929, Final Batch Loss: 0.025529859587550163\n",
      "Epoch 2894, Loss: 0.06322532892227173, Final Batch Loss: 0.027562834322452545\n",
      "Epoch 2895, Loss: 0.09310041554272175, Final Batch Loss: 0.02927250601351261\n",
      "Epoch 2896, Loss: 0.06862673535943031, Final Batch Loss: 0.022968266159296036\n",
      "Epoch 2897, Loss: 0.07632731273770332, Final Batch Loss: 0.044894907623529434\n",
      "Epoch 2898, Loss: 0.25099509954452515, Final Batch Loss: 0.14073896408081055\n",
      "Epoch 2899, Loss: 0.1048655416816473, Final Batch Loss: 0.029028503224253654\n",
      "Epoch 2900, Loss: 0.1231754869222641, Final Batch Loss: 0.08760469406843185\n",
      "Epoch 2901, Loss: 0.07427864521741867, Final Batch Loss: 0.023855872452259064\n",
      "Epoch 2902, Loss: 0.11215054616332054, Final Batch Loss: 0.05246379226446152\n",
      "Epoch 2903, Loss: 0.05481026880443096, Final Batch Loss: 0.023425696417689323\n",
      "Epoch 2904, Loss: 0.07074378989636898, Final Batch Loss: 0.04494774341583252\n",
      "Epoch 2905, Loss: 0.11380764469504356, Final Batch Loss: 0.015178341418504715\n",
      "Epoch 2906, Loss: 0.1512448899447918, Final Batch Loss: 0.10602787137031555\n",
      "Epoch 2907, Loss: 0.14461173117160797, Final Batch Loss: 0.09018528461456299\n",
      "Epoch 2908, Loss: 0.12734439224004745, Final Batch Loss: 0.05130831152200699\n",
      "Epoch 2909, Loss: 0.15228716656565666, Final Batch Loss: 0.10015024244785309\n",
      "Epoch 2910, Loss: 0.08163784071803093, Final Batch Loss: 0.034856535494327545\n",
      "Epoch 2911, Loss: 0.06966940872371197, Final Batch Loss: 0.028017627075314522\n",
      "Epoch 2912, Loss: 0.11896734684705734, Final Batch Loss: 0.06891050934791565\n",
      "Epoch 2913, Loss: 0.08181127905845642, Final Batch Loss: 0.022731002420186996\n",
      "Epoch 2914, Loss: 0.1383380927145481, Final Batch Loss: 0.05911566689610481\n",
      "Epoch 2915, Loss: 0.14306895062327385, Final Batch Loss: 0.08772411197423935\n",
      "Epoch 2916, Loss: 0.16155079752206802, Final Batch Loss: 0.12264588475227356\n",
      "Epoch 2917, Loss: 0.09652346558868885, Final Batch Loss: 0.06982729583978653\n",
      "Epoch 2918, Loss: 0.09082327783107758, Final Batch Loss: 0.0513199083507061\n",
      "Epoch 2919, Loss: 0.11949959024786949, Final Batch Loss: 0.07507191598415375\n",
      "Epoch 2920, Loss: 0.20167360454797745, Final Batch Loss: 0.13170623779296875\n",
      "Epoch 2921, Loss: 0.09269939363002777, Final Batch Loss: 0.057231348007917404\n",
      "Epoch 2922, Loss: 0.1008295938372612, Final Batch Loss: 0.03652004152536392\n",
      "Epoch 2923, Loss: 0.09223451465368271, Final Batch Loss: 0.05955289676785469\n",
      "Epoch 2924, Loss: 0.1224311962723732, Final Batch Loss: 0.09265293926000595\n",
      "Epoch 2925, Loss: 0.13907042890787125, Final Batch Loss: 0.06870061904191971\n",
      "Epoch 2926, Loss: 0.1705649197101593, Final Batch Loss: 0.08455618470907211\n",
      "Epoch 2927, Loss: 0.12504864111542702, Final Batch Loss: 0.06175534799695015\n",
      "Epoch 2928, Loss: 0.11306233704090118, Final Batch Loss: 0.057198550552129745\n",
      "Epoch 2929, Loss: 0.10815554484724998, Final Batch Loss: 0.011136043816804886\n",
      "Epoch 2930, Loss: 0.11631659045815468, Final Batch Loss: 0.08498519659042358\n",
      "Epoch 2931, Loss: 0.06421190686523914, Final Batch Loss: 0.022730255499482155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2932, Loss: 0.09757084399461746, Final Batch Loss: 0.062091805040836334\n",
      "Epoch 2933, Loss: 0.1297097820788622, Final Batch Loss: 0.020976142957806587\n",
      "Epoch 2934, Loss: 0.1064261868596077, Final Batch Loss: 0.03382711112499237\n",
      "Epoch 2935, Loss: 0.04523231461644173, Final Batch Loss: 0.016580091789364815\n",
      "Epoch 2936, Loss: 0.05664518289268017, Final Batch Loss: 0.02271215058863163\n",
      "Epoch 2937, Loss: 0.1249607689678669, Final Batch Loss: 0.0843375101685524\n",
      "Epoch 2938, Loss: 0.1108873300254345, Final Batch Loss: 0.06106938049197197\n",
      "Epoch 2939, Loss: 0.07999824360013008, Final Batch Loss: 0.040151629596948624\n",
      "Epoch 2940, Loss: 0.1682889722287655, Final Batch Loss: 0.11875572055578232\n",
      "Epoch 2941, Loss: 0.07626470923423767, Final Batch Loss: 0.018583722412586212\n",
      "Epoch 2942, Loss: 0.093033067882061, Final Batch Loss: 0.024948537349700928\n",
      "Epoch 2943, Loss: 0.064430246129632, Final Batch Loss: 0.023899676278233528\n",
      "Epoch 2944, Loss: 0.10335879400372505, Final Batch Loss: 0.035179559141397476\n",
      "Epoch 2945, Loss: 0.09058432653546333, Final Batch Loss: 0.04097412899136543\n",
      "Epoch 2946, Loss: 0.051016682758927345, Final Batch Loss: 0.017548488453030586\n",
      "Epoch 2947, Loss: 0.10693194717168808, Final Batch Loss: 0.06354035437107086\n",
      "Epoch 2948, Loss: 0.11166239529848099, Final Batch Loss: 0.07737228274345398\n",
      "Epoch 2949, Loss: 0.05680335499346256, Final Batch Loss: 0.02103949524462223\n",
      "Epoch 2950, Loss: 0.1258629634976387, Final Batch Loss: 0.08118510246276855\n",
      "Epoch 2951, Loss: 0.05949169769883156, Final Batch Loss: 0.0374981127679348\n",
      "Epoch 2952, Loss: 0.14169441536068916, Final Batch Loss: 0.09713102132081985\n",
      "Epoch 2953, Loss: 0.07521477341651917, Final Batch Loss: 0.022298552095890045\n",
      "Epoch 2954, Loss: 0.0770008247345686, Final Batch Loss: 0.027623983100056648\n",
      "Epoch 2955, Loss: 0.147229116410017, Final Batch Loss: 0.11173846572637558\n",
      "Epoch 2956, Loss: 0.07833176292479038, Final Batch Loss: 0.02804652415215969\n",
      "Epoch 2957, Loss: 0.06931131053715944, Final Batch Loss: 0.012439203448593616\n",
      "Epoch 2958, Loss: 0.04651241563260555, Final Batch Loss: 0.01889822445809841\n",
      "Epoch 2959, Loss: 0.07585412263870239, Final Batch Loss: 0.019346125423908234\n",
      "Epoch 2960, Loss: 0.05662007816135883, Final Batch Loss: 0.014873998239636421\n",
      "Epoch 2961, Loss: 0.0515490397810936, Final Batch Loss: 0.018536344170570374\n",
      "Epoch 2962, Loss: 0.16603177785873413, Final Batch Loss: 0.1213383749127388\n",
      "Epoch 2963, Loss: 0.06441922299563885, Final Batch Loss: 0.04162945970892906\n",
      "Epoch 2964, Loss: 0.08496472239494324, Final Batch Loss: 0.008285962045192719\n",
      "Epoch 2965, Loss: 0.13685542345046997, Final Batch Loss: 0.04836618900299072\n",
      "Epoch 2966, Loss: 0.08550497516989708, Final Batch Loss: 0.04470008239150047\n",
      "Epoch 2967, Loss: 0.043334705755114555, Final Batch Loss: 0.014646852388978004\n",
      "Epoch 2968, Loss: 0.10016091167926788, Final Batch Loss: 0.041607651859521866\n",
      "Epoch 2969, Loss: 0.27736862003803253, Final Batch Loss: 0.23819948732852936\n",
      "Epoch 2970, Loss: 0.058120954781770706, Final Batch Loss: 0.025095481425523758\n",
      "Epoch 2971, Loss: 0.12258971855044365, Final Batch Loss: 0.06285998225212097\n",
      "Epoch 2972, Loss: 0.05757511593401432, Final Batch Loss: 0.022444793954491615\n",
      "Epoch 2973, Loss: 0.062571014277637, Final Batch Loss: 0.01308643352240324\n",
      "Epoch 2974, Loss: 0.1140185035765171, Final Batch Loss: 0.04966062679886818\n",
      "Epoch 2975, Loss: 0.06684080697596073, Final Batch Loss: 0.04243030399084091\n",
      "Epoch 2976, Loss: 0.11858600378036499, Final Batch Loss: 0.048027925193309784\n",
      "Epoch 2977, Loss: 0.0965348370373249, Final Batch Loss: 0.04050607979297638\n",
      "Epoch 2978, Loss: 0.10173012688755989, Final Batch Loss: 0.07544143497943878\n",
      "Epoch 2979, Loss: 0.08886918053030968, Final Batch Loss: 0.046626895666122437\n",
      "Epoch 2980, Loss: 0.11037345416843891, Final Batch Loss: 0.027237484231591225\n",
      "Epoch 2981, Loss: 0.08517525438219309, Final Batch Loss: 0.014616542495787144\n",
      "Epoch 2982, Loss: 0.13569750636816025, Final Batch Loss: 0.10239849984645844\n",
      "Epoch 2983, Loss: 0.07273732498288155, Final Batch Loss: 0.03531406447291374\n",
      "Epoch 2984, Loss: 0.07199133653193712, Final Batch Loss: 0.010130827315151691\n",
      "Epoch 2985, Loss: 0.14195330813527107, Final Batch Loss: 0.09159452468156815\n",
      "Epoch 2986, Loss: 0.12403959408402443, Final Batch Loss: 0.071419358253479\n",
      "Epoch 2987, Loss: 0.12515441328287125, Final Batch Loss: 0.05354974418878555\n",
      "Epoch 2988, Loss: 0.15288906171917915, Final Batch Loss: 0.09864264726638794\n",
      "Epoch 2989, Loss: 0.06792792305350304, Final Batch Loss: 0.024802416563034058\n",
      "Epoch 2990, Loss: 0.053234679624438286, Final Batch Loss: 0.024425184354186058\n",
      "Epoch 2991, Loss: 0.096703439950943, Final Batch Loss: 0.0414334274828434\n",
      "Epoch 2992, Loss: 0.09162726253271103, Final Batch Loss: 0.0330033004283905\n",
      "Epoch 2993, Loss: 0.07066472992300987, Final Batch Loss: 0.03111673891544342\n",
      "Epoch 2994, Loss: 0.060715085826814175, Final Batch Loss: 0.013211949728429317\n",
      "Epoch 2995, Loss: 0.06350252963602543, Final Batch Loss: 0.02133249305188656\n",
      "Epoch 2996, Loss: 0.17294568195939064, Final Batch Loss: 0.12197232246398926\n",
      "Epoch 2997, Loss: 0.042274586856365204, Final Batch Loss: 0.008069358766078949\n",
      "Epoch 2998, Loss: 0.064668670296669, Final Batch Loss: 0.023459531366825104\n",
      "Epoch 2999, Loss: 0.10031884908676147, Final Batch Loss: 0.06501638889312744\n",
      "Epoch 3000, Loss: 0.06384479440748692, Final Batch Loss: 0.03551902994513512\n",
      "Epoch 3001, Loss: 0.1036478541791439, Final Batch Loss: 0.04358338937163353\n",
      "Epoch 3002, Loss: 0.03628294449299574, Final Batch Loss: 0.015229909680783749\n",
      "Epoch 3003, Loss: 0.06132133677601814, Final Batch Loss: 0.013189662247896194\n",
      "Epoch 3004, Loss: 0.08585753478109837, Final Batch Loss: 0.024594387039542198\n",
      "Epoch 3005, Loss: 0.05446105636656284, Final Batch Loss: 0.020970119163393974\n",
      "Epoch 3006, Loss: 0.08430325984954834, Final Batch Loss: 0.057426583021879196\n",
      "Epoch 3007, Loss: 0.0696621099486947, Final Batch Loss: 0.01445274893194437\n",
      "Epoch 3008, Loss: 0.06753785163164139, Final Batch Loss: 0.025819048285484314\n",
      "Epoch 3009, Loss: 0.0743676945567131, Final Batch Loss: 0.05021996051073074\n",
      "Epoch 3010, Loss: 0.24925082176923752, Final Batch Loss: 0.18979983031749725\n",
      "Epoch 3011, Loss: 0.09446199610829353, Final Batch Loss: 0.05222143232822418\n",
      "Epoch 3012, Loss: 0.11651535704731941, Final Batch Loss: 0.05349571630358696\n",
      "Epoch 3013, Loss: 0.08771717548370361, Final Batch Loss: 0.03306952863931656\n",
      "Epoch 3014, Loss: 0.14019658975303173, Final Batch Loss: 0.11162155121564865\n",
      "Epoch 3015, Loss: 0.14757224172353745, Final Batch Loss: 0.10280971974134445\n",
      "Epoch 3016, Loss: 0.0378724280744791, Final Batch Loss: 0.011496925726532936\n",
      "Epoch 3017, Loss: 0.1184726282954216, Final Batch Loss: 0.08887174725532532\n",
      "Epoch 3018, Loss: 0.08372034505009651, Final Batch Loss: 0.033655062317848206\n",
      "Epoch 3019, Loss: 0.08048655092716217, Final Batch Loss: 0.03316694125533104\n",
      "Epoch 3020, Loss: 0.05538652464747429, Final Batch Loss: 0.01604287698864937\n",
      "Epoch 3021, Loss: 0.13835088163614273, Final Batch Loss: 0.0738341435790062\n",
      "Epoch 3022, Loss: 0.09391075372695923, Final Batch Loss: 0.02201692759990692\n",
      "Epoch 3023, Loss: 0.05385987646877766, Final Batch Loss: 0.026598148047924042\n",
      "Epoch 3024, Loss: 0.14459914341568947, Final Batch Loss: 0.057491909712553024\n",
      "Epoch 3025, Loss: 0.08879778161644936, Final Batch Loss: 0.02892717346549034\n",
      "Epoch 3026, Loss: 0.04361218400299549, Final Batch Loss: 0.016826322302222252\n",
      "Epoch 3027, Loss: 0.030924290884286165, Final Batch Loss: 0.007541850674897432\n",
      "Epoch 3028, Loss: 0.09849665313959122, Final Batch Loss: 0.04871296510100365\n",
      "Epoch 3029, Loss: 0.07153604924678802, Final Batch Loss: 0.03505773842334747\n",
      "Epoch 3030, Loss: 0.14321276918053627, Final Batch Loss: 0.09587904810905457\n",
      "Epoch 3031, Loss: 0.1513933762907982, Final Batch Loss: 0.10399971902370453\n",
      "Epoch 3032, Loss: 0.08602127060294151, Final Batch Loss: 0.04790390282869339\n",
      "Epoch 3033, Loss: 0.1684350147843361, Final Batch Loss: 0.10537612438201904\n",
      "Epoch 3034, Loss: 0.08045936934649944, Final Batch Loss: 0.02274204231798649\n",
      "Epoch 3035, Loss: 0.14838699251413345, Final Batch Loss: 0.07982605695724487\n",
      "Epoch 3036, Loss: 0.07589371502399445, Final Batch Loss: 0.04025077074766159\n",
      "Epoch 3037, Loss: 0.13658259063959122, Final Batch Loss: 0.0954364463686943\n",
      "Epoch 3038, Loss: 0.1313046459108591, Final Batch Loss: 0.021439554169774055\n",
      "Epoch 3039, Loss: 0.13159990683197975, Final Batch Loss: 0.09405821561813354\n",
      "Epoch 3040, Loss: 0.11817646399140358, Final Batch Loss: 0.0384046696126461\n",
      "Epoch 3041, Loss: 0.08421759866178036, Final Batch Loss: 0.01197601668536663\n",
      "Epoch 3042, Loss: 0.12064487114548683, Final Batch Loss: 0.06944829225540161\n",
      "Epoch 3043, Loss: 0.0863548219203949, Final Batch Loss: 0.05828936770558357\n",
      "Epoch 3044, Loss: 0.07607141509652138, Final Batch Loss: 0.033975131809711456\n",
      "Epoch 3045, Loss: 0.0717121846973896, Final Batch Loss: 0.03481774777173996\n",
      "Epoch 3046, Loss: 0.07024825178086758, Final Batch Loss: 0.030106445774435997\n",
      "Epoch 3047, Loss: 0.08632073551416397, Final Batch Loss: 0.03604329377412796\n",
      "Epoch 3048, Loss: 0.13763541728258133, Final Batch Loss: 0.036699771881103516\n",
      "Epoch 3049, Loss: 0.0788722075521946, Final Batch Loss: 0.025043468922376633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3050, Loss: 0.09638455882668495, Final Batch Loss: 0.035066790878772736\n",
      "Epoch 3051, Loss: 0.1234033852815628, Final Batch Loss: 0.08225996047258377\n",
      "Epoch 3052, Loss: 0.054551295936107635, Final Batch Loss: 0.03168892115354538\n",
      "Epoch 3053, Loss: 0.14268016070127487, Final Batch Loss: 0.07238597422838211\n",
      "Epoch 3054, Loss: 0.15322420001029968, Final Batch Loss: 0.09555875509977341\n",
      "Epoch 3055, Loss: 0.09925293549895287, Final Batch Loss: 0.049970366060733795\n",
      "Epoch 3056, Loss: 0.1641795001924038, Final Batch Loss: 0.12391169369220734\n",
      "Epoch 3057, Loss: 0.12971437722444534, Final Batch Loss: 0.09278330206871033\n",
      "Epoch 3058, Loss: 0.0788441076874733, Final Batch Loss: 0.036778468638658524\n",
      "Epoch 3059, Loss: 0.11412285640835762, Final Batch Loss: 0.032365862280130386\n",
      "Epoch 3060, Loss: 0.07920767180621624, Final Batch Loss: 0.04959816858172417\n",
      "Epoch 3061, Loss: 0.1669447347521782, Final Batch Loss: 0.09617464244365692\n",
      "Epoch 3062, Loss: 0.10128321871161461, Final Batch Loss: 0.05955994129180908\n",
      "Epoch 3063, Loss: 0.21364953368902206, Final Batch Loss: 0.17469631135463715\n",
      "Epoch 3064, Loss: 0.12085771188139915, Final Batch Loss: 0.058287363499403\n",
      "Epoch 3065, Loss: 0.06858170591294765, Final Batch Loss: 0.02840082161128521\n",
      "Epoch 3066, Loss: 0.07260899245738983, Final Batch Loss: 0.033172864466905594\n",
      "Epoch 3067, Loss: 0.05477950815111399, Final Batch Loss: 0.012755085714161396\n",
      "Epoch 3068, Loss: 0.08719820901751518, Final Batch Loss: 0.06056876853108406\n",
      "Epoch 3069, Loss: 0.0646927859634161, Final Batch Loss: 0.015360379591584206\n",
      "Epoch 3070, Loss: 0.052463728934526443, Final Batch Loss: 0.0231375303119421\n",
      "Epoch 3071, Loss: 0.06365209724754095, Final Batch Loss: 0.01186253409832716\n",
      "Epoch 3072, Loss: 0.18986786156892776, Final Batch Loss: 0.10122799128293991\n",
      "Epoch 3073, Loss: 0.0556366853415966, Final Batch Loss: 0.022645175457000732\n",
      "Epoch 3074, Loss: 0.1315639428794384, Final Batch Loss: 0.10251673310995102\n",
      "Epoch 3075, Loss: 0.07608575001358986, Final Batch Loss: 0.029680747538805008\n",
      "Epoch 3076, Loss: 0.12072727829217911, Final Batch Loss: 0.04130899906158447\n",
      "Epoch 3077, Loss: 0.05072478763759136, Final Batch Loss: 0.03217577561736107\n",
      "Epoch 3078, Loss: 0.15979404747486115, Final Batch Loss: 0.09030542522668839\n",
      "Epoch 3079, Loss: 0.08620192483067513, Final Batch Loss: 0.04581458121538162\n",
      "Epoch 3080, Loss: 0.07371610775589943, Final Batch Loss: 0.029944341629743576\n",
      "Epoch 3081, Loss: 0.18183857575058937, Final Batch Loss: 0.12424588203430176\n",
      "Epoch 3082, Loss: 0.15380147472023964, Final Batch Loss: 0.12032148241996765\n",
      "Epoch 3083, Loss: 0.07779406569898129, Final Batch Loss: 0.019118888303637505\n",
      "Epoch 3084, Loss: 0.08957437053322792, Final Batch Loss: 0.048270080238580704\n",
      "Epoch 3085, Loss: 0.0498293973505497, Final Batch Loss: 0.014334537088871002\n",
      "Epoch 3086, Loss: 0.13299486599862576, Final Batch Loss: 0.019930830225348473\n",
      "Epoch 3087, Loss: 0.10719345696270466, Final Batch Loss: 0.08505341410636902\n",
      "Epoch 3088, Loss: 0.1526564359664917, Final Batch Loss: 0.09333677589893341\n",
      "Epoch 3089, Loss: 0.11892064660787582, Final Batch Loss: 0.08361592143774033\n",
      "Epoch 3090, Loss: 0.22612715512514114, Final Batch Loss: 0.10545236617326736\n",
      "Epoch 3091, Loss: 0.0651144478470087, Final Batch Loss: 0.02596679888665676\n",
      "Epoch 3092, Loss: 0.14133722707629204, Final Batch Loss: 0.1057225912809372\n",
      "Epoch 3093, Loss: 0.1121680960059166, Final Batch Loss: 0.062017228454351425\n",
      "Epoch 3094, Loss: 0.09489642828702927, Final Batch Loss: 0.01735454797744751\n",
      "Epoch 3095, Loss: 0.06461703777313232, Final Batch Loss: 0.03275704011321068\n",
      "Epoch 3096, Loss: 0.10354985296726227, Final Batch Loss: 0.04233865439891815\n",
      "Epoch 3097, Loss: 0.17527154833078384, Final Batch Loss: 0.1405571848154068\n",
      "Epoch 3098, Loss: 0.05106349289417267, Final Batch Loss: 0.028693977743387222\n",
      "Epoch 3099, Loss: 0.06844336353242397, Final Batch Loss: 0.02416417934000492\n",
      "Epoch 3100, Loss: 0.11906387284398079, Final Batch Loss: 0.08276089280843735\n",
      "Epoch 3101, Loss: 0.07696779817342758, Final Batch Loss: 0.02549610286951065\n",
      "Epoch 3102, Loss: 0.12454476952552795, Final Batch Loss: 0.0735590010881424\n",
      "Epoch 3103, Loss: 0.07242086343467236, Final Batch Loss: 0.016308190301060677\n",
      "Epoch 3104, Loss: 0.05983276478946209, Final Batch Loss: 0.02111363597214222\n",
      "Epoch 3105, Loss: 0.05674872174859047, Final Batch Loss: 0.018014397472143173\n",
      "Epoch 3106, Loss: 0.06323692202568054, Final Batch Loss: 0.016561388969421387\n",
      "Epoch 3107, Loss: 0.18494433537125587, Final Batch Loss: 0.15536247193813324\n",
      "Epoch 3108, Loss: 0.0737231969833374, Final Batch Loss: 0.025817029178142548\n",
      "Epoch 3109, Loss: 0.10157286375761032, Final Batch Loss: 0.05011008679866791\n",
      "Epoch 3110, Loss: 0.056248366832733154, Final Batch Loss: 0.020908553153276443\n",
      "Epoch 3111, Loss: 0.0672483891248703, Final Batch Loss: 0.034476231783628464\n",
      "Epoch 3112, Loss: 0.10214714519679546, Final Batch Loss: 0.08040972799062729\n",
      "Epoch 3113, Loss: 0.052922578528523445, Final Batch Loss: 0.021754790097475052\n",
      "Epoch 3114, Loss: 0.0756981847807765, Final Batch Loss: 0.015588699840009212\n",
      "Epoch 3115, Loss: 0.10786698386073112, Final Batch Loss: 0.032072823494672775\n",
      "Epoch 3116, Loss: 0.08386744931340218, Final Batch Loss: 0.03375783935189247\n",
      "Epoch 3117, Loss: 0.16081967949867249, Final Batch Loss: 0.09561825543642044\n",
      "Epoch 3118, Loss: 0.050022522918879986, Final Batch Loss: 0.0140039948746562\n",
      "Epoch 3119, Loss: 0.05598846450448036, Final Batch Loss: 0.011696401983499527\n",
      "Epoch 3120, Loss: 0.20694396644830704, Final Batch Loss: 0.12932738661766052\n",
      "Epoch 3121, Loss: 0.1180722825229168, Final Batch Loss: 0.07548095285892487\n",
      "Epoch 3122, Loss: 0.06685136258602142, Final Batch Loss: 0.021644961088895798\n",
      "Epoch 3123, Loss: 0.09891519695520401, Final Batch Loss: 0.05521133542060852\n",
      "Epoch 3124, Loss: 0.07703631557524204, Final Batch Loss: 0.025546951219439507\n",
      "Epoch 3125, Loss: 0.07710203900933266, Final Batch Loss: 0.03236842527985573\n",
      "Epoch 3126, Loss: 0.07263593375682831, Final Batch Loss: 0.022963982075452805\n",
      "Epoch 3127, Loss: 0.0959963845089078, Final Batch Loss: 0.01455031055957079\n",
      "Epoch 3128, Loss: 0.05265883915126324, Final Batch Loss: 0.021328816190361977\n",
      "Epoch 3129, Loss: 0.0826499443501234, Final Batch Loss: 0.017436271533370018\n",
      "Epoch 3130, Loss: 0.08603069558739662, Final Batch Loss: 0.022498656064271927\n",
      "Epoch 3131, Loss: 0.07089794985949993, Final Batch Loss: 0.016337020322680473\n",
      "Epoch 3132, Loss: 0.17981313914060593, Final Batch Loss: 0.12470638751983643\n",
      "Epoch 3133, Loss: 0.05152193084359169, Final Batch Loss: 0.015149485319852829\n",
      "Epoch 3134, Loss: 0.0867572333663702, Final Batch Loss: 0.0635770931839943\n",
      "Epoch 3135, Loss: 0.11241161823272705, Final Batch Loss: 0.058355752378702164\n",
      "Epoch 3136, Loss: 0.09172626957297325, Final Batch Loss: 0.057899314910173416\n",
      "Epoch 3137, Loss: 0.1060614064335823, Final Batch Loss: 0.07824056595563889\n",
      "Epoch 3138, Loss: 0.08970881998538971, Final Batch Loss: 0.047361548990011215\n",
      "Epoch 3139, Loss: 0.16720229014754295, Final Batch Loss: 0.11146435886621475\n",
      "Epoch 3140, Loss: 0.0778644448146224, Final Batch Loss: 0.014111182652413845\n",
      "Epoch 3141, Loss: 0.07559381797909737, Final Batch Loss: 0.03255420923233032\n",
      "Epoch 3142, Loss: 0.10368147492408752, Final Batch Loss: 0.056179288774728775\n",
      "Epoch 3143, Loss: 0.09772757068276405, Final Batch Loss: 0.04366593062877655\n",
      "Epoch 3144, Loss: 0.07185874134302139, Final Batch Loss: 0.03337369114160538\n",
      "Epoch 3145, Loss: 0.059926681220531464, Final Batch Loss: 0.01726643368601799\n",
      "Epoch 3146, Loss: 0.10681958124041557, Final Batch Loss: 0.0393378846347332\n",
      "Epoch 3147, Loss: 0.03980231937021017, Final Batch Loss: 0.0096448278054595\n",
      "Epoch 3148, Loss: 0.057558528147637844, Final Batch Loss: 0.007588925771415234\n",
      "Epoch 3149, Loss: 0.1554243080317974, Final Batch Loss: 0.04965389892458916\n",
      "Epoch 3150, Loss: 0.09448430687189102, Final Batch Loss: 0.05287686362862587\n",
      "Epoch 3151, Loss: 0.10989811271429062, Final Batch Loss: 0.060870055109262466\n",
      "Epoch 3152, Loss: 0.055789701640605927, Final Batch Loss: 0.02319321036338806\n",
      "Epoch 3153, Loss: 0.22009477764368057, Final Batch Loss: 0.12993550300598145\n",
      "Epoch 3154, Loss: 0.24956796318292618, Final Batch Loss: 0.1314951330423355\n",
      "Epoch 3155, Loss: 0.07507892139256, Final Batch Loss: 0.04938634857535362\n",
      "Epoch 3156, Loss: 0.07451323978602886, Final Batch Loss: 0.023195737972855568\n",
      "Epoch 3157, Loss: 0.08090555854141712, Final Batch Loss: 0.016858482733368874\n",
      "Epoch 3158, Loss: 0.10408109426498413, Final Batch Loss: 0.033075861632823944\n",
      "Epoch 3159, Loss: 0.054198989644646645, Final Batch Loss: 0.023612521588802338\n",
      "Epoch 3160, Loss: 0.10293827578425407, Final Batch Loss: 0.060679055750370026\n",
      "Epoch 3161, Loss: 0.12102609500288963, Final Batch Loss: 0.07321096211671829\n",
      "Epoch 3162, Loss: 0.07527407258749008, Final Batch Loss: 0.04014737531542778\n",
      "Epoch 3163, Loss: 0.08813291415572166, Final Batch Loss: 0.026888035237789154\n",
      "Epoch 3164, Loss: 0.08964820578694344, Final Batch Loss: 0.05542859062552452\n",
      "Epoch 3165, Loss: 0.08385005407035351, Final Batch Loss: 0.029662149026989937\n",
      "Epoch 3166, Loss: 0.1500677429139614, Final Batch Loss: 0.11855343729257584\n",
      "Epoch 3167, Loss: 0.06464304961264133, Final Batch Loss: 0.02408047951757908\n",
      "Epoch 3168, Loss: 0.05897515453398228, Final Batch Loss: 0.03836071863770485\n",
      "Epoch 3169, Loss: 0.0804546419531107, Final Batch Loss: 0.014837050810456276\n",
      "Epoch 3170, Loss: 0.06179637089371681, Final Batch Loss: 0.011623553931713104\n",
      "Epoch 3171, Loss: 0.03337996732443571, Final Batch Loss: 0.008917217142879963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3172, Loss: 0.0960499495267868, Final Batch Loss: 0.03413384035229683\n",
      "Epoch 3173, Loss: 0.05221178941428661, Final Batch Loss: 0.021516593173146248\n",
      "Epoch 3174, Loss: 0.08618757501244545, Final Batch Loss: 0.04377500340342522\n",
      "Epoch 3175, Loss: 0.14442671462893486, Final Batch Loss: 0.1003105416893959\n",
      "Epoch 3176, Loss: 0.050143979489803314, Final Batch Loss: 0.023582592606544495\n",
      "Epoch 3177, Loss: 0.04908460006117821, Final Batch Loss: 0.015476234257221222\n",
      "Epoch 3178, Loss: 0.09198429435491562, Final Batch Loss: 0.02233290672302246\n",
      "Epoch 3179, Loss: 0.06571873649954796, Final Batch Loss: 0.01916946843266487\n",
      "Epoch 3180, Loss: 0.0683111222460866, Final Batch Loss: 0.013097953982651234\n",
      "Epoch 3181, Loss: 0.05781165696680546, Final Batch Loss: 0.026173943653702736\n",
      "Epoch 3182, Loss: 0.050506047904491425, Final Batch Loss: 0.01945986971259117\n",
      "Epoch 3183, Loss: 0.04145216569304466, Final Batch Loss: 0.017802005633711815\n",
      "Epoch 3184, Loss: 0.1362552009522915, Final Batch Loss: 0.08366438001394272\n",
      "Epoch 3185, Loss: 0.04941144585609436, Final Batch Loss: 0.02897564321756363\n",
      "Epoch 3186, Loss: 0.1470944546163082, Final Batch Loss: 0.05746428295969963\n",
      "Epoch 3187, Loss: 0.0697105173021555, Final Batch Loss: 0.019857576116919518\n",
      "Epoch 3188, Loss: 0.07522587664425373, Final Batch Loss: 0.04627839848399162\n",
      "Epoch 3189, Loss: 0.06754410080611706, Final Batch Loss: 0.04884566366672516\n",
      "Epoch 3190, Loss: 0.06313997693359852, Final Batch Loss: 0.009753314778208733\n",
      "Epoch 3191, Loss: 0.08069725893437862, Final Batch Loss: 0.028762632980942726\n",
      "Epoch 3192, Loss: 0.028822608292102814, Final Batch Loss: 0.015103582292795181\n",
      "Epoch 3193, Loss: 0.046848079189658165, Final Batch Loss: 0.02863628976047039\n",
      "Epoch 3194, Loss: 0.0960247777402401, Final Batch Loss: 0.049920931458473206\n",
      "Epoch 3195, Loss: 0.05932329595088959, Final Batch Loss: 0.021838366985321045\n",
      "Epoch 3196, Loss: 0.04772944934666157, Final Batch Loss: 0.01158788613975048\n",
      "Epoch 3197, Loss: 0.07318361662328243, Final Batch Loss: 0.020341278985142708\n",
      "Epoch 3198, Loss: 0.05938427522778511, Final Batch Loss: 0.030013738200068474\n",
      "Epoch 3199, Loss: 0.07990332506597042, Final Batch Loss: 0.051204949617385864\n",
      "Epoch 3200, Loss: 0.060743603855371475, Final Batch Loss: 0.02934853732585907\n",
      "Epoch 3201, Loss: 0.019430978689342737, Final Batch Loss: 0.005412745755165815\n",
      "Epoch 3202, Loss: 0.07481583580374718, Final Batch Loss: 0.018228746950626373\n",
      "Epoch 3203, Loss: 0.04544069059193134, Final Batch Loss: 0.025275640189647675\n",
      "Epoch 3204, Loss: 0.07264963537454605, Final Batch Loss: 0.03655734285712242\n",
      "Epoch 3205, Loss: 0.03774759219959378, Final Batch Loss: 0.007029992062598467\n",
      "Epoch 3206, Loss: 0.048991503193974495, Final Batch Loss: 0.020808350294828415\n",
      "Epoch 3207, Loss: 0.1862115990370512, Final Batch Loss: 0.17100799083709717\n",
      "Epoch 3208, Loss: 0.11349978670477867, Final Batch Loss: 0.05864750221371651\n",
      "Epoch 3209, Loss: 0.10433627292513847, Final Batch Loss: 0.05804476886987686\n",
      "Epoch 3210, Loss: 0.0382662508636713, Final Batch Loss: 0.015940899029374123\n",
      "Epoch 3211, Loss: 0.08538999781012535, Final Batch Loss: 0.046002622693777084\n",
      "Epoch 3212, Loss: 0.04885192587971687, Final Batch Loss: 0.009649965912103653\n",
      "Epoch 3213, Loss: 0.12447918951511383, Final Batch Loss: 0.061819665133953094\n",
      "Epoch 3214, Loss: 0.09605035372078419, Final Batch Loss: 0.06876514852046967\n",
      "Epoch 3215, Loss: 0.05634566210210323, Final Batch Loss: 0.035371288657188416\n",
      "Epoch 3216, Loss: 0.04962236061692238, Final Batch Loss: 0.013434037566184998\n",
      "Epoch 3217, Loss: 0.0492023965343833, Final Batch Loss: 0.00847871508449316\n",
      "Epoch 3218, Loss: 0.10019030794501305, Final Batch Loss: 0.04829571023583412\n",
      "Epoch 3219, Loss: 0.04898068681359291, Final Batch Loss: 0.0324694998562336\n",
      "Epoch 3220, Loss: 0.14898443967103958, Final Batch Loss: 0.05798689275979996\n",
      "Epoch 3221, Loss: 0.03321346873417497, Final Batch Loss: 0.006773040164262056\n",
      "Epoch 3222, Loss: 0.04731881059706211, Final Batch Loss: 0.021489659324288368\n",
      "Epoch 3223, Loss: 0.08258087188005447, Final Batch Loss: 0.03941712900996208\n",
      "Epoch 3224, Loss: 0.07850039564073086, Final Batch Loss: 0.029081670567393303\n",
      "Epoch 3225, Loss: 0.13399151898920536, Final Batch Loss: 0.01812867261469364\n",
      "Epoch 3226, Loss: 0.10631117224693298, Final Batch Loss: 0.04686740040779114\n",
      "Epoch 3227, Loss: 0.0656956098973751, Final Batch Loss: 0.039123356342315674\n",
      "Epoch 3228, Loss: 0.03969750367105007, Final Batch Loss: 0.014373298734426498\n",
      "Epoch 3229, Loss: 0.05369790457189083, Final Batch Loss: 0.021421758458018303\n",
      "Epoch 3230, Loss: 0.06507197953760624, Final Batch Loss: 0.03563307225704193\n",
      "Epoch 3231, Loss: 0.08363364636898041, Final Batch Loss: 0.0634174793958664\n",
      "Epoch 3232, Loss: 0.05732835456728935, Final Batch Loss: 0.015846598893404007\n",
      "Epoch 3233, Loss: 0.24151400476694107, Final Batch Loss: 0.179023876786232\n",
      "Epoch 3234, Loss: 0.11478758603334427, Final Batch Loss: 0.05627401918172836\n",
      "Epoch 3235, Loss: 0.07960866671055555, Final Batch Loss: 0.011558162979781628\n",
      "Epoch 3236, Loss: 0.06156151369214058, Final Batch Loss: 0.024925701320171356\n",
      "Epoch 3237, Loss: 0.10599568486213684, Final Batch Loss: 0.047345396131277084\n",
      "Epoch 3238, Loss: 0.12474177032709122, Final Batch Loss: 0.09155931323766708\n",
      "Epoch 3239, Loss: 0.05464458838105202, Final Batch Loss: 0.031660791486501694\n",
      "Epoch 3240, Loss: 0.08517254516482353, Final Batch Loss: 0.05197373777627945\n",
      "Epoch 3241, Loss: 0.07781277224421501, Final Batch Loss: 0.039711158722639084\n",
      "Epoch 3242, Loss: 0.09618335589766502, Final Batch Loss: 0.053689975291490555\n",
      "Epoch 3243, Loss: 0.09368248656392097, Final Batch Loss: 0.04013263061642647\n",
      "Epoch 3244, Loss: 0.12268110923469067, Final Batch Loss: 0.022311246022582054\n",
      "Epoch 3245, Loss: 0.11341910809278488, Final Batch Loss: 0.06274532526731491\n",
      "Epoch 3246, Loss: 0.10960720106959343, Final Batch Loss: 0.06978084146976471\n",
      "Epoch 3247, Loss: 0.029190685134381056, Final Batch Loss: 0.006352949421852827\n",
      "Epoch 3248, Loss: 0.05943083390593529, Final Batch Loss: 0.018899966031312943\n",
      "Epoch 3249, Loss: 0.127862261608243, Final Batch Loss: 0.10661406069993973\n",
      "Epoch 3250, Loss: 0.08269594050943851, Final Batch Loss: 0.025309225544333458\n",
      "Epoch 3251, Loss: 0.15290793403983116, Final Batch Loss: 0.10907109081745148\n",
      "Epoch 3252, Loss: 0.08852898515760899, Final Batch Loss: 0.02771533839404583\n",
      "Epoch 3253, Loss: 0.09075300395488739, Final Batch Loss: 0.03229833021759987\n",
      "Epoch 3254, Loss: 0.10728588700294495, Final Batch Loss: 0.06595360487699509\n",
      "Epoch 3255, Loss: 0.23967328667640686, Final Batch Loss: 0.13440188765525818\n",
      "Epoch 3256, Loss: 0.10503973439335823, Final Batch Loss: 0.061618804931640625\n",
      "Epoch 3257, Loss: 0.07550979778170586, Final Batch Loss: 0.038334012031555176\n",
      "Epoch 3258, Loss: 0.28576219268143177, Final Batch Loss: 0.2607231140136719\n",
      "Epoch 3259, Loss: 0.09718722850084305, Final Batch Loss: 0.03625444322824478\n",
      "Epoch 3260, Loss: 0.09345115348696709, Final Batch Loss: 0.02289736643433571\n",
      "Epoch 3261, Loss: 0.14917989820241928, Final Batch Loss: 0.07465100288391113\n",
      "Epoch 3262, Loss: 0.06649279408156872, Final Batch Loss: 0.019665131345391273\n",
      "Epoch 3263, Loss: 0.1410771757364273, Final Batch Loss: 0.10579709708690643\n",
      "Epoch 3264, Loss: 0.16817549616098404, Final Batch Loss: 0.06127926707267761\n",
      "Epoch 3265, Loss: 0.11078768037259579, Final Batch Loss: 0.019379576668143272\n",
      "Epoch 3266, Loss: 0.08444313704967499, Final Batch Loss: 0.03684000298380852\n",
      "Epoch 3267, Loss: 0.11413178220391273, Final Batch Loss: 0.04685830697417259\n",
      "Epoch 3268, Loss: 0.071843471378088, Final Batch Loss: 0.03999050706624985\n",
      "Epoch 3269, Loss: 0.04517647624015808, Final Batch Loss: 0.026681939139962196\n",
      "Epoch 3270, Loss: 0.08191246911883354, Final Batch Loss: 0.04185548424720764\n",
      "Epoch 3271, Loss: 0.038920036517083645, Final Batch Loss: 0.009441724978387356\n",
      "Epoch 3272, Loss: 0.0732906237244606, Final Batch Loss: 0.03027072548866272\n",
      "Epoch 3273, Loss: 0.10737268254160881, Final Batch Loss: 0.05055283010005951\n",
      "Epoch 3274, Loss: 0.08129610121250153, Final Batch Loss: 0.04162381589412689\n",
      "Epoch 3275, Loss: 0.04745328240096569, Final Batch Loss: 0.015459155663847923\n",
      "Epoch 3276, Loss: 0.08997451886534691, Final Batch Loss: 0.032706234604120255\n",
      "Epoch 3277, Loss: 0.09541019424796104, Final Batch Loss: 0.05647265166044235\n",
      "Epoch 3278, Loss: 0.09853236749768257, Final Batch Loss: 0.06582734733819962\n",
      "Epoch 3279, Loss: 0.07202828116714954, Final Batch Loss: 0.04629324749112129\n",
      "Epoch 3280, Loss: 0.06250760518014431, Final Batch Loss: 0.018163645640015602\n",
      "Epoch 3281, Loss: 0.07051040604710579, Final Batch Loss: 0.018713198602199554\n",
      "Epoch 3282, Loss: 0.041840190067887306, Final Batch Loss: 0.011599984019994736\n",
      "Epoch 3283, Loss: 0.05935496650636196, Final Batch Loss: 0.02473372034728527\n",
      "Epoch 3284, Loss: 0.11553460359573364, Final Batch Loss: 0.04752881079912186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3285, Loss: 0.05261899530887604, Final Batch Loss: 0.029030390083789825\n",
      "Epoch 3286, Loss: 0.04858667403459549, Final Batch Loss: 0.024868201464414597\n",
      "Epoch 3287, Loss: 0.08198421262204647, Final Batch Loss: 0.06337137520313263\n",
      "Epoch 3288, Loss: 0.04253517650067806, Final Batch Loss: 0.011338884010910988\n",
      "Epoch 3289, Loss: 0.04854710027575493, Final Batch Loss: 0.013848453760147095\n",
      "Epoch 3290, Loss: 0.06450439896434546, Final Batch Loss: 0.010426226072013378\n",
      "Epoch 3291, Loss: 0.06676293350756168, Final Batch Loss: 0.018705876544117928\n",
      "Epoch 3292, Loss: 0.0656014559790492, Final Batch Loss: 0.014867655001580715\n",
      "Epoch 3293, Loss: 0.04427513852715492, Final Batch Loss: 0.01773167960345745\n",
      "Epoch 3294, Loss: 0.04754524119198322, Final Batch Loss: 0.025210535153746605\n",
      "Epoch 3295, Loss: 0.051189983263611794, Final Batch Loss: 0.017539052292704582\n",
      "Epoch 3296, Loss: 0.06502179987728596, Final Batch Loss: 0.023144135251641273\n",
      "Epoch 3297, Loss: 0.09041027165949345, Final Batch Loss: 0.0675145611166954\n",
      "Epoch 3298, Loss: 0.06047692149877548, Final Batch Loss: 0.019223645329475403\n",
      "Epoch 3299, Loss: 0.05516253039240837, Final Batch Loss: 0.028555095195770264\n",
      "Epoch 3300, Loss: 0.15241757035255432, Final Batch Loss: 0.11606012284755707\n",
      "Epoch 3301, Loss: 0.05917033459991217, Final Batch Loss: 0.015211719088256359\n",
      "Epoch 3302, Loss: 0.06732619553804398, Final Batch Loss: 0.035475462675094604\n",
      "Epoch 3303, Loss: 0.06423415243625641, Final Batch Loss: 0.01930929720401764\n",
      "Epoch 3304, Loss: 0.05848567187786102, Final Batch Loss: 0.026040345430374146\n",
      "Epoch 3305, Loss: 0.04368188604712486, Final Batch Loss: 0.01513238251209259\n",
      "Epoch 3306, Loss: 0.03205682337284088, Final Batch Loss: 0.007987800985574722\n",
      "Epoch 3307, Loss: 0.0679544135928154, Final Batch Loss: 0.018051013350486755\n",
      "Epoch 3308, Loss: 0.13197482749819756, Final Batch Loss: 0.0799308568239212\n",
      "Epoch 3309, Loss: 0.0847640298306942, Final Batch Loss: 0.01930791512131691\n",
      "Epoch 3310, Loss: 0.05715356767177582, Final Batch Loss: 0.035005927085876465\n",
      "Epoch 3311, Loss: 0.08223423734307289, Final Batch Loss: 0.026971131563186646\n",
      "Epoch 3312, Loss: 0.07429109141230583, Final Batch Loss: 0.04058249294757843\n",
      "Epoch 3313, Loss: 0.07112810760736465, Final Batch Loss: 0.03150246664881706\n",
      "Epoch 3314, Loss: 0.041678459383547306, Final Batch Loss: 0.009156105108559132\n",
      "Epoch 3315, Loss: 0.08580072969198227, Final Batch Loss: 0.04914414882659912\n",
      "Epoch 3316, Loss: 0.0583927184343338, Final Batch Loss: 0.014310415834188461\n",
      "Epoch 3317, Loss: 0.08237200230360031, Final Batch Loss: 0.06619889289140701\n",
      "Epoch 3318, Loss: 0.06314936745911837, Final Batch Loss: 0.015072089619934559\n",
      "Epoch 3319, Loss: 0.07470724359154701, Final Batch Loss: 0.021495994180440903\n",
      "Epoch 3320, Loss: 0.19376447796821594, Final Batch Loss: 0.11629041284322739\n",
      "Epoch 3321, Loss: 0.1623520851135254, Final Batch Loss: 0.06851089000701904\n",
      "Epoch 3322, Loss: 0.28047752380371094, Final Batch Loss: 0.19167618453502655\n",
      "Epoch 3323, Loss: 0.15558412671089172, Final Batch Loss: 0.08673573285341263\n",
      "Epoch 3324, Loss: 0.11945909261703491, Final Batch Loss: 0.054119452834129333\n",
      "Epoch 3325, Loss: 0.0835252720862627, Final Batch Loss: 0.024669906124472618\n",
      "Epoch 3326, Loss: 0.12785538658499718, Final Batch Loss: 0.02393661066889763\n",
      "Epoch 3327, Loss: 0.12342058680951595, Final Batch Loss: 0.01675722561776638\n",
      "Epoch 3328, Loss: 0.0868236692622304, Final Batch Loss: 0.011159856803715229\n",
      "Epoch 3329, Loss: 0.11426767706871033, Final Batch Loss: 0.0653432235121727\n",
      "Epoch 3330, Loss: 0.1759292595088482, Final Batch Loss: 0.1282086968421936\n",
      "Epoch 3331, Loss: 0.06704586744308472, Final Batch Loss: 0.023329585790634155\n",
      "Epoch 3332, Loss: 0.10011022910475731, Final Batch Loss: 0.04760471358895302\n",
      "Epoch 3333, Loss: 0.045132165774703026, Final Batch Loss: 0.029853081330657005\n",
      "Epoch 3334, Loss: 0.14266318455338478, Final Batch Loss: 0.114524707198143\n",
      "Epoch 3335, Loss: 0.057083792984485626, Final Batch Loss: 0.02919839695096016\n",
      "Epoch 3336, Loss: 0.06722603179514408, Final Batch Loss: 0.04793446138501167\n",
      "Epoch 3337, Loss: 0.17717450484633446, Final Batch Loss: 0.12873341143131256\n",
      "Epoch 3338, Loss: 0.03547513112425804, Final Batch Loss: 0.012935860082507133\n",
      "Epoch 3339, Loss: 0.08203155547380447, Final Batch Loss: 0.038751114159822464\n",
      "Epoch 3340, Loss: 0.0921189971268177, Final Batch Loss: 0.04812992736697197\n",
      "Epoch 3341, Loss: 0.08289255201816559, Final Batch Loss: 0.05285553261637688\n",
      "Epoch 3342, Loss: 0.09370716661214828, Final Batch Loss: 0.03279096633195877\n",
      "Epoch 3343, Loss: 0.06375276111066341, Final Batch Loss: 0.029609179124236107\n",
      "Epoch 3344, Loss: 0.18411130085587502, Final Batch Loss: 0.12192201614379883\n",
      "Epoch 3345, Loss: 0.054961519315838814, Final Batch Loss: 0.027002623304724693\n",
      "Epoch 3346, Loss: 0.1044430211186409, Final Batch Loss: 0.03180457651615143\n",
      "Epoch 3347, Loss: 0.1958487704396248, Final Batch Loss: 0.12644502520561218\n",
      "Epoch 3348, Loss: 0.08762326464056969, Final Batch Loss: 0.04522412642836571\n",
      "Epoch 3349, Loss: 0.10323264263570309, Final Batch Loss: 0.07899975776672363\n",
      "Epoch 3350, Loss: 0.09508569166064262, Final Batch Loss: 0.057625651359558105\n",
      "Epoch 3351, Loss: 0.06638609059154987, Final Batch Loss: 0.04407710209488869\n",
      "Epoch 3352, Loss: 0.06968753039836884, Final Batch Loss: 0.019116945564746857\n",
      "Epoch 3353, Loss: 0.05979788023978472, Final Batch Loss: 0.01429691631346941\n",
      "Epoch 3354, Loss: 0.08544890955090523, Final Batch Loss: 0.027645692229270935\n",
      "Epoch 3355, Loss: 0.06277257576584816, Final Batch Loss: 0.023190129548311234\n",
      "Epoch 3356, Loss: 0.09103967435657978, Final Batch Loss: 0.02962876670062542\n",
      "Epoch 3357, Loss: 0.06653325259685516, Final Batch Loss: 0.01978340744972229\n",
      "Epoch 3358, Loss: 0.041828453540802, Final Batch Loss: 0.013073472306132317\n",
      "Epoch 3359, Loss: 0.05586745776236057, Final Batch Loss: 0.03939690813422203\n",
      "Epoch 3360, Loss: 0.05743035115301609, Final Batch Loss: 0.03194792941212654\n",
      "Epoch 3361, Loss: 0.05544283986091614, Final Batch Loss: 0.01321662962436676\n",
      "Epoch 3362, Loss: 0.08828666806221008, Final Batch Loss: 0.05572456866502762\n",
      "Epoch 3363, Loss: 0.073707340285182, Final Batch Loss: 0.015071684494614601\n",
      "Epoch 3364, Loss: 0.13528060913085938, Final Batch Loss: 0.07608061283826828\n",
      "Epoch 3365, Loss: 0.036701591685414314, Final Batch Loss: 0.01587821915745735\n",
      "Epoch 3366, Loss: 0.05891476571559906, Final Batch Loss: 0.019252147525548935\n",
      "Epoch 3367, Loss: 0.08096430357545614, Final Batch Loss: 0.007736398838460445\n",
      "Epoch 3368, Loss: 0.07205657474696636, Final Batch Loss: 0.03113061748445034\n",
      "Epoch 3369, Loss: 0.0811117198318243, Final Batch Loss: 0.017436480149626732\n",
      "Epoch 3370, Loss: 0.1697724685072899, Final Batch Loss: 0.13242080807685852\n",
      "Epoch 3371, Loss: 0.04767405800521374, Final Batch Loss: 0.00549560971558094\n",
      "Epoch 3372, Loss: 0.08268614485859871, Final Batch Loss: 0.03892902657389641\n",
      "Epoch 3373, Loss: 0.06364492699503899, Final Batch Loss: 0.032286666333675385\n",
      "Epoch 3374, Loss: 0.0714532695710659, Final Batch Loss: 0.016352899372577667\n",
      "Epoch 3375, Loss: 0.1415972039103508, Final Batch Loss: 0.07469478994607925\n",
      "Epoch 3376, Loss: 0.0353687172755599, Final Batch Loss: 0.00583756435662508\n",
      "Epoch 3377, Loss: 0.10009826347231865, Final Batch Loss: 0.05751093104481697\n",
      "Epoch 3378, Loss: 0.09067332372069359, Final Batch Loss: 0.049223385751247406\n",
      "Epoch 3379, Loss: 0.07444917783141136, Final Batch Loss: 0.0436398871243\n",
      "Epoch 3380, Loss: 0.04596584662795067, Final Batch Loss: 0.020017478615045547\n",
      "Epoch 3381, Loss: 0.0778932124376297, Final Batch Loss: 0.026425883173942566\n",
      "Epoch 3382, Loss: 0.1636277697980404, Final Batch Loss: 0.12866544723510742\n",
      "Epoch 3383, Loss: 0.05908835306763649, Final Batch Loss: 0.03670414537191391\n",
      "Epoch 3384, Loss: 0.17844095081090927, Final Batch Loss: 0.1031140461564064\n",
      "Epoch 3385, Loss: 0.03921163734048605, Final Batch Loss: 0.008348598144948483\n",
      "Epoch 3386, Loss: 0.028282080311328173, Final Batch Loss: 0.006572330836206675\n",
      "Epoch 3387, Loss: 0.08222640492022038, Final Batch Loss: 0.028475726023316383\n",
      "Epoch 3388, Loss: 0.07999247498810291, Final Batch Loss: 0.056802794337272644\n",
      "Epoch 3389, Loss: 0.1264459602534771, Final Batch Loss: 0.09392660111188889\n",
      "Epoch 3390, Loss: 0.04152887873351574, Final Batch Loss: 0.017794977873563766\n",
      "Epoch 3391, Loss: 0.07934355549514294, Final Batch Loss: 0.024115638807415962\n",
      "Epoch 3392, Loss: 0.13981591537594795, Final Batch Loss: 0.03038560226559639\n",
      "Epoch 3393, Loss: 0.11341575905680656, Final Batch Loss: 0.07577912509441376\n",
      "Epoch 3394, Loss: 0.06654983945190907, Final Batch Loss: 0.028674790635704994\n",
      "Epoch 3395, Loss: 0.0555840227752924, Final Batch Loss: 0.023593103513121605\n",
      "Epoch 3396, Loss: 0.08620136231184006, Final Batch Loss: 0.03497077897191048\n",
      "Epoch 3397, Loss: 0.04811710678040981, Final Batch Loss: 0.03325469791889191\n",
      "Epoch 3398, Loss: 0.035470802104100585, Final Batch Loss: 0.0035431755241006613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3399, Loss: 0.07821427471935749, Final Batch Loss: 0.05523168295621872\n",
      "Epoch 3400, Loss: 0.048065196722745895, Final Batch Loss: 0.0365605428814888\n",
      "Epoch 3401, Loss: 0.0991789810359478, Final Batch Loss: 0.04091872647404671\n",
      "Epoch 3402, Loss: 0.1196637712419033, Final Batch Loss: 0.09251702576875687\n",
      "Epoch 3403, Loss: 0.07347718067467213, Final Batch Loss: 0.030406368896365166\n",
      "Epoch 3404, Loss: 0.0612784456461668, Final Batch Loss: 0.01649993471801281\n",
      "Epoch 3405, Loss: 0.04459908604621887, Final Batch Loss: 0.014724679291248322\n",
      "Epoch 3406, Loss: 0.0759141743183136, Final Batch Loss: 0.04239761456847191\n",
      "Epoch 3407, Loss: 0.04100324958562851, Final Batch Loss: 0.006938491016626358\n",
      "Epoch 3408, Loss: 0.18433571606874466, Final Batch Loss: 0.11650223284959793\n",
      "Epoch 3409, Loss: 0.05414733476936817, Final Batch Loss: 0.014873890206217766\n",
      "Epoch 3410, Loss: 0.06527207046747208, Final Batch Loss: 0.04404040426015854\n",
      "Epoch 3411, Loss: 0.08344961702823639, Final Batch Loss: 0.012746848165988922\n",
      "Epoch 3412, Loss: 0.07468506880104542, Final Batch Loss: 0.029690342023968697\n",
      "Epoch 3413, Loss: 0.1512182019650936, Final Batch Loss: 0.11941144615411758\n",
      "Epoch 3414, Loss: 0.07720827497541904, Final Batch Loss: 0.05419553071260452\n",
      "Epoch 3415, Loss: 0.050297693349421024, Final Batch Loss: 0.03705577552318573\n",
      "Epoch 3416, Loss: 0.040478493086993694, Final Batch Loss: 0.014801903627812862\n",
      "Epoch 3417, Loss: 0.07206153683364391, Final Batch Loss: 0.022628916427493095\n",
      "Epoch 3418, Loss: 0.21046405285596848, Final Batch Loss: 0.154807910323143\n",
      "Epoch 3419, Loss: 0.07257886976003647, Final Batch Loss: 0.03457583114504814\n",
      "Epoch 3420, Loss: 0.09576299786567688, Final Batch Loss: 0.05821709707379341\n",
      "Epoch 3421, Loss: 0.12911763787269592, Final Batch Loss: 0.09458974003791809\n",
      "Epoch 3422, Loss: 0.055141099728643894, Final Batch Loss: 0.008710646070539951\n",
      "Epoch 3423, Loss: 0.05034005269408226, Final Batch Loss: 0.022720079869031906\n",
      "Epoch 3424, Loss: 0.04065188579261303, Final Batch Loss: 0.00892454944550991\n",
      "Epoch 3425, Loss: 0.08733795583248138, Final Batch Loss: 0.024575278162956238\n",
      "Epoch 3426, Loss: 0.07205866277217865, Final Batch Loss: 0.0403098426759243\n",
      "Epoch 3427, Loss: 0.04972563683986664, Final Batch Loss: 0.021629413589835167\n",
      "Epoch 3428, Loss: 0.041293345391750336, Final Batch Loss: 0.01899590529501438\n",
      "Epoch 3429, Loss: 0.1507020890712738, Final Batch Loss: 0.08048111200332642\n",
      "Epoch 3430, Loss: 0.05942110903561115, Final Batch Loss: 0.01666487567126751\n",
      "Epoch 3431, Loss: 0.057716378942131996, Final Batch Loss: 0.021252429112792015\n",
      "Epoch 3432, Loss: 0.11421837285161018, Final Batch Loss: 0.06321853399276733\n",
      "Epoch 3433, Loss: 0.05527685210108757, Final Batch Loss: 0.025655757635831833\n",
      "Epoch 3434, Loss: 0.06810756027698517, Final Batch Loss: 0.03146490454673767\n",
      "Epoch 3435, Loss: 0.0881645018234849, Final Batch Loss: 0.01289778295904398\n",
      "Epoch 3436, Loss: 0.11200480721890926, Final Batch Loss: 0.08927792310714722\n",
      "Epoch 3437, Loss: 0.052995054982602596, Final Batch Loss: 0.015148847363889217\n",
      "Epoch 3438, Loss: 0.03773583099246025, Final Batch Loss: 0.012531576678156853\n",
      "Epoch 3439, Loss: 0.07035032659769058, Final Batch Loss: 0.01783287525177002\n",
      "Epoch 3440, Loss: 0.05177832581102848, Final Batch Loss: 0.028490612283349037\n",
      "Epoch 3441, Loss: 0.0921098068356514, Final Batch Loss: 0.045924313366413116\n",
      "Epoch 3442, Loss: 0.06960194557905197, Final Batch Loss: 0.048955291509628296\n",
      "Epoch 3443, Loss: 0.04335576854646206, Final Batch Loss: 0.017761191353201866\n",
      "Epoch 3444, Loss: 0.03926460538059473, Final Batch Loss: 0.008502603508532047\n",
      "Epoch 3445, Loss: 0.04522761143743992, Final Batch Loss: 0.02576684206724167\n",
      "Epoch 3446, Loss: 0.06357822753489017, Final Batch Loss: 0.02221672423183918\n",
      "Epoch 3447, Loss: 0.08666400797665119, Final Batch Loss: 0.07036721706390381\n",
      "Epoch 3448, Loss: 0.05965634994208813, Final Batch Loss: 0.018615437671542168\n",
      "Epoch 3449, Loss: 0.04673243686556816, Final Batch Loss: 0.016508249565958977\n",
      "Epoch 3450, Loss: 0.08709819242358208, Final Batch Loss: 0.04191669076681137\n",
      "Epoch 3451, Loss: 0.06673584133386612, Final Batch Loss: 0.03907262161374092\n",
      "Epoch 3452, Loss: 0.06859177723526955, Final Batch Loss: 0.029522277414798737\n",
      "Epoch 3453, Loss: 0.044931402429938316, Final Batch Loss: 0.028982559219002724\n",
      "Epoch 3454, Loss: 0.026876854710280895, Final Batch Loss: 0.008491636253893375\n",
      "Epoch 3455, Loss: 0.051344387233257294, Final Batch Loss: 0.01730016991496086\n",
      "Epoch 3456, Loss: 0.09152986481785774, Final Batch Loss: 0.05312452092766762\n",
      "Epoch 3457, Loss: 0.11505229026079178, Final Batch Loss: 0.06017701327800751\n",
      "Epoch 3458, Loss: 0.0864274725317955, Final Batch Loss: 0.03286091238260269\n",
      "Epoch 3459, Loss: 0.09910126961767673, Final Batch Loss: 0.01824604906141758\n",
      "Epoch 3460, Loss: 0.0375740360468626, Final Batch Loss: 0.01592175103724003\n",
      "Epoch 3461, Loss: 0.03613848611712456, Final Batch Loss: 0.00931655801832676\n",
      "Epoch 3462, Loss: 0.12659166008234024, Final Batch Loss: 0.06317138671875\n",
      "Epoch 3463, Loss: 0.02683217218145728, Final Batch Loss: 0.006382181774824858\n",
      "Epoch 3464, Loss: 0.12512578163295984, Final Batch Loss: 0.015537098981440067\n",
      "Epoch 3465, Loss: 0.05414043925702572, Final Batch Loss: 0.020294805988669395\n",
      "Epoch 3466, Loss: 0.10172953084111214, Final Batch Loss: 0.057775139808654785\n",
      "Epoch 3467, Loss: 0.05933537520468235, Final Batch Loss: 0.03368755057454109\n",
      "Epoch 3468, Loss: 0.0703785065561533, Final Batch Loss: 0.02194075845181942\n",
      "Epoch 3469, Loss: 0.05368375591933727, Final Batch Loss: 0.03160066902637482\n",
      "Epoch 3470, Loss: 0.04826910048723221, Final Batch Loss: 0.02207369916141033\n",
      "Epoch 3471, Loss: 0.11315474659204483, Final Batch Loss: 0.07562161237001419\n",
      "Epoch 3472, Loss: 0.04389637429267168, Final Batch Loss: 0.009415079839527607\n",
      "Epoch 3473, Loss: 0.08361497148871422, Final Batch Loss: 0.03801971673965454\n",
      "Epoch 3474, Loss: 0.08193160593509674, Final Batch Loss: 0.039653677493333817\n",
      "Epoch 3475, Loss: 0.10122789442539215, Final Batch Loss: 0.06884151697158813\n",
      "Epoch 3476, Loss: 0.056145159527659416, Final Batch Loss: 0.021193528547883034\n",
      "Epoch 3477, Loss: 0.10962368920445442, Final Batch Loss: 0.08221759647130966\n",
      "Epoch 3478, Loss: 0.054159242659807205, Final Batch Loss: 0.02629455365240574\n",
      "Epoch 3479, Loss: 0.13841962069272995, Final Batch Loss: 0.10368393361568451\n",
      "Epoch 3480, Loss: 0.08193051069974899, Final Batch Loss: 0.045032572001218796\n",
      "Epoch 3481, Loss: 0.08269441686570644, Final Batch Loss: 0.01839648000895977\n",
      "Epoch 3482, Loss: 0.0985075905919075, Final Batch Loss: 0.08148512989282608\n",
      "Epoch 3483, Loss: 0.04804620146751404, Final Batch Loss: 0.013912040740251541\n",
      "Epoch 3484, Loss: 0.1028200276196003, Final Batch Loss: 0.03263694420456886\n",
      "Epoch 3485, Loss: 0.0535751897841692, Final Batch Loss: 0.017192242667078972\n",
      "Epoch 3486, Loss: 0.06384547054767609, Final Batch Loss: 0.017614763230085373\n",
      "Epoch 3487, Loss: 0.03385855630040169, Final Batch Loss: 0.012808280065655708\n",
      "Epoch 3488, Loss: 0.028803416527807713, Final Batch Loss: 0.014618211425840855\n",
      "Epoch 3489, Loss: 0.03879218548536301, Final Batch Loss: 0.01846405677497387\n",
      "Epoch 3490, Loss: 0.08837028220295906, Final Batch Loss: 0.05238534137606621\n",
      "Epoch 3491, Loss: 0.06100122258067131, Final Batch Loss: 0.022787779569625854\n",
      "Epoch 3492, Loss: 0.058372579514980316, Final Batch Loss: 0.016900725662708282\n",
      "Epoch 3493, Loss: 0.04524606838822365, Final Batch Loss: 0.013635363429784775\n",
      "Epoch 3494, Loss: 0.14622578583657742, Final Batch Loss: 0.11872120201587677\n",
      "Epoch 3495, Loss: 0.05843229778110981, Final Batch Loss: 0.03323778882622719\n",
      "Epoch 3496, Loss: 0.04726269282400608, Final Batch Loss: 0.02790718711912632\n",
      "Epoch 3497, Loss: 0.03788990341126919, Final Batch Loss: 0.01364363543689251\n",
      "Epoch 3498, Loss: 0.06586645543575287, Final Batch Loss: 0.04494417458772659\n",
      "Epoch 3499, Loss: 0.14341668412089348, Final Batch Loss: 0.03818048909306526\n",
      "Epoch 3500, Loss: 0.060070622712373734, Final Batch Loss: 0.0053664930164813995\n",
      "Epoch 3501, Loss: 0.07863187789916992, Final Batch Loss: 0.04157726466655731\n",
      "Epoch 3502, Loss: 0.057492220774292946, Final Batch Loss: 0.032326728105545044\n",
      "Epoch 3503, Loss: 0.08651020750403404, Final Batch Loss: 0.056901540607213974\n",
      "Epoch 3504, Loss: 0.07862861454486847, Final Batch Loss: 0.03190944716334343\n",
      "Epoch 3505, Loss: 0.10946618020534515, Final Batch Loss: 0.07265585660934448\n",
      "Epoch 3506, Loss: 0.05438424460589886, Final Batch Loss: 0.027014100924134254\n",
      "Epoch 3507, Loss: 0.047406491823494434, Final Batch Loss: 0.014170248992741108\n",
      "Epoch 3508, Loss: 0.07779953442513943, Final Batch Loss: 0.020812371745705605\n",
      "Epoch 3509, Loss: 0.21280107647180557, Final Batch Loss: 0.13102152943611145\n",
      "Epoch 3510, Loss: 0.09875839203596115, Final Batch Loss: 0.036587174981832504\n",
      "Epoch 3511, Loss: 0.04428853280842304, Final Batch Loss: 0.027728907763957977\n",
      "Epoch 3512, Loss: 0.07440880686044693, Final Batch Loss: 0.029441475868225098\n",
      "Epoch 3513, Loss: 0.05205551674589515, Final Batch Loss: 0.007378434296697378\n",
      "Epoch 3514, Loss: 0.06506418436765671, Final Batch Loss: 0.03082491084933281\n",
      "Epoch 3515, Loss: 0.1418619006872177, Final Batch Loss: 0.11840999871492386\n",
      "Epoch 3516, Loss: 0.10720964148640633, Final Batch Loss: 0.05082414299249649\n",
      "Epoch 3517, Loss: 0.07203530892729759, Final Batch Loss: 0.02831597998738289\n",
      "Epoch 3518, Loss: 0.0524606890976429, Final Batch Loss: 0.010413073003292084\n",
      "Epoch 3519, Loss: 0.0314364992082119, Final Batch Loss: 0.00741465762257576\n",
      "Epoch 3520, Loss: 0.14881175011396408, Final Batch Loss: 0.04482550919055939\n",
      "Epoch 3521, Loss: 0.06263238750398159, Final Batch Loss: 0.03111897222697735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3522, Loss: 0.08676860481500626, Final Batch Loss: 0.04826654866337776\n",
      "Epoch 3523, Loss: 0.03817979618906975, Final Batch Loss: 0.01621004194021225\n",
      "Epoch 3524, Loss: 0.1333312951028347, Final Batch Loss: 0.09696752578020096\n",
      "Epoch 3525, Loss: 0.04947525076568127, Final Batch Loss: 0.018238557502627373\n",
      "Epoch 3526, Loss: 0.07331226952373981, Final Batch Loss: 0.054298873990774155\n",
      "Epoch 3527, Loss: 0.025350334122776985, Final Batch Loss: 0.010818172246217728\n",
      "Epoch 3528, Loss: 0.09317183308303356, Final Batch Loss: 0.07239633053541183\n",
      "Epoch 3529, Loss: 0.16166268847882748, Final Batch Loss: 0.13137701153755188\n",
      "Epoch 3530, Loss: 0.13284080382436514, Final Batch Loss: 0.008671401999890804\n",
      "Epoch 3531, Loss: 0.0742554422467947, Final Batch Loss: 0.016578542068600655\n",
      "Epoch 3532, Loss: 0.07520048134028912, Final Batch Loss: 0.023371832445263863\n",
      "Epoch 3533, Loss: 0.0603856947273016, Final Batch Loss: 0.03812494874000549\n",
      "Epoch 3534, Loss: 0.038581132888793945, Final Batch Loss: 0.019083218649029732\n",
      "Epoch 3535, Loss: 0.08188003301620483, Final Batch Loss: 0.04138791188597679\n",
      "Epoch 3536, Loss: 0.10425990447402, Final Batch Loss: 0.08106880635023117\n",
      "Epoch 3537, Loss: 0.13092700392007828, Final Batch Loss: 0.0913953185081482\n",
      "Epoch 3538, Loss: 0.05440708063542843, Final Batch Loss: 0.023755615577101707\n",
      "Epoch 3539, Loss: 0.10814842954277992, Final Batch Loss: 0.06122332811355591\n",
      "Epoch 3540, Loss: 0.0427887961268425, Final Batch Loss: 0.023077666759490967\n",
      "Epoch 3541, Loss: 0.08298287354409695, Final Batch Loss: 0.03005569986999035\n",
      "Epoch 3542, Loss: 0.11196685582399368, Final Batch Loss: 0.07564225792884827\n",
      "Epoch 3543, Loss: 0.07766762748360634, Final Batch Loss: 0.03973617032170296\n",
      "Epoch 3544, Loss: 0.0553436940535903, Final Batch Loss: 0.009537051431834698\n",
      "Epoch 3545, Loss: 0.058069758117198944, Final Batch Loss: 0.020491257309913635\n",
      "Epoch 3546, Loss: 0.11059760674834251, Final Batch Loss: 0.04570355638861656\n",
      "Epoch 3547, Loss: 0.06832962110638618, Final Batch Loss: 0.025759335607290268\n",
      "Epoch 3548, Loss: 0.04570898599922657, Final Batch Loss: 0.02055194042623043\n",
      "Epoch 3549, Loss: 0.06703998520970345, Final Batch Loss: 0.020348627120256424\n",
      "Epoch 3550, Loss: 0.05384483002126217, Final Batch Loss: 0.02047201432287693\n",
      "Epoch 3551, Loss: 0.100709717720747, Final Batch Loss: 0.06364643573760986\n",
      "Epoch 3552, Loss: 0.07543785311281681, Final Batch Loss: 0.05225541815161705\n",
      "Epoch 3553, Loss: 0.11995023116469383, Final Batch Loss: 0.08937046676874161\n",
      "Epoch 3554, Loss: 0.04747008718550205, Final Batch Loss: 0.028827853500843048\n",
      "Epoch 3555, Loss: 0.0471909586340189, Final Batch Loss: 0.02802938222885132\n",
      "Epoch 3556, Loss: 0.10970066487789154, Final Batch Loss: 0.04393528401851654\n",
      "Epoch 3557, Loss: 0.055381251499056816, Final Batch Loss: 0.016287779435515404\n",
      "Epoch 3558, Loss: 0.04894214682281017, Final Batch Loss: 0.017401164397597313\n",
      "Epoch 3559, Loss: 0.06216013804078102, Final Batch Loss: 0.021788910031318665\n",
      "Epoch 3560, Loss: 0.09128591418266296, Final Batch Loss: 0.06036725267767906\n",
      "Epoch 3561, Loss: 0.07133206166327, Final Batch Loss: 0.05354972183704376\n",
      "Epoch 3562, Loss: 0.06210464611649513, Final Batch Loss: 0.04150407388806343\n",
      "Epoch 3563, Loss: 0.039594002068042755, Final Batch Loss: 0.023183587938547134\n",
      "Epoch 3564, Loss: 0.08955549076199532, Final Batch Loss: 0.07102806866168976\n",
      "Epoch 3565, Loss: 0.08102407306432724, Final Batch Loss: 0.041476115584373474\n",
      "Epoch 3566, Loss: 0.053098744712769985, Final Batch Loss: 0.03980332240462303\n",
      "Epoch 3567, Loss: 0.07133131846785545, Final Batch Loss: 0.03262393921613693\n",
      "Epoch 3568, Loss: 0.06177731230854988, Final Batch Loss: 0.03260887414216995\n",
      "Epoch 3569, Loss: 0.05576789379119873, Final Batch Loss: 0.019614234566688538\n",
      "Epoch 3570, Loss: 0.04102475568652153, Final Batch Loss: 0.023124752566218376\n",
      "Epoch 3571, Loss: 0.04564459063112736, Final Batch Loss: 0.013877706602215767\n",
      "Epoch 3572, Loss: 0.042309232987463474, Final Batch Loss: 0.004417312331497669\n",
      "Epoch 3573, Loss: 0.09218882396817207, Final Batch Loss: 0.055999018251895905\n",
      "Epoch 3574, Loss: 0.08539802953600883, Final Batch Loss: 0.02653774991631508\n",
      "Epoch 3575, Loss: 0.09600899182260036, Final Batch Loss: 0.07118964195251465\n",
      "Epoch 3576, Loss: 0.04736512806266546, Final Batch Loss: 0.00927125196903944\n",
      "Epoch 3577, Loss: 0.0897744782269001, Final Batch Loss: 0.03457081690430641\n",
      "Epoch 3578, Loss: 0.0534183643758297, Final Batch Loss: 0.016033057123422623\n",
      "Epoch 3579, Loss: 0.0672372318804264, Final Batch Loss: 0.02611750364303589\n",
      "Epoch 3580, Loss: 0.05964531935751438, Final Batch Loss: 0.01957015134394169\n",
      "Epoch 3581, Loss: 0.11979037895798683, Final Batch Loss: 0.08747172355651855\n",
      "Epoch 3582, Loss: 0.07452310994267464, Final Batch Loss: 0.046875424683094025\n",
      "Epoch 3583, Loss: 0.06455964967608452, Final Batch Loss: 0.016218043863773346\n",
      "Epoch 3584, Loss: 0.47909165546298027, Final Batch Loss: 0.4192216694355011\n",
      "Epoch 3585, Loss: 0.05666669178754091, Final Batch Loss: 0.014637154527008533\n",
      "Epoch 3586, Loss: 0.056578327901661396, Final Batch Loss: 0.014376801438629627\n",
      "Epoch 3587, Loss: 0.07248707115650177, Final Batch Loss: 0.035863153636455536\n",
      "Epoch 3588, Loss: 0.11307781934738159, Final Batch Loss: 0.07645964622497559\n",
      "Epoch 3589, Loss: 0.13639531657099724, Final Batch Loss: 0.07784318178892136\n",
      "Epoch 3590, Loss: 0.07033500634133816, Final Batch Loss: 0.016789963468909264\n",
      "Epoch 3591, Loss: 0.05445052124559879, Final Batch Loss: 0.029032012447714806\n",
      "Epoch 3592, Loss: 0.04501033388078213, Final Batch Loss: 0.014436570927500725\n",
      "Epoch 3593, Loss: 0.09105623513460159, Final Batch Loss: 0.04777047410607338\n",
      "Epoch 3594, Loss: 0.055735863745212555, Final Batch Loss: 0.018626075237989426\n",
      "Epoch 3595, Loss: 0.06724555231630802, Final Batch Loss: 0.029079770669341087\n",
      "Epoch 3596, Loss: 0.08930818177759647, Final Batch Loss: 0.023157307878136635\n",
      "Epoch 3597, Loss: 0.035058097913861275, Final Batch Loss: 0.017523566260933876\n",
      "Epoch 3598, Loss: 0.09706272557377815, Final Batch Loss: 0.041823916137218475\n",
      "Epoch 3599, Loss: 0.07559261843562126, Final Batch Loss: 0.0529426634311676\n",
      "Epoch 3600, Loss: 0.04887408763170242, Final Batch Loss: 0.02353600412607193\n",
      "Epoch 3601, Loss: 0.048847051337361336, Final Batch Loss: 0.01734842173755169\n",
      "Epoch 3602, Loss: 0.028120643459260464, Final Batch Loss: 0.014696148224174976\n",
      "Epoch 3603, Loss: 0.0540190814062953, Final Batch Loss: 0.03966005519032478\n",
      "Epoch 3604, Loss: 0.045405808836221695, Final Batch Loss: 0.019242875277996063\n",
      "Epoch 3605, Loss: 0.07858998142182827, Final Batch Loss: 0.024924257770180702\n",
      "Epoch 3606, Loss: 0.14540383592247963, Final Batch Loss: 0.12062005698680878\n",
      "Epoch 3607, Loss: 0.16657743975520134, Final Batch Loss: 0.14312922954559326\n",
      "Epoch 3608, Loss: 0.06342167779803276, Final Batch Loss: 0.024151820689439774\n",
      "Epoch 3609, Loss: 0.03241002094000578, Final Batch Loss: 0.009146531112492085\n",
      "Epoch 3610, Loss: 0.04828763473778963, Final Batch Loss: 0.012009565718472004\n",
      "Epoch 3611, Loss: 0.058390527963638306, Final Batch Loss: 0.010380160063505173\n",
      "Epoch 3612, Loss: 0.061050763353705406, Final Batch Loss: 0.022707408294081688\n",
      "Epoch 3613, Loss: 0.029470127075910568, Final Batch Loss: 0.013770032674074173\n",
      "Epoch 3614, Loss: 0.09358448907732964, Final Batch Loss: 0.03811062499880791\n",
      "Epoch 3615, Loss: 0.061762197874486446, Final Batch Loss: 0.012246358208358288\n",
      "Epoch 3616, Loss: 0.15218234807252884, Final Batch Loss: 0.062826007604599\n",
      "Epoch 3617, Loss: 0.03745603282004595, Final Batch Loss: 0.01369441021233797\n",
      "Epoch 3618, Loss: 0.08863561600446701, Final Batch Loss: 0.059714481234550476\n",
      "Epoch 3619, Loss: 0.031146125867962837, Final Batch Loss: 0.0072154272347688675\n",
      "Epoch 3620, Loss: 0.11404205858707428, Final Batch Loss: 0.030827075242996216\n",
      "Epoch 3621, Loss: 0.12309248559176922, Final Batch Loss: 0.09843176603317261\n",
      "Epoch 3622, Loss: 0.058926910161972046, Final Batch Loss: 0.030124211683869362\n",
      "Epoch 3623, Loss: 0.051475247368216515, Final Batch Loss: 0.0201969426125288\n",
      "Epoch 3624, Loss: 0.07606644928455353, Final Batch Loss: 0.008924946188926697\n",
      "Epoch 3625, Loss: 0.07359306886792183, Final Batch Loss: 0.041566915810108185\n",
      "Epoch 3626, Loss: 0.0714202020317316, Final Batch Loss: 0.026780156418681145\n",
      "Epoch 3627, Loss: 0.08285035565495491, Final Batch Loss: 0.03780823573470116\n",
      "Epoch 3628, Loss: 0.07607634924352169, Final Batch Loss: 0.052649397403001785\n",
      "Epoch 3629, Loss: 0.03714323043823242, Final Batch Loss: 0.01875707134604454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3630, Loss: 0.06391988322138786, Final Batch Loss: 0.03014887124300003\n",
      "Epoch 3631, Loss: 0.06628292426466942, Final Batch Loss: 0.04284752905368805\n",
      "Epoch 3632, Loss: 0.14531448483467102, Final Batch Loss: 0.09378471225500107\n",
      "Epoch 3633, Loss: 0.09254274144768715, Final Batch Loss: 0.046612177044153214\n",
      "Epoch 3634, Loss: 0.03337707184255123, Final Batch Loss: 0.013402694836258888\n",
      "Epoch 3635, Loss: 0.07943955063819885, Final Batch Loss: 0.03248026221990585\n",
      "Epoch 3636, Loss: 0.11886051669716835, Final Batch Loss: 0.04532988741993904\n",
      "Epoch 3637, Loss: 0.0381639301776886, Final Batch Loss: 0.017962029203772545\n",
      "Epoch 3638, Loss: 0.04952920414507389, Final Batch Loss: 0.03365126624703407\n",
      "Epoch 3639, Loss: 0.035670037381350994, Final Batch Loss: 0.010938248597085476\n",
      "Epoch 3640, Loss: 0.06538308039307594, Final Batch Loss: 0.03343372046947479\n",
      "Epoch 3641, Loss: 0.09600692987442017, Final Batch Loss: 0.04298032447695732\n",
      "Epoch 3642, Loss: 0.046948038041591644, Final Batch Loss: 0.026299593970179558\n",
      "Epoch 3643, Loss: 0.06405571103096008, Final Batch Loss: 0.012130588293075562\n",
      "Epoch 3644, Loss: 0.09054884873330593, Final Batch Loss: 0.06380342692136765\n",
      "Epoch 3645, Loss: 0.03314569778740406, Final Batch Loss: 0.017607683315873146\n",
      "Epoch 3646, Loss: 0.04076883988454938, Final Batch Loss: 0.007404190022498369\n",
      "Epoch 3647, Loss: 0.053930727764964104, Final Batch Loss: 0.034729477018117905\n",
      "Epoch 3648, Loss: 0.10135206952691078, Final Batch Loss: 0.060074757784605026\n",
      "Epoch 3649, Loss: 0.07475146930664778, Final Batch Loss: 0.013758684508502483\n",
      "Epoch 3650, Loss: 0.08412574045360088, Final Batch Loss: 0.02413577027618885\n",
      "Epoch 3651, Loss: 0.05237250216305256, Final Batch Loss: 0.030952082946896553\n",
      "Epoch 3652, Loss: 0.08201710321009159, Final Batch Loss: 0.029870061203837395\n",
      "Epoch 3653, Loss: 0.06057067587971687, Final Batch Loss: 0.033345360308885574\n",
      "Epoch 3654, Loss: 0.030388230457901955, Final Batch Loss: 0.01335880532860756\n",
      "Epoch 3655, Loss: 0.04710805043578148, Final Batch Loss: 0.012975633144378662\n",
      "Epoch 3656, Loss: 0.043768074829131365, Final Batch Loss: 0.007359333802014589\n",
      "Epoch 3657, Loss: 0.0661826804280281, Final Batch Loss: 0.04008924588561058\n",
      "Epoch 3658, Loss: 0.06558137200772762, Final Batch Loss: 0.01811179332435131\n",
      "Epoch 3659, Loss: 0.03926713019609451, Final Batch Loss: 0.0194594394415617\n",
      "Epoch 3660, Loss: 0.035001098178327084, Final Batch Loss: 0.009987064637243748\n",
      "Epoch 3661, Loss: 0.05568886734545231, Final Batch Loss: 0.02823108434677124\n",
      "Epoch 3662, Loss: 0.04528132826089859, Final Batch Loss: 0.007134776562452316\n",
      "Epoch 3663, Loss: 0.06025339104235172, Final Batch Loss: 0.0308376532047987\n",
      "Epoch 3664, Loss: 0.05713025480508804, Final Batch Loss: 0.02486218884587288\n",
      "Epoch 3665, Loss: 0.11765280365943909, Final Batch Loss: 0.07207117229700089\n",
      "Epoch 3666, Loss: 0.07314353808760643, Final Batch Loss: 0.041533201932907104\n",
      "Epoch 3667, Loss: 0.08827625587582588, Final Batch Loss: 0.03398121893405914\n",
      "Epoch 3668, Loss: 0.06281417235732079, Final Batch Loss: 0.016293376684188843\n",
      "Epoch 3669, Loss: 0.07959477230906487, Final Batch Loss: 0.02503136545419693\n",
      "Epoch 3670, Loss: 0.07341829314827919, Final Batch Loss: 0.04987335577607155\n",
      "Epoch 3671, Loss: 0.07539986073970795, Final Batch Loss: 0.01556146889925003\n",
      "Epoch 3672, Loss: 0.05061512254178524, Final Batch Loss: 0.01863115094602108\n",
      "Epoch 3673, Loss: 0.0913156159222126, Final Batch Loss: 0.05765114352107048\n",
      "Epoch 3674, Loss: 0.06643173098564148, Final Batch Loss: 0.04321102797985077\n",
      "Epoch 3675, Loss: 0.13159172981977463, Final Batch Loss: 0.03275196999311447\n",
      "Epoch 3676, Loss: 0.11181426607072353, Final Batch Loss: 0.023242389783263206\n",
      "Epoch 3677, Loss: 0.04753652587532997, Final Batch Loss: 0.0161026231944561\n",
      "Epoch 3678, Loss: 0.03604589216411114, Final Batch Loss: 0.016768569126725197\n",
      "Epoch 3679, Loss: 0.056123122572898865, Final Batch Loss: 0.030135970562696457\n",
      "Epoch 3680, Loss: 0.03764722775667906, Final Batch Loss: 0.023295389488339424\n",
      "Epoch 3681, Loss: 0.038362110033631325, Final Batch Loss: 0.018729986622929573\n",
      "Epoch 3682, Loss: 0.07147299684584141, Final Batch Loss: 0.052188899368047714\n",
      "Epoch 3683, Loss: 0.06466355919837952, Final Batch Loss: 0.03331036865711212\n",
      "Epoch 3684, Loss: 0.04043591767549515, Final Batch Loss: 0.017408203333616257\n",
      "Epoch 3685, Loss: 0.046340132132172585, Final Batch Loss: 0.02867787890136242\n",
      "Epoch 3686, Loss: 0.05226825177669525, Final Batch Loss: 0.023494770750403404\n",
      "Epoch 3687, Loss: 0.05823158286511898, Final Batch Loss: 0.03492256626486778\n",
      "Epoch 3688, Loss: 0.06482709012925625, Final Batch Loss: 0.02844524197280407\n",
      "Epoch 3689, Loss: 0.12146470416337252, Final Batch Loss: 0.008325939066708088\n",
      "Epoch 3690, Loss: 0.035497305914759636, Final Batch Loss: 0.016229942440986633\n",
      "Epoch 3691, Loss: 0.03635326446965337, Final Batch Loss: 0.007341668475419283\n",
      "Epoch 3692, Loss: 0.025661575607955456, Final Batch Loss: 0.005510321818292141\n",
      "Epoch 3693, Loss: 0.07614334113895893, Final Batch Loss: 0.022782376036047935\n",
      "Epoch 3694, Loss: 0.08775351196527481, Final Batch Loss: 0.022888101637363434\n",
      "Epoch 3695, Loss: 0.08679414354264736, Final Batch Loss: 0.05894550681114197\n",
      "Epoch 3696, Loss: 0.027267795987427235, Final Batch Loss: 0.00946465041488409\n",
      "Epoch 3697, Loss: 0.03949370048940182, Final Batch Loss: 0.019361086189746857\n",
      "Epoch 3698, Loss: 0.048703634180128574, Final Batch Loss: 0.011479885317385197\n",
      "Epoch 3699, Loss: 0.05986917391419411, Final Batch Loss: 0.014562319964170456\n",
      "Epoch 3700, Loss: 0.07008437253534794, Final Batch Loss: 0.0420849435031414\n",
      "Epoch 3701, Loss: 0.04470293642953038, Final Batch Loss: 0.006679453421384096\n",
      "Epoch 3702, Loss: 0.03592555783689022, Final Batch Loss: 0.012130636721849442\n",
      "Epoch 3703, Loss: 0.042260102927684784, Final Batch Loss: 0.02193385176360607\n",
      "Epoch 3704, Loss: 0.06472339667379856, Final Batch Loss: 0.01477871648967266\n",
      "Epoch 3705, Loss: 0.051181089133024216, Final Batch Loss: 0.007615014910697937\n",
      "Epoch 3706, Loss: 0.09518638346344233, Final Batch Loss: 0.08237214386463165\n",
      "Epoch 3707, Loss: 0.03190168784931302, Final Batch Loss: 0.005679725203663111\n",
      "Epoch 3708, Loss: 0.08683980628848076, Final Batch Loss: 0.07097957283258438\n",
      "Epoch 3709, Loss: 0.13056811690330505, Final Batch Loss: 0.09223863482475281\n",
      "Epoch 3710, Loss: 0.0661046002060175, Final Batch Loss: 0.023914752528071404\n",
      "Epoch 3711, Loss: 0.03804828226566315, Final Batch Loss: 0.010783582925796509\n",
      "Epoch 3712, Loss: 0.04044337198138237, Final Batch Loss: 0.020916564390063286\n",
      "Epoch 3713, Loss: 0.07362537831068039, Final Batch Loss: 0.02489340305328369\n",
      "Epoch 3714, Loss: 0.06173832714557648, Final Batch Loss: 0.029096905142068863\n",
      "Epoch 3715, Loss: 0.03870795667171478, Final Batch Loss: 0.004897415637969971\n",
      "Epoch 3716, Loss: 0.0409700246527791, Final Batch Loss: 0.005364955402910709\n",
      "Epoch 3717, Loss: 0.04721422865986824, Final Batch Loss: 0.016276227310299873\n",
      "Epoch 3718, Loss: 0.041306053288280964, Final Batch Loss: 0.014728137291967869\n",
      "Epoch 3719, Loss: 0.06197274290025234, Final Batch Loss: 0.029352432116866112\n",
      "Epoch 3720, Loss: 0.06265025399625301, Final Batch Loss: 0.02275891788303852\n",
      "Epoch 3721, Loss: 0.1597910411655903, Final Batch Loss: 0.04608512297272682\n",
      "Epoch 3722, Loss: 0.23983638733625412, Final Batch Loss: 0.07161317020654678\n",
      "Epoch 3723, Loss: 0.05114440992474556, Final Batch Loss: 0.03315170109272003\n",
      "Epoch 3724, Loss: 0.03901777137070894, Final Batch Loss: 0.01114435214549303\n",
      "Epoch 3725, Loss: 0.07926194742321968, Final Batch Loss: 0.034676484763622284\n",
      "Epoch 3726, Loss: 0.09467510506510735, Final Batch Loss: 0.05883479863405228\n",
      "Epoch 3727, Loss: 0.08270858228206635, Final Batch Loss: 0.05002103000879288\n",
      "Epoch 3728, Loss: 0.05061872489750385, Final Batch Loss: 0.008950138464570045\n",
      "Epoch 3729, Loss: 0.1804608404636383, Final Batch Loss: 0.1115867868065834\n",
      "Epoch 3730, Loss: 0.09856889024376869, Final Batch Loss: 0.03413500264286995\n",
      "Epoch 3731, Loss: 0.0586241390556097, Final Batch Loss: 0.031250741332769394\n",
      "Epoch 3732, Loss: 0.052272072061896324, Final Batch Loss: 0.02527761645615101\n",
      "Epoch 3733, Loss: 0.06317377090454102, Final Batch Loss: 0.019925974309444427\n",
      "Epoch 3734, Loss: 0.07080399617552757, Final Batch Loss: 0.0194084532558918\n",
      "Epoch 3735, Loss: 0.07210525311529636, Final Batch Loss: 0.017783554270863533\n",
      "Epoch 3736, Loss: 0.03939236141741276, Final Batch Loss: 0.016983047127723694\n",
      "Epoch 3737, Loss: 0.04473800212144852, Final Batch Loss: 0.020412608981132507\n",
      "Epoch 3738, Loss: 0.053384557366371155, Final Batch Loss: 0.020669367164373398\n",
      "Epoch 3739, Loss: 0.07220167852938175, Final Batch Loss: 0.02270440198481083\n",
      "Epoch 3740, Loss: 0.03147396817803383, Final Batch Loss: 0.017731456086039543\n",
      "Epoch 3741, Loss: 0.06827424839138985, Final Batch Loss: 0.021141838282346725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3742, Loss: 0.06082942150533199, Final Batch Loss: 0.017399000003933907\n",
      "Epoch 3743, Loss: 0.14583633467555046, Final Batch Loss: 0.12856504321098328\n",
      "Epoch 3744, Loss: 0.066916074603796, Final Batch Loss: 0.04175449535250664\n",
      "Epoch 3745, Loss: 0.03385224100202322, Final Batch Loss: 0.009852849878370762\n",
      "Epoch 3746, Loss: 0.07988417148590088, Final Batch Loss: 0.03325214982032776\n",
      "Epoch 3747, Loss: 0.08517611026763916, Final Batch Loss: 0.06657665967941284\n",
      "Epoch 3748, Loss: 0.05408790335059166, Final Batch Loss: 0.021670278161764145\n",
      "Epoch 3749, Loss: 0.04822243005037308, Final Batch Loss: 0.022186633199453354\n",
      "Epoch 3750, Loss: 0.06925845146179199, Final Batch Loss: 0.04454490542411804\n",
      "Epoch 3751, Loss: 0.06987405754625797, Final Batch Loss: 0.030981915071606636\n",
      "Epoch 3752, Loss: 0.11829526349902153, Final Batch Loss: 0.0862661674618721\n",
      "Epoch 3753, Loss: 0.054774943739175797, Final Batch Loss: 0.02927233651280403\n",
      "Epoch 3754, Loss: 0.07254103198647499, Final Batch Loss: 0.01052292063832283\n",
      "Epoch 3755, Loss: 0.10287288390100002, Final Batch Loss: 0.02751939557492733\n",
      "Epoch 3756, Loss: 0.06472491752356291, Final Batch Loss: 0.011409359984099865\n",
      "Epoch 3757, Loss: 0.06957422196865082, Final Batch Loss: 0.022420279681682587\n",
      "Epoch 3758, Loss: 0.04170682653784752, Final Batch Loss: 0.018761811777949333\n",
      "Epoch 3759, Loss: 0.02357605192810297, Final Batch Loss: 0.0068196142092347145\n",
      "Epoch 3760, Loss: 0.06301013007760048, Final Batch Loss: 0.03889700397849083\n",
      "Epoch 3761, Loss: 0.05486199166625738, Final Batch Loss: 0.013699430041015148\n",
      "Epoch 3762, Loss: 0.05113381892442703, Final Batch Loss: 0.023268921300768852\n",
      "Epoch 3763, Loss: 0.20643506571650505, Final Batch Loss: 0.18043339252471924\n",
      "Epoch 3764, Loss: 0.06257718801498413, Final Batch Loss: 0.022301409393548965\n",
      "Epoch 3765, Loss: 0.05233409069478512, Final Batch Loss: 0.033544186502695084\n",
      "Epoch 3766, Loss: 0.0583247859030962, Final Batch Loss: 0.034033894538879395\n",
      "Epoch 3767, Loss: 0.02963325846940279, Final Batch Loss: 0.010205964557826519\n",
      "Epoch 3768, Loss: 0.08086946234107018, Final Batch Loss: 0.025170549750328064\n",
      "Epoch 3769, Loss: 0.06453777849674225, Final Batch Loss: 0.03273438289761543\n",
      "Epoch 3770, Loss: 0.06999119324609637, Final Batch Loss: 0.0054633463732898235\n",
      "Epoch 3771, Loss: 0.06725964695215225, Final Batch Loss: 0.041966792196035385\n",
      "Epoch 3772, Loss: 0.03937686700373888, Final Batch Loss: 0.023968856781721115\n",
      "Epoch 3773, Loss: 0.042965322732925415, Final Batch Loss: 0.01729850098490715\n",
      "Epoch 3774, Loss: 0.040818114299327135, Final Batch Loss: 0.007389545906335115\n",
      "Epoch 3775, Loss: 0.058186693117022514, Final Batch Loss: 0.03929242119193077\n",
      "Epoch 3776, Loss: 0.05691297445446253, Final Batch Loss: 0.01479147095233202\n",
      "Epoch 3777, Loss: 0.07916953042149544, Final Batch Loss: 0.045527152717113495\n",
      "Epoch 3778, Loss: 0.14462751895189285, Final Batch Loss: 0.07712835818529129\n",
      "Epoch 3779, Loss: 0.08909598737955093, Final Batch Loss: 0.036798037588596344\n",
      "Epoch 3780, Loss: 0.029517059214413166, Final Batch Loss: 0.010781626217067242\n",
      "Epoch 3781, Loss: 0.04259882867336273, Final Batch Loss: 0.016698570922017097\n",
      "Epoch 3782, Loss: 0.08548122644424438, Final Batch Loss: 0.05949878320097923\n",
      "Epoch 3783, Loss: 0.059701334685087204, Final Batch Loss: 0.03572109714150429\n",
      "Epoch 3784, Loss: 0.0699652498587966, Final Batch Loss: 0.014546151272952557\n",
      "Epoch 3785, Loss: 0.08200152963399887, Final Batch Loss: 0.033291321247816086\n",
      "Epoch 3786, Loss: 0.05836258828639984, Final Batch Loss: 0.013649314641952515\n",
      "Epoch 3787, Loss: 0.04434605687856674, Final Batch Loss: 0.01369490660727024\n",
      "Epoch 3788, Loss: 0.08409284800291061, Final Batch Loss: 0.019745036959648132\n",
      "Epoch 3789, Loss: 0.02611128892749548, Final Batch Loss: 0.010741944424808025\n",
      "Epoch 3790, Loss: 0.061069972813129425, Final Batch Loss: 0.017882101237773895\n",
      "Epoch 3791, Loss: 0.08543659746646881, Final Batch Loss: 0.06861288100481033\n",
      "Epoch 3792, Loss: 0.04292663373053074, Final Batch Loss: 0.015752246603369713\n",
      "Epoch 3793, Loss: 0.09028773941099644, Final Batch Loss: 0.06398637592792511\n",
      "Epoch 3794, Loss: 0.04278599750250578, Final Batch Loss: 0.006005234085023403\n",
      "Epoch 3795, Loss: 0.09214966185390949, Final Batch Loss: 0.031004970893263817\n",
      "Epoch 3796, Loss: 0.04386306740343571, Final Batch Loss: 0.020561175420880318\n",
      "Epoch 3797, Loss: 0.19949380680918694, Final Batch Loss: 0.15610113739967346\n",
      "Epoch 3798, Loss: 0.08914845436811447, Final Batch Loss: 0.04764002934098244\n",
      "Epoch 3799, Loss: 0.06173395551741123, Final Batch Loss: 0.039674822241067886\n",
      "Epoch 3800, Loss: 0.06107852794229984, Final Batch Loss: 0.0319061204791069\n",
      "Epoch 3801, Loss: 0.13918170146644115, Final Batch Loss: 0.10821004956960678\n",
      "Epoch 3802, Loss: 0.07369185611605644, Final Batch Loss: 0.03555958345532417\n",
      "Epoch 3803, Loss: 0.05340974358841777, Final Batch Loss: 0.007401920389384031\n",
      "Epoch 3804, Loss: 0.09372760728001595, Final Batch Loss: 0.06726061552762985\n",
      "Epoch 3805, Loss: 0.054039848037064075, Final Batch Loss: 0.014090367592871189\n",
      "Epoch 3806, Loss: 0.051214734092354774, Final Batch Loss: 0.03513060137629509\n",
      "Epoch 3807, Loss: 0.06469647865742445, Final Batch Loss: 0.013021538965404034\n",
      "Epoch 3808, Loss: 0.06365250796079636, Final Batch Loss: 0.027711767703294754\n",
      "Epoch 3809, Loss: 0.05103657301515341, Final Batch Loss: 0.009291823022067547\n",
      "Epoch 3810, Loss: 0.03732455661520362, Final Batch Loss: 0.004406848456710577\n",
      "Epoch 3811, Loss: 0.07153739780187607, Final Batch Loss: 0.059390485286712646\n",
      "Epoch 3812, Loss: 0.05397465452551842, Final Batch Loss: 0.02815699204802513\n",
      "Epoch 3813, Loss: 0.07576378248631954, Final Batch Loss: 0.05080496519804001\n",
      "Epoch 3814, Loss: 0.03991736797615886, Final Batch Loss: 0.007202714215964079\n",
      "Epoch 3815, Loss: 0.06071129068732262, Final Batch Loss: 0.021248672157526016\n",
      "Epoch 3816, Loss: 0.04945147782564163, Final Batch Loss: 0.019382594153285027\n",
      "Epoch 3817, Loss: 0.04686519503593445, Final Batch Loss: 0.015290480107069016\n",
      "Epoch 3818, Loss: 0.052177177742123604, Final Batch Loss: 0.014384949579834938\n",
      "Epoch 3819, Loss: 0.11627766489982605, Final Batch Loss: 0.03417272865772247\n",
      "Epoch 3820, Loss: 0.039528973400592804, Final Batch Loss: 0.01730266585946083\n",
      "Epoch 3821, Loss: 0.03687742166221142, Final Batch Loss: 0.008875427767634392\n",
      "Epoch 3822, Loss: 0.04414127441123128, Final Batch Loss: 0.0070296418853104115\n",
      "Epoch 3823, Loss: 0.06997408857569098, Final Batch Loss: 0.005560839083045721\n",
      "Epoch 3824, Loss: 0.10077069327235222, Final Batch Loss: 0.009272415190935135\n",
      "Epoch 3825, Loss: 0.021883749635890126, Final Batch Loss: 0.0038544379640370607\n",
      "Epoch 3826, Loss: 0.03065081126987934, Final Batch Loss: 0.012365410104393959\n",
      "Epoch 3827, Loss: 0.02500080643221736, Final Batch Loss: 0.006197275128215551\n",
      "Epoch 3828, Loss: 0.056578392162919044, Final Batch Loss: 0.022966982796788216\n",
      "Epoch 3829, Loss: 0.03787392936646938, Final Batch Loss: 0.021090125665068626\n",
      "Epoch 3830, Loss: 0.05343741178512573, Final Batch Loss: 0.02974487654864788\n",
      "Epoch 3831, Loss: 0.030161865521222353, Final Batch Loss: 0.0037942552007734776\n",
      "Epoch 3832, Loss: 0.118298658169806, Final Batch Loss: 0.015528948046267033\n",
      "Epoch 3833, Loss: 0.05343635752797127, Final Batch Loss: 0.03185368329286575\n",
      "Epoch 3834, Loss: 0.058660038746893406, Final Batch Loss: 0.009617977775633335\n",
      "Epoch 3835, Loss: 0.20044730231165886, Final Batch Loss: 0.16696423292160034\n",
      "Epoch 3836, Loss: 0.026606903411448002, Final Batch Loss: 0.00937488954514265\n",
      "Epoch 3837, Loss: 0.019157754722982645, Final Batch Loss: 0.005734205711632967\n",
      "Epoch 3838, Loss: 0.052648122888058424, Final Batch Loss: 0.005841515492647886\n",
      "Epoch 3839, Loss: 0.054440561681985855, Final Batch Loss: 0.0331844836473465\n",
      "Epoch 3840, Loss: 0.07798646157607436, Final Batch Loss: 0.006581199821084738\n",
      "Epoch 3841, Loss: 0.02902668761089444, Final Batch Loss: 0.007210074458271265\n",
      "Epoch 3842, Loss: 0.025169805623590946, Final Batch Loss: 0.013052083551883698\n",
      "Epoch 3843, Loss: 0.03380445018410683, Final Batch Loss: 0.008098440244793892\n",
      "Epoch 3844, Loss: 0.09302031248807907, Final Batch Loss: 0.03831690549850464\n",
      "Epoch 3845, Loss: 0.07645987160503864, Final Batch Loss: 0.03054567612707615\n",
      "Epoch 3846, Loss: 0.05209640599787235, Final Batch Loss: 0.01776203326880932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3847, Loss: 0.026659993454813957, Final Batch Loss: 0.013608219102025032\n",
      "Epoch 3848, Loss: 0.03447738382965326, Final Batch Loss: 0.0077287582680583\n",
      "Epoch 3849, Loss: 0.046200129203498363, Final Batch Loss: 0.010785254649817944\n",
      "Epoch 3850, Loss: 0.13523675501346588, Final Batch Loss: 0.11157379299402237\n",
      "Epoch 3851, Loss: 0.03334923833608627, Final Batch Loss: 0.006529105827212334\n",
      "Epoch 3852, Loss: 0.02876681461930275, Final Batch Loss: 0.015853868797421455\n",
      "Epoch 3853, Loss: 0.061627378687262535, Final Batch Loss: 0.039189621806144714\n",
      "Epoch 3854, Loss: 0.06520123220980167, Final Batch Loss: 0.016667531803250313\n",
      "Epoch 3855, Loss: 0.20584439858794212, Final Batch Loss: 0.15132930874824524\n",
      "Epoch 3856, Loss: 0.08194180577993393, Final Batch Loss: 0.040164798498153687\n",
      "Epoch 3857, Loss: 0.09303208068013191, Final Batch Loss: 0.04437767341732979\n",
      "Epoch 3858, Loss: 0.11343034729361534, Final Batch Loss: 0.04588238522410393\n",
      "Epoch 3859, Loss: 0.11238439753651619, Final Batch Loss: 0.06805741786956787\n",
      "Epoch 3860, Loss: 0.032861876767128706, Final Batch Loss: 0.005034932401031256\n",
      "Epoch 3861, Loss: 0.08077499642968178, Final Batch Loss: 0.024608757346868515\n",
      "Epoch 3862, Loss: 0.049534911289811134, Final Batch Loss: 0.019727798178792\n",
      "Epoch 3863, Loss: 0.2035508155822754, Final Batch Loss: 0.17035533487796783\n",
      "Epoch 3864, Loss: 0.09452598541975021, Final Batch Loss: 0.046616002917289734\n",
      "Epoch 3865, Loss: 0.04924373887479305, Final Batch Loss: 0.022428400814533234\n",
      "Epoch 3866, Loss: 0.10265840962529182, Final Batch Loss: 0.06331123411655426\n",
      "Epoch 3867, Loss: 0.05578936077654362, Final Batch Loss: 0.01801532693207264\n",
      "Epoch 3868, Loss: 0.07621932402253151, Final Batch Loss: 0.03681996837258339\n",
      "Epoch 3869, Loss: 0.06479238905012608, Final Batch Loss: 0.025978675112128258\n",
      "Epoch 3870, Loss: 0.08579021319746971, Final Batch Loss: 0.06798524409532547\n",
      "Epoch 3871, Loss: 0.15705478191375732, Final Batch Loss: 0.08927420526742935\n",
      "Epoch 3872, Loss: 0.08484293520450592, Final Batch Loss: 0.035027701407670975\n",
      "Epoch 3873, Loss: 0.08467885106801987, Final Batch Loss: 0.05255924537777901\n",
      "Epoch 3874, Loss: 0.020040294155478477, Final Batch Loss: 0.0055949511006474495\n",
      "Epoch 3875, Loss: 0.045215385034680367, Final Batch Loss: 0.0192432701587677\n",
      "Epoch 3876, Loss: 0.09086901322007179, Final Batch Loss: 0.0472838319838047\n",
      "Epoch 3877, Loss: 0.06347930990159512, Final Batch Loss: 0.04230384901165962\n",
      "Epoch 3878, Loss: 0.12564050778746605, Final Batch Loss: 0.09427035599946976\n",
      "Epoch 3879, Loss: 0.04032598342746496, Final Batch Loss: 0.00678048562258482\n",
      "Epoch 3880, Loss: 0.03702858183532953, Final Batch Loss: 0.010058262385427952\n",
      "Epoch 3881, Loss: 0.10206219181418419, Final Batch Loss: 0.0818549171090126\n",
      "Epoch 3882, Loss: 0.16222400218248367, Final Batch Loss: 0.1343613564968109\n",
      "Epoch 3883, Loss: 0.0361185297369957, Final Batch Loss: 0.016156796365976334\n",
      "Epoch 3884, Loss: 0.054244402796030045, Final Batch Loss: 0.03583129867911339\n",
      "Epoch 3885, Loss: 0.21218160167336464, Final Batch Loss: 0.16580724716186523\n",
      "Epoch 3886, Loss: 0.1875005941838026, Final Batch Loss: 0.1755581647157669\n",
      "Epoch 3887, Loss: 0.08050641231238842, Final Batch Loss: 0.03122193180024624\n",
      "Epoch 3888, Loss: 0.10370689257979393, Final Batch Loss: 0.06573203206062317\n",
      "Epoch 3889, Loss: 0.26862215995788574, Final Batch Loss: 0.17196759581565857\n",
      "Epoch 3890, Loss: 0.11641783639788628, Final Batch Loss: 0.07952579855918884\n",
      "Epoch 3891, Loss: 0.09871019050478935, Final Batch Loss: 0.07011809200048447\n",
      "Epoch 3892, Loss: 0.11077197454869747, Final Batch Loss: 0.08606290072202682\n",
      "Epoch 3893, Loss: 0.06513056345283985, Final Batch Loss: 0.015099624171853065\n",
      "Epoch 3894, Loss: 0.06062082573771477, Final Batch Loss: 0.023378513753414154\n",
      "Epoch 3895, Loss: 0.03513768268749118, Final Batch Loss: 0.0070807612501084805\n",
      "Epoch 3896, Loss: 0.04443663731217384, Final Batch Loss: 0.01583680883049965\n",
      "Epoch 3897, Loss: 0.05448395945131779, Final Batch Loss: 0.014864040538668633\n",
      "Epoch 3898, Loss: 0.03541932860389352, Final Batch Loss: 0.003470750991255045\n",
      "Epoch 3899, Loss: 0.0681077167391777, Final Batch Loss: 0.04744818061590195\n",
      "Epoch 3900, Loss: 0.0594868678599596, Final Batch Loss: 0.03663155436515808\n",
      "Epoch 3901, Loss: 0.05745682679116726, Final Batch Loss: 0.03436983749270439\n",
      "Epoch 3902, Loss: 0.041805583983659744, Final Batch Loss: 0.014888593927025795\n",
      "Epoch 3903, Loss: 0.14904962852597237, Final Batch Loss: 0.12067671865224838\n",
      "Epoch 3904, Loss: 0.03362465836107731, Final Batch Loss: 0.0178479366004467\n",
      "Epoch 3905, Loss: 0.09156399592757225, Final Batch Loss: 0.016963396221399307\n",
      "Epoch 3906, Loss: 0.013331607216969132, Final Batch Loss: 0.0023499701637774706\n",
      "Epoch 3907, Loss: 0.10011881962418556, Final Batch Loss: 0.05674296244978905\n",
      "Epoch 3908, Loss: 0.02907391730695963, Final Batch Loss: 0.008910040371119976\n",
      "Epoch 3909, Loss: 0.05247808201238513, Final Batch Loss: 0.0062949988059699535\n",
      "Epoch 3910, Loss: 0.037749046459794044, Final Batch Loss: 0.017463024705648422\n",
      "Epoch 3911, Loss: 0.07518243044614792, Final Batch Loss: 0.037498995661735535\n",
      "Epoch 3912, Loss: 0.04338154010474682, Final Batch Loss: 0.008734805509448051\n",
      "Epoch 3913, Loss: 0.048136137425899506, Final Batch Loss: 0.021451827138662338\n",
      "Epoch 3914, Loss: 0.03035491518676281, Final Batch Loss: 0.010708387941122055\n",
      "Epoch 3915, Loss: 0.04781586118042469, Final Batch Loss: 0.01641233079135418\n",
      "Epoch 3916, Loss: 0.05449276231229305, Final Batch Loss: 0.0340045690536499\n",
      "Epoch 3917, Loss: 0.058409376069903374, Final Batch Loss: 0.016602618619799614\n",
      "Epoch 3918, Loss: 0.050134556367993355, Final Batch Loss: 0.036617472767829895\n",
      "Epoch 3919, Loss: 0.04622301273047924, Final Batch Loss: 0.016571691259741783\n",
      "Epoch 3920, Loss: 0.08483681082725525, Final Batch Loss: 0.01836743950843811\n",
      "Epoch 3921, Loss: 0.07253812998533249, Final Batch Loss: 0.026179302483797073\n",
      "Epoch 3922, Loss: 0.10464353114366531, Final Batch Loss: 0.06854379177093506\n",
      "Epoch 3923, Loss: 0.10442964732646942, Final Batch Loss: 0.0343998521566391\n",
      "Epoch 3924, Loss: 0.03326418809592724, Final Batch Loss: 0.017431441694498062\n",
      "Epoch 3925, Loss: 0.07982958666980267, Final Batch Loss: 0.06280411034822464\n",
      "Epoch 3926, Loss: 0.09243275038897991, Final Batch Loss: 0.03008919022977352\n",
      "Epoch 3927, Loss: 0.06703969836235046, Final Batch Loss: 0.03232472017407417\n",
      "Epoch 3928, Loss: 0.042158907279372215, Final Batch Loss: 0.025283753871917725\n",
      "Epoch 3929, Loss: 0.06701496616005898, Final Batch Loss: 0.03451387584209442\n",
      "Epoch 3930, Loss: 0.05710386857390404, Final Batch Loss: 0.02285747230052948\n",
      "Epoch 3931, Loss: 0.08544641733169556, Final Batch Loss: 0.04910716786980629\n",
      "Epoch 3932, Loss: 0.056795405223965645, Final Batch Loss: 0.029390107840299606\n",
      "Epoch 3933, Loss: 0.06572483666241169, Final Batch Loss: 0.03620526194572449\n",
      "Epoch 3934, Loss: 0.025942133739590645, Final Batch Loss: 0.013083000667393208\n",
      "Epoch 3935, Loss: 0.07944942265748978, Final Batch Loss: 0.030235502868890762\n",
      "Epoch 3936, Loss: 0.0763123668730259, Final Batch Loss: 0.0592268630862236\n",
      "Epoch 3937, Loss: 0.09300010092556477, Final Batch Loss: 0.062139011919498444\n",
      "Epoch 3938, Loss: 0.059115784242749214, Final Batch Loss: 0.02896362543106079\n",
      "Epoch 3939, Loss: 0.08883580937981606, Final Batch Loss: 0.041469473391771317\n",
      "Epoch 3940, Loss: 0.047995383851230145, Final Batch Loss: 0.011532085947692394\n",
      "Epoch 3941, Loss: 0.07560998387634754, Final Batch Loss: 0.05715589225292206\n",
      "Epoch 3942, Loss: 0.03135658893734217, Final Batch Loss: 0.010475500486791134\n",
      "Epoch 3943, Loss: 0.09022893570363522, Final Batch Loss: 0.01971476338803768\n",
      "Epoch 3944, Loss: 0.06789866462349892, Final Batch Loss: 0.025925815105438232\n",
      "Epoch 3945, Loss: 0.10423069167882204, Final Batch Loss: 0.013686795718967915\n",
      "Epoch 3946, Loss: 0.09034595638513565, Final Batch Loss: 0.0574556365609169\n",
      "Epoch 3947, Loss: 0.03952176030725241, Final Batch Loss: 0.013636906631290913\n",
      "Epoch 3948, Loss: 0.02427760185673833, Final Batch Loss: 0.005467555020004511\n",
      "Epoch 3949, Loss: 0.048183929175138474, Final Batch Loss: 0.017568564042448997\n",
      "Epoch 3950, Loss: 0.08278747089207172, Final Batch Loss: 0.028709715232253075\n",
      "Epoch 3951, Loss: 0.06887617707252502, Final Batch Loss: 0.03518533334136009\n",
      "Epoch 3952, Loss: 0.06974363327026367, Final Batch Loss: 0.037682533264160156\n",
      "Epoch 3953, Loss: 0.06442827079445124, Final Batch Loss: 0.0126017602160573\n",
      "Epoch 3954, Loss: 0.08244872000068426, Final Batch Loss: 0.012874684296548367\n",
      "Epoch 3955, Loss: 0.05456359963864088, Final Batch Loss: 0.013991045765578747\n",
      "Epoch 3956, Loss: 0.033553012646734715, Final Batch Loss: 0.008728648535907269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3957, Loss: 0.07429897040128708, Final Batch Loss: 0.04178861528635025\n",
      "Epoch 3958, Loss: 0.06605309993028641, Final Batch Loss: 0.03585048392415047\n",
      "Epoch 3959, Loss: 0.07088112644851208, Final Batch Loss: 0.04877297952771187\n",
      "Epoch 3960, Loss: 0.053959859535098076, Final Batch Loss: 0.03457086905837059\n",
      "Epoch 3961, Loss: 0.09080505277961493, Final Batch Loss: 0.07568997144699097\n",
      "Epoch 3962, Loss: 0.1207792554050684, Final Batch Loss: 0.017911473289132118\n",
      "Epoch 3963, Loss: 0.04044651333242655, Final Batch Loss: 0.0259845070540905\n",
      "Epoch 3964, Loss: 0.03104213159531355, Final Batch Loss: 0.012573261745274067\n",
      "Epoch 3965, Loss: 0.023913544602692127, Final Batch Loss: 0.01277479063719511\n",
      "Epoch 3966, Loss: 0.06480892933905125, Final Batch Loss: 0.03381923586130142\n",
      "Epoch 3967, Loss: 0.06230047158896923, Final Batch Loss: 0.04103432968258858\n",
      "Epoch 3968, Loss: 0.1359063945710659, Final Batch Loss: 0.10559780150651932\n",
      "Epoch 3969, Loss: 0.046725520864129066, Final Batch Loss: 0.01157720573246479\n",
      "Epoch 3970, Loss: 0.09263461083173752, Final Batch Loss: 0.01565702259540558\n",
      "Epoch 3971, Loss: 0.04685712791979313, Final Batch Loss: 0.020457619801163673\n",
      "Epoch 3972, Loss: 0.03105489257723093, Final Batch Loss: 0.010898801498115063\n",
      "Epoch 3973, Loss: 0.06770581007003784, Final Batch Loss: 0.02870342507958412\n",
      "Epoch 3974, Loss: 0.06616021879017353, Final Batch Loss: 0.023424552753567696\n",
      "Epoch 3975, Loss: 0.1698657739907503, Final Batch Loss: 0.14400827884674072\n",
      "Epoch 3976, Loss: 0.040477276779711246, Final Batch Loss: 0.010946975089609623\n",
      "Epoch 3977, Loss: 0.06842870078980923, Final Batch Loss: 0.016110768541693687\n",
      "Epoch 3978, Loss: 0.035677975974977016, Final Batch Loss: 0.021231750026345253\n",
      "Epoch 3979, Loss: 0.04485431686043739, Final Batch Loss: 0.02513968199491501\n",
      "Epoch 3980, Loss: 0.055988335981965065, Final Batch Loss: 0.035436250269412994\n",
      "Epoch 3981, Loss: 0.02475075051188469, Final Batch Loss: 0.009002994745969772\n",
      "Epoch 3982, Loss: 0.061807507649064064, Final Batch Loss: 0.04591776058077812\n",
      "Epoch 3983, Loss: 0.06471503339707851, Final Batch Loss: 0.020899763330817223\n",
      "Epoch 3984, Loss: 0.06056269444525242, Final Batch Loss: 0.02552712894976139\n",
      "Epoch 3985, Loss: 0.09367652982473373, Final Batch Loss: 0.059037547558546066\n",
      "Epoch 3986, Loss: 0.03323069028556347, Final Batch Loss: 0.008907770738005638\n",
      "Epoch 3987, Loss: 0.08787480369210243, Final Batch Loss: 0.035957783460617065\n",
      "Epoch 3988, Loss: 0.12263784557580948, Final Batch Loss: 0.06302012503147125\n",
      "Epoch 3989, Loss: 0.07022683881223202, Final Batch Loss: 0.005505232140421867\n",
      "Epoch 3990, Loss: 0.037281738594174385, Final Batch Loss: 0.02261546067893505\n",
      "Epoch 3991, Loss: 0.04187657870352268, Final Batch Loss: 0.019557302817702293\n",
      "Epoch 3992, Loss: 0.037899771705269814, Final Batch Loss: 0.015712058171629906\n",
      "Epoch 3993, Loss: 0.0557320024818182, Final Batch Loss: 0.03856801986694336\n",
      "Epoch 3994, Loss: 0.05960197560489178, Final Batch Loss: 0.019171370193362236\n",
      "Epoch 3995, Loss: 0.037425738759338856, Final Batch Loss: 0.00749503169208765\n",
      "Epoch 3996, Loss: 0.06696774810552597, Final Batch Loss: 0.014893949031829834\n",
      "Epoch 3997, Loss: 0.0958573017269373, Final Batch Loss: 0.07432157546281815\n",
      "Epoch 3998, Loss: 0.046489980071783066, Final Batch Loss: 0.019469143822789192\n",
      "Epoch 3999, Loss: 0.047387282364070415, Final Batch Loss: 0.015265814028680325\n",
      "Epoch 4000, Loss: 0.031845245976001024, Final Batch Loss: 0.004417507443577051\n",
      "Epoch 4001, Loss: 0.08109202235937119, Final Batch Loss: 0.030770115554332733\n",
      "Epoch 4002, Loss: 0.042507841251790524, Final Batch Loss: 0.008282347582280636\n",
      "Epoch 4003, Loss: 0.049640143290162086, Final Batch Loss: 0.018535995855927467\n",
      "Epoch 4004, Loss: 0.04185549821704626, Final Batch Loss: 0.0028395717963576317\n",
      "Epoch 4005, Loss: 0.07434634305536747, Final Batch Loss: 0.05765559896826744\n",
      "Epoch 4006, Loss: 0.06387723796069622, Final Batch Loss: 0.04138723015785217\n",
      "Epoch 4007, Loss: 0.0458423737436533, Final Batch Loss: 0.025427762418985367\n",
      "Epoch 4008, Loss: 0.043668447993695736, Final Batch Loss: 0.015618140809237957\n",
      "Epoch 4009, Loss: 0.046743517741560936, Final Batch Loss: 0.022881492972373962\n",
      "Epoch 4010, Loss: 0.05800678953528404, Final Batch Loss: 0.0370887815952301\n",
      "Epoch 4011, Loss: 0.02414068253710866, Final Batch Loss: 0.003960984293371439\n",
      "Epoch 4012, Loss: 0.03672078484669328, Final Batch Loss: 0.007320641074329615\n",
      "Epoch 4013, Loss: 0.08345384895801544, Final Batch Loss: 0.03949242830276489\n",
      "Epoch 4014, Loss: 0.03179154358804226, Final Batch Loss: 0.012609301134943962\n",
      "Epoch 4015, Loss: 0.05442971922457218, Final Batch Loss: 0.015654927119612694\n",
      "Epoch 4016, Loss: 0.0600705835968256, Final Batch Loss: 0.03912739455699921\n",
      "Epoch 4017, Loss: 0.055505501106381416, Final Batch Loss: 0.0312940776348114\n",
      "Epoch 4018, Loss: 0.05847921967506409, Final Batch Loss: 0.023757610470056534\n",
      "Epoch 4019, Loss: 0.06808304600417614, Final Batch Loss: 0.04374406486749649\n",
      "Epoch 4020, Loss: 0.10665329359471798, Final Batch Loss: 0.020400306209921837\n",
      "Epoch 4021, Loss: 0.10287205874919891, Final Batch Loss: 0.0630953311920166\n",
      "Epoch 4022, Loss: 0.05964575707912445, Final Batch Loss: 0.03420449048280716\n",
      "Epoch 4023, Loss: 0.04973335191607475, Final Batch Loss: 0.026860898360610008\n",
      "Epoch 4024, Loss: 0.050557222217321396, Final Batch Loss: 0.03202679753303528\n",
      "Epoch 4025, Loss: 0.06616997625678778, Final Batch Loss: 0.006948213092982769\n",
      "Epoch 4026, Loss: 0.05854428932070732, Final Batch Loss: 0.017671164125204086\n",
      "Epoch 4027, Loss: 0.03338598506525159, Final Batch Loss: 0.007061439100652933\n",
      "Epoch 4028, Loss: 0.044551180209964514, Final Batch Loss: 0.006812246050685644\n",
      "Epoch 4029, Loss: 0.06006734445691109, Final Batch Loss: 0.03529632091522217\n",
      "Epoch 4030, Loss: 0.027698892634361982, Final Batch Loss: 0.00758917024359107\n",
      "Epoch 4031, Loss: 0.05296660587191582, Final Batch Loss: 0.030041497200727463\n",
      "Epoch 4032, Loss: 0.05054297111928463, Final Batch Loss: 0.03510193154215813\n",
      "Epoch 4033, Loss: 0.04609847441315651, Final Batch Loss: 0.025616969913244247\n",
      "Epoch 4034, Loss: 0.05667612515389919, Final Batch Loss: 0.0277879498898983\n",
      "Epoch 4035, Loss: 0.0651185791939497, Final Batch Loss: 0.016111446544528008\n",
      "Epoch 4036, Loss: 0.11536750942468643, Final Batch Loss: 0.01459452509880066\n",
      "Epoch 4037, Loss: 0.07860917504876852, Final Batch Loss: 0.00820713397115469\n",
      "Epoch 4038, Loss: 0.07543487474322319, Final Batch Loss: 0.03222796693444252\n",
      "Epoch 4039, Loss: 0.08819848671555519, Final Batch Loss: 0.04653976112604141\n",
      "Epoch 4040, Loss: 0.02517047617584467, Final Batch Loss: 0.015444605611264706\n",
      "Epoch 4041, Loss: 0.05167762190103531, Final Batch Loss: 0.020767537876963615\n",
      "Epoch 4042, Loss: 0.07899647206068039, Final Batch Loss: 0.05054113641381264\n",
      "Epoch 4043, Loss: 0.15993034094572067, Final Batch Loss: 0.074153833091259\n",
      "Epoch 4044, Loss: 0.07017284817993641, Final Batch Loss: 0.03963203355669975\n",
      "Epoch 4045, Loss: 0.036476495675742626, Final Batch Loss: 0.010378959588706493\n",
      "Epoch 4046, Loss: 0.05362824164330959, Final Batch Loss: 0.015025535598397255\n",
      "Epoch 4047, Loss: 0.17318361066281796, Final Batch Loss: 0.012585392221808434\n",
      "Epoch 4048, Loss: 0.07901781797409058, Final Batch Loss: 0.0088224858045578\n",
      "Epoch 4049, Loss: 0.05486411787569523, Final Batch Loss: 0.026905937120318413\n",
      "Epoch 4050, Loss: 0.06053253263235092, Final Batch Loss: 0.03148927912116051\n",
      "Epoch 4051, Loss: 0.09132634103298187, Final Batch Loss: 0.04857475683093071\n",
      "Epoch 4052, Loss: 0.0701348353177309, Final Batch Loss: 0.02050500549376011\n",
      "Epoch 4053, Loss: 0.05445295013487339, Final Batch Loss: 0.02482226863503456\n",
      "Epoch 4054, Loss: 0.03401993261650205, Final Batch Loss: 0.0049508907832205296\n",
      "Epoch 4055, Loss: 0.053599839098751545, Final Batch Loss: 0.009419982321560383\n",
      "Epoch 4056, Loss: 0.0328469118103385, Final Batch Loss: 0.013192812912166119\n",
      "Epoch 4057, Loss: 0.0737372636795044, Final Batch Loss: 0.031719207763671875\n",
      "Epoch 4058, Loss: 0.044212086126208305, Final Batch Loss: 0.023330841213464737\n",
      "Epoch 4059, Loss: 0.0790672916918993, Final Batch Loss: 0.02388215996325016\n",
      "Epoch 4060, Loss: 0.06835243664681911, Final Batch Loss: 0.01665487326681614\n",
      "Epoch 4061, Loss: 0.06989619228988886, Final Batch Loss: 0.010369406081736088\n",
      "Epoch 4062, Loss: 0.03292247746139765, Final Batch Loss: 0.021232130005955696\n",
      "Epoch 4063, Loss: 0.16142180562019348, Final Batch Loss: 0.12645436823368073\n",
      "Epoch 4064, Loss: 0.08462336659431458, Final Batch Loss: 0.05121993273496628\n",
      "Epoch 4065, Loss: 0.10656250454485416, Final Batch Loss: 0.08977627754211426\n",
      "Epoch 4066, Loss: 0.0842946209013462, Final Batch Loss: 0.059024881571531296\n",
      "Epoch 4067, Loss: 0.0993464682251215, Final Batch Loss: 0.0681818351149559\n",
      "Epoch 4068, Loss: 0.05358199216425419, Final Batch Loss: 0.020391402766108513\n",
      "Epoch 4069, Loss: 0.03987526707351208, Final Batch Loss: 0.01342972181737423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4070, Loss: 0.05935695953667164, Final Batch Loss: 0.027080709114670753\n",
      "Epoch 4071, Loss: 0.06418643891811371, Final Batch Loss: 0.023775000125169754\n",
      "Epoch 4072, Loss: 0.0686834305524826, Final Batch Loss: 0.010756902396678925\n",
      "Epoch 4073, Loss: 0.055617835372686386, Final Batch Loss: 0.023431621491909027\n",
      "Epoch 4074, Loss: 0.032724492251873016, Final Batch Loss: 0.014013459905982018\n",
      "Epoch 4075, Loss: 0.03861612640321255, Final Batch Loss: 0.017857750877738\n",
      "Epoch 4076, Loss: 0.016112164594233036, Final Batch Loss: 0.006650615483522415\n",
      "Epoch 4077, Loss: 0.07384051382541656, Final Batch Loss: 0.01662459596991539\n",
      "Epoch 4078, Loss: 0.06567146815359592, Final Batch Loss: 0.029597604647278786\n",
      "Epoch 4079, Loss: 0.033388218842446804, Final Batch Loss: 0.008329481817781925\n",
      "Epoch 4080, Loss: 0.10990654863417149, Final Batch Loss: 0.08423282951116562\n",
      "Epoch 4081, Loss: 0.0327290752902627, Final Batch Loss: 0.008046331815421581\n",
      "Epoch 4082, Loss: 0.06599857099354267, Final Batch Loss: 0.05129842832684517\n",
      "Epoch 4083, Loss: 0.08612025156617165, Final Batch Loss: 0.05799679458141327\n",
      "Epoch 4084, Loss: 0.01930832676589489, Final Batch Loss: 0.005004221573472023\n",
      "Epoch 4085, Loss: 0.042625537142157555, Final Batch Loss: 0.024636313319206238\n",
      "Epoch 4086, Loss: 0.10813095979392529, Final Batch Loss: 0.08510573208332062\n",
      "Epoch 4087, Loss: 0.024407777935266495, Final Batch Loss: 0.010731508955359459\n",
      "Epoch 4088, Loss: 0.054980800952762365, Final Batch Loss: 0.007776775863021612\n",
      "Epoch 4089, Loss: 0.03220804501324892, Final Batch Loss: 0.01196241844445467\n",
      "Epoch 4090, Loss: 0.05151659436523914, Final Batch Loss: 0.027330944314599037\n",
      "Epoch 4091, Loss: 0.03214045614004135, Final Batch Loss: 0.013470474630594254\n",
      "Epoch 4092, Loss: 0.015558883547782898, Final Batch Loss: 0.007986214943230152\n",
      "Epoch 4093, Loss: 0.12616092339158058, Final Batch Loss: 0.07854035496711731\n",
      "Epoch 4094, Loss: 0.05636834353208542, Final Batch Loss: 0.01859322190284729\n",
      "Epoch 4095, Loss: 0.026049505919218063, Final Batch Loss: 0.011412272229790688\n",
      "Epoch 4096, Loss: 0.06596335209906101, Final Batch Loss: 0.027559718117117882\n",
      "Epoch 4097, Loss: 0.08515392290428281, Final Batch Loss: 0.0066406805999577045\n",
      "Epoch 4098, Loss: 0.06719004735350609, Final Batch Loss: 0.05136909708380699\n",
      "Epoch 4099, Loss: 0.07010235451161861, Final Batch Loss: 0.014759385958313942\n",
      "Epoch 4100, Loss: 0.11612957902252674, Final Batch Loss: 0.10407721251249313\n",
      "Epoch 4101, Loss: 0.053743583615869284, Final Batch Loss: 0.005600848700851202\n",
      "Epoch 4102, Loss: 0.11141389235854149, Final Batch Loss: 0.09771698713302612\n",
      "Epoch 4103, Loss: 0.0570005364716053, Final Batch Loss: 0.01626993715763092\n",
      "Epoch 4104, Loss: 0.05305940750986338, Final Batch Loss: 0.0110843600705266\n",
      "Epoch 4105, Loss: 0.042517904192209244, Final Batch Loss: 0.013816500082612038\n",
      "Epoch 4106, Loss: 0.039459310472011566, Final Batch Loss: 0.016772110015153885\n",
      "Epoch 4107, Loss: 0.06546078249812126, Final Batch Loss: 0.03955591470003128\n",
      "Epoch 4108, Loss: 0.15960508957505226, Final Batch Loss: 0.12373174726963043\n",
      "Epoch 4109, Loss: 0.028547896072268486, Final Batch Loss: 0.008104156702756882\n",
      "Epoch 4110, Loss: 0.0750569049268961, Final Batch Loss: 0.05103873834013939\n",
      "Epoch 4111, Loss: 0.05529050901532173, Final Batch Loss: 0.02912917546927929\n",
      "Epoch 4112, Loss: 0.036584263667464256, Final Batch Loss: 0.011227836832404137\n",
      "Epoch 4113, Loss: 0.022898505441844463, Final Batch Loss: 0.010239205323159695\n",
      "Epoch 4114, Loss: 0.032157580368220806, Final Batch Loss: 0.010900632478296757\n",
      "Epoch 4115, Loss: 0.053357476368546486, Final Batch Loss: 0.017629267647862434\n",
      "Epoch 4116, Loss: 0.03434797190129757, Final Batch Loss: 0.01792708784341812\n",
      "Epoch 4117, Loss: 0.02924563642591238, Final Batch Loss: 0.011177550069987774\n",
      "Epoch 4118, Loss: 0.07328896224498749, Final Batch Loss: 0.035649679601192474\n",
      "Epoch 4119, Loss: 0.029561215080320835, Final Batch Loss: 0.005916311405599117\n",
      "Epoch 4120, Loss: 0.026018290780484676, Final Batch Loss: 0.0082657216116786\n",
      "Epoch 4121, Loss: 0.05018246825784445, Final Batch Loss: 0.008237975649535656\n",
      "Epoch 4122, Loss: 0.14927706122398376, Final Batch Loss: 0.12969620525836945\n",
      "Epoch 4123, Loss: 0.029439513571560383, Final Batch Loss: 0.013937258161604404\n",
      "Epoch 4124, Loss: 0.013489635894075036, Final Batch Loss: 0.0015186138916760683\n",
      "Epoch 4125, Loss: 0.024794279597699642, Final Batch Loss: 0.008246292360126972\n",
      "Epoch 4126, Loss: 0.03335398994386196, Final Batch Loss: 0.02266114577651024\n",
      "Epoch 4127, Loss: 0.05631000502035022, Final Batch Loss: 0.006750951986759901\n",
      "Epoch 4128, Loss: 0.03690437553450465, Final Batch Loss: 0.005599550437182188\n",
      "Epoch 4129, Loss: 0.09828219003975391, Final Batch Loss: 0.015640640631318092\n",
      "Epoch 4130, Loss: 0.046799853444099426, Final Batch Loss: 0.01648050732910633\n",
      "Epoch 4131, Loss: 0.039550186367705464, Final Batch Loss: 0.0036599349696189165\n",
      "Epoch 4132, Loss: 0.02194633986800909, Final Batch Loss: 0.007170385681092739\n",
      "Epoch 4133, Loss: 0.02570991450920701, Final Batch Loss: 0.004940316546708345\n",
      "Epoch 4134, Loss: 0.04470119904726744, Final Batch Loss: 0.012182473205029964\n",
      "Epoch 4135, Loss: 0.04057535529136658, Final Batch Loss: 0.015049604699015617\n",
      "Epoch 4136, Loss: 0.024710541125386953, Final Batch Loss: 0.00403517996892333\n",
      "Epoch 4137, Loss: 0.030257324688136578, Final Batch Loss: 0.013537929393351078\n",
      "Epoch 4138, Loss: 0.04672003164887428, Final Batch Loss: 0.011588145047426224\n",
      "Epoch 4139, Loss: 0.08311416953802109, Final Batch Loss: 0.0643659383058548\n",
      "Epoch 4140, Loss: 0.03868758585304022, Final Batch Loss: 0.014027499593794346\n",
      "Epoch 4141, Loss: 0.026280345395207405, Final Batch Loss: 0.012969075702130795\n",
      "Epoch 4142, Loss: 0.054408058524131775, Final Batch Loss: 0.038798216730356216\n",
      "Epoch 4143, Loss: 0.036047397181391716, Final Batch Loss: 0.02415498159825802\n",
      "Epoch 4144, Loss: 0.04153172019869089, Final Batch Loss: 0.013126255013048649\n",
      "Epoch 4145, Loss: 0.02073397347703576, Final Batch Loss: 0.0061752828769385815\n",
      "Epoch 4146, Loss: 0.040112975519150496, Final Batch Loss: 0.006657994817942381\n",
      "Epoch 4147, Loss: 0.030247732065618038, Final Batch Loss: 0.014089136384427547\n",
      "Epoch 4148, Loss: 0.02699861628934741, Final Batch Loss: 0.0062674409709870815\n",
      "Epoch 4149, Loss: 0.06431703548878431, Final Batch Loss: 0.01479462068527937\n",
      "Epoch 4150, Loss: 0.01676657935604453, Final Batch Loss: 0.004423593636602163\n",
      "Epoch 4151, Loss: 0.0438825897872448, Final Batch Loss: 0.023322390392422676\n",
      "Epoch 4152, Loss: 0.0508955642580986, Final Batch Loss: 0.019859792664647102\n",
      "Epoch 4153, Loss: 0.03996981866657734, Final Batch Loss: 0.022404469549655914\n",
      "Epoch 4154, Loss: 0.05952296871691942, Final Batch Loss: 0.0043161483481526375\n",
      "Epoch 4155, Loss: 0.10858556441962719, Final Batch Loss: 0.027515841647982597\n",
      "Epoch 4156, Loss: 0.05074205156415701, Final Batch Loss: 0.008113465271890163\n",
      "Epoch 4157, Loss: 0.24336074478924274, Final Batch Loss: 0.22006478905677795\n",
      "Epoch 4158, Loss: 0.04555502254515886, Final Batch Loss: 0.01221445295959711\n",
      "Epoch 4159, Loss: 0.05623530223965645, Final Batch Loss: 0.010461527854204178\n",
      "Epoch 4160, Loss: 0.04049747623503208, Final Batch Loss: 0.024386001750826836\n",
      "Epoch 4161, Loss: 0.04872910864651203, Final Batch Loss: 0.029052676633000374\n",
      "Epoch 4162, Loss: 0.015907592605799437, Final Batch Loss: 0.00383934797719121\n",
      "Epoch 4163, Loss: 0.030225728638470173, Final Batch Loss: 0.02095768041908741\n",
      "Epoch 4164, Loss: 0.02849212707951665, Final Batch Loss: 0.006135410163551569\n",
      "Epoch 4165, Loss: 0.041605609469115734, Final Batch Loss: 0.01562467496842146\n",
      "Epoch 4166, Loss: 0.051960138604044914, Final Batch Loss: 0.04099484533071518\n",
      "Epoch 4167, Loss: 0.08877120446413755, Final Batch Loss: 0.004428493790328503\n",
      "Epoch 4168, Loss: 0.04383251257240772, Final Batch Loss: 0.019127117469906807\n",
      "Epoch 4169, Loss: 0.08878859598189592, Final Batch Loss: 0.015163791365921497\n",
      "Epoch 4170, Loss: 0.03329913690686226, Final Batch Loss: 0.01671997457742691\n",
      "Epoch 4171, Loss: 0.07191878743469715, Final Batch Loss: 0.019758550450205803\n",
      "Epoch 4172, Loss: 0.1439172886312008, Final Batch Loss: 0.11416393518447876\n",
      "Epoch 4173, Loss: 0.03931390028446913, Final Batch Loss: 0.011631566099822521\n",
      "Epoch 4174, Loss: 0.052392635494470596, Final Batch Loss: 0.039038997143507004\n",
      "Epoch 4175, Loss: 0.0480552613735199, Final Batch Loss: 0.016508664935827255\n",
      "Epoch 4176, Loss: 0.018296246882528067, Final Batch Loss: 0.0024185138754546642\n",
      "Epoch 4177, Loss: 0.044351741671562195, Final Batch Loss: 0.015981372445821762\n",
      "Epoch 4178, Loss: 0.06349643506109715, Final Batch Loss: 0.02644939534366131\n",
      "Epoch 4179, Loss: 0.031922608613967896, Final Batch Loss: 0.012202853336930275\n",
      "Epoch 4180, Loss: 0.034695565700531006, Final Batch Loss: 0.021046141162514687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4181, Loss: 0.07365554571151733, Final Batch Loss: 0.03474411368370056\n",
      "Epoch 4182, Loss: 0.01721620326861739, Final Batch Loss: 0.0044265310280025005\n",
      "Epoch 4183, Loss: 0.09179825335741043, Final Batch Loss: 0.06808169931173325\n",
      "Epoch 4184, Loss: 0.057044324465096, Final Batch Loss: 0.014899124391376972\n",
      "Epoch 4185, Loss: 0.050357178784906864, Final Batch Loss: 0.034981995820999146\n",
      "Epoch 4186, Loss: 0.038171613588929176, Final Batch Loss: 0.021819522604346275\n",
      "Epoch 4187, Loss: 0.08368126954883337, Final Batch Loss: 0.00680458452552557\n",
      "Epoch 4188, Loss: 0.045953660272061825, Final Batch Loss: 0.010065595619380474\n",
      "Epoch 4189, Loss: 0.03515700623393059, Final Batch Loss: 0.013017572462558746\n",
      "Epoch 4190, Loss: 0.06887716427445412, Final Batch Loss: 0.031991709023714066\n",
      "Epoch 4191, Loss: 0.04724981356412172, Final Batch Loss: 0.007750813849270344\n",
      "Epoch 4192, Loss: 0.1668387297540903, Final Batch Loss: 0.14766085147857666\n",
      "Epoch 4193, Loss: 0.020990341436117887, Final Batch Loss: 0.005980068352073431\n",
      "Epoch 4194, Loss: 0.06082449480891228, Final Batch Loss: 0.006055969744920731\n",
      "Epoch 4195, Loss: 0.03360912203788757, Final Batch Loss: 0.014396635815501213\n",
      "Epoch 4196, Loss: 0.15923628956079483, Final Batch Loss: 0.126527339220047\n",
      "Epoch 4197, Loss: 0.05847033206373453, Final Batch Loss: 0.014779255725443363\n",
      "Epoch 4198, Loss: 0.08413050323724747, Final Batch Loss: 0.013854876160621643\n",
      "Epoch 4199, Loss: 0.06729319179430604, Final Batch Loss: 0.006286280695348978\n",
      "Epoch 4200, Loss: 0.05588941462337971, Final Batch Loss: 0.04227561503648758\n",
      "Epoch 4201, Loss: 0.16975803673267365, Final Batch Loss: 0.10531876981258392\n",
      "Epoch 4202, Loss: 0.05313189700245857, Final Batch Loss: 0.04152445122599602\n",
      "Epoch 4203, Loss: 0.03148020803928375, Final Batch Loss: 0.009735075756907463\n",
      "Epoch 4204, Loss: 0.06991436053067446, Final Batch Loss: 0.011969530023634434\n",
      "Epoch 4205, Loss: 0.035718441009521484, Final Batch Loss: 0.017022034153342247\n",
      "Epoch 4206, Loss: 0.049737260676920414, Final Batch Loss: 0.013990269042551517\n",
      "Epoch 4207, Loss: 0.027232050895690918, Final Batch Loss: 0.010153153911232948\n",
      "Epoch 4208, Loss: 0.03537194547243416, Final Batch Loss: 0.002948460401967168\n",
      "Epoch 4209, Loss: 0.03836171608418226, Final Batch Loss: 0.006777058355510235\n",
      "Epoch 4210, Loss: 0.04665939649567008, Final Batch Loss: 0.039225492626428604\n",
      "Epoch 4211, Loss: 0.03333916421979666, Final Batch Loss: 0.014158389531075954\n",
      "Epoch 4212, Loss: 0.0647378396242857, Final Batch Loss: 0.03488648682832718\n",
      "Epoch 4213, Loss: 0.03269027918577194, Final Batch Loss: 0.013030683621764183\n",
      "Epoch 4214, Loss: 0.06436606869101524, Final Batch Loss: 0.03578425943851471\n",
      "Epoch 4215, Loss: 0.024227282963693142, Final Batch Loss: 0.004000457935035229\n",
      "Epoch 4216, Loss: 0.05344127677381039, Final Batch Loss: 0.0267275832593441\n",
      "Epoch 4217, Loss: 0.0841072853654623, Final Batch Loss: 0.008186763152480125\n",
      "Epoch 4218, Loss: 0.062019214034080505, Final Batch Loss: 0.031966015696525574\n",
      "Epoch 4219, Loss: 0.03646491002291441, Final Batch Loss: 0.020987173542380333\n",
      "Epoch 4220, Loss: 0.11317892745137215, Final Batch Loss: 0.021791856735944748\n",
      "Epoch 4221, Loss: 0.07255963236093521, Final Batch Loss: 0.03594498708844185\n",
      "Epoch 4222, Loss: 0.04153737425804138, Final Batch Loss: 0.022693242877721786\n",
      "Epoch 4223, Loss: 0.017757334746420383, Final Batch Loss: 0.005466105416417122\n",
      "Epoch 4224, Loss: 0.1030271491035819, Final Batch Loss: 0.09391330182552338\n",
      "Epoch 4225, Loss: 0.13627688959240913, Final Batch Loss: 0.08630599081516266\n",
      "Epoch 4226, Loss: 0.07051271572709084, Final Batch Loss: 0.032372262328863144\n",
      "Epoch 4227, Loss: 0.15588154271245003, Final Batch Loss: 0.12670844793319702\n",
      "Epoch 4228, Loss: 0.04322493262588978, Final Batch Loss: 0.012088991701602936\n",
      "Epoch 4229, Loss: 0.07391291856765747, Final Batch Loss: 0.02968798577785492\n",
      "Epoch 4230, Loss: 0.03889991715550423, Final Batch Loss: 0.014511534944176674\n",
      "Epoch 4231, Loss: 0.12269346043467522, Final Batch Loss: 0.08603645116090775\n",
      "Epoch 4232, Loss: 0.2881900053471327, Final Batch Loss: 0.26827743649482727\n",
      "Epoch 4233, Loss: 0.03374985698610544, Final Batch Loss: 0.015375240705907345\n",
      "Epoch 4234, Loss: 0.06947110593318939, Final Batch Loss: 0.026437684893608093\n",
      "Epoch 4235, Loss: 0.10505750775337219, Final Batch Loss: 0.06473656743764877\n",
      "Epoch 4236, Loss: 0.052924782037734985, Final Batch Loss: 0.022447815164923668\n",
      "Epoch 4237, Loss: 0.04020546842366457, Final Batch Loss: 0.028471585363149643\n",
      "Epoch 4238, Loss: 0.06184012070298195, Final Batch Loss: 0.04383055493235588\n",
      "Epoch 4239, Loss: 0.042463069781661034, Final Batch Loss: 0.018486082553863525\n",
      "Epoch 4240, Loss: 0.055731674656271935, Final Batch Loss: 0.028779981657862663\n",
      "Epoch 4241, Loss: 0.03376940917223692, Final Batch Loss: 0.01445287000387907\n",
      "Epoch 4242, Loss: 0.020654750987887383, Final Batch Loss: 0.004038449376821518\n",
      "Epoch 4243, Loss: 0.025337778963148594, Final Batch Loss: 0.00810674112290144\n",
      "Epoch 4244, Loss: 0.03602781146764755, Final Batch Loss: 0.017170900478959084\n",
      "Epoch 4245, Loss: 0.030832700431346893, Final Batch Loss: 0.014436071738600731\n",
      "Epoch 4246, Loss: 0.05210361070930958, Final Batch Loss: 0.03181449696421623\n",
      "Epoch 4247, Loss: 0.05907217785716057, Final Batch Loss: 0.029909633100032806\n",
      "Epoch 4248, Loss: 0.04268472082912922, Final Batch Loss: 0.013523252680897713\n",
      "Epoch 4249, Loss: 0.02349364012479782, Final Batch Loss: 0.014699152670800686\n",
      "Epoch 4250, Loss: 0.037089366000145674, Final Batch Loss: 0.005844346713274717\n",
      "Epoch 4251, Loss: 0.04706981033086777, Final Batch Loss: 0.017949966713786125\n",
      "Epoch 4252, Loss: 0.09402001090347767, Final Batch Loss: 0.077670618891716\n",
      "Epoch 4253, Loss: 0.06783424690365791, Final Batch Loss: 0.03721797466278076\n",
      "Epoch 4254, Loss: 0.10937821120023727, Final Batch Loss: 0.06290413439273834\n",
      "Epoch 4255, Loss: 0.06853121519088745, Final Batch Loss: 0.046135228127241135\n",
      "Epoch 4256, Loss: 0.06272342801094055, Final Batch Loss: 0.03916740044951439\n",
      "Epoch 4257, Loss: 0.029157405719161034, Final Batch Loss: 0.016217002645134926\n",
      "Epoch 4258, Loss: 0.05977727100253105, Final Batch Loss: 0.045575130730867386\n",
      "Epoch 4259, Loss: 0.042484049685299397, Final Batch Loss: 0.026971591636538506\n",
      "Epoch 4260, Loss: 0.0886343214660883, Final Batch Loss: 0.02780310995876789\n",
      "Epoch 4261, Loss: 0.09669291600584984, Final Batch Loss: 0.047326184809207916\n",
      "Epoch 4262, Loss: 0.04760143533349037, Final Batch Loss: 0.025308076292276382\n",
      "Epoch 4263, Loss: 0.037743596360087395, Final Batch Loss: 0.018772628158330917\n",
      "Epoch 4264, Loss: 0.03152447193861008, Final Batch Loss: 0.016247015446424484\n",
      "Epoch 4265, Loss: 0.06619115173816681, Final Batch Loss: 0.013603109866380692\n",
      "Epoch 4266, Loss: 0.023769935593008995, Final Batch Loss: 0.00469251349568367\n",
      "Epoch 4267, Loss: 0.06554394215345383, Final Batch Loss: 0.018225762993097305\n",
      "Epoch 4268, Loss: 0.017565542366355658, Final Batch Loss: 0.005696294363588095\n",
      "Epoch 4269, Loss: 0.09509268589317799, Final Batch Loss: 0.07917619496583939\n",
      "Epoch 4270, Loss: 0.06029939092695713, Final Batch Loss: 0.01874651201069355\n",
      "Epoch 4271, Loss: 0.03903004061430693, Final Batch Loss: 0.014583338983356953\n",
      "Epoch 4272, Loss: 0.03690030239522457, Final Batch Loss: 0.009298518300056458\n",
      "Epoch 4273, Loss: 0.0341102434322238, Final Batch Loss: 0.02459046244621277\n",
      "Epoch 4274, Loss: 0.025031391065567732, Final Batch Loss: 0.006825522985309362\n",
      "Epoch 4275, Loss: 0.039342404808849096, Final Batch Loss: 0.00474267965182662\n",
      "Epoch 4276, Loss: 0.1068681851029396, Final Batch Loss: 0.02054879069328308\n",
      "Epoch 4277, Loss: 0.0600560549646616, Final Batch Loss: 0.03119884617626667\n",
      "Epoch 4278, Loss: 0.04599837586283684, Final Batch Loss: 0.004120394587516785\n",
      "Epoch 4279, Loss: 0.028660384006798267, Final Batch Loss: 0.008167463354766369\n",
      "Epoch 4280, Loss: 0.11270854994654655, Final Batch Loss: 0.09523902088403702\n",
      "Epoch 4281, Loss: 0.07146907225251198, Final Batch Loss: 0.02561105415225029\n",
      "Epoch 4282, Loss: 0.02897013071924448, Final Batch Loss: 0.00981407891958952\n",
      "Epoch 4283, Loss: 0.042867518961429596, Final Batch Loss: 0.013975311070680618\n",
      "Epoch 4284, Loss: 0.07428807206451893, Final Batch Loss: 0.05897355452179909\n",
      "Epoch 4285, Loss: 0.1294163055717945, Final Batch Loss: 0.06021907553076744\n",
      "Epoch 4286, Loss: 0.044244375079870224, Final Batch Loss: 0.010913662612438202\n",
      "Epoch 4287, Loss: 0.01911546429619193, Final Batch Loss: 0.004314878489822149\n",
      "Epoch 4288, Loss: 0.04115656577050686, Final Batch Loss: 0.0219303909689188\n",
      "Epoch 4289, Loss: 0.08161531947553158, Final Batch Loss: 0.02155294083058834\n",
      "Epoch 4290, Loss: 0.04894337244331837, Final Batch Loss: 0.03259516507387161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4291, Loss: 0.06711207143962383, Final Batch Loss: 0.021610425785183907\n",
      "Epoch 4292, Loss: 0.05130522511899471, Final Batch Loss: 0.030603790655732155\n",
      "Epoch 4293, Loss: 0.034315768629312515, Final Batch Loss: 0.020363355055451393\n",
      "Epoch 4294, Loss: 0.0524727925658226, Final Batch Loss: 0.028536776080727577\n",
      "Epoch 4295, Loss: 0.06745949573814869, Final Batch Loss: 0.05293885990977287\n",
      "Epoch 4296, Loss: 0.06441669352352619, Final Batch Loss: 0.04844991862773895\n",
      "Epoch 4297, Loss: 0.031018109060823917, Final Batch Loss: 0.010323229245841503\n",
      "Epoch 4298, Loss: 0.04060851037502289, Final Batch Loss: 0.014770964160561562\n",
      "Epoch 4299, Loss: 0.031193207949399948, Final Batch Loss: 0.004999252036213875\n",
      "Epoch 4300, Loss: 0.03592381998896599, Final Batch Loss: 0.02606377936899662\n",
      "Epoch 4301, Loss: 0.03609758801758289, Final Batch Loss: 0.015131453052163124\n",
      "Epoch 4302, Loss: 0.02805157331749797, Final Batch Loss: 0.005736998748034239\n",
      "Epoch 4303, Loss: 0.06684012338519096, Final Batch Loss: 0.03133339434862137\n",
      "Epoch 4304, Loss: 0.03805163037031889, Final Batch Loss: 0.023519692942500114\n",
      "Epoch 4305, Loss: 0.027875022031366825, Final Batch Loss: 0.00957734789699316\n",
      "Epoch 4306, Loss: 0.04953496530652046, Final Batch Loss: 0.03580935671925545\n",
      "Epoch 4307, Loss: 0.12205819971859455, Final Batch Loss: 0.10957541316747665\n",
      "Epoch 4308, Loss: 0.040116770192980766, Final Batch Loss: 0.01663876883685589\n",
      "Epoch 4309, Loss: 0.16347758285701275, Final Batch Loss: 0.14646831154823303\n",
      "Epoch 4310, Loss: 0.06410379894077778, Final Batch Loss: 0.0055323150008916855\n",
      "Epoch 4311, Loss: 0.038313526660203934, Final Batch Loss: 0.019008725881576538\n",
      "Epoch 4312, Loss: 0.06067667901515961, Final Batch Loss: 0.0168163999915123\n",
      "Epoch 4313, Loss: 0.03392430488020182, Final Batch Loss: 0.011477661319077015\n",
      "Epoch 4314, Loss: 0.06787456572055817, Final Batch Loss: 0.01894277334213257\n",
      "Epoch 4315, Loss: 0.10337189212441444, Final Batch Loss: 0.07006353884935379\n",
      "Epoch 4316, Loss: 0.014854216948151588, Final Batch Loss: 0.004581942223012447\n",
      "Epoch 4317, Loss: 0.051496938802301884, Final Batch Loss: 0.014402450062334538\n",
      "Epoch 4318, Loss: 0.1974988803267479, Final Batch Loss: 0.14937542378902435\n",
      "Epoch 4319, Loss: 0.06284044124186039, Final Batch Loss: 0.01679752953350544\n",
      "Epoch 4320, Loss: 0.09944426082074642, Final Batch Loss: 0.003940870985388756\n",
      "Epoch 4321, Loss: 0.06168440729379654, Final Batch Loss: 0.023882180452346802\n",
      "Epoch 4322, Loss: 0.043180545791983604, Final Batch Loss: 0.018150167539715767\n",
      "Epoch 4323, Loss: 0.12560464814305305, Final Batch Loss: 0.10805162042379379\n",
      "Epoch 4324, Loss: 0.044217213056981564, Final Batch Loss: 0.014387871138751507\n",
      "Epoch 4325, Loss: 0.062206609174609184, Final Batch Loss: 0.022294046357274055\n",
      "Epoch 4326, Loss: 0.045892384834587574, Final Batch Loss: 0.012639016844332218\n",
      "Epoch 4327, Loss: 0.028095736168324947, Final Batch Loss: 0.007840604521334171\n",
      "Epoch 4328, Loss: 0.3054410517215729, Final Batch Loss: 0.22933629155158997\n",
      "Epoch 4329, Loss: 0.17509391158819199, Final Batch Loss: 0.12138298898935318\n",
      "Epoch 4330, Loss: 0.08837715163826942, Final Batch Loss: 0.014058131724596024\n",
      "Epoch 4331, Loss: 0.029123133048415184, Final Batch Loss: 0.010953983291983604\n",
      "Epoch 4332, Loss: 0.17346707731485367, Final Batch Loss: 0.03837274760007858\n",
      "Epoch 4333, Loss: 0.10571014508605003, Final Batch Loss: 0.02089008316397667\n",
      "Epoch 4334, Loss: 0.14634954929351807, Final Batch Loss: 0.04998007416725159\n",
      "Epoch 4335, Loss: 0.05376824829727411, Final Batch Loss: 0.013036278076469898\n",
      "Epoch 4336, Loss: 0.05045049823820591, Final Batch Loss: 0.03199893236160278\n",
      "Epoch 4337, Loss: 0.027890947414562106, Final Batch Loss: 0.0036831970792263746\n",
      "Epoch 4338, Loss: 0.12867634743452072, Final Batch Loss: 0.028197742998600006\n",
      "Epoch 4339, Loss: 0.11588308215141296, Final Batch Loss: 0.061574216932058334\n",
      "Epoch 4340, Loss: 0.06302642170339823, Final Batch Loss: 0.01408678013831377\n",
      "Epoch 4341, Loss: 0.042103759944438934, Final Batch Loss: 0.01991783082485199\n",
      "Epoch 4342, Loss: 0.07137924805283546, Final Batch Loss: 0.043831344693899155\n",
      "Epoch 4343, Loss: 0.04483557026833296, Final Batch Loss: 0.011472280137240887\n",
      "Epoch 4344, Loss: 0.06894813850522041, Final Batch Loss: 0.05513857305049896\n",
      "Epoch 4345, Loss: 0.07306672260165215, Final Batch Loss: 0.01905997097492218\n",
      "Epoch 4346, Loss: 0.038905419409275055, Final Batch Loss: 0.013294877484440804\n",
      "Epoch 4347, Loss: 0.03992113657295704, Final Batch Loss: 0.011489953845739365\n",
      "Epoch 4348, Loss: 0.056340971030294895, Final Batch Loss: 0.04344601929187775\n",
      "Epoch 4349, Loss: 0.14933132007718086, Final Batch Loss: 0.03684679791331291\n",
      "Epoch 4350, Loss: 0.05781945027410984, Final Batch Loss: 0.027408072724938393\n",
      "Epoch 4351, Loss: 0.08528807014226913, Final Batch Loss: 0.06607795506715775\n",
      "Epoch 4352, Loss: 0.058089274913072586, Final Batch Loss: 0.029641712084412575\n",
      "Epoch 4353, Loss: 0.04304434917867184, Final Batch Loss: 0.024957900866866112\n",
      "Epoch 4354, Loss: 0.0641612634062767, Final Batch Loss: 0.04547913372516632\n",
      "Epoch 4355, Loss: 0.07010461203753948, Final Batch Loss: 0.0179321076720953\n",
      "Epoch 4356, Loss: 0.050853488966822624, Final Batch Loss: 0.03257829695940018\n",
      "Epoch 4357, Loss: 0.03283771686255932, Final Batch Loss: 0.01657208427786827\n",
      "Epoch 4358, Loss: 0.033076328225433826, Final Batch Loss: 0.015203368850052357\n",
      "Epoch 4359, Loss: 0.06540894322097301, Final Batch Loss: 0.02259683795273304\n",
      "Epoch 4360, Loss: 0.05749835539609194, Final Batch Loss: 0.006233681924641132\n",
      "Epoch 4361, Loss: 0.05766008421778679, Final Batch Loss: 0.024983596056699753\n",
      "Epoch 4362, Loss: 0.031278365291655064, Final Batch Loss: 0.007897437550127506\n",
      "Epoch 4363, Loss: 0.09914720058441162, Final Batch Loss: 0.059880632907152176\n",
      "Epoch 4364, Loss: 0.17766062915325165, Final Batch Loss: 0.09170446544885635\n",
      "Epoch 4365, Loss: 0.07714260928332806, Final Batch Loss: 0.053965941071510315\n",
      "Epoch 4366, Loss: 0.0190682215616107, Final Batch Loss: 0.006282242015004158\n",
      "Epoch 4367, Loss: 0.06288564391434193, Final Batch Loss: 0.04421332851052284\n",
      "Epoch 4368, Loss: 0.02684813179075718, Final Batch Loss: 0.008457161486148834\n",
      "Epoch 4369, Loss: 0.055114217568188906, Final Batch Loss: 0.04816161468625069\n",
      "Epoch 4370, Loss: 0.0911959670484066, Final Batch Loss: 0.055728767067193985\n",
      "Epoch 4371, Loss: 0.043342627584934235, Final Batch Loss: 0.017941707745194435\n",
      "Epoch 4372, Loss: 0.04246840160340071, Final Batch Loss: 0.027438580989837646\n",
      "Epoch 4373, Loss: 0.029396112076938152, Final Batch Loss: 0.011033580638468266\n",
      "Epoch 4374, Loss: 0.016663034446537495, Final Batch Loss: 0.004693211987614632\n",
      "Epoch 4375, Loss: 0.03039155900478363, Final Batch Loss: 0.014456311240792274\n",
      "Epoch 4376, Loss: 0.08056841604411602, Final Batch Loss: 0.05502721294760704\n",
      "Epoch 4377, Loss: 0.09005010686814785, Final Batch Loss: 0.06425928324460983\n",
      "Epoch 4378, Loss: 0.024256001226603985, Final Batch Loss: 0.012496115639805794\n",
      "Epoch 4379, Loss: 0.06270501017570496, Final Batch Loss: 0.038969434797763824\n",
      "Epoch 4380, Loss: 0.0447453036904335, Final Batch Loss: 0.008965805172920227\n",
      "Epoch 4381, Loss: 0.036094341427087784, Final Batch Loss: 0.019677812233567238\n",
      "Epoch 4382, Loss: 0.05163258593529463, Final Batch Loss: 0.014971381984651089\n",
      "Epoch 4383, Loss: 0.035735392943024635, Final Batch Loss: 0.01714286021888256\n",
      "Epoch 4384, Loss: 0.0824047178030014, Final Batch Loss: 0.05710839852690697\n",
      "Epoch 4385, Loss: 0.03840252012014389, Final Batch Loss: 0.006935667246580124\n",
      "Epoch 4386, Loss: 0.04158899001777172, Final Batch Loss: 0.028474045917391777\n",
      "Epoch 4387, Loss: 0.030485153198242188, Final Batch Loss: 0.011714767664670944\n",
      "Epoch 4388, Loss: 0.05558846332132816, Final Batch Loss: 0.020193161442875862\n",
      "Epoch 4389, Loss: 0.043165139853954315, Final Batch Loss: 0.028661783784627914\n",
      "Epoch 4390, Loss: 0.050919633358716965, Final Batch Loss: 0.0370824933052063\n",
      "Epoch 4391, Loss: 0.01814548671245575, Final Batch Loss: 0.004037785343825817\n",
      "Epoch 4392, Loss: 0.04720559902489185, Final Batch Loss: 0.027761610224843025\n",
      "Epoch 4393, Loss: 0.016740757506340742, Final Batch Loss: 0.005063644144684076\n",
      "Epoch 4394, Loss: 0.05979732982814312, Final Batch Loss: 0.008086292073130608\n",
      "Epoch 4395, Loss: 0.03010521735996008, Final Batch Loss: 0.017188802361488342\n",
      "Epoch 4396, Loss: 0.02145690657198429, Final Batch Loss: 0.007197403348982334\n",
      "Epoch 4397, Loss: 0.04034986533224583, Final Batch Loss: 0.021759718656539917\n",
      "Epoch 4398, Loss: 0.04669411201030016, Final Batch Loss: 0.013967112638056278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4399, Loss: 0.08910618349909782, Final Batch Loss: 0.050107114017009735\n",
      "Epoch 4400, Loss: 0.04464785102754831, Final Batch Loss: 0.011114382185041904\n",
      "Epoch 4401, Loss: 0.056336731649935246, Final Batch Loss: 0.0417533703148365\n",
      "Epoch 4402, Loss: 0.040905795991420746, Final Batch Loss: 0.015682164579629898\n",
      "Epoch 4403, Loss: 0.0622902438044548, Final Batch Loss: 0.03792957961559296\n",
      "Epoch 4404, Loss: 0.06699508987367153, Final Batch Loss: 0.029104186221957207\n",
      "Epoch 4405, Loss: 0.027601631823927164, Final Batch Loss: 0.004707115236669779\n",
      "Epoch 4406, Loss: 0.01921993028372526, Final Batch Loss: 0.009932894259691238\n",
      "Epoch 4407, Loss: 0.050960492342710495, Final Batch Loss: 0.013748299330472946\n",
      "Epoch 4408, Loss: 0.0610020300373435, Final Batch Loss: 0.00933211948722601\n",
      "Epoch 4409, Loss: 0.12025557365268469, Final Batch Loss: 0.10717110335826874\n",
      "Epoch 4410, Loss: 0.03765048284549266, Final Batch Loss: 0.0015320387901738286\n",
      "Epoch 4411, Loss: 0.04256658162921667, Final Batch Loss: 0.013284672982990742\n",
      "Epoch 4412, Loss: 0.04032720439136028, Final Batch Loss: 0.025709964334964752\n",
      "Epoch 4413, Loss: 0.023357372265309095, Final Batch Loss: 0.007501247804611921\n",
      "Epoch 4414, Loss: 0.02955725695937872, Final Batch Loss: 0.017670687288045883\n",
      "Epoch 4415, Loss: 0.032669282518327236, Final Batch Loss: 0.008435715921223164\n",
      "Epoch 4416, Loss: 0.0728776641190052, Final Batch Loss: 0.03296886384487152\n",
      "Epoch 4417, Loss: 0.039223271887749434, Final Batch Loss: 0.005399610381573439\n",
      "Epoch 4418, Loss: 0.03528970759361982, Final Batch Loss: 0.007756938226521015\n",
      "Epoch 4419, Loss: 0.06276512891054153, Final Batch Loss: 0.014328710734844208\n",
      "Epoch 4420, Loss: 0.03007200639694929, Final Batch Loss: 0.01063856016844511\n",
      "Epoch 4421, Loss: 0.04135590046644211, Final Batch Loss: 0.009157296270132065\n",
      "Epoch 4422, Loss: 0.03149621095508337, Final Batch Loss: 0.010762657038867474\n",
      "Epoch 4423, Loss: 0.018134990241378546, Final Batch Loss: 0.006487030070275068\n",
      "Epoch 4424, Loss: 0.07187525741755962, Final Batch Loss: 0.05282885581254959\n",
      "Epoch 4425, Loss: 0.026877386495471, Final Batch Loss: 0.014445396140217781\n",
      "Epoch 4426, Loss: 0.11042349692434072, Final Batch Loss: 0.1002284362912178\n",
      "Epoch 4427, Loss: 0.04925323184579611, Final Batch Loss: 0.04093062877655029\n",
      "Epoch 4428, Loss: 0.04034246038645506, Final Batch Loss: 0.013137822039425373\n",
      "Epoch 4429, Loss: 0.04293868411332369, Final Batch Loss: 0.00795659888535738\n",
      "Epoch 4430, Loss: 0.04242648649960756, Final Batch Loss: 0.007570267654955387\n",
      "Epoch 4431, Loss: 0.05508343502879143, Final Batch Loss: 0.03271156921982765\n",
      "Epoch 4432, Loss: 0.031071593053638935, Final Batch Loss: 0.015041223727166653\n",
      "Epoch 4433, Loss: 0.05617891997098923, Final Batch Loss: 0.01728728413581848\n",
      "Epoch 4434, Loss: 0.01743671717122197, Final Batch Loss: 0.00657105864956975\n",
      "Epoch 4435, Loss: 0.05211507622152567, Final Batch Loss: 0.0413072369992733\n",
      "Epoch 4436, Loss: 0.04108524788171053, Final Batch Loss: 0.010059076361358166\n",
      "Epoch 4437, Loss: 0.0837411917746067, Final Batch Loss: 0.03666424751281738\n",
      "Epoch 4438, Loss: 0.04280305653810501, Final Batch Loss: 0.00820108875632286\n",
      "Epoch 4439, Loss: 0.05590224638581276, Final Batch Loss: 0.04195844754576683\n",
      "Epoch 4440, Loss: 0.03207943122833967, Final Batch Loss: 0.010636045597493649\n",
      "Epoch 4441, Loss: 0.03515700623393059, Final Batch Loss: 0.01672038622200489\n",
      "Epoch 4442, Loss: 0.021891526179388165, Final Batch Loss: 0.003634904744103551\n",
      "Epoch 4443, Loss: 0.019692768808454275, Final Batch Loss: 0.004854049999266863\n",
      "Epoch 4444, Loss: 0.03478453494608402, Final Batch Loss: 0.019281992688775063\n",
      "Epoch 4445, Loss: 0.033003758639097214, Final Batch Loss: 0.011635761708021164\n",
      "Epoch 4446, Loss: 0.05070194788277149, Final Batch Loss: 0.02417004108428955\n",
      "Epoch 4447, Loss: 0.01607778435572982, Final Batch Loss: 0.005054531153291464\n",
      "Epoch 4448, Loss: 0.09695183858275414, Final Batch Loss: 0.05135628581047058\n",
      "Epoch 4449, Loss: 0.051497310400009155, Final Batch Loss: 0.03027733974158764\n",
      "Epoch 4450, Loss: 0.027931914199143648, Final Batch Loss: 0.005885666701942682\n",
      "Epoch 4451, Loss: 0.0267720150295645, Final Batch Loss: 0.0031457918230444193\n",
      "Epoch 4452, Loss: 0.054550884291529655, Final Batch Loss: 0.024348998442292213\n",
      "Epoch 4453, Loss: 0.05444884579628706, Final Batch Loss: 0.012577085755765438\n",
      "Epoch 4454, Loss: 0.057725436985492706, Final Batch Loss: 0.0353185199201107\n",
      "Epoch 4455, Loss: 0.044763216748833656, Final Batch Loss: 0.02138384059071541\n",
      "Epoch 4456, Loss: 0.13682742416858673, Final Batch Loss: 0.09994682669639587\n",
      "Epoch 4457, Loss: 0.018868139944970608, Final Batch Loss: 0.0072729140520095825\n",
      "Epoch 4458, Loss: 0.0939738042652607, Final Batch Loss: 0.06288239359855652\n",
      "Epoch 4459, Loss: 0.11411665380001068, Final Batch Loss: 0.0571146234869957\n",
      "Epoch 4460, Loss: 0.09617122635245323, Final Batch Loss: 0.03087073192000389\n",
      "Epoch 4461, Loss: 0.044333016499876976, Final Batch Loss: 0.012785656377673149\n",
      "Epoch 4462, Loss: 0.48339654318988323, Final Batch Loss: 0.4652836322784424\n",
      "Epoch 4463, Loss: 0.06895486079156399, Final Batch Loss: 0.053581833839416504\n",
      "Epoch 4464, Loss: 0.050981104373931885, Final Batch Loss: 0.016603078693151474\n",
      "Epoch 4465, Loss: 0.06879315711557865, Final Batch Loss: 0.027709847316145897\n",
      "Epoch 4466, Loss: 0.040521541610360146, Final Batch Loss: 0.026058509945869446\n",
      "Epoch 4467, Loss: 0.06312461756169796, Final Batch Loss: 0.04576503112912178\n",
      "Epoch 4468, Loss: 0.028872926719486713, Final Batch Loss: 0.009633653797209263\n",
      "Epoch 4469, Loss: 0.085374616086483, Final Batch Loss: 0.0540158785879612\n",
      "Epoch 4470, Loss: 0.05075530963949859, Final Batch Loss: 0.002904607681557536\n",
      "Epoch 4471, Loss: 0.02705488633364439, Final Batch Loss: 0.009484131820499897\n",
      "Epoch 4472, Loss: 0.03549224231392145, Final Batch Loss: 0.011858963407576084\n",
      "Epoch 4473, Loss: 0.028885599225759506, Final Batch Loss: 0.007640203461050987\n",
      "Epoch 4474, Loss: 0.07002956420183182, Final Batch Loss: 0.03468959406018257\n",
      "Epoch 4475, Loss: 0.028765404364094138, Final Batch Loss: 0.003133211052045226\n",
      "Epoch 4476, Loss: 0.0899960957467556, Final Batch Loss: 0.07180715352296829\n",
      "Epoch 4477, Loss: 0.045493073761463165, Final Batch Loss: 0.0197029747068882\n",
      "Epoch 4478, Loss: 0.06890332326292992, Final Batch Loss: 0.015620563179254532\n",
      "Epoch 4479, Loss: 0.04086412116885185, Final Batch Loss: 0.0135258249938488\n",
      "Epoch 4480, Loss: 0.046977246180176735, Final Batch Loss: 0.02266932837665081\n",
      "Epoch 4481, Loss: 0.050308115780353546, Final Batch Loss: 0.01975884847342968\n",
      "Epoch 4482, Loss: 0.06478821393102407, Final Batch Loss: 0.01148334052413702\n",
      "Epoch 4483, Loss: 0.015114185865968466, Final Batch Loss: 0.005230025853961706\n",
      "Epoch 4484, Loss: 0.040773628279566765, Final Batch Loss: 0.025366071611642838\n",
      "Epoch 4485, Loss: 0.03328463062644005, Final Batch Loss: 0.01289663091301918\n",
      "Epoch 4486, Loss: 0.0259407302364707, Final Batch Loss: 0.009514751844108105\n",
      "Epoch 4487, Loss: 0.10580012202262878, Final Batch Loss: 0.057687412947416306\n",
      "Epoch 4488, Loss: 0.02442958392202854, Final Batch Loss: 0.008530300110578537\n",
      "Epoch 4489, Loss: 0.024565664120018482, Final Batch Loss: 0.004272096790373325\n",
      "Epoch 4490, Loss: 0.03526225406676531, Final Batch Loss: 0.010202775709331036\n",
      "Epoch 4491, Loss: 0.017258654348552227, Final Batch Loss: 0.010341592133045197\n",
      "Epoch 4492, Loss: 0.03979536145925522, Final Batch Loss: 0.020981894806027412\n",
      "Epoch 4493, Loss: 0.056540222838521004, Final Batch Loss: 0.0169364046305418\n",
      "Epoch 4494, Loss: 0.03752479888498783, Final Batch Loss: 0.017350781708955765\n",
      "Epoch 4495, Loss: 0.041239471174776554, Final Batch Loss: 0.011110481806099415\n",
      "Epoch 4496, Loss: 0.07861681841313839, Final Batch Loss: 0.0675998106598854\n",
      "Epoch 4497, Loss: 0.026604641694575548, Final Batch Loss: 0.00503500597551465\n",
      "Epoch 4498, Loss: 0.055779049172997475, Final Batch Loss: 0.011648723855614662\n",
      "Epoch 4499, Loss: 0.10028139688074589, Final Batch Loss: 0.08285064250230789\n",
      "Epoch 4500, Loss: 0.028990823309868574, Final Batch Loss: 0.00614367937669158\n",
      "Epoch 4501, Loss: 0.036676965188235044, Final Batch Loss: 0.004360861610621214\n",
      "Epoch 4502, Loss: 0.03713516145944595, Final Batch Loss: 0.01267334446310997\n",
      "Epoch 4503, Loss: 0.07261381670832634, Final Batch Loss: 0.027049683034420013\n",
      "Epoch 4504, Loss: 0.04971819184720516, Final Batch Loss: 0.03421437740325928\n",
      "Epoch 4505, Loss: 0.06952925771474838, Final Batch Loss: 0.03796198591589928\n",
      "Epoch 4506, Loss: 0.030293709598481655, Final Batch Loss: 0.0037845363840460777\n",
      "Epoch 4507, Loss: 0.11375384591519833, Final Batch Loss: 0.09086787700653076\n",
      "Epoch 4508, Loss: 0.11747773364186287, Final Batch Loss: 0.04256630316376686\n",
      "Epoch 4509, Loss: 0.048102449625730515, Final Batch Loss: 0.02496306225657463\n",
      "Epoch 4510, Loss: 0.031227319967001677, Final Batch Loss: 0.006686616223305464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4511, Loss: 0.04159349016845226, Final Batch Loss: 0.02800159342586994\n",
      "Epoch 4512, Loss: 0.07929855212569237, Final Batch Loss: 0.051980145275592804\n",
      "Epoch 4513, Loss: 0.02592936623841524, Final Batch Loss: 0.005164708010852337\n",
      "Epoch 4514, Loss: 0.09209022764116526, Final Batch Loss: 0.08190193772315979\n",
      "Epoch 4515, Loss: 0.02724501071497798, Final Batch Loss: 0.00683834170922637\n",
      "Epoch 4516, Loss: 0.12409013509750366, Final Batch Loss: 0.044004082679748535\n",
      "Epoch 4517, Loss: 0.0689233485609293, Final Batch Loss: 0.04083585366606712\n",
      "Epoch 4518, Loss: 0.16732782870531082, Final Batch Loss: 0.10474240779876709\n",
      "Epoch 4519, Loss: 0.1098401602357626, Final Batch Loss: 0.01712018810212612\n",
      "Epoch 4520, Loss: 0.06665358319878578, Final Batch Loss: 0.03798580542206764\n",
      "Epoch 4521, Loss: 0.03636162914335728, Final Batch Loss: 0.02348567359149456\n",
      "Epoch 4522, Loss: 0.04370688833296299, Final Batch Loss: 0.019681235775351524\n",
      "Epoch 4523, Loss: 0.025729963555932045, Final Batch Loss: 0.014487872831523418\n",
      "Epoch 4524, Loss: 0.04834041744470596, Final Batch Loss: 0.025234585627913475\n",
      "Epoch 4525, Loss: 0.028790820389986038, Final Batch Loss: 0.013687077909708023\n",
      "Epoch 4526, Loss: 0.04989004088565707, Final Batch Loss: 0.004486944060772657\n",
      "Epoch 4527, Loss: 0.060295820236206055, Final Batch Loss: 0.026715748012065887\n",
      "Epoch 4528, Loss: 0.08266002032905817, Final Batch Loss: 0.06896688044071198\n",
      "Epoch 4529, Loss: 0.017025591107085347, Final Batch Loss: 0.0037386866752058268\n",
      "Epoch 4530, Loss: 0.034987056627869606, Final Batch Loss: 0.007639758288860321\n",
      "Epoch 4531, Loss: 0.03483422100543976, Final Batch Loss: 0.009230619296431541\n",
      "Epoch 4532, Loss: 0.022410220000892878, Final Batch Loss: 0.004800169263035059\n",
      "Epoch 4533, Loss: 0.0442441962659359, Final Batch Loss: 0.02714090794324875\n",
      "Epoch 4534, Loss: 0.09872685000300407, Final Batch Loss: 0.0128558911383152\n",
      "Epoch 4535, Loss: 0.06385688856244087, Final Batch Loss: 0.02117267996072769\n",
      "Epoch 4536, Loss: 0.01896987110376358, Final Batch Loss: 0.008369620889425278\n",
      "Epoch 4537, Loss: 0.10122228320688009, Final Batch Loss: 0.08658395707607269\n",
      "Epoch 4538, Loss: 0.024971163365989923, Final Batch Loss: 0.005453560966998339\n",
      "Epoch 4539, Loss: 0.09125538915395737, Final Batch Loss: 0.06771647930145264\n",
      "Epoch 4540, Loss: 0.028942836448550224, Final Batch Loss: 0.013232925906777382\n",
      "Epoch 4541, Loss: 0.06223459169268608, Final Batch Loss: 0.04348200559616089\n",
      "Epoch 4542, Loss: 0.30594202876091003, Final Batch Loss: 0.015267044305801392\n",
      "Epoch 4543, Loss: 0.04327934421598911, Final Batch Loss: 0.015222586691379547\n",
      "Epoch 4544, Loss: 0.07095569185912609, Final Batch Loss: 0.013651615008711815\n",
      "Epoch 4545, Loss: 0.025152079295367002, Final Batch Loss: 0.005143155809491873\n",
      "Epoch 4546, Loss: 0.036407713778316975, Final Batch Loss: 0.009747293777763844\n",
      "Epoch 4547, Loss: 0.02444087155163288, Final Batch Loss: 0.012563258409500122\n",
      "Epoch 4548, Loss: 0.038807984441518784, Final Batch Loss: 0.014699574559926987\n",
      "Epoch 4549, Loss: 0.05121403839439154, Final Batch Loss: 0.008215273730456829\n",
      "Epoch 4550, Loss: 0.042335620848461986, Final Batch Loss: 0.0029995578806847334\n",
      "Epoch 4551, Loss: 0.05106654204428196, Final Batch Loss: 0.03298337757587433\n",
      "Epoch 4552, Loss: 0.056791599839925766, Final Batch Loss: 0.023000765591859818\n",
      "Epoch 4553, Loss: 0.019735688343644142, Final Batch Loss: 0.009233440272510052\n",
      "Epoch 4554, Loss: 0.06635366566479206, Final Batch Loss: 0.017240596935153008\n",
      "Epoch 4555, Loss: 0.022580997552722692, Final Batch Loss: 0.005915481131523848\n",
      "Epoch 4556, Loss: 0.04587096720933914, Final Batch Loss: 0.025013485923409462\n",
      "Epoch 4557, Loss: 0.05392084829509258, Final Batch Loss: 0.03466492518782616\n",
      "Epoch 4558, Loss: 0.025011965073645115, Final Batch Loss: 0.0048385923728346825\n",
      "Epoch 4559, Loss: 0.022993536666035652, Final Batch Loss: 0.014831986278295517\n",
      "Epoch 4560, Loss: 0.058623798191547394, Final Batch Loss: 0.023653727024793625\n",
      "Epoch 4561, Loss: 0.027035200968384743, Final Batch Loss: 0.0080699622631073\n",
      "Epoch 4562, Loss: 0.04689822532236576, Final Batch Loss: 0.030013062059879303\n",
      "Epoch 4563, Loss: 0.09667831659317017, Final Batch Loss: 0.0668647289276123\n",
      "Epoch 4564, Loss: 0.059879397973418236, Final Batch Loss: 0.018407059833407402\n",
      "Epoch 4565, Loss: 0.021801753900945187, Final Batch Loss: 0.010323481634259224\n",
      "Epoch 4566, Loss: 0.06305285077542067, Final Batch Loss: 0.055628757923841476\n",
      "Epoch 4567, Loss: 0.026810254901647568, Final Batch Loss: 0.013335018418729305\n",
      "Epoch 4568, Loss: 0.019904011394828558, Final Batch Loss: 0.0045613511465489864\n",
      "Epoch 4569, Loss: 0.02111114375293255, Final Batch Loss: 0.007973350584506989\n",
      "Epoch 4570, Loss: 0.04213360231369734, Final Batch Loss: 0.031204307451844215\n",
      "Epoch 4571, Loss: 0.013700237963348627, Final Batch Loss: 0.004795062821358442\n",
      "Epoch 4572, Loss: 0.08407875383272767, Final Batch Loss: 0.07895093411207199\n",
      "Epoch 4573, Loss: 0.03771388903260231, Final Batch Loss: 0.02064564824104309\n",
      "Epoch 4574, Loss: 0.028025959618389606, Final Batch Loss: 0.012355337850749493\n",
      "Epoch 4575, Loss: 0.05307588167488575, Final Batch Loss: 0.006835104897618294\n",
      "Epoch 4576, Loss: 0.04641408659517765, Final Batch Loss: 0.015865374356508255\n",
      "Epoch 4577, Loss: 0.049360075034201145, Final Batch Loss: 0.03755045309662819\n",
      "Epoch 4578, Loss: 0.032103621400892735, Final Batch Loss: 0.011862154118716717\n",
      "Epoch 4579, Loss: 0.17441071569919586, Final Batch Loss: 0.10332027077674866\n",
      "Epoch 4580, Loss: 0.04340740479528904, Final Batch Loss: 0.01065005175769329\n",
      "Epoch 4581, Loss: 0.03986064810305834, Final Batch Loss: 0.004663397558033466\n",
      "Epoch 4582, Loss: 0.058216447941958904, Final Batch Loss: 0.046923521906137466\n",
      "Epoch 4583, Loss: 0.044019236229360104, Final Batch Loss: 0.003938890062272549\n",
      "Epoch 4584, Loss: 0.03580999933183193, Final Batch Loss: 0.01880696974694729\n",
      "Epoch 4585, Loss: 0.035910794511437416, Final Batch Loss: 0.009482702240347862\n",
      "Epoch 4586, Loss: 0.045615531504154205, Final Batch Loss: 0.0356002077460289\n",
      "Epoch 4587, Loss: 0.020465089939534664, Final Batch Loss: 0.0076835304498672485\n",
      "Epoch 4588, Loss: 0.05827699042856693, Final Batch Loss: 0.026885760948061943\n",
      "Epoch 4589, Loss: 0.02060133870691061, Final Batch Loss: 0.011350629851222038\n",
      "Epoch 4590, Loss: 0.0594344288110733, Final Batch Loss: 0.05169985443353653\n",
      "Epoch 4591, Loss: 0.017321451799944043, Final Batch Loss: 0.0028308548498898745\n",
      "Epoch 4592, Loss: 0.047513640485703945, Final Batch Loss: 0.0149638457223773\n",
      "Epoch 4593, Loss: 0.026466062758117914, Final Batch Loss: 0.007742027286440134\n",
      "Epoch 4594, Loss: 0.02739415131509304, Final Batch Loss: 0.008695762604475021\n",
      "Epoch 4595, Loss: 0.09187226556241512, Final Batch Loss: 0.021392790600657463\n",
      "Epoch 4596, Loss: 0.05965460278093815, Final Batch Loss: 0.04861757159233093\n",
      "Epoch 4597, Loss: 0.021711396984755993, Final Batch Loss: 0.012740780599415302\n",
      "Epoch 4598, Loss: 0.07256995141506195, Final Batch Loss: 0.038624655455350876\n",
      "Epoch 4599, Loss: 0.03445983398705721, Final Batch Loss: 0.006277642212808132\n",
      "Epoch 4600, Loss: 0.043689433485269547, Final Batch Loss: 0.019085554406046867\n",
      "Epoch 4601, Loss: 0.02278681006282568, Final Batch Loss: 0.011314819566905499\n",
      "Epoch 4602, Loss: 0.01724368380382657, Final Batch Loss: 0.0038040182553231716\n",
      "Epoch 4603, Loss: 0.06127049960196018, Final Batch Loss: 0.026502976194024086\n",
      "Epoch 4604, Loss: 0.02389204315841198, Final Batch Loss: 0.004629271104931831\n",
      "Epoch 4605, Loss: 0.04510708153247833, Final Batch Loss: 0.004812121391296387\n",
      "Epoch 4606, Loss: 0.02312159352004528, Final Batch Loss: 0.011126667261123657\n",
      "Epoch 4607, Loss: 0.049005575478076935, Final Batch Loss: 0.036878421902656555\n",
      "Epoch 4608, Loss: 0.013806399423629045, Final Batch Loss: 0.0015676734037697315\n",
      "Epoch 4609, Loss: 0.04037164617329836, Final Batch Loss: 0.009180775843560696\n",
      "Epoch 4610, Loss: 0.027089614421129227, Final Batch Loss: 0.008748592808842659\n",
      "Epoch 4611, Loss: 0.02503502508625388, Final Batch Loss: 0.004401772748678923\n",
      "Epoch 4612, Loss: 0.043423859402537346, Final Batch Loss: 0.02104083076119423\n",
      "Epoch 4613, Loss: 0.031649747397750616, Final Batch Loss: 0.004539966117590666\n",
      "Epoch 4614, Loss: 0.029795429669320583, Final Batch Loss: 0.015469098463654518\n",
      "Epoch 4615, Loss: 0.07644214853644371, Final Batch Loss: 0.04126879200339317\n",
      "Epoch 4616, Loss: 0.05027652904391289, Final Batch Loss: 0.00972094014286995\n",
      "Epoch 4617, Loss: 0.044211167842149734, Final Batch Loss: 0.0054439157247543335\n",
      "Epoch 4618, Loss: 0.02696530893445015, Final Batch Loss: 0.007290910929441452\n",
      "Epoch 4619, Loss: 0.07272792607545853, Final Batch Loss: 0.04104945436120033\n",
      "Epoch 4620, Loss: 0.043019539676606655, Final Batch Loss: 0.010608966462314129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4621, Loss: 0.03283082786947489, Final Batch Loss: 0.018263371661305428\n",
      "Epoch 4622, Loss: 0.05634252727031708, Final Batch Loss: 0.013628244400024414\n",
      "Epoch 4623, Loss: 0.038352878764271736, Final Batch Loss: 0.01952105574309826\n",
      "Epoch 4624, Loss: 0.10893730632960796, Final Batch Loss: 0.09993964433670044\n",
      "Epoch 4625, Loss: 0.0559863205999136, Final Batch Loss: 0.022603338584303856\n",
      "Epoch 4626, Loss: 0.04248453862965107, Final Batch Loss: 0.02316303737461567\n",
      "Epoch 4627, Loss: 0.03307504393160343, Final Batch Loss: 0.018293695524334908\n",
      "Epoch 4628, Loss: 0.013854455202817917, Final Batch Loss: 0.006856279913336039\n",
      "Epoch 4629, Loss: 0.05261869169771671, Final Batch Loss: 0.03006810136139393\n",
      "Epoch 4630, Loss: 0.03687780164182186, Final Batch Loss: 0.01433584839105606\n",
      "Epoch 4631, Loss: 0.04131005937233567, Final Batch Loss: 0.00632478529587388\n",
      "Epoch 4632, Loss: 0.02832372486591339, Final Batch Loss: 0.004361458122730255\n",
      "Epoch 4633, Loss: 0.03811363875865936, Final Batch Loss: 0.01658821851015091\n",
      "Epoch 4634, Loss: 0.03628725744783878, Final Batch Loss: 0.014733726158738136\n",
      "Epoch 4635, Loss: 0.05975419469177723, Final Batch Loss: 0.027193812653422356\n",
      "Epoch 4636, Loss: 0.03407322242856026, Final Batch Loss: 0.012691285461187363\n",
      "Epoch 4637, Loss: 0.07795269042253494, Final Batch Loss: 0.03465097025036812\n",
      "Epoch 4638, Loss: 0.03560423571616411, Final Batch Loss: 0.009809338487684727\n",
      "Epoch 4639, Loss: 0.017118998803198338, Final Batch Loss: 0.008115106262266636\n",
      "Epoch 4640, Loss: 0.12428436428308487, Final Batch Loss: 0.05071602761745453\n",
      "Epoch 4641, Loss: 0.029737750999629498, Final Batch Loss: 0.008575341664254665\n",
      "Epoch 4642, Loss: 0.04785354994237423, Final Batch Loss: 0.03907673805952072\n",
      "Epoch 4643, Loss: 0.048518357798457146, Final Batch Loss: 0.02128198929131031\n",
      "Epoch 4644, Loss: 0.027521432377398014, Final Batch Loss: 0.009775730781257153\n",
      "Epoch 4645, Loss: 0.03441851586103439, Final Batch Loss: 0.015237707644701004\n",
      "Epoch 4646, Loss: 0.04393745446577668, Final Batch Loss: 0.03908862918615341\n",
      "Epoch 4647, Loss: 0.03910785913467407, Final Batch Loss: 0.029488155618309975\n",
      "Epoch 4648, Loss: 0.04745311848819256, Final Batch Loss: 0.030592959374189377\n",
      "Epoch 4649, Loss: 0.041623349068686366, Final Batch Loss: 0.0027034420054405928\n",
      "Epoch 4650, Loss: 0.02760643232613802, Final Batch Loss: 0.010631407611072063\n",
      "Epoch 4651, Loss: 0.039888788014650345, Final Batch Loss: 0.01793891191482544\n",
      "Epoch 4652, Loss: 0.05527610331773758, Final Batch Loss: 0.02644379809498787\n",
      "Epoch 4653, Loss: 0.10096218809485435, Final Batch Loss: 0.031238984316587448\n",
      "Epoch 4654, Loss: 0.01558931521140039, Final Batch Loss: 0.0030266435351222754\n",
      "Epoch 4655, Loss: 0.032034095376729965, Final Batch Loss: 0.014638874679803848\n",
      "Epoch 4656, Loss: 0.027434545569121838, Final Batch Loss: 0.010976956225931644\n",
      "Epoch 4657, Loss: 0.0633935914374888, Final Batch Loss: 0.004805950913578272\n",
      "Epoch 4658, Loss: 0.06433126796036959, Final Batch Loss: 0.010791846551001072\n",
      "Epoch 4659, Loss: 0.138393834233284, Final Batch Loss: 0.10040823370218277\n",
      "Epoch 4660, Loss: 0.0573888523504138, Final Batch Loss: 0.047097403556108475\n",
      "Epoch 4661, Loss: 0.05720845051109791, Final Batch Loss: 0.0236229058355093\n",
      "Epoch 4662, Loss: 0.050305819138884544, Final Batch Loss: 0.010817872360348701\n",
      "Epoch 4663, Loss: 0.0725919920951128, Final Batch Loss: 0.0422782264649868\n",
      "Epoch 4664, Loss: 0.04515585629269481, Final Batch Loss: 0.005198079627007246\n",
      "Epoch 4665, Loss: 0.09395177662372589, Final Batch Loss: 0.0626940205693245\n",
      "Epoch 4666, Loss: 0.02864664513617754, Final Batch Loss: 0.011600005440413952\n",
      "Epoch 4667, Loss: 0.08466777764260769, Final Batch Loss: 0.01589198224246502\n",
      "Epoch 4668, Loss: 0.036756960675120354, Final Batch Loss: 0.018293317407369614\n",
      "Epoch 4669, Loss: 0.025572005659341812, Final Batch Loss: 0.011887635104358196\n",
      "Epoch 4670, Loss: 0.17520969081670046, Final Batch Loss: 0.1606610268354416\n",
      "Epoch 4671, Loss: 0.048631091602146626, Final Batch Loss: 0.011679050512611866\n",
      "Epoch 4672, Loss: 0.01934673497453332, Final Batch Loss: 0.005466809030622244\n",
      "Epoch 4673, Loss: 0.09109184518456459, Final Batch Loss: 0.040711671113967896\n",
      "Epoch 4674, Loss: 0.019563638139516115, Final Batch Loss: 0.007631575223058462\n",
      "Epoch 4675, Loss: 0.04838769230991602, Final Batch Loss: 0.03591398522257805\n",
      "Epoch 4676, Loss: 0.05348860286176205, Final Batch Loss: 0.012419352307915688\n",
      "Epoch 4677, Loss: 0.03490982111543417, Final Batch Loss: 0.009831874631345272\n",
      "Epoch 4678, Loss: 0.041766750626266, Final Batch Loss: 0.01011634524911642\n",
      "Epoch 4679, Loss: 0.05412949062883854, Final Batch Loss: 0.01560223288834095\n",
      "Epoch 4680, Loss: 0.08521166071295738, Final Batch Loss: 0.0480579175055027\n",
      "Epoch 4681, Loss: 0.05729319527745247, Final Batch Loss: 0.025104276835918427\n",
      "Epoch 4682, Loss: 0.03453150764107704, Final Batch Loss: 0.01552748866379261\n",
      "Epoch 4683, Loss: 0.06755131483078003, Final Batch Loss: 0.04727725684642792\n",
      "Epoch 4684, Loss: 0.021989324130117893, Final Batch Loss: 0.012122314423322678\n",
      "Epoch 4685, Loss: 0.03561817482113838, Final Batch Loss: 0.016711361706256866\n",
      "Epoch 4686, Loss: 0.07029774412512779, Final Batch Loss: 0.04354534670710564\n",
      "Epoch 4687, Loss: 0.057770634070038795, Final Batch Loss: 0.007584741339087486\n",
      "Epoch 4688, Loss: 0.04566107131540775, Final Batch Loss: 0.016553683206439018\n",
      "Epoch 4689, Loss: 0.046089005656540394, Final Batch Loss: 0.007917509414255619\n",
      "Epoch 4690, Loss: 0.04088012548163533, Final Batch Loss: 0.0032577035017311573\n",
      "Epoch 4691, Loss: 0.09635927528142929, Final Batch Loss: 0.044460881501436234\n",
      "Epoch 4692, Loss: 0.01968788355588913, Final Batch Loss: 0.007309027947485447\n",
      "Epoch 4693, Loss: 0.051538363099098206, Final Batch Loss: 0.02915295772254467\n",
      "Epoch 4694, Loss: 0.0672809686511755, Final Batch Loss: 0.0372193418443203\n",
      "Epoch 4695, Loss: 0.0715582575649023, Final Batch Loss: 0.04457179456949234\n",
      "Epoch 4696, Loss: 0.0479419119656086, Final Batch Loss: 0.020880240947008133\n",
      "Epoch 4697, Loss: 0.08166283369064331, Final Batch Loss: 0.04864591732621193\n",
      "Epoch 4698, Loss: 0.0864371694624424, Final Batch Loss: 0.025042887777090073\n",
      "Epoch 4699, Loss: 0.07280302233994007, Final Batch Loss: 0.02652754820883274\n",
      "Epoch 4700, Loss: 0.06280061416327953, Final Batch Loss: 0.010770728811621666\n",
      "Epoch 4701, Loss: 0.03547927923500538, Final Batch Loss: 0.016345681622624397\n",
      "Epoch 4702, Loss: 0.04600001685321331, Final Batch Loss: 0.021358614787459373\n",
      "Epoch 4703, Loss: 0.03497396968305111, Final Batch Loss: 0.015123415738344193\n",
      "Epoch 4704, Loss: 0.051336612552404404, Final Batch Loss: 0.008702438324689865\n",
      "Epoch 4705, Loss: 0.045064644888043404, Final Batch Loss: 0.012566177174448967\n",
      "Epoch 4706, Loss: 0.05401945114135742, Final Batch Loss: 0.03678152337670326\n",
      "Epoch 4707, Loss: 0.0621962733566761, Final Batch Loss: 0.0036733411252498627\n",
      "Epoch 4708, Loss: 0.024305124767124653, Final Batch Loss: 0.013203722424805164\n",
      "Epoch 4709, Loss: 0.03668653219938278, Final Batch Loss: 0.02382439188659191\n",
      "Epoch 4710, Loss: 0.01774230459704995, Final Batch Loss: 0.004022798966616392\n",
      "Epoch 4711, Loss: 0.018064549658447504, Final Batch Loss: 0.006586604285985231\n",
      "Epoch 4712, Loss: 0.05615571420639753, Final Batch Loss: 0.0036404645070433617\n",
      "Epoch 4713, Loss: 0.03798373555764556, Final Batch Loss: 0.002660789992660284\n",
      "Epoch 4714, Loss: 0.02658035885542631, Final Batch Loss: 0.010073564015328884\n",
      "Epoch 4715, Loss: 0.034668197855353355, Final Batch Loss: 0.008163772523403168\n",
      "Epoch 4716, Loss: 0.0388052212074399, Final Batch Loss: 0.010291888378560543\n",
      "Epoch 4717, Loss: 0.04304710403084755, Final Batch Loss: 0.020154694095253944\n",
      "Epoch 4718, Loss: 0.013880675891414285, Final Batch Loss: 0.0020563763100653887\n",
      "Epoch 4719, Loss: 0.06970863603055477, Final Batch Loss: 0.05878464877605438\n",
      "Epoch 4720, Loss: 0.0382778299972415, Final Batch Loss: 0.02380199544131756\n",
      "Epoch 4721, Loss: 0.06329866126179695, Final Batch Loss: 0.008204273879528046\n",
      "Epoch 4722, Loss: 0.043077029287815094, Final Batch Loss: 0.033127956092357635\n",
      "Epoch 4723, Loss: 0.05295461602509022, Final Batch Loss: 0.01674761064350605\n",
      "Epoch 4724, Loss: 0.09349288046360016, Final Batch Loss: 0.06736079603433609\n",
      "Epoch 4725, Loss: 0.026928226463496685, Final Batch Loss: 0.0036254869773983955\n",
      "Epoch 4726, Loss: 0.051067715510725975, Final Batch Loss: 0.022808246314525604\n",
      "Epoch 4727, Loss: 0.054159085266292095, Final Batch Loss: 0.011487155221402645\n",
      "Epoch 4728, Loss: 0.024731282610446215, Final Batch Loss: 0.005010251421481371\n",
      "Epoch 4729, Loss: 0.12301129847764969, Final Batch Loss: 0.053203701972961426\n",
      "Epoch 4730, Loss: 0.055352140218019485, Final Batch Loss: 0.016646523028612137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4731, Loss: 0.05178777314722538, Final Batch Loss: 0.019146865233778954\n",
      "Epoch 4732, Loss: 0.08316313102841377, Final Batch Loss: 0.06457648426294327\n",
      "Epoch 4733, Loss: 0.04921284411102533, Final Batch Loss: 0.012056873179972172\n",
      "Epoch 4734, Loss: 0.023634139448404312, Final Batch Loss: 0.0021057967096567154\n",
      "Epoch 4735, Loss: 0.04732196964323521, Final Batch Loss: 0.02265194058418274\n",
      "Epoch 4736, Loss: 0.05095309764146805, Final Batch Loss: 0.028253359720110893\n",
      "Epoch 4737, Loss: 0.05175399221479893, Final Batch Loss: 0.028751207515597343\n",
      "Epoch 4738, Loss: 0.05224475637078285, Final Batch Loss: 0.01902935653924942\n",
      "Epoch 4739, Loss: 0.051930627785623074, Final Batch Loss: 0.00871309544891119\n",
      "Epoch 4740, Loss: 0.04742551036179066, Final Batch Loss: 0.0277960617095232\n",
      "Epoch 4741, Loss: 0.060830400325357914, Final Batch Loss: 0.013713126070797443\n",
      "Epoch 4742, Loss: 0.040632705204188824, Final Batch Loss: 0.015311061404645443\n",
      "Epoch 4743, Loss: 0.0349190179258585, Final Batch Loss: 0.008220523595809937\n",
      "Epoch 4744, Loss: 0.062052784487605095, Final Batch Loss: 0.039892781525850296\n",
      "Epoch 4745, Loss: 0.040508066304028034, Final Batch Loss: 0.029948782175779343\n",
      "Epoch 4746, Loss: 0.0558779202401638, Final Batch Loss: 0.03031701222062111\n",
      "Epoch 4747, Loss: 0.06951811723411083, Final Batch Loss: 0.024338779971003532\n",
      "Epoch 4748, Loss: 0.042033152654767036, Final Batch Loss: 0.008928461000323296\n",
      "Epoch 4749, Loss: 0.19353989884257317, Final Batch Loss: 0.15294094383716583\n",
      "Epoch 4750, Loss: 0.02635355293750763, Final Batch Loss: 0.0151919424533844\n",
      "Epoch 4751, Loss: 0.039097405737265944, Final Batch Loss: 0.003876641159877181\n",
      "Epoch 4752, Loss: 0.019860639236867428, Final Batch Loss: 0.01176862046122551\n",
      "Epoch 4753, Loss: 0.09540338348597288, Final Batch Loss: 0.08627735823392868\n",
      "Epoch 4754, Loss: 0.09651735424995422, Final Batch Loss: 0.08106323331594467\n",
      "Epoch 4755, Loss: 0.13585399091243744, Final Batch Loss: 0.08063225448131561\n",
      "Epoch 4756, Loss: 0.01814791653305292, Final Batch Loss: 0.004479142837226391\n",
      "Epoch 4757, Loss: 0.06005076877772808, Final Batch Loss: 0.031915340572595596\n",
      "Epoch 4758, Loss: 0.02190172951668501, Final Batch Loss: 0.008397734723985195\n",
      "Epoch 4759, Loss: 0.025677647441625595, Final Batch Loss: 0.011360542848706245\n",
      "Epoch 4760, Loss: 0.1238778829574585, Final Batch Loss: 0.1057414561510086\n",
      "Epoch 4761, Loss: 0.01685900939628482, Final Batch Loss: 0.004058671649545431\n",
      "Epoch 4762, Loss: 0.0801035650074482, Final Batch Loss: 0.06115129217505455\n",
      "Epoch 4763, Loss: 0.026254983618855476, Final Batch Loss: 0.01172434538602829\n",
      "Epoch 4764, Loss: 0.038624235428869724, Final Batch Loss: 0.015448651276528835\n",
      "Epoch 4765, Loss: 0.09313139319419861, Final Batch Loss: 0.06857415288686752\n",
      "Epoch 4766, Loss: 0.10377919301390648, Final Batch Loss: 0.06269915401935577\n",
      "Epoch 4767, Loss: 0.08684010058641434, Final Batch Loss: 0.0643448457121849\n",
      "Epoch 4768, Loss: 0.03972201608121395, Final Batch Loss: 0.0267329141497612\n",
      "Epoch 4769, Loss: 0.040767488069832325, Final Batch Loss: 0.012416948564350605\n",
      "Epoch 4770, Loss: 0.04454034473747015, Final Batch Loss: 0.010316356085240841\n",
      "Epoch 4771, Loss: 0.03037216793745756, Final Batch Loss: 0.009592645801603794\n",
      "Epoch 4772, Loss: 0.030144686345010996, Final Batch Loss: 0.0053895884193480015\n",
      "Epoch 4773, Loss: 0.0390609335154295, Final Batch Loss: 0.015546923503279686\n",
      "Epoch 4774, Loss: 0.04770469292998314, Final Batch Loss: 0.023420795798301697\n",
      "Epoch 4775, Loss: 0.02982535120099783, Final Batch Loss: 0.01952051743865013\n",
      "Epoch 4776, Loss: 0.037286922335624695, Final Batch Loss: 0.02145775966346264\n",
      "Epoch 4777, Loss: 0.022108441218733788, Final Batch Loss: 0.0115221431478858\n",
      "Epoch 4778, Loss: 0.07906909938901663, Final Batch Loss: 0.06907623261213303\n",
      "Epoch 4779, Loss: 0.034247562289237976, Final Batch Loss: 0.008739262819290161\n",
      "Epoch 4780, Loss: 0.07153509743511677, Final Batch Loss: 0.060534972697496414\n",
      "Epoch 4781, Loss: 0.05080597102642059, Final Batch Loss: 0.013269525021314621\n",
      "Epoch 4782, Loss: 0.02911055088043213, Final Batch Loss: 0.010980295017361641\n",
      "Epoch 4783, Loss: 0.02077868627384305, Final Batch Loss: 0.004279446322470903\n",
      "Epoch 4784, Loss: 0.026598768308758736, Final Batch Loss: 0.015489141456782818\n",
      "Epoch 4785, Loss: 0.12596346624195576, Final Batch Loss: 0.020766662433743477\n",
      "Epoch 4786, Loss: 0.02897137589752674, Final Batch Loss: 0.007999449968338013\n",
      "Epoch 4787, Loss: 0.027352729812264442, Final Batch Loss: 0.012542706914246082\n",
      "Epoch 4788, Loss: 0.04331177659332752, Final Batch Loss: 0.01574605330824852\n",
      "Epoch 4789, Loss: 0.04526137001812458, Final Batch Loss: 0.011087549850344658\n",
      "Epoch 4790, Loss: 0.0421672947704792, Final Batch Loss: 0.019223397597670555\n",
      "Epoch 4791, Loss: 0.08684459887444973, Final Batch Loss: 0.07280876487493515\n",
      "Epoch 4792, Loss: 0.019904180895537138, Final Batch Loss: 0.006541652139276266\n",
      "Epoch 4793, Loss: 0.047727957367897034, Final Batch Loss: 0.01635025069117546\n",
      "Epoch 4794, Loss: 0.032044971361756325, Final Batch Loss: 0.015408655628561974\n",
      "Epoch 4795, Loss: 0.06711008213460445, Final Batch Loss: 0.02417672984302044\n",
      "Epoch 4796, Loss: 0.0190751850605011, Final Batch Loss: 0.004829864017665386\n",
      "Epoch 4797, Loss: 0.0477526281028986, Final Batch Loss: 0.0275453869253397\n",
      "Epoch 4798, Loss: 0.024190775118768215, Final Batch Loss: 0.015402990393340588\n",
      "Epoch 4799, Loss: 0.023742353077977896, Final Batch Loss: 0.0076985745690763\n",
      "Epoch 4800, Loss: 0.04328369069844484, Final Batch Loss: 0.009084180928766727\n",
      "Epoch 4801, Loss: 0.050202501937747, Final Batch Loss: 0.023524893447756767\n",
      "Epoch 4802, Loss: 0.03045358811505139, Final Batch Loss: 0.0036055867094546556\n",
      "Epoch 4803, Loss: 0.07983227074146271, Final Batch Loss: 0.0545528307557106\n",
      "Epoch 4804, Loss: 0.028473938815295696, Final Batch Loss: 0.016286326572299004\n",
      "Epoch 4805, Loss: 0.01840062253177166, Final Batch Loss: 0.005513499490916729\n",
      "Epoch 4806, Loss: 0.060114895924925804, Final Batch Loss: 0.03175821155309677\n",
      "Epoch 4807, Loss: 0.04727604240179062, Final Batch Loss: 0.01949916034936905\n",
      "Epoch 4808, Loss: 0.030678810318931937, Final Batch Loss: 0.0038130858447402716\n",
      "Epoch 4809, Loss: 0.037122730165719986, Final Batch Loss: 0.01646619290113449\n",
      "Epoch 4810, Loss: 0.034062582068145275, Final Batch Loss: 0.023253697901964188\n",
      "Epoch 4811, Loss: 0.09471292421221733, Final Batch Loss: 0.05264287069439888\n",
      "Epoch 4812, Loss: 0.03604420181363821, Final Batch Loss: 0.022238560020923615\n",
      "Epoch 4813, Loss: 0.09940538182854652, Final Batch Loss: 0.05108806863427162\n",
      "Epoch 4814, Loss: 0.032818108797073364, Final Batch Loss: 0.019030727446079254\n",
      "Epoch 4815, Loss: 0.022418956272304058, Final Batch Loss: 0.006536967121064663\n",
      "Epoch 4816, Loss: 0.03096067439764738, Final Batch Loss: 0.020178936421871185\n",
      "Epoch 4817, Loss: 0.01777273090556264, Final Batch Loss: 0.004080113489180803\n",
      "Epoch 4818, Loss: 0.11589958891272545, Final Batch Loss: 0.03135949745774269\n",
      "Epoch 4819, Loss: 0.09111064579337835, Final Batch Loss: 0.08340880274772644\n",
      "Epoch 4820, Loss: 0.054993235506117344, Final Batch Loss: 0.04078575596213341\n",
      "Epoch 4821, Loss: 0.04444959666579962, Final Batch Loss: 0.014312571845948696\n",
      "Epoch 4822, Loss: 0.022608097176998854, Final Batch Loss: 0.007717830594629049\n",
      "Epoch 4823, Loss: 0.045841729268431664, Final Batch Loss: 0.022081799805164337\n",
      "Epoch 4824, Loss: 0.06849485076963902, Final Batch Loss: 0.04106540232896805\n",
      "Epoch 4825, Loss: 0.0746631771326065, Final Batch Loss: 0.03702772408723831\n",
      "Epoch 4826, Loss: 0.21246709115803242, Final Batch Loss: 0.018489794805645943\n",
      "Epoch 4827, Loss: 0.020792456343770027, Final Batch Loss: 0.011659890413284302\n",
      "Epoch 4828, Loss: 0.05602536164224148, Final Batch Loss: 0.010578667744994164\n",
      "Epoch 4829, Loss: 0.07880278676748276, Final Batch Loss: 0.028477437794208527\n",
      "Epoch 4830, Loss: 0.0300152525305748, Final Batch Loss: 0.013898184522986412\n",
      "Epoch 4831, Loss: 0.04636675864458084, Final Batch Loss: 0.027193736284971237\n",
      "Epoch 4832, Loss: 0.06792484782636166, Final Batch Loss: 0.02107776515185833\n",
      "Epoch 4833, Loss: 0.03750125830993056, Final Batch Loss: 0.006993606220930815\n",
      "Epoch 4834, Loss: 0.0359471058472991, Final Batch Loss: 0.02767803706228733\n",
      "Epoch 4835, Loss: 0.05009819380939007, Final Batch Loss: 0.015803547576069832\n",
      "Epoch 4836, Loss: 0.035922310315072536, Final Batch Loss: 0.01397677231580019\n",
      "Epoch 4837, Loss: 0.15570026077330112, Final Batch Loss: 0.13310997188091278\n",
      "Epoch 4838, Loss: 0.10267674922943115, Final Batch Loss: 0.053365156054496765\n",
      "Epoch 4839, Loss: 0.057737430557608604, Final Batch Loss: 0.03910348191857338\n",
      "Epoch 4840, Loss: 0.05505853705108166, Final Batch Loss: 0.027083128690719604\n",
      "Epoch 4841, Loss: 0.049621039070189, Final Batch Loss: 0.007209070958197117\n",
      "Epoch 4842, Loss: 0.028030221350491047, Final Batch Loss: 0.012880640104413033\n",
      "Epoch 4843, Loss: 0.024744620081037283, Final Batch Loss: 0.004915121477097273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4844, Loss: 0.022891396190971136, Final Batch Loss: 0.0043238489888608456\n",
      "Epoch 4845, Loss: 0.02894625812768936, Final Batch Loss: 0.010599156841635704\n",
      "Epoch 4846, Loss: 0.020608361810445786, Final Batch Loss: 0.006574421189725399\n",
      "Epoch 4847, Loss: 0.04963554348796606, Final Batch Loss: 0.003092731349170208\n",
      "Epoch 4848, Loss: 0.028460773639380932, Final Batch Loss: 0.01227936428040266\n",
      "Epoch 4849, Loss: 0.02216820791363716, Final Batch Loss: 0.007952241227030754\n",
      "Epoch 4850, Loss: 0.06819468596950173, Final Batch Loss: 0.006633235607296228\n",
      "Epoch 4851, Loss: 0.05822578817605972, Final Batch Loss: 0.021890047937631607\n",
      "Epoch 4852, Loss: 0.05504034645855427, Final Batch Loss: 0.033318180590867996\n",
      "Epoch 4853, Loss: 0.03775767423212528, Final Batch Loss: 0.012672888115048409\n",
      "Epoch 4854, Loss: 0.04033508989959955, Final Batch Loss: 0.01115360390394926\n",
      "Epoch 4855, Loss: 0.10537439212203026, Final Batch Loss: 0.07588862627744675\n",
      "Epoch 4856, Loss: 0.08427033387124538, Final Batch Loss: 0.0762641653418541\n",
      "Epoch 4857, Loss: 0.03162092063575983, Final Batch Loss: 0.00849782396107912\n",
      "Epoch 4858, Loss: 0.058768102899193764, Final Batch Loss: 0.013550868257880211\n",
      "Epoch 4859, Loss: 0.029499469324946404, Final Batch Loss: 0.01104188896715641\n",
      "Epoch 4860, Loss: 0.24892225675284863, Final Batch Loss: 0.23615328967571259\n",
      "Epoch 4861, Loss: 0.0934694055467844, Final Batch Loss: 0.07682490348815918\n",
      "Epoch 4862, Loss: 0.04664789326488972, Final Batch Loss: 0.030090657994151115\n",
      "Epoch 4863, Loss: 0.07831545174121857, Final Batch Loss: 0.04737895727157593\n",
      "Epoch 4864, Loss: 0.04973240662366152, Final Batch Loss: 0.04005221650004387\n",
      "Epoch 4865, Loss: 0.04230920784175396, Final Batch Loss: 0.009106619283556938\n",
      "Epoch 4866, Loss: 0.05421479605138302, Final Batch Loss: 0.022794591262936592\n",
      "Epoch 4867, Loss: 0.1378316804766655, Final Batch Loss: 0.08379941433668137\n",
      "Epoch 4868, Loss: 0.09168903902173042, Final Batch Loss: 0.06809131056070328\n",
      "Epoch 4869, Loss: 0.02285377774387598, Final Batch Loss: 0.009547820314764977\n",
      "Epoch 4870, Loss: 0.05857127904891968, Final Batch Loss: 0.04379037022590637\n",
      "Epoch 4871, Loss: 0.13336940109729767, Final Batch Loss: 0.11461709439754486\n",
      "Epoch 4872, Loss: 0.025424906983971596, Final Batch Loss: 0.009988821111619473\n",
      "Epoch 4873, Loss: 0.01965441508218646, Final Batch Loss: 0.013264334760606289\n",
      "Epoch 4874, Loss: 0.050317609682679176, Final Batch Loss: 0.037451520562171936\n",
      "Epoch 4875, Loss: 0.06628307141363621, Final Batch Loss: 0.025333600118756294\n",
      "Epoch 4876, Loss: 0.2851179428398609, Final Batch Loss: 0.2601366341114044\n",
      "Epoch 4877, Loss: 0.037461401894688606, Final Batch Loss: 0.01849242113530636\n",
      "Epoch 4878, Loss: 0.022072432097047567, Final Batch Loss: 0.003721491899341345\n",
      "Epoch 4879, Loss: 0.05407451093196869, Final Batch Loss: 0.02016153559088707\n",
      "Epoch 4880, Loss: 0.0347320232540369, Final Batch Loss: 0.02271859161555767\n",
      "Epoch 4881, Loss: 0.01716406224295497, Final Batch Loss: 0.005540459882467985\n",
      "Epoch 4882, Loss: 0.03458399698138237, Final Batch Loss: 0.01408831775188446\n",
      "Epoch 4883, Loss: 0.037975599989295006, Final Batch Loss: 0.018397007137537003\n",
      "Epoch 4884, Loss: 0.0414916668087244, Final Batch Loss: 0.013211717829108238\n",
      "Epoch 4885, Loss: 0.08860655874013901, Final Batch Loss: 0.0024384409189224243\n",
      "Epoch 4886, Loss: 0.1035541258752346, Final Batch Loss: 0.07227695733308792\n",
      "Epoch 4887, Loss: 0.04541843570768833, Final Batch Loss: 0.008115546777844429\n",
      "Epoch 4888, Loss: 0.049225348979234695, Final Batch Loss: 0.030302412807941437\n",
      "Epoch 4889, Loss: 0.04971174895763397, Final Batch Loss: 0.0319279246032238\n",
      "Epoch 4890, Loss: 0.01636434649117291, Final Batch Loss: 0.00390215334482491\n",
      "Epoch 4891, Loss: 0.016170676331967115, Final Batch Loss: 0.006965166423469782\n",
      "Epoch 4892, Loss: 0.011753849918022752, Final Batch Loss: 0.0023550980258733034\n",
      "Epoch 4893, Loss: 0.05464896745979786, Final Batch Loss: 0.03913969546556473\n",
      "Epoch 4894, Loss: 0.028535638004541397, Final Batch Loss: 0.009755771607160568\n",
      "Epoch 4895, Loss: 0.12134966626763344, Final Batch Loss: 0.09520645439624786\n",
      "Epoch 4896, Loss: 0.026771578937768936, Final Batch Loss: 0.011463665403425694\n",
      "Epoch 4897, Loss: 0.024953750427812338, Final Batch Loss: 0.00502368388697505\n",
      "Epoch 4898, Loss: 0.0647369772195816, Final Batch Loss: 0.024106282740831375\n",
      "Epoch 4899, Loss: 0.17210886254906654, Final Batch Loss: 0.13160160183906555\n",
      "Epoch 4900, Loss: 0.03938534762710333, Final Batch Loss: 0.008544147945940495\n",
      "Epoch 4901, Loss: 0.04165206849575043, Final Batch Loss: 0.0323844775557518\n",
      "Epoch 4902, Loss: 0.043452313169837, Final Batch Loss: 0.01102006621658802\n",
      "Epoch 4903, Loss: 0.02563839592039585, Final Batch Loss: 0.010284621268510818\n",
      "Epoch 4904, Loss: 0.05231789406388998, Final Batch Loss: 0.006714570336043835\n",
      "Epoch 4905, Loss: 0.053241999819874763, Final Batch Loss: 0.016614390537142754\n",
      "Epoch 4906, Loss: 0.03399126697331667, Final Batch Loss: 0.006779228337109089\n",
      "Epoch 4907, Loss: 0.025284224655479193, Final Batch Loss: 0.0072772628627717495\n",
      "Epoch 4908, Loss: 0.03888322040438652, Final Batch Loss: 0.026745961979031563\n",
      "Epoch 4909, Loss: 0.09899998269975185, Final Batch Loss: 0.08191873133182526\n",
      "Epoch 4910, Loss: 0.02589974459260702, Final Batch Loss: 0.00537541788071394\n",
      "Epoch 4911, Loss: 0.026377858128398657, Final Batch Loss: 0.005774709861725569\n",
      "Epoch 4912, Loss: 0.03570606280118227, Final Batch Loss: 0.024954842403531075\n",
      "Epoch 4913, Loss: 0.06780130416154861, Final Batch Loss: 0.05057321861386299\n",
      "Epoch 4914, Loss: 0.023082262137904763, Final Batch Loss: 0.003689451375976205\n",
      "Epoch 4915, Loss: 0.024699400179088116, Final Batch Loss: 0.008527523837983608\n",
      "Epoch 4916, Loss: 0.0856202058494091, Final Batch Loss: 0.06624939292669296\n",
      "Epoch 4917, Loss: 0.03885550983250141, Final Batch Loss: 0.01773759536445141\n",
      "Epoch 4918, Loss: 0.04631930682808161, Final Batch Loss: 0.0073552923277020454\n",
      "Epoch 4919, Loss: 0.03256507217884064, Final Batch Loss: 0.01677447184920311\n",
      "Epoch 4920, Loss: 0.06804709881544113, Final Batch Loss: 0.012049607932567596\n",
      "Epoch 4921, Loss: 0.05156788369640708, Final Batch Loss: 0.006547981407493353\n",
      "Epoch 4922, Loss: 0.07553325220942497, Final Batch Loss: 0.027240782976150513\n",
      "Epoch 4923, Loss: 0.07260800339281559, Final Batch Loss: 0.053978923708200455\n",
      "Epoch 4924, Loss: 0.07874015159904957, Final Batch Loss: 0.053873565047979355\n",
      "Epoch 4925, Loss: 0.12888611108064651, Final Batch Loss: 0.04114297032356262\n",
      "Epoch 4926, Loss: 0.12003708258271217, Final Batch Loss: 0.098138228058815\n",
      "Epoch 4927, Loss: 0.027851007413119078, Final Batch Loss: 0.0029262355528771877\n",
      "Epoch 4928, Loss: 0.03326249960809946, Final Batch Loss: 0.008014998398721218\n",
      "Epoch 4929, Loss: 0.051607511937618256, Final Batch Loss: 0.016075342893600464\n",
      "Epoch 4930, Loss: 0.06322874687612057, Final Batch Loss: 0.03654191270470619\n",
      "Epoch 4931, Loss: 0.034544115886092186, Final Batch Loss: 0.017333395779132843\n",
      "Epoch 4932, Loss: 0.04071027413010597, Final Batch Loss: 0.018630532547831535\n",
      "Epoch 4933, Loss: 0.11399549804627895, Final Batch Loss: 0.02157737873494625\n",
      "Epoch 4934, Loss: 0.02816907875239849, Final Batch Loss: 0.012909560464322567\n",
      "Epoch 4935, Loss: 0.045694345608353615, Final Batch Loss: 0.0216861329972744\n",
      "Epoch 4936, Loss: 0.07019990868866444, Final Batch Loss: 0.04189091920852661\n",
      "Epoch 4937, Loss: 0.0760854035615921, Final Batch Loss: 0.04397883638739586\n",
      "Epoch 4938, Loss: 0.06914401985704899, Final Batch Loss: 0.0118910763412714\n",
      "Epoch 4939, Loss: 0.029530368745326996, Final Batch Loss: 0.015504158101975918\n",
      "Epoch 4940, Loss: 0.019573996774852276, Final Batch Loss: 0.011294221505522728\n",
      "Epoch 4941, Loss: 0.03178935497999191, Final Batch Loss: 0.00960138812661171\n",
      "Epoch 4942, Loss: 0.03633787343278527, Final Batch Loss: 0.005647408310323954\n",
      "Epoch 4943, Loss: 0.04702213406562805, Final Batch Loss: 0.023005684837698936\n",
      "Epoch 4944, Loss: 0.06932635232806206, Final Batch Loss: 0.03459259122610092\n",
      "Epoch 4945, Loss: 0.036438736598938704, Final Batch Loss: 0.0069149392656981945\n",
      "Epoch 4946, Loss: 0.026900889351963997, Final Batch Loss: 0.010058589279651642\n",
      "Epoch 4947, Loss: 0.022962419781833887, Final Batch Loss: 0.004060959909111261\n",
      "Epoch 4948, Loss: 0.035081354435533285, Final Batch Loss: 0.004440743010491133\n",
      "Epoch 4949, Loss: 0.030544565990567207, Final Batch Loss: 0.007150309160351753\n",
      "Epoch 4950, Loss: 0.03817731887102127, Final Batch Loss: 0.015364224091172218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4951, Loss: 0.024468901567161083, Final Batch Loss: 0.013794058002531528\n",
      "Epoch 4952, Loss: 0.025354809127748013, Final Batch Loss: 0.010884663090109825\n",
      "Epoch 4953, Loss: 0.04976326413452625, Final Batch Loss: 0.016536524519324303\n",
      "Epoch 4954, Loss: 0.04089111275970936, Final Batch Loss: 0.02004464529454708\n",
      "Epoch 4955, Loss: 0.021038567647337914, Final Batch Loss: 0.012227213010191917\n",
      "Epoch 4956, Loss: 0.061504163313657045, Final Batch Loss: 0.05406690761446953\n",
      "Epoch 4957, Loss: 0.023931321687996387, Final Batch Loss: 0.008213036693632603\n",
      "Epoch 4958, Loss: 0.06736332178115845, Final Batch Loss: 0.058160267770290375\n",
      "Epoch 4959, Loss: 0.03411849564872682, Final Batch Loss: 0.0027697307523339987\n",
      "Epoch 4960, Loss: 0.025328420335426927, Final Batch Loss: 0.002936318749561906\n",
      "Epoch 4961, Loss: 0.0910960454493761, Final Batch Loss: 0.08091916143894196\n",
      "Epoch 4962, Loss: 0.03879686817526817, Final Batch Loss: 0.02848508022725582\n",
      "Epoch 4963, Loss: 0.014544368255883455, Final Batch Loss: 0.00399794103577733\n",
      "Epoch 4964, Loss: 0.05256442353129387, Final Batch Loss: 0.013146679848432541\n",
      "Epoch 4965, Loss: 0.051237622275948524, Final Batch Loss: 0.035174500197172165\n",
      "Epoch 4966, Loss: 0.021739297546446323, Final Batch Loss: 0.012580031529068947\n",
      "Epoch 4967, Loss: 0.014350981451570988, Final Batch Loss: 0.006561279296875\n",
      "Epoch 4968, Loss: 0.12591827474534512, Final Batch Loss: 0.10458024591207504\n",
      "Epoch 4969, Loss: 0.025566657073795795, Final Batch Loss: 0.00790808629244566\n",
      "Epoch 4970, Loss: 0.022124251816421747, Final Batch Loss: 0.005105479154735804\n",
      "Epoch 4971, Loss: 0.03602240141481161, Final Batch Loss: 0.028476696461439133\n",
      "Epoch 4972, Loss: 0.04208984971046448, Final Batch Loss: 0.02157151699066162\n",
      "Epoch 4973, Loss: 0.03456126665696502, Final Batch Loss: 0.005867793690413237\n",
      "Epoch 4974, Loss: 0.02063122484833002, Final Batch Loss: 0.009406011551618576\n",
      "Epoch 4975, Loss: 0.017911371309310198, Final Batch Loss: 0.007408586796373129\n",
      "Epoch 4976, Loss: 0.04063405259512365, Final Batch Loss: 0.0038904750254005194\n",
      "Epoch 4977, Loss: 0.010051942430436611, Final Batch Loss: 0.004681618418544531\n",
      "Epoch 4978, Loss: 0.023044098168611526, Final Batch Loss: 0.010254846885800362\n",
      "Epoch 4979, Loss: 0.08113920129835606, Final Batch Loss: 0.05935502424836159\n",
      "Epoch 4980, Loss: 0.036032565869390965, Final Batch Loss: 0.027397090569138527\n",
      "Epoch 4981, Loss: 0.029922785237431526, Final Batch Loss: 0.022044390439987183\n",
      "Epoch 4982, Loss: 0.02129262313246727, Final Batch Loss: 0.004098175093531609\n",
      "Epoch 4983, Loss: 0.03683531191200018, Final Batch Loss: 0.024429678916931152\n",
      "Epoch 4984, Loss: 0.06560139544308186, Final Batch Loss: 0.03679224103689194\n",
      "Epoch 4985, Loss: 0.025619668420404196, Final Batch Loss: 0.018559817224740982\n",
      "Epoch 4986, Loss: 0.02281256765127182, Final Batch Loss: 0.010515810921788216\n",
      "Epoch 4987, Loss: 0.05019475892186165, Final Batch Loss: 0.014538019895553589\n",
      "Epoch 4988, Loss: 0.03652029484510422, Final Batch Loss: 0.0045265331864356995\n",
      "Epoch 4989, Loss: 0.05629546893760562, Final Batch Loss: 0.005132544320076704\n",
      "Epoch 4990, Loss: 0.03259759861975908, Final Batch Loss: 0.018674694001674652\n",
      "Epoch 4991, Loss: 0.03246297687292099, Final Batch Loss: 0.012263089418411255\n",
      "Epoch 4992, Loss: 0.014501032419502735, Final Batch Loss: 0.006534253247082233\n",
      "Epoch 4993, Loss: 0.028201733715832233, Final Batch Loss: 0.0034500816836953163\n",
      "Epoch 4994, Loss: 0.04783694352954626, Final Batch Loss: 0.005835673771798611\n",
      "Epoch 4995, Loss: 0.03766830684617162, Final Batch Loss: 0.00642668129876256\n",
      "Epoch 4996, Loss: 0.024650673381984234, Final Batch Loss: 0.01444722805172205\n",
      "Epoch 4997, Loss: 0.07280628383159637, Final Batch Loss: 0.05839843675494194\n",
      "Epoch 4998, Loss: 0.02844502916559577, Final Batch Loss: 0.0072770616970956326\n",
      "Epoch 4999, Loss: 0.03556772507727146, Final Batch Loss: 0.026293423026800156\n",
      "Epoch 5000, Loss: 0.02438660617917776, Final Batch Loss: 0.016383269801735878\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  1  0  0  0  0  0  0  0]\n",
      " [ 0 12  0  0  1  0  0  0  0]\n",
      " [ 0  0  8  0  0  0  0  0  1]\n",
      " [ 0  0  0 14  0  0  0  0  0]\n",
      " [ 0  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  1  0  0  3  0  0  8]\n",
      " [ 1  0  0  0  0  1 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0]\n",
      " [ 0  0  0  0  0  1  0  0  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.944     0.944     0.944        18\n",
      "           1      0.923     0.923     0.923        13\n",
      "           2      0.889     0.889     0.889         9\n",
      "           3      1.000     1.000     1.000        14\n",
      "           4      0.923     1.000     0.960        12\n",
      "           5      0.600     0.250     0.353        12\n",
      "           6      1.000     0.833     0.909        12\n",
      "           7      1.000     1.000     1.000         6\n",
      "           8      0.250     0.750     0.375         4\n",
      "\n",
      "    accuracy                          0.850       100\n",
      "   macro avg      0.837     0.843     0.817       100\n",
      "weighted avg      0.883     0.850     0.852       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
