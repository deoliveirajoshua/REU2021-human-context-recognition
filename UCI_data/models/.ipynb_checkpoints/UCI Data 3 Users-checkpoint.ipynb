{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 tBodyAcc-mean()-X</th>\n",
       "      <th>2 tBodyAcc-mean()-Y</th>\n",
       "      <th>3 tBodyAcc-mean()-Z</th>\n",
       "      <th>4 tBodyAcc-std()-X</th>\n",
       "      <th>5 tBodyAcc-std()-Y</th>\n",
       "      <th>6 tBodyAcc-std()-Z</th>\n",
       "      <th>7 tBodyAcc-mad()-X</th>\n",
       "      <th>8 tBodyAcc-mad()-Y</th>\n",
       "      <th>9 tBodyAcc-mad()-Z</th>\n",
       "      <th>10 tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>32 tBodyAcc-arCoeff()-Y,3</th>\n",
       "      <th>33 tBodyAcc-arCoeff()-Y,4</th>\n",
       "      <th>34 tBodyAcc-arCoeff()-Z,1</th>\n",
       "      <th>35 tBodyAcc-arCoeff()-Z,2</th>\n",
       "      <th>36 tBodyAcc-arCoeff()-Z,3</th>\n",
       "      <th>37 tBodyAcc-arCoeff()-Z,4</th>\n",
       "      <th>38 tBodyAcc-correlation()-X,Y</th>\n",
       "      <th>39 tBodyAcc-correlation()-X,Z</th>\n",
       "      <th>40 tBodyAcc-correlation()-Y,Z</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264106</td>\n",
       "      <td>-0.095246</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>-0.465085</td>\n",
       "      <td>0.491936</td>\n",
       "      <td>-0.190884</td>\n",
       "      <td>0.376314</td>\n",
       "      <td>0.435129</td>\n",
       "      <td>0.660790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294310</td>\n",
       "      <td>-0.281211</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>-0.022153</td>\n",
       "      <td>-0.016657</td>\n",
       "      <td>-0.220643</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>-0.072692</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342256</td>\n",
       "      <td>-0.332564</td>\n",
       "      <td>0.239281</td>\n",
       "      <td>-0.136204</td>\n",
       "      <td>0.173863</td>\n",
       "      <td>-0.299493</td>\n",
       "      <td>-0.124698</td>\n",
       "      <td>-0.181105</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323154</td>\n",
       "      <td>-0.170813</td>\n",
       "      <td>0.294938</td>\n",
       "      <td>-0.306081</td>\n",
       "      <td>0.482148</td>\n",
       "      <td>-0.470129</td>\n",
       "      <td>-0.305693</td>\n",
       "      <td>-0.362654</td>\n",
       "      <td>0.507459</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434728</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>-0.269069</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>-0.088952</td>\n",
       "      <td>-0.155804</td>\n",
       "      <td>-0.189763</td>\n",
       "      <td>0.599213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>0.299665</td>\n",
       "      <td>-0.057193</td>\n",
       "      <td>-0.181233</td>\n",
       "      <td>-0.195387</td>\n",
       "      <td>0.039905</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>-0.282301</td>\n",
       "      <td>0.043616</td>\n",
       "      <td>0.060410</td>\n",
       "      <td>0.210795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119821</td>\n",
       "      <td>0.293112</td>\n",
       "      <td>-0.425386</td>\n",
       "      <td>0.267986</td>\n",
       "      <td>-0.205315</td>\n",
       "      <td>0.142117</td>\n",
       "      <td>-0.211822</td>\n",
       "      <td>-0.251582</td>\n",
       "      <td>-0.283335</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>0.273853</td>\n",
       "      <td>-0.007749</td>\n",
       "      <td>-0.147468</td>\n",
       "      <td>-0.235309</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.059280</td>\n",
       "      <td>-0.322552</td>\n",
       "      <td>-0.029456</td>\n",
       "      <td>0.080585</td>\n",
       "      <td>0.117440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034260</td>\n",
       "      <td>0.239835</td>\n",
       "      <td>-0.364480</td>\n",
       "      <td>0.121335</td>\n",
       "      <td>0.188717</td>\n",
       "      <td>-0.207505</td>\n",
       "      <td>-0.198555</td>\n",
       "      <td>-0.225866</td>\n",
       "      <td>-0.274504</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>0.273387</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>-0.045022</td>\n",
       "      <td>-0.218218</td>\n",
       "      <td>-0.103822</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>-0.304515</td>\n",
       "      <td>-0.098913</td>\n",
       "      <td>0.332584</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119962</td>\n",
       "      <td>0.080689</td>\n",
       "      <td>-0.420093</td>\n",
       "      <td>0.197763</td>\n",
       "      <td>-0.033780</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>-0.226826</td>\n",
       "      <td>-0.184700</td>\n",
       "      <td>-0.198452</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>0.289654</td>\n",
       "      <td>-0.018843</td>\n",
       "      <td>-0.158281</td>\n",
       "      <td>-0.219139</td>\n",
       "      <td>-0.111412</td>\n",
       "      <td>0.268893</td>\n",
       "      <td>-0.310487</td>\n",
       "      <td>-0.068200</td>\n",
       "      <td>0.319473</td>\n",
       "      <td>0.101702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101761</td>\n",
       "      <td>-0.108375</td>\n",
       "      <td>-0.438356</td>\n",
       "      <td>0.250837</td>\n",
       "      <td>-0.234309</td>\n",
       "      <td>0.232444</td>\n",
       "      <td>-0.257775</td>\n",
       "      <td>-0.231103</td>\n",
       "      <td>-0.189915</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>0.351503</td>\n",
       "      <td>-0.012423</td>\n",
       "      <td>-0.203867</td>\n",
       "      <td>-0.269270</td>\n",
       "      <td>-0.087212</td>\n",
       "      <td>0.177404</td>\n",
       "      <td>-0.377404</td>\n",
       "      <td>-0.038678</td>\n",
       "      <td>0.229430</td>\n",
       "      <td>0.269013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156435</td>\n",
       "      <td>0.097870</td>\n",
       "      <td>-0.405691</td>\n",
       "      <td>0.183340</td>\n",
       "      <td>-0.056556</td>\n",
       "      <td>0.054368</td>\n",
       "      <td>-0.266442</td>\n",
       "      <td>-0.291113</td>\n",
       "      <td>-0.200293</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
       "0                0.288585            -0.020294            -0.132905   \n",
       "1                0.278419            -0.016411            -0.123520   \n",
       "2                0.279653            -0.019467            -0.113462   \n",
       "3                0.279174            -0.026201            -0.123283   \n",
       "4                0.276629            -0.016570            -0.115362   \n",
       "...                   ...                  ...                  ...   \n",
       "7347             0.299665            -0.057193            -0.181233   \n",
       "7348             0.273853            -0.007749            -0.147468   \n",
       "7349             0.273387            -0.017011            -0.045022   \n",
       "7350             0.289654            -0.018843            -0.158281   \n",
       "7351             0.351503            -0.012423            -0.203867   \n",
       "\n",
       "      4 tBodyAcc-std()-X  5 tBodyAcc-std()-Y  6 tBodyAcc-std()-Z  \\\n",
       "0              -0.995279           -0.983111           -0.913526   \n",
       "1              -0.998245           -0.975300           -0.960322   \n",
       "2              -0.995380           -0.967187           -0.978944   \n",
       "3              -0.996091           -0.983403           -0.990675   \n",
       "4              -0.998139           -0.980817           -0.990482   \n",
       "...                  ...                 ...                 ...   \n",
       "7347           -0.195387            0.039905            0.077078   \n",
       "7348           -0.235309            0.004816            0.059280   \n",
       "7349           -0.218218           -0.103822            0.274533   \n",
       "7350           -0.219139           -0.111412            0.268893   \n",
       "7351           -0.269270           -0.087212            0.177404   \n",
       "\n",
       "      7 tBodyAcc-mad()-X  8 tBodyAcc-mad()-Y  9 tBodyAcc-mad()-Z  \\\n",
       "0              -0.995112           -0.983185           -0.923527   \n",
       "1              -0.998807           -0.974914           -0.957686   \n",
       "2              -0.996520           -0.963668           -0.977469   \n",
       "3              -0.997099           -0.982750           -0.989302   \n",
       "4              -0.998321           -0.979672           -0.990441   \n",
       "...                  ...                 ...                 ...   \n",
       "7347           -0.282301            0.043616            0.060410   \n",
       "7348           -0.322552           -0.029456            0.080585   \n",
       "7349           -0.304515           -0.098913            0.332584   \n",
       "7350           -0.310487           -0.068200            0.319473   \n",
       "7351           -0.377404           -0.038678            0.229430   \n",
       "\n",
       "      10 tBodyAcc-max()-X  ...  32 tBodyAcc-arCoeff()-Y,3  \\\n",
       "0               -0.934724  ...                   0.264106   \n",
       "1               -0.943068  ...                   0.294310   \n",
       "2               -0.938692  ...                   0.342256   \n",
       "3               -0.938692  ...                   0.323154   \n",
       "4               -0.942469  ...                   0.434728   \n",
       "...                   ...  ...                        ...   \n",
       "7347             0.210795  ...                  -0.119821   \n",
       "7348             0.117440  ...                   0.034260   \n",
       "7349             0.043999  ...                   0.119962   \n",
       "7350             0.101702  ...                   0.101761   \n",
       "7351             0.269013  ...                  -0.156435   \n",
       "\n",
       "      33 tBodyAcc-arCoeff()-Y,4  34 tBodyAcc-arCoeff()-Z,1  \\\n",
       "0                     -0.095246                   0.278851   \n",
       "1                     -0.281211                   0.085988   \n",
       "2                     -0.332564                   0.239281   \n",
       "3                     -0.170813                   0.294938   \n",
       "4                     -0.315375                   0.439744   \n",
       "...                         ...                        ...   \n",
       "7347                   0.293112                  -0.425386   \n",
       "7348                   0.239835                  -0.364480   \n",
       "7349                   0.080689                  -0.420093   \n",
       "7350                  -0.108375                  -0.438356   \n",
       "7351                   0.097870                  -0.405691   \n",
       "\n",
       "      35 tBodyAcc-arCoeff()-Z,2  36 tBodyAcc-arCoeff()-Z,3  \\\n",
       "0                     -0.465085                   0.491936   \n",
       "1                     -0.022153                  -0.016657   \n",
       "2                     -0.136204                   0.173863   \n",
       "3                     -0.306081                   0.482148   \n",
       "4                     -0.269069                   0.179414   \n",
       "...                         ...                        ...   \n",
       "7347                   0.267986                  -0.205315   \n",
       "7348                   0.121335                   0.188717   \n",
       "7349                   0.197763                  -0.033780   \n",
       "7350                   0.250837                  -0.234309   \n",
       "7351                   0.183340                  -0.056556   \n",
       "\n",
       "      37 tBodyAcc-arCoeff()-Z,4  38 tBodyAcc-correlation()-X,Y  \\\n",
       "0                     -0.190884                       0.376314   \n",
       "1                     -0.220643                      -0.013429   \n",
       "2                     -0.299493                      -0.124698   \n",
       "3                     -0.470129                      -0.305693   \n",
       "4                     -0.088952                      -0.155804   \n",
       "...                         ...                            ...   \n",
       "7347                   0.142117                      -0.211822   \n",
       "7348                  -0.207505                      -0.198555   \n",
       "7349                   0.016677                      -0.226826   \n",
       "7350                   0.232444                      -0.257775   \n",
       "7351                   0.054368                      -0.266442   \n",
       "\n",
       "      39 tBodyAcc-correlation()-X,Z  40 tBodyAcc-correlation()-Y,Z  Subject  \n",
       "0                          0.435129                       0.660790        1  \n",
       "1                         -0.072692                       0.579382        1  \n",
       "2                         -0.181105                       0.608900        1  \n",
       "3                         -0.362654                       0.507459        1  \n",
       "4                         -0.189763                       0.599213        1  \n",
       "...                             ...                            ...      ...  \n",
       "7347                      -0.251582                      -0.283335       30  \n",
       "7348                      -0.225866                      -0.274504       30  \n",
       "7349                      -0.184700                      -0.198452       30  \n",
       "7350                      -0.231103                      -0.189915       30  \n",
       "7351                      -0.291113                      -0.200293       30  \n",
       "\n",
       "[7352 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "X_train = train_data.iloc[:,:40]\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[(X_train['Subject'] == 1) | (X_train['Subject'] == 3) | (X_train['Subject'] == 5)]\n",
    "X_train = X_train.iloc[:,:-1].values\n",
    "\n",
    "y_train = y_train[(y_train['Subject'] == 1) | (y_train['Subject'] == 3) | (y_train['Subject'] == 5)]\n",
    "y_train = y_train.values\n",
    "y_train = y_train.flatten()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    else:\n",
    "        y_train[k] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 40):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 8000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.423388361930847, Final Batch Loss: 1.106520652770996\n",
      "Epoch 2, Loss: 4.414681434631348, Final Batch Loss: 1.1072912216186523\n",
      "Epoch 3, Loss: 4.400613307952881, Final Batch Loss: 1.098887324333191\n",
      "Epoch 4, Loss: 4.399634122848511, Final Batch Loss: 1.0974323749542236\n",
      "Epoch 5, Loss: 4.391550421714783, Final Batch Loss: 1.097523808479309\n",
      "Epoch 6, Loss: 4.382089614868164, Final Batch Loss: 1.0965602397918701\n",
      "Epoch 7, Loss: 4.381512403488159, Final Batch Loss: 1.0955828428268433\n",
      "Epoch 8, Loss: 4.373029828071594, Final Batch Loss: 1.0974196195602417\n",
      "Epoch 9, Loss: 4.376599192619324, Final Batch Loss: 1.1110889911651611\n",
      "Epoch 10, Loss: 4.368778824806213, Final Batch Loss: 1.1046879291534424\n",
      "Epoch 11, Loss: 4.345440149307251, Final Batch Loss: 1.0903809070587158\n",
      "Epoch 12, Loss: 4.345860123634338, Final Batch Loss: 1.0926845073699951\n",
      "Epoch 13, Loss: 4.32937479019165, Final Batch Loss: 1.0784826278686523\n",
      "Epoch 14, Loss: 4.320988297462463, Final Batch Loss: 1.0822926759719849\n",
      "Epoch 15, Loss: 4.299968719482422, Final Batch Loss: 1.0628103017807007\n",
      "Epoch 16, Loss: 4.282374143600464, Final Batch Loss: 1.0633410215377808\n",
      "Epoch 17, Loss: 4.258930802345276, Final Batch Loss: 1.0653858184814453\n",
      "Epoch 18, Loss: 4.218104958534241, Final Batch Loss: 1.0405497550964355\n",
      "Epoch 19, Loss: 4.19374680519104, Final Batch Loss: 1.0595532655715942\n",
      "Epoch 20, Loss: 4.17369818687439, Final Batch Loss: 1.0566182136535645\n",
      "Epoch 21, Loss: 4.111340880393982, Final Batch Loss: 1.0056947469711304\n",
      "Epoch 22, Loss: 4.0531200766563416, Final Batch Loss: 0.9993727803230286\n",
      "Epoch 23, Loss: 3.97249174118042, Final Batch Loss: 0.974263608455658\n",
      "Epoch 24, Loss: 3.9107097387313843, Final Batch Loss: 0.9865244626998901\n",
      "Epoch 25, Loss: 3.854931890964508, Final Batch Loss: 0.949626624584198\n",
      "Epoch 26, Loss: 3.764819860458374, Final Batch Loss: 0.9569069147109985\n",
      "Epoch 27, Loss: 3.6614970564842224, Final Batch Loss: 0.8799206018447876\n",
      "Epoch 28, Loss: 3.620831251144409, Final Batch Loss: 0.909690260887146\n",
      "Epoch 29, Loss: 3.55399090051651, Final Batch Loss: 0.8766680955886841\n",
      "Epoch 30, Loss: 3.492013692855835, Final Batch Loss: 0.8821543455123901\n",
      "Epoch 31, Loss: 3.4095155000686646, Final Batch Loss: 0.8473283648490906\n",
      "Epoch 32, Loss: 3.381077289581299, Final Batch Loss: 0.8167382478713989\n",
      "Epoch 33, Loss: 3.3130943179130554, Final Batch Loss: 0.7891395688056946\n",
      "Epoch 34, Loss: 3.283220589160919, Final Batch Loss: 0.8562825918197632\n",
      "Epoch 35, Loss: 3.157568037509918, Final Batch Loss: 0.7465636134147644\n",
      "Epoch 36, Loss: 3.1160524487495422, Final Batch Loss: 0.7910997867584229\n",
      "Epoch 37, Loss: 3.0920233130455017, Final Batch Loss: 0.7100304961204529\n",
      "Epoch 38, Loss: 3.0064048171043396, Final Batch Loss: 0.7122487425804138\n",
      "Epoch 39, Loss: 2.9819000959396362, Final Batch Loss: 0.7259272336959839\n",
      "Epoch 40, Loss: 2.923222303390503, Final Batch Loss: 0.726646363735199\n",
      "Epoch 41, Loss: 2.9142737984657288, Final Batch Loss: 0.7515427470207214\n",
      "Epoch 42, Loss: 2.9260868430137634, Final Batch Loss: 0.7419933080673218\n",
      "Epoch 43, Loss: 2.7397202849388123, Final Batch Loss: 0.6368522644042969\n",
      "Epoch 44, Loss: 2.7199482917785645, Final Batch Loss: 0.6421912908554077\n",
      "Epoch 45, Loss: 2.7373872995376587, Final Batch Loss: 0.7388495206832886\n",
      "Epoch 46, Loss: 2.7219645380973816, Final Batch Loss: 0.7057130336761475\n",
      "Epoch 47, Loss: 2.737418830394745, Final Batch Loss: 0.7158056497573853\n",
      "Epoch 48, Loss: 2.5199954509735107, Final Batch Loss: 0.5790883302688599\n",
      "Epoch 49, Loss: 2.640527129173279, Final Batch Loss: 0.7218829989433289\n",
      "Epoch 50, Loss: 2.566817045211792, Final Batch Loss: 0.7107096910476685\n",
      "Epoch 51, Loss: 2.5498926043510437, Final Batch Loss: 0.6419381499290466\n",
      "Epoch 52, Loss: 2.444738209247589, Final Batch Loss: 0.5207301378250122\n",
      "Epoch 53, Loss: 2.4581586122512817, Final Batch Loss: 0.7239142060279846\n",
      "Epoch 54, Loss: 2.444076418876648, Final Batch Loss: 0.5999181270599365\n",
      "Epoch 55, Loss: 2.413660228252411, Final Batch Loss: 0.6435922384262085\n",
      "Epoch 56, Loss: 2.3385008573532104, Final Batch Loss: 0.6223212480545044\n",
      "Epoch 57, Loss: 2.4083224534988403, Final Batch Loss: 0.5787792205810547\n",
      "Epoch 58, Loss: 2.3277747631073, Final Batch Loss: 0.5663821697235107\n",
      "Epoch 59, Loss: 2.3762890696525574, Final Batch Loss: 0.6395403742790222\n",
      "Epoch 60, Loss: 2.3266093730926514, Final Batch Loss: 0.6166650056838989\n",
      "Epoch 61, Loss: 2.2215298414230347, Final Batch Loss: 0.5248008966445923\n",
      "Epoch 62, Loss: 2.173778772354126, Final Batch Loss: 0.4702460765838623\n",
      "Epoch 63, Loss: 2.177213490009308, Final Batch Loss: 0.6188074946403503\n",
      "Epoch 64, Loss: 2.103013277053833, Final Batch Loss: 0.5011320114135742\n",
      "Epoch 65, Loss: 2.238980233669281, Final Batch Loss: 0.5656256675720215\n",
      "Epoch 66, Loss: 2.213007688522339, Final Batch Loss: 0.6001051664352417\n",
      "Epoch 67, Loss: 2.033038228750229, Final Batch Loss: 0.4294153153896332\n",
      "Epoch 68, Loss: 2.138502836227417, Final Batch Loss: 0.4592008888721466\n",
      "Epoch 69, Loss: 2.1376722157001495, Final Batch Loss: 0.6307318806648254\n",
      "Epoch 70, Loss: 2.151344656944275, Final Batch Loss: 0.5579390525817871\n",
      "Epoch 71, Loss: 2.0563376247882843, Final Batch Loss: 0.49643760919570923\n",
      "Epoch 72, Loss: 1.9899287223815918, Final Batch Loss: 0.4167449474334717\n",
      "Epoch 73, Loss: 2.093298375606537, Final Batch Loss: 0.5704213976860046\n",
      "Epoch 74, Loss: 2.0162360966205597, Final Batch Loss: 0.47695982456207275\n",
      "Epoch 75, Loss: 2.05023792386055, Final Batch Loss: 0.4761970341205597\n",
      "Epoch 76, Loss: 1.9399626851081848, Final Batch Loss: 0.3975805640220642\n",
      "Epoch 77, Loss: 1.9679112136363983, Final Batch Loss: 0.48772236704826355\n",
      "Epoch 78, Loss: 2.023462265729904, Final Batch Loss: 0.4923507273197174\n",
      "Epoch 79, Loss: 2.0253734588623047, Final Batch Loss: 0.5580963492393494\n",
      "Epoch 80, Loss: 1.9712045788764954, Final Batch Loss: 0.5220383405685425\n",
      "Epoch 81, Loss: 1.9826635122299194, Final Batch Loss: 0.5043530464172363\n",
      "Epoch 82, Loss: 1.8894506096839905, Final Batch Loss: 0.3884868323802948\n",
      "Epoch 83, Loss: 1.9453156292438507, Final Batch Loss: 0.48181843757629395\n",
      "Epoch 84, Loss: 1.853869616985321, Final Batch Loss: 0.41465264558792114\n",
      "Epoch 85, Loss: 1.8820880949497223, Final Batch Loss: 0.4392978250980377\n",
      "Epoch 86, Loss: 1.859739601612091, Final Batch Loss: 0.4206264913082123\n",
      "Epoch 87, Loss: 1.878608912229538, Final Batch Loss: 0.49363675713539124\n",
      "Epoch 88, Loss: 1.8790148198604584, Final Batch Loss: 0.47689077258110046\n",
      "Epoch 89, Loss: 1.8199626207351685, Final Batch Loss: 0.46197953820228577\n",
      "Epoch 90, Loss: 1.8498774468898773, Final Batch Loss: 0.43618953227996826\n",
      "Epoch 91, Loss: 1.7697319686412811, Final Batch Loss: 0.38957977294921875\n",
      "Epoch 92, Loss: 1.907359927892685, Final Batch Loss: 0.5347134470939636\n",
      "Epoch 93, Loss: 1.7949010133743286, Final Batch Loss: 0.40004977583885193\n",
      "Epoch 94, Loss: 1.8978974521160126, Final Batch Loss: 0.5169788599014282\n",
      "Epoch 95, Loss: 1.7746164500713348, Final Batch Loss: 0.43315833806991577\n",
      "Epoch 96, Loss: 1.8651808500289917, Final Batch Loss: 0.4771571457386017\n",
      "Epoch 97, Loss: 1.775643140077591, Final Batch Loss: 0.3959161043167114\n",
      "Epoch 98, Loss: 1.8287340998649597, Final Batch Loss: 0.4149859845638275\n",
      "Epoch 99, Loss: 1.7553223967552185, Final Batch Loss: 0.39213302731513977\n",
      "Epoch 100, Loss: 1.7638105750083923, Final Batch Loss: 0.393451064825058\n",
      "Epoch 101, Loss: 1.706511378288269, Final Batch Loss: 0.3551700711250305\n",
      "Epoch 102, Loss: 1.785611629486084, Final Batch Loss: 0.5528272986412048\n",
      "Epoch 103, Loss: 1.6783999502658844, Final Batch Loss: 0.41747090220451355\n",
      "Epoch 104, Loss: 1.6377996802330017, Final Batch Loss: 0.4103347361087799\n",
      "Epoch 105, Loss: 1.7617289125919342, Final Batch Loss: 0.4906071722507477\n",
      "Epoch 106, Loss: 1.5936031937599182, Final Batch Loss: 0.36540523171424866\n",
      "Epoch 107, Loss: 1.783617079257965, Final Batch Loss: 0.5515224933624268\n",
      "Epoch 108, Loss: 1.6676306426525116, Final Batch Loss: 0.41025757789611816\n",
      "Epoch 109, Loss: 1.6158265471458435, Final Batch Loss: 0.34022730588912964\n",
      "Epoch 110, Loss: 1.7059393227100372, Final Batch Loss: 0.4661107659339905\n",
      "Epoch 111, Loss: 1.683251678943634, Final Batch Loss: 0.46661853790283203\n",
      "Epoch 112, Loss: 1.683653861284256, Final Batch Loss: 0.5040099620819092\n",
      "Epoch 113, Loss: 1.6913814544677734, Final Batch Loss: 0.45132872462272644\n",
      "Epoch 114, Loss: 1.7316486239433289, Final Batch Loss: 0.48028793931007385\n",
      "Epoch 115, Loss: 1.634596437215805, Final Batch Loss: 0.37220942974090576\n",
      "Epoch 116, Loss: 1.595733791589737, Final Batch Loss: 0.33804938197135925\n",
      "Epoch 117, Loss: 1.6559308767318726, Final Batch Loss: 0.41091689467430115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Loss: 1.5675285756587982, Final Batch Loss: 0.33710482716560364\n",
      "Epoch 119, Loss: 1.6062824428081512, Final Batch Loss: 0.4504278898239136\n",
      "Epoch 120, Loss: 1.6113475561141968, Final Batch Loss: 0.37121543288230896\n",
      "Epoch 121, Loss: 1.5357573330402374, Final Batch Loss: 0.33077025413513184\n",
      "Epoch 122, Loss: 1.524772822856903, Final Batch Loss: 0.3650254011154175\n",
      "Epoch 123, Loss: 1.6199499070644379, Final Batch Loss: 0.35445427894592285\n",
      "Epoch 124, Loss: 1.5843006074428558, Final Batch Loss: 0.3587477505207062\n",
      "Epoch 125, Loss: 1.5827563107013702, Final Batch Loss: 0.44205939769744873\n",
      "Epoch 126, Loss: 1.6080499589443207, Final Batch Loss: 0.42940303683280945\n",
      "Epoch 127, Loss: 1.5013201832771301, Final Batch Loss: 0.4069351255893707\n",
      "Epoch 128, Loss: 1.6251308023929596, Final Batch Loss: 0.4741588532924652\n",
      "Epoch 129, Loss: 1.5754830241203308, Final Batch Loss: 0.42176541686058044\n",
      "Epoch 130, Loss: 1.5865478217601776, Final Batch Loss: 0.42679986357688904\n",
      "Epoch 131, Loss: 1.5000515580177307, Final Batch Loss: 0.35854753851890564\n",
      "Epoch 132, Loss: 1.510188639163971, Final Batch Loss: 0.3974354863166809\n",
      "Epoch 133, Loss: 1.5018233954906464, Final Batch Loss: 0.3358428180217743\n",
      "Epoch 134, Loss: 1.5991795659065247, Final Batch Loss: 0.4881249666213989\n",
      "Epoch 135, Loss: 1.5294156670570374, Final Batch Loss: 0.38230785727500916\n",
      "Epoch 136, Loss: 1.619596689939499, Final Batch Loss: 0.5768283605575562\n",
      "Epoch 137, Loss: 1.6338126957416534, Final Batch Loss: 0.4392886459827423\n",
      "Epoch 138, Loss: 1.500795692205429, Final Batch Loss: 0.42643678188323975\n",
      "Epoch 139, Loss: 1.6676665246486664, Final Batch Loss: 0.5448224544525146\n",
      "Epoch 140, Loss: 1.43487349152565, Final Batch Loss: 0.3228389024734497\n",
      "Epoch 141, Loss: 1.4461738467216492, Final Batch Loss: 0.42754021286964417\n",
      "Epoch 142, Loss: 1.4386246502399445, Final Batch Loss: 0.35271403193473816\n",
      "Epoch 143, Loss: 1.4338723123073578, Final Batch Loss: 0.34873849153518677\n",
      "Epoch 144, Loss: 1.4585539102554321, Final Batch Loss: 0.27495214343070984\n",
      "Epoch 145, Loss: 1.3467604219913483, Final Batch Loss: 0.22107461094856262\n",
      "Epoch 146, Loss: 1.49634450674057, Final Batch Loss: 0.35921511054039\n",
      "Epoch 147, Loss: 1.469583421945572, Final Batch Loss: 0.3659707009792328\n",
      "Epoch 148, Loss: 1.4666537642478943, Final Batch Loss: 0.33611881732940674\n",
      "Epoch 149, Loss: 1.4196328222751617, Final Batch Loss: 0.3725409507751465\n",
      "Epoch 150, Loss: 1.4474775195121765, Final Batch Loss: 0.38314583897590637\n",
      "Epoch 151, Loss: 1.4611437916755676, Final Batch Loss: 0.3668287992477417\n",
      "Epoch 152, Loss: 1.4916363656520844, Final Batch Loss: 0.3327208459377289\n",
      "Epoch 153, Loss: 1.4484218955039978, Final Batch Loss: 0.353058785200119\n",
      "Epoch 154, Loss: 1.422859787940979, Final Batch Loss: 0.2970450818538666\n",
      "Epoch 155, Loss: 1.3987818360328674, Final Batch Loss: 0.33908811211586\n",
      "Epoch 156, Loss: 1.3619911670684814, Final Batch Loss: 0.3150138556957245\n",
      "Epoch 157, Loss: 1.3706660568714142, Final Batch Loss: 0.2980251908302307\n",
      "Epoch 158, Loss: 1.4481897950172424, Final Batch Loss: 0.3671884536743164\n",
      "Epoch 159, Loss: 1.3968656957149506, Final Batch Loss: 0.2841391861438751\n",
      "Epoch 160, Loss: 1.3857044577598572, Final Batch Loss: 0.32922473549842834\n",
      "Epoch 161, Loss: 1.400316059589386, Final Batch Loss: 0.31485715508461\n",
      "Epoch 162, Loss: 1.3768964409828186, Final Batch Loss: 0.3106677234172821\n",
      "Epoch 163, Loss: 1.4436102509498596, Final Batch Loss: 0.3793407380580902\n",
      "Epoch 164, Loss: 1.434368520975113, Final Batch Loss: 0.384151428937912\n",
      "Epoch 165, Loss: 1.4193677306175232, Final Batch Loss: 0.3575899004936218\n",
      "Epoch 166, Loss: 1.3838198781013489, Final Batch Loss: 0.3152921497821808\n",
      "Epoch 167, Loss: 1.2883500158786774, Final Batch Loss: 0.332784503698349\n",
      "Epoch 168, Loss: 1.4195552468299866, Final Batch Loss: 0.29584670066833496\n",
      "Epoch 169, Loss: 1.3741164207458496, Final Batch Loss: 0.30073124170303345\n",
      "Epoch 170, Loss: 1.2664282619953156, Final Batch Loss: 0.2891791760921478\n",
      "Epoch 171, Loss: 1.3315264284610748, Final Batch Loss: 0.3285275399684906\n",
      "Epoch 172, Loss: 1.5226674377918243, Final Batch Loss: 0.4471401572227478\n",
      "Epoch 173, Loss: 1.3583299815654755, Final Batch Loss: 0.36245837807655334\n",
      "Epoch 174, Loss: 1.363546073436737, Final Batch Loss: 0.3838631212711334\n",
      "Epoch 175, Loss: 1.279055655002594, Final Batch Loss: 0.2621226906776428\n",
      "Epoch 176, Loss: 1.3476332426071167, Final Batch Loss: 0.2889695465564728\n",
      "Epoch 177, Loss: 1.344158560037613, Final Batch Loss: 0.38899487257003784\n",
      "Epoch 178, Loss: 1.403639167547226, Final Batch Loss: 0.4297217130661011\n",
      "Epoch 179, Loss: 1.3608070313930511, Final Batch Loss: 0.3472042381763458\n",
      "Epoch 180, Loss: 1.2484474778175354, Final Batch Loss: 0.3438590466976166\n",
      "Epoch 181, Loss: 1.3209185600280762, Final Batch Loss: 0.324771910905838\n",
      "Epoch 182, Loss: 1.2750702202320099, Final Batch Loss: 0.29608142375946045\n",
      "Epoch 183, Loss: 1.3210644721984863, Final Batch Loss: 0.3457953631877899\n",
      "Epoch 184, Loss: 1.3108973801136017, Final Batch Loss: 0.33821892738342285\n",
      "Epoch 185, Loss: 1.2572204172611237, Final Batch Loss: 0.29564911127090454\n",
      "Epoch 186, Loss: 1.2612080872058868, Final Batch Loss: 0.3213135302066803\n",
      "Epoch 187, Loss: 1.3837888836860657, Final Batch Loss: 0.3908039629459381\n",
      "Epoch 188, Loss: 1.2806939780712128, Final Batch Loss: 0.3589591085910797\n",
      "Epoch 189, Loss: 1.347540259361267, Final Batch Loss: 0.2924521863460541\n",
      "Epoch 190, Loss: 1.2682284116744995, Final Batch Loss: 0.3249841630458832\n",
      "Epoch 191, Loss: 1.3048531413078308, Final Batch Loss: 0.31486013531684875\n",
      "Epoch 192, Loss: 1.15740068256855, Final Batch Loss: 0.22025714814662933\n",
      "Epoch 193, Loss: 1.2442654073238373, Final Batch Loss: 0.30258917808532715\n",
      "Epoch 194, Loss: 1.2435203790664673, Final Batch Loss: 0.2831972539424896\n",
      "Epoch 195, Loss: 1.231608808040619, Final Batch Loss: 0.32062122225761414\n",
      "Epoch 196, Loss: 1.387754201889038, Final Batch Loss: 0.35375747084617615\n",
      "Epoch 197, Loss: 1.2794866263866425, Final Batch Loss: 0.3035179376602173\n",
      "Epoch 198, Loss: 1.3453790843486786, Final Batch Loss: 0.3765834867954254\n",
      "Epoch 199, Loss: 1.3126228153705597, Final Batch Loss: 0.30093446373939514\n",
      "Epoch 200, Loss: 1.3408509194850922, Final Batch Loss: 0.37669771909713745\n",
      "Epoch 201, Loss: 1.330503225326538, Final Batch Loss: 0.346235454082489\n",
      "Epoch 202, Loss: 1.3349327445030212, Final Batch Loss: 0.380066454410553\n",
      "Epoch 203, Loss: 1.2681569457054138, Final Batch Loss: 0.3227286636829376\n",
      "Epoch 204, Loss: 1.2339501082897186, Final Batch Loss: 0.26639044284820557\n",
      "Epoch 205, Loss: 1.2385939359664917, Final Batch Loss: 0.3012193739414215\n",
      "Epoch 206, Loss: 1.3078573048114777, Final Batch Loss: 0.29130953550338745\n",
      "Epoch 207, Loss: 1.267291933298111, Final Batch Loss: 0.30797433853149414\n",
      "Epoch 208, Loss: 1.2261647582054138, Final Batch Loss: 0.300307035446167\n",
      "Epoch 209, Loss: 1.160228818655014, Final Batch Loss: 0.2216986119747162\n",
      "Epoch 210, Loss: 1.2295479625463486, Final Batch Loss: 0.2380954772233963\n",
      "Epoch 211, Loss: 1.1955460011959076, Final Batch Loss: 0.27538812160491943\n",
      "Epoch 212, Loss: 1.2708746790885925, Final Batch Loss: 0.31715333461761475\n",
      "Epoch 213, Loss: 1.1706122159957886, Final Batch Loss: 0.25828415155410767\n",
      "Epoch 214, Loss: 1.2848564982414246, Final Batch Loss: 0.43127408623695374\n",
      "Epoch 215, Loss: 1.2804975807666779, Final Batch Loss: 0.34137827157974243\n",
      "Epoch 216, Loss: 1.2467699646949768, Final Batch Loss: 0.32172122597694397\n",
      "Epoch 217, Loss: 1.2100113034248352, Final Batch Loss: 0.3098846971988678\n",
      "Epoch 218, Loss: 1.1976613998413086, Final Batch Loss: 0.28901639580726624\n",
      "Epoch 219, Loss: 1.1581584513187408, Final Batch Loss: 0.24414986371994019\n",
      "Epoch 220, Loss: 1.2217921614646912, Final Batch Loss: 0.31577545404434204\n",
      "Epoch 221, Loss: 1.1288825869560242, Final Batch Loss: 0.3012738525867462\n",
      "Epoch 222, Loss: 1.088691622018814, Final Batch Loss: 0.20335620641708374\n",
      "Epoch 223, Loss: 1.1367841362953186, Final Batch Loss: 0.2473161518573761\n",
      "Epoch 224, Loss: 1.1438970565795898, Final Batch Loss: 0.24752914905548096\n",
      "Epoch 225, Loss: 1.15128293633461, Final Batch Loss: 0.27393290400505066\n",
      "Epoch 226, Loss: 1.2571007013320923, Final Batch Loss: 0.34105750918388367\n",
      "Epoch 227, Loss: 1.2102175056934357, Final Batch Loss: 0.36345353722572327\n",
      "Epoch 228, Loss: 1.2470545768737793, Final Batch Loss: 0.29891568422317505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229, Loss: 1.2322736084461212, Final Batch Loss: 0.348574161529541\n",
      "Epoch 230, Loss: 1.183184802532196, Final Batch Loss: 0.2727689743041992\n",
      "Epoch 231, Loss: 1.2314192652702332, Final Batch Loss: 0.40854591131210327\n",
      "Epoch 232, Loss: 1.261514812707901, Final Batch Loss: 0.3243790566921234\n",
      "Epoch 233, Loss: 1.0891081094741821, Final Batch Loss: 0.2661568224430084\n",
      "Epoch 234, Loss: 1.2179326713085175, Final Batch Loss: 0.348407119512558\n",
      "Epoch 235, Loss: 1.1892613768577576, Final Batch Loss: 0.28361913561820984\n",
      "Epoch 236, Loss: 1.154431939125061, Final Batch Loss: 0.2907073497772217\n",
      "Epoch 237, Loss: 1.1513017565011978, Final Batch Loss: 0.23035399615764618\n",
      "Epoch 238, Loss: 1.1891465783119202, Final Batch Loss: 0.28894639015197754\n",
      "Epoch 239, Loss: 1.1436255127191544, Final Batch Loss: 0.353447288274765\n",
      "Epoch 240, Loss: 1.0881021171808243, Final Batch Loss: 0.261931836605072\n",
      "Epoch 241, Loss: 1.1472705155611038, Final Batch Loss: 0.26792314648628235\n",
      "Epoch 242, Loss: 1.1779573261737823, Final Batch Loss: 0.2860390543937683\n",
      "Epoch 243, Loss: 1.1159214079380035, Final Batch Loss: 0.2462277114391327\n",
      "Epoch 244, Loss: 1.161039486527443, Final Batch Loss: 0.24231849610805511\n",
      "Epoch 245, Loss: 1.2451554834842682, Final Batch Loss: 0.3450334072113037\n",
      "Epoch 246, Loss: 1.1047218888998032, Final Batch Loss: 0.1807970553636551\n",
      "Epoch 247, Loss: 1.200131356716156, Final Batch Loss: 0.3029440641403198\n",
      "Epoch 248, Loss: 1.1777265667915344, Final Batch Loss: 0.32885950803756714\n",
      "Epoch 249, Loss: 1.194304883480072, Final Batch Loss: 0.40060609579086304\n",
      "Epoch 250, Loss: 1.1825781166553497, Final Batch Loss: 0.31980252265930176\n",
      "Epoch 251, Loss: 1.200565218925476, Final Batch Loss: 0.37527915835380554\n",
      "Epoch 252, Loss: 1.0257184356451035, Final Batch Loss: 0.18721596896648407\n",
      "Epoch 253, Loss: 1.0994036793708801, Final Batch Loss: 0.21819192171096802\n",
      "Epoch 254, Loss: 1.083701565861702, Final Batch Loss: 0.23019300401210785\n",
      "Epoch 255, Loss: 1.0460637211799622, Final Batch Loss: 0.18733757734298706\n",
      "Epoch 256, Loss: 1.0459977090358734, Final Batch Loss: 0.184738889336586\n",
      "Epoch 257, Loss: 1.1232009828090668, Final Batch Loss: 0.25342467427253723\n",
      "Epoch 258, Loss: 1.2093671560287476, Final Batch Loss: 0.3551100492477417\n",
      "Epoch 259, Loss: 1.115124300122261, Final Batch Loss: 0.23398104310035706\n",
      "Epoch 260, Loss: 1.1099651902914047, Final Batch Loss: 0.30941861867904663\n",
      "Epoch 261, Loss: 1.2722476124763489, Final Batch Loss: 0.4204989969730377\n",
      "Epoch 262, Loss: 1.1979472190141678, Final Batch Loss: 0.30506935715675354\n",
      "Epoch 263, Loss: 1.2230161726474762, Final Batch Loss: 0.32306745648384094\n",
      "Epoch 264, Loss: 1.249089777469635, Final Batch Loss: 0.37913307547569275\n",
      "Epoch 265, Loss: 1.1496595591306686, Final Batch Loss: 0.31222495436668396\n",
      "Epoch 266, Loss: 1.1573014706373215, Final Batch Loss: 0.30217766761779785\n",
      "Epoch 267, Loss: 1.0841553658246994, Final Batch Loss: 0.25780072808265686\n",
      "Epoch 268, Loss: 1.1026260703802109, Final Batch Loss: 0.23216460645198822\n",
      "Epoch 269, Loss: 0.9959287643432617, Final Batch Loss: 0.19275878369808197\n",
      "Epoch 270, Loss: 1.0218689292669296, Final Batch Loss: 0.1816873699426651\n",
      "Epoch 271, Loss: 1.1203250885009766, Final Batch Loss: 0.2309110462665558\n",
      "Epoch 272, Loss: 1.186582326889038, Final Batch Loss: 0.3550539016723633\n",
      "Epoch 273, Loss: 1.201886385679245, Final Batch Loss: 0.2850185036659241\n",
      "Epoch 274, Loss: 1.1424919962882996, Final Batch Loss: 0.29421013593673706\n",
      "Epoch 275, Loss: 1.1464927345514297, Final Batch Loss: 0.33910298347473145\n",
      "Epoch 276, Loss: 1.084430754184723, Final Batch Loss: 0.23830507695674896\n",
      "Epoch 277, Loss: 1.0703200548887253, Final Batch Loss: 0.2566118538379669\n",
      "Epoch 278, Loss: 1.1278875470161438, Final Batch Loss: 0.26644212007522583\n",
      "Epoch 279, Loss: 1.2227537035942078, Final Batch Loss: 0.29674071073532104\n",
      "Epoch 280, Loss: 1.0241910070180893, Final Batch Loss: 0.2559817433357239\n",
      "Epoch 281, Loss: 1.1747246086597443, Final Batch Loss: 0.31120017170906067\n",
      "Epoch 282, Loss: 1.1377476304769516, Final Batch Loss: 0.3955352008342743\n",
      "Epoch 283, Loss: 1.1133795827627182, Final Batch Loss: 0.2826337516307831\n",
      "Epoch 284, Loss: 1.0829334259033203, Final Batch Loss: 0.2657890319824219\n",
      "Epoch 285, Loss: 1.1448589265346527, Final Batch Loss: 0.27359089255332947\n",
      "Epoch 286, Loss: 1.0777150094509125, Final Batch Loss: 0.229689359664917\n",
      "Epoch 287, Loss: 1.1092178672552109, Final Batch Loss: 0.36766958236694336\n",
      "Epoch 288, Loss: 1.1882753670215607, Final Batch Loss: 0.29562094807624817\n",
      "Epoch 289, Loss: 1.013592779636383, Final Batch Loss: 0.2260061353445053\n",
      "Epoch 290, Loss: 1.1671046018600464, Final Batch Loss: 0.3719516694545746\n",
      "Epoch 291, Loss: 1.129762127995491, Final Batch Loss: 0.28023526072502136\n",
      "Epoch 292, Loss: 1.1652668118476868, Final Batch Loss: 0.3025970757007599\n",
      "Epoch 293, Loss: 1.0178207159042358, Final Batch Loss: 0.1981937140226364\n",
      "Epoch 294, Loss: 1.0919679999351501, Final Batch Loss: 0.27476274967193604\n",
      "Epoch 295, Loss: 1.1441891491413116, Final Batch Loss: 0.2928354740142822\n",
      "Epoch 296, Loss: 1.09614759683609, Final Batch Loss: 0.2618265748023987\n",
      "Epoch 297, Loss: 1.0805469155311584, Final Batch Loss: 0.2623586058616638\n",
      "Epoch 298, Loss: 1.0394758135080338, Final Batch Loss: 0.21546529233455658\n",
      "Epoch 299, Loss: 1.0961545407772064, Final Batch Loss: 0.36395469307899475\n",
      "Epoch 300, Loss: 1.0398952811956406, Final Batch Loss: 0.2390463948249817\n",
      "Epoch 301, Loss: 1.158238023519516, Final Batch Loss: 0.33839359879493713\n",
      "Epoch 302, Loss: 0.9507812559604645, Final Batch Loss: 0.1406131535768509\n",
      "Epoch 303, Loss: 1.1960700452327728, Final Batch Loss: 0.41291430592536926\n",
      "Epoch 304, Loss: 1.1496258229017258, Final Batch Loss: 0.3496234714984894\n",
      "Epoch 305, Loss: 1.1580933332443237, Final Batch Loss: 0.3087507486343384\n",
      "Epoch 306, Loss: 1.049377292394638, Final Batch Loss: 0.2959021329879761\n",
      "Epoch 307, Loss: 1.0428884476423264, Final Batch Loss: 0.2557832598686218\n",
      "Epoch 308, Loss: 1.132163092494011, Final Batch Loss: 0.3532275855541229\n",
      "Epoch 309, Loss: 1.0882558077573776, Final Batch Loss: 0.3110882341861725\n",
      "Epoch 310, Loss: 1.117418795824051, Final Batch Loss: 0.22195234894752502\n",
      "Epoch 311, Loss: 1.0047601461410522, Final Batch Loss: 0.2147352248430252\n",
      "Epoch 312, Loss: 1.0673695355653763, Final Batch Loss: 0.27368369698524475\n",
      "Epoch 313, Loss: 1.1455620527267456, Final Batch Loss: 0.3167039155960083\n",
      "Epoch 314, Loss: 0.995792955160141, Final Batch Loss: 0.22595082223415375\n",
      "Epoch 315, Loss: 1.1138783991336823, Final Batch Loss: 0.25198519229888916\n",
      "Epoch 316, Loss: 1.1389023214578629, Final Batch Loss: 0.29821136593818665\n",
      "Epoch 317, Loss: 1.0923629999160767, Final Batch Loss: 0.27248311042785645\n",
      "Epoch 318, Loss: 1.1497694849967957, Final Batch Loss: 0.28029346466064453\n",
      "Epoch 319, Loss: 1.1174647510051727, Final Batch Loss: 0.3212912082672119\n",
      "Epoch 320, Loss: 1.0509720593690872, Final Batch Loss: 0.2490951269865036\n",
      "Epoch 321, Loss: 1.0707023590803146, Final Batch Loss: 0.2642970681190491\n",
      "Epoch 322, Loss: 0.9893271028995514, Final Batch Loss: 0.18772274255752563\n",
      "Epoch 323, Loss: 1.0469980686903, Final Batch Loss: 0.2061157375574112\n",
      "Epoch 324, Loss: 1.0302537679672241, Final Batch Loss: 0.2452545315027237\n",
      "Epoch 325, Loss: 1.010379895567894, Final Batch Loss: 0.22172534465789795\n",
      "Epoch 326, Loss: 1.0517044514417648, Final Batch Loss: 0.323496550321579\n",
      "Epoch 327, Loss: 1.0867780148983002, Final Batch Loss: 0.2952449321746826\n",
      "Epoch 328, Loss: 1.0862902849912643, Final Batch Loss: 0.29502010345458984\n",
      "Epoch 329, Loss: 1.163175791501999, Final Batch Loss: 0.3371303081512451\n",
      "Epoch 330, Loss: 1.1293491870164871, Final Batch Loss: 0.3388839066028595\n",
      "Epoch 331, Loss: 1.036960020661354, Final Batch Loss: 0.2821061611175537\n",
      "Epoch 332, Loss: 1.031612902879715, Final Batch Loss: 0.26202958822250366\n",
      "Epoch 333, Loss: 1.0211523175239563, Final Batch Loss: 0.23653286695480347\n",
      "Epoch 334, Loss: 0.9445383846759796, Final Batch Loss: 0.20863740146160126\n",
      "Epoch 335, Loss: 1.0098346918821335, Final Batch Loss: 0.22305910289287567\n",
      "Epoch 336, Loss: 1.1472229063510895, Final Batch Loss: 0.3804956376552582\n",
      "Epoch 337, Loss: 0.994141697883606, Final Batch Loss: 0.24915683269500732\n",
      "Epoch 338, Loss: 0.957439661026001, Final Batch Loss: 0.2156805694103241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339, Loss: 1.017445519566536, Final Batch Loss: 0.18891353905200958\n",
      "Epoch 340, Loss: 1.0874235779047012, Final Batch Loss: 0.3409554064273834\n",
      "Epoch 341, Loss: 1.0329335182905197, Final Batch Loss: 0.2870337665081024\n",
      "Epoch 342, Loss: 1.1517601013183594, Final Batch Loss: 0.3476986587047577\n",
      "Epoch 343, Loss: 0.9810952991247177, Final Batch Loss: 0.29141443967819214\n",
      "Epoch 344, Loss: 0.9896731078624725, Final Batch Loss: 0.18824778497219086\n",
      "Epoch 345, Loss: 0.9621337652206421, Final Batch Loss: 0.232023686170578\n",
      "Epoch 346, Loss: 1.1676239520311356, Final Batch Loss: 0.3711208701133728\n",
      "Epoch 347, Loss: 1.0385797172784805, Final Batch Loss: 0.3200135827064514\n",
      "Epoch 348, Loss: 1.0825107097625732, Final Batch Loss: 0.2878725528717041\n",
      "Epoch 349, Loss: 0.9739629328250885, Final Batch Loss: 0.2566473186016083\n",
      "Epoch 350, Loss: 0.9423007518053055, Final Batch Loss: 0.18872462213039398\n",
      "Epoch 351, Loss: 0.8878106027841568, Final Batch Loss: 0.1536715030670166\n",
      "Epoch 352, Loss: 0.9621172249317169, Final Batch Loss: 0.2520619034767151\n",
      "Epoch 353, Loss: 0.970269501209259, Final Batch Loss: 0.24279356002807617\n",
      "Epoch 354, Loss: 1.040956199169159, Final Batch Loss: 0.36544400453567505\n",
      "Epoch 355, Loss: 0.9537719637155533, Final Batch Loss: 0.2095981389284134\n",
      "Epoch 356, Loss: 0.9884665161371231, Final Batch Loss: 0.26453956961631775\n",
      "Epoch 357, Loss: 1.0607879161834717, Final Batch Loss: 0.2863253057003021\n",
      "Epoch 358, Loss: 1.0317149758338928, Final Batch Loss: 0.2968522310256958\n",
      "Epoch 359, Loss: 0.9057957828044891, Final Batch Loss: 0.14741896092891693\n",
      "Epoch 360, Loss: 0.9779842495918274, Final Batch Loss: 0.15474869310855865\n",
      "Epoch 361, Loss: 0.9993492811918259, Final Batch Loss: 0.23733794689178467\n",
      "Epoch 362, Loss: 1.0108978301286697, Final Batch Loss: 0.21962174773216248\n",
      "Epoch 363, Loss: 1.0831385254859924, Final Batch Loss: 0.3301370441913605\n",
      "Epoch 364, Loss: 1.0357047021389008, Final Batch Loss: 0.23921604454517365\n",
      "Epoch 365, Loss: 0.9554811418056488, Final Batch Loss: 0.23014827072620392\n",
      "Epoch 366, Loss: 1.0586028546094894, Final Batch Loss: 0.2799488306045532\n",
      "Epoch 367, Loss: 1.0628093034029007, Final Batch Loss: 0.28374144434928894\n",
      "Epoch 368, Loss: 1.0160516202449799, Final Batch Loss: 0.20810577273368835\n",
      "Epoch 369, Loss: 1.0711636990308762, Final Batch Loss: 0.3332553207874298\n",
      "Epoch 370, Loss: 1.0101282894611359, Final Batch Loss: 0.27242332696914673\n",
      "Epoch 371, Loss: 0.9381794333457947, Final Batch Loss: 0.17681391537189484\n",
      "Epoch 372, Loss: 1.0488227903842926, Final Batch Loss: 0.2360672652721405\n",
      "Epoch 373, Loss: 1.0545957833528519, Final Batch Loss: 0.35486075282096863\n",
      "Epoch 374, Loss: 1.0460693836212158, Final Batch Loss: 0.3116726875305176\n",
      "Epoch 375, Loss: 0.9466148167848587, Final Batch Loss: 0.22382204234600067\n",
      "Epoch 376, Loss: 1.0990659892559052, Final Batch Loss: 0.30216461420059204\n",
      "Epoch 377, Loss: 0.9131376296281815, Final Batch Loss: 0.21846914291381836\n",
      "Epoch 378, Loss: 0.9899006485939026, Final Batch Loss: 0.19284602999687195\n",
      "Epoch 379, Loss: 1.0202630162239075, Final Batch Loss: 0.30747896432876587\n",
      "Epoch 380, Loss: 1.0659662336111069, Final Batch Loss: 0.28001144528388977\n",
      "Epoch 381, Loss: 1.014318659901619, Final Batch Loss: 0.2757343053817749\n",
      "Epoch 382, Loss: 0.9326051771640778, Final Batch Loss: 0.15694662928581238\n",
      "Epoch 383, Loss: 0.976482480764389, Final Batch Loss: 0.2776099741458893\n",
      "Epoch 384, Loss: 0.9307930171489716, Final Batch Loss: 0.19862034916877747\n",
      "Epoch 385, Loss: 0.9710498154163361, Final Batch Loss: 0.2729499042034149\n",
      "Epoch 386, Loss: 0.9758230745792389, Final Batch Loss: 0.2189294993877411\n",
      "Epoch 387, Loss: 0.9700711518526077, Final Batch Loss: 0.19228167831897736\n",
      "Epoch 388, Loss: 0.9635733664035797, Final Batch Loss: 0.19260813295841217\n",
      "Epoch 389, Loss: 0.9706352055072784, Final Batch Loss: 0.18779250979423523\n",
      "Epoch 390, Loss: 0.9606987833976746, Final Batch Loss: 0.23016001284122467\n",
      "Epoch 391, Loss: 1.0107893198728561, Final Batch Loss: 0.2573082745075226\n",
      "Epoch 392, Loss: 1.023373007774353, Final Batch Loss: 0.2551841139793396\n",
      "Epoch 393, Loss: 0.8785255998373032, Final Batch Loss: 0.16588559746742249\n",
      "Epoch 394, Loss: 0.9433734118938446, Final Batch Loss: 0.17748457193374634\n",
      "Epoch 395, Loss: 0.9615009725093842, Final Batch Loss: 0.24401696026325226\n",
      "Epoch 396, Loss: 1.0329046696424484, Final Batch Loss: 0.3240373134613037\n",
      "Epoch 397, Loss: 1.0995776951313019, Final Batch Loss: 0.38750529289245605\n",
      "Epoch 398, Loss: 1.0280574560165405, Final Batch Loss: 0.2884141802787781\n",
      "Epoch 399, Loss: 0.9917814433574677, Final Batch Loss: 0.23763027787208557\n",
      "Epoch 400, Loss: 0.9696534276008606, Final Batch Loss: 0.25012534856796265\n",
      "Epoch 401, Loss: 0.9886857122182846, Final Batch Loss: 0.34274670481681824\n",
      "Epoch 402, Loss: 0.9571212828159332, Final Batch Loss: 0.2699573040008545\n",
      "Epoch 403, Loss: 1.0270823538303375, Final Batch Loss: 0.3006896674633026\n",
      "Epoch 404, Loss: 0.9761131554841995, Final Batch Loss: 0.25761353969573975\n",
      "Epoch 405, Loss: 1.0317880809307098, Final Batch Loss: 0.26788175106048584\n",
      "Epoch 406, Loss: 0.930863156914711, Final Batch Loss: 0.19535821676254272\n",
      "Epoch 407, Loss: 0.9384183138608932, Final Batch Loss: 0.23323431611061096\n",
      "Epoch 408, Loss: 1.0252736657857895, Final Batch Loss: 0.38243842124938965\n",
      "Epoch 409, Loss: 0.9570587575435638, Final Batch Loss: 0.22085735201835632\n",
      "Epoch 410, Loss: 0.8926742970943451, Final Batch Loss: 0.16451744735240936\n",
      "Epoch 411, Loss: 0.8994123786687851, Final Batch Loss: 0.18635550141334534\n",
      "Epoch 412, Loss: 0.8970544040203094, Final Batch Loss: 0.21422432363033295\n",
      "Epoch 413, Loss: 1.001859113574028, Final Batch Loss: 0.3192250430583954\n",
      "Epoch 414, Loss: 0.9667014628648758, Final Batch Loss: 0.22820523381233215\n",
      "Epoch 415, Loss: 0.8952967077493668, Final Batch Loss: 0.1679508090019226\n",
      "Epoch 416, Loss: 1.019128531217575, Final Batch Loss: 0.24036648869514465\n",
      "Epoch 417, Loss: 0.9055508524179459, Final Batch Loss: 0.17234018445014954\n",
      "Epoch 418, Loss: 0.9425933510065079, Final Batch Loss: 0.19778482615947723\n",
      "Epoch 419, Loss: 0.9881025701761246, Final Batch Loss: 0.2815564274787903\n",
      "Epoch 420, Loss: 0.9215112179517746, Final Batch Loss: 0.1788884848356247\n",
      "Epoch 421, Loss: 0.8566815704107285, Final Batch Loss: 0.16946624219417572\n",
      "Epoch 422, Loss: 0.8844133615493774, Final Batch Loss: 0.17670223116874695\n",
      "Epoch 423, Loss: 0.8789486438035965, Final Batch Loss: 0.19807559251785278\n",
      "Epoch 424, Loss: 0.9206397980451584, Final Batch Loss: 0.3088788390159607\n",
      "Epoch 425, Loss: 1.0104040801525116, Final Batch Loss: 0.31046760082244873\n",
      "Epoch 426, Loss: 0.9678827226161957, Final Batch Loss: 0.2605016827583313\n",
      "Epoch 427, Loss: 0.9507831633090973, Final Batch Loss: 0.22104592621326447\n",
      "Epoch 428, Loss: 1.0744011849164963, Final Batch Loss: 0.19311846792697906\n",
      "Epoch 429, Loss: 0.8742951154708862, Final Batch Loss: 0.22877374291419983\n",
      "Epoch 430, Loss: 0.9153216928243637, Final Batch Loss: 0.22610104084014893\n",
      "Epoch 431, Loss: 0.8836061358451843, Final Batch Loss: 0.24643105268478394\n",
      "Epoch 432, Loss: 0.9311900436878204, Final Batch Loss: 0.2732425332069397\n",
      "Epoch 433, Loss: 1.0050116330385208, Final Batch Loss: 0.2891939878463745\n",
      "Epoch 434, Loss: 0.8600735068321228, Final Batch Loss: 0.14733001589775085\n",
      "Epoch 435, Loss: 0.9410313069820404, Final Batch Loss: 0.2025279849767685\n",
      "Epoch 436, Loss: 0.9381295442581177, Final Batch Loss: 0.26180562376976013\n",
      "Epoch 437, Loss: 0.9172911047935486, Final Batch Loss: 0.2151985764503479\n",
      "Epoch 438, Loss: 0.9170906990766525, Final Batch Loss: 0.22193920612335205\n",
      "Epoch 439, Loss: 1.0268732905387878, Final Batch Loss: 0.32990601658821106\n",
      "Epoch 440, Loss: 0.9628576040267944, Final Batch Loss: 0.23952452838420868\n",
      "Epoch 441, Loss: 0.911098375916481, Final Batch Loss: 0.23551982641220093\n",
      "Epoch 442, Loss: 0.9179789125919342, Final Batch Loss: 0.23696459829807281\n",
      "Epoch 443, Loss: 0.9718299806118011, Final Batch Loss: 0.28400033712387085\n",
      "Epoch 444, Loss: 0.9573983699083328, Final Batch Loss: 0.23180419206619263\n",
      "Epoch 445, Loss: 0.9723165184259415, Final Batch Loss: 0.24798288941383362\n",
      "Epoch 446, Loss: 0.8630199730396271, Final Batch Loss: 0.23429101705551147\n",
      "Epoch 447, Loss: 0.9682475626468658, Final Batch Loss: 0.28192445635795593\n",
      "Epoch 448, Loss: 0.9459665566682816, Final Batch Loss: 0.19303758442401886\n",
      "Epoch 449, Loss: 0.8155197352170944, Final Batch Loss: 0.18290157616138458\n",
      "Epoch 450, Loss: 0.8796049654483795, Final Batch Loss: 0.16341295838356018\n",
      "Epoch 451, Loss: 0.9635568410158157, Final Batch Loss: 0.35815295577049255\n",
      "Epoch 452, Loss: 0.9354662299156189, Final Batch Loss: 0.3064735531806946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453, Loss: 0.8550612777471542, Final Batch Loss: 0.2198757529258728\n",
      "Epoch 454, Loss: 0.886033907532692, Final Batch Loss: 0.18007001280784607\n",
      "Epoch 455, Loss: 0.7815287262201309, Final Batch Loss: 0.10591772198677063\n",
      "Epoch 456, Loss: 1.009181022644043, Final Batch Loss: 0.3622971475124359\n",
      "Epoch 457, Loss: 0.8875086605548859, Final Batch Loss: 0.22528895735740662\n",
      "Epoch 458, Loss: 0.9021607935428619, Final Batch Loss: 0.21675199270248413\n",
      "Epoch 459, Loss: 0.8748362064361572, Final Batch Loss: 0.22208935022354126\n",
      "Epoch 460, Loss: 0.8373570889234543, Final Batch Loss: 0.22717872262001038\n",
      "Epoch 461, Loss: 1.0051553547382355, Final Batch Loss: 0.24081842601299286\n",
      "Epoch 462, Loss: 0.9686192274093628, Final Batch Loss: 0.21854327619075775\n",
      "Epoch 463, Loss: 0.8021692037582397, Final Batch Loss: 0.19508422911167145\n",
      "Epoch 464, Loss: 1.0001437067985535, Final Batch Loss: 0.3318391442298889\n",
      "Epoch 465, Loss: 0.8134221732616425, Final Batch Loss: 0.18851903080940247\n",
      "Epoch 466, Loss: 0.7865519821643829, Final Batch Loss: 0.1517145335674286\n",
      "Epoch 467, Loss: 0.8325061351060867, Final Batch Loss: 0.16356511414051056\n",
      "Epoch 468, Loss: 0.7976012378931046, Final Batch Loss: 0.16608543694019318\n",
      "Epoch 469, Loss: 1.0040253549814224, Final Batch Loss: 0.2513258457183838\n",
      "Epoch 470, Loss: 0.9204734861850739, Final Batch Loss: 0.23845116794109344\n",
      "Epoch 471, Loss: 0.8723406046628952, Final Batch Loss: 0.1812572032213211\n",
      "Epoch 472, Loss: 0.8805092573165894, Final Batch Loss: 0.19253820180892944\n",
      "Epoch 473, Loss: 0.7764957249164581, Final Batch Loss: 0.16673435270786285\n",
      "Epoch 474, Loss: 0.9054422378540039, Final Batch Loss: 0.24627366662025452\n",
      "Epoch 475, Loss: 0.8328956067562103, Final Batch Loss: 0.16931526362895966\n",
      "Epoch 476, Loss: 0.8690184950828552, Final Batch Loss: 0.2231617122888565\n",
      "Epoch 477, Loss: 0.846559152007103, Final Batch Loss: 0.13475190103054047\n",
      "Epoch 478, Loss: 0.7967724949121475, Final Batch Loss: 0.18965256214141846\n",
      "Epoch 479, Loss: 0.8674158602952957, Final Batch Loss: 0.2217438519001007\n",
      "Epoch 480, Loss: 0.7725385874509811, Final Batch Loss: 0.1273902803659439\n",
      "Epoch 481, Loss: 0.9292214512825012, Final Batch Loss: 0.26211875677108765\n",
      "Epoch 482, Loss: 0.7633963525295258, Final Batch Loss: 0.14180530607700348\n",
      "Epoch 483, Loss: 0.8087486177682877, Final Batch Loss: 0.20907999575138092\n",
      "Epoch 484, Loss: 0.775526374578476, Final Batch Loss: 0.158230721950531\n",
      "Epoch 485, Loss: 0.9035732299089432, Final Batch Loss: 0.20506523549556732\n",
      "Epoch 486, Loss: 0.8336678147315979, Final Batch Loss: 0.16918347775936127\n",
      "Epoch 487, Loss: 0.8089479953050613, Final Batch Loss: 0.15563875436782837\n",
      "Epoch 488, Loss: 0.9353034496307373, Final Batch Loss: 0.3321484923362732\n",
      "Epoch 489, Loss: 0.819019079208374, Final Batch Loss: 0.18055056035518646\n",
      "Epoch 490, Loss: 0.7711419761180878, Final Batch Loss: 0.1867658942937851\n",
      "Epoch 491, Loss: 0.9972180277109146, Final Batch Loss: 0.3389735817909241\n",
      "Epoch 492, Loss: 0.7724782824516296, Final Batch Loss: 0.16401581466197968\n",
      "Epoch 493, Loss: 0.8248349130153656, Final Batch Loss: 0.18415415287017822\n",
      "Epoch 494, Loss: 0.9130998104810715, Final Batch Loss: 0.2534325122833252\n",
      "Epoch 495, Loss: 0.8769824504852295, Final Batch Loss: 0.16228076815605164\n",
      "Epoch 496, Loss: 0.859953373670578, Final Batch Loss: 0.19356155395507812\n",
      "Epoch 497, Loss: 0.8015487492084503, Final Batch Loss: 0.1911771595478058\n",
      "Epoch 498, Loss: 0.9735535681247711, Final Batch Loss: 0.31245627999305725\n",
      "Epoch 499, Loss: 0.8542426526546478, Final Batch Loss: 0.13041187822818756\n",
      "Epoch 500, Loss: 0.8521393984556198, Final Batch Loss: 0.2951267957687378\n",
      "Epoch 501, Loss: 0.8396879136562347, Final Batch Loss: 0.20126233994960785\n",
      "Epoch 502, Loss: 0.7872490286827087, Final Batch Loss: 0.21073728799819946\n",
      "Epoch 503, Loss: 0.7740676999092102, Final Batch Loss: 0.17294944822788239\n",
      "Epoch 504, Loss: 0.8913432359695435, Final Batch Loss: 0.245875746011734\n",
      "Epoch 505, Loss: 0.7508914023637772, Final Batch Loss: 0.17566271126270294\n",
      "Epoch 506, Loss: 0.8260570019483566, Final Batch Loss: 0.2490529716014862\n",
      "Epoch 507, Loss: 0.7628638446331024, Final Batch Loss: 0.2101784646511078\n",
      "Epoch 508, Loss: 0.8442135453224182, Final Batch Loss: 0.20608946681022644\n",
      "Epoch 509, Loss: 0.8333142101764679, Final Batch Loss: 0.2157510221004486\n",
      "Epoch 510, Loss: 0.9607315361499786, Final Batch Loss: 0.2631372809410095\n",
      "Epoch 511, Loss: 0.7939292043447495, Final Batch Loss: 0.1352456957101822\n",
      "Epoch 512, Loss: 0.956955686211586, Final Batch Loss: 0.30286023020744324\n",
      "Epoch 513, Loss: 0.6893659830093384, Final Batch Loss: 0.12959934771060944\n",
      "Epoch 514, Loss: 0.9178817123174667, Final Batch Loss: 0.1884022355079651\n",
      "Epoch 515, Loss: 0.7570438086986542, Final Batch Loss: 0.1530117392539978\n",
      "Epoch 516, Loss: 0.8666739463806152, Final Batch Loss: 0.18181897699832916\n",
      "Epoch 517, Loss: 0.7320360541343689, Final Batch Loss: 0.13850973546504974\n",
      "Epoch 518, Loss: 0.8436661660671234, Final Batch Loss: 0.20987839996814728\n",
      "Epoch 519, Loss: 0.7410979568958282, Final Batch Loss: 0.09781749546527863\n",
      "Epoch 520, Loss: 0.7317531108856201, Final Batch Loss: 0.13439132273197174\n",
      "Epoch 521, Loss: 0.9044228792190552, Final Batch Loss: 0.21389181911945343\n",
      "Epoch 522, Loss: 0.7162831425666809, Final Batch Loss: 0.1485939770936966\n",
      "Epoch 523, Loss: 0.7893359661102295, Final Batch Loss: 0.137200728058815\n",
      "Epoch 524, Loss: 0.7541540265083313, Final Batch Loss: 0.1674259454011917\n",
      "Epoch 525, Loss: 0.6926611512899399, Final Batch Loss: 0.09431985020637512\n",
      "Epoch 526, Loss: 0.7490172982215881, Final Batch Loss: 0.1369054764509201\n",
      "Epoch 527, Loss: 0.798689603805542, Final Batch Loss: 0.1938338577747345\n",
      "Epoch 528, Loss: 0.7421337962150574, Final Batch Loss: 0.1769639551639557\n",
      "Epoch 529, Loss: 0.7958879172801971, Final Batch Loss: 0.16346974670886993\n",
      "Epoch 530, Loss: 0.7924668490886688, Final Batch Loss: 0.20852753520011902\n",
      "Epoch 531, Loss: 0.8259628862142563, Final Batch Loss: 0.26925548911094666\n",
      "Epoch 532, Loss: 0.8056240677833557, Final Batch Loss: 0.23851120471954346\n",
      "Epoch 533, Loss: 0.8024666458368301, Final Batch Loss: 0.2043372094631195\n",
      "Epoch 534, Loss: 0.9098065942525864, Final Batch Loss: 0.2359369993209839\n",
      "Epoch 535, Loss: 0.9144867807626724, Final Batch Loss: 0.2618553936481476\n",
      "Epoch 536, Loss: 0.8210255056619644, Final Batch Loss: 0.2068454474210739\n",
      "Epoch 537, Loss: 0.8458642959594727, Final Batch Loss: 0.23777148127555847\n",
      "Epoch 538, Loss: 0.851727619767189, Final Batch Loss: 0.19400590658187866\n",
      "Epoch 539, Loss: 0.8052372485399246, Final Batch Loss: 0.18010202050209045\n",
      "Epoch 540, Loss: 0.836547926068306, Final Batch Loss: 0.23878240585327148\n",
      "Epoch 541, Loss: 0.7849588543176651, Final Batch Loss: 0.21393167972564697\n",
      "Epoch 542, Loss: 0.7209764569997787, Final Batch Loss: 0.15238691866397858\n",
      "Epoch 543, Loss: 0.7923521399497986, Final Batch Loss: 0.2275393158197403\n",
      "Epoch 544, Loss: 0.897565171122551, Final Batch Loss: 0.2523878216743469\n",
      "Epoch 545, Loss: 0.816063404083252, Final Batch Loss: 0.2785501480102539\n",
      "Epoch 546, Loss: 0.7525327354669571, Final Batch Loss: 0.13473671674728394\n",
      "Epoch 547, Loss: 0.8377943783998489, Final Batch Loss: 0.22707587480545044\n",
      "Epoch 548, Loss: 0.7886144816875458, Final Batch Loss: 0.2342562973499298\n",
      "Epoch 549, Loss: 0.8592672049999237, Final Batch Loss: 0.22469112277030945\n",
      "Epoch 550, Loss: 0.8071711361408234, Final Batch Loss: 0.2072017937898636\n",
      "Epoch 551, Loss: 0.6831281632184982, Final Batch Loss: 0.13560625910758972\n",
      "Epoch 552, Loss: 0.8348176777362823, Final Batch Loss: 0.1989833116531372\n",
      "Epoch 553, Loss: 0.7067859768867493, Final Batch Loss: 0.18624497950077057\n",
      "Epoch 554, Loss: 0.8469201624393463, Final Batch Loss: 0.14146341383457184\n",
      "Epoch 555, Loss: 0.7418344169855118, Final Batch Loss: 0.11807827651500702\n",
      "Epoch 556, Loss: 0.8938193470239639, Final Batch Loss: 0.28112998604774475\n",
      "Epoch 557, Loss: 0.7255383729934692, Final Batch Loss: 0.2068079710006714\n",
      "Epoch 558, Loss: 0.6745863258838654, Final Batch Loss: 0.13312175869941711\n",
      "Epoch 559, Loss: 0.9021298736333847, Final Batch Loss: 0.31166765093803406\n",
      "Epoch 560, Loss: 0.7784709930419922, Final Batch Loss: 0.19604186713695526\n",
      "Epoch 561, Loss: 0.7156761288642883, Final Batch Loss: 0.19328667223453522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 562, Loss: 0.7747479230165482, Final Batch Loss: 0.22112144529819489\n",
      "Epoch 563, Loss: 0.7526158094406128, Final Batch Loss: 0.18163089454174042\n",
      "Epoch 564, Loss: 0.7156475335359573, Final Batch Loss: 0.14542561769485474\n",
      "Epoch 565, Loss: 0.6478592678904533, Final Batch Loss: 0.08852667361497879\n",
      "Epoch 566, Loss: 0.726361557841301, Final Batch Loss: 0.18645422160625458\n",
      "Epoch 567, Loss: 0.7697610259056091, Final Batch Loss: 0.1441870927810669\n",
      "Epoch 568, Loss: 0.8133872747421265, Final Batch Loss: 0.1336386501789093\n",
      "Epoch 569, Loss: 0.7972746044397354, Final Batch Loss: 0.20314857363700867\n",
      "Epoch 570, Loss: 0.6962703764438629, Final Batch Loss: 0.1953417807817459\n",
      "Epoch 571, Loss: 0.673380434513092, Final Batch Loss: 0.1314118355512619\n",
      "Epoch 572, Loss: 0.753204345703125, Final Batch Loss: 0.263571172952652\n",
      "Epoch 573, Loss: 0.7072664946317673, Final Batch Loss: 0.1394711434841156\n",
      "Epoch 574, Loss: 0.7646220177412033, Final Batch Loss: 0.19049391150474548\n",
      "Epoch 575, Loss: 0.7498174905776978, Final Batch Loss: 0.1383374035358429\n",
      "Epoch 576, Loss: 0.8132035732269287, Final Batch Loss: 0.16074307262897491\n",
      "Epoch 577, Loss: 0.8829344362020493, Final Batch Loss: 0.2281314730644226\n",
      "Epoch 578, Loss: 0.7814498990774155, Final Batch Loss: 0.14400583505630493\n",
      "Epoch 579, Loss: 0.7664835304021835, Final Batch Loss: 0.1843881458044052\n",
      "Epoch 580, Loss: 0.6872265934944153, Final Batch Loss: 0.1396476924419403\n",
      "Epoch 581, Loss: 0.7958977371454239, Final Batch Loss: 0.1294070929288864\n",
      "Epoch 582, Loss: 0.8347900807857513, Final Batch Loss: 0.2638833522796631\n",
      "Epoch 583, Loss: 0.7295352667570114, Final Batch Loss: 0.19524908065795898\n",
      "Epoch 584, Loss: 0.789006382226944, Final Batch Loss: 0.19320835173130035\n",
      "Epoch 585, Loss: 0.7639849036931992, Final Batch Loss: 0.15176761150360107\n",
      "Epoch 586, Loss: 0.8717335015535355, Final Batch Loss: 0.27390560507774353\n",
      "Epoch 587, Loss: 0.9285502955317497, Final Batch Loss: 0.38543641567230225\n",
      "Epoch 588, Loss: 0.6702921092510223, Final Batch Loss: 0.13670890033245087\n",
      "Epoch 589, Loss: 0.753766655921936, Final Batch Loss: 0.11783476173877716\n",
      "Epoch 590, Loss: 0.8272462338209152, Final Batch Loss: 0.21498921513557434\n",
      "Epoch 591, Loss: 0.7492020577192307, Final Batch Loss: 0.19631637632846832\n",
      "Epoch 592, Loss: 0.83304563164711, Final Batch Loss: 0.18556012213230133\n",
      "Epoch 593, Loss: 0.7238237410783768, Final Batch Loss: 0.24836963415145874\n",
      "Epoch 594, Loss: 0.8995414972305298, Final Batch Loss: 0.2655503749847412\n",
      "Epoch 595, Loss: 0.6938410252332687, Final Batch Loss: 0.1797540932893753\n",
      "Epoch 596, Loss: 0.7202391922473907, Final Batch Loss: 0.1598871797323227\n",
      "Epoch 597, Loss: 0.8191287964582443, Final Batch Loss: 0.2931743562221527\n",
      "Epoch 598, Loss: 0.7758381515741348, Final Batch Loss: 0.2573249638080597\n",
      "Epoch 599, Loss: 0.7773396223783493, Final Batch Loss: 0.21914173662662506\n",
      "Epoch 600, Loss: 0.6899091601371765, Final Batch Loss: 0.15135635435581207\n",
      "Epoch 601, Loss: 0.6897233873605728, Final Batch Loss: 0.23019565641880035\n",
      "Epoch 602, Loss: 0.8802964687347412, Final Batch Loss: 0.328355610370636\n",
      "Epoch 603, Loss: 0.667519137263298, Final Batch Loss: 0.17419612407684326\n",
      "Epoch 604, Loss: 0.7872051000595093, Final Batch Loss: 0.16599641740322113\n",
      "Epoch 605, Loss: 0.745985358953476, Final Batch Loss: 0.1995352804660797\n",
      "Epoch 606, Loss: 0.726651668548584, Final Batch Loss: 0.15755902230739594\n",
      "Epoch 607, Loss: 0.6851591989398003, Final Batch Loss: 0.12258199602365494\n",
      "Epoch 608, Loss: 0.876515343785286, Final Batch Loss: 0.2036082148551941\n",
      "Epoch 609, Loss: 0.7149673253297806, Final Batch Loss: 0.14302751421928406\n",
      "Epoch 610, Loss: 0.765129953622818, Final Batch Loss: 0.14590968191623688\n",
      "Epoch 611, Loss: 0.7315299957990646, Final Batch Loss: 0.156610369682312\n",
      "Epoch 612, Loss: 0.6752747744321823, Final Batch Loss: 0.14529171586036682\n",
      "Epoch 613, Loss: 0.8350107073783875, Final Batch Loss: 0.2386823445558548\n",
      "Epoch 614, Loss: 0.7646630108356476, Final Batch Loss: 0.23873288929462433\n",
      "Epoch 615, Loss: 0.7419334799051285, Final Batch Loss: 0.21378423273563385\n",
      "Epoch 616, Loss: 0.7453254610300064, Final Batch Loss: 0.1642555296421051\n",
      "Epoch 617, Loss: 0.6808455437421799, Final Batch Loss: 0.0725264847278595\n",
      "Epoch 618, Loss: 0.6925084665417671, Final Batch Loss: 0.11789511889219284\n",
      "Epoch 619, Loss: 0.7300805747509003, Final Batch Loss: 0.1409902721643448\n",
      "Epoch 620, Loss: 0.7329775542020798, Final Batch Loss: 0.1655248999595642\n",
      "Epoch 621, Loss: 0.7661477774381638, Final Batch Loss: 0.23946015536785126\n",
      "Epoch 622, Loss: 0.8631541281938553, Final Batch Loss: 0.30112335085868835\n",
      "Epoch 623, Loss: 0.7872295528650284, Final Batch Loss: 0.2421298772096634\n",
      "Epoch 624, Loss: 0.7394991666078568, Final Batch Loss: 0.17008960247039795\n",
      "Epoch 625, Loss: 0.7462499439716339, Final Batch Loss: 0.20614753663539886\n",
      "Epoch 626, Loss: 0.8672624826431274, Final Batch Loss: 0.31694939732551575\n",
      "Epoch 627, Loss: 0.7456592619419098, Final Batch Loss: 0.16758303344249725\n",
      "Epoch 628, Loss: 0.8613564372062683, Final Batch Loss: 0.20817811787128448\n",
      "Epoch 629, Loss: 0.8251869380474091, Final Batch Loss: 0.21738870441913605\n",
      "Epoch 630, Loss: 0.5978802517056465, Final Batch Loss: 0.09090793877840042\n",
      "Epoch 631, Loss: 0.8008355647325516, Final Batch Loss: 0.2624744474887848\n",
      "Epoch 632, Loss: 0.6832599192857742, Final Batch Loss: 0.18453428149223328\n",
      "Epoch 633, Loss: 0.7325232774019241, Final Batch Loss: 0.15586614608764648\n",
      "Epoch 634, Loss: 0.6320248395204544, Final Batch Loss: 0.14684462547302246\n",
      "Epoch 635, Loss: 0.7447968125343323, Final Batch Loss: 0.23534078896045685\n",
      "Epoch 636, Loss: 0.7771882861852646, Final Batch Loss: 0.19885866343975067\n",
      "Epoch 637, Loss: 0.785808265209198, Final Batch Loss: 0.15362496674060822\n",
      "Epoch 638, Loss: 0.7417453825473785, Final Batch Loss: 0.17417839169502258\n",
      "Epoch 639, Loss: 0.6336487829685211, Final Batch Loss: 0.16608108580112457\n",
      "Epoch 640, Loss: 0.7752508819103241, Final Batch Loss: 0.26088160276412964\n",
      "Epoch 641, Loss: 0.826268881559372, Final Batch Loss: 0.19726912677288055\n",
      "Epoch 642, Loss: 0.8034636676311493, Final Batch Loss: 0.2573978006839752\n",
      "Epoch 643, Loss: 0.7491573393344879, Final Batch Loss: 0.26662304997444153\n",
      "Epoch 644, Loss: 0.6289902031421661, Final Batch Loss: 0.14862821996212006\n",
      "Epoch 645, Loss: 0.768778532743454, Final Batch Loss: 0.19321246445178986\n",
      "Epoch 646, Loss: 0.7362065613269806, Final Batch Loss: 0.20291368663311005\n",
      "Epoch 647, Loss: 0.8216769844293594, Final Batch Loss: 0.22641193866729736\n",
      "Epoch 648, Loss: 0.6426565498113632, Final Batch Loss: 0.13390621542930603\n",
      "Epoch 649, Loss: 0.8774934411048889, Final Batch Loss: 0.31483256816864014\n",
      "Epoch 650, Loss: 0.6882666796445847, Final Batch Loss: 0.11071597039699554\n",
      "Epoch 651, Loss: 0.8030537813901901, Final Batch Loss: 0.26698949933052063\n",
      "Epoch 652, Loss: 0.7763124257326126, Final Batch Loss: 0.22521843016147614\n",
      "Epoch 653, Loss: 0.7232935279607773, Final Batch Loss: 0.18075640499591827\n",
      "Epoch 654, Loss: 0.8170866519212723, Final Batch Loss: 0.299982488155365\n",
      "Epoch 655, Loss: 0.6710019111633301, Final Batch Loss: 0.15194685757160187\n",
      "Epoch 656, Loss: 0.6691892445087433, Final Batch Loss: 0.15994049608707428\n",
      "Epoch 657, Loss: 0.7296833097934723, Final Batch Loss: 0.26743972301483154\n",
      "Epoch 658, Loss: 0.6745286583900452, Final Batch Loss: 0.19620096683502197\n",
      "Epoch 659, Loss: 0.7510430216789246, Final Batch Loss: 0.21803607046604156\n",
      "Epoch 660, Loss: 0.7196718454360962, Final Batch Loss: 0.195302814245224\n",
      "Epoch 661, Loss: 0.6873930543661118, Final Batch Loss: 0.19840200245380402\n",
      "Epoch 662, Loss: 0.7393673658370972, Final Batch Loss: 0.23335018754005432\n",
      "Epoch 663, Loss: 0.7232540249824524, Final Batch Loss: 0.21259590983390808\n",
      "Epoch 664, Loss: 0.713262528181076, Final Batch Loss: 0.1577029973268509\n",
      "Epoch 665, Loss: 0.6442433744668961, Final Batch Loss: 0.13551536202430725\n",
      "Epoch 666, Loss: 0.7601484507322311, Final Batch Loss: 0.21916677057743073\n",
      "Epoch 667, Loss: 0.6286898404359818, Final Batch Loss: 0.12582074105739594\n",
      "Epoch 668, Loss: 0.5986264646053314, Final Batch Loss: 0.15578575432300568\n",
      "Epoch 669, Loss: 0.7498933523893356, Final Batch Loss: 0.1763126403093338\n",
      "Epoch 670, Loss: 0.7941671460866928, Final Batch Loss: 0.19172513484954834\n",
      "Epoch 671, Loss: 0.6933396756649017, Final Batch Loss: 0.21080471575260162\n",
      "Epoch 672, Loss: 0.6786759346723557, Final Batch Loss: 0.15702101588249207\n",
      "Epoch 673, Loss: 0.7040069848299026, Final Batch Loss: 0.1635182946920395\n",
      "Epoch 674, Loss: 0.6532060280442238, Final Batch Loss: 0.12282196432352066\n",
      "Epoch 675, Loss: 0.7151222676038742, Final Batch Loss: 0.12971752882003784\n",
      "Epoch 676, Loss: 0.635967880487442, Final Batch Loss: 0.13201218843460083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 677, Loss: 0.7341469377279282, Final Batch Loss: 0.17882388830184937\n",
      "Epoch 678, Loss: 0.776715099811554, Final Batch Loss: 0.18213799595832825\n",
      "Epoch 679, Loss: 0.73601333796978, Final Batch Loss: 0.17983317375183105\n",
      "Epoch 680, Loss: 0.7599701285362244, Final Batch Loss: 0.15341079235076904\n",
      "Epoch 681, Loss: 0.6884826421737671, Final Batch Loss: 0.1090153306722641\n",
      "Epoch 682, Loss: 0.6769784688949585, Final Batch Loss: 0.13727357983589172\n",
      "Epoch 683, Loss: 0.722874291241169, Final Batch Loss: 0.11944551020860672\n",
      "Epoch 684, Loss: 0.8186356872320175, Final Batch Loss: 0.2537585198879242\n",
      "Epoch 685, Loss: 0.6839887499809265, Final Batch Loss: 0.09453067183494568\n",
      "Epoch 686, Loss: 0.7442409470677376, Final Batch Loss: 0.1021120473742485\n",
      "Epoch 687, Loss: 0.6477579772472382, Final Batch Loss: 0.13837535679340363\n",
      "Epoch 688, Loss: 0.7007957994937897, Final Batch Loss: 0.17622263729572296\n",
      "Epoch 689, Loss: 0.791960820555687, Final Batch Loss: 0.2218841165304184\n",
      "Epoch 690, Loss: 0.675669863820076, Final Batch Loss: 0.11573046445846558\n",
      "Epoch 691, Loss: 0.6004848629236221, Final Batch Loss: 0.13177400827407837\n",
      "Epoch 692, Loss: 0.6698931306600571, Final Batch Loss: 0.11459420621395111\n",
      "Epoch 693, Loss: 0.683454617857933, Final Batch Loss: 0.13914836943149567\n",
      "Epoch 694, Loss: 0.6867752522230148, Final Batch Loss: 0.16948069632053375\n",
      "Epoch 695, Loss: 0.6709449887275696, Final Batch Loss: 0.09605881571769714\n",
      "Epoch 696, Loss: 0.6512839198112488, Final Batch Loss: 0.07297195494174957\n",
      "Epoch 697, Loss: 0.5857335329055786, Final Batch Loss: 0.14039038121700287\n",
      "Epoch 698, Loss: 0.7670388966798782, Final Batch Loss: 0.22308039665222168\n",
      "Epoch 699, Loss: 0.7436332255601883, Final Batch Loss: 0.17601843178272247\n",
      "Epoch 700, Loss: 0.8232730776071548, Final Batch Loss: 0.2646084427833557\n",
      "Epoch 701, Loss: 0.827520877122879, Final Batch Loss: 0.2042689025402069\n",
      "Epoch 702, Loss: 0.7662619799375534, Final Batch Loss: 0.2847306728363037\n",
      "Epoch 703, Loss: 0.8159018605947495, Final Batch Loss: 0.27028781175613403\n",
      "Epoch 704, Loss: 0.7069204449653625, Final Batch Loss: 0.17313814163208008\n",
      "Epoch 705, Loss: 0.6454356908798218, Final Batch Loss: 0.1665443778038025\n",
      "Epoch 706, Loss: 0.7591610550880432, Final Batch Loss: 0.1833420693874359\n",
      "Epoch 707, Loss: 0.6313284486532211, Final Batch Loss: 0.18992701172828674\n",
      "Epoch 708, Loss: 0.658213809132576, Final Batch Loss: 0.13906170427799225\n",
      "Epoch 709, Loss: 0.7276918441057205, Final Batch Loss: 0.14933735132217407\n",
      "Epoch 710, Loss: 0.7494678199291229, Final Batch Loss: 0.17917239665985107\n",
      "Epoch 711, Loss: 0.7754958868026733, Final Batch Loss: 0.16354399919509888\n",
      "Epoch 712, Loss: 0.6941571831703186, Final Batch Loss: 0.2161138355731964\n",
      "Epoch 713, Loss: 0.7746097445487976, Final Batch Loss: 0.17014631628990173\n",
      "Epoch 714, Loss: 0.6817778944969177, Final Batch Loss: 0.23052085936069489\n",
      "Epoch 715, Loss: 0.752202183008194, Final Batch Loss: 0.18994636833667755\n",
      "Epoch 716, Loss: 0.6000885590910912, Final Batch Loss: 0.1570156067609787\n",
      "Epoch 717, Loss: 0.5973271429538727, Final Batch Loss: 0.11793677508831024\n",
      "Epoch 718, Loss: 0.655964657664299, Final Batch Loss: 0.1384212225675583\n",
      "Epoch 719, Loss: 0.6228010058403015, Final Batch Loss: 0.18943698704242706\n",
      "Epoch 720, Loss: 0.7428887635469437, Final Batch Loss: 0.15094523131847382\n",
      "Epoch 721, Loss: 0.7110781669616699, Final Batch Loss: 0.12173676490783691\n",
      "Epoch 722, Loss: 0.7161780595779419, Final Batch Loss: 0.16980701684951782\n",
      "Epoch 723, Loss: 0.6195138990879059, Final Batch Loss: 0.12266848981380463\n",
      "Epoch 724, Loss: 0.6554437726736069, Final Batch Loss: 0.1629265546798706\n",
      "Epoch 725, Loss: 0.6049441397190094, Final Batch Loss: 0.11513043195009232\n",
      "Epoch 726, Loss: 0.6814195811748505, Final Batch Loss: 0.17961665987968445\n",
      "Epoch 727, Loss: 0.6816714704036713, Final Batch Loss: 0.21528789401054382\n",
      "Epoch 728, Loss: 0.734173133969307, Final Batch Loss: 0.2154252976179123\n",
      "Epoch 729, Loss: 0.6051262021064758, Final Batch Loss: 0.09168532490730286\n",
      "Epoch 730, Loss: 0.7164672762155533, Final Batch Loss: 0.1891096979379654\n",
      "Epoch 731, Loss: 0.87638358771801, Final Batch Loss: 0.23447473347187042\n",
      "Epoch 732, Loss: 0.7122096121311188, Final Batch Loss: 0.20360945165157318\n",
      "Epoch 733, Loss: 0.6786738187074661, Final Batch Loss: 0.1932150274515152\n",
      "Epoch 734, Loss: 0.6096519380807877, Final Batch Loss: 0.170257568359375\n",
      "Epoch 735, Loss: 0.5489279180765152, Final Batch Loss: 0.14709913730621338\n",
      "Epoch 736, Loss: 0.7158884257078171, Final Batch Loss: 0.16950279474258423\n",
      "Epoch 737, Loss: 0.666624166071415, Final Batch Loss: 0.14946864545345306\n",
      "Epoch 738, Loss: 0.6117967665195465, Final Batch Loss: 0.14459992945194244\n",
      "Epoch 739, Loss: 0.7608778923749924, Final Batch Loss: 0.27648109197616577\n",
      "Epoch 740, Loss: 0.6510745510458946, Final Batch Loss: 0.14663326740264893\n",
      "Epoch 741, Loss: 0.6811100989580154, Final Batch Loss: 0.17274992167949677\n",
      "Epoch 742, Loss: 0.5647928416728973, Final Batch Loss: 0.1113554984331131\n",
      "Epoch 743, Loss: 0.7112173736095428, Final Batch Loss: 0.21877676248550415\n",
      "Epoch 744, Loss: 0.6929181665182114, Final Batch Loss: 0.21020837128162384\n",
      "Epoch 745, Loss: 0.7160492688417435, Final Batch Loss: 0.20968179404735565\n",
      "Epoch 746, Loss: 0.6149279922246933, Final Batch Loss: 0.17926722764968872\n",
      "Epoch 747, Loss: 0.7889924794435501, Final Batch Loss: 0.19253240525722504\n",
      "Epoch 748, Loss: 0.6676072180271149, Final Batch Loss: 0.15287327766418457\n",
      "Epoch 749, Loss: 0.7104900479316711, Final Batch Loss: 0.1255902200937271\n",
      "Epoch 750, Loss: 0.7400271445512772, Final Batch Loss: 0.1807873547077179\n",
      "Epoch 751, Loss: 0.7091639041900635, Final Batch Loss: 0.17658212780952454\n",
      "Epoch 752, Loss: 0.7132499814033508, Final Batch Loss: 0.2724592685699463\n",
      "Epoch 753, Loss: 0.8052843511104584, Final Batch Loss: 0.23935209214687347\n",
      "Epoch 754, Loss: 0.6702042520046234, Final Batch Loss: 0.15889358520507812\n",
      "Epoch 755, Loss: 0.7413942217826843, Final Batch Loss: 0.19061243534088135\n",
      "Epoch 756, Loss: 0.6982046663761139, Final Batch Loss: 0.1957423835992813\n",
      "Epoch 757, Loss: 0.6641920953989029, Final Batch Loss: 0.15399223566055298\n",
      "Epoch 758, Loss: 0.5625483244657516, Final Batch Loss: 0.10408806800842285\n",
      "Epoch 759, Loss: 0.6474129557609558, Final Batch Loss: 0.13185249269008636\n",
      "Epoch 760, Loss: 0.7341194599866867, Final Batch Loss: 0.1800031065940857\n",
      "Epoch 761, Loss: 0.7824996411800385, Final Batch Loss: 0.16727381944656372\n",
      "Epoch 762, Loss: 0.5780658051371574, Final Batch Loss: 0.1069977805018425\n",
      "Epoch 763, Loss: 0.6295271813869476, Final Batch Loss: 0.13951712846755981\n",
      "Epoch 764, Loss: 0.6537852436304092, Final Batch Loss: 0.1764233112335205\n",
      "Epoch 765, Loss: 0.591357558965683, Final Batch Loss: 0.09930747747421265\n",
      "Epoch 766, Loss: 0.7241092771291733, Final Batch Loss: 0.16477070748806\n",
      "Epoch 767, Loss: 0.632272943854332, Final Batch Loss: 0.1607438027858734\n",
      "Epoch 768, Loss: 0.6390844285488129, Final Batch Loss: 0.19188642501831055\n",
      "Epoch 769, Loss: 0.6784705370664597, Final Batch Loss: 0.1508597731590271\n",
      "Epoch 770, Loss: 0.7638445794582367, Final Batch Loss: 0.23714493215084076\n",
      "Epoch 771, Loss: 0.681774839758873, Final Batch Loss: 0.18694806098937988\n",
      "Epoch 772, Loss: 0.6731368154287338, Final Batch Loss: 0.16979341208934784\n",
      "Epoch 773, Loss: 0.6511858999729156, Final Batch Loss: 0.15552997589111328\n",
      "Epoch 774, Loss: 0.6118099987506866, Final Batch Loss: 0.13686157763004303\n",
      "Epoch 775, Loss: 0.7033691257238388, Final Batch Loss: 0.2521296739578247\n",
      "Epoch 776, Loss: 0.6219202280044556, Final Batch Loss: 0.189022958278656\n",
      "Epoch 777, Loss: 0.7390607595443726, Final Batch Loss: 0.16645044088363647\n",
      "Epoch 778, Loss: 0.7171616107225418, Final Batch Loss: 0.2003454715013504\n",
      "Epoch 779, Loss: 0.7083577215671539, Final Batch Loss: 0.1778843253850937\n",
      "Epoch 780, Loss: 0.6769916266202927, Final Batch Loss: 0.13956357538700104\n",
      "Epoch 781, Loss: 0.8099360466003418, Final Batch Loss: 0.2862055003643036\n",
      "Epoch 782, Loss: 0.7705460488796234, Final Batch Loss: 0.2792428731918335\n",
      "Epoch 783, Loss: 0.6349005997180939, Final Batch Loss: 0.19064883887767792\n",
      "Epoch 784, Loss: 0.6954381540417671, Final Batch Loss: 0.11225911229848862\n",
      "Epoch 785, Loss: 0.6131115630269051, Final Batch Loss: 0.12196702510118484\n",
      "Epoch 786, Loss: 0.5417090430855751, Final Batch Loss: 0.11720464378595352\n",
      "Epoch 787, Loss: 0.5309850201010704, Final Batch Loss: 0.08351654559373856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 788, Loss: 0.5906983464956284, Final Batch Loss: 0.10981442034244537\n",
      "Epoch 789, Loss: 0.6414440721273422, Final Batch Loss: 0.1594630628824234\n",
      "Epoch 790, Loss: 0.5920511931180954, Final Batch Loss: 0.14356954395771027\n",
      "Epoch 791, Loss: 0.6561779826879501, Final Batch Loss: 0.22433607280254364\n",
      "Epoch 792, Loss: 0.6615923121571541, Final Batch Loss: 0.12466954439878464\n",
      "Epoch 793, Loss: 0.651913121342659, Final Batch Loss: 0.13804282248020172\n",
      "Epoch 794, Loss: 0.5844428390264511, Final Batch Loss: 0.1412920206785202\n",
      "Epoch 795, Loss: 0.6508402675390244, Final Batch Loss: 0.20005571842193604\n",
      "Epoch 796, Loss: 0.7317245304584503, Final Batch Loss: 0.25263097882270813\n",
      "Epoch 797, Loss: 0.7114172428846359, Final Batch Loss: 0.1886722296476364\n",
      "Epoch 798, Loss: 0.6829952597618103, Final Batch Loss: 0.13267405331134796\n",
      "Epoch 799, Loss: 0.6671641021966934, Final Batch Loss: 0.23181937634944916\n",
      "Epoch 800, Loss: 0.6788956820964813, Final Batch Loss: 0.1996288299560547\n",
      "Epoch 801, Loss: 0.6491583287715912, Final Batch Loss: 0.12350226938724518\n",
      "Epoch 802, Loss: 0.5986568331718445, Final Batch Loss: 0.1723802536725998\n",
      "Epoch 803, Loss: 0.49684201925992966, Final Batch Loss: 0.08374786376953125\n",
      "Epoch 804, Loss: 0.5812374129891396, Final Batch Loss: 0.059592120349407196\n",
      "Epoch 805, Loss: 0.6290410310029984, Final Batch Loss: 0.17052823305130005\n",
      "Epoch 806, Loss: 0.7104756087064743, Final Batch Loss: 0.19163638353347778\n",
      "Epoch 807, Loss: 0.6309569776058197, Final Batch Loss: 0.1780124455690384\n",
      "Epoch 808, Loss: 0.6861669719219208, Final Batch Loss: 0.18035197257995605\n",
      "Epoch 809, Loss: 0.681466281414032, Final Batch Loss: 0.13294045627117157\n",
      "Epoch 810, Loss: 0.7746051698923111, Final Batch Loss: 0.305848628282547\n",
      "Epoch 811, Loss: 0.6822031438350677, Final Batch Loss: 0.2396206557750702\n",
      "Epoch 812, Loss: 0.6828635782003403, Final Batch Loss: 0.14407208561897278\n",
      "Epoch 813, Loss: 0.5858055576682091, Final Batch Loss: 0.13610966503620148\n",
      "Epoch 814, Loss: 0.639251597225666, Final Batch Loss: 0.09581462293863297\n",
      "Epoch 815, Loss: 0.6631319671869278, Final Batch Loss: 0.16473659873008728\n",
      "Epoch 816, Loss: 0.6567945778369904, Final Batch Loss: 0.15004494786262512\n",
      "Epoch 817, Loss: 0.6842612326145172, Final Batch Loss: 0.19938480854034424\n",
      "Epoch 818, Loss: 0.5599000230431557, Final Batch Loss: 0.10451497882604599\n",
      "Epoch 819, Loss: 0.5542588606476784, Final Batch Loss: 0.1819252371788025\n",
      "Epoch 820, Loss: 0.5484203696250916, Final Batch Loss: 0.1578086018562317\n",
      "Epoch 821, Loss: 0.5750320255756378, Final Batch Loss: 0.10175782442092896\n",
      "Epoch 822, Loss: 0.6284925788640976, Final Batch Loss: 0.15268340706825256\n",
      "Epoch 823, Loss: 0.6915246248245239, Final Batch Loss: 0.18986795842647552\n",
      "Epoch 824, Loss: 0.6644657701253891, Final Batch Loss: 0.19006101787090302\n",
      "Epoch 825, Loss: 0.6009417399764061, Final Batch Loss: 0.10171014815568924\n",
      "Epoch 826, Loss: 0.7002369463443756, Final Batch Loss: 0.24922490119934082\n",
      "Epoch 827, Loss: 0.7191397249698639, Final Batch Loss: 0.22815656661987305\n",
      "Epoch 828, Loss: 0.6594753786921501, Final Batch Loss: 0.16707274317741394\n",
      "Epoch 829, Loss: 0.6891696006059647, Final Batch Loss: 0.1647731065750122\n",
      "Epoch 830, Loss: 0.7046851068735123, Final Batch Loss: 0.14607037603855133\n",
      "Epoch 831, Loss: 0.6772620975971222, Final Batch Loss: 0.18878376483917236\n",
      "Epoch 832, Loss: 0.7597754597663879, Final Batch Loss: 0.25622978806495667\n",
      "Epoch 833, Loss: 0.5365446209907532, Final Batch Loss: 0.14288364350795746\n",
      "Epoch 834, Loss: 0.6410066485404968, Final Batch Loss: 0.18718400597572327\n",
      "Epoch 835, Loss: 0.7720453292131424, Final Batch Loss: 0.2564801871776581\n",
      "Epoch 836, Loss: 0.6598349958658218, Final Batch Loss: 0.14713512361049652\n",
      "Epoch 837, Loss: 0.5517466813325882, Final Batch Loss: 0.1639406979084015\n",
      "Epoch 838, Loss: 0.6090051531791687, Final Batch Loss: 0.17078803479671478\n",
      "Epoch 839, Loss: 0.625006228685379, Final Batch Loss: 0.16992905735969543\n",
      "Epoch 840, Loss: 0.6429470479488373, Final Batch Loss: 0.189169242978096\n",
      "Epoch 841, Loss: 0.6782308220863342, Final Batch Loss: 0.24790066480636597\n",
      "Epoch 842, Loss: 0.5506331846117973, Final Batch Loss: 0.1303941011428833\n",
      "Epoch 843, Loss: 0.5802052393555641, Final Batch Loss: 0.1584658920764923\n",
      "Epoch 844, Loss: 0.5892520397901535, Final Batch Loss: 0.12657392024993896\n",
      "Epoch 845, Loss: 0.6060655489563942, Final Batch Loss: 0.13554631173610687\n",
      "Epoch 846, Loss: 0.6259612292051315, Final Batch Loss: 0.16765642166137695\n",
      "Epoch 847, Loss: 0.5170941948890686, Final Batch Loss: 0.11958549916744232\n",
      "Epoch 848, Loss: 0.6510557606816292, Final Batch Loss: 0.1243109256029129\n",
      "Epoch 849, Loss: 0.5720554068684578, Final Batch Loss: 0.17886632680892944\n",
      "Epoch 850, Loss: 0.6214278489351273, Final Batch Loss: 0.12668752670288086\n",
      "Epoch 851, Loss: 0.5995292514562607, Final Batch Loss: 0.1264709234237671\n",
      "Epoch 852, Loss: 0.48717285692691803, Final Batch Loss: 0.08898262679576874\n",
      "Epoch 853, Loss: 0.5978831201791763, Final Batch Loss: 0.1447651982307434\n",
      "Epoch 854, Loss: 0.6677121296525002, Final Batch Loss: 0.2363782823085785\n",
      "Epoch 855, Loss: 0.6052152961492538, Final Batch Loss: 0.12064093351364136\n",
      "Epoch 856, Loss: 0.5140188410878181, Final Batch Loss: 0.08254974335432053\n",
      "Epoch 857, Loss: 0.4880959540605545, Final Batch Loss: 0.10567090660333633\n",
      "Epoch 858, Loss: 0.5839890539646149, Final Batch Loss: 0.11431398242712021\n",
      "Epoch 859, Loss: 0.588578999042511, Final Batch Loss: 0.1357230842113495\n",
      "Epoch 860, Loss: 0.5437584519386292, Final Batch Loss: 0.1337863951921463\n",
      "Epoch 861, Loss: 0.7004912942647934, Final Batch Loss: 0.20744158327579498\n",
      "Epoch 862, Loss: 0.6401540488004684, Final Batch Loss: 0.10678572952747345\n",
      "Epoch 863, Loss: 0.626082643866539, Final Batch Loss: 0.13958339393138885\n",
      "Epoch 864, Loss: 0.6521000117063522, Final Batch Loss: 0.18527227640151978\n",
      "Epoch 865, Loss: 0.665807381272316, Final Batch Loss: 0.24301961064338684\n",
      "Epoch 866, Loss: 0.5894576460123062, Final Batch Loss: 0.14737236499786377\n",
      "Epoch 867, Loss: 0.5433818623423576, Final Batch Loss: 0.12098915129899979\n",
      "Epoch 868, Loss: 0.8559806197881699, Final Batch Loss: 0.3007783889770508\n",
      "Epoch 869, Loss: 0.5965198278427124, Final Batch Loss: 0.13373737037181854\n",
      "Epoch 870, Loss: 0.6238438040018082, Final Batch Loss: 0.1298975646495819\n",
      "Epoch 871, Loss: 0.6241367757320404, Final Batch Loss: 0.2019202709197998\n",
      "Epoch 872, Loss: 0.5654891952872276, Final Batch Loss: 0.09404102712869644\n",
      "Epoch 873, Loss: 0.6798054650425911, Final Batch Loss: 0.2303936779499054\n",
      "Epoch 874, Loss: 0.5675123557448387, Final Batch Loss: 0.1335478127002716\n",
      "Epoch 875, Loss: 0.5818939208984375, Final Batch Loss: 0.12746790051460266\n",
      "Epoch 876, Loss: 0.6630550771951675, Final Batch Loss: 0.23610074818134308\n",
      "Epoch 877, Loss: 0.7341854274272919, Final Batch Loss: 0.1864093691110611\n",
      "Epoch 878, Loss: 0.6363286525011063, Final Batch Loss: 0.14012101292610168\n",
      "Epoch 879, Loss: 0.6358478665351868, Final Batch Loss: 0.18098747730255127\n",
      "Epoch 880, Loss: 0.6213826239109039, Final Batch Loss: 0.19874757528305054\n",
      "Epoch 881, Loss: 0.5891755893826485, Final Batch Loss: 0.1244456022977829\n",
      "Epoch 882, Loss: 0.5962445512413979, Final Batch Loss: 0.12389922887086868\n",
      "Epoch 883, Loss: 0.5908948257565498, Final Batch Loss: 0.1256621927022934\n",
      "Epoch 884, Loss: 0.7290968894958496, Final Batch Loss: 0.19528517127037048\n",
      "Epoch 885, Loss: 0.6454174518585205, Final Batch Loss: 0.18209514021873474\n",
      "Epoch 886, Loss: 0.5822398215532303, Final Batch Loss: 0.1402413547039032\n",
      "Epoch 887, Loss: 0.5784738883376122, Final Batch Loss: 0.11003953963518143\n",
      "Epoch 888, Loss: 0.6651947200298309, Final Batch Loss: 0.1771477907896042\n",
      "Epoch 889, Loss: 0.6445553600788116, Final Batch Loss: 0.1513916552066803\n",
      "Epoch 890, Loss: 0.544448547065258, Final Batch Loss: 0.1672494262456894\n",
      "Epoch 891, Loss: 0.530195340514183, Final Batch Loss: 0.12052719295024872\n",
      "Epoch 892, Loss: 0.6814296692609787, Final Batch Loss: 0.21881021559238434\n",
      "Epoch 893, Loss: 0.578583225607872, Final Batch Loss: 0.1766107827425003\n",
      "Epoch 894, Loss: 0.6871337294578552, Final Batch Loss: 0.23762089014053345\n",
      "Epoch 895, Loss: 0.58506840467453, Final Batch Loss: 0.10201580822467804\n",
      "Epoch 896, Loss: 0.7339788153767586, Final Batch Loss: 0.265048623085022\n",
      "Epoch 897, Loss: 0.5774690806865692, Final Batch Loss: 0.15585745871067047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 898, Loss: 0.6152035519480705, Final Batch Loss: 0.12329725921154022\n",
      "Epoch 899, Loss: 0.7039682418107986, Final Batch Loss: 0.226139634847641\n",
      "Epoch 900, Loss: 0.6107500568032265, Final Batch Loss: 0.14580366015434265\n",
      "Epoch 901, Loss: 0.520521841943264, Final Batch Loss: 0.09957578778266907\n",
      "Epoch 902, Loss: 0.5969284623861313, Final Batch Loss: 0.1628260612487793\n",
      "Epoch 903, Loss: 0.541644275188446, Final Batch Loss: 0.12861374020576477\n",
      "Epoch 904, Loss: 0.5729932561516762, Final Batch Loss: 0.1626276671886444\n",
      "Epoch 905, Loss: 0.6431593596935272, Final Batch Loss: 0.191880002617836\n",
      "Epoch 906, Loss: 0.5969880893826485, Final Batch Loss: 0.0948118194937706\n",
      "Epoch 907, Loss: 0.6866386830806732, Final Batch Loss: 0.2295769304037094\n",
      "Epoch 908, Loss: 0.6892965584993362, Final Batch Loss: 0.22875069081783295\n",
      "Epoch 909, Loss: 0.5591249614953995, Final Batch Loss: 0.09525490552186966\n",
      "Epoch 910, Loss: 0.581444039940834, Final Batch Loss: 0.11783730983734131\n",
      "Epoch 911, Loss: 0.6505670547485352, Final Batch Loss: 0.15223577618598938\n",
      "Epoch 912, Loss: 0.578509084880352, Final Batch Loss: 0.11294599622488022\n",
      "Epoch 913, Loss: 0.6027094572782516, Final Batch Loss: 0.14484964311122894\n",
      "Epoch 914, Loss: 0.7499954700469971, Final Batch Loss: 0.18603235483169556\n",
      "Epoch 915, Loss: 0.6693901270627975, Final Batch Loss: 0.2748105227947235\n",
      "Epoch 916, Loss: 0.442191980779171, Final Batch Loss: 0.07281572371721268\n",
      "Epoch 917, Loss: 0.6096418350934982, Final Batch Loss: 0.1652965247631073\n",
      "Epoch 918, Loss: 0.5875050872564316, Final Batch Loss: 0.06805577874183655\n",
      "Epoch 919, Loss: 0.5744036585092545, Final Batch Loss: 0.09613054990768433\n",
      "Epoch 920, Loss: 0.7010035514831543, Final Batch Loss: 0.1615927666425705\n",
      "Epoch 921, Loss: 0.5734212249517441, Final Batch Loss: 0.1253836452960968\n",
      "Epoch 922, Loss: 0.5623156502842903, Final Batch Loss: 0.15186475217342377\n",
      "Epoch 923, Loss: 0.668886125087738, Final Batch Loss: 0.25749117136001587\n",
      "Epoch 924, Loss: 0.7631169483065605, Final Batch Loss: 0.2970885932445526\n",
      "Epoch 925, Loss: 0.710250198841095, Final Batch Loss: 0.20920822024345398\n",
      "Epoch 926, Loss: 0.6538145393133163, Final Batch Loss: 0.16620154678821564\n",
      "Epoch 927, Loss: 0.7121383100748062, Final Batch Loss: 0.2689814865589142\n",
      "Epoch 928, Loss: 0.6747042238712311, Final Batch Loss: 0.1497933566570282\n",
      "Epoch 929, Loss: 0.48669441044330597, Final Batch Loss: 0.11173740774393082\n",
      "Epoch 930, Loss: 0.6943705826997757, Final Batch Loss: 0.09206594526767731\n",
      "Epoch 931, Loss: 0.6505879238247871, Final Batch Loss: 0.23428145051002502\n",
      "Epoch 932, Loss: 0.5518781468272209, Final Batch Loss: 0.10115018486976624\n",
      "Epoch 933, Loss: 0.5560820624232292, Final Batch Loss: 0.15794767439365387\n",
      "Epoch 934, Loss: 0.582742378115654, Final Batch Loss: 0.10387371480464935\n",
      "Epoch 935, Loss: 0.6706790179014206, Final Batch Loss: 0.17559868097305298\n",
      "Epoch 936, Loss: 0.611393541097641, Final Batch Loss: 0.15805836021900177\n",
      "Epoch 937, Loss: 0.5695261657238007, Final Batch Loss: 0.1696605086326599\n",
      "Epoch 938, Loss: 0.5385364517569542, Final Batch Loss: 0.16740640997886658\n",
      "Epoch 939, Loss: 0.6742430776357651, Final Batch Loss: 0.16919037699699402\n",
      "Epoch 940, Loss: 0.5658801048994064, Final Batch Loss: 0.11149092018604279\n",
      "Epoch 941, Loss: 0.5120596438646317, Final Batch Loss: 0.10359076410531998\n",
      "Epoch 942, Loss: 0.5943527817726135, Final Batch Loss: 0.18182475864887238\n",
      "Epoch 943, Loss: 0.5609354376792908, Final Batch Loss: 0.13667252659797668\n",
      "Epoch 944, Loss: 0.6103273630142212, Final Batch Loss: 0.12970000505447388\n",
      "Epoch 945, Loss: 0.6046182215213776, Final Batch Loss: 0.11620192229747772\n",
      "Epoch 946, Loss: 0.6606467738747597, Final Batch Loss: 0.22822074592113495\n",
      "Epoch 947, Loss: 0.6755440533161163, Final Batch Loss: 0.13831433653831482\n",
      "Epoch 948, Loss: 0.6890923231840134, Final Batch Loss: 0.2861555516719818\n",
      "Epoch 949, Loss: 0.5256650745868683, Final Batch Loss: 0.06807936728000641\n",
      "Epoch 950, Loss: 0.6024144291877747, Final Batch Loss: 0.10149005055427551\n",
      "Epoch 951, Loss: 0.601132869720459, Final Batch Loss: 0.15827089548110962\n",
      "Epoch 952, Loss: 0.49061116576194763, Final Batch Loss: 0.1609782725572586\n",
      "Epoch 953, Loss: 0.5304028317332268, Final Batch Loss: 0.09655339270830154\n",
      "Epoch 954, Loss: 0.6627724766731262, Final Batch Loss: 0.13920612633228302\n",
      "Epoch 955, Loss: 0.47946714237332344, Final Batch Loss: 0.04646160826086998\n",
      "Epoch 956, Loss: 0.6139919236302376, Final Batch Loss: 0.11885557323694229\n",
      "Epoch 957, Loss: 0.5731399059295654, Final Batch Loss: 0.1279657781124115\n",
      "Epoch 958, Loss: 0.5441591143608093, Final Batch Loss: 0.11502662301063538\n",
      "Epoch 959, Loss: 0.5548641234636307, Final Batch Loss: 0.08264673501253128\n",
      "Epoch 960, Loss: 0.5912530422210693, Final Batch Loss: 0.09836508333683014\n",
      "Epoch 961, Loss: 0.48931463807821274, Final Batch Loss: 0.07882830500602722\n",
      "Epoch 962, Loss: 0.6142820715904236, Final Batch Loss: 0.11800634860992432\n",
      "Epoch 963, Loss: 0.6089510098099709, Final Batch Loss: 0.14045865833759308\n",
      "Epoch 964, Loss: 0.6060110777616501, Final Batch Loss: 0.15392860770225525\n",
      "Epoch 965, Loss: 0.5875309631228447, Final Batch Loss: 0.16627655923366547\n",
      "Epoch 966, Loss: 0.5746689140796661, Final Batch Loss: 0.14697231352329254\n",
      "Epoch 967, Loss: 0.624725729227066, Final Batch Loss: 0.1671012043952942\n",
      "Epoch 968, Loss: 0.6769227609038353, Final Batch Loss: 0.11443094164133072\n",
      "Epoch 969, Loss: 0.6637881100177765, Final Batch Loss: 0.16666199266910553\n",
      "Epoch 970, Loss: 0.5917081758379936, Final Batch Loss: 0.09220278263092041\n",
      "Epoch 971, Loss: 0.58066276460886, Final Batch Loss: 0.09604943543672562\n",
      "Epoch 972, Loss: 0.6226375326514244, Final Batch Loss: 0.18407724797725677\n",
      "Epoch 973, Loss: 0.6746055334806442, Final Batch Loss: 0.18967653810977936\n",
      "Epoch 974, Loss: 0.5283868461847305, Final Batch Loss: 0.12391120940446854\n",
      "Epoch 975, Loss: 0.5528478398919106, Final Batch Loss: 0.11949410289525986\n",
      "Epoch 976, Loss: 0.5366071984171867, Final Batch Loss: 0.11596290022134781\n",
      "Epoch 977, Loss: 0.46107759699225426, Final Batch Loss: 0.057370852679014206\n",
      "Epoch 978, Loss: 0.5049102008342743, Final Batch Loss: 0.09698838740587234\n",
      "Epoch 979, Loss: 0.5342348664999008, Final Batch Loss: 0.09293700009584427\n",
      "Epoch 980, Loss: 0.4983311593532562, Final Batch Loss: 0.07684772461652756\n",
      "Epoch 981, Loss: 0.5116867944598198, Final Batch Loss: 0.13332341611385345\n",
      "Epoch 982, Loss: 0.5849283039569855, Final Batch Loss: 0.17036189138889313\n",
      "Epoch 983, Loss: 0.5275533944368362, Final Batch Loss: 0.08547769486904144\n",
      "Epoch 984, Loss: 0.5546151548624039, Final Batch Loss: 0.11378456652164459\n",
      "Epoch 985, Loss: 0.7423631250858307, Final Batch Loss: 0.2304251790046692\n",
      "Epoch 986, Loss: 0.6789828911423683, Final Batch Loss: 0.20355218648910522\n",
      "Epoch 987, Loss: 0.5859602466225624, Final Batch Loss: 0.22476106882095337\n",
      "Epoch 988, Loss: 0.5873722285032272, Final Batch Loss: 0.21035648882389069\n",
      "Epoch 989, Loss: 0.6166305989027023, Final Batch Loss: 0.1298435628414154\n",
      "Epoch 990, Loss: 0.6109611392021179, Final Batch Loss: 0.11749700456857681\n",
      "Epoch 991, Loss: 0.5615487173199654, Final Batch Loss: 0.14051249623298645\n",
      "Epoch 992, Loss: 0.5372480750083923, Final Batch Loss: 0.12239896506071091\n",
      "Epoch 993, Loss: 0.5952485725283623, Final Batch Loss: 0.2218174785375595\n",
      "Epoch 994, Loss: 0.6502835899591446, Final Batch Loss: 0.2406911700963974\n",
      "Epoch 995, Loss: 0.5653678849339485, Final Batch Loss: 0.12120627611875534\n",
      "Epoch 996, Loss: 0.7094754725694656, Final Batch Loss: 0.20612086355686188\n",
      "Epoch 997, Loss: 0.5635155737400055, Final Batch Loss: 0.1259937435388565\n",
      "Epoch 998, Loss: 0.5550210028886795, Final Batch Loss: 0.1716274619102478\n",
      "Epoch 999, Loss: 0.5521986335515976, Final Batch Loss: 0.1285841464996338\n",
      "Epoch 1000, Loss: 0.5776698514819145, Final Batch Loss: 0.09204540401697159\n",
      "Epoch 1001, Loss: 0.5782210826873779, Final Batch Loss: 0.12387147545814514\n",
      "Epoch 1002, Loss: 0.6043084785342216, Final Batch Loss: 0.12023866921663284\n",
      "Epoch 1003, Loss: 0.4961182251572609, Final Batch Loss: 0.12637197971343994\n",
      "Epoch 1004, Loss: 0.5760368630290031, Final Batch Loss: 0.07426441460847855\n",
      "Epoch 1005, Loss: 0.7344257012009621, Final Batch Loss: 0.22162535786628723\n",
      "Epoch 1006, Loss: 0.6413684785366058, Final Batch Loss: 0.07881313562393188\n",
      "Epoch 1007, Loss: 0.5918617621064186, Final Batch Loss: 0.16592133045196533\n",
      "Epoch 1008, Loss: 0.5388758257031441, Final Batch Loss: 0.1056436151266098\n",
      "Epoch 1009, Loss: 0.545758992433548, Final Batch Loss: 0.11404849588871002\n",
      "Epoch 1010, Loss: 0.5089995712041855, Final Batch Loss: 0.14725331962108612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1011, Loss: 0.4796103686094284, Final Batch Loss: 0.08420572429895401\n",
      "Epoch 1012, Loss: 0.5461175367236137, Final Batch Loss: 0.1406795233488083\n",
      "Epoch 1013, Loss: 0.45591843873262405, Final Batch Loss: 0.10924261808395386\n",
      "Epoch 1014, Loss: 0.5859989076852798, Final Batch Loss: 0.13338814675807953\n",
      "Epoch 1015, Loss: 0.5279834419488907, Final Batch Loss: 0.14701174199581146\n",
      "Epoch 1016, Loss: 0.6411350443959236, Final Batch Loss: 0.23375649750232697\n",
      "Epoch 1017, Loss: 0.5773485898971558, Final Batch Loss: 0.11556282639503479\n",
      "Epoch 1018, Loss: 0.6739226058125496, Final Batch Loss: 0.10921034961938858\n",
      "Epoch 1019, Loss: 0.6884605288505554, Final Batch Loss: 0.2908054292201996\n",
      "Epoch 1020, Loss: 0.5136124044656754, Final Batch Loss: 0.12450215965509415\n",
      "Epoch 1021, Loss: 0.6037067174911499, Final Batch Loss: 0.14079183340072632\n",
      "Epoch 1022, Loss: 0.5352222844958305, Final Batch Loss: 0.16268573701381683\n",
      "Epoch 1023, Loss: 0.5305489525198936, Final Batch Loss: 0.11893235892057419\n",
      "Epoch 1024, Loss: 0.5698796063661575, Final Batch Loss: 0.17868852615356445\n",
      "Epoch 1025, Loss: 0.6734841465950012, Final Batch Loss: 0.14524850249290466\n",
      "Epoch 1026, Loss: 0.5586800277233124, Final Batch Loss: 0.19339938461780548\n",
      "Epoch 1027, Loss: 0.5656412690877914, Final Batch Loss: 0.1688249409198761\n",
      "Epoch 1028, Loss: 0.60684834420681, Final Batch Loss: 0.12760905921459198\n",
      "Epoch 1029, Loss: 0.6065418869256973, Final Batch Loss: 0.1620868742465973\n",
      "Epoch 1030, Loss: 0.61223304271698, Final Batch Loss: 0.16809603571891785\n",
      "Epoch 1031, Loss: 0.5678385868668556, Final Batch Loss: 0.1757228821516037\n",
      "Epoch 1032, Loss: 0.6767625063657761, Final Batch Loss: 0.2375088334083557\n",
      "Epoch 1033, Loss: 0.6015274375677109, Final Batch Loss: 0.20339223742485046\n",
      "Epoch 1034, Loss: 0.6814218461513519, Final Batch Loss: 0.19798746705055237\n",
      "Epoch 1035, Loss: 0.6092229261994362, Final Batch Loss: 0.10214326530694962\n",
      "Epoch 1036, Loss: 0.48167135566473007, Final Batch Loss: 0.06737595796585083\n",
      "Epoch 1037, Loss: 0.5801094025373459, Final Batch Loss: 0.11039720475673676\n",
      "Epoch 1038, Loss: 0.5131644383072853, Final Batch Loss: 0.101075679063797\n",
      "Epoch 1039, Loss: 0.5562669783830643, Final Batch Loss: 0.12786182761192322\n",
      "Epoch 1040, Loss: 0.5112435147166252, Final Batch Loss: 0.09710502624511719\n",
      "Epoch 1041, Loss: 0.5610211119055748, Final Batch Loss: 0.10332728177309036\n",
      "Epoch 1042, Loss: 0.6011007726192474, Final Batch Loss: 0.16936592757701874\n",
      "Epoch 1043, Loss: 0.5696217268705368, Final Batch Loss: 0.17380864918231964\n",
      "Epoch 1044, Loss: 0.5541131868958473, Final Batch Loss: 0.09146115928888321\n",
      "Epoch 1045, Loss: 0.48545295745134354, Final Batch Loss: 0.06138833612203598\n",
      "Epoch 1046, Loss: 0.5567008927464485, Final Batch Loss: 0.07671990990638733\n",
      "Epoch 1047, Loss: 0.603247195482254, Final Batch Loss: 0.15496201813220978\n",
      "Epoch 1048, Loss: 0.540587805211544, Final Batch Loss: 0.11062756925821304\n",
      "Epoch 1049, Loss: 0.5392502769827843, Final Batch Loss: 0.1449013352394104\n",
      "Epoch 1050, Loss: 0.541351206600666, Final Batch Loss: 0.06733330339193344\n",
      "Epoch 1051, Loss: 0.5890677645802498, Final Batch Loss: 0.18857325613498688\n",
      "Epoch 1052, Loss: 0.5065921545028687, Final Batch Loss: 0.13264207541942596\n",
      "Epoch 1053, Loss: 0.4516466408967972, Final Batch Loss: 0.08228396624326706\n",
      "Epoch 1054, Loss: 0.562895305454731, Final Batch Loss: 0.15981723368167877\n",
      "Epoch 1055, Loss: 0.5844512358307838, Final Batch Loss: 0.07266873866319656\n",
      "Epoch 1056, Loss: 0.4673319309949875, Final Batch Loss: 0.0635782778263092\n",
      "Epoch 1057, Loss: 0.6854531019926071, Final Batch Loss: 0.3172186017036438\n",
      "Epoch 1058, Loss: 0.6800368130207062, Final Batch Loss: 0.16563311219215393\n",
      "Epoch 1059, Loss: 0.5767836421728134, Final Batch Loss: 0.1097245141863823\n",
      "Epoch 1060, Loss: 0.667189359664917, Final Batch Loss: 0.23818841576576233\n",
      "Epoch 1061, Loss: 0.5710897445678711, Final Batch Loss: 0.1208718866109848\n",
      "Epoch 1062, Loss: 0.6881271004676819, Final Batch Loss: 0.17416873574256897\n",
      "Epoch 1063, Loss: 0.5661531984806061, Final Batch Loss: 0.14541621506214142\n",
      "Epoch 1064, Loss: 0.6066005676984787, Final Batch Loss: 0.14216169714927673\n",
      "Epoch 1065, Loss: 0.5441764444112778, Final Batch Loss: 0.10765353590250015\n",
      "Epoch 1066, Loss: 0.569328285753727, Final Batch Loss: 0.17860175669193268\n",
      "Epoch 1067, Loss: 0.5980031862854958, Final Batch Loss: 0.08972489833831787\n",
      "Epoch 1068, Loss: 0.607773631811142, Final Batch Loss: 0.12638063728809357\n",
      "Epoch 1069, Loss: 0.5121012404561043, Final Batch Loss: 0.11728163063526154\n",
      "Epoch 1070, Loss: 0.613612949848175, Final Batch Loss: 0.1729690134525299\n",
      "Epoch 1071, Loss: 0.60101717710495, Final Batch Loss: 0.11350520700216293\n",
      "Epoch 1072, Loss: 0.5242559984326363, Final Batch Loss: 0.13832907378673553\n",
      "Epoch 1073, Loss: 0.5914107635617256, Final Batch Loss: 0.20571142435073853\n",
      "Epoch 1074, Loss: 0.5954036265611649, Final Batch Loss: 0.1464819759130478\n",
      "Epoch 1075, Loss: 0.6415331736207008, Final Batch Loss: 0.19481654465198517\n",
      "Epoch 1076, Loss: 0.620771236717701, Final Batch Loss: 0.2050250768661499\n",
      "Epoch 1077, Loss: 0.596448227763176, Final Batch Loss: 0.13673216104507446\n",
      "Epoch 1078, Loss: 0.6263617724180222, Final Batch Loss: 0.14911764860153198\n",
      "Epoch 1079, Loss: 0.6838817447423935, Final Batch Loss: 0.2353920191526413\n",
      "Epoch 1080, Loss: 0.5637993216514587, Final Batch Loss: 0.09542189538478851\n",
      "Epoch 1081, Loss: 0.5173167809844017, Final Batch Loss: 0.10398095846176147\n",
      "Epoch 1082, Loss: 0.5781324952840805, Final Batch Loss: 0.18027588725090027\n",
      "Epoch 1083, Loss: 0.5364741086959839, Final Batch Loss: 0.1865396648645401\n",
      "Epoch 1084, Loss: 0.5968837961554527, Final Batch Loss: 0.10853778570890427\n",
      "Epoch 1085, Loss: 0.6008992344141006, Final Batch Loss: 0.1323542296886444\n",
      "Epoch 1086, Loss: 0.5268609747290611, Final Batch Loss: 0.07091132551431656\n",
      "Epoch 1087, Loss: 0.497457891702652, Final Batch Loss: 0.0944809839129448\n",
      "Epoch 1088, Loss: 0.5182365700602531, Final Batch Loss: 0.09128354489803314\n",
      "Epoch 1089, Loss: 0.5562920644879341, Final Batch Loss: 0.0717054232954979\n",
      "Epoch 1090, Loss: 0.6252341121435165, Final Batch Loss: 0.15442074835300446\n",
      "Epoch 1091, Loss: 0.5399161577224731, Final Batch Loss: 0.10490286350250244\n",
      "Epoch 1092, Loss: 0.5562369748950005, Final Batch Loss: 0.11003074049949646\n",
      "Epoch 1093, Loss: 0.4527973532676697, Final Batch Loss: 0.13196228444576263\n",
      "Epoch 1094, Loss: 0.6142410188913345, Final Batch Loss: 0.14235541224479675\n",
      "Epoch 1095, Loss: 0.6074307560920715, Final Batch Loss: 0.18239128589630127\n",
      "Epoch 1096, Loss: 0.6434578970074654, Final Batch Loss: 0.19897502660751343\n",
      "Epoch 1097, Loss: 0.5925658941268921, Final Batch Loss: 0.17840521037578583\n",
      "Epoch 1098, Loss: 0.588385172188282, Final Batch Loss: 0.09323606640100479\n",
      "Epoch 1099, Loss: 0.6774377971887589, Final Batch Loss: 0.17172925174236298\n",
      "Epoch 1100, Loss: 0.45626426488161087, Final Batch Loss: 0.11199930310249329\n",
      "Epoch 1101, Loss: 0.5923290699720383, Final Batch Loss: 0.09787160158157349\n",
      "Epoch 1102, Loss: 0.4937729313969612, Final Batch Loss: 0.112312912940979\n",
      "Epoch 1103, Loss: 0.5566762462258339, Final Batch Loss: 0.11914809793233871\n",
      "Epoch 1104, Loss: 0.587929256260395, Final Batch Loss: 0.20849716663360596\n",
      "Epoch 1105, Loss: 0.4519515112042427, Final Batch Loss: 0.08505726605653763\n",
      "Epoch 1106, Loss: 0.6003858000040054, Final Batch Loss: 0.11850219964981079\n",
      "Epoch 1107, Loss: 0.564571350812912, Final Batch Loss: 0.13505259156227112\n",
      "Epoch 1108, Loss: 0.5028980076313019, Final Batch Loss: 0.08361391723155975\n",
      "Epoch 1109, Loss: 0.49584678560495377, Final Batch Loss: 0.12855547666549683\n",
      "Epoch 1110, Loss: 0.5950043126940727, Final Batch Loss: 0.17215599119663239\n",
      "Epoch 1111, Loss: 0.4724225327372551, Final Batch Loss: 0.1224258542060852\n",
      "Epoch 1112, Loss: 0.4386991336941719, Final Batch Loss: 0.06753769516944885\n",
      "Epoch 1113, Loss: 0.47756653279066086, Final Batch Loss: 0.12321366369724274\n",
      "Epoch 1114, Loss: 0.5993605628609657, Final Batch Loss: 0.14892169833183289\n",
      "Epoch 1115, Loss: 0.4834573492407799, Final Batch Loss: 0.15281905233860016\n",
      "Epoch 1116, Loss: 0.5036375522613525, Final Batch Loss: 0.12314154952764511\n",
      "Epoch 1117, Loss: 0.4713195636868477, Final Batch Loss: 0.13427121937274933\n",
      "Epoch 1118, Loss: 0.46593254059553146, Final Batch Loss: 0.11034809052944183\n",
      "Epoch 1119, Loss: 0.4767691418528557, Final Batch Loss: 0.07730836421251297\n",
      "Epoch 1120, Loss: 0.5794961079955101, Final Batch Loss: 0.10973004251718521\n",
      "Epoch 1121, Loss: 0.537829115986824, Final Batch Loss: 0.12024606764316559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1122, Loss: 0.6008926182985306, Final Batch Loss: 0.13844172656536102\n",
      "Epoch 1123, Loss: 0.4858857989311218, Final Batch Loss: 0.10177694261074066\n",
      "Epoch 1124, Loss: 0.551874928176403, Final Batch Loss: 0.08739655464887619\n",
      "Epoch 1125, Loss: 0.6226316839456558, Final Batch Loss: 0.2144785076379776\n",
      "Epoch 1126, Loss: 0.5581081435084343, Final Batch Loss: 0.1067783311009407\n",
      "Epoch 1127, Loss: 0.48955079168081284, Final Batch Loss: 0.14928337931632996\n",
      "Epoch 1128, Loss: 0.41272328794002533, Final Batch Loss: 0.06422886252403259\n",
      "Epoch 1129, Loss: 0.5893320962786674, Final Batch Loss: 0.14398911595344543\n",
      "Epoch 1130, Loss: 0.5250893905758858, Final Batch Loss: 0.09665220230817795\n",
      "Epoch 1131, Loss: 0.5290614664554596, Final Batch Loss: 0.1195702850818634\n",
      "Epoch 1132, Loss: 0.4978224188089371, Final Batch Loss: 0.13290293514728546\n",
      "Epoch 1133, Loss: 0.561140313744545, Final Batch Loss: 0.1252613365650177\n",
      "Epoch 1134, Loss: 0.580109030008316, Final Batch Loss: 0.18474023044109344\n",
      "Epoch 1135, Loss: 0.43668653070926666, Final Batch Loss: 0.11362140625715256\n",
      "Epoch 1136, Loss: 0.5097975432872772, Final Batch Loss: 0.06836093217134476\n",
      "Epoch 1137, Loss: 0.5298173278570175, Final Batch Loss: 0.1655489057302475\n",
      "Epoch 1138, Loss: 0.5688721165060997, Final Batch Loss: 0.14776313304901123\n",
      "Epoch 1139, Loss: 0.5389076620340347, Final Batch Loss: 0.11623521894216537\n",
      "Epoch 1140, Loss: 0.4538052901625633, Final Batch Loss: 0.06607215851545334\n",
      "Epoch 1141, Loss: 0.47279559075832367, Final Batch Loss: 0.1277620792388916\n",
      "Epoch 1142, Loss: 0.5998318269848824, Final Batch Loss: 0.1456148475408554\n",
      "Epoch 1143, Loss: 0.5136706680059433, Final Batch Loss: 0.16868571937084198\n",
      "Epoch 1144, Loss: 0.5277382284402847, Final Batch Loss: 0.12132956832647324\n",
      "Epoch 1145, Loss: 0.5699707269668579, Final Batch Loss: 0.12915141880512238\n",
      "Epoch 1146, Loss: 0.5154456794261932, Final Batch Loss: 0.06259247660636902\n",
      "Epoch 1147, Loss: 0.4333125688135624, Final Batch Loss: 0.05345211550593376\n",
      "Epoch 1148, Loss: 0.4475580155849457, Final Batch Loss: 0.11077219247817993\n",
      "Epoch 1149, Loss: 0.569606363773346, Final Batch Loss: 0.18711844086647034\n",
      "Epoch 1150, Loss: 0.48115047067403793, Final Batch Loss: 0.10175707191228867\n",
      "Epoch 1151, Loss: 0.5845542922616005, Final Batch Loss: 0.17272016406059265\n",
      "Epoch 1152, Loss: 0.4910949692130089, Final Batch Loss: 0.0871613621711731\n",
      "Epoch 1153, Loss: 0.5455911532044411, Final Batch Loss: 0.1551702469587326\n",
      "Epoch 1154, Loss: 0.5503519922494888, Final Batch Loss: 0.167754128575325\n",
      "Epoch 1155, Loss: 0.5809618830680847, Final Batch Loss: 0.2148231863975525\n",
      "Epoch 1156, Loss: 0.5097838267683983, Final Batch Loss: 0.12796901166439056\n",
      "Epoch 1157, Loss: 0.5097873508930206, Final Batch Loss: 0.10365188866853714\n",
      "Epoch 1158, Loss: 0.5537048429250717, Final Batch Loss: 0.12538599967956543\n",
      "Epoch 1159, Loss: 0.4694284200668335, Final Batch Loss: 0.07972699403762817\n",
      "Epoch 1160, Loss: 0.5486124455928802, Final Batch Loss: 0.21087966859340668\n",
      "Epoch 1161, Loss: 0.505908414721489, Final Batch Loss: 0.15161798894405365\n",
      "Epoch 1162, Loss: 0.5468540713191032, Final Batch Loss: 0.09143895655870438\n",
      "Epoch 1163, Loss: 0.5337807461619377, Final Batch Loss: 0.10071888566017151\n",
      "Epoch 1164, Loss: 0.5260215699672699, Final Batch Loss: 0.07791873812675476\n",
      "Epoch 1165, Loss: 0.5098741725087166, Final Batch Loss: 0.11112306267023087\n",
      "Epoch 1166, Loss: 0.5241789519786835, Final Batch Loss: 0.07797353714704514\n",
      "Epoch 1167, Loss: 0.5709646344184875, Final Batch Loss: 0.1422121375799179\n",
      "Epoch 1168, Loss: 0.5667746290564537, Final Batch Loss: 0.14299696683883667\n",
      "Epoch 1169, Loss: 0.547445110976696, Final Batch Loss: 0.058320097625255585\n",
      "Epoch 1170, Loss: 0.5755724981427193, Final Batch Loss: 0.17229723930358887\n",
      "Epoch 1171, Loss: 0.6124006062746048, Final Batch Loss: 0.06504161655902863\n",
      "Epoch 1172, Loss: 0.537739485502243, Final Batch Loss: 0.1493034064769745\n",
      "Epoch 1173, Loss: 0.49132564663887024, Final Batch Loss: 0.1266024112701416\n",
      "Epoch 1174, Loss: 0.4547341838479042, Final Batch Loss: 0.09273681789636612\n",
      "Epoch 1175, Loss: 0.59271240234375, Final Batch Loss: 0.19570712745189667\n",
      "Epoch 1176, Loss: 0.5249608010053635, Final Batch Loss: 0.14535051584243774\n",
      "Epoch 1177, Loss: 0.4428125247359276, Final Batch Loss: 0.06855174899101257\n",
      "Epoch 1178, Loss: 0.5674031600356102, Final Batch Loss: 0.21385110914707184\n",
      "Epoch 1179, Loss: 0.46552299708127975, Final Batch Loss: 0.06294267624616623\n",
      "Epoch 1180, Loss: 0.4445664919912815, Final Batch Loss: 0.05233433470129967\n",
      "Epoch 1181, Loss: 0.5040619820356369, Final Batch Loss: 0.12026415020227432\n",
      "Epoch 1182, Loss: 0.7109050378203392, Final Batch Loss: 0.27613547444343567\n",
      "Epoch 1183, Loss: 0.5168521031737328, Final Batch Loss: 0.15894685685634613\n",
      "Epoch 1184, Loss: 0.4375552609562874, Final Batch Loss: 0.11703164130449295\n",
      "Epoch 1185, Loss: 0.6008229851722717, Final Batch Loss: 0.19270983338356018\n",
      "Epoch 1186, Loss: 0.47469596564769745, Final Batch Loss: 0.08917350322008133\n",
      "Epoch 1187, Loss: 0.5100927799940109, Final Batch Loss: 0.1485636830329895\n",
      "Epoch 1188, Loss: 0.4855155348777771, Final Batch Loss: 0.12827204167842865\n",
      "Epoch 1189, Loss: 0.49146465957164764, Final Batch Loss: 0.15297231078147888\n",
      "Epoch 1190, Loss: 0.6142294481396675, Final Batch Loss: 0.16566559672355652\n",
      "Epoch 1191, Loss: 0.47250591963529587, Final Batch Loss: 0.08029812574386597\n",
      "Epoch 1192, Loss: 0.4611584432423115, Final Batch Loss: 0.04428096488118172\n",
      "Epoch 1193, Loss: 0.5402646213769913, Final Batch Loss: 0.12420038878917694\n",
      "Epoch 1194, Loss: 0.5961971804499626, Final Batch Loss: 0.188713937997818\n",
      "Epoch 1195, Loss: 0.6597461253404617, Final Batch Loss: 0.1929863542318344\n",
      "Epoch 1196, Loss: 0.5234600752592087, Final Batch Loss: 0.1237793043255806\n",
      "Epoch 1197, Loss: 0.4865141361951828, Final Batch Loss: 0.10584212094545364\n",
      "Epoch 1198, Loss: 0.40832769125699997, Final Batch Loss: 0.03767531365156174\n",
      "Epoch 1199, Loss: 0.5335787385702133, Final Batch Loss: 0.0722416490316391\n",
      "Epoch 1200, Loss: 0.45242125540971756, Final Batch Loss: 0.07564198970794678\n",
      "Epoch 1201, Loss: 0.4739278480410576, Final Batch Loss: 0.07668960839509964\n",
      "Epoch 1202, Loss: 0.5494479835033417, Final Batch Loss: 0.15536822378635406\n",
      "Epoch 1203, Loss: 0.5700671896338463, Final Batch Loss: 0.09571903198957443\n",
      "Epoch 1204, Loss: 0.5613546893000603, Final Batch Loss: 0.09599136561155319\n",
      "Epoch 1205, Loss: 0.44934430718421936, Final Batch Loss: 0.12942636013031006\n",
      "Epoch 1206, Loss: 0.4566831886768341, Final Batch Loss: 0.10373815894126892\n",
      "Epoch 1207, Loss: 0.621101513504982, Final Batch Loss: 0.26398345828056335\n",
      "Epoch 1208, Loss: 0.5606983453035355, Final Batch Loss: 0.14301201701164246\n",
      "Epoch 1209, Loss: 0.4342031329870224, Final Batch Loss: 0.11560259014368057\n",
      "Epoch 1210, Loss: 0.56850865483284, Final Batch Loss: 0.09109924733638763\n",
      "Epoch 1211, Loss: 0.4372304826974869, Final Batch Loss: 0.06658589094877243\n",
      "Epoch 1212, Loss: 0.47095657140016556, Final Batch Loss: 0.145403653383255\n",
      "Epoch 1213, Loss: 0.4187982603907585, Final Batch Loss: 0.1309671550989151\n",
      "Epoch 1214, Loss: 0.42917976900935173, Final Batch Loss: 0.061923783272504807\n",
      "Epoch 1215, Loss: 0.5391902178525925, Final Batch Loss: 0.11683852970600128\n",
      "Epoch 1216, Loss: 0.5125806853175163, Final Batch Loss: 0.14988702535629272\n",
      "Epoch 1217, Loss: 0.4695298932492733, Final Batch Loss: 0.06148812547326088\n",
      "Epoch 1218, Loss: 0.485716849565506, Final Batch Loss: 0.0912867933511734\n",
      "Epoch 1219, Loss: 0.4845300018787384, Final Batch Loss: 0.09705857187509537\n",
      "Epoch 1220, Loss: 0.5731346905231476, Final Batch Loss: 0.17884977161884308\n",
      "Epoch 1221, Loss: 0.44027888774871826, Final Batch Loss: 0.09536924213171005\n",
      "Epoch 1222, Loss: 0.6026297360658646, Final Batch Loss: 0.2530430555343628\n",
      "Epoch 1223, Loss: 0.5668008625507355, Final Batch Loss: 0.17194558680057526\n",
      "Epoch 1224, Loss: 0.5142815709114075, Final Batch Loss: 0.1117158755660057\n",
      "Epoch 1225, Loss: 0.6080557852983475, Final Batch Loss: 0.12904387712478638\n",
      "Epoch 1226, Loss: 0.3864278681576252, Final Batch Loss: 0.0551362968981266\n",
      "Epoch 1227, Loss: 0.5408255532383919, Final Batch Loss: 0.18990571796894073\n",
      "Epoch 1228, Loss: 0.4651698097586632, Final Batch Loss: 0.1353064477443695\n",
      "Epoch 1229, Loss: 0.45718903839588165, Final Batch Loss: 0.10532699525356293\n",
      "Epoch 1230, Loss: 0.4887392818927765, Final Batch Loss: 0.12076525390148163\n",
      "Epoch 1231, Loss: 0.4831886440515518, Final Batch Loss: 0.07652441412210464\n",
      "Epoch 1232, Loss: 0.47661272436380386, Final Batch Loss: 0.1371053159236908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1233, Loss: 0.4616934806108475, Final Batch Loss: 0.09452906996011734\n",
      "Epoch 1234, Loss: 0.43082840368151665, Final Batch Loss: 0.13019531965255737\n",
      "Epoch 1235, Loss: 0.42727748304605484, Final Batch Loss: 0.07280490547418594\n",
      "Epoch 1236, Loss: 0.46361149102449417, Final Batch Loss: 0.13128195703029633\n",
      "Epoch 1237, Loss: 0.5012400150299072, Final Batch Loss: 0.11514182388782501\n",
      "Epoch 1238, Loss: 0.5300321951508522, Final Batch Loss: 0.19154790043830872\n",
      "Epoch 1239, Loss: 0.5300706923007965, Final Batch Loss: 0.17018482089042664\n",
      "Epoch 1240, Loss: 0.5145389661192894, Final Batch Loss: 0.15989963710308075\n",
      "Epoch 1241, Loss: 0.49787843972444534, Final Batch Loss: 0.12015128880739212\n",
      "Epoch 1242, Loss: 0.6994008421897888, Final Batch Loss: 0.29491379857063293\n",
      "Epoch 1243, Loss: 0.4577020928263664, Final Batch Loss: 0.10517169535160065\n",
      "Epoch 1244, Loss: 0.5268229693174362, Final Batch Loss: 0.18679411709308624\n",
      "Epoch 1245, Loss: 0.5401047617197037, Final Batch Loss: 0.12536852061748505\n",
      "Epoch 1246, Loss: 0.512256883084774, Final Batch Loss: 0.13360446691513062\n",
      "Epoch 1247, Loss: 0.472190510481596, Final Batch Loss: 0.05977841094136238\n",
      "Epoch 1248, Loss: 0.5009994730353355, Final Batch Loss: 0.08010244369506836\n",
      "Epoch 1249, Loss: 0.5331351310014725, Final Batch Loss: 0.0629236102104187\n",
      "Epoch 1250, Loss: 0.6822093278169632, Final Batch Loss: 0.3043995797634125\n",
      "Epoch 1251, Loss: 0.5216605141758919, Final Batch Loss: 0.08725252002477646\n",
      "Epoch 1252, Loss: 0.39778389781713486, Final Batch Loss: 0.06517553329467773\n",
      "Epoch 1253, Loss: 0.5162760317325592, Final Batch Loss: 0.11573264002799988\n",
      "Epoch 1254, Loss: 0.4401455894112587, Final Batch Loss: 0.10345464944839478\n",
      "Epoch 1255, Loss: 0.5116896405816078, Final Batch Loss: 0.18401749432086945\n",
      "Epoch 1256, Loss: 0.5156953781843185, Final Batch Loss: 0.19026319682598114\n",
      "Epoch 1257, Loss: 0.46630748361349106, Final Batch Loss: 0.10745526105165482\n",
      "Epoch 1258, Loss: 0.4981910213828087, Final Batch Loss: 0.13144342601299286\n",
      "Epoch 1259, Loss: 0.42832987010478973, Final Batch Loss: 0.07262317836284637\n",
      "Epoch 1260, Loss: 0.5193627402186394, Final Batch Loss: 0.18422962725162506\n",
      "Epoch 1261, Loss: 0.5830243229866028, Final Batch Loss: 0.10738341510295868\n",
      "Epoch 1262, Loss: 0.5587226003408432, Final Batch Loss: 0.13717880845069885\n",
      "Epoch 1263, Loss: 0.430487759411335, Final Batch Loss: 0.11063424497842789\n",
      "Epoch 1264, Loss: 0.6333316564559937, Final Batch Loss: 0.19078095257282257\n",
      "Epoch 1265, Loss: 0.5529615730047226, Final Batch Loss: 0.18098853528499603\n",
      "Epoch 1266, Loss: 0.5223449766635895, Final Batch Loss: 0.1704760044813156\n",
      "Epoch 1267, Loss: 0.4358658269047737, Final Batch Loss: 0.13243383169174194\n",
      "Epoch 1268, Loss: 0.5162169113755226, Final Batch Loss: 0.15148741006851196\n",
      "Epoch 1269, Loss: 0.484307125210762, Final Batch Loss: 0.10114007443189621\n",
      "Epoch 1270, Loss: 0.5209357440471649, Final Batch Loss: 0.10974616557359695\n",
      "Epoch 1271, Loss: 0.48514699190855026, Final Batch Loss: 0.15043827891349792\n",
      "Epoch 1272, Loss: 0.4650851637125015, Final Batch Loss: 0.07767348736524582\n",
      "Epoch 1273, Loss: 0.48108743131160736, Final Batch Loss: 0.1255195140838623\n",
      "Epoch 1274, Loss: 0.6186065748333931, Final Batch Loss: 0.22910474240779877\n",
      "Epoch 1275, Loss: 0.46436236798763275, Final Batch Loss: 0.12051365524530411\n",
      "Epoch 1276, Loss: 0.4906756803393364, Final Batch Loss: 0.09325375407934189\n",
      "Epoch 1277, Loss: 0.5538115799427032, Final Batch Loss: 0.17089064419269562\n",
      "Epoch 1278, Loss: 0.444364458322525, Final Batch Loss: 0.10270922631025314\n",
      "Epoch 1279, Loss: 0.4921625703573227, Final Batch Loss: 0.16142116487026215\n",
      "Epoch 1280, Loss: 0.49350661784410477, Final Batch Loss: 0.10528798401355743\n",
      "Epoch 1281, Loss: 0.5179497748613358, Final Batch Loss: 0.1040676012635231\n",
      "Epoch 1282, Loss: 0.5689160227775574, Final Batch Loss: 0.17981065809726715\n",
      "Epoch 1283, Loss: 0.5509485900402069, Final Batch Loss: 0.13227730989456177\n",
      "Epoch 1284, Loss: 0.5442970246076584, Final Batch Loss: 0.16661573946475983\n",
      "Epoch 1285, Loss: 0.4000566452741623, Final Batch Loss: 0.0949523001909256\n",
      "Epoch 1286, Loss: 0.4195440784096718, Final Batch Loss: 0.08125656098127365\n",
      "Epoch 1287, Loss: 0.5371700450778008, Final Batch Loss: 0.1790085732936859\n",
      "Epoch 1288, Loss: 0.4320167973637581, Final Batch Loss: 0.09141161292791367\n",
      "Epoch 1289, Loss: 0.5482112988829613, Final Batch Loss: 0.12503156065940857\n",
      "Epoch 1290, Loss: 0.38807591050863266, Final Batch Loss: 0.10901299118995667\n",
      "Epoch 1291, Loss: 0.4308483526110649, Final Batch Loss: 0.09450574219226837\n",
      "Epoch 1292, Loss: 0.399361964315176, Final Batch Loss: 0.05548129603266716\n",
      "Epoch 1293, Loss: 0.4849727973341942, Final Batch Loss: 0.0683654323220253\n",
      "Epoch 1294, Loss: 0.40124591439962387, Final Batch Loss: 0.08254916965961456\n",
      "Epoch 1295, Loss: 0.41385579109191895, Final Batch Loss: 0.08836668729782104\n",
      "Epoch 1296, Loss: 0.6021822988986969, Final Batch Loss: 0.1615195870399475\n",
      "Epoch 1297, Loss: 0.47755108028650284, Final Batch Loss: 0.11894064396619797\n",
      "Epoch 1298, Loss: 0.6483728513121605, Final Batch Loss: 0.2398250699043274\n",
      "Epoch 1299, Loss: 0.49669089168310165, Final Batch Loss: 0.14610642194747925\n",
      "Epoch 1300, Loss: 0.4766339361667633, Final Batch Loss: 0.15940964221954346\n",
      "Epoch 1301, Loss: 0.4706016257405281, Final Batch Loss: 0.07417131960391998\n",
      "Epoch 1302, Loss: 0.4659838378429413, Final Batch Loss: 0.11472737789154053\n",
      "Epoch 1303, Loss: 0.5474963039159775, Final Batch Loss: 0.08265992254018784\n",
      "Epoch 1304, Loss: 0.504308432340622, Final Batch Loss: 0.1636200100183487\n",
      "Epoch 1305, Loss: 0.6581523567438126, Final Batch Loss: 0.2105887532234192\n",
      "Epoch 1306, Loss: 0.37329964339733124, Final Batch Loss: 0.12964613735675812\n",
      "Epoch 1307, Loss: 0.45115725696086884, Final Batch Loss: 0.15949344635009766\n",
      "Epoch 1308, Loss: 0.4676312953233719, Final Batch Loss: 0.12894107401371002\n",
      "Epoch 1309, Loss: 0.40445683896541595, Final Batch Loss: 0.13568827509880066\n",
      "Epoch 1310, Loss: 0.47659818828105927, Final Batch Loss: 0.16101083159446716\n",
      "Epoch 1311, Loss: 0.501342311501503, Final Batch Loss: 0.1079755648970604\n",
      "Epoch 1312, Loss: 0.4537593796849251, Final Batch Loss: 0.15676863491535187\n",
      "Epoch 1313, Loss: 0.46802569925785065, Final Batch Loss: 0.09277769178152084\n",
      "Epoch 1314, Loss: 0.5022981315851212, Final Batch Loss: 0.0676484927535057\n",
      "Epoch 1315, Loss: 0.5457220077514648, Final Batch Loss: 0.15601207315921783\n",
      "Epoch 1316, Loss: 0.48209258913993835, Final Batch Loss: 0.1276940554380417\n",
      "Epoch 1317, Loss: 0.49487362802028656, Final Batch Loss: 0.14852295815944672\n",
      "Epoch 1318, Loss: 0.5552956312894821, Final Batch Loss: 0.15825816988945007\n",
      "Epoch 1319, Loss: 0.509257361292839, Final Batch Loss: 0.1534494012594223\n",
      "Epoch 1320, Loss: 0.5627439767122269, Final Batch Loss: 0.16009922325611115\n",
      "Epoch 1321, Loss: 0.6640932112932205, Final Batch Loss: 0.14943747222423553\n",
      "Epoch 1322, Loss: 0.49796006828546524, Final Batch Loss: 0.09510895609855652\n",
      "Epoch 1323, Loss: 0.4933789372444153, Final Batch Loss: 0.09144552797079086\n",
      "Epoch 1324, Loss: 0.4990513250231743, Final Batch Loss: 0.1846856325864792\n",
      "Epoch 1325, Loss: 0.41600069403648376, Final Batch Loss: 0.08175253868103027\n",
      "Epoch 1326, Loss: 0.48490068316459656, Final Batch Loss: 0.12110723555088043\n",
      "Epoch 1327, Loss: 0.46644391119480133, Final Batch Loss: 0.15185032784938812\n",
      "Epoch 1328, Loss: 0.4881753996014595, Final Batch Loss: 0.14721614122390747\n",
      "Epoch 1329, Loss: 0.38856182247400284, Final Batch Loss: 0.08307803422212601\n",
      "Epoch 1330, Loss: 0.4514690041542053, Final Batch Loss: 0.09491891413927078\n",
      "Epoch 1331, Loss: 0.3738730847835541, Final Batch Loss: 0.09432747960090637\n",
      "Epoch 1332, Loss: 0.6152878701686859, Final Batch Loss: 0.07881531119346619\n",
      "Epoch 1333, Loss: 0.5651505440473557, Final Batch Loss: 0.15749509632587433\n",
      "Epoch 1334, Loss: 0.6025021150708199, Final Batch Loss: 0.17557722330093384\n",
      "Epoch 1335, Loss: 0.48670728504657745, Final Batch Loss: 0.12176629155874252\n",
      "Epoch 1336, Loss: 0.42954009771347046, Final Batch Loss: 0.08027853071689606\n",
      "Epoch 1337, Loss: 0.5552147477865219, Final Batch Loss: 0.12735582888126373\n",
      "Epoch 1338, Loss: 0.44702519848942757, Final Batch Loss: 0.06142645701766014\n",
      "Epoch 1339, Loss: 0.5839599221944809, Final Batch Loss: 0.1292763650417328\n",
      "Epoch 1340, Loss: 0.4051410108804703, Final Batch Loss: 0.12402074784040451\n",
      "Epoch 1341, Loss: 0.48654426634311676, Final Batch Loss: 0.16283102333545685\n",
      "Epoch 1342, Loss: 0.4621470496058464, Final Batch Loss: 0.08481084555387497\n",
      "Epoch 1343, Loss: 0.5144594237208366, Final Batch Loss: 0.09757869690656662\n",
      "Epoch 1344, Loss: 0.5606510415673256, Final Batch Loss: 0.13338592648506165\n",
      "Epoch 1345, Loss: 0.5327090285718441, Final Batch Loss: 0.057531263679265976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1346, Loss: 0.5540878474712372, Final Batch Loss: 0.15765278041362762\n",
      "Epoch 1347, Loss: 0.4209713861346245, Final Batch Loss: 0.07446292787790298\n",
      "Epoch 1348, Loss: 0.5207292139530182, Final Batch Loss: 0.24868243932724\n",
      "Epoch 1349, Loss: 0.5205445140600204, Final Batch Loss: 0.14331239461898804\n",
      "Epoch 1350, Loss: 0.4672503247857094, Final Batch Loss: 0.1528359055519104\n",
      "Epoch 1351, Loss: 0.4403808228671551, Final Batch Loss: 0.1551748365163803\n",
      "Epoch 1352, Loss: 0.5776679962873459, Final Batch Loss: 0.15852290391921997\n",
      "Epoch 1353, Loss: 0.47185181826353073, Final Batch Loss: 0.104585200548172\n",
      "Epoch 1354, Loss: 0.4906332939863205, Final Batch Loss: 0.2150125652551651\n",
      "Epoch 1355, Loss: 0.5937569662928581, Final Batch Loss: 0.15248307585716248\n",
      "Epoch 1356, Loss: 0.39652271941304207, Final Batch Loss: 0.10124186426401138\n",
      "Epoch 1357, Loss: 0.5369207262992859, Final Batch Loss: 0.1275700330734253\n",
      "Epoch 1358, Loss: 0.506583459675312, Final Batch Loss: 0.08746308833360672\n",
      "Epoch 1359, Loss: 0.4642370939254761, Final Batch Loss: 0.11916134506464005\n",
      "Epoch 1360, Loss: 0.46103325486183167, Final Batch Loss: 0.056266434490680695\n",
      "Epoch 1361, Loss: 0.6641881614923477, Final Batch Loss: 0.20073553919792175\n",
      "Epoch 1362, Loss: 0.42232199758291245, Final Batch Loss: 0.1421910524368286\n",
      "Epoch 1363, Loss: 0.4527062252163887, Final Batch Loss: 0.09460871666669846\n",
      "Epoch 1364, Loss: 0.371528685092926, Final Batch Loss: 0.07103956490755081\n",
      "Epoch 1365, Loss: 0.5162916481494904, Final Batch Loss: 0.19861061871051788\n",
      "Epoch 1366, Loss: 0.4989074394106865, Final Batch Loss: 0.11073395609855652\n",
      "Epoch 1367, Loss: 0.4547344334423542, Final Batch Loss: 0.04616754129528999\n",
      "Epoch 1368, Loss: 0.4826909676194191, Final Batch Loss: 0.16662730276584625\n",
      "Epoch 1369, Loss: 0.4685041606426239, Final Batch Loss: 0.13841287791728973\n",
      "Epoch 1370, Loss: 0.4654579758644104, Final Batch Loss: 0.1374063342809677\n",
      "Epoch 1371, Loss: 0.4397926330566406, Final Batch Loss: 0.1354008913040161\n",
      "Epoch 1372, Loss: 0.42326728254556656, Final Batch Loss: 0.10070734471082687\n",
      "Epoch 1373, Loss: 0.4988052248954773, Final Batch Loss: 0.18804550170898438\n",
      "Epoch 1374, Loss: 0.46566156297922134, Final Batch Loss: 0.10429070144891739\n",
      "Epoch 1375, Loss: 0.38767583668231964, Final Batch Loss: 0.03448496013879776\n",
      "Epoch 1376, Loss: 0.6556065902113914, Final Batch Loss: 0.21980972588062286\n",
      "Epoch 1377, Loss: 0.44125644862651825, Final Batch Loss: 0.08039751648902893\n",
      "Epoch 1378, Loss: 0.4286266565322876, Final Batch Loss: 0.07794340699911118\n",
      "Epoch 1379, Loss: 0.40759166330099106, Final Batch Loss: 0.07543043792247772\n",
      "Epoch 1380, Loss: 0.5224450975656509, Final Batch Loss: 0.13491791486740112\n",
      "Epoch 1381, Loss: 0.5392677336931229, Final Batch Loss: 0.11140179634094238\n",
      "Epoch 1382, Loss: 0.5207943022251129, Final Batch Loss: 0.15039432048797607\n",
      "Epoch 1383, Loss: 0.7397747188806534, Final Batch Loss: 0.28798338770866394\n",
      "Epoch 1384, Loss: 0.4839819595217705, Final Batch Loss: 0.1358082890510559\n",
      "Epoch 1385, Loss: 0.4898676946759224, Final Batch Loss: 0.1036813035607338\n",
      "Epoch 1386, Loss: 0.3662826418876648, Final Batch Loss: 0.08188996464014053\n",
      "Epoch 1387, Loss: 0.6317146569490433, Final Batch Loss: 0.2931821644306183\n",
      "Epoch 1388, Loss: 0.4365309253334999, Final Batch Loss: 0.10957524180412292\n",
      "Epoch 1389, Loss: 0.4381073862314224, Final Batch Loss: 0.07301855832338333\n",
      "Epoch 1390, Loss: 0.6409731507301331, Final Batch Loss: 0.1649937629699707\n",
      "Epoch 1391, Loss: 0.5488862320780754, Final Batch Loss: 0.12659332156181335\n",
      "Epoch 1392, Loss: 0.42175978422164917, Final Batch Loss: 0.08745449036359787\n",
      "Epoch 1393, Loss: 0.49955521523952484, Final Batch Loss: 0.12833821773529053\n",
      "Epoch 1394, Loss: 0.5923746228218079, Final Batch Loss: 0.15693750977516174\n",
      "Epoch 1395, Loss: 0.6340074613690376, Final Batch Loss: 0.275454580783844\n",
      "Epoch 1396, Loss: 0.4734228476881981, Final Batch Loss: 0.10677701234817505\n",
      "Epoch 1397, Loss: 0.5549846664071083, Final Batch Loss: 0.17388589680194855\n",
      "Epoch 1398, Loss: 0.42041852325201035, Final Batch Loss: 0.04059098660945892\n",
      "Epoch 1399, Loss: 0.5484455227851868, Final Batch Loss: 0.22916008532047272\n",
      "Epoch 1400, Loss: 0.518263041973114, Final Batch Loss: 0.15293987095355988\n",
      "Epoch 1401, Loss: 0.5285694673657417, Final Batch Loss: 0.16440191864967346\n",
      "Epoch 1402, Loss: 0.48859722167253494, Final Batch Loss: 0.12810483574867249\n",
      "Epoch 1403, Loss: 0.5277965217828751, Final Batch Loss: 0.14017802476882935\n",
      "Epoch 1404, Loss: 0.4901898503303528, Final Batch Loss: 0.16447284817695618\n",
      "Epoch 1405, Loss: 0.48886872082948685, Final Batch Loss: 0.10509797930717468\n",
      "Epoch 1406, Loss: 0.5371275767683983, Final Batch Loss: 0.16963529586791992\n",
      "Epoch 1407, Loss: 0.39238113909959793, Final Batch Loss: 0.14587710797786713\n",
      "Epoch 1408, Loss: 0.5318018421530724, Final Batch Loss: 0.14667056500911713\n",
      "Epoch 1409, Loss: 0.4599040076136589, Final Batch Loss: 0.11212051659822464\n",
      "Epoch 1410, Loss: 0.4366220086812973, Final Batch Loss: 0.06312937289476395\n",
      "Epoch 1411, Loss: 0.44755014032125473, Final Batch Loss: 0.13739891350269318\n",
      "Epoch 1412, Loss: 0.4621903672814369, Final Batch Loss: 0.08935815095901489\n",
      "Epoch 1413, Loss: 0.43576765805482864, Final Batch Loss: 0.10713523626327515\n",
      "Epoch 1414, Loss: 0.41123582422733307, Final Batch Loss: 0.07971865683794022\n",
      "Epoch 1415, Loss: 0.48784852772951126, Final Batch Loss: 0.1791726052761078\n",
      "Epoch 1416, Loss: 0.5043115615844727, Final Batch Loss: 0.10935934633016586\n",
      "Epoch 1417, Loss: 0.5251705273985863, Final Batch Loss: 0.21770787239074707\n",
      "Epoch 1418, Loss: 0.4761338233947754, Final Batch Loss: 0.09715250134468079\n",
      "Epoch 1419, Loss: 0.4235881268978119, Final Batch Loss: 0.13789746165275574\n",
      "Epoch 1420, Loss: 0.5112141668796539, Final Batch Loss: 0.13047276437282562\n",
      "Epoch 1421, Loss: 0.47934557497501373, Final Batch Loss: 0.17040151357650757\n",
      "Epoch 1422, Loss: 0.4014882892370224, Final Batch Loss: 0.08324208855628967\n",
      "Epoch 1423, Loss: 0.5887691155076027, Final Batch Loss: 0.17566511034965515\n",
      "Epoch 1424, Loss: 0.42444049566984177, Final Batch Loss: 0.07287099212408066\n",
      "Epoch 1425, Loss: 0.5377576500177383, Final Batch Loss: 0.17982792854309082\n",
      "Epoch 1426, Loss: 0.41554078459739685, Final Batch Loss: 0.09459078311920166\n",
      "Epoch 1427, Loss: 0.43817199021577835, Final Batch Loss: 0.08876043558120728\n",
      "Epoch 1428, Loss: 0.4671746864914894, Final Batch Loss: 0.1381973922252655\n",
      "Epoch 1429, Loss: 0.5484256744384766, Final Batch Loss: 0.15561145544052124\n",
      "Epoch 1430, Loss: 0.4960876554250717, Final Batch Loss: 0.14274322986602783\n",
      "Epoch 1431, Loss: 0.4126129150390625, Final Batch Loss: 0.08651762455701828\n",
      "Epoch 1432, Loss: 0.46254856139421463, Final Batch Loss: 0.10191397368907928\n",
      "Epoch 1433, Loss: 0.4375738129019737, Final Batch Loss: 0.07619347423315048\n",
      "Epoch 1434, Loss: 0.5345612615346909, Final Batch Loss: 0.1575184166431427\n",
      "Epoch 1435, Loss: 0.4378684312105179, Final Batch Loss: 0.09313176572322845\n",
      "Epoch 1436, Loss: 0.4667193219065666, Final Batch Loss: 0.10044196248054504\n",
      "Epoch 1437, Loss: 0.4619232639670372, Final Batch Loss: 0.11518853902816772\n",
      "Epoch 1438, Loss: 0.42783477902412415, Final Batch Loss: 0.12665316462516785\n",
      "Epoch 1439, Loss: 0.5793446749448776, Final Batch Loss: 0.20591075718402863\n",
      "Epoch 1440, Loss: 0.4310498610138893, Final Batch Loss: 0.0708276778459549\n",
      "Epoch 1441, Loss: 0.527227483689785, Final Batch Loss: 0.13985954225063324\n",
      "Epoch 1442, Loss: 0.4856679141521454, Final Batch Loss: 0.12812016904354095\n",
      "Epoch 1443, Loss: 0.565313421189785, Final Batch Loss: 0.1190170869231224\n",
      "Epoch 1444, Loss: 0.4940847232937813, Final Batch Loss: 0.08226579427719116\n",
      "Epoch 1445, Loss: 0.41859646886587143, Final Batch Loss: 0.12546785175800323\n",
      "Epoch 1446, Loss: 0.4906188026070595, Final Batch Loss: 0.11255311220884323\n",
      "Epoch 1447, Loss: 0.4479023888707161, Final Batch Loss: 0.08982694149017334\n",
      "Epoch 1448, Loss: 0.47345536947250366, Final Batch Loss: 0.11700659245252609\n",
      "Epoch 1449, Loss: 0.5428693369030952, Final Batch Loss: 0.1449199765920639\n",
      "Epoch 1450, Loss: 0.44451044499874115, Final Batch Loss: 0.10599332302808762\n",
      "Epoch 1451, Loss: 0.4884578213095665, Final Batch Loss: 0.08614058792591095\n",
      "Epoch 1452, Loss: 0.36910638958215714, Final Batch Loss: 0.09568314254283905\n",
      "Epoch 1453, Loss: 0.4358287379145622, Final Batch Loss: 0.07536447048187256\n",
      "Epoch 1454, Loss: 0.3299851156771183, Final Batch Loss: 0.03828931227326393\n",
      "Epoch 1455, Loss: 0.4816775247454643, Final Batch Loss: 0.0901152640581131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1456, Loss: 0.571976937353611, Final Batch Loss: 0.1443488746881485\n",
      "Epoch 1457, Loss: 0.47239358723163605, Final Batch Loss: 0.11104679107666016\n",
      "Epoch 1458, Loss: 0.4272575229406357, Final Batch Loss: 0.10091044008731842\n",
      "Epoch 1459, Loss: 0.4380682557821274, Final Batch Loss: 0.06902579218149185\n",
      "Epoch 1460, Loss: 0.39632365852594376, Final Batch Loss: 0.08180147409439087\n",
      "Epoch 1461, Loss: 0.5024144947528839, Final Batch Loss: 0.08069047331809998\n",
      "Epoch 1462, Loss: 0.44736728072166443, Final Batch Loss: 0.15236392617225647\n",
      "Epoch 1463, Loss: 0.5500708892941475, Final Batch Loss: 0.09410146623849869\n",
      "Epoch 1464, Loss: 0.4573544003069401, Final Batch Loss: 0.06213284656405449\n",
      "Epoch 1465, Loss: 0.47191331535577774, Final Batch Loss: 0.10241939127445221\n",
      "Epoch 1466, Loss: 0.4345437064766884, Final Batch Loss: 0.11757570505142212\n",
      "Epoch 1467, Loss: 0.49540823698043823, Final Batch Loss: 0.20572611689567566\n",
      "Epoch 1468, Loss: 0.45039333403110504, Final Batch Loss: 0.15910157561302185\n",
      "Epoch 1469, Loss: 0.5292764753103256, Final Batch Loss: 0.0998198613524437\n",
      "Epoch 1470, Loss: 0.4743962585926056, Final Batch Loss: 0.14480061829090118\n",
      "Epoch 1471, Loss: 0.5687289834022522, Final Batch Loss: 0.1266302764415741\n",
      "Epoch 1472, Loss: 0.5075058117508888, Final Batch Loss: 0.09723099321126938\n",
      "Epoch 1473, Loss: 0.48361077904701233, Final Batch Loss: 0.15602028369903564\n",
      "Epoch 1474, Loss: 0.505282573401928, Final Batch Loss: 0.07940685003995895\n",
      "Epoch 1475, Loss: 0.43698879331350327, Final Batch Loss: 0.11376045644283295\n",
      "Epoch 1476, Loss: 0.4982174411416054, Final Batch Loss: 0.09124010801315308\n",
      "Epoch 1477, Loss: 0.4744812771677971, Final Batch Loss: 0.13494032621383667\n",
      "Epoch 1478, Loss: 0.5400246381759644, Final Batch Loss: 0.1582145243883133\n",
      "Epoch 1479, Loss: 0.5410432666540146, Final Batch Loss: 0.12824887037277222\n",
      "Epoch 1480, Loss: 0.4789778143167496, Final Batch Loss: 0.08175726979970932\n",
      "Epoch 1481, Loss: 0.4253470301628113, Final Batch Loss: 0.1082116886973381\n",
      "Epoch 1482, Loss: 0.46306484937667847, Final Batch Loss: 0.07875507324934006\n",
      "Epoch 1483, Loss: 0.4195006266236305, Final Batch Loss: 0.15312698483467102\n",
      "Epoch 1484, Loss: 0.42747438699007034, Final Batch Loss: 0.09163758158683777\n",
      "Epoch 1485, Loss: 0.43251754343509674, Final Batch Loss: 0.17004263401031494\n",
      "Epoch 1486, Loss: 0.5342764034867287, Final Batch Loss: 0.16853199899196625\n",
      "Epoch 1487, Loss: 0.6264470964670181, Final Batch Loss: 0.24403232336044312\n",
      "Epoch 1488, Loss: 0.5056394189596176, Final Batch Loss: 0.05947801470756531\n",
      "Epoch 1489, Loss: 0.42565272748470306, Final Batch Loss: 0.07647392153739929\n",
      "Epoch 1490, Loss: 0.4519747868180275, Final Batch Loss: 0.10096879303455353\n",
      "Epoch 1491, Loss: 0.4595189765095711, Final Batch Loss: 0.1266399472951889\n",
      "Epoch 1492, Loss: 0.3953585736453533, Final Batch Loss: 0.06205287203192711\n",
      "Epoch 1493, Loss: 0.5073480308055878, Final Batch Loss: 0.162308007478714\n",
      "Epoch 1494, Loss: 0.5723131820559502, Final Batch Loss: 0.21586893498897552\n",
      "Epoch 1495, Loss: 0.4808404818177223, Final Batch Loss: 0.13103850185871124\n",
      "Epoch 1496, Loss: 0.5231514573097229, Final Batch Loss: 0.1382148116827011\n",
      "Epoch 1497, Loss: 0.4643709994852543, Final Batch Loss: 0.03427848592400551\n",
      "Epoch 1498, Loss: 0.5920598581433296, Final Batch Loss: 0.2682214677333832\n",
      "Epoch 1499, Loss: 0.4473574534058571, Final Batch Loss: 0.19073882699012756\n",
      "Epoch 1500, Loss: 0.5352851450443268, Final Batch Loss: 0.18252938985824585\n",
      "Epoch 1501, Loss: 0.5514723062515259, Final Batch Loss: 0.1755572259426117\n",
      "Epoch 1502, Loss: 0.5284284427762032, Final Batch Loss: 0.16037683188915253\n",
      "Epoch 1503, Loss: 0.6210950538516045, Final Batch Loss: 0.16575954854488373\n",
      "Epoch 1504, Loss: 0.5078643336892128, Final Batch Loss: 0.15393346548080444\n",
      "Epoch 1505, Loss: 0.5933280438184738, Final Batch Loss: 0.1603391319513321\n",
      "Epoch 1506, Loss: 0.4616403952240944, Final Batch Loss: 0.11790719628334045\n",
      "Epoch 1507, Loss: 0.38302601128816605, Final Batch Loss: 0.03912179172039032\n",
      "Epoch 1508, Loss: 0.39574120193719864, Final Batch Loss: 0.05236472189426422\n",
      "Epoch 1509, Loss: 0.6480825766921043, Final Batch Loss: 0.2986353635787964\n",
      "Epoch 1510, Loss: 0.4893760532140732, Final Batch Loss: 0.1452808380126953\n",
      "Epoch 1511, Loss: 0.3320668637752533, Final Batch Loss: 0.05324140936136246\n",
      "Epoch 1512, Loss: 0.4596056789159775, Final Batch Loss: 0.12470665574073792\n",
      "Epoch 1513, Loss: 0.4701519086956978, Final Batch Loss: 0.09602914750576019\n",
      "Epoch 1514, Loss: 0.46993548423051834, Final Batch Loss: 0.1360636055469513\n",
      "Epoch 1515, Loss: 0.46818043291568756, Final Batch Loss: 0.09444015473127365\n",
      "Epoch 1516, Loss: 0.4304676502943039, Final Batch Loss: 0.11160704493522644\n",
      "Epoch 1517, Loss: 0.521681260317564, Final Batch Loss: 0.2029573619365692\n",
      "Epoch 1518, Loss: 0.4775760918855667, Final Batch Loss: 0.12107836455106735\n",
      "Epoch 1519, Loss: 0.5032521039247513, Final Batch Loss: 0.12778803706169128\n",
      "Epoch 1520, Loss: 0.3819391056895256, Final Batch Loss: 0.06646658480167389\n",
      "Epoch 1521, Loss: 0.46335040777921677, Final Batch Loss: 0.07574047148227692\n",
      "Epoch 1522, Loss: 0.3887047953903675, Final Batch Loss: 0.06134862080216408\n",
      "Epoch 1523, Loss: 0.44700947403907776, Final Batch Loss: 0.12148375809192657\n",
      "Epoch 1524, Loss: 0.5102265626192093, Final Batch Loss: 0.1615501493215561\n",
      "Epoch 1525, Loss: 0.49186329543590546, Final Batch Loss: 0.15074656903743744\n",
      "Epoch 1526, Loss: 0.45142511278390884, Final Batch Loss: 0.1008700281381607\n",
      "Epoch 1527, Loss: 0.45744791626930237, Final Batch Loss: 0.10470250993967056\n",
      "Epoch 1528, Loss: 0.42078350484371185, Final Batch Loss: 0.09931360185146332\n",
      "Epoch 1529, Loss: 0.4781164973974228, Final Batch Loss: 0.1051093265414238\n",
      "Epoch 1530, Loss: 0.3694342225790024, Final Batch Loss: 0.06148183345794678\n",
      "Epoch 1531, Loss: 0.4783268868923187, Final Batch Loss: 0.09593454003334045\n",
      "Epoch 1532, Loss: 0.4343412443995476, Final Batch Loss: 0.10138636082410812\n",
      "Epoch 1533, Loss: 0.4388980120420456, Final Batch Loss: 0.12274619191884995\n",
      "Epoch 1534, Loss: 0.3613497242331505, Final Batch Loss: 0.06441166996955872\n",
      "Epoch 1535, Loss: 0.3484261594712734, Final Batch Loss: 0.04405536875128746\n",
      "Epoch 1536, Loss: 0.5553137436509132, Final Batch Loss: 0.17056772112846375\n",
      "Epoch 1537, Loss: 0.39978543668985367, Final Batch Loss: 0.09162919968366623\n",
      "Epoch 1538, Loss: 0.3950387015938759, Final Batch Loss: 0.10166797041893005\n",
      "Epoch 1539, Loss: 0.4452616050839424, Final Batch Loss: 0.13811036944389343\n",
      "Epoch 1540, Loss: 0.4064757078886032, Final Batch Loss: 0.0855361819267273\n",
      "Epoch 1541, Loss: 0.36104821413755417, Final Batch Loss: 0.07180770486593246\n",
      "Epoch 1542, Loss: 0.42015744745731354, Final Batch Loss: 0.10502272099256516\n",
      "Epoch 1543, Loss: 0.4577684625983238, Final Batch Loss: 0.08482915163040161\n",
      "Epoch 1544, Loss: 0.43075133115053177, Final Batch Loss: 0.14699985086917877\n",
      "Epoch 1545, Loss: 0.5153044760227203, Final Batch Loss: 0.1738973706960678\n",
      "Epoch 1546, Loss: 0.39355334639549255, Final Batch Loss: 0.09281344711780548\n",
      "Epoch 1547, Loss: 0.6537138223648071, Final Batch Loss: 0.2276884913444519\n",
      "Epoch 1548, Loss: 0.5553037971258163, Final Batch Loss: 0.1587965041399002\n",
      "Epoch 1549, Loss: 0.3940582796931267, Final Batch Loss: 0.09138578176498413\n",
      "Epoch 1550, Loss: 0.4967777505517006, Final Batch Loss: 0.21488548815250397\n",
      "Epoch 1551, Loss: 0.3489915654063225, Final Batch Loss: 0.07243868708610535\n",
      "Epoch 1552, Loss: 0.44980838894844055, Final Batch Loss: 0.06769523024559021\n",
      "Epoch 1553, Loss: 0.5396931022405624, Final Batch Loss: 0.13432954251766205\n",
      "Epoch 1554, Loss: 0.44524959847331047, Final Batch Loss: 0.15177014470100403\n",
      "Epoch 1555, Loss: 0.4347670152783394, Final Batch Loss: 0.09324653446674347\n",
      "Epoch 1556, Loss: 0.4637753963470459, Final Batch Loss: 0.09709165245294571\n",
      "Epoch 1557, Loss: 0.48476044833660126, Final Batch Loss: 0.06855650991201401\n",
      "Epoch 1558, Loss: 0.4510543420910835, Final Batch Loss: 0.10402493178844452\n",
      "Epoch 1559, Loss: 0.49134308844804764, Final Batch Loss: 0.12436439096927643\n",
      "Epoch 1560, Loss: 0.48919302225112915, Final Batch Loss: 0.1626998484134674\n",
      "Epoch 1561, Loss: 0.43787001073360443, Final Batch Loss: 0.07690273225307465\n",
      "Epoch 1562, Loss: 0.539355106651783, Final Batch Loss: 0.15915057063102722\n",
      "Epoch 1563, Loss: 0.4598260372877121, Final Batch Loss: 0.06422138214111328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1564, Loss: 0.3668038360774517, Final Batch Loss: 0.049879077821969986\n",
      "Epoch 1565, Loss: 0.4520398899912834, Final Batch Loss: 0.10693912953138351\n",
      "Epoch 1566, Loss: 0.432494904845953, Final Batch Loss: 0.20061685144901276\n",
      "Epoch 1567, Loss: 0.35264336317777634, Final Batch Loss: 0.07970363646745682\n",
      "Epoch 1568, Loss: 0.48518361896276474, Final Batch Loss: 0.07204226404428482\n",
      "Epoch 1569, Loss: 0.5612867847084999, Final Batch Loss: 0.2045598030090332\n",
      "Epoch 1570, Loss: 0.41006071865558624, Final Batch Loss: 0.08712726831436157\n",
      "Epoch 1571, Loss: 0.39168839156627655, Final Batch Loss: 0.07035057246685028\n",
      "Epoch 1572, Loss: 0.5265659540891647, Final Batch Loss: 0.11378826200962067\n",
      "Epoch 1573, Loss: 0.3127148374915123, Final Batch Loss: 0.052697569131851196\n",
      "Epoch 1574, Loss: 0.5230390131473541, Final Batch Loss: 0.09528889507055283\n",
      "Epoch 1575, Loss: 0.4738747179508209, Final Batch Loss: 0.15610191226005554\n",
      "Epoch 1576, Loss: 0.43030454218387604, Final Batch Loss: 0.07894167304039001\n",
      "Epoch 1577, Loss: 0.3973907306790352, Final Batch Loss: 0.08137516677379608\n",
      "Epoch 1578, Loss: 0.43314049020409584, Final Batch Loss: 0.05154331400990486\n",
      "Epoch 1579, Loss: 0.3756243884563446, Final Batch Loss: 0.07790695130825043\n",
      "Epoch 1580, Loss: 0.45295166969299316, Final Batch Loss: 0.046988606452941895\n",
      "Epoch 1581, Loss: 0.4289569705724716, Final Batch Loss: 0.09984256327152252\n",
      "Epoch 1582, Loss: 0.32219136506319046, Final Batch Loss: 0.07961855828762054\n",
      "Epoch 1583, Loss: 0.49510248750448227, Final Batch Loss: 0.09033296257257462\n",
      "Epoch 1584, Loss: 0.65155378729105, Final Batch Loss: 0.3512100279331207\n",
      "Epoch 1585, Loss: 0.38108448684215546, Final Batch Loss: 0.09497793763875961\n",
      "Epoch 1586, Loss: 0.41728993132710457, Final Batch Loss: 0.059833329170942307\n",
      "Epoch 1587, Loss: 0.386784590780735, Final Batch Loss: 0.03515166789293289\n",
      "Epoch 1588, Loss: 0.33588822931051254, Final Batch Loss: 0.03332924842834473\n",
      "Epoch 1589, Loss: 0.4923277646303177, Final Batch Loss: 0.17717906832695007\n",
      "Epoch 1590, Loss: 0.36846792697906494, Final Batch Loss: 0.09111005812883377\n",
      "Epoch 1591, Loss: 0.5184808671474457, Final Batch Loss: 0.12373339384794235\n",
      "Epoch 1592, Loss: 0.3790149763226509, Final Batch Loss: 0.10802903026342392\n",
      "Epoch 1593, Loss: 0.3784281834959984, Final Batch Loss: 0.10392740368843079\n",
      "Epoch 1594, Loss: 0.4326254725456238, Final Batch Loss: 0.10277161002159119\n",
      "Epoch 1595, Loss: 0.4579463601112366, Final Batch Loss: 0.08608999848365784\n",
      "Epoch 1596, Loss: 0.46693187952041626, Final Batch Loss: 0.13282813131809235\n",
      "Epoch 1597, Loss: 0.42466332390904427, Final Batch Loss: 0.10116566717624664\n",
      "Epoch 1598, Loss: 0.4786239005625248, Final Batch Loss: 0.053556520491838455\n",
      "Epoch 1599, Loss: 0.3972359076142311, Final Batch Loss: 0.06816138327121735\n",
      "Epoch 1600, Loss: 0.452289916574955, Final Batch Loss: 0.10988212376832962\n",
      "Epoch 1601, Loss: 0.5434679388999939, Final Batch Loss: 0.11111820489168167\n",
      "Epoch 1602, Loss: 0.36272023618221283, Final Batch Loss: 0.057893842458724976\n",
      "Epoch 1603, Loss: 0.4371221289038658, Final Batch Loss: 0.13090969622135162\n",
      "Epoch 1604, Loss: 0.44514796137809753, Final Batch Loss: 0.11309131979942322\n",
      "Epoch 1605, Loss: 0.34131042286753654, Final Batch Loss: 0.05488859489560127\n",
      "Epoch 1606, Loss: 0.45946088433265686, Final Batch Loss: 0.05788370221853256\n",
      "Epoch 1607, Loss: 0.4058935195207596, Final Batch Loss: 0.07429676502943039\n",
      "Epoch 1608, Loss: 0.3990327790379524, Final Batch Loss: 0.10732679069042206\n",
      "Epoch 1609, Loss: 0.40376975387334824, Final Batch Loss: 0.18025542795658112\n",
      "Epoch 1610, Loss: 0.44117463380098343, Final Batch Loss: 0.09999100118875504\n",
      "Epoch 1611, Loss: 0.5116446800529957, Final Batch Loss: 0.05678090825676918\n",
      "Epoch 1612, Loss: 0.44048159569501877, Final Batch Loss: 0.09213367104530334\n",
      "Epoch 1613, Loss: 0.42810574173927307, Final Batch Loss: 0.10052040964365005\n",
      "Epoch 1614, Loss: 0.5071296691894531, Final Batch Loss: 0.08717285096645355\n",
      "Epoch 1615, Loss: 0.42054469883441925, Final Batch Loss: 0.09450389444828033\n",
      "Epoch 1616, Loss: 0.5256387814879417, Final Batch Loss: 0.1409274786710739\n",
      "Epoch 1617, Loss: 0.3395882323384285, Final Batch Loss: 0.08601265400648117\n",
      "Epoch 1618, Loss: 0.3914293572306633, Final Batch Loss: 0.12084553390741348\n",
      "Epoch 1619, Loss: 0.431787446141243, Final Batch Loss: 0.09982095658779144\n",
      "Epoch 1620, Loss: 0.43934500217437744, Final Batch Loss: 0.11350678652524948\n",
      "Epoch 1621, Loss: 0.48562660813331604, Final Batch Loss: 0.16195881366729736\n",
      "Epoch 1622, Loss: 0.45801424235105515, Final Batch Loss: 0.1354226917028427\n",
      "Epoch 1623, Loss: 0.4035484865307808, Final Batch Loss: 0.08835601806640625\n",
      "Epoch 1624, Loss: 0.32374123111367226, Final Batch Loss: 0.052312154322862625\n",
      "Epoch 1625, Loss: 0.42650818824768066, Final Batch Loss: 0.11031890660524368\n",
      "Epoch 1626, Loss: 0.5153237134218216, Final Batch Loss: 0.10928195714950562\n",
      "Epoch 1627, Loss: 0.463556244969368, Final Batch Loss: 0.1734633892774582\n",
      "Epoch 1628, Loss: 0.425775870680809, Final Batch Loss: 0.11685160547494888\n",
      "Epoch 1629, Loss: 0.46281544864177704, Final Batch Loss: 0.19295930862426758\n",
      "Epoch 1630, Loss: 0.4955346956849098, Final Batch Loss: 0.1369044929742813\n",
      "Epoch 1631, Loss: 0.4534936994314194, Final Batch Loss: 0.088630311191082\n",
      "Epoch 1632, Loss: 0.543769720941782, Final Batch Loss: 0.17950578033924103\n",
      "Epoch 1633, Loss: 0.3614533469080925, Final Batch Loss: 0.08269625902175903\n",
      "Epoch 1634, Loss: 0.5614555701613426, Final Batch Loss: 0.19359399378299713\n",
      "Epoch 1635, Loss: 0.4624068886041641, Final Batch Loss: 0.09395475685596466\n",
      "Epoch 1636, Loss: 0.3660689815878868, Final Batch Loss: 0.08271586149930954\n",
      "Epoch 1637, Loss: 0.5006250217556953, Final Batch Loss: 0.14482392370700836\n",
      "Epoch 1638, Loss: 0.37117481231689453, Final Batch Loss: 0.076954185962677\n",
      "Epoch 1639, Loss: 0.4273235872387886, Final Batch Loss: 0.099451944231987\n",
      "Epoch 1640, Loss: 0.4626905433833599, Final Batch Loss: 0.12052837759256363\n",
      "Epoch 1641, Loss: 0.4942656457424164, Final Batch Loss: 0.17328792810440063\n",
      "Epoch 1642, Loss: 0.4815507009625435, Final Batch Loss: 0.13425838947296143\n",
      "Epoch 1643, Loss: 0.36713719367980957, Final Batch Loss: 0.06588664650917053\n",
      "Epoch 1644, Loss: 0.5364814549684525, Final Batch Loss: 0.13079936802387238\n",
      "Epoch 1645, Loss: 0.557696521282196, Final Batch Loss: 0.24620594084262848\n",
      "Epoch 1646, Loss: 0.4392033778131008, Final Batch Loss: 0.05909347161650658\n",
      "Epoch 1647, Loss: 0.4560970291495323, Final Batch Loss: 0.13286975026130676\n",
      "Epoch 1648, Loss: 0.39446574449539185, Final Batch Loss: 0.09555082023143768\n",
      "Epoch 1649, Loss: 0.4767993539571762, Final Batch Loss: 0.06957713514566422\n",
      "Epoch 1650, Loss: 0.368473868817091, Final Batch Loss: 0.10289935022592545\n",
      "Epoch 1651, Loss: 0.48324117064476013, Final Batch Loss: 0.17142274975776672\n",
      "Epoch 1652, Loss: 0.4085766673088074, Final Batch Loss: 0.113202303647995\n",
      "Epoch 1653, Loss: 0.43371138721704483, Final Batch Loss: 0.1115533709526062\n",
      "Epoch 1654, Loss: 0.39160752668976784, Final Batch Loss: 0.04822802171111107\n",
      "Epoch 1655, Loss: 0.4139591008424759, Final Batch Loss: 0.10560187697410583\n",
      "Epoch 1656, Loss: 0.3522832989692688, Final Batch Loss: 0.04275790601968765\n",
      "Epoch 1657, Loss: 0.4178057946264744, Final Batch Loss: 0.11364928632974625\n",
      "Epoch 1658, Loss: 0.3169473260641098, Final Batch Loss: 0.05706324055790901\n",
      "Epoch 1659, Loss: 0.4671245291829109, Final Batch Loss: 0.1305810958147049\n",
      "Epoch 1660, Loss: 0.42033810168504715, Final Batch Loss: 0.14608585834503174\n",
      "Epoch 1661, Loss: 0.6261397525668144, Final Batch Loss: 0.29150354862213135\n",
      "Epoch 1662, Loss: 0.36228156834840775, Final Batch Loss: 0.0886896476149559\n",
      "Epoch 1663, Loss: 0.41702156513929367, Final Batch Loss: 0.07581330090761185\n",
      "Epoch 1664, Loss: 0.48799198120832443, Final Batch Loss: 0.11682334542274475\n",
      "Epoch 1665, Loss: 0.37398096174001694, Final Batch Loss: 0.08637499809265137\n",
      "Epoch 1666, Loss: 0.4815371632575989, Final Batch Loss: 0.09670528024435043\n",
      "Epoch 1667, Loss: 0.5283445417881012, Final Batch Loss: 0.22126713395118713\n",
      "Epoch 1668, Loss: 0.46420370042324066, Final Batch Loss: 0.07165410369634628\n",
      "Epoch 1669, Loss: 0.4544869661331177, Final Batch Loss: 0.15643221139907837\n",
      "Epoch 1670, Loss: 0.4106907248497009, Final Batch Loss: 0.11439843475818634\n",
      "Epoch 1671, Loss: 0.38916637003421783, Final Batch Loss: 0.09607003629207611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1672, Loss: 0.43458935618400574, Final Batch Loss: 0.12123453617095947\n",
      "Epoch 1673, Loss: 0.5403264686465263, Final Batch Loss: 0.2112852931022644\n",
      "Epoch 1674, Loss: 0.4705916941165924, Final Batch Loss: 0.07604777812957764\n",
      "Epoch 1675, Loss: 0.47415387630462646, Final Batch Loss: 0.13943031430244446\n",
      "Epoch 1676, Loss: 0.5032451674342155, Final Batch Loss: 0.1964106559753418\n",
      "Epoch 1677, Loss: 0.3966749291867018, Final Batch Loss: 0.028096957132220268\n",
      "Epoch 1678, Loss: 0.43826593458652496, Final Batch Loss: 0.08242994546890259\n",
      "Epoch 1679, Loss: 0.3866298496723175, Final Batch Loss: 0.1312939077615738\n",
      "Epoch 1680, Loss: 0.4210178554058075, Final Batch Loss: 0.07520323246717453\n",
      "Epoch 1681, Loss: 0.5095678940415382, Final Batch Loss: 0.18132048845291138\n",
      "Epoch 1682, Loss: 0.5093706846237183, Final Batch Loss: 0.13583976030349731\n",
      "Epoch 1683, Loss: 0.3704635761678219, Final Batch Loss: 0.049413781613111496\n",
      "Epoch 1684, Loss: 0.4838186651468277, Final Batch Loss: 0.10121948271989822\n",
      "Epoch 1685, Loss: 0.49161987751722336, Final Batch Loss: 0.0768841877579689\n",
      "Epoch 1686, Loss: 0.47093184292316437, Final Batch Loss: 0.12112146615982056\n",
      "Epoch 1687, Loss: 0.4760381951928139, Final Batch Loss: 0.0979544147849083\n",
      "Epoch 1688, Loss: 0.42296481877565384, Final Batch Loss: 0.0976276695728302\n",
      "Epoch 1689, Loss: 0.41959405690431595, Final Batch Loss: 0.052324630320072174\n",
      "Epoch 1690, Loss: 0.3853220045566559, Final Batch Loss: 0.05562622845172882\n",
      "Epoch 1691, Loss: 0.3502007946372032, Final Batch Loss: 0.046572379767894745\n",
      "Epoch 1692, Loss: 0.41292615234851837, Final Batch Loss: 0.10539356619119644\n",
      "Epoch 1693, Loss: 0.5460203811526299, Final Batch Loss: 0.22720815241336823\n",
      "Epoch 1694, Loss: 0.46762141585350037, Final Batch Loss: 0.12510910630226135\n",
      "Epoch 1695, Loss: 0.4179864823818207, Final Batch Loss: 0.07792969793081284\n",
      "Epoch 1696, Loss: 0.38538969308137894, Final Batch Loss: 0.07730041444301605\n",
      "Epoch 1697, Loss: 0.30695850029587746, Final Batch Loss: 0.06773852556943893\n",
      "Epoch 1698, Loss: 0.5717403516173363, Final Batch Loss: 0.25495341420173645\n",
      "Epoch 1699, Loss: 0.48317646980285645, Final Batch Loss: 0.1036573126912117\n",
      "Epoch 1700, Loss: 0.4666125550866127, Final Batch Loss: 0.14881004393100739\n",
      "Epoch 1701, Loss: 0.40718793869018555, Final Batch Loss: 0.06735475361347198\n",
      "Epoch 1702, Loss: 0.47051651775836945, Final Batch Loss: 0.11646562814712524\n",
      "Epoch 1703, Loss: 0.5326466485857964, Final Batch Loss: 0.10039328783750534\n",
      "Epoch 1704, Loss: 0.43974463641643524, Final Batch Loss: 0.14707443118095398\n",
      "Epoch 1705, Loss: 0.47666480392217636, Final Batch Loss: 0.10231762379407883\n",
      "Epoch 1706, Loss: 0.3830322176218033, Final Batch Loss: 0.08457217365503311\n",
      "Epoch 1707, Loss: 0.395502507686615, Final Batch Loss: 0.10710299015045166\n",
      "Epoch 1708, Loss: 0.3282350227236748, Final Batch Loss: 0.075428806245327\n",
      "Epoch 1709, Loss: 0.4604600891470909, Final Batch Loss: 0.13923682272434235\n",
      "Epoch 1710, Loss: 0.29288819059729576, Final Batch Loss: 0.0832848995923996\n",
      "Epoch 1711, Loss: 0.31783532723784447, Final Batch Loss: 0.052176956087350845\n",
      "Epoch 1712, Loss: 0.4237859770655632, Final Batch Loss: 0.06735245883464813\n",
      "Epoch 1713, Loss: 0.4776882454752922, Final Batch Loss: 0.10275205969810486\n",
      "Epoch 1714, Loss: 0.393849715590477, Final Batch Loss: 0.08583766967058182\n",
      "Epoch 1715, Loss: 0.365355484187603, Final Batch Loss: 0.035598210990428925\n",
      "Epoch 1716, Loss: 0.4583274573087692, Final Batch Loss: 0.11563464254140854\n",
      "Epoch 1717, Loss: 0.41499223560094833, Final Batch Loss: 0.10268821567296982\n",
      "Epoch 1718, Loss: 0.42158282548189163, Final Batch Loss: 0.07452652603387833\n",
      "Epoch 1719, Loss: 0.3537508212029934, Final Batch Loss: 0.033843714743852615\n",
      "Epoch 1720, Loss: 0.4158618673682213, Final Batch Loss: 0.08900894969701767\n",
      "Epoch 1721, Loss: 0.42500317841768265, Final Batch Loss: 0.1310306340456009\n",
      "Epoch 1722, Loss: 0.46087294071912766, Final Batch Loss: 0.20005446672439575\n",
      "Epoch 1723, Loss: 0.3720714747905731, Final Batch Loss: 0.07698699086904526\n",
      "Epoch 1724, Loss: 0.49696023017168045, Final Batch Loss: 0.1902313381433487\n",
      "Epoch 1725, Loss: 0.34239189326763153, Final Batch Loss: 0.05001479387283325\n",
      "Epoch 1726, Loss: 0.5411561504006386, Final Batch Loss: 0.20214462280273438\n",
      "Epoch 1727, Loss: 0.3780910409986973, Final Batch Loss: 0.1195235550403595\n",
      "Epoch 1728, Loss: 0.4352208822965622, Final Batch Loss: 0.1016780436038971\n",
      "Epoch 1729, Loss: 0.4011741578578949, Final Batch Loss: 0.0749056488275528\n",
      "Epoch 1730, Loss: 0.46425608545541763, Final Batch Loss: 0.13470305502414703\n",
      "Epoch 1731, Loss: 0.4158365875482559, Final Batch Loss: 0.1092921569943428\n",
      "Epoch 1732, Loss: 0.4527279809117317, Final Batch Loss: 0.082723468542099\n",
      "Epoch 1733, Loss: 0.3801802285015583, Final Batch Loss: 0.1223798468708992\n",
      "Epoch 1734, Loss: 0.48185110837221146, Final Batch Loss: 0.12964536249637604\n",
      "Epoch 1735, Loss: 0.359191358089447, Final Batch Loss: 0.08815231174230576\n",
      "Epoch 1736, Loss: 0.39402343332767487, Final Batch Loss: 0.06829841434955597\n",
      "Epoch 1737, Loss: 0.442315898835659, Final Batch Loss: 0.13364249467849731\n",
      "Epoch 1738, Loss: 0.39855803176760674, Final Batch Loss: 0.04664259031414986\n",
      "Epoch 1739, Loss: 0.314848180860281, Final Batch Loss: 0.03102988749742508\n",
      "Epoch 1740, Loss: 0.3610523045063019, Final Batch Loss: 0.06037487834692001\n",
      "Epoch 1741, Loss: 0.4445098266005516, Final Batch Loss: 0.13624076545238495\n",
      "Epoch 1742, Loss: 0.2763366028666496, Final Batch Loss: 0.05386311560869217\n",
      "Epoch 1743, Loss: 0.39363278448581696, Final Batch Loss: 0.06420885026454926\n",
      "Epoch 1744, Loss: 0.4463134631514549, Final Batch Loss: 0.11792848259210587\n",
      "Epoch 1745, Loss: 0.3401249945163727, Final Batch Loss: 0.03324726223945618\n",
      "Epoch 1746, Loss: 0.4513045847415924, Final Batch Loss: 0.10193566977977753\n",
      "Epoch 1747, Loss: 0.4470706954598427, Final Batch Loss: 0.11314304918050766\n",
      "Epoch 1748, Loss: 0.3623674698174, Final Batch Loss: 0.08285658806562424\n",
      "Epoch 1749, Loss: 0.42380938678979874, Final Batch Loss: 0.13264231383800507\n",
      "Epoch 1750, Loss: 0.3006816916167736, Final Batch Loss: 0.019448768347501755\n",
      "Epoch 1751, Loss: 0.48861996084451675, Final Batch Loss: 0.11519432067871094\n",
      "Epoch 1752, Loss: 0.5579741075634956, Final Batch Loss: 0.24068059027194977\n",
      "Epoch 1753, Loss: 0.38955145329236984, Final Batch Loss: 0.06392977386713028\n",
      "Epoch 1754, Loss: 0.41445543617010117, Final Batch Loss: 0.104365274310112\n",
      "Epoch 1755, Loss: 0.48561298847198486, Final Batch Loss: 0.1703740507364273\n",
      "Epoch 1756, Loss: 0.37123535200953484, Final Batch Loss: 0.07249189913272858\n",
      "Epoch 1757, Loss: 0.4556584656238556, Final Batch Loss: 0.11491300910711288\n",
      "Epoch 1758, Loss: 0.3109448030591011, Final Batch Loss: 0.06289244443178177\n",
      "Epoch 1759, Loss: 0.4299982562661171, Final Batch Loss: 0.12758959829807281\n",
      "Epoch 1760, Loss: 0.2602844424545765, Final Batch Loss: 0.052174001932144165\n",
      "Epoch 1761, Loss: 0.5302290394902229, Final Batch Loss: 0.14871343970298767\n",
      "Epoch 1762, Loss: 0.512422226369381, Final Batch Loss: 0.09533916413784027\n",
      "Epoch 1763, Loss: 0.3457961827516556, Final Batch Loss: 0.07658591866493225\n",
      "Epoch 1764, Loss: 0.46177224814891815, Final Batch Loss: 0.1746336817741394\n",
      "Epoch 1765, Loss: 0.5793854519724846, Final Batch Loss: 0.1506965607404709\n",
      "Epoch 1766, Loss: 0.3337029404938221, Final Batch Loss: 0.0614437498152256\n",
      "Epoch 1767, Loss: 0.45836322754621506, Final Batch Loss: 0.13373252749443054\n",
      "Epoch 1768, Loss: 0.48686905205249786, Final Batch Loss: 0.18155191838741302\n",
      "Epoch 1769, Loss: 0.49349573999643326, Final Batch Loss: 0.06254736334085464\n",
      "Epoch 1770, Loss: 0.4509918689727783, Final Batch Loss: 0.07779134809970856\n",
      "Epoch 1771, Loss: 0.35569220408797264, Final Batch Loss: 0.058854151517152786\n",
      "Epoch 1772, Loss: 0.45484165102243423, Final Batch Loss: 0.06322727352380753\n",
      "Epoch 1773, Loss: 0.45895981043577194, Final Batch Loss: 0.10077891498804092\n",
      "Epoch 1774, Loss: 0.535543367266655, Final Batch Loss: 0.14988276362419128\n",
      "Epoch 1775, Loss: 0.3655826710164547, Final Batch Loss: 0.0544436015188694\n",
      "Epoch 1776, Loss: 0.3852113410830498, Final Batch Loss: 0.1157887652516365\n",
      "Epoch 1777, Loss: 0.3742298521101475, Final Batch Loss: 0.02761167660355568\n",
      "Epoch 1778, Loss: 0.415682315826416, Final Batch Loss: 0.06377554684877396\n",
      "Epoch 1779, Loss: 0.3784947283565998, Final Batch Loss: 0.037445347756147385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1780, Loss: 0.4359709694981575, Final Batch Loss: 0.1187889501452446\n",
      "Epoch 1781, Loss: 0.3721938803792, Final Batch Loss: 0.07083836942911148\n",
      "Epoch 1782, Loss: 0.44487590342760086, Final Batch Loss: 0.14760340750217438\n",
      "Epoch 1783, Loss: 0.4338093101978302, Final Batch Loss: 0.1341172754764557\n",
      "Epoch 1784, Loss: 0.3523833006620407, Final Batch Loss: 0.07427876442670822\n",
      "Epoch 1785, Loss: 0.37796440720558167, Final Batch Loss: 0.09857963770627975\n",
      "Epoch 1786, Loss: 0.38757724314928055, Final Batch Loss: 0.11368399113416672\n",
      "Epoch 1787, Loss: 0.4112745374441147, Final Batch Loss: 0.08433520048856735\n",
      "Epoch 1788, Loss: 0.3228665962815285, Final Batch Loss: 0.06452226638793945\n",
      "Epoch 1789, Loss: 0.46465686708688736, Final Batch Loss: 0.10756868869066238\n",
      "Epoch 1790, Loss: 0.4420963451266289, Final Batch Loss: 0.07172578573226929\n",
      "Epoch 1791, Loss: 0.33798861876130104, Final Batch Loss: 0.041566699743270874\n",
      "Epoch 1792, Loss: 0.46628986299037933, Final Batch Loss: 0.16662383079528809\n",
      "Epoch 1793, Loss: 0.43210872262716293, Final Batch Loss: 0.12740200757980347\n",
      "Epoch 1794, Loss: 0.494281530380249, Final Batch Loss: 0.14596796035766602\n",
      "Epoch 1795, Loss: 0.44244277477264404, Final Batch Loss: 0.11426503211259842\n",
      "Epoch 1796, Loss: 0.44645414501428604, Final Batch Loss: 0.08271685987710953\n",
      "Epoch 1797, Loss: 0.418514147400856, Final Batch Loss: 0.04140951484441757\n",
      "Epoch 1798, Loss: 0.4927259087562561, Final Batch Loss: 0.08938899636268616\n",
      "Epoch 1799, Loss: 0.38621681928634644, Final Batch Loss: 0.07623683661222458\n",
      "Epoch 1800, Loss: 0.3819793276488781, Final Batch Loss: 0.0602882094681263\n",
      "Epoch 1801, Loss: 0.41169192641973495, Final Batch Loss: 0.106271892786026\n",
      "Epoch 1802, Loss: 0.47444838285446167, Final Batch Loss: 0.12224047631025314\n",
      "Epoch 1803, Loss: 0.36252201721072197, Final Batch Loss: 0.1003551185131073\n",
      "Epoch 1804, Loss: 0.42862093448638916, Final Batch Loss: 0.08148559182882309\n",
      "Epoch 1805, Loss: 0.3001779839396477, Final Batch Loss: 0.04868946969509125\n",
      "Epoch 1806, Loss: 0.38259872049093246, Final Batch Loss: 0.09028082340955734\n",
      "Epoch 1807, Loss: 0.46193838119506836, Final Batch Loss: 0.20124106109142303\n",
      "Epoch 1808, Loss: 0.3342233821749687, Final Batch Loss: 0.08520598709583282\n",
      "Epoch 1809, Loss: 0.4069557189941406, Final Batch Loss: 0.10900634527206421\n",
      "Epoch 1810, Loss: 0.40640487149357796, Final Batch Loss: 0.12772120535373688\n",
      "Epoch 1811, Loss: 0.4439326450228691, Final Batch Loss: 0.09982291609048843\n",
      "Epoch 1812, Loss: 0.48232028633356094, Final Batch Loss: 0.12777143716812134\n",
      "Epoch 1813, Loss: 0.36673086881637573, Final Batch Loss: 0.08049984276294708\n",
      "Epoch 1814, Loss: 0.4584554359316826, Final Batch Loss: 0.152303084731102\n",
      "Epoch 1815, Loss: 0.3519502356648445, Final Batch Loss: 0.07241298258304596\n",
      "Epoch 1816, Loss: 0.4744045212864876, Final Batch Loss: 0.19627895951271057\n",
      "Epoch 1817, Loss: 0.41787702590227127, Final Batch Loss: 0.08025890588760376\n",
      "Epoch 1818, Loss: 0.4048518091440201, Final Batch Loss: 0.11307558417320251\n",
      "Epoch 1819, Loss: 0.3513867035508156, Final Batch Loss: 0.10051510483026505\n",
      "Epoch 1820, Loss: 0.3491283133625984, Final Batch Loss: 0.07244908064603806\n",
      "Epoch 1821, Loss: 0.5447715595364571, Final Batch Loss: 0.22862005233764648\n",
      "Epoch 1822, Loss: 0.40990155935287476, Final Batch Loss: 0.11636350303888321\n",
      "Epoch 1823, Loss: 0.4523795321583748, Final Batch Loss: 0.15663747489452362\n",
      "Epoch 1824, Loss: 0.40247635170817375, Final Batch Loss: 0.08428734540939331\n",
      "Epoch 1825, Loss: 0.3237049877643585, Final Batch Loss: 0.05649586021900177\n",
      "Epoch 1826, Loss: 0.4378320872783661, Final Batch Loss: 0.08686354756355286\n",
      "Epoch 1827, Loss: 0.4396251514554024, Final Batch Loss: 0.10567190498113632\n",
      "Epoch 1828, Loss: 0.4373766556382179, Final Batch Loss: 0.08144259452819824\n",
      "Epoch 1829, Loss: 0.4684647098183632, Final Batch Loss: 0.1455088108778\n",
      "Epoch 1830, Loss: 0.2666739020496607, Final Batch Loss: 0.021197179332375526\n",
      "Epoch 1831, Loss: 0.39114488661289215, Final Batch Loss: 0.09645131975412369\n",
      "Epoch 1832, Loss: 0.3941226154565811, Final Batch Loss: 0.08048075437545776\n",
      "Epoch 1833, Loss: 0.4251032844185829, Final Batch Loss: 0.13032275438308716\n",
      "Epoch 1834, Loss: 0.3898107782006264, Final Batch Loss: 0.12549848854541779\n",
      "Epoch 1835, Loss: 0.4009747803211212, Final Batch Loss: 0.1041654720902443\n",
      "Epoch 1836, Loss: 0.3814315162599087, Final Batch Loss: 0.05771805718541145\n",
      "Epoch 1837, Loss: 0.4309183359146118, Final Batch Loss: 0.10333528369665146\n",
      "Epoch 1838, Loss: 0.39894916862249374, Final Batch Loss: 0.1187000572681427\n",
      "Epoch 1839, Loss: 0.3957948870956898, Final Batch Loss: 0.15625256299972534\n",
      "Epoch 1840, Loss: 0.4541907086968422, Final Batch Loss: 0.11895885318517685\n",
      "Epoch 1841, Loss: 0.4336526244878769, Final Batch Loss: 0.15689720213413239\n",
      "Epoch 1842, Loss: 0.4558051824569702, Final Batch Loss: 0.17497320473194122\n",
      "Epoch 1843, Loss: 0.448823481798172, Final Batch Loss: 0.11539249867200851\n",
      "Epoch 1844, Loss: 0.3738052025437355, Final Batch Loss: 0.09417928755283356\n",
      "Epoch 1845, Loss: 0.4894184395670891, Final Batch Loss: 0.13768161833286285\n",
      "Epoch 1846, Loss: 0.3936058059334755, Final Batch Loss: 0.13748835027217865\n",
      "Epoch 1847, Loss: 0.3825630322098732, Final Batch Loss: 0.10156786441802979\n",
      "Epoch 1848, Loss: 0.39966849237680435, Final Batch Loss: 0.06399836391210556\n",
      "Epoch 1849, Loss: 0.4240306094288826, Final Batch Loss: 0.04254176467657089\n",
      "Epoch 1850, Loss: 0.38781771436333656, Final Batch Loss: 0.08779065310955048\n",
      "Epoch 1851, Loss: 0.38462863862514496, Final Batch Loss: 0.0866798385977745\n",
      "Epoch 1852, Loss: 0.4157850965857506, Final Batch Loss: 0.07225640118122101\n",
      "Epoch 1853, Loss: 0.313794381916523, Final Batch Loss: 0.06254119426012039\n",
      "Epoch 1854, Loss: 0.3728061765432358, Final Batch Loss: 0.0980120524764061\n",
      "Epoch 1855, Loss: 0.4128766879439354, Final Batch Loss: 0.1370677798986435\n",
      "Epoch 1856, Loss: 0.375544935464859, Final Batch Loss: 0.1073378175497055\n",
      "Epoch 1857, Loss: 0.3415433540940285, Final Batch Loss: 0.09784369170665741\n",
      "Epoch 1858, Loss: 0.3896128907799721, Final Batch Loss: 0.07489076256752014\n",
      "Epoch 1859, Loss: 0.48266172409057617, Final Batch Loss: 0.17243365943431854\n",
      "Epoch 1860, Loss: 0.4752394072711468, Final Batch Loss: 0.20975446701049805\n",
      "Epoch 1861, Loss: 0.4656015411019325, Final Batch Loss: 0.10633525997400284\n",
      "Epoch 1862, Loss: 0.40633441507816315, Final Batch Loss: 0.046349652111530304\n",
      "Epoch 1863, Loss: 0.32701222971081734, Final Batch Loss: 0.04930344596505165\n",
      "Epoch 1864, Loss: 0.36114928871393204, Final Batch Loss: 0.05821117013692856\n",
      "Epoch 1865, Loss: 0.4186663292348385, Final Batch Loss: 0.14654996991157532\n",
      "Epoch 1866, Loss: 0.4299050569534302, Final Batch Loss: 0.1445031464099884\n",
      "Epoch 1867, Loss: 0.42195311188697815, Final Batch Loss: 0.10229167342185974\n",
      "Epoch 1868, Loss: 0.3920615464448929, Final Batch Loss: 0.07448060065507889\n",
      "Epoch 1869, Loss: 0.31627006083726883, Final Batch Loss: 0.05210687965154648\n",
      "Epoch 1870, Loss: 0.4395904168486595, Final Batch Loss: 0.16608865559101105\n",
      "Epoch 1871, Loss: 0.40269797295331955, Final Batch Loss: 0.07241815328598022\n",
      "Epoch 1872, Loss: 0.6097678542137146, Final Batch Loss: 0.1565418839454651\n",
      "Epoch 1873, Loss: 0.3786577545106411, Final Batch Loss: 0.09747785329818726\n",
      "Epoch 1874, Loss: 0.34898021444678307, Final Batch Loss: 0.11613944172859192\n",
      "Epoch 1875, Loss: 0.4242607653141022, Final Batch Loss: 0.10195556282997131\n",
      "Epoch 1876, Loss: 0.5077579021453857, Final Batch Loss: 0.10431810468435287\n",
      "Epoch 1877, Loss: 0.4630776196718216, Final Batch Loss: 0.06718636304140091\n",
      "Epoch 1878, Loss: 0.3851618245244026, Final Batch Loss: 0.08715902268886566\n",
      "Epoch 1879, Loss: 0.4826796129345894, Final Batch Loss: 0.12056300044059753\n",
      "Epoch 1880, Loss: 0.4054391607642174, Final Batch Loss: 0.14244699478149414\n",
      "Epoch 1881, Loss: 0.39775116741657257, Final Batch Loss: 0.10084819793701172\n",
      "Epoch 1882, Loss: 0.4143071696162224, Final Batch Loss: 0.1425834447145462\n",
      "Epoch 1883, Loss: 0.4480381906032562, Final Batch Loss: 0.12924520671367645\n",
      "Epoch 1884, Loss: 0.4262075871229172, Final Batch Loss: 0.14396411180496216\n",
      "Epoch 1885, Loss: 0.5043110623955727, Final Batch Loss: 0.17494945228099823\n",
      "Epoch 1886, Loss: 0.40150557458400726, Final Batch Loss: 0.07370620220899582\n",
      "Epoch 1887, Loss: 0.3929162174463272, Final Batch Loss: 0.12611369788646698\n",
      "Epoch 1888, Loss: 0.35503027588129044, Final Batch Loss: 0.059527166187763214\n",
      "Epoch 1889, Loss: 0.507525809109211, Final Batch Loss: 0.17413268983364105\n",
      "Epoch 1890, Loss: 0.3653458207845688, Final Batch Loss: 0.09183316677808762\n",
      "Epoch 1891, Loss: 0.4279863312840462, Final Batch Loss: 0.11773615330457687\n",
      "Epoch 1892, Loss: 0.38518255949020386, Final Batch Loss: 0.10531322658061981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1893, Loss: 0.48380783200263977, Final Batch Loss: 0.1768946647644043\n",
      "Epoch 1894, Loss: 0.38661885261535645, Final Batch Loss: 0.10471604019403458\n",
      "Epoch 1895, Loss: 0.40439385175704956, Final Batch Loss: 0.08059055358171463\n",
      "Epoch 1896, Loss: 0.39428645372390747, Final Batch Loss: 0.06646882742643356\n",
      "Epoch 1897, Loss: 0.4208987131714821, Final Batch Loss: 0.12507346272468567\n",
      "Epoch 1898, Loss: 0.37644242495298386, Final Batch Loss: 0.10877101123332977\n",
      "Epoch 1899, Loss: 0.34300368279218674, Final Batch Loss: 0.08442827314138412\n",
      "Epoch 1900, Loss: 0.30153220519423485, Final Batch Loss: 0.08265860378742218\n",
      "Epoch 1901, Loss: 0.4033655449748039, Final Batch Loss: 0.1378118097782135\n",
      "Epoch 1902, Loss: 0.3478349447250366, Final Batch Loss: 0.04596530646085739\n",
      "Epoch 1903, Loss: 0.3752128407359123, Final Batch Loss: 0.09455139189958572\n",
      "Epoch 1904, Loss: 0.4400224983692169, Final Batch Loss: 0.08794063329696655\n",
      "Epoch 1905, Loss: 0.4043201729655266, Final Batch Loss: 0.12771257758140564\n",
      "Epoch 1906, Loss: 0.2729782424867153, Final Batch Loss: 0.05923861265182495\n",
      "Epoch 1907, Loss: 0.33823054283857346, Final Batch Loss: 0.08995602279901505\n",
      "Epoch 1908, Loss: 0.36292628198862076, Final Batch Loss: 0.07519447803497314\n",
      "Epoch 1909, Loss: 0.38193849474191666, Final Batch Loss: 0.10022622346878052\n",
      "Epoch 1910, Loss: 0.3980563059449196, Final Batch Loss: 0.11093464493751526\n",
      "Epoch 1911, Loss: 0.3158833347260952, Final Batch Loss: 0.04569115862250328\n",
      "Epoch 1912, Loss: 0.39549633115530014, Final Batch Loss: 0.06736104935407639\n",
      "Epoch 1913, Loss: 0.4502122551202774, Final Batch Loss: 0.10290369391441345\n",
      "Epoch 1914, Loss: 0.3720608875155449, Final Batch Loss: 0.11378509551286697\n",
      "Epoch 1915, Loss: 0.3823291137814522, Final Batch Loss: 0.11355583369731903\n",
      "Epoch 1916, Loss: 0.34949948638677597, Final Batch Loss: 0.058826714754104614\n",
      "Epoch 1917, Loss: 0.32986605539917946, Final Batch Loss: 0.09591298550367355\n",
      "Epoch 1918, Loss: 0.3879580795764923, Final Batch Loss: 0.07677121460437775\n",
      "Epoch 1919, Loss: 0.45996981486678123, Final Batch Loss: 0.06098266318440437\n",
      "Epoch 1920, Loss: 0.4623045399785042, Final Batch Loss: 0.11472975462675095\n",
      "Epoch 1921, Loss: 0.3294602483510971, Final Batch Loss: 0.07845830172300339\n",
      "Epoch 1922, Loss: 0.4122193455696106, Final Batch Loss: 0.1281244307756424\n",
      "Epoch 1923, Loss: 0.487605944275856, Final Batch Loss: 0.1480073183774948\n",
      "Epoch 1924, Loss: 0.45753078907728195, Final Batch Loss: 0.09988778829574585\n",
      "Epoch 1925, Loss: 0.3390025608241558, Final Batch Loss: 0.06087086722254753\n",
      "Epoch 1926, Loss: 0.3825770989060402, Final Batch Loss: 0.10239312052726746\n",
      "Epoch 1927, Loss: 0.4007209688425064, Final Batch Loss: 0.14922024309635162\n",
      "Epoch 1928, Loss: 0.4292886182665825, Final Batch Loss: 0.040324509143829346\n",
      "Epoch 1929, Loss: 0.39666102081537247, Final Batch Loss: 0.1684618890285492\n",
      "Epoch 1930, Loss: 0.4476613476872444, Final Batch Loss: 0.17470087110996246\n",
      "Epoch 1931, Loss: 0.5294029861688614, Final Batch Loss: 0.08359518647193909\n",
      "Epoch 1932, Loss: 0.412947878241539, Final Batch Loss: 0.10634556412696838\n",
      "Epoch 1933, Loss: 0.34359631687402725, Final Batch Loss: 0.07329664379358292\n",
      "Epoch 1934, Loss: 0.4000137150287628, Final Batch Loss: 0.05697622895240784\n",
      "Epoch 1935, Loss: 0.41448991745710373, Final Batch Loss: 0.09669288992881775\n",
      "Epoch 1936, Loss: 0.3636500984430313, Final Batch Loss: 0.08758047223091125\n",
      "Epoch 1937, Loss: 0.36397600173950195, Final Batch Loss: 0.06809976696968079\n",
      "Epoch 1938, Loss: 0.6403195485472679, Final Batch Loss: 0.21456538140773773\n",
      "Epoch 1939, Loss: 0.3294369503855705, Final Batch Loss: 0.07223276793956757\n",
      "Epoch 1940, Loss: 0.28101884946227074, Final Batch Loss: 0.03085164725780487\n",
      "Epoch 1941, Loss: 0.33125079050660133, Final Batch Loss: 0.05236155167222023\n",
      "Epoch 1942, Loss: 0.4016639068722725, Final Batch Loss: 0.11724060028791428\n",
      "Epoch 1943, Loss: 0.365849994122982, Final Batch Loss: 0.07364942133426666\n",
      "Epoch 1944, Loss: 0.42681071907281876, Final Batch Loss: 0.07439514249563217\n",
      "Epoch 1945, Loss: 0.31624119356274605, Final Batch Loss: 0.0669453963637352\n",
      "Epoch 1946, Loss: 0.4561532810330391, Final Batch Loss: 0.06611654907464981\n",
      "Epoch 1947, Loss: 0.3751953952014446, Final Batch Loss: 0.031316276639699936\n",
      "Epoch 1948, Loss: 0.4172918274998665, Final Batch Loss: 0.11859239637851715\n",
      "Epoch 1949, Loss: 0.39769261330366135, Final Batch Loss: 0.10711005330085754\n",
      "Epoch 1950, Loss: 0.4566151946783066, Final Batch Loss: 0.12493617832660675\n",
      "Epoch 1951, Loss: 0.3970029801130295, Final Batch Loss: 0.1285845786333084\n",
      "Epoch 1952, Loss: 0.39952167868614197, Final Batch Loss: 0.12170366197824478\n",
      "Epoch 1953, Loss: 0.4434427097439766, Final Batch Loss: 0.1473456770181656\n",
      "Epoch 1954, Loss: 0.4783228412270546, Final Batch Loss: 0.14325912296772003\n",
      "Epoch 1955, Loss: 0.34125883877277374, Final Batch Loss: 0.07140720635652542\n",
      "Epoch 1956, Loss: 0.3470624051988125, Final Batch Loss: 0.055724337697029114\n",
      "Epoch 1957, Loss: 0.32037026435136795, Final Batch Loss: 0.0901547446846962\n",
      "Epoch 1958, Loss: 0.40040212869644165, Final Batch Loss: 0.12716270983219147\n",
      "Epoch 1959, Loss: 0.3673596829175949, Final Batch Loss: 0.11863149702548981\n",
      "Epoch 1960, Loss: 0.47258254885673523, Final Batch Loss: 0.1882936805486679\n",
      "Epoch 1961, Loss: 0.4135848879814148, Final Batch Loss: 0.12004689872264862\n",
      "Epoch 1962, Loss: 0.4363939091563225, Final Batch Loss: 0.05282723903656006\n",
      "Epoch 1963, Loss: 0.39574258401989937, Final Batch Loss: 0.034334611147642136\n",
      "Epoch 1964, Loss: 0.33656464517116547, Final Batch Loss: 0.11157174408435822\n",
      "Epoch 1965, Loss: 0.38228652998805046, Final Batch Loss: 0.05404209718108177\n",
      "Epoch 1966, Loss: 0.3746788427233696, Final Batch Loss: 0.07737328112125397\n",
      "Epoch 1967, Loss: 0.35963740944862366, Final Batch Loss: 0.07986266165971756\n",
      "Epoch 1968, Loss: 0.3609440252184868, Final Batch Loss: 0.10172101855278015\n",
      "Epoch 1969, Loss: 0.37512558698654175, Final Batch Loss: 0.02870408445596695\n",
      "Epoch 1970, Loss: 0.4605172649025917, Final Batch Loss: 0.10963995009660721\n",
      "Epoch 1971, Loss: 0.3700025901198387, Final Batch Loss: 0.11846856027841568\n",
      "Epoch 1972, Loss: 0.35672565549612045, Final Batch Loss: 0.11896675080060959\n",
      "Epoch 1973, Loss: 0.37813274562358856, Final Batch Loss: 0.037785813212394714\n",
      "Epoch 1974, Loss: 0.3276298623532057, Final Batch Loss: 0.02093922160565853\n",
      "Epoch 1975, Loss: 0.32753055915236473, Final Batch Loss: 0.048356156796216965\n",
      "Epoch 1976, Loss: 0.3885868266224861, Final Batch Loss: 0.07353904843330383\n",
      "Epoch 1977, Loss: 0.38552992045879364, Final Batch Loss: 0.08525808900594711\n",
      "Epoch 1978, Loss: 0.33557721599936485, Final Batch Loss: 0.05141955241560936\n",
      "Epoch 1979, Loss: 0.37593936175107956, Final Batch Loss: 0.05917632579803467\n",
      "Epoch 1980, Loss: 0.3582408204674721, Final Batch Loss: 0.059734754264354706\n",
      "Epoch 1981, Loss: 0.39123130589723587, Final Batch Loss: 0.10296787321567535\n",
      "Epoch 1982, Loss: 0.29736335203051567, Final Batch Loss: 0.04679979011416435\n",
      "Epoch 1983, Loss: 0.28252122551202774, Final Batch Loss: 0.05124130845069885\n",
      "Epoch 1984, Loss: 0.3758842274546623, Final Batch Loss: 0.07304203510284424\n",
      "Epoch 1985, Loss: 0.4185032248497009, Final Batch Loss: 0.14639174938201904\n",
      "Epoch 1986, Loss: 0.39516716450452805, Final Batch Loss: 0.10252702981233597\n",
      "Epoch 1987, Loss: 0.4623398780822754, Final Batch Loss: 0.19756142795085907\n",
      "Epoch 1988, Loss: 0.3637474551796913, Final Batch Loss: 0.0678868219256401\n",
      "Epoch 1989, Loss: 0.31549935787916183, Final Batch Loss: 0.05861520767211914\n",
      "Epoch 1990, Loss: 0.33713819459080696, Final Batch Loss: 0.09771450608968735\n",
      "Epoch 1991, Loss: 0.4763679951429367, Final Batch Loss: 0.1685422658920288\n",
      "Epoch 1992, Loss: 0.3555871918797493, Final Batch Loss: 0.09247187525033951\n",
      "Epoch 1993, Loss: 0.34408876299858093, Final Batch Loss: 0.0924096554517746\n",
      "Epoch 1994, Loss: 0.3842889294028282, Final Batch Loss: 0.07108330726623535\n",
      "Epoch 1995, Loss: 0.27960697934031487, Final Batch Loss: 0.08522115647792816\n",
      "Epoch 1996, Loss: 0.2791481874883175, Final Batch Loss: 0.0631144568324089\n",
      "Epoch 1997, Loss: 0.32207853719592094, Final Batch Loss: 0.0535486675798893\n",
      "Epoch 1998, Loss: 0.451127827167511, Final Batch Loss: 0.10103977471590042\n",
      "Epoch 1999, Loss: 0.4490414410829544, Final Batch Loss: 0.09079063683748245\n",
      "Epoch 2000, Loss: 0.40510932356119156, Final Batch Loss: 0.11384035646915436\n",
      "Epoch 2001, Loss: 0.4385518431663513, Final Batch Loss: 0.079793281853199\n",
      "Epoch 2002, Loss: 0.4099600613117218, Final Batch Loss: 0.0981145054101944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2003, Loss: 0.43198494613170624, Final Batch Loss: 0.079403355717659\n",
      "Epoch 2004, Loss: 0.4471145421266556, Final Batch Loss: 0.15226095914840698\n",
      "Epoch 2005, Loss: 0.33274585008621216, Final Batch Loss: 0.04587075859308243\n",
      "Epoch 2006, Loss: 0.3133847452700138, Final Batch Loss: 0.04875171184539795\n",
      "Epoch 2007, Loss: 0.3176090568304062, Final Batch Loss: 0.03312160074710846\n",
      "Epoch 2008, Loss: 0.43676143884658813, Final Batch Loss: 0.11427422612905502\n",
      "Epoch 2009, Loss: 0.3031233139336109, Final Batch Loss: 0.05410218611359596\n",
      "Epoch 2010, Loss: 0.3625938165932894, Final Batch Loss: 0.028011398389935493\n",
      "Epoch 2011, Loss: 0.41754622757434845, Final Batch Loss: 0.054466359317302704\n",
      "Epoch 2012, Loss: 0.4292733147740364, Final Batch Loss: 0.11394345760345459\n",
      "Epoch 2013, Loss: 0.3536123037338257, Final Batch Loss: 0.08138149231672287\n",
      "Epoch 2014, Loss: 0.33599887043237686, Final Batch Loss: 0.06549210101366043\n",
      "Epoch 2015, Loss: 0.3287520371377468, Final Batch Loss: 0.04810761287808418\n",
      "Epoch 2016, Loss: 0.4349614977836609, Final Batch Loss: 0.14502933621406555\n",
      "Epoch 2017, Loss: 0.26702431961894035, Final Batch Loss: 0.032001011073589325\n",
      "Epoch 2018, Loss: 0.3805245906114578, Final Batch Loss: 0.08052311092615128\n",
      "Epoch 2019, Loss: 0.3347080610692501, Final Batch Loss: 0.07350673526525497\n",
      "Epoch 2020, Loss: 0.31095074862241745, Final Batch Loss: 0.08049516379833221\n",
      "Epoch 2021, Loss: 0.33213598281145096, Final Batch Loss: 0.0553416982293129\n",
      "Epoch 2022, Loss: 0.39842039719223976, Final Batch Loss: 0.09844991564750671\n",
      "Epoch 2023, Loss: 0.3091377057135105, Final Batch Loss: 0.04820042848587036\n",
      "Epoch 2024, Loss: 0.42521023005247116, Final Batch Loss: 0.1580679714679718\n",
      "Epoch 2025, Loss: 0.3656483069062233, Final Batch Loss: 0.12254541367292404\n",
      "Epoch 2026, Loss: 0.4694629982113838, Final Batch Loss: 0.11799085140228271\n",
      "Epoch 2027, Loss: 0.40140917524695396, Final Batch Loss: 0.053984928876161575\n",
      "Epoch 2028, Loss: 0.2650794945657253, Final Batch Loss: 0.020182885229587555\n",
      "Epoch 2029, Loss: 0.4467935338616371, Final Batch Loss: 0.0726211667060852\n",
      "Epoch 2030, Loss: 0.4981194883584976, Final Batch Loss: 0.08360789716243744\n",
      "Epoch 2031, Loss: 0.3868703544139862, Final Batch Loss: 0.09443917125463486\n",
      "Epoch 2032, Loss: 0.3454192541539669, Final Batch Loss: 0.03259124234318733\n",
      "Epoch 2033, Loss: 0.3544974997639656, Final Batch Loss: 0.07329358160495758\n",
      "Epoch 2034, Loss: 0.42767997831106186, Final Batch Loss: 0.11144477874040604\n",
      "Epoch 2035, Loss: 0.4257253259420395, Final Batch Loss: 0.10505804419517517\n",
      "Epoch 2036, Loss: 0.455375574529171, Final Batch Loss: 0.17037329077720642\n",
      "Epoch 2037, Loss: 0.3852550685405731, Final Batch Loss: 0.09161002933979034\n",
      "Epoch 2038, Loss: 0.4116184934973717, Final Batch Loss: 0.11340499669313431\n",
      "Epoch 2039, Loss: 0.38880519568920135, Final Batch Loss: 0.06866070628166199\n",
      "Epoch 2040, Loss: 0.4573633521795273, Final Batch Loss: 0.10213114321231842\n",
      "Epoch 2041, Loss: 0.3733781985938549, Final Batch Loss: 0.1907009333372116\n",
      "Epoch 2042, Loss: 0.4971114918589592, Final Batch Loss: 0.23397737741470337\n",
      "Epoch 2043, Loss: 0.3661278709769249, Final Batch Loss: 0.1317853331565857\n",
      "Epoch 2044, Loss: 0.36247263103723526, Final Batch Loss: 0.048024266958236694\n",
      "Epoch 2045, Loss: 0.37963446229696274, Final Batch Loss: 0.08604320138692856\n",
      "Epoch 2046, Loss: 0.3279353752732277, Final Batch Loss: 0.07458682358264923\n",
      "Epoch 2047, Loss: 0.49647165834903717, Final Batch Loss: 0.15865953266620636\n",
      "Epoch 2048, Loss: 0.3743152841925621, Final Batch Loss: 0.10960377007722855\n",
      "Epoch 2049, Loss: 0.4724332243204117, Final Batch Loss: 0.12492314726114273\n",
      "Epoch 2050, Loss: 0.33562173694372177, Final Batch Loss: 0.10250943899154663\n",
      "Epoch 2051, Loss: 0.30651815608143806, Final Batch Loss: 0.04015554115176201\n",
      "Epoch 2052, Loss: 0.4265140891075134, Final Batch Loss: 0.1580735594034195\n",
      "Epoch 2053, Loss: 0.33643490076065063, Final Batch Loss: 0.09635258466005325\n",
      "Epoch 2054, Loss: 0.371427021920681, Final Batch Loss: 0.0870857909321785\n",
      "Epoch 2055, Loss: 0.45502109825611115, Final Batch Loss: 0.1830437034368515\n",
      "Epoch 2056, Loss: 0.345435731112957, Final Batch Loss: 0.08986008912324905\n",
      "Epoch 2057, Loss: 0.3802065774798393, Final Batch Loss: 0.1034260094165802\n",
      "Epoch 2058, Loss: 0.3092362768948078, Final Batch Loss: 0.0780591368675232\n",
      "Epoch 2059, Loss: 0.25958047807216644, Final Batch Loss: 0.0432322658598423\n",
      "Epoch 2060, Loss: 0.5653385072946548, Final Batch Loss: 0.2652100622653961\n",
      "Epoch 2061, Loss: 0.37189236283302307, Final Batch Loss: 0.0796094760298729\n",
      "Epoch 2062, Loss: 0.29541049897670746, Final Batch Loss: 0.028415046632289886\n",
      "Epoch 2063, Loss: 0.4703781232237816, Final Batch Loss: 0.17532071471214294\n",
      "Epoch 2064, Loss: 0.44496164470911026, Final Batch Loss: 0.11915325373411179\n",
      "Epoch 2065, Loss: 0.42754602432250977, Final Batch Loss: 0.16176772117614746\n",
      "Epoch 2066, Loss: 0.3851156681776047, Final Batch Loss: 0.05324734002351761\n",
      "Epoch 2067, Loss: 0.48442108929157257, Final Batch Loss: 0.17346185445785522\n",
      "Epoch 2068, Loss: 0.3073479700833559, Final Batch Loss: 0.02313569374382496\n",
      "Epoch 2069, Loss: 0.3259222097694874, Final Batch Loss: 0.08061867207288742\n",
      "Epoch 2070, Loss: 0.31900710985064507, Final Batch Loss: 0.03636674955487251\n",
      "Epoch 2071, Loss: 0.32810433954000473, Final Batch Loss: 0.07116623967885971\n",
      "Epoch 2072, Loss: 0.3262315168976784, Final Batch Loss: 0.09537597000598907\n",
      "Epoch 2073, Loss: 0.30531923472881317, Final Batch Loss: 0.03316495567560196\n",
      "Epoch 2074, Loss: 0.3741591200232506, Final Batch Loss: 0.09973705559968948\n",
      "Epoch 2075, Loss: 0.46237820386886597, Final Batch Loss: 0.10591690242290497\n",
      "Epoch 2076, Loss: 0.3650011643767357, Final Batch Loss: 0.12786298990249634\n",
      "Epoch 2077, Loss: 0.3716117963194847, Final Batch Loss: 0.050660960376262665\n",
      "Epoch 2078, Loss: 0.4183042347431183, Final Batch Loss: 0.09644361585378647\n",
      "Epoch 2079, Loss: 0.37815508246421814, Final Batch Loss: 0.10338790714740753\n",
      "Epoch 2080, Loss: 0.35015111044049263, Final Batch Loss: 0.037606481462717056\n",
      "Epoch 2081, Loss: 0.5131753087043762, Final Batch Loss: 0.1738656908273697\n",
      "Epoch 2082, Loss: 0.3493317849934101, Final Batch Loss: 0.04300679638981819\n",
      "Epoch 2083, Loss: 0.3860701471567154, Final Batch Loss: 0.08253321051597595\n",
      "Epoch 2084, Loss: 0.3477066680788994, Final Batch Loss: 0.09861066937446594\n",
      "Epoch 2085, Loss: 0.33815034106373787, Final Batch Loss: 0.09977422654628754\n",
      "Epoch 2086, Loss: 0.2740533724427223, Final Batch Loss: 0.037553392350673676\n",
      "Epoch 2087, Loss: 0.3204108700156212, Final Batch Loss: 0.04650645703077316\n",
      "Epoch 2088, Loss: 0.3818400353193283, Final Batch Loss: 0.06869959086179733\n",
      "Epoch 2089, Loss: 0.3774401508271694, Final Batch Loss: 0.04034033790230751\n",
      "Epoch 2090, Loss: 0.34850525110960007, Final Batch Loss: 0.04498545080423355\n",
      "Epoch 2091, Loss: 0.3223055526614189, Final Batch Loss: 0.10324978828430176\n",
      "Epoch 2092, Loss: 0.3751509562134743, Final Batch Loss: 0.10806670784950256\n",
      "Epoch 2093, Loss: 0.4375131204724312, Final Batch Loss: 0.10315738618373871\n",
      "Epoch 2094, Loss: 0.3679528199136257, Final Batch Loss: 0.05175405368208885\n",
      "Epoch 2095, Loss: 0.33270855993032455, Final Batch Loss: 0.08665238320827484\n",
      "Epoch 2096, Loss: 0.34174762293696404, Final Batch Loss: 0.05433826521039009\n",
      "Epoch 2097, Loss: 0.3503604307770729, Final Batch Loss: 0.057020872831344604\n",
      "Epoch 2098, Loss: 0.3087389562278986, Final Batch Loss: 0.031234117224812508\n",
      "Epoch 2099, Loss: 0.5120658576488495, Final Batch Loss: 0.08337379992008209\n",
      "Epoch 2100, Loss: 0.5089626908302307, Final Batch Loss: 0.2546127140522003\n",
      "Epoch 2101, Loss: 0.4170699790120125, Final Batch Loss: 0.11334974318742752\n",
      "Epoch 2102, Loss: 0.40828266739845276, Final Batch Loss: 0.09773029386997223\n",
      "Epoch 2103, Loss: 0.3586368039250374, Final Batch Loss: 0.053276218473911285\n",
      "Epoch 2104, Loss: 0.6134170964360237, Final Batch Loss: 0.2515515387058258\n",
      "Epoch 2105, Loss: 0.45048725605010986, Final Batch Loss: 0.1681387573480606\n",
      "Epoch 2106, Loss: 0.444964662194252, Final Batch Loss: 0.18301773071289062\n",
      "Epoch 2107, Loss: 0.5288900434970856, Final Batch Loss: 0.15356068313121796\n",
      "Epoch 2108, Loss: 0.38652367889881134, Final Batch Loss: 0.0745871514081955\n",
      "Epoch 2109, Loss: 0.3689423240721226, Final Batch Loss: 0.11607137322425842\n",
      "Epoch 2110, Loss: 0.3563345670700073, Final Batch Loss: 0.04918266087770462\n",
      "Epoch 2111, Loss: 0.2954919673502445, Final Batch Loss: 0.04246014729142189\n",
      "Epoch 2112, Loss: 0.36095593869686127, Final Batch Loss: 0.08944789320230484\n",
      "Epoch 2113, Loss: 0.4194916561245918, Final Batch Loss: 0.04176928848028183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2114, Loss: 0.37668631970882416, Final Batch Loss: 0.09507495909929276\n",
      "Epoch 2115, Loss: 0.38621217757463455, Final Batch Loss: 0.1258162409067154\n",
      "Epoch 2116, Loss: 0.3617847114801407, Final Batch Loss: 0.06424009799957275\n",
      "Epoch 2117, Loss: 0.3194144070148468, Final Batch Loss: 0.043065838515758514\n",
      "Epoch 2118, Loss: 0.3594362661242485, Final Batch Loss: 0.09544769674539566\n",
      "Epoch 2119, Loss: 0.3531111404299736, Final Batch Loss: 0.09883648157119751\n",
      "Epoch 2120, Loss: 0.47123385965824127, Final Batch Loss: 0.12734360992908478\n",
      "Epoch 2121, Loss: 0.389064185321331, Final Batch Loss: 0.0679389014840126\n",
      "Epoch 2122, Loss: 0.45296310260891914, Final Batch Loss: 0.04493623599410057\n",
      "Epoch 2123, Loss: 0.3890420012176037, Final Batch Loss: 0.052897799760103226\n",
      "Epoch 2124, Loss: 0.3762652724981308, Final Batch Loss: 0.08674541860818863\n",
      "Epoch 2125, Loss: 0.39345991611480713, Final Batch Loss: 0.10833100229501724\n",
      "Epoch 2126, Loss: 0.3458622917532921, Final Batch Loss: 0.05385563522577286\n",
      "Epoch 2127, Loss: 0.3925594389438629, Final Batch Loss: 0.13305076956748962\n",
      "Epoch 2128, Loss: 0.30514760687947273, Final Batch Loss: 0.057040948420763016\n",
      "Epoch 2129, Loss: 0.3652694821357727, Final Batch Loss: 0.14758004248142242\n",
      "Epoch 2130, Loss: 0.3313760794699192, Final Batch Loss: 0.06049341335892677\n",
      "Epoch 2131, Loss: 0.4350641593337059, Final Batch Loss: 0.1616157442331314\n",
      "Epoch 2132, Loss: 0.3970532566308975, Final Batch Loss: 0.08601786941289902\n",
      "Epoch 2133, Loss: 0.294008057564497, Final Batch Loss: 0.07209071516990662\n",
      "Epoch 2134, Loss: 0.46531225740909576, Final Batch Loss: 0.09408751875162125\n",
      "Epoch 2135, Loss: 0.41345594078302383, Final Batch Loss: 0.08147408068180084\n",
      "Epoch 2136, Loss: 0.37082386016845703, Final Batch Loss: 0.049153149127960205\n",
      "Epoch 2137, Loss: 0.3649546094238758, Final Batch Loss: 0.04658551141619682\n",
      "Epoch 2138, Loss: 0.43014542013406754, Final Batch Loss: 0.11200957000255585\n",
      "Epoch 2139, Loss: 0.4838340952992439, Final Batch Loss: 0.11684142053127289\n",
      "Epoch 2140, Loss: 0.4282683879137039, Final Batch Loss: 0.12889203429222107\n",
      "Epoch 2141, Loss: 0.32196883857250214, Final Batch Loss: 0.06996750086545944\n",
      "Epoch 2142, Loss: 0.3066091649234295, Final Batch Loss: 0.09802823513746262\n",
      "Epoch 2143, Loss: 0.5096302479505539, Final Batch Loss: 0.16439610719680786\n",
      "Epoch 2144, Loss: 0.368045412003994, Final Batch Loss: 0.060803622007369995\n",
      "Epoch 2145, Loss: 0.32871919125318527, Final Batch Loss: 0.07489212602376938\n",
      "Epoch 2146, Loss: 0.4014091193675995, Final Batch Loss: 0.10430283844470978\n",
      "Epoch 2147, Loss: 0.2977084070444107, Final Batch Loss: 0.08131397515535355\n",
      "Epoch 2148, Loss: 0.43386833369731903, Final Batch Loss: 0.10467081516981125\n",
      "Epoch 2149, Loss: 0.35805007070302963, Final Batch Loss: 0.11767806857824326\n",
      "Epoch 2150, Loss: 0.3394653797149658, Final Batch Loss: 0.13294094800949097\n",
      "Epoch 2151, Loss: 0.3976737894117832, Final Batch Loss: 0.03343648836016655\n",
      "Epoch 2152, Loss: 0.40326881408691406, Final Batch Loss: 0.1289432942867279\n",
      "Epoch 2153, Loss: 0.4526308700442314, Final Batch Loss: 0.10794591158628464\n",
      "Epoch 2154, Loss: 0.3240427076816559, Final Batch Loss: 0.09654603898525238\n",
      "Epoch 2155, Loss: 0.38636909797787666, Final Batch Loss: 0.05961494520306587\n",
      "Epoch 2156, Loss: 0.33818797767162323, Final Batch Loss: 0.05445082485675812\n",
      "Epoch 2157, Loss: 0.42995549738407135, Final Batch Loss: 0.08177297562360764\n",
      "Epoch 2158, Loss: 0.4906684532761574, Final Batch Loss: 0.15036259591579437\n",
      "Epoch 2159, Loss: 0.2971216253936291, Final Batch Loss: 0.059329938143491745\n",
      "Epoch 2160, Loss: 0.387742318212986, Final Batch Loss: 0.06460440158843994\n",
      "Epoch 2161, Loss: 0.3477635197341442, Final Batch Loss: 0.047581348568201065\n",
      "Epoch 2162, Loss: 0.35088301450014114, Final Batch Loss: 0.059746403247117996\n",
      "Epoch 2163, Loss: 0.46312563866376877, Final Batch Loss: 0.17283771932125092\n",
      "Epoch 2164, Loss: 0.4743388518691063, Final Batch Loss: 0.2111324965953827\n",
      "Epoch 2165, Loss: 0.3423239216208458, Final Batch Loss: 0.021608911454677582\n",
      "Epoch 2166, Loss: 0.27635986916720867, Final Batch Loss: 0.09518920630216599\n",
      "Epoch 2167, Loss: 0.47652462869882584, Final Batch Loss: 0.07772278040647507\n",
      "Epoch 2168, Loss: 0.2569279707968235, Final Batch Loss: 0.04843491315841675\n",
      "Epoch 2169, Loss: 0.4954173266887665, Final Batch Loss: 0.14825066924095154\n",
      "Epoch 2170, Loss: 0.44968364387750626, Final Batch Loss: 0.11545954644680023\n",
      "Epoch 2171, Loss: 0.34433463960886, Final Batch Loss: 0.07941203564405441\n",
      "Epoch 2172, Loss: 0.2997306324541569, Final Batch Loss: 0.03949529305100441\n",
      "Epoch 2173, Loss: 0.24793926440179348, Final Batch Loss: 0.02091524936258793\n",
      "Epoch 2174, Loss: 0.5157379508018494, Final Batch Loss: 0.12764579057693481\n",
      "Epoch 2175, Loss: 0.33839962631464005, Final Batch Loss: 0.08334646373987198\n",
      "Epoch 2176, Loss: 0.3532462418079376, Final Batch Loss: 0.07573322206735611\n",
      "Epoch 2177, Loss: 0.3384878896176815, Final Batch Loss: 0.08594712615013123\n",
      "Epoch 2178, Loss: 0.2905704453587532, Final Batch Loss: 0.06308271735906601\n",
      "Epoch 2179, Loss: 0.4740551486611366, Final Batch Loss: 0.12278924882411957\n",
      "Epoch 2180, Loss: 0.29335233196616173, Final Batch Loss: 0.07249005138874054\n",
      "Epoch 2181, Loss: 0.3207000270485878, Final Batch Loss: 0.06124632805585861\n",
      "Epoch 2182, Loss: 0.3869604021310806, Final Batch Loss: 0.047933049499988556\n",
      "Epoch 2183, Loss: 0.4388737231492996, Final Batch Loss: 0.15133434534072876\n",
      "Epoch 2184, Loss: 0.338138934224844, Final Batch Loss: 0.05033615604043007\n",
      "Epoch 2185, Loss: 0.2996198832988739, Final Batch Loss: 0.07955073565244675\n",
      "Epoch 2186, Loss: 0.24406207352876663, Final Batch Loss: 0.04745400696992874\n",
      "Epoch 2187, Loss: 0.29030897840857506, Final Batch Loss: 0.09710592031478882\n",
      "Epoch 2188, Loss: 0.28500127233564854, Final Batch Loss: 0.11181110143661499\n",
      "Epoch 2189, Loss: 0.38110487908124924, Final Batch Loss: 0.0705752745270729\n",
      "Epoch 2190, Loss: 0.3594088815152645, Final Batch Loss: 0.0579390786588192\n",
      "Epoch 2191, Loss: 0.35851506516337395, Final Batch Loss: 0.060256753116846085\n",
      "Epoch 2192, Loss: 0.34476298093795776, Final Batch Loss: 0.05328141152858734\n",
      "Epoch 2193, Loss: 0.31118201836943626, Final Batch Loss: 0.08392871916294098\n",
      "Epoch 2194, Loss: 0.5311335250735283, Final Batch Loss: 0.3453497588634491\n",
      "Epoch 2195, Loss: 0.47920625656843185, Final Batch Loss: 0.16305352747440338\n",
      "Epoch 2196, Loss: 0.3549495041370392, Final Batch Loss: 0.14739033579826355\n",
      "Epoch 2197, Loss: 0.4686730280518532, Final Batch Loss: 0.1503339409828186\n",
      "Epoch 2198, Loss: 0.3017488121986389, Final Batch Loss: 0.08613479882478714\n",
      "Epoch 2199, Loss: 0.3546718806028366, Final Batch Loss: 0.05487172305583954\n",
      "Epoch 2200, Loss: 0.31095497123897076, Final Batch Loss: 0.0282034482806921\n",
      "Epoch 2201, Loss: 0.32621457427740097, Final Batch Loss: 0.1375257819890976\n",
      "Epoch 2202, Loss: 0.34896399825811386, Final Batch Loss: 0.10460513830184937\n",
      "Epoch 2203, Loss: 0.44460220634937286, Final Batch Loss: 0.13573557138442993\n",
      "Epoch 2204, Loss: 0.3624330312013626, Final Batch Loss: 0.12052157521247864\n",
      "Epoch 2205, Loss: 0.31576697155833244, Final Batch Loss: 0.06045969948172569\n",
      "Epoch 2206, Loss: 0.36874430626630783, Final Batch Loss: 0.10205269604921341\n",
      "Epoch 2207, Loss: 0.4199841283261776, Final Batch Loss: 0.1906772404909134\n",
      "Epoch 2208, Loss: 0.35789383947849274, Final Batch Loss: 0.08109034597873688\n",
      "Epoch 2209, Loss: 0.3303286135196686, Final Batch Loss: 0.04732444882392883\n",
      "Epoch 2210, Loss: 0.3162194937467575, Final Batch Loss: 0.0446401908993721\n",
      "Epoch 2211, Loss: 0.37742749601602554, Final Batch Loss: 0.09088008850812912\n",
      "Epoch 2212, Loss: 0.2818670757114887, Final Batch Loss: 0.0754915401339531\n",
      "Epoch 2213, Loss: 0.38227975741028786, Final Batch Loss: 0.1082470640540123\n",
      "Epoch 2214, Loss: 0.37078047543764114, Final Batch Loss: 0.06944554299116135\n",
      "Epoch 2215, Loss: 0.37730205059051514, Final Batch Loss: 0.12382291257381439\n",
      "Epoch 2216, Loss: 0.39750581607222557, Final Batch Loss: 0.15292535722255707\n",
      "Epoch 2217, Loss: 0.2828757278621197, Final Batch Loss: 0.0563662014901638\n",
      "Epoch 2218, Loss: 0.43282371759414673, Final Batch Loss: 0.09062476456165314\n",
      "Epoch 2219, Loss: 0.25868700817227364, Final Batch Loss: 0.05327632278203964\n",
      "Epoch 2220, Loss: 0.3650146424770355, Final Batch Loss: 0.11216495931148529\n",
      "Epoch 2221, Loss: 0.38863594830036163, Final Batch Loss: 0.06655964255332947\n",
      "Epoch 2222, Loss: 0.36458079889416695, Final Batch Loss: 0.05181972309947014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2223, Loss: 0.40537581592798233, Final Batch Loss: 0.14968527853488922\n",
      "Epoch 2224, Loss: 0.395598903298378, Final Batch Loss: 0.13584282994270325\n",
      "Epoch 2225, Loss: 0.29858793690800667, Final Batch Loss: 0.05883224681019783\n",
      "Epoch 2226, Loss: 0.3779483959078789, Final Batch Loss: 0.08215969800949097\n",
      "Epoch 2227, Loss: 0.33267341554164886, Final Batch Loss: 0.03527330607175827\n",
      "Epoch 2228, Loss: 0.31206800788640976, Final Batch Loss: 0.07212959975004196\n",
      "Epoch 2229, Loss: 0.45568760484457016, Final Batch Loss: 0.07079152762889862\n",
      "Epoch 2230, Loss: 0.3120430260896683, Final Batch Loss: 0.06138528138399124\n",
      "Epoch 2231, Loss: 0.3394886627793312, Final Batch Loss: 0.04317876696586609\n",
      "Epoch 2232, Loss: 0.3591733053326607, Final Batch Loss: 0.1007036343216896\n",
      "Epoch 2233, Loss: 0.33829465508461, Final Batch Loss: 0.10135819017887115\n",
      "Epoch 2234, Loss: 0.27351994812488556, Final Batch Loss: 0.05266676843166351\n",
      "Epoch 2235, Loss: 0.4494825340807438, Final Batch Loss: 0.19386915862560272\n",
      "Epoch 2236, Loss: 0.3911905363202095, Final Batch Loss: 0.021266214549541473\n",
      "Epoch 2237, Loss: 0.3681847006082535, Final Batch Loss: 0.09437748789787292\n",
      "Epoch 2238, Loss: 0.41991516947746277, Final Batch Loss: 0.1001417487859726\n",
      "Epoch 2239, Loss: 0.3322603479027748, Final Batch Loss: 0.06471084803342819\n",
      "Epoch 2240, Loss: 0.4653497710824013, Final Batch Loss: 0.15220744907855988\n",
      "Epoch 2241, Loss: 0.4629424214363098, Final Batch Loss: 0.1658802181482315\n",
      "Epoch 2242, Loss: 0.2991749048233032, Final Batch Loss: 0.04143989086151123\n",
      "Epoch 2243, Loss: 0.3261256590485573, Final Batch Loss: 0.0324120968580246\n",
      "Epoch 2244, Loss: 0.28245692513883114, Final Batch Loss: 0.02706892602145672\n",
      "Epoch 2245, Loss: 0.3142157830297947, Final Batch Loss: 0.058407220989465714\n",
      "Epoch 2246, Loss: 0.26997239515185356, Final Batch Loss: 0.054185882210731506\n",
      "Epoch 2247, Loss: 0.42544762790203094, Final Batch Loss: 0.1283181607723236\n",
      "Epoch 2248, Loss: 0.35312460362911224, Final Batch Loss: 0.09049776941537857\n",
      "Epoch 2249, Loss: 0.2811039313673973, Final Batch Loss: 0.08243861794471741\n",
      "Epoch 2250, Loss: 0.3422171175479889, Final Batch Loss: 0.11558761447668076\n",
      "Epoch 2251, Loss: 0.42862405627965927, Final Batch Loss: 0.13590484857559204\n",
      "Epoch 2252, Loss: 0.38722001388669014, Final Batch Loss: 0.13241764903068542\n",
      "Epoch 2253, Loss: 0.3491463176906109, Final Batch Loss: 0.1474442034959793\n",
      "Epoch 2254, Loss: 0.30467400699853897, Final Batch Loss: 0.09621735662221909\n",
      "Epoch 2255, Loss: 0.2654966749250889, Final Batch Loss: 0.03760991245508194\n",
      "Epoch 2256, Loss: 0.3494555987417698, Final Batch Loss: 0.10405850410461426\n",
      "Epoch 2257, Loss: 0.26133934408426285, Final Batch Loss: 0.0667538046836853\n",
      "Epoch 2258, Loss: 0.3303958624601364, Final Batch Loss: 0.07223784923553467\n",
      "Epoch 2259, Loss: 0.29928045347332954, Final Batch Loss: 0.06690935045480728\n",
      "Epoch 2260, Loss: 0.2918512709438801, Final Batch Loss: 0.055223848670721054\n",
      "Epoch 2261, Loss: 0.3894325867295265, Final Batch Loss: 0.07821141183376312\n",
      "Epoch 2262, Loss: 0.35282840579748154, Final Batch Loss: 0.08639106154441833\n",
      "Epoch 2263, Loss: 0.2825269103050232, Final Batch Loss: 0.0890953466296196\n",
      "Epoch 2264, Loss: 0.37175409495830536, Final Batch Loss: 0.1133277490735054\n",
      "Epoch 2265, Loss: 0.38713284581899643, Final Batch Loss: 0.13258276879787445\n",
      "Epoch 2266, Loss: 0.3526116833090782, Final Batch Loss: 0.05315864086151123\n",
      "Epoch 2267, Loss: 0.31157106906175613, Final Batch Loss: 0.05896218866109848\n",
      "Epoch 2268, Loss: 0.357026182115078, Final Batch Loss: 0.12173644453287125\n",
      "Epoch 2269, Loss: 0.41643424704670906, Final Batch Loss: 0.1566096842288971\n",
      "Epoch 2270, Loss: 0.45020266994833946, Final Batch Loss: 0.16848179697990417\n",
      "Epoch 2271, Loss: 0.29041898623108864, Final Batch Loss: 0.04940974712371826\n",
      "Epoch 2272, Loss: 0.41067127883434296, Final Batch Loss: 0.10354148596525192\n",
      "Epoch 2273, Loss: 0.4734466038644314, Final Batch Loss: 0.237573504447937\n",
      "Epoch 2274, Loss: 0.4185495898127556, Final Batch Loss: 0.06705345213413239\n",
      "Epoch 2275, Loss: 0.310853723436594, Final Batch Loss: 0.07283324748277664\n",
      "Epoch 2276, Loss: 0.35833921283483505, Final Batch Loss: 0.10171972215175629\n",
      "Epoch 2277, Loss: 0.43387290835380554, Final Batch Loss: 0.17915649712085724\n",
      "Epoch 2278, Loss: 0.31579048559069633, Final Batch Loss: 0.10038788616657257\n",
      "Epoch 2279, Loss: 0.29007283598184586, Final Batch Loss: 0.03540005907416344\n",
      "Epoch 2280, Loss: 0.37604768574237823, Final Batch Loss: 0.0847015455365181\n",
      "Epoch 2281, Loss: 0.28339894115924835, Final Batch Loss: 0.09442801028490067\n",
      "Epoch 2282, Loss: 0.30948785692453384, Final Batch Loss: 0.11257560551166534\n",
      "Epoch 2283, Loss: 0.4152316525578499, Final Batch Loss: 0.12366931885480881\n",
      "Epoch 2284, Loss: 0.3681716024875641, Final Batch Loss: 0.15724408626556396\n",
      "Epoch 2285, Loss: 0.27394314482808113, Final Batch Loss: 0.032325755804777145\n",
      "Epoch 2286, Loss: 0.3725375570356846, Final Batch Loss: 0.051901403814554214\n",
      "Epoch 2287, Loss: 0.2265167161822319, Final Batch Loss: 0.04178455471992493\n",
      "Epoch 2288, Loss: 0.327494602650404, Final Batch Loss: 0.09013613313436508\n",
      "Epoch 2289, Loss: 0.3056635186076164, Final Batch Loss: 0.03995518386363983\n",
      "Epoch 2290, Loss: 0.26635831221938133, Final Batch Loss: 0.052855219691991806\n",
      "Epoch 2291, Loss: 0.33223798871040344, Final Batch Loss: 0.0905485600233078\n",
      "Epoch 2292, Loss: 0.3197008706629276, Final Batch Loss: 0.08455327153205872\n",
      "Epoch 2293, Loss: 0.3939148187637329, Final Batch Loss: 0.075285904109478\n",
      "Epoch 2294, Loss: 0.23175326734781265, Final Batch Loss: 0.02655649185180664\n",
      "Epoch 2295, Loss: 0.44051916152238846, Final Batch Loss: 0.16080500185489655\n",
      "Epoch 2296, Loss: 0.22741695307195187, Final Batch Loss: 0.023500526323914528\n",
      "Epoch 2297, Loss: 0.45792439207434654, Final Batch Loss: 0.060542311519384384\n",
      "Epoch 2298, Loss: 0.38247860595583916, Final Batch Loss: 0.06062967702746391\n",
      "Epoch 2299, Loss: 0.32567546889185905, Final Batch Loss: 0.10051707178354263\n",
      "Epoch 2300, Loss: 0.36373427510261536, Final Batch Loss: 0.10140833258628845\n",
      "Epoch 2301, Loss: 0.41296593844890594, Final Batch Loss: 0.156483456492424\n",
      "Epoch 2302, Loss: 0.40989724546670914, Final Batch Loss: 0.12457463145256042\n",
      "Epoch 2303, Loss: 0.33723044767975807, Final Batch Loss: 0.08804363012313843\n",
      "Epoch 2304, Loss: 0.3145385980606079, Final Batch Loss: 0.059639543294906616\n",
      "Epoch 2305, Loss: 0.3027358762919903, Final Batch Loss: 0.08107660710811615\n",
      "Epoch 2306, Loss: 0.38778890669345856, Final Batch Loss: 0.0976085439324379\n",
      "Epoch 2307, Loss: 0.2911924421787262, Final Batch Loss: 0.11521667242050171\n",
      "Epoch 2308, Loss: 0.3657134547829628, Final Batch Loss: 0.05620204657316208\n",
      "Epoch 2309, Loss: 0.3586617633700371, Final Batch Loss: 0.03546712547540665\n",
      "Epoch 2310, Loss: 0.3307008743286133, Final Batch Loss: 0.09010670334100723\n",
      "Epoch 2311, Loss: 0.3191283904016018, Final Batch Loss: 0.08252628147602081\n",
      "Epoch 2312, Loss: 0.3992144614458084, Final Batch Loss: 0.10990921407938004\n",
      "Epoch 2313, Loss: 0.3291793540120125, Final Batch Loss: 0.06601614505052567\n",
      "Epoch 2314, Loss: 0.3395300544798374, Final Batch Loss: 0.05468473210930824\n",
      "Epoch 2315, Loss: 0.4507417231798172, Final Batch Loss: 0.07169102877378464\n",
      "Epoch 2316, Loss: 0.3059283271431923, Final Batch Loss: 0.07516197115182877\n",
      "Epoch 2317, Loss: 0.36915150471031666, Final Batch Loss: 0.01660681702196598\n",
      "Epoch 2318, Loss: 0.30712558329105377, Final Batch Loss: 0.05572293698787689\n",
      "Epoch 2319, Loss: 0.2932749204337597, Final Batch Loss: 0.07409421354532242\n",
      "Epoch 2320, Loss: 0.3063153736293316, Final Batch Loss: 0.04654823616147041\n",
      "Epoch 2321, Loss: 0.44149669259786606, Final Batch Loss: 0.1372818797826767\n",
      "Epoch 2322, Loss: 0.28319815918803215, Final Batch Loss: 0.09527278691530228\n",
      "Epoch 2323, Loss: 0.2841666340827942, Final Batch Loss: 0.05879297852516174\n",
      "Epoch 2324, Loss: 0.3159153573215008, Final Batch Loss: 0.05304388329386711\n",
      "Epoch 2325, Loss: 0.2936738170683384, Final Batch Loss: 0.07181411236524582\n",
      "Epoch 2326, Loss: 0.3712163493037224, Final Batch Loss: 0.07895467430353165\n",
      "Epoch 2327, Loss: 0.32877950742840767, Final Batch Loss: 0.06859088689088821\n",
      "Epoch 2328, Loss: 0.4561890512704849, Final Batch Loss: 0.18426543474197388\n",
      "Epoch 2329, Loss: 0.40279579907655716, Final Batch Loss: 0.11758522689342499\n",
      "Epoch 2330, Loss: 0.36208270117640495, Final Batch Loss: 0.03693742677569389\n",
      "Epoch 2331, Loss: 0.32218803465366364, Final Batch Loss: 0.09332207590341568\n",
      "Epoch 2332, Loss: 0.23591405898332596, Final Batch Loss: 0.021749649196863174\n",
      "Epoch 2333, Loss: 0.4554023817181587, Final Batch Loss: 0.14931119978427887\n",
      "Epoch 2334, Loss: 0.45209068059921265, Final Batch Loss: 0.16714270412921906\n",
      "Epoch 2335, Loss: 0.38574791699647903, Final Batch Loss: 0.15197882056236267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2336, Loss: 0.4101961627602577, Final Batch Loss: 0.11588016897439957\n",
      "Epoch 2337, Loss: 0.28213270381093025, Final Batch Loss: 0.028696004301309586\n",
      "Epoch 2338, Loss: 0.3495509698987007, Final Batch Loss: 0.10496211796998978\n",
      "Epoch 2339, Loss: 0.42294809222221375, Final Batch Loss: 0.17408591508865356\n",
      "Epoch 2340, Loss: 0.4748389795422554, Final Batch Loss: 0.11476562917232513\n",
      "Epoch 2341, Loss: 0.45228318870067596, Final Batch Loss: 0.13905960321426392\n",
      "Epoch 2342, Loss: 0.3852080665528774, Final Batch Loss: 0.038389164954423904\n",
      "Epoch 2343, Loss: 0.33314650505781174, Final Batch Loss: 0.06293163448572159\n",
      "Epoch 2344, Loss: 0.45582424849271774, Final Batch Loss: 0.17951928079128265\n",
      "Epoch 2345, Loss: 0.40641002357006073, Final Batch Loss: 0.10207134485244751\n",
      "Epoch 2346, Loss: 0.45369040220975876, Final Batch Loss: 0.15159910917282104\n",
      "Epoch 2347, Loss: 0.3652988038957119, Final Batch Loss: 0.04479608312249184\n",
      "Epoch 2348, Loss: 0.374591164290905, Final Batch Loss: 0.06475530564785004\n",
      "Epoch 2349, Loss: 0.4246516302227974, Final Batch Loss: 0.15800689160823822\n",
      "Epoch 2350, Loss: 0.3854588493704796, Final Batch Loss: 0.11481467634439468\n",
      "Epoch 2351, Loss: 0.3737350404262543, Final Batch Loss: 0.08582141250371933\n",
      "Epoch 2352, Loss: 0.4040863811969757, Final Batch Loss: 0.025952830910682678\n",
      "Epoch 2353, Loss: 0.3286304771900177, Final Batch Loss: 0.07345887273550034\n",
      "Epoch 2354, Loss: 0.32253533601760864, Final Batch Loss: 0.07082060724496841\n",
      "Epoch 2355, Loss: 0.2816205993294716, Final Batch Loss: 0.02512621134519577\n",
      "Epoch 2356, Loss: 0.40355808287858963, Final Batch Loss: 0.0838962122797966\n",
      "Epoch 2357, Loss: 0.3248964510858059, Final Batch Loss: 0.04638402536511421\n",
      "Epoch 2358, Loss: 0.33570919558405876, Final Batch Loss: 0.11406397819519043\n",
      "Epoch 2359, Loss: 0.3734358660876751, Final Batch Loss: 0.12323056906461716\n",
      "Epoch 2360, Loss: 0.3199014216661453, Final Batch Loss: 0.033787988126277924\n",
      "Epoch 2361, Loss: 0.31435247510671616, Final Batch Loss: 0.05372774600982666\n",
      "Epoch 2362, Loss: 0.3487720265984535, Final Batch Loss: 0.07256276160478592\n",
      "Epoch 2363, Loss: 0.3492167629301548, Final Batch Loss: 0.1180311068892479\n",
      "Epoch 2364, Loss: 0.23718455620110035, Final Batch Loss: 0.01978149078786373\n",
      "Epoch 2365, Loss: 0.30495932698249817, Final Batch Loss: 0.07107929140329361\n",
      "Epoch 2366, Loss: 0.3748831935226917, Final Batch Loss: 0.070226289331913\n",
      "Epoch 2367, Loss: 0.3004687689244747, Final Batch Loss: 0.11802563071250916\n",
      "Epoch 2368, Loss: 0.3533132076263428, Final Batch Loss: 0.04971911758184433\n",
      "Epoch 2369, Loss: 0.33654220774769783, Final Batch Loss: 0.058264877647161484\n",
      "Epoch 2370, Loss: 0.2807825952768326, Final Batch Loss: 0.06263242661952972\n",
      "Epoch 2371, Loss: 0.31515875458717346, Final Batch Loss: 0.0591546893119812\n",
      "Epoch 2372, Loss: 0.407448410987854, Final Batch Loss: 0.16497543454170227\n",
      "Epoch 2373, Loss: 0.30925774946808815, Final Batch Loss: 0.04332299157977104\n",
      "Epoch 2374, Loss: 0.44395700842142105, Final Batch Loss: 0.14099939167499542\n",
      "Epoch 2375, Loss: 0.3989742547273636, Final Batch Loss: 0.06849236786365509\n",
      "Epoch 2376, Loss: 0.42621394991874695, Final Batch Loss: 0.10528811067342758\n",
      "Epoch 2377, Loss: 0.313051450997591, Final Batch Loss: 0.053679805248975754\n",
      "Epoch 2378, Loss: 0.3234952241182327, Final Batch Loss: 0.06666538119316101\n",
      "Epoch 2379, Loss: 0.32660220190882683, Final Batch Loss: 0.06095610186457634\n",
      "Epoch 2380, Loss: 0.25525498017668724, Final Batch Loss: 0.045665279030799866\n",
      "Epoch 2381, Loss: 0.23649030178785324, Final Batch Loss: 0.03328419476747513\n",
      "Epoch 2382, Loss: 0.39053450524806976, Final Batch Loss: 0.19031871855258942\n",
      "Epoch 2383, Loss: 0.3826388120651245, Final Batch Loss: 0.10104740411043167\n",
      "Epoch 2384, Loss: 0.242470171302557, Final Batch Loss: 0.04480917379260063\n",
      "Epoch 2385, Loss: 0.5405514985322952, Final Batch Loss: 0.13463369011878967\n",
      "Epoch 2386, Loss: 0.35669541358947754, Final Batch Loss: 0.08280450850725174\n",
      "Epoch 2387, Loss: 0.2920812517404556, Final Batch Loss: 0.05209576338529587\n",
      "Epoch 2388, Loss: 0.436821848154068, Final Batch Loss: 0.15358322858810425\n",
      "Epoch 2389, Loss: 0.26039913296699524, Final Batch Loss: 0.03980760648846626\n",
      "Epoch 2390, Loss: 0.42196351289749146, Final Batch Loss: 0.15781396627426147\n",
      "Epoch 2391, Loss: 0.40568241477012634, Final Batch Loss: 0.1354798674583435\n",
      "Epoch 2392, Loss: 0.37800344079732895, Final Batch Loss: 0.1128779947757721\n",
      "Epoch 2393, Loss: 0.4135947749018669, Final Batch Loss: 0.11030842363834381\n",
      "Epoch 2394, Loss: 0.4021984711289406, Final Batch Loss: 0.14399702847003937\n",
      "Epoch 2395, Loss: 0.5129454210400581, Final Batch Loss: 0.1597689986228943\n",
      "Epoch 2396, Loss: 0.502677358686924, Final Batch Loss: 0.21485905349254608\n",
      "Epoch 2397, Loss: 0.38713788241147995, Final Batch Loss: 0.06292352825403214\n",
      "Epoch 2398, Loss: 0.3515401463955641, Final Batch Loss: 0.024124769493937492\n",
      "Epoch 2399, Loss: 0.3590925633907318, Final Batch Loss: 0.09027576446533203\n",
      "Epoch 2400, Loss: 0.3970194533467293, Final Batch Loss: 0.14686566591262817\n",
      "Epoch 2401, Loss: 0.33584243059158325, Final Batch Loss: 0.07647999376058578\n",
      "Epoch 2402, Loss: 0.28371743857860565, Final Batch Loss: 0.032361678779125214\n",
      "Epoch 2403, Loss: 0.3541541025042534, Final Batch Loss: 0.06659646332263947\n",
      "Epoch 2404, Loss: 0.4336143881082535, Final Batch Loss: 0.15844492614269257\n",
      "Epoch 2405, Loss: 0.4584218040108681, Final Batch Loss: 0.17112776637077332\n",
      "Epoch 2406, Loss: 0.3299768231809139, Final Batch Loss: 0.046451348811388016\n",
      "Epoch 2407, Loss: 0.44682905822992325, Final Batch Loss: 0.21465368568897247\n",
      "Epoch 2408, Loss: 0.26263221725821495, Final Batch Loss: 0.04986996203660965\n",
      "Epoch 2409, Loss: 0.3005590923130512, Final Batch Loss: 0.03844008222222328\n",
      "Epoch 2410, Loss: 0.301469873636961, Final Batch Loss: 0.10953264683485031\n",
      "Epoch 2411, Loss: 0.275709493085742, Final Batch Loss: 0.02049260400235653\n",
      "Epoch 2412, Loss: 0.2934435270726681, Final Batch Loss: 0.050172481685876846\n",
      "Epoch 2413, Loss: 0.3713751770555973, Final Batch Loss: 0.13386507332324982\n",
      "Epoch 2414, Loss: 0.38619638979434967, Final Batch Loss: 0.13218890130519867\n",
      "Epoch 2415, Loss: 0.35929544270038605, Final Batch Loss: 0.19041545689105988\n",
      "Epoch 2416, Loss: 0.3975675478577614, Final Batch Loss: 0.1389339119195938\n",
      "Epoch 2417, Loss: 0.3312372863292694, Final Batch Loss: 0.08181403577327728\n",
      "Epoch 2418, Loss: 0.328046228736639, Final Batch Loss: 0.08610828965902328\n",
      "Epoch 2419, Loss: 0.2655515968799591, Final Batch Loss: 0.03940479829907417\n",
      "Epoch 2420, Loss: 0.29535187780857086, Final Batch Loss: 0.038559265434741974\n",
      "Epoch 2421, Loss: 0.36552298814058304, Final Batch Loss: 0.04435313493013382\n",
      "Epoch 2422, Loss: 0.20373252965509892, Final Batch Loss: 0.0364723764359951\n",
      "Epoch 2423, Loss: 0.26815012842416763, Final Batch Loss: 0.0382038950920105\n",
      "Epoch 2424, Loss: 0.3900020346045494, Final Batch Loss: 0.10786884278059006\n",
      "Epoch 2425, Loss: 0.3665327653288841, Final Batch Loss: 0.06988096982240677\n",
      "Epoch 2426, Loss: 0.30518676713109016, Final Batch Loss: 0.04740840941667557\n",
      "Epoch 2427, Loss: 0.3402073048055172, Final Batch Loss: 0.08410125970840454\n",
      "Epoch 2428, Loss: 0.2911982089281082, Final Batch Loss: 0.05714556574821472\n",
      "Epoch 2429, Loss: 0.28527189418673515, Final Batch Loss: 0.05831749364733696\n",
      "Epoch 2430, Loss: 0.3164282776415348, Final Batch Loss: 0.054419465363025665\n",
      "Epoch 2431, Loss: 0.2412189058959484, Final Batch Loss: 0.03156876191496849\n",
      "Epoch 2432, Loss: 0.2946985997259617, Final Batch Loss: 0.07904046773910522\n",
      "Epoch 2433, Loss: 0.34205029159784317, Final Batch Loss: 0.058617137372493744\n",
      "Epoch 2434, Loss: 0.24943769350647926, Final Batch Loss: 0.04455070197582245\n",
      "Epoch 2435, Loss: 0.4320341572165489, Final Batch Loss: 0.11484063416719437\n",
      "Epoch 2436, Loss: 0.24647833034396172, Final Batch Loss: 0.07701798528432846\n",
      "Epoch 2437, Loss: 0.370058324187994, Final Batch Loss: 0.15210314095020294\n",
      "Epoch 2438, Loss: 0.38623645156621933, Final Batch Loss: 0.08349829912185669\n",
      "Epoch 2439, Loss: 0.47279732674360275, Final Batch Loss: 0.16310621798038483\n",
      "Epoch 2440, Loss: 0.3704848699271679, Final Batch Loss: 0.10509950667619705\n",
      "Epoch 2441, Loss: 0.46915586292743683, Final Batch Loss: 0.1504833996295929\n",
      "Epoch 2442, Loss: 0.291359543800354, Final Batch Loss: 0.08009029924869537\n",
      "Epoch 2443, Loss: 0.31429148092865944, Final Batch Loss: 0.031759459525346756\n",
      "Epoch 2444, Loss: 0.3632082175463438, Final Batch Loss: 0.022125689312815666\n",
      "Epoch 2445, Loss: 0.3181835412979126, Final Batch Loss: 0.06701581180095673\n",
      "Epoch 2446, Loss: 0.3479818105697632, Final Batch Loss: 0.08186649531126022\n",
      "Epoch 2447, Loss: 0.4428733866661787, Final Batch Loss: 0.25030940771102905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2448, Loss: 0.4574846364557743, Final Batch Loss: 0.11588764190673828\n",
      "Epoch 2449, Loss: 0.3317002207040787, Final Batch Loss: 0.030883297324180603\n",
      "Epoch 2450, Loss: 0.32529525458812714, Final Batch Loss: 0.08954458683729172\n",
      "Epoch 2451, Loss: 0.34404466301202774, Final Batch Loss: 0.06475622206926346\n",
      "Epoch 2452, Loss: 0.283585537225008, Final Batch Loss: 0.0647025927901268\n",
      "Epoch 2453, Loss: 0.28439556062221527, Final Batch Loss: 0.08426927030086517\n",
      "Epoch 2454, Loss: 0.327700586989522, Final Batch Loss: 0.024303944781422615\n",
      "Epoch 2455, Loss: 0.30913543701171875, Final Batch Loss: 0.09036695957183838\n",
      "Epoch 2456, Loss: 0.339966781437397, Final Batch Loss: 0.08155329525470734\n",
      "Epoch 2457, Loss: 0.27823444828391075, Final Batch Loss: 0.04238159954547882\n",
      "Epoch 2458, Loss: 0.27260661870241165, Final Batch Loss: 0.047877684235572815\n",
      "Epoch 2459, Loss: 0.3654084652662277, Final Batch Loss: 0.074162058532238\n",
      "Epoch 2460, Loss: 0.32650409266352654, Final Batch Loss: 0.03322753682732582\n",
      "Epoch 2461, Loss: 0.37521761655807495, Final Batch Loss: 0.09139339625835419\n",
      "Epoch 2462, Loss: 0.2526920214295387, Final Batch Loss: 0.04821863770484924\n",
      "Epoch 2463, Loss: 0.31852009147405624, Final Batch Loss: 0.07691823691129684\n",
      "Epoch 2464, Loss: 0.36079002171754837, Final Batch Loss: 0.10514437407255173\n",
      "Epoch 2465, Loss: 0.33181852847337723, Final Batch Loss: 0.028221994638442993\n",
      "Epoch 2466, Loss: 0.3129656985402107, Final Batch Loss: 0.06910735368728638\n",
      "Epoch 2467, Loss: 0.27644145116209984, Final Batch Loss: 0.04326998069882393\n",
      "Epoch 2468, Loss: 0.3571137934923172, Final Batch Loss: 0.06839685142040253\n",
      "Epoch 2469, Loss: 0.3863038718700409, Final Batch Loss: 0.1187213733792305\n",
      "Epoch 2470, Loss: 0.3294157572090626, Final Batch Loss: 0.05605668947100639\n",
      "Epoch 2471, Loss: 0.27476103976368904, Final Batch Loss: 0.03904416784644127\n",
      "Epoch 2472, Loss: 0.34836337715387344, Final Batch Loss: 0.06929928064346313\n",
      "Epoch 2473, Loss: 0.3309314511716366, Final Batch Loss: 0.14344123005867004\n",
      "Epoch 2474, Loss: 0.36107610911130905, Final Batch Loss: 0.10899786651134491\n",
      "Epoch 2475, Loss: 0.3576884754002094, Final Batch Loss: 0.11648116260766983\n",
      "Epoch 2476, Loss: 0.37590889260172844, Final Batch Loss: 0.1098666861653328\n",
      "Epoch 2477, Loss: 0.28118161484599113, Final Batch Loss: 0.0641501322388649\n",
      "Epoch 2478, Loss: 0.31279294937849045, Final Batch Loss: 0.033960748463869095\n",
      "Epoch 2479, Loss: 0.36478341370821, Final Batch Loss: 0.11515463143587112\n",
      "Epoch 2480, Loss: 0.37417715042829514, Final Batch Loss: 0.10723171383142471\n",
      "Epoch 2481, Loss: 0.374057499691844, Final Batch Loss: 0.02703707478940487\n",
      "Epoch 2482, Loss: 0.4301252216100693, Final Batch Loss: 0.14943984150886536\n",
      "Epoch 2483, Loss: 0.2676069736480713, Final Batch Loss: 0.08486603200435638\n",
      "Epoch 2484, Loss: 0.3331567868590355, Final Batch Loss: 0.10678791254758835\n",
      "Epoch 2485, Loss: 0.263332549482584, Final Batch Loss: 0.03803250938653946\n",
      "Epoch 2486, Loss: 0.2990945242345333, Final Batch Loss: 0.05104035511612892\n",
      "Epoch 2487, Loss: 0.3288160525262356, Final Batch Loss: 0.10588717460632324\n",
      "Epoch 2488, Loss: 0.2995719090104103, Final Batch Loss: 0.0790090262889862\n",
      "Epoch 2489, Loss: 0.40294720232486725, Final Batch Loss: 0.11611321568489075\n",
      "Epoch 2490, Loss: 0.3158780559897423, Final Batch Loss: 0.07429856061935425\n",
      "Epoch 2491, Loss: 0.3330649882555008, Final Batch Loss: 0.05285409837961197\n",
      "Epoch 2492, Loss: 0.32616346701979637, Final Batch Loss: 0.10331390798091888\n",
      "Epoch 2493, Loss: 0.48929616808891296, Final Batch Loss: 0.1991773545742035\n",
      "Epoch 2494, Loss: 0.3548818342387676, Final Batch Loss: 0.09120912104845047\n",
      "Epoch 2495, Loss: 0.36796969920396805, Final Batch Loss: 0.05203236639499664\n",
      "Epoch 2496, Loss: 0.3763142116367817, Final Batch Loss: 0.06056996062397957\n",
      "Epoch 2497, Loss: 0.39557362347841263, Final Batch Loss: 0.08421478420495987\n",
      "Epoch 2498, Loss: 0.3479512259364128, Final Batch Loss: 0.07675834745168686\n",
      "Epoch 2499, Loss: 0.35198503732681274, Final Batch Loss: 0.058950409293174744\n",
      "Epoch 2500, Loss: 0.35788141563534737, Final Batch Loss: 0.12804251909255981\n",
      "Epoch 2501, Loss: 0.2968572899699211, Final Batch Loss: 0.08856000006198883\n",
      "Epoch 2502, Loss: 0.27769680321216583, Final Batch Loss: 0.07018353044986725\n",
      "Epoch 2503, Loss: 0.4174259901046753, Final Batch Loss: 0.10944948345422745\n",
      "Epoch 2504, Loss: 0.3720388077199459, Final Batch Loss: 0.12627890706062317\n",
      "Epoch 2505, Loss: 0.3523253984749317, Final Batch Loss: 0.09186436235904694\n",
      "Epoch 2506, Loss: 0.4217449948191643, Final Batch Loss: 0.16404573619365692\n",
      "Epoch 2507, Loss: 0.5103193372488022, Final Batch Loss: 0.04967569559812546\n",
      "Epoch 2508, Loss: 0.3055374175310135, Final Batch Loss: 0.05389173701405525\n",
      "Epoch 2509, Loss: 0.3263138718903065, Final Batch Loss: 0.08366291970014572\n",
      "Epoch 2510, Loss: 0.37237006425857544, Final Batch Loss: 0.06392928212881088\n",
      "Epoch 2511, Loss: 0.3657124266028404, Final Batch Loss: 0.03752406686544418\n",
      "Epoch 2512, Loss: 0.3507956936955452, Final Batch Loss: 0.12858814001083374\n",
      "Epoch 2513, Loss: 0.493963360786438, Final Batch Loss: 0.14114589989185333\n",
      "Epoch 2514, Loss: 0.2480140421539545, Final Batch Loss: 0.022590236738324165\n",
      "Epoch 2515, Loss: 0.3377118594944477, Final Batch Loss: 0.04904210940003395\n",
      "Epoch 2516, Loss: 0.3189650811254978, Final Batch Loss: 0.09713565558195114\n",
      "Epoch 2517, Loss: 0.296998955309391, Final Batch Loss: 0.04159029945731163\n",
      "Epoch 2518, Loss: 0.28553128615021706, Final Batch Loss: 0.08399248868227005\n",
      "Epoch 2519, Loss: 0.364043191075325, Final Batch Loss: 0.14916777610778809\n",
      "Epoch 2520, Loss: 0.2586877942085266, Final Batch Loss: 0.05441340431571007\n",
      "Epoch 2521, Loss: 0.4148336537182331, Final Batch Loss: 0.1936216652393341\n",
      "Epoch 2522, Loss: 0.49222324788570404, Final Batch Loss: 0.12873587012290955\n",
      "Epoch 2523, Loss: 0.37907397374510765, Final Batch Loss: 0.037986334413290024\n",
      "Epoch 2524, Loss: 0.33635152876377106, Final Batch Loss: 0.0910537838935852\n",
      "Epoch 2525, Loss: 0.2873752936720848, Final Batch Loss: 0.09629176557064056\n",
      "Epoch 2526, Loss: 0.37367672473192215, Final Batch Loss: 0.06490698456764221\n",
      "Epoch 2527, Loss: 0.35040362924337387, Final Batch Loss: 0.09019850939512253\n",
      "Epoch 2528, Loss: 0.2756742835044861, Final Batch Loss: 0.07511751353740692\n",
      "Epoch 2529, Loss: 0.3982761614024639, Final Batch Loss: 0.17689572274684906\n",
      "Epoch 2530, Loss: 0.37138984352350235, Final Batch Loss: 0.09978111833333969\n",
      "Epoch 2531, Loss: 0.372445248067379, Final Batch Loss: 0.09940853714942932\n",
      "Epoch 2532, Loss: 0.2977282293140888, Final Batch Loss: 0.04360255226492882\n",
      "Epoch 2533, Loss: 0.4183211252093315, Final Batch Loss: 0.16461621224880219\n",
      "Epoch 2534, Loss: 0.2939346507191658, Final Batch Loss: 0.07197405397891998\n",
      "Epoch 2535, Loss: 0.34650422632694244, Final Batch Loss: 0.10863276571035385\n",
      "Epoch 2536, Loss: 0.44182832539081573, Final Batch Loss: 0.11115384101867676\n",
      "Epoch 2537, Loss: 0.38493533805012703, Final Batch Loss: 0.05483150854706764\n",
      "Epoch 2538, Loss: 0.27003396674990654, Final Batch Loss: 0.07211685925722122\n",
      "Epoch 2539, Loss: 0.5262380912899971, Final Batch Loss: 0.1738257110118866\n",
      "Epoch 2540, Loss: 0.35662252083420753, Final Batch Loss: 0.07641768455505371\n",
      "Epoch 2541, Loss: 0.338118065148592, Final Batch Loss: 0.12108233571052551\n",
      "Epoch 2542, Loss: 0.33085836842656136, Final Batch Loss: 0.059944115579128265\n",
      "Epoch 2543, Loss: 0.30681222304701805, Final Batch Loss: 0.03752729669213295\n",
      "Epoch 2544, Loss: 0.4434749037027359, Final Batch Loss: 0.09364466369152069\n",
      "Epoch 2545, Loss: 0.3858001232147217, Final Batch Loss: 0.10064169764518738\n",
      "Epoch 2546, Loss: 0.29126113280653954, Final Batch Loss: 0.07076016813516617\n",
      "Epoch 2547, Loss: 0.3597658798098564, Final Batch Loss: 0.08547643572092056\n",
      "Epoch 2548, Loss: 0.46220435947179794, Final Batch Loss: 0.12188532203435898\n",
      "Epoch 2549, Loss: 0.3697386123239994, Final Batch Loss: 0.12819933891296387\n",
      "Epoch 2550, Loss: 0.3460025377571583, Final Batch Loss: 0.03766176477074623\n",
      "Epoch 2551, Loss: 0.45185675472021103, Final Batch Loss: 0.11989282071590424\n",
      "Epoch 2552, Loss: 0.4062468558549881, Final Batch Loss: 0.10820583999156952\n",
      "Epoch 2553, Loss: 0.3333387188613415, Final Batch Loss: 0.06253859400749207\n",
      "Epoch 2554, Loss: 0.4206279218196869, Final Batch Loss: 0.07732236385345459\n",
      "Epoch 2555, Loss: 0.3759643882513046, Final Batch Loss: 0.09109510481357574\n",
      "Epoch 2556, Loss: 0.3118368610739708, Final Batch Loss: 0.10450264811515808\n",
      "Epoch 2557, Loss: 0.3118162304162979, Final Batch Loss: 0.07436922192573547\n",
      "Epoch 2558, Loss: 0.33279287815093994, Final Batch Loss: 0.04323422908782959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2559, Loss: 0.2725042290985584, Final Batch Loss: 0.062442880123853683\n",
      "Epoch 2560, Loss: 0.35486697778105736, Final Batch Loss: 0.15073484182357788\n",
      "Epoch 2561, Loss: 0.3614252768456936, Final Batch Loss: 0.16195853054523468\n",
      "Epoch 2562, Loss: 0.3021077774465084, Final Batch Loss: 0.08280804753303528\n",
      "Epoch 2563, Loss: 0.3202517703175545, Final Batch Loss: 0.0629279762506485\n",
      "Epoch 2564, Loss: 0.3674117997288704, Final Batch Loss: 0.12304110080003738\n",
      "Epoch 2565, Loss: 0.3355265483260155, Final Batch Loss: 0.08990292996168137\n",
      "Epoch 2566, Loss: 0.26131507381796837, Final Batch Loss: 0.07532957941293716\n",
      "Epoch 2567, Loss: 0.3354734927415848, Final Batch Loss: 0.027974113821983337\n",
      "Epoch 2568, Loss: 0.3599918819963932, Final Batch Loss: 0.11430320143699646\n",
      "Epoch 2569, Loss: 0.37007417529821396, Final Batch Loss: 0.11189782619476318\n",
      "Epoch 2570, Loss: 0.36667221039533615, Final Batch Loss: 0.12378457933664322\n",
      "Epoch 2571, Loss: 0.32380181178450584, Final Batch Loss: 0.10597813874483109\n",
      "Epoch 2572, Loss: 0.3103732094168663, Final Batch Loss: 0.06867271661758423\n",
      "Epoch 2573, Loss: 0.32816873118281364, Final Batch Loss: 0.094239242374897\n",
      "Epoch 2574, Loss: 0.3869161829352379, Final Batch Loss: 0.13504530489444733\n",
      "Epoch 2575, Loss: 0.2589986138045788, Final Batch Loss: 0.054767027497291565\n",
      "Epoch 2576, Loss: 0.282642163336277, Final Batch Loss: 0.05871504545211792\n",
      "Epoch 2577, Loss: 0.296836256980896, Final Batch Loss: 0.06118474528193474\n",
      "Epoch 2578, Loss: 0.4010840132832527, Final Batch Loss: 0.1914019137620926\n",
      "Epoch 2579, Loss: 0.3207521364092827, Final Batch Loss: 0.03912085294723511\n",
      "Epoch 2580, Loss: 0.29264434427022934, Final Batch Loss: 0.030604757368564606\n",
      "Epoch 2581, Loss: 0.39175692945718765, Final Batch Loss: 0.19237573444843292\n",
      "Epoch 2582, Loss: 0.28748682141304016, Final Batch Loss: 0.0491650253534317\n",
      "Epoch 2583, Loss: 0.38940883055329323, Final Batch Loss: 0.08998804539442062\n",
      "Epoch 2584, Loss: 0.3458397015929222, Final Batch Loss: 0.08803625404834747\n",
      "Epoch 2585, Loss: 0.3205914720892906, Final Batch Loss: 0.03939969837665558\n",
      "Epoch 2586, Loss: 0.3271314837038517, Final Batch Loss: 0.040333863347768784\n",
      "Epoch 2587, Loss: 0.4106023535132408, Final Batch Loss: 0.04468045383691788\n",
      "Epoch 2588, Loss: 0.24534102901816368, Final Batch Loss: 0.06997231394052505\n",
      "Epoch 2589, Loss: 0.3634971156716347, Final Batch Loss: 0.15732866525650024\n",
      "Epoch 2590, Loss: 0.4004022628068924, Final Batch Loss: 0.10222870111465454\n",
      "Epoch 2591, Loss: 0.2556844502687454, Final Batch Loss: 0.03245462477207184\n",
      "Epoch 2592, Loss: 0.49352961778640747, Final Batch Loss: 0.19366507232189178\n",
      "Epoch 2593, Loss: 0.28912875056266785, Final Batch Loss: 0.05848586559295654\n",
      "Epoch 2594, Loss: 0.418144054710865, Final Batch Loss: 0.07476247847080231\n",
      "Epoch 2595, Loss: 0.4022631384432316, Final Batch Loss: 0.05586058273911476\n",
      "Epoch 2596, Loss: 0.2572387754917145, Final Batch Loss: 0.04024304822087288\n",
      "Epoch 2597, Loss: 0.30202409997582436, Final Batch Loss: 0.022963471710681915\n",
      "Epoch 2598, Loss: 0.4179586172103882, Final Batch Loss: 0.104270339012146\n",
      "Epoch 2599, Loss: 0.378894355148077, Final Batch Loss: 0.14274561405181885\n",
      "Epoch 2600, Loss: 0.36087632179260254, Final Batch Loss: 0.09382431209087372\n",
      "Epoch 2601, Loss: 0.31809152849018574, Final Batch Loss: 0.030071904882788658\n",
      "Epoch 2602, Loss: 0.3882707878947258, Final Batch Loss: 0.07318335771560669\n",
      "Epoch 2603, Loss: 0.3412013500928879, Final Batch Loss: 0.08859401941299438\n",
      "Epoch 2604, Loss: 0.3531657010316849, Final Batch Loss: 0.055458419024944305\n",
      "Epoch 2605, Loss: 0.3031245768070221, Final Batch Loss: 0.07770750671625137\n",
      "Epoch 2606, Loss: 0.3507242575287819, Final Batch Loss: 0.08606939017772675\n",
      "Epoch 2607, Loss: 0.2337108924984932, Final Batch Loss: 0.0627346858382225\n",
      "Epoch 2608, Loss: 0.3796752244234085, Final Batch Loss: 0.08630574494600296\n",
      "Epoch 2609, Loss: 0.2929675970226526, Final Batch Loss: 0.021975906565785408\n",
      "Epoch 2610, Loss: 0.3373069390654564, Final Batch Loss: 0.06654751300811768\n",
      "Epoch 2611, Loss: 0.3366248905658722, Final Batch Loss: 0.0871213749051094\n",
      "Epoch 2612, Loss: 0.374329648911953, Final Batch Loss: 0.15538708865642548\n",
      "Epoch 2613, Loss: 0.3197080250829458, Final Batch Loss: 0.06300520151853561\n",
      "Epoch 2614, Loss: 0.3382132276892662, Final Batch Loss: 0.15402984619140625\n",
      "Epoch 2615, Loss: 0.35345522686839104, Final Batch Loss: 0.15291815996170044\n",
      "Epoch 2616, Loss: 0.29991083964705467, Final Batch Loss: 0.07799363136291504\n",
      "Epoch 2617, Loss: 0.2570938691496849, Final Batch Loss: 0.0443313829600811\n",
      "Epoch 2618, Loss: 0.3943985402584076, Final Batch Loss: 0.15672637522220612\n",
      "Epoch 2619, Loss: 0.2330196276307106, Final Batch Loss: 0.03969526290893555\n",
      "Epoch 2620, Loss: 0.3201947547495365, Final Batch Loss: 0.117420993745327\n",
      "Epoch 2621, Loss: 0.2849055044353008, Final Batch Loss: 0.03966740891337395\n",
      "Epoch 2622, Loss: 0.31128570809960365, Final Batch Loss: 0.09296144545078278\n",
      "Epoch 2623, Loss: 0.3184095975011587, Final Batch Loss: 0.01989383064210415\n",
      "Epoch 2624, Loss: 0.3420696407556534, Final Batch Loss: 0.0970747172832489\n",
      "Epoch 2625, Loss: 0.33997830003499985, Final Batch Loss: 0.1387283354997635\n",
      "Epoch 2626, Loss: 0.25746235623955727, Final Batch Loss: 0.04655539244413376\n",
      "Epoch 2627, Loss: 0.4561859779059887, Final Batch Loss: 0.17963290214538574\n",
      "Epoch 2628, Loss: 0.38698020577430725, Final Batch Loss: 0.09125636518001556\n",
      "Epoch 2629, Loss: 0.38009653985500336, Final Batch Loss: 0.13081978261470795\n",
      "Epoch 2630, Loss: 0.28863056749105453, Final Batch Loss: 0.08582552522420883\n",
      "Epoch 2631, Loss: 0.43136656284332275, Final Batch Loss: 0.16243918240070343\n",
      "Epoch 2632, Loss: 0.3287256248295307, Final Batch Loss: 0.07872091233730316\n",
      "Epoch 2633, Loss: 0.29728036001324654, Final Batch Loss: 0.06181492656469345\n",
      "Epoch 2634, Loss: 0.3010079264640808, Final Batch Loss: 0.07116124778985977\n",
      "Epoch 2635, Loss: 0.5933574587106705, Final Batch Loss: 0.24411310255527496\n",
      "Epoch 2636, Loss: 0.32528837211430073, Final Batch Loss: 0.028587045148015022\n",
      "Epoch 2637, Loss: 0.4112795367836952, Final Batch Loss: 0.13286535441875458\n",
      "Epoch 2638, Loss: 0.39489351585507393, Final Batch Loss: 0.06069783493876457\n",
      "Epoch 2639, Loss: 0.5081645771861076, Final Batch Loss: 0.17766234278678894\n",
      "Epoch 2640, Loss: 0.2682502456009388, Final Batch Loss: 0.042919281870126724\n",
      "Epoch 2641, Loss: 0.3490735962986946, Final Batch Loss: 0.0883459597826004\n",
      "Epoch 2642, Loss: 0.3613494671881199, Final Batch Loss: 0.07302872836589813\n",
      "Epoch 2643, Loss: 0.3545408323407173, Final Batch Loss: 0.09925339370965958\n",
      "Epoch 2644, Loss: 0.44923682510852814, Final Batch Loss: 0.07813169062137604\n",
      "Epoch 2645, Loss: 0.2976566031575203, Final Batch Loss: 0.12921495735645294\n",
      "Epoch 2646, Loss: 0.42205213755369186, Final Batch Loss: 0.21205604076385498\n",
      "Epoch 2647, Loss: 0.3698149807751179, Final Batch Loss: 0.13900627195835114\n",
      "Epoch 2648, Loss: 0.31704724580049515, Final Batch Loss: 0.08523167669773102\n",
      "Epoch 2649, Loss: 0.43736111000180244, Final Batch Loss: 0.16744501888751984\n",
      "Epoch 2650, Loss: 0.34116460382938385, Final Batch Loss: 0.08373621106147766\n",
      "Epoch 2651, Loss: 0.35099585354328156, Final Batch Loss: 0.06947172433137894\n",
      "Epoch 2652, Loss: 0.27815456315875053, Final Batch Loss: 0.05175105854868889\n",
      "Epoch 2653, Loss: 0.33833521604537964, Final Batch Loss: 0.06104046851396561\n",
      "Epoch 2654, Loss: 0.29299452528357506, Final Batch Loss: 0.061134617775678635\n",
      "Epoch 2655, Loss: 0.4043267220258713, Final Batch Loss: 0.05079617351293564\n",
      "Epoch 2656, Loss: 0.3270731121301651, Final Batch Loss: 0.08401858061552048\n",
      "Epoch 2657, Loss: 0.4052915722131729, Final Batch Loss: 0.1669483482837677\n",
      "Epoch 2658, Loss: 0.3926410973072052, Final Batch Loss: 0.1150207445025444\n",
      "Epoch 2659, Loss: 0.4871123284101486, Final Batch Loss: 0.13367576897144318\n",
      "Epoch 2660, Loss: 0.37460967898368835, Final Batch Loss: 0.09853316843509674\n",
      "Epoch 2661, Loss: 0.4698225408792496, Final Batch Loss: 0.103090800344944\n",
      "Epoch 2662, Loss: 0.3067058399319649, Final Batch Loss: 0.04718950763344765\n",
      "Epoch 2663, Loss: 0.35057324916124344, Final Batch Loss: 0.0956210345029831\n",
      "Epoch 2664, Loss: 0.3529299721121788, Final Batch Loss: 0.13133107125759125\n",
      "Epoch 2665, Loss: 0.3092131968587637, Final Batch Loss: 0.020390866324305534\n",
      "Epoch 2666, Loss: 0.3591150641441345, Final Batch Loss: 0.0502827912569046\n",
      "Epoch 2667, Loss: 0.3496032878756523, Final Batch Loss: 0.08273777365684509\n",
      "Epoch 2668, Loss: 0.3722919560968876, Final Batch Loss: 0.16424350440502167\n",
      "Epoch 2669, Loss: 0.34937935695052147, Final Batch Loss: 0.1029801219701767\n",
      "Epoch 2670, Loss: 0.25920525193214417, Final Batch Loss: 0.03609894961118698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2671, Loss: 0.44982510060071945, Final Batch Loss: 0.09056410193443298\n",
      "Epoch 2672, Loss: 0.3465760126709938, Final Batch Loss: 0.10071615874767303\n",
      "Epoch 2673, Loss: 0.2334772925823927, Final Batch Loss: 0.019662370905280113\n",
      "Epoch 2674, Loss: 0.3289535567164421, Final Batch Loss: 0.08662568032741547\n",
      "Epoch 2675, Loss: 0.41282568871974945, Final Batch Loss: 0.10820304602384567\n",
      "Epoch 2676, Loss: 0.32031701132655144, Final Batch Loss: 0.10178050398826599\n",
      "Epoch 2677, Loss: 0.278983311727643, Final Batch Loss: 0.023196162655949593\n",
      "Epoch 2678, Loss: 0.4127861335873604, Final Batch Loss: 0.15967264771461487\n",
      "Epoch 2679, Loss: 0.32359763234853745, Final Batch Loss: 0.07746195048093796\n",
      "Epoch 2680, Loss: 0.2651307098567486, Final Batch Loss: 0.08037618547677994\n",
      "Epoch 2681, Loss: 0.3323122300207615, Final Batch Loss: 0.048341695219278336\n",
      "Epoch 2682, Loss: 0.2572747692465782, Final Batch Loss: 0.05215734615921974\n",
      "Epoch 2683, Loss: 0.2843859866261482, Final Batch Loss: 0.05552233010530472\n",
      "Epoch 2684, Loss: 0.33317407965660095, Final Batch Loss: 0.09480932354927063\n",
      "Epoch 2685, Loss: 0.30062827467918396, Final Batch Loss: 0.04793165624141693\n",
      "Epoch 2686, Loss: 0.4137700982391834, Final Batch Loss: 0.19041118025779724\n",
      "Epoch 2687, Loss: 0.43802811205387115, Final Batch Loss: 0.16952890157699585\n",
      "Epoch 2688, Loss: 0.36429891735315323, Final Batch Loss: 0.09936497360467911\n",
      "Epoch 2689, Loss: 0.4546946957707405, Final Batch Loss: 0.1579410880804062\n",
      "Epoch 2690, Loss: 0.3096977397799492, Final Batch Loss: 0.06498731672763824\n",
      "Epoch 2691, Loss: 0.45084715634584427, Final Batch Loss: 0.16935931146144867\n",
      "Epoch 2692, Loss: 0.34519296139478683, Final Batch Loss: 0.09194744378328323\n",
      "Epoch 2693, Loss: 0.43781037628650665, Final Batch Loss: 0.11749318242073059\n",
      "Epoch 2694, Loss: 0.4176286458969116, Final Batch Loss: 0.15739814937114716\n",
      "Epoch 2695, Loss: 0.36891815438866615, Final Batch Loss: 0.12673892080783844\n",
      "Epoch 2696, Loss: 0.2794417664408684, Final Batch Loss: 0.027406755834817886\n",
      "Epoch 2697, Loss: 0.3148891367018223, Final Batch Loss: 0.08365880697965622\n",
      "Epoch 2698, Loss: 0.3874189592897892, Final Batch Loss: 0.07982196658849716\n",
      "Epoch 2699, Loss: 0.31221940740942955, Final Batch Loss: 0.04917499050498009\n",
      "Epoch 2700, Loss: 0.41017087548971176, Final Batch Loss: 0.09577757120132446\n",
      "Epoch 2701, Loss: 0.32858507707715034, Final Batch Loss: 0.052223626524209976\n",
      "Epoch 2702, Loss: 0.3322492241859436, Final Batch Loss: 0.06559570133686066\n",
      "Epoch 2703, Loss: 0.3390786238014698, Final Batch Loss: 0.03965434059500694\n",
      "Epoch 2704, Loss: 0.24042869359254837, Final Batch Loss: 0.03204498440027237\n",
      "Epoch 2705, Loss: 0.2968924753367901, Final Batch Loss: 0.044657979160547256\n",
      "Epoch 2706, Loss: 0.29761936888098717, Final Batch Loss: 0.10232316702604294\n",
      "Epoch 2707, Loss: 0.25425391271710396, Final Batch Loss: 0.05604870617389679\n",
      "Epoch 2708, Loss: 0.27793167904019356, Final Batch Loss: 0.06253769993782043\n",
      "Epoch 2709, Loss: 0.278912790119648, Final Batch Loss: 0.02831624448299408\n",
      "Epoch 2710, Loss: 0.3138599060475826, Final Batch Loss: 0.11025325208902359\n",
      "Epoch 2711, Loss: 0.23067593947052956, Final Batch Loss: 0.07041911780834198\n",
      "Epoch 2712, Loss: 0.37795402109622955, Final Batch Loss: 0.11109374463558197\n",
      "Epoch 2713, Loss: 0.3882349655032158, Final Batch Loss: 0.050613194704055786\n",
      "Epoch 2714, Loss: 0.24967274628579617, Final Batch Loss: 0.018475467339158058\n",
      "Epoch 2715, Loss: 0.28341710567474365, Final Batch Loss: 0.03442315012216568\n",
      "Epoch 2716, Loss: 0.3254415839910507, Final Batch Loss: 0.0997428297996521\n",
      "Epoch 2717, Loss: 0.21325598284602165, Final Batch Loss: 0.02308439090847969\n",
      "Epoch 2718, Loss: 0.3452644683420658, Final Batch Loss: 0.126373291015625\n",
      "Epoch 2719, Loss: 0.23958507366478443, Final Batch Loss: 0.026578174903988838\n",
      "Epoch 2720, Loss: 0.46928324550390244, Final Batch Loss: 0.18983305990695953\n",
      "Epoch 2721, Loss: 0.29918649047613144, Final Batch Loss: 0.042797498404979706\n",
      "Epoch 2722, Loss: 0.35164131596684456, Final Batch Loss: 0.06234311684966087\n",
      "Epoch 2723, Loss: 0.37180404365062714, Final Batch Loss: 0.1247297152876854\n",
      "Epoch 2724, Loss: 0.301140233874321, Final Batch Loss: 0.042517174035310745\n",
      "Epoch 2725, Loss: 0.3326087035238743, Final Batch Loss: 0.14979766309261322\n",
      "Epoch 2726, Loss: 0.4249293804168701, Final Batch Loss: 0.09256904572248459\n",
      "Epoch 2727, Loss: 0.32806936651468277, Final Batch Loss: 0.08834882080554962\n",
      "Epoch 2728, Loss: 0.3156871683895588, Final Batch Loss: 0.06383666396141052\n",
      "Epoch 2729, Loss: 0.4506194740533829, Final Batch Loss: 0.11539312452077866\n",
      "Epoch 2730, Loss: 0.2237023040652275, Final Batch Loss: 0.044434934854507446\n",
      "Epoch 2731, Loss: 0.34268227219581604, Final Batch Loss: 0.08571323752403259\n",
      "Epoch 2732, Loss: 0.3186449855566025, Final Batch Loss: 0.05950610712170601\n",
      "Epoch 2733, Loss: 0.3190658167004585, Final Batch Loss: 0.08276965469121933\n",
      "Epoch 2734, Loss: 0.3934565559029579, Final Batch Loss: 0.09464101493358612\n",
      "Epoch 2735, Loss: 0.30228960886597633, Final Batch Loss: 0.05564899370074272\n",
      "Epoch 2736, Loss: 0.3751375526189804, Final Batch Loss: 0.13629138469696045\n",
      "Epoch 2737, Loss: 0.4743538424372673, Final Batch Loss: 0.20894025266170502\n",
      "Epoch 2738, Loss: 0.3804929628968239, Final Batch Loss: 0.090749092400074\n",
      "Epoch 2739, Loss: 0.303742241114378, Final Batch Loss: 0.0763610303401947\n",
      "Epoch 2740, Loss: 0.29642773047089577, Final Batch Loss: 0.04081401601433754\n",
      "Epoch 2741, Loss: 0.23955762386322021, Final Batch Loss: 0.0378919392824173\n",
      "Epoch 2742, Loss: 0.3359547331929207, Final Batch Loss: 0.037956491112709045\n",
      "Epoch 2743, Loss: 0.29125758074223995, Final Batch Loss: 0.020858367905020714\n",
      "Epoch 2744, Loss: 0.3018806278705597, Final Batch Loss: 0.07992628216743469\n",
      "Epoch 2745, Loss: 0.46253446862101555, Final Batch Loss: 0.1738925725221634\n",
      "Epoch 2746, Loss: 0.41729745268821716, Final Batch Loss: 0.12725308537483215\n",
      "Epoch 2747, Loss: 0.27883192524313927, Final Batch Loss: 0.09673266112804413\n",
      "Epoch 2748, Loss: 0.3273463845252991, Final Batch Loss: 0.047191619873046875\n",
      "Epoch 2749, Loss: 0.3272635042667389, Final Batch Loss: 0.04187095910310745\n",
      "Epoch 2750, Loss: 0.375126164406538, Final Batch Loss: 0.08228173851966858\n",
      "Epoch 2751, Loss: 0.4250342547893524, Final Batch Loss: 0.10438335686922073\n",
      "Epoch 2752, Loss: 0.3559839352965355, Final Batch Loss: 0.12091968953609467\n",
      "Epoch 2753, Loss: 0.2773411124944687, Final Batch Loss: 0.09071265906095505\n",
      "Epoch 2754, Loss: 0.35375770926475525, Final Batch Loss: 0.06553297489881516\n",
      "Epoch 2755, Loss: 0.35299766063690186, Final Batch Loss: 0.1042318120598793\n",
      "Epoch 2756, Loss: 0.29627053812146187, Final Batch Loss: 0.05860970541834831\n",
      "Epoch 2757, Loss: 0.2872237600386143, Final Batch Loss: 0.04930737614631653\n",
      "Epoch 2758, Loss: 0.4046754464507103, Final Batch Loss: 0.14171098172664642\n",
      "Epoch 2759, Loss: 0.3547493815422058, Final Batch Loss: 0.09980770945549011\n",
      "Epoch 2760, Loss: 0.4336896240711212, Final Batch Loss: 0.10279467701911926\n",
      "Epoch 2761, Loss: 0.4008963331580162, Final Batch Loss: 0.10436415672302246\n",
      "Epoch 2762, Loss: 0.41136355698108673, Final Batch Loss: 0.11295408010482788\n",
      "Epoch 2763, Loss: 0.2496807724237442, Final Batch Loss: 0.03895265981554985\n",
      "Epoch 2764, Loss: 0.2955792248249054, Final Batch Loss: 0.08215745538473129\n",
      "Epoch 2765, Loss: 0.3881910927593708, Final Batch Loss: 0.11571007966995239\n",
      "Epoch 2766, Loss: 0.2964732423424721, Final Batch Loss: 0.048998359590768814\n",
      "Epoch 2767, Loss: 0.35977574437856674, Final Batch Loss: 0.07367627322673798\n",
      "Epoch 2768, Loss: 0.2124146744608879, Final Batch Loss: 0.029345761984586716\n",
      "Epoch 2769, Loss: 0.29305246099829674, Final Batch Loss: 0.13034798204898834\n",
      "Epoch 2770, Loss: 0.3933130092918873, Final Batch Loss: 0.12537652254104614\n",
      "Epoch 2771, Loss: 0.2750370465219021, Final Batch Loss: 0.040498144924640656\n",
      "Epoch 2772, Loss: 0.24216375686228275, Final Batch Loss: 0.024066714569926262\n",
      "Epoch 2773, Loss: 0.4022785872220993, Final Batch Loss: 0.12084096670150757\n",
      "Epoch 2774, Loss: 0.3894031122326851, Final Batch Loss: 0.09614833444356918\n",
      "Epoch 2775, Loss: 0.34231699258089066, Final Batch Loss: 0.05071356147527695\n",
      "Epoch 2776, Loss: 0.3762725219130516, Final Batch Loss: 0.0818982794880867\n",
      "Epoch 2777, Loss: 0.3309714086353779, Final Batch Loss: 0.06290998309850693\n",
      "Epoch 2778, Loss: 0.4420255199074745, Final Batch Loss: 0.19728656113147736\n",
      "Epoch 2779, Loss: 0.30806876346468925, Final Batch Loss: 0.043254826217889786\n",
      "Epoch 2780, Loss: 0.2618675194680691, Final Batch Loss: 0.07977224886417389\n",
      "Epoch 2781, Loss: 0.24804737605154514, Final Batch Loss: 0.016451863572001457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2782, Loss: 0.29648636281490326, Final Batch Loss: 0.09121763706207275\n",
      "Epoch 2783, Loss: 0.2831636145710945, Final Batch Loss: 0.023631520569324493\n",
      "Epoch 2784, Loss: 0.3905853182077408, Final Batch Loss: 0.05953356623649597\n",
      "Epoch 2785, Loss: 0.2360972799360752, Final Batch Loss: 0.05826292932033539\n",
      "Epoch 2786, Loss: 0.30953801795840263, Final Batch Loss: 0.07475787401199341\n",
      "Epoch 2787, Loss: 0.26080407574772835, Final Batch Loss: 0.029035214334726334\n",
      "Epoch 2788, Loss: 0.4806074686348438, Final Batch Loss: 0.2041720300912857\n",
      "Epoch 2789, Loss: 0.3032986633479595, Final Batch Loss: 0.04600224271416664\n",
      "Epoch 2790, Loss: 0.38027147948741913, Final Batch Loss: 0.03547566384077072\n",
      "Epoch 2791, Loss: 0.3599121570587158, Final Batch Loss: 0.08268793672323227\n",
      "Epoch 2792, Loss: 0.2866956926882267, Final Batch Loss: 0.07864487171173096\n",
      "Epoch 2793, Loss: 0.2315448448061943, Final Batch Loss: 0.049819931387901306\n",
      "Epoch 2794, Loss: 0.27349287271499634, Final Batch Loss: 0.06539085507392883\n",
      "Epoch 2795, Loss: 0.2603023611009121, Final Batch Loss: 0.09404073655605316\n",
      "Epoch 2796, Loss: 0.3430420830845833, Final Batch Loss: 0.07502421736717224\n",
      "Epoch 2797, Loss: 0.2940240651369095, Final Batch Loss: 0.08542925119400024\n",
      "Epoch 2798, Loss: 0.34242676943540573, Final Batch Loss: 0.06252823024988174\n",
      "Epoch 2799, Loss: 0.32344305142760277, Final Batch Loss: 0.08505883067846298\n",
      "Epoch 2800, Loss: 0.39652179181575775, Final Batch Loss: 0.1586565226316452\n",
      "Epoch 2801, Loss: 0.3064303398132324, Final Batch Loss: 0.06270033866167068\n",
      "Epoch 2802, Loss: 0.23419761657714844, Final Batch Loss: 0.03820299729704857\n",
      "Epoch 2803, Loss: 0.38707131147384644, Final Batch Loss: 0.0385400652885437\n",
      "Epoch 2804, Loss: 0.43949223309755325, Final Batch Loss: 0.17733775079250336\n",
      "Epoch 2805, Loss: 0.28042149171233177, Final Batch Loss: 0.045907553285360336\n",
      "Epoch 2806, Loss: 0.2979578971862793, Final Batch Loss: 0.0848008543252945\n",
      "Epoch 2807, Loss: 0.3956475779414177, Final Batch Loss: 0.06329195201396942\n",
      "Epoch 2808, Loss: 0.30771682411432266, Final Batch Loss: 0.10205052047967911\n",
      "Epoch 2809, Loss: 0.423524908721447, Final Batch Loss: 0.14738178253173828\n",
      "Epoch 2810, Loss: 0.33005208149552345, Final Batch Loss: 0.07588311284780502\n",
      "Epoch 2811, Loss: 0.35529497265815735, Final Batch Loss: 0.12005099654197693\n",
      "Epoch 2812, Loss: 0.2892224043607712, Final Batch Loss: 0.0357862263917923\n",
      "Epoch 2813, Loss: 0.25249722227454185, Final Batch Loss: 0.06659951061010361\n",
      "Epoch 2814, Loss: 0.40896203368902206, Final Batch Loss: 0.08369075506925583\n",
      "Epoch 2815, Loss: 0.40749847888946533, Final Batch Loss: 0.12353739887475967\n",
      "Epoch 2816, Loss: 0.35222266614437103, Final Batch Loss: 0.03656420856714249\n",
      "Epoch 2817, Loss: 0.3586817942559719, Final Batch Loss: 0.06054931506514549\n",
      "Epoch 2818, Loss: 0.3467327617108822, Final Batch Loss: 0.1305997371673584\n",
      "Epoch 2819, Loss: 0.42760705202817917, Final Batch Loss: 0.17567898333072662\n",
      "Epoch 2820, Loss: 0.2655446343123913, Final Batch Loss: 0.04891340434551239\n",
      "Epoch 2821, Loss: 0.2702927812933922, Final Batch Loss: 0.05015694350004196\n",
      "Epoch 2822, Loss: 0.3444093018770218, Final Batch Loss: 0.14660705626010895\n",
      "Epoch 2823, Loss: 0.3379368707537651, Final Batch Loss: 0.07907725125551224\n",
      "Epoch 2824, Loss: 0.31321123614907265, Final Batch Loss: 0.13291215896606445\n",
      "Epoch 2825, Loss: 0.25639483705163, Final Batch Loss: 0.03184916824102402\n",
      "Epoch 2826, Loss: 0.2460515834391117, Final Batch Loss: 0.050843495875597\n",
      "Epoch 2827, Loss: 0.3491932675242424, Final Batch Loss: 0.07612110674381256\n",
      "Epoch 2828, Loss: 0.1897864043712616, Final Batch Loss: 0.03632311895489693\n",
      "Epoch 2829, Loss: 0.3578069433569908, Final Batch Loss: 0.12125182896852493\n",
      "Epoch 2830, Loss: 0.3087453208863735, Final Batch Loss: 0.09078289568424225\n",
      "Epoch 2831, Loss: 0.2829779125750065, Final Batch Loss: 0.044013384729623795\n",
      "Epoch 2832, Loss: 0.37560228630900383, Final Batch Loss: 0.05930235609412193\n",
      "Epoch 2833, Loss: 0.30309415608644485, Final Batch Loss: 0.11817196756601334\n",
      "Epoch 2834, Loss: 0.2864695005118847, Final Batch Loss: 0.039092738181352615\n",
      "Epoch 2835, Loss: 0.3049360066652298, Final Batch Loss: 0.10011032968759537\n",
      "Epoch 2836, Loss: 0.26828499138355255, Final Batch Loss: 0.06061988323926926\n",
      "Epoch 2837, Loss: 0.279434472322464, Final Batch Loss: 0.06579343229532242\n",
      "Epoch 2838, Loss: 0.23463405342772603, Final Batch Loss: 0.0037926104851067066\n",
      "Epoch 2839, Loss: 0.2968525104224682, Final Batch Loss: 0.10891066491603851\n",
      "Epoch 2840, Loss: 0.38046870008111, Final Batch Loss: 0.0380203016102314\n",
      "Epoch 2841, Loss: 0.3206707835197449, Final Batch Loss: 0.07399453967809677\n",
      "Epoch 2842, Loss: 0.2991289794445038, Final Batch Loss: 0.05361495167016983\n",
      "Epoch 2843, Loss: 0.2893841564655304, Final Batch Loss: 0.08629937469959259\n",
      "Epoch 2844, Loss: 0.34209152311086655, Final Batch Loss: 0.050492092967033386\n",
      "Epoch 2845, Loss: 0.2327587828040123, Final Batch Loss: 0.04761257395148277\n",
      "Epoch 2846, Loss: 0.37212812900543213, Final Batch Loss: 0.07224072515964508\n",
      "Epoch 2847, Loss: 0.2774140536785126, Final Batch Loss: 0.04358956590294838\n",
      "Epoch 2848, Loss: 0.26506059616804123, Final Batch Loss: 0.05520686134696007\n",
      "Epoch 2849, Loss: 0.39053963124752045, Final Batch Loss: 0.06812739372253418\n",
      "Epoch 2850, Loss: 0.3971770331263542, Final Batch Loss: 0.13772204518318176\n",
      "Epoch 2851, Loss: 0.32165706530213356, Final Batch Loss: 0.035024192184209824\n",
      "Epoch 2852, Loss: 0.32610372453927994, Final Batch Loss: 0.047545574605464935\n",
      "Epoch 2853, Loss: 0.2527206055819988, Final Batch Loss: 0.02786640077829361\n",
      "Epoch 2854, Loss: 0.314440980553627, Final Batch Loss: 0.10019286721944809\n",
      "Epoch 2855, Loss: 0.19680778682231903, Final Batch Loss: 0.0326915979385376\n",
      "Epoch 2856, Loss: 0.31220151484012604, Final Batch Loss: 0.06883969902992249\n",
      "Epoch 2857, Loss: 0.2783667892217636, Final Batch Loss: 0.045367345213890076\n",
      "Epoch 2858, Loss: 0.3033681996166706, Final Batch Loss: 0.1285708248615265\n",
      "Epoch 2859, Loss: 0.38662515580654144, Final Batch Loss: 0.10929015278816223\n",
      "Epoch 2860, Loss: 0.304907638579607, Final Batch Loss: 0.0622207373380661\n",
      "Epoch 2861, Loss: 0.3100515305995941, Final Batch Loss: 0.06749975681304932\n",
      "Epoch 2862, Loss: 0.27270109206438065, Final Batch Loss: 0.023957550525665283\n",
      "Epoch 2863, Loss: 0.3019363656640053, Final Batch Loss: 0.07651148736476898\n",
      "Epoch 2864, Loss: 0.30631233751773834, Final Batch Loss: 0.08243763446807861\n",
      "Epoch 2865, Loss: 0.2279190868139267, Final Batch Loss: 0.05701233819127083\n",
      "Epoch 2866, Loss: 0.3834325224161148, Final Batch Loss: 0.09218872338533401\n",
      "Epoch 2867, Loss: 0.3944348804652691, Final Batch Loss: 0.13126884400844574\n",
      "Epoch 2868, Loss: 0.28731194511055946, Final Batch Loss: 0.07763008773326874\n",
      "Epoch 2869, Loss: 0.374255008995533, Final Batch Loss: 0.039710745215415955\n",
      "Epoch 2870, Loss: 0.31749704852700233, Final Batch Loss: 0.03919514641165733\n",
      "Epoch 2871, Loss: 0.3535684421658516, Final Batch Loss: 0.08682135492563248\n",
      "Epoch 2872, Loss: 0.3063869848847389, Final Batch Loss: 0.06584973633289337\n",
      "Epoch 2873, Loss: 0.2476953286677599, Final Batch Loss: 0.026417093351483345\n",
      "Epoch 2874, Loss: 0.4088914692401886, Final Batch Loss: 0.12401384115219116\n",
      "Epoch 2875, Loss: 0.2940197307616472, Final Batch Loss: 0.027599791064858437\n",
      "Epoch 2876, Loss: 0.32624630257487297, Final Batch Loss: 0.06169575825333595\n",
      "Epoch 2877, Loss: 0.2938479520380497, Final Batch Loss: 0.03927327319979668\n",
      "Epoch 2878, Loss: 0.25917765125632286, Final Batch Loss: 0.0653584897518158\n",
      "Epoch 2879, Loss: 0.3362160250544548, Final Batch Loss: 0.10942178964614868\n",
      "Epoch 2880, Loss: 0.31850916147232056, Final Batch Loss: 0.09411410242319107\n",
      "Epoch 2881, Loss: 0.23988277465105057, Final Batch Loss: 0.028249777853488922\n",
      "Epoch 2882, Loss: 0.47089457511901855, Final Batch Loss: 0.1311681717634201\n",
      "Epoch 2883, Loss: 0.3788841813802719, Final Batch Loss: 0.12839192152023315\n",
      "Epoch 2884, Loss: 0.5477303676307201, Final Batch Loss: 0.16699592769145966\n",
      "Epoch 2885, Loss: 0.3298431932926178, Final Batch Loss: 0.11620493233203888\n",
      "Epoch 2886, Loss: 0.43584151566028595, Final Batch Loss: 0.1855282187461853\n",
      "Epoch 2887, Loss: 0.2983817830681801, Final Batch Loss: 0.0741339698433876\n",
      "Epoch 2888, Loss: 0.3665619269013405, Final Batch Loss: 0.08840558677911758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2889, Loss: 0.22152891755104065, Final Batch Loss: 0.04104068875312805\n",
      "Epoch 2890, Loss: 0.263818820938468, Final Batch Loss: 0.1006452888250351\n",
      "Epoch 2891, Loss: 0.28894366323947906, Final Batch Loss: 0.04234685003757477\n",
      "Epoch 2892, Loss: 0.3126905784010887, Final Batch Loss: 0.05384538322687149\n",
      "Epoch 2893, Loss: 0.2955159768462181, Final Batch Loss: 0.07025590538978577\n",
      "Epoch 2894, Loss: 0.33727625384926796, Final Batch Loss: 0.01915695145726204\n",
      "Epoch 2895, Loss: 0.2673509418964386, Final Batch Loss: 0.06525283306837082\n",
      "Epoch 2896, Loss: 0.2952819652855396, Final Batch Loss: 0.12078659236431122\n",
      "Epoch 2897, Loss: 0.2828940898180008, Final Batch Loss: 0.12807190418243408\n",
      "Epoch 2898, Loss: 0.3696533292531967, Final Batch Loss: 0.1267436295747757\n",
      "Epoch 2899, Loss: 0.2995501421391964, Final Batch Loss: 0.058842673897743225\n",
      "Epoch 2900, Loss: 0.3093803897500038, Final Batch Loss: 0.08535211533308029\n",
      "Epoch 2901, Loss: 0.3804470933973789, Final Batch Loss: 0.0914783775806427\n",
      "Epoch 2902, Loss: 0.3443025015294552, Final Batch Loss: 0.13962751626968384\n",
      "Epoch 2903, Loss: 0.3263073042035103, Final Batch Loss: 0.06727655977010727\n",
      "Epoch 2904, Loss: 0.3522067479789257, Final Batch Loss: 0.0779435783624649\n",
      "Epoch 2905, Loss: 0.3109067976474762, Final Batch Loss: 0.06770501285791397\n",
      "Epoch 2906, Loss: 0.39258769899606705, Final Batch Loss: 0.07042529433965683\n",
      "Epoch 2907, Loss: 0.3253793716430664, Final Batch Loss: 0.04779859632253647\n",
      "Epoch 2908, Loss: 0.2075546346604824, Final Batch Loss: 0.014119360595941544\n",
      "Epoch 2909, Loss: 0.29151923581957817, Final Batch Loss: 0.08936858922243118\n",
      "Epoch 2910, Loss: 0.3124250993132591, Final Batch Loss: 0.07094954699277878\n",
      "Epoch 2911, Loss: 0.20979249104857445, Final Batch Loss: 0.029690247029066086\n",
      "Epoch 2912, Loss: 0.26367488503456116, Final Batch Loss: 0.04515620693564415\n",
      "Epoch 2913, Loss: 0.36288847774267197, Final Batch Loss: 0.09706305712461472\n",
      "Epoch 2914, Loss: 0.33523422107100487, Final Batch Loss: 0.13134635984897614\n",
      "Epoch 2915, Loss: 0.32146839797496796, Final Batch Loss: 0.07604850083589554\n",
      "Epoch 2916, Loss: 0.27775609865784645, Final Batch Loss: 0.056534294039011\n",
      "Epoch 2917, Loss: 0.3076237738132477, Final Batch Loss: 0.08485240489244461\n",
      "Epoch 2918, Loss: 0.3859335780143738, Final Batch Loss: 0.10895401984453201\n",
      "Epoch 2919, Loss: 0.34719254076480865, Final Batch Loss: 0.09760751575231552\n",
      "Epoch 2920, Loss: 0.2979149632155895, Final Batch Loss: 0.04521149769425392\n",
      "Epoch 2921, Loss: 0.43583228811621666, Final Batch Loss: 0.21261580288410187\n",
      "Epoch 2922, Loss: 0.39798653870821, Final Batch Loss: 0.1535959243774414\n",
      "Epoch 2923, Loss: 0.5112274810671806, Final Batch Loss: 0.16734273731708527\n",
      "Epoch 2924, Loss: 0.32810981571674347, Final Batch Loss: 0.09503397345542908\n",
      "Epoch 2925, Loss: 0.33312249928712845, Final Batch Loss: 0.08528604358434677\n",
      "Epoch 2926, Loss: 0.30886922776699066, Final Batch Loss: 0.07581351697444916\n",
      "Epoch 2927, Loss: 0.25390471890568733, Final Batch Loss: 0.08643894642591476\n",
      "Epoch 2928, Loss: 0.3489070422947407, Final Batch Loss: 0.061552662402391434\n",
      "Epoch 2929, Loss: 0.30881643667817116, Final Batch Loss: 0.11208532005548477\n",
      "Epoch 2930, Loss: 0.33762313425540924, Final Batch Loss: 0.05781383812427521\n",
      "Epoch 2931, Loss: 0.3079940602183342, Final Batch Loss: 0.04831945151090622\n",
      "Epoch 2932, Loss: 0.30500127375125885, Final Batch Loss: 0.046767979860305786\n",
      "Epoch 2933, Loss: 0.26900096237659454, Final Batch Loss: 0.07747393101453781\n",
      "Epoch 2934, Loss: 0.19083136320114136, Final Batch Loss: 0.030339516699314117\n",
      "Epoch 2935, Loss: 0.32754483073949814, Final Batch Loss: 0.044780999422073364\n",
      "Epoch 2936, Loss: 0.3228616788983345, Final Batch Loss: 0.10746467858552933\n",
      "Epoch 2937, Loss: 0.2803009822964668, Final Batch Loss: 0.07922081649303436\n",
      "Epoch 2938, Loss: 0.3626827038824558, Final Batch Loss: 0.07944254577159882\n",
      "Epoch 2939, Loss: 0.26314799953252077, Final Batch Loss: 0.01513258833438158\n",
      "Epoch 2940, Loss: 0.34287771582603455, Final Batch Loss: 0.10334992408752441\n",
      "Epoch 2941, Loss: 0.3474591299891472, Final Batch Loss: 0.09295810759067535\n",
      "Epoch 2942, Loss: 0.3453291058540344, Final Batch Loss: 0.08818335831165314\n",
      "Epoch 2943, Loss: 0.41741448640823364, Final Batch Loss: 0.12293503433465958\n",
      "Epoch 2944, Loss: 0.25206562876701355, Final Batch Loss: 0.0660940557718277\n",
      "Epoch 2945, Loss: 0.3905811682343483, Final Batch Loss: 0.15544955432415009\n",
      "Epoch 2946, Loss: 0.30916623026132584, Final Batch Loss: 0.08768940716981888\n",
      "Epoch 2947, Loss: 0.2940131016075611, Final Batch Loss: 0.12134093046188354\n",
      "Epoch 2948, Loss: 0.2922808900475502, Final Batch Loss: 0.039738669991493225\n",
      "Epoch 2949, Loss: 0.3559875190258026, Final Batch Loss: 0.06678982824087143\n",
      "Epoch 2950, Loss: 0.33684298396110535, Final Batch Loss: 0.1545364260673523\n",
      "Epoch 2951, Loss: 0.29195573553442955, Final Batch Loss: 0.047060657292604446\n",
      "Epoch 2952, Loss: 0.2877496965229511, Final Batch Loss: 0.044852737337350845\n",
      "Epoch 2953, Loss: 0.29785624146461487, Final Batch Loss: 0.07777724415063858\n",
      "Epoch 2954, Loss: 0.27782658860087395, Final Batch Loss: 0.07048599421977997\n",
      "Epoch 2955, Loss: 0.3175187408924103, Final Batch Loss: 0.07642693817615509\n",
      "Epoch 2956, Loss: 0.30361347272992134, Final Batch Loss: 0.033964887261390686\n",
      "Epoch 2957, Loss: 0.31828518211841583, Final Batch Loss: 0.10312390327453613\n",
      "Epoch 2958, Loss: 0.40948887914419174, Final Batch Loss: 0.07966291159391403\n",
      "Epoch 2959, Loss: 0.24688541889190674, Final Batch Loss: 0.05133483186364174\n",
      "Epoch 2960, Loss: 0.2979477196931839, Final Batch Loss: 0.02334655448794365\n",
      "Epoch 2961, Loss: 0.499313622713089, Final Batch Loss: 0.2249261885881424\n",
      "Epoch 2962, Loss: 0.3412937521934509, Final Batch Loss: 0.14727434515953064\n",
      "Epoch 2963, Loss: 0.38991712778806686, Final Batch Loss: 0.08277353644371033\n",
      "Epoch 2964, Loss: 0.17202415317296982, Final Batch Loss: 0.03826737031340599\n",
      "Epoch 2965, Loss: 0.353412427008152, Final Batch Loss: 0.12409704178571701\n",
      "Epoch 2966, Loss: 0.4219708479940891, Final Batch Loss: 0.21466666460037231\n",
      "Epoch 2967, Loss: 0.28281403332948685, Final Batch Loss: 0.07853806018829346\n",
      "Epoch 2968, Loss: 0.38854019343852997, Final Batch Loss: 0.11821445822715759\n",
      "Epoch 2969, Loss: 0.30991995707154274, Final Batch Loss: 0.06015455350279808\n",
      "Epoch 2970, Loss: 0.3189973384141922, Final Batch Loss: 0.05263379216194153\n",
      "Epoch 2971, Loss: 0.32191867753863335, Final Batch Loss: 0.1027441993355751\n",
      "Epoch 2972, Loss: 0.26185956597328186, Final Batch Loss: 0.03535008803009987\n",
      "Epoch 2973, Loss: 0.4212600812315941, Final Batch Loss: 0.07008251547813416\n",
      "Epoch 2974, Loss: 0.33635615184903145, Final Batch Loss: 0.07182729244232178\n",
      "Epoch 2975, Loss: 0.2680802009999752, Final Batch Loss: 0.06873786449432373\n",
      "Epoch 2976, Loss: 0.22117522172629833, Final Batch Loss: 0.018286289647221565\n",
      "Epoch 2977, Loss: 0.274517223238945, Final Batch Loss: 0.09077458828687668\n",
      "Epoch 2978, Loss: 0.34308425337076187, Final Batch Loss: 0.08433464914560318\n",
      "Epoch 2979, Loss: 0.3140447214245796, Final Batch Loss: 0.09102659672498703\n",
      "Epoch 2980, Loss: 0.354978546500206, Final Batch Loss: 0.08679655939340591\n",
      "Epoch 2981, Loss: 0.32163170725107193, Final Batch Loss: 0.0860406830906868\n",
      "Epoch 2982, Loss: 0.37699369341135025, Final Batch Loss: 0.07324277609586716\n",
      "Epoch 2983, Loss: 0.3520372323691845, Final Batch Loss: 0.07788469642400742\n",
      "Epoch 2984, Loss: 0.3026571646332741, Final Batch Loss: 0.05541810393333435\n",
      "Epoch 2985, Loss: 0.30991481617093086, Final Batch Loss: 0.10195125639438629\n",
      "Epoch 2986, Loss: 0.39867938309907913, Final Batch Loss: 0.2031237632036209\n",
      "Epoch 2987, Loss: 0.26982497051358223, Final Batch Loss: 0.024038586765527725\n",
      "Epoch 2988, Loss: 0.2684951610863209, Final Batch Loss: 0.07315999269485474\n",
      "Epoch 2989, Loss: 0.37489572167396545, Final Batch Loss: 0.078824482858181\n",
      "Epoch 2990, Loss: 0.3297232575714588, Final Batch Loss: 0.16861024498939514\n",
      "Epoch 2991, Loss: 0.34134289994835854, Final Batch Loss: 0.18043237924575806\n",
      "Epoch 2992, Loss: 0.30075516551733017, Final Batch Loss: 0.05880085378885269\n",
      "Epoch 2993, Loss: 0.32209570333361626, Final Batch Loss: 0.05706551671028137\n",
      "Epoch 2994, Loss: 0.2718582786619663, Final Batch Loss: 0.04044690355658531\n",
      "Epoch 2995, Loss: 0.2516818046569824, Final Batch Loss: 0.08801357448101044\n",
      "Epoch 2996, Loss: 0.2797077391296625, Final Batch Loss: 0.017005370929837227\n",
      "Epoch 2997, Loss: 0.31178727746009827, Final Batch Loss: 0.09866683185100555\n",
      "Epoch 2998, Loss: 0.283526960760355, Final Batch Loss: 0.07648022472858429\n",
      "Epoch 2999, Loss: 0.29237332940101624, Final Batch Loss: 0.08388613909482956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000, Loss: 0.2909632436931133, Final Batch Loss: 0.05504864826798439\n",
      "Epoch 3001, Loss: 0.4160740375518799, Final Batch Loss: 0.15140293538570404\n",
      "Epoch 3002, Loss: 0.3481299430131912, Final Batch Loss: 0.1469646394252777\n",
      "Epoch 3003, Loss: 0.3566148541867733, Final Batch Loss: 0.13306155800819397\n",
      "Epoch 3004, Loss: 0.3191523365676403, Final Batch Loss: 0.0822797641158104\n",
      "Epoch 3005, Loss: 0.38037843629717827, Final Batch Loss: 0.10494299232959747\n",
      "Epoch 3006, Loss: 0.3898564428091049, Final Batch Loss: 0.10747180134057999\n",
      "Epoch 3007, Loss: 0.43269938230514526, Final Batch Loss: 0.10956514626741409\n",
      "Epoch 3008, Loss: 0.3233313485980034, Final Batch Loss: 0.07285027205944061\n",
      "Epoch 3009, Loss: 0.3631247617304325, Final Batch Loss: 0.13412843644618988\n",
      "Epoch 3010, Loss: 0.3715081512928009, Final Batch Loss: 0.11287697404623032\n",
      "Epoch 3011, Loss: 0.19837068393826485, Final Batch Loss: 0.05164835974574089\n",
      "Epoch 3012, Loss: 0.24358795583248138, Final Batch Loss: 0.05955065041780472\n",
      "Epoch 3013, Loss: 0.4064459502696991, Final Batch Loss: 0.11720442771911621\n",
      "Epoch 3014, Loss: 0.4274674206972122, Final Batch Loss: 0.13397736847400665\n",
      "Epoch 3015, Loss: 0.38156165927648544, Final Batch Loss: 0.08917675167322159\n",
      "Epoch 3016, Loss: 0.32247599214315414, Final Batch Loss: 0.14067204296588898\n",
      "Epoch 3017, Loss: 0.3349238783121109, Final Batch Loss: 0.09616576880216599\n",
      "Epoch 3018, Loss: 0.25709161162376404, Final Batch Loss: 0.05945093184709549\n",
      "Epoch 3019, Loss: 0.3180641606450081, Final Batch Loss: 0.07480912655591965\n",
      "Epoch 3020, Loss: 0.2803776115179062, Final Batch Loss: 0.06815366446971893\n",
      "Epoch 3021, Loss: 0.459859287366271, Final Batch Loss: 0.22723929584026337\n",
      "Epoch 3022, Loss: 0.3389250263571739, Final Batch Loss: 0.07119831442832947\n",
      "Epoch 3023, Loss: 0.26431051827967167, Final Batch Loss: 0.027686430141329765\n",
      "Epoch 3024, Loss: 0.3553481698036194, Final Batch Loss: 0.07382921129465103\n",
      "Epoch 3025, Loss: 0.42602234333753586, Final Batch Loss: 0.11887554824352264\n",
      "Epoch 3026, Loss: 0.32483885809779167, Final Batch Loss: 0.10337519645690918\n",
      "Epoch 3027, Loss: 0.31664055585861206, Final Batch Loss: 0.07901564240455627\n",
      "Epoch 3028, Loss: 0.3330235406756401, Final Batch Loss: 0.049465201795101166\n",
      "Epoch 3029, Loss: 0.30863623693585396, Final Batch Loss: 0.13354851305484772\n",
      "Epoch 3030, Loss: 0.2740177623927593, Final Batch Loss: 0.06981931626796722\n",
      "Epoch 3031, Loss: 0.2797110825777054, Final Batch Loss: 0.07147281616926193\n",
      "Epoch 3032, Loss: 0.3027527555823326, Final Batch Loss: 0.08584398031234741\n",
      "Epoch 3033, Loss: 0.2897692434489727, Final Batch Loss: 0.051991190761327744\n",
      "Epoch 3034, Loss: 0.22587131336331367, Final Batch Loss: 0.0392029695212841\n",
      "Epoch 3035, Loss: 0.2885090373456478, Final Batch Loss: 0.06003738194704056\n",
      "Epoch 3036, Loss: 0.19675451889634132, Final Batch Loss: 0.049033500254154205\n",
      "Epoch 3037, Loss: 0.2701004110276699, Final Batch Loss: 0.09003250300884247\n",
      "Epoch 3038, Loss: 0.34928562864661217, Final Batch Loss: 0.05256957188248634\n",
      "Epoch 3039, Loss: 0.3506612256169319, Final Batch Loss: 0.09845372289419174\n",
      "Epoch 3040, Loss: 0.248503215610981, Final Batch Loss: 0.0650264173746109\n",
      "Epoch 3041, Loss: 0.40957292914390564, Final Batch Loss: 0.14301882684230804\n",
      "Epoch 3042, Loss: 0.28389373421669006, Final Batch Loss: 0.06957028806209564\n",
      "Epoch 3043, Loss: 0.3191063515841961, Final Batch Loss: 0.05178556218743324\n",
      "Epoch 3044, Loss: 0.3135428912937641, Final Batch Loss: 0.05104764923453331\n",
      "Epoch 3045, Loss: 0.35161054506897926, Final Batch Loss: 0.13723982870578766\n",
      "Epoch 3046, Loss: 0.3694686144590378, Final Batch Loss: 0.036460429430007935\n",
      "Epoch 3047, Loss: 0.27194784209132195, Final Batch Loss: 0.03677265718579292\n",
      "Epoch 3048, Loss: 0.239974744617939, Final Batch Loss: 0.023710720241069794\n",
      "Epoch 3049, Loss: 0.2686113156378269, Final Batch Loss: 0.02816677838563919\n",
      "Epoch 3050, Loss: 0.3434699885547161, Final Batch Loss: 0.1247905045747757\n",
      "Epoch 3051, Loss: 0.3216523975133896, Final Batch Loss: 0.0817144513130188\n",
      "Epoch 3052, Loss: 0.31976768374443054, Final Batch Loss: 0.09317344427108765\n",
      "Epoch 3053, Loss: 0.36924440413713455, Final Batch Loss: 0.15334565937519073\n",
      "Epoch 3054, Loss: 0.4582812748849392, Final Batch Loss: 0.19519208371639252\n",
      "Epoch 3055, Loss: 0.3376198709011078, Final Batch Loss: 0.10049066692590714\n",
      "Epoch 3056, Loss: 0.25510287657380104, Final Batch Loss: 0.06472963839769363\n",
      "Epoch 3057, Loss: 0.389626182615757, Final Batch Loss: 0.05959579348564148\n",
      "Epoch 3058, Loss: 0.29763489589095116, Final Batch Loss: 0.05644969642162323\n",
      "Epoch 3059, Loss: 0.31030987203121185, Final Batch Loss: 0.06398258358240128\n",
      "Epoch 3060, Loss: 0.30397574231028557, Final Batch Loss: 0.059008266776800156\n",
      "Epoch 3061, Loss: 0.33109544962644577, Final Batch Loss: 0.0783064141869545\n",
      "Epoch 3062, Loss: 0.3061698190867901, Final Batch Loss: 0.0254114530980587\n",
      "Epoch 3063, Loss: 0.22498943656682968, Final Batch Loss: 0.061298418790102005\n",
      "Epoch 3064, Loss: 0.4350441247224808, Final Batch Loss: 0.15534985065460205\n",
      "Epoch 3065, Loss: 0.3675185777246952, Final Batch Loss: 0.09372518211603165\n",
      "Epoch 3066, Loss: 0.22385862842202187, Final Batch Loss: 0.05756792053580284\n",
      "Epoch 3067, Loss: 0.31116263568401337, Final Batch Loss: 0.08630813658237457\n",
      "Epoch 3068, Loss: 0.24991748854517937, Final Batch Loss: 0.07361525297164917\n",
      "Epoch 3069, Loss: 0.38012032583355904, Final Batch Loss: 0.1624094843864441\n",
      "Epoch 3070, Loss: 0.20893275178968906, Final Batch Loss: 0.03283454477787018\n",
      "Epoch 3071, Loss: 0.3427140638232231, Final Batch Loss: 0.06321888417005539\n",
      "Epoch 3072, Loss: 0.26519162580370903, Final Batch Loss: 0.03329509496688843\n",
      "Epoch 3073, Loss: 0.2797615770250559, Final Batch Loss: 0.030396880581974983\n",
      "Epoch 3074, Loss: 0.18034150078892708, Final Batch Loss: 0.014848748221993446\n",
      "Epoch 3075, Loss: 0.2935333549976349, Final Batch Loss: 0.03808693587779999\n",
      "Epoch 3076, Loss: 0.4228314086794853, Final Batch Loss: 0.1364283412694931\n",
      "Epoch 3077, Loss: 0.36293231323361397, Final Batch Loss: 0.06358551979064941\n",
      "Epoch 3078, Loss: 0.39273129403591156, Final Batch Loss: 0.07853385806083679\n",
      "Epoch 3079, Loss: 0.33503324538469315, Final Batch Loss: 0.06375642120838165\n",
      "Epoch 3080, Loss: 0.48031262308359146, Final Batch Loss: 0.2223109006881714\n",
      "Epoch 3081, Loss: 0.3582815118134022, Final Batch Loss: 0.04897468164563179\n",
      "Epoch 3082, Loss: 0.21066797338426113, Final Batch Loss: 0.027085373178124428\n",
      "Epoch 3083, Loss: 0.28740664198994637, Final Batch Loss: 0.05861395597457886\n",
      "Epoch 3084, Loss: 0.3207676336169243, Final Batch Loss: 0.03164619952440262\n",
      "Epoch 3085, Loss: 0.3068636953830719, Final Batch Loss: 0.05453050136566162\n",
      "Epoch 3086, Loss: 0.30362556129693985, Final Batch Loss: 0.07307896763086319\n",
      "Epoch 3087, Loss: 0.36589865759015083, Final Batch Loss: 0.04708675667643547\n",
      "Epoch 3088, Loss: 0.31271762028336525, Final Batch Loss: 0.14345747232437134\n",
      "Epoch 3089, Loss: 0.2868146561086178, Final Batch Loss: 0.0735553652048111\n",
      "Epoch 3090, Loss: 0.29242922738194466, Final Batch Loss: 0.0899524986743927\n",
      "Epoch 3091, Loss: 0.2815776988863945, Final Batch Loss: 0.0373561792075634\n",
      "Epoch 3092, Loss: 0.37385233119130135, Final Batch Loss: 0.16979442536830902\n",
      "Epoch 3093, Loss: 0.23790530860424042, Final Batch Loss: 0.0673273429274559\n",
      "Epoch 3094, Loss: 0.2723742797970772, Final Batch Loss: 0.05258042365312576\n",
      "Epoch 3095, Loss: 0.23023485764861107, Final Batch Loss: 0.06965519487857819\n",
      "Epoch 3096, Loss: 0.3409130349755287, Final Batch Loss: 0.09040574729442596\n",
      "Epoch 3097, Loss: 0.4370107427239418, Final Batch Loss: 0.10520794987678528\n",
      "Epoch 3098, Loss: 0.4617914408445358, Final Batch Loss: 0.14512291550636292\n",
      "Epoch 3099, Loss: 0.40209928900003433, Final Batch Loss: 0.11977178603410721\n",
      "Epoch 3100, Loss: 0.36794576793909073, Final Batch Loss: 0.04760377109050751\n",
      "Epoch 3101, Loss: 0.4248078390955925, Final Batch Loss: 0.13648417592048645\n",
      "Epoch 3102, Loss: 0.31480006501078606, Final Batch Loss: 0.033685352653265\n",
      "Epoch 3103, Loss: 0.47549059987068176, Final Batch Loss: 0.24774309992790222\n",
      "Epoch 3104, Loss: 0.3623218908905983, Final Batch Loss: 0.06981857866048813\n",
      "Epoch 3105, Loss: 0.2836745046079159, Final Batch Loss: 0.052565403282642365\n",
      "Epoch 3106, Loss: 0.2652061767876148, Final Batch Loss: 0.04743819311261177\n",
      "Epoch 3107, Loss: 0.27705227956175804, Final Batch Loss: 0.04104790464043617\n",
      "Epoch 3108, Loss: 0.3273649923503399, Final Batch Loss: 0.1004929170012474\n",
      "Epoch 3109, Loss: 0.2782936915755272, Final Batch Loss: 0.05593554675579071\n",
      "Epoch 3110, Loss: 0.2573925219476223, Final Batch Loss: 0.03328990563750267\n",
      "Epoch 3111, Loss: 0.23219920694828033, Final Batch Loss: 0.04163730889558792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3112, Loss: 0.2969602420926094, Final Batch Loss: 0.03760477900505066\n",
      "Epoch 3113, Loss: 0.2698837220668793, Final Batch Loss: 0.10848771780729294\n",
      "Epoch 3114, Loss: 0.2781447395682335, Final Batch Loss: 0.07432370632886887\n",
      "Epoch 3115, Loss: 0.2861183173954487, Final Batch Loss: 0.08966998010873795\n",
      "Epoch 3116, Loss: 0.29949451610445976, Final Batch Loss: 0.02761688083410263\n",
      "Epoch 3117, Loss: 0.2402656553313136, Final Batch Loss: 0.012400797568261623\n",
      "Epoch 3118, Loss: 0.2667317856103182, Final Batch Loss: 0.023747006431221962\n",
      "Epoch 3119, Loss: 0.3556697852909565, Final Batch Loss: 0.044288355857133865\n",
      "Epoch 3120, Loss: 0.28913893923163414, Final Batch Loss: 0.04246273636817932\n",
      "Epoch 3121, Loss: 0.2842583619058132, Final Batch Loss: 0.07571643590927124\n",
      "Epoch 3122, Loss: 0.22243319638073444, Final Batch Loss: 0.025567060336470604\n",
      "Epoch 3123, Loss: 0.396077960729599, Final Batch Loss: 0.09458418190479279\n",
      "Epoch 3124, Loss: 0.2264021709561348, Final Batch Loss: 0.0335383266210556\n",
      "Epoch 3125, Loss: 0.33976122736930847, Final Batch Loss: 0.10625498741865158\n",
      "Epoch 3126, Loss: 0.3305784910917282, Final Batch Loss: 0.09310110658407211\n",
      "Epoch 3127, Loss: 0.2154538407921791, Final Batch Loss: 0.04284504055976868\n",
      "Epoch 3128, Loss: 0.27046928741037846, Final Batch Loss: 0.09157899767160416\n",
      "Epoch 3129, Loss: 0.3880377858877182, Final Batch Loss: 0.11536598205566406\n",
      "Epoch 3130, Loss: 0.3642907962203026, Final Batch Loss: 0.0737500786781311\n",
      "Epoch 3131, Loss: 0.2133314087986946, Final Batch Loss: 0.03149070218205452\n",
      "Epoch 3132, Loss: 0.35084084048867226, Final Batch Loss: 0.035909559577703476\n",
      "Epoch 3133, Loss: 0.34449366480112076, Final Batch Loss: 0.1223769262433052\n",
      "Epoch 3134, Loss: 0.2838538810610771, Final Batch Loss: 0.09054697304964066\n",
      "Epoch 3135, Loss: 0.36207451671361923, Final Batch Loss: 0.07301449775695801\n",
      "Epoch 3136, Loss: 0.289663840085268, Final Batch Loss: 0.06123162433505058\n",
      "Epoch 3137, Loss: 0.40656837075948715, Final Batch Loss: 0.1407691389322281\n",
      "Epoch 3138, Loss: 0.2952562943100929, Final Batch Loss: 0.07416781038045883\n",
      "Epoch 3139, Loss: 0.3885314166545868, Final Batch Loss: 0.13117218017578125\n",
      "Epoch 3140, Loss: 0.38111406937241554, Final Batch Loss: 0.07379746437072754\n",
      "Epoch 3141, Loss: 0.3292361907660961, Final Batch Loss: 0.0980958640575409\n",
      "Epoch 3142, Loss: 0.4006796143949032, Final Batch Loss: 0.10188280045986176\n",
      "Epoch 3143, Loss: 0.34259912371635437, Final Batch Loss: 0.08475956320762634\n",
      "Epoch 3144, Loss: 0.36252065747976303, Final Batch Loss: 0.0658164918422699\n",
      "Epoch 3145, Loss: 0.3357938975095749, Final Batch Loss: 0.07663929462432861\n",
      "Epoch 3146, Loss: 0.40172741562128067, Final Batch Loss: 0.10297512263059616\n",
      "Epoch 3147, Loss: 0.23132890835404396, Final Batch Loss: 0.05710673704743385\n",
      "Epoch 3148, Loss: 0.2922445014119148, Final Batch Loss: 0.03550095856189728\n",
      "Epoch 3149, Loss: 0.3297887071967125, Final Batch Loss: 0.05928083509206772\n",
      "Epoch 3150, Loss: 0.3041013516485691, Final Batch Loss: 0.11634141951799393\n",
      "Epoch 3151, Loss: 0.25629910081624985, Final Batch Loss: 0.039603449404239655\n",
      "Epoch 3152, Loss: 0.20224307104945183, Final Batch Loss: 0.04221528023481369\n",
      "Epoch 3153, Loss: 0.27332964539527893, Final Batch Loss: 0.06835374981164932\n",
      "Epoch 3154, Loss: 0.34447426348924637, Final Batch Loss: 0.0698094442486763\n",
      "Epoch 3155, Loss: 0.3427511937916279, Final Batch Loss: 0.14051908254623413\n",
      "Epoch 3156, Loss: 0.37197093665599823, Final Batch Loss: 0.08283692598342896\n",
      "Epoch 3157, Loss: 0.3374907560646534, Final Batch Loss: 0.052042458206415176\n",
      "Epoch 3158, Loss: 0.42722543329000473, Final Batch Loss: 0.1583457887172699\n",
      "Epoch 3159, Loss: 0.26453281939029694, Final Batch Loss: 0.06798194348812103\n",
      "Epoch 3160, Loss: 0.3324982300400734, Final Batch Loss: 0.060487616807222366\n",
      "Epoch 3161, Loss: 0.4015471152961254, Final Batch Loss: 0.036125410348176956\n",
      "Epoch 3162, Loss: 0.497507780790329, Final Batch Loss: 0.1470872163772583\n",
      "Epoch 3163, Loss: 0.24104739353060722, Final Batch Loss: 0.03778659179806709\n",
      "Epoch 3164, Loss: 0.25373340025544167, Final Batch Loss: 0.06486979126930237\n",
      "Epoch 3165, Loss: 0.3046082127839327, Final Batch Loss: 0.021179893985390663\n",
      "Epoch 3166, Loss: 0.3127789869904518, Final Batch Loss: 0.0906844511628151\n",
      "Epoch 3167, Loss: 0.3782406114041805, Final Batch Loss: 0.15327204763889313\n",
      "Epoch 3168, Loss: 0.3111471123993397, Final Batch Loss: 0.1422339379787445\n",
      "Epoch 3169, Loss: 0.2882350943982601, Final Batch Loss: 0.11031289398670197\n",
      "Epoch 3170, Loss: 0.4113340303301811, Final Batch Loss: 0.16253605484962463\n",
      "Epoch 3171, Loss: 0.3138868659734726, Final Batch Loss: 0.040453650057315826\n",
      "Epoch 3172, Loss: 0.44626518711447716, Final Batch Loss: 0.17963846027851105\n",
      "Epoch 3173, Loss: 0.27457780949771404, Final Batch Loss: 0.023919524624943733\n",
      "Epoch 3174, Loss: 0.31810935214161873, Final Batch Loss: 0.04811699688434601\n",
      "Epoch 3175, Loss: 0.313548868522048, Final Batch Loss: 0.13836006820201874\n",
      "Epoch 3176, Loss: 0.2540081571787596, Final Batch Loss: 0.016447173431515694\n",
      "Epoch 3177, Loss: 0.2905941791832447, Final Batch Loss: 0.05658667907118797\n",
      "Epoch 3178, Loss: 0.3859054520726204, Final Batch Loss: 0.12186849117279053\n",
      "Epoch 3179, Loss: 0.2682988680899143, Final Batch Loss: 0.033888883888721466\n",
      "Epoch 3180, Loss: 0.394141748547554, Final Batch Loss: 0.1151650920510292\n",
      "Epoch 3181, Loss: 0.2911479137837887, Final Batch Loss: 0.05202057585120201\n",
      "Epoch 3182, Loss: 0.4199230894446373, Final Batch Loss: 0.09052372723817825\n",
      "Epoch 3183, Loss: 0.27296607941389084, Final Batch Loss: 0.03995645046234131\n",
      "Epoch 3184, Loss: 0.29941175878047943, Final Batch Loss: 0.07344966381788254\n",
      "Epoch 3185, Loss: 0.21637307107448578, Final Batch Loss: 0.0455036461353302\n",
      "Epoch 3186, Loss: 0.2501576989889145, Final Batch Loss: 0.05890166014432907\n",
      "Epoch 3187, Loss: 0.3542604837566614, Final Batch Loss: 0.1032140776515007\n",
      "Epoch 3188, Loss: 0.24172667786478996, Final Batch Loss: 0.04545166715979576\n",
      "Epoch 3189, Loss: 0.3124134689569473, Final Batch Loss: 0.054985806345939636\n",
      "Epoch 3190, Loss: 0.2848869748413563, Final Batch Loss: 0.0456736758351326\n",
      "Epoch 3191, Loss: 0.22003259509801865, Final Batch Loss: 0.07111038267612457"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
