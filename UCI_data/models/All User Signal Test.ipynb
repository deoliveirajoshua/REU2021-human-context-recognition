{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 tBodyAcc-mean()-X</th>\n",
       "      <th>2 tBodyAcc-mean()-Y</th>\n",
       "      <th>3 tBodyAcc-mean()-Z</th>\n",
       "      <th>4 tBodyAcc-std()-X</th>\n",
       "      <th>5 tBodyAcc-std()-Y</th>\n",
       "      <th>6 tBodyAcc-std()-Z</th>\n",
       "      <th>7 tBodyAcc-mad()-X</th>\n",
       "      <th>8 tBodyAcc-mad()-Y</th>\n",
       "      <th>9 tBodyAcc-mad()-Z</th>\n",
       "      <th>10 tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>33 tBodyAcc-arCoeff()-Y,4</th>\n",
       "      <th>34 tBodyAcc-arCoeff()-Z,1</th>\n",
       "      <th>35 tBodyAcc-arCoeff()-Z,2</th>\n",
       "      <th>36 tBodyAcc-arCoeff()-Z,3</th>\n",
       "      <th>37 tBodyAcc-arCoeff()-Z,4</th>\n",
       "      <th>38 tBodyAcc-correlation()-X,Y</th>\n",
       "      <th>39 tBodyAcc-correlation()-X,Z</th>\n",
       "      <th>40 tBodyAcc-correlation()-Y,Z</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095246</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>-0.465085</td>\n",
       "      <td>0.491936</td>\n",
       "      <td>-0.190884</td>\n",
       "      <td>0.376314</td>\n",
       "      <td>0.435129</td>\n",
       "      <td>0.660790</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281211</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>-0.022153</td>\n",
       "      <td>-0.016657</td>\n",
       "      <td>-0.220643</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>-0.072692</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332564</td>\n",
       "      <td>0.239281</td>\n",
       "      <td>-0.136204</td>\n",
       "      <td>0.173863</td>\n",
       "      <td>-0.299493</td>\n",
       "      <td>-0.124698</td>\n",
       "      <td>-0.181105</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170813</td>\n",
       "      <td>0.294938</td>\n",
       "      <td>-0.306081</td>\n",
       "      <td>0.482148</td>\n",
       "      <td>-0.470129</td>\n",
       "      <td>-0.305693</td>\n",
       "      <td>-0.362654</td>\n",
       "      <td>0.507459</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>-0.269069</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>-0.088952</td>\n",
       "      <td>-0.155804</td>\n",
       "      <td>-0.189763</td>\n",
       "      <td>0.599213</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
       "0             0.288585            -0.020294            -0.132905   \n",
       "1             0.278419            -0.016411            -0.123520   \n",
       "2             0.279653            -0.019467            -0.113462   \n",
       "3             0.279174            -0.026201            -0.123283   \n",
       "4             0.276629            -0.016570            -0.115362   \n",
       "\n",
       "   4 tBodyAcc-std()-X  5 tBodyAcc-std()-Y  6 tBodyAcc-std()-Z  \\\n",
       "0           -0.995279           -0.983111           -0.913526   \n",
       "1           -0.998245           -0.975300           -0.960322   \n",
       "2           -0.995380           -0.967187           -0.978944   \n",
       "3           -0.996091           -0.983403           -0.990675   \n",
       "4           -0.998139           -0.980817           -0.990482   \n",
       "\n",
       "   7 tBodyAcc-mad()-X  8 tBodyAcc-mad()-Y  9 tBodyAcc-mad()-Z  \\\n",
       "0           -0.995112           -0.983185           -0.923527   \n",
       "1           -0.998807           -0.974914           -0.957686   \n",
       "2           -0.996520           -0.963668           -0.977469   \n",
       "3           -0.997099           -0.982750           -0.989302   \n",
       "4           -0.998321           -0.979672           -0.990441   \n",
       "\n",
       "   10 tBodyAcc-max()-X  ...  33 tBodyAcc-arCoeff()-Y,4  \\\n",
       "0            -0.934724  ...                  -0.095246   \n",
       "1            -0.943068  ...                  -0.281211   \n",
       "2            -0.938692  ...                  -0.332564   \n",
       "3            -0.938692  ...                  -0.170813   \n",
       "4            -0.942469  ...                  -0.315375   \n",
       "\n",
       "   34 tBodyAcc-arCoeff()-Z,1  35 tBodyAcc-arCoeff()-Z,2  \\\n",
       "0                   0.278851                  -0.465085   \n",
       "1                   0.085988                  -0.022153   \n",
       "2                   0.239281                  -0.136204   \n",
       "3                   0.294938                  -0.306081   \n",
       "4                   0.439744                  -0.269069   \n",
       "\n",
       "   36 tBodyAcc-arCoeff()-Z,3  37 tBodyAcc-arCoeff()-Z,4  \\\n",
       "0                   0.491936                  -0.190884   \n",
       "1                  -0.016657                  -0.220643   \n",
       "2                   0.173863                  -0.299493   \n",
       "3                   0.482148                  -0.470129   \n",
       "4                   0.179414                  -0.088952   \n",
       "\n",
       "   38 tBodyAcc-correlation()-X,Y  39 tBodyAcc-correlation()-X,Z  \\\n",
       "0                       0.376314                       0.435129   \n",
       "1                      -0.013429                      -0.072692   \n",
       "2                      -0.124698                      -0.181105   \n",
       "3                      -0.305693                      -0.362654   \n",
       "4                      -0.155804                      -0.189763   \n",
       "\n",
       "   40 tBodyAcc-correlation()-Y,Z  Activity  Subject  \n",
       "0                       0.660790         5        1  \n",
       "1                       0.579382         5        1  \n",
       "2                       0.608900         5        1  \n",
       "3                       0.507459         5        1  \n",
       "4                       0.599213         5        1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dataframe = pd.read_csv('../data/features.txt', delimiter = '\\n', header = None)\n",
    "names = name_dataframe.values.tolist()\n",
    "names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "data = pd.read_csv('../data/X_train.txt', delim_whitespace = True, header = None) #Read in train dataframe\n",
    "data.columns = names #Setting column names\n",
    "\n",
    "X_train = data.loc[:,'1 tBodyAcc-mean()-X':'40 tBodyAcc-correlation()-Y,Z'] #Selecting only acceleration columns\n",
    "\n",
    "y_train_activity = pd.read_csv('../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "y_train_subject = pd.read_csv('../data/subject_train.txt', header = None)\n",
    "y_train_subject.columns = ['Subject']\n",
    "\n",
    "GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "GAN_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 tBodyAcc-mean()-X</th>\n",
       "      <th>2 tBodyAcc-mean()-Y</th>\n",
       "      <th>3 tBodyAcc-mean()-Z</th>\n",
       "      <th>4 tBodyAcc-std()-X</th>\n",
       "      <th>5 tBodyAcc-std()-Y</th>\n",
       "      <th>6 tBodyAcc-std()-Z</th>\n",
       "      <th>7 tBodyAcc-mad()-X</th>\n",
       "      <th>8 tBodyAcc-mad()-Y</th>\n",
       "      <th>9 tBodyAcc-mad()-Z</th>\n",
       "      <th>10 tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>33 tBodyAcc-arCoeff()-Y,4</th>\n",
       "      <th>34 tBodyAcc-arCoeff()-Z,1</th>\n",
       "      <th>35 tBodyAcc-arCoeff()-Z,2</th>\n",
       "      <th>36 tBodyAcc-arCoeff()-Z,3</th>\n",
       "      <th>37 tBodyAcc-arCoeff()-Z,4</th>\n",
       "      <th>38 tBodyAcc-correlation()-X,Y</th>\n",
       "      <th>39 tBodyAcc-correlation()-X,Z</th>\n",
       "      <th>40 tBodyAcc-correlation()-Y,Z</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.325886</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.016658</td>\n",
       "      <td>-0.982403</td>\n",
       "      <td>-0.946708</td>\n",
       "      <td>-0.736155</td>\n",
       "      <td>-0.983673</td>\n",
       "      <td>-0.945060</td>\n",
       "      <td>-0.727475</td>\n",
       "      <td>-0.916762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240399</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>-0.241857</td>\n",
       "      <td>0.085815</td>\n",
       "      <td>0.295091</td>\n",
       "      <td>0.449622</td>\n",
       "      <td>0.474105</td>\n",
       "      <td>0.934028</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.277510</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>-0.109895</td>\n",
       "      <td>-0.997329</td>\n",
       "      <td>-0.994007</td>\n",
       "      <td>-0.997461</td>\n",
       "      <td>-0.997473</td>\n",
       "      <td>-0.992929</td>\n",
       "      <td>-0.997822</td>\n",
       "      <td>-0.940043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087491</td>\n",
       "      <td>0.470310</td>\n",
       "      <td>-0.093276</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.148952</td>\n",
       "      <td>-0.255320</td>\n",
       "      <td>-0.064980</td>\n",
       "      <td>-0.035453</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.273027</td>\n",
       "      <td>-0.010197</td>\n",
       "      <td>-0.111153</td>\n",
       "      <td>-0.993125</td>\n",
       "      <td>-0.990001</td>\n",
       "      <td>-0.995408</td>\n",
       "      <td>-0.993589</td>\n",
       "      <td>-0.989888</td>\n",
       "      <td>-0.994165</td>\n",
       "      <td>-0.936510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139590</td>\n",
       "      <td>0.561777</td>\n",
       "      <td>-0.243836</td>\n",
       "      <td>0.318708</td>\n",
       "      <td>-0.114301</td>\n",
       "      <td>0.016113</td>\n",
       "      <td>-0.051272</td>\n",
       "      <td>-0.162163</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.275676</td>\n",
       "      <td>-0.021264</td>\n",
       "      <td>-0.110801</td>\n",
       "      <td>-0.997862</td>\n",
       "      <td>-0.990091</td>\n",
       "      <td>-0.994593</td>\n",
       "      <td>-0.998333</td>\n",
       "      <td>-0.989473</td>\n",
       "      <td>-0.994485</td>\n",
       "      <td>-0.944567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045064</td>\n",
       "      <td>0.490784</td>\n",
       "      <td>-0.204408</td>\n",
       "      <td>0.227647</td>\n",
       "      <td>-0.196758</td>\n",
       "      <td>-0.097759</td>\n",
       "      <td>0.084406</td>\n",
       "      <td>0.107562</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303689</td>\n",
       "      <td>-0.004243</td>\n",
       "      <td>-0.150850</td>\n",
       "      <td>-0.956503</td>\n",
       "      <td>-0.838672</td>\n",
       "      <td>-0.943010</td>\n",
       "      <td>-0.962006</td>\n",
       "      <td>-0.844821</td>\n",
       "      <td>-0.937195</td>\n",
       "      <td>-0.916064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115909</td>\n",
       "      <td>-0.046818</td>\n",
       "      <td>-0.036225</td>\n",
       "      <td>-0.032126</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>-0.910582</td>\n",
       "      <td>-0.706014</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.279998</td>\n",
       "      <td>-0.008851</td>\n",
       "      <td>-0.103882</td>\n",
       "      <td>-0.996803</td>\n",
       "      <td>-0.976461</td>\n",
       "      <td>-0.981593</td>\n",
       "      <td>-0.997044</td>\n",
       "      <td>-0.976664</td>\n",
       "      <td>-0.980501</td>\n",
       "      <td>-0.941751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.211139</td>\n",
       "      <td>-0.052256</td>\n",
       "      <td>-0.103982</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>0.122814</td>\n",
       "      <td>-0.125760</td>\n",
       "      <td>0.476450</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.270066</td>\n",
       "      <td>-0.050618</td>\n",
       "      <td>-0.092672</td>\n",
       "      <td>-0.989959</td>\n",
       "      <td>-0.854669</td>\n",
       "      <td>-0.969782</td>\n",
       "      <td>-0.991565</td>\n",
       "      <td>-0.841260</td>\n",
       "      <td>-0.969198</td>\n",
       "      <td>-0.931457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141419</td>\n",
       "      <td>0.073816</td>\n",
       "      <td>-0.172261</td>\n",
       "      <td>0.255886</td>\n",
       "      <td>-0.030827</td>\n",
       "      <td>0.497752</td>\n",
       "      <td>-0.443817</td>\n",
       "      <td>-0.474042</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.272359</td>\n",
       "      <td>-0.001138</td>\n",
       "      <td>-0.111587</td>\n",
       "      <td>-0.988056</td>\n",
       "      <td>-0.831742</td>\n",
       "      <td>-0.932375</td>\n",
       "      <td>-0.990062</td>\n",
       "      <td>-0.853840</td>\n",
       "      <td>-0.939390</td>\n",
       "      <td>-0.924010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542931</td>\n",
       "      <td>-0.046517</td>\n",
       "      <td>-0.033900</td>\n",
       "      <td>0.234856</td>\n",
       "      <td>-0.231697</td>\n",
       "      <td>-0.412537</td>\n",
       "      <td>0.053695</td>\n",
       "      <td>-0.046689</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.325338</td>\n",
       "      <td>-0.026044</td>\n",
       "      <td>-0.122840</td>\n",
       "      <td>-0.967250</td>\n",
       "      <td>-0.928780</td>\n",
       "      <td>-0.957976</td>\n",
       "      <td>-0.969361</td>\n",
       "      <td>-0.937497</td>\n",
       "      <td>-0.954042</td>\n",
       "      <td>-0.905316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052980</td>\n",
       "      <td>0.105778</td>\n",
       "      <td>-0.062396</td>\n",
       "      <td>-0.000555</td>\n",
       "      <td>-0.163072</td>\n",
       "      <td>-0.660981</td>\n",
       "      <td>-0.198059</td>\n",
       "      <td>-0.551580</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.275396</td>\n",
       "      <td>-0.014879</td>\n",
       "      <td>-0.102163</td>\n",
       "      <td>-0.994710</td>\n",
       "      <td>-0.983213</td>\n",
       "      <td>-0.978956</td>\n",
       "      <td>-0.994796</td>\n",
       "      <td>-0.983103</td>\n",
       "      <td>-0.979648</td>\n",
       "      <td>-0.941938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036970</td>\n",
       "      <td>-0.027282</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>-0.082829</td>\n",
       "      <td>0.107020</td>\n",
       "      <td>-0.013751</td>\n",
       "      <td>-0.187977</td>\n",
       "      <td>0.393204</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
       "0               0.325886             0.000817             0.016658   \n",
       "1               0.277510            -0.016334            -0.109895   \n",
       "2               0.273027            -0.010197            -0.111153   \n",
       "3               0.275676            -0.021264            -0.110801   \n",
       "4               0.303689            -0.004243            -0.150850   \n",
       "..                   ...                  ...                  ...   \n",
       "213             0.279998            -0.008851            -0.103882   \n",
       "214             0.270066            -0.050618            -0.092672   \n",
       "215             0.272359            -0.001138            -0.111587   \n",
       "216             0.325338            -0.026044            -0.122840   \n",
       "217             0.275396            -0.014879            -0.102163   \n",
       "\n",
       "     4 tBodyAcc-std()-X  5 tBodyAcc-std()-Y  6 tBodyAcc-std()-Z  \\\n",
       "0             -0.982403           -0.946708           -0.736155   \n",
       "1             -0.997329           -0.994007           -0.997461   \n",
       "2             -0.993125           -0.990001           -0.995408   \n",
       "3             -0.997862           -0.990091           -0.994593   \n",
       "4             -0.956503           -0.838672           -0.943010   \n",
       "..                  ...                 ...                 ...   \n",
       "213           -0.996803           -0.976461           -0.981593   \n",
       "214           -0.989959           -0.854669           -0.969782   \n",
       "215           -0.988056           -0.831742           -0.932375   \n",
       "216           -0.967250           -0.928780           -0.957976   \n",
       "217           -0.994710           -0.983213           -0.978956   \n",
       "\n",
       "     7 tBodyAcc-mad()-X  8 tBodyAcc-mad()-Y  9 tBodyAcc-mad()-Z  \\\n",
       "0             -0.983673           -0.945060           -0.727475   \n",
       "1             -0.997473           -0.992929           -0.997822   \n",
       "2             -0.993589           -0.989888           -0.994165   \n",
       "3             -0.998333           -0.989473           -0.994485   \n",
       "4             -0.962006           -0.844821           -0.937195   \n",
       "..                  ...                 ...                 ...   \n",
       "213           -0.997044           -0.976664           -0.980501   \n",
       "214           -0.991565           -0.841260           -0.969198   \n",
       "215           -0.990062           -0.853840           -0.939390   \n",
       "216           -0.969361           -0.937497           -0.954042   \n",
       "217           -0.994796           -0.983103           -0.979648   \n",
       "\n",
       "     10 tBodyAcc-max()-X  ...  33 tBodyAcc-arCoeff()-Y,4  \\\n",
       "0              -0.916762  ...                  -0.240399   \n",
       "1              -0.940043  ...                  -0.087491   \n",
       "2              -0.936510  ...                  -0.139590   \n",
       "3              -0.944567  ...                   0.045064   \n",
       "4              -0.916064  ...                  -0.115909   \n",
       "..                   ...  ...                        ...   \n",
       "213            -0.941751  ...                   0.015619   \n",
       "214            -0.931457  ...                  -0.141419   \n",
       "215            -0.924010  ...                   0.542931   \n",
       "216            -0.905316  ...                  -0.052980   \n",
       "217            -0.941938  ...                  -0.036970   \n",
       "\n",
       "     34 tBodyAcc-arCoeff()-Z,1  35 tBodyAcc-arCoeff()-Z,2  \\\n",
       "0                    -0.000734                  -0.241857   \n",
       "1                     0.470310                  -0.093276   \n",
       "2                     0.561777                  -0.243836   \n",
       "3                     0.490784                  -0.204408   \n",
       "4                    -0.046818                  -0.036225   \n",
       "..                         ...                        ...   \n",
       "213                   0.211139                  -0.052256   \n",
       "214                   0.073816                  -0.172261   \n",
       "215                  -0.046517                  -0.033900   \n",
       "216                   0.105778                  -0.062396   \n",
       "217                  -0.027282                   0.060474   \n",
       "\n",
       "     36 tBodyAcc-arCoeff()-Z,3  37 tBodyAcc-arCoeff()-Z,4  \\\n",
       "0                     0.085815                   0.295091   \n",
       "1                    -0.000678                   0.148952   \n",
       "2                     0.318708                  -0.114301   \n",
       "3                     0.227647                  -0.196758   \n",
       "4                    -0.032126                   0.011889   \n",
       "..                         ...                        ...   \n",
       "213                  -0.103982                   0.008793   \n",
       "214                   0.255886                  -0.030827   \n",
       "215                   0.234856                  -0.231697   \n",
       "216                  -0.000555                  -0.163072   \n",
       "217                  -0.082829                   0.107020   \n",
       "\n",
       "     38 tBodyAcc-correlation()-X,Y  39 tBodyAcc-correlation()-X,Z  \\\n",
       "0                         0.449622                       0.474105   \n",
       "1                        -0.255320                      -0.064980   \n",
       "2                         0.016113                      -0.051272   \n",
       "3                        -0.097759                       0.084406   \n",
       "4                        -0.910582                      -0.706014   \n",
       "..                             ...                            ...   \n",
       "213                       0.122814                      -0.125760   \n",
       "214                       0.497752                      -0.443817   \n",
       "215                      -0.412537                       0.053695   \n",
       "216                      -0.660981                      -0.198059   \n",
       "217                      -0.013751                      -0.187977   \n",
       "\n",
       "     40 tBodyAcc-correlation()-Y,Z  Activity  Subject  \n",
       "0                         0.934028         6        1  \n",
       "1                        -0.035453         4        1  \n",
       "2                        -0.162163         6        1  \n",
       "3                         0.107562         5        1  \n",
       "4                         0.373016         4        1  \n",
       "..                             ...       ...      ...  \n",
       "213                       0.476450         5       30  \n",
       "214                      -0.474042         4       30  \n",
       "215                      -0.046689         5       30  \n",
       "216                      -0.551580         4       30  \n",
       "217                       0.393204         4       30  \n",
       "\n",
       "[218 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAN_data = GAN_data.groupby('Subject').apply(pd.DataFrame.sample, frac=0.03).reset_index(drop=True)\n",
    "GAN_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(data, current_user_tested):\n",
    "    \"\"\"\n",
    "    data: DataFrame\n",
    "    current_user_tested: int\n",
    "    \n",
    "    Returns: numpy arrays of X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    data = data.copy(deep = True)\n",
    "    \n",
    "    #Getting only acceleration columns\n",
    "    X_train = data.loc[:, \"1 tBodyAcc-mean()-X\": \"40 tBodyAcc-correlation()-Y,Z\"].values \n",
    "    #Selecting all activity labels\n",
    "    y_train = data.loc[:, \"Activity\"].values\n",
    "    \n",
    "    #X_test is the acceleration data for the current user being tested on\n",
    "    X_test = data[data['Subject'] == current_user_tested].loc[:, \"1 tBodyAcc-mean()-X\": \"40 tBodyAcc-correlation()-Y,Z\"].values \n",
    "    #y_test is the activity label for the current user being tested on\n",
    "    y_test = data[data['Subject'] == current_user_tested].loc[:, \"Activity\"].values \n",
    "    \n",
    "    #Zero indexing all activity labels since they start at 1\n",
    "    y_train -= 1 \n",
    "    y_test -=1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = 40):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 25),\n",
    "            classifier_block(25, 20),\n",
    "            classifier_block(20, 15),\n",
    "            classifier_block(15, 10),\n",
    "            nn.Linear(10, 6)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(X_train, y_train, X_test, y_test):\n",
    "    lr = 0.001\n",
    "    n_epochs = 1000\n",
    "    batch_size = 250\n",
    "    \n",
    "    model = Classifier()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    train_features = torch.tensor(X_train)\n",
    "    train_labels = torch.tensor(y_train)\n",
    "    test_features = torch.tensor(X_test)\n",
    "    test_labels = torch.tensor(y_test)\n",
    "    \n",
    "    train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)\n",
    "    \n",
    "    return model, train_loader, test_loader, optimizer, criterion\n",
    "\n",
    "def training_loop(model, train_loader, test_loader, optimizer, criterion, n_epochs = 1000):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features.float())\n",
    "\n",
    "            loss = criterion(preds, labels) \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')\n",
    "    \n",
    "    return model, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, test_loader):\n",
    "    softmax = nn.Softmax(dim = 1)\n",
    "    for batch in test_loader: #Runs once since the batch is the entire testing data\n",
    "        features, labels = batch\n",
    "        _, preds = torch.max(softmax(model(features.float())), dim = 1) #Getting the model's predictions\n",
    "        report = metrics.classification_report(labels, preds, digits = 3, output_dict = True, zero_division = 0)\n",
    "        f1_score = pd.DataFrame(report).transpose().loc['weighted avg', :]['f1-score']\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7878259420394897, Final Batch Loss: 1.7878259420394897\n",
      "Epoch 2, Loss: 1.786778211593628, Final Batch Loss: 1.786778211593628\n",
      "Epoch 3, Loss: 1.7858480215072632, Final Batch Loss: 1.7858480215072632\n",
      "Epoch 4, Loss: 1.7847460508346558, Final Batch Loss: 1.7847460508346558\n",
      "Epoch 5, Loss: 1.7807590961456299, Final Batch Loss: 1.7807590961456299\n",
      "Epoch 6, Loss: 1.7825628519058228, Final Batch Loss: 1.7825628519058228\n",
      "Epoch 7, Loss: 1.782076120376587, Final Batch Loss: 1.782076120376587\n",
      "Epoch 8, Loss: 1.7821141481399536, Final Batch Loss: 1.7821141481399536\n",
      "Epoch 9, Loss: 1.780045986175537, Final Batch Loss: 1.780045986175537\n",
      "Epoch 10, Loss: 1.7767857313156128, Final Batch Loss: 1.7767857313156128\n",
      "Epoch 11, Loss: 1.7735542058944702, Final Batch Loss: 1.7735542058944702\n",
      "Epoch 12, Loss: 1.7753028869628906, Final Batch Loss: 1.7753028869628906\n",
      "Epoch 13, Loss: 1.7754052877426147, Final Batch Loss: 1.7754052877426147\n",
      "Epoch 14, Loss: 1.7732810974121094, Final Batch Loss: 1.7732810974121094\n",
      "Epoch 15, Loss: 1.7706173658370972, Final Batch Loss: 1.7706173658370972\n",
      "Epoch 16, Loss: 1.7729350328445435, Final Batch Loss: 1.7729350328445435\n",
      "Epoch 17, Loss: 1.7689181566238403, Final Batch Loss: 1.7689181566238403\n",
      "Epoch 18, Loss: 1.768331527709961, Final Batch Loss: 1.768331527709961\n",
      "Epoch 19, Loss: 1.7718675136566162, Final Batch Loss: 1.7718675136566162\n",
      "Epoch 20, Loss: 1.7632766962051392, Final Batch Loss: 1.7632766962051392\n",
      "Epoch 21, Loss: 1.7590843439102173, Final Batch Loss: 1.7590843439102173\n",
      "Epoch 22, Loss: 1.7560827732086182, Final Batch Loss: 1.7560827732086182\n",
      "Epoch 23, Loss: 1.755884051322937, Final Batch Loss: 1.755884051322937\n",
      "Epoch 24, Loss: 1.7559291124343872, Final Batch Loss: 1.7559291124343872\n",
      "Epoch 25, Loss: 1.7533001899719238, Final Batch Loss: 1.7533001899719238\n",
      "Epoch 26, Loss: 1.7491542100906372, Final Batch Loss: 1.7491542100906372\n",
      "Epoch 27, Loss: 1.7485401630401611, Final Batch Loss: 1.7485401630401611\n",
      "Epoch 28, Loss: 1.7451202869415283, Final Batch Loss: 1.7451202869415283\n",
      "Epoch 29, Loss: 1.7393678426742554, Final Batch Loss: 1.7393678426742554\n",
      "Epoch 30, Loss: 1.7357312440872192, Final Batch Loss: 1.7357312440872192\n",
      "Epoch 31, Loss: 1.72628915309906, Final Batch Loss: 1.72628915309906\n",
      "Epoch 32, Loss: 1.7227039337158203, Final Batch Loss: 1.7227039337158203\n",
      "Epoch 33, Loss: 1.7184544801712036, Final Batch Loss: 1.7184544801712036\n",
      "Epoch 34, Loss: 1.7161977291107178, Final Batch Loss: 1.7161977291107178\n",
      "Epoch 35, Loss: 1.7052282094955444, Final Batch Loss: 1.7052282094955444\n",
      "Epoch 36, Loss: 1.702654242515564, Final Batch Loss: 1.702654242515564\n",
      "Epoch 37, Loss: 1.6929457187652588, Final Batch Loss: 1.6929457187652588\n",
      "Epoch 38, Loss: 1.68657386302948, Final Batch Loss: 1.68657386302948\n",
      "Epoch 39, Loss: 1.6788218021392822, Final Batch Loss: 1.6788218021392822\n",
      "Epoch 40, Loss: 1.6674989461898804, Final Batch Loss: 1.6674989461898804\n",
      "Epoch 41, Loss: 1.6697512865066528, Final Batch Loss: 1.6697512865066528\n",
      "Epoch 42, Loss: 1.659072756767273, Final Batch Loss: 1.659072756767273\n",
      "Epoch 43, Loss: 1.6487236022949219, Final Batch Loss: 1.6487236022949219\n",
      "Epoch 44, Loss: 1.6238830089569092, Final Batch Loss: 1.6238830089569092\n",
      "Epoch 45, Loss: 1.624433159828186, Final Batch Loss: 1.624433159828186\n",
      "Epoch 46, Loss: 1.6117234230041504, Final Batch Loss: 1.6117234230041504\n",
      "Epoch 47, Loss: 1.6019079685211182, Final Batch Loss: 1.6019079685211182\n",
      "Epoch 48, Loss: 1.5955135822296143, Final Batch Loss: 1.5955135822296143\n",
      "Epoch 49, Loss: 1.582063913345337, Final Batch Loss: 1.582063913345337\n",
      "Epoch 50, Loss: 1.5720361471176147, Final Batch Loss: 1.5720361471176147\n",
      "Epoch 51, Loss: 1.560110092163086, Final Batch Loss: 1.560110092163086\n",
      "Epoch 52, Loss: 1.5476861000061035, Final Batch Loss: 1.5476861000061035\n",
      "Epoch 53, Loss: 1.5303021669387817, Final Batch Loss: 1.5303021669387817\n",
      "Epoch 54, Loss: 1.526882290840149, Final Batch Loss: 1.526882290840149\n",
      "Epoch 55, Loss: 1.515408992767334, Final Batch Loss: 1.515408992767334\n",
      "Epoch 56, Loss: 1.4931275844573975, Final Batch Loss: 1.4931275844573975\n",
      "Epoch 57, Loss: 1.5009726285934448, Final Batch Loss: 1.5009726285934448\n",
      "Epoch 58, Loss: 1.4772191047668457, Final Batch Loss: 1.4772191047668457\n",
      "Epoch 59, Loss: 1.4588886499404907, Final Batch Loss: 1.4588886499404907\n",
      "Epoch 60, Loss: 1.4295132160186768, Final Batch Loss: 1.4295132160186768\n",
      "Epoch 61, Loss: 1.4399548768997192, Final Batch Loss: 1.4399548768997192\n",
      "Epoch 62, Loss: 1.4412921667099, Final Batch Loss: 1.4412921667099\n",
      "Epoch 63, Loss: 1.4271800518035889, Final Batch Loss: 1.4271800518035889\n",
      "Epoch 64, Loss: 1.4197804927825928, Final Batch Loss: 1.4197804927825928\n",
      "Epoch 65, Loss: 1.3707935810089111, Final Batch Loss: 1.3707935810089111\n",
      "Epoch 66, Loss: 1.4054534435272217, Final Batch Loss: 1.4054534435272217\n",
      "Epoch 67, Loss: 1.3817805051803589, Final Batch Loss: 1.3817805051803589\n",
      "Epoch 68, Loss: 1.3778117895126343, Final Batch Loss: 1.3778117895126343\n",
      "Epoch 69, Loss: 1.3826591968536377, Final Batch Loss: 1.3826591968536377\n",
      "Epoch 70, Loss: 1.3444275856018066, Final Batch Loss: 1.3444275856018066\n",
      "Epoch 71, Loss: 1.3346881866455078, Final Batch Loss: 1.3346881866455078\n",
      "Epoch 72, Loss: 1.3295127153396606, Final Batch Loss: 1.3295127153396606\n",
      "Epoch 73, Loss: 1.358034372329712, Final Batch Loss: 1.358034372329712\n",
      "Epoch 74, Loss: 1.3054180145263672, Final Batch Loss: 1.3054180145263672\n",
      "Epoch 75, Loss: 1.3275068998336792, Final Batch Loss: 1.3275068998336792\n",
      "Epoch 76, Loss: 1.3037375211715698, Final Batch Loss: 1.3037375211715698\n",
      "Epoch 77, Loss: 1.2884483337402344, Final Batch Loss: 1.2884483337402344\n",
      "Epoch 78, Loss: 1.3089802265167236, Final Batch Loss: 1.3089802265167236\n",
      "Epoch 79, Loss: 1.2667878866195679, Final Batch Loss: 1.2667878866195679\n",
      "Epoch 80, Loss: 1.2900278568267822, Final Batch Loss: 1.2900278568267822\n",
      "Epoch 81, Loss: 1.23591148853302, Final Batch Loss: 1.23591148853302\n",
      "Epoch 82, Loss: 1.2580816745758057, Final Batch Loss: 1.2580816745758057\n",
      "Epoch 83, Loss: 1.2615967988967896, Final Batch Loss: 1.2615967988967896\n",
      "Epoch 84, Loss: 1.2193681001663208, Final Batch Loss: 1.2193681001663208\n",
      "Epoch 85, Loss: 1.2285345792770386, Final Batch Loss: 1.2285345792770386\n",
      "Epoch 86, Loss: 1.2417274713516235, Final Batch Loss: 1.2417274713516235\n",
      "Epoch 87, Loss: 1.2025043964385986, Final Batch Loss: 1.2025043964385986\n",
      "Epoch 88, Loss: 1.220725417137146, Final Batch Loss: 1.220725417137146\n",
      "Epoch 89, Loss: 1.2188498973846436, Final Batch Loss: 1.2188498973846436\n",
      "Epoch 90, Loss: 1.2111207246780396, Final Batch Loss: 1.2111207246780396\n",
      "Epoch 91, Loss: 1.214853048324585, Final Batch Loss: 1.214853048324585\n",
      "Epoch 92, Loss: 1.215549349784851, Final Batch Loss: 1.215549349784851\n",
      "Epoch 93, Loss: 1.2359209060668945, Final Batch Loss: 1.2359209060668945\n",
      "Epoch 94, Loss: 1.199196219444275, Final Batch Loss: 1.199196219444275\n",
      "Epoch 95, Loss: 1.1826069355010986, Final Batch Loss: 1.1826069355010986\n",
      "Epoch 96, Loss: 1.1907767057418823, Final Batch Loss: 1.1907767057418823\n",
      "Epoch 97, Loss: 1.2254616022109985, Final Batch Loss: 1.2254616022109985\n",
      "Epoch 98, Loss: 1.2348779439926147, Final Batch Loss: 1.2348779439926147\n",
      "Epoch 99, Loss: 1.180998682975769, Final Batch Loss: 1.180998682975769\n",
      "Epoch 100, Loss: 1.1559395790100098, Final Batch Loss: 1.1559395790100098\n",
      "Epoch 101, Loss: 1.1788771152496338, Final Batch Loss: 1.1788771152496338\n",
      "Epoch 102, Loss: 1.203343391418457, Final Batch Loss: 1.203343391418457\n",
      "Epoch 103, Loss: 1.1656575202941895, Final Batch Loss: 1.1656575202941895\n",
      "Epoch 104, Loss: 1.1692826747894287, Final Batch Loss: 1.1692826747894287\n",
      "Epoch 105, Loss: 1.1819720268249512, Final Batch Loss: 1.1819720268249512\n",
      "Epoch 106, Loss: 1.1515954732894897, Final Batch Loss: 1.1515954732894897\n",
      "Epoch 107, Loss: 1.174236536026001, Final Batch Loss: 1.174236536026001\n",
      "Epoch 108, Loss: 1.1562626361846924, Final Batch Loss: 1.1562626361846924\n",
      "Epoch 109, Loss: 1.1738286018371582, Final Batch Loss: 1.1738286018371582\n",
      "Epoch 110, Loss: 1.2043371200561523, Final Batch Loss: 1.2043371200561523\n",
      "Epoch 111, Loss: 1.1362135410308838, Final Batch Loss: 1.1362135410308838\n",
      "Epoch 112, Loss: 1.1552714109420776, Final Batch Loss: 1.1552714109420776\n",
      "Epoch 113, Loss: 1.141790747642517, Final Batch Loss: 1.141790747642517\n",
      "Epoch 114, Loss: 1.1210453510284424, Final Batch Loss: 1.1210453510284424\n",
      "Epoch 115, Loss: 1.1525169610977173, Final Batch Loss: 1.1525169610977173\n",
      "Epoch 116, Loss: 1.1509277820587158, Final Batch Loss: 1.1509277820587158\n",
      "Epoch 117, Loss: 1.1308237314224243, Final Batch Loss: 1.1308237314224243\n",
      "Epoch 118, Loss: 1.1227060556411743, Final Batch Loss: 1.1227060556411743\n",
      "Epoch 119, Loss: 1.1317741870880127, Final Batch Loss: 1.1317741870880127\n",
      "Epoch 120, Loss: 1.13153874874115, Final Batch Loss: 1.13153874874115\n",
      "Epoch 121, Loss: 1.1315679550170898, Final Batch Loss: 1.1315679550170898\n",
      "Epoch 122, Loss: 1.1339772939682007, Final Batch Loss: 1.1339772939682007\n",
      "Epoch 123, Loss: 1.1445666551589966, Final Batch Loss: 1.1445666551589966\n",
      "Epoch 124, Loss: 1.103859543800354, Final Batch Loss: 1.103859543800354\n",
      "Epoch 125, Loss: 1.1299599409103394, Final Batch Loss: 1.1299599409103394\n",
      "Epoch 126, Loss: 1.1027575731277466, Final Batch Loss: 1.1027575731277466\n",
      "Epoch 127, Loss: 1.1130703687667847, Final Batch Loss: 1.1130703687667847\n",
      "Epoch 128, Loss: 1.1004176139831543, Final Batch Loss: 1.1004176139831543\n",
      "Epoch 129, Loss: 1.1285945177078247, Final Batch Loss: 1.1285945177078247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130, Loss: 1.130394458770752, Final Batch Loss: 1.130394458770752\n",
      "Epoch 131, Loss: 1.1336355209350586, Final Batch Loss: 1.1336355209350586\n",
      "Epoch 132, Loss: 1.1307388544082642, Final Batch Loss: 1.1307388544082642\n",
      "Epoch 133, Loss: 1.0637767314910889, Final Batch Loss: 1.0637767314910889\n",
      "Epoch 134, Loss: 1.0808111429214478, Final Batch Loss: 1.0808111429214478\n",
      "Epoch 135, Loss: 1.0978724956512451, Final Batch Loss: 1.0978724956512451\n",
      "Epoch 136, Loss: 1.0825631618499756, Final Batch Loss: 1.0825631618499756\n",
      "Epoch 137, Loss: 1.1002819538116455, Final Batch Loss: 1.1002819538116455\n",
      "Epoch 138, Loss: 1.0967830419540405, Final Batch Loss: 1.0967830419540405\n",
      "Epoch 139, Loss: 1.080553650856018, Final Batch Loss: 1.080553650856018\n",
      "Epoch 140, Loss: 1.0935121774673462, Final Batch Loss: 1.0935121774673462\n",
      "Epoch 141, Loss: 1.1125088930130005, Final Batch Loss: 1.1125088930130005\n",
      "Epoch 142, Loss: 1.1127363443374634, Final Batch Loss: 1.1127363443374634\n",
      "Epoch 143, Loss: 1.085721492767334, Final Batch Loss: 1.085721492767334\n",
      "Epoch 144, Loss: 1.0901933908462524, Final Batch Loss: 1.0901933908462524\n",
      "Epoch 145, Loss: 1.0550880432128906, Final Batch Loss: 1.0550880432128906\n",
      "Epoch 146, Loss: 1.054226279258728, Final Batch Loss: 1.054226279258728\n",
      "Epoch 147, Loss: 1.0573214292526245, Final Batch Loss: 1.0573214292526245\n",
      "Epoch 148, Loss: 1.0704020261764526, Final Batch Loss: 1.0704020261764526\n",
      "Epoch 149, Loss: 1.07894766330719, Final Batch Loss: 1.07894766330719\n",
      "Epoch 150, Loss: 1.063555359840393, Final Batch Loss: 1.063555359840393\n",
      "Epoch 151, Loss: 1.0763278007507324, Final Batch Loss: 1.0763278007507324\n",
      "Epoch 152, Loss: 1.0613576173782349, Final Batch Loss: 1.0613576173782349\n",
      "Epoch 153, Loss: 1.0288870334625244, Final Batch Loss: 1.0288870334625244\n",
      "Epoch 154, Loss: 1.070963740348816, Final Batch Loss: 1.070963740348816\n",
      "Epoch 155, Loss: 1.043841004371643, Final Batch Loss: 1.043841004371643\n",
      "Epoch 156, Loss: 1.0650473833084106, Final Batch Loss: 1.0650473833084106\n",
      "Epoch 157, Loss: 1.0702824592590332, Final Batch Loss: 1.0702824592590332\n",
      "Epoch 158, Loss: 1.0572741031646729, Final Batch Loss: 1.0572741031646729\n",
      "Epoch 159, Loss: 1.0471771955490112, Final Batch Loss: 1.0471771955490112\n",
      "Epoch 160, Loss: 1.0628039836883545, Final Batch Loss: 1.0628039836883545\n",
      "Epoch 161, Loss: 1.0678677558898926, Final Batch Loss: 1.0678677558898926\n",
      "Epoch 162, Loss: 1.0389094352722168, Final Batch Loss: 1.0389094352722168\n",
      "Epoch 163, Loss: 1.049765944480896, Final Batch Loss: 1.049765944480896\n",
      "Epoch 164, Loss: 1.0482240915298462, Final Batch Loss: 1.0482240915298462\n",
      "Epoch 165, Loss: 1.047020673751831, Final Batch Loss: 1.047020673751831\n",
      "Epoch 166, Loss: 1.0066518783569336, Final Batch Loss: 1.0066518783569336\n",
      "Epoch 167, Loss: 1.0767509937286377, Final Batch Loss: 1.0767509937286377\n",
      "Epoch 168, Loss: 1.0498566627502441, Final Batch Loss: 1.0498566627502441\n",
      "Epoch 169, Loss: 1.0431418418884277, Final Batch Loss: 1.0431418418884277\n",
      "Epoch 170, Loss: 1.0169090032577515, Final Batch Loss: 1.0169090032577515\n",
      "Epoch 171, Loss: 1.0271410942077637, Final Batch Loss: 1.0271410942077637\n",
      "Epoch 172, Loss: 1.0048882961273193, Final Batch Loss: 1.0048882961273193\n",
      "Epoch 173, Loss: 1.0046782493591309, Final Batch Loss: 1.0046782493591309\n",
      "Epoch 174, Loss: 1.0181851387023926, Final Batch Loss: 1.0181851387023926\n",
      "Epoch 175, Loss: 0.9913333654403687, Final Batch Loss: 0.9913333654403687\n",
      "Epoch 176, Loss: 0.9865657687187195, Final Batch Loss: 0.9865657687187195\n",
      "Epoch 177, Loss: 0.9732638001441956, Final Batch Loss: 0.9732638001441956\n",
      "Epoch 178, Loss: 1.00883150100708, Final Batch Loss: 1.00883150100708\n",
      "Epoch 179, Loss: 1.0035370588302612, Final Batch Loss: 1.0035370588302612\n",
      "Epoch 180, Loss: 0.9674983024597168, Final Batch Loss: 0.9674983024597168\n",
      "Epoch 181, Loss: 0.967754602432251, Final Batch Loss: 0.967754602432251\n",
      "Epoch 182, Loss: 0.9827411770820618, Final Batch Loss: 0.9827411770820618\n",
      "Epoch 183, Loss: 1.001656174659729, Final Batch Loss: 1.001656174659729\n",
      "Epoch 184, Loss: 0.9798728823661804, Final Batch Loss: 0.9798728823661804\n",
      "Epoch 185, Loss: 0.9819959402084351, Final Batch Loss: 0.9819959402084351\n",
      "Epoch 186, Loss: 0.9957213997840881, Final Batch Loss: 0.9957213997840881\n",
      "Epoch 187, Loss: 0.968617856502533, Final Batch Loss: 0.968617856502533\n",
      "Epoch 188, Loss: 0.9849753379821777, Final Batch Loss: 0.9849753379821777\n",
      "Epoch 189, Loss: 0.9523734450340271, Final Batch Loss: 0.9523734450340271\n",
      "Epoch 190, Loss: 1.003244400024414, Final Batch Loss: 1.003244400024414\n",
      "Epoch 191, Loss: 0.9755125045776367, Final Batch Loss: 0.9755125045776367\n",
      "Epoch 192, Loss: 0.9521176218986511, Final Batch Loss: 0.9521176218986511\n",
      "Epoch 193, Loss: 0.9958792924880981, Final Batch Loss: 0.9958792924880981\n",
      "Epoch 194, Loss: 0.974848747253418, Final Batch Loss: 0.974848747253418\n",
      "Epoch 195, Loss: 0.9631764888763428, Final Batch Loss: 0.9631764888763428\n",
      "Epoch 196, Loss: 0.959047794342041, Final Batch Loss: 0.959047794342041\n",
      "Epoch 197, Loss: 0.9876576662063599, Final Batch Loss: 0.9876576662063599\n",
      "Epoch 198, Loss: 0.963209331035614, Final Batch Loss: 0.963209331035614\n",
      "Epoch 199, Loss: 0.9671559929847717, Final Batch Loss: 0.9671559929847717\n",
      "Epoch 200, Loss: 0.9466999173164368, Final Batch Loss: 0.9466999173164368\n",
      "Epoch 201, Loss: 0.9638034701347351, Final Batch Loss: 0.9638034701347351\n",
      "Epoch 202, Loss: 0.9302735924720764, Final Batch Loss: 0.9302735924720764\n",
      "Epoch 203, Loss: 0.944553017616272, Final Batch Loss: 0.944553017616272\n",
      "Epoch 204, Loss: 0.924937903881073, Final Batch Loss: 0.924937903881073\n",
      "Epoch 205, Loss: 0.9416680335998535, Final Batch Loss: 0.9416680335998535\n",
      "Epoch 206, Loss: 0.9238736629486084, Final Batch Loss: 0.9238736629486084\n",
      "Epoch 207, Loss: 0.8922226428985596, Final Batch Loss: 0.8922226428985596\n",
      "Epoch 208, Loss: 0.938917338848114, Final Batch Loss: 0.938917338848114\n",
      "Epoch 209, Loss: 0.894067108631134, Final Batch Loss: 0.894067108631134\n",
      "Epoch 210, Loss: 0.9364892244338989, Final Batch Loss: 0.9364892244338989\n",
      "Epoch 211, Loss: 0.8975868821144104, Final Batch Loss: 0.8975868821144104\n",
      "Epoch 212, Loss: 0.9482866525650024, Final Batch Loss: 0.9482866525650024\n",
      "Epoch 213, Loss: 0.8971959352493286, Final Batch Loss: 0.8971959352493286\n",
      "Epoch 214, Loss: 0.9485063552856445, Final Batch Loss: 0.9485063552856445\n",
      "Epoch 215, Loss: 0.9471567869186401, Final Batch Loss: 0.9471567869186401\n",
      "Epoch 216, Loss: 0.9227287173271179, Final Batch Loss: 0.9227287173271179\n",
      "Epoch 217, Loss: 0.9308692812919617, Final Batch Loss: 0.9308692812919617\n",
      "Epoch 218, Loss: 0.9133703112602234, Final Batch Loss: 0.9133703112602234\n",
      "Epoch 219, Loss: 0.9320834875106812, Final Batch Loss: 0.9320834875106812\n",
      "Epoch 220, Loss: 0.8802679777145386, Final Batch Loss: 0.8802679777145386\n",
      "Epoch 221, Loss: 0.9334923028945923, Final Batch Loss: 0.9334923028945923\n",
      "Epoch 222, Loss: 0.9320075511932373, Final Batch Loss: 0.9320075511932373\n",
      "Epoch 223, Loss: 0.9094775319099426, Final Batch Loss: 0.9094775319099426\n",
      "Epoch 224, Loss: 0.9052949547767639, Final Batch Loss: 0.9052949547767639\n",
      "Epoch 225, Loss: 0.9163475632667542, Final Batch Loss: 0.9163475632667542\n",
      "Epoch 226, Loss: 0.8912721872329712, Final Batch Loss: 0.8912721872329712\n",
      "Epoch 227, Loss: 0.8906511068344116, Final Batch Loss: 0.8906511068344116\n",
      "Epoch 228, Loss: 0.9142889976501465, Final Batch Loss: 0.9142889976501465\n",
      "Epoch 229, Loss: 0.9046387076377869, Final Batch Loss: 0.9046387076377869\n",
      "Epoch 230, Loss: 0.922710120677948, Final Batch Loss: 0.922710120677948\n",
      "Epoch 231, Loss: 0.8917058706283569, Final Batch Loss: 0.8917058706283569\n",
      "Epoch 232, Loss: 0.8910596370697021, Final Batch Loss: 0.8910596370697021\n",
      "Epoch 233, Loss: 0.8967248797416687, Final Batch Loss: 0.8967248797416687\n",
      "Epoch 234, Loss: 0.8869816660881042, Final Batch Loss: 0.8869816660881042\n",
      "Epoch 235, Loss: 0.9056892991065979, Final Batch Loss: 0.9056892991065979\n",
      "Epoch 236, Loss: 0.8957540392875671, Final Batch Loss: 0.8957540392875671\n",
      "Epoch 237, Loss: 0.8909956812858582, Final Batch Loss: 0.8909956812858582\n",
      "Epoch 238, Loss: 0.8775268197059631, Final Batch Loss: 0.8775268197059631\n",
      "Epoch 239, Loss: 0.8671413660049438, Final Batch Loss: 0.8671413660049438\n",
      "Epoch 240, Loss: 0.871455192565918, Final Batch Loss: 0.871455192565918\n",
      "Epoch 241, Loss: 0.8908288478851318, Final Batch Loss: 0.8908288478851318\n",
      "Epoch 242, Loss: 0.9180639982223511, Final Batch Loss: 0.9180639982223511\n",
      "Epoch 243, Loss: 0.865264892578125, Final Batch Loss: 0.865264892578125\n",
      "Epoch 244, Loss: 0.8983595967292786, Final Batch Loss: 0.8983595967292786\n",
      "Epoch 245, Loss: 0.8747153282165527, Final Batch Loss: 0.8747153282165527\n",
      "Epoch 246, Loss: 0.9044672250747681, Final Batch Loss: 0.9044672250747681\n",
      "Epoch 247, Loss: 0.8644974231719971, Final Batch Loss: 0.8644974231719971\n",
      "Epoch 248, Loss: 0.8881335258483887, Final Batch Loss: 0.8881335258483887\n",
      "Epoch 249, Loss: 0.869662344455719, Final Batch Loss: 0.869662344455719\n",
      "Epoch 250, Loss: 0.8877564668655396, Final Batch Loss: 0.8877564668655396\n",
      "Epoch 251, Loss: 0.8719252347946167, Final Batch Loss: 0.8719252347946167\n",
      "Epoch 252, Loss: 0.8686086535453796, Final Batch Loss: 0.8686086535453796\n",
      "Epoch 253, Loss: 0.8521153330802917, Final Batch Loss: 0.8521153330802917\n",
      "Epoch 254, Loss: 0.8799235224723816, Final Batch Loss: 0.8799235224723816\n",
      "Epoch 255, Loss: 0.8383185863494873, Final Batch Loss: 0.8383185863494873\n",
      "Epoch 256, Loss: 0.845156192779541, Final Batch Loss: 0.845156192779541\n",
      "Epoch 257, Loss: 0.8981002569198608, Final Batch Loss: 0.8981002569198608\n",
      "Epoch 258, Loss: 0.8761556148529053, Final Batch Loss: 0.8761556148529053\n",
      "Epoch 259, Loss: 0.8195988535881042, Final Batch Loss: 0.8195988535881042\n",
      "Epoch 260, Loss: 0.8484414219856262, Final Batch Loss: 0.8484414219856262\n",
      "Epoch 261, Loss: 0.821028470993042, Final Batch Loss: 0.821028470993042\n",
      "Epoch 262, Loss: 0.8548708558082581, Final Batch Loss: 0.8548708558082581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263, Loss: 0.8700008988380432, Final Batch Loss: 0.8700008988380432\n",
      "Epoch 264, Loss: 0.8296788334846497, Final Batch Loss: 0.8296788334846497\n",
      "Epoch 265, Loss: 0.8286680579185486, Final Batch Loss: 0.8286680579185486\n",
      "Epoch 266, Loss: 0.9003423452377319, Final Batch Loss: 0.9003423452377319\n",
      "Epoch 267, Loss: 0.8050321340560913, Final Batch Loss: 0.8050321340560913\n",
      "Epoch 268, Loss: 0.8887962102890015, Final Batch Loss: 0.8887962102890015\n",
      "Epoch 269, Loss: 0.8116957545280457, Final Batch Loss: 0.8116957545280457\n",
      "Epoch 270, Loss: 0.8567867279052734, Final Batch Loss: 0.8567867279052734\n",
      "Epoch 271, Loss: 0.8199712038040161, Final Batch Loss: 0.8199712038040161\n",
      "Epoch 272, Loss: 0.8527567982673645, Final Batch Loss: 0.8527567982673645\n",
      "Epoch 273, Loss: 0.8488361239433289, Final Batch Loss: 0.8488361239433289\n",
      "Epoch 274, Loss: 0.833950936794281, Final Batch Loss: 0.833950936794281\n",
      "Epoch 275, Loss: 0.832815408706665, Final Batch Loss: 0.832815408706665\n",
      "Epoch 276, Loss: 0.835023820400238, Final Batch Loss: 0.835023820400238\n",
      "Epoch 277, Loss: 0.8327621817588806, Final Batch Loss: 0.8327621817588806\n",
      "Epoch 278, Loss: 0.8264920711517334, Final Batch Loss: 0.8264920711517334\n",
      "Epoch 279, Loss: 0.8822122812271118, Final Batch Loss: 0.8822122812271118\n",
      "Epoch 280, Loss: 0.8399139642715454, Final Batch Loss: 0.8399139642715454\n",
      "Epoch 281, Loss: 0.8365708589553833, Final Batch Loss: 0.8365708589553833\n",
      "Epoch 282, Loss: 0.7916789650917053, Final Batch Loss: 0.7916789650917053\n",
      "Epoch 283, Loss: 0.8504912853240967, Final Batch Loss: 0.8504912853240967\n",
      "Epoch 284, Loss: 0.804517388343811, Final Batch Loss: 0.804517388343811\n",
      "Epoch 285, Loss: 0.8260475397109985, Final Batch Loss: 0.8260475397109985\n",
      "Epoch 286, Loss: 0.8319577574729919, Final Batch Loss: 0.8319577574729919\n",
      "Epoch 287, Loss: 0.8360186219215393, Final Batch Loss: 0.8360186219215393\n",
      "Epoch 288, Loss: 0.8334969878196716, Final Batch Loss: 0.8334969878196716\n",
      "Epoch 289, Loss: 0.8355799913406372, Final Batch Loss: 0.8355799913406372\n",
      "Epoch 290, Loss: 0.8220731616020203, Final Batch Loss: 0.8220731616020203\n",
      "Epoch 291, Loss: 0.8440704345703125, Final Batch Loss: 0.8440704345703125\n",
      "Epoch 292, Loss: 0.7870689034461975, Final Batch Loss: 0.7870689034461975\n",
      "Epoch 293, Loss: 0.8279064893722534, Final Batch Loss: 0.8279064893722534\n",
      "Epoch 294, Loss: 0.8081218600273132, Final Batch Loss: 0.8081218600273132\n",
      "Epoch 295, Loss: 0.8172387480735779, Final Batch Loss: 0.8172387480735779\n",
      "Epoch 296, Loss: 0.810789942741394, Final Batch Loss: 0.810789942741394\n",
      "Epoch 297, Loss: 0.8205723762512207, Final Batch Loss: 0.8205723762512207\n",
      "Epoch 298, Loss: 0.803019106388092, Final Batch Loss: 0.803019106388092\n",
      "Epoch 299, Loss: 0.8024324178695679, Final Batch Loss: 0.8024324178695679\n",
      "Epoch 300, Loss: 0.8016304969787598, Final Batch Loss: 0.8016304969787598\n",
      "Epoch 301, Loss: 0.8085074424743652, Final Batch Loss: 0.8085074424743652\n",
      "Epoch 302, Loss: 0.8080046772956848, Final Batch Loss: 0.8080046772956848\n",
      "Epoch 303, Loss: 0.7650209665298462, Final Batch Loss: 0.7650209665298462\n",
      "Epoch 304, Loss: 0.79031902551651, Final Batch Loss: 0.79031902551651\n",
      "Epoch 305, Loss: 0.8217228651046753, Final Batch Loss: 0.8217228651046753\n",
      "Epoch 306, Loss: 0.7910982966423035, Final Batch Loss: 0.7910982966423035\n",
      "Epoch 307, Loss: 0.7657611966133118, Final Batch Loss: 0.7657611966133118\n",
      "Epoch 308, Loss: 0.8077672123908997, Final Batch Loss: 0.8077672123908997\n",
      "Epoch 309, Loss: 0.8123944997787476, Final Batch Loss: 0.8123944997787476\n",
      "Epoch 310, Loss: 0.8023833632469177, Final Batch Loss: 0.8023833632469177\n",
      "Epoch 311, Loss: 0.7850719094276428, Final Batch Loss: 0.7850719094276428\n",
      "Epoch 312, Loss: 0.7867265343666077, Final Batch Loss: 0.7867265343666077\n",
      "Epoch 313, Loss: 0.7950354218482971, Final Batch Loss: 0.7950354218482971\n",
      "Epoch 314, Loss: 0.842070460319519, Final Batch Loss: 0.842070460319519\n",
      "Epoch 315, Loss: 0.7763931751251221, Final Batch Loss: 0.7763931751251221\n",
      "Epoch 316, Loss: 0.805858850479126, Final Batch Loss: 0.805858850479126\n",
      "Epoch 317, Loss: 0.7850632071495056, Final Batch Loss: 0.7850632071495056\n",
      "Epoch 318, Loss: 0.8324677348136902, Final Batch Loss: 0.8324677348136902\n",
      "Epoch 319, Loss: 0.8058271408081055, Final Batch Loss: 0.8058271408081055\n",
      "Epoch 320, Loss: 0.7817540764808655, Final Batch Loss: 0.7817540764808655\n",
      "Epoch 321, Loss: 0.732980489730835, Final Batch Loss: 0.732980489730835\n",
      "Epoch 322, Loss: 0.8007104396820068, Final Batch Loss: 0.8007104396820068\n",
      "Epoch 323, Loss: 0.797840416431427, Final Batch Loss: 0.797840416431427\n",
      "Epoch 324, Loss: 0.7990638613700867, Final Batch Loss: 0.7990638613700867\n",
      "Epoch 325, Loss: 0.7881293892860413, Final Batch Loss: 0.7881293892860413\n",
      "Epoch 326, Loss: 0.7509608864784241, Final Batch Loss: 0.7509608864784241\n",
      "Epoch 327, Loss: 0.7723283171653748, Final Batch Loss: 0.7723283171653748\n",
      "Epoch 328, Loss: 0.7892559766769409, Final Batch Loss: 0.7892559766769409\n",
      "Epoch 329, Loss: 0.7874148488044739, Final Batch Loss: 0.7874148488044739\n",
      "Epoch 330, Loss: 0.765786349773407, Final Batch Loss: 0.765786349773407\n",
      "Epoch 331, Loss: 0.7635504603385925, Final Batch Loss: 0.7635504603385925\n",
      "Epoch 332, Loss: 0.7770216464996338, Final Batch Loss: 0.7770216464996338\n",
      "Epoch 333, Loss: 0.7873901724815369, Final Batch Loss: 0.7873901724815369\n",
      "Epoch 334, Loss: 0.8155521154403687, Final Batch Loss: 0.8155521154403687\n",
      "Epoch 335, Loss: 0.78936767578125, Final Batch Loss: 0.78936767578125\n",
      "Epoch 336, Loss: 0.7944480776786804, Final Batch Loss: 0.7944480776786804\n",
      "Epoch 337, Loss: 0.7715519666671753, Final Batch Loss: 0.7715519666671753\n",
      "Epoch 338, Loss: 0.756613552570343, Final Batch Loss: 0.756613552570343\n",
      "Epoch 339, Loss: 0.769855797290802, Final Batch Loss: 0.769855797290802\n",
      "Epoch 340, Loss: 0.7599530220031738, Final Batch Loss: 0.7599530220031738\n",
      "Epoch 341, Loss: 0.7733373641967773, Final Batch Loss: 0.7733373641967773\n",
      "Epoch 342, Loss: 0.7579247951507568, Final Batch Loss: 0.7579247951507568\n",
      "Epoch 343, Loss: 0.7835741639137268, Final Batch Loss: 0.7835741639137268\n",
      "Epoch 344, Loss: 0.7525418996810913, Final Batch Loss: 0.7525418996810913\n",
      "Epoch 345, Loss: 0.7745921015739441, Final Batch Loss: 0.7745921015739441\n",
      "Epoch 346, Loss: 0.7567005157470703, Final Batch Loss: 0.7567005157470703\n",
      "Epoch 347, Loss: 0.7871968150138855, Final Batch Loss: 0.7871968150138855\n",
      "Epoch 348, Loss: 0.7831286787986755, Final Batch Loss: 0.7831286787986755\n",
      "Epoch 349, Loss: 0.7546961903572083, Final Batch Loss: 0.7546961903572083\n",
      "Epoch 350, Loss: 0.763247549533844, Final Batch Loss: 0.763247549533844\n",
      "Epoch 351, Loss: 0.7562903165817261, Final Batch Loss: 0.7562903165817261\n",
      "Epoch 352, Loss: 0.7482555508613586, Final Batch Loss: 0.7482555508613586\n",
      "Epoch 353, Loss: 0.7683002352714539, Final Batch Loss: 0.7683002352714539\n",
      "Epoch 354, Loss: 0.7694360613822937, Final Batch Loss: 0.7694360613822937\n",
      "Epoch 355, Loss: 0.7670770883560181, Final Batch Loss: 0.7670770883560181\n",
      "Epoch 356, Loss: 0.7624131441116333, Final Batch Loss: 0.7624131441116333\n",
      "Epoch 357, Loss: 0.7546992897987366, Final Batch Loss: 0.7546992897987366\n",
      "Epoch 358, Loss: 0.7405775785446167, Final Batch Loss: 0.7405775785446167\n",
      "Epoch 359, Loss: 0.731182336807251, Final Batch Loss: 0.731182336807251\n",
      "Epoch 360, Loss: 0.7359856963157654, Final Batch Loss: 0.7359856963157654\n",
      "Epoch 361, Loss: 0.7508595585823059, Final Batch Loss: 0.7508595585823059\n",
      "Epoch 362, Loss: 0.7227076888084412, Final Batch Loss: 0.7227076888084412\n",
      "Epoch 363, Loss: 0.7550452947616577, Final Batch Loss: 0.7550452947616577\n",
      "Epoch 364, Loss: 0.7504063248634338, Final Batch Loss: 0.7504063248634338\n",
      "Epoch 365, Loss: 0.7683030366897583, Final Batch Loss: 0.7683030366897583\n",
      "Epoch 366, Loss: 0.7339476346969604, Final Batch Loss: 0.7339476346969604\n",
      "Epoch 367, Loss: 0.741529643535614, Final Batch Loss: 0.741529643535614\n",
      "Epoch 368, Loss: 0.7503833770751953, Final Batch Loss: 0.7503833770751953\n",
      "Epoch 369, Loss: 0.7604313492774963, Final Batch Loss: 0.7604313492774963\n",
      "Epoch 370, Loss: 0.7498300671577454, Final Batch Loss: 0.7498300671577454\n",
      "Epoch 371, Loss: 0.7782660722732544, Final Batch Loss: 0.7782660722732544\n",
      "Epoch 372, Loss: 0.7207796573638916, Final Batch Loss: 0.7207796573638916\n",
      "Epoch 373, Loss: 0.7458588480949402, Final Batch Loss: 0.7458588480949402\n",
      "Epoch 374, Loss: 0.7432149052619934, Final Batch Loss: 0.7432149052619934\n",
      "Epoch 375, Loss: 0.7259752750396729, Final Batch Loss: 0.7259752750396729\n",
      "Epoch 376, Loss: 0.7419870495796204, Final Batch Loss: 0.7419870495796204\n",
      "Epoch 377, Loss: 0.7477943897247314, Final Batch Loss: 0.7477943897247314\n",
      "Epoch 378, Loss: 0.7498220801353455, Final Batch Loss: 0.7498220801353455\n",
      "Epoch 379, Loss: 0.7411927580833435, Final Batch Loss: 0.7411927580833435\n",
      "Epoch 380, Loss: 0.7441816926002502, Final Batch Loss: 0.7441816926002502\n",
      "Epoch 381, Loss: 0.7589043974876404, Final Batch Loss: 0.7589043974876404\n",
      "Epoch 382, Loss: 0.760208010673523, Final Batch Loss: 0.760208010673523\n",
      "Epoch 383, Loss: 0.7306773066520691, Final Batch Loss: 0.7306773066520691\n",
      "Epoch 384, Loss: 0.7814134955406189, Final Batch Loss: 0.7814134955406189\n",
      "Epoch 385, Loss: 0.7464543581008911, Final Batch Loss: 0.7464543581008911\n",
      "Epoch 386, Loss: 0.7307292222976685, Final Batch Loss: 0.7307292222976685\n",
      "Epoch 387, Loss: 0.7333862781524658, Final Batch Loss: 0.7333862781524658\n",
      "Epoch 388, Loss: 0.7323460578918457, Final Batch Loss: 0.7323460578918457\n",
      "Epoch 389, Loss: 0.754216194152832, Final Batch Loss: 0.754216194152832\n",
      "Epoch 390, Loss: 0.7473265528678894, Final Batch Loss: 0.7473265528678894\n",
      "Epoch 391, Loss: 0.7610723376274109, Final Batch Loss: 0.7610723376274109\n",
      "Epoch 392, Loss: 0.7277905941009521, Final Batch Loss: 0.7277905941009521\n",
      "Epoch 393, Loss: 0.7085365056991577, Final Batch Loss: 0.7085365056991577\n",
      "Epoch 394, Loss: 0.7032482028007507, Final Batch Loss: 0.7032482028007507\n",
      "Epoch 395, Loss: 0.7260164022445679, Final Batch Loss: 0.7260164022445679\n",
      "Epoch 396, Loss: 0.7407909631729126, Final Batch Loss: 0.7407909631729126\n",
      "Epoch 397, Loss: 0.7307966947555542, Final Batch Loss: 0.7307966947555542\n",
      "Epoch 398, Loss: 0.7202107906341553, Final Batch Loss: 0.7202107906341553\n",
      "Epoch 399, Loss: 0.7321332097053528, Final Batch Loss: 0.7321332097053528\n",
      "Epoch 400, Loss: 0.7385221123695374, Final Batch Loss: 0.7385221123695374\n",
      "Epoch 401, Loss: 0.7141993045806885, Final Batch Loss: 0.7141993045806885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402, Loss: 0.7579106092453003, Final Batch Loss: 0.7579106092453003\n",
      "Epoch 403, Loss: 0.7333508133888245, Final Batch Loss: 0.7333508133888245\n",
      "Epoch 404, Loss: 0.749610185623169, Final Batch Loss: 0.749610185623169\n",
      "Epoch 405, Loss: 0.7211530208587646, Final Batch Loss: 0.7211530208587646\n",
      "Epoch 406, Loss: 0.7254535555839539, Final Batch Loss: 0.7254535555839539\n",
      "Epoch 407, Loss: 0.7395862936973572, Final Batch Loss: 0.7395862936973572\n",
      "Epoch 408, Loss: 0.7093861699104309, Final Batch Loss: 0.7093861699104309\n",
      "Epoch 409, Loss: 0.7257923483848572, Final Batch Loss: 0.7257923483848572\n",
      "Epoch 410, Loss: 0.7230382561683655, Final Batch Loss: 0.7230382561683655\n",
      "Epoch 411, Loss: 0.700825572013855, Final Batch Loss: 0.700825572013855\n",
      "Epoch 412, Loss: 0.7247857451438904, Final Batch Loss: 0.7247857451438904\n",
      "Epoch 413, Loss: 0.7408142685890198, Final Batch Loss: 0.7408142685890198\n",
      "Epoch 414, Loss: 0.6852356195449829, Final Batch Loss: 0.6852356195449829\n",
      "Epoch 415, Loss: 0.7017635107040405, Final Batch Loss: 0.7017635107040405\n",
      "Epoch 416, Loss: 0.7190130949020386, Final Batch Loss: 0.7190130949020386\n",
      "Epoch 417, Loss: 0.7091368436813354, Final Batch Loss: 0.7091368436813354\n",
      "Epoch 418, Loss: 0.7422232627868652, Final Batch Loss: 0.7422232627868652\n",
      "Epoch 419, Loss: 0.6947360038757324, Final Batch Loss: 0.6947360038757324\n",
      "Epoch 420, Loss: 0.7060896158218384, Final Batch Loss: 0.7060896158218384\n",
      "Epoch 421, Loss: 0.7259342074394226, Final Batch Loss: 0.7259342074394226\n",
      "Epoch 422, Loss: 0.7182776927947998, Final Batch Loss: 0.7182776927947998\n",
      "Epoch 423, Loss: 0.7357909083366394, Final Batch Loss: 0.7357909083366394\n",
      "Epoch 424, Loss: 0.6944881677627563, Final Batch Loss: 0.6944881677627563\n",
      "Epoch 425, Loss: 0.7608205676078796, Final Batch Loss: 0.7608205676078796\n",
      "Epoch 426, Loss: 0.7430009245872498, Final Batch Loss: 0.7430009245872498\n",
      "Epoch 427, Loss: 0.6969668865203857, Final Batch Loss: 0.6969668865203857\n",
      "Epoch 428, Loss: 0.7323282361030579, Final Batch Loss: 0.7323282361030579\n",
      "Epoch 429, Loss: 0.7257608771324158, Final Batch Loss: 0.7257608771324158\n",
      "Epoch 430, Loss: 0.6832585334777832, Final Batch Loss: 0.6832585334777832\n",
      "Epoch 431, Loss: 0.6800608038902283, Final Batch Loss: 0.6800608038902283\n",
      "Epoch 432, Loss: 0.6987835168838501, Final Batch Loss: 0.6987835168838501\n",
      "Epoch 433, Loss: 0.7087437510490417, Final Batch Loss: 0.7087437510490417\n",
      "Epoch 434, Loss: 0.745927095413208, Final Batch Loss: 0.745927095413208\n",
      "Epoch 435, Loss: 0.6955994963645935, Final Batch Loss: 0.6955994963645935\n",
      "Epoch 436, Loss: 0.719451904296875, Final Batch Loss: 0.719451904296875\n",
      "Epoch 437, Loss: 0.7317391037940979, Final Batch Loss: 0.7317391037940979\n",
      "Epoch 438, Loss: 0.6756609082221985, Final Batch Loss: 0.6756609082221985\n",
      "Epoch 439, Loss: 0.6925017833709717, Final Batch Loss: 0.6925017833709717\n",
      "Epoch 440, Loss: 0.6969596147537231, Final Batch Loss: 0.6969596147537231\n",
      "Epoch 441, Loss: 0.6971623301506042, Final Batch Loss: 0.6971623301506042\n",
      "Epoch 442, Loss: 0.696597695350647, Final Batch Loss: 0.696597695350647\n",
      "Epoch 443, Loss: 0.6881182789802551, Final Batch Loss: 0.6881182789802551\n",
      "Epoch 444, Loss: 0.6844450831413269, Final Batch Loss: 0.6844450831413269\n",
      "Epoch 445, Loss: 0.6983100771903992, Final Batch Loss: 0.6983100771903992\n",
      "Epoch 446, Loss: 0.6945743560791016, Final Batch Loss: 0.6945743560791016\n",
      "Epoch 447, Loss: 0.6797288656234741, Final Batch Loss: 0.6797288656234741\n",
      "Epoch 448, Loss: 0.7109424471855164, Final Batch Loss: 0.7109424471855164\n",
      "Epoch 449, Loss: 0.6983701586723328, Final Batch Loss: 0.6983701586723328\n",
      "Epoch 450, Loss: 0.6901029348373413, Final Batch Loss: 0.6901029348373413\n",
      "Epoch 451, Loss: 0.7121810913085938, Final Batch Loss: 0.7121810913085938\n",
      "Epoch 452, Loss: 0.6812024712562561, Final Batch Loss: 0.6812024712562561\n",
      "Epoch 453, Loss: 0.6716321110725403, Final Batch Loss: 0.6716321110725403\n",
      "Epoch 454, Loss: 0.6989746689796448, Final Batch Loss: 0.6989746689796448\n",
      "Epoch 455, Loss: 0.6920800805091858, Final Batch Loss: 0.6920800805091858\n",
      "Epoch 456, Loss: 0.7020389437675476, Final Batch Loss: 0.7020389437675476\n",
      "Epoch 457, Loss: 0.674010157585144, Final Batch Loss: 0.674010157585144\n",
      "Epoch 458, Loss: 0.6838582754135132, Final Batch Loss: 0.6838582754135132\n",
      "Epoch 459, Loss: 0.6952426433563232, Final Batch Loss: 0.6952426433563232\n",
      "Epoch 460, Loss: 0.6947861909866333, Final Batch Loss: 0.6947861909866333\n",
      "Epoch 461, Loss: 0.7022376656532288, Final Batch Loss: 0.7022376656532288\n",
      "Epoch 462, Loss: 0.6993453502655029, Final Batch Loss: 0.6993453502655029\n",
      "Epoch 463, Loss: 0.6873210668563843, Final Batch Loss: 0.6873210668563843\n",
      "Epoch 464, Loss: 0.6669518947601318, Final Batch Loss: 0.6669518947601318\n",
      "Epoch 465, Loss: 0.7153750061988831, Final Batch Loss: 0.7153750061988831\n",
      "Epoch 466, Loss: 0.704507052898407, Final Batch Loss: 0.704507052898407\n",
      "Epoch 467, Loss: 0.6740137338638306, Final Batch Loss: 0.6740137338638306\n",
      "Epoch 468, Loss: 0.6961238980293274, Final Batch Loss: 0.6961238980293274\n",
      "Epoch 469, Loss: 0.7028750777244568, Final Batch Loss: 0.7028750777244568\n",
      "Epoch 470, Loss: 0.6624979972839355, Final Batch Loss: 0.6624979972839355\n",
      "Epoch 471, Loss: 0.6632111668586731, Final Batch Loss: 0.6632111668586731\n",
      "Epoch 472, Loss: 0.6771411299705505, Final Batch Loss: 0.6771411299705505\n",
      "Epoch 473, Loss: 0.7462074160575867, Final Batch Loss: 0.7462074160575867\n",
      "Epoch 474, Loss: 0.6737813949584961, Final Batch Loss: 0.6737813949584961\n",
      "Epoch 475, Loss: 0.669329047203064, Final Batch Loss: 0.669329047203064\n",
      "Epoch 476, Loss: 0.6578480005264282, Final Batch Loss: 0.6578480005264282\n",
      "Epoch 477, Loss: 0.6530396342277527, Final Batch Loss: 0.6530396342277527\n",
      "Epoch 478, Loss: 0.6891162991523743, Final Batch Loss: 0.6891162991523743\n",
      "Epoch 479, Loss: 0.664853572845459, Final Batch Loss: 0.664853572845459\n",
      "Epoch 480, Loss: 0.6983903050422668, Final Batch Loss: 0.6983903050422668\n",
      "Epoch 481, Loss: 0.6955502033233643, Final Batch Loss: 0.6955502033233643\n",
      "Epoch 482, Loss: 0.6757369637489319, Final Batch Loss: 0.6757369637489319\n",
      "Epoch 483, Loss: 0.6707357168197632, Final Batch Loss: 0.6707357168197632\n",
      "Epoch 484, Loss: 0.685624361038208, Final Batch Loss: 0.685624361038208\n",
      "Epoch 485, Loss: 0.6824596524238586, Final Batch Loss: 0.6824596524238586\n",
      "Epoch 486, Loss: 0.683894157409668, Final Batch Loss: 0.683894157409668\n",
      "Epoch 487, Loss: 0.7118319869041443, Final Batch Loss: 0.7118319869041443\n",
      "Epoch 488, Loss: 0.6605603694915771, Final Batch Loss: 0.6605603694915771\n",
      "Epoch 489, Loss: 0.6510215401649475, Final Batch Loss: 0.6510215401649475\n",
      "Epoch 490, Loss: 0.6998922228813171, Final Batch Loss: 0.6998922228813171\n",
      "Epoch 491, Loss: 0.6685565710067749, Final Batch Loss: 0.6685565710067749\n",
      "Epoch 492, Loss: 0.6661301851272583, Final Batch Loss: 0.6661301851272583\n",
      "Epoch 493, Loss: 0.6742435693740845, Final Batch Loss: 0.6742435693740845\n",
      "Epoch 494, Loss: 0.6673636436462402, Final Batch Loss: 0.6673636436462402\n",
      "Epoch 495, Loss: 0.6798528432846069, Final Batch Loss: 0.6798528432846069\n",
      "Epoch 496, Loss: 0.664814293384552, Final Batch Loss: 0.664814293384552\n",
      "Epoch 497, Loss: 0.6536153554916382, Final Batch Loss: 0.6536153554916382\n",
      "Epoch 498, Loss: 0.6739566326141357, Final Batch Loss: 0.6739566326141357\n",
      "Epoch 499, Loss: 0.7177979946136475, Final Batch Loss: 0.7177979946136475\n",
      "Epoch 500, Loss: 0.6534497141838074, Final Batch Loss: 0.6534497141838074\n",
      "Epoch 501, Loss: 0.6847748160362244, Final Batch Loss: 0.6847748160362244\n",
      "Epoch 502, Loss: 0.6714396476745605, Final Batch Loss: 0.6714396476745605\n",
      "Epoch 503, Loss: 0.6759042739868164, Final Batch Loss: 0.6759042739868164\n",
      "Epoch 504, Loss: 0.6735502481460571, Final Batch Loss: 0.6735502481460571\n",
      "Epoch 505, Loss: 0.6753420829772949, Final Batch Loss: 0.6753420829772949\n",
      "Epoch 506, Loss: 0.6838529109954834, Final Batch Loss: 0.6838529109954834\n",
      "Epoch 507, Loss: 0.7001211047172546, Final Batch Loss: 0.7001211047172546\n",
      "Epoch 508, Loss: 0.6451416611671448, Final Batch Loss: 0.6451416611671448\n",
      "Epoch 509, Loss: 0.6567094922065735, Final Batch Loss: 0.6567094922065735\n",
      "Epoch 510, Loss: 0.6361439824104309, Final Batch Loss: 0.6361439824104309\n",
      "Epoch 511, Loss: 0.6456246376037598, Final Batch Loss: 0.6456246376037598\n",
      "Epoch 512, Loss: 0.6409782767295837, Final Batch Loss: 0.6409782767295837\n",
      "Epoch 513, Loss: 0.6407157182693481, Final Batch Loss: 0.6407157182693481\n",
      "Epoch 514, Loss: 0.6920562982559204, Final Batch Loss: 0.6920562982559204\n",
      "Epoch 515, Loss: 0.633566677570343, Final Batch Loss: 0.633566677570343\n",
      "Epoch 516, Loss: 0.6592087149620056, Final Batch Loss: 0.6592087149620056\n",
      "Epoch 517, Loss: 0.6615062952041626, Final Batch Loss: 0.6615062952041626\n",
      "Epoch 518, Loss: 0.66550213098526, Final Batch Loss: 0.66550213098526\n",
      "Epoch 519, Loss: 0.6527509093284607, Final Batch Loss: 0.6527509093284607\n",
      "Epoch 520, Loss: 0.6465573310852051, Final Batch Loss: 0.6465573310852051\n",
      "Epoch 521, Loss: 0.6713681221008301, Final Batch Loss: 0.6713681221008301\n",
      "Epoch 522, Loss: 0.6741566061973572, Final Batch Loss: 0.6741566061973572\n",
      "Epoch 523, Loss: 0.6425076127052307, Final Batch Loss: 0.6425076127052307\n",
      "Epoch 524, Loss: 0.6664267778396606, Final Batch Loss: 0.6664267778396606\n",
      "Epoch 525, Loss: 0.6626168489456177, Final Batch Loss: 0.6626168489456177\n",
      "Epoch 526, Loss: 0.6366532444953918, Final Batch Loss: 0.6366532444953918\n",
      "Epoch 527, Loss: 0.6769717931747437, Final Batch Loss: 0.6769717931747437\n",
      "Epoch 528, Loss: 0.6473652124404907, Final Batch Loss: 0.6473652124404907\n",
      "Epoch 529, Loss: 0.6362979412078857, Final Batch Loss: 0.6362979412078857\n",
      "Epoch 530, Loss: 0.6546206474304199, Final Batch Loss: 0.6546206474304199\n",
      "Epoch 531, Loss: 0.6425743699073792, Final Batch Loss: 0.6425743699073792\n",
      "Epoch 532, Loss: 0.6736838817596436, Final Batch Loss: 0.6736838817596436\n",
      "Epoch 533, Loss: 0.682149350643158, Final Batch Loss: 0.682149350643158\n",
      "Epoch 534, Loss: 0.6514734625816345, Final Batch Loss: 0.6514734625816345\n",
      "Epoch 535, Loss: 0.6506251096725464, Final Batch Loss: 0.6506251096725464\n",
      "Epoch 536, Loss: 0.6564677953720093, Final Batch Loss: 0.6564677953720093\n",
      "Epoch 537, Loss: 0.678748607635498, Final Batch Loss: 0.678748607635498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 538, Loss: 0.6605439782142639, Final Batch Loss: 0.6605439782142639\n",
      "Epoch 539, Loss: 0.6506525278091431, Final Batch Loss: 0.6506525278091431\n",
      "Epoch 540, Loss: 0.6259909272193909, Final Batch Loss: 0.6259909272193909\n",
      "Epoch 541, Loss: 0.6362252831459045, Final Batch Loss: 0.6362252831459045\n",
      "Epoch 542, Loss: 0.6614155769348145, Final Batch Loss: 0.6614155769348145\n",
      "Epoch 543, Loss: 0.6271560192108154, Final Batch Loss: 0.6271560192108154\n",
      "Epoch 544, Loss: 0.672586977481842, Final Batch Loss: 0.672586977481842\n",
      "Epoch 545, Loss: 0.6484466791152954, Final Batch Loss: 0.6484466791152954\n",
      "Epoch 546, Loss: 0.6360553503036499, Final Batch Loss: 0.6360553503036499\n",
      "Epoch 547, Loss: 0.6340427994728088, Final Batch Loss: 0.6340427994728088\n",
      "Epoch 548, Loss: 0.6492859721183777, Final Batch Loss: 0.6492859721183777\n",
      "Epoch 549, Loss: 0.6542497277259827, Final Batch Loss: 0.6542497277259827\n",
      "Epoch 550, Loss: 0.6498773097991943, Final Batch Loss: 0.6498773097991943\n",
      "Epoch 551, Loss: 0.6434166431427002, Final Batch Loss: 0.6434166431427002\n",
      "Epoch 552, Loss: 0.6571290493011475, Final Batch Loss: 0.6571290493011475\n",
      "Epoch 553, Loss: 0.6428167819976807, Final Batch Loss: 0.6428167819976807\n",
      "Epoch 554, Loss: 0.6283501982688904, Final Batch Loss: 0.6283501982688904\n",
      "Epoch 555, Loss: 0.632950484752655, Final Batch Loss: 0.632950484752655\n",
      "Epoch 556, Loss: 0.6339059472084045, Final Batch Loss: 0.6339059472084045\n",
      "Epoch 557, Loss: 0.6431537866592407, Final Batch Loss: 0.6431537866592407\n",
      "Epoch 558, Loss: 0.6660439968109131, Final Batch Loss: 0.6660439968109131\n",
      "Epoch 559, Loss: 0.6690390110015869, Final Batch Loss: 0.6690390110015869\n",
      "Epoch 560, Loss: 0.635699987411499, Final Batch Loss: 0.635699987411499\n",
      "Epoch 561, Loss: 0.6510162353515625, Final Batch Loss: 0.6510162353515625\n",
      "Epoch 562, Loss: 0.6326128244400024, Final Batch Loss: 0.6326128244400024\n",
      "Epoch 563, Loss: 0.6378703713417053, Final Batch Loss: 0.6378703713417053\n",
      "Epoch 564, Loss: 0.6835376024246216, Final Batch Loss: 0.6835376024246216\n",
      "Epoch 565, Loss: 0.6701164841651917, Final Batch Loss: 0.6701164841651917\n",
      "Epoch 566, Loss: 0.645885705947876, Final Batch Loss: 0.645885705947876\n",
      "Epoch 567, Loss: 0.6415337324142456, Final Batch Loss: 0.6415337324142456\n",
      "Epoch 568, Loss: 0.6487403512001038, Final Batch Loss: 0.6487403512001038\n",
      "Epoch 569, Loss: 0.6633386611938477, Final Batch Loss: 0.6633386611938477\n",
      "Epoch 570, Loss: 0.6427822709083557, Final Batch Loss: 0.6427822709083557\n",
      "Epoch 571, Loss: 0.6359968781471252, Final Batch Loss: 0.6359968781471252\n",
      "Epoch 572, Loss: 0.6355595588684082, Final Batch Loss: 0.6355595588684082\n",
      "Epoch 573, Loss: 0.6353690028190613, Final Batch Loss: 0.6353690028190613\n",
      "Epoch 574, Loss: 0.6708260774612427, Final Batch Loss: 0.6708260774612427\n",
      "Epoch 575, Loss: 0.6195422410964966, Final Batch Loss: 0.6195422410964966\n",
      "Epoch 576, Loss: 0.6466266512870789, Final Batch Loss: 0.6466266512870789\n",
      "Epoch 577, Loss: 0.6037988066673279, Final Batch Loss: 0.6037988066673279\n",
      "Epoch 578, Loss: 0.6422262787818909, Final Batch Loss: 0.6422262787818909\n",
      "Epoch 579, Loss: 0.6337106823921204, Final Batch Loss: 0.6337106823921204\n",
      "Epoch 580, Loss: 0.6563822627067566, Final Batch Loss: 0.6563822627067566\n",
      "Epoch 581, Loss: 0.6327390670776367, Final Batch Loss: 0.6327390670776367\n",
      "Epoch 582, Loss: 0.6628912091255188, Final Batch Loss: 0.6628912091255188\n",
      "Epoch 583, Loss: 0.6288034319877625, Final Batch Loss: 0.6288034319877625\n",
      "Epoch 584, Loss: 0.6195580959320068, Final Batch Loss: 0.6195580959320068\n",
      "Epoch 585, Loss: 0.6620603203773499, Final Batch Loss: 0.6620603203773499\n",
      "Epoch 586, Loss: 0.6141065955162048, Final Batch Loss: 0.6141065955162048\n",
      "Epoch 587, Loss: 0.6278099417686462, Final Batch Loss: 0.6278099417686462\n",
      "Epoch 588, Loss: 0.6545501351356506, Final Batch Loss: 0.6545501351356506\n",
      "Epoch 589, Loss: 0.6471249461174011, Final Batch Loss: 0.6471249461174011\n",
      "Epoch 590, Loss: 0.6185622811317444, Final Batch Loss: 0.6185622811317444\n",
      "Epoch 591, Loss: 0.6211572289466858, Final Batch Loss: 0.6211572289466858\n",
      "Epoch 592, Loss: 0.6482830047607422, Final Batch Loss: 0.6482830047607422\n",
      "Epoch 593, Loss: 0.5885500907897949, Final Batch Loss: 0.5885500907897949\n",
      "Epoch 594, Loss: 0.6435601711273193, Final Batch Loss: 0.6435601711273193\n",
      "Epoch 595, Loss: 0.64170241355896, Final Batch Loss: 0.64170241355896\n",
      "Epoch 596, Loss: 0.6152619123458862, Final Batch Loss: 0.6152619123458862\n",
      "Epoch 597, Loss: 0.6150749921798706, Final Batch Loss: 0.6150749921798706\n",
      "Epoch 598, Loss: 0.6044939160346985, Final Batch Loss: 0.6044939160346985\n",
      "Epoch 599, Loss: 0.6250275373458862, Final Batch Loss: 0.6250275373458862\n",
      "Epoch 600, Loss: 0.6161282062530518, Final Batch Loss: 0.6161282062530518\n",
      "Epoch 601, Loss: 0.6414037346839905, Final Batch Loss: 0.6414037346839905\n",
      "Epoch 602, Loss: 0.6194679737091064, Final Batch Loss: 0.6194679737091064\n",
      "Epoch 603, Loss: 0.6231650114059448, Final Batch Loss: 0.6231650114059448\n",
      "Epoch 604, Loss: 0.6488321423530579, Final Batch Loss: 0.6488321423530579\n",
      "Epoch 605, Loss: 0.6367447376251221, Final Batch Loss: 0.6367447376251221\n",
      "Epoch 606, Loss: 0.6358886957168579, Final Batch Loss: 0.6358886957168579\n",
      "Epoch 607, Loss: 0.6240416169166565, Final Batch Loss: 0.6240416169166565\n",
      "Epoch 608, Loss: 0.6328085660934448, Final Batch Loss: 0.6328085660934448\n",
      "Epoch 609, Loss: 0.6234817504882812, Final Batch Loss: 0.6234817504882812\n",
      "Epoch 610, Loss: 0.6085630655288696, Final Batch Loss: 0.6085630655288696\n",
      "Epoch 611, Loss: 0.6286437511444092, Final Batch Loss: 0.6286437511444092\n",
      "Epoch 612, Loss: 0.6167734861373901, Final Batch Loss: 0.6167734861373901\n",
      "Epoch 613, Loss: 0.6006031632423401, Final Batch Loss: 0.6006031632423401\n",
      "Epoch 614, Loss: 0.6171022653579712, Final Batch Loss: 0.6171022653579712\n",
      "Epoch 615, Loss: 0.6355783343315125, Final Batch Loss: 0.6355783343315125\n",
      "Epoch 616, Loss: 0.6269477605819702, Final Batch Loss: 0.6269477605819702\n",
      "Epoch 617, Loss: 0.6288029551506042, Final Batch Loss: 0.6288029551506042\n",
      "Epoch 618, Loss: 0.5942949056625366, Final Batch Loss: 0.5942949056625366\n",
      "Epoch 619, Loss: 0.6328349113464355, Final Batch Loss: 0.6328349113464355\n",
      "Epoch 620, Loss: 0.6017035841941833, Final Batch Loss: 0.6017035841941833\n",
      "Epoch 621, Loss: 0.6049033403396606, Final Batch Loss: 0.6049033403396606\n",
      "Epoch 622, Loss: 0.6127234697341919, Final Batch Loss: 0.6127234697341919\n",
      "Epoch 623, Loss: 0.6192116141319275, Final Batch Loss: 0.6192116141319275\n",
      "Epoch 624, Loss: 0.6266811490058899, Final Batch Loss: 0.6266811490058899\n",
      "Epoch 625, Loss: 0.6623432040214539, Final Batch Loss: 0.6623432040214539\n",
      "Epoch 626, Loss: 0.6215906143188477, Final Batch Loss: 0.6215906143188477\n",
      "Epoch 627, Loss: 0.6486909985542297, Final Batch Loss: 0.6486909985542297\n",
      "Epoch 628, Loss: 0.5856035351753235, Final Batch Loss: 0.5856035351753235\n",
      "Epoch 629, Loss: 0.6230548620223999, Final Batch Loss: 0.6230548620223999\n",
      "Epoch 630, Loss: 0.5887833833694458, Final Batch Loss: 0.5887833833694458\n",
      "Epoch 631, Loss: 0.638296365737915, Final Batch Loss: 0.638296365737915\n",
      "Epoch 632, Loss: 0.6222350597381592, Final Batch Loss: 0.6222350597381592\n",
      "Epoch 633, Loss: 0.6031730771064758, Final Batch Loss: 0.6031730771064758\n",
      "Epoch 634, Loss: 0.6361976861953735, Final Batch Loss: 0.6361976861953735\n",
      "Epoch 635, Loss: 0.6238495111465454, Final Batch Loss: 0.6238495111465454\n",
      "Epoch 636, Loss: 0.6056963205337524, Final Batch Loss: 0.6056963205337524\n",
      "Epoch 637, Loss: 0.6099545359611511, Final Batch Loss: 0.6099545359611511\n",
      "Epoch 638, Loss: 0.6111290454864502, Final Batch Loss: 0.6111290454864502\n",
      "Epoch 639, Loss: 0.6228365302085876, Final Batch Loss: 0.6228365302085876\n",
      "Epoch 640, Loss: 0.6465217471122742, Final Batch Loss: 0.6465217471122742\n",
      "Epoch 641, Loss: 0.6302987337112427, Final Batch Loss: 0.6302987337112427\n",
      "Epoch 642, Loss: 0.6218487024307251, Final Batch Loss: 0.6218487024307251\n",
      "Epoch 643, Loss: 0.609992504119873, Final Batch Loss: 0.609992504119873\n",
      "Epoch 644, Loss: 0.6078290939331055, Final Batch Loss: 0.6078290939331055\n",
      "Epoch 645, Loss: 0.6139004230499268, Final Batch Loss: 0.6139004230499268\n",
      "Epoch 646, Loss: 0.6112141013145447, Final Batch Loss: 0.6112141013145447\n",
      "Epoch 647, Loss: 0.627746045589447, Final Batch Loss: 0.627746045589447\n",
      "Epoch 648, Loss: 0.6151111721992493, Final Batch Loss: 0.6151111721992493\n",
      "Epoch 649, Loss: 0.6588385105133057, Final Batch Loss: 0.6588385105133057\n",
      "Epoch 650, Loss: 0.6402647495269775, Final Batch Loss: 0.6402647495269775\n",
      "Epoch 651, Loss: 0.6052742600440979, Final Batch Loss: 0.6052742600440979\n",
      "Epoch 652, Loss: 0.6394574642181396, Final Batch Loss: 0.6394574642181396\n",
      "Epoch 653, Loss: 0.6034963130950928, Final Batch Loss: 0.6034963130950928\n",
      "Epoch 654, Loss: 0.617709219455719, Final Batch Loss: 0.617709219455719\n",
      "Epoch 655, Loss: 0.5868825912475586, Final Batch Loss: 0.5868825912475586\n",
      "Epoch 656, Loss: 0.6160562038421631, Final Batch Loss: 0.6160562038421631\n",
      "Epoch 657, Loss: 0.5964648723602295, Final Batch Loss: 0.5964648723602295\n",
      "Epoch 658, Loss: 0.6137591004371643, Final Batch Loss: 0.6137591004371643\n",
      "Epoch 659, Loss: 0.6114855408668518, Final Batch Loss: 0.6114855408668518\n",
      "Epoch 660, Loss: 0.6162937879562378, Final Batch Loss: 0.6162937879562378\n",
      "Epoch 661, Loss: 0.6055250763893127, Final Batch Loss: 0.6055250763893127\n",
      "Epoch 662, Loss: 0.626128077507019, Final Batch Loss: 0.626128077507019\n",
      "Epoch 663, Loss: 0.5839377045631409, Final Batch Loss: 0.5839377045631409\n",
      "Epoch 664, Loss: 0.632307231426239, Final Batch Loss: 0.632307231426239\n",
      "Epoch 665, Loss: 0.5857837200164795, Final Batch Loss: 0.5857837200164795\n",
      "Epoch 666, Loss: 0.5766174793243408, Final Batch Loss: 0.5766174793243408\n",
      "Epoch 667, Loss: 0.6060997247695923, Final Batch Loss: 0.6060997247695923\n",
      "Epoch 668, Loss: 0.5764695405960083, Final Batch Loss: 0.5764695405960083\n",
      "Epoch 669, Loss: 0.5813599228858948, Final Batch Loss: 0.5813599228858948\n",
      "Epoch 670, Loss: 0.6012608408927917, Final Batch Loss: 0.6012608408927917\n",
      "Epoch 671, Loss: 0.583306610584259, Final Batch Loss: 0.583306610584259\n",
      "Epoch 672, Loss: 0.5995020866394043, Final Batch Loss: 0.5995020866394043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673, Loss: 0.5920640230178833, Final Batch Loss: 0.5920640230178833\n",
      "Epoch 674, Loss: 0.6133078336715698, Final Batch Loss: 0.6133078336715698\n",
      "Epoch 675, Loss: 0.6488122344017029, Final Batch Loss: 0.6488122344017029\n",
      "Epoch 676, Loss: 0.6288819909095764, Final Batch Loss: 0.6288819909095764\n",
      "Epoch 677, Loss: 0.5799027681350708, Final Batch Loss: 0.5799027681350708\n",
      "Epoch 678, Loss: 0.6035512685775757, Final Batch Loss: 0.6035512685775757\n",
      "Epoch 679, Loss: 0.5979938507080078, Final Batch Loss: 0.5979938507080078\n",
      "Epoch 680, Loss: 0.5811524987220764, Final Batch Loss: 0.5811524987220764\n",
      "Epoch 681, Loss: 0.6083056330680847, Final Batch Loss: 0.6083056330680847\n",
      "Epoch 682, Loss: 0.5654834508895874, Final Batch Loss: 0.5654834508895874\n",
      "Epoch 683, Loss: 0.6509070992469788, Final Batch Loss: 0.6509070992469788\n",
      "Epoch 684, Loss: 0.6038361191749573, Final Batch Loss: 0.6038361191749573\n",
      "Epoch 685, Loss: 0.5658625960350037, Final Batch Loss: 0.5658625960350037\n",
      "Epoch 686, Loss: 0.5648903250694275, Final Batch Loss: 0.5648903250694275\n",
      "Epoch 687, Loss: 0.5987883806228638, Final Batch Loss: 0.5987883806228638\n",
      "Epoch 688, Loss: 0.5944259166717529, Final Batch Loss: 0.5944259166717529\n",
      "Epoch 689, Loss: 0.64772629737854, Final Batch Loss: 0.64772629737854\n",
      "Epoch 690, Loss: 0.5664262175559998, Final Batch Loss: 0.5664262175559998\n",
      "Epoch 691, Loss: 0.5557971000671387, Final Batch Loss: 0.5557971000671387\n",
      "Epoch 692, Loss: 0.5605370402336121, Final Batch Loss: 0.5605370402336121\n",
      "Epoch 693, Loss: 0.5958751440048218, Final Batch Loss: 0.5958751440048218\n",
      "Epoch 694, Loss: 0.5670573115348816, Final Batch Loss: 0.5670573115348816\n",
      "Epoch 695, Loss: 0.5962453484535217, Final Batch Loss: 0.5962453484535217\n",
      "Epoch 696, Loss: 0.5921444892883301, Final Batch Loss: 0.5921444892883301\n",
      "Epoch 697, Loss: 0.6034533977508545, Final Batch Loss: 0.6034533977508545\n",
      "Epoch 698, Loss: 0.5501376986503601, Final Batch Loss: 0.5501376986503601\n",
      "Epoch 699, Loss: 0.5833436250686646, Final Batch Loss: 0.5833436250686646\n",
      "Epoch 700, Loss: 0.684478759765625, Final Batch Loss: 0.684478759765625\n",
      "Epoch 701, Loss: 0.5756855010986328, Final Batch Loss: 0.5756855010986328\n",
      "Epoch 702, Loss: 0.5496796369552612, Final Batch Loss: 0.5496796369552612\n",
      "Epoch 703, Loss: 0.5640225410461426, Final Batch Loss: 0.5640225410461426\n",
      "Epoch 704, Loss: 0.5870724320411682, Final Batch Loss: 0.5870724320411682\n",
      "Epoch 705, Loss: 0.5656457543373108, Final Batch Loss: 0.5656457543373108\n",
      "Epoch 706, Loss: 0.5481131672859192, Final Batch Loss: 0.5481131672859192\n",
      "Epoch 707, Loss: 0.5999486446380615, Final Batch Loss: 0.5999486446380615\n",
      "Epoch 708, Loss: 0.6049346923828125, Final Batch Loss: 0.6049346923828125\n",
      "Epoch 709, Loss: 0.6100061535835266, Final Batch Loss: 0.6100061535835266\n",
      "Epoch 710, Loss: 0.5926026701927185, Final Batch Loss: 0.5926026701927185\n",
      "Epoch 711, Loss: 0.5812380313873291, Final Batch Loss: 0.5812380313873291\n",
      "Epoch 712, Loss: 0.5852678418159485, Final Batch Loss: 0.5852678418159485\n",
      "Epoch 713, Loss: 0.5659546256065369, Final Batch Loss: 0.5659546256065369\n",
      "Epoch 714, Loss: 0.5762975811958313, Final Batch Loss: 0.5762975811958313\n",
      "Epoch 715, Loss: 0.584016740322113, Final Batch Loss: 0.584016740322113\n",
      "Epoch 716, Loss: 0.5870663523674011, Final Batch Loss: 0.5870663523674011\n",
      "Epoch 717, Loss: 0.5493156909942627, Final Batch Loss: 0.5493156909942627\n",
      "Epoch 718, Loss: 0.5578243732452393, Final Batch Loss: 0.5578243732452393\n",
      "Epoch 719, Loss: 0.591188371181488, Final Batch Loss: 0.591188371181488\n",
      "Epoch 720, Loss: 0.5850363969802856, Final Batch Loss: 0.5850363969802856\n",
      "Epoch 721, Loss: 0.5528496503829956, Final Batch Loss: 0.5528496503829956\n",
      "Epoch 722, Loss: 0.560523509979248, Final Batch Loss: 0.560523509979248\n",
      "Epoch 723, Loss: 0.5665951371192932, Final Batch Loss: 0.5665951371192932\n",
      "Epoch 724, Loss: 0.5523882508277893, Final Batch Loss: 0.5523882508277893\n",
      "Epoch 725, Loss: 0.5710386633872986, Final Batch Loss: 0.5710386633872986\n",
      "Epoch 726, Loss: 0.557805597782135, Final Batch Loss: 0.557805597782135\n",
      "Epoch 727, Loss: 0.5675535798072815, Final Batch Loss: 0.5675535798072815\n",
      "Epoch 728, Loss: 0.6378887295722961, Final Batch Loss: 0.6378887295722961\n",
      "Epoch 729, Loss: 0.531859815120697, Final Batch Loss: 0.531859815120697\n",
      "Epoch 730, Loss: 0.5529129505157471, Final Batch Loss: 0.5529129505157471\n",
      "Epoch 731, Loss: 0.5459249019622803, Final Batch Loss: 0.5459249019622803\n",
      "Epoch 732, Loss: 0.610795795917511, Final Batch Loss: 0.610795795917511\n",
      "Epoch 733, Loss: 0.5723315477371216, Final Batch Loss: 0.5723315477371216\n",
      "Epoch 734, Loss: 0.5532519221305847, Final Batch Loss: 0.5532519221305847\n",
      "Epoch 735, Loss: 0.586458146572113, Final Batch Loss: 0.586458146572113\n",
      "Epoch 736, Loss: 0.5485281348228455, Final Batch Loss: 0.5485281348228455\n",
      "Epoch 737, Loss: 0.5953661799430847, Final Batch Loss: 0.5953661799430847\n",
      "Epoch 738, Loss: 0.5869256854057312, Final Batch Loss: 0.5869256854057312\n",
      "Epoch 739, Loss: 0.602741539478302, Final Batch Loss: 0.602741539478302\n",
      "Epoch 740, Loss: 0.6009421944618225, Final Batch Loss: 0.6009421944618225\n",
      "Epoch 741, Loss: 0.5665135383605957, Final Batch Loss: 0.5665135383605957\n",
      "Epoch 742, Loss: 0.5634475946426392, Final Batch Loss: 0.5634475946426392\n",
      "Epoch 743, Loss: 0.5480670928955078, Final Batch Loss: 0.5480670928955078\n",
      "Epoch 744, Loss: 0.5546802878379822, Final Batch Loss: 0.5546802878379822\n",
      "Epoch 745, Loss: 0.5559569001197815, Final Batch Loss: 0.5559569001197815\n",
      "Epoch 746, Loss: 0.5592608451843262, Final Batch Loss: 0.5592608451843262\n",
      "Epoch 747, Loss: 0.566511332988739, Final Batch Loss: 0.566511332988739\n",
      "Epoch 748, Loss: 0.5522747039794922, Final Batch Loss: 0.5522747039794922\n",
      "Epoch 749, Loss: 0.5520279407501221, Final Batch Loss: 0.5520279407501221\n",
      "Epoch 750, Loss: 0.5813182592391968, Final Batch Loss: 0.5813182592391968\n",
      "Epoch 751, Loss: 0.5169032216072083, Final Batch Loss: 0.5169032216072083\n",
      "Epoch 752, Loss: 0.5650902986526489, Final Batch Loss: 0.5650902986526489\n",
      "Epoch 753, Loss: 0.5288990139961243, Final Batch Loss: 0.5288990139961243\n",
      "Epoch 754, Loss: 0.5258980393409729, Final Batch Loss: 0.5258980393409729\n",
      "Epoch 755, Loss: 0.5688439011573792, Final Batch Loss: 0.5688439011573792\n",
      "Epoch 756, Loss: 0.5742815136909485, Final Batch Loss: 0.5742815136909485\n",
      "Epoch 757, Loss: 0.5277099013328552, Final Batch Loss: 0.5277099013328552\n",
      "Epoch 758, Loss: 0.5339089035987854, Final Batch Loss: 0.5339089035987854\n",
      "Epoch 759, Loss: 0.5689051747322083, Final Batch Loss: 0.5689051747322083\n",
      "Epoch 760, Loss: 0.5559996366500854, Final Batch Loss: 0.5559996366500854\n",
      "Epoch 761, Loss: 0.5869886875152588, Final Batch Loss: 0.5869886875152588\n",
      "Epoch 762, Loss: 0.5697394013404846, Final Batch Loss: 0.5697394013404846\n",
      "Epoch 763, Loss: 0.5327916145324707, Final Batch Loss: 0.5327916145324707\n",
      "Epoch 764, Loss: 0.5380569696426392, Final Batch Loss: 0.5380569696426392\n",
      "Epoch 765, Loss: 0.5290849208831787, Final Batch Loss: 0.5290849208831787\n",
      "Epoch 766, Loss: 0.6093723177909851, Final Batch Loss: 0.6093723177909851\n",
      "Epoch 767, Loss: 0.5312342643737793, Final Batch Loss: 0.5312342643737793\n",
      "Epoch 768, Loss: 0.5316259860992432, Final Batch Loss: 0.5316259860992432\n",
      "Epoch 769, Loss: 0.5770645141601562, Final Batch Loss: 0.5770645141601562\n",
      "Epoch 770, Loss: 0.5445765256881714, Final Batch Loss: 0.5445765256881714\n",
      "Epoch 771, Loss: 0.5863136649131775, Final Batch Loss: 0.5863136649131775\n",
      "Epoch 772, Loss: 0.5751320719718933, Final Batch Loss: 0.5751320719718933\n",
      "Epoch 773, Loss: 0.5390880703926086, Final Batch Loss: 0.5390880703926086\n",
      "Epoch 774, Loss: 0.5628224015235901, Final Batch Loss: 0.5628224015235901\n",
      "Epoch 775, Loss: 0.5683566331863403, Final Batch Loss: 0.5683566331863403\n",
      "Epoch 776, Loss: 0.5401437878608704, Final Batch Loss: 0.5401437878608704\n",
      "Epoch 777, Loss: 0.5816181302070618, Final Batch Loss: 0.5816181302070618\n",
      "Epoch 778, Loss: 0.550645112991333, Final Batch Loss: 0.550645112991333\n",
      "Epoch 779, Loss: 0.5806741118431091, Final Batch Loss: 0.5806741118431091\n",
      "Epoch 780, Loss: 0.5531123876571655, Final Batch Loss: 0.5531123876571655\n",
      "Epoch 781, Loss: 0.5271642208099365, Final Batch Loss: 0.5271642208099365\n",
      "Epoch 782, Loss: 0.5510736703872681, Final Batch Loss: 0.5510736703872681\n",
      "Epoch 783, Loss: 0.5542548298835754, Final Batch Loss: 0.5542548298835754\n",
      "Epoch 784, Loss: 0.5523403882980347, Final Batch Loss: 0.5523403882980347\n",
      "Epoch 785, Loss: 0.5379762053489685, Final Batch Loss: 0.5379762053489685\n",
      "Epoch 786, Loss: 0.5562732815742493, Final Batch Loss: 0.5562732815742493\n",
      "Epoch 787, Loss: 0.5106601715087891, Final Batch Loss: 0.5106601715087891\n",
      "Epoch 788, Loss: 0.5555282235145569, Final Batch Loss: 0.5555282235145569\n",
      "Epoch 789, Loss: 0.5700729489326477, Final Batch Loss: 0.5700729489326477\n",
      "Epoch 790, Loss: 0.5810365080833435, Final Batch Loss: 0.5810365080833435\n",
      "Epoch 791, Loss: 0.5493654608726501, Final Batch Loss: 0.5493654608726501\n",
      "Epoch 792, Loss: 0.5709883570671082, Final Batch Loss: 0.5709883570671082\n",
      "Epoch 793, Loss: 0.5245044827461243, Final Batch Loss: 0.5245044827461243\n",
      "Epoch 794, Loss: 0.5708496570587158, Final Batch Loss: 0.5708496570587158\n",
      "Epoch 795, Loss: 0.5865222215652466, Final Batch Loss: 0.5865222215652466\n",
      "Epoch 796, Loss: 0.5523961186408997, Final Batch Loss: 0.5523961186408997\n",
      "Epoch 797, Loss: 0.5112267732620239, Final Batch Loss: 0.5112267732620239\n",
      "Epoch 798, Loss: 0.5451075434684753, Final Batch Loss: 0.5451075434684753\n",
      "Epoch 799, Loss: 0.5365824103355408, Final Batch Loss: 0.5365824103355408\n",
      "Epoch 800, Loss: 0.5205667614936829, Final Batch Loss: 0.5205667614936829\n",
      "Epoch 801, Loss: 0.5100541710853577, Final Batch Loss: 0.5100541710853577\n",
      "Epoch 802, Loss: 0.5219577550888062, Final Batch Loss: 0.5219577550888062\n",
      "Epoch 803, Loss: 0.5442726016044617, Final Batch Loss: 0.5442726016044617\n",
      "Epoch 804, Loss: 0.5678629279136658, Final Batch Loss: 0.5678629279136658\n",
      "Epoch 805, Loss: 0.5046253204345703, Final Batch Loss: 0.5046253204345703\n",
      "Epoch 806, Loss: 0.5465406179428101, Final Batch Loss: 0.5465406179428101\n",
      "Epoch 807, Loss: 0.5300156474113464, Final Batch Loss: 0.5300156474113464\n",
      "Epoch 808, Loss: 0.541333019733429, Final Batch Loss: 0.541333019733429\n",
      "Epoch 809, Loss: 0.5070607662200928, Final Batch Loss: 0.5070607662200928\n",
      "Epoch 810, Loss: 0.5016078352928162, Final Batch Loss: 0.5016078352928162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 811, Loss: 0.5236705541610718, Final Batch Loss: 0.5236705541610718\n",
      "Epoch 812, Loss: 0.5198904871940613, Final Batch Loss: 0.5198904871940613\n",
      "Epoch 813, Loss: 0.5164024233818054, Final Batch Loss: 0.5164024233818054\n",
      "Epoch 814, Loss: 0.5083781480789185, Final Batch Loss: 0.5083781480789185\n",
      "Epoch 815, Loss: 0.4874233603477478, Final Batch Loss: 0.4874233603477478\n",
      "Epoch 816, Loss: 0.6120728254318237, Final Batch Loss: 0.6120728254318237\n",
      "Epoch 817, Loss: 0.5623838901519775, Final Batch Loss: 0.5623838901519775\n",
      "Epoch 818, Loss: 0.5083295702934265, Final Batch Loss: 0.5083295702934265\n",
      "Epoch 819, Loss: 0.5788191556930542, Final Batch Loss: 0.5788191556930542\n",
      "Epoch 820, Loss: 0.5307239294052124, Final Batch Loss: 0.5307239294052124\n",
      "Epoch 821, Loss: 0.5185619592666626, Final Batch Loss: 0.5185619592666626\n",
      "Epoch 822, Loss: 0.5045744776725769, Final Batch Loss: 0.5045744776725769\n",
      "Epoch 823, Loss: 0.5168071389198303, Final Batch Loss: 0.5168071389198303\n",
      "Epoch 824, Loss: 0.5653396248817444, Final Batch Loss: 0.5653396248817444\n",
      "Epoch 825, Loss: 0.5231724381446838, Final Batch Loss: 0.5231724381446838\n",
      "Epoch 826, Loss: 0.47823455929756165, Final Batch Loss: 0.47823455929756165\n",
      "Epoch 827, Loss: 0.5048823356628418, Final Batch Loss: 0.5048823356628418\n",
      "Epoch 828, Loss: 0.5059963464736938, Final Batch Loss: 0.5059963464736938\n",
      "Epoch 829, Loss: 0.5275151133537292, Final Batch Loss: 0.5275151133537292\n",
      "Epoch 830, Loss: 0.5373640656471252, Final Batch Loss: 0.5373640656471252\n",
      "Epoch 831, Loss: 0.5265108942985535, Final Batch Loss: 0.5265108942985535\n",
      "Epoch 832, Loss: 0.5561122894287109, Final Batch Loss: 0.5561122894287109\n",
      "Epoch 833, Loss: 0.4953358769416809, Final Batch Loss: 0.4953358769416809\n",
      "Epoch 834, Loss: 0.522602915763855, Final Batch Loss: 0.522602915763855\n",
      "Epoch 835, Loss: 0.5453516244888306, Final Batch Loss: 0.5453516244888306\n",
      "Epoch 836, Loss: 0.4912676215171814, Final Batch Loss: 0.4912676215171814\n",
      "Epoch 837, Loss: 0.5000692009925842, Final Batch Loss: 0.5000692009925842\n",
      "Epoch 838, Loss: 0.49214601516723633, Final Batch Loss: 0.49214601516723633\n",
      "Epoch 839, Loss: 0.4872581362724304, Final Batch Loss: 0.4872581362724304\n",
      "Epoch 840, Loss: 0.5241585969924927, Final Batch Loss: 0.5241585969924927\n",
      "Epoch 841, Loss: 0.5629799365997314, Final Batch Loss: 0.5629799365997314\n",
      "Epoch 842, Loss: 0.5122277736663818, Final Batch Loss: 0.5122277736663818\n",
      "Epoch 843, Loss: 0.4951229691505432, Final Batch Loss: 0.4951229691505432\n",
      "Epoch 844, Loss: 0.5274032950401306, Final Batch Loss: 0.5274032950401306\n",
      "Epoch 845, Loss: 0.5502700209617615, Final Batch Loss: 0.5502700209617615\n",
      "Epoch 846, Loss: 0.5152731537818909, Final Batch Loss: 0.5152731537818909\n",
      "Epoch 847, Loss: 0.5079952478408813, Final Batch Loss: 0.5079952478408813\n",
      "Epoch 848, Loss: 0.5322108864784241, Final Batch Loss: 0.5322108864784241\n",
      "Epoch 849, Loss: 0.5191779732704163, Final Batch Loss: 0.5191779732704163\n",
      "Epoch 850, Loss: 0.5262603163719177, Final Batch Loss: 0.5262603163719177\n",
      "Epoch 851, Loss: 0.48263487219810486, Final Batch Loss: 0.48263487219810486\n",
      "Epoch 852, Loss: 0.5081408619880676, Final Batch Loss: 0.5081408619880676\n",
      "Epoch 853, Loss: 0.5424349308013916, Final Batch Loss: 0.5424349308013916\n",
      "Epoch 854, Loss: 0.5057873725891113, Final Batch Loss: 0.5057873725891113\n",
      "Epoch 855, Loss: 0.48102644085884094, Final Batch Loss: 0.48102644085884094\n",
      "Epoch 856, Loss: 0.4910878837108612, Final Batch Loss: 0.4910878837108612\n",
      "Epoch 857, Loss: 0.47920238971710205, Final Batch Loss: 0.47920238971710205\n",
      "Epoch 858, Loss: 0.5400379300117493, Final Batch Loss: 0.5400379300117493\n",
      "Epoch 859, Loss: 0.5502476692199707, Final Batch Loss: 0.5502476692199707\n",
      "Epoch 860, Loss: 0.518933117389679, Final Batch Loss: 0.518933117389679\n",
      "Epoch 861, Loss: 0.4942072629928589, Final Batch Loss: 0.4942072629928589\n",
      "Epoch 862, Loss: 0.5221933722496033, Final Batch Loss: 0.5221933722496033\n",
      "Epoch 863, Loss: 0.5087535977363586, Final Batch Loss: 0.5087535977363586\n",
      "Epoch 864, Loss: 0.5123406052589417, Final Batch Loss: 0.5123406052589417\n",
      "Epoch 865, Loss: 0.5338931083679199, Final Batch Loss: 0.5338931083679199\n",
      "Epoch 866, Loss: 0.5385029911994934, Final Batch Loss: 0.5385029911994934\n",
      "Epoch 867, Loss: 0.5100752115249634, Final Batch Loss: 0.5100752115249634\n",
      "Epoch 868, Loss: 0.5167008638381958, Final Batch Loss: 0.5167008638381958\n",
      "Epoch 869, Loss: 0.5099359750747681, Final Batch Loss: 0.5099359750747681\n",
      "Epoch 870, Loss: 0.532343327999115, Final Batch Loss: 0.532343327999115\n",
      "Epoch 871, Loss: 0.5280933380126953, Final Batch Loss: 0.5280933380126953\n",
      "Epoch 872, Loss: 0.4815633296966553, Final Batch Loss: 0.4815633296966553\n",
      "Epoch 873, Loss: 0.5191158056259155, Final Batch Loss: 0.5191158056259155\n",
      "Epoch 874, Loss: 0.5117381811141968, Final Batch Loss: 0.5117381811141968\n",
      "Epoch 875, Loss: 0.49271315336227417, Final Batch Loss: 0.49271315336227417\n",
      "Epoch 876, Loss: 0.5073647499084473, Final Batch Loss: 0.5073647499084473\n",
      "Epoch 877, Loss: 0.47043415904045105, Final Batch Loss: 0.47043415904045105\n",
      "Epoch 878, Loss: 0.55096036195755, Final Batch Loss: 0.55096036195755\n",
      "Epoch 879, Loss: 0.4958028197288513, Final Batch Loss: 0.4958028197288513\n",
      "Epoch 880, Loss: 0.5134011507034302, Final Batch Loss: 0.5134011507034302\n",
      "Epoch 881, Loss: 0.5356142520904541, Final Batch Loss: 0.5356142520904541\n",
      "Epoch 882, Loss: 0.5171758532524109, Final Batch Loss: 0.5171758532524109\n",
      "Epoch 883, Loss: 0.4812523126602173, Final Batch Loss: 0.4812523126602173\n",
      "Epoch 884, Loss: 0.4983787536621094, Final Batch Loss: 0.4983787536621094\n",
      "Epoch 885, Loss: 0.5102773904800415, Final Batch Loss: 0.5102773904800415\n",
      "Epoch 886, Loss: 0.5278738141059875, Final Batch Loss: 0.5278738141059875\n",
      "Epoch 887, Loss: 0.49095216393470764, Final Batch Loss: 0.49095216393470764\n",
      "Epoch 888, Loss: 0.5124312043190002, Final Batch Loss: 0.5124312043190002\n",
      "Epoch 889, Loss: 0.5309112668037415, Final Batch Loss: 0.5309112668037415\n",
      "Epoch 890, Loss: 0.48179271817207336, Final Batch Loss: 0.48179271817207336\n",
      "Epoch 891, Loss: 0.5036002397537231, Final Batch Loss: 0.5036002397537231\n",
      "Epoch 892, Loss: 0.48724478483200073, Final Batch Loss: 0.48724478483200073\n",
      "Epoch 893, Loss: 0.5096169710159302, Final Batch Loss: 0.5096169710159302\n",
      "Epoch 894, Loss: 0.5037133097648621, Final Batch Loss: 0.5037133097648621\n",
      "Epoch 895, Loss: 0.5399247407913208, Final Batch Loss: 0.5399247407913208\n",
      "Epoch 896, Loss: 0.48705199360847473, Final Batch Loss: 0.48705199360847473\n",
      "Epoch 897, Loss: 0.48675256967544556, Final Batch Loss: 0.48675256967544556\n",
      "Epoch 898, Loss: 0.48113730549812317, Final Batch Loss: 0.48113730549812317\n",
      "Epoch 899, Loss: 0.4887678921222687, Final Batch Loss: 0.4887678921222687\n",
      "Epoch 900, Loss: 0.48456481099128723, Final Batch Loss: 0.48456481099128723\n",
      "Epoch 901, Loss: 0.4890320897102356, Final Batch Loss: 0.4890320897102356\n",
      "Epoch 902, Loss: 0.4758791923522949, Final Batch Loss: 0.4758791923522949\n",
      "Epoch 903, Loss: 0.5298143029212952, Final Batch Loss: 0.5298143029212952\n",
      "Epoch 904, Loss: 0.4984598159790039, Final Batch Loss: 0.4984598159790039\n",
      "Epoch 905, Loss: 0.4603518545627594, Final Batch Loss: 0.4603518545627594\n",
      "Epoch 906, Loss: 0.4876771569252014, Final Batch Loss: 0.4876771569252014\n",
      "Epoch 907, Loss: 0.5130837559700012, Final Batch Loss: 0.5130837559700012\n",
      "Epoch 908, Loss: 0.5208234786987305, Final Batch Loss: 0.5208234786987305\n",
      "Epoch 909, Loss: 0.5009035468101501, Final Batch Loss: 0.5009035468101501\n",
      "Epoch 910, Loss: 0.4802727699279785, Final Batch Loss: 0.4802727699279785\n",
      "Epoch 911, Loss: 0.46621450781822205, Final Batch Loss: 0.46621450781822205\n",
      "Epoch 912, Loss: 0.49591752886772156, Final Batch Loss: 0.49591752886772156\n",
      "Epoch 913, Loss: 0.5007610321044922, Final Batch Loss: 0.5007610321044922\n",
      "Epoch 914, Loss: 0.49992066621780396, Final Batch Loss: 0.49992066621780396\n",
      "Epoch 915, Loss: 0.4565739035606384, Final Batch Loss: 0.4565739035606384\n",
      "Epoch 916, Loss: 0.48825958371162415, Final Batch Loss: 0.48825958371162415\n",
      "Epoch 917, Loss: 0.45991528034210205, Final Batch Loss: 0.45991528034210205\n",
      "Epoch 918, Loss: 0.49846789240837097, Final Batch Loss: 0.49846789240837097\n",
      "Epoch 919, Loss: 0.4864385426044464, Final Batch Loss: 0.4864385426044464\n",
      "Epoch 920, Loss: 0.4921298027038574, Final Batch Loss: 0.4921298027038574\n",
      "Epoch 921, Loss: 0.47991082072257996, Final Batch Loss: 0.47991082072257996\n",
      "Epoch 922, Loss: 0.49402913451194763, Final Batch Loss: 0.49402913451194763\n",
      "Epoch 923, Loss: 0.5146605968475342, Final Batch Loss: 0.5146605968475342\n",
      "Epoch 924, Loss: 0.48876526951789856, Final Batch Loss: 0.48876526951789856\n",
      "Epoch 925, Loss: 0.49143359065055847, Final Batch Loss: 0.49143359065055847\n",
      "Epoch 926, Loss: 0.4876137971878052, Final Batch Loss: 0.4876137971878052\n",
      "Epoch 927, Loss: 0.5172749161720276, Final Batch Loss: 0.5172749161720276\n",
      "Epoch 928, Loss: 0.4947328269481659, Final Batch Loss: 0.4947328269481659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 929, Loss: 0.5085602402687073, Final Batch Loss: 0.5085602402687073\n",
      "Epoch 930, Loss: 0.4599432945251465, Final Batch Loss: 0.4599432945251465\n",
      "Epoch 931, Loss: 0.5227734446525574, Final Batch Loss: 0.5227734446525574\n",
      "Epoch 932, Loss: 0.518881618976593, Final Batch Loss: 0.518881618976593\n",
      "Epoch 933, Loss: 0.4664376378059387, Final Batch Loss: 0.4664376378059387\n",
      "Epoch 934, Loss: 0.4742336869239807, Final Batch Loss: 0.4742336869239807\n",
      "Epoch 935, Loss: 0.46605536341667175, Final Batch Loss: 0.46605536341667175\n",
      "Epoch 936, Loss: 0.48427248001098633, Final Batch Loss: 0.48427248001098633\n",
      "Epoch 937, Loss: 0.491139680147171, Final Batch Loss: 0.491139680147171\n",
      "Epoch 938, Loss: 0.48199141025543213, Final Batch Loss: 0.48199141025543213\n",
      "Epoch 939, Loss: 0.4862942397594452, Final Batch Loss: 0.4862942397594452\n",
      "Epoch 940, Loss: 0.4766271412372589, Final Batch Loss: 0.4766271412372589\n",
      "Epoch 941, Loss: 0.4814637303352356, Final Batch Loss: 0.4814637303352356\n",
      "Epoch 942, Loss: 0.4813186228275299, Final Batch Loss: 0.4813186228275299\n",
      "Epoch 943, Loss: 0.4738485813140869, Final Batch Loss: 0.4738485813140869\n",
      "Epoch 944, Loss: 0.45334187150001526, Final Batch Loss: 0.45334187150001526\n",
      "Epoch 945, Loss: 0.4873267114162445, Final Batch Loss: 0.4873267114162445\n",
      "Epoch 946, Loss: 0.4717664420604706, Final Batch Loss: 0.4717664420604706\n",
      "Epoch 947, Loss: 0.46894824504852295, Final Batch Loss: 0.46894824504852295\n",
      "Epoch 948, Loss: 0.4747178554534912, Final Batch Loss: 0.4747178554534912\n",
      "Epoch 949, Loss: 0.5042120814323425, Final Batch Loss: 0.5042120814323425\n",
      "Epoch 950, Loss: 0.4438313841819763, Final Batch Loss: 0.4438313841819763\n",
      "Epoch 951, Loss: 0.4654811918735504, Final Batch Loss: 0.4654811918735504\n",
      "Epoch 952, Loss: 0.48903849720954895, Final Batch Loss: 0.48903849720954895\n",
      "Epoch 953, Loss: 0.4763447344303131, Final Batch Loss: 0.4763447344303131\n",
      "Epoch 954, Loss: 0.5281216502189636, Final Batch Loss: 0.5281216502189636\n",
      "Epoch 955, Loss: 0.455899715423584, Final Batch Loss: 0.455899715423584\n",
      "Epoch 956, Loss: 0.45939841866493225, Final Batch Loss: 0.45939841866493225\n",
      "Epoch 957, Loss: 0.498420774936676, Final Batch Loss: 0.498420774936676\n",
      "Epoch 958, Loss: 0.48687586188316345, Final Batch Loss: 0.48687586188316345\n",
      "Epoch 959, Loss: 0.4891802668571472, Final Batch Loss: 0.4891802668571472\n",
      "Epoch 960, Loss: 0.44571205973625183, Final Batch Loss: 0.44571205973625183\n",
      "Epoch 961, Loss: 0.46274906396865845, Final Batch Loss: 0.46274906396865845\n",
      "Epoch 962, Loss: 0.5078887343406677, Final Batch Loss: 0.5078887343406677\n",
      "Epoch 963, Loss: 0.49755313992500305, Final Batch Loss: 0.49755313992500305\n",
      "Epoch 964, Loss: 0.4681982696056366, Final Batch Loss: 0.4681982696056366\n",
      "Epoch 965, Loss: 0.5084458589553833, Final Batch Loss: 0.5084458589553833\n",
      "Epoch 966, Loss: 0.509782075881958, Final Batch Loss: 0.509782075881958\n",
      "Epoch 967, Loss: 0.4527502954006195, Final Batch Loss: 0.4527502954006195\n",
      "Epoch 968, Loss: 0.5117304921150208, Final Batch Loss: 0.5117304921150208\n",
      "Epoch 969, Loss: 0.46562543511390686, Final Batch Loss: 0.46562543511390686\n",
      "Epoch 970, Loss: 0.4842648208141327, Final Batch Loss: 0.4842648208141327\n",
      "Epoch 971, Loss: 0.473003625869751, Final Batch Loss: 0.473003625869751\n",
      "Epoch 972, Loss: 0.46171167492866516, Final Batch Loss: 0.46171167492866516\n",
      "Epoch 973, Loss: 0.4783863425254822, Final Batch Loss: 0.4783863425254822\n",
      "Epoch 974, Loss: 0.4413340091705322, Final Batch Loss: 0.4413340091705322\n",
      "Epoch 975, Loss: 0.4618012309074402, Final Batch Loss: 0.4618012309074402\n",
      "Epoch 976, Loss: 0.46869856119155884, Final Batch Loss: 0.46869856119155884\n",
      "Epoch 977, Loss: 0.43987366557121277, Final Batch Loss: 0.43987366557121277\n",
      "Epoch 978, Loss: 0.5223946571350098, Final Batch Loss: 0.5223946571350098\n",
      "Epoch 979, Loss: 0.46465155482292175, Final Batch Loss: 0.46465155482292175\n",
      "Epoch 980, Loss: 0.45423948764801025, Final Batch Loss: 0.45423948764801025\n",
      "Epoch 981, Loss: 0.4622846841812134, Final Batch Loss: 0.4622846841812134\n",
      "Epoch 982, Loss: 0.48597970604896545, Final Batch Loss: 0.48597970604896545\n",
      "Epoch 983, Loss: 0.43923741579055786, Final Batch Loss: 0.43923741579055786\n",
      "Epoch 984, Loss: 0.47342348098754883, Final Batch Loss: 0.47342348098754883\n",
      "Epoch 985, Loss: 0.4706701934337616, Final Batch Loss: 0.4706701934337616\n",
      "Epoch 986, Loss: 0.47476932406425476, Final Batch Loss: 0.47476932406425476\n",
      "Epoch 987, Loss: 0.4598788321018219, Final Batch Loss: 0.4598788321018219\n",
      "Epoch 988, Loss: 0.515505850315094, Final Batch Loss: 0.515505850315094\n",
      "Epoch 989, Loss: 0.5303370356559753, Final Batch Loss: 0.5303370356559753\n",
      "Epoch 990, Loss: 0.46118679642677307, Final Batch Loss: 0.46118679642677307\n",
      "Epoch 991, Loss: 0.4749557077884674, Final Batch Loss: 0.4749557077884674\n",
      "Epoch 992, Loss: 0.4745979309082031, Final Batch Loss: 0.4745979309082031\n",
      "Epoch 993, Loss: 0.46961739659309387, Final Batch Loss: 0.46961739659309387\n",
      "Epoch 994, Loss: 0.4645824134349823, Final Batch Loss: 0.4645824134349823\n",
      "Epoch 995, Loss: 0.44999200105667114, Final Batch Loss: 0.44999200105667114\n",
      "Epoch 996, Loss: 0.46057161688804626, Final Batch Loss: 0.46057161688804626\n",
      "Epoch 997, Loss: 0.43855252861976624, Final Batch Loss: 0.43855252861976624\n",
      "Epoch 998, Loss: 0.44884493947029114, Final Batch Loss: 0.44884493947029114\n",
      "Epoch 999, Loss: 0.4480458199977875, Final Batch Loss: 0.4480458199977875\n",
      "Epoch 1000, Loss: 0.4480532705783844, Final Batch Loss: 0.4480532705783844\n"
     ]
    }
   ],
   "source": [
    "subject_numbers = list(GAN_data['Subject'].unique()) #list of all unique subject numbers\n",
    "n_subjects = len(subject_numbers) #Number of unique subjects\n",
    "n_iters = 100\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_train_test_split(GAN_data, subject_numbers[0]) #Subject_numbers[0] is a placeholder\n",
    "\n",
    "model, train_loader, test_loader, optimizer, criterion = initialize_params(X_train, y_train, X_test, y_test)\n",
    "#Train model on all user data ONCE\n",
    "model, _ = training_loop(model, train_loader, test_loader, optimizer, criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = np.zeros((n_subjects, n_iters + 1))\n",
    "\n",
    "for k in range(n_subjects):\n",
    "    for j in range(n_iters):\n",
    "        #Get the train test split of the current user (train data is always all users, test data changes)\n",
    "        X_train, y_train, X_test, y_test = get_train_test_split(GAN_data, subject_numbers[k])\n",
    "        #Change the data in the test loader to reflect the current user\n",
    "        _, _, test_loader, _, _ = initialize_params(X_train, y_train, X_test, y_test)\n",
    "        #Get the performance on the current user\n",
    "        f1_score = evaluation(model, test_loader)\n",
    "        #Modify the zero matrix with the current f1 score\n",
    "        all_scores[k, j] = f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55      ,  0.58      ,  0.47619048, ...,  0.65      ,\n",
       "         0.65      ,  1.        ],\n",
       "       [ 0.86      ,  0.86      ,  0.8       , ...,  0.86      ,\n",
       "         0.76666667,  3.        ],\n",
       "       [ 0.73148148,  0.67195767,  0.87830688, ...,  0.73148148,\n",
       "         0.77777778,  5.        ],\n",
       "       ...,\n",
       "       [ 0.62809917,  0.62809917,  0.58181818, ...,  0.62809917,\n",
       "         0.62809917, 28.        ],\n",
       "       [ 0.7       ,  0.69      ,  0.8       , ...,  0.8       ,\n",
       "         0.71666667, 29.        ],\n",
       "       [ 0.62012987,  0.62012987,  0.62012987, ...,  0.62012987,\n",
       "         0.70649351, 30.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(n_subjects):\n",
    "    all_scores[k, -1] = subject_numbers[k]\n",
    "    \n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 'Subject Number']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = list(np.arange(0, n_iters, 1)) + ['Subject Number']\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results saved to All User F-1 Scores.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"../model_outputs/All User F-1 Scores.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile) \n",
    "    csvwriter.writerow(heading)\n",
    "    csvwriter.writerows(all_scores)\n",
    "\n",
    "print(\"Model results saved to All User F-1 Scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
