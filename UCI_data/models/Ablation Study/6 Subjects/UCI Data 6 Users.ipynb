{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '90 tBodyAccJerk-max()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '272 fBodyAcc-mad()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>42 tGravityAcc-mean()-Y</th>\n",
       "      <th>43 tGravityAcc-mean()-Z</th>\n",
       "      <th>51 tGravityAcc-max()-Y</th>\n",
       "      <th>52 tGravityAcc-max()-Z</th>\n",
       "      <th>54 tGravityAcc-min()-Y</th>\n",
       "      <th>55 tGravityAcc-min()-Z</th>\n",
       "      <th>56 tGravityAcc-sma()</th>\n",
       "      <th>58 tGravityAcc-energy()-Y</th>\n",
       "      <th>59 tGravityAcc-energy()-Z</th>\n",
       "      <th>475 fBodyGyro-bandsEnergy()-1,8</th>\n",
       "      <th>...</th>\n",
       "      <th>282 fBodyAcc-energy()-X</th>\n",
       "      <th>303 fBodyAcc-bandsEnergy()-1,8</th>\n",
       "      <th>311 fBodyAcc-bandsEnergy()-1,16</th>\n",
       "      <th>315 fBodyAcc-bandsEnergy()-1,24</th>\n",
       "      <th>382 fBodyAccJerk-bandsEnergy()-1,8</th>\n",
       "      <th>504 fBodyAccMag-std()</th>\n",
       "      <th>505 fBodyAccMag-mad()</th>\n",
       "      <th>509 fBodyAccMag-energy()</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140840</td>\n",
       "      <td>0.115375</td>\n",
       "      <td>-0.161265</td>\n",
       "      <td>0.124660</td>\n",
       "      <td>-0.123213</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>-0.375426</td>\n",
       "      <td>-0.970905</td>\n",
       "      <td>-0.975510</td>\n",
       "      <td>-0.999454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.956134</td>\n",
       "      <td>-0.948870</td>\n",
       "      <td>-0.998285</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141551</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>-0.161343</td>\n",
       "      <td>0.122586</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.383430</td>\n",
       "      <td>-0.970583</td>\n",
       "      <td>-0.978500</td>\n",
       "      <td>-0.999856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-0.975866</td>\n",
       "      <td>-0.975777</td>\n",
       "      <td>-0.999472</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142010</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.094566</td>\n",
       "      <td>-0.114893</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>-0.401602</td>\n",
       "      <td>-0.970368</td>\n",
       "      <td>-0.981672</td>\n",
       "      <td>-0.999954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.999972</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.989015</td>\n",
       "      <td>-0.985594</td>\n",
       "      <td>-0.999807</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143976</td>\n",
       "      <td>0.099850</td>\n",
       "      <td>-0.163711</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>0.095753</td>\n",
       "      <td>-0.400278</td>\n",
       "      <td>-0.969400</td>\n",
       "      <td>-0.982420</td>\n",
       "      <td>-0.999931</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-0.986742</td>\n",
       "      <td>-0.983524</td>\n",
       "      <td>-0.999770</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.148750</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.166786</td>\n",
       "      <td>0.091682</td>\n",
       "      <td>-0.121834</td>\n",
       "      <td>0.094059</td>\n",
       "      <td>-0.400477</td>\n",
       "      <td>-0.967051</td>\n",
       "      <td>-0.984363</td>\n",
       "      <td>-0.999926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-0.990063</td>\n",
       "      <td>-0.992324</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>-0.222004</td>\n",
       "      <td>-0.039492</td>\n",
       "      <td>-0.214233</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.071977</td>\n",
       "      <td>-0.405132</td>\n",
       "      <td>-0.918375</td>\n",
       "      <td>-0.995193</td>\n",
       "      <td>-0.053258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674230</td>\n",
       "      <td>-0.684177</td>\n",
       "      <td>-0.666429</td>\n",
       "      <td>-0.668164</td>\n",
       "      <td>-0.839256</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.007392</td>\n",
       "      <td>-0.584282</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>-0.242054</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>-0.231477</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.234998</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.358934</td>\n",
       "      <td>-0.902880</td>\n",
       "      <td>-0.995151</td>\n",
       "      <td>-0.029411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705580</td>\n",
       "      <td>-0.726986</td>\n",
       "      <td>-0.704444</td>\n",
       "      <td>-0.705435</td>\n",
       "      <td>-0.854278</td>\n",
       "      <td>-0.275373</td>\n",
       "      <td>-0.172448</td>\n",
       "      <td>-0.632536</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>-0.236950</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>-0.249134</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.216004</td>\n",
       "      <td>-0.068919</td>\n",
       "      <td>-0.377025</td>\n",
       "      <td>-0.907561</td>\n",
       "      <td>-0.995450</td>\n",
       "      <td>0.161404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692379</td>\n",
       "      <td>-0.655263</td>\n",
       "      <td>-0.674515</td>\n",
       "      <td>-0.684729</td>\n",
       "      <td>-0.815380</td>\n",
       "      <td>-0.220288</td>\n",
       "      <td>-0.216074</td>\n",
       "      <td>-0.641170</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>-0.233230</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.244267</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>-0.210542</td>\n",
       "      <td>-0.040009</td>\n",
       "      <td>-0.440050</td>\n",
       "      <td>-0.910648</td>\n",
       "      <td>-0.998824</td>\n",
       "      <td>0.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.693098</td>\n",
       "      <td>-0.643425</td>\n",
       "      <td>-0.677215</td>\n",
       "      <td>-0.685088</td>\n",
       "      <td>-0.822905</td>\n",
       "      <td>-0.234539</td>\n",
       "      <td>-0.220443</td>\n",
       "      <td>-0.663579</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>-0.233292</td>\n",
       "      <td>-0.020954</td>\n",
       "      <td>-0.240956</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>-0.212149</td>\n",
       "      <td>-0.047491</td>\n",
       "      <td>-0.432003</td>\n",
       "      <td>-0.910579</td>\n",
       "      <td>-0.998144</td>\n",
       "      <td>-0.129277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.731037</td>\n",
       "      <td>-0.709495</td>\n",
       "      <td>-0.728519</td>\n",
       "      <td>-0.727441</td>\n",
       "      <td>-0.834215</td>\n",
       "      <td>-0.342670</td>\n",
       "      <td>-0.146649</td>\n",
       "      <td>-0.698087</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      42 tGravityAcc-mean()-Y  43 tGravityAcc-mean()-Z  \\\n",
       "0                   -0.140840                 0.115375   \n",
       "1                   -0.141551                 0.109379   \n",
       "2                   -0.142010                 0.101884   \n",
       "3                   -0.143976                 0.099850   \n",
       "4                   -0.148750                 0.094486   \n",
       "...                       ...                      ...   \n",
       "7347                -0.222004                -0.039492   \n",
       "7348                -0.242054                -0.039863   \n",
       "7349                -0.236950                -0.026805   \n",
       "7350                -0.233230                -0.004984   \n",
       "7351                -0.233292                -0.020954   \n",
       "\n",
       "      51 tGravityAcc-max()-Y  52 tGravityAcc-max()-Z  54 tGravityAcc-min()-Y  \\\n",
       "0                  -0.161265                0.124660               -0.123213   \n",
       "1                  -0.161343                0.122586               -0.114893   \n",
       "2                  -0.163711                0.094566               -0.114893   \n",
       "3                  -0.163711                0.093425               -0.121336   \n",
       "4                  -0.166786                0.091682               -0.121834   \n",
       "...                      ...                     ...                     ...   \n",
       "7347               -0.214233               -0.016391               -0.234998   \n",
       "7348               -0.231477               -0.016391               -0.234998   \n",
       "7349               -0.249134                0.024684               -0.216004   \n",
       "7350               -0.244267                0.024684               -0.210542   \n",
       "7351               -0.240956                0.003031               -0.212149   \n",
       "\n",
       "      55 tGravityAcc-min()-Z  56 tGravityAcc-sma()  58 tGravityAcc-energy()-Y  \\\n",
       "0                   0.056483             -0.375426                  -0.970905   \n",
       "1                   0.102764             -0.383430                  -0.970583   \n",
       "2                   0.102764             -0.401602                  -0.970368   \n",
       "3                   0.095753             -0.400278                  -0.969400   \n",
       "4                   0.094059             -0.400477                  -0.967051   \n",
       "...                      ...                   ...                        ...   \n",
       "7347               -0.071977             -0.405132                  -0.918375   \n",
       "7348               -0.068919             -0.358934                  -0.902880   \n",
       "7349               -0.068919             -0.377025                  -0.907561   \n",
       "7350               -0.040009             -0.440050                  -0.910648   \n",
       "7351               -0.047491             -0.432003                  -0.910579   \n",
       "\n",
       "      59 tGravityAcc-energy()-Z  475 fBodyGyro-bandsEnergy()-1,8  ...  \\\n",
       "0                     -0.975510                        -0.999454  ...   \n",
       "1                     -0.978500                        -0.999856  ...   \n",
       "2                     -0.981672                        -0.999954  ...   \n",
       "3                     -0.982420                        -0.999931  ...   \n",
       "4                     -0.984363                        -0.999926  ...   \n",
       "...                         ...                              ...  ...   \n",
       "7347                  -0.995193                        -0.053258  ...   \n",
       "7348                  -0.995151                        -0.029411  ...   \n",
       "7349                  -0.995450                         0.161404  ...   \n",
       "7350                  -0.998824                         0.193585  ...   \n",
       "7351                  -0.998144                        -0.129277  ...   \n",
       "\n",
       "      282 fBodyAcc-energy()-X  303 fBodyAcc-bandsEnergy()-1,8  \\\n",
       "0                   -0.999968                       -0.999963   \n",
       "1                   -0.999991                       -0.999996   \n",
       "2                   -0.999969                       -0.999989   \n",
       "3                   -0.999975                       -0.999989   \n",
       "4                   -0.999990                       -0.999994   \n",
       "...                       ...                             ...   \n",
       "7347                -0.674230                       -0.684177   \n",
       "7348                -0.705580                       -0.726986   \n",
       "7349                -0.692379                       -0.655263   \n",
       "7350                -0.693098                       -0.643425   \n",
       "7351                -0.731037                       -0.709495   \n",
       "\n",
       "      311 fBodyAcc-bandsEnergy()-1,16  315 fBodyAcc-bandsEnergy()-1,24  \\\n",
       "0                           -0.999969                        -0.999971   \n",
       "1                           -0.999994                        -0.999992   \n",
       "2                           -0.999983                        -0.999972   \n",
       "3                           -0.999986                        -0.999977   \n",
       "4                           -0.999993                        -0.999991   \n",
       "...                               ...                              ...   \n",
       "7347                        -0.666429                        -0.668164   \n",
       "7348                        -0.704444                        -0.705435   \n",
       "7349                        -0.674515                        -0.684729   \n",
       "7350                        -0.677215                        -0.685088   \n",
       "7351                        -0.728519                        -0.727441   \n",
       "\n",
       "      382 fBodyAccJerk-bandsEnergy()-1,8  504 fBodyAccMag-std()  \\\n",
       "0                              -0.999986              -0.956134   \n",
       "1                              -0.999996              -0.975866   \n",
       "2                              -0.999994              -0.989015   \n",
       "3                              -0.999998              -0.986742   \n",
       "4                              -0.999995              -0.990063   \n",
       "...                                  ...                    ...   \n",
       "7347                           -0.839256              -0.232600   \n",
       "7348                           -0.854278              -0.275373   \n",
       "7349                           -0.815380              -0.220288   \n",
       "7350                           -0.822905              -0.234539   \n",
       "7351                           -0.834215              -0.342670   \n",
       "\n",
       "      505 fBodyAccMag-mad()  509 fBodyAccMag-energy()  Subject  Activity  \n",
       "0                 -0.948870                 -0.998285        1         5  \n",
       "1                 -0.975777                 -0.999472        1         5  \n",
       "2                 -0.985594                 -0.999807        1         5  \n",
       "3                 -0.983524                 -0.999770        1         5  \n",
       "4                 -0.992324                 -0.999873        1         5  \n",
       "...                     ...                       ...      ...       ...  \n",
       "7347              -0.007392                 -0.584282       30         2  \n",
       "7348              -0.172448                 -0.632536       30         2  \n",
       "7349              -0.216074                 -0.641170       30         2  \n",
       "7350              -0.220443                 -0.663579       30         2  \n",
       "7351              -0.146649                 -0.698087       30         2  \n",
       "\n",
       "[7352 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_names = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "train_column_names = train_names.values.tolist()\n",
    "train_column_names = [k for row in train_column_names for k in row]\n",
    "\n",
    "train_data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None)\n",
    "train_data.columns = train_column_names\n",
    "\n",
    "### Single dataframe column\n",
    "\n",
    "y_train = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "y_train.columns = ['Subject']\n",
    "\n",
    "y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "y_train_activity.columns = ['Activity']\n",
    "\n",
    "X_train_1 = train_data[sub_features]\n",
    "X_train_2 = train_data[act_features]\n",
    "X_train_data = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "\n",
    "X_train_data = pd.concat([X_train_data, y_train, y_train_activity], axis = 1)\n",
    "X_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,:-2].values\n",
    "y_train = X_train_data[(X_train_data['Subject'].isin([1, 3, 5, 7, 8, 11])) & (X_train_data['Activity'].isin([1, 3, 4]))].iloc[:,-2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(y_train)):\n",
    "    if y_train[k] == 1:\n",
    "        y_train[k] = 0\n",
    "    elif y_train[k] == 3:\n",
    "        y_train[k] = 1\n",
    "    elif y_train[k] == 5:\n",
    "        y_train[k] = 2\n",
    "    elif y_train[k] == 7:\n",
    "        y_train[k] = 3\n",
    "    elif y_train[k] == 8:\n",
    "        y_train[k] = 4\n",
    "    else:\n",
    "        y_train[k] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 30),\n",
    "            classifier_block(30, 20),\n",
    "            classifier_block(20, 20),\n",
    "            classifier_block(20, 10),\n",
    "            nn.Linear(10, 6)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 7500\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.2122732400894165, Final Batch Loss: 1.7754740715026855\n",
      "Epoch 2, Loss: 7.201203227043152, Final Batch Loss: 1.7696133852005005\n",
      "Epoch 3, Loss: 7.223655343055725, Final Batch Loss: 1.8082623481750488\n",
      "Epoch 4, Loss: 7.230260729789734, Final Batch Loss: 1.8228994607925415\n",
      "Epoch 5, Loss: 7.220026254653931, Final Batch Loss: 1.8119713068008423\n",
      "Epoch 6, Loss: 7.186437129974365, Final Batch Loss: 1.7772868871688843\n",
      "Epoch 7, Loss: 7.229429125785828, Final Batch Loss: 1.8337804079055786\n",
      "Epoch 8, Loss: 7.187489986419678, Final Batch Loss: 1.7922366857528687\n",
      "Epoch 9, Loss: 7.186165690422058, Final Batch Loss: 1.7947300672531128\n",
      "Epoch 10, Loss: 7.202981472015381, Final Batch Loss: 1.8163419961929321\n",
      "Epoch 11, Loss: 7.171931028366089, Final Batch Loss: 1.7839041948318481\n",
      "Epoch 12, Loss: 7.160185098648071, Final Batch Loss: 1.7748974561691284\n",
      "Epoch 13, Loss: 7.162075996398926, Final Batch Loss: 1.785776138305664\n",
      "Epoch 14, Loss: 7.151643753051758, Final Batch Loss: 1.7837070226669312\n",
      "Epoch 15, Loss: 7.1407623291015625, Final Batch Loss: 1.7847915887832642\n",
      "Epoch 16, Loss: 7.1275941133499146, Final Batch Loss: 1.7698522806167603\n",
      "Epoch 17, Loss: 7.105930209159851, Final Batch Loss: 1.7598332166671753\n",
      "Epoch 18, Loss: 7.100773930549622, Final Batch Loss: 1.7671486139297485\n",
      "Epoch 19, Loss: 7.111149787902832, Final Batch Loss: 1.7934013605117798\n",
      "Epoch 20, Loss: 7.100911498069763, Final Batch Loss: 1.800552487373352\n",
      "Epoch 21, Loss: 7.036898851394653, Final Batch Loss: 1.7370656728744507\n",
      "Epoch 22, Loss: 7.040957808494568, Final Batch Loss: 1.7689539194107056\n",
      "Epoch 23, Loss: 7.014694452285767, Final Batch Loss: 1.74760901927948\n",
      "Epoch 24, Loss: 6.991034030914307, Final Batch Loss: 1.7450448274612427\n",
      "Epoch 25, Loss: 6.934356093406677, Final Batch Loss: 1.7254997491836548\n",
      "Epoch 26, Loss: 6.872297048568726, Final Batch Loss: 1.6934528350830078\n",
      "Epoch 27, Loss: 6.843447804450989, Final Batch Loss: 1.6984764337539673\n",
      "Epoch 28, Loss: 6.832260966300964, Final Batch Loss: 1.7072514295578003\n",
      "Epoch 29, Loss: 6.713291049003601, Final Batch Loss: 1.6140583753585815\n",
      "Epoch 30, Loss: 6.725415349006653, Final Batch Loss: 1.69994056224823\n",
      "Epoch 31, Loss: 6.625887870788574, Final Batch Loss: 1.6479183435440063\n",
      "Epoch 32, Loss: 6.626760959625244, Final Batch Loss: 1.683034062385559\n",
      "Epoch 33, Loss: 6.559456467628479, Final Batch Loss: 1.6781049966812134\n",
      "Epoch 34, Loss: 6.392521023750305, Final Batch Loss: 1.4751638174057007\n",
      "Epoch 35, Loss: 6.4942803382873535, Final Batch Loss: 1.6603797674179077\n",
      "Epoch 36, Loss: 6.399061560630798, Final Batch Loss: 1.6471953392028809\n",
      "Epoch 37, Loss: 6.326308846473694, Final Batch Loss: 1.6482161283493042\n",
      "Epoch 38, Loss: 6.112557411193848, Final Batch Loss: 1.4369072914123535\n",
      "Epoch 39, Loss: 6.112597942352295, Final Batch Loss: 1.5061426162719727\n",
      "Epoch 40, Loss: 6.052250504493713, Final Batch Loss: 1.5256627798080444\n",
      "Epoch 41, Loss: 5.997582197189331, Final Batch Loss: 1.5081018209457397\n",
      "Epoch 42, Loss: 5.857519268989563, Final Batch Loss: 1.4550608396530151\n",
      "Epoch 43, Loss: 5.754952788352966, Final Batch Loss: 1.4020706415176392\n",
      "Epoch 44, Loss: 5.72760272026062, Final Batch Loss: 1.410294532775879\n",
      "Epoch 45, Loss: 5.667667269706726, Final Batch Loss: 1.3466697931289673\n",
      "Epoch 46, Loss: 5.683564305305481, Final Batch Loss: 1.496152400970459\n",
      "Epoch 47, Loss: 5.626603841781616, Final Batch Loss: 1.4666248559951782\n",
      "Epoch 48, Loss: 5.541589975357056, Final Batch Loss: 1.411849021911621\n",
      "Epoch 49, Loss: 5.393085837364197, Final Batch Loss: 1.266098141670227\n",
      "Epoch 50, Loss: 5.380772113800049, Final Batch Loss: 1.3547778129577637\n",
      "Epoch 51, Loss: 5.184552311897278, Final Batch Loss: 1.2133328914642334\n",
      "Epoch 52, Loss: 5.288928389549255, Final Batch Loss: 1.3181633949279785\n",
      "Epoch 53, Loss: 5.134787678718567, Final Batch Loss: 1.2693923711776733\n",
      "Epoch 54, Loss: 5.176890969276428, Final Batch Loss: 1.3516387939453125\n",
      "Epoch 55, Loss: 5.017766952514648, Final Batch Loss: 1.260222315788269\n",
      "Epoch 56, Loss: 4.998446464538574, Final Batch Loss: 1.2781907320022583\n",
      "Epoch 57, Loss: 4.967322587966919, Final Batch Loss: 1.2078180313110352\n",
      "Epoch 58, Loss: 4.940704107284546, Final Batch Loss: 1.2442923784255981\n",
      "Epoch 59, Loss: 4.727153420448303, Final Batch Loss: 1.1185213327407837\n",
      "Epoch 60, Loss: 4.8962671756744385, Final Batch Loss: 1.241620421409607\n",
      "Epoch 61, Loss: 4.82463276386261, Final Batch Loss: 1.2722240686416626\n",
      "Epoch 62, Loss: 4.649655342102051, Final Batch Loss: 1.1237415075302124\n",
      "Epoch 63, Loss: 4.551088929176331, Final Batch Loss: 1.0059164762496948\n",
      "Epoch 64, Loss: 4.6560691595077515, Final Batch Loss: 1.1526981592178345\n",
      "Epoch 65, Loss: 4.525594592094421, Final Batch Loss: 1.038493037223816\n",
      "Epoch 66, Loss: 4.395294725894928, Final Batch Loss: 0.9537963271141052\n",
      "Epoch 67, Loss: 4.581670641899109, Final Batch Loss: 1.2368415594100952\n",
      "Epoch 68, Loss: 4.485877633094788, Final Batch Loss: 1.0849038362503052\n",
      "Epoch 69, Loss: 4.253075480461121, Final Batch Loss: 0.9628369808197021\n",
      "Epoch 70, Loss: 4.37458074092865, Final Batch Loss: 1.0499223470687866\n",
      "Epoch 71, Loss: 4.3629924058914185, Final Batch Loss: 1.0669920444488525\n",
      "Epoch 72, Loss: 4.3080073595047, Final Batch Loss: 1.077436923980713\n",
      "Epoch 73, Loss: 4.329999804496765, Final Batch Loss: 1.1646143198013306\n",
      "Epoch 74, Loss: 4.08511757850647, Final Batch Loss: 0.9581320285797119\n",
      "Epoch 75, Loss: 4.196709990501404, Final Batch Loss: 1.1387380361557007\n",
      "Epoch 76, Loss: 4.089427471160889, Final Batch Loss: 0.8989746570587158\n",
      "Epoch 77, Loss: 4.018098592758179, Final Batch Loss: 0.9251675605773926\n",
      "Epoch 78, Loss: 4.002812087535858, Final Batch Loss: 0.9361560940742493\n",
      "Epoch 79, Loss: 4.120490849018097, Final Batch Loss: 1.0763897895812988\n",
      "Epoch 80, Loss: 3.9161733984947205, Final Batch Loss: 0.8784905076026917\n",
      "Epoch 81, Loss: 3.9478414058685303, Final Batch Loss: 0.9839240908622742\n",
      "Epoch 82, Loss: 3.88081818819046, Final Batch Loss: 0.8724381327629089\n",
      "Epoch 83, Loss: 3.8674495220184326, Final Batch Loss: 0.9058505892753601\n",
      "Epoch 84, Loss: 3.848015785217285, Final Batch Loss: 0.9621007442474365\n",
      "Epoch 85, Loss: 3.6683799624443054, Final Batch Loss: 0.7880178093910217\n",
      "Epoch 86, Loss: 3.832433581352234, Final Batch Loss: 0.9402401447296143\n",
      "Epoch 87, Loss: 3.94178307056427, Final Batch Loss: 1.0954128503799438\n",
      "Epoch 88, Loss: 3.916099965572357, Final Batch Loss: 1.0513077974319458\n",
      "Epoch 89, Loss: 3.7898056507110596, Final Batch Loss: 0.9381799697875977\n",
      "Epoch 90, Loss: 3.6777917742729187, Final Batch Loss: 0.7132441997528076\n",
      "Epoch 91, Loss: 3.7314441800117493, Final Batch Loss: 0.8749787211418152\n",
      "Epoch 92, Loss: 3.7524408102035522, Final Batch Loss: 0.8986697196960449\n",
      "Epoch 93, Loss: 3.8845834732055664, Final Batch Loss: 1.0195649862289429\n",
      "Epoch 94, Loss: 3.7350290417671204, Final Batch Loss: 0.7756320834159851\n",
      "Epoch 95, Loss: 3.733313202857971, Final Batch Loss: 0.810494601726532\n",
      "Epoch 96, Loss: 3.7366607785224915, Final Batch Loss: 0.8057157397270203\n",
      "Epoch 97, Loss: 3.8478262424468994, Final Batch Loss: 0.9783086776733398\n",
      "Epoch 98, Loss: 3.9941062331199646, Final Batch Loss: 1.2048146724700928\n",
      "Epoch 99, Loss: 3.8513524532318115, Final Batch Loss: 1.0549254417419434\n",
      "Epoch 100, Loss: 4.002430260181427, Final Batch Loss: 1.2172688245773315\n",
      "Epoch 101, Loss: 3.643560290336609, Final Batch Loss: 0.8615231513977051\n",
      "Epoch 102, Loss: 3.680765926837921, Final Batch Loss: 0.9316821098327637\n",
      "Epoch 103, Loss: 3.842660129070282, Final Batch Loss: 1.1460949182510376\n",
      "Epoch 104, Loss: 3.613736629486084, Final Batch Loss: 0.8590288162231445\n",
      "Epoch 105, Loss: 3.7452683448791504, Final Batch Loss: 0.9631209373474121\n",
      "Epoch 106, Loss: 3.7078680992126465, Final Batch Loss: 0.9671533107757568\n",
      "Epoch 107, Loss: 3.603958308696747, Final Batch Loss: 0.8859524726867676\n",
      "Epoch 108, Loss: 3.4321975111961365, Final Batch Loss: 0.7494301795959473\n",
      "Epoch 109, Loss: 3.6691725850105286, Final Batch Loss: 0.9995932579040527\n",
      "Epoch 110, Loss: 3.7731131315231323, Final Batch Loss: 0.9679985046386719\n",
      "Epoch 111, Loss: 3.4720656871795654, Final Batch Loss: 0.8484194278717041\n",
      "Epoch 112, Loss: 3.5381150245666504, Final Batch Loss: 0.8241507411003113\n",
      "Epoch 113, Loss: 3.5646902918815613, Final Batch Loss: 0.8015744686126709\n",
      "Epoch 114, Loss: 3.4905367493629456, Final Batch Loss: 0.7331088185310364\n",
      "Epoch 115, Loss: 3.463469088077545, Final Batch Loss: 0.7969312071800232\n",
      "Epoch 116, Loss: 3.3737640380859375, Final Batch Loss: 0.7325740456581116\n",
      "Epoch 117, Loss: 3.372615337371826, Final Batch Loss: 0.871191680431366\n",
      "Epoch 118, Loss: 3.6311322450637817, Final Batch Loss: 0.9797781109809875\n",
      "Epoch 119, Loss: 3.521481931209564, Final Batch Loss: 0.86741042137146\n",
      "Epoch 120, Loss: 3.376299738883972, Final Batch Loss: 0.7538444995880127\n",
      "Epoch 121, Loss: 3.500772535800934, Final Batch Loss: 0.811478316783905\n",
      "Epoch 122, Loss: 3.401996076107025, Final Batch Loss: 0.7336446642875671\n",
      "Epoch 123, Loss: 3.5752018094062805, Final Batch Loss: 0.953859269618988\n",
      "Epoch 124, Loss: 3.4680115580558777, Final Batch Loss: 0.8051402568817139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 3.418674111366272, Final Batch Loss: 0.8852679133415222\n",
      "Epoch 126, Loss: 3.465124011039734, Final Batch Loss: 0.8655560612678528\n",
      "Epoch 127, Loss: 3.485266327857971, Final Batch Loss: 0.8692747950553894\n",
      "Epoch 128, Loss: 3.3785840272903442, Final Batch Loss: 0.8138038516044617\n",
      "Epoch 129, Loss: 3.5379728078842163, Final Batch Loss: 0.874905526638031\n",
      "Epoch 130, Loss: 3.5189709067344666, Final Batch Loss: 0.9454662799835205\n",
      "Epoch 131, Loss: 3.281304955482483, Final Batch Loss: 0.6732514500617981\n",
      "Epoch 132, Loss: 3.525645136833191, Final Batch Loss: 0.8852221369743347\n",
      "Epoch 133, Loss: 3.5156440138816833, Final Batch Loss: 0.9719145894050598\n",
      "Epoch 134, Loss: 3.2105689644813538, Final Batch Loss: 0.5852248072624207\n",
      "Epoch 135, Loss: 3.3640941381454468, Final Batch Loss: 0.7972886562347412\n",
      "Epoch 136, Loss: 3.2427006363868713, Final Batch Loss: 0.7599804997444153\n",
      "Epoch 137, Loss: 3.2162252068519592, Final Batch Loss: 0.6856860518455505\n",
      "Epoch 138, Loss: 3.3459941148757935, Final Batch Loss: 0.8057329058647156\n",
      "Epoch 139, Loss: 3.4168803691864014, Final Batch Loss: 0.8524889945983887\n",
      "Epoch 140, Loss: 3.221669614315033, Final Batch Loss: 0.7694048285484314\n",
      "Epoch 141, Loss: 3.347226560115814, Final Batch Loss: 0.9138192534446716\n",
      "Epoch 142, Loss: 3.403220057487488, Final Batch Loss: 0.8219177722930908\n",
      "Epoch 143, Loss: 3.3660301566123962, Final Batch Loss: 0.8628970980644226\n",
      "Epoch 144, Loss: 3.35275000333786, Final Batch Loss: 0.7584133744239807\n",
      "Epoch 145, Loss: 3.4692224264144897, Final Batch Loss: 0.9509771466255188\n",
      "Epoch 146, Loss: 3.39927738904953, Final Batch Loss: 0.9256672263145447\n",
      "Epoch 147, Loss: 3.22389155626297, Final Batch Loss: 0.6472877860069275\n",
      "Epoch 148, Loss: 3.2021461725234985, Final Batch Loss: 0.7411314845085144\n",
      "Epoch 149, Loss: 3.190531313419342, Final Batch Loss: 0.8483906388282776\n",
      "Epoch 150, Loss: 3.3023616671562195, Final Batch Loss: 0.8374309539794922\n",
      "Epoch 151, Loss: 3.2251542806625366, Final Batch Loss: 0.7975633144378662\n",
      "Epoch 152, Loss: 3.1778594255447388, Final Batch Loss: 0.6360782980918884\n",
      "Epoch 153, Loss: 3.100645959377289, Final Batch Loss: 0.6069155335426331\n",
      "Epoch 154, Loss: 3.2299463748931885, Final Batch Loss: 0.7867513298988342\n",
      "Epoch 155, Loss: 3.185010850429535, Final Batch Loss: 0.8368792533874512\n",
      "Epoch 156, Loss: 3.182853639125824, Final Batch Loss: 0.8006144165992737\n",
      "Epoch 157, Loss: 3.1139045357704163, Final Batch Loss: 0.6314077973365784\n",
      "Epoch 158, Loss: 3.298510730266571, Final Batch Loss: 0.8969720005989075\n",
      "Epoch 159, Loss: 3.3052560091018677, Final Batch Loss: 0.9339000582695007\n",
      "Epoch 160, Loss: 3.196349561214447, Final Batch Loss: 0.6958102583885193\n",
      "Epoch 161, Loss: 3.154181480407715, Final Batch Loss: 0.7568792700767517\n",
      "Epoch 162, Loss: 3.2571876645088196, Final Batch Loss: 0.8291298747062683\n",
      "Epoch 163, Loss: 3.177704334259033, Final Batch Loss: 0.7455112338066101\n",
      "Epoch 164, Loss: 3.0809677243232727, Final Batch Loss: 0.6506898999214172\n",
      "Epoch 165, Loss: 3.1534674763679504, Final Batch Loss: 0.7073686122894287\n",
      "Epoch 166, Loss: 3.200923502445221, Final Batch Loss: 0.8390280604362488\n",
      "Epoch 167, Loss: 3.2234312891960144, Final Batch Loss: 0.8755364418029785\n",
      "Epoch 168, Loss: 3.167521893978119, Final Batch Loss: 0.7573065757751465\n",
      "Epoch 169, Loss: 3.265334725379944, Final Batch Loss: 0.8587271571159363\n",
      "Epoch 170, Loss: 3.0636999011039734, Final Batch Loss: 0.7066642642021179\n",
      "Epoch 171, Loss: 3.1550883054733276, Final Batch Loss: 0.7953920364379883\n",
      "Epoch 172, Loss: 2.9551252722740173, Final Batch Loss: 0.6023767590522766\n",
      "Epoch 173, Loss: 3.2297565937042236, Final Batch Loss: 0.8441057801246643\n",
      "Epoch 174, Loss: 3.1544713377952576, Final Batch Loss: 0.790983259677887\n",
      "Epoch 175, Loss: 3.1275487542152405, Final Batch Loss: 0.8236597180366516\n",
      "Epoch 176, Loss: 2.95478492975235, Final Batch Loss: 0.5920833945274353\n",
      "Epoch 177, Loss: 3.258263111114502, Final Batch Loss: 0.9597219824790955\n",
      "Epoch 178, Loss: 3.219251573085785, Final Batch Loss: 0.7775695323944092\n",
      "Epoch 179, Loss: 3.173151433467865, Final Batch Loss: 0.788398802280426\n",
      "Epoch 180, Loss: 3.2132793068885803, Final Batch Loss: 0.9117734432220459\n",
      "Epoch 181, Loss: 3.1696831583976746, Final Batch Loss: 0.9090366959571838\n",
      "Epoch 182, Loss: 2.823781728744507, Final Batch Loss: 0.568462610244751\n",
      "Epoch 183, Loss: 3.0609620213508606, Final Batch Loss: 0.7795166373252869\n",
      "Epoch 184, Loss: 2.914007544517517, Final Batch Loss: 0.5981475710868835\n",
      "Epoch 185, Loss: 2.831591695547104, Final Batch Loss: 0.4999707043170929\n",
      "Epoch 186, Loss: 3.0567545890808105, Final Batch Loss: 0.6872715353965759\n",
      "Epoch 187, Loss: 3.2743935585021973, Final Batch Loss: 0.977081298828125\n",
      "Epoch 188, Loss: 2.999305248260498, Final Batch Loss: 0.7286515831947327\n",
      "Epoch 189, Loss: 3.1603970527648926, Final Batch Loss: 0.8836180567741394\n",
      "Epoch 190, Loss: 3.033288359642029, Final Batch Loss: 0.7395851612091064\n",
      "Epoch 191, Loss: 2.9123822450637817, Final Batch Loss: 0.6391804814338684\n",
      "Epoch 192, Loss: 3.112034797668457, Final Batch Loss: 0.8673222661018372\n",
      "Epoch 193, Loss: 2.825718879699707, Final Batch Loss: 0.5551928877830505\n",
      "Epoch 194, Loss: 2.8238818049430847, Final Batch Loss: 0.6268709301948547\n",
      "Epoch 195, Loss: 2.9384577870368958, Final Batch Loss: 0.7313112616539001\n",
      "Epoch 196, Loss: 3.021321654319763, Final Batch Loss: 0.7495114207267761\n",
      "Epoch 197, Loss: 2.8961321115493774, Final Batch Loss: 0.6601623296737671\n",
      "Epoch 198, Loss: 3.020611822605133, Final Batch Loss: 0.7393388748168945\n",
      "Epoch 199, Loss: 3.086195230484009, Final Batch Loss: 0.8182852268218994\n",
      "Epoch 200, Loss: 3.283939778804779, Final Batch Loss: 1.0346161127090454\n",
      "Epoch 201, Loss: 2.837732255458832, Final Batch Loss: 0.6097294688224792\n",
      "Epoch 202, Loss: 3.005718767642975, Final Batch Loss: 0.6990361213684082\n",
      "Epoch 203, Loss: 3.1230944395065308, Final Batch Loss: 0.903465986251831\n",
      "Epoch 204, Loss: 2.7661075592041016, Final Batch Loss: 0.48745179176330566\n",
      "Epoch 205, Loss: 3.050602376461029, Final Batch Loss: 0.7957964539527893\n",
      "Epoch 206, Loss: 3.1300572752952576, Final Batch Loss: 0.9483534693717957\n",
      "Epoch 207, Loss: 3.012111186981201, Final Batch Loss: 0.7930837273597717\n",
      "Epoch 208, Loss: 2.8320754766464233, Final Batch Loss: 0.6173243522644043\n",
      "Epoch 209, Loss: 3.0006012320518494, Final Batch Loss: 0.7606473565101624\n",
      "Epoch 210, Loss: 2.9741461873054504, Final Batch Loss: 0.7449015974998474\n",
      "Epoch 211, Loss: 2.898807466030121, Final Batch Loss: 0.6038612723350525\n",
      "Epoch 212, Loss: 2.856445014476776, Final Batch Loss: 0.6745256781578064\n",
      "Epoch 213, Loss: 2.887920618057251, Final Batch Loss: 0.6877991557121277\n",
      "Epoch 214, Loss: 3.043944299221039, Final Batch Loss: 0.9226393103599548\n",
      "Epoch 215, Loss: 2.863838851451874, Final Batch Loss: 0.7304556369781494\n",
      "Epoch 216, Loss: 2.9700594544410706, Final Batch Loss: 0.7524281144142151\n",
      "Epoch 217, Loss: 2.886806070804596, Final Batch Loss: 0.7798431515693665\n",
      "Epoch 218, Loss: 2.859472095966339, Final Batch Loss: 0.6855887770652771\n",
      "Epoch 219, Loss: 3.053028166294098, Final Batch Loss: 0.7674528956413269\n",
      "Epoch 220, Loss: 2.7406797409057617, Final Batch Loss: 0.6414969563484192\n",
      "Epoch 221, Loss: 2.770731806755066, Final Batch Loss: 0.6250283122062683\n",
      "Epoch 222, Loss: 2.96752005815506, Final Batch Loss: 0.8998550772666931\n",
      "Epoch 223, Loss: 2.9830583333969116, Final Batch Loss: 0.8512637615203857\n",
      "Epoch 224, Loss: 2.9432759881019592, Final Batch Loss: 0.7225280404090881\n",
      "Epoch 225, Loss: 3.1110554337501526, Final Batch Loss: 0.9410561919212341\n",
      "Epoch 226, Loss: 2.70929217338562, Final Batch Loss: 0.6075348258018494\n",
      "Epoch 227, Loss: 2.7396491169929504, Final Batch Loss: 0.620254635810852\n",
      "Epoch 228, Loss: 2.6687633395195007, Final Batch Loss: 0.534935474395752\n",
      "Epoch 229, Loss: 2.9288511276245117, Final Batch Loss: 0.8077334761619568\n",
      "Epoch 230, Loss: 2.8687551021575928, Final Batch Loss: 0.7634890079498291\n",
      "Epoch 231, Loss: 2.6996448040008545, Final Batch Loss: 0.6265950798988342\n",
      "Epoch 232, Loss: 2.7987142205238342, Final Batch Loss: 0.6144051551818848\n",
      "Epoch 233, Loss: 2.8092252016067505, Final Batch Loss: 0.703677237033844\n",
      "Epoch 234, Loss: 2.86217337846756, Final Batch Loss: 0.7800226211547852\n",
      "Epoch 235, Loss: 2.7874492406845093, Final Batch Loss: 0.7267913222312927\n",
      "Epoch 236, Loss: 2.7852442860603333, Final Batch Loss: 0.7099531292915344\n",
      "Epoch 237, Loss: 2.7790655493736267, Final Batch Loss: 0.6837763786315918\n",
      "Epoch 238, Loss: 2.938722789287567, Final Batch Loss: 0.8075091242790222\n",
      "Epoch 239, Loss: 2.835758686065674, Final Batch Loss: 0.7727449536323547\n",
      "Epoch 240, Loss: 2.7063387632369995, Final Batch Loss: 0.620127260684967\n",
      "Epoch 241, Loss: 2.6277599334716797, Final Batch Loss: 0.4749763011932373\n",
      "Epoch 242, Loss: 2.7862623929977417, Final Batch Loss: 0.7709925174713135\n",
      "Epoch 243, Loss: 2.543972760438919, Final Batch Loss: 0.46380236744880676\n",
      "Epoch 244, Loss: 2.972178816795349, Final Batch Loss: 0.8734645843505859\n",
      "Epoch 245, Loss: 2.6967852115631104, Final Batch Loss: 0.6842144131660461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246, Loss: 2.726570963859558, Final Batch Loss: 0.6323999166488647\n",
      "Epoch 247, Loss: 2.9163171648979187, Final Batch Loss: 0.8442924618721008\n",
      "Epoch 248, Loss: 2.7927886843681335, Final Batch Loss: 0.7453708648681641\n",
      "Epoch 249, Loss: 2.842215836048126, Final Batch Loss: 0.7729714512825012\n",
      "Epoch 250, Loss: 2.817236006259918, Final Batch Loss: 0.808158814907074\n",
      "Epoch 251, Loss: 2.6408957839012146, Final Batch Loss: 0.5900532603263855\n",
      "Epoch 252, Loss: 2.725791096687317, Final Batch Loss: 0.6410121917724609\n",
      "Epoch 253, Loss: 2.8559731245040894, Final Batch Loss: 0.7744452357292175\n",
      "Epoch 254, Loss: 2.689640164375305, Final Batch Loss: 0.5972920060157776\n",
      "Epoch 255, Loss: 2.468200922012329, Final Batch Loss: 0.4051171541213989\n",
      "Epoch 256, Loss: 2.7879785895347595, Final Batch Loss: 0.7357255816459656\n",
      "Epoch 257, Loss: 2.7309444546699524, Final Batch Loss: 0.632167398929596\n",
      "Epoch 258, Loss: 2.6822787523269653, Final Batch Loss: 0.644807755947113\n",
      "Epoch 259, Loss: 2.7627103328704834, Final Batch Loss: 0.6765682101249695\n",
      "Epoch 260, Loss: 2.646707057952881, Final Batch Loss: 0.5268579125404358\n",
      "Epoch 261, Loss: 2.6443390250205994, Final Batch Loss: 0.6081870198249817\n",
      "Epoch 262, Loss: 2.6705963611602783, Final Batch Loss: 0.5653652548789978\n",
      "Epoch 263, Loss: 2.746711552143097, Final Batch Loss: 0.6288972496986389\n",
      "Epoch 264, Loss: 2.7286481261253357, Final Batch Loss: 0.6770520806312561\n",
      "Epoch 265, Loss: 2.637075901031494, Final Batch Loss: 0.7177574038505554\n",
      "Epoch 266, Loss: 2.616060733795166, Final Batch Loss: 0.6286147236824036\n",
      "Epoch 267, Loss: 2.6909916400909424, Final Batch Loss: 0.6866433620452881\n",
      "Epoch 268, Loss: 2.728499710559845, Final Batch Loss: 0.75713711977005\n",
      "Epoch 269, Loss: 2.721708059310913, Final Batch Loss: 0.7111751437187195\n",
      "Epoch 270, Loss: 2.58130419254303, Final Batch Loss: 0.568244993686676\n",
      "Epoch 271, Loss: 2.846895456314087, Final Batch Loss: 0.8089852929115295\n",
      "Epoch 272, Loss: 2.7167659997940063, Final Batch Loss: 0.7788443565368652\n",
      "Epoch 273, Loss: 2.629656136035919, Final Batch Loss: 0.6194050908088684\n",
      "Epoch 274, Loss: 2.668335437774658, Final Batch Loss: 0.7076930999755859\n",
      "Epoch 275, Loss: 2.5550103783607483, Final Batch Loss: 0.6373216509819031\n",
      "Epoch 276, Loss: 2.614704191684723, Final Batch Loss: 0.6448802947998047\n",
      "Epoch 277, Loss: 2.5476250052452087, Final Batch Loss: 0.5790046453475952\n",
      "Epoch 278, Loss: 2.6464452743530273, Final Batch Loss: 0.5724270343780518\n",
      "Epoch 279, Loss: 2.5698041319847107, Final Batch Loss: 0.5866268873214722\n",
      "Epoch 280, Loss: 2.5785216689109802, Final Batch Loss: 0.5736165046691895\n",
      "Epoch 281, Loss: 2.533759832382202, Final Batch Loss: 0.5668389797210693\n",
      "Epoch 282, Loss: 2.666059374809265, Final Batch Loss: 0.6988203525543213\n",
      "Epoch 283, Loss: 2.475725680589676, Final Batch Loss: 0.49874016642570496\n",
      "Epoch 284, Loss: 2.614815890789032, Final Batch Loss: 0.726828396320343\n",
      "Epoch 285, Loss: 2.7151284217834473, Final Batch Loss: 0.7116060853004456\n",
      "Epoch 286, Loss: 2.7110028862953186, Final Batch Loss: 0.7537021636962891\n",
      "Epoch 287, Loss: 2.6311452388763428, Final Batch Loss: 0.6662472486495972\n",
      "Epoch 288, Loss: 2.393585056066513, Final Batch Loss: 0.45780661702156067\n",
      "Epoch 289, Loss: 2.4562692046165466, Final Batch Loss: 0.6020435690879822\n",
      "Epoch 290, Loss: 2.5356112718582153, Final Batch Loss: 0.6176847815513611\n",
      "Epoch 291, Loss: 2.587819993495941, Final Batch Loss: 0.6575145125389099\n",
      "Epoch 292, Loss: 2.403938889503479, Final Batch Loss: 0.574787437915802\n",
      "Epoch 293, Loss: 2.654455006122589, Final Batch Loss: 0.6458614468574524\n",
      "Epoch 294, Loss: 2.4301547408103943, Final Batch Loss: 0.501945436000824\n",
      "Epoch 295, Loss: 2.416776716709137, Final Batch Loss: 0.518319845199585\n",
      "Epoch 296, Loss: 2.537694275379181, Final Batch Loss: 0.6176671385765076\n",
      "Epoch 297, Loss: 2.4467146396636963, Final Batch Loss: 0.6354162096977234\n",
      "Epoch 298, Loss: 2.625178873538971, Final Batch Loss: 0.7898322939872742\n",
      "Epoch 299, Loss: 2.8239861726760864, Final Batch Loss: 0.8150014281272888\n",
      "Epoch 300, Loss: 2.614412486553192, Final Batch Loss: 0.7453717589378357\n",
      "Epoch 301, Loss: 2.478420853614807, Final Batch Loss: 0.6607552170753479\n",
      "Epoch 302, Loss: 2.463557720184326, Final Batch Loss: 0.6697640419006348\n",
      "Epoch 303, Loss: 2.469305992126465, Final Batch Loss: 0.6062377095222473\n",
      "Epoch 304, Loss: 2.7127259969711304, Final Batch Loss: 0.7212931513786316\n",
      "Epoch 305, Loss: 2.4797106981277466, Final Batch Loss: 0.6215240359306335\n",
      "Epoch 306, Loss: 2.484639346599579, Final Batch Loss: 0.5479816794395447\n",
      "Epoch 307, Loss: 2.4445191621780396, Final Batch Loss: 0.5609250664710999\n",
      "Epoch 308, Loss: 2.5389456748962402, Final Batch Loss: 0.6316571235656738\n",
      "Epoch 309, Loss: 2.4949158430099487, Final Batch Loss: 0.5985917448997498\n",
      "Epoch 310, Loss: 2.6625418066978455, Final Batch Loss: 0.6496487259864807\n",
      "Epoch 311, Loss: 2.5854305028915405, Final Batch Loss: 0.6799859404563904\n",
      "Epoch 312, Loss: 2.490088164806366, Final Batch Loss: 0.622742772102356\n",
      "Epoch 313, Loss: 2.3077451288700104, Final Batch Loss: 0.3644823133945465\n",
      "Epoch 314, Loss: 2.419961452484131, Final Batch Loss: 0.5824263095855713\n",
      "Epoch 315, Loss: 2.392405480146408, Final Batch Loss: 0.485863596200943\n",
      "Epoch 316, Loss: 2.386556625366211, Final Batch Loss: 0.5707635283470154\n",
      "Epoch 317, Loss: 2.4360352754592896, Final Batch Loss: 0.5440087914466858\n",
      "Epoch 318, Loss: 2.311497300863266, Final Batch Loss: 0.4441284239292145\n",
      "Epoch 319, Loss: 2.52767550945282, Final Batch Loss: 0.7091591358184814\n",
      "Epoch 320, Loss: 2.425571620464325, Final Batch Loss: 0.6142417788505554\n",
      "Epoch 321, Loss: 2.4944236874580383, Final Batch Loss: 0.6256027221679688\n",
      "Epoch 322, Loss: 2.516291320323944, Final Batch Loss: 0.6679075360298157\n",
      "Epoch 323, Loss: 2.3193316757678986, Final Batch Loss: 0.4921240508556366\n",
      "Epoch 324, Loss: 2.4862703680992126, Final Batch Loss: 0.5053677558898926\n",
      "Epoch 325, Loss: 2.6100016832351685, Final Batch Loss: 0.806607723236084\n",
      "Epoch 326, Loss: 2.580357074737549, Final Batch Loss: 0.6541367769241333\n",
      "Epoch 327, Loss: 2.714489221572876, Final Batch Loss: 0.867074191570282\n",
      "Epoch 328, Loss: 2.458276689052582, Final Batch Loss: 0.6335365772247314\n",
      "Epoch 329, Loss: 2.2758113741874695, Final Batch Loss: 0.5275806188583374\n",
      "Epoch 330, Loss: 2.663644850254059, Final Batch Loss: 0.8611482977867126\n",
      "Epoch 331, Loss: 2.74179744720459, Final Batch Loss: 0.9294807314872742\n",
      "Epoch 332, Loss: 2.5458900928497314, Final Batch Loss: 0.6297658085823059\n",
      "Epoch 333, Loss: 2.6135276556015015, Final Batch Loss: 0.8046055436134338\n",
      "Epoch 334, Loss: 2.567085921764374, Final Batch Loss: 0.721641480922699\n",
      "Epoch 335, Loss: 2.3370354771614075, Final Batch Loss: 0.6080336570739746\n",
      "Epoch 336, Loss: 2.4548980593681335, Final Batch Loss: 0.64280766248703\n",
      "Epoch 337, Loss: 2.39740788936615, Final Batch Loss: 0.581421434879303\n",
      "Epoch 338, Loss: 2.393890619277954, Final Batch Loss: 0.5189523696899414\n",
      "Epoch 339, Loss: 2.296770751476288, Final Batch Loss: 0.5240989327430725\n",
      "Epoch 340, Loss: 2.308579832315445, Final Batch Loss: 0.49264904856681824\n",
      "Epoch 341, Loss: 2.3341984152793884, Final Batch Loss: 0.6520968675613403\n",
      "Epoch 342, Loss: 2.273747146129608, Final Batch Loss: 0.5222864151000977\n",
      "Epoch 343, Loss: 2.4359148144721985, Final Batch Loss: 0.7227461338043213\n",
      "Epoch 344, Loss: 2.5293127298355103, Final Batch Loss: 0.7022204399108887\n",
      "Epoch 345, Loss: 2.453021466732025, Final Batch Loss: 0.7116088271141052\n",
      "Epoch 346, Loss: 2.38317608833313, Final Batch Loss: 0.5906124711036682\n",
      "Epoch 347, Loss: 2.494301497936249, Final Batch Loss: 0.6861028075218201\n",
      "Epoch 348, Loss: 2.3635876774787903, Final Batch Loss: 0.6430033445358276\n",
      "Epoch 349, Loss: 2.44575434923172, Final Batch Loss: 0.644151508808136\n",
      "Epoch 350, Loss: 2.1292471289634705, Final Batch Loss: 0.468916654586792\n",
      "Epoch 351, Loss: 2.414700984954834, Final Batch Loss: 0.6775400638580322\n",
      "Epoch 352, Loss: 2.3300551772117615, Final Batch Loss: 0.6059715151786804\n",
      "Epoch 353, Loss: 2.3549294769763947, Final Batch Loss: 0.48367324471473694\n",
      "Epoch 354, Loss: 2.3300497233867645, Final Batch Loss: 0.4733025133609772\n",
      "Epoch 355, Loss: 2.213592678308487, Final Batch Loss: 0.4918613135814667\n",
      "Epoch 356, Loss: 2.31144842505455, Final Batch Loss: 0.5855901837348938\n",
      "Epoch 357, Loss: 2.4215216040611267, Final Batch Loss: 0.6085166335105896\n",
      "Epoch 358, Loss: 2.3147000074386597, Final Batch Loss: 0.6345289349555969\n",
      "Epoch 359, Loss: 2.3541361689567566, Final Batch Loss: 0.6891481876373291\n",
      "Epoch 360, Loss: 2.22464582324028, Final Batch Loss: 0.49449750781059265\n",
      "Epoch 361, Loss: 2.2890829741954803, Final Batch Loss: 0.4799307882785797\n",
      "Epoch 362, Loss: 2.205545127391815, Final Batch Loss: 0.5219963192939758\n",
      "Epoch 363, Loss: 2.2566083669662476, Final Batch Loss: 0.5407195687294006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364, Loss: 2.4191601872444153, Final Batch Loss: 0.7557616233825684\n",
      "Epoch 365, Loss: 2.2928234934806824, Final Batch Loss: 0.5730604529380798\n",
      "Epoch 366, Loss: 2.432244837284088, Final Batch Loss: 0.6694057583808899\n",
      "Epoch 367, Loss: 2.24878191947937, Final Batch Loss: 0.41325342655181885\n",
      "Epoch 368, Loss: 2.4781416058540344, Final Batch Loss: 0.749946117401123\n",
      "Epoch 369, Loss: 2.4366230964660645, Final Batch Loss: 0.6932472586631775\n",
      "Epoch 370, Loss: 2.512822389602661, Final Batch Loss: 0.7684975266456604\n",
      "Epoch 371, Loss: 2.223301649093628, Final Batch Loss: 0.5542516112327576\n",
      "Epoch 372, Loss: 2.333981364965439, Final Batch Loss: 0.6857178807258606\n",
      "Epoch 373, Loss: 2.2862476110458374, Final Batch Loss: 0.6203104853630066\n",
      "Epoch 374, Loss: 2.1416999995708466, Final Batch Loss: 0.44584527611732483\n",
      "Epoch 375, Loss: 2.3416351675987244, Final Batch Loss: 0.6927652955055237\n",
      "Epoch 376, Loss: 2.389854371547699, Final Batch Loss: 0.7631773352622986\n",
      "Epoch 377, Loss: 2.1679723262786865, Final Batch Loss: 0.5145670771598816\n",
      "Epoch 378, Loss: 2.1972371637821198, Final Batch Loss: 0.5690496563911438\n",
      "Epoch 379, Loss: 2.159750670194626, Final Batch Loss: 0.4929948151111603\n",
      "Epoch 380, Loss: 2.3819003105163574, Final Batch Loss: 0.7457106709480286\n",
      "Epoch 381, Loss: 2.384261906147003, Final Batch Loss: 0.7095246315002441\n",
      "Epoch 382, Loss: 2.258595794439316, Final Batch Loss: 0.49483755230903625\n",
      "Epoch 383, Loss: 2.2427210211753845, Final Batch Loss: 0.5577632784843445\n",
      "Epoch 384, Loss: 2.3641390800476074, Final Batch Loss: 0.6986207365989685\n",
      "Epoch 385, Loss: 2.1545992493629456, Final Batch Loss: 0.429699182510376\n",
      "Epoch 386, Loss: 2.330411434173584, Final Batch Loss: 0.6698043942451477\n",
      "Epoch 387, Loss: 2.1402797996997833, Final Batch Loss: 0.4483341872692108\n",
      "Epoch 388, Loss: 2.255432665348053, Final Batch Loss: 0.5234383940696716\n",
      "Epoch 389, Loss: 2.3773549795150757, Final Batch Loss: 0.7448854446411133\n",
      "Epoch 390, Loss: 2.148863196372986, Final Batch Loss: 0.5011455416679382\n",
      "Epoch 391, Loss: 2.066344201564789, Final Batch Loss: 0.4314849376678467\n",
      "Epoch 392, Loss: 2.158182978630066, Final Batch Loss: 0.5423462986946106\n",
      "Epoch 393, Loss: 2.007460117340088, Final Batch Loss: 0.30052345991134644\n",
      "Epoch 394, Loss: 2.341525614261627, Final Batch Loss: 0.6281271576881409\n",
      "Epoch 395, Loss: 2.1842973828315735, Final Batch Loss: 0.5381763577461243\n",
      "Epoch 396, Loss: 2.13933926820755, Final Batch Loss: 0.40051209926605225\n",
      "Epoch 397, Loss: 2.3041934967041016, Final Batch Loss: 0.5929152369499207\n",
      "Epoch 398, Loss: 2.141501694917679, Final Batch Loss: 0.4554378092288971\n",
      "Epoch 399, Loss: 2.349767506122589, Final Batch Loss: 0.680516242980957\n",
      "Epoch 400, Loss: 2.2013065814971924, Final Batch Loss: 0.49167001247406006\n",
      "Epoch 401, Loss: 2.0796392261981964, Final Batch Loss: 0.48258593678474426\n",
      "Epoch 402, Loss: 2.2591373920440674, Final Batch Loss: 0.5424714684486389\n",
      "Epoch 403, Loss: 2.3232580423355103, Final Batch Loss: 0.7496199011802673\n",
      "Epoch 404, Loss: 2.3663314282894135, Final Batch Loss: 0.7438926696777344\n",
      "Epoch 405, Loss: 2.0576493740081787, Final Batch Loss: 0.4432016611099243\n",
      "Epoch 406, Loss: 2.0720636546611786, Final Batch Loss: 0.4797907769680023\n",
      "Epoch 407, Loss: 2.241692900657654, Final Batch Loss: 0.5212576985359192\n",
      "Epoch 408, Loss: 2.1950870752334595, Final Batch Loss: 0.5475343465805054\n",
      "Epoch 409, Loss: 1.975943922996521, Final Batch Loss: 0.34102094173431396\n",
      "Epoch 410, Loss: 2.054326295852661, Final Batch Loss: 0.5533362627029419\n",
      "Epoch 411, Loss: 2.246717691421509, Final Batch Loss: 0.5644941926002502\n",
      "Epoch 412, Loss: 2.5124666690826416, Final Batch Loss: 0.7979989051818848\n",
      "Epoch 413, Loss: 2.0916334092617035, Final Batch Loss: 0.4387148320674896\n",
      "Epoch 414, Loss: 2.050491154193878, Final Batch Loss: 0.3887534439563751\n",
      "Epoch 415, Loss: 2.105398118495941, Final Batch Loss: 0.513750433921814\n",
      "Epoch 416, Loss: 2.150939404964447, Final Batch Loss: 0.4931927025318146\n",
      "Epoch 417, Loss: 2.3886688947677612, Final Batch Loss: 0.7027873992919922\n",
      "Epoch 418, Loss: 2.2586143612861633, Final Batch Loss: 0.6311283707618713\n",
      "Epoch 419, Loss: 2.1168706715106964, Final Batch Loss: 0.5441797375679016\n",
      "Epoch 420, Loss: 2.2203237414360046, Final Batch Loss: 0.6577618718147278\n",
      "Epoch 421, Loss: 2.0439855754375458, Final Batch Loss: 0.4410805404186249\n",
      "Epoch 422, Loss: 2.17264261841774, Final Batch Loss: 0.6367533206939697\n",
      "Epoch 423, Loss: 2.15501406788826, Final Batch Loss: 0.5426144599914551\n",
      "Epoch 424, Loss: 2.191555082798004, Final Batch Loss: 0.5892277359962463\n",
      "Epoch 425, Loss: 2.138979345560074, Final Batch Loss: 0.600222647190094\n",
      "Epoch 426, Loss: 2.2227837443351746, Final Batch Loss: 0.5746484398841858\n",
      "Epoch 427, Loss: 2.368310511112213, Final Batch Loss: 0.7251278758049011\n",
      "Epoch 428, Loss: 2.1819797456264496, Final Batch Loss: 0.5686221718788147\n",
      "Epoch 429, Loss: 2.106239229440689, Final Batch Loss: 0.5459571480751038\n",
      "Epoch 430, Loss: 1.9578075408935547, Final Batch Loss: 0.4033229351043701\n",
      "Epoch 431, Loss: 2.269783616065979, Final Batch Loss: 0.7043963074684143\n",
      "Epoch 432, Loss: 2.170356720685959, Final Batch Loss: 0.5826855301856995\n",
      "Epoch 433, Loss: 2.1323228776454926, Final Batch Loss: 0.5997748970985413\n",
      "Epoch 434, Loss: 2.2565779089927673, Final Batch Loss: 0.5836306214332581\n",
      "Epoch 435, Loss: 2.083968162536621, Final Batch Loss: 0.5075394511222839\n",
      "Epoch 436, Loss: 1.8612444400787354, Final Batch Loss: 0.26243507862091064\n",
      "Epoch 437, Loss: 2.0278809666633606, Final Batch Loss: 0.4452706277370453\n",
      "Epoch 438, Loss: 2.251562178134918, Final Batch Loss: 0.6849861145019531\n",
      "Epoch 439, Loss: 2.0030079185962677, Final Batch Loss: 0.34738031029701233\n",
      "Epoch 440, Loss: 2.0644005835056305, Final Batch Loss: 0.540863573551178\n",
      "Epoch 441, Loss: 2.02418315410614, Final Batch Loss: 0.446406751871109\n",
      "Epoch 442, Loss: 1.9027888178825378, Final Batch Loss: 0.4452073276042938\n",
      "Epoch 443, Loss: 2.2250953316688538, Final Batch Loss: 0.756398618221283\n",
      "Epoch 444, Loss: 1.9472007751464844, Final Batch Loss: 0.46646633744239807\n",
      "Epoch 445, Loss: 2.055070221424103, Final Batch Loss: 0.4479111135005951\n",
      "Epoch 446, Loss: 2.2964830100536346, Final Batch Loss: 0.7507987022399902\n",
      "Epoch 447, Loss: 1.9660629034042358, Final Batch Loss: 0.3709794282913208\n",
      "Epoch 448, Loss: 2.0858936607837677, Final Batch Loss: 0.5228895545005798\n",
      "Epoch 449, Loss: 2.2706921994686127, Final Batch Loss: 0.7406505942344666\n",
      "Epoch 450, Loss: 2.0804313719272614, Final Batch Loss: 0.5390928387641907\n",
      "Epoch 451, Loss: 2.1661078929901123, Final Batch Loss: 0.5855014324188232\n",
      "Epoch 452, Loss: 2.354337692260742, Final Batch Loss: 0.8459429144859314\n",
      "Epoch 453, Loss: 2.0185554921627045, Final Batch Loss: 0.5030815005302429\n",
      "Epoch 454, Loss: 2.1066451370716095, Final Batch Loss: 0.6496191620826721\n",
      "Epoch 455, Loss: 2.0154700875282288, Final Batch Loss: 0.45271480083465576\n",
      "Epoch 456, Loss: 1.970488727092743, Final Batch Loss: 0.39088988304138184\n",
      "Epoch 457, Loss: 1.969623178243637, Final Batch Loss: 0.39973053336143494\n",
      "Epoch 458, Loss: 1.9116997122764587, Final Batch Loss: 0.35004720091819763\n",
      "Epoch 459, Loss: 2.157610297203064, Final Batch Loss: 0.662466824054718\n",
      "Epoch 460, Loss: 2.0392158031463623, Final Batch Loss: 0.44463256001472473\n",
      "Epoch 461, Loss: 1.9572178423404694, Final Batch Loss: 0.4386427700519562\n",
      "Epoch 462, Loss: 1.91649129986763, Final Batch Loss: 0.33153703808784485\n",
      "Epoch 463, Loss: 1.9737570583820343, Final Batch Loss: 0.43882325291633606\n",
      "Epoch 464, Loss: 2.118388056755066, Final Batch Loss: 0.5479748249053955\n",
      "Epoch 465, Loss: 2.0916047394275665, Final Batch Loss: 0.47907555103302\n",
      "Epoch 466, Loss: 2.066947877407074, Final Batch Loss: 0.48564180731773376\n",
      "Epoch 467, Loss: 2.0796187818050385, Final Batch Loss: 0.616122305393219\n",
      "Epoch 468, Loss: 2.0697694420814514, Final Batch Loss: 0.5678097009658813\n",
      "Epoch 469, Loss: 2.097590535879135, Final Batch Loss: 0.5752632021903992\n",
      "Epoch 470, Loss: 2.126407414674759, Final Batch Loss: 0.6258063912391663\n",
      "Epoch 471, Loss: 2.1682917773723602, Final Batch Loss: 0.6265211701393127\n",
      "Epoch 472, Loss: 2.1326819360256195, Final Batch Loss: 0.610760509967804\n",
      "Epoch 473, Loss: 1.9792550206184387, Final Batch Loss: 0.43637147545814514\n",
      "Epoch 474, Loss: 1.9100859761238098, Final Batch Loss: 0.29400181770324707\n",
      "Epoch 475, Loss: 2.030021011829376, Final Batch Loss: 0.5089130401611328\n",
      "Epoch 476, Loss: 2.0679937601089478, Final Batch Loss: 0.5647060871124268\n",
      "Epoch 477, Loss: 2.0673038363456726, Final Batch Loss: 0.5098368525505066\n",
      "Epoch 478, Loss: 2.0847426652908325, Final Batch Loss: 0.4873410165309906\n",
      "Epoch 479, Loss: 1.9773233234882355, Final Batch Loss: 0.4336347281932831\n",
      "Epoch 480, Loss: 2.06359001994133, Final Batch Loss: 0.526475727558136\n",
      "Epoch 481, Loss: 2.0455567836761475, Final Batch Loss: 0.5462119579315186\n",
      "Epoch 482, Loss: 2.0496663749217987, Final Batch Loss: 0.49022141098976135\n",
      "Epoch 483, Loss: 1.9941090643405914, Final Batch Loss: 0.5111164450645447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484, Loss: 1.8976151943206787, Final Batch Loss: 0.44309791922569275\n",
      "Epoch 485, Loss: 1.9810509979724884, Final Batch Loss: 0.4576173722743988\n",
      "Epoch 486, Loss: 1.8026002645492554, Final Batch Loss: 0.2888897657394409\n",
      "Epoch 487, Loss: 1.9853165745735168, Final Batch Loss: 0.5165212750434875\n",
      "Epoch 488, Loss: 2.0669674277305603, Final Batch Loss: 0.5854443907737732\n",
      "Epoch 489, Loss: 1.9191868901252747, Final Batch Loss: 0.3949424922466278\n",
      "Epoch 490, Loss: 1.8255977928638458, Final Batch Loss: 0.40915241837501526\n",
      "Epoch 491, Loss: 2.045965254306793, Final Batch Loss: 0.5605723261833191\n",
      "Epoch 492, Loss: 2.0804422795772552, Final Batch Loss: 0.5741807818412781\n",
      "Epoch 493, Loss: 1.9588356912136078, Final Batch Loss: 0.3345658481121063\n",
      "Epoch 494, Loss: 2.0242116153240204, Final Batch Loss: 0.4232899844646454\n",
      "Epoch 495, Loss: 1.9705640077590942, Final Batch Loss: 0.5270573496818542\n",
      "Epoch 496, Loss: 1.893362283706665, Final Batch Loss: 0.45849213004112244\n",
      "Epoch 497, Loss: 2.0329586565494537, Final Batch Loss: 0.5732614398002625\n",
      "Epoch 498, Loss: 2.1386708319187164, Final Batch Loss: 0.6754655241966248\n",
      "Epoch 499, Loss: 2.0210755467414856, Final Batch Loss: 0.5233649015426636\n",
      "Epoch 500, Loss: 2.063553899526596, Final Batch Loss: 0.6348544359207153\n",
      "Epoch 501, Loss: 2.236539363861084, Final Batch Loss: 0.6720418334007263\n",
      "Epoch 502, Loss: 1.9289775788784027, Final Batch Loss: 0.4582270085811615\n",
      "Epoch 503, Loss: 2.114004075527191, Final Batch Loss: 0.6125819087028503\n",
      "Epoch 504, Loss: 2.05234956741333, Final Batch Loss: 0.5465594530105591\n",
      "Epoch 505, Loss: 2.0937450230121613, Final Batch Loss: 0.6113309860229492\n",
      "Epoch 506, Loss: 2.1493559777736664, Final Batch Loss: 0.5988314747810364\n",
      "Epoch 507, Loss: 1.8310285806655884, Final Batch Loss: 0.3610246181488037\n",
      "Epoch 508, Loss: 1.9233469069004059, Final Batch Loss: 0.4402411878108978\n",
      "Epoch 509, Loss: 1.9209152460098267, Final Batch Loss: 0.4880581200122833\n",
      "Epoch 510, Loss: 2.020902156829834, Final Batch Loss: 0.5651338696479797\n",
      "Epoch 511, Loss: 1.944868803024292, Final Batch Loss: 0.48345234990119934\n",
      "Epoch 512, Loss: 2.0008094906806946, Final Batch Loss: 0.5030641555786133\n",
      "Epoch 513, Loss: 2.161274403333664, Final Batch Loss: 0.6625308394432068\n",
      "Epoch 514, Loss: 1.8865244686603546, Final Batch Loss: 0.4309299886226654\n",
      "Epoch 515, Loss: 2.0020042955875397, Final Batch Loss: 0.4453740417957306\n",
      "Epoch 516, Loss: 1.9664768874645233, Final Batch Loss: 0.4703592360019684\n",
      "Epoch 517, Loss: 2.003618150949478, Final Batch Loss: 0.5886874198913574\n",
      "Epoch 518, Loss: 1.9892028272151947, Final Batch Loss: 0.5442609786987305\n",
      "Epoch 519, Loss: 2.0010069608688354, Final Batch Loss: 0.4675140678882599\n",
      "Epoch 520, Loss: 1.804067462682724, Final Batch Loss: 0.3624805212020874\n",
      "Epoch 521, Loss: 2.071429431438446, Final Batch Loss: 0.6243513226509094\n",
      "Epoch 522, Loss: 1.9630297422409058, Final Batch Loss: 0.49985018372535706\n",
      "Epoch 523, Loss: 1.8121342360973358, Final Batch Loss: 0.3021828830242157\n",
      "Epoch 524, Loss: 1.9896853566169739, Final Batch Loss: 0.4691941738128662\n",
      "Epoch 525, Loss: 2.021990805864334, Final Batch Loss: 0.5874853730201721\n",
      "Epoch 526, Loss: 1.9691926836967468, Final Batch Loss: 0.5550738573074341\n",
      "Epoch 527, Loss: 2.0676877200603485, Final Batch Loss: 0.5647333264350891\n",
      "Epoch 528, Loss: 2.025892972946167, Final Batch Loss: 0.5396785140037537\n",
      "Epoch 529, Loss: 1.8688215017318726, Final Batch Loss: 0.3851764500141144\n",
      "Epoch 530, Loss: 1.9709372818470001, Final Batch Loss: 0.5912309885025024\n",
      "Epoch 531, Loss: 1.8463002741336823, Final Batch Loss: 0.3043242394924164\n",
      "Epoch 532, Loss: 1.8807673752307892, Final Batch Loss: 0.3945485055446625\n",
      "Epoch 533, Loss: 1.916389286518097, Final Batch Loss: 0.4898318946361542\n",
      "Epoch 534, Loss: 1.9417628347873688, Final Batch Loss: 0.45786261558532715\n",
      "Epoch 535, Loss: 1.8883607685565948, Final Batch Loss: 0.40839841961860657\n",
      "Epoch 536, Loss: 1.9893093407154083, Final Batch Loss: 0.47894132137298584\n",
      "Epoch 537, Loss: 1.908397614955902, Final Batch Loss: 0.46635380387306213\n",
      "Epoch 538, Loss: 1.806808352470398, Final Batch Loss: 0.3615424931049347\n",
      "Epoch 539, Loss: 1.891118049621582, Final Batch Loss: 0.5620449781417847\n",
      "Epoch 540, Loss: 1.9548932909965515, Final Batch Loss: 0.5146390795707703\n",
      "Epoch 541, Loss: 1.9201765060424805, Final Batch Loss: 0.4916304051876068\n",
      "Epoch 542, Loss: 2.1023941338062286, Final Batch Loss: 0.7315827012062073\n",
      "Epoch 543, Loss: 1.977295160293579, Final Batch Loss: 0.5869500041007996\n",
      "Epoch 544, Loss: 1.9174461960792542, Final Batch Loss: 0.4714067280292511\n",
      "Epoch 545, Loss: 1.925262451171875, Final Batch Loss: 0.4837472438812256\n",
      "Epoch 546, Loss: 2.080821841955185, Final Batch Loss: 0.6079670190811157\n",
      "Epoch 547, Loss: 2.3163838982582092, Final Batch Loss: 0.8518442511558533\n",
      "Epoch 548, Loss: 2.016595482826233, Final Batch Loss: 0.5145950317382812\n",
      "Epoch 549, Loss: 1.9365763664245605, Final Batch Loss: 0.5113726258277893\n",
      "Epoch 550, Loss: 1.8646557331085205, Final Batch Loss: 0.40094950795173645\n",
      "Epoch 551, Loss: 1.826749861240387, Final Batch Loss: 0.40443649888038635\n",
      "Epoch 552, Loss: 1.9307465851306915, Final Batch Loss: 0.5614367723464966\n",
      "Epoch 553, Loss: 2.0662682056427, Final Batch Loss: 0.640486478805542\n",
      "Epoch 554, Loss: 2.1026421189308167, Final Batch Loss: 0.6322420239448547\n",
      "Epoch 555, Loss: 2.139234960079193, Final Batch Loss: 0.7548865675926208\n",
      "Epoch 556, Loss: 1.9742829501628876, Final Batch Loss: 0.5339683294296265\n",
      "Epoch 557, Loss: 1.8969172835350037, Final Batch Loss: 0.48105940222740173\n",
      "Epoch 558, Loss: 1.967287689447403, Final Batch Loss: 0.4873797595500946\n",
      "Epoch 559, Loss: 1.953167974948883, Final Batch Loss: 0.5368432998657227\n",
      "Epoch 560, Loss: 2.020218789577484, Final Batch Loss: 0.5167599320411682\n",
      "Epoch 561, Loss: 1.8864578604698181, Final Batch Loss: 0.4528231620788574\n",
      "Epoch 562, Loss: 1.8534075319766998, Final Batch Loss: 0.38435235619544983\n",
      "Epoch 563, Loss: 2.014545053243637, Final Batch Loss: 0.4646742641925812\n",
      "Epoch 564, Loss: 2.0003781020641327, Final Batch Loss: 0.6389207243919373\n",
      "Epoch 565, Loss: 1.9145494997501373, Final Batch Loss: 0.5492277145385742\n",
      "Epoch 566, Loss: 1.831369549036026, Final Batch Loss: 0.30215731263160706\n",
      "Epoch 567, Loss: 1.9142075181007385, Final Batch Loss: 0.4883967936038971\n",
      "Epoch 568, Loss: 1.925612360239029, Final Batch Loss: 0.5479351282119751\n",
      "Epoch 569, Loss: 1.9804164469242096, Final Batch Loss: 0.6156365275382996\n",
      "Epoch 570, Loss: 1.9463619589805603, Final Batch Loss: 0.5746260285377502\n",
      "Epoch 571, Loss: 1.817284733057022, Final Batch Loss: 0.4156831204891205\n",
      "Epoch 572, Loss: 2.04013255238533, Final Batch Loss: 0.5451890230178833\n",
      "Epoch 573, Loss: 1.9335384368896484, Final Batch Loss: 0.3901344835758209\n",
      "Epoch 574, Loss: 1.8601562082767487, Final Batch Loss: 0.38650795817375183\n",
      "Epoch 575, Loss: 1.8091365993022919, Final Batch Loss: 0.40905651450157166\n",
      "Epoch 576, Loss: 1.872549593448639, Final Batch Loss: 0.48850104212760925\n",
      "Epoch 577, Loss: 1.6401792764663696, Final Batch Loss: 0.20535016059875488\n",
      "Epoch 578, Loss: 1.9243295192718506, Final Batch Loss: 0.5249710083007812\n",
      "Epoch 579, Loss: 1.879424810409546, Final Batch Loss: 0.49936676025390625\n",
      "Epoch 580, Loss: 1.766979157924652, Final Batch Loss: 0.394317626953125\n",
      "Epoch 581, Loss: 1.9731031358242035, Final Batch Loss: 0.5534643530845642\n",
      "Epoch 582, Loss: 1.871131807565689, Final Batch Loss: 0.41017946600914\n",
      "Epoch 583, Loss: 1.8228271007537842, Final Batch Loss: 0.352907657623291\n",
      "Epoch 584, Loss: 1.7068021893501282, Final Batch Loss: 0.28953787684440613\n",
      "Epoch 585, Loss: 1.7828565537929535, Final Batch Loss: 0.4527575969696045\n",
      "Epoch 586, Loss: 1.8207159042358398, Final Batch Loss: 0.44996753334999084\n",
      "Epoch 587, Loss: 1.9247092306613922, Final Batch Loss: 0.5373895168304443\n",
      "Epoch 588, Loss: 1.7196164727210999, Final Batch Loss: 0.36808356642723083\n",
      "Epoch 589, Loss: 1.8874025344848633, Final Batch Loss: 0.46876323223114014\n",
      "Epoch 590, Loss: 1.9317317306995392, Final Batch Loss: 0.5150692462921143\n",
      "Epoch 591, Loss: 1.8460542261600494, Final Batch Loss: 0.42312297224998474\n",
      "Epoch 592, Loss: 1.9161458909511566, Final Batch Loss: 0.4516158401966095\n",
      "Epoch 593, Loss: 1.898525357246399, Final Batch Loss: 0.5464926362037659\n",
      "Epoch 594, Loss: 1.9144992232322693, Final Batch Loss: 0.5451100468635559\n",
      "Epoch 595, Loss: 1.8724272549152374, Final Batch Loss: 0.3828136622905731\n",
      "Epoch 596, Loss: 1.8719042241573334, Final Batch Loss: 0.42029866576194763\n",
      "Epoch 597, Loss: 1.8675456643104553, Final Batch Loss: 0.38499435782432556\n",
      "Epoch 598, Loss: 2.0001639127731323, Final Batch Loss: 0.6882927417755127\n",
      "Epoch 599, Loss: 1.9579229652881622, Final Batch Loss: 0.5698671340942383\n",
      "Epoch 600, Loss: 1.9001473784446716, Final Batch Loss: 0.4503796100616455\n",
      "Epoch 601, Loss: 1.7292727828025818, Final Batch Loss: 0.3311100900173187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 602, Loss: 1.8055574297904968, Final Batch Loss: 0.4464750289916992\n",
      "Epoch 603, Loss: 1.8533784449100494, Final Batch Loss: 0.4651630222797394\n",
      "Epoch 604, Loss: 1.8265920877456665, Final Batch Loss: 0.40550291538238525\n",
      "Epoch 605, Loss: 1.8023832440376282, Final Batch Loss: 0.4051534831523895\n",
      "Epoch 606, Loss: 1.8205997943878174, Final Batch Loss: 0.3826424181461334\n",
      "Epoch 607, Loss: 1.8709399700164795, Final Batch Loss: 0.5027607083320618\n",
      "Epoch 608, Loss: 1.886692613363266, Final Batch Loss: 0.5616721510887146\n",
      "Epoch 609, Loss: 1.8774722218513489, Final Batch Loss: 0.5367278456687927\n",
      "Epoch 610, Loss: 1.7877157628536224, Final Batch Loss: 0.44538363814353943\n",
      "Epoch 611, Loss: 1.8004356026649475, Final Batch Loss: 0.40230679512023926\n",
      "Epoch 612, Loss: 1.8317511975765228, Final Batch Loss: 0.46217596530914307\n",
      "Epoch 613, Loss: 1.9599643051624298, Final Batch Loss: 0.6175402998924255\n",
      "Epoch 614, Loss: 1.754242181777954, Final Batch Loss: 0.3872312307357788\n",
      "Epoch 615, Loss: 1.8376831412315369, Final Batch Loss: 0.44479238986968994\n",
      "Epoch 616, Loss: 1.9201112687587738, Final Batch Loss: 0.5658466219902039\n",
      "Epoch 617, Loss: 1.985781729221344, Final Batch Loss: 0.6305331587791443\n",
      "Epoch 618, Loss: 1.6810665428638458, Final Batch Loss: 0.3509193956851959\n",
      "Epoch 619, Loss: 1.7122208774089813, Final Batch Loss: 0.3493938446044922\n",
      "Epoch 620, Loss: 1.8343074917793274, Final Batch Loss: 0.47116637229919434\n",
      "Epoch 621, Loss: 1.764934927225113, Final Batch Loss: 0.455366849899292\n",
      "Epoch 622, Loss: 1.8475587964057922, Final Batch Loss: 0.5253836512565613\n",
      "Epoch 623, Loss: 1.9386384189128876, Final Batch Loss: 0.4805033206939697\n",
      "Epoch 624, Loss: 1.844882220029831, Final Batch Loss: 0.4865323305130005\n",
      "Epoch 625, Loss: 1.8747358322143555, Final Batch Loss: 0.5348826050758362\n",
      "Epoch 626, Loss: 1.7549394369125366, Final Batch Loss: 0.3782535493373871\n",
      "Epoch 627, Loss: 1.7744831442832947, Final Batch Loss: 0.4703933000564575\n",
      "Epoch 628, Loss: 1.6993294656276703, Final Batch Loss: 0.3587460219860077\n",
      "Epoch 629, Loss: 1.8866565525531769, Final Batch Loss: 0.5570306181907654\n",
      "Epoch 630, Loss: 1.6427904069423676, Final Batch Loss: 0.290606826543808\n",
      "Epoch 631, Loss: 1.7913420796394348, Final Batch Loss: 0.4088253676891327\n",
      "Epoch 632, Loss: 1.8216077387332916, Final Batch Loss: 0.49025729298591614\n",
      "Epoch 633, Loss: 1.864580512046814, Final Batch Loss: 0.4740254878997803\n",
      "Epoch 634, Loss: 1.9614234268665314, Final Batch Loss: 0.6647642850875854\n",
      "Epoch 635, Loss: 1.822340041399002, Final Batch Loss: 0.5049084424972534\n",
      "Epoch 636, Loss: 1.865353763103485, Final Batch Loss: 0.5312454104423523\n",
      "Epoch 637, Loss: 1.8287258446216583, Final Batch Loss: 0.45054876804351807\n",
      "Epoch 638, Loss: 1.7793468236923218, Final Batch Loss: 0.42770469188690186\n",
      "Epoch 639, Loss: 1.7484623193740845, Final Batch Loss: 0.43840113282203674\n",
      "Epoch 640, Loss: 1.7102310359477997, Final Batch Loss: 0.4017504155635834\n",
      "Epoch 641, Loss: 1.6688281893730164, Final Batch Loss: 0.21088403463363647\n",
      "Epoch 642, Loss: 1.8640678226947784, Final Batch Loss: 0.49525174498558044\n",
      "Epoch 643, Loss: 1.7818377017974854, Final Batch Loss: 0.4762929379940033\n",
      "Epoch 644, Loss: 1.6381711661815643, Final Batch Loss: 0.25266069173812866\n",
      "Epoch 645, Loss: 1.683860257267952, Final Batch Loss: 0.24057306349277496\n",
      "Epoch 646, Loss: 1.7417550086975098, Final Batch Loss: 0.44679927825927734\n",
      "Epoch 647, Loss: 1.744208961725235, Final Batch Loss: 0.4149709939956665\n",
      "Epoch 648, Loss: 1.7888538539409637, Final Batch Loss: 0.4429868161678314\n",
      "Epoch 649, Loss: 1.7489281296730042, Final Batch Loss: 0.46575722098350525\n",
      "Epoch 650, Loss: 1.7473081350326538, Final Batch Loss: 0.40457209944725037\n",
      "Epoch 651, Loss: 1.705441027879715, Final Batch Loss: 0.30732738971710205\n",
      "Epoch 652, Loss: 1.7918303310871124, Final Batch Loss: 0.3416968584060669\n",
      "Epoch 653, Loss: 1.691805750131607, Final Batch Loss: 0.39657890796661377\n",
      "Epoch 654, Loss: 1.8207251727581024, Final Batch Loss: 0.4802035987377167\n",
      "Epoch 655, Loss: 1.7958135902881622, Final Batch Loss: 0.49749183654785156\n",
      "Epoch 656, Loss: 1.5755616575479507, Final Batch Loss: 0.2185654193162918\n",
      "Epoch 657, Loss: 1.7807126641273499, Final Batch Loss: 0.40283527970314026\n",
      "Epoch 658, Loss: 1.8481793105602264, Final Batch Loss: 0.5165091753005981\n",
      "Epoch 659, Loss: 1.7300537526607513, Final Batch Loss: 0.42688700556755066\n",
      "Epoch 660, Loss: 1.752209633588791, Final Batch Loss: 0.43215712904930115\n",
      "Epoch 661, Loss: 1.7376348078250885, Final Batch Loss: 0.4467698335647583\n",
      "Epoch 662, Loss: 1.7673527896404266, Final Batch Loss: 0.4531722068786621\n",
      "Epoch 663, Loss: 1.8161494135856628, Final Batch Loss: 0.4001806676387787\n",
      "Epoch 664, Loss: 1.766579806804657, Final Batch Loss: 0.5450050234794617\n",
      "Epoch 665, Loss: 1.666957974433899, Final Batch Loss: 0.33728906512260437\n",
      "Epoch 666, Loss: 1.8760827779769897, Final Batch Loss: 0.5361218452453613\n",
      "Epoch 667, Loss: 1.648825228214264, Final Batch Loss: 0.3539849817752838\n",
      "Epoch 668, Loss: 1.962498426437378, Final Batch Loss: 0.6490362286567688\n",
      "Epoch 669, Loss: 1.7623518705368042, Final Batch Loss: 0.5406885147094727\n",
      "Epoch 670, Loss: 1.7170364260673523, Final Batch Loss: 0.42367836833000183\n",
      "Epoch 671, Loss: 1.781272292137146, Final Batch Loss: 0.49110063910484314\n",
      "Epoch 672, Loss: 1.6484163403511047, Final Batch Loss: 0.30027446150779724\n",
      "Epoch 673, Loss: 1.8311237394809723, Final Batch Loss: 0.47094619274139404\n",
      "Epoch 674, Loss: 1.6680115759372711, Final Batch Loss: 0.41190657019615173\n",
      "Epoch 675, Loss: 2.0357576608657837, Final Batch Loss: 0.7373273968696594\n",
      "Epoch 676, Loss: 1.7839402258396149, Final Batch Loss: 0.4429377317428589\n",
      "Epoch 677, Loss: 1.8195789158344269, Final Batch Loss: 0.42337897419929504\n",
      "Epoch 678, Loss: 1.7094188332557678, Final Batch Loss: 0.38096359372138977\n",
      "Epoch 679, Loss: 1.6952846050262451, Final Batch Loss: 0.451043963432312\n",
      "Epoch 680, Loss: 1.6135902106761932, Final Batch Loss: 0.28468409180641174\n",
      "Epoch 681, Loss: 1.6700558960437775, Final Batch Loss: 0.3358994424343109\n",
      "Epoch 682, Loss: 1.7770809531211853, Final Batch Loss: 0.5245675444602966\n",
      "Epoch 683, Loss: 1.7868417501449585, Final Batch Loss: 0.35476139187812805\n",
      "Epoch 684, Loss: 1.618504285812378, Final Batch Loss: 0.38584867119789124\n",
      "Epoch 685, Loss: 1.784766674041748, Final Batch Loss: 0.3348011076450348\n",
      "Epoch 686, Loss: 1.8602893352508545, Final Batch Loss: 0.6212887167930603\n",
      "Epoch 687, Loss: 1.8215726613998413, Final Batch Loss: 0.5018491744995117\n",
      "Epoch 688, Loss: 1.6597321927547455, Final Batch Loss: 0.40292859077453613\n",
      "Epoch 689, Loss: 1.6173731684684753, Final Batch Loss: 0.30761557817459106\n",
      "Epoch 690, Loss: 1.8106861114501953, Final Batch Loss: 0.4775327146053314\n",
      "Epoch 691, Loss: 1.7844353318214417, Final Batch Loss: 0.4653783142566681\n",
      "Epoch 692, Loss: 1.6577541530132294, Final Batch Loss: 0.3925209045410156\n",
      "Epoch 693, Loss: 1.8886695206165314, Final Batch Loss: 0.6049206852912903\n",
      "Epoch 694, Loss: 1.522994190454483, Final Batch Loss: 0.28392723202705383\n",
      "Epoch 695, Loss: 1.6277405321598053, Final Batch Loss: 0.3302142918109894\n",
      "Epoch 696, Loss: 1.785789579153061, Final Batch Loss: 0.4192383289337158\n",
      "Epoch 697, Loss: 2.070871740579605, Final Batch Loss: 0.7211690545082092\n",
      "Epoch 698, Loss: 1.8358256220817566, Final Batch Loss: 0.4865235984325409\n",
      "Epoch 699, Loss: 1.6609946489334106, Final Batch Loss: 0.3331429660320282\n",
      "Epoch 700, Loss: 1.6269314289093018, Final Batch Loss: 0.28789371252059937\n",
      "Epoch 701, Loss: 1.773875653743744, Final Batch Loss: 0.43936800956726074\n",
      "Epoch 702, Loss: 1.8978050649166107, Final Batch Loss: 0.5582117438316345\n",
      "Epoch 703, Loss: 1.8960638642311096, Final Batch Loss: 0.6195588111877441\n",
      "Epoch 704, Loss: 1.7075978219509125, Final Batch Loss: 0.3755442798137665\n",
      "Epoch 705, Loss: 1.8249560594558716, Final Batch Loss: 0.5569310784339905\n",
      "Epoch 706, Loss: 1.6919317245483398, Final Batch Loss: 0.42604729533195496\n",
      "Epoch 707, Loss: 1.7566340565681458, Final Batch Loss: 0.45696282386779785\n",
      "Epoch 708, Loss: 2.0391966700553894, Final Batch Loss: 0.7622480392456055\n",
      "Epoch 709, Loss: 1.8088442981243134, Final Batch Loss: 0.42327606678009033\n",
      "Epoch 710, Loss: 1.6674795150756836, Final Batch Loss: 0.41589030623435974\n",
      "Epoch 711, Loss: 1.9425043165683746, Final Batch Loss: 0.6657750606536865\n",
      "Epoch 712, Loss: 1.5789092779159546, Final Batch Loss: 0.2924745976924896\n",
      "Epoch 713, Loss: 1.7668139040470123, Final Batch Loss: 0.4136154353618622\n",
      "Epoch 714, Loss: 1.7903105914592743, Final Batch Loss: 0.4975031614303589\n",
      "Epoch 715, Loss: 1.6899684965610504, Final Batch Loss: 0.40432658791542053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716, Loss: 1.8239224553108215, Final Batch Loss: 0.6089188456535339\n",
      "Epoch 717, Loss: 1.6605550646781921, Final Batch Loss: 0.40937232971191406\n",
      "Epoch 718, Loss: 1.689359188079834, Final Batch Loss: 0.34687575697898865\n",
      "Epoch 719, Loss: 1.7601469457149506, Final Batch Loss: 0.4582540988922119\n",
      "Epoch 720, Loss: 1.7070590257644653, Final Batch Loss: 0.42623355984687805\n",
      "Epoch 721, Loss: 1.7597325146198273, Final Batch Loss: 0.5216872096061707\n",
      "Epoch 722, Loss: 1.8105688989162445, Final Batch Loss: 0.506458044052124\n",
      "Epoch 723, Loss: 1.9860742390155792, Final Batch Loss: 0.7793607115745544\n",
      "Epoch 724, Loss: 1.7800657153129578, Final Batch Loss: 0.6059417724609375\n",
      "Epoch 725, Loss: 1.6909634172916412, Final Batch Loss: 0.3933037519454956\n",
      "Epoch 726, Loss: 1.6030994951725006, Final Batch Loss: 0.29777511954307556\n",
      "Epoch 727, Loss: 1.7344104051589966, Final Batch Loss: 0.46432507038116455\n",
      "Epoch 728, Loss: 1.692603886127472, Final Batch Loss: 0.47285541892051697\n",
      "Epoch 729, Loss: 1.6092335879802704, Final Batch Loss: 0.3560378849506378\n",
      "Epoch 730, Loss: 1.6567064821720123, Final Batch Loss: 0.39577504992485046\n",
      "Epoch 731, Loss: 1.5828697085380554, Final Batch Loss: 0.35652339458465576\n",
      "Epoch 732, Loss: 1.8773073256015778, Final Batch Loss: 0.5769109129905701\n",
      "Epoch 733, Loss: 1.7259958386421204, Final Batch Loss: 0.4330604076385498\n",
      "Epoch 734, Loss: 1.817154049873352, Final Batch Loss: 0.4627285897731781\n",
      "Epoch 735, Loss: 1.5700243711471558, Final Batch Loss: 0.3587050437927246\n",
      "Epoch 736, Loss: 1.6678891777992249, Final Batch Loss: 0.3846067488193512\n",
      "Epoch 737, Loss: 1.615685224533081, Final Batch Loss: 0.34161579608917236\n",
      "Epoch 738, Loss: 1.7269594073295593, Final Batch Loss: 0.5155655741691589\n",
      "Epoch 739, Loss: 1.652710199356079, Final Batch Loss: 0.4392738342285156\n",
      "Epoch 740, Loss: 1.7100183963775635, Final Batch Loss: 0.3779207766056061\n",
      "Epoch 741, Loss: 1.8626395165920258, Final Batch Loss: 0.612294614315033\n",
      "Epoch 742, Loss: 1.7064061760902405, Final Batch Loss: 0.4172522723674774\n",
      "Epoch 743, Loss: 1.6162236630916595, Final Batch Loss: 0.3360869586467743\n",
      "Epoch 744, Loss: 1.7312950789928436, Final Batch Loss: 0.4972344934940338\n",
      "Epoch 745, Loss: 1.676316499710083, Final Batch Loss: 0.3647187650203705\n",
      "Epoch 746, Loss: 1.755491942167282, Final Batch Loss: 0.3929530680179596\n",
      "Epoch 747, Loss: 1.6683248281478882, Final Batch Loss: 0.45765694975852966\n",
      "Epoch 748, Loss: 1.4731841683387756, Final Batch Loss: 0.2553772032260895\n",
      "Epoch 749, Loss: 1.5273812413215637, Final Batch Loss: 0.2654803693294525\n",
      "Epoch 750, Loss: 1.5457985401153564, Final Batch Loss: 0.2810767590999603\n",
      "Epoch 751, Loss: 1.7205279767513275, Final Batch Loss: 0.3849528133869171\n",
      "Epoch 752, Loss: 1.680114984512329, Final Batch Loss: 0.3652578294277191\n",
      "Epoch 753, Loss: 1.7144146263599396, Final Batch Loss: 0.3980971574783325\n",
      "Epoch 754, Loss: 1.6023902893066406, Final Batch Loss: 0.3349721431732178\n",
      "Epoch 755, Loss: 1.5760434567928314, Final Batch Loss: 0.30537325143814087\n",
      "Epoch 756, Loss: 1.6274641156196594, Final Batch Loss: 0.3818282186985016\n",
      "Epoch 757, Loss: 1.7798369824886322, Final Batch Loss: 0.4069524109363556\n",
      "Epoch 758, Loss: 1.7155868411064148, Final Batch Loss: 0.5366657376289368\n",
      "Epoch 759, Loss: 1.6049818396568298, Final Batch Loss: 0.46278560161590576\n",
      "Epoch 760, Loss: 1.5946952104568481, Final Batch Loss: 0.39845386147499084\n",
      "Epoch 761, Loss: 1.7859176993370056, Final Batch Loss: 0.5799258351325989\n",
      "Epoch 762, Loss: 1.4466254264116287, Final Batch Loss: 0.23315344750881195\n",
      "Epoch 763, Loss: 1.6982687413692474, Final Batch Loss: 0.47805485129356384\n",
      "Epoch 764, Loss: 1.644141137599945, Final Batch Loss: 0.466452956199646\n",
      "Epoch 765, Loss: 1.723468393087387, Final Batch Loss: 0.34956738352775574\n",
      "Epoch 766, Loss: 1.791654646396637, Final Batch Loss: 0.5855756402015686\n",
      "Epoch 767, Loss: 1.819933831691742, Final Batch Loss: 0.5113345980644226\n",
      "Epoch 768, Loss: 1.6277280449867249, Final Batch Loss: 0.36019816994667053\n",
      "Epoch 769, Loss: 1.7350417971611023, Final Batch Loss: 0.49272164702415466\n",
      "Epoch 770, Loss: 1.703688532114029, Final Batch Loss: 0.4347364902496338\n",
      "Epoch 771, Loss: 1.553219884634018, Final Batch Loss: 0.2805539071559906\n",
      "Epoch 772, Loss: 1.5837092101573944, Final Batch Loss: 0.27379199862480164\n",
      "Epoch 773, Loss: 1.6212012469768524, Final Batch Loss: 0.4216100871562958\n",
      "Epoch 774, Loss: 1.576872318983078, Final Batch Loss: 0.3162137567996979\n",
      "Epoch 775, Loss: 1.6908196806907654, Final Batch Loss: 0.3737771213054657\n",
      "Epoch 776, Loss: 1.656454086303711, Final Batch Loss: 0.37762460112571716\n",
      "Epoch 777, Loss: 1.7503932416439056, Final Batch Loss: 0.46791115403175354\n",
      "Epoch 778, Loss: 1.6779133975505829, Final Batch Loss: 0.42574265599250793\n",
      "Epoch 779, Loss: 1.6557318568229675, Final Batch Loss: 0.4724067151546478\n",
      "Epoch 780, Loss: 1.6326632499694824, Final Batch Loss: 0.42809951305389404\n",
      "Epoch 781, Loss: 1.7686301469802856, Final Batch Loss: 0.5979990363121033\n",
      "Epoch 782, Loss: 1.6919241845607758, Final Batch Loss: 0.456724613904953\n",
      "Epoch 783, Loss: 1.6595509052276611, Final Batch Loss: 0.4904857873916626\n",
      "Epoch 784, Loss: 1.5406847596168518, Final Batch Loss: 0.4123598337173462\n",
      "Epoch 785, Loss: 1.4952880442142487, Final Batch Loss: 0.2956588864326477\n",
      "Epoch 786, Loss: 1.5995996296405792, Final Batch Loss: 0.33941295742988586\n",
      "Epoch 787, Loss: 1.6194199621677399, Final Batch Loss: 0.3881411552429199\n",
      "Epoch 788, Loss: 1.6222823858261108, Final Batch Loss: 0.4686456620693207\n",
      "Epoch 789, Loss: 1.702797770500183, Final Batch Loss: 0.4153095483779907\n",
      "Epoch 790, Loss: 1.773062825202942, Final Batch Loss: 0.49219104647636414\n",
      "Epoch 791, Loss: 1.5738554298877716, Final Batch Loss: 0.30536091327667236\n",
      "Epoch 792, Loss: 1.5660920143127441, Final Batch Loss: 0.3510455787181854\n",
      "Epoch 793, Loss: 1.4783025979995728, Final Batch Loss: 0.29197028279304504\n",
      "Epoch 794, Loss: 1.698191374540329, Final Batch Loss: 0.5770341753959656\n",
      "Epoch 795, Loss: 1.63849475979805, Final Batch Loss: 0.3454562723636627\n",
      "Epoch 796, Loss: 1.6184853315353394, Final Batch Loss: 0.36731183528900146\n",
      "Epoch 797, Loss: 1.5453991889953613, Final Batch Loss: 0.33166173100471497\n",
      "Epoch 798, Loss: 1.6740787625312805, Final Batch Loss: 0.5141544342041016\n",
      "Epoch 799, Loss: 1.6573229134082794, Final Batch Loss: 0.5048683881759644\n",
      "Epoch 800, Loss: 1.6165904998779297, Final Batch Loss: 0.34839928150177\n",
      "Epoch 801, Loss: 1.522557020187378, Final Batch Loss: 0.3436070382595062\n",
      "Epoch 802, Loss: 1.708386778831482, Final Batch Loss: 0.4692955017089844\n",
      "Epoch 803, Loss: 1.6582965552806854, Final Batch Loss: 0.3947622776031494\n",
      "Epoch 804, Loss: 1.5522333979606628, Final Batch Loss: 0.28376874327659607\n",
      "Epoch 805, Loss: 1.6229594349861145, Final Batch Loss: 0.4124877452850342\n",
      "Epoch 806, Loss: 1.671994000673294, Final Batch Loss: 0.4539128541946411\n",
      "Epoch 807, Loss: 1.548517495393753, Final Batch Loss: 0.3587290048599243\n",
      "Epoch 808, Loss: 1.7374257147312164, Final Batch Loss: 0.5437136888504028\n",
      "Epoch 809, Loss: 1.5712214708328247, Final Batch Loss: 0.4417128264904022\n",
      "Epoch 810, Loss: 1.5533213913440704, Final Batch Loss: 0.3782844543457031\n",
      "Epoch 811, Loss: 1.5795782506465912, Final Batch Loss: 0.3531140983104706\n",
      "Epoch 812, Loss: 1.7100813686847687, Final Batch Loss: 0.4197390079498291\n",
      "Epoch 813, Loss: 1.6562600433826447, Final Batch Loss: 0.52507084608078\n",
      "Epoch 814, Loss: 1.899131864309311, Final Batch Loss: 0.7213156819343567\n",
      "Epoch 815, Loss: 1.5544836521148682, Final Batch Loss: 0.3840446174144745\n",
      "Epoch 816, Loss: 1.5236369669437408, Final Batch Loss: 0.31957465410232544\n",
      "Epoch 817, Loss: 1.4978914558887482, Final Batch Loss: 0.2877369225025177\n",
      "Epoch 818, Loss: 1.7361262142658234, Final Batch Loss: 0.5679904818534851\n",
      "Epoch 819, Loss: 1.6717523634433746, Final Batch Loss: 0.4517843425273895\n",
      "Epoch 820, Loss: 1.812183529138565, Final Batch Loss: 0.4989866316318512\n",
      "Epoch 821, Loss: 1.6150237321853638, Final Batch Loss: 0.4306586682796478\n",
      "Epoch 822, Loss: 1.5754849016666412, Final Batch Loss: 0.3869673013687134\n",
      "Epoch 823, Loss: 1.6034635603427887, Final Batch Loss: 0.3673213720321655\n",
      "Epoch 824, Loss: 1.7452420890331268, Final Batch Loss: 0.5013354420661926\n",
      "Epoch 825, Loss: 1.4618487358093262, Final Batch Loss: 0.3064357042312622\n",
      "Epoch 826, Loss: 1.5204001069068909, Final Batch Loss: 0.36130404472351074\n",
      "Epoch 827, Loss: 1.558589607477188, Final Batch Loss: 0.45762285590171814\n",
      "Epoch 828, Loss: 1.6151678562164307, Final Batch Loss: 0.43776047229766846\n",
      "Epoch 829, Loss: 1.6324580013751984, Final Batch Loss: 0.43629100918769836\n",
      "Epoch 830, Loss: 1.72057044506073, Final Batch Loss: 0.4675917625427246\n",
      "Epoch 831, Loss: 1.8180221617221832, Final Batch Loss: 0.5994848608970642\n",
      "Epoch 832, Loss: 1.6802706122398376, Final Batch Loss: 0.5037930011749268\n",
      "Epoch 833, Loss: 1.5850769877433777, Final Batch Loss: 0.3999526798725128\n",
      "Epoch 834, Loss: 1.7742214798927307, Final Batch Loss: 0.5930177569389343\n",
      "Epoch 835, Loss: 1.5711047053337097, Final Batch Loss: 0.3299213647842407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836, Loss: 1.531578540802002, Final Batch Loss: 0.35894975066185\n",
      "Epoch 837, Loss: 1.5933770835399628, Final Batch Loss: 0.3275667130947113\n",
      "Epoch 838, Loss: 1.546514242887497, Final Batch Loss: 0.38050684332847595\n",
      "Epoch 839, Loss: 1.6257371604442596, Final Batch Loss: 0.39912667870521545\n",
      "Epoch 840, Loss: 1.6136417984962463, Final Batch Loss: 0.41360923647880554\n",
      "Epoch 841, Loss: 1.5189038813114166, Final Batch Loss: 0.3648984432220459\n",
      "Epoch 842, Loss: 1.4950476288795471, Final Batch Loss: 0.28110170364379883\n",
      "Epoch 843, Loss: 1.5407128632068634, Final Batch Loss: 0.3584451377391815\n",
      "Epoch 844, Loss: 1.5783381760120392, Final Batch Loss: 0.4919722080230713\n",
      "Epoch 845, Loss: 1.5055809915065765, Final Batch Loss: 0.3915993869304657\n",
      "Epoch 846, Loss: 1.6177342236042023, Final Batch Loss: 0.4508824348449707\n",
      "Epoch 847, Loss: 1.7307230532169342, Final Batch Loss: 0.46861532330513\n",
      "Epoch 848, Loss: 1.5172579288482666, Final Batch Loss: 0.4191684424877167\n",
      "Epoch 849, Loss: 1.4972546994686127, Final Batch Loss: 0.292836993932724\n",
      "Epoch 850, Loss: 1.5408351123332977, Final Batch Loss: 0.3739220201969147\n",
      "Epoch 851, Loss: 1.5780431628227234, Final Batch Loss: 0.3652906119823456\n",
      "Epoch 852, Loss: 1.561573177576065, Final Batch Loss: 0.3800297677516937\n",
      "Epoch 853, Loss: 1.501845121383667, Final Batch Loss: 0.3402463495731354\n",
      "Epoch 854, Loss: 1.4836167097091675, Final Batch Loss: 0.30072611570358276\n",
      "Epoch 855, Loss: 1.4662352204322815, Final Batch Loss: 0.33468639850616455\n",
      "Epoch 856, Loss: 1.4169879853725433, Final Batch Loss: 0.2546265423297882\n",
      "Epoch 857, Loss: 1.7297685146331787, Final Batch Loss: 0.4448697566986084\n",
      "Epoch 858, Loss: 1.6142246723175049, Final Batch Loss: 0.461813360452652\n",
      "Epoch 859, Loss: 1.4466275572776794, Final Batch Loss: 0.3664042055606842\n",
      "Epoch 860, Loss: 1.6101652383804321, Final Batch Loss: 0.42171502113342285\n",
      "Epoch 861, Loss: 1.5979412496089935, Final Batch Loss: 0.415360689163208\n",
      "Epoch 862, Loss: 1.4350799918174744, Final Batch Loss: 0.2620655596256256\n",
      "Epoch 863, Loss: 1.5228912830352783, Final Batch Loss: 0.36372390389442444\n",
      "Epoch 864, Loss: 1.5054987072944641, Final Batch Loss: 0.3734058439731598\n",
      "Epoch 865, Loss: 1.455362781882286, Final Batch Loss: 0.2492702454328537\n",
      "Epoch 866, Loss: 1.4989262223243713, Final Batch Loss: 0.2911851108074188\n",
      "Epoch 867, Loss: 1.6507876515388489, Final Batch Loss: 0.36050698161125183\n",
      "Epoch 868, Loss: 1.627882719039917, Final Batch Loss: 0.5109630227088928\n",
      "Epoch 869, Loss: 1.6327435374259949, Final Batch Loss: 0.37825867533683777\n",
      "Epoch 870, Loss: 1.460423618555069, Final Batch Loss: 0.19555407762527466\n",
      "Epoch 871, Loss: 1.5571340024471283, Final Batch Loss: 0.3082996606826782\n",
      "Epoch 872, Loss: 1.5800406336784363, Final Batch Loss: 0.40835002064704895\n",
      "Epoch 873, Loss: 1.4871807992458344, Final Batch Loss: 0.3520428240299225\n",
      "Epoch 874, Loss: 1.7953265309333801, Final Batch Loss: 0.7099220752716064\n",
      "Epoch 875, Loss: 1.727609246969223, Final Batch Loss: 0.4888509511947632\n",
      "Epoch 876, Loss: 1.5540043711662292, Final Batch Loss: 0.3620336055755615\n",
      "Epoch 877, Loss: 1.5675155222415924, Final Batch Loss: 0.27225589752197266\n",
      "Epoch 878, Loss: 1.601802259683609, Final Batch Loss: 0.4257543385028839\n",
      "Epoch 879, Loss: 1.489354133605957, Final Batch Loss: 0.3216449022293091\n",
      "Epoch 880, Loss: 1.535867154598236, Final Batch Loss: 0.3656461536884308\n",
      "Epoch 881, Loss: 1.4670864045619965, Final Batch Loss: 0.37129339575767517\n",
      "Epoch 882, Loss: 1.547459602355957, Final Batch Loss: 0.3869478702545166\n",
      "Epoch 883, Loss: 1.5668607950210571, Final Batch Loss: 0.41122356057167053\n",
      "Epoch 884, Loss: 1.5288954377174377, Final Batch Loss: 0.3509639501571655\n",
      "Epoch 885, Loss: 1.625234842300415, Final Batch Loss: 0.4114954173564911\n",
      "Epoch 886, Loss: 1.4242544770240784, Final Batch Loss: 0.3301723301410675\n",
      "Epoch 887, Loss: 1.7452894747257233, Final Batch Loss: 0.6763905882835388\n",
      "Epoch 888, Loss: 1.6147948503494263, Final Batch Loss: 0.4606093466281891\n",
      "Epoch 889, Loss: 1.5193019360303879, Final Batch Loss: 0.24456889927387238\n",
      "Epoch 890, Loss: 1.4827591478824615, Final Batch Loss: 0.3769178092479706\n",
      "Epoch 891, Loss: 1.5344893634319305, Final Batch Loss: 0.3766305446624756\n",
      "Epoch 892, Loss: 1.507742553949356, Final Batch Loss: 0.33936938643455505\n",
      "Epoch 893, Loss: 1.5372788310050964, Final Batch Loss: 0.36338356137275696\n",
      "Epoch 894, Loss: 1.5127585232257843, Final Batch Loss: 0.296377569437027\n",
      "Epoch 895, Loss: 1.6202491223812103, Final Batch Loss: 0.44442304968833923\n",
      "Epoch 896, Loss: 1.465520054101944, Final Batch Loss: 0.3073684573173523\n",
      "Epoch 897, Loss: 1.5823251008987427, Final Batch Loss: 0.39642059803009033\n",
      "Epoch 898, Loss: 1.4468879103660583, Final Batch Loss: 0.4028206169605255\n",
      "Epoch 899, Loss: 1.5572055876255035, Final Batch Loss: 0.4017937183380127\n",
      "Epoch 900, Loss: 1.573091983795166, Final Batch Loss: 0.41916322708129883\n",
      "Epoch 901, Loss: 1.4606424272060394, Final Batch Loss: 0.300173282623291\n",
      "Epoch 902, Loss: 1.4769590497016907, Final Batch Loss: 0.2926415503025055\n",
      "Epoch 903, Loss: 1.6470772325992584, Final Batch Loss: 0.4871037006378174\n",
      "Epoch 904, Loss: 1.4797710180282593, Final Batch Loss: 0.3265826404094696\n",
      "Epoch 905, Loss: 1.288105234503746, Final Batch Loss: 0.20012636482715607\n",
      "Epoch 906, Loss: 1.4126032292842865, Final Batch Loss: 0.32495126128196716\n",
      "Epoch 907, Loss: 1.6366145610809326, Final Batch Loss: 0.4494379758834839\n",
      "Epoch 908, Loss: 1.5441566407680511, Final Batch Loss: 0.319659560918808\n",
      "Epoch 909, Loss: 1.7506577372550964, Final Batch Loss: 0.5120638012886047\n",
      "Epoch 910, Loss: 1.570253163576126, Final Batch Loss: 0.3865727484226227\n",
      "Epoch 911, Loss: 1.601146250963211, Final Batch Loss: 0.4453853666782379\n",
      "Epoch 912, Loss: 1.4583396017551422, Final Batch Loss: 0.28129786252975464\n",
      "Epoch 913, Loss: 1.7297513782978058, Final Batch Loss: 0.57243412733078\n",
      "Epoch 914, Loss: 1.5312528610229492, Final Batch Loss: 0.33275464177131653\n",
      "Epoch 915, Loss: 1.4811910688877106, Final Batch Loss: 0.29586172103881836\n",
      "Epoch 916, Loss: 1.5720085799694061, Final Batch Loss: 0.4017229974269867\n",
      "Epoch 917, Loss: 1.5578665733337402, Final Batch Loss: 0.42174088954925537\n",
      "Epoch 918, Loss: 1.4366643726825714, Final Batch Loss: 0.34845617413520813\n",
      "Epoch 919, Loss: 1.625081717967987, Final Batch Loss: 0.4992017447948456\n",
      "Epoch 920, Loss: 1.4546318650245667, Final Batch Loss: 0.33322980999946594\n",
      "Epoch 921, Loss: 1.4475176930427551, Final Batch Loss: 0.3544605076313019\n",
      "Epoch 922, Loss: 1.4400852620601654, Final Batch Loss: 0.34194615483283997\n",
      "Epoch 923, Loss: 1.5655554234981537, Final Batch Loss: 0.45356011390686035\n",
      "Epoch 924, Loss: 1.6640240550041199, Final Batch Loss: 0.529202401638031\n",
      "Epoch 925, Loss: 1.5880872309207916, Final Batch Loss: 0.44222643971443176\n",
      "Epoch 926, Loss: 1.563299983739853, Final Batch Loss: 0.39418768882751465\n",
      "Epoch 927, Loss: 1.5368717312812805, Final Batch Loss: 0.4291221797466278\n",
      "Epoch 928, Loss: 1.5184938311576843, Final Batch Loss: 0.42714062333106995\n",
      "Epoch 929, Loss: 1.6067902743816376, Final Batch Loss: 0.4826243221759796\n",
      "Epoch 930, Loss: 1.5999786257743835, Final Batch Loss: 0.5565119981765747\n",
      "Epoch 931, Loss: 1.4594610929489136, Final Batch Loss: 0.3485567569732666\n",
      "Epoch 932, Loss: 1.3840500116348267, Final Batch Loss: 0.3244982361793518\n",
      "Epoch 933, Loss: 1.5147310495376587, Final Batch Loss: 0.39842167496681213\n",
      "Epoch 934, Loss: 1.6407237946987152, Final Batch Loss: 0.4886636435985565\n",
      "Epoch 935, Loss: 1.582118034362793, Final Batch Loss: 0.3971613943576813\n",
      "Epoch 936, Loss: 1.6441640555858612, Final Batch Loss: 0.4210875332355499\n",
      "Epoch 937, Loss: 1.4963194727897644, Final Batch Loss: 0.3616136610507965\n",
      "Epoch 938, Loss: 1.5852023661136627, Final Batch Loss: 0.4852161407470703\n",
      "Epoch 939, Loss: 1.4535500407218933, Final Batch Loss: 0.3491773307323456\n",
      "Epoch 940, Loss: 1.3690586686134338, Final Batch Loss: 0.28154003620147705\n",
      "Epoch 941, Loss: 1.5089733600616455, Final Batch Loss: 0.44056233763694763\n",
      "Epoch 942, Loss: 1.7021672129631042, Final Batch Loss: 0.5293367505073547\n",
      "Epoch 943, Loss: 1.767807811498642, Final Batch Loss: 0.586129903793335\n",
      "Epoch 944, Loss: 1.6223969757556915, Final Batch Loss: 0.478900671005249\n",
      "Epoch 945, Loss: 1.4895571768283844, Final Batch Loss: 0.29824385046958923\n",
      "Epoch 946, Loss: 1.4558868408203125, Final Batch Loss: 0.35363903641700745\n",
      "Epoch 947, Loss: 1.3995153307914734, Final Batch Loss: 0.293119877576828\n",
      "Epoch 948, Loss: 1.4480009973049164, Final Batch Loss: 0.38604721426963806\n",
      "Epoch 949, Loss: 1.2888018116354942, Final Batch Loss: 0.1198003813624382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950, Loss: 1.2817013561725616, Final Batch Loss: 0.20523536205291748\n",
      "Epoch 951, Loss: 1.5197080373764038, Final Batch Loss: 0.36985841393470764\n",
      "Epoch 952, Loss: 1.7616770565509796, Final Batch Loss: 0.5396527647972107\n",
      "Epoch 953, Loss: 1.4697563350200653, Final Batch Loss: 0.27195778489112854\n",
      "Epoch 954, Loss: 1.2497799098491669, Final Batch Loss: 0.16708403825759888\n",
      "Epoch 955, Loss: 1.4730979204177856, Final Batch Loss: 0.3853257894515991\n",
      "Epoch 956, Loss: 1.5456006228923798, Final Batch Loss: 0.43428120017051697\n",
      "Epoch 957, Loss: 1.4922065138816833, Final Batch Loss: 0.32576635479927063\n",
      "Epoch 958, Loss: 1.4803130626678467, Final Batch Loss: 0.502004086971283\n",
      "Epoch 959, Loss: 1.36717027425766, Final Batch Loss: 0.29243728518486023\n",
      "Epoch 960, Loss: 1.622635692358017, Final Batch Loss: 0.5633317828178406\n",
      "Epoch 961, Loss: 1.5633420944213867, Final Batch Loss: 0.4158886671066284\n",
      "Epoch 962, Loss: 1.5446638464927673, Final Batch Loss: 0.45203062891960144\n",
      "Epoch 963, Loss: 1.3968352675437927, Final Batch Loss: 0.35101795196533203\n",
      "Epoch 964, Loss: 1.4186185896396637, Final Batch Loss: 0.3213564455509186\n",
      "Epoch 965, Loss: 1.2864821702241898, Final Batch Loss: 0.23261873424053192\n",
      "Epoch 966, Loss: 1.4971969723701477, Final Batch Loss: 0.37576988339424133\n",
      "Epoch 967, Loss: 1.4038425087928772, Final Batch Loss: 0.3700699806213379\n",
      "Epoch 968, Loss: 1.6612447500228882, Final Batch Loss: 0.5534127354621887\n",
      "Epoch 969, Loss: 1.68015456199646, Final Batch Loss: 0.490343302488327\n",
      "Epoch 970, Loss: 1.4673918336629868, Final Batch Loss: 0.24339057505130768\n",
      "Epoch 971, Loss: 1.5131720900535583, Final Batch Loss: 0.37484610080718994\n",
      "Epoch 972, Loss: 1.3769661486148834, Final Batch Loss: 0.309954971075058\n",
      "Epoch 973, Loss: 1.5191826820373535, Final Batch Loss: 0.33623167872428894\n",
      "Epoch 974, Loss: 1.5073590576648712, Final Batch Loss: 0.3724287748336792\n",
      "Epoch 975, Loss: 1.5642270743846893, Final Batch Loss: 0.4609408676624298\n",
      "Epoch 976, Loss: 1.4918610453605652, Final Batch Loss: 0.4487331807613373\n",
      "Epoch 977, Loss: 1.4383321404457092, Final Batch Loss: 0.2641398310661316\n",
      "Epoch 978, Loss: 1.4174477458000183, Final Batch Loss: 0.2902984917163849\n",
      "Epoch 979, Loss: 1.5196782946586609, Final Batch Loss: 0.4363515079021454\n",
      "Epoch 980, Loss: 1.399855613708496, Final Batch Loss: 0.30753961205482483\n",
      "Epoch 981, Loss: 1.4284590482711792, Final Batch Loss: 0.29739245772361755\n",
      "Epoch 982, Loss: 1.4405127167701721, Final Batch Loss: 0.31606772541999817\n",
      "Epoch 983, Loss: 1.5515808165073395, Final Batch Loss: 0.41294780373573303\n",
      "Epoch 984, Loss: 1.5719030499458313, Final Batch Loss: 0.43791165947914124\n",
      "Epoch 985, Loss: 1.5653732120990753, Final Batch Loss: 0.4552716910839081\n",
      "Epoch 986, Loss: 1.3532107919454575, Final Batch Loss: 0.23000897467136383\n",
      "Epoch 987, Loss: 1.4384250342845917, Final Batch Loss: 0.4019388258457184\n",
      "Epoch 988, Loss: 1.5095483362674713, Final Batch Loss: 0.4036855697631836\n",
      "Epoch 989, Loss: 1.500168800354004, Final Batch Loss: 0.3996390104293823\n",
      "Epoch 990, Loss: 1.559505581855774, Final Batch Loss: 0.3987247049808502\n",
      "Epoch 991, Loss: 1.542137086391449, Final Batch Loss: 0.4012179374694824\n",
      "Epoch 992, Loss: 1.2986424565315247, Final Batch Loss: 0.2506980001926422\n",
      "Epoch 993, Loss: 1.4065595865249634, Final Batch Loss: 0.378167062997818\n",
      "Epoch 994, Loss: 1.6929322183132172, Final Batch Loss: 0.5701225399971008\n",
      "Epoch 995, Loss: 1.372322291135788, Final Batch Loss: 0.33535292744636536\n",
      "Epoch 996, Loss: 1.5373808145523071, Final Batch Loss: 0.46151337027549744\n",
      "Epoch 997, Loss: 1.3736562728881836, Final Batch Loss: 0.28474047780036926\n",
      "Epoch 998, Loss: 1.592269241809845, Final Batch Loss: 0.44818398356437683\n",
      "Epoch 999, Loss: 1.447697013616562, Final Batch Loss: 0.3780999481678009\n",
      "Epoch 1000, Loss: 1.4705635607242584, Final Batch Loss: 0.28718915581703186\n",
      "Epoch 1001, Loss: 1.583318054676056, Final Batch Loss: 0.5093680024147034\n",
      "Epoch 1002, Loss: 1.4688242673873901, Final Batch Loss: 0.37674960494041443\n",
      "Epoch 1003, Loss: 1.5325675308704376, Final Batch Loss: 0.4124012887477875\n",
      "Epoch 1004, Loss: 1.3898242115974426, Final Batch Loss: 0.3164169490337372\n",
      "Epoch 1005, Loss: 1.5356556177139282, Final Batch Loss: 0.5188785195350647\n",
      "Epoch 1006, Loss: 1.3885735869407654, Final Batch Loss: 0.2165011763572693\n",
      "Epoch 1007, Loss: 1.5243625044822693, Final Batch Loss: 0.3788937032222748\n",
      "Epoch 1008, Loss: 1.4927010834217072, Final Batch Loss: 0.39224061369895935\n",
      "Epoch 1009, Loss: 1.2681556195020676, Final Batch Loss: 0.2113633006811142\n",
      "Epoch 1010, Loss: 1.5137560069561005, Final Batch Loss: 0.45294201374053955\n",
      "Epoch 1011, Loss: 1.304540529847145, Final Batch Loss: 0.23768936097621918\n",
      "Epoch 1012, Loss: 1.4730960428714752, Final Batch Loss: 0.3968835771083832\n",
      "Epoch 1013, Loss: 1.3061320930719376, Final Batch Loss: 0.22490651905536652\n",
      "Epoch 1014, Loss: 1.5792073607444763, Final Batch Loss: 0.49037542939186096\n",
      "Epoch 1015, Loss: 1.3570646047592163, Final Batch Loss: 0.22153335809707642\n",
      "Epoch 1016, Loss: 1.4973384141921997, Final Batch Loss: 0.37135303020477295\n",
      "Epoch 1017, Loss: 1.5608383119106293, Final Batch Loss: 0.44695234298706055\n",
      "Epoch 1018, Loss: 1.5427118837833405, Final Batch Loss: 0.3713909387588501\n",
      "Epoch 1019, Loss: 1.6166765987873077, Final Batch Loss: 0.4804896116256714\n",
      "Epoch 1020, Loss: 1.4199662804603577, Final Batch Loss: 0.29493480920791626\n",
      "Epoch 1021, Loss: 1.469982385635376, Final Batch Loss: 0.3824779689311981\n",
      "Epoch 1022, Loss: 1.340888351202011, Final Batch Loss: 0.34469637274742126\n",
      "Epoch 1023, Loss: 1.440981686115265, Final Batch Loss: 0.3705993890762329\n",
      "Epoch 1024, Loss: 1.2904409319162369, Final Batch Loss: 0.21432878077030182\n",
      "Epoch 1025, Loss: 1.5096425712108612, Final Batch Loss: 0.4069822132587433\n",
      "Epoch 1026, Loss: 1.4523314833641052, Final Batch Loss: 0.3554691970348358\n",
      "Epoch 1027, Loss: 1.6748753488063812, Final Batch Loss: 0.5812394618988037\n",
      "Epoch 1028, Loss: 1.6785250008106232, Final Batch Loss: 0.5062373280525208\n",
      "Epoch 1029, Loss: 1.425651103258133, Final Batch Loss: 0.30097395181655884\n",
      "Epoch 1030, Loss: 1.3389368057250977, Final Batch Loss: 0.31563857197761536\n",
      "Epoch 1031, Loss: 1.309032291173935, Final Batch Loss: 0.2486686110496521\n",
      "Epoch 1032, Loss: 1.5513981878757477, Final Batch Loss: 0.43093064427375793\n",
      "Epoch 1033, Loss: 1.5418500006198883, Final Batch Loss: 0.48698529601097107\n",
      "Epoch 1034, Loss: 1.464707612991333, Final Batch Loss: 0.4425729811191559\n",
      "Epoch 1035, Loss: 1.3257900476455688, Final Batch Loss: 0.24195700883865356\n",
      "Epoch 1036, Loss: 1.5097722113132477, Final Batch Loss: 0.38719379901885986\n",
      "Epoch 1037, Loss: 1.5439052283763885, Final Batch Loss: 0.5258767008781433\n",
      "Epoch 1038, Loss: 1.4024239480495453, Final Batch Loss: 0.3273581862449646\n",
      "Epoch 1039, Loss: 1.3609975576400757, Final Batch Loss: 0.303069144487381\n",
      "Epoch 1040, Loss: 1.3529501855373383, Final Batch Loss: 0.26236993074417114\n",
      "Epoch 1041, Loss: 1.5629435777664185, Final Batch Loss: 0.4571840465068817\n",
      "Epoch 1042, Loss: 1.4845167398452759, Final Batch Loss: 0.4024094343185425\n",
      "Epoch 1043, Loss: 1.2847385257482529, Final Batch Loss: 0.24103818833827972\n",
      "Epoch 1044, Loss: 1.4415812492370605, Final Batch Loss: 0.4080452620983124\n",
      "Epoch 1045, Loss: 1.3325804471969604, Final Batch Loss: 0.30426493287086487\n",
      "Epoch 1046, Loss: 1.3591667711734772, Final Batch Loss: 0.32020285725593567\n",
      "Epoch 1047, Loss: 1.4908653795719147, Final Batch Loss: 0.3294130265712738\n",
      "Epoch 1048, Loss: 1.617453247308731, Final Batch Loss: 0.4653158187866211\n",
      "Epoch 1049, Loss: 1.271128624677658, Final Batch Loss: 0.29420599341392517\n",
      "Epoch 1050, Loss: 1.5546446442604065, Final Batch Loss: 0.4602414071559906\n",
      "Epoch 1051, Loss: 1.580923080444336, Final Batch Loss: 0.4739164412021637\n",
      "Epoch 1052, Loss: 1.3380976170301437, Final Batch Loss: 0.24180342257022858\n",
      "Epoch 1053, Loss: 1.7358971536159515, Final Batch Loss: 0.6620102524757385\n",
      "Epoch 1054, Loss: 1.5141283869743347, Final Batch Loss: 0.41157957911491394\n",
      "Epoch 1055, Loss: 1.4607920050621033, Final Batch Loss: 0.4640533924102783\n",
      "Epoch 1056, Loss: 1.5784164369106293, Final Batch Loss: 0.47570785880088806\n",
      "Epoch 1057, Loss: 1.3099547028541565, Final Batch Loss: 0.2568971514701843\n",
      "Epoch 1058, Loss: 1.5019596815109253, Final Batch Loss: 0.36765968799591064\n",
      "Epoch 1059, Loss: 1.5020668506622314, Final Batch Loss: 0.3526564836502075\n",
      "Epoch 1060, Loss: 1.3887674808502197, Final Batch Loss: 0.26408651471138\n",
      "Epoch 1061, Loss: 1.3567368388175964, Final Batch Loss: 0.321200430393219\n",
      "Epoch 1062, Loss: 1.5172515511512756, Final Batch Loss: 0.47133591771125793\n",
      "Epoch 1063, Loss: 1.5360837876796722, Final Batch Loss: 0.4656813144683838\n",
      "Epoch 1064, Loss: 1.5136840343475342, Final Batch Loss: 0.4762692451477051\n",
      "Epoch 1065, Loss: 1.5069338977336884, Final Batch Loss: 0.3693736791610718\n",
      "Epoch 1066, Loss: 1.5165864825248718, Final Batch Loss: 0.4994504153728485\n",
      "Epoch 1067, Loss: 1.5137250125408173, Final Batch Loss: 0.5025818347930908\n",
      "Epoch 1068, Loss: 1.5493204295635223, Final Batch Loss: 0.43983975052833557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1069, Loss: 1.449997067451477, Final Batch Loss: 0.29812589287757874\n",
      "Epoch 1070, Loss: 1.5781706869602203, Final Batch Loss: 0.4253939092159271\n",
      "Epoch 1071, Loss: 1.309017226099968, Final Batch Loss: 0.23423133790493011\n",
      "Epoch 1072, Loss: 1.5021711885929108, Final Batch Loss: 0.462333083152771\n",
      "Epoch 1073, Loss: 1.3540350794792175, Final Batch Loss: 0.32350683212280273\n",
      "Epoch 1074, Loss: 1.4585977792739868, Final Batch Loss: 0.3969514071941376\n",
      "Epoch 1075, Loss: 1.349179983139038, Final Batch Loss: 0.34182462096214294\n",
      "Epoch 1076, Loss: 1.3815447390079498, Final Batch Loss: 0.2359999418258667\n",
      "Epoch 1077, Loss: 1.4026933908462524, Final Batch Loss: 0.3184632956981659\n",
      "Epoch 1078, Loss: 1.574257642030716, Final Batch Loss: 0.49301013350486755\n",
      "Epoch 1079, Loss: 1.3783433437347412, Final Batch Loss: 0.3317658603191376\n",
      "Epoch 1080, Loss: 1.5138502418994904, Final Batch Loss: 0.49461209774017334\n",
      "Epoch 1081, Loss: 1.4162605702877045, Final Batch Loss: 0.3491375148296356\n",
      "Epoch 1082, Loss: 1.2314637899398804, Final Batch Loss: 0.26031559705734253\n",
      "Epoch 1083, Loss: 1.5564717650413513, Final Batch Loss: 0.46534669399261475\n",
      "Epoch 1084, Loss: 1.5288792550563812, Final Batch Loss: 0.3982146978378296\n",
      "Epoch 1085, Loss: 1.6483349800109863, Final Batch Loss: 0.5271508097648621\n",
      "Epoch 1086, Loss: 1.2931763529777527, Final Batch Loss: 0.25758665800094604\n",
      "Epoch 1087, Loss: 1.4606586396694183, Final Batch Loss: 0.40378573536872864\n",
      "Epoch 1088, Loss: 1.3689877539873123, Final Batch Loss: 0.16621096432209015\n",
      "Epoch 1089, Loss: 1.4472678899765015, Final Batch Loss: 0.397204726934433\n",
      "Epoch 1090, Loss: 1.565766990184784, Final Batch Loss: 0.47925451397895813\n",
      "Epoch 1091, Loss: 1.3836578726768494, Final Batch Loss: 0.36130833625793457\n",
      "Epoch 1092, Loss: 1.3925550878047943, Final Batch Loss: 0.31276798248291016\n",
      "Epoch 1093, Loss: 1.396047443151474, Final Batch Loss: 0.2982005178928375\n",
      "Epoch 1094, Loss: 1.3825438022613525, Final Batch Loss: 0.3819098174571991\n",
      "Epoch 1095, Loss: 1.299269512295723, Final Batch Loss: 0.17915432155132294\n",
      "Epoch 1096, Loss: 1.4906117022037506, Final Batch Loss: 0.4151977002620697\n",
      "Epoch 1097, Loss: 1.4608599841594696, Final Batch Loss: 0.4455529749393463\n",
      "Epoch 1098, Loss: 1.367644339799881, Final Batch Loss: 0.28787538409233093\n",
      "Epoch 1099, Loss: 1.4258624911308289, Final Batch Loss: 0.4059092700481415\n",
      "Epoch 1100, Loss: 1.2667693048715591, Final Batch Loss: 0.23631228506565094\n",
      "Epoch 1101, Loss: 1.5289321541786194, Final Batch Loss: 0.46258047223091125\n",
      "Epoch 1102, Loss: 1.368080034852028, Final Batch Loss: 0.24856697022914886\n",
      "Epoch 1103, Loss: 1.3081960678100586, Final Batch Loss: 0.2731054723262787\n",
      "Epoch 1104, Loss: 1.5147367119789124, Final Batch Loss: 0.4907158613204956\n",
      "Epoch 1105, Loss: 1.295715183019638, Final Batch Loss: 0.3064627945423126\n",
      "Epoch 1106, Loss: 1.6295274794101715, Final Batch Loss: 0.6258805394172668\n",
      "Epoch 1107, Loss: 1.42179337143898, Final Batch Loss: 0.3062497079372406\n",
      "Epoch 1108, Loss: 1.505534291267395, Final Batch Loss: 0.5210844874382019\n",
      "Epoch 1109, Loss: 1.3984390497207642, Final Batch Loss: 0.3148030638694763\n",
      "Epoch 1110, Loss: 1.3831391632556915, Final Batch Loss: 0.3140212595462799\n",
      "Epoch 1111, Loss: 1.358111321926117, Final Batch Loss: 0.345749169588089\n",
      "Epoch 1112, Loss: 1.3194485008716583, Final Batch Loss: 0.29877325892448425\n",
      "Epoch 1113, Loss: 1.453661322593689, Final Batch Loss: 0.342562198638916\n",
      "Epoch 1114, Loss: 1.3923635482788086, Final Batch Loss: 0.3277888000011444\n",
      "Epoch 1115, Loss: 1.2622868865728378, Final Batch Loss: 0.20121853053569794\n",
      "Epoch 1116, Loss: 1.5277095139026642, Final Batch Loss: 0.47998595237731934\n",
      "Epoch 1117, Loss: 1.3384191989898682, Final Batch Loss: 0.28933650255203247\n",
      "Epoch 1118, Loss: 1.507377564907074, Final Batch Loss: 0.3562107980251312\n",
      "Epoch 1119, Loss: 1.2147045284509659, Final Batch Loss: 0.1529189795255661\n",
      "Epoch 1120, Loss: 1.3156831562519073, Final Batch Loss: 0.26325175166130066\n",
      "Epoch 1121, Loss: 1.3884607553482056, Final Batch Loss: 0.2931101322174072\n",
      "Epoch 1122, Loss: 1.5127486884593964, Final Batch Loss: 0.3071941137313843\n",
      "Epoch 1123, Loss: 1.3735241889953613, Final Batch Loss: 0.38341593742370605\n",
      "Epoch 1124, Loss: 1.643929362297058, Final Batch Loss: 0.5755108594894409\n",
      "Epoch 1125, Loss: 1.3357426226139069, Final Batch Loss: 0.3758259117603302\n",
      "Epoch 1126, Loss: 1.5644678473472595, Final Batch Loss: 0.5537877678871155\n",
      "Epoch 1127, Loss: 1.4645963609218597, Final Batch Loss: 0.38518407940864563\n",
      "Epoch 1128, Loss: 1.3580602258443832, Final Batch Loss: 0.22906626760959625\n",
      "Epoch 1129, Loss: 1.374975472688675, Final Batch Loss: 0.29778847098350525\n",
      "Epoch 1130, Loss: 1.405167579650879, Final Batch Loss: 0.40457168221473694\n",
      "Epoch 1131, Loss: 1.277338519692421, Final Batch Loss: 0.22834797203540802\n",
      "Epoch 1132, Loss: 1.374882459640503, Final Batch Loss: 0.3129168152809143\n",
      "Epoch 1133, Loss: 1.314501792192459, Final Batch Loss: 0.2268252968788147\n",
      "Epoch 1134, Loss: 1.3104754984378815, Final Batch Loss: 0.29116156697273254\n",
      "Epoch 1135, Loss: 1.3248088359832764, Final Batch Loss: 0.2768177092075348\n",
      "Epoch 1136, Loss: 1.426629513502121, Final Batch Loss: 0.36390987038612366\n",
      "Epoch 1137, Loss: 1.4124750792980194, Final Batch Loss: 0.3751051425933838\n",
      "Epoch 1138, Loss: 1.3743345737457275, Final Batch Loss: 0.3551781177520752\n",
      "Epoch 1139, Loss: 1.4205023050308228, Final Batch Loss: 0.37923625111579895\n",
      "Epoch 1140, Loss: 1.6227152943611145, Final Batch Loss: 0.4794979989528656\n",
      "Epoch 1141, Loss: 1.50336953997612, Final Batch Loss: 0.5197354555130005\n",
      "Epoch 1142, Loss: 1.5058323740959167, Final Batch Loss: 0.43128475546836853\n",
      "Epoch 1143, Loss: 1.325156956911087, Final Batch Loss: 0.27023518085479736\n",
      "Epoch 1144, Loss: 1.4468745291233063, Final Batch Loss: 0.42140400409698486\n",
      "Epoch 1145, Loss: 1.3875584602355957, Final Batch Loss: 0.267494797706604\n",
      "Epoch 1146, Loss: 1.4242965281009674, Final Batch Loss: 0.30795547366142273\n",
      "Epoch 1147, Loss: 1.4104254245758057, Final Batch Loss: 0.2967076301574707\n",
      "Epoch 1148, Loss: 1.3444508016109467, Final Batch Loss: 0.26095715165138245\n",
      "Epoch 1149, Loss: 1.3620473742485046, Final Batch Loss: 0.3081439733505249\n",
      "Epoch 1150, Loss: 1.3653535842895508, Final Batch Loss: 0.3726547956466675\n",
      "Epoch 1151, Loss: 1.3939028680324554, Final Batch Loss: 0.31935998797416687\n",
      "Epoch 1152, Loss: 1.4956320822238922, Final Batch Loss: 0.3637279272079468\n",
      "Epoch 1153, Loss: 1.4932034015655518, Final Batch Loss: 0.5044063925743103\n",
      "Epoch 1154, Loss: 1.3549177944660187, Final Batch Loss: 0.3081805408000946\n",
      "Epoch 1155, Loss: 1.5662139356136322, Final Batch Loss: 0.5474079251289368\n",
      "Epoch 1156, Loss: 1.423611432313919, Final Batch Loss: 0.37605372071266174\n",
      "Epoch 1157, Loss: 1.2936763763427734, Final Batch Loss: 0.25363340973854065\n",
      "Epoch 1158, Loss: 1.4221519529819489, Final Batch Loss: 0.4453704357147217\n",
      "Epoch 1159, Loss: 1.4525854587554932, Final Batch Loss: 0.4033897817134857\n",
      "Epoch 1160, Loss: 1.3174259811639786, Final Batch Loss: 0.24905849993228912\n",
      "Epoch 1161, Loss: 1.5006902515888214, Final Batch Loss: 0.47744831442832947\n",
      "Epoch 1162, Loss: 1.4906057715415955, Final Batch Loss: 0.4825625419616699\n",
      "Epoch 1163, Loss: 1.2450381815433502, Final Batch Loss: 0.29819217324256897\n",
      "Epoch 1164, Loss: 1.3527919352054596, Final Batch Loss: 0.3078048825263977\n",
      "Epoch 1165, Loss: 1.2769345045089722, Final Batch Loss: 0.2920648455619812\n",
      "Epoch 1166, Loss: 1.537104308605194, Final Batch Loss: 0.46053025126457214\n",
      "Epoch 1167, Loss: 1.5096318423748016, Final Batch Loss: 0.44915878772735596\n",
      "Epoch 1168, Loss: 1.3467093110084534, Final Batch Loss: 0.32056543231010437\n",
      "Epoch 1169, Loss: 1.5012189149856567, Final Batch Loss: 0.43668147921562195\n",
      "Epoch 1170, Loss: 1.4038690030574799, Final Batch Loss: 0.33457422256469727\n",
      "Epoch 1171, Loss: 1.4501716196537018, Final Batch Loss: 0.4406471252441406\n",
      "Epoch 1172, Loss: 1.4208855628967285, Final Batch Loss: 0.41312870383262634\n",
      "Epoch 1173, Loss: 1.2979112267494202, Final Batch Loss: 0.29800647497177124\n",
      "Epoch 1174, Loss: 1.2287687808275223, Final Batch Loss: 0.15410025417804718\n",
      "Epoch 1175, Loss: 1.2931089103221893, Final Batch Loss: 0.2651112973690033\n",
      "Epoch 1176, Loss: 1.4138000309467316, Final Batch Loss: 0.43860968947410583\n",
      "Epoch 1177, Loss: 1.2645147740840912, Final Batch Loss: 0.32868993282318115\n",
      "Epoch 1178, Loss: 1.2249534726142883, Final Batch Loss: 0.32206353545188904\n",
      "Epoch 1179, Loss: 1.3308829367160797, Final Batch Loss: 0.2929740250110626\n",
      "Epoch 1180, Loss: 1.4452168345451355, Final Batch Loss: 0.4545201063156128\n",
      "Epoch 1181, Loss: 1.4654208719730377, Final Batch Loss: 0.48936188220977783\n",
      "Epoch 1182, Loss: 1.3821067214012146, Final Batch Loss: 0.37594613432884216\n",
      "Epoch 1183, Loss: 1.463011771440506, Final Batch Loss: 0.4373248815536499\n",
      "Epoch 1184, Loss: 1.3065580427646637, Final Batch Loss: 0.23046189546585083\n",
      "Epoch 1185, Loss: 1.2168578654527664, Final Batch Loss: 0.24781860411167145\n",
      "Epoch 1186, Loss: 1.4981995820999146, Final Batch Loss: 0.5471829771995544\n",
      "Epoch 1187, Loss: 1.4755015969276428, Final Batch Loss: 0.4460770785808563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1188, Loss: 1.4755988121032715, Final Batch Loss: 0.41522547602653503\n",
      "Epoch 1189, Loss: 1.4583194553852081, Final Batch Loss: 0.3471967279911041\n",
      "Epoch 1190, Loss: 1.298824429512024, Final Batch Loss: 0.22980928421020508\n",
      "Epoch 1191, Loss: 1.3460810482501984, Final Batch Loss: 0.36872291564941406\n",
      "Epoch 1192, Loss: 1.5283059179782867, Final Batch Loss: 0.4888511002063751\n",
      "Epoch 1193, Loss: 1.3124442398548126, Final Batch Loss: 0.30700814723968506\n",
      "Epoch 1194, Loss: 1.4573800563812256, Final Batch Loss: 0.399107962846756\n",
      "Epoch 1195, Loss: 1.4998181462287903, Final Batch Loss: 0.3558969795703888\n",
      "Epoch 1196, Loss: 1.429944932460785, Final Batch Loss: 0.3497330844402313\n",
      "Epoch 1197, Loss: 1.3137995898723602, Final Batch Loss: 0.30245283246040344\n",
      "Epoch 1198, Loss: 1.2775560021400452, Final Batch Loss: 0.2724255621433258\n",
      "Epoch 1199, Loss: 1.3091390430927277, Final Batch Loss: 0.267907053232193\n",
      "Epoch 1200, Loss: 1.195293053984642, Final Batch Loss: 0.16222648322582245\n",
      "Epoch 1201, Loss: 1.3153096735477448, Final Batch Loss: 0.33996954560279846\n",
      "Epoch 1202, Loss: 1.7532284259796143, Final Batch Loss: 0.6466395258903503\n",
      "Epoch 1203, Loss: 1.3700651228427887, Final Batch Loss: 0.26712337136268616\n",
      "Epoch 1204, Loss: 1.2546802163124084, Final Batch Loss: 0.20071792602539062\n",
      "Epoch 1205, Loss: 1.3834484219551086, Final Batch Loss: 0.43825313448905945\n",
      "Epoch 1206, Loss: 1.4067614078521729, Final Batch Loss: 0.3687940537929535\n",
      "Epoch 1207, Loss: 1.408880203962326, Final Batch Loss: 0.2907862365245819\n",
      "Epoch 1208, Loss: 1.426713615655899, Final Batch Loss: 0.35558855533599854\n",
      "Epoch 1209, Loss: 1.3095330595970154, Final Batch Loss: 0.33960381150245667\n",
      "Epoch 1210, Loss: 1.2363260090351105, Final Batch Loss: 0.2734818458557129\n",
      "Epoch 1211, Loss: 1.4416771829128265, Final Batch Loss: 0.4681779146194458\n",
      "Epoch 1212, Loss: 1.5916852056980133, Final Batch Loss: 0.5305323600769043\n",
      "Epoch 1213, Loss: 1.151934489607811, Final Batch Loss: 0.1613849550485611\n",
      "Epoch 1214, Loss: 1.3370088338851929, Final Batch Loss: 0.37235236167907715\n",
      "Epoch 1215, Loss: 1.451543241739273, Final Batch Loss: 0.5154543519020081\n",
      "Epoch 1216, Loss: 1.4577215313911438, Final Batch Loss: 0.34353911876678467\n",
      "Epoch 1217, Loss: 1.348337471485138, Final Batch Loss: 0.36450862884521484\n",
      "Epoch 1218, Loss: 1.3069278001785278, Final Batch Loss: 0.3181382417678833\n",
      "Epoch 1219, Loss: 1.4090081453323364, Final Batch Loss: 0.3538943827152252\n",
      "Epoch 1220, Loss: 1.3187256157398224, Final Batch Loss: 0.3318137526512146\n",
      "Epoch 1221, Loss: 1.395570456981659, Final Batch Loss: 0.3094080984592438\n",
      "Epoch 1222, Loss: 1.4364924728870392, Final Batch Loss: 0.27895888686180115\n",
      "Epoch 1223, Loss: 1.36703422665596, Final Batch Loss: 0.2733787000179291\n",
      "Epoch 1224, Loss: 1.3466026782989502, Final Batch Loss: 0.32645437121391296\n",
      "Epoch 1225, Loss: 1.5491018891334534, Final Batch Loss: 0.615591824054718\n",
      "Epoch 1226, Loss: 1.4964637160301208, Final Batch Loss: 0.5176488161087036\n",
      "Epoch 1227, Loss: 1.3527876436710358, Final Batch Loss: 0.3939903676509857\n",
      "Epoch 1228, Loss: 1.4703924059867859, Final Batch Loss: 0.32097581028938293\n",
      "Epoch 1229, Loss: 1.1737250536680222, Final Batch Loss: 0.1714548021554947\n",
      "Epoch 1230, Loss: 1.413500815629959, Final Batch Loss: 0.3307107388973236\n",
      "Epoch 1231, Loss: 1.382846862077713, Final Batch Loss: 0.3855734169483185\n",
      "Epoch 1232, Loss: 1.3494081497192383, Final Batch Loss: 0.43830540776252747\n",
      "Epoch 1233, Loss: 1.4962091445922852, Final Batch Loss: 0.4475497007369995\n",
      "Epoch 1234, Loss: 1.3401631712913513, Final Batch Loss: 0.3509227931499481\n",
      "Epoch 1235, Loss: 1.381512701511383, Final Batch Loss: 0.34985411167144775\n",
      "Epoch 1236, Loss: 1.3181058168411255, Final Batch Loss: 0.3249763548374176\n",
      "Epoch 1237, Loss: 1.2493517398834229, Final Batch Loss: 0.24858301877975464\n",
      "Epoch 1238, Loss: 1.2943408489227295, Final Batch Loss: 0.32664263248443604\n",
      "Epoch 1239, Loss: 1.2073371708393097, Final Batch Loss: 0.27543753385543823\n",
      "Epoch 1240, Loss: 1.5413711369037628, Final Batch Loss: 0.5011319518089294\n",
      "Epoch 1241, Loss: 1.3646253943443298, Final Batch Loss: 0.44559481739997864\n",
      "Epoch 1242, Loss: 1.5647406578063965, Final Batch Loss: 0.5020121932029724\n",
      "Epoch 1243, Loss: 1.3759543299674988, Final Batch Loss: 0.39683911204338074\n",
      "Epoch 1244, Loss: 1.3781817853450775, Final Batch Loss: 0.3900095522403717\n",
      "Epoch 1245, Loss: 1.4171937704086304, Final Batch Loss: 0.3158000409603119\n",
      "Epoch 1246, Loss: 1.3837426900863647, Final Batch Loss: 0.3869950771331787\n",
      "Epoch 1247, Loss: 1.343588411808014, Final Batch Loss: 0.37519630789756775\n",
      "Epoch 1248, Loss: 1.4339747428894043, Final Batch Loss: 0.35555174946784973\n",
      "Epoch 1249, Loss: 1.3912543654441833, Final Batch Loss: 0.421405553817749\n",
      "Epoch 1250, Loss: 1.4502713978290558, Final Batch Loss: 0.40472424030303955\n",
      "Epoch 1251, Loss: 1.262489065527916, Final Batch Loss: 0.24293340742588043\n",
      "Epoch 1252, Loss: 1.4128336906433105, Final Batch Loss: 0.37922772765159607\n",
      "Epoch 1253, Loss: 1.3574137687683105, Final Batch Loss: 0.32986587285995483\n",
      "Epoch 1254, Loss: 1.2702268660068512, Final Batch Loss: 0.32571929693222046\n",
      "Epoch 1255, Loss: 1.2901200950145721, Final Batch Loss: 0.34043705463409424\n",
      "Epoch 1256, Loss: 1.3962301015853882, Final Batch Loss: 0.2725810706615448\n",
      "Epoch 1257, Loss: 1.1335867941379547, Final Batch Loss: 0.17903077602386475\n",
      "Epoch 1258, Loss: 1.3539289236068726, Final Batch Loss: 0.301701158285141\n",
      "Epoch 1259, Loss: 1.3643009960651398, Final Batch Loss: 0.2894250452518463\n",
      "Epoch 1260, Loss: 1.488779753446579, Final Batch Loss: 0.47524455189704895\n",
      "Epoch 1261, Loss: 1.347478449344635, Final Batch Loss: 0.3939584195613861\n",
      "Epoch 1262, Loss: 1.2297939658164978, Final Batch Loss: 0.1905137300491333\n",
      "Epoch 1263, Loss: 1.353408306837082, Final Batch Loss: 0.32795584201812744\n",
      "Epoch 1264, Loss: 1.4155275225639343, Final Batch Loss: 0.393383264541626\n",
      "Epoch 1265, Loss: 1.229772910475731, Final Batch Loss: 0.23131375014781952\n",
      "Epoch 1266, Loss: 1.4276485443115234, Final Batch Loss: 0.3744376599788666\n",
      "Epoch 1267, Loss: 1.573321908712387, Final Batch Loss: 0.5800849199295044\n",
      "Epoch 1268, Loss: 1.321708232164383, Final Batch Loss: 0.35673508048057556\n",
      "Epoch 1269, Loss: 1.297385960817337, Final Batch Loss: 0.27023613452911377\n",
      "Epoch 1270, Loss: 1.293193519115448, Final Batch Loss: 0.2532172203063965\n",
      "Epoch 1271, Loss: 1.3460386097431183, Final Batch Loss: 0.31720736622810364\n",
      "Epoch 1272, Loss: 1.6762025356292725, Final Batch Loss: 0.560046374797821\n",
      "Epoch 1273, Loss: 1.2591787427663803, Final Batch Loss: 0.3005705773830414\n",
      "Epoch 1274, Loss: 1.2565439939498901, Final Batch Loss: 0.2691102921962738\n",
      "Epoch 1275, Loss: 1.2248475253582, Final Batch Loss: 0.3089231252670288\n",
      "Epoch 1276, Loss: 1.3182849287986755, Final Batch Loss: 0.3085491359233856\n",
      "Epoch 1277, Loss: 1.384348750114441, Final Batch Loss: 0.31169331073760986\n",
      "Epoch 1278, Loss: 1.2399930953979492, Final Batch Loss: 0.2216130495071411\n",
      "Epoch 1279, Loss: 1.7936919033527374, Final Batch Loss: 0.9000832438468933\n",
      "Epoch 1280, Loss: 1.3131764084100723, Final Batch Loss: 0.3883577287197113\n",
      "Epoch 1281, Loss: 1.2268158495426178, Final Batch Loss: 0.2731885015964508\n",
      "Epoch 1282, Loss: 1.2933025658130646, Final Batch Loss: 0.286106675863266\n",
      "Epoch 1283, Loss: 1.3540220856666565, Final Batch Loss: 0.2543505132198334\n",
      "Epoch 1284, Loss: 1.3300094306468964, Final Batch Loss: 0.3269909918308258\n",
      "Epoch 1285, Loss: 1.3284363746643066, Final Batch Loss: 0.3466031551361084\n",
      "Epoch 1286, Loss: 1.4459431767463684, Final Batch Loss: 0.516913115978241\n",
      "Epoch 1287, Loss: 1.3598250150680542, Final Batch Loss: 0.3408474028110504\n",
      "Epoch 1288, Loss: 1.188302993774414, Final Batch Loss: 0.19520121812820435\n",
      "Epoch 1289, Loss: 1.2436400353908539, Final Batch Loss: 0.20374006032943726\n",
      "Epoch 1290, Loss: 1.1955014765262604, Final Batch Loss: 0.19475460052490234\n",
      "Epoch 1291, Loss: 1.2781681567430496, Final Batch Loss: 0.2471148818731308\n",
      "Epoch 1292, Loss: 1.155946210026741, Final Batch Loss: 0.18485857546329498\n",
      "Epoch 1293, Loss: 1.2090524584054947, Final Batch Loss: 0.23163209855556488\n",
      "Epoch 1294, Loss: 1.334469497203827, Final Batch Loss: 0.3486848771572113\n",
      "Epoch 1295, Loss: 1.3171119689941406, Final Batch Loss: 0.43900421261787415\n",
      "Epoch 1296, Loss: 1.3315496444702148, Final Batch Loss: 0.3800565302371979\n",
      "Epoch 1297, Loss: 1.308390736579895, Final Batch Loss: 0.41235658526420593\n",
      "Epoch 1298, Loss: 1.3714503645896912, Final Batch Loss: 0.3783823251724243\n",
      "Epoch 1299, Loss: 1.2887710332870483, Final Batch Loss: 0.28496477007865906\n",
      "Epoch 1300, Loss: 1.2883678078651428, Final Batch Loss: 0.3324190080165863\n",
      "Epoch 1301, Loss: 1.498155564069748, Final Batch Loss: 0.523895800113678\n",
      "Epoch 1302, Loss: 1.3127309083938599, Final Batch Loss: 0.3436777889728546\n",
      "Epoch 1303, Loss: 1.286168247461319, Final Batch Loss: 0.2554529905319214\n",
      "Epoch 1304, Loss: 1.3225851655006409, Final Batch Loss: 0.3962349593639374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1305, Loss: 1.2415606528520584, Final Batch Loss: 0.21458174288272858\n",
      "Epoch 1306, Loss: 1.366644412279129, Final Batch Loss: 0.353705495595932\n",
      "Epoch 1307, Loss: 1.3542907238006592, Final Batch Loss: 0.3427448570728302\n",
      "Epoch 1308, Loss: 1.280475988984108, Final Batch Loss: 0.24544526636600494\n",
      "Epoch 1309, Loss: 1.3145692348480225, Final Batch Loss: 0.42206504940986633\n",
      "Epoch 1310, Loss: 1.2442958652973175, Final Batch Loss: 0.3306504487991333\n",
      "Epoch 1311, Loss: 1.1909892559051514, Final Batch Loss: 0.16398659348487854\n",
      "Epoch 1312, Loss: 1.3626370131969452, Final Batch Loss: 0.4924405813217163\n",
      "Epoch 1313, Loss: 1.2723204791545868, Final Batch Loss: 0.32230526208877563\n",
      "Epoch 1314, Loss: 1.4435847401618958, Final Batch Loss: 0.37575340270996094\n",
      "Epoch 1315, Loss: 1.4097589552402496, Final Batch Loss: 0.42229655385017395\n",
      "Epoch 1316, Loss: 1.2747739553451538, Final Batch Loss: 0.3299253284931183\n",
      "Epoch 1317, Loss: 1.1798352599143982, Final Batch Loss: 0.2282804250717163\n",
      "Epoch 1318, Loss: 1.2154263854026794, Final Batch Loss: 0.302011638879776\n",
      "Epoch 1319, Loss: 1.2013979703187943, Final Batch Loss: 0.24911411106586456\n",
      "Epoch 1320, Loss: 1.2455612272024155, Final Batch Loss: 0.20133094489574432\n",
      "Epoch 1321, Loss: 1.3707616925239563, Final Batch Loss: 0.44272831082344055\n",
      "Epoch 1322, Loss: 1.5072535574436188, Final Batch Loss: 0.5515236854553223\n",
      "Epoch 1323, Loss: 1.3094960749149323, Final Batch Loss: 0.2829025089740753\n",
      "Epoch 1324, Loss: 1.2192509770393372, Final Batch Loss: 0.3298433721065521\n",
      "Epoch 1325, Loss: 1.3449784219264984, Final Batch Loss: 0.40077972412109375\n",
      "Epoch 1326, Loss: 1.4241948425769806, Final Batch Loss: 0.39169028401374817\n",
      "Epoch 1327, Loss: 1.4731889218091965, Final Batch Loss: 0.5422685146331787\n",
      "Epoch 1328, Loss: 1.3032113760709763, Final Batch Loss: 0.24877874553203583\n",
      "Epoch 1329, Loss: 1.181069791316986, Final Batch Loss: 0.25421854853630066\n",
      "Epoch 1330, Loss: 1.4079303741455078, Final Batch Loss: 0.414878249168396\n",
      "Epoch 1331, Loss: 1.4012739062309265, Final Batch Loss: 0.21591347455978394\n",
      "Epoch 1332, Loss: 1.5208736658096313, Final Batch Loss: 0.5098150372505188\n",
      "Epoch 1333, Loss: 1.2826626896858215, Final Batch Loss: 0.26654282212257385\n",
      "Epoch 1334, Loss: 1.3642280995845795, Final Batch Loss: 0.3931701183319092\n",
      "Epoch 1335, Loss: 1.392184555530548, Final Batch Loss: 0.494169145822525\n",
      "Epoch 1336, Loss: 1.2424497604370117, Final Batch Loss: 0.29451611638069153\n",
      "Epoch 1337, Loss: 1.3664533197879791, Final Batch Loss: 0.4300400912761688\n",
      "Epoch 1338, Loss: 1.2716107964515686, Final Batch Loss: 0.31875261664390564\n",
      "Epoch 1339, Loss: 1.2987322211265564, Final Batch Loss: 0.26238569617271423\n",
      "Epoch 1340, Loss: 1.2265892922878265, Final Batch Loss: 0.2897810935974121\n",
      "Epoch 1341, Loss: 1.3914073407649994, Final Batch Loss: 0.4411587417125702\n",
      "Epoch 1342, Loss: 1.1103569120168686, Final Batch Loss: 0.20570914447307587\n",
      "Epoch 1343, Loss: 1.2229826301336288, Final Batch Loss: 0.21833403408527374\n",
      "Epoch 1344, Loss: 1.2096602320671082, Final Batch Loss: 0.25227734446525574\n",
      "Epoch 1345, Loss: 1.2367470115423203, Final Batch Loss: 0.18610937893390656\n",
      "Epoch 1346, Loss: 1.2812325954437256, Final Batch Loss: 0.28187882900238037\n",
      "Epoch 1347, Loss: 1.3339645564556122, Final Batch Loss: 0.2842704653739929\n",
      "Epoch 1348, Loss: 1.409703016281128, Final Batch Loss: 0.36337578296661377\n",
      "Epoch 1349, Loss: 1.2621623873710632, Final Batch Loss: 0.28075674176216125\n",
      "Epoch 1350, Loss: 1.2909093797206879, Final Batch Loss: 0.31542763113975525\n",
      "Epoch 1351, Loss: 1.339171439409256, Final Batch Loss: 0.31635621190071106\n",
      "Epoch 1352, Loss: 1.3141824901103973, Final Batch Loss: 0.3257685899734497\n",
      "Epoch 1353, Loss: 1.3273492753505707, Final Batch Loss: 0.2828056514263153\n",
      "Epoch 1354, Loss: 1.3878163397312164, Final Batch Loss: 0.4440804719924927\n",
      "Epoch 1355, Loss: 1.2638209760189056, Final Batch Loss: 0.26766306161880493\n",
      "Epoch 1356, Loss: 1.276442512869835, Final Batch Loss: 0.21655188500881195\n",
      "Epoch 1357, Loss: 1.2285216301679611, Final Batch Loss: 0.22194986045360565\n",
      "Epoch 1358, Loss: 1.4306608140468597, Final Batch Loss: 0.507466733455658\n",
      "Epoch 1359, Loss: 1.4149636179208755, Final Batch Loss: 0.4297035038471222\n",
      "Epoch 1360, Loss: 1.3858715891838074, Final Batch Loss: 0.45069754123687744\n",
      "Epoch 1361, Loss: 1.318615436553955, Final Batch Loss: 0.3450559079647064\n",
      "Epoch 1362, Loss: 1.338578224182129, Final Batch Loss: 0.33336591720581055\n",
      "Epoch 1363, Loss: 1.2727034240961075, Final Batch Loss: 0.1857234090566635\n",
      "Epoch 1364, Loss: 1.3493009209632874, Final Batch Loss: 0.3580802381038666\n",
      "Epoch 1365, Loss: 1.2789602875709534, Final Batch Loss: 0.345925897359848\n",
      "Epoch 1366, Loss: 1.4669422805309296, Final Batch Loss: 0.5298126935958862\n",
      "Epoch 1367, Loss: 1.3329782783985138, Final Batch Loss: 0.391800194978714\n",
      "Epoch 1368, Loss: 1.1823959648609161, Final Batch Loss: 0.28099870681762695\n",
      "Epoch 1369, Loss: 1.2568967044353485, Final Batch Loss: 0.25573086738586426\n",
      "Epoch 1370, Loss: 1.2207398414611816, Final Batch Loss: 0.28112635016441345\n",
      "Epoch 1371, Loss: 1.4514406621456146, Final Batch Loss: 0.46142661571502686\n",
      "Epoch 1372, Loss: 1.191013291478157, Final Batch Loss: 0.23376812040805817\n",
      "Epoch 1373, Loss: 1.2250191271305084, Final Batch Loss: 0.3145526945590973\n",
      "Epoch 1374, Loss: 1.363862931728363, Final Batch Loss: 0.3660297691822052\n",
      "Epoch 1375, Loss: 1.5488660633563995, Final Batch Loss: 0.5639988780021667\n",
      "Epoch 1376, Loss: 1.2415269315242767, Final Batch Loss: 0.3548685610294342\n",
      "Epoch 1377, Loss: 1.1352979093790054, Final Batch Loss: 0.2385401874780655\n",
      "Epoch 1378, Loss: 1.2534004151821136, Final Batch Loss: 0.3082948327064514\n",
      "Epoch 1379, Loss: 1.2952642440795898, Final Batch Loss: 0.2942397892475128\n",
      "Epoch 1380, Loss: 1.3022115528583527, Final Batch Loss: 0.3155973255634308\n",
      "Epoch 1381, Loss: 1.3947894871234894, Final Batch Loss: 0.3378065824508667\n",
      "Epoch 1382, Loss: 1.337528258562088, Final Batch Loss: 0.39395686984062195\n",
      "Epoch 1383, Loss: 1.2545278072357178, Final Batch Loss: 0.24306827783584595\n",
      "Epoch 1384, Loss: 1.3245745301246643, Final Batch Loss: 0.3079625964164734\n",
      "Epoch 1385, Loss: 1.3007942140102386, Final Batch Loss: 0.347182035446167\n",
      "Epoch 1386, Loss: 1.3210117518901825, Final Batch Loss: 0.3459934890270233\n",
      "Epoch 1387, Loss: 1.329364687204361, Final Batch Loss: 0.4055137634277344\n",
      "Epoch 1388, Loss: 1.2743276953697205, Final Batch Loss: 0.33212777972221375\n",
      "Epoch 1389, Loss: 1.5029587745666504, Final Batch Loss: 0.47168388962745667\n",
      "Epoch 1390, Loss: 1.1139091700315475, Final Batch Loss: 0.21588681638240814\n",
      "Epoch 1391, Loss: 1.1151645928621292, Final Batch Loss: 0.1927027851343155\n",
      "Epoch 1392, Loss: 1.1008969694375992, Final Batch Loss: 0.17841412127017975\n",
      "Epoch 1393, Loss: 1.3539057075977325, Final Batch Loss: 0.41150638461112976\n",
      "Epoch 1394, Loss: 1.1395432502031326, Final Batch Loss: 0.20962218940258026\n",
      "Epoch 1395, Loss: 1.1833521276712418, Final Batch Loss: 0.3076021373271942\n",
      "Epoch 1396, Loss: 1.2810661792755127, Final Batch Loss: 0.3724433481693268\n",
      "Epoch 1397, Loss: 1.2066007852554321, Final Batch Loss: 0.24462103843688965\n",
      "Epoch 1398, Loss: 1.1244370937347412, Final Batch Loss: 0.13904976844787598\n",
      "Epoch 1399, Loss: 1.275347650051117, Final Batch Loss: 0.3514314591884613\n",
      "Epoch 1400, Loss: 1.2766197621822357, Final Batch Loss: 0.351404070854187\n",
      "Epoch 1401, Loss: 1.507180392742157, Final Batch Loss: 0.5231292247772217\n",
      "Epoch 1402, Loss: 1.1466136425733566, Final Batch Loss: 0.20285378396511078\n",
      "Epoch 1403, Loss: 1.3765351176261902, Final Batch Loss: 0.38877081871032715\n",
      "Epoch 1404, Loss: 1.3288886845111847, Final Batch Loss: 0.3481924831867218\n",
      "Epoch 1405, Loss: 1.3594469130039215, Final Batch Loss: 0.3449527323246002\n",
      "Epoch 1406, Loss: 1.2971726953983307, Final Batch Loss: 0.3616429567337036\n",
      "Epoch 1407, Loss: 1.2385412156581879, Final Batch Loss: 0.2316892147064209\n",
      "Epoch 1408, Loss: 1.1489893198013306, Final Batch Loss: 0.3004925847053528\n",
      "Epoch 1409, Loss: 1.254348486661911, Final Batch Loss: 0.347638338804245\n",
      "Epoch 1410, Loss: 1.1854320615530014, Final Batch Loss: 0.20735855400562286\n",
      "Epoch 1411, Loss: 1.3335427045822144, Final Batch Loss: 0.4404262602329254\n",
      "Epoch 1412, Loss: 1.422338217496872, Final Batch Loss: 0.4432077407836914\n",
      "Epoch 1413, Loss: 1.3282583951950073, Final Batch Loss: 0.3644527494907379\n",
      "Epoch 1414, Loss: 1.145251289010048, Final Batch Loss: 0.2364383190870285\n",
      "Epoch 1415, Loss: 1.2332188487052917, Final Batch Loss: 0.32844483852386475\n",
      "Epoch 1416, Loss: 1.2282249331474304, Final Batch Loss: 0.25902268290519714\n",
      "Epoch 1417, Loss: 1.1715168803930283, Final Batch Loss: 0.23956619203090668\n",
      "Epoch 1418, Loss: 1.3075986206531525, Final Batch Loss: 0.3146931231021881\n",
      "Epoch 1419, Loss: 1.0674538761377335, Final Batch Loss: 0.19578419625759125\n",
      "Epoch 1420, Loss: 1.1947870254516602, Final Batch Loss: 0.26710912585258484\n",
      "Epoch 1421, Loss: 1.3999225795269012, Final Batch Loss: 0.36639419198036194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1422, Loss: 1.3889362812042236, Final Batch Loss: 0.4743201434612274\n",
      "Epoch 1423, Loss: 1.164310023188591, Final Batch Loss: 0.18020950257778168\n",
      "Epoch 1424, Loss: 1.1214685589075089, Final Batch Loss: 0.21132414042949677\n",
      "Epoch 1425, Loss: 1.3278699219226837, Final Batch Loss: 0.3523584306240082\n",
      "Epoch 1426, Loss: 1.1400618851184845, Final Batch Loss: 0.20475643873214722\n",
      "Epoch 1427, Loss: 1.2083138823509216, Final Batch Loss: 0.2629283368587494\n",
      "Epoch 1428, Loss: 1.2734407186508179, Final Batch Loss: 0.33700868487358093\n",
      "Epoch 1429, Loss: 1.2235176265239716, Final Batch Loss: 0.3399147689342499\n",
      "Epoch 1430, Loss: 1.221011459827423, Final Batch Loss: 0.2925744354724884\n",
      "Epoch 1431, Loss: 1.453177124261856, Final Batch Loss: 0.4415030777454376\n",
      "Epoch 1432, Loss: 1.1738207638263702, Final Batch Loss: 0.17564284801483154\n",
      "Epoch 1433, Loss: 1.2859514653682709, Final Batch Loss: 0.34880590438842773\n",
      "Epoch 1434, Loss: 1.2022010385990143, Final Batch Loss: 0.3113507032394409\n",
      "Epoch 1435, Loss: 1.1933903694152832, Final Batch Loss: 0.23402976989746094\n",
      "Epoch 1436, Loss: 1.526489943265915, Final Batch Loss: 0.6207243204116821\n",
      "Epoch 1437, Loss: 1.5086341798305511, Final Batch Loss: 0.46030399203300476\n",
      "Epoch 1438, Loss: 1.136526107788086, Final Batch Loss: 0.20051562786102295\n",
      "Epoch 1439, Loss: 1.3136842846870422, Final Batch Loss: 0.4153071939945221\n",
      "Epoch 1440, Loss: 1.2665724456310272, Final Batch Loss: 0.35672447085380554\n",
      "Epoch 1441, Loss: 1.2849212288856506, Final Batch Loss: 0.28874722123146057\n",
      "Epoch 1442, Loss: 1.3556854128837585, Final Batch Loss: 0.3892473876476288\n",
      "Epoch 1443, Loss: 1.3530931174755096, Final Batch Loss: 0.3025246262550354\n",
      "Epoch 1444, Loss: 1.3118333220481873, Final Batch Loss: 0.37055909633636475\n",
      "Epoch 1445, Loss: 1.107691690325737, Final Batch Loss: 0.18136177957057953\n",
      "Epoch 1446, Loss: 1.1860976219177246, Final Batch Loss: 0.26274797320365906\n",
      "Epoch 1447, Loss: 1.223120093345642, Final Batch Loss: 0.2901429235935211\n",
      "Epoch 1448, Loss: 1.255200058221817, Final Batch Loss: 0.32381775975227356\n",
      "Epoch 1449, Loss: 1.2108943462371826, Final Batch Loss: 0.24767744541168213\n",
      "Epoch 1450, Loss: 1.1828989088535309, Final Batch Loss: 0.24682515859603882\n",
      "Epoch 1451, Loss: 1.3062858283519745, Final Batch Loss: 0.3207962214946747\n",
      "Epoch 1452, Loss: 1.424246907234192, Final Batch Loss: 0.38543012738227844\n",
      "Epoch 1453, Loss: 1.365406185388565, Final Batch Loss: 0.37483301758766174\n",
      "Epoch 1454, Loss: 1.315898209810257, Final Batch Loss: 0.2680590748786926\n",
      "Epoch 1455, Loss: 1.5179393887519836, Final Batch Loss: 0.4893918037414551\n",
      "Epoch 1456, Loss: 1.153564766049385, Final Batch Loss: 0.23870347440242767\n",
      "Epoch 1457, Loss: 1.2786764949560165, Final Batch Loss: 0.36816903948783875\n",
      "Epoch 1458, Loss: 1.2393682897090912, Final Batch Loss: 0.4076817035675049\n",
      "Epoch 1459, Loss: 1.2092992663383484, Final Batch Loss: 0.3437652289867401\n",
      "Epoch 1460, Loss: 1.3845040798187256, Final Batch Loss: 0.29639697074890137\n",
      "Epoch 1461, Loss: 1.4969173073768616, Final Batch Loss: 0.57224041223526\n",
      "Epoch 1462, Loss: 1.3963953852653503, Final Batch Loss: 0.4092446267604828\n",
      "Epoch 1463, Loss: 1.2914404720067978, Final Batch Loss: 0.21099181473255157\n",
      "Epoch 1464, Loss: 1.3391894698143005, Final Batch Loss: 0.3084086775779724\n",
      "Epoch 1465, Loss: 1.1686716228723526, Final Batch Loss: 0.272401362657547\n",
      "Epoch 1466, Loss: 1.2414734959602356, Final Batch Loss: 0.3122687041759491\n",
      "Epoch 1467, Loss: 1.2953905761241913, Final Batch Loss: 0.3073757588863373\n",
      "Epoch 1468, Loss: 1.2846254110336304, Final Batch Loss: 0.3014321029186249\n",
      "Epoch 1469, Loss: 1.3071284294128418, Final Batch Loss: 0.4464491903781891\n",
      "Epoch 1470, Loss: 1.1837447136640549, Final Batch Loss: 0.22365818917751312\n",
      "Epoch 1471, Loss: 1.3146501779556274, Final Batch Loss: 0.3326251804828644\n",
      "Epoch 1472, Loss: 1.276228442788124, Final Batch Loss: 0.24216483533382416\n",
      "Epoch 1473, Loss: 1.0849428921937943, Final Batch Loss: 0.1875760704278946\n",
      "Epoch 1474, Loss: 1.2259843796491623, Final Batch Loss: 0.2303812950849533\n",
      "Epoch 1475, Loss: 1.2530586123466492, Final Batch Loss: 0.3212485611438751\n",
      "Epoch 1476, Loss: 1.1903333961963654, Final Batch Loss: 0.2816985547542572\n",
      "Epoch 1477, Loss: 1.178979828953743, Final Batch Loss: 0.27384743094444275\n",
      "Epoch 1478, Loss: 1.1715013682842255, Final Batch Loss: 0.32104119658470154\n",
      "Epoch 1479, Loss: 1.2851445972919464, Final Batch Loss: 0.43290480971336365\n",
      "Epoch 1480, Loss: 1.4280706942081451, Final Batch Loss: 0.5307578444480896\n",
      "Epoch 1481, Loss: 1.2979950904846191, Final Batch Loss: 0.34942150115966797\n",
      "Epoch 1482, Loss: 1.1473602056503296, Final Batch Loss: 0.22301650047302246\n",
      "Epoch 1483, Loss: 1.2552964091300964, Final Batch Loss: 0.4368005096912384\n",
      "Epoch 1484, Loss: 1.3278294503688812, Final Batch Loss: 0.3831803798675537\n",
      "Epoch 1485, Loss: 1.0749891698360443, Final Batch Loss: 0.26441577076911926\n",
      "Epoch 1486, Loss: 1.2529573291540146, Final Batch Loss: 0.3347834348678589\n",
      "Epoch 1487, Loss: 1.2559388875961304, Final Batch Loss: 0.35771045088768005\n",
      "Epoch 1488, Loss: 1.0286025181412697, Final Batch Loss: 0.09178049117326736\n",
      "Epoch 1489, Loss: 1.1930601447820663, Final Batch Loss: 0.2410847693681717\n",
      "Epoch 1490, Loss: 1.0364436507225037, Final Batch Loss: 0.09990203380584717\n",
      "Epoch 1491, Loss: 1.266764372587204, Final Batch Loss: 0.2798100411891937\n",
      "Epoch 1492, Loss: 1.2375206798315048, Final Batch Loss: 0.41127219796180725\n",
      "Epoch 1493, Loss: 1.2746501863002777, Final Batch Loss: 0.35596394538879395\n",
      "Epoch 1494, Loss: 1.2407067716121674, Final Batch Loss: 0.30422958731651306\n",
      "Epoch 1495, Loss: 1.3915432393550873, Final Batch Loss: 0.3073975741863251\n",
      "Epoch 1496, Loss: 1.3367415219545364, Final Batch Loss: 0.40967491269111633\n",
      "Epoch 1497, Loss: 1.2881626784801483, Final Batch Loss: 0.3410871922969818\n",
      "Epoch 1498, Loss: 1.2467701733112335, Final Batch Loss: 0.3093841075897217\n",
      "Epoch 1499, Loss: 1.2089338898658752, Final Batch Loss: 0.22532695531845093\n",
      "Epoch 1500, Loss: 1.4007318019866943, Final Batch Loss: 0.4709914028644562\n",
      "Epoch 1501, Loss: 1.3032212555408478, Final Batch Loss: 0.40816447138786316\n",
      "Epoch 1502, Loss: 1.2249692231416702, Final Batch Loss: 0.1975235491991043\n",
      "Epoch 1503, Loss: 1.2433275878429413, Final Batch Loss: 0.3285244405269623\n",
      "Epoch 1504, Loss: 1.1836272031068802, Final Batch Loss: 0.3001311123371124\n",
      "Epoch 1505, Loss: 1.2536320686340332, Final Batch Loss: 0.23253625631332397\n",
      "Epoch 1506, Loss: 1.3294657170772552, Final Batch Loss: 0.4250189960002899\n",
      "Epoch 1507, Loss: 1.1574338525533676, Final Batch Loss: 0.22401364147663116\n",
      "Epoch 1508, Loss: 1.2429709732532501, Final Batch Loss: 0.3016033172607422\n",
      "Epoch 1509, Loss: 1.1012440919876099, Final Batch Loss: 0.19213207066059113\n",
      "Epoch 1510, Loss: 1.3713297992944717, Final Batch Loss: 0.18803294003009796\n",
      "Epoch 1511, Loss: 1.1636159718036652, Final Batch Loss: 0.28943684697151184\n",
      "Epoch 1512, Loss: 1.2594726085662842, Final Batch Loss: 0.3213871121406555\n",
      "Epoch 1513, Loss: 1.2613800168037415, Final Batch Loss: 0.27350616455078125\n",
      "Epoch 1514, Loss: 1.2196959257125854, Final Batch Loss: 0.3483123779296875\n",
      "Epoch 1515, Loss: 1.2942940294742584, Final Batch Loss: 0.37265050411224365\n",
      "Epoch 1516, Loss: 1.3553124964237213, Final Batch Loss: 0.39931520819664\n",
      "Epoch 1517, Loss: 1.139371395111084, Final Batch Loss: 0.24786198139190674\n",
      "Epoch 1518, Loss: 1.2461433708667755, Final Batch Loss: 0.39519742131233215\n",
      "Epoch 1519, Loss: 1.2953181266784668, Final Batch Loss: 0.4282206594944\n",
      "Epoch 1520, Loss: 1.1818962395191193, Final Batch Loss: 0.3168052136898041\n",
      "Epoch 1521, Loss: 1.1940158754587173, Final Batch Loss: 0.24685700237751007\n",
      "Epoch 1522, Loss: 1.188206046819687, Final Batch Loss: 0.32680806517601013\n",
      "Epoch 1523, Loss: 1.1720648854970932, Final Batch Loss: 0.2621435821056366\n",
      "Epoch 1524, Loss: 1.1631552278995514, Final Batch Loss: 0.3058812916278839\n",
      "Epoch 1525, Loss: 1.2673311680555344, Final Batch Loss: 0.3840426206588745\n",
      "Epoch 1526, Loss: 1.3119129836559296, Final Batch Loss: 0.37485194206237793\n",
      "Epoch 1527, Loss: 1.127375140786171, Final Batch Loss: 0.201994851231575\n",
      "Epoch 1528, Loss: 1.248449981212616, Final Batch Loss: 0.36554476618766785\n",
      "Epoch 1529, Loss: 1.3095922470092773, Final Batch Loss: 0.3943409025669098\n",
      "Epoch 1530, Loss: 1.2750605940818787, Final Batch Loss: 0.2871004343032837\n",
      "Epoch 1531, Loss: 1.0848407000303268, Final Batch Loss: 0.20728738605976105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1532, Loss: 1.194572851061821, Final Batch Loss: 0.27322015166282654\n",
      "Epoch 1533, Loss: 1.4835038781166077, Final Batch Loss: 0.3746885359287262\n",
      "Epoch 1534, Loss: 1.2316545248031616, Final Batch Loss: 0.23690998554229736\n",
      "Epoch 1535, Loss: 1.0631815940141678, Final Batch Loss: 0.14719389379024506\n",
      "Epoch 1536, Loss: 1.1964928209781647, Final Batch Loss: 0.2773021161556244\n",
      "Epoch 1537, Loss: 1.0457990914583206, Final Batch Loss: 0.19370712339878082\n",
      "Epoch 1538, Loss: 1.152698889374733, Final Batch Loss: 0.1918129175901413\n",
      "Epoch 1539, Loss: 1.1253606528043747, Final Batch Loss: 0.21807950735092163\n",
      "Epoch 1540, Loss: 1.1594751179218292, Final Batch Loss: 0.22379237413406372\n",
      "Epoch 1541, Loss: 1.349101573228836, Final Batch Loss: 0.38361236453056335\n",
      "Epoch 1542, Loss: 1.3490265011787415, Final Batch Loss: 0.4106614589691162\n",
      "Epoch 1543, Loss: 1.3936078548431396, Final Batch Loss: 0.435933917760849\n",
      "Epoch 1544, Loss: 1.360942304134369, Final Batch Loss: 0.4088442325592041\n",
      "Epoch 1545, Loss: 1.2637836933135986, Final Batch Loss: 0.38446250557899475\n",
      "Epoch 1546, Loss: 1.2556270956993103, Final Batch Loss: 0.3203533887863159\n",
      "Epoch 1547, Loss: 1.1690365970134735, Final Batch Loss: 0.2075955867767334\n",
      "Epoch 1548, Loss: 1.2453994899988174, Final Batch Loss: 0.4130397140979767\n",
      "Epoch 1549, Loss: 1.210692584514618, Final Batch Loss: 0.3393131494522095\n",
      "Epoch 1550, Loss: 1.1333388090133667, Final Batch Loss: 0.2721395492553711\n",
      "Epoch 1551, Loss: 1.185346007347107, Final Batch Loss: 0.33315470814704895\n",
      "Epoch 1552, Loss: 1.0528519302606583, Final Batch Loss: 0.16168689727783203\n",
      "Epoch 1553, Loss: 1.3772829473018646, Final Batch Loss: 0.4799301326274872\n",
      "Epoch 1554, Loss: 1.3601857125759125, Final Batch Loss: 0.42982617020606995\n",
      "Epoch 1555, Loss: 1.2580333352088928, Final Batch Loss: 0.3312160074710846\n",
      "Epoch 1556, Loss: 1.3534528613090515, Final Batch Loss: 0.38516440987586975\n",
      "Epoch 1557, Loss: 1.1432456970214844, Final Batch Loss: 0.2647349238395691\n",
      "Epoch 1558, Loss: 1.2988479137420654, Final Batch Loss: 0.38987836241722107\n",
      "Epoch 1559, Loss: 1.1803081184625626, Final Batch Loss: 0.24175752699375153\n",
      "Epoch 1560, Loss: 1.1518991887569427, Final Batch Loss: 0.2740165889263153\n",
      "Epoch 1561, Loss: 1.2448550164699554, Final Batch Loss: 0.25746122002601624\n",
      "Epoch 1562, Loss: 1.4242914319038391, Final Batch Loss: 0.4388089179992676\n",
      "Epoch 1563, Loss: 1.2027213275432587, Final Batch Loss: 0.25655314326286316\n",
      "Epoch 1564, Loss: 1.2644338607788086, Final Batch Loss: 0.3190830647945404\n",
      "Epoch 1565, Loss: 1.2388734221458435, Final Batch Loss: 0.3105487823486328\n",
      "Epoch 1566, Loss: 1.2729156911373138, Final Batch Loss: 0.2960081994533539\n",
      "Epoch 1567, Loss: 1.1839044690132141, Final Batch Loss: 0.3139287829399109\n",
      "Epoch 1568, Loss: 1.266343355178833, Final Batch Loss: 0.35184159874916077\n",
      "Epoch 1569, Loss: 1.432777315378189, Final Batch Loss: 0.5181137919425964\n",
      "Epoch 1570, Loss: 1.258835643529892, Final Batch Loss: 0.31506842374801636\n",
      "Epoch 1571, Loss: 1.1844483017921448, Final Batch Loss: 0.2872946262359619\n",
      "Epoch 1572, Loss: 1.1525228768587112, Final Batch Loss: 0.23888973891735077\n",
      "Epoch 1573, Loss: 1.2283920049667358, Final Batch Loss: 0.3152689039707184\n",
      "Epoch 1574, Loss: 1.0376285761594772, Final Batch Loss: 0.190921351313591\n",
      "Epoch 1575, Loss: 1.2453982532024384, Final Batch Loss: 0.36805853247642517\n",
      "Epoch 1576, Loss: 1.1062350124120712, Final Batch Loss: 0.2552591860294342\n",
      "Epoch 1577, Loss: 1.2732529938220978, Final Batch Loss: 0.36078158020973206\n",
      "Epoch 1578, Loss: 1.299692839384079, Final Batch Loss: 0.3986671268939972\n",
      "Epoch 1579, Loss: 1.2272053062915802, Final Batch Loss: 0.27154091000556946\n",
      "Epoch 1580, Loss: 1.2310007512569427, Final Batch Loss: 0.30700692534446716\n",
      "Epoch 1581, Loss: 1.2286416590213776, Final Batch Loss: 0.26903268694877625\n",
      "Epoch 1582, Loss: 1.121370866894722, Final Batch Loss: 0.28317001461982727\n",
      "Epoch 1583, Loss: 1.1966908872127533, Final Batch Loss: 0.3603021204471588\n",
      "Epoch 1584, Loss: 1.1848913431167603, Final Batch Loss: 0.2537812888622284\n",
      "Epoch 1585, Loss: 1.2842195928096771, Final Batch Loss: 0.3313773572444916\n",
      "Epoch 1586, Loss: 1.3998235166072845, Final Batch Loss: 0.47369325160980225\n",
      "Epoch 1587, Loss: 1.2548778653144836, Final Batch Loss: 0.32275307178497314\n",
      "Epoch 1588, Loss: 1.3294975012540817, Final Batch Loss: 0.4152037799358368\n",
      "Epoch 1589, Loss: 1.3017266988754272, Final Batch Loss: 0.3794986307621002\n",
      "Epoch 1590, Loss: 1.226180076599121, Final Batch Loss: 0.2579730451107025\n",
      "Epoch 1591, Loss: 1.3402129113674164, Final Batch Loss: 0.33709660172462463\n",
      "Epoch 1592, Loss: 1.2062474638223648, Final Batch Loss: 0.24977420270442963\n",
      "Epoch 1593, Loss: 1.231522113084793, Final Batch Loss: 0.3044240474700928\n",
      "Epoch 1594, Loss: 1.1916503012180328, Final Batch Loss: 0.3162969946861267\n",
      "Epoch 1595, Loss: 1.1718123108148575, Final Batch Loss: 0.22139866650104523\n",
      "Epoch 1596, Loss: 1.2189432084560394, Final Batch Loss: 0.3168241083621979\n",
      "Epoch 1597, Loss: 1.1938313245773315, Final Batch Loss: 0.2975786626338959\n",
      "Epoch 1598, Loss: 1.1833181381225586, Final Batch Loss: 0.26181331276893616\n",
      "Epoch 1599, Loss: 1.2915680408477783, Final Batch Loss: 0.38147974014282227\n",
      "Epoch 1600, Loss: 1.2526388466358185, Final Batch Loss: 0.2809496521949768\n",
      "Epoch 1601, Loss: 1.257365196943283, Final Batch Loss: 0.3524599075317383\n",
      "Epoch 1602, Loss: 1.0500391572713852, Final Batch Loss: 0.2007121592760086\n",
      "Epoch 1603, Loss: 1.2106456458568573, Final Batch Loss: 0.29071754217147827\n",
      "Epoch 1604, Loss: 1.1886473596096039, Final Batch Loss: 0.2892747223377228\n",
      "Epoch 1605, Loss: 1.3363235592842102, Final Batch Loss: 0.4787408113479614\n",
      "Epoch 1606, Loss: 1.6243251264095306, Final Batch Loss: 0.6316377520561218\n",
      "Epoch 1607, Loss: 1.2656235694885254, Final Batch Loss: 0.3711576759815216\n",
      "Epoch 1608, Loss: 1.2108405530452728, Final Batch Loss: 0.35706138610839844\n",
      "Epoch 1609, Loss: 1.0538587123155594, Final Batch Loss: 0.19539444148540497\n",
      "Epoch 1610, Loss: 1.3165334463119507, Final Batch Loss: 0.41247543692588806\n",
      "Epoch 1611, Loss: 1.2168482393026352, Final Batch Loss: 0.30048322677612305\n",
      "Epoch 1612, Loss: 1.0842182338237762, Final Batch Loss: 0.2297951579093933\n",
      "Epoch 1613, Loss: 1.069573998451233, Final Batch Loss: 0.24391573667526245\n",
      "Epoch 1614, Loss: 1.1605055034160614, Final Batch Loss: 0.3020405173301697\n",
      "Epoch 1615, Loss: 1.1899744868278503, Final Batch Loss: 0.25518998503685\n",
      "Epoch 1616, Loss: 1.2752921283245087, Final Batch Loss: 0.28791287541389465\n",
      "Epoch 1617, Loss: 1.3443750143051147, Final Batch Loss: 0.3727816641330719\n",
      "Epoch 1618, Loss: 1.1661063134670258, Final Batch Loss: 0.29717811942100525\n",
      "Epoch 1619, Loss: 1.3147060573101044, Final Batch Loss: 0.47981539368629456\n",
      "Epoch 1620, Loss: 1.2295968234539032, Final Batch Loss: 0.35229960083961487\n",
      "Epoch 1621, Loss: 1.032282516360283, Final Batch Loss: 0.14425276219844818\n",
      "Epoch 1622, Loss: 1.2372737973928452, Final Batch Loss: 0.32739245891571045\n",
      "Epoch 1623, Loss: 1.304421752691269, Final Batch Loss: 0.2843974828720093\n",
      "Epoch 1624, Loss: 1.1328908950090408, Final Batch Loss: 0.18630941212177277\n",
      "Epoch 1625, Loss: 1.2367074489593506, Final Batch Loss: 0.29652687907218933\n",
      "Epoch 1626, Loss: 1.1587360501289368, Final Batch Loss: 0.282741516828537\n",
      "Epoch 1627, Loss: 1.3178203403949738, Final Batch Loss: 0.2823789417743683\n",
      "Epoch 1628, Loss: 1.156146913766861, Final Batch Loss: 0.3226095736026764\n",
      "Epoch 1629, Loss: 1.3129362910985947, Final Batch Loss: 0.48144420981407166\n",
      "Epoch 1630, Loss: 1.252711445093155, Final Batch Loss: 0.30796316266059875\n",
      "Epoch 1631, Loss: 1.3783288300037384, Final Batch Loss: 0.44165289402008057\n",
      "Epoch 1632, Loss: 1.2402125298976898, Final Batch Loss: 0.3280528485774994\n",
      "Epoch 1633, Loss: 1.1191312968730927, Final Batch Loss: 0.25083455443382263\n",
      "Epoch 1634, Loss: 1.1420675218105316, Final Batch Loss: 0.2538898289203644\n",
      "Epoch 1635, Loss: 1.252591073513031, Final Batch Loss: 0.32240036129951477\n",
      "Epoch 1636, Loss: 1.1440745145082474, Final Batch Loss: 0.21860170364379883\n",
      "Epoch 1637, Loss: 1.3680815696716309, Final Batch Loss: 0.3275264799594879\n",
      "Epoch 1638, Loss: 1.1005255281925201, Final Batch Loss: 0.2003338485956192\n",
      "Epoch 1639, Loss: 1.0574727058410645, Final Batch Loss: 0.13362959027290344\n",
      "Epoch 1640, Loss: 1.2408983260393143, Final Batch Loss: 0.46558740735054016\n",
      "Epoch 1641, Loss: 1.2333564460277557, Final Batch Loss: 0.34896233677864075\n",
      "Epoch 1642, Loss: 1.1494833827018738, Final Batch Loss: 0.17049932479858398\n",
      "Epoch 1643, Loss: 1.3228197693824768, Final Batch Loss: 0.4166167676448822\n",
      "Epoch 1644, Loss: 1.0260230004787445, Final Batch Loss: 0.17277033627033234\n",
      "Epoch 1645, Loss: 1.0639546364545822, Final Batch Loss: 0.19858942925930023\n",
      "Epoch 1646, Loss: 1.1784973293542862, Final Batch Loss: 0.14251868426799774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1647, Loss: 1.1895824819803238, Final Batch Loss: 0.3090722858905792\n",
      "Epoch 1648, Loss: 1.2188178598880768, Final Batch Loss: 0.3111515939235687\n",
      "Epoch 1649, Loss: 0.9882957860827446, Final Batch Loss: 0.09755241125822067\n",
      "Epoch 1650, Loss: 1.1445441097021103, Final Batch Loss: 0.21524246037006378\n",
      "Epoch 1651, Loss: 1.191641390323639, Final Batch Loss: 0.2843005955219269\n",
      "Epoch 1652, Loss: 1.2260555624961853, Final Batch Loss: 0.3426867425441742\n",
      "Epoch 1653, Loss: 1.2938191592693329, Final Batch Loss: 0.4164632558822632\n",
      "Epoch 1654, Loss: 1.1862429082393646, Final Batch Loss: 0.3277307450771332\n",
      "Epoch 1655, Loss: 1.2698149979114532, Final Batch Loss: 0.27099892497062683\n",
      "Epoch 1656, Loss: 1.3174484968185425, Final Batch Loss: 0.4197547435760498\n",
      "Epoch 1657, Loss: 1.100326120853424, Final Batch Loss: 0.22268922626972198\n",
      "Epoch 1658, Loss: 1.2949749827384949, Final Batch Loss: 0.40964701771736145\n",
      "Epoch 1659, Loss: 1.1473515778779984, Final Batch Loss: 0.23069603741168976\n",
      "Epoch 1660, Loss: 1.349042683839798, Final Batch Loss: 0.47663092613220215\n",
      "Epoch 1661, Loss: 1.1851801872253418, Final Batch Loss: 0.33861449360847473\n",
      "Epoch 1662, Loss: 1.2386312186717987, Final Batch Loss: 0.334051251411438\n",
      "Epoch 1663, Loss: 1.0590467900037766, Final Batch Loss: 0.1652570366859436\n",
      "Epoch 1664, Loss: 1.2221210896968842, Final Batch Loss: 0.32857632637023926\n",
      "Epoch 1665, Loss: 1.220426321029663, Final Batch Loss: 0.35423484444618225\n",
      "Epoch 1666, Loss: 1.0921715945005417, Final Batch Loss: 0.24815402925014496\n",
      "Epoch 1667, Loss: 1.0722895562648773, Final Batch Loss: 0.18582415580749512\n",
      "Epoch 1668, Loss: 1.145425632596016, Final Batch Loss: 0.2370987981557846\n",
      "Epoch 1669, Loss: 1.1801259219646454, Final Batch Loss: 0.28489142656326294\n",
      "Epoch 1670, Loss: 1.0826749205589294, Final Batch Loss: 0.2748367488384247\n",
      "Epoch 1671, Loss: 1.215247705578804, Final Batch Loss: 0.3365023136138916\n",
      "Epoch 1672, Loss: 1.1462405622005463, Final Batch Loss: 0.278319388628006\n",
      "Epoch 1673, Loss: 1.171854317188263, Final Batch Loss: 0.3103848397731781\n",
      "Epoch 1674, Loss: 1.2557528913021088, Final Batch Loss: 0.30316299200057983\n",
      "Epoch 1675, Loss: 1.182058960199356, Final Batch Loss: 0.26842620968818665\n",
      "Epoch 1676, Loss: 1.3386048376560211, Final Batch Loss: 0.4995749294757843\n",
      "Epoch 1677, Loss: 1.5217260718345642, Final Batch Loss: 0.4514636993408203\n",
      "Epoch 1678, Loss: 1.309617042541504, Final Batch Loss: 0.4044172763824463\n",
      "Epoch 1679, Loss: 1.2134191393852234, Final Batch Loss: 0.39958861470222473\n",
      "Epoch 1680, Loss: 1.1361746340990067, Final Batch Loss: 0.2775561809539795\n",
      "Epoch 1681, Loss: 1.180865466594696, Final Batch Loss: 0.27282339334487915\n",
      "Epoch 1682, Loss: 1.1642403900623322, Final Batch Loss: 0.26472845673561096\n",
      "Epoch 1683, Loss: 1.2012263238430023, Final Batch Loss: 0.338568776845932\n",
      "Epoch 1684, Loss: 0.9997839033603668, Final Batch Loss: 0.1375221610069275\n",
      "Epoch 1685, Loss: 1.1206600368022919, Final Batch Loss: 0.26927199959754944\n",
      "Epoch 1686, Loss: 1.0996471494436264, Final Batch Loss: 0.21751044690608978\n",
      "Epoch 1687, Loss: 1.0508855432271957, Final Batch Loss: 0.17952580749988556\n",
      "Epoch 1688, Loss: 1.225758820772171, Final Batch Loss: 0.3159278929233551\n",
      "Epoch 1689, Loss: 1.2233846187591553, Final Batch Loss: 0.3236263692378998\n",
      "Epoch 1690, Loss: 1.2518306523561478, Final Batch Loss: 0.4247997999191284\n",
      "Epoch 1691, Loss: 1.0942252576351166, Final Batch Loss: 0.25546079874038696\n",
      "Epoch 1692, Loss: 1.2148041427135468, Final Batch Loss: 0.31291884183883667\n",
      "Epoch 1693, Loss: 1.1942015588283539, Final Batch Loss: 0.2884872853755951\n",
      "Epoch 1694, Loss: 1.1581135392189026, Final Batch Loss: 0.3014427721500397\n",
      "Epoch 1695, Loss: 1.0267685800790787, Final Batch Loss: 0.24864552915096283\n",
      "Epoch 1696, Loss: 1.2639499008655548, Final Batch Loss: 0.34530317783355713\n",
      "Epoch 1697, Loss: 1.1099950969219208, Final Batch Loss: 0.1843763142824173\n",
      "Epoch 1698, Loss: 1.4004877805709839, Final Batch Loss: 0.43889132142066956\n",
      "Epoch 1699, Loss: 1.1034047603607178, Final Batch Loss: 0.25845620036125183\n",
      "Epoch 1700, Loss: 1.0759109258651733, Final Batch Loss: 0.24261461198329926\n",
      "Epoch 1701, Loss: 1.0095503330230713, Final Batch Loss: 0.20981959998607635\n",
      "Epoch 1702, Loss: 1.1395178437232971, Final Batch Loss: 0.3147270679473877\n",
      "Epoch 1703, Loss: 1.1168425232172012, Final Batch Loss: 0.23086868226528168\n",
      "Epoch 1704, Loss: 1.0646766126155853, Final Batch Loss: 0.13033637404441833\n",
      "Epoch 1705, Loss: 1.1346382051706314, Final Batch Loss: 0.21699567139148712\n",
      "Epoch 1706, Loss: 1.3365917056798935, Final Batch Loss: 0.33601367473602295\n",
      "Epoch 1707, Loss: 1.1758046746253967, Final Batch Loss: 0.32040998339653015\n",
      "Epoch 1708, Loss: 1.3486039340496063, Final Batch Loss: 0.4311693012714386\n",
      "Epoch 1709, Loss: 1.2100532799959183, Final Batch Loss: 0.36799368262290955\n",
      "Epoch 1710, Loss: 1.0739838778972626, Final Batch Loss: 0.1581290066242218\n",
      "Epoch 1711, Loss: 1.1048006564378738, Final Batch Loss: 0.24505239725112915\n",
      "Epoch 1712, Loss: 1.042163372039795, Final Batch Loss: 0.17557822167873383\n",
      "Epoch 1713, Loss: 1.2598815560340881, Final Batch Loss: 0.35034897923469543\n",
      "Epoch 1714, Loss: 1.2231815606355667, Final Batch Loss: 0.45732352137565613\n",
      "Epoch 1715, Loss: 1.2442131638526917, Final Batch Loss: 0.31406310200691223\n",
      "Epoch 1716, Loss: 1.299887478351593, Final Batch Loss: 0.3904419243335724\n",
      "Epoch 1717, Loss: 1.2692062854766846, Final Batch Loss: 0.3302794396877289\n",
      "Epoch 1718, Loss: 1.1409519761800766, Final Batch Loss: 0.22407712042331696\n",
      "Epoch 1719, Loss: 1.4043909311294556, Final Batch Loss: 0.43032944202423096\n",
      "Epoch 1720, Loss: 1.3348003923892975, Final Batch Loss: 0.35716870427131653\n",
      "Epoch 1721, Loss: 1.1735727339982986, Final Batch Loss: 0.38364413380622864\n",
      "Epoch 1722, Loss: 1.1524792462587357, Final Batch Loss: 0.32782286405563354\n",
      "Epoch 1723, Loss: 1.2117880880832672, Final Batch Loss: 0.42057499289512634\n",
      "Epoch 1724, Loss: 1.6862775087356567, Final Batch Loss: 0.8062413334846497\n",
      "Epoch 1725, Loss: 1.2389689683914185, Final Batch Loss: 0.26077941060066223\n",
      "Epoch 1726, Loss: 1.1620901376008987, Final Batch Loss: 0.2191661149263382\n",
      "Epoch 1727, Loss: 1.3584310710430145, Final Batch Loss: 0.38360095024108887\n",
      "Epoch 1728, Loss: 1.557169646024704, Final Batch Loss: 0.5619198083877563\n",
      "Epoch 1729, Loss: 1.2985105514526367, Final Batch Loss: 0.27142173051834106\n",
      "Epoch 1730, Loss: 1.257458209991455, Final Batch Loss: 0.2923583984375\n",
      "Epoch 1731, Loss: 1.1525655686855316, Final Batch Loss: 0.2612646520137787\n",
      "Epoch 1732, Loss: 1.1813962012529373, Final Batch Loss: 0.17722593247890472\n",
      "Epoch 1733, Loss: 1.2457717955112457, Final Batch Loss: 0.35661932826042175\n",
      "Epoch 1734, Loss: 1.2447719871997833, Final Batch Loss: 0.43351617455482483\n",
      "Epoch 1735, Loss: 1.0724588930606842, Final Batch Loss: 0.21879135072231293\n",
      "Epoch 1736, Loss: 1.0820818543434143, Final Batch Loss: 0.21348470449447632\n",
      "Epoch 1737, Loss: 1.0882253646850586, Final Batch Loss: 0.25684496760368347\n",
      "Epoch 1738, Loss: 1.190425157546997, Final Batch Loss: 0.36129096150398254\n",
      "Epoch 1739, Loss: 1.18648199737072, Final Batch Loss: 0.35124996304512024\n",
      "Epoch 1740, Loss: 1.239791989326477, Final Batch Loss: 0.38026317954063416\n",
      "Epoch 1741, Loss: 1.2808281779289246, Final Batch Loss: 0.36151814460754395\n",
      "Epoch 1742, Loss: 1.1097586750984192, Final Batch Loss: 0.25824546813964844\n",
      "Epoch 1743, Loss: 1.0820357501506805, Final Batch Loss: 0.1743033528327942\n",
      "Epoch 1744, Loss: 1.281414270401001, Final Batch Loss: 0.4554673731327057\n",
      "Epoch 1745, Loss: 1.2466216385364532, Final Batch Loss: 0.346417635679245\n",
      "Epoch 1746, Loss: 1.1239104717969894, Final Batch Loss: 0.23795633018016815\n",
      "Epoch 1747, Loss: 1.2485068887472153, Final Batch Loss: 0.23029495775699615\n",
      "Epoch 1748, Loss: 1.148876577615738, Final Batch Loss: 0.2386733889579773\n",
      "Epoch 1749, Loss: 1.038806214928627, Final Batch Loss: 0.18603764474391937\n",
      "Epoch 1750, Loss: 1.2727664113044739, Final Batch Loss: 0.42657604813575745\n",
      "Epoch 1751, Loss: 1.227950245141983, Final Batch Loss: 0.30608344078063965\n",
      "Epoch 1752, Loss: 1.307274729013443, Final Batch Loss: 0.3249049186706543\n",
      "Epoch 1753, Loss: 1.2458919137716293, Final Batch Loss: 0.3464548885822296\n",
      "Epoch 1754, Loss: 1.2206206321716309, Final Batch Loss: 0.34156346321105957\n",
      "Epoch 1755, Loss: 0.9806848764419556, Final Batch Loss: 0.16883337497711182\n",
      "Epoch 1756, Loss: 1.2055103182792664, Final Batch Loss: 0.2588702440261841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1757, Loss: 1.1337716281414032, Final Batch Loss: 0.18756723403930664\n",
      "Epoch 1758, Loss: 1.0161162912845612, Final Batch Loss: 0.13209909200668335\n",
      "Epoch 1759, Loss: 1.051227629184723, Final Batch Loss: 0.22745740413665771\n",
      "Epoch 1760, Loss: 1.0261657685041428, Final Batch Loss: 0.16666440665721893\n",
      "Epoch 1761, Loss: 1.248698741197586, Final Batch Loss: 0.2563743591308594\n",
      "Epoch 1762, Loss: 1.1369334310293198, Final Batch Loss: 0.16393424570560455\n",
      "Epoch 1763, Loss: 1.1177786588668823, Final Batch Loss: 0.24978387355804443\n",
      "Epoch 1764, Loss: 1.1765336692333221, Final Batch Loss: 0.2652214467525482\n",
      "Epoch 1765, Loss: 1.174330711364746, Final Batch Loss: 0.3047211468219757\n",
      "Epoch 1766, Loss: 1.0653815418481827, Final Batch Loss: 0.1707395762205124\n",
      "Epoch 1767, Loss: 1.3967552483081818, Final Batch Loss: 0.5348489880561829\n",
      "Epoch 1768, Loss: 1.235387235879898, Final Batch Loss: 0.4286215305328369\n",
      "Epoch 1769, Loss: 1.0966573655605316, Final Batch Loss: 0.0869424045085907\n",
      "Epoch 1770, Loss: 1.3155682981014252, Final Batch Loss: 0.32153570652008057\n",
      "Epoch 1771, Loss: 1.509140133857727, Final Batch Loss: 0.556110143661499\n",
      "Epoch 1772, Loss: 1.1087915152311325, Final Batch Loss: 0.2196374535560608\n",
      "Epoch 1773, Loss: 1.0437981337308884, Final Batch Loss: 0.13428978621959686\n",
      "Epoch 1774, Loss: 1.1437486857175827, Final Batch Loss: 0.21668539941310883\n",
      "Epoch 1775, Loss: 1.1720127314329147, Final Batch Loss: 0.23681579530239105\n",
      "Epoch 1776, Loss: 1.0854783207178116, Final Batch Loss: 0.19123582541942596\n",
      "Epoch 1777, Loss: 1.0831569731235504, Final Batch Loss: 0.2434520572423935\n",
      "Epoch 1778, Loss: 1.1512110233306885, Final Batch Loss: 0.31818684935569763\n",
      "Epoch 1779, Loss: 1.2870287597179413, Final Batch Loss: 0.45942768454551697\n",
      "Epoch 1780, Loss: 1.2469526827335358, Final Batch Loss: 0.3036518394947052\n",
      "Epoch 1781, Loss: 1.1645682752132416, Final Batch Loss: 0.29945167899131775\n",
      "Epoch 1782, Loss: 1.1886941492557526, Final Batch Loss: 0.31071385741233826\n",
      "Epoch 1783, Loss: 1.2168325185775757, Final Batch Loss: 0.2838238477706909\n",
      "Epoch 1784, Loss: 1.284606397151947, Final Batch Loss: 0.3372056186199188\n",
      "Epoch 1785, Loss: 1.285274714231491, Final Batch Loss: 0.3643699884414673\n",
      "Epoch 1786, Loss: 1.1998626589775085, Final Batch Loss: 0.34842610359191895\n",
      "Epoch 1787, Loss: 1.3227773606777191, Final Batch Loss: 0.3315581977367401\n",
      "Epoch 1788, Loss: 1.2891526818275452, Final Batch Loss: 0.4646720588207245\n",
      "Epoch 1789, Loss: 1.1742062866687775, Final Batch Loss: 0.30509015917778015\n",
      "Epoch 1790, Loss: 1.1780425906181335, Final Batch Loss: 0.3739205300807953\n",
      "Epoch 1791, Loss: 1.2548052668571472, Final Batch Loss: 0.3382439911365509\n",
      "Epoch 1792, Loss: 1.2237573564052582, Final Batch Loss: 0.399617463350296\n",
      "Epoch 1793, Loss: 1.0644800513982773, Final Batch Loss: 0.19709713757038116\n",
      "Epoch 1794, Loss: 1.329365462064743, Final Batch Loss: 0.36046096682548523\n",
      "Epoch 1795, Loss: 1.220417097210884, Final Batch Loss: 0.22612883150577545\n",
      "Epoch 1796, Loss: 1.0914263874292374, Final Batch Loss: 0.24600251019001007\n",
      "Epoch 1797, Loss: 1.089021846652031, Final Batch Loss: 0.31174734234809875\n",
      "Epoch 1798, Loss: 1.113136202096939, Final Batch Loss: 0.26116180419921875\n",
      "Epoch 1799, Loss: 1.2724282294511795, Final Batch Loss: 0.4290589392185211\n",
      "Epoch 1800, Loss: 1.1107377409934998, Final Batch Loss: 0.2961874306201935\n",
      "Epoch 1801, Loss: 1.1569689959287643, Final Batch Loss: 0.2935201823711395\n",
      "Epoch 1802, Loss: 1.1674005687236786, Final Batch Loss: 0.2985214293003082\n",
      "Epoch 1803, Loss: 1.124031901359558, Final Batch Loss: 0.23374581336975098\n",
      "Epoch 1804, Loss: 1.028457686305046, Final Batch Loss: 0.2026614397764206\n",
      "Epoch 1805, Loss: 1.1587912738323212, Final Batch Loss: 0.29223909974098206\n",
      "Epoch 1806, Loss: 1.0566549450159073, Final Batch Loss: 0.22051407396793365\n",
      "Epoch 1807, Loss: 1.0879351496696472, Final Batch Loss: 0.29593488574028015\n",
      "Epoch 1808, Loss: 1.0581439435482025, Final Batch Loss: 0.25931641459465027\n",
      "Epoch 1809, Loss: 1.1830895245075226, Final Batch Loss: 0.27260395884513855\n",
      "Epoch 1810, Loss: 1.043187603354454, Final Batch Loss: 0.1728423684835434\n",
      "Epoch 1811, Loss: 1.1203608214855194, Final Batch Loss: 0.16982495784759521\n",
      "Epoch 1812, Loss: 1.0682542771100998, Final Batch Loss: 0.18911047279834747\n",
      "Epoch 1813, Loss: 1.169359102845192, Final Batch Loss: 0.2333931177854538\n",
      "Epoch 1814, Loss: 1.0062605291604996, Final Batch Loss: 0.18375666439533234\n",
      "Epoch 1815, Loss: 1.1714710295200348, Final Batch Loss: 0.16592732071876526\n",
      "Epoch 1816, Loss: 1.1503265500068665, Final Batch Loss: 0.29147449135780334\n",
      "Epoch 1817, Loss: 1.031277909874916, Final Batch Loss: 0.14910854399204254\n",
      "Epoch 1818, Loss: 1.434956431388855, Final Batch Loss: 0.2691764235496521\n",
      "Epoch 1819, Loss: 1.1326066106557846, Final Batch Loss: 0.23907141387462616\n",
      "Epoch 1820, Loss: 1.237621396780014, Final Batch Loss: 0.43990829586982727\n",
      "Epoch 1821, Loss: 1.2486828863620758, Final Batch Loss: 0.3639029264450073\n",
      "Epoch 1822, Loss: 1.0228170454502106, Final Batch Loss: 0.20185448229312897\n",
      "Epoch 1823, Loss: 1.151855781674385, Final Batch Loss: 0.297313392162323\n",
      "Epoch 1824, Loss: 0.9897006303071976, Final Batch Loss: 0.19821690022945404\n",
      "Epoch 1825, Loss: 1.1755070388317108, Final Batch Loss: 0.28916868567466736\n",
      "Epoch 1826, Loss: 1.3562091886997223, Final Batch Loss: 0.5384549498558044\n",
      "Epoch 1827, Loss: 1.1634549796581268, Final Batch Loss: 0.3172909915447235\n",
      "Epoch 1828, Loss: 1.249380201101303, Final Batch Loss: 0.36263036727905273\n",
      "Epoch 1829, Loss: 1.0265513360500336, Final Batch Loss: 0.18359965085983276\n",
      "Epoch 1830, Loss: 1.1422205567359924, Final Batch Loss: 0.28709015250205994\n",
      "Epoch 1831, Loss: 1.2215159237384796, Final Batch Loss: 0.3270857036113739\n",
      "Epoch 1832, Loss: 1.3070073425769806, Final Batch Loss: 0.43895772099494934\n",
      "Epoch 1833, Loss: 1.1162266731262207, Final Batch Loss: 0.2488812804222107\n",
      "Epoch 1834, Loss: 1.0419036895036697, Final Batch Loss: 0.23235009610652924\n",
      "Epoch 1835, Loss: 1.1593212187290192, Final Batch Loss: 0.28240469098091125\n",
      "Epoch 1836, Loss: 1.139914944767952, Final Batch Loss: 0.30806055665016174\n",
      "Epoch 1837, Loss: 0.9931458383798599, Final Batch Loss: 0.17016427218914032\n",
      "Epoch 1838, Loss: 1.119788721203804, Final Batch Loss: 0.3162344694137573\n",
      "Epoch 1839, Loss: 1.0231121480464935, Final Batch Loss: 0.18547196686267853\n",
      "Epoch 1840, Loss: 1.1603029817342758, Final Batch Loss: 0.377927303314209\n",
      "Epoch 1841, Loss: 0.9674649089574814, Final Batch Loss: 0.1805570274591446\n",
      "Epoch 1842, Loss: 1.1313350200653076, Final Batch Loss: 0.284250944852829\n",
      "Epoch 1843, Loss: 1.0438204407691956, Final Batch Loss: 0.16230261325836182\n",
      "Epoch 1844, Loss: 1.0141960829496384, Final Batch Loss: 0.13750573992729187\n",
      "Epoch 1845, Loss: 1.1568192541599274, Final Batch Loss: 0.25469663739204407\n",
      "Epoch 1846, Loss: 1.0800059288740158, Final Batch Loss: 0.15813031792640686\n",
      "Epoch 1847, Loss: 1.1248821169137955, Final Batch Loss: 0.2299339920282364\n",
      "Epoch 1848, Loss: 1.0810475945472717, Final Batch Loss: 0.22787868976593018\n",
      "Epoch 1849, Loss: 1.3252106308937073, Final Batch Loss: 0.34162700176239014\n",
      "Epoch 1850, Loss: 1.270802527666092, Final Batch Loss: 0.41860851645469666\n",
      "Epoch 1851, Loss: 1.2625984847545624, Final Batch Loss: 0.381070613861084\n",
      "Epoch 1852, Loss: 1.177420973777771, Final Batch Loss: 0.3696102797985077\n",
      "Epoch 1853, Loss: 1.1603252291679382, Final Batch Loss: 0.39149829745292664\n",
      "Epoch 1854, Loss: 1.2145171463489532, Final Batch Loss: 0.40320780873298645\n",
      "Epoch 1855, Loss: 1.0426041334867477, Final Batch Loss: 0.213962122797966\n",
      "Epoch 1856, Loss: 1.1000603884458542, Final Batch Loss: 0.2325863093137741\n",
      "Epoch 1857, Loss: 1.0617556422948837, Final Batch Loss: 0.2571711242198944\n",
      "Epoch 1858, Loss: 1.1794383227825165, Final Batch Loss: 0.3460424244403839\n",
      "Epoch 1859, Loss: 0.9909777492284775, Final Batch Loss: 0.14537270367145538\n",
      "Epoch 1860, Loss: 0.9398495852947235, Final Batch Loss: 0.15794524550437927\n",
      "Epoch 1861, Loss: 1.208594411611557, Final Batch Loss: 0.34074124693870544\n",
      "Epoch 1862, Loss: 1.0617156773805618, Final Batch Loss: 0.25591668486595154\n",
      "Epoch 1863, Loss: 1.1957587599754333, Final Batch Loss: 0.26996636390686035\n",
      "Epoch 1864, Loss: 1.2719360291957855, Final Batch Loss: 0.41168275475502014\n",
      "Epoch 1865, Loss: 1.124420702457428, Final Batch Loss: 0.2277989387512207\n",
      "Epoch 1866, Loss: 1.1962106823921204, Final Batch Loss: 0.30360832810401917\n",
      "Epoch 1867, Loss: 1.1953091323375702, Final Batch Loss: 0.35078105330467224\n",
      "Epoch 1868, Loss: 0.9971779882907867, Final Batch Loss: 0.17599163949489594\n",
      "Epoch 1869, Loss: 1.1922191232442856, Final Batch Loss: 0.371774286031723\n",
      "Epoch 1870, Loss: 1.0432077795267105, Final Batch Loss: 0.2186381071805954\n",
      "Epoch 1871, Loss: 1.0857429802417755, Final Batch Loss: 0.2494230717420578\n",
      "Epoch 1872, Loss: 1.1919351518154144, Final Batch Loss: 0.3563898801803589\n",
      "Epoch 1873, Loss: 1.1270670294761658, Final Batch Loss: 0.26848849654197693\n",
      "Epoch 1874, Loss: 1.0297136753797531, Final Batch Loss: 0.2228681445121765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1875, Loss: 1.1051560938358307, Final Batch Loss: 0.2812946140766144\n",
      "Epoch 1876, Loss: 1.028589203953743, Final Batch Loss: 0.2521789073944092\n",
      "Epoch 1877, Loss: 1.1280581802129745, Final Batch Loss: 0.24326403439044952\n",
      "Epoch 1878, Loss: 1.1310684382915497, Final Batch Loss: 0.29517561197280884\n",
      "Epoch 1879, Loss: 1.0033809393644333, Final Batch Loss: 0.1825474500656128\n",
      "Epoch 1880, Loss: 1.2919068932533264, Final Batch Loss: 0.5306987166404724\n",
      "Epoch 1881, Loss: 1.154416263103485, Final Batch Loss: 0.3073473870754242\n",
      "Epoch 1882, Loss: 1.2643749713897705, Final Batch Loss: 0.3884686231613159\n",
      "Epoch 1883, Loss: 1.0713395476341248, Final Batch Loss: 0.15088224411010742\n",
      "Epoch 1884, Loss: 1.0033458769321442, Final Batch Loss: 0.14119353890419006\n",
      "Epoch 1885, Loss: 1.217332899570465, Final Batch Loss: 0.2706731855869293\n",
      "Epoch 1886, Loss: 1.1953724920749664, Final Batch Loss: 0.32719266414642334\n",
      "Epoch 1887, Loss: 1.1854407787322998, Final Batch Loss: 0.27647820115089417\n",
      "Epoch 1888, Loss: 0.9662241041660309, Final Batch Loss: 0.1778048872947693\n",
      "Epoch 1889, Loss: 1.1396630555391312, Final Batch Loss: 0.3517136573791504\n",
      "Epoch 1890, Loss: 1.2575660049915314, Final Batch Loss: 0.3754887878894806\n",
      "Epoch 1891, Loss: 1.1283537596464157, Final Batch Loss: 0.2590571343898773\n",
      "Epoch 1892, Loss: 1.0947168171405792, Final Batch Loss: 0.2155424952507019\n",
      "Epoch 1893, Loss: 1.131812572479248, Final Batch Loss: 0.272258996963501\n",
      "Epoch 1894, Loss: 1.2633440345525742, Final Batch Loss: 0.5387107729911804\n",
      "Epoch 1895, Loss: 1.1752911657094955, Final Batch Loss: 0.21080808341503143\n",
      "Epoch 1896, Loss: 1.0373735576868057, Final Batch Loss: 0.2351980060338974\n",
      "Epoch 1897, Loss: 1.0203272998332977, Final Batch Loss: 0.18802732229232788\n",
      "Epoch 1898, Loss: 1.1580633521080017, Final Batch Loss: 0.27676665782928467\n",
      "Epoch 1899, Loss: 1.1311651468276978, Final Batch Loss: 0.25689640641212463\n",
      "Epoch 1900, Loss: 1.2283117771148682, Final Batch Loss: 0.34669578075408936\n",
      "Epoch 1901, Loss: 1.1067262440919876, Final Batch Loss: 0.32006075978279114\n",
      "Epoch 1902, Loss: 1.215717077255249, Final Batch Loss: 0.37450528144836426\n",
      "Epoch 1903, Loss: 1.3208591043949127, Final Batch Loss: 0.4771933853626251\n",
      "Epoch 1904, Loss: 1.1546765714883804, Final Batch Loss: 0.22471337020397186\n",
      "Epoch 1905, Loss: 1.2015710473060608, Final Batch Loss: 0.398419588804245\n",
      "Epoch 1906, Loss: 0.9764664620161057, Final Batch Loss: 0.21075570583343506\n",
      "Epoch 1907, Loss: 1.129432737827301, Final Batch Loss: 0.4140779674053192\n",
      "Epoch 1908, Loss: 1.1943013221025467, Final Batch Loss: 0.31195953488349915\n",
      "Epoch 1909, Loss: 1.1713100522756577, Final Batch Loss: 0.29003992676734924\n",
      "Epoch 1910, Loss: 1.0431277751922607, Final Batch Loss: 0.1940455585718155\n",
      "Epoch 1911, Loss: 1.0015515983104706, Final Batch Loss: 0.1605064421892166\n",
      "Epoch 1912, Loss: 1.274790734052658, Final Batch Loss: 0.33934691548347473\n",
      "Epoch 1913, Loss: 1.1402325183153152, Final Batch Loss: 0.31532999873161316\n",
      "Epoch 1914, Loss: 1.1472521424293518, Final Batch Loss: 0.3005220293998718\n",
      "Epoch 1915, Loss: 1.1624571532011032, Final Batch Loss: 0.305698424577713\n",
      "Epoch 1916, Loss: 1.072642520070076, Final Batch Loss: 0.18180619180202484\n",
      "Epoch 1917, Loss: 1.2704616785049438, Final Batch Loss: 0.3544262647628784\n",
      "Epoch 1918, Loss: 1.155298188328743, Final Batch Loss: 0.3178752362728119\n",
      "Epoch 1919, Loss: 1.153023138642311, Final Batch Loss: 0.3669787645339966\n",
      "Epoch 1920, Loss: 1.228945016860962, Final Batch Loss: 0.2965438663959503\n",
      "Epoch 1921, Loss: 0.9787396043539047, Final Batch Loss: 0.18747417628765106\n",
      "Epoch 1922, Loss: 1.2098350524902344, Final Batch Loss: 0.3812904357910156\n",
      "Epoch 1923, Loss: 1.1504587680101395, Final Batch Loss: 0.2307976931333542\n",
      "Epoch 1924, Loss: 1.0481974333524704, Final Batch Loss: 0.30814358592033386\n",
      "Epoch 1925, Loss: 1.1099655330181122, Final Batch Loss: 0.3082520067691803\n",
      "Epoch 1926, Loss: 1.110008716583252, Final Batch Loss: 0.20825499296188354\n",
      "Epoch 1927, Loss: 1.1063294410705566, Final Batch Loss: 0.17933475971221924\n",
      "Epoch 1928, Loss: 1.2209464609622955, Final Batch Loss: 0.3337971270084381\n",
      "Epoch 1929, Loss: 1.0846343487501144, Final Batch Loss: 0.24139083921909332\n",
      "Epoch 1930, Loss: 1.1051955819129944, Final Batch Loss: 0.24893450736999512\n",
      "Epoch 1931, Loss: 1.0755030810832977, Final Batch Loss: 0.18708926439285278\n",
      "Epoch 1932, Loss: 1.0021289736032486, Final Batch Loss: 0.2658655047416687\n",
      "Epoch 1933, Loss: 1.3340407609939575, Final Batch Loss: 0.3921873867511749\n",
      "Epoch 1934, Loss: 1.1083549708127975, Final Batch Loss: 0.21472205221652985\n",
      "Epoch 1935, Loss: 1.0098678916692734, Final Batch Loss: 0.1915019005537033\n",
      "Epoch 1936, Loss: 1.0138015747070312, Final Batch Loss: 0.15565697848796844\n",
      "Epoch 1937, Loss: 1.3279046416282654, Final Batch Loss: 0.4977073669433594\n",
      "Epoch 1938, Loss: 1.065972939133644, Final Batch Loss: 0.2657218873500824\n",
      "Epoch 1939, Loss: 1.0511774569749832, Final Batch Loss: 0.20491540431976318\n",
      "Epoch 1940, Loss: 1.1930020153522491, Final Batch Loss: 0.29889267683029175\n",
      "Epoch 1941, Loss: 1.1918461322784424, Final Batch Loss: 0.38129958510398865\n",
      "Epoch 1942, Loss: 1.2671007215976715, Final Batch Loss: 0.3913719654083252\n",
      "Epoch 1943, Loss: 1.0608210116624832, Final Batch Loss: 0.1893319934606552\n",
      "Epoch 1944, Loss: 1.120390847325325, Final Batch Loss: 0.35758209228515625\n",
      "Epoch 1945, Loss: 1.0436695367097855, Final Batch Loss: 0.2262507528066635\n",
      "Epoch 1946, Loss: 1.1518586426973343, Final Batch Loss: 0.24783174693584442\n",
      "Epoch 1947, Loss: 1.3214950859546661, Final Batch Loss: 0.43414172530174255\n",
      "Epoch 1948, Loss: 1.2284567058086395, Final Batch Loss: 0.3565715253353119\n",
      "Epoch 1949, Loss: 1.3366883397102356, Final Batch Loss: 0.3967466652393341\n",
      "Epoch 1950, Loss: 1.1886340975761414, Final Batch Loss: 0.31143495440483093\n",
      "Epoch 1951, Loss: 1.124430075287819, Final Batch Loss: 0.2940211594104767\n",
      "Epoch 1952, Loss: 1.1270509958267212, Final Batch Loss: 0.2776499092578888\n",
      "Epoch 1953, Loss: 1.1271930932998657, Final Batch Loss: 0.33441123366355896\n",
      "Epoch 1954, Loss: 1.0493455529212952, Final Batch Loss: 0.14968514442443848\n",
      "Epoch 1955, Loss: 1.0869158804416656, Final Batch Loss: 0.31511837244033813\n",
      "Epoch 1956, Loss: 1.1805192828178406, Final Batch Loss: 0.3499191999435425\n",
      "Epoch 1957, Loss: 1.1549796164035797, Final Batch Loss: 0.3754511773586273\n",
      "Epoch 1958, Loss: 1.0362328886985779, Final Batch Loss: 0.2468343824148178\n",
      "Epoch 1959, Loss: 1.1710953563451767, Final Batch Loss: 0.40637218952178955\n",
      "Epoch 1960, Loss: 1.0350495874881744, Final Batch Loss: 0.2620275020599365\n",
      "Epoch 1961, Loss: 1.094537764787674, Final Batch Loss: 0.21606546640396118\n",
      "Epoch 1962, Loss: 1.044512689113617, Final Batch Loss: 0.1873776912689209\n",
      "Epoch 1963, Loss: 1.0680804401636124, Final Batch Loss: 0.22774533927440643\n",
      "Epoch 1964, Loss: 1.213043138384819, Final Batch Loss: 0.3645026981830597\n",
      "Epoch 1965, Loss: 1.2103552520275116, Final Batch Loss: 0.2585335075855255\n",
      "Epoch 1966, Loss: 0.9461197853088379, Final Batch Loss: 0.17323844134807587\n",
      "Epoch 1967, Loss: 1.0270951986312866, Final Batch Loss: 0.25063711404800415\n",
      "Epoch 1968, Loss: 1.0226836949586868, Final Batch Loss: 0.15627974271774292\n",
      "Epoch 1969, Loss: 1.1712179481983185, Final Batch Loss: 0.2730921506881714\n",
      "Epoch 1970, Loss: 0.9329590797424316, Final Batch Loss: 0.23980186879634857\n",
      "Epoch 1971, Loss: 1.1186453998088837, Final Batch Loss: 0.23187601566314697\n",
      "Epoch 1972, Loss: 1.1249529868364334, Final Batch Loss: 0.274088054895401\n",
      "Epoch 1973, Loss: 1.1238413453102112, Final Batch Loss: 0.2571495473384857\n",
      "Epoch 1974, Loss: 1.112890288233757, Final Batch Loss: 0.23560132086277008\n",
      "Epoch 1975, Loss: 1.3182417750358582, Final Batch Loss: 0.5109150409698486\n",
      "Epoch 1976, Loss: 1.163758546113968, Final Batch Loss: 0.2927446663379669\n",
      "Epoch 1977, Loss: 1.0944779515266418, Final Batch Loss: 0.24908597767353058\n",
      "Epoch 1978, Loss: 1.2150287926197052, Final Batch Loss: 0.3949465751647949\n",
      "Epoch 1979, Loss: 1.1427113115787506, Final Batch Loss: 0.25043097138404846\n",
      "Epoch 1980, Loss: 1.18176731467247, Final Batch Loss: 0.3167325556278229\n",
      "Epoch 1981, Loss: 1.180473119020462, Final Batch Loss: 0.4011937081813812\n",
      "Epoch 1982, Loss: 1.06820710003376, Final Batch Loss: 0.21614022552967072\n",
      "Epoch 1983, Loss: 1.0191123336553574, Final Batch Loss: 0.17101287841796875\n",
      "Epoch 1984, Loss: 1.0768158733844757, Final Batch Loss: 0.2778971493244171\n",
      "Epoch 1985, Loss: 1.105399951338768, Final Batch Loss: 0.31246417760849\n",
      "Epoch 1986, Loss: 1.1616946309804916, Final Batch Loss: 0.33901944756507874\n",
      "Epoch 1987, Loss: 1.1740940511226654, Final Batch Loss: 0.29398006200790405\n",
      "Epoch 1988, Loss: 1.1272183060646057, Final Batch Loss: 0.32746168971061707\n",
      "Epoch 1989, Loss: 1.1566576063632965, Final Batch Loss: 0.30710265040397644\n",
      "Epoch 1990, Loss: 1.0601710677146912, Final Batch Loss: 0.26828745007514954\n",
      "Epoch 1991, Loss: 1.1818704456090927, Final Batch Loss: 0.37640348076820374\n",
      "Epoch 1992, Loss: 1.1923216581344604, Final Batch Loss: 0.3540113866329193\n",
      "Epoch 1993, Loss: 1.023963749408722, Final Batch Loss: 0.29412031173706055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1994, Loss: 1.1024278700351715, Final Batch Loss: 0.257757306098938\n",
      "Epoch 1995, Loss: 1.2162836194038391, Final Batch Loss: 0.3624459505081177\n",
      "Epoch 1996, Loss: 1.1665264964103699, Final Batch Loss: 0.30883702635765076\n",
      "Epoch 1997, Loss: 1.2594961524009705, Final Batch Loss: 0.42145898938179016\n",
      "Epoch 1998, Loss: 1.2952057123184204, Final Batch Loss: 0.30392348766326904\n",
      "Epoch 1999, Loss: 1.1507584303617477, Final Batch Loss: 0.19741086661815643\n",
      "Epoch 2000, Loss: 1.180069237947464, Final Batch Loss: 0.28471776843070984\n",
      "Epoch 2001, Loss: 1.2324815690517426, Final Batch Loss: 0.24312341213226318\n",
      "Epoch 2002, Loss: 1.216652661561966, Final Batch Loss: 0.28595638275146484\n",
      "Epoch 2003, Loss: 1.1989937126636505, Final Batch Loss: 0.30865317583084106\n",
      "Epoch 2004, Loss: 1.049341008067131, Final Batch Loss: 0.2274247258901596\n",
      "Epoch 2005, Loss: 1.3369979858398438, Final Batch Loss: 0.39100131392478943\n",
      "Epoch 2006, Loss: 1.1677916646003723, Final Batch Loss: 0.2959311902523041\n",
      "Epoch 2007, Loss: 0.9410010501742363, Final Batch Loss: 0.11810781806707382\n",
      "Epoch 2008, Loss: 1.3003508746623993, Final Batch Loss: 0.34804368019104004\n",
      "Epoch 2009, Loss: 1.170189917087555, Final Batch Loss: 0.32556384801864624\n",
      "Epoch 2010, Loss: 1.1747074276208878, Final Batch Loss: 0.2652745544910431\n",
      "Epoch 2011, Loss: 1.0751212239265442, Final Batch Loss: 0.2627618610858917\n",
      "Epoch 2012, Loss: 1.1312581896781921, Final Batch Loss: 0.2944500148296356\n",
      "Epoch 2013, Loss: 1.0889734476804733, Final Batch Loss: 0.3132669925689697\n",
      "Epoch 2014, Loss: 1.0842389911413193, Final Batch Loss: 0.2674482762813568\n",
      "Epoch 2015, Loss: 0.9796847701072693, Final Batch Loss: 0.18948648869991302\n",
      "Epoch 2016, Loss: 1.2043005526065826, Final Batch Loss: 0.4008047878742218\n",
      "Epoch 2017, Loss: 1.1184900999069214, Final Batch Loss: 0.3464412987232208\n",
      "Epoch 2018, Loss: 1.0229566544294357, Final Batch Loss: 0.22156627476215363\n",
      "Epoch 2019, Loss: 1.2370857149362564, Final Batch Loss: 0.367848664522171\n",
      "Epoch 2020, Loss: 1.0334865003824234, Final Batch Loss: 0.18593944609165192\n",
      "Epoch 2021, Loss: 0.9808817431330681, Final Batch Loss: 0.11937897652387619\n",
      "Epoch 2022, Loss: 0.9463859349489212, Final Batch Loss: 0.17257465422153473\n",
      "Epoch 2023, Loss: 1.0284759253263474, Final Batch Loss: 0.1990291029214859\n",
      "Epoch 2024, Loss: 0.9099310338497162, Final Batch Loss: 0.12969382107257843\n",
      "Epoch 2025, Loss: 1.1365173757076263, Final Batch Loss: 0.2761487066745758\n",
      "Epoch 2026, Loss: 1.010855719447136, Final Batch Loss: 0.1786314845085144\n",
      "Epoch 2027, Loss: 1.1631112396717072, Final Batch Loss: 0.27927833795547485\n",
      "Epoch 2028, Loss: 1.1173622161149979, Final Batch Loss: 0.36574968695640564\n",
      "Epoch 2029, Loss: 1.0674416571855545, Final Batch Loss: 0.22058402001857758\n",
      "Epoch 2030, Loss: 1.0120194852352142, Final Batch Loss: 0.23528192937374115\n",
      "Epoch 2031, Loss: 1.0108127146959305, Final Batch Loss: 0.19867204129695892\n",
      "Epoch 2032, Loss: 1.1599750965833664, Final Batch Loss: 0.2204311639070511\n",
      "Epoch 2033, Loss: 1.1154344081878662, Final Batch Loss: 0.2843959331512451\n",
      "Epoch 2034, Loss: 1.2601979970932007, Final Batch Loss: 0.3051455318927765\n",
      "Epoch 2035, Loss: 1.208654761314392, Final Batch Loss: 0.35583817958831787\n",
      "Epoch 2036, Loss: 0.9317785054445267, Final Batch Loss: 0.1966298222541809\n",
      "Epoch 2037, Loss: 1.1186997294425964, Final Batch Loss: 0.3309374451637268\n",
      "Epoch 2038, Loss: 0.8930350914597511, Final Batch Loss: 0.10768083482980728\n",
      "Epoch 2039, Loss: 1.2974055707454681, Final Batch Loss: 0.433932900428772\n",
      "Epoch 2040, Loss: 1.2171519994735718, Final Batch Loss: 0.36706307530403137\n",
      "Epoch 2041, Loss: 1.1133209317922592, Final Batch Loss: 0.21213798224925995\n",
      "Epoch 2042, Loss: 1.2343445718288422, Final Batch Loss: 0.2587718963623047\n",
      "Epoch 2043, Loss: 1.2314405739307404, Final Batch Loss: 0.2723318934440613\n",
      "Epoch 2044, Loss: 1.1069187670946121, Final Batch Loss: 0.32746008038520813\n",
      "Epoch 2045, Loss: 0.9959581047296524, Final Batch Loss: 0.1925278753042221\n",
      "Epoch 2046, Loss: 1.137807935476303, Final Batch Loss: 0.3011897802352905\n",
      "Epoch 2047, Loss: 1.1054779291152954, Final Batch Loss: 0.26938867568969727\n",
      "Epoch 2048, Loss: 1.0933009833097458, Final Batch Loss: 0.27653419971466064\n",
      "Epoch 2049, Loss: 1.0053294599056244, Final Batch Loss: 0.2316174954175949\n",
      "Epoch 2050, Loss: 1.1874345690011978, Final Batch Loss: 0.3238653242588043\n",
      "Epoch 2051, Loss: 0.9470067992806435, Final Batch Loss: 0.10188285261392593\n",
      "Epoch 2052, Loss: 1.150530382990837, Final Batch Loss: 0.3538128137588501\n",
      "Epoch 2053, Loss: 1.114693135023117, Final Batch Loss: 0.2960171401500702\n",
      "Epoch 2054, Loss: 1.0274606049060822, Final Batch Loss: 0.18930722773075104\n",
      "Epoch 2055, Loss: 1.0906099379062653, Final Batch Loss: 0.22113345563411713\n",
      "Epoch 2056, Loss: 1.2227755337953568, Final Batch Loss: 0.453109472990036\n",
      "Epoch 2057, Loss: 0.9565077275037766, Final Batch Loss: 0.1777704805135727\n",
      "Epoch 2058, Loss: 0.909537136554718, Final Batch Loss: 0.15681152045726776\n",
      "Epoch 2059, Loss: 0.8896786868572235, Final Batch Loss: 0.174275204539299\n",
      "Epoch 2060, Loss: 1.1905105262994766, Final Batch Loss: 0.4608626067638397\n",
      "Epoch 2061, Loss: 1.0019785165786743, Final Batch Loss: 0.23528648912906647\n",
      "Epoch 2062, Loss: 1.061809241771698, Final Batch Loss: 0.28537094593048096\n",
      "Epoch 2063, Loss: 1.1034294068813324, Final Batch Loss: 0.29134058952331543\n",
      "Epoch 2064, Loss: 1.1090683490037918, Final Batch Loss: 0.35813769698143005\n",
      "Epoch 2065, Loss: 1.0769684165716171, Final Batch Loss: 0.3067605197429657\n",
      "Epoch 2066, Loss: 1.0537493228912354, Final Batch Loss: 0.3206413686275482\n",
      "Epoch 2067, Loss: 1.0354140251874924, Final Batch Loss: 0.17340095341205597\n",
      "Epoch 2068, Loss: 1.0736132860183716, Final Batch Loss: 0.29206350445747375\n",
      "Epoch 2069, Loss: 1.093405693769455, Final Batch Loss: 0.281255304813385\n",
      "Epoch 2070, Loss: 1.0247813761234283, Final Batch Loss: 0.2357957512140274\n",
      "Epoch 2071, Loss: 1.190769374370575, Final Batch Loss: 0.3993584215641022\n",
      "Epoch 2072, Loss: 1.2831811606884003, Final Batch Loss: 0.3890456259250641\n",
      "Epoch 2073, Loss: 1.164110243320465, Final Batch Loss: 0.24012315273284912\n",
      "Epoch 2074, Loss: 1.238526701927185, Final Batch Loss: 0.23581868410110474\n",
      "Epoch 2075, Loss: 1.1982865035533905, Final Batch Loss: 0.28574350476264954\n",
      "Epoch 2076, Loss: 0.9613633006811142, Final Batch Loss: 0.17823345959186554\n",
      "Epoch 2077, Loss: 1.0976179242134094, Final Batch Loss: 0.33909639716148376\n",
      "Epoch 2078, Loss: 0.9278906285762787, Final Batch Loss: 0.15722383558750153\n",
      "Epoch 2079, Loss: 1.1017425954341888, Final Batch Loss: 0.2623555660247803\n",
      "Epoch 2080, Loss: 1.2830386012792587, Final Batch Loss: 0.5007691979408264\n",
      "Epoch 2081, Loss: 1.0972567647695541, Final Batch Loss: 0.20920901000499725\n",
      "Epoch 2082, Loss: 1.1222774982452393, Final Batch Loss: 0.2828299105167389\n",
      "Epoch 2083, Loss: 1.3245052993297577, Final Batch Loss: 0.452895849943161\n",
      "Epoch 2084, Loss: 1.064347743988037, Final Batch Loss: 0.27486467361450195\n",
      "Epoch 2085, Loss: 1.2405019700527191, Final Batch Loss: 0.4090118408203125\n",
      "Epoch 2086, Loss: 1.2494958639144897, Final Batch Loss: 0.42242559790611267\n",
      "Epoch 2087, Loss: 1.1812698394060135, Final Batch Loss: 0.41480720043182373\n",
      "Epoch 2088, Loss: 1.1696167886257172, Final Batch Loss: 0.2860141098499298\n",
      "Epoch 2089, Loss: 1.1846328228712082, Final Batch Loss: 0.3125283420085907\n",
      "Epoch 2090, Loss: 1.0099966675043106, Final Batch Loss: 0.2888225018978119\n",
      "Epoch 2091, Loss: 1.0919683426618576, Final Batch Loss: 0.2753857672214508\n",
      "Epoch 2092, Loss: 1.1312849074602127, Final Batch Loss: 0.36704328656196594\n",
      "Epoch 2093, Loss: 1.350422978401184, Final Batch Loss: 0.2558418810367584\n",
      "Epoch 2094, Loss: 0.9175491034984589, Final Batch Loss: 0.2011377215385437\n",
      "Epoch 2095, Loss: 1.1165468394756317, Final Batch Loss: 0.3061620891094208\n",
      "Epoch 2096, Loss: 1.045862227678299, Final Batch Loss: 0.22069014608860016\n",
      "Epoch 2097, Loss: 1.2172509133815765, Final Batch Loss: 0.31387773156166077\n",
      "Epoch 2098, Loss: 1.04022516310215, Final Batch Loss: 0.24044083058834076\n",
      "Epoch 2099, Loss: 1.0868301838636398, Final Batch Loss: 0.33637699484825134\n",
      "Epoch 2100, Loss: 1.1225890964269638, Final Batch Loss: 0.23915086686611176\n",
      "Epoch 2101, Loss: 1.0469343066215515, Final Batch Loss: 0.2488248199224472\n",
      "Epoch 2102, Loss: 1.1628047227859497, Final Batch Loss: 0.28705066442489624\n",
      "Epoch 2103, Loss: 1.1081889420747757, Final Batch Loss: 0.39005568623542786\n",
      "Epoch 2104, Loss: 0.9938853979110718, Final Batch Loss: 0.2196076661348343\n",
      "Epoch 2105, Loss: 1.0140673965215683, Final Batch Loss: 0.2037818282842636\n",
      "Epoch 2106, Loss: 0.9031255692243576, Final Batch Loss: 0.13173842430114746\n",
      "Epoch 2107, Loss: 1.2045756578445435, Final Batch Loss: 0.36908766627311707\n",
      "Epoch 2108, Loss: 1.2437409460544586, Final Batch Loss: 0.4965679943561554\n",
      "Epoch 2109, Loss: 1.2306694388389587, Final Batch Loss: 0.354544073343277\n",
      "Epoch 2110, Loss: 1.1825612038373947, Final Batch Loss: 0.3233221769332886\n",
      "Epoch 2111, Loss: 1.0085665732622147, Final Batch Loss: 0.15097694098949432\n",
      "Epoch 2112, Loss: 1.0984709560871124, Final Batch Loss: 0.23694270849227905\n",
      "Epoch 2113, Loss: 1.113045334815979, Final Batch Loss: 0.2789686918258667\n",
      "Epoch 2114, Loss: 1.174918755888939, Final Batch Loss: 0.27605894207954407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2115, Loss: 0.9839958250522614, Final Batch Loss: 0.14649802446365356\n",
      "Epoch 2116, Loss: 1.0656463950872421, Final Batch Loss: 0.1988338977098465\n",
      "Epoch 2117, Loss: 1.0327265113592148, Final Batch Loss: 0.1966799944639206\n",
      "Epoch 2118, Loss: 1.0675718188285828, Final Batch Loss: 0.2538459599018097\n",
      "Epoch 2119, Loss: 1.0266534984111786, Final Batch Loss: 0.22962899506092072\n",
      "Epoch 2120, Loss: 0.90053541213274, Final Batch Loss: 0.10397026687860489\n",
      "Epoch 2121, Loss: 1.054023489356041, Final Batch Loss: 0.23245202004909515\n",
      "Epoch 2122, Loss: 1.3521929681301117, Final Batch Loss: 0.48427537083625793\n",
      "Epoch 2123, Loss: 1.028205156326294, Final Batch Loss: 0.26556655764579773\n",
      "Epoch 2124, Loss: 1.023687019944191, Final Batch Loss: 0.22183763980865479\n",
      "Epoch 2125, Loss: 1.209316909313202, Final Batch Loss: 0.3732609450817108\n",
      "Epoch 2126, Loss: 0.9289917796850204, Final Batch Loss: 0.13863436877727509\n",
      "Epoch 2127, Loss: 0.9733016788959503, Final Batch Loss: 0.20079249143600464\n",
      "Epoch 2128, Loss: 1.0191079080104828, Final Batch Loss: 0.1774933934211731\n",
      "Epoch 2129, Loss: 1.062274232506752, Final Batch Loss: 0.221842423081398\n",
      "Epoch 2130, Loss: 1.0140630155801773, Final Batch Loss: 0.1671469658613205\n",
      "Epoch 2131, Loss: 1.0268130898475647, Final Batch Loss: 0.21830038726329803\n",
      "Epoch 2132, Loss: 1.2199193239212036, Final Batch Loss: 0.37827861309051514\n",
      "Epoch 2133, Loss: 1.1057980358600616, Final Batch Loss: 0.3181459605693817\n",
      "Epoch 2134, Loss: 0.9841307401657104, Final Batch Loss: 0.14864107966423035\n",
      "Epoch 2135, Loss: 1.270743727684021, Final Batch Loss: 0.48409557342529297\n",
      "Epoch 2136, Loss: 1.098526954650879, Final Batch Loss: 0.25918033719062805\n",
      "Epoch 2137, Loss: 1.0601933747529984, Final Batch Loss: 0.22194407880306244\n",
      "Epoch 2138, Loss: 0.9891201108694077, Final Batch Loss: 0.17251215875148773\n",
      "Epoch 2139, Loss: 1.1044330298900604, Final Batch Loss: 0.35533714294433594\n",
      "Epoch 2140, Loss: 1.247658520936966, Final Batch Loss: 0.46433231234550476\n",
      "Epoch 2141, Loss: 1.2763295769691467, Final Batch Loss: 0.3271690011024475\n",
      "Epoch 2142, Loss: 0.9199336171150208, Final Batch Loss: 0.16372741758823395\n",
      "Epoch 2143, Loss: 1.0595383942127228, Final Batch Loss: 0.27439895272254944\n",
      "Epoch 2144, Loss: 1.040202185511589, Final Batch Loss: 0.24621666967868805\n",
      "Epoch 2145, Loss: 1.051447868347168, Final Batch Loss: 0.3047867715358734\n",
      "Epoch 2146, Loss: 0.9542218148708344, Final Batch Loss: 0.19220419228076935\n",
      "Epoch 2147, Loss: 1.1642003953456879, Final Batch Loss: 0.37163451313972473\n",
      "Epoch 2148, Loss: 1.0895043909549713, Final Batch Loss: 0.29654327034950256\n",
      "Epoch 2149, Loss: 1.227979764342308, Final Batch Loss: 0.39489200711250305\n",
      "Epoch 2150, Loss: 1.026907041668892, Final Batch Loss: 0.29590603709220886\n",
      "Epoch 2151, Loss: 0.9821754097938538, Final Batch Loss: 0.19055581092834473\n",
      "Epoch 2152, Loss: 0.9023866355419159, Final Batch Loss: 0.20676223933696747\n",
      "Epoch 2153, Loss: 0.9884382337331772, Final Batch Loss: 0.21110127866268158\n",
      "Epoch 2154, Loss: 0.9500874727964401, Final Batch Loss: 0.17541086673736572\n",
      "Epoch 2155, Loss: 0.9804461002349854, Final Batch Loss: 0.15861529111862183\n",
      "Epoch 2156, Loss: 0.8464639782905579, Final Batch Loss: 0.15103672444820404\n",
      "Epoch 2157, Loss: 1.0659471899271011, Final Batch Loss: 0.20033113658428192\n",
      "Epoch 2158, Loss: 1.1787799298763275, Final Batch Loss: 0.27459338307380676\n",
      "Epoch 2159, Loss: 1.1535305082798004, Final Batch Loss: 0.2609773278236389\n",
      "Epoch 2160, Loss: 1.173872172832489, Final Batch Loss: 0.292201966047287\n",
      "Epoch 2161, Loss: 1.2052858918905258, Final Batch Loss: 0.3499099910259247\n",
      "Epoch 2162, Loss: 1.0623772889375687, Final Batch Loss: 0.23198743164539337\n",
      "Epoch 2163, Loss: 1.1823123395442963, Final Batch Loss: 0.35903409123420715\n",
      "Epoch 2164, Loss: 1.2055634558200836, Final Batch Loss: 0.4117651879787445\n",
      "Epoch 2165, Loss: 1.0717889666557312, Final Batch Loss: 0.3314971327781677\n",
      "Epoch 2166, Loss: 1.0791915208101273, Final Batch Loss: 0.2212129384279251\n",
      "Epoch 2167, Loss: 1.0768815875053406, Final Batch Loss: 0.22788548469543457\n",
      "Epoch 2168, Loss: 1.026189610362053, Final Batch Loss: 0.27911925315856934\n",
      "Epoch 2169, Loss: 1.084692820906639, Final Batch Loss: 0.19687791168689728\n",
      "Epoch 2170, Loss: 1.033167541027069, Final Batch Loss: 0.20683014392852783\n",
      "Epoch 2171, Loss: 1.1328425109386444, Final Batch Loss: 0.33809390664100647\n",
      "Epoch 2172, Loss: 0.8778060674667358, Final Batch Loss: 0.13132630288600922\n",
      "Epoch 2173, Loss: 1.259960800409317, Final Batch Loss: 0.4473179280757904\n",
      "Epoch 2174, Loss: 1.0859807431697845, Final Batch Loss: 0.30634742975234985\n",
      "Epoch 2175, Loss: 0.8916215598583221, Final Batch Loss: 0.17262856662273407\n",
      "Epoch 2176, Loss: 1.2997909933328629, Final Batch Loss: 0.5426836609840393\n",
      "Epoch 2177, Loss: 1.2088143825531006, Final Batch Loss: 0.26521316170692444\n",
      "Epoch 2178, Loss: 0.9998945146799088, Final Batch Loss: 0.2181602269411087\n",
      "Epoch 2179, Loss: 1.0772231966257095, Final Batch Loss: 0.2533952295780182\n",
      "Epoch 2180, Loss: 1.2251248806715012, Final Batch Loss: 0.4053223431110382\n",
      "Epoch 2181, Loss: 1.2441815286874771, Final Batch Loss: 0.39662274718284607\n",
      "Epoch 2182, Loss: 1.1660766750574112, Final Batch Loss: 0.3138537108898163\n",
      "Epoch 2183, Loss: 1.4050913751125336, Final Batch Loss: 0.4086874723434448\n",
      "Epoch 2184, Loss: 1.0568674355745316, Final Batch Loss: 0.23079200088977814\n",
      "Epoch 2185, Loss: 1.467963993549347, Final Batch Loss: 0.5758466124534607\n",
      "Epoch 2186, Loss: 1.137098640203476, Final Batch Loss: 0.23752808570861816\n",
      "Epoch 2187, Loss: 1.0157910585403442, Final Batch Loss: 0.17699049413204193\n",
      "Epoch 2188, Loss: 1.2784579992294312, Final Batch Loss: 0.4154154360294342\n",
      "Epoch 2189, Loss: 1.2435150444507599, Final Batch Loss: 0.41343262791633606\n",
      "Epoch 2190, Loss: 1.1876382529735565, Final Batch Loss: 0.31489992141723633\n",
      "Epoch 2191, Loss: 1.0410638600587845, Final Batch Loss: 0.2389703392982483\n",
      "Epoch 2192, Loss: 1.1372676193714142, Final Batch Loss: 0.32436928153038025\n",
      "Epoch 2193, Loss: 0.9061862826347351, Final Batch Loss: 0.12711341679096222\n",
      "Epoch 2194, Loss: 0.9633235037326813, Final Batch Loss: 0.20132975280284882\n",
      "Epoch 2195, Loss: 1.089964359998703, Final Batch Loss: 0.3307645916938782\n",
      "Epoch 2196, Loss: 0.9206561893224716, Final Batch Loss: 0.15343543887138367\n",
      "Epoch 2197, Loss: 1.0865067541599274, Final Batch Loss: 0.27954766154289246\n",
      "Epoch 2198, Loss: 1.1737749874591827, Final Batch Loss: 0.3917563259601593\n",
      "Epoch 2199, Loss: 1.2135677486658096, Final Batch Loss: 0.3703206479549408\n",
      "Epoch 2200, Loss: 1.005171298980713, Final Batch Loss: 0.1756446361541748\n",
      "Epoch 2201, Loss: 1.0427546948194504, Final Batch Loss: 0.21968525648117065\n",
      "Epoch 2202, Loss: 1.0818869024515152, Final Batch Loss: 0.226755753159523\n",
      "Epoch 2203, Loss: 1.2623249888420105, Final Batch Loss: 0.37119099497795105\n",
      "Epoch 2204, Loss: 1.0169873535633087, Final Batch Loss: 0.16784144937992096\n",
      "Epoch 2205, Loss: 1.0345767587423325, Final Batch Loss: 0.18231795728206635\n",
      "Epoch 2206, Loss: 1.237994834780693, Final Batch Loss: 0.47388896346092224\n",
      "Epoch 2207, Loss: 1.1495274156332016, Final Batch Loss: 0.3636547327041626\n",
      "Epoch 2208, Loss: 1.214822605252266, Final Batch Loss: 0.3911963701248169\n",
      "Epoch 2209, Loss: 0.8813630938529968, Final Batch Loss: 0.1758056879043579\n",
      "Epoch 2210, Loss: 1.0488187670707703, Final Batch Loss: 0.25654879212379456\n",
      "Epoch 2211, Loss: 1.0952673554420471, Final Batch Loss: 0.2953038215637207\n",
      "Epoch 2212, Loss: 0.9714090377092361, Final Batch Loss: 0.22517456114292145\n",
      "Epoch 2213, Loss: 1.1590432971715927, Final Batch Loss: 0.32098835706710815\n",
      "Epoch 2214, Loss: 1.047726646065712, Final Batch Loss: 0.22900108993053436\n",
      "Epoch 2215, Loss: 0.9421223551034927, Final Batch Loss: 0.17382527887821198\n",
      "Epoch 2216, Loss: 1.0447397977113724, Final Batch Loss: 0.3879947364330292\n",
      "Epoch 2217, Loss: 0.9529512226581573, Final Batch Loss: 0.25176358222961426\n",
      "Epoch 2218, Loss: 0.9881403595209122, Final Batch Loss: 0.15158282220363617\n",
      "Epoch 2219, Loss: 1.0317508280277252, Final Batch Loss: 0.28185924887657166\n",
      "Epoch 2220, Loss: 1.076257735490799, Final Batch Loss: 0.24998444318771362\n",
      "Epoch 2221, Loss: 1.0412934869527817, Final Batch Loss: 0.24387484788894653\n",
      "Epoch 2222, Loss: 1.4740126579999924, Final Batch Loss: 0.5646035671234131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2223, Loss: 0.9627524018287659, Final Batch Loss: 0.18366341292858124\n",
      "Epoch 2224, Loss: 1.141909271478653, Final Batch Loss: 0.28712594509124756\n",
      "Epoch 2225, Loss: 0.9757853448390961, Final Batch Loss: 0.18960680067539215\n",
      "Epoch 2226, Loss: 1.016938716173172, Final Batch Loss: 0.2771068215370178\n",
      "Epoch 2227, Loss: 0.9874809384346008, Final Batch Loss: 0.22368331253528595\n",
      "Epoch 2228, Loss: 1.2304214984178543, Final Batch Loss: 0.4201301634311676\n",
      "Epoch 2229, Loss: 1.1996451914310455, Final Batch Loss: 0.3451756238937378\n",
      "Epoch 2230, Loss: 0.9679138883948326, Final Batch Loss: 0.11861249059438705\n",
      "Epoch 2231, Loss: 1.0939772725105286, Final Batch Loss: 0.28827306628227234\n",
      "Epoch 2232, Loss: 1.0061094164848328, Final Batch Loss: 0.24762213230133057\n",
      "Epoch 2233, Loss: 1.1422374546527863, Final Batch Loss: 0.3475058078765869\n",
      "Epoch 2234, Loss: 1.0935970842838287, Final Batch Loss: 0.3565211296081543\n",
      "Epoch 2235, Loss: 0.9289829134941101, Final Batch Loss: 0.16184426844120026\n",
      "Epoch 2236, Loss: 1.2837627679109573, Final Batch Loss: 0.5118264555931091\n",
      "Epoch 2237, Loss: 0.9509505033493042, Final Batch Loss: 0.17913655936717987\n",
      "Epoch 2238, Loss: 1.0590153932571411, Final Batch Loss: 0.28167304396629333\n",
      "Epoch 2239, Loss: 1.086729422211647, Final Batch Loss: 0.27146410942077637\n",
      "Epoch 2240, Loss: 1.0514117181301117, Final Batch Loss: 0.28999775648117065\n",
      "Epoch 2241, Loss: 1.0086232274770737, Final Batch Loss: 0.2593354284763336\n",
      "Epoch 2242, Loss: 0.9823731631040573, Final Batch Loss: 0.13200540840625763\n",
      "Epoch 2243, Loss: 0.8838347680866718, Final Batch Loss: 0.058337558060884476\n",
      "Epoch 2244, Loss: 1.1736226677894592, Final Batch Loss: 0.28731998801231384\n",
      "Epoch 2245, Loss: 1.0334997773170471, Final Batch Loss: 0.2337736040353775\n",
      "Epoch 2246, Loss: 0.9446341097354889, Final Batch Loss: 0.19530194997787476\n",
      "Epoch 2247, Loss: 0.934899091720581, Final Batch Loss: 0.23526965081691742\n",
      "Epoch 2248, Loss: 0.9146858602762222, Final Batch Loss: 0.16589882969856262\n",
      "Epoch 2249, Loss: 0.9976756274700165, Final Batch Loss: 0.27616605162620544\n",
      "Epoch 2250, Loss: 1.2977404445409775, Final Batch Loss: 0.47933658957481384\n",
      "Epoch 2251, Loss: 1.0421016663312912, Final Batch Loss: 0.20856529474258423\n",
      "Epoch 2252, Loss: 1.0244603604078293, Final Batch Loss: 0.20443707704544067\n",
      "Epoch 2253, Loss: 1.0467999130487442, Final Batch Loss: 0.1439521461725235\n",
      "Epoch 2254, Loss: 1.0306438654661179, Final Batch Loss: 0.17071925103664398\n",
      "Epoch 2255, Loss: 1.4895681887865067, Final Batch Loss: 0.5818032026290894\n",
      "Epoch 2256, Loss: 1.1590394079685211, Final Batch Loss: 0.37557634711265564\n",
      "Epoch 2257, Loss: 1.1743636727333069, Final Batch Loss: 0.32617899775505066\n",
      "Epoch 2258, Loss: 1.0021947026252747, Final Batch Loss: 0.2138931304216385\n",
      "Epoch 2259, Loss: 1.1270191371440887, Final Batch Loss: 0.3528631031513214\n",
      "Epoch 2260, Loss: 1.0421635806560516, Final Batch Loss: 0.29938068985939026\n",
      "Epoch 2261, Loss: 0.9891552776098251, Final Batch Loss: 0.17724056541919708\n",
      "Epoch 2262, Loss: 1.116776093840599, Final Batch Loss: 0.34785377979278564\n",
      "Epoch 2263, Loss: 0.911379411816597, Final Batch Loss: 0.13015969097614288\n",
      "Epoch 2264, Loss: 1.0841603577136993, Final Batch Loss: 0.2838033139705658\n",
      "Epoch 2265, Loss: 0.9922285079956055, Final Batch Loss: 0.20747798681259155\n",
      "Epoch 2266, Loss: 1.0611541867256165, Final Batch Loss: 0.23116523027420044\n",
      "Epoch 2267, Loss: 1.0620596259832382, Final Batch Loss: 0.24057386815547943\n",
      "Epoch 2268, Loss: 1.104155957698822, Final Batch Loss: 0.29822102189064026\n",
      "Epoch 2269, Loss: 1.1456419825553894, Final Batch Loss: 0.2841067612171173\n",
      "Epoch 2270, Loss: 1.1004959046840668, Final Batch Loss: 0.22615712881088257\n",
      "Epoch 2271, Loss: 0.9727189838886261, Final Batch Loss: 0.21918219327926636\n",
      "Epoch 2272, Loss: 0.9531340897083282, Final Batch Loss: 0.1418580710887909\n",
      "Epoch 2273, Loss: 1.0758688002824783, Final Batch Loss: 0.2905219495296478\n",
      "Epoch 2274, Loss: 1.1801424473524094, Final Batch Loss: 0.337032288312912\n",
      "Epoch 2275, Loss: 0.9924396574497223, Final Batch Loss: 0.21896976232528687\n",
      "Epoch 2276, Loss: 1.2142214477062225, Final Batch Loss: 0.3459688425064087\n",
      "Epoch 2277, Loss: 1.0707880407571793, Final Batch Loss: 0.20183223485946655\n",
      "Epoch 2278, Loss: 0.9594004899263382, Final Batch Loss: 0.2456454634666443\n",
      "Epoch 2279, Loss: 1.0114207118749619, Final Batch Loss: 0.125040665268898\n",
      "Epoch 2280, Loss: 1.0344643294811249, Final Batch Loss: 0.2230939269065857\n",
      "Epoch 2281, Loss: 0.939394548535347, Final Batch Loss: 0.16077961027622223\n",
      "Epoch 2282, Loss: 2.5121378898620605, Final Batch Loss: 1.7246875762939453\n",
      "Epoch 2283, Loss: 1.0636039823293686, Final Batch Loss: 0.24992656707763672\n",
      "Epoch 2284, Loss: 1.1045456230640411, Final Batch Loss: 0.27383899688720703\n",
      "Epoch 2285, Loss: 1.0926157534122467, Final Batch Loss: 0.2793595790863037\n",
      "Epoch 2286, Loss: 1.0925036519765854, Final Batch Loss: 0.23363067209720612\n",
      "Epoch 2287, Loss: 1.033275067806244, Final Batch Loss: 0.2529045641422272\n",
      "Epoch 2288, Loss: 1.0368774086236954, Final Batch Loss: 0.13975955545902252\n",
      "Epoch 2289, Loss: 1.2244882881641388, Final Batch Loss: 0.3913061320781708\n",
      "Epoch 2290, Loss: 1.1109139919281006, Final Batch Loss: 0.2238120436668396\n",
      "Epoch 2291, Loss: 1.1082721650600433, Final Batch Loss: 0.31889471411705017\n",
      "Epoch 2292, Loss: 0.927595391869545, Final Batch Loss: 0.17762358486652374\n",
      "Epoch 2293, Loss: 1.0831573158502579, Final Batch Loss: 0.1418205350637436\n",
      "Epoch 2294, Loss: 1.1392933875322342, Final Batch Loss: 0.29316943883895874\n",
      "Epoch 2295, Loss: 1.1533409655094147, Final Batch Loss: 0.3135562241077423\n",
      "Epoch 2296, Loss: 1.1445957273244858, Final Batch Loss: 0.3712208569049835\n",
      "Epoch 2297, Loss: 1.1010849475860596, Final Batch Loss: 0.3807288110256195\n",
      "Epoch 2298, Loss: 1.0532619059085846, Final Batch Loss: 0.26515379548072815\n",
      "Epoch 2299, Loss: 0.9636232405900955, Final Batch Loss: 0.20528046786785126\n",
      "Epoch 2300, Loss: 1.1004176437854767, Final Batch Loss: 0.2904352843761444\n",
      "Epoch 2301, Loss: 1.1361500024795532, Final Batch Loss: 0.28083059191703796\n",
      "Epoch 2302, Loss: 1.1002798676490784, Final Batch Loss: 0.25338801741600037\n",
      "Epoch 2303, Loss: 1.1204703450202942, Final Batch Loss: 0.272186815738678\n",
      "Epoch 2304, Loss: 1.1270946711301804, Final Batch Loss: 0.22663326561450958\n",
      "Epoch 2305, Loss: 1.1531782448291779, Final Batch Loss: 0.25707319378852844\n",
      "Epoch 2306, Loss: 1.141345351934433, Final Batch Loss: 0.2874907851219177\n",
      "Epoch 2307, Loss: 1.1331258863210678, Final Batch Loss: 0.3627067804336548\n",
      "Epoch 2308, Loss: 1.1699542701244354, Final Batch Loss: 0.27146872878074646\n",
      "Epoch 2309, Loss: 0.9977773278951645, Final Batch Loss: 0.15002651512622833\n",
      "Epoch 2310, Loss: 1.130704864859581, Final Batch Loss: 0.3475026786327362\n",
      "Epoch 2311, Loss: 1.1550646871328354, Final Batch Loss: 0.32692185044288635\n",
      "Epoch 2312, Loss: 1.1152037680149078, Final Batch Loss: 0.2985338866710663\n",
      "Epoch 2313, Loss: 1.1489196568727493, Final Batch Loss: 0.21386192739009857\n",
      "Epoch 2314, Loss: 1.0147376656532288, Final Batch Loss: 0.27654311060905457\n",
      "Epoch 2315, Loss: 1.1872990280389786, Final Batch Loss: 0.3641538918018341\n",
      "Epoch 2316, Loss: 1.0935486406087875, Final Batch Loss: 0.21082089841365814\n",
      "Epoch 2317, Loss: 1.1309290379285812, Final Batch Loss: 0.23963601887226105\n",
      "Epoch 2318, Loss: 1.1164610385894775, Final Batch Loss: 0.4236813485622406\n",
      "Epoch 2319, Loss: 1.139662191271782, Final Batch Loss: 0.4203355610370636\n",
      "Epoch 2320, Loss: 0.9907502830028534, Final Batch Loss: 0.267731249332428\n",
      "Epoch 2321, Loss: 1.0898758620023727, Final Batch Loss: 0.21298034489154816\n",
      "Epoch 2322, Loss: 1.1069570034742355, Final Batch Loss: 0.3478017747402191\n",
      "Epoch 2323, Loss: 0.9289060235023499, Final Batch Loss: 0.22036923468112946\n",
      "Epoch 2324, Loss: 1.031941905617714, Final Batch Loss: 0.2054082304239273\n",
      "Epoch 2325, Loss: 0.8996246606111526, Final Batch Loss: 0.1726876050233841\n",
      "Epoch 2326, Loss: 0.8666517287492752, Final Batch Loss: 0.12816007435321808\n",
      "Epoch 2327, Loss: 1.061072364449501, Final Batch Loss: 0.2596658766269684\n",
      "Epoch 2328, Loss: 0.9042974263429642, Final Batch Loss: 0.17831529676914215\n",
      "Epoch 2329, Loss: 0.9083068370819092, Final Batch Loss: 0.1708870530128479\n",
      "Epoch 2330, Loss: 1.017317533493042, Final Batch Loss: 0.23617058992385864\n",
      "Epoch 2331, Loss: 1.030638873577118, Final Batch Loss: 0.21397052705287933\n",
      "Epoch 2332, Loss: 1.0214380025863647, Final Batch Loss: 0.27773410081863403\n",
      "Epoch 2333, Loss: 1.0893641859292984, Final Batch Loss: 0.3188031017780304\n",
      "Epoch 2334, Loss: 0.9314552247524261, Final Batch Loss: 0.20429717004299164\n",
      "Epoch 2335, Loss: 1.0604764074087143, Final Batch Loss: 0.24662064015865326\n",
      "Epoch 2336, Loss: 1.10587278008461, Final Batch Loss: 0.29708102345466614\n",
      "Epoch 2337, Loss: 1.0523674190044403, Final Batch Loss: 0.30028554797172546\n",
      "Epoch 2338, Loss: 0.997030034661293, Final Batch Loss: 0.2586846649646759\n",
      "Epoch 2339, Loss: 0.982676550745964, Final Batch Loss: 0.25504863262176514\n",
      "Epoch 2340, Loss: 0.9801550507545471, Final Batch Loss: 0.20161791145801544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2341, Loss: 0.8417253941297531, Final Batch Loss: 0.08073601126670837\n",
      "Epoch 2342, Loss: 0.971091166138649, Final Batch Loss: 0.25447142124176025\n",
      "Epoch 2343, Loss: 1.2198425084352493, Final Batch Loss: 0.45489978790283203\n",
      "Epoch 2344, Loss: 0.9622344076633453, Final Batch Loss: 0.15086834132671356\n",
      "Epoch 2345, Loss: 0.9013738706707954, Final Batch Loss: 0.08712317794561386\n",
      "Epoch 2346, Loss: 1.060547947883606, Final Batch Loss: 0.30877211689949036\n",
      "Epoch 2347, Loss: 1.151922270655632, Final Batch Loss: 0.38267675042152405\n",
      "Epoch 2348, Loss: 1.1820167899131775, Final Batch Loss: 0.42615294456481934\n",
      "Epoch 2349, Loss: 1.0071675777435303, Final Batch Loss: 0.2608906626701355\n",
      "Epoch 2350, Loss: 1.0369712114334106, Final Batch Loss: 0.2060365080833435\n",
      "Epoch 2351, Loss: 1.2056653499603271, Final Batch Loss: 0.3255433440208435\n",
      "Epoch 2352, Loss: 0.8693297654390335, Final Batch Loss: 0.12890173494815826\n",
      "Epoch 2353, Loss: 1.0557780116796494, Final Batch Loss: 0.2717059552669525\n",
      "Epoch 2354, Loss: 0.867279902100563, Final Batch Loss: 0.1359199732542038\n",
      "Epoch 2355, Loss: 1.1422316282987595, Final Batch Loss: 0.30894824862480164\n",
      "Epoch 2356, Loss: 0.9854103475809097, Final Batch Loss: 0.19533811509609222\n",
      "Epoch 2357, Loss: 1.0322183519601822, Final Batch Loss: 0.13579635322093964\n",
      "Epoch 2358, Loss: 1.1057024002075195, Final Batch Loss: 0.2643143832683563\n",
      "Epoch 2359, Loss: 1.0523059368133545, Final Batch Loss: 0.30824020504951477\n",
      "Epoch 2360, Loss: 1.0691374093294144, Final Batch Loss: 0.3804905116558075\n",
      "Epoch 2361, Loss: 1.269111543893814, Final Batch Loss: 0.3759680986404419\n",
      "Epoch 2362, Loss: 1.1000816971063614, Final Batch Loss: 0.2539035975933075\n",
      "Epoch 2363, Loss: 0.9684922397136688, Final Batch Loss: 0.16830237209796906\n",
      "Epoch 2364, Loss: 1.114944726228714, Final Batch Loss: 0.31837379932403564\n",
      "Epoch 2365, Loss: 1.1239488571882248, Final Batch Loss: 0.34088218212127686\n",
      "Epoch 2366, Loss: 1.0346583724021912, Final Batch Loss: 0.2608215808868408\n",
      "Epoch 2367, Loss: 1.1924677789211273, Final Batch Loss: 0.37246695160865784\n",
      "Epoch 2368, Loss: 0.9706604331731796, Final Batch Loss: 0.19374381005764008\n",
      "Epoch 2369, Loss: 1.047482669353485, Final Batch Loss: 0.24994665384292603\n",
      "Epoch 2370, Loss: 1.0140483677387238, Final Batch Loss: 0.24176335334777832\n",
      "Epoch 2371, Loss: 0.9007223397493362, Final Batch Loss: 0.1120443344116211\n",
      "Epoch 2372, Loss: 0.9697767049074173, Final Batch Loss: 0.18228478729724884\n",
      "Epoch 2373, Loss: 1.0427805483341217, Final Batch Loss: 0.2848758101463318\n",
      "Epoch 2374, Loss: 1.1666125357151031, Final Batch Loss: 0.4181075990200043\n",
      "Epoch 2375, Loss: 1.1935918480157852, Final Batch Loss: 0.41794440150260925\n",
      "Epoch 2376, Loss: 1.0478759109973907, Final Batch Loss: 0.28736671805381775\n",
      "Epoch 2377, Loss: 1.0203195065259933, Final Batch Loss: 0.3094104826450348\n",
      "Epoch 2378, Loss: 0.9960334897041321, Final Batch Loss: 0.19017504155635834\n",
      "Epoch 2379, Loss: 0.9532055407762527, Final Batch Loss: 0.22526095807552338\n",
      "Epoch 2380, Loss: 0.9329787492752075, Final Batch Loss: 0.1261259913444519\n",
      "Epoch 2381, Loss: 1.1249507665634155, Final Batch Loss: 0.41565874218940735\n",
      "Epoch 2382, Loss: 1.0268938541412354, Final Batch Loss: 0.1809421330690384\n",
      "Epoch 2383, Loss: 1.193529188632965, Final Batch Loss: 0.3737057149410248\n",
      "Epoch 2384, Loss: 1.0565384775400162, Final Batch Loss: 0.22748015820980072\n",
      "Epoch 2385, Loss: 0.8792243003845215, Final Batch Loss: 0.2190883308649063\n",
      "Epoch 2386, Loss: 0.829828530550003, Final Batch Loss: 0.130038782954216\n",
      "Epoch 2387, Loss: 1.1832730323076248, Final Batch Loss: 0.3217565715312958\n",
      "Epoch 2388, Loss: 0.93977090716362, Final Batch Loss: 0.1587468385696411\n",
      "Epoch 2389, Loss: 1.0863251239061356, Final Batch Loss: 0.2978542149066925\n",
      "Epoch 2390, Loss: 0.9633410274982452, Final Batch Loss: 0.1954530030488968\n",
      "Epoch 2391, Loss: 1.1975515335798264, Final Batch Loss: 0.44296303391456604\n",
      "Epoch 2392, Loss: 0.8423747643828392, Final Batch Loss: 0.11300734430551529\n",
      "Epoch 2393, Loss: 0.8227531760931015, Final Batch Loss: 0.1506025642156601\n",
      "Epoch 2394, Loss: 1.1259740740060806, Final Batch Loss: 0.22272942960262299\n",
      "Epoch 2395, Loss: 1.2082620412111282, Final Batch Loss: 0.3932037353515625\n",
      "Epoch 2396, Loss: 0.9218268692493439, Final Batch Loss: 0.15824000537395477\n",
      "Epoch 2397, Loss: 1.2856355011463165, Final Batch Loss: 0.44295990467071533\n",
      "Epoch 2398, Loss: 1.1018649190664291, Final Batch Loss: 0.2726210057735443\n",
      "Epoch 2399, Loss: 1.1119707226753235, Final Batch Loss: 0.2819429934024811\n",
      "Epoch 2400, Loss: 1.0315076112747192, Final Batch Loss: 0.22582857310771942\n",
      "Epoch 2401, Loss: 0.9551273882389069, Final Batch Loss: 0.281698077917099\n",
      "Epoch 2402, Loss: 0.9785812944173813, Final Batch Loss: 0.23368607461452484\n",
      "Epoch 2403, Loss: 1.0889212936162949, Final Batch Loss: 0.2863236367702484\n",
      "Epoch 2404, Loss: 0.9245412945747375, Final Batch Loss: 0.18353068828582764\n",
      "Epoch 2405, Loss: 1.0368142575025558, Final Batch Loss: 0.25875410437583923\n",
      "Epoch 2406, Loss: 1.0132540315389633, Final Batch Loss: 0.23742787539958954\n",
      "Epoch 2407, Loss: 0.9568463265895844, Final Batch Loss: 0.21124567091464996\n",
      "Epoch 2408, Loss: 0.9256348609924316, Final Batch Loss: 0.2441561073064804\n",
      "Epoch 2409, Loss: 0.9253902286291122, Final Batch Loss: 0.19171889126300812\n",
      "Epoch 2410, Loss: 0.9359977543354034, Final Batch Loss: 0.12113815546035767\n",
      "Epoch 2411, Loss: 1.0419766008853912, Final Batch Loss: 0.29404446482658386\n",
      "Epoch 2412, Loss: 0.9279147237539291, Final Batch Loss: 0.1940569132566452\n",
      "Epoch 2413, Loss: 1.0254432559013367, Final Batch Loss: 0.2020624279975891\n",
      "Epoch 2414, Loss: 0.9490098059177399, Final Batch Loss: 0.20002450048923492\n",
      "Epoch 2415, Loss: 1.1202085465192795, Final Batch Loss: 0.31705060601234436\n",
      "Epoch 2416, Loss: 1.142931267619133, Final Batch Loss: 0.38303303718566895\n",
      "Epoch 2417, Loss: 1.161665454506874, Final Batch Loss: 0.48272502422332764\n",
      "Epoch 2418, Loss: 1.1707538068294525, Final Batch Loss: 0.28295445442199707\n",
      "Epoch 2419, Loss: 1.000622995197773, Final Batch Loss: 0.12175974994897842\n",
      "Epoch 2420, Loss: 1.0572442710399628, Final Batch Loss: 0.35257795453071594\n",
      "Epoch 2421, Loss: 1.1541244685649872, Final Batch Loss: 0.3564142882823944\n",
      "Epoch 2422, Loss: 0.9505570083856583, Final Batch Loss: 0.19579045474529266\n",
      "Epoch 2423, Loss: 1.0102374702692032, Final Batch Loss: 0.2525434195995331\n",
      "Epoch 2424, Loss: 1.0829860121011734, Final Batch Loss: 0.24232111871242523\n",
      "Epoch 2425, Loss: 0.940248042345047, Final Batch Loss: 0.18730522692203522\n",
      "Epoch 2426, Loss: 1.1969938427209854, Final Batch Loss: 0.4507054090499878\n",
      "Epoch 2427, Loss: 0.9114966094493866, Final Batch Loss: 0.2612749934196472\n",
      "Epoch 2428, Loss: 1.2690888792276382, Final Batch Loss: 0.5284265875816345\n",
      "Epoch 2429, Loss: 1.0063349157571793, Final Batch Loss: 0.2351677566766739\n",
      "Epoch 2430, Loss: 0.9795031547546387, Final Batch Loss: 0.15652015805244446\n",
      "Epoch 2431, Loss: 1.0737668722867966, Final Batch Loss: 0.3228317201137543\n",
      "Epoch 2432, Loss: 1.1884918808937073, Final Batch Loss: 0.3539837598800659\n",
      "Epoch 2433, Loss: 1.016974851489067, Final Batch Loss: 0.2491883635520935\n",
      "Epoch 2434, Loss: 1.0193903893232346, Final Batch Loss: 0.2388177067041397\n",
      "Epoch 2435, Loss: 1.064084231853485, Final Batch Loss: 0.2061067819595337\n",
      "Epoch 2436, Loss: 0.9279953688383102, Final Batch Loss: 0.20345772802829742\n",
      "Epoch 2437, Loss: 1.0770826488733292, Final Batch Loss: 0.29677003622055054\n",
      "Epoch 2438, Loss: 0.9679456800222397, Final Batch Loss: 0.1928405910730362\n",
      "Epoch 2439, Loss: 1.012147679924965, Final Batch Loss: 0.16611911356449127\n",
      "Epoch 2440, Loss: 0.9977579563856125, Final Batch Loss: 0.1935162991285324\n",
      "Epoch 2441, Loss: 0.9365251511335373, Final Batch Loss: 0.1596086174249649\n",
      "Epoch 2442, Loss: 1.063072845339775, Final Batch Loss: 0.12961693108081818\n",
      "Epoch 2443, Loss: 1.0399833470582962, Final Batch Loss: 0.2733341455459595\n",
      "Epoch 2444, Loss: 1.1784384101629257, Final Batch Loss: 0.4452013671398163\n",
      "Epoch 2445, Loss: 1.08614581823349, Final Batch Loss: 0.19685377180576324\n",
      "Epoch 2446, Loss: 1.1970366835594177, Final Batch Loss: 0.3553984463214874\n",
      "Epoch 2447, Loss: 0.95906662940979, Final Batch Loss: 0.28498375415802\n",
      "Epoch 2448, Loss: 1.0246590375900269, Final Batch Loss: 0.2464447021484375\n",
      "Epoch 2449, Loss: 0.992433950304985, Final Batch Loss: 0.16789399087429047\n",
      "Epoch 2450, Loss: 1.0758662223815918, Final Batch Loss: 0.20889168977737427\n",
      "Epoch 2451, Loss: 1.1397383511066437, Final Batch Loss: 0.2026464343070984\n",
      "Epoch 2452, Loss: 0.8764794617891312, Final Batch Loss: 0.20003747940063477\n",
      "Epoch 2453, Loss: 0.9210252612829208, Final Batch Loss: 0.15935125946998596\n",
      "Epoch 2454, Loss: 1.0631159991025925, Final Batch Loss: 0.3510415554046631\n",
      "Epoch 2455, Loss: 1.2131377458572388, Final Batch Loss: 0.4445491135120392\n",
      "Epoch 2456, Loss: 0.9545546621084213, Final Batch Loss: 0.15463675558567047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2457, Loss: 0.9651827365159988, Final Batch Loss: 0.1674613207578659\n",
      "Epoch 2458, Loss: 0.9956735670566559, Final Batch Loss: 0.19890053570270538\n",
      "Epoch 2459, Loss: 0.9133454114198685, Final Batch Loss: 0.15210697054862976\n",
      "Epoch 2460, Loss: 1.1455857902765274, Final Batch Loss: 0.2437364012002945\n",
      "Epoch 2461, Loss: 1.167076200246811, Final Batch Loss: 0.2618696689605713\n",
      "Epoch 2462, Loss: 0.9897026121616364, Final Batch Loss: 0.17746229469776154\n",
      "Epoch 2463, Loss: 0.8957777470350266, Final Batch Loss: 0.18934817612171173\n",
      "Epoch 2464, Loss: 1.1481893956661224, Final Batch Loss: 0.3855881690979004\n",
      "Epoch 2465, Loss: 0.995487779378891, Final Batch Loss: 0.3100254535675049\n",
      "Epoch 2466, Loss: 0.9376391619443893, Final Batch Loss: 0.10237246751785278\n",
      "Epoch 2467, Loss: 0.9647349566221237, Final Batch Loss: 0.23374295234680176\n",
      "Epoch 2468, Loss: 1.024794578552246, Final Batch Loss: 0.17406576871871948\n",
      "Epoch 2469, Loss: 1.0079709589481354, Final Batch Loss: 0.2571514844894409\n",
      "Epoch 2470, Loss: 1.036319226026535, Final Batch Loss: 0.24974338710308075\n",
      "Epoch 2471, Loss: 1.0470996499061584, Final Batch Loss: 0.2746451795101166\n",
      "Epoch 2472, Loss: 0.8875351846218109, Final Batch Loss: 0.13919733464717865\n",
      "Epoch 2473, Loss: 1.2411822229623795, Final Batch Loss: 0.48791369795799255\n",
      "Epoch 2474, Loss: 1.0560255348682404, Final Batch Loss: 0.31445255875587463\n",
      "Epoch 2475, Loss: 0.9180691689252853, Final Batch Loss: 0.3089238703250885\n",
      "Epoch 2476, Loss: 0.826453723013401, Final Batch Loss: 0.11538717895746231\n",
      "Epoch 2477, Loss: 1.1388976871967316, Final Batch Loss: 0.37612542510032654\n",
      "Epoch 2478, Loss: 1.179253101348877, Final Batch Loss: 0.45604145526885986\n",
      "Epoch 2479, Loss: 1.0741722881793976, Final Batch Loss: 0.27027732133865356\n",
      "Epoch 2480, Loss: 1.0785042643547058, Final Batch Loss: 0.2531936764717102\n",
      "Epoch 2481, Loss: 0.9707196205854416, Final Batch Loss: 0.1820010393857956\n",
      "Epoch 2482, Loss: 1.024187445640564, Final Batch Loss: 0.2972348630428314\n",
      "Epoch 2483, Loss: 1.1279587596654892, Final Batch Loss: 0.2452838271856308\n",
      "Epoch 2484, Loss: 0.8994095623493195, Final Batch Loss: 0.15116113424301147\n",
      "Epoch 2485, Loss: 0.8770453035831451, Final Batch Loss: 0.1628250628709793\n",
      "Epoch 2486, Loss: 1.0498613119125366, Final Batch Loss: 0.2686907947063446\n",
      "Epoch 2487, Loss: 1.0687010139226913, Final Batch Loss: 0.21433337032794952\n",
      "Epoch 2488, Loss: 0.93704953789711, Final Batch Loss: 0.18818016350269318\n",
      "Epoch 2489, Loss: 0.9856661111116409, Final Batch Loss: 0.2641652524471283\n",
      "Epoch 2490, Loss: 0.8991926312446594, Final Batch Loss: 0.15630146861076355\n",
      "Epoch 2491, Loss: 1.0900481045246124, Final Batch Loss: 0.379671186208725\n",
      "Epoch 2492, Loss: 1.037416934967041, Final Batch Loss: 0.3198993504047394\n",
      "Epoch 2493, Loss: 1.165748342871666, Final Batch Loss: 0.2446463257074356\n",
      "Epoch 2494, Loss: 0.9621179848909378, Final Batch Loss: 0.28485652804374695\n",
      "Epoch 2495, Loss: 0.9319474548101425, Final Batch Loss: 0.14142052829265594\n",
      "Epoch 2496, Loss: 1.0611077100038528, Final Batch Loss: 0.27389323711395264\n",
      "Epoch 2497, Loss: 1.0288249403238297, Final Batch Loss: 0.2950412333011627\n",
      "Epoch 2498, Loss: 0.9522690027952194, Final Batch Loss: 0.15470921993255615\n",
      "Epoch 2499, Loss: 0.8951208591461182, Final Batch Loss: 0.15726222097873688\n",
      "Epoch 2500, Loss: 1.0129140764474869, Final Batch Loss: 0.2658446729183197\n",
      "Epoch 2501, Loss: 1.1012368351221085, Final Batch Loss: 0.20382916927337646\n",
      "Epoch 2502, Loss: 0.9377576112747192, Final Batch Loss: 0.22855456173419952\n",
      "Epoch 2503, Loss: 1.0659219026565552, Final Batch Loss: 0.28258392214775085\n",
      "Epoch 2504, Loss: 1.1684699654579163, Final Batch Loss: 0.431411474943161\n",
      "Epoch 2505, Loss: 1.0037617832422256, Final Batch Loss: 0.20204831659793854\n",
      "Epoch 2506, Loss: 1.0845101326704025, Final Batch Loss: 0.20396919548511505\n",
      "Epoch 2507, Loss: 0.9459685981273651, Final Batch Loss: 0.2771192789077759\n",
      "Epoch 2508, Loss: 0.9343161135911942, Final Batch Loss: 0.14457592368125916\n",
      "Epoch 2509, Loss: 1.1092268824577332, Final Batch Loss: 0.35549506545066833\n",
      "Epoch 2510, Loss: 1.1077322959899902, Final Batch Loss: 0.26835647225379944\n",
      "Epoch 2511, Loss: 1.058656170964241, Final Batch Loss: 0.31867191195487976\n",
      "Epoch 2512, Loss: 1.1918106824159622, Final Batch Loss: 0.41147902607917786\n",
      "Epoch 2513, Loss: 1.231961965560913, Final Batch Loss: 0.36824020743370056\n",
      "Epoch 2514, Loss: 0.9179673790931702, Final Batch Loss: 0.14743776619434357\n",
      "Epoch 2515, Loss: 1.0160171538591385, Final Batch Loss: 0.27971240878105164\n",
      "Epoch 2516, Loss: 1.1263968795537949, Final Batch Loss: 0.24581094086170197\n",
      "Epoch 2517, Loss: 0.9111439883708954, Final Batch Loss: 0.20700396597385406\n",
      "Epoch 2518, Loss: 1.0073847025632858, Final Batch Loss: 0.2500266432762146\n",
      "Epoch 2519, Loss: 1.1048327386379242, Final Batch Loss: 0.36729416251182556\n",
      "Epoch 2520, Loss: 1.0673511177301407, Final Batch Loss: 0.3079623281955719\n",
      "Epoch 2521, Loss: 1.1005049645900726, Final Batch Loss: 0.2809395492076874\n",
      "Epoch 2522, Loss: 1.109689638018608, Final Batch Loss: 0.37577199935913086\n",
      "Epoch 2523, Loss: 0.9599051028490067, Final Batch Loss: 0.1670713573694229\n",
      "Epoch 2524, Loss: 0.9308655112981796, Final Batch Loss: 0.1317271888256073\n",
      "Epoch 2525, Loss: 0.9799632281064987, Final Batch Loss: 0.21958374977111816\n",
      "Epoch 2526, Loss: 1.0029463171958923, Final Batch Loss: 0.2369905710220337\n",
      "Epoch 2527, Loss: 0.981382891535759, Final Batch Loss: 0.2362843006849289\n",
      "Epoch 2528, Loss: 0.8419846668839455, Final Batch Loss: 0.10236463695764542\n",
      "Epoch 2529, Loss: 0.9501934796571732, Final Batch Loss: 0.19631560146808624\n",
      "Epoch 2530, Loss: 1.0045940279960632, Final Batch Loss: 0.22878384590148926\n",
      "Epoch 2531, Loss: 1.0584977716207504, Final Batch Loss: 0.37992775440216064\n",
      "Epoch 2532, Loss: 1.0256655514240265, Final Batch Loss: 0.2416755110025406\n",
      "Epoch 2533, Loss: 0.972451701760292, Final Batch Loss: 0.2617206275463104\n",
      "Epoch 2534, Loss: 1.0688727647066116, Final Batch Loss: 0.17806069552898407\n",
      "Epoch 2535, Loss: 0.9158369079232216, Final Batch Loss: 0.090858094394207\n",
      "Epoch 2536, Loss: 1.1099739372730255, Final Batch Loss: 0.32949355244636536\n",
      "Epoch 2537, Loss: 1.0827975273132324, Final Batch Loss: 0.35876667499542236\n",
      "Epoch 2538, Loss: 0.9029542952775955, Final Batch Loss: 0.2508699595928192\n",
      "Epoch 2539, Loss: 0.9603144377470016, Final Batch Loss: 0.20538978278636932\n",
      "Epoch 2540, Loss: 1.0174718350172043, Final Batch Loss: 0.3177674114704132\n",
      "Epoch 2541, Loss: 0.9923307299613953, Final Batch Loss: 0.23025888204574585\n",
      "Epoch 2542, Loss: 0.8963375985622406, Final Batch Loss: 0.1921248584985733\n",
      "Epoch 2543, Loss: 1.1826287060976028, Final Batch Loss: 0.35938742756843567\n",
      "Epoch 2544, Loss: 0.9095840752124786, Final Batch Loss: 0.1495882123708725\n",
      "Epoch 2545, Loss: 1.0498139411211014, Final Batch Loss: 0.17554672062397003\n",
      "Epoch 2546, Loss: 0.9224914610385895, Final Batch Loss: 0.19587798416614532\n",
      "Epoch 2547, Loss: 0.9401673972606659, Final Batch Loss: 0.27843907475471497\n",
      "Epoch 2548, Loss: 1.0835868567228317, Final Batch Loss: 0.3347263038158417\n",
      "Epoch 2549, Loss: 0.964372456073761, Final Batch Loss: 0.2017609030008316\n",
      "Epoch 2550, Loss: 0.9796335399150848, Final Batch Loss: 0.24926058948040009\n",
      "Epoch 2551, Loss: 0.9247273355722427, Final Batch Loss: 0.1680837720632553\n",
      "Epoch 2552, Loss: 1.029154285788536, Final Batch Loss: 0.22463715076446533\n",
      "Epoch 2553, Loss: 1.0740745812654495, Final Batch Loss: 0.32913652062416077\n",
      "Epoch 2554, Loss: 0.9336355775594711, Final Batch Loss: 0.1824771910905838\n",
      "Epoch 2555, Loss: 1.0737942159175873, Final Batch Loss: 0.3973793089389801\n",
      "Epoch 2556, Loss: 0.9447097927331924, Final Batch Loss: 0.21689645946025848\n",
      "Epoch 2557, Loss: 1.1552494764328003, Final Batch Loss: 0.3423742949962616\n",
      "Epoch 2558, Loss: 0.8925532847642899, Final Batch Loss: 0.1630609929561615\n",
      "Epoch 2559, Loss: 1.001981481909752, Final Batch Loss: 0.22079789638519287\n",
      "Epoch 2560, Loss: 1.0011253207921982, Final Batch Loss: 0.2898075580596924\n",
      "Epoch 2561, Loss: 1.1395747661590576, Final Batch Loss: 0.38323894143104553\n",
      "Epoch 2562, Loss: 1.0600106120109558, Final Batch Loss: 0.2529970705509186\n",
      "Epoch 2563, Loss: 1.057669311761856, Final Batch Loss: 0.25185415148735046\n",
      "Epoch 2564, Loss: 1.1816106885671616, Final Batch Loss: 0.39571237564086914\n",
      "Epoch 2565, Loss: 1.0535775274038315, Final Batch Loss: 0.1769995540380478\n",
      "Epoch 2566, Loss: 1.1018475890159607, Final Batch Loss: 0.3337212800979614\n",
      "Epoch 2567, Loss: 1.0384461730718613, Final Batch Loss: 0.19416050612926483\n",
      "Epoch 2568, Loss: 0.9928397685289383, Final Batch Loss: 0.2106233835220337\n",
      "Epoch 2569, Loss: 0.8760417401790619, Final Batch Loss: 0.14355871081352234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2570, Loss: 1.0113529711961746, Final Batch Loss: 0.2775306701660156\n",
      "Epoch 2571, Loss: 0.8711409866809845, Final Batch Loss: 0.09565922617912292\n",
      "Epoch 2572, Loss: 0.9784254729747772, Final Batch Loss: 0.2455945611000061\n",
      "Epoch 2573, Loss: 0.9224057048559189, Final Batch Loss: 0.2700740098953247\n",
      "Epoch 2574, Loss: 1.0086723864078522, Final Batch Loss: 0.28601542115211487\n",
      "Epoch 2575, Loss: 1.0150014162063599, Final Batch Loss: 0.2530845105648041\n",
      "Epoch 2576, Loss: 0.9809141457080841, Final Batch Loss: 0.24833112955093384\n",
      "Epoch 2577, Loss: 1.0411102771759033, Final Batch Loss: 0.3223468065261841\n",
      "Epoch 2578, Loss: 1.0203500390052795, Final Batch Loss: 0.20460724830627441\n",
      "Epoch 2579, Loss: 0.9747428894042969, Final Batch Loss: 0.21690809726715088\n",
      "Epoch 2580, Loss: 0.9888438284397125, Final Batch Loss: 0.20918618142604828\n",
      "Epoch 2581, Loss: 1.0496399700641632, Final Batch Loss: 0.261648952960968\n",
      "Epoch 2582, Loss: 1.0063591748476028, Final Batch Loss: 0.24401690065860748\n",
      "Epoch 2583, Loss: 1.069179967045784, Final Batch Loss: 0.3595215976238251\n",
      "Epoch 2584, Loss: 0.885771855711937, Final Batch Loss: 0.14963297545909882\n",
      "Epoch 2585, Loss: 0.9718802571296692, Final Batch Loss: 0.29360705614089966\n",
      "Epoch 2586, Loss: 0.8089470863342285, Final Batch Loss: 0.15116287767887115\n",
      "Epoch 2587, Loss: 1.1305053532123566, Final Batch Loss: 0.25724467635154724\n",
      "Epoch 2588, Loss: 1.0046805441379547, Final Batch Loss: 0.2212257981300354\n",
      "Epoch 2589, Loss: 1.0541542768478394, Final Batch Loss: 0.256599098443985\n",
      "Epoch 2590, Loss: 0.9285068511962891, Final Batch Loss: 0.2164406180381775\n",
      "Epoch 2591, Loss: 0.9932318478822708, Final Batch Loss: 0.31698402762413025\n",
      "Epoch 2592, Loss: 1.038023218512535, Final Batch Loss: 0.23909761011600494\n",
      "Epoch 2593, Loss: 1.0223441570997238, Final Batch Loss: 0.27779826521873474\n",
      "Epoch 2594, Loss: 1.305606096982956, Final Batch Loss: 0.5156590938568115\n",
      "Epoch 2595, Loss: 1.0399603247642517, Final Batch Loss: 0.26773402094841003\n",
      "Epoch 2596, Loss: 0.9065859317779541, Final Batch Loss: 0.19377607107162476\n",
      "Epoch 2597, Loss: 1.1410216689109802, Final Batch Loss: 0.3143675923347473\n",
      "Epoch 2598, Loss: 1.0901992172002792, Final Batch Loss: 0.310023695230484\n",
      "Epoch 2599, Loss: 0.973816990852356, Final Batch Loss: 0.24028714001178741\n",
      "Epoch 2600, Loss: 1.031638190150261, Final Batch Loss: 0.21139861643314362\n",
      "Epoch 2601, Loss: 1.061420038342476, Final Batch Loss: 0.27653634548187256\n",
      "Epoch 2602, Loss: 1.0586650371551514, Final Batch Loss: 0.3658100366592407\n",
      "Epoch 2603, Loss: 0.9806304425001144, Final Batch Loss: 0.21254844963550568\n",
      "Epoch 2604, Loss: 1.0139864534139633, Final Batch Loss: 0.22560442984104156\n",
      "Epoch 2605, Loss: 1.1211041063070297, Final Batch Loss: 0.34909355640411377\n",
      "Epoch 2606, Loss: 1.066068857908249, Final Batch Loss: 0.2840970456600189\n",
      "Epoch 2607, Loss: 0.983481228351593, Final Batch Loss: 0.28549131751060486\n",
      "Epoch 2608, Loss: 0.9292908012866974, Final Batch Loss: 0.19019681215286255\n",
      "Epoch 2609, Loss: 1.0604001730680466, Final Batch Loss: 0.23658303916454315\n",
      "Epoch 2610, Loss: 1.0210835188627243, Final Batch Loss: 0.3597361743450165\n",
      "Epoch 2611, Loss: 0.9843793958425522, Final Batch Loss: 0.2581806778907776\n",
      "Epoch 2612, Loss: 0.8938788622617722, Final Batch Loss: 0.1953873187303543\n",
      "Epoch 2613, Loss: 0.9925958067178726, Final Batch Loss: 0.23541021347045898\n",
      "Epoch 2614, Loss: 0.8946486711502075, Final Batch Loss: 0.16800785064697266\n",
      "Epoch 2615, Loss: 0.9033278971910477, Final Batch Loss: 0.1952732652425766\n",
      "Epoch 2616, Loss: 1.0444394648075104, Final Batch Loss: 0.3495130240917206\n",
      "Epoch 2617, Loss: 0.9089642763137817, Final Batch Loss: 0.2093387246131897\n",
      "Epoch 2618, Loss: 0.9745333194732666, Final Batch Loss: 0.13410423696041107\n",
      "Epoch 2619, Loss: 1.135744720697403, Final Batch Loss: 0.30129674077033997\n",
      "Epoch 2620, Loss: 1.0918273031711578, Final Batch Loss: 0.2621649205684662\n",
      "Epoch 2621, Loss: 0.9254160970449448, Final Batch Loss: 0.2485181838274002\n",
      "Epoch 2622, Loss: 1.2330268025398254, Final Batch Loss: 0.36683475971221924\n",
      "Epoch 2623, Loss: 0.9297534823417664, Final Batch Loss: 0.1608179658651352\n",
      "Epoch 2624, Loss: 1.1153551936149597, Final Batch Loss: 0.252420574426651\n",
      "Epoch 2625, Loss: 1.220310926437378, Final Batch Loss: 0.4346521198749542\n",
      "Epoch 2626, Loss: 0.9293195009231567, Final Batch Loss: 0.2015986442565918\n",
      "Epoch 2627, Loss: 0.9697395414113998, Final Batch Loss: 0.24969382584095\n",
      "Epoch 2628, Loss: 0.8972740322351456, Final Batch Loss: 0.1338672786951065\n",
      "Epoch 2629, Loss: 1.0305331349372864, Final Batch Loss: 0.3087175786495209\n",
      "Epoch 2630, Loss: 0.8495113253593445, Final Batch Loss: 0.12524905800819397\n",
      "Epoch 2631, Loss: 1.1399487257003784, Final Batch Loss: 0.23477023839950562\n",
      "Epoch 2632, Loss: 1.3340037316083908, Final Batch Loss: 0.5676822662353516\n",
      "Epoch 2633, Loss: 1.0755769610404968, Final Batch Loss: 0.34843477606773376\n",
      "Epoch 2634, Loss: 1.0012005269527435, Final Batch Loss: 0.2435859590768814\n",
      "Epoch 2635, Loss: 0.9941098242998123, Final Batch Loss: 0.16364343464374542\n",
      "Epoch 2636, Loss: 1.1496090292930603, Final Batch Loss: 0.3675720989704132\n",
      "Epoch 2637, Loss: 1.0583268404006958, Final Batch Loss: 0.17222368717193604\n",
      "Epoch 2638, Loss: 1.0622603595256805, Final Batch Loss: 0.3043278753757477\n",
      "Epoch 2639, Loss: 1.1316534727811813, Final Batch Loss: 0.18916864693164825\n",
      "Epoch 2640, Loss: 0.924996942281723, Final Batch Loss: 0.2004258632659912\n",
      "Epoch 2641, Loss: 1.0329753756523132, Final Batch Loss: 0.34319376945495605\n",
      "Epoch 2642, Loss: 1.0243542939424515, Final Batch Loss: 0.324905663728714\n",
      "Epoch 2643, Loss: 1.081010788679123, Final Batch Loss: 0.3087567687034607\n",
      "Epoch 2644, Loss: 1.1799394637346268, Final Batch Loss: 0.459477037191391\n",
      "Epoch 2645, Loss: 0.9486615508794785, Final Batch Loss: 0.23370391130447388\n",
      "Epoch 2646, Loss: 1.1029965281486511, Final Batch Loss: 0.2678453326225281\n",
      "Epoch 2647, Loss: 0.8707111179828644, Final Batch Loss: 0.16822125017642975\n",
      "Epoch 2648, Loss: 1.1216547191143036, Final Batch Loss: 0.2615094482898712\n",
      "Epoch 2649, Loss: 1.0459032356739044, Final Batch Loss: 0.27124735713005066\n",
      "Epoch 2650, Loss: 0.977850928902626, Final Batch Loss: 0.2354034185409546\n",
      "Epoch 2651, Loss: 0.9322885125875473, Final Batch Loss: 0.18057197332382202\n",
      "Epoch 2652, Loss: 0.9762332886457443, Final Batch Loss: 0.22611503303050995\n",
      "Epoch 2653, Loss: 0.9140282720327377, Final Batch Loss: 0.26085036993026733\n",
      "Epoch 2654, Loss: 0.9737814366817474, Final Batch Loss: 0.24691079556941986\n",
      "Epoch 2655, Loss: 0.891484260559082, Final Batch Loss: 0.14022576808929443\n",
      "Epoch 2656, Loss: 1.0029848664999008, Final Batch Loss: 0.2744481861591339\n",
      "Epoch 2657, Loss: 1.054951697587967, Final Batch Loss: 0.3342273533344269\n",
      "Epoch 2658, Loss: 0.9874021708965302, Final Batch Loss: 0.24335765838623047\n",
      "Epoch 2659, Loss: 1.0293996930122375, Final Batch Loss: 0.29400745034217834\n",
      "Epoch 2660, Loss: 1.0880401730537415, Final Batch Loss: 0.27657976746559143\n",
      "Epoch 2661, Loss: 1.0857752561569214, Final Batch Loss: 0.36117076873779297\n",
      "Epoch 2662, Loss: 0.9273259937763214, Final Batch Loss: 0.19604289531707764\n",
      "Epoch 2663, Loss: 1.029684841632843, Final Batch Loss: 0.24705515801906586\n",
      "Epoch 2664, Loss: 0.9748427048325539, Final Batch Loss: 0.10391173511743546\n",
      "Epoch 2665, Loss: 1.1647428423166275, Final Batch Loss: 0.4775444269180298\n",
      "Epoch 2666, Loss: 0.876734048128128, Final Batch Loss: 0.20452816784381866\n",
      "Epoch 2667, Loss: 0.9402201920747757, Final Batch Loss: 0.2501227557659149\n",
      "Epoch 2668, Loss: 0.9707395732402802, Final Batch Loss: 0.22455750405788422\n",
      "Epoch 2669, Loss: 1.1136292964220047, Final Batch Loss: 0.34772542119026184\n",
      "Epoch 2670, Loss: 0.8631731420755386, Final Batch Loss: 0.17485328018665314\n",
      "Epoch 2671, Loss: 0.9260566085577011, Final Batch Loss: 0.2659432888031006\n",
      "Epoch 2672, Loss: 1.0970515608787537, Final Batch Loss: 0.30568453669548035\n",
      "Epoch 2673, Loss: 0.980273962020874, Final Batch Loss: 0.1687292903661728\n",
      "Epoch 2674, Loss: 0.9069273918867111, Final Batch Loss: 0.15184883773326874\n",
      "Epoch 2675, Loss: 0.9414833337068558, Final Batch Loss: 0.1773330569267273\n",
      "Epoch 2676, Loss: 1.0495516657829285, Final Batch Loss: 0.3220961093902588\n",
      "Epoch 2677, Loss: 1.055338516831398, Final Batch Loss: 0.3386485278606415\n",
      "Epoch 2678, Loss: 1.036712110042572, Final Batch Loss: 0.32545599341392517\n",
      "Epoch 2679, Loss: 0.9149221777915955, Final Batch Loss: 0.2216651290655136\n",
      "Epoch 2680, Loss: 0.918707013130188, Final Batch Loss: 0.26423749327659607\n",
      "Epoch 2681, Loss: 1.025671362876892, Final Batch Loss: 0.2772804796695709\n",
      "Epoch 2682, Loss: 1.0271052718162537, Final Batch Loss: 0.20716804265975952\n",
      "Epoch 2683, Loss: 0.9798683673143387, Final Batch Loss: 0.2717377245426178\n",
      "Epoch 2684, Loss: 0.9058683812618256, Final Batch Loss: 0.1778051108121872\n",
      "Epoch 2685, Loss: 0.9385088384151459, Final Batch Loss: 0.3144693672657013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2686, Loss: 0.8841194659471512, Final Batch Loss: 0.1784721463918686\n",
      "Epoch 2687, Loss: 0.774186335504055, Final Batch Loss: 0.10452937334775925\n",
      "Epoch 2688, Loss: 1.0733893662691116, Final Batch Loss: 0.38293150067329407\n",
      "Epoch 2689, Loss: 1.0578272640705109, Final Batch Loss: 0.3329276144504547\n",
      "Epoch 2690, Loss: 1.0057633072137833, Final Batch Loss: 0.2784021198749542\n",
      "Epoch 2691, Loss: 0.998710885643959, Final Batch Loss: 0.20535309612751007\n",
      "Epoch 2692, Loss: 1.0497942566871643, Final Batch Loss: 0.2920214831829071\n",
      "Epoch 2693, Loss: 1.1375051736831665, Final Batch Loss: 0.4239913523197174\n",
      "Epoch 2694, Loss: 0.9380895420908928, Final Batch Loss: 0.12483879178762436\n",
      "Epoch 2695, Loss: 1.2812643945217133, Final Batch Loss: 0.39309176802635193\n",
      "Epoch 2696, Loss: 1.0335922539234161, Final Batch Loss: 0.3172314465045929\n",
      "Epoch 2697, Loss: 1.016973838210106, Final Batch Loss: 0.2747141122817993\n",
      "Epoch 2698, Loss: 1.0850726068019867, Final Batch Loss: 0.25445833802223206\n",
      "Epoch 2699, Loss: 1.114045724272728, Final Batch Loss: 0.27160629630088806\n",
      "Epoch 2700, Loss: 1.065512478351593, Final Batch Loss: 0.30383509397506714\n",
      "Epoch 2701, Loss: 1.135249748826027, Final Batch Loss: 0.3999979794025421\n",
      "Epoch 2702, Loss: 1.147527500987053, Final Batch Loss: 0.3894976079463959\n",
      "Epoch 2703, Loss: 0.8619026690721512, Final Batch Loss: 0.16771401464939117\n",
      "Epoch 2704, Loss: 0.9267335534095764, Final Batch Loss: 0.2328725904226303\n",
      "Epoch 2705, Loss: 0.9564134031534195, Final Batch Loss: 0.2158517986536026\n",
      "Epoch 2706, Loss: 1.0135248750448227, Final Batch Loss: 0.21640680730342865\n",
      "Epoch 2707, Loss: 0.8406118303537369, Final Batch Loss: 0.20529834926128387\n",
      "Epoch 2708, Loss: 0.9285079538822174, Final Batch Loss: 0.21091611683368683\n",
      "Epoch 2709, Loss: 0.9247808307409286, Final Batch Loss: 0.18990929424762726\n",
      "Epoch 2710, Loss: 0.9861866682767868, Final Batch Loss: 0.3291565477848053\n",
      "Epoch 2711, Loss: 0.8864839524030685, Final Batch Loss: 0.16888105869293213\n",
      "Epoch 2712, Loss: 1.141047865152359, Final Batch Loss: 0.36232027411460876\n",
      "Epoch 2713, Loss: 0.9807448089122772, Final Batch Loss: 0.29305651783943176\n",
      "Epoch 2714, Loss: 0.886241003870964, Final Batch Loss: 0.1792610138654709\n",
      "Epoch 2715, Loss: 0.8957231491804123, Final Batch Loss: 0.16742946207523346\n",
      "Epoch 2716, Loss: 0.9901818335056305, Final Batch Loss: 0.14946451783180237\n",
      "Epoch 2717, Loss: 1.0647805780172348, Final Batch Loss: 0.23279033601284027\n",
      "Epoch 2718, Loss: 0.884304404258728, Final Batch Loss: 0.16860826313495636\n",
      "Epoch 2719, Loss: 0.9206322431564331, Final Batch Loss: 0.19659800827503204\n",
      "Epoch 2720, Loss: 0.9327993988990784, Final Batch Loss: 0.20503157377243042\n",
      "Epoch 2721, Loss: 0.9943633079528809, Final Batch Loss: 0.17790377140045166\n",
      "Epoch 2722, Loss: 1.0924378484487534, Final Batch Loss: 0.2444041520357132\n",
      "Epoch 2723, Loss: 0.9466876536607742, Final Batch Loss: 0.2061282843351364\n",
      "Epoch 2724, Loss: 1.07814921438694, Final Batch Loss: 0.23390908539295197\n",
      "Epoch 2725, Loss: 1.1867589354515076, Final Batch Loss: 0.4711538255214691\n",
      "Epoch 2726, Loss: 0.9255278408527374, Final Batch Loss: 0.24263684451580048\n",
      "Epoch 2727, Loss: 1.0822057873010635, Final Batch Loss: 0.34225475788116455\n",
      "Epoch 2728, Loss: 0.9965880066156387, Final Batch Loss: 0.2815258800983429\n",
      "Epoch 2729, Loss: 0.9679399728775024, Final Batch Loss: 0.21398895978927612\n",
      "Epoch 2730, Loss: 0.8716188967227936, Final Batch Loss: 0.16384638845920563\n",
      "Epoch 2731, Loss: 0.8969955295324326, Final Batch Loss: 0.2039928287267685\n",
      "Epoch 2732, Loss: 1.078686073422432, Final Batch Loss: 0.2067696452140808\n",
      "Epoch 2733, Loss: 1.0203349590301514, Final Batch Loss: 0.33473241329193115\n",
      "Epoch 2734, Loss: 0.9539884775876999, Final Batch Loss: 0.17572306096553802\n",
      "Epoch 2735, Loss: 0.9736878871917725, Final Batch Loss: 0.21924585103988647\n",
      "Epoch 2736, Loss: 1.1244818717241287, Final Batch Loss: 0.24557293951511383\n",
      "Epoch 2737, Loss: 0.9503684043884277, Final Batch Loss: 0.2706720232963562\n",
      "Epoch 2738, Loss: 0.8803887516260147, Final Batch Loss: 0.1760663390159607\n",
      "Epoch 2739, Loss: 0.8739453703165054, Final Batch Loss: 0.19092844426631927\n",
      "Epoch 2740, Loss: 0.9033416956663132, Final Batch Loss: 0.1515907496213913\n",
      "Epoch 2741, Loss: 0.9316833764314651, Final Batch Loss: 0.1993492692708969\n",
      "Epoch 2742, Loss: 1.055361494421959, Final Batch Loss: 0.36173343658447266\n",
      "Epoch 2743, Loss: 0.9739673584699631, Final Batch Loss: 0.14218752086162567\n",
      "Epoch 2744, Loss: 0.8866251111030579, Final Batch Loss: 0.14188088476657867\n",
      "Epoch 2745, Loss: 1.0140136182308197, Final Batch Loss: 0.26073843240737915\n",
      "Epoch 2746, Loss: 1.09177266061306, Final Batch Loss: 0.31073132157325745\n",
      "Epoch 2747, Loss: 0.9949317127466202, Final Batch Loss: 0.18750351667404175\n",
      "Epoch 2748, Loss: 0.9021486192941666, Final Batch Loss: 0.19290153682231903\n",
      "Epoch 2749, Loss: 0.831815704703331, Final Batch Loss: 0.15123087167739868\n",
      "Epoch 2750, Loss: 0.8578275889158249, Final Batch Loss: 0.201297327876091\n",
      "Epoch 2751, Loss: 1.014901265501976, Final Batch Loss: 0.29673609137535095\n",
      "Epoch 2752, Loss: 0.9104353189468384, Final Batch Loss: 0.1667054146528244\n",
      "Epoch 2753, Loss: 0.9410810247063637, Final Batch Loss: 0.11616954952478409\n",
      "Epoch 2754, Loss: 1.178846850991249, Final Batch Loss: 0.43649303913116455\n",
      "Epoch 2755, Loss: 0.8012218698859215, Final Batch Loss: 0.0810341015458107\n",
      "Epoch 2756, Loss: 0.9465001374483109, Final Batch Loss: 0.21438384056091309\n",
      "Epoch 2757, Loss: 1.103449821472168, Final Batch Loss: 0.3385380208492279\n",
      "Epoch 2758, Loss: 0.8275329545140266, Final Batch Loss: 0.07972439378499985\n",
      "Epoch 2759, Loss: 1.005434662103653, Final Batch Loss: 0.16824208199977875\n",
      "Epoch 2760, Loss: 1.0012410879135132, Final Batch Loss: 0.26807472109794617\n",
      "Epoch 2761, Loss: 0.8434786349534988, Final Batch Loss: 0.14789645373821259\n",
      "Epoch 2762, Loss: 1.158537745475769, Final Batch Loss: 0.36124303936958313\n",
      "Epoch 2763, Loss: 0.9269260615110397, Final Batch Loss: 0.22282099723815918\n",
      "Epoch 2764, Loss: 1.037135750055313, Final Batch Loss: 0.1819635033607483\n",
      "Epoch 2765, Loss: 1.0425768792629242, Final Batch Loss: 0.1956215500831604\n",
      "Epoch 2766, Loss: 1.0741191804409027, Final Batch Loss: 0.26659438014030457\n",
      "Epoch 2767, Loss: 0.9773157313466072, Final Batch Loss: 0.11594704538583755\n",
      "Epoch 2768, Loss: 1.2069875299930573, Final Batch Loss: 0.2612553834915161\n",
      "Epoch 2769, Loss: 1.0289369970560074, Final Batch Loss: 0.14941181242465973\n",
      "Epoch 2770, Loss: 0.8950026705861092, Final Batch Loss: 0.08880876749753952\n",
      "Epoch 2771, Loss: 1.0887399166822433, Final Batch Loss: 0.3326016366481781\n",
      "Epoch 2772, Loss: 1.0968980342149734, Final Batch Loss: 0.30761632323265076\n",
      "Epoch 2773, Loss: 1.1316090524196625, Final Batch Loss: 0.36761245131492615\n",
      "Epoch 2774, Loss: 1.0785026848316193, Final Batch Loss: 0.33015382289886475\n",
      "Epoch 2775, Loss: 1.2617882192134857, Final Batch Loss: 0.4559398889541626\n",
      "Epoch 2776, Loss: 0.9357822388410568, Final Batch Loss: 0.1919700652360916\n",
      "Epoch 2777, Loss: 1.074956163764, Final Batch Loss: 0.19341613352298737\n",
      "Epoch 2778, Loss: 1.0487680584192276, Final Batch Loss: 0.3263082206249237\n",
      "Epoch 2779, Loss: 1.027634784579277, Final Batch Loss: 0.3746687173843384\n",
      "Epoch 2780, Loss: 1.12516587972641, Final Batch Loss: 0.24101132154464722\n",
      "Epoch 2781, Loss: 1.0148938298225403, Final Batch Loss: 0.267365962266922\n",
      "Epoch 2782, Loss: 1.0453655421733856, Final Batch Loss: 0.3126843571662903\n",
      "Epoch 2783, Loss: 1.0130909830331802, Final Batch Loss: 0.2967491149902344\n",
      "Epoch 2784, Loss: 1.1072027683258057, Final Batch Loss: 0.42383649945259094\n",
      "Epoch 2785, Loss: 0.8634829074144363, Final Batch Loss: 0.19119827449321747\n",
      "Epoch 2786, Loss: 0.893778532743454, Final Batch Loss: 0.23933418095111847\n",
      "Epoch 2787, Loss: 0.9556155651807785, Final Batch Loss: 0.2009672075510025\n",
      "Epoch 2788, Loss: 0.9070814698934555, Final Batch Loss: 0.21296484768390656\n",
      "Epoch 2789, Loss: 1.070028007030487, Final Batch Loss: 0.30754104256629944\n",
      "Epoch 2790, Loss: 0.9861796796321869, Final Batch Loss: 0.31053808331489563\n",
      "Epoch 2791, Loss: 0.9369388967752457, Final Batch Loss: 0.22009159624576569\n",
      "Epoch 2792, Loss: 1.1479169428348541, Final Batch Loss: 0.31347420811653137\n",
      "Epoch 2793, Loss: 0.9706773906946182, Final Batch Loss: 0.20383518934249878\n",
      "Epoch 2794, Loss: 1.0174189358949661, Final Batch Loss: 0.3176165521144867\n",
      "Epoch 2795, Loss: 1.2238185852766037, Final Batch Loss: 0.4135031998157501\n",
      "Epoch 2796, Loss: 0.970148965716362, Final Batch Loss: 0.21107681095600128\n",
      "Epoch 2797, Loss: 0.8817687481641769, Final Batch Loss: 0.16628725826740265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2798, Loss: 0.8964814022183418, Final Batch Loss: 0.11945372074842453\n",
      "Epoch 2799, Loss: 1.0227600634098053, Final Batch Loss: 0.19803665578365326\n",
      "Epoch 2800, Loss: 0.986883357167244, Final Batch Loss: 0.2546086311340332\n",
      "Epoch 2801, Loss: 1.0772799402475357, Final Batch Loss: 0.30407461524009705\n",
      "Epoch 2802, Loss: 1.1136680841445923, Final Batch Loss: 0.35144665837287903\n",
      "Epoch 2803, Loss: 1.0637188404798508, Final Batch Loss: 0.30482447147369385\n",
      "Epoch 2804, Loss: 1.0724558532238007, Final Batch Loss: 0.2624429166316986\n",
      "Epoch 2805, Loss: 1.0723160952329636, Final Batch Loss: 0.18995143473148346\n",
      "Epoch 2806, Loss: 1.2251745164394379, Final Batch Loss: 0.30323299765586853\n",
      "Epoch 2807, Loss: 1.005457803606987, Final Batch Loss: 0.22985310852527618\n",
      "Epoch 2808, Loss: 1.2209277153015137, Final Batch Loss: 0.4500296413898468\n",
      "Epoch 2809, Loss: 1.132968619465828, Final Batch Loss: 0.4578726291656494\n",
      "Epoch 2810, Loss: 1.0514452755451202, Final Batch Loss: 0.19872397184371948\n",
      "Epoch 2811, Loss: 0.9372460693120956, Final Batch Loss: 0.2503893971443176\n",
      "Epoch 2812, Loss: 0.9243277907371521, Final Batch Loss: 0.19620515406131744\n",
      "Epoch 2813, Loss: 0.9436978399753571, Final Batch Loss: 0.17502538859844208\n",
      "Epoch 2814, Loss: 0.9488416016101837, Final Batch Loss: 0.21366436779499054\n",
      "Epoch 2815, Loss: 1.2668943852186203, Final Batch Loss: 0.3640671670436859\n",
      "Epoch 2816, Loss: 0.9061071127653122, Final Batch Loss: 0.15911583602428436\n",
      "Epoch 2817, Loss: 0.9964989423751831, Final Batch Loss: 0.24407511949539185\n",
      "Epoch 2818, Loss: 0.9665227234363556, Final Batch Loss: 0.2677420675754547\n",
      "Epoch 2819, Loss: 0.9331526160240173, Final Batch Loss: 0.24382470548152924\n",
      "Epoch 2820, Loss: 1.098914086818695, Final Batch Loss: 0.26824429631233215\n",
      "Epoch 2821, Loss: 0.9643773436546326, Final Batch Loss: 0.24734461307525635\n",
      "Epoch 2822, Loss: 1.001027524471283, Final Batch Loss: 0.2910595238208771\n",
      "Epoch 2823, Loss: 0.9664756208658218, Final Batch Loss: 0.2309107929468155\n",
      "Epoch 2824, Loss: 1.010143831372261, Final Batch Loss: 0.19975726306438446\n",
      "Epoch 2825, Loss: 1.0363265424966812, Final Batch Loss: 0.2546478509902954\n",
      "Epoch 2826, Loss: 0.9019266813993454, Final Batch Loss: 0.13556094467639923\n",
      "Epoch 2827, Loss: 0.9069321900606155, Final Batch Loss: 0.2066107541322708\n",
      "Epoch 2828, Loss: 0.871441200375557, Final Batch Loss: 0.22567759454250336\n",
      "Epoch 2829, Loss: 1.046249270439148, Final Batch Loss: 0.2768218219280243\n",
      "Epoch 2830, Loss: 1.033079817891121, Final Batch Loss: 0.24737055599689484\n",
      "Epoch 2831, Loss: 1.0154882371425629, Final Batch Loss: 0.15254203975200653\n",
      "Epoch 2832, Loss: 0.8092551082372665, Final Batch Loss: 0.1289336234331131\n",
      "Epoch 2833, Loss: 0.9350379258394241, Final Batch Loss: 0.23668430745601654\n",
      "Epoch 2834, Loss: 1.3190341889858246, Final Batch Loss: 0.6287996768951416\n",
      "Epoch 2835, Loss: 1.0268703997135162, Final Batch Loss: 0.2240106463432312\n",
      "Epoch 2836, Loss: 0.8589900434017181, Final Batch Loss: 0.14937762916088104\n",
      "Epoch 2837, Loss: 1.1293238252401352, Final Batch Loss: 0.33657824993133545\n",
      "Epoch 2838, Loss: 0.9378906190395355, Final Batch Loss: 0.19585555791854858\n",
      "Epoch 2839, Loss: 1.2679944932460785, Final Batch Loss: 0.5338360667228699\n",
      "Epoch 2840, Loss: 0.9470814019441605, Final Batch Loss: 0.19111360609531403\n",
      "Epoch 2841, Loss: 1.2142896503210068, Final Batch Loss: 0.40541794896125793\n",
      "Epoch 2842, Loss: 0.9460027366876602, Final Batch Loss: 0.14669308066368103\n",
      "Epoch 2843, Loss: 0.9712684899568558, Final Batch Loss: 0.2872517704963684\n",
      "Epoch 2844, Loss: 0.8576997965574265, Final Batch Loss: 0.15398365259170532\n",
      "Epoch 2845, Loss: 1.0267179012298584, Final Batch Loss: 0.23777741193771362\n",
      "Epoch 2846, Loss: 0.8971615582704544, Final Batch Loss: 0.18138718605041504\n",
      "Epoch 2847, Loss: 0.9204393625259399, Final Batch Loss: 0.21297530829906464\n",
      "Epoch 2848, Loss: 0.8953882306814194, Final Batch Loss: 0.2215026170015335\n",
      "Epoch 2849, Loss: 1.1148826777935028, Final Batch Loss: 0.3421076238155365\n",
      "Epoch 2850, Loss: 0.8766436278820038, Final Batch Loss: 0.19540667533874512\n",
      "Epoch 2851, Loss: 1.0876227617263794, Final Batch Loss: 0.35018405318260193\n",
      "Epoch 2852, Loss: 1.0020646452903748, Final Batch Loss: 0.2203441709280014\n",
      "Epoch 2853, Loss: 1.0810560286045074, Final Batch Loss: 0.3195858299732208\n",
      "Epoch 2854, Loss: 0.987495169043541, Final Batch Loss: 0.19610927999019623\n",
      "Epoch 2855, Loss: 0.8980071991682053, Final Batch Loss: 0.17136424779891968\n",
      "Epoch 2856, Loss: 1.1225126832723618, Final Batch Loss: 0.339342325925827\n",
      "Epoch 2857, Loss: 0.9606603682041168, Final Batch Loss: 0.20470879971981049\n",
      "Epoch 2858, Loss: 0.9396966993808746, Final Batch Loss: 0.2914395034313202\n",
      "Epoch 2859, Loss: 1.0100573897361755, Final Batch Loss: 0.2268327921628952\n",
      "Epoch 2860, Loss: 1.0070901215076447, Final Batch Loss: 0.24552643299102783\n",
      "Epoch 2861, Loss: 0.976021483540535, Final Batch Loss: 0.2335970550775528\n",
      "Epoch 2862, Loss: 1.0630915462970734, Final Batch Loss: 0.2778087556362152\n",
      "Epoch 2863, Loss: 0.8766418695449829, Final Batch Loss: 0.1515512764453888\n",
      "Epoch 2864, Loss: 0.8826131820678711, Final Batch Loss: 0.19727174937725067\n",
      "Epoch 2865, Loss: 0.9072979241609573, Final Batch Loss: 0.19745047390460968\n",
      "Epoch 2866, Loss: 0.942825973033905, Final Batch Loss: 0.14572550356388092\n",
      "Epoch 2867, Loss: 0.8803684413433075, Final Batch Loss: 0.1748698353767395\n",
      "Epoch 2868, Loss: 0.8764970749616623, Final Batch Loss: 0.12663929164409637\n",
      "Epoch 2869, Loss: 1.0015935748815536, Final Batch Loss: 0.26801517605781555\n",
      "Epoch 2870, Loss: 1.1436616331338882, Final Batch Loss: 0.48001787066459656\n",
      "Epoch 2871, Loss: 0.8682326525449753, Final Batch Loss: 0.17489050328731537\n",
      "Epoch 2872, Loss: 1.265730082988739, Final Batch Loss: 0.48363634943962097\n",
      "Epoch 2873, Loss: 1.052456185221672, Final Batch Loss: 0.36018356680870056\n",
      "Epoch 2874, Loss: 0.8449603021144867, Final Batch Loss: 0.16788595914840698\n",
      "Epoch 2875, Loss: 1.0305333137512207, Final Batch Loss: 0.29039767384529114\n",
      "Epoch 2876, Loss: 0.860499307513237, Final Batch Loss: 0.17042626440525055\n",
      "Epoch 2877, Loss: 0.9291318207979202, Final Batch Loss: 0.13286541402339935\n",
      "Epoch 2878, Loss: 0.873521089553833, Final Batch Loss: 0.10794630646705627\n",
      "Epoch 2879, Loss: 0.9661830961704254, Final Batch Loss: 0.22149044275283813\n",
      "Epoch 2880, Loss: 1.0036088973283768, Final Batch Loss: 0.27680036425590515\n",
      "Epoch 2881, Loss: 1.002556025981903, Final Batch Loss: 0.25078535079956055\n",
      "Epoch 2882, Loss: 0.8654199838638306, Final Batch Loss: 0.16430562734603882\n",
      "Epoch 2883, Loss: 0.9645903557538986, Final Batch Loss: 0.275602787733078\n",
      "Epoch 2884, Loss: 1.128465324640274, Final Batch Loss: 0.343346506357193\n",
      "Epoch 2885, Loss: 1.0162145048379898, Final Batch Loss: 0.3334980309009552\n",
      "Epoch 2886, Loss: 0.9916587769985199, Final Batch Loss: 0.23829741775989532\n",
      "Epoch 2887, Loss: 1.0301341861486435, Final Batch Loss: 0.31346842646598816\n",
      "Epoch 2888, Loss: 0.9320105463266373, Final Batch Loss: 0.13234563171863556\n",
      "Epoch 2889, Loss: 0.8706698715686798, Final Batch Loss: 0.15775202214717865\n",
      "Epoch 2890, Loss: 0.9401481598615646, Final Batch Loss: 0.18900148570537567\n",
      "Epoch 2891, Loss: 1.0045069009065628, Final Batch Loss: 0.2879194915294647\n",
      "Epoch 2892, Loss: 0.9752772152423859, Final Batch Loss: 0.24057672917842865\n",
      "Epoch 2893, Loss: 0.8796941190958023, Final Batch Loss: 0.23454070091247559\n",
      "Epoch 2894, Loss: 0.8987676203250885, Final Batch Loss: 0.19281668961048126\n",
      "Epoch 2895, Loss: 0.8335159122943878, Final Batch Loss: 0.08570629358291626\n",
      "Epoch 2896, Loss: 0.9011871218681335, Final Batch Loss: 0.12290441989898682\n",
      "Epoch 2897, Loss: 0.8859536200761795, Final Batch Loss: 0.1225275993347168\n",
      "Epoch 2898, Loss: 0.7685783728957176, Final Batch Loss: 0.11534007638692856\n",
      "Epoch 2899, Loss: 1.0435006320476532, Final Batch Loss: 0.38257691264152527\n",
      "Epoch 2900, Loss: 1.0310224890708923, Final Batch Loss: 0.267177015542984\n",
      "Epoch 2901, Loss: 0.831304058432579, Final Batch Loss: 0.13686561584472656\n",
      "Epoch 2902, Loss: 0.8888376504182816, Final Batch Loss: 0.13922707736492157\n",
      "Epoch 2903, Loss: 0.9884805232286453, Final Batch Loss: 0.3017391562461853\n",
      "Epoch 2904, Loss: 0.89495849609375, Final Batch Loss: 0.17910605669021606\n",
      "Epoch 2905, Loss: 0.9247552156448364, Final Batch Loss: 0.2012527585029602\n",
      "Epoch 2906, Loss: 0.8678176403045654, Final Batch Loss: 0.19053514301776886\n",
      "Epoch 2907, Loss: 0.9955248385667801, Final Batch Loss: 0.2944406270980835\n",
      "Epoch 2908, Loss: 1.0163637846708298, Final Batch Loss: 0.1997184157371521\n",
      "Epoch 2909, Loss: 1.0239545702934265, Final Batch Loss: 0.25388985872268677\n",
      "Epoch 2910, Loss: 0.89973483979702, Final Batch Loss: 0.23724843561649323\n",
      "Epoch 2911, Loss: 0.8537745028734207, Final Batch Loss: 0.16086004674434662\n",
      "Epoch 2912, Loss: 0.893930584192276, Final Batch Loss: 0.17741228640079498\n",
      "Epoch 2913, Loss: 0.9769684672355652, Final Batch Loss: 0.24265284836292267\n",
      "Epoch 2914, Loss: 0.953310489654541, Final Batch Loss: 0.19651027023792267\n",
      "Epoch 2915, Loss: 0.9042326211929321, Final Batch Loss: 0.17736952006816864\n",
      "Epoch 2916, Loss: 1.1361190378665924, Final Batch Loss: 0.3880586326122284\n",
      "Epoch 2917, Loss: 1.0127958208322525, Final Batch Loss: 0.3311198055744171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2918, Loss: 0.9588225483894348, Final Batch Loss: 0.25102999806404114\n",
      "Epoch 2919, Loss: 0.9481523036956787, Final Batch Loss: 0.2986557185649872\n",
      "Epoch 2920, Loss: 1.0349257737398148, Final Batch Loss: 0.22940993309020996\n",
      "Epoch 2921, Loss: 0.9573485106229782, Final Batch Loss: 0.25021791458129883\n",
      "Epoch 2922, Loss: 1.012904867529869, Final Batch Loss: 0.26744183897972107\n",
      "Epoch 2923, Loss: 0.9344264417886734, Final Batch Loss: 0.18943564593791962\n",
      "Epoch 2924, Loss: 0.9398434907197952, Final Batch Loss: 0.19328059256076813\n",
      "Epoch 2925, Loss: 0.9661493003368378, Final Batch Loss: 0.2929856777191162\n",
      "Epoch 2926, Loss: 0.9814110994338989, Final Batch Loss: 0.33341214060783386\n",
      "Epoch 2927, Loss: 1.1122950464487076, Final Batch Loss: 0.428251713514328\n",
      "Epoch 2928, Loss: 1.1305407136678696, Final Batch Loss: 0.3269330561161041\n",
      "Epoch 2929, Loss: 0.921797975897789, Final Batch Loss: 0.29795101284980774\n",
      "Epoch 2930, Loss: 0.9633121192455292, Final Batch Loss: 0.24164266884326935\n",
      "Epoch 2931, Loss: 0.9895265698432922, Final Batch Loss: 0.33163872361183167\n",
      "Epoch 2932, Loss: 0.976889356970787, Final Batch Loss: 0.15532656013965607\n",
      "Epoch 2933, Loss: 1.068961426615715, Final Batch Loss: 0.24060846865177155\n",
      "Epoch 2934, Loss: 1.0092672258615494, Final Batch Loss: 0.3436383903026581\n",
      "Epoch 2935, Loss: 0.8973904997110367, Final Batch Loss: 0.2590451240539551\n",
      "Epoch 2936, Loss: 1.024227187037468, Final Batch Loss: 0.2771114706993103\n",
      "Epoch 2937, Loss: 0.8697004020214081, Final Batch Loss: 0.16821503639221191\n",
      "Epoch 2938, Loss: 0.8106718584895134, Final Batch Loss: 0.07365905493497849\n",
      "Epoch 2939, Loss: 1.095284953713417, Final Batch Loss: 0.4198519289493561\n",
      "Epoch 2940, Loss: 0.8978220224380493, Final Batch Loss: 0.1810752898454666\n",
      "Epoch 2941, Loss: 0.9379767626523972, Final Batch Loss: 0.13262249529361725\n",
      "Epoch 2942, Loss: 0.8233138769865036, Final Batch Loss: 0.07086028158664703\n",
      "Epoch 2943, Loss: 0.8512558341026306, Final Batch Loss: 0.16097234189510345\n",
      "Epoch 2944, Loss: 1.0771310180425644, Final Batch Loss: 0.3624018728733063\n",
      "Epoch 2945, Loss: 1.0369787961244583, Final Batch Loss: 0.27883386611938477\n",
      "Epoch 2946, Loss: 0.8553991615772247, Final Batch Loss: 0.17402099072933197\n",
      "Epoch 2947, Loss: 0.8517780303955078, Final Batch Loss: 0.1700575351715088\n",
      "Epoch 2948, Loss: 1.0031867921352386, Final Batch Loss: 0.2917498052120209\n",
      "Epoch 2949, Loss: 1.1825996786355972, Final Batch Loss: 0.5188724398612976\n",
      "Epoch 2950, Loss: 0.9080618470907211, Final Batch Loss: 0.1672406941652298\n",
      "Epoch 2951, Loss: 0.9154129177331924, Final Batch Loss: 0.2301274538040161\n",
      "Epoch 2952, Loss: 0.9337871819734573, Final Batch Loss: 0.22298483550548553\n",
      "Epoch 2953, Loss: 1.0159445106983185, Final Batch Loss: 0.29002508521080017\n",
      "Epoch 2954, Loss: 0.9794795215129852, Final Batch Loss: 0.2018204778432846\n",
      "Epoch 2955, Loss: 0.9757283478975296, Final Batch Loss: 0.24297523498535156\n",
      "Epoch 2956, Loss: 0.8420321643352509, Final Batch Loss: 0.1551414579153061\n",
      "Epoch 2957, Loss: 0.9195082187652588, Final Batch Loss: 0.1684563159942627\n",
      "Epoch 2958, Loss: 0.8203233182430267, Final Batch Loss: 0.15872777998447418\n",
      "Epoch 2959, Loss: 0.9225281924009323, Final Batch Loss: 0.2471160739660263\n",
      "Epoch 2960, Loss: 0.9907289743423462, Final Batch Loss: 0.2621842324733734\n",
      "Epoch 2961, Loss: 1.0363345742225647, Final Batch Loss: 0.18184292316436768\n",
      "Epoch 2962, Loss: 0.9706484079360962, Final Batch Loss: 0.20668494701385498\n",
      "Epoch 2963, Loss: 1.197353571653366, Final Batch Loss: 0.3942519426345825\n",
      "Epoch 2964, Loss: 0.9370887279510498, Final Batch Loss: 0.2373083382844925\n",
      "Epoch 2965, Loss: 0.9098977446556091, Final Batch Loss: 0.22175247967243195\n",
      "Epoch 2966, Loss: 0.8059747070074081, Final Batch Loss: 0.14157655835151672\n",
      "Epoch 2967, Loss: 0.9000892341136932, Final Batch Loss: 0.16629861295223236\n",
      "Epoch 2968, Loss: 0.8075080215930939, Final Batch Loss: 0.14486654102802277\n",
      "Epoch 2969, Loss: 0.985689789056778, Final Batch Loss: 0.2700240910053253\n",
      "Epoch 2970, Loss: 0.8379575461149216, Final Batch Loss: 0.15951131284236908\n",
      "Epoch 2971, Loss: 0.9638290256261826, Final Batch Loss: 0.2819522023200989\n",
      "Epoch 2972, Loss: 0.8243254870176315, Final Batch Loss: 0.18729548156261444\n",
      "Epoch 2973, Loss: 1.0129211992025375, Final Batch Loss: 0.25082114338874817\n",
      "Epoch 2974, Loss: 1.0338550060987473, Final Batch Loss: 0.24228113889694214\n",
      "Epoch 2975, Loss: 0.8413650840520859, Final Batch Loss: 0.1395358443260193\n",
      "Epoch 2976, Loss: 0.9808757901191711, Final Batch Loss: 0.22949743270874023\n",
      "Epoch 2977, Loss: 0.77219258248806, Final Batch Loss: 0.1327495127916336\n",
      "Epoch 2978, Loss: 0.9261413067579269, Final Batch Loss: 0.1411682367324829\n",
      "Epoch 2979, Loss: 1.1146935522556305, Final Batch Loss: 0.4125336706638336\n",
      "Epoch 2980, Loss: 0.9417029172182083, Final Batch Loss: 0.17450207471847534\n",
      "Epoch 2981, Loss: 0.9269872158765793, Final Batch Loss: 0.22279702126979828\n",
      "Epoch 2982, Loss: 0.9063409715890884, Final Batch Loss: 0.16988758742809296\n",
      "Epoch 2983, Loss: 1.0670845955610275, Final Batch Loss: 0.26886311173439026\n",
      "Epoch 2984, Loss: 0.9087211787700653, Final Batch Loss: 0.2406519055366516\n",
      "Epoch 2985, Loss: 1.1180685609579086, Final Batch Loss: 0.3697551488876343\n",
      "Epoch 2986, Loss: 0.9862325191497803, Final Batch Loss: 0.31772905588150024\n",
      "Epoch 2987, Loss: 1.1298861652612686, Final Batch Loss: 0.4146937429904938\n",
      "Epoch 2988, Loss: 1.1261036098003387, Final Batch Loss: 0.3280962109565735\n",
      "Epoch 2989, Loss: 0.8681999146938324, Final Batch Loss: 0.15321801602840424\n",
      "Epoch 2990, Loss: 0.9756311029195786, Final Batch Loss: 0.2953032851219177\n",
      "Epoch 2991, Loss: 0.8520989045500755, Final Batch Loss: 0.11702627688646317\n",
      "Epoch 2992, Loss: 0.8301802277565002, Final Batch Loss: 0.16971848905086517\n",
      "Epoch 2993, Loss: 0.8573550879955292, Final Batch Loss: 0.1194067895412445\n",
      "Epoch 2994, Loss: 1.0591686964035034, Final Batch Loss: 0.3457256257534027\n",
      "Epoch 2995, Loss: 0.9995000958442688, Final Batch Loss: 0.22427906095981598\n",
      "Epoch 2996, Loss: 1.2490004003047943, Final Batch Loss: 0.41755035519599915\n",
      "Epoch 2997, Loss: 0.8712714910507202, Final Batch Loss: 0.1446669101715088\n",
      "Epoch 2998, Loss: 1.1430529057979584, Final Batch Loss: 0.2765117287635803\n",
      "Epoch 2999, Loss: 0.9592158198356628, Final Batch Loss: 0.23482297360897064\n",
      "Epoch 3000, Loss: 0.8663648068904877, Final Batch Loss: 0.1316840648651123\n",
      "Epoch 3001, Loss: 0.8901856541633606, Final Batch Loss: 0.21783286333084106\n",
      "Epoch 3002, Loss: 0.9097113758325577, Final Batch Loss: 0.21038968861103058\n",
      "Epoch 3003, Loss: 0.9259870201349258, Final Batch Loss: 0.2940204441547394\n",
      "Epoch 3004, Loss: 0.9732102900743484, Final Batch Loss: 0.27890077233314514\n",
      "Epoch 3005, Loss: 1.020234301686287, Final Batch Loss: 0.25900396704673767\n",
      "Epoch 3006, Loss: 0.8694640249013901, Final Batch Loss: 0.1371934860944748\n",
      "Epoch 3007, Loss: 0.9440706372261047, Final Batch Loss: 0.1925007700920105\n",
      "Epoch 3008, Loss: 0.816240057349205, Final Batch Loss: 0.1663055121898651\n",
      "Epoch 3009, Loss: 1.2102985084056854, Final Batch Loss: 0.47456875443458557\n",
      "Epoch 3010, Loss: 1.0486215204000473, Final Batch Loss: 0.2580547630786896\n",
      "Epoch 3011, Loss: 0.9711783677339554, Final Batch Loss: 0.29213419556617737\n",
      "Epoch 3012, Loss: 1.0136292725801468, Final Batch Loss: 0.33121320605278015\n",
      "Epoch 3013, Loss: 0.9762848764657974, Final Batch Loss: 0.32179418206214905\n",
      "Epoch 3014, Loss: 0.9117270112037659, Final Batch Loss: 0.2700258195400238\n",
      "Epoch 3015, Loss: 0.8177262544631958, Final Batch Loss: 0.08498015999794006\n",
      "Epoch 3016, Loss: 0.958827942609787, Final Batch Loss: 0.27530941367149353\n",
      "Epoch 3017, Loss: 0.9059986025094986, Final Batch Loss: 0.2131986767053604\n",
      "Epoch 3018, Loss: 0.9527743458747864, Final Batch Loss: 0.25375381112098694\n",
      "Epoch 3019, Loss: 0.901893138885498, Final Batch Loss: 0.24386417865753174\n",
      "Epoch 3020, Loss: 0.9758063554763794, Final Batch Loss: 0.18282021582126617\n",
      "Epoch 3021, Loss: 0.9455975592136383, Final Batch Loss: 0.1858568638563156\n",
      "Epoch 3022, Loss: 0.9781274646520615, Final Batch Loss: 0.25443708896636963\n",
      "Epoch 3023, Loss: 1.0262703150510788, Final Batch Loss: 0.29603442549705505\n",
      "Epoch 3024, Loss: 1.1367559432983398, Final Batch Loss: 0.31620675325393677\n",
      "Epoch 3025, Loss: 0.908267468214035, Final Batch Loss: 0.16703349351882935\n",
      "Epoch 3026, Loss: 0.8947401493787766, Final Batch Loss: 0.23234115540981293\n",
      "Epoch 3027, Loss: 0.9540284723043442, Final Batch Loss: 0.2746705710887909\n",
      "Epoch 3028, Loss: 0.9837460070848465, Final Batch Loss: 0.2607564628124237\n",
      "Epoch 3029, Loss: 1.2196938395500183, Final Batch Loss: 0.3453015089035034\n",
      "Epoch 3030, Loss: 1.1342247426509857, Final Batch Loss: 0.38324880599975586\n",
      "Epoch 3031, Loss: 1.0393505096435547, Final Batch Loss: 0.27439001202583313\n",
      "Epoch 3032, Loss: 0.9265929162502289, Final Batch Loss: 0.2641207277774811\n",
      "Epoch 3033, Loss: 0.9212555140256882, Final Batch Loss: 0.22299377620220184\n",
      "Epoch 3034, Loss: 0.8839237242937088, Final Batch Loss: 0.12774492800235748\n",
      "Epoch 3035, Loss: 0.9947288036346436, Final Batch Loss: 0.28239861130714417\n",
      "Epoch 3036, Loss: 0.8876057267189026, Final Batch Loss: 0.16880808770656586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3037, Loss: 0.8813710436224937, Final Batch Loss: 0.10160823911428452\n",
      "Epoch 3038, Loss: 1.000208631157875, Final Batch Loss: 0.28883492946624756\n",
      "Epoch 3039, Loss: 1.0212631225585938, Final Batch Loss: 0.2988396883010864\n",
      "Epoch 3040, Loss: 0.9989015609025955, Final Batch Loss: 0.23402392864227295\n",
      "Epoch 3041, Loss: 0.9955971837043762, Final Batch Loss: 0.2745606601238251\n",
      "Epoch 3042, Loss: 0.8922934830188751, Final Batch Loss: 0.23752331733703613\n",
      "Epoch 3043, Loss: 0.9535425454378128, Final Batch Loss: 0.22484111785888672\n",
      "Epoch 3044, Loss: 0.9447747468948364, Final Batch Loss: 0.1844150573015213\n",
      "Epoch 3045, Loss: 0.9796386510133743, Final Batch Loss: 0.3127278983592987\n",
      "Epoch 3046, Loss: 1.0746360570192337, Final Batch Loss: 0.26231005787849426\n",
      "Epoch 3047, Loss: 0.9140456914901733, Final Batch Loss: 0.11410623788833618\n",
      "Epoch 3048, Loss: 1.024990975856781, Final Batch Loss: 0.20145879685878754\n",
      "Epoch 3049, Loss: 0.949413850903511, Final Batch Loss: 0.24206535518169403\n",
      "Epoch 3050, Loss: 0.9171631336212158, Final Batch Loss: 0.19167955219745636\n",
      "Epoch 3051, Loss: 1.1068863421678543, Final Batch Loss: 0.37930095195770264\n",
      "Epoch 3052, Loss: 0.9361246228218079, Final Batch Loss: 0.24160589277744293\n",
      "Epoch 3053, Loss: 0.9884300827980042, Final Batch Loss: 0.22364778816699982\n",
      "Epoch 3054, Loss: 0.9220324009656906, Final Batch Loss: 0.1489170342683792\n",
      "Epoch 3055, Loss: 0.9424929767847061, Final Batch Loss: 0.18778085708618164\n",
      "Epoch 3056, Loss: 1.0506844073534012, Final Batch Loss: 0.19983036816120148\n",
      "Epoch 3057, Loss: 1.0471200793981552, Final Batch Loss: 0.2949713468551636\n",
      "Epoch 3058, Loss: 0.934142455458641, Final Batch Loss: 0.21049220860004425\n",
      "Epoch 3059, Loss: 0.9266302436590195, Final Batch Loss: 0.22253809869289398\n",
      "Epoch 3060, Loss: 1.022738054394722, Final Batch Loss: 0.2994917929172516\n",
      "Epoch 3061, Loss: 1.037440925836563, Final Batch Loss: 0.30685141682624817\n",
      "Epoch 3062, Loss: 1.0387099236249924, Final Batch Loss: 0.25952965021133423\n",
      "Epoch 3063, Loss: 0.9144250452518463, Final Batch Loss: 0.22595526278018951\n",
      "Epoch 3064, Loss: 1.0578991621732712, Final Batch Loss: 0.45121899247169495\n",
      "Epoch 3065, Loss: 0.8750534802675247, Final Batch Loss: 0.16366101801395416\n",
      "Epoch 3066, Loss: 1.005284696817398, Final Batch Loss: 0.20560747385025024\n",
      "Epoch 3067, Loss: 1.010392501950264, Final Batch Loss: 0.3269912600517273\n",
      "Epoch 3068, Loss: 1.0762707144021988, Final Batch Loss: 0.3625836670398712\n",
      "Epoch 3069, Loss: 0.9987397938966751, Final Batch Loss: 0.27737218141555786\n",
      "Epoch 3070, Loss: 0.8834326267242432, Final Batch Loss: 0.16020573675632477\n",
      "Epoch 3071, Loss: 1.0509259551763535, Final Batch Loss: 0.33325955271720886\n",
      "Epoch 3072, Loss: 0.9861052185297012, Final Batch Loss: 0.17436377704143524\n",
      "Epoch 3073, Loss: 1.1255462914705276, Final Batch Loss: 0.3333636522293091\n",
      "Epoch 3074, Loss: 1.1221320778131485, Final Batch Loss: 0.4051434099674225\n",
      "Epoch 3075, Loss: 1.1397210657596588, Final Batch Loss: 0.4618622958660126\n",
      "Epoch 3076, Loss: 1.1587098240852356, Final Batch Loss: 0.4138164222240448\n",
      "Epoch 3077, Loss: 1.0089998096227646, Final Batch Loss: 0.24191629886627197\n",
      "Epoch 3078, Loss: 1.0723916590213776, Final Batch Loss: 0.3585056960582733\n",
      "Epoch 3079, Loss: 0.9239617586135864, Final Batch Loss: 0.15632763504981995\n",
      "Epoch 3080, Loss: 0.9155917912721634, Final Batch Loss: 0.12957461178302765\n",
      "Epoch 3081, Loss: 0.8198869079351425, Final Batch Loss: 0.13778744637966156\n",
      "Epoch 3082, Loss: 0.926948681473732, Final Batch Loss: 0.24339155852794647\n",
      "Epoch 3083, Loss: 0.8554940670728683, Final Batch Loss: 0.18585622310638428\n",
      "Epoch 3084, Loss: 0.9914277642965317, Final Batch Loss: 0.2062336802482605\n",
      "Epoch 3085, Loss: 0.7740077748894691, Final Batch Loss: 0.07492024451494217\n",
      "Epoch 3086, Loss: 1.0357825458049774, Final Batch Loss: 0.36395323276519775\n",
      "Epoch 3087, Loss: 1.042660728096962, Final Batch Loss: 0.32180914282798767\n",
      "Epoch 3088, Loss: 0.9605504125356674, Final Batch Loss: 0.2124532014131546\n",
      "Epoch 3089, Loss: 1.107995644211769, Final Batch Loss: 0.2895844578742981\n",
      "Epoch 3090, Loss: 0.9378266334533691, Final Batch Loss: 0.24795104563236237\n",
      "Epoch 3091, Loss: 0.8376897275447845, Final Batch Loss: 0.1449996381998062\n",
      "Epoch 3092, Loss: 0.945944607257843, Final Batch Loss: 0.16903066635131836\n",
      "Epoch 3093, Loss: 0.8765849471092224, Final Batch Loss: 0.21236248314380646\n",
      "Epoch 3094, Loss: 0.7936246395111084, Final Batch Loss: 0.14085140824317932\n",
      "Epoch 3095, Loss: 0.924673318862915, Final Batch Loss: 0.18168587982654572\n",
      "Epoch 3096, Loss: 0.8966551721096039, Final Batch Loss: 0.15623657405376434\n",
      "Epoch 3097, Loss: 1.0170491933822632, Final Batch Loss: 0.3937326967716217\n",
      "Epoch 3098, Loss: 0.8296540230512619, Final Batch Loss: 0.1296953707933426\n",
      "Epoch 3099, Loss: 0.814178578555584, Final Batch Loss: 0.10820082575082779\n",
      "Epoch 3100, Loss: 1.058427631855011, Final Batch Loss: 0.43073928356170654\n",
      "Epoch 3101, Loss: 1.0165090262889862, Final Batch Loss: 0.24085043370723724\n",
      "Epoch 3102, Loss: 1.010453850030899, Final Batch Loss: 0.2598625123500824\n",
      "Epoch 3103, Loss: 1.1004715114831924, Final Batch Loss: 0.3806173503398895\n",
      "Epoch 3104, Loss: 0.8895784765481949, Final Batch Loss: 0.20858174562454224\n",
      "Epoch 3105, Loss: 1.004366397857666, Final Batch Loss: 0.3634888231754303\n",
      "Epoch 3106, Loss: 0.8742939084768295, Final Batch Loss: 0.1870683878660202\n",
      "Epoch 3107, Loss: 0.9445376098155975, Final Batch Loss: 0.14325790107250214\n",
      "Epoch 3108, Loss: 0.8872036337852478, Final Batch Loss: 0.1688409447669983\n",
      "Epoch 3109, Loss: 0.8633821755647659, Final Batch Loss: 0.14856776595115662\n",
      "Epoch 3110, Loss: 0.9070556461811066, Final Batch Loss: 0.1968194991350174\n",
      "Epoch 3111, Loss: 0.9444943219423294, Final Batch Loss: 0.1693231463432312\n",
      "Epoch 3112, Loss: 1.1606201380491257, Final Batch Loss: 0.511172354221344\n",
      "Epoch 3113, Loss: 0.880414605140686, Final Batch Loss: 0.2564496099948883\n",
      "Epoch 3114, Loss: 0.8461975008249283, Final Batch Loss: 0.17446714639663696\n",
      "Epoch 3115, Loss: 0.9969577342271805, Final Batch Loss: 0.2240932732820511\n",
      "Epoch 3116, Loss: 0.7983290553092957, Final Batch Loss: 0.19079051911830902\n",
      "Epoch 3117, Loss: 0.799914687871933, Final Batch Loss: 0.18840603530406952\n",
      "Epoch 3118, Loss: 0.7207403853535652, Final Batch Loss: 0.08878511935472488\n",
      "Epoch 3119, Loss: 0.9626116901636124, Final Batch Loss: 0.15792223811149597\n",
      "Epoch 3120, Loss: 0.9762884080410004, Final Batch Loss: 0.29743316769599915\n",
      "Epoch 3121, Loss: 0.8930811434984207, Final Batch Loss: 0.19913482666015625\n",
      "Epoch 3122, Loss: 0.9104327708482742, Final Batch Loss: 0.19379080832004547\n",
      "Epoch 3123, Loss: 0.9205543696880341, Final Batch Loss: 0.26285454630851746\n",
      "Epoch 3124, Loss: 1.0241192281246185, Final Batch Loss: 0.24621625244617462\n",
      "Epoch 3125, Loss: 1.1078636944293976, Final Batch Loss: 0.40060463547706604\n",
      "Epoch 3126, Loss: 1.0398307144641876, Final Batch Loss: 0.2710607051849365\n",
      "Epoch 3127, Loss: 0.9684125930070877, Final Batch Loss: 0.24075336754322052\n",
      "Epoch 3128, Loss: 0.985505610704422, Final Batch Loss: 0.27405253052711487\n",
      "Epoch 3129, Loss: 0.9059329479932785, Final Batch Loss: 0.13577325642108917\n",
      "Epoch 3130, Loss: 0.9503847658634186, Final Batch Loss: 0.22820734977722168\n",
      "Epoch 3131, Loss: 1.1388858705759048, Final Batch Loss: 0.4215419292449951\n",
      "Epoch 3132, Loss: 0.8531532436609268, Final Batch Loss: 0.18825341761112213\n",
      "Epoch 3133, Loss: 1.164163038134575, Final Batch Loss: 0.41405585408210754\n",
      "Epoch 3134, Loss: 1.074545294046402, Final Batch Loss: 0.41570448875427246\n",
      "Epoch 3135, Loss: 0.9634149968624115, Final Batch Loss: 0.1887519806623459\n",
      "Epoch 3136, Loss: 1.0701032876968384, Final Batch Loss: 0.1389836221933365\n",
      "Epoch 3137, Loss: 0.9132276475429535, Final Batch Loss: 0.20820991694927216\n",
      "Epoch 3138, Loss: 0.9379355758428574, Final Batch Loss: 0.22343184053897858\n",
      "Epoch 3139, Loss: 0.8594526648521423, Final Batch Loss: 0.21131540834903717\n",
      "Epoch 3140, Loss: 1.0168960690498352, Final Batch Loss: 0.341670960187912\n",
      "Epoch 3141, Loss: 0.8974907100200653, Final Batch Loss: 0.18397223949432373\n",
      "Epoch 3142, Loss: 1.0147831439971924, Final Batch Loss: 0.3540484607219696\n",
      "Epoch 3143, Loss: 0.9957500100135803, Final Batch Loss: 0.22321546077728271\n",
      "Epoch 3144, Loss: 0.9969105571508408, Final Batch Loss: 0.27931684255599976\n",
      "Epoch 3145, Loss: 0.8576161116361618, Final Batch Loss: 0.20004482567310333\n",
      "Epoch 3146, Loss: 0.9997411966323853, Final Batch Loss: 0.3346851170063019\n",
      "Epoch 3147, Loss: 1.014940619468689, Final Batch Loss: 0.32477864623069763\n",
      "Epoch 3148, Loss: 0.8767487555742264, Final Batch Loss: 0.15793190896511078\n",
      "Epoch 3149, Loss: 0.9237542301416397, Final Batch Loss: 0.2213657945394516\n",
      "Epoch 3150, Loss: 1.0090815424919128, Final Batch Loss: 0.26247477531433105\n",
      "Epoch 3151, Loss: 0.8528920561075211, Final Batch Loss: 0.28327858448028564\n",
      "Epoch 3152, Loss: 0.8457620143890381, Final Batch Loss: 0.14325924217700958\n",
      "Epoch 3153, Loss: 0.8576891273260117, Final Batch Loss: 0.1740817278623581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3154, Loss: 0.9857674092054367, Final Batch Loss: 0.2878223955631256\n",
      "Epoch 3155, Loss: 1.0865922272205353, Final Batch Loss: 0.2911672294139862\n",
      "Epoch 3156, Loss: 0.8658904731273651, Final Batch Loss: 0.14159034192562103\n",
      "Epoch 3157, Loss: 1.0212726444005966, Final Batch Loss: 0.20877192914485931\n",
      "Epoch 3158, Loss: 1.1844160705804825, Final Batch Loss: 0.41993388533592224\n",
      "Epoch 3159, Loss: 0.8748020529747009, Final Batch Loss: 0.19582635164260864\n",
      "Epoch 3160, Loss: 0.9575993418693542, Final Batch Loss: 0.34168514609336853\n",
      "Epoch 3161, Loss: 0.8188352286815643, Final Batch Loss: 0.13412214815616608\n",
      "Epoch 3162, Loss: 0.8749859482049942, Final Batch Loss: 0.19534708559513092\n",
      "Epoch 3163, Loss: 0.8540685474872589, Final Batch Loss: 0.18404239416122437\n",
      "Epoch 3164, Loss: 0.9360399693250656, Final Batch Loss: 0.22948873043060303\n",
      "Epoch 3165, Loss: 0.9413832873106003, Final Batch Loss: 0.3065691888332367\n",
      "Epoch 3166, Loss: 1.0338291227817535, Final Batch Loss: 0.24005310237407684\n",
      "Epoch 3167, Loss: 0.9104515612125397, Final Batch Loss: 0.26874199509620667\n",
      "Epoch 3168, Loss: 0.8090658187866211, Final Batch Loss: 0.05210809409618378\n",
      "Epoch 3169, Loss: 1.181070551276207, Final Batch Loss: 0.41219988465309143\n",
      "Epoch 3170, Loss: 1.0491708815097809, Final Batch Loss: 0.2324821501970291\n",
      "Epoch 3171, Loss: 0.8407836854457855, Final Batch Loss: 0.16772592067718506\n",
      "Epoch 3172, Loss: 1.072073221206665, Final Batch Loss: 0.4002006947994232\n",
      "Epoch 3173, Loss: 0.8871810436248779, Final Batch Loss: 0.18233288824558258\n",
      "Epoch 3174, Loss: 0.8760587722063065, Final Batch Loss: 0.1811366081237793\n",
      "Epoch 3175, Loss: 0.8442016839981079, Final Batch Loss: 0.16874869167804718\n",
      "Epoch 3176, Loss: 0.9583183079957962, Final Batch Loss: 0.19240622222423553\n",
      "Epoch 3177, Loss: 0.9574441760778427, Final Batch Loss: 0.10058203339576721\n",
      "Epoch 3178, Loss: 0.8857069462537766, Final Batch Loss: 0.17058853805065155\n",
      "Epoch 3179, Loss: 1.03435717523098, Final Batch Loss: 0.29244253039360046\n",
      "Epoch 3180, Loss: 0.8566638082265854, Final Batch Loss: 0.1592620462179184\n",
      "Epoch 3181, Loss: 0.9831797927618027, Final Batch Loss: 0.228857159614563\n",
      "Epoch 3182, Loss: 0.961144357919693, Final Batch Loss: 0.24058003723621368\n",
      "Epoch 3183, Loss: 0.8984609097242355, Final Batch Loss: 0.19862574338912964\n",
      "Epoch 3184, Loss: 0.8585345298051834, Final Batch Loss: 0.153356671333313\n",
      "Epoch 3185, Loss: 0.9461445212364197, Final Batch Loss: 0.1903027445077896\n",
      "Epoch 3186, Loss: 0.8330976068973541, Final Batch Loss: 0.167756125330925\n",
      "Epoch 3187, Loss: 0.8585994839668274, Final Batch Loss: 0.10015687346458435\n",
      "Epoch 3188, Loss: 0.938475027680397, Final Batch Loss: 0.22578786313533783\n",
      "Epoch 3189, Loss: 0.8107828497886658, Final Batch Loss: 0.10183951258659363\n",
      "Epoch 3190, Loss: 0.9383985549211502, Final Batch Loss: 0.3298061788082123\n",
      "Epoch 3191, Loss: 0.9047902077436447, Final Batch Loss: 0.24883872270584106\n",
      "Epoch 3192, Loss: 0.8916633725166321, Final Batch Loss: 0.19892065227031708\n",
      "Epoch 3193, Loss: 0.8942475467920303, Final Batch Loss: 0.16705042123794556\n",
      "Epoch 3194, Loss: 0.824841920286417, Final Batch Loss: 0.05860850587487221\n",
      "Epoch 3195, Loss: 0.995001882314682, Final Batch Loss: 0.22753740847110748\n",
      "Epoch 3196, Loss: 0.9562664330005646, Final Batch Loss: 0.25830304622650146\n",
      "Epoch 3197, Loss: 0.6838089413940907, Final Batch Loss: 0.05673620477318764\n",
      "Epoch 3198, Loss: 1.0114467293024063, Final Batch Loss: 0.24193327128887177\n",
      "Epoch 3199, Loss: 0.9557105451822281, Final Batch Loss: 0.2699623107910156\n",
      "Epoch 3200, Loss: 1.3356575965881348, Final Batch Loss: 0.6873912811279297\n",
      "Epoch 3201, Loss: 0.9139998406171799, Final Batch Loss: 0.20467717945575714\n",
      "Epoch 3202, Loss: 1.1350618749856949, Final Batch Loss: 0.4105494022369385\n",
      "Epoch 3203, Loss: 0.9867534935474396, Final Batch Loss: 0.2689898908138275\n",
      "Epoch 3204, Loss: 0.8127682656049728, Final Batch Loss: 0.17022724449634552\n",
      "Epoch 3205, Loss: 0.9421527981758118, Final Batch Loss: 0.1678512692451477\n",
      "Epoch 3206, Loss: 0.8612624406814575, Final Batch Loss: 0.15158189833164215\n",
      "Epoch 3207, Loss: 0.9528566300868988, Final Batch Loss: 0.30451759696006775\n",
      "Epoch 3208, Loss: 1.1004000306129456, Final Batch Loss: 0.4617650806903839\n",
      "Epoch 3209, Loss: 0.9027209430932999, Final Batch Loss: 0.22545139491558075\n",
      "Epoch 3210, Loss: 0.7142941430211067, Final Batch Loss: 0.11042586714029312\n",
      "Epoch 3211, Loss: 0.871161475777626, Final Batch Loss: 0.15331700444221497\n",
      "Epoch 3212, Loss: 0.8367753326892853, Final Batch Loss: 0.15115223824977875\n",
      "Epoch 3213, Loss: 0.8923119232058525, Final Batch Loss: 0.12099524587392807\n",
      "Epoch 3214, Loss: 0.8935850262641907, Final Batch Loss: 0.19691181182861328\n",
      "Epoch 3215, Loss: 1.1677203327417374, Final Batch Loss: 0.5537803769111633\n",
      "Epoch 3216, Loss: 1.086905300617218, Final Batch Loss: 0.3474729061126709\n",
      "Epoch 3217, Loss: 0.826359823346138, Final Batch Loss: 0.16999919712543488\n",
      "Epoch 3218, Loss: 0.943325087428093, Final Batch Loss: 0.17536942660808563\n",
      "Epoch 3219, Loss: 1.0011248886585236, Final Batch Loss: 0.30616530776023865\n",
      "Epoch 3220, Loss: 0.9493486732244492, Final Batch Loss: 0.2394670695066452\n",
      "Epoch 3221, Loss: 0.883485272526741, Final Batch Loss: 0.13819187879562378\n",
      "Epoch 3222, Loss: 0.7963743433356285, Final Batch Loss: 0.09869923442602158\n",
      "Epoch 3223, Loss: 0.987539991736412, Final Batch Loss: 0.13725785911083221\n",
      "Epoch 3224, Loss: 1.1265177577733994, Final Batch Loss: 0.441013902425766\n",
      "Epoch 3225, Loss: 1.072187602519989, Final Batch Loss: 0.261003702878952\n",
      "Epoch 3226, Loss: 0.9455368518829346, Final Batch Loss: 0.19278983771800995\n",
      "Epoch 3227, Loss: 0.821183055639267, Final Batch Loss: 0.13041387498378754\n",
      "Epoch 3228, Loss: 1.021731197834015, Final Batch Loss: 0.2794705927371979\n",
      "Epoch 3229, Loss: 0.9933415353298187, Final Batch Loss: 0.20141857862472534\n",
      "Epoch 3230, Loss: 0.9620525687932968, Final Batch Loss: 0.2711539566516876\n",
      "Epoch 3231, Loss: 0.9463712871074677, Final Batch Loss: 0.3103733956813812\n",
      "Epoch 3232, Loss: 0.9789708405733109, Final Batch Loss: 0.21421287953853607\n",
      "Epoch 3233, Loss: 1.044243946671486, Final Batch Loss: 0.24214082956314087\n",
      "Epoch 3234, Loss: 1.0274825692176819, Final Batch Loss: 0.2780754864215851\n",
      "Epoch 3235, Loss: 1.1409862637519836, Final Batch Loss: 0.22777360677719116\n",
      "Epoch 3236, Loss: 1.0345500260591507, Final Batch Loss: 0.28805282711982727\n",
      "Epoch 3237, Loss: 1.052703619003296, Final Batch Loss: 0.2926970422267914\n",
      "Epoch 3238, Loss: 0.8678808063268661, Final Batch Loss: 0.16291742026805878\n",
      "Epoch 3239, Loss: 0.9308576285839081, Final Batch Loss: 0.24389247596263885\n",
      "Epoch 3240, Loss: 0.9261519908905029, Final Batch Loss: 0.2200831025838852\n",
      "Epoch 3241, Loss: 0.815222755074501, Final Batch Loss: 0.17187082767486572\n",
      "Epoch 3242, Loss: 0.943852111697197, Final Batch Loss: 0.24322743713855743\n",
      "Epoch 3243, Loss: 0.9175045788288116, Final Batch Loss: 0.23580104112625122\n",
      "Epoch 3244, Loss: 0.9219617396593094, Final Batch Loss: 0.24119983613491058\n",
      "Epoch 3245, Loss: 1.0376048684120178, Final Batch Loss: 0.28692466020584106\n",
      "Epoch 3246, Loss: 1.1271005123853683, Final Batch Loss: 0.42701682448387146\n",
      "Epoch 3247, Loss: 1.0093853771686554, Final Batch Loss: 0.22764520347118378\n",
      "Epoch 3248, Loss: 0.9342331439256668, Final Batch Loss: 0.24703024327754974\n",
      "Epoch 3249, Loss: 0.7432678639888763, Final Batch Loss: 0.1175190806388855\n",
      "Epoch 3250, Loss: 0.947482168674469, Final Batch Loss: 0.174013152718544\n",
      "Epoch 3251, Loss: 0.7953045517206192, Final Batch Loss: 0.15728120505809784\n",
      "Epoch 3252, Loss: 0.8723612651228905, Final Batch Loss: 0.06521759182214737\n",
      "Epoch 3253, Loss: 0.8747321963310242, Final Batch Loss: 0.2307216078042984\n",
      "Epoch 3254, Loss: 0.8298514112830162, Final Batch Loss: 0.10617468506097794\n",
      "Epoch 3255, Loss: 0.963723212480545, Final Batch Loss: 0.29038622975349426\n",
      "Epoch 3256, Loss: 0.8120611160993576, Final Batch Loss: 0.1801711767911911\n",
      "Epoch 3257, Loss: 0.9160540103912354, Final Batch Loss: 0.14982345700263977\n",
      "Epoch 3258, Loss: 1.0954225063323975, Final Batch Loss: 0.37094709277153015\n",
      "Epoch 3259, Loss: 0.9682515263557434, Final Batch Loss: 0.288483589887619\n",
      "Epoch 3260, Loss: 0.9568530172109604, Final Batch Loss: 0.1638595461845398\n",
      "Epoch 3261, Loss: 1.0765002369880676, Final Batch Loss: 0.3631019592285156\n",
      "Epoch 3262, Loss: 0.805085264146328, Final Batch Loss: 0.10314659029245377\n",
      "Epoch 3263, Loss: 0.9302282631397247, Final Batch Loss: 0.28579798340797424\n",
      "Epoch 3264, Loss: 0.825318843126297, Final Batch Loss: 0.1282178908586502\n",
      "Epoch 3265, Loss: 0.9532546401023865, Final Batch Loss: 0.36701083183288574\n",
      "Epoch 3266, Loss: 0.9535660445690155, Final Batch Loss: 0.23281127214431763\n",
      "Epoch 3267, Loss: 0.9655764698982239, Final Batch Loss: 0.23397177457809448\n",
      "Epoch 3268, Loss: 0.9947609752416611, Final Batch Loss: 0.28193560242652893\n",
      "Epoch 3269, Loss: 0.8625718504190445, Final Batch Loss: 0.2015833854675293\n",
      "Epoch 3270, Loss: 0.722197949886322, Final Batch Loss: 0.11020395159721375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3271, Loss: 0.9584718942642212, Final Batch Loss: 0.14040054380893707\n",
      "Epoch 3272, Loss: 0.8396414890885353, Final Batch Loss: 0.07134706526994705\n",
      "Epoch 3273, Loss: 0.8372255861759186, Final Batch Loss: 0.11032173037528992\n",
      "Epoch 3274, Loss: 0.8762831091880798, Final Batch Loss: 0.23232094943523407\n",
      "Epoch 3275, Loss: 0.7865899205207825, Final Batch Loss: 0.1304580569267273\n",
      "Epoch 3276, Loss: 0.9558036625385284, Final Batch Loss: 0.23852656781673431\n",
      "Epoch 3277, Loss: 0.9855790436267853, Final Batch Loss: 0.31189921498298645\n",
      "Epoch 3278, Loss: 0.8647233098745346, Final Batch Loss: 0.17164616286754608\n",
      "Epoch 3279, Loss: 0.8357497379183769, Final Batch Loss: 0.10972493141889572\n",
      "Epoch 3280, Loss: 0.9510458111763, Final Batch Loss: 0.20402424037456512\n",
      "Epoch 3281, Loss: 0.8889158070087433, Final Batch Loss: 0.24631179869174957\n",
      "Epoch 3282, Loss: 0.9228096306324005, Final Batch Loss: 0.26898282766342163\n",
      "Epoch 3283, Loss: 0.9127170592546463, Final Batch Loss: 0.204998180270195\n",
      "Epoch 3284, Loss: 0.9459433108568192, Final Batch Loss: 0.2499663233757019\n",
      "Epoch 3285, Loss: 0.8502185642719269, Final Batch Loss: 0.1848166584968567\n",
      "Epoch 3286, Loss: 0.8142828941345215, Final Batch Loss: 0.16156785190105438\n",
      "Epoch 3287, Loss: 1.0313850045204163, Final Batch Loss: 0.28822559118270874\n",
      "Epoch 3288, Loss: 1.0614213794469833, Final Batch Loss: 0.4117497503757477\n",
      "Epoch 3289, Loss: 1.0375139713287354, Final Batch Loss: 0.26686012744903564\n",
      "Epoch 3290, Loss: 1.0711062103509903, Final Batch Loss: 0.38933005928993225\n",
      "Epoch 3291, Loss: 0.9043335616588593, Final Batch Loss: 0.2767083942890167\n",
      "Epoch 3292, Loss: 0.940333291888237, Final Batch Loss: 0.23270291090011597\n",
      "Epoch 3293, Loss: 1.128549873828888, Final Batch Loss: 0.45205938816070557\n",
      "Epoch 3294, Loss: 1.1149765104055405, Final Batch Loss: 0.3777472972869873\n",
      "Epoch 3295, Loss: 1.051459938287735, Final Batch Loss: 0.3110671043395996\n",
      "Epoch 3296, Loss: 0.7030288204550743, Final Batch Loss: 0.09988120943307877\n",
      "Epoch 3297, Loss: 0.7804261073470116, Final Batch Loss: 0.09787435084581375\n",
      "Epoch 3298, Loss: 0.9592938125133514, Final Batch Loss: 0.23900264501571655\n",
      "Epoch 3299, Loss: 0.9455273896455765, Final Batch Loss: 0.21881185472011566\n",
      "Epoch 3300, Loss: 0.9358531385660172, Final Batch Loss: 0.28631001710891724\n",
      "Epoch 3301, Loss: 0.9873690903186798, Final Batch Loss: 0.2190612107515335\n",
      "Epoch 3302, Loss: 1.0484265387058258, Final Batch Loss: 0.35591578483581543\n",
      "Epoch 3303, Loss: 0.9791121035814285, Final Batch Loss: 0.3313691020011902\n",
      "Epoch 3304, Loss: 1.022647649049759, Final Batch Loss: 0.23215055465698242\n",
      "Epoch 3305, Loss: 0.9468562006950378, Final Batch Loss: 0.23286177217960358\n",
      "Epoch 3306, Loss: 1.1201753616333008, Final Batch Loss: 0.42346644401550293\n",
      "Epoch 3307, Loss: 0.7720346078276634, Final Batch Loss: 0.07682115584611893\n",
      "Epoch 3308, Loss: 0.8520647287368774, Final Batch Loss: 0.17365407943725586\n",
      "Epoch 3309, Loss: 0.8961769938468933, Final Batch Loss: 0.18947191536426544\n",
      "Epoch 3310, Loss: 0.9365776479244232, Final Batch Loss: 0.2620539367198944\n",
      "Epoch 3311, Loss: 0.8805247992277145, Final Batch Loss: 0.22249539196491241\n",
      "Epoch 3312, Loss: 0.8095193654298782, Final Batch Loss: 0.1758444756269455\n",
      "Epoch 3313, Loss: 0.9564169943332672, Final Batch Loss: 0.25817403197288513\n",
      "Epoch 3314, Loss: 0.8184307813644409, Final Batch Loss: 0.24066989123821259\n",
      "Epoch 3315, Loss: 0.798276349902153, Final Batch Loss: 0.19753699004650116\n",
      "Epoch 3316, Loss: 0.8655834197998047, Final Batch Loss: 0.23342956602573395\n",
      "Epoch 3317, Loss: 0.8131229504942894, Final Batch Loss: 0.10470027476549149\n",
      "Epoch 3318, Loss: 0.8650034815073013, Final Batch Loss: 0.1377989649772644\n",
      "Epoch 3319, Loss: 0.9439988285303116, Final Batch Loss: 0.2208481878042221\n",
      "Epoch 3320, Loss: 0.8618400245904922, Final Batch Loss: 0.2999964952468872\n",
      "Epoch 3321, Loss: 0.8795027285814285, Final Batch Loss: 0.19065576791763306\n",
      "Epoch 3322, Loss: 0.8495485782623291, Final Batch Loss: 0.23344290256500244\n",
      "Epoch 3323, Loss: 0.9612667113542557, Final Batch Loss: 0.1724082976579666\n",
      "Epoch 3324, Loss: 0.9338813573122025, Final Batch Loss: 0.21613067388534546\n",
      "Epoch 3325, Loss: 0.9215220361948013, Final Batch Loss: 0.1798582524061203\n",
      "Epoch 3326, Loss: 0.9131272584199905, Final Batch Loss: 0.28357771039009094\n",
      "Epoch 3327, Loss: 0.8564396724104881, Final Batch Loss: 0.09688545018434525\n",
      "Epoch 3328, Loss: 0.8018254190683365, Final Batch Loss: 0.13577502965927124\n",
      "Epoch 3329, Loss: 0.9777472764253616, Final Batch Loss: 0.3022690713405609\n",
      "Epoch 3330, Loss: 1.0023871660232544, Final Batch Loss: 0.3348792791366577\n",
      "Epoch 3331, Loss: 0.9201314449310303, Final Batch Loss: 0.1824934333562851\n",
      "Epoch 3332, Loss: 0.977349653840065, Final Batch Loss: 0.19752927124500275\n",
      "Epoch 3333, Loss: 0.9201982170343399, Final Batch Loss: 0.20348183810710907\n",
      "Epoch 3334, Loss: 1.0189480930566788, Final Batch Loss: 0.3521481454372406\n",
      "Epoch 3335, Loss: 0.9366697818040848, Final Batch Loss: 0.2451433539390564\n",
      "Epoch 3336, Loss: 0.9824001044034958, Final Batch Loss: 0.2808257043361664\n",
      "Epoch 3337, Loss: 0.9080088883638382, Final Batch Loss: 0.12999968230724335\n",
      "Epoch 3338, Loss: 1.0215900242328644, Final Batch Loss: 0.33299747109413147\n",
      "Epoch 3339, Loss: 1.0293909013271332, Final Batch Loss: 0.29198169708251953\n",
      "Epoch 3340, Loss: 0.9786902815103531, Final Batch Loss: 0.2504828870296478\n",
      "Epoch 3341, Loss: 1.1824322491884232, Final Batch Loss: 0.35841789841651917\n",
      "Epoch 3342, Loss: 1.067230686545372, Final Batch Loss: 0.2923920154571533\n",
      "Epoch 3343, Loss: 0.9599383026361465, Final Batch Loss: 0.21404600143432617\n",
      "Epoch 3344, Loss: 1.0233270674943924, Final Batch Loss: 0.3819805681705475\n",
      "Epoch 3345, Loss: 0.9158137738704681, Final Batch Loss: 0.10421639680862427\n",
      "Epoch 3346, Loss: 1.1680756360292435, Final Batch Loss: 0.44646838307380676\n",
      "Epoch 3347, Loss: 0.9059956967830658, Final Batch Loss: 0.18239706754684448\n",
      "Epoch 3348, Loss: 0.8227223455905914, Final Batch Loss: 0.135719895362854\n",
      "Epoch 3349, Loss: 0.8973984718322754, Final Batch Loss: 0.22562678158283234\n",
      "Epoch 3350, Loss: 0.8730157166719437, Final Batch Loss: 0.15348437428474426\n",
      "Epoch 3351, Loss: 0.9168945699930191, Final Batch Loss: 0.17001795768737793\n",
      "Epoch 3352, Loss: 0.9777111709117889, Final Batch Loss: 0.26863521337509155\n",
      "Epoch 3353, Loss: 1.0022740066051483, Final Batch Loss: 0.28825587034225464\n",
      "Epoch 3354, Loss: 0.868512287735939, Final Batch Loss: 0.1528080254793167\n",
      "Epoch 3355, Loss: 0.8498741984367371, Final Batch Loss: 0.1402633637189865\n",
      "Epoch 3356, Loss: 1.1205096244812012, Final Batch Loss: 0.2533222734928131\n",
      "Epoch 3357, Loss: 0.9569259583950043, Final Batch Loss: 0.2586286962032318\n",
      "Epoch 3358, Loss: 0.8383577466011047, Final Batch Loss: 0.26044198870658875\n",
      "Epoch 3359, Loss: 0.8304235637187958, Final Batch Loss: 0.15985797345638275\n",
      "Epoch 3360, Loss: 0.8005033284425735, Final Batch Loss: 0.15571223199367523\n",
      "Epoch 3361, Loss: 0.9411445111036301, Final Batch Loss: 0.30481216311454773\n",
      "Epoch 3362, Loss: 1.0260618031024933, Final Batch Loss: 0.3372528851032257\n",
      "Epoch 3363, Loss: 0.8978472501039505, Final Batch Loss: 0.2695198953151703\n",
      "Epoch 3364, Loss: 0.8996223881840706, Final Batch Loss: 0.12385601550340652\n",
      "Epoch 3365, Loss: 0.8354155421257019, Final Batch Loss: 0.15597029030323029\n",
      "Epoch 3366, Loss: 0.8130549043416977, Final Batch Loss: 0.13319581747055054\n",
      "Epoch 3367, Loss: 0.9107052832841873, Final Batch Loss: 0.2516252398490906\n",
      "Epoch 3368, Loss: 0.9920449703931808, Final Batch Loss: 0.29601970314979553\n",
      "Epoch 3369, Loss: 1.1675946563482285, Final Batch Loss: 0.4101541042327881\n",
      "Epoch 3370, Loss: 0.9798587560653687, Final Batch Loss: 0.19219328463077545\n",
      "Epoch 3371, Loss: 0.8452153429389, Final Batch Loss: 0.10778839141130447\n",
      "Epoch 3372, Loss: 0.892697274684906, Final Batch Loss: 0.22193074226379395\n",
      "Epoch 3373, Loss: 0.9803365021944046, Final Batch Loss: 0.25464344024658203\n",
      "Epoch 3374, Loss: 0.9626555144786835, Final Batch Loss: 0.3108823001384735\n",
      "Epoch 3375, Loss: 0.9285930395126343, Final Batch Loss: 0.27761584520339966\n",
      "Epoch 3376, Loss: 0.8013037070631981, Final Batch Loss: 0.07642204314470291\n",
      "Epoch 3377, Loss: 0.844025120139122, Final Batch Loss: 0.1716100424528122\n",
      "Epoch 3378, Loss: 0.9668091535568237, Final Batch Loss: 0.25079140067100525\n",
      "Epoch 3379, Loss: 0.8340920880436897, Final Batch Loss: 0.08043178170919418\n",
      "Epoch 3380, Loss: 0.9220709651708603, Final Batch Loss: 0.2491937279701233\n",
      "Epoch 3381, Loss: 0.8455165922641754, Final Batch Loss: 0.2133665829896927\n",
      "Epoch 3382, Loss: 0.9625503122806549, Final Batch Loss: 0.2823282480239868\n",
      "Epoch 3383, Loss: 0.9273708313703537, Final Batch Loss: 0.24368451535701752\n",
      "Epoch 3384, Loss: 0.9171777367591858, Final Batch Loss: 0.239331915974617\n",
      "Epoch 3385, Loss: 0.9283882677555084, Final Batch Loss: 0.18886065483093262\n",
      "Epoch 3386, Loss: 0.858702078461647, Final Batch Loss: 0.13679909706115723\n",
      "Epoch 3387, Loss: 0.8577626794576645, Final Batch Loss: 0.14587678015232086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3388, Loss: 1.0256701111793518, Final Batch Loss: 0.40893682837486267\n",
      "Epoch 3389, Loss: 0.7808854728937149, Final Batch Loss: 0.14952965080738068\n",
      "Epoch 3390, Loss: 1.1107519268989563, Final Batch Loss: 0.3329910933971405\n",
      "Epoch 3391, Loss: 0.8570856302976608, Final Batch Loss: 0.18269024789333344\n",
      "Epoch 3392, Loss: 0.7647838518023491, Final Batch Loss: 0.08270279318094254\n",
      "Epoch 3393, Loss: 1.0713688880205154, Final Batch Loss: 0.33126142621040344\n",
      "Epoch 3394, Loss: 0.9173769801855087, Final Batch Loss: 0.20680218935012817\n",
      "Epoch 3395, Loss: 0.9191174656152725, Final Batch Loss: 0.21959109604358673\n",
      "Epoch 3396, Loss: 0.818356066942215, Final Batch Loss: 0.15944238007068634\n",
      "Epoch 3397, Loss: 1.071772187948227, Final Batch Loss: 0.3627684414386749\n",
      "Epoch 3398, Loss: 0.7864332646131516, Final Batch Loss: 0.14945746958255768\n",
      "Epoch 3399, Loss: 0.9120223224163055, Final Batch Loss: 0.21406997740268707\n",
      "Epoch 3400, Loss: 1.0191601514816284, Final Batch Loss: 0.31851187348365784\n",
      "Epoch 3401, Loss: 0.9406487196683884, Final Batch Loss: 0.18015818297863007\n",
      "Epoch 3402, Loss: 0.8883668258786201, Final Batch Loss: 0.0971420630812645\n",
      "Epoch 3403, Loss: 0.9441030472517014, Final Batch Loss: 0.26157042384147644\n",
      "Epoch 3404, Loss: 0.8771159648895264, Final Batch Loss: 0.25605252385139465\n",
      "Epoch 3405, Loss: 0.9570639282464981, Final Batch Loss: 0.23835420608520508\n",
      "Epoch 3406, Loss: 0.7236123308539391, Final Batch Loss: 0.08642607182264328\n",
      "Epoch 3407, Loss: 0.9074520021677017, Final Batch Loss: 0.20793741941452026\n",
      "Epoch 3408, Loss: 0.9041025638580322, Final Batch Loss: 0.29123374819755554\n",
      "Epoch 3409, Loss: 0.8031032830476761, Final Batch Loss: 0.08698031306266785\n",
      "Epoch 3410, Loss: 0.8679975867271423, Final Batch Loss: 0.21464043855667114\n",
      "Epoch 3411, Loss: 1.0032597780227661, Final Batch Loss: 0.30169886350631714\n",
      "Epoch 3412, Loss: 0.8287223055958748, Final Batch Loss: 0.08378037065267563\n",
      "Epoch 3413, Loss: 0.9361640512943268, Final Batch Loss: 0.16262219846248627\n",
      "Epoch 3414, Loss: 0.8879696428775787, Final Batch Loss: 0.145301952958107\n",
      "Epoch 3415, Loss: 0.8187261819839478, Final Batch Loss: 0.12776677310466766\n",
      "Epoch 3416, Loss: 0.9650821387767792, Final Batch Loss: 0.28761571645736694\n",
      "Epoch 3417, Loss: 1.2089267075061798, Final Batch Loss: 0.49105584621429443\n",
      "Epoch 3418, Loss: 1.163471132516861, Final Batch Loss: 0.33094337582588196\n",
      "Epoch 3419, Loss: 1.1383771449327469, Final Batch Loss: 0.4664647877216339\n",
      "Epoch 3420, Loss: 0.9857717230916023, Final Batch Loss: 0.09806302934885025\n",
      "Epoch 3421, Loss: 1.1168984919786453, Final Batch Loss: 0.26044294238090515\n",
      "Epoch 3422, Loss: 0.9642478823661804, Final Batch Loss: 0.2175561785697937\n",
      "Epoch 3423, Loss: 0.9389539361000061, Final Batch Loss: 0.3005143105983734\n",
      "Epoch 3424, Loss: 0.9659287482500076, Final Batch Loss: 0.20309056341648102\n",
      "Epoch 3425, Loss: 1.1056123971939087, Final Batch Loss: 0.30132564902305603\n",
      "Epoch 3426, Loss: 1.054723098874092, Final Batch Loss: 0.24071012437343597\n",
      "Epoch 3427, Loss: 1.0461219400167465, Final Batch Loss: 0.2135864943265915\n",
      "Epoch 3428, Loss: 0.909638062119484, Final Batch Loss: 0.20079214870929718\n",
      "Epoch 3429, Loss: 1.008234828710556, Final Batch Loss: 0.2649202346801758\n",
      "Epoch 3430, Loss: 0.9251494854688644, Final Batch Loss: 0.12456533312797546\n",
      "Epoch 3431, Loss: 0.8942754417657852, Final Batch Loss: 0.1959371119737625\n",
      "Epoch 3432, Loss: 1.0685783326625824, Final Batch Loss: 0.2981160283088684\n",
      "Epoch 3433, Loss: 0.9662925153970718, Final Batch Loss: 0.2625223696231842\n",
      "Epoch 3434, Loss: 0.9482972919940948, Final Batch Loss: 0.2249365895986557\n",
      "Epoch 3435, Loss: 0.8701409697532654, Final Batch Loss: 0.19048811495304108\n",
      "Epoch 3436, Loss: 1.0115035474300385, Final Batch Loss: 0.3079359829425812\n",
      "Epoch 3437, Loss: 1.0988159626722336, Final Batch Loss: 0.3277563750743866\n",
      "Epoch 3438, Loss: 0.9726905226707458, Final Batch Loss: 0.22903932631015778\n",
      "Epoch 3439, Loss: 0.8927343785762787, Final Batch Loss: 0.07973377406597137\n",
      "Epoch 3440, Loss: 0.9584342390298843, Final Batch Loss: 0.23029936850070953\n",
      "Epoch 3441, Loss: 1.0105320811271667, Final Batch Loss: 0.15593720972537994\n",
      "Epoch 3442, Loss: 0.8233407959342003, Final Batch Loss: 0.11773360520601273\n",
      "Epoch 3443, Loss: 1.0860655456781387, Final Batch Loss: 0.3580910265445709\n",
      "Epoch 3444, Loss: 0.9396136999130249, Final Batch Loss: 0.20158474147319794\n",
      "Epoch 3445, Loss: 0.879974976181984, Final Batch Loss: 0.18857310712337494\n",
      "Epoch 3446, Loss: 0.9487656950950623, Final Batch Loss: 0.2590264081954956\n",
      "Epoch 3447, Loss: 0.8516972661018372, Final Batch Loss: 0.16217632591724396\n",
      "Epoch 3448, Loss: 0.9194899201393127, Final Batch Loss: 0.27689793705940247\n",
      "Epoch 3449, Loss: 0.8562547340989113, Final Batch Loss: 0.07499166578054428\n",
      "Epoch 3450, Loss: 0.9265718311071396, Final Batch Loss: 0.17630302906036377\n",
      "Epoch 3451, Loss: 0.8729483634233475, Final Batch Loss: 0.2013675421476364\n",
      "Epoch 3452, Loss: 0.923638105392456, Final Batch Loss: 0.26146093010902405\n",
      "Epoch 3453, Loss: 0.9828896969556808, Final Batch Loss: 0.2131998986005783\n",
      "Epoch 3454, Loss: 1.0713416785001755, Final Batch Loss: 0.25459831953048706\n",
      "Epoch 3455, Loss: 0.8328351229429245, Final Batch Loss: 0.18812547624111176\n",
      "Epoch 3456, Loss: 1.1792152971029282, Final Batch Loss: 0.3531440496444702\n",
      "Epoch 3457, Loss: 0.8553920686244965, Final Batch Loss: 0.15726973116397858\n",
      "Epoch 3458, Loss: 0.822703406214714, Final Batch Loss: 0.1535741090774536\n",
      "Epoch 3459, Loss: 0.9465129226446152, Final Batch Loss: 0.23468388617038727\n",
      "Epoch 3460, Loss: 0.8501105904579163, Final Batch Loss: 0.2630569636821747\n",
      "Epoch 3461, Loss: 0.9948003888130188, Final Batch Loss: 0.3017040491104126\n",
      "Epoch 3462, Loss: 0.7863803803920746, Final Batch Loss: 0.09634599089622498\n",
      "Epoch 3463, Loss: 0.8198642432689667, Final Batch Loss: 0.1572365015745163\n",
      "Epoch 3464, Loss: 0.9870123416185379, Final Batch Loss: 0.3134104609489441\n",
      "Epoch 3465, Loss: 0.7950742095708847, Final Batch Loss: 0.12502287328243256\n",
      "Epoch 3466, Loss: 1.0374582707881927, Final Batch Loss: 0.3666355311870575\n",
      "Epoch 3467, Loss: 0.9046304076910019, Final Batch Loss: 0.22083784639835358\n",
      "Epoch 3468, Loss: 0.8833590000867844, Final Batch Loss: 0.19241541624069214\n",
      "Epoch 3469, Loss: 0.8955486118793488, Final Batch Loss: 0.1892494410276413\n",
      "Epoch 3470, Loss: 0.8343761265277863, Final Batch Loss: 0.17757801711559296\n",
      "Epoch 3471, Loss: 1.2255004048347473, Final Batch Loss: 0.518876850605011\n",
      "Epoch 3472, Loss: 0.9206315875053406, Final Batch Loss: 0.13479749858379364\n",
      "Epoch 3473, Loss: 0.8179972171783447, Final Batch Loss: 0.12264731526374817\n",
      "Epoch 3474, Loss: 0.8525056838989258, Final Batch Loss: 0.14129076898097992\n",
      "Epoch 3475, Loss: 0.8682322204113007, Final Batch Loss: 0.19847087562084198\n",
      "Epoch 3476, Loss: 0.7926558554172516, Final Batch Loss: 0.12917445600032806\n",
      "Epoch 3477, Loss: 0.9681663811206818, Final Batch Loss: 0.32683244347572327\n",
      "Epoch 3478, Loss: 0.965569794178009, Final Batch Loss: 0.33586886525154114\n",
      "Epoch 3479, Loss: 1.025394156575203, Final Batch Loss: 0.2950995862483978\n",
      "Epoch 3480, Loss: 0.9109308123588562, Final Batch Loss: 0.2393857091665268\n",
      "Epoch 3481, Loss: 0.8832941949367523, Final Batch Loss: 0.20783072710037231\n",
      "Epoch 3482, Loss: 0.8971386551856995, Final Batch Loss: 0.2516586184501648\n",
      "Epoch 3483, Loss: 1.0094996392726898, Final Batch Loss: 0.3548455536365509\n",
      "Epoch 3484, Loss: 0.9094330668449402, Final Batch Loss: 0.25602084398269653\n",
      "Epoch 3485, Loss: 1.0266191959381104, Final Batch Loss: 0.222273588180542\n",
      "Epoch 3486, Loss: 0.955343633890152, Final Batch Loss: 0.22922147810459137\n",
      "Epoch 3487, Loss: 0.9781985133886337, Final Batch Loss: 0.2726894021034241\n",
      "Epoch 3488, Loss: 0.9467914998531342, Final Batch Loss: 0.2844504415988922\n",
      "Epoch 3489, Loss: 0.8571559190750122, Final Batch Loss: 0.2046133428812027\n",
      "Epoch 3490, Loss: 0.897421732544899, Final Batch Loss: 0.2485005259513855\n",
      "Epoch 3491, Loss: 0.8572015315294266, Final Batch Loss: 0.1363394558429718\n",
      "Epoch 3492, Loss: 0.8673836886882782, Final Batch Loss: 0.1264447420835495\n",
      "Epoch 3493, Loss: 0.8975209593772888, Final Batch Loss: 0.17524902522563934\n",
      "Epoch 3494, Loss: 1.074570819735527, Final Batch Loss: 0.3518640100955963\n",
      "Epoch 3495, Loss: 0.9189033433794975, Final Batch Loss: 0.10551851242780685\n",
      "Epoch 3496, Loss: 0.8778466284275055, Final Batch Loss: 0.18579071760177612\n",
      "Epoch 3497, Loss: 0.8938841670751572, Final Batch Loss: 0.26895618438720703\n",
      "Epoch 3498, Loss: 0.9446161687374115, Final Batch Loss: 0.30666452646255493\n",
      "Epoch 3499, Loss: 0.8266414180397987, Final Batch Loss: 0.10187283903360367\n",
      "Epoch 3500, Loss: 0.8497691601514816, Final Batch Loss: 0.18642038106918335\n",
      "Epoch 3501, Loss: 0.9731718301773071, Final Batch Loss: 0.19978289306163788\n",
      "Epoch 3502, Loss: 0.7975993305444717, Final Batch Loss: 0.15804459154605865\n",
      "Epoch 3503, Loss: 0.8623057007789612, Final Batch Loss: 0.24625177681446075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3504, Loss: 0.7850212156772614, Final Batch Loss: 0.15835343301296234\n",
      "Epoch 3505, Loss: 1.0666055381298065, Final Batch Loss: 0.41012248396873474\n",
      "Epoch 3506, Loss: 1.152727410197258, Final Batch Loss: 0.3506499230861664\n",
      "Epoch 3507, Loss: 1.035419538617134, Final Batch Loss: 0.29240962862968445\n",
      "Epoch 3508, Loss: 0.9813246130943298, Final Batch Loss: 0.2825852334499359\n",
      "Epoch 3509, Loss: 0.8859931081533432, Final Batch Loss: 0.2618076503276825\n",
      "Epoch 3510, Loss: 1.246238499879837, Final Batch Loss: 0.4784766733646393\n",
      "Epoch 3511, Loss: 1.0180899649858475, Final Batch Loss: 0.2526070177555084\n",
      "Epoch 3512, Loss: 0.9354869723320007, Final Batch Loss: 0.282488614320755\n",
      "Epoch 3513, Loss: 0.9919945895671844, Final Batch Loss: 0.27947768568992615\n",
      "Epoch 3514, Loss: 1.0377446860074997, Final Batch Loss: 0.3379284143447876\n",
      "Epoch 3515, Loss: 0.9640365391969681, Final Batch Loss: 0.25995033979415894\n",
      "Epoch 3516, Loss: 0.9059281498193741, Final Batch Loss: 0.30297112464904785\n",
      "Epoch 3517, Loss: 0.791315570473671, Final Batch Loss: 0.17423559725284576\n",
      "Epoch 3518, Loss: 0.8822687417268753, Final Batch Loss: 0.1641434133052826\n",
      "Epoch 3519, Loss: 0.8545468151569366, Final Batch Loss: 0.17206060886383057\n",
      "Epoch 3520, Loss: 0.8879625499248505, Final Batch Loss: 0.22844575345516205\n",
      "Epoch 3521, Loss: 0.8290763646364212, Final Batch Loss: 0.1749613732099533\n",
      "Epoch 3522, Loss: 0.8542321473360062, Final Batch Loss: 0.21451079845428467\n",
      "Epoch 3523, Loss: 0.8434315770864487, Final Batch Loss: 0.18038131296634674\n",
      "Epoch 3524, Loss: 0.7836994603276253, Final Batch Loss: 0.11064112931489944\n",
      "Epoch 3525, Loss: 0.8183904364705086, Final Batch Loss: 0.07637319713830948\n",
      "Epoch 3526, Loss: 0.8887195289134979, Final Batch Loss: 0.17093390226364136\n",
      "Epoch 3527, Loss: 1.1111256927251816, Final Batch Loss: 0.4773976802825928\n",
      "Epoch 3528, Loss: 0.9982075989246368, Final Batch Loss: 0.3237491846084595\n",
      "Epoch 3529, Loss: 0.9608703702688217, Final Batch Loss: 0.23479081690311432\n",
      "Epoch 3530, Loss: 0.7320327162742615, Final Batch Loss: 0.1470380276441574\n",
      "Epoch 3531, Loss: 0.8605474233627319, Final Batch Loss: 0.19369173049926758\n",
      "Epoch 3532, Loss: 0.9571248590946198, Final Batch Loss: 0.25825104117393494\n",
      "Epoch 3533, Loss: 0.8766264319419861, Final Batch Loss: 0.2363169640302658\n",
      "Epoch 3534, Loss: 0.8404228240251541, Final Batch Loss: 0.25165823101997375\n",
      "Epoch 3535, Loss: 0.8226616382598877, Final Batch Loss: 0.1456497460603714\n",
      "Epoch 3536, Loss: 0.7600126415491104, Final Batch Loss: 0.10292407870292664\n",
      "Epoch 3537, Loss: 0.8136434406042099, Final Batch Loss: 0.18174193799495697\n",
      "Epoch 3538, Loss: 0.9687185287475586, Final Batch Loss: 0.2579197585582733\n",
      "Epoch 3539, Loss: 0.9347181767225266, Final Batch Loss: 0.26069390773773193\n",
      "Epoch 3540, Loss: 0.8437348306179047, Final Batch Loss: 0.20789647102355957\n",
      "Epoch 3541, Loss: 0.8428360521793365, Final Batch Loss: 0.1678496152162552\n",
      "Epoch 3542, Loss: 0.8169688880443573, Final Batch Loss: 0.1740247756242752\n",
      "Epoch 3543, Loss: 0.8809249699115753, Final Batch Loss: 0.16525082290172577\n",
      "Epoch 3544, Loss: 1.1103511899709702, Final Batch Loss: 0.3697855770587921\n",
      "Epoch 3545, Loss: 0.7118017673492432, Final Batch Loss: 0.15118329226970673\n",
      "Epoch 3546, Loss: 0.8958809077739716, Final Batch Loss: 0.2568140923976898\n",
      "Epoch 3547, Loss: 0.7914060056209564, Final Batch Loss: 0.1502997875213623\n",
      "Epoch 3548, Loss: 0.8812100142240524, Final Batch Loss: 0.2532670497894287\n",
      "Epoch 3549, Loss: 0.9854525178670883, Final Batch Loss: 0.1955837458372116\n",
      "Epoch 3550, Loss: 0.922438770532608, Final Batch Loss: 0.239327535033226\n",
      "Epoch 3551, Loss: 0.8945490717887878, Final Batch Loss: 0.2231122851371765\n",
      "Epoch 3552, Loss: 0.9517859518527985, Final Batch Loss: 0.23903362452983856\n",
      "Epoch 3553, Loss: 0.9385933727025986, Final Batch Loss: 0.26436302065849304\n",
      "Epoch 3554, Loss: 0.9769220948219299, Final Batch Loss: 0.16994355618953705\n",
      "Epoch 3555, Loss: 1.0031719952821732, Final Batch Loss: 0.34293583035469055\n",
      "Epoch 3556, Loss: 1.0969671308994293, Final Batch Loss: 0.40506884455680847\n",
      "Epoch 3557, Loss: 0.8350914269685745, Final Batch Loss: 0.2530016303062439\n",
      "Epoch 3558, Loss: 0.8015504255890846, Final Batch Loss: 0.11852554231882095\n",
      "Epoch 3559, Loss: 0.8237051516771317, Final Batch Loss: 0.13649310171604156\n",
      "Epoch 3560, Loss: 0.9383337646722794, Final Batch Loss: 0.22935271263122559\n",
      "Epoch 3561, Loss: 0.9268913418054581, Final Batch Loss: 0.19320286810398102\n",
      "Epoch 3562, Loss: 0.9734385907649994, Final Batch Loss: 0.23118643462657928\n",
      "Epoch 3563, Loss: 0.9016193747520447, Final Batch Loss: 0.2237457036972046\n",
      "Epoch 3564, Loss: 0.7877184152603149, Final Batch Loss: 0.1324993520975113\n",
      "Epoch 3565, Loss: 0.9701405912637711, Final Batch Loss: 0.2510845363140106\n",
      "Epoch 3566, Loss: 0.843341737985611, Final Batch Loss: 0.181054949760437\n",
      "Epoch 3567, Loss: 0.9037511050701141, Final Batch Loss: 0.23310886323451996\n",
      "Epoch 3568, Loss: 0.9105694890022278, Final Batch Loss: 0.28633520007133484\n",
      "Epoch 3569, Loss: 1.0663478672504425, Final Batch Loss: 0.33533093333244324\n",
      "Epoch 3570, Loss: 0.852817177772522, Final Batch Loss: 0.14866667985916138\n",
      "Epoch 3571, Loss: 0.8611742183566093, Final Batch Loss: 0.10416210442781448\n",
      "Epoch 3572, Loss: 0.7185252159833908, Final Batch Loss: 0.13513201475143433\n",
      "Epoch 3573, Loss: 0.9711111187934875, Final Batch Loss: 0.3052344024181366\n",
      "Epoch 3574, Loss: 0.9440614432096481, Final Batch Loss: 0.27085837721824646\n",
      "Epoch 3575, Loss: 0.812348410487175, Final Batch Loss: 0.1559915542602539\n",
      "Epoch 3576, Loss: 0.8266935348510742, Final Batch Loss: 0.25396040081977844\n",
      "Epoch 3577, Loss: 0.8532447665929794, Final Batch Loss: 0.167039692401886\n",
      "Epoch 3578, Loss: 0.7548938244581223, Final Batch Loss: 0.1511952430009842\n",
      "Epoch 3579, Loss: 0.9465938061475754, Final Batch Loss: 0.2918720543384552\n",
      "Epoch 3580, Loss: 0.8254256248474121, Final Batch Loss: 0.216283917427063\n",
      "Epoch 3581, Loss: 0.9544658213853836, Final Batch Loss: 0.14789806306362152\n",
      "Epoch 3582, Loss: 0.9003216326236725, Final Batch Loss: 0.24658234417438507\n",
      "Epoch 3583, Loss: 0.9270946532487869, Final Batch Loss: 0.2342473268508911\n",
      "Epoch 3584, Loss: 0.7583312690258026, Final Batch Loss: 0.2157517522573471\n",
      "Epoch 3585, Loss: 0.9660939276218414, Final Batch Loss: 0.31349506974220276\n",
      "Epoch 3586, Loss: 0.8877844363451004, Final Batch Loss: 0.14827929437160492\n",
      "Epoch 3587, Loss: 1.087377205491066, Final Batch Loss: 0.3358783721923828\n",
      "Epoch 3588, Loss: 1.0301550775766373, Final Batch Loss: 0.34037673473358154\n",
      "Epoch 3589, Loss: 1.0063937306404114, Final Batch Loss: 0.38770291209220886\n",
      "Epoch 3590, Loss: 0.7808128744363785, Final Batch Loss: 0.18919546902179718\n",
      "Epoch 3591, Loss: 0.9761834889650345, Final Batch Loss: 0.2274775356054306\n",
      "Epoch 3592, Loss: 0.9017355740070343, Final Batch Loss: 0.13443563878536224\n",
      "Epoch 3593, Loss: 0.8788891285657883, Final Batch Loss: 0.16891400516033173\n",
      "Epoch 3594, Loss: 0.8704320937395096, Final Batch Loss: 0.2230851650238037\n",
      "Epoch 3595, Loss: 0.9618435055017471, Final Batch Loss: 0.3292801082134247\n",
      "Epoch 3596, Loss: 0.9863315671682358, Final Batch Loss: 0.3452501595020294\n",
      "Epoch 3597, Loss: 1.0646765530109406, Final Batch Loss: 0.2851077616214752\n",
      "Epoch 3598, Loss: 0.8201525509357452, Final Batch Loss: 0.16344447433948517\n",
      "Epoch 3599, Loss: 0.7402990683913231, Final Batch Loss: 0.09115172177553177\n",
      "Epoch 3600, Loss: 0.8970975279808044, Final Batch Loss: 0.17579114437103271\n",
      "Epoch 3601, Loss: 0.9416004419326782, Final Batch Loss: 0.23667891323566437\n",
      "Epoch 3602, Loss: 0.82399120926857, Final Batch Loss: 0.21983368694782257\n",
      "Epoch 3603, Loss: 0.95025834441185, Final Batch Loss: 0.27997466921806335\n",
      "Epoch 3604, Loss: 0.8473028689622879, Final Batch Loss: 0.19231587648391724\n",
      "Epoch 3605, Loss: 0.9487214684486389, Final Batch Loss: 0.3124173581600189\n",
      "Epoch 3606, Loss: 1.0085225701332092, Final Batch Loss: 0.2728852927684784\n",
      "Epoch 3607, Loss: 0.8146981671452522, Final Batch Loss: 0.11313269287347794\n",
      "Epoch 3608, Loss: 0.8817789554595947, Final Batch Loss: 0.21006618440151215\n",
      "Epoch 3609, Loss: 0.8443337827920914, Final Batch Loss: 0.12690424919128418\n",
      "Epoch 3610, Loss: 0.9395418465137482, Final Batch Loss: 0.23897504806518555\n",
      "Epoch 3611, Loss: 1.0899466574192047, Final Batch Loss: 0.29851141571998596\n",
      "Epoch 3612, Loss: 0.9116693288087845, Final Batch Loss: 0.16975678503513336\n",
      "Epoch 3613, Loss: 1.040877565741539, Final Batch Loss: 0.27252721786499023\n",
      "Epoch 3614, Loss: 0.8261515274643898, Final Batch Loss: 0.08639978617429733\n",
      "Epoch 3615, Loss: 0.8322406560182571, Final Batch Loss: 0.12009599804878235\n",
      "Epoch 3616, Loss: 1.0745579898357391, Final Batch Loss: 0.3084779679775238\n",
      "Epoch 3617, Loss: 0.9665993750095367, Final Batch Loss: 0.27727004885673523\n",
      "Epoch 3618, Loss: 0.7919010668992996, Final Batch Loss: 0.11230498552322388\n",
      "Epoch 3619, Loss: 0.8045010715723038, Final Batch Loss: 0.15212132036685944\n",
      "Epoch 3620, Loss: 0.9594169855117798, Final Batch Loss: 0.3263026773929596\n",
      "Epoch 3621, Loss: 0.8215560615062714, Final Batch Loss: 0.22137445211410522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3622, Loss: 0.7623101323843002, Final Batch Loss: 0.095796138048172\n",
      "Epoch 3623, Loss: 0.8547640740871429, Final Batch Loss: 0.22159408032894135\n",
      "Epoch 3624, Loss: 0.9142870455980301, Final Batch Loss: 0.34154102206230164\n",
      "Epoch 3625, Loss: 0.7283002287149429, Final Batch Loss: 0.1549355834722519\n",
      "Epoch 3626, Loss: 0.9122848957777023, Final Batch Loss: 0.24455226957798004\n",
      "Epoch 3627, Loss: 0.7948790937662125, Final Batch Loss: 0.18365977704524994\n",
      "Epoch 3628, Loss: 0.7978773415088654, Final Batch Loss: 0.21538351476192474\n",
      "Epoch 3629, Loss: 0.8030949383974075, Final Batch Loss: 0.20092560350894928\n",
      "Epoch 3630, Loss: 0.7673721835017204, Final Batch Loss: 0.10869445651769638\n",
      "Epoch 3631, Loss: 0.9286676496267319, Final Batch Loss: 0.24488961696624756\n",
      "Epoch 3632, Loss: 0.9736749827861786, Final Batch Loss: 0.2686696946620941\n",
      "Epoch 3633, Loss: 1.0580443292856216, Final Batch Loss: 0.3321619927883148\n",
      "Epoch 3634, Loss: 0.7975384593009949, Final Batch Loss: 0.13878947496414185\n",
      "Epoch 3635, Loss: 0.9874048084020615, Final Batch Loss: 0.28621017932891846\n",
      "Epoch 3636, Loss: 0.8758897334337234, Final Batch Loss: 0.23693950474262238\n",
      "Epoch 3637, Loss: 0.8481530696153641, Final Batch Loss: 0.17538923025131226\n",
      "Epoch 3638, Loss: 0.9197569191455841, Final Batch Loss: 0.24506528675556183\n",
      "Epoch 3639, Loss: 0.9156810343265533, Final Batch Loss: 0.2903180718421936\n",
      "Epoch 3640, Loss: 1.0687572360038757, Final Batch Loss: 0.2995868921279907\n",
      "Epoch 3641, Loss: 0.8474014103412628, Final Batch Loss: 0.22393165528774261\n",
      "Epoch 3642, Loss: 0.9007028341293335, Final Batch Loss: 0.17478589713573456\n",
      "Epoch 3643, Loss: 0.8584589287638664, Final Batch Loss: 0.11815132945775986\n",
      "Epoch 3644, Loss: 0.8337996900081635, Final Batch Loss: 0.2074885219335556\n",
      "Epoch 3645, Loss: 1.0504649579524994, Final Batch Loss: 0.4181176722049713\n",
      "Epoch 3646, Loss: 0.8167463093996048, Final Batch Loss: 0.16075219213962555\n",
      "Epoch 3647, Loss: 1.0722376704216003, Final Batch Loss: 0.38294827938079834\n",
      "Epoch 3648, Loss: 0.92072494328022, Final Batch Loss: 0.31522542238235474\n",
      "Epoch 3649, Loss: 0.957695871591568, Final Batch Loss: 0.2784057557582855\n",
      "Epoch 3650, Loss: 0.8695461302995682, Final Batch Loss: 0.09963798522949219\n",
      "Epoch 3651, Loss: 0.8235955834388733, Final Batch Loss: 0.16177649796009064\n",
      "Epoch 3652, Loss: 0.9970237612724304, Final Batch Loss: 0.32815834879875183\n",
      "Epoch 3653, Loss: 0.8405334800481796, Final Batch Loss: 0.325722336769104\n",
      "Epoch 3654, Loss: 0.7528981268405914, Final Batch Loss: 0.16970042884349823\n",
      "Epoch 3655, Loss: 0.8838464319705963, Final Batch Loss: 0.2065068930387497\n",
      "Epoch 3656, Loss: 0.7973752021789551, Final Batch Loss: 0.16264128684997559\n",
      "Epoch 3657, Loss: 0.9203158617019653, Final Batch Loss: 0.2022048979997635\n",
      "Epoch 3658, Loss: 0.7996303364634514, Final Batch Loss: 0.11175139993429184\n",
      "Epoch 3659, Loss: 0.9259653985500336, Final Batch Loss: 0.2878345549106598\n",
      "Epoch 3660, Loss: 0.688580684363842, Final Batch Loss: 0.11328225582838058\n",
      "Epoch 3661, Loss: 0.6465925201773643, Final Batch Loss: 0.06290441006422043\n",
      "Epoch 3662, Loss: 0.9564054608345032, Final Batch Loss: 0.2466834932565689\n",
      "Epoch 3663, Loss: 0.8565829545259476, Final Batch Loss: 0.18343417346477509\n",
      "Epoch 3664, Loss: 1.0087624192237854, Final Batch Loss: 0.3393542468547821\n",
      "Epoch 3665, Loss: 0.7948589771986008, Final Batch Loss: 0.13880756497383118\n",
      "Epoch 3666, Loss: 0.9757745414972305, Final Batch Loss: 0.27014386653900146\n",
      "Epoch 3667, Loss: 1.0475174486637115, Final Batch Loss: 0.2753492295742035\n",
      "Epoch 3668, Loss: 1.0275694131851196, Final Batch Loss: 0.22117452323436737\n",
      "Epoch 3669, Loss: 0.8792497366666794, Final Batch Loss: 0.2763234078884125\n",
      "Epoch 3670, Loss: 0.8584069162607193, Final Batch Loss: 0.13161231577396393\n",
      "Epoch 3671, Loss: 0.8146879225969315, Final Batch Loss: 0.15443363785743713\n",
      "Epoch 3672, Loss: 0.8495381772518158, Final Batch Loss: 0.174067422747612\n",
      "Epoch 3673, Loss: 0.9585933536291122, Final Batch Loss: 0.2709047198295593\n",
      "Epoch 3674, Loss: 0.9268169850111008, Final Batch Loss: 0.26448991894721985\n",
      "Epoch 3675, Loss: 0.8541667461395264, Final Batch Loss: 0.22137987613677979\n",
      "Epoch 3676, Loss: 0.8809120059013367, Final Batch Loss: 0.1780521124601364\n",
      "Epoch 3677, Loss: 0.9175068736076355, Final Batch Loss: 0.18076933920383453\n",
      "Epoch 3678, Loss: 0.9050539880990982, Final Batch Loss: 0.2593529522418976\n",
      "Epoch 3679, Loss: 0.8710747510194778, Final Batch Loss: 0.1962047666311264\n",
      "Epoch 3680, Loss: 0.822445422410965, Final Batch Loss: 0.16258426010608673\n",
      "Epoch 3681, Loss: 0.9482003301382065, Final Batch Loss: 0.3161819875240326\n",
      "Epoch 3682, Loss: 0.9397764503955841, Final Batch Loss: 0.2925350069999695\n",
      "Epoch 3683, Loss: 0.9796248376369476, Final Batch Loss: 0.29303768277168274\n",
      "Epoch 3684, Loss: 0.9110938161611557, Final Batch Loss: 0.2930358350276947\n",
      "Epoch 3685, Loss: 0.9138713330030441, Final Batch Loss: 0.2130172699689865\n",
      "Epoch 3686, Loss: 0.9528333842754364, Final Batch Loss: 0.13703706860542297\n",
      "Epoch 3687, Loss: 0.7322437167167664, Final Batch Loss: 0.19009333848953247\n",
      "Epoch 3688, Loss: 0.8956182301044464, Final Batch Loss: 0.23939043283462524\n",
      "Epoch 3689, Loss: 0.9372487962245941, Final Batch Loss: 0.2991698086261749\n",
      "Epoch 3690, Loss: 0.9422536939382553, Final Batch Loss: 0.29476305842399597\n",
      "Epoch 3691, Loss: 0.8099195882678032, Final Batch Loss: 0.1242394670844078\n",
      "Epoch 3692, Loss: 1.146096020936966, Final Batch Loss: 0.35435423254966736\n",
      "Epoch 3693, Loss: 0.9508631974458694, Final Batch Loss: 0.23769275844097137\n",
      "Epoch 3694, Loss: 0.9743181467056274, Final Batch Loss: 0.2699944078922272\n",
      "Epoch 3695, Loss: 0.7373670637607574, Final Batch Loss: 0.13301418721675873\n",
      "Epoch 3696, Loss: 1.1356751322746277, Final Batch Loss: 0.3856513500213623\n",
      "Epoch 3697, Loss: 0.9606881439685822, Final Batch Loss: 0.3203827440738678\n",
      "Epoch 3698, Loss: 0.8593276664614677, Final Batch Loss: 0.0752241238951683\n",
      "Epoch 3699, Loss: 0.9670170992612839, Final Batch Loss: 0.23695380985736847\n",
      "Epoch 3700, Loss: 0.8903402388095856, Final Batch Loss: 0.12938331067562103\n",
      "Epoch 3701, Loss: 0.8448863625526428, Final Batch Loss: 0.20814692974090576\n",
      "Epoch 3702, Loss: 0.8962374180555344, Final Batch Loss: 0.20186559855937958\n",
      "Epoch 3703, Loss: 0.9087280184030533, Final Batch Loss: 0.2901814579963684\n",
      "Epoch 3704, Loss: 0.863147109746933, Final Batch Loss: 0.13119040429592133\n",
      "Epoch 3705, Loss: 1.1249797493219376, Final Batch Loss: 0.3710661828517914\n",
      "Epoch 3706, Loss: 0.8906655311584473, Final Batch Loss: 0.2531808018684387\n",
      "Epoch 3707, Loss: 0.827438548207283, Final Batch Loss: 0.19895100593566895\n",
      "Epoch 3708, Loss: 0.8740791231393814, Final Batch Loss: 0.28085872530937195\n",
      "Epoch 3709, Loss: 0.7948300689458847, Final Batch Loss: 0.15057392418384552\n",
      "Epoch 3710, Loss: 0.9130794107913971, Final Batch Loss: 0.2777424454689026\n",
      "Epoch 3711, Loss: 0.8904375731945038, Final Batch Loss: 0.19580090045928955\n",
      "Epoch 3712, Loss: 1.0598679333925247, Final Batch Loss: 0.33999550342559814\n",
      "Epoch 3713, Loss: 0.8258219063282013, Final Batch Loss: 0.162406787276268\n",
      "Epoch 3714, Loss: 0.93425253033638, Final Batch Loss: 0.20925390720367432\n",
      "Epoch 3715, Loss: 0.9695798307657242, Final Batch Loss: 0.3145096004009247\n",
      "Epoch 3716, Loss: 0.7662602216005325, Final Batch Loss: 0.1316569447517395\n",
      "Epoch 3717, Loss: 0.8190219104290009, Final Batch Loss: 0.20956279337406158\n",
      "Epoch 3718, Loss: 0.7315996438264847, Final Batch Loss: 0.15432092547416687\n",
      "Epoch 3719, Loss: 0.8946167230606079, Final Batch Loss: 0.2756626307964325\n",
      "Epoch 3720, Loss: 0.8432570025324821, Final Batch Loss: 0.1242113932967186\n",
      "Epoch 3721, Loss: 0.8614263236522675, Final Batch Loss: 0.27382540702819824\n",
      "Epoch 3722, Loss: 0.7384841740131378, Final Batch Loss: 0.10295096039772034\n",
      "Epoch 3723, Loss: 0.8822435438632965, Final Batch Loss: 0.23126836121082306\n",
      "Epoch 3724, Loss: 0.8899052888154984, Final Batch Loss: 0.211431086063385\n",
      "Epoch 3725, Loss: 0.8625785708427429, Final Batch Loss: 0.21772901713848114\n",
      "Epoch 3726, Loss: 0.9867361932992935, Final Batch Loss: 0.3071958124637604\n",
      "Epoch 3727, Loss: 1.1028656661510468, Final Batch Loss: 0.401180237531662\n",
      "Epoch 3728, Loss: 0.8660744279623032, Final Batch Loss: 0.24018247425556183\n",
      "Epoch 3729, Loss: 0.8536961823701859, Final Batch Loss: 0.22428162395954132\n",
      "Epoch 3730, Loss: 0.782951295375824, Final Batch Loss: 0.1739511936903\n",
      "Epoch 3731, Loss: 0.7357455790042877, Final Batch Loss: 0.13934718072414398\n",
      "Epoch 3732, Loss: 0.9421386420726776, Final Batch Loss: 0.16164691746234894\n",
      "Epoch 3733, Loss: 0.8819176703691483, Final Batch Loss: 0.211517795920372\n",
      "Epoch 3734, Loss: 0.7132042348384857, Final Batch Loss: 0.1279962658882141\n",
      "Epoch 3735, Loss: 0.9448894411325455, Final Batch Loss: 0.31506386399269104\n",
      "Epoch 3736, Loss: 1.0510118305683136, Final Batch Loss: 0.32581284642219543\n",
      "Epoch 3737, Loss: 0.9389842301607132, Final Batch Loss: 0.2182708978652954\n",
      "Epoch 3738, Loss: 0.9157098978757858, Final Batch Loss: 0.18649780750274658\n",
      "Epoch 3739, Loss: 0.845443993806839, Final Batch Loss: 0.19666580855846405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3740, Loss: 0.840474471449852, Final Batch Loss: 0.12850360572338104\n",
      "Epoch 3741, Loss: 0.6674973592162132, Final Batch Loss: 0.08688435703516006\n",
      "Epoch 3742, Loss: 0.8955796957015991, Final Batch Loss: 0.2649121582508087\n",
      "Epoch 3743, Loss: 0.9394944906234741, Final Batch Loss: 0.23762990534305573\n",
      "Epoch 3744, Loss: 0.8198306411504745, Final Batch Loss: 0.13289685547351837\n",
      "Epoch 3745, Loss: 1.087835043668747, Final Batch Loss: 0.2788754403591156\n",
      "Epoch 3746, Loss: 0.7813878655433655, Final Batch Loss: 0.22071264684200287\n",
      "Epoch 3747, Loss: 0.9058506041765213, Final Batch Loss: 0.31425783038139343\n",
      "Epoch 3748, Loss: 1.1205066293478012, Final Batch Loss: 0.38019001483917236\n",
      "Epoch 3749, Loss: 1.0028458833694458, Final Batch Loss: 0.3037392199039459\n",
      "Epoch 3750, Loss: 0.8202444612979889, Final Batch Loss: 0.1449265331029892\n",
      "Epoch 3751, Loss: 0.8833160176873207, Final Batch Loss: 0.09111087769269943\n",
      "Epoch 3752, Loss: 0.9173904657363892, Final Batch Loss: 0.18858206272125244\n",
      "Epoch 3753, Loss: 0.8605422228574753, Final Batch Loss: 0.19224508106708527\n",
      "Epoch 3754, Loss: 0.8593049496412277, Final Batch Loss: 0.13792285323143005\n",
      "Epoch 3755, Loss: 0.8167820125818253, Final Batch Loss: 0.17835889756679535\n",
      "Epoch 3756, Loss: 0.769069641828537, Final Batch Loss: 0.1245204508304596\n",
      "Epoch 3757, Loss: 0.9262179434299469, Final Batch Loss: 0.2342231124639511\n",
      "Epoch 3758, Loss: 1.1100474447011948, Final Batch Loss: 0.3104665279388428\n",
      "Epoch 3759, Loss: 0.8234183639287949, Final Batch Loss: 0.1948137879371643\n",
      "Epoch 3760, Loss: 1.0177623480558395, Final Batch Loss: 0.291289746761322\n",
      "Epoch 3761, Loss: 0.9146847873926163, Final Batch Loss: 0.1627722531557083\n",
      "Epoch 3762, Loss: 1.0264542400836945, Final Batch Loss: 0.294077605009079\n",
      "Epoch 3763, Loss: 1.0860304087400436, Final Batch Loss: 0.3343740701675415\n",
      "Epoch 3764, Loss: 0.8989181220531464, Final Batch Loss: 0.16935217380523682\n",
      "Epoch 3765, Loss: 0.9912518858909607, Final Batch Loss: 0.25012579560279846\n",
      "Epoch 3766, Loss: 1.0203580260276794, Final Batch Loss: 0.29800933599472046\n",
      "Epoch 3767, Loss: 0.7482673637568951, Final Batch Loss: 0.058566343039274216\n",
      "Epoch 3768, Loss: 1.0766868144273758, Final Batch Loss: 0.24733750522136688\n",
      "Epoch 3769, Loss: 0.9252908080816269, Final Batch Loss: 0.2985248565673828\n",
      "Epoch 3770, Loss: 0.8545102328062057, Final Batch Loss: 0.2848621904850006\n",
      "Epoch 3771, Loss: 1.0886727571487427, Final Batch Loss: 0.3577042818069458\n",
      "Epoch 3772, Loss: 0.8943751603364944, Final Batch Loss: 0.1595851629972458\n",
      "Epoch 3773, Loss: 0.8752922862768173, Final Batch Loss: 0.16263152658939362\n",
      "Epoch 3774, Loss: 0.8039189726114273, Final Batch Loss: 0.1847396045923233\n",
      "Epoch 3775, Loss: 0.891106978058815, Final Batch Loss: 0.26847222447395325\n",
      "Epoch 3776, Loss: 0.9237707853317261, Final Batch Loss: 0.24810828268527985\n",
      "Epoch 3777, Loss: 0.9266659468412399, Final Batch Loss: 0.23622286319732666\n",
      "Epoch 3778, Loss: 0.9934940189123154, Final Batch Loss: 0.33374595642089844\n",
      "Epoch 3779, Loss: 0.7842061966657639, Final Batch Loss: 0.14621834456920624\n",
      "Epoch 3780, Loss: 0.8840771019458771, Final Batch Loss: 0.19310568273067474\n",
      "Epoch 3781, Loss: 0.8213566690683365, Final Batch Loss: 0.15913623571395874\n",
      "Epoch 3782, Loss: 0.9327818155288696, Final Batch Loss: 0.2681525647640228\n",
      "Epoch 3783, Loss: 0.9703489392995834, Final Batch Loss: 0.25403133034706116\n",
      "Epoch 3784, Loss: 0.8408015370368958, Final Batch Loss: 0.2457527071237564\n",
      "Epoch 3785, Loss: 0.7159659266471863, Final Batch Loss: 0.12654559314250946\n",
      "Epoch 3786, Loss: 1.0807122886180878, Final Batch Loss: 0.4190625846385956\n",
      "Epoch 3787, Loss: 0.882908508181572, Final Batch Loss: 0.23518015444278717\n",
      "Epoch 3788, Loss: 0.7695767283439636, Final Batch Loss: 0.16607028245925903\n",
      "Epoch 3789, Loss: 0.8428528383374214, Final Batch Loss: 0.12314461916685104\n",
      "Epoch 3790, Loss: 0.87093386054039, Final Batch Loss: 0.16132427752017975\n",
      "Epoch 3791, Loss: 0.8308574706315994, Final Batch Loss: 0.14503951370716095\n",
      "Epoch 3792, Loss: 0.7842688411474228, Final Batch Loss: 0.12424030900001526\n",
      "Epoch 3793, Loss: 1.022185578942299, Final Batch Loss: 0.3147179186344147\n",
      "Epoch 3794, Loss: 0.8293007165193558, Final Batch Loss: 0.13791604340076447\n",
      "Epoch 3795, Loss: 0.8800722062587738, Final Batch Loss: 0.12036684155464172\n",
      "Epoch 3796, Loss: 0.974091574549675, Final Batch Loss: 0.29299184679985046\n",
      "Epoch 3797, Loss: 0.8547554463148117, Final Batch Loss: 0.2604369521141052\n",
      "Epoch 3798, Loss: 1.1699738055467606, Final Batch Loss: 0.3783509433269501\n",
      "Epoch 3799, Loss: 0.9951294511556625, Final Batch Loss: 0.29154881834983826\n",
      "Epoch 3800, Loss: 1.0262454599142075, Final Batch Loss: 0.3465546667575836\n",
      "Epoch 3801, Loss: 0.7964019924402237, Final Batch Loss: 0.22718657553195953\n",
      "Epoch 3802, Loss: 0.7402336448431015, Final Batch Loss: 0.10226160287857056\n",
      "Epoch 3803, Loss: 0.896041601896286, Final Batch Loss: 0.20193606615066528\n",
      "Epoch 3804, Loss: 1.0185214132070541, Final Batch Loss: 0.3028368651866913\n",
      "Epoch 3805, Loss: 1.0622425377368927, Final Batch Loss: 0.40614643692970276\n",
      "Epoch 3806, Loss: 0.6975651532411575, Final Batch Loss: 0.13148120045661926\n",
      "Epoch 3807, Loss: 0.8539200127124786, Final Batch Loss: 0.2009420245885849\n",
      "Epoch 3808, Loss: 0.8155462145805359, Final Batch Loss: 0.14712898433208466\n",
      "Epoch 3809, Loss: 0.9876356869935989, Final Batch Loss: 0.19914774596691132\n",
      "Epoch 3810, Loss: 0.7077392786741257, Final Batch Loss: 0.13034439086914062\n",
      "Epoch 3811, Loss: 0.7721783891320229, Final Batch Loss: 0.12325961142778397\n",
      "Epoch 3812, Loss: 0.766709491610527, Final Batch Loss: 0.15892644226551056\n",
      "Epoch 3813, Loss: 0.8332477062940598, Final Batch Loss: 0.21194708347320557\n",
      "Epoch 3814, Loss: 0.8927324861288071, Final Batch Loss: 0.20517468452453613\n",
      "Epoch 3815, Loss: 0.7313036024570465, Final Batch Loss: 0.1285579651594162\n",
      "Epoch 3816, Loss: 1.025385171175003, Final Batch Loss: 0.2595161199569702\n",
      "Epoch 3817, Loss: 0.990948885679245, Final Batch Loss: 0.3680768311023712\n",
      "Epoch 3818, Loss: 1.072243481874466, Final Batch Loss: 0.36251339316368103\n",
      "Epoch 3819, Loss: 0.9101225435733795, Final Batch Loss: 0.27272436022758484\n",
      "Epoch 3820, Loss: 1.1188294440507889, Final Batch Loss: 0.30736637115478516\n",
      "Epoch 3821, Loss: 0.7153106480836868, Final Batch Loss: 0.15164031088352203\n",
      "Epoch 3822, Loss: 0.8521494269371033, Final Batch Loss: 0.25701677799224854\n",
      "Epoch 3823, Loss: 0.7753202617168427, Final Batch Loss: 0.156849667429924\n",
      "Epoch 3824, Loss: 0.9979447722434998, Final Batch Loss: 0.31503602862358093\n",
      "Epoch 3825, Loss: 0.8405304402112961, Final Batch Loss: 0.2950023114681244\n",
      "Epoch 3826, Loss: 0.7299717888236046, Final Batch Loss: 0.09317442029714584\n",
      "Epoch 3827, Loss: 0.7541604265570641, Final Batch Loss: 0.10659555345773697\n",
      "Epoch 3828, Loss: 0.7587604373693466, Final Batch Loss: 0.18143199384212494\n",
      "Epoch 3829, Loss: 0.9809649437665939, Final Batch Loss: 0.33468928933143616\n",
      "Epoch 3830, Loss: 0.8831089437007904, Final Batch Loss: 0.25951266288757324\n",
      "Epoch 3831, Loss: 0.8194427490234375, Final Batch Loss: 0.18926741182804108\n",
      "Epoch 3832, Loss: 0.7291266471147537, Final Batch Loss: 0.16559313237667084\n",
      "Epoch 3833, Loss: 0.8013028874993324, Final Batch Loss: 0.07862859219312668\n",
      "Epoch 3834, Loss: 0.8868611603975296, Final Batch Loss: 0.23109014332294464\n",
      "Epoch 3835, Loss: 0.9942923486232758, Final Batch Loss: 0.3197803199291229\n",
      "Epoch 3836, Loss: 0.8273289054632187, Final Batch Loss: 0.20288018882274628\n",
      "Epoch 3837, Loss: 0.9610630869865417, Final Batch Loss: 0.19590754806995392\n",
      "Epoch 3838, Loss: 0.9708781838417053, Final Batch Loss: 0.2599240243434906\n",
      "Epoch 3839, Loss: 1.0080675333738327, Final Batch Loss: 0.42631688714027405\n",
      "Epoch 3840, Loss: 0.8348987698554993, Final Batch Loss: 0.2152467519044876\n",
      "Epoch 3841, Loss: 1.0219284892082214, Final Batch Loss: 0.3441300094127655\n",
      "Epoch 3842, Loss: 0.8855485320091248, Final Batch Loss: 0.14986327290534973\n",
      "Epoch 3843, Loss: 0.9743265211582184, Final Batch Loss: 0.291873961687088\n",
      "Epoch 3844, Loss: 1.0560331046581268, Final Batch Loss: 0.40021833777427673\n",
      "Epoch 3845, Loss: 0.7440487667918205, Final Batch Loss: 0.0928412601351738\n",
      "Epoch 3846, Loss: 0.8088917881250381, Final Batch Loss: 0.1879221796989441\n",
      "Epoch 3847, Loss: 0.8564478158950806, Final Batch Loss: 0.12602896988391876\n",
      "Epoch 3848, Loss: 0.7294116914272308, Final Batch Loss: 0.10973969101905823\n",
      "Epoch 3849, Loss: 0.8648064732551575, Final Batch Loss: 0.24439716339111328\n",
      "Epoch 3850, Loss: 0.9730273634195328, Final Batch Loss: 0.30894383788108826\n",
      "Epoch 3851, Loss: 0.9322790950536728, Final Batch Loss: 0.12669529020786285\n",
      "Epoch 3852, Loss: 1.0201504230499268, Final Batch Loss: 0.35677337646484375\n",
      "Epoch 3853, Loss: 0.9790136516094208, Final Batch Loss: 0.26760318875312805\n",
      "Epoch 3854, Loss: 0.7859618365764618, Final Batch Loss: 0.2568475008010864\n",
      "Epoch 3855, Loss: 0.8015559017658234, Final Batch Loss: 0.2261093407869339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3856, Loss: 0.8002222031354904, Final Batch Loss: 0.15300825238227844\n",
      "Epoch 3857, Loss: 0.9744731932878494, Final Batch Loss: 0.31090375781059265\n",
      "Epoch 3858, Loss: 0.8344851285219193, Final Batch Loss: 0.20073121786117554\n",
      "Epoch 3859, Loss: 0.901367112994194, Final Batch Loss: 0.20514720678329468\n",
      "Epoch 3860, Loss: 0.8155633211135864, Final Batch Loss: 0.21059608459472656\n",
      "Epoch 3861, Loss: 0.8769248276948929, Final Batch Loss: 0.224564328789711\n",
      "Epoch 3862, Loss: 0.8823796361684799, Final Batch Loss: 0.21147222816944122\n",
      "Epoch 3863, Loss: 0.9091786444187164, Final Batch Loss: 0.17025436460971832\n",
      "Epoch 3864, Loss: 0.9640263020992279, Final Batch Loss: 0.34055814146995544\n",
      "Epoch 3865, Loss: 0.6989777535200119, Final Batch Loss: 0.07990559935569763\n",
      "Epoch 3866, Loss: 1.0019937753677368, Final Batch Loss: 0.20854021608829498\n",
      "Epoch 3867, Loss: 0.7712125182151794, Final Batch Loss: 0.15361453592777252\n",
      "Epoch 3868, Loss: 0.8864529728889465, Final Batch Loss: 0.2962406575679779\n",
      "Epoch 3869, Loss: 0.7399321869015694, Final Batch Loss: 0.09853895753622055\n",
      "Epoch 3870, Loss: 0.8454720824956894, Final Batch Loss: 0.18113704025745392\n",
      "Epoch 3871, Loss: 1.0478494316339493, Final Batch Loss: 0.38417136669158936\n",
      "Epoch 3872, Loss: 0.7671076729893684, Final Batch Loss: 0.1152731403708458\n",
      "Epoch 3873, Loss: 0.9593493044376373, Final Batch Loss: 0.3839919865131378\n",
      "Epoch 3874, Loss: 1.1663247644901276, Final Batch Loss: 0.3523665964603424\n",
      "Epoch 3875, Loss: 0.9568142592906952, Final Batch Loss: 0.23170872032642365\n",
      "Epoch 3876, Loss: 0.865350753068924, Final Batch Loss: 0.3128444254398346\n",
      "Epoch 3877, Loss: 0.9136232882738113, Final Batch Loss: 0.22198164463043213\n",
      "Epoch 3878, Loss: 0.758448526263237, Final Batch Loss: 0.15550483763217926\n",
      "Epoch 3879, Loss: 0.763903334736824, Final Batch Loss: 0.1370922476053238\n",
      "Epoch 3880, Loss: 0.8512584418058395, Final Batch Loss: 0.2276432067155838\n",
      "Epoch 3881, Loss: 0.6340683475136757, Final Batch Loss: 0.09990180283784866\n",
      "Epoch 3882, Loss: 1.0180895328521729, Final Batch Loss: 0.36067819595336914\n",
      "Epoch 3883, Loss: 0.8783955574035645, Final Batch Loss: 0.19586598873138428\n",
      "Epoch 3884, Loss: 0.9281554520130157, Final Batch Loss: 0.26161637902259827\n",
      "Epoch 3885, Loss: 0.6975096352398396, Final Batch Loss: 0.06183502450585365\n",
      "Epoch 3886, Loss: 0.7444251999258995, Final Batch Loss: 0.06359284371137619\n",
      "Epoch 3887, Loss: 1.0561437159776688, Final Batch Loss: 0.4657870829105377\n",
      "Epoch 3888, Loss: 0.907704159617424, Final Batch Loss: 0.28060340881347656\n",
      "Epoch 3889, Loss: 1.1436362117528915, Final Batch Loss: 0.5134867429733276\n",
      "Epoch 3890, Loss: 1.037996619939804, Final Batch Loss: 0.3806804120540619\n",
      "Epoch 3891, Loss: 0.9414964318275452, Final Batch Loss: 0.3357495367527008\n",
      "Epoch 3892, Loss: 0.8973790258169174, Final Batch Loss: 0.1679442971944809\n",
      "Epoch 3893, Loss: 0.8797028958797455, Final Batch Loss: 0.2318210005760193\n",
      "Epoch 3894, Loss: 0.8741578757762909, Final Batch Loss: 0.1088402271270752\n",
      "Epoch 3895, Loss: 0.898698702454567, Final Batch Loss: 0.2529180943965912\n",
      "Epoch 3896, Loss: 0.7619567066431046, Final Batch Loss: 0.13459458947181702\n",
      "Epoch 3897, Loss: 0.8265768587589264, Final Batch Loss: 0.13726867735385895\n",
      "Epoch 3898, Loss: 0.8567162603139877, Final Batch Loss: 0.22975750267505646\n",
      "Epoch 3899, Loss: 0.723013237118721, Final Batch Loss: 0.18036361038684845\n",
      "Epoch 3900, Loss: 0.8572110682725906, Final Batch Loss: 0.19877563416957855\n",
      "Epoch 3901, Loss: 0.862658217549324, Final Batch Loss: 0.2624501883983612\n",
      "Epoch 3902, Loss: 1.0322175025939941, Final Batch Loss: 0.15710638463497162\n",
      "Epoch 3903, Loss: 0.8735388517379761, Final Batch Loss: 0.1412031203508377\n",
      "Epoch 3904, Loss: 0.9987306594848633, Final Batch Loss: 0.2624376714229584\n",
      "Epoch 3905, Loss: 0.7905540689826012, Final Batch Loss: 0.10815947502851486\n",
      "Epoch 3906, Loss: 0.9029439389705658, Final Batch Loss: 0.17447827756404877\n",
      "Epoch 3907, Loss: 0.9175396859645844, Final Batch Loss: 0.2536086440086365\n",
      "Epoch 3908, Loss: 0.9800940006971359, Final Batch Loss: 0.31399819254875183\n",
      "Epoch 3909, Loss: 0.9108299612998962, Final Batch Loss: 0.24493063986301422\n",
      "Epoch 3910, Loss: 0.8726148456335068, Final Batch Loss: 0.17429886758327484\n",
      "Epoch 3911, Loss: 0.8503838181495667, Final Batch Loss: 0.2042578011751175\n",
      "Epoch 3912, Loss: 0.8194355368614197, Final Batch Loss: 0.18621690571308136\n",
      "Epoch 3913, Loss: 1.1710392981767654, Final Batch Loss: 0.41187748312950134\n",
      "Epoch 3914, Loss: 0.9513558894395828, Final Batch Loss: 0.3379645049571991\n",
      "Epoch 3915, Loss: 0.7728565633296967, Final Batch Loss: 0.14719848334789276\n",
      "Epoch 3916, Loss: 0.7774837464094162, Final Batch Loss: 0.21632267534732819\n",
      "Epoch 3917, Loss: 0.6714788377285004, Final Batch Loss: 0.12043946981430054\n",
      "Epoch 3918, Loss: 0.9170747995376587, Final Batch Loss: 0.30740830302238464\n",
      "Epoch 3919, Loss: 0.9968758672475815, Final Batch Loss: 0.272964745759964\n",
      "Epoch 3920, Loss: 0.8453379720449448, Final Batch Loss: 0.2156694531440735\n",
      "Epoch 3921, Loss: 0.8944095373153687, Final Batch Loss: 0.2381066232919693\n",
      "Epoch 3922, Loss: 0.8767648786306381, Final Batch Loss: 0.14731405675411224\n",
      "Epoch 3923, Loss: 0.8839650675654411, Final Batch Loss: 0.11491163820028305\n",
      "Epoch 3924, Loss: 0.8457939922809601, Final Batch Loss: 0.22234398126602173\n",
      "Epoch 3925, Loss: 0.8989095240831375, Final Batch Loss: 0.19684475660324097\n",
      "Epoch 3926, Loss: 0.9223141074180603, Final Batch Loss: 0.34862759709358215\n",
      "Epoch 3927, Loss: 1.0352260619401932, Final Batch Loss: 0.3867206275463104\n",
      "Epoch 3928, Loss: 0.9183548390865326, Final Batch Loss: 0.28677836060523987\n",
      "Epoch 3929, Loss: 0.8111884891986847, Final Batch Loss: 0.18193624913692474\n",
      "Epoch 3930, Loss: 0.6701633781194687, Final Batch Loss: 0.1321842521429062\n",
      "Epoch 3931, Loss: 0.9253052026033401, Final Batch Loss: 0.22802704572677612\n",
      "Epoch 3932, Loss: 0.9484767764806747, Final Batch Loss: 0.23414303362369537\n",
      "Epoch 3933, Loss: 0.952544704079628, Final Batch Loss: 0.3464122712612152\n",
      "Epoch 3934, Loss: 0.9384529888629913, Final Batch Loss: 0.23126168549060822\n",
      "Epoch 3935, Loss: 0.8323998600244522, Final Batch Loss: 0.14942392706871033\n",
      "Epoch 3936, Loss: 0.878775343298912, Final Batch Loss: 0.1432143598794937\n",
      "Epoch 3937, Loss: 0.7921765595674515, Final Batch Loss: 0.13269250094890594\n",
      "Epoch 3938, Loss: 0.8601514995098114, Final Batch Loss: 0.15817217528820038\n",
      "Epoch 3939, Loss: 0.8480891734361649, Final Batch Loss: 0.19976681470870972\n",
      "Epoch 3940, Loss: 0.8276319056749344, Final Batch Loss: 0.21097679436206818\n",
      "Epoch 3941, Loss: 1.015348419547081, Final Batch Loss: 0.3498118817806244\n",
      "Epoch 3942, Loss: 0.8296153992414474, Final Batch Loss: 0.21868126094341278\n",
      "Epoch 3943, Loss: 0.8143903613090515, Final Batch Loss: 0.1387902945280075\n",
      "Epoch 3944, Loss: 0.731919676065445, Final Batch Loss: 0.1363229751586914\n",
      "Epoch 3945, Loss: 1.0162716209888458, Final Batch Loss: 0.2621930241584778\n",
      "Epoch 3946, Loss: 0.8878207206726074, Final Batch Loss: 0.2543753385543823\n",
      "Epoch 3947, Loss: 0.9061211794614792, Final Batch Loss: 0.24764478206634521\n",
      "Epoch 3948, Loss: 1.0073119103908539, Final Batch Loss: 0.3174123466014862\n",
      "Epoch 3949, Loss: 0.7218457385897636, Final Batch Loss: 0.10596803575754166\n",
      "Epoch 3950, Loss: 0.866733506321907, Final Batch Loss: 0.27468425035476685\n",
      "Epoch 3951, Loss: 0.7269425466656685, Final Batch Loss: 0.09432811290025711\n",
      "Epoch 3952, Loss: 0.8400891721248627, Final Batch Loss: 0.19190026819705963\n",
      "Epoch 3953, Loss: 0.8036099672317505, Final Batch Loss: 0.2317546010017395\n",
      "Epoch 3954, Loss: 0.9377095550298691, Final Batch Loss: 0.26968157291412354\n",
      "Epoch 3955, Loss: 0.9549139440059662, Final Batch Loss: 0.16217771172523499\n",
      "Epoch 3956, Loss: 0.9538520574569702, Final Batch Loss: 0.26927536725997925\n",
      "Epoch 3957, Loss: 0.9701777398586273, Final Batch Loss: 0.28472718596458435\n",
      "Epoch 3958, Loss: 0.8416655361652374, Final Batch Loss: 0.19066941738128662\n",
      "Epoch 3959, Loss: 0.83870729804039, Final Batch Loss: 0.17527252435684204\n",
      "Epoch 3960, Loss: 1.0226314514875412, Final Batch Loss: 0.3672296106815338\n",
      "Epoch 3961, Loss: 0.9834702014923096, Final Batch Loss: 0.23673604428768158\n",
      "Epoch 3962, Loss: 0.8265354186296463, Final Batch Loss: 0.19687910377979279\n",
      "Epoch 3963, Loss: 0.8029061555862427, Final Batch Loss: 0.17400747537612915\n",
      "Epoch 3964, Loss: 0.7669681906700134, Final Batch Loss: 0.1901741623878479\n",
      "Epoch 3965, Loss: 0.7994016855955124, Final Batch Loss: 0.1714901477098465\n",
      "Epoch 3966, Loss: 0.7501503527164459, Final Batch Loss: 0.1634042114019394\n",
      "Epoch 3967, Loss: 0.7893457859754562, Final Batch Loss: 0.2113417237997055\n",
      "Epoch 3968, Loss: 0.8312708735466003, Final Batch Loss: 0.21223236620426178\n",
      "Epoch 3969, Loss: 0.8135291263461113, Final Batch Loss: 0.11697033792734146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3970, Loss: 0.8742377609014511, Final Batch Loss: 0.19575941562652588\n",
      "Epoch 3971, Loss: 0.7967499345541, Final Batch Loss: 0.18118572235107422\n",
      "Epoch 3972, Loss: 0.814272940158844, Final Batch Loss: 0.1942722052335739\n",
      "Epoch 3973, Loss: 0.9265973567962646, Final Batch Loss: 0.24179695546627045\n",
      "Epoch 3974, Loss: 0.8552422523498535, Final Batch Loss: 0.2790813744068146\n",
      "Epoch 3975, Loss: 0.7589405924081802, Final Batch Loss: 0.0906326174736023\n",
      "Epoch 3976, Loss: 0.7756411731243134, Final Batch Loss: 0.11883676052093506\n",
      "Epoch 3977, Loss: 0.8080468773841858, Final Batch Loss: 0.1438957154750824\n",
      "Epoch 3978, Loss: 0.9271691292524338, Final Batch Loss: 0.25032445788383484\n",
      "Epoch 3979, Loss: 0.7089451253414154, Final Batch Loss: 0.1330076903104782\n",
      "Epoch 3980, Loss: 0.8370363265275955, Final Batch Loss: 0.2406138926744461\n",
      "Epoch 3981, Loss: 0.5976227298378944, Final Batch Loss: 0.07658951729536057\n",
      "Epoch 3982, Loss: 0.9388845562934875, Final Batch Loss: 0.22628949582576752\n",
      "Epoch 3983, Loss: 0.8151070475578308, Final Batch Loss: 0.1471375823020935\n",
      "Epoch 3984, Loss: 0.8753911107778549, Final Batch Loss: 0.17868094146251678\n",
      "Epoch 3985, Loss: 0.8652734607458115, Final Batch Loss: 0.1745271235704422\n",
      "Epoch 3986, Loss: 0.7778917402029037, Final Batch Loss: 0.17493575811386108\n",
      "Epoch 3987, Loss: 0.7884530276060104, Final Batch Loss: 0.21735310554504395\n",
      "Epoch 3988, Loss: 1.0735539495944977, Final Batch Loss: 0.48104920983314514\n",
      "Epoch 3989, Loss: 1.0464510917663574, Final Batch Loss: 0.2933942377567291\n",
      "Epoch 3990, Loss: 0.9625082015991211, Final Batch Loss: 0.22489525377750397\n",
      "Epoch 3991, Loss: 0.8540567755699158, Final Batch Loss: 0.16903497278690338\n",
      "Epoch 3992, Loss: 0.7929392158985138, Final Batch Loss: 0.16769425570964813\n",
      "Epoch 3993, Loss: 1.0963385254144669, Final Batch Loss: 0.3138095438480377\n",
      "Epoch 3994, Loss: 0.9180366545915604, Final Batch Loss: 0.16821981966495514\n",
      "Epoch 3995, Loss: 0.9244813770055771, Final Batch Loss: 0.26617860794067383\n",
      "Epoch 3996, Loss: 0.8515483736991882, Final Batch Loss: 0.1752261072397232\n",
      "Epoch 3997, Loss: 0.9473875612020493, Final Batch Loss: 0.22284911572933197\n",
      "Epoch 3998, Loss: 0.8839820623397827, Final Batch Loss: 0.14653097093105316\n",
      "Epoch 3999, Loss: 0.955956220626831, Final Batch Loss: 0.25486478209495544\n",
      "Epoch 4000, Loss: 0.8271472901105881, Final Batch Loss: 0.20245367288589478\n",
      "Epoch 4001, Loss: 1.0210903733968735, Final Batch Loss: 0.3176964819431305\n",
      "Epoch 4002, Loss: 0.832080289721489, Final Batch Loss: 0.15492276847362518\n",
      "Epoch 4003, Loss: 0.9015611559152603, Final Batch Loss: 0.34818804264068604\n",
      "Epoch 4004, Loss: 0.9849737584590912, Final Batch Loss: 0.2839946448802948\n",
      "Epoch 4005, Loss: 0.8688551187515259, Final Batch Loss: 0.2522072494029999\n",
      "Epoch 4006, Loss: 0.7477146908640862, Final Batch Loss: 0.1095644012093544\n",
      "Epoch 4007, Loss: 0.9519251734018326, Final Batch Loss: 0.2706112861633301\n",
      "Epoch 4008, Loss: 0.9088621437549591, Final Batch Loss: 0.35186877846717834\n",
      "Epoch 4009, Loss: 0.8285429924726486, Final Batch Loss: 0.2211116999387741\n",
      "Epoch 4010, Loss: 0.9989486634731293, Final Batch Loss: 0.36783310770988464\n",
      "Epoch 4011, Loss: 0.8587899059057236, Final Batch Loss: 0.28579434752464294\n",
      "Epoch 4012, Loss: 0.8644529730081558, Final Batch Loss: 0.3096456825733185\n",
      "Epoch 4013, Loss: 0.9693911671638489, Final Batch Loss: 0.17589856684207916\n",
      "Epoch 4014, Loss: 0.7867368161678314, Final Batch Loss: 0.13416892290115356\n",
      "Epoch 4015, Loss: 0.9536635652184486, Final Batch Loss: 0.3757675886154175\n",
      "Epoch 4016, Loss: 0.7223660722374916, Final Batch Loss: 0.08153317123651505\n",
      "Epoch 4017, Loss: 0.7746133953332901, Final Batch Loss: 0.1869361847639084\n",
      "Epoch 4018, Loss: 0.9407676756381989, Final Batch Loss: 0.23986823856830597\n",
      "Epoch 4019, Loss: 0.860028013586998, Final Batch Loss: 0.24682652950286865\n",
      "Epoch 4020, Loss: 0.8640475273132324, Final Batch Loss: 0.20869670808315277\n",
      "Epoch 4021, Loss: 0.7915274202823639, Final Batch Loss: 0.1446680873632431\n",
      "Epoch 4022, Loss: 0.8245778828859329, Final Batch Loss: 0.1872665137052536\n",
      "Epoch 4023, Loss: 0.9350282549858093, Final Batch Loss: 0.27342489361763\n",
      "Epoch 4024, Loss: 1.0456433594226837, Final Batch Loss: 0.2210370898246765\n",
      "Epoch 4025, Loss: 0.7684569358825684, Final Batch Loss: 0.1651310920715332\n",
      "Epoch 4026, Loss: 1.0181843042373657, Final Batch Loss: 0.4082831144332886\n",
      "Epoch 4027, Loss: 0.7475609257817268, Final Batch Loss: 0.09091360121965408\n",
      "Epoch 4028, Loss: 1.053359940648079, Final Batch Loss: 0.427778959274292\n",
      "Epoch 4029, Loss: 0.755916565656662, Final Batch Loss: 0.14117486774921417\n",
      "Epoch 4030, Loss: 0.8944364488124847, Final Batch Loss: 0.15416809916496277\n",
      "Epoch 4031, Loss: 0.8912538290023804, Final Batch Loss: 0.24867868423461914\n",
      "Epoch 4032, Loss: 0.8716212809085846, Final Batch Loss: 0.2517299950122833\n",
      "Epoch 4033, Loss: 0.8849962130188942, Final Batch Loss: 0.11011762171983719\n",
      "Epoch 4034, Loss: 0.830508679151535, Final Batch Loss: 0.20680885016918182\n",
      "Epoch 4035, Loss: 0.9119954854249954, Final Batch Loss: 0.3040665090084076\n",
      "Epoch 4036, Loss: 0.8847179859876633, Final Batch Loss: 0.17112235724925995\n",
      "Epoch 4037, Loss: 0.7524532750248909, Final Batch Loss: 0.10504978150129318\n",
      "Epoch 4038, Loss: 0.7635132968425751, Final Batch Loss: 0.12983731925487518\n",
      "Epoch 4039, Loss: 0.8589361682534218, Final Batch Loss: 0.11222664266824722\n",
      "Epoch 4040, Loss: 0.7752892374992371, Final Batch Loss: 0.12908025085926056\n",
      "Epoch 4041, Loss: 0.8720436543226242, Final Batch Loss: 0.2156047374010086\n",
      "Epoch 4042, Loss: 0.8218428939580917, Final Batch Loss: 0.24908345937728882\n",
      "Epoch 4043, Loss: 0.8212123215198517, Final Batch Loss: 0.18385589122772217\n",
      "Epoch 4044, Loss: 0.9351954013109207, Final Batch Loss: 0.2610118091106415\n",
      "Epoch 4045, Loss: 0.8856867104768753, Final Batch Loss: 0.22807110846042633\n",
      "Epoch 4046, Loss: 0.7515929937362671, Final Batch Loss: 0.14520218968391418\n",
      "Epoch 4047, Loss: 0.8533274084329605, Final Batch Loss: 0.16117887198925018\n",
      "Epoch 4048, Loss: 0.8990979790687561, Final Batch Loss: 0.34735167026519775\n",
      "Epoch 4049, Loss: 0.8952438980340958, Final Batch Loss: 0.21814216673374176\n",
      "Epoch 4050, Loss: 0.7904063165187836, Final Batch Loss: 0.18752311170101166\n",
      "Epoch 4051, Loss: 0.7283822745084763, Final Batch Loss: 0.17252741754055023\n",
      "Epoch 4052, Loss: 0.838370144367218, Final Batch Loss: 0.23089516162872314\n",
      "Epoch 4053, Loss: 0.7684490233659744, Final Batch Loss: 0.1779956966638565\n",
      "Epoch 4054, Loss: 0.7510085105895996, Final Batch Loss: 0.17875181138515472\n",
      "Epoch 4055, Loss: 0.8097930252552032, Final Batch Loss: 0.21137456595897675\n",
      "Epoch 4056, Loss: 0.8403775840997696, Final Batch Loss: 0.21111424267292023\n",
      "Epoch 4057, Loss: 1.0702783912420273, Final Batch Loss: 0.36019429564476013\n",
      "Epoch 4058, Loss: 0.7366564273834229, Final Batch Loss: 0.1487351804971695\n",
      "Epoch 4059, Loss: 0.9589880108833313, Final Batch Loss: 0.28285154700279236\n",
      "Epoch 4060, Loss: 0.886273205280304, Final Batch Loss: 0.1976410150527954\n",
      "Epoch 4061, Loss: 0.8304392993450165, Final Batch Loss: 0.2694315016269684\n",
      "Epoch 4062, Loss: 0.9425252377986908, Final Batch Loss: 0.32145535945892334\n",
      "Epoch 4063, Loss: 0.9355895817279816, Final Batch Loss: 0.2578531801700592\n",
      "Epoch 4064, Loss: 0.7210172563791275, Final Batch Loss: 0.13975296914577484\n",
      "Epoch 4065, Loss: 0.8348415642976761, Final Batch Loss: 0.20763356983661652\n",
      "Epoch 4066, Loss: 0.8731914907693863, Final Batch Loss: 0.2398713082075119\n",
      "Epoch 4067, Loss: 0.736994594335556, Final Batch Loss: 0.1430816650390625\n",
      "Epoch 4068, Loss: 0.7908808141946793, Final Batch Loss: 0.15972180664539337\n",
      "Epoch 4069, Loss: 0.8424852639436722, Final Batch Loss: 0.14559979736804962\n",
      "Epoch 4070, Loss: 0.7773880213499069, Final Batch Loss: 0.17831724882125854\n",
      "Epoch 4071, Loss: 0.8676324784755707, Final Batch Loss: 0.24078501760959625\n",
      "Epoch 4072, Loss: 0.7435369044542313, Final Batch Loss: 0.1194399893283844\n",
      "Epoch 4073, Loss: 0.9726257771253586, Final Batch Loss: 0.4144705832004547\n",
      "Epoch 4074, Loss: 0.8625129759311676, Final Batch Loss: 0.24251733720302582\n",
      "Epoch 4075, Loss: 0.8235772103071213, Final Batch Loss: 0.18482273817062378\n",
      "Epoch 4076, Loss: 1.1534122377634048, Final Batch Loss: 0.10120052099227905\n",
      "Epoch 4077, Loss: 0.970827728509903, Final Batch Loss: 0.3347623348236084\n",
      "Epoch 4078, Loss: 0.828601598739624, Final Batch Loss: 0.12400737404823303\n",
      "Epoch 4079, Loss: 0.9063751995563507, Final Batch Loss: 0.289679616689682\n",
      "Epoch 4080, Loss: 0.9267195612192154, Final Batch Loss: 0.21668505668640137\n",
      "Epoch 4081, Loss: 0.8598832041025162, Final Batch Loss: 0.21678024530410767\n",
      "Epoch 4082, Loss: 0.7566611245274544, Final Batch Loss: 0.11279013007879257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4083, Loss: 1.0015969574451447, Final Batch Loss: 0.2378121018409729\n",
      "Epoch 4084, Loss: 1.0126309394836426, Final Batch Loss: 0.2913397252559662\n",
      "Epoch 4085, Loss: 1.1786935329437256, Final Batch Loss: 0.5018198490142822\n",
      "Epoch 4086, Loss: 0.8168037384748459, Final Batch Loss: 0.247282013297081\n",
      "Epoch 4087, Loss: 0.8224385231733322, Final Batch Loss: 0.14286796748638153\n",
      "Epoch 4088, Loss: 0.7820836454629898, Final Batch Loss: 0.14342330396175385\n",
      "Epoch 4089, Loss: 0.8726175874471664, Final Batch Loss: 0.2043006271123886\n",
      "Epoch 4090, Loss: 0.7972206845879555, Final Batch Loss: 0.11405021697282791\n",
      "Epoch 4091, Loss: 0.8918105661869049, Final Batch Loss: 0.2591851055622101\n",
      "Epoch 4092, Loss: 0.7936928570270538, Final Batch Loss: 0.16885161399841309\n",
      "Epoch 4093, Loss: 0.9339627474546432, Final Batch Loss: 0.26936858892440796\n",
      "Epoch 4094, Loss: 0.8073471188545227, Final Batch Loss: 0.1962602734565735\n",
      "Epoch 4095, Loss: 0.7461281716823578, Final Batch Loss: 0.14506398141384125\n",
      "Epoch 4096, Loss: 0.7830184735357761, Final Batch Loss: 0.05811067298054695\n",
      "Epoch 4097, Loss: 0.8697654753923416, Final Batch Loss: 0.25472232699394226\n",
      "Epoch 4098, Loss: 0.9377787858247757, Final Batch Loss: 0.26631855964660645\n",
      "Epoch 4099, Loss: 0.7025200873613358, Final Batch Loss: 0.14457112550735474\n",
      "Epoch 4100, Loss: 0.809174120426178, Final Batch Loss: 0.1583377718925476\n",
      "Epoch 4101, Loss: 0.7748113945126534, Final Batch Loss: 0.10628344863653183\n",
      "Epoch 4102, Loss: 0.8360031098127365, Final Batch Loss: 0.18740524351596832\n",
      "Epoch 4103, Loss: 0.7725718915462494, Final Batch Loss: 0.16906248033046722\n",
      "Epoch 4104, Loss: 1.0109670758247375, Final Batch Loss: 0.28254368901252747\n",
      "Epoch 4105, Loss: 0.9849558770656586, Final Batch Loss: 0.30451157689094543\n",
      "Epoch 4106, Loss: 0.8431015610694885, Final Batch Loss: 0.3006124794483185\n",
      "Epoch 4107, Loss: 1.0075797140598297, Final Batch Loss: 0.2896515429019928\n",
      "Epoch 4108, Loss: 0.8599828630685806, Final Batch Loss: 0.1702927201986313\n",
      "Epoch 4109, Loss: 0.6953939497470856, Final Batch Loss: 0.13627146184444427\n",
      "Epoch 4110, Loss: 0.8491335064172745, Final Batch Loss: 0.23580096662044525\n",
      "Epoch 4111, Loss: 0.9016624093055725, Final Batch Loss: 0.1900501400232315\n",
      "Epoch 4112, Loss: 0.9450854659080505, Final Batch Loss: 0.2577874958515167\n",
      "Epoch 4113, Loss: 0.8935076892375946, Final Batch Loss: 0.2453535795211792\n",
      "Epoch 4114, Loss: 0.9984190464019775, Final Batch Loss: 0.270643025636673\n",
      "Epoch 4115, Loss: 0.9588489085435867, Final Batch Loss: 0.3228164613246918\n",
      "Epoch 4116, Loss: 0.9319329857826233, Final Batch Loss: 0.2518532872200012\n",
      "Epoch 4117, Loss: 0.7960185781121254, Final Batch Loss: 0.12233776599168777\n",
      "Epoch 4118, Loss: 0.9981436878442764, Final Batch Loss: 0.3145749866962433\n",
      "Epoch 4119, Loss: 1.085442915558815, Final Batch Loss: 0.4836691617965698\n",
      "Epoch 4120, Loss: 1.0263287425041199, Final Batch Loss: 0.41301360726356506\n",
      "Epoch 4121, Loss: 0.9688976109027863, Final Batch Loss: 0.2727927267551422\n",
      "Epoch 4122, Loss: 0.7971108108758926, Final Batch Loss: 0.20262742042541504\n",
      "Epoch 4123, Loss: 0.8698911964893341, Final Batch Loss: 0.279231458902359\n",
      "Epoch 4124, Loss: 0.7716071754693985, Final Batch Loss: 0.15948234498500824\n",
      "Epoch 4125, Loss: 0.9009767323732376, Final Batch Loss: 0.27302512526512146\n",
      "Epoch 4126, Loss: 0.6189162768423557, Final Batch Loss: 0.060272399336099625\n",
      "Epoch 4127, Loss: 0.9012663513422012, Final Batch Loss: 0.29468271136283875\n",
      "Epoch 4128, Loss: 0.7797721400856972, Final Batch Loss: 0.09292284399271011\n",
      "Epoch 4129, Loss: 0.8772009164094925, Final Batch Loss: 0.21160216629505157\n",
      "Epoch 4130, Loss: 0.9362115114927292, Final Batch Loss: 0.2753782868385315\n",
      "Epoch 4131, Loss: 0.8474843129515648, Final Batch Loss: 0.11024237424135208\n",
      "Epoch 4132, Loss: 0.8022561073303223, Final Batch Loss: 0.13700410723686218\n",
      "Epoch 4133, Loss: 0.7927653566002846, Final Batch Loss: 0.12191259115934372\n",
      "Epoch 4134, Loss: 0.9595335274934769, Final Batch Loss: 0.3807872235774994\n",
      "Epoch 4135, Loss: 0.8738283812999725, Final Batch Loss: 0.2666601240634918\n",
      "Epoch 4136, Loss: 0.8235795795917511, Final Batch Loss: 0.21716618537902832\n",
      "Epoch 4137, Loss: 0.9006557613611221, Final Batch Loss: 0.22509777545928955\n",
      "Epoch 4138, Loss: 0.8527895361185074, Final Batch Loss: 0.21518748998641968\n",
      "Epoch 4139, Loss: 0.7726812809705734, Final Batch Loss: 0.14231161773204803\n",
      "Epoch 4140, Loss: 0.8829134702682495, Final Batch Loss: 0.23627740144729614\n",
      "Epoch 4141, Loss: 0.9565678536891937, Final Batch Loss: 0.22730682790279388\n",
      "Epoch 4142, Loss: 1.0180437117815018, Final Batch Loss: 0.2654593884944916\n",
      "Epoch 4143, Loss: 0.9497358947992325, Final Batch Loss: 0.3022019863128662\n",
      "Epoch 4144, Loss: 1.0554298907518387, Final Batch Loss: 0.23455069959163666\n",
      "Epoch 4145, Loss: 0.8834925293922424, Final Batch Loss: 0.2157331109046936\n",
      "Epoch 4146, Loss: 0.9256232678890228, Final Batch Loss: 0.2939154803752899\n",
      "Epoch 4147, Loss: 0.9557978510856628, Final Batch Loss: 0.25592342019081116\n",
      "Epoch 4148, Loss: 0.9580502361059189, Final Batch Loss: 0.34572911262512207\n",
      "Epoch 4149, Loss: 0.8974402546882629, Final Batch Loss: 0.1554550975561142\n",
      "Epoch 4150, Loss: 1.0077874213457108, Final Batch Loss: 0.3469713628292084\n",
      "Epoch 4151, Loss: 0.8507509976625443, Final Batch Loss: 0.1448523998260498\n",
      "Epoch 4152, Loss: 0.8574557602405548, Final Batch Loss: 0.1643247902393341\n",
      "Epoch 4153, Loss: 0.8592119365930557, Final Batch Loss: 0.18129493296146393\n",
      "Epoch 4154, Loss: 0.6701006442308426, Final Batch Loss: 0.0993543267250061\n",
      "Epoch 4155, Loss: 0.9567551463842392, Final Batch Loss: 0.41570624709129333\n",
      "Epoch 4156, Loss: 0.7346305698156357, Final Batch Loss: 0.15044009685516357\n",
      "Epoch 4157, Loss: 0.6229642033576965, Final Batch Loss: 0.06632806360721588\n",
      "Epoch 4158, Loss: 0.9139949381351471, Final Batch Loss: 0.283933162689209\n",
      "Epoch 4159, Loss: 0.86837949603796, Final Batch Loss: 0.10475263744592667\n",
      "Epoch 4160, Loss: 0.9825748801231384, Final Batch Loss: 0.3991379737854004\n",
      "Epoch 4161, Loss: 0.7535931766033173, Final Batch Loss: 0.16410404443740845\n",
      "Epoch 4162, Loss: 0.8028674423694611, Final Batch Loss: 0.22275133430957794\n",
      "Epoch 4163, Loss: 0.8176240026950836, Final Batch Loss: 0.15729045867919922\n",
      "Epoch 4164, Loss: 0.8855942040681839, Final Batch Loss: 0.2853400409221649\n",
      "Epoch 4165, Loss: 0.8667815327644348, Final Batch Loss: 0.24601809680461884\n",
      "Epoch 4166, Loss: 0.8264693468809128, Final Batch Loss: 0.14008992910385132\n",
      "Epoch 4167, Loss: 0.8776915371417999, Final Batch Loss: 0.1682519167661667\n",
      "Epoch 4168, Loss: 1.0007499307394028, Final Batch Loss: 0.32125285267829895\n",
      "Epoch 4169, Loss: 0.8626262098550797, Final Batch Loss: 0.19839303195476532\n",
      "Epoch 4170, Loss: 0.9426366239786148, Final Batch Loss: 0.22293905913829803\n",
      "Epoch 4171, Loss: 0.9792162925004959, Final Batch Loss: 0.32621434330940247\n",
      "Epoch 4172, Loss: 0.7931045144796371, Final Batch Loss: 0.1922387033700943\n",
      "Epoch 4173, Loss: 1.1728333085775375, Final Batch Loss: 0.4240058362483978\n",
      "Epoch 4174, Loss: 0.8957539051771164, Final Batch Loss: 0.19051557779312134\n",
      "Epoch 4175, Loss: 0.869504377245903, Final Batch Loss: 0.15926216542720795\n",
      "Epoch 4176, Loss: 0.8996641933917999, Final Batch Loss: 0.20314939320087433\n",
      "Epoch 4177, Loss: 0.7493579089641571, Final Batch Loss: 0.12838752567768097\n",
      "Epoch 4178, Loss: 0.7273994535207748, Final Batch Loss: 0.13313408195972443\n",
      "Epoch 4179, Loss: 0.8727287948131561, Final Batch Loss: 0.14167864620685577\n",
      "Epoch 4180, Loss: 0.843688115477562, Final Batch Loss: 0.1865978240966797\n",
      "Epoch 4181, Loss: 1.093861997127533, Final Batch Loss: 0.30040350556373596\n",
      "Epoch 4182, Loss: 0.8107818216085434, Final Batch Loss: 0.1431012600660324\n",
      "Epoch 4183, Loss: 0.7984225451946259, Final Batch Loss: 0.19254648685455322\n",
      "Epoch 4184, Loss: 0.83668452501297, Final Batch Loss: 0.2491946965456009\n",
      "Epoch 4185, Loss: 0.7614372968673706, Final Batch Loss: 0.16053210198879242\n",
      "Epoch 4186, Loss: 0.8621180653572083, Final Batch Loss: 0.1880919337272644\n",
      "Epoch 4187, Loss: 0.6373852044343948, Final Batch Loss: 0.1022588312625885\n",
      "Epoch 4188, Loss: 0.8054250180721283, Final Batch Loss: 0.2047531008720398\n",
      "Epoch 4189, Loss: 0.841823548078537, Final Batch Loss: 0.17757724225521088\n",
      "Epoch 4190, Loss: 0.9261544495820999, Final Batch Loss: 0.278534859418869\n",
      "Epoch 4191, Loss: 0.8117288053035736, Final Batch Loss: 0.22098536789417267\n",
      "Epoch 4192, Loss: 0.7822038680315018, Final Batch Loss: 0.1430685818195343\n",
      "Epoch 4193, Loss: 0.694394513964653, Final Batch Loss: 0.11714127659797668\n",
      "Epoch 4194, Loss: 0.8992725610733032, Final Batch Loss: 0.19431249797344208\n",
      "Epoch 4195, Loss: 0.8485859930515289, Final Batch Loss: 0.23355351388454437\n",
      "Epoch 4196, Loss: 0.8074056208133698, Final Batch Loss: 0.22182337939739227\n",
      "Epoch 4197, Loss: 0.6898093000054359, Final Batch Loss: 0.09660262614488602\n",
      "Epoch 4198, Loss: 0.8349080830812454, Final Batch Loss: 0.23122738301753998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4199, Loss: 0.8382553309202194, Final Batch Loss: 0.21634091436862946\n",
      "Epoch 4200, Loss: 0.8440712243318558, Final Batch Loss: 0.16961438953876495\n",
      "Epoch 4201, Loss: 0.7625923603773117, Final Batch Loss: 0.13488896191120148\n",
      "Epoch 4202, Loss: 0.8519291877746582, Final Batch Loss: 0.19003523886203766\n",
      "Epoch 4203, Loss: 0.8458812832832336, Final Batch Loss: 0.15107281506061554\n",
      "Epoch 4204, Loss: 0.8353104144334793, Final Batch Loss: 0.18774956464767456\n",
      "Epoch 4205, Loss: 0.7152266800403595, Final Batch Loss: 0.13948912918567657\n",
      "Epoch 4206, Loss: 0.7012856230139732, Final Batch Loss: 0.12449190765619278\n",
      "Epoch 4207, Loss: 0.9300785213708878, Final Batch Loss: 0.310126930475235\n",
      "Epoch 4208, Loss: 0.7597485035657883, Final Batch Loss: 0.18601691722869873\n",
      "Epoch 4209, Loss: 0.7232598885893822, Final Batch Loss: 0.06740064173936844\n",
      "Epoch 4210, Loss: 0.8331800401210785, Final Batch Loss: 0.13809871673583984\n",
      "Epoch 4211, Loss: 0.7265845835208893, Final Batch Loss: 0.11967122554779053\n",
      "Epoch 4212, Loss: 0.8057891875505447, Final Batch Loss: 0.1767103523015976\n",
      "Epoch 4213, Loss: 0.8148966580629349, Final Batch Loss: 0.1967514604330063\n",
      "Epoch 4214, Loss: 0.8610218018293381, Final Batch Loss: 0.19583582878112793\n",
      "Epoch 4215, Loss: 0.7431030869483948, Final Batch Loss: 0.15453839302062988\n",
      "Epoch 4216, Loss: 0.7644409388303757, Final Batch Loss: 0.20254193246364594\n",
      "Epoch 4217, Loss: 0.8168347626924515, Final Batch Loss: 0.19581355154514313\n",
      "Epoch 4218, Loss: 0.8074786812067032, Final Batch Loss: 0.20226575434207916\n",
      "Epoch 4219, Loss: 0.868512749671936, Final Batch Loss: 0.1637602001428604\n",
      "Epoch 4220, Loss: 0.7334067225456238, Final Batch Loss: 0.12516723573207855\n",
      "Epoch 4221, Loss: 0.9604593813419342, Final Batch Loss: 0.3637857437133789\n",
      "Epoch 4222, Loss: 0.8772106915712357, Final Batch Loss: 0.22384613752365112\n",
      "Epoch 4223, Loss: 0.9597089737653732, Final Batch Loss: 0.29358914494514465\n",
      "Epoch 4224, Loss: 0.8475984781980515, Final Batch Loss: 0.209647998213768\n",
      "Epoch 4225, Loss: 1.2952977120876312, Final Batch Loss: 0.6425522565841675\n",
      "Epoch 4226, Loss: 0.9439183920621872, Final Batch Loss: 0.26018500328063965\n",
      "Epoch 4227, Loss: 0.7857908606529236, Final Batch Loss: 0.14322783052921295\n",
      "Epoch 4228, Loss: 0.9112474918365479, Final Batch Loss: 0.29661718010902405\n",
      "Epoch 4229, Loss: 0.7751082628965378, Final Batch Loss: 0.13575433194637299\n",
      "Epoch 4230, Loss: 1.0440606474876404, Final Batch Loss: 0.16453108191490173\n",
      "Epoch 4231, Loss: 1.000091016292572, Final Batch Loss: 0.29568150639533997\n",
      "Epoch 4232, Loss: 0.9370461255311966, Final Batch Loss: 0.2835806906223297\n",
      "Epoch 4233, Loss: 1.0579003393650055, Final Batch Loss: 0.3262229561805725\n",
      "Epoch 4234, Loss: 0.7668364271521568, Final Batch Loss: 0.08223053067922592\n",
      "Epoch 4235, Loss: 0.7046235948801041, Final Batch Loss: 0.14073874056339264\n",
      "Epoch 4236, Loss: 1.1976029723882675, Final Batch Loss: 0.34116330742836\n",
      "Epoch 4237, Loss: 0.836767166852951, Final Batch Loss: 0.16483832895755768\n",
      "Epoch 4238, Loss: 1.065409168601036, Final Batch Loss: 0.3649345338344574\n",
      "Epoch 4239, Loss: 0.9050175100564957, Final Batch Loss: 0.202802836894989\n",
      "Epoch 4240, Loss: 0.7786562591791153, Final Batch Loss: 0.15739552676677704\n",
      "Epoch 4241, Loss: 0.7874907851219177, Final Batch Loss: 0.19726906716823578\n",
      "Epoch 4242, Loss: 0.859202116727829, Final Batch Loss: 0.23161570727825165\n",
      "Epoch 4243, Loss: 0.8832143992185593, Final Batch Loss: 0.21844536066055298\n",
      "Epoch 4244, Loss: 0.743027999997139, Final Batch Loss: 0.08892863988876343\n",
      "Epoch 4245, Loss: 0.8363281190395355, Final Batch Loss: 0.13772736489772797\n",
      "Epoch 4246, Loss: 0.9636078327894211, Final Batch Loss: 0.2519634962081909\n",
      "Epoch 4247, Loss: 1.4125595092773438, Final Batch Loss: 0.7088866829872131\n",
      "Epoch 4248, Loss: 0.8350649774074554, Final Batch Loss: 0.21741388738155365\n",
      "Epoch 4249, Loss: 1.0943734794855118, Final Batch Loss: 0.48518478870391846\n",
      "Epoch 4250, Loss: 0.8538325130939484, Final Batch Loss: 0.14550745487213135\n",
      "Epoch 4251, Loss: 1.012343481183052, Final Batch Loss: 0.3723407983779907\n",
      "Epoch 4252, Loss: 0.869864821434021, Final Batch Loss: 0.22530537843704224\n",
      "Epoch 4253, Loss: 0.9267651587724686, Final Batch Loss: 0.2513960301876068\n",
      "Epoch 4254, Loss: 0.8998298794031143, Final Batch Loss: 0.18726122379302979\n",
      "Epoch 4255, Loss: 0.9074439629912376, Final Batch Loss: 0.12416074424982071\n",
      "Epoch 4256, Loss: 0.899217277765274, Final Batch Loss: 0.21848200261592865\n",
      "Epoch 4257, Loss: 0.8445649743080139, Final Batch Loss: 0.16545850038528442\n",
      "Epoch 4258, Loss: 0.8517022281885147, Final Batch Loss: 0.26556041836738586\n",
      "Epoch 4259, Loss: 0.7333852201700211, Final Batch Loss: 0.13195283710956573\n",
      "Epoch 4260, Loss: 0.8805097043514252, Final Batch Loss: 0.2442687749862671\n",
      "Epoch 4261, Loss: 0.9556685984134674, Final Batch Loss: 0.29412320256233215\n",
      "Epoch 4262, Loss: 0.935108482837677, Final Batch Loss: 0.2643093466758728\n",
      "Epoch 4263, Loss: 0.7263780385255814, Final Batch Loss: 0.16060315072536469\n",
      "Epoch 4264, Loss: 0.8457918167114258, Final Batch Loss: 0.1833413690328598\n",
      "Epoch 4265, Loss: 0.7234827131032944, Final Batch Loss: 0.09780862927436829\n",
      "Epoch 4266, Loss: 0.7298539578914642, Final Batch Loss: 0.09637415409088135\n",
      "Epoch 4267, Loss: 1.0004348903894424, Final Batch Loss: 0.3047838807106018\n",
      "Epoch 4268, Loss: 0.9515897929668427, Final Batch Loss: 0.2482292205095291\n",
      "Epoch 4269, Loss: 0.7928627133369446, Final Batch Loss: 0.08996972441673279\n",
      "Epoch 4270, Loss: 0.777036540210247, Final Batch Loss: 0.07407761365175247\n",
      "Epoch 4271, Loss: 0.8660041093826294, Final Batch Loss: 0.20136944949626923\n",
      "Epoch 4272, Loss: 0.7194394618272781, Final Batch Loss: 0.16362345218658447\n",
      "Epoch 4273, Loss: 0.8834604471921921, Final Batch Loss: 0.20461679995059967\n",
      "Epoch 4274, Loss: 0.8035080879926682, Final Batch Loss: 0.20892655849456787\n",
      "Epoch 4275, Loss: 0.8888630717992783, Final Batch Loss: 0.14613156020641327\n",
      "Epoch 4276, Loss: 0.8406466394662857, Final Batch Loss: 0.16175395250320435\n",
      "Epoch 4277, Loss: 0.9932257533073425, Final Batch Loss: 0.26973649859428406\n",
      "Epoch 4278, Loss: 0.7012529820203781, Final Batch Loss: 0.13099117577075958\n",
      "Epoch 4279, Loss: 0.6960886791348457, Final Batch Loss: 0.10901076346635818\n",
      "Epoch 4280, Loss: 0.8125482797622681, Final Batch Loss: 0.17888708412647247\n",
      "Epoch 4281, Loss: 0.6958680748939514, Final Batch Loss: 0.17516732215881348\n",
      "Epoch 4282, Loss: 0.7520089596509933, Final Batch Loss: 0.19445760548114777\n",
      "Epoch 4283, Loss: 0.7655829936265945, Final Batch Loss: 0.2158784717321396\n",
      "Epoch 4284, Loss: 0.8253898918628693, Final Batch Loss: 0.22391021251678467\n",
      "Epoch 4285, Loss: 0.9855658859014511, Final Batch Loss: 0.3669213354587555\n",
      "Epoch 4286, Loss: 0.6913846656680107, Final Batch Loss: 0.09608537703752518\n",
      "Epoch 4287, Loss: 0.9737091809511185, Final Batch Loss: 0.3294108808040619\n",
      "Epoch 4288, Loss: 0.8106017708778381, Final Batch Loss: 0.21103866398334503\n",
      "Epoch 4289, Loss: 0.8059997856616974, Final Batch Loss: 0.12671662867069244\n",
      "Epoch 4290, Loss: 0.9038832932710648, Final Batch Loss: 0.24923501908779144\n",
      "Epoch 4291, Loss: 0.8261871933937073, Final Batch Loss: 0.25044816732406616\n",
      "Epoch 4292, Loss: 0.8860370963811874, Final Batch Loss: 0.2122766375541687\n",
      "Epoch 4293, Loss: 0.7745533734560013, Final Batch Loss: 0.16869580745697021\n",
      "Epoch 4294, Loss: 1.0070418417453766, Final Batch Loss: 0.3160479962825775\n",
      "Epoch 4295, Loss: 0.8821174055337906, Final Batch Loss: 0.26183250546455383\n",
      "Epoch 4296, Loss: 0.8003074377775192, Final Batch Loss: 0.2571156322956085\n",
      "Epoch 4297, Loss: 0.716818742454052, Final Batch Loss: 0.11935191601514816\n",
      "Epoch 4298, Loss: 0.7518094182014465, Final Batch Loss: 0.19939808547496796\n",
      "Epoch 4299, Loss: 0.8154218345880508, Final Batch Loss: 0.14248748123645782\n",
      "Epoch 4300, Loss: 0.7209417968988419, Final Batch Loss: 0.18053649365901947\n",
      "Epoch 4301, Loss: 0.8351403027772903, Final Batch Loss: 0.14023317396640778\n",
      "Epoch 4302, Loss: 0.9572934210300446, Final Batch Loss: 0.3278158903121948\n",
      "Epoch 4303, Loss: 0.8074073195457458, Final Batch Loss: 0.1448090821504593\n",
      "Epoch 4304, Loss: 0.8617779165506363, Final Batch Loss: 0.23853270709514618\n",
      "Epoch 4305, Loss: 1.046780377626419, Final Batch Loss: 0.3959430754184723\n",
      "Epoch 4306, Loss: 0.8496139794588089, Final Batch Loss: 0.13674812018871307\n",
      "Epoch 4307, Loss: 0.7630298435688019, Final Batch Loss: 0.17880471050739288\n",
      "Epoch 4308, Loss: 0.7828964963555336, Final Batch Loss: 0.09584274142980576\n",
      "Epoch 4309, Loss: 0.7722002416849136, Final Batch Loss: 0.15486061573028564\n",
      "Epoch 4310, Loss: 0.7408414483070374, Final Batch Loss: 0.09907174110412598\n",
      "Epoch 4311, Loss: 0.7788445353507996, Final Batch Loss: 0.12706157565116882\n",
      "Epoch 4312, Loss: 0.8459094762802124, Final Batch Loss: 0.19848720729351044\n",
      "Epoch 4313, Loss: 0.6455328166484833, Final Batch Loss: 0.16724032163619995\n",
      "Epoch 4314, Loss: 0.8559821397066116, Final Batch Loss: 0.1871979683637619\n",
      "Epoch 4315, Loss: 0.8183320909738541, Final Batch Loss: 0.24367420375347137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4316, Loss: 0.937360942363739, Final Batch Loss: 0.33561423420906067\n",
      "Epoch 4317, Loss: 0.7768667936325073, Final Batch Loss: 0.18926477432250977\n",
      "Epoch 4318, Loss: 0.8579865396022797, Final Batch Loss: 0.19583427906036377\n",
      "Epoch 4319, Loss: 1.0649537295103073, Final Batch Loss: 0.384378045797348\n",
      "Epoch 4320, Loss: 1.003883734345436, Final Batch Loss: 0.31097909808158875\n",
      "Epoch 4321, Loss: 1.0346168875694275, Final Batch Loss: 0.47436657547950745\n",
      "Epoch 4322, Loss: 0.7740837186574936, Final Batch Loss: 0.15223102271556854\n",
      "Epoch 4323, Loss: 0.7748115807771683, Final Batch Loss: 0.1604091376066208\n",
      "Epoch 4324, Loss: 0.8348007202148438, Final Batch Loss: 0.17537175118923187\n",
      "Epoch 4325, Loss: 0.8453132063150406, Final Batch Loss: 0.1754828691482544\n",
      "Epoch 4326, Loss: 0.7427536696195602, Final Batch Loss: 0.27583107352256775\n",
      "Epoch 4327, Loss: 0.8049457967281342, Final Batch Loss: 0.25836285948753357\n",
      "Epoch 4328, Loss: 0.8050875663757324, Final Batch Loss: 0.15708807110786438\n",
      "Epoch 4329, Loss: 0.7815455347299576, Final Batch Loss: 0.1426851898431778\n",
      "Epoch 4330, Loss: 0.8276452273130417, Final Batch Loss: 0.17391692101955414\n",
      "Epoch 4331, Loss: 0.7478393614292145, Final Batch Loss: 0.18487299978733063\n",
      "Epoch 4332, Loss: 0.7874061018228531, Final Batch Loss: 0.146322563290596\n",
      "Epoch 4333, Loss: 0.9073228240013123, Final Batch Loss: 0.24067693948745728\n",
      "Epoch 4334, Loss: 0.7699731737375259, Final Batch Loss: 0.14412227272987366\n",
      "Epoch 4335, Loss: 1.1039207875728607, Final Batch Loss: 0.38726457953453064\n",
      "Epoch 4336, Loss: 0.7264757752418518, Final Batch Loss: 0.16496895253658295\n",
      "Epoch 4337, Loss: 0.7699009925127029, Final Batch Loss: 0.1857542246580124\n",
      "Epoch 4338, Loss: 0.8011755794286728, Final Batch Loss: 0.19091124832630157\n",
      "Epoch 4339, Loss: 0.8614518791437149, Final Batch Loss: 0.196665421128273\n",
      "Epoch 4340, Loss: 0.8316676318645477, Final Batch Loss: 0.2603350579738617\n",
      "Epoch 4341, Loss: 0.9130520969629288, Final Batch Loss: 0.21944095194339752\n",
      "Epoch 4342, Loss: 0.7914867848157883, Final Batch Loss: 0.17243419587612152\n",
      "Epoch 4343, Loss: 0.7430209219455719, Final Batch Loss: 0.1129675805568695\n",
      "Epoch 4344, Loss: 0.9655447751283646, Final Batch Loss: 0.32737600803375244\n",
      "Epoch 4345, Loss: 0.8696963936090469, Final Batch Loss: 0.2409406304359436\n",
      "Epoch 4346, Loss: 0.8174590766429901, Final Batch Loss: 0.20554451644420624\n",
      "Epoch 4347, Loss: 0.7623168975114822, Final Batch Loss: 0.13645611703395844\n",
      "Epoch 4348, Loss: 0.774968832731247, Final Batch Loss: 0.16861172020435333\n",
      "Epoch 4349, Loss: 0.7644778862595558, Final Batch Loss: 0.10927867144346237\n",
      "Epoch 4350, Loss: 0.8540343344211578, Final Batch Loss: 0.16328026354312897\n",
      "Epoch 4351, Loss: 0.7997356206178665, Final Batch Loss: 0.1649647057056427\n",
      "Epoch 4352, Loss: 0.924585685133934, Final Batch Loss: 0.30823805928230286\n",
      "Epoch 4353, Loss: 0.8547437340021133, Final Batch Loss: 0.2694496810436249\n",
      "Epoch 4354, Loss: 0.8417405486106873, Final Batch Loss: 0.1991748958826065\n",
      "Epoch 4355, Loss: 0.7574223726987839, Final Batch Loss: 0.13342082500457764\n",
      "Epoch 4356, Loss: 0.7854493856430054, Final Batch Loss: 0.15193535387516022\n",
      "Epoch 4357, Loss: 0.9271267503499985, Final Batch Loss: 0.24921582639217377\n",
      "Epoch 4358, Loss: 0.7897201031446457, Final Batch Loss: 0.2582019567489624\n",
      "Epoch 4359, Loss: 0.8640017807483673, Final Batch Loss: 0.2363344430923462\n",
      "Epoch 4360, Loss: 0.9139411151409149, Final Batch Loss: 0.26561495661735535\n",
      "Epoch 4361, Loss: 0.8425732553005219, Final Batch Loss: 0.19787633419036865\n",
      "Epoch 4362, Loss: 0.7021353244781494, Final Batch Loss: 0.12340456247329712\n",
      "Epoch 4363, Loss: 0.8755041807889938, Final Batch Loss: 0.19289852678775787\n",
      "Epoch 4364, Loss: 0.75765360891819, Final Batch Loss: 0.15885429084300995\n",
      "Epoch 4365, Loss: 0.9472426027059555, Final Batch Loss: 0.3950904607772827\n",
      "Epoch 4366, Loss: 0.9953529387712479, Final Batch Loss: 0.43932127952575684\n",
      "Epoch 4367, Loss: 0.7771075516939163, Final Batch Loss: 0.22086121141910553\n",
      "Epoch 4368, Loss: 0.8291100561618805, Final Batch Loss: 0.23561491072177887\n",
      "Epoch 4369, Loss: 0.6932520493865013, Final Batch Loss: 0.0864899680018425\n",
      "Epoch 4370, Loss: 0.8170185983181, Final Batch Loss: 0.21089418232440948\n",
      "Epoch 4371, Loss: 0.9376195073127747, Final Batch Loss: 0.3450045585632324\n",
      "Epoch 4372, Loss: 0.7995111495256424, Final Batch Loss: 0.2116018682718277\n",
      "Epoch 4373, Loss: 0.7559683322906494, Final Batch Loss: 0.18026959896087646\n",
      "Epoch 4374, Loss: 0.7486494332551956, Final Batch Loss: 0.16148553788661957\n",
      "Epoch 4375, Loss: 0.7542757391929626, Final Batch Loss: 0.22307588160037994\n",
      "Epoch 4376, Loss: 0.6736544147133827, Final Batch Loss: 0.0625300332903862\n",
      "Epoch 4377, Loss: 1.0840464383363724, Final Batch Loss: 0.4074987471103668\n",
      "Epoch 4378, Loss: 0.8498434275388718, Final Batch Loss: 0.22691138088703156\n",
      "Epoch 4379, Loss: 0.7630062848329544, Final Batch Loss: 0.1354367434978485\n",
      "Epoch 4380, Loss: 0.7744041830301285, Final Batch Loss: 0.18381015956401825\n",
      "Epoch 4381, Loss: 0.8820909410715103, Final Batch Loss: 0.33884188532829285\n",
      "Epoch 4382, Loss: 0.846544548869133, Final Batch Loss: 0.17022769153118134\n",
      "Epoch 4383, Loss: 0.7614491134881973, Final Batch Loss: 0.1890290230512619\n",
      "Epoch 4384, Loss: 0.9733814150094986, Final Batch Loss: 0.2755536735057831\n",
      "Epoch 4385, Loss: 0.7436126172542572, Final Batch Loss: 0.15407754480838776\n",
      "Epoch 4386, Loss: 0.7640209943056107, Final Batch Loss: 0.28193193674087524\n",
      "Epoch 4387, Loss: 0.7602850198745728, Final Batch Loss: 0.1709313839673996\n",
      "Epoch 4388, Loss: 0.8033467531204224, Final Batch Loss: 0.16108901798725128\n",
      "Epoch 4389, Loss: 0.7745221853256226, Final Batch Loss: 0.16194675862789154\n",
      "Epoch 4390, Loss: 0.6708266064524651, Final Batch Loss: 0.1129482313990593\n",
      "Epoch 4391, Loss: 0.7485452890396118, Final Batch Loss: 0.1161905825138092\n",
      "Epoch 4392, Loss: 1.1027682572603226, Final Batch Loss: 0.41581377387046814\n",
      "Epoch 4393, Loss: 0.6917133405804634, Final Batch Loss: 0.08089444786310196\n",
      "Epoch 4394, Loss: 0.9118113815784454, Final Batch Loss: 0.3020904064178467\n",
      "Epoch 4395, Loss: 0.8709251433610916, Final Batch Loss: 0.14156927168369293\n",
      "Epoch 4396, Loss: 0.8022964149713516, Final Batch Loss: 0.16786359250545502\n",
      "Epoch 4397, Loss: 0.8664857745170593, Final Batch Loss: 0.24624834954738617\n",
      "Epoch 4398, Loss: 0.954698383808136, Final Batch Loss: 0.2465975433588028\n",
      "Epoch 4399, Loss: 0.9528401792049408, Final Batch Loss: 0.3226698935031891\n",
      "Epoch 4400, Loss: 1.0649384409189224, Final Batch Loss: 0.4260902404785156\n",
      "Epoch 4401, Loss: 0.976411908864975, Final Batch Loss: 0.29192498326301575\n",
      "Epoch 4402, Loss: 0.9790302813053131, Final Batch Loss: 0.3214258551597595\n",
      "Epoch 4403, Loss: 0.7001090943813324, Final Batch Loss: 0.16738806664943695\n",
      "Epoch 4404, Loss: 0.7783427238464355, Final Batch Loss: 0.15948836505413055\n",
      "Epoch 4405, Loss: 0.8593091368675232, Final Batch Loss: 0.19483761489391327\n",
      "Epoch 4406, Loss: 0.9197817444801331, Final Batch Loss: 0.2573120892047882\n",
      "Epoch 4407, Loss: 0.7764330953359604, Final Batch Loss: 0.252024382352829\n",
      "Epoch 4408, Loss: 0.7993841618299484, Final Batch Loss: 0.20008637011051178\n",
      "Epoch 4409, Loss: 0.8635685443878174, Final Batch Loss: 0.24183742702007294\n",
      "Epoch 4410, Loss: 0.9099591225385666, Final Batch Loss: 0.32061541080474854\n",
      "Epoch 4411, Loss: 0.7114786207675934, Final Batch Loss: 0.1476716250181198\n",
      "Epoch 4412, Loss: 0.868911549448967, Final Batch Loss: 0.23372481763362885\n",
      "Epoch 4413, Loss: 0.8248768597841263, Final Batch Loss: 0.21110141277313232\n",
      "Epoch 4414, Loss: 0.7094422429800034, Final Batch Loss: 0.17608143389225006\n",
      "Epoch 4415, Loss: 0.7375537604093552, Final Batch Loss: 0.20332735776901245\n",
      "Epoch 4416, Loss: 0.7298069596290588, Final Batch Loss: 0.20412755012512207\n",
      "Epoch 4417, Loss: 0.939235657453537, Final Batch Loss: 0.27159109711647034\n",
      "Epoch 4418, Loss: 0.9027594774961472, Final Batch Loss: 0.2971917390823364\n",
      "Epoch 4419, Loss: 0.8079369813203812, Final Batch Loss: 0.14260245859622955\n",
      "Epoch 4420, Loss: 0.9129478335380554, Final Batch Loss: 0.28274914622306824\n",
      "Epoch 4421, Loss: 0.8349903523921967, Final Batch Loss: 0.2615601122379303\n",
      "Epoch 4422, Loss: 0.9041682630777359, Final Batch Loss: 0.3382713794708252\n",
      "Epoch 4423, Loss: 0.9684793800115585, Final Batch Loss: 0.2204129695892334\n",
      "Epoch 4424, Loss: 0.8458654433488846, Final Batch Loss: 0.26698145270347595\n",
      "Epoch 4425, Loss: 0.8734809458255768, Final Batch Loss: 0.18556886911392212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4426, Loss: 0.9620684832334518, Final Batch Loss: 0.2471473067998886\n",
      "Epoch 4427, Loss: 0.9410486221313477, Final Batch Loss: 0.25440528988838196\n",
      "Epoch 4428, Loss: 0.7407260835170746, Final Batch Loss: 0.14285726845264435\n",
      "Epoch 4429, Loss: 0.903497502207756, Final Batch Loss: 0.28769853711128235\n",
      "Epoch 4430, Loss: 0.8240361213684082, Final Batch Loss: 0.19461219012737274\n",
      "Epoch 4431, Loss: 0.9615655392408371, Final Batch Loss: 0.35478460788726807\n",
      "Epoch 4432, Loss: 0.7871924340724945, Final Batch Loss: 0.20633380115032196\n",
      "Epoch 4433, Loss: 0.7870852053165436, Final Batch Loss: 0.21472673118114471\n",
      "Epoch 4434, Loss: 0.7456847280263901, Final Batch Loss: 0.1385171264410019\n",
      "Epoch 4435, Loss: 0.8564528971910477, Final Batch Loss: 0.22140009701251984\n",
      "Epoch 4436, Loss: 0.634503461420536, Final Batch Loss: 0.10277498513460159\n",
      "Epoch 4437, Loss: 0.7155404686927795, Final Batch Loss: 0.17178483307361603\n",
      "Epoch 4438, Loss: 0.7577420771121979, Final Batch Loss: 0.21721398830413818\n",
      "Epoch 4439, Loss: 0.7821209728717804, Final Batch Loss: 0.1528639793395996\n",
      "Epoch 4440, Loss: 0.8759862035512924, Final Batch Loss: 0.15468326210975647\n",
      "Epoch 4441, Loss: 0.7629882097244263, Final Batch Loss: 0.22571659088134766\n",
      "Epoch 4442, Loss: 0.8114193677902222, Final Batch Loss: 0.24583981931209564\n",
      "Epoch 4443, Loss: 0.6823040246963501, Final Batch Loss: 0.1389693170785904\n",
      "Epoch 4444, Loss: 0.7285095304250717, Final Batch Loss: 0.14504651725292206\n",
      "Epoch 4445, Loss: 0.5698466524481773, Final Batch Loss: 0.08866032212972641\n",
      "Epoch 4446, Loss: 0.9977005422115326, Final Batch Loss: 0.39609161019325256\n",
      "Epoch 4447, Loss: 0.731861025094986, Final Batch Loss: 0.14803306758403778\n",
      "Epoch 4448, Loss: 0.9572958946228027, Final Batch Loss: 0.3584948480129242\n",
      "Epoch 4449, Loss: 1.287764385342598, Final Batch Loss: 0.6496248841285706\n",
      "Epoch 4450, Loss: 0.893793523311615, Final Batch Loss: 0.1821521520614624\n",
      "Epoch 4451, Loss: 0.6848400421440601, Final Batch Loss: 0.06061895564198494\n",
      "Epoch 4452, Loss: 0.7724990695714951, Final Batch Loss: 0.13723422586917877\n",
      "Epoch 4453, Loss: 0.721779465675354, Final Batch Loss: 0.11633113026618958\n",
      "Epoch 4454, Loss: 0.9183847010135651, Final Batch Loss: 0.3053950369358063\n",
      "Epoch 4455, Loss: 0.7094414159655571, Final Batch Loss: 0.08212987333536148\n",
      "Epoch 4456, Loss: 0.8572549074888229, Final Batch Loss: 0.23081858456134796\n",
      "Epoch 4457, Loss: 0.624249167740345, Final Batch Loss: 0.07516556233167648\n",
      "Epoch 4458, Loss: 0.7720177322626114, Final Batch Loss: 0.12866176664829254\n",
      "Epoch 4459, Loss: 0.8317426592111588, Final Batch Loss: 0.25788164138793945\n",
      "Epoch 4460, Loss: 0.8383461385965347, Final Batch Loss: 0.16467204689979553\n",
      "Epoch 4461, Loss: 0.8359935283660889, Final Batch Loss: 0.18434739112854004\n",
      "Epoch 4462, Loss: 0.7904218137264252, Final Batch Loss: 0.16330133378505707\n",
      "Epoch 4463, Loss: 0.8564690798521042, Final Batch Loss: 0.20569683611392975\n",
      "Epoch 4464, Loss: 0.8986422270536423, Final Batch Loss: 0.15518154203891754\n",
      "Epoch 4465, Loss: 0.7600579708814621, Final Batch Loss: 0.04198509454727173\n",
      "Epoch 4466, Loss: 0.8815126568078995, Final Batch Loss: 0.30569228529930115\n",
      "Epoch 4467, Loss: 0.7892652302980423, Final Batch Loss: 0.21615125238895416\n",
      "Epoch 4468, Loss: 0.9555964320898056, Final Batch Loss: 0.2829534709453583\n",
      "Epoch 4469, Loss: 0.834649845957756, Final Batch Loss: 0.16777221858501434\n",
      "Epoch 4470, Loss: 1.0056383907794952, Final Batch Loss: 0.19663089513778687\n",
      "Epoch 4471, Loss: 0.8400442451238632, Final Batch Loss: 0.17120866477489471\n",
      "Epoch 4472, Loss: 0.9215433597564697, Final Batch Loss: 0.27722668647766113\n",
      "Epoch 4473, Loss: 0.8078154474496841, Final Batch Loss: 0.18462572991847992\n",
      "Epoch 4474, Loss: 0.9380102753639221, Final Batch Loss: 0.17238812148571014\n",
      "Epoch 4475, Loss: 0.783233605325222, Final Batch Loss: 0.07124287635087967\n",
      "Epoch 4476, Loss: 0.7221757471561432, Final Batch Loss: 0.11876067519187927\n",
      "Epoch 4477, Loss: 0.816776379942894, Final Batch Loss: 0.13112710416316986\n",
      "Epoch 4478, Loss: 0.8532949537038803, Final Batch Loss: 0.2600363790988922\n",
      "Epoch 4479, Loss: 0.8891019225120544, Final Batch Loss: 0.2826331555843353\n",
      "Epoch 4480, Loss: 0.7377417087554932, Final Batch Loss: 0.12474796175956726\n",
      "Epoch 4481, Loss: 0.770884320139885, Final Batch Loss: 0.19506452977657318\n",
      "Epoch 4482, Loss: 0.8217264413833618, Final Batch Loss: 0.14090220630168915\n",
      "Epoch 4483, Loss: 0.7356524392962456, Final Batch Loss: 0.09078908711671829\n",
      "Epoch 4484, Loss: 0.7728420495986938, Final Batch Loss: 0.152760311961174\n",
      "Epoch 4485, Loss: 0.7533132135868073, Final Batch Loss: 0.1290162354707718\n",
      "Epoch 4486, Loss: 0.8744726330041885, Final Batch Loss: 0.30303749442100525\n",
      "Epoch 4487, Loss: 0.8848845660686493, Final Batch Loss: 0.24014174938201904\n",
      "Epoch 4488, Loss: 0.7168007791042328, Final Batch Loss: 0.1206243634223938\n",
      "Epoch 4489, Loss: 0.8900386542081833, Final Batch Loss: 0.23236392438411713\n",
      "Epoch 4490, Loss: 0.9807114601135254, Final Batch Loss: 0.14548984169960022\n",
      "Epoch 4491, Loss: 0.707504466176033, Final Batch Loss: 0.14962632954120636\n",
      "Epoch 4492, Loss: 0.9294848293066025, Final Batch Loss: 0.24307577311992645\n",
      "Epoch 4493, Loss: 0.650689385831356, Final Batch Loss: 0.11488649994134903\n",
      "Epoch 4494, Loss: 0.7315742671489716, Final Batch Loss: 0.19251300394535065\n",
      "Epoch 4495, Loss: 0.7751546502113342, Final Batch Loss: 0.13387030363082886\n",
      "Epoch 4496, Loss: 0.8573504984378815, Final Batch Loss: 0.2414635568857193\n",
      "Epoch 4497, Loss: 0.8903560489416122, Final Batch Loss: 0.1450488567352295\n",
      "Epoch 4498, Loss: 0.8339585065841675, Final Batch Loss: 0.3666746914386749\n",
      "Epoch 4499, Loss: 0.8419702351093292, Final Batch Loss: 0.22323910892009735\n",
      "Epoch 4500, Loss: 0.6740755699574947, Final Batch Loss: 0.0441041998565197\n",
      "Epoch 4501, Loss: 0.8569500893354416, Final Batch Loss: 0.21353775262832642\n",
      "Epoch 4502, Loss: 0.9498630613088608, Final Batch Loss: 0.1833815723657608\n",
      "Epoch 4503, Loss: 0.6817862838506699, Final Batch Loss: 0.13583384454250336\n",
      "Epoch 4504, Loss: 0.8186299800872803, Final Batch Loss: 0.14689458906650543\n",
      "Epoch 4505, Loss: 1.0438809096813202, Final Batch Loss: 0.40609657764434814\n",
      "Epoch 4506, Loss: 0.8668453991413116, Final Batch Loss: 0.20625782012939453\n",
      "Epoch 4507, Loss: 1.0032964199781418, Final Batch Loss: 0.2876558005809784\n",
      "Epoch 4508, Loss: 1.062385082244873, Final Batch Loss: 0.3933694660663605\n",
      "Epoch 4509, Loss: 0.7880885004997253, Final Batch Loss: 0.16032715141773224\n",
      "Epoch 4510, Loss: 0.9107886999845505, Final Batch Loss: 0.23621083796024323\n",
      "Epoch 4511, Loss: 1.001643344759941, Final Batch Loss: 0.33312955498695374\n",
      "Epoch 4512, Loss: 0.9011569172143936, Final Batch Loss: 0.26346662640571594\n",
      "Epoch 4513, Loss: 0.7865179479122162, Final Batch Loss: 0.09393399953842163\n",
      "Epoch 4514, Loss: 1.0710117518901825, Final Batch Loss: 0.28343722224235535\n",
      "Epoch 4515, Loss: 0.8128307461738586, Final Batch Loss: 0.14317439496517181\n",
      "Epoch 4516, Loss: 0.8816605061292648, Final Batch Loss: 0.20619012415409088\n",
      "Epoch 4517, Loss: 0.920857310295105, Final Batch Loss: 0.2225864976644516\n",
      "Epoch 4518, Loss: 1.094936266541481, Final Batch Loss: 0.2891257107257843\n",
      "Epoch 4519, Loss: 0.7115163803100586, Final Batch Loss: 0.22142398357391357\n",
      "Epoch 4520, Loss: 0.8274387717247009, Final Batch Loss: 0.18426847457885742\n",
      "Epoch 4521, Loss: 0.757234126329422, Final Batch Loss: 0.11612871289253235\n",
      "Epoch 4522, Loss: 0.7474451810121536, Final Batch Loss: 0.22780932486057281\n",
      "Epoch 4523, Loss: 0.7133226692676544, Final Batch Loss: 0.10290330648422241\n",
      "Epoch 4524, Loss: 0.9173949956893921, Final Batch Loss: 0.28440919518470764\n",
      "Epoch 4525, Loss: 0.6338285729289055, Final Batch Loss: 0.0804254487156868\n",
      "Epoch 4526, Loss: 0.8148779422044754, Final Batch Loss: 0.22780482470989227\n",
      "Epoch 4527, Loss: 0.7046196758747101, Final Batch Loss: 0.07872515916824341\n",
      "Epoch 4528, Loss: 0.6586681604385376, Final Batch Loss: 0.10661035776138306\n",
      "Epoch 4529, Loss: 0.7669467777013779, Final Batch Loss: 0.233847975730896\n",
      "Epoch 4530, Loss: 0.7987082600593567, Final Batch Loss: 0.22080476582050323\n",
      "Epoch 4531, Loss: 0.7388213574886322, Final Batch Loss: 0.16681812703609467\n",
      "Epoch 4532, Loss: 0.8012664914131165, Final Batch Loss: 0.16902323067188263\n",
      "Epoch 4533, Loss: 0.7855115532875061, Final Batch Loss: 0.20586638152599335\n",
      "Epoch 4534, Loss: 0.7825647443532944, Final Batch Loss: 0.1872968077659607\n",
      "Epoch 4535, Loss: 0.9578592777252197, Final Batch Loss: 0.21839334070682526\n",
      "Epoch 4536, Loss: 0.8449574261903763, Final Batch Loss: 0.25661078095436096\n",
      "Epoch 4537, Loss: 0.814748153090477, Final Batch Loss: 0.14057309925556183\n",
      "Epoch 4538, Loss: 0.745832085609436, Final Batch Loss: 0.1823371797800064\n",
      "Epoch 4539, Loss: 0.7332075089216232, Final Batch Loss: 0.14409220218658447\n",
      "Epoch 4540, Loss: 0.7976791188120842, Final Batch Loss: 0.11878500133752823\n",
      "Epoch 4541, Loss: 0.8313028961420059, Final Batch Loss: 0.1933753937482834\n",
      "Epoch 4542, Loss: 0.8638508319854736, Final Batch Loss: 0.28333204984664917\n",
      "Epoch 4543, Loss: 0.8971084654331207, Final Batch Loss: 0.2866269052028656\n",
      "Epoch 4544, Loss: 0.7656624764204025, Final Batch Loss: 0.1404234766960144\n",
      "Epoch 4545, Loss: 1.0727535337209702, Final Batch Loss: 0.35555222630500793\n",
      "Epoch 4546, Loss: 0.8285481035709381, Final Batch Loss: 0.167313352227211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4547, Loss: 0.9278420060873032, Final Batch Loss: 0.28681039810180664\n",
      "Epoch 4548, Loss: 0.8683157712221146, Final Batch Loss: 0.19206446409225464\n",
      "Epoch 4549, Loss: 0.7586843371391296, Final Batch Loss: 0.1664595603942871\n",
      "Epoch 4550, Loss: 0.8095604926347733, Final Batch Loss: 0.20335541665554047\n",
      "Epoch 4551, Loss: 0.8258681297302246, Final Batch Loss: 0.11206269264221191\n",
      "Epoch 4552, Loss: 0.7433397024869919, Final Batch Loss: 0.15964064002037048\n",
      "Epoch 4553, Loss: 0.9593190401792526, Final Batch Loss: 0.4283403158187866\n",
      "Epoch 4554, Loss: 0.7400994151830673, Final Batch Loss: 0.1718275099992752\n",
      "Epoch 4555, Loss: 0.7036284804344177, Final Batch Loss: 0.18602250516414642\n",
      "Epoch 4556, Loss: 0.8175749778747559, Final Batch Loss: 0.20449453592300415\n",
      "Epoch 4557, Loss: 0.7753826081752777, Final Batch Loss: 0.141946479678154\n",
      "Epoch 4558, Loss: 0.7776127904653549, Final Batch Loss: 0.23496238887310028\n",
      "Epoch 4559, Loss: 0.8174017667770386, Final Batch Loss: 0.1934414505958557\n",
      "Epoch 4560, Loss: 0.9176942408084869, Final Batch Loss: 0.20987915992736816\n",
      "Epoch 4561, Loss: 0.8408057242631912, Final Batch Loss: 0.15866954624652863\n",
      "Epoch 4562, Loss: 0.8708856254816055, Final Batch Loss: 0.23462337255477905\n",
      "Epoch 4563, Loss: 0.8494914174079895, Final Batch Loss: 0.18679232895374298\n",
      "Epoch 4564, Loss: 1.0572847425937653, Final Batch Loss: 0.33417806029319763\n",
      "Epoch 4565, Loss: 0.9802112281322479, Final Batch Loss: 0.34806767106056213\n",
      "Epoch 4566, Loss: 0.871771514415741, Final Batch Loss: 0.3095519542694092\n",
      "Epoch 4567, Loss: 0.657474409788847, Final Batch Loss: 0.05843135342001915\n",
      "Epoch 4568, Loss: 0.7790013551712036, Final Batch Loss: 0.25575003027915955\n",
      "Epoch 4569, Loss: 0.9426988065242767, Final Batch Loss: 0.12514270842075348\n",
      "Epoch 4570, Loss: 0.7520485818386078, Final Batch Loss: 0.1009795069694519\n",
      "Epoch 4571, Loss: 0.7511890903115273, Final Batch Loss: 0.11865990608930588\n",
      "Epoch 4572, Loss: 0.855891078710556, Final Batch Loss: 0.31006884574890137\n",
      "Epoch 4573, Loss: 0.9077829420566559, Final Batch Loss: 0.27745839953422546\n",
      "Epoch 4574, Loss: 0.8132471591234207, Final Batch Loss: 0.2064085602760315\n",
      "Epoch 4575, Loss: 0.7542370408773422, Final Batch Loss: 0.1578417867422104\n",
      "Epoch 4576, Loss: 0.7861553579568863, Final Batch Loss: 0.20457126200199127\n",
      "Epoch 4577, Loss: 0.8477076441049576, Final Batch Loss: 0.24036677181720734\n",
      "Epoch 4578, Loss: 0.9036864340305328, Final Batch Loss: 0.25348660349845886\n",
      "Epoch 4579, Loss: 0.9721424281597137, Final Batch Loss: 0.2974186837673187\n",
      "Epoch 4580, Loss: 0.9559123367071152, Final Batch Loss: 0.2900543212890625\n",
      "Epoch 4581, Loss: 0.973042905330658, Final Batch Loss: 0.23179490864276886\n",
      "Epoch 4582, Loss: 0.793551117181778, Final Batch Loss: 0.21461229026317596\n",
      "Epoch 4583, Loss: 0.9040548950433731, Final Batch Loss: 0.25947555899620056\n",
      "Epoch 4584, Loss: 0.7525307089090347, Final Batch Loss: 0.16267576813697815\n",
      "Epoch 4585, Loss: 0.7139943689107895, Final Batch Loss: 0.16200536489486694\n",
      "Epoch 4586, Loss: 0.8894679397344589, Final Batch Loss: 0.18784888088703156\n",
      "Epoch 4587, Loss: 0.7237377464771271, Final Batch Loss: 0.17645782232284546\n",
      "Epoch 4588, Loss: 0.7306348234415054, Final Batch Loss: 0.11238124966621399\n",
      "Epoch 4589, Loss: 0.8967188596725464, Final Batch Loss: 0.27948057651519775\n",
      "Epoch 4590, Loss: 0.9996048957109451, Final Batch Loss: 0.3570493459701538\n",
      "Epoch 4591, Loss: 0.7469414472579956, Final Batch Loss: 0.25689953565597534\n",
      "Epoch 4592, Loss: 0.7021466083824635, Final Batch Loss: 0.05501203611493111\n",
      "Epoch 4593, Loss: 0.9319157004356384, Final Batch Loss: 0.29584428668022156\n",
      "Epoch 4594, Loss: 0.7834384143352509, Final Batch Loss: 0.18973805010318756\n",
      "Epoch 4595, Loss: 0.9909921735525131, Final Batch Loss: 0.3422296345233917\n",
      "Epoch 4596, Loss: 0.9811086654663086, Final Batch Loss: 0.3455740511417389\n",
      "Epoch 4597, Loss: 0.8850576728582382, Final Batch Loss: 0.27264299988746643\n",
      "Epoch 4598, Loss: 0.7863454073667526, Final Batch Loss: 0.2224709391593933\n",
      "Epoch 4599, Loss: 0.8031385242938995, Final Batch Loss: 0.1913122534751892\n",
      "Epoch 4600, Loss: 0.7281918972730637, Final Batch Loss: 0.15224754810333252\n",
      "Epoch 4601, Loss: 0.727340079843998, Final Batch Loss: 0.10154782980680466\n",
      "Epoch 4602, Loss: 0.774470180273056, Final Batch Loss: 0.2029743641614914\n",
      "Epoch 4603, Loss: 0.8402467221021652, Final Batch Loss: 0.18007464706897736\n",
      "Epoch 4604, Loss: 0.7932799011468887, Final Batch Loss: 0.22103559970855713\n",
      "Epoch 4605, Loss: 0.8051006346940994, Final Batch Loss: 0.2588671147823334\n",
      "Epoch 4606, Loss: 1.0060914754867554, Final Batch Loss: 0.297137588262558\n",
      "Epoch 4607, Loss: 0.7899082154035568, Final Batch Loss: 0.1492435783147812\n",
      "Epoch 4608, Loss: 0.7661964446306229, Final Batch Loss: 0.20366127789020538\n",
      "Epoch 4609, Loss: 0.741992399096489, Final Batch Loss: 0.2587338984012604\n",
      "Epoch 4610, Loss: 0.7607513666152954, Final Batch Loss: 0.11761733889579773\n",
      "Epoch 4611, Loss: 0.6393401846289635, Final Batch Loss: 0.09158620983362198\n",
      "Epoch 4612, Loss: 0.8143000155687332, Final Batch Loss: 0.1620861440896988\n",
      "Epoch 4613, Loss: 0.8666402399539948, Final Batch Loss: 0.15973731875419617\n",
      "Epoch 4614, Loss: 0.9162661135196686, Final Batch Loss: 0.21694596111774445\n",
      "Epoch 4615, Loss: 0.8714367300271988, Final Batch Loss: 0.20333866775035858\n",
      "Epoch 4616, Loss: 0.830113485455513, Final Batch Loss: 0.1875135749578476\n",
      "Epoch 4617, Loss: 0.7657060772180557, Final Batch Loss: 0.13504640758037567\n",
      "Epoch 4618, Loss: 0.7022354602813721, Final Batch Loss: 0.11993762850761414\n",
      "Epoch 4619, Loss: 0.8037453070282936, Final Batch Loss: 0.11913792043924332\n",
      "Epoch 4620, Loss: 0.7776054292917252, Final Batch Loss: 0.13205604255199432\n",
      "Epoch 4621, Loss: 0.9498373717069626, Final Batch Loss: 0.27867141366004944\n",
      "Epoch 4622, Loss: 0.8054203689098358, Final Batch Loss: 0.17960548400878906\n",
      "Epoch 4623, Loss: 0.8615261763334274, Final Batch Loss: 0.22633777558803558\n",
      "Epoch 4624, Loss: 0.7470079362392426, Final Batch Loss: 0.19985711574554443\n",
      "Epoch 4625, Loss: 0.8012551963329315, Final Batch Loss: 0.19509673118591309\n",
      "Epoch 4626, Loss: 0.8037110567092896, Final Batch Loss: 0.17956824600696564\n",
      "Epoch 4627, Loss: 0.8815598487854004, Final Batch Loss: 0.2853485941886902\n",
      "Epoch 4628, Loss: 0.9151254296302795, Final Batch Loss: 0.27286919951438904\n",
      "Epoch 4629, Loss: 0.8222567439079285, Final Batch Loss: 0.14991529285907745\n",
      "Epoch 4630, Loss: 0.8164307177066803, Final Batch Loss: 0.18010355532169342\n",
      "Epoch 4631, Loss: 0.9362097978591919, Final Batch Loss: 0.3180435299873352\n",
      "Epoch 4632, Loss: 0.9204186499118805, Final Batch Loss: 0.37766197323799133\n",
      "Epoch 4633, Loss: 0.8497699052095413, Final Batch Loss: 0.16984663903713226\n",
      "Epoch 4634, Loss: 0.9520786851644516, Final Batch Loss: 0.35398074984550476\n",
      "Epoch 4635, Loss: 1.106949046254158, Final Batch Loss: 0.40813758969306946\n",
      "Epoch 4636, Loss: 0.895091637969017, Final Batch Loss: 0.22482185065746307\n",
      "Epoch 4637, Loss: 0.7593314498662949, Final Batch Loss: 0.1770728975534439\n",
      "Epoch 4638, Loss: 0.7842835113406181, Final Batch Loss: 0.10238804668188095\n",
      "Epoch 4639, Loss: 0.8417900949716568, Final Batch Loss: 0.2582961320877075\n",
      "Epoch 4640, Loss: 0.8821833953261375, Final Batch Loss: 0.37109431624412537\n",
      "Epoch 4641, Loss: 0.7318993359804153, Final Batch Loss: 0.17475372552871704\n",
      "Epoch 4642, Loss: 0.7851650416851044, Final Batch Loss: 0.2749722898006439\n",
      "Epoch 4643, Loss: 0.7795778661966324, Final Batch Loss: 0.16570161283016205\n",
      "Epoch 4644, Loss: 0.7094838619232178, Final Batch Loss: 0.18477988243103027\n",
      "Epoch 4645, Loss: 0.7783573120832443, Final Batch Loss: 0.20710217952728271\n",
      "Epoch 4646, Loss: 0.7981460094451904, Final Batch Loss: 0.20215962827205658\n",
      "Epoch 4647, Loss: 0.8539239764213562, Final Batch Loss: 0.2071530967950821\n",
      "Epoch 4648, Loss: 0.7435553893446922, Final Batch Loss: 0.0970146432518959\n",
      "Epoch 4649, Loss: 0.8128289133310318, Final Batch Loss: 0.14385561645030975\n",
      "Epoch 4650, Loss: 0.7024675756692886, Final Batch Loss: 0.1700269728899002\n",
      "Epoch 4651, Loss: 0.7059966027736664, Final Batch Loss: 0.13071763515472412\n",
      "Epoch 4652, Loss: 0.7857367247343063, Final Batch Loss: 0.16764457523822784\n",
      "Epoch 4653, Loss: 0.8303952813148499, Final Batch Loss: 0.12944850325584412\n",
      "Epoch 4654, Loss: 0.7456670552492142, Final Batch Loss: 0.1641698181629181\n",
      "Epoch 4655, Loss: 0.789423868060112, Final Batch Loss: 0.2700967788696289\n",
      "Epoch 4656, Loss: 0.701483428478241, Final Batch Loss: 0.13085566461086273\n",
      "Epoch 4657, Loss: 0.7120070606470108, Final Batch Loss: 0.15204325318336487\n",
      "Epoch 4658, Loss: 0.9318612068891525, Final Batch Loss: 0.3750559985637665\n",
      "Epoch 4659, Loss: 0.7743036299943924, Final Batch Loss: 0.1899908185005188\n",
      "Epoch 4660, Loss: 0.7393466010689735, Final Batch Loss: 0.11615989357233047\n",
      "Epoch 4661, Loss: 0.8131009340286255, Final Batch Loss: 0.23903203010559082\n",
      "Epoch 4662, Loss: 0.7928613126277924, Final Batch Loss: 0.13089974224567413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4663, Loss: 0.8158709704875946, Final Batch Loss: 0.15093664824962616\n",
      "Epoch 4664, Loss: 1.1039368212223053, Final Batch Loss: 0.47122666239738464\n",
      "Epoch 4665, Loss: 0.9463176280260086, Final Batch Loss: 0.33167874813079834\n",
      "Epoch 4666, Loss: 0.8302830159664154, Final Batch Loss: 0.23506426811218262\n",
      "Epoch 4667, Loss: 0.9063878804445267, Final Batch Loss: 0.19510607421398163\n",
      "Epoch 4668, Loss: 0.7728242874145508, Final Batch Loss: 0.20582245290279388\n",
      "Epoch 4669, Loss: 0.9781032055616379, Final Batch Loss: 0.4282020628452301\n",
      "Epoch 4670, Loss: 0.7929625362157822, Final Batch Loss: 0.13468517363071442\n",
      "Epoch 4671, Loss: 0.9942068606615067, Final Batch Loss: 0.3498859405517578\n",
      "Epoch 4672, Loss: 0.7878792136907578, Final Batch Loss: 0.1619068682193756\n",
      "Epoch 4673, Loss: 0.8379660695791245, Final Batch Loss: 0.19075500965118408\n",
      "Epoch 4674, Loss: 0.8967916071414948, Final Batch Loss: 0.13555322587490082\n",
      "Epoch 4675, Loss: 0.8917300701141357, Final Batch Loss: 0.19390805065631866\n",
      "Epoch 4676, Loss: 0.7377679068595171, Final Batch Loss: 0.02606724388897419\n",
      "Epoch 4677, Loss: 0.7938196361064911, Final Batch Loss: 0.13724826276302338\n",
      "Epoch 4678, Loss: 0.6828116476535797, Final Batch Loss: 0.1506795883178711\n",
      "Epoch 4679, Loss: 0.7694374620914459, Final Batch Loss: 0.17365913093090057\n",
      "Epoch 4680, Loss: 0.8386297076940536, Final Batch Loss: 0.1618499606847763\n",
      "Epoch 4681, Loss: 0.804306223988533, Final Batch Loss: 0.1459258496761322\n",
      "Epoch 4682, Loss: 0.855048730969429, Final Batch Loss: 0.26364415884017944\n",
      "Epoch 4683, Loss: 0.7470907866954803, Final Batch Loss: 0.153656467795372\n",
      "Epoch 4684, Loss: 0.9198384135961533, Final Batch Loss: 0.32081127166748047\n",
      "Epoch 4685, Loss: 0.6632427349686623, Final Batch Loss: 0.10754730552434921\n",
      "Epoch 4686, Loss: 0.8558852076530457, Final Batch Loss: 0.2348172664642334\n",
      "Epoch 4687, Loss: 0.8986814618110657, Final Batch Loss: 0.26227840781211853\n",
      "Epoch 4688, Loss: 1.058063805103302, Final Batch Loss: 0.4356495141983032\n",
      "Epoch 4689, Loss: 0.8613612800836563, Final Batch Loss: 0.17006491124629974\n",
      "Epoch 4690, Loss: 0.718068428337574, Final Batch Loss: 0.09877195209264755\n",
      "Epoch 4691, Loss: 0.8541280180215836, Final Batch Loss: 0.23766262829303741\n",
      "Epoch 4692, Loss: 0.907950758934021, Final Batch Loss: 0.16871805489063263\n",
      "Epoch 4693, Loss: 0.7931675463914871, Final Batch Loss: 0.14216965436935425\n",
      "Epoch 4694, Loss: 0.8157056346535683, Final Batch Loss: 0.08451666682958603\n",
      "Epoch 4695, Loss: 0.7933503538370132, Final Batch Loss: 0.22859561443328857\n",
      "Epoch 4696, Loss: 0.76385198533535, Final Batch Loss: 0.13663296401500702\n",
      "Epoch 4697, Loss: 1.2141082137823105, Final Batch Loss: 0.6812686324119568\n",
      "Epoch 4698, Loss: 0.6967912837862968, Final Batch Loss: 0.09748443216085434\n",
      "Epoch 4699, Loss: 0.780267745256424, Final Batch Loss: 0.17903806269168854\n",
      "Epoch 4700, Loss: 0.679820328950882, Final Batch Loss: 0.1577831655740738\n",
      "Epoch 4701, Loss: 0.709297351539135, Final Batch Loss: 0.103087417781353\n",
      "Epoch 4702, Loss: 0.8300563842058182, Final Batch Loss: 0.14899325370788574\n",
      "Epoch 4703, Loss: 0.8010004758834839, Final Batch Loss: 0.19708918035030365\n",
      "Epoch 4704, Loss: 0.932504266500473, Final Batch Loss: 0.30753031373023987\n",
      "Epoch 4705, Loss: 0.8710887730121613, Final Batch Loss: 0.17246073484420776\n",
      "Epoch 4706, Loss: 0.9107404351234436, Final Batch Loss: 0.3489888906478882\n",
      "Epoch 4707, Loss: 0.8596436381340027, Final Batch Loss: 0.2774314880371094\n",
      "Epoch 4708, Loss: 0.7973185330629349, Final Batch Loss: 0.23458033800125122\n",
      "Epoch 4709, Loss: 0.7237828820943832, Final Batch Loss: 0.15043672919273376\n",
      "Epoch 4710, Loss: 0.7213264256715775, Final Batch Loss: 0.13113705813884735\n",
      "Epoch 4711, Loss: 0.8217625096440315, Final Batch Loss: 0.2929825186729431\n",
      "Epoch 4712, Loss: 0.9645728468894958, Final Batch Loss: 0.22627119719982147\n",
      "Epoch 4713, Loss: 0.718396283686161, Final Batch Loss: 0.11610541492700577\n",
      "Epoch 4714, Loss: 0.7691905349493027, Final Batch Loss: 0.1359606385231018\n",
      "Epoch 4715, Loss: 0.6876414865255356, Final Batch Loss: 0.16500477492809296\n",
      "Epoch 4716, Loss: 0.7652702927589417, Final Batch Loss: 0.2548666298389435\n",
      "Epoch 4717, Loss: 0.7162175476551056, Final Batch Loss: 0.1830662488937378\n",
      "Epoch 4718, Loss: 0.9469733238220215, Final Batch Loss: 0.29575687646865845\n",
      "Epoch 4719, Loss: 0.8368135094642639, Final Batch Loss: 0.221185103058815\n",
      "Epoch 4720, Loss: 0.9036712050437927, Final Batch Loss: 0.26968857645988464\n",
      "Epoch 4721, Loss: 0.8925093561410904, Final Batch Loss: 0.29712536931037903\n",
      "Epoch 4722, Loss: 0.8164013624191284, Final Batch Loss: 0.19734513759613037\n",
      "Epoch 4723, Loss: 0.7883184552192688, Final Batch Loss: 0.1277771294116974\n",
      "Epoch 4724, Loss: 0.603276938199997, Final Batch Loss: 0.131996750831604\n",
      "Epoch 4725, Loss: 0.8204257637262344, Final Batch Loss: 0.19877569377422333\n",
      "Epoch 4726, Loss: 0.7252090871334076, Final Batch Loss: 0.15068531036376953\n",
      "Epoch 4727, Loss: 0.8917089104652405, Final Batch Loss: 0.2665720582008362\n",
      "Epoch 4728, Loss: 0.8417909294366837, Final Batch Loss: 0.26514482498168945\n",
      "Epoch 4729, Loss: 0.7362913116812706, Final Batch Loss: 0.1137208566069603\n",
      "Epoch 4730, Loss: 0.7718785107135773, Final Batch Loss: 0.15452395379543304\n",
      "Epoch 4731, Loss: 0.8381875157356262, Final Batch Loss: 0.28478291630744934\n",
      "Epoch 4732, Loss: 0.6872512102127075, Final Batch Loss: 0.1449560672044754\n",
      "Epoch 4733, Loss: 0.8061339408159256, Final Batch Loss: 0.27932024002075195\n",
      "Epoch 4734, Loss: 0.7051769122481346, Final Batch Loss: 0.08519167453050613\n",
      "Epoch 4735, Loss: 0.7678524851799011, Final Batch Loss: 0.20695972442626953\n",
      "Epoch 4736, Loss: 0.7632536441087723, Final Batch Loss: 0.13927333056926727\n",
      "Epoch 4737, Loss: 0.7529351860284805, Final Batch Loss: 0.15360558032989502\n",
      "Epoch 4738, Loss: 0.8430042862892151, Final Batch Loss: 0.1902785301208496\n",
      "Epoch 4739, Loss: 0.8411732986569405, Final Batch Loss: 0.1152845099568367\n",
      "Epoch 4740, Loss: 0.7336561530828476, Final Batch Loss: 0.12419527769088745\n",
      "Epoch 4741, Loss: 0.6514963135123253, Final Batch Loss: 0.09517381340265274\n",
      "Epoch 4742, Loss: 0.9910268783569336, Final Batch Loss: 0.3086976110935211\n",
      "Epoch 4743, Loss: 0.7647048532962799, Final Batch Loss: 0.11997845768928528\n",
      "Epoch 4744, Loss: 0.8469303101301193, Final Batch Loss: 0.20737898349761963\n",
      "Epoch 4745, Loss: 0.8788284733891487, Final Batch Loss: 0.12419969588518143\n",
      "Epoch 4746, Loss: 0.8549579828977585, Final Batch Loss: 0.2067784070968628\n",
      "Epoch 4747, Loss: 0.8388939946889877, Final Batch Loss: 0.22938789427280426\n",
      "Epoch 4748, Loss: 0.8118936568498611, Final Batch Loss: 0.22444181144237518\n",
      "Epoch 4749, Loss: 0.7974495887756348, Final Batch Loss: 0.17956028878688812\n",
      "Epoch 4750, Loss: 0.8800945729017258, Final Batch Loss: 0.27394208312034607\n",
      "Epoch 4751, Loss: 0.7437850050628185, Final Batch Loss: 0.03874638304114342\n",
      "Epoch 4752, Loss: 0.7769353240728378, Final Batch Loss: 0.15293379127979279\n",
      "Epoch 4753, Loss: 0.9732964038848877, Final Batch Loss: 0.26651397347450256\n",
      "Epoch 4754, Loss: 0.8805517852306366, Final Batch Loss: 0.2506698668003082\n",
      "Epoch 4755, Loss: 0.7175321206450462, Final Batch Loss: 0.09153025597333908\n",
      "Epoch 4756, Loss: 0.784655824303627, Final Batch Loss: 0.16375887393951416\n",
      "Epoch 4757, Loss: 0.6970135495066643, Final Batch Loss: 0.10748589783906937\n",
      "Epoch 4758, Loss: 0.8228272795677185, Final Batch Loss: 0.1889401227235794\n",
      "Epoch 4759, Loss: 0.859275296330452, Final Batch Loss: 0.22437722980976105\n",
      "Epoch 4760, Loss: 0.8405156582593918, Final Batch Loss: 0.24350030720233917\n",
      "Epoch 4761, Loss: 0.8619482144713402, Final Batch Loss: 0.2281786948442459\n",
      "Epoch 4762, Loss: 0.8268036395311356, Final Batch Loss: 0.1914328932762146\n",
      "Epoch 4763, Loss: 0.9340728670358658, Final Batch Loss: 0.26968222856521606\n",
      "Epoch 4764, Loss: 0.7775995880365372, Final Batch Loss: 0.18872982263565063\n",
      "Epoch 4765, Loss: 0.9661258608102798, Final Batch Loss: 0.395828515291214\n",
      "Epoch 4766, Loss: 0.8703423738479614, Final Batch Loss: 0.25535622239112854\n",
      "Epoch 4767, Loss: 0.9531041085720062, Final Batch Loss: 0.38917991518974304\n",
      "Epoch 4768, Loss: 0.8387651443481445, Final Batch Loss: 0.25181642174720764\n",
      "Epoch 4769, Loss: 0.9822420924901962, Final Batch Loss: 0.40792274475097656\n",
      "Epoch 4770, Loss: 0.7269596308469772, Final Batch Loss: 0.16563884913921356\n",
      "Epoch 4771, Loss: 0.7687608897686005, Final Batch Loss: 0.21858535706996918\n",
      "Epoch 4772, Loss: 0.7753056138753891, Final Batch Loss: 0.1545361429452896\n",
      "Epoch 4773, Loss: 0.8165048360824585, Final Batch Loss: 0.21383138000965118\n",
      "Epoch 4774, Loss: 0.7649260759353638, Final Batch Loss: 0.1771174818277359\n",
      "Epoch 4775, Loss: 0.9287834316492081, Final Batch Loss: 0.30250662565231323\n",
      "Epoch 4776, Loss: 0.776204451918602, Final Batch Loss: 0.10468131303787231\n",
      "Epoch 4777, Loss: 0.8203238546848297, Final Batch Loss: 0.1801462322473526\n",
      "Epoch 4778, Loss: 0.687917448580265, Final Batch Loss: 0.09171352535486221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4779, Loss: 0.7735482305288315, Final Batch Loss: 0.16474489867687225\n",
      "Epoch 4780, Loss: 0.6648713573813438, Final Batch Loss: 0.06973270326852798\n",
      "Epoch 4781, Loss: 0.8503506928682327, Final Batch Loss: 0.19400574266910553\n",
      "Epoch 4782, Loss: 0.8193717747926712, Final Batch Loss: 0.1942186802625656\n",
      "Epoch 4783, Loss: 0.9734979420900345, Final Batch Loss: 0.4100480377674103\n",
      "Epoch 4784, Loss: 0.9303697943687439, Final Batch Loss: 0.2892216444015503\n",
      "Epoch 4785, Loss: 1.1329977959394455, Final Batch Loss: 0.24658942222595215\n",
      "Epoch 4786, Loss: 0.9095617234706879, Final Batch Loss: 0.25820019841194153\n",
      "Epoch 4787, Loss: 0.9159674793481827, Final Batch Loss: 0.19490523636341095\n",
      "Epoch 4788, Loss: 0.9339387118816376, Final Batch Loss: 0.24029141664505005\n",
      "Epoch 4789, Loss: 0.9597325325012207, Final Batch Loss: 0.3371237516403198\n",
      "Epoch 4790, Loss: 0.8008184731006622, Final Batch Loss: 0.13430069386959076\n",
      "Epoch 4791, Loss: 0.6628636866807938, Final Batch Loss: 0.09770742058753967\n",
      "Epoch 4792, Loss: 0.7298355400562286, Final Batch Loss: 0.18862809240818024\n",
      "Epoch 4793, Loss: 0.8327027186751366, Final Batch Loss: 0.12414673715829849\n",
      "Epoch 4794, Loss: 0.7607886791229248, Final Batch Loss: 0.21960867941379547\n",
      "Epoch 4795, Loss: 0.843986377120018, Final Batch Loss: 0.27818018198013306\n",
      "Epoch 4796, Loss: 0.8585740327835083, Final Batch Loss: 0.23266254365444183\n",
      "Epoch 4797, Loss: 0.837066039443016, Final Batch Loss: 0.16154679656028748\n",
      "Epoch 4798, Loss: 0.8408112823963165, Final Batch Loss: 0.17604418098926544\n",
      "Epoch 4799, Loss: 0.9255332946777344, Final Batch Loss: 0.27942803502082825\n",
      "Epoch 4800, Loss: 0.8495807349681854, Final Batch Loss: 0.22443361580371857\n",
      "Epoch 4801, Loss: 0.7191960290074348, Final Batch Loss: 0.08538471907377243\n",
      "Epoch 4802, Loss: 0.6248685047030449, Final Batch Loss: 0.03430936485528946\n",
      "Epoch 4803, Loss: 0.7453484907746315, Final Batch Loss: 0.0798313245177269\n",
      "Epoch 4804, Loss: 0.785891018807888, Final Batch Loss: 0.11438403278589249\n",
      "Epoch 4805, Loss: 0.7776719331741333, Final Batch Loss: 0.1311771720647812\n",
      "Epoch 4806, Loss: 0.7916447818279266, Final Batch Loss: 0.24692225456237793\n",
      "Epoch 4807, Loss: 0.8327846825122833, Final Batch Loss: 0.22336316108703613\n",
      "Epoch 4808, Loss: 0.6929396092891693, Final Batch Loss: 0.16954255104064941\n",
      "Epoch 4809, Loss: 0.7506320625543594, Final Batch Loss: 0.1331196427345276\n",
      "Epoch 4810, Loss: 0.7292423695325851, Final Batch Loss: 0.16056117415428162\n",
      "Epoch 4811, Loss: 0.7672708630561829, Final Batch Loss: 0.1775726079940796\n",
      "Epoch 4812, Loss: 0.8198021650314331, Final Batch Loss: 0.210837259888649\n",
      "Epoch 4813, Loss: 0.7140795737504959, Final Batch Loss: 0.09411153197288513\n",
      "Epoch 4814, Loss: 0.9156914800405502, Final Batch Loss: 0.23310215771198273\n",
      "Epoch 4815, Loss: 0.7389870434999466, Final Batch Loss: 0.164810910820961\n",
      "Epoch 4816, Loss: 0.7426675111055374, Final Batch Loss: 0.16756309568881989\n",
      "Epoch 4817, Loss: 0.68220055103302, Final Batch Loss: 0.11754748225212097\n",
      "Epoch 4818, Loss: 0.7126386165618896, Final Batch Loss: 0.1488226056098938\n",
      "Epoch 4819, Loss: 0.915722131729126, Final Batch Loss: 0.3683374226093292\n",
      "Epoch 4820, Loss: 0.6642149463295937, Final Batch Loss: 0.09767531603574753\n",
      "Epoch 4821, Loss: 0.804359495639801, Final Batch Loss: 0.12044742703437805\n",
      "Epoch 4822, Loss: 0.8547281622886658, Final Batch Loss: 0.3132476508617401\n",
      "Epoch 4823, Loss: 0.9258884787559509, Final Batch Loss: 0.35443946719169617\n",
      "Epoch 4824, Loss: 0.6940689608454704, Final Batch Loss: 0.12051110714673996\n",
      "Epoch 4825, Loss: 0.7688004523515701, Final Batch Loss: 0.19765716791152954\n",
      "Epoch 4826, Loss: 0.8696359843015671, Final Batch Loss: 0.24347998201847076\n",
      "Epoch 4827, Loss: 0.8151163458824158, Final Batch Loss: 0.14858998358249664\n",
      "Epoch 4828, Loss: 0.8675735592842102, Final Batch Loss: 0.15850286185741425\n",
      "Epoch 4829, Loss: 0.9275153130292892, Final Batch Loss: 0.3465348184108734\n",
      "Epoch 4830, Loss: 0.7075846567749977, Final Batch Loss: 0.07463308423757553\n",
      "Epoch 4831, Loss: 0.8343898206949234, Final Batch Loss: 0.22625024616718292\n",
      "Epoch 4832, Loss: 0.8850215822458267, Final Batch Loss: 0.24009265005588531\n",
      "Epoch 4833, Loss: 0.950115293264389, Final Batch Loss: 0.24185480177402496\n",
      "Epoch 4834, Loss: 0.862950012087822, Final Batch Loss: 0.17165452241897583\n",
      "Epoch 4835, Loss: 1.1866147369146347, Final Batch Loss: 0.44722166657447815\n",
      "Epoch 4836, Loss: 1.1329523772001266, Final Batch Loss: 0.38695016503334045\n",
      "Epoch 4837, Loss: 1.0087419152259827, Final Batch Loss: 0.4130392074584961\n",
      "Epoch 4838, Loss: 0.816124677658081, Final Batch Loss: 0.19464673101902008\n",
      "Epoch 4839, Loss: 0.8726819604635239, Final Batch Loss: 0.24911372363567352\n",
      "Epoch 4840, Loss: 0.787693053483963, Final Batch Loss: 0.1761355996131897\n",
      "Epoch 4841, Loss: 0.6718226224184036, Final Batch Loss: 0.10186746716499329\n",
      "Epoch 4842, Loss: 0.8480518013238907, Final Batch Loss: 0.22256118059158325\n",
      "Epoch 4843, Loss: 0.8061152696609497, Final Batch Loss: 0.19614146649837494\n",
      "Epoch 4844, Loss: 0.7808261215686798, Final Batch Loss: 0.15191447734832764\n",
      "Epoch 4845, Loss: 0.7313503473997116, Final Batch Loss: 0.14317961037158966\n",
      "Epoch 4846, Loss: 0.7750163674354553, Final Batch Loss: 0.15459297597408295\n",
      "Epoch 4847, Loss: 0.7884670794010162, Final Batch Loss: 0.12667253613471985\n",
      "Epoch 4848, Loss: 0.7716009616851807, Final Batch Loss: 0.1401790976524353\n",
      "Epoch 4849, Loss: 0.759520873427391, Final Batch Loss: 0.1189156174659729\n",
      "Epoch 4850, Loss: 0.6922724395990372, Final Batch Loss: 0.12565678358078003\n",
      "Epoch 4851, Loss: 0.9203837662935257, Final Batch Loss: 0.2614971101284027\n",
      "Epoch 4852, Loss: 0.6984622925519943, Final Batch Loss: 0.15647761523723602\n",
      "Epoch 4853, Loss: 0.7587634325027466, Final Batch Loss: 0.1744396835565567\n",
      "Epoch 4854, Loss: 0.8014739900827408, Final Batch Loss: 0.2020743042230606\n",
      "Epoch 4855, Loss: 0.7158508077263832, Final Batch Loss: 0.20566321909427643\n",
      "Epoch 4856, Loss: 0.8330990821123123, Final Batch Loss: 0.19092373549938202\n",
      "Epoch 4857, Loss: 0.6699724793434143, Final Batch Loss: 0.12843510508537292\n",
      "Epoch 4858, Loss: 0.7744365036487579, Final Batch Loss: 0.13318359851837158\n",
      "Epoch 4859, Loss: 1.2377876192331314, Final Batch Loss: 0.5972921252250671\n",
      "Epoch 4860, Loss: 0.7843766063451767, Final Batch Loss: 0.1626526117324829\n",
      "Epoch 4861, Loss: 0.788055419921875, Final Batch Loss: 0.2133617252111435\n",
      "Epoch 4862, Loss: 0.8769755810499191, Final Batch Loss: 0.3041623532772064\n",
      "Epoch 4863, Loss: 0.8314663916826248, Final Batch Loss: 0.22160543501377106\n",
      "Epoch 4864, Loss: 0.7643016874790192, Final Batch Loss: 0.19015668332576752\n",
      "Epoch 4865, Loss: 0.7633151561021805, Final Batch Loss: 0.21696482598781586\n",
      "Epoch 4866, Loss: 0.7152664363384247, Final Batch Loss: 0.17452256381511688\n",
      "Epoch 4867, Loss: 0.8021426796913147, Final Batch Loss: 0.2618599832057953\n",
      "Epoch 4868, Loss: 0.7606183364987373, Final Batch Loss: 0.11862262338399887\n",
      "Epoch 4869, Loss: 0.7901155352592468, Final Batch Loss: 0.1288062036037445\n",
      "Epoch 4870, Loss: 0.7251920476555824, Final Batch Loss: 0.08643364161252975\n",
      "Epoch 4871, Loss: 0.6729872860014439, Final Batch Loss: 0.059772852808237076\n",
      "Epoch 4872, Loss: 0.8356764763593674, Final Batch Loss: 0.24551858007907867\n",
      "Epoch 4873, Loss: 0.7837536185979843, Final Batch Loss: 0.23450537025928497\n",
      "Epoch 4874, Loss: 0.7837428450584412, Final Batch Loss: 0.16609720885753632\n",
      "Epoch 4875, Loss: 0.690514400601387, Final Batch Loss: 0.12908729910850525\n",
      "Epoch 4876, Loss: 0.6855496019124985, Final Batch Loss: 0.1906241923570633\n",
      "Epoch 4877, Loss: 0.9585859477519989, Final Batch Loss: 0.2512877881526947\n",
      "Epoch 4878, Loss: 0.9428986310958862, Final Batch Loss: 0.2892071008682251\n",
      "Epoch 4879, Loss: 0.6593289002776146, Final Batch Loss: 0.1128116026520729\n",
      "Epoch 4880, Loss: 0.7047319859266281, Final Batch Loss: 0.14338567852973938\n",
      "Epoch 4881, Loss: 0.8059514313936234, Final Batch Loss: 0.17550057172775269\n",
      "Epoch 4882, Loss: 0.8354735672473907, Final Batch Loss: 0.23099900782108307\n",
      "Epoch 4883, Loss: 0.7754251509904861, Final Batch Loss: 0.23147960007190704\n",
      "Epoch 4884, Loss: 0.9116126447916031, Final Batch Loss: 0.23310883343219757\n",
      "Epoch 4885, Loss: 0.8751738592982292, Final Batch Loss: 0.10300933569669724\n",
      "Epoch 4886, Loss: 0.7886529415845871, Final Batch Loss: 0.16390927135944366\n",
      "Epoch 4887, Loss: 0.8720498606562614, Final Batch Loss: 0.08347833901643753\n",
      "Epoch 4888, Loss: 0.744963750243187, Final Batch Loss: 0.19546206295490265\n",
      "Epoch 4889, Loss: 0.6247195824980736, Final Batch Loss: 0.07072704285383224\n",
      "Epoch 4890, Loss: 0.7559813261032104, Final Batch Loss: 0.20946799218654633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4891, Loss: 0.7059192508459091, Final Batch Loss: 0.191511869430542\n",
      "Epoch 4892, Loss: 0.77715964615345, Final Batch Loss: 0.21242660284042358\n",
      "Epoch 4893, Loss: 0.6005425751209259, Final Batch Loss: 0.1032436192035675\n",
      "Epoch 4894, Loss: 0.8952742666006088, Final Batch Loss: 0.3108404576778412\n",
      "Epoch 4895, Loss: 0.902768611907959, Final Batch Loss: 0.3422585427761078\n",
      "Epoch 4896, Loss: 0.8032767325639725, Final Batch Loss: 0.2561759352684021\n",
      "Epoch 4897, Loss: 0.8614049702882767, Final Batch Loss: 0.24092477560043335\n",
      "Epoch 4898, Loss: 0.7284226790070534, Final Batch Loss: 0.1158834919333458\n",
      "Epoch 4899, Loss: 0.8955048173666, Final Batch Loss: 0.2563338279724121\n",
      "Epoch 4900, Loss: 0.6982655301690102, Final Batch Loss: 0.08298603445291519\n",
      "Epoch 4901, Loss: 0.7068361788988113, Final Batch Loss: 0.08886206150054932\n",
      "Epoch 4902, Loss: 0.728611670434475, Final Batch Loss: 0.07436303049325943\n",
      "Epoch 4903, Loss: 0.769209623336792, Final Batch Loss: 0.1999761313199997\n",
      "Epoch 4904, Loss: 0.7527541667222977, Final Batch Loss: 0.1763235479593277\n",
      "Epoch 4905, Loss: 0.9665973633527756, Final Batch Loss: 0.2673986852169037\n",
      "Epoch 4906, Loss: 0.8065260648727417, Final Batch Loss: 0.15020978450775146\n",
      "Epoch 4907, Loss: 0.9138099998235703, Final Batch Loss: 0.14106960594654083\n",
      "Epoch 4908, Loss: 1.0480816215276718, Final Batch Loss: 0.36208832263946533\n",
      "Epoch 4909, Loss: 0.8920422047376633, Final Batch Loss: 0.11968505382537842\n",
      "Epoch 4910, Loss: 0.8756560683250427, Final Batch Loss: 0.28963983058929443\n",
      "Epoch 4911, Loss: 0.8139375299215317, Final Batch Loss: 0.22914423048496246\n",
      "Epoch 4912, Loss: 0.7920115292072296, Final Batch Loss: 0.23007138073444366\n",
      "Epoch 4913, Loss: 0.7163419276475906, Final Batch Loss: 0.10933837294578552\n",
      "Epoch 4914, Loss: 0.7444803789258003, Final Batch Loss: 0.10961552709341049\n",
      "Epoch 4915, Loss: 0.6788943558931351, Final Batch Loss: 0.0741635262966156\n",
      "Epoch 4916, Loss: 1.0000145584344864, Final Batch Loss: 0.3066507875919342\n",
      "Epoch 4917, Loss: 1.0469902157783508, Final Batch Loss: 0.2805632948875427\n",
      "Epoch 4918, Loss: 0.9589687138795853, Final Batch Loss: 0.278962105512619\n",
      "Epoch 4919, Loss: 0.8947295546531677, Final Batch Loss: 0.2511298358440399\n",
      "Epoch 4920, Loss: 0.8019892275333405, Final Batch Loss: 0.20453912019729614\n",
      "Epoch 4921, Loss: 0.8126574456691742, Final Batch Loss: 0.2671807110309601\n",
      "Epoch 4922, Loss: 0.7795667052268982, Final Batch Loss: 0.2088726907968521\n",
      "Epoch 4923, Loss: 0.8049524128437042, Final Batch Loss: 0.28699496388435364\n",
      "Epoch 4924, Loss: 1.0820543318986893, Final Batch Loss: 0.3724879324436188\n",
      "Epoch 4925, Loss: 1.1186324954032898, Final Batch Loss: 0.2942945957183838\n",
      "Epoch 4926, Loss: 0.8139457851648331, Final Batch Loss: 0.14942766726016998\n",
      "Epoch 4927, Loss: 0.8731789737939835, Final Batch Loss: 0.22662250697612762\n",
      "Epoch 4928, Loss: 0.7492341510951519, Final Batch Loss: 0.04368133470416069\n",
      "Epoch 4929, Loss: 0.8311391398310661, Final Batch Loss: 0.12114832550287247\n",
      "Epoch 4930, Loss: 0.8546288907527924, Final Batch Loss: 0.1474410444498062\n",
      "Epoch 4931, Loss: 0.9431908428668976, Final Batch Loss: 0.26577726006507874\n",
      "Epoch 4932, Loss: 0.9235905185341835, Final Batch Loss: 0.11974931508302689\n",
      "Epoch 4933, Loss: 0.7526668012142181, Final Batch Loss: 0.21610896289348602\n",
      "Epoch 4934, Loss: 0.7787261009216309, Final Batch Loss: 0.16920106112957\n",
      "Epoch 4935, Loss: 0.7300229519605637, Final Batch Loss: 0.1634855419397354\n",
      "Epoch 4936, Loss: 0.7515548542141914, Final Batch Loss: 0.1068013533949852\n",
      "Epoch 4937, Loss: 0.7534842044115067, Final Batch Loss: 0.12892360985279083\n",
      "Epoch 4938, Loss: 1.1255462169647217, Final Batch Loss: 0.5313925743103027\n",
      "Epoch 4939, Loss: 0.6212325841188431, Final Batch Loss: 0.13701066374778748\n",
      "Epoch 4940, Loss: 0.9402031749486923, Final Batch Loss: 0.28602519631385803\n",
      "Epoch 4941, Loss: 1.1837218701839447, Final Batch Loss: 0.5854650735855103\n",
      "Epoch 4942, Loss: 0.7213279455900192, Final Batch Loss: 0.19448347389698029\n",
      "Epoch 4943, Loss: 0.8713064193725586, Final Batch Loss: 0.07974645495414734\n",
      "Epoch 4944, Loss: 0.7470064759254456, Final Batch Loss: 0.19683068990707397\n",
      "Epoch 4945, Loss: 0.9481426626443863, Final Batch Loss: 0.3143966495990753\n",
      "Epoch 4946, Loss: 0.9221402257680893, Final Batch Loss: 0.27612069249153137\n",
      "Epoch 4947, Loss: 0.8034608215093613, Final Batch Loss: 0.08110883831977844\n",
      "Epoch 4948, Loss: 1.0087502300739288, Final Batch Loss: 0.40557023882865906\n",
      "Epoch 4949, Loss: 0.7392344176769257, Final Batch Loss: 0.18926624953746796\n",
      "Epoch 4950, Loss: 0.8609393313527107, Final Batch Loss: 0.11044248193502426\n",
      "Epoch 4951, Loss: 0.858536034822464, Final Batch Loss: 0.1604291796684265\n",
      "Epoch 4952, Loss: 0.8399338126182556, Final Batch Loss: 0.16271401941776276\n",
      "Epoch 4953, Loss: 0.6725806817412376, Final Batch Loss: 0.10574259608983994\n",
      "Epoch 4954, Loss: 0.9089198857545853, Final Batch Loss: 0.28976085782051086\n",
      "Epoch 4955, Loss: 0.825543686747551, Final Batch Loss: 0.22825969755649567\n",
      "Epoch 4956, Loss: 0.7458249479532242, Final Batch Loss: 0.140282541513443\n",
      "Epoch 4957, Loss: 0.8944947272539139, Final Batch Loss: 0.34425854682922363\n",
      "Epoch 4958, Loss: 0.6839901059865952, Final Batch Loss: 0.14280740916728973\n",
      "Epoch 4959, Loss: 0.8267852887511253, Final Batch Loss: 0.11057644337415695\n",
      "Epoch 4960, Loss: 0.853689044713974, Final Batch Loss: 0.335711807012558\n",
      "Epoch 4961, Loss: 0.9474155902862549, Final Batch Loss: 0.34032782912254333\n",
      "Epoch 4962, Loss: 1.019247904419899, Final Batch Loss: 0.3231702446937561\n",
      "Epoch 4963, Loss: 0.8023474961519241, Final Batch Loss: 0.16109658777713776\n",
      "Epoch 4964, Loss: 0.7020336538553238, Final Batch Loss: 0.1894257813692093\n",
      "Epoch 4965, Loss: 0.6288807094097137, Final Batch Loss: 0.056364044547080994\n",
      "Epoch 4966, Loss: 0.786876454949379, Final Batch Loss: 0.2556343376636505\n",
      "Epoch 4967, Loss: 0.7607135772705078, Final Batch Loss: 0.12564845383167267\n",
      "Epoch 4968, Loss: 0.6434799283742905, Final Batch Loss: 0.14162372052669525\n",
      "Epoch 4969, Loss: 0.8776856958866119, Final Batch Loss: 0.27625495195388794\n",
      "Epoch 4970, Loss: 0.7432707622647285, Final Batch Loss: 0.11152137070894241\n",
      "Epoch 4971, Loss: 0.7472322583198547, Final Batch Loss: 0.20626170933246613\n",
      "Epoch 4972, Loss: 0.8544856905937195, Final Batch Loss: 0.20492708683013916\n",
      "Epoch 4973, Loss: 0.9178019911050797, Final Batch Loss: 0.26874610781669617\n",
      "Epoch 4974, Loss: 0.7896496057510376, Final Batch Loss: 0.1941370964050293\n",
      "Epoch 4975, Loss: 0.8596683442592621, Final Batch Loss: 0.2790151834487915\n",
      "Epoch 4976, Loss: 0.7792591899633408, Final Batch Loss: 0.14194032549858093\n",
      "Epoch 4977, Loss: 0.7328165844082832, Final Batch Loss: 0.08408302813768387\n",
      "Epoch 4978, Loss: 1.121851697564125, Final Batch Loss: 0.6007843613624573\n",
      "Epoch 4979, Loss: 0.7213502749800682, Final Batch Loss: 0.12457361072301865\n",
      "Epoch 4980, Loss: 0.8775309324264526, Final Batch Loss: 0.1821531504392624\n",
      "Epoch 4981, Loss: 0.851499542593956, Final Batch Loss: 0.21206988394260406\n",
      "Epoch 4982, Loss: 0.6949694082140923, Final Batch Loss: 0.12137378007173538\n",
      "Epoch 4983, Loss: 0.8090815991163254, Final Batch Loss: 0.2179252952337265\n",
      "Epoch 4984, Loss: 0.6743832230567932, Final Batch Loss: 0.09421706199645996\n",
      "Epoch 4985, Loss: 0.7496226876974106, Final Batch Loss: 0.16824142634868622\n",
      "Epoch 4986, Loss: 1.0049790143966675, Final Batch Loss: 0.39094066619873047\n",
      "Epoch 4987, Loss: 0.67710280418396, Final Batch Loss: 0.2009289413690567\n",
      "Epoch 4988, Loss: 0.9627382308244705, Final Batch Loss: 0.39149394631385803\n",
      "Epoch 4989, Loss: 0.8021652847528458, Final Batch Loss: 0.24230839312076569\n",
      "Epoch 4990, Loss: 0.8016591668128967, Final Batch Loss: 0.11472770571708679\n",
      "Epoch 4991, Loss: 0.784435510635376, Final Batch Loss: 0.1695491522550583\n",
      "Epoch 4992, Loss: 0.915955662727356, Final Batch Loss: 0.35714390873908997\n",
      "Epoch 4993, Loss: 0.7190290167927742, Final Batch Loss: 0.1072242334485054\n",
      "Epoch 4994, Loss: 0.7733276560902596, Final Batch Loss: 0.10269013792276382\n",
      "Epoch 4995, Loss: 0.9044743031263351, Final Batch Loss: 0.26976895332336426\n",
      "Epoch 4996, Loss: 0.7017738148570061, Final Batch Loss: 0.0697648748755455\n",
      "Epoch 4997, Loss: 0.7886464446783066, Final Batch Loss: 0.10796031355857849\n",
      "Epoch 4998, Loss: 0.6342336162924767, Final Batch Loss: 0.0729924812912941\n",
      "Epoch 4999, Loss: 0.7447074502706528, Final Batch Loss: 0.24450291693210602\n",
      "Epoch 5000, Loss: 0.779389813542366, Final Batch Loss: 0.1461741179227829\n",
      "Epoch 5001, Loss: 0.7636329382658005, Final Batch Loss: 0.22226674854755402\n",
      "Epoch 5002, Loss: 0.7579501867294312, Final Batch Loss: 0.14944787323474884\n",
      "Epoch 5003, Loss: 0.8236834183335304, Final Batch Loss: 0.24152594804763794\n",
      "Epoch 5004, Loss: 0.5987074859440327, Final Batch Loss: 0.05945228412747383\n",
      "Epoch 5005, Loss: 0.7133998274803162, Final Batch Loss: 0.07673269510269165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5006, Loss: 0.8455570787191391, Final Batch Loss: 0.28861674666404724\n",
      "Epoch 5007, Loss: 0.9306307435035706, Final Batch Loss: 0.3001311123371124\n",
      "Epoch 5008, Loss: 0.813572958111763, Final Batch Loss: 0.20016402006149292\n",
      "Epoch 5009, Loss: 0.6977052688598633, Final Batch Loss: 0.07087387144565582\n",
      "Epoch 5010, Loss: 0.7520215958356857, Final Batch Loss: 0.17243333160877228\n",
      "Epoch 5011, Loss: 0.7404113411903381, Final Batch Loss: 0.1847340315580368\n",
      "Epoch 5012, Loss: 0.6528649553656578, Final Batch Loss: 0.11050913482904434\n",
      "Epoch 5013, Loss: 0.7656202912330627, Final Batch Loss: 0.188074991106987\n",
      "Epoch 5014, Loss: 0.695032000541687, Final Batch Loss: 0.15833121538162231\n",
      "Epoch 5015, Loss: 0.70196932554245, Final Batch Loss: 0.15636558830738068\n",
      "Epoch 5016, Loss: 0.9818863719701767, Final Batch Loss: 0.2793690860271454\n",
      "Epoch 5017, Loss: 0.8954457640647888, Final Batch Loss: 0.2448796033859253\n",
      "Epoch 5018, Loss: 0.9162735790014267, Final Batch Loss: 0.35669219493865967\n",
      "Epoch 5019, Loss: 0.8337532579898834, Final Batch Loss: 0.23710960149765015\n",
      "Epoch 5020, Loss: 0.8200190886855125, Final Batch Loss: 0.12331307679414749\n",
      "Epoch 5021, Loss: 0.9777658730745316, Final Batch Loss: 0.3008934557437897\n",
      "Epoch 5022, Loss: 0.6846848651766777, Final Batch Loss: 0.09427972882986069\n",
      "Epoch 5023, Loss: 0.8614697605371475, Final Batch Loss: 0.2693050503730774\n",
      "Epoch 5024, Loss: 0.9442318379878998, Final Batch Loss: 0.34238627552986145\n",
      "Epoch 5025, Loss: 0.7112172022461891, Final Batch Loss: 0.11212547868490219\n",
      "Epoch 5026, Loss: 0.7843161970376968, Final Batch Loss: 0.2438725233078003\n",
      "Epoch 5027, Loss: 0.7573227435350418, Final Batch Loss: 0.21222610771656036\n",
      "Epoch 5028, Loss: 0.7029708474874496, Final Batch Loss: 0.13896948099136353\n",
      "Epoch 5029, Loss: 0.7943846881389618, Final Batch Loss: 0.1293918639421463\n",
      "Epoch 5030, Loss: 0.8364920914173126, Final Batch Loss: 0.24492402374744415\n",
      "Epoch 5031, Loss: 0.7664955854415894, Final Batch Loss: 0.23447585105895996\n",
      "Epoch 5032, Loss: 0.895421028137207, Final Batch Loss: 0.2511945962905884\n",
      "Epoch 5033, Loss: 0.6814890652894974, Final Batch Loss: 0.16038785874843597\n",
      "Epoch 5034, Loss: 0.748941607773304, Final Batch Loss: 0.12012428790330887\n",
      "Epoch 5035, Loss: 0.7080772668123245, Final Batch Loss: 0.20602190494537354\n",
      "Epoch 5036, Loss: 0.7499292939901352, Final Batch Loss: 0.2165999412536621\n",
      "Epoch 5037, Loss: 0.9134606271982193, Final Batch Loss: 0.2192520648241043\n",
      "Epoch 5038, Loss: 0.7163741886615753, Final Batch Loss: 0.17796705663204193\n",
      "Epoch 5039, Loss: 0.748932734131813, Final Batch Loss: 0.2061384916305542\n",
      "Epoch 5040, Loss: 0.6950177252292633, Final Batch Loss: 0.12506286799907684\n",
      "Epoch 5041, Loss: 0.7523984611034393, Final Batch Loss: 0.18049217760562897\n",
      "Epoch 5042, Loss: 0.6712485142052174, Final Batch Loss: 0.06016657128930092\n",
      "Epoch 5043, Loss: 0.7787929475307465, Final Batch Loss: 0.21629786491394043\n",
      "Epoch 5044, Loss: 0.6706636995077133, Final Batch Loss: 0.17732590436935425\n",
      "Epoch 5045, Loss: 0.8714320808649063, Final Batch Loss: 0.24107909202575684\n",
      "Epoch 5046, Loss: 0.7836044803261757, Final Batch Loss: 0.09597156196832657\n",
      "Epoch 5047, Loss: 0.9747025817632675, Final Batch Loss: 0.30446723103523254\n",
      "Epoch 5048, Loss: 0.8736886978149414, Final Batch Loss: 0.2681756317615509\n",
      "Epoch 5049, Loss: 0.8707980811595917, Final Batch Loss: 0.23842684924602509\n",
      "Epoch 5050, Loss: 0.8159032426774502, Final Batch Loss: 0.058577653020620346\n",
      "Epoch 5051, Loss: 0.6220229342579842, Final Batch Loss: 0.11825988441705704\n",
      "Epoch 5052, Loss: 0.838991105556488, Final Batch Loss: 0.24240826070308685\n",
      "Epoch 5053, Loss: 0.8191267400979996, Final Batch Loss: 0.25868430733680725\n",
      "Epoch 5054, Loss: 0.6216361001133919, Final Batch Loss: 0.09012340754270554\n",
      "Epoch 5055, Loss: 0.621682196855545, Final Batch Loss: 0.09268137812614441\n",
      "Epoch 5056, Loss: 0.7839890867471695, Final Batch Loss: 0.2774238884449005\n",
      "Epoch 5057, Loss: 0.7616420388221741, Final Batch Loss: 0.20988135039806366\n",
      "Epoch 5058, Loss: 0.7749543637037277, Final Batch Loss: 0.12959542870521545\n",
      "Epoch 5059, Loss: 1.0465043634176254, Final Batch Loss: 0.3863692581653595\n",
      "Epoch 5060, Loss: 0.7459438294172287, Final Batch Loss: 0.1868104189634323\n",
      "Epoch 5061, Loss: 0.9424260854721069, Final Batch Loss: 0.32276299595832825\n",
      "Epoch 5062, Loss: 0.6733661666512489, Final Batch Loss: 0.07469024509191513\n",
      "Epoch 5063, Loss: 0.883205771446228, Final Batch Loss: 0.3142857849597931\n",
      "Epoch 5064, Loss: 0.8070430010557175, Final Batch Loss: 0.19322969019412994\n",
      "Epoch 5065, Loss: 0.6418255642056465, Final Batch Loss: 0.0884576067328453\n",
      "Epoch 5066, Loss: 0.7299860268831253, Final Batch Loss: 0.16900990903377533\n",
      "Epoch 5067, Loss: 0.6983353197574615, Final Batch Loss: 0.13956260681152344\n",
      "Epoch 5068, Loss: 0.8805387318134308, Final Batch Loss: 0.3253592252731323\n",
      "Epoch 5069, Loss: 0.8958531469106674, Final Batch Loss: 0.22277259826660156\n",
      "Epoch 5070, Loss: 1.0088207870721817, Final Batch Loss: 0.33207011222839355\n",
      "Epoch 5071, Loss: 0.8277852982282639, Final Batch Loss: 0.2500667870044708\n",
      "Epoch 5072, Loss: 1.1558459401130676, Final Batch Loss: 0.4848758280277252\n",
      "Epoch 5073, Loss: 0.9116222262382507, Final Batch Loss: 0.19615237414836884\n",
      "Epoch 5074, Loss: 0.7688375264406204, Final Batch Loss: 0.13897816836833954\n",
      "Epoch 5075, Loss: 0.6980557441711426, Final Batch Loss: 0.15518216788768768\n",
      "Epoch 5076, Loss: 0.9171339571475983, Final Batch Loss: 0.227045938372612\n",
      "Epoch 5077, Loss: 0.6163867935538292, Final Batch Loss: 0.07944966107606888\n",
      "Epoch 5078, Loss: 0.7861261963844299, Final Batch Loss: 0.21840284764766693\n",
      "Epoch 5079, Loss: 0.7577532827854156, Final Batch Loss: 0.18441538512706757\n",
      "Epoch 5080, Loss: 0.6956672538071871, Final Batch Loss: 0.023945989087224007\n",
      "Epoch 5081, Loss: 0.6331843361258507, Final Batch Loss: 0.10245617479085922\n",
      "Epoch 5082, Loss: 0.7945346385240555, Final Batch Loss: 0.22990834712982178\n",
      "Epoch 5083, Loss: 0.5670468881726265, Final Batch Loss: 0.06667623668909073\n",
      "Epoch 5084, Loss: 0.6678207069635391, Final Batch Loss: 0.23126506805419922\n",
      "Epoch 5085, Loss: 0.8079474717378616, Final Batch Loss: 0.1781439185142517\n",
      "Epoch 5086, Loss: 1.0803278386592865, Final Batch Loss: 0.22872865200042725\n",
      "Epoch 5087, Loss: 0.721912682056427, Final Batch Loss: 0.16423563659191132\n",
      "Epoch 5088, Loss: 0.9397411793470383, Final Batch Loss: 0.28738346695899963\n",
      "Epoch 5089, Loss: 0.7202459946274757, Final Batch Loss: 0.10060573369264603\n",
      "Epoch 5090, Loss: 0.7107729315757751, Final Batch Loss: 0.15305690467357635\n",
      "Epoch 5091, Loss: 0.8675206899642944, Final Batch Loss: 0.24329029023647308\n",
      "Epoch 5092, Loss: 0.7977489680051804, Final Batch Loss: 0.23235146701335907\n",
      "Epoch 5093, Loss: 0.7777428328990936, Final Batch Loss: 0.21598540246486664\n",
      "Epoch 5094, Loss: 0.9474773854017258, Final Batch Loss: 0.17138828337192535\n",
      "Epoch 5095, Loss: 0.8472912758588791, Final Batch Loss: 0.18285198509693146\n",
      "Epoch 5096, Loss: 0.9298805743455887, Final Batch Loss: 0.14282192289829254\n",
      "Epoch 5097, Loss: 0.8778139650821686, Final Batch Loss: 0.28652873635292053\n",
      "Epoch 5098, Loss: 0.7590935826301575, Final Batch Loss: 0.19481728971004486\n",
      "Epoch 5099, Loss: 0.6727479621767998, Final Batch Loss: 0.06724364310503006\n",
      "Epoch 5100, Loss: 0.8489115983247757, Final Batch Loss: 0.21276265382766724\n",
      "Epoch 5101, Loss: 0.8440297245979309, Final Batch Loss: 0.21520067751407623\n",
      "Epoch 5102, Loss: 1.0256942957639694, Final Batch Loss: 0.4697991609573364\n",
      "Epoch 5103, Loss: 0.8330559283494949, Final Batch Loss: 0.27502748370170593\n",
      "Epoch 5104, Loss: 0.9632235020399094, Final Batch Loss: 0.35199686884880066\n",
      "Epoch 5105, Loss: 0.7010147199034691, Final Batch Loss: 0.10576721280813217\n",
      "Epoch 5106, Loss: 0.91007861495018, Final Batch Loss: 0.2980710566043854\n",
      "Epoch 5107, Loss: 0.8325685560703278, Final Batch Loss: 0.16433390974998474\n",
      "Epoch 5108, Loss: 0.7645971328020096, Final Batch Loss: 0.2051311731338501\n",
      "Epoch 5109, Loss: 0.7964402437210083, Final Batch Loss: 0.1921844333410263\n",
      "Epoch 5110, Loss: 0.8057885244488716, Final Batch Loss: 0.11054003983736038\n",
      "Epoch 5111, Loss: 0.6803841888904572, Final Batch Loss: 0.10689741373062134\n",
      "Epoch 5112, Loss: 0.8453839719295502, Final Batch Loss: 0.2645544111728668\n",
      "Epoch 5113, Loss: 0.8379473239183426, Final Batch Loss: 0.1922484040260315\n",
      "Epoch 5114, Loss: 0.6625618785619736, Final Batch Loss: 0.14279277622699738\n",
      "Epoch 5115, Loss: 0.8397294580936432, Final Batch Loss: 0.1520315557718277\n",
      "Epoch 5116, Loss: 0.9036384969949722, Final Batch Loss: 0.2565842568874359\n",
      "Epoch 5117, Loss: 0.8189550340175629, Final Batch Loss: 0.2426053285598755\n",
      "Epoch 5118, Loss: 0.6984691768884659, Final Batch Loss: 0.1418694704771042\n",
      "Epoch 5119, Loss: 0.81792351603508, Final Batch Loss: 0.14198215305805206\n",
      "Epoch 5120, Loss: 0.6960352063179016, Final Batch Loss: 0.17191874980926514\n",
      "Epoch 5121, Loss: 0.855560913681984, Final Batch Loss: 0.30383703112602234\n",
      "Epoch 5122, Loss: 0.7470067292451859, Final Batch Loss: 0.205480694770813\n",
      "Epoch 5123, Loss: 0.7089751064777374, Final Batch Loss: 0.11358734965324402\n",
      "Epoch 5124, Loss: 0.8082052767276764, Final Batch Loss: 0.24621444940567017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5125, Loss: 0.8063110560178757, Final Batch Loss: 0.1806415319442749\n",
      "Epoch 5126, Loss: 0.74192014336586, Final Batch Loss: 0.15869753062725067\n",
      "Epoch 5127, Loss: 0.7176316902041435, Final Batch Loss: 0.1231510117650032\n",
      "Epoch 5128, Loss: 0.7774620950222015, Final Batch Loss: 0.1528477817773819\n",
      "Epoch 5129, Loss: 0.6645619049668312, Final Batch Loss: 0.11884469538927078\n",
      "Epoch 5130, Loss: 0.82020203769207, Final Batch Loss: 0.2696743309497833\n",
      "Epoch 5131, Loss: 0.785055935382843, Final Batch Loss: 0.12173345685005188\n",
      "Epoch 5132, Loss: 0.6841446980834007, Final Batch Loss: 0.1135324016213417\n",
      "Epoch 5133, Loss: 0.7574680149555206, Final Batch Loss: 0.2088519185781479\n",
      "Epoch 5134, Loss: 0.7789893597364426, Final Batch Loss: 0.22161848843097687\n",
      "Epoch 5135, Loss: 0.7749269977211952, Final Batch Loss: 0.11508812755346298\n",
      "Epoch 5136, Loss: 0.7523532658815384, Final Batch Loss: 0.23037607967853546\n",
      "Epoch 5137, Loss: 0.7022362500429153, Final Batch Loss: 0.09652355313301086\n",
      "Epoch 5138, Loss: 0.8638669103384018, Final Batch Loss: 0.29321083426475525\n",
      "Epoch 5139, Loss: 0.7547120451927185, Final Batch Loss: 0.21501918137073517\n",
      "Epoch 5140, Loss: 0.6798175126314163, Final Batch Loss: 0.13305257260799408\n",
      "Epoch 5141, Loss: 0.6883928924798965, Final Batch Loss: 0.0461130291223526\n",
      "Epoch 5142, Loss: 0.5851178243756294, Final Batch Loss: 0.07851938158273697\n",
      "Epoch 5143, Loss: 0.7258198857307434, Final Batch Loss: 0.2159285694360733\n",
      "Epoch 5144, Loss: 0.735993891954422, Final Batch Loss: 0.175642728805542\n",
      "Epoch 5145, Loss: 0.790745422244072, Final Batch Loss: 0.20359553396701813\n",
      "Epoch 5146, Loss: 0.8005523383617401, Final Batch Loss: 0.18899162113666534\n",
      "Epoch 5147, Loss: 0.7399825155735016, Final Batch Loss: 0.2118520885705948\n",
      "Epoch 5148, Loss: 0.7013289630413055, Final Batch Loss: 0.132090225815773\n",
      "Epoch 5149, Loss: 0.5812840834259987, Final Batch Loss: 0.07956140488386154\n",
      "Epoch 5150, Loss: 0.7783420234918594, Final Batch Loss: 0.21051394939422607\n",
      "Epoch 5151, Loss: 0.7408306002616882, Final Batch Loss: 0.12942956387996674\n",
      "Epoch 5152, Loss: 0.849996030330658, Final Batch Loss: 0.16425518691539764\n",
      "Epoch 5153, Loss: 0.6550969928503036, Final Batch Loss: 0.11973042041063309\n",
      "Epoch 5154, Loss: 0.764413058757782, Final Batch Loss: 0.1411581188440323\n",
      "Epoch 5155, Loss: 0.9939254522323608, Final Batch Loss: 0.36592891812324524\n",
      "Epoch 5156, Loss: 0.7555309981107712, Final Batch Loss: 0.13160750269889832\n",
      "Epoch 5157, Loss: 0.5804450437426567, Final Batch Loss: 0.08072098344564438\n",
      "Epoch 5158, Loss: 0.693315364420414, Final Batch Loss: 0.14721354842185974\n",
      "Epoch 5159, Loss: 0.6158588379621506, Final Batch Loss: 0.06916143000125885\n",
      "Epoch 5160, Loss: 0.7170830368995667, Final Batch Loss: 0.14300626516342163\n",
      "Epoch 5161, Loss: 0.7110783904790878, Final Batch Loss: 0.21537940204143524\n",
      "Epoch 5162, Loss: 0.7682584971189499, Final Batch Loss: 0.2760772407054901\n",
      "Epoch 5163, Loss: 0.8462222367525101, Final Batch Loss: 0.2954311668872833\n",
      "Epoch 5164, Loss: 0.84625044465065, Final Batch Loss: 0.21772344410419464\n",
      "Epoch 5165, Loss: 0.7435969859361649, Final Batch Loss: 0.1838538497686386\n",
      "Epoch 5166, Loss: 0.7596689760684967, Final Batch Loss: 0.1998012512922287\n",
      "Epoch 5167, Loss: 0.6721319407224655, Final Batch Loss: 0.11968615651130676\n",
      "Epoch 5168, Loss: 0.7371894270181656, Final Batch Loss: 0.20135049521923065\n",
      "Epoch 5169, Loss: 0.8883047848939896, Final Batch Loss: 0.27658024430274963\n",
      "Epoch 5170, Loss: 0.8157687783241272, Final Batch Loss: 0.28270238637924194\n",
      "Epoch 5171, Loss: 0.9299634248018265, Final Batch Loss: 0.2777227759361267\n",
      "Epoch 5172, Loss: 0.8573843389749527, Final Batch Loss: 0.21395288407802582\n",
      "Epoch 5173, Loss: 0.6990064308047295, Final Batch Loss: 0.08662987500429153\n",
      "Epoch 5174, Loss: 0.765500970184803, Final Batch Loss: 0.11468615382909775\n",
      "Epoch 5175, Loss: 0.6728358864784241, Final Batch Loss: 0.1393042504787445\n",
      "Epoch 5176, Loss: 0.9296854585409164, Final Batch Loss: 0.34080255031585693\n",
      "Epoch 5177, Loss: 0.8412028849124908, Final Batch Loss: 0.19209463894367218\n",
      "Epoch 5178, Loss: 0.9111150205135345, Final Batch Loss: 0.23414933681488037\n",
      "Epoch 5179, Loss: 0.67533740401268, Final Batch Loss: 0.18067121505737305\n",
      "Epoch 5180, Loss: 0.7652245461940765, Final Batch Loss: 0.19936440885066986\n",
      "Epoch 5181, Loss: 0.8706222921609879, Final Batch Loss: 0.24515365064144135\n",
      "Epoch 5182, Loss: 0.7193930596113205, Final Batch Loss: 0.11146923899650574\n",
      "Epoch 5183, Loss: 1.0900639742612839, Final Batch Loss: 0.4759535491466522\n",
      "Epoch 5184, Loss: 0.9944440126419067, Final Batch Loss: 0.32495665550231934\n",
      "Epoch 5185, Loss: 0.7448823750019073, Final Batch Loss: 0.17025595903396606\n",
      "Epoch 5186, Loss: 0.933970719575882, Final Batch Loss: 0.22839589416980743\n",
      "Epoch 5187, Loss: 0.7849623113870621, Final Batch Loss: 0.16949857771396637\n",
      "Epoch 5188, Loss: 0.9066011160612106, Final Batch Loss: 0.17959930002689362\n",
      "Epoch 5189, Loss: 1.1682228296995163, Final Batch Loss: 0.30462613701820374\n",
      "Epoch 5190, Loss: 0.9162298887968063, Final Batch Loss: 0.1290096491575241\n",
      "Epoch 5191, Loss: 0.9856810122728348, Final Batch Loss: 0.27903276681900024\n",
      "Epoch 5192, Loss: 0.9238355308771133, Final Batch Loss: 0.22667814791202545\n",
      "Epoch 5193, Loss: 1.0201537907123566, Final Batch Loss: 0.38298943638801575\n",
      "Epoch 5194, Loss: 0.9082761853933334, Final Batch Loss: 0.17525149881839752\n",
      "Epoch 5195, Loss: 0.8460504710674286, Final Batch Loss: 0.30427390336990356\n",
      "Epoch 5196, Loss: 1.0430908799171448, Final Batch Loss: 0.2711262106895447\n",
      "Epoch 5197, Loss: 1.0950246006250381, Final Batch Loss: 0.48360589146614075\n",
      "Epoch 5198, Loss: 0.9919617474079132, Final Batch Loss: 0.3735501766204834\n",
      "Epoch 5199, Loss: 0.9767816364765167, Final Batch Loss: 0.2426716834306717\n",
      "Epoch 5200, Loss: 0.8471098840236664, Final Batch Loss: 0.22874416410923004\n",
      "Epoch 5201, Loss: 0.6885987296700478, Final Batch Loss: 0.11115481704473495\n",
      "Epoch 5202, Loss: 0.8110962212085724, Final Batch Loss: 0.26304417848587036\n",
      "Epoch 5203, Loss: 0.8492763787508011, Final Batch Loss: 0.23289519548416138\n",
      "Epoch 5204, Loss: 0.6079318597912788, Final Batch Loss: 0.07057023793458939\n",
      "Epoch 5205, Loss: 0.7648390680551529, Final Batch Loss: 0.25132235884666443\n",
      "Epoch 5206, Loss: 0.9114224016666412, Final Batch Loss: 0.2939744293689728\n",
      "Epoch 5207, Loss: 0.9015373438596725, Final Batch Loss: 0.2106580138206482\n",
      "Epoch 5208, Loss: 0.9287744164466858, Final Batch Loss: 0.2448081374168396\n",
      "Epoch 5209, Loss: 0.8599705100059509, Final Batch Loss: 0.1762504130601883\n",
      "Epoch 5210, Loss: 0.8691623508930206, Final Batch Loss: 0.272350937128067\n",
      "Epoch 5211, Loss: 0.8210055530071259, Final Batch Loss: 0.2000526785850525\n",
      "Epoch 5212, Loss: 0.7738134190440178, Final Batch Loss: 0.07748746126890182\n",
      "Epoch 5213, Loss: 0.9541789889335632, Final Batch Loss: 0.33930668234825134\n",
      "Epoch 5214, Loss: 0.7170974388718605, Final Batch Loss: 0.09282185882329941\n",
      "Epoch 5215, Loss: 0.7400566339492798, Final Batch Loss: 0.20141015946865082\n",
      "Epoch 5216, Loss: 0.7255256846547127, Final Batch Loss: 0.07555689662694931\n",
      "Epoch 5217, Loss: 0.7372268587350845, Final Batch Loss: 0.12608827650547028\n",
      "Epoch 5218, Loss: 0.8143090605735779, Final Batch Loss: 0.18986280262470245\n",
      "Epoch 5219, Loss: 1.0053502023220062, Final Batch Loss: 0.34996771812438965\n",
      "Epoch 5220, Loss: 0.7275021225214005, Final Batch Loss: 0.10901939868927002\n",
      "Epoch 5221, Loss: 0.8959954679012299, Final Batch Loss: 0.22234803438186646\n",
      "Epoch 5222, Loss: 0.9092514961957932, Final Batch Loss: 0.2382466197013855\n",
      "Epoch 5223, Loss: 0.7440245598554611, Final Batch Loss: 0.20808154344558716\n",
      "Epoch 5224, Loss: 0.7598969638347626, Final Batch Loss: 0.1552097499370575\n",
      "Epoch 5225, Loss: 0.7565228044986725, Final Batch Loss: 0.1981998234987259\n",
      "Epoch 5226, Loss: 0.6682024747133255, Final Batch Loss: 0.13944628834724426\n",
      "Epoch 5227, Loss: 0.6461794748902321, Final Batch Loss: 0.10870984941720963\n",
      "Epoch 5228, Loss: 0.8332699835300446, Final Batch Loss: 0.1375742256641388\n",
      "Epoch 5229, Loss: 0.8339596390724182, Final Batch Loss: 0.3066490888595581\n",
      "Epoch 5230, Loss: 0.8164029866456985, Final Batch Loss: 0.22858957946300507\n",
      "Epoch 5231, Loss: 0.8311145603656769, Final Batch Loss: 0.17541831731796265\n",
      "Epoch 5232, Loss: 0.6836685687303543, Final Batch Loss: 0.08524951338768005\n",
      "Epoch 5233, Loss: 0.9019422680139542, Final Batch Loss: 0.3234698474407196\n",
      "Epoch 5234, Loss: 0.7882807105779648, Final Batch Loss: 0.13623781502246857\n",
      "Epoch 5235, Loss: 0.811949610710144, Final Batch Loss: 0.21478325128555298\n",
      "Epoch 5236, Loss: 0.7184936255216599, Final Batch Loss: 0.07999743521213531\n",
      "Epoch 5237, Loss: 0.6226840764284134, Final Batch Loss: 0.13305220007896423\n",
      "Epoch 5238, Loss: 0.8016487061977386, Final Batch Loss: 0.13955135643482208\n",
      "Epoch 5239, Loss: 0.7831961363554001, Final Batch Loss: 0.17274963855743408\n",
      "Epoch 5240, Loss: 0.8233524113893509, Final Batch Loss: 0.20127318799495697\n",
      "Epoch 5241, Loss: 0.6980366855859756, Final Batch Loss: 0.09211710095405579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5242, Loss: 0.7348817065358162, Final Batch Loss: 0.11936088651418686\n",
      "Epoch 5243, Loss: 0.7621669769287109, Final Batch Loss: 0.2153490036725998\n",
      "Epoch 5244, Loss: 0.8522400408983231, Final Batch Loss: 0.28591516613960266\n",
      "Epoch 5245, Loss: 0.8507724851369858, Final Batch Loss: 0.24261528253555298\n",
      "Epoch 5246, Loss: 1.07192263007164, Final Batch Loss: 0.34769904613494873\n",
      "Epoch 5247, Loss: 1.0150160193443298, Final Batch Loss: 0.2680632770061493\n",
      "Epoch 5248, Loss: 1.0845400840044022, Final Batch Loss: 0.2974548041820526\n",
      "Epoch 5249, Loss: 0.9177619814872742, Final Batch Loss: 0.25239166617393494\n",
      "Epoch 5250, Loss: 0.9027764648199081, Final Batch Loss: 0.22790215909481049\n",
      "Epoch 5251, Loss: 1.0465572327375412, Final Batch Loss: 0.3542647063732147\n",
      "Epoch 5252, Loss: 1.0342770367860794, Final Batch Loss: 0.3832920789718628\n",
      "Epoch 5253, Loss: 0.8346798717975616, Final Batch Loss: 0.17080669105052948\n",
      "Epoch 5254, Loss: 0.9181876331567764, Final Batch Loss: 0.21995942294597626\n",
      "Epoch 5255, Loss: 0.9326219260692596, Final Batch Loss: 0.22687570750713348\n",
      "Epoch 5256, Loss: 0.8152468875050545, Final Batch Loss: 0.10760227590799332\n",
      "Epoch 5257, Loss: 0.7453868836164474, Final Batch Loss: 0.11173340678215027\n",
      "Epoch 5258, Loss: 0.7570090293884277, Final Batch Loss: 0.1915246844291687\n",
      "Epoch 5259, Loss: 0.6653113663196564, Final Batch Loss: 0.13026610016822815\n",
      "Epoch 5260, Loss: 0.7425105720758438, Final Batch Loss: 0.18399782478809357\n",
      "Epoch 5261, Loss: 0.7320909947156906, Final Batch Loss: 0.17733556032180786\n",
      "Epoch 5262, Loss: 0.8416866660118103, Final Batch Loss: 0.23644880950450897\n",
      "Epoch 5263, Loss: 0.7848505973815918, Final Batch Loss: 0.14202480018138885\n",
      "Epoch 5264, Loss: 0.7646785601973534, Final Batch Loss: 0.12351075559854507\n",
      "Epoch 5265, Loss: 0.6825687661767006, Final Batch Loss: 0.09489182382822037\n",
      "Epoch 5266, Loss: 0.7067730948328972, Final Batch Loss: 0.12315314263105392\n",
      "Epoch 5267, Loss: 0.8551847338676453, Final Batch Loss: 0.3118923604488373\n",
      "Epoch 5268, Loss: 0.840228945016861, Final Batch Loss: 0.30817940831184387\n",
      "Epoch 5269, Loss: 0.8243648409843445, Final Batch Loss: 0.20481230318546295\n",
      "Epoch 5270, Loss: 0.7234490513801575, Final Batch Loss: 0.1703416109085083\n",
      "Epoch 5271, Loss: 0.9259088784456253, Final Batch Loss: 0.28659573197364807\n",
      "Epoch 5272, Loss: 0.6763310879468918, Final Batch Loss: 0.14186784625053406\n",
      "Epoch 5273, Loss: 0.8083988726139069, Final Batch Loss: 0.22756808996200562\n",
      "Epoch 5274, Loss: 1.0238280147314072, Final Batch Loss: 0.466096431016922\n",
      "Epoch 5275, Loss: 0.6682487800717354, Final Batch Loss: 0.1597157120704651\n",
      "Epoch 5276, Loss: 0.8120582550764084, Final Batch Loss: 0.15626215934753418\n",
      "Epoch 5277, Loss: 0.7994042783975601, Final Batch Loss: 0.1993401199579239\n",
      "Epoch 5278, Loss: 0.758430615067482, Final Batch Loss: 0.20665298402309418\n",
      "Epoch 5279, Loss: 0.5822237804532051, Final Batch Loss: 0.09683499485254288\n",
      "Epoch 5280, Loss: 1.0119110494852066, Final Batch Loss: 0.3253193795681\n",
      "Epoch 5281, Loss: 0.9352922141551971, Final Batch Loss: 0.2729584276676178\n",
      "Epoch 5282, Loss: 0.8622585609555244, Final Batch Loss: 0.10665427893400192\n",
      "Epoch 5283, Loss: 0.7816347181797028, Final Batch Loss: 0.22090548276901245\n",
      "Epoch 5284, Loss: 0.8006233274936676, Final Batch Loss: 0.18340639770030975\n",
      "Epoch 5285, Loss: 0.8545705676078796, Final Batch Loss: 0.23710320889949799\n",
      "Epoch 5286, Loss: 0.7097282707691193, Final Batch Loss: 0.1268014907836914\n",
      "Epoch 5287, Loss: 0.8619079887866974, Final Batch Loss: 0.2763631045818329\n",
      "Epoch 5288, Loss: 0.781784787774086, Final Batch Loss: 0.23546232283115387\n",
      "Epoch 5289, Loss: 0.7163518145680428, Final Batch Loss: 0.10923793166875839\n",
      "Epoch 5290, Loss: 0.7061396166682243, Final Batch Loss: 0.1788993626832962\n",
      "Epoch 5291, Loss: 0.8191360235214233, Final Batch Loss: 0.1651672124862671\n",
      "Epoch 5292, Loss: 0.8357345759868622, Final Batch Loss: 0.17768515646457672\n",
      "Epoch 5293, Loss: 0.7996659129858017, Final Batch Loss: 0.19574010372161865\n",
      "Epoch 5294, Loss: 0.599105142056942, Final Batch Loss: 0.09232526272535324\n",
      "Epoch 5295, Loss: 0.9306426346302032, Final Batch Loss: 0.38444384932518005\n",
      "Epoch 5296, Loss: 0.8204111158847809, Final Batch Loss: 0.2512204051017761\n",
      "Epoch 5297, Loss: 0.8788258582353592, Final Batch Loss: 0.17969126999378204\n",
      "Epoch 5298, Loss: 0.8928834646940231, Final Batch Loss: 0.2656015157699585\n",
      "Epoch 5299, Loss: 1.2362935841083527, Final Batch Loss: 0.5111317038536072\n",
      "Epoch 5300, Loss: 0.9662466943264008, Final Batch Loss: 0.31161224842071533\n",
      "Epoch 5301, Loss: 0.913616493344307, Final Batch Loss: 0.18838877975940704\n",
      "Epoch 5302, Loss: 0.7406494915485382, Final Batch Loss: 0.11144658923149109\n",
      "Epoch 5303, Loss: 0.7791562378406525, Final Batch Loss: 0.2694723904132843\n",
      "Epoch 5304, Loss: 0.7274289429187775, Final Batch Loss: 0.1392344832420349\n",
      "Epoch 5305, Loss: 0.6630041599273682, Final Batch Loss: 0.08342844247817993\n",
      "Epoch 5306, Loss: 0.7987319529056549, Final Batch Loss: 0.22837911546230316\n",
      "Epoch 5307, Loss: 0.832801342010498, Final Batch Loss: 0.2641017436981201\n",
      "Epoch 5308, Loss: 0.7254633009433746, Final Batch Loss: 0.16895373165607452\n",
      "Epoch 5309, Loss: 0.6900965198874474, Final Batch Loss: 0.06595493108034134\n",
      "Epoch 5310, Loss: 0.8465428203344345, Final Batch Loss: 0.27622660994529724\n",
      "Epoch 5311, Loss: 0.7200241535902023, Final Batch Loss: 0.17126041650772095\n",
      "Epoch 5312, Loss: 0.7353147268295288, Final Batch Loss: 0.16991955041885376\n",
      "Epoch 5313, Loss: 0.8127669095993042, Final Batch Loss: 0.171599343419075\n",
      "Epoch 5314, Loss: 0.708547331392765, Final Batch Loss: 0.07644761353731155\n",
      "Epoch 5315, Loss: 0.8115587383508682, Final Batch Loss: 0.14395591616630554\n",
      "Epoch 5316, Loss: 0.7273494154214859, Final Batch Loss: 0.1816655844449997\n",
      "Epoch 5317, Loss: 0.7049607336521149, Final Batch Loss: 0.12144175171852112\n",
      "Epoch 5318, Loss: 0.8438323587179184, Final Batch Loss: 0.2380163073539734\n",
      "Epoch 5319, Loss: 0.7366196513175964, Final Batch Loss: 0.1823711395263672\n",
      "Epoch 5320, Loss: 0.83094821870327, Final Batch Loss: 0.3332644999027252\n",
      "Epoch 5321, Loss: 0.7845442742109299, Final Batch Loss: 0.20911163091659546\n",
      "Epoch 5322, Loss: 0.7049852609634399, Final Batch Loss: 0.17148703336715698\n",
      "Epoch 5323, Loss: 0.713791012763977, Final Batch Loss: 0.13427521288394928\n",
      "Epoch 5324, Loss: 0.832345187664032, Final Batch Loss: 0.2581259608268738\n",
      "Epoch 5325, Loss: 0.9876858592033386, Final Batch Loss: 0.40528038144111633\n",
      "Epoch 5326, Loss: 0.8084776401519775, Final Batch Loss: 0.2529847025871277\n",
      "Epoch 5327, Loss: 0.7970400750637054, Final Batch Loss: 0.12555648386478424\n",
      "Epoch 5328, Loss: 0.8900711834430695, Final Batch Loss: 0.29242074489593506\n",
      "Epoch 5329, Loss: 0.6996310576796532, Final Batch Loss: 0.08624619990587234\n",
      "Epoch 5330, Loss: 0.7748597711324692, Final Batch Loss: 0.23416048288345337\n",
      "Epoch 5331, Loss: 0.7649524509906769, Final Batch Loss: 0.2023840695619583\n",
      "Epoch 5332, Loss: 0.8921156376600266, Final Batch Loss: 0.2696700692176819\n",
      "Epoch 5333, Loss: 0.9106622040271759, Final Batch Loss: 0.3281468451023102\n",
      "Epoch 5334, Loss: 0.7701438143849373, Final Batch Loss: 0.11395233124494553\n",
      "Epoch 5335, Loss: 0.728143997490406, Final Batch Loss: 0.09095492213964462\n",
      "Epoch 5336, Loss: 0.8491034060716629, Final Batch Loss: 0.24120372533798218\n",
      "Epoch 5337, Loss: 0.8165682256221771, Final Batch Loss: 0.18310587108135223\n",
      "Epoch 5338, Loss: 0.7383620887994766, Final Batch Loss: 0.13708993792533875\n",
      "Epoch 5339, Loss: 0.6852554231882095, Final Batch Loss: 0.11026713252067566\n",
      "Epoch 5340, Loss: 0.6773782968521118, Final Batch Loss: 0.09547582268714905\n",
      "Epoch 5341, Loss: 0.8113294094800949, Final Batch Loss: 0.21582110226154327\n",
      "Epoch 5342, Loss: 0.6807408928871155, Final Batch Loss: 0.14253558218479156\n",
      "Epoch 5343, Loss: 0.8069724142551422, Final Batch Loss: 0.24560405313968658\n",
      "Epoch 5344, Loss: 0.7824034094810486, Final Batch Loss: 0.15602438151836395\n",
      "Epoch 5345, Loss: 0.8086621165275574, Final Batch Loss: 0.22489513456821442\n",
      "Epoch 5346, Loss: 0.9377160668373108, Final Batch Loss: 0.383076548576355\n",
      "Epoch 5347, Loss: 0.8994639217853546, Final Batch Loss: 0.18994908034801483\n",
      "Epoch 5348, Loss: 0.7096137404441833, Final Batch Loss: 0.13791634142398834\n",
      "Epoch 5349, Loss: 0.8304646760225296, Final Batch Loss: 0.2589547336101532\n",
      "Epoch 5350, Loss: 0.8870594352483749, Final Batch Loss: 0.2599499523639679\n",
      "Epoch 5351, Loss: 0.6764862537384033, Final Batch Loss: 0.1739492267370224\n",
      "Epoch 5352, Loss: 0.8463549464941025, Final Batch Loss: 0.2220669388771057\n",
      "Epoch 5353, Loss: 0.8489323705434799, Final Batch Loss: 0.28614571690559387\n",
      "Epoch 5354, Loss: 1.0170203149318695, Final Batch Loss: 0.38615646958351135\n",
      "Epoch 5355, Loss: 0.7392161339521408, Final Batch Loss: 0.152231365442276\n",
      "Epoch 5356, Loss: 0.9204988181591034, Final Batch Loss: 0.24309879541397095\n",
      "Epoch 5357, Loss: 0.7022137492895126, Final Batch Loss: 0.13873174786567688\n",
      "Epoch 5358, Loss: 0.7971616238355637, Final Batch Loss: 0.17917500436306\n",
      "Epoch 5359, Loss: 0.6240829974412918, Final Batch Loss: 0.14952678978443146\n",
      "Epoch 5360, Loss: 0.9710751920938492, Final Batch Loss: 0.3018229305744171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5361, Loss: 0.8431957364082336, Final Batch Loss: 0.3215869665145874\n",
      "Epoch 5362, Loss: 0.7528077885508537, Final Batch Loss: 0.1056409552693367\n",
      "Epoch 5363, Loss: 0.7701398432254791, Final Batch Loss: 0.13187278807163239\n",
      "Epoch 5364, Loss: 0.6786440908908844, Final Batch Loss: 0.08225293457508087\n",
      "Epoch 5365, Loss: 0.7342962473630905, Final Batch Loss: 0.1959119588136673\n",
      "Epoch 5366, Loss: 0.691925972700119, Final Batch Loss: 0.12158548831939697\n",
      "Epoch 5367, Loss: 0.7546064406633377, Final Batch Loss: 0.18469227850437164\n",
      "Epoch 5368, Loss: 0.8470653891563416, Final Batch Loss: 0.3112885057926178\n",
      "Epoch 5369, Loss: 0.8190647512674332, Final Batch Loss: 0.21359342336654663\n",
      "Epoch 5370, Loss: 0.6957432925701141, Final Batch Loss: 0.1717015653848648\n",
      "Epoch 5371, Loss: 0.7983229979872704, Final Batch Loss: 0.07926113158464432\n",
      "Epoch 5372, Loss: 0.8457579761743546, Final Batch Loss: 0.23523615300655365\n",
      "Epoch 5373, Loss: 0.834744468331337, Final Batch Loss: 0.2537987232208252\n",
      "Epoch 5374, Loss: 0.7207499444484711, Final Batch Loss: 0.15557335317134857\n",
      "Epoch 5375, Loss: 0.8494780957698822, Final Batch Loss: 0.16837257146835327\n",
      "Epoch 5376, Loss: 0.9104278236627579, Final Batch Loss: 0.20464760065078735\n",
      "Epoch 5377, Loss: 1.0518365651369095, Final Batch Loss: 0.17198319733142853\n",
      "Epoch 5378, Loss: 0.9221298694610596, Final Batch Loss: 0.13166223466396332\n",
      "Epoch 5379, Loss: 0.7486138790845871, Final Batch Loss: 0.12886717915534973\n",
      "Epoch 5380, Loss: 0.9989622086286545, Final Batch Loss: 0.2859930694103241\n",
      "Epoch 5381, Loss: 0.9109472036361694, Final Batch Loss: 0.1492583006620407\n",
      "Epoch 5382, Loss: 0.7324452698230743, Final Batch Loss: 0.12708346545696259\n",
      "Epoch 5383, Loss: 0.7828523814678192, Final Batch Loss: 0.14603713154792786\n",
      "Epoch 5384, Loss: 0.7758480906486511, Final Batch Loss: 0.17643333971500397\n",
      "Epoch 5385, Loss: 0.9422415941953659, Final Batch Loss: 0.2480727881193161\n",
      "Epoch 5386, Loss: 0.7802900969982147, Final Batch Loss: 0.2268514186143875\n",
      "Epoch 5387, Loss: 0.8553731888532639, Final Batch Loss: 0.2633949816226959\n",
      "Epoch 5388, Loss: 0.6421467363834381, Final Batch Loss: 0.07787296175956726\n",
      "Epoch 5389, Loss: 0.8662415593862534, Final Batch Loss: 0.17915910482406616\n",
      "Epoch 5390, Loss: 0.8933481276035309, Final Batch Loss: 0.23138076066970825\n",
      "Epoch 5391, Loss: 0.725384071469307, Final Batch Loss: 0.08653825521469116\n",
      "Epoch 5392, Loss: 0.6837029308080673, Final Batch Loss: 0.12922532856464386\n",
      "Epoch 5393, Loss: 0.6889151483774185, Final Batch Loss: 0.23182953894138336\n",
      "Epoch 5394, Loss: 0.7446447536349297, Final Batch Loss: 0.20944128930568695\n",
      "Epoch 5395, Loss: 0.8480878174304962, Final Batch Loss: 0.3414837419986725\n",
      "Epoch 5396, Loss: 0.8864914625883102, Final Batch Loss: 0.21110673248767853\n",
      "Epoch 5397, Loss: 0.8212117999792099, Final Batch Loss: 0.26326796412467957\n",
      "Epoch 5398, Loss: 0.7594047635793686, Final Batch Loss: 0.22251014411449432\n",
      "Epoch 5399, Loss: 1.0816998332738876, Final Batch Loss: 0.46826326847076416\n",
      "Epoch 5400, Loss: 0.7138864696025848, Final Batch Loss: 0.1289544254541397\n",
      "Epoch 5401, Loss: 0.8490242660045624, Final Batch Loss: 0.23394127190113068\n",
      "Epoch 5402, Loss: 0.8959946036338806, Final Batch Loss: 0.24263258278369904\n",
      "Epoch 5403, Loss: 0.7005172222852707, Final Batch Loss: 0.11647230386734009\n",
      "Epoch 5404, Loss: 0.7794292718172073, Final Batch Loss: 0.1569102555513382\n",
      "Epoch 5405, Loss: 0.8705637902021408, Final Batch Loss: 0.2113577127456665\n",
      "Epoch 5406, Loss: 0.9492505490779877, Final Batch Loss: 0.27422574162483215\n",
      "Epoch 5407, Loss: 0.8584692627191544, Final Batch Loss: 0.24942374229431152\n",
      "Epoch 5408, Loss: 0.7625858560204506, Final Batch Loss: 0.11652062088251114\n",
      "Epoch 5409, Loss: 0.8135368078947067, Final Batch Loss: 0.2413000464439392\n",
      "Epoch 5410, Loss: 0.8176063001155853, Final Batch Loss: 0.3098476827144623\n",
      "Epoch 5411, Loss: 0.7647536545991898, Final Batch Loss: 0.19978158175945282\n",
      "Epoch 5412, Loss: 0.6944379210472107, Final Batch Loss: 0.20059216022491455\n",
      "Epoch 5413, Loss: 0.6875559687614441, Final Batch Loss: 0.09303998947143555\n",
      "Epoch 5414, Loss: 0.8542672544717789, Final Batch Loss: 0.2799944281578064\n",
      "Epoch 5415, Loss: 0.8330191224813461, Final Batch Loss: 0.27726706862449646\n",
      "Epoch 5416, Loss: 0.776207908987999, Final Batch Loss: 0.23464716970920563\n",
      "Epoch 5417, Loss: 0.8177257031202316, Final Batch Loss: 0.18687279522418976\n",
      "Epoch 5418, Loss: 0.8155922740697861, Final Batch Loss: 0.2218862771987915\n",
      "Epoch 5419, Loss: 0.7286126464605331, Final Batch Loss: 0.1488923877477646\n",
      "Epoch 5420, Loss: 0.8978253602981567, Final Batch Loss: 0.33418163657188416\n",
      "Epoch 5421, Loss: 0.822380930185318, Final Batch Loss: 0.21905052661895752\n",
      "Epoch 5422, Loss: 0.806775227189064, Final Batch Loss: 0.25496652722358704\n",
      "Epoch 5423, Loss: 0.7465333789587021, Final Batch Loss: 0.14401493966579437\n",
      "Epoch 5424, Loss: 0.8529399931430817, Final Batch Loss: 0.12944717705249786\n",
      "Epoch 5425, Loss: 1.103117749094963, Final Batch Loss: 0.4595681428909302\n",
      "Epoch 5426, Loss: 1.0252296477556229, Final Batch Loss: 0.17166005074977875\n",
      "Epoch 5427, Loss: 0.9778042584657669, Final Batch Loss: 0.302068293094635\n",
      "Epoch 5428, Loss: 0.7547909021377563, Final Batch Loss: 0.18183672428131104\n",
      "Epoch 5429, Loss: 0.8891864269971848, Final Batch Loss: 0.2714953124523163\n",
      "Epoch 5430, Loss: 0.9757235050201416, Final Batch Loss: 0.20217038691043854\n",
      "Epoch 5431, Loss: 0.8650380969047546, Final Batch Loss: 0.3108759820461273\n",
      "Epoch 5432, Loss: 0.7680258452892303, Final Batch Loss: 0.18240106105804443\n",
      "Epoch 5433, Loss: 0.6661661118268967, Final Batch Loss: 0.1264701783657074\n",
      "Epoch 5434, Loss: 0.7898708134889603, Final Batch Loss: 0.183266744017601\n",
      "Epoch 5435, Loss: 0.7798288762569427, Final Batch Loss: 0.2579142153263092\n",
      "Epoch 5436, Loss: 0.8643164187669754, Final Batch Loss: 0.26907217502593994\n",
      "Epoch 5437, Loss: 0.8680217862129211, Final Batch Loss: 0.20205621421337128\n",
      "Epoch 5438, Loss: 0.6632825061678886, Final Batch Loss: 0.09741493314504623\n",
      "Epoch 5439, Loss: 0.7940242290496826, Final Batch Loss: 0.3221425712108612\n",
      "Epoch 5440, Loss: 0.7511957734823227, Final Batch Loss: 0.15166430175304413\n",
      "Epoch 5441, Loss: 0.7676523923873901, Final Batch Loss: 0.18397383391857147\n",
      "Epoch 5442, Loss: 0.7162716537714005, Final Batch Loss: 0.09311461448669434\n",
      "Epoch 5443, Loss: 0.6870466992259026, Final Batch Loss: 0.11705923825502396\n",
      "Epoch 5444, Loss: 0.6657910719513893, Final Batch Loss: 0.10085216909646988\n",
      "Epoch 5445, Loss: 0.7701111733913422, Final Batch Loss: 0.205370232462883\n",
      "Epoch 5446, Loss: 0.6361918151378632, Final Batch Loss: 0.11310964822769165\n",
      "Epoch 5447, Loss: 0.7987837642431259, Final Batch Loss: 0.19020885229110718\n",
      "Epoch 5448, Loss: 0.7338972315192223, Final Batch Loss: 0.0993010625243187\n",
      "Epoch 5449, Loss: 0.9382065534591675, Final Batch Loss: 0.26149073243141174\n",
      "Epoch 5450, Loss: 0.69666837900877, Final Batch Loss: 0.08925295621156693\n",
      "Epoch 5451, Loss: 0.8109879791736603, Final Batch Loss: 0.14956794679164886\n",
      "Epoch 5452, Loss: 0.6940485313534737, Final Batch Loss: 0.07888796180486679\n",
      "Epoch 5453, Loss: 0.9023437201976776, Final Batch Loss: 0.3030526340007782\n",
      "Epoch 5454, Loss: 0.8017672449350357, Final Batch Loss: 0.14801131188869476\n",
      "Epoch 5455, Loss: 0.7486114054918289, Final Batch Loss: 0.15220049023628235\n",
      "Epoch 5456, Loss: 0.7708794921636581, Final Batch Loss: 0.22609983384609222\n",
      "Epoch 5457, Loss: 0.8805186599493027, Final Batch Loss: 0.22518104314804077\n",
      "Epoch 5458, Loss: 0.766303300857544, Final Batch Loss: 0.13747183978557587\n",
      "Epoch 5459, Loss: 1.0610118806362152, Final Batch Loss: 0.42830196022987366\n",
      "Epoch 5460, Loss: 0.8967141062021255, Final Batch Loss: 0.21778734028339386\n",
      "Epoch 5461, Loss: 0.7680590003728867, Final Batch Loss: 0.14162977039813995\n",
      "Epoch 5462, Loss: 0.7030810639262199, Final Batch Loss: 0.10782105475664139\n",
      "Epoch 5463, Loss: 0.8243941068649292, Final Batch Loss: 0.1483364701271057\n",
      "Epoch 5464, Loss: 0.744710810482502, Final Batch Loss: 0.10819486528635025\n",
      "Epoch 5465, Loss: 0.6681187450885773, Final Batch Loss: 0.13958202302455902\n",
      "Epoch 5466, Loss: 0.7711783796548843, Final Batch Loss: 0.20872505009174347\n",
      "Epoch 5467, Loss: 0.8108533769845963, Final Batch Loss: 0.2041662335395813\n",
      "Epoch 5468, Loss: 0.9047017842531204, Final Batch Loss: 0.3014213740825653\n",
      "Epoch 5469, Loss: 0.7378351986408234, Final Batch Loss: 0.15459351241588593\n",
      "Epoch 5470, Loss: 0.8972766995429993, Final Batch Loss: 0.28843986988067627\n",
      "Epoch 5471, Loss: 0.7497497498989105, Final Batch Loss: 0.18203644454479218\n",
      "Epoch 5472, Loss: 0.8002148419618607, Final Batch Loss: 0.19690847396850586\n",
      "Epoch 5473, Loss: 0.7025385946035385, Final Batch Loss: 0.15550582110881805\n",
      "Epoch 5474, Loss: 0.7016559168696404, Final Batch Loss: 0.12336979061365128\n",
      "Epoch 5475, Loss: 0.9627377092838287, Final Batch Loss: 0.23697102069854736\n",
      "Epoch 5476, Loss: 0.7868203520774841, Final Batch Loss: 0.2634958326816559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5477, Loss: 0.7767530083656311, Final Batch Loss: 0.14624567329883575\n",
      "Epoch 5478, Loss: 0.7319897413253784, Final Batch Loss: 0.1906389594078064\n",
      "Epoch 5479, Loss: 0.6281962543725967, Final Batch Loss: 0.13397014141082764\n",
      "Epoch 5480, Loss: 0.6793890818953514, Final Batch Loss: 0.21525698900222778\n",
      "Epoch 5481, Loss: 0.7304007336497307, Final Batch Loss: 0.16190969944000244\n",
      "Epoch 5482, Loss: 0.7642493396997452, Final Batch Loss: 0.18056075274944305\n",
      "Epoch 5483, Loss: 0.8014984130859375, Final Batch Loss: 0.19665920734405518\n",
      "Epoch 5484, Loss: 0.9570470899343491, Final Batch Loss: 0.307879775762558\n",
      "Epoch 5485, Loss: 0.7906267046928406, Final Batch Loss: 0.19012778997421265\n",
      "Epoch 5486, Loss: 0.9652110189199448, Final Batch Loss: 0.32389676570892334\n",
      "Epoch 5487, Loss: 1.0591465383768082, Final Batch Loss: 0.22433358430862427\n",
      "Epoch 5488, Loss: 0.8232555687427521, Final Batch Loss: 0.234443798661232\n",
      "Epoch 5489, Loss: 0.8350791037082672, Final Batch Loss: 0.2143155336380005\n",
      "Epoch 5490, Loss: 0.9998789429664612, Final Batch Loss: 0.32310107350349426\n",
      "Epoch 5491, Loss: 0.7804078459739685, Final Batch Loss: 0.1467650979757309\n",
      "Epoch 5492, Loss: 1.0321600288152695, Final Batch Loss: 0.27645349502563477\n",
      "Epoch 5493, Loss: 0.8074786812067032, Final Batch Loss: 0.19309741258621216\n",
      "Epoch 5494, Loss: 0.741450309753418, Final Batch Loss: 0.15973679721355438\n",
      "Epoch 5495, Loss: 0.8247657343745232, Final Batch Loss: 0.1153719499707222\n",
      "Epoch 5496, Loss: 0.751832127571106, Final Batch Loss: 0.1550053209066391\n",
      "Epoch 5497, Loss: 0.6841670423746109, Final Batch Loss: 0.2058604508638382\n",
      "Epoch 5498, Loss: 0.7934355139732361, Final Batch Loss: 0.22445110976696014\n",
      "Epoch 5499, Loss: 0.7439911514520645, Final Batch Loss: 0.20562241971492767\n",
      "Epoch 5500, Loss: 0.8404405415058136, Final Batch Loss: 0.3012533485889435\n",
      "Epoch 5501, Loss: 0.8038001954555511, Final Batch Loss: 0.11044138669967651\n",
      "Epoch 5502, Loss: 1.2674487680196762, Final Batch Loss: 0.7158493995666504\n",
      "Epoch 5503, Loss: 0.6489478126168251, Final Batch Loss: 0.12051474303007126\n",
      "Epoch 5504, Loss: 0.7673888653516769, Final Batch Loss: 0.17339126765727997\n",
      "Epoch 5505, Loss: 0.707903653383255, Final Batch Loss: 0.07945533096790314\n",
      "Epoch 5506, Loss: 0.7993289232254028, Final Batch Loss: 0.19346986711025238\n",
      "Epoch 5507, Loss: 0.8283434510231018, Final Batch Loss: 0.2580583691596985\n",
      "Epoch 5508, Loss: 0.8409700095653534, Final Batch Loss: 0.3001604676246643\n",
      "Epoch 5509, Loss: 0.9097837507724762, Final Batch Loss: 0.22466249763965607\n",
      "Epoch 5510, Loss: 0.7019097730517387, Final Batch Loss: 0.11776325851678848\n",
      "Epoch 5511, Loss: 0.7568108439445496, Final Batch Loss: 0.125283882021904\n",
      "Epoch 5512, Loss: 0.6938280239701271, Final Batch Loss: 0.10239941626787186\n",
      "Epoch 5513, Loss: 0.7598233446478844, Final Batch Loss: 0.09685973078012466\n",
      "Epoch 5514, Loss: 0.6538627669215202, Final Batch Loss: 0.11842630058526993\n",
      "Epoch 5515, Loss: 0.787039577960968, Final Batch Loss: 0.2590043842792511\n",
      "Epoch 5516, Loss: 0.6815380156040192, Final Batch Loss: 0.1622624397277832\n",
      "Epoch 5517, Loss: 0.6858185231685638, Final Batch Loss: 0.1370842009782791\n",
      "Epoch 5518, Loss: 0.7270868569612503, Final Batch Loss: 0.1890469789505005\n",
      "Epoch 5519, Loss: 0.7091279476881027, Final Batch Loss: 0.2451021671295166\n",
      "Epoch 5520, Loss: 0.7351307570934296, Final Batch Loss: 0.2333512306213379\n",
      "Epoch 5521, Loss: 0.9346718937158585, Final Batch Loss: 0.37907764315605164\n",
      "Epoch 5522, Loss: 0.8413158357143402, Final Batch Loss: 0.186105415225029\n",
      "Epoch 5523, Loss: 0.6412792950868607, Final Batch Loss: 0.13666777312755585\n",
      "Epoch 5524, Loss: 1.0246028006076813, Final Batch Loss: 0.45337411761283875\n",
      "Epoch 5525, Loss: 0.8100118786096573, Final Batch Loss: 0.2258751392364502\n",
      "Epoch 5526, Loss: 0.676223948597908, Final Batch Loss: 0.12625758349895477\n",
      "Epoch 5527, Loss: 0.756413146853447, Final Batch Loss: 0.1173396110534668\n",
      "Epoch 5528, Loss: 0.5938259065151215, Final Batch Loss: 0.08552667498588562\n",
      "Epoch 5529, Loss: 0.760791003704071, Final Batch Loss: 0.22551357746124268\n",
      "Epoch 5530, Loss: 0.7058463245630264, Final Batch Loss: 0.1823885589838028\n",
      "Epoch 5531, Loss: 0.8331828713417053, Final Batch Loss: 0.1534574031829834\n",
      "Epoch 5532, Loss: 0.6868069171905518, Final Batch Loss: 0.16113696992397308\n",
      "Epoch 5533, Loss: 0.7634569704532623, Final Batch Loss: 0.18200433254241943\n",
      "Epoch 5534, Loss: 0.670283168554306, Final Batch Loss: 0.13058072328567505\n",
      "Epoch 5535, Loss: 0.7995471656322479, Final Batch Loss: 0.1811627894639969\n",
      "Epoch 5536, Loss: 0.6292629688978195, Final Batch Loss: 0.10579755902290344\n",
      "Epoch 5537, Loss: 0.8122224062681198, Final Batch Loss: 0.14922338724136353\n",
      "Epoch 5538, Loss: 1.1659027189016342, Final Batch Loss: 0.488628625869751\n",
      "Epoch 5539, Loss: 0.8776170760393143, Final Batch Loss: 0.3068574368953705\n",
      "Epoch 5540, Loss: 0.8269864171743393, Final Batch Loss: 0.24401415884494781\n",
      "Epoch 5541, Loss: 0.6575239300727844, Final Batch Loss: 0.16696171462535858\n",
      "Epoch 5542, Loss: 0.7916251420974731, Final Batch Loss: 0.23113547265529633\n",
      "Epoch 5543, Loss: 0.8135932087898254, Final Batch Loss: 0.23406843841075897\n",
      "Epoch 5544, Loss: 0.8252429813146591, Final Batch Loss: 0.2629986107349396\n",
      "Epoch 5545, Loss: 0.7795706689357758, Final Batch Loss: 0.17513161897659302\n",
      "Epoch 5546, Loss: 0.829995647072792, Final Batch Loss: 0.22215954959392548\n",
      "Epoch 5547, Loss: 0.8565118312835693, Final Batch Loss: 0.2718232572078705\n",
      "Epoch 5548, Loss: 0.7373806834220886, Final Batch Loss: 0.15539893507957458\n",
      "Epoch 5549, Loss: 0.7375426739454269, Final Batch Loss: 0.13487018644809723\n",
      "Epoch 5550, Loss: 0.7079189270734787, Final Batch Loss: 0.157114639878273\n",
      "Epoch 5551, Loss: 0.5890735983848572, Final Batch Loss: 0.06318233907222748\n",
      "Epoch 5552, Loss: 0.9169647693634033, Final Batch Loss: 0.30074065923690796\n",
      "Epoch 5553, Loss: 0.8155592828989029, Final Batch Loss: 0.2595803439617157\n",
      "Epoch 5554, Loss: 0.8134562969207764, Final Batch Loss: 0.35492420196533203\n",
      "Epoch 5555, Loss: 0.8163492977619171, Final Batch Loss: 0.2422078400850296\n",
      "Epoch 5556, Loss: 0.7695171535015106, Final Batch Loss: 0.18229103088378906\n",
      "Epoch 5557, Loss: 0.8165783286094666, Final Batch Loss: 0.15995295345783234\n",
      "Epoch 5558, Loss: 0.7375540137290955, Final Batch Loss: 0.20588625967502594\n",
      "Epoch 5559, Loss: 0.808149516582489, Final Batch Loss: 0.2148832231760025\n",
      "Epoch 5560, Loss: 0.7029460817575455, Final Batch Loss: 0.1942484825849533\n",
      "Epoch 5561, Loss: 0.6194824427366257, Final Batch Loss: 0.12177002429962158\n",
      "Epoch 5562, Loss: 0.7900363355875015, Final Batch Loss: 0.2266579121351242\n",
      "Epoch 5563, Loss: 0.6131257116794586, Final Batch Loss: 0.08661776781082153\n",
      "Epoch 5564, Loss: 0.7337213009595871, Final Batch Loss: 0.1518070250749588\n",
      "Epoch 5565, Loss: 0.6381615400314331, Final Batch Loss: 0.13682346045970917\n",
      "Epoch 5566, Loss: 1.075934648513794, Final Batch Loss: 0.5161833763122559\n",
      "Epoch 5567, Loss: 0.8468180596828461, Final Batch Loss: 0.2051481008529663\n",
      "Epoch 5568, Loss: 0.7005745619535446, Final Batch Loss: 0.12837818264961243\n",
      "Epoch 5569, Loss: 0.7850416302680969, Final Batch Loss: 0.20458275079727173\n",
      "Epoch 5570, Loss: 0.8207115232944489, Final Batch Loss: 0.177565336227417\n",
      "Epoch 5571, Loss: 0.7492406815290451, Final Batch Loss: 0.2515781819820404\n",
      "Epoch 5572, Loss: 0.8732965886592865, Final Batch Loss: 0.21762514114379883\n",
      "Epoch 5573, Loss: 0.7497527003288269, Final Batch Loss: 0.20206600427627563\n",
      "Epoch 5574, Loss: 0.7877391427755356, Final Batch Loss: 0.24494613707065582\n",
      "Epoch 5575, Loss: 0.77698715031147, Final Batch Loss: 0.15157873928546906\n",
      "Epoch 5576, Loss: 0.6659770384430885, Final Batch Loss: 0.0842146947979927\n",
      "Epoch 5577, Loss: 0.8583861291408539, Final Batch Loss: 0.25588318705558777\n",
      "Epoch 5578, Loss: 0.932026818394661, Final Batch Loss: 0.2709193527698517\n",
      "Epoch 5579, Loss: 0.9677030444145203, Final Batch Loss: 0.29441139101982117\n",
      "Epoch 5580, Loss: 0.8933307826519012, Final Batch Loss: 0.13353976607322693\n",
      "Epoch 5581, Loss: 0.768916055560112, Final Batch Loss: 0.16411377489566803\n",
      "Epoch 5582, Loss: 0.8565466105937958, Final Batch Loss: 0.21910446882247925\n",
      "Epoch 5583, Loss: 0.8902469277381897, Final Batch Loss: 0.15690751373767853\n",
      "Epoch 5584, Loss: 1.1752310544252396, Final Batch Loss: 0.47866055369377136\n",
      "Epoch 5585, Loss: 0.7360273450613022, Final Batch Loss: 0.07629948854446411\n",
      "Epoch 5586, Loss: 0.8121405243873596, Final Batch Loss: 0.19364507496356964\n",
      "Epoch 5587, Loss: 0.8326862156391144, Final Batch Loss: 0.2224973440170288\n",
      "Epoch 5588, Loss: 0.8000472038984299, Final Batch Loss: 0.2574981153011322\n",
      "Epoch 5589, Loss: 0.8741447478532791, Final Batch Loss: 0.235440194606781\n",
      "Epoch 5590, Loss: 1.1110907793045044, Final Batch Loss: 0.5020985007286072\n",
      "Epoch 5591, Loss: 0.7620369493961334, Final Batch Loss: 0.21127641201019287\n",
      "Epoch 5592, Loss: 0.6943674013018608, Final Batch Loss: 0.13432364165782928\n",
      "Epoch 5593, Loss: 0.7335153520107269, Final Batch Loss: 0.1313837617635727\n",
      "Epoch 5594, Loss: 0.7356620281934738, Final Batch Loss: 0.1130312979221344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5595, Loss: 0.7589389085769653, Final Batch Loss: 0.1316239833831787\n",
      "Epoch 5596, Loss: 0.8005589544773102, Final Batch Loss: 0.254911333322525\n",
      "Epoch 5597, Loss: 0.723785862326622, Final Batch Loss: 0.2097156047821045\n",
      "Epoch 5598, Loss: 0.8482355624437332, Final Batch Loss: 0.21058368682861328\n",
      "Epoch 5599, Loss: 0.7227549031376839, Final Batch Loss: 0.11685127764940262\n",
      "Epoch 5600, Loss: 0.8774722516536713, Final Batch Loss: 0.2941255569458008\n",
      "Epoch 5601, Loss: 0.7640096098184586, Final Batch Loss: 0.1773982048034668\n",
      "Epoch 5602, Loss: 0.9347238391637802, Final Batch Loss: 0.38529476523399353\n",
      "Epoch 5603, Loss: 0.8882365971803665, Final Batch Loss: 0.12731944024562836\n",
      "Epoch 5604, Loss: 0.7866373807191849, Final Batch Loss: 0.19663995504379272\n",
      "Epoch 5605, Loss: 0.8332319855690002, Final Batch Loss: 0.23405295610427856\n",
      "Epoch 5606, Loss: 1.0293439328670502, Final Batch Loss: 0.3829796314239502\n",
      "Epoch 5607, Loss: 0.8123130202293396, Final Batch Loss: 0.23322255909442902\n",
      "Epoch 5608, Loss: 0.8321798145771027, Final Batch Loss: 0.16434286534786224\n",
      "Epoch 5609, Loss: 0.9234557151794434, Final Batch Loss: 0.28105291724205017\n",
      "Epoch 5610, Loss: 0.629231508821249, Final Batch Loss: 0.05393926426768303\n",
      "Epoch 5611, Loss: 0.7645155191421509, Final Batch Loss: 0.1839846968650818\n",
      "Epoch 5612, Loss: 0.6630943790078163, Final Batch Loss: 0.0761677697300911\n",
      "Epoch 5613, Loss: 0.8306117653846741, Final Batch Loss: 0.15130169689655304\n",
      "Epoch 5614, Loss: 0.6587600409984589, Final Batch Loss: 0.12141138315200806\n",
      "Epoch 5615, Loss: 0.748050183057785, Final Batch Loss: 0.21038998663425446\n",
      "Epoch 5616, Loss: 0.5057643763720989, Final Batch Loss: 0.036830391734838486\n",
      "Epoch 5617, Loss: 0.8225571364164352, Final Batch Loss: 0.20196734368801117\n",
      "Epoch 5618, Loss: 0.6868171468377113, Final Batch Loss: 0.09313198179006577\n",
      "Epoch 5619, Loss: 0.7373197078704834, Final Batch Loss: 0.17794786393642426\n",
      "Epoch 5620, Loss: 0.5859408974647522, Final Batch Loss: 0.13941876590251923\n",
      "Epoch 5621, Loss: 0.7804459035396576, Final Batch Loss: 0.23861223459243774\n",
      "Epoch 5622, Loss: 0.6712484508752823, Final Batch Loss: 0.1321621686220169\n",
      "Epoch 5623, Loss: 0.7623051851987839, Final Batch Loss: 0.15573416650295258\n",
      "Epoch 5624, Loss: 0.6635099612176418, Final Batch Loss: 0.0466742180287838\n",
      "Epoch 5625, Loss: 0.8064634203910828, Final Batch Loss: 0.24008809030056\n",
      "Epoch 5626, Loss: 0.6923313662409782, Final Batch Loss: 0.06347877532243729\n",
      "Epoch 5627, Loss: 0.7720396965742111, Final Batch Loss: 0.14785631000995636\n",
      "Epoch 5628, Loss: 0.8129339665174484, Final Batch Loss: 0.29936620593070984\n",
      "Epoch 5629, Loss: 0.8098144829273224, Final Batch Loss: 0.2696760892868042\n",
      "Epoch 5630, Loss: 0.976802185177803, Final Batch Loss: 0.35322466492652893\n",
      "Epoch 5631, Loss: 0.7795664370059967, Final Batch Loss: 0.1943572759628296\n",
      "Epoch 5632, Loss: 0.7904903292655945, Final Batch Loss: 0.1851140260696411\n",
      "Epoch 5633, Loss: 0.7481860220432281, Final Batch Loss: 0.2424013465642929\n",
      "Epoch 5634, Loss: 0.7811722904443741, Final Batch Loss: 0.1497153788805008\n",
      "Epoch 5635, Loss: 0.7835698276758194, Final Batch Loss: 0.26487651467323303\n",
      "Epoch 5636, Loss: 0.9802367836236954, Final Batch Loss: 0.3744557797908783\n",
      "Epoch 5637, Loss: 0.7726135402917862, Final Batch Loss: 0.17846499383449554\n",
      "Epoch 5638, Loss: 0.820360541343689, Final Batch Loss: 0.18896770477294922\n",
      "Epoch 5639, Loss: 0.6830893233418465, Final Batch Loss: 0.11778389662504196\n",
      "Epoch 5640, Loss: 0.9623424559831619, Final Batch Loss: 0.18162058293819427\n",
      "Epoch 5641, Loss: 1.154534250497818, Final Batch Loss: 0.5920305848121643\n",
      "Epoch 5642, Loss: 0.6617138907313347, Final Batch Loss: 0.09626316279172897\n",
      "Epoch 5643, Loss: 0.7646098881959915, Final Batch Loss: 0.1793193370103836\n",
      "Epoch 5644, Loss: 0.7252565026283264, Final Batch Loss: 0.2141485959291458\n",
      "Epoch 5645, Loss: 0.7089022174477577, Final Batch Loss: 0.10467184334993362\n",
      "Epoch 5646, Loss: 0.8350712060928345, Final Batch Loss: 0.2521727979183197\n",
      "Epoch 5647, Loss: 0.8139716386795044, Final Batch Loss: 0.03939731419086456\n",
      "Epoch 5648, Loss: 1.0200286358594894, Final Batch Loss: 0.38218238949775696\n",
      "Epoch 5649, Loss: 0.6858613267540932, Final Batch Loss: 0.06340465694665909\n",
      "Epoch 5650, Loss: 0.7569421976804733, Final Batch Loss: 0.16992421448230743\n",
      "Epoch 5651, Loss: 0.791897714138031, Final Batch Loss: 0.1630702167749405\n",
      "Epoch 5652, Loss: 0.7089767381548882, Final Batch Loss: 0.10109096020460129\n",
      "Epoch 5653, Loss: 0.6548283994197845, Final Batch Loss: 0.09874534606933594\n",
      "Epoch 5654, Loss: 0.9488099068403244, Final Batch Loss: 0.3284817636013031\n",
      "Epoch 5655, Loss: 0.8770048767328262, Final Batch Loss: 0.3142862915992737\n",
      "Epoch 5656, Loss: 0.7361677587032318, Final Batch Loss: 0.19541601836681366\n",
      "Epoch 5657, Loss: 0.987042561173439, Final Batch Loss: 0.31686219573020935\n",
      "Epoch 5658, Loss: 0.6818308979272842, Final Batch Loss: 0.13889513909816742\n",
      "Epoch 5659, Loss: 0.8218069821596146, Final Batch Loss: 0.2853636145591736\n",
      "Epoch 5660, Loss: 0.7583330571651459, Final Batch Loss: 0.20373179018497467\n",
      "Epoch 5661, Loss: 0.8487613648176193, Final Batch Loss: 0.2821309566497803\n",
      "Epoch 5662, Loss: 0.7039725184440613, Final Batch Loss: 0.18304134905338287\n",
      "Epoch 5663, Loss: 1.0257194191217422, Final Batch Loss: 0.2947074770927429\n",
      "Epoch 5664, Loss: 0.781720444560051, Final Batch Loss: 0.23510503768920898\n",
      "Epoch 5665, Loss: 0.8827993273735046, Final Batch Loss: 0.36009684205055237\n",
      "Epoch 5666, Loss: 0.7547320500016212, Final Batch Loss: 0.1835521012544632\n",
      "Epoch 5667, Loss: 0.6699483022093773, Final Batch Loss: 0.0766567513346672\n",
      "Epoch 5668, Loss: 0.7857615202665329, Final Batch Loss: 0.23077034950256348\n",
      "Epoch 5669, Loss: 1.0132869482040405, Final Batch Loss: 0.4617682993412018\n",
      "Epoch 5670, Loss: 0.9327291250228882, Final Batch Loss: 0.3646077811717987\n",
      "Epoch 5671, Loss: 0.7895816937088966, Final Batch Loss: 0.10009341686964035\n",
      "Epoch 5672, Loss: 0.9091989696025848, Final Batch Loss: 0.33878612518310547\n",
      "Epoch 5673, Loss: 0.796002209186554, Final Batch Loss: 0.1779823899269104\n",
      "Epoch 5674, Loss: 0.7765009254217148, Final Batch Loss: 0.0869671106338501\n",
      "Epoch 5675, Loss: 0.5975434631109238, Final Batch Loss: 0.07565180957317352\n",
      "Epoch 5676, Loss: 0.7414384186267853, Final Batch Loss: 0.17648003995418549\n",
      "Epoch 5677, Loss: 0.6658397540450096, Final Batch Loss: 0.09315464645624161\n",
      "Epoch 5678, Loss: 0.8426545113325119, Final Batch Loss: 0.31363967061042786\n",
      "Epoch 5679, Loss: 0.7560891658067703, Final Batch Loss: 0.1302870512008667\n",
      "Epoch 5680, Loss: 0.7334536388516426, Final Batch Loss: 0.06706332415342331\n",
      "Epoch 5681, Loss: 0.9512273073196411, Final Batch Loss: 0.30835187435150146\n",
      "Epoch 5682, Loss: 0.8316456973552704, Final Batch Loss: 0.10566112399101257\n",
      "Epoch 5683, Loss: 0.8366702646017075, Final Batch Loss: 0.28588852286338806\n",
      "Epoch 5684, Loss: 0.7124764919281006, Final Batch Loss: 0.1811056137084961\n",
      "Epoch 5685, Loss: 0.7619153708219528, Final Batch Loss: 0.19237865507602692\n",
      "Epoch 5686, Loss: 0.6535212248563766, Final Batch Loss: 0.09530648589134216\n",
      "Epoch 5687, Loss: 0.5810912624001503, Final Batch Loss: 0.08076564222574234\n",
      "Epoch 5688, Loss: 0.6946103870868683, Final Batch Loss: 0.21983538568019867\n",
      "Epoch 5689, Loss: 0.6585960164666176, Final Batch Loss: 0.08997105807065964\n",
      "Epoch 5690, Loss: 0.5954680293798447, Final Batch Loss: 0.14249061048030853\n",
      "Epoch 5691, Loss: 0.8526768684387207, Final Batch Loss: 0.3073745667934418\n",
      "Epoch 5692, Loss: 0.745678573846817, Final Batch Loss: 0.1902221292257309\n",
      "Epoch 5693, Loss: 0.7464814931154251, Final Batch Loss: 0.25348514318466187\n",
      "Epoch 5694, Loss: 0.8262199908494949, Final Batch Loss: 0.20748168230056763\n",
      "Epoch 5695, Loss: 0.7777446061372757, Final Batch Loss: 0.1865905374288559\n",
      "Epoch 5696, Loss: 0.6528757214546204, Final Batch Loss: 0.15786057710647583\n",
      "Epoch 5697, Loss: 0.797406330704689, Final Batch Loss: 0.17625021934509277\n",
      "Epoch 5698, Loss: 0.8023676872253418, Final Batch Loss: 0.27065321803092957\n",
      "Epoch 5699, Loss: 0.8793811798095703, Final Batch Loss: 0.31092241406440735\n",
      "Epoch 5700, Loss: 0.9099257737398148, Final Batch Loss: 0.3155042231082916\n",
      "Epoch 5701, Loss: 0.6361056789755821, Final Batch Loss: 0.0819300040602684\n",
      "Epoch 5702, Loss: 0.7802484333515167, Final Batch Loss: 0.19426755607128143\n",
      "Epoch 5703, Loss: 0.7340662926435471, Final Batch Loss: 0.16049881279468536\n",
      "Epoch 5704, Loss: 1.0222408920526505, Final Batch Loss: 0.426814466714859\n",
      "Epoch 5705, Loss: 0.7198349088430405, Final Batch Loss: 0.1427699327468872\n",
      "Epoch 5706, Loss: 0.7500606775283813, Final Batch Loss: 0.22802533209323883\n",
      "Epoch 5707, Loss: 0.6311506181955338, Final Batch Loss: 0.13143712282180786\n",
      "Epoch 5708, Loss: 0.8034621179103851, Final Batch Loss: 0.17802979052066803\n",
      "Epoch 5709, Loss: 0.6883794665336609, Final Batch Loss: 0.1260177493095398\n",
      "Epoch 5710, Loss: 0.6696812063455582, Final Batch Loss: 0.11293566226959229\n",
      "Epoch 5711, Loss: 0.8219508528709412, Final Batch Loss: 0.23268945515155792\n",
      "Epoch 5712, Loss: 0.8083992451429367, Final Batch Loss: 0.2109813690185547\n",
      "Epoch 5713, Loss: 0.7759736329317093, Final Batch Loss: 0.1715223789215088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5714, Loss: 0.8624592423439026, Final Batch Loss: 0.18075698614120483\n",
      "Epoch 5715, Loss: 0.7025874853134155, Final Batch Loss: 0.19444231688976288\n",
      "Epoch 5716, Loss: 0.6685435399413109, Final Batch Loss: 0.11271382123231888\n",
      "Epoch 5717, Loss: 0.8046403229236603, Final Batch Loss: 0.1838821917772293\n",
      "Epoch 5718, Loss: 0.6684848219156265, Final Batch Loss: 0.15781603753566742\n",
      "Epoch 5719, Loss: 0.7008927166461945, Final Batch Loss: 0.19068898260593414\n",
      "Epoch 5720, Loss: 0.7710964530706406, Final Batch Loss: 0.22713816165924072\n",
      "Epoch 5721, Loss: 0.8621577620506287, Final Batch Loss: 0.3159613609313965\n",
      "Epoch 5722, Loss: 0.625841774046421, Final Batch Loss: 0.08549340814352036\n",
      "Epoch 5723, Loss: 0.7399019449949265, Final Batch Loss: 0.1616092175245285\n",
      "Epoch 5724, Loss: 0.7237615883350372, Final Batch Loss: 0.214244544506073\n",
      "Epoch 5725, Loss: 0.7741407752037048, Final Batch Loss: 0.2445141226053238\n",
      "Epoch 5726, Loss: 1.1880930811166763, Final Batch Loss: 0.6242268681526184\n",
      "Epoch 5727, Loss: 0.8572304993867874, Final Batch Loss: 0.37612029910087585\n",
      "Epoch 5728, Loss: 0.7173461765050888, Final Batch Loss: 0.1760186105966568\n",
      "Epoch 5729, Loss: 0.7108651697635651, Final Batch Loss: 0.07915946841239929\n",
      "Epoch 5730, Loss: 0.7550276964902878, Final Batch Loss: 0.1631695181131363\n",
      "Epoch 5731, Loss: 0.7842070311307907, Final Batch Loss: 0.23594635725021362\n",
      "Epoch 5732, Loss: 0.7672526091337204, Final Batch Loss: 0.22588373720645905\n",
      "Epoch 5733, Loss: 0.7020392119884491, Final Batch Loss: 0.18246585130691528\n",
      "Epoch 5734, Loss: 0.8374916762113571, Final Batch Loss: 0.2993604838848114\n",
      "Epoch 5735, Loss: 0.7533291876316071, Final Batch Loss: 0.22965312004089355\n",
      "Epoch 5736, Loss: 0.7032776027917862, Final Batch Loss: 0.18060334026813507\n",
      "Epoch 5737, Loss: 0.8166855126619339, Final Batch Loss: 0.2908780574798584\n",
      "Epoch 5738, Loss: 0.583611786365509, Final Batch Loss: 0.10930615663528442\n",
      "Epoch 5739, Loss: 0.8341219127178192, Final Batch Loss: 0.22461213171482086\n",
      "Epoch 5740, Loss: 0.7565006017684937, Final Batch Loss: 0.21568970382213593\n",
      "Epoch 5741, Loss: 0.8242975622415543, Final Batch Loss: 0.18050330877304077\n",
      "Epoch 5742, Loss: 0.7865014970302582, Final Batch Loss: 0.12362277507781982\n",
      "Epoch 5743, Loss: 0.8635578006505966, Final Batch Loss: 0.17612482607364655\n",
      "Epoch 5744, Loss: 0.9406159073114395, Final Batch Loss: 0.3839370906352997\n",
      "Epoch 5745, Loss: 0.866062194108963, Final Batch Loss: 0.21337728202342987\n",
      "Epoch 5746, Loss: 0.7976348698139191, Final Batch Loss: 0.1862325817346573\n",
      "Epoch 5747, Loss: 0.8316743522882462, Final Batch Loss: 0.1787419468164444\n",
      "Epoch 5748, Loss: 0.8707504272460938, Final Batch Loss: 0.23277272284030914\n",
      "Epoch 5749, Loss: 0.7483478635549545, Final Batch Loss: 0.12838219106197357\n",
      "Epoch 5750, Loss: 0.7921982705593109, Final Batch Loss: 0.14144690334796906\n",
      "Epoch 5751, Loss: 0.7424061447381973, Final Batch Loss: 0.2200954556465149\n",
      "Epoch 5752, Loss: 0.9155824482440948, Final Batch Loss: 0.30669859051704407\n",
      "Epoch 5753, Loss: 0.9504564702510834, Final Batch Loss: 0.27121222019195557\n",
      "Epoch 5754, Loss: 0.7965157479047775, Final Batch Loss: 0.1886691004037857\n",
      "Epoch 5755, Loss: 0.733452782034874, Final Batch Loss: 0.23793210089206696\n",
      "Epoch 5756, Loss: 0.7281251102685928, Final Batch Loss: 0.169808030128479\n",
      "Epoch 5757, Loss: 0.6739770174026489, Final Batch Loss: 0.15176905691623688\n",
      "Epoch 5758, Loss: 0.7816845774650574, Final Batch Loss: 0.13114072382450104\n",
      "Epoch 5759, Loss: 0.839753583073616, Final Batch Loss: 0.2156430333852768\n",
      "Epoch 5760, Loss: 0.6552997529506683, Final Batch Loss: 0.14384697377681732\n",
      "Epoch 5761, Loss: 0.7735894024372101, Final Batch Loss: 0.19901640713214874\n",
      "Epoch 5762, Loss: 0.8082233518362045, Final Batch Loss: 0.15231749415397644\n",
      "Epoch 5763, Loss: 0.7141397446393967, Final Batch Loss: 0.20260906219482422\n",
      "Epoch 5764, Loss: 0.5729378126561642, Final Batch Loss: 0.03190488740801811\n",
      "Epoch 5765, Loss: 0.7420344650745392, Final Batch Loss: 0.11507350206375122\n",
      "Epoch 5766, Loss: 0.6823163628578186, Final Batch Loss: 0.1732749342918396\n",
      "Epoch 5767, Loss: 0.6985460221767426, Final Batch Loss: 0.1672767996788025\n",
      "Epoch 5768, Loss: 0.6411898955702782, Final Batch Loss: 0.103479765355587\n",
      "Epoch 5769, Loss: 0.7679088860750198, Final Batch Loss: 0.25195643305778503\n",
      "Epoch 5770, Loss: 0.8276541531085968, Final Batch Loss: 0.22231513261795044\n",
      "Epoch 5771, Loss: 0.8563919514417648, Final Batch Loss: 0.1845913678407669\n",
      "Epoch 5772, Loss: 0.7117069810628891, Final Batch Loss: 0.22139056026935577\n",
      "Epoch 5773, Loss: 0.7642820030450821, Final Batch Loss: 0.15113164484500885\n",
      "Epoch 5774, Loss: 1.191194400191307, Final Batch Loss: 0.6093111634254456\n",
      "Epoch 5775, Loss: 0.7459383606910706, Final Batch Loss: 0.23730574548244476\n",
      "Epoch 5776, Loss: 0.6957307904958725, Final Batch Loss: 0.15490944683551788\n",
      "Epoch 5777, Loss: 0.8552421629428864, Final Batch Loss: 0.3087981641292572\n",
      "Epoch 5778, Loss: 0.7303547263145447, Final Batch Loss: 0.18528734147548676\n",
      "Epoch 5779, Loss: 0.7377669662237167, Final Batch Loss: 0.22670261561870575\n",
      "Epoch 5780, Loss: 0.7780274152755737, Final Batch Loss: 0.1607353389263153\n",
      "Epoch 5781, Loss: 0.6793378442525864, Final Batch Loss: 0.1332636922597885\n",
      "Epoch 5782, Loss: 0.5876593366265297, Final Batch Loss: 0.11149776726961136\n",
      "Epoch 5783, Loss: 0.809309259057045, Final Batch Loss: 0.25987643003463745\n",
      "Epoch 5784, Loss: 0.7531254217028618, Final Batch Loss: 0.11896904557943344\n",
      "Epoch 5785, Loss: 0.7126687914133072, Final Batch Loss: 0.15496429800987244\n",
      "Epoch 5786, Loss: 0.6661155745387077, Final Batch Loss: 0.08554352074861526\n",
      "Epoch 5787, Loss: 0.8589923232793808, Final Batch Loss: 0.33649173378944397\n",
      "Epoch 5788, Loss: 0.6853231489658356, Final Batch Loss: 0.12683828175067902\n",
      "Epoch 5789, Loss: 0.8442801237106323, Final Batch Loss: 0.17852742969989777\n",
      "Epoch 5790, Loss: 0.9735482186079025, Final Batch Loss: 0.39501360058784485\n",
      "Epoch 5791, Loss: 0.6036433503031731, Final Batch Loss: 0.09362771362066269\n",
      "Epoch 5792, Loss: 0.7407439053058624, Final Batch Loss: 0.18344314396381378\n",
      "Epoch 5793, Loss: 0.7547220438718796, Final Batch Loss: 0.08883652091026306\n",
      "Epoch 5794, Loss: 0.7367453202605247, Final Batch Loss: 0.10143833607435226\n",
      "Epoch 5795, Loss: 0.8992659002542496, Final Batch Loss: 0.272749125957489\n",
      "Epoch 5796, Loss: 0.6909397840499878, Final Batch Loss: 0.16012480854988098\n",
      "Epoch 5797, Loss: 0.6325940042734146, Final Batch Loss: 0.129033163189888\n",
      "Epoch 5798, Loss: 0.7069423124194145, Final Batch Loss: 0.08897355943918228\n",
      "Epoch 5799, Loss: 0.7914291322231293, Final Batch Loss: 0.2136927843093872\n",
      "Epoch 5800, Loss: 0.8792988359928131, Final Batch Loss: 0.30618423223495483\n",
      "Epoch 5801, Loss: 0.7764108031988144, Final Batch Loss: 0.1816137582063675\n",
      "Epoch 5802, Loss: 0.6993998140096664, Final Batch Loss: 0.1588611751794815\n",
      "Epoch 5803, Loss: 0.901959627866745, Final Batch Loss: 0.2762802541255951\n",
      "Epoch 5804, Loss: 0.6578884720802307, Final Batch Loss: 0.13081099092960358\n",
      "Epoch 5805, Loss: 0.7492230981588364, Final Batch Loss: 0.18986396491527557\n",
      "Epoch 5806, Loss: 0.6977880448102951, Final Batch Loss: 0.09341442584991455\n",
      "Epoch 5807, Loss: 0.7536642849445343, Final Batch Loss: 0.09654033184051514\n",
      "Epoch 5808, Loss: 0.7094282358884811, Final Batch Loss: 0.17541714012622833\n",
      "Epoch 5809, Loss: 0.7385739237070084, Final Batch Loss: 0.18594498932361603\n",
      "Epoch 5810, Loss: 0.671635203063488, Final Batch Loss: 0.06300882250070572\n",
      "Epoch 5811, Loss: 0.6380429416894913, Final Batch Loss: 0.12570224702358246\n",
      "Epoch 5812, Loss: 0.6879246532917023, Final Batch Loss: 0.13043303787708282\n",
      "Epoch 5813, Loss: 0.6554482877254486, Final Batch Loss: 0.22450359165668488\n",
      "Epoch 5814, Loss: 0.8431855291128159, Final Batch Loss: 0.27714815735816956\n",
      "Epoch 5815, Loss: 0.7414226830005646, Final Batch Loss: 0.16602566838264465\n",
      "Epoch 5816, Loss: 0.7942773252725601, Final Batch Loss: 0.19584746658802032\n",
      "Epoch 5817, Loss: 0.7053562998771667, Final Batch Loss: 0.12530101835727692\n",
      "Epoch 5818, Loss: 0.8143466264009476, Final Batch Loss: 0.24496696889400482\n",
      "Epoch 5819, Loss: 0.7034830003976822, Final Batch Loss: 0.22317862510681152\n",
      "Epoch 5820, Loss: 0.7944061607122421, Final Batch Loss: 0.274348646402359\n",
      "Epoch 5821, Loss: 0.7276711165904999, Final Batch Loss: 0.19210390746593475\n",
      "Epoch 5822, Loss: 0.8085939139127731, Final Batch Loss: 0.19268928468227386\n",
      "Epoch 5823, Loss: 0.7103291153907776, Final Batch Loss: 0.2085699886083603\n",
      "Epoch 5824, Loss: 0.7385477647185326, Final Batch Loss: 0.08568062633275986\n",
      "Epoch 5825, Loss: 0.7213411182165146, Final Batch Loss: 0.1834927201271057\n",
      "Epoch 5826, Loss: 0.711927518248558, Final Batch Loss: 0.17619644105434418\n",
      "Epoch 5827, Loss: 0.6955525577068329, Final Batch Loss: 0.1338186413049698\n",
      "Epoch 5828, Loss: 0.6472067385911942, Final Batch Loss: 0.13001684844493866\n",
      "Epoch 5829, Loss: 0.6393677741289139, Final Batch Loss: 0.12959247827529907\n",
      "Epoch 5830, Loss: 0.8305472135543823, Final Batch Loss: 0.23320011794567108\n",
      "Epoch 5831, Loss: 0.6700269430875778, Final Batch Loss: 0.10915780067443848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5832, Loss: 0.6324765719473362, Final Batch Loss: 0.056018877774477005\n",
      "Epoch 5833, Loss: 0.6885442286729813, Final Batch Loss: 0.15630997717380524\n",
      "Epoch 5834, Loss: 0.7005033865571022, Final Batch Loss: 0.10540283471345901\n",
      "Epoch 5835, Loss: 0.7522236704826355, Final Batch Loss: 0.23352275788784027\n",
      "Epoch 5836, Loss: 0.7075655609369278, Final Batch Loss: 0.1724836379289627\n",
      "Epoch 5837, Loss: 0.6934056505560875, Final Batch Loss: 0.2666396498680115\n",
      "Epoch 5838, Loss: 0.7207550257444382, Final Batch Loss: 0.1723344475030899\n",
      "Epoch 5839, Loss: 0.6991896703839302, Final Batch Loss: 0.12162365764379501\n",
      "Epoch 5840, Loss: 0.732027143239975, Final Batch Loss: 0.14659282565116882\n",
      "Epoch 5841, Loss: 0.6615201681852341, Final Batch Loss: 0.15514682233333588\n",
      "Epoch 5842, Loss: 0.6699189022183418, Final Batch Loss: 0.09253425151109695\n",
      "Epoch 5843, Loss: 0.956745907664299, Final Batch Loss: 0.3567357361316681\n",
      "Epoch 5844, Loss: 0.7044435441493988, Final Batch Loss: 0.13668206334114075\n",
      "Epoch 5845, Loss: 0.5243697986006737, Final Batch Loss: 0.08683241158723831\n",
      "Epoch 5846, Loss: 0.7320592105388641, Final Batch Loss: 0.15135617554187775\n",
      "Epoch 5847, Loss: 0.6578339636325836, Final Batch Loss: 0.11594182252883911\n",
      "Epoch 5848, Loss: 0.7822245061397552, Final Batch Loss: 0.1998031884431839\n",
      "Epoch 5849, Loss: 0.9666385501623154, Final Batch Loss: 0.41325104236602783\n",
      "Epoch 5850, Loss: 0.7068658247590065, Final Batch Loss: 0.10960619896650314\n",
      "Epoch 5851, Loss: 0.7660302072763443, Final Batch Loss: 0.1250864416360855\n",
      "Epoch 5852, Loss: 0.8185897022485733, Final Batch Loss: 0.21177421510219574\n",
      "Epoch 5853, Loss: 0.8415295779705048, Final Batch Loss: 0.1857297420501709\n",
      "Epoch 5854, Loss: 0.6599927246570587, Final Batch Loss: 0.09338438510894775\n",
      "Epoch 5855, Loss: 0.9191367030143738, Final Batch Loss: 0.39958319067955017\n",
      "Epoch 5856, Loss: 0.6949648261070251, Final Batch Loss: 0.19827860593795776\n",
      "Epoch 5857, Loss: 0.872566893696785, Final Batch Loss: 0.3439891040325165\n",
      "Epoch 5858, Loss: 0.8573329448699951, Final Batch Loss: 0.2848803400993347\n",
      "Epoch 5859, Loss: 0.8038712441921234, Final Batch Loss: 0.22764980792999268\n",
      "Epoch 5860, Loss: 0.7474710494279861, Final Batch Loss: 0.2976803183555603\n",
      "Epoch 5861, Loss: 0.7576256096363068, Final Batch Loss: 0.17852997779846191\n",
      "Epoch 5862, Loss: 0.6658134236931801, Final Batch Loss: 0.11109527200460434\n",
      "Epoch 5863, Loss: 0.8104958459734917, Final Batch Loss: 0.3093518018722534\n",
      "Epoch 5864, Loss: 0.8135587573051453, Final Batch Loss: 0.19260692596435547\n",
      "Epoch 5865, Loss: 0.7372943013906479, Final Batch Loss: 0.2088383287191391\n",
      "Epoch 5866, Loss: 0.8998450189828873, Final Batch Loss: 0.36653628945350647\n",
      "Epoch 5867, Loss: 0.8346530497074127, Final Batch Loss: 0.26310086250305176\n",
      "Epoch 5868, Loss: 0.7045039236545563, Final Batch Loss: 0.18875128030776978\n",
      "Epoch 5869, Loss: 0.8163889795541763, Final Batch Loss: 0.1336166262626648\n",
      "Epoch 5870, Loss: 0.7281859964132309, Final Batch Loss: 0.19488728046417236\n",
      "Epoch 5871, Loss: 0.7638916820287704, Final Batch Loss: 0.14775945246219635\n",
      "Epoch 5872, Loss: 0.7891763001680374, Final Batch Loss: 0.19615471363067627\n",
      "Epoch 5873, Loss: 0.8377732932567596, Final Batch Loss: 0.12574750185012817\n",
      "Epoch 5874, Loss: 0.6444040387868881, Final Batch Loss: 0.08561795949935913\n",
      "Epoch 5875, Loss: 0.907746747136116, Final Batch Loss: 0.15627537667751312\n",
      "Epoch 5876, Loss: 0.7236507311463356, Final Batch Loss: 0.12272308021783829\n",
      "Epoch 5877, Loss: 0.7161929756402969, Final Batch Loss: 0.1512213945388794\n",
      "Epoch 5878, Loss: 0.8866187185049057, Final Batch Loss: 0.31933483481407166\n",
      "Epoch 5879, Loss: 0.7281797528266907, Final Batch Loss: 0.21956439316272736\n",
      "Epoch 5880, Loss: 1.133770927786827, Final Batch Loss: 0.5698742270469666\n",
      "Epoch 5881, Loss: 0.7660785466432571, Final Batch Loss: 0.2064821720123291\n",
      "Epoch 5882, Loss: 0.6036064177751541, Final Batch Loss: 0.07055553793907166\n",
      "Epoch 5883, Loss: 0.7283495217561722, Final Batch Loss: 0.11575508117675781\n",
      "Epoch 5884, Loss: 0.9249242022633553, Final Batch Loss: 0.4033162295818329\n",
      "Epoch 5885, Loss: 0.6843523979187012, Final Batch Loss: 0.17225487530231476\n",
      "Epoch 5886, Loss: 0.7113661915063858, Final Batch Loss: 0.12957102060317993\n",
      "Epoch 5887, Loss: 0.6468473374843597, Final Batch Loss: 0.05874989926815033\n",
      "Epoch 5888, Loss: 0.8512518554925919, Final Batch Loss: 0.21887148916721344\n",
      "Epoch 5889, Loss: 0.8561060875654221, Final Batch Loss: 0.26808279752731323\n",
      "Epoch 5890, Loss: 0.8254291117191315, Final Batch Loss: 0.3082604706287384\n",
      "Epoch 5891, Loss: 0.6478075943887234, Final Batch Loss: 0.03536350652575493\n",
      "Epoch 5892, Loss: 0.791419267654419, Final Batch Loss: 0.07681387662887573\n",
      "Epoch 5893, Loss: 0.7065951973199844, Final Batch Loss: 0.20641465485095978\n",
      "Epoch 5894, Loss: 0.7607898414134979, Final Batch Loss: 0.15727479755878448\n",
      "Epoch 5895, Loss: 0.7163209617137909, Final Batch Loss: 0.147522434592247\n",
      "Epoch 5896, Loss: 0.8604936599731445, Final Batch Loss: 0.2812594771385193\n",
      "Epoch 5897, Loss: 0.8322102278470993, Final Batch Loss: 0.23166126012802124\n",
      "Epoch 5898, Loss: 0.8198630213737488, Final Batch Loss: 0.245299831032753\n",
      "Epoch 5899, Loss: 0.7657720446586609, Final Batch Loss: 0.17482425272464752\n",
      "Epoch 5900, Loss: 0.8200806528329849, Final Batch Loss: 0.19492881000041962\n",
      "Epoch 5901, Loss: 0.9074821025133133, Final Batch Loss: 0.2432987242937088\n",
      "Epoch 5902, Loss: 0.8741747289896011, Final Batch Loss: 0.32505539059638977\n",
      "Epoch 5903, Loss: 0.6139648929238319, Final Batch Loss: 0.07739215344190598\n",
      "Epoch 5904, Loss: 0.6132739931344986, Final Batch Loss: 0.09899041056632996\n",
      "Epoch 5905, Loss: 0.8350955247879028, Final Batch Loss: 0.1467774659395218\n",
      "Epoch 5906, Loss: 0.7461136728525162, Final Batch Loss: 0.12588946521282196\n",
      "Epoch 5907, Loss: 0.7394190281629562, Final Batch Loss: 0.2557912766933441\n",
      "Epoch 5908, Loss: 0.812333419919014, Final Batch Loss: 0.2122248411178589\n",
      "Epoch 5909, Loss: 0.7095858454704285, Final Batch Loss: 0.22513514757156372\n",
      "Epoch 5910, Loss: 0.711312010884285, Final Batch Loss: 0.1405869424343109\n",
      "Epoch 5911, Loss: 0.6886600106954575, Final Batch Loss: 0.17611157894134521\n",
      "Epoch 5912, Loss: 0.7558738589286804, Final Batch Loss: 0.13904689252376556\n",
      "Epoch 5913, Loss: 0.5907610505819321, Final Batch Loss: 0.07644546031951904\n",
      "Epoch 5914, Loss: 0.7315427660942078, Final Batch Loss: 0.15620256960391998\n",
      "Epoch 5915, Loss: 0.7574442625045776, Final Batch Loss: 0.1856354922056198\n",
      "Epoch 5916, Loss: 0.6798286139965057, Final Batch Loss: 0.14944447576999664\n",
      "Epoch 5917, Loss: 0.6562977433204651, Final Batch Loss: 0.15399713814258575\n",
      "Epoch 5918, Loss: 0.7751306295394897, Final Batch Loss: 0.23000949621200562\n",
      "Epoch 5919, Loss: 0.7345816344022751, Final Batch Loss: 0.13347995281219482\n",
      "Epoch 5920, Loss: 0.80653215944767, Final Batch Loss: 0.217375710606575\n",
      "Epoch 5921, Loss: 0.7450583577156067, Final Batch Loss: 0.17953936755657196\n",
      "Epoch 5922, Loss: 0.7035619542002678, Final Batch Loss: 0.0953618511557579\n",
      "Epoch 5923, Loss: 0.7525353133678436, Final Batch Loss: 0.21777848899364471\n",
      "Epoch 5924, Loss: 0.6559473425149918, Final Batch Loss: 0.07040426135063171\n",
      "Epoch 5925, Loss: 0.6856745854020119, Final Batch Loss: 0.11455178260803223\n",
      "Epoch 5926, Loss: 0.7815912812948227, Final Batch Loss: 0.19414597749710083\n",
      "Epoch 5927, Loss: 0.6789404600858688, Final Batch Loss: 0.16501860320568085\n",
      "Epoch 5928, Loss: 0.7503352016210556, Final Batch Loss: 0.10238024592399597\n",
      "Epoch 5929, Loss: 0.9064226001501083, Final Batch Loss: 0.4081690311431885\n",
      "Epoch 5930, Loss: 0.7627030909061432, Final Batch Loss: 0.19811517000198364\n",
      "Epoch 5931, Loss: 0.7824579328298569, Final Batch Loss: 0.19972944259643555\n",
      "Epoch 5932, Loss: 0.747053861618042, Final Batch Loss: 0.13052977621555328\n",
      "Epoch 5933, Loss: 0.7601122707128525, Final Batch Loss: 0.18523721396923065\n",
      "Epoch 5934, Loss: 0.7708699107170105, Final Batch Loss: 0.13629373908042908\n",
      "Epoch 5935, Loss: 0.7647600919008255, Final Batch Loss: 0.25343549251556396\n",
      "Epoch 5936, Loss: 0.8206401765346527, Final Batch Loss: 0.1874093860387802\n",
      "Epoch 5937, Loss: 0.8419531881809235, Final Batch Loss: 0.2498900294303894\n",
      "Epoch 5938, Loss: 0.8130258172750473, Final Batch Loss: 0.26662498712539673\n",
      "Epoch 5939, Loss: 0.6848362758755684, Final Batch Loss: 0.11823868006467819\n",
      "Epoch 5940, Loss: 0.8631809800863266, Final Batch Loss: 0.2900833189487457\n",
      "Epoch 5941, Loss: 0.7797329872846603, Final Batch Loss: 0.19840854406356812\n",
      "Epoch 5942, Loss: 0.684206411242485, Final Batch Loss: 0.15667352080345154\n",
      "Epoch 5943, Loss: 0.759601429104805, Final Batch Loss: 0.16823238134384155\n",
      "Epoch 5944, Loss: 0.7015203684568405, Final Batch Loss: 0.17304502427577972\n",
      "Epoch 5945, Loss: 0.7144366949796677, Final Batch Loss: 0.1718641072511673\n",
      "Epoch 5946, Loss: 0.7648919373750687, Final Batch Loss: 0.2091027945280075\n",
      "Epoch 5947, Loss: 0.9240037500858307, Final Batch Loss: 0.22404150664806366\n",
      "Epoch 5948, Loss: 0.7018443495035172, Final Batch Loss: 0.17409281432628632\n",
      "Epoch 5949, Loss: 0.7752355188131332, Final Batch Loss: 0.1634935438632965\n",
      "Epoch 5950, Loss: 0.9168858677148819, Final Batch Loss: 0.369250625371933\n",
      "Epoch 5951, Loss: 0.755857914686203, Final Batch Loss: 0.17260466516017914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5952, Loss: 0.8333892077207565, Final Batch Loss: 0.29737627506256104\n",
      "Epoch 5953, Loss: 0.7178509533405304, Final Batch Loss: 0.2648787498474121\n",
      "Epoch 5954, Loss: 0.7235163748264313, Final Batch Loss: 0.24121026694774628\n",
      "Epoch 5955, Loss: 0.6701826006174088, Final Batch Loss: 0.12585116922855377\n",
      "Epoch 5956, Loss: 0.7893603593111038, Final Batch Loss: 0.2511664628982544\n",
      "Epoch 5957, Loss: 0.7655558884143829, Final Batch Loss: 0.2111273854970932\n",
      "Epoch 5958, Loss: 0.8027520477771759, Final Batch Loss: 0.1927403062582016\n",
      "Epoch 5959, Loss: 0.8023108243942261, Final Batch Loss: 0.1940428763628006\n",
      "Epoch 5960, Loss: 0.6584293991327286, Final Batch Loss: 0.1537010818719864\n",
      "Epoch 5961, Loss: 0.6430234536528587, Final Batch Loss: 0.08331712335348129\n",
      "Epoch 5962, Loss: 0.7406264692544937, Final Batch Loss: 0.12724944949150085\n",
      "Epoch 5963, Loss: 0.7728588134050369, Final Batch Loss: 0.17451120913028717\n",
      "Epoch 5964, Loss: 0.5978545174002647, Final Batch Loss: 0.07863909751176834\n",
      "Epoch 5965, Loss: 0.6543976068496704, Final Batch Loss: 0.17291706800460815\n",
      "Epoch 5966, Loss: 0.7723316699266434, Final Batch Loss: 0.25389084219932556\n",
      "Epoch 5967, Loss: 0.7973454296588898, Final Batch Loss: 0.23783476650714874\n",
      "Epoch 5968, Loss: 0.8094363063573837, Final Batch Loss: 0.3026730716228485\n",
      "Epoch 5969, Loss: 0.8478723019361496, Final Batch Loss: 0.2127004861831665\n",
      "Epoch 5970, Loss: 0.6181587427854538, Final Batch Loss: 0.07898862659931183\n",
      "Epoch 5971, Loss: 0.7368630915880203, Final Batch Loss: 0.21824105083942413\n",
      "Epoch 5972, Loss: 0.824015274643898, Final Batch Loss: 0.21055828034877777\n",
      "Epoch 5973, Loss: 0.7561474442481995, Final Batch Loss: 0.17878775298595428\n",
      "Epoch 5974, Loss: 0.7757106274366379, Final Batch Loss: 0.1995091289281845\n",
      "Epoch 5975, Loss: 0.6881812065839767, Final Batch Loss: 0.14157728850841522\n",
      "Epoch 5976, Loss: 0.814905509352684, Final Batch Loss: 0.17675651609897614\n",
      "Epoch 5977, Loss: 0.7391908764839172, Final Batch Loss: 0.15349914133548737\n",
      "Epoch 5978, Loss: 0.842744842171669, Final Batch Loss: 0.17453742027282715\n",
      "Epoch 5979, Loss: 0.7685147598385811, Final Batch Loss: 0.1155371442437172\n",
      "Epoch 5980, Loss: 0.6718315854668617, Final Batch Loss: 0.14954937994480133\n",
      "Epoch 5981, Loss: 0.6661332100629807, Final Batch Loss: 0.09237194061279297\n",
      "Epoch 5982, Loss: 0.6762716770172119, Final Batch Loss: 0.20790649950504303\n",
      "Epoch 5983, Loss: 0.9106086492538452, Final Batch Loss: 0.26223263144493103\n",
      "Epoch 5984, Loss: 0.8503939062356949, Final Batch Loss: 0.2686914801597595\n",
      "Epoch 5985, Loss: 0.5690756067633629, Final Batch Loss: 0.13073962926864624\n",
      "Epoch 5986, Loss: 0.700703501701355, Final Batch Loss: 0.16521593928337097\n",
      "Epoch 5987, Loss: 0.8806476891040802, Final Batch Loss: 0.3666619062423706\n",
      "Epoch 5988, Loss: 0.8193409591913223, Final Batch Loss: 0.12848179042339325\n",
      "Epoch 5989, Loss: 0.8323814421892166, Final Batch Loss: 0.1672738641500473\n",
      "Epoch 5990, Loss: 0.7246986925601959, Final Batch Loss: 0.18098072707653046\n",
      "Epoch 5991, Loss: 0.6574098765850067, Final Batch Loss: 0.14972291886806488\n",
      "Epoch 5992, Loss: 0.6266346015036106, Final Batch Loss: 0.03824811056256294\n",
      "Epoch 5993, Loss: 0.7731391191482544, Final Batch Loss: 0.21884626150131226\n",
      "Epoch 5994, Loss: 0.6521009057760239, Final Batch Loss: 0.11905831098556519\n",
      "Epoch 5995, Loss: 0.8752762228250504, Final Batch Loss: 0.349094957113266\n",
      "Epoch 5996, Loss: 0.7459727674722672, Final Batch Loss: 0.22726373374462128\n",
      "Epoch 5997, Loss: 0.8296913951635361, Final Batch Loss: 0.2955116927623749\n",
      "Epoch 5998, Loss: 0.7005726397037506, Final Batch Loss: 0.18831342458724976\n",
      "Epoch 5999, Loss: 0.7497918084263802, Final Batch Loss: 0.21887314319610596\n",
      "Epoch 6000, Loss: 0.6663610190153122, Final Batch Loss: 0.08124640583992004\n",
      "Epoch 6001, Loss: 0.6036057956516743, Final Batch Loss: 0.06207389011979103\n",
      "Epoch 6002, Loss: 0.6794935390353203, Final Batch Loss: 0.09643811732530594\n",
      "Epoch 6003, Loss: 0.820464700460434, Final Batch Loss: 0.23850278556346893\n",
      "Epoch 6004, Loss: 0.6938261538743973, Final Batch Loss: 0.14293745160102844\n",
      "Epoch 6005, Loss: 0.7939717918634415, Final Batch Loss: 0.14651615917682648\n",
      "Epoch 6006, Loss: 0.7067592740058899, Final Batch Loss: 0.1740921288728714\n",
      "Epoch 6007, Loss: 0.722405731678009, Final Batch Loss: 0.2080933302640915\n",
      "Epoch 6008, Loss: 0.8234543353319168, Final Batch Loss: 0.24196232855319977\n",
      "Epoch 6009, Loss: 0.6475674957036972, Final Batch Loss: 0.1564086228609085\n",
      "Epoch 6010, Loss: 0.6351137459278107, Final Batch Loss: 0.18160299956798553\n",
      "Epoch 6011, Loss: 0.6462678574025631, Final Batch Loss: 0.0592339001595974\n",
      "Epoch 6012, Loss: 0.6884195357561111, Final Batch Loss: 0.17221902310848236\n",
      "Epoch 6013, Loss: 0.810052677989006, Final Batch Loss: 0.3280182182788849\n",
      "Epoch 6014, Loss: 0.6810244470834732, Final Batch Loss: 0.14357168972492218\n",
      "Epoch 6015, Loss: 0.6920635998249054, Final Batch Loss: 0.16668100655078888\n",
      "Epoch 6016, Loss: 0.6412153244018555, Final Batch Loss: 0.09872862696647644\n",
      "Epoch 6017, Loss: 0.7963635325431824, Final Batch Loss: 0.15680719912052155\n",
      "Epoch 6018, Loss: 0.8817905411124229, Final Batch Loss: 0.3234286606311798\n",
      "Epoch 6019, Loss: 0.759838804602623, Final Batch Loss: 0.2155310958623886\n",
      "Epoch 6020, Loss: 0.8874492049217224, Final Batch Loss: 0.19787418842315674\n",
      "Epoch 6021, Loss: 1.0226422399282455, Final Batch Loss: 0.4376774728298187\n",
      "Epoch 6022, Loss: 0.7019671350717545, Final Batch Loss: 0.13550512492656708\n",
      "Epoch 6023, Loss: 0.7729143649339676, Final Batch Loss: 0.14085371792316437\n",
      "Epoch 6024, Loss: 0.8307343274354935, Final Batch Loss: 0.3399968445301056\n",
      "Epoch 6025, Loss: 0.8342655450105667, Final Batch Loss: 0.2859090566635132\n",
      "Epoch 6026, Loss: 0.7795204520225525, Final Batch Loss: 0.19181257486343384\n",
      "Epoch 6027, Loss: 0.795286700129509, Final Batch Loss: 0.236736461520195\n",
      "Epoch 6028, Loss: 0.715879887342453, Final Batch Loss: 0.15895570814609528\n",
      "Epoch 6029, Loss: 0.8424896746873856, Final Batch Loss: 0.22202984988689423\n",
      "Epoch 6030, Loss: 0.8293496370315552, Final Batch Loss: 0.2242414355278015\n",
      "Epoch 6031, Loss: 0.7891264855861664, Final Batch Loss: 0.224492609500885\n",
      "Epoch 6032, Loss: 0.8378964066505432, Final Batch Loss: 0.2760036289691925\n",
      "Epoch 6033, Loss: 0.8258031606674194, Final Batch Loss: 0.1537066549062729\n",
      "Epoch 6034, Loss: 0.7315318733453751, Final Batch Loss: 0.1407403200864792\n",
      "Epoch 6035, Loss: 0.6333579495549202, Final Batch Loss: 0.12237899750471115\n",
      "Epoch 6036, Loss: 0.7303315103054047, Final Batch Loss: 0.15381836891174316\n",
      "Epoch 6037, Loss: 0.7939703017473221, Final Batch Loss: 0.2967188060283661\n",
      "Epoch 6038, Loss: 0.7180736660957336, Final Batch Loss: 0.22262592613697052\n",
      "Epoch 6039, Loss: 0.7274781614542007, Final Batch Loss: 0.15463876724243164\n",
      "Epoch 6040, Loss: 0.6687619760632515, Final Batch Loss: 0.2046583890914917\n",
      "Epoch 6041, Loss: 0.6583876758813858, Final Batch Loss: 0.15073780715465546\n",
      "Epoch 6042, Loss: 0.5877306684851646, Final Batch Loss: 0.08095330744981766\n",
      "Epoch 6043, Loss: 0.8905003368854523, Final Batch Loss: 0.2677310109138489\n",
      "Epoch 6044, Loss: 0.8709921091794968, Final Batch Loss: 0.243923619389534\n",
      "Epoch 6045, Loss: 0.6550397202372551, Final Batch Loss: 0.0649620071053505\n",
      "Epoch 6046, Loss: 0.7110155671834946, Final Batch Loss: 0.13242052495479584\n",
      "Epoch 6047, Loss: 0.8614394068717957, Final Batch Loss: 0.264630526304245\n",
      "Epoch 6048, Loss: 0.9485861510038376, Final Batch Loss: 0.3843434154987335\n",
      "Epoch 6049, Loss: 0.6477153599262238, Final Batch Loss: 0.08293665945529938\n",
      "Epoch 6050, Loss: 0.5741407833993435, Final Batch Loss: 0.059625979512929916\n",
      "Epoch 6051, Loss: 0.7996833026409149, Final Batch Loss: 0.17118321359157562\n",
      "Epoch 6052, Loss: 0.8988527208566666, Final Batch Loss: 0.2946203947067261\n",
      "Epoch 6053, Loss: 0.6390897296369076, Final Batch Loss: 0.04158620163798332\n",
      "Epoch 6054, Loss: 1.019112467765808, Final Batch Loss: 0.4390881061553955\n",
      "Epoch 6055, Loss: 0.7254282385110855, Final Batch Loss: 0.1849634200334549\n",
      "Epoch 6056, Loss: 0.7311185449361801, Final Batch Loss: 0.19054269790649414\n",
      "Epoch 6057, Loss: 0.7010721378028393, Final Batch Loss: 0.047054704278707504\n",
      "Epoch 6058, Loss: 0.7698678225278854, Final Batch Loss: 0.2558573782444\n",
      "Epoch 6059, Loss: 0.6967921257019043, Final Batch Loss: 0.17523978650569916\n",
      "Epoch 6060, Loss: 0.7612739950418472, Final Batch Loss: 0.18612365424633026\n",
      "Epoch 6061, Loss: 0.597321443259716, Final Batch Loss: 0.15185604989528656\n",
      "Epoch 6062, Loss: 0.8468934148550034, Final Batch Loss: 0.2581760585308075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6063, Loss: 0.7738873809576035, Final Batch Loss: 0.2863369584083557\n",
      "Epoch 6064, Loss: 0.9511523842811584, Final Batch Loss: 0.34950706362724304\n",
      "Epoch 6065, Loss: 0.7198437005281448, Final Batch Loss: 0.13016800582408905\n",
      "Epoch 6066, Loss: 0.7708205133676529, Final Batch Loss: 0.17594800889492035\n",
      "Epoch 6067, Loss: 0.7728869915008545, Final Batch Loss: 0.20503537356853485\n",
      "Epoch 6068, Loss: 0.7364725172519684, Final Batch Loss: 0.16065005958080292\n",
      "Epoch 6069, Loss: 0.8599593639373779, Final Batch Loss: 0.28171274065971375\n",
      "Epoch 6070, Loss: 0.5770115554332733, Final Batch Loss: 0.13079364597797394\n",
      "Epoch 6071, Loss: 0.8162067234516144, Final Batch Loss: 0.21605689823627472\n",
      "Epoch 6072, Loss: 0.8072731196880341, Final Batch Loss: 0.23051966726779938\n",
      "Epoch 6073, Loss: 0.6365174055099487, Final Batch Loss: 0.0873333215713501\n",
      "Epoch 6074, Loss: 0.7030940055847168, Final Batch Loss: 0.1280450075864792\n",
      "Epoch 6075, Loss: 0.7558071613311768, Final Batch Loss: 0.17062263190746307\n",
      "Epoch 6076, Loss: 0.6566305682063103, Final Batch Loss: 0.10842777043581009\n",
      "Epoch 6077, Loss: 0.8225457966327667, Final Batch Loss: 0.229029580950737\n",
      "Epoch 6078, Loss: 0.663552351295948, Final Batch Loss: 0.08395528048276901\n",
      "Epoch 6079, Loss: 0.7115601152181625, Final Batch Loss: 0.16615347564220428\n",
      "Epoch 6080, Loss: 0.8318207785487175, Final Batch Loss: 0.3372138440608978\n",
      "Epoch 6081, Loss: 0.7201279252767563, Final Batch Loss: 0.11807870864868164\n",
      "Epoch 6082, Loss: 0.717956930398941, Final Batch Loss: 0.17228837311267853\n",
      "Epoch 6083, Loss: 0.6532948315143585, Final Batch Loss: 0.21092450618743896\n",
      "Epoch 6084, Loss: 0.615168284624815, Final Batch Loss: 0.03785991296172142\n",
      "Epoch 6085, Loss: 0.7607798129320145, Final Batch Loss: 0.15956293046474457\n",
      "Epoch 6086, Loss: 0.8284031897783279, Final Batch Loss: 0.28608831763267517\n",
      "Epoch 6087, Loss: 0.6467754170298576, Final Batch Loss: 0.12374158948659897\n",
      "Epoch 6088, Loss: 0.7911894619464874, Final Batch Loss: 0.19472068548202515\n",
      "Epoch 6089, Loss: 0.6291643530130386, Final Batch Loss: 0.12643863260746002\n",
      "Epoch 6090, Loss: 0.9197562336921692, Final Batch Loss: 0.26295915246009827\n",
      "Epoch 6091, Loss: 0.6564199849963188, Final Batch Loss: 0.10727445036172867\n",
      "Epoch 6092, Loss: 0.6982928663492203, Final Batch Loss: 0.1468629688024521\n",
      "Epoch 6093, Loss: 0.6978812515735626, Final Batch Loss: 0.16874365508556366\n",
      "Epoch 6094, Loss: 0.6465697810053825, Final Batch Loss: 0.10763827711343765\n",
      "Epoch 6095, Loss: 1.0054097175598145, Final Batch Loss: 0.30682578682899475\n",
      "Epoch 6096, Loss: 0.7215776890516281, Final Batch Loss: 0.22005200386047363\n",
      "Epoch 6097, Loss: 0.767877072095871, Final Batch Loss: 0.21216315031051636\n",
      "Epoch 6098, Loss: 0.8655093461275101, Final Batch Loss: 0.1627887636423111\n",
      "Epoch 6099, Loss: 0.6236041486263275, Final Batch Loss: 0.14020241796970367\n",
      "Epoch 6100, Loss: 0.7139111161231995, Final Batch Loss: 0.21656960248947144\n",
      "Epoch 6101, Loss: 0.7691110223531723, Final Batch Loss: 0.13419316709041595\n",
      "Epoch 6102, Loss: 0.6411715522408485, Final Batch Loss: 0.17430616915225983\n",
      "Epoch 6103, Loss: 0.6710643023252487, Final Batch Loss: 0.1352723091840744\n",
      "Epoch 6104, Loss: 0.628848060965538, Final Batch Loss: 0.07087241113185883\n",
      "Epoch 6105, Loss: 0.7698609083890915, Final Batch Loss: 0.22217726707458496\n",
      "Epoch 6106, Loss: 0.8977734297513962, Final Batch Loss: 0.17269766330718994\n",
      "Epoch 6107, Loss: 0.7987399995326996, Final Batch Loss: 0.11616700887680054\n",
      "Epoch 6108, Loss: 0.5484579764306545, Final Batch Loss: 0.037523817270994186\n",
      "Epoch 6109, Loss: 0.7740332633256912, Final Batch Loss: 0.21352940797805786\n",
      "Epoch 6110, Loss: 0.9325477182865143, Final Batch Loss: 0.22915856540203094\n",
      "Epoch 6111, Loss: 0.948028139770031, Final Batch Loss: 0.09818948060274124\n",
      "Epoch 6112, Loss: 1.2041079699993134, Final Batch Loss: 0.5106869339942932\n",
      "Epoch 6113, Loss: 1.015751674771309, Final Batch Loss: 0.25502488017082214\n",
      "Epoch 6114, Loss: 0.8923456817865372, Final Batch Loss: 0.24478541314601898\n",
      "Epoch 6115, Loss: 1.0362955182790756, Final Batch Loss: 0.2963692247867584\n",
      "Epoch 6116, Loss: 0.995610237121582, Final Batch Loss: 0.2098889797925949\n",
      "Epoch 6117, Loss: 0.9078961610794067, Final Batch Loss: 0.19434334337711334\n",
      "Epoch 6118, Loss: 0.9812509566545486, Final Batch Loss: 0.379715234041214\n",
      "Epoch 6119, Loss: 0.9457884877920151, Final Batch Loss: 0.24983888864517212\n",
      "Epoch 6120, Loss: 1.0164608806371689, Final Batch Loss: 0.30949336290359497\n",
      "Epoch 6121, Loss: 1.0927293300628662, Final Batch Loss: 0.3730778992176056\n",
      "Epoch 6122, Loss: 0.7547276765108109, Final Batch Loss: 0.14651550352573395\n",
      "Epoch 6123, Loss: 1.0770603716373444, Final Batch Loss: 0.30657365918159485\n",
      "Epoch 6124, Loss: 0.8844840228557587, Final Batch Loss: 0.15843291580677032\n",
      "Epoch 6125, Loss: 0.7834424525499344, Final Batch Loss: 0.2257280945777893\n",
      "Epoch 6126, Loss: 1.0642024278640747, Final Batch Loss: 0.33736422657966614\n",
      "Epoch 6127, Loss: 1.1280866116285324, Final Batch Loss: 0.3784034252166748\n",
      "Epoch 6128, Loss: 0.9227616786956787, Final Batch Loss: 0.18836592137813568\n",
      "Epoch 6129, Loss: 0.9140629470348358, Final Batch Loss: 0.1911115050315857\n",
      "Epoch 6130, Loss: 0.8678460568189621, Final Batch Loss: 0.22522906959056854\n",
      "Epoch 6131, Loss: 0.7460539266467094, Final Batch Loss: 0.12263212352991104\n",
      "Epoch 6132, Loss: 0.8421347588300705, Final Batch Loss: 0.19009999930858612\n",
      "Epoch 6133, Loss: 0.8126930892467499, Final Batch Loss: 0.20673729479312897\n",
      "Epoch 6134, Loss: 0.8848330974578857, Final Batch Loss: 0.25168970227241516\n",
      "Epoch 6135, Loss: 0.7822657823562622, Final Batch Loss: 0.19768469035625458\n",
      "Epoch 6136, Loss: 0.967157855629921, Final Batch Loss: 0.2763615548610687\n",
      "Epoch 6137, Loss: 0.7051159590482712, Final Batch Loss: 0.10799700021743774\n",
      "Epoch 6138, Loss: 0.8174216598272324, Final Batch Loss: 0.18374736607074738\n",
      "Epoch 6139, Loss: 0.6713161319494247, Final Batch Loss: 0.07210822403430939\n",
      "Epoch 6140, Loss: 0.8574776947498322, Final Batch Loss: 0.23402386903762817\n",
      "Epoch 6141, Loss: 1.1167435050010681, Final Batch Loss: 0.4517362117767334\n",
      "Epoch 6142, Loss: 0.8646004498004913, Final Batch Loss: 0.345823734998703\n",
      "Epoch 6143, Loss: 1.024718463420868, Final Batch Loss: 0.2154473513364792\n",
      "Epoch 6144, Loss: 0.7836983799934387, Final Batch Loss: 0.1644294112920761\n",
      "Epoch 6145, Loss: 0.8285606801509857, Final Batch Loss: 0.16405044496059418\n",
      "Epoch 6146, Loss: 0.7449158281087875, Final Batch Loss: 0.13728129863739014\n",
      "Epoch 6147, Loss: 0.8757401704788208, Final Batch Loss: 0.2602646052837372\n",
      "Epoch 6148, Loss: 0.7264309823513031, Final Batch Loss: 0.1591663807630539\n",
      "Epoch 6149, Loss: 0.8093784153461456, Final Batch Loss: 0.21339051425457\n",
      "Epoch 6150, Loss: 0.8027807772159576, Final Batch Loss: 0.1902131587266922\n",
      "Epoch 6151, Loss: 0.8329111486673355, Final Batch Loss: 0.19474387168884277\n",
      "Epoch 6152, Loss: 0.6553526818752289, Final Batch Loss: 0.12261110544204712\n",
      "Epoch 6153, Loss: 0.7749523371458054, Final Batch Loss: 0.16870807111263275\n",
      "Epoch 6154, Loss: 0.8922212272882462, Final Batch Loss: 0.26088815927505493\n",
      "Epoch 6155, Loss: 0.7032221332192421, Final Batch Loss: 0.09130150824785233\n",
      "Epoch 6156, Loss: 0.7480309084057808, Final Batch Loss: 0.08612532168626785\n",
      "Epoch 6157, Loss: 0.7145605683326721, Final Batch Loss: 0.15313254296779633\n",
      "Epoch 6158, Loss: 0.7900414615869522, Final Batch Loss: 0.259336918592453\n",
      "Epoch 6159, Loss: 0.7693528383970261, Final Batch Loss: 0.240131676197052\n",
      "Epoch 6160, Loss: 0.8699229806661606, Final Batch Loss: 0.2908324897289276\n",
      "Epoch 6161, Loss: 0.6759215891361237, Final Batch Loss: 0.08838602900505066\n",
      "Epoch 6162, Loss: 0.7460926324129105, Final Batch Loss: 0.13376694917678833\n",
      "Epoch 6163, Loss: 0.9036711007356644, Final Batch Loss: 0.3129529654979706\n",
      "Epoch 6164, Loss: 0.7533571049571037, Final Batch Loss: 0.12355182319879532\n",
      "Epoch 6165, Loss: 0.8567995578050613, Final Batch Loss: 0.09523946046829224\n",
      "Epoch 6166, Loss: 0.8458758443593979, Final Batch Loss: 0.13588747382164001\n",
      "Epoch 6167, Loss: 0.8455092310905457, Final Batch Loss: 0.14960028231143951\n",
      "Epoch 6168, Loss: 0.8549156188964844, Final Batch Loss: 0.21436560153961182\n",
      "Epoch 6169, Loss: 0.8016927987337112, Final Batch Loss: 0.1718846708536148\n",
      "Epoch 6170, Loss: 0.8785627335309982, Final Batch Loss: 0.25015437602996826\n",
      "Epoch 6171, Loss: 0.8231900185346603, Final Batch Loss: 0.25111621618270874\n",
      "Epoch 6172, Loss: 0.8792480528354645, Final Batch Loss: 0.23316943645477295\n",
      "Epoch 6173, Loss: 0.64842090010643, Final Batch Loss: 0.14851276576519012\n",
      "Epoch 6174, Loss: 0.6460888981819153, Final Batch Loss: 0.1453918069601059\n",
      "Epoch 6175, Loss: 0.7572929412126541, Final Batch Loss: 0.14999505877494812\n",
      "Epoch 6176, Loss: 0.9115354269742966, Final Batch Loss: 0.26177021861076355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6177, Loss: 0.9055799841880798, Final Batch Loss: 0.3064119815826416\n",
      "Epoch 6178, Loss: 0.8250184655189514, Final Batch Loss: 0.1755904108285904\n",
      "Epoch 6179, Loss: 0.7138227745890617, Final Batch Loss: 0.11310070008039474\n",
      "Epoch 6180, Loss: 0.6889849007129669, Final Batch Loss: 0.14825738966464996\n",
      "Epoch 6181, Loss: 0.7990357726812363, Final Batch Loss: 0.24932421743869781\n",
      "Epoch 6182, Loss: 0.7645419836044312, Final Batch Loss: 0.24080128967761993\n",
      "Epoch 6183, Loss: 0.8157194778323174, Final Batch Loss: 0.09843894094228745\n",
      "Epoch 6184, Loss: 0.7965060621500015, Final Batch Loss: 0.20363366603851318\n",
      "Epoch 6185, Loss: 0.7593882828950882, Final Batch Loss: 0.07358862459659576\n",
      "Epoch 6186, Loss: 0.7740999907255173, Final Batch Loss: 0.1967463493347168\n",
      "Epoch 6187, Loss: 0.8527403622865677, Final Batch Loss: 0.3273807466030121\n",
      "Epoch 6188, Loss: 0.7619646191596985, Final Batch Loss: 0.13483889400959015\n",
      "Epoch 6189, Loss: 0.7483090460300446, Final Batch Loss: 0.14918385446071625\n",
      "Epoch 6190, Loss: 0.8966923356056213, Final Batch Loss: 0.27056440711021423\n",
      "Epoch 6191, Loss: 0.592685729265213, Final Batch Loss: 0.12728722393512726\n",
      "Epoch 6192, Loss: 0.7882627695798874, Final Batch Loss: 0.17844927310943604\n",
      "Epoch 6193, Loss: 0.7953106164932251, Final Batch Loss: 0.2632771134376526\n",
      "Epoch 6194, Loss: 0.8916426599025726, Final Batch Loss: 0.32492026686668396\n",
      "Epoch 6195, Loss: 0.6617914140224457, Final Batch Loss: 0.20684374868869781\n",
      "Epoch 6196, Loss: 0.7604390382766724, Final Batch Loss: 0.2801819145679474\n",
      "Epoch 6197, Loss: 0.6595321074128151, Final Batch Loss: 0.08696522563695908\n",
      "Epoch 6198, Loss: 0.9459429979324341, Final Batch Loss: 0.14343415200710297\n",
      "Epoch 6199, Loss: 0.731552354991436, Final Batch Loss: 0.09980457276105881\n",
      "Epoch 6200, Loss: 0.6879614740610123, Final Batch Loss: 0.208783820271492\n",
      "Epoch 6201, Loss: 0.8159125298261642, Final Batch Loss: 0.20994792878627777\n",
      "Epoch 6202, Loss: 0.6886304169893265, Final Batch Loss: 0.14318229258060455\n",
      "Epoch 6203, Loss: 0.5802960880100727, Final Batch Loss: 0.060111213475465775\n",
      "Epoch 6204, Loss: 0.771988645195961, Final Batch Loss: 0.2090429663658142\n",
      "Epoch 6205, Loss: 0.6642369292676449, Final Batch Loss: 0.058935146778821945\n",
      "Epoch 6206, Loss: 0.6189857348799706, Final Batch Loss: 0.083063505589962\n",
      "Epoch 6207, Loss: 0.7654194533824921, Final Batch Loss: 0.13966383039951324\n",
      "Epoch 6208, Loss: 0.629752017557621, Final Batch Loss: 0.08456184715032578\n",
      "Epoch 6209, Loss: 0.6281323209404945, Final Batch Loss: 0.11277303844690323\n",
      "Epoch 6210, Loss: 0.6693798303604126, Final Batch Loss: 0.1940138339996338\n",
      "Epoch 6211, Loss: 0.6270215734839439, Final Batch Loss: 0.16905362904071808\n",
      "Epoch 6212, Loss: 0.6052179522812366, Final Batch Loss: 0.03554539754986763\n",
      "Epoch 6213, Loss: 0.6856202930212021, Final Batch Loss: 0.14967206120491028\n",
      "Epoch 6214, Loss: 0.7099395543336868, Final Batch Loss: 0.15783803164958954\n",
      "Epoch 6215, Loss: 0.7561793625354767, Final Batch Loss: 0.24667493999004364\n",
      "Epoch 6216, Loss: 0.684539332985878, Final Batch Loss: 0.1637711077928543\n",
      "Epoch 6217, Loss: 0.6109066382050514, Final Batch Loss: 0.1211669072508812\n",
      "Epoch 6218, Loss: 0.8329729586839676, Final Batch Loss: 0.20333479344844818\n",
      "Epoch 6219, Loss: 0.7697951048612595, Final Batch Loss: 0.1490345001220703\n",
      "Epoch 6220, Loss: 0.6981167793273926, Final Batch Loss: 0.08696568012237549\n",
      "Epoch 6221, Loss: 0.6774175465106964, Final Batch Loss: 0.21127694845199585\n",
      "Epoch 6222, Loss: 0.711673691868782, Final Batch Loss: 0.14087572693824768\n",
      "Epoch 6223, Loss: 0.7637777328491211, Final Batch Loss: 0.16882629692554474\n",
      "Epoch 6224, Loss: 0.7457764595746994, Final Batch Loss: 0.27124324440956116\n",
      "Epoch 6225, Loss: 1.0186108648777008, Final Batch Loss: 0.506818950176239\n",
      "Epoch 6226, Loss: 0.7597754299640656, Final Batch Loss: 0.182536318898201\n",
      "Epoch 6227, Loss: 0.7439903169870377, Final Batch Loss: 0.23283231258392334\n",
      "Epoch 6228, Loss: 0.7155158519744873, Final Batch Loss: 0.23840831220149994\n",
      "Epoch 6229, Loss: 0.661760687828064, Final Batch Loss: 0.15652303397655487\n",
      "Epoch 6230, Loss: 0.8102720379829407, Final Batch Loss: 0.22773826122283936\n",
      "Epoch 6231, Loss: 0.7117680013179779, Final Batch Loss: 0.24752546846866608\n",
      "Epoch 6232, Loss: 0.6742501631379128, Final Batch Loss: 0.07705166190862656\n",
      "Epoch 6233, Loss: 0.8056056052446365, Final Batch Loss: 0.28332701325416565\n",
      "Epoch 6234, Loss: 0.791572816669941, Final Batch Loss: 0.11207089573144913\n",
      "Epoch 6235, Loss: 0.7100395709276199, Final Batch Loss: 0.1815698742866516\n",
      "Epoch 6236, Loss: 0.7176695466041565, Final Batch Loss: 0.1488267332315445\n",
      "Epoch 6237, Loss: 0.8591341227293015, Final Batch Loss: 0.28465262055397034\n",
      "Epoch 6238, Loss: 0.8717876374721527, Final Batch Loss: 0.19684936106204987\n",
      "Epoch 6239, Loss: 0.7724775522947311, Final Batch Loss: 0.21369294822216034\n",
      "Epoch 6240, Loss: 0.718940407037735, Final Batch Loss: 0.2043846845626831\n",
      "Epoch 6241, Loss: 0.7067302390933037, Final Batch Loss: 0.07757212966680527\n",
      "Epoch 6242, Loss: 0.6979602724313736, Final Batch Loss: 0.1935470551252365\n",
      "Epoch 6243, Loss: 0.7575030028820038, Final Batch Loss: 0.12612484395503998\n",
      "Epoch 6244, Loss: 0.6671707630157471, Final Batch Loss: 0.22866962850093842\n",
      "Epoch 6245, Loss: 0.6903678476810455, Final Batch Loss: 0.11730816960334778\n",
      "Epoch 6246, Loss: 0.7746355533599854, Final Batch Loss: 0.24967120587825775\n",
      "Epoch 6247, Loss: 0.6068500876426697, Final Batch Loss: 0.10137087106704712\n",
      "Epoch 6248, Loss: 0.6738820225000381, Final Batch Loss: 0.16155296564102173\n",
      "Epoch 6249, Loss: 0.629889715462923, Final Batch Loss: 0.06063905730843544\n",
      "Epoch 6250, Loss: 0.6414407342672348, Final Batch Loss: 0.1849653273820877\n",
      "Epoch 6251, Loss: 0.6006506457924843, Final Batch Loss: 0.1042131558060646\n",
      "Epoch 6252, Loss: 0.751348152756691, Final Batch Loss: 0.21230196952819824\n",
      "Epoch 6253, Loss: 0.5185348838567734, Final Batch Loss: 0.10689900070428848\n",
      "Epoch 6254, Loss: 0.6342123746871948, Final Batch Loss: 0.16653724014759064\n",
      "Epoch 6255, Loss: 0.6846768110990524, Final Batch Loss: 0.14424170553684235\n",
      "Epoch 6256, Loss: 0.6819191724061966, Final Batch Loss: 0.1722847819328308\n",
      "Epoch 6257, Loss: 0.664882056415081, Final Batch Loss: 0.15033666789531708\n",
      "Epoch 6258, Loss: 0.6578729748725891, Final Batch Loss: 0.10544478893280029\n",
      "Epoch 6259, Loss: 0.5984828993678093, Final Batch Loss: 0.06677254289388657\n",
      "Epoch 6260, Loss: 0.6918638795614243, Final Batch Loss: 0.17034520208835602\n",
      "Epoch 6261, Loss: 0.9935036152601242, Final Batch Loss: 0.38248100876808167\n",
      "Epoch 6262, Loss: 0.6819460988044739, Final Batch Loss: 0.20637869834899902\n",
      "Epoch 6263, Loss: 0.5945827700197697, Final Batch Loss: 0.05136652663350105\n",
      "Epoch 6264, Loss: 0.7685118764638901, Final Batch Loss: 0.18763600289821625\n",
      "Epoch 6265, Loss: 0.6880461126565933, Final Batch Loss: 0.13875152170658112\n",
      "Epoch 6266, Loss: 0.6684466898441315, Final Batch Loss: 0.14555199444293976\n",
      "Epoch 6267, Loss: 0.7860590815544128, Final Batch Loss: 0.15565532445907593\n",
      "Epoch 6268, Loss: 0.7651334702968597, Final Batch Loss: 0.3189404606819153\n",
      "Epoch 6269, Loss: 0.6851381361484528, Final Batch Loss: 0.13803131878376007\n",
      "Epoch 6270, Loss: 0.7168437391519547, Final Batch Loss: 0.16395121812820435\n",
      "Epoch 6271, Loss: 0.6602731049060822, Final Batch Loss: 0.13510434329509735\n",
      "Epoch 6272, Loss: 0.6944055408239365, Final Batch Loss: 0.07767266035079956\n",
      "Epoch 6273, Loss: 0.9169718474149704, Final Batch Loss: 0.27686741948127747\n",
      "Epoch 6274, Loss: 0.8659331500530243, Final Batch Loss: 0.13947637379169464\n",
      "Epoch 6275, Loss: 0.7049900889396667, Final Batch Loss: 0.1365984082221985\n",
      "Epoch 6276, Loss: 0.698615275323391, Final Batch Loss: 0.06340136379003525\n",
      "Epoch 6277, Loss: 0.7696790844202042, Final Batch Loss: 0.22303785383701324\n",
      "Epoch 6278, Loss: 0.7183561697602272, Final Batch Loss: 0.11917034536600113\n",
      "Epoch 6279, Loss: 0.79585762321949, Final Batch Loss: 0.20718629658222198\n",
      "Epoch 6280, Loss: 0.8259929567575455, Final Batch Loss: 0.19161857664585114\n",
      "Epoch 6281, Loss: 0.6508452072739601, Final Batch Loss: 0.07424715906381607\n",
      "Epoch 6282, Loss: 0.9666992425918579, Final Batch Loss: 0.4323408603668213\n",
      "Epoch 6283, Loss: 1.0791620165109634, Final Batch Loss: 0.2996651828289032\n",
      "Epoch 6284, Loss: 0.699077345430851, Final Batch Loss: 0.12076067179441452\n",
      "Epoch 6285, Loss: 0.7218239903450012, Final Batch Loss: 0.13004283607006073\n",
      "Epoch 6286, Loss: 0.8031079918146133, Final Batch Loss: 0.22540225088596344\n",
      "Epoch 6287, Loss: 0.7768440768122673, Final Batch Loss: 0.18507546186447144\n",
      "Epoch 6288, Loss: 0.8249872177839279, Final Batch Loss: 0.22922049462795258\n",
      "Epoch 6289, Loss: 0.6207821369171143, Final Batch Loss: 0.08492383360862732\n",
      "Epoch 6290, Loss: 0.7744890600442886, Final Batch Loss: 0.2794409692287445\n",
      "Epoch 6291, Loss: 0.6599622219800949, Final Batch Loss: 0.15556378662586212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6292, Loss: 1.0917310118675232, Final Batch Loss: 0.48893073201179504\n",
      "Epoch 6293, Loss: 0.8219533562660217, Final Batch Loss: 0.23976635932922363\n",
      "Epoch 6294, Loss: 1.1714523434638977, Final Batch Loss: 0.4201580584049225\n",
      "Epoch 6295, Loss: 0.7064070627093315, Final Batch Loss: 0.15677405893802643\n",
      "Epoch 6296, Loss: 0.6341300755739212, Final Batch Loss: 0.12528198957443237\n",
      "Epoch 6297, Loss: 0.6473290249705315, Final Batch Loss: 0.0780048742890358\n",
      "Epoch 6298, Loss: 0.7458755075931549, Final Batch Loss: 0.11569282412528992\n",
      "Epoch 6299, Loss: 0.6523251086473465, Final Batch Loss: 0.18400681018829346\n",
      "Epoch 6300, Loss: 0.8541244566440582, Final Batch Loss: 0.23586444556713104\n",
      "Epoch 6301, Loss: 0.7579910606145859, Final Batch Loss: 0.16732239723205566\n",
      "Epoch 6302, Loss: 0.7825722694396973, Final Batch Loss: 0.25023993849754333\n",
      "Epoch 6303, Loss: 0.6412699595093727, Final Batch Loss: 0.1965140551328659\n",
      "Epoch 6304, Loss: 0.815644845366478, Final Batch Loss: 0.23742718994617462\n",
      "Epoch 6305, Loss: 0.677461564540863, Final Batch Loss: 0.25263458490371704\n",
      "Epoch 6306, Loss: 0.6943978667259216, Final Batch Loss: 0.12988592684268951\n",
      "Epoch 6307, Loss: 0.8634981960058212, Final Batch Loss: 0.18699288368225098\n",
      "Epoch 6308, Loss: 0.7810859829187393, Final Batch Loss: 0.19627809524536133\n",
      "Epoch 6309, Loss: 0.7294121384620667, Final Batch Loss: 0.1477499008178711\n",
      "Epoch 6310, Loss: 0.6151792332530022, Final Batch Loss: 0.09331536293029785\n",
      "Epoch 6311, Loss: 0.7543375194072723, Final Batch Loss: 0.2107296735048294\n",
      "Epoch 6312, Loss: 0.8445995599031448, Final Batch Loss: 0.23352128267288208\n",
      "Epoch 6313, Loss: 0.7117440551519394, Final Batch Loss: 0.12851722538471222\n",
      "Epoch 6314, Loss: 0.7440529838204384, Final Batch Loss: 0.037080131471157074\n",
      "Epoch 6315, Loss: 0.7345078885555267, Final Batch Loss: 0.21136750280857086\n",
      "Epoch 6316, Loss: 0.5897364169359207, Final Batch Loss: 0.1287699043750763\n",
      "Epoch 6317, Loss: 0.6673156321048737, Final Batch Loss: 0.08360806107521057\n",
      "Epoch 6318, Loss: 0.7025985866785049, Final Batch Loss: 0.1342061161994934\n",
      "Epoch 6319, Loss: 0.8442064970731735, Final Batch Loss: 0.2644595205783844\n",
      "Epoch 6320, Loss: 0.6381496638059616, Final Batch Loss: 0.13046883046627045\n",
      "Epoch 6321, Loss: 0.7162297815084457, Final Batch Loss: 0.17051231861114502\n",
      "Epoch 6322, Loss: 0.5919207409024239, Final Batch Loss: 0.1000046506524086\n",
      "Epoch 6323, Loss: 0.5408098623156548, Final Batch Loss: 0.1010022833943367\n",
      "Epoch 6324, Loss: 0.6012032553553581, Final Batch Loss: 0.1419363170862198\n",
      "Epoch 6325, Loss: 0.7123374342918396, Final Batch Loss: 0.13258357346057892\n",
      "Epoch 6326, Loss: 0.5197752863168716, Final Batch Loss: 0.08698022365570068\n",
      "Epoch 6327, Loss: 0.6113409101963043, Final Batch Loss: 0.15559816360473633\n",
      "Epoch 6328, Loss: 0.7456575781106949, Final Batch Loss: 0.15426193177700043\n",
      "Epoch 6329, Loss: 0.860056534409523, Final Batch Loss: 0.3265727162361145\n",
      "Epoch 6330, Loss: 0.6764250919222832, Final Batch Loss: 0.12391719967126846\n",
      "Epoch 6331, Loss: 0.7916094809770584, Final Batch Loss: 0.19398044049739838\n",
      "Epoch 6332, Loss: 0.7619792371988297, Final Batch Loss: 0.1397995799779892\n",
      "Epoch 6333, Loss: 0.7008526399731636, Final Batch Loss: 0.09540878981351852\n",
      "Epoch 6334, Loss: 0.795714758336544, Final Batch Loss: 0.2609198987483978\n",
      "Epoch 6335, Loss: 0.7302074432373047, Final Batch Loss: 0.24379825592041016\n",
      "Epoch 6336, Loss: 0.7289851605892181, Final Batch Loss: 0.2816745340824127\n",
      "Epoch 6337, Loss: 0.768365815281868, Final Batch Loss: 0.2663314640522003\n",
      "Epoch 6338, Loss: 0.680683322250843, Final Batch Loss: 0.0799938216805458\n",
      "Epoch 6339, Loss: 0.8866103142499924, Final Batch Loss: 0.4092916250228882\n",
      "Epoch 6340, Loss: 0.5867001563310623, Final Batch Loss: 0.10529342293739319\n",
      "Epoch 6341, Loss: 0.9150799214839935, Final Batch Loss: 0.34955716133117676\n",
      "Epoch 6342, Loss: 0.6312676519155502, Final Batch Loss: 0.11593057960271835\n",
      "Epoch 6343, Loss: 0.6221952065825462, Final Batch Loss: 0.1450667530298233\n",
      "Epoch 6344, Loss: 0.7227703183889389, Final Batch Loss: 0.21274101734161377\n",
      "Epoch 6345, Loss: 0.7772787436842918, Final Batch Loss: 0.12115482240915298\n",
      "Epoch 6346, Loss: 0.719994068145752, Final Batch Loss: 0.26444926857948303\n",
      "Epoch 6347, Loss: 0.6502499580383301, Final Batch Loss: 0.22122575342655182\n",
      "Epoch 6348, Loss: 1.0437947660684586, Final Batch Loss: 0.5064041018486023\n",
      "Epoch 6349, Loss: 0.6178301647305489, Final Batch Loss: 0.06652217358350754\n",
      "Epoch 6350, Loss: 0.7421661168336868, Final Batch Loss: 0.2375987023115158\n",
      "Epoch 6351, Loss: 0.708450436592102, Final Batch Loss: 0.18204309046268463\n",
      "Epoch 6352, Loss: 0.7109654471278191, Final Batch Loss: 0.0940377339720726\n",
      "Epoch 6353, Loss: 0.5845103561878204, Final Batch Loss: 0.0897212028503418\n",
      "Epoch 6354, Loss: 0.6254998594522476, Final Batch Loss: 0.0667705088853836\n",
      "Epoch 6355, Loss: 0.7054299935698509, Final Batch Loss: 0.17101459205150604\n",
      "Epoch 6356, Loss: 0.5783396773040295, Final Batch Loss: 0.05469829961657524\n",
      "Epoch 6357, Loss: 0.6193088963627815, Final Batch Loss: 0.0872277244925499\n",
      "Epoch 6358, Loss: 0.788109764456749, Final Batch Loss: 0.15124554932117462\n",
      "Epoch 6359, Loss: 0.6950175240635872, Final Batch Loss: 0.11690793186426163\n",
      "Epoch 6360, Loss: 0.7104531824588776, Final Batch Loss: 0.15506266057491302\n",
      "Epoch 6361, Loss: 0.5863710045814514, Final Batch Loss: 0.09721916913986206\n",
      "Epoch 6362, Loss: 0.8276936113834381, Final Batch Loss: 0.2850490212440491\n",
      "Epoch 6363, Loss: 0.6432724446058273, Final Batch Loss: 0.06296736001968384\n",
      "Epoch 6364, Loss: 0.8730615973472595, Final Batch Loss: 0.15398551523685455\n",
      "Epoch 6365, Loss: 0.7403721511363983, Final Batch Loss: 0.27204373478889465\n",
      "Epoch 6366, Loss: 0.6759712845087051, Final Batch Loss: 0.19239802658557892\n",
      "Epoch 6367, Loss: 0.615269124507904, Final Batch Loss: 0.15507207810878754\n",
      "Epoch 6368, Loss: 0.6311265826225281, Final Batch Loss: 0.06902056932449341\n",
      "Epoch 6369, Loss: 0.8984874486923218, Final Batch Loss: 0.2084726095199585\n",
      "Epoch 6370, Loss: 0.6442215293645859, Final Batch Loss: 0.17195333540439606\n",
      "Epoch 6371, Loss: 0.8072174340486526, Final Batch Loss: 0.17662757635116577\n",
      "Epoch 6372, Loss: 0.8801296502351761, Final Batch Loss: 0.25740325450897217\n",
      "Epoch 6373, Loss: 0.8789588809013367, Final Batch Loss: 0.12863747775554657\n",
      "Epoch 6374, Loss: 0.7267449498176575, Final Batch Loss: 0.2110680788755417\n",
      "Epoch 6375, Loss: 0.9614091664552689, Final Batch Loss: 0.33101603388786316\n",
      "Epoch 6376, Loss: 0.8427011147141457, Final Batch Loss: 0.12180302292108536\n",
      "Epoch 6377, Loss: 0.7755748778581619, Final Batch Loss: 0.13267157971858978\n",
      "Epoch 6378, Loss: 0.7343431711196899, Final Batch Loss: 0.25573065876960754\n",
      "Epoch 6379, Loss: 0.6297676302492619, Final Batch Loss: 0.04687051847577095\n",
      "Epoch 6380, Loss: 0.9079826325178146, Final Batch Loss: 0.31948938965797424\n",
      "Epoch 6381, Loss: 0.652014896273613, Final Batch Loss: 0.184517502784729\n",
      "Epoch 6382, Loss: 0.819373607635498, Final Batch Loss: 0.19755865633487701\n",
      "Epoch 6383, Loss: 0.8083893656730652, Final Batch Loss: 0.24803531169891357\n",
      "Epoch 6384, Loss: 0.719868928194046, Final Batch Loss: 0.14033369719982147\n",
      "Epoch 6385, Loss: 0.6814417764544487, Final Batch Loss: 0.07262834161520004\n",
      "Epoch 6386, Loss: 0.5952292829751968, Final Batch Loss: 0.13053452968597412\n",
      "Epoch 6387, Loss: 0.6526143252849579, Final Batch Loss: 0.16563598811626434\n",
      "Epoch 6388, Loss: 0.6221805289387703, Final Batch Loss: 0.11612225323915482\n",
      "Epoch 6389, Loss: 0.6731793880462646, Final Batch Loss: 0.1207282543182373\n",
      "Epoch 6390, Loss: 0.9753832072019577, Final Batch Loss: 0.38461124897003174\n",
      "Epoch 6391, Loss: 0.7409181743860245, Final Batch Loss: 0.1570860743522644\n",
      "Epoch 6392, Loss: 0.7704025506973267, Final Batch Loss: 0.2512107193470001\n",
      "Epoch 6393, Loss: 0.5944626033306122, Final Batch Loss: 0.07158508896827698\n",
      "Epoch 6394, Loss: 0.7500933408737183, Final Batch Loss: 0.20118212699890137\n",
      "Epoch 6395, Loss: 0.7170918732881546, Final Batch Loss: 0.18290239572525024\n",
      "Epoch 6396, Loss: 0.6966053619980812, Final Batch Loss: 0.09838128834962845\n",
      "Epoch 6397, Loss: 0.8060044795274734, Final Batch Loss: 0.18704088032245636\n",
      "Epoch 6398, Loss: 0.8918440192937851, Final Batch Loss: 0.3298177421092987\n",
      "Epoch 6399, Loss: 0.5599998012185097, Final Batch Loss: 0.0671887919306755\n",
      "Epoch 6400, Loss: 0.7692505568265915, Final Batch Loss: 0.2032438963651657\n",
      "Epoch 6401, Loss: 0.7635764479637146, Final Batch Loss: 0.21094970405101776\n",
      "Epoch 6402, Loss: 0.6009486243128777, Final Batch Loss: 0.1101701632142067\n",
      "Epoch 6403, Loss: 0.6277976557612419, Final Batch Loss: 0.09531650692224503\n",
      "Epoch 6404, Loss: 0.7520138174295425, Final Batch Loss: 0.19850033521652222\n",
      "Epoch 6405, Loss: 0.5863903686404228, Final Batch Loss: 0.12332529574632645\n",
      "Epoch 6406, Loss: 0.8595184832811356, Final Batch Loss: 0.25499197840690613\n",
      "Epoch 6407, Loss: 0.7349164634943008, Final Batch Loss: 0.17864573001861572\n",
      "Epoch 6408, Loss: 0.6556779146194458, Final Batch Loss: 0.14638815820217133\n",
      "Epoch 6409, Loss: 0.5810955166816711, Final Batch Loss: 0.139010950922966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6410, Loss: 0.8476432263851166, Final Batch Loss: 0.21999479830265045\n",
      "Epoch 6411, Loss: 0.7049009427428246, Final Batch Loss: 0.2603130042552948\n",
      "Epoch 6412, Loss: 0.6038052290678024, Final Batch Loss: 0.12899446487426758\n",
      "Epoch 6413, Loss: 0.7927204668521881, Final Batch Loss: 0.16684310138225555\n",
      "Epoch 6414, Loss: 0.9488411545753479, Final Batch Loss: 0.3024992346763611\n",
      "Epoch 6415, Loss: 0.7409001737833023, Final Batch Loss: 0.1951460838317871\n",
      "Epoch 6416, Loss: 0.8355953693389893, Final Batch Loss: 0.18014098703861237\n",
      "Epoch 6417, Loss: 0.7268498092889786, Final Batch Loss: 0.1525278240442276\n",
      "Epoch 6418, Loss: 0.7704930454492569, Final Batch Loss: 0.19479461014270782\n",
      "Epoch 6419, Loss: 0.6813251376152039, Final Batch Loss: 0.12508253753185272\n",
      "Epoch 6420, Loss: 0.7283819019794464, Final Batch Loss: 0.19310200214385986\n",
      "Epoch 6421, Loss: 0.6702966690063477, Final Batch Loss: 0.15019144117832184\n",
      "Epoch 6422, Loss: 0.660323366522789, Final Batch Loss: 0.12524041533470154\n",
      "Epoch 6423, Loss: 0.708790048956871, Final Batch Loss: 0.17677970230579376\n",
      "Epoch 6424, Loss: 0.7778909802436829, Final Batch Loss: 0.22584684193134308\n",
      "Epoch 6425, Loss: 0.7661015093326569, Final Batch Loss: 0.22707052528858185\n",
      "Epoch 6426, Loss: 0.7197112143039703, Final Batch Loss: 0.12938153743743896\n",
      "Epoch 6427, Loss: 0.6717355325818062, Final Batch Loss: 0.1245979592204094\n",
      "Epoch 6428, Loss: 0.6750465929508209, Final Batch Loss: 0.10088586807250977\n",
      "Epoch 6429, Loss: 0.822453036904335, Final Batch Loss: 0.29106149077415466\n",
      "Epoch 6430, Loss: 0.7104781121015549, Final Batch Loss: 0.129693403840065\n",
      "Epoch 6431, Loss: 0.6177183762192726, Final Batch Loss: 0.17020057141780853\n",
      "Epoch 6432, Loss: 0.743606299161911, Final Batch Loss: 0.18888701498508453\n",
      "Epoch 6433, Loss: 0.778478741645813, Final Batch Loss: 0.08919310569763184\n",
      "Epoch 6434, Loss: 0.726528711616993, Final Batch Loss: 0.09949883073568344\n",
      "Epoch 6435, Loss: 0.7660092413425446, Final Batch Loss: 0.2316301465034485\n",
      "Epoch 6436, Loss: 0.6733824089169502, Final Batch Loss: 0.11948379129171371\n",
      "Epoch 6437, Loss: 0.6625777706503868, Final Batch Loss: 0.19014276564121246\n",
      "Epoch 6438, Loss: 0.7435420900583267, Final Batch Loss: 0.19103722274303436\n",
      "Epoch 6439, Loss: 0.7527183815836906, Final Batch Loss: 0.16304825246334076\n",
      "Epoch 6440, Loss: 0.8230278789997101, Final Batch Loss: 0.18864445388317108\n",
      "Epoch 6441, Loss: 0.6658032685518265, Final Batch Loss: 0.15489186346530914\n",
      "Epoch 6442, Loss: 0.7221014499664307, Final Batch Loss: 0.1999417394399643\n",
      "Epoch 6443, Loss: 0.6287600249052048, Final Batch Loss: 0.12488102912902832\n",
      "Epoch 6444, Loss: 0.7344735041260719, Final Batch Loss: 0.11163774877786636\n",
      "Epoch 6445, Loss: 0.7222110629081726, Final Batch Loss: 0.23850923776626587\n",
      "Epoch 6446, Loss: 0.5978665426373482, Final Batch Loss: 0.11693208664655685\n",
      "Epoch 6447, Loss: 0.760885700583458, Final Batch Loss: 0.27551910281181335\n",
      "Epoch 6448, Loss: 0.9012420922517776, Final Batch Loss: 0.23017533123493195\n",
      "Epoch 6449, Loss: 0.8192566633224487, Final Batch Loss: 0.22413547337055206\n",
      "Epoch 6450, Loss: 0.7586123794317245, Final Batch Loss: 0.22377116978168488\n",
      "Epoch 6451, Loss: 0.6981969773769379, Final Batch Loss: 0.1374552696943283\n",
      "Epoch 6452, Loss: 0.7265027910470963, Final Batch Loss: 0.16661003232002258\n",
      "Epoch 6453, Loss: 0.6474340483546257, Final Batch Loss: 0.11237799376249313\n",
      "Epoch 6454, Loss: 0.7882149517536163, Final Batch Loss: 0.2858678698539734\n",
      "Epoch 6455, Loss: 0.7944291979074478, Final Batch Loss: 0.21160805225372314\n",
      "Epoch 6456, Loss: 0.6961541995406151, Final Batch Loss: 0.08731383830308914\n",
      "Epoch 6457, Loss: 0.7048664838075638, Final Batch Loss: 0.20804445445537567\n",
      "Epoch 6458, Loss: 0.7095116749405861, Final Batch Loss: 0.11784888058900833\n",
      "Epoch 6459, Loss: 0.6573465168476105, Final Batch Loss: 0.13226161897182465\n",
      "Epoch 6460, Loss: 0.6834312975406647, Final Batch Loss: 0.1508648842573166\n",
      "Epoch 6461, Loss: 0.6979068964719772, Final Batch Loss: 0.15427187085151672\n",
      "Epoch 6462, Loss: 0.6775910258293152, Final Batch Loss: 0.06971114873886108\n",
      "Epoch 6463, Loss: 0.8865074068307877, Final Batch Loss: 0.29395821690559387\n",
      "Epoch 6464, Loss: 1.1474076509475708, Final Batch Loss: 0.5513272881507874\n",
      "Epoch 6465, Loss: 0.7014778405427933, Final Batch Loss: 0.23561501502990723\n",
      "Epoch 6466, Loss: 0.7228591367602348, Final Batch Loss: 0.11461073905229568\n",
      "Epoch 6467, Loss: 0.6894523799419403, Final Batch Loss: 0.14807382225990295\n",
      "Epoch 6468, Loss: 0.8276638239622116, Final Batch Loss: 0.13116584718227386\n",
      "Epoch 6469, Loss: 0.6715382114052773, Final Batch Loss: 0.07821673899888992\n",
      "Epoch 6470, Loss: 0.6881693676114082, Final Batch Loss: 0.089176706969738\n",
      "Epoch 6471, Loss: 0.7290414720773697, Final Batch Loss: 0.27761268615722656\n",
      "Epoch 6472, Loss: 0.7795178443193436, Final Batch Loss: 0.24643097817897797\n",
      "Epoch 6473, Loss: 0.7722884565591812, Final Batch Loss: 0.2356080412864685\n",
      "Epoch 6474, Loss: 0.6026132702827454, Final Batch Loss: 0.12730157375335693\n",
      "Epoch 6475, Loss: 0.7513729184865952, Final Batch Loss: 0.21045994758605957\n",
      "Epoch 6476, Loss: 0.6125412955880165, Final Batch Loss: 0.16696858406066895\n",
      "Epoch 6477, Loss: 0.6974821984767914, Final Batch Loss: 0.1365257054567337\n",
      "Epoch 6478, Loss: 0.6636809930205345, Final Batch Loss: 0.16914457082748413\n",
      "Epoch 6479, Loss: 0.7399021238088608, Final Batch Loss: 0.22197329998016357\n",
      "Epoch 6480, Loss: 0.9209022521972656, Final Batch Loss: 0.3139595091342926\n",
      "Epoch 6481, Loss: 0.7956081628799438, Final Batch Loss: 0.17105509340763092\n",
      "Epoch 6482, Loss: 0.7518720626831055, Final Batch Loss: 0.18199747800827026\n",
      "Epoch 6483, Loss: 0.7439785748720169, Final Batch Loss: 0.22328484058380127\n",
      "Epoch 6484, Loss: 0.5268403142690659, Final Batch Loss: 0.04724997282028198\n",
      "Epoch 6485, Loss: 0.7237168699502945, Final Batch Loss: 0.17603148519992828\n",
      "Epoch 6486, Loss: 0.7460737600922585, Final Batch Loss: 0.11843540519475937\n",
      "Epoch 6487, Loss: 0.8619089126586914, Final Batch Loss: 0.28049612045288086\n",
      "Epoch 6488, Loss: 0.6256238743662834, Final Batch Loss: 0.13043345510959625\n",
      "Epoch 6489, Loss: 0.7856975346803665, Final Batch Loss: 0.24464881420135498\n",
      "Epoch 6490, Loss: 0.7542944401502609, Final Batch Loss: 0.2161705046892166\n",
      "Epoch 6491, Loss: 0.7065379694104195, Final Batch Loss: 0.21811318397521973\n",
      "Epoch 6492, Loss: 0.7493049949407578, Final Batch Loss: 0.15528623759746552\n",
      "Epoch 6493, Loss: 0.7871164083480835, Final Batch Loss: 0.24681244790554047\n",
      "Epoch 6494, Loss: 0.6216553747653961, Final Batch Loss: 0.13076771795749664\n",
      "Epoch 6495, Loss: 0.6411089450120926, Final Batch Loss: 0.17573894560337067\n",
      "Epoch 6496, Loss: 0.8884085863828659, Final Batch Loss: 0.23200680315494537\n",
      "Epoch 6497, Loss: 0.6874051243066788, Final Batch Loss: 0.2141081541776657\n",
      "Epoch 6498, Loss: 0.6590767502784729, Final Batch Loss: 0.1401529461145401\n",
      "Epoch 6499, Loss: 0.6682535707950592, Final Batch Loss: 0.14021407067775726\n",
      "Epoch 6500, Loss: 0.6540336310863495, Final Batch Loss: 0.10944443941116333\n",
      "Epoch 6501, Loss: 0.6250235736370087, Final Batch Loss: 0.15677101910114288\n",
      "Epoch 6502, Loss: 0.6944328993558884, Final Batch Loss: 0.1675449162721634\n",
      "Epoch 6503, Loss: 0.6998551189899445, Final Batch Loss: 0.16443221271038055\n",
      "Epoch 6504, Loss: 0.8180204331874847, Final Batch Loss: 0.2856309711933136\n",
      "Epoch 6505, Loss: 0.6282803192734718, Final Batch Loss: 0.12383600324392319\n",
      "Epoch 6506, Loss: 0.7507152259349823, Final Batch Loss: 0.2341497391462326\n",
      "Epoch 6507, Loss: 0.5923982076346874, Final Batch Loss: 0.049193914979696274\n",
      "Epoch 6508, Loss: 0.6734067350625992, Final Batch Loss: 0.09969949722290039\n",
      "Epoch 6509, Loss: 0.6365108042955399, Final Batch Loss: 0.12554769217967987\n",
      "Epoch 6510, Loss: 0.7463080883026123, Final Batch Loss: 0.2402186393737793\n",
      "Epoch 6511, Loss: 0.6598433405160904, Final Batch Loss: 0.14122359454631805\n",
      "Epoch 6512, Loss: 1.083712413907051, Final Batch Loss: 0.36906537413597107\n",
      "Epoch 6513, Loss: 0.9612561464309692, Final Batch Loss: 0.213195338845253\n",
      "Epoch 6514, Loss: 0.8216121345758438, Final Batch Loss: 0.24983829259872437\n",
      "Epoch 6515, Loss: 0.8622124344110489, Final Batch Loss: 0.2540682554244995\n",
      "Epoch 6516, Loss: 0.7805608659982681, Final Batch Loss: 0.20201610028743744\n",
      "Epoch 6517, Loss: 0.8734319806098938, Final Batch Loss: 0.34280773997306824\n",
      "Epoch 6518, Loss: 0.7793112546205521, Final Batch Loss: 0.13914106786251068\n",
      "Epoch 6519, Loss: 0.8194662779569626, Final Batch Loss: 0.1962258219718933\n",
      "Epoch 6520, Loss: 0.6908321529626846, Final Batch Loss: 0.07748094201087952\n",
      "Epoch 6521, Loss: 0.5582229867577553, Final Batch Loss: 0.09701017290353775\n",
      "Epoch 6522, Loss: 0.795807421207428, Final Batch Loss: 0.26074734330177307\n",
      "Epoch 6523, Loss: 0.7953821420669556, Final Batch Loss: 0.2363678365945816\n",
      "Epoch 6524, Loss: 0.7122287601232529, Final Batch Loss: 0.2021806687116623\n",
      "Epoch 6525, Loss: 0.7187501266598701, Final Batch Loss: 0.11844072490930557\n",
      "Epoch 6526, Loss: 0.7970328480005264, Final Batch Loss: 0.20883171260356903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6527, Loss: 0.7426735758781433, Final Batch Loss: 0.14298853278160095\n",
      "Epoch 6528, Loss: 0.6530926898121834, Final Batch Loss: 0.08173545449972153\n",
      "Epoch 6529, Loss: 0.6483470946550369, Final Batch Loss: 0.10673442482948303\n",
      "Epoch 6530, Loss: 0.5447533465921879, Final Batch Loss: 0.03824340179562569\n",
      "Epoch 6531, Loss: 0.8037305027246475, Final Batch Loss: 0.24997979402542114\n",
      "Epoch 6532, Loss: 0.7271232903003693, Final Batch Loss: 0.26999178528785706\n",
      "Epoch 6533, Loss: 0.6133973002433777, Final Batch Loss: 0.15392106771469116\n",
      "Epoch 6534, Loss: 0.7327158004045486, Final Batch Loss: 0.15218476951122284\n",
      "Epoch 6535, Loss: 0.69490697234869, Final Batch Loss: 0.0999864861369133\n",
      "Epoch 6536, Loss: 0.6239620968699455, Final Batch Loss: 0.0819132998585701\n",
      "Epoch 6537, Loss: 0.7309581637382507, Final Batch Loss: 0.15516309440135956\n",
      "Epoch 6538, Loss: 0.6354417949914932, Final Batch Loss: 0.1254994124174118\n",
      "Epoch 6539, Loss: 0.661968357861042, Final Batch Loss: 0.12140391021966934\n",
      "Epoch 6540, Loss: 0.5522761419415474, Final Batch Loss: 0.0683274194598198\n",
      "Epoch 6541, Loss: 0.8568350821733475, Final Batch Loss: 0.310576468706131\n",
      "Epoch 6542, Loss: 0.674022987484932, Final Batch Loss: 0.10630485415458679\n",
      "Epoch 6543, Loss: 0.819820374250412, Final Batch Loss: 0.18766920268535614\n",
      "Epoch 6544, Loss: 0.7100130319595337, Final Batch Loss: 0.13237030804157257\n",
      "Epoch 6545, Loss: 0.8005599975585938, Final Batch Loss: 0.21406809985637665\n",
      "Epoch 6546, Loss: 0.6301608085632324, Final Batch Loss: 0.11744958162307739\n",
      "Epoch 6547, Loss: 0.6266734525561333, Final Batch Loss: 0.08997491747140884\n",
      "Epoch 6548, Loss: 0.7809432148933411, Final Batch Loss: 0.20347899198532104\n",
      "Epoch 6549, Loss: 0.7059246450662613, Final Batch Loss: 0.15785624086856842\n",
      "Epoch 6550, Loss: 0.726833701133728, Final Batch Loss: 0.18415890634059906\n",
      "Epoch 6551, Loss: 0.6952241659164429, Final Batch Loss: 0.14754758775234222\n",
      "Epoch 6552, Loss: 0.91029854118824, Final Batch Loss: 0.34061098098754883\n",
      "Epoch 6553, Loss: 0.7755266577005386, Final Batch Loss: 0.1592220813035965\n",
      "Epoch 6554, Loss: 0.8240163922309875, Final Batch Loss: 0.22196198999881744\n",
      "Epoch 6555, Loss: 0.8097414374351501, Final Batch Loss: 0.2788151502609253\n",
      "Epoch 6556, Loss: 0.7622819393873215, Final Batch Loss: 0.19462452828884125\n",
      "Epoch 6557, Loss: 0.645887017250061, Final Batch Loss: 0.05841080844402313\n",
      "Epoch 6558, Loss: 0.6490274742245674, Final Batch Loss: 0.0943751111626625\n",
      "Epoch 6559, Loss: 0.744316890835762, Final Batch Loss: 0.19240935146808624\n",
      "Epoch 6560, Loss: 0.6460908874869347, Final Batch Loss: 0.08681123703718185\n",
      "Epoch 6561, Loss: 0.6400309652090073, Final Batch Loss: 0.14965702593326569\n",
      "Epoch 6562, Loss: 0.6889408379793167, Final Batch Loss: 0.16548459231853485\n",
      "Epoch 6563, Loss: 0.7087410613894463, Final Batch Loss: 0.11806408315896988\n",
      "Epoch 6564, Loss: 1.341990351676941, Final Batch Loss: 0.6965685486793518\n",
      "Epoch 6565, Loss: 0.7259397655725479, Final Batch Loss: 0.13817273080348969\n",
      "Epoch 6566, Loss: 0.932232216000557, Final Batch Loss: 0.33940353989601135\n",
      "Epoch 6567, Loss: 0.7595239579677582, Final Batch Loss: 0.20366902649402618\n",
      "Epoch 6568, Loss: 0.7697550803422928, Final Batch Loss: 0.21230310201644897\n",
      "Epoch 6569, Loss: 0.6349247917532921, Final Batch Loss: 0.12347482889890671\n",
      "Epoch 6570, Loss: 0.9996063709259033, Final Batch Loss: 0.37654444575309753\n",
      "Epoch 6571, Loss: 0.8039709180593491, Final Batch Loss: 0.1706923246383667\n",
      "Epoch 6572, Loss: 0.7412014752626419, Final Batch Loss: 0.13701726496219635\n",
      "Epoch 6573, Loss: 0.6760604679584503, Final Batch Loss: 0.13003888726234436\n",
      "Epoch 6574, Loss: 0.7687020152807236, Final Batch Loss: 0.2345981001853943\n",
      "Epoch 6575, Loss: 0.7211902290582657, Final Batch Loss: 0.2674708068370819\n",
      "Epoch 6576, Loss: 0.5755943804979324, Final Batch Loss: 0.07306909561157227\n",
      "Epoch 6577, Loss: 0.6512210369110107, Final Batch Loss: 0.16638822853565216\n",
      "Epoch 6578, Loss: 0.798402264714241, Final Batch Loss: 0.21970337629318237\n",
      "Epoch 6579, Loss: 0.7780409157276154, Final Batch Loss: 0.14403694868087769\n",
      "Epoch 6580, Loss: 0.7387863025069237, Final Batch Loss: 0.17561320960521698\n",
      "Epoch 6581, Loss: 0.6322216093540192, Final Batch Loss: 0.09494683146476746\n",
      "Epoch 6582, Loss: 0.8605947196483612, Final Batch Loss: 0.1852845698595047\n",
      "Epoch 6583, Loss: 0.6742458492517471, Final Batch Loss: 0.14294910430908203\n",
      "Epoch 6584, Loss: 0.80720354616642, Final Batch Loss: 0.22815094888210297\n",
      "Epoch 6585, Loss: 0.6995383203029633, Final Batch Loss: 0.16138070821762085\n",
      "Epoch 6586, Loss: 0.8138789981603622, Final Batch Loss: 0.15267041325569153\n",
      "Epoch 6587, Loss: 0.6895335093140602, Final Batch Loss: 0.19950176775455475\n",
      "Epoch 6588, Loss: 0.6138691008090973, Final Batch Loss: 0.1353815495967865\n",
      "Epoch 6589, Loss: 0.6569644808769226, Final Batch Loss: 0.1771678924560547\n",
      "Epoch 6590, Loss: 0.5456499084830284, Final Batch Loss: 0.08668201416730881\n",
      "Epoch 6591, Loss: 0.8785872235894203, Final Batch Loss: 0.38953840732574463\n",
      "Epoch 6592, Loss: 0.8080475032329559, Final Batch Loss: 0.2539222538471222\n",
      "Epoch 6593, Loss: 0.9930801242589951, Final Batch Loss: 0.2981680929660797\n",
      "Epoch 6594, Loss: 0.6119133532047272, Final Batch Loss: 0.1306038200855255\n",
      "Epoch 6595, Loss: 0.6688154861330986, Final Batch Loss: 0.20787523686885834\n",
      "Epoch 6596, Loss: 0.5908971428871155, Final Batch Loss: 0.10137152671813965\n",
      "Epoch 6597, Loss: 0.7755030989646912, Final Batch Loss: 0.1719222515821457\n",
      "Epoch 6598, Loss: 0.5704697892069817, Final Batch Loss: 0.09576616436243057\n",
      "Epoch 6599, Loss: 0.7807848006486893, Final Batch Loss: 0.21626345813274384\n",
      "Epoch 6600, Loss: 0.529970794916153, Final Batch Loss: 0.0677255243062973\n",
      "Epoch 6601, Loss: 0.7658814340829849, Final Batch Loss: 0.23170320689678192\n",
      "Epoch 6602, Loss: 0.9178385138511658, Final Batch Loss: 0.48116540908813477\n",
      "Epoch 6603, Loss: 0.7449971139431, Final Batch Loss: 0.2208881974220276\n",
      "Epoch 6604, Loss: 0.8057066798210144, Final Batch Loss: 0.21227139234542847\n",
      "Epoch 6605, Loss: 0.6456465423107147, Final Batch Loss: 0.12934580445289612\n",
      "Epoch 6606, Loss: 0.6489167809486389, Final Batch Loss: 0.16457074880599976\n",
      "Epoch 6607, Loss: 0.8283556401729584, Final Batch Loss: 0.30198532342910767\n",
      "Epoch 6608, Loss: 0.7138012275099754, Final Batch Loss: 0.1106591746211052\n",
      "Epoch 6609, Loss: 0.7622601091861725, Final Batch Loss: 0.2482326477766037\n",
      "Epoch 6610, Loss: 0.9596647322177887, Final Batch Loss: 0.4375624358654022\n",
      "Epoch 6611, Loss: 0.7023352161049843, Final Batch Loss: 0.08618731051683426\n",
      "Epoch 6612, Loss: 0.7235283479094505, Final Batch Loss: 0.19604100286960602\n",
      "Epoch 6613, Loss: 0.8851008117198944, Final Batch Loss: 0.291416734457016\n",
      "Epoch 6614, Loss: 0.7643746733665466, Final Batch Loss: 0.1619998663663864\n",
      "Epoch 6615, Loss: 0.660174161195755, Final Batch Loss: 0.16048961877822876\n",
      "Epoch 6616, Loss: 0.855400025844574, Final Batch Loss: 0.2053896188735962\n",
      "Epoch 6617, Loss: 0.6818061843514442, Final Batch Loss: 0.10462387651205063\n",
      "Epoch 6618, Loss: 0.8174017667770386, Final Batch Loss: 0.21997809410095215\n",
      "Epoch 6619, Loss: 0.6304032057523727, Final Batch Loss: 0.10588747262954712\n",
      "Epoch 6620, Loss: 0.776101142168045, Final Batch Loss: 0.18051832914352417\n",
      "Epoch 6621, Loss: 0.6457753926515579, Final Batch Loss: 0.14731217920780182\n",
      "Epoch 6622, Loss: 0.8761787265539169, Final Batch Loss: 0.2784312069416046\n",
      "Epoch 6623, Loss: 0.612403217703104, Final Batch Loss: 0.04733780398964882\n",
      "Epoch 6624, Loss: 0.7651619464159012, Final Batch Loss: 0.25618603825569153\n",
      "Epoch 6625, Loss: 0.6624082997441292, Final Batch Loss: 0.10847116261720657\n",
      "Epoch 6626, Loss: 0.859961986541748, Final Batch Loss: 0.3446846008300781\n",
      "Epoch 6627, Loss: 0.7915680706501007, Final Batch Loss: 0.2687872052192688\n",
      "Epoch 6628, Loss: 0.6413254141807556, Final Batch Loss: 0.18216270208358765\n",
      "Epoch 6629, Loss: 0.7484824061393738, Final Batch Loss: 0.1549634486436844\n",
      "Epoch 6630, Loss: 0.7882314920425415, Final Batch Loss: 0.20138783752918243\n",
      "Epoch 6631, Loss: 0.6889277398586273, Final Batch Loss: 0.13144132494926453\n",
      "Epoch 6632, Loss: 0.7319177463650703, Final Batch Loss: 0.12154906243085861\n",
      "Epoch 6633, Loss: 0.793235182762146, Final Batch Loss: 0.1678193062543869\n",
      "Epoch 6634, Loss: 0.9244371801614761, Final Batch Loss: 0.3177434802055359\n",
      "Epoch 6635, Loss: 0.8523063659667969, Final Batch Loss: 0.2521682381629944\n",
      "Epoch 6636, Loss: 0.7111479640007019, Final Batch Loss: 0.17645561695098877\n",
      "Epoch 6637, Loss: 0.7240394204854965, Final Batch Loss: 0.21358990669250488\n",
      "Epoch 6638, Loss: 0.7548512667417526, Final Batch Loss: 0.17672719061374664\n",
      "Epoch 6639, Loss: 0.6140781044960022, Final Batch Loss: 0.052268147468566895\n",
      "Epoch 6640, Loss: 0.754920594394207, Final Batch Loss: 0.21218997240066528\n",
      "Epoch 6641, Loss: 0.811042457818985, Final Batch Loss: 0.2492760568857193\n",
      "Epoch 6642, Loss: 0.7966211289167404, Final Batch Loss: 0.339016318321228\n",
      "Epoch 6643, Loss: 0.6601929366588593, Final Batch Loss: 0.12247076630592346\n",
      "Epoch 6644, Loss: 0.7877277135848999, Final Batch Loss: 0.15416255593299866\n",
      "Epoch 6645, Loss: 0.6692664176225662, Final Batch Loss: 0.18014539778232574\n",
      "Epoch 6646, Loss: 0.6556301340460777, Final Batch Loss: 0.11634460836648941\n",
      "Epoch 6647, Loss: 0.6927424371242523, Final Batch Loss: 0.14187251031398773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6648, Loss: 0.7018834426999092, Final Batch Loss: 0.12322888523340225\n",
      "Epoch 6649, Loss: 0.7536184042692184, Final Batch Loss: 0.23176921904087067\n",
      "Epoch 6650, Loss: 0.6312838271260262, Final Batch Loss: 0.12270162254571915\n",
      "Epoch 6651, Loss: 0.7325271666049957, Final Batch Loss: 0.19974808394908905\n",
      "Epoch 6652, Loss: 0.7767752707004547, Final Batch Loss: 0.24461989104747772\n",
      "Epoch 6653, Loss: 0.5114392973482609, Final Batch Loss: 0.05526978150010109\n",
      "Epoch 6654, Loss: 0.6766442134976387, Final Batch Loss: 0.07769004255533218\n",
      "Epoch 6655, Loss: 0.7284651026129723, Final Batch Loss: 0.12248823791742325\n",
      "Epoch 6656, Loss: 0.6169536970555782, Final Batch Loss: 0.06054132804274559\n",
      "Epoch 6657, Loss: 0.6853105574846268, Final Batch Loss: 0.15354539453983307\n",
      "Epoch 6658, Loss: 0.6716850847005844, Final Batch Loss: 0.15298469364643097\n",
      "Epoch 6659, Loss: 0.9377057254314423, Final Batch Loss: 0.38631176948547363\n",
      "Epoch 6660, Loss: 0.7319286167621613, Final Batch Loss: 0.14693590998649597\n",
      "Epoch 6661, Loss: 0.7019090801477432, Final Batch Loss: 0.13309518992900848\n",
      "Epoch 6662, Loss: 0.6318848729133606, Final Batch Loss: 0.06978835165500641\n",
      "Epoch 6663, Loss: 0.6752214580774307, Final Batch Loss: 0.14443548023700714\n",
      "Epoch 6664, Loss: 0.9012869000434875, Final Batch Loss: 0.1626281589269638\n",
      "Epoch 6665, Loss: 0.6621120274066925, Final Batch Loss: 0.14849205315113068\n",
      "Epoch 6666, Loss: 0.8706709891557693, Final Batch Loss: 0.3469625413417816\n",
      "Epoch 6667, Loss: 0.6499964073300362, Final Batch Loss: 0.11528303474187851\n",
      "Epoch 6668, Loss: 0.6629848480224609, Final Batch Loss: 0.12649638950824738\n",
      "Epoch 6669, Loss: 0.7363385483622551, Final Batch Loss: 0.24443179368972778\n",
      "Epoch 6670, Loss: 0.5704651772975922, Final Batch Loss: 0.06654644012451172\n",
      "Epoch 6671, Loss: 0.6680717766284943, Final Batch Loss: 0.10890820622444153\n",
      "Epoch 6672, Loss: 0.6799706220626831, Final Batch Loss: 0.14252541959285736\n",
      "Epoch 6673, Loss: 0.7933269590139389, Final Batch Loss: 0.22365140914916992\n",
      "Epoch 6674, Loss: 0.6190417036414146, Final Batch Loss: 0.07705337554216385\n",
      "Epoch 6675, Loss: 0.7902768403291702, Final Batch Loss: 0.26291507482528687\n",
      "Epoch 6676, Loss: 0.7306831553578377, Final Batch Loss: 0.13003836572170258\n",
      "Epoch 6677, Loss: 0.7009878605604172, Final Batch Loss: 0.13331906497478485\n",
      "Epoch 6678, Loss: 0.9014971554279327, Final Batch Loss: 0.36292874813079834\n",
      "Epoch 6679, Loss: 0.7752672582864761, Final Batch Loss: 0.22166232764720917\n",
      "Epoch 6680, Loss: 0.6872664093971252, Final Batch Loss: 0.14295344054698944\n",
      "Epoch 6681, Loss: 0.797283411026001, Final Batch Loss: 0.10007792711257935\n",
      "Epoch 6682, Loss: 0.896849736571312, Final Batch Loss: 0.34426143765449524\n",
      "Epoch 6683, Loss: 0.7459701299667358, Final Batch Loss: 0.21176473796367645\n",
      "Epoch 6684, Loss: 0.6795448064804077, Final Batch Loss: 0.1682521253824234\n",
      "Epoch 6685, Loss: 0.7141599208116531, Final Batch Loss: 0.19507785141468048\n",
      "Epoch 6686, Loss: 0.7515982985496521, Final Batch Loss: 0.13635419309139252\n",
      "Epoch 6687, Loss: 0.7726333290338516, Final Batch Loss: 0.23086875677108765\n",
      "Epoch 6688, Loss: 0.8452707231044769, Final Batch Loss: 0.15090061724185944\n",
      "Epoch 6689, Loss: 0.6767705380916595, Final Batch Loss: 0.13352236151695251\n",
      "Epoch 6690, Loss: 0.909405454993248, Final Batch Loss: 0.3005530536174774\n",
      "Epoch 6691, Loss: 0.8388104140758514, Final Batch Loss: 0.30551040172576904\n",
      "Epoch 6692, Loss: 0.7908304184675217, Final Batch Loss: 0.22430960834026337\n",
      "Epoch 6693, Loss: 0.630671426653862, Final Batch Loss: 0.14137490093708038\n",
      "Epoch 6694, Loss: 0.6287801861763, Final Batch Loss: 0.1605788767337799\n",
      "Epoch 6695, Loss: 0.7138166725635529, Final Batch Loss: 0.1333145946264267\n",
      "Epoch 6696, Loss: 0.6170894354581833, Final Batch Loss: 0.09122821688652039\n",
      "Epoch 6697, Loss: 0.6516675651073456, Final Batch Loss: 0.16877855360507965\n",
      "Epoch 6698, Loss: 0.7747561112046242, Final Batch Loss: 0.1606682986021042\n",
      "Epoch 6699, Loss: 0.9358416795730591, Final Batch Loss: 0.2398223727941513\n",
      "Epoch 6700, Loss: 0.8265321105718613, Final Batch Loss: 0.3177098333835602\n",
      "Epoch 6701, Loss: 0.6760205626487732, Final Batch Loss: 0.19794346392154694\n",
      "Epoch 6702, Loss: 0.8428184539079666, Final Batch Loss: 0.29029449820518494\n",
      "Epoch 6703, Loss: 0.6765047907829285, Final Batch Loss: 0.12934906780719757\n",
      "Epoch 6704, Loss: 0.7225654572248459, Final Batch Loss: 0.17830966413021088\n",
      "Epoch 6705, Loss: 0.7410121262073517, Final Batch Loss: 0.19087666273117065\n",
      "Epoch 6706, Loss: 0.8826437741518021, Final Batch Loss: 0.32831332087516785\n",
      "Epoch 6707, Loss: 0.7160047590732574, Final Batch Loss: 0.23651640117168427\n",
      "Epoch 6708, Loss: 0.8685037791728973, Final Batch Loss: 0.3528006076812744\n",
      "Epoch 6709, Loss: 0.8609930872917175, Final Batch Loss: 0.2952193319797516\n",
      "Epoch 6710, Loss: 0.6692585200071335, Final Batch Loss: 0.1899910420179367\n",
      "Epoch 6711, Loss: 0.7601446807384491, Final Batch Loss: 0.15330003201961517\n",
      "Epoch 6712, Loss: 0.6336365565657616, Final Batch Loss: 0.11660010367631912\n",
      "Epoch 6713, Loss: 0.8827891945838928, Final Batch Loss: 0.3181517422199249\n",
      "Epoch 6714, Loss: 0.8235832452774048, Final Batch Loss: 0.08336099982261658\n",
      "Epoch 6715, Loss: 0.7268734201788902, Final Batch Loss: 0.22732122242450714\n",
      "Epoch 6716, Loss: 0.6776313334703445, Final Batch Loss: 0.1797717660665512\n",
      "Epoch 6717, Loss: 0.6948723644018173, Final Batch Loss: 0.16155901551246643\n",
      "Epoch 6718, Loss: 0.770885780453682, Final Batch Loss: 0.12645652890205383\n",
      "Epoch 6719, Loss: 0.7769010663032532, Final Batch Loss: 0.20534570515155792\n",
      "Epoch 6720, Loss: 0.5618588514626026, Final Batch Loss: 0.057318080216646194\n",
      "Epoch 6721, Loss: 0.8692076355218887, Final Batch Loss: 0.2033516764640808\n",
      "Epoch 6722, Loss: 0.7142413258552551, Final Batch Loss: 0.17885339260101318\n",
      "Epoch 6723, Loss: 0.7543393522500992, Final Batch Loss: 0.19864429533481598\n",
      "Epoch 6724, Loss: 0.6674495935440063, Final Batch Loss: 0.1570068746805191\n",
      "Epoch 6725, Loss: 0.6949359029531479, Final Batch Loss: 0.17020784318447113\n",
      "Epoch 6726, Loss: 0.6809664815664291, Final Batch Loss: 0.16591447591781616\n",
      "Epoch 6727, Loss: 0.6219930723309517, Final Batch Loss: 0.08001063019037247\n",
      "Epoch 6728, Loss: 0.7521559149026871, Final Batch Loss: 0.24869436025619507\n",
      "Epoch 6729, Loss: 0.69283227622509, Final Batch Loss: 0.20584578812122345\n",
      "Epoch 6730, Loss: 0.8913560956716537, Final Batch Loss: 0.37176308035850525\n",
      "Epoch 6731, Loss: 0.7639437764883041, Final Batch Loss: 0.29488885402679443\n",
      "Epoch 6732, Loss: 0.7340392023324966, Final Batch Loss: 0.21030382812023163\n",
      "Epoch 6733, Loss: 0.630752682685852, Final Batch Loss: 0.10890403389930725\n",
      "Epoch 6734, Loss: 0.7053099721670151, Final Batch Loss: 0.32097384333610535\n",
      "Epoch 6735, Loss: 0.7108046561479568, Final Batch Loss: 0.23942522704601288\n",
      "Epoch 6736, Loss: 0.6790213882923126, Final Batch Loss: 0.18666021525859833\n",
      "Epoch 6737, Loss: 0.6837633401155472, Final Batch Loss: 0.15023379027843475\n",
      "Epoch 6738, Loss: 0.5736355185508728, Final Batch Loss: 0.09829971194267273\n",
      "Epoch 6739, Loss: 0.7291214913129807, Final Batch Loss: 0.17556016147136688\n",
      "Epoch 6740, Loss: 0.8547457754611969, Final Batch Loss: 0.3542816936969757\n",
      "Epoch 6741, Loss: 0.7197723910212517, Final Batch Loss: 0.11190470308065414\n",
      "Epoch 6742, Loss: 0.618978425860405, Final Batch Loss: 0.13998043537139893\n",
      "Epoch 6743, Loss: 1.042972445487976, Final Batch Loss: 0.33907774090766907\n",
      "Epoch 6744, Loss: 0.6451604589819908, Final Batch Loss: 0.11861006170511246\n",
      "Epoch 6745, Loss: 0.8155311495065689, Final Batch Loss: 0.28556379675865173\n",
      "Epoch 6746, Loss: 0.7509821206331253, Final Batch Loss: 0.17418037354946136\n",
      "Epoch 6747, Loss: 0.6064535081386566, Final Batch Loss: 0.11282327026128769\n",
      "Epoch 6748, Loss: 0.8014576435089111, Final Batch Loss: 0.30401065945625305\n",
      "Epoch 6749, Loss: 0.7085629403591156, Final Batch Loss: 0.27406975626945496\n",
      "Epoch 6750, Loss: 0.7978755235671997, Final Batch Loss: 0.279403418302536\n",
      "Epoch 6751, Loss: 0.7333648651838303, Final Batch Loss: 0.2552589178085327\n",
      "Epoch 6752, Loss: 0.9932485967874527, Final Batch Loss: 0.35807883739471436\n",
      "Epoch 6753, Loss: 0.6808587908744812, Final Batch Loss: 0.18396614491939545\n",
      "Epoch 6754, Loss: 0.7053319588303566, Final Batch Loss: 0.2638913691043854\n",
      "Epoch 6755, Loss: 0.7508197575807571, Final Batch Loss: 0.18441419303417206\n",
      "Epoch 6756, Loss: 0.6493181362748146, Final Batch Loss: 0.1167004182934761\n",
      "Epoch 6757, Loss: 0.6362443044781685, Final Batch Loss: 0.12171521037817001\n",
      "Epoch 6758, Loss: 0.7710200697183609, Final Batch Loss: 0.20076967775821686\n",
      "Epoch 6759, Loss: 0.7579437792301178, Final Batch Loss: 0.17886024713516235\n",
      "Epoch 6760, Loss: 0.6373886093497276, Final Batch Loss: 0.10930400341749191\n",
      "Epoch 6761, Loss: 0.8879816830158234, Final Batch Loss: 0.38070347905158997\n",
      "Epoch 6762, Loss: 0.7135795652866364, Final Batch Loss: 0.24540065228939056\n",
      "Epoch 6763, Loss: 0.717943586409092, Final Batch Loss: 0.11828400939702988\n",
      "Epoch 6764, Loss: 0.6661951020359993, Final Batch Loss: 0.11037858575582504\n",
      "Epoch 6765, Loss: 0.9194029420614243, Final Batch Loss: 0.21533559262752533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6766, Loss: 0.6254995614290237, Final Batch Loss: 0.10192833095788956\n",
      "Epoch 6767, Loss: 0.5923541113734245, Final Batch Loss: 0.06782714277505875\n",
      "Epoch 6768, Loss: 0.6226533055305481, Final Batch Loss: 0.09114807844161987\n",
      "Epoch 6769, Loss: 0.6229848638176918, Final Batch Loss: 0.09760207682847977\n",
      "Epoch 6770, Loss: 0.5812167450785637, Final Batch Loss: 0.10478552430868149\n",
      "Epoch 6771, Loss: 0.8164206147193909, Final Batch Loss: 0.31676366925239563\n",
      "Epoch 6772, Loss: 0.5653752237558365, Final Batch Loss: 0.1130969226360321\n",
      "Epoch 6773, Loss: 0.7406584993004799, Final Batch Loss: 0.11952684074640274\n",
      "Epoch 6774, Loss: 0.8323862254619598, Final Batch Loss: 0.2363937497138977\n",
      "Epoch 6775, Loss: 0.7174571752548218, Final Batch Loss: 0.17367666959762573\n",
      "Epoch 6776, Loss: 0.7697865068912506, Final Batch Loss: 0.31015071272850037\n",
      "Epoch 6777, Loss: 0.8406318724155426, Final Batch Loss: 0.20297454297542572\n",
      "Epoch 6778, Loss: 0.7698105126619339, Final Batch Loss: 0.24442023038864136\n",
      "Epoch 6779, Loss: 0.7511943578720093, Final Batch Loss: 0.17264340817928314\n",
      "Epoch 6780, Loss: 0.7034053429961205, Final Batch Loss: 0.10326532274484634\n",
      "Epoch 6781, Loss: 0.7859313488006592, Final Batch Loss: 0.3455156087875366\n",
      "Epoch 6782, Loss: 0.6844400018453598, Final Batch Loss: 0.15657426416873932\n",
      "Epoch 6783, Loss: 0.7703695073723793, Final Batch Loss: 0.10161151736974716\n",
      "Epoch 6784, Loss: 0.68585404753685, Final Batch Loss: 0.17808763682842255\n",
      "Epoch 6785, Loss: 0.7648544758558273, Final Batch Loss: 0.17203660309314728\n",
      "Epoch 6786, Loss: 0.8655697405338287, Final Batch Loss: 0.2765379548072815\n",
      "Epoch 6787, Loss: 0.6664602905511856, Final Batch Loss: 0.20467637479305267\n",
      "Epoch 6788, Loss: 0.5977440550923347, Final Batch Loss: 0.09185215085744858\n",
      "Epoch 6789, Loss: 0.6073024496436119, Final Batch Loss: 0.12119541317224503\n",
      "Epoch 6790, Loss: 0.7481282949447632, Final Batch Loss: 0.12858574092388153\n",
      "Epoch 6791, Loss: 0.8294915407896042, Final Batch Loss: 0.27438411116600037\n",
      "Epoch 6792, Loss: 0.6192374974489212, Final Batch Loss: 0.14182336628437042\n",
      "Epoch 6793, Loss: 0.6628340482711792, Final Batch Loss: 0.09055566787719727\n",
      "Epoch 6794, Loss: 0.6896997392177582, Final Batch Loss: 0.14622066915035248\n",
      "Epoch 6795, Loss: 0.6970203667879105, Final Batch Loss: 0.16377899050712585\n",
      "Epoch 6796, Loss: 0.8253555297851562, Final Batch Loss: 0.3414672911167145\n",
      "Epoch 6797, Loss: 0.7684886008501053, Final Batch Loss: 0.23397396504878998\n",
      "Epoch 6798, Loss: 0.6266813203692436, Final Batch Loss: 0.0779673382639885\n",
      "Epoch 6799, Loss: 0.7854552268981934, Final Batch Loss: 0.2689298689365387\n",
      "Epoch 6800, Loss: 0.674183115363121, Final Batch Loss: 0.15680353343486786\n",
      "Epoch 6801, Loss: 0.7397659569978714, Final Batch Loss: 0.19982977211475372\n",
      "Epoch 6802, Loss: 0.7371417731046677, Final Batch Loss: 0.26431503891944885\n",
      "Epoch 6803, Loss: 0.7582258433103561, Final Batch Loss: 0.22274701297283173\n",
      "Epoch 6804, Loss: 0.6713172197341919, Final Batch Loss: 0.16141630709171295\n",
      "Epoch 6805, Loss: 0.714222714304924, Final Batch Loss: 0.19443471729755402\n",
      "Epoch 6806, Loss: 0.7081866264343262, Final Batch Loss: 0.15456219017505646\n",
      "Epoch 6807, Loss: 0.6954496055841446, Final Batch Loss: 0.2530858814716339\n",
      "Epoch 6808, Loss: 0.7482039034366608, Final Batch Loss: 0.24551261961460114\n",
      "Epoch 6809, Loss: 0.6633385196328163, Final Batch Loss: 0.08285339921712875\n",
      "Epoch 6810, Loss: 0.7038515657186508, Final Batch Loss: 0.15740251541137695\n",
      "Epoch 6811, Loss: 0.6551482677459717, Final Batch Loss: 0.17281509935855865\n",
      "Epoch 6812, Loss: 0.6739149317145348, Final Batch Loss: 0.12067798525094986\n",
      "Epoch 6813, Loss: 0.6102788671851158, Final Batch Loss: 0.08025038987398148\n",
      "Epoch 6814, Loss: 0.6733278930187225, Final Batch Loss: 0.13917787373065948\n",
      "Epoch 6815, Loss: 0.5897749215364456, Final Batch Loss: 0.10966381430625916\n",
      "Epoch 6816, Loss: 0.6646536588668823, Final Batch Loss: 0.1934828758239746\n",
      "Epoch 6817, Loss: 0.5947419106960297, Final Batch Loss: 0.16110876202583313\n",
      "Epoch 6818, Loss: 0.6279819905757904, Final Batch Loss: 0.1510215848684311\n",
      "Epoch 6819, Loss: 0.5905110836029053, Final Batch Loss: 0.12839017808437347\n",
      "Epoch 6820, Loss: 0.7877362966537476, Final Batch Loss: 0.11559897661209106\n",
      "Epoch 6821, Loss: 0.7700759172439575, Final Batch Loss: 0.20177330076694489\n",
      "Epoch 6822, Loss: 0.6446135491132736, Final Batch Loss: 0.1811729222536087\n",
      "Epoch 6823, Loss: 0.6499000564217567, Final Batch Loss: 0.12248953431844711\n",
      "Epoch 6824, Loss: 0.849711999297142, Final Batch Loss: 0.2513534724712372\n",
      "Epoch 6825, Loss: 0.7261143177747726, Final Batch Loss: 0.21470297873020172\n",
      "Epoch 6826, Loss: 0.7835675477981567, Final Batch Loss: 0.16492272913455963\n",
      "Epoch 6827, Loss: 0.8508753329515457, Final Batch Loss: 0.2828185558319092\n",
      "Epoch 6828, Loss: 0.5684712007641792, Final Batch Loss: 0.0854353979229927\n",
      "Epoch 6829, Loss: 0.5644722655415535, Final Batch Loss: 0.08558566123247147\n",
      "Epoch 6830, Loss: 0.679790809750557, Final Batch Loss: 0.13201259076595306\n",
      "Epoch 6831, Loss: 0.6784150898456573, Final Batch Loss: 0.13017308712005615\n",
      "Epoch 6832, Loss: 0.7253539450466633, Final Batch Loss: 0.06173718348145485\n",
      "Epoch 6833, Loss: 0.7273885905742645, Final Batch Loss: 0.2209922820329666\n",
      "Epoch 6834, Loss: 0.7284945845603943, Final Batch Loss: 0.2339382767677307\n",
      "Epoch 6835, Loss: 0.6542037725448608, Final Batch Loss: 0.19907625019550323\n",
      "Epoch 6836, Loss: 0.7464431375265121, Final Batch Loss: 0.22038288414478302\n",
      "Epoch 6837, Loss: 0.6380841583013535, Final Batch Loss: 0.16633723676204681\n",
      "Epoch 6838, Loss: 0.7158824279904366, Final Batch Loss: 0.12178076058626175\n",
      "Epoch 6839, Loss: 0.6001608818769455, Final Batch Loss: 0.10882335901260376\n",
      "Epoch 6840, Loss: 0.612562395632267, Final Batch Loss: 0.10552936047315598\n",
      "Epoch 6841, Loss: 0.6424311213195324, Final Batch Loss: 0.03288218006491661\n",
      "Epoch 6842, Loss: 0.5958825424313545, Final Batch Loss: 0.1055980995297432\n",
      "Epoch 6843, Loss: 0.7232642471790314, Final Batch Loss: 0.25357627868652344\n",
      "Epoch 6844, Loss: 0.70177361369133, Final Batch Loss: 0.22312353551387787\n",
      "Epoch 6845, Loss: 0.8409765809774399, Final Batch Loss: 0.2616298496723175\n",
      "Epoch 6846, Loss: 0.8754483014345169, Final Batch Loss: 0.38223668932914734\n",
      "Epoch 6847, Loss: 0.5382410064339638, Final Batch Loss: 0.09164930135011673\n",
      "Epoch 6848, Loss: 0.6735943406820297, Final Batch Loss: 0.15364040434360504\n",
      "Epoch 6849, Loss: 0.6681231111288071, Final Batch Loss: 0.19546709954738617\n",
      "Epoch 6850, Loss: 0.7133546471595764, Final Batch Loss: 0.1508624404668808\n",
      "Epoch 6851, Loss: 0.6768866255879402, Final Batch Loss: 0.09598840028047562\n",
      "Epoch 6852, Loss: 0.6891065686941147, Final Batch Loss: 0.13050083816051483\n",
      "Epoch 6853, Loss: 0.7765962779521942, Final Batch Loss: 0.248540461063385\n",
      "Epoch 6854, Loss: 0.8830341547727585, Final Batch Loss: 0.3372305929660797\n",
      "Epoch 6855, Loss: 0.83428855240345, Final Batch Loss: 0.26266908645629883\n",
      "Epoch 6856, Loss: 0.6962423771619797, Final Batch Loss: 0.07404729723930359\n",
      "Epoch 6857, Loss: 0.9563664197921753, Final Batch Loss: 0.31688737869262695\n",
      "Epoch 6858, Loss: 0.8877477496862411, Final Batch Loss: 0.2512727677822113\n",
      "Epoch 6859, Loss: 0.9821217954158783, Final Batch Loss: 0.3046892285346985\n",
      "Epoch 6860, Loss: 0.8007695227861404, Final Batch Loss: 0.19792640209197998\n",
      "Epoch 6861, Loss: 0.784079447388649, Final Batch Loss: 0.13428767025470734\n",
      "Epoch 6862, Loss: 0.8455805480480194, Final Batch Loss: 0.3599245250225067\n",
      "Epoch 6863, Loss: 0.7001999914646149, Final Batch Loss: 0.1665143221616745\n",
      "Epoch 6864, Loss: 0.8128593862056732, Final Batch Loss: 0.22260314226150513\n",
      "Epoch 6865, Loss: 0.8439340591430664, Final Batch Loss: 0.26443788409233093\n",
      "Epoch 6866, Loss: 0.7703167200088501, Final Batch Loss: 0.2644879221916199\n",
      "Epoch 6867, Loss: 0.832692101597786, Final Batch Loss: 0.28246691823005676\n",
      "Epoch 6868, Loss: 0.7684957683086395, Final Batch Loss: 0.23757629096508026\n",
      "Epoch 6869, Loss: 0.7872350513935089, Final Batch Loss: 0.2668059766292572\n",
      "Epoch 6870, Loss: 0.7051214575767517, Final Batch Loss: 0.12823240458965302\n",
      "Epoch 6871, Loss: 0.6777965798974037, Final Batch Loss: 0.1814238280057907\n",
      "Epoch 6872, Loss: 0.6272459700703621, Final Batch Loss: 0.1098332330584526\n",
      "Epoch 6873, Loss: 0.6941343545913696, Final Batch Loss: 0.1510864794254303\n",
      "Epoch 6874, Loss: 0.8219841569662094, Final Batch Loss: 0.2866501212120056\n",
      "Epoch 6875, Loss: 0.7745354771614075, Final Batch Loss: 0.20682847499847412\n",
      "Epoch 6876, Loss: 0.8084001690149307, Final Batch Loss: 0.3046150207519531\n",
      "Epoch 6877, Loss: 0.7753916531801224, Final Batch Loss: 0.23665952682495117\n",
      "Epoch 6878, Loss: 0.7394255772233009, Final Batch Loss: 0.24808205664157867\n",
      "Epoch 6879, Loss: 0.5778853744268417, Final Batch Loss: 0.1266453117132187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6880, Loss: 0.7941178679466248, Final Batch Loss: 0.25667604804039\n",
      "Epoch 6881, Loss: 0.7922433316707611, Final Batch Loss: 0.169089213013649\n",
      "Epoch 6882, Loss: 0.7703248560428619, Final Batch Loss: 0.19045805931091309\n",
      "Epoch 6883, Loss: 0.6683589555323124, Final Batch Loss: 0.060575950890779495\n",
      "Epoch 6884, Loss: 0.6870785504579544, Final Batch Loss: 0.1326223611831665\n",
      "Epoch 6885, Loss: 0.8387442380189896, Final Batch Loss: 0.30482760071754456\n",
      "Epoch 6886, Loss: 0.6622762903571129, Final Batch Loss: 0.1115674301981926\n",
      "Epoch 6887, Loss: 0.6286556199193001, Final Batch Loss: 0.09576710313558578\n",
      "Epoch 6888, Loss: 0.8298239707946777, Final Batch Loss: 0.21911637485027313\n",
      "Epoch 6889, Loss: 0.8570670187473297, Final Batch Loss: 0.41491973400115967\n",
      "Epoch 6890, Loss: 0.5934877470135689, Final Batch Loss: 0.10902593284845352\n",
      "Epoch 6891, Loss: 0.6759777218103409, Final Batch Loss: 0.13590988516807556\n",
      "Epoch 6892, Loss: 0.6077220886945724, Final Batch Loss: 0.10074126720428467\n",
      "Epoch 6893, Loss: 0.665306955575943, Final Batch Loss: 0.1955745369195938\n",
      "Epoch 6894, Loss: 0.7962479293346405, Final Batch Loss: 0.3016640841960907\n",
      "Epoch 6895, Loss: 0.961688369512558, Final Batch Loss: 0.3961295783519745\n",
      "Epoch 6896, Loss: 0.6386555209755898, Final Batch Loss: 0.18660672008991241\n",
      "Epoch 6897, Loss: 0.6981435045599937, Final Batch Loss: 0.11012109369039536\n",
      "Epoch 6898, Loss: 0.7999484539031982, Final Batch Loss: 0.1787140965461731\n",
      "Epoch 6899, Loss: 0.8625460118055344, Final Batch Loss: 0.22900712490081787\n",
      "Epoch 6900, Loss: 0.8259893357753754, Final Batch Loss: 0.2176494598388672\n",
      "Epoch 6901, Loss: 0.7703247368335724, Final Batch Loss: 0.1823192983865738\n",
      "Epoch 6902, Loss: 0.9401163458824158, Final Batch Loss: 0.28793635964393616\n",
      "Epoch 6903, Loss: 0.8349859118461609, Final Batch Loss: 0.1914016604423523\n",
      "Epoch 6904, Loss: 0.9361241012811661, Final Batch Loss: 0.40707024931907654\n",
      "Epoch 6905, Loss: 0.9622508585453033, Final Batch Loss: 0.3536123037338257\n",
      "Epoch 6906, Loss: 0.8529631346464157, Final Batch Loss: 0.21451841294765472\n",
      "Epoch 6907, Loss: 0.9286301881074905, Final Batch Loss: 0.27756965160369873\n",
      "Epoch 6908, Loss: 0.8913147449493408, Final Batch Loss: 0.13783122599124908\n",
      "Epoch 6909, Loss: 0.6165863275527954, Final Batch Loss: 0.09400471299886703\n",
      "Epoch 6910, Loss: 0.8647917062044144, Final Batch Loss: 0.23358575999736786\n",
      "Epoch 6911, Loss: 0.777040883898735, Final Batch Loss: 0.1768435686826706\n",
      "Epoch 6912, Loss: 0.7639919370412827, Final Batch Loss: 0.13745005428791046\n",
      "Epoch 6913, Loss: 0.609100729227066, Final Batch Loss: 0.08772018551826477\n",
      "Epoch 6914, Loss: 0.8861460238695145, Final Batch Loss: 0.24944663047790527\n",
      "Epoch 6915, Loss: 0.768351137638092, Final Batch Loss: 0.2728712856769562\n",
      "Epoch 6916, Loss: 0.615572914481163, Final Batch Loss: 0.13140837848186493\n",
      "Epoch 6917, Loss: 0.8534665554761887, Final Batch Loss: 0.13532251119613647\n",
      "Epoch 6918, Loss: 0.68169154971838, Final Batch Loss: 0.19652265310287476\n",
      "Epoch 6919, Loss: 0.8574943393468857, Final Batch Loss: 0.3351106345653534\n",
      "Epoch 6920, Loss: 0.702964723110199, Final Batch Loss: 0.13317136466503143\n",
      "Epoch 6921, Loss: 0.7165532931685448, Final Batch Loss: 0.2332579642534256\n",
      "Epoch 6922, Loss: 0.900674045085907, Final Batch Loss: 0.3052268922328949\n",
      "Epoch 6923, Loss: 0.6789909973740578, Final Batch Loss: 0.08497416228055954\n",
      "Epoch 6924, Loss: 0.7999598234891891, Final Batch Loss: 0.28477033972740173\n",
      "Epoch 6925, Loss: 0.8582610338926315, Final Batch Loss: 0.39757058024406433\n",
      "Epoch 6926, Loss: 0.9144666343927383, Final Batch Loss: 0.2629382908344269\n",
      "Epoch 6927, Loss: 0.764962375164032, Final Batch Loss: 0.253958523273468\n",
      "Epoch 6928, Loss: 0.7815915495157242, Final Batch Loss: 0.32370516657829285\n",
      "Epoch 6929, Loss: 0.6686799973249435, Final Batch Loss: 0.1381916105747223\n",
      "Epoch 6930, Loss: 0.7106946557760239, Final Batch Loss: 0.12694083154201508\n",
      "Epoch 6931, Loss: 0.8482906073331833, Final Batch Loss: 0.2259882539510727\n",
      "Epoch 6932, Loss: 0.7950889021158218, Final Batch Loss: 0.18881529569625854\n",
      "Epoch 6933, Loss: 0.7834523618221283, Final Batch Loss: 0.17464913427829742\n",
      "Epoch 6934, Loss: 0.9098583608865738, Final Batch Loss: 0.23900222778320312\n",
      "Epoch 6935, Loss: 0.7310778498649597, Final Batch Loss: 0.20651763677597046\n",
      "Epoch 6936, Loss: 0.6543646454811096, Final Batch Loss: 0.1395360231399536\n",
      "Epoch 6937, Loss: 0.8708181828260422, Final Batch Loss: 0.3062526285648346\n",
      "Epoch 6938, Loss: 0.5671870708465576, Final Batch Loss: 0.07499316334724426\n",
      "Epoch 6939, Loss: 0.5982835441827774, Final Batch Loss: 0.1180378794670105\n",
      "Epoch 6940, Loss: 0.7456678599119186, Final Batch Loss: 0.23760227859020233\n",
      "Epoch 6941, Loss: 0.685760572552681, Final Batch Loss: 0.08166147768497467\n",
      "Epoch 6942, Loss: 0.930716261267662, Final Batch Loss: 0.34191903471946716\n",
      "Epoch 6943, Loss: 0.7209957838058472, Final Batch Loss: 0.1790526658296585\n",
      "Epoch 6944, Loss: 0.6566594392061234, Final Batch Loss: 0.1926809698343277\n",
      "Epoch 6945, Loss: 0.6826777905225754, Final Batch Loss: 0.1440344601869583\n",
      "Epoch 6946, Loss: 0.6717154681682587, Final Batch Loss: 0.2189895659685135\n",
      "Epoch 6947, Loss: 0.7594656944274902, Final Batch Loss: 0.2589334547519684\n",
      "Epoch 6948, Loss: 0.6264801472425461, Final Batch Loss: 0.14174236357212067\n",
      "Epoch 6949, Loss: 0.7949502170085907, Final Batch Loss: 0.14899404346942902\n",
      "Epoch 6950, Loss: 0.7487445324659348, Final Batch Loss: 0.15050345659255981\n",
      "Epoch 6951, Loss: 0.886210709810257, Final Batch Loss: 0.3049190938472748\n",
      "Epoch 6952, Loss: 0.7827264070510864, Final Batch Loss: 0.3176155686378479\n",
      "Epoch 6953, Loss: 0.6530031859874725, Final Batch Loss: 0.15820924937725067\n",
      "Epoch 6954, Loss: 1.0150635987520218, Final Batch Loss: 0.43270230293273926\n",
      "Epoch 6955, Loss: 0.6612483859062195, Final Batch Loss: 0.1672898530960083\n",
      "Epoch 6956, Loss: 0.7916902601718903, Final Batch Loss: 0.21498262882232666\n",
      "Epoch 6957, Loss: 0.7467058300971985, Final Batch Loss: 0.19321443140506744\n",
      "Epoch 6958, Loss: 0.7799418270587921, Final Batch Loss: 0.16412471234798431\n",
      "Epoch 6959, Loss: 0.7389369904994965, Final Batch Loss: 0.14519159495830536\n",
      "Epoch 6960, Loss: 0.6477328464388847, Final Batch Loss: 0.10558588057756424\n",
      "Epoch 6961, Loss: 0.7364204153418541, Final Batch Loss: 0.09417591243982315\n",
      "Epoch 6962, Loss: 0.7025477290153503, Final Batch Loss: 0.16123664379119873\n",
      "Epoch 6963, Loss: 0.6438769474625587, Final Batch Loss: 0.12172912806272507\n",
      "Epoch 6964, Loss: 0.7840643227100372, Final Batch Loss: 0.22290058434009552\n",
      "Epoch 6965, Loss: 0.7418390214443207, Final Batch Loss: 0.23420345783233643\n",
      "Epoch 6966, Loss: 0.8008058071136475, Final Batch Loss: 0.21916288137435913\n",
      "Epoch 6967, Loss: 0.765585720539093, Final Batch Loss: 0.18447941541671753\n",
      "Epoch 6968, Loss: 0.7044088467955589, Final Batch Loss: 0.10143188387155533\n",
      "Epoch 6969, Loss: 0.8329740166664124, Final Batch Loss: 0.20713560283184052\n",
      "Epoch 6970, Loss: 0.7435346692800522, Final Batch Loss: 0.17008014023303986\n",
      "Epoch 6971, Loss: 0.6847463399171829, Final Batch Loss: 0.1908041089773178\n",
      "Epoch 6972, Loss: 0.6253114156424999, Final Batch Loss: 0.04247790202498436\n",
      "Epoch 6973, Loss: 0.6119097508490086, Final Batch Loss: 0.057875875383615494\n",
      "Epoch 6974, Loss: 0.6370195969939232, Final Batch Loss: 0.07701199501752853\n",
      "Epoch 6975, Loss: 0.8756591826677322, Final Batch Loss: 0.38961872458457947\n",
      "Epoch 6976, Loss: 0.6351027190685272, Final Batch Loss: 0.12938813865184784\n",
      "Epoch 6977, Loss: 0.5801389515399933, Final Batch Loss: 0.046671077609062195\n",
      "Epoch 6978, Loss: 0.7494424432516098, Final Batch Loss: 0.2322232872247696\n",
      "Epoch 6979, Loss: 0.6619761809706688, Final Batch Loss: 0.11405234783887863\n",
      "Epoch 6980, Loss: 0.8842533528804779, Final Batch Loss: 0.25016215443611145\n",
      "Epoch 6981, Loss: 0.8134701400995255, Final Batch Loss: 0.3115725815296173\n",
      "Epoch 6982, Loss: 0.7382550984621048, Final Batch Loss: 0.1309741884469986\n",
      "Epoch 6983, Loss: 0.6678777933120728, Final Batch Loss: 0.1723295897245407\n",
      "Epoch 6984, Loss: 0.6887393593788147, Final Batch Loss: 0.23235784471035004\n",
      "Epoch 6985, Loss: 0.7370734363794327, Final Batch Loss: 0.18119250237941742\n",
      "Epoch 6986, Loss: 0.6341682448983192, Final Batch Loss: 0.15808182954788208\n",
      "Epoch 6987, Loss: 0.7012212425470352, Final Batch Loss: 0.16784964501857758\n",
      "Epoch 6988, Loss: 0.5357905700802803, Final Batch Loss: 0.07356756180524826\n",
      "Epoch 6989, Loss: 0.7626353725790977, Final Batch Loss: 0.27947187423706055\n",
      "Epoch 6990, Loss: 0.7725071609020233, Final Batch Loss: 0.27497467398643494\n",
      "Epoch 6991, Loss: 0.6362036019563675, Final Batch Loss: 0.1799248903989792\n",
      "Epoch 6992, Loss: 0.7444739788770676, Final Batch Loss: 0.223378524184227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6993, Loss: 0.7556620687246323, Final Batch Loss: 0.18640436232089996\n",
      "Epoch 6994, Loss: 0.8079124689102173, Final Batch Loss: 0.27850082516670227\n",
      "Epoch 6995, Loss: 0.8609261214733124, Final Batch Loss: 0.30034521222114563\n",
      "Epoch 6996, Loss: 1.0363205075263977, Final Batch Loss: 0.4064328968524933\n",
      "Epoch 6997, Loss: 0.855386808514595, Final Batch Loss: 0.22390438616275787\n",
      "Epoch 6998, Loss: 0.9648839384317398, Final Batch Loss: 0.26760783791542053\n",
      "Epoch 6999, Loss: 1.0266605913639069, Final Batch Loss: 0.32883593440055847\n",
      "Epoch 7000, Loss: 0.8513134717941284, Final Batch Loss: 0.2918361723423004\n",
      "Epoch 7001, Loss: 0.739567942917347, Final Batch Loss: 0.11188525706529617\n",
      "Epoch 7002, Loss: 0.7802871763706207, Final Batch Loss: 0.13366544246673584\n",
      "Epoch 7003, Loss: 0.924637034535408, Final Batch Loss: 0.33698323369026184\n",
      "Epoch 7004, Loss: 0.7680896818637848, Final Batch Loss: 0.23946942389011383\n",
      "Epoch 7005, Loss: 0.8098471611738205, Final Batch Loss: 0.17485930025577545\n",
      "Epoch 7006, Loss: 0.7118726968765259, Final Batch Loss: 0.13195496797561646\n",
      "Epoch 7007, Loss: 0.6475715860724449, Final Batch Loss: 0.08959496766328812\n",
      "Epoch 7008, Loss: 0.8148961812257767, Final Batch Loss: 0.17219030857086182\n",
      "Epoch 7009, Loss: 0.5727328956127167, Final Batch Loss: 0.1411294937133789\n",
      "Epoch 7010, Loss: 0.7991514056921005, Final Batch Loss: 0.2438390851020813\n",
      "Epoch 7011, Loss: 0.7967348694801331, Final Batch Loss: 0.3312002420425415\n",
      "Epoch 7012, Loss: 0.9462235420942307, Final Batch Loss: 0.3369580805301666\n",
      "Epoch 7013, Loss: 0.6135857701301575, Final Batch Loss: 0.06588992476463318\n",
      "Epoch 7014, Loss: 0.7829733490943909, Final Batch Loss: 0.17648033797740936\n",
      "Epoch 7015, Loss: 0.7622004747390747, Final Batch Loss: 0.19403402507305145\n",
      "Epoch 7016, Loss: 0.7232602536678314, Final Batch Loss: 0.1757810264825821\n",
      "Epoch 7017, Loss: 0.6348540112376213, Final Batch Loss: 0.06676933914422989\n",
      "Epoch 7018, Loss: 0.6479174420237541, Final Batch Loss: 0.09768406301736832\n",
      "Epoch 7019, Loss: 0.6248380392789841, Final Batch Loss: 0.1866583675146103\n",
      "Epoch 7020, Loss: 0.5691016763448715, Final Batch Loss: 0.07308218628168106\n",
      "Epoch 7021, Loss: 0.6800783202052116, Final Batch Loss: 0.11799447983503342\n",
      "Epoch 7022, Loss: 0.637066513299942, Final Batch Loss: 0.200465127825737\n",
      "Epoch 7023, Loss: 0.6353113949298859, Final Batch Loss: 0.1459037959575653\n",
      "Epoch 7024, Loss: 0.7641210407018661, Final Batch Loss: 0.1591074913740158\n",
      "Epoch 7025, Loss: 0.7329825162887573, Final Batch Loss: 0.24183231592178345\n",
      "Epoch 7026, Loss: 0.8275713175535202, Final Batch Loss: 0.35595211386680603\n",
      "Epoch 7027, Loss: 0.7962343841791153, Final Batch Loss: 0.16656281054019928\n",
      "Epoch 7028, Loss: 0.8089387863874435, Final Batch Loss: 0.31513407826423645\n",
      "Epoch 7029, Loss: 0.6095175668597221, Final Batch Loss: 0.15022091567516327\n",
      "Epoch 7030, Loss: 0.6864237785339355, Final Batch Loss: 0.16115368902683258\n",
      "Epoch 7031, Loss: 0.6854546964168549, Final Batch Loss: 0.1397685557603836\n",
      "Epoch 7032, Loss: 0.6748405694961548, Final Batch Loss: 0.09208610653877258\n",
      "Epoch 7033, Loss: 0.8153944909572601, Final Batch Loss: 0.21541698276996613\n",
      "Epoch 7034, Loss: 0.5815589167177677, Final Batch Loss: 0.034630466252565384\n",
      "Epoch 7035, Loss: 0.7285278588533401, Final Batch Loss: 0.17690210044384003\n",
      "Epoch 7036, Loss: 0.5810008496046066, Final Batch Loss: 0.11945268511772156\n",
      "Epoch 7037, Loss: 0.7213259860873222, Final Batch Loss: 0.25225239992141724\n",
      "Epoch 7038, Loss: 0.8289058655500412, Final Batch Loss: 0.2339351624250412\n",
      "Epoch 7039, Loss: 0.6172387823462486, Final Batch Loss: 0.10285628587007523\n",
      "Epoch 7040, Loss: 0.7426114529371262, Final Batch Loss: 0.18811309337615967\n",
      "Epoch 7041, Loss: 0.7348121255636215, Final Batch Loss: 0.17222772538661957\n",
      "Epoch 7042, Loss: 0.6393722333014011, Final Batch Loss: 0.044168416410684586\n",
      "Epoch 7043, Loss: 0.9020250141620636, Final Batch Loss: 0.39118456840515137\n",
      "Epoch 7044, Loss: 0.7585448324680328, Final Batch Loss: 0.20455384254455566\n",
      "Epoch 7045, Loss: 0.6465296596288681, Final Batch Loss: 0.12791194021701813\n",
      "Epoch 7046, Loss: 0.5063171871006489, Final Batch Loss: 0.0421302355825901\n",
      "Epoch 7047, Loss: 0.6820208132266998, Final Batch Loss: 0.13078314065933228\n",
      "Epoch 7048, Loss: 0.6411311402916908, Final Batch Loss: 0.089853934943676\n",
      "Epoch 7049, Loss: 0.7645289599895477, Final Batch Loss: 0.31768134236335754\n",
      "Epoch 7050, Loss: 0.6258708536624908, Final Batch Loss: 0.18270623683929443\n",
      "Epoch 7051, Loss: 0.6159749701619148, Final Batch Loss: 0.11366984993219376\n",
      "Epoch 7052, Loss: 0.6673894822597504, Final Batch Loss: 0.16604310274124146\n",
      "Epoch 7053, Loss: 0.7329719811677933, Final Batch Loss: 0.20596717298030853\n",
      "Epoch 7054, Loss: 0.6131595745682716, Final Batch Loss: 0.12931989133358002\n",
      "Epoch 7055, Loss: 0.8375153988599777, Final Batch Loss: 0.34100353717803955\n",
      "Epoch 7056, Loss: 0.6609076485037804, Final Batch Loss: 0.1746697574853897\n",
      "Epoch 7057, Loss: 0.6047010719776154, Final Batch Loss: 0.162558451294899\n",
      "Epoch 7058, Loss: 0.8780301511287689, Final Batch Loss: 0.2929931581020355\n",
      "Epoch 7059, Loss: 0.6168104037642479, Final Batch Loss: 0.1148514673113823\n",
      "Epoch 7060, Loss: 0.6877710446715355, Final Batch Loss: 0.10370274633169174\n",
      "Epoch 7061, Loss: 0.75845567882061, Final Batch Loss: 0.2013983130455017\n",
      "Epoch 7062, Loss: 0.8078646808862686, Final Batch Loss: 0.2172033041715622\n",
      "Epoch 7063, Loss: 0.6543226987123489, Final Batch Loss: 0.1681791990995407\n",
      "Epoch 7064, Loss: 0.6881241649389267, Final Batch Loss: 0.2597932517528534\n",
      "Epoch 7065, Loss: 0.6415790468454361, Final Batch Loss: 0.14294728636741638\n",
      "Epoch 7066, Loss: 0.697732076048851, Final Batch Loss: 0.20897866785526276\n",
      "Epoch 7067, Loss: 0.6003031134605408, Final Batch Loss: 0.14434480667114258\n",
      "Epoch 7068, Loss: 0.9062621593475342, Final Batch Loss: 0.23144829273223877\n",
      "Epoch 7069, Loss: 0.6221296563744545, Final Batch Loss: 0.13397715985774994\n",
      "Epoch 7070, Loss: 0.9592772722244263, Final Batch Loss: 0.3509329855442047\n",
      "Epoch 7071, Loss: 0.518675372004509, Final Batch Loss: 0.08016668260097504\n",
      "Epoch 7072, Loss: 0.7503692507743835, Final Batch Loss: 0.21168898046016693\n",
      "Epoch 7073, Loss: 0.7174039930105209, Final Batch Loss: 0.19724446535110474\n",
      "Epoch 7074, Loss: 0.7374818623065948, Final Batch Loss: 0.23057645559310913\n",
      "Epoch 7075, Loss: 0.519665539264679, Final Batch Loss: 0.06391340494155884\n",
      "Epoch 7076, Loss: 0.7609800770878792, Final Batch Loss: 0.17932800948619843\n",
      "Epoch 7077, Loss: 0.6590600609779358, Final Batch Loss: 0.09014970064163208\n",
      "Epoch 7078, Loss: 0.7041501551866531, Final Batch Loss: 0.2064952403306961\n",
      "Epoch 7079, Loss: 0.6222364753484726, Final Batch Loss: 0.11738443374633789\n",
      "Epoch 7080, Loss: 0.6873068585991859, Final Batch Loss: 0.1193549633026123\n",
      "Epoch 7081, Loss: 0.5681943148374557, Final Batch Loss: 0.12978602945804596\n",
      "Epoch 7082, Loss: 0.4743746668100357, Final Batch Loss: 0.05465365946292877\n",
      "Epoch 7083, Loss: 0.7744166702032089, Final Batch Loss: 0.21068739891052246\n",
      "Epoch 7084, Loss: 0.54765984416008, Final Batch Loss: 0.09896545857191086\n",
      "Epoch 7085, Loss: 0.6152941733598709, Final Batch Loss: 0.08849576115608215\n",
      "Epoch 7086, Loss: 0.816260039806366, Final Batch Loss: 0.28430652618408203\n",
      "Epoch 7087, Loss: 0.7990107536315918, Final Batch Loss: 0.2681913673877716\n",
      "Epoch 7088, Loss: 0.7726776003837585, Final Batch Loss: 0.26314976811408997\n",
      "Epoch 7089, Loss: 0.7167278230190277, Final Batch Loss: 0.1539766639471054\n",
      "Epoch 7090, Loss: 0.7768940478563309, Final Batch Loss: 0.16232667863368988\n",
      "Epoch 7091, Loss: 0.633762925863266, Final Batch Loss: 0.1533580869436264\n",
      "Epoch 7092, Loss: 0.7716899663209915, Final Batch Loss: 0.26840290427207947\n",
      "Epoch 7093, Loss: 0.6807134598493576, Final Batch Loss: 0.12453675270080566\n",
      "Epoch 7094, Loss: 0.840556725859642, Final Batch Loss: 0.22815102338790894\n",
      "Epoch 7095, Loss: 0.7461204305291176, Final Batch Loss: 0.1415645033121109\n",
      "Epoch 7096, Loss: 0.6378425359725952, Final Batch Loss: 0.1312260776758194\n",
      "Epoch 7097, Loss: 0.6568528860807419, Final Batch Loss: 0.15115834772586823\n",
      "Epoch 7098, Loss: 0.6322138532996178, Final Batch Loss: 0.09747207909822464\n",
      "Epoch 7099, Loss: 0.5967221930623055, Final Batch Loss: 0.1525087058544159\n",
      "Epoch 7100, Loss: 0.5220656059682369, Final Batch Loss: 0.04954321309924126\n",
      "Epoch 7101, Loss: 0.5880157388746738, Final Batch Loss: 0.058452848345041275\n",
      "Epoch 7102, Loss: 0.8751218020915985, Final Batch Loss: 0.36434921622276306\n",
      "Epoch 7103, Loss: 0.7498389929533005, Final Batch Loss: 0.210646852850914\n",
      "Epoch 7104, Loss: 0.7451781928539276, Final Batch Loss: 0.15612533688545227\n",
      "Epoch 7105, Loss: 0.7163592055439949, Final Batch Loss: 0.0829557552933693\n",
      "Epoch 7106, Loss: 0.8094326108694077, Final Batch Loss: 0.27695655822753906\n",
      "Epoch 7107, Loss: 0.7829425185918808, Final Batch Loss: 0.2772156894207001\n",
      "Epoch 7108, Loss: 0.6201435625553131, Final Batch Loss: 0.14250026643276215\n",
      "Epoch 7109, Loss: 0.6257824301719666, Final Batch Loss: 0.19672159850597382\n",
      "Epoch 7110, Loss: 0.624925896525383, Final Batch Loss: 0.11674851179122925\n",
      "Epoch 7111, Loss: 0.6849706918001175, Final Batch Loss: 0.12604951858520508\n",
      "Epoch 7112, Loss: 0.6741371601819992, Final Batch Loss: 0.17533089220523834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7113, Loss: 0.6535527110099792, Final Batch Loss: 0.13679508864879608\n",
      "Epoch 7114, Loss: 0.7574344575405121, Final Batch Loss: 0.19840341806411743\n",
      "Epoch 7115, Loss: 0.6909087598323822, Final Batch Loss: 0.15033189952373505\n",
      "Epoch 7116, Loss: 1.2008715867996216, Final Batch Loss: 0.2553366720676422\n",
      "Epoch 7117, Loss: 0.9375607222318649, Final Batch Loss: 0.21297673881053925\n",
      "Epoch 7118, Loss: 0.682496652007103, Final Batch Loss: 0.1932375282049179\n",
      "Epoch 7119, Loss: 0.7155111283063889, Final Batch Loss: 0.12697990238666534\n",
      "Epoch 7120, Loss: 0.76332838088274, Final Batch Loss: 0.10087937861680984\n",
      "Epoch 7121, Loss: 0.9051713049411774, Final Batch Loss: 0.3407953083515167\n",
      "Epoch 7122, Loss: 0.6408932209014893, Final Batch Loss: 0.12012630701065063\n",
      "Epoch 7123, Loss: 0.8407673090696335, Final Batch Loss: 0.25272372364997864\n",
      "Epoch 7124, Loss: 0.7962040603160858, Final Batch Loss: 0.27359458804130554\n",
      "Epoch 7125, Loss: 0.7205534428358078, Final Batch Loss: 0.16421259939670563\n",
      "Epoch 7126, Loss: 0.8089076280593872, Final Batch Loss: 0.20870570838451385\n",
      "Epoch 7127, Loss: 0.7483935356140137, Final Batch Loss: 0.17693538963794708\n",
      "Epoch 7128, Loss: 0.6678815856575966, Final Batch Loss: 0.11894736438989639\n",
      "Epoch 7129, Loss: 0.9951082617044449, Final Batch Loss: 0.4458044767379761\n",
      "Epoch 7130, Loss: 0.7984608858823776, Final Batch Loss: 0.21658621728420258\n",
      "Epoch 7131, Loss: 0.795354038476944, Final Batch Loss: 0.14880332350730896\n",
      "Epoch 7132, Loss: 0.8550220727920532, Final Batch Loss: 0.25958243012428284\n",
      "Epoch 7133, Loss: 0.7193777710199356, Final Batch Loss: 0.21649737656116486\n",
      "Epoch 7134, Loss: 0.6542443931102753, Final Batch Loss: 0.1210644543170929\n",
      "Epoch 7135, Loss: 0.7414178997278214, Final Batch Loss: 0.11870282888412476\n",
      "Epoch 7136, Loss: 0.7554632350802422, Final Batch Loss: 0.24250781536102295\n",
      "Epoch 7137, Loss: 0.7032753303647041, Final Batch Loss: 0.08005739003419876\n",
      "Epoch 7138, Loss: 0.6665844097733498, Final Batch Loss: 0.09159687906503677\n",
      "Epoch 7139, Loss: 0.8804264515638351, Final Batch Loss: 0.18531803786754608\n",
      "Epoch 7140, Loss: 0.8369735479354858, Final Batch Loss: 0.18921081721782684\n",
      "Epoch 7141, Loss: 0.6176842376589775, Final Batch Loss: 0.09196823090314865\n",
      "Epoch 7142, Loss: 0.8249203115701675, Final Batch Loss: 0.19957925379276276\n",
      "Epoch 7143, Loss: 0.745251789689064, Final Batch Loss: 0.1447610855102539\n",
      "Epoch 7144, Loss: 0.9967453330755234, Final Batch Loss: 0.5052922368049622\n",
      "Epoch 7145, Loss: 0.5455646608024836, Final Batch Loss: 0.02674523927271366\n",
      "Epoch 7146, Loss: 0.6309176832437515, Final Batch Loss: 0.14143528044223785\n",
      "Epoch 7147, Loss: 0.7078303396701813, Final Batch Loss: 0.22103893756866455\n",
      "Epoch 7148, Loss: 0.6378781870007515, Final Batch Loss: 0.09503486007452011\n",
      "Epoch 7149, Loss: 0.7516067624092102, Final Batch Loss: 0.22093528509140015\n",
      "Epoch 7150, Loss: 0.9098344296216965, Final Batch Loss: 0.24578018486499786\n",
      "Epoch 7151, Loss: 0.7476322799921036, Final Batch Loss: 0.21766047179698944\n",
      "Epoch 7152, Loss: 0.6551462486386299, Final Batch Loss: 0.071935273706913\n",
      "Epoch 7153, Loss: 0.8975159227848053, Final Batch Loss: 0.27986639738082886\n",
      "Epoch 7154, Loss: 0.7006195187568665, Final Batch Loss: 0.24518819153308868\n",
      "Epoch 7155, Loss: 0.7896367013454437, Final Batch Loss: 0.17924851179122925\n",
      "Epoch 7156, Loss: 0.7384090721607208, Final Batch Loss: 0.15405596792697906\n",
      "Epoch 7157, Loss: 0.6644088923931122, Final Batch Loss: 0.12093088030815125\n",
      "Epoch 7158, Loss: 0.7774814516305923, Final Batch Loss: 0.13972783088684082\n",
      "Epoch 7159, Loss: 0.9987504631280899, Final Batch Loss: 0.5093966126441956\n",
      "Epoch 7160, Loss: 0.6596538946032524, Final Batch Loss: 0.09755287319421768\n",
      "Epoch 7161, Loss: 0.6836148798465729, Final Batch Loss: 0.14483492076396942\n",
      "Epoch 7162, Loss: 0.719995528459549, Final Batch Loss: 0.24616102874279022\n",
      "Epoch 7163, Loss: 0.8366931527853012, Final Batch Loss: 0.17840664088726044\n",
      "Epoch 7164, Loss: 0.787020206451416, Final Batch Loss: 0.16771160066127777\n",
      "Epoch 7165, Loss: 0.701072096824646, Final Batch Loss: 0.19443292915821075\n",
      "Epoch 7166, Loss: 0.6923091262578964, Final Batch Loss: 0.2385161966085434\n",
      "Epoch 7167, Loss: 0.6695653945207596, Final Batch Loss: 0.16804319620132446\n",
      "Epoch 7168, Loss: 0.9394871145486832, Final Batch Loss: 0.42113569378852844\n",
      "Epoch 7169, Loss: 0.6714807748794556, Final Batch Loss: 0.20498858392238617\n",
      "Epoch 7170, Loss: 0.8433778434991837, Final Batch Loss: 0.23605895042419434\n",
      "Epoch 7171, Loss: 0.6816596314311028, Final Batch Loss: 0.10078198462724686\n",
      "Epoch 7172, Loss: 0.6892506033182144, Final Batch Loss: 0.12596754729747772\n",
      "Epoch 7173, Loss: 0.6256050616502762, Final Batch Loss: 0.1431434005498886\n",
      "Epoch 7174, Loss: 0.7333589643239975, Final Batch Loss: 0.24924278259277344\n",
      "Epoch 7175, Loss: 0.6539952009916306, Final Batch Loss: 0.1456054300069809\n",
      "Epoch 7176, Loss: 0.7347994744777679, Final Batch Loss: 0.20922952890396118\n",
      "Epoch 7177, Loss: 1.1807022839784622, Final Batch Loss: 0.5871972441673279\n",
      "Epoch 7178, Loss: 0.8160490989685059, Final Batch Loss: 0.22011591494083405\n",
      "Epoch 7179, Loss: 0.6186613440513611, Final Batch Loss: 0.1444663256406784\n",
      "Epoch 7180, Loss: 0.8318471163511276, Final Batch Loss: 0.25883325934410095\n",
      "Epoch 7181, Loss: 0.6827105209231377, Final Batch Loss: 0.09596040099859238\n",
      "Epoch 7182, Loss: 0.645799420773983, Final Batch Loss: 0.10736658424139023\n",
      "Epoch 7183, Loss: 0.6162470355629921, Final Batch Loss: 0.21162474155426025\n",
      "Epoch 7184, Loss: 0.666771799325943, Final Batch Loss: 0.22382299602031708\n",
      "Epoch 7185, Loss: 0.7999632209539413, Final Batch Loss: 0.2202553004026413\n",
      "Epoch 7186, Loss: 0.7893640249967575, Final Batch Loss: 0.21045224368572235\n",
      "Epoch 7187, Loss: 0.7584933340549469, Final Batch Loss: 0.19560009241104126\n",
      "Epoch 7188, Loss: 1.1454458981752396, Final Batch Loss: 0.4661391079425812\n",
      "Epoch 7189, Loss: 0.7189022451639175, Final Batch Loss: 0.16227973997592926\n",
      "Epoch 7190, Loss: 0.7827530354261398, Final Batch Loss: 0.1804189831018448\n",
      "Epoch 7191, Loss: 0.7839718461036682, Final Batch Loss: 0.15979759395122528\n",
      "Epoch 7192, Loss: 0.8546354174613953, Final Batch Loss: 0.32426562905311584\n",
      "Epoch 7193, Loss: 0.7160970270633698, Final Batch Loss: 0.18952791392803192\n",
      "Epoch 7194, Loss: 0.8821922540664673, Final Batch Loss: 0.3788717985153198\n",
      "Epoch 7195, Loss: 0.626866452395916, Final Batch Loss: 0.1382351964712143\n",
      "Epoch 7196, Loss: 0.8377627283334732, Final Batch Loss: 0.20209598541259766\n",
      "Epoch 7197, Loss: 0.6330714076757431, Final Batch Loss: 0.12499687075614929\n",
      "Epoch 7198, Loss: 0.824332594871521, Final Batch Loss: 0.25364866852760315\n",
      "Epoch 7199, Loss: 0.6418987959623337, Final Batch Loss: 0.11129796504974365\n",
      "Epoch 7200, Loss: 0.58453269302845, Final Batch Loss: 0.04501558840274811\n",
      "Epoch 7201, Loss: 0.7016889303922653, Final Batch Loss: 0.18202130496501923\n",
      "Epoch 7202, Loss: 0.7191458344459534, Final Batch Loss: 0.2907874584197998\n",
      "Epoch 7203, Loss: 0.6379212066531181, Final Batch Loss: 0.1232614740729332\n",
      "Epoch 7204, Loss: 0.7796259224414825, Final Batch Loss: 0.18274402618408203\n",
      "Epoch 7205, Loss: 0.8046512603759766, Final Batch Loss: 0.24359853565692902\n",
      "Epoch 7206, Loss: 0.7279346883296967, Final Batch Loss: 0.13802407681941986\n",
      "Epoch 7207, Loss: 0.750602588057518, Final Batch Loss: 0.19670553505420685\n",
      "Epoch 7208, Loss: 0.5828372612595558, Final Batch Loss: 0.07727477699518204\n",
      "Epoch 7209, Loss: 0.5860537588596344, Final Batch Loss: 0.10852540284395218\n",
      "Epoch 7210, Loss: 0.661980465054512, Final Batch Loss: 0.19810301065444946\n",
      "Epoch 7211, Loss: 0.6615642309188843, Final Batch Loss: 0.1365405172109604\n",
      "Epoch 7212, Loss: 0.7701270133256912, Final Batch Loss: 0.29165443778038025\n",
      "Epoch 7213, Loss: 0.7883711457252502, Final Batch Loss: 0.2518896162509918\n",
      "Epoch 7214, Loss: 0.7525927275419235, Final Batch Loss: 0.20466893911361694\n",
      "Epoch 7215, Loss: 0.7837210595607758, Final Batch Loss: 0.2510474622249603\n",
      "Epoch 7216, Loss: 0.6581024006009102, Final Batch Loss: 0.08020363003015518\n",
      "Epoch 7217, Loss: 0.6579647064208984, Final Batch Loss: 0.16200217604637146\n",
      "Epoch 7218, Loss: 0.7905510291457176, Final Batch Loss: 0.24469394981861115\n",
      "Epoch 7219, Loss: 0.7163882553577423, Final Batch Loss: 0.18260972201824188\n",
      "Epoch 7220, Loss: 0.977916955947876, Final Batch Loss: 0.41374993324279785\n",
      "Epoch 7221, Loss: 0.6512787938117981, Final Batch Loss: 0.10482347011566162\n",
      "Epoch 7222, Loss: 1.1152443587779999, Final Batch Loss: 0.48656201362609863\n",
      "Epoch 7223, Loss: 1.1023963242769241, Final Batch Loss: 0.45390424132347107\n",
      "Epoch 7224, Loss: 1.0910511910915375, Final Batch Loss: 0.4046970307826996\n",
      "Epoch 7225, Loss: 0.6516821756958961, Final Batch Loss: 0.10981079190969467\n",
      "Epoch 7226, Loss: 0.7188922762870789, Final Batch Loss: 0.151320680975914\n",
      "Epoch 7227, Loss: 0.7339809387922287, Final Batch Loss: 0.20102764666080475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7228, Loss: 0.9847130477428436, Final Batch Loss: 0.31174930930137634\n",
      "Epoch 7229, Loss: 0.8128800541162491, Final Batch Loss: 0.15005609393119812\n",
      "Epoch 7230, Loss: 0.6741908043622971, Final Batch Loss: 0.1812606304883957\n",
      "Epoch 7231, Loss: 0.7936667501926422, Final Batch Loss: 0.26487961411476135\n",
      "Epoch 7232, Loss: 0.7667458951473236, Final Batch Loss: 0.23724037408828735\n",
      "Epoch 7233, Loss: 0.8030628263950348, Final Batch Loss: 0.2885667383670807\n",
      "Epoch 7234, Loss: 0.723318487405777, Final Batch Loss: 0.16007043421268463\n",
      "Epoch 7235, Loss: 0.6901243776082993, Final Batch Loss: 0.1995939016342163\n",
      "Epoch 7236, Loss: 0.678400307893753, Final Batch Loss: 0.13830237090587616\n",
      "Epoch 7237, Loss: 0.6732314229011536, Final Batch Loss: 0.14660018682479858\n",
      "Epoch 7238, Loss: 0.6433755904436111, Final Batch Loss: 0.15767717361450195\n",
      "Epoch 7239, Loss: 0.7204462364315987, Final Batch Loss: 0.10261210054159164\n",
      "Epoch 7240, Loss: 0.7521772235631943, Final Batch Loss: 0.23734624683856964\n",
      "Epoch 7241, Loss: 0.5953771770000458, Final Batch Loss: 0.10685780644416809\n",
      "Epoch 7242, Loss: 0.826501190662384, Final Batch Loss: 0.25761768221855164\n",
      "Epoch 7243, Loss: 0.8677747845649719, Final Batch Loss: 0.1600044071674347\n",
      "Epoch 7244, Loss: 0.982427105307579, Final Batch Loss: 0.2508383095264435\n",
      "Epoch 7245, Loss: 0.6878362149000168, Final Batch Loss: 0.20305085182189941\n",
      "Epoch 7246, Loss: 0.7364893555641174, Final Batch Loss: 0.13086558878421783\n",
      "Epoch 7247, Loss: 0.6830343157052994, Final Batch Loss: 0.18423740565776825\n",
      "Epoch 7248, Loss: 0.7047948613762856, Final Batch Loss: 0.11187700182199478\n",
      "Epoch 7249, Loss: 0.5256953425705433, Final Batch Loss: 0.04653877392411232\n",
      "Epoch 7250, Loss: 0.6523556187748909, Final Batch Loss: 0.14968211948871613\n",
      "Epoch 7251, Loss: 0.6375458315014839, Final Batch Loss: 0.12469583004713058\n",
      "Epoch 7252, Loss: 0.6326616257429123, Final Batch Loss: 0.17477194964885712\n",
      "Epoch 7253, Loss: 0.7701293230056763, Final Batch Loss: 0.18563562631607056\n",
      "Epoch 7254, Loss: 0.9491756111383438, Final Batch Loss: 0.21545539796352386\n",
      "Epoch 7255, Loss: 0.8899874687194824, Final Batch Loss: 0.3394806385040283\n",
      "Epoch 7256, Loss: 0.7091947644948959, Final Batch Loss: 0.1443849354982376\n",
      "Epoch 7257, Loss: 0.7468991279602051, Final Batch Loss: 0.21447627246379852\n",
      "Epoch 7258, Loss: 0.6637323647737503, Final Batch Loss: 0.20715175569057465\n",
      "Epoch 7259, Loss: 0.7021080553531647, Final Batch Loss: 0.13009825348854065\n",
      "Epoch 7260, Loss: 0.8348207473754883, Final Batch Loss: 0.3475833833217621\n",
      "Epoch 7261, Loss: 0.7194084972143173, Final Batch Loss: 0.1789262741804123\n",
      "Epoch 7262, Loss: 0.7764277458190918, Final Batch Loss: 0.21873228251934052\n",
      "Epoch 7263, Loss: 0.7294901907444, Final Batch Loss: 0.22273869812488556\n",
      "Epoch 7264, Loss: 0.8242931663990021, Final Batch Loss: 0.2927500903606415\n",
      "Epoch 7265, Loss: 0.6803330928087234, Final Batch Loss: 0.16843383014202118\n",
      "Epoch 7266, Loss: 0.9061615318059921, Final Batch Loss: 0.5047566294670105\n",
      "Epoch 7267, Loss: 0.655090942978859, Final Batch Loss: 0.14008773863315582\n",
      "Epoch 7268, Loss: 0.9326990693807602, Final Batch Loss: 0.29648932814598083\n",
      "Epoch 7269, Loss: 0.7406195551156998, Final Batch Loss: 0.22824031114578247\n",
      "Epoch 7270, Loss: 0.7494446337223053, Final Batch Loss: 0.21631909906864166\n",
      "Epoch 7271, Loss: 0.7502012699842453, Final Batch Loss: 0.2725960314273834\n",
      "Epoch 7272, Loss: 0.8958318531513214, Final Batch Loss: 0.22698773443698883\n",
      "Epoch 7273, Loss: 0.675335705280304, Final Batch Loss: 0.1340242326259613\n",
      "Epoch 7274, Loss: 0.9471785873174667, Final Batch Loss: 0.35819339752197266\n",
      "Epoch 7275, Loss: 0.8265232741832733, Final Batch Loss: 0.1753777116537094\n",
      "Epoch 7276, Loss: 0.7313849776983261, Final Batch Loss: 0.16552014648914337\n",
      "Epoch 7277, Loss: 0.6506700068712234, Final Batch Loss: 0.16729836165905\n",
      "Epoch 7278, Loss: 0.5909876078367233, Final Batch Loss: 0.13569602370262146\n",
      "Epoch 7279, Loss: 0.581710658967495, Final Batch Loss: 0.10362131148576736\n",
      "Epoch 7280, Loss: 0.7560538649559021, Final Batch Loss: 0.1423107236623764\n",
      "Epoch 7281, Loss: 0.7113864496350288, Final Batch Loss: 0.2728269398212433\n",
      "Epoch 7282, Loss: 0.7110806033015251, Final Batch Loss: 0.25629034638404846\n",
      "Epoch 7283, Loss: 0.742781862616539, Final Batch Loss: 0.17429590225219727\n",
      "Epoch 7284, Loss: 0.6600703746080399, Final Batch Loss: 0.13858218491077423\n",
      "Epoch 7285, Loss: 0.5631762072443962, Final Batch Loss: 0.11925842612981796\n",
      "Epoch 7286, Loss: 0.706564337015152, Final Batch Loss: 0.19544337689876556\n",
      "Epoch 7287, Loss: 0.5968129113316536, Final Batch Loss: 0.07683245092630386\n",
      "Epoch 7288, Loss: 0.6019440814852715, Final Batch Loss: 0.07781776040792465\n",
      "Epoch 7289, Loss: 0.5641249045729637, Final Batch Loss: 0.07574676722288132\n",
      "Epoch 7290, Loss: 0.5781858637928963, Final Batch Loss: 0.08139406889677048\n",
      "Epoch 7291, Loss: 0.5143135264515877, Final Batch Loss: 0.0683356374502182\n",
      "Epoch 7292, Loss: 0.6610378921031952, Final Batch Loss: 0.19555532932281494\n",
      "Epoch 7293, Loss: 0.715207576751709, Final Batch Loss: 0.13643242418766022\n",
      "Epoch 7294, Loss: 0.7374537140130997, Final Batch Loss: 0.25689423084259033\n",
      "Epoch 7295, Loss: 0.7782247215509415, Final Batch Loss: 0.3219743072986603\n",
      "Epoch 7296, Loss: 0.7148570269346237, Final Batch Loss: 0.2509707510471344\n",
      "Epoch 7297, Loss: 0.7453873157501221, Final Batch Loss: 0.19509409368038177\n",
      "Epoch 7298, Loss: 0.6719866394996643, Final Batch Loss: 0.13550078868865967\n",
      "Epoch 7299, Loss: 0.5503047555685043, Final Batch Loss: 0.062417373061180115\n",
      "Epoch 7300, Loss: 0.7643352895975113, Final Batch Loss: 0.23781758546829224\n",
      "Epoch 7301, Loss: 0.6864713653922081, Final Batch Loss: 0.07094617933034897\n",
      "Epoch 7302, Loss: 0.5936853885650635, Final Batch Loss: 0.07541336119174957\n",
      "Epoch 7303, Loss: 0.8445804417133331, Final Batch Loss: 0.3000095784664154\n",
      "Epoch 7304, Loss: 0.6824751123785973, Final Batch Loss: 0.07787933200597763\n",
      "Epoch 7305, Loss: 0.8221795409917831, Final Batch Loss: 0.18204759061336517\n",
      "Epoch 7306, Loss: 0.7008958980441093, Final Batch Loss: 0.11159307509660721\n",
      "Epoch 7307, Loss: 0.7252660468220711, Final Batch Loss: 0.12182962149381638\n",
      "Epoch 7308, Loss: 0.6389716044068336, Final Batch Loss: 0.0822213813662529\n",
      "Epoch 7309, Loss: 0.6486325934529305, Final Batch Loss: 0.10833790153265\n",
      "Epoch 7310, Loss: 0.6953854784369469, Final Batch Loss: 0.08212318271398544\n",
      "Epoch 7311, Loss: 0.8274298012256622, Final Batch Loss: 0.33517464995384216\n",
      "Epoch 7312, Loss: 0.5743812620639801, Final Batch Loss: 0.11442727595567703\n",
      "Epoch 7313, Loss: 0.7335756570100784, Final Batch Loss: 0.09799104928970337\n",
      "Epoch 7314, Loss: 0.7250280119478703, Final Batch Loss: 0.061974797397851944\n",
      "Epoch 7315, Loss: 0.6513766497373581, Final Batch Loss: 0.14141762256622314\n",
      "Epoch 7316, Loss: 0.6437544226646423, Final Batch Loss: 0.09700259566307068\n",
      "Epoch 7317, Loss: 0.7997503578662872, Final Batch Loss: 0.2193802446126938\n",
      "Epoch 7318, Loss: 0.6290226876735687, Final Batch Loss: 0.20979052782058716\n",
      "Epoch 7319, Loss: 0.7612819969654083, Final Batch Loss: 0.1390477567911148\n",
      "Epoch 7320, Loss: 0.6338298618793488, Final Batch Loss: 0.11661234498023987\n",
      "Epoch 7321, Loss: 0.8756420314311981, Final Batch Loss: 0.3353769779205322\n",
      "Epoch 7322, Loss: 0.7580022141337395, Final Batch Loss: 0.18077082931995392\n",
      "Epoch 7323, Loss: 0.5936018712818623, Final Batch Loss: 0.059364963322877884\n",
      "Epoch 7324, Loss: 0.5987222790718079, Final Batch Loss: 0.08014507591724396\n",
      "Epoch 7325, Loss: 0.6021890416741371, Final Batch Loss: 0.1563565880060196\n",
      "Epoch 7326, Loss: 0.7365235313773155, Final Batch Loss: 0.12073440104722977\n",
      "Epoch 7327, Loss: 0.7224652022123337, Final Batch Loss: 0.21111978590488434\n",
      "Epoch 7328, Loss: 0.7824815362691879, Final Batch Loss: 0.10519528388977051\n",
      "Epoch 7329, Loss: 0.7398377805948257, Final Batch Loss: 0.12875105440616608\n",
      "Epoch 7330, Loss: 0.6627229154109955, Final Batch Loss: 0.1608513742685318\n",
      "Epoch 7331, Loss: 0.7836451530456543, Final Batch Loss: 0.25021782517433167\n",
      "Epoch 7332, Loss: 0.6007833853363991, Final Batch Loss: 0.1159953773021698\n",
      "Epoch 7333, Loss: 0.8049464225769043, Final Batch Loss: 0.20242510735988617\n",
      "Epoch 7334, Loss: 0.7453537136316299, Final Batch Loss: 0.25357577204704285\n",
      "Epoch 7335, Loss: 0.7619133144617081, Final Batch Loss: 0.17924875020980835\n",
      "Epoch 7336, Loss: 0.853619396686554, Final Batch Loss: 0.21103690564632416\n",
      "Epoch 7337, Loss: 0.7721776515245438, Final Batch Loss: 0.18211060762405396\n",
      "Epoch 7338, Loss: 0.5984880588948727, Final Batch Loss: 0.04768114909529686\n",
      "Epoch 7339, Loss: 0.7423926442861557, Final Batch Loss: 0.17738455533981323\n",
      "Epoch 7340, Loss: 0.6389187723398209, Final Batch Loss: 0.2061193585395813\n",
      "Epoch 7341, Loss: 0.6904885768890381, Final Batch Loss: 0.15931428968906403\n",
      "Epoch 7342, Loss: 0.7199339270591736, Final Batch Loss: 0.15474368631839752\n",
      "Epoch 7343, Loss: 0.716154545545578, Final Batch Loss: 0.21992963552474976\n",
      "Epoch 7344, Loss: 0.7560631185770035, Final Batch Loss: 0.12884189188480377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7345, Loss: 0.7876277640461922, Final Batch Loss: 0.38081225752830505\n",
      "Epoch 7346, Loss: 0.9003543704748154, Final Batch Loss: 0.41624119877815247\n",
      "Epoch 7347, Loss: 0.6351439505815506, Final Batch Loss: 0.14796219766139984\n",
      "Epoch 7348, Loss: 0.6169584542512894, Final Batch Loss: 0.12941987812519073\n",
      "Epoch 7349, Loss: 0.7501126080751419, Final Batch Loss: 0.15728174149990082\n",
      "Epoch 7350, Loss: 0.8355694264173508, Final Batch Loss: 0.2047542929649353\n",
      "Epoch 7351, Loss: 0.7565014213323593, Final Batch Loss: 0.24302752315998077\n",
      "Epoch 7352, Loss: 0.5531417354941368, Final Batch Loss: 0.1035127118229866\n",
      "Epoch 7353, Loss: 0.6663285046815872, Final Batch Loss: 0.1460689902305603\n",
      "Epoch 7354, Loss: 0.7554869651794434, Final Batch Loss: 0.22124236822128296\n",
      "Epoch 7355, Loss: 0.7664656788110733, Final Batch Loss: 0.2680375277996063\n",
      "Epoch 7356, Loss: 0.7598600536584854, Final Batch Loss: 0.20101888477802277\n",
      "Epoch 7357, Loss: 0.8144114464521408, Final Batch Loss: 0.30793172121047974\n",
      "Epoch 7358, Loss: 0.6023069769144058, Final Batch Loss: 0.12427288293838501\n",
      "Epoch 7359, Loss: 0.6049354337155819, Final Batch Loss: 0.048854898661375046\n",
      "Epoch 7360, Loss: 0.7599032670259476, Final Batch Loss: 0.25839510560035706\n",
      "Epoch 7361, Loss: 0.6818192452192307, Final Batch Loss: 0.1788133829832077\n",
      "Epoch 7362, Loss: 0.5180266350507736, Final Batch Loss: 0.12200251966714859\n",
      "Epoch 7363, Loss: 0.9457600265741348, Final Batch Loss: 0.44867274165153503\n",
      "Epoch 7364, Loss: 0.7186587601900101, Final Batch Loss: 0.22920973598957062\n",
      "Epoch 7365, Loss: 0.7162458226084709, Final Batch Loss: 0.12256019562482834\n",
      "Epoch 7366, Loss: 0.7664490044116974, Final Batch Loss: 0.18736690282821655\n",
      "Epoch 7367, Loss: 0.7502534985542297, Final Batch Loss: 0.11101788282394409\n",
      "Epoch 7368, Loss: 0.703079916536808, Final Batch Loss: 0.08386790007352829\n",
      "Epoch 7369, Loss: 0.6676957458257675, Final Batch Loss: 0.1887655109167099\n",
      "Epoch 7370, Loss: 0.6763447970151901, Final Batch Loss: 0.13095369935035706\n",
      "Epoch 7371, Loss: 0.6893187612295151, Final Batch Loss: 0.12598298490047455\n",
      "Epoch 7372, Loss: 0.6950787007808685, Final Batch Loss: 0.16221687197685242\n",
      "Epoch 7373, Loss: 0.6430681496858597, Final Batch Loss: 0.2721102237701416\n",
      "Epoch 7374, Loss: 0.7244715094566345, Final Batch Loss: 0.14590275287628174\n",
      "Epoch 7375, Loss: 0.8742586076259613, Final Batch Loss: 0.3282368779182434\n",
      "Epoch 7376, Loss: 0.8997357934713364, Final Batch Loss: 0.3288443982601166\n",
      "Epoch 7377, Loss: 0.6494596377015114, Final Batch Loss: 0.1051316037774086\n",
      "Epoch 7378, Loss: 0.8629418909549713, Final Batch Loss: 0.26180973649024963\n",
      "Epoch 7379, Loss: 0.5188282653689384, Final Batch Loss: 0.08407511562108994\n",
      "Epoch 7380, Loss: 0.7094666510820389, Final Batch Loss: 0.20004816353321075\n",
      "Epoch 7381, Loss: 0.7021307125687599, Final Batch Loss: 0.07253364473581314\n",
      "Epoch 7382, Loss: 0.7069562822580338, Final Batch Loss: 0.1921611875295639\n",
      "Epoch 7383, Loss: 0.8585038483142853, Final Batch Loss: 0.2555526793003082\n",
      "Epoch 7384, Loss: 0.6616833359003067, Final Batch Loss: 0.21094219386577606\n",
      "Epoch 7385, Loss: 0.5432954281568527, Final Batch Loss: 0.0710606649518013\n",
      "Epoch 7386, Loss: 0.7514840215444565, Final Batch Loss: 0.20057310163974762\n",
      "Epoch 7387, Loss: 0.8088585436344147, Final Batch Loss: 0.23725879192352295\n",
      "Epoch 7388, Loss: 0.6418199837207794, Final Batch Loss: 0.10545019060373306\n",
      "Epoch 7389, Loss: 0.6979136317968369, Final Batch Loss: 0.2377358078956604\n",
      "Epoch 7390, Loss: 0.8947839587926865, Final Batch Loss: 0.28506284952163696\n",
      "Epoch 7391, Loss: 0.5765451565384865, Final Batch Loss: 0.08910470455884933\n",
      "Epoch 7392, Loss: 0.6711075454950333, Final Batch Loss: 0.15006490051746368\n",
      "Epoch 7393, Loss: 0.802235335111618, Final Batch Loss: 0.3467846214771271\n",
      "Epoch 7394, Loss: 0.6268923357129097, Final Batch Loss: 0.10690822452306747\n",
      "Epoch 7395, Loss: 0.6878170371055603, Final Batch Loss: 0.18439792096614838\n",
      "Epoch 7396, Loss: 0.9148958921432495, Final Batch Loss: 0.42629191279411316\n",
      "Epoch 7397, Loss: 0.6161016076803207, Final Batch Loss: 0.12983059883117676\n",
      "Epoch 7398, Loss: 1.0456919074058533, Final Batch Loss: 0.5421968698501587\n",
      "Epoch 7399, Loss: 0.5944338887929916, Final Batch Loss: 0.07068580389022827\n",
      "Epoch 7400, Loss: 0.7288172692060471, Final Batch Loss: 0.2147279530763626\n",
      "Epoch 7401, Loss: 0.6544322967529297, Final Batch Loss: 0.1379447728395462\n",
      "Epoch 7402, Loss: 0.5600299313664436, Final Batch Loss: 0.066220723092556\n",
      "Epoch 7403, Loss: 0.636578768491745, Final Batch Loss: 0.1352398842573166\n",
      "Epoch 7404, Loss: 0.7896309494972229, Final Batch Loss: 0.32224351167678833\n",
      "Epoch 7405, Loss: 0.5206444039940834, Final Batch Loss: 0.07818270474672318\n",
      "Epoch 7406, Loss: 0.812359631061554, Final Batch Loss: 0.3393932580947876\n",
      "Epoch 7407, Loss: 0.6082121655344963, Final Batch Loss: 0.13695843517780304\n",
      "Epoch 7408, Loss: 0.5768627151846886, Final Batch Loss: 0.09583256393671036\n",
      "Epoch 7409, Loss: 0.6084132269024849, Final Batch Loss: 0.09557872265577316\n",
      "Epoch 7410, Loss: 0.6410920694470406, Final Batch Loss: 0.11935249716043472\n",
      "Epoch 7411, Loss: 0.5760618075728416, Final Batch Loss: 0.10278002172708511\n",
      "Epoch 7412, Loss: 0.5267516300082207, Final Batch Loss: 0.035194359719753265\n",
      "Epoch 7413, Loss: 0.7790357172489166, Final Batch Loss: 0.1283087581396103\n",
      "Epoch 7414, Loss: 0.5441451743245125, Final Batch Loss: 0.07706809788942337\n",
      "Epoch 7415, Loss: 0.6905125379562378, Final Batch Loss: 0.2354864627122879\n",
      "Epoch 7416, Loss: 0.5422314964234829, Final Batch Loss: 0.050710003823041916\n",
      "Epoch 7417, Loss: 0.6453450620174408, Final Batch Loss: 0.1762230545282364\n",
      "Epoch 7418, Loss: 0.7112577557563782, Final Batch Loss: 0.2204430252313614\n",
      "Epoch 7419, Loss: 0.6449186056852341, Final Batch Loss: 0.1558101922273636\n",
      "Epoch 7420, Loss: 0.7017111033201218, Final Batch Loss: 0.16086113452911377\n",
      "Epoch 7421, Loss: 0.7315451055765152, Final Batch Loss: 0.1812514066696167\n",
      "Epoch 7422, Loss: 0.6467775627970695, Final Batch Loss: 0.11195597797632217\n",
      "Epoch 7423, Loss: 0.7119133993983269, Final Batch Loss: 0.06690157204866409\n",
      "Epoch 7424, Loss: 0.7527830004692078, Final Batch Loss: 0.19071291387081146\n",
      "Epoch 7425, Loss: 0.6639094054698944, Final Batch Loss: 0.21856732666492462\n",
      "Epoch 7426, Loss: 0.634930495172739, Final Batch Loss: 0.052323225885629654\n",
      "Epoch 7427, Loss: 0.6418007016181946, Final Batch Loss: 0.13250119984149933\n",
      "Epoch 7428, Loss: 0.5559720229357481, Final Batch Loss: 0.03017851524055004\n",
      "Epoch 7429, Loss: 0.6718209236860275, Final Batch Loss: 0.15669412910938263\n",
      "Epoch 7430, Loss: 0.7361721098423004, Final Batch Loss: 0.14800363779067993\n",
      "Epoch 7431, Loss: 0.552582360804081, Final Batch Loss: 0.08541183918714523\n",
      "Epoch 7432, Loss: 0.6487681865692139, Final Batch Loss: 0.18009722232818604\n",
      "Epoch 7433, Loss: 0.5245162546634674, Final Batch Loss: 0.07121661305427551\n",
      "Epoch 7434, Loss: 0.5975197926163673, Final Batch Loss: 0.11080250144004822\n",
      "Epoch 7435, Loss: 0.6058751046657562, Final Batch Loss: 0.15371377766132355\n",
      "Epoch 7436, Loss: 0.6474535316228867, Final Batch Loss: 0.19777913391590118\n",
      "Epoch 7437, Loss: 0.6387446075677872, Final Batch Loss: 0.1338600367307663\n",
      "Epoch 7438, Loss: 0.9429671168327332, Final Batch Loss: 0.2044697403907776\n",
      "Epoch 7439, Loss: 0.7929848432540894, Final Batch Loss: 0.20741446316242218\n",
      "Epoch 7440, Loss: 0.6236005127429962, Final Batch Loss: 0.17936645448207855\n",
      "Epoch 7441, Loss: 0.8359933197498322, Final Batch Loss: 0.3484894931316376\n",
      "Epoch 7442, Loss: 0.7588409185409546, Final Batch Loss: 0.22001022100448608\n",
      "Epoch 7443, Loss: 0.6528491005301476, Final Batch Loss: 0.15458421409130096\n",
      "Epoch 7444, Loss: 0.6602213382720947, Final Batch Loss: 0.23403923213481903\n",
      "Epoch 7445, Loss: 0.6252420097589493, Final Batch Loss: 0.09007477760314941\n",
      "Epoch 7446, Loss: 0.7421552687883377, Final Batch Loss: 0.2772800624370575\n",
      "Epoch 7447, Loss: 0.8683621287345886, Final Batch Loss: 0.3163965940475464\n",
      "Epoch 7448, Loss: 0.6219476833939552, Final Batch Loss: 0.10897896438837051\n",
      "Epoch 7449, Loss: 0.6739833131432533, Final Batch Loss: 0.16399191319942474\n",
      "Epoch 7450, Loss: 0.5800868645310402, Final Batch Loss: 0.16425003111362457\n",
      "Epoch 7451, Loss: 0.5960646644234657, Final Batch Loss: 0.08140402287244797\n",
      "Epoch 7452, Loss: 0.6130174696445465, Final Batch Loss: 0.13527227938175201\n",
      "Epoch 7453, Loss: 0.6548871397972107, Final Batch Loss: 0.15541191399097443\n",
      "Epoch 7454, Loss: 1.0870124697685242, Final Batch Loss: 0.5306310057640076\n",
      "Epoch 7455, Loss: 0.531623087823391, Final Batch Loss: 0.0702032819390297\n",
      "Epoch 7456, Loss: 0.5087888315320015, Final Batch Loss: 0.09798295050859451\n",
      "Epoch 7457, Loss: 0.599359542131424, Final Batch Loss: 0.13304494321346283\n",
      "Epoch 7458, Loss: 0.7053532302379608, Final Batch Loss: 0.23114293813705444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7459, Loss: 0.5407152660191059, Final Batch Loss: 0.04013741388916969\n",
      "Epoch 7460, Loss: 0.7716070413589478, Final Batch Loss: 0.3221118748188019\n",
      "Epoch 7461, Loss: 0.5133897066116333, Final Batch Loss: 0.07282645255327225\n",
      "Epoch 7462, Loss: 0.5704617127776146, Final Batch Loss: 0.11886375397443771\n",
      "Epoch 7463, Loss: 0.742773525416851, Final Batch Loss: 0.3063651919364929\n",
      "Epoch 7464, Loss: 0.5250743329524994, Final Batch Loss: 0.06064896285533905\n",
      "Epoch 7465, Loss: 0.8185313791036606, Final Batch Loss: 0.2591424882411957\n",
      "Epoch 7466, Loss: 0.7307102680206299, Final Batch Loss: 0.2306591421365738\n",
      "Epoch 7467, Loss: 0.5562446713447571, Final Batch Loss: 0.12082779407501221\n",
      "Epoch 7468, Loss: 0.6581430584192276, Final Batch Loss: 0.158664271235466\n",
      "Epoch 7469, Loss: 0.65699203312397, Final Batch Loss: 0.13412229716777802\n",
      "Epoch 7470, Loss: 0.6688989400863647, Final Batch Loss: 0.196701779961586\n",
      "Epoch 7471, Loss: 0.533233217895031, Final Batch Loss: 0.0978088453412056\n",
      "Epoch 7472, Loss: 0.49383579939603806, Final Batch Loss: 0.08646485209465027\n",
      "Epoch 7473, Loss: 0.6168794259428978, Final Batch Loss: 0.0984412431716919\n",
      "Epoch 7474, Loss: 0.8023374527692795, Final Batch Loss: 0.2309430092573166\n",
      "Epoch 7475, Loss: 0.6052028685808182, Final Batch Loss: 0.12671419978141785\n",
      "Epoch 7476, Loss: 0.5378838777542114, Final Batch Loss: 0.10356462001800537\n",
      "Epoch 7477, Loss: 0.7222287952899933, Final Batch Loss: 0.15313902497291565\n",
      "Epoch 7478, Loss: 0.5910757556557655, Final Batch Loss: 0.1631859391927719\n",
      "Epoch 7479, Loss: 0.5615082457661629, Final Batch Loss: 0.07284541428089142\n",
      "Epoch 7480, Loss: 0.6402889788150787, Final Batch Loss: 0.08783063292503357\n",
      "Epoch 7481, Loss: 0.6680072322487831, Final Batch Loss: 0.1231229230761528\n",
      "Epoch 7482, Loss: 0.5865655168890953, Final Batch Loss: 0.06841286271810532\n",
      "Epoch 7483, Loss: 0.5989618077874184, Final Batch Loss: 0.10712514072656631\n",
      "Epoch 7484, Loss: 0.6928701549768448, Final Batch Loss: 0.1382308453321457\n",
      "Epoch 7485, Loss: 0.64665687084198, Final Batch Loss: 0.16902786493301392\n",
      "Epoch 7486, Loss: 0.6792699843645096, Final Batch Loss: 0.16743801534175873\n",
      "Epoch 7487, Loss: 0.6186451837420464, Final Batch Loss: 0.06544839590787888\n",
      "Epoch 7488, Loss: 0.6496250256896019, Final Batch Loss: 0.16962508857250214\n",
      "Epoch 7489, Loss: 0.731369785964489, Final Batch Loss: 0.20371325314044952\n",
      "Epoch 7490, Loss: 0.6338876634836197, Final Batch Loss: 0.15455757081508636\n",
      "Epoch 7491, Loss: 0.6019570901989937, Final Batch Loss: 0.17171432077884674\n",
      "Epoch 7492, Loss: 0.6666839867830276, Final Batch Loss: 0.2214951515197754\n",
      "Epoch 7493, Loss: 0.603896901011467, Final Batch Loss: 0.08930898457765579\n",
      "Epoch 7494, Loss: 0.7312375009059906, Final Batch Loss: 0.2783229947090149\n",
      "Epoch 7495, Loss: 0.5991004854440689, Final Batch Loss: 0.13719257712364197\n",
      "Epoch 7496, Loss: 0.6189305707812309, Final Batch Loss: 0.13794384896755219\n",
      "Epoch 7497, Loss: 0.7351219952106476, Final Batch Loss: 0.12671566009521484\n",
      "Epoch 7498, Loss: 0.7571132481098175, Final Batch Loss: 0.22966091334819794\n",
      "Epoch 7499, Loss: 0.8052560538053513, Final Batch Loss: 0.2761664092540741\n",
      "Epoch 7500, Loss: 0.7849661409854889, Final Batch Loss: 0.2695326507091522\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0  0  0  0  0]\n",
      " [ 0 22  1  0  0  0]\n",
      " [ 0  0 17  0  1  0]\n",
      " [ 0  0  0 16  0  0]\n",
      " [ 1  1  0  0 21  0]\n",
      " [ 0  0  0  0  0 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.971     1.000     0.986        34\n",
      "           1      0.957     0.957     0.957        23\n",
      "           2      0.944     0.944     0.944        18\n",
      "           3      1.000     1.000     1.000        16\n",
      "           4      0.955     0.913     0.933        23\n",
      "           5      1.000     1.000     1.000        27\n",
      "\n",
      "    accuracy                          0.972       141\n",
      "   macro avg      0.971     0.969     0.970       141\n",
      "weighted avg      0.972     0.972     0.971       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'../../../saved_models/UCI 6 User Classifier Ablation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
