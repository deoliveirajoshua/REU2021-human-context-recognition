{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_features = ['42 tGravityAcc-mean()-Y',\n",
    " '43 tGravityAcc-mean()-Z',\n",
    " '51 tGravityAcc-max()-Y',\n",
    " '52 tGravityAcc-max()-Z',\n",
    " '54 tGravityAcc-min()-Y',\n",
    " '55 tGravityAcc-min()-Z',\n",
    " '56 tGravityAcc-sma()',\n",
    " '58 tGravityAcc-energy()-Y',\n",
    " '59 tGravityAcc-energy()-Z',\n",
    " '128 tBodyGyro-mad()-Y',\n",
    " '141 tBodyGyro-iqr()-Y',\n",
    " '428 fBodyGyro-std()-Y',\n",
    " '434 fBodyGyro-max()-Y',\n",
    " '475 fBodyGyro-bandsEnergy()-1,8',\n",
    " '483 fBodyGyro-bandsEnergy()-1,16',\n",
    " '487 fBodyGyro-bandsEnergy()-1,24',\n",
    " '559 angle(X,gravityMean)',\n",
    " '560 angle(Y,gravityMean)',\n",
    " '561 angle(Z,gravityMean)']\n",
    "\n",
    "act_features = ['4 tBodyAcc-std()-X',\n",
    " '7 tBodyAcc-mad()-X',\n",
    " '10 tBodyAcc-max()-X',\n",
    " '17 tBodyAcc-energy()-X',\n",
    " '202 tBodyAccMag-std()',\n",
    " '203 tBodyAccMag-mad()',\n",
    " '215 tGravityAccMag-std()',\n",
    " '216 tGravityAccMag-mad()',\n",
    " '266 fBodyAcc-mean()-X',\n",
    " '269 fBodyAcc-std()-X',\n",
    " '282 fBodyAcc-energy()-X',\n",
    " '303 fBodyAcc-bandsEnergy()-1,8',\n",
    " '311 fBodyAcc-bandsEnergy()-1,16',\n",
    " '315 fBodyAcc-bandsEnergy()-1,24',\n",
    " '382 fBodyAccJerk-bandsEnergy()-1,8',\n",
    " '504 fBodyAccMag-std()',\n",
    " '505 fBodyAccMag-mad()',\n",
    " '509 fBodyAccMag-energy()']\n",
    "\n",
    "input_shape = len(sub_features) + len(act_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.LeakyReLU(0.05)\n",
    "    )\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_dim = input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            classifier_block(feature_dim, 35),\n",
    "            classifier_block(35, 30),\n",
    "            classifier_block(30, 25),\n",
    "            classifier_block(25, 20),\n",
    "            nn.Linear(20, 18)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines each generator layer\n",
    "#input and output dimensions needed\n",
    "def generator_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.BatchNorm1d(output_dim),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "\n",
    "#returns n_samples of z_dim (number of dimensions of latent space) noise\n",
    "def get_noise(n_samples, z_dim):\n",
    "    return torch.randn(n_samples, z_dim)\n",
    "\n",
    "#defines generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim = 10, feature_dim = input_shape, hidden_dim = 128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            generator_block(z_dim, 80),\n",
    "            generator_block(80, 60),\n",
    "            generator_block(60, 50),\n",
    "            nn.Linear(50, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise):\n",
    "        return self.gen(noise)\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(torch.load(f'../../../saved_models/{model_name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label is a list of integers specifying which labels to filter by\n",
    "#users is a list of integers specifying which users to filter by\n",
    "#y_label is a string, either \"Activity\" or \"Subject\" depending on what y output needs to be returned\n",
    "def start_data(label, users, y_label, sub_features, act_features):\n",
    "    #get the dataframe column names\n",
    "    name_dataframe = pd.read_csv('../../../data/features.txt', delimiter = '\\n', header = None)\n",
    "    names = name_dataframe.values.tolist()\n",
    "    names = [k for row in names for k in row] #List of column names\n",
    "\n",
    "    data = pd.read_csv('../../../data/X_train.txt', delim_whitespace = True, header = None) #Read in dataframe\n",
    "    data.columns = names #Setting column names\n",
    "    \n",
    "    X_train_1 = data[sub_features]\n",
    "    X_train_2 = data[act_features]\n",
    "    X_train = pd.concat([X_train_1, X_train_2], axis = 1)\n",
    "    \n",
    "    y_train_activity = pd.read_csv('../../../data/y_train.txt', header = None)\n",
    "    y_train_activity.columns = ['Activity']\n",
    "    \n",
    "    y_train_subject = pd.read_csv('../../../data/subject_train.txt', header = None)\n",
    "    y_train_subject.columns = ['Subject']\n",
    "    \n",
    "    GAN_data = pd.concat([X_train, y_train_activity, y_train_subject], axis = 1)\n",
    "    GAN_data = GAN_data[GAN_data['Activity'].isin(label)]\n",
    "    GAN_data = GAN_data[GAN_data['Subject'].isin(users)]\n",
    "    \n",
    "    X_1 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_2 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_3 = GAN_data[(GAN_data['Subject'] == 1) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_4 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_5 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_6 = GAN_data[(GAN_data['Subject'] == 3) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_7 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_8 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_9 = GAN_data[(GAN_data['Subject'] == 5) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_10 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_11 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_12 = GAN_data[(GAN_data['Subject'] == 7) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_13 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_14 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_15 = GAN_data[(GAN_data['Subject'] == 8) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    X_16 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 1)].iloc[:,:-2].values\n",
    "    X_17 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 3)].iloc[:,:-2].values\n",
    "    X_18 = GAN_data[(GAN_data['Subject'] == 11) & (GAN_data['Activity'] == 4)].iloc[:,:-2].values\n",
    "    \n",
    "    X_train = np.concatenate((X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16, X_17, X_18))\n",
    "    y_train = [0] * len(X_1) + [1] * len(X_2) + [2] * len(X_3) + [3] * len(X_4) + [4] * len(X_5) + [5] * len(X_6) + [6] * len(X_7) + [7] * len(X_8) + [8] * len(X_9) + [9] * len(X_10) + [10] * len(X_11) + [11] * len(X_12) + [12] * len(X_13) + [13] * len(X_14) + [14] * len(X_15) + [15] * len(X_16) + [16] * len(X_17) + [17] * len(X_18)\n",
    "    \n",
    "    return X_train, np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [1, 3, 4]\n",
    "users = [1, 3, 5, 7, 8, 11]\n",
    "\n",
    "X, y = start_data(activities, users, \"Activity\", sub_features, act_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "model = Classifier()\n",
    "lr = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 250\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_features = torch.tensor(X_train)\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_features = torch.tensor(X_test)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_data = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = len(test_labels), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.47152829170227, Final Batch Loss: 2.8191897869110107\n",
      "Epoch 2, Loss: 11.849348306655884, Final Batch Loss: 3.20412278175354\n",
      "Epoch 3, Loss: 11.515205144882202, Final Batch Loss: 2.8727762699127197\n",
      "Epoch 4, Loss: 11.438592672348022, Final Batch Loss: 2.7963149547576904\n",
      "Epoch 5, Loss: 11.810645818710327, Final Batch Loss: 3.1783719062805176\n",
      "Epoch 6, Loss: 11.46007251739502, Final Batch Loss: 2.8263754844665527\n",
      "Epoch 7, Loss: 11.582205772399902, Final Batch Loss: 2.9480462074279785\n",
      "Epoch 8, Loss: 11.707587003707886, Final Batch Loss: 3.075031042098999\n",
      "Epoch 9, Loss: 11.583173751831055, Final Batch Loss: 2.9598474502563477\n",
      "Epoch 10, Loss: 11.358701705932617, Final Batch Loss: 2.7361624240875244\n",
      "Epoch 11, Loss: 11.407397985458374, Final Batch Loss: 2.786602258682251\n",
      "Epoch 12, Loss: 11.47432017326355, Final Batch Loss: 2.861170530319214\n",
      "Epoch 13, Loss: 11.641939163208008, Final Batch Loss: 3.034548282623291\n",
      "Epoch 14, Loss: 11.554683923721313, Final Batch Loss: 2.9545767307281494\n",
      "Epoch 15, Loss: 11.539154529571533, Final Batch Loss: 2.949026107788086\n",
      "Epoch 16, Loss: 11.457511901855469, Final Batch Loss: 2.87491774559021\n",
      "Epoch 17, Loss: 11.588738203048706, Final Batch Loss: 3.022162914276123\n",
      "Epoch 18, Loss: 11.473459005355835, Final Batch Loss: 2.919116497039795\n",
      "Epoch 19, Loss: 11.497880935668945, Final Batch Loss: 2.96353816986084\n",
      "Epoch 20, Loss: 11.282841920852661, Final Batch Loss: 2.7690343856811523\n",
      "Epoch 21, Loss: 11.262495994567871, Final Batch Loss: 2.771249532699585\n",
      "Epoch 22, Loss: 11.04156494140625, Final Batch Loss: 2.591102361679077\n",
      "Epoch 23, Loss: 11.262635231018066, Final Batch Loss: 2.857689619064331\n",
      "Epoch 24, Loss: 11.378189086914062, Final Batch Loss: 3.0145905017852783\n",
      "Epoch 25, Loss: 10.880273580551147, Final Batch Loss: 2.5816245079040527\n",
      "Epoch 26, Loss: 11.14999532699585, Final Batch Loss: 2.9000091552734375\n",
      "Epoch 27, Loss: 11.632558822631836, Final Batch Loss: 3.4243695735931396\n",
      "Epoch 28, Loss: 10.464637041091919, Final Batch Loss: 2.340885639190674\n",
      "Epoch 29, Loss: 10.443576574325562, Final Batch Loss: 2.423410654067993\n",
      "Epoch 30, Loss: 10.487343072891235, Final Batch Loss: 2.5708422660827637\n",
      "Epoch 31, Loss: 10.713082075119019, Final Batch Loss: 2.8810126781463623\n",
      "Epoch 32, Loss: 10.106932878494263, Final Batch Loss: 2.320622444152832\n",
      "Epoch 33, Loss: 10.47191333770752, Final Batch Loss: 2.7840893268585205\n",
      "Epoch 34, Loss: 10.268637895584106, Final Batch Loss: 2.598097562789917\n",
      "Epoch 35, Loss: 10.277133464813232, Final Batch Loss: 2.6839513778686523\n",
      "Epoch 36, Loss: 10.457818984985352, Final Batch Loss: 2.9083433151245117\n",
      "Epoch 37, Loss: 9.419942617416382, Final Batch Loss: 2.0024335384368896\n",
      "Epoch 38, Loss: 10.576618432998657, Final Batch Loss: 3.1940619945526123\n",
      "Epoch 39, Loss: 10.206956624984741, Final Batch Loss: 2.8693184852600098\n",
      "Epoch 40, Loss: 10.012078523635864, Final Batch Loss: 2.760503053665161\n",
      "Epoch 41, Loss: 10.148988723754883, Final Batch Loss: 2.9214751720428467\n",
      "Epoch 42, Loss: 9.669703722000122, Final Batch Loss: 2.4942922592163086\n",
      "Epoch 43, Loss: 9.973806381225586, Final Batch Loss: 2.8110671043395996\n",
      "Epoch 44, Loss: 8.685163855552673, Final Batch Loss: 1.548683762550354\n",
      "Epoch 45, Loss: 10.56572413444519, Final Batch Loss: 3.4731459617614746\n",
      "Epoch 46, Loss: 8.974676728248596, Final Batch Loss: 1.912518858909607\n",
      "Epoch 47, Loss: 9.455435276031494, Final Batch Loss: 2.479515552520752\n",
      "Epoch 48, Loss: 10.067103147506714, Final Batch Loss: 3.0685038566589355\n",
      "Epoch 49, Loss: 9.814574003219604, Final Batch Loss: 2.852764368057251\n",
      "Epoch 50, Loss: 8.832959532737732, Final Batch Loss: 1.9234551191329956\n",
      "Epoch 51, Loss: 9.427264928817749, Final Batch Loss: 2.499457597732544\n",
      "Epoch 52, Loss: 9.712242841720581, Final Batch Loss: 2.8571178913116455\n",
      "Epoch 53, Loss: 9.41372275352478, Final Batch Loss: 2.607970714569092\n",
      "Epoch 54, Loss: 9.504386186599731, Final Batch Loss: 2.654714584350586\n",
      "Epoch 55, Loss: 10.276060581207275, Final Batch Loss: 3.4588732719421387\n",
      "Epoch 56, Loss: 9.330153226852417, Final Batch Loss: 2.379115104675293\n",
      "Epoch 57, Loss: 8.983359336853027, Final Batch Loss: 2.089787006378174\n",
      "Epoch 58, Loss: 9.268033504486084, Final Batch Loss: 2.47176456451416\n",
      "Epoch 59, Loss: 9.212529182434082, Final Batch Loss: 2.470996379852295\n",
      "Epoch 60, Loss: 9.23556637763977, Final Batch Loss: 2.526210308074951\n",
      "Epoch 61, Loss: 8.725371360778809, Final Batch Loss: 2.0397117137908936\n",
      "Epoch 62, Loss: 9.357440710067749, Final Batch Loss: 2.6607656478881836\n",
      "Epoch 63, Loss: 8.372295379638672, Final Batch Loss: 1.7072136402130127\n",
      "Epoch 64, Loss: 9.214126825332642, Final Batch Loss: 2.572657585144043\n",
      "Epoch 65, Loss: 9.20211124420166, Final Batch Loss: 2.5894343852996826\n",
      "Epoch 66, Loss: 8.701828956604004, Final Batch Loss: 2.031587600708008\n",
      "Epoch 67, Loss: 8.148116111755371, Final Batch Loss: 1.5527830123901367\n",
      "Epoch 68, Loss: 8.457894682884216, Final Batch Loss: 1.9249461889266968\n",
      "Epoch 69, Loss: 9.161319255828857, Final Batch Loss: 2.6519651412963867\n",
      "Epoch 70, Loss: 8.612128257751465, Final Batch Loss: 2.191603899002075\n",
      "Epoch 71, Loss: 8.373912930488586, Final Batch Loss: 1.8617275953292847\n",
      "Epoch 72, Loss: 9.015486240386963, Final Batch Loss: 2.5853891372680664\n",
      "Epoch 73, Loss: 8.659964323043823, Final Batch Loss: 2.20074462890625\n",
      "Epoch 74, Loss: 9.049885988235474, Final Batch Loss: 2.6375036239624023\n",
      "Epoch 75, Loss: 8.014193058013916, Final Batch Loss: 1.6314246654510498\n",
      "Epoch 76, Loss: 9.069487810134888, Final Batch Loss: 2.665348529815674\n",
      "Epoch 77, Loss: 8.212266683578491, Final Batch Loss: 1.8708863258361816\n",
      "Epoch 78, Loss: 8.727486371994019, Final Batch Loss: 2.425121545791626\n",
      "Epoch 79, Loss: 8.307868242263794, Final Batch Loss: 1.916130781173706\n",
      "Epoch 80, Loss: 8.458518743515015, Final Batch Loss: 2.136040687561035\n",
      "Epoch 81, Loss: 8.785039186477661, Final Batch Loss: 2.50429105758667\n",
      "Epoch 82, Loss: 8.125815987586975, Final Batch Loss: 1.8408340215682983\n",
      "Epoch 83, Loss: 8.201931715011597, Final Batch Loss: 1.8566343784332275\n",
      "Epoch 84, Loss: 9.306926727294922, Final Batch Loss: 3.066192865371704\n",
      "Epoch 85, Loss: 8.272826433181763, Final Batch Loss: 2.061711311340332\n",
      "Epoch 86, Loss: 8.720798015594482, Final Batch Loss: 2.464106798171997\n",
      "Epoch 87, Loss: 7.692739129066467, Final Batch Loss: 1.4279552698135376\n",
      "Epoch 88, Loss: 7.580294609069824, Final Batch Loss: 1.4248650074005127\n",
      "Epoch 89, Loss: 7.86167311668396, Final Batch Loss: 1.7727824449539185\n",
      "Epoch 90, Loss: 9.06025493144989, Final Batch Loss: 2.971467971801758\n",
      "Epoch 91, Loss: 7.598907113075256, Final Batch Loss: 1.6272742748260498\n",
      "Epoch 92, Loss: 8.177209615707397, Final Batch Loss: 2.1350979804992676\n",
      "Epoch 93, Loss: 8.343908667564392, Final Batch Loss: 2.4143259525299072\n",
      "Epoch 94, Loss: 7.154867053031921, Final Batch Loss: 1.1158336400985718\n",
      "Epoch 95, Loss: 8.104594588279724, Final Batch Loss: 2.1934354305267334\n",
      "Epoch 96, Loss: 7.246466636657715, Final Batch Loss: 1.1934969425201416\n",
      "Epoch 97, Loss: 8.45851194858551, Final Batch Loss: 2.4364514350891113\n",
      "Epoch 98, Loss: 8.054255843162537, Final Batch Loss: 2.094430685043335\n",
      "Epoch 99, Loss: 8.167095065116882, Final Batch Loss: 2.0692834854125977\n",
      "Epoch 100, Loss: 8.214991688728333, Final Batch Loss: 2.029646873474121\n",
      "Epoch 101, Loss: 8.526861429214478, Final Batch Loss: 2.3410651683807373\n",
      "Epoch 102, Loss: 7.899332642555237, Final Batch Loss: 1.8760313987731934\n",
      "Epoch 103, Loss: 7.398297905921936, Final Batch Loss: 1.59874427318573\n",
      "Epoch 104, Loss: 7.6617515087127686, Final Batch Loss: 1.7824015617370605\n",
      "Epoch 105, Loss: 7.480406641960144, Final Batch Loss: 1.640101432800293\n",
      "Epoch 106, Loss: 7.616502523422241, Final Batch Loss: 1.769269585609436\n",
      "Epoch 107, Loss: 9.061944127082825, Final Batch Loss: 3.1961379051208496\n",
      "Epoch 108, Loss: 8.14431893825531, Final Batch Loss: 2.3329641819000244\n",
      "Epoch 109, Loss: 9.175971746444702, Final Batch Loss: 3.1343846321105957\n",
      "Epoch 110, Loss: 8.547273874282837, Final Batch Loss: 2.481387138366699\n",
      "Epoch 111, Loss: 7.870047211647034, Final Batch Loss: 1.9429397583007812\n",
      "Epoch 112, Loss: 8.298497319221497, Final Batch Loss: 2.530341148376465\n",
      "Epoch 113, Loss: 7.767690420150757, Final Batch Loss: 2.0361955165863037\n",
      "Epoch 114, Loss: 7.745887279510498, Final Batch Loss: 1.9866400957107544\n",
      "Epoch 115, Loss: 6.8040231466293335, Final Batch Loss: 1.1389403343200684\n",
      "Epoch 116, Loss: 6.412133157253265, Final Batch Loss: 0.7598963379859924\n",
      "Epoch 117, Loss: 7.251144528388977, Final Batch Loss: 1.5451200008392334\n",
      "Epoch 118, Loss: 7.9736316204071045, Final Batch Loss: 2.275735855102539\n",
      "Epoch 119, Loss: 8.511857867240906, Final Batch Loss: 2.8471665382385254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Loss: 7.439812660217285, Final Batch Loss: 1.8327945470809937\n",
      "Epoch 121, Loss: 6.890466928482056, Final Batch Loss: 1.3686771392822266\n",
      "Epoch 122, Loss: 7.15029239654541, Final Batch Loss: 1.5648698806762695\n",
      "Epoch 123, Loss: 7.132758617401123, Final Batch Loss: 1.5615966320037842\n",
      "Epoch 124, Loss: 7.451349973678589, Final Batch Loss: 1.8407793045043945\n",
      "Epoch 125, Loss: 6.272697389125824, Final Batch Loss: 0.6964297890663147\n",
      "Epoch 126, Loss: 8.037787199020386, Final Batch Loss: 2.405247211456299\n",
      "Epoch 127, Loss: 6.8154075145721436, Final Batch Loss: 1.3051953315734863\n",
      "Epoch 128, Loss: 7.734760284423828, Final Batch Loss: 2.17348051071167\n",
      "Epoch 129, Loss: 7.014529585838318, Final Batch Loss: 1.513114094734192\n",
      "Epoch 130, Loss: 6.475626885890961, Final Batch Loss: 0.9986839890480042\n",
      "Epoch 131, Loss: 7.548804759979248, Final Batch Loss: 2.026113271713257\n",
      "Epoch 132, Loss: 8.16225278377533, Final Batch Loss: 2.744904041290283\n",
      "Epoch 133, Loss: 5.8200298845767975, Final Batch Loss: 0.36841216683387756\n",
      "Epoch 134, Loss: 7.486270427703857, Final Batch Loss: 2.065356492996216\n",
      "Epoch 135, Loss: 7.001251459121704, Final Batch Loss: 1.5965195894241333\n",
      "Epoch 136, Loss: 7.510815620422363, Final Batch Loss: 2.100618839263916\n",
      "Epoch 137, Loss: 7.793442010879517, Final Batch Loss: 2.433460235595703\n",
      "Epoch 138, Loss: 7.751928329467773, Final Batch Loss: 2.29429292678833\n",
      "Epoch 139, Loss: 7.538841605186462, Final Batch Loss: 2.094292640686035\n",
      "Epoch 140, Loss: 6.740041375160217, Final Batch Loss: 1.321816325187683\n",
      "Epoch 141, Loss: 7.635401725769043, Final Batch Loss: 2.2246170043945312\n",
      "Epoch 142, Loss: 8.076305627822876, Final Batch Loss: 2.687509775161743\n",
      "Epoch 143, Loss: 6.459106683731079, Final Batch Loss: 1.0034226179122925\n",
      "Epoch 144, Loss: 6.763937711715698, Final Batch Loss: 1.529212474822998\n",
      "Epoch 145, Loss: 7.326716661453247, Final Batch Loss: 2.1033263206481934\n",
      "Epoch 146, Loss: 6.546052694320679, Final Batch Loss: 1.3022724390029907\n",
      "Epoch 147, Loss: 6.224959194660187, Final Batch Loss: 0.8890194296836853\n",
      "Epoch 148, Loss: 7.591584801673889, Final Batch Loss: 2.3826775550842285\n",
      "Epoch 149, Loss: 7.234356045722961, Final Batch Loss: 1.988896369934082\n",
      "Epoch 150, Loss: 6.08761328458786, Final Batch Loss: 0.8606405854225159\n",
      "Epoch 151, Loss: 7.202922940254211, Final Batch Loss: 2.0136072635650635\n",
      "Epoch 152, Loss: 7.580114841461182, Final Batch Loss: 2.440062999725342\n",
      "Epoch 153, Loss: 6.459323763847351, Final Batch Loss: 1.3164066076278687\n",
      "Epoch 154, Loss: 7.108498573303223, Final Batch Loss: 2.017984628677368\n",
      "Epoch 155, Loss: 7.218504309654236, Final Batch Loss: 2.1397597789764404\n",
      "Epoch 156, Loss: 7.877227544784546, Final Batch Loss: 2.8445255756378174\n",
      "Epoch 157, Loss: 5.8394288420677185, Final Batch Loss: 0.748792827129364\n",
      "Epoch 158, Loss: 6.381287097930908, Final Batch Loss: 1.3426626920700073\n",
      "Epoch 159, Loss: 7.090408444404602, Final Batch Loss: 1.9958889484405518\n",
      "Epoch 160, Loss: 7.262676000595093, Final Batch Loss: 2.166983127593994\n",
      "Epoch 161, Loss: 6.711694717407227, Final Batch Loss: 1.7364344596862793\n",
      "Epoch 162, Loss: 8.239310383796692, Final Batch Loss: 3.3312673568725586\n",
      "Epoch 163, Loss: 6.093412160873413, Final Batch Loss: 1.0997521877288818\n",
      "Epoch 164, Loss: 6.925552725791931, Final Batch Loss: 1.9434187412261963\n",
      "Epoch 165, Loss: 5.659005224704742, Final Batch Loss: 0.6462141871452332\n",
      "Epoch 166, Loss: 6.553977608680725, Final Batch Loss: 1.5155279636383057\n",
      "Epoch 167, Loss: 6.823182106018066, Final Batch Loss: 1.8686364889144897\n",
      "Epoch 168, Loss: 5.576867938041687, Final Batch Loss: 0.6535181999206543\n",
      "Epoch 169, Loss: 7.60261082649231, Final Batch Loss: 2.601370096206665\n",
      "Epoch 170, Loss: 6.2957412004470825, Final Batch Loss: 1.2154369354248047\n",
      "Epoch 171, Loss: 6.407425880432129, Final Batch Loss: 1.4882233142852783\n",
      "Epoch 172, Loss: 8.314509391784668, Final Batch Loss: 3.290314197540283\n",
      "Epoch 173, Loss: 6.234870791435242, Final Batch Loss: 1.2874410152435303\n",
      "Epoch 174, Loss: 7.029294013977051, Final Batch Loss: 2.103889226913452\n",
      "Epoch 175, Loss: 7.131391763687134, Final Batch Loss: 2.157778263092041\n",
      "Epoch 176, Loss: 6.832184195518494, Final Batch Loss: 1.7880700826644897\n",
      "Epoch 177, Loss: 6.065043210983276, Final Batch Loss: 1.0407078266143799\n",
      "Epoch 178, Loss: 6.989730358123779, Final Batch Loss: 2.0540289878845215\n",
      "Epoch 179, Loss: 6.218955039978027, Final Batch Loss: 1.2433336973190308\n",
      "Epoch 180, Loss: 7.34137237071991, Final Batch Loss: 2.4959399700164795\n",
      "Epoch 181, Loss: 5.785062313079834, Final Batch Loss: 0.8526630401611328\n",
      "Epoch 182, Loss: 6.049409866333008, Final Batch Loss: 1.0320919752120972\n",
      "Epoch 183, Loss: 5.996742844581604, Final Batch Loss: 1.0845950841903687\n",
      "Epoch 184, Loss: 5.794155061244965, Final Batch Loss: 0.9550798535346985\n",
      "Epoch 185, Loss: 6.150155067443848, Final Batch Loss: 1.313050627708435\n",
      "Epoch 186, Loss: 5.5755303502082825, Final Batch Loss: 0.717153012752533\n",
      "Epoch 187, Loss: 6.547051310539246, Final Batch Loss: 1.7893675565719604\n",
      "Epoch 188, Loss: 6.952783942222595, Final Batch Loss: 2.1935129165649414\n",
      "Epoch 189, Loss: 6.455942392349243, Final Batch Loss: 1.6941378116607666\n",
      "Epoch 190, Loss: 5.501611888408661, Final Batch Loss: 0.6611973643302917\n",
      "Epoch 191, Loss: 6.3228490352630615, Final Batch Loss: 1.6345295906066895\n",
      "Epoch 192, Loss: 5.375046670436859, Final Batch Loss: 0.6282015442848206\n",
      "Epoch 193, Loss: 5.90444278717041, Final Batch Loss: 1.227192997932434\n",
      "Epoch 194, Loss: 5.996664524078369, Final Batch Loss: 1.324427604675293\n",
      "Epoch 195, Loss: 5.4162821769714355, Final Batch Loss: 0.5770255327224731\n",
      "Epoch 196, Loss: 5.66113942861557, Final Batch Loss: 0.7993687987327576\n",
      "Epoch 197, Loss: 6.501724362373352, Final Batch Loss: 1.6094971895217896\n",
      "Epoch 198, Loss: 6.55520224571228, Final Batch Loss: 1.720166802406311\n",
      "Epoch 199, Loss: 6.871263146400452, Final Batch Loss: 2.078291654586792\n",
      "Epoch 200, Loss: 5.272993266582489, Final Batch Loss: 0.5376129746437073\n",
      "Epoch 201, Loss: 8.984840631484985, Final Batch Loss: 4.2274932861328125\n",
      "Epoch 202, Loss: 5.919887900352478, Final Batch Loss: 1.230513572692871\n",
      "Epoch 203, Loss: 5.478755056858063, Final Batch Loss: 0.6507770419120789\n",
      "Epoch 204, Loss: 5.139100193977356, Final Batch Loss: 0.28938448429107666\n",
      "Epoch 205, Loss: 6.6518179178237915, Final Batch Loss: 1.9120067358016968\n",
      "Epoch 206, Loss: 6.58799409866333, Final Batch Loss: 1.9127013683319092\n",
      "Epoch 207, Loss: 6.31305992603302, Final Batch Loss: 1.7038882970809937\n",
      "Epoch 208, Loss: 6.665545225143433, Final Batch Loss: 2.0406806468963623\n",
      "Epoch 209, Loss: 6.4678473472595215, Final Batch Loss: 1.8380093574523926\n",
      "Epoch 210, Loss: 5.232642710208893, Final Batch Loss: 0.6519072651863098\n",
      "Epoch 211, Loss: 7.488677263259888, Final Batch Loss: 2.7960972785949707\n",
      "Epoch 212, Loss: 6.858629822731018, Final Batch Loss: 2.2712135314941406\n",
      "Epoch 213, Loss: 6.782399773597717, Final Batch Loss: 2.114213228225708\n",
      "Epoch 214, Loss: 5.909221410751343, Final Batch Loss: 1.318962812423706\n",
      "Epoch 215, Loss: 5.81008243560791, Final Batch Loss: 1.1539701223373413\n",
      "Epoch 216, Loss: 5.0831568241119385, Final Batch Loss: 0.5633835792541504\n",
      "Epoch 217, Loss: 6.285762548446655, Final Batch Loss: 1.788550853729248\n",
      "Epoch 218, Loss: 5.0284464955329895, Final Batch Loss: 0.5154797434806824\n",
      "Epoch 219, Loss: 7.373746275901794, Final Batch Loss: 2.800013542175293\n",
      "Epoch 220, Loss: 5.247647941112518, Final Batch Loss: 0.7884756922721863\n",
      "Epoch 221, Loss: 6.047785520553589, Final Batch Loss: 1.561072826385498\n",
      "Epoch 222, Loss: 6.311335563659668, Final Batch Loss: 1.7939174175262451\n",
      "Epoch 223, Loss: 6.663900375366211, Final Batch Loss: 1.938340425491333\n",
      "Epoch 224, Loss: 5.5775556564331055, Final Batch Loss: 0.9700009822845459\n",
      "Epoch 225, Loss: 6.173418164253235, Final Batch Loss: 1.7085881233215332\n",
      "Epoch 226, Loss: 5.44231390953064, Final Batch Loss: 1.0523087978363037\n",
      "Epoch 227, Loss: 5.555324077606201, Final Batch Loss: 1.0001633167266846\n",
      "Epoch 228, Loss: 5.516500115394592, Final Batch Loss: 1.0514945983886719\n",
      "Epoch 229, Loss: 7.776012301445007, Final Batch Loss: 3.349806308746338\n",
      "Epoch 230, Loss: 5.608193755149841, Final Batch Loss: 1.2216976881027222\n",
      "Epoch 231, Loss: 6.7818474769592285, Final Batch Loss: 2.242828845977783\n",
      "Epoch 232, Loss: 5.073241353034973, Final Batch Loss: 0.39669370651245117\n",
      "Epoch 233, Loss: 5.043677031993866, Final Batch Loss: 0.3671557307243347\n",
      "Epoch 234, Loss: 4.9110912680625916, Final Batch Loss: 0.31411439180374146\n",
      "Epoch 235, Loss: 6.702953338623047, Final Batch Loss: 2.1871118545532227\n",
      "Epoch 236, Loss: 6.302621841430664, Final Batch Loss: 1.97615647315979\n",
      "Epoch 237, Loss: 6.233595967292786, Final Batch Loss: 1.8431681394577026\n",
      "Epoch 238, Loss: 6.10909628868103, Final Batch Loss: 1.775408387184143\n",
      "Epoch 239, Loss: 6.481432557106018, Final Batch Loss: 2.166727066040039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240, Loss: 5.8328821659088135, Final Batch Loss: 1.513062834739685\n",
      "Epoch 241, Loss: 6.28644585609436, Final Batch Loss: 2.0195205211639404\n",
      "Epoch 242, Loss: 6.748702764511108, Final Batch Loss: 2.4750936031341553\n",
      "Epoch 243, Loss: 7.179920196533203, Final Batch Loss: 2.8934214115142822\n",
      "Epoch 244, Loss: 4.886824518442154, Final Batch Loss: 0.4146139919757843\n",
      "Epoch 245, Loss: 5.621129512786865, Final Batch Loss: 1.1597720384597778\n",
      "Epoch 246, Loss: 6.28826367855072, Final Batch Loss: 1.8255550861358643\n",
      "Epoch 247, Loss: 6.01843786239624, Final Batch Loss: 1.6488233804702759\n",
      "Epoch 248, Loss: 6.562819242477417, Final Batch Loss: 2.087630271911621\n",
      "Epoch 249, Loss: 4.968777418136597, Final Batch Loss: 0.5257753133773804\n",
      "Epoch 250, Loss: 8.00783634185791, Final Batch Loss: 3.534735918045044\n",
      "Epoch 251, Loss: 5.416376948356628, Final Batch Loss: 1.0895708799362183\n",
      "Epoch 252, Loss: 6.116926431655884, Final Batch Loss: 1.859900712966919\n",
      "Epoch 253, Loss: 5.8925323486328125, Final Batch Loss: 1.4726859331130981\n",
      "Epoch 254, Loss: 5.199721932411194, Final Batch Loss: 0.7344422340393066\n",
      "Epoch 255, Loss: 6.103392004966736, Final Batch Loss: 1.7105275392532349\n",
      "Epoch 256, Loss: 6.045421719551086, Final Batch Loss: 1.7295445203781128\n",
      "Epoch 257, Loss: 5.75929868221283, Final Batch Loss: 1.50632905960083\n",
      "Epoch 258, Loss: 4.528740227222443, Final Batch Loss: 0.20941728353500366\n",
      "Epoch 259, Loss: 6.241949200630188, Final Batch Loss: 1.970473289489746\n",
      "Epoch 260, Loss: 6.134994149208069, Final Batch Loss: 1.737099051475525\n",
      "Epoch 261, Loss: 5.486690998077393, Final Batch Loss: 1.2308334112167358\n",
      "Epoch 262, Loss: 4.358196511864662, Final Batch Loss: 0.19369126856327057\n",
      "Epoch 263, Loss: 4.7793644070625305, Final Batch Loss: 0.4780675768852234\n",
      "Epoch 264, Loss: 4.893131971359253, Final Batch Loss: 0.6820377111434937\n",
      "Epoch 265, Loss: 6.0558048486709595, Final Batch Loss: 1.7767555713653564\n",
      "Epoch 266, Loss: 4.706207633018494, Final Batch Loss: 0.4538015127182007\n",
      "Epoch 267, Loss: 6.096435308456421, Final Batch Loss: 1.9272867441177368\n",
      "Epoch 268, Loss: 5.054906845092773, Final Batch Loss: 0.8516837358474731\n",
      "Epoch 269, Loss: 4.849291086196899, Final Batch Loss: 0.6219785213470459\n",
      "Epoch 270, Loss: 5.3550262451171875, Final Batch Loss: 1.1701196432113647\n",
      "Epoch 271, Loss: 4.699309885501862, Final Batch Loss: 0.5520203709602356\n",
      "Epoch 272, Loss: 5.519771218299866, Final Batch Loss: 1.3373768329620361\n",
      "Epoch 273, Loss: 6.296264410018921, Final Batch Loss: 2.0076732635498047\n",
      "Epoch 274, Loss: 7.227390289306641, Final Batch Loss: 2.904700994491577\n",
      "Epoch 275, Loss: 5.30189836025238, Final Batch Loss: 1.003461241722107\n",
      "Epoch 276, Loss: 6.11072301864624, Final Batch Loss: 1.8535350561141968\n",
      "Epoch 277, Loss: 4.426683068275452, Final Batch Loss: 0.3811420202255249\n",
      "Epoch 278, Loss: 4.821676015853882, Final Batch Loss: 0.6721593141555786\n",
      "Epoch 279, Loss: 5.153398811817169, Final Batch Loss: 0.9212577939033508\n",
      "Epoch 280, Loss: 5.3056734800338745, Final Batch Loss: 1.2247992753982544\n",
      "Epoch 281, Loss: 5.870670914649963, Final Batch Loss: 1.703630805015564\n",
      "Epoch 282, Loss: 4.296954661607742, Final Batch Loss: 0.13891974091529846\n",
      "Epoch 283, Loss: 6.979049563407898, Final Batch Loss: 2.7470545768737793\n",
      "Epoch 284, Loss: 6.323729991912842, Final Batch Loss: 2.0704221725463867\n",
      "Epoch 285, Loss: 4.477553129196167, Final Batch Loss: 0.3601562976837158\n",
      "Epoch 286, Loss: 6.344881892204285, Final Batch Loss: 2.240187406539917\n",
      "Epoch 287, Loss: 6.8019700050354, Final Batch Loss: 2.6793031692504883\n",
      "Epoch 288, Loss: 5.835395812988281, Final Batch Loss: 1.6888911724090576\n",
      "Epoch 289, Loss: 4.664580762386322, Final Batch Loss: 0.45260900259017944\n",
      "Epoch 290, Loss: 7.818680047988892, Final Batch Loss: 3.5820393562316895\n",
      "Epoch 291, Loss: 5.728665828704834, Final Batch Loss: 1.4348745346069336\n",
      "Epoch 292, Loss: 4.499602943658829, Final Batch Loss: 0.3503519594669342\n",
      "Epoch 293, Loss: 5.600755095481873, Final Batch Loss: 1.4393401145935059\n",
      "Epoch 294, Loss: 5.990879654884338, Final Batch Loss: 1.949159860610962\n",
      "Epoch 295, Loss: 6.008056282997131, Final Batch Loss: 1.8115811347961426\n",
      "Epoch 296, Loss: 6.122431755065918, Final Batch Loss: 2.011993408203125\n",
      "Epoch 297, Loss: 7.4698721170425415, Final Batch Loss: 3.2486019134521484\n",
      "Epoch 298, Loss: 6.461623311042786, Final Batch Loss: 2.279435157775879\n",
      "Epoch 299, Loss: 5.285099506378174, Final Batch Loss: 1.155378818511963\n",
      "Epoch 300, Loss: 4.816602051258087, Final Batch Loss: 0.6243525147438049\n",
      "Epoch 301, Loss: 5.394850134849548, Final Batch Loss: 1.261563777923584\n",
      "Epoch 302, Loss: 5.679017424583435, Final Batch Loss: 1.6224133968353271\n",
      "Epoch 303, Loss: 5.325072646141052, Final Batch Loss: 1.1987268924713135\n",
      "Epoch 304, Loss: 5.20529317855835, Final Batch Loss: 1.1133288145065308\n",
      "Epoch 305, Loss: 5.210110425949097, Final Batch Loss: 1.0638935565948486\n",
      "Epoch 306, Loss: 7.604750633239746, Final Batch Loss: 3.501145362854004\n",
      "Epoch 307, Loss: 4.344060331583023, Final Batch Loss: 0.33728745579719543\n",
      "Epoch 308, Loss: 4.535711288452148, Final Batch Loss: 0.5253973007202148\n",
      "Epoch 309, Loss: 5.21251904964447, Final Batch Loss: 1.157922625541687\n",
      "Epoch 310, Loss: 5.522805690765381, Final Batch Loss: 1.5459805727005005\n",
      "Epoch 311, Loss: 4.9882195591926575, Final Batch Loss: 0.9663731455802917\n",
      "Epoch 312, Loss: 5.447653770446777, Final Batch Loss: 1.440936803817749\n",
      "Epoch 313, Loss: 4.791906833648682, Final Batch Loss: 0.8198591470718384\n",
      "Epoch 314, Loss: 6.129580497741699, Final Batch Loss: 2.1392712593078613\n",
      "Epoch 315, Loss: 5.784103274345398, Final Batch Loss: 1.8872088193893433\n",
      "Epoch 316, Loss: 6.035175323486328, Final Batch Loss: 2.1005773544311523\n",
      "Epoch 317, Loss: 4.099292874336243, Final Batch Loss: 0.26828157901763916\n",
      "Epoch 318, Loss: 6.60243284702301, Final Batch Loss: 2.7028965950012207\n",
      "Epoch 319, Loss: 4.926027178764343, Final Batch Loss: 1.0279500484466553\n",
      "Epoch 320, Loss: 4.431081056594849, Final Batch Loss: 0.4576880931854248\n",
      "Epoch 321, Loss: 7.473533272743225, Final Batch Loss: 3.4981446266174316\n",
      "Epoch 322, Loss: 5.7686203718185425, Final Batch Loss: 1.775571346282959\n",
      "Epoch 323, Loss: 5.785618543624878, Final Batch Loss: 1.8726475238800049\n",
      "Epoch 324, Loss: 4.329050987958908, Final Batch Loss: 0.32168492674827576\n",
      "Epoch 325, Loss: 5.3900768756866455, Final Batch Loss: 1.5629304647445679\n",
      "Epoch 326, Loss: 3.8748556673526764, Final Batch Loss: 0.14641669392585754\n",
      "Epoch 327, Loss: 5.994258403778076, Final Batch Loss: 2.1354880332946777\n",
      "Epoch 328, Loss: 5.8484296798706055, Final Batch Loss: 1.9580997228622437\n",
      "Epoch 329, Loss: 4.016308635473251, Final Batch Loss: 0.11475488543510437\n",
      "Epoch 330, Loss: 5.667274117469788, Final Batch Loss: 1.7326867580413818\n",
      "Epoch 331, Loss: 6.242149472236633, Final Batch Loss: 2.2634315490722656\n",
      "Epoch 332, Loss: 4.5265615582466125, Final Batch Loss: 0.6084707379341125\n",
      "Epoch 333, Loss: 4.542015790939331, Final Batch Loss: 0.5907751321792603\n",
      "Epoch 334, Loss: 5.895859003067017, Final Batch Loss: 2.0382044315338135\n",
      "Epoch 335, Loss: 5.574493646621704, Final Batch Loss: 1.7708967924118042\n",
      "Epoch 336, Loss: 4.768762946128845, Final Batch Loss: 0.9462506771087646\n",
      "Epoch 337, Loss: 5.395302653312683, Final Batch Loss: 1.648796796798706\n",
      "Epoch 338, Loss: 5.931144714355469, Final Batch Loss: 2.099013328552246\n",
      "Epoch 339, Loss: 5.041450023651123, Final Batch Loss: 1.2772867679595947\n",
      "Epoch 340, Loss: 4.992849826812744, Final Batch Loss: 1.186815619468689\n",
      "Epoch 341, Loss: 6.233220815658569, Final Batch Loss: 2.3407673835754395\n",
      "Epoch 342, Loss: 6.015470385551453, Final Batch Loss: 2.266996145248413\n",
      "Epoch 343, Loss: 5.702770113945007, Final Batch Loss: 1.7774358987808228\n",
      "Epoch 344, Loss: 5.420431733131409, Final Batch Loss: 1.3461928367614746\n",
      "Epoch 345, Loss: 5.328717350959778, Final Batch Loss: 1.4643982648849487\n",
      "Epoch 346, Loss: 6.612266778945923, Final Batch Loss: 2.6517910957336426\n",
      "Epoch 347, Loss: 4.874303221702576, Final Batch Loss: 0.8690056800842285\n",
      "Epoch 348, Loss: 5.475786924362183, Final Batch Loss: 1.5933525562286377\n",
      "Epoch 349, Loss: 5.219324946403503, Final Batch Loss: 1.4762465953826904\n",
      "Epoch 350, Loss: 5.127209305763245, Final Batch Loss: 1.2271369695663452\n",
      "Epoch 351, Loss: 6.42860221862793, Final Batch Loss: 2.47111177444458\n",
      "Epoch 352, Loss: 6.076958417892456, Final Batch Loss: 2.1492795944213867\n",
      "Epoch 353, Loss: 5.503476977348328, Final Batch Loss: 1.7256836891174316\n",
      "Epoch 354, Loss: 5.402645826339722, Final Batch Loss: 1.6532437801361084\n",
      "Epoch 355, Loss: 5.094463586807251, Final Batch Loss: 1.4149739742279053\n",
      "Epoch 356, Loss: 4.816309809684753, Final Batch Loss: 1.0627634525299072\n",
      "Epoch 357, Loss: 5.364639043807983, Final Batch Loss: 1.6881483793258667\n",
      "Epoch 358, Loss: 5.009598612785339, Final Batch Loss: 1.2700870037078857\n",
      "Epoch 359, Loss: 3.9875257313251495, Final Batch Loss: 0.24238446354866028\n",
      "Epoch 360, Loss: 4.460301578044891, Final Batch Loss: 0.7604296803474426\n",
      "Epoch 361, Loss: 4.734354138374329, Final Batch Loss: 0.8957531452178955\n",
      "Epoch 362, Loss: 5.884130954742432, Final Batch Loss: 2.158700942993164\n",
      "Epoch 363, Loss: 5.0466238260269165, Final Batch Loss: 1.3705793619155884\n",
      "Epoch 364, Loss: 4.291662573814392, Final Batch Loss: 0.6706969738006592\n",
      "Epoch 365, Loss: 4.1530497670173645, Final Batch Loss: 0.4339485764503479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366, Loss: 5.6969475746154785, Final Batch Loss: 1.9875762462615967\n",
      "Epoch 367, Loss: 5.396764278411865, Final Batch Loss: 1.5891907215118408\n",
      "Epoch 368, Loss: 4.616159379482269, Final Batch Loss: 0.7697433829307556\n",
      "Epoch 369, Loss: 4.5358182191848755, Final Batch Loss: 0.8056502342224121\n",
      "Epoch 370, Loss: 5.715209007263184, Final Batch Loss: 2.0151607990264893\n",
      "Epoch 371, Loss: 5.79586398601532, Final Batch Loss: 2.1302034854888916\n",
      "Epoch 372, Loss: 5.402514934539795, Final Batch Loss: 1.6134018898010254\n",
      "Epoch 373, Loss: 6.008200407028198, Final Batch Loss: 2.128931999206543\n",
      "Epoch 374, Loss: 6.367953896522522, Final Batch Loss: 2.367478132247925\n",
      "Epoch 375, Loss: 5.716311693191528, Final Batch Loss: 1.9071452617645264\n",
      "Epoch 376, Loss: 4.636010468006134, Final Batch Loss: 0.6420760750770569\n",
      "Epoch 377, Loss: 9.25981056690216, Final Batch Loss: 5.003968238830566\n",
      "Epoch 378, Loss: 4.958210706710815, Final Batch Loss: 0.8373358249664307\n",
      "Epoch 379, Loss: 4.576722323894501, Final Batch Loss: 0.44410890340805054\n",
      "Epoch 380, Loss: 5.013798117637634, Final Batch Loss: 0.9252420663833618\n",
      "Epoch 381, Loss: 4.357025623321533, Final Batch Loss: 0.3128316402435303\n",
      "Epoch 382, Loss: 4.018772512674332, Final Batch Loss: 0.09236171841621399\n",
      "Epoch 383, Loss: 5.200504422187805, Final Batch Loss: 1.3116849660873413\n",
      "Epoch 384, Loss: 5.508287787437439, Final Batch Loss: 1.5782833099365234\n",
      "Epoch 385, Loss: 4.901579022407532, Final Batch Loss: 1.1687885522842407\n",
      "Epoch 386, Loss: 4.844535231590271, Final Batch Loss: 1.1447093486785889\n",
      "Epoch 387, Loss: 4.498993396759033, Final Batch Loss: 0.8226432800292969\n",
      "Epoch 388, Loss: 3.8739033713936806, Final Batch Loss: 0.1103958860039711\n",
      "Epoch 389, Loss: 4.507285416126251, Final Batch Loss: 0.5920129418373108\n",
      "Epoch 390, Loss: 5.318350195884705, Final Batch Loss: 1.4510458707809448\n",
      "Epoch 391, Loss: 5.542036056518555, Final Batch Loss: 1.673197627067566\n",
      "Epoch 392, Loss: 3.8968932703137398, Final Batch Loss: 0.07592762261629105\n",
      "Epoch 393, Loss: 4.61529153585434, Final Batch Loss: 0.9116641879081726\n",
      "Epoch 394, Loss: 4.317399263381958, Final Batch Loss: 0.6128982305526733\n",
      "Epoch 395, Loss: 5.783592939376831, Final Batch Loss: 2.1601758003234863\n",
      "Epoch 396, Loss: 5.297735929489136, Final Batch Loss: 1.5707511901855469\n",
      "Epoch 397, Loss: 4.074412375688553, Final Batch Loss: 0.3373929560184479\n",
      "Epoch 398, Loss: 3.9173694774508476, Final Batch Loss: 0.10421406477689743\n",
      "Epoch 399, Loss: 5.886718273162842, Final Batch Loss: 2.0987765789031982\n",
      "Epoch 400, Loss: 4.88922917842865, Final Batch Loss: 1.2498090267181396\n",
      "Epoch 401, Loss: 3.8505620807409286, Final Batch Loss: 0.22186051309108734\n",
      "Epoch 402, Loss: 4.770169973373413, Final Batch Loss: 1.2632807493209839\n",
      "Epoch 403, Loss: 4.978761434555054, Final Batch Loss: 1.354080319404602\n",
      "Epoch 404, Loss: 4.340464234352112, Final Batch Loss: 0.7539340257644653\n",
      "Epoch 405, Loss: 3.750375971198082, Final Batch Loss: 0.1769183725118637\n",
      "Epoch 406, Loss: 5.955700635910034, Final Batch Loss: 2.4132614135742188\n",
      "Epoch 407, Loss: 5.806904315948486, Final Batch Loss: 2.3075151443481445\n",
      "Epoch 408, Loss: 4.447540640830994, Final Batch Loss: 0.8249846696853638\n",
      "Epoch 409, Loss: 4.596534311771393, Final Batch Loss: 0.9105684161186218\n",
      "Epoch 410, Loss: 6.014181852340698, Final Batch Loss: 2.3667876720428467\n",
      "Epoch 411, Loss: 5.495252847671509, Final Batch Loss: 2.002281665802002\n",
      "Epoch 412, Loss: 5.678787112236023, Final Batch Loss: 2.0523123741149902\n",
      "Epoch 413, Loss: 4.107109218835831, Final Batch Loss: 0.40406420826911926\n",
      "Epoch 414, Loss: 4.921822190284729, Final Batch Loss: 1.2084424495697021\n",
      "Epoch 415, Loss: 3.6294830963015556, Final Batch Loss: 0.035707466304302216\n",
      "Epoch 416, Loss: 4.875015020370483, Final Batch Loss: 1.2682814598083496\n",
      "Epoch 417, Loss: 5.510381817817688, Final Batch Loss: 1.9427847862243652\n",
      "Epoch 418, Loss: 5.537395358085632, Final Batch Loss: 1.9127991199493408\n",
      "Epoch 419, Loss: 4.269597798585892, Final Batch Loss: 0.37125006318092346\n",
      "Epoch 420, Loss: 4.653274893760681, Final Batch Loss: 0.7940274477005005\n",
      "Epoch 421, Loss: 4.0102488696575165, Final Batch Loss: 0.07491609454154968\n",
      "Epoch 422, Loss: 5.0047279596328735, Final Batch Loss: 1.3376617431640625\n",
      "Epoch 423, Loss: 5.882249474525452, Final Batch Loss: 2.240032434463501\n",
      "Epoch 424, Loss: 3.5324732065200806, Final Batch Loss: 0.06712400913238525\n",
      "Epoch 425, Loss: 5.233325958251953, Final Batch Loss: 1.728403091430664\n",
      "Epoch 426, Loss: 4.716415286064148, Final Batch Loss: 1.2866270542144775\n",
      "Epoch 427, Loss: 5.420114994049072, Final Batch Loss: 1.8229283094406128\n",
      "Epoch 428, Loss: 4.337656795978546, Final Batch Loss: 0.7287306189537048\n",
      "Epoch 429, Loss: 6.102311611175537, Final Batch Loss: 2.54679799079895\n",
      "Epoch 430, Loss: 5.143314957618713, Final Batch Loss: 1.7385857105255127\n",
      "Epoch 431, Loss: 4.417327582836151, Final Batch Loss: 0.7811817526817322\n",
      "Epoch 432, Loss: 4.470632970333099, Final Batch Loss: 0.9630045294761658\n",
      "Epoch 433, Loss: 4.344937562942505, Final Batch Loss: 0.9421371221542358\n",
      "Epoch 434, Loss: 3.5671214312314987, Final Batch Loss: 0.12786976993083954\n",
      "Epoch 435, Loss: 5.603229641914368, Final Batch Loss: 2.124405860900879\n",
      "Epoch 436, Loss: 3.7212649136781693, Final Batch Loss: 0.21053357422351837\n",
      "Epoch 437, Loss: 3.697722941637039, Final Batch Loss: 0.21473494172096252\n",
      "Epoch 438, Loss: 4.766332387924194, Final Batch Loss: 1.2985961437225342\n",
      "Epoch 439, Loss: 4.355840086936951, Final Batch Loss: 1.0267200469970703\n",
      "Epoch 440, Loss: 4.79475462436676, Final Batch Loss: 1.2946518659591675\n",
      "Epoch 441, Loss: 4.8851470947265625, Final Batch Loss: 1.513131856918335\n",
      "Epoch 442, Loss: 4.047331809997559, Final Batch Loss: 0.5653170347213745\n",
      "Epoch 443, Loss: 5.832550764083862, Final Batch Loss: 2.40773344039917\n",
      "Epoch 444, Loss: 4.247913956642151, Final Batch Loss: 0.8582215309143066\n",
      "Epoch 445, Loss: 4.51652455329895, Final Batch Loss: 1.0420602560043335\n",
      "Epoch 446, Loss: 4.976564288139343, Final Batch Loss: 1.3729804754257202\n",
      "Epoch 447, Loss: 4.133683264255524, Final Batch Loss: 0.43908339738845825\n",
      "Epoch 448, Loss: 5.342965126037598, Final Batch Loss: 1.7593228816986084\n",
      "Epoch 449, Loss: 3.567732110619545, Final Batch Loss: 0.15667404234409332\n",
      "Epoch 450, Loss: 4.014789462089539, Final Batch Loss: 0.5324193239212036\n",
      "Epoch 451, Loss: 4.780524253845215, Final Batch Loss: 1.1733779907226562\n",
      "Epoch 452, Loss: 5.446335434913635, Final Batch Loss: 2.019765853881836\n",
      "Epoch 453, Loss: 4.782549500465393, Final Batch Loss: 1.3870402574539185\n",
      "Epoch 454, Loss: 3.6188922077417374, Final Batch Loss: 0.17411722242832184\n",
      "Epoch 455, Loss: 5.32632851600647, Final Batch Loss: 1.7803170680999756\n",
      "Epoch 456, Loss: 4.724699378013611, Final Batch Loss: 1.3794527053833008\n",
      "Epoch 457, Loss: 5.07893705368042, Final Batch Loss: 1.6327568292617798\n",
      "Epoch 458, Loss: 3.775632083415985, Final Batch Loss: 0.4588119387626648\n",
      "Epoch 459, Loss: 5.410600066184998, Final Batch Loss: 2.09909987449646\n",
      "Epoch 460, Loss: 6.112118721008301, Final Batch Loss: 2.620387315750122\n",
      "Epoch 461, Loss: 5.57119357585907, Final Batch Loss: 2.12288236618042\n",
      "Epoch 462, Loss: 5.153103232383728, Final Batch Loss: 1.7032639980316162\n",
      "Epoch 463, Loss: 4.071451961994171, Final Batch Loss: 0.5422443747520447\n",
      "Epoch 464, Loss: 5.475269436836243, Final Batch Loss: 2.0541810989379883\n",
      "Epoch 465, Loss: 5.032899260520935, Final Batch Loss: 1.6423945426940918\n",
      "Epoch 466, Loss: 4.615178346633911, Final Batch Loss: 1.1572566032409668\n",
      "Epoch 467, Loss: 3.764445200562477, Final Batch Loss: 0.21884192526340485\n",
      "Epoch 468, Loss: 4.523877501487732, Final Batch Loss: 0.7474969625473022\n",
      "Epoch 469, Loss: 5.601527810096741, Final Batch Loss: 1.7739777565002441\n",
      "Epoch 470, Loss: 5.249571204185486, Final Batch Loss: 1.663246989250183\n",
      "Epoch 471, Loss: 4.594967246055603, Final Batch Loss: 1.119046926498413\n",
      "Epoch 472, Loss: 3.9308853447437286, Final Batch Loss: 0.4740443527698517\n",
      "Epoch 473, Loss: 4.150332450866699, Final Batch Loss: 0.6120333671569824\n",
      "Epoch 474, Loss: 3.7701933830976486, Final Batch Loss: 0.235025092959404\n",
      "Epoch 475, Loss: 5.799079418182373, Final Batch Loss: 2.3906452655792236\n",
      "Epoch 476, Loss: 3.971805214881897, Final Batch Loss: 0.5920896530151367\n",
      "Epoch 477, Loss: 3.5093600526452065, Final Batch Loss: 0.04891715198755264\n",
      "Epoch 478, Loss: 7.893640160560608, Final Batch Loss: 4.3991899490356445\n",
      "Epoch 479, Loss: 4.963109493255615, Final Batch Loss: 1.6054118871688843\n",
      "Epoch 480, Loss: 3.7310251593589783, Final Batch Loss: 0.4056008458137512\n",
      "Epoch 481, Loss: 3.6258636713027954, Final Batch Loss: 0.41468346118927\n",
      "Epoch 482, Loss: 4.519461393356323, Final Batch Loss: 1.1327061653137207\n",
      "Epoch 483, Loss: 4.713820815086365, Final Batch Loss: 1.4578909873962402\n",
      "Epoch 484, Loss: 3.6168533712625504, Final Batch Loss: 0.22825588285923004\n",
      "Epoch 485, Loss: 3.4670685827732086, Final Batch Loss: 0.21332183480262756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486, Loss: 4.709448218345642, Final Batch Loss: 1.3015530109405518\n",
      "Epoch 487, Loss: 5.127090573310852, Final Batch Loss: 1.845913290977478\n",
      "Epoch 488, Loss: 5.177156209945679, Final Batch Loss: 1.813278079032898\n",
      "Epoch 489, Loss: 4.680817484855652, Final Batch Loss: 1.403882622718811\n",
      "Epoch 490, Loss: 3.66582253575325, Final Batch Loss: 0.15211203694343567\n",
      "Epoch 491, Loss: 3.5050889290869236, Final Batch Loss: 0.05736065283417702\n",
      "Epoch 492, Loss: 4.1286643743515015, Final Batch Loss: 0.6893587112426758\n",
      "Epoch 493, Loss: 4.632035851478577, Final Batch Loss: 1.2157760858535767\n",
      "Epoch 494, Loss: 5.029819130897522, Final Batch Loss: 1.7080535888671875\n",
      "Epoch 495, Loss: 3.386156866326928, Final Batch Loss: 0.018126079812645912\n",
      "Epoch 496, Loss: 4.041746258735657, Final Batch Loss: 0.6984872817993164\n",
      "Epoch 497, Loss: 4.531804203987122, Final Batch Loss: 1.266272783279419\n",
      "Epoch 498, Loss: 4.6249672174453735, Final Batch Loss: 1.3462321758270264\n",
      "Epoch 499, Loss: 4.813445329666138, Final Batch Loss: 1.4854013919830322\n",
      "Epoch 500, Loss: 4.1400880217552185, Final Batch Loss: 0.8186772465705872\n",
      "Epoch 501, Loss: 4.925074815750122, Final Batch Loss: 1.6400707960128784\n",
      "Epoch 502, Loss: 4.579883575439453, Final Batch Loss: 1.2790651321411133\n",
      "Epoch 503, Loss: 4.191285610198975, Final Batch Loss: 0.8718844652175903\n",
      "Epoch 504, Loss: 3.632908135652542, Final Batch Loss: 0.315384179353714\n",
      "Epoch 505, Loss: 4.981996059417725, Final Batch Loss: 1.6967663764953613\n",
      "Epoch 506, Loss: 4.07195919752121, Final Batch Loss: 0.6690912842750549\n",
      "Epoch 507, Loss: 4.595083475112915, Final Batch Loss: 1.1851123571395874\n",
      "Epoch 508, Loss: 4.86989152431488, Final Batch Loss: 1.4725666046142578\n",
      "Epoch 509, Loss: 5.27865195274353, Final Batch Loss: 1.848244547843933\n",
      "Epoch 510, Loss: 4.50557816028595, Final Batch Loss: 1.2758618593215942\n",
      "Epoch 511, Loss: 5.7222007513046265, Final Batch Loss: 2.287480354309082\n",
      "Epoch 512, Loss: 5.755432724952698, Final Batch Loss: 2.3598392009735107\n",
      "Epoch 513, Loss: 4.8210673332214355, Final Batch Loss: 1.3510479927062988\n",
      "Epoch 514, Loss: 4.589730739593506, Final Batch Loss: 1.0318310260772705\n",
      "Epoch 515, Loss: 3.744448632001877, Final Batch Loss: 0.27290698885917664\n",
      "Epoch 516, Loss: 3.96360319852829, Final Batch Loss: 0.5089330077171326\n",
      "Epoch 517, Loss: 3.404515877366066, Final Batch Loss: 0.046209827065467834\n",
      "Epoch 518, Loss: 3.3238160647451878, Final Batch Loss: 0.05601787939667702\n",
      "Epoch 519, Loss: 6.452667236328125, Final Batch Loss: 2.9884531497955322\n",
      "Epoch 520, Loss: 4.786061882972717, Final Batch Loss: 1.6097640991210938\n",
      "Epoch 521, Loss: 3.375725043937564, Final Batch Loss: 0.025780929252505302\n",
      "Epoch 522, Loss: 5.848160624504089, Final Batch Loss: 2.533566474914551\n",
      "Epoch 523, Loss: 3.4562671929597855, Final Batch Loss: 0.16467948257923126\n",
      "Epoch 524, Loss: 3.352089934051037, Final Batch Loss: 0.016066960990428925\n",
      "Epoch 525, Loss: 3.781949460506439, Final Batch Loss: 0.5494289994239807\n",
      "Epoch 526, Loss: 3.8631513714790344, Final Batch Loss: 0.7574522495269775\n",
      "Epoch 527, Loss: 3.273621555417776, Final Batch Loss: 0.031586285680532455\n",
      "Epoch 528, Loss: 3.7229193449020386, Final Batch Loss: 0.35016703605651855\n",
      "Epoch 529, Loss: 3.9002419114112854, Final Batch Loss: 0.6147351861000061\n",
      "Epoch 530, Loss: 5.595463037490845, Final Batch Loss: 2.376098871231079\n",
      "Epoch 531, Loss: 5.464684367179871, Final Batch Loss: 2.250332832336426\n",
      "Epoch 532, Loss: 5.270975828170776, Final Batch Loss: 1.8987867832183838\n",
      "Epoch 533, Loss: 4.337036967277527, Final Batch Loss: 1.1077804565429688\n",
      "Epoch 534, Loss: 3.8805588483810425, Final Batch Loss: 0.6379969120025635\n",
      "Epoch 535, Loss: 3.9623308777809143, Final Batch Loss: 0.6678672432899475\n",
      "Epoch 536, Loss: 4.0207677483558655, Final Batch Loss: 0.887195885181427\n",
      "Epoch 537, Loss: 3.2919817045331, Final Batch Loss: 0.03443456441164017\n",
      "Epoch 538, Loss: 4.014184653759003, Final Batch Loss: 0.7096629738807678\n",
      "Epoch 539, Loss: 4.773315191268921, Final Batch Loss: 1.4565317630767822\n",
      "Epoch 540, Loss: 4.739531993865967, Final Batch Loss: 1.411484956741333\n",
      "Epoch 541, Loss: 3.8925694823265076, Final Batch Loss: 0.5747254490852356\n",
      "Epoch 542, Loss: 4.7954782247543335, Final Batch Loss: 1.5708446502685547\n",
      "Epoch 543, Loss: 3.9475905299186707, Final Batch Loss: 0.7200780510902405\n",
      "Epoch 544, Loss: 4.442246913909912, Final Batch Loss: 1.1711117029190063\n",
      "Epoch 545, Loss: 3.7438011467456818, Final Batch Loss: 0.45656898617744446\n",
      "Epoch 546, Loss: 3.622445896267891, Final Batch Loss: 0.20015771687030792\n",
      "Epoch 547, Loss: 5.550651669502258, Final Batch Loss: 2.2112374305725098\n",
      "Epoch 548, Loss: 3.3544819056987762, Final Batch Loss: 0.0801820456981659\n",
      "Epoch 549, Loss: 3.617156967520714, Final Batch Loss: 0.17042957246303558\n",
      "Epoch 550, Loss: 3.350828103721142, Final Batch Loss: 0.019066862761974335\n",
      "Epoch 551, Loss: 3.641963839530945, Final Batch Loss: 0.41376960277557373\n",
      "Epoch 552, Loss: 3.5157304853200912, Final Batch Loss: 0.21382926404476166\n",
      "Epoch 553, Loss: 4.820780873298645, Final Batch Loss: 1.6274492740631104\n",
      "Epoch 554, Loss: 5.040329992771149, Final Batch Loss: 1.8402400016784668\n",
      "Epoch 555, Loss: 3.7349623441696167, Final Batch Loss: 0.48011088371276855\n",
      "Epoch 556, Loss: 3.3153486251831055, Final Batch Loss: 0.10647070407867432\n",
      "Epoch 557, Loss: 3.908064067363739, Final Batch Loss: 0.7619515061378479\n",
      "Epoch 558, Loss: 4.84662264585495, Final Batch Loss: 1.7159055471420288\n",
      "Epoch 559, Loss: 5.8274946212768555, Final Batch Loss: 2.644406795501709\n",
      "Epoch 560, Loss: 3.873591423034668, Final Batch Loss: 0.5834907293319702\n",
      "Epoch 561, Loss: 4.7367918491363525, Final Batch Loss: 1.3831875324249268\n",
      "Epoch 562, Loss: 5.615655064582825, Final Batch Loss: 2.2048885822296143\n",
      "Epoch 563, Loss: 5.6065661907196045, Final Batch Loss: 2.004549026489258\n",
      "Epoch 564, Loss: 3.679810553789139, Final Batch Loss: 0.06422403454780579\n",
      "Epoch 565, Loss: 4.65990424156189, Final Batch Loss: 0.7471237182617188\n",
      "Epoch 566, Loss: 5.54567551612854, Final Batch Loss: 1.8198318481445312\n",
      "Epoch 567, Loss: 3.9208708107471466, Final Batch Loss: 0.2646278440952301\n",
      "Epoch 568, Loss: 5.068576455116272, Final Batch Loss: 1.5815784931182861\n",
      "Epoch 569, Loss: 3.772240787744522, Final Batch Loss: 0.4080440104007721\n",
      "Epoch 570, Loss: 3.7021497786045074, Final Batch Loss: 0.3862129747867584\n",
      "Epoch 571, Loss: 3.594992220401764, Final Batch Loss: 0.3636252284049988\n",
      "Epoch 572, Loss: 3.4615676552057266, Final Batch Loss: 0.2346414476633072\n",
      "Epoch 573, Loss: 3.9281976222991943, Final Batch Loss: 0.7600058317184448\n",
      "Epoch 574, Loss: 7.914957404136658, Final Batch Loss: 4.804553508758545\n",
      "Epoch 575, Loss: 3.446823626756668, Final Batch Loss: 0.3029010593891144\n",
      "Epoch 576, Loss: 3.223411962389946, Final Batch Loss: 0.18085186183452606\n",
      "Epoch 577, Loss: 4.001400470733643, Final Batch Loss: 0.958740770816803\n",
      "Epoch 578, Loss: 3.6516462862491608, Final Batch Loss: 0.4284009039402008\n",
      "Epoch 579, Loss: 4.373324036598206, Final Batch Loss: 1.2692817449569702\n",
      "Epoch 580, Loss: 3.9356756806373596, Final Batch Loss: 0.7162832617759705\n",
      "Epoch 581, Loss: 3.2298115268349648, Final Batch Loss: 0.015744425356388092\n",
      "Epoch 582, Loss: 3.3751760572195053, Final Batch Loss: 0.031605228781700134\n",
      "Epoch 583, Loss: 4.31246018409729, Final Batch Loss: 0.9822788238525391\n",
      "Epoch 584, Loss: 4.38947981595993, Final Batch Loss: 0.9968388676643372\n",
      "Epoch 585, Loss: 3.877746105194092, Final Batch Loss: 0.7195767164230347\n",
      "Epoch 586, Loss: 3.329232394695282, Final Batch Loss: 0.13901787996292114\n",
      "Epoch 587, Loss: 3.2412413135170937, Final Batch Loss: 0.06760410219430923\n",
      "Epoch 588, Loss: 4.378358006477356, Final Batch Loss: 1.0838980674743652\n",
      "Epoch 589, Loss: 3.6219824254512787, Final Batch Loss: 0.45904186367988586\n",
      "Epoch 590, Loss: 3.3041570261120796, Final Batch Loss: 0.08786917477846146\n",
      "Epoch 591, Loss: 5.068893909454346, Final Batch Loss: 1.6320494413375854\n",
      "Epoch 592, Loss: 3.929874062538147, Final Batch Loss: 0.7027715444564819\n",
      "Epoch 593, Loss: 4.514715790748596, Final Batch Loss: 1.2209964990615845\n",
      "Epoch 594, Loss: 3.458322450518608, Final Batch Loss: 0.1705818623304367\n",
      "Epoch 595, Loss: 4.874592185020447, Final Batch Loss: 1.5696938037872314\n",
      "Epoch 596, Loss: 5.464117467403412, Final Batch Loss: 2.3016295433044434\n",
      "Epoch 597, Loss: 3.2015783339738846, Final Batch Loss: 0.07152695953845978\n",
      "Epoch 598, Loss: 4.489427328109741, Final Batch Loss: 1.4034606218338013\n",
      "Epoch 599, Loss: 6.3649996519088745, Final Batch Loss: 3.1706371307373047\n",
      "Epoch 600, Loss: 5.1426955461502075, Final Batch Loss: 1.8995763063430786\n",
      "Epoch 601, Loss: 5.174347162246704, Final Batch Loss: 1.8002135753631592\n",
      "Epoch 602, Loss: 5.057209014892578, Final Batch Loss: 1.7628737688064575\n",
      "Epoch 603, Loss: 4.032006204128265, Final Batch Loss: 0.8540753722190857\n",
      "Epoch 604, Loss: 3.4198487997055054, Final Batch Loss: 0.30037832260131836\n",
      "Epoch 605, Loss: 5.206732392311096, Final Batch Loss: 1.8955843448638916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606, Loss: 4.637246310710907, Final Batch Loss: 1.4664802551269531\n",
      "Epoch 607, Loss: 3.885760724544525, Final Batch Loss: 0.7690334916114807\n",
      "Epoch 608, Loss: 4.572026014328003, Final Batch Loss: 1.400357961654663\n",
      "Epoch 609, Loss: 4.464388608932495, Final Batch Loss: 1.4243335723876953\n",
      "Epoch 610, Loss: 7.793935418128967, Final Batch Loss: 4.671600341796875\n",
      "Epoch 611, Loss: 5.201603889465332, Final Batch Loss: 2.0320839881896973\n",
      "Epoch 612, Loss: 3.514577180147171, Final Batch Loss: 0.1870613396167755\n",
      "Epoch 613, Loss: 3.4720893651247025, Final Batch Loss: 0.07099922001361847\n",
      "Epoch 614, Loss: 5.28800892829895, Final Batch Loss: 1.8229297399520874\n",
      "Epoch 615, Loss: 4.712815761566162, Final Batch Loss: 1.3437732458114624\n",
      "Epoch 616, Loss: 3.8274523317813873, Final Batch Loss: 0.4250674545764923\n",
      "Epoch 617, Loss: 4.771581768989563, Final Batch Loss: 1.4494421482086182\n",
      "Epoch 618, Loss: 4.014526844024658, Final Batch Loss: 0.6789455413818359\n",
      "Epoch 619, Loss: 3.9803332090377808, Final Batch Loss: 0.7387360334396362\n",
      "Epoch 620, Loss: 3.9250694513320923, Final Batch Loss: 0.8082275390625\n",
      "Epoch 621, Loss: 4.2687371373176575, Final Batch Loss: 1.1392921209335327\n",
      "Epoch 622, Loss: 4.352783203125, Final Batch Loss: 1.2349568605422974\n",
      "Epoch 623, Loss: 5.460097312927246, Final Batch Loss: 2.343122720718384\n",
      "Epoch 624, Loss: 4.085568726062775, Final Batch Loss: 0.9782965779304504\n",
      "Epoch 625, Loss: 3.4898464381694794, Final Batch Loss: 0.2515331208705902\n",
      "Epoch 626, Loss: 3.7742995619773865, Final Batch Loss: 0.5758238434791565\n",
      "Epoch 627, Loss: 9.121630728244781, Final Batch Loss: 6.014986991882324\n",
      "Epoch 628, Loss: 4.7420249581336975, Final Batch Loss: 1.660847544670105\n",
      "Epoch 629, Loss: 3.250087134540081, Final Batch Loss: 0.03557436913251877\n",
      "Epoch 630, Loss: 3.4133924543857574, Final Batch Loss: 0.15753379464149475\n",
      "Epoch 631, Loss: 3.9281774163246155, Final Batch Loss: 0.6403674483299255\n",
      "Epoch 632, Loss: 4.667898416519165, Final Batch Loss: 1.224579095840454\n",
      "Epoch 633, Loss: 3.912396490573883, Final Batch Loss: 0.5693415999412537\n",
      "Epoch 634, Loss: 3.22047071903944, Final Batch Loss: 0.061416082084178925\n",
      "Epoch 635, Loss: 5.108556151390076, Final Batch Loss: 1.9982035160064697\n",
      "Epoch 636, Loss: 3.260867416858673, Final Batch Loss: 0.12911981344223022\n",
      "Epoch 637, Loss: 3.7420257925987244, Final Batch Loss: 0.611894428730011\n",
      "Epoch 638, Loss: 4.941640734672546, Final Batch Loss: 1.8589783906936646\n",
      "Epoch 639, Loss: 3.4753766655921936, Final Batch Loss: 0.43892771005630493\n",
      "Epoch 640, Loss: 3.121607296168804, Final Batch Loss: 0.07753466814756393\n",
      "Epoch 641, Loss: 4.66769140958786, Final Batch Loss: 1.5364243984222412\n",
      "Epoch 642, Loss: 5.107235312461853, Final Batch Loss: 2.0027408599853516\n",
      "Epoch 643, Loss: 3.2386253625154495, Final Batch Loss: 0.14480336010456085\n",
      "Epoch 644, Loss: 3.543341726064682, Final Batch Loss: 0.4184456765651703\n",
      "Epoch 645, Loss: 4.486135959625244, Final Batch Loss: 1.3330230712890625\n",
      "Epoch 646, Loss: 4.625063061714172, Final Batch Loss: 1.643956184387207\n",
      "Epoch 647, Loss: 3.1895264238119125, Final Batch Loss: 0.1801256388425827\n",
      "Epoch 648, Loss: 3.945607006549835, Final Batch Loss: 0.8660702705383301\n",
      "Epoch 649, Loss: 4.724987268447876, Final Batch Loss: 1.6988794803619385\n",
      "Epoch 650, Loss: 3.6353012919425964, Final Batch Loss: 0.5687666535377502\n",
      "Epoch 651, Loss: 5.125225901603699, Final Batch Loss: 1.8034682273864746\n",
      "Epoch 652, Loss: 3.247789589688182, Final Batch Loss: 0.027375904843211174\n",
      "Epoch 653, Loss: 4.28260064125061, Final Batch Loss: 1.1322084665298462\n",
      "Epoch 654, Loss: 4.416092216968536, Final Batch Loss: 1.379966139793396\n",
      "Epoch 655, Loss: 4.181890249252319, Final Batch Loss: 1.0555394887924194\n",
      "Epoch 656, Loss: 6.3220614194869995, Final Batch Loss: 3.2743420600891113\n",
      "Epoch 657, Loss: 4.469369292259216, Final Batch Loss: 1.359236478805542\n",
      "Epoch 658, Loss: 5.149731278419495, Final Batch Loss: 1.9996583461761475\n",
      "Epoch 659, Loss: 4.003965616226196, Final Batch Loss: 0.8345019817352295\n",
      "Epoch 660, Loss: 3.9547889828681946, Final Batch Loss: 0.7778378129005432\n",
      "Epoch 661, Loss: 5.77732127904892, Final Batch Loss: 2.7518301010131836\n",
      "Epoch 662, Loss: 5.7015790939331055, Final Batch Loss: 2.514091968536377\n",
      "Epoch 663, Loss: 5.409192442893982, Final Batch Loss: 2.3039889335632324\n",
      "Epoch 664, Loss: 5.439164757728577, Final Batch Loss: 2.4662840366363525\n",
      "Epoch 665, Loss: 3.5187711119651794, Final Batch Loss: 0.4601842164993286\n",
      "Epoch 666, Loss: 3.323011875152588, Final Batch Loss: 0.06668806076049805\n",
      "Epoch 667, Loss: 4.590098142623901, Final Batch Loss: 1.381600022315979\n",
      "Epoch 668, Loss: 3.667156547307968, Final Batch Loss: 0.4655973017215729\n",
      "Epoch 669, Loss: 4.790503740310669, Final Batch Loss: 1.5727077722549438\n",
      "Epoch 670, Loss: 3.3366807103157043, Final Batch Loss: 0.254239559173584\n",
      "Epoch 671, Loss: 4.955578625202179, Final Batch Loss: 1.7900733947753906\n",
      "Epoch 672, Loss: 4.2550724148750305, Final Batch Loss: 1.2513978481292725\n",
      "Epoch 673, Loss: 3.241826593875885, Final Batch Loss: 0.11772507429122925\n",
      "Epoch 674, Loss: 5.497752070426941, Final Batch Loss: 2.392042636871338\n",
      "Epoch 675, Loss: 4.685433387756348, Final Batch Loss: 1.5525732040405273\n",
      "Epoch 676, Loss: 4.6393885016441345, Final Batch Loss: 1.5416374206542969\n",
      "Epoch 677, Loss: 4.270140528678894, Final Batch Loss: 1.2334290742874146\n",
      "Epoch 678, Loss: 4.695606529712677, Final Batch Loss: 1.606507420539856\n",
      "Epoch 679, Loss: 3.5411297082901, Final Batch Loss: 0.48885971307754517\n",
      "Epoch 680, Loss: 5.355710029602051, Final Batch Loss: 2.2984843254089355\n",
      "Epoch 681, Loss: 4.053455948829651, Final Batch Loss: 0.9727721810340881\n",
      "Epoch 682, Loss: 3.7674301266670227, Final Batch Loss: 0.569564163684845\n",
      "Epoch 683, Loss: 4.571548581123352, Final Batch Loss: 1.3103519678115845\n",
      "Epoch 684, Loss: 4.954545259475708, Final Batch Loss: 1.648377776145935\n",
      "Epoch 685, Loss: 3.3503336682915688, Final Batch Loss: 0.12371359020471573\n",
      "Epoch 686, Loss: 3.7358681559562683, Final Batch Loss: 0.5515252947807312\n",
      "Epoch 687, Loss: 5.49336576461792, Final Batch Loss: 2.468853712081909\n",
      "Epoch 688, Loss: 4.180295348167419, Final Batch Loss: 1.0715765953063965\n",
      "Epoch 689, Loss: 5.177067041397095, Final Batch Loss: 2.1019980907440186\n",
      "Epoch 690, Loss: 5.048995494842529, Final Batch Loss: 1.8202149868011475\n",
      "Epoch 691, Loss: 3.4192762076854706, Final Batch Loss: 0.16488966345787048\n",
      "Epoch 692, Loss: 4.2739481925964355, Final Batch Loss: 1.145801067352295\n",
      "Epoch 693, Loss: 4.6071617603302, Final Batch Loss: 1.4540061950683594\n",
      "Epoch 694, Loss: 4.457634329795837, Final Batch Loss: 1.4314881563186646\n",
      "Epoch 695, Loss: 4.710571527481079, Final Batch Loss: 1.6396825313568115\n",
      "Epoch 696, Loss: 3.0577630400657654, Final Batch Loss: 0.03838425874710083\n",
      "Epoch 697, Loss: 3.5772414803504944, Final Batch Loss: 0.6689225435256958\n",
      "Epoch 698, Loss: 3.532823860645294, Final Batch Loss: 0.5177894830703735\n",
      "Epoch 699, Loss: 3.76665860414505, Final Batch Loss: 0.8159077167510986\n",
      "Epoch 700, Loss: 5.0407754778862, Final Batch Loss: 2.007936954498291\n",
      "Epoch 701, Loss: 3.71386057138443, Final Batch Loss: 0.6845514178276062\n",
      "Epoch 702, Loss: 3.652437210083008, Final Batch Loss: 0.6885823011398315\n",
      "Epoch 703, Loss: 3.011552467942238, Final Batch Loss: 0.04719473421573639\n",
      "Epoch 704, Loss: 4.728569149971008, Final Batch Loss: 1.6895678043365479\n",
      "Epoch 705, Loss: 4.046800911426544, Final Batch Loss: 1.055537462234497\n",
      "Epoch 706, Loss: 9.154652118682861, Final Batch Loss: 6.164638519287109\n",
      "Epoch 707, Loss: 5.77896773815155, Final Batch Loss: 2.5745999813079834\n",
      "Epoch 708, Loss: 3.631835401058197, Final Batch Loss: 0.5859895944595337\n",
      "Epoch 709, Loss: 5.677649438381195, Final Batch Loss: 2.6610770225524902\n",
      "Epoch 710, Loss: 4.555564641952515, Final Batch Loss: 1.3990724086761475\n",
      "Epoch 711, Loss: 5.347868800163269, Final Batch Loss: 1.9189763069152832\n",
      "Epoch 712, Loss: 4.986550688743591, Final Batch Loss: 1.4291152954101562\n",
      "Epoch 713, Loss: 3.9328058660030365, Final Batch Loss: 0.40748485922813416\n",
      "Epoch 714, Loss: 3.550629671663046, Final Batch Loss: 0.06162267550826073\n",
      "Epoch 715, Loss: 6.367031097412109, Final Batch Loss: 3.05599308013916\n",
      "Epoch 716, Loss: 4.070574641227722, Final Batch Loss: 0.9646316766738892\n",
      "Epoch 717, Loss: 4.855795681476593, Final Batch Loss: 1.679011583328247\n",
      "Epoch 718, Loss: 4.9843796491622925, Final Batch Loss: 1.7793458700180054\n",
      "Epoch 719, Loss: 5.510333299636841, Final Batch Loss: 2.3591909408569336\n",
      "Epoch 720, Loss: 4.9948636293411255, Final Batch Loss: 1.75063157081604\n",
      "Epoch 721, Loss: 3.3545176312327385, Final Batch Loss: 0.07262938469648361\n",
      "Epoch 722, Loss: 4.811930418014526, Final Batch Loss: 1.4782812595367432\n",
      "Epoch 723, Loss: 5.33938729763031, Final Batch Loss: 2.1059365272521973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 724, Loss: 3.910991609096527, Final Batch Loss: 0.8793471455574036\n",
      "Epoch 725, Loss: 4.476660370826721, Final Batch Loss: 1.3313987255096436\n",
      "Epoch 726, Loss: 3.3229491792153567, Final Batch Loss: 0.0039016089867800474\n",
      "Epoch 727, Loss: 4.079948306083679, Final Batch Loss: 0.8072774410247803\n",
      "Epoch 728, Loss: 4.279118776321411, Final Batch Loss: 1.125335454940796\n",
      "Epoch 729, Loss: 3.969914436340332, Final Batch Loss: 0.9242698550224304\n",
      "Epoch 730, Loss: 3.6891977190971375, Final Batch Loss: 0.6097168922424316\n",
      "Epoch 731, Loss: 4.075595915317535, Final Batch Loss: 1.0431197881698608\n",
      "Epoch 732, Loss: 3.763964891433716, Final Batch Loss: 0.7942554354667664\n",
      "Epoch 733, Loss: 3.233017936348915, Final Batch Loss: 0.03994037210941315\n",
      "Epoch 734, Loss: 4.047766923904419, Final Batch Loss: 0.8299673795700073\n",
      "Epoch 735, Loss: 5.355869293212891, Final Batch Loss: 2.1886909008026123\n",
      "Epoch 736, Loss: 3.440331280231476, Final Batch Loss: 0.44182640314102173\n",
      "Epoch 737, Loss: 3.4380204677581787, Final Batch Loss: 0.4430657625198364\n",
      "Epoch 738, Loss: 4.369110345840454, Final Batch Loss: 1.2923434972763062\n",
      "Epoch 739, Loss: 3.216064453125, Final Batch Loss: 0.3025062084197998\n",
      "Epoch 740, Loss: 3.547262489795685, Final Batch Loss: 0.547356128692627\n",
      "Epoch 741, Loss: 4.866986513137817, Final Batch Loss: 1.963148593902588\n",
      "Epoch 742, Loss: 4.306439757347107, Final Batch Loss: 1.320270299911499\n",
      "Epoch 743, Loss: 3.189834237098694, Final Batch Loss: 0.2587115168571472\n",
      "Epoch 744, Loss: 3.2888376712799072, Final Batch Loss: 0.26303523778915405\n",
      "Epoch 745, Loss: 3.39077827334404, Final Batch Loss: 0.40085461735725403\n",
      "Epoch 746, Loss: 3.9800426363945007, Final Batch Loss: 0.9783118963241577\n",
      "Epoch 747, Loss: 4.151156544685364, Final Batch Loss: 1.2422654628753662\n",
      "Epoch 748, Loss: 4.3633410930633545, Final Batch Loss: 1.4063767194747925\n",
      "Epoch 749, Loss: 3.840532422065735, Final Batch Loss: 0.9829937219619751\n",
      "Epoch 750, Loss: 4.345324158668518, Final Batch Loss: 1.303721308708191\n",
      "Epoch 751, Loss: 3.968196928501129, Final Batch Loss: 1.0210882425308228\n",
      "Epoch 752, Loss: 4.508279740810394, Final Batch Loss: 1.4942498207092285\n",
      "Epoch 753, Loss: 4.887737572193146, Final Batch Loss: 1.867126703262329\n",
      "Epoch 754, Loss: 5.50618588924408, Final Batch Loss: 2.4720585346221924\n",
      "Epoch 755, Loss: 3.150532692670822, Final Batch Loss: 0.07545128464698792\n",
      "Epoch 756, Loss: 3.1860501766204834, Final Batch Loss: 0.3028205633163452\n",
      "Epoch 757, Loss: 4.848407506942749, Final Batch Loss: 1.946276068687439\n",
      "Epoch 758, Loss: 5.319413900375366, Final Batch Loss: 2.419142007827759\n",
      "Epoch 759, Loss: 4.659608960151672, Final Batch Loss: 1.752869725227356\n",
      "Epoch 760, Loss: 3.7271523475646973, Final Batch Loss: 0.7240474820137024\n",
      "Epoch 761, Loss: 3.9898838996887207, Final Batch Loss: 1.0526773929595947\n",
      "Epoch 762, Loss: 3.013495594263077, Final Batch Loss: 0.17267319560050964\n",
      "Epoch 763, Loss: 3.54084450006485, Final Batch Loss: 0.5752056837081909\n",
      "Epoch 764, Loss: 3.941625654697418, Final Batch Loss: 0.9888158440589905\n",
      "Epoch 765, Loss: 4.608476758003235, Final Batch Loss: 1.6662062406539917\n",
      "Epoch 766, Loss: 2.9952780324965715, Final Batch Loss: 0.01615259237587452\n",
      "Epoch 767, Loss: 3.091488592326641, Final Batch Loss: 0.0660288855433464\n",
      "Epoch 768, Loss: 3.0078718215227127, Final Batch Loss: 0.06282933056354523\n",
      "Epoch 769, Loss: 4.881072580814362, Final Batch Loss: 1.9324793815612793\n",
      "Epoch 770, Loss: 3.0242653489112854, Final Batch Loss: 0.18393707275390625\n",
      "Epoch 771, Loss: 4.940062761306763, Final Batch Loss: 2.019760847091675\n",
      "Epoch 772, Loss: 4.294483780860901, Final Batch Loss: 1.3948860168457031\n",
      "Epoch 773, Loss: 3.228028327226639, Final Batch Loss: 0.31975725293159485\n",
      "Epoch 774, Loss: 3.736134171485901, Final Batch Loss: 0.6777240037918091\n",
      "Epoch 775, Loss: 4.043392360210419, Final Batch Loss: 1.091241717338562\n",
      "Epoch 776, Loss: 3.0634248219430447, Final Batch Loss: 0.040099915117025375\n",
      "Epoch 777, Loss: 3.012557454407215, Final Batch Loss: 0.04131335765123367\n",
      "Epoch 778, Loss: 4.65882009267807, Final Batch Loss: 1.6678996086120605\n",
      "Epoch 779, Loss: 4.809218406677246, Final Batch Loss: 1.7347908020019531\n",
      "Epoch 780, Loss: 3.9051140546798706, Final Batch Loss: 0.9991199970245361\n",
      "Epoch 781, Loss: 4.529115974903107, Final Batch Loss: 1.6035536527633667\n",
      "Epoch 782, Loss: 5.063292026519775, Final Batch Loss: 2.2173590660095215\n",
      "Epoch 783, Loss: 2.9525779634714127, Final Batch Loss: 0.06502695381641388\n",
      "Epoch 784, Loss: 3.670847713947296, Final Batch Loss: 0.7698744535446167\n",
      "Epoch 785, Loss: 3.707708716392517, Final Batch Loss: 0.8453280329704285\n",
      "Epoch 786, Loss: 4.427281558513641, Final Batch Loss: 1.5254651308059692\n",
      "Epoch 787, Loss: 3.274173468351364, Final Batch Loss: 0.3712799847126007\n",
      "Epoch 788, Loss: 3.0794761069118977, Final Batch Loss: 0.008153486996889114\n",
      "Epoch 789, Loss: 3.134322033729404, Final Batch Loss: 0.005119429435580969\n",
      "Epoch 790, Loss: 4.797085642814636, Final Batch Loss: 1.8027549982070923\n",
      "Epoch 791, Loss: 3.8230230808258057, Final Batch Loss: 0.7318663597106934\n",
      "Epoch 792, Loss: 3.2300295382738113, Final Batch Loss: 0.17321880161762238\n",
      "Epoch 793, Loss: 3.0398757606744766, Final Batch Loss: 0.08694835007190704\n",
      "Epoch 794, Loss: 4.690965175628662, Final Batch Loss: 1.7194257974624634\n",
      "Epoch 795, Loss: 3.9713184237480164, Final Batch Loss: 1.0370949506759644\n",
      "Epoch 796, Loss: 3.7515020966529846, Final Batch Loss: 0.7903497219085693\n",
      "Epoch 797, Loss: 3.455203652381897, Final Batch Loss: 0.5825383067131042\n",
      "Epoch 798, Loss: 2.8069434762001038, Final Batch Loss: 0.043643176555633545\n",
      "Epoch 799, Loss: 3.717769503593445, Final Batch Loss: 0.8080551624298096\n",
      "Epoch 800, Loss: 6.112618565559387, Final Batch Loss: 3.194068431854248\n",
      "Epoch 801, Loss: 3.1754134595394135, Final Batch Loss: 0.3108094036579132\n",
      "Epoch 802, Loss: 3.940902292728424, Final Batch Loss: 1.0342466831207275\n",
      "Epoch 803, Loss: 4.78788298368454, Final Batch Loss: 1.846605658531189\n",
      "Epoch 804, Loss: 3.059566962532699, Final Batch Loss: 0.011040079407393932\n",
      "Epoch 805, Loss: 5.6647087931633, Final Batch Loss: 2.5809342861175537\n",
      "Epoch 806, Loss: 4.912022173404694, Final Batch Loss: 1.9001708030700684\n",
      "Epoch 807, Loss: 4.666568219661713, Final Batch Loss: 1.6677734851837158\n",
      "Epoch 808, Loss: 3.679831564426422, Final Batch Loss: 0.6695493459701538\n",
      "Epoch 809, Loss: 2.8786150347441435, Final Batch Loss: 0.012024534866213799\n",
      "Epoch 810, Loss: 3.905675947666168, Final Batch Loss: 1.0713528394699097\n",
      "Epoch 811, Loss: 2.9502293169498444, Final Batch Loss: 0.04473617672920227\n",
      "Epoch 812, Loss: 3.136056900024414, Final Batch Loss: 0.2257954478263855\n",
      "Epoch 813, Loss: 3.3593433499336243, Final Batch Loss: 0.4892035126686096\n",
      "Epoch 814, Loss: 5.0819743275642395, Final Batch Loss: 2.2048556804656982\n",
      "Epoch 815, Loss: 4.1362205147743225, Final Batch Loss: 1.3353261947631836\n",
      "Epoch 816, Loss: 4.63360196352005, Final Batch Loss: 1.7657289505004883\n",
      "Epoch 817, Loss: 4.194368600845337, Final Batch Loss: 1.262824296951294\n",
      "Epoch 818, Loss: 3.9251864552497864, Final Batch Loss: 0.9038622975349426\n",
      "Epoch 819, Loss: 3.0870686508715153, Final Batch Loss: 0.02179107442498207\n",
      "Epoch 820, Loss: 2.999685361981392, Final Batch Loss: 0.04357059299945831\n",
      "Epoch 821, Loss: 4.286733448505402, Final Batch Loss: 1.255388855934143\n",
      "Epoch 822, Loss: 3.125493824481964, Final Batch Loss: 0.21450525522232056\n",
      "Epoch 823, Loss: 3.3256147503852844, Final Batch Loss: 0.39948034286499023\n",
      "Epoch 824, Loss: 4.333824872970581, Final Batch Loss: 1.4347232580184937\n",
      "Epoch 825, Loss: 2.961536094546318, Final Batch Loss: 0.017584845423698425\n",
      "Epoch 826, Loss: 2.9407227486371994, Final Batch Loss: 0.04616008698940277\n",
      "Epoch 827, Loss: 2.949418805539608, Final Batch Loss: 0.023903869092464447\n",
      "Epoch 828, Loss: 5.34003221988678, Final Batch Loss: 2.3572282791137695\n",
      "Epoch 829, Loss: 4.24310302734375, Final Batch Loss: 1.2420990467071533\n",
      "Epoch 830, Loss: 4.5883830189704895, Final Batch Loss: 1.6150147914886475\n",
      "Epoch 831, Loss: 3.710362672805786, Final Batch Loss: 0.6952629089355469\n",
      "Epoch 832, Loss: 3.0322047509253025, Final Batch Loss: 0.026289764791727066\n",
      "Epoch 833, Loss: 3.93489408493042, Final Batch Loss: 1.0010175704956055\n",
      "Epoch 834, Loss: 3.5960739254951477, Final Batch Loss: 0.7947860956192017\n",
      "Epoch 835, Loss: 4.636291921138763, Final Batch Loss: 1.758138656616211\n",
      "Epoch 836, Loss: 2.8512347983196378, Final Batch Loss: 0.007117156870663166\n",
      "Epoch 837, Loss: 4.724420845508575, Final Batch Loss: 1.7822214365005493\n",
      "Epoch 838, Loss: 3.5823808312416077, Final Batch Loss: 0.5704817771911621\n",
      "Epoch 839, Loss: 3.7187787890434265, Final Batch Loss: 0.7278493046760559\n",
      "Epoch 840, Loss: 3.521198570728302, Final Batch Loss: 0.5979224443435669\n",
      "Epoch 841, Loss: 3.780117690563202, Final Batch Loss: 0.9642689824104309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 842, Loss: 3.0545461028814316, Final Batch Loss: 0.20335863530635834\n",
      "Epoch 843, Loss: 3.494328737258911, Final Batch Loss: 0.6059512495994568\n",
      "Epoch 844, Loss: 3.3752819299697876, Final Batch Loss: 0.5444170236587524\n",
      "Epoch 845, Loss: 4.550788581371307, Final Batch Loss: 1.6469533443450928\n",
      "Epoch 846, Loss: 4.171574115753174, Final Batch Loss: 1.260866641998291\n",
      "Epoch 847, Loss: 3.7873141765594482, Final Batch Loss: 0.9543063640594482\n",
      "Epoch 848, Loss: 3.8889005184173584, Final Batch Loss: 1.0582356452941895\n",
      "Epoch 849, Loss: 2.867929119616747, Final Batch Loss: 0.022853214293718338\n",
      "Epoch 850, Loss: 3.5889591574668884, Final Batch Loss: 0.63143390417099\n",
      "Epoch 851, Loss: 3.155915230512619, Final Batch Loss: 0.2588061988353729\n",
      "Epoch 852, Loss: 4.267593443393707, Final Batch Loss: 1.3931831121444702\n",
      "Epoch 853, Loss: 4.090645372867584, Final Batch Loss: 1.225710391998291\n",
      "Epoch 854, Loss: 4.001108527183533, Final Batch Loss: 1.048330545425415\n",
      "Epoch 855, Loss: 2.938773561269045, Final Batch Loss: 0.052007902413606644\n",
      "Epoch 856, Loss: 4.581381916999817, Final Batch Loss: 1.6365113258361816\n",
      "Epoch 857, Loss: 5.374340295791626, Final Batch Loss: 2.6199584007263184\n",
      "Epoch 858, Loss: 3.909143805503845, Final Batch Loss: 1.0941169261932373\n",
      "Epoch 859, Loss: 4.8683212995529175, Final Batch Loss: 2.1421735286712646\n",
      "Epoch 860, Loss: 2.785620192065835, Final Batch Loss: 0.02651790715754032\n",
      "Epoch 861, Loss: 3.765161097049713, Final Batch Loss: 0.9235166907310486\n",
      "Epoch 862, Loss: 3.6156631112098694, Final Batch Loss: 0.7537080645561218\n",
      "Epoch 863, Loss: 3.508091688156128, Final Batch Loss: 0.6494922637939453\n",
      "Epoch 864, Loss: 4.930348038673401, Final Batch Loss: 2.133289098739624\n",
      "Epoch 865, Loss: 2.821977549698204, Final Batch Loss: 0.006508344318717718\n",
      "Epoch 866, Loss: 2.917069086804986, Final Batch Loss: 0.018429825082421303\n",
      "Epoch 867, Loss: 3.086921900510788, Final Batch Loss: 0.23217013478279114\n",
      "Epoch 868, Loss: 2.7752756122499704, Final Batch Loss: 0.009519422426819801\n",
      "Epoch 869, Loss: 2.7879983820021152, Final Batch Loss: 0.027320925146341324\n",
      "Epoch 870, Loss: 3.0411964654922485, Final Batch Loss: 0.26318222284317017\n",
      "Epoch 871, Loss: 5.093716025352478, Final Batch Loss: 2.3670170307159424\n",
      "Epoch 872, Loss: 3.21072581410408, Final Batch Loss: 0.40262141823768616\n",
      "Epoch 873, Loss: 4.440452039241791, Final Batch Loss: 1.5859038829803467\n",
      "Epoch 874, Loss: 2.8599220742471516, Final Batch Loss: 0.004874133039265871\n",
      "Epoch 875, Loss: 4.320680379867554, Final Batch Loss: 1.5036205053329468\n",
      "Epoch 876, Loss: 3.35514497756958, Final Batch Loss: 0.47996896505355835\n",
      "Epoch 877, Loss: 2.9744395166635513, Final Batch Loss: 0.11430700123310089\n",
      "Epoch 878, Loss: 3.0007826387882233, Final Batch Loss: 0.21483245491981506\n",
      "Epoch 879, Loss: 4.517895400524139, Final Batch Loss: 1.7753058671951294\n",
      "Epoch 880, Loss: 2.80598296970129, Final Batch Loss: 0.04653247445821762\n",
      "Epoch 881, Loss: 3.039960741996765, Final Batch Loss: 0.21706873178482056\n",
      "Epoch 882, Loss: 3.2842951118946075, Final Batch Loss: 0.49430277943611145\n",
      "Epoch 883, Loss: 4.797169983386993, Final Batch Loss: 2.252962589263916\n",
      "Epoch 884, Loss: 3.7292619943618774, Final Batch Loss: 0.9643135666847229\n",
      "Epoch 885, Loss: 2.84308926993981, Final Batch Loss: 0.0011030309833586216\n",
      "Epoch 886, Loss: 4.421633005142212, Final Batch Loss: 1.5887517929077148\n",
      "Epoch 887, Loss: 4.263155281543732, Final Batch Loss: 1.3623192310333252\n",
      "Epoch 888, Loss: 3.726281762123108, Final Batch Loss: 0.8715628385543823\n",
      "Epoch 889, Loss: 3.0103545859456062, Final Batch Loss: 0.1233452633023262\n",
      "Epoch 890, Loss: 3.4707733392715454, Final Batch Loss: 0.6484759449958801\n",
      "Epoch 891, Loss: 4.787521064281464, Final Batch Loss: 2.0109636783599854\n",
      "Epoch 892, Loss: 2.8060745894908905, Final Batch Loss: 0.06761792302131653\n",
      "Epoch 893, Loss: 3.5564380884170532, Final Batch Loss: 0.6829211711883545\n",
      "Epoch 894, Loss: 4.378394603729248, Final Batch Loss: 1.409866213798523\n",
      "Epoch 895, Loss: 4.635949969291687, Final Batch Loss: 1.5929123163223267\n",
      "Epoch 896, Loss: 2.860806013457477, Final Batch Loss: 0.006867851130664349\n",
      "Epoch 897, Loss: 2.8836588487029076, Final Batch Loss: 0.09324922412633896\n",
      "Epoch 898, Loss: 2.8519472777843475, Final Batch Loss: 0.05172380805015564\n",
      "Epoch 899, Loss: 2.869840733706951, Final Batch Loss: 0.041281215846538544\n",
      "Epoch 900, Loss: 3.546254336833954, Final Batch Loss: 0.8636416792869568\n",
      "Epoch 901, Loss: 3.094182640314102, Final Batch Loss: 0.2519942820072174\n",
      "Epoch 902, Loss: 4.228353500366211, Final Batch Loss: 1.3779404163360596\n",
      "Epoch 903, Loss: 3.654591143131256, Final Batch Loss: 0.6564103364944458\n",
      "Epoch 904, Loss: 3.042877249419689, Final Batch Loss: 0.07963000982999802\n",
      "Epoch 905, Loss: 4.108884572982788, Final Batch Loss: 1.2325429916381836\n",
      "Epoch 906, Loss: 3.1955166161060333, Final Batch Loss: 0.3502139151096344\n",
      "Epoch 907, Loss: 4.977928340435028, Final Batch Loss: 2.1308207511901855\n",
      "Epoch 908, Loss: 3.7884300351142883, Final Batch Loss: 1.0354925394058228\n",
      "Epoch 909, Loss: 4.005235850811005, Final Batch Loss: 1.1872807741165161\n",
      "Epoch 910, Loss: 4.858838617801666, Final Batch Loss: 2.102621555328369\n",
      "Epoch 911, Loss: 3.88908189535141, Final Batch Loss: 1.2447540760040283\n",
      "Epoch 912, Loss: 3.9358023405075073, Final Batch Loss: 1.2329680919647217\n",
      "Epoch 913, Loss: 5.249737322330475, Final Batch Loss: 2.381495475769043\n",
      "Epoch 914, Loss: 3.3654420375823975, Final Batch Loss: 0.337565541267395\n",
      "Epoch 915, Loss: 3.1684886775910854, Final Batch Loss: 0.044938381761312485\n",
      "Epoch 916, Loss: 3.1509760916233063, Final Batch Loss: 0.13600173592567444\n",
      "Epoch 917, Loss: 3.885033071041107, Final Batch Loss: 1.1030364036560059\n",
      "Epoch 918, Loss: 3.505521595478058, Final Batch Loss: 0.6668450236320496\n",
      "Epoch 919, Loss: 2.8300910349935293, Final Batch Loss: 0.02357081137597561\n",
      "Epoch 920, Loss: 2.961641311645508, Final Batch Loss: 0.12924647331237793\n",
      "Epoch 921, Loss: 2.861355963163078, Final Batch Loss: 0.011198642663657665\n",
      "Epoch 922, Loss: 3.7114890813827515, Final Batch Loss: 0.8359506130218506\n",
      "Epoch 923, Loss: 3.0309992730617523, Final Batch Loss: 0.31134751439094543\n",
      "Epoch 924, Loss: 3.1284700632095337, Final Batch Loss: 0.4000397324562073\n",
      "Epoch 925, Loss: 4.61341118812561, Final Batch Loss: 1.7821887731552124\n",
      "Epoch 926, Loss: 3.8185564279556274, Final Batch Loss: 0.9613872170448303\n",
      "Epoch 927, Loss: 4.00227814912796, Final Batch Loss: 1.1778113842010498\n",
      "Epoch 928, Loss: 2.827820647507906, Final Batch Loss: 0.027114082127809525\n",
      "Epoch 929, Loss: 5.775289595127106, Final Batch Loss: 3.1147093772888184\n",
      "Epoch 930, Loss: 2.8537729382514954, Final Batch Loss: 0.09548455476760864\n",
      "Epoch 931, Loss: 3.7709019780158997, Final Batch Loss: 1.0311988592147827\n",
      "Epoch 932, Loss: 3.784727692604065, Final Batch Loss: 1.0193212032318115\n",
      "Epoch 933, Loss: 2.9934437572956085, Final Batch Loss: 0.273508757352829\n",
      "Epoch 934, Loss: 2.7568326145410538, Final Batch Loss: 0.03828926384449005\n",
      "Epoch 935, Loss: 2.900876708328724, Final Batch Loss: 0.09639669209718704\n",
      "Epoch 936, Loss: 2.8101419508457184, Final Batch Loss: 0.05459079146385193\n",
      "Epoch 937, Loss: 3.2203481197357178, Final Batch Loss: 0.47033387422561646\n",
      "Epoch 938, Loss: 2.9027677476406097, Final Batch Loss: 0.16774234175682068\n",
      "Epoch 939, Loss: 3.463459014892578, Final Batch Loss: 0.7179206013679504\n",
      "Epoch 940, Loss: 3.6078511476516724, Final Batch Loss: 0.9271655678749084\n",
      "Epoch 941, Loss: 5.661355793476105, Final Batch Loss: 2.9311537742614746\n",
      "Epoch 942, Loss: 4.045205116271973, Final Batch Loss: 1.292690634727478\n",
      "Epoch 943, Loss: 2.80194840580225, Final Batch Loss: 0.09139726310968399\n",
      "Epoch 944, Loss: 3.4057061076164246, Final Batch Loss: 0.7209203839302063\n",
      "Epoch 945, Loss: 2.870025634765625, Final Batch Loss: 0.04624295234680176\n",
      "Epoch 946, Loss: 4.683927655220032, Final Batch Loss: 1.9158352613449097\n",
      "Epoch 947, Loss: 2.9395513981580734, Final Batch Loss: 0.19345729053020477\n",
      "Epoch 948, Loss: 3.000475749373436, Final Batch Loss: 0.17643623054027557\n",
      "Epoch 949, Loss: 4.081351220607758, Final Batch Loss: 1.1239289045333862\n",
      "Epoch 950, Loss: 4.627152502536774, Final Batch Loss: 1.7398910522460938\n",
      "Epoch 951, Loss: 3.579926550388336, Final Batch Loss: 0.6413973569869995\n",
      "Epoch 952, Loss: 2.8113577868789434, Final Batch Loss: 0.02653961442410946\n",
      "Epoch 953, Loss: 4.704270422458649, Final Batch Loss: 2.012476921081543\n",
      "Epoch 954, Loss: 4.426666736602783, Final Batch Loss: 1.5286169052124023\n",
      "Epoch 955, Loss: 3.939834237098694, Final Batch Loss: 0.9063530564308167\n",
      "Epoch 956, Loss: 4.144002020359039, Final Batch Loss: 1.0032036304473877\n",
      "Epoch 957, Loss: 3.7118133902549744, Final Batch Loss: 0.5392904281616211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958, Loss: 3.2066579312086105, Final Batch Loss: 0.2493027299642563\n",
      "Epoch 959, Loss: 4.530634045600891, Final Batch Loss: 1.666626214981079\n",
      "Epoch 960, Loss: 2.968375889584422, Final Batch Loss: 0.021971849724650383\n",
      "Epoch 961, Loss: 2.8400266375392675, Final Batch Loss: 0.025928577408194542\n",
      "Epoch 962, Loss: 4.9903756976127625, Final Batch Loss: 2.2642264366149902\n",
      "Epoch 963, Loss: 2.93300661444664, Final Batch Loss: 0.288528174161911\n",
      "Epoch 964, Loss: 5.6598071455955505, Final Batch Loss: 2.8910467624664307\n",
      "Epoch 965, Loss: 2.8648619651794434, Final Batch Loss: 0.13899070024490356\n",
      "Epoch 966, Loss: 3.881665587425232, Final Batch Loss: 1.104422926902771\n",
      "Epoch 967, Loss: 3.1408873200416565, Final Batch Loss: 0.5032818913459778\n",
      "Epoch 968, Loss: 3.0987549871206284, Final Batch Loss: 0.2435959428548813\n",
      "Epoch 969, Loss: 2.721512481570244, Final Batch Loss: 0.02839551866054535\n",
      "Epoch 970, Loss: 4.418718039989471, Final Batch Loss: 1.6498992443084717\n",
      "Epoch 971, Loss: 2.9648245871067047, Final Batch Loss: 0.254462331533432\n",
      "Epoch 972, Loss: 5.251572608947754, Final Batch Loss: 2.5063655376434326\n",
      "Epoch 973, Loss: 3.089409112930298, Final Batch Loss: 0.4006229639053345\n",
      "Epoch 974, Loss: 3.1738579273223877, Final Batch Loss: 0.4061412811279297\n",
      "Epoch 975, Loss: 5.5409093499183655, Final Batch Loss: 2.9024481773376465\n",
      "Epoch 976, Loss: 3.4112271070480347, Final Batch Loss: 0.7324908375740051\n",
      "Epoch 977, Loss: 4.503969132900238, Final Batch Loss: 1.7638590335845947\n",
      "Epoch 978, Loss: 2.7812107279896736, Final Batch Loss: 0.020630069077014923\n",
      "Epoch 979, Loss: 3.9729679226875305, Final Batch Loss: 1.2481441497802734\n",
      "Epoch 980, Loss: 4.437152564525604, Final Batch Loss: 1.6288669109344482\n",
      "Epoch 981, Loss: 4.7615363001823425, Final Batch Loss: 1.96806800365448\n",
      "Epoch 982, Loss: 2.819963902235031, Final Batch Loss: 0.11954662203788757\n",
      "Epoch 983, Loss: 2.806117907166481, Final Batch Loss: 0.06753112375736237\n",
      "Epoch 984, Loss: 5.153942286968231, Final Batch Loss: 2.371762990951538\n",
      "Epoch 985, Loss: 2.870357856154442, Final Batch Loss: 0.13589154183864594\n",
      "Epoch 986, Loss: 3.282245457172394, Final Batch Loss: 0.4782733917236328\n",
      "Epoch 987, Loss: 4.79022616147995, Final Batch Loss: 2.017972230911255\n",
      "Epoch 988, Loss: 4.504890084266663, Final Batch Loss: 1.8057199716567993\n",
      "Epoch 989, Loss: 3.1551678478717804, Final Batch Loss: 0.4337175190448761\n",
      "Epoch 990, Loss: 3.005579650402069, Final Batch Loss: 0.3595145344734192\n",
      "Epoch 991, Loss: 2.9582244157791138, Final Batch Loss: 0.30341941118240356\n",
      "Epoch 992, Loss: 3.034473478794098, Final Batch Loss: 0.34288281202316284\n",
      "Epoch 993, Loss: 4.467832565307617, Final Batch Loss: 1.7531349658966064\n",
      "Epoch 994, Loss: 3.2395692467689514, Final Batch Loss: 0.5083636045455933\n",
      "Epoch 995, Loss: 2.7087078355252743, Final Batch Loss: 0.045996155589818954\n",
      "Epoch 996, Loss: 4.378738701343536, Final Batch Loss: 1.6447813510894775\n",
      "Epoch 997, Loss: 3.9490354657173157, Final Batch Loss: 1.4035638570785522\n",
      "Epoch 998, Loss: 2.8940508663654327, Final Batch Loss: 0.3063142001628876\n",
      "Epoch 999, Loss: 2.810516506433487, Final Batch Loss: 0.1880941092967987\n",
      "Epoch 1000, Loss: 3.7536689043045044, Final Batch Loss: 1.0808613300323486\n",
      "Epoch 1001, Loss: 2.6664958144538105, Final Batch Loss: 0.005194738041609526\n",
      "Epoch 1002, Loss: 4.811059772968292, Final Batch Loss: 2.193429708480835\n",
      "Epoch 1003, Loss: 4.660244941711426, Final Batch Loss: 1.9449070692062378\n",
      "Epoch 1004, Loss: 3.6147764325141907, Final Batch Loss: 0.877295196056366\n",
      "Epoch 1005, Loss: 3.494230628013611, Final Batch Loss: 0.68004310131073\n",
      "Epoch 1006, Loss: 5.168137788772583, Final Batch Loss: 2.2928152084350586\n",
      "Epoch 1007, Loss: 3.2440633475780487, Final Batch Loss: 0.48058709502220154\n",
      "Epoch 1008, Loss: 4.739704728126526, Final Batch Loss: 1.8729013204574585\n",
      "Epoch 1009, Loss: 3.989090085029602, Final Batch Loss: 1.2055017948150635\n",
      "Epoch 1010, Loss: 3.484939455986023, Final Batch Loss: 0.7553715705871582\n",
      "Epoch 1011, Loss: 4.477806210517883, Final Batch Loss: 1.687941074371338\n",
      "Epoch 1012, Loss: 4.21998393535614, Final Batch Loss: 1.3774906396865845\n",
      "Epoch 1013, Loss: 4.977881193161011, Final Batch Loss: 2.020688056945801\n",
      "Epoch 1014, Loss: 4.246155917644501, Final Batch Loss: 1.2506303787231445\n",
      "Epoch 1015, Loss: 3.9603787064552307, Final Batch Loss: 1.1184139251708984\n",
      "Epoch 1016, Loss: 5.094609022140503, Final Batch Loss: 2.2511613368988037\n",
      "Epoch 1017, Loss: 8.768351972103119, Final Batch Loss: 6.091014862060547\n",
      "Epoch 1018, Loss: 4.2352060079574585, Final Batch Loss: 1.5356487035751343\n",
      "Epoch 1019, Loss: 4.131252884864807, Final Batch Loss: 1.3933632373809814\n",
      "Epoch 1020, Loss: 3.517603278160095, Final Batch Loss: 0.7063331007957458\n",
      "Epoch 1021, Loss: 2.9150423109531403, Final Batch Loss: 0.1596263349056244\n",
      "Epoch 1022, Loss: 2.99615778028965, Final Batch Loss: 0.24391646683216095\n",
      "Epoch 1023, Loss: 3.1236546635627747, Final Batch Loss: 0.3627220392227173\n",
      "Epoch 1024, Loss: 2.9198089987039566, Final Batch Loss: 0.17066217958927155\n",
      "Epoch 1025, Loss: 4.639722168445587, Final Batch Loss: 1.9450008869171143\n",
      "Epoch 1026, Loss: 3.7409024834632874, Final Batch Loss: 1.0627689361572266\n",
      "Epoch 1027, Loss: 4.663387894630432, Final Batch Loss: 2.0712685585021973\n",
      "Epoch 1028, Loss: 2.785819411277771, Final Batch Loss: 0.07327979803085327\n",
      "Epoch 1029, Loss: 3.7269362807273865, Final Batch Loss: 1.0597615242004395\n",
      "Epoch 1030, Loss: 4.7341830134391785, Final Batch Loss: 2.0819966793060303\n",
      "Epoch 1031, Loss: 4.4325408935546875, Final Batch Loss: 1.6752333641052246\n",
      "Epoch 1032, Loss: 2.915251910686493, Final Batch Loss: 0.10154896974563599\n",
      "Epoch 1033, Loss: 6.2089338302612305, Final Batch Loss: 3.3429768085479736\n",
      "Epoch 1034, Loss: 2.833188071846962, Final Batch Loss: 0.08483095467090607\n",
      "Epoch 1035, Loss: 3.3717561960220337, Final Batch Loss: 0.7218491435050964\n",
      "Epoch 1036, Loss: 2.907821446657181, Final Batch Loss: 0.20978829264640808\n",
      "Epoch 1037, Loss: 4.216777741909027, Final Batch Loss: 1.4881367683410645\n",
      "Epoch 1038, Loss: 2.8594658533111215, Final Batch Loss: 0.013530687429010868\n",
      "Epoch 1039, Loss: 4.237679898738861, Final Batch Loss: 1.436464786529541\n",
      "Epoch 1040, Loss: 4.129731774330139, Final Batch Loss: 1.3701865673065186\n",
      "Epoch 1041, Loss: 3.0730701684951782, Final Batch Loss: 0.35830092430114746\n",
      "Epoch 1042, Loss: 3.9550870060920715, Final Batch Loss: 1.2776033878326416\n",
      "Epoch 1043, Loss: 4.343175768852234, Final Batch Loss: 1.6805827617645264\n",
      "Epoch 1044, Loss: 2.831422507762909, Final Batch Loss: 0.13924193382263184\n",
      "Epoch 1045, Loss: 3.9315784573554993, Final Batch Loss: 1.305446743965149\n",
      "Epoch 1046, Loss: 2.7089961022138596, Final Batch Loss: 0.07819788157939911\n",
      "Epoch 1047, Loss: 2.959409713745117, Final Batch Loss: 0.33518093824386597\n",
      "Epoch 1048, Loss: 4.67436558008194, Final Batch Loss: 2.12418270111084\n",
      "Epoch 1049, Loss: 3.108969509601593, Final Batch Loss: 0.501070499420166\n",
      "Epoch 1050, Loss: 3.4872626066207886, Final Batch Loss: 0.8221649527549744\n",
      "Epoch 1051, Loss: 2.7991111446172, Final Batch Loss: 0.02144821174442768\n",
      "Epoch 1052, Loss: 3.95884370803833, Final Batch Loss: 1.2792108058929443\n",
      "Epoch 1053, Loss: 2.630961725488305, Final Batch Loss: 0.021961936727166176\n",
      "Epoch 1054, Loss: 5.441860020160675, Final Batch Loss: 2.8177552223205566\n",
      "Epoch 1055, Loss: 2.7219367027282715, Final Batch Loss: 0.10551172494888306\n",
      "Epoch 1056, Loss: 2.8469332456588745, Final Batch Loss: 0.20086008310317993\n",
      "Epoch 1057, Loss: 3.689478278160095, Final Batch Loss: 1.015493392944336\n",
      "Epoch 1058, Loss: 2.752994626760483, Final Batch Loss: 0.1434374749660492\n",
      "Epoch 1059, Loss: 3.8649396896362305, Final Batch Loss: 1.1218372583389282\n",
      "Epoch 1060, Loss: 5.082694351673126, Final Batch Loss: 2.397275447845459\n",
      "Epoch 1061, Loss: 2.8638264536857605, Final Batch Loss: 0.27510470151901245\n",
      "Epoch 1062, Loss: 2.8308709263801575, Final Batch Loss: 0.15566134452819824\n",
      "Epoch 1063, Loss: 3.570390284061432, Final Batch Loss: 0.962192177772522\n",
      "Epoch 1064, Loss: 3.2385255694389343, Final Batch Loss: 0.6666564345359802\n",
      "Epoch 1065, Loss: 3.2191932797431946, Final Batch Loss: 0.5317851305007935\n",
      "Epoch 1066, Loss: 2.8878824412822723, Final Batch Loss: 0.3567334711551666\n",
      "Epoch 1067, Loss: 2.9016517400741577, Final Batch Loss: 0.2671610713005066\n",
      "Epoch 1068, Loss: 2.7130854427814484, Final Batch Loss: 0.19494041800498962\n",
      "Epoch 1069, Loss: 2.770048603415489, Final Batch Loss: 0.15977053344249725\n",
      "Epoch 1070, Loss: 3.6954562067985535, Final Batch Loss: 1.0376951694488525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1071, Loss: 3.268916606903076, Final Batch Loss: 0.6398210525512695\n",
      "Epoch 1072, Loss: 3.828421890735626, Final Batch Loss: 0.9750008583068848\n",
      "Epoch 1073, Loss: 3.1063064485788345, Final Batch Loss: 0.17251278460025787\n",
      "Epoch 1074, Loss: 4.584186792373657, Final Batch Loss: 1.705440640449524\n",
      "Epoch 1075, Loss: 4.401472866535187, Final Batch Loss: 1.4987678527832031\n",
      "Epoch 1076, Loss: 3.810866892337799, Final Batch Loss: 1.167781114578247\n",
      "Epoch 1077, Loss: 2.743650991935283, Final Batch Loss: 0.005362768191844225\n",
      "Epoch 1078, Loss: 4.744516551494598, Final Batch Loss: 2.0059497356414795\n",
      "Epoch 1079, Loss: 3.9768389463424683, Final Batch Loss: 1.0873874425888062\n",
      "Epoch 1080, Loss: 2.9897509668953717, Final Batch Loss: 0.002372190821915865\n",
      "Epoch 1081, Loss: 4.694511890411377, Final Batch Loss: 1.8098270893096924\n",
      "Epoch 1082, Loss: 3.6937058568000793, Final Batch Loss: 0.9145692586898804\n",
      "Epoch 1083, Loss: 4.280526399612427, Final Batch Loss: 1.6352330446243286\n",
      "Epoch 1084, Loss: 3.6542824506759644, Final Batch Loss: 0.9311285018920898\n",
      "Epoch 1085, Loss: 2.7103939205408096, Final Batch Loss: 0.020393922924995422\n",
      "Epoch 1086, Loss: 2.789559558033943, Final Batch Loss: 0.10012127459049225\n",
      "Epoch 1087, Loss: 3.264769971370697, Final Batch Loss: 0.6158847212791443\n",
      "Epoch 1088, Loss: 4.19003963470459, Final Batch Loss: 1.58335542678833\n",
      "Epoch 1089, Loss: 2.869998663663864, Final Batch Loss: 0.2033359706401825\n",
      "Epoch 1090, Loss: 3.017510086297989, Final Batch Loss: 0.3611240088939667\n",
      "Epoch 1091, Loss: 2.7845067530870438, Final Batch Loss: 0.22504384815692902\n",
      "Epoch 1092, Loss: 2.802693262696266, Final Batch Loss: 0.18279196321964264\n",
      "Epoch 1093, Loss: 2.910487949848175, Final Batch Loss: 0.2919812798500061\n",
      "Epoch 1094, Loss: 2.760477900505066, Final Batch Loss: 0.12355893850326538\n",
      "Epoch 1095, Loss: 3.021415650844574, Final Batch Loss: 0.4042053818702698\n",
      "Epoch 1096, Loss: 3.0903390645980835, Final Batch Loss: 0.5036987066268921\n",
      "Epoch 1097, Loss: 4.038342714309692, Final Batch Loss: 1.4677860736846924\n",
      "Epoch 1098, Loss: 2.490481021552114, Final Batch Loss: 0.00048303857329301536\n",
      "Epoch 1099, Loss: 4.52163553237915, Final Batch Loss: 1.8305414915084839\n",
      "Epoch 1100, Loss: 3.1434696912765503, Final Batch Loss: 0.49117445945739746\n",
      "Epoch 1101, Loss: 3.52904212474823, Final Batch Loss: 0.8359078168869019\n",
      "Epoch 1102, Loss: 3.3906217217445374, Final Batch Loss: 0.819517195224762\n",
      "Epoch 1103, Loss: 4.959349811077118, Final Batch Loss: 2.3585026264190674\n",
      "Epoch 1104, Loss: 2.6438372805714607, Final Batch Loss: 0.10680747777223587\n",
      "Epoch 1105, Loss: 2.736391596496105, Final Batch Loss: 0.1049644872546196\n",
      "Epoch 1106, Loss: 3.4023037552833557, Final Batch Loss: 0.8267753720283508\n",
      "Epoch 1107, Loss: 5.9250563979148865, Final Batch Loss: 3.331784725189209\n",
      "Epoch 1108, Loss: 4.493981659412384, Final Batch Loss: 1.942561149597168\n",
      "Epoch 1109, Loss: 4.13194614648819, Final Batch Loss: 1.4636030197143555\n",
      "Epoch 1110, Loss: 2.6779146641492844, Final Batch Loss: 0.22712473571300507\n",
      "Epoch 1111, Loss: 5.1865933537483215, Final Batch Loss: 2.663815498352051\n",
      "Epoch 1112, Loss: 2.8946997225284576, Final Batch Loss: 0.39765027165412903\n",
      "Epoch 1113, Loss: 3.267825663089752, Final Batch Loss: 0.8170570135116577\n",
      "Epoch 1114, Loss: 2.8156566470861435, Final Batch Loss: 0.20312194526195526\n",
      "Epoch 1115, Loss: 2.7183673083782196, Final Batch Loss: 0.1680898368358612\n",
      "Epoch 1116, Loss: 3.844829797744751, Final Batch Loss: 1.3659356832504272\n",
      "Epoch 1117, Loss: 2.6209170818328857, Final Batch Loss: 0.09345662593841553\n",
      "Epoch 1118, Loss: 3.2117327451705933, Final Batch Loss: 0.6068341135978699\n",
      "Epoch 1119, Loss: 2.619130168110132, Final Batch Loss: 0.03565547242760658\n",
      "Epoch 1120, Loss: 4.194364964962006, Final Batch Loss: 1.547773838043213\n",
      "Epoch 1121, Loss: 4.743837893009186, Final Batch Loss: 2.184854745864868\n",
      "Epoch 1122, Loss: 2.6115262401872315, Final Batch Loss: 0.0005096090608276427\n",
      "Epoch 1123, Loss: 3.4834010004997253, Final Batch Loss: 0.7636806964874268\n",
      "Epoch 1124, Loss: 3.826555013656616, Final Batch Loss: 0.9461145997047424\n",
      "Epoch 1125, Loss: 2.9448795914649963, Final Batch Loss: 0.09267622232437134\n",
      "Epoch 1126, Loss: 4.4186853766441345, Final Batch Loss: 1.6018426418304443\n",
      "Epoch 1127, Loss: 2.8344205617904663, Final Batch Loss: 0.1109691858291626\n",
      "Epoch 1128, Loss: 3.278969883918762, Final Batch Loss: 0.6225136518478394\n",
      "Epoch 1129, Loss: 3.027910351753235, Final Batch Loss: 0.40090322494506836\n",
      "Epoch 1130, Loss: 6.056607007980347, Final Batch Loss: 3.526289224624634\n",
      "Epoch 1131, Loss: 3.6678289771080017, Final Batch Loss: 1.0476373434066772\n",
      "Epoch 1132, Loss: 3.761061429977417, Final Batch Loss: 1.0410369634628296\n",
      "Epoch 1133, Loss: 3.5780600905418396, Final Batch Loss: 0.9307258725166321\n",
      "Epoch 1134, Loss: 2.837589591741562, Final Batch Loss: 0.21877822279930115\n",
      "Epoch 1135, Loss: 2.5962039383593947, Final Batch Loss: 0.002148345345631242\n",
      "Epoch 1136, Loss: 4.609501898288727, Final Batch Loss: 2.032952308654785\n",
      "Epoch 1137, Loss: 3.4172046780586243, Final Batch Loss: 0.8615127205848694\n",
      "Epoch 1138, Loss: 3.322842240333557, Final Batch Loss: 0.7269969582557678\n",
      "Epoch 1139, Loss: 3.093760907649994, Final Batch Loss: 0.4625079035758972\n",
      "Epoch 1140, Loss: 3.183706045150757, Final Batch Loss: 0.5623986721038818\n",
      "Epoch 1141, Loss: 3.2685993909835815, Final Batch Loss: 0.5145501494407654\n",
      "Epoch 1142, Loss: 2.631236999295652, Final Batch Loss: 0.015544562600553036\n",
      "Epoch 1143, Loss: 3.3269723057746887, Final Batch Loss: 0.7661693692207336\n",
      "Epoch 1144, Loss: 2.5462181214243174, Final Batch Loss: 0.024978185072541237\n",
      "Epoch 1145, Loss: 2.7426221072673798, Final Batch Loss: 0.22586259245872498\n",
      "Epoch 1146, Loss: 4.042526364326477, Final Batch Loss: 1.600182294845581\n",
      "Epoch 1147, Loss: 3.330533504486084, Final Batch Loss: 0.8311530947685242\n",
      "Epoch 1148, Loss: 2.563622400164604, Final Batch Loss: 0.03001062572002411\n",
      "Epoch 1149, Loss: 3.1506864428520203, Final Batch Loss: 0.6012923717498779\n",
      "Epoch 1150, Loss: 5.373891115188599, Final Batch Loss: 2.735199451446533\n",
      "Epoch 1151, Loss: 2.7954253405332565, Final Batch Loss: 0.15108107030391693\n",
      "Epoch 1152, Loss: 3.9604076743125916, Final Batch Loss: 1.2943205833435059\n",
      "Epoch 1153, Loss: 3.976793110370636, Final Batch Loss: 1.4845699071884155\n",
      "Epoch 1154, Loss: 3.485568881034851, Final Batch Loss: 0.7149257659912109\n",
      "Epoch 1155, Loss: 2.7646521599963307, Final Batch Loss: 0.012209204025566578\n",
      "Epoch 1156, Loss: 3.0264095664024353, Final Batch Loss: 0.3324575424194336\n",
      "Epoch 1157, Loss: 4.445443272590637, Final Batch Loss: 1.8370388746261597\n",
      "Epoch 1158, Loss: 3.114964485168457, Final Batch Loss: 0.5343500971794128\n",
      "Epoch 1159, Loss: 2.6029857425019145, Final Batch Loss: 0.010189655236899853\n",
      "Epoch 1160, Loss: 2.649789769202471, Final Batch Loss: 0.05387004092335701\n",
      "Epoch 1161, Loss: 2.8716636896133423, Final Batch Loss: 0.329375684261322\n",
      "Epoch 1162, Loss: 3.8029330372810364, Final Batch Loss: 1.2813973426818848\n",
      "Epoch 1163, Loss: 3.4255520701408386, Final Batch Loss: 0.8639285564422607\n",
      "Epoch 1164, Loss: 4.395476877689362, Final Batch Loss: 1.8604756593704224\n",
      "Epoch 1165, Loss: 2.685092031955719, Final Batch Loss: 0.14196103811264038\n",
      "Epoch 1166, Loss: 3.330484390258789, Final Batch Loss: 0.8375641703605652\n",
      "Epoch 1167, Loss: 2.8336711823940277, Final Batch Loss: 0.40211614966392517\n",
      "Epoch 1168, Loss: 4.124733507633209, Final Batch Loss: 1.544987678527832\n",
      "Epoch 1169, Loss: 3.0609370470046997, Final Batch Loss: 0.4482737183570862\n",
      "Epoch 1170, Loss: 4.78593635559082, Final Batch Loss: 1.9759860038757324\n",
      "Epoch 1171, Loss: 4.425841689109802, Final Batch Loss: 1.4785292148590088\n",
      "Epoch 1172, Loss: 4.724084377288818, Final Batch Loss: 1.7783128023147583\n",
      "Epoch 1173, Loss: 3.056894913315773, Final Batch Loss: 0.23191775381565094\n",
      "Epoch 1174, Loss: 2.705265307566151, Final Batch Loss: 0.0013188959565013647\n",
      "Epoch 1175, Loss: 3.771833121776581, Final Batch Loss: 1.0327122211456299\n",
      "Epoch 1176, Loss: 4.143941044807434, Final Batch Loss: 1.5875365734100342\n",
      "Epoch 1177, Loss: 3.8676218390464783, Final Batch Loss: 1.2188665866851807\n",
      "Epoch 1178, Loss: 4.657448470592499, Final Batch Loss: 2.1298601627349854\n",
      "Epoch 1179, Loss: 2.523655779659748, Final Batch Loss: 0.07888699322938919\n",
      "Epoch 1180, Loss: 2.659773763269186, Final Batch Loss: 0.04073863849043846\n",
      "Epoch 1181, Loss: 2.7699414789676666, Final Batch Loss: 0.029147475957870483\n",
      "Epoch 1182, Loss: 3.371362268924713, Final Batch Loss: 0.5042250752449036\n",
      "Epoch 1183, Loss: 3.372920274734497, Final Batch Loss: 0.46970921754837036\n",
      "Epoch 1184, Loss: 4.117004811763763, Final Batch Loss: 1.5735121965408325\n",
      "Epoch 1185, Loss: 4.392805874347687, Final Batch Loss: 1.9177883863449097\n",
      "Epoch 1186, Loss: 3.539025068283081, Final Batch Loss: 1.0203042030334473\n",
      "Epoch 1187, Loss: 2.694844573736191, Final Batch Loss: 0.19787630438804626\n",
      "Epoch 1188, Loss: 2.833052381873131, Final Batch Loss: 0.19051013886928558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1189, Loss: 2.828086197376251, Final Batch Loss: 0.14064419269561768\n",
      "Epoch 1190, Loss: 3.3643609285354614, Final Batch Loss: 0.8243544101715088\n",
      "Epoch 1191, Loss: 2.5485523119568825, Final Batch Loss: 0.0644320622086525\n",
      "Epoch 1192, Loss: 2.498950468376279, Final Batch Loss: 0.023105906322598457\n",
      "Epoch 1193, Loss: 2.51838258234784, Final Batch Loss: 0.003095123451203108\n",
      "Epoch 1194, Loss: 2.6657223515212536, Final Batch Loss: 0.04207555577158928\n",
      "Epoch 1195, Loss: 2.417239189147949, Final Batch Loss: 0.01752007007598877\n",
      "Epoch 1196, Loss: 2.516035782173276, Final Batch Loss: 0.025395618751645088\n",
      "Epoch 1197, Loss: 5.358885228633881, Final Batch Loss: 2.832484006881714\n",
      "Epoch 1198, Loss: 2.746062085032463, Final Batch Loss: 0.07030753791332245\n",
      "Epoch 1199, Loss: 3.4954617023468018, Final Batch Loss: 0.5917540788650513\n",
      "Epoch 1200, Loss: 3.1946147978305817, Final Batch Loss: 0.260466068983078\n",
      "Epoch 1201, Loss: 3.6581867337226868, Final Batch Loss: 0.7282465696334839\n",
      "Epoch 1202, Loss: 4.777942478656769, Final Batch Loss: 2.0647449493408203\n",
      "Epoch 1203, Loss: 2.8434121906757355, Final Batch Loss: 0.2340439260005951\n",
      "Epoch 1204, Loss: 6.493511438369751, Final Batch Loss: 3.8730757236480713\n",
      "Epoch 1205, Loss: 4.374395906925201, Final Batch Loss: 1.7634586095809937\n",
      "Epoch 1206, Loss: 2.8267133980989456, Final Batch Loss: 0.14046676456928253\n",
      "Epoch 1207, Loss: 6.762818157672882, Final Batch Loss: 4.030194282531738\n",
      "Epoch 1208, Loss: 2.952157974243164, Final Batch Loss: 0.12024176120758057\n",
      "Epoch 1209, Loss: 3.0241150110960007, Final Batch Loss: 0.10874716937541962\n",
      "Epoch 1210, Loss: 3.371239125728607, Final Batch Loss: 0.44852155447006226\n",
      "Epoch 1211, Loss: 2.7979358211159706, Final Batch Loss: 0.04826239496469498\n",
      "Epoch 1212, Loss: 2.8421389013528824, Final Batch Loss: 0.09754796326160431\n",
      "Epoch 1213, Loss: 2.5312046222388744, Final Batch Loss: 0.008334379643201828\n",
      "Epoch 1214, Loss: 4.707608640193939, Final Batch Loss: 2.1035561561584473\n",
      "Epoch 1215, Loss: 3.1780986189842224, Final Batch Loss: 0.4881839156150818\n",
      "Epoch 1216, Loss: 3.064765214920044, Final Batch Loss: 0.4724288582801819\n",
      "Epoch 1217, Loss: 3.834864318370819, Final Batch Loss: 1.3122458457946777\n",
      "Epoch 1218, Loss: 2.605252366978675, Final Batch Loss: 0.005223792511969805\n",
      "Epoch 1219, Loss: 4.08873051404953, Final Batch Loss: 1.4719533920288086\n",
      "Epoch 1220, Loss: 4.4443793296813965, Final Batch Loss: 1.8666702508926392\n",
      "Epoch 1221, Loss: 4.622937440872192, Final Batch Loss: 2.0882818698883057\n",
      "Epoch 1222, Loss: 3.4255564212799072, Final Batch Loss: 0.8456538319587708\n",
      "Epoch 1223, Loss: 4.236010134220123, Final Batch Loss: 1.6358344554901123\n",
      "Epoch 1224, Loss: 5.287811458110809, Final Batch Loss: 2.643981695175171\n",
      "Epoch 1225, Loss: 2.627503853291273, Final Batch Loss: 0.022206109017133713\n",
      "Epoch 1226, Loss: 4.553730487823486, Final Batch Loss: 1.9796582460403442\n",
      "Epoch 1227, Loss: 3.2013628482818604, Final Batch Loss: 0.45970308780670166\n",
      "Epoch 1228, Loss: 2.6397379329428077, Final Batch Loss: 0.0033547570928931236\n",
      "Epoch 1229, Loss: 4.646317422389984, Final Batch Loss: 1.8157542943954468\n",
      "Epoch 1230, Loss: 3.167535722255707, Final Batch Loss: 0.5414333343505859\n",
      "Epoch 1231, Loss: 4.332521438598633, Final Batch Loss: 1.8398346900939941\n",
      "Epoch 1232, Loss: 3.7575567960739136, Final Batch Loss: 1.044319987297058\n",
      "Epoch 1233, Loss: 2.9025583416223526, Final Batch Loss: 0.13563551008701324\n",
      "Epoch 1234, Loss: 3.974041521549225, Final Batch Loss: 1.2625696659088135\n",
      "Epoch 1235, Loss: 2.795621410012245, Final Batch Loss: 0.040528371930122375\n",
      "Epoch 1236, Loss: 3.769108235836029, Final Batch Loss: 0.8901789784431458\n",
      "Epoch 1237, Loss: 4.475395560264587, Final Batch Loss: 1.713053584098816\n",
      "Epoch 1238, Loss: 3.118430733680725, Final Batch Loss: 0.5038030743598938\n",
      "Epoch 1239, Loss: 2.9409234523773193, Final Batch Loss: 0.30711227655410767\n",
      "Epoch 1240, Loss: 2.845617115497589, Final Batch Loss: 0.22880595922470093\n",
      "Epoch 1241, Loss: 3.3958036303520203, Final Batch Loss: 0.8031635284423828\n",
      "Epoch 1242, Loss: 3.9540905952453613, Final Batch Loss: 1.2895957231521606\n",
      "Epoch 1243, Loss: 4.696885645389557, Final Batch Loss: 2.0959792137145996\n",
      "Epoch 1244, Loss: 3.7640827298164368, Final Batch Loss: 0.9971054196357727\n",
      "Epoch 1245, Loss: 4.5288673639297485, Final Batch Loss: 1.9826443195343018\n",
      "Epoch 1246, Loss: 2.721536822617054, Final Batch Loss: 0.09860480576753616\n",
      "Epoch 1247, Loss: 2.6750820130109787, Final Batch Loss: 0.11453686654567719\n",
      "Epoch 1248, Loss: 3.246480882167816, Final Batch Loss: 0.5985548496246338\n",
      "Epoch 1249, Loss: 4.2560306787490845, Final Batch Loss: 1.6317249536514282\n",
      "Epoch 1250, Loss: 2.5975202238187194, Final Batch Loss: 0.0023443615064024925\n",
      "Epoch 1251, Loss: 2.923859655857086, Final Batch Loss: 0.47759485244750977\n",
      "Epoch 1252, Loss: 2.6266380324959755, Final Batch Loss: 0.07031253725290298\n",
      "Epoch 1253, Loss: 3.42002010345459, Final Batch Loss: 0.8992921113967896\n",
      "Epoch 1254, Loss: 3.2885839343070984, Final Batch Loss: 0.8329005837440491\n",
      "Epoch 1255, Loss: 2.5854144223267213, Final Batch Loss: 0.0012343652779236436\n",
      "Epoch 1256, Loss: 2.5645727906376123, Final Batch Loss: 0.017450252547860146\n",
      "Epoch 1257, Loss: 2.749166861176491, Final Batch Loss: 0.10104061663150787\n",
      "Epoch 1258, Loss: 4.561070144176483, Final Batch Loss: 1.9919641017913818\n",
      "Epoch 1259, Loss: 2.5947364740713965, Final Batch Loss: 0.00029118589009158313\n",
      "Epoch 1260, Loss: 4.812897801399231, Final Batch Loss: 2.2059154510498047\n",
      "Epoch 1261, Loss: 2.7693022936582565, Final Batch Loss: 0.22069381177425385\n",
      "Epoch 1262, Loss: 2.4784092530608177, Final Batch Loss: 0.06311162561178207\n",
      "Epoch 1263, Loss: 2.831163704395294, Final Batch Loss: 0.2911031246185303\n",
      "Epoch 1264, Loss: 2.892082542181015, Final Batch Loss: 0.3439299166202545\n",
      "Epoch 1265, Loss: 2.6581408828496933, Final Batch Loss: 0.15153716504573822\n",
      "Epoch 1266, Loss: 2.8985036313533783, Final Batch Loss: 0.38133159279823303\n",
      "Epoch 1267, Loss: 2.7954548597335815, Final Batch Loss: 0.34645092487335205\n",
      "Epoch 1268, Loss: 4.29921942949295, Final Batch Loss: 1.893876314163208\n",
      "Epoch 1269, Loss: 4.357579469680786, Final Batch Loss: 1.8840346336364746\n",
      "Epoch 1270, Loss: 2.7800168693065643, Final Batch Loss: 0.24740371108055115\n",
      "Epoch 1271, Loss: 6.055385649204254, Final Batch Loss: 3.625462532043457\n",
      "Epoch 1272, Loss: 2.669547215104103, Final Batch Loss: 0.18704690039157867\n",
      "Epoch 1273, Loss: 2.6493502259254456, Final Batch Loss: 0.16770070791244507\n",
      "Epoch 1274, Loss: 4.305675983428955, Final Batch Loss: 1.66278076171875\n",
      "Epoch 1275, Loss: 2.538690038025379, Final Batch Loss: 0.04261375218629837\n",
      "Epoch 1276, Loss: 2.5528519973158836, Final Batch Loss: 0.07822819799184799\n",
      "Epoch 1277, Loss: 3.8739944100379944, Final Batch Loss: 1.3494195938110352\n",
      "Epoch 1278, Loss: 3.301081657409668, Final Batch Loss: 0.8100877404212952\n",
      "Epoch 1279, Loss: 3.684412956237793, Final Batch Loss: 1.0445929765701294\n",
      "Epoch 1280, Loss: 3.7971338033676147, Final Batch Loss: 0.985602617263794\n",
      "Epoch 1281, Loss: 3.2970788776874542, Final Batch Loss: 0.4883817136287689\n",
      "Epoch 1282, Loss: 3.895496129989624, Final Batch Loss: 1.0644869804382324\n",
      "Epoch 1283, Loss: 3.2250747084617615, Final Batch Loss: 0.5995510220527649\n",
      "Epoch 1284, Loss: 2.653857685625553, Final Batch Loss: 0.0012143626809120178\n",
      "Epoch 1285, Loss: 2.876924604177475, Final Batch Loss: 0.2522856295108795\n",
      "Epoch 1286, Loss: 2.665615694480948, Final Batch Loss: 0.0011302995262667537\n",
      "Epoch 1287, Loss: 3.1734426021575928, Final Batch Loss: 0.6540158987045288\n",
      "Epoch 1288, Loss: 2.6366944797337055, Final Batch Loss: 0.05575161054730415\n",
      "Epoch 1289, Loss: 2.745137393474579, Final Batch Loss: 0.16884881258010864\n",
      "Epoch 1290, Loss: 2.6226357221603394, Final Batch Loss: 0.06835377216339111\n",
      "Epoch 1291, Loss: 2.4674244076013565, Final Batch Loss: 0.0346498042345047\n",
      "Epoch 1292, Loss: 4.353303670883179, Final Batch Loss: 1.7926028966903687\n",
      "Epoch 1293, Loss: 2.9433118402957916, Final Batch Loss: 0.42870429158210754\n",
      "Epoch 1294, Loss: 3.396395206451416, Final Batch Loss: 0.9956360459327698\n",
      "Epoch 1295, Loss: 2.5853595547378063, Final Batch Loss: 0.01853035017848015\n",
      "Epoch 1296, Loss: 3.1707608103752136, Final Batch Loss: 0.5867459774017334\n",
      "Epoch 1297, Loss: 4.815349280834198, Final Batch Loss: 2.168621063232422\n",
      "Epoch 1298, Loss: 2.8276475965976715, Final Batch Loss: 0.35461488366127014\n",
      "Epoch 1299, Loss: 4.370189070701599, Final Batch Loss: 1.8265315294265747\n",
      "Epoch 1300, Loss: 2.5619756504893303, Final Batch Loss: 0.06303977221250534\n",
      "Epoch 1301, Loss: 3.1552517414093018, Final Batch Loss: 0.7329822778701782\n",
      "Epoch 1302, Loss: 3.844209372997284, Final Batch Loss: 1.3241567611694336\n",
      "Epoch 1303, Loss: 3.3439740538597107, Final Batch Loss: 0.8216959238052368\n",
      "Epoch 1304, Loss: 2.4362137522548437, Final Batch Loss: 0.03099670074880123\n",
      "Epoch 1305, Loss: 3.4713934659957886, Final Batch Loss: 1.0987435579299927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1306, Loss: 2.5631619840860367, Final Batch Loss: 0.1617148071527481\n",
      "Epoch 1307, Loss: 3.962088644504547, Final Batch Loss: 1.5023550987243652\n",
      "Epoch 1308, Loss: 4.540788054466248, Final Batch Loss: 2.1337130069732666\n",
      "Epoch 1309, Loss: 2.42843113001436, Final Batch Loss: 0.009220636449754238\n",
      "Epoch 1310, Loss: 2.520793601870537, Final Batch Loss: 0.11031101644039154\n",
      "Epoch 1311, Loss: 2.7318476140499115, Final Batch Loss: 0.2736292779445648\n",
      "Epoch 1312, Loss: 2.9006646275520325, Final Batch Loss: 0.5031733512878418\n",
      "Epoch 1313, Loss: 5.986223459243774, Final Batch Loss: 3.5061166286468506\n",
      "Epoch 1314, Loss: 3.1243547797203064, Final Batch Loss: 0.6921011805534363\n",
      "Epoch 1315, Loss: 3.513166844844818, Final Batch Loss: 1.059449315071106\n",
      "Epoch 1316, Loss: 2.71977536380291, Final Batch Loss: 0.1863946169614792\n",
      "Epoch 1317, Loss: 4.236230790615082, Final Batch Loss: 1.5283817052841187\n",
      "Epoch 1318, Loss: 2.859084904193878, Final Batch Loss: 0.08895450830459595\n",
      "Epoch 1319, Loss: 3.298913598060608, Final Batch Loss: 0.6695078611373901\n",
      "Epoch 1320, Loss: 2.959559738636017, Final Batch Loss: 0.3171239495277405\n",
      "Epoch 1321, Loss: 2.6624158434569836, Final Batch Loss: 0.04472682997584343\n",
      "Epoch 1322, Loss: 2.603852102532983, Final Batch Loss: 0.004765937104821205\n",
      "Epoch 1323, Loss: 4.103182554244995, Final Batch Loss: 1.465949296951294\n",
      "Epoch 1324, Loss: 2.78316268324852, Final Batch Loss: 0.2837742865085602\n",
      "Epoch 1325, Loss: 2.632005751132965, Final Batch Loss: 0.12970906496047974\n",
      "Epoch 1326, Loss: 2.6016325801610947, Final Batch Loss: 0.12753234803676605\n",
      "Epoch 1327, Loss: 3.449814796447754, Final Batch Loss: 1.036953091621399\n",
      "Epoch 1328, Loss: 2.513696812093258, Final Batch Loss: 0.04660620540380478\n",
      "Epoch 1329, Loss: 2.4699713476002216, Final Batch Loss: 0.027001071721315384\n",
      "Epoch 1330, Loss: 2.5095843449234962, Final Batch Loss: 0.021320439875125885\n",
      "Epoch 1331, Loss: 4.406322240829468, Final Batch Loss: 1.9856505393981934\n",
      "Epoch 1332, Loss: 2.5491751432418823, Final Batch Loss: 0.16079390048980713\n",
      "Epoch 1333, Loss: 2.4908615117892623, Final Batch Loss: 0.004342175088822842\n",
      "Epoch 1334, Loss: 4.9121774435043335, Final Batch Loss: 2.4403486251831055\n",
      "Epoch 1335, Loss: 2.454311141744256, Final Batch Loss: 0.013465413823723793\n",
      "Epoch 1336, Loss: 2.8171210289001465, Final Batch Loss: 0.44557058811187744\n",
      "Epoch 1337, Loss: 2.4212307035923004, Final Batch Loss: 0.03967437148094177\n",
      "Epoch 1338, Loss: 2.566180944442749, Final Batch Loss: 0.229905366897583\n",
      "Epoch 1339, Loss: 2.4257081151008606, Final Batch Loss: 0.10392272472381592\n",
      "Epoch 1340, Loss: 3.1704564094543457, Final Batch Loss: 0.727266788482666\n",
      "Epoch 1341, Loss: 4.006853222846985, Final Batch Loss: 1.5776879787445068\n",
      "Epoch 1342, Loss: 3.212648093700409, Final Batch Loss: 0.7859036922454834\n",
      "Epoch 1343, Loss: 3.3370309472084045, Final Batch Loss: 0.92522132396698\n",
      "Epoch 1344, Loss: 4.227693676948547, Final Batch Loss: 1.8001809120178223\n",
      "Epoch 1345, Loss: 2.6112291114404798, Final Batch Loss: 0.015032579191029072\n",
      "Epoch 1346, Loss: 5.129435658454895, Final Batch Loss: 2.533346652984619\n",
      "Epoch 1347, Loss: 3.271103799343109, Final Batch Loss: 0.9328568577766418\n",
      "Epoch 1348, Loss: 2.351866142707877, Final Batch Loss: 0.0014115142403170466\n",
      "Epoch 1349, Loss: 2.4583910554647446, Final Batch Loss: 0.12447910010814667\n",
      "Epoch 1350, Loss: 3.0150468945503235, Final Batch Loss: 0.508971095085144\n",
      "Epoch 1351, Loss: 2.4663374349474907, Final Batch Loss: 0.0012019798159599304\n",
      "Epoch 1352, Loss: 2.911927044391632, Final Batch Loss: 0.5792887210845947\n",
      "Epoch 1353, Loss: 4.855352163314819, Final Batch Loss: 2.4321322441101074\n",
      "Epoch 1354, Loss: 3.9614341259002686, Final Batch Loss: 1.6614021062850952\n",
      "Epoch 1355, Loss: 3.765627920627594, Final Batch Loss: 1.3070248365402222\n",
      "Epoch 1356, Loss: 4.118525266647339, Final Batch Loss: 1.7001478672027588\n",
      "Epoch 1357, Loss: 3.895901381969452, Final Batch Loss: 1.4022287130355835\n",
      "Epoch 1358, Loss: 2.5545851439237595, Final Batch Loss: 0.11301784217357635\n",
      "Epoch 1359, Loss: 4.065745294094086, Final Batch Loss: 1.429023265838623\n",
      "Epoch 1360, Loss: 3.962229073047638, Final Batch Loss: 1.347123384475708\n",
      "Epoch 1361, Loss: 2.548071503639221, Final Batch Loss: 0.03670471906661987\n",
      "Epoch 1362, Loss: 2.456290753558278, Final Batch Loss: 0.018996568396687508\n",
      "Epoch 1363, Loss: 2.4618213437497616, Final Batch Loss: 0.06050523743033409\n",
      "Epoch 1364, Loss: 2.6612829715013504, Final Batch Loss: 0.13504453003406525\n",
      "Epoch 1365, Loss: 2.823513627052307, Final Batch Loss: 0.47087788581848145\n",
      "Epoch 1366, Loss: 3.489896535873413, Final Batch Loss: 1.1658071279525757\n",
      "Epoch 1367, Loss: 4.480114221572876, Final Batch Loss: 2.15244197845459\n",
      "Epoch 1368, Loss: 3.320776641368866, Final Batch Loss: 0.7409110069274902\n",
      "Epoch 1369, Loss: 2.901980958878994, Final Batch Loss: 0.024260900914669037\n",
      "Epoch 1370, Loss: 4.963191390037537, Final Batch Loss: 1.8424432277679443\n",
      "Epoch 1371, Loss: 3.3983675241470337, Final Batch Loss: 0.353440523147583\n",
      "Epoch 1372, Loss: 3.899168610572815, Final Batch Loss: 1.0512216091156006\n",
      "Epoch 1373, Loss: 2.7839378640055656, Final Batch Loss: 0.10066276043653488\n",
      "Epoch 1374, Loss: 7.66483473777771, Final Batch Loss: 5.160856246948242\n",
      "Epoch 1375, Loss: 5.066101670265198, Final Batch Loss: 2.336648464202881\n",
      "Epoch 1376, Loss: 3.437748983502388, Final Batch Loss: 0.040998294949531555\n",
      "Epoch 1377, Loss: 5.2937469482421875, Final Batch Loss: 1.8925422430038452\n",
      "Epoch 1378, Loss: 4.993307590484619, Final Batch Loss: 1.9330602884292603\n",
      "Epoch 1379, Loss: 4.448295831680298, Final Batch Loss: 1.4763784408569336\n",
      "Epoch 1380, Loss: 4.627316534519196, Final Batch Loss: 1.7825783491134644\n",
      "Epoch 1381, Loss: 3.0992431342601776, Final Batch Loss: 0.4036596119403839\n",
      "Epoch 1382, Loss: 4.099613070487976, Final Batch Loss: 1.2912755012512207\n",
      "Epoch 1383, Loss: 4.33275032043457, Final Batch Loss: 1.6502240896224976\n",
      "Epoch 1384, Loss: 2.749153718352318, Final Batch Loss: 0.10734717547893524\n",
      "Epoch 1385, Loss: 3.9032936692237854, Final Batch Loss: 1.4460716247558594\n",
      "Epoch 1386, Loss: 4.555050492286682, Final Batch Loss: 2.089836597442627\n",
      "Epoch 1387, Loss: 2.9143823385238647, Final Batch Loss: 0.5122283697128296\n",
      "Epoch 1388, Loss: 2.635541781783104, Final Batch Loss: 0.1287911981344223\n",
      "Epoch 1389, Loss: 2.590335965156555, Final Batch Loss: 0.05265229940414429\n",
      "Epoch 1390, Loss: 5.152275383472443, Final Batch Loss: 2.7736496925354004\n",
      "Epoch 1391, Loss: 2.4380123112350702, Final Batch Loss: 0.016803273931145668\n",
      "Epoch 1392, Loss: 2.4406195068731904, Final Batch Loss: 0.015064754523336887\n",
      "Epoch 1393, Loss: 3.6262502670288086, Final Batch Loss: 1.1554310321807861\n",
      "Epoch 1394, Loss: 5.7115907073020935, Final Batch Loss: 3.2962486743927\n",
      "Epoch 1395, Loss: 2.5813197642564774, Final Batch Loss: 0.16419555246829987\n",
      "Epoch 1396, Loss: 4.812195718288422, Final Batch Loss: 2.292235851287842\n",
      "Epoch 1397, Loss: 2.6243679225444794, Final Batch Loss: 0.09773263335227966\n",
      "Epoch 1398, Loss: 5.217078745365143, Final Batch Loss: 2.648958206176758\n",
      "Epoch 1399, Loss: 4.916380524635315, Final Batch Loss: 2.367204427719116\n",
      "Epoch 1400, Loss: 4.326241552829742, Final Batch Loss: 1.6953071355819702\n",
      "Epoch 1401, Loss: 3.037907838821411, Final Batch Loss: 0.549060583114624\n",
      "Epoch 1402, Loss: 3.9122488498687744, Final Batch Loss: 1.460231065750122\n",
      "Epoch 1403, Loss: 4.056119024753571, Final Batch Loss: 1.483725905418396\n",
      "Epoch 1404, Loss: 2.7909274846315384, Final Batch Loss: 0.16461952030658722\n",
      "Epoch 1405, Loss: 2.645958503591828, Final Batch Loss: 0.0018152202246710658\n",
      "Epoch 1406, Loss: 4.384898245334625, Final Batch Loss: 1.8749492168426514\n",
      "Epoch 1407, Loss: 4.3010333776474, Final Batch Loss: 1.7398453950881958\n",
      "Epoch 1408, Loss: 3.5631004571914673, Final Batch Loss: 1.0954084396362305\n",
      "Epoch 1409, Loss: 2.493018393870443, Final Batch Loss: 0.00348353898152709\n",
      "Epoch 1410, Loss: 2.408473215997219, Final Batch Loss: 0.06050815433263779\n",
      "Epoch 1411, Loss: 3.1987643241882324, Final Batch Loss: 0.7461068034172058\n",
      "Epoch 1412, Loss: 2.894883394241333, Final Batch Loss: 0.4461238980293274\n",
      "Epoch 1413, Loss: 4.017166376113892, Final Batch Loss: 1.6307512521743774\n",
      "Epoch 1414, Loss: 2.8166920840740204, Final Batch Loss: 0.4705059230327606\n",
      "Epoch 1415, Loss: 4.486843168735504, Final Batch Loss: 2.1152191162109375\n",
      "Epoch 1416, Loss: 3.504448354244232, Final Batch Loss: 1.143669843673706\n",
      "Epoch 1417, Loss: 2.5887151286005974, Final Batch Loss: 0.11007612198591232\n",
      "Epoch 1418, Loss: 2.881526529788971, Final Batch Loss: 0.45911020040512085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1419, Loss: 2.4950083941221237, Final Batch Loss: 0.08158133924007416\n",
      "Epoch 1420, Loss: 3.8129826188087463, Final Batch Loss: 1.3427144289016724\n",
      "Epoch 1421, Loss: 2.452057698741555, Final Batch Loss: 0.01851455308496952\n",
      "Epoch 1422, Loss: 3.1250906586647034, Final Batch Loss: 0.7521965503692627\n",
      "Epoch 1423, Loss: 2.5776195526123047, Final Batch Loss: 0.24961167573928833\n",
      "Epoch 1424, Loss: 2.7854765951633453, Final Batch Loss: 0.41490092873573303\n",
      "Epoch 1425, Loss: 2.4397555142641068, Final Batch Loss: 0.015802040696144104\n",
      "Epoch 1426, Loss: 4.422608852386475, Final Batch Loss: 2.0213775634765625\n",
      "Epoch 1427, Loss: 2.413932040333748, Final Batch Loss: 0.07723434269428253\n",
      "Epoch 1428, Loss: 3.1464620232582092, Final Batch Loss: 0.7255752682685852\n",
      "Epoch 1429, Loss: 2.5269664898514748, Final Batch Loss: 0.08011976629495621\n",
      "Epoch 1430, Loss: 3.8524405360221863, Final Batch Loss: 1.4534599781036377\n",
      "Epoch 1431, Loss: 3.8791282176971436, Final Batch Loss: 1.5282212495803833\n",
      "Epoch 1432, Loss: 2.562797151505947, Final Batch Loss: 0.009105585515499115\n",
      "Epoch 1433, Loss: 3.305357277393341, Final Batch Loss: 0.9156098961830139\n",
      "Epoch 1434, Loss: 5.485731780529022, Final Batch Loss: 2.9527621269226074\n",
      "Epoch 1435, Loss: 2.7806906700134277, Final Batch Loss: 0.3029543161392212\n",
      "Epoch 1436, Loss: 3.079560160636902, Final Batch Loss: 0.6789965629577637\n",
      "Epoch 1437, Loss: 2.3776999511028407, Final Batch Loss: 0.0002004899288294837\n",
      "Epoch 1438, Loss: 3.4863731265068054, Final Batch Loss: 1.1114557981491089\n",
      "Epoch 1439, Loss: 2.3658090729732066, Final Batch Loss: 0.00014149141497910023\n",
      "Epoch 1440, Loss: 3.4433390498161316, Final Batch Loss: 1.1772425174713135\n",
      "Epoch 1441, Loss: 2.40105689316988, Final Batch Loss: 0.1179080680012703\n",
      "Epoch 1442, Loss: 2.640585705637932, Final Batch Loss: 0.20442049205303192\n",
      "Epoch 1443, Loss: 3.2531864643096924, Final Batch Loss: 0.8981059789657593\n",
      "Epoch 1444, Loss: 2.5391399413347244, Final Batch Loss: 0.14961238205432892\n",
      "Epoch 1445, Loss: 2.5479839220643044, Final Batch Loss: 0.07928351312875748\n",
      "Epoch 1446, Loss: 2.4557729475200176, Final Batch Loss: 0.03821135684847832\n",
      "Epoch 1447, Loss: 4.2878183126449585, Final Batch Loss: 1.89003324508667\n",
      "Epoch 1448, Loss: 5.468174755573273, Final Batch Loss: 3.1655588150024414\n",
      "Epoch 1449, Loss: 2.444811560213566, Final Batch Loss: 0.1205991879105568\n",
      "Epoch 1450, Loss: 4.661031007766724, Final Batch Loss: 2.18510103225708\n",
      "Epoch 1451, Loss: 3.0398090481758118, Final Batch Loss: 0.5229396224021912\n",
      "Epoch 1452, Loss: 3.701498806476593, Final Batch Loss: 1.26365327835083\n",
      "Epoch 1453, Loss: 4.886967360973358, Final Batch Loss: 2.395477294921875\n",
      "Epoch 1454, Loss: 5.641343533992767, Final Batch Loss: 3.24410080909729\n",
      "Epoch 1455, Loss: 2.5235189646482468, Final Batch Loss: 0.08551518619060516\n",
      "Epoch 1456, Loss: 2.615032024681568, Final Batch Loss: 0.08617586642503738\n",
      "Epoch 1457, Loss: 3.1414427757263184, Final Batch Loss: 0.3746047019958496\n",
      "Epoch 1458, Loss: 2.7800928130745888, Final Batch Loss: 0.08182159811258316\n",
      "Epoch 1459, Loss: 3.1383777260780334, Final Batch Loss: 0.5626333951950073\n",
      "Epoch 1460, Loss: 2.6596128344535828, Final Batch Loss: 0.07245421409606934\n",
      "Epoch 1461, Loss: 3.342822790145874, Final Batch Loss: 0.9558825492858887\n",
      "Epoch 1462, Loss: 2.4270103834569454, Final Batch Loss: 0.02835473045706749\n",
      "Epoch 1463, Loss: 3.825753688812256, Final Batch Loss: 1.5549216270446777\n",
      "Epoch 1464, Loss: 3.956805408000946, Final Batch Loss: 1.6203747987747192\n",
      "Epoch 1465, Loss: 3.898827016353607, Final Batch Loss: 1.5611350536346436\n",
      "Epoch 1466, Loss: 2.38974155113101, Final Batch Loss: 0.03288484737277031\n",
      "Epoch 1467, Loss: 2.5771357119083405, Final Batch Loss: 0.19924184679985046\n",
      "Epoch 1468, Loss: 2.870266020298004, Final Batch Loss: 0.5156754851341248\n",
      "Epoch 1469, Loss: 3.6061956882476807, Final Batch Loss: 1.330984354019165\n",
      "Epoch 1470, Loss: 2.541149854660034, Final Batch Loss: 0.213104248046875\n",
      "Epoch 1471, Loss: 3.3125043511390686, Final Batch Loss: 0.9911131262779236\n",
      "Epoch 1472, Loss: 4.059970378875732, Final Batch Loss: 1.755492925643921\n",
      "Epoch 1473, Loss: 2.440935157239437, Final Batch Loss: 0.12025465816259384\n",
      "Epoch 1474, Loss: 3.4628909826278687, Final Batch Loss: 1.0836189985275269\n",
      "Epoch 1475, Loss: 2.7011344730854034, Final Batch Loss: 0.28731295466423035\n",
      "Epoch 1476, Loss: 2.9301855266094208, Final Batch Loss: 0.3528161346912384\n",
      "Epoch 1477, Loss: 2.5585051886737347, Final Batch Loss: 0.027679752558469772\n",
      "Epoch 1478, Loss: 5.013921678066254, Final Batch Loss: 2.4780356884002686\n",
      "Epoch 1479, Loss: 2.3858792055398226, Final Batch Loss: 0.01925818808376789\n",
      "Epoch 1480, Loss: 3.899740755558014, Final Batch Loss: 1.2014927864074707\n",
      "Epoch 1481, Loss: 3.9894005060195923, Final Batch Loss: 1.3269883394241333\n",
      "Epoch 1482, Loss: 4.709818482398987, Final Batch Loss: 2.192469596862793\n",
      "Epoch 1483, Loss: 3.7695393562316895, Final Batch Loss: 1.268770694732666\n",
      "Epoch 1484, Loss: 3.4582887291908264, Final Batch Loss: 0.9509938955307007\n",
      "Epoch 1485, Loss: 3.944709897041321, Final Batch Loss: 1.5068984031677246\n",
      "Epoch 1486, Loss: 2.525507863610983, Final Batch Loss: 0.020939942449331284\n",
      "Epoch 1487, Loss: 3.977489948272705, Final Batch Loss: 1.5502851009368896\n",
      "Epoch 1488, Loss: 2.536526545882225, Final Batch Loss: 0.012159153819084167\n",
      "Epoch 1489, Loss: 3.2366384267807007, Final Batch Loss: 0.8014196753501892\n",
      "Epoch 1490, Loss: 2.3101615998893976, Final Batch Loss: 0.015591392293572426\n",
      "Epoch 1491, Loss: 4.062839984893799, Final Batch Loss: 1.668023705482483\n",
      "Epoch 1492, Loss: 2.509708776138723, Final Batch Loss: 0.009881271980702877\n",
      "Epoch 1493, Loss: 2.91194611787796, Final Batch Loss: 0.520235538482666\n",
      "Epoch 1494, Loss: 2.3587737157940865, Final Batch Loss: 0.04413319379091263\n",
      "Epoch 1495, Loss: 4.291324436664581, Final Batch Loss: 1.922043800354004\n",
      "Epoch 1496, Loss: 2.504133239388466, Final Batch Loss: 0.185812309384346\n",
      "Epoch 1497, Loss: 5.091028451919556, Final Batch Loss: 2.727285861968994\n",
      "Epoch 1498, Loss: 2.6269789040088654, Final Batch Loss: 0.15101423859596252\n",
      "Epoch 1499, Loss: 2.4805464819073677, Final Batch Loss: 0.08817429095506668\n",
      "Epoch 1500, Loss: 2.5215235576033592, Final Batch Loss: 0.1248105987906456\n",
      "Epoch 1501, Loss: 2.867529571056366, Final Batch Loss: 0.5788185596466064\n",
      "Epoch 1502, Loss: 3.8668267726898193, Final Batch Loss: 1.543798565864563\n",
      "Epoch 1503, Loss: 3.047982335090637, Final Batch Loss: 0.7849235534667969\n",
      "Epoch 1504, Loss: 2.5984065234661102, Final Batch Loss: 0.12937626242637634\n",
      "Epoch 1505, Loss: 2.922350764274597, Final Batch Loss: 0.43380510807037354\n",
      "Epoch 1506, Loss: 3.81486439704895, Final Batch Loss: 1.366501808166504\n",
      "Epoch 1507, Loss: 2.7585708498954773, Final Batch Loss: 0.2636946439743042\n",
      "Epoch 1508, Loss: 3.8668670058250427, Final Batch Loss: 1.3564186096191406\n",
      "Epoch 1509, Loss: 2.823236584663391, Final Batch Loss: 0.3378540873527527\n",
      "Epoch 1510, Loss: 3.372549831867218, Final Batch Loss: 0.9035390615463257\n",
      "Epoch 1511, Loss: 2.8982855081558228, Final Batch Loss: 0.3153361678123474\n",
      "Epoch 1512, Loss: 3.672560930252075, Final Batch Loss: 0.8993186354637146\n",
      "Epoch 1513, Loss: 2.7116292640566826, Final Batch Loss: 0.06263720244169235\n",
      "Epoch 1514, Loss: 3.4186325669288635, Final Batch Loss: 0.8621610999107361\n",
      "Epoch 1515, Loss: 2.6538805812597275, Final Batch Loss: 0.20519094169139862\n",
      "Epoch 1516, Loss: 2.440521813929081, Final Batch Loss: 0.04049757868051529\n",
      "Epoch 1517, Loss: 2.3467183080065297, Final Batch Loss: 8.129743218887597e-05\n",
      "Epoch 1518, Loss: 4.118474662303925, Final Batch Loss: 1.7166820764541626\n",
      "Epoch 1519, Loss: 2.5954646468162537, Final Batch Loss: 0.16554772853851318\n",
      "Epoch 1520, Loss: 3.1694576740264893, Final Batch Loss: 0.8620434403419495\n",
      "Epoch 1521, Loss: 6.531094670295715, Final Batch Loss: 4.139245986938477\n",
      "Epoch 1522, Loss: 2.3471063636243343, Final Batch Loss: 0.030369069427251816\n",
      "Epoch 1523, Loss: 2.552129440009594, Final Batch Loss: 0.0221896693110466\n",
      "Epoch 1524, Loss: 3.2714161574840546, Final Batch Loss: 0.46819719672203064\n",
      "Epoch 1525, Loss: 4.4317784905433655, Final Batch Loss: 1.511339545249939\n",
      "Epoch 1526, Loss: 2.8530268147587776, Final Batch Loss: 0.11683548241853714\n",
      "Epoch 1527, Loss: 4.716979444026947, Final Batch Loss: 2.1244428157806396\n",
      "Epoch 1528, Loss: 4.401417315006256, Final Batch Loss: 1.8399631977081299\n",
      "Epoch 1529, Loss: 3.972668468952179, Final Batch Loss: 1.4672825336456299\n",
      "Epoch 1530, Loss: 4.623433172702789, Final Batch Loss: 2.1684069633483887\n",
      "Epoch 1531, Loss: 2.6636767387390137, Final Batch Loss: 0.04786121845245361\n",
      "Epoch 1532, Loss: 2.8557545989751816, Final Batch Loss: 0.09331612288951874\n",
      "Epoch 1533, Loss: 3.386583924293518, Final Batch Loss: 0.5997314453125\n",
      "Epoch 1534, Loss: 3.8702057600021362, Final Batch Loss: 1.195621132850647\n",
      "Epoch 1535, Loss: 2.864644080400467, Final Batch Loss: 0.3325944244861603\n",
      "Epoch 1536, Loss: 3.8345293402671814, Final Batch Loss: 1.3085136413574219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1537, Loss: 3.9097840785980225, Final Batch Loss: 1.2686009407043457\n",
      "Epoch 1538, Loss: 3.93011075258255, Final Batch Loss: 1.323555827140808\n",
      "Epoch 1539, Loss: 2.710615836083889, Final Batch Loss: 0.06798084825277328\n",
      "Epoch 1540, Loss: 3.1196927428245544, Final Batch Loss: 0.45286786556243896\n",
      "Epoch 1541, Loss: 3.7071617245674133, Final Batch Loss: 0.970252513885498\n",
      "Epoch 1542, Loss: 3.7630770206451416, Final Batch Loss: 1.1506319046020508\n",
      "Epoch 1543, Loss: 3.7399407029151917, Final Batch Loss: 1.1087372303009033\n",
      "Epoch 1544, Loss: 4.449946105480194, Final Batch Loss: 2.033456325531006\n",
      "Epoch 1545, Loss: 3.225314438343048, Final Batch Loss: 0.8383380770683289\n",
      "Epoch 1546, Loss: 3.2912744879722595, Final Batch Loss: 0.856974184513092\n",
      "Epoch 1547, Loss: 4.789579689502716, Final Batch Loss: 2.2824461460113525\n",
      "Epoch 1548, Loss: 3.7602648735046387, Final Batch Loss: 1.3635128736495972\n",
      "Epoch 1549, Loss: 3.213930070400238, Final Batch Loss: 0.8331921100616455\n",
      "Epoch 1550, Loss: 3.1618970036506653, Final Batch Loss: 0.7552566528320312\n",
      "Epoch 1551, Loss: 2.4909604340791702, Final Batch Loss: 0.03509648144245148\n",
      "Epoch 1552, Loss: 3.192276656627655, Final Batch Loss: 0.7236694693565369\n",
      "Epoch 1553, Loss: 5.994235992431641, Final Batch Loss: 3.4802844524383545\n",
      "Epoch 1554, Loss: 2.632338985800743, Final Batch Loss: 0.22230730950832367\n",
      "Epoch 1555, Loss: 4.670031309127808, Final Batch Loss: 2.227288246154785\n",
      "Epoch 1556, Loss: 2.582421910017729, Final Batch Loss: 0.018804024904966354\n",
      "Epoch 1557, Loss: 3.353022277355194, Final Batch Loss: 0.8235660195350647\n",
      "Epoch 1558, Loss: 3.8679868578910828, Final Batch Loss: 1.3835787773132324\n",
      "Epoch 1559, Loss: 6.973640859127045, Final Batch Loss: 4.40512752532959\n",
      "Epoch 1560, Loss: 3.24686735868454, Final Batch Loss: 0.6919772624969482\n",
      "Epoch 1561, Loss: 2.655939355492592, Final Batch Loss: 0.23449786007404327\n",
      "Epoch 1562, Loss: 3.7291491627693176, Final Batch Loss: 1.3541240692138672\n",
      "Epoch 1563, Loss: 5.018453478813171, Final Batch Loss: 2.557101011276245\n",
      "Epoch 1564, Loss: 2.4085095450282097, Final Batch Loss: 0.0663476511836052\n",
      "Epoch 1565, Loss: 2.6961328387260437, Final Batch Loss: 0.29881560802459717\n",
      "Epoch 1566, Loss: 2.4873108118772507, Final Batch Loss: 0.13751016557216644\n",
      "Epoch 1567, Loss: 2.772221863269806, Final Batch Loss: 0.42049920558929443\n",
      "Epoch 1568, Loss: 3.4853047728538513, Final Batch Loss: 1.1308544874191284\n",
      "Epoch 1569, Loss: 2.8289604783058167, Final Batch Loss: 0.4171926975250244\n",
      "Epoch 1570, Loss: 2.5868289470672607, Final Batch Loss: 0.3413751721382141\n",
      "Epoch 1571, Loss: 2.4133121222257614, Final Batch Loss: 0.0953013151884079\n",
      "Epoch 1572, Loss: 3.3050541281700134, Final Batch Loss: 1.0177412033081055\n",
      "Epoch 1573, Loss: 3.3950303196907043, Final Batch Loss: 1.0897119045257568\n",
      "Epoch 1574, Loss: 4.442036807537079, Final Batch Loss: 2.1403822898864746\n",
      "Epoch 1575, Loss: 3.0665460228919983, Final Batch Loss: 0.7097139954566956\n",
      "Epoch 1576, Loss: 2.4222319684922695, Final Batch Loss: 0.04738348349928856\n",
      "Epoch 1577, Loss: 4.323689639568329, Final Batch Loss: 1.841850996017456\n",
      "Epoch 1578, Loss: 2.831665463745594, Final Batch Loss: 0.11020702868700027\n",
      "Epoch 1579, Loss: 2.6257398426532745, Final Batch Loss: 0.1662125289440155\n",
      "Epoch 1580, Loss: 3.0088801980018616, Final Batch Loss: 0.6061146855354309\n",
      "Epoch 1581, Loss: 2.398638666374609, Final Batch Loss: 0.001091004116460681\n",
      "Epoch 1582, Loss: 3.568510591983795, Final Batch Loss: 1.147790551185608\n",
      "Epoch 1583, Loss: 3.3003824949264526, Final Batch Loss: 0.9944437146186829\n",
      "Epoch 1584, Loss: 2.396662917919457, Final Batch Loss: 0.008616176433861256\n",
      "Epoch 1585, Loss: 2.3149616476148367, Final Batch Loss: 0.004437243565917015\n",
      "Epoch 1586, Loss: 4.189791560173035, Final Batch Loss: 1.8809558153152466\n",
      "Epoch 1587, Loss: 4.069652438163757, Final Batch Loss: 1.7060210704803467\n",
      "Epoch 1588, Loss: 2.282010746188462, Final Batch Loss: 0.011443925090134144\n",
      "Epoch 1589, Loss: 2.562290921807289, Final Batch Loss: 0.18544407188892365\n",
      "Epoch 1590, Loss: 2.4085268191993237, Final Batch Loss: 0.05879829451441765\n",
      "Epoch 1591, Loss: 2.5928513407707214, Final Batch Loss: 0.28584301471710205\n",
      "Epoch 1592, Loss: 2.3356235921382904, Final Batch Loss: 0.07734766602516174\n",
      "Epoch 1593, Loss: 4.507998943328857, Final Batch Loss: 2.222330331802368\n",
      "Epoch 1594, Loss: 4.121525824069977, Final Batch Loss: 1.8080507516860962\n",
      "Epoch 1595, Loss: 2.355691702105105, Final Batch Loss: 0.005099148489534855\n",
      "Epoch 1596, Loss: 2.5290628746151924, Final Batch Loss: 0.10068906098604202\n",
      "Epoch 1597, Loss: 2.52584321051836, Final Batch Loss: 0.10774702578783035\n",
      "Epoch 1598, Loss: 3.9903116822242737, Final Batch Loss: 1.7415274381637573\n",
      "Epoch 1599, Loss: 2.81051766872406, Final Batch Loss: 0.5301451086997986\n",
      "Epoch 1600, Loss: 2.6089440286159515, Final Batch Loss: 0.32469621300697327\n",
      "Epoch 1601, Loss: 3.5137858986854553, Final Batch Loss: 1.2040581703186035\n",
      "Epoch 1602, Loss: 3.2999643087387085, Final Batch Loss: 1.0961970090866089\n",
      "Epoch 1603, Loss: 3.981129825115204, Final Batch Loss: 1.6794652938842773\n",
      "Epoch 1604, Loss: 3.0292391180992126, Final Batch Loss: 0.7597126960754395\n",
      "Epoch 1605, Loss: 2.489295646548271, Final Batch Loss: 0.18366388976573944\n",
      "Epoch 1606, Loss: 2.9257179498672485, Final Batch Loss: 0.31996190547943115\n",
      "Epoch 1607, Loss: 2.9202316403388977, Final Batch Loss: 0.3047977685928345\n",
      "Epoch 1608, Loss: 2.5123802423477173, Final Batch Loss: 0.0867232084274292\n",
      "Epoch 1609, Loss: 2.721859499812126, Final Batch Loss: 0.13008354604244232\n",
      "Epoch 1610, Loss: 2.7987014949321747, Final Batch Loss: 0.3112848103046417\n",
      "Epoch 1611, Loss: 2.370166108943522, Final Batch Loss: 0.0037360647693276405\n",
      "Epoch 1612, Loss: 2.412683345377445, Final Batch Loss: 0.022235490381717682\n",
      "Epoch 1613, Loss: 2.3490103483200073, Final Batch Loss: 0.04845559597015381\n",
      "Epoch 1614, Loss: 3.363102614879608, Final Batch Loss: 1.0894341468811035\n",
      "Epoch 1615, Loss: 2.899547815322876, Final Batch Loss: 0.6435011625289917\n",
      "Epoch 1616, Loss: 2.3317614786756167, Final Batch Loss: 3.7431014789035544e-05\n",
      "Epoch 1617, Loss: 3.8340081572532654, Final Batch Loss: 1.5527141094207764\n",
      "Epoch 1618, Loss: 2.7459866106510162, Final Batch Loss: 0.4604838788509369\n",
      "Epoch 1619, Loss: 2.6851098239421844, Final Batch Loss: 0.1940191686153412\n",
      "Epoch 1620, Loss: 2.3805289790034294, Final Batch Loss: 0.07331746071577072\n",
      "Epoch 1621, Loss: 2.4455473870038986, Final Batch Loss: 0.04721258580684662\n",
      "Epoch 1622, Loss: 2.337081804929767, Final Batch Loss: 0.00029881304362788796\n",
      "Epoch 1623, Loss: 5.122122943401337, Final Batch Loss: 2.6869759559631348\n",
      "Epoch 1624, Loss: 2.487966611981392, Final Batch Loss: 0.24673981964588165\n",
      "Epoch 1625, Loss: 5.042612433433533, Final Batch Loss: 2.728060722351074\n",
      "Epoch 1626, Loss: 3.8001927733421326, Final Batch Loss: 1.3462241888046265\n",
      "Epoch 1627, Loss: 2.6254183053970337, Final Batch Loss: 0.044501662254333496\n",
      "Epoch 1628, Loss: 4.247172594070435, Final Batch Loss: 1.7234947681427002\n",
      "Epoch 1629, Loss: 4.431677579879761, Final Batch Loss: 1.9191410541534424\n",
      "Epoch 1630, Loss: 2.7473754584789276, Final Batch Loss: 0.3684852421283722\n",
      "Epoch 1631, Loss: 2.6978726387023926, Final Batch Loss: 0.3616843819618225\n",
      "Epoch 1632, Loss: 2.708255559206009, Final Batch Loss: 0.49006667733192444\n",
      "Epoch 1633, Loss: 2.388842947781086, Final Batch Loss: 0.08856267482042313\n",
      "Epoch 1634, Loss: 3.500618815422058, Final Batch Loss: 1.1472822427749634\n",
      "Epoch 1635, Loss: 2.883063316345215, Final Batch Loss: 0.548804759979248\n",
      "Epoch 1636, Loss: 2.4070361852645874, Final Batch Loss: 0.12117511034011841\n",
      "Epoch 1637, Loss: 3.381599724292755, Final Batch Loss: 1.082740306854248\n",
      "Epoch 1638, Loss: 4.228456556797028, Final Batch Loss: 1.8635615110397339\n",
      "Epoch 1639, Loss: 2.417741537094116, Final Batch Loss: 0.0351719856262207\n",
      "Epoch 1640, Loss: 2.4031951799988747, Final Batch Loss: 0.04485438019037247\n",
      "Epoch 1641, Loss: 2.3609730494208634, Final Batch Loss: 0.0029922020621597767\n",
      "Epoch 1642, Loss: 3.029865860939026, Final Batch Loss: 0.7707597017288208\n",
      "Epoch 1643, Loss: 2.5929466784000397, Final Batch Loss: 0.30887386202812195\n",
      "Epoch 1644, Loss: 3.230879068374634, Final Batch Loss: 0.9724022150039673\n",
      "Epoch 1645, Loss: 3.129760503768921, Final Batch Loss: 0.842300295829773\n",
      "Epoch 1646, Loss: 2.369074085727334, Final Batch Loss: 0.028154591098427773\n",
      "Epoch 1647, Loss: 2.3994156308472157, Final Batch Loss: 0.029872257262468338\n",
      "Epoch 1648, Loss: 4.4417736530303955, Final Batch Loss: 2.0256595611572266\n",
      "Epoch 1649, Loss: 3.4012523889541626, Final Batch Loss: 0.9241934418678284\n",
      "Epoch 1650, Loss: 3.1235548853874207, Final Batch Loss: 0.558637261390686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1651, Loss: 4.074104070663452, Final Batch Loss: 1.7406988143920898\n",
      "Epoch 1652, Loss: 3.353494167327881, Final Batch Loss: 0.9408635497093201\n",
      "Epoch 1653, Loss: 4.073047518730164, Final Batch Loss: 1.7323393821716309\n",
      "Epoch 1654, Loss: 2.850034534931183, Final Batch Loss: 0.5028352737426758\n",
      "Epoch 1655, Loss: 2.3439379390329123, Final Batch Loss: 0.02986878715455532\n",
      "Epoch 1656, Loss: 3.2584484219551086, Final Batch Loss: 0.9244416952133179\n",
      "Epoch 1657, Loss: 2.189704740419984, Final Batch Loss: 0.029621386900544167\n",
      "Epoch 1658, Loss: 3.1742374300956726, Final Batch Loss: 0.6329588890075684\n",
      "Epoch 1659, Loss: 2.422173112630844, Final Batch Loss: 0.04076370596885681\n",
      "Epoch 1660, Loss: 2.66272234916687, Final Batch Loss: 0.2884335517883301\n",
      "Epoch 1661, Loss: 3.0720296502113342, Final Batch Loss: 0.7372633814811707\n",
      "Epoch 1662, Loss: 3.721621572971344, Final Batch Loss: 1.3863496780395508\n",
      "Epoch 1663, Loss: 3.6540295481681824, Final Batch Loss: 1.3850796222686768\n",
      "Epoch 1664, Loss: 2.90685898065567, Final Batch Loss: 0.7204148769378662\n",
      "Epoch 1665, Loss: 2.3880760818719864, Final Batch Loss: 0.09360535442829132\n",
      "Epoch 1666, Loss: 3.7313116788864136, Final Batch Loss: 1.4478883743286133\n",
      "Epoch 1667, Loss: 2.4793512374162674, Final Batch Loss: 0.16671718657016754\n",
      "Epoch 1668, Loss: 3.290297508239746, Final Batch Loss: 0.5188279151916504\n",
      "Epoch 1669, Loss: 3.2540355771780014, Final Batch Loss: 0.12320278584957123\n",
      "Epoch 1670, Loss: 3.799008548259735, Final Batch Loss: 0.7450771331787109\n",
      "Epoch 1671, Loss: 3.0443455278873444, Final Batch Loss: 0.2908535301685333\n",
      "Epoch 1672, Loss: 4.125457406044006, Final Batch Loss: 1.6548259258270264\n",
      "Epoch 1673, Loss: 2.43477426469326, Final Batch Loss: 0.08234058320522308\n",
      "Epoch 1674, Loss: 2.430201221257448, Final Batch Loss: 0.03602259233593941\n",
      "Epoch 1675, Loss: 2.3256939467974007, Final Batch Loss: 0.0042580184526741505\n",
      "Epoch 1676, Loss: 2.50279101729393, Final Batch Loss: 0.20537909865379333\n",
      "Epoch 1677, Loss: 4.5515881180763245, Final Batch Loss: 2.2384543418884277\n",
      "Epoch 1678, Loss: 3.6525057554244995, Final Batch Loss: 1.4325135946273804\n",
      "Epoch 1679, Loss: 3.7948713898658752, Final Batch Loss: 1.6015070676803589\n",
      "Epoch 1680, Loss: 3.846922755241394, Final Batch Loss: 1.597048282623291\n",
      "Epoch 1681, Loss: 2.3625203110277653, Final Batch Loss: 0.022637519985437393\n",
      "Epoch 1682, Loss: 2.743394523859024, Final Batch Loss: 0.4019128978252411\n",
      "Epoch 1683, Loss: 5.283095479011536, Final Batch Loss: 2.9624392986297607\n",
      "Epoch 1684, Loss: 2.5040637850761414, Final Batch Loss: 0.2950493097305298\n",
      "Epoch 1685, Loss: 4.578804135322571, Final Batch Loss: 2.174386978149414\n",
      "Epoch 1686, Loss: 2.480405814945698, Final Batch Loss: 0.03955394774675369\n",
      "Epoch 1687, Loss: 3.723207116127014, Final Batch Loss: 1.2599400281906128\n",
      "Epoch 1688, Loss: 3.800791084766388, Final Batch Loss: 1.312051773071289\n",
      "Epoch 1689, Loss: 2.4920587614178658, Final Batch Loss: 0.05689621716737747\n",
      "Epoch 1690, Loss: 2.4796275570988655, Final Batch Loss: 0.07214004546403885\n",
      "Epoch 1691, Loss: 3.939896523952484, Final Batch Loss: 1.4771783351898193\n",
      "Epoch 1692, Loss: 3.6119813919067383, Final Batch Loss: 1.2233365774154663\n",
      "Epoch 1693, Loss: 4.436479806900024, Final Batch Loss: 2.1130597591400146\n",
      "Epoch 1694, Loss: 4.013729214668274, Final Batch Loss: 1.4819560050964355\n",
      "Epoch 1695, Loss: 5.046509504318237, Final Batch Loss: 2.543691635131836\n",
      "Epoch 1696, Loss: 4.365554451942444, Final Batch Loss: 1.9062538146972656\n",
      "Epoch 1697, Loss: 2.610522150993347, Final Batch Loss: 0.36120808124542236\n",
      "Epoch 1698, Loss: 3.4426515698432922, Final Batch Loss: 1.18827223777771\n",
      "Epoch 1699, Loss: 4.08503657579422, Final Batch Loss: 1.758510708808899\n",
      "Epoch 1700, Loss: 3.464028239250183, Final Batch Loss: 1.1723873615264893\n",
      "Epoch 1701, Loss: 4.148856520652771, Final Batch Loss: 1.8470361232757568\n",
      "Epoch 1702, Loss: 4.046076834201813, Final Batch Loss: 1.7864632606506348\n",
      "Epoch 1703, Loss: 2.383062496781349, Final Batch Loss: 0.08350463211536407\n",
      "Epoch 1704, Loss: 3.712197244167328, Final Batch Loss: 1.4333550930023193\n",
      "Epoch 1705, Loss: 4.254184305667877, Final Batch Loss: 1.9446778297424316\n",
      "Epoch 1706, Loss: 2.429371029138565, Final Batch Loss: 0.17737361788749695\n",
      "Epoch 1707, Loss: 2.2939237654209137, Final Batch Loss: 0.0731530487537384\n",
      "Epoch 1708, Loss: 2.4020410627126694, Final Batch Loss: 0.11949913203716278\n",
      "Epoch 1709, Loss: 2.608033001422882, Final Batch Loss: 0.28568464517593384\n",
      "Epoch 1710, Loss: 4.198713541030884, Final Batch Loss: 1.9058444499969482\n",
      "Epoch 1711, Loss: 3.8752681016921997, Final Batch Loss: 1.6351008415222168\n",
      "Epoch 1712, Loss: 2.202035464346409, Final Batch Loss: 0.025307752192020416\n",
      "Epoch 1713, Loss: 3.8904587626457214, Final Batch Loss: 1.607182264328003\n",
      "Epoch 1714, Loss: 2.6971041560173035, Final Batch Loss: 0.44374704360961914\n",
      "Epoch 1715, Loss: 3.3997113704681396, Final Batch Loss: 1.078474760055542\n",
      "Epoch 1716, Loss: 3.2825557589530945, Final Batch Loss: 1.0363298654556274\n",
      "Epoch 1717, Loss: 3.765341818332672, Final Batch Loss: 1.5562176704406738\n",
      "Epoch 1718, Loss: 2.8631811141967773, Final Batch Loss: 0.5967836380004883\n",
      "Epoch 1719, Loss: 2.4262926876544952, Final Batch Loss: 0.2066279947757721\n",
      "Epoch 1720, Loss: 2.5247930884361267, Final Batch Loss: 0.20883506536483765\n",
      "Epoch 1721, Loss: 2.2446238379343413, Final Batch Loss: 6.19869097135961e-05\n",
      "Epoch 1722, Loss: 3.1496281027793884, Final Batch Loss: 0.8695731163024902\n",
      "Epoch 1723, Loss: 2.375603273510933, Final Batch Loss: 0.1257311850786209\n",
      "Epoch 1724, Loss: 2.3462963700294495, Final Batch Loss: 0.14031106233596802\n",
      "Epoch 1725, Loss: 2.4391833283007145, Final Batch Loss: 0.04440656676888466\n",
      "Epoch 1726, Loss: 2.802745521068573, Final Batch Loss: 0.7735565900802612\n",
      "Epoch 1727, Loss: 3.370628297328949, Final Batch Loss: 1.29215669631958\n",
      "Epoch 1728, Loss: 2.268514646217227, Final Batch Loss: 0.023193849250674248\n",
      "Epoch 1729, Loss: 2.3374501913785934, Final Batch Loss: 0.06626598536968231\n",
      "Epoch 1730, Loss: 2.5260804891586304, Final Batch Loss: 0.08805662393569946\n",
      "Epoch 1731, Loss: 3.0211594700813293, Final Batch Loss: 0.6265364289283752\n",
      "Epoch 1732, Loss: 3.9967063665390015, Final Batch Loss: 1.5118614435195923\n",
      "Epoch 1733, Loss: 2.336411501513794, Final Batch Loss: 0.0015656605828553438\n",
      "Epoch 1734, Loss: 2.3653737232089043, Final Batch Loss: 0.06323551386594772\n",
      "Epoch 1735, Loss: 2.434671275317669, Final Batch Loss: 0.11042439192533493\n",
      "Epoch 1736, Loss: 3.1880143880844116, Final Batch Loss: 0.902458906173706\n",
      "Epoch 1737, Loss: 3.690996527671814, Final Batch Loss: 1.4563276767730713\n",
      "Epoch 1738, Loss: 2.956106126308441, Final Batch Loss: 0.5154355764389038\n",
      "Epoch 1739, Loss: 2.6362656876444817, Final Batch Loss: 0.10002916306257248\n",
      "Epoch 1740, Loss: 5.3804017305374146, Final Batch Loss: 2.7338078022003174\n",
      "Epoch 1741, Loss: 4.4509289264678955, Final Batch Loss: 1.8300652503967285\n",
      "Epoch 1742, Loss: 4.431991398334503, Final Batch Loss: 2.047915458679199\n",
      "Epoch 1743, Loss: 2.4778628423810005, Final Batch Loss: 0.09424585849046707\n",
      "Epoch 1744, Loss: 2.6621474027633667, Final Batch Loss: 0.3246816396713257\n",
      "Epoch 1745, Loss: 2.5787506699562073, Final Batch Loss: 0.33652204275131226\n",
      "Epoch 1746, Loss: 5.04353541135788, Final Batch Loss: 2.6969656944274902\n",
      "Epoch 1747, Loss: 3.6630287170410156, Final Batch Loss: 1.325348138809204\n",
      "Epoch 1748, Loss: 4.142687559127808, Final Batch Loss: 1.627539038658142\n",
      "Epoch 1749, Loss: 2.803222507238388, Final Batch Loss: 0.28078439831733704\n",
      "Epoch 1750, Loss: 2.945553332567215, Final Batch Loss: 0.4994005262851715\n",
      "Epoch 1751, Loss: 2.6284322813153267, Final Batch Loss: 0.11664295941591263\n",
      "Epoch 1752, Loss: 3.962681293487549, Final Batch Loss: 1.5250649452209473\n",
      "Epoch 1753, Loss: 4.194319248199463, Final Batch Loss: 1.8048917055130005\n",
      "Epoch 1754, Loss: 2.8811334669589996, Final Batch Loss: 0.39268770813941956\n",
      "Epoch 1755, Loss: 2.8702369332313538, Final Batch Loss: 0.3857254385948181\n",
      "Epoch 1756, Loss: 2.647922545671463, Final Batch Loss: 0.32476702332496643\n",
      "Epoch 1757, Loss: 2.5280443876981735, Final Batch Loss: 0.16004584729671478\n",
      "Epoch 1758, Loss: 2.604682058095932, Final Batch Loss: 0.23961904644966125\n",
      "Epoch 1759, Loss: 2.4570246785879135, Final Batch Loss: 0.1467171162366867\n",
      "Epoch 1760, Loss: 2.5728765726089478, Final Batch Loss: 0.29111701250076294\n",
      "Epoch 1761, Loss: 2.5694212168455124, Final Batch Loss: 0.2216595858335495\n",
      "Epoch 1762, Loss: 2.2293121037073433, Final Batch Loss: 0.004115920979529619\n",
      "Epoch 1763, Loss: 3.6602699160575867, Final Batch Loss: 1.3909308910369873\n",
      "Epoch 1764, Loss: 5.09938907623291, Final Batch Loss: 2.878875255584717\n",
      "Epoch 1765, Loss: 3.7165534496307373, Final Batch Loss: 1.554481029510498\n",
      "Epoch 1766, Loss: 4.83986759185791, Final Batch Loss: 2.595848321914673\n",
      "Epoch 1767, Loss: 3.3442752957344055, Final Batch Loss: 1.0809792280197144\n",
      "Epoch 1768, Loss: 3.6689429879188538, Final Batch Loss: 1.3165841102600098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1769, Loss: 2.4882955849170685, Final Batch Loss: 0.02829030156135559\n",
      "Epoch 1770, Loss: 3.3864729404449463, Final Batch Loss: 0.8078858852386475\n",
      "Epoch 1771, Loss: 2.8286811113357544, Final Batch Loss: 0.2993069291114807\n",
      "Epoch 1772, Loss: 3.44102144241333, Final Batch Loss: 0.9328659176826477\n",
      "Epoch 1773, Loss: 4.425418853759766, Final Batch Loss: 1.8197236061096191\n",
      "Epoch 1774, Loss: 4.306971371173859, Final Batch Loss: 1.9665813446044922\n",
      "Epoch 1775, Loss: 6.957727611064911, Final Batch Loss: 4.67924690246582\n",
      "Epoch 1776, Loss: 2.3184930027346127, Final Batch Loss: 0.0006989181856624782\n",
      "Epoch 1777, Loss: 4.3971540331840515, Final Batch Loss: 2.0730342864990234\n",
      "Epoch 1778, Loss: 4.25029456615448, Final Batch Loss: 1.856715440750122\n",
      "Epoch 1779, Loss: 2.4570325165987015, Final Batch Loss: 0.09583123028278351\n",
      "Epoch 1780, Loss: 2.435680791735649, Final Batch Loss: 0.08821336925029755\n",
      "Epoch 1781, Loss: 3.5016010999679565, Final Batch Loss: 1.089970588684082\n",
      "Epoch 1782, Loss: 2.629348672926426, Final Batch Loss: 0.11874959617853165\n",
      "Epoch 1783, Loss: 2.569780111312866, Final Batch Loss: 0.04026997089385986\n",
      "Epoch 1784, Loss: 2.8460319340229034, Final Batch Loss: 0.31481316685676575\n",
      "Epoch 1785, Loss: 2.7225867062807083, Final Batch Loss: 0.13388581573963165\n",
      "Epoch 1786, Loss: 4.012560129165649, Final Batch Loss: 1.5579290390014648\n",
      "Epoch 1787, Loss: 2.6095149517059326, Final Batch Loss: 0.12323397397994995\n",
      "Epoch 1788, Loss: 4.032337307929993, Final Batch Loss: 1.6740888357162476\n",
      "Epoch 1789, Loss: 2.642098367214203, Final Batch Loss: 0.3067528009414673\n",
      "Epoch 1790, Loss: 2.5776887834072113, Final Batch Loss: 0.33700230717658997\n",
      "Epoch 1791, Loss: 2.544411316514015, Final Batch Loss: 0.21115697920322418\n",
      "Epoch 1792, Loss: 4.136139154434204, Final Batch Loss: 1.8587201833724976\n",
      "Epoch 1793, Loss: 2.757887065410614, Final Batch Loss: 0.5012505054473877\n",
      "Epoch 1794, Loss: 2.508375346660614, Final Batch Loss: 0.30217933654785156\n",
      "Epoch 1795, Loss: 3.0487213134765625, Final Batch Loss: 0.7891190648078918\n",
      "Epoch 1796, Loss: 2.1519373152405024, Final Batch Loss: 0.029590139165520668\n",
      "Epoch 1797, Loss: 4.764561772346497, Final Batch Loss: 2.6490321159362793\n",
      "Epoch 1798, Loss: 2.4558174312114716, Final Batch Loss: 0.23821213841438293\n",
      "Epoch 1799, Loss: 4.822804749011993, Final Batch Loss: 2.534294605255127\n",
      "Epoch 1800, Loss: 2.336572215775959, Final Batch Loss: 0.0008621074957773089\n",
      "Epoch 1801, Loss: 2.4208709001541138, Final Batch Loss: 0.17357081174850464\n",
      "Epoch 1802, Loss: 3.1079153418540955, Final Batch Loss: 0.8546943664550781\n",
      "Epoch 1803, Loss: 3.2838300466537476, Final Batch Loss: 0.9523451328277588\n",
      "Epoch 1804, Loss: 3.5192272663116455, Final Batch Loss: 1.2588143348693848\n",
      "Epoch 1805, Loss: 2.6281099021434784, Final Batch Loss: 0.29088088870048523\n",
      "Epoch 1806, Loss: 4.368172585964203, Final Batch Loss: 2.015312433242798\n",
      "Epoch 1807, Loss: 3.4814085364341736, Final Batch Loss: 1.174881935119629\n",
      "Epoch 1808, Loss: 2.314060050994158, Final Batch Loss: 0.02653856948018074\n",
      "Epoch 1809, Loss: 2.404385596513748, Final Batch Loss: 0.13868555426597595\n",
      "Epoch 1810, Loss: 2.410563662648201, Final Batch Loss: 0.14809013903141022\n",
      "Epoch 1811, Loss: 2.3738593459129333, Final Batch Loss: 0.14525943994522095\n",
      "Epoch 1812, Loss: 2.3295277804136276, Final Batch Loss: 0.1272449940443039\n",
      "Epoch 1813, Loss: 3.62460058927536, Final Batch Loss: 1.418676733970642\n",
      "Epoch 1814, Loss: 3.8720531463623047, Final Batch Loss: 1.678989052772522\n",
      "Epoch 1815, Loss: 2.347779557108879, Final Batch Loss: 0.1605045646429062\n",
      "Epoch 1816, Loss: 2.287635952234268, Final Batch Loss: 0.12845000624656677\n",
      "Epoch 1817, Loss: 2.325647335499525, Final Batch Loss: 0.025029342621564865\n",
      "Epoch 1818, Loss: 2.5916953086853027, Final Batch Loss: 0.332396924495697\n",
      "Epoch 1819, Loss: 2.208572331815958, Final Batch Loss: 0.05876154080033302\n",
      "Epoch 1820, Loss: 4.063842356204987, Final Batch Loss: 1.918656349182129\n",
      "Epoch 1821, Loss: 4.248981058597565, Final Batch Loss: 2.0928359031677246\n",
      "Epoch 1822, Loss: 2.3347361758351326, Final Batch Loss: 0.11748618632555008\n",
      "Epoch 1823, Loss: 2.3219988867640495, Final Batch Loss: 0.10084103792905807\n",
      "Epoch 1824, Loss: 3.7054446935653687, Final Batch Loss: 1.5207886695861816\n",
      "Epoch 1825, Loss: 2.364623636007309, Final Batch Loss: 0.2544309198856354\n",
      "Epoch 1826, Loss: 2.3193090967833996, Final Batch Loss: 0.003368895500898361\n",
      "Epoch 1827, Loss: 2.9395066499710083, Final Batch Loss: 0.8409419059753418\n",
      "Epoch 1828, Loss: 4.341436684131622, Final Batch Loss: 2.229356050491333\n",
      "Epoch 1829, Loss: 2.237417943775654, Final Batch Loss: 0.07508397847414017\n",
      "Epoch 1830, Loss: 2.519751138985157, Final Batch Loss: 0.008380956947803497\n",
      "Epoch 1831, Loss: 2.70181985758245, Final Batch Loss: 0.03012271784245968\n",
      "Epoch 1832, Loss: 3.412759482860565, Final Batch Loss: 0.7211233973503113\n",
      "Epoch 1833, Loss: 3.0137150287628174, Final Batch Loss: 0.390078604221344\n",
      "Epoch 1834, Loss: 3.6886791586875916, Final Batch Loss: 1.3204469680786133\n",
      "Epoch 1835, Loss: 3.148260474205017, Final Batch Loss: 0.9616366028785706\n",
      "Epoch 1836, Loss: 2.3123584147542715, Final Batch Loss: 0.028025006875395775\n",
      "Epoch 1837, Loss: 3.0264487266540527, Final Batch Loss: 0.8165181875228882\n",
      "Epoch 1838, Loss: 2.716650128364563, Final Batch Loss: 0.3669222593307495\n",
      "Epoch 1839, Loss: 2.6472348272800446, Final Batch Loss: 0.17844954133033752\n",
      "Epoch 1840, Loss: 2.9459481835365295, Final Batch Loss: 0.5255032181739807\n",
      "Epoch 1841, Loss: 3.870516359806061, Final Batch Loss: 1.484613060951233\n",
      "Epoch 1842, Loss: 4.258868515491486, Final Batch Loss: 1.902383804321289\n",
      "Epoch 1843, Loss: 2.9375988841056824, Final Batch Loss: 0.6740497350692749\n",
      "Epoch 1844, Loss: 3.9446603655815125, Final Batch Loss: 1.5823516845703125\n",
      "Epoch 1845, Loss: 2.559417814016342, Final Batch Loss: 0.18107905983924866\n",
      "Epoch 1846, Loss: 2.441039562225342, Final Batch Loss: 0.18156373500823975\n",
      "Epoch 1847, Loss: 3.355120539665222, Final Batch Loss: 1.0199733972549438\n",
      "Epoch 1848, Loss: 2.562960773706436, Final Batch Loss: 0.3534195125102997\n",
      "Epoch 1849, Loss: 4.0033140778541565, Final Batch Loss: 1.9017754793167114\n",
      "Epoch 1850, Loss: 2.255679924041033, Final Batch Loss: 0.00964387133717537\n",
      "Epoch 1851, Loss: 2.3293051347136497, Final Batch Loss: 0.10911204665899277\n",
      "Epoch 1852, Loss: 5.8674479722976685, Final Batch Loss: 3.622175693511963\n",
      "Epoch 1853, Loss: 2.2664988338947296, Final Batch Loss: 0.04123649001121521\n",
      "Epoch 1854, Loss: 2.408993398072198, Final Batch Loss: 0.0018720973748713732\n",
      "Epoch 1855, Loss: 4.599891304969788, Final Batch Loss: 2.0329928398132324\n",
      "Epoch 1856, Loss: 2.777887213975191, Final Batch Loss: 0.005711663514375687\n",
      "Epoch 1857, Loss: 4.181457102298737, Final Batch Loss: 1.591416597366333\n",
      "Epoch 1858, Loss: 4.190323650836945, Final Batch Loss: 1.665311574935913\n",
      "Epoch 1859, Loss: 2.592502124607563, Final Batch Loss: 0.041062481701374054\n",
      "Epoch 1860, Loss: 4.39588338136673, Final Batch Loss: 1.9637556076049805\n",
      "Epoch 1861, Loss: 4.268310844898224, Final Batch Loss: 1.8961414098739624\n",
      "Epoch 1862, Loss: 3.8346320390701294, Final Batch Loss: 1.4572217464447021\n",
      "Epoch 1863, Loss: 4.168198347091675, Final Batch Loss: 1.669156789779663\n",
      "Epoch 1864, Loss: 3.4973138570785522, Final Batch Loss: 1.0645052194595337\n",
      "Epoch 1865, Loss: 3.4237833619117737, Final Batch Loss: 1.0652611255645752\n",
      "Epoch 1866, Loss: 2.8565123677253723, Final Batch Loss: 0.4929860830307007\n",
      "Epoch 1867, Loss: 2.594561606645584, Final Batch Loss: 0.27009615302085876\n",
      "Epoch 1868, Loss: 2.376774586737156, Final Batch Loss: 0.04039362818002701\n",
      "Epoch 1869, Loss: 4.004543483257294, Final Batch Loss: 1.7775994539260864\n",
      "Epoch 1870, Loss: 6.678976774215698, Final Batch Loss: 4.351221084594727\n",
      "Epoch 1871, Loss: 2.338330015540123, Final Batch Loss: 0.15558432042598724\n",
      "Epoch 1872, Loss: 2.97272652387619, Final Batch Loss: 0.72471684217453\n",
      "Epoch 1873, Loss: 3.074565827846527, Final Batch Loss: 0.7651726603507996\n",
      "Epoch 1874, Loss: 2.2864165604114532, Final Batch Loss: 0.08898428082466125\n",
      "Epoch 1875, Loss: 2.4866887629032135, Final Batch Loss: 0.16951152682304382\n",
      "Epoch 1876, Loss: 2.2314005736261606, Final Batch Loss: 0.012440109625458717\n",
      "Epoch 1877, Loss: 4.023388087749481, Final Batch Loss: 1.7211790084838867\n",
      "Epoch 1878, Loss: 2.4794201254844666, Final Batch Loss: 0.26261186599731445\n",
      "Epoch 1879, Loss: 2.3210678696632385, Final Batch Loss: 0.14263063669204712\n",
      "Epoch 1880, Loss: 2.1064615957438946, Final Batch Loss: 0.026926111429929733\n",
      "Epoch 1881, Loss: 2.336395487189293, Final Batch Loss: 0.15180204808712006\n",
      "Epoch 1882, Loss: 2.2453601136803627, Final Batch Loss: 0.06963328272104263\n",
      "Epoch 1883, Loss: 3.2629077434539795, Final Batch Loss: 1.11271071434021\n",
      "Epoch 1884, Loss: 2.361497789621353, Final Batch Loss: 0.08209595084190369\n",
      "Epoch 1885, Loss: 2.314511939883232, Final Batch Loss: 0.1911768764257431\n",
      "Epoch 1886, Loss: 2.444949060678482, Final Batch Loss: 0.30325832962989807\n",
      "Epoch 1887, Loss: 2.3602842539548874, Final Batch Loss: 0.21801702678203583\n",
      "Epoch 1888, Loss: 2.1196787676308304, Final Batch Loss: 0.0009432157967239618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1889, Loss: 2.1627541603520513, Final Batch Loss: 0.014039796777069569\n",
      "Epoch 1890, Loss: 2.5263779759407043, Final Batch Loss: 0.392236590385437\n",
      "Epoch 1891, Loss: 2.4747816920280457, Final Batch Loss: 0.2951972484588623\n",
      "Epoch 1892, Loss: 2.575602948665619, Final Batch Loss: 0.501775324344635\n",
      "Epoch 1893, Loss: 3.1937249302864075, Final Batch Loss: 1.069350004196167\n",
      "Epoch 1894, Loss: 2.33181394636631, Final Batch Loss: 0.17429108917713165\n",
      "Epoch 1895, Loss: 2.1880394965410233, Final Batch Loss: 0.09471972286701202\n",
      "Epoch 1896, Loss: 2.458469331264496, Final Batch Loss: 0.3220670819282532\n",
      "Epoch 1897, Loss: 2.0650592371821404, Final Batch Loss: 0.0379631444811821\n",
      "Epoch 1898, Loss: 2.26474205031991, Final Batch Loss: 0.04141298308968544\n",
      "Epoch 1899, Loss: 2.4141086041927338, Final Batch Loss: 0.3825080692768097\n",
      "Epoch 1900, Loss: 3.0562654733657837, Final Batch Loss: 0.9594721794128418\n",
      "Epoch 1901, Loss: 2.4036370515823364, Final Batch Loss: 0.3029131293296814\n",
      "Epoch 1902, Loss: 2.850109279155731, Final Batch Loss: 0.524886965751648\n",
      "Epoch 1903, Loss: 3.3133498430252075, Final Batch Loss: 1.0054559707641602\n",
      "Epoch 1904, Loss: 2.3969569578766823, Final Batch Loss: 0.08963940292596817\n",
      "Epoch 1905, Loss: 2.1889800392091274, Final Batch Loss: 0.03313743695616722\n",
      "Epoch 1906, Loss: 3.115697205066681, Final Batch Loss: 0.8709474205970764\n",
      "Epoch 1907, Loss: 2.184487547725439, Final Batch Loss: 0.038540150970220566\n",
      "Epoch 1908, Loss: 2.3743650913238525, Final Batch Loss: 0.2204572558403015\n",
      "Epoch 1909, Loss: 2.1600703597068787, Final Batch Loss: 0.15827864408493042\n",
      "Epoch 1910, Loss: 2.947172522544861, Final Batch Loss: 0.7432342171669006\n",
      "Epoch 1911, Loss: 2.4488671123981476, Final Batch Loss: 0.35764238238334656\n",
      "Epoch 1912, Loss: 2.15332586504519, Final Batch Loss: 0.011271962895989418\n",
      "Epoch 1913, Loss: 2.3036414235830307, Final Batch Loss: 0.1997263878583908\n",
      "Epoch 1914, Loss: 3.8800140023231506, Final Batch Loss: 1.8964745998382568\n",
      "Epoch 1915, Loss: 2.3267910480499268, Final Batch Loss: 0.2764971852302551\n",
      "Epoch 1916, Loss: 2.13265660405159, Final Batch Loss: 0.09156718850135803\n",
      "Epoch 1917, Loss: 2.0970097706012893, Final Batch Loss: 0.00012182447244413197\n",
      "Epoch 1918, Loss: 2.3964726328849792, Final Batch Loss: 0.4429548382759094\n",
      "Epoch 1919, Loss: 2.1273798793554306, Final Batch Loss: 0.06731872260570526\n",
      "Epoch 1920, Loss: 3.1355886459350586, Final Batch Loss: 0.9184399247169495\n",
      "Epoch 1921, Loss: 3.19460928440094, Final Batch Loss: 0.9920582175254822\n",
      "Epoch 1922, Loss: 2.229547180235386, Final Batch Loss: 0.027615047991275787\n",
      "Epoch 1923, Loss: 2.2002204954624176, Final Batch Loss: 0.11024418473243713\n",
      "Epoch 1924, Loss: 3.360143303871155, Final Batch Loss: 1.3961304426193237\n",
      "Epoch 1925, Loss: 2.4861003160476685, Final Batch Loss: 0.4406846761703491\n",
      "Epoch 1926, Loss: 2.1223560231737792, Final Batch Loss: 0.004134203772991896\n",
      "Epoch 1927, Loss: 2.4935725331306458, Final Batch Loss: 0.5638973712921143\n",
      "Epoch 1928, Loss: 3.4836281538009644, Final Batch Loss: 1.3966952562332153\n",
      "Epoch 1929, Loss: 3.1404111981391907, Final Batch Loss: 1.123960018157959\n",
      "Epoch 1930, Loss: 2.1775171235203743, Final Batch Loss: 0.01688755303621292\n",
      "Epoch 1931, Loss: 2.2822676301002502, Final Batch Loss: 0.2200230360031128\n",
      "Epoch 1932, Loss: 2.25359708070755, Final Batch Loss: 0.28389132022857666\n",
      "Epoch 1933, Loss: 4.187023401260376, Final Batch Loss: 2.1156654357910156\n",
      "Epoch 1934, Loss: 2.1961104571819305, Final Batch Loss: 0.1313118040561676\n",
      "Epoch 1935, Loss: 2.0742408568039536, Final Batch Loss: 0.014782303012907505\n",
      "Epoch 1936, Loss: 3.235485076904297, Final Batch Loss: 1.135522484779358\n",
      "Epoch 1937, Loss: 2.9662728309631348, Final Batch Loss: 0.7938581705093384\n",
      "Epoch 1938, Loss: 2.514512747526169, Final Batch Loss: 0.27340438961982727\n",
      "Epoch 1939, Loss: 2.9569915533065796, Final Batch Loss: 0.673822283744812\n",
      "Epoch 1940, Loss: 2.474171757657132, Final Batch Loss: 9.059865078597795e-06\n",
      "Epoch 1941, Loss: 3.9588831067085266, Final Batch Loss: 1.5157004594802856\n",
      "Epoch 1942, Loss: 2.6005300730466843, Final Batch Loss: 0.226998969912529\n",
      "Epoch 1943, Loss: 2.23670843988657, Final Batch Loss: 0.06474832445383072\n",
      "Epoch 1944, Loss: 4.774861812591553, Final Batch Loss: 2.6653566360473633\n",
      "Epoch 1945, Loss: 2.373952239751816, Final Batch Loss: 0.23012670874595642\n",
      "Epoch 1946, Loss: 2.1993774245493114, Final Batch Loss: 0.005582220386713743\n",
      "Epoch 1947, Loss: 3.1499991416931152, Final Batch Loss: 1.0301522016525269\n",
      "Epoch 1948, Loss: 2.976280927658081, Final Batch Loss: 0.8706907629966736\n",
      "Epoch 1949, Loss: 2.232530627399683, Final Batch Loss: 0.03812048211693764\n",
      "Epoch 1950, Loss: 2.3477416038513184, Final Batch Loss: 0.08821576833724976\n",
      "Epoch 1951, Loss: 3.921951472759247, Final Batch Loss: 1.7321248054504395\n",
      "Epoch 1952, Loss: 2.203843481838703, Final Batch Loss: 0.10928935557603836\n",
      "Epoch 1953, Loss: 2.729600727558136, Final Batch Loss: 0.5889421105384827\n",
      "Epoch 1954, Loss: 3.0222065448760986, Final Batch Loss: 0.9296869039535522\n",
      "Epoch 1955, Loss: 5.26363867521286, Final Batch Loss: 3.080824613571167\n",
      "Epoch 1956, Loss: 2.673332095146179, Final Batch Loss: 0.43011021614074707\n",
      "Epoch 1957, Loss: 8.26817101240158, Final Batch Loss: 5.903494834899902\n",
      "Epoch 1958, Loss: 2.8203753232955933, Final Batch Loss: 0.5808342099189758\n",
      "Epoch 1959, Loss: 2.3482175320386887, Final Batch Loss: 0.15422247350215912\n",
      "Epoch 1960, Loss: 2.398534268140793, Final Batch Loss: 0.3451865613460541\n",
      "Epoch 1961, Loss: 2.223072849214077, Final Batch Loss: 0.11679931730031967\n",
      "Epoch 1962, Loss: 3.220161557197571, Final Batch Loss: 1.203373908996582\n",
      "Epoch 1963, Loss: 4.002817332744598, Final Batch Loss: 1.911138892173767\n",
      "Epoch 1964, Loss: 3.3105002641677856, Final Batch Loss: 1.2390666007995605\n",
      "Epoch 1965, Loss: 3.74464875459671, Final Batch Loss: 1.5999034643173218\n",
      "Epoch 1966, Loss: 2.2379582673311234, Final Batch Loss: 0.10049696266651154\n",
      "Epoch 1967, Loss: 2.2657046914100647, Final Batch Loss: 0.18202590942382812\n",
      "Epoch 1968, Loss: 2.2968074679374695, Final Batch Loss: 0.20607984066009521\n",
      "Epoch 1969, Loss: 2.6886606216430664, Final Batch Loss: 0.5085684061050415\n",
      "Epoch 1970, Loss: 3.798599421977997, Final Batch Loss: 1.6711517572402954\n",
      "Epoch 1971, Loss: 2.145223781466484, Final Batch Loss: 0.06143973767757416\n",
      "Epoch 1972, Loss: 3.1157711148262024, Final Batch Loss: 1.0599098205566406\n",
      "Epoch 1973, Loss: 2.2079529762268066, Final Batch Loss: 0.18144214153289795\n",
      "Epoch 1974, Loss: 2.356033891439438, Final Batch Loss: 0.28330114483833313\n",
      "Epoch 1975, Loss: 5.186136960983276, Final Batch Loss: 3.128650426864624\n",
      "Epoch 1976, Loss: 2.1928096413612366, Final Batch Loss: 0.11768829822540283\n",
      "Epoch 1977, Loss: 2.976276695728302, Final Batch Loss: 0.7894604206085205\n",
      "Epoch 1978, Loss: 3.8637052178382874, Final Batch Loss: 1.7113256454467773\n",
      "Epoch 1979, Loss: 2.029633837635629, Final Batch Loss: 0.0006644901586696506\n",
      "Epoch 1980, Loss: 2.782293140888214, Final Batch Loss: 0.6928795576095581\n",
      "Epoch 1981, Loss: 3.884430706501007, Final Batch Loss: 1.6680234670639038\n",
      "Epoch 1982, Loss: 2.7198075652122498, Final Batch Loss: 0.5409692525863647\n",
      "Epoch 1983, Loss: 2.240554228425026, Final Batch Loss: 0.18702049553394318\n",
      "Epoch 1984, Loss: 2.601301848888397, Final Batch Loss: 0.5167356133460999\n",
      "Epoch 1985, Loss: 2.1528119817376137, Final Batch Loss: 0.07267383486032486\n",
      "Epoch 1986, Loss: 2.302954539656639, Final Batch Loss: 0.13057933747768402\n",
      "Epoch 1987, Loss: 2.808841824531555, Final Batch Loss: 0.6647601127624512\n",
      "Epoch 1988, Loss: 2.740589141845703, Final Batch Loss: 0.7464604377746582\n",
      "Epoch 1989, Loss: 3.821139872074127, Final Batch Loss: 1.7744523286819458\n",
      "Epoch 1990, Loss: 2.5141963809728622, Final Batch Loss: 0.24559526145458221\n",
      "Epoch 1991, Loss: 3.7096360325813293, Final Batch Loss: 1.478521466255188\n",
      "Epoch 1992, Loss: 2.265483759343624, Final Batch Loss: 0.1053968295454979\n",
      "Epoch 1993, Loss: 2.112020019441843, Final Batch Loss: 0.017475906759500504\n",
      "Epoch 1994, Loss: 3.683894991874695, Final Batch Loss: 1.5321478843688965\n",
      "Epoch 1995, Loss: 2.2275192737579346, Final Batch Loss: 0.20655041933059692\n",
      "Epoch 1996, Loss: 3.419050455093384, Final Batch Loss: 1.3513365983963013\n",
      "Epoch 1997, Loss: 2.085562951862812, Final Batch Loss: 0.1125463917851448\n",
      "Epoch 1998, Loss: 3.5581114292144775, Final Batch Loss: 1.463273048400879\n",
      "Epoch 1999, Loss: 2.2042311280965805, Final Batch Loss: 0.20225290954113007\n",
      "Epoch 2000, Loss: 2.1550123505294323, Final Batch Loss: 0.06067768856883049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001, Loss: 2.196734204888344, Final Batch Loss: 0.03312809765338898\n",
      "Epoch 2002, Loss: 3.2386950254440308, Final Batch Loss: 1.3309643268585205\n",
      "Epoch 2003, Loss: 2.01163337752223, Final Batch Loss: 0.03488318249583244\n",
      "Epoch 2004, Loss: 3.659418821334839, Final Batch Loss: 1.5368109941482544\n",
      "Epoch 2005, Loss: 2.1004500053822994, Final Batch Loss: 0.05300011858344078\n",
      "Epoch 2006, Loss: 2.570895403623581, Final Batch Loss: 0.48709383606910706\n",
      "Epoch 2007, Loss: 2.181402713060379, Final Batch Loss: 0.09331992268562317\n",
      "Epoch 2008, Loss: 2.0834241897100583, Final Batch Loss: 0.0015570909017696977\n",
      "Epoch 2009, Loss: 4.126041054725647, Final Batch Loss: 2.108060836791992\n",
      "Epoch 2010, Loss: 2.460490047931671, Final Batch Loss: 0.3457772731781006\n",
      "Epoch 2011, Loss: 1.9908606484532356, Final Batch Loss: 0.05031222850084305\n",
      "Epoch 2012, Loss: 2.8712517619132996, Final Batch Loss: 0.8493216633796692\n",
      "Epoch 2013, Loss: 2.3289463073015213, Final Batch Loss: 0.15576530992984772\n",
      "Epoch 2014, Loss: 2.1912263333797455, Final Batch Loss: 0.06464943289756775\n",
      "Epoch 2015, Loss: 2.325622007250786, Final Batch Loss: 0.06077663600444794\n",
      "Epoch 2016, Loss: 2.594814047217369, Final Batch Loss: 0.22722969949245453\n",
      "Epoch 2017, Loss: 2.214809576049447, Final Batch Loss: 0.018075505271553993\n",
      "Epoch 2018, Loss: 2.4518426954746246, Final Batch Loss: 0.24681684374809265\n",
      "Epoch 2019, Loss: 2.1307634410331957, Final Batch Loss: 0.00024530262453481555\n",
      "Epoch 2020, Loss: 2.527549982070923, Final Batch Loss: 0.46178513765335083\n",
      "Epoch 2021, Loss: 2.0358804394491017, Final Batch Loss: 0.0003649522550404072\n",
      "Epoch 2022, Loss: 2.19453863799572, Final Batch Loss: 0.13676361739635468\n",
      "Epoch 2023, Loss: 2.034952851012349, Final Batch Loss: 0.018258007243275642\n",
      "Epoch 2024, Loss: 1.9677696166327223, Final Batch Loss: 0.0013852057745680213\n",
      "Epoch 2025, Loss: 3.7039817571640015, Final Batch Loss: 1.6737266778945923\n",
      "Epoch 2026, Loss: 3.811299443244934, Final Batch Loss: 1.844786286354065\n",
      "Epoch 2027, Loss: 2.070744553115219, Final Batch Loss: 0.004943647887557745\n",
      "Epoch 2028, Loss: 3.789649248123169, Final Batch Loss: 1.6815265417099\n",
      "Epoch 2029, Loss: 3.6330294013023376, Final Batch Loss: 1.5701724290847778\n",
      "Epoch 2030, Loss: 3.2213958501815796, Final Batch Loss: 1.2205551862716675\n",
      "Epoch 2031, Loss: 3.393566071987152, Final Batch Loss: 1.4299954175949097\n",
      "Epoch 2032, Loss: 2.189921073615551, Final Batch Loss: 0.06164991110563278\n",
      "Epoch 2033, Loss: 2.0990430414676666, Final Batch Loss: 0.15188327431678772\n",
      "Epoch 2034, Loss: 2.6165488958358765, Final Batch Loss: 0.6581735014915466\n",
      "Epoch 2035, Loss: 2.3657787442207336, Final Batch Loss: 0.2738593816757202\n",
      "Epoch 2036, Loss: 2.1318106651306152, Final Batch Loss: 0.13685625791549683\n",
      "Epoch 2037, Loss: 3.804785907268524, Final Batch Loss: 1.685937523841858\n",
      "Epoch 2038, Loss: 2.208572030067444, Final Batch Loss: 0.2693599462509155\n",
      "Epoch 2039, Loss: 2.428040564060211, Final Batch Loss: 0.3904658555984497\n",
      "Epoch 2040, Loss: 3.6007526516914368, Final Batch Loss: 1.5127320289611816\n",
      "Epoch 2041, Loss: 3.1226099729537964, Final Batch Loss: 1.095790982246399\n",
      "Epoch 2042, Loss: 2.0645382888469612, Final Batch Loss: 0.00014959646796341985\n",
      "Epoch 2043, Loss: 2.1784783080220222, Final Batch Loss: 0.09662259370088577\n",
      "Epoch 2044, Loss: 5.14144092798233, Final Batch Loss: 3.0741939544677734\n",
      "Epoch 2045, Loss: 2.616876721382141, Final Batch Loss: 0.5908313989639282\n",
      "Epoch 2046, Loss: 4.153050363063812, Final Batch Loss: 2.232332468032837\n",
      "Epoch 2047, Loss: 3.5744619965553284, Final Batch Loss: 1.628775954246521\n",
      "Epoch 2048, Loss: 2.174242824316025, Final Batch Loss: 0.18437287211418152\n",
      "Epoch 2049, Loss: 2.0090594440698624, Final Batch Loss: 0.10961855947971344\n",
      "Epoch 2050, Loss: 2.0501600447460078, Final Batch Loss: 0.0009195152088068426\n",
      "Epoch 2051, Loss: 2.040925427339971, Final Batch Loss: 0.0037524541839957237\n",
      "Epoch 2052, Loss: 2.9058554768562317, Final Batch Loss: 0.9125187397003174\n",
      "Epoch 2053, Loss: 3.3602293133735657, Final Batch Loss: 1.3926112651824951\n",
      "Epoch 2054, Loss: 2.1711522713303566, Final Batch Loss: 0.1234774962067604\n",
      "Epoch 2055, Loss: 3.460223376750946, Final Batch Loss: 1.517769694328308\n",
      "Epoch 2056, Loss: 3.4603808522224426, Final Batch Loss: 1.387401819229126\n",
      "Epoch 2057, Loss: 3.646551012992859, Final Batch Loss: 1.6143180131912231\n",
      "Epoch 2058, Loss: 3.2219462394714355, Final Batch Loss: 1.1583536863327026\n",
      "Epoch 2059, Loss: 2.9453001022338867, Final Batch Loss: 0.8805791139602661\n",
      "Epoch 2060, Loss: 2.537967711687088, Final Batch Loss: 0.26164403557777405\n",
      "Epoch 2061, Loss: 3.28223192691803, Final Batch Loss: 0.7656223773956299\n",
      "Epoch 2062, Loss: 2.4352902993559837, Final Batch Loss: 0.03568538278341293\n",
      "Epoch 2063, Loss: 3.3043030500411987, Final Batch Loss: 0.9314224123954773\n",
      "Epoch 2064, Loss: 4.093016684055328, Final Batch Loss: 1.7336316108703613\n",
      "Epoch 2065, Loss: 4.012046217918396, Final Batch Loss: 1.7653539180755615\n",
      "Epoch 2066, Loss: 2.4351092875003815, Final Batch Loss: 0.16634538769721985\n",
      "Epoch 2067, Loss: 2.3432378582656384, Final Batch Loss: 0.04696796461939812\n",
      "Epoch 2068, Loss: 2.3843779861927032, Final Batch Loss: 0.09512606263160706\n",
      "Epoch 2069, Loss: 2.167633168399334, Final Batch Loss: 0.037255577743053436\n",
      "Epoch 2070, Loss: 2.1908229775726795, Final Batch Loss: 0.056913334876298904\n",
      "Epoch 2071, Loss: 2.8580650687217712, Final Batch Loss: 0.6894956827163696\n",
      "Epoch 2072, Loss: 2.1620233058929443, Final Batch Loss: 0.12350952625274658\n",
      "Epoch 2073, Loss: 2.1110258139669895, Final Batch Loss: 0.05954110994935036\n",
      "Epoch 2074, Loss: 2.5693345069885254, Final Batch Loss: 0.5720003247261047\n",
      "Epoch 2075, Loss: 2.1289580799639225, Final Batch Loss: 0.028029296547174454\n",
      "Epoch 2076, Loss: 3.294173836708069, Final Batch Loss: 1.1749686002731323\n",
      "Epoch 2077, Loss: 3.061440885066986, Final Batch Loss: 1.0287925004959106\n",
      "Epoch 2078, Loss: 2.358006864786148, Final Batch Loss: 0.26348599791526794\n",
      "Epoch 2079, Loss: 4.704563498497009, Final Batch Loss: 2.4914674758911133\n",
      "Epoch 2080, Loss: 3.799586832523346, Final Batch Loss: 1.5509765148162842\n",
      "Epoch 2081, Loss: 2.3888252376400487, Final Batch Loss: 1.490105023549404e-05\n",
      "Epoch 2082, Loss: 4.594035565853119, Final Batch Loss: 2.3425936698913574\n",
      "Epoch 2083, Loss: 4.023993074893951, Final Batch Loss: 1.8214681148529053\n",
      "Epoch 2084, Loss: 4.544701278209686, Final Batch Loss: 2.4324896335601807\n",
      "Epoch 2085, Loss: 3.242608428001404, Final Batch Loss: 0.9477141499519348\n",
      "Epoch 2086, Loss: 3.257629930973053, Final Batch Loss: 1.0125218629837036\n",
      "Epoch 2087, Loss: 2.427228458225727, Final Batch Loss: 0.03134912997484207\n",
      "Epoch 2088, Loss: 3.457305371761322, Final Batch Loss: 1.0444416999816895\n",
      "Epoch 2089, Loss: 3.1366063356399536, Final Batch Loss: 0.8764209151268005\n",
      "Epoch 2090, Loss: 2.702133595943451, Final Batch Loss: 0.49322015047073364\n",
      "Epoch 2091, Loss: 5.003605306148529, Final Batch Loss: 2.8784432411193848\n",
      "Epoch 2092, Loss: 3.638016104698181, Final Batch Loss: 1.5246403217315674\n",
      "Epoch 2093, Loss: 2.7561283707618713, Final Batch Loss: 0.7425398826599121\n",
      "Epoch 2094, Loss: 2.1663866490125656, Final Batch Loss: 0.04467587172985077\n",
      "Epoch 2095, Loss: 2.876963794231415, Final Batch Loss: 0.8369141817092896\n",
      "Epoch 2096, Loss: 2.7513861656188965, Final Batch Loss: 0.5974932312965393\n",
      "Epoch 2097, Loss: 3.0619930624961853, Final Batch Loss: 0.7462462186813354\n",
      "Epoch 2098, Loss: 2.399526372551918, Final Batch Loss: 0.15481866896152496\n",
      "Epoch 2099, Loss: 2.3044027537107468, Final Batch Loss: 0.13498704135417938\n",
      "Epoch 2100, Loss: 2.284589037299156, Final Batch Loss: 0.14910344779491425\n",
      "Epoch 2101, Loss: 2.355008691549301, Final Batch Loss: 0.10554251074790955\n",
      "Epoch 2102, Loss: 2.198603594209999, Final Batch Loss: 0.0012254356406629086\n",
      "Epoch 2103, Loss: 2.270869329571724, Final Batch Loss: 0.16266466677188873\n",
      "Epoch 2104, Loss: 2.2436836808919907, Final Batch Loss: 0.21228425204753876\n",
      "Epoch 2105, Loss: 2.783707618713379, Final Batch Loss: 0.6457534432411194\n",
      "Epoch 2106, Loss: 4.476105988025665, Final Batch Loss: 2.4009251594543457\n",
      "Epoch 2107, Loss: 2.5499019026756287, Final Batch Loss: 0.4171988368034363\n",
      "Epoch 2108, Loss: 4.28575998544693, Final Batch Loss: 2.0265979766845703\n",
      "Epoch 2109, Loss: 2.3865424543619156, Final Batch Loss: 0.1368589550256729\n",
      "Epoch 2110, Loss: 3.106931447982788, Final Batch Loss: 0.9719644784927368\n",
      "Epoch 2111, Loss: 2.485354781150818, Final Batch Loss: 0.45475661754608154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2112, Loss: 3.349055767059326, Final Batch Loss: 1.2345454692840576\n",
      "Epoch 2113, Loss: 2.010436765078339, Final Batch Loss: 0.0001282609737245366\n",
      "Epoch 2114, Loss: 3.282256007194519, Final Batch Loss: 1.2552071809768677\n",
      "Epoch 2115, Loss: 3.7417694330215454, Final Batch Loss: 1.807631492614746\n",
      "Epoch 2116, Loss: 3.2106988430023193, Final Batch Loss: 1.2481452226638794\n",
      "Epoch 2117, Loss: 2.3233729898929596, Final Batch Loss: 0.13562843203544617\n",
      "Epoch 2118, Loss: 2.8284926414489746, Final Batch Loss: 0.6298580765724182\n",
      "Epoch 2119, Loss: 4.082812249660492, Final Batch Loss: 1.7256768941879272\n",
      "Epoch 2120, Loss: 2.123263878747821, Final Batch Loss: 0.02211563102900982\n",
      "Epoch 2121, Loss: 2.7229523062705994, Final Batch Loss: 0.7306584715843201\n",
      "Epoch 2122, Loss: 2.271785229444504, Final Batch Loss: 0.263190358877182\n",
      "Epoch 2123, Loss: 2.9264360666275024, Final Batch Loss: 0.955488383769989\n",
      "Epoch 2124, Loss: 3.9706616401672363, Final Batch Loss: 1.9357906579971313\n",
      "Epoch 2125, Loss: 1.911175656830892, Final Batch Loss: 0.0016260033007711172\n",
      "Epoch 2126, Loss: 5.852831184864044, Final Batch Loss: 3.810131072998047\n",
      "Epoch 2127, Loss: 3.9879860281944275, Final Batch Loss: 2.0726542472839355\n",
      "Epoch 2128, Loss: 5.671031892299652, Final Batch Loss: 3.5966362953186035\n",
      "Epoch 2129, Loss: 2.6608383655548096, Final Batch Loss: 0.5145522952079773\n",
      "Epoch 2130, Loss: 2.4231050610542297, Final Batch Loss: 0.3245789408683777\n",
      "Epoch 2131, Loss: 3.3209054470062256, Final Batch Loss: 1.294144868850708\n",
      "Epoch 2132, Loss: 2.3135809898376465, Final Batch Loss: 0.19903671741485596\n",
      "Epoch 2133, Loss: 4.317355513572693, Final Batch Loss: 2.1454460620880127\n",
      "Epoch 2134, Loss: 3.972230017185211, Final Batch Loss: 1.8506041765213013\n",
      "Epoch 2135, Loss: 3.1369516849517822, Final Batch Loss: 0.9032343626022339\n",
      "Epoch 2136, Loss: 2.3111603260040283, Final Batch Loss: 0.2599301338195801\n",
      "Epoch 2137, Loss: 3.933418393135071, Final Batch Loss: 1.893775224685669\n",
      "Epoch 2138, Loss: 2.020700328052044, Final Batch Loss: 0.06130116432905197\n",
      "Epoch 2139, Loss: 2.129899762570858, Final Batch Loss: 0.07220669835805893\n",
      "Epoch 2140, Loss: 2.5944502353668213, Final Batch Loss: 0.6090567111968994\n",
      "Epoch 2141, Loss: 3.7952449917793274, Final Batch Loss: 1.73088800907135\n",
      "Epoch 2142, Loss: 2.038381151854992, Final Batch Loss: 0.05342172831296921\n",
      "Epoch 2143, Loss: 2.197041928768158, Final Batch Loss: 0.2082369327545166\n",
      "Epoch 2144, Loss: 2.4287852346897125, Final Batch Loss: 0.28738656640052795\n",
      "Epoch 2145, Loss: 5.583685517311096, Final Batch Loss: 3.524467945098877\n",
      "Epoch 2146, Loss: 2.3910495042800903, Final Batch Loss: 0.37703877687454224\n",
      "Epoch 2147, Loss: 3.365192949771881, Final Batch Loss: 1.2156554460525513\n",
      "Epoch 2148, Loss: 2.478976845741272, Final Batch Loss: 0.20383739471435547\n",
      "Epoch 2149, Loss: 3.7640894651412964, Final Batch Loss: 1.3994629383087158\n",
      "Epoch 2150, Loss: 3.7420385479927063, Final Batch Loss: 1.4276999235153198\n",
      "Epoch 2151, Loss: 2.96748811006546, Final Batch Loss: 0.73269122838974\n",
      "Epoch 2152, Loss: 3.7198460698127747, Final Batch Loss: 1.5210542678833008\n",
      "Epoch 2153, Loss: 2.2994618490338326, Final Batch Loss: 0.10472328215837479\n",
      "Epoch 2154, Loss: 2.0865768706426024, Final Batch Loss: 0.003951243124902248\n",
      "Epoch 2155, Loss: 3.418498396873474, Final Batch Loss: 1.3452566862106323\n",
      "Epoch 2156, Loss: 3.3806419372558594, Final Batch Loss: 1.3793747425079346\n",
      "Epoch 2157, Loss: 2.9391927123069763, Final Batch Loss: 0.8402495384216309\n",
      "Epoch 2158, Loss: 2.569549649953842, Final Batch Loss: 0.39211997389793396\n",
      "Epoch 2159, Loss: 3.961192548274994, Final Batch Loss: 1.713938593864441\n",
      "Epoch 2160, Loss: 3.3386181592941284, Final Batch Loss: 1.1158995628356934\n",
      "Epoch 2161, Loss: 4.006447792053223, Final Batch Loss: 1.7409417629241943\n",
      "Epoch 2162, Loss: 2.597475051879883, Final Batch Loss: 0.1599736213684082\n",
      "Epoch 2163, Loss: 4.3452224135398865, Final Batch Loss: 1.9659125804901123\n",
      "Epoch 2164, Loss: 5.343709349632263, Final Batch Loss: 3.0525267124176025\n",
      "Epoch 2165, Loss: 3.038444936275482, Final Batch Loss: 0.8121427893638611\n",
      "Epoch 2166, Loss: 2.1019212678074837, Final Batch Loss: 0.07601822167634964\n",
      "Epoch 2167, Loss: 2.2133474722504616, Final Batch Loss: 0.09322348982095718\n",
      "Epoch 2168, Loss: 2.370608538389206, Final Batch Loss: 0.15163663029670715\n",
      "Epoch 2169, Loss: 2.398975670337677, Final Batch Loss: 0.15729761123657227\n",
      "Epoch 2170, Loss: 2.282881423830986, Final Batch Loss: 0.09324966371059418\n",
      "Epoch 2171, Loss: 2.2580785900354385, Final Batch Loss: 0.17135341465473175\n",
      "Epoch 2172, Loss: 3.3845629692077637, Final Batch Loss: 1.2856582403182983\n",
      "Epoch 2173, Loss: 2.1111553013324738, Final Batch Loss: 0.08376482129096985\n",
      "Epoch 2174, Loss: 1.9811660815030336, Final Batch Loss: 0.010983603075146675\n",
      "Epoch 2175, Loss: 1.9873755993321538, Final Batch Loss: 0.014662486501038074\n",
      "Epoch 2176, Loss: 2.041941935196519, Final Batch Loss: 0.021372133865952492\n",
      "Epoch 2177, Loss: 4.3588462471961975, Final Batch Loss: 2.426501750946045\n",
      "Epoch 2178, Loss: 4.090145826339722, Final Batch Loss: 2.0900163650512695\n",
      "Epoch 2179, Loss: 2.074175253510475, Final Batch Loss: 0.09520496428012848\n",
      "Epoch 2180, Loss: 3.8786743879318237, Final Batch Loss: 1.9008651971817017\n",
      "Epoch 2181, Loss: 3.4369590878486633, Final Batch Loss: 1.3645243644714355\n",
      "Epoch 2182, Loss: 2.2075843662023544, Final Batch Loss: 0.16280214488506317\n",
      "Epoch 2183, Loss: 2.8066799640655518, Final Batch Loss: 0.6897116899490356\n",
      "Epoch 2184, Loss: 2.5924095511436462, Final Batch Loss: 0.43698787689208984\n",
      "Epoch 2185, Loss: 2.634483814239502, Final Batch Loss: 0.4861379861831665\n",
      "Epoch 2186, Loss: 5.569005787372589, Final Batch Loss: 3.4849956035614014\n",
      "Epoch 2187, Loss: 2.294137790799141, Final Batch Loss: 0.17632301151752472\n",
      "Epoch 2188, Loss: 2.4881504476070404, Final Batch Loss: 0.3848300874233246\n",
      "Epoch 2189, Loss: 2.198940020054579, Final Batch Loss: 0.02901778742671013\n",
      "Epoch 2190, Loss: 2.2691517919301987, Final Batch Loss: 0.18400143086910248\n",
      "Epoch 2191, Loss: 2.0680484026670456, Final Batch Loss: 0.016944751143455505\n",
      "Epoch 2192, Loss: 2.1374371238052845, Final Batch Loss: 0.06089206412434578\n",
      "Epoch 2193, Loss: 2.0581095796078444, Final Batch Loss: 0.01947706751525402\n",
      "Epoch 2194, Loss: 2.0368597577326, Final Batch Loss: 0.004509992431849241\n",
      "Epoch 2195, Loss: 2.2339696288108826, Final Batch Loss: 0.30837899446487427\n",
      "Epoch 2196, Loss: 2.178103893995285, Final Batch Loss: 0.16789141297340393\n",
      "Epoch 2197, Loss: 2.0663829795084894, Final Batch Loss: 0.004992281552404165\n",
      "Epoch 2198, Loss: 2.212594896554947, Final Batch Loss: 0.346841424703598\n",
      "Epoch 2199, Loss: 1.9529723229934461, Final Batch Loss: 0.000645429186988622\n",
      "Epoch 2200, Loss: 3.519427537918091, Final Batch Loss: 1.5378015041351318\n",
      "Epoch 2201, Loss: 2.095712937414646, Final Batch Loss: 0.10111043602228165\n",
      "Epoch 2202, Loss: 2.39782652258873, Final Batch Loss: 0.39126595854759216\n",
      "Epoch 2203, Loss: 3.4516430497169495, Final Batch Loss: 1.5853582620620728\n",
      "Epoch 2204, Loss: 1.9272220497950912, Final Batch Loss: 0.014245248399674892\n",
      "Epoch 2205, Loss: 1.9451827604789287, Final Batch Loss: 0.001179118873551488\n",
      "Epoch 2206, Loss: 1.9570158786373213, Final Batch Loss: 0.0016149348812177777\n",
      "Epoch 2207, Loss: 3.576331377029419, Final Batch Loss: 1.6324105262756348\n",
      "Epoch 2208, Loss: 1.911366512766108, Final Batch Loss: 0.0038601660635322332\n",
      "Epoch 2209, Loss: 2.0810824632644653, Final Batch Loss: 0.043434202671051025\n",
      "Epoch 2210, Loss: 2.064724273979664, Final Batch Loss: 0.06511307507753372\n",
      "Epoch 2211, Loss: 2.3463680148124695, Final Batch Loss: 0.304091215133667\n",
      "Epoch 2212, Loss: 2.0810773447155952, Final Batch Loss: 0.10811204463243484\n",
      "Epoch 2213, Loss: 3.9264872670173645, Final Batch Loss: 1.9927153587341309\n",
      "Epoch 2214, Loss: 2.7245985865592957, Final Batch Loss: 0.7820643782615662\n",
      "Epoch 2215, Loss: 2.7154805660247803, Final Batch Loss: 0.6926193237304688\n",
      "Epoch 2216, Loss: 2.781300723552704, Final Batch Loss: 0.8507943749427795\n",
      "Epoch 2217, Loss: 3.822409451007843, Final Batch Loss: 1.7027289867401123\n",
      "Epoch 2218, Loss: 2.2505545653402805, Final Batch Loss: 0.05342477932572365\n",
      "Epoch 2219, Loss: 2.1745156198740005, Final Batch Loss: 0.04707248508930206\n",
      "Epoch 2220, Loss: 2.0448615653440356, Final Batch Loss: 0.015218923799693584\n",
      "Epoch 2221, Loss: 2.094063922762871, Final Batch Loss: 0.0302036851644516\n",
      "Epoch 2222, Loss: 2.124707445502281, Final Batch Loss: 0.09561197459697723\n",
      "Epoch 2223, Loss: 1.903407471254468, Final Batch Loss: 0.01527668721973896\n",
      "Epoch 2224, Loss: 3.2378726601600647, Final Batch Loss: 1.2665296792984009\n",
      "Epoch 2225, Loss: 2.158465176820755, Final Batch Loss: 0.26510074734687805\n",
      "Epoch 2226, Loss: 2.5875036120414734, Final Batch Loss: 0.5249483585357666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2227, Loss: 2.7790363430976868, Final Batch Loss: 0.5167966485023499\n",
      "Epoch 2228, Loss: 2.3091602688655257, Final Batch Loss: 0.010027981363236904\n",
      "Epoch 2229, Loss: 3.4093525409698486, Final Batch Loss: 1.220625400543213\n",
      "Epoch 2230, Loss: 2.9520336389541626, Final Batch Loss: 0.8166908621788025\n",
      "Epoch 2231, Loss: 2.443139672279358, Final Batch Loss: 0.39562082290649414\n",
      "Epoch 2232, Loss: 2.2975711673498154, Final Batch Loss: 0.11078225076198578\n",
      "Epoch 2233, Loss: 2.1864594938233495, Final Batch Loss: 0.003412497229874134\n",
      "Epoch 2234, Loss: 2.185916006565094, Final Batch Loss: 0.051430463790893555\n",
      "Epoch 2235, Loss: 2.0643847920000553, Final Batch Loss: 0.050898466259241104\n",
      "Epoch 2236, Loss: 2.2352322340011597, Final Batch Loss: 0.1681310534477234\n",
      "Epoch 2237, Loss: 2.0760253872722387, Final Batch Loss: 0.019587775692343712\n",
      "Epoch 2238, Loss: 4.326110005378723, Final Batch Loss: 2.316486120223999\n",
      "Epoch 2239, Loss: 1.9757378697395325, Final Batch Loss: 0.06450515985488892\n",
      "Epoch 2240, Loss: 2.267022430896759, Final Batch Loss: 0.27449363470077515\n",
      "Epoch 2241, Loss: 2.078803963959217, Final Batch Loss: 0.0766187384724617\n",
      "Epoch 2242, Loss: 2.0322642382234335, Final Batch Loss: 0.01631808839738369\n",
      "Epoch 2243, Loss: 2.0511070638895035, Final Batch Loss: 0.19415082037448883\n",
      "Epoch 2244, Loss: 2.091468781232834, Final Batch Loss: 0.2611476480960846\n",
      "Epoch 2245, Loss: 3.3192814588546753, Final Batch Loss: 1.4017155170440674\n",
      "Epoch 2246, Loss: 2.541218727827072, Final Batch Loss: 0.49340131878852844\n",
      "Epoch 2247, Loss: 3.5426852703094482, Final Batch Loss: 1.5634878873825073\n",
      "Epoch 2248, Loss: 2.091762363910675, Final Batch Loss: 0.10119026899337769\n",
      "Epoch 2249, Loss: 2.641385078430176, Final Batch Loss: 0.6485862731933594\n",
      "Epoch 2250, Loss: 2.062366306781769, Final Batch Loss: 0.04107975959777832\n",
      "Epoch 2251, Loss: 3.3571685552597046, Final Batch Loss: 1.4203314781188965\n",
      "Epoch 2252, Loss: 4.151276350021362, Final Batch Loss: 2.073859214782715\n",
      "Epoch 2253, Loss: 2.00209471443668, Final Batch Loss: 0.004761903081089258\n",
      "Epoch 2254, Loss: 3.2036995887756348, Final Batch Loss: 1.328050971031189\n",
      "Epoch 2255, Loss: 1.9714214215055108, Final Batch Loss: 0.01073396299034357\n",
      "Epoch 2256, Loss: 2.0121692903339863, Final Batch Loss: 0.04252098873257637\n",
      "Epoch 2257, Loss: 5.80652928352356, Final Batch Loss: 3.913203716278076\n",
      "Epoch 2258, Loss: 1.983157366514206, Final Batch Loss: 0.04076221585273743\n",
      "Epoch 2259, Loss: 1.9957267781719565, Final Batch Loss: 0.0027341386303305626\n",
      "Epoch 2260, Loss: 2.369877576828003, Final Batch Loss: 0.2902522087097168\n",
      "Epoch 2261, Loss: 2.2365429997444153, Final Batch Loss: 0.220331609249115\n",
      "Epoch 2262, Loss: 3.8715492486953735, Final Batch Loss: 1.9837682247161865\n",
      "Epoch 2263, Loss: 2.323204070329666, Final Batch Loss: 0.3103164732456207\n",
      "Epoch 2264, Loss: 2.1794681549072266, Final Batch Loss: 0.10974204540252686\n",
      "Epoch 2265, Loss: 2.355953127145767, Final Batch Loss: 0.14466390013694763\n",
      "Epoch 2266, Loss: 3.687068819999695, Final Batch Loss: 1.4187723398208618\n",
      "Epoch 2267, Loss: 2.576387256383896, Final Batch Loss: 0.28186890482902527\n",
      "Epoch 2268, Loss: 2.2174859642982483, Final Batch Loss: 0.10615932941436768\n",
      "Epoch 2269, Loss: 2.712689220905304, Final Batch Loss: 0.6686539649963379\n",
      "Epoch 2270, Loss: 2.1019528140313923, Final Batch Loss: 0.007583283353596926\n",
      "Epoch 2271, Loss: 3.759726583957672, Final Batch Loss: 1.7775170803070068\n",
      "Epoch 2272, Loss: 2.2652818262577057, Final Batch Loss: 0.1907190978527069\n",
      "Epoch 2273, Loss: 1.9224955290555954, Final Batch Loss: 0.03355793654918671\n",
      "Epoch 2274, Loss: 2.2528885900974274, Final Batch Loss: 0.34896430373191833\n",
      "Epoch 2275, Loss: 2.751055419445038, Final Batch Loss: 0.7964895367622375\n",
      "Epoch 2276, Loss: 2.3384049236774445, Final Batch Loss: 0.435791939496994\n",
      "Epoch 2277, Loss: 2.101109504699707, Final Batch Loss: 0.17716634273529053\n",
      "Epoch 2278, Loss: 2.3698345124721527, Final Batch Loss: 0.4466429650783539\n",
      "Epoch 2279, Loss: 2.048644170165062, Final Batch Loss: 0.17139087617397308\n",
      "Epoch 2280, Loss: 3.312084436416626, Final Batch Loss: 1.399564266204834\n",
      "Epoch 2281, Loss: 2.08858922123909, Final Batch Loss: 0.1683468520641327\n",
      "Epoch 2282, Loss: 1.9846307784318924, Final Batch Loss: 0.01876296103000641\n",
      "Epoch 2283, Loss: 2.9490593671798706, Final Batch Loss: 0.9863532781600952\n",
      "Epoch 2284, Loss: 2.4619317054748535, Final Batch Loss: 0.5486698150634766\n",
      "Epoch 2285, Loss: 1.9794412292540073, Final Batch Loss: 0.03041810169816017\n",
      "Epoch 2286, Loss: 1.9750562086701393, Final Batch Loss: 0.03342340141534805\n",
      "Epoch 2287, Loss: 2.5973748564720154, Final Batch Loss: 0.509698748588562\n",
      "Epoch 2288, Loss: 2.005846118554473, Final Batch Loss: 0.023453092202544212\n",
      "Epoch 2289, Loss: 1.9892742950469255, Final Batch Loss: 0.01599070616066456\n",
      "Epoch 2290, Loss: 1.9878505561500788, Final Batch Loss: 0.016350338235497475\n",
      "Epoch 2291, Loss: 2.079766094684601, Final Batch Loss: 0.22757089138031006\n",
      "Epoch 2292, Loss: 2.0431890711188316, Final Batch Loss: 0.06612171977758408\n",
      "Epoch 2293, Loss: 3.87966525554657, Final Batch Loss: 1.9517775774002075\n",
      "Epoch 2294, Loss: 1.9344685524702072, Final Batch Loss: 0.08854620158672333\n",
      "Epoch 2295, Loss: 1.909719180315733, Final Batch Loss: 0.0363696925342083\n",
      "Epoch 2296, Loss: 2.1673504114151, Final Batch Loss: 0.3208850622177124\n",
      "Epoch 2297, Loss: 1.989539884030819, Final Batch Loss: 0.08615650981664658\n",
      "Epoch 2298, Loss: 3.8358770608901978, Final Batch Loss: 1.9506657123565674\n",
      "Epoch 2299, Loss: 1.9705396816134453, Final Batch Loss: 0.09377371519804001\n",
      "Epoch 2300, Loss: 2.150658831000328, Final Batch Loss: 0.177915558218956\n",
      "Epoch 2301, Loss: 2.281556397676468, Final Batch Loss: 0.4095439016819\n",
      "Epoch 2302, Loss: 1.8770776391029358, Final Batch Loss: 0.0266454815864563\n",
      "Epoch 2303, Loss: 1.9941802771645598, Final Batch Loss: 0.0004563482361845672\n",
      "Epoch 2304, Loss: 4.248515069484711, Final Batch Loss: 2.3474934101104736\n",
      "Epoch 2305, Loss: 3.981998562812805, Final Batch Loss: 2.1978423595428467\n",
      "Epoch 2306, Loss: 1.9527759714983404, Final Batch Loss: 0.0002942844294011593\n",
      "Epoch 2307, Loss: 3.040628135204315, Final Batch Loss: 1.1984726190567017\n",
      "Epoch 2308, Loss: 2.2920016050338745, Final Batch Loss: 0.3598058223724365\n",
      "Epoch 2309, Loss: 1.8610593462362885, Final Batch Loss: 0.002995767630636692\n",
      "Epoch 2310, Loss: 2.0215853042900562, Final Batch Loss: 0.054051775485277176\n",
      "Epoch 2311, Loss: 2.0769364312291145, Final Batch Loss: 0.12284470349550247\n",
      "Epoch 2312, Loss: 2.722842812538147, Final Batch Loss: 0.8395799994468689\n",
      "Epoch 2313, Loss: 2.097611427307129, Final Batch Loss: 0.18328791856765747\n",
      "Epoch 2314, Loss: 4.305157780647278, Final Batch Loss: 2.4086830615997314\n",
      "Epoch 2315, Loss: 3.8268434405326843, Final Batch Loss: 1.8302217721939087\n",
      "Epoch 2316, Loss: 2.0373319685459137, Final Batch Loss: 0.1424904763698578\n",
      "Epoch 2317, Loss: 2.5845491886138916, Final Batch Loss: 0.6841354370117188\n",
      "Epoch 2318, Loss: 1.9962627291679382, Final Batch Loss: 0.09995204210281372\n",
      "Epoch 2319, Loss: 1.9206222887951299, Final Batch Loss: 2.777537883957848e-05\n",
      "Epoch 2320, Loss: 2.0780161321163177, Final Batch Loss: 0.1388052999973297\n",
      "Epoch 2321, Loss: 1.9358115997165442, Final Batch Loss: 0.0165115799754858\n",
      "Epoch 2322, Loss: 1.855544164776802, Final Batch Loss: 0.05276323854923248\n",
      "Epoch 2323, Loss: 1.9263782538473606, Final Batch Loss: 0.028145436197519302\n",
      "Epoch 2324, Loss: 2.7434775829315186, Final Batch Loss: 0.9000247120857239\n",
      "Epoch 2325, Loss: 2.0356797203421593, Final Batch Loss: 0.040386758744716644\n",
      "Epoch 2326, Loss: 2.550382539629936, Final Batch Loss: 0.13615094125270844\n",
      "Epoch 2327, Loss: 3.1390140652656555, Final Batch Loss: 0.6411503553390503\n",
      "Epoch 2328, Loss: 2.4041699558729306, Final Batch Loss: 0.000631848000921309\n",
      "Epoch 2329, Loss: 2.5094359815120697, Final Batch Loss: 0.19174978137016296\n",
      "Epoch 2330, Loss: 2.37155345082283, Final Batch Loss: 0.19545498490333557\n",
      "Epoch 2331, Loss: 4.38553112745285, Final Batch Loss: 2.29958438873291\n",
      "Epoch 2332, Loss: 2.4971646666526794, Final Batch Loss: 0.5119407773017883\n",
      "Epoch 2333, Loss: 2.3393483236432076, Final Batch Loss: 0.10633683949708939\n",
      "Epoch 2334, Loss: 2.544468938402133, Final Batch Loss: 4.2437604861333966e-05\n",
      "Epoch 2335, Loss: 2.482916980981827, Final Batch Loss: 0.09821382164955139\n",
      "Epoch 2336, Loss: 3.0201606154441833, Final Batch Loss: 0.8508578538894653\n",
      "Epoch 2337, Loss: 2.357051193714142, Final Batch Loss: 0.3045991063117981\n",
      "Epoch 2338, Loss: 2.0389046892523766, Final Batch Loss: 0.09709306806325912\n",
      "Epoch 2339, Loss: 2.5504807829856873, Final Batch Loss: 0.5420878529548645\n",
      "Epoch 2340, Loss: 2.171967662870884, Final Batch Loss: 0.11192486435174942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2341, Loss: 1.9538922203646507, Final Batch Loss: 0.00014578233822248876\n",
      "Epoch 2342, Loss: 3.8717028498649597, Final Batch Loss: 1.9666588306427002\n",
      "Epoch 2343, Loss: 2.250246748328209, Final Batch Loss: 0.24016927182674408\n",
      "Epoch 2344, Loss: 1.9253696617670357, Final Batch Loss: 0.005291622597724199\n",
      "Epoch 2345, Loss: 3.6667131185531616, Final Batch Loss: 1.8747191429138184\n",
      "Epoch 2346, Loss: 3.7538170218467712, Final Batch Loss: 1.8106069564819336\n",
      "Epoch 2347, Loss: 2.669754981994629, Final Batch Loss: 0.744848370552063\n",
      "Epoch 2348, Loss: 3.015547275543213, Final Batch Loss: 1.00092351436615\n",
      "Epoch 2349, Loss: 2.680987298488617, Final Batch Loss: 0.6292793154716492\n",
      "Epoch 2350, Loss: 2.3649982213974, Final Batch Loss: 0.08860141038894653\n",
      "Epoch 2351, Loss: 2.1620004437863827, Final Batch Loss: 0.034434106200933456\n",
      "Epoch 2352, Loss: 2.2342884987592697, Final Batch Loss: 0.19391272962093353\n",
      "Epoch 2353, Loss: 3.1576807498931885, Final Batch Loss: 1.0100276470184326\n",
      "Epoch 2354, Loss: 2.4818976521492004, Final Batch Loss: 0.3470813035964966\n",
      "Epoch 2355, Loss: 2.120135720819235, Final Batch Loss: 0.019959088414907455\n",
      "Epoch 2356, Loss: 4.095698535442352, Final Batch Loss: 2.014594078063965\n",
      "Epoch 2357, Loss: 3.991955041885376, Final Batch Loss: 1.8833632469177246\n",
      "Epoch 2358, Loss: 3.1574016213417053, Final Batch Loss: 1.25430166721344\n",
      "Epoch 2359, Loss: 2.375398635864258, Final Batch Loss: 0.41298067569732666\n",
      "Epoch 2360, Loss: 2.1706280410289764, Final Batch Loss: 0.18170508742332458\n",
      "Epoch 2361, Loss: 3.769128441810608, Final Batch Loss: 1.7310892343521118\n",
      "Epoch 2362, Loss: 3.9937912225723267, Final Batch Loss: 1.8577213287353516\n",
      "Epoch 2363, Loss: 2.632242798805237, Final Batch Loss: 0.62810218334198\n",
      "Epoch 2364, Loss: 2.8654705286026, Final Batch Loss: 0.9254034757614136\n",
      "Epoch 2365, Loss: 1.9839123077690601, Final Batch Loss: 0.024190504103899002\n",
      "Epoch 2366, Loss: 3.6786181330680847, Final Batch Loss: 1.716536045074463\n",
      "Epoch 2367, Loss: 2.8253191113471985, Final Batch Loss: 0.8164753913879395\n",
      "Epoch 2368, Loss: 2.9306458234786987, Final Batch Loss: 0.8314171433448792\n",
      "Epoch 2369, Loss: 2.7197113633155823, Final Batch Loss: 0.8077125549316406\n",
      "Epoch 2370, Loss: 2.065619744360447, Final Batch Loss: 0.07955757528543472\n",
      "Epoch 2371, Loss: 2.076574932783842, Final Batch Loss: 0.016454827040433884\n",
      "Epoch 2372, Loss: 2.023524835705757, Final Batch Loss: 0.11392934620380402\n",
      "Epoch 2373, Loss: 2.849788784980774, Final Batch Loss: 0.9478422403335571\n",
      "Epoch 2374, Loss: 2.245775818824768, Final Batch Loss: 0.3427719473838806\n",
      "Epoch 2375, Loss: 2.024601586163044, Final Batch Loss: 0.07481522113084793\n",
      "Epoch 2376, Loss: 2.2356865108013153, Final Batch Loss: 0.31348857283592224\n",
      "Epoch 2377, Loss: 4.137363851070404, Final Batch Loss: 2.097008228302002\n",
      "Epoch 2378, Loss: 2.2407307624816895, Final Batch Loss: 0.1699199676513672\n",
      "Epoch 2379, Loss: 2.116384267807007, Final Batch Loss: 0.1413225531578064\n",
      "Epoch 2380, Loss: 3.494249701499939, Final Batch Loss: 1.5056458711624146\n",
      "Epoch 2381, Loss: 2.6974956393241882, Final Batch Loss: 0.747269868850708\n",
      "Epoch 2382, Loss: 2.2457980811595917, Final Batch Loss: 0.24802622199058533\n",
      "Epoch 2383, Loss: 2.091992676258087, Final Batch Loss: 0.1863294243812561\n",
      "Epoch 2384, Loss: 4.029167115688324, Final Batch Loss: 2.1769094467163086\n",
      "Epoch 2385, Loss: 3.715639650821686, Final Batch Loss: 1.6327153444290161\n",
      "Epoch 2386, Loss: 3.56753009557724, Final Batch Loss: 1.295796513557434\n",
      "Epoch 2387, Loss: 3.9259026050567627, Final Batch Loss: 1.4156017303466797\n",
      "Epoch 2388, Loss: 3.333111524581909, Final Batch Loss: 1.0186684131622314\n",
      "Epoch 2389, Loss: 3.419052839279175, Final Batch Loss: 1.408564567565918\n",
      "Epoch 2390, Loss: 3.6112058758735657, Final Batch Loss: 1.6640970706939697\n",
      "Epoch 2391, Loss: 4.438408136367798, Final Batch Loss: 2.471343755722046\n",
      "Epoch 2392, Loss: 3.327124536037445, Final Batch Loss: 1.3880469799041748\n",
      "Epoch 2393, Loss: 3.3272393941879272, Final Batch Loss: 1.383274793624878\n",
      "Epoch 2394, Loss: 2.238669604063034, Final Batch Loss: 0.2356908619403839\n",
      "Epoch 2395, Loss: 2.0557591505348682, Final Batch Loss: 0.021923918277025223\n",
      "Epoch 2396, Loss: 3.526923418045044, Final Batch Loss: 1.51461923122406\n",
      "Epoch 2397, Loss: 2.1654564142227173, Final Batch Loss: 0.25165194272994995\n",
      "Epoch 2398, Loss: 4.774461984634399, Final Batch Loss: 2.7974610328674316\n",
      "Epoch 2399, Loss: 3.62463641166687, Final Batch Loss: 1.7670841217041016\n",
      "Epoch 2400, Loss: 2.7630316615104675, Final Batch Loss: 0.84233558177948\n",
      "Epoch 2401, Loss: 3.585849642753601, Final Batch Loss: 1.570819616317749\n",
      "Epoch 2402, Loss: 2.221900776028633, Final Batch Loss: 0.1607387810945511\n",
      "Epoch 2403, Loss: 2.741183876991272, Final Batch Loss: 0.5254554152488708\n",
      "Epoch 2404, Loss: 2.1544294580817223, Final Batch Loss: 0.08282484859228134\n",
      "Epoch 2405, Loss: 2.1897753328084946, Final Batch Loss: 0.14641956984996796\n",
      "Epoch 2406, Loss: 2.1438506692647934, Final Batch Loss: 0.16463105380535126\n",
      "Epoch 2407, Loss: 2.0096906796097755, Final Batch Loss: 0.04174003750085831\n",
      "Epoch 2408, Loss: 2.304886132478714, Final Batch Loss: 0.3171069324016571\n",
      "Epoch 2409, Loss: 1.967956654727459, Final Batch Loss: 0.06352787464857101\n",
      "Epoch 2410, Loss: 1.8295222618617117, Final Batch Loss: 0.004180858377367258\n",
      "Epoch 2411, Loss: 1.8743740068748593, Final Batch Loss: 0.0025692330673336983\n",
      "Epoch 2412, Loss: 2.0547169893980026, Final Batch Loss: 0.23043625056743622\n",
      "Epoch 2413, Loss: 1.9183427039533854, Final Batch Loss: 0.023166710510849953\n",
      "Epoch 2414, Loss: 2.1138668954372406, Final Batch Loss: 0.26733770966529846\n",
      "Epoch 2415, Loss: 1.9128202237188816, Final Batch Loss: 0.05732328072190285\n",
      "Epoch 2416, Loss: 1.9423191957175732, Final Batch Loss: 0.049283068627119064\n",
      "Epoch 2417, Loss: 1.898421298712492, Final Batch Loss: 0.06098134443163872\n",
      "Epoch 2418, Loss: 1.8344774022698402, Final Batch Loss: 0.07831660658121109\n",
      "Epoch 2419, Loss: 3.735836923122406, Final Batch Loss: 1.9156094789505005\n",
      "Epoch 2420, Loss: 1.9290429591201246, Final Batch Loss: 0.007328295614570379\n",
      "Epoch 2421, Loss: 1.7935394807427656, Final Batch Loss: 0.00045372682507149875\n",
      "Epoch 2422, Loss: 2.5420955419540405, Final Batch Loss: 0.7310106754302979\n",
      "Epoch 2423, Loss: 3.91314560174942, Final Batch Loss: 2.0696394443511963\n",
      "Epoch 2424, Loss: 2.5273622274398804, Final Batch Loss: 0.6313406229019165\n",
      "Epoch 2425, Loss: 2.1117803379893303, Final Batch Loss: 0.03890632838010788\n",
      "Epoch 2426, Loss: 2.0434569008648396, Final Batch Loss: 0.05592184141278267\n",
      "Epoch 2427, Loss: 2.229452759027481, Final Batch Loss: 0.2213754951953888\n",
      "Epoch 2428, Loss: 1.9598996676504612, Final Batch Loss: 0.012322131544351578\n",
      "Epoch 2429, Loss: 2.41632804274559, Final Batch Loss: 0.4033749997615814\n",
      "Epoch 2430, Loss: 3.785383462905884, Final Batch Loss: 1.94196355342865\n",
      "Epoch 2431, Loss: 2.0144463777542114, Final Batch Loss: 0.13054543733596802\n",
      "Epoch 2432, Loss: 2.324975311756134, Final Batch Loss: 0.40531647205352783\n",
      "Epoch 2433, Loss: 3.5798164010047913, Final Batch Loss: 1.525978922843933\n",
      "Epoch 2434, Loss: 2.2795298099517822, Final Batch Loss: 0.3678722381591797\n",
      "Epoch 2435, Loss: 1.945204334333539, Final Batch Loss: 0.0071901846677064896\n",
      "Epoch 2436, Loss: 1.9488794822245836, Final Batch Loss: 0.012770773842930794\n",
      "Epoch 2437, Loss: 3.8687241077423096, Final Batch Loss: 1.827772617340088\n",
      "Epoch 2438, Loss: 3.4045042991638184, Final Batch Loss: 1.5529338121414185\n",
      "Epoch 2439, Loss: 2.0165989845991135, Final Batch Loss: 0.04572390019893646\n",
      "Epoch 2440, Loss: 1.967395487241447, Final Batch Loss: 0.006784143857657909\n",
      "Epoch 2441, Loss: 1.921729102730751, Final Batch Loss: 0.08237285912036896\n",
      "Epoch 2442, Loss: 2.1269527971744537, Final Batch Loss: 0.23553761839866638\n",
      "Epoch 2443, Loss: 1.9351362667512149, Final Batch Loss: 0.0017210922669619322\n",
      "Epoch 2444, Loss: 1.9922254085540771, Final Batch Loss: 0.14319133758544922\n",
      "Epoch 2445, Loss: 1.9221156314015388, Final Batch Loss: 0.06575734168291092\n",
      "Epoch 2446, Loss: 1.910871461033821, Final Batch Loss: 0.054209306836128235\n",
      "Epoch 2447, Loss: 2.352039396762848, Final Batch Loss: 0.44745874404907227\n",
      "Epoch 2448, Loss: 2.876061499118805, Final Batch Loss: 1.0614546537399292\n",
      "Epoch 2449, Loss: 1.8364497423171997, Final Batch Loss: 0.05354195833206177\n",
      "Epoch 2450, Loss: 3.762584924697876, Final Batch Loss: 1.8740565776824951\n",
      "Epoch 2451, Loss: 3.1037270426750183, Final Batch Loss: 1.2096736431121826\n",
      "Epoch 2452, Loss: 1.8300763070583344, Final Batch Loss: 0.03363505005836487\n",
      "Epoch 2453, Loss: 3.655513882637024, Final Batch Loss: 1.8720580339431763\n",
      "Epoch 2454, Loss: 6.23684948682785, Final Batch Loss: 4.3458333015441895\n",
      "Epoch 2455, Loss: 1.9260690826922655, Final Batch Loss: 0.022327007725834846\n",
      "Epoch 2456, Loss: 1.8901419414905831, Final Batch Loss: 0.0004051103023812175\n",
      "Epoch 2457, Loss: 2.4123960733413696, Final Batch Loss: 0.4140467047691345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2458, Loss: 2.1914601624011993, Final Batch Loss: 0.20094218850135803\n",
      "Epoch 2459, Loss: 3.721309244632721, Final Batch Loss: 1.781976342201233\n",
      "Epoch 2460, Loss: 2.490277051925659, Final Batch Loss: 0.6049076914787292\n",
      "Epoch 2461, Loss: 2.3811333775520325, Final Batch Loss: 0.5000473856925964\n",
      "Epoch 2462, Loss: 1.9128969758749008, Final Batch Loss: 0.17589227855205536\n",
      "Epoch 2463, Loss: 1.9770160391926765, Final Batch Loss: 0.06668973714113235\n",
      "Epoch 2464, Loss: 2.3303820490837097, Final Batch Loss: 0.43016767501831055\n",
      "Epoch 2465, Loss: 1.9343103167520894, Final Batch Loss: 2.396077979938127e-05\n",
      "Epoch 2466, Loss: 2.250996559858322, Final Batch Loss: 0.35071393847465515\n",
      "Epoch 2467, Loss: 2.1785721480846405, Final Batch Loss: 0.32359036803245544\n",
      "Epoch 2468, Loss: 1.805776808410883, Final Batch Loss: 0.030576858669519424\n",
      "Epoch 2469, Loss: 2.02824504673481, Final Batch Loss: 0.17281140387058258\n",
      "Epoch 2470, Loss: 3.1741307377815247, Final Batch Loss: 1.4174494743347168\n",
      "Epoch 2471, Loss: 4.200962007045746, Final Batch Loss: 2.2895240783691406\n",
      "Epoch 2472, Loss: 1.9638195484876633, Final Batch Loss: 0.11694960296154022\n",
      "Epoch 2473, Loss: 1.826318096369505, Final Batch Loss: 0.054779838770627975\n",
      "Epoch 2474, Loss: 2.530654489994049, Final Batch Loss: 0.687183678150177\n",
      "Epoch 2475, Loss: 2.1123871356248856, Final Batch Loss: 0.21020109951496124\n",
      "Epoch 2476, Loss: 2.290449410676956, Final Batch Loss: 0.46116676926612854\n",
      "Epoch 2477, Loss: 1.8311086993198842, Final Batch Loss: 0.0029451351147145033\n",
      "Epoch 2478, Loss: 2.0918359756469727, Final Batch Loss: 0.24098312854766846\n",
      "Epoch 2479, Loss: 1.9502073228359222, Final Batch Loss: 0.15996864438056946\n",
      "Epoch 2480, Loss: 1.936299178749323, Final Batch Loss: 0.061758849769830704\n",
      "Epoch 2481, Loss: 2.2895328402519226, Final Batch Loss: 0.46069973707199097\n",
      "Epoch 2482, Loss: 1.9198501482605934, Final Batch Loss: 0.07188425213098526\n",
      "Epoch 2483, Loss: 3.640975594520569, Final Batch Loss: 1.836207389831543\n",
      "Epoch 2484, Loss: 1.8630892932415009, Final Batch Loss: 0.1555033028125763\n",
      "Epoch 2485, Loss: 1.8361729509197176, Final Batch Loss: 0.003990901168435812\n",
      "Epoch 2486, Loss: 2.2427442967891693, Final Batch Loss: 0.46045663952827454\n",
      "Epoch 2487, Loss: 1.9452434703707695, Final Batch Loss: 0.09099506586790085\n",
      "Epoch 2488, Loss: 2.2364509105682373, Final Batch Loss: 0.42884063720703125\n",
      "Epoch 2489, Loss: 3.6145747303962708, Final Batch Loss: 1.7474093437194824\n",
      "Epoch 2490, Loss: 1.7698175869882107, Final Batch Loss: 0.06103461608290672\n",
      "Epoch 2491, Loss: 3.122696101665497, Final Batch Loss: 1.3321112394332886\n",
      "Epoch 2492, Loss: 2.0212198197841644, Final Batch Loss: 0.25955548882484436\n",
      "Epoch 2493, Loss: 3.5662325620651245, Final Batch Loss: 1.7835875749588013\n",
      "Epoch 2494, Loss: 4.033840358257294, Final Batch Loss: 2.203082323074341\n",
      "Epoch 2495, Loss: 1.7447033561766148, Final Batch Loss: 0.038012150675058365\n",
      "Epoch 2496, Loss: 3.366058588027954, Final Batch Loss: 1.5916121006011963\n",
      "Epoch 2497, Loss: 3.3303285241127014, Final Batch Loss: 1.522613525390625\n",
      "Epoch 2498, Loss: 2.0349511206150055, Final Batch Loss: 0.07958784699440002\n",
      "Epoch 2499, Loss: 2.132112018764019, Final Batch Loss: 0.07297664135694504\n",
      "Epoch 2500, Loss: 2.315076358616352, Final Batch Loss: 0.034522898495197296\n",
      "Epoch 2501, Loss: 3.728521406650543, Final Batch Loss: 1.5748536586761475\n",
      "Epoch 2502, Loss: 3.020511269569397, Final Batch Loss: 0.994920015335083\n",
      "Epoch 2503, Loss: 3.915862798690796, Final Batch Loss: 1.8764235973358154\n",
      "Epoch 2504, Loss: 2.2705863416194916, Final Batch Loss: 0.3925616443157196\n",
      "Epoch 2505, Loss: 3.7291088104248047, Final Batch Loss: 1.7888684272766113\n",
      "Epoch 2506, Loss: 2.388766795396805, Final Batch Loss: 0.4032839834690094\n",
      "Epoch 2507, Loss: 1.8627543114125729, Final Batch Loss: 0.010280158370733261\n",
      "Epoch 2508, Loss: 2.024701938033104, Final Batch Loss: 0.1264936774969101\n",
      "Epoch 2509, Loss: 1.8779541198164225, Final Batch Loss: 0.007968658581376076\n",
      "Epoch 2510, Loss: 1.8438488668762147, Final Batch Loss: 0.005302650388330221\n",
      "Epoch 2511, Loss: 1.9433543756604195, Final Batch Loss: 0.07799655944108963\n",
      "Epoch 2512, Loss: 2.005666173994541, Final Batch Loss: 0.08360288292169571\n",
      "Epoch 2513, Loss: 1.803836777806282, Final Batch Loss: 0.08856387436389923\n",
      "Epoch 2514, Loss: 2.2349075973033905, Final Batch Loss: 0.4761179983615875\n",
      "Epoch 2515, Loss: 1.8750252649188042, Final Batch Loss: 0.05601077526807785\n",
      "Epoch 2516, Loss: 5.095117270946503, Final Batch Loss: 3.346412181854248\n",
      "Epoch 2517, Loss: 1.8139194883406162, Final Batch Loss: 0.04138827696442604\n",
      "Epoch 2518, Loss: 2.1886730436235666, Final Batch Loss: 0.02796032465994358\n",
      "Epoch 2519, Loss: 2.3189270850270987, Final Batch Loss: 0.021537935361266136\n",
      "Epoch 2520, Loss: 2.436182299628854, Final Batch Loss: 0.017675617709755898\n",
      "Epoch 2521, Loss: 3.180386960506439, Final Batch Loss: 0.756234884262085\n",
      "Epoch 2522, Loss: 3.3932559490203857, Final Batch Loss: 1.1156201362609863\n",
      "Epoch 2523, Loss: 2.420371875166893, Final Batch Loss: 0.14291660487651825\n",
      "Epoch 2524, Loss: 3.8537163138389587, Final Batch Loss: 1.523127794265747\n",
      "Epoch 2525, Loss: 2.2738104024901986, Final Batch Loss: 0.003157750703394413\n",
      "Epoch 2526, Loss: 2.8573792576789856, Final Batch Loss: 0.6873779892921448\n",
      "Epoch 2527, Loss: 3.7532528042793274, Final Batch Loss: 1.593498706817627\n",
      "Epoch 2528, Loss: 2.08776977728121, Final Batch Loss: 0.0028140253853052855\n",
      "Epoch 2529, Loss: 1.9898826481075957, Final Batch Loss: 0.0003449321957305074\n",
      "Epoch 2530, Loss: 3.9046987295150757, Final Batch Loss: 1.9009885787963867\n",
      "Epoch 2531, Loss: 2.042349066119641, Final Batch Loss: 0.0033734100870788097\n",
      "Epoch 2532, Loss: 2.0265736175933853, Final Batch Loss: 0.0013224674621596932\n",
      "Epoch 2533, Loss: 2.2731057703495026, Final Batch Loss: 0.40394356846809387\n",
      "Epoch 2534, Loss: 2.591165006160736, Final Batch Loss: 0.6769370436668396\n",
      "Epoch 2535, Loss: 2.0341498106718063, Final Batch Loss: 0.12592248618602753\n",
      "Epoch 2536, Loss: 3.516477406024933, Final Batch Loss: 1.672251582145691\n",
      "Epoch 2537, Loss: 2.440329611301422, Final Batch Loss: 0.6283103227615356\n",
      "Epoch 2538, Loss: 1.927583783864975, Final Batch Loss: 0.05126705765724182\n",
      "Epoch 2539, Loss: 3.374695122241974, Final Batch Loss: 1.4955165386199951\n",
      "Epoch 2540, Loss: 2.311415910720825, Final Batch Loss: 0.3774033784866333\n",
      "Epoch 2541, Loss: 2.889816701412201, Final Batch Loss: 1.0608727931976318\n",
      "Epoch 2542, Loss: 2.970598042011261, Final Batch Loss: 1.1347736120224\n",
      "Epoch 2543, Loss: 3.3578035831451416, Final Batch Loss: 1.562018871307373\n",
      "Epoch 2544, Loss: 3.7509161233901978, Final Batch Loss: 1.7688261270523071\n",
      "Epoch 2545, Loss: 4.01566755771637, Final Batch Loss: 2.073413372039795\n",
      "Epoch 2546, Loss: 2.4661433696746826, Final Batch Loss: 0.5319547653198242\n",
      "Epoch 2547, Loss: 1.928989291191101, Final Batch Loss: 0.06415796279907227\n",
      "Epoch 2548, Loss: 2.5509058833122253, Final Batch Loss: 0.7128332853317261\n",
      "Epoch 2549, Loss: 1.8989906832575798, Final Batch Loss: 0.06582330912351608\n",
      "Epoch 2550, Loss: 2.556862235069275, Final Batch Loss: 0.7184290289878845\n",
      "Epoch 2551, Loss: 1.8825257252901793, Final Batch Loss: 0.019947869703173637\n",
      "Epoch 2552, Loss: 1.888497905805707, Final Batch Loss: 0.017437951639294624\n",
      "Epoch 2553, Loss: 2.753381371498108, Final Batch Loss: 0.8984304666519165\n",
      "Epoch 2554, Loss: 4.588955104351044, Final Batch Loss: 2.708390951156616\n",
      "Epoch 2555, Loss: 3.0646015405654907, Final Batch Loss: 1.240112066268921\n",
      "Epoch 2556, Loss: 1.8789457008242607, Final Batch Loss: 0.03976350277662277\n",
      "Epoch 2557, Loss: 1.9626356065273285, Final Batch Loss: 0.1517680585384369\n",
      "Epoch 2558, Loss: 1.849561057984829, Final Batch Loss: 0.0030599460005760193\n",
      "Epoch 2559, Loss: 2.2919352054595947, Final Batch Loss: 0.37429606914520264\n",
      "Epoch 2560, Loss: 1.8408446982502937, Final Batch Loss: 0.0353776291012764\n",
      "Epoch 2561, Loss: 2.7795063853263855, Final Batch Loss: 0.9449133276939392\n",
      "Epoch 2562, Loss: 2.2927028834819794, Final Batch Loss: 0.4875495135784149\n",
      "Epoch 2563, Loss: 2.822562575340271, Final Batch Loss: 0.9310439825057983\n",
      "Epoch 2564, Loss: 1.8804424032568932, Final Batch Loss: 0.03270211070775986\n",
      "Epoch 2565, Loss: 2.0701042115688324, Final Batch Loss: 0.2761426270008087\n",
      "Epoch 2566, Loss: 1.8911675699055195, Final Batch Loss: 0.011430371552705765\n",
      "Epoch 2567, Loss: 2.145300805568695, Final Batch Loss: 0.33821582794189453\n",
      "Epoch 2568, Loss: 1.8907217215746641, Final Batch Loss: 0.02887163870036602\n",
      "Epoch 2569, Loss: 1.9542904198169708, Final Batch Loss: 0.17996135354042053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2570, Loss: 1.9196746945381165, Final Batch Loss: 0.1197049617767334\n",
      "Epoch 2571, Loss: 1.7969932686537504, Final Batch Loss: 0.006187094375491142\n",
      "Epoch 2572, Loss: 1.862030129879713, Final Batch Loss: 0.0524316243827343\n",
      "Epoch 2573, Loss: 2.3209055364131927, Final Batch Loss: 0.48048725724220276\n",
      "Epoch 2574, Loss: 2.2723376750946045, Final Batch Loss: 0.47295433282852173\n",
      "Epoch 2575, Loss: 3.7833378314971924, Final Batch Loss: 2.046487331390381\n",
      "Epoch 2576, Loss: 3.127788007259369, Final Batch Loss: 1.3555634021759033\n",
      "Epoch 2577, Loss: 1.9420699328184128, Final Batch Loss: 0.07431654632091522\n",
      "Epoch 2578, Loss: 2.2476484179496765, Final Batch Loss: 0.3465518355369568\n",
      "Epoch 2579, Loss: 2.086697505787015, Final Batch Loss: 0.02297799475491047\n",
      "Epoch 2580, Loss: 2.0140698850154877, Final Batch Loss: 0.08606758713722229\n",
      "Epoch 2581, Loss: 1.987782696262002, Final Batch Loss: 0.005661049857735634\n",
      "Epoch 2582, Loss: 1.91554843634367, Final Batch Loss: 0.02325557917356491\n",
      "Epoch 2583, Loss: 2.352146863937378, Final Batch Loss: 0.3637125492095947\n",
      "Epoch 2584, Loss: 3.2219126224517822, Final Batch Loss: 1.402574062347412\n",
      "Epoch 2585, Loss: 2.014125883579254, Final Batch Loss: 0.18701505661010742\n",
      "Epoch 2586, Loss: 1.7864192984998226, Final Batch Loss: 0.021912839263677597\n",
      "Epoch 2587, Loss: 2.2464858293533325, Final Batch Loss: 0.46585679054260254\n",
      "Epoch 2588, Loss: 2.1432294249534607, Final Batch Loss: 0.39808350801467896\n",
      "Epoch 2589, Loss: 3.1778196692466736, Final Batch Loss: 1.215916633605957\n",
      "Epoch 2590, Loss: 2.0910071283578873, Final Batch Loss: 0.15656723082065582\n",
      "Epoch 2591, Loss: 2.2226655781269073, Final Batch Loss: 0.38875457644462585\n",
      "Epoch 2592, Loss: 2.250103235244751, Final Batch Loss: 0.3276602029800415\n",
      "Epoch 2593, Loss: 3.7442010045051575, Final Batch Loss: 1.8610752820968628\n",
      "Epoch 2594, Loss: 1.9098777477629483, Final Batch Loss: 0.004957288969308138\n",
      "Epoch 2595, Loss: 1.939363770186901, Final Batch Loss: 0.07266341894865036\n",
      "Epoch 2596, Loss: 2.0714541152119637, Final Batch Loss: 0.05245775729417801\n",
      "Epoch 2597, Loss: 3.533272922039032, Final Batch Loss: 1.5787289142608643\n",
      "Epoch 2598, Loss: 1.9003585539758205, Final Batch Loss: 0.02454661950469017\n",
      "Epoch 2599, Loss: 1.9367894381284714, Final Batch Loss: 0.1160411685705185\n",
      "Epoch 2600, Loss: 1.9423604235053062, Final Batch Loss: 0.06937261670827866\n",
      "Epoch 2601, Loss: 2.0421914607286453, Final Batch Loss: 0.2390362173318863\n",
      "Epoch 2602, Loss: 1.8081719202018576, Final Batch Loss: 0.00012432756193447858\n",
      "Epoch 2603, Loss: 1.8187881745398045, Final Batch Loss: 0.04168767109513283\n",
      "Epoch 2604, Loss: 1.901616184040904, Final Batch Loss: 0.021367816254496574\n",
      "Epoch 2605, Loss: 3.2731295824050903, Final Batch Loss: 1.5547209978103638\n",
      "Epoch 2606, Loss: 4.038511335849762, Final Batch Loss: 2.2218728065490723\n",
      "Epoch 2607, Loss: 1.7608099738135934, Final Batch Loss: 0.007123903371393681\n",
      "Epoch 2608, Loss: 1.8333280496299267, Final Batch Loss: 0.018160145729780197\n",
      "Epoch 2609, Loss: 1.7976328008808196, Final Batch Loss: 0.006305324379354715\n",
      "Epoch 2610, Loss: 3.0777252912521362, Final Batch Loss: 1.308401107788086\n",
      "Epoch 2611, Loss: 2.3663523197174072, Final Batch Loss: 0.6272358298301697\n",
      "Epoch 2612, Loss: 2.411574065685272, Final Batch Loss: 0.5832269787788391\n",
      "Epoch 2613, Loss: 3.2882633805274963, Final Batch Loss: 1.4200936555862427\n",
      "Epoch 2614, Loss: 2.216406375169754, Final Batch Loss: 0.39087221026420593\n",
      "Epoch 2615, Loss: 2.0642413944005966, Final Batch Loss: 0.12916485965251923\n",
      "Epoch 2616, Loss: 3.4012069702148438, Final Batch Loss: 1.3963180780410767\n",
      "Epoch 2617, Loss: 1.994844364002347, Final Batch Loss: 0.02502910979092121\n",
      "Epoch 2618, Loss: 3.5432124733924866, Final Batch Loss: 1.575714349746704\n",
      "Epoch 2619, Loss: 2.7720611691474915, Final Batch Loss: 0.8685614466667175\n",
      "Epoch 2620, Loss: 3.3858706951141357, Final Batch Loss: 1.3340636491775513\n",
      "Epoch 2621, Loss: 2.7273807525634766, Final Batch Loss: 0.8700351119041443\n",
      "Epoch 2622, Loss: 3.5377275347709656, Final Batch Loss: 1.7253015041351318\n",
      "Epoch 2623, Loss: 3.3341798782348633, Final Batch Loss: 1.417274832725525\n",
      "Epoch 2624, Loss: 1.9613307267427444, Final Batch Loss: 0.03647245466709137\n",
      "Epoch 2625, Loss: 2.072613649070263, Final Batch Loss: 0.09333794564008713\n",
      "Epoch 2626, Loss: 1.940378662366129, Final Batch Loss: 8.439661905867979e-05\n",
      "Epoch 2627, Loss: 2.9896493554115295, Final Batch Loss: 1.1563514471054077\n",
      "Epoch 2628, Loss: 1.8536810642108321, Final Batch Loss: 0.00574497040361166\n",
      "Epoch 2629, Loss: 1.972274273633957, Final Batch Loss: 0.16001668572425842\n",
      "Epoch 2630, Loss: 1.8175925947725773, Final Batch Loss: 0.06129107251763344\n",
      "Epoch 2631, Loss: 1.8373352512717247, Final Batch Loss: 0.01951821893453598\n",
      "Epoch 2632, Loss: 1.8781594336032867, Final Batch Loss: 0.008986622095108032\n",
      "Epoch 2633, Loss: 1.9028362333774567, Final Batch Loss: 0.038944393396377563\n",
      "Epoch 2634, Loss: 1.8074166178703308, Final Batch Loss: 0.07475847005844116\n",
      "Epoch 2635, Loss: 2.272436708211899, Final Batch Loss: 0.4572829306125641\n",
      "Epoch 2636, Loss: 3.146586060523987, Final Batch Loss: 1.3317540884017944\n",
      "Epoch 2637, Loss: 2.0816406309604645, Final Batch Loss: 0.3410877287387848\n",
      "Epoch 2638, Loss: 3.856875419616699, Final Batch Loss: 1.9270485639572144\n",
      "Epoch 2639, Loss: 2.20123727619648, Final Batch Loss: 0.2087056189775467\n",
      "Epoch 2640, Loss: 5.69266802072525, Final Batch Loss: 3.7504985332489014\n",
      "Epoch 2641, Loss: 3.0496174693107605, Final Batch Loss: 1.0581533908843994\n",
      "Epoch 2642, Loss: 2.139002174139023, Final Batch Loss: 0.06371793150901794\n",
      "Epoch 2643, Loss: 2.6637986302375793, Final Batch Loss: 0.6097943782806396\n",
      "Epoch 2644, Loss: 2.8260886669158936, Final Batch Loss: 0.8510273694992065\n",
      "Epoch 2645, Loss: 2.0097750201821327, Final Batch Loss: 0.021287180483341217\n",
      "Epoch 2646, Loss: 2.0910723507404327, Final Batch Loss: 0.27613893151283264\n",
      "Epoch 2647, Loss: 2.8122323155403137, Final Batch Loss: 0.9351396560668945\n",
      "Epoch 2648, Loss: 2.303258240222931, Final Batch Loss: 0.4082576036453247\n",
      "Epoch 2649, Loss: 1.9804080426692963, Final Batch Loss: 0.13486090302467346\n",
      "Epoch 2650, Loss: 1.733962464844808, Final Batch Loss: 0.002870607888326049\n",
      "Epoch 2651, Loss: 3.4624359607696533, Final Batch Loss: 1.6932588815689087\n",
      "Epoch 2652, Loss: 1.846536822617054, Final Batch Loss: 0.05356026440858841\n",
      "Epoch 2653, Loss: 1.7423725621774793, Final Batch Loss: 0.002586592920124531\n",
      "Epoch 2654, Loss: 2.3343886137008667, Final Batch Loss: 0.6857393383979797\n",
      "Epoch 2655, Loss: 3.518986940383911, Final Batch Loss: 1.7289848327636719\n",
      "Epoch 2656, Loss: 2.424108564853668, Final Batch Loss: 0.6338711380958557\n",
      "Epoch 2657, Loss: 2.956260025501251, Final Batch Loss: 1.190373182296753\n",
      "Epoch 2658, Loss: 2.6943015456199646, Final Batch Loss: 1.0195621252059937\n",
      "Epoch 2659, Loss: 1.8066728124395013, Final Batch Loss: 0.0015430459752678871\n",
      "Epoch 2660, Loss: 1.713620470603928, Final Batch Loss: 0.0005147324409335852\n",
      "Epoch 2661, Loss: 2.095762699842453, Final Batch Loss: 0.34657400846481323\n",
      "Epoch 2662, Loss: 2.90681791305542, Final Batch Loss: 1.223482370376587\n",
      "Epoch 2663, Loss: 1.8186330050230026, Final Batch Loss: 0.0696447342634201\n",
      "Epoch 2664, Loss: 1.9560505002737045, Final Batch Loss: 0.1701972335577011\n",
      "Epoch 2665, Loss: 3.852225959300995, Final Batch Loss: 1.9913368225097656\n",
      "Epoch 2666, Loss: 1.7803936153650284, Final Batch Loss: 0.05090911686420441\n",
      "Epoch 2667, Loss: 3.330050468444824, Final Batch Loss: 1.4602892398834229\n",
      "Epoch 2668, Loss: 2.407826393842697, Final Batch Loss: 0.4661743938922882\n",
      "Epoch 2669, Loss: 3.3586331605911255, Final Batch Loss: 1.5355135202407837\n",
      "Epoch 2670, Loss: 1.8094721902161837, Final Batch Loss: 0.0254280436784029\n",
      "Epoch 2671, Loss: 1.928468381986022, Final Batch Loss: 0.017094647511839867\n",
      "Epoch 2672, Loss: 4.716912508010864, Final Batch Loss: 2.8209614753723145\n",
      "Epoch 2673, Loss: 1.8849517076741904, Final Batch Loss: 0.0036033957730978727\n",
      "Epoch 2674, Loss: 1.9940600800327957, Final Batch Loss: 0.007366399746388197\n",
      "Epoch 2675, Loss: 1.9222329166223062, Final Batch Loss: 0.00013612773909699172\n",
      "Epoch 2676, Loss: 1.8443978652358055, Final Batch Loss: 0.01624079793691635\n",
      "Epoch 2677, Loss: 3.438573956489563, Final Batch Loss: 1.5805954933166504\n",
      "Epoch 2678, Loss: 1.9896312952041626, Final Batch Loss: 0.08350551128387451\n",
      "Epoch 2679, Loss: 1.907747277058661, Final Batch Loss: 0.0153660262003541\n",
      "Epoch 2680, Loss: 1.8324968125671148, Final Batch Loss: 0.017149021849036217\n",
      "Epoch 2681, Loss: 1.9738190472126007, Final Batch Loss: 0.17934289574623108\n",
      "Epoch 2682, Loss: 1.7452656621935603, Final Batch Loss: 3.0874729418428615e-05\n",
      "Epoch 2683, Loss: 1.793144578114152, Final Batch Loss: 0.029863232746720314\n",
      "Epoch 2684, Loss: 2.2962820529937744, Final Batch Loss: 0.6145574450492859\n",
      "Epoch 2685, Loss: 4.126197636127472, Final Batch Loss: 2.2849767208099365\n",
      "Epoch 2686, Loss: 1.7315140776336193, Final Batch Loss: 0.04238651320338249\n",
      "Epoch 2687, Loss: 2.669983685016632, Final Batch Loss: 0.9980878233909607\n",
      "Epoch 2688, Loss: 3.1213337779045105, Final Batch Loss: 1.4855071306228638\n",
      "Epoch 2689, Loss: 1.9020364508032799, Final Batch Loss: 0.09483357518911362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2690, Loss: 1.8168825767934322, Final Batch Loss: 0.039438094943761826\n",
      "Epoch 2691, Loss: 1.8042611703276634, Final Batch Loss: 0.12246551364660263\n",
      "Epoch 2692, Loss: 1.848813682794571, Final Batch Loss: 0.06642964482307434\n",
      "Epoch 2693, Loss: 1.8023197632282972, Final Batch Loss: 0.028064070269465446\n",
      "Epoch 2694, Loss: 1.7626377828419209, Final Batch Loss: 0.03446059301495552\n",
      "Epoch 2695, Loss: 1.7671217878814787, Final Batch Loss: 0.002561028813943267\n",
      "Epoch 2696, Loss: 1.7982077524065971, Final Batch Loss: 0.045034341514110565\n",
      "Epoch 2697, Loss: 1.69869954418391, Final Batch Loss: 0.007988999597728252\n",
      "Epoch 2698, Loss: 1.790506735444069, Final Batch Loss: 0.05387873947620392\n",
      "Epoch 2699, Loss: 2.7161033749580383, Final Batch Loss: 0.9848833084106445\n",
      "Epoch 2700, Loss: 4.270924687385559, Final Batch Loss: 2.644822835922241\n",
      "Epoch 2701, Loss: 1.7856522854417562, Final Batch Loss: 0.028619473800063133\n",
      "Epoch 2702, Loss: 1.6941417325288057, Final Batch Loss: 0.026439892128109932\n",
      "Epoch 2703, Loss: 1.7295235155906994, Final Batch Loss: 0.00021884430316276848\n",
      "Epoch 2704, Loss: 2.6726697087287903, Final Batch Loss: 0.9036775827407837\n",
      "Epoch 2705, Loss: 1.707752519287169, Final Batch Loss: 0.015416973270475864\n",
      "Epoch 2706, Loss: 2.7345434427261353, Final Batch Loss: 1.0236945152282715\n",
      "Epoch 2707, Loss: 1.910756915807724, Final Batch Loss: 0.25510820746421814\n",
      "Epoch 2708, Loss: 1.792311187426094, Final Batch Loss: 9.095255518332124e-05\n",
      "Epoch 2709, Loss: 1.7775058150291443, Final Batch Loss: 0.07461786270141602\n",
      "Epoch 2710, Loss: 1.8089435994625092, Final Batch Loss: 0.1158536970615387\n",
      "Epoch 2711, Loss: 1.6929745302768424, Final Batch Loss: 0.000742279109545052\n",
      "Epoch 2712, Loss: 2.6743627190589905, Final Batch Loss: 0.9753327965736389\n",
      "Epoch 2713, Loss: 2.4522780179977417, Final Batch Loss: 0.7237384915351868\n",
      "Epoch 2714, Loss: 3.3518112301826477, Final Batch Loss: 1.5783191919326782\n",
      "Epoch 2715, Loss: 2.1037862992379814, Final Batch Loss: 0.0014579391572624445\n",
      "Epoch 2716, Loss: 3.272913873195648, Final Batch Loss: 0.8203378915786743\n",
      "Epoch 2717, Loss: 2.243706268025562, Final Batch Loss: 0.0006272017490118742\n",
      "Epoch 2718, Loss: 2.086588143953122, Final Batch Loss: 0.000846027978695929\n",
      "Epoch 2719, Loss: 1.7124274298548698, Final Batch Loss: 0.020733527839183807\n",
      "Epoch 2720, Loss: 2.139195293188095, Final Batch Loss: 0.3539697229862213\n",
      "Epoch 2721, Loss: 1.900929395109415, Final Batch Loss: 0.046664003282785416\n",
      "Epoch 2722, Loss: 2.8231760263442993, Final Batch Loss: 0.9713962078094482\n",
      "Epoch 2723, Loss: 1.9409338682889938, Final Batch Loss: 0.09922300279140472\n",
      "Epoch 2724, Loss: 2.336015611886978, Final Batch Loss: 0.33622464537620544\n",
      "Epoch 2725, Loss: 3.807576537132263, Final Batch Loss: 1.7765772342681885\n",
      "Epoch 2726, Loss: 2.0962849855422974, Final Batch Loss: 0.2751368284225464\n",
      "Epoch 2727, Loss: 2.412602514028549, Final Batch Loss: 0.3788774311542511\n",
      "Epoch 2728, Loss: 4.189608633518219, Final Batch Loss: 1.9695119857788086\n",
      "Epoch 2729, Loss: 2.3924454003572464, Final Batch Loss: 0.030326858162879944\n",
      "Epoch 2730, Loss: 3.3584863543510437, Final Batch Loss: 1.1450932025909424\n",
      "Epoch 2731, Loss: 2.2825421690940857, Final Batch Loss: 0.25275689363479614\n",
      "Epoch 2732, Loss: 2.8507496118545532, Final Batch Loss: 0.9400023818016052\n",
      "Epoch 2733, Loss: 2.910053074359894, Final Batch Loss: 1.0297486782073975\n",
      "Epoch 2734, Loss: 2.0240823328495026, Final Batch Loss: 0.09283104538917542\n",
      "Epoch 2735, Loss: 2.093383342027664, Final Batch Loss: 0.15737482905387878\n",
      "Epoch 2736, Loss: 2.235287219285965, Final Batch Loss: 0.26747986674308777\n",
      "Epoch 2737, Loss: 1.9542308952659369, Final Batch Loss: 0.029855480417609215\n",
      "Epoch 2738, Loss: 1.913575679063797, Final Batch Loss: 0.17498692870140076\n",
      "Epoch 2739, Loss: 1.8851264505647123, Final Batch Loss: 0.007095613982528448\n",
      "Epoch 2740, Loss: 1.8972890749573708, Final Batch Loss: 0.08463523536920547\n",
      "Epoch 2741, Loss: 2.6137168407440186, Final Batch Loss: 0.7477449774742126\n",
      "Epoch 2742, Loss: 3.3871195912361145, Final Batch Loss: 1.5322333574295044\n",
      "Epoch 2743, Loss: 1.7442890400561737, Final Batch Loss: 3.194758028257638e-05\n",
      "Epoch 2744, Loss: 1.655427273362875, Final Batch Loss: 0.007990773767232895\n",
      "Epoch 2745, Loss: 3.6498265266418457, Final Batch Loss: 1.7831306457519531\n",
      "Epoch 2746, Loss: 2.126430779695511, Final Batch Loss: 0.4149787127971649\n",
      "Epoch 2747, Loss: 3.77830171585083, Final Batch Loss: 1.7872564792633057\n",
      "Epoch 2748, Loss: 2.0884307250380516, Final Batch Loss: 0.05448558181524277\n",
      "Epoch 2749, Loss: 2.0847700238227844, Final Batch Loss: 0.13979405164718628\n",
      "Epoch 2750, Loss: 3.4631696343421936, Final Batch Loss: 1.485243558883667\n",
      "Epoch 2751, Loss: 1.921814078465104, Final Batch Loss: 0.00010573305189609528\n",
      "Epoch 2752, Loss: 2.3449531197547913, Final Batch Loss: 0.5667418241500854\n",
      "Epoch 2753, Loss: 1.8785608224570751, Final Batch Loss: 0.04088352248072624\n",
      "Epoch 2754, Loss: 3.3109650015830994, Final Batch Loss: 1.5306941270828247\n",
      "Epoch 2755, Loss: 1.8116523660719395, Final Batch Loss: 0.042894963175058365\n",
      "Epoch 2756, Loss: 1.9174337200820446, Final Batch Loss: 0.03794030472636223\n",
      "Epoch 2757, Loss: 3.457428753376007, Final Batch Loss: 1.6000992059707642\n",
      "Epoch 2758, Loss: 1.8488796800374985, Final Batch Loss: 0.11414621770381927\n",
      "Epoch 2759, Loss: 1.7037332321051508, Final Batch Loss: 0.0019694233778864145\n",
      "Epoch 2760, Loss: 1.8975221600849181, Final Batch Loss: 0.0036493625957518816\n",
      "Epoch 2761, Loss: 1.7978604137024377, Final Batch Loss: 0.0002444683632347733\n",
      "Epoch 2762, Loss: 2.0247055143117905, Final Batch Loss: 0.1911884993314743\n",
      "Epoch 2763, Loss: 3.8292206525802612, Final Batch Loss: 1.9201160669326782\n",
      "Epoch 2764, Loss: 2.0719889998435974, Final Batch Loss: 0.2523617744445801\n",
      "Epoch 2765, Loss: 1.898235410451889, Final Batch Loss: 0.15263745188713074\n",
      "Epoch 2766, Loss: 1.7499719383195043, Final Batch Loss: 0.013778102584183216\n",
      "Epoch 2767, Loss: 1.7506475939881057, Final Batch Loss: 0.002293933881446719\n",
      "Epoch 2768, Loss: 3.253019154071808, Final Batch Loss: 1.4821276664733887\n",
      "Epoch 2769, Loss: 1.8774066418409348, Final Batch Loss: 0.08242665231227875\n",
      "Epoch 2770, Loss: 2.9683996438980103, Final Batch Loss: 0.9648026823997498\n",
      "Epoch 2771, Loss: 2.114835648331791, Final Batch Loss: 0.004860846791416407\n",
      "Epoch 2772, Loss: 4.280779540538788, Final Batch Loss: 2.1412858963012695\n",
      "Epoch 2773, Loss: 2.732000410556793, Final Batch Loss: 0.5803844332695007\n",
      "Epoch 2774, Loss: 2.5709369778633118, Final Batch Loss: 0.4722006320953369\n",
      "Epoch 2775, Loss: 2.123479872941971, Final Batch Loss: 0.13139751553535461\n",
      "Epoch 2776, Loss: 3.458796739578247, Final Batch Loss: 1.5358867645263672\n",
      "Epoch 2777, Loss: 3.493523359298706, Final Batch Loss: 1.5397857427597046\n",
      "Epoch 2778, Loss: 1.9184542894363403, Final Batch Loss: 0.09845864772796631\n",
      "Epoch 2779, Loss: 2.4833105206489563, Final Batch Loss: 0.6263880729675293\n",
      "Epoch 2780, Loss: 1.8404361009597778, Final Batch Loss: 0.01618015766143799\n",
      "Epoch 2781, Loss: 2.4963504672050476, Final Batch Loss: 0.6461230516433716\n",
      "Epoch 2782, Loss: 3.081644833087921, Final Batch Loss: 1.2531191110610962\n",
      "Epoch 2783, Loss: 2.779198944568634, Final Batch Loss: 1.0266683101654053\n",
      "Epoch 2784, Loss: 2.123725712299347, Final Batch Loss: 0.30326026678085327\n",
      "Epoch 2785, Loss: 3.4878223538398743, Final Batch Loss: 1.7440388202667236\n",
      "Epoch 2786, Loss: 3.1591554284095764, Final Batch Loss: 1.3441946506500244\n",
      "Epoch 2787, Loss: 2.029745638370514, Final Batch Loss: 0.31243008375167847\n",
      "Epoch 2788, Loss: 1.8626674190163612, Final Batch Loss: 0.10434059053659439\n",
      "Epoch 2789, Loss: 3.380441725254059, Final Batch Loss: 1.6195749044418335\n",
      "Epoch 2790, Loss: 2.036958336830139, Final Batch Loss: 0.09872502088546753\n",
      "Epoch 2791, Loss: 2.255839992314577, Final Batch Loss: 0.04165862873196602\n",
      "Epoch 2792, Loss: 2.2210388430394232, Final Batch Loss: 0.002596104983240366\n",
      "Epoch 2793, Loss: 2.391818754374981, Final Batch Loss: 0.018266551196575165\n",
      "Epoch 2794, Loss: 2.2987651228904724, Final Batch Loss: 0.3775097131729126\n",
      "Epoch 2795, Loss: 1.948114350438118, Final Batch Loss: 0.14260025322437286\n",
      "Epoch 2796, Loss: 1.8950141426175833, Final Batch Loss: 0.024573838338255882\n",
      "Epoch 2797, Loss: 1.9927153885364532, Final Batch Loss: 0.13365837931632996\n",
      "Epoch 2798, Loss: 1.8537066280841827, Final Batch Loss: 0.12745532393455505\n",
      "Epoch 2799, Loss: 1.855540944263339, Final Batch Loss: 0.02318371646106243\n",
      "Epoch 2800, Loss: 2.603127956390381, Final Batch Loss: 0.7174010872840881\n",
      "Epoch 2801, Loss: 1.8078046292066574, Final Batch Loss: 0.08137564361095428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2802, Loss: 3.7959583401679993, Final Batch Loss: 2.003607749938965\n",
      "Epoch 2803, Loss: 3.5675321221351624, Final Batch Loss: 1.8034770488739014\n",
      "Epoch 2804, Loss: 1.8107073083519936, Final Batch Loss: 0.033305682241916656\n",
      "Epoch 2805, Loss: 1.8832600861787796, Final Batch Loss: 0.1462469846010208\n",
      "Epoch 2806, Loss: 1.8649650029838085, Final Batch Loss: 0.03603420779109001\n",
      "Epoch 2807, Loss: 1.886859754100442, Final Batch Loss: 0.0145289096981287\n",
      "Epoch 2808, Loss: 3.5876330733299255, Final Batch Loss: 1.7036012411117554\n",
      "Epoch 2809, Loss: 2.995968759059906, Final Batch Loss: 1.1645536422729492\n",
      "Epoch 2810, Loss: 1.9705924093723297, Final Batch Loss: 0.12104848027229309\n",
      "Epoch 2811, Loss: 1.877839706838131, Final Batch Loss: 0.03874143213033676\n",
      "Epoch 2812, Loss: 2.097046345472336, Final Batch Loss: 0.2997411787509918\n",
      "Epoch 2813, Loss: 1.9694207161664963, Final Batch Loss: 0.1830085963010788\n",
      "Epoch 2814, Loss: 1.9230602383613586, Final Batch Loss: 0.09636670351028442\n",
      "Epoch 2815, Loss: 2.928464353084564, Final Batch Loss: 1.1888971328735352\n",
      "Epoch 2816, Loss: 3.7141029834747314, Final Batch Loss: 1.9470990896224976\n",
      "Epoch 2817, Loss: 2.0790145248174667, Final Batch Loss: 0.19578073918819427\n",
      "Epoch 2818, Loss: 1.8874173993244767, Final Batch Loss: 0.010463380254805088\n",
      "Epoch 2819, Loss: 1.9756716042757034, Final Batch Loss: 0.15705974400043488\n",
      "Epoch 2820, Loss: 2.320378065109253, Final Batch Loss: 0.5280064940452576\n",
      "Epoch 2821, Loss: 1.9309293404221535, Final Batch Loss: 0.10391370207071304\n",
      "Epoch 2822, Loss: 3.4701549410820007, Final Batch Loss: 1.7219256162643433\n",
      "Epoch 2823, Loss: 2.830373167991638, Final Batch Loss: 1.0991169214248657\n",
      "Epoch 2824, Loss: 1.7778175733983517, Final Batch Loss: 0.06158938631415367\n",
      "Epoch 2825, Loss: 1.7862157057970762, Final Batch Loss: 0.01549033634364605\n",
      "Epoch 2826, Loss: 1.904405616223812, Final Batch Loss: 0.044392310082912445\n",
      "Epoch 2827, Loss: 2.1377916038036346, Final Batch Loss: 0.43482348322868347\n",
      "Epoch 2828, Loss: 1.8280059248209, Final Batch Loss: 0.15611566603183746\n",
      "Epoch 2829, Loss: 4.005459904670715, Final Batch Loss: 2.3722500801086426\n",
      "Epoch 2830, Loss: 2.643152952194214, Final Batch Loss: 0.7954259514808655\n",
      "Epoch 2831, Loss: 1.9696468701986305, Final Batch Loss: 4.2199197196168825e-05\n",
      "Epoch 2832, Loss: 2.436486095190048, Final Batch Loss: 0.3337399661540985\n",
      "Epoch 2833, Loss: 2.713846206665039, Final Batch Loss: 0.6549489498138428\n",
      "Epoch 2834, Loss: 2.090396136045456, Final Batch Loss: 0.06337359547615051\n",
      "Epoch 2835, Loss: 1.9970502890646458, Final Batch Loss: 0.01875406876206398\n",
      "Epoch 2836, Loss: 3.6592950224876404, Final Batch Loss: 1.8745486736297607\n",
      "Epoch 2837, Loss: 1.8749590013176203, Final Batch Loss: 0.019281575456261635\n",
      "Epoch 2838, Loss: 3.522480010986328, Final Batch Loss: 1.7205229997634888\n",
      "Epoch 2839, Loss: 2.4073996543884277, Final Batch Loss: 0.7259687185287476\n",
      "Epoch 2840, Loss: 3.3685542345046997, Final Batch Loss: 1.5843055248260498\n",
      "Epoch 2841, Loss: 1.6680785082280636, Final Batch Loss: 0.015157517045736313\n",
      "Epoch 2842, Loss: 2.7690552473068237, Final Batch Loss: 0.996929407119751\n",
      "Epoch 2843, Loss: 1.990119680762291, Final Batch Loss: 0.18577705323696136\n",
      "Epoch 2844, Loss: 1.7493296032771468, Final Batch Loss: 0.011642015539109707\n",
      "Epoch 2845, Loss: 1.9857906848192215, Final Batch Loss: 0.2302863746881485\n",
      "Epoch 2846, Loss: 3.5817516446113586, Final Batch Loss: 1.7870631217956543\n",
      "Epoch 2847, Loss: 2.1246674060821533, Final Batch Loss: 0.3572961091995239\n",
      "Epoch 2848, Loss: 2.2614978551864624, Final Batch Loss: 0.32642149925231934\n",
      "Epoch 2849, Loss: 1.9411814995110035, Final Batch Loss: 0.03318806365132332\n",
      "Epoch 2850, Loss: 3.3861122727394104, Final Batch Loss: 1.4112603664398193\n",
      "Epoch 2851, Loss: 4.702521026134491, Final Batch Loss: 2.6732475757598877\n",
      "Epoch 2852, Loss: 2.3094691038131714, Final Batch Loss: 0.44238048791885376\n",
      "Epoch 2853, Loss: 2.9793028235435486, Final Batch Loss: 0.8064092993736267\n",
      "Epoch 2854, Loss: 5.027347922325134, Final Batch Loss: 2.395369052886963\n",
      "Epoch 2855, Loss: 4.9005783796310425, Final Batch Loss: 2.111384391784668\n",
      "Epoch 2856, Loss: 3.0484264194965363, Final Batch Loss: 0.48151490092277527\n",
      "Epoch 2857, Loss: 2.430705646984279, Final Batch Loss: 0.011482815258204937\n",
      "Epoch 2858, Loss: 2.3057254077866673, Final Batch Loss: 0.012903762049973011\n",
      "Epoch 2859, Loss: 2.252129800617695, Final Batch Loss: 0.08527756482362747\n",
      "Epoch 2860, Loss: 2.1423344798386097, Final Batch Loss: 0.05630548670887947\n",
      "Epoch 2861, Loss: 2.0554576627910137, Final Batch Loss: 0.05841268226504326\n",
      "Epoch 2862, Loss: 2.2333985567092896, Final Batch Loss: 0.18121641874313354\n",
      "Epoch 2863, Loss: 1.892262710724026, Final Batch Loss: 0.0025774375535547733\n",
      "Epoch 2864, Loss: 1.8882320821285248, Final Batch Loss: 0.010040491819381714\n",
      "Epoch 2865, Loss: 1.9062071163207293, Final Batch Loss: 0.00545928068459034\n",
      "Epoch 2866, Loss: 2.042919158935547, Final Batch Loss: 0.23437470197677612\n",
      "Epoch 2867, Loss: 3.1654289960861206, Final Batch Loss: 1.2575496435165405\n",
      "Epoch 2868, Loss: 2.873790740966797, Final Batch Loss: 0.8910812139511108\n",
      "Epoch 2869, Loss: 5.173325955867767, Final Batch Loss: 3.156566619873047\n",
      "Epoch 2870, Loss: 2.0391757916659117, Final Batch Loss: 0.008912308141589165\n",
      "Epoch 2871, Loss: 1.9059591442346573, Final Batch Loss: 0.022612348198890686\n",
      "Epoch 2872, Loss: 2.1241191774606705, Final Batch Loss: 0.14856989681720734\n",
      "Epoch 2873, Loss: 3.1217087507247925, Final Batch Loss: 1.1323546171188354\n",
      "Epoch 2874, Loss: 1.988461397588253, Final Batch Loss: 0.013345204293727875\n",
      "Epoch 2875, Loss: 2.4902841448783875, Final Batch Loss: 0.4711151123046875\n",
      "Epoch 2876, Loss: 1.9482168331742287, Final Batch Loss: 0.07318118959665298\n",
      "Epoch 2877, Loss: 1.9269258650019765, Final Batch Loss: 0.01099350769072771\n",
      "Epoch 2878, Loss: 2.9287548661231995, Final Batch Loss: 0.916317343711853\n",
      "Epoch 2879, Loss: 2.459234833717346, Final Batch Loss: 0.4322148561477661\n",
      "Epoch 2880, Loss: 2.012700006365776, Final Batch Loss: 0.038838788866996765\n",
      "Epoch 2881, Loss: 2.1237991973757744, Final Batch Loss: 0.06569873541593552\n",
      "Epoch 2882, Loss: 3.51087749004364, Final Batch Loss: 1.5934112071990967\n",
      "Epoch 2883, Loss: 1.8621912822127342, Final Batch Loss: 0.06655188649892807\n",
      "Epoch 2884, Loss: 2.049703598022461, Final Batch Loss: 0.18628311157226562\n",
      "Epoch 2885, Loss: 1.992429032921791, Final Batch Loss: 0.2180432826280594\n",
      "Epoch 2886, Loss: 1.7356307012960315, Final Batch Loss: 0.012677083723247051\n",
      "Epoch 2887, Loss: 1.9149555414915085, Final Batch Loss: 0.13260214030742645\n",
      "Epoch 2888, Loss: 1.772571586072445, Final Batch Loss: 0.05196037143468857\n",
      "Epoch 2889, Loss: 1.6898150574415922, Final Batch Loss: 0.004888368770480156\n",
      "Epoch 2890, Loss: 2.7537039518356323, Final Batch Loss: 1.035520076751709\n",
      "Epoch 2891, Loss: 1.6934044398367405, Final Batch Loss: 0.04194307699799538\n",
      "Epoch 2892, Loss: 1.632556612137705, Final Batch Loss: 0.0008516260422766209\n",
      "Epoch 2893, Loss: 1.8874016851186752, Final Batch Loss: 0.11785392463207245\n",
      "Epoch 2894, Loss: 2.345123678445816, Final Batch Loss: 0.6126882433891296\n",
      "Epoch 2895, Loss: 1.7337933480739594, Final Batch Loss: 0.07953500747680664\n",
      "Epoch 2896, Loss: 1.846134327352047, Final Batch Loss: 0.052026234567165375\n",
      "Epoch 2897, Loss: 1.6637712782248855, Final Batch Loss: 0.012512749992311\n",
      "Epoch 2898, Loss: 1.7310549598405487, Final Batch Loss: 6.067568756407127e-05\n",
      "Epoch 2899, Loss: 1.6860674545168877, Final Batch Loss: 0.013745062053203583\n",
      "Epoch 2900, Loss: 1.9886820614337921, Final Batch Loss: 0.37032708525657654\n",
      "Epoch 2901, Loss: 2.7785771787166595, Final Batch Loss: 1.1539534330368042\n",
      "Epoch 2902, Loss: 1.860182747244835, Final Batch Loss: 0.19333909451961517\n",
      "Epoch 2903, Loss: 1.7870958745479584, Final Batch Loss: 0.12901923060417175\n",
      "Epoch 2904, Loss: 2.7310051321983337, Final Batch Loss: 1.0361459255218506\n",
      "Epoch 2905, Loss: 3.036338984966278, Final Batch Loss: 1.286325216293335\n",
      "Epoch 2906, Loss: 1.849970504641533, Final Batch Loss: 0.12954728305339813\n",
      "Epoch 2907, Loss: 1.902481459081173, Final Batch Loss: 0.09469597786664963\n",
      "Epoch 2908, Loss: 1.9111156165599823, Final Batch Loss: 0.19462940096855164\n",
      "Epoch 2909, Loss: 1.7394360192120075, Final Batch Loss: 0.0026774294674396515\n",
      "Epoch 2910, Loss: 1.8530967384576797, Final Batch Loss: 0.134465292096138\n",
      "Epoch 2911, Loss: 1.686715006828308, Final Batch Loss: 0.03163999319076538\n",
      "Epoch 2912, Loss: 1.7396053709089756, Final Batch Loss: 0.026556912809610367\n",
      "Epoch 2913, Loss: 1.636459682136774, Final Batch Loss: 0.038148824125528336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2914, Loss: 2.432169198989868, Final Batch Loss: 0.7648911476135254\n",
      "Epoch 2915, Loss: 2.406299650669098, Final Batch Loss: 0.7142685055732727\n",
      "Epoch 2916, Loss: 1.761390522820875, Final Batch Loss: 0.0010205067228525877\n",
      "Epoch 2917, Loss: 2.1985180526971817, Final Batch Loss: 0.05106861889362335\n",
      "Epoch 2918, Loss: 2.5128582069883123, Final Batch Loss: 0.0004984090337529778\n",
      "Epoch 2919, Loss: 3.0985642671585083, Final Batch Loss: 0.6526675224304199\n",
      "Epoch 2920, Loss: 2.223629519343376, Final Batch Loss: 0.13636158406734467\n",
      "Epoch 2921, Loss: 2.012347660958767, Final Batch Loss: 0.07318373769521713\n",
      "Epoch 2922, Loss: 1.9684055261313915, Final Batch Loss: 0.023410122841596603\n",
      "Epoch 2923, Loss: 2.8631988763809204, Final Batch Loss: 0.8158189654350281\n",
      "Epoch 2924, Loss: 1.8854430303908885, Final Batch Loss: 0.004053591284900904\n",
      "Epoch 2925, Loss: 3.1125895380973816, Final Batch Loss: 1.2040085792541504\n",
      "Epoch 2926, Loss: 1.8911470603197813, Final Batch Loss: 0.014798982068896294\n",
      "Epoch 2927, Loss: 1.9520078848581761, Final Batch Loss: 0.001861388562247157\n",
      "Epoch 2928, Loss: 2.91457599401474, Final Batch Loss: 0.9176841974258423\n",
      "Epoch 2929, Loss: 2.280399888753891, Final Batch Loss: 0.46462419629096985\n",
      "Epoch 2930, Loss: 2.1048928797245026, Final Batch Loss: 0.1399078667163849\n",
      "Epoch 2931, Loss: 1.9511313512921333, Final Batch Loss: 0.09397882968187332\n",
      "Epoch 2932, Loss: 4.142014563083649, Final Batch Loss: 2.2966322898864746\n",
      "Epoch 2933, Loss: 2.2693322598934174, Final Batch Loss: 0.4612251818180084\n",
      "Epoch 2934, Loss: 1.79669576510787, Final Batch Loss: 0.05777614936232567\n",
      "Epoch 2935, Loss: 2.735793948173523, Final Batch Loss: 0.9116963744163513\n",
      "Epoch 2936, Loss: 1.8010038752108812, Final Batch Loss: 0.02212006039917469\n",
      "Epoch 2937, Loss: 1.920559585094452, Final Batch Loss: 0.17285501956939697\n",
      "Epoch 2938, Loss: 2.909623384475708, Final Batch Loss: 1.1499009132385254\n",
      "Epoch 2939, Loss: 1.6736052026972175, Final Batch Loss: 0.008144973777234554\n",
      "Epoch 2940, Loss: 2.370151698589325, Final Batch Loss: 0.5795652270317078\n",
      "Epoch 2941, Loss: 2.5524399876594543, Final Batch Loss: 0.841199517250061\n",
      "Epoch 2942, Loss: 2.036770820617676, Final Batch Loss: 0.1513059139251709\n",
      "Epoch 2943, Loss: 3.833985984325409, Final Batch Loss: 2.103771448135376\n",
      "Epoch 2944, Loss: 2.1108627021312714, Final Batch Loss: 0.36194083094596863\n",
      "Epoch 2945, Loss: 2.0958207100629807, Final Batch Loss: 0.2283768504858017\n",
      "Epoch 2946, Loss: 1.7645802865736187, Final Batch Loss: 0.006840146612375975\n",
      "Epoch 2947, Loss: 2.000591278076172, Final Batch Loss: 0.2532992959022522\n",
      "Epoch 2948, Loss: 4.2611929178237915, Final Batch Loss: 2.5543932914733887\n",
      "Epoch 2949, Loss: 2.013054847717285, Final Batch Loss: 0.36190637946128845\n",
      "Epoch 2950, Loss: 1.86654694378376, Final Batch Loss: 0.19266898930072784\n",
      "Epoch 2951, Loss: 1.6955409746151417, Final Batch Loss: 0.0033426384907215834\n",
      "Epoch 2952, Loss: 1.7355978549458086, Final Batch Loss: 0.0041675628162920475\n",
      "Epoch 2953, Loss: 1.9123231023550034, Final Batch Loss: 0.1813415139913559\n",
      "Epoch 2954, Loss: 5.749568521976471, Final Batch Loss: 4.0565080642700195\n",
      "Epoch 2955, Loss: 1.8452234640717506, Final Batch Loss: 0.09958640486001968\n",
      "Epoch 2956, Loss: 1.9867372661828995, Final Batch Loss: 0.20333607494831085\n",
      "Epoch 2957, Loss: 1.8914096653461456, Final Batch Loss: 0.24125438928604126\n",
      "Epoch 2958, Loss: 2.6557819843292236, Final Batch Loss: 0.9222068786621094\n",
      "Epoch 2959, Loss: 3.893584668636322, Final Batch Loss: 2.1258373260498047\n",
      "Epoch 2960, Loss: 1.9154368489980698, Final Batch Loss: 0.09220181405544281\n",
      "Epoch 2961, Loss: 2.708631753921509, Final Batch Loss: 0.9014164209365845\n",
      "Epoch 2962, Loss: 1.9309858033448108, Final Batch Loss: 0.00010024998482549563\n",
      "Epoch 2963, Loss: 1.936100099235773, Final Batch Loss: 0.05084023252129555\n",
      "Epoch 2964, Loss: 2.4931772351264954, Final Batch Loss: 0.565983772277832\n",
      "Epoch 2965, Loss: 1.8345110546797514, Final Batch Loss: 0.022752655670046806\n",
      "Epoch 2966, Loss: 3.0570889115333557, Final Batch Loss: 1.3462703227996826\n",
      "Epoch 2967, Loss: 1.8695284724235535, Final Batch Loss: 0.10195964574813843\n",
      "Epoch 2968, Loss: 1.8697869330644608, Final Batch Loss: 0.054519787430763245\n",
      "Epoch 2969, Loss: 1.9633734449744225, Final Batch Loss: 0.12206771224737167\n",
      "Epoch 2970, Loss: 1.795323966536671, Final Batch Loss: 0.0038887844420969486\n",
      "Epoch 2971, Loss: 1.7654030341655016, Final Batch Loss: 0.010431291535496712\n",
      "Epoch 2972, Loss: 5.940427362918854, Final Batch Loss: 4.286674976348877\n",
      "Epoch 2973, Loss: 1.7120457359706052, Final Batch Loss: 0.00096941675292328\n",
      "Epoch 2974, Loss: 3.123474657535553, Final Batch Loss: 1.4010140895843506\n",
      "Epoch 2975, Loss: 2.4668206572532654, Final Batch Loss: 0.6169594526290894\n",
      "Epoch 2976, Loss: 1.8612224902026355, Final Batch Loss: 0.00704021705314517\n",
      "Epoch 2977, Loss: 2.006934568285942, Final Batch Loss: 0.15582875907421112\n",
      "Epoch 2978, Loss: 3.7828962802886963, Final Batch Loss: 1.9728999137878418\n",
      "Epoch 2979, Loss: 3.498735249042511, Final Batch Loss: 1.6836687326431274\n",
      "Epoch 2980, Loss: 1.8362506872508675, Final Batch Loss: 0.0015278107021003962\n",
      "Epoch 2981, Loss: 3.407334864139557, Final Batch Loss: 1.3214699029922485\n",
      "Epoch 2982, Loss: 2.0211628787219524, Final Batch Loss: 0.059400927275419235\n",
      "Epoch 2983, Loss: 2.1745088696479797, Final Batch Loss: 0.083163321018219\n",
      "Epoch 2984, Loss: 4.441534101963043, Final Batch Loss: 2.5035529136657715\n",
      "Epoch 2985, Loss: 1.8579228594899178, Final Batch Loss: 0.03699357062578201\n",
      "Epoch 2986, Loss: 3.504654347896576, Final Batch Loss: 1.6989492177963257\n",
      "Epoch 2987, Loss: 3.172618269920349, Final Batch Loss: 1.3595314025878906\n",
      "Epoch 2988, Loss: 1.8091065610497026, Final Batch Loss: 0.0002337421028641984\n",
      "Epoch 2989, Loss: 1.9174115136265755, Final Batch Loss: 0.09645146876573563\n",
      "Epoch 2990, Loss: 1.6591804139316082, Final Batch Loss: 0.020052682608366013\n",
      "Epoch 2991, Loss: 1.7295301649719477, Final Batch Loss: 0.011629054322838783\n",
      "Epoch 2992, Loss: 1.885170802474022, Final Batch Loss: 0.10152344405651093\n",
      "Epoch 2993, Loss: 2.305603504180908, Final Batch Loss: 0.5257757902145386\n",
      "Epoch 2994, Loss: 4.177119076251984, Final Batch Loss: 2.476356029510498\n",
      "Epoch 2995, Loss: 1.829773049801588, Final Batch Loss: 0.04787133261561394\n",
      "Epoch 2996, Loss: 3.0200037360191345, Final Batch Loss: 1.074825644493103\n",
      "Epoch 2997, Loss: 2.356909841299057, Final Batch Loss: 0.3425764739513397\n",
      "Epoch 2998, Loss: 2.03233871050179, Final Batch Loss: 0.013089092448353767\n",
      "Epoch 2999, Loss: 2.1602010104979854, Final Batch Loss: 7.199982064776123e-05\n",
      "Epoch 3000, Loss: 3.4206157326698303, Final Batch Loss: 1.3140380382537842\n",
      "Epoch 3001, Loss: 1.9930914640426636, Final Batch Loss: 0.030870258808135986\n",
      "Epoch 3002, Loss: 2.043369084596634, Final Batch Loss: 0.25056663155555725\n",
      "Epoch 3003, Loss: 3.1057109236717224, Final Batch Loss: 1.3318506479263306\n",
      "Epoch 3004, Loss: 1.8469069302082062, Final Batch Loss: 0.025463372468948364\n",
      "Epoch 3005, Loss: 2.533344805240631, Final Batch Loss: 0.8365537524223328\n",
      "Epoch 3006, Loss: 2.554958999156952, Final Batch Loss: 0.9258198738098145\n",
      "Epoch 3007, Loss: 1.7671297304332256, Final Batch Loss: 0.04180772230029106\n",
      "Epoch 3008, Loss: 1.8028646524762735, Final Batch Loss: 0.0011082704877480865\n",
      "Epoch 3009, Loss: 3.3846134543418884, Final Batch Loss: 1.5531610250473022\n",
      "Epoch 3010, Loss: 1.8579987566918135, Final Batch Loss: 0.0049338024109601974\n",
      "Epoch 3011, Loss: 1.8109943307936192, Final Batch Loss: 0.009888354688882828\n",
      "Epoch 3012, Loss: 1.7465354539453983, Final Batch Loss: 0.04367843642830849\n",
      "Epoch 3013, Loss: 3.66769340634346, Final Batch Loss: 1.8937437534332275\n",
      "Epoch 3014, Loss: 3.5285627841949463, Final Batch Loss: 1.878326654434204\n",
      "Epoch 3015, Loss: 1.8315680176019669, Final Batch Loss: 0.14913363754749298\n",
      "Epoch 3016, Loss: 3.5354382395744324, Final Batch Loss: 1.7756160497665405\n",
      "Epoch 3017, Loss: 1.7654793784022331, Final Batch Loss: 0.06779026240110397\n",
      "Epoch 3018, Loss: 3.857632339000702, Final Batch Loss: 2.027841329574585\n",
      "Epoch 3019, Loss: 3.5638474822044373, Final Batch Loss: 1.7712652683258057\n",
      "Epoch 3020, Loss: 2.1784306168556213, Final Batch Loss: 0.49265938997268677\n",
      "Epoch 3021, Loss: 1.8851981684565544, Final Batch Loss: 0.03951292484998703\n",
      "Epoch 3022, Loss: 3.331619441509247, Final Batch Loss: 1.4725840091705322\n",
      "Epoch 3023, Loss: 1.8794098403304815, Final Batch Loss: 0.008790722116827965\n",
      "Epoch 3024, Loss: 1.9734706804156303, Final Batch Loss: 0.09805517643690109\n",
      "Epoch 3025, Loss: 2.185499846935272, Final Batch Loss: 0.4567239284515381\n",
      "Epoch 3026, Loss: 1.7599917072802782, Final Batch Loss: 0.0057715196162462234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3027, Loss: 3.542141556739807, Final Batch Loss: 1.8001818656921387\n",
      "Epoch 3028, Loss: 1.7454343718127348, Final Batch Loss: 0.00025138078490272164\n",
      "Epoch 3029, Loss: 2.6710864305496216, Final Batch Loss: 0.9546899199485779\n",
      "Epoch 3030, Loss: 3.0011967420578003, Final Batch Loss: 1.3177731037139893\n",
      "Epoch 3031, Loss: 1.7678525242954493, Final Batch Loss: 0.0297741387039423\n",
      "Epoch 3032, Loss: 3.5627259612083435, Final Batch Loss: 1.8429632186889648\n",
      "Epoch 3033, Loss: 1.6399615921400255, Final Batch Loss: 0.00012778419477399439\n",
      "Epoch 3034, Loss: 2.540079891681671, Final Batch Loss: 0.7861009836196899\n",
      "Epoch 3035, Loss: 1.7298134118318558, Final Batch Loss: 0.016414955258369446\n",
      "Epoch 3036, Loss: 1.8131622225046158, Final Batch Loss: 0.11282415688037872\n",
      "Epoch 3037, Loss: 1.7251196950674057, Final Batch Loss: 0.09542582929134369\n",
      "Epoch 3038, Loss: 3.3391916751861572, Final Batch Loss: 1.6857446432113647\n",
      "Epoch 3039, Loss: 1.9210637211799622, Final Batch Loss: 0.2501370310783386\n",
      "Epoch 3040, Loss: 1.670706793665886, Final Batch Loss: 0.037608906626701355\n",
      "Epoch 3041, Loss: 4.278267502784729, Final Batch Loss: 2.715754747390747\n",
      "Epoch 3042, Loss: 1.6821338012814522, Final Batch Loss: 0.07683766633272171\n",
      "Epoch 3043, Loss: 5.149309039115906, Final Batch Loss: 3.425287961959839\n",
      "Epoch 3044, Loss: 2.480469584465027, Final Batch Loss: 0.5767698287963867\n",
      "Epoch 3045, Loss: 2.198622480034828, Final Batch Loss: 0.038022592663764954\n",
      "Epoch 3046, Loss: 4.138446807861328, Final Batch Loss: 1.854196310043335\n",
      "Epoch 3047, Loss: 3.5396066308021545, Final Batch Loss: 1.1176395416259766\n",
      "Epoch 3048, Loss: 5.21460896730423, Final Batch Loss: 2.881746292114258\n",
      "Epoch 3049, Loss: 2.7084038257598877, Final Batch Loss: 0.4928209185600281\n",
      "Epoch 3050, Loss: 2.2071501538157463, Final Batch Loss: 0.08228843659162521\n",
      "Epoch 3051, Loss: 4.279145658016205, Final Batch Loss: 2.229691982269287\n",
      "Epoch 3052, Loss: 4.1200268268585205, Final Batch Loss: 2.0977933406829834\n",
      "Epoch 3053, Loss: 2.0119367681909353, Final Batch Loss: 0.001990600721910596\n",
      "Epoch 3054, Loss: 1.8885724768042564, Final Batch Loss: 0.06744053214788437\n",
      "Epoch 3055, Loss: 1.8856044132262468, Final Batch Loss: 0.009227486327290535\n",
      "Epoch 3056, Loss: 2.4171411097049713, Final Batch Loss: 0.47645458579063416\n",
      "Epoch 3057, Loss: 2.4430440068244934, Final Batch Loss: 0.4188143014907837\n",
      "Epoch 3058, Loss: 2.6321378350257874, Final Batch Loss: 0.8268194198608398\n",
      "Epoch 3059, Loss: 2.7185633778572083, Final Batch Loss: 0.9984881281852722\n",
      "Epoch 3060, Loss: 3.148285150527954, Final Batch Loss: 1.3928625583648682\n",
      "Epoch 3061, Loss: 2.4344449043273926, Final Batch Loss: 0.6589617729187012\n",
      "Epoch 3062, Loss: 2.7928245067596436, Final Batch Loss: 1.016758918762207\n",
      "Epoch 3063, Loss: 1.9312013387680054, Final Batch Loss: 0.16446298360824585\n",
      "Epoch 3064, Loss: 1.8027745792642236, Final Batch Loss: 0.007790775038301945\n",
      "Epoch 3065, Loss: 1.7039172624936327, Final Batch Loss: 0.0010938619961962104\n",
      "Epoch 3066, Loss: 2.152204215526581, Final Batch Loss: 0.4465376138687134\n",
      "Epoch 3067, Loss: 1.778867468237877, Final Batch Loss: 0.05605439841747284\n",
      "Epoch 3068, Loss: 1.754540242254734, Final Batch Loss: 0.07106206566095352\n",
      "Epoch 3069, Loss: 1.7473675995133817, Final Batch Loss: 0.0053145079873502254\n",
      "Epoch 3070, Loss: 1.8029606118798256, Final Batch Loss: 0.053637661039829254\n",
      "Epoch 3071, Loss: 2.2883704900741577, Final Batch Loss: 0.5757578611373901\n",
      "Epoch 3072, Loss: 2.761291205883026, Final Batch Loss: 1.0501009225845337\n",
      "Epoch 3073, Loss: 1.717335183173418, Final Batch Loss: 0.03377797082066536\n",
      "Epoch 3074, Loss: 1.8520786240696907, Final Batch Loss: 0.07159287482500076\n",
      "Epoch 3075, Loss: 1.7788229323923588, Final Batch Loss: 0.0005390383303165436\n",
      "Epoch 3076, Loss: 1.8371216114610434, Final Batch Loss: 0.0022607501596212387\n",
      "Epoch 3077, Loss: 2.3550246357917786, Final Batch Loss: 0.5634610056877136\n",
      "Epoch 3078, Loss: 2.912832796573639, Final Batch Loss: 1.108060359954834\n",
      "Epoch 3079, Loss: 1.9172238111495972, Final Batch Loss: 0.1270676851272583\n",
      "Epoch 3080, Loss: 2.384201943874359, Final Batch Loss: 0.6616527438163757\n",
      "Epoch 3081, Loss: 3.9572858214378357, Final Batch Loss: 2.2958555221557617\n",
      "Epoch 3082, Loss: 1.8296677842736244, Final Batch Loss: 0.028677858412265778\n",
      "Epoch 3083, Loss: 1.777052205055952, Final Batch Loss: 0.019233983010053635\n",
      "Epoch 3084, Loss: 1.745694637298584, Final Batch Loss: 0.06971800327301025\n",
      "Epoch 3085, Loss: 1.9267966151237488, Final Batch Loss: 0.19456541538238525\n",
      "Epoch 3086, Loss: 1.8073042435571551, Final Batch Loss: 0.0003978414461016655\n",
      "Epoch 3087, Loss: 2.8910646438598633, Final Batch Loss: 1.2543325424194336\n",
      "Epoch 3088, Loss: 3.8011040091514587, Final Batch Loss: 2.1250391006469727\n",
      "Epoch 3089, Loss: 1.7765501290559769, Final Batch Loss: 0.01788957417011261\n",
      "Epoch 3090, Loss: 1.8574316054582596, Final Batch Loss: 0.20478899776935577\n",
      "Epoch 3091, Loss: 1.7407579571008682, Final Batch Loss: 0.11516208946704865\n",
      "Epoch 3092, Loss: 1.7735300734639168, Final Batch Loss: 0.06934959441423416\n",
      "Epoch 3093, Loss: 1.6841212059371173, Final Batch Loss: 0.0008283997885882854\n",
      "Epoch 3094, Loss: 1.7243490256369114, Final Batch Loss: 0.0030853785574436188\n",
      "Epoch 3095, Loss: 2.5629903078079224, Final Batch Loss: 0.84088134765625\n",
      "Epoch 3096, Loss: 1.685870934277773, Final Batch Loss: 0.019480574876070023\n",
      "Epoch 3097, Loss: 1.6906545367091894, Final Batch Loss: 0.02953215502202511\n",
      "Epoch 3098, Loss: 2.044939160346985, Final Batch Loss: 0.35915201902389526\n",
      "Epoch 3099, Loss: 1.6112700396915898, Final Batch Loss: 0.0014025861164554954\n",
      "Epoch 3100, Loss: 1.6293040220625699, Final Batch Loss: 0.0063315038569271564\n",
      "Epoch 3101, Loss: 1.713733296841383, Final Batch Loss: 0.05711815878748894\n",
      "Epoch 3102, Loss: 3.2289119958877563, Final Batch Loss: 1.5393822193145752\n",
      "Epoch 3103, Loss: 2.3849180340766907, Final Batch Loss: 0.6643500328063965\n",
      "Epoch 3104, Loss: 1.6926013380289078, Final Batch Loss: 0.027960672974586487\n",
      "Epoch 3105, Loss: 1.7657387154176831, Final Batch Loss: 0.0015503065660595894\n",
      "Epoch 3106, Loss: 1.8258751183748245, Final Batch Loss: 0.10713161528110504\n",
      "Epoch 3107, Loss: 1.7430131807923317, Final Batch Loss: 0.11298579722642899\n",
      "Epoch 3108, Loss: 2.8305745720863342, Final Batch Loss: 1.1186691522598267\n",
      "Epoch 3109, Loss: 2.2436110377311707, Final Batch Loss: 0.6071083545684814\n",
      "Epoch 3110, Loss: 1.849577710032463, Final Batch Loss: 0.12442468106746674\n",
      "Epoch 3111, Loss: 2.2893482744693756, Final Batch Loss: 0.7269710302352905\n",
      "Epoch 3112, Loss: 2.82411527633667, Final Batch Loss: 1.1614693403244019\n",
      "Epoch 3113, Loss: 1.878160685300827, Final Batch Loss: 0.2786942422389984\n",
      "Epoch 3114, Loss: 1.681839495897293, Final Batch Loss: 0.027704566717147827\n",
      "Epoch 3115, Loss: 1.7478740066289902, Final Batch Loss: 0.04628096520900726\n",
      "Epoch 3116, Loss: 1.9960735142230988, Final Batch Loss: 0.2695671617984772\n",
      "Epoch 3117, Loss: 1.704536410048604, Final Batch Loss: 0.02267143316566944\n",
      "Epoch 3118, Loss: 2.4853256344795227, Final Batch Loss: 0.8030570149421692\n",
      "Epoch 3119, Loss: 1.6959773576818407, Final Batch Loss: 0.0043578422628343105\n",
      "Epoch 3120, Loss: 1.7931937519460917, Final Batch Loss: 0.001547330990433693\n",
      "Epoch 3121, Loss: 1.7296702127205208, Final Batch Loss: 0.001473889802582562\n",
      "Epoch 3122, Loss: 1.6633472368121147, Final Batch Loss: 0.0807756707072258\n",
      "Epoch 3123, Loss: 3.827788472175598, Final Batch Loss: 2.0952391624450684\n",
      "Epoch 3124, Loss: 2.0362510681152344, Final Batch Loss: 0.34782910346984863\n",
      "Epoch 3125, Loss: 1.7822472900152206, Final Batch Loss: 0.16718505322933197\n",
      "Epoch 3126, Loss: 2.5044716894626617, Final Batch Loss: 0.9414263367652893\n",
      "Epoch 3127, Loss: 1.643008555052802, Final Batch Loss: 0.0024787436705082655\n",
      "Epoch 3128, Loss: 1.6992641538381577, Final Batch Loss: 0.05191294848918915\n",
      "Epoch 3129, Loss: 1.6341620832681656, Final Batch Loss: 0.018731489777565002\n",
      "Epoch 3130, Loss: 1.714044228196144, Final Batch Loss: 0.12059749662876129\n",
      "Epoch 3131, Loss: 2.1848167777061462, Final Batch Loss: 0.5862092971801758\n",
      "Epoch 3132, Loss: 2.2265998125076294, Final Batch Loss: 0.6495418548583984\n",
      "Epoch 3133, Loss: 1.6291056564077735, Final Batch Loss: 0.003390043042600155\n",
      "Epoch 3134, Loss: 1.7321605440229177, Final Batch Loss: 0.028945064172148705\n",
      "Epoch 3135, Loss: 3.0079101026058197, Final Batch Loss: 1.4016385078430176\n",
      "Epoch 3136, Loss: 1.6624755915254354, Final Batch Loss: 0.0023384150117635727\n",
      "Epoch 3137, Loss: 2.2905940413475037, Final Batch Loss: 0.6924490928649902\n",
      "Epoch 3138, Loss: 1.6952986046671867, Final Batch Loss: 0.059019140899181366\n",
      "Epoch 3139, Loss: 1.5940232761204243, Final Batch Loss: 0.054729390889406204\n",
      "Epoch 3140, Loss: 1.5849377000704408, Final Batch Loss: 0.005213000811636448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3141, Loss: 1.574083813233301, Final Batch Loss: 0.0017210922669619322\n",
      "Epoch 3142, Loss: 1.6365549881011248, Final Batch Loss: 0.015190275385975838\n",
      "Epoch 3143, Loss: 2.2354620695114136, Final Batch Loss: 0.6696228981018066\n",
      "Epoch 3144, Loss: 2.1324451565742493, Final Batch Loss: 0.5186571478843689\n",
      "Epoch 3145, Loss: 1.6111987195909023, Final Batch Loss: 0.061109866946935654\n",
      "Epoch 3146, Loss: 1.7651379108428955, Final Batch Loss: 0.1494302749633789\n",
      "Epoch 3147, Loss: 1.73784738779068, Final Batch Loss: 0.14541023969650269\n",
      "Epoch 3148, Loss: 5.0545783042907715, Final Batch Loss: 3.4210262298583984\n",
      "Epoch 3149, Loss: 1.6999548077583313, Final Batch Loss: 0.027764976024627686\n",
      "Epoch 3150, Loss: 1.6846117675304413, Final Batch Loss: 0.05971384048461914\n",
      "Epoch 3151, Loss: 3.457992196083069, Final Batch Loss: 1.836324691772461\n",
      "Epoch 3152, Loss: 3.211574822664261, Final Batch Loss: 1.5741145610809326\n",
      "Epoch 3153, Loss: 1.5712105876300484, Final Batch Loss: 0.00225242436863482\n",
      "Epoch 3154, Loss: 3.5931633710861206, Final Batch Loss: 1.9750388860702515\n",
      "Epoch 3155, Loss: 3.098974347114563, Final Batch Loss: 1.5471696853637695\n",
      "Epoch 3156, Loss: 3.5705349445343018, Final Batch Loss: 2.008297920227051\n",
      "Epoch 3157, Loss: 1.6925326138734818, Final Batch Loss: 0.07414831221103668\n",
      "Epoch 3158, Loss: 1.741587933152914, Final Batch Loss: 0.03715324029326439\n",
      "Epoch 3159, Loss: 1.665535181760788, Final Batch Loss: 0.034927159547805786\n",
      "Epoch 3160, Loss: 1.837755635380745, Final Batch Loss: 0.17575789988040924\n",
      "Epoch 3161, Loss: 4.295288324356079, Final Batch Loss: 2.695028305053711\n",
      "Epoch 3162, Loss: 2.8974757194519043, Final Batch Loss: 1.2111183404922485\n",
      "Epoch 3163, Loss: 2.7549014687538147, Final Batch Loss: 1.0121444463729858\n",
      "Epoch 3164, Loss: 1.8671148251742125, Final Batch Loss: 0.019045693799853325\n",
      "Epoch 3165, Loss: 2.032057497650385, Final Batch Loss: 0.05968812480568886\n",
      "Epoch 3166, Loss: 2.2843841910362244, Final Batch Loss: 0.4474642276763916\n",
      "Epoch 3167, Loss: 3.7209864258766174, Final Batch Loss: 1.8896832466125488\n",
      "Epoch 3168, Loss: 2.2536078095436096, Final Batch Loss: 0.524169385433197\n",
      "Epoch 3169, Loss: 2.7634605765342712, Final Batch Loss: 1.0652480125427246\n",
      "Epoch 3170, Loss: 1.6655236261431128, Final Batch Loss: 0.0023247378412634134\n",
      "Epoch 3171, Loss: 1.9551482200622559, Final Batch Loss: 0.14292043447494507\n",
      "Epoch 3172, Loss: 3.124984920024872, Final Batch Loss: 1.323203444480896\n",
      "Epoch 3173, Loss: 1.7011269761715084, Final Batch Loss: 0.002994103590026498\n",
      "Epoch 3174, Loss: 1.7136219162493944, Final Batch Loss: 0.017503084614872932\n",
      "Epoch 3175, Loss: 1.6330534466542304, Final Batch Loss: 0.006571824196726084\n",
      "Epoch 3176, Loss: 1.946574330329895, Final Batch Loss: 0.17582368850708008\n",
      "Epoch 3177, Loss: 1.999090999364853, Final Batch Loss: 0.4072791635990143\n",
      "Epoch 3178, Loss: 1.809481292963028, Final Batch Loss: 0.12934473156929016\n",
      "Epoch 3179, Loss: 2.5174671411514282, Final Batch Loss: 0.8916113376617432\n",
      "Epoch 3180, Loss: 2.692543387413025, Final Batch Loss: 1.010277509689331\n",
      "Epoch 3181, Loss: 1.6627377644181252, Final Batch Loss: 0.1016135886311531\n",
      "Epoch 3182, Loss: 1.6892977058887482, Final Batch Loss: 0.03825071454048157\n",
      "Epoch 3183, Loss: 2.6812862157821655, Final Batch Loss: 0.8941308856010437\n",
      "Epoch 3184, Loss: 1.860936053097248, Final Batch Loss: 0.0033075883984565735\n",
      "Epoch 3185, Loss: 1.9875853657722473, Final Batch Loss: 0.34803012013435364\n",
      "Epoch 3186, Loss: 1.7510093413293362, Final Batch Loss: 0.018478509038686752\n",
      "Epoch 3187, Loss: 2.6409286856651306, Final Batch Loss: 0.8395328521728516\n",
      "Epoch 3188, Loss: 1.891779974102974, Final Batch Loss: 0.15677209198474884\n",
      "Epoch 3189, Loss: 1.5969073064625263, Final Batch Loss: 0.03992273285984993\n",
      "Epoch 3190, Loss: 2.6129088401794434, Final Batch Loss: 0.9273634552955627\n",
      "Epoch 3191, Loss: 1.6134233623743057, Final Batch Loss: 0.003952905535697937\n",
      "Epoch 3192, Loss: 3.6178403794765472, Final Batch Loss: 2.0413856506347656\n",
      "Epoch 3193, Loss: 1.9033757001161575, Final Batch Loss: 0.18792618811130524\n",
      "Epoch 3194, Loss: 2.437208592891693, Final Batch Loss: 0.8074742555618286\n",
      "Epoch 3195, Loss: 1.9560520350933075, Final Batch Loss: 0.21436479687690735\n",
      "Epoch 3196, Loss: 1.674268677830696, Final Batch Loss: 0.0627870112657547\n",
      "Epoch 3197, Loss: 1.641260340809822, Final Batch Loss: 0.03464554250240326\n",
      "Epoch 3198, Loss: 3.4456286430358887, Final Batch Loss: 1.8318203687667847\n",
      "Epoch 3199, Loss: 1.7210234247613698, Final Batch Loss: 0.0029437087941914797\n",
      "Epoch 3200, Loss: 2.9110926687717438, Final Batch Loss: 1.313672661781311\n",
      "Epoch 3201, Loss: 2.12580469250679, Final Batch Loss: 0.44102969765663147\n",
      "Epoch 3202, Loss: 1.6779248472303152, Final Batch Loss: 0.01412430964410305\n",
      "Epoch 3203, Loss: 1.5452447184361517, Final Batch Loss: 0.002663994673639536\n",
      "Epoch 3204, Loss: 2.451859712600708, Final Batch Loss: 0.7530369758605957\n",
      "Epoch 3205, Loss: 1.5777746638050303, Final Batch Loss: 0.0006610354175791144\n",
      "Epoch 3206, Loss: 3.3551473021507263, Final Batch Loss: 1.7430815696716309\n",
      "Epoch 3207, Loss: 1.886675089597702, Final Batch Loss: 0.25681009888648987\n",
      "Epoch 3208, Loss: 3.4853225350379944, Final Batch Loss: 1.8715351819992065\n",
      "Epoch 3209, Loss: 1.8626916706562042, Final Batch Loss: 0.22037386894226074\n",
      "Epoch 3210, Loss: 3.4518536925315857, Final Batch Loss: 1.7474831342697144\n",
      "Epoch 3211, Loss: 3.362114727497101, Final Batch Loss: 1.648284912109375\n",
      "Epoch 3212, Loss: 1.7038570567965508, Final Batch Loss: 0.0202934667468071\n",
      "Epoch 3213, Loss: 1.755499660037458, Final Batch Loss: 0.009166537784039974\n",
      "Epoch 3214, Loss: 1.5761118835653178, Final Batch Loss: 0.00034338299883529544\n",
      "Epoch 3215, Loss: 1.5990461444016546, Final Batch Loss: 0.001723829424008727\n",
      "Epoch 3216, Loss: 2.8767586052417755, Final Batch Loss: 1.3261680603027344\n",
      "Epoch 3217, Loss: 1.769960142672062, Final Batch Loss: 0.034055568277835846\n",
      "Epoch 3218, Loss: 6.518411010503769, Final Batch Loss: 4.783487319946289\n",
      "Epoch 3219, Loss: 2.1358672380447388, Final Batch Loss: 0.498357892036438\n",
      "Epoch 3220, Loss: 2.949983835220337, Final Batch Loss: 1.2589713335037231\n",
      "Epoch 3221, Loss: 2.5957566499710083, Final Batch Loss: 0.7668981552124023\n",
      "Epoch 3222, Loss: 1.7115190010517836, Final Batch Loss: 0.0030288081616163254\n",
      "Epoch 3223, Loss: 3.624308228492737, Final Batch Loss: 1.854462742805481\n",
      "Epoch 3224, Loss: 2.0293517634272575, Final Batch Loss: 0.07956814020872116\n",
      "Epoch 3225, Loss: 1.9008430049871095, Final Batch Loss: 0.0007964776013977826\n",
      "Epoch 3226, Loss: 1.9609614736400545, Final Batch Loss: 0.0065691000781953335\n",
      "Epoch 3227, Loss: 3.2941667437553406, Final Batch Loss: 1.4185155630111694\n",
      "Epoch 3228, Loss: 3.0065011978149414, Final Batch Loss: 1.2111034393310547\n",
      "Epoch 3229, Loss: 1.9193554371595383, Final Batch Loss: 0.14851801097393036\n",
      "Epoch 3230, Loss: 2.110076069831848, Final Batch Loss: 0.30004119873046875\n",
      "Epoch 3231, Loss: 2.6054599285125732, Final Batch Loss: 0.7804151177406311\n",
      "Epoch 3232, Loss: 2.5631780326366425, Final Batch Loss: 0.8862556219100952\n",
      "Epoch 3233, Loss: 3.6135271191596985, Final Batch Loss: 1.863268494606018\n",
      "Epoch 3234, Loss: 1.6952587355444848, Final Batch Loss: 4.6132929128361866e-05\n",
      "Epoch 3235, Loss: 1.8704555034637451, Final Batch Loss: 0.1648213267326355\n",
      "Epoch 3236, Loss: 1.6839633099734783, Final Batch Loss: 0.047532182186841965\n",
      "Epoch 3237, Loss: 2.324239730834961, Final Batch Loss: 0.668613851070404\n",
      "Epoch 3238, Loss: 3.7773293256759644, Final Batch Loss: 2.044328451156616\n",
      "Epoch 3239, Loss: 2.271101713180542, Final Batch Loss: 0.43670421838760376\n",
      "Epoch 3240, Loss: 4.499029159545898, Final Batch Loss: 2.707537889480591\n",
      "Epoch 3241, Loss: 1.8802780584956054, Final Batch Loss: 0.0004724340105894953\n",
      "Epoch 3242, Loss: 2.3643433451652527, Final Batch Loss: 0.5266662240028381\n",
      "Epoch 3243, Loss: 2.3503051698207855, Final Batch Loss: 0.6793304681777954\n",
      "Epoch 3244, Loss: 2.6466000080108643, Final Batch Loss: 1.0214120149612427\n",
      "Epoch 3245, Loss: 1.9848948866128922, Final Batch Loss: 0.24231578409671783\n",
      "Epoch 3246, Loss: 1.5964321158826351, Final Batch Loss: 0.004317011684179306\n",
      "Epoch 3247, Loss: 1.698496662080288, Final Batch Loss: 0.06754594296216965\n",
      "Epoch 3248, Loss: 1.7157988008111715, Final Batch Loss: 0.027340179309248924\n",
      "Epoch 3249, Loss: 1.7177354730665684, Final Batch Loss: 0.037952128797769547\n",
      "Epoch 3250, Loss: 1.703173816204071, Final Batch Loss: 0.05678832530975342\n",
      "Epoch 3251, Loss: 1.57001849450171, Final Batch Loss: 0.002087915316224098\n",
      "Epoch 3252, Loss: 2.5038071274757385, Final Batch Loss: 0.849031925201416\n",
      "Epoch 3253, Loss: 1.8551427721977234, Final Batch Loss: 0.30175742506980896\n",
      "Epoch 3254, Loss: 1.5394878182560205, Final Batch Loss: 0.01840091682970524\n",
      "Epoch 3255, Loss: 3.6696246564388275, Final Batch Loss: 2.082420825958252\n",
      "Epoch 3256, Loss: 1.731778459623456, Final Batch Loss: 0.005981402471661568\n",
      "Epoch 3257, Loss: 4.612895756959915, Final Batch Loss: 3.000955104827881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3258, Loss: 1.638672056782525, Final Batch Loss: 0.0006861716392450035\n",
      "Epoch 3259, Loss: 1.6164070777595043, Final Batch Loss: 0.030049841850996017\n",
      "Epoch 3260, Loss: 1.7304382771253586, Final Batch Loss: 0.11251305043697357\n",
      "Epoch 3261, Loss: 1.8426194041967392, Final Batch Loss: 0.2178833931684494\n",
      "Epoch 3262, Loss: 2.206624746322632, Final Batch Loss: 0.5727699995040894\n",
      "Epoch 3263, Loss: 1.7844828590750694, Final Batch Loss: 0.11725827306509018\n",
      "Epoch 3264, Loss: 1.998452678322792, Final Batch Loss: 0.22354306280612946\n",
      "Epoch 3265, Loss: 5.889948487281799, Final Batch Loss: 4.107176780700684\n",
      "Epoch 3266, Loss: 2.355895757675171, Final Batch Loss: 0.6997230052947998\n",
      "Epoch 3267, Loss: 1.5998271256685257, Final Batch Loss: 0.024645254015922546\n",
      "Epoch 3268, Loss: 3.6841124296188354, Final Batch Loss: 1.93455171585083\n",
      "Epoch 3269, Loss: 2.091562718153, Final Batch Loss: 0.2141614854335785\n",
      "Epoch 3270, Loss: 2.2368796467781067, Final Batch Loss: 0.3297237157821655\n",
      "Epoch 3271, Loss: 2.7971800565719604, Final Batch Loss: 0.9578055143356323\n",
      "Epoch 3272, Loss: 2.1828648895025253, Final Batch Loss: 0.22291283309459686\n",
      "Epoch 3273, Loss: 2.13007752597332, Final Batch Loss: 0.21151654422283173\n",
      "Epoch 3274, Loss: 2.252459228038788, Final Batch Loss: 0.38106346130371094\n",
      "Epoch 3275, Loss: 1.8939195238053799, Final Batch Loss: 0.021561618894338608\n",
      "Epoch 3276, Loss: 1.8501698952168226, Final Batch Loss: 0.010236619040369987\n",
      "Epoch 3277, Loss: 1.829937748145312, Final Batch Loss: 0.005199007224291563\n",
      "Epoch 3278, Loss: 2.833010733127594, Final Batch Loss: 1.0466303825378418\n",
      "Epoch 3279, Loss: 1.780181184411049, Final Batch Loss: 0.20021040737628937\n",
      "Epoch 3280, Loss: 1.9958901703357697, Final Batch Loss: 0.27128881216049194\n",
      "Epoch 3281, Loss: 1.8874660730361938, Final Batch Loss: 0.11187082529067993\n",
      "Epoch 3282, Loss: 1.6797038540244102, Final Batch Loss: 0.08453752845525742\n",
      "Epoch 3283, Loss: 1.8709033131599426, Final Batch Loss: 0.282677561044693\n",
      "Epoch 3284, Loss: 1.6233135173097253, Final Batch Loss: 0.014199883677065372\n",
      "Epoch 3285, Loss: 1.7067339159548283, Final Batch Loss: 0.01586434617638588\n",
      "Epoch 3286, Loss: 1.7380089312791824, Final Batch Loss: 0.11390413343906403\n",
      "Epoch 3287, Loss: 1.5644100308418274, Final Batch Loss: 0.03097081184387207\n",
      "Epoch 3288, Loss: 1.729213159531355, Final Batch Loss: 0.0573599748313427\n",
      "Epoch 3289, Loss: 1.565331525169313, Final Batch Loss: 0.001964307390153408\n",
      "Epoch 3290, Loss: 3.0567919611930847, Final Batch Loss: 1.5525357723236084\n",
      "Epoch 3291, Loss: 1.7037351205945015, Final Batch Loss: 0.026120103895664215\n",
      "Epoch 3292, Loss: 3.1922091245651245, Final Batch Loss: 1.6124885082244873\n",
      "Epoch 3293, Loss: 1.7722701225429773, Final Batch Loss: 0.016301551833748817\n",
      "Epoch 3294, Loss: 2.4509664475917816, Final Batch Loss: 0.794879674911499\n",
      "Epoch 3295, Loss: 1.6487849492114037, Final Batch Loss: 0.0037604111712425947\n",
      "Epoch 3296, Loss: 3.0324700474739075, Final Batch Loss: 1.3424464464187622\n",
      "Epoch 3297, Loss: 2.6435931026935577, Final Batch Loss: 1.0310394763946533\n",
      "Epoch 3298, Loss: 1.6292513862717897, Final Batch Loss: 0.0016032711137086153\n",
      "Epoch 3299, Loss: 2.318734735250473, Final Batch Loss: 0.6881649494171143\n",
      "Epoch 3300, Loss: 1.6858424171805382, Final Batch Loss: 0.07174911350011826\n",
      "Epoch 3301, Loss: 1.610134556889534, Final Batch Loss: 0.09659932553768158\n",
      "Epoch 3302, Loss: 1.569933946011588, Final Batch Loss: 0.000851387856528163\n",
      "Epoch 3303, Loss: 1.6578171662986279, Final Batch Loss: 0.032783906906843185\n",
      "Epoch 3304, Loss: 1.8910191059112549, Final Batch Loss: 0.2422718107700348\n",
      "Epoch 3305, Loss: 1.6647974126390181, Final Batch Loss: 0.0003800861886702478\n",
      "Epoch 3306, Loss: 2.6602070033550262, Final Batch Loss: 0.9935913681983948\n",
      "Epoch 3307, Loss: 1.724639137275517, Final Batch Loss: 0.011484582908451557\n",
      "Epoch 3308, Loss: 2.0435178577899933, Final Batch Loss: 0.44283533096313477\n",
      "Epoch 3309, Loss: 1.6195037700235844, Final Batch Loss: 0.041829559952020645\n",
      "Epoch 3310, Loss: 4.467868000268936, Final Batch Loss: 2.8180229663848877\n",
      "Epoch 3311, Loss: 1.7002497017383575, Final Batch Loss: 0.07327038049697876\n",
      "Epoch 3312, Loss: 1.6557326959446073, Final Batch Loss: 0.013951514847576618\n",
      "Epoch 3313, Loss: 1.6351071782410145, Final Batch Loss: 0.059595245867967606\n",
      "Epoch 3314, Loss: 1.694781230064109, Final Batch Loss: 0.003703997703269124\n",
      "Epoch 3315, Loss: 1.9209899604320526, Final Batch Loss: 0.365885466337204\n",
      "Epoch 3316, Loss: 1.8029739707708359, Final Batch Loss: 0.19780422747135162\n",
      "Epoch 3317, Loss: 1.85439795255661, Final Batch Loss: 0.24020695686340332\n",
      "Epoch 3318, Loss: 1.7638577595353127, Final Batch Loss: 0.11832309514284134\n",
      "Epoch 3319, Loss: 1.622414879500866, Final Batch Loss: 0.05592229217290878\n",
      "Epoch 3320, Loss: 1.571043714415282, Final Batch Loss: 0.004012748133391142\n",
      "Epoch 3321, Loss: 1.5800607850542292, Final Batch Loss: 0.0005895545473322272\n",
      "Epoch 3322, Loss: 1.7353849932551384, Final Batch Loss: 0.03601443022489548\n",
      "Epoch 3323, Loss: 1.7793843448162079, Final Batch Loss: 0.19736307859420776\n",
      "Epoch 3324, Loss: 1.6301311515271664, Final Batch Loss: 0.031269874423742294\n",
      "Epoch 3325, Loss: 2.9088030457496643, Final Batch Loss: 1.264060139656067\n",
      "Epoch 3326, Loss: 1.6449321191757917, Final Batch Loss: 0.02973109297454357\n",
      "Epoch 3327, Loss: 1.5663071796298027, Final Batch Loss: 0.032856814563274384\n",
      "Epoch 3328, Loss: 1.5970276910811663, Final Batch Loss: 0.02074018307030201\n",
      "Epoch 3329, Loss: 1.5687704030424356, Final Batch Loss: 0.021139079704880714\n",
      "Epoch 3330, Loss: 1.8415542840957642, Final Batch Loss: 0.3480600118637085\n",
      "Epoch 3331, Loss: 1.565048485994339, Final Batch Loss: 0.12825655937194824\n",
      "Epoch 3332, Loss: 2.4372825622558594, Final Batch Loss: 0.8820295333862305\n",
      "Epoch 3333, Loss: 4.427060961723328, Final Batch Loss: 2.893284320831299\n",
      "Epoch 3334, Loss: 1.6150827780365944, Final Batch Loss: 0.027131833136081696\n",
      "Epoch 3335, Loss: 1.5915656457655132, Final Batch Loss: 0.0040146480314433575\n",
      "Epoch 3336, Loss: 1.728780023753643, Final Batch Loss: 0.10123013705015182\n",
      "Epoch 3337, Loss: 2.7716512978076935, Final Batch Loss: 1.2180328369140625\n",
      "Epoch 3338, Loss: 3.004786103963852, Final Batch Loss: 1.4344991445541382\n",
      "Epoch 3339, Loss: 1.6154991425573826, Final Batch Loss: 0.011243436485528946\n",
      "Epoch 3340, Loss: 1.8726586513221264, Final Batch Loss: 0.014348428696393967\n",
      "Epoch 3341, Loss: 1.9865341677796096, Final Batch Loss: 0.0029318227898329496\n",
      "Epoch 3342, Loss: 2.0771229937672615, Final Batch Loss: 0.08886725455522537\n",
      "Epoch 3343, Loss: 2.6055753231048584, Final Batch Loss: 0.6782575249671936\n",
      "Epoch 3344, Loss: 1.9348038956522942, Final Batch Loss: 0.10932301729917526\n",
      "Epoch 3345, Loss: 1.6970490836538374, Final Batch Loss: 0.004110934678465128\n",
      "Epoch 3346, Loss: 1.5789487268775702, Final Batch Loss: 0.0013874676078557968\n",
      "Epoch 3347, Loss: 2.298181414604187, Final Batch Loss: 0.682587742805481\n",
      "Epoch 3348, Loss: 1.939761996269226, Final Batch Loss: 0.41351786255836487\n",
      "Epoch 3349, Loss: 1.593645790591836, Final Batch Loss: 0.01274699904024601\n",
      "Epoch 3350, Loss: 1.5749770402908325, Final Batch Loss: 0.013507753610610962\n",
      "Epoch 3351, Loss: 2.8832486271858215, Final Batch Loss: 1.2921974658966064\n",
      "Epoch 3352, Loss: 2.25697985291481, Final Batch Loss: 0.7648780941963196\n",
      "Epoch 3353, Loss: 1.562571793794632, Final Batch Loss: 0.050440430641174316\n",
      "Epoch 3354, Loss: 2.1203821301460266, Final Batch Loss: 0.6333091855049133\n",
      "Epoch 3355, Loss: 2.215984284877777, Final Batch Loss: 0.7121463418006897\n",
      "Epoch 3356, Loss: 1.8526235818862915, Final Batch Loss: 0.29962068796157837\n",
      "Epoch 3357, Loss: 3.4202064871788025, Final Batch Loss: 1.6886389255523682\n",
      "Epoch 3358, Loss: 1.6979989781975746, Final Batch Loss: 0.07473690062761307\n",
      "Epoch 3359, Loss: 1.8334781732410192, Final Batch Loss: 0.02709551714360714\n",
      "Epoch 3360, Loss: 2.127022475004196, Final Batch Loss: 0.14854195713996887\n",
      "Epoch 3361, Loss: 3.779611587524414, Final Batch Loss: 1.6665480136871338\n",
      "Epoch 3362, Loss: 1.9973927582614124, Final Batch Loss: 0.007517501246184111\n",
      "Epoch 3363, Loss: 2.1764408499002457, Final Batch Loss: 0.21606911718845367\n",
      "Epoch 3364, Loss: 1.9184719030745327, Final Batch Loss: 0.004622132051736116\n",
      "Epoch 3365, Loss: 1.741544783115387, Final Batch Loss: 0.011961400508880615\n",
      "Epoch 3366, Loss: 1.7197229098528624, Final Batch Loss: 0.024506373330950737\n",
      "Epoch 3367, Loss: 2.5559515058994293, Final Batch Loss: 0.963116466999054\n",
      "Epoch 3368, Loss: 1.5943154469132423, Final Batch Loss: 0.029281891882419586\n",
      "Epoch 3369, Loss: 2.3646835684776306, Final Batch Loss: 0.8154664039611816\n",
      "Epoch 3370, Loss: 1.61609215894714, Final Batch Loss: 0.002861930523067713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3371, Loss: 1.7180011719465256, Final Batch Loss: 0.05665688216686249\n",
      "Epoch 3372, Loss: 3.652470052242279, Final Batch Loss: 1.9115339517593384\n",
      "Epoch 3373, Loss: 2.937748670578003, Final Batch Loss: 1.3177993297576904\n",
      "Epoch 3374, Loss: 2.142964094877243, Final Batch Loss: 0.44857892394065857\n",
      "Epoch 3375, Loss: 1.6148600801825523, Final Batch Loss: 0.0829920694231987\n",
      "Epoch 3376, Loss: 1.6845300486311316, Final Batch Loss: 0.0053275516256690025\n",
      "Epoch 3377, Loss: 1.5255272015929222, Final Batch Loss: 0.03035079687833786\n",
      "Epoch 3378, Loss: 1.5541988909244537, Final Batch Loss: 0.009771019220352173\n",
      "Epoch 3379, Loss: 2.233783721923828, Final Batch Loss: 0.598027765750885\n",
      "Epoch 3380, Loss: 1.8702697455883026, Final Batch Loss: 0.20201966166496277\n",
      "Epoch 3381, Loss: 1.5057973908260465, Final Batch Loss: 0.0027913199737668037\n",
      "Epoch 3382, Loss: 1.6572703015990555, Final Batch Loss: 0.000924993772059679\n",
      "Epoch 3383, Loss: 1.6817976534366608, Final Batch Loss: 0.032712727785110474\n",
      "Epoch 3384, Loss: 1.5785139445215464, Final Batch Loss: 0.0034522954374551773\n",
      "Epoch 3385, Loss: 1.7230685502290726, Final Batch Loss: 0.17426525056362152\n",
      "Epoch 3386, Loss: 2.0446351766586304, Final Batch Loss: 0.49945124983787537\n",
      "Epoch 3387, Loss: 1.5032545689027756, Final Batch Loss: 0.0017733338754624128\n",
      "Epoch 3388, Loss: 1.604012837371556, Final Batch Loss: 0.00013839241000823677\n",
      "Epoch 3389, Loss: 1.6175040611997247, Final Batch Loss: 0.011951387859880924\n",
      "Epoch 3390, Loss: 2.581219971179962, Final Batch Loss: 1.028322458267212\n",
      "Epoch 3391, Loss: 1.5926658809185028, Final Batch Loss: 0.08388441801071167\n",
      "Epoch 3392, Loss: 2.6226980686187744, Final Batch Loss: 1.0204248428344727\n",
      "Epoch 3393, Loss: 4.698742181062698, Final Batch Loss: 3.1802890300750732\n",
      "Epoch 3394, Loss: 1.764914408326149, Final Batch Loss: 0.15993694961071014\n",
      "Epoch 3395, Loss: 6.819327026605606, Final Batch Loss: 5.28449821472168\n",
      "Epoch 3396, Loss: 1.707820512354374, Final Batch Loss: 0.07034198194742203\n",
      "Epoch 3397, Loss: 4.196669340133667, Final Batch Loss: 2.312891721725464\n",
      "Epoch 3398, Loss: 2.2785560339689255, Final Batch Loss: 0.22323672473430634\n",
      "Epoch 3399, Loss: 2.0716752256266773, Final Batch Loss: 0.005521997343748808\n",
      "Epoch 3400, Loss: 2.033268607221544, Final Batch Loss: 0.01453313883394003\n",
      "Epoch 3401, Loss: 2.0059057146136183, Final Batch Loss: 0.00042989550274796784\n",
      "Epoch 3402, Loss: 1.9678789973258972, Final Batch Loss: 0.230438232421875\n",
      "Epoch 3403, Loss: 2.273085594177246, Final Batch Loss: 0.4974062442779541\n",
      "Epoch 3404, Loss: 1.9142998605966568, Final Batch Loss: 0.1536603718996048\n",
      "Epoch 3405, Loss: 1.8421659022569656, Final Batch Loss: 0.13230569660663605\n",
      "Epoch 3406, Loss: 3.2163215279579163, Final Batch Loss: 1.5470869541168213\n",
      "Epoch 3407, Loss: 1.6888860035687685, Final Batch Loss: 0.016995148733258247\n",
      "Epoch 3408, Loss: 2.3124555945396423, Final Batch Loss: 0.6574726104736328\n",
      "Epoch 3409, Loss: 1.931805670261383, Final Batch Loss: 0.3517281413078308\n",
      "Epoch 3410, Loss: 2.169459581375122, Final Batch Loss: 0.5583254098892212\n",
      "Epoch 3411, Loss: 1.5403752946294844, Final Batch Loss: 0.0043362402357161045\n",
      "Epoch 3412, Loss: 1.6158087067306042, Final Batch Loss: 0.00911657139658928\n",
      "Epoch 3413, Loss: 1.7110300798667595, Final Batch Loss: 0.0005743046058341861\n",
      "Epoch 3414, Loss: 1.972541257739067, Final Batch Loss: 0.2052401751279831\n",
      "Epoch 3415, Loss: 1.6999208331108093, Final Batch Loss: 0.11075407266616821\n",
      "Epoch 3416, Loss: 1.9385508298873901, Final Batch Loss: 0.4045895040035248\n",
      "Epoch 3417, Loss: 1.615624401718378, Final Batch Loss: 0.05604008212685585\n",
      "Epoch 3418, Loss: 1.642425961792469, Final Batch Loss: 0.07111380249261856\n",
      "Epoch 3419, Loss: 4.426063269376755, Final Batch Loss: 2.8672053813934326\n",
      "Epoch 3420, Loss: 3.1634875535964966, Final Batch Loss: 1.6511788368225098\n",
      "Epoch 3421, Loss: 1.855625107884407, Final Batch Loss: 0.22349949181079865\n",
      "Epoch 3422, Loss: 6.6307860016822815, Final Batch Loss: 4.988340377807617\n",
      "Epoch 3423, Loss: 1.8137844838202, Final Batch Loss: 0.04350120201706886\n",
      "Epoch 3424, Loss: 2.4543582797050476, Final Batch Loss: 0.2501581907272339\n",
      "Epoch 3425, Loss: 2.937766134738922, Final Batch Loss: 0.564357340335846\n",
      "Epoch 3426, Loss: 2.8695589303970337, Final Batch Loss: 0.21940255165100098\n",
      "Epoch 3427, Loss: 2.897976279258728, Final Batch Loss: 0.2825922966003418\n",
      "Epoch 3428, Loss: 2.3093162178993225, Final Batch Loss: 0.2778483033180237\n",
      "Epoch 3429, Loss: 2.1794918924570084, Final Batch Loss: 0.1497589498758316\n",
      "Epoch 3430, Loss: 2.04507102095522, Final Batch Loss: 0.0013255628291517496\n",
      "Epoch 3431, Loss: 2.2695310711860657, Final Batch Loss: 0.38603729009628296\n",
      "Epoch 3432, Loss: 1.943129975348711, Final Batch Loss: 0.025896865874528885\n",
      "Epoch 3433, Loss: 1.9302041418850422, Final Batch Loss: 0.03414808586239815\n",
      "Epoch 3434, Loss: 2.625169098377228, Final Batch Loss: 0.6415794491767883\n",
      "Epoch 3435, Loss: 1.9999918937683105, Final Batch Loss: 0.14536458253860474\n",
      "Epoch 3436, Loss: 5.2590107917785645, Final Batch Loss: 3.451746702194214\n",
      "Epoch 3437, Loss: 1.8867650218307972, Final Batch Loss: 0.01218070462346077\n",
      "Epoch 3438, Loss: 2.129784047603607, Final Batch Loss: 0.3517953157424927\n",
      "Epoch 3439, Loss: 3.3717001080513, Final Batch Loss: 1.5747476816177368\n",
      "Epoch 3440, Loss: 8.064572215080261, Final Batch Loss: 6.301360130310059\n",
      "Epoch 3441, Loss: 1.866406712681055, Final Batch Loss: 0.026719186455011368\n",
      "Epoch 3442, Loss: 2.021530075930059, Final Batch Loss: 0.005073173902928829\n",
      "Epoch 3443, Loss: 3.4533299803733826, Final Batch Loss: 1.4229973554611206\n",
      "Epoch 3444, Loss: 1.9911969211825635, Final Batch Loss: 0.00037102968781255186\n",
      "Epoch 3445, Loss: 3.9618579149246216, Final Batch Loss: 1.989891529083252\n",
      "Epoch 3446, Loss: 3.534390926361084, Final Batch Loss: 1.7088332176208496\n",
      "Epoch 3447, Loss: 3.260771870613098, Final Batch Loss: 1.3667484521865845\n",
      "Epoch 3448, Loss: 1.8035146240144968, Final Batch Loss: 0.012214267626404762\n",
      "Epoch 3449, Loss: 1.8412992348894477, Final Batch Loss: 0.00734782125800848\n",
      "Epoch 3450, Loss: 2.427389919757843, Final Batch Loss: 0.6474838256835938\n",
      "Epoch 3451, Loss: 3.062883138656616, Final Batch Loss: 1.3806865215301514\n",
      "Epoch 3452, Loss: 1.762266144156456, Final Batch Loss: 0.02291904389858246\n",
      "Epoch 3453, Loss: 1.7164796404540539, Final Batch Loss: 0.009854476898908615\n",
      "Epoch 3454, Loss: 1.6250058729201555, Final Batch Loss: 0.01460045762360096\n",
      "Epoch 3455, Loss: 2.262157678604126, Final Batch Loss: 0.5604707598686218\n",
      "Epoch 3456, Loss: 1.6859219372272491, Final Batch Loss: 0.07326915860176086\n",
      "Epoch 3457, Loss: 2.860381782054901, Final Batch Loss: 1.262472152709961\n",
      "Epoch 3458, Loss: 1.7774082235991955, Final Batch Loss: 0.0451495461165905\n",
      "Epoch 3459, Loss: 2.133214814355597, Final Batch Loss: 0.001769644906744361\n",
      "Epoch 3460, Loss: 2.426974408328533, Final Batch Loss: 0.08799319714307785\n",
      "Epoch 3461, Loss: 2.486235290300101, Final Batch Loss: 0.00544884754344821\n",
      "Epoch 3462, Loss: 3.1349000930786133, Final Batch Loss: 1.1495883464813232\n",
      "Epoch 3463, Loss: 1.7999124201014638, Final Batch Loss: 0.009039667434990406\n",
      "Epoch 3464, Loss: 1.8407495096325874, Final Batch Loss: 0.07518107444047928\n",
      "Epoch 3465, Loss: 1.6354416199028492, Final Batch Loss: 0.05544354394078255\n",
      "Epoch 3466, Loss: 1.558625886682421, Final Batch Loss: 0.0022980966605246067\n",
      "Epoch 3467, Loss: 1.5935700852423906, Final Batch Loss: 0.0019666869193315506\n",
      "Epoch 3468, Loss: 2.893235146999359, Final Batch Loss: 1.3068286180496216\n",
      "Epoch 3469, Loss: 1.7111764550209045, Final Batch Loss: 0.1476026475429535\n",
      "Epoch 3470, Loss: 2.77771332859993, Final Batch Loss: 1.136549949645996\n",
      "Epoch 3471, Loss: 1.6943275779485703, Final Batch Loss: 0.17338119447231293\n",
      "Epoch 3472, Loss: 2.104573577642441, Final Batch Loss: 0.523307204246521\n",
      "Epoch 3473, Loss: 1.6640455275774002, Final Batch Loss: 0.07290072739124298\n",
      "Epoch 3474, Loss: 1.689519327133894, Final Batch Loss: 0.0485500805079937\n",
      "Epoch 3475, Loss: 1.5825070925056934, Final Batch Loss: 0.03237428143620491\n",
      "Epoch 3476, Loss: 2.871315598487854, Final Batch Loss: 1.2871079444885254\n",
      "Epoch 3477, Loss: 2.308007299900055, Final Batch Loss: 0.7184343338012695\n",
      "Epoch 3478, Loss: 3.7574665546417236, Final Batch Loss: 2.1816823482513428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3479, Loss: 2.5241830945014954, Final Batch Loss: 0.8212340474128723\n",
      "Epoch 3480, Loss: 2.9300079941749573, Final Batch Loss: 1.2725435495376587\n",
      "Epoch 3481, Loss: 2.91131454706192, Final Batch Loss: 1.0772430896759033\n",
      "Epoch 3482, Loss: 2.324005216360092, Final Batch Loss: 0.48906615376472473\n",
      "Epoch 3483, Loss: 1.957770437002182, Final Batch Loss: 0.2586372196674347\n",
      "Epoch 3484, Loss: 1.6311361306579784, Final Batch Loss: 0.0012828224571421742\n",
      "Epoch 3485, Loss: 1.7312540486454964, Final Batch Loss: 0.08128849416971207\n",
      "Epoch 3486, Loss: 1.6716912961564958, Final Batch Loss: 0.005751607473939657\n",
      "Epoch 3487, Loss: 1.7242452800273895, Final Batch Loss: 0.03516450524330139\n",
      "Epoch 3488, Loss: 1.6079939976334572, Final Batch Loss: 0.07170084863901138\n",
      "Epoch 3489, Loss: 3.1453593373298645, Final Batch Loss: 1.5369064807891846\n",
      "Epoch 3490, Loss: 2.4755550622940063, Final Batch Loss: 0.8332652449607849\n",
      "Epoch 3491, Loss: 2.4443666338920593, Final Batch Loss: 0.8705092072486877\n",
      "Epoch 3492, Loss: 3.3883715867996216, Final Batch Loss: 1.7969638109207153\n",
      "Epoch 3493, Loss: 2.5519169867038727, Final Batch Loss: 0.9967127442359924\n",
      "Epoch 3494, Loss: 1.5677014238899574, Final Batch Loss: 0.0007394201820716262\n",
      "Epoch 3495, Loss: 1.9606429636478424, Final Batch Loss: 0.03444930911064148\n",
      "Epoch 3496, Loss: 3.9267836213111877, Final Batch Loss: 1.7971994876861572\n",
      "Epoch 3497, Loss: 4.225002586841583, Final Batch Loss: 2.036041259765625\n",
      "Epoch 3498, Loss: 2.0775476885028183, Final Batch Loss: 0.004726309794932604\n",
      "Epoch 3499, Loss: 3.594367206096649, Final Batch Loss: 1.7380292415618896\n",
      "Epoch 3500, Loss: 2.0566823184490204, Final Batch Loss: 0.2725997865200043\n",
      "Epoch 3501, Loss: 3.138915002346039, Final Batch Loss: 1.390371561050415\n",
      "Epoch 3502, Loss: 3.168780744075775, Final Batch Loss: 1.3975944519042969\n",
      "Epoch 3503, Loss: 2.0240565836429596, Final Batch Loss: 0.17422530055046082\n",
      "Epoch 3504, Loss: 2.1567313373088837, Final Batch Loss: 0.40695706009864807\n",
      "Epoch 3505, Loss: 3.056407153606415, Final Batch Loss: 1.3954732418060303\n",
      "Epoch 3506, Loss: 1.7010905295610428, Final Batch Loss: 0.03391142189502716\n",
      "Epoch 3507, Loss: 2.4635047912597656, Final Batch Loss: 0.7883034944534302\n",
      "Epoch 3508, Loss: 1.7038946002721786, Final Batch Loss: 0.07160608470439911\n",
      "Epoch 3509, Loss: 3.3521607518196106, Final Batch Loss: 1.7602245807647705\n",
      "Epoch 3510, Loss: 1.5581412613391876, Final Batch Loss: 0.07780060172080994\n",
      "Epoch 3511, Loss: 1.680393397808075, Final Batch Loss: 0.06985741853713989\n",
      "Epoch 3512, Loss: 1.5666738734580576, Final Batch Loss: 0.002477316651493311\n",
      "Epoch 3513, Loss: 1.6754411198198795, Final Batch Loss: 0.017565283924341202\n",
      "Epoch 3514, Loss: 2.082881987094879, Final Batch Loss: 0.4586336016654968\n",
      "Epoch 3515, Loss: 1.756125032901764, Final Batch Loss: 0.07910168170928955\n",
      "Epoch 3516, Loss: 1.6105805889237672, Final Batch Loss: 0.0031735554803162813\n",
      "Epoch 3517, Loss: 1.5388973988592625, Final Batch Loss: 0.0011152960360050201\n",
      "Epoch 3518, Loss: 1.7076907306909561, Final Batch Loss: 0.052904918789863586\n",
      "Epoch 3519, Loss: 1.676825050264597, Final Batch Loss: 0.019002769142389297\n",
      "Epoch 3520, Loss: 1.956207036972046, Final Batch Loss: 0.3577415943145752\n",
      "Epoch 3521, Loss: 1.60442628338933, Final Batch Loss: 0.05770008638501167\n",
      "Epoch 3522, Loss: 1.6030127555131912, Final Batch Loss: 0.05076681077480316\n",
      "Epoch 3523, Loss: 1.8008810430765152, Final Batch Loss: 0.09255321323871613\n",
      "Epoch 3524, Loss: 1.6013287417590618, Final Batch Loss: 0.0547226183116436\n",
      "Epoch 3525, Loss: 2.7017813324928284, Final Batch Loss: 1.1186981201171875\n",
      "Epoch 3526, Loss: 1.706520900130272, Final Batch Loss: 0.16973437368869781\n",
      "Epoch 3527, Loss: 2.099338859319687, Final Batch Loss: 0.5728857517242432\n",
      "Epoch 3528, Loss: 1.5676673627458513, Final Batch Loss: 0.007101414259523153\n",
      "Epoch 3529, Loss: 2.409525752067566, Final Batch Loss: 0.8229184746742249\n",
      "Epoch 3530, Loss: 3.027243673801422, Final Batch Loss: 1.4362668991088867\n",
      "Epoch 3531, Loss: 1.9754063189029694, Final Batch Loss: 0.5299602150917053\n",
      "Epoch 3532, Loss: 2.030450254678726, Final Batch Loss: 0.5141362547874451\n",
      "Epoch 3533, Loss: 1.4956048470921814, Final Batch Loss: 0.006302718538790941\n",
      "Epoch 3534, Loss: 1.570185137912631, Final Batch Loss: 0.011908156797289848\n",
      "Epoch 3535, Loss: 1.5631094820564613, Final Batch Loss: 0.0016037471359595656\n",
      "Epoch 3536, Loss: 2.5309287011623383, Final Batch Loss: 0.9418513178825378\n",
      "Epoch 3537, Loss: 1.5779482815414667, Final Batch Loss: 0.02722151018679142\n",
      "Epoch 3538, Loss: 1.5253972876816988, Final Batch Loss: 0.022486822679638863\n",
      "Epoch 3539, Loss: 1.732259839773178, Final Batch Loss: 0.14616087079048157\n",
      "Epoch 3540, Loss: 3.35421359539032, Final Batch Loss: 1.8724138736724854\n",
      "Epoch 3541, Loss: 1.5421033062739298, Final Batch Loss: 0.0011570908827707171\n",
      "Epoch 3542, Loss: 1.4802187206223607, Final Batch Loss: 0.004278791137039661\n",
      "Epoch 3543, Loss: 1.5944872498512268, Final Batch Loss: 0.1255442500114441\n",
      "Epoch 3544, Loss: 2.487046539783478, Final Batch Loss: 0.970137894153595\n",
      "Epoch 3545, Loss: 1.6099442839622498, Final Batch Loss: 0.09961262345314026\n",
      "Epoch 3546, Loss: 1.5950979180634022, Final Batch Loss: 0.05052464082837105\n",
      "Epoch 3547, Loss: 1.7578518390655518, Final Batch Loss: 0.2628138065338135\n",
      "Epoch 3548, Loss: 1.4493531991029158, Final Batch Loss: 0.0014365125680342317\n",
      "Epoch 3549, Loss: 2.8090773224830627, Final Batch Loss: 1.253922939300537\n",
      "Epoch 3550, Loss: 1.6950176507234573, Final Batch Loss: 0.16405554115772247\n",
      "Epoch 3551, Loss: 1.59305514767766, Final Batch Loss: 0.008287090808153152\n",
      "Epoch 3552, Loss: 1.7070424854755402, Final Batch Loss: 0.1716192364692688\n",
      "Epoch 3553, Loss: 1.456276890821755, Final Batch Loss: 0.003994819708168507\n",
      "Epoch 3554, Loss: 1.7626368403434753, Final Batch Loss: 0.20316475629806519\n",
      "Epoch 3555, Loss: 1.7435596138238907, Final Batch Loss: 0.16681011021137238\n",
      "Epoch 3556, Loss: 1.5251934193074703, Final Batch Loss: 0.03651302680373192\n",
      "Epoch 3557, Loss: 1.5541225452907383, Final Batch Loss: 0.004167087841778994\n",
      "Epoch 3558, Loss: 2.3016541600227356, Final Batch Loss: 0.6687930226325989\n",
      "Epoch 3559, Loss: 1.7668760269880295, Final Batch Loss: 0.14242295920848846\n",
      "Epoch 3560, Loss: 1.66902856528759, Final Batch Loss: 0.18570686876773834\n",
      "Epoch 3561, Loss: 1.578217601403594, Final Batch Loss: 0.00818411074578762\n",
      "Epoch 3562, Loss: 1.492616817355156, Final Batch Loss: 0.051174864172935486\n",
      "Epoch 3563, Loss: 1.7720066756010056, Final Batch Loss: 0.23300571739673615\n",
      "Epoch 3564, Loss: 1.5415665805339813, Final Batch Loss: 0.17899629473686218\n",
      "Epoch 3565, Loss: 1.4526620978140272, Final Batch Loss: 0.0007491880678571761\n",
      "Epoch 3566, Loss: 2.03071129322052, Final Batch Loss: 0.5780470371246338\n",
      "Epoch 3567, Loss: 1.6273595541715622, Final Batch Loss: 0.09505744278430939\n",
      "Epoch 3568, Loss: 1.561720789410174, Final Batch Loss: 0.004761428572237492\n",
      "Epoch 3569, Loss: 2.3558966517448425, Final Batch Loss: 0.7425008416175842\n",
      "Epoch 3570, Loss: 1.5196048114448786, Final Batch Loss: 0.0038630161434412003\n",
      "Epoch 3571, Loss: 3.261232018470764, Final Batch Loss: 1.6603422164916992\n",
      "Epoch 3572, Loss: 1.9828080534934998, Final Batch Loss: 0.4634785056114197\n",
      "Epoch 3573, Loss: 2.3759743571281433, Final Batch Loss: 0.8965858817100525\n",
      "Epoch 3574, Loss: 2.2667853236198425, Final Batch Loss: 0.7198469638824463\n",
      "Epoch 3575, Loss: 1.542393668089062, Final Batch Loss: 0.0021629766561090946\n",
      "Epoch 3576, Loss: 1.6554153263568878, Final Batch Loss: 0.0845600962638855\n",
      "Epoch 3577, Loss: 1.6093068229965866, Final Batch Loss: 0.005800794344395399\n",
      "Epoch 3578, Loss: 1.8315578550100327, Final Batch Loss: 0.23785264790058136\n",
      "Epoch 3579, Loss: 1.600066365674138, Final Batch Loss: 0.018665852025151253\n",
      "Epoch 3580, Loss: 1.5454938348848373, Final Batch Loss: 0.0012010273057967424\n",
      "Epoch 3581, Loss: 1.5473831240087748, Final Batch Loss: 0.025132114067673683\n",
      "Epoch 3582, Loss: 1.5884761903434992, Final Batch Loss: 0.028032192960381508\n",
      "Epoch 3583, Loss: 1.5197619716636837, Final Batch Loss: 0.005648366641253233\n",
      "Epoch 3584, Loss: 1.504332562792115, Final Batch Loss: 0.0013659204123541713\n",
      "Epoch 3585, Loss: 2.2438284158706665, Final Batch Loss: 0.6968416571617126\n",
      "Epoch 3586, Loss: 3.3885030150413513, Final Batch Loss: 1.950540542602539\n",
      "Epoch 3587, Loss: 1.5007120473310351, Final Batch Loss: 0.01287869643419981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3588, Loss: 4.4661844074726105, Final Batch Loss: 2.884767770767212\n",
      "Epoch 3589, Loss: 3.620903193950653, Final Batch Loss: 2.1227550506591797\n",
      "Epoch 3590, Loss: 3.2493180632591248, Final Batch Loss: 1.2563581466674805\n",
      "Epoch 3591, Loss: 2.221771282143891, Final Batch Loss: 0.006192070432007313\n",
      "Epoch 3592, Loss: 2.0998667031526566, Final Batch Loss: 0.13676144182682037\n",
      "Epoch 3593, Loss: 1.9889068379998207, Final Batch Loss: 0.01867661625146866\n",
      "Epoch 3594, Loss: 1.7516593355685472, Final Batch Loss: 0.03016621060669422\n",
      "Epoch 3595, Loss: 3.5689433813095093, Final Batch Loss: 1.8517035245895386\n",
      "Epoch 3596, Loss: 1.776527851819992, Final Batch Loss: 0.06046360731124878\n",
      "Epoch 3597, Loss: 3.876695454120636, Final Batch Loss: 2.248373508453369\n",
      "Epoch 3598, Loss: 2.0560518205165863, Final Batch Loss: 0.5222533345222473\n",
      "Epoch 3599, Loss: 2.0831089317798615, Final Batch Loss: 0.23587647080421448\n",
      "Epoch 3600, Loss: 2.5823720693588257, Final Batch Loss: 0.5455668568611145\n",
      "Epoch 3601, Loss: 2.3385807275772095, Final Batch Loss: 0.26205670833587646\n",
      "Epoch 3602, Loss: 2.085793196620216, Final Batch Loss: 3.266281055402942e-05\n",
      "Epoch 3603, Loss: 2.1624645590782166, Final Batch Loss: 0.34565621614456177\n",
      "Epoch 3604, Loss: 2.4605165123939514, Final Batch Loss: 0.5841214656829834\n",
      "Epoch 3605, Loss: 1.7725373804569244, Final Batch Loss: 0.07363757491111755\n",
      "Epoch 3606, Loss: 1.780460949987173, Final Batch Loss: 0.05660348758101463\n",
      "Epoch 3607, Loss: 1.820382535457611, Final Batch Loss: 0.062372803688049316\n",
      "Epoch 3608, Loss: 1.7953432723879814, Final Batch Loss: 0.07218163460493088\n",
      "Epoch 3609, Loss: 1.6259717494249344, Final Batch Loss: 0.007560804486274719\n",
      "Epoch 3610, Loss: 1.6911522522568703, Final Batch Loss: 0.0351092591881752\n",
      "Epoch 3611, Loss: 1.7425333462888375, Final Batch Loss: 0.0017822586232796311\n",
      "Epoch 3612, Loss: 1.8284009397029877, Final Batch Loss: 0.21627232432365417\n",
      "Epoch 3613, Loss: 3.041491210460663, Final Batch Loss: 1.4212809801101685\n",
      "Epoch 3614, Loss: 1.73019190877676, Final Batch Loss: 0.031864605844020844\n",
      "Epoch 3615, Loss: 2.1688855290412903, Final Batch Loss: 0.6142392754554749\n",
      "Epoch 3616, Loss: 1.583649811334908, Final Batch Loss: 0.010108468122780323\n",
      "Epoch 3617, Loss: 1.5557136628776789, Final Batch Loss: 0.027673492208123207\n",
      "Epoch 3618, Loss: 2.284911572933197, Final Batch Loss: 0.7445541024208069\n",
      "Epoch 3619, Loss: 1.768265888094902, Final Batch Loss: 0.18135155737400055\n",
      "Epoch 3620, Loss: 1.705673635005951, Final Batch Loss: 0.18168571591377258\n",
      "Epoch 3621, Loss: 2.2075962126255035, Final Batch Loss: 0.5933783054351807\n",
      "Epoch 3622, Loss: 1.4807161558128428, Final Batch Loss: 0.0001616347290109843\n",
      "Epoch 3623, Loss: 1.6405517905950546, Final Batch Loss: 0.12521331012248993\n",
      "Epoch 3624, Loss: 2.607370972633362, Final Batch Loss: 1.0183881521224976\n",
      "Epoch 3625, Loss: 2.034593850374222, Final Batch Loss: 0.4471973180770874\n",
      "Epoch 3626, Loss: 1.7395485476590693, Final Batch Loss: 0.006729795131832361\n",
      "Epoch 3627, Loss: 1.6502980784716783, Final Batch Loss: 0.0001436368766007945\n",
      "Epoch 3628, Loss: 2.681253731250763, Final Batch Loss: 1.1353213787078857\n",
      "Epoch 3629, Loss: 2.182274341583252, Final Batch Loss: 0.5993977785110474\n",
      "Epoch 3630, Loss: 2.847515046596527, Final Batch Loss: 1.2435195446014404\n",
      "Epoch 3631, Loss: 1.6984047903679311, Final Batch Loss: 0.00402343412861228\n",
      "Epoch 3632, Loss: 1.6542017490137368, Final Batch Loss: 0.0003363520372658968\n",
      "Epoch 3633, Loss: 1.7261456307023764, Final Batch Loss: 0.019496357068419456\n",
      "Epoch 3634, Loss: 2.1310562193393707, Final Batch Loss: 0.6540161967277527\n",
      "Epoch 3635, Loss: 1.5186663774074987, Final Batch Loss: 0.0012986568035557866\n",
      "Epoch 3636, Loss: 1.6760448664426804, Final Batch Loss: 0.2073923796415329\n",
      "Epoch 3637, Loss: 1.9134696424007416, Final Batch Loss: 0.2597407400608063\n",
      "Epoch 3638, Loss: 3.37764248251915, Final Batch Loss: 1.819814920425415\n",
      "Epoch 3639, Loss: 3.1744877099990845, Final Batch Loss: 1.740539789199829\n",
      "Epoch 3640, Loss: 2.263548403978348, Final Batch Loss: 0.7526820302009583\n",
      "Epoch 3641, Loss: 1.5860977219417691, Final Batch Loss: 0.015502308495342731\n",
      "Epoch 3642, Loss: 1.6244277525693178, Final Batch Loss: 0.009532647207379341\n",
      "Epoch 3643, Loss: 1.6067295372486115, Final Batch Loss: 0.0388166606426239\n",
      "Epoch 3644, Loss: 1.6351993302814662, Final Batch Loss: 0.004763327073305845\n",
      "Epoch 3645, Loss: 1.6586695741862059, Final Batch Loss: 0.005972040817141533\n",
      "Epoch 3646, Loss: 1.5513843288645148, Final Batch Loss: 0.012394427321851254\n",
      "Epoch 3647, Loss: 1.5728113166987896, Final Batch Loss: 0.042452555149793625\n",
      "Epoch 3648, Loss: 2.2009483873844147, Final Batch Loss: 0.6637514233589172\n",
      "Epoch 3649, Loss: 1.5415387572720647, Final Batch Loss: 0.004229885526001453\n",
      "Epoch 3650, Loss: 2.1941343545913696, Final Batch Loss: 0.6745625734329224\n",
      "Epoch 3651, Loss: 1.5299152042716742, Final Batch Loss: 0.020771941170096397\n",
      "Epoch 3652, Loss: 2.2348833978176117, Final Batch Loss: 0.7292609810829163\n",
      "Epoch 3653, Loss: 1.6131827346980572, Final Batch Loss: 0.018643151968717575\n",
      "Epoch 3654, Loss: 1.7043174281716347, Final Batch Loss: 0.08872053772211075\n",
      "Epoch 3655, Loss: 1.7550966311246157, Final Batch Loss: 0.016109542921185493\n",
      "Epoch 3656, Loss: 2.7177838385105133, Final Batch Loss: 1.1747695207595825\n",
      "Epoch 3657, Loss: 1.815947636961937, Final Batch Loss: 0.21971197426319122\n",
      "Epoch 3658, Loss: 2.4339608252048492, Final Batch Loss: 0.8765757083892822\n",
      "Epoch 3659, Loss: 1.5763032287359238, Final Batch Loss: 0.03561405837535858\n",
      "Epoch 3660, Loss: 4.542048931121826, Final Batch Loss: 2.962810516357422\n",
      "Epoch 3661, Loss: 1.5341299660503864, Final Batch Loss: 0.022601742297410965\n",
      "Epoch 3662, Loss: 3.306077688932419, Final Batch Loss: 1.7032989263534546\n",
      "Epoch 3663, Loss: 1.5677584069781005, Final Batch Loss: 0.0008505540899932384\n",
      "Epoch 3664, Loss: 1.6555602997541428, Final Batch Loss: 0.11392615735530853\n",
      "Epoch 3665, Loss: 4.643839925527573, Final Batch Loss: 3.11417818069458\n",
      "Epoch 3666, Loss: 1.6896587163209915, Final Batch Loss: 0.08947493135929108\n",
      "Epoch 3667, Loss: 1.452478852123022, Final Batch Loss: 0.024126853793859482\n",
      "Epoch 3668, Loss: 1.5248787789605558, Final Batch Loss: 0.006414064671844244\n",
      "Epoch 3669, Loss: 1.7494480609893799, Final Batch Loss: 0.186720073223114\n",
      "Epoch 3670, Loss: 2.473159909248352, Final Batch Loss: 0.9700335264205933\n",
      "Epoch 3671, Loss: 2.053811937570572, Final Batch Loss: 0.4463869631290436\n",
      "Epoch 3672, Loss: 5.523904860019684, Final Batch Loss: 3.928377628326416\n",
      "Epoch 3673, Loss: 1.6230748444795609, Final Batch Loss: 0.12350057065486908\n",
      "Epoch 3674, Loss: 2.2776553630828857, Final Batch Loss: 0.6313174962997437\n",
      "Epoch 3675, Loss: 1.7861446384340525, Final Batch Loss: 0.010670864954590797\n",
      "Epoch 3676, Loss: 1.9878667530138046, Final Batch Loss: 0.0021856960374861956\n",
      "Epoch 3677, Loss: 5.270929515361786, Final Batch Loss: 3.103762626647949\n",
      "Epoch 3678, Loss: 2.156624583527446, Final Batch Loss: 0.003283349797129631\n",
      "Epoch 3679, Loss: 3.0255091786384583, Final Batch Loss: 0.6887577176094055\n",
      "Epoch 3680, Loss: 2.746785283088684, Final Batch Loss: 0.5204532742500305\n",
      "Epoch 3681, Loss: 1.9849155548436102, Final Batch Loss: 0.00048756631440483034\n",
      "Epoch 3682, Loss: 1.7874156534671783, Final Batch Loss: 0.02611592411994934\n",
      "Epoch 3683, Loss: 3.1311943531036377, Final Batch Loss: 1.4441393613815308\n",
      "Epoch 3684, Loss: 1.8639717400074005, Final Batch Loss: 0.29609397053718567\n",
      "Epoch 3685, Loss: 1.5587849295698106, Final Batch Loss: 0.0065168715082108974\n",
      "Epoch 3686, Loss: 1.5420005288906395, Final Batch Loss: 0.0024295118637382984\n",
      "Epoch 3687, Loss: 1.6878657042980194, Final Batch Loss: 0.12867161631584167\n",
      "Epoch 3688, Loss: 2.891820549964905, Final Batch Loss: 1.3519569635391235\n",
      "Epoch 3689, Loss: 1.8608230352401733, Final Batch Loss: 0.3082592785358429\n",
      "Epoch 3690, Loss: 1.7848917543888092, Final Batch Loss: 0.18621701002120972\n",
      "Epoch 3691, Loss: 2.131492167711258, Final Batch Loss: 0.5573257803916931\n",
      "Epoch 3692, Loss: 1.5673878081142902, Final Batch Loss: 0.009582120925188065\n",
      "Epoch 3693, Loss: 2.573675811290741, Final Batch Loss: 1.0281370878219604\n",
      "Epoch 3694, Loss: 1.8428495824337006, Final Batch Loss: 0.29255905747413635\n",
      "Epoch 3695, Loss: 1.6463806927204132, Final Batch Loss: 0.1499907672405243\n",
      "Epoch 3696, Loss: 2.833357274532318, Final Batch Loss: 1.398093819618225\n",
      "Epoch 3697, Loss: 1.5380149073898792, Final Batch Loss: 0.034546393901109695\n",
      "Epoch 3698, Loss: 3.8095047175884247, Final Batch Loss: 2.323697090148926\n",
      "Epoch 3699, Loss: 1.5039547206833959, Final Batch Loss: 0.011303198523819447\n",
      "Epoch 3700, Loss: 1.518141239113902, Final Batch Loss: 3.9219088648678735e-05\n",
      "Epoch 3701, Loss: 2.1104963421821594, Final Batch Loss: 0.613399088382721\n",
      "Epoch 3702, Loss: 2.545644164085388, Final Batch Loss: 0.8823410868644714\n",
      "Epoch 3703, Loss: 1.5869113844819367, Final Batch Loss: 0.0020688814111053944\n",
      "Epoch 3704, Loss: 1.5108996983617544, Final Batch Loss: 0.024554645642638206\n",
      "Epoch 3705, Loss: 2.354243129491806, Final Batch Loss: 0.8363235592842102\n",
      "Epoch 3706, Loss: 1.7066805958747864, Final Batch Loss: 0.061540067195892334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3707, Loss: 2.2460416555404663, Final Batch Loss: 0.7182989120483398\n",
      "Epoch 3708, Loss: 1.5870780125260353, Final Batch Loss: 0.1020270362496376\n",
      "Epoch 3709, Loss: 1.8855160474777222, Final Batch Loss: 0.28060585260391235\n",
      "Epoch 3710, Loss: 1.6360381096601486, Final Batch Loss: 0.06279025971889496\n",
      "Epoch 3711, Loss: 1.9113367199897766, Final Batch Loss: 0.3062169551849365\n",
      "Epoch 3712, Loss: 3.1559854447841644, Final Batch Loss: 1.6334365606307983\n",
      "Epoch 3713, Loss: 1.5302203744649887, Final Batch Loss: 0.10646341741085052\n",
      "Epoch 3714, Loss: 3.8073295056819916, Final Batch Loss: 2.3404455184936523\n",
      "Epoch 3715, Loss: 1.5783802419900894, Final Batch Loss: 0.08308543264865875\n",
      "Epoch 3716, Loss: 2.8318133652210236, Final Batch Loss: 1.2989892959594727\n",
      "Epoch 3717, Loss: 1.608878127517528, Final Batch Loss: 0.00012778419477399439\n",
      "Epoch 3718, Loss: 1.5361427381285466, Final Batch Loss: 0.0007303669699467719\n",
      "Epoch 3719, Loss: 1.5423251958563924, Final Batch Loss: 0.015460756607353687\n",
      "Epoch 3720, Loss: 1.735650897026062, Final Batch Loss: 0.2566095292568207\n",
      "Epoch 3721, Loss: 1.582450307905674, Final Batch Loss: 0.11237061768770218\n",
      "Epoch 3722, Loss: 1.4675854877568781, Final Batch Loss: 0.00500841299071908\n",
      "Epoch 3723, Loss: 1.4392560319975019, Final Batch Loss: 0.002732117660343647\n",
      "Epoch 3724, Loss: 1.5089067248627543, Final Batch Loss: 0.00791910570114851\n",
      "Epoch 3725, Loss: 1.5886950939893723, Final Batch Loss: 0.0667375773191452\n",
      "Epoch 3726, Loss: 1.4480137382633984, Final Batch Loss: 0.004599586594849825\n",
      "Epoch 3727, Loss: 1.4231554567813873, Final Batch Loss: 0.026806116104125977\n",
      "Epoch 3728, Loss: 3.116977572441101, Final Batch Loss: 1.5716737508773804\n",
      "Epoch 3729, Loss: 2.5714903473854065, Final Batch Loss: 1.1203359365463257\n",
      "Epoch 3730, Loss: 1.4864424986299127, Final Batch Loss: 0.0034701151307672262\n",
      "Epoch 3731, Loss: 3.7899667620658875, Final Batch Loss: 1.9736201763153076\n",
      "Epoch 3732, Loss: 2.262055844068527, Final Batch Loss: 0.14900639653205872\n",
      "Epoch 3733, Loss: 3.1508601903915405, Final Batch Loss: 1.0214734077453613\n",
      "Epoch 3734, Loss: 2.220207318663597, Final Batch Loss: 0.16161005198955536\n",
      "Epoch 3735, Loss: 2.2248058915138245, Final Batch Loss: 0.16856372356414795\n",
      "Epoch 3736, Loss: 2.3322407007217407, Final Batch Loss: 0.4347681403160095\n",
      "Epoch 3737, Loss: 1.8520665487158112, Final Batch Loss: 0.0002356490003876388\n",
      "Epoch 3738, Loss: 1.7435129921068437, Final Batch Loss: 0.0006846229662187397\n",
      "Epoch 3739, Loss: 1.6728936433792114, Final Batch Loss: 0.044282376766204834\n",
      "Epoch 3740, Loss: 1.7429726272821426, Final Batch Loss: 0.14307697117328644\n",
      "Epoch 3741, Loss: 1.560810893861344, Final Batch Loss: 0.0004226268210913986\n",
      "Epoch 3742, Loss: 1.7782403975725174, Final Batch Loss: 0.236918106675148\n",
      "Epoch 3743, Loss: 1.5811240011826158, Final Batch Loss: 0.0012498432770371437\n",
      "Epoch 3744, Loss: 1.509706199169159, Final Batch Loss: 0.05266055464744568\n",
      "Epoch 3745, Loss: 3.665249317884445, Final Batch Loss: 2.1729440689086914\n",
      "Epoch 3746, Loss: 1.5397136509418488, Final Batch Loss: 0.055609315633773804\n",
      "Epoch 3747, Loss: 1.6482957421103492, Final Batch Loss: 0.0014967439929023385\n",
      "Epoch 3748, Loss: 3.0884541273117065, Final Batch Loss: 1.450477123260498\n",
      "Epoch 3749, Loss: 2.046540766954422, Final Batch Loss: 0.44469648599624634\n",
      "Epoch 3750, Loss: 1.7307234853506088, Final Batch Loss: 0.14805693924427032\n",
      "Epoch 3751, Loss: 2.8345298767089844, Final Batch Loss: 0.943469762802124\n",
      "Epoch 3752, Loss: 1.880948362770141, Final Batch Loss: 6.270212179515511e-05\n",
      "Epoch 3753, Loss: 1.84619448275771, Final Batch Loss: 0.001262939884327352\n",
      "Epoch 3754, Loss: 1.5699834822908088, Final Batch Loss: 1.1801649634435307e-05\n",
      "Epoch 3755, Loss: 1.5709216073155403, Final Batch Loss: 0.0355781689286232\n",
      "Epoch 3756, Loss: 1.7527105212211609, Final Batch Loss: 0.14448779821395874\n",
      "Epoch 3757, Loss: 1.6245503202080727, Final Batch Loss: 0.022174395620822906\n",
      "Epoch 3758, Loss: 1.7052191197872162, Final Batch Loss: 0.18542584776878357\n",
      "Epoch 3759, Loss: 1.5843678880482912, Final Batch Loss: 0.0033936072140932083\n",
      "Epoch 3760, Loss: 1.4788849165779538, Final Batch Loss: 0.00039593485416844487\n",
      "Epoch 3761, Loss: 1.5970059782266617, Final Batch Loss: 0.12671278417110443\n",
      "Epoch 3762, Loss: 1.5854820758104324, Final Batch Loss: 0.0649527758359909\n",
      "Epoch 3763, Loss: 2.1191272735595703, Final Batch Loss: 0.7188916206359863\n",
      "Epoch 3764, Loss: 1.4964105896651745, Final Batch Loss: 0.043622177094221115\n",
      "Epoch 3765, Loss: 5.3306973576545715, Final Batch Loss: 3.8746511936187744\n",
      "Epoch 3766, Loss: 1.566773697733879, Final Batch Loss: 0.13823331892490387\n",
      "Epoch 3767, Loss: 1.5315716564655304, Final Batch Loss: 0.1134127676486969\n",
      "Epoch 3768, Loss: 1.734080895781517, Final Batch Loss: 0.07623021304607391\n",
      "Epoch 3769, Loss: 1.656170830130577, Final Batch Loss: 0.14612071216106415\n",
      "Epoch 3770, Loss: 1.5484366789460182, Final Batch Loss: 0.009242959320545197\n",
      "Epoch 3771, Loss: 1.5530386326136068, Final Batch Loss: 0.0011399445356801152\n",
      "Epoch 3772, Loss: 1.6326976141426712, Final Batch Loss: 0.0039031526539474726\n",
      "Epoch 3773, Loss: 3.1670866906642914, Final Batch Loss: 1.6813112497329712\n",
      "Epoch 3774, Loss: 1.4809273518621922, Final Batch Loss: 0.04751149192452431\n",
      "Epoch 3775, Loss: 1.6120377629995346, Final Batch Loss: 0.18431715667247772\n",
      "Epoch 3776, Loss: 3.4266419410705566, Final Batch Loss: 1.7898736000061035\n",
      "Epoch 3777, Loss: 2.4103309512138367, Final Batch Loss: 1.0023728609085083\n",
      "Epoch 3778, Loss: 2.631492853164673, Final Batch Loss: 1.2052116394042969\n",
      "Epoch 3779, Loss: 1.5401987507939339, Final Batch Loss: 0.016359485685825348\n",
      "Epoch 3780, Loss: 1.5771360436920077, Final Batch Loss: 0.003353331470862031\n",
      "Epoch 3781, Loss: 1.6351438537240028, Final Batch Loss: 0.013728953897953033\n",
      "Epoch 3782, Loss: 2.6545200645923615, Final Batch Loss: 1.0483678579330444\n",
      "Epoch 3783, Loss: 1.6316646050254349, Final Batch Loss: 0.00048601735034026206\n",
      "Epoch 3784, Loss: 1.6162958536297083, Final Batch Loss: 0.03045741654932499\n",
      "Epoch 3785, Loss: 1.9500214271247387, Final Batch Loss: 0.05957738682627678\n",
      "Epoch 3786, Loss: 1.893834188580513, Final Batch Loss: 0.13764168322086334\n",
      "Epoch 3787, Loss: 1.5976468017906882, Final Batch Loss: 0.0008466235012747347\n",
      "Epoch 3788, Loss: 2.3827155232429504, Final Batch Loss: 0.7860808968544006\n",
      "Epoch 3789, Loss: 1.5913468897342682, Final Batch Loss: 0.02539166808128357\n",
      "Epoch 3790, Loss: 1.8272214829921722, Final Batch Loss: 0.06905815005302429\n",
      "Epoch 3791, Loss: 1.694263182580471, Final Batch Loss: 0.0783548578619957\n",
      "Epoch 3792, Loss: 2.369177222251892, Final Batch Loss: 0.6528998017311096\n",
      "Epoch 3793, Loss: 1.7387558883056045, Final Batch Loss: 0.008345610462129116\n",
      "Epoch 3794, Loss: 1.4652422405779362, Final Batch Loss: 0.035410765558481216\n",
      "Epoch 3795, Loss: 3.3110386729240417, Final Batch Loss: 1.8166801929473877\n",
      "Epoch 3796, Loss: 1.8721323013305664, Final Batch Loss: 0.5102448463439941\n",
      "Epoch 3797, Loss: 1.478588154539466, Final Batch Loss: 0.028414985164999962\n",
      "Epoch 3798, Loss: 1.7967481315135956, Final Batch Loss: 0.39505356550216675\n",
      "Epoch 3799, Loss: 1.4679252227942925, Final Batch Loss: 0.00023552982020191848\n",
      "Epoch 3800, Loss: 1.7157417833805084, Final Batch Loss: 0.22432178258895874\n",
      "Epoch 3801, Loss: 1.4927368946373463, Final Batch Loss: 0.023196179419755936\n",
      "Epoch 3802, Loss: 1.967118263244629, Final Batch Loss: 0.5460959672927856\n",
      "Epoch 3803, Loss: 1.556234285235405, Final Batch Loss: 0.14969490468502045\n",
      "Epoch 3804, Loss: 4.264587074518204, Final Batch Loss: 2.856538772583008\n",
      "Epoch 3805, Loss: 1.479286859743297, Final Batch Loss: 0.004194985143840313\n",
      "Epoch 3806, Loss: 1.5139610767364502, Final Batch Loss: 0.09282952547073364\n",
      "Epoch 3807, Loss: 1.5383249139413238, Final Batch Loss: 0.006889990530908108\n",
      "Epoch 3808, Loss: 1.4938558279536664, Final Batch Loss: 0.00026782741770148277\n",
      "Epoch 3809, Loss: 1.4435217373429623, Final Batch Loss: 3.766942609217949e-05\n",
      "Epoch 3810, Loss: 1.4530446770368144, Final Batch Loss: 0.0015854182420298457\n",
      "Epoch 3811, Loss: 1.7275250852108002, Final Batch Loss: 0.26619625091552734\n",
      "Epoch 3812, Loss: 1.5079603716731071, Final Batch Loss: 0.038594745099544525\n",
      "Epoch 3813, Loss: 1.6987976506352425, Final Batch Loss: 0.09596986323595047\n",
      "Epoch 3814, Loss: 1.7406322360038757, Final Batch Loss: 0.28966283798217773\n",
      "Epoch 3815, Loss: 2.6008701622486115, Final Batch Loss: 1.2090907096862793\n",
      "Epoch 3816, Loss: 1.4975824877619743, Final Batch Loss: 0.11295577138662338\n",
      "Epoch 3817, Loss: 1.6620833234628662, Final Batch Loss: 0.0004969792207702994\n",
      "Epoch 3818, Loss: 5.358969867229462, Final Batch Loss: 3.4295215606689453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3819, Loss: 2.451206147670746, Final Batch Loss: 0.7016691565513611\n",
      "Epoch 3820, Loss: 1.9269227534532547, Final Batch Loss: 0.142105832695961\n",
      "Epoch 3821, Loss: 1.8251320272684097, Final Batch Loss: 0.11137504875659943\n",
      "Epoch 3822, Loss: 2.3465096950531006, Final Batch Loss: 0.7014057636260986\n",
      "Epoch 3823, Loss: 1.666495956480503, Final Batch Loss: 0.01078573614358902\n",
      "Epoch 3824, Loss: 1.6112353913486004, Final Batch Loss: 0.034077923744916916\n",
      "Epoch 3825, Loss: 1.5191899687051773, Final Batch Loss: 0.017665311694145203\n",
      "Epoch 3826, Loss: 1.6351404413580894, Final Batch Loss: 0.09716694802045822\n",
      "Epoch 3827, Loss: 1.7185220271348953, Final Batch Loss: 0.1703592985868454\n",
      "Epoch 3828, Loss: 3.0027609169483185, Final Batch Loss: 1.5604350566864014\n",
      "Epoch 3829, Loss: 1.8156527876853943, Final Batch Loss: 0.30496588349342346\n",
      "Epoch 3830, Loss: 1.4997104960493743, Final Batch Loss: 0.002805109601467848\n",
      "Epoch 3831, Loss: 1.8049027919769287, Final Batch Loss: 0.27302828431129456\n",
      "Epoch 3832, Loss: 1.5374588668346405, Final Batch Loss: 0.04399847984313965\n",
      "Epoch 3833, Loss: 2.5640814006328583, Final Batch Loss: 1.0685385465621948\n",
      "Epoch 3834, Loss: 1.4921416528522968, Final Batch Loss: 0.042289841920137405\n",
      "Epoch 3835, Loss: 2.317060947418213, Final Batch Loss: 0.825539767742157\n",
      "Epoch 3836, Loss: 1.3740208148956299, Final Batch Loss: 0.005481332540512085\n",
      "Epoch 3837, Loss: 1.5963511541485786, Final Batch Loss: 0.07367212325334549\n",
      "Epoch 3838, Loss: 2.0394720435142517, Final Batch Loss: 0.5368696451187134\n",
      "Epoch 3839, Loss: 2.300658017396927, Final Batch Loss: 0.8782293200492859\n",
      "Epoch 3840, Loss: 1.49956411391031, Final Batch Loss: 0.001061471994034946\n",
      "Epoch 3841, Loss: 1.5241133719682693, Final Batch Loss: 0.07288499176502228\n",
      "Epoch 3842, Loss: 1.6452184077352285, Final Batch Loss: 0.021119004115462303\n",
      "Epoch 3843, Loss: 1.526505846530199, Final Batch Loss: 0.021736253052949905\n",
      "Epoch 3844, Loss: 1.4122992679476738, Final Batch Loss: 0.07110991328954697\n",
      "Epoch 3845, Loss: 1.6079599931836128, Final Batch Loss: 0.1183629110455513\n",
      "Epoch 3846, Loss: 3.3986512422561646, Final Batch Loss: 1.8230140209197998\n",
      "Epoch 3847, Loss: 1.4883097754936898, Final Batch Loss: 0.00022420754248742014\n",
      "Epoch 3848, Loss: 3.297452747821808, Final Batch Loss: 1.8150962591171265\n",
      "Epoch 3849, Loss: 1.5167857389897108, Final Batch Loss: 0.025338320061564445\n",
      "Epoch 3850, Loss: 4.051624536514282, Final Batch Loss: 2.6150667667388916\n",
      "Epoch 3851, Loss: 1.3709779729542788, Final Batch Loss: 0.00047791501856409013\n",
      "Epoch 3852, Loss: 4.521115034818649, Final Batch Loss: 2.8872721195220947\n",
      "Epoch 3853, Loss: 2.0309690833091736, Final Batch Loss: 0.36488592624664307\n",
      "Epoch 3854, Loss: 1.668199760839343, Final Batch Loss: 0.022134287282824516\n",
      "Epoch 3855, Loss: 1.6740385723533109, Final Batch Loss: 0.00018249277491122484\n",
      "Epoch 3856, Loss: 1.672761054825969, Final Batch Loss: 0.0008071978809311986\n",
      "Epoch 3857, Loss: 2.808418393135071, Final Batch Loss: 1.0368231534957886\n",
      "Epoch 3858, Loss: 2.539773643016815, Final Batch Loss: 0.9868588447570801\n",
      "Epoch 3859, Loss: 2.3279140293598175, Final Batch Loss: 0.7012541890144348\n",
      "Epoch 3860, Loss: 3.001869112253189, Final Batch Loss: 1.5107951164245605\n",
      "Epoch 3861, Loss: 3.2171113193035126, Final Batch Loss: 1.6935313940048218\n",
      "Epoch 3862, Loss: 1.6582654984667897, Final Batch Loss: 0.007327822037041187\n",
      "Epoch 3863, Loss: 1.7303716391324997, Final Batch Loss: 0.03268168866634369\n",
      "Epoch 3864, Loss: 1.7400648273760453, Final Batch Loss: 0.0017780937487259507\n",
      "Epoch 3865, Loss: 1.6814761087298393, Final Batch Loss: 0.09495305269956589\n",
      "Epoch 3866, Loss: 1.6808488499373198, Final Batch Loss: 0.01787973754107952\n",
      "Epoch 3867, Loss: 3.427137166261673, Final Batch Loss: 1.9403846263885498\n",
      "Epoch 3868, Loss: 1.4568866172630806, Final Batch Loss: 6.425174069590867e-05\n",
      "Epoch 3869, Loss: 1.502855822443962, Final Batch Loss: 0.010449931025505066\n",
      "Epoch 3870, Loss: 1.998874992132187, Final Batch Loss: 0.5003814697265625\n",
      "Epoch 3871, Loss: 2.5885097086429596, Final Batch Loss: 1.0936017036437988\n",
      "Epoch 3872, Loss: 1.5913131386041641, Final Batch Loss: 0.16744311153888702\n",
      "Epoch 3873, Loss: 1.456305667757988, Final Batch Loss: 0.06098605692386627\n",
      "Epoch 3874, Loss: 1.6201203167438507, Final Batch Loss: 0.18817844986915588\n",
      "Epoch 3875, Loss: 1.4627946428954601, Final Batch Loss: 0.03891183063387871\n",
      "Epoch 3876, Loss: 1.462721364106983, Final Batch Loss: 0.005871193017810583\n",
      "Epoch 3877, Loss: 1.4655333457267261, Final Batch Loss: 2.0503786799963564e-05\n",
      "Epoch 3878, Loss: 1.3602208788797725, Final Batch Loss: 0.00024589852546341717\n",
      "Epoch 3879, Loss: 2.3545584082603455, Final Batch Loss: 0.9952712655067444\n",
      "Epoch 3880, Loss: 1.5448091691359878, Final Batch Loss: 0.0031323200091719627\n",
      "Epoch 3881, Loss: 2.0655177235603333, Final Batch Loss: 0.6875044107437134\n",
      "Epoch 3882, Loss: 3.3767379224300385, Final Batch Loss: 1.9842069149017334\n",
      "Epoch 3883, Loss: 1.4588898122310638, Final Batch Loss: 0.020048826932907104\n",
      "Epoch 3884, Loss: 1.6199119726661593, Final Batch Loss: 0.001919690752401948\n",
      "Epoch 3885, Loss: 1.6581673212349415, Final Batch Loss: 0.046788353472948074\n",
      "Epoch 3886, Loss: 2.0293722450733185, Final Batch Loss: 0.48142901062965393\n",
      "Epoch 3887, Loss: 1.6231294916942716, Final Batch Loss: 0.014930878765881062\n",
      "Epoch 3888, Loss: 1.5521540569607168, Final Batch Loss: 0.0023054706398397684\n",
      "Epoch 3889, Loss: 2.9391985535621643, Final Batch Loss: 1.4199435710906982\n",
      "Epoch 3890, Loss: 1.6271584704518318, Final Batch Loss: 0.05329311639070511\n",
      "Epoch 3891, Loss: 1.4871976593858562, Final Batch Loss: 0.0001262346631847322\n",
      "Epoch 3892, Loss: 1.6300718560814857, Final Batch Loss: 0.11234483867883682\n",
      "Epoch 3893, Loss: 2.8437965512275696, Final Batch Loss: 1.4249134063720703\n",
      "Epoch 3894, Loss: 1.545185477938503, Final Batch Loss: 0.0066784038208425045\n",
      "Epoch 3895, Loss: 1.458698667556746, Final Batch Loss: 0.0002115741081070155\n",
      "Epoch 3896, Loss: 1.4216996608301997, Final Batch Loss: 0.006417381577193737\n",
      "Epoch 3897, Loss: 1.5225310623645782, Final Batch Loss: 0.04089954495429993\n",
      "Epoch 3898, Loss: 2.2737734615802765, Final Batch Loss: 0.8002877831459045\n",
      "Epoch 3899, Loss: 1.5452709570527077, Final Batch Loss: 0.11944115906953812\n",
      "Epoch 3900, Loss: 1.4286360181868076, Final Batch Loss: 0.008618894964456558\n",
      "Epoch 3901, Loss: 1.5114316960098222, Final Batch Loss: 0.0010621865512803197\n",
      "Epoch 3902, Loss: 1.3959312313236296, Final Batch Loss: 0.004676120821386576\n",
      "Epoch 3903, Loss: 2.041843205690384, Final Batch Loss: 0.6329068541526794\n",
      "Epoch 3904, Loss: 1.4062815603574563, Final Batch Loss: 2.455681169521995e-05\n",
      "Epoch 3905, Loss: 1.5536823198199272, Final Batch Loss: 0.07832487672567368\n",
      "Epoch 3906, Loss: 1.5072981100529432, Final Batch Loss: 0.028302816674113274\n",
      "Epoch 3907, Loss: 1.606082171201706, Final Batch Loss: 0.10733732581138611\n",
      "Epoch 3908, Loss: 5.664657533168793, Final Batch Loss: 4.225981712341309\n",
      "Epoch 3909, Loss: 2.420860767364502, Final Batch Loss: 1.0800504684448242\n",
      "Epoch 3910, Loss: 1.682589828968048, Final Batch Loss: 0.08149838447570801\n",
      "Epoch 3911, Loss: 1.5638012024573982, Final Batch Loss: 0.004710766952484846\n",
      "Epoch 3912, Loss: 1.4230117452098057, Final Batch Loss: 0.0012484145117923617\n",
      "Epoch 3913, Loss: 1.7014586627483368, Final Batch Loss: 0.06724360585212708\n",
      "Epoch 3914, Loss: 1.6274063810706139, Final Batch Loss: 0.03793846815824509\n",
      "Epoch 3915, Loss: 1.554062494309619, Final Batch Loss: 0.0024852838832885027\n",
      "Epoch 3916, Loss: 1.4561966171022505, Final Batch Loss: 0.0035074164625257254\n",
      "Epoch 3917, Loss: 1.475069960579276, Final Batch Loss: 0.01096980832517147\n",
      "Epoch 3918, Loss: 1.430855297949165, Final Batch Loss: 0.0020737587474286556\n",
      "Epoch 3919, Loss: 2.548973798751831, Final Batch Loss: 1.1363191604614258\n",
      "Epoch 3920, Loss: 1.5269306935369968, Final Batch Loss: 0.03599131479859352\n",
      "Epoch 3921, Loss: 1.359212172916159, Final Batch Loss: 0.0010798105504363775\n",
      "Epoch 3922, Loss: 1.429453533142805, Final Batch Loss: 0.018388044089078903\n",
      "Epoch 3923, Loss: 1.4240265302360058, Final Batch Loss: 0.04601733013987541\n",
      "Epoch 3924, Loss: 1.4194550439715385, Final Batch Loss: 0.02071356028318405\n",
      "Epoch 3925, Loss: 1.4196121785789728, Final Batch Loss: 0.005826631560921669\n",
      "Epoch 3926, Loss: 1.4594265818595886, Final Batch Loss: 0.044479429721832275\n",
      "Epoch 3927, Loss: 1.354798415210098, Final Batch Loss: 0.004937598016113043\n",
      "Epoch 3928, Loss: 1.4106439277529716, Final Batch Loss: 0.0769534632563591\n",
      "Epoch 3929, Loss: 1.5442314594984055, Final Batch Loss: 0.1513018161058426\n",
      "Epoch 3930, Loss: 2.9545504450798035, Final Batch Loss: 1.5458793640136719\n",
      "Epoch 3931, Loss: 1.4022172931581736, Final Batch Loss: 0.018355155363678932\n",
      "Epoch 3932, Loss: 3.776541292667389, Final Batch Loss: 2.375678062438965\n",
      "Epoch 3933, Loss: 1.3681095764040947, Final Batch Loss: 0.002016298472881317\n",
      "Epoch 3934, Loss: 2.024670898914337, Final Batch Loss: 0.4182029962539673\n",
      "Epoch 3935, Loss: 2.104533314704895, Final Batch Loss: 0.14449936151504517\n",
      "Epoch 3936, Loss: 1.8908897773362696, Final Batch Loss: 0.005090846214443445\n",
      "Epoch 3937, Loss: 1.7221415787935257, Final Batch Loss: 0.032142505049705505\n",
      "Epoch 3938, Loss: 1.5768350884318352, Final Batch Loss: 0.03435751050710678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3939, Loss: 3.077225536108017, Final Batch Loss: 1.6118876934051514\n",
      "Epoch 3940, Loss: 3.1218065321445465, Final Batch Loss: 1.6389029026031494\n",
      "Epoch 3941, Loss: 3.444815158843994, Final Batch Loss: 1.9300570487976074\n",
      "Epoch 3942, Loss: 1.6382537707686424, Final Batch Loss: 0.07624336332082748\n",
      "Epoch 3943, Loss: 2.6373429894447327, Final Batch Loss: 1.0223395824432373\n",
      "Epoch 3944, Loss: 1.520713844394777, Final Batch Loss: 0.00047064671525731683\n",
      "Epoch 3945, Loss: 2.986246883869171, Final Batch Loss: 1.4636801481246948\n",
      "Epoch 3946, Loss: 1.672804519534111, Final Batch Loss: 0.12504900991916656\n",
      "Epoch 3947, Loss: 3.329056054353714, Final Batch Loss: 1.7825138568878174\n",
      "Epoch 3948, Loss: 2.020858407020569, Final Batch Loss: 0.585625171661377\n",
      "Epoch 3949, Loss: 1.4579298051539809, Final Batch Loss: 0.003394082421436906\n",
      "Epoch 3950, Loss: 2.115828573703766, Final Batch Loss: 0.4098591208457947\n",
      "Epoch 3951, Loss: 1.6877953144721687, Final Batch Loss: 0.0039932760410010815\n",
      "Epoch 3952, Loss: 1.5462927709158976, Final Batch Loss: 0.00037555795279331505\n",
      "Epoch 3953, Loss: 2.0154465436935425, Final Batch Loss: 0.3931407332420349\n",
      "Epoch 3954, Loss: 1.7978307902812958, Final Batch Loss: 0.2641519606113434\n",
      "Epoch 3955, Loss: 1.5260110730305314, Final Batch Loss: 0.009352915920317173\n",
      "Epoch 3956, Loss: 1.6297063539968804, Final Batch Loss: 0.001562208984978497\n",
      "Epoch 3957, Loss: 1.5188403595238924, Final Batch Loss: 0.0038456786423921585\n",
      "Epoch 3958, Loss: 2.8698728680610657, Final Batch Loss: 1.3646997213363647\n",
      "Epoch 3959, Loss: 1.5535970404744148, Final Batch Loss: 0.0802290216088295\n",
      "Epoch 3960, Loss: 3.6079022586345673, Final Batch Loss: 2.0722267627716064\n",
      "Epoch 3961, Loss: 1.743101328611374, Final Batch Loss: 0.2448626160621643\n",
      "Epoch 3962, Loss: 2.1690003871917725, Final Batch Loss: 0.5126288533210754\n",
      "Epoch 3963, Loss: 2.8519136905670166, Final Batch Loss: 1.1803032159805298\n",
      "Epoch 3964, Loss: 4.1391960978508, Final Batch Loss: 2.414187431335449\n",
      "Epoch 3965, Loss: 1.7609051391482353, Final Batch Loss: 0.04796575754880905\n",
      "Epoch 3966, Loss: 1.7513842452317476, Final Batch Loss: 0.00948907621204853\n",
      "Epoch 3967, Loss: 2.8713008165359497, Final Batch Loss: 1.1445611715316772\n",
      "Epoch 3968, Loss: 3.462510824203491, Final Batch Loss: 1.607682704925537\n",
      "Epoch 3969, Loss: 7.942207038402557, Final Batch Loss: 5.730172634124756\n",
      "Epoch 3970, Loss: 2.4387190490961075, Final Batch Loss: 0.06817550957202911\n",
      "Epoch 3971, Loss: 2.496296137571335, Final Batch Loss: 0.009039312601089478\n",
      "Epoch 3972, Loss: 3.7231571078300476, Final Batch Loss: 1.1346062421798706\n",
      "Epoch 3973, Loss: 3.8755374550819397, Final Batch Loss: 1.6211152076721191\n",
      "Epoch 3974, Loss: 2.366081118583679, Final Batch Loss: 0.12408620119094849\n",
      "Epoch 3975, Loss: 2.198669459670782, Final Batch Loss: 0.06009289249777794\n",
      "Epoch 3976, Loss: 3.747104287147522, Final Batch Loss: 1.7228782176971436\n",
      "Epoch 3977, Loss: 3.8148475885391235, Final Batch Loss: 2.08198618888855\n",
      "Epoch 3978, Loss: 1.8163131922483444, Final Batch Loss: 0.14540807902812958\n",
      "Epoch 3979, Loss: 1.8064361289143562, Final Batch Loss: 0.0636756494641304\n",
      "Epoch 3980, Loss: 1.6658820109441876, Final Batch Loss: 0.011605252511799335\n",
      "Epoch 3981, Loss: 1.9769758880138397, Final Batch Loss: 0.3115849792957306\n",
      "Epoch 3982, Loss: 1.756558682769537, Final Batch Loss: 0.04766392335295677\n",
      "Epoch 3983, Loss: 1.664765901863575, Final Batch Loss: 0.03267384320497513\n",
      "Epoch 3984, Loss: 4.070424735546112, Final Batch Loss: 2.5126802921295166\n",
      "Epoch 3985, Loss: 1.5927876195055433, Final Batch Loss: 0.00035637227119877934\n",
      "Epoch 3986, Loss: 2.420216977596283, Final Batch Loss: 0.6685975193977356\n",
      "Epoch 3987, Loss: 2.3645440340042114, Final Batch Loss: 0.5574068427085876\n",
      "Epoch 3988, Loss: 1.9090027380734682, Final Batch Loss: 0.014217512682080269\n",
      "Epoch 3989, Loss: 1.9594581574201584, Final Batch Loss: 0.13294453918933868\n",
      "Epoch 3990, Loss: 1.893832616508007, Final Batch Loss: 0.04758822172880173\n",
      "Epoch 3991, Loss: 2.331052005290985, Final Batch Loss: 0.6295380592346191\n",
      "Epoch 3992, Loss: 2.8523311018943787, Final Batch Loss: 1.0762425661087036\n",
      "Epoch 3993, Loss: 1.5376728624105453, Final Batch Loss: 0.022781088948249817\n",
      "Epoch 3994, Loss: 2.1591953337192535, Final Batch Loss: 0.6123923659324646\n",
      "Epoch 3995, Loss: 1.521764905191958, Final Batch Loss: 0.00858816597610712\n",
      "Epoch 3996, Loss: 2.4248388409614563, Final Batch Loss: 0.9279210567474365\n",
      "Epoch 3997, Loss: 1.5249305330216885, Final Batch Loss: 0.05177214369177818\n",
      "Epoch 3998, Loss: 1.5209298767149448, Final Batch Loss: 0.04520321264863014\n",
      "Epoch 3999, Loss: 1.4507825402542949, Final Batch Loss: 0.008480134420096874\n",
      "Epoch 4000, Loss: 1.548954963684082, Final Batch Loss: 0.040051013231277466\n",
      "Epoch 4001, Loss: 2.7177931368350983, Final Batch Loss: 1.26285719871521\n",
      "Epoch 4002, Loss: 1.5083623379468918, Final Batch Loss: 0.08931587636470795\n",
      "Epoch 4003, Loss: 1.5551893413066864, Final Batch Loss: 0.14982309937477112\n",
      "Epoch 4004, Loss: 1.7547289431095123, Final Batch Loss: 0.2568569481372833\n",
      "Epoch 4005, Loss: 1.477285211905837, Final Batch Loss: 0.007963573560118675\n",
      "Epoch 4006, Loss: 1.4912538882344961, Final Batch Loss: 0.009651308879256248\n",
      "Epoch 4007, Loss: 1.4715158380568027, Final Batch Loss: 0.01941147819161415\n",
      "Epoch 4008, Loss: 1.5696487873792648, Final Batch Loss: 0.10054989159107208\n",
      "Epoch 4009, Loss: 1.548093169927597, Final Batch Loss: 0.08910685777664185\n",
      "Epoch 4010, Loss: 4.049209475517273, Final Batch Loss: 2.5967600345611572\n",
      "Epoch 4011, Loss: 1.797557532787323, Final Batch Loss: 0.30807164311408997\n",
      "Epoch 4012, Loss: 1.38085751933977, Final Batch Loss: 0.005980573128908873\n",
      "Epoch 4013, Loss: 2.7143835723400116, Final Batch Loss: 1.3121973276138306\n",
      "Epoch 4014, Loss: 1.4074299691710621, Final Batch Loss: 0.002286797622218728\n",
      "Epoch 4015, Loss: 1.6648506820201874, Final Batch Loss: 0.1581318974494934\n",
      "Epoch 4016, Loss: 3.1271484196186066, Final Batch Loss: 1.6220558881759644\n",
      "Epoch 4017, Loss: 1.48854043148458, Final Batch Loss: 0.011292001232504845\n",
      "Epoch 4018, Loss: 3.002161532640457, Final Batch Loss: 1.5190362930297852\n",
      "Epoch 4019, Loss: 1.890913188457489, Final Batch Loss: 0.4975736737251282\n",
      "Epoch 4020, Loss: 1.455806597834453, Final Batch Loss: 0.0012328175362199545\n",
      "Epoch 4021, Loss: 1.471022518351674, Final Batch Loss: 0.01789531297981739\n",
      "Epoch 4022, Loss: 2.5239089727401733, Final Batch Loss: 1.0806431770324707\n",
      "Epoch 4023, Loss: 1.4927533771842718, Final Batch Loss: 0.020517608150839806\n",
      "Epoch 4024, Loss: 1.420817750506103, Final Batch Loss: 0.008006620220839977\n",
      "Epoch 4025, Loss: 1.6105148792266846, Final Batch Loss: 0.25267136096954346\n",
      "Epoch 4026, Loss: 1.6004515886306763, Final Batch Loss: 0.13182789087295532\n",
      "Epoch 4027, Loss: 1.3769333455711603, Final Batch Loss: 0.0072364602237939835\n",
      "Epoch 4028, Loss: 2.7449535131454468, Final Batch Loss: 1.3630220890045166\n",
      "Epoch 4029, Loss: 3.350350648164749, Final Batch Loss: 1.8864338397979736\n",
      "Epoch 4030, Loss: 2.213010549545288, Final Batch Loss: 0.7640294432640076\n",
      "Epoch 4031, Loss: 2.5751712322235107, Final Batch Loss: 1.1725949048995972\n",
      "Epoch 4032, Loss: 1.5539708182332106, Final Batch Loss: 0.0005376085755415261\n",
      "Epoch 4033, Loss: 1.5449440851807594, Final Batch Loss: 0.05409807711839676\n",
      "Epoch 4034, Loss: 1.541544759646058, Final Batch Loss: 0.022507453337311745\n",
      "Epoch 4035, Loss: 1.5251944437623024, Final Batch Loss: 0.03114727884531021\n",
      "Epoch 4036, Loss: 1.5180145632475615, Final Batch Loss: 0.02557585947215557\n",
      "Epoch 4037, Loss: 1.4790660049766302, Final Batch Loss: 0.021563252434134483\n",
      "Epoch 4038, Loss: 1.40870126709342, Final Batch Loss: 0.004873540252447128\n",
      "Epoch 4039, Loss: 1.3718375735916197, Final Batch Loss: 0.005107569042593241\n",
      "Epoch 4040, Loss: 3.480942815542221, Final Batch Loss: 2.055044174194336\n",
      "Epoch 4041, Loss: 1.524239644408226, Final Batch Loss: 0.021227195858955383\n",
      "Epoch 4042, Loss: 2.042409211397171, Final Batch Loss: 0.5825739502906799\n",
      "Epoch 4043, Loss: 2.0646269619464874, Final Batch Loss: 0.5362287163734436\n",
      "Epoch 4044, Loss: 2.8116395473480225, Final Batch Loss: 1.3859474658966064\n",
      "Epoch 4045, Loss: 1.551813387311995, Final Batch Loss: 0.00970467273145914\n",
      "Epoch 4046, Loss: 2.942991763353348, Final Batch Loss: 1.5357658863067627\n",
      "Epoch 4047, Loss: 1.5173646518960595, Final Batch Loss: 0.011452645994722843\n",
      "Epoch 4048, Loss: 2.014187812805176, Final Batch Loss: 0.5069146156311035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4049, Loss: 1.4464938921155408, Final Batch Loss: 5.9126061387360096e-05\n",
      "Epoch 4050, Loss: 1.494982056086883, Final Batch Loss: 0.0016600412782281637\n",
      "Epoch 4051, Loss: 1.552949259057641, Final Batch Loss: 0.016783932223916054\n",
      "Epoch 4052, Loss: 1.491168433218263, Final Batch Loss: 0.0016227898886427283\n",
      "Epoch 4053, Loss: 1.5776715576648712, Final Batch Loss: 0.10079491138458252\n",
      "Epoch 4054, Loss: 1.6407749727368355, Final Batch Loss: 0.12146136909723282\n",
      "Epoch 4055, Loss: 1.5286088660359383, Final Batch Loss: 0.08873177319765091\n",
      "Epoch 4056, Loss: 1.4842076562345028, Final Batch Loss: 0.046275731176137924\n",
      "Epoch 4057, Loss: 1.3154811947606504, Final Batch Loss: 0.003886172082275152\n",
      "Epoch 4058, Loss: 1.4030675264075398, Final Batch Loss: 0.00428223330527544\n",
      "Epoch 4059, Loss: 2.3601332008838654, Final Batch Loss: 0.9509032964706421\n",
      "Epoch 4060, Loss: 1.3542810697108507, Final Batch Loss: 0.02943018265068531\n",
      "Epoch 4061, Loss: 1.5183126330375671, Final Batch Loss: 0.03665047883987427\n",
      "Epoch 4062, Loss: 2.2318551540374756, Final Batch Loss: 0.7900119423866272\n",
      "Epoch 4063, Loss: 1.360765543722664, Final Batch Loss: 7.533743337262422e-05\n",
      "Epoch 4064, Loss: 1.4150129782792646, Final Batch Loss: 0.00011991735664196312\n",
      "Epoch 4065, Loss: 2.1480026841163635, Final Batch Loss: 0.7179964184761047\n",
      "Epoch 4066, Loss: 1.4225675344496267, Final Batch Loss: 0.00015436411194968969\n",
      "Epoch 4067, Loss: 1.419480748474598, Final Batch Loss: 0.03678733855485916\n",
      "Epoch 4068, Loss: 2.8313968181610107, Final Batch Loss: 1.4601142406463623\n",
      "Epoch 4069, Loss: 1.5675277560949326, Final Batch Loss: 0.045474812388420105\n",
      "Epoch 4070, Loss: 1.7888134121894836, Final Batch Loss: 0.36645177006721497\n",
      "Epoch 4071, Loss: 2.065734565258026, Final Batch Loss: 0.6825171113014221\n",
      "Epoch 4072, Loss: 2.4234438836574554, Final Batch Loss: 1.0897679328918457\n",
      "Epoch 4073, Loss: 1.6558913737535477, Final Batch Loss: 0.21351231634616852\n",
      "Epoch 4074, Loss: 1.355750059010461, Final Batch Loss: 0.002054129960015416\n",
      "Epoch 4075, Loss: 2.536718785762787, Final Batch Loss: 1.1057839393615723\n",
      "Epoch 4076, Loss: 1.43220055103302, Final Batch Loss: 0.0358145534992218\n",
      "Epoch 4077, Loss: 2.688528209924698, Final Batch Loss: 1.303916096687317\n",
      "Epoch 4078, Loss: 1.9561882019042969, Final Batch Loss: 0.6114333868026733\n",
      "Epoch 4079, Loss: 2.330598086118698, Final Batch Loss: 0.9299665689468384\n",
      "Epoch 4080, Loss: 1.4908143691718578, Final Batch Loss: 0.004258137196302414\n",
      "Epoch 4081, Loss: 1.4886231264099479, Final Batch Loss: 0.012547596357762814\n",
      "Epoch 4082, Loss: 1.4935080706145527, Final Batch Loss: 1.3589766240329482e-05\n",
      "Epoch 4083, Loss: 1.887089490890503, Final Batch Loss: 0.3908045291900635\n",
      "Epoch 4084, Loss: 1.6426032334566116, Final Batch Loss: 0.08498470485210419\n",
      "Epoch 4085, Loss: 2.4865260124206543, Final Batch Loss: 1.0077924728393555\n",
      "Epoch 4086, Loss: 1.3991659255698323, Final Batch Loss: 0.0052548618987202644\n",
      "Epoch 4087, Loss: 1.5430495366454124, Final Batch Loss: 0.06342919915914536\n",
      "Epoch 4088, Loss: 2.642115890979767, Final Batch Loss: 1.1529978513717651\n",
      "Epoch 4089, Loss: 2.8572528958320618, Final Batch Loss: 1.3400825262069702\n",
      "Epoch 4090, Loss: 1.5471421412657946, Final Batch Loss: 0.0015480450820177794\n",
      "Epoch 4091, Loss: 1.3807804638054222, Final Batch Loss: 0.002247666707262397\n",
      "Epoch 4092, Loss: 1.4562284981366247, Final Batch Loss: 0.0037551855202764273\n",
      "Epoch 4093, Loss: 2.298912614583969, Final Batch Loss: 0.8281776309013367\n",
      "Epoch 4094, Loss: 2.7730432748794556, Final Batch Loss: 1.3424477577209473\n",
      "Epoch 4095, Loss: 1.4913298711180687, Final Batch Loss: 0.08089276403188705\n",
      "Epoch 4096, Loss: 1.4533969657495618, Final Batch Loss: 0.0073085324838757515\n",
      "Epoch 4097, Loss: 1.3387596959073562, Final Batch Loss: 0.00047708096099086106\n",
      "Epoch 4098, Loss: 2.4523937106132507, Final Batch Loss: 1.0960909128189087\n",
      "Epoch 4099, Loss: 1.3911270706448704, Final Batch Loss: 0.0023824183735996485\n",
      "Epoch 4100, Loss: 3.117366760969162, Final Batch Loss: 1.7907289266586304\n",
      "Epoch 4101, Loss: 1.3926395061425865, Final Batch Loss: 0.005335140507668257\n",
      "Epoch 4102, Loss: 2.3507803976535797, Final Batch Loss: 0.9674654006958008\n",
      "Epoch 4103, Loss: 1.6315537691116333, Final Batch Loss: 0.20126166939735413\n",
      "Epoch 4104, Loss: 1.3688747291453183, Final Batch Loss: 0.005989104975014925\n",
      "Epoch 4105, Loss: 1.3394852159544826, Final Batch Loss: 0.010466682724654675\n",
      "Epoch 4106, Loss: 1.4177300576120615, Final Batch Loss: 0.012337321415543556\n",
      "Epoch 4107, Loss: 1.4225829541683197, Final Batch Loss: 0.07927635312080383\n",
      "Epoch 4108, Loss: 2.1078935861587524, Final Batch Loss: 0.7553815841674805\n",
      "Epoch 4109, Loss: 2.4438456892967224, Final Batch Loss: 1.0945336818695068\n",
      "Epoch 4110, Loss: 1.5425819531083107, Final Batch Loss: 0.0334785059094429\n",
      "Epoch 4111, Loss: 1.4829231891781092, Final Batch Loss: 0.009031752124428749\n",
      "Epoch 4112, Loss: 1.4071951502701268, Final Batch Loss: 0.0016736084362491965\n",
      "Epoch 4113, Loss: 1.4542052446904563, Final Batch Loss: 1.3351351299206726e-05\n",
      "Epoch 4114, Loss: 1.5024456378159812, Final Batch Loss: 2.6940935640595853e-05\n",
      "Epoch 4115, Loss: 1.4516010861843824, Final Batch Loss: 0.022792158648371696\n",
      "Epoch 4116, Loss: 1.4313903916627169, Final Batch Loss: 0.017619984224438667\n",
      "Epoch 4117, Loss: 2.577360063791275, Final Batch Loss: 1.2387124300003052\n",
      "Epoch 4118, Loss: 2.0102965235710144, Final Batch Loss: 0.6373092532157898\n",
      "Epoch 4119, Loss: 2.4100804924964905, Final Batch Loss: 0.9507920742034912\n",
      "Epoch 4120, Loss: 1.5202368386089802, Final Batch Loss: 0.019790221005678177\n",
      "Epoch 4121, Loss: 3.2401550710201263, Final Batch Loss: 1.6790779829025269\n",
      "Epoch 4122, Loss: 1.5270794616590138, Final Batch Loss: 5.185469490243122e-05\n",
      "Epoch 4123, Loss: 1.3825629092752934, Final Batch Loss: 0.02434641495347023\n",
      "Epoch 4124, Loss: 1.4692995324730873, Final Batch Loss: 0.07419291883707047\n",
      "Epoch 4125, Loss: 1.7452887892723083, Final Batch Loss: 0.4055537283420563\n",
      "Epoch 4126, Loss: 1.3840534836053848, Final Batch Loss: 0.0019601434469223022\n",
      "Epoch 4127, Loss: 3.4991995692253113, Final Batch Loss: 2.083806276321411\n",
      "Epoch 4128, Loss: 1.3263465657364577, Final Batch Loss: 0.002643544925376773\n",
      "Epoch 4129, Loss: 1.3927705995738506, Final Batch Loss: 0.02091250941157341\n",
      "Epoch 4130, Loss: 1.310937212780118, Final Batch Loss: 0.010381506755948067\n",
      "Epoch 4131, Loss: 1.5920005291700363, Final Batch Loss: 0.1093246191740036\n",
      "Epoch 4132, Loss: 1.4425226971507072, Final Batch Loss: 0.10746463388204575\n",
      "Epoch 4133, Loss: 1.3851566184312105, Final Batch Loss: 0.006157593801617622\n",
      "Epoch 4134, Loss: 3.347549259662628, Final Batch Loss: 2.026594400405884\n",
      "Epoch 4135, Loss: 1.7246689200401306, Final Batch Loss: 0.34008878469467163\n",
      "Epoch 4136, Loss: 2.3926051557064056, Final Batch Loss: 0.9065126180648804\n",
      "Epoch 4137, Loss: 1.635678393766284, Final Batch Loss: 0.008862210437655449\n",
      "Epoch 4138, Loss: 1.868951492011547, Final Batch Loss: 0.07077144831418991\n",
      "Epoch 4139, Loss: 3.076643794775009, Final Batch Loss: 1.5428738594055176\n",
      "Epoch 4140, Loss: 1.4433670327998698, Final Batch Loss: 0.00312530854716897\n",
      "Epoch 4141, Loss: 1.4332071896642447, Final Batch Loss: 0.01924041472375393\n",
      "Epoch 4142, Loss: 1.4298656806349754, Final Batch Loss: 0.03809305280447006\n",
      "Epoch 4143, Loss: 1.7217510640621185, Final Batch Loss: 0.37727275490760803\n",
      "Epoch 4144, Loss: 2.8678632378578186, Final Batch Loss: 1.4272319078445435\n",
      "Epoch 4145, Loss: 2.6375737488269806, Final Batch Loss: 1.2258124351501465\n",
      "Epoch 4146, Loss: 2.9306589663028717, Final Batch Loss: 1.624778151512146\n",
      "Epoch 4147, Loss: 1.5532644372433424, Final Batch Loss: 0.019490746781229973\n",
      "Epoch 4148, Loss: 5.514843761920929, Final Batch Loss: 3.8683738708496094\n",
      "Epoch 4149, Loss: 3.552698642015457, Final Batch Loss: 2.0199646949768066\n",
      "Epoch 4150, Loss: 4.087976455688477, Final Batch Loss: 2.0618300437927246\n",
      "Epoch 4151, Loss: 2.3108190558850765, Final Batch Loss: 0.059085216373205185\n",
      "Epoch 4152, Loss: 2.4634649446234107, Final Batch Loss: 0.001359730027616024\n",
      "Epoch 4153, Loss: 4.369887351989746, Final Batch Loss: 1.8849918842315674\n",
      "Epoch 4154, Loss: 7.2355563044548035, Final Batch Loss: 4.802703857421875\n",
      "Epoch 4155, Loss: 2.160608698541182, Final Batch Loss: 0.0001436368766007945\n",
      "Epoch 4156, Loss: 2.0870898216962814, Final Batch Loss: 0.07478480041027069\n",
      "Epoch 4157, Loss: 5.448866784572601, Final Batch Loss: 3.561917781829834\n",
      "Epoch 4158, Loss: 3.5026251077651978, Final Batch Loss: 1.6459845304489136\n",
      "Epoch 4159, Loss: 4.702838659286499, Final Batch Loss: 2.8782429695129395\n",
      "Epoch 4160, Loss: 1.92856764793396, Final Batch Loss: 0.12690222263336182\n",
      "Epoch 4161, Loss: 3.369745373725891, Final Batch Loss: 1.5096467733383179\n",
      "Epoch 4162, Loss: 1.8301924174884334, Final Batch Loss: 0.0011180347064509988\n",
      "Epoch 4163, Loss: 1.9407491870224476, Final Batch Loss: 0.04719223454594612\n",
      "Epoch 4164, Loss: 1.723265469071066, Final Batch Loss: 2.50339189733495e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4165, Loss: 1.7929856665432453, Final Batch Loss: 0.047875989228487015\n",
      "Epoch 4166, Loss: 2.7403976917266846, Final Batch Loss: 1.075478434562683\n",
      "Epoch 4167, Loss: 1.7275457307696342, Final Batch Loss: 0.033575110137462616\n",
      "Epoch 4168, Loss: 1.6913240551948547, Final Batch Loss: 0.07288122177124023\n",
      "Epoch 4169, Loss: 1.6602557376027107, Final Batch Loss: 0.07478911429643631\n",
      "Epoch 4170, Loss: 1.6790826171636581, Final Batch Loss: 0.15093664824962616\n",
      "Epoch 4171, Loss: 1.8301764130592346, Final Batch Loss: 0.34491726756095886\n",
      "Epoch 4172, Loss: 2.899241864681244, Final Batch Loss: 1.3357360363006592\n",
      "Epoch 4173, Loss: 1.744536381214857, Final Batch Loss: 0.046459417790174484\n",
      "Epoch 4174, Loss: 1.9349185763858259, Final Batch Loss: 0.0026755272410809994\n",
      "Epoch 4175, Loss: 3.4989580512046814, Final Batch Loss: 1.6710058450698853\n",
      "Epoch 4176, Loss: 1.8097956953570247, Final Batch Loss: 0.01174499373883009\n",
      "Epoch 4177, Loss: 2.7577365040779114, Final Batch Loss: 1.1222786903381348\n",
      "Epoch 4178, Loss: 1.6226266101002693, Final Batch Loss: 0.07357422262430191\n",
      "Epoch 4179, Loss: 1.6106157004833221, Final Batch Loss: 0.09858730435371399\n",
      "Epoch 4180, Loss: 3.098870724439621, Final Batch Loss: 1.6242997646331787\n",
      "Epoch 4181, Loss: 2.3794838786125183, Final Batch Loss: 0.9267374277114868\n",
      "Epoch 4182, Loss: 1.6386719346046448, Final Batch Loss: 0.15662705898284912\n",
      "Epoch 4183, Loss: 1.5154617468360811, Final Batch Loss: 0.0028962830547243357\n",
      "Epoch 4184, Loss: 1.7011744510382414, Final Batch Loss: 0.03107447735965252\n",
      "Epoch 4185, Loss: 1.62512544170022, Final Batch Loss: 0.05453965440392494\n",
      "Epoch 4186, Loss: 3.2391331791877747, Final Batch Loss: 1.6058008670806885\n",
      "Epoch 4187, Loss: 2.727230191230774, Final Batch Loss: 1.1252719163894653\n",
      "Epoch 4188, Loss: 2.2679879665374756, Final Batch Loss: 0.6851803660392761\n",
      "Epoch 4189, Loss: 1.7953543961048126, Final Batch Loss: 0.20807477831840515\n",
      "Epoch 4190, Loss: 2.5796653628349304, Final Batch Loss: 0.985223650932312\n",
      "Epoch 4191, Loss: 1.5433373453561217, Final Batch Loss: 0.0026731493417173624\n",
      "Epoch 4192, Loss: 1.8625078797340393, Final Batch Loss: 0.3932669460773468\n",
      "Epoch 4193, Loss: 1.6005034600384533, Final Batch Loss: 0.005971685517579317\n",
      "Epoch 4194, Loss: 1.5043847484630533, Final Batch Loss: 0.0002735478919930756\n",
      "Epoch 4195, Loss: 1.4661070816218853, Final Batch Loss: 0.005166631191968918\n",
      "Epoch 4196, Loss: 2.3833419382572174, Final Batch Loss: 0.8542621731758118\n",
      "Epoch 4197, Loss: 2.2717228829860687, Final Batch Loss: 0.815072238445282\n",
      "Epoch 4198, Loss: 1.4373836359009147, Final Batch Loss: 0.004475339315831661\n",
      "Epoch 4199, Loss: 1.5227925081853755, Final Batch Loss: 0.0005638201837427914\n",
      "Epoch 4200, Loss: 1.5877291448414326, Final Batch Loss: 0.021944094449281693\n",
      "Epoch 4201, Loss: 1.6357451118528843, Final Batch Loss: 0.05677143111824989\n",
      "Epoch 4202, Loss: 3.0553295016288757, Final Batch Loss: 1.461058259010315\n",
      "Epoch 4203, Loss: 2.754887193441391, Final Batch Loss: 1.248928427696228\n",
      "Epoch 4204, Loss: 1.648105040192604, Final Batch Loss: 0.11687217652797699\n",
      "Epoch 4205, Loss: 2.5503323078155518, Final Batch Loss: 1.1141611337661743\n",
      "Epoch 4206, Loss: 1.470268969424069, Final Batch Loss: 0.0036586271598935127\n",
      "Epoch 4207, Loss: 1.6266206502914429, Final Batch Loss: 0.07994535565376282\n",
      "Epoch 4208, Loss: 1.6354415565729141, Final Batch Loss: 0.1490035206079483\n",
      "Epoch 4209, Loss: 1.6249882951378822, Final Batch Loss: 0.1052641049027443\n",
      "Epoch 4210, Loss: 1.4684477444025106, Final Batch Loss: 2.9801878554280847e-05\n",
      "Epoch 4211, Loss: 1.5520731955766678, Final Batch Loss: 0.08254794776439667\n",
      "Epoch 4212, Loss: 3.0758002400398254, Final Batch Loss: 1.6582238674163818\n",
      "Epoch 4213, Loss: 1.9154898524284363, Final Batch Loss: 0.5020599365234375\n",
      "Epoch 4214, Loss: 1.422995707951486, Final Batch Loss: 0.008404127322137356\n",
      "Epoch 4215, Loss: 4.022499263286591, Final Batch Loss: 2.627042293548584\n",
      "Epoch 4216, Loss: 1.3715769029222429, Final Batch Loss: 0.0024656630121171474\n",
      "Epoch 4217, Loss: 1.4495947598479688, Final Batch Loss: 0.004032695200294256\n",
      "Epoch 4218, Loss: 1.3754633858334273, Final Batch Loss: 0.0012044801842421293\n",
      "Epoch 4219, Loss: 1.5561964377411641, Final Batch Loss: 0.000164018536452204\n",
      "Epoch 4220, Loss: 2.5130703151226044, Final Batch Loss: 1.1053646802902222\n",
      "Epoch 4221, Loss: 1.4883784111589193, Final Batch Loss: 0.017779147252440453\n",
      "Epoch 4222, Loss: 1.3045338401570916, Final Batch Loss: 0.006354839541018009\n",
      "Epoch 4223, Loss: 1.4453807132085785, Final Batch Loss: 0.0017959432443603873\n",
      "Epoch 4224, Loss: 1.3678237944841385, Final Batch Loss: 0.01749219000339508\n",
      "Epoch 4225, Loss: 1.3472655820660293, Final Batch Loss: 0.002862881403416395\n",
      "Epoch 4226, Loss: 1.3743287189863622, Final Batch Loss: 0.0025647147558629513\n",
      "Epoch 4227, Loss: 2.1666964292526245, Final Batch Loss: 0.7850982546806335\n",
      "Epoch 4228, Loss: 2.751442402601242, Final Batch Loss: 1.3069050312042236\n",
      "Epoch 4229, Loss: 2.2577247619628906, Final Batch Loss: 0.8955946564674377\n",
      "Epoch 4230, Loss: 1.7495800852775574, Final Batch Loss: 0.31243547797203064\n",
      "Epoch 4231, Loss: 1.4511606693267822, Final Batch Loss: 0.11137312650680542\n",
      "Epoch 4232, Loss: 2.468115746974945, Final Batch Loss: 1.0193393230438232\n",
      "Epoch 4233, Loss: 1.3694966174662113, Final Batch Loss: 0.0075857676565647125\n",
      "Epoch 4234, Loss: 1.54283650231082, Final Batch Loss: 0.001369848963804543\n",
      "Epoch 4235, Loss: 1.5148069309070706, Final Batch Loss: 0.011369203217327595\n",
      "Epoch 4236, Loss: 2.3622852861881256, Final Batch Loss: 0.8694400191307068\n",
      "Epoch 4237, Loss: 1.584699166356586, Final Batch Loss: 0.0009939497103914618\n",
      "Epoch 4238, Loss: 1.5439968353603035, Final Batch Loss: 0.0012248402927070856\n",
      "Epoch 4239, Loss: 1.4787303656339645, Final Batch Loss: 0.06993256509304047\n",
      "Epoch 4240, Loss: 1.420291431248188, Final Batch Loss: 0.011248387396335602\n",
      "Epoch 4241, Loss: 1.521801933646202, Final Batch Loss: 0.016373559832572937\n",
      "Epoch 4242, Loss: 1.4684979226440191, Final Batch Loss: 0.020623764023184776\n",
      "Epoch 4243, Loss: 1.4347637868486345, Final Batch Loss: 0.0024535334669053555\n",
      "Epoch 4244, Loss: 1.5360921397805214, Final Batch Loss: 0.1021653488278389\n",
      "Epoch 4245, Loss: 1.430834800004959, Final Batch Loss: 0.03129968047142029\n",
      "Epoch 4246, Loss: 3.1732889115810394, Final Batch Loss: 1.7614690065383911\n",
      "Epoch 4247, Loss: 1.4038193593733013, Final Batch Loss: 0.0051674614660441875\n",
      "Epoch 4248, Loss: 1.3157186325988732, Final Batch Loss: 0.00052426423644647\n",
      "Epoch 4249, Loss: 1.4501577807823196, Final Batch Loss: 0.0015116228023543954\n",
      "Epoch 4250, Loss: 3.3840406835079193, Final Batch Loss: 1.9706816673278809\n",
      "Epoch 4251, Loss: 1.698518455028534, Final Batch Loss: 0.26597633957862854\n",
      "Epoch 4252, Loss: 1.5227530542761087, Final Batch Loss: 0.013623828068375587\n",
      "Epoch 4253, Loss: 1.3943598461337388, Final Batch Loss: 0.0030996394343674183\n",
      "Epoch 4254, Loss: 1.472657963167876, Final Batch Loss: 0.00712828291580081\n",
      "Epoch 4255, Loss: 2.2130935192108154, Final Batch Loss: 0.7382404804229736\n",
      "Epoch 4256, Loss: 1.41292211599648, Final Batch Loss: 0.016591312363743782\n",
      "Epoch 4257, Loss: 1.4434078093618155, Final Batch Loss: 0.018365805968642235\n",
      "Epoch 4258, Loss: 1.4369744583964348, Final Batch Loss: 0.070820651948452\n",
      "Epoch 4259, Loss: 2.3870997428894043, Final Batch Loss: 1.0231621265411377\n",
      "Epoch 4260, Loss: 3.81781467795372, Final Batch Loss: 2.385171890258789\n",
      "Epoch 4261, Loss: 1.4522127844393253, Final Batch Loss: 0.040192220360040665\n",
      "Epoch 4262, Loss: 2.886980801820755, Final Batch Loss: 1.4825094938278198\n",
      "Epoch 4263, Loss: 1.3993387250229716, Final Batch Loss: 0.009852352552115917\n",
      "Epoch 4264, Loss: 1.4815621350426227, Final Batch Loss: 0.001968947472050786\n",
      "Epoch 4265, Loss: 1.481416055932641, Final Batch Loss: 0.02527741529047489\n",
      "Epoch 4266, Loss: 1.450359582901001, Final Batch Loss: 0.004730343818664551\n",
      "Epoch 4267, Loss: 1.6133946776390076, Final Batch Loss: 0.23340943455696106\n",
      "Epoch 4268, Loss: 1.4337875619530678, Final Batch Loss: 0.10286686569452286\n",
      "Epoch 4269, Loss: 1.6425115168094635, Final Batch Loss: 0.2501404583454132\n",
      "Epoch 4270, Loss: 2.8701902329921722, Final Batch Loss: 1.56025230884552\n",
      "Epoch 4271, Loss: 1.3600235140911536, Final Batch Loss: 0.00016878610767889768\n",
      "Epoch 4272, Loss: 2.782584935426712, Final Batch Loss: 1.4666905403137207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4273, Loss: 2.6203404366970062, Final Batch Loss: 1.1615824699401855\n",
      "Epoch 4274, Loss: 1.3230544589459896, Final Batch Loss: 0.013333793729543686\n",
      "Epoch 4275, Loss: 2.2653743624687195, Final Batch Loss: 0.942147970199585\n",
      "Epoch 4276, Loss: 1.4635535515844822, Final Batch Loss: 0.05164060369133949\n",
      "Epoch 4277, Loss: 1.3532891813665628, Final Batch Loss: 0.01933279074728489\n",
      "Epoch 4278, Loss: 1.3182168666971847, Final Batch Loss: 0.0015587572706863284\n",
      "Epoch 4279, Loss: 2.389964610338211, Final Batch Loss: 0.9892034530639648\n",
      "Epoch 4280, Loss: 2.052469879388809, Final Batch Loss: 0.7756443023681641\n",
      "Epoch 4281, Loss: 1.3429733663797379, Final Batch Loss: 0.03673700988292694\n",
      "Epoch 4282, Loss: 2.1584618985652924, Final Batch Loss: 0.7782040238380432\n",
      "Epoch 4283, Loss: 1.67715722322464, Final Batch Loss: 0.33186593651771545\n",
      "Epoch 4284, Loss: 1.4196697250008583, Final Batch Loss: 0.05259314924478531\n",
      "Epoch 4285, Loss: 2.5723610520362854, Final Batch Loss: 1.167608380317688\n",
      "Epoch 4286, Loss: 2.817707806825638, Final Batch Loss: 1.4001579284667969\n",
      "Epoch 4287, Loss: 1.3610638377722353, Final Batch Loss: 0.002276450162753463\n",
      "Epoch 4288, Loss: 1.4741243151947856, Final Batch Loss: 0.010631117038428783\n",
      "Epoch 4289, Loss: 3.2462854385375977, Final Batch Loss: 1.804057002067566\n",
      "Epoch 4290, Loss: 1.4298693351447582, Final Batch Loss: 0.03424600884318352\n",
      "Epoch 4291, Loss: 2.3213917911052704, Final Batch Loss: 0.9450358152389526\n",
      "Epoch 4292, Loss: 1.6735076904296875, Final Batch Loss: 0.18270260095596313\n",
      "Epoch 4293, Loss: 1.43590687494725, Final Batch Loss: 0.010654824785888195\n",
      "Epoch 4294, Loss: 2.689219444990158, Final Batch Loss: 1.2821205854415894\n",
      "Epoch 4295, Loss: 1.5070432648062706, Final Batch Loss: 0.07305022329092026\n",
      "Epoch 4296, Loss: 1.9605531096458435, Final Batch Loss: 0.7069582939147949\n",
      "Epoch 4297, Loss: 1.3165215589106083, Final Batch Loss: 0.015549961477518082\n",
      "Epoch 4298, Loss: 1.5934217870235443, Final Batch Loss: 0.29296380281448364\n",
      "Epoch 4299, Loss: 1.358875285834074, Final Batch Loss: 0.043205779045820236\n",
      "Epoch 4300, Loss: 1.3779835324676242, Final Batch Loss: 0.0003672163875307888\n",
      "Epoch 4301, Loss: 1.437126100063324, Final Batch Loss: 0.11174386739730835\n",
      "Epoch 4302, Loss: 1.4052802524529397, Final Batch Loss: 0.00441374396905303\n",
      "Epoch 4303, Loss: 1.4470863044261932, Final Batch Loss: 0.14847752451896667\n",
      "Epoch 4304, Loss: 1.4594842419028282, Final Batch Loss: 0.07216887921094894\n",
      "Epoch 4305, Loss: 1.3584257275797427, Final Batch Loss: 0.004947206471115351\n",
      "Epoch 4306, Loss: 1.8056550323963165, Final Batch Loss: 0.5005253553390503\n",
      "Epoch 4307, Loss: 3.244253695011139, Final Batch Loss: 1.8965446949005127\n",
      "Epoch 4308, Loss: 1.469891756772995, Final Batch Loss: 0.0687004029750824\n",
      "Epoch 4309, Loss: 1.424066993407905, Final Batch Loss: 0.005371305160224438\n",
      "Epoch 4310, Loss: 1.2844113442115486, Final Batch Loss: 0.004231903236359358\n",
      "Epoch 4311, Loss: 1.5697771310806274, Final Batch Loss: 0.2665409445762634\n",
      "Epoch 4312, Loss: 1.5412878543138504, Final Batch Loss: 0.20918937027454376\n",
      "Epoch 4313, Loss: 1.528383070603013, Final Batch Loss: 0.026814008131623268\n",
      "Epoch 4314, Loss: 3.5023784935474396, Final Batch Loss: 2.1295478343963623\n",
      "Epoch 4315, Loss: 1.5151403918862343, Final Batch Loss: 0.08244026452302933\n",
      "Epoch 4316, Loss: 1.5415328815579414, Final Batch Loss: 0.09394953399896622\n",
      "Epoch 4317, Loss: 1.5062966861296445, Final Batch Loss: 0.003504327731207013\n",
      "Epoch 4318, Loss: 3.309621512889862, Final Batch Loss: 1.7451300621032715\n",
      "Epoch 4319, Loss: 1.5525732010573847, Final Batch Loss: 7.724463648628443e-05\n",
      "Epoch 4320, Loss: 1.9143961668014526, Final Batch Loss: 0.16847115755081177\n",
      "Epoch 4321, Loss: 1.7333687501159147, Final Batch Loss: 9.238292841473594e-05\n",
      "Epoch 4322, Loss: 1.808669663965702, Final Batch Loss: 0.048609472811222076\n",
      "Epoch 4323, Loss: 3.197151482105255, Final Batch Loss: 1.4515788555145264\n",
      "Epoch 4324, Loss: 1.6942971646785736, Final Batch Loss: 0.003389805555343628\n",
      "Epoch 4325, Loss: 1.5979399145580828, Final Batch Loss: 0.003141945693641901\n",
      "Epoch 4326, Loss: 4.440652430057526, Final Batch Loss: 2.8428213596343994\n",
      "Epoch 4327, Loss: 1.5988346299272962, Final Batch Loss: 0.00035637227119877934\n",
      "Epoch 4328, Loss: 1.551806747667797, Final Batch Loss: 2.2172682292875834e-05\n",
      "Epoch 4329, Loss: 1.523316445061937, Final Batch Loss: 0.0024026355240494013\n",
      "Epoch 4330, Loss: 1.455899235988909, Final Batch Loss: 7.211902266135439e-05\n",
      "Epoch 4331, Loss: 1.4338258877396584, Final Batch Loss: 0.03804508596658707\n",
      "Epoch 4332, Loss: 1.4275976903736591, Final Batch Loss: 0.0010175295174121857\n",
      "Epoch 4333, Loss: 1.3949495926499367, Final Batch Loss: 0.026766307651996613\n",
      "Epoch 4334, Loss: 2.928693562746048, Final Batch Loss: 1.5204828977584839\n",
      "Epoch 4335, Loss: 1.4118881156900898, Final Batch Loss: 0.0005021026590839028\n",
      "Epoch 4336, Loss: 1.5423362080473453, Final Batch Loss: 0.0009940688032656908\n",
      "Epoch 4337, Loss: 1.5632055327296257, Final Batch Loss: 0.03742703050374985\n",
      "Epoch 4338, Loss: 1.6235932111740112, Final Batch Loss: 0.0935094952583313\n",
      "Epoch 4339, Loss: 1.5189205212518573, Final Batch Loss: 0.0032614869996905327\n",
      "Epoch 4340, Loss: 2.00483775138855, Final Batch Loss: 0.5570417642593384\n",
      "Epoch 4341, Loss: 1.4592916937544942, Final Batch Loss: 0.004505601711571217\n",
      "Epoch 4342, Loss: 1.623479553963989, Final Batch Loss: 0.003793188836425543\n",
      "Epoch 4343, Loss: 1.8212335631251335, Final Batch Loss: 0.08464925736188889\n",
      "Epoch 4344, Loss: 2.757597327232361, Final Batch Loss: 1.0866152048110962\n",
      "Epoch 4345, Loss: 1.81589974462986, Final Batch Loss: 0.11507560312747955\n",
      "Epoch 4346, Loss: 1.5584512652421836, Final Batch Loss: 0.00042250767000950873\n",
      "Epoch 4347, Loss: 1.8229160904884338, Final Batch Loss: 0.2448364794254303\n",
      "Epoch 4348, Loss: 2.533409506082535, Final Batch Loss: 1.13089120388031\n",
      "Epoch 4349, Loss: 1.4705248987302184, Final Batch Loss: 0.009958229027688503\n",
      "Epoch 4350, Loss: 1.4600782016059384, Final Batch Loss: 0.00027509720530360937\n",
      "Epoch 4351, Loss: 1.4396104142069817, Final Batch Loss: 0.040490709245204926\n",
      "Epoch 4352, Loss: 1.3842744515786762, Final Batch Loss: 5.185469490243122e-05\n",
      "Epoch 4353, Loss: 1.572780042886734, Final Batch Loss: 0.0947999656200409\n",
      "Epoch 4354, Loss: 1.769016981124878, Final Batch Loss: 0.3446967899799347\n",
      "Epoch 4355, Loss: 1.3347020850051194, Final Batch Loss: 0.003187577472999692\n",
      "Epoch 4356, Loss: 1.600113905966282, Final Batch Loss: 0.06847066432237625\n",
      "Epoch 4357, Loss: 1.6726467488333583, Final Batch Loss: 0.0023837266489863396\n",
      "Epoch 4358, Loss: 1.8809110075235367, Final Batch Loss: 0.2165849357843399\n",
      "Epoch 4359, Loss: 1.8357845544815063, Final Batch Loss: 0.3324100077152252\n",
      "Epoch 4360, Loss: 1.5121474378247513, Final Batch Loss: 0.00019274283840786666\n",
      "Epoch 4361, Loss: 2.5585581958293915, Final Batch Loss: 1.0942277908325195\n",
      "Epoch 4362, Loss: 1.5338153839111328, Final Batch Loss: 0.09828859567642212\n",
      "Epoch 4363, Loss: 2.420566350221634, Final Batch Loss: 1.006935477256775\n",
      "Epoch 4364, Loss: 4.366973459720612, Final Batch Loss: 2.8931498527526855\n",
      "Epoch 4365, Loss: 2.0612393617630005, Final Batch Loss: 0.763598620891571\n",
      "Epoch 4366, Loss: 3.553778797388077, Final Batch Loss: 2.1259608268737793\n",
      "Epoch 4367, Loss: 3.422714799642563, Final Batch Loss: 2.0017967224121094\n",
      "Epoch 4368, Loss: 3.5247257351875305, Final Batch Loss: 2.0927555561065674\n",
      "Epoch 4369, Loss: 2.0393172800540924, Final Batch Loss: 0.4867679178714752\n",
      "Epoch 4370, Loss: 1.6353285312652588, Final Batch Loss: 0.06443887948989868\n",
      "Epoch 4371, Loss: 1.939832478761673, Final Batch Loss: 0.26285094022750854\n",
      "Epoch 4372, Loss: 1.757869377732277, Final Batch Loss: 0.12554098665714264\n",
      "Epoch 4373, Loss: 1.8293769359588623, Final Batch Loss: 0.18556231260299683\n",
      "Epoch 4374, Loss: 1.709645390510559, Final Batch Loss: 0.09192413091659546\n",
      "Epoch 4375, Loss: 2.107109785079956, Final Batch Loss: 0.6266889572143555\n",
      "Epoch 4376, Loss: 1.6483242213726044, Final Batch Loss: 0.11966180801391602\n",
      "Epoch 4377, Loss: 1.6432754807174206, Final Batch Loss: 0.02734540030360222\n",
      "Epoch 4378, Loss: 6.763367384672165, Final Batch Loss: 5.232729911804199\n",
      "Epoch 4379, Loss: 1.4371975297108293, Final Batch Loss: 0.0013193720951676369\n",
      "Epoch 4380, Loss: 1.5232004225254059, Final Batch Loss: 0.12622734904289246\n",
      "Epoch 4381, Loss: 1.3119608977576718, Final Batch Loss: 0.0015488782664760947\n",
      "Epoch 4382, Loss: 2.4503891468048096, Final Batch Loss: 1.0536720752716064\n",
      "Epoch 4383, Loss: 1.3280096533708274, Final Batch Loss: 0.0021470370702445507\n",
      "Epoch 4384, Loss: 2.773230403661728, Final Batch Loss: 1.3431804180145264\n",
      "Epoch 4385, Loss: 1.417232720181346, Final Batch Loss: 0.019792674109339714\n",
      "Epoch 4386, Loss: 1.513213850557804, Final Batch Loss: 0.09716770797967911\n",
      "Epoch 4387, Loss: 3.795503079891205, Final Batch Loss: 2.4054598808288574\n",
      "Epoch 4388, Loss: 1.41920829936862, Final Batch Loss: 0.06050310656428337\n",
      "Epoch 4389, Loss: 2.5744574069976807, Final Batch Loss: 1.1853216886520386\n",
      "Epoch 4390, Loss: 1.4178029894828796, Final Batch Loss: 0.08332324028015137\n",
      "Epoch 4391, Loss: 1.4052157793194056, Final Batch Loss: 0.01270509697496891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4392, Loss: 1.4139211943256669, Final Batch Loss: 0.0006191005813889205\n",
      "Epoch 4393, Loss: 2.843819350004196, Final Batch Loss: 1.543851375579834\n",
      "Epoch 4394, Loss: 1.4058551527559757, Final Batch Loss: 0.03516680374741554\n",
      "Epoch 4395, Loss: 1.391578372567892, Final Batch Loss: 0.03808685764670372\n",
      "Epoch 4396, Loss: 1.360361571307294, Final Batch Loss: 0.00026294111739844084\n",
      "Epoch 4397, Loss: 2.7759951651096344, Final Batch Loss: 1.4736671447753906\n",
      "Epoch 4398, Loss: 1.4402743285754696, Final Batch Loss: 0.0017765468219295144\n",
      "Epoch 4399, Loss: 1.4323957413434982, Final Batch Loss: 0.06610052287578583\n",
      "Epoch 4400, Loss: 3.1372254490852356, Final Batch Loss: 1.8037620782852173\n",
      "Epoch 4401, Loss: 1.344749415293336, Final Batch Loss: 0.01906990446150303\n",
      "Epoch 4402, Loss: 2.2385632395744324, Final Batch Loss: 0.8866991400718689\n",
      "Epoch 4403, Loss: 1.8340699225664139, Final Batch Loss: 0.19003845751285553\n",
      "Epoch 4404, Loss: 1.7028294122574152, Final Batch Loss: 0.00012885693286079913\n",
      "Epoch 4405, Loss: 3.1612912714481354, Final Batch Loss: 1.624543309211731\n",
      "Epoch 4406, Loss: 2.3092005252838135, Final Batch Loss: 0.6706083416938782\n",
      "Epoch 4407, Loss: 1.6029569134116173, Final Batch Loss: 0.0622231587767601\n",
      "Epoch 4408, Loss: 1.4944658437743783, Final Batch Loss: 0.00045468006283044815\n",
      "Epoch 4409, Loss: 1.4925101809203625, Final Batch Loss: 0.01196787878870964\n",
      "Epoch 4410, Loss: 2.1704406440258026, Final Batch Loss: 0.6631728410720825\n",
      "Epoch 4411, Loss: 3.324563503265381, Final Batch Loss: 1.7726315259933472\n",
      "Epoch 4412, Loss: 2.6179041266441345, Final Batch Loss: 1.2117217779159546\n",
      "Epoch 4413, Loss: 1.466686487197876, Final Batch Loss: 0.03999340534210205\n",
      "Epoch 4414, Loss: 1.4173728749155998, Final Batch Loss: 0.0611567422747612\n",
      "Epoch 4415, Loss: 1.4606708912178874, Final Batch Loss: 0.0083638159558177\n",
      "Epoch 4416, Loss: 2.5062163174152374, Final Batch Loss: 1.06966233253479\n",
      "Epoch 4417, Loss: 2.3451600670814514, Final Batch Loss: 0.9854762554168701\n",
      "Epoch 4418, Loss: 1.3042317405343056, Final Batch Loss: 0.022933490574359894\n",
      "Epoch 4419, Loss: 1.4963523922488093, Final Batch Loss: 0.0027973828837275505\n",
      "Epoch 4420, Loss: 2.404508948326111, Final Batch Loss: 0.9126842021942139\n",
      "Epoch 4421, Loss: 2.120756149291992, Final Batch Loss: 0.5709578394889832\n",
      "Epoch 4422, Loss: 1.5756329684518278, Final Batch Loss: 0.000760385300964117\n",
      "Epoch 4423, Loss: 1.5430458597838879, Final Batch Loss: 0.025702159851789474\n",
      "Epoch 4424, Loss: 1.7898325026035309, Final Batch Loss: 0.44244933128356934\n",
      "Epoch 4425, Loss: 1.4192118076607585, Final Batch Loss: 0.01127255242317915\n",
      "Epoch 4426, Loss: 1.5145379005680297, Final Batch Loss: 1.537788011773955e-05\n",
      "Epoch 4427, Loss: 1.5830738618969917, Final Batch Loss: 0.07959500700235367\n",
      "Epoch 4428, Loss: 1.4542263597249985, Final Batch Loss: 0.03618161380290985\n",
      "Epoch 4429, Loss: 1.334440290927887, Final Batch Loss: 0.005662709474563599\n",
      "Epoch 4430, Loss: 2.7038993537425995, Final Batch Loss: 1.2767729759216309\n",
      "Epoch 4431, Loss: 2.63670814037323, Final Batch Loss: 1.3297775983810425\n",
      "Epoch 4432, Loss: 1.4865268059074879, Final Batch Loss: 0.05734928324818611\n",
      "Epoch 4433, Loss: 1.3707640133798122, Final Batch Loss: 0.006058778613805771\n",
      "Epoch 4434, Loss: 1.404560076072812, Final Batch Loss: 0.009793566539883614\n",
      "Epoch 4435, Loss: 1.316222174820723, Final Batch Loss: 0.0004592079494614154\n",
      "Epoch 4436, Loss: 1.6449939608573914, Final Batch Loss: 0.25282198190689087\n",
      "Epoch 4437, Loss: 1.7298317849636078, Final Batch Loss: 0.29106345772743225\n",
      "Epoch 4438, Loss: 2.414681762456894, Final Batch Loss: 1.022098422050476\n",
      "Epoch 4439, Loss: 2.7483082711696625, Final Batch Loss: 1.3387683629989624\n",
      "Epoch 4440, Loss: 3.319956064224243, Final Batch Loss: 1.9710286855697632\n",
      "Epoch 4441, Loss: 1.5788517389446497, Final Batch Loss: 0.019121715798974037\n",
      "Epoch 4442, Loss: 4.123644948005676, Final Batch Loss: 2.515742778778076\n",
      "Epoch 4443, Loss: 1.3479368072003126, Final Batch Loss: 0.00516342930495739\n",
      "Epoch 4444, Loss: 1.5822719484567642, Final Batch Loss: 0.17755450308322906\n",
      "Epoch 4445, Loss: 2.278371751308441, Final Batch Loss: 0.7416454553604126\n",
      "Epoch 4446, Loss: 1.5482658930122852, Final Batch Loss: 0.024797488003969193\n",
      "Epoch 4447, Loss: 2.399989515542984, Final Batch Loss: 0.8526133298873901\n",
      "Epoch 4448, Loss: 1.6061207689344883, Final Batch Loss: 0.035190973430871964\n",
      "Epoch 4449, Loss: 2.8311072289943695, Final Batch Loss: 1.3911163806915283\n",
      "Epoch 4450, Loss: 1.4796520816162229, Final Batch Loss: 0.010829840786755085\n",
      "Epoch 4451, Loss: 2.619070053100586, Final Batch Loss: 1.209714412689209\n",
      "Epoch 4452, Loss: 1.4640069678425789, Final Batch Loss: 0.005587436258792877\n",
      "Epoch 4453, Loss: 1.8119435254484415, Final Batch Loss: 0.027107352390885353\n",
      "Epoch 4454, Loss: 2.4497761577367783, Final Batch Loss: 0.11687938868999481\n",
      "Epoch 4455, Loss: 2.320528320968151, Final Batch Loss: 0.09299182146787643\n",
      "Epoch 4456, Loss: 2.0896023400127888, Final Batch Loss: 0.007572043687105179\n",
      "Epoch 4457, Loss: 4.315013766288757, Final Batch Loss: 2.191108226776123\n",
      "Epoch 4458, Loss: 1.8427859488874674, Final Batch Loss: 0.03103160299360752\n",
      "Epoch 4459, Loss: 1.790499554015696, Final Batch Loss: 0.0055109718814492226\n",
      "Epoch 4460, Loss: 5.990508437156677, Final Batch Loss: 4.438634395599365\n",
      "Epoch 4461, Loss: 4.593272417783737, Final Batch Loss: 3.0457823276519775\n",
      "Epoch 4462, Loss: 1.5999623148236424, Final Batch Loss: 0.0016245751176029444\n",
      "Epoch 4463, Loss: 1.9359680116176605, Final Batch Loss: 0.3144403398036957\n",
      "Epoch 4464, Loss: 1.79743235866772, Final Batch Loss: 0.0004970983718521893\n",
      "Epoch 4465, Loss: 3.3692100644111633, Final Batch Loss: 1.4939308166503906\n",
      "Epoch 4466, Loss: 3.928934693336487, Final Batch Loss: 2.028249502182007\n",
      "Epoch 4467, Loss: 2.220479041337967, Final Batch Loss: 0.4378187358379364\n",
      "Epoch 4468, Loss: 2.103164851665497, Final Batch Loss: 0.2964516282081604\n",
      "Epoch 4469, Loss: 2.0413965582847595, Final Batch Loss: 0.33553487062454224\n",
      "Epoch 4470, Loss: 1.605384878872428, Final Batch Loss: 0.00012158608296886086\n",
      "Epoch 4471, Loss: 4.842262417078018, Final Batch Loss: 3.1460464000701904\n",
      "Epoch 4472, Loss: 1.6388871371746063, Final Batch Loss: 0.016473472118377686\n",
      "Epoch 4473, Loss: 3.344105064868927, Final Batch Loss: 1.7328553199768066\n",
      "Epoch 4474, Loss: 1.8402954526245594, Final Batch Loss: 0.04690017178654671\n",
      "Epoch 4475, Loss: 2.2920291423797607, Final Batch Loss: 0.5495118498802185\n",
      "Epoch 4476, Loss: 1.6246124263852835, Final Batch Loss: 0.010605758056044579\n",
      "Epoch 4477, Loss: 1.6609445661306381, Final Batch Loss: 0.028837934136390686\n",
      "Epoch 4478, Loss: 2.04236501455307, Final Batch Loss: 0.5081273317337036\n",
      "Epoch 4479, Loss: 2.5059684813022614, Final Batch Loss: 0.8468155264854431\n",
      "Epoch 4480, Loss: 1.5327458444517106, Final Batch Loss: 0.0022339883726090193\n",
      "Epoch 4481, Loss: 1.4852016493678093, Final Batch Loss: 0.005985431373119354\n",
      "Epoch 4482, Loss: 1.5145011497661471, Final Batch Loss: 0.007990892045199871\n",
      "Epoch 4483, Loss: 1.494654755690135, Final Batch Loss: 0.00039641151670366526\n",
      "Epoch 4484, Loss: 1.412036415655166, Final Batch Loss: 0.002028909046202898\n",
      "Epoch 4485, Loss: 3.0595268309116364, Final Batch Loss: 1.5941691398620605\n",
      "Epoch 4486, Loss: 1.4849600717425346, Final Batch Loss: 0.057277463376522064\n",
      "Epoch 4487, Loss: 1.4284491348080337, Final Batch Loss: 0.005218218546360731\n",
      "Epoch 4488, Loss: 2.693655401468277, Final Batch Loss: 1.2160611152648926\n",
      "Epoch 4489, Loss: 1.6808125972747803, Final Batch Loss: 0.2986376881599426\n",
      "Epoch 4490, Loss: 2.684269279241562, Final Batch Loss: 1.1946876049041748\n",
      "Epoch 4491, Loss: 1.5045700781047344, Final Batch Loss: 0.004696410149335861\n",
      "Epoch 4492, Loss: 2.683960646390915, Final Batch Loss: 1.2676341533660889\n",
      "Epoch 4493, Loss: 1.5390084579121321, Final Batch Loss: 0.0024568631779402494\n",
      "Epoch 4494, Loss: 1.534546557624708, Final Batch Loss: 0.00022921319759916514\n",
      "Epoch 4495, Loss: 1.455008964985609, Final Batch Loss: 0.017655473202466965\n",
      "Epoch 4496, Loss: 2.8944345712661743, Final Batch Loss: 1.4529931545257568\n",
      "Epoch 4497, Loss: 2.3614738285541534, Final Batch Loss: 0.9249876737594604\n",
      "Epoch 4498, Loss: 1.4658089587464929, Final Batch Loss: 0.009912551380693913\n",
      "Epoch 4499, Loss: 2.97432804107666, Final Batch Loss: 1.4501105546951294\n",
      "Epoch 4500, Loss: 2.231238931417465, Final Batch Loss: 0.7957625389099121\n",
      "Epoch 4501, Loss: 1.4953310526907444, Final Batch Loss: 0.04632273688912392\n",
      "Epoch 4502, Loss: 1.9715389013290405, Final Batch Loss: 0.5144762396812439\n",
      "Epoch 4503, Loss: 1.4517795331776142, Final Batch Loss: 0.04436551406979561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4504, Loss: 1.9166177213191986, Final Batch Loss: 0.5350395441055298\n",
      "Epoch 4505, Loss: 1.4992907084524632, Final Batch Loss: 0.05843955650925636\n",
      "Epoch 4506, Loss: 1.3912353813648224, Final Batch Loss: 0.021024107933044434\n",
      "Epoch 4507, Loss: 2.2159386575222015, Final Batch Loss: 0.8273929953575134\n",
      "Epoch 4508, Loss: 4.198502033948898, Final Batch Loss: 2.812020778656006\n",
      "Epoch 4509, Loss: 1.3532328363507986, Final Batch Loss: 0.022943859919905663\n",
      "Epoch 4510, Loss: 1.6070863343775272, Final Batch Loss: 0.03488675132393837\n",
      "Epoch 4511, Loss: 4.767364144325256, Final Batch Loss: 3.039306640625\n",
      "Epoch 4512, Loss: 1.8317191042006016, Final Batch Loss: 0.04211384430527687\n",
      "Epoch 4513, Loss: 1.930668167769909, Final Batch Loss: 0.11188500374555588\n",
      "Epoch 4514, Loss: 2.199108362197876, Final Batch Loss: 0.34496361017227173\n",
      "Epoch 4515, Loss: 1.8497233539819717, Final Batch Loss: 0.1055838018655777\n",
      "Epoch 4516, Loss: 1.6018247185274959, Final Batch Loss: 0.007761913351714611\n",
      "Epoch 4517, Loss: 1.5416983431205153, Final Batch Loss: 0.009505725465714931\n",
      "Epoch 4518, Loss: 1.5470224524615332, Final Batch Loss: 0.00013731967192143202\n",
      "Epoch 4519, Loss: 3.366508424282074, Final Batch Loss: 1.9417351484298706\n",
      "Epoch 4520, Loss: 1.5371469035744667, Final Batch Loss: 0.0839361622929573\n",
      "Epoch 4521, Loss: 1.4746073335409164, Final Batch Loss: 0.008032873272895813\n",
      "Epoch 4522, Loss: 1.4798002187162638, Final Batch Loss: 0.020163560286164284\n",
      "Epoch 4523, Loss: 2.2577419579029083, Final Batch Loss: 0.7604652643203735\n",
      "Epoch 4524, Loss: 1.3419120585313067, Final Batch Loss: 0.001692411839030683\n",
      "Epoch 4525, Loss: 5.10992243885994, Final Batch Loss: 3.6517767906188965\n",
      "Epoch 4526, Loss: 1.5022343348246068, Final Batch Loss: 0.0022653888445347548\n",
      "Epoch 4527, Loss: 1.5530229700962082, Final Batch Loss: 0.0009248746791854501\n",
      "Epoch 4528, Loss: 1.6013021655380726, Final Batch Loss: 0.006698060780763626\n",
      "Epoch 4529, Loss: 1.6534336488693953, Final Batch Loss: 0.014220921322703362\n",
      "Epoch 4530, Loss: 3.1783367395401, Final Batch Loss: 1.600156545639038\n",
      "Epoch 4531, Loss: 1.9171744287014008, Final Batch Loss: 0.3233128488063812\n",
      "Epoch 4532, Loss: 2.844044655561447, Final Batch Loss: 1.3352943658828735\n",
      "Epoch 4533, Loss: 1.4195934804156423, Final Batch Loss: 0.013150391168892384\n",
      "Epoch 4534, Loss: 2.8802621960639954, Final Batch Loss: 1.3316700458526611\n",
      "Epoch 4535, Loss: 1.4756758543662727, Final Batch Loss: 0.00627926317974925\n",
      "Epoch 4536, Loss: 1.405085762962699, Final Batch Loss: 0.013231685385107994\n",
      "Epoch 4537, Loss: 1.53614458325319, Final Batch Loss: 0.0028040397446602583\n",
      "Epoch 4538, Loss: 3.580249071121216, Final Batch Loss: 2.150023937225342\n",
      "Epoch 4539, Loss: 1.4142105511855334, Final Batch Loss: 0.003257565898820758\n",
      "Epoch 4540, Loss: 1.8858149349689484, Final Batch Loss: 0.3393811285495758\n",
      "Epoch 4541, Loss: 1.5343276935163885, Final Batch Loss: 0.0026814716402441263\n",
      "Epoch 4542, Loss: 1.4903254390519578, Final Batch Loss: 0.0001538873475510627\n",
      "Epoch 4543, Loss: 1.606990773230791, Final Batch Loss: 0.04830487444996834\n",
      "Epoch 4544, Loss: 1.4484664108604193, Final Batch Loss: 0.0254452433437109\n",
      "Epoch 4545, Loss: 1.525274836923927, Final Batch Loss: 0.007753515150398016\n",
      "Epoch 4546, Loss: 1.6095132678747177, Final Batch Loss: 0.13307665288448334\n",
      "Epoch 4547, Loss: 1.3998814371880144, Final Batch Loss: 0.003877028590068221\n",
      "Epoch 4548, Loss: 1.4856362640857697, Final Batch Loss: 0.17469188570976257\n",
      "Epoch 4549, Loss: 1.4298938112333417, Final Batch Loss: 0.006568034179508686\n",
      "Epoch 4550, Loss: 2.5962939858436584, Final Batch Loss: 1.2563621997833252\n",
      "Epoch 4551, Loss: 2.845217317342758, Final Batch Loss: 1.4997715950012207\n",
      "Epoch 4552, Loss: 1.460614237934351, Final Batch Loss: 0.03555021062493324\n",
      "Epoch 4553, Loss: 1.5350672164931893, Final Batch Loss: 0.008774415589869022\n",
      "Epoch 4554, Loss: 1.5149571495130658, Final Batch Loss: 0.0067200856283307076\n",
      "Epoch 4555, Loss: 1.5469756326638162, Final Batch Loss: 0.006098946090787649\n",
      "Epoch 4556, Loss: 1.659969374537468, Final Batch Loss: 0.22056053578853607\n",
      "Epoch 4557, Loss: 1.4545348845422268, Final Batch Loss: 0.03390761837363243\n",
      "Epoch 4558, Loss: 1.3962879171594977, Final Batch Loss: 0.009330122731626034\n",
      "Epoch 4559, Loss: 1.4079751893877983, Final Batch Loss: 0.02304183691740036\n",
      "Epoch 4560, Loss: 1.4223206220194697, Final Batch Loss: 0.008668768219649792\n",
      "Epoch 4561, Loss: 1.4072009436786175, Final Batch Loss: 0.002036403864622116\n",
      "Epoch 4562, Loss: 1.3921734234318137, Final Batch Loss: 0.0008641323074698448\n",
      "Epoch 4563, Loss: 1.29556520213373, Final Batch Loss: 0.002600147621706128\n",
      "Epoch 4564, Loss: 1.4404566287994385, Final Batch Loss: 0.10007134079933167\n",
      "Epoch 4565, Loss: 1.4216952808201313, Final Batch Loss: 0.053535182029008865\n",
      "Epoch 4566, Loss: 1.5786228477954865, Final Batch Loss: 0.2665879726409912\n",
      "Epoch 4567, Loss: 2.105291038751602, Final Batch Loss: 0.691119372844696\n",
      "Epoch 4568, Loss: 1.3972332387929782, Final Batch Loss: 0.0012278169160708785\n",
      "Epoch 4569, Loss: 1.3473965987504926, Final Batch Loss: 0.00016378014697693288\n",
      "Epoch 4570, Loss: 1.2741032908088528, Final Batch Loss: 0.00032455421751365066\n",
      "Epoch 4571, Loss: 1.472355879843235, Final Batch Loss: 0.12423697859048843\n",
      "Epoch 4572, Loss: 2.0142131745815277, Final Batch Loss: 0.8183532357215881\n",
      "Epoch 4573, Loss: 1.281875524276984, Final Batch Loss: 0.0002300474588992074\n",
      "Epoch 4574, Loss: 2.1974716186523438, Final Batch Loss: 0.9168695211410522\n",
      "Epoch 4575, Loss: 1.6825258433818817, Final Batch Loss: 0.4181809425354004\n",
      "Epoch 4576, Loss: 2.1121393740177155, Final Batch Loss: 0.7436263561248779\n",
      "Epoch 4577, Loss: 1.2599834681022912, Final Batch Loss: 0.001973706530407071\n",
      "Epoch 4578, Loss: 1.2766849966792506, Final Batch Loss: 7.462222856702283e-05\n",
      "Epoch 4579, Loss: 1.4578997259959579, Final Batch Loss: 0.0037859445437788963\n",
      "Epoch 4580, Loss: 1.578239381313324, Final Batch Loss: 0.1604185700416565\n",
      "Epoch 4581, Loss: 2.1474306285381317, Final Batch Loss: 0.8740350604057312\n",
      "Epoch 4582, Loss: 1.757091373205185, Final Batch Loss: 0.4585760831832886\n",
      "Epoch 4583, Loss: 3.20736625790596, Final Batch Loss: 1.8157460689544678\n",
      "Epoch 4584, Loss: 1.2771547641605139, Final Batch Loss: 0.016270002350211143\n",
      "Epoch 4585, Loss: 1.3986057173460722, Final Batch Loss: 0.019041014835238457\n",
      "Epoch 4586, Loss: 1.2991209081374109, Final Batch Loss: 0.004059290047734976\n",
      "Epoch 4587, Loss: 1.283855115994811, Final Batch Loss: 0.00998065434396267\n",
      "Epoch 4588, Loss: 1.2895060312002897, Final Batch Loss: 0.010249597951769829\n",
      "Epoch 4589, Loss: 1.3536534226150252, Final Batch Loss: 0.00027616979787126184\n",
      "Epoch 4590, Loss: 1.5902557671070099, Final Batch Loss: 0.2944395840167999\n",
      "Epoch 4591, Loss: 1.2923370581120253, Final Batch Loss: 0.02189522795379162\n",
      "Epoch 4592, Loss: 1.3958280431106687, Final Batch Loss: 0.009694402106106281\n",
      "Epoch 4593, Loss: 1.3111233175732195, Final Batch Loss: 0.002388602588325739\n",
      "Epoch 4594, Loss: 1.3433921430259943, Final Batch Loss: 0.003270873799920082\n",
      "Epoch 4595, Loss: 1.386760470457375, Final Batch Loss: 0.0044122012332081795\n",
      "Epoch 4596, Loss: 1.3652420935904956, Final Batch Loss: 2.0503786799963564e-05\n",
      "Epoch 4597, Loss: 1.4420665204524994, Final Batch Loss: 0.14043879508972168\n",
      "Epoch 4598, Loss: 1.2757573947310448, Final Batch Loss: 0.022171132266521454\n",
      "Epoch 4599, Loss: 1.2902195323258638, Final Batch Loss: 0.004312263801693916\n",
      "Epoch 4600, Loss: 1.321967106545344, Final Batch Loss: 0.0017018134240061045\n",
      "Epoch 4601, Loss: 1.7156580686569214, Final Batch Loss: 0.3835781216621399\n",
      "Epoch 4602, Loss: 1.3002884044544771, Final Batch Loss: 0.0014675810234621167\n",
      "Epoch 4603, Loss: 1.3672395199537277, Final Batch Loss: 0.012695327401161194\n",
      "Epoch 4604, Loss: 2.0332349240779877, Final Batch Loss: 0.7143145203590393\n",
      "Epoch 4605, Loss: 1.3239591931924224, Final Batch Loss: 0.003330281935632229\n",
      "Epoch 4606, Loss: 2.8717524111270905, Final Batch Loss: 1.6150965690612793\n",
      "Epoch 4607, Loss: 1.9312083721160889, Final Batch Loss: 0.5867039561271667\n",
      "Epoch 4608, Loss: 1.5434328317642212, Final Batch Loss: 0.2586131989955902\n",
      "Epoch 4609, Loss: 1.4303679838776588, Final Batch Loss: 0.097406305372715\n",
      "Epoch 4610, Loss: 3.601748675107956, Final Batch Loss: 2.3292555809020996\n",
      "Epoch 4611, Loss: 1.3390977457165718, Final Batch Loss: 0.03685248643159866\n",
      "Epoch 4612, Loss: 1.3081968259066343, Final Batch Loss: 0.01761518232524395\n",
      "Epoch 4613, Loss: 1.2934600110584142, Final Batch Loss: 2.90866428258596e-05\n",
      "Epoch 4614, Loss: 1.895035296678543, Final Batch Loss: 0.6054962277412415\n",
      "Epoch 4615, Loss: 1.3715768568217754, Final Batch Loss: 0.054472487419843674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4616, Loss: 1.3434634134173393, Final Batch Loss: 0.04337187856435776\n",
      "Epoch 4617, Loss: 1.2943406151607633, Final Batch Loss: 0.011369203217327595\n",
      "Epoch 4618, Loss: 1.3095363155007362, Final Batch Loss: 0.036507852375507355\n",
      "Epoch 4619, Loss: 1.4093246832489967, Final Batch Loss: 0.0017893984913825989\n",
      "Epoch 4620, Loss: 2.372121125459671, Final Batch Loss: 1.0016798973083496\n",
      "Epoch 4621, Loss: 1.463351845741272, Final Batch Loss: 0.16949310898780823\n",
      "Epoch 4622, Loss: 2.401535242795944, Final Batch Loss: 1.1648471355438232\n",
      "Epoch 4623, Loss: 1.5548105239868164, Final Batch Loss: 0.1830383837223053\n",
      "Epoch 4624, Loss: 4.159240156412125, Final Batch Loss: 2.749357223510742\n",
      "Epoch 4625, Loss: 1.362344890832901, Final Batch Loss: 0.07813912630081177\n",
      "Epoch 4626, Loss: 1.6617021262645721, Final Batch Loss: 0.14282143115997314\n",
      "Epoch 4627, Loss: 1.6940665617585182, Final Batch Loss: 0.11272586137056351\n",
      "Epoch 4628, Loss: 1.6817254647612572, Final Batch Loss: 0.057202599942684174\n",
      "Epoch 4629, Loss: 2.41757071018219, Final Batch Loss: 0.8394102454185486\n",
      "Epoch 4630, Loss: 2.685141295194626, Final Batch Loss: 1.0615959167480469\n",
      "Epoch 4631, Loss: 1.788670539855957, Final Batch Loss: 0.23857852816581726\n",
      "Epoch 4632, Loss: 1.4722697660326958, Final Batch Loss: 0.054565392434597015\n",
      "Epoch 4633, Loss: 1.5519488379359245, Final Batch Loss: 0.07191475480794907\n",
      "Epoch 4634, Loss: 2.8372191786766052, Final Batch Loss: 1.3624541759490967\n",
      "Epoch 4635, Loss: 1.9469285011291504, Final Batch Loss: 0.6141172051429749\n",
      "Epoch 4636, Loss: 1.3099604295566678, Final Batch Loss: 0.013586078770458698\n",
      "Epoch 4637, Loss: 1.3235553838312626, Final Batch Loss: 0.036130908876657486\n",
      "Epoch 4638, Loss: 1.5118015557527542, Final Batch Loss: 0.18920789659023285\n",
      "Epoch 4639, Loss: 3.5753062069416046, Final Batch Loss: 2.252333402633667\n",
      "Epoch 4640, Loss: 2.166663348674774, Final Batch Loss: 0.8620178699493408\n",
      "Epoch 4641, Loss: 3.1560474932193756, Final Batch Loss: 1.8076657056808472\n",
      "Epoch 4642, Loss: 3.3947140872478485, Final Batch Loss: 1.9549157619476318\n",
      "Epoch 4643, Loss: 1.4099039621651173, Final Batch Loss: 0.05107938125729561\n",
      "Epoch 4644, Loss: 2.3872088193893433, Final Batch Loss: 1.0498911142349243\n",
      "Epoch 4645, Loss: 1.3391717460181098, Final Batch Loss: 0.0004020121123176068\n",
      "Epoch 4646, Loss: 1.5059897899627686, Final Batch Loss: 0.06655880808830261\n",
      "Epoch 4647, Loss: 1.4545025452971458, Final Batch Loss: 0.06251951307058334\n",
      "Epoch 4648, Loss: 1.4078949447721243, Final Batch Loss: 0.0010587330907583237\n",
      "Epoch 4649, Loss: 1.4344086796045303, Final Batch Loss: 0.07077966630458832\n",
      "Epoch 4650, Loss: 2.374824106693268, Final Batch Loss: 1.0462491512298584\n",
      "Epoch 4651, Loss: 2.4369482696056366, Final Batch Loss: 1.083755373954773\n",
      "Epoch 4652, Loss: 1.420450548408553, Final Batch Loss: 0.0026694636326283216\n",
      "Epoch 4653, Loss: 1.4536450859159231, Final Batch Loss: 0.010470693930983543\n",
      "Epoch 4654, Loss: 1.4532834449782968, Final Batch Loss: 0.00967173371464014\n",
      "Epoch 4655, Loss: 3.2174250781536102, Final Batch Loss: 1.7901885509490967\n",
      "Epoch 4656, Loss: 1.3476622528396547, Final Batch Loss: 0.005691868718713522\n",
      "Epoch 4657, Loss: 2.3289820849895477, Final Batch Loss: 1.021547794342041\n",
      "Epoch 4658, Loss: 1.3306332249194384, Final Batch Loss: 0.008213551715016365\n",
      "Epoch 4659, Loss: 1.4140115287154913, Final Batch Loss: 0.03065778873860836\n",
      "Epoch 4660, Loss: 2.9121638238430023, Final Batch Loss: 1.5299991369247437\n",
      "Epoch 4661, Loss: 1.4330370910465717, Final Batch Loss: 0.047662217170000076\n",
      "Epoch 4662, Loss: 1.3096956806257367, Final Batch Loss: 0.0021043317392468452\n",
      "Epoch 4663, Loss: 1.3389470892725512, Final Batch Loss: 0.0010339635191485286\n",
      "Epoch 4664, Loss: 2.0046235620975494, Final Batch Loss: 0.718394935131073\n",
      "Epoch 4665, Loss: 1.3680673241594832, Final Batch Loss: 2.0265558760002023e-06\n",
      "Epoch 4666, Loss: 2.2414266169071198, Final Batch Loss: 0.9196845293045044\n",
      "Epoch 4667, Loss: 1.3612415165043785, Final Batch Loss: 7.676783570786938e-05\n",
      "Epoch 4668, Loss: 1.4019756382331252, Final Batch Loss: 0.009697235189378262\n",
      "Epoch 4669, Loss: 1.7741400301456451, Final Batch Loss: 0.46317389607429504\n",
      "Epoch 4670, Loss: 1.2781601082533598, Final Batch Loss: 0.006791366264224052\n",
      "Epoch 4671, Loss: 1.436897687613964, Final Batch Loss: 0.12012558430433273\n",
      "Epoch 4672, Loss: 1.2941320706158876, Final Batch Loss: 0.02810208685696125\n",
      "Epoch 4673, Loss: 1.297721380367875, Final Batch Loss: 0.023898864164948463\n",
      "Epoch 4674, Loss: 1.3655518796294928, Final Batch Loss: 0.009771963581442833\n",
      "Epoch 4675, Loss: 1.9848066568374634, Final Batch Loss: 0.6390863656997681\n",
      "Epoch 4676, Loss: 1.2747062221169472, Final Batch Loss: 0.04807028919458389\n",
      "Epoch 4677, Loss: 1.5093665421009064, Final Batch Loss: 0.15653562545776367\n",
      "Epoch 4678, Loss: 2.8427107334136963, Final Batch Loss: 1.4619874954223633\n",
      "Epoch 4679, Loss: 1.425356276333332, Final Batch Loss: 0.07794693857431412\n",
      "Epoch 4680, Loss: 1.3804542869329453, Final Batch Loss: 0.03353084623813629\n",
      "Epoch 4681, Loss: 2.4749012887477875, Final Batch Loss: 1.188888669013977\n",
      "Epoch 4682, Loss: 1.2845148984342813, Final Batch Loss: 0.005146825686097145\n",
      "Epoch 4683, Loss: 1.5126909911632538, Final Batch Loss: 0.12751692533493042\n",
      "Epoch 4684, Loss: 1.2781699588522315, Final Batch Loss: 0.00490331556648016\n",
      "Epoch 4685, Loss: 2.51208832859993, Final Batch Loss: 1.2378811836242676\n",
      "Epoch 4686, Loss: 3.2098488807678223, Final Batch Loss: 1.9496922492980957\n",
      "Epoch 4687, Loss: 1.28442885982804, Final Batch Loss: 0.0010961245279759169\n",
      "Epoch 4688, Loss: 1.2884207908064127, Final Batch Loss: 0.02352377213537693\n",
      "Epoch 4689, Loss: 1.336752887815237, Final Batch Loss: 0.026120569556951523\n",
      "Epoch 4690, Loss: 1.3732592676969944, Final Batch Loss: 0.00013004888023715466\n",
      "Epoch 4691, Loss: 1.9448655843734741, Final Batch Loss: 0.611083984375\n",
      "Epoch 4692, Loss: 1.3041900023818016, Final Batch Loss: 0.03851892799139023\n",
      "Epoch 4693, Loss: 1.343654862648691, Final Batch Loss: 0.00012909532233607024\n",
      "Epoch 4694, Loss: 3.661671459674835, Final Batch Loss: 2.2882542610168457\n",
      "Epoch 4695, Loss: 1.3054517693817616, Final Batch Loss: 0.0013452060520648956\n",
      "Epoch 4696, Loss: 1.4186810962855816, Final Batch Loss: 0.03349153324961662\n",
      "Epoch 4697, Loss: 1.3951928652822971, Final Batch Loss: 0.007229004055261612\n",
      "Epoch 4698, Loss: 1.8201642632484436, Final Batch Loss: 0.5226603150367737\n",
      "Epoch 4699, Loss: 1.40349513432011, Final Batch Loss: 0.007566483225673437\n",
      "Epoch 4700, Loss: 1.3074956776108593, Final Batch Loss: 0.0022657455410808325\n",
      "Epoch 4701, Loss: 1.3504225388169289, Final Batch Loss: 0.035148389637470245\n",
      "Epoch 4702, Loss: 3.6839756071567535, Final Batch Loss: 2.2716617584228516\n",
      "Epoch 4703, Loss: 1.5268809795379639, Final Batch Loss: 0.20711100101470947\n",
      "Epoch 4704, Loss: 1.4421624523602077, Final Batch Loss: 5.94836674281396e-05\n",
      "Epoch 4705, Loss: 1.6487377062439919, Final Batch Loss: 0.11545909196138382\n",
      "Epoch 4706, Loss: 1.702292561531067, Final Batch Loss: 0.11294949054718018\n",
      "Epoch 4707, Loss: 2.4860293567180634, Final Batch Loss: 0.954838752746582\n",
      "Epoch 4708, Loss: 1.520746961236, Final Batch Loss: 0.056336358189582825\n",
      "Epoch 4709, Loss: 3.3262849152088165, Final Batch Loss: 1.8178510665893555\n",
      "Epoch 4710, Loss: 2.2217444479465485, Final Batch Loss: 0.771459698677063\n",
      "Epoch 4711, Loss: 2.6324976682662964, Final Batch Loss: 1.1489506959915161\n",
      "Epoch 4712, Loss: 1.5243757418356836, Final Batch Loss: 0.006488328333944082\n",
      "Epoch 4713, Loss: 2.8976874351501465, Final Batch Loss: 1.3979706764221191\n",
      "Epoch 4714, Loss: 3.2750945687294006, Final Batch Loss: 1.7853490114212036\n",
      "Epoch 4715, Loss: 3.4199767410755157, Final Batch Loss: 1.9747458696365356\n",
      "Epoch 4716, Loss: 2.602369338274002, Final Batch Loss: 1.1297690868377686\n",
      "Epoch 4717, Loss: 1.3282341939629987, Final Batch Loss: 5.9126061387360096e-05\n",
      "Epoch 4718, Loss: 1.5001616179943085, Final Batch Loss: 0.06895256042480469\n",
      "Epoch 4719, Loss: 1.3643472596158972, Final Batch Loss: 0.00019727191829588264\n",
      "Epoch 4720, Loss: 1.3684482052922249, Final Batch Loss: 0.022002287209033966\n",
      "Epoch 4721, Loss: 1.334126609377563, Final Batch Loss: 0.005095827393233776\n",
      "Epoch 4722, Loss: 2.958203047513962, Final Batch Loss: 1.667177677154541\n",
      "Epoch 4723, Loss: 1.3715003319084644, Final Batch Loss: 0.04032893851399422\n",
      "Epoch 4724, Loss: 1.381082517444156, Final Batch Loss: 0.0013275867095217109\n",
      "Epoch 4725, Loss: 1.268011539708823, Final Batch Loss: 0.0008101756684482098\n",
      "Epoch 4726, Loss: 1.3316238864790648, Final Batch Loss: 0.002885822905227542\n",
      "Epoch 4727, Loss: 1.4003439247608185, Final Batch Loss: 0.15014567971229553\n",
      "Epoch 4728, Loss: 1.49251689016819, Final Batch Loss: 0.2360670417547226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4729, Loss: 1.7720660865306854, Final Batch Loss: 0.5282641649246216\n",
      "Epoch 4730, Loss: 1.3864504024386406, Final Batch Loss: 0.08604451268911362\n",
      "Epoch 4731, Loss: 1.2221353522327263, Final Batch Loss: 0.0004520586517173797\n",
      "Epoch 4732, Loss: 2.1209224462509155, Final Batch Loss: 0.7862394452095032\n",
      "Epoch 4733, Loss: 1.3286064676940441, Final Batch Loss: 0.00939224287867546\n",
      "Epoch 4734, Loss: 1.2995269399834797, Final Batch Loss: 0.0013734203530475497\n",
      "Epoch 4735, Loss: 1.2671750627923757, Final Batch Loss: 0.0034809254575520754\n",
      "Epoch 4736, Loss: 1.3046638434752822, Final Batch Loss: 0.012691325508058071\n",
      "Epoch 4737, Loss: 1.2825522134662606, Final Batch Loss: 0.00024005869636312127\n",
      "Epoch 4738, Loss: 1.3624056465923786, Final Batch Loss: 0.045300740748643875\n",
      "Epoch 4739, Loss: 2.098693609237671, Final Batch Loss: 0.7497652173042297\n",
      "Epoch 4740, Loss: 1.3044868224533275, Final Batch Loss: 0.0010246747406199574\n",
      "Epoch 4741, Loss: 1.6919107735157013, Final Batch Loss: 0.3010241389274597\n",
      "Epoch 4742, Loss: 1.3815077058970928, Final Batch Loss: 0.03852305933833122\n",
      "Epoch 4743, Loss: 1.4607309624552727, Final Batch Loss: 0.025732599198818207\n",
      "Epoch 4744, Loss: 1.393285708501935, Final Batch Loss: 0.026037411764264107\n",
      "Epoch 4745, Loss: 1.328599825501442, Final Batch Loss: 0.017945662140846252\n",
      "Epoch 4746, Loss: 1.4632257223129272, Final Batch Loss: 0.14533179998397827\n",
      "Epoch 4747, Loss: 1.369034020230174, Final Batch Loss: 0.014347253367304802\n",
      "Epoch 4748, Loss: 1.3174726217985153, Final Batch Loss: 0.0058205872774124146\n",
      "Epoch 4749, Loss: 1.6062787175178528, Final Batch Loss: 0.39060401916503906\n",
      "Epoch 4750, Loss: 2.388671427965164, Final Batch Loss: 1.1972181797027588\n",
      "Epoch 4751, Loss: 1.3894425071775913, Final Batch Loss: 0.01637602224946022\n",
      "Epoch 4752, Loss: 3.71116504073143, Final Batch Loss: 2.4935779571533203\n",
      "Epoch 4753, Loss: 1.4437925326637924, Final Batch Loss: 0.007358353119343519\n",
      "Epoch 4754, Loss: 3.0981049239635468, Final Batch Loss: 1.8268744945526123\n",
      "Epoch 4755, Loss: 2.1818695068359375, Final Batch Loss: 0.8346454501152039\n",
      "Epoch 4756, Loss: 2.5506733059883118, Final Batch Loss: 1.207939624786377\n",
      "Epoch 4757, Loss: 2.8030685484409332, Final Batch Loss: 1.4823758602142334\n",
      "Epoch 4758, Loss: 1.4227448962628841, Final Batch Loss: 0.04414471611380577\n",
      "Epoch 4759, Loss: 3.1570454835891724, Final Batch Loss: 1.9041764736175537\n",
      "Epoch 4760, Loss: 1.8432132601737976, Final Batch Loss: 0.6076529622077942\n",
      "Epoch 4761, Loss: 1.37114667147398, Final Batch Loss: 0.013824544847011566\n",
      "Epoch 4762, Loss: 1.3782932721078396, Final Batch Loss: 0.026348169893026352\n",
      "Epoch 4763, Loss: 1.2529771791305393, Final Batch Loss: 0.0037038789596408606\n",
      "Epoch 4764, Loss: 1.5728916227817535, Final Batch Loss: 0.30320313572883606\n",
      "Epoch 4765, Loss: 3.4034385085105896, Final Batch Loss: 2.186647891998291\n",
      "Epoch 4766, Loss: 4.074508428573608, Final Batch Loss: 2.696160078048706\n",
      "Epoch 4767, Loss: 1.7733894549310207, Final Batch Loss: 0.03330602869391441\n",
      "Epoch 4768, Loss: 3.5402104258537292, Final Batch Loss: 1.685239315032959\n",
      "Epoch 4769, Loss: 1.9700494334101677, Final Batch Loss: 0.05055331438779831\n",
      "Epoch 4770, Loss: 2.0164611488580704, Final Batch Loss: 0.04355986416339874\n",
      "Epoch 4771, Loss: 3.1404571533203125, Final Batch Loss: 1.2185910940170288\n",
      "Epoch 4772, Loss: 1.9285185926819395, Final Batch Loss: 3.5523738915799186e-05\n",
      "Epoch 4773, Loss: 4.512821972370148, Final Batch Loss: 2.576462984085083\n",
      "Epoch 4774, Loss: 2.2482008039951324, Final Batch Loss: 0.35698750615119934\n",
      "Epoch 4775, Loss: 1.7342893574386835, Final Batch Loss: 0.02444274164736271\n",
      "Epoch 4776, Loss: 2.8324816823005676, Final Batch Loss: 1.1586170196533203\n",
      "Epoch 4777, Loss: 3.5159093141555786, Final Batch Loss: 1.9157201051712036\n",
      "Epoch 4778, Loss: 3.05059677362442, Final Batch Loss: 1.516381859779358\n",
      "Epoch 4779, Loss: 1.6735443519428372, Final Batch Loss: 0.0017231153324246407\n",
      "Epoch 4780, Loss: 1.5418354123830795, Final Batch Loss: 0.033124521374702454\n",
      "Epoch 4781, Loss: 1.4631726704537868, Final Batch Loss: 0.03095197305083275\n",
      "Epoch 4782, Loss: 1.7291388511657715, Final Batch Loss: 0.2271426022052765\n",
      "Epoch 4783, Loss: 2.8496772050857544, Final Batch Loss: 1.483337640762329\n",
      "Epoch 4784, Loss: 1.375404991209507, Final Batch Loss: 0.07704419642686844\n",
      "Epoch 4785, Loss: 1.3899093121290207, Final Batch Loss: 0.0823291689157486\n",
      "Epoch 4786, Loss: 1.378637945279479, Final Batch Loss: 0.0110438521951437\n",
      "Epoch 4787, Loss: 1.5157545506954193, Final Batch Loss: 0.16006991267204285\n",
      "Epoch 4788, Loss: 2.502073973417282, Final Batch Loss: 1.2283639907836914\n",
      "Epoch 4789, Loss: 3.556695878505707, Final Batch Loss: 2.2125754356384277\n",
      "Epoch 4790, Loss: 1.9409947395324707, Final Batch Loss: 0.6402559876441956\n",
      "Epoch 4791, Loss: 2.0037371814250946, Final Batch Loss: 0.6530614495277405\n",
      "Epoch 4792, Loss: 1.4054633863270283, Final Batch Loss: 0.011762548238039017\n",
      "Epoch 4793, Loss: 1.3833178617060184, Final Batch Loss: 0.03382522240281105\n",
      "Epoch 4794, Loss: 2.1660734117031097, Final Batch Loss: 0.6911008954048157\n",
      "Epoch 4795, Loss: 3.0000303089618683, Final Batch Loss: 1.601557731628418\n",
      "Epoch 4796, Loss: 3.410668760538101, Final Batch Loss: 1.8854138851165771\n",
      "Epoch 4797, Loss: 1.6743876919499598, Final Batch Loss: 0.00025006983196362853\n",
      "Epoch 4798, Loss: 1.606153678148985, Final Batch Loss: 0.035147469490766525\n",
      "Epoch 4799, Loss: 1.6472432669252157, Final Batch Loss: 0.029637819156050682\n",
      "Epoch 4800, Loss: 1.7325393855571747, Final Batch Loss: 0.17045018076896667\n",
      "Epoch 4801, Loss: 2.597567141056061, Final Batch Loss: 1.1393197774887085\n",
      "Epoch 4802, Loss: 2.771706461906433, Final Batch Loss: 1.2502936124801636\n",
      "Epoch 4803, Loss: 1.6460029475783813, Final Batch Loss: 3.755022044060752e-05\n",
      "Epoch 4804, Loss: 1.5715625435695983, Final Batch Loss: 0.0006223172531463206\n",
      "Epoch 4805, Loss: 1.6966700167395175, Final Batch Loss: 0.007433256600052118\n",
      "Epoch 4806, Loss: 1.6655755043029785, Final Batch Loss: 0.10925686359405518\n",
      "Epoch 4807, Loss: 1.4427951741963625, Final Batch Loss: 0.02100508101284504\n",
      "Epoch 4808, Loss: 1.5964234992861748, Final Batch Loss: 0.12387358397245407\n",
      "Epoch 4809, Loss: 1.4108364947605878, Final Batch Loss: 0.003065056400373578\n",
      "Epoch 4810, Loss: 1.3499519292381592, Final Batch Loss: 0.0002165798214264214\n",
      "Epoch 4811, Loss: 2.543564409017563, Final Batch Loss: 1.1174713373184204\n",
      "Epoch 4812, Loss: 1.2986643422627822, Final Batch Loss: 0.0016488541150465608\n",
      "Epoch 4813, Loss: 1.3781178994686343, Final Batch Loss: 0.00019751029321923852\n",
      "Epoch 4814, Loss: 1.2919890919001773, Final Batch Loss: 0.0009091534884646535\n",
      "Epoch 4815, Loss: 1.5062027275562286, Final Batch Loss: 0.1513393223285675\n",
      "Epoch 4816, Loss: 1.4881651252508163, Final Batch Loss: 0.14724455773830414\n",
      "Epoch 4817, Loss: 1.3741186708211899, Final Batch Loss: 0.0722915381193161\n",
      "Epoch 4818, Loss: 2.411604553461075, Final Batch Loss: 1.0734738111495972\n",
      "Epoch 4819, Loss: 1.6940356492996216, Final Batch Loss: 0.2953036427497864\n",
      "Epoch 4820, Loss: 3.0114562809467316, Final Batch Loss: 1.7537473440170288\n",
      "Epoch 4821, Loss: 2.4807799458503723, Final Batch Loss: 1.1964473724365234\n",
      "Epoch 4822, Loss: 1.9145382642745972, Final Batch Loss: 0.49815893173217773\n",
      "Epoch 4823, Loss: 1.2993425130830474, Final Batch Loss: 1.6689286894688848e-06\n",
      "Epoch 4824, Loss: 1.3545415289700031, Final Batch Loss: 0.05401473119854927\n",
      "Epoch 4825, Loss: 1.4304097252897918, Final Batch Loss: 0.0029599922709167004\n",
      "Epoch 4826, Loss: 1.5010513067245483, Final Batch Loss: 0.11613765358924866\n",
      "Epoch 4827, Loss: 1.4052481204271317, Final Batch Loss: 0.06935738027095795\n",
      "Epoch 4828, Loss: 1.4147955774133152, Final Batch Loss: 1.5020257706055418e-05\n",
      "Epoch 4829, Loss: 1.3537431032164022, Final Batch Loss: 0.0015065044863149524\n",
      "Epoch 4830, Loss: 1.2594138756394386, Final Batch Loss: 0.008317001163959503\n",
      "Epoch 4831, Loss: 2.3264001309871674, Final Batch Loss: 1.0482860803604126\n",
      "Epoch 4832, Loss: 1.392259456217289, Final Batch Loss: 0.10474003106355667\n",
      "Epoch 4833, Loss: 1.250610558076005, Final Batch Loss: 6.48477507638745e-05\n",
      "Epoch 4834, Loss: 1.4825089126825333, Final Batch Loss: 0.17159844934940338\n",
      "Epoch 4835, Loss: 1.361776102334261, Final Batch Loss: 0.03251795843243599\n",
      "Epoch 4836, Loss: 1.3766367305070162, Final Batch Loss: 0.008824871852993965\n",
      "Epoch 4837, Loss: 2.0837573409080505, Final Batch Loss: 0.8487805724143982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4838, Loss: 2.234969973564148, Final Batch Loss: 0.9415914416313171\n",
      "Epoch 4839, Loss: 2.9893225729465485, Final Batch Loss: 1.7114454507827759\n",
      "Epoch 4840, Loss: 1.278227571863681, Final Batch Loss: 0.00708792032673955\n",
      "Epoch 4841, Loss: 1.3176463171839714, Final Batch Loss: 0.03783585876226425\n",
      "Epoch 4842, Loss: 1.439064309000969, Final Batch Loss: 0.18396542966365814\n",
      "Epoch 4843, Loss: 1.263045007828623, Final Batch Loss: 0.007197285536676645\n",
      "Epoch 4844, Loss: 2.295118272304535, Final Batch Loss: 0.9885662794113159\n",
      "Epoch 4845, Loss: 1.3027906124480069, Final Batch Loss: 0.00304758595302701\n",
      "Epoch 4846, Loss: 2.630748301744461, Final Batch Loss: 1.3609620332717896\n",
      "Epoch 4847, Loss: 1.316563732456416, Final Batch Loss: 0.004216946195811033\n",
      "Epoch 4848, Loss: 1.3365399427711964, Final Batch Loss: 0.03967689350247383\n",
      "Epoch 4849, Loss: 3.9015497863292694, Final Batch Loss: 2.5900707244873047\n",
      "Epoch 4850, Loss: 1.556344769662246, Final Batch Loss: 0.0021451336797326803\n",
      "Epoch 4851, Loss: 1.5259921334218234, Final Batch Loss: 0.0007613382767885923\n",
      "Epoch 4852, Loss: 1.66660536499694, Final Batch Loss: 0.006835174281150103\n",
      "Epoch 4853, Loss: 3.5764139890670776, Final Batch Loss: 1.9557071924209595\n",
      "Epoch 4854, Loss: 1.5524459341540933, Final Batch Loss: 0.005810157395899296\n",
      "Epoch 4855, Loss: 1.4961996395140886, Final Batch Loss: 0.02807125635445118\n",
      "Epoch 4856, Loss: 1.6644582077860832, Final Batch Loss: 0.06639014929533005\n",
      "Epoch 4857, Loss: 1.6045358898118138, Final Batch Loss: 0.012323898263275623\n",
      "Epoch 4858, Loss: 2.4914695620536804, Final Batch Loss: 1.0627409219741821\n",
      "Epoch 4859, Loss: 1.4320095144212246, Final Batch Loss: 0.0022116266191005707\n",
      "Epoch 4860, Loss: 1.4292334346100688, Final Batch Loss: 0.010143044404685497\n",
      "Epoch 4861, Loss: 1.4710306327906437, Final Batch Loss: 0.0006300609675236046\n",
      "Epoch 4862, Loss: 1.69286447763443, Final Batch Loss: 0.28420060873031616\n",
      "Epoch 4863, Loss: 1.358145110309124, Final Batch Loss: 0.015556417405605316\n",
      "Epoch 4864, Loss: 3.3198978900909424, Final Batch Loss: 2.117778778076172\n",
      "Epoch 4865, Loss: 2.5944529473781586, Final Batch Loss: 1.2854318618774414\n",
      "Epoch 4866, Loss: 1.3182900473475456, Final Batch Loss: 0.07598220556974411\n",
      "Epoch 4867, Loss: 1.3926961896941066, Final Batch Loss: 0.006006049923598766\n",
      "Epoch 4868, Loss: 1.5439628302119672, Final Batch Loss: 0.0018297373317182064\n",
      "Epoch 4869, Loss: 5.007100462913513, Final Batch Loss: 3.41582989692688\n",
      "Epoch 4870, Loss: 3.1141238808631897, Final Batch Loss: 1.73777174949646\n",
      "Epoch 4871, Loss: 1.3032109439373016, Final Batch Loss: 0.001444011926651001\n",
      "Epoch 4872, Loss: 1.3429325425531715, Final Batch Loss: 0.0022142434027045965\n",
      "Epoch 4873, Loss: 1.481213180879422, Final Batch Loss: 5.1616290875244886e-05\n",
      "Epoch 4874, Loss: 3.129311978816986, Final Batch Loss: 1.5774927139282227\n",
      "Epoch 4875, Loss: 2.720088928937912, Final Batch Loss: 1.2715179920196533\n",
      "Epoch 4876, Loss: 1.474795937538147, Final Batch Loss: 0.1087750792503357\n",
      "Epoch 4877, Loss: 2.37296199798584, Final Batch Loss: 1.001696228981018\n",
      "Epoch 4878, Loss: 1.4365327134728432, Final Batch Loss: 0.05622942000627518\n",
      "Epoch 4879, Loss: 1.4338043183088303, Final Batch Loss: 0.07045052945613861\n",
      "Epoch 4880, Loss: 1.4575783982872963, Final Batch Loss: 0.09207472950220108\n",
      "Epoch 4881, Loss: 1.3278225045651197, Final Batch Loss: 0.01791861467063427\n",
      "Epoch 4882, Loss: 1.3198887157486752, Final Batch Loss: 0.0019500303314998746\n",
      "Epoch 4883, Loss: 1.3424774502636865, Final Batch Loss: 0.00031680811662226915\n",
      "Epoch 4884, Loss: 1.3838135576952482, Final Batch Loss: 6.806619057897478e-05\n",
      "Epoch 4885, Loss: 1.2544266618788242, Final Batch Loss: 0.010728184133768082\n",
      "Epoch 4886, Loss: 1.4410637393593788, Final Batch Loss: 0.06847823411226273\n",
      "Epoch 4887, Loss: 1.3257357943803072, Final Batch Loss: 0.022452669218182564\n",
      "Epoch 4888, Loss: 1.3317736220778897, Final Batch Loss: 0.00018249277491122484\n",
      "Epoch 4889, Loss: 1.3752307817339897, Final Batch Loss: 0.07061972469091415\n",
      "Epoch 4890, Loss: 1.5380920618772507, Final Batch Loss: 0.21532835066318512\n",
      "Epoch 4891, Loss: 1.3907312005758286, Final Batch Loss: 0.03348530828952789\n",
      "Epoch 4892, Loss: 1.7500714361667633, Final Batch Loss: 0.4978363811969757\n",
      "Epoch 4893, Loss: 1.3433203659951687, Final Batch Loss: 0.04531669244170189\n",
      "Epoch 4894, Loss: 1.3408549334853888, Final Batch Loss: 0.008807620033621788\n",
      "Epoch 4895, Loss: 1.8498318195343018, Final Batch Loss: 0.5584635734558105\n",
      "Epoch 4896, Loss: 1.1933167912065983, Final Batch Loss: 0.011601481586694717\n",
      "Epoch 4897, Loss: 2.6311243772506714, Final Batch Loss: 1.3858479261398315\n",
      "Epoch 4898, Loss: 1.3616823963820934, Final Batch Loss: 0.01713917776942253\n",
      "Epoch 4899, Loss: 1.2044980712234974, Final Batch Loss: 0.023620177060365677\n",
      "Epoch 4900, Loss: 2.1245410442352295, Final Batch Loss: 0.8058356642723083\n",
      "Epoch 4901, Loss: 2.8765642642974854, Final Batch Loss: 1.5711431503295898\n",
      "Epoch 4902, Loss: 1.473467081784861, Final Batch Loss: 8.344646857949556e-07\n",
      "Epoch 4903, Loss: 1.606491968035698, Final Batch Loss: 0.019684210419654846\n",
      "Epoch 4904, Loss: 2.3371705412864685, Final Batch Loss: 0.714398205280304\n",
      "Epoch 4905, Loss: 1.5553626883774996, Final Batch Loss: 0.029123740270733833\n",
      "Epoch 4906, Loss: 2.6565257012844086, Final Batch Loss: 1.155954360961914\n",
      "Epoch 4907, Loss: 1.3893287858227268, Final Batch Loss: 0.0018862566212192178\n",
      "Epoch 4908, Loss: 1.7705495059490204, Final Batch Loss: 0.4104193150997162\n",
      "Epoch 4909, Loss: 1.391127884911839, Final Batch Loss: 0.0005970602505840361\n",
      "Epoch 4910, Loss: 1.5716364234685898, Final Batch Loss: 0.17890818417072296\n",
      "Epoch 4911, Loss: 1.4122530184686184, Final Batch Loss: 0.05752858147025108\n",
      "Epoch 4912, Loss: 1.286737473681569, Final Batch Loss: 0.02861136384308338\n",
      "Epoch 4913, Loss: 1.3360678413882852, Final Batch Loss: 0.008164719678461552\n",
      "Epoch 4914, Loss: 2.733760803937912, Final Batch Loss: 1.4137818813323975\n",
      "Epoch 4915, Loss: 1.6256954073905945, Final Batch Loss: 0.31770601868629456\n",
      "Epoch 4916, Loss: 2.911168694496155, Final Batch Loss: 1.6614665985107422\n",
      "Epoch 4917, Loss: 1.2704458868829533, Final Batch Loss: 0.0012417471734806895\n",
      "Epoch 4918, Loss: 1.3712362721562386, Final Batch Loss: 0.07269123941659927\n",
      "Epoch 4919, Loss: 1.3999700771528296, Final Batch Loss: 0.00012063252506777644\n",
      "Epoch 4920, Loss: 1.1941013619107252, Final Batch Loss: 5.495397272170521e-05\n",
      "Epoch 4921, Loss: 3.5865795016288757, Final Batch Loss: 2.312593460083008\n",
      "Epoch 4922, Loss: 1.729274958372116, Final Batch Loss: 0.45522841811180115\n",
      "Epoch 4923, Loss: 1.288726732833311, Final Batch Loss: 0.0021757043432444334\n",
      "Epoch 4924, Loss: 2.415402501821518, Final Batch Loss: 1.0357965230941772\n",
      "Epoch 4925, Loss: 1.421692177420482, Final Batch Loss: 0.0025537756737321615\n",
      "Epoch 4926, Loss: 1.4734999993816018, Final Batch Loss: 0.006343112327158451\n",
      "Epoch 4927, Loss: 1.4436775036156178, Final Batch Loss: 0.011964816600084305\n",
      "Epoch 4928, Loss: 1.5258901119232178, Final Batch Loss: 0.1405816376209259\n",
      "Epoch 4929, Loss: 1.4102612808346748, Final Batch Loss: 0.048176057636737823\n",
      "Epoch 4930, Loss: 1.37405613809824, Final Batch Loss: 0.044240064918994904\n",
      "Epoch 4931, Loss: 1.3300996503094211, Final Batch Loss: 0.0014384171226993203\n",
      "Epoch 4932, Loss: 1.3378431926248595, Final Batch Loss: 0.0011849532602354884\n",
      "Epoch 4933, Loss: 1.315888705663383, Final Batch Loss: 0.015262129716575146\n",
      "Epoch 4934, Loss: 1.3756178396288306, Final Batch Loss: 0.0028325694147497416\n",
      "Epoch 4935, Loss: 1.3396994025679305, Final Batch Loss: 0.0018344969721511006\n",
      "Epoch 4936, Loss: 3.2111971974372864, Final Batch Loss: 1.8861737251281738\n",
      "Epoch 4937, Loss: 1.3743135705590248, Final Batch Loss: 0.07982847839593887\n",
      "Epoch 4938, Loss: 1.2855202443897724, Final Batch Loss: 0.017381835728883743\n",
      "Epoch 4939, Loss: 2.2245565354824066, Final Batch Loss: 0.9168915748596191\n",
      "Epoch 4940, Loss: 1.3580962345004082, Final Batch Loss: 0.023427240550518036\n",
      "Epoch 4941, Loss: 1.282262287219055, Final Batch Loss: 0.0011520899133756757\n",
      "Epoch 4942, Loss: 1.3643543687649071, Final Batch Loss: 0.004830476362258196\n",
      "Epoch 4943, Loss: 1.4601864367723465, Final Batch Loss: 0.2005275934934616\n",
      "Epoch 4944, Loss: 1.2737935427576303, Final Batch Loss: 0.023743117228150368\n",
      "Epoch 4945, Loss: 1.3308898671821225, Final Batch Loss: 0.00029416524921543896\n",
      "Epoch 4946, Loss: 1.3052804158069193, Final Batch Loss: 0.003819315228611231\n",
      "Epoch 4947, Loss: 1.2563227096979972, Final Batch Loss: 6.341733387671411e-05\n",
      "Epoch 4948, Loss: 2.258621007204056, Final Batch Loss: 0.9387751817703247\n",
      "Epoch 4949, Loss: 1.3313982672989368, Final Batch Loss: 0.03646107390522957\n",
      "Epoch 4950, Loss: 1.345810508049908, Final Batch Loss: 0.00022146634000819176\n",
      "Epoch 4951, Loss: 1.3022350179962814, Final Batch Loss: 0.0031688022427260876\n",
      "Epoch 4952, Loss: 2.4129579663276672, Final Batch Loss: 1.1398991346359253\n",
      "Epoch 4953, Loss: 1.3329931481748645, Final Batch Loss: 5.6265202147187665e-05\n",
      "Epoch 4954, Loss: 1.271877576597035, Final Batch Loss: 0.002075781114399433\n",
      "Epoch 4955, Loss: 1.307113221468171, Final Batch Loss: 0.0001323135511483997\n",
      "Epoch 4956, Loss: 2.2112933695316315, Final Batch Loss: 0.9499272108078003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4957, Loss: 2.425272226333618, Final Batch Loss: 1.1285241842269897\n",
      "Epoch 4958, Loss: 2.131401091814041, Final Batch Loss: 0.9526341557502747\n",
      "Epoch 4959, Loss: 1.2760076075792313, Final Batch Loss: 0.015598669648170471\n",
      "Epoch 4960, Loss: 1.251242162194103, Final Batch Loss: 0.001997025217860937\n",
      "Epoch 4961, Loss: 1.3099542075069621, Final Batch Loss: 0.0003108495147898793\n",
      "Epoch 4962, Loss: 1.2823466360568716, Final Batch Loss: 2.3841855067985307e-07\n",
      "Epoch 4963, Loss: 2.0835949182510376, Final Batch Loss: 0.7918334603309631\n",
      "Epoch 4964, Loss: 1.2479453468695283, Final Batch Loss: 0.013949164189398289\n",
      "Epoch 4965, Loss: 1.262990732677281, Final Batch Loss: 0.006417025811970234\n",
      "Epoch 4966, Loss: 1.3410422268789262, Final Batch Loss: 0.0025100174825638533\n",
      "Epoch 4967, Loss: 1.3789387344149873, Final Batch Loss: 1.5258672647178173e-05\n",
      "Epoch 4968, Loss: 1.3325711004436016, Final Batch Loss: 0.005957465618848801\n",
      "Epoch 4969, Loss: 2.720726191997528, Final Batch Loss: 1.4784116744995117\n",
      "Epoch 4970, Loss: 1.3352941945195198, Final Batch Loss: 0.028172440826892853\n",
      "Epoch 4971, Loss: 1.6383939981460571, Final Batch Loss: 0.34853434562683105\n",
      "Epoch 4972, Loss: 3.4685316383838654, Final Batch Loss: 2.171849012374878\n",
      "Epoch 4973, Loss: 1.9278502762317657, Final Batch Loss: 0.640385091304779\n",
      "Epoch 4974, Loss: 4.8550994992256165, Final Batch Loss: 3.546004295349121\n",
      "Epoch 4975, Loss: 1.4932697114290931, Final Batch Loss: 2.4676019165781327e-05\n",
      "Epoch 4976, Loss: 1.9166436959058046, Final Batch Loss: 0.01910276897251606\n",
      "Epoch 4977, Loss: 3.102625846862793, Final Batch Loss: 1.1599184274673462\n",
      "Epoch 4978, Loss: 1.9996814699843526, Final Batch Loss: 0.004577633924782276\n",
      "Epoch 4979, Loss: 2.6028401851654053, Final Batch Loss: 0.8481659889221191\n",
      "Epoch 4980, Loss: 1.793708440294722, Final Batch Loss: 8.177422569133341e-05\n",
      "Epoch 4981, Loss: 1.8110133475856856, Final Batch Loss: 0.0015393561916425824\n",
      "Epoch 4982, Loss: 2.9060324132442474, Final Batch Loss: 1.223115086555481\n",
      "Epoch 4983, Loss: 1.695323851890862, Final Batch Loss: 0.012948480434715748\n",
      "Epoch 4984, Loss: 1.7903824700042605, Final Batch Loss: 0.01075377594679594\n",
      "Epoch 4985, Loss: 1.7655608467757702, Final Batch Loss: 0.03587297722697258\n",
      "Epoch 4986, Loss: 1.8694221610203385, Final Batch Loss: 0.012601041235029697\n",
      "Epoch 4987, Loss: 2.477917492389679, Final Batch Loss: 0.7793269753456116\n",
      "Epoch 4988, Loss: 1.695981414988637, Final Batch Loss: 0.016428442671895027\n",
      "Epoch 4989, Loss: 1.7394802570336196, Final Batch Loss: 1.1920922133867862e-06\n",
      "Epoch 4990, Loss: 2.7577807903289795, Final Batch Loss: 1.206743597984314\n",
      "Epoch 4991, Loss: 2.5663142800331116, Final Batch Loss: 1.0467177629470825\n",
      "Epoch 4992, Loss: 1.6499841660261154, Final Batch Loss: 0.15741993486881256\n",
      "Epoch 4993, Loss: 1.4922951964399545, Final Batch Loss: 0.00013529339048545808\n",
      "Epoch 4994, Loss: 3.1035444140434265, Final Batch Loss: 1.5764546394348145\n",
      "Epoch 4995, Loss: 1.9445366859436035, Final Batch Loss: 0.4815621078014374\n",
      "Epoch 4996, Loss: 1.4810856860131025, Final Batch Loss: 0.013697205111384392\n",
      "Epoch 4997, Loss: 1.5078414047602564, Final Batch Loss: 0.00210111984051764\n",
      "Epoch 4998, Loss: 2.4596816301345825, Final Batch Loss: 0.898522675037384\n",
      "Epoch 4999, Loss: 2.4529789984226227, Final Batch Loss: 1.0214275121688843\n",
      "Epoch 5000, Loss: 1.9975832998752594, Final Batch Loss: 0.5742982029914856\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features.float())\n",
    "        \n",
    "        loss = criterion(preds, labels.long()) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss}, Final Batch Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  7  1  0  0  0  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 10  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 13  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  4  0  0  1  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11  0]\n",
      " [ 0  0  3  0  0  0  0  0  7  0  0  0  0  0  0  0  0  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91667   1.00000   0.95652        22\n",
      "           1    0.87500   1.00000   0.93333         7\n",
      "           2    0.50000   0.87500   0.63636         8\n",
      "           3    1.00000   1.00000   1.00000        12\n",
      "           4    1.00000   1.00000   1.00000         9\n",
      "           5    0.00000   0.00000   0.00000        12\n",
      "           6    0.93750   1.00000   0.96774        15\n",
      "           7    1.00000   0.90909   0.95238        11\n",
      "           8    0.35714   1.00000   0.52632        10\n",
      "           9    0.90909   1.00000   0.95238        10\n",
      "          10    0.92857   1.00000   0.96296        13\n",
      "          11    0.50000   0.50000   0.50000         2\n",
      "          12    1.00000   0.80000   0.88889        10\n",
      "          13    1.00000   0.83333   0.90909         6\n",
      "          14    0.50000   0.12500   0.20000         8\n",
      "          15    1.00000   1.00000   1.00000        10\n",
      "          16    1.00000   1.00000   1.00000        11\n",
      "          17    0.50000   0.16667   0.25000        12\n",
      "\n",
      "    accuracy                        0.81383       188\n",
      "   macro avg    0.77355   0.78939   0.75755       188\n",
      "weighted avg    0.79302   0.81383   0.78143       188\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hkimr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hkimr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hkimr\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    features, labels = batch\n",
    "    _, preds = torch.max(softmax(model(features.float())), dim = 1)\n",
    "    print(metrics.confusion_matrix((labels).cpu(), preds.cpu()))\n",
    "    print(metrics.classification_report((labels).cpu(), preds.cpu(), digits = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_1 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_1 = np.zeros(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_2 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_2 = np.ones(n_samples)\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U0A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_3 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_3 = np.ones(n_samples) + 1\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_4 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_4 = np.ones(n_samples) + 2\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_5 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_5 = np.ones(n_samples) + 3\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U1A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_6 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_6 = np.ones(n_samples) + 4\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_7 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_7 = np.ones(n_samples) + 5\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_8 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_8 = np.ones(n_samples) + 6\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U2A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_9 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_9 = np.ones(n_samples) + 7\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_10 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_10 = np.ones(n_samples) + 8\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_11 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_11 = np.ones(n_samples) + 9\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U3A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_12 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_12 = np.ones(n_samples) + 10\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_13 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_13 = np.ones(n_samples) + 11\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_14 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_14 = np.ones(n_samples) + 12\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U4A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_15 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_15 = np.ones(n_samples) + 13\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A0 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_16 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_16 = np.ones(n_samples) + 14\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A1 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_17 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_17 = np.ones(n_samples) + 15\n",
    "\n",
    "gen = Generator(z_dim = 100)\n",
    "gen.eval()\n",
    "load_model(gen, \"U5A2 Solo GAN Ablation_gen.param\")\n",
    "latent_vectors = get_noise(n_samples, 100)\n",
    "fake_features_18 = gen(latent_vectors).detach().numpy()\n",
    "\n",
    "y_18 = np.ones(n_samples) + 16\n",
    "\n",
    "\n",
    "fake_features = np.concatenate((fake_features_1, fake_features_2, fake_features_3, fake_features_4, fake_features_5, fake_features_6,\n",
    "                         fake_features_7, fake_features_8, fake_features_9, fake_features_10, fake_features_11, fake_features_12,\n",
    "                               fake_features_13, fake_features_14, fake_features_15, fake_features_16, fake_features_17, fake_features_18))\n",
    "fake_labels = np.concatenate((y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10, y_11, y_12, y_13, y_14, y_15, y_16, y_17, y_18))\n",
    "\n",
    "fake_features = torch.Tensor(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0 10  0  0  0  0  0  2  0  0  3]\n",
      " [ 0  0  0 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  1  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  3  0  0  5  0  0  0  0  0  7]\n",
      " [ 0  0  0  0  0  0 17  0  0  0  0  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0 13  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  1  0 11  0  0  4  0  0  0  0  0  3]\n",
      " [ 0  1  0  0  0  0  0  0  0 19  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0  0  0  0 18  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0  0 10  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  7  8  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0  0  0]\n",
      " [ 0  0 12  0  0  0  0  0  4  0  0  0  0  0  1  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 19  0]\n",
      " [ 0  0  5  0  0  0  0  0 10  0  0  0  0  0  0  0  0  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    1.00000   1.00000   1.00000        20\n",
      "         1.0    0.95238   1.00000   0.97561        20\n",
      "         2.0    0.17857   0.25000   0.20833        20\n",
      "         3.0    1.00000   1.00000   1.00000        20\n",
      "         4.0    0.90000   0.90000   0.90000        20\n",
      "         5.0    0.00000   0.00000   0.00000        20\n",
      "         6.0    0.89474   0.85000   0.87179        20\n",
      "         7.0    0.87500   0.35000   0.50000        20\n",
      "         8.0    0.22917   0.55000   0.32353        20\n",
      "         9.0    0.76000   0.95000   0.84444        20\n",
      "        10.0    0.58065   0.90000   0.70588        20\n",
      "        11.0    0.45455   0.50000   0.47619        20\n",
      "        12.0    1.00000   0.35000   0.51852        20\n",
      "        13.0    0.71429   1.00000   0.83333        20\n",
      "        14.0    0.33333   0.05000   0.08696        20\n",
      "        15.0    1.00000   1.00000   1.00000        20\n",
      "        16.0    1.00000   0.95000   0.97436        20\n",
      "        17.0    0.23810   0.25000   0.24390        20\n",
      "\n",
      "    accuracy                        0.65833       360\n",
      "   macro avg    0.67282   0.65833   0.63683       360\n",
      "weighted avg    0.67282   0.65833   0.63683       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, preds = torch.max(softmax(model(fake_features.float())), dim = 1)\n",
    "print(metrics.confusion_matrix((fake_labels), preds.cpu()))\n",
    "print(metrics.classification_report((fake_labels), preds.cpu(), digits = 5, zero_division = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
